# \(\mu\)P\({}^{2}\): Effective Sharpness Aware Minimization

Requires Layerwise Perturbation Scaling

Moritz Haas\({}^{1}\) &Jin Xu\({}^{2}\) &Volkan Cevher\({}^{3,4}\) &Leena Chennuru Vankadara\({}^{4}\)

\({}^{1}\)University of Tubingen, Tubingen AI Center\({}^{*}\) &\({}^{2}\)University of Oxford\({}^{*}\)

\({}^{3}\)LIONS, EPFL\({}^{*}\) &\({}^{4}\)AGI Foundations, Amazon

This work was conducted during Moritz', Jin's and Volkan's time at Amazon. Correspondence to: mo.haas@uni-tuebingen.de

###### Abstract

Sharpness Aware Minimization (SAM) enhances performance across various neural architectures and datasets. As models are continually scaled up to improve performance, a rigorous understanding of SAM's scaling behaviour is paramount. To this end, we study the infinite-width limit of neural networks trained with SAM, using the Tensor Programs framework. Our findings reveal that the dynamics of standard SAM effectively reduce to applying SAM solely in the last layer in wide neural networks, even with optimal hyperparameters. In contrast, we identify a stable parameterization with layerwise perturbation scaling, which we call _Maximal Update and Perturbation Parameterization_ (\(\mu\)P\({}^{2}\)), that ensures all layers are both feature learning and effectively perturbed in the limit. Through experiments with MLPs, ResNets and Vision Transformers, we empirically demonstrate that \(\mu\)P\({}^{2}\) achieves hyperparameter transfer of the joint optimum of learning rate and perturbation radius across model scales. Moreover, we provide an intuitive condition to derive \(\mu\)P\({}^{2}\) for other perturbation rules like Adaptive SAM and SAM-ON, also ensuring balanced perturbation effects across all layers.

## 1 Introduction

Sharpness Aware Minimization (SAM) (Foret et al., 2021) and its variants (Kwon et al., 2021; Muller et al., 2024) improves generalization performance across a wide range of neural architectures and datasets (Chen et al., 2021; Kaddour et al., 2022). In the SAM formulation, we minimize a given loss \(L\) between our prediction and the data \(y\) as a function of the architecture's weights \(W\), where an adversary simultaneously maximizes the same loss by perturbing the weights within a budget \(\rho\).

A standard SAM update for an \(L\)-hidden layer multi layer perceptron (MLP) is given by

\[W_{t+1}^{l}=W_{t}^{l}-\eta_{l}\nabla_{W^{l}}\mathcal{L}\left(f \left(\xi_{t};W_{t}+\bm{\varepsilon}_{t}\right),y_{t}\right),\ \text{ with }\ \ \bm{\varepsilon}_{t}^{l}=\rho\cdot\frac{\nabla_{W^{l}}\mathcal{L}(f(\xi_{t};W_{ t}),y_{t})}{\|\nabla_{\mathbf{W}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t})\|_{F}},\] (SAM)

where \(\mathbf{W}=[W^{1},\dots,W^{L+1}]\), \(t\) is the iteration count and \(\bm{\varepsilon}_{t}^{l}\) denotes the perturbation in the \(l\)-th MLP layer with width \(n\in\mathbb{N}\), and where we define an \(L\)-hidden layer MLP iteratively via

\[h^{1}(\xi):=W^{1}\xi,\qquad x^{l}(\xi):=\phi(h^{l}(\xi)),\qquad h ^{l+1}(\xi):=W^{l+1}x^{l}(\xi),\qquad f(\xi):=W^{L+1}x^{L}(\xi),\]

for inputs \(\xi\in\mathbb{R}^{d_{n}}\) with trainable weight matrices \(W^{1}\in\mathbb{R}^{n\times d_{in}}\), \(W^{l}\in\mathbb{R}^{n\times n}\) for \(l\in[2,L]\), and \(W^{L+1}\in\mathbb{R}^{d_{m}\times n}\). We call \(h^{l}\) preactivations, \(x^{l}\) activations, and \(f(\xi)\) output function. Despite the inherent difficulty of non-convex, non-concave optimization, SAM is quite successful in practice.

On the other hand, the steadily growing scale of foundation models has sparked considerable interest in scaling laws of model size and dataset size (Kaplan et al., 2020; Zhai et al., 2022). To rigorously understand learning dynamics under width scaling, Yang and Hu (2021) have recently provided general infinite-width theory for SGD, which has since been shown to be a good model for understanding the properties of large models (Vyas et al., 2024). Yang and Hu (2021) show that standard parameterizations (SP), including He or LeCun initialization (He et al., 2015; LeCun et al., 2002) with a global learning rate, do not learn features in the infinite-width limit.

Instead, a different scaling of layerwise initialization variances and learning rates, termed _Maximal Update Parameterization_ (\(\mu\)P), is necessary to achieve feature learning in all layers in wide networks. A crucial practical benefit of \(\mu\)P is the transferability of the optimal learning rate across model scales (Yang et al., 2022). This can drastically reduce computational costs as it allows to tune hyperparameters on smaller representative models and then to train the large model only once.

**Contributions.** In this paper, we adopt a scaling perspective to understand SAM's learning dynamics. Using the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023), this work provides the first infinite-width theory for SAM with important practical consequences:

1. We show that training an MLP with the standard (SAM) update rule is equivalent to applying perturbations only in the last layer in the infinite-width limit, even if the perturbation radius is properly tuned. This holds for any width-dependent scaling of layerwise initialization variances and learning rates, including SP and \(\mu\)P.
2. We demonstrate that the optimal perturbation radius can shift significantly in \(\mu\)P (Figure 1).
3. We postulate that jointly transferring the optimal learning rate \(\eta\) and perturbation radius \(\rho\) requires width-independent feature learning and _effective perturbations in every layer_ in the infinite-width limit. We define the perturbation of a trainable weight tensor to be _effective_ iff its effect on the output function scales width-independently. We show that this can be achieved with _layerwise scalings_ of the perturbation radius, and provide a complete characterization of perturbation scaling parameterizations into four regimes: unstable, vanishing, nontrivial and effective perturbations.
4. We derive the _Maximal Update and Perturbation Parameterization_ (\(\mu\)P\({}^{2}\)) that achieves both feature learning and effective perturbations in all layers in the infinite-width limit. We empirically demonstrate that \(\mu\)P\({}^{2}\) achieves hyperparameter transfer in both learning rate \(\eta\) and perturbation radius \(\rho\) (Figure 1).
5. We provide a versatile (spectral) scaling condition (\(*\)) applicable to architectures such as ResNets and Vision Transformers (ViTs), and to various SAM variants like SAM-ON and Adaptive SAM (ASAM), and any SAM updates modeled in a Tensor Program.

Figure 1: _Left and center_ (\(\bm{\mu}\)P\({}^{2}\) transfers both \(\eta\) and \(\rho\)): Test accuracy as a function of learning rate \(\eta\) and perturbation radius \(\rho\) of a 3-layer MLP in \(\mu\)P trained with SAM on CIFAR10 for various widths with global perturbation scaling \(\rho\cdot n^{-1/2}\) (_left_) and our layerwise perturbations scaling \(\mu\)P\({}^{2}\) (_right_), averaged over 3 independent runs.’\(\times\)’ denotes the optimum. Blue contours (the darker, the wider) denote the region within \(1\%\) of the optimal test accuracy smoothened with a Gaussian filter. Grey regions (the lighter, the wider) denote the unstable regime below \(30\%\) test accuracy. _Right_ (\(\bm{\mu}\)P\({}^{2}\) improves generalization): Same as left but sliced at the optimal learning rate of both parameterizations for width \(4096\) with the base optimizer SGD in \(\mu\)P (dashed line) as a baseline. Average and \(2\sigma\)-CI from \(16\) independent runs. Global perturbation scaling \(\rho\cdot n^{-1/2}\) achieves a width-independent critical perturbation radius at which training becomes unstable, but does not consistently improve over SGD in \(\mu\)P and does not transfer the optimal \((\eta,\rho)\). \(\mu\)P\({}^{2}\) achieves joint transfer in \((\eta,\rho)\) and improves generalization performance.

Background and related work

We here provide a short summary of related work. A more detailed account is provided in Appendix B.

**Sharpness Aware Minimization.** SAM was motivated as an inductive bias towards flatter minima and it provably reduces properties of the Hessian that are related to sharpness in simpler settings (Bartlett et al., 2023; Wen et al., 2023; Monzio Compagnoni et al., 2023). However a full understanding of why SAM works so well remains elusive (Andriushchenko et al., 2023; Wen et al., 2024). For example, applying SAM on only the normalization layers (SAM-ON) often improves generalization further despite increasing sharpness (Muller et al., 2024). A plethora of SAM variants have recently been proposed with the purpose of even stronger performance or reducing SAM's computational and memory complexity. We focus on two variants of Adaptive SAM (ASAM) (Kwon et al., 2021) which achieve the overall strongest results in Muller et al. (2024) (see Appendix F.4 for more details).

**Tensor Programs.** We build on the Tensor Programs framework (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023; Yang et al., 2022, 2023b), which covers many modern deep learning architectures, optimization algorithms and arbitrary \(abc\)-parametrizations. Each \(abc\)-parameterization is essentially defined by a layerwise scaling of initialization variance and learning rate as a function of network width. Beyond pure infinite-width limits, the simple \(\frac{1}{\sqrt{L}}\)-scaling allows depth-scaling in ResNets and unlocks hyperparameter transfer across depths (Hayou et al., 2021; Li et al., 2021; Bordelon et al., 2023; Yang et al., 2023b). Noci et al. (2022, 2024) provide infinite width and depth analyses for Transformers with the goal of preventing rank collapse.

Look-LayerSAM (Liu et al., 2022) already considers layerwise perturbation scaling with the goal of preserving good performance under large batch training. However, achieving \(\mu\)P\({}^{2}\) with Look-LayerSAM requires nontrivial layerwise learning rate and perturbation rescaling (see Appendix B).

## 3 SAM induces vanishing perturbations in wide neural networks

This section shows that under the standard (SAM) update rule, weight perturbations induced by SAM vanish in the infinite-width limit in every layer except the output layer. We later demonstrate that other SAM variants also selectively perturb other subsets of layers. For enhanced readability of some formulae, we use colors to distinguish four regimes of perturbation behaviour: Unstable, vanishing, nontrivial and effective perturbations.

While our theory covers any stable parameterization including He and LeCun initializations, for concreteness and for the clarity of exposition, we first present our results for MLPs under \(\mu\)P:

\[\text{initialize}\quad W^{1}\sim\mathcal{N}(0,1/d_{in}),\ W^{l} \in\mathbb{R}^{n\times n}\sim\mathcal{N}(0,1/n)\text{ for }l\in[2,L],\ W^{L+1}\sim\mathcal{N}(0,1/n^{2})\] \[\text{with layerwise SGD learning rates}\quad\eta_{1}=\eta n,\ \eta_{l}=\eta,\text{ for }l\in[2,L],\ \eta_{L+1}=\eta n^{-1}.\]

By analyzing the infinite-width behaviour of the SAM update rule, we show that the training dynamics under standard (SAM) become unstable as the network width increases. This result is first stated informally below in Proposition 1 and then more formally in the next section.

**Proposition 1** (**Instability of standard SAM parameterization in wide neural networks)**.: _Under \(\mu\)P with the standard (SAM) update rule and default perturbation given in (SAM), the output function becomes unbounded after the first update step in the infinite-width limit for any fixed, positive learning rate \(\eta>0\) and perturbation radius \(\rho>0\)._

Hence, to achieve stable optimization, it is necessary to introduce some width-dependent perturbation scaling \(\rho n^{-d}\) for some suitable \(d>0\). To understand the layerwise scaling behaviour of SAM under this scaling, we define the notion of _vanishing perturbations_.

**Vanishing perturbations.** The weight perturbation \(\bm{\varepsilon}^{l}\) perturbs the \(l\)-th layer's activations as

\[x^{l}+\tilde{\delta}x^{l}=\phi((W^{l}+\bm{\varepsilon}^{l})(x^{l-1}+\tilde{ \delta}x^{l-1})),\]

where \(\tilde{\delta}x^{l}\) denotes the perturbation of the \(l\)-th layer's activations accumulated from the weight perturbations \(\{\bm{\varepsilon}^{l^{\prime}}\}_{l^{\prime}\in[l]}\) in all previous layers. We say a layer \(l\) has _vanishing perturbations_ if \(\tilde{\delta}x^{l}\to 0\) as the width approaches infinity. This occurs if the weight perturbations in all previous layers are too small when measured in spectral norm, that is if \(\|\bm{\varepsilon}^{l^{\prime}}\|_{*}/\|W^{l^{\prime}}\|_{*}\to 0\text{ for all }l^{\prime}\in[l]\).

Informally, Proposition 2 below shows that for every choice of a decay parameter \(d>0\), either the training dynamics of SAM are unstable or all the hidden layers of the network have vanishing perturbations in the limit. The formal results are stated in the next section.

**Proposition 2** (**Global perturbation scaling is unstable or induces vanishing perturbations)**.: _Fix \(\rho>0\) and \(t\in\mathbb{N}\). Let \(\hat{f}_{t}\) denote the infinite-width limit of the output function after training an MLP of width \(n\) with the SAM update rule (SAM) with perturbation radius \(\rho n^{-d}\) for \(t\) steps. If \(d<1/2\), then output perturbations blow up, and \(\hat{f}_{t}\) is unstable. If \(d>1/2\), then the perturbations in all layers vanish and \(\hat{f}_{t}\) corresponds to the limit after \(t\) steps of SGD. If \(d=1/2\), then only the last layer is effectively perturbed, all other layers have vanishing perturbations._

Figure 2 shows statistics of an MLP trained with (SAM) with global width-dependent scaling \(\rho n^{-1/2}\) versus the same MLP trained with SAM where _only the last-layer weights are perturbed_ and \(\varepsilon^{l}=0\) for all \(l\in[L]\). As predicted by Proposition 2, both training algorithms produce equivalent training dynamics, already at moderate width, and last-layer perturbations are scaled correctly.

**Remark 3** (**Practical implications)**.: According to Proposition 2, for sufficiently wide models, any performance gains from standard SAM are primarily due to applying the SAM update rule to the last layer even with a properly tuned perturbation radius. This implies that, when applying the standard SAM update rule (SAM), one can remove the inner backward pass beyond the last layer and nearly recover the computational cost of SGD. However, it may be undesirable for optimal generalization if only the last layer is perturbed (Figure 1). 

**Layerwise perturbation scaling.** In the next section, we show that correcting the (SAM) update rule to achieve _effective perturbations_ in every single layer requires introducing additional hyperparameters -- layerwise width-dependent scaling of the perturbation radius. This is similar in spirit to \(\mu\)P which corrects standard parameterization by introducing layerwise scaling of the learning rates. We postulate that achieving width-independent scaling of both updates and perturbations is a necessary condition for hyperparameter transfer under SAM. We also lay all theoretical foundations and derive the stable parameterization, we call _maximal update and perturbation parameterization_ (\(\mu\)P\({}^{2}\)) that achieves both **feature learning and effective perturbations in all layers in the infinite-width limit**.

Figure 1 shows that \(\mu\)P\({}^{2}\) indeed achieves hyperparameter transfer in the optimal joint choice of \((\eta,\rho)\), while also achieving the best generalization performance (Table 2).

**General perturbation scaling condition.** For intuitive understanding and a generalization to other perturbation rules, a simple condition for achieving effective perturbations in any layer follows from our results: **in every layer, perturbations should scale like updates in \(\mu\)P**.

Figure 2: ((SAM) **effectively only perturbs the last layer) Layerwise weight perturbations (top) and normalized activation updates \(\|\Delta x^{l}\|_{2}\) (bottom) for SAM, last-layer SAM and SGD as a baseline across widths after training a \(3\)-layer MLP in \(\mu\)P with global perturbation scaling \(\rho\cdot n^{-1/2}\) for 20 epochs on CIFAR10. Average and CI are computed from \(4\) independent runs. Perturbations are normalized by the weight spectral norm to measure their effect on the layer’s output. Activation updates are normalized by \(\sqrt{\text{dim}(\Delta x^{l})}\) to measure coordinatewise updates. We provide more neural network statistics in Appendix H.1.**

The reason is that both updates and perturbations are gradient-based \(\nabla_{W^{l}}\mathcal{L}=\nabla_{h^{l}}\mathcal{L}\cdot(x^{l-1})^{\top}\), and thus low-rank and correlated with the incoming activations \(x^{l-1}\). Therefore updates and perturbations introduce the same LLN-like scaling factors, and require the same layerwise scaling corrections. Like Yang et al. (2023), we can rephrase this condition in terms of weight spectral norms to: For a weight matrix \(W^{l}_{t}\in\mathbb{R}^{\texttt{fan\_out\_for\_in}}\), its update \(\delta W^{l}_{t}\) and its perturbation \(\bm{\varepsilon}^{l}_{t}\), it should hold at all times \(t\) that

\[\|\bm{\varepsilon}^{l}_{t}\|_{*}=\Theta\left(\|\delta W^{l}_{t}\|_{*}\right)= \Theta\left(\|W^{l}_{t}\|_{*}\right)=\Theta\left(\sqrt{\texttt{fan\_out\_in}} \right).\] ( \[*\] )

with big-O notation that only tracks dependence on network width (Definition C.1). We discuss the spectral perspective in more detail in Appendix F.7.

## 4 Sharpness Aware Minimization in the infinite-width limit

Characterization of layerwise perturbation scaling: Unstable, vanishing, nontrivial and effective perturbations

To systematically and rigorously understand the width-scaling behaviour of neural networks trained under the SAM update rule, we propose a new class of parameterizations, which we refer to as _\(bcd\)-parameterizations_. Motivated by the analysis in Section 3, the class of \(bcd\)-parameterizations naturally extends \(abc\)-parameterizations (Yang and Hu, 2021) by including layerwise scaling of the perturbation radius. By setting all weight multiplier exponents \(a_{l}=0\), we do not need to modify the MLP architecture and recover representatives of each \(abc\)-parameterization that capture their essence and condense all equations: Ignoring numerical considerations (Blake et al., 2024), each \(abc\)-parameterization is essentially a layerwise initialization and learning rate scaling. The effects of weight multipliers on SAM are more nuanced than for SGD or Adam (see Remark 12 and Appendix F.6).

To study the infinite-width behaviour of networks trained with SAM in any \(bcd\)-parameterization, we utilize the theoretical framework of Ne\(\otimes\)or \(\top\)programs (Yang et al., 2023). We write the two forward and backward passes for each SAM update (ascent/perturbation step then descent/update step) using the Ne\(\otimes\)or\(\top\)computation rules and rigorously track all relevant scalings as provided by the Ne\(\otimes\)or\(\top\)master theorem. All proofs are provided in Appendix E. The full formal result statements can be found in Appendix D. Further theoretical considerations and generalizations around perturbation scaling are provided in Appendix F.

**Assumptions.** For clarity of exposition, we present our main results for MLPs. Their extension to other architectures is discussed in Appendix F.5. For all of the results in this section, we assume that the used activation function is either \(\tanh\) or \(\sigma\)-\(\texttt{gelu}\) for \(\sigma>0\) sufficiently small. For small enough \(\sigma>0\), \(\sigma\)-\(\texttt{gelu}\) (Definition C.9) approximates ReLU arbitrarily well. We also assume constant training time as width \(n\to\infty\). We assume batch size \(1\) for clarity, but our results can be extended without further complications to arbitrary fixed batch size as well as differing fixed batch sizes for the ascent/perturbation and the descent/update step, as sometimes used for SAM (Foret et al., 2021). Considering small perturbation batch size is practical, as it has been observed to enhance SAM's generalization properties (Andriushchenko and Flammarion, 2022).

**Definition 4** (\(bcd\)-parametrization).: A _\(bcd\)-parametrization_\(\{b_{l}\}_{l\in[L+1]}\cup\{c_{l}\}_{l\in[L+1]}\cup\{d\}_{l}\) defines the training of an MLP with SAM in the following way:

1. Initialize weights iid as \(W^{l}_{ij}\sim\mathcal{N}(0,n^{-2b_{l}})\).
2. Train the weights using the SAM update rule with layerwise learning rates, \[W^{l}_{t+1}=W^{l}_{t}-\eta n^{-c_{l}}\nabla_{W^{l}}\mathcal{L}\left(f\left( \bm{\xi}_{t};W_{t}+\bm{\varepsilon}_{t}\right),y_{t}\right),\] with the scaled perturbation \(\bm{\varepsilon}_{t}\) via layerwise perturbation radii, \[\bm{\varepsilon}_{t}:=\rho n^{-d}\frac{v_{t}}{\|v_{t}\|},\quad\text{with} \quad v_{t}=(v^{1}_{t},\dots,v^{L+1}_{t}),\quad v^{l}_{t}:=n^{-d_{l}}\cdot \nabla_{W^{l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t}),\] (LP)

W.l.o.g. we set \(\|v_{t}\|=\Theta(1)\), which prevents nontrivial width-dependence from the denominator. This imposes the constraints: \(d_{1}\geq 1/2-\min(b_{L+1},c_{L+1}),\ d_{l}\geq 1-\min(b_{L+1},c_{L+1})\) for \(l\in[2,L],\) and \(d_{L+1}\geq 1/2,\) with at least one equality required to hold (see Appendix E.1.3). The normalization \(v_{t}/\|v_{t}\|\) removes one degree of freedom from \(\{d_{l}\}_{l\in[L+1]}\) via the equivalence \(\{d^{\prime}_{l}\}_{l\in[L+1]}\cong\{d_{l}\}_{l\in[L+1]}\) iff there exists a \(C\in\mathbb{R}\) such that \(d^{\prime}_{l}=d_{l}+C\) for all \(l\in[L+1]\).

**Stability.** To ensure that the training dynamics of SAM are well-behaved with scale, we require \(bcd\)-parameterizations to satisfy conditions of stability. Perturbed weights \(\tilde{W}^{l}=W^{l}+\bm{\varepsilon}^{l}\) induce perturbed activations \(x^{l}+\tilde{\delta}x^{l}\) and a perturbed output function \(\tilde{f}_{t}(\xi):=f_{\tilde{W}_{t}}(\xi)\). We call a \(bcd\)-parameterization _stable_ (Definition C.3) if the hidden activations have width-independent scaling \(\Theta(1)\) at initialization and during training, and neither the updates nor the perturbations \(\tilde{\delta}x^{l}\) of the activations or output logits \(\tilde{f}_{t}-f_{t}\) blow up at any point in training.

For stating the conditions that characterize the class of stable \(bcd\)-parameterizations, we define the _maximal feature perturbation scaling_\(\tilde{r}\) of a \(bcd\)-parameterization as

\[\tilde{r}:=\min(b_{L+1},c_{L+1})+d+\min_{l=1}^{L}(d_{l}-\mathbb{I}(l\neq 1)).\]

Similar to the maximal feature update scaling \(r\) from Yang and Hu (2021), \(\tilde{r}\) describes how much the last hidden-layer activations are perturbed as a function of width, \(x^{L}+\tilde{\delta}x^{L}=\Theta(n^{-\tilde{r}})\). Hidden-layer activation perturbations do not explode with width if and only if \(\tilde{r}\geq 0\). The output perturbations not to blow up if and only if \(d+d_{L+1}\geq 1\) and \(b_{L+1}+\tilde{r}\geq 1\). In particular, this implies that any stable \(bc\)-parameterization together with naive perturbation scaling \(d_{l}=d=0\) for all \(l\in[L+1]\) is _unstable due to blowup in the last layer_. We formally state the stability characterization in Theorem D.2. Ideally, we will later require width-independent perturbation scaling which is attained iff \(\tilde{r}=0\).

**Effective SGD dynamics.** Within the class of stable parameterizations, there are parameterizations in which perturbations in the output vanish in the infinite-width limit at any point during training. In other words, SAM training dynamics collapses to SGD dynamics with scale. We are mostly interested in the opposing class of parameterizations with non-vanishing perturbations. We characterize this class in Theorem 6 and refer to them as _perturbation nontrivial_ (Definition 5).

**Definition 5** (**Perturbation nontriviality)**.: We say that a stable \(bcd\)-parametrization is _perturbation nontrivial_ if there exists a training routine, \(t\in\mathbb{N}_{0}\) and \(\xi\in\mathbb{R}^{d_{\text{in}}}\) such that \(\tilde{\delta}f_{t}(\xi):=f_{\tilde{W}_{t}}(\xi)-f_{\tilde{W}_{t}}(\xi)=\Omega (1)\). Otherwise, the \(bcd\)-parametrization is _perturbation trivial_. 

**Theorem 6** (**Perturbation nontriviality characterization)**.: _A stable \(bcd\)-parametrization is perturbation nontrivial if and only if \(d+d_{L+1}=1\) or \(\min(b_{L+1},c_{L+1})+\tilde{r}=1\)._

For the class of stable and perturbation nontrivial \(bcd\)-parameterizations, SAM learning is both stable and deviates from SGD dynamics. A natural question to ask here is: what should be the ideal SAM behaviour in the infinite-width limit? To address this question, we make the following crucial distinction between _non-vanishing_ and _effective perturbations_.

**Non-vanishing versus effective perturbations.** Recall that the weight perturbation \(\bm{\varepsilon}^{l}\) perturbs the \(l\)-th layer's activations as

\[x^{l}+\tilde{\delta}x^{l}=\phi((W^{l}+\bm{\varepsilon}^{l})(x^{l-1}+\tilde{ \delta}x^{l-1})),\]

where \(\tilde{\delta}x^{l}\) denotes the perturbation of the \(l\)-th layer's activations accumulated from the weight perturbations \(\{\bm{\varepsilon}^{l^{\prime}}\}_{l^{\prime}\in[l]}\) in all previous layers. Therefore, perturbations \(\tilde{\delta}x^{l}\) can stem both from weight perturbations \(\bm{\varepsilon}^{l^{\prime}}\) in a previous layer \(l^{\prime}<l\) and/or from weight perturbations \(\bm{\varepsilon}^{l}\) in the current layer \(l\). Intuitively, if we perturb a layer, we want this to affect the next layer's activations and thereby have a nontrivial effect on the output function. Otherwise one can simply set the layer's perturbations to \(0\) by design and not change the learning algorithm in the infinite-width limit. This motivates the definition of _effective perturbations_, which demands the weight perturbations of the current layer to contribute non-vanishingly. From the weight perspective (\(*\)), effective \(l\)-th layer perturbations are achieved if and only if weight perturbations scale like the weights in spectral norm, \(\|\bm{\varepsilon}^{l}\|_{*}/\|W^{l}\|_{*}=\Theta(1)\). Without an effective perturbation \(\bm{\varepsilon}^{l}\) of the \(l\)-th layer, this layer does not inherit SAM's inductive bias towards low spectral norm of the Hessian or enhanced sparsity and does not improve generalization performance. We provide empirical evidence for these claims in Appendix H.2. Therefore a distinction between _non-vanishing_ and _effective perturbations_ is crucial.

**Definition 7** (Non-vanishing perturbations).: For \(l\in[L]\), we say that a stable parameterization has _non-vanishing perturbations in the \(l\)-th layer_ if there exists a \(t\in\mathbb{N}\) such that \(\tilde{\delta}x^{l}_{t}=\Omega(1)\). 

**Definition 8** (**Effective perturbations)**.: For \(l\in[L+1]\), we say that a stable parameterization effectively perturbs the \(l\)-th layer_ if there exists a \(t\in\mathbb{N}\) such that \(\bm{\varepsilon}^{l}_{t}(x^{l-1}_{t}+\tilde{\delta}x^{l-1}_{t})=\Theta(1)\), where \(x^{0}_{t}+\tilde{\delta}x^{0}_{t}=\xi_{t}\).

Theorem 9 provides a characterization of stable \(bcd\)-parameterizations with vanishing perturbations in any given layer.

**Theorem 9** (Vanishing perturbation characterization).: _For any \(l_{0}\in[L]\), the following statements are equivalent:_

1. _A stable_ \(bcd\)_-parametrization has vanishing perturbations in layer_ \(l_{0}\)_._
2. _A stable_ \(bcd\)_-parametrization has vanishing perturbations in layer_ \(l\) _for all_ \(1\leq l\leq l_{0}\)_._
3. \(\tilde{r}_{l_{0}}:=\min(b_{L+1},c_{L+1})+d+\min_{m=1}^{l_{0}}(d_{m}-\mathbb{I} (m\neq 1))>0\)_._

It follows from Theorem 9 that any stable \(bcd\)-parameterization that performs updates in the original gradient direction (i.e., \(d_{l}=C\) for all \(l\in[L+1]\) for some \(C\in\mathbb{R}\)) has vanishing perturbations in all input and hidden layers \(l\in[L]\), and the last layer \(l=L+1\) is effectively perturbed if and only if \(d=1/2\). This covers the case of both standard and maximal update parameterizations with global scaling of the perturbation radius discussed in Section 3. Negating the conditions of Theorem 9 implies that a stable \(bcd\)-parameterization has non-vanishing perturbations in layer \(l_{0}\) if and only if \(\tilde{r}_{l_{0}}=0\). Achieving _effective perturbations_ is a stronger requirement for which Theorem 10 provides the necessary and sufficient conditions.

**Theorem 10** (**Effective perturbation characterization)**.: _For \(l\in[L]\), a stable \(bcd\)-parametrization effectively perturbs the \(l\)-th layer if and only if \(\min(b_{L+1},c_{L+1})+d+d_{l}-\mathbb{I}(l\neq 1)=0\)._

_A stable \(bcd\)-parametrization effectively perturbs the last layer if and only if \(d+d_{L+1}=1\)._

### Maximal Update and Perturbation Parameterization (\(\mu\)P\({}^{2}\))

We postulate that just as the optimal learning rate transfers across widths under \(\mu\)P for SGD and Adam due to non-vanishing width-independent feature evolution in all layers, the optimal learning rate and perturbation radius may be jointly transferable across widths if additionally the weight perturbations induce width-independent perturbations of the activations in all layers. Here, we show that, for every stable initialization and learning rate scaling with \(b_{L+1}\geq 1\), there exists a unique stable layerwise perturbation scaling which effectively perturbs every single layer. We term this layerwise perturbation scaling \(\{d_{l}\}_{l\in[L+1]}\cup\{d\}\) the Maximal Perturbation Parameterization (MPP). This concludes the phase characterization of perturbation scaling behaviours (Figure 3).

**Theorem 11** (**Maximal Perturbation Parameterization (MPP))**.: _Consider any stable \(bcd\)-parametrization \(\{b_{l}\}_{l\in[L+1]}\cup\{c_{l}\}_{l\in[L+1]}\cup\{d_{l}\}_{l\in[L+1]}\cup\{d\}\). If \(b_{L+1}<1\), then there does not exist a stable choice of \(\{d_{l}\}_{l\in[L+1]}\cup\{d\}\) that achieves effective perturbations before the last layer. If \(b_{L+1}\geq 1\), then up to the equivalence \(d^{\prime}_{l}=d_{l}+C\), \(C\in\mathbb{R}\), \(\forall l\in[L+1]\), the unique stable choice \(\{d_{l}\}_{l\in[L+1]}\cup\{d\}\) that effectively perturbs all layers \(l\in[L+1]\) is given by_

\[d=-1/2,\qquad d_{l}=\begin{cases}1/2-\min(b_{L+1},c_{L+1})&l=1,\\ 3/2-\min(b_{L+1},c_{L+1})&l\in[2,L],\\ 3/2&l=L+1.\end{cases}\] (1)

Figure 3: (**Perturbation phase characterization of bcd-parameterizations**) Given a choice of layerwise initialization and learning rate scalings \(\{b_{l},c_{l}\}_{l\in[L+1]}\), the maximal feature perturbation scaling \(\tilde{r}\) and the last-layer perturbation scaling \(d+d_{L+1}\) completely determine whether a \(bcd\)-parameterization is unstable, has effective SGD dynamics, effective perturbations in some but not all layers or effective perturbations in all layers. In SP or NTP (left), there does not exist a choice of perturbation scalings that achieves effective perturbations in all layers, whereas in \(\mu\)P (right), there is a unique choice as provided in Theorem 11.

**Maximal Update and Perturbation Parameterization \(\bm{\mu}\)P2.** To achieve feature learning in every layer and hyperparameter transfer in the learning rate, \(\mu\)P is the unique2 choice of layerwise initialization variance and learning rate scalings \(\{b_{l},c_{l}\}_{l\in[L+1]}\)(Yang and Hu, 2021). Together with Theorem 11, this shows that there exists a _unique_2\(bcd\)-parameterization that achieves both feature learning and effective perturbations in all layers, we call _maximal update and perturbation parametrization_, \(\mu\)P2 for short. Now that we have found a parameterization that achieves width-independent scaling of both activation updates and activation perturbations, \(\mu\)P2 fulfills essential necessary conditions for hyperparameter transfer to occur in both \(\eta\) and \(\rho\).

Footnote 2: Strictly speaking, unique up to smaller last-layer initialization \(b_{L+1}\geq 1\).

**Remark 12** (**Achieving \(\bm{\mu}\)P2 with weight multipliers)**.: Appendix F.6 covers the extension of our results to nontrivial weight multipliers. We show that, for each choice of weight multipliers \(\{a_{l}\}_{l\in[L+1]}\), there is a unique2 choice of \(bcd\)-hyperparameters that achieves effective perturbations in all layers. But unlike for SGD or Adam, these parameterizations lead to slightly different training algorithms, because differing subsets of layers contribute non-vanishingly to the joint gradient normalization term \(\|\nabla_{\mathbf{W}}\mathcal{L}\|_{F}\) in (SAM). The term \(\|\nabla_{\mathbf{W}}\mathcal{L}\|_{F}\) couples all layers so that there do not exist layerwise but only layer-coupled equivalence classes for (SAM). Most importantly, **instead of adapting** (SAM), **we can adapt the architecture with the weight multipliers \(n^{-a_{l}}\cdot W^{l}\) with**

Footnote 2: Strictly speaking, unique up to smaller last-layer initialization \(b_{L+1}\geq 1\).

\[a_{l}=-1/2\cdot\mathbb{I}(l=1)+1/2\cdot\mathbb{I}(l=L+1)\] ( \[a\text{-}\mu P^{2}\] )

**to achieve effective perturbations in all layers with naive perturbation and learning rate scaling such that all layers contribute non-vanishingly to the joint gradient norm (Appendix F.6).** One downside of (\(a\text{-}\mu P^{2}\)), that also applies to naive weight multipliers \(a_{l}=0\), is its incompatibility with unit scaling considerations for low precision training (Blake et al., 2024). 

**Alternative perturbation scaling definitions.** Scaling equivalent to (\(a\text{-}\mu P^{2}\)) can be achieved without multipliers by scaling the numerator and denominator terms in (LP) independently, and choosing to scale all denominator terms to be width-independent (see perturbation rule (DP) and Appendix F.7 for more details). The ablations in Appendix H.4 suggest that this has a negligible effect on the optimal generalization performance of \(\mu\)P2, but can be more stable given suboptimal hyperparameters. Gradient normalization in each layer separately is uncommon and performs slightly worse (Appendix H.5). Appendix F.2 discusses further considerations that led to Definition 4.

**Trivial, lazy, and feature learning regimes.** A small last-layer initialization variance \(b_{L+1}\geq 1\) is required for stable feature learning. Theorem 11 shows that \(b_{L+1}\geq 1\) is also required for effective hidden-layer perturbations. Beyond this condition, the choice of \(\{b_{l}\}\) and \(\{c_{l}\}\) is decoupled from that of perturbation scalings \(\{d_{l}\}\cup\{d\}\) for stable \(bcd\)-parameterizations, because the scale of the activations of a layer \(l\) is entirely determined by the scale of initialization \(b_{l}\) and learning rates \(c_{l}\), given stability. Consequently, whether a parameterization is _trivial_, in the _lazy regime_, or in the _feature learning regime_ is independent of the choice of \(d_{l}\)'s provided that all stability constraints are met. A complete characterization of these regimes for the class of \(bc\)-parameterizations has been provided in Yang and Hu (2021) and remains unchanged for the class of stable \(bcd\)-parameterizations. For completeness, formal definitions and the corresponding results are stated in Appendices C and D.

### Generalizations to other architectures and SAM variants

**Generalization to other architectures.** Our results can be extended to other common layer types, that are representable as a Ne\(\otimes\)or\(\top\)program, including all ResNet and Transformer components (Appendix F.5). All considered layer types behave like input, hidden or output layers. Most importantly, normalization layer weights and biases scale like input layer weights to the input \(1\).

**Generalization to other SAM variants.** We would like to find the correct layerwise perturbation scaling without writing out the Ne\(\otimes\)or\(\top\)program for every perturbation rule individually. Formally justified by our proof in Appendix E, we rephrase our equivalent spectral scaling condition (\(*\)) from Section 3 to: maximal stable perturbations are achieved in \(\mu\)P if and only if \(\varepsilon^{l}=\Theta(\delta W^{l})\). This condition holds as soon as weight updates \(\delta W^{l}\) and perturbations \(\varepsilon^{l}\) are both correlated with the incoming activations \(x^{l-1}\), for example if both are gradient-based. Table 1 summarizes the application of this condition to two ASAM variants that perform well empirically but cannot be written as a Ne\(\otimes\)or\(\top\)program. Additional details are provided in Appendix F.4. We demonstrate that these scalings perform well and transfer hyperparameters in the next section. Note that for hidden layers in\(\mu\)P\({}^{2}\), it holds that \(\bm{\varepsilon}^{l}=\Theta(n^{-1})\) but \(W^{l}=\Theta(n^{-1/2})\) entrywise, due to large initialization, showing that it is crucial to compare perturbations to updates or to measure weight scalings in spectral norm.

The maximal update and perturbation parameterization \(\bm{\mu\mathrm{P}^{2}}\) achieves hyperparameter transfer and improved generalization

In this section, we provide experimental results showing that \(\mu\)P\({}^{2}\) achieves hyperparameter transfer in both \(\eta\) and \(\rho\) across architectures, and that \(\mu\)P\({}^{2}\) also improves generalization over SP and \(\mu\)P with global perturbations - even after multi-epoch training to convergence. We train MLPs and ResNets (He et al., 2016) on CIFAR10 (Krizhevsky et al., 2009) and Vision Transformers (ViTs) (Dosovitskiy et al., 2021) on Imagenet1K (Deng et al., 2009). While we directly implement _bcd_-parameterizations for MLPs and ResNets in PyTorch (Paszke et al., 2019), we use the mup-package (Yang et al., 2022) as a basis for ViT experiments. Pseudocode and a spectral derivation of our \(\mu\)P\({}^{2}\)-implementation for ViTs, which is equivalent to (\(a\)-\(\mu P^{2}\)), are provided in Appendix F.7. All experimental details are stated in Appendix G and all supplemental experiments can be found in Appendix H.

**Comparing candidate parameterizations in MLPs.** Figure 1 shows test accuracy as a function of learning rate and perturbation radius for MLPs of varying width. While previous \(\mu\)P-literature mostly focuses on the more immediate transfer in training error, for SAM it is crucial to consider optimality in test error as the perturbation radius acts as a regularizer, so that optimality in test error typically coincides with suboptimal training error. In \(\mu\)P without perturbation scaling, the regime of stable perturbation radii shrinks (Figure H.6). In \(\mu\)P with global perturbation scaling \(\rho\cdot n^{-1/2}\), the regime of stable \(\rho\) remains invariant under width scaling, but there is no significant improvement of SAM beyond SGD, so that the optimal perturbation radius fluctuates within its stable regime due to noise. Only \(\mu\)P\({}^{2}\) consistently achieves hyperparameter transfer across widths, and achieves significant improvement over its base optimizer SGD in \(\mu\)P at scale. The full hyperparameter landscapes are provided in Appendix H.3.

\(\rho\)**-transfer in ViTs.** Figure 4 shows that the optimal perturbation radius transfers for ViT-S/16 on Imagenet1K trained with SAM in \(\mu\)P\({}^{2}\). While Andriushchenko and Flammarion (2022, Appendix E.3) observe diminishing benefits of SAM at large widths in SP, here the improvements beyond the base optimizer AdamW in \(\mu\)P are particularly large.

\(\rho\)**-transfer for SAM variants in \(\mu\)P\({}^{2}\).** Figure 6 shows that training a ResNet-18 in \(\mu\)P\({}^{2}\) achieves hyperparameter transfer in \(\rho\) for all considered SAM variants with varying width. \(\mu\)P with global perturbation scaling (\(\mu\)P-global) has a width-invariant stability threshold in \(\rho\) and the optimal \(\rho\) clearly shifts toward that threshold. It would be interesting to see whether this shift continues with larger width and leads to suboptimal performance of \(\mu\)P-global in wider ResNets. Table 2 shows that all SAM variants perform similarly well in \(\mu\)P\({}^{2}\), some slightly outperforming the best

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Perturbed under global scaling?**} & \multicolumn{3}{c}{**For effective perturbations with \(\mu\)P\({}^{2}\):**} \\  & Input, biases, norm. & Other & Output & Global & Input- & Hidden- & Output- \\ \hline \hline SAM & ✗ & ✗ & ✓ & \(n^{1/2}\) & \(n^{1/2}\) & \(n^{-1/2}\) & \(n^{-3/2}\) \\ Layer. ASAM & ✗ & ✓ & ✗ & \(1\) & \(1\) & \(n^{-1}\) & \(1\) \\ Elem. ASAM & ✓ & ✓ & ✓ & \(n^{1/2}\) & \(1\) & \(1\) & \(1\) \\ SAM-ON & ✓ & - & - & \(n^{1/2}\) & \(1\) & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 1: (**Layerwise perturbation scaling for effective perturbations in \(\mu\)P**) Without layerwise perturbation scaling (_left_), each SAM variant perturbs a different subset of layers at large width \(n\to\infty\), but we provide the unique layerwise perturbation rescaling \(\mu\)P\({}^{2}\) (_right_) that achieves effective perturbations in all layers. This parameterization transfers both the optimal \(\eta\) and \(\rho\) across widths.

Figure 4: (\(\rho\)**-transfer in ViTs) Training a ViT with SAM in \(\mu\)P\({}^{2}\) on ImageNet1K from scratch for 100 epochs yields \(\rho\)-transfer and large improvements over AdamW in \(\mu\)P (dashed lines).**

performing variant SAM-ON in SP. This suggests that for ResNets, even with a proper layerwise balance, normalization layer perturbations may suffice, and performance differences in SP are primarily caused by varying degrees to which the normalization layers are perturbed.

Without providing an explanation, Muller et al. (2024, Section 5.3) observe that only SAM-ON and elementwise ASAM sufficiently perturb normalization layers in SP. Table 1 (_left_) explains these observations by showing that only these two SAM variants effectively perturb normalization layers under global perturbation scaling. Table 1 (_right_) also provides full control over which layers to perturb. For transferring the optimal \(\rho\) with SAM-ON in \(\mu\)P, our theory predicts the global scaling \(\rho=\Theta(n^{1/2})\) which is confirmed by our empirical observations (Figure 6). However, properly understanding the role of normalization layer perturbations remains an important question for future work. Note that we report results after fine-tuning all hyperparameters. The performance gain of \(\mu\)P\({}^{2}\) over SP and \(\mu\)P-global is likely much higher in larger models, for which fine-tuning is infeasible and the lack of feature learning and effective perturbations is more pronounced. Even under optimal HPs, \(\mu\)P\({}^{2}\) appears to stabilize SAM's training dynamics compared to SP (Figure 5).

## 6 Future work

This study may serve as an inspiration of how scaling theory can be used to understand and improve training procedures in minimax optimization and beyond. To reach a fully practical theory of deep learning, it will be necessary to take data distributions and training dynamics into account in more detail than it is possible with current Tensor Program theory (Everett et al., 2024). Existing Tensor Program theory assumes constant batch size and training time, and does not make statements about generalization. For example, we observe that MLPs and ResNets in SP can sometimes display HP transfer in \(\eta\) and \(\rho\) after multi-epoch training to convergence (Appendix H.3.2). This goes beyond the observations by Everett et al. (2024) as we observe transfer even without tuning layerwise learning rates or weight multipliers. This transfer strongly contradicts the infinite-width theory from Yang and Hu (2021) which predicts output blowup under large learning rates, and it shows that the exact conditions which enable hyperparameter transfer in practice are not fully understood. It also remains unclear how to optimally adapt (SAM) when increasing network depth. We plan to address some of these questions in upcoming work.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & SAM global & SAM \(\mu\)P\({}^{2}\) & SAM-ON \(\mu\)P\({}^{2}\) & Elem. ASAM \(\mu\)P\({}^{2}\) \\ \hline SP & \(97.00_{\pm 0.03}(+0.96)\) & \(97.00_{\pm 0.03}(+0.96)\) & \(\mathbf{97.29}_{\pm 0.06}(+1.26)\) & \(97.15_{\pm 0.01}(+1.11)\) \\ \(\mu\)P & \(\mathbf{97.19}_{\pm 0.05}(+0.93)\) & \(\mathbf{97.23}_{\pm 0.08}(+0.97)\) & \(\mathbf{97.34}_{\pm 0.08}(+1.08)\) & \(\mathbf{97.32}_{\pm 0.05}(+1.06)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: (**Performance of \(\mu\)P\({}^{2}\)) Average test accuracy\({}_{\pm\text{standard deviation across 4 runs}}\) (\(+\) improvement of SAM over SGD) for ResNet-18 with width multiplier \(4\) on CIFAR10 using SGD as a base optimizer. In bold, all parameterizations within a \(2\sigma\)-CI from the best-performing variant SAM-ON in \(\mu\)P\({}^{2}\).**

Figure 5: (**Stable training dynamics) SAM in \(\mu\)P\({}^{2}\) stabilizes training dynamics for a ResNet-18 with width multiplier \(2\). For a ResNet-18 with width multiplier \(2\).

Figure 6: (\(\rho\)**-transfer of ASAM variants in \(\mu\)P\({}^{2}\)) Test error as a function of perturbation radius \(\rho\) after \(200\) epochs of training a ResNet-18 in \(\mu\)P\({}^{2}\) on CIFAR10 with various SAM variants (see subplot title). CI over 2 independent runs. Darker lines correspond to larger width multipliers. Other hyperparameters are tuned at base width multiplier \(0.5\). \(\mu\)P\({}^{2}\) achieves transfer in \(\rho\) and large improvements over the base optimizer (dashed lines) SGD in \(\mu\)P with momentum and weight decay.**

[MISSING_PAGE_FAIL:11]

[MISSING_PAGE_FAIL:12]

Yann LeCun, Leon Bottou, Genevieve B Orr, and Klaus-Robert Muller. Efficient backprop. In _Neural networks: Tricks of the trade_. Springer, 2002. Cited on page 2.
* [10]M. Li, M. Nica, and D. Roy (2021) The future is log-gaussian: resnets and their infinite-depth-and-width limit at initialization. In Advances in Neural Information Processing Systems (NeurIPS), Cited on page 3.
* [11]Y. Liu, S. Mai, X. Chen, C. Hsieh, and Y. You (2022) Towards efficient and scalable sharpness-aware minimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [12]P. M. Long and P. L. Bartlett (2023) Sharpness-aware minimization and the edge of stability. arXiv:2309.12488. Cited on page 1.
* [13]S. Mei, A. Montanari, and P. Nguyen (2018) A mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences115 (33), pp. E7665-E7671. Cited on page 1.
* [14]E. Monzio Compagnoni, L. Biggio, A. Orvieto, F. W. Proske, H. Kersting, and A. Lucchi (2023) An SDE for modeling SAM: theory and insights. In Proceedings of the 40th International Conference on Machine Learning (ICML), Cited on page 3.
* [15]M. Muller, T. Vlaar, D. Rolnick, and M. Hein (2024) Normalization layers are all that sharpness-aware minimization needs. Advances in Neural Information Processing Systems (NeurIPS)36. Cited on page 1.
* [16]R. M. Neal (1996) Priors for infinite networks. Springer New York. Cited on page 1.
* [17]L. Noci, S. Anagnostidis, L. Biggio, A. Orvieto, S. Pal Singh, and A. Lucchi (2022) Signal propagation in transformers: theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems (NeurIPS)35. Cited on page 3.
* [18]L. Noci, C. Li, M. Li, B. He, T. Hofmann, C. J. Maddison, and D. Roy (2024) The shaped transformer: attention models in the infinite depth-and-width limit. Advances in Neural Information Processing Systems (NeurIPS)36. Cited on page 3.
* [19]A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala (2019) PyTorch: an imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems (NeurIPS), Cited on page 3.
* [20]B. Poole, S. Lahiri, M. Raghu, J. Sohl-Dickstein, and S. Ganguli (2016) Exponential expressivity in deep neural networks through transient chaos. Advances in Neural Information Processing Systems (NeurIPS)29. Cited on page 3.
* [21]D. Samuel (2022) (Adaptive) SAM Optimizer (PyTorch). Note: https://github.com/davda54/sam Cited on page 3.
* [22]S. S. Schoenholz, J. Gilmer, S. Ganguli, and J. Sohl-Dickstein (2016) Deep information propagation. arXiv:1611.01232. Cited on page 3.
* [23]S. Shin, D. Lee, M. Andriushchenko, and N. Lee (2023) The effects of overparameterization on sharpness-aware minimization: an empirical and theoretical analysis. arXiv:2311.17539. Cited on page 3.
* [24]L. Chennuru Vankadara, J. Xu, M. Haas, and V. Cevher (2024) On feature learning in structured state space models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems (NeurIPS), Cited on page 3.
* [25]J. Sun, J. Sun, and J. Sun (2020) A new approach to the learning of neural networks. In Proceedings of the 2020 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [26]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [27]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [28]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [29]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [30]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [31]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [32]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [33]J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [34]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [35]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [36]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [37]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [38]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [39]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [40]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [41]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [42]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [43]J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [44]J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [45]J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [46]J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [47]J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [48]J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [49]J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [50]J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [51]J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [52]J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [53]J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, J. Sun, and J. Sun (2021) A new approach to the learning of neural networks. In Proceedings of the 2021 IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 3.
* [54]J. Sun, J. Sun, J.

Nikhil Vyas, Alexander Atanasov, Blake Bordelon, Depen Morwani, Sabarish Sainathan, and Cengiz Pehlevan. Feature-learning networks are consistent across widths at realistic scales. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2024. Cited on page 2, 16, 60.
* Wen et al. (2023) Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How sharpness-aware minimization minimizes sharpness? In The Eleventh International Conference on Learning Representations (ICLR), Cited on page 3, 17.
* Wen et al. (2023) Kaiyue Wen, Zhiyuan Li, and Tengyu Ma. Sharpness minimization algorithms do not only minimize sharpness to achieve better generalization. _Advances in Neural Information Processing Systems (NeurIPS)_, 36, 2024. Cited on page 3, 17.
* Wenger et al. (2023) Jonathan Wenger, Felix Dangel, and Agustinus Kristiadi. On the disconnect between theory and practice of overparametrized neural networks. _arXiv:2310.00137_, 2023. Cited on page 16.
* Wu et al. (2020) Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Zhicheng Yan, Masayoshi Tomizuka, Joseph Gonzalez, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing for computer vision. _arXiv:2006.03677_, 2020. Cited on page 51.
* L. Xiao, J. Pennington, and S. Schoenholz (2020) Disentangling trainability and generalization in deep neural networks. In International Conference on Machine Learning (ICML), Cited on page 16.
* G. Yang (2019) Wide feedforward or recurrent neural networks of any architecture are gaussian processes. _Advances in Neural Information Processing Systems (NeurIPS)_32, 2019. Cited on page 2, 3, 16, 42.
* G. Yang (2021) Tensor programs iii: neural matrix laws. _arXiv:2009.10685_, 2021. Cited on page 24.
* G. Yang and E. J. Hu (2021) Tensor programs iv: feature learning in infinite-width neural networks. In International Conference on Machine Learning (ICML), Cited on page 2, 3, 5, 6, 8, 10, 16, 18, 20, 21, 29, 31, 32, 33, 41.
* G. Yang and E. Littwin (2023) Tensor programs ivb: adaptive optimization in the infinite-width limit. _arXiv:2308.01814_, 2023. Cited on page 2, 3, 16, 17, 19, 41.
* G. Yang, E. J. Hu, I. Babuschkin, S. Sidor, X. Liu, D. Farhi, N. Ryder, J. Pachocki, W. Chen, and J. Gao (2022) Tensor programs v: tuning large neural networks via zero-shot hyperparameter transfer. _arXiv:2203.03466_, 2022. Cited on page 2, 3, 9, 16, 19, 43, 51, 57, 60, 61, 71.
* G. Yang, J. B. Simon, and J. Bernstein (2023) A spectral condition for feature learning. _arXiv:2310.17813_, 2023a. Cited on page 5, 17, 39, 47.
* G. Yang, D. Yu, C. Zhu, and S. Hayou (2023) Tensor programs vi: feature learning in infinite-depth neural networks. _arXiv:2310.02244_, 2023b. Cited on page 5, 16, 17, 43.
* Y. You, J. Li, S. Reddi, J. H., S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer, and C. Hsieh (2020) Large batch optimization for deep learning: training bert in 76 minutes. In International Conference on Learning Representations (ICLR), Cited on page 17.
* X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer (2022) Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Cited on page 2, 17.

## Appendix Contents.

* A Notation
* B Detailed related work
* C Definitions
* D Extensive main results
* E Proof of main results
* E.1 Tensor program formulation
* E.2 The infinite-width limit
* E.3 Concluding the proof of all main results
* E.4 Analytic expression of the features after first SAM update
* F Generalizations and further perturbation scaling considerations
* F.1 Overview over choices of \(d_{l}\) and \(d\)
* F.2 Other ways to introduce layerwise perturbation scaling
* F.3 Extension to SAM without gradient normalization
* F.4 Extension to Adaptive SAM
* F.5 Representing general architectures and adaptive optimizers as Tensor Programs
* F.6 Influence of width-dependent weight multipliers on \(bcd\)-parameterizations
* F.7 The spectral perspective on \(\mu P^{2}\)
* G Experimental details
* H Supplemental experiments
* H.1 SAM is approximately LL-SAM in \(\mu\)P with global perturbation scaling
* H.2 Propagating perturbations from the first layer does not inherit SAM's benefits
* H.3 Hyperparameter transfer
* H.4 Gradient norm contributions have negligible effects on generalization performance
* H.5 SAM with layerwise gradient normalization
* H.6 Test error over the course of training

## Appendix A Notation

## Appendix B Detailed related work

**Signal propagation.** Our work can be seen as scaling theory with the goal of preventing both vanishing and exploding signals in forward and backward passes, where the analysis of SAM requires considering stability of perturbations in each layer as well. In this sense, we build on a rich literature, often restricted to an analysis at initialization (Schoenholz et al., 2016; Poole et al., 2016; Hanin and Rolnick, 2018; Xiao et al., 2020). For scaling neural networks to infinite depth, residual connections have been found to be beneficial for stabilizing signal propagation while retaining expressivity. The simple \(\frac{1}{\sqrt{L}}\)-scaling allows depth-scaling in ResNets and unlocks hyperparameter transfer (Hayou et al., 2021; Li et al., 2021; Bordelon et al., 2023; Yang et al., 2023b). Noci et al. (2022, 2024) provide infinite width and depth analyses for Transformers with the goal of preventing rank collapse and attaining a limit that has behaviour consistent with that of moderately large networks.

**Tensor Programs.** After kernel-based approaches to understand infinite-width limits of neural networks (Neal, 1996; Jacot et al., 2018) and applications of mean-field theory (Mei et al., 2018), the Tensor Program series (Yang, 2019; Yang and Hu, 2021; Yang and Littwin, 2023; Yang et al., 2022, 2023b) marks the first important break through in the theory of large neural networks. The framework covers many modern deep learning architectures, optimization algorithms and arbitrary \(abc\)-parameterizations, where each \(abc\)-parameterization is essentially defined by a layerwise scaling of initialization variance and learning rate as a function of network width. Yang and Hu (2021) propose the _maximal update parameterization_ (\(\mu\)P) and show that it is the unique stable parameterization that achieves feature learning in all layers in the limit of infinite width. In this framework, training neural networks with a global learning rate \(\eta>0\) for all layers and with He or LeCun initialization falls under the category of so called _standard parameterization_ (SP). The neural tangent parameterization (NTP), studied in the neural tangent kernel literature, differs but does not achieve feature learning in any layer, and is therefore less useful to describe the behaviour of finite width networks than \(\mu\)P (Wenger et al., 2023; Vyas et al., 2024). Yang and Littwin (2023) characterize stable learning with adaptive optimizers at infinite width into a feature learning versus a (nonlinear) operator regime. SAM

\begin{table}
\begin{tabular}{l l} \hline \hline Symbol & Meaning \\ \hline \hline \(n,\eta,\rho\) & width, learning rate, perturbation radius \\ \(\phi\), \(\mathcal{L}\), \((\xi_{t},y_{t})\) & activation function, loss function, input and label at time \(t\) \\ \(\|v\|:=\|v\|_{2}\), \(\|W\|:=\|W\|_{F}\) & 2-norm as standard for vectors, Frobenius norm as standard for matrices \\ \(\|W\|_{*}\) & spectral norm for matrices (also called operator norm) \\ \(W_{t}^{l}\) & trainable weights at time \(t\) in layer \(l\) \\ \(\delta W_{t}^{l}\) & weight updates at time \(t\) in layer \(l\) \\ \(\varepsilon_{t}^{l}\) & weight perturbations at time \(t\) in layer \(l\) \\ \(\|v_{t}\|_{1}\) & norm of the rescaled gradient in the perturbation denominator \\ \(h_{t}^{l},x_{t}^{l}\) & preactivations and activations at time \(t\) in layer \(l\) \\ \(\delta x_{t}^{l}\) & activation updates at time \(t\) in layer \(l\) \\ \(\delta\hat{x}_{t}^{l}\) & activation perturbations at time \(t\) in layer \(l\) \\ \(\delta f_{t},\hat{\delta f}_{t}\) & update/perturbation of the output function at time \(t\) \\ \hline \(\chi_{t}=\mathcal{L}(f_{t}(\xi_{t}),y_{t})\) & derivative of loss w.r.t. output function at time \(t\) \\ \(\delta W_{t}^{l}=\bm{e}_{t}^{l}\) & weight perturbations at time \(t\) in layer \(l\) (with \(\tilde{\delta}\) for consistency) \\ \(\odot\) & elementwise multiplication \\ \(dz_{t}=\theta_{\nabla}^{-1}\nabla_{z}f\) & derivative of output function w.r.t. \(z\in\{h^{l},x^{l}\}\) at time \(t\), normalized to \(\Theta(1)\) \\ \(dz_{SAM,t}\) & derivative of perturbed output function w.r.t. perturbed \(z\in\{\tilde{h}^{l},\tilde{x}^{l}\}\) at time \(t\), normalized to \(\Theta(1)\) \\ \(\theta_{\nabla}\) & scaling of the activation gradients \\ \(\theta_{l},\tilde{\theta}_{l}\) & update and perturbation scaling of \(h_{t}^{l}\) and \(x_{t}^{l}\) \\ \(\theta_{W^{l}},\tilde{\theta}_{W^{l}}\) & update and perturbation scaling of \(W_{t}^{l}\) \\ \(\hat{\theta}\) & limit scaling; under stability, all considered scalings \(\hat{\theta}\in\{0,1\}\) \\ \(Z^{z}\) & random variable distributed according to the limiting distribution \\  & for the entries of the TP vector \(z\) specified by the TP Master Theorem \\ \hline \hline \end{tabular}
\end{table}
Table A.1: (**Notation**) Overview over notation used in the main paper (top) and in the appendix (bottom).

is not covered by the update rule definition in Yang and Littwin (2023) since the nested application of the gradient w.r.t. the weights is not a coordinatewise optimizer anymore. Yang et al. (2023) show that \(\mu\)P is equivalent to the spectral scaling conditions on the weights \(\|\Delta W^{l}\|=\Theta(\sqrt{n_{l}/n_{l-1}})\) and \(\|\Delta W^{l}\|=\Theta(\sqrt{n_{l}/n_{l-1}})\). Hence Bernstein et al. (2020) would have achieved their goal of an optimizer with automatic update scaling, if they had normalized by the spectral instead of the Frobenius norm and multiplied by \(\sqrt{\text{fan\_out}/\text{fan\_in}}\) in each layer. While recent works have considered joint limits of infinite width and depth (Yang et al., 2023; Hayou and Yang, 2023), the data distribution has not been taken into account in Tensor Program literature. The study of scaling laws of jointly scaling model size, data set size and training time has predominantly been empirical (Kaplan et al., 2020; Zhai et al., 2022; Hoffmann et al., 2022; Besiroglu et al., 2024). Developing theory to inform Pareto optimal trade offs in a principled manner constitutes an important direction for future work.

As an example of scaling theory for second order optimization, Ishikawa and Karakida (2023) derive \(\mu\)P for KFAC and Shampoo. This scaling rule differs from \(\mu\)P for SGD. Similarly, Vankadara et al. (2024) show that maximal updates are achieved by another different scaling rule for non-standard architectures like structured state space models.

**Sharpness Aware Minimization.** Sharpness aware minimization (SAM) (Foret et al., 2021) has shown to be extremely effective and robust in improving generalization performance across a wide range of architectures and settings (Chen et al., 2021; Kaddour et al., 2022). SAM was motivated as an inductive bias towards flatter minima and it has been understood to have an gradient-norm adaptive edge of stability at which it drifts towards minima with smaller spectral norm of the Hessian (Long and Bartlett, 2023; Bartlett et al., 2023). However a full understanding of why SAM works so well remains elusive. While correlations between flatness and generalization have been observed in some settings (Hochreiter and Schmidhuber, 1997; Jiang* et al., 2020), other studies have questioned the usefulness of sharpness as a measure for generalization, especially for modern architectures (Dinh et al., 2017; Andriushchenko et al., 2023; Wen et al., 2024). Applying SAM on only the normalization layers often even improves generalization in vision tasks despite increasing sharpness (Muller et al., 2024). Adaptive SAM (ASAM) (Kwon et al., 2021) is a variant of SAM derived from a sharpness definition that is invariant to weight rescalings with respect to a chosen normalization operator that leave the output function invariant. The results in Muller et al. (2024) suggest that two of the most promising normalization operators are elementwise normalization \(T^{l}_{w}(x)=|W^{l}|\odot x\) and layerwise normalization \(T^{l}_{w}(x)=\|W^{l}\|_{F}\cdot x\). We state the resulting update rules and a scaling analysis in Appendix F.4. A variant of SAM that is often studied theoretically because of its simplicity does not normalize the gradient of the perturbation. Our theory covers this variant too (Appendix F.3), but Dai et al. (2024) argue that normalizing the gradients for the perturbation is crucial. Monzio Compagnoni et al. (2023) find that unnormalized SAM gets stuck around saddles while SAM slowly escapes through additional Hessian-induced noise. This suggests that the additional effort of analysing the original SAM update rule with gradient normalization is necessary for practically useful theory. Dauphin et al. (2024) draw connections between SAM and other second order optimizers like gradient penalties and weight noise. They show that SAM is able to effectively use second order information implicitly using ReLU, whereas the other two methods close the gap to SAM when using ReLU since they require the localized second order information that ReLU provides in contrast to ReLU. Wen et al. (2023) show that worst-case, ascent and average case sharpness are biased towards minimizing the maximal eigenvalue, minimal non-zero eigenvalue and trace of the Hessian, respectively. With an architecture-agnostic analysis, they show that 1-SAM minimizes the trace of Hessian like average-case sharpness, for small enough \(\eta\) and \(\rho\). Similarly, the theoretical results by Andriushchenko and Flammarion (2022) rely on the assumption that learning rate \(\eta\) and perturbation radius \(\rho\) are chosen sufficiently close to \(0\). Arguably, the empirically optimal choice of \(\eta\) and \(\rho\) lies outside of this gradient flow-like regime and has qualitatively different properties (see e.g. edge of stability literature (Cohen et al., 2020; Arora et al., 2022)).

**Scaling theory for SAM.** Shin et al. (2023) suggest that the generalisation improvement by SAM continues to increase with growing overparametrization. This corroborates empirical observations that performance monotonically improves with scale, and understanding the infinite-width limit is not only of theoretical interest but entails immediate practical benefits.

Liu et al. (2022) introduce Look-LayerSAM with layerwise perturbation scaling for preserving good performance under large batch training for enhanced training parallelization. They use LAMB (You et al., 2020) for layerwise learning rate scaling for large batch training. The update scaling strategy in these kinds of algorithms follows

\[W_{t+1}^{l}=W_{t}^{l}-\eta_{t}\phi(\|W_{t}^{l}\|_{F})\frac{\nabla_{W^{l}}\mathcal{ L}}{\|\nabla_{W^{l}}L\|_{F}},\]

with some \(\phi:\mathbb{R}_{+}\rightarrow\mathbb{R}_{+}\) and where \(\nabla_{W^{l}}\mathcal{L}\) may be replaced by Adam's \(\frac{m_{t}}{\sqrt{v_{t}}+\epsilon}\). In practice, often simple functions like \(\phi(x)=\max(c,\min(x,C))\) or \(\phi(x)=x\) are used. The idea is to ensure that the update has the same order of magnitude as the weights. Look-LayerSAM follows an analogous approach for layerwise perturbation scaling. A derivation of \(\mu\)P for LAMB could also yield feature learning in all layers in the infinite-width limit as well as hyperparameter transfer. It certainly requires layerwise learning rate scaling. In the case \(\phi(x)=x\), following a heuristic scaling derivation as in Appendix F.4 leads to layerwise learning rate scalings \(\eta_{1}=\eta_{L+1}=\Theta(1)\) and \(\eta_{l}=\Theta(n^{-1/2})\) for hidden layers \(l\in[2,L]\). With a bounded function like \(\phi(x)=\max(c,\min(x,C))\), the scalings become \(\eta_{1}=\Theta(n^{1/2})\), \(\eta_{L+1}=\Theta(n^{-1/2})\) and \(\eta_{l}=\Theta(1)\) for hidden layers \(l\in[2,L]\). We leave a closer investigation of feature learning and hyperparameter transfer with LAMB and Look-LayerSAM in SP and \(\mu\)P to future work.

## Appendix C Definitions

In this section, we collect all definitions that do not appear in the main text. With minor modifications, we adopt all definitions from Yang and Hu (2021). If not stated otherwise, limits are taken with respect to width \(n\rightarrow\infty\).

**Definition C.1** (**Big-O Notation)**.: Given a sequence of scalar random variables \(c=\{c_{n}\in\mathbb{R}\}_{n=1}^{\infty}\), we write \(c=\Theta\left(n^{-a}\right)\) if there exist constants \(A,B\) such that for almost every instantiation of \(c=\{c_{n}\in\mathbb{R}\}_{n=1}^{\infty}\), for \(n\) large enough, \(An^{-a}\leq|c_{n}|\leq Bn^{-a}\). Given a sequence of random vectors \(x=\{x_{n}\in\mathbb{R}^{n}\}_{n=1}^{\infty}\), we say \(x\) has coordinates of size \(\Theta\left(n^{-a}\right)\) and write \(x=\Theta\left(n^{-a}\right)\) to mean the scalar random variable sequence \(\left\{\sqrt{\left\|x_{n}\right\|^{2}/n}\right\}_{n}\) is \(\Theta\left(n^{-a}\right)\). For the definition of \(c=O(n^{-a})\) and \(c=\Omega(n^{-a})\), adapt the above definition of \(c=\Theta(n^{-a})\) by replacing \(An^{-a}\leq|c_{n}|\leq Bn^{-a}\) with \(|c_{n}|\leq Bn^{-a}\) and \(An^{-a}\leq|c_{n}|\), respectively. We write \(x_{n}=o(n^{-a})\) if \(n^{a}\cdot\sqrt{\left\|x_{n}\right\|^{2}/n}\to 0\) almost surely. 

**Definition C.2** (**Training routine**).: A _training routine_ is a combination of base learning rate \(\eta\geq 0\), perturbation radius \(\rho\geq 0\), training sequence \(\{(\xi_{t},y_{t})\}_{t\in\mathbb{N}}\) and a continuously differentiable loss function \(\mathcal{L}(f(\xi),y)\) using the SAM update rule with layerwise perturbation scaling (LP). 

In addition to the stability conditions from the corresponding SGD result, we demand that the activation perturbations do not blow up. Otherwise the perturbations would strictly dominate both the initialization and the updates which makes the perturbation too strong and is avoided in practice.

**Definition C.3** (**Stability)**.: We say a \(bcd\)-parametrization of an \(L\)-hidden layer MLP is _stable_ if

1. For every nonzero input \(\xi\in\mathbb{R}^{d_{n}}\!\{0\}\), \[h_{0}^{l},x_{0}^{l}=\Theta_{\xi}(1),\ \forall l\in[L],\quad\text{and}\quad \mathbb{E}f_{0}(\xi)^{2}=O_{\xi}(1),\] where the expectation is taken over the random initialization.
2. For any training routine, any time \(t\in\mathbb{N}\), \(l\in[L]\), \(\xi\in\mathbb{R}^{d_{n}}\), we have \[h_{t}^{l}(\xi)-h_{0}^{l}(\xi),x_{t}^{l}(\xi)-x_{0}^{l}(\xi)=O_{*}(1),\quad \text{and}\quad f_{t}(\xi)=O_{*}(1),\] where the hidden constant in \(O_{*}\) can depend on the training routine, \(t\), \(\xi\), \(l\) and the initial function \(f_{0}\).
3. For any training routine, any time \(t\in\mathbb{N}_{0}\), \(l\in[L]\), \(\xi\in\mathbb{R}^{d_{n}}\), for the perturbed (pre-)activation \(\tilde{h}_{t}^{l}:=h^{l}(\tilde{W}_{t}),\tilde{x}_{t}^{l}:=x^{l}(\tilde{W}_{t})\) and output function \(\tilde{f}_{t}(\tilde{W}_{t})\) we have \[\tilde{h}_{t}^{l}(\xi)-h_{t}^{l}(\xi),\tilde{x}_{t}^{l}(\xi)-x_{t}^{l}(\xi)=O_{ *}(1),\quad\text{and}\quad\tilde{f}_{t}(\xi)=O_{*}(1),\] where the hidden constant in \(O_{*}\) can depend on the training routine, \(t\), \(\xi\), \(l\) and the initial function \(f_{0}\).

**Definition C.4** (**Nontriviality)**.: We say a \(bcd\)-parametrization is _trivial_ if for every training routine, \(f_{t}(\xi)-f_{0}(\xi)\to 0\) almost surely for \(n\to\infty\), for every time \(t>0\) and input \(\xi\in\mathbb{R}^{d_{\text{\tiny{in}}}}\). Otherwise the \(bcd\)-parametrization is _nontrivial_. 

**Definition C.5** (**Feature learning)**.: We say a \(bcd\)-parametrization _admits feature learning in the \(l\)-th layer_ if there exists a training routine, a time \(t>0\) and input \(\xi\) such that \(x_{t}^{l}(\xi)-x_{0}^{l}(\xi)=\Omega_{*}(1)\), where the constant may depend on the training routine, the time \(t\), the input \(\xi\) and the initial function \(f_{0}\) but not on the width \(n\). 

**Definition C.6** (**Vanishing perturbations)**.: Let \(l\in[L]\). We say that a stable \(bcd\)-parametrization _has vanishing perturbations in the \(l\)-th layer_ if for any training routine, \(t\in\mathbb{N}_{0}\) and \(\xi\in\mathbb{R}^{d_{\text{\tiny{in}}}}\), it holds that \(\tilde{x}_{t}^{l}-x_{t}^{l}=o(1)\), and it _has vanishing perturbations in the output_ if for any training routine, \(t\in\mathbb{N}_{0}\) and \(\xi\in\mathbb{R}^{d_{\text{\tiny{in}}}}\) it holds that \(\tilde{\delta}f_{t}(\xi):=f_{\tilde{W}_{t}}(\xi)-f_{W_{t}}(\xi)=o(1)\). 

**Definition C.7** (**Perturbation nontriviality)**.: Let \(l\in[L]\). We say that a stable \(bcd\)-parametrization is _perturbation nontrivial with respect to the \(l\)-th layer_ if and only if it does not have vanishing perturbations in the \(l\)-th layer. A stable \(bcd\)-parametrization is _perturbation nontrivial with respect to the output_ if it does not have vanishing perturbations in the output. 

**Definition C.8** (**Effective perturbations)**.: Let \(l\in[L+1]\). We say that a stable \(bcd\)-parametrization _effectively perturbs the \(l\)-th layer_ if there exists a training routine, \(t\in\mathbb{N}\) and \(\xi\in\mathbb{R}^{d_{\text{\tiny{in}}}}\) such that \(\tilde{\delta}W_{t}^{l}\tilde{x}_{t}^{l-1}(\xi)=\Theta(1)\) where \(\tilde{\delta}W_{t}^{l}\) is defined in (LP) and \(\tilde{x}_{t}^{0}=x_{t}^{0}=\xi_{t}\). 

**Definition C.9** (\(\sigma\)**-gelu)**.: Define \(\sigma\)-gelu to be the function \(x\mapsto\frac{\pi}{2}\left(1+\operatorname{erf}\left(\sigma^{-1}x\right) \right)+\sigma\frac{e^{-\sigma^{-2}x^{2}}}{2\sqrt{\pi}}\). 

In order to apply the Tensor Program Master Theorem, all Nonlin and Moment operations in the Ne\(\otimes\)OR\(\top\) program, which do not only contain parameters as inputs, are required to be pseudo-Lipschitz in all of their arguments. For training with SGD, this is fulfilled as soon as \(\phi^{\prime}\) is pseudo-Lipschitz. Both tanh as well as \(\sigma\)-gelu fulfill this assumption.

**Definition C.10** (**Pseudo-Lipschitz)**.: A function \(f:\mathbb{R}^{k}\to\mathbb{R}\) is called _pseudo-Lipschitz of degree \(d\)_ if there exists a \(C>0\) such that \(|f(x)-f(y)|\leq C\|x-y\|(1+\sum_{i=1}^{k}|x_{i}|^{d}+|y_{i}|^{d})\). We say \(f\) is _pseudo-Lipschitz_ if it is so for any degree \(d\). 

## Appendix D Extensive main results

Using the formal definitions from Appendix C, here we provide the full formal statements of all of our main theoretical results together with further details and implications. The proof of all statements is provided in Appendix E. Since SAM evaluates the gradients on perturbed weights, it is not covered by the update rule definition in Yang and Littwin (2023) and an infinite-width analysis requires explicitly deriving the corresponding Ne\(\otimes\)or\(\top\) program, scalings and infinite-width limits.

Recall that our definition of \(bcd\)-parameterizations extends _abc_-parameterizations by setting the maximal perturbation scaling to \(n^{-d}\) and allowing relative downweighting \(n^{-d_{l}}\) of the global scaling in each layer \(l\). The perturbation scaling does not affect the choice of layerwise initialization variance scalings \(b_{l}\) and the layerwise learning rate scalings \(c_{l}\). Common \(bc\)-parametrizations for SGD are summarized in Table D.1. SAM with SGD as a base optimizer requires the same scalings. Similarly, SAM with Adam as a base optimizer requires the same scalings as Adam (Yang et al., 2022, Table 3). Recall that, for convenience, we require width-independent denominator scaling \(\|v_{t}\|=\Theta(1)\) of the scaled gradient for the perturbation (LP), which imposes the constraints

\[d_{1}\geq 1/2-\min(b_{L+1},c_{L+1}),\quad d_{l}\geq 1-\min(b_{L+1},c_{L+1})\text{ for }l \in[2,L],\quad d_{L+1}\geq 1/2.\] (D.1)

All (pre-)activation and function outputs can be thought of as outputs given a fixed input \(\xi\in\mathbb{R}^{d_{\text{\tiny{in}}}}\backslash\{0\}\) with \(d_{in}\in\mathbb{N}\) fixed, e.g. \(f_{t}:=f_{W_{t}}:=f_{W_{t}}(\xi)\). For the perturbed weights we write \(\tilde{W}_{t}:=W_{t}+\tilde{\delta}W_{t}\), with \(\tilde{\delta}W_{t}\) defined in (LP) as \(\varepsilon_{t}^{l}\). Here we write weight perturbations as \(\tilde{\delta}W_{t}^{l}\) instead of \(\varepsilon_{t}^{l}\) to show the resemblance to weight updates \(\delta W_{t}^{l}\). Perturbed activations and function outputs at time \(t\) are written as \(\tilde{x}_{t}^{l}(\xi)=x_{\tilde{W}_{t}}^{l}(\xi)\) and \(\tilde{f}_{t}(\xi)=f_{\tilde{W}_{t}^{l}}(\xi)\). Recall that for all of the results in this section we make the following smoothness assumption on the activation function.

**Assumption 1** (**Smooth activation function)**.: The used activation function is either tanh or \(\sigma\)-gelu for \(\sigma>0\) sufficiently small.

We define the maximal feature update scale of a \(bcd\)-parameterization

\[r:=\min(b_{L+1},c_{L+1},d+d_{L+1})+\min_{l=1}^{L}(c_{l}-\mathbb{I}(l\neq 1)).\] (D.2)

as well as the maximal feature perturbation scale of a \(bcd\)-parameterization

\[\tilde{r}:=\min(b_{L+1},c_{L+1})+d+\min_{l=1}^{L}(d_{l}-\mathbb{I}(l\neq 1)).\] (D.3)

Stability requires the constraints (a-c) from SGD and additional perturbation stability constraints (d-e) that include the layerwise perturbation scales \(\{d_{l}\}_{l=1,\dots,L+1}\).

**Theorem D.2** (**Stability characterization**).: _A \(bcd\)-parametrization is stable if and only if all of the following are true:_

1. _(Stability at initialization,_ \(h_{0}^{l},x_{0}^{l}=\Theta(1)\) _for all_ \(l\)_,_ \(f_{0}=O(1)\)_)_ \(b_{1}=0\)_,_ \(b_{l}=1/2\) _for_ \(l\in[2,L]\) _and_ \(b_{L+1}\geq 1/2\)_._
2. _(Features do not blow up during training, i.e._ \(\Delta x_{t}^{l}=O(1)\) _for all_ \(l\)_)_ \(r\geq 0\)_._
3. _(Output function does not blow up during training, i.e._ \(\Delta W_{t}^{L+1}x_{t}^{L},W_{0}^{L+1}\Delta x_{t}^{L}=O(1)\)_)_ \(c_{L+1}\geq 1\) _and_ \(b_{L+1}+r\geq 1\)_._
4. _(Feature perturbations do not blow up, i.e._ \(\bar{\delta}x_{t}^{l}=O(1)\) _for all_ \(l\)_)_ \(\tilde{r}\geq 0\)_._
5. _(Output function perturbations do not blow up during training, i.e._ \(\tilde{\delta}W_{t}^{L+1}\tilde{x}_{t}^{L},W_{t}^{L+1}\tilde{\delta}x_{t}^{L}=O (1)\)_)_ \(d+d_{L+1}\geq 1\) _and_ \(b_{L+1}+\tilde{r}\geq 1\)_._

The nontriviality and feature learning characterizations from SGD remain unaltered. This is because in the definition of \(r\), it holds that \(d+d_{L+1}\geq 1\) (from perturbation stability), and \(\min(b_{L+1},c_{L+1})\leq 1\) already had to hold for nontriviality in SGD, so that stable perturbation scaling does not affect \(r\).

**Theorem D.3** (**Nontriviality characterization**).: _A stable \(bcd\)-parametrization is nontrivial if and only if \(c_{L+1}=1\) or \(\min(b_{L+1},c_{L+1})+r=1\)._

As for nontriviality, the conditions under which a stable, nontrivial parameterization is feature learning in the infinite-width limit are decoupled from the choice of perturbation scalings \(\{d_{l}\}_{l\in[L+1]}\cup\{d\}\). Hence the conditions are the same as for SGD. Below we provide a slightly refined result in terms of the maximal feature update scale \(r_{l_{0}}\) of a \(bcd\)-parameterization up to layer \(l_{0}\) (as provided in the Appendix of Yang and Hu (2021)).

**Theorem D.4** (**Feature learning characterization**).: _For any \(l_{0}\in[L]\), the following statements are equivalent:_

1. _A stable, nontrivial_ \(bcd\)_-parametrization admits feature learning in layer_ \(l_{0}\)_._
2. _A stable, nontrivial_ \(bcd\)_-parametrization admits feature learning in layer_ \(l\) _for all_ \(l\geq l_{0}\)_._
3. \(r_{l_{0}}:=\min(b_{L+1},c_{L+1},d+d_{L+1})+\min_{m=1}^{l_{0}}(c_{m}-\mathbb{I} (m\neq 1))=0\)_._

_Consequently, a stable, nontrivial \(bcd\)-parametrization admits feature learning (at least in the last layer activations) if and only if \(r=0\)._

**Remark D.5** (**Effective feature learning**).: As for perturbations, feature learning in later layers can be caused by weight updates in earlier layers that propagate through the network. One could demand effective feature learning in the \(l\)-th layer as \(\delta W_{t}^{l}x_{t}^{l-1}=\Theta(1)\) and it would occur if and only if \(\min(b_{L+1},c_{L+1},d+d_{L+1})+c_{l}-\mathbb{I}(l\neq 1)=0\). 

As for nontriviality, perturbation nontriviality in the output is attained if the constraints for \(\tilde{\delta}W_{t}^{L+1}\tilde{x}_{t}^{L}\) or \(W_{t}^{l}\tilde{\delta}x_{t}^{L}\) are exactly satisfied.

**Theorem D.6** (**Perturbation nontriviality characterization**).: _Let \(l\in[L]\). A stable \(bcd\)-parametrization is perturbation nontrivial with respect to the \(l\)-th layer if and only if_

\[\tilde{r}_{l}:=\min(b_{L+1},c_{L+1})+d+\min_{m=1}^{l}(d_{m}-\mathbb{I}(m\neq 1 ))=0.\]

_A stable \(bcd\)-parametrization is perturbation nontrivial with respect to the output if and only if \(d+d_{L+1}=1\) or \(\min(b_{L+1},c_{L+1})+\tilde{r}=1\)._The converse formulation of the perturbation-nontriviality results characterizes the regime of vanishing perturbations.

**Corollary D.7** (**Vanishing perturbation characterization)**.: _For any \(l_{0}\in[L]\), the following statements are equivalent:_

1. _A stable_ \(bcd\)_-parametrization has vanishing perturbations in layer_ \(l_{0}\)_._
2. _A stable_ \(bcd\)_-parametrization has vanishing perturbations in layer_ \(l\) _for all_ \(1\leq l\leq l_{0}\)_._
3. \(\tilde{r}_{l_{0}}:=\min(b_{L+1},c_{L+1})+d+\min_{m=1}^{l_{0}}(d_{m}-\mathbb{I} (m\neq 1))>0\)_._

_A stable \(bcd\)-parametrization has vanishing perturbations with respect to all layers and the output function if and only if \(d_{L+1}>1/2\) and \(\tilde{r}>\max(0,1-b_{L+1})\). This case reduces to the results in Yang and Hu (2021)._

For perturbation nontriviality it suffices that the perturbation in any of the previous layers is scaled correctly. For effective perturbations, we need the correct scaling in exactly that layer.

**Theorem D.8** (**Effective perturbation characterization)**.: _For \(l\in[L]\), a stable \(bcd\)-parametrization effectively performs SAM in the \(l\)-th layer if and only if \(\min(b_{L+1},c_{L+1})+d+d_{l}-\mathbb{I}(l\neq 1)=0\)._

_A stable \(bcd\)-parametrization effectively performs SAM in the last layer if and only if \(d+d_{L+1}=1\)._

The above understanding of all update and perturbation scalings allows us to extract the most important consequences of different choices of perturbation scaling on the learning dynamics. Beyond vanishing hidden layer perturbations, the following theorem shows that the joint gradient norm \(\|v_{t}\|\) can be approximated efficiently without an additional backward pass under global perturbation scaling.

**Theorem D.9** (**Global Perturbation Scaling)**.: _Given any stable \(bcd\)-parametrization \(\{b_{l}\}_{l\in[L+1]}\cup\{c_{l}\}_{l\in[L+1]}\cup\{d_{l}\}_{l\in[L+1]}\cup\{d\}\). The parametrization performs updates in the original gradient direction if and only if \(d_{l}=C\) for all \(l\in[L+1]\) for some \(C\in\mathbb{R}\). In this case, the parametrization has vanishing perturbations in all hidden layers \(l\in[L]\), and the last layer \(l=L+1\) is effectively perturbed if and only if \(d=1/2\). If \(b_{L+1}>1/2\) (as in \(\mu\)P), the gradient norm is dominated by the last layer and simplifies to,_

\[\|v_{t}\|=\Theta(n^{1/2-C}),\qquad\|v_{t}\|-\mathcal{L}^{\prime}(f_{t}(\xi_{t }),y_{t})\|x_{t}^{L}\|=o(n^{1/2-C}).\]

One might suspect that it is desirable to let all layers contribute non-vanishingly to the gradient norm in the denominator of (LP). The following proposition shows that this should be avoided with our definition of \(bcd\)-parametzations. Of course, if we add even more hyperparameters by decoupling numerator and denominator scalings, we can set all contributions to \(\Theta(1)\), which is what we do in Appendix F.7.

**Proposition D.10** (**Balancing gradient norm contributions)**.: _Given any stable \(bcd\)-parametrization \(\{b_{l}\}_{l\in[L+1]}\cup\{c_{l}\}_{l\in[L+1]}\cup\{d_{l}\}_{l\in[L+1]}\cup\{d\}\). If all layers contribute to the gradient norm non-vanishingly in the limit, i.e. \(\|v_{t}^{l}\|=\Theta(\|v_{t}\|)\) for all \(l\in[L+1],t\in\mathbb{N}_{0}\), then the parametrization has vanishing perturbations in all hidden layers \(l\in[L]\). Such a parametrization effectively performs SAM in the last layer \(l=L+1\) if and only if \(d=1/2\)._

The following theorem provides the unique correct perturbation scaling for any stable \(bc\)-parameterization with \(b_{L+1}\geq 1\).

**Theorem D.11** (**Perturbation Scaling Choice for Effective Perturbations)**.: _Given any stable \(bcd\)-parametrization \(\{b_{l}\}_{l\in[L+1]}\cup\{c_{l}\}_{l\in[L+1]}\cup\{d_{l}\}_{l\in[L+1]}\cup\{d\}\). If \(b_{L+1}<1\), then there does not exist a stable choice of \(\{d_{l}\}_{l\in[L+1]}\cup\{d\}\) that achieves effective perturbations before the last layer. If \(b_{L+1}\geq 1\), then up to the equivalence \(d_{l}^{\prime}=d_{l}+C\), \(C\in\mathbb{R}\), \(\forall l\in[L+1]\), the unique stable choice \(\{d_{l}\}_{l\in[L+1]}\cup\{d\}\) with effective perturbations in all layers \(l\in[L+1]\) is given by_

\[d=-1/2,\qquad d_{l}=\begin{cases}1/2-\min(b_{L+1},c_{L+1})&l=1,\\ 3/2-\min(b_{L+1},c_{L+1})&l\in[2,L],\\ 3/2&l=L+1.\end{cases}\] (D.4)

_In this parameterization, the first layer dominates the gradient norm as_

\[\|v_{t}\|=\Theta(1),\qquad\big{|}\|v_{t}^{1}\|-\|v_{t}\|\big{|}=\Theta(n^{-1/ 2}).\]Table D.2 summarizes the consequences of Theorem D.11. Together with Theorem D.11, the following proposition suggests that \(b_{L+1}=1\) is a good choice. However \(b_{L+1}>1\) can also induce effective perturbations, as long as \(d\) and \(d_{L+1}\) are chosen correctly.

**Proposition D.12** (**Effects of last-layer initialization \(b_{L+1}\) on all perturbations)**.: _If a stable \(bcd\)-parametrization with \(\min(b_{L+1},c_{L+1})\leq 1\) is perturbation nontrivial with respect to any hidden layer \(l\in[L]\), it is also perturbation nontrivial with respect to the output._

Lastly, the following proposition shows that effective perturbations from the first layer propagate through the entire network.

**Proposition D.13** (**Perturbations propagate through the forward pass)**.: _All stable \(bcd\)-parametrizations with \(d_{1}=-\min(b_{L+1},c_{L+1})-d\) effectively perturb the first layer and are perturbation nontrivial in all layers._

**Remark D.14** (**Efficiency gains)**.: The above results may be used for efficiency gains. Given any stable \(bcd\)-parametrization, we can compute the maximal layer \(l_{0}\) such that \(\tilde{r}_{l_{0}}>0\), and in wide networks do not have to compute SAM perturbations before layer \(l_{0}+1\); as soon as \(b_{L+1}>1/2\) (as for \(\mu\)P), the gradient norm for the SAM update rule is approximately given by \(\|\nabla L_{t}\|\approx\mathcal{L}^{\prime}(f_{t}(\xi_{t}),y_{t})\|x_{t}^{L}\|\), which can directly be computed without an additional backward pass. The practical recommendation from our experiments however is to either use \(\mu\)P\({}^{2}\) or to completely abstain from perturbations. 

**Remark D.15** (**SAM without gradient normalization**).: For the SAM update rule without gradient normalization simply set \(d=0\) and remove the gradient norm constraints (D.1) to arrive at the

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Definition & Naive & Global (stable) & Effective \\ \hline \hline \(d\) & \(\rho n^{-d}\) & 0 & \(1/2\) & \(-1/2\) \\ \(d_{l}\) & \(n^{-d_{l}}\nabla_{W^{l}}\mathcal{L}_{t}\) & \(1/2\) & \(1/2\) & \(\begin{cases}1/2-c_{\nabla}&l=1,\\ 3/2-c_{\nabla}&l\in[2,L],\\ 3/2&l=L+1.\end{cases}\) \\ \(\tilde{r}\) & Equation (D.3) & \(c_{\nabla}-1/2\) & \(c_{\nabla}\) & \(0\) \\ \hline Stable? & ✗ & ✓ & ✓ \\ Last layer effectively perturbed? & ✗ & ✓ & ✓ \\ All layers effectively perturbed? & ✗ & ✗ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table D.2: (**Perturbation scalings**) Overview over important choices of the global perturbation scaling \(\rho n^{-d}\) and the layerwise perturbation scalings \(n^{-d_{l}}\) for training MLPs without biases with SAM: Naive scaling without width dependence (Naive), maximal stable global scaling along the original gradient direction (Global) and the unique scaling that achieves effective perturbations in all layers (Effective). An extensive overview that characterizes all possible choices of perturbation scaling is provided in Appendix F.1. Recall the gradient scaling \(c_{\nabla}:=\min(b_{L+1},c_{L+1})\).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Definition & SP & SP (stable) & NTP (stable) & \(\mu\)P \\ \hline \hline \(b_{l}\) & \(\mathcal{N}(0,n^{-2b_{l}})\) & \(\begin{cases}0&l=1,\\ 1/2&l\geq 2.\end{cases}\) & \(\begin{cases}0&l=1,\\ 1/2&l\geq 2.\end{cases}\) & \(\begin{cases}0&l=1,\\ 1/2&l\geq 2.\end{cases}\) & \(\begin{cases}0&l=1,\\ 1/2&l\in[2,L],\\ 1,&l=L+1.\end{cases}\) \\ \(c_{l}\) & LR \(\eta n^{-c_{l}}\) & 0 & \(1\) & \(\begin{cases}0&l=1,\\ 1&l\geq 2.\end{cases}\) & \(\begin{cases}-1&l=1,\\ 0&l\in[2,L],\\ 1&l=L+1.\end{cases}\) \\ \(r\) & Equation (D.2) & -1 & 1/2 & 1/2 & 0 \\ \hline Stable? & & ✓ & ✓ & ✓ \\ Nontrivial? & & ✓ & ✓ & ✓ \\ Feature learning? & & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table D.1: (**\(bc\)-parametrizations**) Overview over common implicitly used \(bc\)-parametrizations for training MLPs without biases in standard parametrization (SP), standard parametrization with maximal stable nonadaptive LR \(c=1\) (SP (stable)), neural tangent parametrization (NTP) and maximal update parametrization (\(\mu\)P).

adapted Ne\(\otimes\)or\(\top\) program and \(bcd\)-constraints. Note that standard parametrization gets even more unstable without dividing by \(\|\nabla L\|=\Theta(n^{1/2})\), now requiring \(d_{L+1}\geq 1\) for stability. Similar to the previous results, this shows that unawareness of \(bcd\)-parametrizations requires strongly scaling down \(\rho\) for stability, while vasting computation on vanishing perturbations before the last layer. More details can be found in Appendix F.3. 

## Appendix E Proof of main results

In this section we derive the Ne\(\otimes\)or\(\top\) program that corresponds to training a MLP without biases with SAM. For simplicity and clarity of the proof, we prove the one-dimensional case \(d_{in}=1\), \(d_{out}=1\), but an extension to arbitrary but fixed \(d_{in}\), \(d_{out}\) is straightforward. Recall Assumption 1 that allows us to apply the Tensor Program Master Theorem and explicitly state the infinite-width limit of training MLPs with SAM in Appendix E.2.

### Tensor program formulation

#### e.1.1 Tensor Program initialization

We initialize the matrices \(W_{0}^{2},\ldots,W_{0}^{L}\) as \((W_{0}^{l})_{\alpha\beta}\sim\mathcal{N}(0,1/n)\), which absorbs \(b_{l}=1/2\).

We initialize the input layer matrix \(W_{0}^{1}\in\mathbb{R}^{n\times 1}\) and normalized output layer matrix \(\hat{W}_{0}^{L+1}=W_{0}^{L+1}n^{b_{L+1}}\in\mathbb{R}^{1\times n}\) as \((W_{0}^{1})_{\alpha},(\hat{W}_{0}^{L+1})_{\alpha}\sim\mathcal{N}(0,1)\), as initial vectors should have a distribution that is \(\Theta(1)\).

In the Ne\(\otimes\)or\(\top\)formulation, we write all quantities as \(\theta_{z}z\), where \(\theta_{z}\) denotes their scaling \(n^{C}\) for some \(C\in\mathbb{R}\) and \(z\) therefore has a \(\Theta(1)\) distribution. The stability, nontriviality and feature learning conditions then stem from requiring either \(\theta_{z}\to 0\) or \(\theta_{z}=1\) depending on \(z\) and its desired scale.

#### e.1.2 First forward pass

We denote a definition of a Tensor Program (TP) or Ne\(\otimes\)or\(\top\)computation as \(:=\). Compared to MLPs trained with SGD nothing changes in the first forward pass,

\[h_{0}^{1}(\xi):=W_{0}^{1}\xi\quad\text{(NL)},\quad x_{0}^{l}:=\phi(h_{0}^{l}) \quad\text{(NL)},\quad h_{0}^{l+1}:=W_{0}^{l+1}x_{0}^{l}.\quad\text{(MatMul)}\]

In the case of MuP, \(f_{0}(\xi)=W_{0}^{L+1}x_{0}^{L}(\xi)\to 0\) defines a scalar in the TP.

Observe the scalings \(x_{0}^{1}=\Theta(h_{0}^{1})=\Theta(n^{-b_{1}}),x_{0}^{l}=\Theta(h_{0}^{l})= \Theta(n^{1/2-b_{l}})\) for \(l\in[2,L]\) due to CLT, independence at initialization and \(x_{0}^{l}=\Theta(h_{0}^{l})=\Theta(1)\) by stability. Hence stability at initialization inductively requires \(b_{1}=0\), \(b_{l}=1/2\) for \(l\in[2,L]\) and \(b_{L+1}\geq 1/2\).

#### e.1.3 First backward pass

The chain rule of the derivative remains the same, we just evaluate on different weights compared to standard SGD. We denote the adversarially perturbed weights by \(\hat{W}_{t}^{l}\) and the normalized perturbations by \(\tilde{\delta}W_{t}^{l}\). Before computing the updates we have to compute a full backward pass to determine these perturbed weights for each layer, and then compute a forward pass with these perturbed weights to compute the perturbed preactivations \(\tilde{h}_{t}^{l}\) that we will need for computing the SAM update. Therefore the Ne\(\otimes\)or\(\top\) program for SAM maintains a perturbed copy of all preactivations, activations, last-layer weights and logits just for computing the updates of the actual parameters.

Under MuP, the loss derivative with respect to the function remains \(\chi_{0}:=\mathcal{L}^{\prime}(f_{0}(\xi_{0}),y_{0})\to\overset{\circ}{\chi}_{ 0}:=\mathcal{L}^{\prime}(0,y_{0})\). For the weight perturbation, we need to perform a SGD backward pass,

\[dx_{0}^{L}:=\hat{W}_{0}^{L+1},\quad dh_{0}^{l}:=dx_{0}^{l}\odot\phi^{\prime}(h _{0}^{l}),\quad dx_{0}^{l-1}:=(W_{0}^{l})^{l}dh_{0}^{l},\]

where \(dz:=\theta_{\nabla}^{-1}\nabla_{z}f\). For SGD (and for SAM, as we will see later) all gradients have scaling \(\theta_{\nabla}:=n^{-b_{L+1}}\) in the first step, whereas we overload the notation \(\theta_{\nabla}:=n^{-\min(b_{L+1},c_{L+1})}\) for all later steps. For clarity of presentation assume \(b_{L+1}\geq c_{L+1}\) here, the other case follows analogously. For the first step this can be understood from

\[\nabla_{x^{L}}f_{0}=W_{0}^{L+1}=\Theta(n^{-b_{L+1}}),\qquad\nabla_{h^{L}}f_{0 }=\nabla_{x^{L}}f_{0}\odot\phi^{\prime}(h_{0}^{L})=\Theta(n^{-b_{L+1}}),\]since \(h_{0}^{L}=\Theta(1)\) by the stability assumption, and this scale \(\Theta(n^{-b_{L+1}})\) propagates through all layers via the chain rule and remains stable in later backward passes. For hidden layer gradients, observe that

\[\nabla_{x^{L-1}}f_{t} = (W_{t}^{L})^{T}\nabla_{h\cdot L}f_{t}=(W_{0}^{L}+\Delta W_{t}^{L}) ^{T}\nabla_{h\cdot L}f_{t}\] \[= \Theta\left((W_{0}^{L})^{T}\nabla_{h^{L}}f_{t}-n^{-c_{L}}\sum_{s= 0}^{t-1}((\nabla_{h^{L}}f_{s})^{T}\nabla_{h^{L}}f_{t})x_{s}^{L-1}\right)\] \[= \Theta(n^{1-2b_{L}}\theta_{\nabla}-n^{-c_{L}}\theta_{\nabla}^{2}n )=\Theta(\theta_{\nabla}),\]

where first term's scale stems from the products \((W_{0}^{L})^{T}W_{0}^{L}v=\Theta(n^{1-2b_{L}}v)\) due to Yang (2021), \(b_{L}=1/2\) for stability at initialization and \(b_{L+1}+c_{L}\geq 1\) for update stability during training (\(r\geq 0\)). If we allowed the second term to strictly dominate, the gradient scale would explode iteratively in the backward pass.

**The gradient norm.** Before computing the weight perturbations, we need to compute the gradient norm for the SAM update. The gradient norm at time \(t\) in each layer \(l\in[2,L]\) is given by the scalar,

\[\theta_{\nabla}^{-2}\left\|\frac{\partial L_{t}}{\partial W^{l}}\right\|^{2}= \sum_{i,j=1}^{n}\left(\chi_{t}(dh_{t}^{l})_{i}(x_{t}^{l-1})_{j}\right)^{2}=\chi _{t}^{2}\|dh_{t}^{l}(x_{t}^{l-1})^{T}\|_{F}^{2}=\chi_{t}^{2}\big{(}(dh_{t}^{l} )^{T}dh_{t}^{l}\big{)}\big{(}(x_{t}^{l-1})^{T}x_{t}^{l-1}\big{)},\]

where \(\chi_{t}=\mathcal{L}^{\prime}(f_{t}(\xi_{t}),y_{t})\) and we used \(\partial h^{l}/\partial W_{ij}^{l}=(x_{j}^{l-1}\delta_{ik})_{k=1,\ldots,n}\).

Hence the gradient norm of all weights jointly is given by the unnormalized scalar

\[\|\nabla_{w}L_{t}\|^{2}=\chi_{t}^{2}\left(n\theta_{\nabla}^{2} \frac{(dh_{t}^{1})^{T}dh_{t}^{1}}{n}(\xi_{t}^{T}\xi_{t})+\sum_{l=2}^{L}n^{2} \theta_{\nabla}^{2}\frac{(dh_{t}^{l})^{T}dh_{t}^{l}}{n}\frac{(x_{t}^{l-1})^{T} x_{t}^{l-1}}{n}+n\frac{(x_{t}^{L})^{T}x_{t}^{L}}{n}\right),\] (E.1)

with scaling \(\theta_{\|\nabla\|}^{2}=\Theta(n^{2}\theta_{\nabla}^{2}+n)=\Theta(n)\), because stability at initialization requires \(b_{L+1}\geq 1/2\) so that \(n^{2}\theta_{\nabla}^{2}\leq n\). Note that the first layer contributes vanishingly to the gradient norm, the hidden layer gradients only if \(b_{L+1}=1/2\) (equivalently \(f_{0}=\Theta(1)\)) and the last-layer activations always in dominating order. So in \(\mu\)P, in the limit, \(\|\nabla_{w}L_{t}\|=\mathcal{L}^{\prime}(f_{t}(\xi_{t}),y_{t})\|x_{t}^{L}\|\). This means that the unscaled gradient always aligns with the last-layer activation. For learning in \(\mu\)P, this dominance is corrected by the layerwise learning rates.

The squared norm of the rescaled gradient is given by

\[\|v_{t}\|^{2}=\chi_{t}^{2}\bigg{(} n\theta_{\nabla}^{2}n^{-2d_{t}}\frac{(dh_{t}^{1})^{T}dh_{t}^{1}}{n}( \xi_{t}^{T}\xi_{t})\] (E.2) \[+\sum_{l=2}^{L}n^{2}\theta_{\nabla}^{2}n^{-2d_{l}}\frac{(dh_{t}^{ l})^{T}dh_{t}^{l}}{n}\frac{(x_{t}^{l-1})^{T}x_{t}^{l-1}}{n}+nn^{-2d_{L+1}} \frac{(x_{t}^{L})^{T}x_{t}^{L}}{n}\bigg{)},\]

with scaling \(\theta_{v}^{2}=\Theta(n^{1-2d_{1}}\theta_{\nabla}^{2}+\sum_{l=2}^{L}n^{2-2d_{l }}\theta_{\nabla}^{2}+n^{1-2d_{L+1}})\). For simplicity, set \(\theta_{v}=1\). This raises the constraints \(n^{1-2d_{1}}\theta_{\nabla}^{2}\leq 1\), \(n^{2-2d_{l}}\theta_{\nabla}^{2}\leq 1\) for \(l\in[2,L]\) and \(n^{1-2d_{L+1}}\leq 1\), which can be rewritten as

\[d_{1}\geq 1/2-\min(b_{L+1},c_{L+1}),\quad d_{l}\geq 1-\min(b_{L+1},c_{L+1}) \text{ for }l\in[2,L],\quad d_{L+1}\geq 1/2,\]

where at least one equality is demanded to hold in order to attain \(\theta_{v}=1\). If one of the equalities holds, the respective layer contributes to the norm non-vanishingly in the limit.

Thus, applying the square root and dividing by \(\theta_{v}=1\) the square root of (E.2) defines a normalized TP scalar.

**Perturbations.** Stability implies that also the perturbed (pre-)activations and output function remain \(\Theta(1)\) and \(O(1)\) respectively. Otherwise a SAM training step would induce blowup in the updates. We call this weaker property of just the perturbations _perturbation stability_.

**Definition E.1** (Perturbation stability).: We call a \(bcd\)-parametrization _perturbation stable_ if and only if \(\hat{h}_{t}^{l},\hat{x}_{t}^{l}=\Theta(1)\) for all \(l\in[L]\) and \(t\in\mathbb{N}\) and \(\hat{\delta}f_{t}=O(1)\) for all \(t\in\mathbb{N}\).

Mathematically we get the normalized weight perturbations for \(l\in\{2,\ldots,L\}\),

\[\tilde{\delta}W_{0}^{L+1}:=\frac{\rho\ \chi_{0}\ x_{0}^{L}}{\|v_{0}\|},\quad \tilde{\delta}W_{0}^{l}=\frac{\rho\ \chi_{0}\ dh_{0}^{l}\ (x_{0}^{l-1})^{T}}{\|v_{0}\|},\quad \tilde{\delta}W_{0}^{1}=\frac{\rho\ \chi_{0}\ dh_{0}^{1}\ \xi_{0}^{T}}{\|v_{0}\|},\]

which scale as \(\tilde{\theta}_{L+1}:=\tilde{\theta}_{W^{L+1}}:=n^{-(d+d_{L+1})}\), \(\Theta(n^{(d+d_{l})-b_{L+1}})\) and \(\Theta(n^{-(d+d_{1})-b_{L+1}})\) respectively. But the Ne\(\otimes\)or\(\top\) program computation rules do not allow to compute matrices \(\tilde{\delta}W_{0}^{l},l\in[L]\), therefore we use the weight updates to directly compute the preactivation and activation changes analogous to the \(t\)-th forward pass. For all \(t\geq 0\), we write

\[\tilde{h}_{t}^{l}=h_{t}^{l}+\tilde{\theta}_{l}\tilde{\delta}h_{t}^{l},\qquad \tilde{x}_{t}^{l}=x_{t}^{l}+\tilde{\theta}_{l}\tilde{\delta}x_{t}^{l},\]

with the perturbations for \(l\in[2,L]\),

\[\tilde{\delta}h_{0}^{1}(\xi) := +\frac{\rho\chi_{0}(\xi_{0}^{T}\xi)dh_{0}^{1}}{\|v_{0}\|},\] \[\tilde{\delta}x_{t}^{l} := \tilde{\theta}_{l}^{-1}(\phi(h_{t}^{l}+\tilde{\theta}_{l}\tilde{ \delta}h_{t}^{l})-\phi(h_{t}^{l})),\] \[\tilde{\theta}_{l}\tilde{\delta}h_{0}^{l} := \tilde{\theta}_{l-1}W_{0}^{l}\tilde{\delta}x_{0}^{l-1}+(\tilde{W}_ {0}^{l}-W_{0}^{l})\tilde{x}_{0}^{l-1}\] \[= \tilde{\theta}_{l-1}W_{0}^{l}\tilde{\delta}x_{0}^{l-1}+\rho\tilde {\theta}_{W^{l}}\frac{\chi_{0}\ (x_{0}^{l-1})^{T}\tilde{x}_{0}^{l-1}}{\|v_{0}\|} \frac{h_{0}^{l}}{n}dh_{0}^{l},\]

which defines a NonLin operation with the vectors \(W_{0}^{l}\tilde{\delta}x_{0}^{l-1}\) and \(dh_{0}^{l}\) and everything else treated as scalars, and with first backward pass scalings \(\tilde{\theta}_{W^{1}}:=n^{-(d+d_{l})}\theta_{\nabla}\), \(\tilde{\theta}_{W^{l}}:=n^{1-(d+d_{l})}\theta_{\nabla}\) and \(\tilde{\theta}_{l}:=\max(\tilde{\theta}_{l-1},\tilde{\theta}_{W^{l}})=\max_{m= 1}^{l}\tilde{\theta}_{W^{m}}\), where we used that \(\tilde{x}_{0}^{l-1}=\Theta(1)\) due to perturbation stability. Note that these scalings may implicitly increase when \(t>0\) since \(\theta_{\nabla}=n^{-b_{L+1}}\) gets replaced by \(\theta_{\nabla}=n^{-\min(b_{L+1},c_{L+1})}\).

The activation perturbations can then simply be defined via the NonLin operation,

\[\tilde{\delta}x_{0}^{l}:=\tilde{\theta}_{l}^{-1}(\phi(h_{0}^{l}+\tilde{\theta }_{l}\tilde{\delta}h_{0}^{l})-\phi(h_{0}^{l})),\]

with the same scaling as \(\tilde{\delta}h_{0}^{l}\).

The perturbation of the scalar output function can simply be defined via the NonLin operation,

\[\tilde{\delta}f_{0}:=\tilde{W}_{0}^{L+1}\tilde{x}_{0}^{L}-W_{0}^{L+1}x_{0}^{L} =\tilde{\theta}_{L+1}^{\prime}\frac{\tilde{\delta}W_{0}^{L+1}\tilde{x}_{0}^{L }}{n}+\tilde{\theta}_{L\nabla}^{\prime}\frac{\tilde{W}_{0}^{L+1}\tilde{\delta} x_{0}^{L}}{n},\]

with \(\tilde{\theta}_{L+1}^{\prime}:=n\tilde{\theta}_{W^{L+1}}\) and \(\tilde{\theta}_{L\nabla}^{\prime}:=n\theta_{\nabla}\tilde{\theta}_{L}\).

**SAM Update.** Finally, we can compute the SAM updates as follows. In the case \(\min(b_{L+1},c_{L+1})\leq d+d_{L+1}\) the weight perturbation scale is dominated by the weight scale, so that

\[dx_{SAM,0}^{L}:=\hat{W}_{0}^{L+1}+\tilde{\theta}_{(L+1)/\nabla}\ \tilde{\delta}W_{0}^{L+1},\]

with \(\tilde{\theta}_{(L+1)/\nabla}:=\tilde{\theta}_{L+1}/\theta_{\nabla}\leq 1\), whereas if \(\min(b_{L+1},c_{L+1})>d+d_{L+1}\) we write

\[dx_{SAM,0}^{L}:=\tilde{\theta}_{\nabla/(L+1)}\hat{W}_{0}^{L+1}+\tilde{\delta} W_{0}^{L+1},\]

with \(\theta_{\nabla/(L+1)}:=\theta_{\nabla}/\tilde{\theta}_{L+1}\leq 1\). In any case, the scaling of \(dx_{SAM,0}^{L}\) and all other SAM gradients is \(\theta_{SAM}:=\max(\theta_{\nabla},n^{-(d+d_{L+1})})=n^{-\min(b_{L+1},c_{L+1},d +d_{L+1})}\). The other SAM gradients are given by

\[dh_{SAM,0}^{l} := dx_{SAM,0}^{l}\odot\phi^{\prime}(\tilde{h}_{0}^{l})\] \[dx_{SAM,0}^{l-1} := (\tilde{W}_{0}^{l})^{T}dh_{SAM,0}^{l}=(W_{0}^{l}+\tilde{\theta}_{ W^{l}}\tilde{\delta}W_{0}^{l})^{T}dh_{SAM,0}^{l}\] \[= (W_{0}^{l})^{T}dh_{SAM,0}^{l}+\rho\theta_{SAM}\tilde{\theta}_{ W^{l}}\frac{\chi_{0}\ (dh_{0}^{l})^{T}dh_{SAM,0}^{l-1}}{n}x_{0}^{l-1}.\]

where the last line define a NonLin operation in the vectors \((W_{0}^{l})^{T}dh_{SAM,t}^{l}\) and \(x_{0}^{l-1}\) and everything else treated as scalars. Consequently, \(\nabla_{h_{0}^{l}}f|_{\tilde{W}_{0}}\) is of the same scale as \(\nabla_{x_{0}^{l}}^{\cdot}f|_{\tilde{W}_{0}}\) and \(\nabla_{x_{0}^{l-1}}^{\cdot-1}f|_{\tilde{W}_{0}}\) is of the scale \(\max(\theta_{SAM},\tilde{\theta}_{W^{l}}\theta_{SAM})=\theta_{SAM}\) since \(\tilde{\theta}_{W^{l}}\leq 1\) is required for perturbation stability.

Note that for SAM's weight updates the loss derivative is also evaluated on the perturbed weights,

\[\tilde{\chi}_{0}:=\mathcal{L}^{\prime}(\tilde{W}_{0}^{L+1}\tilde{x}_{0}^{L},y_{ 0}).\]

**Constraints on the output function.** Assuming \(\tilde{x}_{0}^{L}=\Theta(1)\) (perturbation stability), we get \(\tilde{\chi}_{0}=O(1)\) if and only if \(\tilde{\delta}W_{0}^{L+1}=O(n^{-1})\) if and only if \(d+d_{L+1}\geq 1\).

We have \(\tilde{\chi}_{0}=\Theta(1)\) if and only if \(\tilde{f}_{0}=\tilde{W}_{0}^{L+1}\tilde{x}_{0}^{L}=\Theta(1)\). This can either be caused by changes in the last-layer weights, by non-vanishing initial function \(W_{0}^{L+1}x_{0}^{L}\) (if and only if \(b_{L+1}=1/2\)) or by \(W_{0}^{L+1}\tilde{\delta}\tilde{x}_{0}^{L}=\Theta(1)\), which holds if and only if \(b_{L+1}+\tilde{r}_{L}=1\) (analogously, \(W_{0}^{L+1}\tilde{\delta}\tilde{x}_{0}^{L}=O(1)\) if and only if \(b_{L+1}+\tilde{r}_{L}\geq 1\)). The first case requires \(\tilde{\delta}W_{0}^{L+1}=\Theta(n^{-1})\), since \(\tilde{\delta}W_{0}^{L+1}\) and \(\tilde{x}_{0}^{L}\) are highly correlated. \(\tilde{\delta}W_{0}^{L+1}=\Theta(n^{-1})\) is fulfilled if and only if \(d+d_{L+1}=1\) (the analogue to \(c_{L+1}\geq 1\) for stability and \(c_{L+1}=1\) for nontriviality).

Hence perturbation stability of the output function holds only if \(d+d_{L+1}\geq 1\) and \(b_{L+1}+\tilde{r}_{L}\geq 1\). Then, perturbation nontriviality holds if and only if \(d+d_{L+1}=1\) or \(b_{L+1}+\tilde{r}_{L}=1\).

In the \(t\)-th backward pass, \(b_{L+1}+\tilde{r}_{L}\geq 1\) will be replaced by the slightly stronger constraint \(b_{L+1}+\tilde{r}\geq 1\).

#### e.1.4 \(t\)-th forward pass

Formally, we sum the updates in each step,

\[\hat{W}_{t}^{L+1}:=\hat{W}_{0}^{L+1}+\theta_{L+1/\nabla}(\delta W_{1}^{L+1}+ \cdots+\delta W_{t}^{L+1}),\]

where \(\delta W_{t+1}^{L+1}:=-\eta\ \tilde{\chi}_{t}\ (\tilde{x}_{t}^{L})^{T}\) denotes the normalized change in the weights \(W^{L+1}\) (as a row vector) of scaling \(\theta_{L+1}=\theta_{W^{L+1}}=n^{-c_{L+1}}\) under perturbation stability and nontriviality so that \(\hat{W}_{t}^{L+1}\) scales as \(\theta_{\nabla}=n^{-\min(b_{L+1},c_{L+1})}\). \(\delta W_{t+1}^{L+1}\) should not be confused with \(\tilde{\delta}W_{t+1}^{L+1}\) which denotes the perturbation of the weights at time \(t+1\). For every nontrivial stable parametrization we have \(\tilde{\chi}_{t}=\Theta(1)\) and \(\tilde{x}_{t}^{L}=\Theta(1)\) which requires \(\tilde{\theta}_{L}\leq 1\). In the case \(c_{L+1}<b_{L+1}\), we write \(\hat{W}_{t}^{L+1}:=n^{-b_{L+1}+c_{L+1}}\hat{W}_{0}^{L+1}+(\delta W_{1}^{L+1}+ \cdots+\delta W_{t}^{L+1})\) with the same scaling \(\theta_{\nabla}=n^{-\min(b_{L+1},c_{L+1})}\).

For preactivations and activations we also sum the changes from each step,

\[h_{t}^{l}:=h_{0}^{l}+\theta_{l}(\delta h_{1}^{l}+\cdots+\delta h_{t}^{l}), \qquad x_{t}^{l}:=x_{0}^{l}+\theta_{l}(\delta x_{1}^{l}+\cdots+\delta x_{t}^{l}).\]

Using the fact that

\[W_{t}^{1}-W_{t-1}^{1}=-\eta\tilde{\chi}_{t-1}\theta_{W^{1}}dh_{SAM,t-1}^{1} \xi_{t-1}^{T},\]

yields the normalized preactivation updates

\[\delta h_{t}^{1}(\xi):=-\eta\tilde{\chi}_{t-1}dh_{SAM,t-1}^{1}\xi_{t-1}^{T} \xi\quad\text{(NL)},\]

with scaling \(\theta_{1}=\theta_{W^{1}}=n^{-c_{1}}\theta_{SAM}=n^{-c_{1}-\min(b_{L+1},c_{L+ 1},d+d_{L+1})}\) as for SGD under perturbation stability and nontriviality where \(\tilde{\chi}_{t-1}=\Theta(1)\).

For \(l\in[2,L]\), it holds that

\[W_{t}^{l}-W_{t-1}^{l}=-\eta\tilde{\chi}_{t-1}\theta_{W^{l}}\frac{1}{n}dh_{SAM,t-1}^{l}(\tilde{x}_{t-1}^{l-1})^{T},\]

with the right scaling \(\theta_{W^{l}}=n^{1-c_{l}-\min(b_{L+1},c_{L+1},d+d_{L+1})}\) as for SGD under perturbation stability \(\tilde{x}_{t-1}^{l-1}=\Theta(1)\), so that we get \(\delta h_{t}^{l}\) using a telescope sum,

\[\theta_{l}\delta h_{t}^{l}= W_{t}^{l}x_{t}^{l-1}-W_{t-1}^{l}x_{t-1}^{l-1}=W_{t-1}^{l}(x_{t}^{l-1 }-x_{t-1}^{l-1})+(W_{t}^{l}-W_{t-1}^{l})x_{t}^{l-1}\] \[= \theta_{l-1}\left(W_{0}^{l}\delta x_{t}^{l-1}+\sum_{s=1}^{t-1}(W_ {s}^{l}-W_{s-1}^{l})\delta x_{t}^{l-1}\right)+(W_{t}^{l}-W_{t-1}^{l})x_{t}^{l-1}\] \[= \theta_{l-1}\left(W_{0}^{l}\delta x_{t}^{l-1}-\eta\theta_{W^{l}} \sum_{s=1}^{t-1}\tilde{\chi}_{s-1}\frac{(\tilde{x}_{s-1}^{l-1})^{T}\delta x_{t}^{ l-1}}{n}dh_{SAM,s-1}^{l}\right)\]\[-\eta\theta_{W^{l}}\tilde{\chi}_{t-1}\frac{(\tilde{x}_{t-1}^{l-1})^{T}x_{t}^{l-1}} {n}dh_{SAM,t-1}^{l},\]

which defines a NonLin operation with the vectors \(W_{0}^{l}\delta x_{t}^{l-1},dh_{SAM,0}^{l},dh_{SAM,t-1}^{l}\) and everything else treated as scalars. The scaling is given by

\[\theta_{l}=\max(\theta_{l-1},\theta_{W^{l}}\theta_{l-1},\theta_{W^{l}})=\max_{ m=1}^{l}\theta_{W^{m}}=n^{-r_{l}},\]

with

\[r_{l}:=\min(b_{L+1},c_{L+1},d+d_{L+1})+\min_{m=1}^{l}(c_{m}-\mathbb{I}(m\neq 1 )),\]

where \(\theta_{W^{l}}\leq 1\) for all \(l\in[L]\) for stability. Note that for \(l_{1}\leq l_{2}\), it holds that \(\theta_{l_{1}}\leq\theta_{l_{2}}\), which explains the sufficiency of \(\theta_{L}=n^{-r_{L}}=n^{-r}\) for the stability of the activation updates.

Activations with the same scaling \(\theta_{l}\) can then simply be defined via the NonLin operation

\[\delta x_{t}^{l}:=\theta_{l}^{-1}(\phi(h_{t-1}^{l}+\theta_{l}\delta h_{t}^{l} )-\phi(h_{t-1}^{l})).\]

The updates of the output function are scalars defined as

\[\delta f_{t}:=\theta_{L+1}^{\prime}\frac{\delta W_{t}^{L+1}x_{t}^{L}}{n}+ \theta_{L\nabla}^{\prime}\frac{\hat{W}_{t-1}^{L+1}\delta x_{t}^{L}}{n},\]

where \(\theta_{L+1}^{\prime}=n\theta_{L+1}=n^{1-c_{L+1}}\) and \(\theta_{L\nabla}^{\prime}=n\theta_{\nabla}\theta_{L}=n^{1-\min(b_{L+1},c_{L+1 })-r_{L}}\), where we will see why \(W_{t-1}^{L+1}=\Theta(n^{-\min(b_{L+1},c_{L+1})})\) in the next paragraph. This leads to the constraints \(c_{L+1}\geq 1\) and \(b_{L+1}+r\geq 1\) for the stability of the output function, where equality in either constraint leads to nontriviality.

#### e.1.5 \(t\)-th backward pass

**Perturbations.** Due to linearity and stability, the last layer remains

\[dx_{t}^{L}:=\hat{W}_{t}^{L+1},\]

with scaling \(\theta_{\nabla}=n^{-\min(b_{L+1},c_{L+1})}\).

As in the first backward pass, we use the weight updates to directly compute the preactivation and activation perturbations similar to the \(t\)-th forward pass but performing SGD instead of SAM in the last step. The SGD backward pass for the perturbation is given by

\[dh_{t}^{l} := dx_{t}^{l}\odot\phi^{\prime}(h_{t}^{l}),\] \[dx_{t}^{l-1} := (W_{t}^{l})^{T}dh_{t}^{l}\] \[= \left(W_{0}^{l}-\eta\theta_{W^{l}}\sum_{s=1}^{t}\tilde{\chi}_{s-1 }\frac{1}{n}dh_{SAM,s-1}^{l}(\tilde{x}_{s-1}^{l-1})^{T}\right)^{T}dh_{t}^{l}\] \[= W_{0}^{l}dh_{t}^{l}-\eta(n^{1-c_{l}}\theta_{SAM}\theta_{\nabla}) \sum_{s=1}^{t}\tilde{\chi}_{s-1}\frac{(dh_{SAM,s-1}^{l})^{T}dh_{t}^{l}}{n} \tilde{x}_{s-1}^{l-1},\]

with scaling \(\max(\theta_{\nabla},n^{1-c_{l}}\theta_{SAM}\theta_{\nabla})=\theta_{\nabla}\), since \(n^{1-c_{l}}\theta_{SAM}\leq 1\) is implied by \(r\geq 0\) required for the stability of (pre-)activation updates.

We write \(\chi_{t}=\mathcal{L}^{\prime}(f_{t}(\xi_{t}),y_{t})\) for the derivative of the loss with respect to the unperturbed function (which is \(\Theta(1)\) under stability and nontriviality), and get

\[\tilde{\delta}h_{t}^{1}(\xi) := +\frac{\rho\chi_{t}(\xi_{t}^{T}\xi)dh_{t}^{1}}{\|v_{t}\|},\] \[\tilde{\theta}_{l}\tilde{\delta}h_{t}^{l} := \tilde{\theta}_{l-1}W_{t}^{l}\tilde{\delta}x_{t}^{l-1}+(\tilde{W}_ {t}^{l}-W_{t}^{l})\tilde{x}_{t}^{l-1}\] \[= \tilde{\theta}_{l-1}\left(W_{0}^{l}\tilde{\delta}x_{t}^{l-1}+ \sum_{s=1}^{t}(W_{s}^{l}-W_{s-1}^{l})\tilde{\delta}x_{t}^{l-1}\right)+(\tilde{W} _{t}^{l}-W_{t}^{l})\tilde{x}_{t}^{l-1}\]\[= \tilde{\theta}_{l-1}\left(W_{0}^{l}\tilde{\delta}x_{t}^{l-1}-\eta(n^{1 -c_{l}}\theta_{SAM})\sum_{s=1}^{t}\tilde{\chi}_{s-1}\frac{(\tilde{x}_{s-1}^{l-1} )^{T}\tilde{\delta}x_{t}^{l-1}}{n}dh^{l}_{SAM,s-1}\right)\] \[+\rho\tilde{\theta}_{W^{l}}\frac{\chi_{t}}{\|v_{t}\|}\frac{(x_{t} ^{l-1})^{T}\tilde{x}_{t}^{l-1}}{n}dh^{l}_{t},\]

which defines a NonLin operation with the vectors \(W_{0}^{l}\tilde{\delta}x_{t}^{l-1},dh^{l}_{SAM,0},\ldots,dh^{l}_{SAM,t-1},dh^{l} _{t}\), and where we can now define the definitive scalings \(\tilde{\theta}_{1}:=\tilde{\theta}_{W^{1}}:=n^{-(d+d_{1})}\theta_{\nabla}=n^{- (\min(b_{L+1},c_{L+1})+d+d_{1})}\), \(\tilde{\theta}_{W^{l}}:=n^{1-(d+d_{l})}\theta_{\nabla}=n^{-(\min(b_{L+1},c_{L+1 })+d+(d_{l}-1))}\) and \(\tilde{\theta}_{l}=\max(\tilde{\theta}_{l-1},n^{1-c_{l}}\theta_{SAM}\tilde{ \theta}_{l-1},\tilde{\theta}_{W^{l}})=\max_{m=1}^{l}\tilde{\theta}_{W^{m}}=n^{ -\tilde{r}_{l}}\) with

\[\tilde{r}_{l}:=\min(b_{L+1},c_{L+1})+d+\min_{m=1}^{l}(d_{m}-\mathbb{I}(m\neq 1 )),\]

where we used that \(n^{1-c_{l}}\theta_{SAM}\leq 1\) due to \(r\geq 0\) for stability and \(\tilde{x}_{t}^{l-1}=\Theta(1)\) due to perturbation stability. Perturbation stability of the hidden layer (pre-)activations \(\tilde{\delta}h^{l},\tilde{\delta}x^{l}=O(1)\) for all \(l\in[L]\) holds if and only if \(\tilde{r}:=\tilde{r}_{L}\geq 0\) since \(\tilde{r}_{l}\geq\tilde{r}_{L}\) for all \(l\leq L\).

The activation perturbations \(\tilde{\delta}x_{t}^{l}\) and the perturbation of the output function \(\tilde{\delta}f_{t}\) can be defined exactly as in the first backward pass,

\[\tilde{\delta}x_{t}^{l} := \tilde{\theta}_{l}^{-1}(\phi(h_{t}^{l}+\tilde{\theta}_{l}\tilde{ \delta}h_{t}^{l})-\phi(h_{t}^{l})),\] \[\tilde{\delta}f_{t} := \tilde{W}_{t}^{L+1}\tilde{x}_{t}^{L}-W_{t}^{L+1}x_{t}^{L}=\tilde {\theta}_{L+1}^{\prime}\frac{\delta W_{t}^{L+1}\tilde{x}_{t}^{L}}{n}+\tilde {\theta}_{L\nabla}^{\prime}\frac{\tilde{W}_{t}^{L+1}\tilde{\delta}x_{t}^{L}}{ n},\]

with \(\tilde{\delta}W_{t}^{L+1}:=\frac{\rho\,\chi_{t}\,x_{t}^{L}}{\|v_{t}\|}\) and the same scalings \(\tilde{\theta}_{l}\), \(\tilde{\theta}_{L+1}^{\prime}=n^{1-(d+d_{L+1})}\) and \(\tilde{\theta}_{L\nabla}^{\prime}=n\theta_{\nabla}\tilde{\theta}_{L}=n^{1- \min(b_{L+1},c_{L+1})-\tilde{r}}\) since \(W_{t}^{L+1}=W_{0}^{L+1}+\Delta W_{t}^{L+1}=\max(n^{-b_{L+1}},n^{-c_{L+1}})\), which yields the slightly stronger constraint (than in the first backward pass) \(\min(b_{L+1},c_{L+1})+\tilde{r}\geq 1\) for perturbation stability and either \(\tilde{\theta}_{L+1}^{\prime}=1\) or \(\min(b_{L+1},c_{L+1})+\tilde{r}=1\) for perturbation nontriviality.

**SAM Update.** For each \(l\in\{1,\ldots,L\}\), as in the first backward pass, we get

\[dx_{SAM,t}^{L}:=\hat{W}_{t}^{L+1}+\tilde{\theta}_{(L+1)/\nabla}\ \tilde{\delta}W_{t}^{L+1},\]

with scaling \(\theta_{SAM}=n^{-\min(b_{L+1},c_{L+1},d_{L+1}+1/2)}\) as well as

\[dh_{SAM,t}^{l}:=dx_{SAM,t}^{l}\odot\phi^{\prime}(\tilde{h}_{t}^{l}).\]

For \(dx_{SAM,t}^{l}\) we again use a telescope sum over the weight changes,

\[dx_{SAM,t}^{l-1} := (\tilde{W}_{t}^{l})^{T}dh_{SAM,t}^{l}=(W_{0}^{l}+\theta_{W^{l}} \sum_{s=1}^{t}\delta W_{s}^{l}+\tilde{\theta}_{W^{l}}\tilde{\delta}W_{t}^{l})^ {T}dh_{SAM,t}^{l}\] \[= (W_{0}^{l})^{T}dh_{SAM,t}^{l}-\eta(n^{1-c_{l}}\theta_{SAM})\sum_{ s=1}^{t}\tilde{\chi}_{s-1}\frac{(dh_{SAM,s-1}^{l})^{T}dh_{SAM,t}^{l}}{n}\tilde{x}_{s-1}^{l-1}\] \[+\rho(n^{1/2-d_{l}}\theta_{\nabla})\frac{\chi_{t}}{\|v_{t}\|} \frac{(dh_{t}^{l})^{T}dh_{SAM,t}^{l}}{n}x_{t}^{l-1},\]

which defines a NonLin operation in the vectors \((W_{0}^{l})^{T}dh_{SAM,t}^{l},\tilde{x}_{0}^{l-1},\ldots,\tilde{x}_{t-1}^{l-1},x_{t}^{l-1}\) and everything else treated as scalars. Note that the scalings remain \(\theta_{SAM}\), since \(\nabla_{x_{t}^{l-1}}f\|_{\tilde{W}_{t}}=\Theta(\max(\theta_{SAM},n^{1-c_{l}} \theta_{SAM}^{2},n^{1/2-d_{l}}\theta_{\nabla}\theta_{SAM}))=\Theta(\theta_{SAM})\) under stability, nontriviality, perturbation stability and perturbation nontriviality.

Finally define the loss derivative on the perturbed output function

\[\tilde{\chi}_{t}:=\mathcal{L}^{\prime}(\tilde{W}_{t}^{L+1}\tilde{x}_{t}^{L},y_{t}),\]

and compute the normalized change in \(W^{L+1}\),

\[\delta W_{t+1}^{L+1}:=-\eta\tilde{\chi}_{t}\tilde{x}_{t}^{L}.\]

### The infinite-width limit

In this section, we apply the Master Theorem's computation rules to derive the marginal distributions \(Z\) corresponding to the vectors of the program constructed above. According to the Master Theorem, each such vector \(z\) will have roughly iid coordinates distributed like \(Z^{z}\) in the large \(n\) limit.

We assume stability holds, so that \(\theta\to\mathring{\theta}\in\{0,1\}\) for all scalars \(\theta\) in the program.

For the first forward pass, we have

\[Z^{h_{0}^{1}(\xi)}=\xi Z^{W_{0}^{1}},\qquad Z^{x_{0}^{l}(\xi)}=\phi(Z^{h_{0}^{ 1}(\xi)}),\qquad Z^{h_{0}^{l+1}(\xi)}=Z^{W_{0}^{l+1}x_{0}^{l}(\xi)}.\]

If \(b_{L+1}>1/2\) then \(\mathring{f}_{0}=0\), otherwise if \(b_{L+1}=1/2\) then \(\mathring{f}_{0}\) converges to a nontrivial Gaussian. For the details we refer to Appendix H.4.1 in Yang and Hu (2021), as at initialization their results still hold here.

For the first SGD backward pass, we have

\[Z^{dx_{0}^{l}(\xi)}=Z^{\mathring{W}_{0}^{L+1}},\qquad Z^{dh_{0}^{1}(\xi)}=Z^{ dx_{0}^{l}(\xi)}\phi^{\prime}(Z^{h_{0}^{l}(\xi)}),\qquad Z^{dx_{0}^{l-1}(\xi)}=Z^{(W _{0}^{l})^{T}dh_{0}^{1}(\xi)},\]

where \(\mathring{Z}^{dx_{0}^{l}(\xi)}=0\) and \(Z^{dx_{0}^{l}(\xi)}=\mathring{Z}^{dx_{0}^{l}(\xi)}\) for all \(\xi\in\mathcal{X}\).

For general \(t>0\), we have

\[Z^{dx_{t}^{L}(\xi)} = Z^{\mathring{W}_{t}^{L+1}},\] \[Z^{dh_{t}^{l}(\xi)} = Z^{dx_{t}^{l}(\xi)}\phi^{\prime}(Z^{h_{t}^{l}(\xi)}),\] \[Z^{dx_{t}^{l-1}(\xi)} = Z^{(W_{0}^{l})^{T}dh_{t}^{1}(\xi)}-\eta\mathring{\theta}_{W^{l}} \sum_{s=1}^{t}\mathring{\mathring{\chi}}_{s-1}\mathbb{E}[Z^{dh_{SAM,s-1}^{s}}Z ^{dh_{t}^{1}}]Z^{\mathring{x}_{s-1}^{l-1}},\]

where \(\mathring{\mathring{\chi}}_{s}=\mathcal{L}^{\prime}(\mathring{\mathring{f}}_{s }(\xi_{s}),y_{s})\) for \(s<t\), and \(Z^{(W_{0}^{l})^{T}dh_{t}^{1}(\xi)}\) is a \(\Theta(1)\) random variable distributed as

\[Z^{(W_{0}^{l})^{T}dh_{t}^{1}(\xi)}=\mathring{Z}^{(W_{0}^{l})^{T}dh_{t}^{1}(\xi )}+\sum_{v\in\mathcal{V}:\;W_{0}^{l}\in\mathcal{V}}Z^{v}\;\mathbb{E}\frac{ \partial Z^{dh_{t}^{1}(\xi)}}{\partial\mathring{Z}^{W_{0}^{l}v}}.\]

For all \(t\geq 0\), the limit of the gradient norm is given by

\[\|\mathring{v}\|=\mathring{\chi}_{t}\left(\mathring{\theta}_{\|v^{1}\|}^{2} \mathbb{E}[Z^{(dh_{t}^{1})^{2}}](\xi_{t}^{T}\xi_{t})+\sum_{l=2}^{L}\mathring{ \theta}_{\|v^{l}\|}^{2}\mathbb{E}[Z^{(dh_{t}^{1})^{2}}]\mathbb{E}[Z^{(x_{t}^{l -1})^{2}}]+\mathring{\theta}_{\|v^{L+1}\|}^{2}\frac{(x_{t}^{L})^{T}x_{t}^{L}}{ n}\right)^{1/2},\] (E.3)

where \(\mathring{\chi}_{t}=\mathcal{L}^{\prime}(\mathring{f}_{t}(\xi_{t}),y_{t})\), \(\theta_{\|v^{1}\|}^{2}:=n^{1-2d_{l}}\theta_{\nabla}^{2}\), \(\theta_{\|v^{l}\|}^{2}:=n^{2-2d_{l}}\theta_{\nabla}^{2}\) for \(l\in[2,L]\) and \(\theta_{\|v^{L+1}\|}^{2}:=n^{1-2d_{L+1}}\), and where \(\mathring{\theta}_{\|v^{L+1}\|}^{2}=1\) if and only if \(d_{L+1}=1/2\) and \(\mathring{\theta}_{\|v^{L+1}\|}^{2}=0\) if and only if \(d_{L+1}>1/2\), while \(\mathring{\theta}_{\|v^{l}\|}^{2}=1\) if and only if \(2d_{l}=1+\mathbb{I}(l>1)-2\min(b_{L+1},c_{L+1})\) and \(\mathring{\theta}_{\|v^{l}\|}^{2}=0\) if and only if \(2d_{l}>1+\mathbb{I}(l>1)-2\min(b_{L+1},c_{L+1})\).

For the last-layer weight perturbations (for \(\theta_{\nabla}\geq\tilde{\theta}_{L+1}\), else \(Z^{\mathring{W}_{t}^{L+1}}=Z^{\mathring{\sharp}W_{t}^{L+1}}\)) we have

\[Z^{\mathring{W}_{t}^{L+1}}=Z^{\mathring{W}_{t}^{L+1}}+\mathring{\mathring{\theta }}_{(L+1)/\nabla}Z^{\mathring{\sharp}W_{t}^{L+1}},\qquad Z^{\mathring{\sharp }W_{t}^{L+1}}=\frac{\rho\,\mathring{\chi}_{t}}{\|\mathring{v}\|}\,Z^{x_{t}^{L}}.\]

Note that \(\mathring{\chi}_{t}\) cancels itself out and we purely get a perturbation in distribution \(Z^{x_{t}^{L}}\) scaled to have standard deviation \(\rho\).

For all \(t\geq 0\) and \(l\in[1,L]\), we have

\[Z^{\mathring{h}_{t}^{l}}=Z^{h_{t}^{l}}+\mathring{\mathring{\theta}}_{I}Z^{ \mathring{\sharp}h_{t}^{l}},\qquad Z^{\mathring{x}_{t}^{l}}=Z^{x_{t}^{l}}+ \mathring{\mathring{\theta}}_{I}Z^{\mathring{\delta}x_{t}^{l}},\]

where for \(l=1\),

\[Z^{\mathring{\sharp}h_{t}^{1}(\xi)}=+\frac{\rho\mathring{\chi}_{t}(\xi_{t}^{T} \xi)}{\|\mathring{v}\|}Z^{dh_{t}^{1}}.\]

[MISSING_PAGE_FAIL:30]

If \(\hat{\theta}_{l}=0\), then

\[Z^{\delta x_{t}^{l}}=\phi^{\prime}(Z^{h_{t-1}^{l}})Z^{\delta h_{t}^{l}},\]

otherwise \(\hat{\theta}_{l}=1\) and

\[Z^{\delta x_{t}^{l}}=\phi(Z^{h_{t}^{l}})-\phi(Z^{h_{t-1}^{l}}).\]

The last-layer SAM weight update is given by

\[Z^{\hat{W}_{t}^{L+1}}=Z^{\hat{W}_{0}^{L+1}}+\mathring{\theta}_{L+1/\nabla}(Z^{ \delta W_{1}^{L+1}}+\cdots+Z^{\delta W_{t}^{L+1}}),\]

with \(Z^{\delta W_{t}^{L+1}}=-\eta\mathring{\chi}_{t-1}Z^{\hat{x}_{t-1}^{L}}\).

For \(t>0\), the SAM function update is given by

\[\mathring{f}_{t}=\mathring{f}_{0}+\mathring{\delta}f_{1}+\cdots+\mathring{ \delta}f_{t},\]

with \(\mathring{\delta}f_{t}=\mathring{\theta}^{\prime}_{L+1}\mathbb{E}[Z^{\delta W _{t}^{L+1}}Z^{x_{t}^{L}}]+\mathring{\theta}^{\prime}_{L\nabla}\mathbb{E}[Z^{ \hat{W}_{t-1}^{L+1}}Z^{\delta x_{t}^{L}}]\).

### Concluding the proof of all main results

After writing out the Ne\(\otimes\)or\(\top\)program and its limit, as well as tracking all scalings, the main results stated in Appendix D all follow from the Tensor Program Master Theorem and from the characterization results in Yang and Hu (2021) in the following way.

Formally Yang and Hu (2021) show feature learning for SGD with small enough learning rate \(\eta>0\) by proving \(\partial_{\eta}^{2}\mathbb{E}(Z^{x_{t}^{1}(\xi_{0})})^{2}\neq 0\) at \(\eta=0\), and they show that learning does not occur in the kernel regime by showing \(\partial_{\eta}^{3}\mathring{f}_{1}\neq 0\), hence \(\mathring{f}_{1}-\mathring{f}_{0}\) is not linear in \(\eta\).

Both \(\mathbb{E}(Z^{x_{1}^{l}(\xi_{0})})^{2}\) and \(\mathring{f}_{1}\) are defined via Ne\(\otimes\)or\(\top\)computations and can be written as a composition of additions, multiplications, the expectation operator, applications of \(\phi\) and \(\phi^{\prime}\), overall applications of infinitely differentiable, pseudo-Lipschitz functions to (Gaussian) random variables, \(\eta\) and \(\rho\). Consequently \(\mathbb{E}(Z^{x_{1}^{l}(\xi_{0})})^{2}\) and \(\mathring{f}_{1}\) are infinitely often differentiable as a function of both \(\eta\) and \(\rho\), where differentiating the expectation operator is covered in Yang and Hu (2021, Lemma H.39). Since Yang and Hu (2021) cover the case \(\rho=0\), their proofs immediately show the correctness of the derived scalings for SAM as long as \(\eta>0\) and \(\rho>0\) are chosen small enough. Both the gradient evaluation for the perturbation as well as the gradient evaluation for the updates stay arbitrarily close to those of SGD if \(\rho>0\) is chosen small enough. The conditions for stability, nontriviality, feature learning, perturbation nontriviality and effective perturbations now follow from considering the respective scaling.

#### e.3.1 Proof of Theorem d.2

A \(bcd\)-parameterization is stable if and only if all scalings in the Tensor Program have the limit \(\mathring{\theta}\in\{0,1\}\), where \(\mathring{\theta}=1\) is required for activations at initialization (for which nothing changes compared to SGD). Potential cancellations are taken care of for sufficiently small \(\eta>0\) and \(\rho>0\) by the argument above. Now collecting all constraints that are already stated in the Tensor Program formulation at the respective step concludes the proof.

#### e.3.2 Proof of Theorem d.3

A stable \(bcd\)-parameterization is nontrivial if and only if \(\mathring{f}_{t}=\Theta(1)\) if and only if \(\mathring{\theta}^{\prime}_{L+1}=1\) or \(\mathring{\theta}^{\prime}_{L\nabla}=1\).

#### e.3.3 Proof of Theorem d.4

A stable \(bcd\)-parametrization is feature learning in layer \(l\) if and only if the feature update scaling \(\mathring{\theta}_{l}=1\) where

\[\theta_{l}=n^{-r_{l}},\quad r_{l}:=\min(b_{L+1},c_{L+1},d_{L+1}+1/2)+\min_{m=1 }^{l}(c_{m}-\mathbb{I}(m\neq 1)).\]

Hence a stable \(bcd\)-parametrization is feature learning in layer \(l\) if and only if \(r_{l}=0\).

Since for all \(l_{1}\leq l_{2}\), it holds that \(r_{l_{1}}\geq r_{l_{2}}\geq 0\), we get the equivalence for any \(l_{0}\in[L]\): A stable \(bcd\)-parametrization is feature learning in layer \(l_{0}\) if and only if it is feature learning in layer \(l\) for all \(l\geq l_{0}\) if and only if \(r_{l_{0}}=0\).

#### e.3.4 Proof of Theorem D.6

Given a stable \(bcd\)-parametrization, perturbation triviality is fulfilled if and only if \(\mathring{\tilde{\theta}}^{\prime}_{L+1}=0\) and \(\mathring{\tilde{\theta}}^{\prime}_{L\nabla}=0\), where \(\tilde{\theta}^{\prime}_{L+1}=n^{1/2-d_{L+1}}\) and \(\tilde{\theta}^{\prime}_{L\nabla}=n\theta_{\nabla}\tilde{\theta}_{L}=n^{1- \min(b_{L+1},c_{L+1})-\tilde{r}}\), hence if and only if \(d_{L+1}>1/2\) and \(\min(b_{L+1},c_{L+1})+\tilde{r}>1\).

In that case, \(\mathring{\tilde{f}}_{t}=\mathring{f}_{t}\), but \(\mathring{f}_{t}\) may still be affected by non-vanishing SAM perturbations in \(\delta W^{L+1}_{t}\) and \(\delta x^{L}_{t}\). Only when all SAM perturbations vanish are we effectively only using SGD. By definition, the perturbation scale in the \(l\)-th layer vanishes if and only if \(\mathring{\tilde{\theta}}_{l}=0\), where \(\tilde{\theta}_{l}=n^{-\tilde{r}_{l}}\) with \(\tilde{r}_{l}=\min(b_{L+1},c_{L+1})+1/2+\min_{m=1}^{l}(d_{m}-\mathbb{I}(m\neq 1))\), hence if and only if \(\tilde{r}_{l}>0\). Since \(\tilde{r}_{l}\geq\tilde{r}_{L}=\tilde{r}\) for all \(l\leq L\), we get \(\mathring{\tilde{\theta}}_{l}=0\) for all \(l\in[L]\) if and only if \(\tilde{r}>0\). Similarly, for any reference layer \(l_{0}\in[L]\), we get \(\tilde{\tilde{\theta}}_{l}=0\) for all \(l\leq l_{0}\) if and only if \(\tilde{r}_{l_{0}}>0\). In words, for any \(l_{0}\in[L]\), we have vanishing perturbations in layer \(l_{0}\) if and only if we have vanishing perturbations until layer \(l_{0}\) if and only if \(\tilde{r}_{l_{0}}>0\).

Altogether, a stable \(bcd\)-parametrization has vanishing perturbations if and only if \(\tilde{r}>0\), \(d_{L+1}>1/2\) and \(\min(b_{L+1},c_{L+1})+\tilde{r}>1\). This case reduces to the results in Yang and Hu (2021) in the limit. Since stability requires \(c_{L+1}\geq 1\) and \(\tilde{r}\geq 0\), we can rewrite the equivalence conditions as \(d_{L+1}\geq 1/2\) and \(\tilde{r}>\max(0,1-b_{L+1})\).

#### e.3.5 Proof of Theorem D.8

Recall \(\tilde{\theta}_{W^{1}}:=n^{-(d+d_{1})}\theta_{\nabla}\), \(\tilde{\theta}_{W^{1}}:=n^{1-(d+d_{l})}\theta_{\nabla}\) and, for the last layer \(\tilde{\theta}_{W^{L+1}}:=n^{-(d+d_{L+1})}\).

As opposed to perturbation nontriviality, we are not only interested in \(\tilde{\theta}_{l}=\max(\tilde{\theta}_{l-1},\tilde{\theta}_{W^{1}})=\max_{m= 1}^{l}\tilde{\theta}_{W^{m}}\to 1\), but in a non-vanishing contribution of the perturbations in layer \(l\), i.e. \(\tilde{\theta}_{W^{l}}=1\) or, for the last layer, \(\tilde{\theta}_{L+1}=1\).

#### e.3.6 Proof of Theorem D.9

The limit of the gradient norm is defined as a Ne\(\otimes\)or\(\top\) program scalar (E.3). Note that for \(b_{L+1}>1/2\), the last-layer scaling strictly dominates all other scalings leading to the simplified gradient norm formula.

Now consider an arbitrary stable choice of layerwise initialization variances \(\{b_{l}\}_{l\in[L+1]}\) and learning rates \(\{c_{l}\}_{l\in[L+1]}\). To fulfill the gradient norm constraints (D.1), we have to choose \(d_{l}=C=1/2\) for all \(l\in[L+1]\), because stability requires \(\min(b_{L+1},c_{L+1})\geq 1/2\). Now stability of the output function perturbations requires \(d\geq 1/2\), where \(d>1/2\) yields vanishing perturbations and \(d=1/2\) yields effective last-layer SAM through the term \(\tilde{\delta}W^{L+1}_{t}\tilde{x}^{L}_{t}\). After choosing \(d\geq 1/2\), we get \(\tilde{r}\geq\min(b_{L+1},c_{L+1})\geq 1/2>0\) which implies vanishing perturbations in all hidden layers.

#### e.3.7 Proof of Proposition D.10

To achieve non-vanishing gradient norm contribution of the last layer in (D.1), we need to choose \(d_{L+1}=1/2\), which requires \(d\geq 1/2\) for stability of the output function perturbations. Achieving non-vanishing gradient norm contributions of all layers requires \(d_{1}=1/2-\min(b_{L+1},c_{L+1})\) and \(d_{l}=1-\min(b_{L+1},c_{L+1})\) for \(l\in[2,L]\), which results in \(\tilde{r}=d\geq 1/2>0\) which implies vanishing perturbations in all hidden layers.

#### e.3.8 Proof of Theorem D.11

Given a stable \(bcd\)-parametrization, we know \(d+d_{L+1}\geq 1\), so that the feature learning constraint \(r\) is not affected by any stable choice of \(d\cup\{d_{l}\}_{l\in[L+1]}\). The maximal stable choice of layerwise initialization variances \(\{b_{l}\}_{l\in[L+1]}\) and learning rates \(\{c_{l}\}_{l\in[L+1]}\) that constitute \(\mu\)P is therefore unaffected by the perturbation scalings \(d\cup\{d_{l}\}_{l\in[L+1]}\).

Stability of the output function perturbations requires \(b_{L+1}+\tilde{r}\geq 1\). Hence if \(b_{L+1}<1\), then \(\tilde{r}\geq 1-b_{L+1}>0\), which implies vanishing perturbations in all hidden layers.

From now on consider \(b_{L+1}\geq 1\). Recall \(c_{\nabla}:=\min(b_{L+1},c_{L+1})\). In \(\mu\)P, \(c_{\nabla}=1\), but effective perturbations in all layers can be achieved more generally for \(c_{\nabla}\geq 1\). Choosing \(d_{1}=1/2-c_{\nabla}\) saturates the gradient norm constraint (D.1). To reach effective perturbations already in the first layer \(\tilde{r}_{1}=c_{\nabla}+d+d_{1}=0\), we need \(d=-1/2\). For perturbation stability and last-layer effective perturbations, we need \(d+d_{L+1}=1\) which requires \(d_{L+1}=3/2\). Achieving perturbation stability and effective perturbations in all hidden layers requires \(\tilde{\theta}_{W^{l}}=1\) which is equivalent to \(c_{\nabla}+d+d_{l}-\mathbb{I}(l\neq 1)=0\). For \(l\in[2,L]\), we therefore need \(d_{l}=3/2-c_{\nabla}\). This choice of \(\{d_{l}\}_{l\in[L+1]}\) achieves effective perturbations in all layers.

To show uniqueness we iterate through all possibilities of saturating the norm bound constraint (D.1). We have considered the cases \(d_{L+1}=1/2\) in (b) leading to vanishing perturbations in all hidden layers and \(d_{1}=1/2-c_{\nabla}\) in (c) with only one choice for effective perturbations in all layers. Lastly consider \(d_{l}=1-c_{\nabla}\) for \(l\in[2,L]\) for non-vanishing gradient contribution of the hidden layers. Note that all hidden layers play the same role in all relevant constraints. Effective perturbations in any hidden layer \(l\in[2,L]\) requires \(\tilde{\theta}_{W^{l}}=1\) for which we need \(d=0\). But then, as \(d_{1}\geq 1/2-c_{\nabla}\), it holds that \(\tilde{r}_{1}\geq 1/2\) implying vanishing perturbations in the first layer. This shows the uniqueness of (1).

For the gradient norm statements, note that the gradient norm \(\|v_{t}\|\) can be written as a Ne\(\otimes\)or\(\top\)computation rule (E.2) where the layer scalings in this parameterization are \(\Theta(1)\) for the input layer, \(\Theta(n^{-1/2})\) for hidden layers and \(\Theta(n^{-1})\) for the output layer. Now the Tensor Program master theorem immediately implies the result.

#### e.3.9 Proof of Proposition d.12

Perturbation nontriviality with respect to any hidden layer is equivalent to \(\tilde{r}=0\). Since \(\min(b_{L+1},c_{L+1})\leq 1\), we get \(\min(b_{L+1},c_{L+1})+\tilde{r}\leq 1\). Since stability requires \(\min(b_{L+1},c_{L+1})+\tilde{r}\geq 1\), we get \(\min(b_{L+1},c_{L+1})+\tilde{r}=1\), which implies perturbation nontriviality with respect to the output.

#### e.3.10 Proof of Proposition d.13

The constraint is the same constraint as in Theorem D.8, which implies effective perturbations in the first layer. Now \(\tilde{r}_{l}\leq\tilde{r}_{1}=0\) implies perturbation nontriviality in all hidden layers due to Theorem D.6.

### Analytic expression of the features after first SAM update

Below we state the analytic expression of the first SAM update, but leave a closer analysis of its fine-grained dynamics in comparison to SGD to future work. Before looking into the effective perturbation regime, we restate Lemma H.37 in Yang and Hu (2021) with a more detailed proof.

First, we define \(\ell\in[L]\) as the unique index that satisfies \(\theta_{L}=\cdots=\theta_{\ell}=1>\theta_{\ell-1}\geq\cdots\geq\theta_{1}\). In words, \(\ell\) is the first layer in which feature learning occurs. Analogously, we define \(\tilde{\ell}\in[L]\) as the unique index that satisfies \(1=\frac{\tilde{\theta}_{L}}{\tilde{\theta}_{L}}=\cdots=\frac{\tilde{\theta}_{ \tilde{\ell}}}{\tilde{\theta}_{L}}>\frac{\tilde{\theta}_{\tilde{\ell}-1}}{ \tilde{\theta}_{L}}\geq\cdots\geq\frac{\tilde{\theta}_{\tilde{\ell}}}{\tilde{ \theta}_{L}}\).

**Lemma E.2** (**Features after first SGD step**).: _Defining \(Z^{l}_{t}:=Z^{h_{t}^{l}}\), \(\gamma^{l}(\eta)=\mathbb{E}\phi(Z^{l}_{0})\phi(Z^{l}_{1})\) for \(l\geq 1\), \(\gamma^{0}=\xi^{T}_{0}\xi\) and \(\gamma^{l}_{11}(\eta)=\mathbb{E}\phi^{\prime}(Z^{l}_{0})\phi^{\prime}(Z^{l}_{1})\), we have_

\[Z^{\ell-1}_{1}=Z^{\ell-1}_{0},\ldots,Z^{1}_{1}=Z^{1}_{0},\]

_and, for all \(l\geq\ell\),_

\[Z^{l}_{1}=Z^{l}_{0}+\mathbb{I}_{l>\ell}\hat{Z}^{W^{l}_{0}\xi^{\ell-1}_{1}}+ \eta\beta^{l}Z^{dx^{l}_{0}}\phi^{\prime}(Z^{l}_{0}),\]

_where \(\beta^{l}\) is defined recursively by_

\[\beta^{l}=\beta^{l}(\eta)=-\dot{\chi}_{0}\gamma^{l-1}(\eta)+\beta^{l-1}(\eta) \gamma^{l-1}_{11}(\eta),\]

_with \(\beta^{\ell-1}=0\). Note that \(\beta^{l}(0)<0\) for all \(l\geq\ell\)._Proof.: By the defining infinite-width equations, assuming \(\hat{\theta}_{W^{l}/l}=1\) (so minimal stable choice of \(c_{l}\)),

\[Z^{l}_{1}=Z^{l}_{0}+\hat{\theta}_{(\ell-1)/\ell}Z^{W^{l}_{0}\delta x^{l-1}_{1}}- \eta\check{\gamma}_{0}\gamma^{l-1}Z^{dx^{l}_{0}}\phi^{\prime}(Z^{l}_{0}).\]

At \(l=\ell\), we get \(\hat{\theta}_{(\ell-1)/\ell}=0\), whereas for \(l>\ell\) we get \(\hat{\theta}_{(l-1)/l}=1\), which results in \(\hat{\theta}_{(\ell-1)/\ell}=\mathbb{I}_{l>\ell}\).

Now, for \(l>\ell\), the second term decomposes into \(\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}\) and

\[\check{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}=Z^{dh^{l}_{0}}\mathbb{E}\frac{\partial Z ^{\check{Z}^{l-1}_{1}}}{\partial\check{Z}^{(W^{l}_{0})^{T}dh^{l}_{0}}}.\]

Since by induction hypothesis,

\[Z^{\delta x^{l-1}_{1}}=\phi(Z^{l-1}_{1})-\phi(Z^{l-1}_{0})=\phi\left(Z^{l-1}_{ 0}+\mathbb{I}_{l>\ell}\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}+\eta\beta^{l-1}Z^{ dx^{l-1}_{0}}\phi^{\prime}(Z^{l-1}_{0})\right)-\phi(Z^{l-1}_{0}),\]

where \(Z^{dx^{l-1}_{0}}=Z^{(W^{l}_{0})^{T}dh^{l}_{0}}\) is the only dependence on \(\hat{Z}^{(W^{l}_{0})^{T}dh^{l}_{0}}\), we get

\[\frac{\partial Z^{\delta x^{l-1}_{1}}}{\partial\check{Z}^{(W^{l}_{0})^{T}dh^{ l}_{0}}}=\phi^{\prime}(Z^{l-1}_{1})\eta\beta^{l-1}\phi^{\prime}(Z^{l-1}_{0}).\]

Plugging the derivative back into the defining equation and noticing that \(Z^{dh^{l}_{0}}=Z^{dx^{l}_{0}}\phi^{\prime}(Z^{l}_{0})\) concludes the proof. 

An analogous analysis for the perturbation at initialization shows.

**Lemma E.3** (**Feature perturbation at initialization)**.: _The perturbation trivial layers fulfill_

\[Z^{\tilde{h}^{\tilde{\ell}-1}_{0}}=Z^{h^{\tilde{\ell}-1}_{0}},\dots,Z^{\tilde {h}^{1}_{0}}=Z^{h^{1}_{0}},\]

_and, for all \(l\geq\tilde{\ell}\),_

\[Z^{\tilde{h}^{l}_{0}}=Z^{h^{l}_{0}}+\mathbb{I}_{l>\tilde{\ell}}\hat{Z}^{W^{l}_ {0}\tilde{\delta}x^{l-1}_{0}}+\rho\tilde{\beta}^{l}Z^{dx^{l}_{0}}\phi^{\prime} (Z^{h^{l}_{0}}),\]

_where \(\tilde{\beta}^{l}\) independent of \(\eta\) is defined recursively by_

\[\tilde{\beta}^{l}=\tilde{\beta}^{l}(\rho)=\frac{\check{\chi}_{0}}{\|\nabla L_ {0}\|}\mathbb{E}[\phi(Z^{h^{l-1}_{0}})\phi(Z^{\tilde{h}^{l-1}_{0}})]+\tilde{ \beta}^{l-1}\mathbb{E}[\phi^{\prime}(Z^{h^{l-1}_{0}})\phi^{\prime}(Z^{\tilde{ h}^{l-1}_{0}})]\]

_with \(\tilde{\beta}^{\tilde{\ell}-1}=0\). Note that \(\tilde{\beta}^{l}(0)>0\) for all \(l\geq\tilde{\ell}\)._

**Remark E.4**.: If \(\tilde{\ell}=1\), in the definition of \(\tilde{\beta}^{l}\) replace \(\mathbb{E}[\phi(Z^{h^{l-1}_{0}})\phi(Z^{\tilde{h}^{l-1}_{0}})]\) by \(\xi^{T}_{0}\xi\). 

Now we are ready to state the closed form expression for the first SAM update.

**Lemma E.5** (**Features after first SAM update)**.: _Defining \(Z^{l}_{t}:=Z^{h^{l}_{t}}\) and \(\tilde{Z}^{l}_{t}:=Z^{h^{l}_{t}}\), we have_

\[Z^{\ell-1}_{1}=Z^{\ell-1}_{0},\dots,Z^{1}_{1}=Z^{1}_{0},\]

_and, for all \(l\geq\ell\),_

\[Z^{l}_{1}=Z^{l}_{0}+\mathbb{I}_{l>\ell}\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}+ \eta\beta^{l}Z^{dx^{l}_{SAM,0}}\phi^{\prime}(\tilde{Z}^{l}_{0})+\eta\gamma^{l} Z^{dh^{l}_{0}},\]

_where \(\beta^{l}\) is defined recursively by_

\[\beta^{l}=\beta^{l}(\eta)=-\hat{\tilde{\chi}}_{0}\mathbb{E}[\phi(\tilde{Z}^{l-1 }_{0})\phi(Z^{l-1}_{1})]+\beta^{l-1}(\eta)\mathbb{E}[\phi^{\prime}(Z^{l-1}_{1} )\phi^{\prime}(\tilde{Z}^{l-1}_{0})],\]

_with \(\beta^{\ell-1}=0\), and \(\gamma^{l}=\gamma^{l}(\eta)\) is recursively defined by_

\[\gamma^{l}:=\beta^{l-1}\rho\tilde{\beta}^{l-1}\mathbb{E}[\phi^{\prime}(Z^{l-1 }_{1})\phi^{\prime}(Z^{l-1}_{0})\phi^{\prime\prime}(\tilde{Z}^{l-1}_{0})Z^{dx^{ l-1}_{SAM,0}}]+\gamma^{l-1}\mathbb{E}[\phi^{\prime}(Z^{l-1}_{0})\phi^{\prime}(Z^{l-1}_{1} )],\]

_with \(\gamma^{\ell-1}=\gamma^{\ell}=0\)._

**Remark E.6**.: If \(\ell=1\), in the definition of \(\beta^{l}\) replace \(\mathbb{E}[\phi(\tilde{Z}^{l-1}_{0})\phi(Z^{l-1}_{1})]\) by \((\xi^{T}_{t-1}\xi)\).

Proof.: By the defining infinite-width equations, for \(l\geq\ell\), assuming \(\hat{\theta}_{W^{l}/l}=1\) (so minimal stable choice of \(c_{l}\)),

\[Z^{l}_{1}=Z^{l}_{0}+\hat{\theta}_{(l-1)/l}Z^{W^{l}_{0}\delta x^{l-1}_{1}}-\eta \hat{\chi}_{0}\mathbb{E}[\phi(\tilde{Z}^{l-1}_{0})\phi(Z^{l-1}_{1})]Z^{dx^{l}_{ SAM,0}\phi}\phi^{\prime}(\tilde{Z}^{l}_{0}).\] (E.4)

At \(l=\ell\), we get \(\hat{\theta}_{(\ell-1)/\ell}=0\) and \(\hat{\theta}_{W^{\ell}/\ell}=1\), whereas for \(l>\ell\) we get \(\hat{\theta}_{(l-1)/l}=1\) and \(\hat{\theta}_{W^{l}/l}=1\) (under minimal stable choice of \(c_{l}\)), which results in \(\hat{\theta}_{(l-1)/l}=\mathbb{I}_{l>\ell}\). Now, for \(l>\ell\), the second term decomposes into \(\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}\) and \(\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}\). For the rest of the proof it remains to analyse \(\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}\). Since by induction hypothesis,

\[Z^{\delta x^{l-1}_{1}}= \phi(Z^{l-1}_{1})-\phi(Z^{l-1}_{0})\] \[= \phi\left(Z^{l-1}_{0}+\mathbb{I}_{l>\ell}\hat{Z}^{W^{l}_{0}\delta x ^{l-1}_{1}}+\eta\beta^{l-1}Z^{dx^{l-1}_{SAM,0}\phi}\phi^{\prime}(\tilde{Z}^{l- 1}_{0})+\eta\gamma^{l-1}Z^{dh^{l-1}_{0}}\right)-\phi(Z^{l-1}_{0}),\]

where \(Z^{dx^{l-1}_{SAM,0}}=Z^{(W^{l}_{0})^{T}dh^{l}_{SAM,0}}+\hat{\rho}\hat{\theta} ^{\hat{\tilde{Z}}_{0}}_{W^{l}}\frac{\hat{\chi}_{0}}{\|\nabla L_{0}\|}\mathbb{E }[Z^{dh^{l}_{0}}Z^{dh^{l}_{SAM,0}}]Z^{x^{l-1}_{0}}\) with the second term independent of \((W^{l}_{0})^{T}\) and by Lemma E.3 we know \(\tilde{Z}^{l-1}_{0}=Z^{l-1}_{0}+\mathbb{I}_{l-1>\ell}\hat{Z}^{W^{l-1}_{0}\delta x ^{l-2}_{0}}+\rho\tilde{\beta}^{l-1}Z^{dx^{l-1}_{0}}\phi^{\prime}(Z^{l-1}_{0})\), where only the last term with \(Z^{dx^{l-1}_{0}}=Z^{(W^{l}_{0})^{T}dh^{l}_{0}}\) influences \(\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}\), we get

\[\hat{Z}^{W^{l}_{0}\delta x^{l-1}_{1}}=Z^{dh^{l}_{0}}\mathbb{E}\frac{\partial Z ^{\delta x^{l-1}_{1}}}{\partial\hat{Z}^{(W^{l}_{0})^{T}dh^{l}_{0}}}+Z^{dh^{l} _{SAM,0}}\mathbb{E}\frac{\partial Z^{\delta x^{l-1}_{1}}}{\partial\hat{Z}^{( W^{l}_{0})^{T}dh^{l}_{SAM,0}}},\] (E.5)

with

\[\frac{\partial Z^{\delta x^{l-1}_{1}}}{\partial\hat{Z}^{(W^{l}_{0})^{T}dh^{l}_ {SAM,0}}}=\phi^{\prime}(Z^{l-1}_{1})\eta\beta^{l-1}\phi^{\prime}(\tilde{Z}^{ l-1}_{0}),\]

and, using \(Z^{dh^{l-1}_{0}}=Z^{dx^{l-1}_{0}}\phi^{\prime}(Z^{l-1}_{0})=Z^{(W^{l}_{0})^{T} dh^{l}_{0}}\phi^{\prime}(Z^{l-1}_{0})\), yields

\[\frac{\partial Z^{\delta x^{l-1}_{1}}}{\partial\hat{Z}^{(W^{l}_{0 })^{T}dh^{l}_{0}}}= \phi^{\prime}(Z^{l-1}_{1})\left(\eta\beta^{l-1}Z^{dx^{l-1}_{SAM,0 }\phi^{\prime\prime}(\tilde{Z}^{l-1}_{0})}\frac{\partial\tilde{Z}^{l-1}_{0}}{ \partial\hat{Z}^{(W^{l}_{0})^{T}dh^{l}_{0}}}+\eta\gamma^{l-1}\phi^{\prime}(Z^{l -1}_{0})\right)\] \[= \phi^{\prime}(Z^{l-1}_{1})\left(\eta\beta^{l-1}Z^{dx^{l-1}_{SAM,0 }\phi^{\prime\prime}(\tilde{Z}^{l-1}_{0})}\rho\tilde{\beta}^{l-1}\phi^{ \prime}(Z^{l-1}_{0})+\eta\gamma^{l-1}\phi^{\prime}(Z^{l-1}_{0})\right).\]

Plugging Eq. (E.5) back into the defining equation (E.4) and noticing that \(Z^{dh^{l}_{SAM,0}}=Z^{dx^{l}_{SAM,0}\phi^{\prime}(\tilde{Z}^{l}_{0})}\) as well as \(Z^{dh^{l}_{0}}=Z^{dx^{l}_{0}}\phi^{\prime}(Z^{l}_{0})\) concludes the proof. 

## Appendix F Generalizations and further perturbation scaling considerations

### Overview over choices of \(d_{l}\) and \(d\)

Since for some combinations of architectures and datasets it turns out that performing SAM on a subset of layers performs better than effective perturbations in all layers (Muller et al., 2024), we would like to know how to choose \(d\) and \(d_{l}\) to adjust which layers should be effectively perturbed and which should have vanishing weight perturbations. In practice, simply set all perturbations that should vanish to \(0\) by design, and use the global scaling \(d\) and relative scalings \(d_{l}\) from \(\mu\)P\({}^{2}\) for the perturbed layers. This section is instead interested in a complete characterization of all possible choices of \(\{d_{l}\}_{l\in[L+1]}\) and \(d\). The heuristic derivation only requires the gradient norm constraints (D.1) and the perturbation stability constraints that require \(\tilde{\delta}W^{1}=O(1)\) and \(\tilde{\delta}W^{l}=O(n^{-1})\) for \(l>1\) given by

\[d_{l}\geq\begin{cases}-c_{\nabla}-d&\text{if $l$ is input-like,}\\ 1-c_{\nabla}-d&\text{if $l$ is hidden-like,}\\ 1-d&\text{if $l$ is output-like,}\end{cases}\] (F.1)

where a layer is effectively perturbed if and only if equality holds in the respective perturbation stability inequality. This heuristic claim yields the characterization of all phases of the choices of perturbation scalings \(d\) and \(d_{l}\) in Table F.1 and allows us to formulate a simple rule of how to choose \(d\) and \(d_{l}\) given the information which layers should be effectively perturbed, and which should have vanishing weight perturbations.

**Choice of perturbation scaling from list of layers to effectively perturb.** We denote the set of all layers by \(\mathcal{L}\), whereas the subset of layers, which we want to effectively perturb, is denoted by \(\mathcal{L}_{SAM}\subseteq\mathcal{L}\).

1. If there exists an input-like layer \(l\in\mathcal{L}_{SAM}\), set \(d=-1/2\). Input-like layers are effectively perturbed if and only if \(d_{l}=1/2-c_{\nabla}\). Hidden-like (respectively, output-like) layers are effectively perturbed if and only if \(d_{l}=3/2-c_{\nabla}\) (respectively, \(d_{l}=3/2\)). For all layers that have vanishing weight perturbations, do not perturb these weights or choose \(d_{l}>1/2-c_{\nabla}\) for input-like, \(d_{l}>3/2-c_{\nabla}\) for hidden-like and \(d_{l}>3/2\) for output-like layers.
2. If all input-like layers should have vanishing weight perturbations but there exists a hidden-like layer \(l\in\mathcal{L}_{SAM}\), set \(d=0\). Hidden-like layers are effectively perturbed if and only if \(d_{l}=1-c_{\nabla}\). Output-like layers are effectively perturbed if and only if \(d_{L+1}=1\). For all layers that have vanishing weight perturbations, do not perturb these weights, or set \(d_{l}>c_{\nabla}\) for input-like, \(d_{l}>1-c_{\nabla}\) for hidden-like and \(d_{l}>1\) for output-like layers (as required by the perturbation stability and gradient norm constraints).
3. If both all input-like and all hidden-like layers have vanishing weight perturbations, but there exists some output-like layer \(l\in\mathcal{L}_{SAM}\), then set \(d=1/2\). Output-like layers are effectively perturbed if and only if \(d_{l}=1/2\). For all layers that have vanishing weight perturbations, do not perturb these weights or set \(d_{l}\geq 1/2-c_{\nabla}\) for input-like, \(d_{l}\geq 1-c_{\nabla}\) for hidden-like and \(d_{l}>1/2\) for output-like layers (as required by the perturbation stability and gradient norm constraints).
4. If \(\mathcal{L}_{SAM}=\emptyset\), then set \(d>1/2\) or simply perform SGD.

**Example F.1** (**First-layer-only effective perturbations)**.: Instead of simply using the rule set above, we derive the necessary choice of perturbation scaling from the scaling equalities and the norm constraints (D.1). To achieve first-layer effective perturbations, but trivial weight perturbations in all other layers, we need \(\tilde{\theta}_{W^{1}}=1\) and \(\tilde{\tilde{\theta}}_{W^{l}}=0\), for which we will choose \(\tilde{\theta}_{W^{l}}=n^{-1}\). This requires setting

\[d_{1}=-(c_{\nabla}+d),\qquad d_{l}=2-c_{\nabla}-d,\qquad d_{L+1}=2-d,\]

where one of the constraints (D.1) has to be fulfilled. Plugging the above \(d_{l}\)-choices into (D.1) results in the constraints \(d\leq-1/2\), \(d\leq 1\), \(d\leq 3/2\), hence choose \(d=-1/2\) so that only the first layer contributes non-vanishingly to the gradient norm. Note that \(\tilde{r}=0\) and output perturbation nontriviality holds if and only if \(\min(b_{L+1},c_{L+1})=1\) (as in \(\mu\)P). We apply this perturbation scaling in Appendix H.2 to show that propagating perturbations from early layers are not enough to inherit SAM's inductive bias that leads to improved generalization performance.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline  & \multicolumn{3}{c|}{Effective perturbations possible} & \multicolumn{3}{c}{Gradient norm may be dominated by} \\  & input-like & hidden-like & output-like & input-like & hidden-like & output-like \\ \hline \hline \(d=-1/2\) & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ \(d\in(-1/2,0)\) & ✗ & ✓ & ✓ & ✓ & ✗ & ✗ \\ \(d=0\) & ✗ & ✓ & ✓ & ✓ & ✓ & ✗ \\ \(d\in(0,1/2)\) & ✗ & ✗ & ✓ & ✓ & ✓ & ✗ \\ \(d=1/2\) & ✗ & ✗ & ✓ & ✓ & ✓ & ✓ \\ \(d>1/2\) & ✗ & ✗ & ✗ & ✓ & ✓ & ✓ \\ \hline \end{tabular}
\end{table}
Table F.1: **(Characterization of perturbation scalings)** Overview over the regimes of all possible choices of \(d\) and \(d_{l}\). A layer is effectively perturbed if and only \(d_{l}\) satisfies (F.1). At least one layer must satisfy equality in its gradient norm constraint (D.1). This table summarizes which layers can exhibit effective perturbations, and which may dominate the gradient norm, given a choice of \(d\). The choice \(d<-1/2\) results in perturbation blowup \(\tilde{r}<0\). At the critical \(d=-1/2\) (respectively, \(d=0\); \(d=1/2\)) a input-like (respectively hidden-like; output-like) layer is effectively perturbed if and only if it dominates the gradient norm. Consequently \(d=-1/2\) implies effective perturbations in at least one input-like layer.

### Other ways to introduce layerwise perturbation scaling

Before presenting alternative ways how layerwise perturbation scaling could be accomplished, let us collect desirable properties that a definition should fulfill:

* Layerwise perturbation scaling should enable stable, effective perturbations in every layer.
* The perturbation step should require at most one additional forward and backward pass in each update step.
* The adapted optimization algorithm should recover the original (SAM) algorithm when not using layerwise perturbation scaling.

We start with the simplest case where the perturbations are normalized in each layer separately.

**Remark F.2** (**SAM with layerwise gradient normalization**).: For (SAM) with layerwise gradient normalization of the perturbations

\[\bm{\varepsilon}^{l}=\rho_{l}\cdot\nabla_{W^{l}}\mathcal{L}/\|\nabla_{W^{l}} \mathcal{L}\|,\] (LN)

where \(\|\cdot\|\) may denote the Frobenius or the spectral norm (equivalent under limited perturbation batch size), the spectral scaling condition (\(*\)) immediately yields the correct layerwise perturbation scaling \(\rho_{l}\stackrel{{!}}{{=}}\Theta(\sqrt{\texttt{fan\_out}/ \texttt{fan\_in}})\). 

However, in practice, perturbations are usually globally normalized across layers, according to the GitHub repositories provided by Foret et al. (2021); Samuel (2022); Kwon et al. (2021); Andriushchenko and Flammarion (2022); Muller et al. (2024). Preliminary ablations in Appendix H.5 suggest that layer-coupled SAM with global normalization slightly outperforms SAM with layerwise gradient normalization. As our goal in this paper is to study (SAM) as applied in practice, we consider SAM with joint gradient normalization.

A first alternative to Definition 4 could scale perturbations after the joint gradient normalization. Opposed to Definition 4, for this variant the perturbation norm, i.e. the radius of the adversarial ascent ball, is not guaranteed to be \(\rho n^{-d}\), but \(\rho(\sum_{l\in[L+1]}\rho_{l}^{2})^{1/2}\). The correct perturbation scaling for this version more immediately follows from the condition that perturbations scale like updates.

**Remark F.3** (**Layerwise perturbation scaling after joint gradient normalization**).: For (SAM) with joint gradient normalization of the perturbations

\[\bm{\varepsilon}^{l}=\rho_{l}\cdot\frac{\nabla_{W^{l}}\mathcal{L}}{\|\nabla_{ \mathbf{W}}\mathcal{L}\|},\]

the correct perturbation scaling in \(\mu\)P is given by \(\rho_{l}\stackrel{{!}}{{=}}\Theta(n^{1/2}\cdot\texttt{fan\_out}/ \texttt{fan\_in})\).

To understand this scaling rule, note that for \(b_{L+1}>1/2\) (such as in \(\mu\)P), the last layer always dominates the gradient norm (see Eq. (E.2) for the TP argument), resulting in the scaling

\[\|\nabla_{\mathbf{W}}\mathcal{L}\|_{F}\approx\mathcal{L}^{\prime}(f_{t}(\xi_{t }),y_{t})\|x^{L}\|=\Theta(n^{1/2}).\]

Thus, compared to SAM without gradient normalization (Appendix F.3), \(\|\nabla_{\mathbf{W}}\mathcal{L}\|_{F}\) always contributes the scaling \(n^{1/2}\). Noting that perturbations should scale like updates, and updates receive the layerwise learning rates \(\eta_{l}\stackrel{{!}}{{=}}\Theta(\texttt{fan\_out}/\texttt{fan \_in})\) concludes the derivation. 

In Definition 4, we accept the additional layer-coupling complications that the layerwise gradient scaling before the joint gradient normalization entails in order to analytically control the perturbation radius to \(\rho n^{-d}\). To simplify the analysis as much as possible, we will first ensure width-independence of the normalization, so that the layerwise perturbation scaling is not affected by the normalization term. Then, layerwise perturbations should be scaled like updates.

Another alternative to layerwise perturbation scaling as in Definition 4 is motivated by the observation, that in \(\mu\)P\({}^{2}\) with Definition 4, only the first layer dominates the joint gradient norm (Theorem D.11). To let all layers contribute width-independently to the joint gradient norm, we can introduce even more hyperparameters (with limited benefit) by decoupling the numerator and denominator scalings in the perturbation. Opposed to Definition 4, the perturbation norm is again not analytically set with the choice of \(\rho\), but may be smaller. Empirically, we do not observe performance differences due to denominator contribution scaling (Appendix H.4). This is the perturbation scaling we implement for ViTs (see Algorithm 1 for details).

**Remark F.4** (SAM with decoupled perturbation numerator and denominator scaling).: For (SAM) with perturbations

\[\bm{\varepsilon}^{l}=\rho n^{-d_{l}}\frac{\nabla_{W^{l}}\mathcal{L}}{\|v\|}\quad \text{with}\quad\|v\|^{2}=\sum_{l=1}^{L+1}n^{-2\tilde{d}_{l}}\|\nabla_{W^{l}} \mathcal{L}\|^{2},\] (DP)

with layerwise perturbation radii \(\rho\cdot n^{-d_{l}}\) and separate gradient norm scaling \(n^{-\tilde{d}_{l}}\). 

In all alternatives, nontrivial layerwise perturbation scaling is necessary for effective perturbations in every layer, which necessarily changes the direction away from the original gradient direction. Such a layerwise gradient rescaling can also be achieved by adapting the architecture with width-dependent weight multipliers. The multipliers (\(a\)-\(\mu P^{2}\)) achieve effective perturbations without layerwise perturbation scaling such that all layers contribute non-vanishingly to the joint gradient norm. They rescale the gradients equivalently to (DP) when scaling all denominator terms to be width-independent. See Appendix F.6 for all details about weight multipliers.

**Adapting the TP-based analysis.** Our TP-based analysis covers all of the above perturbation scaling alternatives with minor adjustments. We just have to replace the normalized TP scalar (E.2). If we want to express \(\|\nabla_{\mathbf{W}}\mathcal{L}\|_{F}\), we just drop all perturbation scaling terms \(n^{-d_{l}}\). For the examples of (LN) and (DP), we replace (E.2) in each layer separately by the normalized TP scalars,

\[\|\nabla_{W^{1}}\mathcal{L}_{t}\|:=\chi_{t}\left(\frac{(dh_{t}^{1})^{T}dh_{t} ^{1}}{n}(\xi_{t}^{T}\xi_{t})\right)^{1/2},\]

with scaling \(\theta_{\|\nabla_{1}\|}=n^{1/2}\theta_{\nabla}\) for the first layer, where \(\theta_{\nabla}\) is overloaded to denote \(\theta_{\nabla}=n^{-b_{L+1}}\) in the first step and \(\theta_{\nabla}=n^{-\min(b_{L+1},c_{L+1})}\) in all later steps (in \(\mu\)P, \(\theta_{\nabla}=n^{-1}\) always),

\[\|\nabla_{W^{l}}\mathcal{L}_{t}\|:=\chi_{t}\left(\frac{(dh_{t}^{l})^{T}dh_{t} ^{l}}{n}\frac{(x_{t}^{l-1})^{T}x_{t}^{l-1}}{n}\right)^{1/2},\]

with scaling \(\theta_{\|\nabla_{L+1}\|}=n\theta_{\nabla}\) for all hidden layers \(l\in[2,L]\), and

\[\|\nabla_{W^{L+1}}\mathcal{L}_{t}\|:=\chi_{t}\left(\frac{(x_{t}^{L})^{T}x_{t} ^{L}}{n}\right)^{1/2}.\]

with scaling \(\sqrt{n}\) for the output layer, with respective normalized limits

\[\hat{\chi}_{t}(\mathbb{E}[Z^{(dh_{t}^{1})^{2}}](\xi_{t}^{T}\xi_{t}))^{1/2}, \quad\hat{\chi}_{t}(\mathbb{E}[Z^{(dh_{t}^{1})^{2}}]\mathbb{E}[Z^{(x_{t}^{L-1 })^{2}}])^{1/2},\quad\hat{\chi}_{t}(\mathbb{E}[Z^{(x_{t}^{L})^{2}}])^{1/2},\]

where \(\hat{\chi}_{t}=\mathcal{L}^{\prime}(\hat{f}_{t}(\xi_{t}),y_{t})\).

The adapted scalings can then be tracked as before to derive the maximal stable layerwise perturbation scaling. Consider for example input layers in (LN). In \(\mu\)P, we know \(\|\nabla_{W^{1}}\mathcal{L}_{t}\|=\Theta(n^{-1/2})\) and \(\nabla_{W^{1}}\mathcal{L}_{t}=\Theta(n^{-1})\) entrywise. Effective perturbations are achieved with \(\bm{\varepsilon}^{1}=\Theta(1)\), so choose \(\rho_{l}=n^{1/2}\) as expected from (\(*\)). Proceed similarly for all layers and perturbation scaling variants.

### Extension to SAM without gradient normalization

Andriushchenko and Flammario (2022) and Andriushchenko et al. (2023) consider the SAM update without normalizing the gradient in the adversarial ascent. The corresponding update rule is given by

\[W_{t}=W_{t}-\eta\nabla_{w}\mathcal{L}(f(\xi_{t};W_{t}+\rho v_{t},y_{t})),y_{t} ),\qquad v_{t}=\nabla_{w}\mathcal{L}(f(\xi_{t};W_{t}).\]

The Ne\(\otimes\)or\(\top\) program for this update rule with arbitrary \(v_{t}^{l}=n^{-c_{l}}\nabla_{w}\mathcal{L}(f(\xi_{t};W_{t})\) is also easily adapted from the above derivation. Just note that the gradient norm appears in an equation if and only if the perturbation radius \(\rho n^{-d}\) appears. Without dividing by \(\|v_{t}\|\), the parameter \(d\) becomes superfluous. Simply set \(d=0\) and remove the gradient norm constraints (D.1) to arrive at the Ne\(\otimes\)or\(\top\) program and \(bcd\)-constraints for the update rule without gradient normalization.

Perturbation scaling \(d_{l}\) plays a similar role as learning rate scaling \(c_{l}\) as both scale similar gradients. We get effective perturbations in the \(l\)-th layer from the equation

[MISSING_PAGE_EMPTY:39]

* choose \(d_{l}\) to satisfy its norm constraint,
* choose \(d\) to induce maximal stable perturbations in that layer,
* choose all other \(d_{l^{\prime}}\), \(l^{\prime}\neq l\), minimal to both satisfy its norm constraint as well as perturbation stability \(\tilde{\delta}W^{l}_{t}=\begin{cases}O(1)&l=1,\\ O(n^{-1})&l>1.\end{cases}\)
* From the above configurations, choose the unique one that yields maximal stable perturbations in all layers.3 F.4.1 Elementwise ASAM If we want to be invariant to elementwise rescaling operators \(T^{l}_{w}(x)=|W^{l}|\odot x\) where \(x,W^{l}\in\mathbb{R}^{m\times n}\) and \(\odot\) denotes elementwise multiplication, the resulting ASAM perturbation rule (where we introduce (layer-wise) perturbation scalings \(\{d\}\cup\{d_{l}\}_{l\in[L+1]}\)) replaces (LP) and is given by \[\tilde{\delta}W^{l}_{t}:=\rho n^{-d}\frac{n^{-d_{l}}|W^{l}|\odot|W^{l}|\odot \nabla_{W^{l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t})}{\|\nabla_{ASAM}^{elem}\|},\] (F.4) with normalization \[\|\nabla_{ASAM}^{elem}\|:=\sum_{l=1}^{L+1}n^{-d_{l}}\left\||W^{l}|\odot\nabla_ {W^{l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t})\right\|_{F},\] where the absolute values \(|W^{l}|\) are computed and multiplied elementwise. To find the correct perturbation scalings, we track the typical elementwise scaling of each quantity as before.

Footnote 3: There must exist such a choice of \(\{d_{l}\}_{l\in[L+1]}\) and \(d\), because \(\{d_{l}\}_{l\in[L+1]}\) allow to set any relative scalings between layers and \(d\) determines the global scaling, which overall allows to set all possible layerwise scalings. Any deviation from the choice that achieves effective perturbations in all layers either results in blowup or a vanishing effect of the weight perturbation in some layer. This shows uniqueness.

**Elementwise ASAM in \(\mu\)P.** In \(\mu\)P, the layerwise weights and gradients scale as (F.2). For \(\|\nabla_{ASAM}^{elem}\|=O(1)\), we therefore replace the constraints (D.1) by the constraints

\[d_{l}\geq 1/2-c_{\nabla},\text{ for }l\in[L],\qquad d_{L+1}\geq-1/2,\] (F.5)

where we can choose \(\{d_{l}\}_{l\in[L+1]}\) to achieve equality in at least one constraint to achieve \(\|\nabla_{ASAM}^{elem}\|=\Theta(1)\).

The layerwise perturbations scale as \(\tilde{\delta}W^{l}_{t}=n^{-d}\begin{cases}\Theta(n^{-d_{l}}\theta_{\nabla})& l=1,\\ \Theta(n^{-1-d_{l}}\theta_{\nabla})&l\in[2,L],\\ \Theta(n^{-d_{L+1}}n^{-2})&l=L+1.\end{cases}\)

Stable and nontrivial perturbations in each layer are achieved under condition (F.3), which induces the constraints for optimal layerwise perturbation scaling

\[d+d_{l}=-c_{\nabla},\text{ for }l\in[L],\qquad d+d_{L+1}=-1.\]

Irrespective which of the above norm constraints (F.5) we satisfy, we need \(d=-1/2\) to achieve optimal layerwise perturbation scaling. Hence \(d=d_{L+1}=-1/2\) and \(d_{l}=1/2-c_{\nabla}\) for \(l\in[L]\) is the unique choice of \(\{d\}\cup\{d_{l}\}_{l\in[L+1]}\) modulo norm scaling equivalence that achieves \(\Theta(1)\) perturbation scaling in all layers. With this choice all layers contribute non-vanishingly to the gradient norm. In \(\mu\)P \(c_{\nabla}=1\), so that \(d_{l}=-1/2\) for all \(l\in[L+1]\), so that ASAM does not require layerwise rescaling of the gradients, but upscaling of the perturbation by \(n^{1/2}\) to achieve nontrivial perturbations in any layer. This may explain why ASAM often outperforms SAM in large models: By only requiring global scaling, ASAM achieves maximal stable perturbations in all layers if the perturbation radius is tuned globally at every width.

If instead of a global gradient norm \(\|\nabla_{ASAM}^{elem}\|\), one would want to normalize in each layer separately with \(\|\nabla_{ASAM}^{elem,l}\|:=n^{-d_{l}}\|\|W^{l}|\odot\nabla_{W^{l}}\mathcal{ L}(f(\xi_{t};W_{t}),y_{t})\|_{F},\) the layerwise perturbation scalings become \(\tilde{\delta}W^{l}_{t}=n^{-d}\begin{cases}\Theta(n^{-1/2})&l=1,\\ \Theta(n^{-3/2})&l>1.\end{cases}\) Again, to achieve maximal stable perturbations in all layers we need \(d=-1/2\) and no layerwise adaptation of the gradient norm.

#### f.4.2 Layerwise ASAM

ASAM with layerwise rescaling as in Muller et al. (2024) employs the layerwise transformations \(T_{w}^{l}(x)=\|W^{l}\|_{F}\cdot x\). This ASAM perturbation rule replaces (LP) and is given by

\[\tilde{\delta}W_{t}^{l}:=\rho n^{-d}\frac{n^{-d_{t}}\|W^{l}\|_{F}^{2}\nabla_{W^{ l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t})}{\|\nabla_{ASAM}^{layer}\|},\] (F.6)

with normalization

\[\|\nabla_{ASAM}^{layer}\|:=\sum_{l=1}^{L+1}n^{-d_{t}}\|W^{l}\|_{F}\|\nabla_{W^ {l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t})\|_{F}.\]

**Layerwise ASAM in \(\mu\)P.** In \(\mu\)P, we have \(\|W^{l}\|_{F}=\begin{cases}\Theta(n^{1/2})&l=1,\\ \Theta(n^{1/2})&l\in[2,L],\\ \Theta(n^{-1/2})&l=L+1.\end{cases}\)

Hence, the norm constraints (D.1) are now replaced by

\[d_{1}\geq 1-c_{\nabla},\qquad d_{l}\geq 3/2-c_{\nabla}\quad\text{for }l\in[2,L], \qquad d_{L+1}\geq 0.\]

The scale of the perturbation numerator now scales as \(\tilde{\delta}W_{t}^{l}=n^{-d}\begin{cases}\Theta(n^{-d_{l}}n\theta_{\nabla}) &l=1,\\ \Theta(n^{-d_{L+1}}n\theta_{\nabla})&l\in[2,L],\\ \Theta(n^{-d_{L+1}}n^{-1})&l=L+1.\end{cases}\)

In \(\mu\)P, achieving maximal stable perturbations (F.3) is therefore equivalent to satisfying the constraints

\[d+d_{1}=0,\qquad d+d_{l}=1\quad\text{for }l\in[2,L],\qquad d+d_{L+1}=0.\]

Now we can simultaneously satisfy the first- and last-layer norm constraints with \(d_{1}=0\) and \(d_{L+1}=0\), while achieving effective perturbations in all layers with \(d=0\) and \(d_{l}=1\). Satisfying the norm constraint in the hidden layers with \(d_{l}=1/2\) would imply vanishing perturbations in the first and last layer (by requiring \(d\geq 1/2\)).

### Representing general architectures and adaptive optimizers as Tensor Programs

Here, we lay out explicitly how to write some of the building blocks in ResNets and ViTs in a Tensor Program and provide further scaling considerations. According to Yang and Hu (2021), it is straightforward to generalize scaling conditions that induce feature learning in MLPs to these other common neural network building blocks. Since perturbations should always scale like updates, the conditions for stable feature learning and those for stable effective perturbations are analogous.

One potential complication in the case of SAM would be a contribution to the joint gradient normalization \(\|v_{t}\|\) that differs from the classical input, hidden or output layer contribution. But we will see that these contributions do not differ for any of the considered layer types.

**Layernorm.** The Layernorm operation is defined as

\[h_{t}^{l+1}=\gamma_{t}\frac{x_{t}^{l}-\nu_{t}^{l}}{\sigma_{t}^{l}+\bm{\varepsilon }}+\beta_{t}^{l},\]

where \(\bm{\varepsilon}>0\) is a small positive constant, \(\gamma_{t}^{l},\beta_{t}^{l}\) are learnable parameters and \(\nu_{t}^{l}=\frac{1}{n}\sum_{i=1}^{n}(x_{t}^{l})_{i}\) is an Avg operation as in Yang and Littwin (2023, Def. 2.6.1) and \(\sigma_{t}^{l}=\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{t}^{l}-\nu_{t}^{l})^{2}}\) is a composition of Nonlin, Avg and Nonlin. The parameters \(\gamma_{t}^{l},\beta_{t}^{l}\) can be seen as input weights to the input \(1\). They should be initialized as \(\gamma_{0}^{l}=1\) and \(\beta_{0}^{l}=0\). In the forward pass, the layernorm preserves stability \(h_{t}^{l+1}=\Theta(1)\) when \(\gamma_{t}^{l}+\beta_{t}^{l}=\Theta(1)\) except for the Lebesgue nullset of learning rates for which they exactly cancel each other out. Recall the notation \(dz=\theta_{z}^{-1}\partial f/\partial z\), where \(\theta_{z}=n^{C}\) for some \(C\in\mathbb{R}\) denotes the width-dependent scaling. The derivatives are

\[d\beta_{t}^{l}=dh_{t}^{l+1},\qquad d\gamma_{t}^{l}=dh_{t}^{l+1}\frac{x_{t}^{l}- \nu_{t}^{l}}{\sigma_{t}^{l}+\bm{\varepsilon}}.\]These gradients coincide both in shape and scaling with the scaling we expect for an input layer, resulting in the same gradient spectral/Frobenius norm scaling. Continuing the backward pass, using \(\frac{\partial\sigma_{t}^{l}}{\partial x_{t}^{l}}=\frac{x_{t}^{l}-\nu_{t}^{l}}{n \sigma_{t}^{l}}\), we get

\[dx_{t}^{l} = dh_{t}^{l+1}\gamma_{t}^{l}\left(\frac{1}{\sigma_{t}^{l}+\bm{ \varepsilon}}(I-\frac{1}{n})-\frac{x_{t}^{l}-\nu_{t}^{l}}{(\sigma_{t}^{l}+\bm {\varepsilon})^{2}}\frac{\partial\sigma_{t}^{l}}{\partial x_{t}^{l}}\right)\] \[= dh_{t}^{l+1}\gamma_{t}^{l}\left(\frac{1}{\sigma_{t}^{l}+\bm{ \varepsilon}}(I-\frac{1}{n}11^{T})-\frac{x_{t}^{l}-\nu_{t}^{l}}{(\sigma_{t}^{l }+\bm{\varepsilon})^{2}}\frac{(x_{t}^{l}-\nu_{t}^{l})^{T}}{n\sigma_{t}^{l}} \right),\]

which preserves the order as long as \(\gamma_{t}^{l}=\Theta(1)\), since \(x_{t}^{l}=\Theta(1)\), we know \(\nu_{t}^{l},\sigma_{t}^{l}=\Theta(1)\).

Note that Layernorm removes the necessity to avoid blowup in the activations \(x_{t}^{l}\) in the forward pass (ignoring potential numerical issues), and always rescales to \(\Theta(\max(\gamma_{t}^{l},\beta_{t}^{l}))\). However, in the backward pass, a scaling \(x_{t}^{l}=\Theta(n^{c})\), with \(c>0\), results in \(dx_{t}^{l}=\Theta(n^{-c}dh_{t}^{l+1}\gamma_{t}^{l})\), hence vanishing gradients. The gradients would only stabilize if \(\phi^{\prime}(h_{t}^{l})=\Theta(h_{t}^{l})\), but no popular activation function has a scale equivariant derivative. Yang (2019) shows how to write Batchnorm and Average Pooling as a Tensor Program.

**Convolutions.** Convolutional layers can be seen as a collection of dense weight matrices where width corresponds to the number of channels (Yang, 2019). With kernel positions \(ker\), input channels \([n^{l}]\) and output channels \([n^{l+1}]\), the weights of a stride-1 convolution are given by \(\{W_{i\alpha\beta}^{l}\}_{i\in ker,\alpha\in[n^{l+1}],\beta\in[n^{l}]}\), so that for each \(i\in ker\), \(W_{i}^{l}\in\mathbb{R}^{n^{l+1}\times n^{l}}\) is a dense matrix. With \(\{x_{i\alpha}^{l}\}_{i\in pos^{l},\alpha\in[n^{l}]}\), the convolution operation is given by

\[(W^{l}*x)_{i\alpha}=\sum_{\beta,j:j+i\in pos^{l}}W_{j\alpha\beta}^{l}x_{i+j, \beta}^{l},\]

which performs MatMul and Avg and where \(ker,pos^{l}\) are assumed to be of fixed size. For \(ker\) of fixed size, convolutional weights scale like hidden layer weight matrices, also in Frobenius norm contributing to \(\|v_{t}\|\).

**Residual connections.** A residual connection propagates the current activation forward, skipping an arbitrarily complex nonlinear block \(f_{t}^{l}:\mathbb{R}^{n_{l}}\rightarrow\mathbb{R}^{n_{l+1}}\) in between, where \(f_{t}^{l}\) can depend on time-dependent parameters like a weight matrix. The forward pass can be written as

\[x_{t}^{l}=x_{t}^{l-1}+f_{t}^{l}(x_{t}^{l-1}).\]

If \(x_{t}^{l}=\Theta(1)\) for all layers \(l\) holds in the model without residual connections, it also holds in the model with residual connections. At fixed depth, \(f_{t}^{l}=o(1)\) should be avoided, as it would hold that \(x_{t}^{l+1}=x_{t}^{l}\) in the infinite-width limit and the layer would be superfluous. The derivative of the activations becomes

\[dx_{t}^{l-1}=dx_{t}^{l}+dx_{t}^{l}\frac{\partial f_{t}^{l}}{\partial x_{t}^{l -1}},\]

where the second term stays the same as without the residual connection. For the example of \(f_{t}^{l}\) being a fully connected layer we get \(dx_{t}^{l-1}=dx_{t}^{l}+(W_{t}^{l})^{T}\left(dx_{t}^{l}\odot\phi^{\prime}(W_{t} ^{l}x_{t}^{l-1})\right)\). In this example, the derivative with respect to the weights becomes

\[\frac{\partial f_{t}}{\partial W_{t}^{l}}=dx_{t}^{l}\frac{\partial x_{t}^{l}}{ \partial W_{t}^{l}}=dx_{t}^{l}\frac{\partial f_{t}^{l}}{\partial W_{t}^{l}}=( dx_{t}^{l}\odot\phi^{\prime}(W_{t}^{l}x_{t}^{l-1}))(x_{t}^{l-1})^{T},\]

where the residual connection does not alter the functional dependence on \(dx_{t}^{l}\) and \(x_{t}^{l}\) compared to a MLP, but implicitly influences the weight gradient since \(dx_{t}^{l}\) and \(x_{t}^{l}\) are altered. As for the forward pass, the gradient scaling \(dx_{t}^{l}\) gets stabilized in the backward pass so that \(\frac{\partial f_{t}^{l}}{\partial x_{t}^{l-1}}\) is now allowed to be vanishing with width. Again, we are not aware of an architecture in which that would be desirable. Since a residual connection does not introduce learnable parameters, it interferes in \(\|v_{t}\|\) only implicitly through the stabilized gradients in earlier layers, which can contribute non-vanishingly to \(\|v_{t}\|\) even if later layers are wrongly scaled and their scaling is not adapted.

**Adam as a base optimizer.** When using Adam or similar adaptive optimizers as a base optimizer, the learning rate should scale as \(\Theta(1)\) for input-like layers and biases, and \(\Theta(n^{-1})\) for hidden and output layers (Yang et al., 2022). Yang et al. (2023b) provide proofs for arbitrary optimizers that perform generalized, nonlinear outer products. In the example of Adam, the update rule can be written as

\[\phi(u^{1}_{\alpha},\ldots,u^{k}_{\alpha},v^{1}_{\beta},\ldots,v^{k}_{\beta})= \sum_{i}\gamma_{i}u^{i}_{\alpha}v^{k}_{\beta}/\left(\sum_{i}\omega_{i}(u^{i}_{ \alpha}v^{i}_{\beta})^{2}\right)^{1/2},\]

where \(\gamma_{i},\omega_{i}\) are the weights that stem from the moving averages. By using a learning rate of \(n^{-1}\) and using the fact that both \(u\) and \(v\) have approximately iid coordinates of order \(\Theta(1)\), the law of large numbers yields \(\Theta(1)\) updates of the form

\[\frac{1}{n}\sum_{\beta=1}^{n}\phi(u^{1}_{\alpha},\ldots,u^{k}_{\alpha},v^{1}_ {\beta},\ldots,v^{k}_{\beta})x_{\beta}=\mathbb{E}\phi(u^{1}_{\alpha},\ldots,u ^{k}_{\alpha},Z^{v^{1}},\ldots,Z^{v^{k}})Z^{x}.\]

Any other learning rate scaling would either result in blowup or vanishing updates.

Adaptive optimizers have not been used for the ascent/perturbation step. In the descent/update step, nothing changes compared to unperturbed optimization as long as we ensure stable perturbations.

### Influence of width-dependent weight multipliers on \(bcd\)-parameterizations

Our definition of \(bcd\)-parameterizations is convenient because it purely adapts the learning algorithm but not the architecture. **We can also adapt the architecture by using layerwise width-dependent weight multipliers to effectively perturb all layers without any perturbation scaling.** The reason is that layerwise weight multipliers scale the layerwise gradients. Here, we study how the introduction of weight multipliers affects \(bcd\)-parameterizations.

In this section, we consider \(L\)-hidden layer MLPs with weight multipliers \(\{a_{l}\}_{l\in[L+1]}\), width \(n\in\mathbb{N}\), inputs \(\xi\in\mathbb{R}^{d_{n}}\), and with outputs \(f(\xi):=n^{-a_{L+1}}W^{L+1}x^{L}(\xi)\) where the activations \(x^{L}(\xi)\) are defined via the iteration

\[h^{1}(\xi):=n^{-a_{1}}W^{1}\xi,\qquad x^{l}(\xi):=\phi(h^{l}(\xi)),\qquad h^{ l+1}(\xi):=n^{-a_{l+1}}W^{l+1}x^{l}(\xi).\]

We define \(abcd\)-parameterizations in the same way as \(bcd\)-parameterizations, but instead of MLPs we use MLPs with weight multipliers \(\{a_{l}\}_{l\in[L+1]}\).

**Definition F.6** (\(abcd\)-parametrization).: An \(abcd\)-_parametrization_\(\{a_{l}\}_{l\in[L+1]}\cup\{b_{l}\}_{l\in[L+1]}\cup\{c_{l}\}_{l\in[L+1]} \cup\{d_{l}\}_{l\in[L+1]}\cup\{d\}\) defines the training of an MLP with weight multipliers \(\{a_{l}\}_{l\in[L+1]}\) with SAM in the following way:

1. Initialize weights iid as \(W^{l}_{ij}\sim\mathcal{N}(0,n^{-2b_{l}})\).
2. Train the weights using the SAM update rule with layerwise learning rates, \[W^{l}_{t+1}=W^{l}_{t}-\eta n^{-c_{l}}\nabla_{W^{l}}\mathcal{L}\left(f\left(\xi _{t};W_{t}+\bm{\varepsilon}_{t}\right),y_{t}\right),\] with the scaled perturbation \(\bm{\varepsilon}_{t}\) via layerwise perturbation radii, \[\bm{\varepsilon}_{t}:=\rho n^{-d}\frac{v_{t}}{\|v_{t}\|},\quad\text{with}\quad v _{t}=(v^{1}_{t},\ldots,v^{L+1}_{t}),\quad v^{l}_{t}:=n^{-d_{l}}\cdot\nabla_{W ^{l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t}),\] (LP)

W.l.o.g. we set \(\|v_{t}\|=\Theta(1)\), which prevents nontrivial width-dependence from the denominator. This imposes the constraints:

\[d_{1}+a_{1}\geq 1/2-c_{\nabla},\qquad d_{l}+a_{l}\geq 1-c_{\nabla},\qquad d_{L+1 }+a_{L+1}\geq 1/2,\]

with at least one equality required to hold, where \(l\in[2,L]\), and where \(\nabla_{x^{L}}f=n^{-a_{L+1}}W^{L+1}=\Theta(n^{-c_{\nabla}})\) with \(c_{\nabla}=\min(b_{L+1}+a_{L+1},c_{L+1}+2a_{L+1})\). The normalization \(v_{t}/\|v_{t}\|\) removes one degree of freedom from \(\{d_{l}\}_{l\in[L+1]}\) via the equivalence \(\{d^{l}_{l}\}_{l\in[L+1]}\cong\{d_{l}\}_{l\in[L+1]}\) iff there exists a \(C\in\mathbb{R}\) such that \(d^{l}_{l}=d_{l}+C\) for all \(l\in[L+1]\). 

#### f.6.1 \(abcd\)-equivalence classes

Update scalings behave as in SGD. The weight multiplier \(n^{-a_{l}}\) scales the gradient \(\nabla_{W^{l}}f\) by \(n^{-a_{l}}\). In the following forward pass, another multiplication of the weight updates with \(n^{-a_{l}}\) leads to the activation update scaling \(n^{-2a_{l}}\). This can be counteracted by adapting the learning rate scaling. For \(abc\)-parameterizations and SGD training, this induces the layerwise equivalence between parameterizations with \((a_{l},b_{l},c_{l})\) or with \((a_{l}+\theta_{l},b_{l}-\theta_{l},c_{l}-2\theta_{l})\). The extension of all of our results to Adam as a base optimizer is straightforward, since learning rate scalings and perturbation scalings are decoupled. For Adam, \(c_{l}\) should be adapted to \(c_{l}-\theta_{l}\).

Again, perturbations with joint gradient normalization complicate matters compared to SGD and Adam. Keeping the gradient norm scalings invariant under \(a_{l}\mapsto a_{l}+\theta\) would require \(d_{l}\mapsto d_{l}-\theta\), but keeping the activation perturbation scaling invariant would require \(d_{l}\mapsto d_{l}-2\theta\) as for updates. Consequently, an exact equivalence between \(abcd\)-parameterizations at finite width requires \(\theta\) to be the same for all layers and the conflicting gradient norm in the denominator and perturbation scaling in the numerator to be accounted for by adapting the global perturbation scaling \(d\mapsto d-\theta\) (together with \(d_{l}\mapsto d_{l}-\theta\)). In other words, (SAM) with layer-coupling gradient normalization (LP) does not have layerwise analytical equivalence classes at finite width. Below, we provide two alternative perturbation rules that resolve these complications and recover layerwise equivalence classes. The following lemma formally states the layer-coupled equivalence relation for the perturbation rule (LP). All proofs are provided at the end of this section.

**Lemma F.7** (\(abcd\)**-equivalence classes**).: _Let \(f_{t}(\xi)\) denote the output of a MLP in a stable \(abcd\)-parameterization with weight multipliers \(\{a_{l}\}_{l\in[L+1]}\) after \(t\) steps of training with the SAM update rule with layerwise perturbation scaling (LP) using a fixed sequence of batches and evaluated on input \(\xi\). Then for any \(\theta\in\mathbb{R}\) and any \(C\in\mathbb{R}\), \(f_{t}(\xi)\) stays fixed for all \(t\) and \(\xi\) if, for all \(l\in[L+1]\),_

\[(a_{l},b_{l},c_{l},d_{l},d)\text{ is reparameterized to }(a_{l}+\theta,b_{l}-\theta,c_{l} -2\theta,d_{l}-\theta+C,d-\theta).\]

**Remark F.8** (**Infinite-width equivalences**).: In the infinite-width limit, \(abcd\)-parameterizations remain equivalent under \((a_{l}+\theta_{l},b_{l}-\theta_{l},c_{l}-2\theta_{l},d_{l}-2\theta_{l},d)\) layerwise as long as the set of layers that contribute to the gradient norm non-vanishingly remains invariant. The gradient norm constraints for \(\|v^{l}\|=O(1)\) become

\[d_{1}+a_{1}\geq 1/2-c_{\nabla},\qquad d_{l}+a_{l}\geq 1-c_{\nabla},\qquad d_{L+ 1}+a_{L+1}\geq 1/2,\]

where \(\nabla_{x^{l}}f=n^{-a_{L+1}}W^{L+1}=\Theta(n^{-c_{\nabla}})\) with \(c_{\nabla}=\min(b_{L+1}+a_{L+1},c_{L+1}+2a_{L+1})\) remains invariant under equivalence transformations. 

**Remark F.9** (**SAM with layerwise gradient normalization**).: As the layer coupling is induced by the joint gradient normalization in the perturbations, layerwise gradient normalization simplifies the analysis. For (SAM) with layerwise gradient normalization (LN) of the perturbations global perturbation scaling \(d\) is superfluous, and there exist layerwise equivalence classes: For any \(\{\theta_{l}\}_{l\in[L+1]}\subset\mathbb{R}\),

\[(a_{l},b_{l},c_{l},d_{l})\text{ is equivalent to }(a_{l}+\theta_{l},b_{l}- \theta_{l},c_{l}-2\theta_{l},d_{l}-\theta_{l}).\]

To understand this equivalence, observe that any layerwise gradient scaling is cancelled out by the normalization \(\nabla_{W^{l}}\tilde{\mathcal{L}}/\|\nabla_{W^{l}}\mathcal{L}\|\). Only the \(n^{-a_{l}}\) factor from subsequent forward passes has to be counteracted. 

**Remark F.10** (**SAM with decoupled perturbation numerator and denominator scaling**).: A perturbation rule with joint gradient normalization and layerwise equivalence classes can be achieved by introducing even more hyperparameters and decoupling numerator and denominator scalings of each layer. For (SAM) with perturbations (DP) with layerwise perturbation radii \(\rho\cdot n^{-d_{l}}\) and separate gradient norm scaling \(n^{-\tilde{d}_{l}}\), global perturbation scaling \(d\) is superfluous, and there exist layerwise equivalence classes: For any \(\{\theta_{l}\}_{l\in[L+1]}\subset\mathbb{R}\),

\[(a_{l},b_{l},c_{l},d_{l},\tilde{d}_{l})\text{ is equivalent to }(a_{l}+\theta_{l},b_{l}- \theta_{l},c_{l}-2\theta_{l},d_{l}-2\theta_{l},\tilde{d}_{l}-\theta_{l}).\]

This perturbation rule also allows us to recover an analytical equivalence between trivial weight multipliers \(a_{l}=0\) for all \(l\), and any other weight multipliers. 

#### f.6.2 \(\mu P^{2}\) under non-trivial weight multipliers.

Our goal here is to find the weight multipliers that simplify the necessary perturbation scaling for effective perturbations in all layers as much as possible. The non-existence of layerwise equivalence classes in \(abcd\)-parameterizations from (LP) is not an issue if we are interested in effective perturbation properties and recovering \(\mu\)P\({}^{2}\) for arbitrary weight multipliers \(\{a_{l}\}_{l\in[L+1]}\), as the equivalence breaks due to varying gradient norm contributions, which are inconsequential for achieving effective perturbations.

As we aim to reproduce \(\mu\)P\({}^{2}\), we restrict ourselves to the \(\mu\)P equivalence class of \(abc\)-parameterizations. We do not allow layerwise perturbation scaling and are interested in the maximal stable choice of global perturbation scaling \(\rho n^{-d}\) to at least achieve non-vanishing perturbations in some layers. The following lemma shows even more: **The choice**

\[a_{l}=-1/2\cdot\mathbb{I}(l=1)+1/2\cdot\mathbb{I}(l=L+1)\]

**achieves effective perturbations in all layers with the naive (SAM) update rule with naive perturbation scaling \(\rho\cdot n^{0}\), and all layers contribute non-vanishingly to the joint gradient norm.** Hence this seems to be a natural choice of weight multipliers for SAM. However, it is in conflict with unit scaling considerations (Blake et al., 2024). Effectively, naive learning rate and perturbation scaling with these multipliers is equivalent to (DP) where all denominator terms are scaled to be width independent, as implemented by Algorithm 1, which resembles our implementation for ViTs. Our ablations in Appendix H.4 suggest that gradient norm contributions have a negligible effect on generalization performance.

**Lemma F.11** (**Naive perturbation scaling can effectively perturb all layers)**.: _Consider an \(abc\)-parameterization where \(\{(a_{l},b_{l},c_{l})\}_{l\in[L+1]}\) are chosen from the \(\mu\)P equivalence class, and where there is some \(C\in\mathbb{R}\) such that \(d_{l}=C\) for all \(l\in[L+1]\). This reduces to training a MLP with weight multipliers with (SAM) with global perturbation scaling \(\rho n^{-d}\) for some \(d\in\mathbb{R}\). Effective perturbations in all layers are achieved and all layers contribute non-vanishingly to the gradient norm if and only if_

\[a_{1}=-d-1/2,\qquad a_{l}=-d\quad\text{ for }l\in[2,L],\qquad a_{L+1}=-d+1/2.\]

Achieving \(\mu\)P\({}^{2}\) with the current implementation of the mup-package requires both an adaptation of the architecture and of the learning algorithm, as the following lemma shows. Hence the package is not particularly suited for SAM learning in \(\mu\)P\({}^{2}\) when the goal is simple perturbation scaling.

**Lemma F.12** (**Effective perturbations with the mup-package)**.: _Consider an \(abcd\)-parameterization where \(\{(a_{l},b_{l},c_{l})\}_{l\in[L+1]}\) are chosen from the \(\mu\)P equivalence class, and with the weight multipliers \(a_{L+1}=\mathbb{I}(l=L+1)\) as in the mup-package._

1. _(mup-package global scaling effectively perturbs hidden layers) Under global scaling_ \(d_{l}=C\)_,_ \(C\in\mathbb{R}\)_, for all_ \(l\in[L+1]\)_, maximal stable perturbations are achieved with_ \(d=0\)_. In this parameterization, hidden layers are effectively perturbed, but input and output layers are not effectively perturbed._
2. _(\(\mu\)P\({}^{2}\) with the mup-package) Effective perturbations in all layers are achieved with the choice_ \(d=d_{1}=d_{L+1}=-1/2\) _and_ \(d_{l}=1/2\) _for_ \(l\in[2,L]\)_._

The following lemma covers the general case how to achieve \(\mu\)P\({}^{2}\) given arbitrary weight multipliers.

**Lemma F.13** (\(\mu\)P\({}^{2}\) with arbitrary weight multipliers).: _Consider an \(abcd\)-parameterization where \(\{(a_{l},b_{l},c_{l})\}_{l\in[L+1]}\) are chosen from the \(\mu\)P equivalence class. Then effective perturbations in all layers are achieved with the choice \(d=\min_{l\in[L+1]}(-a_{l}-1/2\mathbb{I}(l=1)+1/2\mathbb{I}(l=L+1))\), and_

\[d_{1}=-1-d-2a_{1},\qquad d_{l}=-d-2a_{l},\text{ for }l\in[2,L],\qquad d_{L+1}=1-d-2a_{L+1}.\]

The following lemma shows that weight multipliers that achieve \(\mu\)P\({}^{2}\) with naive perturbation scaling under perturbations with layerwise normalization (LN) are exactly the same as the ones for (LP).

**Lemma F.14** ((LN) **with naive perturbation scaling can effectively perturb all layers)**.: _Consider (SAM) with layerwise normalization (LN). Assume \(\{(a_{l},b_{l},c_{l})\}_{l\in[L+1]}\) are chosen from the \(\mu\)P equivalence class, and assume there is some \(C\in\mathbb{R}\) such that \(d_{l}=C\) for all \(l\in[L+1]\). Then all layers are effectively perturbed if the multipliers are chosen as_

\[a_{1}=-1/2-C,\qquad a_{l}=-C,\qquad a_{L+1}=1/2-C.\]

Proof of Lemma F.14.: As derived in Appendix F.7, under \(a_{l}=0\) for all \(l\in[L+1]\), all layers are effectively perturbed if and only if \(d_{l}=-1/2\cdot\mathbb{I}(l=1)+1/2\cdot\mathbb{I}(l=L+1)\). Now we can exploit the layerwise equivalence relation to enforce \(d_{l}=C\) in each layer by adapting all \(a_{l}\). 

Proof of Lemma F.11.: In general, in the \(abc\)-equivalence class of \(\mu\)P, the \(l\)-th layer's gradient norm is scaled by \(n^{-a_{l}}\). This induces the generalized gradient norm constraints for \(\|\nabla_{W}L\|=\Theta(1)\),

\[d_{1}+a_{1}\geq-1/2,\qquad d_{l}+a_{l}\geq 0,\qquad d_{L+1}+a_{L+1}\geq 1/2.\]Effective perturbations are achieved when \(\rho n^{-d-d_{l}-a_{l}}\nabla_{W^{l}}L=\Theta(n^{-\mathbb{I}(l>1)})\), which induces the perturbation stability constraints

\[d+d_{1}+2a_{1}\geq-1,\qquad d+d_{l}+2a_{l}\geq 0,\qquad d+d_{L+1}+2a_{L+1}\geq 1,\]

with effective perturbations whenever the equality of the respective layer holds.

Under global scaling, the gradient norm constraints become, for some \(C\in\mathbb{R}\),

\[C+a_{1}\geq-1/2,\qquad C+a_{l}\geq 0,\qquad C+a_{L+1}\geq 1/2,\]

and the conditions for effective perturbations become

\[d+C+2a_{1}\geq-1,\qquad d+C+2a_{l}\geq 0,\qquad d+C+2a_{L+1}\geq 1.\]

As \(d+C\) is a common term in all layers, we get the relations \(a_{l}=a_{1}+1/2\), \(a_{L+1}=a_{1}+1\), so that all gradient norm constraints are simultaneously satisfied with \(C=-a_{l}\) and effective perturbations are achieved in all layers with \(d=-a_{l}\). 

Proof of Lemma f.12.: Under the choice \(a_{l}=\mathbb{I}(l=L+1)\), the gradient norm constraints become

\[d_{1}\geq-1/2,\qquad d_{l}\geq 0,\qquad d_{L+1}\geq-1/2,\]

and the conditions for effective perturbations become

\[d+d_{1}\geq-1,\qquad d+d_{l}\geq 0,\qquad d+d_{L+1}\geq-1.\]

Proof of (a):

Satisfying the gradient norm constraints with global scaling requires \(d_{l}=0\) for all \(l\in[L+1]\), then the minimal stable choice of \(d\) is \(d=0\) which only effectively perturbs hidden layers.

Proof of (b):

The choice \(d=-1/2\) and \(d_{1}=-1/2\) saturates the gradient norm constraint and achieves effective perturbations in the input layer. Then the choice \(d_{l}=1/2\) and \(d_{L+1}=-1/2\) satisfies the gradient norm constraints and achieves effective perturbations in all layers. 

Proof of Lemma f.7.: To understand the influence of weight multipliers on updates and perturbations, first note that under an equivalence transformation of all \(abcd\)-parameters w.l.o.g from \(a_{l}=0\) for all \(l\in[L+1]\), the scalings of \(h^{l},x^{l}\) and of \(n^{-a_{l}}W^{l}\) remain invariant. This implies that the scalings of \(\nabla_{x^{L}}f=n^{-a_{L+1}}W^{L+1}\), \(\nabla_{h^{l}}f=\nabla_{x^{l}}f\odot\phi^{\prime}(h^{l})\) and \(\nabla_{x^{l}}f\) for all \(l\in[L]\) also remain invariant. Hence the weight gradients, \(\nabla_{W^{L+1}}f=n^{-a_{L+1}}x^{L}\) and \(\nabla_{W^{l}}f=n^{-a_{l}}\nabla_{h^{l}}f\cdot(x^{l-1})^{\top}\) are scaled by \(n^{-a_{l}}\) in each layer.

In the following forward pass, we get

\[h^{l}=n^{-a_{l}}(W^{l}+\Delta W^{l})x^{l-1}=n^{-a_{l}}(W^{l}-\eta n^{-c_{l}} \nabla_{W^{l}}\mathcal{L})x^{l-1},\]

so that activation/output updates and perturbations of layer \(l\) are scaled by \(n^{-2a_{l}}\).

Again, a complication compared to SGD or Adam arises through the gradient normalization of SAM's weight perturbation. If the gradients are simply normalized layerwise \(\boldsymbol{\varepsilon}^{l}=\rho\cdot n^{-d_{l}}\cdot\nabla_{W^{l}}\mathcal{ L}/\|\nabla_{W^{l}}\mathcal{L}\|\), the \(n^{-a_{l}}\)-term from the backward pass cancels out, and only in the forward pass we get a scaling \(n^{-a_{l}}\). Hence an exact layerwise equivalence still exists for SAM with layerwise gradient normalization:

\[(a_{l},b_{l},c_{l},d_{l})\text{ is equivalent to }(a_{l}+\theta_{l},b_{l}-\theta_{l},c_{l}-2\theta_{l},d_{l}-\theta_{l}).\]

Under joint gradient normalization (SAM), as we consider in our definition of \(bcd\)-parameterizations, keeping the gradient norm scalings invariant under \(a_{l}\mapsto a_{l}+\theta\) would require \(d_{l}\mapsto d_{l}-\theta\), but keeping the perturbation scaling invariant would require \(d_{l}\mapsto d_{l}-2\theta\) as for updates. Consequently, due to the layer coupling of joint gradient normalization \(\|\nabla_{\mathbf{W}}\mathcal{L}\|\), an exact equivalence between \(abcd\)-parameterizations at finite width requires \(\theta\) to be the same for all layers and the conflicting gradient norm in the denominator and perturbation scaling in the numerator to be accounted for by \(d_{l}\mapsto d_{l}-\theta\) and \(d\mapsto d-\theta\).

In the infinite-width limit, \(abcd\)-parameterizations remain equivalent under \((a_{l}+\theta_{l},b_{l}-\theta_{l},c_{l}-2\theta_{l},d_{l}-2\theta_{l},d)\) layerwise as long as the set of layers that contributes to the gradient norm non-vanishingly remains invariant. The gradient norm constraints for \(\|v^{l}\|=O(1)\) become

\[d_{1}+a_{1}\geq 1/2-c_{\nabla},\qquad d_{l}+a_{l}\geq 1-c_{\nabla},\qquad d_{L+1} +a_{L+1}\geq 1/2,\]

where \(\nabla_{x^{L}}f=n^{-a_{L+1}}W^{L+1}=\Theta(n^{-c\nabla})\) with \(c_{\nabla}=\min(b_{L+1}+a_{L+1},c_{L+1}+2a_{L+1})\) remains invariant under equivalence transformations. 

Proof of Lemma f.13.: First, the choices of \(d_{l}\) ensure that the constraints for effective perturbations from the proof of Lemma f.11 are saturated in each layer. It is left to show, that these choices satisfy the \(\|\nabla_{W}L\|=\Theta(1)\)-constraints. For input layers, since \(-d\geq a_{1}+1/2\), it holds that \(d_{1}+a_{1}\geq-1/2\). For hidden layers, since \(-d\geq a_{l}\), it holds that \(d_{l}+a_{l}\geq 0\). For output layers, since \(-d\geq a_{L+1}-1/2\), it holds that \(d_{L+1}+a_{L+1}\geq 1/2\). Observe that the minimizer in the definition of \(d\) saturates its gradient norm constraint. 

### The spectral perspective on \(\mu P^{2}\)

While Tensor Programs allow to track the transformations of vectors like activations, Yang et al. (2023a) provide an equivalent formulation in terms of weight matrix spectral norms. They find that the spectral norm measures the effect of a weight update on the activations, under certain non-cancellation assumptions and limited batch size. For all MLP layers, they show that \(\mu\)P is equivalent to achieving the condition

\[\|W_{t}^{l}\|_{*}=\Theta\left(\sqrt{\tfrac{\texttt{fan\_out}}{\texttt{fan\_ in}}}\right)\quad\text{and}\quad\|\Delta W_{t}^{l}\|_{*}=\Theta\left(\sqrt{\tfrac{ \texttt{fan\_out}}{\texttt{fan\_in}}}\right),\]

at all times \(t\), where \(W_{t}^{l}:\mathbb{R}^{\texttt{fan\_in}}\rightarrow\mathbb{R}^{\texttt{fan\_ out}}\). This condition is achieved with initialization \(\sigma_{l}\), SGD learning rate \(\eta_{l}\) and Adam learning rate \(\eta_{l}^{Adam}\) chosen as,

\[\sigma_{l}=\Theta\left(\frac{1}{\sqrt{\texttt{fan\_in}}}\min\left\{1,\sqrt{ \tfrac{\texttt{fan\_out}}{\texttt{fan\_in}}}\right\}\right),\quad\eta_{l}= \Theta\left(\frac{\texttt{fan\_out}}{\texttt{fan\_in}}\right),\quad\eta_{l}^ {Adam}=\Theta\left(\frac{1}{\texttt{fan\_in}}\right).\]

This generalizes \(\mu\)P to varying widths inside the network. For varying widths, we adopt the notation \(W^{l}:\mathbb{R}^{n_{l-1}}\rightarrow\mathbb{R}^{n_{l}}\) with \(n_{0}=d_{in}\) and \(n_{L+1}=d_{out}\), whereas \(\texttt{fan\_in}\) and \(\texttt{fan\_out}\) always adapt to the weight matrix under consideration.

To understand why the spectral norm is desirable, note that \(\Delta W^{l}=\eta_{l}\nabla_{h^{l}}\mathcal{L}(x^{l-1})^{\top}\) is low rank and aligned with the incoming activations. For batch size \(1\), we even have rank-1 updates with \(\|\Delta W^{l}\|_{*}=\eta_{l}\|\nabla_{h^{l}}\mathcal{L}\|_{2}\|x^{l-1}\|_{2}\), aligned with the incoming activations \(x^{l-1}\), hence \(\|\Delta W^{l}x^{l-1}\|_{2}=\|\Delta W^{l}\|_{*}\|x^{l-1}\|_{2}\). This allows to achieve \(\|\Delta x^{l}\|_{2}=\Theta(\sqrt{n_{l}})\) irrespective of the layer type with \(\|\Delta W_{t}^{l}\|_{*}=\Theta(\sqrt{n_{l}/n_{l-1}})\).

Our simple condition that perturbations should scale like updates, which is rigorously justified by our Tensor Program based proof in Appendix E, now allows to derive the correct perturbation scalings using the spectral weight perspective.

**Layerwise perturbations.** As a simple starting point, consider a variant of (SAM) that does not globally normalize the gradient of all layers jointly, but uses layerwise normalization (LN), resulting in the layerwise perturbation rule,

\[\epsilon^{l}=\rho_{l}\cdot\nabla_{W^{l}}\mathcal{L}/\|\nabla_{W^{l}}\mathcal{ L}\|,\]

where \(\|\cdot\|\) may denote either the spectral or the Frobenius norm (equivalent under limited perturbation batch size). Without the global normalization, the scalings of all layers are not coupled, and the spectral condition \(\|\epsilon^{l}\|_{*}=\Theta(\sqrt{\texttt{fan\_out}/\texttt{fan\_in}})\) immediately requires choosing

\[\rho_{l}=\rho\cdot\sqrt{\texttt{fan\_out}/\texttt{fan\_in}}\]

for effective perturbations in layer \(l\) with width-independent hyperparameter \(\rho\geq 0\).

**Perturbations with global gradient normalization.** Perturbations that are globally normalized across layers have usually been implemented practice according to the GitHub repositories provided by Foret et al. (2021); Samuel (2022); Kwon et al. (2021); Andriushchenko and Flammarion (2022); Muller et al. (2024). Since we are interested in analysing (SAM) as it is applied in practice, we study variants with joint gradient normalization in more detail. Preliminary ablations in Appendix H.5 suggest that layer-coupled SAM with global normalization slightly outperforms SAM with layerwise gradient normalization. To simplify the analysis as much as possible, we will first ensure width-independence of the normalization, so that the layerwise perturbation scaling is not affected by the normalization term. Then, layerwise perturbations should again be scaled like updates.

**Separate denominator scalings.** If we allow to scale each denominator term separately from the corresponding numerator term (DP), the perturbation radius in each layer for the numerator can be scaled like updates, \(\rho_{l}=\Theta\left(\frac{\texttt{fan\_out}}{\texttt{fan\_in}}\right)\).

Now, to ensure \(\Theta(1)\) in the denominator, each input and hidden-like gradient norm \(\|\nabla_{W^{l}}\mathcal{L}\|_{F}\), \(l\in[L]\), achieves width-independence if it scaled by \(\sqrt{\texttt{fan\_out}/\texttt{fan\_in}}\). The same rule applies to biases when understanding them as weights \(\mathbb{R}\rightarrow\mathbb{R}^{n_{l}}\) to the input \(1\). These scalings are derived in the next paragraph. The last-layer gradient norm \(\|\nabla_{W^{L+1}}\mathcal{L}\|_{F}\) should be scaled as \((n_{L}n_{L+1})^{-1/2}\), and \(\|\nabla_{b^{L+1}}\mathcal{L}\|_{F}\) as \(n_{L+1}^{-1/2}\).

If we care about the correct width-independent constants, observe that the learning rate scaling \(\eta_{L+1}=\Theta(\texttt{fan\_out}/\texttt{fan\_in})\) induces \(\|W^{L+1}\|=\|\Delta W^{L+1}\|=\Theta(\sqrt{n_{L+1}^{3}/n_{L}})\). If we wanted to achieve \(\Delta W^{L+1}=\Theta(\sqrt{n_{L+1}/n_{L}})\) we would need \(\eta_{L+1}=\Theta(1/\texttt{fan\_in})\). As \(n_{L+1}=d_{out}\) is width-independent, \(\sqrt{\texttt{fan\_out}/\texttt{fan\_in}}\) would result in the same width-dependent scaling for the last layer, but ignoring large constants can introduce a significant width-independent spectral distortion. For example in ImageNet1K, \(n_{L+1}\) is large. By tuning input, hidden and output multipliers such constant distortions may be corrected. The multiplier used in the mup-package does not correct this distortion. Using a base width at which SP is recovered may also cement such spectral distortions, if no multipliers are tuned.

**Derivation of gradient norm \(\|\nabla_{W^{l}}\mathcal{L}\|_{F}\) scalings.** In this paragraph, \(\|\cdot\|\) may denote the Frobenius or spectral norm. As all matrices are of limited rank, both norms scale equivalently. As a first step, \(\|\nabla_{h^{l}}\mathcal{L}\|=\Theta(\frac{1}{\sqrt{n_{l}}})\) can be reconstructed from

\[\Theta(\sqrt{n_{l}/n_{l-1}})=\|\Delta W^{l}\|_{*}=\eta_{l}\|\nabla_{h^{l}} \mathcal{L}\|\|x^{l-1}\|_{2}=n_{l}/n_{l-1}\cdot\sqrt{n_{l-1}}\|\nabla_{h^{l}} \mathcal{L}\|.\]

Now, for input and hidden layers, \(\|\nabla_{W^{l}}\mathcal{L}\|=\|\nabla_{h^{l}}\mathcal{L}\|\|x^{l-1}\|_{2}= \Theta(\sqrt{n_{l-1}/n_{l}})\). Multiplying by the inverse yields width-independent scaling. The output layer gradient \(\nabla_{W^{L+1}}\mathcal{L}\in\mathbb{R}^{n_{L+1}\times n_{L}}\) is given by \((\nabla_{W^{L+1}}\mathcal{L})_{ij}=x_{j}^{L}=\Theta(1)\), so that \(\|\nabla_{W^{L+1}}\mathcal{L}\|=\Theta(\sqrt{n_{L}n_{L+1}})\). Biases before the last layer follow the scheme \(\|\nabla_{b^{l}}\mathcal{L}\|=\|\nabla_{h^{l}}\mathcal{L}\|=\Theta(\sqrt{1/n_ {l}})=\Theta(\sqrt{\texttt{fan\_in}/\texttt{fan\_out}})\). The last layer bias \(\|\nabla_{b^{L+1}}\mathcal{L}\|=\sqrt{n_{L+1}}\) scales width-independently as it should, but needs to be scaled by a different constant \(1/\sqrt{\texttt{fan\_out}}\) than earlier layers.

**Extensions to ASAM.** As ASAM cannot be written as a \(\texttt{Ne}\otimes\texttt{or}\top\)program, its scaling can only be derived heuristically. As provided in Table 1 and derived in Appendix F.4, elementwise ASAM scales all layer types correctly in relation to each other, and it suffices to rescale the global perturbation radius by \(\sqrt{n_{L}}\), assuming all width dimensions scale proportionally. For SAM-ON, we only perturb input-like layers such as normalization layers. As the conditions for correct scaling remain the same, the above scalings for input layers in SAM also apply to SAM-ON.

For layerwise ASAM, first note that \(\|W^{l}_{t}\|_{F}=\Theta(\|W^{l}_{0}\|_{F})=\Theta(\sqrt{n_{l}})\) for input and hidden layers \(l\in[L]\). As the numerator contains \(\|W^{l}_{t}\|_{F}^{2}\), it requires the layerwise perturbation scaling \(\frac{1}{\texttt{fan\_in}}\). In the denominator, width independence is achieved with the multiplier \(\sqrt{\frac{1}{\texttt{fan\_in}}}\), since \(\|W^{l}\|_{F}\|\nabla_{W^{l}}\mathcal{L}\|_{*}=\sqrt{n_{l}}\sqrt{\frac{n_{l-1 }}{n_{l}}}=\sqrt{n_{l-1}}\). Again, the output layer requires a special treatment. Due to its small initialization, it holds that \(\|W^{L+1}\|_{F}^{2}=\|\Delta W^{L+1}\|_{F}^{2}=\Theta(\frac{n_{L+1}^{3}}{n_{L }})\). For perturbations that fulfill the spectral condition \(\rho_{L+1}\|W^{L+1}\|_{F}^{2}\|\nabla_{W^{l+1}}\mathcal{L}\|_{*}=\Theta(\sqrt{ \frac{n_{L+1}}{n_{L}}})\), we need to choose \(\rho_{L+1}=\rho\cdot\frac{1}{n_{L+1}^{3}}\) (width-independent, but very small). The last-layer denominator term scales as \(\|W^{L+1}\|_{F}\|\nabla_{W^{l+1}}\mathcal{L}\|_{*}=\Theta(\sqrt{\frac{n_{L+1} ^{3}}{n_{L}}}\cdot\sqrt{n_{L}n_{L+1}})=\Theta(n_{L+1}^{2})\), which is width independent, but can be a large constant, as for ImageNet1K. The output bias numerator exactly conforms with the correct scaling \(\|\nabla_{b^{L+1}}\mathcal{L}\|_{F}^{2}=n_{L+1}=\texttt{fan\_out/fan\_in}=\Theta(1)\).

Note that weight decay may break statements like \(\|W_{t}^{t}\|_{F}=\Theta(\|W_{0}^{l}\|_{F})\) over long training. Everett et al. (2024) have recently observed more generally that scalings may evolve differently over long training than predicted by pure infinite-width TP theory, because alignments evolve dynamically between CLT- and LLN-like behaviour.

**Using the \(\mathtt{mup}\)-package.** The \(\mathtt{mup}\)-package introduces the output layer weight multiplier \(n_{L}^{-1}\) so that input and output layer learning rates may be scaled by the same width-dependent factor. Hence, only the last-layer scalings change. The scalings of \(n_{L}^{-1}W^{L+1}\) and \(n_{L}^{-1}\Delta W^{L+1}\) remain the unique ones that achieve \(\mu\)P, but \(\nabla_{W^{L+1}}\mathcal{L}\) is scaled by \(n_{L}^{-1}\). This requires adapting the last-layer learning rate \(\eta_{L+1}\) to scale like input layers. For SAM, the last-layer perturbation radius can now be scaled like input layers. That is, assuming proportionally growing width \(n\), in the numerator \(\rho_{L+1}=\rho_{1}=\rho\cdot n\) and \(\rho_{l}=\rho\) for \(l\in[2,L]\), and the gradient norm contributions should be scaled by \(\sqrt{n}\) for input and output layers, and by \(1\) for hidden layers. The Tensor Program perspective on weight multipliers can be found in Appendix F.6. The correct width-independent constants are achieved with the last-layer numerator scaling \(\rho_{L+1}=\rho\cdot n_{L}\) and the last-layer denominator scaling \(\sqrt{n_{L}/n_{L+1}}\), since \(\nabla_{W^{L+1}}\mathcal{L}=\Theta(\sqrt{n_{L+1}/n_{L}})\) and for the numerator we get an additional \(n_{L}^{-1}\) in the forward pass.

For SAM-ON nothing changes, as only input-like layers are perturbed. For elementwise ASAM, ignoring width-independent constants, nothing changes as the weight multiplier \(n_{L}^{-1}\) increases the weight scaling \(W^{L+1}\) and decreases the gradient scaling \(\nabla_{W^{L+1}}\mathcal{L}\) by the same amount. The additional \(n_{L}^{-1}\)-factor in the numerator is cancelled out by the additional \(W^{L+1}\)-factor. For the correct width-independent constants with decoupled numerator and denominator scaling, we would scale the denominator by \(\sqrt{n_{L}/n_{L+1}^{3}}\) with or without weight multiplier, and scale the numerator by \(\rho_{L+1}=\rho\cdot n_{L}/n_{L+1}^{2}\) with or without weight multiplier. For the example of layerwise ASAM, we still get for the denominator \(\|W^{L+1}\|_{F}\|\nabla_{W^{L+1}}\mathcal{L}\|_{*}=\Theta(n_{L+1}^{2})\), again because the weights \(W^{L+1}\) are scaled up by \(n_{L}\) and the gradient is scaled down by the same amount. In the numerator, the upscaling of the weights also cancels out the downscaling of the gradient and additional \(n_{L}^{-1}\) in the subsequent forward pass, leading to an unchanged \(\rho_{L+1}=\rho\cdot n_{L+1}^{-3}\), which is width-independent but potentially leads to numerical issues.

**Code for \(\mu\)P\({}^{2}\) with separate denominator scalings.** Algorithm 1 provides a PyTorch code example that implements the above \(\mu\)P\({}^{2}\) scalings for SAM, scaling the gradient norm contributions of all layers to \(\Theta(1)\) (equivalent to (\(a\)-\(\mu P^{2}\)) together with naive perturbation and learning rate scaling). We adapt the popular SAM implementation Samuel (2022) using the \(\mathtt{mup}\)-package. This code resembles our implementation for the ViT experiments. In the \(\mathtt{mup}\)-package,'vector-like' parameters scale as \(n\times\text{constant}\) or \(\text{constant}\times n\) and include input and output weights. The last-layer multiplier \(n_{L}^{-1}\) is chosen so that input and output layers can be scaled by the same width-dependent factor. On the other hand,'matrix-like' parameters scale as \(n\times n\) and include hidden weights. The implementation uses a base width at which \(\mu\)P\({}^{2}\) and SP are equivalent; all width-dependent scalings then scale with width-multipliers \(\texttt{width}/\texttt{base\_width}\). This allows to immediately transfer well-performing settings from SP to \(\mu\)P\({}^{2}\).

Let us recapitulate how the \(\mu\)P\({}^{2}\) scaling in the following code arises. The crucial variables to track are factor, group["rho"] and group["gradnorm_scaling"]. For limited batch size, the spectral and Frobenius norm of gradients scale equivalently, and we get, for all \(l\in[L]\),

\[\|\nabla_{W^{l}}\mathcal{L}\|_{F}=\Theta(\|\nabla_{W^{l}}\mathcal{L}\|_{*})= \Theta\left(\sqrt{\frac{\texttt{fan\_in}}{\texttt{fan\_out}}}\right).\]

We want to scale each weight's contribution in the denominator to be width-independent, hence need the factor \(\sqrt{\texttt{factor}}\) with factor \(=\texttt{fan\_out/fan\_in}\). For the numerator, the spectral condition (\(*\)) demands \(\|\rho_{l}\cdot\nabla_{W^{l}}\mathcal{L}\|_{*}^{\frac{l}{2}}\equiv\Theta(\sqrt{ \frac{\texttt{fan\_out}}{\texttt{fan\_in}}})\), so that we need to scale the weight's perturbation radius to \(\rho_{l}=\rho\cdot\texttt{factor}\). Since the \(\mathtt{mup}\)-package sets the last-layer weight multiplier such that input and output layers can be scaled in the same way, the implementation is short. For optimal numerical properties however, this choice of multipliers is sub-optimal (Blake et al., 2024).

```
1importmath,torch
2frommupimportMuAdamW
3
4#specifyparameterization
5parameterization='mupp'#'sp-naive','mup-naive'
6#for'mup-global'use'mup-naive'andscalerhoacordingly
7
8#specifymodelandhyperparameters
9model,lr,rho,weight_decay,last_layer_weight_name=...
10
11
12
13#adaptSAMttoallowgradientnormscalingofeachweighttensor
14classSAM(torch.optim.Optimizer):
15...
16
17defgrad_norm(self):
18grads=[]
19fori,groupinenumerate(self.param_groups):
20forpingroup["params"]:
21grads.append((group["gradnorm_scaling"]*p.grad).norm(p=2))
22norm=torch.stack(grads).norm(p=2)
23returnnorm
24
25@torch.no_grad()
26deffirst_step(self):#perturbationstepbeforetheweightupdate
27grad_norm=self.grad_norm()
28forgroupinself.param_groups:
29scale=group["rho"]/(grad_norm+1e-12)
30forpingroup["params"]:
31ifp.gradisNone:continue
32self.state[p]["old_p"]=p.data.clone()
33e_w=p.grad*scale.to(p)
34
35p.add_(e_w)#climbtothelocalmaximum"w+e(w)"
36
37
38
39#setwidth-dependentrhondgradientnormscalingforeachweight
40param_groups=[]
41forname,pinmodel.named_parameters():
42ifp.infshape.ninf()==0or'naive'inparameterization:
43factor=1
44elifp.infshape.ninf()==1:
45#vector-like
46fordinp.infshape:
47ifd.base_dimisnotNone:
48factor=d.dim/d.base_dim#width
49break
50elifp.infshape.ninf()==2:
51#matrix-like
52factor=(p.infshape[0].dim/p.infshape[1].dim)*(p.infshape[1].base_dim/p.infshape[0].base_dim)#fan_out/fan_in
53else:
54raiseNotImplementedError
55
56group={
57"params":[p],
58"lr":lr,
59"rho":rho"&factor,
60"gradnorm_scaling":math.sqrt(factor),
61}
62param_groups.append(group)optimizer=SAM(param_groups, base_optimizer=MuAdamWifparameterization=='mup'elsetorch.optim.AdamW,weight_decay=weight_decay) ```

Algorithm 1: Pytorch implementation of \(\mu\)P\({}^{2}\) for SAM using the mup-package. Key changes from the original implementation that correct the layerwise perturbation scaling are highlighted with gray boxes. This code decouples the scalings of numerator and denominator terms following (DP), and scales the gradient norm contributions of all layers by group["gradnorm_scaling"] in the denominator to be width-independent. The numerator terms group["rho"] of all weight tensors are scaled to achieve effective perturbations. This scaling is equivalent to (\(a\)-\(\mu P^{2}\)) together with naive perturbation and learning rate scaling.

## Appendix G Experimental details

If not mentioned otherwise, experiments use the settings specified in this section.

**Implementation details.** For MLPs, we exactly implement our Definition 4 of \(bcd\)-parameterizations to precisely validate our theoretical results. For ResNets and ViTs, the width varies inside the network, so that we implement the spectral scaling rules derived in Appendix F.7. Like the mup-package, we introduce a base width at which SP and \(\mu\)P are equivalent, allowing to immediately transfer setups that perform well in SP. We use the mup-package only for ViTs, and our implementation of \(\mu\)P\({}^{2}\) resembles the pseudocode provided in Algorithm 1. For ResNets, we use no width-dependent last-layer multiplier. At initialization, \(\mu\)P differs from SP only through a smaller last layer initialization. For MLPs we exactly implement the \(bcd\)-parameterization with \(b_{L+1}=1\), but use the large width-independent input layer initialization variance \(2\) instead of the width-independent \(2/d_{in}\) in \(\mu\)P, which can be seen as a tuned initialization variance multiplier. For ResNets and Vits, we initialize the last layer to \(0\) in \(\mu\)P, which corresponds to \(b_{L+1}\rightarrow\infty\) and which recovers the limit behaviour \(f_{0}\to 0\) already at finite width. We are working on making Python code to reproduce all of our experiments publicly available.

**MLPs.** We train 3-layer MLPs without biases with ReLU activation function for \(20\) epochs with constant learning rate, using SGD as base optimizer as specified in Definition 4, but allow for SGD batchsize larger than \(1\), defaulting to batch size \(64\). We evaluate the test accuracy after every epoch and use the snapshot across training with the best accuracy. This is necessary as the test accuracy is not monotonically increasing across training, while the training accuracy is. For ResNets we do not observe such harmful overfitting. For the standard parametrization, we use He initialization (He et al., 2015) and don't tune multipliers to mimic standard training procedures. For \(\mu\)P, we resort to the optimal multipliers from Yang et al. (2022). We then find the optimal learning rate and perturbation radius for each \(bcd\)-parametrization and SAM variant separately.

**ResNets.** For ResNet18 experiments, we augment the CIFAR10 data with random crops and random horizontal flips, set labelssmoothing to \(0.1\) and use a cosine learning rate schedule. ResNets in \(\mu\)P have base width \(0.5\), gradient norm scaling according to Definition 4 and their last layer is initialized to \(0\). For SP, we again adopt the standard hyperparameters from Muller et al. (2024) by using a momentum of \(0.9\), weight decay \(0.0005\), an output multiplier of \(1.0\), and individually tuned learning rate and perturbation radius for each SAM variant. For \(\mu\)P, at base width multiplier \(0.5\) compared to the original width, for each SAM variant, we perform a random grid search over the hyperparameters learning rate, perturbation radius, output multiplier \([2^{-8},2^{-7},\dots,2^{8}]\), weight decay \([0,10^{-5},10^{-4},5\cdot 10^{-4},10^{-3},10^{-2}]\) and momentum \([0,0.1,0.4,0.7,0.9]\). Learning rate and perturbation radius grids were either set to \([2^{-10},2^{-9},\dots,2^{1}]\) or centered around recommendations from the literature. The optimal hyperparameter configurations found from at least \(150\) runs for each SAM variant are summarized in Table G.1. Learning rates and perturbation radii were further tuned with the experiments from Appendix H.3.3.

**ViTs.** We train ViT-S/16 with 6 layers and 12 attention heads on ImageNet1K (Deng et al., 2009) and a ViT-S/4 with 12 layers and 12 attention heads on CIFAR100 (Krizhevsky et al., 2009) (see Appendix H.6), again adopting the hyperparameter settings from Muller et al. (2024). This means we use AdamW as a base optimizer with warmup and a cosine learning rate decay. For CIFAR100, we use random crops, random horizontal flips and AutoAugment as data augmentations. For Imagenet we use the original preprocessing from Huggingface vit-base-patch16-224 (Wu et al., 2020). For \(\mu\)P, we tune multipliers at a basewidth \(384\), initialize the last layer and query weights to \(0\). By

[MISSING_PAGE_FAIL:52]

Supplemental experiments

This section provides more extensive empirical evaluations to validate the claims of the main paper. By naive perturbation scaling (naive) we denote parameterizations that do not adapt any perturbation scalings (\(d=d_{l}=0\) for all \(l\)). Global perturbation scaling (global) denotes the maximal stable scaling \(n^{-d}\) of the global perturbation radius that achieves effective perturbations in some layers without layerwise perturbation scaling (\(d_{l}=0\) for all \(l\)).

### SAM is approximately LL-SAM in \(\mu\)P with global perturbation scaling

Figure 11 compares SAM in \(\mu\)P under global perturbation scaling (\(\mu\)P-global) with SAM under global perturbation scaling where only the last-layer weights are perturbed (LL-SAM) by showing more neural network statistics that are related to SAM's inductive bias and to learning in general. From top-left to bottom right, the statistics are: Frobenius norm of the layerwise weight perturbation (which is closely related to spectral norm as perturbations are low rank); Frobenius norm of the layerwise weight perturbation normalized by the weight spectral norm to upper bound the influence of the perturbations on the output; spectral norm of the weight updates across training scaled by the spectral condition \(n^{1/2}\), \(1\) and \(n^{-1/2}\) for input, hidden and output layers respectively; norm of the activation updates for each layer normalized by the square root of the layer's output dimension to measure coordinatewise update scaling; layerwise effective feature ranks measured as in Andriushchenko et al. (2023a) by the minimal amount of singular values to make up \(99\%\) of the variance of the activations in a given layer; gradient norm, Hessian spectral norm and Hessian trace of loss with respect to weights; training accuracy, test accuracy after optimally stopping.

Observe that, especially for large widths, global perturbation scaling effectively only perturbs the last layer, as predicted by Theorem 11. Last-layer SAM is more similar to \(\mu\)P-global SAM than SGD on all of the tracked statistics, in particular at large widths. Only perturbing the last layer still affects the gradients in earlier layers so that weight updates and activations change in all layers. We find that SAM in \(\mu\)P with global scaling does not consistently improve generalization performance over SGD, whereas \(\mu\)P\({}^{2}\) does improve over SGD for all widths (Figure 13). Last-layer perturbation norms coincide by design with the global perturbation radius \(n^{-d}\rho\) and their effect on the activations stays \(\Theta(1)\) with increasing width as measured in relation to weight spectral norm. Formally the last-layer perturbation norm converges due to

\[\|\tilde{W}^{L+1}-W^{L+1}\|_{F}=n^{-d}\rho\|\frac{\chi_{t}x_{t}^{L}}{\|v_{t} \|}\|_{F}\to n^{-d}\rho\|\frac{x_{t}^{L}}{\|x_{t}^{L}\|}\|_{F}=n^{-d}\rho \to 0,\]

where the loss derivative \(\chi_{t}\) always cancels out due to the normalization and the global gradient norm \(\|v_{t}\|\) is dominated by the last-layer gradient norm due to the global scaling (Theorem 11). Normalizing the weight perturbations by the weight spectral norm measures the influence of the perturbations on the activations. Note that this influence is also vanishing. Feature ranks stay close to initialization, since random initialization has high rank and training does low effective rank updates. Here we do not observe that SAM reduces the feature rank compared to SGD. The Hessian spectral norm and trace are quite noisy. The last-layer Hessian spectral norm explodes with width in \(\mu\)P, because last-layer learning rate is scaled as \(n^{-1}\), hence the edge of stability explodes. ResNets in \(\mu\)P are more stable, their Hessian spectral norm even shrinks with width (not shown).

Contrast the results for \(\mu\)P-global with the results for \(\mu\)P\({}^{2}\) in Figure 22 for a comparison with SGD in \(\mu\)P. The Hessian spectral norm is reduced by SAM as you would expect. Additionally \(\mu\)P\({}^{2}\) shows low variability in performance and all other statistics. SAM in \(\mu\)P\({}^{2}\) does not reduce the feature rank compared to SGD in \(\mu\)P. This suggests that the conclusions drawn by Andriushchenko et al. (2023a) do not apply to MLPs in \(\mu\)P.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{ResNet-18 on CIFAR10} & \multicolumn{3}{c}{ViT on CIFAR100} & \multicolumn{3}{c}{ViT on ImageNet1K} \\ Width multiplier & 0.5 & 1 & 2 & 4 & 0.5 & 1 & 2 & 0.5 & 1 & 2 \\ \hline Seconds per epoch & 109 & 161 & 327 & 803 & 209 & 327 & 777 & 2550 & 4151 & 9802 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **(Training time per epoch)** Training time (in seconds) per epoch of the entire data loading and training pipeline of SAM in \(\mu\)P\({}^{2}\) on a single NVIDIA A10G GPU.

### Propagating perturbations from the first layer does not inherit SAM's benefits

Here we apply a parametrization that only effectively perturbs the first layer weights (derived in Example F.1). Figure H.2 shows that effective first-layer SAM loses both \(\mu\)P\({}^{2}\) SAM's improvement in test accuracy as well as SAM's inductive bias towards smaller gradient norm and Hessian norm, i.e. lower sharpness in MLPs. This performance deterioration occurs although the perturbation of first-layer SAM has an effect of the same order of magnitude as \(\mu\)P\({}^{2}\) on weight and activation updates in all layers. This shows that mere propagation of weight perturbations from earlier layers cannot replace effective weight perturbations in each layer in order to benefit from SAM. It is crucial to correctly adjust the layerwise perturbation scaling, and to distinguish between effective perturbations and perturbation nontriviality in each layer.

SAM in \(\mu\)P\({}^{2}\), on the other hand, achieves the correct perturbation and update scaling, has lower final gradient and Hessian spectral norm, improves test accuracy over SGD and has overall lower variance between training runs.

Figure H.1: Several neural network statistics for SAM (blue), LL-SAM (green) and SGD as a baseline (orange) across width after training a \(3\)-layer MLP in \(\mu\)P-global for 20 epochs with the optimal learning rate \(0.3432\) and perturbation radius \(0.2154\). The statistics are explained in the text of Appendix H.1.

Figure H.2: Same neural network statistics as in Figure H.1 but SAM-SGD in \(\mu\)P\({}^{2}\) (blue) versus MUP with perturbations scaled to only effectively perturb the first layer weights (green) with SGD in \(\mu\)P as a baseline. The first-layer perturbation parameterization performs worse than \(\mu\)P\({}^{2}\) and results in gradient norm and Hessian norm similar to that of SGD, larger than those of SAM. While the spectral norm of the weights converges to a similar quantity as for \(\mu\)P\({}^{2}\), the effect of the weight changes on the hidden activation updates behaves more like SGD. Feature ranks all look similar.

### Hyperparameter transfer

In this section, we provide supplemental evidence that, \(\mu\)P\({}^{2}\) is the unique perturbation scaling that robustly achieves hyperparameter transfer in \(\mu\)P both for the optimal learning rate and the optimal perturbation radius across neural architectures and datasets. But we also show that both MLPs and ResNets in SP can sometimes achieve hyperparameter transfer after long training.

#### h.3.1 MLPs in \(\mu\)P

Figure H.3 shows that in \(\mu\)P\({}^{2}\) the optimal hyperparameters in terms of test accuracy transfer in both learning rate and perturbation radius at sufficient width, and test accuracy monotonically improves with model scale. In addition, SAM in \(\mu\)P\({}^{2}\) outperforms SAM in \(\mu\)P with global perturbation scaling at all widths.

While other works focus on hyperparameter transfer in training loss, we are ultimately interested in transfer with respect to test accuracy. Especially under harmful overfitting, the test accuracy is affected by nontrivial interactions between the learning rate and the perturbation radius. While the joint optimum is slightly shifting towards larger learning rate and perturbation radius for small widths, it remains remarkably stable for sufficient width \(\geq 1024\). Note that slight shifts in the optimal learning rate due to finite width biases have also been observed in earlier works (Yang et al., 2022).

Figure H.3: Training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch SAM training as a function of learning rate (left) with perturbation radius \(\rho=0.2154\), and as a function of perturbation radius (right) with learning rate \(\eta=0.4529\) in \(\mu\)P\({}^{2}\). The optimal learning rate transfers. The smaller the perturbation radius the better the training accuracy. For sufficiently wide MLPs, the validation-optimal perturbation radius transfers as well and SAM reduces harmful overfitting.

Figure H.4: Training accuracy (top) and test accuracy (bottom) after optimally stopping 20 epoch SAM training as a function of learning rate (left) and perturbation radius (right) in \(\mu\)P-global with the same base learning rate and perturbation radius as in Figure H.9. For global perturbation scaling, we do not observe a benefit of SAM over SGD.

Figure H.5: Same as Figure H.4 but with input multiplier \(0.0305\) and small output multiplier \(0.0098\). Note that networks with width at most \(256\) perform better in terms of test accuracy than with the other multiplier choice in Figure H.4, but the multipliers here have worse width scaling properties. To the best of our knowledge, the issue that optimally tuned hyperparameters on small models may scale worse than slightly suboptimal hyperparameters has not been stated before. This raises the question when and how can we use small models to predict the optimal hyperparameters of large models.

[MISSING_PAGE_FAIL:59]

#### h.3.2 Some variants of SP can transfer optimal hyperparameters on CIFAR-10

Surprisingly, after training MLPs to convergence on CIFAR-10, some variants of SP with naive perturbation scaling can transfer learning rate and perturbation radius, against the prediction by infinite-width theory. For SGD, this has originally been observed in GitHub issue 52 of the "up-package4. We train MLPs with SAM in variants of SP in Figure H.10 and also observe that some variants achieve transfer while for others the optimal learning rate shrinks as in Yang et al. (2022).

Footnote 4: Without tuned weight multipliers, MLPs trained with SGD in SP on CIFAR-10 can transfer the optimal learning rate: https://github.com/microsoft/mup/issues/52

To achieve shrinking stable and optimal learning rates in SP, Yang et al. (2022) use weight multipliers tuned at base width \(256\) and normalize the initialization variance to be invariant to these weight multipliers, according to the Jupyter notebook5 provided for reproducing their experiments. In addition, they initialize the last layer to \(0\) which contradicts SP scaling but results in more striking shrinkage of the optimal learning rate. We observe that both with and without weight multipliers, MLPs trained with SAM in SP-naive have surprisingly good transfer properties on CIFAR-10. With tuned multipliers but initialization that is invariant to these multipliers, the optimal learning rate shrinks. Because we are training to convergence, pure infinite-width theory does not adequately describe the training dynamics anymore (Vyas et al., 2024). Infinite-width theory implies that scaling the width further would eventually break the learning rate transfer. It remains a matter of ongoing work to understand whether this stability of SP is a finite-width or a long training time effect, and whether this empirical stability is particular to multi-epoch training on vision datasets. As shrinkage of the optimal learning rate in SP has generally been observed in language settings (see e.g. Brown et al., 2020, Table 2.1), we expect the same shrinkage for SAM in SP in such settings.

Figure H.8: Mean (over \(3\) runs) of training accuracy (top) and of test accuracy (bottom) after optimally stopping 20 epoch SAM training of a MLP in \(\mu\)P-global as a function of learning rate and perturbation radius. The global scaling of the perturbation radius by \(n^{-1/2}\) compared to \(\mu\)P-naive (Figure H.7) makes the stable regime invariant to width. But the suboptimal layerwise perturbation scaling that only perturbs the last layer does not consistently improve over SGD (\(\rho=0\)).

Figure H.9: Mean (over \(3\) runs) of training accuracy (top) and of test accuracy (bottom) after optimally stopping 20 epoch SAM training of a MLP in \(\mu\)P\({}^{2}\) as a function of learning rate and perturbation radius. At sufficient width, the optimal hyperparameters are stable in terms of test accuracy, even under severe overfitting.

Note that the learning rate transfer in SP here is a much stronger observation than in Everett et al. (2024) who choose the correct layerwise learning rates for SP. Hence their SP merely deviates from \(\mu\)P through a larger output layer initialization and key-query normalization by \(\sqrt{d}\) in SP versus by \(d\) in \(\mu\)P. Here however we even observe transfer in our stricter understanding of SP without any layerwise learning rates or weight multipliers. This is not a peculiarity of SAM; we observe the same learning rate transfer in plain SGD without any momentum or weight decay for MLPs on CIFAR-10 (not shown).

ResNets in SP show hyperparameter transfer across most SAM variants too, as soon as we tune momentum, weight decay and labelsmoothing (Figure 11). This is in line with previous empirical observations (Yang et al., 2022, Figure 16 for SGD) but contradicts infinite-width theory as for MLPs.

#### h.3.3 ResNets

In this section, we plot averages and \(\sigma\)-CI from 2 independent runs.

Figure 11: Training accuracy (top) and test accuracy (bottom) after optimally stopping 100 epoch SAM training as a function of learning rate and perturbation radius in SP-naive without regularization (left) and with tuned regularization (right) using momentum \(0.9\), weightdecay \(0.0005\) and labelsmoothing \(0.1\). CI denote the minimal and maximal value from 4 independent runs. Without regularization, the optimal learning rate shrinks with width. Given the learning rate, the optimal perturbation radius seems quite stable, but since the optimal learning rate shifts, the performance scales worse than for \(\mu\)P\({}^{2}\) with the fixed learning rate that is tuned on the small model. With optimal regularization, both optimal learning rate and perturbation radius remain remarkably stable. We plan to investigate this mechanism in an upcoming work.

Figure 10: Optimal learning rate and perturbation radius (cross) and regions within \(1\%\) of the optimal test accuracy (mean over \(4\) runs) after optimally stopping 20 epoch SAM training of a MLP in different variants of SP for varying widths (the darker, the wider). Observe transfer properties in SP almost as stable as in \(\mu\)P\({}^{2}\) (Figure 1) and against infinite-width predictions slightly growing with width, both with and without the tuned weight multipliers by Yang et al. (2022). Only when normalizing the initialization variance to be independent of the width-independent weight multipliers (initnorm), does the regime of stable learning rates shrink at the widths considered. Additionally initializing the last layer to zero (\(\text{ll}=0\)) (as in the Jupyter notebook provided to reproduce Figure 3 in Yang et al. (2022)) shows even more pronounced learning rate shrinkage, but does not correspond to SP scaling anymore.

ResNets in \(\mu\)P\({}^{2}\) transfer both the optimal learning rate and perturbation radius for SAM (Figure H.12), SAM-ON (Figure H.14) and elementwise ASAM (Figure H.15), as well as different alternatives of scaling the gradient norm contributions to SAM's denominator (Figure H.18). This suggests correctness of the derived scalings. At width multipliers \(2\) and \(4\), \(\mu\)P\({}^{2}\) achieves the same or slightly better test accuracy than SP in all SAM variants.

Figure H.13 shows ResNets trained with SAM in different parameterizations. In ResNets of practical scale, \(\rho\) remains quite stable in \(\mu\)P\({}^{2}\) but surprisingly also in SP-NAIVE. In \(\mu\)P, for naive perturbation scaling the regime of stable perturbation radii shrinks, for global perturbation scaling, the optimal perturbation radius shifts, approaching its maximal stable value, which stays invariant to width scaling. Here, it would be interesting to see whether even larger width would lead to suboptimal performance of \(\mu\)P-global. \(\mu\)P\({}^{2}\) is most robust to the choice of \(\rho\) and achieves the best test accuracy.

#### a.3.4 ASAM variants

As we are not aware of any use of ASAM with MLPs in the literature and since the amount of necessary experiments for ViTs exceeds our computational budget, we only show that ResNets trained with the all of the discussed SAM variants in \(\mu\)P\({}^{2}\) transfer the optimal \((\eta,\rho)\).

For the examples of elementwise ASAM and SAM-ON the global perturbation scaling \(n^{1/2}\) suffices to reach \(\mu\)P\({}^{2}\). The stability of the optimal perturbation radius in the applied scaling \(n^{1/2}\) shows that in \(\mu\)P with naive perturbation scaling the optimal perturbation radius would grow as \(n^{1/2}\).

See the previous section, for a discussion of the remarkable stability of ResNets in SP. For the example of elementwise ASAM in SP, the optimal perturbation radius seems to grow.

For layerwise ASAM (Figure H.16), the optimal perturbation radius seems to grow in both SP and \(\mu\)P\({}^{2}\), suggesting that our scaling condition does not perfectly apply to this variant, although \(\mu\)P\({}^{2}\) (\(97.09_{\pm 0.03}(+0.83)\)) still outperforms SP (\(96.86_{\pm 0.05}(+0.83)\)) in terms of the optimal test

Figure H.12: Training accuracy (top) and test accuracy (bottom) after optimally stopping 200 epoch SAM training as a function of learning rate and of perturbation radius in SP (left) and in \(\mu\)P\({}^{2}\) (right) with optimized momentum \(0.9\), weight decay \(5\cdot 10^{-4}\) and labels smoothing \(0.1\) for both \(\mu\)P\({}^{2}\) and SP. In \(\mu\)P\({}^{2}\), the base learning rate is \(\eta=2^{-4}\) and the base perturbation radius is \(\rho=2^{-4}\), in SP \(\eta=0.05\) and \(\rho=0.1\), respectively. Observe monotonic improvement with width in both training and test error. Optimal hyperparameters transfer across widths, surprisingly in both \(\mu\)P\({}^{2}\) and SP.

Figure H.13: Test accuracy after optimally stopping 200 epoch SAM training as a function of perturbation radius in various parameterizations. Dashed lines denote the base optimizer SGD with tuned momentum and weight decay in the respective parameterization.

[MISSING_PAGE_EMPTY:63]

### Gradient norm contributions have negligible effects on generalization performance

In this section we provide ablations concerning the question which layers should contribute non-vanishingly to the gradient norm in the denominator of the layerwise SAM perturbation rule (LP).

For MLPs, in Figure H.17 we scale all contributions to \(\Theta(1)\), and then set the contribution of individual layers to zero, one by one. We observe no significant effect on the optimal test loss or hyperparameter transfer for MLPs. Any layer's contribution to the gradient normalization in the denominator of the SAM update rule can be set to \(0\) without a significant effect on the test loss. This raises the question which effect the gradient normalization has in \(\mu\)P. Does it contribute a scaling correction in SP, but may be dropped entirely in \(\mu\)P?

For ResNets, Figure H.18(a) shows accuracies when rescaling all layers' gradient norms to \(\Theta(1)\), and Figure H.18(b) shows the results when using the original global gradient norm rescaled to \(\Theta(1)\). Again, both methods achieve similar optimal test accuracy. The first variant shows cleaner hyperparameter transfer and monotonous improvement with width. When comparing to our original definition (LP) in Figure H.12, optimal performance is similar but rescaling all layers' gradient norm contributions to \(\Theta(1)\) may even produce a slightly more stable hyperparameter-loss landscape for ResNets.

Figure H.17: Scaling the gradient norm contributions of all layers to \(\Theta(1)\) (first row) and then setting the first layer gradient norm to \(0\) (2nd row), respectively the hidden layer (3rd row), last-layer (4th row). Each individual layer seems to have vanishing contribution to the optimal test error.

### SAM with layerwise gradient normalization

Here we consider SAM without the gradient normalization over all layers jointly. Instead we apply the layerwise perturbation rule presented in Appendix F.7,

\[\bm{\varepsilon}_{t}^{l}=\rho_{l}\cdot\nabla_{W^{l}}\mathcal{L}(f(\xi_{t};W_{t}), y_{t})/\|\nabla_{W^{l}}\mathcal{L}(f(\xi_{t};W_{t}),y_{t})\|_{F}.\]

In SP, we consider a global constant \(\rho_{l}=\rho\), whereas for \(\mu\)P\({}^{2}\) the spectral condition (\(*\)) requires \(\rho_{l}=\rho\cdot\sqrt{\texttt{fan\_out}/\texttt{fan\_in}}\).

Overall, SAM without layer coupling performs decently, but is outperformed by the original SAM in particular in ResNets, in \(\mu\)P\({}^{2}\) and at large width. But note that for ResNets we adopt the hyperparameters tuned for the original SAM with layer coupling, so that these ablations only serve as preliminary experiments.

**MLPs.** SAM without layer coupling achieves similar optimal generalization in \(\mu\)P\({}^{2}\) at each width compared to Figure H.9. The regime of stable \((\eta,\rho)\) stays width-independent, but does not transfer the optimum consistently. This suggests complex or noisy dependence of the training dynamics on \(\rho\).

**ResNets.** Figure H.20 shows that decoupled SAM has decent performance, but is worse than original SAM with global normalization (Figure H.13) in both SP and \(\mu\)P\({}^{2}\), in particular at large width. As expected, \(\rho\) transfers in \(\mu\)P\({}^{2}\).

Figure H.19: **(SAM with layerwise normalization in MLPs) Test accuracy as a function of learning rate \(\eta\) and perturbation radius \(\rho\) for an optimally-stopped MLP trained with SAM with layerwise normalization.**

Figure H.18: Same as Figure H.12 but with scaling of the gradient norms in the SAM perturbation (LP) denominator that scales all terms to \(\Theta(1)\) (left) and only global denominator scaling \(\frac{\|\nabla L\|}{n_{L}}\) (right). All denominator scalings achieve similar optimal accuracy, show HP transfer in learning rate and monotonic test accuracy improvement with width. In global denominator scaling, the optimal \(\rho\) shifts with width.

### Test error over the course of training

Figure 21 shows the test error of ResNets and ViTs over the course of training. \(\mu\)P\({}^{2}\) always achieves the best final test accuracy. In ResNets it also achieves a decent test accuracy the fastest and removes training instabilities of SAM in SP. While SGD in \(\mu\)P alone cannot compete with SAM in SP, SAM in \(\mu\)P\({}^{2}\) uniformly dominates over the entire course of training. Our theory suggests that in \(\mu\)P\({}^{2}\) the gradients are scaled correctly from the beginning, whereas in SP they have to self-stabilize first, which slows down convergence. We plan a closer analysis in an upcoming work.

In ViTs, \(\mu\)P generally achieves decent accuracy faster than SP, since gradient norms are already scaled correctly at initialization. SAM converges slower than the base optimizer AdamW in favor of drifting towards a better generalizing local minimum or saddle point. For ViTs at this moderate scale, SAM in SP catches up to SAM in \(\mu\)P\({}^{2}\) at the end of training.

Figure 21: Training a ResNet-18 with width multiplier \(2\) on CIFAR10 (left) and a ViT with width multiplier \(2\) on CIFAR100 (right). SGD and AdamW are the respective base optimizers.

Figure 20: **(SAM with layerwise normalization in ResNets) Test accuracy as a function of perturbation radius \(\rho\) for ResNets trained with SAM with layerwise normalization.**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction we state our main contributions while acknowledging related work. All main claims are theoretically proven and/or empirically verified. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are discussed in the future work section as well as in the section in the appendix that is related to the respective limitation. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We state all assumptions in the main paper and Appendix C, and provide all formal proofs in Appendix E. Guidelines: * The answer NA means that the paper does not include theoretical results.

* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All experimental details are disclosed in Appendix G. Our perturbation scaling rules are clearly stated in the main paper. Their implementation with flexible fan_in and fan_out is explained in Appendix F.7, together with pseudocode for implementing our proposed scaling rule. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We only propose width-dependent scaling of hyperparameters of an existing optimization algorithm. This can be easily implemented by following the scaling rules that we clearly specify in the main paper. In Appendix F.7 we even provide a code example that contains the essential modifications.We are working on making Python code to reproduce all of our experiments publicly available. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All experimental details are disclosed in the main paper or Appendix G. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: As stated in Appendix G, we repeat all main experiments with multiple independent runs and report confidence bands within the empirical \(2.5\%\)- to \(97.5\%\)-quantiles. When we repeat experiments on Vision Transformers that we have also conducted on MLPs or ResNets, we do not use multiple runs due to limitations in computational resources. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the type of GPU used and number of GPU seconds required for each experiment in Appendix G. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We provide a theoretical analysis of a widely used optimization algorithm, point out the algorithm's limitations in large models and propose a correction. We do not foresee any ethical concerns. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper provides fundamental research toward understanding and improving existing optimization algorithms for neural networks. We do not release any model or data and do not consider generative models. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We only use standard vision architectures and vision datasets in our experiments and do not release any data or models. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **License for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the standard CIFAR10, CIFAR100 (Krizhevsky et al., 2009) and ImageNet1K (Deng et al., 2009) datasets following the standard practice. We also cite the Python assets PyTorch(Paszke et al., 2019), mup (Yang et al., 2022) and the GitHub repository implementing SAM (Samuel, 2022) that we use as a basis for our experiments. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.