**A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics**

**Puze Liu\({}^{\,1}\)\({}^{\,2}\)** **Jonas Gunster\({}^{\,1}\)** **Niklas Funk\({}^{\,1}\)** **Simon Groger\({}^{\,1}\)**

\begin{tabular}{c c c c}
**Dong Chen\({}^{\,3}\)** & **Haitham Bou-Ammar\({}^{\,4}\)** **Julius Jankowski\({}^{\,5}\)** **Ante Maric\({}^{\,5}\)** \\
**Sylvain Calinon\({}^{\,5}\)** & **Andrej Orsula\({}^{\,6}\)** **Miguel Olivares-Mendez\({}^{\,6}\)** **Hongyi Zhou\({}^{\,7}\)** **Rudolf Lioutikov\({}^{\,7}\)** **Gerhard Neumann\({}^{\,7}\)** **Amarildo Likmeta\({}^{\,8}\)** **\({}^{\,9}\)** \\
**Amirhossein Zhalehmehrabi\({}^{\,9}\)** & **Thomas Bonenfant\({}^{\,9}\)** **Marcello Restelli\({}^{\,9}\)** \\
**Davide Tateo\({}^{\,1}\)** & **Ziyuan Liu\({}^{\,3}\)** & **Jan Peters\({}^{\,1}\)\({}^{\,2}\)\({}^{\,10}\)** \\

air-hockey-challenge@robot-learning.net \\ \end{tabular}

**Abstract**

Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited. Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-to-real gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks. The competition's results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging. Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution. The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.

Introduction

Modern machine learning techniques, particularly with the advent of Large Language Models (LLMs) and Diffusion Models (DMs), had a disrupting impact on many real-world applications, such as language processing and generation Yang et al. (2019), Brown et al. (2020), Achiam et al. (2023), board and video games Silver et al. (2016), Vinyals et al. (2019), image generation Ramesh et al. (2021), Rombach et al. (2022), Croitoru et al. (2023), and speech synthesis Prenger et al. (2019), Kong et al. (2020). However, while researchers are trying to bring these novel approaches to the real world, purely data-driven approaches are still struggling in real-world robotics, particularly when facing dynamic tasks. Indeed, recently, different foundation models for robotics have been presented Brohan et al. (2023), Zitkovich et al. (2023), O'Neill et al. (2023), Li et al. (2024), Team et al. (2024), Kim et al. (2024). However, to date, these models suffer from long inference times and they do not provide safety guarantees Firoozi et al. (2023), making them unsuitable for fast and dynamic real-world manipulation. Furthermore, industrial applications mostly rely on classical robotics and control techniques, relegating the data-driven methods to the area of research Peres et al. (2020), Dzedzickis et al. (2021).

Indeed, while robotics tasks are often the benchmark of choice in many areas of machine learning research, such as Computer Vision (CV) and Reinforcement Learning (RL), there is often quite a big disconnection between these benchmarks and real-world tasks: often the simulated setup is oversimplified, relieving the machine learning practitioner from the common issues arising when dealing with real robotic platforms. Thus, these benchmarks focus on small aspects of the robotics problem and do not sufficiently capture the complexity and challenges of real-world systems. This makes the deployment of machine learning solutions in real-world platforms challenging and often infeasible. To overcome these challenges and to incentivize the development of approaches that can better transfer onto real robotics systems, our Robot Air Hockey Challenge specifically focuses on a set of identified key issues preventing the deployment in dynamic real-world environments:

**Safety** Real robotics platforms live in the physical world. Therefore, the action performed by the robot must not damage the robot itself, the environment, or the people around the robot, at least at deployment time. Learning-based methods should consider safety problems and avoid dangers at any point during the execution.

**Imperfect Models** It is often challenging to obtain a good model of a real system in simulation. Particularly when dealing with black-box industrial systems that do not comprehensively describe the robot parameters and dynamics. Moreover, the real world presents many discontinuous dynamics that may heavily affect the environment's behavior. Thus, machine learning methods for real systems have to expose sufficient robustness and flexibility to deal with model mismatches.

**Limited Data** Real-world robotic data requires real-world interactions. Unlike the simulation environment, we cannot speed up the data collection process or employ massive parallelization. While large robotics datasets are already available, they are often restricted to a specific robot. It will be impractical to generalize to arbitrary platforms, as every hardware, software interface, and control system differs. As collecting real-world data is costly, difficult, and time-consuming, it is essential to develop methodologies that can adapt to the sim-to-real gap by learning from limited data.

**Reactiveness** Dynamic environments require rapid adaptation and reaction. This requirement forces the agent to be ready to react to unexpected events and to compute the control action in a real-time fashion. Often, control systems are embedded in the robot, resulting in limited computation resources, e.g., no GPU availability and a limited number of cores for the computation.

**Observation Noises and Disturbances** In real-world environments, observation noise and external disturbances will inevitably exist. If not handled properly, these noises and disturbances can seriously affect the performance of the trained model. Improving the robustness of the learning methods against observation noises and disturbances is essential for real-world deployment.

In summary, we introduced the Robot Air Hockey Challenge to provide a benchmark on a challenging, dynamic task, covering all these issues. We argue that the codified nature of the game and the limited workspace easily allow the definition of rigorous evaluation metrics, which are fundamental requirements of proper scientific evaluation. This challenge is a preliminary step towards more realistic benchmarks evaluating robust, reliable, and safe learning techniques that can be easily deployed in real-world robotic applications, going beyond the quasi-static assumption.

### Related Works

Dynamics tasks have always been challenging benchmarks for robotics and machine learning. For example, researchers have focused on dynamics dexterity games such as ball-in-a-cup [Kawato et al., 1994, Kober and Peters, 2008], juggling [Ploeger et al., 2021, Ploeger and Peters, 2022] or diabolo [von Drigalski et al., 2021], but also on sports such as tennis [Zaidi et al., 2023], soccer [Haarnoja et al., 2024] and table tennis [Mulling et al., 2011, Buchler et al., 2022]. Robotics tasks have also been already used as benchmarks for machine learning competitions, such as the Robot open-Ended Autonomous Learning competition (REAL) [Cartoni et al., 2020], Learn to Move [Song et al., 2021], the Real Robot Challenge [Gurtler et al., 2023, Funk et al., 2021], the TOTO Benchmark [Zhou et al., 2023], the MyoSuite Challenge [Caggiano et al., 2023] or the Home Robot Challenge [Yenamandra et al., 2023]. However, most of these tasks focus on simulation or consider limited quasi-static settings, where complex real-time and safety requirements are less critical.

While we are the first to develop a structured benchmark based on robot air hockey, the task is well-known in machine learning and robotics. The first work on robot air hockey can be found in Bishop and Spong [1999], while the first learning-based approach is presented in Bentivegna et al. [2004a,b], where a humanoid robot is trained to learn air hockey skills. Due to the task complexity and versatility, it has been considered a challenging setting for evaluating robotic planning and control [Namiki et al., 2013, Igeta and Namiki, 2017, Liu et al., 2021], perception [Bishop and Spong, 1999, Tadokoro et al., 2022] and even human opponent modeling [Igeta and Namiki, 2015]. On top of that, the air hockey task has been used as a benchmark for many RL [AlAttar et al., 2019, Liu et al., 2022] and robot learning [Xie et al., 2020, Kicki et al., 2023, Liu et al., 2024] approaches. Most recently, after our NeurIPS 2023 Robot Air Hockey challenge, another benchmark on robot air hockey has been proposed by Chuck et al. [2024]. In contrast to our benchmark, it focuses more on perception, and our setting uses a much bigger playing table and requires faster and more reactive motions.

## 2 The Robot Air Hockey Challenge

The Robot Air Hockey Challenge is based on our real-world robot air hockey setup, consisting of two Kuka LBR IIWA 14 robots and an air hockey table (cf. Figure 1). The Kuka robots are equipped with a task-specific end-effector composed of an air hockey mallet connected to a rod, designed to deploy agents that do not strictly comply with the safety constraints. In addition to the real system, we also developed a MuJoCo simulation allowing for efficient testing and evaluation of the proposed control strategies. While the participants had access to an ideal version of the simulator, we evaluated the solution in a modified simulator, including various real-world factors, such as observation loss, tracking loss, model mismatch, and disturbances. Participants were allowed to

Figure 1: Game played in the real world between SpaceR and Air-HocKIT. The Air-HocKIT robot (back) hits the puck and the SpaceR robot (front) defends the attack to take control of the puck.

evaluate their solution once per day and download the dataset obtained from the modified simulator. With this setting, we want to simulate the limited access to real-world data, forcing the participants to deal with the sim-to-real gap. On top of the sim-to-real gap, the approaches should satisfy the deployment requirements, which provide metrics to quantify whether the respective policy would be safe for real-world deployment, i.e., the policy would not harm the real setup. Details of the metrics evaluating the deployability are presented in Section 2.2.

We also provide a robotic baseline capable of playing the full match, based upon our previous work Liu et al. (2021). The robotic baseline integrates typical approaches, such as planning and optimization. This approach performs satisfactorily in simulation but struggles to handle the sim-to-real gap. In addition, we provide a safe RL baseline Liu et al. (2022) for the low-level skills.

### Competition Structure

The Robot Air Hockey Challenge comprises two main stages in simulation: the Qualifying and the Tournament stage. Details about the simulated environment are provided in Table 1 and Appendix A.1. In the third stage, we deployed the solutions from the top three teams in the real-world platform, validating our challenge design. The next paragraphs introduce the challenge's three stages.

Qualifying stageIn this stage, participants need to control the robot to achieve three different sub-tasks in the single-robot environment, namely "Hit", "Defend", and "Prepare". The illustrations of the sub-tasks are shown in Figure 2.

* **Hit** The objective is to hit the puck to score a goal while the opponent moves in a predictable pattern. The puck is randomly initialized with a small initial velocity.
* **Defend** The objective is to stop the incoming puck on the agent side of the table and prevent the opponent from scoring. The puck is randomly initialized on the opponent's side of the table with a random velocity towards the agent's side, simulating an arbitrary hit from the opponent.
* **Prepare** The objective is to dribble the puck to a good hitting position. The puck is initialized close to the table boundary, where a direct hit to the goal is not feasible. The agent should maintain control of the puck, i.e., the agent must keep the puck on its half of the table.

Each task has different success metrics. For the "Hit" tasks, we count each episode as a success if the puck enters the scoring zone with a speed above the threshold. For the "Defend" task, an episode is considered successful if the final velocity of the puck (at the end of the episode) is below the threshold and does not bounce back to the opponent's side of the table. The "Prepare" task is considered successful if the final position of the puck is within a predefined area in the center of the

\begin{table}
\begin{tabular}{l|c}
**Simulation Frequency** & 1000 Hz \\ \hline
**Control Frequency** & 50Hz \\ \hline  & Puck’s X-Y Position, Yaw Angle: \([x,y,\theta]\) \\
**Observation** & Puck’s Velocity: \([\dot{x},\dot{y},\dot{\theta}]\) \\  & Joint Position / Velocity: \([q,\dot{q}]\) \\  & Opponent’s Mallet Position (if applicable): \([x_{o},y_{o},z_{o}]\) \\ \hline
**Control Command** & Desired Joint Position / Velocity \\ \end{tabular}
\end{table}
Table 1: Specification of the Air Hockey Environment

Figure 2: The three tasks in the qualifying stage of the Robot Air Hockey Challenge: from left to right “Defend”, “Hit”, and “Prepare” tasks. The “Defend” task requires stopping an incoming puck to get control of it. The “Hit” task consists of scoring a goal against an opponent, which moves in a fixed pattern. The “Prepare” task consists of repositioning the puck in the central area of the table without losing control of it.

agent's side of the table and the puck speed is below a given threshold. We use the mean success rate over the three sub-tasks as the overall performance metric.

Tournament stageIn this stage, participants trained an agent incorporating high-level skills with a low-level controller to play the full game of Air Hockey. While the qualifying stage focuses more on low-level behavior and individual primitives required to play Air Hockey, this stage requires high-level decision-making and a seamless combination of the previously developed components. The stage is composed of two rounds. In each round, we evaluate the solutions of each team against each other, such that each participant plays exactly one game against each available opponent. Before the start of each run, the authors are allowed to test their agents against the baseline agent provided by the organizers. We also run some friendly games between the currently submitted solution. Once the round started, we performed the evaluation sequentially without allowing further agent modification.

Real-world validation stageFinally, we evaluated the solutions of the tournament's top three teams on our real-world air hockey platform. The deployment of the solution on the real system required another safety layer, both in terms of safety requirements and fine-tuning of the methodology and the algorithm parameters, as the simulated system still presented quite a considerable sim-to-real gap, even with domain randomization and mismatch. For this reason, for every team, the performance in the real robot system was considerably worse than in the simulated setting. Given that this stage required hand tuning and intense help and engineering from our side, the outcome of the competition relied only on the simulated tasks. Details of the safety layer can be found in Appendix A.5. Despite the sim-to-real-gap and the necessary adaptations, the agents were able to play complete games. The full videos of the matches can be found publicly at http://air-hockey-challenge.robot-learning.net.

### Metrics

To evaluate the participants' solution, we focus on two important aspects: **deployability** and task **performance**. In the qualifying stage, performance is the success rate as defined above, while in the tournament stage, performance is the final game score. Based on the deployability score, teams will be categorized into three deployability levels, i.e., _deployable_, _improvable_, and _non-deployable_. Teams at the same deployability level will be ranked based on their winning score.

The deployability is computed by combining various metrics. These metrics are crucial constraints that need to be respected for real-world deployment. The safety constraints are presented in Table 2. We assign a penalty point to each metric based on its level of importance. The following metrics are considered during the evaluation:

* End-Effector's Position Constraints [3 pts]: The x-y-position of the end-effector should stay inside the table's boundaries. The end-effector should remain at the table height \(z_{\text{table}}=0.1645\).
* Joint Position Limit [2 pts]: The joint position should not exceed the position limits.

\begin{table}
\begin{tabular}{l|c|c}
**Constraint name** & **Dim.** & **Constraints** \\ \hline Joint position & 14 & \(q_{l}<q_{cmd}<q_{u}\) \\ \hline Joint velocity & 14 & \(\dot{q}_{l}<\dot{q}_{cmd}<\dot{q}_{u}\) \\ \hline \multirow{3}{*}{End Effector} & \multirow{3}{*}{5} & \(l_{x}<x_{ee}\) \\  & & \(l_{y}<y_{ee}<u_{y}\), \\ \cline{1-1}  & & \(z_{ee}>z_{\text{table}}-0.02\), \\ \cline{1-1}  & & \(z_{ee}<z_{\text{table}}+0.02\). \\ \hline \multirow{3}{*}{Link} & \multirow{3}{*}{2} & \(z_{elbow}>0.25\) \\  & & \(z_{wrist}>0.25\) \\ \hline \end{tabular}
\end{table}
Table 2: Constraints for the 7DoF environment

Figure 3: The tournament stage. The left figure shows the AiRLHockey agent (right side of the table) performing a hitting motion, while the right figure shows the Air-HocKIT (left side of the table) defending the attack and taking control of the puck

* Joint Velocity Limit [1 pt]: The joint velocity should not exceed the velocity limits.
* Computation Time [0.5-2 pts]: The computation time at each step should be shorter than 0.02 s. The maximum and the average computation time (per episode) will be considered in this metric.

In the qualification stage, we will run 1000 episodes on each subtask (equivalent to 2.8 hours of actual time) to evaluate the agent. The corresponding penalty points are accumulated if any metrics are violated in an episode (up to 500 steps per episode). The deployability score (DS) is the sum of the penalty points for all episodes. During the tournament stage, a full air hockey game will be evaluated. Each game lasts 15 minutes (45,000 steps), and we treat every 500 steps as an equivalent episode. Penalty points will be refreshed at the beginning of the episode. Based on the deployability score, the ranking will be divided into three categories: _Deployable_ (\(\text{DS}\leq 500\)), Improvable (\(500<\text{DS}\leq 1500\)), and _Non-Deployable_ (\(\text{DS}>1500\)) in the qualifying stage (1000 episodes); _Deployable_ (\(\text{DS}\leq 45\)), _Improvable_ (\(45<\text{DS}\leq 135\)), and _Non-Deployable_ (\(\text{DS}>135\)) in the tournament stage (equivalently 90 episodes).

## 3 Analysis

In this section, we analyze the results of the 2023 edition of the Robot Air Hockey Challenge. Despite being the first year of the competition, we received many competitive solutions and multiple solutions have been deployed in the real robot setup. In particular, it is interesting to see how the participants designed very different solutions coming from completely different perspectives. In general, many participants struggled to achieve satisfactory behaviors due to the particular challenges of the robot air hockey task. Furthermore, we identify four key outcomes of the competition:

**i.** The interplay between performance and deployability requirements makes the design of task objectives difficult. We note that, since most teams use RL in their solutions, the reward function design heavily influences the task performance.

**ii.** Penalty-based safety specification is brittle for out-of-distribution situations. Since deployability score is one important metric in the challenge, teams that adopt more engineered modules, such as inverse kinematics and trajectory interpolation, achieved better compliance with the constraints. Penalty-based training methods can satisfy safety requirements for in-distribution scenarios but may fail catastrophically for out-of-distribution situations.

**iii.** Plain RL is not sufficiently competent in handling long-horizon tasks. While it is possible to train a single agent to play a full game, the resulting approaches were not fully competitive with a human-designed policy.

**iv.** Physical inductive biases play an important role in the era of Embodiment AI. Prior knowledge such as kinematics, geometry, and physical understanding of the real world should not be ignored to obtain a safe, reliable, and robust solution deployable in the real world.

We now discuss in detail the three stages of the competition, highlighting the most relevant insights from each stage.

### Qualifying stage

We present the aggregated results of the qualifying stage in Table 3. Additional details for the performance on each task can be found in Appendix A.3. AiRLHockey won the qualifying stage, outperforming all other teams in all tasks. Their solution is mostly based on imitation learning, i.e., imitating a policy computed by optimal control. For the details, see Appendix A.6. The full RL solution, based on PPO, presented by the Air-HockIT team (Appendix A.8), achieved second place both overall and in all subtasks. Both the first and second solutions achieved similar performance in almost all tasks, except the prepare task, which is quite hard to solve for RL approaches. Indeed, we observed the same behavior in previous works on similar environments [Liu et al., 2022]. The GXU-LIPE team achieved third place, however, it did not achieve third place in all tasks. Indeed, the RL3_polimi team, another RL based approach (Appdendix A.9), considerably outperformed this solution in the Defend task, while the AeroTron team was able to gain the third position on the prepare task by a short margin. It is worth noting that the gap between the top two teams and the others was quite clear. Indeed, we believe that the top teams leveraged prior knowledge and experience with robotics systems, giving them a competitive advantage against machine learning practitioners, that are not sufficiently familiar with the robotics setting. We conclude that includingrobotics or physics-based inductive biases is still superior to black-box data-driven methods, and indeed most teams chose this type of solution for their final agents.

An important aspect to focus on is that many teams were able to provide satisfactory solutions in terms of performance, but struggled to comply with safety constraints. Indeed, only the ATACOM Baseline (Liu et al., 2022) can get some reasonable behavior while maintaining low safety violations. The safety issues are particularly relevant for the RL3_polimi team, which achieved the third high score but classified pretty low in the leaderboard due to safety constraints. Indeed, the black-box optimization of rule-based controllers used by the team proved to be highly sensitive to out-of-distribution evaluation. It is clear that safety issues are still a topic that is not very well explored and definitively requires more effort and investigation when working with robotic applications.

Ablation studiesFor additional insights into the qualifying stage, we performed an ablation study on the different components causing the sim-to-sim gap between training and evaluation environments. Results of this ablation study can be found in Figure 4. We evaluated the solutions on different versions of the environment: the ideal environment without the sim-to-real gap, the original evaluation environment with all the mismatch factors activated, and four different environments that differ from the ideal one only by the addition of one of the following modifications: i. model mismatch on the robot arm, ii. observation noise, iii. disturbance on the puck dynamics (due to the airflow), iv. unreliable puck tracking (track loss). From our ablation, it is clear that model mismatch plays a marginal role in the sim-to-real gap. This is because the closed-loop controller that tracks the desired trajectory heavily mitigates the effect of the model mismatch. In addition, most of the teams applied a state observer that filters the noisy observation. Thus, the observation noise has a limited impact on the performance. Instead, tracking loss and disturbances majorly affect the outcome of the task. On average, as expected, having all mismatch factors in the environments causes the agents to perform worse. This also holds true for most solutions in most tasks. Among the tasks, the more sensitive to mismatch factors is the "Hit" one, likely due to the fast movements and sensitivity to small angular errors. Instead, a bit surprisingly, the presence of a single factor may increase the performance on a single task of the baseline. This is most notable for SpaceR and RL3_polimi solutions, e.g., under puck disturbances. We argue that the changes in the state distribution are slightly beneficial for RL solutions due to the possibility of moving the puck in areas where the policy is more competent. The same result does not apply to the Air-HocKIT team, as the success rate of this solution is already high, meaning that the policy works well in all areas of the state space. We observe a small improvement also for the AiRLIHockey and Air-HocKIT solutions when using the less relevant mismatch factors. We believe this improvement is due to the stochasticity of the evaluation and would probably disappear with more task repetitions.

### Tournament stage

\begin{table}
\begin{tabular}{l|c|c|c|c|c}
**Team Name** & \begin{tabular}{c} **Hit** \\ **Success** \\ \end{tabular} & \begin{tabular}{c} **Defend** & \begin{tabular}{c} **Prepare** \\ **Success** \\ \end{tabular} & \begin{tabular}{c} **Penalty** \\ **Points** \\ \end{tabular} & 
\begin{tabular}{c} **Score** \\ \end{tabular} \\ \hline \hline \multicolumn{5}{c}{Deployable Teams} \\ \hline AiRLIHockey & 54.9\% & 84.5\% & 90.3\% & 327.5 & 73.8 \\ Air-HocKIT & 52.2\% & 79.0\% & 68.6\% & 341.0 & 66.2 \\ GXU-LIPE & 30.1\% & 29.5\% & 66.2\% & 475.5 & 37.1 \\ SpaceR & 14.4\% & 47.8\% & 47.8\% & 221.0 & 34.4 \\ Baseline & 12.8\% & 25.4\% & 66.3\% & 352.5 & 28.5 \\ Baseline-ATACOM & 18.4\% & 36.1\% & 30.1\% & 33.0 & 27.8 \\ AJoy & 18.4\% & 36.1\% & 20.0\% & 108.0 & 25.8 \\ Kalash Jain & 0.0\% & 19.5\% & 8.3\% & 0.0 & 9.5 \\ \hline \hline \multicolumn{5}{c}{Improvable Teams} \\ \hline RL3\_polimi & 22.7\% & 61.6\% & 36.2\% & 920.0 & 41.0 \\ AeroTron & 33.2\% & 23.2\% & 66.5\% & 594.0 & 35.9 \\ CONFIRMTEAM & 29.3\% & 23.9\% & 65.3\% & 629.0 & 34.4 \\ Tony & 33.2\% & 23.2\% & 54.3\% & 718.0 & 33.4 \\ sprkrd & 0.2\% & 5.5\% & 0.0\% & 1271.0 & 2.3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results from the qualifying stage We present the results of the tournament stage in Table 4. Unfortunately, not all the qualified teams took part in the tournament stage, and one team only took part in one round of the competition. The winner of this stage is the AiRLIHockey team, while SpaceR scored second and Air-HocKIT scored third. The AiRLIHockey team dominated both the first and second rounds of the competition. By looking at the videos of the games, it is clear that the high performance of this agent is mostly due to the superior hitting performance in terms of speed and accuracy. The second place was achieved by SpaceR, with a model-based RL solution based on DreamerV3. While this solution performed slightly worse than other approaches in the qualifying stage, the end-to-end training simplified the porting on the tournament environment.

Differently from SpaceR, the Air-HocKIT team relied on a high-level policy to concatenate different skills. Unfortunately, a faulty policy implementation caused this team to lose many games on the first run due to a high deployability score. A similar issue also affected the solution of the RL3_polimi team. This shows one possible limitation of modular policies, where a faulty component can cause the complete failure of the system. Indeed, in the second round, after fixing the agent in the fine-tuning phase, Air-HocKIT achieved a good score, losing only against the AiRLIHockey team. For the same reasons, also the RL3_polimi performance increased in the second round.

### Real robot deployment

Sim-to-real transfer is a fundamental aspect of robot learning, as good performance on the real robotic system ultimately validates the presented approaches. While the real robot deployment was not part of the competition itself, we believe the sim-to-real transfer is a fundamental aspect of robot learning. In the end, we successfully deployed the top three solutions on the real robot. We recorded two full games between the agents. The deployment went on mostly smoothly, with minor issues (false score detection and malllet flipping) and both games ended up with multiple goals scored. While most of the goals originated from a mistake of the agents controlling or defending the puck, in the videos we still observe goals due to well-placed shots.

\begin{table}
\begin{tabular}{l|c|c|c} \multirow{2}{*}{**Team Name**} & \multirow{2}{*}{**Round 1**} & \multirow{2}{*}{**Round 2**} & **Total** \\  & & & **Points** \\ \hline AiRLIHockey & 15 & 18 & 33 \\ SpaceR & 9 & 12 & 21 \\ Air-HocKIT & 5 & 15 & 20 \\ AJoy & 9 & 6 & 15 \\ RL3\_polimi & 2 & 9 & 11 \\ GXU-LIPE & 2 & 1 & 3 \\ CONFIGMTEAM & 0 & 1 & 1 \\ \end{tabular}
\end{table}
Table 4: Results from the tournament stage

Figure 4: Ablation studies on the effect of each domain discrepancy on the submitted solutions. The graph reports the success rate for each task under different types of environment modifications

It is worth mentioning that the deployment phase was characterized by minor technical issues and required parameter tuning and adaptation for a successful deployment in the real system. Contrary to the simulation, real-world deployment is not a synchronous process, and the system is affected by delays. The delays, approximately 40ms, arise from gathering the observation, action computation, and execution. The deployment may lead to unsafe behaviors as the solutions are not trained in this environment, thus, trajectories that are reasonable in the simulation may become unsafe. Therefore, our safety layer is fundamental to avoid damaging the real platform. Details of the safety layer can be found in Appendix A.5

### Participants solution analysis

In this section, we will briefly discuss the solutions of the AiRLIHockey, SpaceR, Air-HocKIT, and RL3_polimi teams. Detailed descriptions of these solutions can be found in the Appendix A.6, A.7, A.8, and A.9 respectively. The AiRLIHockey solution, the winner of the challenge, is heavily based on classical robotics solutions and obtains highly precise and fast-hitting motions thanks to an optimization-based solution exploiting model predictive control (Jankowski et al., 2023). In this solution, learning is used mostly for state estimation and prediction as well as for the contact planner. The agent's high-level behavior is controlled by a state machine. The SpaceR solution, instead, is a flat policy approach based on Dreamer-V3 (Hafner et al., 2023). This solution exploits inverse kinematics to control directly in task space, avoiding the control in joint space. This approach also avoids any type of reward shaping and exploits instead sparse reward functions and self-play to solve the task. The Air-HocKIT solution is another RL-based solution, but using a state machine to coordinate multiple low-level RL policies, trained with PPO (Schulman et al., 2017). Differently from SpaceR, this team encodes domain knowledge in the reward function to achieve accurate, and safe low-level behaviors. The team tackles the sim-to-real gap w.r.t. the robot's motion with system identification. Finally, the RL3_polimi team also employs a hierarchical control architecture. Their state machine selects skills that are learned either with Deep RL (SAC (Haarnoja et al., 2024) combined with ATACOM (Liu et al., 2022)) or by learning rule-based policies with the PGPE algorithm (Sehnke et al., 2008), following the same scheme of Likmeta et al. (2020).

From the competition's results, it is clear that more structured solutions exploiting robotics priors perform better than unstructured ones. Experience with robotics systems is also extremely beneficial to encode prior information effectively in the reward functions or to perform parameter identification and bridge the sim-to-real gap. In general, data-driven methods are useful for learning models of the environment, for avoiding costly online optimization via behavioral cloning, and for learning dynamic behaviors. Another result is that for complex tasks, using advanced planning methods for control yields high-performance solutions that are still difficult to discover using data-driven methods, even with the help of structured reward functions. While these advanced planning methods are often computationally demanding, for limited tasks it is still possible to distill them through behavioral cloning. On one side, this shows that there are still open questions on learning for control. On the other side, this opens opportunities to use datasets generated with optimal control in combination with modern behavioral cloning and foundation models. Finally, we observe that hardcoding the high-level policy with a state machine is more beneficial than employing a flat policy. This result shows the effectiveness of specialized models against general behavioral models, at least for dynamic tasks like the one presented in this challenge.

### Key research problems

As a final point, we exploited the very diverse backgrounds of our participants to identify the most important research questions tackled within this challenge. Indeed, the teams highlighted a wide variety of research problems such as: i. the **sim-to-real gap** and online adaptation, from the perspective of online learning or system identification; ii. the complex **contact dynamics** between the puck and the mallet; iii. the competitive **multi-agent setting** and the necessity to adapt to the opponent; iv. the design of **curriculum learning** approaches.

However, most teams highlighted two main challenges, even if analyzed from different perspectives. The first one is the **constraint satisfaction** problem. The RL3_polimi team views this problem from the point of view of exploration of RL algorithms. In particular, they view this problem as a reward-shaping problem and point out that in the future, this could be seen through the lenses of multi-objective RL. Coming from a more robotics background, the SpaceR team instead views this problem as a low-level control problem. The Air-HocKIT team's point of view can be seen as in between the previous two, as their idea is to combine reward shaping and action space selection.

The second key research question identified is the **decision-making at different time scales** problem. Again, this issue has been seen in different ways. The AiRLIHockey team frames this problem as a control and state prediction problem, where the fast controller requires long-term prediction to act efficiently. The SpaceR and RL3_polimi teams see this issue from the perspective of Hierarchical RL.

### Limitations

Both the challenge and the proposed analysis are affected by limitations. In particular, in our setting, we assume a good perception system: we track the puck using the Optitrack motion capture system. This assumption neglects one of the most important problems in robotics, namely perception and action-perception coupling. However, while this is a clear limitation that distances our system from generic real-world robotic tasks, we believe that the setting is already too challenging for machine learning and robotics researchers, therefore reducing the scope of the work is reasonable to obtain good solutions. In future iterations of the challenge, we may consider a more complex perception problem or add some tasks requiring direct control from camera images.

Regarding the analysis, unfortunately, our conclusions suffer from a limited sample size. This is due to the particularly challenging tasks and the lack of availability of strong baselines. In the future, we hope to lower the entrance barrier to allow for more competitors. Another issue of the analysis is that currently there is not much understanding of the importance of high-level policy. Indeed, the only insight on this aspect is that a faulty high-level policy may cause severe safety issues, resulting in game losses. We hope that, with better low-level skills baselines, we will be able to dive deeper into the high-level tactics and understand how relevant are the opponent's playing style adaptation techniques.

Finally one of the major limitations of the 2023 edition of the challenge is the limited real-world deployment due to the limited availability of the hardware, the high complexity of the task, and the complexity of the setup. This made it impossible to perform learning and data collection directly from the real world and limited our capability of testing solutions. Unfortunately, our current robotic setup neither can easily support concurrent learning of multiple solutions nor the extensive deployment of learned solutions as done in the TOTO benchmark (Zhou et al., 2023). This is because, differently from other benchmarks, our chosen task is very complex and our system setup requires many different robotics solutions to work together. All these systems may be prone to failure and require robotic engineer supervision. For future iterations, we might consider providing real-world data and we will make the final real-world stage part of the competition. However, it is worth noting that our sim-to-sim approach closely predicts the results we can expect on real-world deployment, showing that our benchmarking approach is a viable way to evaluate real-world transfer performance.

## 4 Conclusion

In this paper, we presented a retrospective on the Robot Air Hockey Challenge. This challenge tries to bridge the gap between machine learning and robotics researchers, focusing on a domain particularly challenging for both research areas. In particular, our challenge focused on some key aspects that are often neglected by typical benchmarks used by machine learning researchers such as safety issues, dynamic environments requiring highly reactive policies and strict computational requirements, heavy sim-to-real gap, limited amount of data that can be collected from real robots, and competitive multiagent settings. While the air hockey task is particularly challenging, we were able to deploy a satisfactory number of solutions employing a variety of techniques also in the real robot. Unsurprisingly, the solution relying more on classical robotics techniques, namely model predictive control, outperformed all of the other methodologies, both in the tournament stage and in real-world deployment. Indeed, in robotics, exploiting good priors is still key to obtaining state-of-the-art performances. However, machine learning approaches are already competitive, and we are confident that a combination of data exploitation and inductive biases will allow the data-driven solution to surpass more classical baselines.

## Acknowledgments and Disclosure of Funding

The Robot Air Hockey Challenge was sponsored by the Huawei group. Organizers are partially supported by the China Scholarship Council (No. 201908080039), the EU's Horizon Europe project ARISE (Grant no.: 101135959), and the German Federal Ministry of Education and Research (BMBF) within the subproject "Modeling and exploration of the operational area, design of the AI assistance as well as legal aspects of the use of technology" of the collaborative KIARA project (grant no. 13N16274).

We acknowledge the participants who contributed to the challenge and provided insightful reports of the challenge:

* AiRLHockey -- Julius Jankowski, Ante Maric, Sylvain Calinon
* Air-HocKIT -- Mustafa Enes Batur, Vincent de Bakker, Atalay Donat, Omer Erdinc Yagmurlu, Marcus Fiedler, Zeqi Jin, Dongxu Yang, Hongyi Zhou, Xiaogang Jia, Onur Celik, Fabian Otto, Rudolf Lioutikov, Gerhard Neumann
* SpaceR -- Andrej Orsula, Miguel Olivares-Mendez
* RL3_polimi -- Amarildo Likmeta, Amirhossein Zhalehmehrabi, Thomas Jean Bernard Bonenfant, Alessandro Montenegro, Davide Salaorni, Marcello Restelli

## References

* Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Generalized autoregressive pretraining for language understanding. _Advances in neural information processing systems_, 32, 2019.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Silver et al. (2016) David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* Vinyals et al. (2019) Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* Croitoru et al. (2023) Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* Prenger et al. (2019) Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network for speech synthesis. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3617-3621. IEEE, 2019.
* Kong et al. (2020) Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. _Advances in Neural Information Processing Systems_, 33:17022-17033, 2020.
* Brohan et al. (2023) Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. In _Robotics: Science and Systems_, 2023.
* Krizhevsky et al. (2014)Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In _Conference on Robot Learning_, pages 2165-2183. PMLR, 2023.
* O'Neill et al. (2023) Abby O'Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, et al. Open x-embodiment: Robotic learning datasets and rt-x models. _arXiv preprint arXiv:2310.08864_, 2023.
* Li et al. (2024) Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-language foundation models as effective robot imitators. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=1FYj0oibGR.
* Team et al. (2024) Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. _arXiv preprint arXiv:2405.12213_, 2024.
* Kim et al. (2024) Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan P Foster, Pannag R Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An open-source vision-language-action model. In _8th Annual Conference on Robot Learning_, 2024. URL https://openreview.net/forum?id=2MmDoG2AE6.
* Firoozi et al. (2023) Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. _The International Journal of Robotics Research_, page 02783649241281508, 2023.
* Peres et al. (2020) Ricardo Silva Peres, Xiaodong Jia, Jay Lee, Keyi Sun, Armando Walter Colombo, and Jose Barata. Industrial artificial intelligence in industry 4.0-systematic review, challenges and outlook. _IEEE access_, 8:220121-220139, 2020.
* Dzedzickis et al. (2021) Andrius Dzedzickis, Jurga Subacute-Zemaitiene, Ernestas Sutinys, Urte Samukaite-Bubniene, and Vytautas Bucinskas. Advanced applications of industrial robotics: New trends and possibilities. _Applied Sciences_, 12(1):135, 2021.
* Kawato et al. (1994) Mitsuo Kawato, Francesca Gandolfo, Hiroaki Gomi, and Yasuhiro Wada. Teaching by showing in kendama based on optimization principle. In _International Conference on Artificial Neural Networks_, pages 601-606. Springer, 1994.
* Kober and Peters (2008) Jens Kober and Jan Peters. Policy search for motor primitives in robotics. _Advances in neural information processing systems_, 21, 2008.
* Ploeger et al. (2021) Kai Ploeger, Michael Lutter, and Jan Peters. High acceleration reinforcement learning for real-world juggling with binary rewards. In _Conference on Robot Learning_, pages 642-653. PMLR, 2021.
* Ploeger and Peters (2022) Kai Ploeger and Jan Peters. Controlling the cascade: Kinematic planning for n-ball toss juggling. In _2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 1139-1144. IEEE, 2022.
* Origalski et al. (2021) Felix von Drigalski, Devwrat Joshi, Takayuki Murooka, Kazutoshi Tanaka, Masashi Hamaya, and Yoshihisa Ijiri. An analytical diabolo model for robotic learning and control. In _2021 IEEE International Conference on Robotics and Automation (ICRA)_, pages 4055-4061. IEEE, 2021.
* Zaidi et al. (2023) Zulfiqar Zaidi, Daniel Martin, Nathaniel Belles, Viacheslav Zakharov, Arjun Krishna, Kin Man Lee, Peter Wagstaff, Sumedh Naik, Matthew Sklar, Sugju Choi, et al. Athletic mobile manipulator system for robotic wheelchair tennis. _IEEE Robotics and Automation Letters_, 8(4):2245-2252, 2023.
* Haarnoja et al. (2024) Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H Huang, Dhruva Tirumala, Jan Humplik, Markus Wulfmeier, Saran Tunyasuvunakool, Noah Y Siegel, Roland Hafner, et al. Learning agile soccer skills for a bipedal robot with deep reinforcement learning. _Science Robotics_, 9(89):eadi8022, 2024.

Katharina Mulling, Jens Kober, and Jan Peters. A biomimetic approach to robot table tennis. _Adaptive Behavior_, 19(5):359-376, 2011.
* Buchler et al. (2022) Dieter Buchler, Simon Guist, Roberto Calandra, Vincent Berenz, Bernhard Scholkopf, and Jan Peters. Learning to play table tennis from scratch using muscular robots. _IEEE Transactions on Robotics_, 2022.
* Cartoni et al. (2020) Emilio Cartoni, Francesco Mannella, Vieri Giuliano Santucci, Jochen Triesch, Elmar Rueckert, and Gianluca Baldassarre. Real-2019: Robot open-ended autonomous learning competition. In Hugo Jair Escalante and Raia Hadsell, editors, _Proceedings of the NeurIPS 2019 Competition and Demonstration Track_, volume 123 of _Proceedings of Machine Learning Research_, pages 142-152. PMLR, 08-14 Dec 2020. URL https://proceedings.mlr.press/v123/cartoni20a.html.
* Song et al. (2021) Seungmoon Song, Lukasz Kidzinski, Xue Bin Peng, Carmichael Ong, Jennifer Hicks, Sergey Levine, Christopher G Atkeson, and Scott L Delp. Deep reinforcement learning for modeling human locomotion control in neuromechanical simulation. _Journal of neuroengineering and rehabilitation_, 18:1-17, 2021.
* Gurtler et al. (2023) Nico Gurtler, Sebastian Blaes, Pavel Kolev, Felix Widmaier, Manuel Wuthrich, Stefan Bauer, Bernhard Scholkopf, and Georg Martius. Benchmarking offline reinforcement learning on real-robot hardware. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=3k5CUGDLNdd.
* Funk et al. (2021) Niklas Funk, Charles Schaff, Rishabh Madan, Takuma Yoneda, Julen Urain De Jesus, Joe Watson, Ethan K Gordon, Felix Widmaier, Stefan Bauer, Siddhartha S Srinivasa, et al. Benchmarking structured policies and policy optimization for real-world dexterous object manipulation. _IEEE Robotics and Automation Letters_, 7(1):478-485, 2021.
* Zhou et al. (2023) Gaoyue Zhou, Victoria Dean, Mohan Kumar Srirama, Aravind Rajeswaran, Jyothish Pari, Kyle Hatch, Aryan Jain, Tianhe Yu, Pieter Abbeel, Lerrel Pinto, et al. Train offline, test online: A real robot learning benchmark. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 9197-9203. IEEE, 2023.
* Caggiano et al. (2023) Vittorio Caggiano, Guillaume Durandau, Huawei Wang, Alberto Chiappa, Alexander Mathis, Pablo Tano, Nisheet Patel, Alexandre Pouget, Pierre Schumacher, Georg Martius, et al. Myochallenge 2022: Learning contact-rich manipulation using a musculoskeletal hand. In _NeurIPS 2022 Competition Track_, pages 233-250. PMLR, 2023.
* Yenamandra et al. (2023) Sriram Yenamandra, Arun Ramachandran, Mukul Khanna, Karmesh Yadav, Devendra Singh Chaplot, Gunjan Chhablani, Alexander Clegg, Theophile Gervet, Vidhi Jain, Ruslan Partsev, Ram Ramrakhya, Andrew Szot, Tsung-Yen Yang, Aaron Edsinger, Charlie Kemp, Binit Shah, Zsolt Kira, Dhruy Batra, Roozbeh Mottaghi, Yonatan Bisk, and Chris Paxton. The homerrobot open vocab in manipulation challenge. In _Thirty-seventh Conference on Neural Information Processing Systems: Competition Track_, 2023. URL https://aihabitat.org/challenge/2023_homerrobot_ovmm/.
* Bishop and Spong (1999) Bradley E Bishop and Mark W Spong. Vision based control of an air hockey playing robot. _IEEE Control Systems Magazine_, 19(3), 1999.
* Bentivegna et al. (2004a) Darrin C Bentivegna, Christopher G Atkeson, and Gordon Cheng. Learning tasks from observation and practice. _Robotics and Autonomous Systems_, 47(2-3):163-169, 2004a.
* Bentivegna et al. (2004b) Darrin C Bentivegna, Christopher G Atkeson, ALES UDE, and Gordon Cheng. Learning to act from observation and practice. _International Journal of Humanoid Robotics_, 1(04):585-611, 2004b.
* Namiki et al. (2013) Akio Namiki, Sakyo Matsushita, Takahiro Ozeki, and Kenzo Nonami. Hierarchical processing architecture for an air-hockey robot system. In _2013 IEEE International Conference on Robotics and Automation_, pages 1187-1192. IEEE, 2013.
* Igeta and Namiki (2017) Kazuki Igeta and Akio Namiki. Algorithm for optimizing attack motions for air-hockey robot by two-step look ahead prediction. In _IEEE/SICE International Symposium on System Integration_, pages 465-470, 2017. ISBN 9781509033294. doi: 10.1109/SII.2016.7844042.
* Igeta et al. (2018)Puze Liu, Davide Tateo, Haitham Bou-Ammar, and Jan Peters. Efficient and reactive planning for high speed robot air hockey. In _2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, pages 586-593. IEEE, 2021.
* Tadokoro et al. [2022] Koichiro Tadokoro, Shotaro Fukuda, and Akio Namiki. Development of air hockey robot with high-speed vision and high-speed wrist. _Journal of Robotics and Mechatronics_, 34(5):956-964, 2022.
* Igeta and Namiki [2015] Kazuki Igeta and Akio Namiki. A decision-making algorithm for an air-hockey robot that decides actions depending on its opponent player's motions. In _2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)_, pages 1840-1845. IEEE, 2015.
* AlAttar et al. [2019] Ahmad AlAttar, Louis Rouillard, and Petar Kormushev. Autonomous air-hockey playing cobot using optimal control and vision-based bayesian tracking. In _International Conference Towards Autonomous Robotic Systems (TAROS)_, 2019. ISBN 9783030253318. doi: 10.1007/978-3-030-25332-5_31.
* Liu et al. [2022] Puze Liu, Davide Tateo, Haitham Bou Ammar, and Jan Peters. Robot reinforcement learning on the constraint manifold. In _Conference on Robot Learning_, pages 1357-1366. PMLR, 2022.
* Xie et al. [2020] Annie Xie, Dylan P. Losey, Ryan Tolsma, Chelsea Finn, and Dorsa Sadigh. Learning latent representations to influence multi-agent interaction. In _Conference on Robot Learning (CoRL)_, 2020.
* Kicki et al. [2023] Piotr Kicki, Puze Liu, Davide Tateo, Haitham Bou-Ammar, Krzysztof Walas, Piotr Skrzypczynski, and Jan Peters. Fast kinodynamic planning on the constraint manifold with deep neural networks. _IEEE Transactions on Robotics_, 2023.
* Liu et al. [2024] Puze Liu, Haitham Bou-Ammar, Jan Peters, and Davide Tateo. Safe reinforcement learning on the constraint manifold: Theory and applications. _arXiv preprint arXiv:2404.09080_, 2024.
* Chuck et al. [2024] Caleb Chuck, Carl Qi, Michael J Munje, Shuozhe Li, Max Rudolph, Chang Shi, Siddhant Agarwal, Harshit Sikchi, Abhinav Peri, Sarthak Dayal, et al. Robot air hockey: A manipulation testbed for robot learning with reinforcement learning. _arXiv preprint arXiv:2405.03113_, 2024.
* Jankowski et al. [2023] Julius Jankowski, Lara Brudermuller, Nick Hawes, and Sylvain Calinon. Vp-sto: Via-point-based stochastic trajectory optimization for reactive robot behavior. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 10125-10131, 2023. doi: 10.1109/ICRA48891.2023.10160214.
* Hafner et al. [2023] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.
* Sehnke et al. [2008] Frank Sehnke, Christian Osendorfer, Thomas Ruckstiess, Alex Graves, Jan Peters, and Jurgen Schmidhuber. Policy gradients with parameter-based exploration for control. In _Artificial Neural Networks-ICANN 2008: 18th International Conference, Prague, Czech Republic, September 3-6, 2008, Proceedings, Part I 18_, pages 387-396. Springer, 2008.
* Likmeta et al. [2020] Amarildo Likmeta, Alberto Maria Metelli, Andrea Tirinzoni, Riccardo Giol, Marcello Restelli, and Danilo Romano. Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving. _Robotics and Autonomous Systems_, 131:103568, 2020. ISSN 0921-8890. doi: https://doi.org/10.1016/j.robot.2020.103568. URL https://www.sciencedirect.com/science/article/pii/S0921889020304085.
* Florence et al. [2022] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In _Proceedings of the 5th Conference on Robot Learning_, volume 164 of _Proceedings of Machine Learning Research_, pages 158-168, 2022.
* Kalman [1960] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. _Journal of Basic Engineering_, 82(1):35-45, 03 1960.
* Kalman and Van der Vaart [2007]* Hansen [2023] Nikolaus Hansen. The cma evolution strategy: A tutorial, 2023.
* Raffin et al. [2021] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. _CoRR_, abs/1801.01290, 2018. URL http://arxiv.org/abs/1801.01290.
* Narvekar et al. [2020] Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E. Taylor, and Peter Stone. Curriculum learning for reinforcement learning domains: A framework and survey. _CoRR_, abs/2003.04960, 2020. URL https://arxiv.org/abs/2003.04960.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We strongly believe that all claims in the paper are substantiated by the results of our competition and by the additional analysis performed on the resulting solutions. In any case, we are aware of the limited scope of the analysis and we highlight these shortcomings in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We explicitly add a limitation section to the paper. We tried our best to cover all the limitations of the challenge and the analysis presented in this retrospective. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [N/A] Justification: As this is a challenge retrospective, there is no theoretical contribution, hence, there are no proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [No] Justification: We disclose most of the information to reproduce the results and most of the codebase is public. However, we do not disclose the implementation of the modified simulator. the reason is twofold: on one hand, we plan to build on the same simulator in the future and we would like, in future editions of the challenge, to prevent participants from exploiting the code knowledge. On the other hand, the idea of the modified simulator is to provide a simulation of the sim-to-real gap, ensuring no no-one has access to the ground truth. However, we will allow anyone interested in evaluating their solution on our environment upon request. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: While we provide the full code, instructions, videos, and reports from every participant, not all solutions are available to the public (some of the competition participants are willing to publish a paper in the future). The air hockey challenge code is open source, and we provide extensive documentation to use the environment. The only part not available to the public is the modified environment. Indeed, we are willing to propose this competition again in the future and we do not want to disclose the modified environment at this stage. However, we will maintain the codebase, and we can rerun the experimental data if possible. In the future, once the current simulator is no longer relevant to the competition, we will publish also the modified simulator. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In the appendix, there is a detailed description of the solution of the teams that have participated in the challenge and submitted the final report. We provide extensive descriptions of all the settings of the challenge and the code is open source. Guidelines: * The answer NA means that the paper does not include experiments.

* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: while this challenge report doesn't contain an in-depth statistical analysis due to the limited sample size, we remark in multiple parts of the paper the limits of this analysis and whenever the results are affected by statistical errors. Indeed, as we use the success rate as a metric, it is difficult to provide meaningful confidence intervals. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The evaluation of the qualifying stage and the tournament stage are conducted on the Huawei Cloud Server. We have provided the computer resources in Appendix A.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: [Yes] Justification: The people involved in the challenge received their compensation regulated by German law, and their contribution is properly stated and acknowledged in this paper. We comply with all data-related concerns and we believe that the possible negative impact of this study is negligible. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: we discuss in detail in the introduction why our challenge can be helpful to have a positive impact on society in the long term, as it helps to bridge the gap between the machine learning community and the robotics one. While bringing the robots to the real world may have some shortcomings, we avoid discussing these as our challenge is very limited in scope (we only highlight some specific issues of the robot learning problem), and the possible negative impact of deployment of robots in the real world are extremely generic, mostly unrelated to the challenge, and well-covered in previous literature and political discussion. On the contrary, we focus on the issue of respecting safety for the deployment in the real world, wich is for sure a desirable property to mitigate the negative impact of machine learning in the real world. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [N/A]Justification: No sensitive data model is published, only data and simulated environment of the air hockey setting, which, to our judgment, should not raise any misuse concerns. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [N/A] Justification: We don't use existing assets for this work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The air hockey challenge code is open source, and we provide extensive documentation to use the environment. The only part not available to the public is the modified environment. We will not publish this environment as it may serve as the basis for the next iterations of the challenge. However, in the future, we may release the old versions of the modified simulator. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used.

* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [N/A] Justification: We don't perform experiments with human subjects. Regarding the participation to the challenge, the modalities of participation are well described in the challenge website. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [N/A] Justification: There is no human experiment in the air hockey challenge. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

Appendix

### Simulated environment details

The details of the simulated environment are presented in Table 1 in the main paper. The observation of the environment consists of the joint position and velocity of the robot arm as well as the puck's Cartesian position and velocity, which add up to 20 dimensions in total. If the environment includes an opponent, the Cartesian position of his mallet is appended to the observation space, resulting in 23 dimensions. The simulation can be controlled with multiple different action interpolation approaches. For pure position control, we offer linear or quadratic interpolation. For position and velocity control, the commands can be interpolated by cubic, quartic, or linear interpolation. If the control command also includes acceleration quintic interpolation is used. Finally it is also possible to bypass the trajectory planar by directly supplying position, velocity and acceleration commands at 1000Hz. The robot specification, including joint limits, are presented in Table 5. The control diagram is depicted in Fig. 5. Extensive details about the environments, tasks, and interfaces together with installation instruction can be found in the online documentation at https://air-hockey-challenges-docs.readthedocs.io/en/latest/.

### Computation resources

The simulated evaluation is conducted on the Huawei Cloud Server using a modified environment. The cloud server is equipped with an Intel(R) Xeon(R) Gold 6278C CPU @ 2.60GHz and 48 GB RAM. To make the competition fairer and keep it accessible to a wider range of enthusiasts, the experiments are fully evaluated on the CPU. The computation time varies across the teams as different methods are used. Since the competition also evaluates the computation time per step, the experiments of the qualifying stage take around 3.5 hours, and the evaluation of the tournament takes around 40 minutes.

Figure 5: Control diagram of the simulated environment

\begin{table}
\begin{tabular}{l|c}
**Specification** & **values per joint** \\ \hline Joint upper limit (rad) & [ 2.967, 2.09, 2.967, 2.094, 2.967, 2.094, 3.054] \\ Joint lower limit (rad) & [-2.967, -2.094, -2.967, -2.094, -2.967, -2.094, -3.054] \\ Joint velocity limit (rad/s) & \(\pm\) [ 1.483, 1.483, 1.745, 1.308, 2.268, 2.356, 2.356] \\ Initial joint position (rad) & [ 0, -0.1960, 0, -1.8436, 0, 0.9704, 0] \\ Initial joint velocity (rad/s) & [ 0, 0, 0, 0, 0, 0, 0] \\ \end{tabular}
\end{table}
Table 5: Specification of robot and environment variables

[MISSING_PAGE_EMPTY:24]

[MISSING_PAGE_EMPTY:25]

### Real-world Deployment

In this section, we highlight the changes we performed to adjust the solutions for real-world deployment.

To increase safety we designed a compliant end effector, composed of a metal rod connected to a gas spring. This rod is connected to a (non-actuated) universal joint, that allows for a larger workspace. Finally, the mallet itself is compliant, allowing for a couple of centimeters of additional compression, thanks to a foam core.

As already discussed, the observations were gathered from the robot's motor encoders and an Optitrack motion capture system. We use the information about the mallet position and orientation from the forward kinematics of the joint position and the Optitrack motion capture system to check if it is flipped, pressed too far into the table, or hitting the table boundaries. Indeed, the presence of the universal joint may cause unwanted flipping of the mallet.

Before executing an action it is checked for safety to prevent the robot from hitting the table surface or boundaries. We also check if the velocity and acceleration required to satisfy the requested setpoint command are achievable by the robot. To prevent the robot from pushing too hard into the table or lifting the end-effector and, consequently, flipping the mallet, we use an Inverse Kinematics approach to adjust the action, ensuring that the end-effector will keep the table height. This controller can correct small errors and lead to a much smoother behavior of the robot but cannot correct larger errors or large vertical velocities, mainly due to the robot's physical limitations. When a mallet flip is detected, the safety layer interferes, lifts the mallet, moves the robot back to its initial configuration, and restarts that robot's episode, and then the agent can continue performing the current task. In the other safety-critical cases, which occur less frequently, the game is paused and restarted after the robots have been moved to their initial position.

The control system generates a trajectory for the next 100ms using the point action provided by the agent. The trajectory starts at the current state of the robot at t=0, the drawn action is set at t=20ms and it is extended for the next 80ms with a constant velocity model. Normally, a new trajectory should arrive after 20ms, and replace the currently executed one, however, this solution prevents jerky motion in case of action delays.

The torque output is computed by an Active Disturbance Rejection Controller (ADRC) with a feed-forward term. We interpolate the trajectory linearly in position, velocity, and acceleration. While the resulting trajectory is not physically consistent and impossible to reproduce, we experimentally found that, for high-frequency trajectories, this behavior reduces oscillating torques compared to quintic polynomial interpolation, commonly used in standard robotics control toolkits.

### Details of the AiRLIHockey solution

The _AiRLIHockey_ agent addresses the uncertainty and long-horizon nature of air hockey by relying on an optimal control formulation of primitive skills such as _shooting_, _defense_, and _preparation_ to generate reactive behavior. While defense and preparation can be addressed through simple heuristics, shooting requires reasoning over longer prediction horizons. Thus, to operate at a 50 Hz replanning rate, offline precomputation is leveraged with an energy-based model trained to quickly recover shooting actions. An additional key to the performance of the solution lies in fast optimization of subsequent mallet trajectories through zero-order methods combined with low-dimensional trajectory

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c}
**Team Name** & **Wins** & **Loses** & **Draws** & **Goals** & **Goals** & **Penalty** \\  & & & & **Socred** & **Received** & **Points** \\ \hline AiRLIHockey & 11 & 0 & 0 & 270 & 8 & 744.0 \\ SpaceR & 7 & 4 & 0 & 31 & 97 & 859.0 \\ Air-HocKIT & 6 & 3 & 2 & 232 & 40 & 1425.0 \\ AJoy & 5 & 6 & 0 & 10 & 357 & 396.5 \\ RL3\_polimi & 3 & 6 & 2 & 25 & 99 & 1669.0 \\ GXU-LIPE & 0 & 8 & 3 & 92 & 49 & 3752.0 \\ CONFIRMTEAM & 0 & 5 & 1 & 59 & 69 & 2186.0 \\ \end{tabular}
\end{table}
Table 11: Aggregate metrics for the tournamentrepresentations [Jankowski et al., 2023]. These trajectories further maximize the shooting velocity while ensuring task space and joint constraints. Fig. 6 illustrates the framework.

#### a.6.1 State Estimation & Prediction

Puck dynamics are modelled as piecewise (locally) linear with three different modes: **(a)**_The puck freely sliding on the table_, **(b)**_The puck is in contact with the wall_, and **(c)**_The puck is in contact with the mallet_. Data collected in simulation is used to learn linear parameters \(\bm{A}_{i},\bm{B}_{i}\), and an individual covariance matrix \(\bm{\Sigma}_{i}\) representing the process noise for each mode. This provides a probability distribution over puck trajectories

\[\Pr_{i}(\bm{s}_{k+1}^{p}|\bm{s}_{k}^{p},\bm{s}_{k}^{m})=\mathcal{N}\Big{(}\bm{ A}_{i}\bm{s}_{k}^{p}+\bm{B}_{i}\bm{s}_{k}^{m},\bm{\Sigma}_{i}\Big{)},\] (1)

with puck state \(\bm{s}_{k}^{p}=(\bm{x}_{k}^{p},\bm{\dot{x}}_{k}^{p})\) and mallet state \(\bm{s}_{k}^{m}=(\bm{x}_{k}^{m},\bm{\dot{x}}_{k}^{m})\). While each of the modes is a Gaussian distribution, marginalizing over the mallet state to propagate the puck state is not possible due to the discontinuity stemming from contacts. Therefore, an extended Kalman filter is utilized to estimate puck states across different timesteps for any given mallet state.

#### a.6.2 Contact Planning

Depending on the desired behavior, the robot is required to generate a _shooting_, _defense_, or _preparation_ contact plan and corresponding trajectory. These subproblems are addressed separately.

**Shooting.** Due to process and observation noise, contact planning for the shooting behavior is posed as a stochastic optimal control problem, and simplified through the following assumptions:

1. The initial timestep of the planning horizon \(k=0\) corresponds to the time of contact between the puck and mallet.
2. The cost metric is evaluated only for the final timestep of the horizon \(k=K\) where the puck is at the goal line.
3. Time of contact is preset and is not a decision variable.
4. Mallet velocity at contact time is always maximized in the shooting direction while respecting dynamic constraints.

Keeping these assumptions in mind, a stochastic optimal control problem is formulated with the aim to minimize a function of the probability distribution of the puck state. To compute the cost of a candidate contact state, the probability distribution over puck trajectories is approximated as described in Section A.6.1. The cost is defined as a weighted sum of the probability of scoring a goal and expected puck velocity at the goal line, with an additional penalty term on trajectories with a low probability of entering the goal. Different trade-offs between puck velocity and scoring probability can be achieved by tuning the cost coefficients. The input space is further reduced to a single dimension by parametrizing the mallet position as a shooting angle relative to the puck. To solve the posed optimization problem at the same control rate of \(50\) Hz as the mid-level controller, the heavy computational burden is transferred to an offline phase in which optimal plans for a variety

Figure 6: Overview of the _AiRLHockey_ agent. Starting from noisy observations, the puck state is estimated subject to learned model parameters. Different behaviors are triggered by a heuristic state machine based on estimated puck states. Contact and trajectory planners subsequently generate a desired action sequence that is tracked on the joint level with constrained quadratic programming.

of scenarios are collected and used to train an energy-based behavior cloning model (Florence et al., 2022) to rapidly recover optimal shooting angles \(\hat{a}\) online:

\[\hat{a}=\operatorname*{argmin}_{a\in\mathcal{A}}E_{\theta}(\bm{s}_{0}^{p},a),\] (2)

where \(\bm{s}_{0}^{p}\) represents an initial puck state. At runtime, this is achieved by iteratively sampling a number of candidate actions at each timestep with recentering and reductions on the sampling variance. Finally, the state-action pair with the lowest energy \(E_{\theta}\) is selected, as shown in Figure 7. Depending on the puck state, it can be observed that the optimal shooting angle produces bank shots as a consequence of rewarding high puck velocities at the goal line. This is due to the kinematics of the robot arm that is able to generate higher mallet velocities for bank shots while satisfying joint velocity limits.

**Defense & Preparation.** To generate a mallet state for deflecting the puck away from the goal, a sampling-based optimization technique is applied online. The objective of the optimization is to achieve a desired vertical puck velocity after contact (e.g. close to zero). The same optimization technique is used to prepare for a shot, with the goal of moving the puck toward the horizontal center by reflecting it against the wall. The target puck state is computed heuristically based on the puck position at the time of the contact.

#### a.6.3 Mallet Trajectory Planning & Control

The mid-level control layer is responsible for executing planned contacts without colliding with the table walls. The desired mallet state at a given point in time therefore serves as a constraint for the trajectory controller. For lower computational burden, the dimensionality of the decision variable is reduced by using a basis function trajectory parameterization that ensures most of the constraints (Jankowski et al., 2023). The utilized parameterization also minimizes an acceleration functional to generate smooth trajectories. Furthermore, since the basis functions are computed offline, a significant part of the computational burden is again transferred from online control loops. The final mallet velocity is used as a decision variable and the error w.r.t. to the desired velocity as the new objective. The optimal trajectory is generated stochastically by sampling a number of

Figure 7: _Above: overview of the interplay between the puck and mallet in scoring a goal. Given the mallet position and the estimated puck position, our framework generates a motion plan for the mallet such that the probability of scoring is maximized. Below: action space sampling for the displayed scenario. Dark blue points show samples at the initial timestep. Iterative recentering and variance reductions lead to the final samples shown in cyan. The red vertical line denotes the selected action._

candidate mallet velocities and evaluating the corresponding rollouts (see Fig. 7). Finally, the first mallet action leading to the lowest-cost trajectory is applied as a control reference.

### Details of the SpaceR solution

The SpaceR agent revolves around leveraging model-based RL to acquire a policy capable of playing the full game of robot air hockey. DreamerV3 [Hafner et al., 2023] is employed as the underlying algorithm to concurrently learn a world model while optimizing actor and critic networks from abstract trajectories generated by the model. The agent is guided towards optimal behaviour purely through sparse rewards corresponding to relevant score-affecting events that are not biased by a potentially complex reward shaping. In order to acquire a policy capable of competing against various approaches, self-play is employed to support the agent in discovering more robust strategies by exploiting and correcting any weaknesses in its prior behaviour.

#### a.7.1 Observation Space

The observation space of the agent is summarized in Table 12 and captures all low-dimensional environment states that are available to the participants, including the state of the controlled robot, opponent robot, and puck. Additionally, the duration of the puck spent on either side of the table is encoded in the observation space to provide the agent with time awareness and avoid faults. All positions are normalized to the range \([-1,1]\) based either on the joint limits or dimensions of the table. The orientation of the puck is encoded as sine and cosine of the angle to preserve its continuity, while the duration of the puck spent on either side of the table is normalized based on the time limit until a fault would be accumulated with the sign corresponding to the side of the table.

#### a.7.2 Action Space

The agent interacts with the environment through continuous high-level actions that express the absolute target position of the mallet as a 2D vector. The target position is normalized to the range \([-1,1]\) within the intersection of the table and the reachable workspace of the robot. The high-level actions are then mapped into lower-level joint space commands using the Inverse Jacobian method. Initially, the relative displacement of the mallet \((\Delta x,\Delta y,\Delta z)\) is determined from its current position to the target, with the position along the Z-axis constrained to the relative height of the table. The target joint displacements are calculated as \((\Delta\theta_{1},\Delta\theta_{2},...,\Delta\theta_{7})=J^{+}\cdot(\Delta x,\Delta y,\Delta z)\) using the pseudo-inverse of the Jacobian matrix \(J^{+}\) while emphasizing the translation along the Z-axis to maintain consistent contact of the mallet with the table. The joint displacements are clipped based on the joint velocity limits to ensure that the motion is dynamically feasible. Finally, the resulting joint positions and velocities are derived through linear interpolation over the duration of the control cycle.

#### a.7.3 Reward Function

The reward function that guides the behaviour of the SpaceR agent is designed to provide sparse signals corresponding to the primary score-affecting events of the game, namely scoring a goal, receiving a goal, and causing a fault. Although a positive reward could also be attributed when the opponent receives a fault, this event is not considered due to the potential ambiguity in credit assignment. The summary in Table 13 illustrates three distinct configurations of the reward function that were adopted to reinforce agents towards different strategies in the form of _balanced_, _aggressive_, and _defensive_ playstyles. The _balanced_ strategy is designed to encourage the agent to play a rounded game by scoring goals while defending their side of the table. By providing a higher reward for

\begin{table}
\begin{tabular}{l|l}
**State** & **Dimension** \\ \hline Participant — Joint positions & 7 (\(DoF\)) \\ Participant — Mallet position & 2 (\(x,y\)) \\ Opponent — Mallet position & 2 (\(x,y\)) \\ Puck position & 2 (\(x,y\)) \\ Puck orientation & 2 (\(sin,cos\)) \\ Fault timer & 1 (\(t\)) \\ \end{tabular}
\end{table}
Table 12: Observation space of the SpaceR agent scoring a goal, the _aggressive_ strategy incentivizes the agent to take more risks for an opportunity to score. In contrast, the _defensive_ strategy does not provide any reward for scoring a goal to encourage the focus on preventing the opponent from scoring.

#### a.7.4 Self-Play

To overcome the limitations of training solely against the baseline, the SpaceR agent employs a form of fictitious self-play to iteratively discover more robust strategies. Each agent is trained against a pool of opponents with frozen weights, with a new opponent uniformly sampled at the beginning of each episode. The pool is gradually expanded with a new model every \(1000\) episodes up to a total of \(25\) opponents. Additionally, the strategies described in the previous section are incorporated into self-play using a two-step procedure. First, three agents following the distinct strategies are trained using self-play against an opponent pool initialized with the baseline agent. Once these agents exhibit stable behaviour, their training is terminated while keeping a history of their checkpoints. Subsequently, a new agent is trained from scratch following the _balanced_ strategy with the opponent pool pre-filled with the checkpoints of the previously trained agents. In this way, the new agent is immediately exposed to a diverse pool of advanced opponents with expertise in different aspects of the game, enhancing the discovery of more robust strategies.

#### a.7.5 Multi-Strategy Ensemble

Recognizing the distinct strengths and weaknesses exhibited by different strategies, the SpaceR agent forms an ensemble that dynamically switches between them based on the estimated score of the match. By default, the _balanced_ strategy is used for the majority of the game. If the opponent is in the lead, the ensemble switches to the _aggressive_ strategy to increase the likelihood of scoring a goal through a more risky playstyle. Conversely, if the opponent is trailing by a significant margin, the _defensive_ strategy is activated to provide a safer alternative in preventing the opponent from scoring a goal. With this design, the intended effect of the ensemble is to maximize the chances of winning by adapting the playstyle of the agent to the current state of the game.

### Details of the Air-HocKIT solution

The Air-HocKIT agent was constructed by integrating five independent PPO Schulman et al. (2017) models, each specialized in handling specific scenarios: hit, fast defend, slow defend, close prepare, and far prepare. Governed by a hand-crafted state machine, the selection of an agent at each time step was dictated by the current observation. A Jacobian-based reset agent was implemented to recover the robot to its original joint configuration before switching to a new agent. Each sub-agent was trained separately with a customized reward function. The main ideas behind each reward function are outlined below.

#### a.8.1 Rewards Design for Agents

**Hit Agent**. The reward function for the hitting model consists of three separate stages. Before the robot hits the puck, moving the end effector towards the puck's position is rewarded. Following contact between the end effector and the puck, a larger reward, linearly scaled with the puck's x-velocity, is provided. Lastly, a substantial reward is granted for scoring a goal, multiplied by the puck's velocity and an additional base reward. The scoring reward is several magnitudes larger than all the previous step rewards, which becomes the main objective of the model after sufficient training

\begin{table}
\begin{tabular}{l|c|c|c}
**Strategy** & **Score a goal** & **Receive a goal** & **Cause a fault** \\ \hline Balanced (default) & \(+\frac{2}{3}\) & \(-1\) & \(-\frac{1}{3}\) \\ Aggressive & \(+1\) & \(-1\) & \(-\frac{1}{3}\) \\ Defensive & 0 & \(-1\) & \(-\frac{1}{3}\) \\ \end{tabular}
\end{table}
Table 13: Components of the SpaceR reward function that reinforce different strategiestime. The reward for the hitting agent is defined as

\[r_{hit}=\begin{cases}\max(0,\frac{p_{p}-p_{ce}}{|p_{p}-p_{ce}|}\cdot v_{ce})&\text {if }|v_{p}|<0.25\text{ and }p_{p,x}<0,\\ 10\cdot|v_{p}|&\text{if hit},\\ 2000+5000\cdot|v_{p}|&\text{if scored},\end{cases}\] (3)

where \(p_{p}\) and \(v_{p}\) represent the puck position and velocity, respectively, the \(p_{ee}\) and \(v_{ee}\) states the end-effector position and velocity.

**Defend Agents**. The two defend agents are separated by the incoming puck's velocity. The reward calculation for the slow defend strategy involves two parts. In the first part of the reward in (4), a modest positive reward is granted when the end-effector contacts the puck for the first time. The value consists of a constant term and an exponential bonus term to encourage smaller puck velocity after contact. The reward for the slow defend agent is defined as

\[r_{\text{defend\_slow}}=\begin{cases}30+100^{1-0.25|v_{p}|}&\text{if }v_{p}>-0.2\text{ and ee touches puck for the first time},\\ \cdots+70&\text{if }|v_{p}|<0.1\text{ and }-0.7<p_{p,x}<-0.2\text{ and }t=T,\\ 0.01&\text{otherwise}.\end{cases}\] (4)

A binary reward was used for the fast-defend agent. The agent receives a negative reward with a value of \(-100\) for being scored by the opponent. The reward remains 0 for all the other cases.

**Prepare Agents**. The two preparation agents are distinguished by the puck's proximity to our goal along the x-direction. For the close preparation, the agent is rewarded for moving the puck to a pre-defined target position, e.g., \((-0.5,0)\) in our case, plus a small reward for moving the end-effector towards the puck. Additionally, a large reward of \(2000\) is granted when the success criterion is fulfilled. The reward for close preparation is defined as

\[r_{\text{proximity}}=\begin{cases}\max(0,\frac{p_{p}-p_{ce}}{|p_{p}-p_{ce}|} \cdot v_{ee})&\text{if }|v_{p}|<0.25\text{ and }p_{p,x}<0,\\ 0&\text{otherwise},\end{cases}\] (5)

\[r_{\text{bouns}}=\begin{cases}2000&\text{if }|v_{p}|<0.5,-0.65<p_{p,x}<-0.35 \text{ and }-0.4<p_{p,y}<0.4,\\ 0&\text{otherwise},\end{cases}\] (6)

\[r_{\text{close\_prepare}}=r_{\text{proximity}}+r_{\text{bouns}}+10\max(0,\min( 0.5,\frac{(-0.5,0)^{T}-p_{p}}{|(-0.5,0)^{T}-p_{p}|}\cdot v_{p})).\] (7)

In contrast, the far prepare agent aims solely to return the puck to the opponent while avoiding faults. This approach stems from our observation that the PPO algorithm encounters challenges when exploring action sequences requiring the end effector to first maneuver to the puck's back-side for preparation. As a result, the reward function (8) is identical to the hit reward with the caveat that the large reward is bound to getting the puck far enough into opponent territory without any puck velocity bonus, upon which the episode ends. The reward function for far preparation is given as

\[r_{\text{far\_prepare}}=\begin{cases}\max(0,\frac{p_{p}-p_{ce}}{|p_{p}-p_{ce}|} \cdot v_{ee})&\text{if }|v_{p}|<0.25\text{ and }p_{p,x}<0,\\ 3000&\text{if }p_{p,x}>0.2,\\ 10\cdot|v_{p}|&\text{otherwise}.\end{cases}\] (8)

#### a.8.2 State Machine

An overview of the hand-crafted state machine is provided in Figure 8.

#### a.8.3 Mitigating Sim2Real Gap

**Observation Noise.** The observation noise during evaluation consists of additive noise on the puck positions and velocities and loss of puck tracking. The latter means that the observed puck position stays constant and hence, the observed puck velocity is zero. This is a major change for the agent as this case does not appear during training. In the Air-HocKIT solution, the additive noise and loss of tracking are modelled in the training environment. The additive noise was modeled with a Gaussian distribution that is estimated with maximum likelihood from observed and smoothed puck trajectories. Additionally, a Kalman filter is applied to the observations (Kalman, 1960) and provide the estimatedmean position as observation to the agent. The observed puck position and the estimated positions are provided in case tracking of the puck is lost. The puck's velocity is estimated by using the finite differences of the estimated puck position.

Model dynamics and controller characteristics.For the same initial conditions, the observed robot movements during evaluation differ from the movement during the training environment. In recorded data, this difference can be up to 1cm, which is critical for tight constraints such as staying within 2cm of the table surface. This mismatch can be explained by differences in the model and controller parameters, including masses, friction coefficients, damping coefficients, and proportional and differential gains of the controller. Air-HocKIT solution includes estimating the correct values of these entities by framing it as a black-box optimization problem. The objective is to minimize the differences between observed and simulated joint movements. This problem was solved using the Covariance Matrix Adaptation Evolution Strategy algorithm (Hansen, 2023).

Hyperparameters.Key hyperparameters for PPO agents are listed below. All other parameters remain consistent with those in stable-baselines3. (Raffin et al., 2021)

\begin{table}
\begin{tabular}{c|l}
**Condition** & **Expression** \\ \hline
1 & \(\Delta p_{p,x}=p_{p,x}-1.51\) \\  & \((\Delta p_{p,x}>-0.2)\vee(\Delta p_{p,x}+\frac{1}{2}v_{p,x}>-0.2)\vee(p_{p,x}\leq\) \\  & \(p_{ee,x})\vee(|p_{p,y}|>m)\vee|p_{p,y}+0.75v_{p,y}|>m\) \\ \hline
2 & \((v_{p,x}>-0.2)\vee(p_{p,x}<p_{ee,x})\) \\ \hline
3 & \(|p_{p,y}|<0.41\lor p_{p,x}>-0.2\) \\ \hline
4 & \([(\Delta p_{p,x}<-0.2)\wedge(\max(|v_{p,x}|,|v_{p,y}|)<0.05)]\wedge\) \\  & \([(\Delta p_{p,x}\leq-0.8)\vee|p_{p,y}|>m]\) \\ \hline
5 & \([((\Delta p_{p,x}<0.3)\wedge(v_{p,x}<-0.5))\vee(v_{p,x}<-1.5)]\wedge(p_{ee,x}< p_{p,x})\) \\ \hline
6 & \([(\Delta p_{p,x}<-0.2)\wedge(\Delta p_{p,x}+v_{p,x}<-0.2)]\wedge(v_{p,x}<0.5) \\  & \(\wedge\neg[(|p_{p,y}|>m)\vee(|p_{p,y}+0.75v_{p,y}|>m)]\wedge(\Delta p_{p,x}+0. 75v_{p,x}>-0.8)\) \\ \hline \end{tabular}
\end{table}
Table 14: **State transition conditions for the state machine in Fig. 8.**

Figure 8: **State Machine of the composite agent.** The figure visualizes the different states of the the composite agent. In states _hit, defend, prepare_ the hitting, defending and preparation agent is activated respectively. In the _ik_ state the agent is supposed to go back to the initial joint position from which it can transition to the different states. The conditions for transitioning are shown in Table 14.

### Details of the RL3_polimi solution

The _RL3_polimi_ team addresses the Air Hockey problem by employing Deep RL techniques and RL Optimized Rule-Based Controllers. Indeed, they propose to train several low-level agents to perform some basic tasks in the game, and combine them via a rule-based controller to play the whole game. In what follows, we present how _RL3_polimi_ models the Air Hockey environment as an MDP, the employed Deep RL techniques, the developed Rule-Based controllers optimized via RL, and finally, how they treat the noise in the evaluation environment.

#### a.9.1 AirHockey as an MDP

In this section, we present how _RL3_polimi_ models the Air Hockey environment as an MDP by defining the state-action space and reward functions employed.

_State Space:_ The _RL3_polimi_ team slightly modifies the observation provided by the original environment. Indeed, they decided to discard the rotation-axis elements of the original observation, add the x and y component of the end effector computed with forward kinematics and add a flag indicating whether a collision happened between the end-effector and the puck. The resulting state space is then scaled between -1 and 1.

\[s=[\mathbf{p}_{\text{puck}}\ \mathbf{\dot{p}}_{\text{puck}}\ \mathbf{q}\ \mathbf{\dot{q}}\ \mathbf{p}_{\text{ee}}\ \mathbf{\dot{p}}_{\text{ee}}\ \text{collision\_flag}]\]

To smooth out the noise in the puck position and velocity observation, _RL3_polimi_ employed a simplified Kalman Filter (Kalman, 1960). In order to deal with noise in the joint positions and velocities, they directly use the agents actions in the previous step and ensure the requested robot poses are valid.

_Action Space: RL3_polimi_ employs a high-level control setting where the agent controls either acceleration of the joints or directly the end-effector position which are then converted to joint position and velocities depending on the low-level task.

_Reward function:_ The designed _reward_ is task-dependent, given that _RL3_polimi_ trains low-level policies for each low level task. Moreover, they shape the reward function instead of only providing a sparse reward for the completion of these tasks. Since this reward shaping greatly affects the performance of the learning algorithm, they employ different reward functions for the DeepRL and Rule-based agents. Nevertheless, in each case, the reward function follows the following general structure:

\[r(s,a)=r_{t}(s,a)+r_{c}(s,a),\]

where \(r_{t}(s,a)\) is the task depended reward and \(r_{c}(s,a)\) is the penalty for violating the constraints. A description of each specific form of \(r_{t}\) is given in the rest of this section.

#### a.9.2 Air Hockey via Deep RL

The _RL3_polimi_ team employs a DeepRL approach to train agents for the _defend_ and _counter-attack_ tasks. Both agents were trained employing the Soft Actor-Critic (SAC, Haarnoja et al., 2018) algorithm in combination with ATACOM (Liu et al., 2022). These agents directly control the desired joint accelerations, which are then converted via ATACOM to joint position and velocities as shown in Figure 9. In this setting, they opted for the control of acceleration as it was difficult to train an agent that directly controls the joint positions while respecting the constraints, especially when it was transferred to the evaluation environment.

\begin{table}
\begin{tabular}{c|c}
**Hyperparameter** & **Value** \\ \hline Number of environments & 40 \\ Number of steps & 512 \\ Batch size & 512 \\ Learning rate & \(5\cdot 10^{-5}\) \\ Gamma (Defend) & 1 \\ Gamma [Hit, Prepare] & 0.99 \\ Number of epochs & 10 \\ Network architecture & (Rossi et al., 2018; Rossi et al., 2018) \\ \end{tabular}
\end{table}
Table 15: List of hyperparameters.

In order to comply with the constraints, the single integration mode of ATACOM proved to be insufficient as it consistently violated joint velocity limits. Despite encountering challenges in implementing the double integration setting of ATACOM, the team achieved success in ensuring that the agent adhered to the specified constraints. This was achieved by constraining joint accelerations within the range \([-1,1]\). The combination of high-level control with the fixed conversion of the action space via ATACOM allowed for resolving the tasks while respecting the robot constraints.

Defend and Counter-attack_RL3_polimi_ designs the _counter-attack task_, which is similar to the defend except for the possibility of scoring when blocking the puck. Indeed, for this task the agent is again penalized if a goal is scored against him and receives a bonus when touching the puck together with an additional bonus proportional to the puck velocity towards the opponent. Moreover, a high positive reward is received when after the puck interaction, a goal is scored. They trained two agents for the defend and counter-attack tasks. In these tasks, the episode starts with the puck moving towards the agent's goal. In both cases, the agent receives a high penalty (-100) when the puck breaches the goal. When the agent successfully interacts with the puck, a reward (+10) is granted, coupled with a reward proportional to the norm of the puck's velocity, with a negative sign when defending and positive sign when counter-attacking. This incentivizes the agent to interact with the puck and lower its velocity while also protecting the goal in the defend task and pushing back the puck in the counter-attack. Moreover, in the counter-attack, the agent can also score in the opponent's goal, for which he receives an additional large bonus of 1000.

Return HomeMoreover, _RL3_polimi_ defines an additional task to train an agent to return to the initial position of the robot. At each step of the training process, the agent incurs a penalty based on the distance to the home configuration, \(r=-\|\mathbf{q}-\mathbf{q}_{home}\|\). In order to combine these policies when building the high-level agent that plays the whole game, the initial states for this task are drawn from the distribution of terminal states of the other tasks

Training via Curriculum LearningCurriculum Learning (CL, Narvekar et al., 2020) was applied to train each agent for the defend and counter-attack tasks. The return home task proved easy enough that no curriculum learning was needed to arrive at a good policy. Indeed, manual curriculums were devised across three levels of complexity: easy, medium, and hard tasks. These levels differ in the initial configuration of the puck, considering its position and velocity. In the easy task, the velocity along the y-axis was constrained and the velocity in the x-axis was kept low. Additionally, the puck was positioned at the end of the table, allowing the agent more reaction time. For the hard task, both x-axis and y-axis velocities were increased, and the puck's initial position was set in the middle of the table. The medium task served as an intermediary between the hard and easy tasks. For transitioning between different complexities a Sigmoid function was used to ensure the gradual increase of complexity.

Training detailsThe agents were trained for a total of 8000 epochs, where each epoch consists in 30 episodes of experience. During the first 3000 epochs the agent was trained with the easy task. After these initial epochs, a _soft_ switch to the medium task was performed over the next 1000 epochs by increasing the proportion of the episodes drawn from the medium task. Between epoch 4000 to 6000, the agent was exclusively trained on the medium task. From epoch 6000 to 7000 the progressive

Figure 9: Architecture of the _RL3_polimi_ Deep RL agents.

switch to the hard task was performed, and then the agent was trained exclusively on experience generated on the hard task.

After concluding the training in the defend task, the resulting policy was applied to the counter-attack one. Through curriculum learning, the task difficulty progressively increased to the hard level similar to the defend task. Employing this method, a counter-attack agent was successfully trained, capable of swiftly returning incoming pucks.

#### a.9.3 RL-Optimized Rule-based Controllers

For the _hit_ and _prepare_ tasks, the _RL3_polimi_ developed a Rule-Based Controller to be optimized via Policy Gradient Parameter-based Exploration [12] following the same approach employed in [13]. In this case, the agent directly controls the position of the end-effector. The actions are transformed from the _task_ space into the _joint_ space, combining inverse kinematics and Anchored Quadratic Programming [1].

The rule-based policies were divided into 2 main branches that resemble the challenge tasks: _hit_ and _prepare_. Each policy is further divided into phases, common to both of them:

* _Adjustment_: move the end-effector on (i) the line linking the puck and the goal for the _hit_; (ii) the vertical line parallel to the short side, passing through the puck, for the _prepare_;
* _Acceleration_: accelerate the end-effector and hit the puck. The _hit_ policy hits the puck to score a goal, while the _prepare_ one makes a more soft bump, in order to re-adjust the puck's position;
* _Final_: this phase only applies to the _hit_ policy. Here, the agent keeps hitting the puck making it reach a desired acceleration, then slowly stops the end-effector following a curved trajectory.

In particular, the **hit policy** computes two quantities, \(d\beta\) and \(ds\) in the following way:

\[d\beta=\begin{cases}\frac{(\theta_{0}+\theta_{1}\cdot t_{phase}\cdot dt)\cdot correction}{2}&,ds=\begin{cases}\theta_{2}&\text{adjustment phase}\\ \frac{ds_{t-1}+\theta_{3}\cdot t_{phase}\cdot dt}{radius+r_{malict}}&\text{acceleration phase}\\ constant&\text{slow-down phase}\end{cases},\]

where \(radius\) is the distance between the center of the puck and the end-effector, while \(correction\) is always defined as:

\[correction=\begin{cases}180-\beta&y_{puck}\leq\frac{table\,width}{2}\\ \beta-180&y_{puck}>\frac{table\,width}{2}\end{cases}.\]

For what concerns the **prepare policy**, the quantities \(d\beta\) and \(ds\) are computed as:

\[d\beta=\begin{cases}\theta_{0}\cdot t_{phase}+\theta_{1}\cdot correction\\ correction\end{cases},ds=\begin{cases}5\cdot 10^{-3}&\text{adjustment phase}\\ \theta_{2}&\text{acceleration phase}\end{cases},\]

where \(correction\) is defined as:

\[correction=\begin{cases}\beta-90&y_{puck}\leq 0\\ 270-\beta&y_{puck}>0\end{cases}.\]

All the quantities appearing in the computation of \(d\beta\) and \(ds\) are explained graphically in Figure 10 (left).

The major issue faced while developing this solution was the extreme sensitivity of the parameters w.r.t. the constraints. Indeed, the return landscape for our rule-based policies was highly non-smooth, i.e. a small variation could yield to a way bigger increase in the constraints violations. We employed PGPE to learn the parameters of the policies, but the learning process showed high sensibility to small parameters' variations.

In particular, for what concerns the _hit_ task, the reward function is based on the construction of a triangle with the opponent's area and the puck as vertices (Figure 11). If the puck, after the hit, lies inside the triangle, a positive reward is assigned, proportional to the puck's velocity; if the puck is outside the triangle, a penalty is assigned. The more the puck is outside the polygon, the bigger the penalty.

\[reward\_before\_hit=\begin{cases}-\frac{||ee_{pos}-puck_{pos}||}{0.5\cdot table _{diag}}&\text{no hit}\\ A+B\cdot\frac{(1-(2-\frac{9}{2})^{2})\cdot||v_{ec}||}{max\_veel}&\text{hit the puck}\\ \frac{1}{1-\gamma}&\text{goal}\end{cases},\] (9)

Where \(A\) and \(B\) are constant values, respectively equal to \(100\) and \(10\), \(table\_diag\) is the diagonal of the table, while \(\alpha=\arctan 2(ee_{y\_vel,ee_{x\_vel}})\). Finally \(max\_vel\) is a constant value equal to the maximum velocity observable in the environment.

If the agent hits the puck, it receives an instantaneous reward and, after that, the triangle approach is applied.

\[reward\_after\_hit=\begin{cases}B+||v_{puck}||&\text{puck inside the triangle}\\ -diff\_angle&\text{puck outside the triangle}\end{cases},\] (10)

where B is a constant equal to \(10\) and

\[diff\_angle=\arctan 2(v_{y_{puck}},v_{x_{puck}})-angle\_border,\] (11)

where \(angle\_border\) is the angle between the puck velocity vector and the closest triangle border.

#### a.9.4 Noise Filtering via State Pre-Processing

In this section, we present how the _RL3_polimi_ team treated the noise introduced in the evaluation environment. For training purposes, both the loss of the puck tracking and the noise in the observation were replicated in the local environment. To have a more realistic replica of the evaluation environment, an approximation of the noise model was performed. After the estimation, _RL3_polimi_

Figure 11: Triangle construction for assigning reward after the mallet hit the puck.

Figure 10: Components of the rule-based controllers: on the left, the Coordinates \(d\beta\) and \(ds\) used to find the next desired position of the end-effector. the puck is the red circle, the grey circle is the mallet and the blue circle is the next desired position of the mallet. On the right, the representation of the Finite State Machine for controlling task changes

modified the local environment in order to make it return noisy observations so to locally evaluate their policies.

To smooth out the noise in the puck position and velocity observation, a simplified Kalman Filter, already provided within the challenge code, was employed.

In the Rule-based approach the de-noised observation is used to plan the next joint positions and velocities, by means of a combination of inverse kinematics and AQP solver. In order to delete the joint positions and velocities observation noise, the agent uses its output at the previous time step (instead of the observation). Such an approach works since the adopted techniques provide desired robot poses that are reachable. Indeed, on one side ATACOM makes the action space to contain only feasible actions, on the other side the same holds thanks to the AQP solver when translating the end-effector coordinates into the ones in the joint space. In the next step, these computed values are used in the agent's internal representation of joints state, overwriting the ones coming from the environment. This is possible since the proposed position is always made reachable thanks to the AQP solver. As one can imagine, the first observation is a noisy one and cannot be overwritten. However, the sensibility to such a noisy observation is negligible.

For what concerns the Deep RL agents, there is no need to overwrite the environment observation since, due to the ATACOM algorithm, the action space contains only feasible actions. No AQP solver is used in the Deep RL approach. As expected, the Rule-based policies were more sensitive to the noisy observations than the loss of tracking.

#### a.9.5 Hierarchical agent

Finally, _RL3_polimi_ combined the described solutions into a single agent which operates at a higher level, dynamically selecting the most appropriate task during the game. To select the task, this agent relied on two components: the _Switcher_ and the _Finite State Machine_ (FSM). The _Switcher_ is the component developed to select a new task when the current one is completed. It consists in a simple rule-based controller which decides the task to employ via geometric considerations. After its last action, each task sends a signal to the high-level agent, notifying its completion. The switcher will select another task, which can potentially be also the same as before. The FSM is added as a second layer of safety to mitigate possible constraints violations while switching tasks. It forces the agent, at the end of each task, to go back to the home and start selecting a new task from there. The structure of the FSM can be observed in Figure 10. (right) The agent always starts a match in the _Home_ state, it is possible to remain in the same state but not switching from a task to another without passing from the home state.