# Marrying Causal Representation Learning with Dynamical Systems for Science

Dingling Yao, Caroline Muller, and Francesco Locatello

Institute of Science and Technology Austria

###### Abstract

Causal representation learning promises to extend causal models to hidden causal variables from raw entangled measurements. However, most progress has focused on proving identifiability results in different settings, and we are not aware of any successful real-world application. At the same time, the field of dynamical systems benefited from deep learning and scaled to countless applications but does not allow parameter identification. In this paper, we draw a clear connection between the two and their key assumptions, allowing us to apply identifiable methods developed in causal representation learning to dynamical systems. At the same time, we can leverage scalable differentiable solvers developed for differential equations to build models that are both identifiable and practical. Overall, we learn explicitly controllable models that isolate the trajectory-specific parameters for further downstream tasks such as out-of-distribution classification or treatment effect estimation. We experiment with a wind simulator with partially known factors of variation. We also apply the resulting model to real-world climate data and successfully answer downstream causal questions in line with existing literature on climate change. Code is available at https://github.com/CausalLearningAI/crl-dynamical-systems.

## 1 Introduction

Causal representation learning (CRL) [54] focuses on _provably_ retrieving high-level latent variables from low-level data. Recently, there have been many casual representation learning works compiling, in various settings, different theoretical identifiability results for these latent variables [8, 26, 32, 36, 37, 58, 60, 62, 65, 66, 70, 73]. The main open challenge that remains for this line of work is the broad applicability to real-world data. Following earlier works in disentangled representations (see [38] for a summary of data sets), existing approaches have largely focused on visual data. This is challenging for various reasons. Most notably, it is unclear what the causal variables should be in computer vision problems and what would be interesting or relevant causal questions. The current standard is to test algorithms on synthetic data sets with "made-up" latent causal graphs, e.g., with the object class of a rendered 3d shape causing its position, hue, and rotation [65].

In parallel, the field of machine learning for science [44, 49] shows promising results on various real-world time series data collected from some underlying dynamical systems. Some of these works primarily focus on time-series forecasting, i.e., building a neural emulator that mimics the behavior of the given times series data [12, 13, 25]; while others try to additionally learn an explicit ordinary differential equation simultaneously [9, 10, 15, 18, 23, 56]. However, to the best of our knowledge, none of these methods provide explicit identifiability analysis indicating whether the discovered equation recovers the ground truth underlying governing process given time series observations; or even whether the learned representation relates to the underlying steering parameters. At the same time, many scientific questions are inherently causal, in the sense that physical laws govern the measurements of all the natural data we can record, e.g., across different environments and experimental settings. Identifying such an underlying physical process can boost scientific understanding and reasoning in numerous fields; for example, in climate science, one could conductsensitivity analysis of _layer thickness_ parameter on atmosphere motion more efficiently, given a neural emulator that identifies the _layer thickness_ in its latent space. However, whether mechanistic models can be practically identified from data is so far unclear (Sutton et al., 2016, Table 1).

This paper aims to identify the underlying _time-invariant_ physical parameters from real-world time series, such as the previously mentioned _layer thickness_ parameter, while still preserving the ability to forecast efficiently. Thus, we connect the two seemingly faraway communities, causal representation learning and machine learning for dynamical systems, by phrasing parameter estimation problems in dynamical systems as a latent variable identification problem in CRL. The benefits are two folds: (1) we can import all identifiability theories for free from causal representation learning works, extending discovery methods with additional identifiability analysis and, e.g., multiview training constructs; (2) we showcase that the scalable mechanistic neural networks (Sutton et al., 2016) recently developed for dynamical systems can be directly employed with causal representation learning, thus providing a scalable implementation for both identifying and forecasting real-world dynamical systems.

Starting by comparing the common assumptions in the field of parameter estimation in dynamical systems and causal representation learning, we carefully justify our proposal to translate any parameter estimation problem into a latent variable identification problem; we differentiate three types of identifiability: _full identifiability_, _partial identifiability_ and _non-identifiability_. We describe concrete scenarios in dynamical systems where each kind of identifiability can be theoretically guaranteed and restate _exemplary_ identifiability theorems from the causal representation learning literature with slight adaptation towards the dynamical system setup. We provide a step-by-step recipe for reformulating a parameter estimation problem into a causal representation learning problem and discuss the challenges and pitfalls in practice. Lastly, we successfully evaluate our parameter identification framework on various _simulated_ and _real-world_ climate data. We highlight the following contributions:

* We establish the connection between causal representation learning and parameter estimation for differential equations by pinpointing the alignment of common assumptions between two communities and providing hands-on guidance on how to rephrase the parameter estimation problem as a latent variable identification problem in causal representation learning.
* We equip discovery methods with provably identifiable parameter estimation approaches from the causal representation learning literature and their specific training constructs. This enables us to maintain both the theoretical results from the latter and the scalability of the former.
* We successfully apply causal representation learning approaches to simulated and real-world climate data, demonstrating identifiability via domain-specific downstream causal tasks (OOD classification and treatment-effect estimation), pushing one step further on the applicability of causal representation for real-world problems.

**Remark on the novelty of the paper:** Our main contribution is establishing a connection between the dynamical systems and causal representation learning fields. As such, we do not introduce a new method per se. Meanwhile, this connection allows us to introduce CRL training constructs in methods that otherwise would not have any identification guarantees. Further, it provides the first avenue for causal representation learning applications on real-world data. These are both major challenges in the respective communities, and we hope this paper will serve as a building block for cross-pollination.

## 2 Parameter Estimation in Dynamical Systems

We consider dynamical systems in the form of

\[\dot{\mathbf{x}}(t)=\mathbf{f}_{\boldsymbol{\theta}}(\mathbf{x}(t))\qquad \mathbf{x}(0)=\mathbf{x}_{0},\ \boldsymbol{\theta}\sim p_{\boldsymbol{\theta}},\ t\in[0,t_{\max}]\] (1)

where \(\mathbf{x}(t)\in\mathcal{X}\subseteq\mathbb{R}^{d}\) denotes the state of a system at time \(t\), \(f_{\boldsymbol{\theta}}\in\mathcal{C}^{1}(\mathcal{X},\mathcal{X})\) is some smooth differentiable vector field representing the constraints that define the system's evolution, characterized by a set of physical parameters \(\boldsymbol{\theta}\in\boldsymbol{\Theta}=\boldsymbol{\Theta}_{1}\times \cdots\times\boldsymbol{\Theta}_{N}\), where \(\boldsymbol{\Theta}\subseteq\mathbb{R}^{N}\) is an open, simply connected real space associated with the probability density \(p_{\boldsymbol{\theta}}\). Formally, \(f_{\boldsymbol{\theta}}\) can be considered as a functional mapped from \(\boldsymbol{\theta}\) through \(M:\boldsymbol{\Theta}\rightarrow\mathcal{C}^{1}(\mathcal{X},\mathcal{X})\). In our setup, we consider _time-invariant_, _trajectory-specific_ parameters \(\boldsymbol{\theta}\) that remain constant for the whole time span \([0,t_{\max}]\), but variable for different trajectories. For instance, consider a robot arm interacting with multiple objects of different mass; a parameter \(\boldsymbol{\theta}\) could be the object's masses \(m\in\mathbb{R}_{+}\) in Newton's second law \(\ddot{x}(t)=\nicefrac{{\mathcal{F}(t)}}{{m}}\), with \(\mathcal{F}(t)\) denote the force applied at time \(t\). Depending on the object the robot arm interacts with, \(m\) can take different values, following the prior distribution \(p_{\boldsymbol{\theta}}\). \(\mathbf{x}(0)=\mathbf{x}_{0}\in\mathcal{X}\)denotes the initial value of the system. Note that higher-order ordinary differential equations can always be rephrased as a first-order ODE. For example, a \(\nu\)-th order ODE in the following form:

\[x^{(\nu)}(t)=f=(x(t),x^{(1)}(t),\ldots,x^{(\nu-1)}(t),\bm{\theta}),\]

can be written as \(\dot{\mathbf{x}}(t)=f_{\bm{\theta}}(\mathbf{x}(t))\), where \(\mathbf{x}(t)=(x(t),x^{(1)}(t),\ldots,x^{(\nu-1)}(t))\in\mathbb{R}^{\nu\cdot d}\) denotes state vector constructed by concatenating the derivatives. Formally, the solution of such a dynamical system can be obtained by integrating the vector field over time: \(\mathbf{x}(t)=\int_{0}^{t}f(\mathbf{x}(\tau),\bm{\theta})d\tau\).

**What do we mean by "parameters"?** The parameters \(\bm{\theta}\) that we consider can be both explicit and implicit. When the functional form of the ODE is given, like Newton's second law, the set of parameters is defined explicitly and uniquely. For real-world physical processes where the functional form of the state evolution is unknown, such as the sea-surface temperature change, we can consider _latitude-related_ features as parameters. Overall, we use _parameters_ to generally refer to any _time-invariant_, _trajectory-specific_ components of the underlying dynamical system.

**Assumption 2.1** (Existence and uniqueness).: For every \(\mathbf{x}_{0}\in\mathcal{X}\), \(\bm{\theta}\in\bm{\Theta}\), there exists a unique continuous solution \(\mathbf{x}_{\bm{\theta}}:[0,t_{\max}]\to\mathcal{X}\) satisfying the ODE (eq. (1)) for all \(t\in[0,t_{\max}]\)[22, 35].

**Assumption 2.2** (Structural identifiability).: An ODE (eq. (1)) is _structurally_ identifiable in the sense that for any \(\bm{\theta}_{1},\bm{\theta}_{2}\in\bm{\Theta}\), \(\mathbf{x}_{\bm{\theta}_{1}}(t)=\mathbf{x}_{\bm{\theta}_{2}}(t)\,\forall t\in [0,t_{\max}]\) holds if and only if \(\bm{\theta}_{1}=\bm{\theta}_{2}\)[7, 67, 69].

_Remark 2.1_.: Asm. 2.2 implies that it is _in principle_ possible to identify the parameter \(\bm{\theta}\) from a trajectory \(\mathbf{x}_{\bm{\theta}}\)[43]. Since this work focuses on providing concrete algorithms that guarantee parameter identifiability _given infinite number of samples_, the structural identifiability assumption is essential as a theoretical ground for further algorithmic analysis. It is noteworthy that a non-structurally identifiable system can become identifiable by reparamatization. For example, linear ODE \(\dot{\mathbf{x}}(t)=ab\mathbf{x}(t)\) with parameters \(a,b\in\mathbb{R}^{2}\) is structurally non-identifiable as \(a,b\) are commutative. But if we define \(c:=ab\) as the overall growth rate of the linear system, then \(c\) is structurally identifiable.

**Problem setting.** Given an observed trajectory \(\mathbf{x}:=(\mathbf{x}_{\bm{\theta}}(t_{0}),\ldots,\mathbf{x}_{\bm{\theta}}(t_ {T}))\in\mathcal{X}^{T}\) over the discretized time grid \(\mathcal{T}:=(t_{0},\ldots,t_{T})\), our goal is to investigate the identifiability of structurally identifiable parameters by formulating concrete conditions under which the parameter \(\bm{\theta}\) is (i) fully identifiable, (ii) partially identifiable, or (iii) non-identifiable _from the observational data_. We establish the identifiability theory for dynamical systems by converting classical parameter estimation problems [7] into a latent variable identification problem in causal representation learning [54]. For both (i) and (ii), we empirically showcase that existing CRL algorithms with slight adaptation can successfully (_partially_) identify the underlying physical parameters.

## 3 Identifiability of Dynamical Systems

This section provides different types of theoretical statements on the identifiability of the underlying _time-invariant_, _trajectory-specific_ physical parameters \(\bm{\theta}\), depending on whether the functional form of \(f_{\bm{\theta}}\) is known or not. We show that the parameters from an ODE with a known functional form can be _fully identified_ while parameters from unknown ODEs are in general _non-identifiable_. However, by incorporating some weak form of supervision, such as multiple similar trajectories generated from certain overlapping parameters [16, 39, 65, 71], parameters from an unknown ODE can also be _partially identified_. Detailed proofs of the theoretical statements are provided in App. B.

### Identifiability of Dynamical Systems with Known Functional Form

We begin with the identifiability analysis of the physical parameters of an ODE with **known** functional form. Many real-world data we record are governed by known physical laws. For example, the bacteria growth in microbiology could be modeled with a simple logistic equation under certain conditions, where the parameter of interest in this case would be the _growth rate_\(r\in\mathbb{R}_{+}\) and _maximum capacity_\(K\in\mathbb{R}_{+}\). Identifying such parameters would be helpful for downstream analysis. To this end, we introduce the definition of _full identifiability_ of a physical parameter vector \(\bm{\theta}\).

**Definition 3.1** (Full identifiability).: A parameter vector \(\bm{\theta}\in\bm{\Theta}\) is fully identified if the estimator \(\hat{\bm{\theta}}\) converges to the ground truth parameter \(\bm{\theta}\) almost surely.

**Definition 3.2** (ODE solver).: An ODE solver \(F:\bm{\Theta}\to\mathcal{X}^{T}\) computes the solution \(\mathbf{x}\) of the ODE \(f_{\bm{\theta}}=M(\bm{\theta})\) (eq. (1)) over a discrete time grid \(T=(t_{1},\ldots,t_{T})\).

**Corollary 3.1** (Full identifiability with known functional form).: _Consider a trajectory \(\mathbf{x}\in\mathcal{X}^{T}\) generated from a ODE \(f_{\bm{\theta}}(\mathbf{x}(t))\) satisfying Asms. 2.1 and 2.2, let \(\hat{\bm{\theta}}\) be an estimator minimizing the following objective:_

\[\mathcal{L}(\hat{\bm{\theta}})=\left\|F(\hat{\bm{\theta}})-\mathbf{x}\right\| _{2}^{2}\] (2)

_then the parameter \(\bm{\theta}\) is **fully-identified** (Defn. 3.1) by the estimator \(\hat{\bm{\theta}}\)._

_Remark 3.1_.: The estimator \(\hat{\theta}\) of eq.2 is considered as some learnable parameters that can be directly optimized. If we have multiple trajectories \(\mathbf{x}\) generated from different realizations of \(\bm{\theta}\sim p_{\bm{\theta}}\), we can also amortize the prediction \(\hat{\bm{\theta}}\) using a smooth encoder \(g:\mathcal{X}^{T}\rightarrow\bm{\Theta}\). In this case, the loss above can be rewritten as: \(\mathcal{L}(g)=\mathbb{E}_{\mathbf{x},t}[\|F(g(\mathbf{x}))-\mathbf{x}(t)\|_ {2}^{2}]\), then the optimal encoder \(g^{*}\in\operatorname*{argmin}\mathcal{L}(g)\) can generalize to unseen trajectories \(\mathbf{x}\) that follow the same class of physical law \(f\) and fully identify their trajectory-specific parameters \(\bm{\theta}\).

**Discussion.** Many works on machine learning for dynamical system identification follow the principle presented in Cor. 3.1, and most of them solely differ concerning the architecture they choose for the ODE solver. For example, SINDy-like ODE discovery methods [9; 10; 23; 24; 47] approximate the ground truth vector field \(f\) using a linear weighted sum over a set of library functions and learn the linear coefficients by sparse regression. For any ODE \(f\) that is linear in \(\bm{\theta}\), i.e., the ground truth vector field is in the form of \(f_{\bm{\theta}}(\mathbf{x},t)=\sum_{i=1}^{m}\theta_{i}\phi_{i}(\mathbf{x})\) for a set of known base functions \(\{\phi_{i}\}_{i\in[m]}\), SINDy-like approaches can fully identify the parameters by imposing some sparsity constraint. Another line of work, gradient matching [68], estimates the parameters probabilistically by modeling the vector field \(f_{\bm{\theta}}\) using a Gaussian Process (GP). The modeled solution \(\mathbf{x}(t)\) is thus also a GP since GP is closed under integrals (a linear operator). Given the functional form of \(f_{\bm{\theta}}\), the model aims to match the estimated gradient \(\hat{\mathbf{x}}\) and the evaluated vector field \(f_{\bm{\theta}}(\mathbf{x}(t))\) by maximizing the likelihood, which is equivalent to minimizing the least-squares loss (eq.2) under Gaussianity assumptions. Hence, the gradient matching approaches can _theoretically_ identify the underlying parameters under Cor. 3.1. Formal statements and proofs for both SINDy-like and gradient matching approaches are provided in App. B. _Note that most ODE discovery approaches [9; 10; 23; 24; 47; 68] refrain from making identifiability statements and explicitly states it is unknown which settings yield identifiability._

### Identifiability of Dynamical Systems without Known Functional Form

In traditional dynamical systems, identifiability analysis usually assumes the functional form of the ODE is known [43]; however, for most real-world time series data, the functional form of underlying physical laws remains uncovered. Machine learning-based approaches for dynamical systems work in a black-box manner and can clone the behavior of an unknown system [12; 13; 46], but understanding and identifiability guarantees of the learned parameters are so far missing. Since most of the physical processes are inherently steered by a few underlying _time-invariant_ parameters, identifying these parameters can be helpful in answering downstream scientific questions. For example, identifying climate zone-related parameters from sea surface temperature data could improve understanding of climate change because the impact of climate change significantly differs in polar and tropical regions. Hence, we aim to provide identifiability analysis for the underlying parameters of an unknown dynamical system by converting the classical parameter estimation problem of dynamical systems into a latent variable identification problem in causal representation learning. We start by listing the common assumptions in CRL and comparing the ground assumptions between these two fields.

**Assumption 3.1** (Determinism).: The data generation process is deterministic in the sense that observation \(\mathbf{x}\) is generated from some latent vector \(\bm{\theta}\) using a deterministic solver \(F\) (Defn. 3.2).

\begin{table}
\begin{tabular}{c c c c|c} \hline \multicolumn{2}{c}{**param. estimation**} & \multicolumn{1}{c}{**CRL**} & \multicolumn{1}{c}{**Explanation**} \\ _ref_ & _assumption_ & _assumption_ & _ref_ & \\ \hline
2.1 & _existence \& uniqueness_ & _determ. gen._ & 3.1 & Both 2.1 and 3.1 implies deterministic generative process. \\  & & & & & \\  & & \(supp(\bm{\theta})=\bm{\Theta}\) & 3.3 & 2.1 implies 3.3 as \(\mathbf{x}_{\bm{\theta}}\) uniquely exists for all \(\bm{\theta}\in\bm{\Theta}\). \\
2.2 & _structural identifiability_ & _injectivity_ & 3.2 & 2.2 implies 3.2 of the solution \(\mathbf{x}_{\bm{\theta}}\). \\ \hline \end{tabular}
\end{table}
Table 1: **Comparing typical assumptions** of parameter estimation for dynamical systems and latent variable identification in causal representation learning. We justify that the common assumptions in both fields are aligned, providing theoretical ground for applying identifiable CRL methods to learning-based parameter estimation approaches in dynamical systems.

**Assumption 3.2** (Injectivity).: For each observation \(\mathbf{x}\), there is only one corresponding latent vector \(\bm{\theta}\), i.e., the ODE solve function \(F\) (Defn. 3.2) is injective in \(\bm{\theta}\).

**Assumption 3.3** (Continuity and full support).: \(p_{\bm{\theta}}\) is smooth and continuous on \(\bm{\Theta}\) with \(p_{\bm{\theta}}>0\) a.e.

**Assumption justification.** Tab. 1 summarizes common assumptions in traditional parameter estimation in dynamical systems and causal representation learning literature. We observe strong alignment between the ground assumptions in these two fields that justifies our idea of employing causal representation learning methods in parameter estimation problems for dynamical systems: (1) Asm. 2.1 implies that given a fixed initial value \(\mathbf{x}_{0}\in\mathcal{X}\), there exists a unique solution \(\mathbf{x}(t),\,t\in[0,t_{\max}]\) for any \(f_{\bm{\theta}}\) with \(\bm{\theta}\in\bm{\Theta}\). In other words, parameter domain \(\bm{\Theta}\) is fully supported (Asm. 3.3), and these ODE solving processes from \(F(\bm{\theta})\) (Defn. 3.2) are deterministic, which aligns with the standard Asm. 3.1 in CRL. Since the ODE solution \(F(\bm{\theta})\) (SS 2) is continuous by definition, the continuity assumption from CRL (Asm. 3.3) is also fulfilled. (2) Asm. 2.2 emphasizes that each trajectory \(\mathbf{x}\) can only be uniquely generated from one parameter vector \(\bm{\theta}\in\bm{\Theta}\), which means the generating process \(F\) (Defn. 3.2) is injective in \(\bm{\theta}\) (Asm. 3.2).

Next, we reformulate the parameter estimation problem in the language of causal representation learning. We first cast the generative process of the dynamical system \(f_{\bm{\theta}}(\mathbf{x}(t))\) as a latent variable model by considering the underlying physical parameters \(\bm{\theta}\sim p_{\bm{\theta}}\) as a set of _latent variables_. Given a trajectory \(\mathbf{x}\) generated by a set of underlying factors \(\bm{\theta}\) based on the vector field \(f_{\bm{\theta}}(\mathbf{x}(t))\), we consider the observed trajectory as some _unknown nonlinear_ mixing of the underlying \(\bm{\theta}\), with the mixing process specified by individual vector field \(f_{\bm{\theta}}(\mathbf{x}(t))\). This interpretation of observations aligns with the standard setup of causal representation learning; for instance, high-dimensional images are usually generated from some lower-dimensional latent generating factors through an unknown nonlinear process. Thus, estimating the parameters of unknown dynamical systems becomes equivalent to inferring the underlying generating factors in causal representation learning.

After transforming the parameter estimation into a latent variable identification problem in CRL, we can directly invoke the identifiability theory from the literature. Based on Locatello et al. [38, Theorem 1.], we conclude that the underlying parameters from an unknown system are in general _non-identifiable_. Nevertheless, several works proposed different weakly supervised learning strategies that can _partially identify_ the latent variables [2, 8, 16, 65, 71]. To this end, we define partial identifiability in the context of dynamical systems by slightly adapting the definition of block-identifiability proposed by Von Kugelgen et al. [65]:

**Definition 3.3** (Partial identifiability).: A partition \(\bm{\theta}_{S}:=(\bm{\theta}_{i})_{i\in S}\) with \(S\subseteq[N]\) of parameter \(\bm{\theta}\in\bm{\Theta}\) is partially identified by an encoder \(g:\mathcal{X}^{T}\rightarrow\bm{\Theta}\) if the estimator \(\hat{\bm{\theta}}_{S}:=g(\mathbf{x})_{S}\) contains all and only information about the ground truth partition \(\bm{\theta}_{S}\), i.e. \(\hat{\bm{\theta}}_{S}=h(\bm{\theta}_{S})\) for some invertible mapping \(h:\bm{\Theta}_{S}\rightarrow\bm{\Theta}_{S}\) where \(\bm{\Theta}_{S}:=\times_{i\in S}\bm{\Theta}_{i}\).

Note that the inferred partition \(\hat{\bm{\theta}}_{S}\) can be a set of _entangled_ latent variables rather than a single one. In the multivariate case, one can consider the \(\hat{\bm{\theta}}_{S}\) as a bijective mixture of the ground truth parameter \(\bm{\theta}_{S}\).

**Corollary 3.2** (Identifiability without known functional form).: _Assume a dynamical system \(f\) satisfying Asms. 2.1 and 2.2, a pair of trajectories \(\mathbf{x},\tilde{\mathbf{x}}\) generated from the same system \(f\) but specified by different parameters \(\bm{\theta},\tilde{\bm{\theta}}\), respectively. Assume a partition of parameters \(\bm{\theta}_{S}\) with \(S\subseteq[N]\) is shared across the pair of parameters \(\bm{\theta},\tilde{\bm{\theta}}\). Let \(g:\mathcal{X}^{T}\rightarrow\bm{\Theta}\) be some smooth encoder and \(\tilde{F}:\bm{\Theta}\rightarrow\mathcal{X}^{T}\) be some left-invertible smooth solver that minimizes the following objective:_

\[\mathcal{L}(g,\hat{F})=\mathbb{E}_{\mathbf{x},\tilde{\mathbf{x}}}\underbrace{ \left\|g(\mathbf{x})_{S}-g(\tilde{\mathbf{x}})_{S}\right\|_{2}^{2}}_{Alg_{ment} }+\underbrace{\left\|\hat{F}(g(\mathbf{x}))-\mathbf{x}\right\|_{2}^{2}+\left\| \hat{F}(g(\tilde{\mathbf{x}}))-\tilde{\mathbf{x}}\right\|_{2}^{2}}_{Sfficiency},\] (3)

_then the shared partition \(\bm{\theta}_{S}\) is partially identified (Defn. 3.3) by \(g\) in the statistical setting._

**Discussion.** We remark that an implicit ODE solver \(\hat{F}\) is introduced in eq. (3) because the functional form \(f_{\bm{\theta}}\) is unknown. Intuitively, Cor. 3.2 provides partial identifiability results for the shared partition of parameters between two trajectories. We can consider the trajectories to be different simulation experiments but with certain sharing conditions, such as two wind simulations that share the same _layer thickness_ parameter. This partial identifiability statement is mainly concluded from the theory in the multiview CRL literature [2, 8, 16, 39, 54, 65, 71]. Note that this corollary is _one exemplary__demonstration_ of achieving partial identifiability in dynamical systems. Many identifiability results from the causal representation works can be reformulated similarly by replacing their decoder with a differentiable ODE solver \(\hat{F}\). The high-level idea of multiview CRL is to identify the shared part between different views by enforcing alignment on the shared coordinates while preserving a sufficient information representation. _Alignment_ can be obtained by either minimizing the \(L_{2}\) loss between the encoding from different views on the shared coordinates [16, 65, 71] or maximizing the correlation on the shared dimensions correspondingly [41, 42]; _Sufficiency_ of the learned representation is often prompted by maximizing the entropy [16, 65, 71, 74] or minimizing the reconstruction error [2, 8, 39, 54]. Other types of causal representation learning works will be further discussed in SS 5.

## 4 CRL-construct of Identifiable Neural Emulators for Dynamical Systems

This section provides a step-by-step construct of a neural emulator that can (1) identify the _time-invariant, trajectory-specific_ physical parameters from some unknown dynamical systems if the identifiability conditions are met and (2) efficiently forecast future time steps. Identifiability can be guaranteed by employing causal representation learning approaches (SS 3) while forecasting ability can be obtained by using an efficient mechanistic solver [47] as a decoder. For the sake of simplicity, we term these identifiable neural emulators as _identifiers_. We remark that the general architecture remains consistent for most CRL approaches, while the learning object differs slightly in _latent regularization_, which is specified by individual identifiability algorithms. Intuitively, the _latent regularization_ can be interpreted as an additional constraint put on the learned encodings imposed by the setting-specific assumptions, such as the _alignment_ term in multiview CRL (Cor. 3.2). In the following, we demonstrate building an _identifier_ in the multiview setting from scratch and showcase how it can be easily generalized to other CRL approaches with slight adaptation.

**Architecture.** Since the parameters of interest are _time-invariant_ and _trajectory-specific_ (SS 2), we input the whole trajectory \(\mathbf{x}=(\mathbf{x}(t_{1}),\dots,\mathbf{x}(t_{T}))\) to a smooth encoder \(g:\mathcal{X}^{T}\rightarrow\mathbf{\Theta}\), as shown in Fig. 1. Then, we decode the trajectory \(\hat{\mathbf{x}}\) from estimated parameter vector \(\hat{\boldsymbol{\theta}}:=g(\mathbf{x})\) using a mechanistic solver [47]. The high-level idea of mechanistic neural networks is to approximate the underlying dynamical system using a set of explicit ODEs \(\mathcal{U}_{\hat{\boldsymbol{\theta}}}:C(\boldsymbol{\alpha},\hat{ \boldsymbol{\theta}})=0\) with learnable coefficients \(\boldsymbol{\alpha}\in\mathbb{R}^{d_{\boldsymbol{\alpha}}}\). The explicit ODE family \(\mathcal{U}_{\hat{\boldsymbol{\theta}}}\) can then be interpreted as a constrained optimization problem and can thus be solved using a _neural relaxed linear programming solver_[47, Sec 3.1].

In more detail, the original design of MNN predicts the coefficients from the input trajectory \(\mathbf{x}\) using an MNN encoder \(g_{\text{mnn}}\); however, as we enforce the estimated parameter \(\boldsymbol{\theta}\) to preserve _sufficient_ information of the entire trajectory \(\mathbf{x}\), we instead predict the coefficients \(\boldsymbol{\alpha}\) from the estimated parameter \(\hat{\boldsymbol{\theta}}\) with the encoder \(g_{\text{mnn}}:\boldsymbol{\Theta}\rightarrow\mathbb{R}^{d_{\boldsymbol{\alpha}}}\). Formally, the coefficients \(\boldsymbol{\alpha}\) are computed as \(\boldsymbol{\alpha}=g_{\text{mnn}}(\hat{\boldsymbol{\theta}})\) where \(\hat{\boldsymbol{\theta}}=g(\mathbf{x})\). The resulting ODE family \(\mathcal{U}_{\hat{\boldsymbol{\theta}}}\) provides a broad variability of ODE

Figure 1: **Model overview with sea surface temperature inputs: Our _mechanistic identifier_ extracts the underlying time-invariant latitude-related parameters \(\boldsymbol{\theta}\), providing a versatile neural emulator for downstream causal analysis.**

parametrizations. A detailed formulation of \(\mathcal{U}_{\hat{\bm{\theta}}}\) at \(t\)[47, eq. (3)] is given by

\[\underbrace{\sum_{i=0}^{l}c_{i}(t;\hat{\bm{\theta}})u^{(i)}}_{\text{linear terms}}+\underbrace{\sum_{j=0}^{r}\phi_{k}(t;\hat{\bm{\theta}})g_{k}(t,\{u^{(j)} \})}_{\text{nonlinear terms}}=b(t;\hat{\bm{\theta}}),\] (4)

where \(u^{(i)}\) is \(i\)-th order approximations of the ground truth state \(\mathbf{x}\). Like in any ODE solving in practice, solving eq. (4) requires discretization of the continuous coefficients in time (e.g., \(c_{i}(t;\hat{\bm{\theta}})\)). Discretizing the ODE representation \(\mathcal{U}_{\hat{\bm{\theta}}}\) gives rise to:

\[\sum_{i=0}^{l}c_{i,t}u_{t}^{(i)}+\sum_{j=0}^{r}\phi_{k,t}g_{k}(\{u_{t}^{(j)}\} )=b_{t}\quad s.t.\quad(u_{t_{1}},u_{t_{1}}^{\prime},\dots)=\omega,\] (5)

where \(\omega\) denotes the initial state vector of the ODE representation \(\mathcal{U}_{\hat{\bm{\theta}}}\). To this end, we present the explicit definition of the learnable coefficients \(\bm{\alpha}:=(c_{i,t},\phi_{k,t},b_{t},s_{t},\omega)\) with \(t\in\mathcal{T},i\in[l],k\in[r]\), which is a concatenation of linear coefficients \(c_{i,t}\), nonlinear coefficients \(\phi_{i,k}\), adaptive step sizes \(s_{t}\) and initial values \(\omega\). Note that we dropped the \(\hat{\bm{\theta}}\) in the notation for simplicity, but all of these coefficients \(\bm{\alpha}\) are predicted from \(\hat{\bm{\theta}}\), as described previously. At last, MNN converts ODE solving into a constrained optimization problem by representing the \(\mathcal{U}_{\hat{\bm{\theta}}}\) using a set of constraints, including ODE equation constraints, initial value constraints, and smoothness constraints [47, Sec 3.1.1]. This optimization problem is then solved by _neural relaxed linear programming_ solver [47, Sec 3.1] in a time-parallel fashion, thus making the overall mechanistic solver scalable and GPU-friendly.

**Learning objective and latent regularizers.** Depending on whether the functional form of the underlying dynamical system is known or not, the proposed neural emulator can be trained using the losses given in Cor. 3.1 or Cor. 3.2, respectively. When the functional form is unknown, we employ CRL approaches to _partially_ identify the physical parameters. We remark that the causal representation learning schemes mainly differ in the latent regularizers, specified by the assumptions and settings. Therefore, we provide a more extensive summary of different causal representation learning approaches and their corresponding latent regularizer in Tab. 6.

## 5 Related Work

**Multi-environment CRL.** Another important line of work in causal representation learning focuses on the multi-environment setup, where the data are collected from multiple different environments and thus _non-identically distributed_. Causal variable identifiability are shown under _single node intervention per node_ with parametric assumptions on the mixing functions [3, 58, 62, 73] or on the latent causal model [11, 58]. These parametric assumptions can be lifted by additionally assuming _paired intervention per node_, as demonstrated by [63, 66]. Overall, given the fruitful literature in multi-environment causal representation learning, we believe applying multi-environments methods to build identifiable neural emulators (SS 4) would be an exciting future avenue.

**CRL and dynamical systems.** Recent CRL works have been tackling the parameter identification problem in dynamical systems in a parametric setting. For instance, Rajendran et al. [50] considers a Gaussian linear time invariant system with control input and Balsells-Rodas et al. [5] assumes a switching dynamical system. By contrast, we show identifiability in a more general setting without specific parameter assumptions on the dynamical systems and prove different granularity of parameter identification under different system prior system knowledge (full identifiability if parametric form is known (Cor. 3.1) and partial identifiability when the system is unknown (Cor. 3.2)). Another closely related line of works, temporal causal representation learning, typically assume an _"intervenable"_ time series in the _latent space_, splitting the latent variables into two partitions, with one following the default dynamics and the other following the intervened dynamics. The goal of temporal CRL is to provably retrieve these _time-varying_ latent causal variables, such as inferring the position of a ball from raw images over time [27, 30, 33, 36, 37, 72]. Unlike these temporal CRL works, our approach models dynamics directly in the observational space, focusing on the _time-invariant, trajectory-specific_ physical parameters such as gravity or mass. Overall, our framework addresses a different hierarchy of problems. We believe both problems are orthogonal yet equally important, encouraging cross-pollination in future work.

**ODE discovery.** The ultimate goal of ODE discovery is to learn a human-interpretable equation for an unknown system, given discretized observations generated from this system. Recently, many machine learning frameworks have been used for ODE discovery, such as sparse linear regression [9, 10, 23, 53], symbolic regression [6, 15, 18], simulation-based inference [14, 56].

Becker et al. [6], d'Ascoli et al. [18] exploit transformer-based approaches to dynamical symbolic regression for univariate ODEs, which is extended by d'Ascoli et al. [15] to multivariate case. Schroder and Macke [56] employs _simulation-based variational inference_ to jointly learn the operators (like addition or multiplication) and the coefficients. However, this approach typically runs simulations inside the training loop, which could introduce a tremendous computational bottleneck when the simulator is inefficient. On the contrary, our approach works offline with pre-collected data, avoiding simulating on the fly. Although ODE discovery methods can provide symbolic equations for data from an unknown trajectory, the inferred equation does not have to align with the ground truth. In other words, theoretical identifiability guarantees for these methods are still missing.

**Identifiability of dynamical systems.** Identifiability of dynamical systems has been studied on a _case-by-case_ basis in traditional system identification literature [4; 64; 43]. Liang and Wu [34] studied ODE identifiability under measurement error. Scholl et al. [55] investigated the identifiability of ODE discovery with non-parametric assumption, but only for univariate cases. More recently, several works have advanced in identifiability analysis of _linear_ ODEs from a _single_ trajectory [17; 48; 59]. Overall, current theoretical results cannot conclude whether an unknown nonlinear ODE can be identified from observational data. Hence, in our work, we do not aim to identify the whole equation of the dynamical systems but instead focus on identifying the time-invariant parameters.

## 6 Experiments

This section provides experiments and results on both simulated and real-world climate data. We first validate full parameter identifiability (Cor. 3.1) under a wide range of known dynamical systems, as demonstrated in SS 6.1. Next, we consider in SSSS 6.2 and 6.3 time series data governed by an unknown physical process, so we employ the multiview CRL approach together with mechanistic neural networks to build our identifiable neural emulator (termed as _mechanistic identifier_), following the steps in SS 4. We compare _mechanistic identifier_ with three baselines: (1) _Ada-GVAE_[39], a traditional multiview model that uses a vanilla decoder instead of a mechanistic solver. (2) _Time-invariant MNN_, proposed by [47]. We choose this variant of MNN as our baseline for a fair comparison. (3) _Contrastive identifier_, a contrastive loss-based CRL approach without a decoder [16; 65; 71]. We train _mechanistic identifier_ using eq. (3) and other baselines following the steps given in the original papers. After training, we evaluate these methods on their identifiability and long-term forecasting capability.

### Theory Validation: ODEBench

We demonstrate point-wise parameter identification results (presented as RMSE, mean \(\pm\) std) over ODE system with known functional forms, including 63 dynamical systems from ODEBench [15] and the Cart-Pole system inspired by [72]. Results are summarized in Tab. 7. For each system, we sample 100 tuples of the parameters \(\theta\) within a valid range to e.g., preserve the chaotic properties. For each tuple, we solve the problem as outlined in Cor. 3.1, by either regressing the estimated observation \(F(\hat{\bm{\theta}})\) onto the true observation \(\mathbf{x}\), or equivalently, regressing the estimated vector field \(f_{\bm{\theta}}(\mathbf{x})\) onto the derivatives \(\dot{\mathbf{x}}\), given the same initial conditions. Note the data derivatives \(\dot{\mathbf{x}}\) can be obtained from numerical approximation in case it is not given a priori. The resulting root-mean-square deviation (RMSE) is calculated and averaged across the parameter dimensions. We report the mean and standard deviation of these averaged RMSEs over the 100 independent runs. From Tab. 7, we observe highly accurate point estimates for all stationary system parameters \(\theta\), thereby validating Cor. 3.1 across various experimental settings.

### Wind Simulation

**Experimental setup.** Our experiment considers longitudinal and latitudinal wind velocities (also termed \(u,v\) wind components) from the global wind simulation data generated by various _layer-thickness_ parameters. Fig. 2 depicts the wind simulation output at a certain time point. To train the multiview approaches, we generate a tuple of three views: After sampling the first view \(\mathbf{x}^{1}\) randomly throughout the whole training set, we sample another trajectory \(\mathbf{x}^{2}\) from a different location which shares the same simulation condition as the first one, compared to the first view, the third view \(\mathbf{x}^{3}\) is then sampled from another simulation but at the same location. Overall, \(\mathbf{x}^{1},\mathbf{x}^{2}\) share the global simulation conditions like the _layer thickness_ parameter

Figure 2: **Wind simulation**: \(u,v\) components [m/s] of simulated air motion over the globe.

while \(\mathbf{x}^{1},\mathbf{x}^{3}\) only share the local features. All three views share global atmosphere-related features that are not specified as simulation conditions. More details about the data generation process and training pipeline are provided in App. C.2.

**Parameter identification.** In this experiment, we use the learned representation to classify the ground-truth labels generated by discretizing the generating factor _layer thickness_, and report the accuracy in Fig. 3. In more detail, we use latent dim=12 for all models and split the learned encodings into three partitions \(S_{1},S_{2},S_{3}\), with four dimensions each. Then, we individually predict the ground truth _layer thickness_ labels from each partition. According to the previously mentioned view-generating process, the _layer thickness_ parameter should be encoded in \(S_{1}\) for both _contrastive_ and _mechanistic identifiers_. This hypothesis is verified by Fig. 3 since both _contrastive_ and _mechanistic identifiers_ show a high accuracy of acc\(\approx\)1 in the first partition \(S_{1}\) and low accuracy in other partitions. On the contrary, _Ada-GVAE_ and _TI-MNN_ performed significantly worse with an average acc. of 60% everywhere. Overall, Fig. 3 shows both the necessity of explicit time modeling using MNN solver (compared to _Ada-GVAE_) and identifiability power of multiview CRL (compared to _TI-MNN_).

### Real-world Sea Surface Temperature

**Experimental setup.** We evaluate the models on sea surface temperature dataset _SST-V2_[21]. For the multiview training, we generate a pair trajectories from a small neighbor region (\(\pm 5^{\circ}\)) along the _same latitude_. We believe these pairs share certain climate properties as the locations from the same latitude share _roughly_ the amount of direct sunlight which will directly affect the sea surface temperature. Further information about the dataset and training procedure is provided in App. C.2.

**Time series forecasting.** We chunk the time series into slices of 4 years in training while keeping last four years as out-of-distribution forecasting task. To predict the last chunk, we input data from 2015 to 2018 to get the learned representation \(\hat{\bm{\theta}}\). Since we assume \(\hat{\bm{\theta}}\) to be _time-invariant_, we decode \(\hat{\bm{\theta}}\) together with 10 initial steps of 2019 to predict the last chunk. Note that _contrastive identifier_ is excluded from this task as it does not have a decoder. As shown in Tab. 2, the forecasting performance of _mechanistic Identifier_ surpasses _Ada-GVAE_ by a great margin, showcasing the superiority of integrating scalable mechanistic solvers in real-world time series datasets. At the same time, _TI-MNN_ performed worse and unstably despite the MNN component, verifying the need of the additional information bottleneck (parameter encoder \(g\)) and the multiview learning scheme.

**Climate-zone classification.** Since there is no ground truth latitude-related parameters available, we design a downstream classification task that verifies our learned representation encodes the latitude-related information. The goal of the task is to predict the climate zone _(tropical, temperate, polar)_ from the learned _shared_ representation because the latitude uniquely defines climate zones. We evaluated the methods in both _in-distribution_ (ID) and _out-of-distribution_ (OOD) setup for all baselines. In the OOD setting, we input data from longitude \(10^{\circ}\) to longitude \(360^{\circ}\) when training the classifier while keeping the first \(10\) degree as our out-of-distribution test data. Tab. 2 show that both _contrastive_ and _mechanistic identifiers_ perform decently, supporting the applicability of identifiable multiview CRL algorithms in dynamical systems. Overall, the performance of multiview CRL-based approaches (_contrastive and mechanistic identifiers_) far exceeds _Ada-GVAE_ and _TI-MNN_, again showcasing the superiority of the combination of causal representation learning and mechanistic solvers.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{3}{c}{SST V2} \\  & **Acc.(ID)(\(\uparrow\))** & **Acc.(OOD)(\(\uparrow\))** & **Forecast. error(\(\downarrow\))** \\ \hline Ada-GVAE & \(0.468\pm 0.001\) & \(0.467\pm 0.000\) & \(0.043\pm 0.044\) \\ TI-MNN & \(0.697\pm 0.049\) & \(0.668\pm 0.074\) & \(0.024\pm 0.016\) \\ Contr. Identifier & \(\bm{0.904\pm 0.011}\) & \(\bm{0.861\pm 0.022}\) & ✗ \\
**Mech. Identifier** & \(\bm{0.902\pm 0.005}\) & \(0.824\pm 0.016\) & \(\bm{0.007\pm 0.003}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Performance evaluation on the SST-V2 data on various types of tasks. Results averaged over three random seeds with standard deviation, provided as (m \(\pm\) std).**

Figure 3: **Prediction accuracy on _layer thickness_ parameter on wind simulation data, evaluated on individual encoding partitions \(S_{1},S_{2},S_{3}\). Results averaged from three random runs.**

**Average treatment effect estimation.** We further investigate the effect of climate zone on average temperature along one specific latitude through _average treatment effect_ (ATE) estimation. Formally, we consider the latitudinal average temperature as outcome \(Y\), two climate zones (_tropical_\((T=0)\), _polar_\((T=1)\)) as binary treatments, and the predicted latitude-specific features as unobserved mediators. Formally, ATE is defined as: \(\text{ATE}:=\mathbb{E}[Y|do(T=1)]-\mathbb{E}[Y|do(T=0)]\). Since ATE cannot be computed directly [20], we estimate it using the popular _AIPW_ estimator [52]. Fig. 4 illustrates that the estimated ATE from the non-identified representation lacks a discernible pattern [51] whereas the identified representation exhibits a noisy yet clear increasing trend, indicating the global warming effect. This is because the non-identified representation failed to isolate the covariates \(\theta\), leading to biased treatment effect estimates. To estimate treatment effects, the covariates (i.e., the latitude-related parameters we identify) must not be influenced by the treatment (i.e., the climate zones). Otherwise, they become confounders, leading to incorrect estimates [19].

## 7 Limitations and Conclusion

In this paper, we build a bridge between causal representation learning and dynamical system identification. By virtue of this connection, we successfully equipped existing mechanistic models (focusing on [47] in practice for scalability reasons) with identification guarantees. Our analysis covers a large number of papers, including [10; 23; 24; 47; 68] explicitly refraining from making identifiability statements. At the same time, our work demonstrated that causal representation learning training constructs are ready to be applied in the real world, and the connection with dynamical systems offers untapped potential due to its relevance in the sciences. This was an overwhelmingly acknowledged limitation of the causal representation learning field [3; 11; 16; 39; 58; 62; 65; 71]. Having clearly demonstrated the mutual benefit of this connection, we hope that future work will scale up identifiable mechanistic models and apply them to even more complex dynamical systems and real scientific questions. Nevertheless, this paper has several technical limitations that could be addressed in future work. First of all, the proposed theory explicitly requires _determinism_ as one of the key assumptions (Asm. 3.1), which directly excludes another important type of differential equation: Stochastic Differential Equations. Second, we assume we directly observe the state \(\mathbf{x}\) without considering measurement noise. Although the empirical results were promising on real-world noisy data (SS 6.3), we believe explicitly modeling measurement noise would elevate the theory. Finally, our identifiability analysis focuses on the infinite data regime, which is unrealistic in real-world scenarios.

## Acknowledgments

We thank Niklas Boers for recommending the SpeedyWeather simulator and Valentino Maiorca for guidance on Fourier transformation for SST data. We are also grateful to Shimeng Huang and Riccardo Cadei for their feedback on the treatment effect estimation experiment and to Jiale Chen and Adeel Pervez for their assistance with the solver implementation. Finally, we appreciate the anonymous reviewers for their insightful suggestions, which helped improve the manuscript.

Figure 4: _Left_: Underlying causal model for SST-V2 data, \(\bm{\theta}\): covariates (latitude-related parameters of interest), \(\mathbf{X}\): outcome (zonal average temperature), \(\mathbf{T}\): treatment (tropical \(\mathbf{T}=0\) or polar \(\mathbf{T}=1\)). _Right:_ Comparison on ATE change ratio between identified and non-identified parameters, computed by \(\nicefrac{{ATE(year)-ATE(1990)}}{{ATE(1990)}}\), averaged over three runs.

## References

* [1] Nasir Ahmed, T_ Natarajan, and Kamisetty R Rao. Discrete cosine transform. _IEEE transactions on Computers_, 100(1):90-93, 1974.
* [2] Kartik Ahuja, Jason S Hartford, and Yoshua Bengio. Weakly supervised representation learning with sparse perturbations. _Advances in Neural Information Processing Systems_, 35:15516-15528, 2022.
* [3] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In _International Conference on Machine Learning_, pages 372-407. PMLR, 2023.
* [4] Karl Johan Astrom and Peter Eykhoff. System identification--a survey. _Automatica_, 7(2):123-162, 1971.
* [5] Carles Balsells-Rodas, Yixin Wang, and Yingzhen Li. On the identifiability of switching dynamical systems, 2023.
* [6] Soren Becker, Michal Klein, Alexander Neitz, Giambattista Parascandolo, and Niki Kilbertus. Predicting ordinary differential equations with transformers. In _International Conference on Machine Learning_, pages 1978-2002. PMLR, 2023.
* [7] Ror Bellman and Karl Johan Astrom. On structural identifiability. _Mathematical biosciences_, 7(3-4):329-339, 1970.
* [8] Johann Brehmer, Pim De Haan, Phillip Lippe, and Taco S Cohen. Weakly supervised causal representation learning. _Advances in Neural Information Processing Systems_, 35:38319-38331, 2022.
* [9] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. _Proceedings of the national academy of sciences_, 113(15):3932-3937, 2016.
* [10] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Sparse identification of nonlinear dynamics with control (sindyc). _IFAC-PapersOnLine_, 49(18):710-715, 2016.
* [11] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Scholkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing. _Advances in Neural Information Processing Systems_, 36, 2024.
* [12] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. _Advances in Neural Information Processing Systems_, 2018.
* [13] Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel. Learning neural event functions for ordinary differential equations. _International Conference on Learning Representations_, 2021.
* [14] Kyle Cranmer, Johann Brehmer, and Gilles Louppe. The frontier of simulation-based inference. _Proceedings of the National Academy of Sciences_, 117(48):30055-30062, 2020.
* [15] Stephane d'Ascoli, Soren Becker, Philippe Schwaller, Alexander Mathis, and Niki Kilbertus. ODEFormer: Symbolic regression of dynamical systems with transformers. In _International Conference on Learning Representations_, 2024.
* [16] Imant Daunhawer, Alice Bizeul, Emanuele Palumbo, Alexander Marx, and Julia E Vogt. Identifiability results for multimodal contrastive learning. _arXiv preprint arXiv:2303.09166_, 2023.
* [17] Xiaoyu Duan, JE Rubin, and David Swigon. Identification of affine dynamical systems from a single trajectory. _Inverse Problems_, 36(8):085004, 2020.
* [18] Stephane d'Ascoli, Pierre-Alexandre Kamienny, Guillaume Lample, and Francois Charton. Deep symbolic regression for recurrence prediction. In _International Conference on Machine Learning_, pages 4520-4536. PMLR, 2022.
* [19] Stefan Feuerriegel, Dennis Frauen, Valentyn Melnychuk, Jonas Schweisthal, Konstantin Hess, Alicia Curth, Stefan Bauer, Niki Kilbertus, Isaac S Kohane, and Mihaela van der Schaar. Causalmachine learning for predicting treatment outcomes. _Nature Medicine_, 30(4):958-968, 2024.
* [20] Paul W Holland. Statistics and causal inference. _Journal of the American statistical Association_, 81(396):945-960, 1986.
* [21] Boyin Huang, Chunying Liu, Viva Banzon, Eric Freeman, Garrett Graham, Bill Hankins, Tom Smith, and Huai-Min Zhang. Improvements of the daily optimum interpolation sea surface temperature (doisst) version 2.1. _Journal of Climate_, 34(8):2923-2939, 2021.
* [22] Edward L Ince. _Ordinary differential equations_. Courier Corporation, 1956.
* [23] Kadierdan Kaheman, J Nathan Kutz, and Steven L Brunton. Sindy-pi: a robust algorithm for parallel implicit sparse identification of nonlinear dynamics. _Proceedings of the Royal Society A_, 476(2242):20200279, 2020.
* [24] Alan A Kaptanoglu, Jared L Callaham, Aleksandr Aravkin, Christopher J Hansen, and Steven L Brunton. Promoting global stability in data-driven models of quadratic nonlinear dynamics. _Physical Review Fluids_, 6(9):094401, 2021.
* [25] Patrick Kidger, Ricky T. Q. Chen, and Terry J. Lyons. "hey, that's not an ode": Faster ode adjoints via seminorms. _International Conference on Machine Learning_, 2021.
* [26] Bohdan Kivva, Goutham Rajendran, Pradeep Ravikumar, and Bryon Aragam. Identifiability of deep generative models without auxiliary information. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 15687-15701. Curran Associates, Inc., 2022.
* [27] David Klindt, Lukas Schott, Yash Sharma, Ivan Ustyuzhaninov, Wieland Brendel, Matthias Bethge, and Dylan Paiton. Towards nonlinear disentanglement in natural data with temporal sparse coding. _arXiv preprint arXiv:2007.10930_, 2020.
* [28] Milan Klower and the SpeedyWeather.jl Contributors. Speedyweather.jl, 2023. URL https://github.com/SpeedyWeather/SpeedyWeather.jl.
* [29] Sebastien Lachapelle and Simon Lacoste-Julien. Partial disentanglement via mechanism sparsity. _arXiv preprint arXiv:2207.07732_, 2022.
* [30] Sebastien Lachapelle, Pau Rodriguez, Yash Sharma, Katie E Everett, Remi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ica. In _Conference on Causal Learning and Reasoning_, pages 428-484. PMLR, 2022.
* [31] Sebastien Lachapelle, Tristan Deleu, Divyat Mahajan, Ioannis Mitliagkas, Yoshua Bengio, Simon Lacoste-Julien, and Quentin Bertrand. Synergies between disentanglement and sparsity: Generalization and identifiability in multi-task learning. In _International Conference on Machine Learning_, pages 18171-18206. PMLR, 2023.
* [32] Sebastien Lachapelle, Divyat Mahajan, Ioannis Mitliagkas, and Simon Lacoste-Julien. Additive decoders for latent variables identification and cartesian-product extrapolation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [33] Zijian Li, Ruichu Cai, Zhenhui Yang, Haiqin Huang, Guangyi Chen, Yifan Shen, Zhengming Chen, Xiangchen Song, Zhifeng Hao, and Kun Zhang. When and how: Learning identifiable latent states for nonstationary time series forecasting. _arXiv preprint arXiv:2402.12767_, 2024.
* [34] Hua Liang and Hulin Wu. Parameter estimation for differential equation models using a framework of measurement error in regression models. _Journal of the American Statistical Association_, 103(484):1570-1583, 2008.
* [35] Ernest Lindelof. Sur l'application de la methode des approximations successives aux equations differentielles ordinaires du premier ordre. _Comptes rendus hebdomadaires des seances de l'Academie des sciences_, 116(3):454-457, 1894.

* [36] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Efstratios Gavves. Causal representation learning for instantaneous and temporal effects in interactive systems. In _The Eleventh International Conference on Learning Representations_, 2022.
* [37] Phillip Lippe, Sara Magliacane, Sindy Lowe, Yuki M Asano, Taco Cohen, and Stratis Gavves. Citris: Causal identifiability from temporal intervened sequences. In _International Conference on Machine Learning_, pages 13557-13603. PMLR, 2022.
* [38] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In _international conference on machine learning_, pages 4114-4124. PMLR, 2019.
* [39] Francesco Locatello, Ben Poole, Gunnar Ratsch, Bernhard Scholkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In _International Conference on Machine Learning_, pages 6348-6359. PMLR, 2020.
* [40] Peter Y Lu, Joan Arino Bernd, and Marin Soljacic. Discovering sparse interpretable dynamics from partial observations. _Communications Physics_, 5(1):206, 2022.
* [41] Qi Lyu and Xiao Fu. On finite-sample identifiability of contrastive learning-based nonlinear independent component analysis. In _International Conference on Machine Learning_, pages 14582-14600. PMLR, 2022.
* [42] Qi Lyu, Xiao Fu, Weiran Wang, and Songtao Lu. Understanding latent correlation-based multiview learning and self-supervision: An identifiability perspective. _arXiv preprint arXiv:2106.07115_, 2021.
* [43] Hongyu Miao, Xiaohua Xia, Alan S Perelson, and Hulin Wu. On identifiability of nonlinear ode models and applications in viral dynamics. _SIAM review_, 53(1):3-39, 2011.
* [44] Eric Mjolsness and Dennis DeCoste. Machine learning for science: state of the art and future prospects. _science_, 293(5537):2051-2055, 2001.
* [45] Gemma Elyse Moran, Dhanya Sridhar, Yixin Wang, and David Blei. Identifiable deep generative models via sparse decoding. _Transactions on Machine Learning Research_, 2022. ISSN 2835-8856. URL https://openreview.net/forum?id=vd0onGWZbE.
* [46] Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, and Pietro Lio. On second order behaviour in augmented neural odes. _Advances in neural information processing systems_, 33:5911-5921, 2020.
* [47] Adeel Pervez, Francesco Locatello, and Efstratios Gavves. Mechanistic neural networks for scientific machine learning. _International Conference on Machine Learning_, 2024.
* [48] Xing Qiu, Tao Xu, Babak Soltanalizadeh, and Hulin Wu. Identifiability analysis of linear ordinary differential equation systems with a single trajectory. _Applied Mathematics and Computation_, 430:127260, 2022.
* [49] Maithra Raghu and Eric Schmidt. A survey of deep learning for scientific discovery. _arXiv preprint arXiv:2003.11755_, 2020.
* [50] Goutham Rajendran, Patrik Reizinger, Wieland Brendel, and Pradeep Kumar Ravikumar. An interventional perspective on identifiability in gaussian lti systems with independent component analysis. In _Causal Learning and Reasoning_, pages 41-70. PMLR, 2024.
* [51] Mika Rantanen, Alexey Yu Karpechko, Antti Lipponen, Kalle Nordling, Otto Hyvarinen, Kimmo Ruosteenoja, Timo Vihma, and Ari Laaksonen. The arctic has warmed nearly four times faster than the globe since 1979. _Communications earth & environment_, 3(1):168, 2022.
* [52] James M Robins, Andrea Rotnitzky, and Lue Ping Zhao. Estimation of regression coefficients when some regressors are not always observed. _Journal of the American statistical Association_, 89(427):846-866, 1994.

* Rudy et al. [2017] Samuel H Rudy, Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Data-driven discovery of partial differential equations. _Science advances_, 3(4):e1602614, 2017.
* Scholkopf et al. [2021] Bernhard Scholkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* Scholl et al. [2023] Philipp Scholl, Aras Bacho, Holger Boche, and Gitta Kutyniok. The uniqueness problem of physical law learning. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.
* Schroder and Macke [2023] Cornelius Schroder and Jakob H Macke. Simultaneous identification of models and parameters of scientific simulators. _arXiv preprint arXiv:2305.15174_, 2023.
* Song et al. [2024] Xiangchen Song, Weiran Yao, Yewen Fan, Xinshuai Dong, Guangyi Chen, Juan Carlos Niebles, Eric Xing, and Kun Zhang. Temporally disentangled representation learning under unknown nonstationarity. _Advances in Neural Information Processing Systems_, 36, 2024.
* Squires et al. [2023] Chandler Squires, Anna Seigal, Salil S. Bhate, and Caroline Uhler. Linear causal disentanglement via interventions. In _International Conference on Machine Learning_, volume 202, pages 32540-32560. PMLR, 2023.
* Stanhope et al. [2014] Shelby Stanhope, Jonathan E Rubin, and David Swigon. Identifiability of linear and linear-in-parameters dynamical systems from a single trajectory. _SIAM Journal on Applied Dynamical Systems_, 13(4):1792-1815, 2014.
* Sturma et al. [2024] Nils Sturma, Chandler Squires, Mathias Drton, and Caroline Uhler. Unpaired multi-domain causal representation learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Tonolini et al. [2020] Francesco Tonolini, Bjorn Sand Jensen, and Roderick Murray-Smith. Variational sparse coding. In _Uncertainty in Artificial Intelligence_, pages 690-700. PMLR, 2020.
* Varici et al. [2023] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, and Ali Tajer. Score-based causal representation learning from interventions: Nonparametric identifiability. In _Causal Representation Learning Workshop at NeurIPS 2023_, 2023. URL https://openreview.net/forum?id=MytNJ61XAV.
* Varici et al. [2024] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, and Ali Tajer. General identifiability and achievability for causal representation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2314-2322. PMLR, 2024.
* Villaverde et al. [2016] Alejandro F Villaverde, Antonio Barreiro, and Antonis Papachristodoulou. Structural identifiability of dynamic systems biology models. _PLoS computational biology_, 12(10):e1005153, 2016.
* Kugelgen et al. [2021] Julius Von Kugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Scholkopf, Michel Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content from style. _Advances in neural information processing systems_, 34:16451-16467, 2021.
* Kugelgen et al. [2024] Julius von Kugelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Kekic, Elias Bareinboim, David Blei, and Bernhard Scholkopf. Nonparametric identifiability of causal representations from unknown interventions. _Advances in Neural Information Processing Systems_, 36, 2024.
* Walter et al. [1997] Eric Walter, Luc Pronzato, and John Norton. _Identification of parametric models from experimental data_, volume 1. Springer, 1997.
* Wenk et al. [2019] Philippe Wenk, Alkis Gotovos, Stefan Bauer, Nico S Gorbach, Andreas Krause, and Joachim M Buhmann. Fast gaussian process based gradient matching for parameter identification in systems of nonlinear odes. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1351-1360. PMLR, 2019.
* Wieland et al. [2021] Franz-Georg Wieland, Adrian L Hauber, Marcus Rosenblatt, Christian Tonsing, and Jens Timmer. On structural and practical identifiability. _Current Opinion in Systems Biology_, 25:60-69, 2021.

* Xu et al. [2024] Danru Xu, Dingling Yao, Sebastien Lachapelle, Perouz Taslakian, Julius von Kugelgen, Francesco Locatello, and Sara Magliacane. A sparsity principle for partially observable causal representation learning. _International Conference on Machine Learning_, 2024.
* Yao et al. [2024] Dingling Yao, Danru Xu, Sebastien Lachapelle, Sara Magliacane, Perouz Taslakian, Georg Martius, Julius von Kugelgen, and Francesco Locatello. Multi-view causal representation learning with partial observability. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=OGtnhKQms.
* Yao et al. [2022] Weiran Yao, Guangyi Chen, and Kun Zhang. Temporally disentangled representation learning. _Advances in Neural Information Processing Systems_, 35:26492-26503, 2022.
* Zhang et al. [2024] Jiaqi Zhang, Kristjan Greenewald, Chandler Squires, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. _Advances in Neural Information Processing Systems_, 36, 2024.
* Zimmermann et al. [2021] Roland S Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel. Contrastive learning inverts the data generating process. In _International Conference on Machine Learning_, pages 12979-12990. PMLR, 2021.

## Appendix A Notation and Terminology

\begin{tabular}{l l} \(f\) & Vector field \\ \(N\) & Dimensionality of parameter \(\bm{\theta}\) \\ \(\bm{\Theta}\) & Parameter domain \\ \(\bm{\theta}\) & Time-invariant parameters for ODE \(f\) \\ \(M\) & Function that maps from \(\bm{\theta}\) to \(f_{\bm{\theta}}\) \\ \(T\) & Number of time steps \\ \(t_{\max}\) & End of the time span \\ \(\mathcal{T}\) & Discretized time grid of size T \\ \(\mathbf{x}(t)\) & ODE solution at time \(t\) \\ \(\mathbf{x}\) & ODE solution for the time grid \(\mathcal{T}\) \\ \(\mathcal{X}\) & State-space domain \\ \(\mathcal{X}^{T}\) & Trajectory domain over time grid \(\mathcal{T}\) \\ \end{tabular}

## Appendix B Proofs

**Corollary 3.1** (Full identifiability with known functional form).: _Consider a trajectory \(\mathbf{x}\in\mathcal{X}^{T}\) generated from a ODE \(f_{\bm{\theta}}(\mathbf{x}(t))\) satisfying Asms. 2.1 and 2.2, let \(\hat{\bm{\theta}}\) be an estimator minimizing the following objective:_

\[\mathcal{L}(\hat{\bm{\theta}})=\left\|F(\hat{\bm{\theta}})-\mathbf{x}\right\| _{2}^{2}\] (2)

_then the parameter \(\bm{\theta}\) is **fully-identified** (Defn. 3.1) by the estimator \(\hat{\bm{\theta}}\)._

Proof.: We begin by showing the global minimum of \(\mathcal{L}(\hat{\bm{\theta}})\) exists and equals zero. Then, we show by contradiction that any estimators \(\hat{\bm{\theta}}\) that obtains this global minimum has to equal the ground truth parameters \(\bm{\theta}\).

**Step 1.** We show that the global minimum zero can be obtained for \(\mathcal{L}(\hat{\bm{\theta}})\). Consider the ground truth parameter \(\bm{\theta}\in\Theta\), then by definition of the ODE solver \(F\) (Defn. 3.2), we have:

\[\mathcal{L}(\bm{\theta})=\left\|F(\bm{\theta})-\mathbf{x}\right\|_{2}^{2}= \left\|\mathbf{x}-\mathbf{x}\right\|_{2}^{2}=0.\] (6)

**Step 2.** Suppose for a contraction that there exists a \(\bm{\theta}^{*}\in\Theta\) that minimizes the loss eq.2 but differs from the ground truth parameters \(\bm{\theta}\), i.e., \(\bm{\theta}^{*}\neq\bm{\theta}\). This implies:

\[\mathcal{L}(\bm{\theta}^{*})=\left\lVert F(\bm{\theta}^{*})-\mathbf{x}\right\rVert _{2}^{2}=0\] (7)

Note that \(\mathcal{L}(\bm{\theta}^{*})\) can be rewritten as:

\[\mathcal{L}(\bm{\theta}^{*})=\sum_{k=1}^{T}\left\lVert F(\bm{\theta}^{*})_{t_ {k}}-\mathbf{x}(t_{k})\right\rVert_{2}^{2}=0\] (8)

To make sure the sum is zero, each individual term has to be zero, that is \(F(\bm{\theta}^{*})_{t_{k}}=\mathbf{x}(t_{k}),\forall t\in\{t_{1},\dots,t_{T}\}\). According to the uniqueness assumption of the ODE (Asm. 2.1), this implies \(\bm{\theta}^{*}=\bm{\theta}\), which leads to a contradiction.

Thus, we have shown that minimizing eq.2 will yield the ground truth parameter \(\bm{\theta}\). In other words, any estimator \(\hat{\bm{\theta}}\) that minimizes eq.2 fully identifies \(\bm{\theta}\). 

**Full identifiability with closed form solution when \(f_{\bm{\theta}}\) is linear in \(\bm{\theta}\)**. We show that a closed-form solution can be obtained through linear least squares when the vector field \(f_{\bm{\theta}}\) is linear in \(\bm{\theta}\) and if we observe a _first-order_ trajectory. A _first-order_ trajectory means the first-order derivatives are included in the state-space vector. This statement is formalized as follows:

**Observation B.1**.: Given a first-order trajectory \((\mathbf{x},\dot{\mathbf{x}})=(\mathbf{x}(t),\dot{\mathbf{x}}(t))_{t\in \mathcal{T}}\) generated from a dynamical system \(f_{\bm{\theta}}(\mathbf{x}(t))\) satisfying Asms. 2.1 and 2.2. In particular, this ODE \(f_{\bm{\theta}}\) can be written as a weighted sum of a set of base functions \(\{\phi_{1},\dots,\phi_{m}\}\), i.e., \(f_{\bm{\theta}}\) is linear in \(\bm{\theta}\):

\[f_{\bm{\theta}}(\mathbf{x}(t))=\sum_{i=1}^{m}\theta_{i}\phi_{i}(\mathbf{x}).\] (9)

Define \(\Phi_{\mathbf{x}}:=[\phi_{i}(\mathbf{x}(t))]_{i\in[m],t\in\mathcal{T}}\in \mathbb{R}^{m\times T}\), then the global optimum of the loss eq.2 is given by

\[\bm{\theta}^{*}=\left(\Phi_{\mathbf{x}}^{\intercal}\Phi_{\mathbf{x}}\right)^{ -1}\phi_{\mathbf{x}}\dot{\mathbf{x}}\] (10)

As a direct implication, SINDy-like approaches [9, 10, 40] and gradient matching [68] can fully identify the underlying physical parameters \(\bm{\theta}\) even with a closed-form solution if the underlying vector field \(f_{\bm{\theta}}\) is can be represented as a sparse weighted sum of the given base functions \(\{\phi_{i}\}_{i\in[m]}\).

### Proofs for partial identifiability

**Corollary 3.2** (Identifiability without known functional form).: _Assume a dynamical system \(f\) satisfying Asms. 2.1 and 2.2, a pair of trajectories \(\mathbf{x},\tilde{\mathbf{x}}\) generated from the same system \(f\) but specified by different parameters \(\bm{\theta},\tilde{\bm{\theta}}\), respectively. Assume a partition of parameters \(\bm{\theta}_{S}\) with \(S\subseteq[N]\) is shared across the pair of parameters \(\bm{\theta},\tilde{\bm{\theta}}\). Let \(g:\mathcal{X}^{T}\rightarrow\Theta\) be some smooth encoder and \(\hat{F}:\bm{\Theta}\rightarrow\mathcal{X}^{T}\) be some left-invertible smooth solver that minimizes the following objective:_

\[\mathcal{L}(g,\hat{F})=\mathbb{E}_{\mathbf{x},\tilde{\mathbf{x}}}\underbrace {\left\lVert g(\mathbf{x})_{S}-g(\tilde{\mathbf{x}})_{S}\right\rVert_{2}^{2}}_ {Alignment}+\underbrace{\left\lVert\hat{F}(g(\mathbf{x}))-\mathbf{x}\right\rVert _{2}^{2}+\left\lVert\hat{F}(g(\tilde{\mathbf{x}}))-\tilde{\mathbf{x}}\right\rVert _{2}^{2}}_{\text{Sinficiency}},\] (3)

_then the shared partition \(\bm{\theta}_{S}\) is partially identified (Defn. 3.3) by \(g\) in the statistical setting._

Proof.: This proof can be directly adapted from the proofs with by Daunhawer et al. [16], Von Kugelgen et al. [65], Yao et al. [71] with slight modification. So we briefly summarize the **Step 1.** and **Step 2.** that are imported from previous work and focus on the modification (**Step 3.**).

**Step 1.** We show that the loss function eq.3 is lower bounded by zero and construct optimal encoder \(g^{*}:\mathcal{X}^{T}\rightarrow\bm{\Theta}\) that reach this lower bound. Define \(g^{*}:\mathcal{X}^{T}\rightarrow\bm{\Theta}:=F^{-1}\) as the inverse of the ground truth data generating process, i.e., for all trajectories \(\mathbf{x}=F(\bm{\theta})\) that generated from parameter \(\bm{\theta}\), it holds:

\[g^{*}(\mathbf{x})=\bm{\theta}\] (11)

Thus, we have shown that the global minimum _zero_ exists and can be obtained by the inverse mixing function \(F^{-1}:\mathcal{X}^{T}\rightarrow\bm{\Theta}\) (Defn. 3.2).

**Step 2.** We show that any optimal encoders \(g\) that minimizes eq. (3) must have the _alignment_ equal zero, in other words, it has to satisfy the _invariance_ condition, which is formalized as

\[g(\mathbf{x})_{S}=g(\tilde{\mathbf{x}})\qquad a.s.\] (12)

Following Yao et al. [71, Lemma D.3], we conclude that both \(g(\mathbf{x})_{S}\) and \(g(\tilde{\mathbf{x}})_{S}\) can only depend on information about the shared partition about the ground truth parameter \(\bm{\theta}_{S}\). In other words,

\[g(\mathbf{x})_{S}=g(\tilde{\mathbf{x}})_{S}=h(\bm{\theta}_{S})\] (13)

for some smooth \(h:\Theta_{S}\rightarrow\Theta_{S}\).

**Step 3.** At last, we show that \(h\) is invertible. Note that any optimal encoders \(g\) that minimizes eq. (3) must have zero reconstruction error on both \(\mathbf{x}\) and \(\tilde{\mathbf{x}}\). Taking \(\mathbf{x}\) as an example, we have

\[\mathbb{E}\left\|\hat{F}(g(\mathbf{x}))-\mathbf{x}\right\|_{2}^{2}=0\] (14)

which implies

\[\hat{F}(g(\mathbf{x}))=\mathbf{x}\qquad a.s.\] (15)

If two continuous functions \(\hat{F}(g(\mathbf{x}))\) and \(\mathbf{x}\) equals _almost_ everywhere on \(\bm{\Theta}\), then they are equal everywhere on \(\bm{\Theta}\), which implies:

\[\hat{F}(g(\mathbf{x}))=\mathbf{x}\qquad\forall\bm{\theta}\in\bm{\Theta}\] (16)

Substituting \(\mathbf{x}\) with the ground truth generating process \(F\):

\[\hat{F}(g(\mathbf{x}))=F(\bm{\theta})\qquad\forall\bm{\theta}\in\bm{\Theta},\] (17)

applying the left inverse of \(\hat{F}\), we have:

\[\hat{F}^{-1}\circ\hat{F}(g(\mathbf{x}))=\hat{F}^{-1}\circ F(\bm{\theta}) \qquad\forall\bm{\theta}\in\bm{\Theta},\] (18)

i.e.,

\[g(\mathbf{x})=\hat{F}^{-1}\circ F(\bm{\theta})=\hat{F}^{-1}\circ F(\bm{\theta }_{S},\bm{\theta}_{\tilde{S}})\qquad\forall\bm{\theta}\in\bm{\Theta},\] (19)

Define \(h^{*}:=\hat{F}^{-1}\circ F\), note that \(h^{*}\) is bijective as a composition of bijections. Imposing the _invariance_ constraint, we have \(g(\mathbf{x})_{S}=h^{*}(\bm{\theta}_{S},\bm{\theta}_{\tilde{S}})_{S}\). Since \(g(\mathbf{x})_{S}\) cannot depend on \(\bm{\theta}_{\tilde{S}}\) as shown in **Step 2**, we have \(g(\mathbf{x})_{S}=h^{*}_{S}(\bm{\theta}_{S})\) with \(h:=h^{*}_{S}:\bm{\Theta}_{S}\rightarrow\bm{\Theta}_{S}\).

Thus we have shown that \(g(\mathbf{x})_{S}\)_partially_ identifies \(\bm{\theta}_{S}\).

## Appendix C Experimental results

**General remarks.** All models used in the experiments (SS 6) (_Ada-GVAE, TI-MNN, contrastive identifier, mechanistic identifier_) were built upon open-sourced code provided by the original works [39, 47, 71], under the MIT license. For _mechanistic identifiers_, we add a regularizer multiplier on the _alignment_ constraint (Defn. 3.3), which is shown in Tabs. 3 and 5.

### Wind simulation: SpeedyWeather.jl

We simulate global air motion using using the ShallowWaterModel from speedy weather Julia package [28]. We consider a _layer thickness_ as the primary generating factor in ShallowWaterModel varying from \(8\mathrm{e}3[\mathrm{m}]\) to \(2\mathrm{e}4[\mathrm{m}]\), which is a reasonable range given by the climate science literature. Taking the minimal and maximal values, we simulate the wind in a binary fashion and obtain 9024 trajectories across the globe under different conditions. Each trajectory constitutes three output variables discretized on ts=121 time steps, on a 3D resolution grid of size: latitude lat=47; longitude lon=96; level lev=1. The three output variables represent _u wind component_ (parallel to longitude), _v wind component_ (parallel to latitude), and _relative vorticity_, respectively. An illustrative example of all three components is depicted in Fig. 5. further details about the simulation output are provided in Tab. 4. In particular, to train more efficiently, we pre-process the data using a _discrete cosine transform_ (DCT) proposed by Ahmed et al. [1] and only keep the first \(50\%\) frequencies. This is feasible as the original data possesses a certain periodic pattern, as shown in Fig. 6.

For all baselines, we train the model till convergence. More training and test details for the tasks in SS 6.2 are summarized in Tab. 3. To validate identifiability, we use LogisticRegression model from scikit-learn in its default setting to evaluate the classification accuracy in Fig. 3.

### Sea surface temperature: SST-V2

The sea surface temperature data SST-V2 [21] contains the _weekly_ sea surface temperature data from 1990 to 2023, on a resolution grid of \(180\times 360\) (latitudes \(\times\) longitudes). An example input is depicted in Fig. 8. Each time series contains \(1727\) times steps. To generate multiple views that share specific climate properties, we sample two different trajectories from a small neighbor region (\(\pm 5^{\circ}\)) along the _same latitude_, as the latitude differs in the amount of direct sunlight thus directly affecting the sea surface temperature.

For a fair comparison, we train all baselines till convergence following the setup summarized in Tab. 5. Similar to the wind simulation data, we pre-process the SST-V2 data using DCT and keep the first \(25\%\) frequencies, as the latitude-related parameters of interest primarily influence long-term dependencies, such as seasonality, which are predominantly captured by low-frequency components. Fig. 7 shows an example of predicted trajectories over three randomly sampled locations. As for the downstream classification task, we use LogisticRegression model from scikit-learn in its default setting to evaluate the classification accuracy in Tab. 2.

### Experiments and computational resources

In this paper, we train four different models, each over three independent seeds. All 12 jobs ran with \(24\)GB of RAM, \(8\) CPU cores, and a single node GPU, which is, in most cases, NVIDIA GeForce RTX2080Ti. Given different model sizes and convergence rates, the required amount of compute

Figure 5: **Example of wind simulation**: _Left:_ longitudinal wind velocity (\(u\)) [m/s]. _Middle_: latitudinal wind velocity \((v)\)[m/s], _Right_: relative vorticity (\(vor\)) [1/s].

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & **Ada-GVAE** & **TI-MNN** & **Cont. Identifier** & **Mech. Identifier** \\ \hline Pre-process & DCT & DCT & DCT & DCT \\ Encoder & 6-layer MLP & 6-layer MLP & 6-layer MLP & 6-layer MLP \\ Decoder & 6-layer MLP & 6-layer MLP & ✗ & 3 proj. \(\times\) 6-layer MLP \\ Time dim & 121 & 121 & 121 & 121 \\ State dim & 2 & 2 & 2 & 2 \\ Hidden dim & 1024 & 1024 & 1024 & 1024 \\ Latent dim & 12 & 12 & 12 & 12 \\ Optimizer & Adam & Adam & Adam & Adam \\ Adam: learning rate & \(1\mathrm{e}{-5}\) & \(1\mathrm{e}{-5}\) & \(1\mathrm{e}{-5}\) & \(1\mathrm{e}{-5}\) \\ Adam: beta1 & 0.9 & 0.9 & 0.9 & 0.9 \\ Adam: beta2 & 0.999 & 0.999 & 0.999 & 0.999 \\ Adam: epsilon & \(1\mathrm{e}{-8}\) & \(1\mathrm{e}{-8}\) & \(1\mathrm{e}{-8}\) & \(1\mathrm{e}{-8}\) \\ Batch size & 1128 & 1128 & 1128 & 1128 \\ Temperature \(\tau\) & ✗ & ✗ & 0.1 & ✗ \\ Alignment reg. & ✗ & ✗ & ✗ & 10 \\ \# Initial values & \(10\) & \(10\) & ✗ & \(10\) \\ \# Iterations & \textless{} 30,000 & \textless{} 30,000 & \textless{} 30,000 & \textless{} 30,000 \\ \# Seeds & 3 & 3 & 3 & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Training setup for wind simulation in § 6.2. Non-applicable fields are marked with ✗.

could vary slightly, despite the pre-fixed training epochs. Thus, we report an upper bound of the compute hours on NVIDIA GeForce RTX2080Ti. On average, all runs converge within 22 GPU hours. Therefore, the experimental results in this paper can be reproduced with 264 GPU hours.

## Appendix D Discussion

**Why mechanistic neural networks**[47]. As mentioned in SS 4, the ODE solver \(F\) given in Cors. 3.1 and 3.2 can be interpreted as the decoder in a traditional representation learning regime; however, several challenges arise when integrating ODE solving in the training loop: First of all, the ODE solver must be differentiable to utilize the automatic differentiation implementation of the state-of-the-art deep learning frameworks; this obstacle has been tacked by the line of work termed _NeuralODE_, which models the ODE vector field using a neural network thus enable differentiability [12, 13, 25]. Nevertheless, most differentiable ODE solvers solve the ODE autoregressively and thus cannot be parallelized by the GPU very efficiently. Dealing with long-term trajectories (for example, weekly climate data during the last few decades) would be extremely computationally heavy. Therefore, we advocate for a time- and memory-efficient differentiable ODE solver: the mechanistic neural networks [47].

\begin{table}
\begin{tabular}{l l} \hline \hline Output variable [unit] & Shape \\ \hline Longitudinal wind velocity (\(u\)) [m/s] & (ts, lev, lat, lon) \\ Latitudinal wind velocity (\(v\)) [m/s] & (ts, lev, lat, lon) \\ Relative vorticity (\(vor\)) [1/s] & (ts, lev, lat, lon) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Wind simulation**: output variables.

Figure 6: **Wind simulation**: _mechanistic identifier_ reconstruction of highly irregular time series. The first half of the trajectory is provided as initial values, while the second half is predicted.

Figure 7: **SST-V2**: _mechanistic identifier_ reconstruction over long-term time series. Results are produced by concatenating subsequently predicted chunks.

**Latent regularizers in CRL.** The framework proposed in SS 4 can be generalized to many causal representation learning works, by specifying the latent regularizes according to individual assumptions and settings. For example, in the multiview setting, the latent regularizer can be the \(L_{2}\)\(alignment\) between the learned representations on the shared partition eq. (3), as it was assumed that the paired views are generated based on this overlapping set of latents [38, 65, 71]; in sparse causal representation learning the underlying generative process assumes observations are generated from sparse latent variables; therefore, the proposed algorithms actively enforce some sparsity constraint on the learned representation [29, 31, 45, 70], We provide a more extensive summary of different causal representation learning approaches and their corresponding latent regularizer in Tab. 6. By replacing the _alignment_ term (Cor. 3.2) with the specific latent constraints, one can plug in many causal representation learning algorithms to construct an identifiable neural emulator using our framework.

Figure 8: **Example of global sea surface temperature in January, 1990.**

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & **Ada-GVAE** & **TI-MNN** & **Cont. Identifier** & **Mech. Identifier** \\ \hline Pre-process & DCT & DCT & DCT & DCT \\ Encoder & 6-layer MLP & 6-layer MLP & 6-layer MLP & 6-layer MLP \\ Decoder & 6-layer MLP & 6-layer MLP & ✗ & 3 proj. \(\times\) 6-layer MLP \\ Time dim & \(208\) & \(208\) & \(208\) & \(208\) \\ State dim & 1 & 1 & 1 & 1 \\ Hidden dim & 1024 & 1024 & 1024 & 1024 \\ Latent dim & 20 & 20 & 20 & 20 \\ Optimizer & Adam & Adam & Adam & Adam \\ Adam: learning rate & \(1{\rm e}{-}5\) & \(1{\rm e}{-}5\) & \(1{\rm e}{-}5\) & \(1{\rm e}{-}5\) \\ Adam: beta1 & 0.9 & 0.9 & 0.9 & 0.9 \\ Adam: beta2 & 0.999 & 0.999 & 0.999 & 0.999 \\ Adam: epsilon & \(1{\rm e}{-}8\) & \(1{\rm e}{-}8\) & \(1{\rm e}{-}8\) & \(1{\rm e}{-}8\) \\ Batch size & 2160 & 2160 & 2160 & 2160 \\ Temperature \(\tau\) & ✗ & ✗ & 0.1 & ✗ \\ Alignment reg. & ✗ & ✗ & 10 \\ \# Initial values & \(10\) & \(10\) & ✗ & \(10\) \\ \# Iterations & \textless{} 30,000 & \textless{} 30,000 & \textless{} 30,000 & \textless{} 30,000 \\ \# Seeds & 3 & 3 & 3 & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Training setup for sea surface temperature in § 6.3. Non-applicable fields are marked with ✗.

**Identifying time-varying parameters** Time-varying parameters \(\bm{\theta}(t)\) could also be potentially identified when they change sparsely in time. For example, a time-varying parameter \(\bm{\theta}_{k}\) remains constant between \((t_{k},t_{k+1})\). Then, the states in between \(\mathbf{x}(t),\mathbf{x}(t+1),\ldots,\mathbf{x}(t+k)\) can be considered as multiple views that share the same parameter \(\bm{\theta}_{k}\). Following this perspective, the time-invariant parameters considered in the scope of this paper remain consistent through the whole timespan \((0,t_{\max}\), thus all discretized states \(\mathbf{x}(t_{1}),\ldots,\mathbf{x}(t_{T})\) are views that share this parameter. This inductive bias is directly built into the architecture design by inputting the whole trajectory into the encoder instead of doing so step by step (where the time axis is considered as batch dimension). From another angle, the time-varying parameters \(\bm{\theta}(t)\) could be interpreted as a _hidden_ part of the state space vector \(\mathbf{x}(t)\) without an explicitly defined differential equation, which gives rise to a partial observable setup. This direction has been studied in the context of sparse system identification without explicit identifiability analysis [40]. In a more general setting, time-varying parameters has been considered as latent trajectories and extensively studied in the field of temporal causal representation learning [33, 57, 72]. A more detailed related work section in this regard is provided in SS 5 under **CRL and dynamical systems**.

**Model evaluation on real-world data.** A great obstacle hindering causal representation learning scaling to real-world data is that no ground truth latent variables are available. Since the methods aim to _identify_ the latent variables, it is hard to validate the identifiability theory without ground truth-generating factors. However, properly evaluating the CRL models on real-world data can be conducted by carefully designing causal downstream tasks, such as climate zone classification and ATE estimation shown in SS 6.3. Overall, we believe by incorporating domain knowledge of the applied datasets, we can use CRL to answer important causal questions from individual fields, thus indirectly validating the identifiability.

\begin{table}
\begin{tabular}{l l l l} \hline \hline
**Principle** & **Assumption** & **Latent regularizer** & **References** \\ \hline \multirow{3}{*}{_multiview_} & _part. shared_ latents & \(\|g(\mathbf{x})_{S}-g(\tilde{\mathbf{x}})_{S}\|_{2}^{2}\) & Locatello et al. [39], Von Kügelen et al. [65] \\  & & & Daunhawer et al. [16], Yao et al. [71] \\  & & \(\|g(\tilde{\mathbf{x}})-g(\mathbf{x})-\delta\|_{2}^{2}\) & Ahuja et al. [2] \\ \hline \multirow{3}{*}{_sparsity_} & _sparse_ causal graph & \(\|g(\mathbf{x})\|_{1}\) & Lachapelle et al. [31], Xu et al. [70] \\  & & Spike and Slab prior & Moran et al. [45], Tonolini et al. [61] \\ \cline{1-1}  & temporal sparsity & \(\text{KL}\left(q(z^{t}\mid x^{t})\|\hat{p}(z^{t}|z^{<t},a^{<t})\right)\) & Lachapelle and Lacoste-Julien [29] \\ \hline \hline \end{tabular}
\end{table}
Table 6: A non-exhaustive summary of latent regularizers in recent CRL approaches.

\begin{table}
\begin{tabular}{l|l|l|l} \hline \hline
**ID** & **System description** & **Equation** & **RMSE (m** \\  & & & \(\pm\)**std**) \\ \hline
1 & RC-circuit (charging capacitor) & \(\frac{\theta_{0}-\frac{\varphi_{0}}{\theta_{1}}}{\theta_{2}}\) & \(3_{\mathrm{e}-2\pm 2e-2}\) \\ \hline
2 & Population growth (naive) & \(\theta_{0}x_{0}\) & \(2_{\mathrm{e}-5\pm 9e-6}\) \\ \hline
3 & Population growth with carrying capacity & \(\theta_{0}x_{0}\left(-\frac{x_{0}}{\theta_{1}}\right)\) & \(4_{\mathrm{e}-5\pm 2e-6}\) \\ \hline
4 & RC-circuit with non-linear resistor (charging capacitor) & \(-0.5^{+}-\frac{1}{\theta_{0}}\frac{1}{\theta_{1}}\) & \(8_{\mathrm{e}-5\pm 4e-4}\) \\ \hline
5 & Velocity of a falling object with air resistance & \(\theta_{0}-\theta_{1}x_{0}^{2}\) & \(6_{\mathrm{e}-4\pm 4e-4}\) \\ \hline
6 & Autocatalysis with one fixed abundant chemical & \(\theta_{0}x_{0}-\theta_{1}x_{0}^{2}\) & \(9_{\mathrm{e}-5\pm 1e-5}\) \\ \hline
7 & Gompertz law for tumor growth & \(\theta_{0}x_{0}\log\left(\theta_{1}x_{0}\right)\) & \(3_{\mathrm{e}-2\pm 4e-2}\) \\ \hline
8 & Logistic equation with Allee effect & \(\theta_{0}x_{0}\left(-1+\frac{\varphi_{0}}{\theta_{1}}\right)\left(1-\frac{ \varphi_{0}}{\theta_{1}}\right)\) & \(8_{\mathrm{e}-3\pm 9e-3}\) \\ \hline
9 & Language death model for two languages & \(\theta_{0}\left(1-x_{0}\right)-\theta_{1}x_{0}\) & \(1_{\mathrm{e}-4\pm 3e-5}\) \\ \hline
10 & Refined language death model for two languages & \(\theta_{0}x_{0}^{\theta_{1}}\left(1-x_{0}\right)-x_{0}\left(1-\theta_{0}\right) \left(1-x_{0}\right)^{\theta_{1}}\) & \(1_{\mathrm{e}-5\pm 2e-5}\) \\ \hline
11 & Naive critical slowing down (statistical mechanics) & \(-x_{0}^{2}\) & \(\not\!\!X\) \\ \hline
12 & Photons in a laser (simple) & \(\theta_{0}x_{0}-\theta_{1}x_{0}^{2}\) & \(4_{\mathrm{e}-3\pm 3e-3}\) \\ \hline
13 & Overdamped bead on a rotating hoop & \(\theta_{0}(\theta_{1}\cos\left(r_{0}\right)-1)\sin\left(x_{0}\right)\) & \(3_{\mathrm{e}-4\pm 7e-5}\) \\ \hline
14 & Budworm outbreak model with predation & \(\theta_{0}x_{0}\left(1-\frac{\varphi_{0}}{\theta_{1}}\right)-\frac{\theta_{3}x _{0}^{2}}{\theta_{1}^{2}+x_{0}^{2}}\) & \(5_{\mathrm{e}-3\pm 9e-4}\) \\ \hline
15 & Budworm outbreak with predation (dimensionless) & \(\theta_{0}x_{0}\left(1-\frac{\varphi_{0}}{\theta_{1}}\right)-\frac{\varphi_{0} ^{2}}{\theta_{1}^{2}+1}\) & \(4_{\mathrm{e}-5\pm 5e-6}\) \\ \hline
16 & Landau equation (typical time scale tau = 1) & \(\theta_{0}x_{0}-\theta_{1}x_{0}^{2}-\theta_{2}5\) & \(5_{\mathrm{e}-3\pm 1e-2}\) \\ \hline
17 & Logistic equation with harvesting/fishing & \(\theta_{0}x_{0}\left(1-\frac{\varphi_{0}}{\theta_{1}}\right)-\theta_{2}\) & \(6_{\mathrm{e}-4\pm 2e-4}\) \\ \hline
18 & Improved logistic equation with harvesting/fishing & \(\theta_{0}x_{0}\left(1-\frac{\varphi_{0}}{\theta_{1}}\right)-\frac{\varphi_{0} }{\theta_{3}+\varphi_{0}}\) & \(4_{\mathrm{e}-2\pm 2e-2}\) \\ \hline
19 & Improved logistic equation with harvesting/fishing (dimensionless) & \(-\frac{\varphi_{0}x_{0}}{\theta_{1}+x_{0}}+x_{0}\cdot\left(1-x_{0}\right)\) & \(4_{\mathrm{e}-5\pm 2e-5}\) \\ \hline
20 & Autocatalytic gene switching (dimensionless) & \(\theta_{0}-\theta_{1}x_{0}+\frac{\varphi_{0}^{2}}{\theta_{1}^{2}+1}\) & \(2_{\mathrm{e}-5\pm 1e-5}\) \\ \hline
21 & Dimensionally reduced SIR infection model for dead people (dimensionless) & \(\theta_{0}-\theta_{1}x_{0}-\frac{\varphi_{0}}{\theta_{1}}\) & \(8_{\mathrm{e}-6\pm 2e-6}\) \\ \hline
22 & Hysteretic activation of a protein expression & \(\theta_{0}+\frac{\theta_{1}x_{0}^{2}}{\theta_{2}+\varphi_{0}^{2}}-\theta_{3}x _{0}\) & \(3_{\mathrm{e}-2\pm 2e-2}\) \\  & (positive feedback, basal promoter expression) & & \\ \hline
23 & Overdamped pendulum with constant driving torque/fireflies/Josephson junction (dimensionless) & \(\theta_{0}-\sin\left(x_{0}\right)\) & \(8_{\mathrm{e}-6\pm 8e-7}\) \\ \hline
24 & Harmonic oscillator without damping & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}x_{0}\end{array}\right.\) & \(4_{\mathrm{e}-4\pm 2e-5}\) \\ \hline
25 & Harmonic oscillator with damping & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}x_{0}-\theta_{1}x_{1}\end{array}\right.\) & \(9_{\mathrm{e}-4\pm 1e-4}\) \\ \hline
26 & Lotka-Volterra competition model (Strogatz version with sheeps and rabbits) & \(\left\{\begin{array}{l}x_{0}(\theta_{0}-\theta_{1}x_{1}-x_{0})\\ x_{1}(\theta_{2}-x_{0}-x_{1})\end{array}\right.\) & \(7_{\mathrm{e}-2\pm 4e-2}\) \\ \hline
27 & Lotka-Volterra simple (as on Wikipedia) & \(\left\{\begin{array}{l}x_{0}(\theta_{0}-\theta_{1}x_{1})\\ -x_{1}(\theta_{2}-\theta_{3}x_{0})\end{array}\right.\) & \(9_{\mathrm{e}-3\pm 1e-3}\) \\ \hline
28 & Pendulum without friction & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}\sin\left(x_{0}\right)\end{array}\right.\) & \(6_{\mathrm{e}-5\pm 2e-5}\) \\ \hline
29 & Dipole fixed point & \(\left\{\begin{array}{l}\theta_{0}x_{0}x_{1}\\ -x_{0}^{2}+x_{1}^{2}\end{array}\right.\) & \(2_{\mathrm{e}-3\pm 3e-4}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Theorem 3.1 validation using ODEs with known functional form**: Experiments on complex dynamical systems from ODEBench [15] and Cart-Pole (inspired by Yao et al. [72]), for **exact parameter identification**. RMSE is computed over 100 randomly sampled parameter groups (nearby chaotic configuration for chaotic systems) and averaged over the parameter dimension.

\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \hline
**ID** & **System description** & **Equation** & **RMSE (m \(\pm\) std)** \\ \hline \hline
30 & RNA molecules catalyzing each others replication & \(\left\{\begin{array}{l}x_{0}(-\theta_{0}x_{0}x_{1}+x_{1})\\ x_{1}(-\theta_{0}x_{0}x_{1}+x_{0})\end{array}\right.\) \\ \hline
31 & SIR infection model only for healthy and sick & \(\left\{\begin{array}{l}-\theta_{0}x_{0}x_{1}\\ \theta_{0}x_{0}x_{1}-\theta_{1}x_{1}\end{array}\right.\) \\ \hline
32 & Damped double well oscillator & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}x_{1}-x_{0}^{3}+x_{0}\end{array}\right.\) \\ \hline
33 & Glider (dimensionless) & \(\left\{\begin{array}{l}-\theta_{0}x_{0}^{2}-\sin\left(x_{1}\right)\\ x_{0}-\frac{\cos\left(x_{1}\right)}{\sigma_{0}}\end{array}\right.\) \\ \hline
34 & Frictionless bead on a rotating hoop (dimensionless) & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}+\cos\left(x_{0}\right)\sin\left(x_{0}\right)\end{array}\right.\) \\ \hline
35 & Rotational dynamics of an object in a shear flow & \(\left\{\begin{array}{l}\cos\left(x_{0}\right)\cot\left(x_{1}\right)\\ \left(\theta_{0}\sin^{2}\left(x_{1}\right)+\cos^{2}\left(x_{1}\right)\right) \sin\left(x_{0}\right)\end{array}\right.\) \\ \hline
36 & Pendulum with non-linear damping, no driving (dimensionless) & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}x_{1}\cos\left(x_{0}\right)-x_{1}-\sin\left(x_{0}\right)\end{array}\right.\) \\ \hline
37 & Van der Pol oscillator (standard form) & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}x_{1}\left(x_{0}^{2}-1\right)-x_{0}\end{array}\right.\) \\ \hline
38 & Van der Pol oscillator (simplified form & \(\left\{\begin{array}{l}\theta_{0}\left(-\frac{x_{0}^{2}}{3}+x_{0}+x_{1} \right)\\ -\frac{x_{0}}{\theta_{0}}\end{array}\right.\) \\ \hline
39 & Glycolytic oscillator, e.g., ADP and F6P in yeast (dimensionless) & \(\left\{\begin{array}{l}x_{1}\\ -\theta_{0}x_{0}+\theta_{1}-2x_{0}^{2}x_{1}\end{array}\right.\) \\ \hline
40 & Duffing equation (weakly non-linear oscillation) & \(\left\{\begin{array}{l}x_{1}\\ \theta_{0}x_{1}\left(-x_{0}^{2}\right)-x_{0}\end{array}\right.\) \\ \hline
41 & Cell cycle model by Tyson for interaction between protein cdc2 and cyclin (dimensionless) & \(\left\{\begin{array}{l}\theta_{0}\left(\theta_{1}+x_{0}^{2}\right)(-x_{0}+x_{ 1})-x_{0}\\ \theta_{2}-x_{0}\end{array}\right.\) \\ \hline
42 & Reduced model for chlorine dioxide-iodine-malonic acid reaction (dimensionless) & \(\left\{\begin{array}{l}\theta_{0}-\frac{\theta_{1}x_{0}x_{1}}{x_{0}^{2}+1}-x_ {0}\\ \theta_{2}x_{0}-\frac{\theta_{1}}{x_{0}^{2}+1}\end{array}\right.\) \\ \hline
43 & Driven pendulum with linear damping / Josephson junction (dimensionless) & \(\left\{\begin{array}{l}x_{1}\\ \theta_{0}-\theta_{1}x_{1}-\sin\left(x_{0}\right)\end{array}\right.\) \\ \hline
44 & Driven pendulum with quadratic damping (dimensionless) & \(\left\{\begin{array}{l}x_{1}\\ \theta_{0}-\theta_{1}x_{1}|x_{1}|-\sin\left(x_{0}\right)\end{array}\right.\) \\ \hline
45 & Isothermal autocatalytic reaction model by Gray and Scott 1985 (dimensionless) & \(\left\{\begin{array}{l}\theta_{0}\cdot(1-x_{0})-x_{0}x_{1}^{2}\\ -\theta_{1}x_{1}+x_{0}x_{1}^{2}\end{array}\right.\) \\ \hline
46 & Interacting bar magnets & \(\left\{\begin{array}{l}\theta_{0}\sin\left(x_{0}-x_{1}\right)-\sin\left(x_ {0}\right)\\ -\theta_{0}\sin\left(x_{0}-x_{1}\right)-\sin\left(x_{1}\right)\end{array}\right.\) \\ \hline
47 & Binocular rivalry model (no oscillations) & \(\left\{\begin{array}{l}x_{0}+\frac{x_{0}x_{1}-\theta_{1}}{x_{0}^{2}+1}\\ -x_{1}+\frac{x_{0}x_{0}-\theta_{1}}{x_{0}^{2}+1}\end{array}\right.\) \\ \hline
48 & Bacterial respiration model for nutrients and oxygen levels & \(\left\{\begin{array}{l}\theta_{0}-\frac{x_{0}x_{1}}{\theta_{1}^{2}+1}-x_{0} \\ \theta_{2}-\frac{x_{0}x_{1}}{\theta_{1}^{2}+1}\end{array}\right.\) \\ \hline
49 & Brusselator: hypothetical chemical oscillation model (dimensionless) & \(\left\{\begin{array}{l}\theta_{1}x_{0}^{2}+1-x_{0}(\theta_{0}+1)+1\\ -\theta_{0}x_{0}-\theta_{1}x_{0}^{2}+1\end{array}\right.\) \\ \hline
50 & Chemical oscillator model by Schnackenberg & \(\left\{\begin{array}{l}\theta_{0}+x_{0}^{2}x_{1}-x_{0}\\ \theta_{1}-x_{0}^{2}x_{1}\end{array}\right.\) \\ \hline
51 & Oscillator death model by Ermentrout and Kopell 1990 & \(\left\{\begin{array}{l}\theta_{0}+\sin\left(x_{1}\right)\cos\left(x_{0} \right)\\ \theta_{1}+\sin\left(x_{1}\right)\cos\left(x_{0}\right)\end{array}\right.\) \\ \hline \hline \end{tabular}

\begin{tabular}{|c|l|l|l|} \hline
**ID** & **System description** & **Equation** & **RMSE (m** \\  & & & \(\pm\)**std)** \\ \hline \hline
52 & Maxwell-Bloch equations (laser dynamics) & \(\left\{\begin{array}{l}\theta_{0}(-x_{0}+x_{1})\\ \theta_{1}(x_{0}x_{2}-x_{1})\\ \theta_{2}(-\theta_{3}x_{0}x_{1}+\theta_{3}-x_{2}+1)\end{array}\right.\) & \(4e-2\pm 4e-2\) \\ \hline
53 & Model for apoptosis (cell death) & \(\left\{\begin{array}{l}\theta_{0}-\theta_{4}x_{0}-\frac{\theta_{5}x_{0}x_{1} }{\theta_{4}+x_{1}}\\ \theta_{1}x_{2}(\theta_{8}+x_{1})-\theta_{6}x_{1}-\frac{\theta_{5}x_{0}x_{1} }{\theta_{4}+x_{1}}\\ -\theta_{1}x_{2}(\theta_{8}+x_{1})+\frac{\theta_{5}x_{1}}{\theta_{4}+x_{1}}+ \frac{\theta_{5}x_{1}}{\theta_{4}+x_{1}}\end{array}\right.\) & \(1e-2\pm 5e-3\) \\ \hline
54 & Lorenz equations in well-behaved periodic regime & \(\left\{\begin{array}{l}\theta_{0}(-x_{0}+x_{1})\\ \theta_{1}x_{0}-x_{0}x_{2}-x_{1}\\ -\theta_{2}x_{2}+x_{0}x_{1}\end{array}\right.\) & \(1e-2\pm 4e-3\) \\ \hline
55 & Lorenz equations in complex periodic regime & \(\left\{\begin{array}{l}\theta_{0}(-x_{0}+x_{1})\\ \theta_{1}x_{0}-x_{0}x_{2}-x_{1}\\ -\theta_{2}x_{2}+x_{0}x_{1}\end{array}\right.\) & \(8e-2\pm 5e-2\) \\ \hline
56 & Lorenz equations (chaotic) & \(\left\{\begin{array}{l}\theta_{0}(-x_{0}+x_{1})\\ \theta_{1}x_{0}-x_{0}x_{2}-x_{1}\\ -\theta_{2}x_{2}+x_{0}x_{1}\end{array}\right.\) & \(4e-2\pm 9e-3\) \\ \hline
57 & R\(\ddot{\rm o}\)ssler attractor (stable fixed point) & \(\left\{\begin{array}{l}\theta_{3}(-x_{1}-x_{2})\\ \theta_{3}(\theta_{0}x_{1}+x_{0})\\ \theta_{3}(\theta_{0}x_{1}+x_{0})\\ \theta_{3}(\theta_{1}+x_{2}(-\theta_{2}+x_{0}))\end{array}\right.\) & \(3e-2\pm 2e-2\) \\ \hline
58 & R\(\ddot{\rm o}\)ssler attractor (periodic) & \(\left\{\begin{array}{l}\theta_{3}(-x_{1}-x_{2})\\ \theta_{3}(\theta_{0}x_{1}+x_{0})\\ \theta_{3}(\theta_{0}x_{1}+x_{0})\\ \theta_{3}(\theta_{1}+x_{2}(-\theta_{2}+x_{0}))\end{array}\right.\) & \(2e-2\pm 2e-2\) \\ \hline
59 & R\(\ddot{\rm o}\)ssler attractor (chaotic) & \(\left\{\begin{array}{l}\theta_{3}(-x_{1}-x_{2})\\ \theta_{3}(\theta_{0}x_{1}+x_{0})\\ \theta_{3}(\theta_{1}+x_{2}(-\theta_{2}+x_{0}))\end{array}\right.\) & \(4e-2\pm 4e-2\) \\ \hline
60 & Aizawa attractor (chaotic) & \(\left\{\begin{array}{l}-\theta_{3}x_{1}+x_{0}(-\theta_{1}+x_{2})\\ \theta_{3}x_{0}+x_{1}(-\theta_{1}+x_{2})\\ \theta_{0}x_{2}+\theta_{2}\theta_{3}x_{0}x_{2}-1/3x_{2}^{3}-\left(x_{0}^{2}+x_ {1}^{2}\right)(\theta_{4}x_{2}+1)\end{array}\right.\) \\ \hline
61 & Chen-Lee attractor (chaotic) & \(\left\{\begin{array}{l}\theta_{0}x_{0}-x_{1}x_{2}\\ \theta_{1}x_{1}+x_{0}x_{2}\\ \theta_{2}x_{2}+\frac{\theta_{5}x_{1}}{\theta_{4}}\end{array}\right.\) & \(4e-2\pm 2e-2\) \\ \hline
62 & Binocular rivalry model with adaptation (oscillations) & \(\left\{\begin{array}{l}-x_{0}+\frac{1}{x_{0}x_{2}+\frac{1}{x_{1}-\theta_{2}+ 1}}\\ \theta_{3}(x_{0}-x_{1})\\ -x_{2}+\frac{\theta_{5}x_{0}x_{0}+\frac{1}{x_{1}-\theta_{2}+1}}{\theta_{5}x_{ 0}+\frac{1}{x_{1}-\theta_{2}+1}}\\ \theta_{3}(x_{2}-x_{3})\end{array}\right.\) & \(1e-3\pm 2e-4\) \\ \hline
63 & SEIR infection model (proportions) & \(\left\{\begin{array}{l}-\theta_{1}x_{0}x_{2}\\ -\theta_{0}x_{1}+\theta_{1}x_{0}x_{2}\\ \theta_{0}x_{1}-\theta_{2}x_{2}\\ \theta_{2}x_{2}\end{array}\right.\) & \(1e-4\pm 5e-5\) \\ \hline - & Cart-Pole (inverted pendulum) & \(\left\{\begin{array}{l}\tilde{x}=\frac{P+m\left[(\alpha^{2}\sin(\alpha)- \alpha\cos(\alpha)\right)}{m_{c}+m_{p}}\\ \hat{\alpha}=\frac{g\sin(\alpha)-\cos(\alpha)\frac{F+m_{p}x_{0}^{2}\sin( \alpha)}{m_{c}+m_{p}}}{l^{\left(\frac{4}{3}-\frac{m_{p}\cos^{2}(\alpha)}{m_{c}+ m_{p}}\right)}\end{array}\right.\) & \(1e-4\pm 6e-\) \\ \hline \hline \end{tabular}

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide detailed proofs in App. B and successfully demonstrate our main claim in SS 6 with various experiments and datasets. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations were discussed in SS 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Assumptions and theoretical statements are given in SS 3; proofs are provided in App. B. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Details about data generating pipeline, model architecture and training setup are provided in App. C for all baselines and experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The wind simulation data used in SS 6.2 is generated from based on an open source library, which is given in App. C.1. Both code for generating this dataset and training are included in the supplementary material, following NeurIPS code and data submission guidelines. Curated code will be published upon acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the training and test details are included in App. C. Guidelines: * The answer NA means that the paper does not include experiments.

* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: All results are computed from three independent runs, as given in Tabs. 3 and 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Compute-related information is provided in App. C.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors carefully reviewed the NeurIPS Code of Ethics and believe none of the concerns from NeurIPS Code of Ethics applies to this work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). 10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper focuses on general representation learning; thus, there is no immediate positive or negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML). 11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The data or model released by this paper do not have a high risk of misuse.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We carefully cited all datasets and works that were used in this paper. Also, the software we used to generate the simulation dataset is open-sourced under an MIT license (see App. C.1). Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: This work focuses on general representation learning and dynamical systems. Thus, we do not release new assets other than the source code to reproduce the experimental results. This code is attached to the submission and all corresponding training details are documented in App. C for reproducing. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve human subjects or crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve human subjects or crowdsourcing. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.