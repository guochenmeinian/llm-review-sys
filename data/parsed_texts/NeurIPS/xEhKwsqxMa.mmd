# Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning

Yingcong Li

University of California, Riverside

yli692@ucr.edu

&Kartik Sreenivasan &Angeliki Giannou

University of Wisconsin-Madison

{ksreenivasa2,giannou}@wisc.edu

&Dimitris Papailiopoulos

University of Wisconsin-Madison

dimitris@papail.io

&Samet Oymak

University of Michigan & UC Riverside

oymak@umich.edu

###### Abstract

Chain-of-thought (CoT) is a method that enables language models to handle complex reasoning tasks by decomposing them into simpler steps. Despite its success, the underlying mechanics of CoT are not yet fully understood. In an attempt to shed light on this, our study investigates the impact of CoT on the ability of transformers to in-context learn a simple to study, yet general family of compositional functions: multi-layer perceptrons (MLPs). In this setting, we find that the success of CoT can be attributed to breaking down in-context learning of a compositional function into two distinct phases: focusing on and filtering data related to each step of the composition and in-context learning the single-step composition function. Through both experimental and theoretical evidence, we demonstrate how CoT significantly reduces the sample complexity of in-context learning (ICL) and facilitates the learning of complex functions that non-CoT methods struggle with. Furthermore, we illustrate how transformers can transition from vanilla in-context learning to mastering a compositional function with CoT by simply incorporating additional layers that perform the necessary data-filtering for CoT via the attention mechanism. In addition to these test-time benefits, we show CoT helps accelerate pretraining by learning shortcuts to represent complex functions and filtering plays an important role in this process. These findings collectively provide insights into the mechanics of CoT, inviting further investigation of its role in complex reasoning tasks.

## 1 Introduction

The advent of transformers (Vaswani et al., 2017) has revolutionized natural language processing, paving the way for remarkable performance in a wide array of tasks. LLMs, such as GPTs (Brown et al., 2020), have demonstrated an unparalleled ability to capture and leverage vast amounts of data, thereby facilitating near human-level performance across a variety of language generation tasks. Despite this success, a deep understanding of their underlying mechanisms remains elusive.

Chain-of-thought prompting (Wei et al., 2022c) is an emergent ability of transformers where the model solves a complex problem (Wei et al., 2022b), by decomposing it into intermediate steps. Intuitively, this underlies the ability of general-purpose language models to accomplish previously-unseen complex tasks by leveraging more basic skills acquired during the pretraining phase. Compositional learning and CoT has enjoyed significant recent success in practical language modeling tasks spanning question answering, code generation, and mathematical reasoning (Perez et al., 2021; Imani et al., 2023; Yuan et al., 2023). In this work, we attempt to demystify some of the mechanics underlying this success and the benefits of CoT in terms of sample complexity and approximation power. Todo this we explore the role of CoT in learning multi-layer perceptrons (MLPs) in-context, which we believe can lead to a first set of insightful observations. Throughout, we ask:

_Does CoT improve in-context learning of MLPs, and what are the underlying mechanics?_

In this work, we identify and thoroughly compare three schemes as illustrated in Figure 1. (a) ICL: In-context learning from input-output pairs provided in the prompt, (b) CoT-I: Examples in the prompt are augmented with intermediate steps, (c) CoT-I/O: The model also outputs intermediate steps during prediction. Our main contributions are:

* **Decomposing CoT into filtering and ICL:** As our central contribution, we establish a rigorous and experimentally-supported abstraction that decouples CoT prompting into a _filtering phase_ and an _in-context learning (ICL) phase_. In _filtering_, the model attends to the relevant tokens within the prompt based on an instruction. In _ICL_, the model runs inference on the filtered prompt to output a _step_ and then moves onto the next _step_ in the chain. Our Theorem 1 develops a theoretical understanding of this two-step procedure and formalizes how filtering and ICL phases of CoT can be implemented via the self-attention mechanism to learn MLPs.
* in contrast to the \(\Omega(kd)\) lower bound without step-augmented prompt. As predicted by our theory, our experiments (see Sec. 4) identify a striking universality phenomenon (as \(k\) varies) and also demonstrate clear approximation benefits of CoT compared to vanilla ICL.
* **Accelerated pretraining via learning shortcuts:** We construct deep linear MLPs where each layer is chosen from a discrete set of matrices. This is in contrast to the above setting, where MLP weights can be arbitrary. We show that CoT can dramatically accelerate pretraining by memorizing these discrete matrices and can infer all layers correctly from a _single_ demonstration. Notably the pretraining loss goes to zero step-by-step where each step _"learns to filter a layer"_. Together, these showcase how CoT identifies composable shortcuts to avoid the need for solving linear regression. In contrast, we show that ICL (without CoT) collapses to linear regression performance as it fails to memorize exponentially many candidates (due to lack of composition).

The paper is organized as follows. In Section 2, we introduce the problem setup. Section 3 states our main theoretical results which decouple CoT into filtering and ICL. Section 4 provides empirical investigations of CoT with 2-layer MLPs, which validates our theoretical findings. Finally, we

Figure 1: An illustration of ICL, CoT-I and CoT-I/O methods, using a 3-layer MLP as an example (top left, where \(\bm{x}\), \(\bm{y}\), \(\bm{s}^{1}\), \(\bm{s}^{2}\) denote input, output and hidden features respectivaly). The ICL method utilizes in-context examples in the form of \((\bm{x},\bm{y})\) and makes predictions directly based on the provided \(\bm{x}_{\text{test}}\). Both CoT-I and CoT-I/O methods admit prompts with samples formed by \((\bm{x},\bm{s}^{1},\bm{s}^{2},\bm{y})\). However, CoT-I/O uniquely makes recurrent predictions by re-inputting the intermediate output (as shown on the right). The performance of these methods is shown on the bottom left, with a more detailed discussion available in Section 4.

elucidate the benefits of CoT during pretraining via experiments on deep linear MLPs in Section 5. Related work and discussion are provided in Sections 6 and 7, respectively.

## 2 Preliminaries and Setup

_Notation._ We denote the set \(\{1,2,\ldots,n\}\) as \([n]\). Vectors and matrices are represented in bold text (e.g., \(\bm{x},\bm{A}\)), while scalars are denoted in plain text (e.g., \(y\)). The input and output domains are symbolized as \(\mathcal{X}\) and \(\mathcal{Y}\) respectively (unless specified otherwise), and \(\bm{x}\in\mathcal{X}\), \(\bm{y}\in\mathcal{Y}\) denote the input and output. Additionally, let \(\mathcal{F}\) be a set of functions from \(\mathcal{X}\) to \(\mathcal{Y}\). Consider a transition function \(f\in\mathcal{F}\) where \(\bm{y}=f(\bm{x})\). In this section, we explore the properties of learning \(f\), assuming that it can be decomposed into simpler functions \((f_{\ell})_{\ell=1}^{L}\), and thus can be expressed as \(f=f_{L}\circ f_{L-1}\circ\cdots\circ f_{1}\).

### In-context Learning

Following the study by Garg et al. (2022), the fundamental problem of vanilla in-context learning (ICL) involves constructing a prompt with input-output pairs in the following manner:

\[\bm{p}_{n}(f)=(\bm{x}_{i},\bm{y}_{i})_{i=1}^{n}\quad\text{where}\quad\bm{y}_{ i}=f(\bm{x}_{i}).\] (P-ICL)

Here the transition function \(f\in\mathcal{F}:\mathcal{X}\rightarrow\mathcal{Y}\) remains constant within a single prompt but can vary across prompts, and the subscript \(n\) signifies the number of in-context samples contained in the prompt. Considering language translation as an example, \(f\) is identified as the target language, and the prompt can be defined as \(\bm{p}(\texttt{Spanish})\) = ((_apple, manzana_), (_ball, pelota_), \(\ldots\)) or \(\bm{p}(\texttt{French})\)=((_cat, chat_), (_flower, fleur_), \(\ldots\)). Let TF denote any auto-regressive model (e.g., Decoder-only Transformer). The aim of in-context learning is to learn a model that can accurately predict the output, given a prompt \(\bm{p}\) and the test input \(\bm{x}_{\text{test}}\), as shown in the following equation:

\[\texttt{TF}(\bm{p}_{n}(\tilde{f}),\bm{x}_{\text{test}})\approx\tilde{f}(\bm{x }_{\text{test}})\] (2.1)

where \(\tilde{f}\in\mathcal{F}\) is the test function which may differ from the functions used during training. Previous work (Zhou et al., 2022; Li et al., 2023; Li et al., 2023) has demonstrated that longer prompts (containing more examples \(n\)) typically enhance the performance of the model.

### Chain-of-thought Prompt and Prediction

As defined in (P-ICL), the prompt in vanilla ICL only contains input-output pairs of the target function. This demands that the model learns the function \(f\in\mathcal{F}\) in one go, which becomes more challenging as \(\mathcal{F}\) grows more complex, since larger models and increased prompt length (\(n\)) are needed to make correct predictions (as depicted by the green curves in Figures 5 and 6). Existing studies on chain-of-thought methods (e.g., (Wei et al., 2022)) observed that prompts containing step-by-step instructions assist the model in decomposing the function and making better predictions. Specifically, consider a function composed of \(L\) subfunctions, represented as \(f:=f_{L}\circ f_{L-1}\circ\ldots f_{1}\). Each intermediate output can be viewed as a step, enabling us to define a length-\(n\) CoT prompt related to \(f\) with \(L\) steps (expressed with \(\bm{s}^{\ell},\ell\in[L]\)) as follows:

\[\bm{p}_{n}(f)=(\bm{x}_{i},\bm{s}_{i}^{1},\cdots\bm{s}_{i}^{L-1},\bm{s}_{i}^{L })_{i=1}^{n}\quad\text{where}\quad\bm{s}_{i}^{\ell}=f_{\ell}(\bm{s}_{i}^{\ell- 1}),\;\ell\in[L].\] (P-CoT)

Here \(\bm{x}_{i}=\bm{s}_{i}^{0}\), \(\bm{y}_{i}=\bm{s}_{i}^{L}\) and \(f_{\ell}\in\mathcal{F}_{\ell}\), which implies that \(f\in\mathcal{F}_{L}\times\cdots\mathcal{F}_{1}:=\mathcal{F}\).

Next we introduce two methodologies for making predictions within the CoT framework:

**CoT over input only (CoT-I).** Contrasted with ICL, CoT-I considers step-by-step instructions as inputs, nonetheless, the prediction for the last token is performed as a single entity. Our experiments indicate that this approach lowers the sample complexity for TF to comprehend the function \(\tilde{f}\) being learned (see the orange curves in Figures 5 and 6). The CoT-I prediction aligns with Eq. (2.1) as follows, while the prompt is determined by (P-CoT).

\[\text{One-shot prediction:}\texttt{TF}(\bm{p}_{n}(\tilde{f}),\bm{x}_{\text{test}}) \approx\tilde{f}(\bm{x}_{\text{test}}).\] (2.2)

**CoT over both input and output (CoT-I/O).** Despite the fact that CoT-I improves the sample complexity of learning \(\tilde{f}\), the TF must still possess the capacity to approximate functions from the function class \(\mathcal{F}\), given that the prediction is made in one shot. To mitigate this challenge, we consider a scenario where in addition to implementing a CoT prompt, we also carry out CoT predictions. Specifically, for a composed problem with inputs formed via (P-CoT), the model recurrently makes \(L\)-step predictions as outlined below:

Step 1: \[\texttt{TF}(\bm{p}_{n}(\tilde{f}),\bm{x}_{\text{test}}):=\hat{\bm{s}}^{1}\] Step 2: \[\texttt{TF}(\bm{p}_{n}(\tilde{f}),\bm{x}_{\text{test}},\hat{\bm{s}}^{1 }):=\hat{\bm{s}}^{2}\] \[\vdots\] Step \[L\] : \[\texttt{TF}(\bm{p}_{n}(\tilde{f}),\bm{x}_{\text{test}},\hat{\bm{s}}^ {1}\cdots,\hat{\bm{s}}^{L-1})\approx\tilde{f}(\bm{x}_{\text{test}}),\] (2.3)

where at each step (step \(\ell\)), the model outputs an intermediate step (\(\hat{\bm{s}}^{\ell}\)) which is then fed back to the input sequence to facilitate the next-step prediction (\(\hat{\bm{s}}^{\ell+1}\)). Following this strategy, the model only needs to learn the union of the sub-function sets, \(\bigcup_{\ell=1}^{L}\mathcal{F}_{\ell}\), whose complexity scales linearly with the number of steps \(L\). Empirical evidence of the benefits of CoT-I/O over ICL and CoT-I in enhancing sample efficiency and model expressivity is reflected in the blue curves shown in Figures 5 and 6.

### Model Training

In Figure 1 and Section 2, we have discussed vanilla ICL, CoT-I and CoT-I/O methods. Intuitively, ICL can be viewed as a special case of CoT-I (or CoT-I/O) if we assume only one step is performed. Consequently, we will focus on implementing CoT-I and CoT-I/O for model training in the following.

Consider the CoT prompt as in (P-CoT), and assume that \(\bm{x}\sim\mathcal{D}_{\mathcal{X}}\), and \(f_{\ell}\sim\mathcal{D}_{\ell},\ell\in[L]\), where \(L\) denotes the number of compositions/steps, such that the final prediction should approximate \(f(\bm{x})=f_{L}(f_{L-1}\dots f_{1}(\bm{x})):=\bm{y}\in\mathcal{Y}\). We define \(\ell(\hat{\bm{y}},\bm{y}):\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) as a loss function. For simplicity, we assume \(f_{\ell}(\dots f_{1}(\bm{x}))\in\mathcal{Y}\), \(\ell\in[L]\). Let \(N\) represent the in-context window of \(\texttt{TF}\), which implies that \(\texttt{TF}\) can only admit a prompt containing up to \(N\) in-context samples. Generally, our goal is to ensure high prediction performance given any length-\(n\) prompt, where \(n\in[N]\). To this end, we train the model using prompts with length from \(1\) to \(N\) equally and aim to minimize the averaged risk over different prompt size. Assuming the model TF is parameterized by \(\bm{\theta}\) and considering meta learning problem, the objective functions for CoT-I and CoT-I/O are defined as follows.

\[\hat{\bm{\theta}}^{\text{CoT-I}}=\arg\min_{\bm{\theta}}\mathbb{E}_{(\bm{x}_{n })_{n=1}^{N},(f_{\ell})_{\ell=1}^{L}}\left[\frac{1}{N}\sum_{n=1}^{N}\ell(\hat{ \bm{y}}_{n},f(\bm{x}_{n}))\right]\]

and

\[\hat{\bm{\theta}}^{\text{CoT-I/O}}=\arg\min_{\bm{\theta}}\mathbb{E}_{(\bm{x}_{ n})_{n=1}^{N},(f_{\ell})_{\ell=1}^{L}}\left[\frac{1}{NL}\sum_{n=1}^{N}\sum_{ \ell=1}^{L}\ell(\hat{\bm{s}}^{\ell}_{n},\bm{s}^{\ell}_{n})\right]\]

where \(\hat{\bm{y}}_{n}=\texttt{TF}(\bm{p}_{n}(f),\bm{x}_{n})\) and \(\hat{\bm{s}}^{\ell}_{n}=\texttt{TF}(\bm{p}_{n}(f),\bm{x}_{n}\cdots\bm{s}^{ \ell-1}_{n})\). \(\bm{p}_{n}(f)\) is given by (P-CoT), and as mentioned previously, \(\bm{s}^{0}=\bm{x}\) and \(\bm{s}^{L}=\bm{y}\). All \(\bm{x}\) and \(f_{\ell}\) are independent, and we take the expectation of the risk over their respective distributions.

## 3 Provable Approximation of MLPs via Chain-of-Thought

In this section, we present our theoretical findings that demonstrate how CoT-I/O can execute filtering over the CoT prompt, thereby learning a 2-layer MLP with input dimension of \(d\) and hidden dimension of \(k\), akin to resolving \(k\)\(d\)-dimensional ReLU problems and \(1\)\(k\)-dimensional linear regression problem. Subsequently, in Section 4.1, we examine the performance of CoT-I/O when learning 2-layer random MLPs. Our experiments indicate that CoT-I/O needs only \(O(\max(d,k))\) in-context samples to learn the corresponding MLP.

We state our main contribution of establishing a result that decouples CoT-based in-context learning (CoT-I/O) into two phases: (1) _Filtering Phase:_ Given a prompt that contains features of multiple MLP layers, retrieve only the features related to a target layer to create an ICL prompt. (2) _ICL Phase:_ Given filtered prompt, learn the target layer weights through gradient descent. Combining these two phases, and looping over all layers, we will show that there exists a transformer architecture such that CoT-I/O can provably approximate a multilayer MLP up to a given resolution. An illustration is provided in Figure 2. To state our result, we assume access to an oracle that performs linear regression and consider the consider the condition number of the data matrix.

**Definition 1** (MLP and condition number): _Consider a multilayer MLP defined by the recursion \(\bm{s}_{i}^{\ell}=\phi(\bm{W}_{\ell}\bm{s}_{i}^{\ell-1})\) for \(\ell\in[L]\), \(i\in[n]\) and \(\bm{s}_{i}^{0}=\bm{x}_{i}\). Here \(\phi(x):=\max(\alpha x,x)\) is a Leaky-ReLU activation with \(1\geq\alpha>0\). Define the feature matrix \(\bm{\Upsilon}_{\ell}=[\bm{s}_{1}^{\ell}\ \dots\ \bm{s}_{n}^{\ell}]^{\top}\) and define its condition number \(\kappa_{\ell}=\sigma_{\max}(\bm{\Upsilon}_{\ell})/\sigma_{\min}(\bm{\Upsilon }_{\ell})\) (with \(\sigma_{\min}:=0\) for fat matrices) and \(\kappa_{\max}=\max_{0\leq\ell<L}\kappa_{\ell}\)._

**Assumption 1** (Oracle Model): _We assume access to a transformer \(\mathtt{TF}_{\mathtt{LR}}\) which can run \(T\) steps of gradient descent on the quadratic loss \(\mathcal{L}(\bm{w})=\sum_{i=1}^{n}(y_{i}-\bm{w}^{\top}\bm{x}_{i})^{2}\) given a prompt of the form \((\bm{x}_{1},y_{1},\dots,\bm{x}_{n},y_{n})\)._

We remark that this assumption is realistic and has been formally established by earlier work (Giannou et al., 2023; Akyurek et al., 2022). Our CoT abstraction builds on these to demonstrate that CoT-I/O can call a blackbox TF model to implement a compositional function when combined with filtering.

We now present our main theoretical contribution. Our result provides a transformer construction that first filters a particular MLP layer from the prompt through the attention mechanism, then applies in-context learning, and repeats this procedure to approximate the MLP output. The precise statement is deferred to the supplementary material.

**Theorem 1** (CoT\(\Leftrightarrow\)Filtering+ICL): _Consider a CoT prompt \(\bm{p}_{n}(f)\) generated from an \(L\)-layer MLP \(f(\cdot)\) as described in Definition 1, and assume given test example \((\bm{x}_{\textit{test}},\bm{s}_{\textit{test}}^{1},\dots\bm{s}_{\textit{test}} ^{L})\). For any resolution \(\epsilon>0\), there exists \(\delta=\delta(\epsilon)\), iteration choice \(T=\mathcal{O}(\kappa_{\max}^{2}\log(1/\epsilon))\), and a backend transformer construction \(\mathtt{TF}_{\mathtt{RF}}\) such that the concatenated transformer \(\mathtt{TF}=\mathtt{TF}_{\mathtt{LR}}\circ\mathtt{TF}_{\mathtt{RF}}\) implements the following: Let \((\hat{\bm{s}}^{i})_{i=0}^{\ell-1}\) denote the first \(\ell-1\) CoT-I/O outputs of TF where \(\hat{\bm{s}}^{0}=\bm{x}_{\textit{test}}\) and set \(\bm{p}[\ell]=(\bm{p}_{n}(f),\bm{x}_{\textit{test}},\hat{\bm{s}}^{1}\dots\hat{ \bm{s}}^{\ell-1})\). At step \(\ell\), TF implements_

1. **Filtering.** _Define the filtered prompt with input/output features of layer_ \(\ell\)_,_ \[\bm{p}_{n}^{\textit{filter}}=\begin{pmatrix}\dots\bm{0},\bm{s}_{1}^{\ell-1},\ \bm{0}\ \dots\bm{0},\bm{s}_{n}^{\ell-1},\ \bm{0}\ \dots\bm{0},\hat{\bm{s}}^{\ell-1}\\ \dots\bm{0},\ \bm{0},\ \bm{s}_{1}^{\ell}\dots\bm{0},\ \bm{0},\ \bm{s}_{n}^{\ell} \dots\bm{0},\ \bm{0}\end{pmatrix}.\] _There exists a fixed projection matrix_ \(\bm{\Pi}\) _that applies individually on tokens such that the backend output obeys_ \(\|\bm{\Pi}(\mathtt{TF}_{\mathtt{RF}}(\bm{p}[\ell]))-\bm{p}_{n}^{\textit{filter}} \|\leq\delta\)_._
2. **Gradient descent.** _The combined model obeys_ \(\|\mathtt{TF}(\bm{p}[\ell])-\bm{s}_{\textit{test}}^{\ell}\|\leq\ell\cdot\epsilon/L\)_._

\(\mathtt{TF}_{\mathtt{RF}}\) _has constant number of layers independent of_ \(n\) _and_ \(T\)_. Consequently, after_ \(L\) _rounds of CoT-I/O, TF outputs_ \(f(\bm{x}_{\textit{test}})\) _up to_ \(\epsilon\) _accuracy._

Figure 2: Depiction of Theorem 1 that formalizes CoT as a Filtering \(+\) ICL procedure. In this illustration, we utilize in-context samples as depicted in Figure 1, characterizing the CoT prompt by \((\bm{x},\bm{s}^{1},\bm{s}^{2},\bm{y})\). We show a transformer TF, composed of \(\mathtt{TF}_{\mathtt{LR}}\circ\mathtt{TF}_{\mathtt{RF}}\). \(\mathtt{TF}_{\mathtt{RF}}\) facilitates the implementation of filtering on the CoT prompt (refer to (P-CoT), illustrated on the left side of the figure), subsequently generating vanilla ICL prompts (refer to (P-ICL), illustrated on the right side of the figure). During each step of the ICL process, \(\mathtt{TF}_{\mathtt{LR}}\) employs gradient descent techniques to address and solve sub-problems, exemplified by linear regression in the context of solving MLPs.

**Remark 1**: _Note that, this result effectively shows that, with a sufficiently good blackbox transformer \(\text{TF}_{\text{LR}}\) (per Assumption 1), CoT-I/O can learn an \(L\)-layer MLP using in-context sample size \(n>\max_{\ell\in[L]}d_{\ell}\) where \(d_{\ell}\) is the input dimension of \(\ell\)th layer. This is assuming condition number \(\kappa_{\max}\) of the problem is finite as soon as all layers become over-determined. Consequently, CoT-I/O needs \(\max(k,d)\) sample complexity to learn a two layer MLP. This provides a formal justification for the observation that empirical CoT-I/O performance is agnostic to \(k\) as long as \(k\leq d\)._

We provide the concrete filtering statements based on the transformer architecture in Appendix A, and the key components of our construction are the following: (i) Inputs are projected through the embedding layer in which a set of encodings, an enumeration of the tokens (\(1,2,\ldots,N\)), an enumeration of the layers (\(1,2,\ldots,L\)) and an identifier for each layer already predicted are all attached. Notice that this "modification" to the input only depends on the sequence length and is agnostic to the token to-be-predicted. This allows for an automated looping over \(L\) predictions. (ii) We use this information to extract the sequence length \(N\) and the current layer \(\ell\) to-be-predicted. (iii) With these at hand, we construct an 'if-then' type of function using the ReLU layers to filter out the samples that are not needed for the prediction.

## 4 Experiments with 2-layer Random MLPs

For a clear exposition, we initially focus on two-layer MLPs, which represent 2-step tasks (e.g., \(L=2\)). We begin by validating Theorem 1 using the CoT-I/O method, demonstrating that in-context learning for a 2-layer MLP with \(d\) input dimensions and \(k\) hidden neurons requires \(O(\max(d,k))\) samples. The results are presented in Section 4.1. Subsequently, in Section 4.2, we compare three different methods: ICL, CoT-I, and CoT-I/O. The empirical evidence highlights the advantages of CoT-I/O, showcasing its ability to reduce sample complexity and enhance model expressivity.

**Dataset.** Consider 2-layer MLPs with input \(\bm{x}\in\mathbb{R}^{d}\), hidden feature (step-1 output) \(\bm{s}\in\mathbb{R}^{k}\), and output \(y\in\mathbb{R}\). Here, \(\bm{s}=f_{1}(\bm{x}):=(\bm{W}\bm{x})_{+}\) and \(y=f_{2}(\bm{s}):=\bm{v}^{\top}\bm{s}\), with \(\bm{W}\in\mathbb{R}^{k\times d}\) and \(\bm{v}\in\mathbb{R}^{k}\) being the parameters of the first and second layer, and \((x)_{+}=\max(x,0)\) being ReLU activation. The function is composed as \(y=\bm{v}^{\top}(\bm{W}\bm{x})_{+}\). We define the function distributions as follows: each entry of \(\bm{W}\) is sampled via \(\bm{W}_{ij}\sim\mathcal{N}(0,\frac{2}{k})\), and \(\bm{v}\sim\mathcal{N}(0,\bm{I}_{k})\), with inputs being randomly sampled through \(\bm{x}\sim\mathcal{N}(0,\bm{I}_{d})\)1. We apply the quadratic loss in our experiments. To avoid the implicit bias due to distribution shift, both training and test datasets are generated following the same strategy.

Footnote 1: Following this strategy for data generation, the expected norms of \(\bm{x}\), \(\bm{s}\) and \(y\) are equivalent, and the risk curves displayed in the figures are normalized for comparison.

Figure 4: We decouple the composed risk of predicting 2-layer MLPs into risks of individual layers.

Figure 3: Solving 2-layer MLPs with varying input dimension \(d\) and hidden neuron size \(k\).

[MISSING_PAGE_EMPTY:7]

### Comparative Analysis of ICL, CoT-I and CoT-I/O

**Varying model sizes (Figure 5).** We initially assess the benefits of CoT-I/O over ICL and CoT-I across different TF models. With \(d=10\) and \(k=8\) fixed, we train three different GPT-2 models: standard, small and tiny GPT-2. The small GPT-2 has \(6\) layers, \(4\) attention heads per layer and \(128\) dimensional embeddings. The standard GPT-2 consists of twice the number of layers, attention heads and embedding dimensionality compared to the small GPT-2, and tiny GPT-2, on the other hand, possesses only half of these hyperparameters compared to the small GPT-2. We evaluate the performance using prompts containing \(n\) in-context samples, where \(n\) ranges from \(1\) to \(N\) (\(N=100\)). The associated test risks are displayed in Figs. 5(b), 5(c) and 5(d). The blue, orange and green curves correspond to CoT-I/O, CoT-I and ICL, respectively. In Fig. 5(a), we present the averaged risks. The results show that using CoT-I/O, the small GPT-2 can solve 2-layer MLPs with approximately \(60\) samples, while CoT-I requires the standard GPT-2. Conversely, ICL is unable to achieve zero test risk even with the standard GPT-2 model and up to \(100\) samples. This indicates that to learn 2-layer MLPs in a single shot, ICL requires at least \(\mathcal{O}(dk+d)\) samples to restore all function parameters. Conversely, CoT-I and CoT-I/O can leverage implicit samples contained in the CoT prompt. Let \(f_{1}\in\mathcal{F}_{1}\) (first layer) and \(f_{2}\in\mathcal{F}_{2}\) (second layer). By comparing the performance of CoT-I and CoT-I/O, it becomes evident that the standard GPT-2 is capable of learning the composed function \(f=f_{2}\circ f_{1}\in\mathcal{F}\), which the small GPT-2 cannot express.

**Varying MLP widths (Figure 6).** Next, we explore how different MLP widths impact the performance (by varying the hidden neuron size \(k\in\{4,8,16\}\)). The corresponding results are depicted in Figure 6. The blue, orange and green curves in Fig. 6(b), 6(c) and 6(d) correspond to hidden layer sizes of \(k=4\), \(8\), and \(16\), respectively. Fig. 6(a) displays the averaged risks. We keep \(d=10,\ N=100\) fixed and train with the small GPT-2 model. As discussed in Section 4.1, CoT-I/O can learn a 2-layer MLP using around \(60\) samples for all \(k=4,8,16\) due to its capability to deconstruct composed functions. However, CoT-I can only learn the narrow MLPs with \(k=4\), and ICL is unable to learn any of them. Moreover, we observe a substantial difference in the performance of ICL and CoT-I with varying \(k\) (e.g., see averaged risks in Fig. 6(a)). This can be explained by the fact that enlarging \(k\) results in more complex \(\mathcal{F}_{1}\) and \(\mathcal{F}_{2}\), thus making the learning of \(\mathcal{F}=\mathcal{F}_{2}\times\mathcal{F}_{1}\) more challenging for ICL and CoT-I.

## 5 Further Investigation on Deep Linear MLPs

In Section 4, we have discussed the approximation benefits of CoT-I/O and how it in-context learns 2-layer random MLPs by parallel learning of \(k\)\(d\)-dimensional ReLU and 1 \(k\)-dimensional linear regression. In this section, we investigate the capability of CoT-I/O in learning longer compositions. For brevity, we will use CoT to refer to CoT-I/O in the rest of the discussion.

**Dataset.** Consider \(L\)-layer linear MLPs with input \(\bm{x}\in\mathbb{R}^{d}\sim\mathcal{N}(0,\bm{I}_{d})\), and output generated by \(\bm{y}=\bm{W}_{L}\bm{W}_{L-1}\cdots\bm{W}_{1}\bm{x}\), where the \(\ell\)th layer is parameterized by \(\bm{W}_{\ell}\in\mathbb{R}^{d\times d}\), \(\ell\in[L]\). In this work, to better understand the emerging ability of CoT, we assume that each layer draws from the same discrete sub-function set \(\bar{\mathcal{F}}=\{\bm{W}_{k}:\bm{\widehat{W}}_{k}^{\top}\bm{\widehat{W}}_{k }=\bm{I},k\in[K]\}\)3. Therefore, to learn the \(L\)-layer neural net, CoT only needs to learn \(\bar{\mathcal{F}}\) with \(|\bar{\mathcal{F}}|=K\), whereas ICL needs to learn the function set \(\bar{\mathcal{F}}^{L}\), which contains \(K^{L}\) random matrices.

Footnote 3: This assumption ensures that the norm of the feature remains constant across layers ( \(\|\bm{x}\|=\|\bm{y}\|=\|\bm{s}^{\ell}\|\)), enabling fair evaluation across different layers.

Figure 7: Evaluations over deep linear MLPs using CoT-I/O and ICL where CoT-\(X\) represents the \(X\)-step CoT-I/O. Fig. 7(a) illustrates point-to-point meta results where the model is trained with substantial number of samples. In contrast, Fig. 7(b) displays the one-shot performance (with only one in-context sample provided) when making predictions during training.

Composition ability of CoT (Figures 7).Set \(d=5\), \(L=6\) and \(K=4\). At each round, we randomly select \(L\) matrices \(\bm{W}_{\ell}\), \(\ell\in[L]\) from \(\mathcal{F}\) so that for any input \(\bm{x}\), we can form a chain

\[\bm{x}\rightarrow\bm{s}^{1}\rightarrow\bm{s}^{2}\cdot\cdot\cdot\rightarrow\bm {s}^{6}:=\bm{y},\]

where \(\bm{s}^{\ell}=\bm{W}_{\ell}\bm{s}^{\ell-1}\), \(\ell\in[L]\) and \(\bm{s}^{0}:=\bm{x}\). Let CoT-\(X\) denote \(X\)-step CoT-I/O method. For example, the in-context sample of CoT-6 has form of \((\bm{x},\bm{s}^{1},\bm{s}^{2},\ldots\bm{s}^{5},\bm{y})\), which contains all the intermediate outputs from each layer; while CoT-3, CoT-2 have prompt samples formed as \((\bm{x},\bm{s}^{2},\bm{s}^{4},\bm{y})\) and \((\bm{x},\bm{s}^{3},\bm{y})\) respectively. In this setting, ICL is also termed as CoT-1, as its prompt contains only \((\bm{x},\bm{y})\) pairs. To solve the length-6 chain, CoT-\(X\) needs to learn a model that can remember \(4^{6/X}\) matrices. Therefore, ICL is face a significantly challenge sincd it needs to remember \(4^{6}=4,096\) matrices (all combinations of the 4 matrices used for training and testing) compared to just 4 for CoT-6.

We train small GPT-2 models using the CoT-2/-3/-6 and ICL methods, and present the results in Fig. 7(a). As evident from the figure, the performance curves of CoT-2 (orange), CoT-3 (green) and CoT-6 (red) overlap, and they can all make precise predictions in one shot (given an in-context example \(n=1\)). It seems that TF has effectively learned to remember up to \(64\) matrices (for CoT-2) and compose up to \(6\) layers (for CoT-6). However, ICL (blue) struggles to learn the 6-layer MLPs in one shot. The black dashed curve shows the solution for linear regression \(y=\bm{\beta}^{\top}\bm{x}\) computed directly via least squares given \(n\) random training samples, where \(\bm{x}\) is the input and \(y\) is from the output of the 6-layer MLPs (e.g., \(\bm{y}[0]\)). The test risks for \(n=1,\ldots 10\) are plotted (in Fig. 7(a)), which show that the ICL curve aligns with the least squares performance. This implies that, instead of remembering all \(4,096\) matrices, ICL solves the problem from the linear regression phase.

In addition to the meta-learning results which highlight the approximation benefits of multi-step CoT, we also investigate the convergence rate of CoT-2/-3/-6 and ICL, with results displayed in Fig. 7(b). We test the one-shot performance during training and find that CoT-6 converges fastest. This is because it has the smallest sub-function set, and given the same tasks (e.g., deep neural nets), shortening the chain leads to slower convergence. This supports the evidence that taking more steps facilitates faster and more effective learning of complex problems.

Evidence of Filtering (Figure 8).As per Theorem 1 and the appendix, transformers can perform filtering over CoT prompts, and the results from 2-layer MLPs align with our theoretical findings. However, can we explicitly observe filtering behaviors? In Fig. 8(a), we display the results of the first 50k iterations from Fig. 7(b), and observe risk drops in CoT-6 (red) at the 15k and 25k iteration (shown as grey dotted and dashed lines). Subsequently, in Fig. 8(b), we plot the test risk of each layer prediction (by feeding the model with correct intermediate features not the predicted ones), where CoT-6 (red) predicts the outputs from all 6 layers (\(\bm{s}^{1},\cdots,\bm{s}^{L}\)). From these figures, we can identify risk drops when predicting different layers, which appear at either 15k (for layer 2, 3, 4, 5, 6) or 25k (for layer 1) iteration. This implies that the model learns to predict each step/layer function independently. Further, we test the filtering evidence of the \(\ell\)th layer by filling irrelevant positions with random features. Specifically, an in-context example is formed by

\[(\bm{z}^{0},\cdots,\bm{s}^{\ell-1},\bm{s}^{\ell},\bm{z}^{\ell+1},\ldots\bm{z}^ {L}),\ \ \text{where}\ \ \bm{s}^{\ell}=\bm{W}_{\ell}(\bm{s}^{\ell-1})\ \ \text{and}\ \ \bm{z}\sim\mathcal{N}(0,\bm{I}_{d}).\]

The test risks are represented by black dotted curves in Fig. 8(b), which aligned precisely with the CoT-6 curves (red). This signifies that each layer's prediction concentrate solely on the corresponding intermediate steps in the prefix, while disregarding irrelevant features. This observation provides evidence that the process of filtering is indeed performed.

Figure 8: Fig. 8(a) is generated by magnifying the initial 50k iterations of Fig. 7(b), and we decouple the composed risks from predicting \(6\)-layer linear MLPs into predictions for each layer, and the results are depicted in Fig. 8(b). Additional implementation details can be found in Section 5.

## 6 Related Work

With the success of LLMs and prompt structure (Lester et al., 2021), there is growing interest in in-context learning (ICL) from both theoretical and experimental lens (Garg et al., 2022, Brown et al., 2020, von Oswald et al., 2022, Dai et al., 2022, Min et al., 2022, Lyu et al., 2022, Li et al., 2023c, Balim et al., 2023, Xie et al., 2021, Min et al., 2021, Wei et al., 2023, Li et al., 2023a). As an extension, chain-of-thought (CoT) prompts have made impressive improvements in performing complex reasoning by decomposing it into step-by-step intermediate solutions (Wei et al., 2022c, Narang et al., 2020, Lampinen et al., 2022, Wei et al., 2022b, Zhou et al., 2022, Nye et al., 2021, Velickovic and Blundell, 2021, Lanchantin et al., 2023), which in general, shows the ability of transformer in solving compositional functions. Lee et al. (2023), Dziri et al. (2023) study the problem of teaching arithmetic to small transformers and show that breaking the task down into small reasoning steps allows the model to learn faster. Li et al. (2023b) show that small student transformer models can learn from rationalizations sampled from significantly larger teacher models. The idea of learning how to compose skills has been well studied in other literatures (Sahni et al., 2017, Liska et al., 2018). More specifically, for the problem of learning shallow networks, there are several well known hardness results Goel et al. (2017, 2020), Zhang et al. (2019). In particular, Hahn and Goyal (2023) shows a formal learnability bound which implies that compositional structure can benefit ICL. However, most of the work focuses on investigating empirical benefits and algorithmic designing of CoT, and there exists little effort studying the underlying mechanisms of CoT.

Considering the expressivity of the transformer architecture itself, Yun et al. (2019) showed that TFs are universal sequence to sequence approximators. More recently, Giannou et al. (2023) use an explicit construction to show that shallow TFs can be used to run general purpose programs as long as we loop them. Other works have also shown the turing-completeness of the TF architecture but these typically require infinite/high precision and recursion around attention layers (Wei et al., 2022a, Perez et al., 2019, 2021, Liu et al., 2022). Closer to our work, Akyurek et al. (2022), Von Oswald et al. (2023), von Oswald et al. (2023) prove that a transformer with constant number of layers can implement gradient descent in solving linear regression, and Giannou et al. (2023) introduce similar results by looping outputs back into inputs. Ahn et al. (2023) prove this result from an optimization perspective and show that the global minimum of the training objective implements a single iteration of preconditioned gradient descent for transformers with a single layer of linear attention. Zhou et al. (2023) introduce the RASP-Generalization Conjecture which says that Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. In this work, we prove CoT can be treated as: first apply filtering on the CoT prompts using special construction, and then in-context learn the filtered prompt.

## 7 Conclusion, Limitations, and Discussion

In this work, we investigate chain-of-thought prompting and shed light on how it enables compositional learning of multilayer perceptrons step-by-step. Specially, we have explored and contrasted three methods: ICL, CoT-I and CoT-I/O, and found that CoT-I/O facilitates better approximation and faster convergence through looping and sample efficiency. Additionally, we empirically and theoretically demonstrated that to learn a 2-layer MLP with \(d\)-dimensional input and \(k\) neurons, CoT-I/O requires \(\mathcal{O}(\max(d,k))\) in-context samples whereas ICL runs into approximation error bottlenecks.

While we have provided both experimental and theoretical results to validate the advantages of CoT, it is important to note that our analysis in the main text pertains to in-distribution scenarios. In an effort to address this limitation and demonstrate the robustness of CoT, we have conducted additional simulations, as detailed in Appendix C.1, where the test samples follow a different distribution than the training examples. Also, we note that our focus has been primarily on MLP-based tasks, where the subproblems are essentially instances of simple linear regression. It would be valuable to explore how CoT might influence the training of tasks characterized by more complex structures, longer compositional chains, and a broader variety of subproblems.

There are several interesting avenues for future research to build on our findings. To what extent does our decoupling of CoT (filtering followed by ICL) align with the empirical evidence in practical problems such as code generation and mathematical reasoning? We have shown that CoT-I/O can rely on a linear regression oracle to learn an MLP. To what extent can transformers approximate MLPs without CoT-I/O (e.g. with CoT-I) and what are the corresponding lower/upper bounds?

## 8 Acknowledgements

This work was supported by the NSF CAREER awards #2046816 and #1844951, NSF grant #2212426, AFOSR & AFRL Center of Excellence Award FA9550-18-1-0166, a Google Research Scholar award, an Adobe Data Science Research award, an Army Research Office grant W911NF2110312, and an ONR Grant No. N00014-21-1-2806.

## References

* Ahn et al. (2023) Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. _arXiv preprint arXiv:2306.00297_, 2023.
* Akyurek et al. (2022) Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* Balim et al. (2023) Haldun Balim, Zhe Du, Samet Oymak, and Necmiye Ozay. Can transformers learn optimal filtering for unknown systems? _arXiv preprint arXiv:2308.08536_, 2023.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Dai et al. (2022) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn inconttext? language models secretly perform gradient descent as meta optimizers. _arXiv preprint arXiv:2212.10559_, 2022.
* Dziri et al. (2023) Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D Hwang, et al. Faith and fate: Limits of transformers on compositionality. _arXiv preprint arXiv:2305.18654_, 2023.
* Garg et al. (2022) Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* Giannou et al. (2023) Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos. Looped transformers as programmable computers. _arXiv preprint arXiv:2301.13196_, 2023.
* Goel et al. (2017) Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. In _Conference on Learning Theory_, pages 1004-1042. PMLR, 2017.
* Goel et al. (2020) Surbhi Goel, Adam Klivans, Pasin Manurangsi, and Daniel Reichman. Tight hardness results for training depth-2 relu networks. _arXiv preprint arXiv:2011.13550_, 2020.
* Hahn and Goyal (2023) Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction. _arXiv preprint arXiv:2303.07971_, 2023.
* Imani et al. (2023) Shima Imani, Liang Du, and Harsh Shrivastava. Mathpromter: Mathematical reasoning using large language models. _arXiv preprint arXiv:2303.05398_, 2023.
* Lampinen et al. (2022) Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? _arXiv preprint arXiv:2204.02329_, 2022.
* Lanchantin et al. (2023) Jack Lanchantin, Shubham Toshniwal, Jason Weston, Arthur Szlam, and Sainbayar Sukhbaatar. Learning to reason and memorize with self-notes. _arXiv preprint arXiv:2305.00833_, 2023.
* Lee et al. (2023) Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, and Dimitris Papailiopoulos. Teaching arithmetic to small transformers. _arXiv preprint arXiv:2307.03381_, 2023.
* Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021.
* Lester et al. (2021)Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. _arXiv preprint arXiv:2302.06015_, 2023a.
* Li et al. (2023b) Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic chain-of-thought distillation: Small models can also" think" step-by-step. _arXiv preprint arXiv:2306.14050_, 2023b.
* Li et al. (2023c) Yingcong Li, M Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. _International Conference on Machine Learning_, 2023c.
* Liska et al. (2018) Adam Liska, German Kruszewski, and Marco Baroni. Memorize or generalize? searching for a compositional rnn in a haystack. _arXiv preprint arXiv:1802.06467_, 2018.
* Liu et al. (2022) Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Transformers learn shortcuts to automata. _arXiv preprint arXiv:2210.10749_, 2022.
* Lyu et al. (2022) Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, and Hannaneh Hajishirzi. Z-icl: Zero-shot in-context learning with pseudo-demonstrations. _arXiv preprint arXiv:2212.09865_, 2022.
* Min et al. (2021) Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. _arXiv preprint arXiv:2110.15943_, 2021.
* Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? _arXiv preprint arXiv:2202.12837_, 2022.
* Narang et al. (2020) Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. Wt5?! training text-to-text models to explain their predictions. _arXiv preprint arXiv:2004.14546_, 2020.
* Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_, 2021.
* Perez et al. (2019) Jorge Perez, Javier Marinkovic, and Pablo Barcelo. On the turing completeness of modern neural network architectures. _arXiv preprint arXiv:1901.03429_, 2019.
* Perez et al. (2021) Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. _The Journal of Machine Learning Research_, 22(1):3463-3497, 2021.
* Perez et al. (2021) Luis Perez, Lizi Ottens, and Sudharshan Viswanathan. Automatic code generation using pre-trained language models. _arXiv preprint arXiv:2102.10535_, 2021.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Sahni et al. (2017) Himanshu Sahni, Saurabh Kumar, Farhan Tejani, and Charles Isbell. Learning to compose skills. _arXiv preprint arXiv:1711.11289_, 2017.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Velickovic and Blundell (2021) Petar Velickovic and Charles Blundell. Neural algorithmic reasoning. _Patterns_, 2(7):100273, 2021.
* Oswald et al. (2022) Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. _arXiv preprint arXiv:2212.07677_, 2022.
* Oswald et al. (2023) Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174. PMLR, 2023.
* Oswald et al. (2020)Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi, Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov, Razvan Pascanu, et al. Uncovering mesa-optimization algorithms in transformers. _arXiv preprint arXiv:2309.05858_, 2023.
* Wei et al. (2022a) Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022a.
* Wei et al. (2022b) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022b.
* Wei et al. (2022c) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022c.
* Wei et al. (2023) Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. _arXiv preprint arXiv:2303.03846_, 2023.
* Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.
* Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. _arXiv preprint arXiv:2111.02080_, 2021.
* Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? _arXiv preprint arXiv:2304.02015_, 2023.
* Yun et al. (2019) Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* Zhang et al. (2019) Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer relu networks via gradient descent. In _The 22nd international conference on artificial intelligence and statistics_, pages 1524-1534. PMLR, 2019.
* Zhou et al. (2022) Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. _arXiv preprint arXiv:2211.09066_, 2022.
* Zhou et al. (2023) Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh Susskind, Samy Bengio, and Preetum Nakkiran. What algorithms can transformers learn? a study in length generalization. _arXiv preprint arXiv:2310.16028_, 2023.

## Appendix A Construction

### The Transformer Architecture

For the purpose of this proof, we consider encoder-based transformer architectures and assume that the positional encodings are appended to the input4. We also consider that the heads are added and each one has each own key, query and value weight matrices. Formally, we have

Footnote 4: We note here that in terms of our construction adding the encodings or appending them to the input can be viewed in a similar manner. Specifically, we can consider that the up-projection step projects the input to zero padded vectors, while the encodings are orthogonal to that projection in the sense that the have zero in the non-zero coordinates of the input. In that case adding the positional encodings corresponds to appending them to the input.

\[\textit{attn}(\bm{X}) =\bm{X}+\sum_{h=1}^{H}\bm{W}_{\mathbf{V}}^{h}\bm{X}\textit{softmax }((\bm{W}_{\mathbf{K}}^{h}\bm{X})^{\top}\bm{W}_{\mathbf{Q}}^{h}\bm{X})\] (A.1) \[\texttt{TF}(\bm{X}) =\textit{attn}(\bm{X})+\bm{W}_{2}(\bm{W}_{1}\textit{attn}(\bm{X })+\bm{b}_{1}\bm{1}_{n})_{+}+\bm{b}_{2}\bm{1}_{n}\] (A.2)

where \(\bm{X}\in\mathbb{R}^{d\times n}\), \(H\) is the number of heads used and \(f(x)=(x)_{+}\) is the ReLU activation. We also make use of the temperature \(\lambda\), which is a parameter of the softmax. Specifically, \(\textit{softmax}(\bm{x})=\{\mathrm{e}^{\lambda x_{i}}/\sum_{j}\mathrm{e}^{ \lambda x_{j}}\}_{i}\). Notice that as \(\lambda\rightarrow\infty\) we have \(\textit{softmax}(\bm{x})\rightarrow\max_{i}x_{i}\). We also assume that the inputs are bounded; we denote with \(N_{max}\) the maximum sequence length of the model.

**Assumption 2**: _Each entry is bounded by some constant \(c\), which implies that \(\|\bm{X}\|\leq c^{\prime}\), for some large \(c^{\prime}\) that depends on the maximum sequence length and the width of the transformer._

### Positional Encodings

In the constructions below we use a combination of the binary representation of each position, as well as some additional information (e.g. 0-1 bits) as described in the following sections. The binary representations and in general encodings we construct, require only logarithmic space with respectto the sequence length. Notice that binary representation of the positions satisfy the following two conditions:

1. Let \(\mathbf{r}_{i}\) be the binary representation of the \(i\)-th position, then there exists \(\varepsilon>0\) such that \(\mathbf{r}_{i}^{\top}\mathbf{r}_{i}>\mathbf{r}_{i}^{\top}\mathbf{r}_{j}+\varepsilon\) for all \(j\neq i\).
2. There exists a one layer transformer that can implement the addition of two pointers (see Lemma 2 ).

### Constructing Some Useful "Black-box" Functions

We follow the construction of previous work [Giannou et al., 2023] and use the following individual implementations as they do. We repeat the statements here for convenience. The first lemma is also similar to Akyurek et al. [2022], we however follow a slightly different proof.

**Lemma 1**: _A transformer with one layer, two heads, and embedding dimension of \(\mathcal{O}(\log n+d)\), where \(d\) is the input dimension and \(n\) is the sequence length, can copy any block of the input to any other block of the input with error \(\varepsilon\) arbitrarily small._

**Proof.** Consider an input of the following form

\[\bm{X}=\begin{bmatrix}\bm{x}_{1}&\bm{x}_{2}&\ldots&\bm{x}_{n}\\ \bm{y}_{1}&\bm{y}_{2}&\ldots&\bm{y}_{n}\\ \mathbf{r}_{1}&\mathbf{r}_{2}&\ldots&\mathbf{r}_{n}\\ \mathbf{r}_{k}&\mathbf{r}_{2}&\ldots&\mathbf{r}_{n}\\ \end{bmatrix}\] (A.3)

where \(\bm{x}_{i},\bm{y}_{i}\in\mathbb{R}^{d}\) are column vectors for all \(i=1,\ldots,n\). We present how we can move a block of data from the \((d+1:2d,k)\) position block to the \((1:d,1)\) position block, meaning to move the point \(\bm{y}_{k}\) to the position of the point \(\bm{x}_{1}\). It is straightforward to generalize the proof that follows to move blocks from \((i:i+k,j)\) to \((i^{\prime}:i^{\prime}+k,j^{\prime})\) for arbitrary \(i,i^{\prime},j,j^{\prime},k\).

Let in Eq. A.1\(\bm{W}_{\mathbf{K}}^{1}=[\bm{0}\quad\bm{0}\quad\bm{I}\quad\bm{0}]\) and \(\bm{W}_{\mathbf{Q}}^{1}=[\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{I}]\) and \(\bm{W}_{\mathbf{K}}^{2}=[\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{I}]\) and \(\bm{W}_{\mathbf{Q}}^{2}=[\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{I}]\) then we have

\[(\bm{W}_{\mathbf{K}}^{1}\bm{X})^{\top}(\bm{W}_{\mathbf{Q}}^{1}\bm{X})=\begin{bmatrix} \mathbf{r}_{1}^{\top}\mathbf{r}_{k}&\mathbf{r}_{1}^{\top}\mathbf{r}_{2}&\ldots &\mathbf{r}_{1}^{\top}\mathbf{r}_{k}&\ldots&\mathbf{r}_{1}^{\top}\mathbf{r}_{ n}\\ \mathbf{r}_{2}^{\top}\mathbf{r}_{k}&\mathbf{r}_{2}^{\top}\mathbf{r}_{2}&\ldots &\mathbf{r}_{2}^{\top}\mathbf{r}_{k}&\ldots&\mathbf{r}_{2}^{\top}\mathbf{r}_{n} \\ \vdots&\vdots&\ddots&\vdots&\ddots&\vdots\\ \mathbf{r}_{k}^{\top}\mathbf{r}_{k}&\mathbf{r}_{k}^{\top}\mathbf{r}_{2}&\ldots &\mathbf{r}_{k}^{\top}\mathbf{r}_{k}&\ldots&\mathbf{r}_{k}^{\top}\mathbf{r}_{ n}\\ \vdots&\vdots&\ddots&\vdots&\ddots&\vdots\\ \mathbf{r}_{n}^{\top}\mathbf{r}_{k}&\mathbf{r}_{n}^{\top}\mathbf{r}_{2}&\ldots &\mathbf{r}_{n}^{\top}\mathbf{r}_{k}&\ldots&\mathbf{r}_{n}^{\top}\mathbf{r}_ {n}\\ \end{bmatrix}\] (A.4)

As \(\lambda\to\infty\) and after the application of the softmax operator the above matrix becomes equal to \([\mathrm{e}_{k}\ \ \mathrm{e}_{2}\ \ \ldots\ \ \mathrm{e}_{k}\ \ \ldots\ \mathrm{e}_{n}]+\epsilon\mathbf{M}\), where \(\mathrm{e}_{i}\) is the one-hot vector with \(1\) in the \(k\)-th position, \(\|M\|\leq 1\) and \(\epsilon\) is controllable by the temperature parameter and can be arbitrary small. Let finally \(\bm{W}_{\mathbf{V}}^{1}=\begin{bmatrix}\bm{0}&\bm{I}&\bm{0}&\bm{0}\\ \bm{0}&\bm{0}&\bm{0}&\bm{0}\end{bmatrix}\) we get that

\[\bm{W}_{\mathbf{V}}^{1}\bm{X}\textit{softmax}((\bm{W}_{\mathbf{K}}^{1}\bm{X}) ^{\top}(\bm{W}_{\mathbf{Q}}^{1}\bm{X}))=\begin{bmatrix}\bm{y}_{k}&\bm{x}_{2}& \ldots&\bm{x}_{n}\\ \bm{0}&\bm{0}&\ldots&\bm{0}\end{bmatrix}+\epsilon\mathbf{M}\] (A.5)

By repeating the exact same steps for the second head and letting \(\bm{W}_{\mathbf{V}}^{2}=\begin{bmatrix}-\bm{I}&\bm{0}&\bm{0}&\bm{0}\\ \bm{0}&\bm{0}&\bm{0}&\bm{0}\end{bmatrix}\) and adding back the residual we get the desired result.

A slightly different implementation can be found in Giannou et al. [2023], in which the ReLU layers are also used, together with indicator vectors that define whether a position should be updated or not.

**Lemma 2**: _There exists a 1-hidden layer feedforward, ReLU network, with \(8d\) activations in the hidden layer and \(d\) neurons in the output layer that when given two \(d\)-dimensional binary vectors representing two non-negative integers, can output the binary vector representation of their sum, as long as the sum is less than \(2^{d+1}\)._

**Lemma 3**: _Let \(\mathbf{A}\in\mathbb{R}^{d\times m}\) and \(\mathbf{B}\in\mathbb{R}^{d\times n}\). Then for any \(\epsilon>0\) there exists a transformer-based function block with 2 layers, 1 head and width \(r=O(d)\) that outputs the multiplication \(\mathbf{A}^{\top}\mathbf{B}+\epsilon\mathbf{M}\) for some \(\|\mathbf{M}\|\leq 1\)._

**Remark 2**: _Notice that based on the proof of this lemma, the matrices/scalars/vectors need to be in the same rows, i.e., \(\mathbf{Q}=[\mathbf{A}\quad\mathbf{B}]\). By also appending the appropriate binary encodings we can move the output at any specific place we choose as in Lemma 1. Also, following the proof of the paper the input matrix is_

\[\bm{X}=\begin{bmatrix}\mathbf{Q}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\mathbf{1}\mathbf{1}^{\top}&\mathbf{0}\\ \bm{I}&\mathbf{0}&\mathbf{0}\\ &\mathbf{r}^{(1)}\\ &\mathbf{r}^{(2)}\end{bmatrix}\]

_where \(\mathbf{r}^{(1)},\mathbf{r}^{(2)}\) are chosen as to specify the position of the result._

### Results on Filtering

**Lemma 4**: _Assume that the input to a transformer layer is of the following form_

\[\bm{X}=\begin{bmatrix}\bm{x}_{1}&\bm{x}_{2}&\ldots&\bm{x}_{n-1}&\bm{x}_{n}\\ \mathbf{0}&\mathbf{0}&\ldots&\mathbf{0}&\mathbf{0}\\ b_{1}&b_{2}&\ldots&b_{n-1}&b_{n}\\ b_{1}^{\prime}&b_{2}^{\prime}&\ldots&b_{n-1}^{\prime}&b_{n}\end{bmatrix}\] (A.6)

_where \(b_{i},b_{i}^{\prime}\in\{0,1\}\), with zero indicating that the corresponding point should be ignored. Then there exists a transformer TF consisting only of a ReLU layer that performs this filtering, i.e.,_

\[\texttt{TF}(\bm{X})=\begin{bmatrix}\mathbf{1}\{b_{1}\neq 0\}\bm{x}_{1}& \mathbf{1}\{b_{2}\neq 0\}\bm{x}_{2}&\ldots&\mathbf{1}\{b_{n-1}\neq 0\}\bm{x}_{n-1}& \mathbf{1}\{b_{n}\neq 0\}\bm{x}_{n}\\ \mathbf{1}\{b_{1}^{\prime}\neq 0\}\bm{x}_{1}&\mathbf{1}\{b_{2}^{\prime}\neq 0\}\bm{x}_{2}& \ldots&\mathbf{1}\{b_{n-1}^{\prime}\neq 0\}\bm{x}_{n-1}&\mathbf{1}\{b_{n}^{ \prime}\neq 0\}\bm{x}_{n}\\ b_{1}&b_{2}&\ldots&b_{n-1}&b_{n}\\ b_{1}^{\prime}&b_{2}^{\prime}&\ldots&b_{n-1}^{\prime}&b_{n}\end{bmatrix}\] (A.7)

* The layer is the following: \[\texttt{TF}(\bm{x}_{i})=x_{i}+(-Cb_{i}-\bm{x}_{i})_{+}-(-Cb_{i}+\bm{x}_{i})_{+}\] (A.8) for some large constant \(C\). Notice that if \(b_{i}=1\) the output is just \(\bm{x}_{i}\). But if \(b_{i}=0\) then the output is zero. For the second set instead of using the bits \(b_{i}\), we use the \(b_{i}^{\prime}\).

**Remark 3**: _Notice that if some \(b_{i}\) is instead of \(1\), \(1\pm\varepsilon\), \(\varepsilon<c/C\). Then the output of the above layer would be_

\[\texttt{TF}(\bm{x}_{i}) =\bm{x}_{i}+(-C\pm C\varepsilon-\bm{x}_{i})-(-C\pm C\varepsilon+ \bm{x}_{i})\] (A.9) \[=\bm{x}_{i}\] (A.10) _while if some \(b_{i}=\pm\varepsilon\) instead of zero, and assuming that \(\bm{x}_{i}>c>0\) or \(\bm{x}_{i}<-c<0\) the output would be_

\[\texttt{TF}(\bm{x}_{i}) =\bm{x}_{i}+(-C\varepsilon-\bm{x}_{i})_{+}-(-Cb_{i}+\bm{x}_{i})_{+}\] (A.11) \[=\bm{x}_{i}\pm C\varepsilon-\bm{x}_{i}\] (A.12) \[\leq c\] (A.13)

_If \(|\bm{x}_{i}|\leq c\), again the output would be less than or equal to \(c\), where \(c\) can be arbitrarily small._

Our target is to create these binary tokens, as to perform the filtering. We now describe for clarity how next word prediction is performed. An \(d\times N\)-dimensional input is given to the transformer architecture, which is up-projected using the embedding layer; the positional encodings are also added/appended to the input. At the last layer, the last token predicted is appended to the initial input to the transformer architecture. The only difference of the new input including the positional encodings ( the input for the next iteration ) is the \(n+1\)-th token. This is a property that our construction maintains, _i.e.,_ the positional encodings used are oblivious to the prediction step performed and arealways the same for each individual token. We consider encoder-based architectures in all of our lemmas below.

In the subsequent lemma we construct an automated process that works along those guidelines. To do so we assume that the input to the transformer contains the following information:

1. An enumeration of the tokens from \(1\) to \(N\).
2. The \(\ln\) of the above enumeration.
3. Zeros for the tokens that correspond to the data points, \(1\) for each token that is the \(\bm{x}_{\text{test}}\) or it is a prediction based on it.
4. An enumeration \(1\) to \(L\) for each one of the data points provided. For example, if we are given three sets of data we would have : \(1\ldots L\ 1\ldots L\ 1\ldots L\).
5. Some extra information that is needed to implement a multiplication step as described in Lemma 3 and to move things to the correct place.

The above information can be viewed as part of the encodings that are appended to the input of the transformer. Formally, we have

**Lemma 5**: _Consider that a prompt with \(n\) in-context samples is given and the \(\ell-1\)-th prediction has been made, and the transformer is to predict the \(\ell\)-th one. Assume the input to the transformer is:_

\[\bm{X}=\left[\begin{array}{ccccccccc}\bm{x}_{1}&\ldots&\bm{s}_{1}^{\ell-1}& \bm{s}_{1}^{\ell}&\ldots&\bm{s}_{2}^{\ell-1}&\bm{s}_{2}^{\ell}&\ldots&\bm{s}_ {n}^{L}&\bm{x}_{\text{tr}}&\ldots&\hat{\bm{s}}^{\ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots& \bm{0}\\ \hline 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ \hline 1&\ldots&\ell&\ell+1&\ldots&L+\ell+1&L+\ell+2&\ldots&n(L+1)&n(L+1)+1& \ldots&N\\ \ln(1)&\ldots&\ln(\ell)&\ln(\ell+1)&\ldots&\ln(L+\ell+1)&\ln(L+\ell+2)&\ldots &\ln(n(L+1))&\ln(n(L+1)+1)&\ldots&\ln(N)\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&\ldots&1&1&1&\ldots&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm{m }_{L+\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{N}\\ \end{array}\right]\]

_where \((\hat{\bm{s}}^{i})_{i=1}^{\ell-1}\) denote the first \(\ell-1\) recurrent outputs of TF and for simplicity, let \(\textit{N}:=n(L+1)+\ell\) denote the total number of tokens. Then there exists a transformer TF consisting of 7 layers that has as output_

\[\textit{TF}(\bm{X})=\left[\begin{array}{ccccccccc}\bm{0}&\ldots&\bm{s}_{1}^{ \ell-1}&\bm{0}&\ldots&\bm{s}_{2}^{\ell-1}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots& \hat{\bm{s}}^{\ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{s}_{1}^{\ell}&\ldots&\bm{0}&\bm{s}_{2}^{\ell}&\ldots& \bm{0}&\bm{0}&\ldots&\bm{0}\\ \hline 0&\ldots&1&0&\ldots&1&0&\ldots&0&0&\ldots&1\\ 0&\ldots&0&1&\ldots&0&1&\ldots&0&0&\ldots&0\\ \hline 1&\ldots&\ell&\ell+1&\ldots&L+\ell+1&L+\ell+2&\ldots&n(L+1)&n(L+1)+1& \ldots&N\\ \ln(1)&\ldots&\ln(\ell)&\ln(\ell+1)&\ldots&\ln(L+\ell+1)&\ln(L+\ell+2)&\ldots &\ln(n(L+1))&\ln(n(L+1)+1)&\ldots&\ln(N)\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&\ldots&1&1&1&\ldots&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm{m }_{L+\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{N}\\ \end{array}\right]\]

_with error up to \(\delta\mathbf{M}\), where \(\|\mathbf{M}\|\leq 1\) and \(\delta>0\) is a constant that is controlled and can be arbitrarily small._

**Proof.**

**Step 1: Extract the sequence length (1 layer).** Let

\[\bm{W}_{\text{K}}=[\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{ 0}]\quad\bm{W}_{\text{Q}}=[\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{0}\quad\bm{0} \quad\bm{0}\quad\bm{0}\quad\bm{1}\quad\bm{0}]\] (A.14)and thus

\[(\bm{W_{\mathrm{K}}}\bm{X})^{\top}(\bm{W_{\mathrm{Q}}}\bm{X})=\begin{bmatrix} \ln(1)&\ln(1)&\ldots&\ln(1)\\ \ln(2)&\ln(2)&\ldots&\ln(2)\\ \vdots&\vdots&\ddots&\vdots\\ \ln(\mathsf{N})&\ln(\mathsf{N})&\ldots&\ln(\mathsf{N})\end{bmatrix}.\] (A.15)

So after the softmax is applied we have

\[\text{\it softmax}((\bm{W_{\mathrm{K}}}\bm{X})^{\top}(\bm{W_{\mathrm{Q}}}\bm{X }))=\begin{bmatrix}\frac{1}{\sum_{i=1}^{\mathsf{N}}i}&\frac{1}{\sum_{i=1}^{ \mathsf{N}}i}&\cdots&\frac{1}{\sum_{i=1}^{\mathsf{N}}i}\\ \frac{1}{\sum_{i=1}^{\mathsf{N}}i}&\frac{1}{\sum_{i=1}^{\mathsf{N}}i}&\cdots &\frac{1}{\sum_{i=1}^{\mathsf{N}}i}\\ \vdots&\vdots&\ddots&\vdots\\ \mathsf{N}&\dot{\mathsf{N}}&\cdots&\dot{\mathsf{N}}\\ \hline\sum_{i=1}^{\mathsf{N}}i&\frac{1}{\sum_{i=1}^{\mathsf{N}}i}&\cdots &\frac{1}{\sum_{i=1}^{\mathsf{N}}i}\end{bmatrix}.\] (A.16)

We then set the weight value matrix as to zero-out all lines except for one line as follows

\[\bm{W_{\mathrm{V}}}\bm{X}=\begin{bmatrix}\bm{0}&\bm{0}&\ldots&\bm{0}\\ 1&2&\ldots&\mathsf{N}\\ \bm{0}&\bm{0}&\ldots&\bm{0}\end{bmatrix}.\] (A.17)

After adding the residual and using an extra head where the softmax returns identity matrix and the value weight matrix is minus the identity, we get attention output

\[\begin{bmatrix}\bm{x}_{1}&\ldots&\bm{s}_{1}^{\ell-1}&\bm{s}_{1}^{\ell}&\ldots &\bm{s}_{2}^{\ell-1}&\bm{s}_{2}^{\ell}&\ldots&\bm{s}_{n}^{L}&\bm{x}_{\text{ test}}&\ldots&\hat{\bm{s}}^{\ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\ldots&\bm{0}\\ \hline 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ \hline\frac{\sum_{i=1}^{\mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N}}i}&\frac{ \sum_{i=1}^{\mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N}}i}&\frac{\sum_{i=1}^{ \mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N}}i}&\frac{\sum_{i=1}^{ \mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N}}i}&\frac{\sum_{i=1}^{ \mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N}}i}&\frac{\sum_{i=1}^{\mathsf{N}}i^{2}}{ \sum_{i=1}^{\mathsf{N}}i}&\frac{\sum_{i=1}^{\mathsf{N}}i^{2}}{\sum_{i=1}^{ \mathsf{N}}i}&\cdots&\frac{\sum_{i=1}^{\mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N} }i}\\ \ln(1)&\ldots&\ln(\ell)&\ln(\ell+1)&\ldots&\ln(L+\ell+1)&\ln(L+\ell+2)&\ldots& \ln(n(L+1))&\ln(n(L+1)+1)&\ldots&\ln(\mathsf{N})\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&1&\ldots&1&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm{ m}_{L+\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{ \mathrm{N}}\end{bmatrix}.\]

Notice that \(\frac{\sum_{i=1}^{\mathsf{N}}i^{2}}{\sum_{i=1}^{\mathsf{N}}i}=\frac{2\mathsf{N} (\mathsf{N}+1)(2\mathsf{N}+1)}{6\mathsf{N}(\mathsf{N}+1)}=\frac{2\mathsf{N}+1}{3}\). We then use the ReLU layer as to multiply with \(3/2\) and subtract \(1\) from this column. This results to attention output

\[\begin{bmatrix}\bm{x}_{1}&\ldots&\bm{s}_{1}^{\ell-1}&\bm{s}_{1}^{\ell}&\ldots& \bm{s}_{2}^{\ell-1}&\bm{s}_{2}^{\ell}&\ldots&\bm{s}_{n}^{L}&\bm{x}_{\text{test}}& \ldots&\hat{\bm{s}}^{\ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\ldots&\bm{0}\\ \hline 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ \hline\mathsf{N}&\ldots&\mathsf{N}&\mathsf{N}&\ldots&\mathsf{N}&\ldots&\mathsf{N}& \mathsf{N}&\ldots&\mathsf{N}\\ \ln(1)&\ldots&\ln(\ell)&\ln(\ell+1)&\ldots&\ln(L+\ell+1)&\ln(L+\ell+2)&\ldots& \ln(n(L+1))&\ln(n(L+1)+1)&\ldots&\ln(\mathsf{N})\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&\ldots&1&1&\ldots&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm{ m}_{L+\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{ \mathrm{N}}\end{bmatrix}.\]

Notice that this step does not involve any error.

**Step 2: Extract the identifier of the prediction to be made \(\ell\) (3 layers).** Now, by setting the key and query weight matrices of the attention as to just keep the all ones row, we get a matrix that attendsequally to all tokens. Then the value weight matrix keeps the row that contains \(\ell\) ones and thus we get the number \(\frac{\ell}{\text{N}}\), propagated in all the sequence length. Then the attention output is as follows:

\[\left[\begin{array}{cccccccccccc}\bm{x}_{1}&\ldots&\bm{s}_{1}^{\ell-1}&\bm{s }_{1}^{\ell}&\ldots&\bm{s}_{2}^{\ell-1}&\bm{s}_{2}^{\ell}&\ldots&\bm{s}_{n}^{L} &\bm{x}_{\text{ret}}&\ldots&\hat{\bm{s}}^{\ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots& \bm{0}\\ \hline 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ \hline\text{N}&\ldots&\text{N}&\ldots&\text{N}&\text{N}&\ldots&\text{N}& \text{N}&\ldots&\text{N}\\ \frac{\ell}{\text{N}}&\ldots&\frac{\ell}{\text{N}}&\ldots&\frac{\ell}{\text{ N}}&\ldots&\frac{\ell}{\text{N}}&\ldots&\frac{\ell}{\text{N}}&\ldots&\frac{ \ell}{\text{N}}\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&\ldots&1&1&\ldots&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm {m}_{L+\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{\text{N} }\end{array}\right].\] (A.18)

So far this step does not involve any error. As described in Lemma 3 to implement multiplication of two values up to any error \(\epsilon\), we need 1) have the two numbers to be multiplied next to each other and an extra structure (some constants, which we consider that are encoded in the last rows of the matrix \(\bm{m}_{i}\)) and 2) the corresponding structure presented in Lemma 3. So we need to make the following transformation in the rows containing N and \(\frac{\ell}{\text{N}}\)

\[\left[\begin{matrix}\text{N}&\text{N}&\ldots&\text{N}\\ \frac{\ell}{\text{N}}&\frac{\ell}{\text{N}}&\ldots&\frac{\ell}{\text{N}} \end{matrix}\right] \rightarrow\left[\begin{matrix}\text{N}&\frac{\ell}{\text{N}}& \ldots&\text{N}\\ \frac{\ell}{\text{N}}&\frac{\ell}{\text{N}}&\ldots&\frac{\ell}{\text{N}} \end{matrix}\right]\] (A.19) \[\rightarrow\left[\begin{matrix}\ell&*&\ldots&*\\ *&*&\ldots&*\end{matrix}\right]\] (A.20) \[\rightarrow\left[\begin{matrix}\ell&\ell&\ldots&\ell\\ *&*&\ldots&*\end{matrix}\right]\] (A.21)

where \(*\) denotes inconsequential values. For the first step we assume that we have the necessary binary representations 5 and then we use Lemma 1 which shows how we can perform the operation of copy/paste. This step involves an error and the current output would be \(\tilde{\bm{X}}_{1}=\bm{X}^{*}+\varepsilon_{1}\bm{M}_{1}\), where \(\varepsilon_{1}\) is controlled and we will determine it in the sequence, \(\|\bm{M}_{1}\|\leq 1\) and \(\bm{X}_{1}^{*}\) is the desired output. For the second step now we use Lemma 3 to perform the multiplication, this will affect some of the other values. To analyze the error of this step, we know that given \(\bm{X}_{1}^{*}\) we will get the desired output \(\bm{X}_{2}^{*}+\varepsilon_{2}\bm{M}_{2}\), where \(\|\bm{M}_{2}\|\leq 1\) and \(\varepsilon_{2}\) is to be determined. However, are given \(\tilde{X}_{1}\) in the multiplication procedure6 which results in the following output \(\tilde{\bm{X}}_{2}=(\tilde{\bm{X}}_{1})^{\top}\tilde{\bm{X}}_{1}+\varepsilon_ {2}\bm{M}_{2}=\bm{X}_{1}^{*\top}\bm{X}_{1}^{*}+\varepsilon_{1}\bm{M}_{1}^{ \top}\bm{X}_{1}^{*}+\varepsilon_{1}\bm{M}_{1}^{\top}\bm{M}_{1}+\varepsilon_{2} \bm{M}_{2}\). Thus by choosing \(\varepsilon_{1},\varepsilon_{2}\) to be of order \(\mathcal{O}(\delta)\)we get that \(\tilde{\bm{X}}_{2}=\bm{X}_{2}^{*}+\frac{\delta}{\mathcal{C}}\|M\|\) for some \(C\) chosen to be large enough. Then for the last step consider the follow (sub-)rows of the matrix \(\bm{X}\)

Footnote 5: the size that we need will be \(2\log\text{N}_{\text{max}}+1\), where \(\text{N}_{\text{max}}\) is the maximum sequence length

Footnote 6: Note that the true error is even smaller since the operation performed only affects one row.

\[\begin{bmatrix}\ell&*&*&\ldots&*\\ \bm{r}_{1}&\bm{r}_{1}&\bm{r}_{1}&\ldots&\bm{r}_{1}\\ \bm{r}_{1}&\bm{r}_{2}&\bm{r}_{3}&\ldots&\bm{r}_{\text{N}}\end{bmatrix}\] (A.22)

where \(\bm{r}_{i}\) is the binary representation of position \(i\). By choosing \(\bm{W}_{\text{K}},\bm{W}_{\text{Q}}\) as to

\[\bm{W}_{\text{K}}\bm{X}=\begin{bmatrix}\bm{r}_{1}&\bm{r}_{2}&\ldots&\bm{r}_{ \text{N}}\end{bmatrix},\quad\bm{W}_{\text{Q}}\bm{X}=\begin{bmatrix}\bm{r}_{1}& \bm{r}_{1}&\ldots&\bm{r}_{1}\end{bmatrix}\] (A.23)and consider \(\bm{W_{\mathrm{V}}}\bm{X}=\begin{bmatrix}\ell&*&\ldots&*\\ \bm{0}&\bm{0}&\ldots&\bm{0}\end{bmatrix}\) we have that

\[\begin{split}\mathit{attn}(\bm{X})&=\bm{X}+\bm{W_{\mathrm{V}}} \bm{X}\mathit{softmax}((\bm{W_{\mathrm{K}}}\bm{X})^{\top}\bm{W_{\mathrm{Q}}} \bm{X})\\ &=\bm{X}+\begin{bmatrix}\ell&*&\ldots&*\\ \bm{0}&\bm{0}&\ldots&\bm{0}\end{bmatrix}\mathit{softmax}\left(\begin{bmatrix} \mathbf{r}_{1}^{\top}\mathbf{r}_{1}&\mathbf{r}_{1}^{\top}\mathbf{r}_{1}&\ldots &\mathbf{r}_{1}^{\top}\mathbf{r}_{1}\\ \mathbf{r}_{1}^{\top}\mathbf{r}_{2}&\mathbf{r}_{1}^{\top}\mathbf{r}_{2}& \ldots&\mathbf{r}_{1}^{\top}\mathbf{r}_{2}\\ \vdots&\vdots&\ddots&\vdots\\ \mathbf{r}_{1}^{\top}\mathbf{r}_{\mathrm{N}}&\mathbf{r}_{1}^{\top}\mathbf{r} _{\mathrm{N}}&\ldots&\mathbf{r}_{1}^{\top}\mathbf{r}_{\mathrm{N}}\end{bmatrix} \right)\\ &=\bm{X}+\begin{bmatrix}\ell&*&\ldots&*\\ \bm{0}&\bm{0}&\ldots&\bm{0}\end{bmatrix}\begin{bmatrix}1&1&\ldots&1\\ 0&0&\ldots&0\\ \vdots&\vdots&\ldots&\vdots\\ 0&0&\ldots&0\end{bmatrix}+\varepsilon_{3}\bm{M}_{3}\end{split}\]

By subtracting one identity head for the first that we focus on as described in Lemma 1 we have that \(\mathit{attn}(\bm{X})\) results in the desired matrix. We output this result and overwrite \(\ell/\text{N}\), thus we have

\[\begin{split}\left[\begin{array}{cccccccccccc}\bm{x}_{1}& \ldots&\bm{s}_{1}^{\ell-1}&\bm{s}_{1}^{\ell}&\ldots&\bm{s}_{2}^{\ell-1}&\bm{s }_{2}^{\ell}&\ldots&\bm{s}_{n}^{L}&\bm{x}_{\text{test}}&\ldots&\hat{\bm{s}}^{ \ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}\\ \hline 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ \hline\text{N}&\ldots&\text{N}&\ldots&\text{N}&\text{N}&\ldots&\text{N}&\text{N}& \ldots&\text{N}\\ \ell&\ldots&\ell&\ell&\ldots&\ell&\ell&\ldots&\ell&\ell&\ldots&\ell\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&\ldots&1&1&\ldots&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm{m}_{L +\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{\text{N}} \end{array}\right]\end{split}\] (A.24)

The new error introduced by this operation is again controlled as to be \(\varepsilon_{3}\bm{M}_{3}=\frac{\delta}{C}\), with \(\|\bm{M}_{3}\|\leq 1\).

Step 3: Create \(\ell+1\) (0 layer).We first copy the row with \(\ell\) to the row with \(N\), this can trivially be done with a ReLU layer that outputs zero everywhere else except for the row of \(N\)s that output \((\ell)_{+}-(N)_{+}\) to account also for the residual. We now use one of the bias terms (notice that \(\ell\) is always positive) and set to one in one of the two rows that contain the \(\ell\), again we account for the residual as before; everything else remains unchanged. Thus, we have

\[\begin{split}\left[\begin{array}{cccccccccccc}\bm{x}_{1}& \ldots&\bm{s}_{1}^{\ell-1}&\bm{s}_{1}^{\ell}&\ldots&\bm{s}_{2}^{\ell-1}&\bm{s }_{2}^{\ell}&\ldots&\bm{s}_{n}^{L}&\bm{x}_{\text{test}}&\ldots&\hat{\bm{s}}^{ \ell-1}\\ \bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}&\ldots&\bm{0}&\bm{0}&\ldots&\bm{0}\\ \hline 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ 1&\ldots&\ell&\ell+1&\ldots&\ell&\ell+1&\ldots&L+1&1&\ldots&\ell\\ \hline\ell+1&\ldots&\ell+1&\ell+1&\ldots&\ell+1&\ell+1&\ldots&\ell+1&\ell+1& \ldots&\ell+1\\ \ell&\ldots&\ell&\ell&\ldots&\ell&\ell&\ldots&\ell&\ell&\ldots&\ell\\ 0&\ldots&0&0&\ldots&0&0&\ldots&0&1&\ldots&1\\ 1&\ldots&1&1&\ldots&1&1&\ldots&1&1&\ldots&1\\ \bm{m}_{1}&\ldots&\bm{m}_{\ell}&\bm{m}_{\ell+1}&\ldots&\bm{m}_{L+\ell+1}&\bm{m}_{L +\ell+2}&\ldots&\bm{m}_{n(L+1)}&\bm{m}_{n(L+1)+1}&\ldots&\bm{m}_{\text{N}} \end{array}\right]\end{split}\] (A.25)

This operation can collectively be implemented (add the bias + copy the row) in the ReLU layer of the previous transformer layer that was not used in the previous step.

**Step 4: Create the binary bits (2 layers).** We will now use the information extracted in the previous steps, to create the binary indicators/bit to identify which tokens we want to filter. This can be easily implemented with one layer of transformer and especially the ReLU part of it. Notice that if we subtract the row that contains the tokens that have already been predicted, _i.e.,_\([\ell\;\ell\;\ldots\;\ell]\) from the row that contains \([1\;2\ldots L\;1\;2\ldots L\ldots L]\) we will get zero only in the positions that we want to filter out and some non-zero quantity in the rest. This is trivially implemented with one ReLU layer. So, we need to implement an if...then type of operation. Basically, if the quantity at hand is zero we want to set the bit to one, while if it non-zero to set it to be zero. This can be implemented with the following ReLU part of a transformer layer

\[\mathtt{TF}(x_{i})=1-(x_{i})_{+}-(-x_{i})_{+}+(x_{i}-1)_{+}+(-x_{i}-1)_{+}-((x_ {i})_{+}-(-x_{i})_{+})\] (A.26)

the last two terms are to account for the residual. Again the rest of the rows do not change and are zeroed-out.

**Step 5: Implement the filtering (1 layer).** We now apply Lemma 4 and our proof is completed. In this step as in Lemma 4, the error remains of the order of the error of the previous step. Thus, by fine-tuning the constants appropriately, depending on 1) the bound on the input as of Assumption 2, the targeted error \(\delta\) and the constant used in Lemma 4 we achieve that the error has the target typed bound. \(\blacksquare\)

**Theorem 2** (Theorem 1 restated): _Consider a prompt \(\bm{p}_{n}(f)\) generated from an \(L\)-layer MLP \(f(\cdot)\) as described in Definition 1, and assume given test example \((\bm{x}_{\text{test}},\bm{s}_{\text{test}}^{1},\ldots\bm{s}_{\text{test}}^{L})\). For any resolution \(\epsilon>0\), there exists \(\delta=\delta(\epsilon)\), iteration choice \(T=\mathcal{O}(\kappa_{\max}^{2}\log(1/\epsilon))\), and a backend transformer \(\mathtt{TF}_{\mathtt{BE}}\) such that the concatenated transformer \(\mathtt{TF}=\mathtt{TF}_{\mathtt{LR}}\circ\mathtt{TF}_{\mathtt{BE}}\) implements the following: Let \((\hat{\bm{s}}^{i})_{i=1}^{\ell-1}\) denote the first \(\ell-1\) CoT-I/O outputs of \(\mathtt{TF}\) and set \(\bm{p}[\ell]=(\bm{p}_{n}(f),\bm{x}_{\text{test}},\hat{\bm{s}}^{1}\ldots\hat{ \bm{s}}^{\ell-1})\). At step \(\ell\), \(\mathtt{TF}\) implements_

1. **Filtering.** _Define the filtered prompt with input/output features of layer_ \(\ell\)_,_ \[\bm{p}_{n}^{\text{filter}}=\begin{pmatrix}\ldots\bm{0},\bm{s}_{1}^{\ell-1},\; \bm{0}\;\ldots\bm{0},\,\bm{s}_{n}^{\ell-1},\;\bm{0}\;\ldots\bm{0},\hat{\bm{s}} ^{\ell-1}\\ \ldots\bm{0},\;\bm{0},\;\bm{s}_{1}^{\ell}\ldots\bm{0},\;\bm{0},\;\;\bm{s}_{n}^ {\ell}\ldots\bm{0},\;\;\bm{0}\end{pmatrix}.\] _There exists a fixed projection matrix_ \(\bm{\Pi}\) _that applies individually on tokens such that the backend output obeys_ \(\|\bm{\Pi}(\mathtt{TF}_{\mathtt{BE}}(\bm{p}[\ell]))-\bm{p}_{n}^{\text{filter}}\| \leq\delta\)_._
2. **Gradient descent.** _The combined model obeys_ \(\|\mathtt{TF}(\bm{p}[\ell])-\bm{s}_{\text{test}}^{\ell}\|\leq\ell\cdot\epsilon/L\)_._

\(\mathtt{TF}_{\mathtt{BE}}\) _has constant number of layers independent of \(T\) and \(n\). Consequently, after \(L\) rounds of CoT-I/O, TF outputs \(f(\bm{x}_{\text{test}})\) up to \(\epsilon\) accuracy._

**Proof.** We apply Lemma 5 from which it is clear that there exists a projection such that the result stated in (1. Filtering) holds, and it is independent to \(T\), \(n\) and \(\ell\). Next we turn to prove (2. Gradient descent). In Definition 1, we assume that the network's activation function is leaky-ReLU, _i.e.,_

\[\phi(x)=\begin{cases}x,&\text{if }x\geq 0\\ \alpha x,&\text{otherwise.}\end{cases}\] (A.27)

Thus, as a first step we construct the inverse of leaky-ReLU and apply it in the second row of \(\bm{p}_{n}^{\text{filter}}\) where the inverse of leaky-ReLU is

\[\phi^{-1}(y)=\begin{cases}y,&\text{if }y\geq 0\\ y/\alpha,&\text{otherwise.}\end{cases}\] (A.28)

This can be implemented with the following activation function (denoted by \(\sigma(\cdot)\)) using ReLUs:

\[\sigma(x)=(x)_{+}-1/\alpha(-x)_{+}.\] (A.29)

After it, it remains \(\mathtt{TF}_{\mathtt{LR}}\) to solve linear regression problems. Taking \(\ell\)th layer, first neuron prediction as an example, and letting \(\bm{x}_{i}^{\prime}:=\bm{s}_{i}^{\ell-1}\), \(y_{i}^{\prime}:=\phi^{-1}(\bm{s}_{i}^{\ell}[0])\) and \(\bm{w}=\bm{W}_{\ell}[0]\), linear regression has form of \(y_{i}^{\prime}=\bm{w}^{\top}\bm{x}_{i}^{\prime}\) for \(i\in[n]\). Notice that the extra zeros do not contribute in the update performed by gradient descent, and after gradient descent has been performed, we apply back the leaky ReLU. Then following Assumption 1, since we assume \(\mathtt{TF}_{\mathtt{LR}}\) performs the same as gradient descent optimizer,given matrix condition as described in Definition 1, after running \(T\) iterations of gradient descend on the linear regression problem and considering each layer prediction with resolution \(\epsilon/L\), we can get that \(\|\mathtt{TF}_{\mathtt{LR}}(\bm{p}[\ell])-\bar{\bm{s}}^{\ell}\|\leq\epsilon/L\), where \(\bar{\bm{s}}^{\ell}=\phi(\bm{W}_{\ell}\hat{\bm{s}}^{\ell-1})\) is the correct prediction if taking \(\hat{\bm{s}}^{\ell-1}\) as input. Then we have

\[\|\mathtt{TF}_{\mathtt{LR}}(\bm{p}[\ell]-\bm{s}^{\ell}_{\text{test}})\|\leq\| \mathtt{TF}_{\mathtt{LR}}(\bm{p}[\ell])-\bar{\bm{s}}^{\ell}\|+\|\bm{W}_{\ell}( \hat{s}^{\ell-1}-\bm{s}^{\ell-1}_{\text{test}})\|\lesssim\epsilon/L+\|\mathtt{ TF}_{\mathtt{LR}}(\bm{p}[\ell-1]-\bm{s}^{\ell-1}_{\text{test}})\|.\]

Let \(\mathtt{TF}_{\mathtt{LR}}(\bm{p}[0])\) returns \(\bm{x}_{\text{test}}\) and therefore \(\|\mathtt{TF}_{\mathtt{LR}}(\bm{p}[0])-\bm{x}_{\text{test}}\|=0\). Combing results in that \(\|\mathtt{TF}_{\mathtt{LR}}(\bm{p}[\ell]-\bm{s}^{\ell}_{\text{test}})\|\lesssim \ell\cdot\epsilon/L\). Since from Lemma 5 we have that we can choose \(\delta\) to be arbitrary. Let \(\delta=\epsilon/L\), where \(L\) is the total predictions we will make. Then it will result in \(\|\mathtt{TF}(\bm{p}[\ell]-\bm{s}^{\ell}_{\text{test}})\|\lesssim\ell\cdot \epsilon/L\), which completes the proof.

## Appendix B Experimental Details

In this section, we provide the implementation details of our experiments.

### Model Evaluation

Recap the same setting as in Section 2.3 and assume we have pretrained models with parameters \(\hat{\bm{\theta}}^{\text{CoT-I}}\) and \(\hat{\bm{\theta}}^{\text{CoT-I/O}}\). Next we make predictions following Section 2.2. Letting \(\ell(\cdot,\cdot):\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}\) be loss function, we can define test risks as follows.

\[\mathcal{L}^{\text{CoT-I}}(n)=\mathbb{E}_{(\bm{x}_{i})_{i=1}^{n},(f_{\ell})_{ \ell=1}^{L}}\left[\ell(\hat{\bm{y}}_{n},f(\bm{x}_{n}))\right]\ \ \text{where}\ \ \hat{\bm{y}}_{n}=\mathtt{TF}(\bm{p}_{n}(f),\bm{x}_{n};\hat{\bm{\theta}}^{ \text{CoT-I}})\]

and

\[\mathcal{L}^{\text{CoT-I/O}}(n)=\mathbb{E}_{(\bm{x}_{i})_{i=1}^{n},(f_{\ell} )_{\ell=1}^{L}}\left[\ell(\hat{\bm{y}}_{n},f(\bm{x}_{n}))\right]\ \ \text{where}\ \ \hat{\bm{y}}_{n}=\mathtt{TF}(\bm{p}_{n}(f),\bm{x}_{n},\hat{\bm{s}}^{1} \cdots,\hat{\bm{s}}^{L-1};\hat{\bm{\theta}}^{\text{CoT-I/O}}).\]

Here, we use \(\mathcal{L}(n)\) to define the test risk when given prompt with \(n\) in-context samples. Then, results shown in Figures 3&48&6&7(a) are test risks \(\mathcal{L}(n)\) given \(n\in[N]\). Following model training and evaluation, we can see that once loss function is the same for both training and predicting, CoT-I (as well as ICL) accepts training risk \(\mathcal{L}^{\text{CoT-I}}_{\text{train}}=\frac{1}{N}\sum_{n=1}^{N}\mathcal{L}^ {\text{CoT-I}}(n)\).

### Implementation

All the transformer experiments use the GPT-2 model (Radford et al., 2019) and our codebase is based on prior works (Garg et al., 2022; Wolf et al., 2019). Specifically, the model is trained using the Adam optimizer with learning rate \(0.0001\) and batch size \(64\), and we train \(500\)k iterations in total for all ICL, CoT-I and CoT-I/O methods. Each iteration randomly samples inputs \(\bm{x}\)s and functions \(\bm{f}\)s. We also apply curriculum learning over the prompt \(n\) as Garg et al. (2022) did for 2-layer random MLPs (Sec. 4.2). For both training and testing, we use the squared error as the loss function, _i.e._, \(\ell(\hat{\bm{y}},\bm{y})=\|\hat{\bm{y}}-\bm{y}\|^{2}\) (or \(\ell(\hat{y},y)=(\hat{y}-y)^{2}\) for scalar).

## Appendix C Additional Experimental Results

### Out-of-distribution Experiments

While our primary focus lies in in-distribution scenarios, wherein the test examples are presumed to follow the same distribution as the training data, this subsection extends our analysis to out-of-distribution (OOD) settings.

To provide a clearer understanding of our model's behavior under distribution shifts and to quantitatively assess the impact on test risk, we conduct experiments with varying levels of distribution shift. Specifically, we evaluate the test risk when the prompt is supplemented with \(100\) in-context examples. The results of these experiments are depicted in Figure 9, which serves to illustrate how our model's performance is affected under these OOD conditions. In Fig. 9(a), we analyze noisy in-context samples during testing. The solid and dashed curves represent the test risks, corresponding to the noisy in-context samples whose (input, output) takes the form of either \((\bm{x},y+\text{noise})\) or \((\bm{x}+\text{noise},y)\), respectively. The results indicate that CoT exhibits greater robustness compared to ICL, and the test risks increase linearly with the noise level, with attributed to the randomized MLPs setting. Additionally, in Figs. 9(b)&9(c), we instead explore out-of-distribution test tasks where test MLPs differ in \((d,k)\) from the training phase. For both subfigures, we firstly train small GPT-2 using 2-layer MLPs with \(d=10,k=8\). In Fig. 9(b), we fix \(d=10\) and vary \(k\) from \(1\) to \(8\), whereas in Fig. 9(c), we fix \(k=8\) and vary \(d\) from \(1\) to \(10\). In both instances, the findings reveal that CoT's performance remains almost consistent when \(k\geq 4\) or \(d\geq 6\), and ICL is unable to surpass it. The improved performance of ICL with smaller values of \(d\) or \(k\) again reinforces our central assertion that ICL requires \(O(dk)\) samples for in-context learning of the 2-layer random MLP, and reducing either \(d\) or \(k\) helps in improving the performance. Given that we employ the ReLU activation function, smaller values of \(d\) or \(k\) can lead to significant bias in the intermediate feature. Consequently, CoT cannot derive substantial benefits from this scenario, resulting in a decline in performance.

### Further Evidence of Model Expressivity

There are two determinants of model learnability in in-context tasks: sample complexity and model expressivity. Sample complexity pertains to the number of samples needed to precisely solve a problem. However, when the transformer model is small, even with a sufficiently large number of samples, due to its lack of expressivity, ICL cannot achieve zero test risk. This contrasts with CoT, which decomposes complex tasks into simpler sub-tasks, thereby requiring smaller models for expression. Figure 5 has illustrated the expressivity of different GPT-2 architectures, showing that the tiny GPT-2 model is too small to express even a single layer of 2-layer MLPs. Additionally, we have run more experiments, and the results are shown in Figure 10. Both Figures 10(a) and 10(b)

Figure 10: We train ICL with more in-context examples. The main conclusion is that: For small GPT, ICL can indeed not approximate the neural net even with many examples (unlike CoT) whereas, for large GPT, ICL can do so (although much less efficient). This is in line with our theoretical intuitions on the expressivity benefits of CoT.

Figure 9: We implement robustness experiments under different distribution shift levels. In Fig. 9(a) we add noise to the label \(y\) (solid) or input features \(\bm{x}\) (dashed). In Fig. 9(b), we in-context learn an MLP with \(k\in[8]\) hidden nodes whereas transformer is trained for MLPs with \(8\) hidden nodes. In Fig. 9(b), we consider misspecification of input dimension: TF is trained with \(d=10\) but we feed a neural net with \(d\leq 10\).

detail training models with MLP tasks of dimensions \(d=10\) and \(k=4\). In Fig. 10(a), we use a small GPT-2 model, and the results show that the test risk stops decreasing even with more in-context examples. In Fig. 10(b), we train a larger model, and the results demonstrate that the standard GPT-2 is sufficient to express a 2-layer MLP with \(d=10\) and \(k=4\).

### Filtering Evidence in 2-layer MLPs

Section 5 has demonstrated the occurrence of filtering in the linear deep MLPs setting (black dotted curves in Fig. 8(b)). In this section, we present further empirical evidence based on the 2-layer MLPs setting discussed in Section 4.2.

Follow the same setting as Figure 3(a) and choose \(d=10\) and \(k=8\). Assume we have a model pretrained using CoT-I/O method. As described in Section 4.2, during training, the prompt consists of in-context samples in the form of \((\bm{x},\bm{s},y)\) where \(\bm{s}=(\bm{W}\bm{x})_{+}\) and \(y=\bm{v}^{\top}\bm{s}\). To investigate filtering,

Figure 11: Fig. 11(a) presents a filtering evidence of 2-layer MLPs. Given a 2-layer MLP in-context example \((\bm{x},\bm{s},y)\), CoT admits \((\bm{x},\bm{s},y)\) as test sample; while test samples of CoT-zero and CoT-random are formed by \((\bm{x},\bm{s},0)\) and \((\bm{x},\bm{s},z)\) where \(z\sim\mathcal{N}(0,d)\). Fig. 11(b) is directly cloned form Fig. 3(a) with error bar for better comparison with results in Fig. 12.

Figure 12: We compare the performance of filtered CoT and ICL. In Fig. 12(a)&12(c), we decouple the composed risk of predicting 2-layer MLPs into risks of individual layers (following Section 4.1), which shows the filtered CoT results. In Fig. 12(b)&12(d), we train two additional models using ICL method taking \((\bm{x},\bm{s})\) and \((\bm{s},y)\) as inputs.

we make three different predictions to evaluate the intermediate output, whose test prompts have in-context examples with the following forms:

\[\text{CoT:}\;(\bm{x},\bm{s},y),\quad\text{CoT-zero:}\;(\bm{x},\bm{s},0),\quad \text{CoT-random:}\;(\bm{x},\bm{s},z),\]

where \(z\sim\mathcal{N}(0,d)\). The results are displayed in Figure 11(a) where blue, orange and green curves represent first layer prediction results using CoT, CoT-zero and CoT-random prompts, respectively. From this figure, we observe that the three curves are well aligned, indicating that when making a prediction for input \(\bm{x}\), \(\mathtt{TF}\) will attend only to \((\bm{x},\bm{s})\) and ignore \(y\). Therefore filling the positions of \(y\) with any random values (or zero) will not change the performance of first layer prediction.

### Comparison of Filtered CoT with ICL

Until now, many experimental results have shown that CoT-I/O provides benefits in terms of sample complexity and model expressivity compared to ICL. As an interpretation, we state that CoT can be decoupled into two phases: _Filtering_ and _ICL_, and theoretical results have been provided to prove this statement. As for the empirical evidence, Sections 5 and C.3 precisely show that filtering does occur in practice. In this section, we provide additional experiments to demonstrate that, after filtering, CoT performs similarly to ICL.

For convenience and easier comparison, we repeat the same results as Fig. 3(a) in Fig. 11(b), where \(d\in\{10,20\}\), \(k\in\{4,8,16\}\), and train with a small GPT-2. We again recap the data setting for the 2-layer MLP, where the in-context examples of CoT prompt are in the form of \((\bm{x},\bm{s},y)\). Given that filtering happens, we make first and second layer predictions following Section 4.1 and results are presented in Fig. 12(a) and Fig. 12(c), respectively. These results show the performances of the filtered CoT prompts. Next, we need to compare the performance with separate ICL training. To achieve this goal, we train a small GPT-2 model using ICL method with prompt containing \((\bm{x},\bm{s})\) pairs (first layer). The test results are shown in Fig. 12(b). Additionally, in Fig. 12(d), we train another small GPT-2 model using ICL but with prompts containing \((\bm{s},y)\) pairs (second layer). By comparing Fig. 12(a) and 12(b), as well as Fig. 12(c) and 12(d), we observe that after filtering, CoT-I/O achieves similar performance as individually training a single-step problem through the ICL phase.

### CoT across Different Sizes of GPT-2

In Figure 5, we have demonstrated that larger models help in improving performance due to their ability of solving more complex function sets. However, since tiny, small and standard GPT-2 models scale the layer number, head number and embedding dimension simultaneously, it is difficult to determine which component has the greatest impact on performance. Therefore in this section, we investigate how different components of transformer model affect the resulting performance by run CoT-I/O on various GPT-2 architectures.

We maintain the same setting as in Section 4.2, fix \(d=10\) and \(k=8\), and consider a base GPT-2 model (small GPT-2) with \(6\) attention layers, \(4\) heads in each layer and \(128\)-dimensional embeddings.

Figure 13: To further investigate how model architectures impact the prediction performance, we fix the number of heads and embedding dimension in Fig. 13(a) and change the layer number in \(\{3,6,12\}\). Similarly for Fig. 13(b)&13(c) but instead, change number of heads (in \(\{2,4,8\}\)) and embedding dimensions (in \(\{64,128,256\}\)).

In Fig. 13(a), we fix the number of heads at \(4\) and the embedding dimension at \(128\), while varying the number of layers in \(\{3,6,12\}\). Similarly, we explore different models with different numbers of heads and embedding dimensions, and the results are respectively presented in Fig. 13(b) and 13(c). Comparing them, we can observe the following: 1) once the problem is sufficiently solved, increasing the model size does not significantly improve the prediction performance (see Fig. 13(b)&13(c)); 2) the number of layers influences model expressivity, particularly for small GPT-2 architecture (see Fig. 13(a)).

### Comparison of Transformer Prediction and Linear Regression

We also provide experimental findings to verify Assumption 1 in this section. Previous work (Giannou et al., 2023; Akyurek et al., 2022) has theoretically proven that TF can perform similar to gradient descent, and empirical evidence from (Dai et al., 2022; Garg et al., 2022; Li et al., 2023c) suggests that TF can even be competitive with Bayes optimizer in certain scenario. To this end, we first repeat the same first/layer predictions from Figure 4 in Figure 14, where \(d=10\) and blue, orange and green solid curves represent the performances of \(k=4,16,64\) using pretrained small GPT-2 models. We also display the evaluations of gradient descent/least square solutions in dashed curves. Specifically, in Fig. 14(a), we solve problem

\[\hat{\bm{w}}_{n}=\arg\min_{\bm{w}}\frac{1}{n}\sum_{i=1}^{n}\|(\bm{w}^{\top}\bm {x}_{i})_{+}-y_{i}\|^{2}\;\;\text{where}\;\;\bm{x}_{i}\sim\mathcal{N}(0,\bm{I }_{d}),\;y_{i}=(\bm{w^{*}}^{\top}\bm{x}_{i})_{+}\]

for some \(\bm{w}^{\star}\sim\mathcal{N}(0,2\bm{I}_{d})\) and \(n\) is the training sample size. Then the normalized test risks are computed by \(\mathcal{L}(n)=\mathbb{E}_{\bm{w}^{\star},\bm{x}}[\|(\hat{\bm{w}}_{n}^{\top} \bm{x})_{+}-y\|^{2}]/d\), and we show point-to-point results for \(n\in[N]\) in black dashed curve in Fig. 14(a)7. As for the second layer, we solve least squares problems as follows

Footnote 7: To mitigate the bias introduced by ReLU activation, we subtract the mean value during prediction, _i.e._, \(\mathcal{L}(n)=\mathbb{E}_{\bm{w}^{\star},\bm{x}}[\|(\hat{\bm{w}}_{n}^{\top} \bm{x})_{+}-y-(\mathbb{E}_{\bm{x}}[(\hat{\bm{w}}_{n}^{\top}\bm{x})_{+}-y])\|^ {2}]/d\).

\[\hat{\bm{v}}_{n}=\bm{S}^{\dagger}\bm{y}\;\;\text{where}\;\;\bm{S}\in\mathbb{R} ^{n\times k},\;\bm{S}[i]=(\bm{W}^{\star}\bm{x}_{i})_{+},\;\bm{y}[i]=\bm{v^{*}} ^{\top}\bm{S}[i],\;\bm{x}_{i}\in\mathcal{N}(0,\bm{I}_{d})\]

for some \(\bm{W}^{\star}\in\mathbb{R}^{k\times d}\sim\mathcal{N}(0,2/k)\) and \(\bm{v}^{\star}\sim\mathcal{N}(0,\bm{I}_{k})\). Here, \({}^{\dagger}\) represents the pseudo-inverse operator. Then we calculate the normalized test risk of least square solution (given \(n\) training samples) as \(\mathcal{L}(n)=\mathbb{E}_{\bm{W}^{\star},\bm{v}^{\star},\bm{x}}[\|\hat{\bm{ v}}_{n}^{\top}\bm{s}-y\|^{2}]/d\) where \(\bm{s}=(\bm{W}^{\star}\bm{x})_{+}\) and \(y=\bm{v^{*}}^{\top}\bm{s}\). The results are presented in Fig. 14(b) where blue, orange and green dashed curves correspond to solving the problem using different values of \(k\in\{4,16,64\}\). In this figure, the curves for \(k=4,16\) are aligned with GPT-2 risk curves, which indicates that TF can efficiently solve linear regression as a least squares optimizer. However, the curve for \(k=64\) does not align, which can be attributed to the increased complexity of the function set with higher dimensionality (\(k=64\)). Learning such complex functions requires a larger TF model.

Figure 14: Fig. 14(a): compare transformer results (solid) with gradient descent optimizer (dashed) when solving first layer of 2-layer MLPs; Fig. 14(b): compare transformer result (solid) with least squares optimizer (dashed) when solving the second layer of 2-layer MLPs.