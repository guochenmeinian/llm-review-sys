# On the Adversarial Robustness of Out-of-distribution Generalization Models

Xin Zou

Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

Corresponding author: Weiwei Liu (liuweiwei863@gmail.com).

###### Abstract

Out-of-distribution (OOD) generalization has attracted increasing research attention in recent years, due to its promising experimental results in real-world applications. Interestingly, we find that existing OOD generalization methods are vulnerable to adversarial attacks. This motivates us to study OOD adversarial robustness. We first present theoretical analyses of OOD adversarial robustness in two different complementary settings. Motivated by the theoretical results, we design two algorithms to improve the OOD adversarial robustness. Finally, we conduct experiments to validate the effectiveness of our proposed algorithms. Our code is available at [https://github.com/ZouXinn/OOD-Adv](https://github.com/ZouXinn/OOD-Adv).

## 1 Introduction

Recent years have witnessed the remarkable success of modern machine learning techniques in many applications. A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is always violated in many practical applications. The test environment is influenced by a range of factors, such as the distributional shifts across the photos caused by different cameras in image classification tasks, the voices of different persons in voice recognition tasks, and the variations between scenes in self-driving tasks [48]. Therefore, there is now a rapidly growing body of research with a focus on generalizing to unseen distributions, namely **out-of-distribution (OOD)** generalization [56].

Deep neural networks (DNNs) have achieved state-of-the-art performance in many fields. However, several prior works [59, 25] have demonstrated that DNNs may be vulnerable to imperceptibly changed adversarial examples, which has increased focus on the adversarial robustness of the models. **Adversarial robustness** refers to the invariance of a model to small perturbations of its input [54], while **adversarial accuracy** refers to a model's prediction performance on adversarial examples generated by an attacker. However, the adversarial robustness of OOD generalization models (OOD adversarial robustness) is less explored, despite its importance in many systems requiring high security such as self-driving cars. We evaluate the adversarial robustness of the models trained with the current OOD generalization algorithms (the detailed experimental settings can be found in Appendix C.3), and present the results in Table 1. Surprisingly, under the PGD-20 [44] attack, the algorithms achieve **nearly \(0\%\)** adversarial accuracy on RotatedMNIST [21], VLCS [15], PACS [38], and OfficeHome [62], and **no more than \(10\%\)** adversarial accuracy on ColoredMNIST [4]. These results show that even if the OOD generalization algorithms generalize well in different scenes, they remain highly vulnerable to adversarial attacks.

Motivated by these limitations of existing algorithms, we provide theoretical analyses for OOD adversarial robustness in two different but complementary OOD settings. We then design twobaseline algorithms to improve the OOD adversarial robustness, based on the implications of our theory, and validate the effectiveness of our proposed algorithms through experiments.

Our **contributions** can be summarized as follows:

1. We evaluate the adversarial robustness of current OOD generalization algorithms and experimentally verify that the current OOD generalization algorithms are vulnerable to adversarial attacks.
2. We present theoretical analyses for the adversarial OOD generalization error bounds in the average case and the limited training environments case. Specifically, our bounds in limited training environments involve a "distance" term between the training and the test environments. We further use a toy example to illustrate how the "distance" term affects the OOD adversarial robustness, which is verified by the experimental results in Section 5.
3. Inspired by our theory, we propose two algorithms to improve OOD adversarial robustness. Extensive experiments show that our algorithms are able to achieve **more than \(53\%\)** average adversarial accuracy over the datasets.

The remainder of this article is structured as follows: SS2 introduces related works. SS3 presents our main theory. SS4 shows our two theory-driven algorithms. SS5 provides our experimental results. Finally, the conclusions are presented in the last section.

## 2 Related Work

### Adversarial Robustness

[59] show that DNNs are fragile to imperceptible distortions in the input space. One of the most popular methods used to improve adversarial robustness is **adversarial training (AT)**. The seminal AT work, the fast gradient sign method [25, FGSM], perturbs a sample towards its gradient direction to increase the loss, then uses the generated sample to train the model. Following this line of research, [44, 47, 34, 11] propose iterative variants of the gradient attack with improved AT frameworks. [70, 30, 41] investigates the adversarial robustness from the perspective of ordinary differential equations. Recently, [50, 66] utilize the data from generative models as data augmentation to improve adversarial robustness. Besides, [43] analyze the trade-off between robustness and fairness, [37] study the worst-class adversarial robustness in adversarial training.

For the theoretical perspective, [46] study the PAC learnability of adversarial robust learning, [69] extend the work of [46] to multiclass case, [72, 31, 5, 18, 67] give theoretical analyses to adversarial training by standard uniform convergence argumentation and giving a bound of the Rademacher complexity, and [65, 78] study adversarial robustness under self-supervised learning.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Algorithm** & **RM** & **CM** & **VLCS** & **PACS** & **OH** & **Avg** \\ \hline ERM [61] & 0.6/0.0 & 5.8/X & 0.0/0.0 & 0.3/0.6 & 0.4/0.0 & 1.4/X \\ MLDG [39] & 0.2/0.0 & 4.8/X & 0.0/0.0 & 0.1/0.3 & 0.6/0.1 & 1.2/X \\ CDANN [42] & 0.9/0.0 & 8.2/X & 3.0/0.0 & 1.5/0.3 & 0.1/0.0 & 2.7/X \\ VREx [33] & 0.2/0.0 & 6.4/X & 0.0/0.0 & 0.0/0.3 & 0.4/0.1 & 1.4/X \\ RSC [28] & 1.0/0.0 & 3.9/X & 0.0/0.0 & 0.1/0.4 & 0.7/0.0 & 1.1/X \\ MAT [64] & X/X & 10.7/X & 0.0/0.0 & 0.7/1.4 & 0.8/0.1 & X/X \\ LDAT [64] & X/X & 7.9/X & 0.0/0.0 & 0.1/0.3 & 0.4/0.1 & X/X \\ \hline \hline \end{tabular}
\end{table}
Table 1: The results (\(\%\)) for some of the current OOD generalization algorithms. We present the results in the form of **a/b**: here, **a** is the OOD adversarial accuracy under PGD-20 attack [44]; and **b** is the OOD adversarial accuracy under AutoAttack [12]. We conduct the \(\ell_{\infty}\)-norm attack. We use the perturbation radius \(\epsilon=0.1\) for RotatedMNIST and ColoredMNIST, and \(\epsilon=\frac{4}{255}\) for VLCS. For the architecture, following [26], we use a small CNN-architecture for RotatedMNIST and ColoredMNIST, and ResNet-50 [27] for VLCS, PACS and OfficeHome. Since [64] do not realize MAT and LDAT for RotatedMNIST, we use X to denote the unrealized results. For more details about the algorithms, please refer to Appendix C.1. We use RM, CM, and OH as the abbreviation of RotatedMNIST, ColoredMNIST, and OfficeHome, respectively.

### Out-of-distribution generalization

OOD generalization aims to train a model with data from the training environments so that it is capable of generalizing to an unseen environment. A large number of algorithms have been developed that aim to improve OOD generalization. One series of works focuses on minimizing the discrepancies between the training environments [40; 17; 42; 58; 1]. The most related work among them is [1], which measures the discrepancy between the domains by \(d_{\mathcal{H}}(S,T)\), while we focus on adversarial robustness and use \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(S,T)\). Meta-learning domain generalization [39; MLDG] leverages the meta-learning approach and simulates train/test distributional shift during training by synthesizing virtual testing domains within each mini-batch. [53; GroupDRO] studies applying distributionally robust optimization (DRO) [19; 36; 77; 13] to learn models that instead minimize the worst-case training loss over a set of pre-defined groups. Another line of works [68; 64] conducts adversarial training to improve the OOD generalization performance. In this work, we focus on improving the OOD adversarial robustness.

From the theoretical perspective, [10] introduce a formal framework and argue that OOD generalization can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. In [51], OOD generalization is cast into an online game where a player (model) minimizes the risk for a "new" distribution presented by an adversary at each time-step. [14] propose a probabilistic framework for domain generalization called Probable Domain Generalization, wherein the key idea is that distribution shifts seen during training should inform us of probable shifts at test time. Notably, all these works focus on OOD generalization performance, while we present theoretical results for OOD adversarial robustness.

### Works relating domain shifts and adversarial robustness

[8] focuses on improving the adversarial robustness of the models by regarding the adversarial distribution as the target domain and then applying the domain adaptation methods, while we focus on improving the model's adversarial robustness on the OOD distribution. [71] studies the relationship between adversarial robustness and OOD generalization, it shows that good adversarial robustness implies good OOD performance when the target domain lies in a Wasserstein ball. While we study the OOD adversarial robustness and propose algorithms to improve OOD adversarial robustness. [2] empirically analyzes the transferability of models' adversarial/certified robustness under distributional shifts. It shows that adversarially trained models do not generalize better without fine-tuning and that the accuracy-robustness trade-off generalizes to the unseen domain. Its results for adversarial robustness can also be found in our experimental results. [29] investigates how to improve the adversarial robustness of a model against ensemble attacks or unseen attacks. [29] regards the adversarial examples for each type of attack (such as FGSM, PGD, CW, and so on) as a domain, and utilizes the OOD generalization methods to improve the models generalization performance under different (maybe unseen) attacks.

[45; 22] study the relationship between the dependence on spurious correlations and the adversarial robustness of the models and show that adversarial training increases the model's reliance on spurious features. [60] studies the relationship between fairness and adversarial robustness and shows that models that are fairer will be less adversarially robust. However, they do not consider the adversarial robustness of the model on the unseen target domain, which is the topic of this paper.

## 3 Theoretical Analysis for OOD Adversarial Robustness

In this section, we present two theorems in two different settings, each of which inspires an algorithm designed to improve the OOD adversarial robustness. The proofs of all results in this section can be found in Appendix A. We first introduce some notations and basic setups.

**Notations.** We define \([n]\coloneqq\{1,2,\cdots,n\}\). We denote scalars and vectors with lowercase letters and lowercase bold letters respectively. We use uppercase letters to denote matrices or random variables, and uppercase bold letters to denote random vectors or random matrices. For a vector \(\mathbf{x}\in\mathbb{R}^{n}\), we define the \(\ell_{p}\)-norm of \(\mathbf{x}\) as \(\|\mathbf{x}\|_{p}\coloneqq\left(\sum_{i=1}^{n}|x_{i}|^{p}\right)^{1/p}\) for \(p\in[1,\infty)\), where \(x_{i}\) is the \(i\)-th element of \(\mathbf{x}\); for \(p=\infty\), we define \(\|\mathbf{x}\|_{\infty}\coloneqq\max\limits_{1\leq i\leq n}|x_{i}|\). For a matrix \(A\in\mathbb{R}^{m\times n}\), the Frobenius norm ofis defined as \(\|A\|_{F}\coloneqq\left(\sum_{i=1}^{m}\sum_{j=1}^{n}A_{ij}^{2}\right)^{\frac{1}{2}}\), where \(A_{ij}\) is the entry of \(A\) at the \(i\)-th row and \(j\)-th column. We define the determinant of \(A\) as \(\det(A)\). \(\mathcal{N}(\mathbf{\mu},\Sigma)\) represents the multivariable Gaussian distribution with mean vector \(\mathbf{\mu}\) and covariance matrix \(\Sigma\). Given \(f,g:\mathbb{R}\rightarrow\mathbb{R}_{+}\), we write \(f=\mathcal{O}(g)\) if there exist \(x_{0},\alpha\in\mathbb{R}_{+}\) such that for all \(x>x_{0}\), we have \(f(x)\leq\alpha g(x)\). We use \(sign(\cdot)\) to denote the sign function [57].

**Setups.** Let \(\mathcal{X}\in\mathbb{R}^{m}\) be the input space and \(\mathcal{Y}\) be the label space. We set \(\mathcal{Y}=\{\pm 1\}\), \(\mathcal{Y}=\{1,2,\cdots,K\}\) (where \(K\) is the number of classes), and \(\mathcal{Y}=\mathbb{R}\) for the binary classification problem, the multi-class classification problem, and the regression problem, respectively. We use \(\ell:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}_{+}\) as the loss function. We consider learning with the hypothesis class \(\mathcal{H}\subseteq\{h:\mathcal{X}\rightarrow\mathcal{Y}\}\). Given a distribution \(\mathcal{D}\) on \(\mathcal{X}\times\mathcal{Y}\), the error of \(h\in\mathcal{H}\) with respect to the loss function \(\ell\) under the distribution \(\mathcal{D}\) is \(\mathcal{R}_{\mathcal{D}}(\ell,h)=\mathop{\mathbb{E}}_{(\mathbf{x},y)\sim\mathcal{ D}}\left[\ell(h(\mathbf{x}),y)\right]\), where \(\mathbf{x}\in\mathcal{X}\) and \(y\in\mathcal{Y}\). We further define \(\mathcal{B}(\cdot):\mathcal{X}\to 2^{\mathcal{X}}\) as a perturbation function that maps an input \(\mathbf{x}\) to a subset \(\mathcal{B}(\mathbf{x})\subseteq\mathcal{X}\), where \(2^{\mathcal{X}}\) is the power set of \(\mathcal{X}\). The adversarial error of the predictor \(h\) under the perturbation function \(\mathcal{B}(\cdot)\) is defined as

\[\mathcal{R}_{\mathcal{D}}^{\mathcal{B}}(\ell,h)=\mathop{\mathbb{E}}_{(\mathbf{x}, y)\sim\mathcal{D}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h( \mathbf{x}^{\prime}),y)\right]\!.\]

### The Average Case

In this section, following [20; 14; 52], we consider the average case, i.e., the case in which the target environment follows a distribution. In this case, we aim to minimize the average target adversarial error of the hypothesis \(h\in\mathcal{H}\), where the average is taken over the distribution of the target environment.

Suppose that \(\mathcal{P}(\mathcal{X}\times\mathcal{Y})\) is the set of all possible probability measures (environments) on \(\mathcal{X}\times\mathcal{Y}\) for the task of interest. Assume there is a prior \(p\) on \(\mathcal{P}(\mathcal{X}\times\mathcal{Y})\), which is the distribution of the environments. Moreover, suppose the process of sampling training and test data is as follows:

(1). We generate the training data according to the following two steps: (i) we sample \(t\) training environments from \(p\), i.e., \(\mathcal{D}_{1},\cdots,\mathcal{D}_{t}\sim p\); (ii) the examples \(\widehat{\mathcal{D}}_{i}=\{(\mathbf{x}_{i1},y_{i1}),\cdots,(\mathbf{x}_{in_{i}},y_{in _{j}})\}\sim\mathcal{D}_{i}^{n_{i}}\) are drawn independent and identically distributed (i.i.d.) from \(\mathcal{D}_{i}\), where \(n_{i}\) is the size of \(\widehat{\mathcal{D}}_{i}\) and \(i\in[t]\). To simplify the notations, we also use \(\widehat{\mathcal{D}}_{i}\) to denote the empirical distribution of the dataset \(\{(\mathbf{x}_{i1},y_{i1}),\cdots,(\mathbf{x}_{in_{i}},y_{in_{i}})\}\). Let \(\widehat{\mathcal{D}}=\frac{1}{t}\sum_{i=1}^{t}\widehat{\mathcal{D}}_{i}\) and \(\tilde{p}=\frac{1}{t}\sum_{i=1}^{t}\mathcal{D}_{i}\). To simplify the problem, we assume \(n_{1}=n_{2}=\cdots=n_{t}=n\), but note that with a more careful analysis, our result in the average case can be extended to the case in which \(n_{1},\cdots,n_{t}\) are not necessarily the same.

(2). For each test example, we first sample an environment from \(p\), i.e., \(\mathcal{D}\sim p\), then sample an example \((\mathbf{x},y)\sim\mathcal{D}\).

Let \(\mathcal{L}_{p}(\ell,h)=\mathop{\mathbb{E}}_{\mathcal{D}\sim p}\left[\mathcal{ R}_{\mathcal{D}}(\ell,h)\right]\) be the average risk of \(h\) on prior \(p\). For the perturbation function \(\mathcal{B}(\cdot)\), we define \(\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)=\mathop{\mathbb{E}}_{\mathcal{D}\sim p} \left[\mathcal{R}_{\mathcal{D}}^{\mathcal{B}}(\ell,h)\right]\) as the average adversarial risk of \(h\). The empirical Rademacher complexity [55; Chapter 26] of the hypothesis class \(\mathcal{H}\) is defined as follows:

\[\mathfrak{R}_{n}(\mathcal{H})=\mathop{\mathbb{E}}_{\mathbf{\sigma}}\left[\sup_{h \in\mathcal{H}}\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}h(\mathbf{x}_{i})\right],\]

where \(\mathbf{\sigma}\) is a Rademacher random vector with i.i.d. entries, and \(\{\mathbf{x}_{1},\cdots,\mathbf{x}_{n}\}\) is a set of data points. The following theorem presents an upper bound for the average adversarial risk of \(h\).

**Theorem 3.1**.: _Suppose the loss function \(\ell\) is bounded, i.e., \(\ell\in[0,U]\). Then with probability of at least \(1-\delta\) over the sampling of \(\widehat{\mathcal{D}}_{1},\cdots,\widehat{\mathcal{D}}_{t}\), the following bound holds for all \(h\in\mathcal{H}\):_

\[\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)\leq\mathcal{L}_{\widehat{\mathcal{D}}}^{ \mathcal{B}}(\ell,h)+2\mathfrak{R}_{t}(\widetilde{\mathcal{G}})+2\mathfrak{R} _{tn}(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\ln 4/\delta}{2t}}+3U\sqrt{\frac{\ln 4/ \delta}{2tn}},\]

_where_

\[\widetilde{\mathcal{G}}=\left\{g_{h}:\mathcal{X}\times\mathcal{Y} \rightarrow\mathbb{R}_{+}\Big{|}g_{h}(\mathbf{x},y)=\sup_{\mathbf{x}^{\prime}\in \mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime},y)),h\in\mathcal{H}\right\}.\]Theorem 3.1 presents a bound for all hypotheses \(h\in\mathcal{H}\). We now consider the convergence property of the adversarial empirical risk minimization (**AERM**) algorithm in the average case. We define the output of the AERM algorithm as \(\hat{h}\in\operatorname*{arg\,inf}_{h\in\mathcal{H}}\mathcal{L}^{\mathcal{B}}_{ \mathcal{D}}(\ell,h)\). We then define the hypothesis with the best adversarial generalization performance as \(h^{\star}\in\operatorname*{arg\,inf}_{h\in\mathcal{H}}\mathcal{L}^{\mathcal{B}}_ {p}(\ell,h)\).

The next corollary shows an upper bound for the excess risk [63, Chapter 4] of AERM:

**Corollary 3.2**.: _Suppose the loss function \(\ell\) is bounded, i.e., \(\ell\in[0,U]\). Then with probability of at least \(1-\delta\) over the sampling of \(\widehat{\mathcal{D}}_{1},\cdots,\widehat{\mathcal{D}}_{t}\), the following bound holds:_

\[\mathcal{L}^{\mathcal{B}}_{p}(\ell,\hat{h})\leq\mathcal{L}^{\mathcal{B}}_{p}( \ell,h^{\star})+4\mathfrak{R}_{t}(\widetilde{\mathcal{G}})+4\mathfrak{R}_{tn} (\widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}8/\delta}{2t}}+3U\sqrt{\frac {\text{ln}8/\delta}{2tn}}.\]

**Remark 1**.: _The convergence rate for both Theorem 3.1 and Corollary 3.2 is \(\mathcal{O}\left(t^{-\frac{1}{2}}\right)\), which prompts us to ask: can we derive a tighter bound that has a faster convergence rate than \(\mathcal{O}\left(t^{-\frac{1}{2}}\right)\)? The next section provides an affirmative answer to this question._

### Theory with Limited Environments

In this section, we provide the theoretical analysis for the limited training environment case, i.e., in the case in which \(t=\mathcal{O}(1)\). Suppose that we have \(t\) training environments \(\mathcal{D}_{1},\cdots,\mathcal{D}_{t}\) and one unknown target environment \(\mathcal{T}\). Our goal is to find a predictor that obtains good adversarial robustness on the target environment \(\mathcal{T}\). Assume we obtain \(n\) examples from each training distribution \(\mathcal{D}_{i}\).

First, we introduce a discrepancy between the distributions. Based on the hypothesis class \(\mathcal{H}\), the seminal work for unsupervised domain adaptation (UDA), [9], defines the discrepancy between two distributions as follows:

\[d_{\mathcal{H}}(\mathcal{D},\mathcal{D}^{\prime})\!=\!2\sup_{h\in\mathcal{H}} \left|\operatorname*{\mathbb{P}}_{\mathbf{x}\sim\mathcal{D}}\left[h(\mathbf{x})=1 \right]\!-\!\operatorname*{\mathbb{P}}_{\mathbf{x}\sim\mathcal{D}^{\prime}}\left[ h(\mathbf{x})=1\right]\right|.\]

[9] analyze the generalization error bound of the target domain for UDA by \(d_{\mathcal{H}\Delta\mathcal{H}}(\cdot,\cdot)\), where \(g\in\mathcal{H}\Delta\mathcal{H}\Longleftrightarrow g=h\oplus h^{\prime}\) for some \(h,h^{\prime}\in\mathcal{H}\); here, \(\oplus\) is the XOR operator. However, the above definition is limited to the binary classification. This paper extends \(d_{\mathcal{H}}\) to the multi-class adversarial case. Given the loss function \(\ell\), distributions \(P\), \(Q\) on \(\mathcal{X}\times\mathcal{Y}\), and hypothesis class \(\mathcal{H}\), we define the adversarial discrepancy as follows:

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q)=\sup_{h\in\mathcal{H}}\left|\operatorname *{\mathbb{E}}_{\mathbf{(x,y)}\sim P}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{ x})}\ell(h(\mathbf{x}^{\prime}),y)\right]-\operatorname*{\mathbb{E}}_{\mathbf{(x,y)} \sim Q}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime }),y)\right]\right|.\]

When \(\mathcal{B}(\mathbf{x})=\{\mathbf{x}\}\), \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q)\) becomes the standard case. It can be easily verified that \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) is symmetric and satisfies the triangle inequality (the proof can be found in Appendix A.3); thus, \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) is a pseudometric.

**Comparison of \(d_{\mathcal{H}}(\cdot,\cdot)\) and \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\).** The theory outlined in [9, Theorem 2] presents an upper bound with a \(d_{\mathcal{H}\Delta\mathcal{H}}(\cdot,\cdot)\) term. To align the feature distributions of the source and target domain, we need to calculate \(d_{\mathcal{H}\Delta\mathcal{H}}(P,Q)\), which takes the supremum over two hypotheses \(h,h^{\prime}\in\mathcal{H}\):

\[d_{\mathcal{H}\Delta\mathcal{H}}(P,Q)=2\sup_{h,h^{\prime}\in\mathcal{H}}\left| \operatorname*{\mathbb{P}}_{\mathbf{x}\sim P}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{ x})\right]-\operatorname*{\mathbb{P}}_{\mathbf{x}\sim Q}\left[h(\mathbf{x})\neq h^{ \prime}(\mathbf{x})\right]\right|.\]

However, the definition of \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) shows that: \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q)\) takes the supremum over one hypothesis \(h\in\mathcal{H}\), which is easier to optimize [76] and can thus significantly ease the minimax optimization in Section 4.2.

**Theorem 3.3**.: _For a given but unknown target distribution \(\mathcal{T}\), let \(\Delta^{t-1}\coloneqq\{(\lambda_{1},\cdots,\lambda_{t})|\lambda_{i}\geq 0,\sum_{i=1} ^{t}\lambda_{i}=1\}\) be the \((t-1)\)-dimensional simplex, and \(\text{Conv}(\mathfrak{D})\coloneqq\left\{\sum_{i=1}^{t}\lambda_{i}\mathcal{D}_{ i}\middle|\mathbf{\lambda}\in\Delta^{t-1}\right\}\) be the convex hull of \(\mathfrak{D}=\{\mathcal{D}_{1},\ldots,\mathcal{D}_{t}\}\). Let \(\mathcal{T}_{P}\in\operatorname*{arg\,inf}_{\mathcal{D}\in\text{Conv}( \mathfrak{D})}d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{D},\mathcal{T})\) be the "projection" of \(\mathcal{T}\)onto \(\text{Conv}(\mathfrak{D})\), and \(\mathbf{\lambda}^{\star}\) be the weight vector where \(\mathcal{T}_{P}=\sum_{i=1}^{t}\lambda_{i}^{\star}\mathcal{D}_{i}\). Assume \(\ell\in[0,U]\). Then: with probability of at least \(1-\delta\), for all \(h\in\mathcal{H}\),_

\[\mathcal{R}_{\mathcal{T}}^{\mathcal{B}}(\ell,h) \leq\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}_{\widehat{\mathcal{D}}_{ i}}^{\mathcal{B}}(\ell,h)+\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{\star}d_{ \ell(\mathcal{H})}^{\mathcal{B}}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D }}_{j})+d_{\ell(\mathcal{H})}^{\mathcal{B}}(\mathcal{T},\mathcal{T}_{P})\] \[+4\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+2\mathfrak{R}_{n}( \widetilde{\mathcal{G}})+6U\sqrt{\frac{\text{ln}8/\delta}{2tn}}+3U\sqrt{\frac {\text{ln}(16t/\delta)}{2n}}.\]

**Remark 2**.: _The first term of the bound is the average empirical adversarial robust error of the model on training environments, and the second term is the weighted average discrepancy on the empirical training distributions \(\widehat{\mathfrak{D}}=\{\widehat{\mathcal{D}}_{1},\cdots,\widehat{\mathcal{D }}_{t}\}\). The third term can be viewed as the **distance** between \(\mathcal{T}\) and the convex hull \(\text{Conv}(\mathfrak{D})\), which is fixed once the task, \(\ell,\widehat{\mathfrak{D}}\) and \(\mathcal{T}\) are given. Thus, minimizing the first two terms of the bound is able to improve the OOD adversarial robustness._

_Moreover, \(\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})\) and \(\mathfrak{R}_{n}(\widetilde{\mathcal{G}})\) measure the capacity of the model, and can be regarded as an implicit regularizer on the model. There are many existing works that present the upper bounds of the empirical Rademacher complexity for neural networks [49, 6, 7, 23] and the adversarial Rademacher complexity [31, 73, 18, 5, 67]. Their results can be summarized as follows: for bounded \(\mathcal{X}\), with proper weak assumptions on the loss function \(\ell\), \(\mathfrak{R}_{n}(\widetilde{\mathcal{G}})\) can be upper-bounded by \(\mathcal{O}(\frac{\mathcal{C}}{\sqrt{n}})\), where \(C\) is a constant that is related to some norm of the parameters of \(h\) and increases with the norm. Thus, we consider constraints on the norm of the parameters of \(h\) in the algorithm designing part of Section 4._

_Last but not least, different from the results in Theorem 3.1, the convergence rate is \(\mathcal{O}\left(\sqrt{\frac{\text{ln}1/\delta}{tn}}+\sqrt{\frac{\text{ln}t/ \delta}{n}}\right)\). When \(t=\mathcal{O}(1)\), \(\mathcal{O}\left(\sqrt{\frac{\text{ln}t/\delta}{n}}\right)\) in Theorem 3.3 converges much faster than \(\mathcal{O}\left(\sqrt{\frac{\text{ln}1/\delta}{t}}\right)\) in Theorem 3.1, since \(n\gg t\)._

The \(d_{\ell(\mathcal{H})}^{\mathcal{B}}(\mathcal{T},\mathcal{T}_{P})\) term in Theorem 3.3 is determined by the distance between the source and target environments. A larger \(d_{\ell(\mathcal{H})}^{\mathcal{B}}(\mathcal{T},\mathcal{T}_{P})\) may lead to worse adversarial robustness of the model on the target domain. Next, we present a toy example to illustrate this case.

**Example 1**.: We consider the input space \(\mathcal{X}=\mathbb{R}^{d}\subseteq\mathbb{R}^{m}\) and label space \(\mathcal{Y}=\{+1,-1\}\). For simplicity, we consider only three distributions on \(\mathcal{X}\times\mathcal{Y}\), i.e., \(\mathcal{D}^{(0)},\mathcal{D}^{(1)}\) and \(\mathcal{D}^{(2)}\). The marginal distributions for \(Y\) satisfy \(\mathcal{D}_{Y}^{(0)}(\{-1\})=\mathcal{D}_{Y}^{(0)}(\{+1\})=\mathcal{D}_{Y}^{(1 )}(\{-1\})=\mathcal{D}_{Y}^{(1)}(\{+1\})=\mathcal{D}_{Y}^{(2)}(\{-1\})= \mathcal{D}_{Y}^{(2)}(\{+1\})=\frac{1}{2}\). For \(\mathcal{D}^{(i)}\), let the conditional distribution of \(\mathbf{X}^{(i)}\) given \(Y\) be \(\mathbf{X}^{(i)}|Y=\gamma\sim\mathcal{N}(y\mathbf{\mu}^{(i)},\sigma^{2}I)\), where \(\mathbf{\mu}^{(i)}\in\mathbb{R}^{d}\) is a non-zero vector and \(\sigma^{2}I\in\mathbb{R}^{d\times d}\) is the covariance matrix, i.e., the elements of \(\mathbf{X}^{(i)}\) are independent. Let \(Q\) be a rotation transformation on \(\mathbb{R}^{d}\), which is a special orthogonal transformation. Suppose that \(\overline{Q}\neq\underline{I}\), where \(\underline{I}\) is the identity transformation. Let \(Q\in\mathbb{R}^{d\times d}\) be the matrix of \(Q\) under some orthonormal basis. We then know that \(Q\) is a rotation matrix and \(Q^{T}Q=QQ^{T}=\overline{I},\det(Q)=1,Q\neq I\). To model the distributional shift among the environments, we apply the transformation \(\underline{Q}\) to \(\mathbf{\mu}^{(0)}\) and use \(\mathbf{\mu}^{(1)}=\underline{Q}\mathbf{\mu}^{(0)},\mathbf{\mu}^{(2)}=\underline{Q^{2}}\bm {\mu}^{(0)}\) as the mean vectors of \(\mathcal{D}^{(1)},\mathcal{D}^{(2)}\) respectively. Here, we consider the \(\ell_{2}\)-norm adversarial attack with radius \(\epsilon\), i.e., \(\mathcal{B}(\mathbf{x})=\{\mathbf{x}^{\prime}:\|\mathbf{x}-\mathbf{x}^{\prime}\|_{2}\leq \epsilon\}\). We use two environments as the training environments and the remainder as the test environment. Theorem 3.4 shows that a larger distance between the training and test environments leads to worse OOD adversarial robustness.

Figure 1: An intuitive visualization of the distributions \(\mathcal{D}^{(0)},\mathcal{D}^{(1)},\mathcal{D}^{(2)}\) (**red, green, blue** respectively). We also represent the three mean vectors \(\mathbf{\mu}\), \(\underline{Q}\mathbf{\mu}\), \(\underline{Q}^{2}\mathbf{\mu}\) using red, green, and blue arrows. The angle between \(\mathbf{\mu},\underline{Q}\mathbf{\mu}\) is \(\alpha\), which is the same as the angle between \(\underline{Q}\mathbf{\mu},\underline{Q}^{2}\mathbf{\mu}\). More details about the relationship between \(\underline{Q}\) and \(\alpha\) can be found in the proof of Theorem A.4 in Appendix A.1.

**Theorem 3.4**.: _Consider the setting in Example 1, and suppose that \(\mathbf{\mu}\) lies in the 2-dimensional subspace \(\mathfrak{R}\) in Theorem A.4 (see Appendix A.1 for details about \(\mathfrak{R}\), and see Figure 1 for an intuitive visualization). Let \(\ell_{01}(x,y)=\mathbbm{1}[x\neq y]\), where \(\mathbbm{1}[\cdot]\) is the indicator function and \(B_{p}^{r}(\mathbf{x})=\{\mathbf{x}^{\prime}:\|\mathbf{x}-\mathbf{x}^{\prime}\|_{p}\leq r\}\). Consider training with hypothesis class \(\mathcal{H}=\{h_{\mathbf{\mu}}:h_{\mathbf{\mu}}(x)=sign(\mathbf{w}^{T}x),\mathbf{w}\in\mathbb{ R}^{d}\}\), and denote \(\mathcal{D}^{(ij)}=\frac{1}{2}\mathcal{D}^{(i)}+\frac{1}{2}\mathcal{D}^{(j)},i,j\in\{0,1,2\},i\neq j\). Consider the \(\ell_{2}\)-norm adversarial attack with radius \(\epsilon\); for notation convenience, let \(\widetilde{\mathcal{R}}_{ij}(\mathbf{w})=\mathcal{R}_{\mathcal{D}^{(ij)}}^{B_{2}^{ \epsilon}}(\ell_{01},h_{\mathbf{\mu}})\) and \(\widetilde{\mathcal{R}}_{i}(\mathbf{w})=\mathcal{R}_{\mathcal{D}^{(i)}}^{B_{2}^{ \epsilon}}(\ell_{01},h_{\mathbf{\mu}})\). Let \(\Phi(\cdot)\) be the distribution function of the standard normal distribution. Let \(\alpha\) denote the angle between \(\mathbf{\mu}\) and \(Q\mathbf{\mu}\), which is the rotation angle of \(Q\) in the subspace \(\mathfrak{R}\). Furthermore, suppose that \(0<\alpha\leq arccos\frac{\epsilon}{\|\mathbf{\mu}_{1}\|_{2}}\). We then have: (1) If we train with \(\mathcal{D}^{(0)}\) and \(\mathcal{D}^{(1)}\), let \(\mathbf{w}_{(01)}{=}\mathbf{\mu}{\downarrow}Q\mathbf{\mu}\), which achieves the minimum of \(\widetilde{\mathcal{R}}_{01}(\mathbf{w})\). Then the adversarial accuracy of \(\mathbf{w}_{(01)}\) on the test distribution \(\mathcal{D}^{(2)}\) is \(\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)}){=}\Phi\left(\frac{\epsilon}{\sigma} -\frac{(\mathbf{\mu}+Q\mathbf{\mu})^{T}Q^{2}\mathbf{\mu}}{\sigma\|\mathbf{\mu}+Q\mathbf{\mu}\|_{2}}\right)\). (2) If we train with \(\mathcal{D}^{(0)}\) and \(\mathcal{D}^{(2)}\), let \(\mathbf{w}_{(02)}{=}\mathbf{\mu}{\downarrow}\underline{Q}^{2}\mathbf{\mu}\), which achieves the minimum of \(\widetilde{\mathcal{R}}_{02}(\mathbf{w})\). Then the adversarial accuracy of \(\mathbf{w}_{(02)}\) on the test distribution \(\mathcal{D}^{(1)}\) is \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)}){=}\Phi\left(\frac{\epsilon}{\sigma} -\frac{(\mathbf{\mu}+Q^{2}\mathbf{\mu})^{T}Q\mathbf{\mu}}{\sigma\|\mathbf{\mu}+Q^{2}\mathbf{\mu}\|_ {2}}\right)\). (3) If we train with \(\mathcal{D}^{(1)}\) and \(\mathcal{D}^{(2)}\), let \(\mathbf{w}_{(12)}{=}\underline{Q}\mathbf{\mu}{\downarrow}\underline{Q}^{2}\mathbf{\mu}\), which achieves the minimum of \(\widetilde{\mathcal{R}}_{12}(\mathbf{w})\). Then the adversarial accuracy of \(\mathbf{w}_{(12)}\) on the test distribution \(\mathcal{D}^{(0)}\) is \(\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)}){=}\Phi\left(\frac{\epsilon}{\sigma} -\frac{(Q\mathbf{\mu}+Q^{2}\mathbf{\mu})^{T}\mathbf{\mu}}{\sigma\|\mathbf{\mu}+Q^{2}\mathbf{\mu}\|_ {2}}\right)\). (4) \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})<\widetilde{\mathcal{R}}_{2}(\mathbf{w}_ {(01)})=\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)})\)._

**Remark 3**.: _Let \(\{i,j,k\}=\{0,1,2\}\). For task \(i\), we use \(\mathcal{D}^{(j)},\mathcal{D}^{(k)}\) as the training environments and \(\mathcal{D}^{(i)}\) as the test environment. \(\widetilde{\mathcal{R}}_{i}(\mathbf{w}_{(jk)})\) is the target adversarial error of the learned classifier \(\mathbf{w}_{(jk)}\) in task \(i\)._

_We now consider the distance between the training and test environments for each task. Intuitively, we regard the angle as the "distance" between two distributions. We define \(\text{angle}(i,j)\) as the angle between the mean vector of \(\mathcal{D}^{(i)}\) and \(\mathcal{D}^{(j)}\), \(i\neq j\). For task \(i\), we define the average "distance" between the test environment and each training environment as \(d_{\text{arg}}(i):=\frac{angle(i,j)+angle(i,k)}{2}\). We use \(d_{\text{arg}}(i)\) as a measure of the distance between the training environments and the test environment for task \(i\)._

_From the settings in Example 1, it can be clearly seen that \(d_{\text{arg}}(0)=\frac{angle(0,1)+angle(0,2)}{2}=\frac{\alpha+2\alpha}{2}= \frac{3\alpha}{2}=d_{\text{arg}}(2)\) and \(d_{\text{arg}}(1)=\frac{angle(1,0)+angle(1,2)}{2}=\frac{\alpha+\alpha}{2}=\alpha\), which implies that \(d_{\text{arg}}(1)<d_{\text{arg}}(0)=d_{\text{arg}}(2)\). Theorem 3.4 tells us that \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})<\widetilde{\mathcal{R}}_{0}(\mathbf{w}_ {(12)})=\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)})\), and thus implies that \(\mathbf{a}\) smaller distance between the training and test environments leads to better OOD adversarial robustness_.

_Moreover, the assumption of the angle between \(\mathbf{\mu}\) and \(Q\mathbf{\mu}\) in Theorem 3.4 is reasonable. Consider \(\epsilon=\frac{\|\mathbf{\mu}\|_{2}}{2}\); in this case, the attack is strong and perceptible to human eyes. Then, \(\alpha\leq arccos\frac{\epsilon}{\|\mathbf{\mu}_{1}\|_{2}}=arccos\frac{1}{2}=\frac{ \pi}{3}\). In this case, the maximal angle between the environments is \(\frac{2\pi}{3}\), which leads to a strong distribution shift. When \(\epsilon<\frac{\|\mathbf{\mu}\|_{2}}{2}\), the allowed rotation angle can be further enlarged, and when \(\epsilon=0\), it becomes the standard case._

_Furthermore, the data distribution here can be regarded as a simplified data model for RotatedMNIST. Moreover, our experimental results in Section 5 are consistent with our analysis here in both the standard and adversarial cases. Please refer to the observation part of Section 5.2 and the table in Appendix D.1 for further details._

## 4 Algorithms

In this section, based on our theory, we present two algorithms that can improve the adversarial robustness of the model on the target environment.

### Adversarial Empirical Risk Minimization (AERM or AT)

Based on Theorem 3.1, which shows an upper bound of the average adversarial risk over the environments, we propose our first algorithm: adversarial empirical risk minimization (AERM, which corresponds to applying AT to multiple source domains). The bound in Theorem 3.1 consists of the average adversarial empirical risk \(\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathsf{B}}(\ell,h)=\frac{1}{t}\sum_{i=1}^ {t}\mathcal{R}_{\widehat{\mathcal{D}}_{i}}^{\mathsf{B}}(\ell,h)\) and two empirical Rademacher complexity terms \(\mathfrak{R}_{t}(\widetilde{\mathcal{G}})+\mathfrak{R}_{tn}(\widetilde{ \mathcal{G}})\). As outlined in Remark 2, the empirical Rademacher complexity implies an implicit regularizer on the model capacity. We choose \(\|\cdot\|_{F}\) as a regularizer on the model parameters. The optimization objective of AT is as follows:

\[L_{\text{AT}}(h)=\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}_{\widehat{\mathcal{D}}_ {i}}^{\mathsf{B}}(\ell,h)+\lambda\|W\|_{F},\]

where \(\lambda\) is a trade-off hyper-parameter, and \(W\) is the parameter matrix of the model. From Remark 1, we know that AT may not generalize well in the case where the training environments are limited. Next, we propose another algorithm for the limited environment case.

### Robust DANN (RDANN)

Theorem 3.3 shows that \(\mathfrak{R}_{n}(\widetilde{\mathcal{G}})\), \(\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}_{\widehat{\mathcal{D}}_{i}}^{\mathsf{B}} (\ell,h)\), and \(\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{t}d_{\ell(\mathcal{H})}^{\mathsf{B}} (\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\) play the key roles in designing the OOD adversarial training methods. However, the weights \(\lambda_{1}^{*},\cdots,\lambda_{t}^{*}\) are unknown, since we have no information about the target distribution \(\mathcal{T}\). Since \(\lambda_{j}^{*}\in[0,1],\forall j\), it is evident that \(\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{*}d_{\ell(\mathcal{H})}^{\mathsf{B}} (\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\leq\frac{1}{t}\sum_{i} \sum_{j}d_{\ell(\mathcal{H})}^{\mathsf{B}}(\widehat{\mathcal{D}}_{i},\widehat{ \mathcal{D}}_{j})\). We therefore turn to optimize the average discrepancy \(\frac{1}{t^{2}}\sum_{i}\sum_{j}d_{\ell(\mathcal{H})}^{\mathsf{B}}(\widehat{ \mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\). To improve the OOD adversarial robustness, we minimize the following:

\[\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}_{\widehat{\mathcal{D}}_{i}}^{\mathsf{B}} (\ell,h)+\lambda_{1}\frac{1}{t^{2}}\sum_{i=1}^{t}\sum_{j=1}^{t}d_{\ell( \mathcal{H})}^{\mathsf{B}}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{ j})+\lambda_{2}\|W\|_{F},\]

where \(\lambda_{1},\lambda_{2}\) are two hyper-parameters, and \(W\) is the parameter matrix of the model. However, the term \(\frac{1}{t^{2}}\sum_{i=1}^{t}\sum_{j=1}^{t}d_{\ell(\mathcal{H})}^{\mathsf{B}} (\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\) is a constant. Motivated by [17], we minimize the discrepancy of the training environments in the feature space of a feature extractor \(f\).

Specifically, we consider \(\mathcal{H}=\{c\circ f|f\in\mathcal{F},c\in\mathcal{C}\}\); this means that the predictor \(h\) consists of a classifier \(c\) and a feature extractor \(f\), where \(\mathcal{F}=\{f:\mathcal{X}\rightarrow\mathcal{Z}\}\), \(\mathcal{C}=\{c:\mathcal{Z}\rightarrow\mathcal{Y}\}\) and \(\mathcal{Z}\subseteq\mathbb{R}^{l}\) is the feature space. Any feature extractor \(f\in\mathcal{F}\) determines a hypothesis class \(\mathcal{H}_{f}=\{c\circ f|c\in\mathcal{C}\}\). Given a feature extractor \(f\), we apply Theorem 3.3 to the hypothesis class \(\mathcal{H}_{f}\). Then with high probability, for any \(c\in\mathcal{C}\), the target error of \(h=c\circ f\) can be controlled mainly by \(\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}_{\widehat{\mathcal{D}}_{i}}^{\mathsf{B}}( \ell,c\circ f)\) and \(\frac{1}{t^{2}}\sum_{i=1}^{t}\sum_{j=1}^{t}d_{\ell(\mathcal{H}_{f})}^{\mathsf{ B}}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\). We then aim to find \(c\) and \(f\) such that \(h=c\circ f\) has good OOD adversarial robustness. Thus, we aim to minimize the following:

\[\overbrace{\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}_{\widehat{\mathcal{D}}_{i}}^{ \mathsf{B}}(\ell,c\circ f)}^{\mathsf{B}}+\lambda_{1}\underbrace{\frac{1}{t^ {2}}\sum_{i=1}^{t}\sum_{j=1}^{t}d_{\ell(\mathcal{H}_{f})}^{\mathsf{B}}( \widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})}_{L_{\text{data}}(\ell,f)}+ \lambda_{2}\|W\|_{F}.\]

To minimize \(L(c\circ f)\), we need to solve a minimax problem. For simplicity, we fix \(\mathcal{B}(\cdot),\ell\) and define the following:

\[D(h,P,Q)=\left|\underset{(\mathbf{x},y)\sim P}{\mathbb{E}}\left[\sup_{\mathbf{x}^{ \prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]-\underset{( \mathbf{x},y)\sim Q}{\mathbb{E}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x })}\ell(h(\mathbf{x}^{\prime}),y)\right]\right|.\]

Since \(D(h,P,P)=0\) and \(D(h,P,Q)=D(h,Q,P)\) for any \(P,Q,h\), our optimization problem can be formulated as follows:

\[\min_{c\in\mathcal{C},f\in\mathcal{F}}\max_{c_{ij}\in\mathcal{C}:1\leq i<j\leq t }L_{\text{cls}}(\ell,c\circ f)+\lambda_{1}\frac{2}{t(t-1)}\sum_{1\leq i<j\leq t }D(c_{ij}\circ f,\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})+\lambda_{ 2}\|W\|_{F}.\]To solve the minimax optimization problem, we adopt an idea similar to that of adversarial neural networks [24] and refer to \(c_{ij}\) as the discriminator. However, there are \(\frac{t(t-1)}{2}\) discriminators in this case, and training with many discriminators may make the optimization process unstable [3]. We therefore opt to use the same discriminator for all \((\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\) pairs. Note that:

\[\max_{c^{\prime}\in\mathcal{C}}\sum_{i<j}D(c^{\prime}\circ f,\widehat{ \mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\leq\max_{c_{ij}\in\mathcal{C}}\sum _{i<j}D(c_{ij}\circ f,\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j}),\]

such that optimizing over a shared discriminator \(c^{\prime}\) is equivalent to optimizing a lower bound of the original objective. Our final optimization problem then becomes:

\[\min_{c\in\mathcal{C},f\in\mathcal{F}}\max_{c^{\prime}\in\mathcal{C}}\underbrace {L_{\text{cls}}(\ell,c\circ f)}_{L_{a}(w,\theta)}+\lambda_{1}\underbrace{\frac{ 2}{t(t-1)}\sum_{1\leq i<j\leq t}D(c^{\prime}\circ f,\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})}_{L_{d}(w^{\prime},\theta)}+\lambda_{2}\underbrace {\|W\|_{F}}_{L_{\text{reg}}(w,\theta)}.\]

where \(w,w^{\prime},\theta\) are parameters for \(c,c^{\prime},f\) respectively. We call our method robust adversarial-domain neural network (**RDANN**), the pseudo-code of which is presented in Algorithm 1.

```
0: the training data \(\widehat{\mathcal{D}}_{1},\dots,\widehat{\mathcal{D}}_{t}\), the number of iterations \(T\), the number of iterations for the inner maximization problem \(k\), the learning rate \(\eta\), the inner max learning rate \(\alpha\), the tradeoff hyper-parameters \(\lambda_{1},\lambda_{2}\).
0: the parameters \(w^{T},\theta^{T}\) for \(c,f\).
1: initialize \(w^{0},\theta^{0},w^{\prime k}\) randomly
2:for\(i\gets 0\) to \(T-1\)do
3: set \(w^{\prime 0}\gets w^{\prime k}\)
4:for\(j\gets 0\) to \(k-1\)do
5:\(w^{\prime(j+1)}\gets w^{\prime j}+\alpha\nabla_{w^{\prime}}L_{d}(w^{ \prime j},\theta^{i})\)
6:endfor
7:\(L_{1}^{i}\gets L_{\text{c}}(w^{i},\theta^{i})+\lambda_{2}L_{\text{reg}}(w ^{i},\theta^{i})\)
8:\(L_{2}^{i}\gets L_{1}^{i}+\lambda_{1}L_{d}(w^{\prime k},\theta^{i})\)
9:\(\theta^{i+1}\leftarrow\theta^{i}-\eta\nabla_{\theta}L_{2}^{i}\)
10:\(w^{i+1}\gets w^{i}-\eta\nabla_{w}L_{1}^{i}\)
11:endfor
```

**Algorithm 1** RDANN

## 5 Experiments

To verify the adversarial robustness of our algorithms, we conduct experiments on the DomainBed benchmark [26], a testbed for OOD generalization that implements consistent experimental protocols across various approaches to ensure fair comparisons. Our code is attached in the supplementary material.

### Experimental Setup

**Datasets.** The datasets we use are RotatedMNIST [21], ColoredMNIST [4], VLCS [15], PACS [38], and OfficeHome [62]. There are several different environments in each dataset. For the DomainBed benchmark, we choose one environment as the test environment and use the others as training environments. We report the average accuracy over different choices of the test environment. Further details about the selected datasets can be found in Appendix C.2.

**Backbone network.** Following [26], we use a small CNN architecture for RotatedMNIST, ColoredMNIST, and ResNet-50 [27] for VLCS, PACS, and OfficeHome.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Algorithm** & **RM** & **CM** & **VLCS** & **PACS** & **OH** & **Avg** \\ \hline ERM & 0.6/0.0 & 5.8/X & 0.0/0.0 & 0.3/0.6 & 0.4/0.0 & 1.4/X \\ MLDG & 0.2/0.0 & 4.8/X & 0.0/0.0 & 0.1/0.3 & 0.6/0.1 & 1.2/X \\ CDANN & 0.9/0.0 & 8.2/X & 3.0/0.0 & 1.5/0.3 & 0.1/0.0 & 2.7/X \\ VREx & 0.2/0.0 & 6.4/X & 0.0/0.0 & 0.0/0.3 & 0.4/0.1 & 1.4/X \\ RSC & 1.0/0.0 & 3.9/X & 0.0/0.0 & 0.1/0.4 & 0.7/0.0 & 1.1/X \\ MAT & X/X & 10.7/X & 0.0/0.0 & 0.7/1.4 & 0.8/0.1 & X/X \\ LDAT & X/X & 7.9/X & 0.0/0.0 & 0.1/0.3 & 0.4/0.1 & X/X \\ AT (ours) & 93.4/**93.3** & **51.6**/X & 42.6/41.8 & **48.1**/47.6 & **30.2/29.8** & 53.2/X \\ RDANN (ours) & **93.5/93.3** & 51.1/X & **44.9/43.9** & **48.1/48.0** & 28.6/27.4 & **53.3**/X \\ \hline \hline \end{tabular}
\end{table}
Table 2: The results (\(\%\)) of the algorithms. Results are presented in the form of **a/b**: here, **a** is the OOD adversarial accuracy under PGD-20 attack [44]; and **b** is the OOD adversarial accuracy under AutoAttack [12]. We conduct the \(\ell_{\infty}\)-norm attack. Since [64] do not realize MAT and LDAT for RotatedMNIST, we use X to denote the unrealized results. Best results for PGD-20 attack are shown in **bold**. For more details about the algorithms, please refer to Appendix C.1. We use RM, CM, and OH as the abbreviation of RotatedMNIST, ColoredMNIST, and OfficeHome, respectively.

**Hyper-parameters for adversarial attack and adversarial training.** We use \(\ell_{\infty}\)-norm attack for both adversarial attack and adversarial training. We use \(\epsilon=0.1\) for ColoredMNIST and RotatedMNIST, and \(\epsilon=4/255\) for VLCS, PACS, and OfficeHome; moreover, following [74; 75], we use PGD-10 to generate adversarial examples at the training stage and PGD-20 at the evaluation stage to avoid overfitting. The step size used to generate adversarial examples is set to be \(\epsilon/4\).

More details of the experimental settings can be found in Appendix C.4.

### Results

Table 2 presents the results of our experiments. As is clear from the table, our proposed algorithms significantly improve the OOD adversarial robustness of the model. For example, under the attack PGD-20, all other algorithms achieve **no more than \(3\%\)** average adversarial accuracy, while our proposed algorithms achieve **more than \(53\%\)** average adversarial accuracy. Moreover, in our datasets, the number of environments (\(t\)) is small. From Table 2, we can see that the overall performance of RDANN is superior to that of AT. RDANN achieves better or comparable adversarial accuracy on most datasets (except OfficeHome). The results are consistent with our claim in Remark 2: when \(t=\mathcal{O}(1)\), the bound in Theorem 3.3 converges faster than that in Theorem 3.1. The detailed results for each test environment are attached in Appendix D.

**Observations.** According to the detailed results for RotatedMNIST and ColoredMNIST in Appendix D.1 and Appendix D.2, we can make the following observations (these phenomena occur in both the adversarial training setting and the standard training setting):

* For RotatedMNIST, we have six environments, each of which corresponds to a rotation angle of the original image. The rotation angle of the \(i\)-th environment is \(i\times 15^{\circ}\), \(i\in\{0,1,2,3,4,5\}\). For task \(i\), we use the \(i\)-th environment as the test environment and the remaining environments as the training environments. Following Remark 3, we define \(d_{\text{avg}}(i)\coloneqq\frac{1}{5}\sum_{j\neq i}\text{angle}(i,j)\) as the average distance between the test environment and each training environment for task \(i\). Then, \(d_{\text{avg}}(0)=45^{\circ}\), \(d_{\text{avg}}(1)=33^{\circ}\), \(d_{\text{avg}}(2)=27^{\circ}\), \(d_{\text{avg}}(3)=27^{\circ}\), \(d_{\text{avg}}(4)=33^{\circ}\) and \(d_{\text{avg}}(5)=45^{\circ}\). We define the PGD-20 adversarial accuracy of the model trained by RDANN for task \(i\) as \(a(i)\); then, \(a(0)=90.8\), \(a(1)=94.7\), \(a(2)=95.3\), \(a(3)=95.6\), \(a(4)=95.5\) and \(a(5)=89.0\). As \(i\) increases, \(d_{\text{avg}}(i)\) first decreases and then increases, while \(a(i)\) first increases and then decreases. The result indicates that \(d_{\text{avg}}(i)\) is anticorrelated with \(a(i)\), which is consistent with the analysis in Remark 3. Note that this phenomenon occurs in all algorithms.
* For ColoredMNIST, we have three environments, each of which corresponds to a correlation between the additional channel and the label. For task \(i\), \(i\in\{0,1,2\}\), we define the correlation as \(\text{cor}(i)\) and \(\text{cor}(1)=0.9\), \(\text{cor}(2)=0.8\), \(\text{cor}(3)=-0.9\). Here, we define \(d_{\text{avg}}(i)=\left\lfloor\frac{1}{2}\sum_{j\neq i}\text{cor}(j)-\text{cor }(i)\right\rfloor\), then \(d_{\text{avg}}(1)=0.95\), \(d_{\text{avg}}(2)=0.8\), \(d_{\text{avg}}(3)=1.75\). Similarly, we can define \(a(i)\) for a given algorithm. The detailed results in Appendix D.2 imply that \(d_{\text{avg}}(i)\) is anticorrelated with \(a(i)\). To further understand this phenomenon, we present another toy example for ColoredMNIST with a different data model in Appendix B.

## 6 Conclusion

In this paper, we focus specifically on out-of-distribution adversarial robustness. First, we show that existing OOD generalization algorithms are easily fooled by adversarial attacks. Motivated by this, we then study the theory of the adversarial robustness of models in two different but complementary OOD settings. Based on our theory, we propose two algorithms, AT and RDANN. Extensive experiments show that our proposed algorithms can significantly improve the OOD adversarial robustness of the model.

## Acknowledgements

This work is supported by the National Natural Science Foundation of China under Grant 61976161, the Fundamental Research Funds for the Central Universities under Grant 2042022rc0016.

## References

* Albuquerque et al. [2021] Isabela Albuquerque, Joao Monteiro, Mohammad Darvishi, Tiago H. Falk, and Ioannis Mitliagkas. Generalizing to unseen domains via distribution matching, 2021.
* Alhamoud et al. [2022] Kumail Alhamoud, Hasan Abed Al Kader Hammoud, Motasem Alfarra, and Bernard Ghanem. Generalizability of adversarial robustness under distribution shifts. _CoRR_, abs/2209.15042, 2022.
* Anonymous [2023] Anonymous. Fairness and accuracy under domain generalization. In _Submitted to The Eleventh International Conference on Learning Representations_, 2023. under review.
* Arjovsky et al. [2019] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _CoRR_, abs/1907.02893, 2019.
* Awasthi et al. [2020] Pranjal Awasthi, Natalie Frank, and Mehryar Mohri. Adversarial learning guarantees for linear hypotheses and neural networks. In _ICML_, volume 119, pages 431-441, 2020.
* Bartlett et al. [2017] Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In _NeurIPS_, pages 6240-6249, 2017.
* Bartlett and Mendelson [2002] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3:463-482, 2002.
* Bashivan et al. [2021] Pouya Bashivan, Reza Bayat, Adam Ibrahim, Kartik Ahuja, Mojtaba Faramarzi, Touraj Laleh, Blake A. Richards, and Irina Rish. Adversarial feature desensitization. In _NeurIPS_, pages 10665-10677, 2021.
* Ben-David et al. [2010] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. _Mach. Learn._, 79(1-2):151-175, 2010.
* Blanchard et al. [2021] Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain generalization by marginal transfer learning. _Journal of Machine Learning Research_, 22:2:1-2:55, 2021.
* Carlini and Wagner [2017] Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. In _SP_, pages 39-57, 2017.
* Croce and Hein [2020] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _ICML_, pages 2206-2216, 2020.
* Duchi and Namkoong [2018] John C. Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. _CoRR_, abs/1810.08750, 2018.
* Eastwood et al. [2022] Cian Eastwood, Alexander Robey, Shashank Singh, Julius von Kugelgen, Hamed Hassani, George J. Pappas, and Bernhard Scholkopf. Probable domain generalization via quantile risk minimization. _CoRR_, abs/2207.09944, 2022.
* Fang et al. [2013] Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In _ICCV_, pages 1657-1664, 2013.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _ICML_, volume 70, pages 1126-1135, 2017.
* Ganin et al. [2016] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. _Journal of Machine Learning Research_, 17:59:1-59:35, 2016.
* Gao and Wang [2021] Qingyi Gao and Xiao Wang. Theoretical investigation of generalization bounds for adversarial learning of deep neural networks. _Journal of Statistical Theory and Practice_, 15:1-28, 2021.
* Gao and Kleywegt [2023] Rui Gao and Anton J. Kleywegt. Distributionally robust stochastic optimization with wasserstein distance. _Mathematics if Operations Research_, 48(2):603-655, 2023.

* [20] Vikas K. Garg, Adam Tauman Kalai, Katrina Ligett, and Zhiwei Steven Wu. Learn to expect the unexpected: Probably approximately correct domain generalization. In _AISTATS_, volume 130, pages 3574-3582, 2021.
* [21] Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In _ICCV_, pages 2551-2559. IEEE Computer Society, 2015.
* [22] Tejas Gokhale, Swaroop Mishra, Man Luo, Bhavdeep Singh Sachdeva, and Chitta Baral. Generalized but not robust? comparing the effects of data modification methods on out-of-domain generalization and adversarial robustness. In _ACL_, pages 2705-2718, 2022.
* [23] Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of neural networks. In _COLT_, volume 75, pages 297-299, 2018.
* [24] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In _NeurIPS_, pages 2672-2680, 2014.
* [25] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In _ICLR_, 2015.
* [26] Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _ICLR_, 2021.
* [27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [28] Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In _ECCV_, volume 12347, pages 124-140, 2020.
* [29] Adam Ibrahim, Charles Guille-Escuret, Ioannis Mitliagkas, Irina Rish, David Krueger, and Pouya Bashivan. Towards out-of-distribution adversarial robustness. _CoRR_, abs/2210.03150, 2022.
* [30] Qiyu Kang, Yang Song, Qinxu Ding, and Wee Peng Tay. Stable neural ODE with lyapunov-stable equilibrium points for defending against adversarial attacks. In _NeurIPS_, pages 14925-14937, 2021.
* [31] Justin Khim and Po-Ling Loh. Adversarial risk bounds for binary classification via function transformation. _CoRR_, abs/1810.09519, 2018.
* [32] Hoki Kim. Torchattacks : A pytorch repository for adversarial attacks. _CoRR_, abs/2010.01950, 2020.
* [33] David Krueger, Ethan Caballero, Jorn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron C. Courville. Out-of-distribution generalization via risk extrapolation (rex). In _ICML_, volume 139, pages 5815-5826, 2021.
* [34] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In _ICLR_, 2017.
* [35] Yann LeCun. The mnist database of handwritten digits. [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/), 1998.
* [36] Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In _NeurIPS_, pages 2692-2701, 2018.
* [37] Boqi Li and Weiwei Liu. WAT: improve the worst-class robustness in adversarial training. In _AAAI_, pages 14982-14990, 2023.
* [38] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain generalization. In _ICCV_, pages 5543-5551, 2017.
* [39] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: Meta-learning for domain generalization. In _AAAI_, pages 3490-3497, 2018.

* Li et al. [2018] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot. Domain generalization with adversarial feature learning. In _CVPR_, pages 5400-5409, 2018.
* Li et al. [2022] Xiyuan Li, Xin Zou, and Weiwei Liu. Defending against adversarial attacks via neural dynamic system. In _NeurIPS_, 2022.
* Li et al. [2018] Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep domain generalization via conditional invariant adversarial networks. In _ECCV_, volume 11219, pages 647-663, 2018.
* Ma et al. [2022] Xinsong Ma, Zekai Wang, and Weiwei Liu. On the tradeoff between robustness and fairness. In _NeurIPS_, 2022.
* Madry et al. [2018] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR_, 2018.
* Moayeri et al. [2022] Mazda Moayeri, Kiarash Banihashem, and Soheil Feizi. Explicit tradeoffs between adversarial and natural distributional robustness. _CoRR_, abs/2209.07592, 2022.
* Montasser et al. [2019] Omar Montasser, Steve Hanneke, and Nathan Srebro. VC classes are adversarially robustly learnable, but only improperly. In _COLT_, pages 2512-2530, 2019.
* Moosavi-Dezfooli et al. [2016] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and accurate method to fool deep neural networks. In _CVPR_, pages 2574-2582, 2016.
* Nagarajan et al. [2021] Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur. Understanding the failure modes of out-of-distribution generalization. In _ICLR_, 2021.
* Neyshabur et al. [2015] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks. In _COLT_, volume 40, pages 1376-1401, 2015.
* Rebuffi et al. [2021] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Florian Stimberg, Olivia Wiles, and Timothy A. Mann. Fixing data augmentation to improve adversarial robustness. _CoRR_, abs/2103.01946, 2021.
* Rosenfeld et al. [2022] Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpolation and extrapolation in domain generalization. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _AISTATS_, volume 151, pages 2641-2657, 2022.
* Ruan et al. [2022] Yangjun Ruan, Yann Dubois, and Chris J. Maddison. Optimal representations for covariate shift. In _ICLR_, 2022.
* Sagawa et al. [2019] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _CoRR_, abs/1911.08731, 2019.
* Salman et al. [2020] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? In _NeurIPS_, 2020.
* From Theory to Algorithms_. Cambridge University Press, 2014.
* Shen et al. [2021] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _CoRR_, abs/2108.13624, 2021.
* [57] Sign function. Sign function, 2023. [Online; last edited on 16-January-2023].
* Sun and Saenko [2016] Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation. In _ECCV_, volume 9915, pages 443-450, 2016.
* Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _ICLR_, 2014.
* Tran et al. [2022] Cuong Tran, Keyu Zhu, Ferdinando Fioretto, and Pascal Van Hentenryck. Fairness increases adversarial vulnerability. _CoRR_, abs/2211.11835, 2022.

* [61] Vladimir Vapnik. _Statistical learning theory_. Wiley, 1998.
* [62] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In _CVPR_, pages 5385-5394, 2017.
* [63] Martin J. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* [64] Qixun Wang, Yifei Wang, Hong Zhu, and Yisen Wang. Improving out-of-distribution generalization by adversarial training with structured priors. _CoRR_, abs/2210.06807, 2022.
* [65] Zekai Wang and Weiwei Liu. Robustness verification for contrastive learning. In _ICML_, pages 22865-22883, 2022.
* [66] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In _ICML_, pages 36246-36263, 2023.
* [67] Jiancong Xiao, Yanbo Fan, Ruoyu Sun, and Zhi-Quan Luo. Adversarial rademacher complexity of deep neural networks. _CoRR_, abs/2211.14966, 2022.
* [68] Shiji Xin, Yifei Wang, Jingtong Su, and Yisen Wang. Domain-wise adversarial training for out-of-distribution generalization, 2022.
* [69] Jingyuan Xu and Weiwei Liu. On robust multiclass learnability. In _NeurIPS_, 2022.
* [70] Hanshu Yan, Jiawei Du, Vincent Y. F. Tan, and Jiashi Feng. On robustness of neural ordinary differential equations. In _ICLR_. OpenReview.net, 2020.
* [71] Mingyang Yi, Lu Hou, Jiacheng Sun, Lifeng Shang, Xin Jiang, Qun Liu, and Zhiming Ma. Improved OOD generalization via adversarial training and pretraining. In _ICML_, pages 11987-11997, 2021.
* [72] Dong Yin, Kannan Ramchandran, and Peter L. Bartlett. Rademacher complexity for adversarially robust generalization. In _ICML_, volume 97, pages 7085-7094, 2019.
* [73] Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John E. Hopcroft, and Liwei Wang. Adversarially robust generalization just requires more unlabeled data. _CoRR_, abs/1906.00555, 2019.
* [74] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoretically principled trade-off between robustness and accuracy. In _ICML_, volume 97, pages 7472-7482, 2019.
* [75] Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan S. Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In _ICML_, volume 119, pages 11278-11287, 2020.
* [76] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I. Jordan. Bridging theory and algorithm for domain adaptation. In _ICML_, volume 97, pages 7404-7413, 2019.
* [77] Zhengyu Zhou and Weiwei Liu. Sample complexity for distributionally robust learning under chi-square divergence. _Journal of Machine Learning Research_, 24(230):1-27, 2023.
* [78] Xin Zou and Weiwei Liu. Generalization bounds for adversarial contrastive learning. _Journal of Machine Learning Research_, 24:114:1-114:54, 2023.

Proofs

In this section, we show the proofs of the results in the mainscript.

### Proofs of the Toy Example

**Lemma A.1**.: _If \(\mathbf{X}\sim\mathcal{N}(\mathbf{\mu},\Sigma)\) where \(\mathbf{X}\sim\mathbb{R}^{n}\), then for any \(A\in\mathbb{R}^{m\times n}\), we have:_

\[A\mathbf{X}\sim\mathcal{N}(A\mathbf{\mu},A\Sigma A^{T})\]

Proof of Lemma a.1.: We prove the lemma by a powerful tool, the characteristic function of random variables. The characteristic function of a random vector \(\mathbf{X}\) is defined as:

\[\phi_{\mathbf{X}}(\mathbf{\omega})=\mathbb{E}\left[e^{i\mathbf{\omega}^{T}\mathbf{X}}\right]\]

Let \(\mathbf{Y}=A\mathbf{X}\), we have:

\[\phi_{\mathbf{Y}}(\mathbf{\omega})=\mathbb{E}\left[e^{i\mathbf{\omega}^{T}\mathbf{Y}}\right]= \mathbb{E}\left[e^{i\mathbf{\omega}^{T}(A\mathbf{X})}\right]=\mathbb{E}\left[e^{i(A^{ T}\mathbf{\omega})^{T}\mathbf{X}}\right]=\phi_{\mathbf{X}}(A^{T}\mathbf{\omega}).\]

Since \(\mathbf{X}\sim\mathcal{N}(\mathbf{\mu},\Sigma)\), we have:

\[\phi_{\mathbf{X}}(\mathbf{\omega})=\mathbb{E}\left[e^{i\mathbf{\omega}^{T}\mathbf{X}}\right]= \mathbb{E}\left[e^{i\mathbf{\omega}^{T}\mathbf{\mu}-\frac{1}{2}\mathbf{\omega}^{T}\Sigma \mathbf{\omega}}\right],\]

so we have:

\[\phi_{\mathbf{Y}}(\mathbf{\omega})=\mathbb{E}\left[e^{i(A^{T}\mathbf{\omega})^{T}\mathbf{\mu}- \frac{1}{2}(A^{T}\mathbf{\omega})^{T}\Sigma(A^{T}\mathbf{\omega})}\right]=\mathbb{E} \left[e^{i\mathbf{\omega}^{T}(A\mathbf{\mu})-\frac{1}{2}\mathbf{\omega}^{T}(A\Sigma A^{T}) \mathbf{\omega}}\right].\]

Since the characteristic function and the distribution is one-to-one, we have: \(\mathbf{Y}\) obeys the multi-variable Gaussian distribution and \(\mathbf{Y}\sim\mathcal{N}(A\mathbf{\mu},A\Sigma A^{T})\). 

**Lemma A.2**.: _Let \(\mathcal{D}_{Y}(Y=1)=\mathcal{D}_{Y}(Y=-1)=\frac{1}{2},X\in\mathbb{R}^{d}\) and \(\mathcal{D}_{X|Y=y}=\mathcal{N}(y\mathbf{\mu},\Sigma)\) where \(\mathbf{\mu}\in\mathbb{R}^{d}\) and \(\Sigma\in\mathbb{R}^{d\times d}\), then for any linear classifier \(h_{\mathbf{\omega}}\in\mathcal{H}\):_

\[\mathcal{R}_{\mathcal{D}^{p}}^{B_{\mathbf{\omega}}^{*}}(\ell_{01},h_{\mathbf{\omega}}) =\Phi\left(\frac{\epsilon\|\mathbf{w}\|_{p^{*}}-\mathbf{w}^{T}\mathbf{\mu}}{\sqrt{\mathbf{w}^{ T}\Sigma\mathbf{w}}}\right),\]

_where \(\frac{1}{p}+\frac{1}{p^{*}}=1\). Further more, if \(p=2\) and \(\Sigma=\sigma^{2}I\), then \(\mathbf{w}^{*}=\underset{\mathbf{w}:h_{\mathbf{\omega}}\in\mathcal{H}}{\arg\min}\mathcal{ R}_{\mathcal{D}}^{B_{\mathbf{\omega}}^{*}}(\ell_{01},h_{\mathbf{\omega}})\), the weight of the most robust \(h_{\mathbf{\omega}}\in\mathcal{H}\) under adversarial attack \(B_{2}^{\epsilon}(\cdot)\) is:_

\[\mathbf{w}^{*}=\mathbf{\mu}.\]

Proof of Lemma a.2.: Let \(h_{\mathbf{\omega}}(\mathbf{x})=\text{sign}(\mathbf{w}^{T}\mathbf{x})\), then we have:

\[\mathcal{R}_{\mathcal{D}^{p}}^{B_{\mathbf{\omega}}^{*}}(\ell_{01},h_ {\mathbf{\omega}}) =\underset{(\mathbf{x},y)\sim\mathcal{D}}{\mathbb{E}}\left\{\sup_{ \mathbf{x}^{\prime}\in B_{\mathbf{\omega}}^{*}(\mathbf{x})}\mathbbm{1}\left[h_{\mathbf{\omega}} (\mathbf{x})\neq y\right]\right\}\] \[=\frac{1}{2}\underset{\mathbf{x}\sim\mathcal{D}_{X|Y=1}}{\mathbb{E}} \left\{\sup_{\mathbf{x}^{\prime}\in B_{\mathbf{\omega}}^{*}(\mathbf{x})}\mathbbm{1}\left[ \mathbf{w}^{T}\mathbf{x}^{\prime}<0\right]\right\}+\frac{1}{2}\underset{\mathbf{x}\sim \mathcal{D}_{X|Y=-1}}{\mathbb{E}}\left\{\sup_{\mathbf{x}^{\prime}\in B_{\mathbf{\omega }}^{*}(\mathbf{x})}\mathbbm{1}\left[\mathbf{w}^{T}\mathbf{x}^{\prime}>0\right]\right\}\] \[=\frac{1}{2}\underset{\mathbf{x}\sim\mathcal{D}_{X|Y=1}}{\mathbb{P}} \left\{\mathbf{w}^{T}\mathbf{x}-\epsilon\|\mathbf{w}\|_{p^{*}}<0\right\}+\frac{1}{2} \underset{\mathbf{x}\sim\mathcal{D}_{X|Y=-1}}{\mathbb{P}}\left\{\mathbf{w}^{T}\mathbf{x} +\epsilon\|\mathbf{w}\|_{p^{*}}>0\right\}\] \[\overset{a}{=}\frac{1}{2}\mathbb{P}\left[\mathcal{N}(\mathbf{w}^{T} \mathbf{\mu},\mathbf{w}^{T}\Sigma\mathbf{w})<\epsilon\|\mathbf{w}\|_{p^{*}}\right]+\frac{1}{2} \mathbb{P}\left[\mathcal{N}(-\mathbf{w}^{T}\mathbf{\mu},\mathbf{w}^{T}\Sigma\mathbf{w})>- \epsilon\|\mathbf{w}\|_{p^{*}}\right]\] \[=\mathbb{P}\left[\mathcal{N}(\mathbf{w}^{T}\mathbf{\mu},\mathbf{w}^{T}\Sigma \mathbf{w})<\epsilon\|\mathbf{w}\|_{p^{*}}\right]=\Phi\left(\frac{\epsilon\|\mathbf{w}\|_{p ^{*}}-\mathbf{w}^{T}\mathbf{\mu}}{\sqrt{\mathbf{w}^{T}\Sigma\mathbf{w}}}\right),\]

where \(a\) is the result of Lemma A.1 when we regard \(\mathbf{w}^{T}\) as \(A\) in Lemma A.1.

If \(p=2\) and \(\Sigma=\sigma^{2}I\), then we have:

\[\mathcal{R}_{\mathcal{D}^{2}}^{B_{2}^{*}}(\ell_{01},h_{\mathbf{w}})=\Phi\left(\frac{ \epsilon\|\mathbf{w}\|_{2}-\mathbf{w}^{T}\mathbf{\mu}}{\sigma\|\mathbf{w}\|_{2}}\right)=\Phi \left(\frac{\epsilon}{\sigma}-\frac{\mathbf{w}^{T}\mathbf{\mu}}{\sigma\|\mathbf{w}\|_{2}} \right).\]

Since \(\Phi(\cdot)\) is increasing, we need to minimize \(\frac{\epsilon}{\sigma}-\frac{\mathbf{w}^{T}\mathbf{\mu}}{\sigma\|\mathbf{w}\|_{2}}\), i.e., to maximize \(\frac{\mathbf{w}^{T}\mathbf{\mu}}{\sigma\|\mathbf{w}\|_{2}}\), it's easy to see that the maximum is achieved if we choose \(\mathbf{w}=\mathbf{\mu}\), so we have \(\mathbf{w}^{\star}=\mathbf{\mu}\). 

**Lemma A.3**.: _Let \(\mathcal{D}_{1},\mathcal{D}_{2}\) to be defined as: \(\mathcal{D}_{1,Y}(Y=1)=\mathcal{D}_{1,Y}(Y=-1)=\mathcal{D}_{2,Y}(Y=-1)= \mathcal{D}_{2,Y}(Y=-1)\frac{1}{2},X\in\mathbb{R}^{d}\) and \(\mathcal{D}_{1,X|Y=y}=\mathcal{N}(y\mathbf{\mu}_{1},\sigma^{2}I),\mathcal{D}_{2,X |Y=y}=\mathcal{N}(y\mathbf{\mu}_{2},\sigma^{2}I)\) where \(\mathbf{\mu}_{1},\mathbf{\mu}_{2}\in\mathbb{R}^{d},\|\mathbf{\mu}_{1}\|_{2}=\|\mathbf{\mu}_{2 }\|_{2}\) and \(\sigma\in\mathbb{R}_{+}\), let \(\mathcal{D}=\frac{1}{2}\mathcal{D}_{1}+\frac{1}{2}\mathcal{D}_{2}\) and suppose \(\alpha\) is the angle between \(\mathbf{\mu}_{1}\) and \(\mathbf{\mu}_{2}\), if \(0<\alpha\leq 2\) arccos \(\frac{\epsilon}{\|\mathbf{\mu}_{1}\|_{2}}\), then \(\mathbf{w}^{\star}=\mathbf{\mu}_{1}+\mathbf{\mu}_{2}\) achieves the minimal robust risk._

Proof of Lemma a.3.: To simplify the notation, we denote \(\mathcal{R}_{\mathcal{D}}^{B_{2}^{*}}(\ell_{01},h_{\mathbf{w}})\) by \(\mathcal{R}(\mathbf{w})\). Then by Lemma A.2 we have:

\[\mathcal{R}(\mathbf{w})=\frac{1}{2}\mathcal{R}_{\mathcal{D}_{1}}^{B_{1}^{*}}(\ell_ {01},h_{\mathbf{w}})+\frac{1}{2}\mathcal{R}_{\mathcal{D}_{2}}^{B_{2}^{*}}(\ell_{0 1},h_{\mathbf{w}})=\frac{1}{2}\Phi\left(\frac{\epsilon\|\mathbf{w}\|_{2}-\mathbf{w}^{T}\bm {\mu}_{1}}{\sigma\|\mathbf{w}\|_{2}}\right)+\frac{1}{2}\Phi\left(\frac{\epsilon\| \mathbf{w}\|_{2}-\mathbf{w}^{T}\mathbf{\mu}_{2}}{\sigma\|\mathbf{w}\|_{2}}\right),\]

where \(\Phi(u)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{u}e^{-\frac{u^{2}}{\tau}}dy\). The proof is divided into two steps. In the first step, we prove that for any weight \(\mathbf{w}\), we have \(\mathcal{R}(\mathbf{\mu}_{P})\leq\mathcal{R}(\mathbf{\mu})\), where \(\mathbf{\mu}_{P}\) is the project of \(\mathbf{\mu}\) onto the space span \(\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\rangle\). Then we can reduce our analysis to the weights in the space span \(\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\rangle\).

**Step 1.** For any \(\mathbf{w}\in\mathbb{R}^{d}\), we take a direct sum decomposition as \(\mathbf{w}=\mathbf{w}_{s}+\mathbf{w}_{t}\) where \(\mathbf{w}_{s}\in\text{span}\left\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\right\rangle\) and \(\mathbf{w}_{t}\in\text{span}\left\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\right\rangle^{\perp}\), the orthogonal complement space of span \(\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\rangle\), then we have:

\[\mathbf{w}^{T}\mathbf{\mu}_{1}=\mathbf{w}_{s}^{T}\mathbf{\mu}_{1}+\mathbf{w}_{t}^{T}\mathbf{\mu}_{1}= \mathbf{w}_{s}^{T}\mathbf{\mu}_{1}\]

Similarly, we have \(\mathbf{w}^{T}\mathbf{\mu}_{2}=\mathbf{w}_{s}^{T}\mathbf{\mu}_{2}\). Since \(\mathbf{w}_{s}\perp\mathbf{w}_{t}\), then we have \(\|\mathbf{w}\|_{2}^{2}=\|\mathbf{w}_{s}\|_{2}^{2}+\|\mathbf{w}_{t}\|_{2}^{2}\), so for any \(\mathbf{w}\in\mathbb{R}^{d}\), we have:

\[\mathcal{R}(\mathbf{w}) =\frac{1}{2}\Phi\left(\frac{\epsilon}{\sigma}-\frac{\mathbf{w}^{T} \mathbf{\mu}_{1}}{\sigma\|\mathbf{w}\|_{2}}\right)+\frac{1}{2}\Phi\left(\frac{\epsilon }{\sigma}-\frac{\mathbf{w}^{T}\mathbf{\mu}_{2}}{\sigma\|\mathbf{w}\|_{2}}\right)\] \[=\frac{1}{2}\Phi\left(\frac{\epsilon}{\sigma}-\frac{\mathbf{w}_{s}^{T }\mathbf{\mu}_{1}}{\sigma\sqrt{\|\mathbf{w}_{s}\|_{2}^{2}+\|\mathbf{w}_{t}\|_{2}^{2}}} \right)+\frac{1}{2}\Phi\left(\frac{\epsilon}{\sigma}-\frac{\mathbf{w}_{s}^{T}\mathbf{ \mu}_{2}}{\sigma\sqrt{\|\mathbf{w}_{s}\|_{2}^{2}+\|\mathbf{w}_{t}\|_{2}^{2}}}\right)\]

So it is obvious that \(\mathcal{R}(\mathbf{w}_{s})\leq\mathcal{R}(\mathbf{w})\).

**Step 2.** According to the above results, we consider \(\mathbf{w}\in\text{span}\left\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\right\rangle\). Suppose the angle between \(\mathbf{\mu}_{1},\mathbf{\mu}_{2}\) is \(\alpha\), note that \(\mathcal{R}(\mathbf{w})\) does not depend on the length of \(\mathbf{w}\), so without loss of generality, we assume \(\|\mathbf{w}\|_{2}=1\) here. Suppose the angle between \(\mathbf{w}\) and \(\mathbf{\mu}_{1}\) is \(\theta\), then the angle between \(\mathbf{w}\) and \(\mathbf{\mu}_{2}\) is \(\alpha-\theta\), then \(\mathbf{w}\) is uniquely determined by \(\theta\), we denote \(\mathcal{R}(\theta)=\mathcal{R}(\mathbf{w})\), then we have:

\[\mathcal{R}(\theta)=\frac{1}{2}\Phi\left(\frac{\epsilon}{\sigma}-\frac{\|\mathbf{ \mu}_{1}\|_{2}\text{cos}\,\theta}{\sigma}\right)+\frac{1}{2}\Phi\left(\frac{ \epsilon}{\sigma}-\frac{\|\mathbf{\mu}_{1}\|_{2}\text{cos}(\alpha-\theta)}{\sigma}\right)\]

By the rule of derivation for composition function, we have:

\[\mathcal{R}^{\prime}(\theta) =\frac{1}{2\sqrt{2\pi}}\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{ \mu}_{1}\|_{2}\text{ cos}\,\theta)^{2}}{2\sigma^{2}}\right\}\frac{\|\mathbf{\mu}_{1}\|_{2}}{ \sigma}\text{sin}\,\theta\] \[\qquad\qquad-\frac{1}{2\sqrt{2\pi}}\text{exp}\left\{-\frac{( \epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}(\alpha-\theta))^{2}}{2\sigma^{2}}\right\}\frac{\|\mathbf{\mu}_{1}\|_{2}}{ \sigma}\text{sin}(\alpha-\theta)\] \[=\frac{\|\mathbf{\mu}_{1}\|_{2}}{2\sqrt{2\pi}\sigma}\left[\text{exp} \left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\theta)^{2}}{2\sigma^{2}}\right\}\text{sin}\,\theta\right.\] \[\qquad\qquad\qquad-\left.\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{ \mu}_{1}\|_{2}\text{ cos}(\alpha-\theta))^{2}}{2\sigma^{2}}\right\}\text{sin}( \alpha-\theta)\right].\]It is easy to see that when \(\theta=\frac{\alpha}{2}\) or \(\theta=\frac{\alpha}{2}+\pi\), \(\mathcal{R}^{\prime}(\theta)=0\), since:

\[\mathcal{R}^{\prime}(\frac{\alpha}{2}) =\frac{\|\mathbf{\mu}_{1}\|_{2}}{2\sqrt{2\pi}\sigma}\left[\text{exp} \left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{cos}\,\frac{\alpha}{2})^{2}}{ 2\sigma^{2}}\right\}\text{sin}\,\,\frac{\alpha}{2}-\text{exp}\left\{-\frac{( \epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{cos}\,\frac{\alpha}{2})^{2}}{2\sigma^{2}} \right\}\text{sin}\,\,\frac{\alpha}{2}\right]\] \[=0\]

Similarly we have \(\mathcal{R}^{\prime}(\frac{\alpha}{2}+\pi)=0\). Next we prove \(\mathcal{R}(\theta)\) achieves the minimum at \(\theta=\frac{\alpha}{2}\), taking the second order derivation of \(\theta\), we have:

\[\frac{2\sqrt{2\pi}\sigma}{\|\mathbf{\mu}_{1}\|_{2}}\mathcal{R}^{\prime \prime}(\theta)=\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\theta)^{2}}{2\sigma^{2}}\right\}\cdot\left(-\frac{\|\mathbf{\mu}_{1}\|_{2} \text{ }(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\theta)\text{sin}^{2}\theta}{\sigma^{2}} \right)\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad+\text{exp}\left\{-\frac{( \epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\theta)^{2}}{2\sigma^{2}}\right\}\text{cos}\,\,\theta\] \[\qquad\qquad\qquad\qquad\qquad\qquad\left.+\text{exp}\left\{- \frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}(\alpha-\theta))^{2}}{2\sigma^{2}}\right\}\cdot\left(\frac{\|\mathbf{\mu}_{1 }\|_{2}(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}(\alpha-\theta))\text{sin}^{2}( \alpha-\theta)}{\sigma^{2}}\right)\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\left.+\text{exp} \left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}(\alpha-\theta))^{2}}{2\sigma^{2}}\right\}\text{cos}( \alpha-\theta)\right.\] \[=\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\theta)^{2}}{2\sigma^{2}}\right\}\left(\text{cos}\,\,\theta-\frac{\|\mathbf{ \mu}_{1}\|_{2}\text{ }(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\theta)\text{sin}^{2}\theta}{\sigma^{2}}\right)\] \[+\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}(\alpha-\theta))^{2}}{2\sigma^{2}}\right\}\left(\text{cos}(\alpha- \theta)\text{-}\frac{\|\mathbf{\mu}_{1}\|_{2}(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,(\alpha-\theta))\text{sin}^{2}(\alpha-\theta)}{\sigma^{2}}\right)\]

Then we have:

\[\frac{2\sqrt{2\pi}\sigma}{\|\mathbf{\mu}_{1}\|_{2}}\mathcal{R}^{\prime \prime}(\frac{\alpha}{2}) =\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\frac{\alpha}{2})^{2}}{2\sigma^{2}}\right\}\left(\text{cos}\,\,\frac{ \alpha}{2}-\frac{\|\mathbf{\mu}_{1}\|_{2}\text{ }(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\frac{\alpha}{2})\text{sin}^{2}\frac{ \alpha}{2}}{\sigma^{2}}\right)\] \[+\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\frac{\alpha}{2})^{2}}{2\sigma^{2}}\right\}\left(\text{cos}\,\,\frac{ \alpha}{2}-\frac{\|\mathbf{\mu}_{1}\|_{2}(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\frac{\alpha}{2})\text{sin}^{2}\frac{\alpha}{2}}{\sigma^{2}}\right)\] \[=\frac{2}{\sigma^{2}}\text{exp}\left\{-\frac{(\epsilon-\|\mathbf{\mu} _{1}\|_{2}\text{ cos}\,\frac{\alpha}{2})^{2}}{2\sigma^{2}}\right\}\left(\sigma^{2} \text{cos}\frac{\alpha}{2}-\|\mathbf{\mu}_{1}\|_{2}(\epsilon-\|\mathbf{\mu}_{1}\|_{2} \text{ cos}\,\frac{\alpha}{2})\text{sin}^{2}\frac{\alpha}{2}\right)\]

Now, for simplicity, let \(k=\|\mathbf{\mu}_{1}\|_{2},x=\text{cos}\frac{\alpha}{2}\), then \(\text{sin}^{2}\frac{\alpha}{2}=1-x^{2}\), then we consider \(f(x)=\sigma^{2}x-k(\epsilon-kx)(1-x^{2})=\sigma^{2}\text{cos}\frac{\alpha}{2}- \|\mathbf{\mu}_{1}\|_{2}(\epsilon-\|\mathbf{\mu}_{1}\|_{2}\text{ cos}\,\frac{\alpha}{2})\text{sin}^{2}\frac{ \alpha}{2}\), where \(\alpha\in(0,\pi)\), so \(x\in(0,1)\). We have:

\[f(x) =\sigma^{2}x-k(\epsilon-kx)(1-x^{2})=\sigma^{2}x-k\epsilon+k^{2} x+k\epsilon x^{2}-k^{2}x^{3}\] \[=k^{2}(x-x^{3})-k\epsilon(1-x^{2})+\sigma^{2}x=k^{2}x(1-x^{2})-k \epsilon(1-x^{2})+\sigma^{2}x\] \[=k(1-x^{2})(kx-\epsilon)+\sigma^{2}x.\]

Since \(x\in(0,1)\), then \(\sigma^{2}x>0\) and \(k(1-x^{2})>0\), by the assumption, \(0<\alpha\leq 2\)\(\text{arccos}\frac{\epsilon}{\|\mathbf{\mu}_{1}\|_{2}}=2\)\(\text{arccos}\frac{\epsilon}{\text{k}}\), so \(x=\text{cos}\frac{\alpha}{2}>\frac{\epsilon}{\epsilon}\), so \(kx-\epsilon\geq 0\), so we have \(f(x)>0\), which means that \(\mathcal{R}^{\prime\prime}(\frac{\alpha}{2})>0\), so we know that \(\theta=\frac{\alpha}{2}\) achieves the minimum of \(\mathcal{R}(\theta)\), which means that the direction \(\mathbf{w}^{*}=\mathbf{\mu}_{1}+\mathbf{\mu}_{2}\) achieves the minimum of \(\mathcal{R}(\mathbf{w})\). Similarly we can show that \(\mathcal{R}(\theta)\) achieves its maximum at \(\theta=\frac{\alpha}{2}+\pi\), which means that the direction \(-(\mathbf{\mu}_{1}+\mathbf{\mu}_{2})\) achieves the maximum of \(\mathcal{R}(\mathbf{w})\) in span \(\langle\mathbf{\mu}_{1},\mathbf{\mu}_{2}\rangle\).

**Theorem 3.4**.: _Consider the setting in Example 1 and suppose that \(\mathbf{\mu}\) lies in the 2-dimensional subspace \(\mathfrak{R}\) in Theorem A.4 (see Appendix A.1 for details about \(\mathfrak{R}\) and see Figure 1 for intuitive illustration). Let \(\ell_{01}(x,y)=\mathbb{1}[x\neq y]\), where \(\mathbb{1}[\cdot]\) is the indicator function and \(B_{r}^{r}(\mathbf{x})=\{\mathbf{x}^{\prime}:\|\mathbf{x}-\mathbf{x}\prime\|_{p}\leq r\}\), consider training with hypothesis class \(\mathcal{H}=\{h_{\mathbf{w}}:h_{\mathbf{w}}(x)=\text{sign}(\mathbf{w}^{T}x),\mathbf{w}\in \mathbb{R}^{d}\}\), and denote \(\mathcal{D}^{(ij)}=\frac{1}{2}\mathcal{D}^{(i)}+\frac{1}{2}\mathcal{D}^{(j)},i, j\in\{0,1,2\},i\neq j\). Consider the \(\ell_{2}\)-norm adversarial attack with radius \(\epsilon\), for notation convenience, let \(\widetilde{\mathcal{R}}_{ij}(\mathbf{w})=\mathcal{R}_{\mathcal{D}^{(ij)}}^{B_{2}^{ *}}(\ell_{01},h_{\mathbf{w}})\) and \(\widetilde{\mathcal{R}}_{i}(\mathbf{w})=\mathcal{R}_{\mathcal{D}^{(i)}}^{B_{2}^{ *}}(\ell_{01},h_{\mathbf{w}})\), let \(\Phi(\cdot)\) be the distribution function of the standard normal distribution, let \(\alpha\) denote the angle between \(\mathbf{\mu}\) and \(Q\mathbf{\mu}\), which is the rotation angle of \(Q\) in subspace \(\mathfrak{R}\), suppose that \(0<\alpha\leq\arccos\frac{\epsilon}{\|\mathbf{\mu}_{1}\|_{2}}\), then we have:_1. _If we train with_ \(\mathcal{D}^{(0)}\) _and_ \(\mathcal{D}^{(1)}\) _under_ \(\mathcal{H}\)_, let_ \(\mathbf{w}_{(01)}=\mathbf{\mu}+\underline{Q\mathbf{\mu}}\)_, which achieves the minimum of_ \(\widetilde{\mathcal{R}}_{01}(\mathbf{w})\)_, then the robust accuracy of_ \(\mathbf{w}_{(01)}\) _on the test distribution_ \(\mathcal{D}^{(2)}\) _is:_ \[\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)})=\Phi\left(\frac{\epsilon}{\sigma}- \frac{(\mathbf{\mu}+\underline{Q\mathbf{\mu}})^{T}\underline{Q^{2}\mathbf{\mu}}}{\sigma\| \mathbf{\mu}+\underline{Q\mathbf{\mu}}\|_{2}}\right).\]
2. _If we train with_ \(\mathcal{D}^{(0)}\) _and_ \(\mathcal{D}^{(2)}\) _under_ \(\mathcal{H}\)_, let_ \(\mathbf{w}_{(02)}=\mathbf{\mu}+\underline{Q^{2}\mathbf{\mu}}\)_, which achieves the minimum of_ \(\widetilde{\mathcal{R}}_{02}(\mathbf{w})\)_, then the robust accuracy of_ \(\mathbf{w}_{(02)}\) _on the test distribution_ \(\mathcal{D}^{(1)}\) _is:_ \[\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})=\Phi\left(\frac{\epsilon}{\sigma}- \frac{(\mathbf{\mu}+\underline{Q^{2}\mathbf{\mu}})^{T}\underline{Q\mathbf{\mu}}}{\sigma\| \underline{Q\mathbf{\mu}}+\underline{Q^{2}\mathbf{\mu}}\|_{2}}\right).\]
3. _If we train with_ \(\mathcal{D}^{(1)}\) _and_ \(\mathcal{D}^{(2)}\) _under_ \(\mathcal{H}\)_, then_ \(\mathbf{w}_{(12)}=\underline{Q\mathbf{\mu}}+\underline{Q^{2}\mathbf{\mu}}\)_, which achieves the minimum of_ \(\widetilde{\mathcal{R}}_{12}(\mathbf{w})\)_, then the robust accuracy of_ \(\mathbf{w}_{(12)}\) _on the test distribution_ \(\mathcal{D}^{(0)}\) _is:_ \[\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)})=\Phi\left(\frac{\epsilon}{\sigma}- \frac{(\underline{Q\mathbf{\mu}}+\underline{Q^{2}\mathbf{\mu}})^{T}\underline{\mathbf{\mu} }}{\sigma\|\underline{Q\mathbf{\mu}}+\underline{Q^{2}\mathbf{\mu}}\|_{2}}\right).\]

_What's more, we have \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})<\widetilde{\mathcal{R}}_{2}(\mathbf{w}_ {(01)})=\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)})\)._

Proof of Theorem 3.4.: The values of \(\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)}),\widetilde{\mathcal{R}}_{1}(\mathbf{w} _{(02)}),\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)})\) is a direct result of Lemma A.2 and Lemma A.3. Next we compare the three adversarial risks.

Let \(C_{(01)}=\frac{(\mathbf{\mu}+\underline{Q\mathbf{\mu}})^{T}\underline{Q^{2}\mathbf{\mu}}}{ \sigma\|\mathbf{\mu}+\underline{Q\mathbf{\mu}}\|_{2}},C_{(02)}=\frac{(\mathbf{\mu}+ \underline{Q^{2}\mathbf{\mu}})^{T}\underline{Q\mathbf{\mu}}}{\sigma\|\underline{Q\mathbf{ \mu}}+\underline{Q^{2}\mathbf{\mu}}\|_{2}},C_{(12)}=\frac{(\underline{Q\mathbf{\mu}}+ \underline{Q^{2}\mathbf{\mu}})^{T}\underline{\mathbf{\mu}}}{\sigma\|\underline{Q\mathbf{ \mu}}+\underline{Q^{2}\mathbf{\mu}}\|_{2}}\), i.e., the subtractor in \(\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)}),\widetilde{\mathcal{R}}_{1}(\mathbf{w} _{(02)}),\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)})\) respectively. Since \(\Phi(u)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{u}e^{-\frac{u^{2}}{2}}du\) is monotonely increasing, to judge which of the three robust errors in Theorem 3.4 is the smallest, we need only to judge which of the corresponding \(C_{(ij)}\) is the largest.

Firstly, let \(Q_{e}\) be the matrix of \(\underline{Q}\) under the orthonormal basis \(\mathbf{e}_{1},\cdots,\mathbf{e}_{d}\) where \((\mathbf{e}_{i})_{j}=\delta_{ij}\) and \(\delta_{ij}=1\) if \(i=j\), \(\delta_{ij}=0\) otherwise. Then we have \(Q\mathbf{\mu}=Q_{e}\mathbf{\mu}\) and we can see that \(C_{(12)}=\frac{(Q_{e}\mathbf{\mu}+\underline{Q^{2}\mathbf{\mu}})^{T}\underline{\mathbf{\mu} }}{\sigma\|Q_{e}\mathbf{\mu}+\underline{Q^{2}\mathbf{\mu}}\|_{2}}=\frac{\mathbf{\mu}^{T} Q_{e}^{T}\underline{\mathbf{\mu}}+\mathbf{\mu}^{T}Q_{e}^{T}\underline{\mathbf{\mu}}}{ \sigma\|\underline{Q\mathbf{\mu}}+\underline{Q^{2}\mathbf{\mu}}\|_{2}}=\frac{\mathbf{\mu} ^{T}Q_{e}\mathbf{\mu}+\mathbf{\mu}^{T}Q_{e}^{T}\underline{\mathbf{\mu}}}{\sigma\|\mathbf{\mu} +Q_{e}\mathbf{\mu}\|_{2}}=\frac{\mathbf{\mu}^{T}Q_{e}^{T}Q_{e}\mathbf{\mu}+\mathbf{\mu}^{T}Q_{e }^{T}\underline{\mathbf{\mu}}}{\sigma\|\mathbf{\mu}+Q_{e}\mathbf{\mu}\|_{2}}=\frac{(\mathbf{ \mu}+Q_{e}\mathbf{\mu})^{T}Q_{e}^{T}\underline{\mathbf{\mu}}}{\sigma\|\mathbf{\mu}+Q_{e} \mathbf{\mu}\|_{2}}=C_{(01)}\) (where \(a\) is from the fact that \(\|Q_{e}\mathbf{\mu}\|_{2}=\|\mathbf{x}\|_{2},\forall\mathbf{x}\) and \(\mathbf{x}^{T}A\mathbf{x}=\mathbf{x}^{T}A^{T}\mathbf{x},\forall\mathbf{x},A\)). So \(\widetilde{\mathcal{R}}_{2}(w_{(01)})=\widetilde{\mathcal{R}}_{0}(w_{(12)})\).

To compare which one of \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})\) and \(\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)})\) is smaller, we note that \(\mathbf{\mu}\) lies in the \(2\)-dimensional subspace \(\mathfrak{R}\in\mathbb{R}^{d}\) in Theorem A.4 as specified in Theorem 3.4, so there exists a scalar \(k\in\mathbb{R}\) such that \(\mathbf{\mu}+\underline{Q^{2}\mathbf{\mu}}=k\)\(\underline{Q\mathbf{\mu}}\), and we know that \(\underline{Q\mathbf{\mu}}\in\operatorname*{arg\,min}_{\mathbf{w}:\mathbf{h}_{\mathbf{\mu}}\in \mathcal{H}}\widetilde{\mathcal{R}}_{02}(\mathbf{w})\) (by Lemma A.2), so we know that \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})\leq\widetilde{\mathcal{R}}_{2}(\mathbf{w} _{(01)})=\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)})\) since \(\inf_{\mathbf{w}:\mathbf{h}_{\mathbf{\mu}}\in\mathcal{H}}\widetilde{\mathcal{R}}_{0}(\mathbf{w} )=\inf_{\mathbf{w}:\mathbf{h}_{\mathbf{\mu}}\in\mathcal{H}}\widetilde{\mathcal{R}}_{1}( \mathbf{w})=\inf_{\mathbf{w}:\mathbf{h}_{\mathbf{\mu}}\in\mathcal{H}}\widetilde{\mathcal{R}}_{ 2}(\mathbf{w})\) (which is obvious according to Lemma A.2). And following the same direction of the proof of Theorem A.4, it's easy to see that \(\mathbf{w}_{(01)}=\mathbf{\mu}+\underline{Q\mathbf{\mu}}\) is not in the same direction of \(\underline{Q^{2}\mathbf{\mu}}\) (the weight attains smallest robust error on \(\mathcal{D}^{(2)}\)), so \(\widetilde{\mathcal{R}}_{2}(\mathbf{w}_{(01)})>\inf_{\mathbf{w}:\mathbf{h}_{\mathbf{\mu}}\in \mathcal{H}}\widetilde{\mathcal{R}}_{2}(\mathbf{w})\), so we have \(\widetilde{\mathcal{R}}_{1}(\mathbf{w}_{(02)})<\widetilde{\mathcal{R}}_{2}(\mathbf{w} _{(01)})=\widetilde{\mathcal{R}}_{0}(\mathbf{w}_{(12)})\). 

**Theorem A.4**.: _Let \(\underline{Q}\) be a rotation transformation on \(\mathbb{R}^{d}\) and \(\underline{Q}\neq\underline{I}\), then there exists at least one \(2\)-dimensional subspace \(\mathfrak{R}\in\mathbb{R}^{d}\) such that there exists a scalar \(k\) s.t. \(\mathbf{r}+\underline{Q^{2}\mathbf{r}}=k\mathbf{r}\) for any \(\mathbf{r}\in\mathfrak{R}\)._

Proof of Theorem a.4.: By the results of matrix theory, we know that since \(\underline{Q}\) is an orthonormal transformation, then there exists an orthonormal basis \(\mathbf{\eta}_{1},\ldots\mathbf{\eta}_{d}\) under which the corresponding matrix of \(\underline{Q}\), \(Q\), has the form:

\[Q=\text{diag}\left\{R_{1},\ldots,R_{m},\lambda_{1},\ldots,\lambda_{r}\right\},\]where \(\lambda_{i}=1\) or \(-1\), \(i=1,2,\cdots,r,0\leq r\leq d\); \(R_{j}=\begin{bmatrix}\cos\theta_{j}&-\sin\theta_{j}\\ \sin\theta_{j}&\cos\theta_{j}\end{bmatrix}\), \(0<\theta_{j}<\pi,j=1,2,\cdots,m,0\leq m\leq\frac{d}{2}\).

Since \(Q\) is a rotation transformation, \(Q\) is a rotation matrix, so \(\lambda_{1}=\cdots=\lambda_{r}=1\). Once given the basis, linear transformations on \(\mathbb{R}^{d}\) and matrices in \(\mathbb{R}^{d\times d}\) has a one-to-one correspondence, so \(Q\neq\underline{I}\) tells us that \(Q\neq I\), where \(I\) is the matrix of \(\underline{I}\) under the orthonormal basis \(\boldsymbol{\eta}_{1},\ldots\boldsymbol{\eta}_{d}\). So we have \(m\geq 1\) in our case, without loss of generality we consider the subspace \(\mathfrak{R}=\text{span}\left\langle\boldsymbol{\eta}_{1},\boldsymbol{\eta}_{ 2}\right\rangle\) be the subspace spanned by the vectors \(\left\{\boldsymbol{\eta}_{1},\boldsymbol{\eta}_{2}\right\}\).

For any \(\boldsymbol{\alpha}\in\mathfrak{R}\), the coordinates of \(\boldsymbol{\alpha}\) under the basis \(\boldsymbol{\eta}_{1},\ldots\boldsymbol{\eta}_{d}\) is then \([\alpha_{1},\alpha_{2},0,\cdots,0]^{T}\) and the coordinates of \(Q\boldsymbol{\alpha},Q^{2}\boldsymbol{\alpha}\) is \(Q[\alpha_{1},\alpha_{2},0,\cdots,0]^{T},Q^{2}[\alpha_{1},\alpha_{2},0,\cdots, 0]^{T}\) respectively. We now prove that there exists \(k=2\text{cos}\ \theta_{1}\) such that \([\alpha_{1},\alpha_{2},0,\cdots,0]^{T}+Q^{2}[\alpha_{1},\alpha_{2},0,\cdots,0] ^{T}=kQ[\alpha_{1},\alpha_{2},0,\cdots,0]^{T}\). We have:

\[Q\begin{bmatrix}\alpha_{1}\\ \alpha_{2}\\ 0\\ \vdots\\ 0\end{bmatrix}=\begin{bmatrix}R_{1}^{2}\begin{pmatrix}\alpha_{1}\\ \alpha_{2}\\ 0\\ \vdots\\ 0\end{pmatrix}\\ \vdots\\ 0\end{bmatrix}=\begin{bmatrix}\alpha_{1}\text{ cos}\ \theta_{1}-\alpha_{2}\text{ sin}\ \theta_{1}\\ \alpha_{1}\text{ sin}\ \theta_{1}+\alpha_{2}\text{ cos}\ \theta_{1}\\ 0\\ \vdots\\ 0\end{bmatrix},\text{and}\]

\[Q^{2}\begin{bmatrix}\alpha_{1}\\ \alpha_{2}\\ 0\\ \vdots\\ 0\end{bmatrix}=\begin{bmatrix}R_{1}^{2}\begin{pmatrix}\alpha_{1}\\ \alpha_{2}\\ 0\\ \vdots\\ 0\end{pmatrix}\\ \vdots\\ 0\end{bmatrix}=\begin{bmatrix}\alpha_{1}\text{ cos}\ 2\theta_{1}-\alpha_{2}\text{ sin}\ 2\theta_{1}\\ \alpha_{1}\text{ sin}\ 2\theta_{1}+\alpha_{2}\text{ cos}\ 2\theta_{1}\\ 0\\ \vdots\\ 0\end{bmatrix}.\]

So the coordinates of \(\boldsymbol{\alpha}+\underline{Q}^{2}\boldsymbol{\alpha}\) is:

\[\begin{bmatrix}\alpha_{1}\\ \alpha_{2}\\ 0\\ \vdots\\ 0\end{bmatrix}+Q^{2}\begin{bmatrix}\alpha_{1}\\ \alpha_{2}\\ 0\\ \vdots\\ 0\end{bmatrix}=\begin{bmatrix}\alpha_{1}\text{ cos}\ 2\theta_{1}+1-\alpha_{2}\text{ sin}\ 2\theta_{1}\\ \alpha_{1}\text{ sin}\ 2\theta_{1}+\alpha_{2}\text{ cos}\ 2\theta_{1}+1\\ 0\\ \vdots\\ 0\end{bmatrix}=\begin{bmatrix}2\alpha_{1}\text{ cos}^{2}\ \theta_{1}-2\alpha_{2}\text{ sin}\ \theta_{1}\text{ cos}\ \theta_{1}\\ 2\alpha_{2}\text{ sin}\ \theta_{1}\text{ cos}\ \theta_{1}+2\alpha_{1}\text{ cos}^{2}\ \theta_{1}\\ 0\\ \vdots\\ 0\end{bmatrix}\] \[=2\text{cos}\ \theta_{1}\begin{bmatrix}\alpha_{1}\text{ cos}\ \theta_{1}-\alpha_{2}\text{ sin}\ \theta_{1}\\ \alpha_{1}\text{ sin}\ \theta_{1}+\alpha_{2}\text{ cos}\ \theta_{1}\\ 0\\ \vdots\\ 0\end{bmatrix}=k\begin{bmatrix}\alpha_{1}\text{ cos}\ \theta_{1}-\alpha_{2}\text{ sin}\ \theta_{1}\\ \alpha_{1}\text{ sin}\ \theta_{1}+\alpha_{2}\text{ cos}\ \theta_{1}\\ 0\\ \vdots\\ 0\end{bmatrix},\]

which is just \(k\) times of the coordinates of \(\underline{Q}\boldsymbol{\alpha}\). So we get \(\boldsymbol{\alpha}+\underline{Q}^{2}\boldsymbol{\alpha}=k\underline{Q} \boldsymbol{\alpha}\) for any \(\boldsymbol{\alpha}\in\mathfrak{R}\). 

### Proofs of the Average Case

**Theorem 3.1**.: _Suppose the loss function is bounded, i.e., \(\ell\in[0,U]\), then with probability at least \(1-\delta\) over the sampling of \(\widehat{\mathcal{D}}_{1},\cdots,\widehat{\mathcal{D}}_{t}\), for all \(h\in\mathcal{H}\), we have:_

\[\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)\leq\mathcal{L}_{\widehat{\mathcal{D}}}^{ \mathcal{B}}(\ell,h)+2\mathfrak{R}_{t}(\widetilde{\mathcal{G}})+2\mathfrak{R}_{tn}( \widetilde{\mathcal{G}})+3U\sqrt{\frac{\ln 4/\delta}{2t}}+3U\sqrt{\frac{\ln 4/\delta}{2tn}},\]

_where_

\[\widetilde{\mathcal{G}}=\left\{g_{h}:\mathcal{X}\times\mathcal{Y}\to \mathbb{R}_{+}\Big{|}g_{h}(\boldsymbol{x},y)=\sup_{\boldsymbol{x}^{\prime} \in\mathcal{B}(\boldsymbol{x})}\ell(h(\boldsymbol{x}^{\prime},y)),h\in \mathcal{H}\right\}.\]

Proof of Theorem 3.1.: Our proof consists of 2 steps, our 2-step uniform convergence analysis is different from (but based on) the standard Rademacher complexity generalization error bounds.

**Step 1, upper bound the difference between \(\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)\) and \(\mathcal{L}_{\widehat{p}}^{\mathcal{B}}(\ell,h)\) for any \(h\in\mathcal{H}\).**Let \(\mathfrak{D}=\{\mathcal{D}_{1},\cdots,\mathcal{D}_{t}\}\), we define:

\[\Phi^{\mathcal{B}}(\mathfrak{D})=\sup_{h\in\mathcal{H}}\left[\mathcal{L}^{ \mathcal{B}}_{p}(\ell,h)-\mathcal{L}^{\mathcal{B}}_{p}(\ell,h)\right].\]

Since \(\ell\in[0,U]\), changing one of the \(\mathcal{D}_{i}\) in \(\mathfrak{D}\) will lead to at most \(\frac{U}{t}\) change in \(\Phi^{\mathcal{B}}(\mathfrak{D})\). So by McDiarmid's inequality we have:

\[\mathbb{P}\left[\Phi^{\mathcal{B}}(\mathfrak{D})-\mathbb{E}(\Phi^{\mathcal{B}} (\mathfrak{D}))\right]\leq\exp\left\{-\frac{2t\epsilon^{2}}{U^{2}}\right\},\]

so with probability at least \(1-\frac{\delta}{4}\) over the choice of \(\mathfrak{D}\), we have:

\[\Phi^{\mathcal{B}}(\mathfrak{D})\leq\mathbb{E}(\Phi^{\mathcal{B}}(\mathfrak{D }))+U\sqrt{\frac{\ln 4/\delta}{2t}}.\] (A.1)

Then we give an upper bound for \(\mathbb{E}(\Phi^{\mathcal{B}}(\mathfrak{D}))\), by the **symmetric technique**, let \(\mathfrak{S}=\{\mathcal{S}_{1},\cdots,\mathcal{S}_{t}\}\) be a set of independent copy of \(\mathfrak{D}\), we have:

\[\mathbb{E}(\Phi^{\mathcal{B}}(\mathfrak{D})) =\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}}\left[\sup_{h\in \mathcal{H}}\left(\mathcal{L}^{\mathcal{B}}_{p}(\ell,h)-\mathcal{L}^{\mathcal{ B}}_{p}(\ell,h)\right)\right]\] \[=\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}}\left[\sup_{h\in \mathcal{H}}\left(\mathop{\mathbb{E}}_{\mathfrak{S}\sim p}\mathcal{R}^{ \mathcal{B}}_{\mathcal{S}}(\ell,h)-\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}^{ \mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)\right)\right]\] \[=\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}}\left[\mathop{ \mathbb{E}}_{\mathfrak{S}\sim p^{t}}\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i=1 }^{t}\left(\mathcal{R}^{\mathcal{B}}_{\mathcal{S}_{i}}(\ell,h)-\mathcal{R}^{ \mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)\right)\right]\] \[\stackrel{{ b}}{{=}}\mathop{\mathbb{E}}_{\mathfrak{D }\sim p^{t},\mathfrak{S}\sim p^{t}}\mathbb{E}\left[\sup_{h\in\mathcal{H}}\frac {1}{t}\sum_{i=1}^{t}\sigma_{i}\left(\mathcal{R}^{\mathcal{B}}_{\mathcal{S}_{i} }(\ell,h)-\mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)\right)\right]\] \[\stackrel{{ c}}{{\leq}}2\mathop{\mathbb{E}}_{ \mathfrak{D}\sim p^{t}}\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}}\left[\sup_ {h\in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{t}\sigma_{i}\mathcal{R}^{\mathcal{B}}_ {\mathcal{D}_{i}}(\ell,h)\right]\] \[=2\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}}\mathop{\mathbb{E} }_{\mathfrak{D}\sim p^{t}}\left[\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{ t}\sigma_{i}\mathop{\mathbb{E}}_{(\mathbf{x}_{i},y_{i})\sim\mathcal{D}_{i}}\left[\sup_{ \mathbf{x}_{i}^{\prime}\in\mathcal{B}(\mathbf{x}_{i})}\ell(h(\mathbf{x}_{i}^{\prime}),y_{ i})\right]\right]\] \[\stackrel{{ d}}{{\leq}}2\mathop{\mathbb{E}}_{ \mathfrak{D}\sim p^{t}}\left(\mathbf{x}_{i},y_{i}\right)\sim\mathcal{D}_{i}}\mathop{ \mathbb{E}}_{\mathfrak{E}}\left[\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{ t}\sigma_{i}\left[\sup_{\mathbf{x}_{i}^{\prime}\in\mathcal{B}(\mathbf{x}_{i})}\ell(h(\mathbf{x}_{i}^{ \prime}),y_{i})\right]\right]\] \[=2\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}}\mathop{\mathbb{E }}_{\mathfrak{D}\sim p_{i}}\left[\mathfrak{R}_{t}(\widetilde{\mathcal{G}}) \right],\]

where: \(a\) uses Jensen's inequality; \(b\) comes from the fact that the random variables \(\sigma_{i}\left(\mathcal{R}^{\mathcal{B}}_{\mathcal{S}_{i}}(\ell,h)-\mathcal{R} ^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)\right)\) and \(\mathcal{R}^{\mathcal{B}}_{\mathcal{S}_{i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_ {\mathcal{D}_{i}}(\ell,h)\) are identically distributed, where \(\sigma_{i}\sim\text{Uniform}(\{\pm 1\})\) and \(\mathbf{\sigma}\in\{\pm 1\}^{t}\) has independent elements \(\sigma_{1},\cdots,\sigma_{t}\); \(c\) is from the property of \(\sup\) that \(\sup(a+b)\leq\sup(a)+\sup(b)\) and the fact that \(\mathbf{\sigma}\) has the same distribution as \(-\mathbf{\sigma}\); \(d\) uses Jensen's inequality.

Since \(\ell\in[0,U]\), change in one data point \((\mathbf{x}_{i},y_{i})\) will cause at most \(\frac{U}{t}\) difference in \(\mathfrak{R}_{t}(\widetilde{\mathcal{G}})\), we then use McDiarmid's inequality to get: with probability at least \(1-\frac{\delta}{4}\),

\[\mathop{\mathbb{E}}_{\mathfrak{D}\sim p^{t}(\mathbf{x}_{i},y_{i})\sim\mathcal{D}_{ i}}\left[\mathfrak{R}_{t}(\widetilde{\mathcal{G}})\right]\leq\mathfrak{R}_{t}( \widetilde{\mathcal{G}})+U\sqrt{\frac{\ln 4/\delta}{2t}}.\] (A.2)

Combine (A.1) with (A.2) we get: with probability at least \(1-\frac{\delta}{2}\),

\[\Phi^{\mathcal{B}}(\mathfrak{D})\leq 29\mathfrak{R}_{t}(\widetilde{\mathcal{G}})+3U \sqrt{\frac{\ln 4/\delta}{2t}}.\] (A.3)

**Step 2, upper bound the difference between \(\mathcal{L}^{\mathcal{B}}_{\tilde{p}}(\ell,h)\) and \(\mathcal{L}^{\mathcal{B}}_{\tilde{\mathcal{D}}}(\ell,h)\) for any \(h\in\mathcal{H}\).**

\[\mathcal{L}^{\mathcal{B}}_{\tilde{p}}(\ell,h)-\mathcal{L}^{\mathcal{B}}_{\widehat {\mathcal{D}}}(\ell,h)=\frac{1}{t}\sum_{i=1}^{t}\left(\mathcal{R}^{\mathcal{B }}_{\mathcal{D}_{i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_ {i}}(\ell,h)\right).\]

Let \(\widehat{\mathfrak{D}}=\widehat{\mathcal{D}}_{i}\cup\cdots\cup\widehat{ \mathcal{D}}_{t}\) and then define:

\[\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})=\sup_{h\in\mathcal{H}}\left( \mathcal{L}^{\mathcal{B}}_{\tilde{p}}(\ell,h)-\mathcal{L}^{\mathcal{B}}_{ \widehat{\mathcal{D}}}(\ell,h)\right)=\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_ {i=1}^{t}\left(\mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)-\mathcal{R }^{\mathcal{B}}_{\widehat{\mathcal{D}}_{i}}(\ell,h)\right).\]

Since \(\ell\in[0,U]\) and there are \(tn\) examples in \(\widehat{\mathfrak{D}}\), changing the \(j\)-th example of \(\widehat{\mathcal{D}}_{i}\), \((\boldsymbol{x}_{ij},y_{ij})\), will cause at most \(\frac{U}{tn}\) change \(\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\). So by McDiarmid's inequality we know: with probability at least \(1-\frac{\delta}{4}\) over the choice of \(\widehat{\mathfrak{D}}\),

\[\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\leq\mathbb{E}\left[\Psi^{\mathcal{ B}}(\widehat{\mathfrak{D}})\right]+U\sqrt{\frac{\text{ln}4/\delta}{2tn}}.\] (A.4)

Next we upper bound \(\mathbb{E}\left[\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\right]\), by the symmetric technique, let \(\widehat{\mathfrak{S}}=\{\widehat{\mathcal{S}}_{1},\cdots,\widehat{\mathcal{ S}}_{t}\}\) be a set of independent copies of \(\widehat{\mathfrak{D}}\) and let \((\tilde{\boldsymbol{x}}_{ij},\tilde{y}_{ij})\) be the \(j\)-th example in \(\widehat{\mathcal{S}}_{i}\), we have:

\[\mathbb{E}\left[\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\right] =\mathbb{E}\left[\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{t} \left(\mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)-\mathcal{R}^{ \mathcal{B}}_{\widehat{\mathcal{D}}_{i}}(\ell,h)\right)\right]\] \[=\mathbb{E}\underset{\widehat{\mathfrak{D}}}{\sup}\left\{\sup_{h \in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{t}\frac{1}{n}\Biggl{[}\underset{(\tilde{ \boldsymbol{x}}_{ij},\tilde{y}_{ij})\sim\widehat{\mathcal{S}}_{i}}{\left(\sup_ {\tilde{\boldsymbol{x}}_{ij}\in\mathcal{B}(\tilde{\boldsymbol{x}}_{ij})}\ell (h(\tilde{\boldsymbol{x}}_{ij}^{\prime}),\tilde{y}_{ij})-\sup_{\boldsymbol{ x}_{ij}\in\mathcal{B}(\boldsymbol{x}_{ij})}\ell(h(\boldsymbol{x}_{ij}^{\prime}),y_{ij}) \right)\Biggr{]}\right\}\] \[\overset{a}{\leq}\underset{\widehat{\mathfrak{D}},\widehat{ \mathfrak{D}}}{\sup}\left\{\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{t} \frac{1}{n}\sum_{j=1}^{n}\left[\underset{\boldsymbol{x}_{ij}^{\prime}\in \mathcal{B}(\tilde{\boldsymbol{x}}_{ij})}{\sup}\ell(h(\tilde{\boldsymbol{x}}_ {ij}^{\prime}),\tilde{y}_{ij})-\sup_{\boldsymbol{x}_{ij}^{\prime}\in\mathcal{ B}(\boldsymbol{x}_{ij})}\ell(h(\boldsymbol{x}_{ij}^{\prime}),y_{ij}) \right]\right\}\] \[\overset{b}{\leq}\underset{\widehat{\mathfrak{D}},\widehat{ \mathfrak{D}}}{\sup}\mathbb{E}\left\{\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i= 1}^{t}\frac{1}{n}\sum_{j=1}^{n}\left[\underset{(\tilde{\boldsymbol{x}}_{ij}^{ \prime}\in\mathcal{B}(\tilde{\boldsymbol{x}}_{ij})}{\sup}\ell(h(\tilde{ \boldsymbol{x}}_{ij}^{\prime}),\tilde{y}_{ij})-\sup_{\boldsymbol{x}_{ij}^{ \prime}\in\mathcal{B}(\boldsymbol{x}_{ij})}\ell(h(\boldsymbol{x}_{ij}^{\prime} ),y_{ij})\right)\right]\right\}\] \[\overset{c}{\leq}2\underset{\widehat{\mathfrak{D}},\widehat{ \mathfrak{D}}}{\mathbb{E}}\left\{\sup_{h\in\mathcal{H}}\frac{1}{t}\sum_{i=1}^{ t}\frac{1}{n}\sum_{j=1}^{n}\left[\underset{\boldsymbol{x}_{ij}^{\prime}\in\mathcal{B}( \tilde{\boldsymbol{x}}_{ij})}{\sup}\ell(h(\boldsymbol{x}_{ij}^{\prime}),y_{ ij})\right]\right\}\] \[\overset{d}{\leq}2\underset{\widehat{\mathfrak{D}}}{\mathbb{E}} \left[\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})\right],\]

where: \(a\) is from Jensen's inequality; \(b\) is from the fact that \(\Sigma_{ij}\left(\sup_{\tilde{\boldsymbol{x}}_{ij}^{\prime}\in\mathcal{B}( \tilde{\boldsymbol{x}}_{ij})}\ell(h(\tilde{\boldsymbol{x}}_{ij}^{\prime}), \tilde{y}_{ij})-\underset{\boldsymbol{x}_{ij}^{\prime}\in\mathcal{B}( \boldsymbol{x}_{ij})}{\sup}\ell(h(\boldsymbol{x}_{ij}^{\prime}),y_{ij})\right)\) has the same distribution as \(\underset{\tilde{\boldsymbol{x}}_{ij}\in\mathcal{B}(\tilde{\boldsymbol{x}}_{ij })}{\sup}\ell(h(\tilde{\boldsymbol{x}}_{ij}^{\prime}),\tilde{y}_{ij})-\sup_ {\boldsymbol{x}_{ij}^{\prime}\in\mathcal{B}(\boldsymbol{x}_{ij})}\ell(h( \boldsymbol{x}_{ij}^{\prime}),y_{ij})\), where \(\Sigma_{ij}\sim\text{Uniform}(\{\pm 1\})\) and \(\boldsymbol{\Sigma}\) has independent entries; \(c\) is from the property of \(\sup\) that \(\sup(a+b)\leq\sup(a)+\sup(b)\) and the fact that \(\boldsymbol{\Sigma}\) has the same distribution as \(-\boldsymbol{\Sigma}\).

Similar as before, if we change one of the \(\{(\boldsymbol{x}_{ij},y_{ij})\}_{i,j=1}^{n}\), the change of \(\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})\) is at most \(\frac{U}{tn}\), so by McDiarmid's inequality, with probability at least \(1-\frac{\delta}{4}\), we have:

\[\underset{\widehat{\mathfrak{D}}}{\mathbb{E}}\left[\mathfrak{R}_{tn}( \widetilde{\mathcal{G}})\right]\leq\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+U \sqrt{\frac{\text{ln}4/\delta}{2tn}}.\] (A.5)Combine (A.4) and (A.5) we have: with probability at least \(1-\frac{\delta}{2}\),

\[\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\leq 2\mathfrak{R}_{tn}(\widetilde{ \mathcal{G}})+3U\sqrt{\frac{\ln\!4/\delta}{2tn}}.\] (A.6)

Combine (A.3) and (A.6) we have: with probability at least \(1-\delta\):

\[\Phi^{\mathcal{B}}(\mathfrak{D})+\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\leq 2 \mathfrak{R}_{t}(\widetilde{\mathcal{G}})+2\mathfrak{R}_{tn}(\widetilde{ \mathcal{G}})+3U\sqrt{\frac{\ln\!4/\delta}{2t}}+3U\sqrt{\frac{\ln\!4/\delta}{2 tn}}.\]

By the property of supremum, we have: with probability at least \(1-\delta\),

\[\sup_{h\in\mathcal{H}}\Big{(}\mathcal{L}_{p}^{\mathcal{B}}(\ell,h )-\mathcal{L}_{\widehat{D}}^{\mathcal{B}}(\ell,h)\Big{)} =\sup_{h\in\mathcal{H}}\Big{(}\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)-\mathcal{L}_{\widehat{\rho}}^{\mathcal{B}}(\ell,h)+\mathcal{L}_{\widehat{ \rho}}^{\mathcal{B}}(\ell,h)-\mathcal{L}_{\widehat{D}}^{\mathcal{B}}(\ell,h) \Big{)}\] \[\leq\sup_{h\in\mathcal{H}}\big{(}\mathcal{L}_{p}^{\mathcal{B}}( \ell,h)-\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)\big{)}+\sup_{h\in\mathcal{H}} \Big{(}\mathcal{L}_{\widehat{\rho}}^{\mathcal{B}}(\ell,h)-\mathcal{L}_{\widehat {D}}^{\mathcal{B}}(\ell,h)\Big{)}\] \[=\Phi^{\mathcal{B}}(\mathfrak{D})+\Psi^{\mathcal{B}}(\widehat{ \mathfrak{D}})\] \[\leq 2\mathfrak{R}_{t}(\widetilde{\mathcal{G}})+2\mathfrak{R}_{tn }(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\ln\!4/\delta}{2t}}+3U\sqrt{\frac{ \ln\!4/\delta}{2tn}}.\]

Proof of Corollary 3.2.: The main idea of the proof is to use risk decomposition to reduce \(\mathcal{L}_{p}^{\mathcal{B}}(\ell,\hat{h})-\mathcal{L}_{p}^{\mathcal{B}}( \ell,h^{\star})\) to \(\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)-\mathcal{L}_{\widehat{\mathcal{D}}}^{ \mathcal{B}}(\ell,h)\) for some \(h\). Then we have: with probability at least \(1-\delta\),

\[\mathcal{L}_{p}^{\mathcal{B}}(\ell,\hat{h})-\mathcal{L}_{p}^{ \mathcal{B}}(\ell,h^{\star}) =\mathcal{L}_{p}^{\mathcal{B}}(\ell,\hat{h})-\mathcal{L}_{\widehat {\mathcal{D}}}^{\mathcal{B}}(\ell,\hat{h})+\mathcal{L}_{\widehat{\mathcal{D} }}^{\mathcal{B}}(\ell,\hat{h})-\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{ B}}(\ell,h^{\star})+\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h^{ \star})-\mathcal{L}_{p}^{\mathcal{B}}(\ell,h^{\star})\] \[\stackrel{{ a}}{{\leq}}\mathcal{L}_{p}^{\mathcal{B}} (\ell,\hat{h})-\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,\hat{h})+ \mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h^{\star})-\mathcal{L} _{p}^{\mathcal{B}}(\ell,h^{\star})\] \[\leq 2\sup_{h\in\mathcal{H}}\Big{|}\mathcal{L}_{p}^{\mathcal{B}} (\ell,h)-\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h)\Big{|}\] \[\stackrel{{ b}}{{\leq}}4\mathfrak{R}_{t}(\widetilde{ \mathcal{G}})+4\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\ln\! 8/\delta}{2t}}+3U\sqrt{\frac{\ln\!8/\delta}{2tn}},\]

where: \(a\) is from the fact that \(\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,\hat{h})\leq\mathcal{L} _{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h^{\star})\) by the definition of \(\hat{h}\); \(b\) is from Theorem 3.1, although in Theorem 3.1, we only have bounds for \(\sup_{h\in\mathcal{H}}\Big{(}\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)-\mathcal{L} _{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h)\Big{)}\), we can use a similar argumentation as in Theorem 3.1 to get the same bound for \(\sup_{h\in\mathcal{H}}\Big{(}\mathcal{L}_{\widehat{\mathcal{D}}}^{\mathcal{B}} (\ell,h)-\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)\Big{)}\), both with probability at least \(1-\delta^{\prime}\), using union bound for these two cases we get a bound for \(\sup_{h\in\mathcal{H}}\Big{|}\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)-\mathcal{L} _{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h)\Big{|}\) with probability at least \(1-2\delta^{\prime}\), take \(\delta=\frac{\delta^{\prime}}{2}\), we get the bound for \(\sup_{h\in\mathcal{H}}\Big{|}\mathcal{L}_{p}^{\mathcal{B}}(\ell,h)-\mathcal{L} _{\widehat{\mathcal{D}}}^{\mathcal{B}}(\ell,h)\Big{|}\) with probability at least \(1-\delta\), this is why we get \(\ln\!4/\delta\) in Theorem 3.1 but \(\ln\!8/\delta\) here.

### Proof of the Limited Environment Case

Proof \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) is a pseudometric.: It is obvious that \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) is symmetric, now we proof that \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) satisfies the triangle inequality. By definition, for distribution \(P,Q,R\):

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q) =\sup_{h\in\mathcal{H}}\left|\underset{(\mathbf{x},y)\sim P}{\mathbb{ E}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y) \right]-\underset{(\mathbf{x},y)\sim Q}{\mathbb{E}}\left[\sup_{\mathbf{x}^{\prime}\in \mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]\right|\] \[=\sup_{h\in\mathcal{H}}\left|\underset{(\mathbf{x},y)\sim P}{\mathbb{ E}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y) \right]-\underset{(\mathbf{x},y)\sim R}{\mathbb{E}}\left[\sup_{\mathbf{x}^{\prime}\in \mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]\right.\] \[\qquad\left.+\underset{(\mathbf{x},y)\sim R}{\mathbb{E}}\left[\sup_{ \mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]- \underset{(\mathbf{x},y)\sim Q}{\mathbb{E}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B }(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]\right|\] \[\overset{a}{\leq}\sup_{h\in\mathcal{H}}\left|\underset{(\mathbf{x},y) \sim P}{\mathbb{E}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h( \mathbf{x}^{\prime}),y)\right]-\underset{(\mathbf{x},y)\sim R}{\mathbb{E}}\left[\sup_ {\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]\right|\] \[\qquad+\sup_{h\in\mathcal{H}}\left|\underset{(\mathbf{x},y)\sim R}{ \mathbb{E}}\left[\sup_{\mathbf{x}^{\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{ \prime}),y)\right]-\underset{(\mathbf{x},y)\sim Q}{\mathbb{E}}\left[\sup_{\mathbf{x}^ {\prime}\in\mathcal{B}(\mathbf{x})}\ell(h(\mathbf{x}^{\prime}),y)\right]\right|\] \[=d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,R)+d^{\mathcal{B}}_{\ell( \mathcal{H})}(R,Q),\]

where: \(a\) is from the subadditivity of the supremum operator. So \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) is a pseudometric. 

Before prove Thorem 3.3, we first give a useful lemma.

**Lemma A.5**.: _For any \(h\in\mathcal{H}\), any distribution \(P,Q\) on, we have:_

\[\left|\mathcal{R}^{\mathcal{B}}_{P}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{Q}(\ell,h)\right|\leq d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q)\]

Proof of Lemma a.5.: From the definition of \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q)\), we can know that:

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(P,Q)=\sup_{h\in\mathcal{H}}\left|\mathcal{ R}^{\mathcal{B}}_{P}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{Q}(\ell,h)\right| \geq\left|\mathcal{R}^{\mathcal{B}}_{P}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{Q}( \ell,h)\right|,\;\forall h\in\mathcal{H},\]

which is just what we want. 

**Theorem 3.3**.: _For a given but unknown target distribution \(\mathcal{T}\), let \(\Delta^{t-1}\coloneqq\{(\lambda_{1},\cdots,\lambda_{t})|\lambda_{i}\geq 0, \sum_{i=1}^{t}\lambda_{i}=1\}\) be the \(t\)-dimensional simplex and \(\text{Conv}(\mathfrak{D})\coloneqq\left\{\sum_{i=1}^{t}\lambda_{i}\mathcal{D}_ {i}|\mathbf{\lambda}\in\Delta^{t-1}\right\}\) be the convex hull of \(\mathfrak{D}=\{\mathcal{D}_{1},\ldots,\mathcal{D}_{t}\}\), define \(\mathcal{T}_{P}\in\underset{\mathcal{D}\in\text{Conv}(\mathfrak{D})}{\arg\inf }d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{D},\mathcal{T})\) be the "projection" of \(\mathcal{T}\) onto \(\text{Conv}(\mathfrak{D})\) and \(\mathbf{\lambda}^{\star}\) be the weight vector where \(\mathcal{T}_{P}=\sum_{i=1}^{t}\lambda_{i}^{\star}\mathcal{D}_{i}\), \(\ell\in[0,U]\), then we have: with probability at least \(1-\delta\), for all \(h\in\mathcal{H}\),_

\[\mathcal{R}^{\mathcal{B}}_{\mathcal{T}}(\ell,h) \leq\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}^{\mathcal{B}}_{\widehat {\mathcal{D}}_{i}}(\ell,h)+\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{\star}d^{ \mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D }}_{j})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T},\mathcal{T}_{P})\] \[+4\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+2\mathfrak{R}_{n}( \widetilde{\mathcal{G}})+6U\sqrt{\frac{n8/\delta}{2tn}}+3U\sqrt{\frac{n(16t/ \delta)}{2n}}\]

Proof of Theorem 3.3.: Our proof is divided into 2 steps, the first step gets results for the population distributions and the second step uses finite sample approximation to get results for empirical distributions.

**Step 1. Get the relationship between the population distributions**.

Lemma A.5 tells us that: for all \(h\in\mathcal{H}\),

\[\mathcal{R}^{\mathcal{B}}_{\mathcal{T}}(\ell,h)\leq\mathcal{R}^{\mathcal{B}}_{ \mathcal{D}}(\ell,h)+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T},\bar{ \mathcal{D}})\leq\mathcal{R}^{\mathcal{B}}_{\mathcal{D}}(\ell,h)+d^{\mathcal{ B}}_{\ell(\mathcal{H})}(\mathcal{T},\mathcal{T}_{P})+d^{\mathcal{B}}_{\ell( \mathcal{H})}(\mathcal{T}_{P},\bar{\mathcal{D}}),\]where the last inequality is from the triangle inequality of \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\) and \(\bar{\mathcal{D}}=\frac{1}{t}\sum_{i=1}^{t}\mathcal{D}_{i}\). Recall that \(\widehat{\mathcal{D}}=\frac{1}{t}\sum_{i=1}^{t}\widehat{\mathcal{D}}_{i}\), then we have:

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\bar{\mathcal{ D}}) \stackrel{{ a}}{{\leq}}d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P}, \widehat{\mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{ T}}_{P},\widehat{\mathcal{D}})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{ \mathcal{D}},\bar{\mathcal{D}})\] \[=d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{ \mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}, \bar{\mathcal{D}})+d^{\mathcal{B}}_{\ell(\mathcal{H})}\left(\sum_{i=1}^{t} \lambda_{i}^{*}\widehat{\mathcal{D}}_{i},\frac{1}{t}\sum_{i=1}^{t}\widehat{ \mathcal{D}}_{i}\right)\] \[=d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{ \mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}, \bar{\mathcal{D}})+\sup_{h\in\mathcal{H}}\left|\mathcal{R}^{\mathcal{B}}_{\sum_ {i}\lambda_{i}^{*}\widehat{\mathcal{D}}_{i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_ {\sum_{i}\frac{1}{t}\widehat{\mathcal{D}}_{i}}(\ell,h)\right|\] \[=d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{ \mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}, \bar{\mathcal{D}})+\sup_{h\in\mathcal{H}}\left|\mathcal{R}^{\mathcal{B}}_{\frac {1}{t}\sum_{i}\sum_{j}\lambda_{j}^{*}\widehat{\mathcal{D}}_{j}}(\ell,h)- \mathcal{R}^{\mathcal{B}}_{\sum_{i}\frac{1}{t}\sum_{j}\lambda_{j}^{*}\widehat{ \mathcal{D}}_{i}}(\ell,h)\right|\] \[\stackrel{{ b}}{{\leq}}d^{\mathcal{B}}_{\ell(\mathcal{ H})}(\mathcal{T}_{P},\widehat{\mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}( \widehat{\mathcal{D}},\bar{\mathcal{D}})+\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j }^{*}\sup_{h\in\mathcal{H}}\left|\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{ D}}_{i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_{j}}(\ell,h)\right|\] \[=d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{ \mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}, \bar{\mathcal{D}})+\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{*}d^{\mathcal{B}}_{ \ell(\mathcal{H})}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j}),\]

where: \(a\) is from the triangle inequality of \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\cdot,\cdot)\); \(b\) follows from the linearity of \(\mathcal{R}^{\mathcal{B}}_{\mathcal{D}}(\ell,h)\) and the subadditivity of the supremum operator. So we have, for any \(h\in\mathcal{H}\):

\[\mathcal{R}^{\mathcal{B}}_{\mathcal{T}}(\ell,h)\leq\mathcal{R}^{ \mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)+d^{\mathcal{B}}_{\ell(\mathcal{H})}( \mathcal{T},\mathcal{T}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T} _{P},\widehat{\mathcal{T}}_{P})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{ \mathcal{D}},\bar{\mathcal{D}})+\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{*}d^{ \mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D }}_{j}).\] (A.7)

**Step 2. We now show the bound of the finite sample approximation error \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{\mathcal{T}}_{P})\) and \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}},\bar{\mathcal{D}})\). Since we have no access to the population distribution \(\bar{\mathcal{D}}\), we also give a finite sample approximation of \(\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)\) and bound the corresponding approximation error.**

The empirical distribution of \(\bar{\mathcal{D}}\) is \(\widehat{\mathcal{D}}=\frac{1}{t}\sum_{i=1}^{t}\widehat{\mathcal{D}}_{i}\), then we have:

\[\sup_{h\in\mathcal{H}}\left(\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell, h)-\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)\right)=\sup_{h\in\mathcal{H}} \frac{1}{t}\sum_{i=1}^{t}\left(\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}_{i }}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_{i}}(\ell,h)\right) =\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}}),\]

where \(\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})\) is defined in the proof of Theorem 3.1. Then we have, with probability at least \(1-\frac{\delta}{4}\), for all \(h\in\mathcal{H}\),

\[\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)=\frac{1}{t}\sum_{i=1}^{t }\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}_{i}}(\ell,h)\leq\frac{1}{t}\sum_ {i=1}^{t}\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_{i}}(\ell,h)+2 \mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}8/\delta}{2 tn}}.\] (A.8)

Note that \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}},\bar{\mathcal{D}})= \sup_{h\in\mathcal{H}}\left|\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)- \mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)\right|\), similar as the bound of \(\Psi^{\mathcal{B}}(\widehat{\mathfrak{D}})=\sup_{h\in\mathcal{H}}\left( \mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{ \bar{\mathcal{D}}}(\ell,h)\right)\), we have that with probability at least \(1-\frac{\delta}{4}\):

\[\sup_{h\in\mathcal{H}}\left(\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell, h)-\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)\right)\leq 2\mathfrak{R}_{tn}( \widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}8/\delta}{2tn}},\]

so with probability at least \(1-\frac{\delta}{2}\):

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}},\bar{\mathcal{D}})= \sup_{h\in\mathcal{H}}\left|\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell, h)-\mathcal{R}^{\mathcal{B}}_{\bar{\mathcal{D}}}(\ell,h)\right|\leq 2\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}8/\delta}{2 tn}}.\] (A.9)

Now we bound \(d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{\mathcal{T}}_{P})\), by the definition of \(\mathcal{T}_{P}\), we have:

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{\mathcal{T}}_{P})= d^{\mathcal{B}}_{\ell(\mathcal{H})}\left(\sum_{i=1}^{t}\lambda_{i}^{*}\mathcal{D}_{i}, \sum_{i=1}^{t}\lambda_{i}^{*}\widehat{\mathcal{D}}_{i}\right)\leq\sum_{i=1}^{t} \lambda_{i}^{*}d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{D}_{i},\widehat{ \mathcal{D}}_{i})\]For each \(i\), we define:

\[\Gamma^{\mathcal{B}}(\widehat{\mathcal{D}}_{i})=\sup_{h\in\mathcal{H}}\left( \mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{ \widehat{\mathcal{D}}_{i}}(\ell,h)\right).\]

Since \(\ell\in[0,U]\), changing one example in \(\widehat{\mathcal{D}}_{i}\) will lead to at most \(\frac{1}{n}\) chang in \(\Gamma^{\mathcal{B}}(\widehat{\mathcal{D}}_{i})\), so by McDiarmid's inequality we know: with probability at least \(1-\frac{\delta}{16t}\),

\[\Gamma^{\mathcal{B}}(\widehat{\mathcal{D}}_{i})\leq\mathbb{E}\left[\Gamma^{ \mathcal{B}}(\widehat{\mathcal{D}}_{i})\right]+U\sqrt{\frac{\text{ln}(16t/ \delta)}{2n}}.\]

Next we upper bound \(\mathbb{E}\left[\Gamma^{\mathcal{B}}(\widehat{\mathcal{D}}_{i})\right]\), by similar analysis as in the proof of Theorem 3.1, we have: with probability at least \(1-\frac{\delta}{16t}\),

\[\mathop{\mathbb{E}}_{\widehat{\mathcal{D}}_{i}}\left[\Gamma^{\mathcal{B}}( \widehat{\mathcal{D}}_{i})\right]\leq\mathfrak{R}_{n}(\widetilde{\mathcal{G}} )+U\sqrt{\frac{\text{ln}(16t/\delta)}{2n}}.\]

Then with probability at least \(1-\frac{\delta}{8t}\),

\[\sup_{h\in\mathcal{H}}\left(\mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell, h)-\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_{i}}(\ell,h)\right)=\Gamma^{ \mathcal{B}}(\widehat{\mathcal{D}}_{i})\leq 2\mathfrak{R}_{n}( \widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}(16t/\delta)}{2n}}.\]

With similar argument, we know that with probability at least \(1-\frac{\delta}{8t}\),

\[\sup_{h\in\mathcal{H}}\left(\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_ {i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)\right)\leq 2 \mathfrak{R}_{n}(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}(16t/\delta)} {2n}}.\]

So with probability at least \(1-\frac{\delta}{4t}\),

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}_{j},\mathcal{D}_{j} )=\sup_{h\in\mathcal{H}}\left|\mathcal{R}^{\mathcal{B}}_{\mathcal{D}_{i}}(\ell,h)-\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}_{i}}(\ell,h)\right|\leq 2 \mathfrak{R}_{n}(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}(16t/\delta)} {2n}}.\]

So with probability at least \(1-\frac{\delta}{4}\),

\[d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{\mathcal{T}}_{P} )\leq\sum_{j=1}^{t}\lambda_{j}^{\star}d^{\mathcal{B}}_{\ell(\mathcal{H})}( \widehat{\mathcal{D}}_{j},\mathcal{D}_{j})\leq 2\mathfrak{R}_{n}( \widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}(16t/\delta)}{2n}}.\] (A.10)

Now combine (A.7), (A.8), (A.9) and (A.10), we have: with probability at least \(1-\delta\), for any \(h\in\mathcal{H}\),

\[\mathcal{R}^{\mathcal{B}}_{\mathcal{T}}(\ell,h) \leq\mathcal{R}^{\mathcal{B}}_{\widehat{\mathcal{D}}}(\ell,h)+d^ {\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T},\mathcal{T}_{P})+d^{\mathcal{B} }_{\ell(\mathcal{H})}(\mathcal{T}_{P},\widehat{\mathcal{T}}_{P})+d^{\mathcal{ B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}},\overline{\mathcal{D}})+\frac{1}{t}\sum_{i} \sum_{j}\lambda_{j}^{\star}d^{\mathcal{B}}_{\ell(\mathcal{H})}(\widehat{ \mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\] \[\leq\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}^{\mathcal{B}}_{\widehat{ \mathcal{D}}_{i}}(\ell,h)+2\mathfrak{R}_{tn}(\widetilde{\mathcal{G}})+3U\sqrt{ \frac{\text{ln}8/\delta}{2tn}}+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T},\mathcal{T}_{P})+2\mathfrak{R}_{n}(\widetilde{\mathcal{G}})\] \[\quad+3U\sqrt{\frac{\text{ln}(16t/\delta)}{2n}}+2\mathfrak{R}_{tn }(\widetilde{\mathcal{G}})+3U\sqrt{\frac{\text{ln}8/\delta}{2tn}}+\frac{1}{t} \sum_{i}\sum_{j}\lambda_{j}^{\star}d^{\mathcal{B}}_{\ell(\mathcal{H})}( \widehat{\mathcal{D}}_{i},\widehat{\mathcal{D}}_{j})\] \[=\frac{1}{t}\sum_{i=1}^{t}\mathcal{R}^{\mathcal{B}}_{\widehat{ \mathcal{D}}_{i}}(\ell,h)+\frac{1}{t}\sum_{i}\sum_{j}\lambda_{j}^{\star}d^{ \mathcal{B}}_{\ell(\mathcal{H})}(\widehat{\mathcal{D}}_{i},\widehat{\mathcal{D} }_{j})+d^{\mathcal{B}}_{\ell(\mathcal{H})}(\mathcal{T},\mathcal{T}_{P})+4 \mathfrak{R}_{tn}(\widetilde{\mathcal{G}})\] \[\quad+2\mathfrak{R}_{n}(\widetilde{\mathcal{G}})+6U\sqrt{\frac{ \text{ln}8/\delta}{2tn}}+3U\sqrt{\frac{\text{ln}(16t/\delta)}{2n}}.\]

## Appendix B Toy Example to Model the ColoredMNIST

**Example 2**.: Consider data point \((\boldsymbol{x},y)\), where \(\boldsymbol{x}\in\mathcal{X}\subseteq\mathbb{R}^{d+1}\) consists of invariant feature \(\boldsymbol{x}_{\text{inv}}\in\mathcal{X}_{\text{inv}}\subseteq\mathbb{R}^{d}\) and spuriously correlated feature \(x_{\text{sp}}\in\{\pm s\}\subsetneq\mathbb{R}\), to simplify the example, let \((\mathbf{x}_{\text{inv}},x_{\text{sp}})\) be the feature observed. Consider the case there is a linear classifier \(h(\mathbf{x}_{\text{inv}})=\mathbf{w}_{\text{inv}}\cdot\mathbf{x}_{\text{inv}}\) such that \(y\cdot h(\mathbf{x}_{\text{inv}})>0\), i.e., \(\mathbf{x}_{\text{inv}}\) is invariant for all distributions on \(\mathcal{X}_{\text{inv}}\times\{\pm 1\}\). Suppose the dataset \(S=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\) is linearly separable, and the dataset \(S\) is induced into two disjoint groups: a **majority group**\(S_{\text{maj}}\) where \(x_{\text{sp}}\cdot y<0\) and a **minority group**\(S_{\text{min}}\) where \(x_{\text{sp}}\cdot y<0\), let \(p=\mathbb{P}[x_{\text{sp}}\cdot y>0]>0.5\), which means that the spurious feature is positively related to the label \(y\).

The next theorem considers the degree of dependence on the spurious feature of the trained linear classifier.

**Theorem B.1** (Theorem 4 of [48]).: _Let \(\mathbb{H}\) be the set fo linear classifiers, \(h(\mathbf{x})=\mathbf{w}_{\text{im}}\mathbf{x}_{\text{im}}+w_{\text{sp}}x_{\text{sp}}\). Consider any task that satisfies all the constraints in Section 3.1 of [48]. Consider a dataset \(S\) drawn from \(\mathcal{D}\) such that the empirical distribution of \(\mathbf{x}_{\text{im}}\) given \(x_{\text{sp}}\cdot y>0\) is identical to the empirical distribution of \(\mathbf{x}_{\text{im}}\) given \(x_{\text{sp}}\cdot y<0\). Let \(\mathbf{w}_{\text{im}}(t)\mathbf{x}_{\text{inv}}+w_{\text{sp}}x_{\text{sp}}\) be initialized to the origin, and trained with an infinitesimal learning rate gradient descent to minimize the exponential loss on a dataset \(S\). Then, for any \((\mathbf{x},y)\in S\), we have:_

\[\Omega\left(\frac{\text{ln}\frac{c+p}{c+\sqrt{p(1-p)}}}{\mathcal{M}\text{ln}(t +1)}\right)\leq\frac{w_{\text{sp}}(t)s}{|\mathbf{w}_{\text{inv}}(t)\cdot\mathbf{x}_{ \text{inv}}|}\leq\mathcal{O}\left(\frac{\text{ln}\frac{p}{1-p}}{\text{ln}(t+1) }\right),\]

_where:_

1. \(p\) _denotes the empirical level of spurious correlation,_ \(p=\frac{1}{|S|}\sum_{(\mathbf{x},y)\in S}\mathbf{1}[x_{\text{sp}}\cdot y>0]\) _which without generality is assumed to satisfy_ \(p\in[0.5,1)\)_._
2. \(\mathcal{M}\) _denotes the maximum value of the margin of the max-margin classifier on_ \(S\)_, i.e.,_ \(\mathcal{M}=\text{max}_{\mathbf{x}\in S}\hat{\mathbf{w}}\cdot\mathbf{x}\) _where_ \(\hat{\mathbf{w}}\) _is the max-margin classifier on_ \(S\)_._
3. \(c\coloneqq\frac{2(2\mathcal{M}-1)}{s^{2}}\)_._

**Remark 4**.: _In the proof of Theorem B.1 (please refer to [48]), the loss function is:_

\[L(\mathbf{w}_{\text{inv}},w_{\text{sp}})=p\mathbb{E}_{\mathbf{x}_{\text{im}}\sim \hat{D}_{\text{inv}}}\left[e^{-(\mathbf{w}_{\text{im}}\mathbf{x}_{\text{im}}+w_{\text{ sp}}s)}\right]+(1-p)\mathbb{E}_{\mathbf{x}_{\text{im}}\sim\hat{D}_{\text{inv}}} \left[e^{-(\mathbf{w}_{\text{im}}\mathbf{x}_{\text{im}}-w_{\text{sp}}s)}\right].\] (B.11)

_We denote \(L_{1}=\mathbb{E}_{\mathbf{x}_{\text{im}}\sim\hat{D}_{\text{inv}}}\left[e^{-(\mathbf{w} _{\text{im}}\mathbf{x}_{\text{inv}}+w_{\text{sp}}s)}\right]\) and \(L_{2}=\mathbb{E}_{\mathbf{x}_{\text{im}}\sim\hat{D}_{\text{inv}}}\left[e^{-(\mathbf{w} _{\text{im}}\mathbf{x}_{\text{inv}}-w_{\text{sp}}s)}\right]\), so \(L=pL_{1}+(1-p)L_{2}\). To model the case in the ColoredMNIST dataset, we let \(S_{1}\) denote the dataset with corruption rate \(0.1\), \(S_{2}\) the dataset with corruption rate \(0.2\), \(S_{3}\) the dataset with corruption rate \(0.9\). For training environment with corruption rate \(p_{\text{train}}\), we have \(L_{\text{train}}=p_{\text{train}}L_{1}+(1-p_{\text{train}})L_{2}\) and \(L_{\text{test}}=p_{\text{test}}L_{1}+(1-p_{\text{test}})L_{2}\) if \(p_{\text{train}}>0.5\), otherwise we have \(L_{\text{train}}=p_{\text{train}}L_{2}+(1-p_{\text{train}})L_{1}\) and \(L_{\text{test}}=p_{\text{test}}L_{2}+(1-p_{\text{test}})L_{1}\). What's more, since the lower bound \(>0\) for \(p>\frac{1}{2}\), we have \(L_{1}<L_{2}\). So in the standard training case:_

1. _When we train with_ \(S_{1},S_{3}\)_, then the overall_ \(p_{\text{train}}=\frac{0.1+0.9}{2}=\frac{1}{2}\)_. So according to Theorem B.1 _we know that_ \(w_{\text{sp}}(t)=0\)_, which means that the classifier just depends on the invariant feature, so the performance on_ \(S_{2}\) _is good. Since_ \(w_{\text{sp}}(t)=0\)_,_ \(L_{1}=L_{2}\)_, so we have_ \(L_{\text{train}}=p_{\text{train}}L_{1}+(1-p_{\text{train}})L_{2}=\mathbb{E}_{\bm {x}_{\text{im}}\sim\hat{D}_{\text{inv}}}\left[e^{-(\mathbf{w}_{\text{im}}\mathbf{x}_{ \text{inv}})}\right]=L_{\text{test}}\)_. So the mixed training distribution here is the same as the test distribution, which means that we can generalize well in this case, which is consistent with our experimental results._
2. _When training with_ \(S_{2},S_{3}\)_, then the overall_ \(p_{\text{train}}=\frac{0.2+0.9}{2}=0.55\)_, which is quite close to_ \(0.5\)_, which means that the correlation between_ \(x_{\text{sp}}\) _and_ \(y\) _is not very strong. It means that the upper bound and the lower bound in Theorem B.1 is small, so_ \(w_{\text{sp}}\) _is small, the classifier mainly depends on the invariant feature and slightly depends on the spurious feature, which is just a little negatively related to_ \(y\) _when_ \(p_{\text{test}}=0.1\) _in the test environment. In this case,_ \(L_{\text{train}}=p_{\text{train}}L_{1}+(1-p_{\text{train}})L_{2}=0.55L_{1}+0.45L_{2}\) _and_ \(L_{\text{test}}=p_{\text{test}}L_{1}+(1-p_{\text{test}})L_{2}=0.1L_{1}+0.9L_{2}\)_, and_ \(L_{\text{test}}-L_{\text{train}}=-0.45L_{1}+0.45L_{2}=0.45(L_{2}-L_{1})>0\)_, but in this case_ \(w_{\text{sp}}\) _is small, so we have_ \(L_{2}-L_{1}\) _is small according to (_B.11_), so the difference between training error and test error is not too large, so the generalization performance for this case is not bad, which is consistent with our experimental results._
3. _Finally, it comes to the failure case in our experimental results, i.e., using_ \(S_{1},S_{2}\) _as the training environments and_ \(S_{3}\) _as the test environment. In this case, the overall_\(\frac{0.1+0.2}{2}=0.15\), so the classifier depends more on the spurious features compared with the above two cases, what's more, since \(p_{\text{test}}=0.9\), the relationship between \(x_{\textit{sp}}\) and \(y\) is almost reversed when we switch to the test stage, so we have \(L_{\textit{train}}=p_{\textit{train}}L_{2}+(1-p_{\textit{train}})L_{1}=0.85L_{1 }+0.15L_{2}\) and \(L_{\textit{test}}=p_{\textit{test}}L_{2}+(1-p_{\textit{test}})L_{1}=0.1L_{1}+0.9L_{2}\) and \(L_{\textit{test}}-L_{\textit{train}}=-0.75L_{1}+0.75L_{2}=0.75(L_{2}-L_{1})>0\), and the \(w_{\textit{sp}}\) here is much larger than that in case 2, which means that \(L_{2}-L_{1}\) here is much larger than that in case 2, so the gap between training error and test error here map be much larger than that in case two, leading to the poor generalization performance in practice._

_We can see that the analysing results here are consistent with our experimental results for ColoredMNIST in Appendix D.2._

## Appendix C Experimental Settings

We run each algorithm 20 times and 1 trial (since adversarial training is time-consuming, we just run 1 trial rather than 3 trials as done in DomainBed). We use part of the training data as the validation set to select the best model of the 20 runs according to the adversarial robustness of the training environments. Following DomainBed, we use random hyper-parameters in the 20 runs.

### Attacked Algorithms

1. Empirical Risk Minimization (**ERM**, [61]) minimizes the errors across domains.
2. Meta-Learning for Domain Generalization (**MLDG**, [39]) leverages MAML [16] to meta-learn how to generalize across domains.
3. Class-conditional DANN (**C-DANN**, [42]) is a variant of DANN [17] matching the conditional distributions \(\mathbb{P}[\phi(X^{d})|Y^{d}=y]\) across domains, for all labels \(y\).
4. Risk Extrapolation (**VREx**, [33]) approximates IRM with a variance penalty.
5. Representation Self-Challenging (**RSC**, [28]) learns robust neural networks by iteratively discarding (challenging) the most activated features.
6. Domain-wise Multiple-perturbation Adversarial Training (**MAT**, [64]) use an universal adversarial perturbation (UAP) with low rank along the dimension of examples (for \(n\) examples, use the convex combination of \(k\)\((k<n)\) perturbations as the universal perturbation) to conduct universal adversarial training (UAT) to improve the OOD generalization.
7. Adversarial Training with Low-rank Decomposed perturbations(**LDAT**, [64]) shares similar idea with MAT, but it uses an UAP with low rank along the input space (for \(N\times N\) images, it constrains the \(N\times N\) UAP matrix has rank \(l<N\)) to conduct universal adversarial training (UAT) to improve the OOD generalization.

### Datasets

We use the following datasets provided by the DomainBed [26]:

1. **ColoredMNIST**[4] is a variant of the MNIST handwritten digit classification dataset [35]. Domain \(d\in\{0.1,0.3,0.9\}\) contains a disjoint set of digits colored either red or blue. The label is a noisy function of the digit and color, such that color bears correlation \(d\) with the label and the digit bears correlation \(0.75\) with the label. This dataset contains \(70000\) examples of dimension \((2,28,28)\) and \(2\) classes.
2. **RotatedMNIST**[21] is a variant of MNIST where domain \(d\in\{0,15,30,45,60,75\}\) contains digits rotated by \(d\) degrees. Our dataset contains \(70000\) examples of dimension \((1,28,8)\) and \(10\) classes.
3. **PACS**[38] comprises four domains \(d\in\{\) art, cartoons, photos, sketches \(\}\). This dataset contains \(9991\) examples of dimension \((3,224,224)\) and \(7\) classes.
4. **VLCS**[15] comprises four photographic domains \(d\in\{\) Caltech101, LabelMe, SUN09, VOC2007 \(\}\). This dataset contains \(10729\) examples of dimension \((3,224,224)\) and \(5\) classes.
5. **OfficeHome**[62] includes domains four \(d\in\{\) art, clipart, product, real \(\}\). This dataset contains \(15588\) examples of dimension \((3,224,224)\) and \(65\) classes.

### Settings for attacking standard OOD algorithms

We evaluate the adversarial robustness of some of the algorithms in Table 1. For adversarial attacks, we use \(\ell_{\infty}\) adversarial perturbation upper bound \(\epsilon=0.1\) for RotatedMNIST and ColoredMNIST, \(\epsilon=4/255\) for others (it is because \(\epsilon=4/255\) is use usually used in \(224\times 224\) colored images); we use the classical FGSM and PGD-20 as the attack methods, for PGD-20, the step size \(\alpha=\epsilon/4\), we realize the attacks according to the package 'torchtatacks' [32].

We train the models in the same hyper-parameter random search setting (except that we just train the model for one trial) in DomainBed, and the we attack the models trained with the best hyper-parameters choosed by the **training-domain validation set** model selection method in DomainBed with the same seed as used during training. For MAT and LDAT, we use the same hyper-parameter random search setting in [64] except that we train with 20 random hyper-parameter for 1 trial and they train with 8 random hyper-parameter for 3 trials.

### Settings for OOD adversarial algorithms

In this subsection, we introduce the settings in our experiments in Section 5. We use the **training-domain validation set** model selection method in the experiment and train the model with 20 random hyper-parameter for 1 trial.

**Basic setting for training hyper-parameter random search.**

1. For AT, we use the same basic random search setting as ERM.
2. For RDANN, we use the same basic random search setting as DANN.

**Setting for adversarial training.** We use PGD-10 as the adversarial attack to generate adversarial examples for adversarial training, with \(\epsilon=0.1\) for RotatedMNIST and ColoredMNIST and \(\epsilon=4/255\) for the other \(224\times 224\) datasets, and we use the attack step size \(\alpha=\epsilon/4\) as usually done in the adversarial training community.

**Setting for evaluating adversarial robustness.** We consider two attack methods, FGSM and PGD-20, to evaluate the OOD adversarial robustness of the trained models. We use the same \(\epsilon\) as that used for training but we do 20 iterations for PGD in the evaluation stage rather than just 10 iterations in the training stage to avoid overfitting, which is also a strategy used by the adversarial robustness community.

## Appendix D Additional Experimental Results

In this section, we show the detailed adversarial robustness of the current OOD generalization algorithms and our proposed algorithms, here we show the adversarial robustness in each test environment, for the ColoredMNIST dataset and RotatedMNIST dataset, we set \(\epsilon=0.1\), and for other \(224\times 224\) datasets, we set \(\epsilon=\frac{4}{255}\).

In Appendices D.1 to D.3, we use FGSM and PGD-20 as the attacks to evaluate the adversarial robustness and the results are shown in the form of triple tuple **(clean accuracy / accuracy under FGSM attack / accuracy under PGD-20 attack)**. Each column represents the results for a test environment. The tables show that compared with existing methods, our methods (AT and RDANN) significantly improve the adversarial robustness of the model on the target domains.

In Appendix D.6, we use AutoAttack as the attacks to evaluate the adversarial robustness. Each column represents the results for a test environment. The results show that under AutoAttack, all the existing methods fails (**no more than \(1\%\)** adversarial accuracy), and our methods significantly improves the adversarial robustness.

[MISSING_PAGE_FAIL:29]

### OfficeHome

\begin{tabular}{l c c c c c} \hline \hline
**Algorithm** & **A** & **C** & **P** & **R** & **Avg** \\ \hline ERM & 62.1/12.6/0.0 & 52.9/20.9/0.3 & 76.4/25.7/0.5 & 76.0/26.3/0.6 & 66.9/21.4/0.4 \\ MLDG & 61.1/13.7/0.6 & 52.9/22.1/0.7 & 76.5/25.7/0.3 & 77.4/24.0/0.9 & 67.0/21.4/0.6 \\ CDANN & 61.8/8.5/0.1 & 53.5/19.3/0.1 & 75.8/24.4/0.1 & 77.6/23.7/0.0 & 67.2/19.0/0.1 \\ VREx & 58.7/8.3/0.0 & 52.4/20.0/0.1 & 75.5/22.5/0.7 & 76.1/23.3/1.0 & 65.7/18.5/0.4 \\ RSC & 59.3/9.1/0.2 & 52.1/18.3/0.3 & 75.2/20.5/1.0 & 74.1/20.6/1.2 & 65.1/17.1/0.7 \\ MAT & 57.3/11.2/0.1 & 54.0/20.2/0.5 & 75.2/26.9/0.8 & 77.4/24.1/1.8 & 66.0/20.6/0.8 \\ LDAT & 61.2/12.9/0.0 & 52.7/20.6/0.4 & 74.5/27.9/0.6 & 78.0/29.5/0.8 & 66.6/22.7/0.4 \\ AT & 29.6/16.4/14.5 & 44.6/36.4/35.0 & 53.2/40.9/38.3 & 53.7/36.1/32.9 & 45.3/32.4/30.2 \\ RDANN & 30.0/17.1/15.3 & 41.9/33.6/32.1 & 48.8/38.2/36.3 & 47.1/33.5/30

### OfficeHome

\begin{tabular}{l c c c c c} \hline \hline
**Algorithm** & **A** & **C** & **P** & **R** & **Avg** \\ \hline ERM & 0.0 & 0.0 & 0.1 & 0.1 & 0.0 \\ MLDG & 0.0 & 0.1 & 0.2 & 0.1 & 0.1 \\ CDANN & 0.0 & 0.1 & 0.0 & 0.1 & 0.0 \\ VREx & 0.0 & 0.1 & 0.2 & 0.1 & 0.1 \\ RSC & 0.0 & 0.1 & 0.1 & 0.0 & 0.0 \\ MAT & 0.0 & 0.3 & 0.2 & 0.1 & 0.1 \\ LDAT & 0.0 & 0.0 & 0.1 & 0.1 & 0.1 \\ AT & 14.2 & 34.6 & 38.1 & 32.4 & 29.8 \\ ADANN & 14.0 & 31.5 & 34.4 & 29.7 & 27.4 \\ \hline \hline \end{tabular}