# Fully Dynamic \(k\)-Clustering in \(\tilde{O}(k)\) Update Time

Sayan Bhattacharya

University of Warwick

s.bhattacharya@warwick.ac.uk &Martin Costa

University of Warwick

martin.costa@warwick.ac.uk &Silvio Lattanzi

Google Research

silviol@google.com &Nikos Parotsidis

Google Research

nikosp@google.com

###### Abstract

We present a \(O(1)\)-approximate fully dynamic algorithm for the \(k\)-median and \(k\)-means problems on metric spaces with amortized update time \(\tilde{O}(k)\) and worst-case query time \(\tilde{O}(k^{2})\). We complement our theoretical analysis with the first in-depth experimental study for the dynamic \(k\)-median problem on general metrics, focusing on comparing our dynamic algorithm to the current state-of-the-art by Henzinger and Kale [20]. Finally, we also provide a lower bound for dynamic \(k\)-median which shows that any \(O(1)\)-approximate algorithm with \(\tilde{O}(\operatorname{poly}(k))\) query time must have \(\tilde{\Omega}(k)\) amortized update time, even in the incremental setting.

## 1 Introduction

Clustering is a fundamental problem in unsupervised learning with several practical applications. In clustering, one is interested in partitioning elements into different groups (i.e. _clusters_), so that elements in the same group are more similar to each other than to elements in other groups. One of the most studied formulations of clustering is the metric clustering formulation. In this setting, elements are represented by points in a metric space, and the distances between points represent how similar the corresponding elements are (the closer elements are, the more similar they are). More formally, the input to our problem consists of a set of points \(U\) in a metric space with distance function \(d:U\times U\to\mathbb{R}_{\geq 0}\), a real number \(p\geq 1\), and an integer \(k\geq 1\). The goal is to compute a subset \(S\subseteq U\) of size \(|S|\leq k\), so as to minimize \(\texttt{cost}(S):=\sum_{x\in U}d(x,S)^{p}\), where \(d(x,S):=\min_{y\in S}d(x,y)\). We refer to the points in \(S\) as _centers_. We note that this captures well-known problems such as \(k\)-median clustering (when \(p=1\)) or \(k\)-means clustering (when \(p=2\)).

Due to this simple and elegant formulation, metric clustering has been extensively studied throughout the years, across a range of computational models [14; 22; 1; 12; 15; 2; 31; 11; 7]. In this paper, we focus on the _dynamic setting_, where the goal is to efficiently maintain a good clustering when the input data keeps changing over time. Because of its immediate real-world applications, the dynamic clustering problem has received a lot of attention from the machine learning community in recent years [16; 6; 10; 24; 18]. Below, we formally describe the model [13; 20; 3] considered in this paper.

At _preprocessing_, the algorithm receives the initial input \(U\). Subsequently, the input keeps changing by means of a sequence of _update_ operations, where each update inserts/deletes a point in \(U\). Throughout this sequence of updates, the algorithm needs to implicitly maintain a solution \(S^{*}\subseteq U\) to the current input \((U,d)\). The algorithm has an approximation ratio of \(\rho\geq 1\) if and only if we always have \(\texttt{cost}(S^{*})\leq\rho\cdot\texttt{opt}(U)\), where \(\texttt{opt}(U):=\min_{S\subseteq U,|S|\leq k}\texttt{cost}(S)\) denotes the optimal objective. Finally, whenever one _queries_ the algorithm, it has to return the list of centers in \(S^{*}\). The performance of a dynamic algorithm is captured by its approximation ratio, its update time, and its query time. Let\(U_{\text{\sc INT}}\) be the state of the input \(U\) at preprocessing. A dynamic algorithm has (amortized) _update time_\(O(\lambda)\) if for any sequence of \(t\geq 0\) updates, the total time it spends on preprocessing and handling the updates is \(O((t+|U_{\text{\sc INT}}|)\cdot\lambda)\). The time it takes to answer a query is called its _query time_.

Our Contributions.Our primary contribution is to design an algorithm for this problem with a near-optimal update time of \(\tilde{O}(k)\), without significantly compromising on the approximation ratio and query time.1 Interestingly, our algorithm can be easily extended to dynamic \(k\)-clustering for any constant \(p\) (see Appendix A). In the theorem below, we summarize our main result for dynamic \(k\)-median and \(k\)-means.

Footnote 1: Throughout this paper, we use \(\tilde{O}(.)\) and \(\tilde{\Omega}(.)\) notations to suppress \(\operatorname{polylog}(n)\) factors, where \(n=|U|\).

**Theorem 1.1**.: _There exists a dynamic algorithm that, with high probability, maintains a \(O(1)\)-approximate solution to the \(k\)-median and \(k\)-means problems for general metric spaces under point insertions and deletions with \(\tilde{O}(k)\) amortized update time and \(\tilde{O}(k^{2})\) worst-case query time._

It is important to note that in practice an algorithm often receives the updates in _batches_ of variable sizes [25, 29], so it is common for there to be a significant number of updates between cluster requests. As a consequence, optimizing the update time is of primary importance in practical applications. For example, if the size of each batch of updates is \(\Omega(k)\), then our amortized query time becomes \(\tilde{O}(k^{2}/k)=\tilde{O}(k)\), because the algorithm has to answer a query after processing at least one batch. Thus, our dynamic algorithm has near-optimal update and query times when the updates arrive in moderately large batches.

In addition, it is possible to observe that any dynamic \(k\)-clustering algorithm must have \(\Omega(k)\) query time, since the solution it needs to return while answering a query can consist of \(k\) centers. We also show a similar lower bound on the _update time_ of any constant approximate dynamic algorithm for this problem. This lower bound holds even in an _incremental_ setting, where we only allow for point-insertions in \(U\). We defer the proof of Theorem 1.2 to Appendix D.

**Theorem 1.2**.: _Any \(O(1)\)-approximate incremental algorithm for the \(k\)-median problem with \(\tilde{O}(\operatorname{poly}(k))\) query time must have \(\tilde{\Omega}(k)\) amortized update time._

Theorem 1.2 implies that the ultimate goal in this line of research is to get a \(O(1)\)-approximate dynamic \(k\)-clustering algorithm with \(\tilde{O}(k)\) update time and \(\tilde{O}(k)\) query time. Prior to this work, however, the state-of-the-art result for dynamic \(k\)-median (and \(k\)-means) was due to Henzinger and Kale [20], who obtained \(O(1)\)-approximation ratio, \(\tilde{O}(k^{2})\) update time and \(\tilde{O}(k^{2})\) query time. In this context, our result is a meaningful step toward obtaining an asymptotically optimal algorithm for the problem.

We supplement the above theorem with experiments that compare our algorithm with that of [20]. To the best of our knowledge, this is the first work in the dynamic clustering literature with a detailed experimental evaluation of dynamic \(k\)-median algorithms for general metrics. Interestingly, we observe that experimentally our algorithm is significantly more efficient than previous work without impacting the quality of the solution.

Our Techniques.We first summarize the approach used in the previous state-of-the-art result. In [20], the authors used a static algorithm for computing a coreset of size \(\tilde{O}(k)\) as a black box to get a fully dynamic algorithm for maintaining a coreset of size \(\tilde{O}(k)\) for general metric spaces in worst-case update time \(\tilde{O}(k^{2})\). Their algorithm works by maintaining a balanced binary tree of depth \(O(\log n)\), where each leaf in the tree corresponds to a point in the metric space [4]. Each internal node of the tree then takes the union of the (weighted) sets of points maintained by its two children and computes its coreset, which is then fed to its parent. They maintain this tree dynamically by taking all the leaves affected by an update and recomputing all the coresets at nodes contained in their leaf-to-root paths. Using state-of-the-art static coreset constructions, this update procedure takes time \(\tilde{O}(k^{2})\). Unfortunately, if one wants to ensure that the final output is a valid coreset of the metric space after each update, then there is no natural way to bypass having to recompute the leaf-to-root path after the deletion of a point that is contained in the final coreset. Hence, it is not at all clear how to modify this algorithm in order to reduce the update time to \(\tilde{O}(k)\).

We circumvent this bottleneck by taking a completely different approach compared to [20]. Our algorithm instead follows from the dynamization of a \(\tilde{O}(nk)\) time static algorithm for \(k\)-median by Mettu and Plaxton [28], where \(n=|U|\). We refer to this static algorithm as the MP algorithm. Informally, the MP algorithm works by constructing a set of \(O(\log(n/k))\)_layers_ by iteratively sampling random points at each layer, defining a clustering of the points in the layer that are 'close' to these samples, and removing these clusters from the layer. The algorithm then obtains a solution to the \(k\)-clustering problem by running a static algorithm for _weighted_\(k\)-median (defined in Section 2), on an instance of size \(O(k\log(n/k))\) defined by the clusterings at each layer. In order to dynamize this algorithm, we design a data structure that maintains \(O(\log(n/k))\) layers that are analogous to the layers in the MP algorithm. By allowing for some'slack' in the way that these layers are defined, we ensure that they can be periodically reconstructed in a way that leads to good amortized update time, while only incurring a small loss in the approximation ratio. The clustering at each layer is obtained by random sampling every time it is reconstructed and is maintained by arbitrarily reassigning a point in a cluster as the new center when the current center is deleted. We obtain a solution to the \(k\)-clustering problem from this data structure in the same way as the (static) MP algorithm--by running a static algorithm for weighted \(k\)-median on an instance defined by the clusterings maintained at each layer.

## 2 Preliminaries

In our computational model, the algorithm has access to the distance \(d(x,y)\) for any \(x,y\in U\) in \(O(1)\) time2. Given any two sets \(X,S\subseteq U\), define _the cost of \(X\) w.r.t. \(S\)_ as \(\mathtt{cost}(S,X):=\sum_{x\in X}\min_{s\in S}d(x,s)\). In addition, let \(\mathtt{cost}(S)=\mathtt{cost}(S,U)\). Next, define an _assignment_ to be a function \(\pi:U\to U\), and say that \(\pi\) assigns \(x\in U\) to \(\pi(x)\). We refer to an assignment \(\pi\) with \(|\pi(U)|\leq m\) as an _\(m\)-assignment_. The cost of \(X\subseteq U\) w.r.t. assignment \(\pi\) is \(\mathtt{cost}(\pi,X):=\sum_{x\in X}d(x,\pi(x))\). We denote \(\mathtt{cost}(\pi,U)\) by \(\mathtt{cost}(\pi)\). For any \(\rho\geq 1\), say that \(\pi\) is \(\rho\)-approximate if \(\mathtt{cost}(\pi)\leq\rho\cdot\mathtt{opt}(U)\), where \(\mathtt{opt}(U)\) is the cost of the optimal solution.

Footnote 2: This is a standard and common assumption in clustering settings.

Given any subset \(U^{\prime}\subseteq U\) and \(x\in U^{\prime}\), we denote by \(B_{U^{\prime}}(x,r)\) the set \(\{y\in U^{\prime}\,|\,d(x,y)\leq r\}\), i.e. the closed ball in \(U^{\prime}\) of radius \(r\geq 0\) centered at \(x\). When it is clear from the context that \(U^{\prime}=U\), we omit the subscript \(U^{\prime}\) and simply write \(B(x,r)\). For \(X\subseteq U^{\prime}\subseteq U\), we define \(B_{U^{\prime}}(X,r)\) to be the union of the balls \(B_{U^{\prime}}(x,r)\) for \(x\in X\).

**Definition 2.1**.: Given \(0<\rho<1\) and subsets \(X\subseteq U^{\prime}\subseteq U\), we define the following real numbers:

\[\nu_{\rho}(X,U^{\prime}):=\min\{r\geq 0\,|\,|B_{U^{\prime}}(X,r)|\geq\rho \cdot|U^{\prime}|\},\mu_{\rho}(U^{\prime}):=\min\{\nu_{\rho}(Y,U^{\prime})\,| \,Y\subseteq U^{\prime},\ |Y|=k\}.\]

In words, the real number \(\nu_{\rho}(X,U^{\prime})\) is the smallest radius \(r\) such that the closed ball of radius \(r\) around \(X\) captures at least a \(\rho\)-proportion of the points in \(U^{\prime}\). The real number \(\mu_{\rho}(U^{\prime})\) is then defined to be the smallest such radius \(\nu_{\rho}(X,U^{\prime})\) over all subsets \(X\subseteq U^{\prime}\) of size \(k\). Note that \(\nu_{\rho}(X,U^{\prime})\) and \(\mu_{\rho}(U^{\prime})\) are both increasing as functions of \(\rho\).

Finally, we will sometimes refer to the _weighted \(k\)-median problem_. An instance of this problem is given by a triple \((U,d,w)\), where \(w:U\to\mathbb{R}_{\geq 0}\) assigns a nonnegative _weight_ to every point \(x\in U\), in a metric space with distance function \(d\). Here, the goal is to compute a subset of at most \(k\) centers \(S\subseteq U\), so as to minimize \(\mathtt{cost}_{w}(S):=\sum_{x\in U}w(u)\cdot d(x,S)\). We let \(\mathtt{opt}_{w}(U):=\min_{S\subseteq U\cdot|S|\leq k}\mathtt{cost}_{w}(S)\) denote the optimal objective value of this weighted \(k\)-median instance. We will analogously use the symbol \(\mathtt{cost}_{w}(S,X):=\sum_{x\in X}w(x)\cdot d(x,S)\) to denote the cost of a set of points \(X\) w.r.t. \(S\) and the weight function \(w\).

## 3 Our Algorithm

Throughout this section, we fix the following parameters: \(\alpha\), \(\beta\), \(\epsilon\), \(\tau\) and \(k^{\prime}\); where \(\alpha\geq 1\) is a sufficiently large constant, \(\beta\) is any constant in the interval \((0,1)\), \(\epsilon>0\) is a sufficiently small constant, \(\tau:=\epsilon\beta\), and \(k^{\prime}:=\max\{k,\log(|U|)\}\).

### The Static Algorithm of [28]

Our starting point is the static algorithm of [28] for computing a \(\tilde{O}(k)\)-assignment \(\sigma:U\to U\), with \(S=\sigma(U)\), such that \(\texttt{cost}(\sigma)\leq O(1)\cdot\texttt{opt}(U)\). The relevant pseudocode appears in Algorithm 1 and Algorithm 2.

```
1:\(i\gets 1\) and \(U_{1}\gets U\)
2:while\(|U_{i}|>\alpha k^{\prime}\)do
3:\((S_{i},C_{i},\sigma_{i},\nu_{i})\leftarrow\texttt{AlmostCover}(U_{i})\)
4:\(U_{i+1}\gets U_{i}\setminus C_{i}\)
5:\(i\gets i+1\)
6:endwhile
7:\(t\gets i\)
8:\(S_{t}\gets U_{t}\), \(C_{t}\gets S_{t}\) and \(\nu_{t}\gets 0\)
9: Assign each \(x\in C_{t}\) to itself (i.e., \(\sigma_{t}(x):=x\in S_{t}\))
10:\(S\leftarrow\cup_{j\in[t]}S_{j}\)
11: Let \(\sigma:U\to S\) be the assignment such that for all \(j\in[t]\) and \(x\in C_{j}\), we have \(\sigma(x)=\sigma_{j}(x)\)
12:return\((S,\sigma,t)\) ```

**Algorithm 1**\(\texttt{StaticAlgo}(U)\)

The algorithm runs for \(t\)_iterations_. Let \(U_{i}\subseteq U\) denote the set of unassigned points at the start of iteration \(i\in[t-1]\) (initially, we have \(U_{1}=U\)). During iteration \(i\), the algorithm samples a set of \(\alpha k^{\prime}\) points \(S_{i}\) as centers, uniformly at random from \(U_{i}\). It then identifies the smallest radius \(\nu_{i}\) such that the balls of radius \(\nu_{i}\) around \(S_{i}\) cover at least \(\beta\)-fraction of the points in \(U_{i}\). Let \(C_{i}\subseteq U_{i}\) denote the set of points captured by these balls (note that \(C_{i}\supseteq S_{i}\) and \(|C_{i}|\geq\beta\cdot|U_{i}|\)). The algorithm assigns each \(x\in C_{i}\) to some center \(\sigma_{i}(x)\in S_{i}\) within a distance of \(\nu_{i}\). It then sets \(U_{i+1}\gets U_{i}\setminus C_{i}\), and proceeds to the next iteration. In the very last iteration \(t\), we have \(|U_{t}|\leq\alpha k^{\prime}\), and the algorithm sets \(S_{t}:=U_{t}\), \(C_{t}:=U_{t}\), and \(\sigma_{t}(x):=x\) for each \(x\in U_{t}\).

The algorithm returns the assignment \(\sigma\) with set of centers \(\sigma(U)=S\), where \(\sigma\) is simply the union of \(\sigma_{i}\) for all \(i\in[t]\).

Fix any \(i\in[t-1]\). Since \(|C_{i}|\geq\beta\cdot|U_{i}|\), we have \(|U_{i+1}|\leq(1-\beta)\cdot|U_{i}|\). As \(\beta\) is a constant, it follows that the algorithm runs for \(t=\tilde{O}(1)\) iterations. Since each iteration picks \(O(k)\) new centers, we get: \(|S|=\tilde{O}(k)\), and hence \(\sigma\) is a \(\tilde{O}(k)\)-assignment. Furthermore, [28] showed that \(\texttt{cost}(\sigma)=O(1)\cdot\texttt{opt}(U)\).

### Our Dynamic Algorithm

The main idea behind our dynamic algorithm is that it maintains the output \(S\) of the static algorithm from Section 3.1 in a lazy manner, by allowing for some small slack at appropriate places. Thus, it maintains \(t\)_layers_, where each layer \(i\in[t]\) corresponds to iteration \(i\) of the static algorithm. In the dynamic setting, the value of \(t\) changes over time. Specifically, layer \(i\in[t]\) consists of the tuple \((U_{i},S_{i},C_{i},\sigma_{i},\nu_{i})\), as in Algorithm 1. Whenever a large fraction of points gets inserted or deleted from some layer \(j\in[t]\), the dynamic algorithm rebuild all the layers \(i\in[j,t]\) from scratch.

The dynamic algorithm maintains the sets \(U_{i},S_{i}\) and \(C_{i}\) explicitly, and the assignment \(\sigma_{i}\) implicitly, in a manner which ensures that for all \(x\in C_{i}\) it can return \(\sigma_{i}(x)\in S_{i}\) in \(O(1)\) time. Furthermore, for each layer \(i\in[t]\), it explicitly maintains the value \(n_{i}\) (resp. \(n_{i}^{*}\)), which denotes the size of the set \(U_{i}\) at (resp. the number of updates to \(U_{i}\) since) the time it was last rebuilt. We explain this in more detail below. The value \(\nu_{i}\) is needed only for the sake of analysis.

**Preprocessing:** At preprocessing, we essentially run the static algorithm from Section 3.1 to set the value of \(t\) and construct the layers \(i\in[t]\). See Algorithm 3 and Algorithm 4. Note that at this stage \(n_{j}^{*}=0\) for all layers \(j\in[t]\).

```
1:for\(i=1,...,t\)do
2: Add \(x\) to \(U_{i}\)
3:\(n_{i}^{*}\gets n_{i}^{*}+1\)
4:endfor
5: Add \(x\) to \(C_{t}\) and \(S_{t}\), and set \(\sigma_{t}(x)\gets x\)
6:Rebuild ```

**Algorithm 5**Insert\((x)\)

**Handling the insertion of a point \(x\) in \(U\):** We simply add the point \(x\) to \(U_{i}\), for each layer \(i\in[t]\). Next, in the last layer \(t\), we create a new center at point \(x\) and assign the point \(x\) to itself. Finally, we call the Rebuild subroutine (to be described below). See Algorithm 5.

**Handling the deletion of a point \(x\) from \(U\):** Let \(j\in[t]\) be the last layer (with the largest index) containing the point \(x\). We remove \(x\) from each layer \(i\in[j]\). Next, if \(x\) was a center at layer \(j\), then we pick any arbitrary point \(x^{\prime}\in\sigma_{j}^{-1}(x)\setminus\{x\}\) (if such a point exists), make \(x^{\prime}\) a center, and assign every point \(y\in\sigma_{j}^{-1}(x)\setminus\{x\}\) to the newly created center \(x^{\prime}\). Finally, we call the Rebuild subroutine (to be described below). See Algorithm 6.

**The rebuild subroutine:** We say that a layer \(i\in[t]\) is _rebuild_ whenever our dynamic algorithm calls ConstructFromLayer\((j)\) for some \(j\leq i\), and that there is an _update_ in layer \(i\) whenever we add/remove a point in \(U_{i}\). It is easy to see that \(n_{i}^{*}\) denotes the number of updates in layer \(i\in[t]\) since the last time it was rebuilt (see Line 3 in Algorithm 4, Algorithm 5 and Algorithm 6). Next, we observe that Line 6 in Algorithm 5 and Line 16 in Algorithm 6, along with the pseudocode of Algorithm 7, imply the following invariant.

```
1:\(j\gets i\)
2:while\(|U_{j}|>\alpha k^{\prime}\)do
3:\(n_{j}\leftarrow|U_{j}|\) and \(n_{j}^{*}\gets 0\)
4:\((S_{j},C_{j},\sigma_{j},\nu_{j})\leftarrow\texttt{AlmostCover}(U_{j})\)
5:\(U_{j+1}\gets U_{j}\setminus C_{j}\)
6:\(j\gets j+1\)
7:endwhile
8:\(t\gets j\)
9:\(S_{t}\gets U_{t}\), \(C_{t}\gets S_{t}\) and \(\nu_{t}\gets 0\)
10: Assign each \(x\in C_{t}\) to itself (i.e., \(\sigma_{t}(x):=x\in S_{t}\))
11:\(S\leftarrow\cup_{j\in[t]}S_{i}\)
12: Let \(\sigma:U\to S\) be the assignment such that for all \(j\in[t]\) and \(x\in C_{j}\), we have \(\sigma(x)=\sigma_{j}(x)\) ```

**Algorithm 6**Insert\((j)\)

**Invariant 3.1**.: \(n_{i}^{*}\leq\tau n_{i}\) for all \(i\in[t]\). Here, \(n_{i}\) is the size of \(U_{i}\) just after the last rebuild of layer \(i\), and \(n_{i}^{*}\) is the number of updates in layer \(i\) since that last rebuild.

Intuitively, the above invariant ensures that the layers maintained by our dynamic algorithm remain _close_ to the layers of the static algorithm in Section 3.1. This is crucially exploited in the proofs of Lemma 3.2 (which helps us bound the update and query times) and Lemma 3.3 (which leads to the desired bound on the approximation ratio). We defer the proofs of these two lemmas to Appendix B.

**Lemma 3.2**.: _We always have \(t=\tilde{O}(1)\), where \(t\) denotes the number of layers maintained by our dynamic algorithm._```
1:for\(i=1,...,t\)do
2:if\(x\in U_{i}\)then
3: Remove \(x\) from \(U_{i}\), and set \(n_{i}^{*}\gets n_{i}^{*}+1\)
4:if\(x\in C_{i}\)then
5: Remove \(x\) from \(C_{i}\)
6:if\(x\in S_{i}\)then
7: Remove \(x\) from \(S_{i}\)
8:if\(\exists\,y\in\sigma_{i}^{-1}(x)\setminus\{x\}\)then
9: Pick any such \(y\) and place it into \(S_{i}\)
10: Set \(\sigma_{i}(z)\) to \(y\) for each \(z\in\sigma_{i}^{-1}(x)\)
11:endif
12:endif
13:endif
14:endif
15:endfor
16:Rebuild ```

**Algorithm 6**\(\mathtt{Delete}(x)\)

**Lemma 3.3**.: _The assignment \(\sigma\) maintained by our dynamic algorithm always satisfies \(\mathtt{cost}(\sigma)=O(1)\cdot\mathtt{opt}(U)\)._

**Answering a query:** Upon receiving a query, we consider a weighted \(k\)-median instance \((S,d,w)\), where each point \(x\in S\) receives a weight \(w(x):=|\sigma^{-1}(x)|\). Next, we compute a \(O(1)\)-approximate solution \(S^{*}\subseteq S\), with \(|S^{*}|\leq k\), to this weighted instance, so that \(\mathtt{cost}_{w}(S^{*},S)\leq O(1)\cdot\mathtt{opt}_{w}(S)\). We then return the centers in \(S^{*}\).

**Corollary 3.4**.: \(\mathtt{cost}(S^{*})=O(1)\cdot\mathtt{opt}(U)\)_._

Corollary 3.4 implies that our dynamic algorithm has \(O(1)\) approximation ratio. It holds because of Lemma 3.3, and its proof immediately follows from [19]. We delegate this proof to Appendix C.

**Corollary 3.5**.: _Our algorithm has \(\tilde{O}(k^{2})\) query time._

Proof (Sketch).: By Lemma 3.2, we have \(t=\tilde{O}(1)\). Since the dynamic algorithm maintains at most \(\tilde{O}(k)\) centers \(S_{i}\) in each layer \(i\) (follows from Invariant 3.1), we have \(|S|=\sum_{i\in[t]}|S_{i}|=\tilde{O}(k)\). Using appropriate data structures (see the proof of Claim 3.7), in \(O(1)\) time we can find the number of points assigned to any center \(x\in S\) under \(\sigma\) (given by \(|\sigma^{-1}(x)|=w(x)\)). Thus, the weighted \(k\)-median instance \((S,d,w)\) is of size \(\tilde{O}(k)\), and upon receiving a query we can construct the instance in \(\tilde{O}(k)\) time. We now run a static \(O(1)\)-approximation algorithm [27] on \((S,d,w)\), which returns the set \(S^{*}\) in \(\tilde{O}(k^{2})\) time. 

**Lemma 3.6**.: _Our algorithm has \(\tilde{O}(k)\) update time._

Corollaries 3.4, 3.5 and Lemma 3.6 imply Theorem 1.1. It now remains to prove Lemma 3.6.

### Proof of Lemma 3.6

We first bound the time taken to handle an update, excluding the time spent on rebuilding the layers.

**Claim 3.7**.: _Excluding the calls to_ Rebuild_, Algorithm 5 and Algorithm 6 both run in \(\tilde{O}(1)\) time._

Proof.: We first describe how our dynamic algorithm maintains the assignment \(\sigma\) implicitly. In each layer \(i\in[t]\), the assignment \(\sigma_{i}\) partitions the set \(C_{i}\) into \(|S_{i}|\)_clusters_\(\{\sigma_{i}^{-1}(x)\}_{x\in S_{i}}\). For each such cluster \(Z=\sigma_{i}^{-1}(x)\), we maintain: (1) a unique id, given by \(\texttt{id}(Z)\), (2) its center, given by \(\texttt{center}(\texttt{id}(Z)):=x\), and (3) its size, given by \(\texttt{size}(\texttt{id}(Z)):=|Z|\). For each point \(y\in Z\), we also maintain the id of the cluster it belongs to, given by \(\texttt{cluster}(y):=\texttt{id}(Z)\). Using these data structures, for any \(y\in C_{i}\) we can report in \(O(1)\) time the center \(\sigma_{i}(y)\), and we can also report the size of a cluster in \(O(1)\) time.

Recall that \(t=\tilde{O}(1)\) as per Lemma 3.2. Thus, Algorithm 5 takes \(\tilde{O}(1)\) time, excluding the call to Rebuild in Line 6. For Algorithm 6, the key thing to note is that using the data structures described above, Lines 8 - 10 can be implemented in \(O(1)\) time, by setting \(y\) as the new center of the cluster \(\sigma_{i}^{-1}(x)\) and by decreasing the size of the cluster by one. It follows that excluding the call to Rebuild in Line 16, Algorithm 6 also runs in \(\tilde{O}(1)\) time. 

**Claim 3.8**.: _A call to_ ConstructFromLayer\((i)\)_, as described in Algorithm 4, takes \(\tilde{O}(k\cdot|U_{i}|)\) time._

Proof (Sketch).: By Lemma 3.2, the **while** loop in Algorithm 4 runs for \(\tilde{O}(1)\) iterations. Hence, within a \(\tilde{O}(1)\) factor, the runtime of Algorithm 4 is dominated by the call to AlmostCover\((U_{j})\) in Line 4. Accordingly, we now consider Algorithm 2. As the ball \(B_{U^{\prime}}(X,r)\) can be computed in \(O(|X|\cdot|U^{\prime}|)\) time, using a binary search the value \(\nu^{\prime}\) (see Line 2) can be found in \(\tilde{O}(k\cdot|U^{\prime}|)\) time. The rest of the steps in Algorithm 2 also take \(\tilde{O}(k\cdot|U^{\prime}|)\) time. Thus, it takes \(\tilde{O}(k\cdot|U_{j}|)\) time to implement Line 4 of Algorithm 4. Hence, the total runtime of Algorithm 4 is \(\sum_{j\in[i,t]}\tilde{O}(k\cdot|U_{j}|)=\sum_{j\in[i,t]}\tilde{O}(k\cdot|U_{i }|)=\tilde{O}(k\cdot|U_{i}|)\). 

Define the potential \(\Phi:=\sum_{i\in[t]}n_{i}^{*}\). For each update in \(U\), the potential \(\Phi\) increases by at most \(t\), excluding the calls to Rebuild (see Algorithm 5 and Algorithm 6). Thus, by Lemma 3.2, each update increases the value of \(\Phi\) by at most \(\tilde{O}(1)\). Now, consider any call to ReconstructFromLayer\((i)\). Algorithm 7 and Invariant 3.1 ensure that just before this call, we had \(n_{i}^{*}\geq\tau n_{i}=\Omega(|U_{i}|)\), since \(\tau\) is a constant. Hence, because of Line 3 in Algorithm 4, during this call the value of \(\Phi\) decreases by at least \(\Omega(|U_{i}|)\). Claim 3.8, in contrast, implies that the time taken to implement this call is \(\tilde{O}(k\cdot|U_{i}|)\).

To summarize, each update in \(U\) creates \(\tilde{O}(1)\) units of potential, and the total time spent on the calls to Rebuild is at most \(\tilde{O}(k)\) times the decrease in the potential. Since the potential \(\Phi\) is always nonegative, it follows that the amortized time spent on the calls to Rebuild is \(\tilde{O}(k)\). This observation, along with Claim 3.7, implies Lemma 3.6.

## 4 Experimental Evaluation

Datasets.We conduct the empirical evaluation of our algorithm on five datasets from the UCI repository [17]: (1) Census[23] with \(2,458,285\) points of dimension \(68\), (2) KDD-Cup[32] containing \(311,029\) points of dimension \(74\), (3) Song[5] with \(515,345\) points of dimension \(90\), (4) Drift[33, 30] with \(13,910\) points of dimension \(129\), and (5) SIFT10M[17] with \(11,164,866\) points of dimension \(128\).3

Footnote 3: We are not aware of any dynamic dataset containing the sequence of arrivals and that is publicly available. We generated sequences of updates following a typical setting used in studies of dynamic algorithms (e.g., [16]).

We generate the dynamic instances that we use in our study as follows. We keep the first \(10,000\) points of each dataset, in the order that they appear in their file. This choice allows us to test against competing algorithms that are not as efficient, and at the same time captures the different practical aspects of the algorithms that we consider; as a sanity check, we perform an experiment on an instance that is an order of magnitude larger to confirm the scalability of our algorithm, and that the relative behavior in terms of the cost of the solutions remains the same among the tested algorithms.

We use the \(L_{2}\) distance of the embeddings of two points to compute their distance. To avoid zero distance without altering the structural properties of the instances, we add \(1/|U|\) to all distances, where \(U\) is the set of points of the instance.

Order of Updates.We obtain a dynamic sequence of updates following the sliding window approach. In this typical approach, one defines a parameter \(\kappa\) indicating the size of the window, and then "slides" the window over the static sequence of points in steps creating at each step \(i\) a point insertion of the \(i\)-th point (if there are more than \(i\) points) and a point deletion of the \((i-\kappa)\)-th point (if \(\kappa>i\)), until each point of the static instance is inserted once and deleted once. We set \(\kappa=2,000\), which results in our dynamic instances having at most \(2,000\) points at any point in time.

Algorithms.We compare our algorithm from Section 3 against the state-of-the-art algorithm for maintaining a solution to the \(k\)-median problem, which is obtained by dynamically maintaining a coreset for the same problem [20]. In particular, [20] presented an algorithm that can dynamically maintain an \(\epsilon\)-coreset of size \(O(\epsilon^{-2}k\operatorname{poly}(\log n,\log(1/\epsilon)))\) for the \(k\)-median and the \(k\)-means problems. Their algorithm has a worst-case update time of \(O(\epsilon^{-2}k^{2}\operatorname{poly}(\log n,\log(1/\epsilon)))\). To the best of our knowledge, this is the first implementation of the algorithm in [20]. For brevity, we refer to our algorithm as OurAlg, and the algorithm from [20] as HK.

Since the exact constants and thresholds required to adhere to the theoretical guarantees of [20] are impractically large, there is no obvious way to use the thresholds provided by the algorithm in practice without making significant modifications. In order to give a fair comparison of the two algorithms, we implemented each of them to have a single parameter controlling a trade-off between update time, query time, and the cost of the solution. OurAlg has a parameter \(\phi\) which we set to be the number of points sampled during the construction of a layer in Algorithm 2. In other words, we sample \(\phi\) points in Line 1 of Algorithm 2 instead of \(\alpha k^{\prime}\). We fix the constants \(\epsilon\) and \(\beta\) used by our algorithm (see Section 3) to \(0.2\) and \(0.5\) respectively. HK has a parameter \(\psi\) which we set to be the number of points sampled during the static coreset construction from [9] which is used as a black box, replacing the large threshold required to adhere to the theoretical guarantees. Since this threshold is replaced by a single parameter, it also replaces the other parameters used by HK, which are only used in this thresholding process. For the bicriteria algorithm required by this static coreset construction, we give the same implementation as the empirical evaluations in [9] and use kmeans++ with 2 iterations of Lloyd's.

For each of \(\phi\) and \(\psi\), we experimented with the values \(250,500,1000\). As the values of \(\phi\) and \(\psi\) are increased, the asymptotic update times and query times of the corresponding algorithms increase, while the cost of the solution decreases.

Setup.All of our code is written in Java and is available online.4 We did not use parallelization. We used a machine with 8 cores, a 2.9Ghz processor, and 16 GiB of main memory.

Footnote 4: https://github.com/martin-costa/NeurIPS23-dynamic-k-clustering

Results.We compare OurAlg\((\phi=500)\) against HK\((\psi=1000)\). We selected these parameters such that they obtain a good solution quality, without these parameters being unreasonably high w.r.t. the size of our datasets. Our experiments suggest that the solution quality produced by OurAlg is robust against different values of \(\phi\) (typically these versions of the algorithm differ by less than \(1\%\)), while that of HK is more sensitive (in several instances the differences are in the range of \(3-9\%\), while for KDD-Cup HK\((\psi=250)\) produces much worse solutions). The update time among the different versions of each algorithm do not differ significantly. Lastly, the query time of OurAlg is less sensitive compared to the query time of HK. Specifically, the average query time of OurAlg\((\phi=1000)\) is roughly \(3\) times slower than OurAlg\((\phi=250)\), while the average query time of HK\((\psi=1000)\) is \(11\) times slower compared to HK\((\psi=250)\). For OurAlg we selected the middle value \(\phi=500\). Since the solution quality of HK drops significantly when using smaller values of \(\psi\), we selected \(\psi=1000\) to prioritize for the quality of the solution produced by the algorithm. Since we did not observe significant difference in the behavior across the datasets, we tuned these parameters considering three or our datasets. We provide the relevant plots in Appendix E.4.

Update Time Evaluation.In Figure 1 (left) we plot the total update time over the sequence of updates for the dataset Song, and only for \(k=50\). OurAlg runs up to more than 2 orders of 

[MISSING_PAGE_FAIL:9]

a window size of \(\kappa=5,000\) and applied it to the first \(100,000\) points. As expected the relative performance in terms of solution cost and query time remains the same, while the gap in terms of update time grows significantly (HK\((\psi=1000)\) performs more than 3 orders of magnitude worse than \(\textsc{OurAlg}(\phi=500)\)). The corresponding plots and numbers are provided in the Appendix.

Conclusion of the Experiments.Our experimental evaluation shows the practicality of our algorithms on a set of five standard datasets used for the evaluation of dynamic algorithms in metric spaces. In particular, our experiments verify the theoretically superior update time of our algorithm w.r.t. to the state-of-the-art algorithm for the fully-dynamic \(k\)-median problem [20], where our algorithm performs orders of magnitude faster than HK. On top of that, the solution quality produced by our algorithm is better than HK in most settings, and the query times of the two algorithms are comparable. We believe that our algorithm is the best choice when it comes to efficiently maintaining a dynamic solution for the \(k\)-median problem in practice.

## References

* [1] Sara Ahmadian, Ashkan Norouzi-Fard, Ola Svensson, and Justin Ward. Better guarantees for k-means and euclidean k-median by primal-dual algorithms. _SIAM Journal on Computing_, 49(4):FOCS17-97, 2019.
* [2] Nir Ailon, Ragesh Jaiswal, and Claire Monteleoni. Streaming k-means approximation. _Advances in neural information processing systems_, 22, 2009.
* [3] MohammadHossein Bateni, Hossein Esfandiari, Hendrik Fichtenberger, Monika Henzinger, Rajesh Jayaram, Vahab Mirrokni, and Andreas Wiese. Optimal fully dynamic \(k\)-center clustering for adaptive and oblivious adversaries. _SODA_, 2023.
* [4] Jon Louis Bentley and James B Saxe. Decomposable searching problems i. static-to-dynamic transformation. _Journal of Algorithms_, 1(4):301-358, 1980.
* [5] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The million song dataset. 2011.
* [6] Sayan Bhattacharya, Silvio Lattanzi, and Nikos Parotsidis. Efficient and stable fully dynamic facility location. _Advances in neural information processing systems_, 2022.
* [7] Michele Borassi, Alessandro Epasto, Silvio Lattanzi, Sergei Vassilvitskii, and Morteza Zadimoghaddam. Sliding window algorithms for k-clustering problems. _Advances in Neural Information Processing Systems_, 33:8716-8727, 2020.
* November 3, 2022_, pages 462-473. IEEE, 2022.
* [9] Vladimir Braverman, Dan Feldman, and Harry Lang. New frameworks for offline and streaming coreset constructions. _CoRR_, abs/1612.00889, 2016.
* [10] Vladimir Braverman, Gereon Frahling, Harry Lang, Christian Sohler, and Lin F Yang. Clustering high dimensional dynamic data streams. In _International Conference on Machine Learning_, pages 576-585. PMLR, 2017.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & Song & Census & KDD-Cup & Drift & SIFT10M \\ \hline \(k=10\) & 0.575 & 0.586 & 2.629 & 0.577 & 0.572 \\ \(k=50\) & 0.568 & 0.577 & 1.888 & 0.570 & 0.564 \\ \(k=100\) & 0.560 & 0.573 & 1.543 & 0.564 & 0.558 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The mean ratio of query times for HK\((\psi=1000)\) divided by that of \(\textsc{OurAlg}(\phi=500)\), for the five datasets that we considered and for \(k\in\{10,50,100\}\).

* Braverman et al. [2016] Vladimir Braverman, Harry Lang, Keith Levin, and Morteza Monemizadeh. Clustering problems on sliding windows. In _Proceedings of the twenty-seventh annual ACM-SIAM symposium on Discrete algorithms_, pages 1374-1390. SIAM, 2016.
* Byrka et al. [2017] Jaroslaw Byrka, Thomas Pensyl, Bartosz Rybicki, Aravind Srinivasan, and Khoa Trinh. An improved approximation for k-median and positive correlation in budgeted optimization. _ACM Transactions on Algorithms (TALG)_, 13(2):1-31, 2017.
* Chan et al. [2018] TH Hubert Chan, Arnaud Guergin, and Mauro Sozio. Fully dynamic k-center clustering. In _Proceedings of the 2018 World Wide Web Conference_, pages 579-587, 2018.
* Charikar et al. [1999] Moses Charikar, Sudipto Guha, Eva Tardos, and David B Shmoys. A constant-factor approximation algorithm for the k-median problem. In _Proceedings of the thirty-first annual ACM symposium on Theory of computing_, pages 1-10, 1999.
* Charikar et al. [2003] Moses Charikar, Liadan O'Callaghan, and Rina Panigrahy. Better streaming algorithms for clustering problems. In _Proceedings of the thirty-fifth annual ACM symposium on Theory of computing_, pages 30-39, 2003.
* Cohen-Addad et al. [2019] Vincent Cohen-Addad, Niklas Oskar D Hjuler, Nikos Parotsidis, David Saulpic, and Chris Schwiegelshohn. Fully dynamic consistent facility location. _Advances in Neural Information Processing Systems_, 32, 2019.
* Dua and Graff [2017] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* Goranci et al. [2021] Gramoz Goranci, Monika Henzinger, Dariusz Leniowski, Christian Schulz, and Alexander Svozil. Fully dynamic \(k\)-center clustering in low dimensional metrics. In _Proceedings of the Symposium on Algorithm Engineering and Experiments, ALENEX 2021, Virtual Conference, January 10-11, 2021_, pages 143-153. SIAM, 2021.
* Guha et al. [2000] Sudipto Guha, Nina Mishra, Rajeev Motwani, and Liadan O'Callaghan. Clustering data streams. In _41st Annual Symposium on Foundations of Computer Science_, pages 359-366. IEEE Computer Society, 2000.
* Henzinger and Kale [2020] Monika Henzinger and Sagar Kale. Fully-dynamic coresets. In _28th Annual European Symposium on Algorithms, ESA 2020, September 7-9, 2020, Pisa, Italy (Virtual Conference)_, volume 173 of _LIPIcs_, pages 57:1-57:21, 2020.
* Huang and Vishnoi [2020] Lingxiao Huang and Nisheeth K. Vishnoi. Coresets for clustering in euclidean spaces: importance sampling is nearly optimal. In _Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC 2020, Chicago, IL, USA, June 22-26, 2020_, pages 1416-1429. ACM, 2020.
* Jain and Vazirani [2001] Kamal Jain and Vijay V Vazirani. Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and lagrangian relaxation. _Journal of the ACM (JACM)_, 48(2):274-296, 2001.
* Kohavi et al. [1996] Ron Kohavi et al. Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid. In _Kdd_, volume 96, pages 202-207, 1996.
* Lattanzi and Vassilvitskii [2017] Silvio Lattanzi and Sergei Vassilvitskii. Consistent k-clustering. In _International Conference on Machine Learning_, pages 1975-1984. PMLR, 2017.
* Liu et al. [2022] Quanquan C. Liu, Jessica Shi, Shangdi Yu, Laxman Dhulipala, and Julian Shun. Parallel batch-dynamic algorithms for k-core decomposition and related graph problems. In _34th ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)_, pages 191-204. ACM, 2022.
* Mettu [2002] Ramgopal R. Mettu. _Approximation Algorithms for NP-Hard Clustering Problems_. PhD thesis, Department of Computer Science, University of Texas at Austin, 2002.
* Mettu and Plaxton [2000] Ramgopal R. Mettu and C. Greg Plaxton. The online median problem. In _41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA_, pages 339-348. IEEE Computer Society, 2000.

* [28] Ramgopal R. Mettu and C. Greg Plaxton. Optimal time bounds for approximate clustering. In _UAI '02, Proceedings of the 18th Conference in Uncertainty in Artificial Intelligence_, pages 344-351. Morgan Kaufmann, 2002.
* [29] Krzysztof Nowicki and Krzysztof Onak. Dynamic graph algorithms with batch updates in the massively parallel computation model. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2939-2958. SIAM, 2021.
* [30] Irene Rodriguez-Lujan, Jordi Fonollosa, Alexander Vergara, Margie Homer, and Ramon Huerta. On the calibration of sensor arrays for pattern recognition using the minimal number of experiments. _Chemometrics and Intelligent Laboratory Systems_, 130:123-134, 2014.
* [31] Michael Shindler, Alex Wong, and Adam Meyerson. Fast and accurate k-means for large datasets. _Advances in neural information processing systems_, 24, 2011.
* [32] S.J. Stolfo, Wei Fan, Wenke Lee, A. Prodromidis, and P.K. Chan. Cost-based modeling for fraud and intrusion detection: results from the jam project. In _Proceedings DARPA Information Survivability Conference and Exposition. DISCEX'00_, volume 2, pages 130-144 vol.2, 2000.
* [33] Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ramon Huerta. Chemical gas sensor drift compensation using classifier ensembles. _Sensors and Actuators B: Chemical_, 166:320-329, 2012.

Extension to \(k\)-Means and \((k,p)\)-Clustering

As stated in [21, 8], while [28] only discusses the \(k\)-median problem, their construction can easily be modified to work for \(k\)-means clustering and further generalized to work for \((k,p)\)-clustering, where the \((k,p)\)-clustering problem is defined in the same way as \(k\)-median problem except that we want to minimize \(\sum_{x\in U}d(x,S)^{p}\) for some \(S\subseteq U\) of size at most \(k\). Note that \((k,1)\)-clustering and \((k,2)\)-clustering correspond to \(k\)-median and \(k\)-means respectively.

We define a \(\rho\)-metric space \((U,d)\) in the same way as a metric space except for relaxing the condition that \(d\) must satisfy the triangle inequality to the condition that \(d(x,y)\leq\rho(d(x,z)+d(z,y))\) for all \(x,y,z\in U\). Given a metric space \((U,d)\) and some \(p\geq 1\), the results in Section 6 of [9] can easily be used to show that \((U,d^{p})\) is a \(2^{p-1}\)-metric space, where \(d^{p}(x,y)\) is defined to be \(d(x,y)^{p}\).

We now show that the assignment \(\sigma\) maintained by our algorithm is \(O(\rho^{3})\)-approximate when \(U\) is a \(\rho\)-metric space (i.e. that \(\mathtt{cost}(\sigma)=O(\rho^{3})\cdot\mathtt{opt}(U)\)) and that the extraction technique of [19] can be generalized to \(\rho\)-metric spaces.

**Lemma A.1**.: _When the underlying space \(U\) is a \(\rho\)-metric space, the assignment \(\sigma\) maintained by our algorithm is \(O(\rho^{3})\)-approximate._

Proof.: By making the appropriate changes to the proofs of Lemma B.3 and Lemma B.4, we get generalizations of these lemmas to \(\rho\)-metric spaces, where the lemma statements are the same except for an extra \(\rho\) factor in the inequalities.

**Lemma A.2**.: _Given any positive \(\xi\), there exists a sufficiently large choice of \(\alpha\) such that \(\nu_{i}\leq 2\rho\cdot\mu_{\gamma}(U_{i}^{\text{\tiny{(DDL)}}})\) for each \(i\in[t-1]\) with probability at least \(1-e^{-\xi k^{\prime}}\)._

**Lemma A.3**.: _Given metric subspaces \(U_{1}\) and \(U_{2}\) of \(U\) such that \(|U_{1}\oplus U_{2}|\leq\epsilon\gamma|U_{1}|\), we have that \(\mu_{\gamma}(U_{1})\leq 2\rho\cdot\mu_{\gamma^{*}}(U_{2})\)._

These two lemmas immediately imply the following generalization of Lemma B.5.

**Lemma A.4**.: \(\nu_{i}\leq 4\rho^{2}\cdot\mu_{\gamma^{*}}(U_{i})\) _for each \(i\in[t-1]\) with probability at least \(1-e^{-\xi k^{\prime}}\)._

The upper bound on \(\mathtt{cost}(\sigma)\) given in Lemma B.6 can be generalized by noticing that \(\mathtt{cost}(\sigma,C_{i})\leq 2\rho\nu_{i}|C_{i}|\) for all \(i\in[t-1]\), which us that

\[\mathtt{cost}(\sigma)\leq\sum_{i=1}^{t}2\rho\nu_{i}|C_{i}|.\]

The lower bound on \(\mathtt{opt}(U)\) given in Lemma B.10 holds for \(\rho\)-metric spaces with no modifications. Hence, we get that with probability at least \(1-e^{-\xi k^{\prime}}\) we have that

\[\mathtt{cost}(\sigma)\leq\sum_{i=1}^{t}2\rho\nu_{i}|C_{i}|\leq\sum_{i=1}^{t}8 \rho^{3}\mu_{i}|C_{i}|\leq\frac{16\rho^{3}r}{1-\gamma^{*}}\ \mathtt{cost}(S).\]

By making the appropriate modifications to the proof of Theorem C.1, we can extend this theorem to work for \(\rho\)-metric spaces. In particular, we can obtain a proof of Theorem A.5 by taking the proof of Theorem C.1 and adding extra \(\rho\) factors whenever the triangle inequality is applied.

**Theorem A.5**.: _Given a \(\phi\)-approximate \(m\)-assignment \(\pi:U\to U\), any \(\psi\)-approximate solution to the weighted \(k\)-median instance \((\pi(U),d,w)\), where each point \(x\in\pi(U)\) receives weight \(w(x):=|\pi^{-1}(x)|\), is also a \(\big{(}\phi\rho+2(1+\phi)\psi\rho^{3}\big{)}\)-approximate solution to the \(k\)-median instance \((U,d)\) where \(U\) is a \(\rho\)-metric space._

Since the algorithm in [27] is \(O(1)\)-approximate on \(O(1)\)-metric spaces, it immediately follows by applying Theorem A.5 and Lemma A.1 that our algorithm maintains a \(O(1)\)-approximate solution to the \(k\)-median problem on \((U,d^{p})\) for \(p=O(1)\). Since the \(k\)-median problem on \((U,d^{p})\) is exactly the \((k,p)\)-clustering problem on \((U,d)\), it follows that our algorithm generalizes to solve instances of \((k,p)\)-clustering in metric spaces.

Proofs of Lemma 3.2 and Lemma 3.3

Throughout this section, we fix \(\gamma\) to be any real such that \(\beta<\gamma<1\) and \(\epsilon\) to be any real such that \(0<\epsilon<\min\{\frac{1-\gamma}{2\gamma},1\}\). Let \(\beta^{*}\) and \(\gamma^{*}\) denote \(\beta(1-\epsilon)\) and \(\gamma(1+2\epsilon)\) respectively.

### Proof of Lemma 3.2

We first prove Lemma B.1, which shows that the sizes of the sets \(U_{i}\) decrease exponentially with \(i\).

**Lemma B.1**.: _For all \(i\in[t-1]\), \(|U_{i+1}|\leq(1-\beta^{*})|U_{i}|\)._

Proof.: Consider the ratio \(|U_{i+1}|/|U_{i}|\). Since \(U_{i+1}\subseteq U_{i}\) and \(U_{i+1}\) is reconstructed every time \(U_{i}\) is reconstructed, it follows that \(|U_{i+1}|/|U_{i}|\) is at most \((n_{i+1}+\ell)/(n_{i}+\ell-\ell^{\prime})\), where \(n_{j}\) is the size of \(U_{j}\) at the time it was last reconstructed and \(\ell\) and \(\ell^{\prime}\) are the number of insertions and deletions that have occurred since the last time \(U_{i+1}\) was reconstructed respectively. By Lemma B.2, we get that this expression is upper bounded by \((n_{i+1}+\tau n_{i+1})/n_{i}\). Now we can observe that

\[\frac{|U_{i+1}|}{|U_{i}|}\leq\frac{n_{i+1}+\ell}{n_{i}+\ell-\ell^{\prime}}\leq \frac{n_{i+1}+\tau n_{i+1}}{n_{i}}\leq\frac{n_{i+1}}{n_{i}}+\tau\leq(1-\beta)+ \epsilon\beta=1-\beta^{*},\]

where we use the facts that \(n_{i+1}\leq(1-\beta)n_{i}\) and \(\tau\leq\epsilon\beta\) in the final inequality.

**Lemma B.2**.: _Given some integer \(i\in[t-1]\), let \(\ell\) and \(\ell^{\prime}\) be the number of insertions and deletions that have occurred since the last time \(U_{i+1}\) was reconstructed respectively. Then we have that_

\[\frac{n_{i+1}+\ell}{n_{i}+\ell-\ell^{\prime}}\leq\frac{n_{i+1}+\tau n_{i+1}}{ n_{i}}.\]

Proof.: First, note that \((n_{i+1}+\ell)/(n_{i}+\ell-\ell^{\prime})\leq(n_{i+1}+\ell)/(n_{i}-\ell^{ \prime})\). Now, given some reals \(A\geq a\geq 0\) and \(0\leq N\leq A-a\), we define a function \(f:[0,1]\rightarrow\mathbb{R}\) by \(f(x)=(a+xN)/(A-(1-x)N)\). The derivative of \(f\) is \(-N(a-A+N)/((x-1)N+A)^{2}\) and is non-negative for all \(x\in[0,1]\). Hence, \(f(x)\leq f(1)\) for all \(x\in[0,1]\).

By setting \(A=n_{i}\), \(a=n_{i+1}\), \(N=\ell+\ell^{\prime}\) and noting that \(\ell+\ell^{\prime}\leq\tau n_{i+1}\) by Invariant 3.1 and \(n_{i+1}\leq(1-\beta)n_{i}\), we get that

\[\ell+\ell^{\prime}\leq\tau n_{i+1}\leq\beta n_{i+1}=(1+\beta)n_{i+1}-n_{i+1} \leq(1+\beta)(1-\beta)n_{i}-n_{i+1}\leq n_{i}-n_{i+1},\]

and hence it follows that

\[\frac{n_{i+1}+\ell}{n_{i}-\ell^{\prime}}=f\left(\frac{\ell}{\ell+\ell^{ \prime}}\right)\leq f(1)=\frac{n_{i+1}+\ell+\ell^{\prime}}{n_{i}}\leq\frac{n_ {i+1}+\tau n_{i+1}}{n_{i}}.\]

Since \(|U_{1}|=|U|\), \(|U_{t-1}|>(1-\tau)\alpha k^{\prime}=\Omega(k)\), and \(\beta^{*}\) is a constant, it follows from Lemma B.1 that \(t=O\left(\log(|U|/k)\right)=\tilde{O}(1)\).

### Proof of Lemma 3.3

Bounding the Radii \(\nu_{i}\) (Lemma B.5).Let \(U_{i}^{\text{OLD}}\) denote the state of the \(i\)th layer the last time it was reconstructed for \(i\in[t]\). We now use the following crucial lemma which is analogous to Lemma 4.3.3 in [26].

**Lemma B.3**.: _Given any positive \(\xi\), there exists a sufficiently large choice of \(\alpha\) such that \(\nu_{i}\leq 2\mu_{\gamma}(U_{i}^{\text{OLD}})\) for each \(i\in[t-1]\) with probability at least \(1-e^{-\xi k^{\prime}}\)._

Henceforth, we fix some positive \(\xi\) and sufficiently large \(\alpha\) such that Lemma B.3 holds.

**Lemma B.4**.: _Given metric subspaces \(U_{1}\) and \(U_{2}\) of \(U\) such that \(|U_{1}\oplus U_{2}|\leq\epsilon\gamma|U_{1}|\), we have that \(\mu_{\gamma}(U_{1})\leq 2\mu_{\gamma^{*}}(U_{2})\).5_

Footnote 5: \(\oplus\) denotes symmetric difference, i.e. \(U_{1}\oplus U_{2}=(U_{1}\setminus U_{2})\cup(U_{2}\setminus U_{1})\).

Proof.: First, note that \((n_{i+1}+\ell)/(n_{i}+\ell-\ell^{\prime})\leq(n_{i+1}+\ell)/(n_{i}-\ell^{ \prime})\). Now, given some reals \(A\geq a\geq 0\) and \(0\leq N\leq A-a\), we define a function \(f:[0,1]\rightarrow\mathbb{R}\) by \(f(x)=(a+xN)/(A-(1-x)N)\). The derivative of \(f\) is \(-N(a-A+N)/((x-1)N+A)^{2}\) and is non Proof.: Let \(X\) be a subset of \(U_{2}\) of size \(k\) such that \(\nu_{\gamma(1+2\epsilon)}(X,U_{2})=\mu_{\gamma(1+2\epsilon)}(U_{2})\), \(\rho=\mu_{\gamma(1+2\epsilon)}(U_{2})\), and \(A=B_{U_{1}}(X,\rho)\). Now note that

\[|A| =|B_{U_{1}\cup U_{2}}(X,\rho)\setminus B_{U_{2}\setminus U_{1}}(X,\rho)|\] \[\geq|B_{U_{2}}(X,\rho)|-|B_{U_{2}\setminus U_{1}}(X,\rho)|\] \[\geq\gamma(1+2\epsilon)|U_{2}|-|U_{2}\setminus U_{1}|\] \[\geq\gamma(1+2\epsilon)|U_{2}|-\epsilon\gamma|U_{1}|\] \[\geq\gamma(1+2\epsilon)\left(|U_{1}|-\epsilon\gamma|U_{1}|\right) -\epsilon\gamma|U_{1}|\] \[=\gamma|U_{1}|+\epsilon\gamma(1-\gamma(1+2\epsilon))|U_{1}|\] \[\geq\gamma|U_{1}|.\]

Since there also exists a subset \(Y\subseteq A\) of size \(k\) such that \(A\subseteq B_{U_{1}}(Y,2\rho)\), it follows that \(\nu_{\gamma}(Y,U_{1})\leq 2\rho\). Hence, \(\mu_{\gamma}(U_{1})\leq\nu_{\gamma}(Y,U_{1})\leq 2\mu_{\gamma(1+2\epsilon)}(U_{ 2})\). 

**Lemma B.5**.: \(\nu_{i}\leq 4\mu_{\gamma^{*}}(U_{i})\) _for each \(i\in[t-1]\) with probability at least \(1-e^{-\xi k^{\prime}}\)._

Proof.: For each \(i\in[t-1]\), \(|U_{i}\oplus U_{i}^{\text{\tiny{OLD}}}|\leq\tau|U_{i}^{\text{\tiny{OLD}}}|\) since, by Invariant 3.1, at most \(\tau|U_{i}^{\text{\tiny{OLD}}}|\) points have been inserted or deleted from \(U_{i}\) since it was last reconstructed. Noticing that \(\tau\leq\epsilon\gamma\), we can see that

\[|U_{i}\oplus U_{i}^{\text{\tiny{OLD}}}|\leq\epsilon\gamma|U_{i}^{\text{\tiny{ OLD}}}|.\]

By now applying Lemma B.4 it follows that \(\mu_{\gamma}(U_{i}^{\text{\tiny{OLD}}})\leq 2\mu_{\gamma^{*}}(U_{i})\). The lemma follows by combining this result with Lemma B.3. 

**Upper Bounding \(\texttt{cost}(\sigma)\) (Lemma B.6)**.: \[\texttt{cost}(\sigma)\leq\sum_{i=1}^{t}2\nu_{i}|C_{i}|.\]

Proof.: We first note that for all \(i\in[t-1]\), \(\texttt{cost}(\sigma,C_{i})\leq 2\nu_{i}|C_{i}|\). This follows directly from the fact that each point \(x\) in \(C_{i}\) is assigned to some point \(y\in C_{i}\) such that \(d(x,y)\leq 2\nu_{i}\). Since the \(C_{i}\) partition \(U\) and \(\texttt{cost}(\sigma,C_{i})=0\), we get:

\[\texttt{cost}(\sigma)=\sum_{i=1}^{t}\texttt{cost}(\sigma,C_{i})\leq\sum_{i=1} ^{t}2\nu_{i}|C_{i}|.\]

Lower Bounding \(\texttt{opt}(U)\) (Lemma B.10).Let \(r\) denote \(\lceil\log_{1-\beta^{*}}\frac{1-\gamma^{*}}{3}\rceil\) and for each \(i\in[t]\) let \(\mu_{i}\) denote \(\mu_{\gamma^{*}}(U_{i})\).

For the rest of this subsection we fix an arbitrary \(S\subseteq U\) of size \(k\). For each \(i\in[t]\), let \(F_{i}\) denote the set \(\{x\in U_{i}\mid d(x,S)\geq\mu_{i}\}\), and for any integer \(m>0\), let \(F_{i}^{m}\) denote \(F_{i}\setminus(\cup_{j>0}F_{i+jm})\) and \(G_{i,m}\) denote the set of all integers \(j\in[t]\) and \(j\equiv i\pmod{m}\).

**Lemma B.7**.: _Given some \(i\in[t]\) and a subset \(X\subseteq F_{i}\), we have that \(|F_{i}|\geq(1-\gamma^{*})|U_{i}|\) and \(\texttt{cost}(S,X)\geq\mu_{i}|X|\)._

Proof.: It follows directly from the definition of \(\mu_{i}\) that we have that \(|F_{i}|\geq(1-\gamma^{*})|U_{i}|\). By the definition of \(F_{i}\), we have that \(\texttt{cost}(S,X)=\sum_{x\in X}d(x,S)\geq\mu_{i}|X|\). 

The following lemma is proven in [26].

**Lemma B.8** ([26], Lemma 4.3.8).: _Given integers \(\ell\in[t]\) and \(m>0\), we have that_

\[\texttt{cost}(S,\cup_{i\in G_{\ell,m}}F_{i}^{m})\geq\sum_{i\in G_{\ell,m}}\mu_ {i}|F_{i}^{m}|.\]

**Lemma B.9**.: _For all \(i\in[t-1]\), we have that \(|F_{i}^{r}|\geq\frac{1}{2}|F_{i}|\)._Proof.: We first note that for all \(i\in[t-r]\), we have that \(|F_{i+r}|\leq\frac{1}{3}|F_{i}|\). This follows from the fact that

\[|F_{i+r}|\leq|U_{i+r}|\leq(1-\beta^{*})^{r}|U_{i}|\leq\frac{(1-\beta^{*})^{r}}{1- \gamma^{*}}|F_{i}|\leq\frac{1}{3}|F_{i}|,\]

where the first inequality follows from the fact that \(F_{i+r}\subseteq U_{i+r}\), the second inequality follows from Lemma B.1, the third inequality follows from Lemma B.7, and the fourth inequality follows from the definition of \(r\). We now get that

\[|F_{i}^{r}|=|F_{i}\setminus\cup_{j>0}F_{i+jr}|\geq|F_{i}|-\sum_{j>0}\frac{1}{ 3^{j}}|F_{i}|\geq\frac{1}{2}|F_{i}|.\]

**Lemma B.10**.: \[\mathtt{cost}(S)\geq\frac{1-\gamma^{*}}{2r}\sum_{i=1}^{t}\mu_{i}|C_{i}|.\]

Proof.: Let \(\ell=\arg\max_{0\leq\ell<r}\{\sum_{i\in G_{\ell,r}}\mu_{i}|F_{i}^{r}|\}\). Then we have that

\[\mathtt{cost}(S) \geq\mathtt{cost}(S,\cup_{i\in G_{\ell,r}}F_{i}^{r})\geq\sum_{i \in G_{\ell,r}}\mu_{i}|F_{i}^{r}|\geq\frac{1}{r}\sum_{i=1}^{t}\mu_{i}|F_{i}^{ r}|\geq\frac{1}{2r}\sum_{i=1}^{t}\mu_{i}|F_{i}|\] \[\geq\frac{1-\gamma^{*}}{2r}\sum_{i=1}^{t}\mu_{i}|U_{i}|\geq\frac{ 1-\gamma^{*}}{2r}\sum_{i=1}^{t}\mu_{i}|C_{i}|.\]

The second inequality follows from Lemma B.8, the third inequality from averaging and the choice of \(\ell\), the fourth inequality from Lemma B.9, and the fifth inequality from Lemma B.7. 

Proof of Lemma 3.3.It follows that with probability at least \(1-e^{-\xi k^{\prime}}\) we have that

\[\mathtt{cost}(\sigma)\leq\sum_{i=1}^{t}2\nu_{i}|C_{i}|\leq\sum_{i=1}^{t}8\mu _{i}|C_{i}|\leq\frac{16r}{1-\gamma^{*}}\,\mathtt{cost}(S)\]

for any set \(S\subseteq U\) of size \(k\). Hence, we have that

\[\mathtt{cost}(\sigma)\leq\frac{16r}{1-\gamma^{*}}\,\mathtt{opt}(U).\]

## Appendix C Proof of Corollary 3.4

In order to prove this corollary, we apply the extraction technique presented in [28] (with full details appearing in [26]) which is a slight generalization of the techniques from [19]. In particular, we use the following theorem which follows as an immediate corollary of Theorem 6 in [26]. For completeness, we provide a proof of this theorem.

**Theorem C.1**.: _Given a \(\phi\)-approximate \(m\)-assignment \(\pi:U\to U\), any \(\psi\)-approximate solution to the weighted \(k\)-median instance \((\pi(U),d,w)\), where each point \(x\in\pi(U)\) receives weight \(w(x):=|\pi^{-1}(x)|\), is also a \((\phi+2(1+\phi)\psi)\)-approximate solution to the \(k\)-median instance \((U,d)\)._

Proof.: Let \(S^{*}\) be a solution to the weighted \(k\)-median instance \((\pi(U),d,w)\) and let \(S\) be an optimal solution to the \(k\)-median instance \((U,d)\). Let \(\phi\) and \(\psi\) be constants such that \(\mathtt{cost}(\pi,U)\leq\phi\cdot\mathtt{opt}(U)\) and \(\mathtt{cost}_{w}(S^{*},\pi(U))\leq\psi\cdot\mathtt{opt}_{w}(\pi(U))\). We now show that \(\mathtt{cost}(S^{*},U)=O(1)\cdot\mathtt{opt}(U)\). We first note that

\[\mathtt{cost}(S^{*},U) =\sum_{x\in U}d(x,S^{*})\] \[\leq\sum_{x\in U}d(x,\pi(x))+\sum_{y\in\pi(U)}w(y)\cdot d(y,S^{*})\] \[=\mathtt{cost}(\pi,U)+\mathtt{cost}_{w}(S^{*},\pi(U))\] \[\leq\phi\cdot\mathtt{opt}(U)+\mathtt{cost}_{w}(S^{*},\pi(U)).\]Now note that, for any \(X\subseteq U\) of size at most \(k\), there exists some \(Y\subseteq\pi(U)\) of size at most \(k\) such that \(\mathtt{cost}_{w}(Y,\pi(U))\leq 2\cdot\mathtt{cost}_{w}(X,\pi(U))\). Since \(\mathtt{cost}_{w}(S^{*},\pi(U))\leq\psi\cdot\mathtt{cost}_{w}(Y,\pi(U))\) for all \(Y\subseteq\pi(Y)\) of size at most \(k\), we get the following.

\[\mathtt{cost}_{w}(S^{*},\pi(U)) \leq 2\psi\cdot\mathtt{cost}_{w}(S,\pi(U))\] \[=2\psi\cdot\sum_{y\in\pi(U)}w(y)\cdot d(y,S)\] \[=2\psi\cdot\sum_{x\in U}d(\pi(x),S)\] \[\leq 2\psi\cdot\sum_{x\in U}d(x,\pi(x))+2\psi\cdot\sum_{x\in U}d( x,S)\] \[=2\psi\cdot\mathtt{cost}(\pi,U)+2\psi\cdot\mathtt{opt}(U)\] \[\leq 2(1+\phi)\psi\cdot\mathtt{opt}(U).\]

By combining these two chains of inequalities, we get that

\[\mathtt{cost}(S^{*},U)\leq\phi\cdot\mathtt{opt}(U)+\mathtt{cost}_{w}(S^{*}, \pi(U))\leq(\phi+2(1+\phi)\psi)\cdot\mathtt{opt}(U).\]

It immediately follows that we can get a \(O(1)\)-approximate solution to the instance \((U,d)\) by running a static weighted \(k\)-median algorithm on the instance \((\sigma(U),d,w)\).

## Appendix D Lower Bounds on Update and Query Time

In the static (i.e. non-dynamic) setting, the \(k\)-median problem is defined as follows: given a metric space \(U\), return a set \(S\) of at most \(k\) points from \(U\) which minimizes the value of \(\sum_{x\in S}d(x,S)\). The following lower bound for the static \(k\)-median problem is proven by Mettu in [26].

**Theorem D.1**.: _Any \(O(1)\)-approximate randomized (static) algorithm for the \(k\)-median problem, which succeeds with even negligible probability, runs in time \(\Omega(nk)\)._

Informally, the proof of this lower bound is obtained by constructing, for each \(\delta>0\), an input distribution of metric spaces (with polynomially bounded aspect ratio) on which no deterministic algorithm for the \(k\)-median problem succeeds with probability more than \(\delta\). Theorem D.1 then follows by an application of Yao's minmax principle.

We can use this lower bound from the static setting in order to get a lower bound for the dynamic setting. First note that any incremental algorithm for \(k\)-median with amortized update time \(u(n,k)\) and query time \(q(n,k)\) can be used to construct a static algorithm for the \(k\)-median problem with running time \(n\cdot u(n,k)+q(n,k)\) by inserting each point in the input metric space \(U\) followed by a solution query. Hence, by Theorem D.1, we must have that \(n\cdot u(n,k)+q(n,k)=\Omega(nk)\). Now assume that some incremental algorithm for \(k\)-median has query time \(\tilde{O}(\mathrm{poly}(k))\). If this algorithm also has an amortized update time of \(\tilde{o}(k)\), then for the range of values of \(k\) where \(q(n,k)=\tilde{o}(nk)\), it follows that \(\tilde{o}(nk)\) is \(\Omega(nk)\), giving a contradiction. Hence, the amortized update time must be \(\tilde{\Omega}(k)\) and Theorem D.2 follows.

**Theorem D.2**.: _Any \(O(1)\)-approximate incremental algorithm for the \(k\)-median problem with \(\tilde{O}(\mathrm{poly}(k))\) query time must have \(\tilde{\Omega}(k)\) amortized update time._

It follows that the update time of our algorithm is optimal up to polylogarithmic factors.

[MISSING_PAGE_EMPTY:18]

Figure 4: The cumulative update time for the different algorithms, on the KDD-Cup dataset for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

Figure 5: The cumulative update time for the different algorithms, on the Drift dataset for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

### Solution Cost Evaluation

Figure 6: The cumulative update time for the different algorithms, on the SIFT10M dataset for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

Figure 7: The solution cost by the different algorithms, on Song for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

Figure 8: The solution cost by the different algorithms, on Census for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

Figure 9: The solution cost by the different algorithms, on KDD-Cup for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

Figure 11: The solution cost by the different algorithms, on SJFT10M for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

Figure 10: The solution cost by the different algorithms, on Drift for \(k=10\) (top left), \(k=50\) (top right), \(k=100\) (bottom).

### Query Time Evaluation

### Parameter Tuning

#### e.4.1 Update Time

Figure 12: The cumulative update time for different parameters of OurAlg and HK, for \(k=50\), on datasets Song (top left), Census (top right), and KDD-Cup (bottom).

\begin{table}
\begin{tabular}{l r r r r r r r r r} \hline \hline  & \multicolumn{2}{c}{Song} & \multicolumn{2}{c}{Census} & \multicolumn{2}{c}{KDD-Cup} & \multicolumn{2}{c}{Drift} & \multicolumn{2}{c}{SIFT10M} \\ \cline{2-11} \cline{3-11}  & OurAlg & HK & OurAlg & HK & OurAlg & HK & OurAlg & HK & OurAlg & HK \\ \hline \(k=10\) & 0.569 & 0.327 & 0.478 & 0.280 & 0.069 & 0.176 & 0.729 & 0.421 & 0.732 & 0.419 \\ \(k=50\) & 0.610 & 0.347 & 0.511 & 0.295 & 0.075 & 0.141 & 0.784 & 0.447 & 0.795 & 0.448 \\ \(k=100\) & 0.665 & 0.373 & 0.552 & 0.317 & 0.085 & 0.131 & 0.857 & 0.483 & 0.866 & 0.483 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The average query times for the algorithm OurAlg\((\phi=500)\) and HK\((\psi=1000)\) (we omit the parameter value from the table to simplify the presentation), on the different datasets that we consider and for \(k\in\{10,50,100\}\).

[MISSING_PAGE_EMPTY:24]

### Randomized Order of Updates

#### e.5.1 Update Time

#### e.5.2 Solution Cost

Figure 14: The cumulative update time for different parameters of OurAlg and HK, for \(k=50\), over a sequence of updates given by a randomized order of the points in the dataset, on the datasets Song (top left), Census (top right), and KDD-Cup (bottom).

Figure 15: The solution cost for different parameters of OurAlg and HK, for \(k=50\), over a sequence of updates given by a randomized order of the points in the dataset, on the datasets Song (top left), Census (top right), and KDD-Cup (bottom).

#### e.5.3 Query Time

### Larger Experiment

The average query times for OurAlg\((\phi=500)\) and HK\((\psi=1000)\) while handling this longer sequence of updates were \(0.416\) and \(0.225\) respectively.

Figure 16: The total update time for OurAlg\((\phi=500)\) and HK\((\psi=1000)\), on the larger instance derived from KDD-Cup, for \(k=50\).

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Song & Census & KDD-Cup \\ \hline HK\((\psi=250)\) & 0.025 & 0.021 & 0.014 \\ HK\((\psi=500)\) & 0.086 & 0.073 & 0.050 \\ HK\((\psi=1000)\) & 0.292 & 0.247 & 0.173 \\ OurAlg\((\phi=250)\) & 0.225 & 0.185 & 0.062 \\ OurAlg\((\phi=500)\) & 0.440 & 0.364 & 0.100 \\ OurAlg\((\phi=1000)\) & 0.723 & 0.605 & 0.165 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The average query times for the algorithm OurAlg and HK with different parameters, for \(k=50\), over a sequence of updates given by a randomized order of the points in each of the datasets that we consider.

Figure 17: The solution cost produced by OurAlg\((\phi=500)\) and HK\((\psi=1000)\) two algorithms, on the larger instance derived from KDD-Cup, for \(k=50\).