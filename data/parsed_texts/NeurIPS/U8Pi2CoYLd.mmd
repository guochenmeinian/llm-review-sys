# Predicting Electricity Consumption with Random Walks on Gaussian Processes

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We consider time-series forecasting problems where data is scarce, difficult to gather, or induces a prohibitive computational cost. As a first attempt, we focus on short-term electricity consumption in France, which is of strategic importance for energy suppliers and public stakeholders. The complexity of this problem and the many levels of geospatial granularity motivate the use of an ensemble of Gaussian Processes (GPs). Whilst GPs are remarkable predictors, they are computationally expensive to train, which calls for a frugal few-shot learning approach. By taking into account performance on GPs trained on a dataset and designing a random walk on these, we mitigate the training cost of our entire Bayesian decision-making procedure. We introduce our algorithm called Domino (ranDOM walk on gaussIaN prOcesses) and present numerical experiments to support its merits.

## 1 Introduction

Forecasting time series is at the centre of machine learning (ML). We focus on problem settings where we might have sparse data, limited compute capacity or unseen scenarios. We instantiate this problem in the setting of short-term electricity consumption prediction, and focus on doing this at the scale of France. For energy suppliers and public stakeholders, the necessity is to be able to predict consumption even in extreme events, such as a heat or cold wave, which can lead to large variations in consumption, possibly at a fairly high granularity.

These scenarios are also characterised by the fact that they do not have as much data as more classical time series forecasting scenarios such as stock price modelling, and therefore deep learning methods which have been a huge part of the recent artificial intelligence (AI) boom might not be as appropriate as they are unable to forecast as efficiently when there is little training data.

For time series forecasting, GPs are well-adapted as they natively quantify uncertainty. However, their performance is indexed on the size of the training data and they are computationally expensive to train. Recently, Few-Shot Learning (FSL) for time series prediction has gained attention both from theoretical [20] and applied perspectives [21], to help mitigate the costs of training. This work is the start of a series of analyses on French regional short-term electricity consumption. We take a FSL approach to training GPs, using a set of GPs trained on synthetic data in a first instance, with the natural next step being with actual electricity consumption data, available at https://www.rte-france.com/en/eco2mix/download-indicators.

We describe our algorithm, called Domino, in Section 2 and in Section 3, we illustrate its performance on a synthetic dataset. We briefly discuss preliminary results and lay down ideas for future works, with the aim to use this present work as a stepping stone towards broader time series forecasting problems where data is scarce, difficult to gather or induces a prohibitive cost.

## 2 Methodology

Due to the stochastic nature of the underlying phenomenon and its periodicity, we use GPs which quantify uncertainty and handle unseen scenarios. We refer to Rasmussen and Williams (2006) for a complete reference on GPs.

Notation.We adopt the following conventional notations to define our problem statement. We model _1 time series_, where \(i\in\{1,2,\ldots,I\}\) is an indicator of the time series used as a subscript. The time series all have \(N\in\mathbb{N}\) entries. The inputs are vectors \(\mathbf{t_{i}}=[t_{i1},t_{i2},\ldots,t_{iN}]\in\mathbb{R}^{N},\forall i\in\{i \}_{i=1}^{I}\). The outputs are \(\mathbf{y_{i}}=[y_{i1},y_{i2},\ldots,y_{iN}]=y_{i}(\mathbf{t})=[y_{i}(t_{1}),y _{i}(t_{2}),\ldots,y_{i}(t_{N})]\in\mathbb{R}^{N}\).

For each time series \(i\in\{i\}_{i=1}^{I}\), we look for the function \(f_{i}:\mathbf{t}\mapsto\mathbf{y_{i}}+\epsilon_{i}(\mathbf{t})\), where \(\epsilon_{i}\sim\mathcal{N}(0,\sigma^{2})\) with \(\sigma\in\mathbb{R}^{N}\). To share knowledge between time series, we leverage an algorithm whose tasks share a common mean.

### Existing work: the MAGMA algorithm

The Multi-tAsk GPs with common MeAn (MAGMA) algorithm [introduced by Leroy et al., 2022] shares knowledge between time series which are modelled by the same GP - this predicts unseen time series by using the common mean, which saves training resources and enables a more accurate prediction. The model is trained with an EM algorithm. Starting from an initialisation, in turn the hyperparameters given a distribution (E-step) and the distribution given the hyperparameters (M-step) are optimised. The optimisation can be made from the values learned at the previous step.

We refer to Leroy et al. (2022) for full explanations of the algorithm and its proof. The MAGMA algorithm is implemented in the MAGMAClustR package (https://arthurlerroy.github.io/MagmaClustR/). Whilst MAGMA is a powerful predictor, it is computationally expensive. In sparse computational resource settings, this limits its applications. We look to sample the GPs output by the algorithm to transfer knowledge to unseen time series with a more frugal approach, _i.e._, by leveraging much less data.

### Our Algorithm: a Random Walk on Gaussian Processes (Domino)

The Domino (standing for ranDOM walk on GaussIaN PrOcess) algorithm takes the output of the MAGMA model and samples these GPs. A random walk switches between the sampled time series at each time point, following a probability for each time series. After each walk, the random walk's performance with respect to each sampled time series is evaluated. Given this, and how often each time series has been sampled during the random walk, the performance and weights of the time series are updated. Until a maximum number of epochs is reached, until the random walk and sampled time series have a Kullback-Leibler (KL) divergence which is lower than a chosen \(\delta\), or until a maximum number of samples over the \(\delta\) threshold have a difference to the threshold with a lower standard deviation than the standard deviation of the in lying time series points and \(\delta\), the walk is repeated with the updated performances serving as a new probability at each epoch.

Notation.We superscript \(w\) the current epoch to identify the information which is specific to it. The performance of the sampled time series at the current epoch is \(p^{w}\). The random walk for the current epoch is \(\mathbf{y}^{w}\in\mathbb{R}^{N}\). The time series sampled at each stage of the random walk for the current epoch is \(\mathbf{z}^{w}\in\mathbb{R}^{N}\). Let \(P(f_{i})\) be the performance of the function \(f_{i}\).

We establish the following condition to end the random walk.

**Condition 1**: _Let \(\delta\) denote a tolerance parameter (the largest divergence between any sample and the Domino at any point), and let \(W\) be the largest number of epochs possible such that at least one of the three following statement holds true._

1. \(\forall i\in I\)_,_ \(\mathrm{KL}(\textsc{Domino}||\mathcal{GP}_{i})\leq\delta\)_: all time series' KL divergence from the Domino is under_ \(\delta\)_;_
2. _For_ \(a\in\mathbb{N}\) _where_ \(a\ll N\)_, there are a points of all the time series whose values difference to the_ \(\delta\) _threshold have a standard deviation lower than that of the in-lying points and their difference to the_ \(\delta\) _threshold;_3. \(w\geq W\in\mathbb{N}\): _the maximum number of epochs set is reached._

Initialisation.Given a GP, sample the \(I\) samples to walk on, set the maximum number of epochs \(W\), choose the KL divergence threshold \(\delta\), the maximal number of outliers \(a\) and \(\lambda\in\mathbb{R}_{0}\) the regularisation constant. Without prior knowledge, the starting performance of each sample is \(p^{0}=\{p_{i}^{0}\}_{i=1}^{I}=\frac{1}{I}\).

```
1:whileCondition 1 is False do:
2: Initialise \(g\) with a random draw from \(\mathcal{I}=\{1,2,\ldots,I\}\) whose hyperparameters follow a categorical distribution with hyperparameters given by: \(\big{\{}\frac{e^{\lambda*i}}{\sum_{k=1}^{I}e^{\lambda*k}}\big{\}}_{i=1}^{I}\)
3: Set \(n=g\) and use the sample from the time series drawn above.
4: Set \(\mathbf{y}^{w}(t_{1})=\mathbf{f}_{\mathbf{g}}(t_{1})\) the first time step using the drawn sample and \(\mathbf{z}^{\mathbf{w}}(t_{1})=g=n\) the randomly drawn sample time series for the first step of the random walk.
5:for\(t_{n}\in t_{2},\ldots,t_{N}\), with a probability of \(p(f_{i}(t_{n}))=\frac{e^{\lambda*i}}{\sum_{k=1}^{I}e^{\lambda*k}}\forall i\in \mathcal{I}\): do
6: Set \(\mathbf{y}^{w}(t_{n})=\mathbf{f}_{\mathbf{i}}(t_{n})\) the next step in the random walk;
7: Set \(\mathbf{z}^{w}(t_{n})=i\) from \(\mathbf{y}_{\mathbf{i}}\) the time series for the step.
8:endfor
9:for each time step \(\mathbf{y}^{w}=y^{w}(\mathbf{t_{n}})=\{y^{w}(\mathbf{t_{1}}),y^{w}(\mathbf{t_ {2}}),\ldots,y^{w}(\mathbf{t_{N}})\}\) of the random walk, evaluate it against the \(I\) time series: do
10: Let \(\mathbf{M}^{w}=\{P(f_{1},y_{1}),\ldots,\{P(f_{I},y_{I})\}\}=\{P(f_{i},y_{i}) \}_{i=1}^{I}\) be the performances of the time series.
11: Let \(m^{w}=\frac{1}{I}=\sum_{i=1}^{I}M_{i}^{w}\) be the average of all performances across time series.
12: Update \(\mathbf{p}^{w}=\{p_{i}^{w}\}_{i=1}^{w}\) with \(\mathbf{z_{i}}\): set \[\mathbf{p_{i}}^{w}=\frac{\prod_{a=1}^{w-1}\exp\bigl{(}\frac{1}{2}-\frac{|i \in z_{a}(\mathbf{t_{n}})|}{I}\bigr{)}*\mathbf{M}_{i}^{w}}{\sum_{i=1}^{I} \bigl{(}\prod_{a=1}^{w-1}\exp\bigl{(}\frac{1}{2}-\frac{|i\in z_{a}(\mathbf{t _{n}})|}{I}\bigr{)}*\mathbf{M}_{i}^{w}\bigr{)}}.\]
13: Store the \(\mathbf{p_{w}}\) performance values, \(\mathbf{m^{w}}\) average performance across all time series, \(\mathbf{M^{w}}\) time series performances, \(\mathbf{z^{w}}\) steps from the random walk and \(\mathbf{y}^{w}\) values from the random walk. for the \(w^{th}\) epoch.
14:endfor
15:ifCondition 1 is not False then
16:\(w=w+1\)
17:endif
18:endwhile ```

**Algorithm 1** Domino.

## 3 Experiments

Datasets.With 10 years of regional half-hourly electricity consumption data and the current consumption levels for the regions, we can aim to predict the short-term electricity needs of France over the next three hours. From a set of time series with similar characteristics, the goal is to predict a hold-out time series from GPs trained on the rest of the set. As a proof of concept, in the present paper we experiment on artificially generated periodic data. The synthetic data is generated with trend, periodic and noise components, using the mockseries Python package https://mockseries.catheu.tech/ which allows us to create time series indexed to a chosen time frame, and with constraints.

Evaluation.We evaluate using the Median Absolute Error (MAE) as it is robust to outliers whilst giving an error in the same unit as the output. Given \(y_{i}\) the \(i^{th}\) sample and \(\hat{y}_{i}\) its predicted value, the MAE is calculated by: \(\mathrm{MAE}(y,\hat{y})=\mathrm{median}\bigl{(}|y_{i}-\hat{y}_{i}|_{i=1}^{I} \bigr{)}\). We use MAGMA to train GPs on the data. We evaluate the model, as well as the Domino trained on the model's samples. The test data is evaluated following the protocol in Appendix A.

Ablation studies.We study the impact of hyperparameter adjustment in Domino. We experiment with time series length to find the optimal number of points per time series for MAGMA and Domino,and we establish the relative performance of both algorithms. Our code and data are available online at https://anonymous.4open.science/r/domino-effect-155D/.

Results.We gather in Table 1 the results of performance when varying the time series length for MAGMA and Domino; the cross-validation results are in Table 2. Domino consistently outperforms MAGMA by a significant margin.

Discussion and limitations.We have used a uniform probability distribution on the sampled time series but can extend the work to a scenario with prior knowledge and therefore a known probability distribution across the samples at initialisation.

Domino dramatically improves on the MAGMA algorithm. A natural next step is to conduct a similar study on MAGMAClust [Leroy et al., 2023], a generalisation of MAGMA which learns cluster-specific means and infers clusters whilst learning the common means.

MAGMA can handle covariates. This is a natural next step for the Domino algorithm. We have worked with in-the-art for electricity consumption and we worked with similar data. This approach will be limited where there is an irregular input and calls for an adaptation of Domino.

Hyperparameters.Domino is controlled by hyperparameters which determine the optimal maximal number of training epochs (30), the percentage of the minimum-maximum range of the output which is an acceptable KL divergence \(\delta\) between the training time series and the random walk learned (5%), the maximal number of points (all outputs, all time series combined) which can be over the \(\delta\) value (3%) and the \(\lambda\) regularisation parameter which smooths the weights when calculating the probability of each time series for the next sample (0.5). The full set of ablation studies are detailed in Appendix B.

Conclusion.In this work, we have used the MAGMA algorithm to predict short-term electricity usage based on GPs with common means. We then performed a random walk on samples of these GPs, which is iterated until there is a low divergence between the sampled time series and the points of the random walk. Our experiments show that this approach, called Domino, yields superior predictive results on a synthetic dataset. This is very promising to tackle similar problems in sparse data settings, with less computational resources, or heavy data settings, paving the way to more frugal probabilistic settings.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Length \(N\)** & **MAGMA** & **Domino** \\ \hline
50 & 8.089 & 4.405 \\  & (0.015) & (1.006) \\ \hline
100 & 6.059 & 4.526 \\  & (0.085) & (0.932) \\ \hline
150 & 33.454 & 3.618 \\  & (0.108) & (0.559) \\ \hline
200 & 48.108 & **3.524** \\  & (0.113) & (0.386) \\ \hline
250 & 5.991 & 4.511 \\  & (0.029) & (0.336) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average (std) MAGMA and Domino MAE on 10 runs.

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Length \(N\)** & **MAGMA** & **Domino** \\ \hline
50 & 8.91 & 4.114 \\  & (0.319) & (0.419) \\ \hline
100 & 6.624 & 5.058 \\  & (0.363) & (0.333) \\ \hline
150 & 33.973 & 4.708 \\  & (0.466) & (0.334) \\ \hline
200 & 48.412 & 9.679 \\  & (0.326) & (0.079) \\ \hline
250 & 56.215 & 4.608 \\  & (0.342) & (0.226) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average (std) MAGMA and Domino MAE at cross-validation on 10 runs.

## References

* Iwata and Kumagai [2020] Tomoharu Iwata and Atsutoshi Kumagai. Few-shot learning for time-series forecasting. _arXiv preprint arXiv:2009.14379_, 2020.
* Xu et al. [2024] Jiangjiao Xu, Ke Li, and Dongdong Li. An automated few-shot learning for time series forecasting in smart grid under data scarcity. _IEEE Transactions on Artificial Intelligence_, 2024.
* Rasmussen and Williams [2006] Carl Edward Rasmussen and Christopher K. I. Williams. _Gaussian processes for machine learning_. Adaptive computation and machine learning. MIT Press, Cambridge, Mass, 2006. ISBN 978-0-262-18253-9. OCLC: ocm61285753.
* Leroy et al. [2022] Arthur Leroy, Pierre Latouche, Benjamin Guedj, and Servane Gey. MAGMA: inference and prediction using multi-task Gaussian processes with common mean. _Machine Learning_, 111(5):1821-1849, May 2022. ISSN 1573-0565. doi: 10.1007/s10994-022-06172-1. URL https://doi.org/10.1007/s10994-022-06172-1.
* Leroy et al. [2023] Arthur Leroy, Pierre Latouche, Benjamin Guedj, and Servane Gey. Cluster-Specific Predictions with Multi-Task Gaussian Processes. _Journal of Machine Learning Research_, 24(5):1-49, 2023. ISSN 1533-7928. URL http://jmlr.org/papers/v24/20-1321.html.

## Appendix A Evaluating the Domino algorithm

With the time series on which DOMINO has been trained as well as their weights, the model is queried by inputting a set of \(M\) time points such that \(M<N\). The time points contain the same information as the training data, that is either \(\mathbf{t}\) as an input; and \(\mathbf{y}\) is the output. Made up of \(M\) time-steps, these have dimension \(\mathbf{y},\mathbf{t}\in\mathbb{R}^{M}\).

The Domino algorithm is also given a set of input time points over which an output must be returned - these will be made up of \(\mathbf{x}\in\mathbb{R}^{N}\) and will consist of the first \(M\) points from the query input, plus \(N-M\) extra points which will be used to predict the next points.

The random walk, starting at the \(M+1^{th}\) point, uses the training time-series and their probabilities to predict the rest of the time series.

## Appendix B Hyperparameter tuning

The Domino model has multiple hyperparameters, which control the learning of probabilities for underlying individuals. We run ablation studies for each hyperparameter: the maximal number of epochs for the learning (Table 3), the maximum percentage \(\delta\) of the data range which is a possible divergence threshold between the Domino and the underlying training individuals (Table 4), the maximum percentage of points in all the time series which can be over the \(\delta\) hyperparameter (Table 5), and the regularisation parameter \(\lambda\) (Table 6). The best results for the hyperparameter are given in bold.

\begin{table}
\begin{tabular}{c c c} \hline \hline \multicolumn{1}{c}{**Maximum percentage of values over \(\delta\)**} & **Result** & **CV** \\ \hline
1\% & 5.130 & 4.67 \\  & (0.732) & (0.360) \\ \hline
2\% & 5.249 & 5.102 \\  & (0.434) & (0.297) \\ \hline
3\% & **5.130** & 5.156 \\  & (0.637) & (0.241) \\ \hline
5\% & 5.344 & 5.110 \\  & (0.521) & (0.375) \\ \hline
10\% & 5.275 & 5.054 \\  & (0.499) & (0.190) \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c} \hline \hline \multicolumn{1}{c}{\(\lambda\)} & **Result** & **CV** \\ \hline
0.5 & **4.698** & 5.450 \\  & (0.473) & (0.407) \\ \hline
1 & 5.308 & 5.231 \\  & (0.506) & (0.566) \\ \hline
1.5 & 5.315 & 5.129 \\  & (0.360) & (0.540) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameter tuning: average (std) MAE for \(\lambda\) the regularisation parameter on 10 runs.

\begin{table}
\begin{tabular}{c c c} \hline \hline \multicolumn{1}{c}{**Max epochs**} & **Result** & **CV** \\ \hline
5 & 5.750 & 5.302 \\  & (0.753) & (0.313) \\ \hline
10 & 5.648 & 5.244 \\  & (0.792) & (0.225) \\ \hline
15 & 5.082 & 5.314 \\  & (0.792) & (0.308) \\ \hline
20 & 5.460 & 5.174 \\  & (0.668) & (0.331) \\ \hline
25 & 5.206 & 4.964 \\  & (0.544) & (0.215) \\ \hline
30 & **5.049** & 5.054 \\  & (0.409) & (0.251) \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c} \hline \hline \multicolumn{1}{c}{\(\delta\)} & **Result** & **CV** \\ \hline
1\% & 5.413 & 5.247 \\  & (0.597) & (0.341) \\ \hline
2\% & 5.218 & 5.034 \\  & (0.502) & (0.218) \\ \hline
3\% & 5.187 & 5.211 \\  & (0.537) & (0.307) \\ \hline
5\% & **5.185** & 5.096 \\  & (0.583) & (0.283) \\ \hline
10\% & 5.640 & 5.331 \\  & (0.594) & (0.453) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Hyperparameter tuning: average (std) MAE for \(\delta\) threshold for divergence on 10 runs.

\begin{table}
\begin{tabular}{c c c} \hline \hline \multicolumn{1}{c}{**Max epochs**} & **Result** & **CV** \\ \hline
5 & 5.750 & 5.302 \\  & (0.753) & (0.313) \\ \hline
10 & 5.648 & 5.244 \\  & (0.792) & (0.225) \\ \hline
15 & 5.082 & 5.314 \\  & (0.792) & (0.308) \\ \hline
20 & 5.460 & 5.174 \\  & (0.668) & (0.331) \\ \hline
25 & 5.206 & 4.964 \\  & (0.544) & (0.215) \\ \hline
30 & **5.049** & 5.054 \\  & (0.409) & (0.251) \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c} \hline \hline \multicolumn{1}{c}{\(\delta\)} & **Result** & **CV** \\ \hline
1\% & 5.413 & 5.217 \\  & (0.597) & (0.341) \\ \hline
2\% & 5.218 & 5.034 \\  & (0.502) & (0.218) \\ \hline
3\% & 5.187 & 5.211 \\  & (0.537) & (0.307) \\ \hline
5\% & **5.185** & 5.096 \\  & (0.583) & (0.283) \\ \hline
10\% & 5.640 & 5.331 \\  & (0.594) & (0.453) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameter tuning: average (std) MAE for maximal number of epochs on 10 runs.