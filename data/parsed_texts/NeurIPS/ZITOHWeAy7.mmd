# A Graph-Theoretic Framework for Understanding Open-World Semi-Supervised Learning

Yiyou Sun, Zhenmei Shi, Yixuan Li

Department of Computer Sciences

University of Wisconsin, Madison

{suniyiyou,zhmeishi,sharonli}@cs.wisc.edu

###### Abstract

Open-world semi-supervised learning aims at inferring both known and novel classes in unlabeled data, by harnessing prior knowledge from a labeled set with known classes. Despite its importance, there is a lack of theoretical foundations for this problem. This paper bridges the gap by formalizing a graph-theoretic framework tailored for the open-world setting, where the clustering can be theoretically characterized by graph factorization. Our graph-theoretic framework illuminates practical algorithms and provides guarantees. In particular, based on our graph formulation, we apply the algorithm called Spectral Open-world Representation Learning (SORL), and show that minimizing our loss is equivalent to performing spectral decomposition on the graph. Such equivalence allows us to derive a provable error bound on the clustering performance for both known and novel classes, and analyze rigorously when labeled data helps. Empirically, SORL can match or outperform several strong baselines on common benchmark datasets, which is appealing for practical usage while enjoying theoretical guarantees. Our code is available at https://github.com/deeplearning-wisc/sorl.

## 1 Introduction

Machine learning models in the open world inevitably encounter data from both known and novel classes [2; 15; 16; 65; 79]. Traditional supervised machine learning models are trained on a closed set of labels, and thus can struggle to effectively cluster new semantic concepts. On the other hand, open-world semi-supervised learning approaches, such as those discussed in studies [7; 63; 69], enable models to distinguish _both known and novel classes_, making them highly desirable for real-world scenarios. As shown in Figure 1, the learner has access to a labeled training dataset \(\mathcal{D}_{l}\) (from known classes) as well as a large unlabeled dataset \(\mathcal{D}_{u}\) (from both known and novel classes). By optimizing feature representations jointly from both labeled and unlabeled data, the learner aims to create meaningful cluster structures that correspond to either known or novel classes. With the explosive growth of data generated in various

Figure 1: Open-world Semi-supervised Learning aims to correctly cluster samples in the novel class and classify samples in the known classes by utilizing knowledge from the labeled data. An open question is _“what is the role of the label information in shaping representations for both known and novel classes?”_ This paper aims to provide a formal understanding.

domains, open-world semi-supervised learning has emerged as a crucial problem in the field of machine learning.

**Motivation.** Different from self-supervised learning [5; 8; 11; 12; 23; 26; 68; 77], open-world semi-supervised learning allows harnessing the power of the labeled data for possible knowledge sharing and transfer to unlabeled data, and from known classes to novel classes. In this joint learning process, we argue that interesting intricacies can arise--the labeled data provided may be beneficial or unhelpful to the resulting clusters. We exemplify the nuances in Figure 1. In one scenario, when the model learns the labeled known classes (e.g., traffic light) by pushing red and green lights closer, such a relationship might transfer to help cluster green and red apples into a coherent cluster. Alternatively, when the connection between the labeled data and the novel class (e.g., flower) is weak, the benefits might be negligible. We argue--perhaps obviously--that a formalized understanding of the intricate phenomenon is needed.

**Theoretical significance.** To date, theoretical understanding of open-world semi-supervised learning is still in its infancy. In this paper, we aim to fill the critical blank by analyzing this important learning problem from a rigorous theoretical standpoint. Our exposition gravitates around the open question: _what is the role of labeled data in shaping representations for both known and novel classes?_ To answer this question, we formalize a graph-theoretic framework tailored for the open-world setting, where the vertices are all the data points and connected sub-graphs form classes (either known or novel). The edges are defined by a combination of supervised and self-supervised signals, which reflects the availability of both labeled and unlabeled data. Importantly, this graph facilitates the understanding of open-world semi-supervised learning from a spectral analysis perspective, where the clustering can be theoretically characterized by graph factorization. Based on the graph-theoretic formulation, we derive a formal error bound by contrasting the clustering performance for all classes, before and after adding the labeling information. Our Theorem 4.2 reveals the sufficient condition for the improved clustering performance for a class. Under the K-means measurement, the unlabeled samples in one class can be better clustered, if their overall connection to the labeled data is stronger than their self-clusterability.

**Practical significance.** Our graph-theoretic framework also illuminates practical algorithms with provided guarantees. In particular, based on our graph formulation, we present the algorithm called Spectral Open-world Representation Learning (SORL) adapted from Sun et al. [64]. Minimizing this loss is equivalent to performing spectral decomposition on the graph (Section 3.2), which brings two key benefits: (1) it allows us to analyze the representation space and resulting clustering performance in closed-form; (2) practically, it enables end-to-end training in the context of deep networks. We show that our learning algorithm leads to strong empirical performance while enjoying theoretical guarantees. The learning objective can be effectively optimized using stochastic gradient descent on modern neural network architecture, making it desirable for real-world applications.

## 2 Problem Setup

We formally describe the data setup and learning goal of open-world semi-supervised learning [7].

**Data setup.** We consider the empirical training set \(\mathcal{D}_{l}\cup\mathcal{D}_{u}\) as a union of labeled and unlabeled data.

1. The labeled set \(\mathcal{D}_{l}=\{\bar{x}_{i},y_{i}\}_{i=1}^{n}\), with \(y_{i}\in\mathcal{Y}_{l}\). The label set \(\mathcal{Y}_{l}\) is known.
2. The unlabeled set \(\mathcal{D}_{u}=\{\bar{x}_{i}\}_{i=1}^{m}\), where each sample \(\bar{x}_{i}\) can come from either known or novel classes1. Note that we do not have access to the labels in \(\mathcal{D}_{u}\). For mathematical convenience, we denote the underlying label set as \(\mathcal{Y}_{\text{all}}\), where \(\mathcal{Y}_{l}\subset\mathcal{Y}_{\text{all}}\). We denote \(C=|\mathcal{Y}_{\text{all}}|\) the total number of classes. Footnote 1: This generalizes the problem of Novel Class Discovery [19; 22; 27; 28; 82; 83], which assumes the unlabeled set is _purely_ from novel classes.

The data setup has practical value for real-world applications. For example, the labeled set is common in supervised learning; and the unlabeled set can be gathered for free from the model's operating environment or the internet. We use \(\mathcal{P}_{l}\) and \(\mathcal{P}\) to denote the marginal distributions of labeled data and all data in the input space, respectively. Further, we let \(\mathcal{P}_{l_{i}}\) denote the distribution of labeled samples with class label \(i\in\mathcal{Y}_{l}\).

**Learning target.** Under the setting, our goal is to learn distinguishable representations _for both known and novel classes_ simultaneously. The representation quality will be measured using classic metrics, such as K-means clustering accuracy, which we will define mathematically in Section 4.2.2. Unlike classic semi-supervised learning [86], we place no assumption on the unlabeled data and allow its semantic space to cover both known and novel classes. The problem is also referred to as open-world representation learning [63], which emphasizes the role of good representation in distinguishing both known and novel classes.

**Theoretical analysis goal.** We aim to comprehend the role of label information in shaping representations for both known and novel classes. It's important to note that our theoretical approach aims to understand the perturbation in the clustering performance by labeling existing, previously unlabeled data points within the dataset. By contrasting the clustering performance before and after labeling these instances, we uncover the underlying structure and relations that the labels may reveal. This analysis provides invaluable insights into how labeling information can be effectively leveraged to enhance the representations of both known and novel classes.

## 3 A Spectral Approach for Open-world Semi-Supervised Learning

In this section, we formalize and tackle the open-world semi-supervised learning problem from a graph-theoretic view. Our fundamental idea is to formulate it as a clustering problem--where similar data points are grouped into the same cluster, by way of possibly utilizing helpful information from the labeled data \(\mathcal{D}_{l}\). This clustering process can be modeled by a graph, where the vertices are all the data points and classes form connected sub-graphs. Specifically, utilizing our graph formulation, we present the algorithm -- Spectral Open-world Representation Learning (SORL) in Section 3.2. The process of minimizing the corresponding loss is fundamentally analogous to executing a spectral decomposition on the graph.

### A Graph-Theoretic Formulation

We start by formally defining the augmentation graph and adjacency matrix. For clarity, we use \(\bar{x}\) to indicate the natural sample (raw inputs without augmentation). Given an \(\bar{x}\), we use \(\mathcal{T}(x|\bar{x})\) to denote the probability of \(x\) being augmented from \(\bar{x}\). For instance, when \(\bar{x}\) represents an image, \(\mathcal{T}(\cdot|\bar{x})\) can be the distribution of common augmentations [11] such as Gaussian blur, color distortion, and random cropping. The augmentation allows us to define a general population space \(\mathcal{X}\), which contains all the original images along with their augmentations. In our case, \(\mathcal{X}\) is composed of augmented samples from both labeled and unlabeled data, with cardinality \(|\mathcal{X}|=N\). We further denote \(\mathcal{X}_{l}\) as the set of samples (along with augmentations) from the labeled data part.

We define the graph \(G(\mathcal{X},w)\) with vertex set \(\mathcal{X}\) and edge weights \(w\). To define edge weights \(w\), we decompose the graph connectivity into two components: (1) self-supervised connectivity \(w^{(u)}\) by treating all points in \(\mathcal{X}\) as entirely unlabeled, and (2) supervised connectivity \(w^{(l)}\) by adding labeled information from \(\mathcal{P}_{l}\) to the graph. We proceed to define these two cases separately.

First, by assuming all points as unlabeled, two samples (\(x\), \(x^{+}\)) are considered a **positive pair** if:

**Unlabeled Case (u):**\(x\) _and \(x^{+}\) are augmented from the same image \(\bar{x}\sim\mathcal{P}\)._

For any two augmented data \(x,x^{\prime}\in\mathcal{X}\), \(w^{(u)}_{xx^{\prime}}\) denotes the marginal probability of generating the pair:

\[w^{(u)}_{xx^{\prime}}\triangleq\mathbb{E}_{\bar{x}\sim\mathcal{P}}\mathcal{T} (x|\bar{x})\mathcal{T}\left(x^{\prime}|\bar{x}\right),\] (1)

which can be viewed as self-supervised connectivity [11, 23]. However, different from self-supervised learning, we have access to the labeled information for a subset of nodes, which _allows adding additional connectivity to the graph_. Accordingly, the positive pair can be defined as:

**Labeled Case (l):**\(x\) _and \(x^{+}\) are augmented from two labeled samples \(\bar{x}_{l}\) and \(\bar{x}^{\prime}_{l}\) with the same known class \(i\). In other words, both \(\bar{x}_{l}\) and \(\bar{x}^{\prime}_{l}\) are drawn independently from \(\mathcal{P}_{l_{i}}\)._

Considering both case (u) and case (l), the overall edge weight for any pair of data \((x,x^{\prime})\) is given by:

\[w_{xx^{\prime}}=\eta_{u}w^{(u)}_{xx^{\prime}}+\eta_{l}w^{(l)}_{xx^{\prime}}, \text{where }w^{(l)}_{xx^{\prime}}\triangleq\sum_{i\in\mathcal{Y}_{l}}\mathbb{E}_{\bar{x}_ {l}\sim\mathcal{P}_{l_{i}}}\mathbb{E}_{\bar{x}^{\prime}_{l}\sim\mathcal{P}_{l_ {i}}}\mathcal{T}(x|\bar{x}_{l})\mathcal{T}\left(x^{\prime}|\bar{x}^{\prime}_{l }\right),\] (2)and \(\eta_{u},\eta_{l}\) modulates the importance between the two cases. The magnitude of \(w_{xx^{\prime}}\) indicates the "positiveness" or similarity between \(x\) and \(x^{\prime}\). We then use \(w_{x}=\sum_{x^{\prime}\in\mathcal{X}}w_{xx^{\prime}}\) to denote the total edge weights connected to a vertex \(x\).

**Remark: A graph perturbation view.** With the graph connectivity defined above, we can now define the adjacency matrix \(A\in\mathbb{R}^{N\times N}\) with entries \(A_{xx^{\prime}}=w_{xx^{\prime}}\). Importantly, the adjacency matrix can be decomposed into two parts:

(3)

which can be regarded as the self-supervised adjacency matrix \(A^{(u)}\) perturbed by additional labeling information encoded in \(A^{(l)}\). This graph perturbation view serves as a critical foundation for our theoretical analysis of the clustering performance in Section 4. As a standard technique in graph theory [14], we use the _normalized adjacency matrix_ of \(G(\mathcal{X},w)\):

\[\tilde{A}\triangleq D^{-\frac{1}{2}}AD^{-\frac{1}{2}},\] (4)

where \(D\in\mathbb{R}^{N\times N}\) is a diagonal matrix with \(D_{xx}=w_{x}\). The normalization balances the degree of each node, reducing the influence of vertices with very large degrees. The normalized adjacency matrix defines the probability of \(x\) and \(x^{\prime}\) being considered as the positive pair from the perspective of augmentation, which helps derive the learning loss as we show next.

### SORL: Spectral Open-World Representation Learning

We present an algorithm called Spectral Open-world Representation Learning (SORL), which can be derived from a spectral decomposition of \(\tilde{A}\). The algorithm has both practical and theoretical values. First, it enables efficient end-to-end training in the context of modern neural networks. More importantly, it allows drawing a theoretical equivalence between learned representations and the top-\(k\) singular vectors of \(\tilde{A}\). Such equivalence facilitates theoretical understanding of the clustering structure encoded in \(\tilde{A}\). Specifically, we consider low-rank matrix approximation:

\[\min_{F\in\mathbb{R}^{N\times k}}\mathcal{L}_{\mathrm{mf}}(F,A)\triangleq \left\|\tilde{A}-FF^{\top}\right\|_{F}^{2}\] (5)

According to the Eckart-Young-Mirsky theorem [17], the minimizer of this loss function is \(F_{k}\in\mathbb{R}^{N\times k}\) such that \(F_{k}F_{k}^{\top}\) contains the top-\(k\) components of \(\tilde{A}\)'s SVD decomposition.

Now, if we view each row \(\mathbf{f}_{x}^{\top}\) of \(F\) as a scaled version of learned feature embedding \(f:\mathcal{X}\mapsto\mathbb{R}^{k}\), the \(\mathcal{L}_{\mathrm{mf}}(F,A)\) can be written as a form of the contrastive learning objective. We formalize this connection in Theorem 3.1 below2.

Footnote 2: Theorem 3.1 is primarily adapted from Theorem 4.1 in [64]. However, there is a distinction in the data setting, as Sun et al. [64] do not consider known class samples within the unlabeled dataset.

**Theorem 3.1**.: _We define \(\mathbf{f}_{x}=\sqrt{w_{x}}f(x)\) for some function \(f\). Recall \(\eta_{u},\eta_{l}\) are coefficients defined in Eq. (2). Then minimizing the loss function \(\mathcal{L}_{\mathrm{mf}}(F,A)\) is equivalent to minimizing the following loss function for \(f\), which we term **Spectral Open-world Representation Learning (SORL)**:_

\[\mathcal{L}_{\text{SORL}}(f)\triangleq-2\eta_{l}\mathcal{L}_{1}(f)-2\eta_{u} \mathcal{L}_{2}(f)+\eta_{l}^{2}\mathcal{L}_{3}(f)+2\eta_{l}\eta_{u}\mathcal{L} _{4}(f)+\eta_{u}^{2}\mathcal{L}_{5}(f),\] (6)

_where_

\[\mathcal{L}_{1}(f) =\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E}}_{\begin{subarray}{ c}\tilde{x}_{l}\sim\mathcal{P}_{l_{i}},\tilde{x}_{i}\sim\mathcal{P}_{l_{i}},\\ x\sim\mathcal{T}(\cdot|\tilde{x}_{l}),x^{\top}\sim\mathcal{T}(\cdot|\tilde{x}_ {l}^{\prime})\end{subarray}}\left[f(x)^{\top}f\left(x^{+}\right)\right], \mathcal{L}_{2}(f)=\mathop{\mathbb{E}}_{\begin{subarray}{c}\tilde{x}_{u}\sim \mathcal{P}_{l_{i}},\\ x\sim\mathcal{T}(\cdot|\tilde{x}_{u}),x^{\top}\sim\mathcal{T}(\cdot|\tilde{x}_ {u})\end{subarray}}\left[f(x)^{\top}f\left(x^{+}\right)\right],\] \[\mathcal{L}_{3}(f) =\sum_{i,j\in\mathcal{Y}_{l}}\mathop{\mathbb{E}}_{\begin{subarray}{ c}\tilde{x}_{l}\sim\mathcal{P}_{l_{i}},\tilde{x}_{l}^{\prime}\sim\mathcal{P}_{l_{j}},\\ x\sim\mathcal{T}(\cdot|\tilde{x}_{l}),x^{\top}\sim\mathcal{T}(\cdot|\tilde{x}_ {l}^{\prime})\end{subarray}}\left[\left(f(x)^{\top}f\left(x^{-}\right)\right)^{ 2}\right],\] \[\mathcal{L}_{4}(f) =\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E}}_{\begin{subarray}{ c}\tilde{x}_{l}\sim\mathcal{P}_{l_{i}},\tilde{x}_{u}\sim\mathcal{P},\\ x\sim\mathcal{T}(\cdot|\tilde{x}_{l}),x^{-}\sim\mathcal{T}(\cdot|\tilde{x}_ {u})\end{subarray}}\left[\left(f(x)^{\top}f\left(x^{-}\right)\right)^{2} \right],\mathcal{L}_{5}(f)=\mathop{\mathbb{E}}_{\begin{subarray}{c}\tilde{x}_{ u}\sim\mathcal{P},\tilde{x}_{u}^{\prime}\sim\mathcal{P},\\ x\sim\mathcal{T}(\cdot|\tilde{x}_{u}),x^{-}\sim\mathcal{T}(\cdot|\tilde{x}_ {u})\end{subarray}}\left[\left(f(x)^{\top}f\left(x^{-}\right)\right)^{2} \right].\]Proof.: _(sketch)_ We can expand \(\mathcal{L}_{\mathrm{mf}}(F,A)\) and obtain

\[\mathcal{L}_{\mathrm{mf}}(F,A)=\sum_{x,x^{\prime}\in\mathcal{X}}\left(\frac{w_{ x^{\prime}}}{\sqrt{w_{x}w_{x^{\prime}}}}-\mathbf{f}_{x}^{\intercal}\mathbf{f}_{x^{ \prime}}\right)^{2}=const+\sum_{x,x^{\prime}\in\mathcal{X}}\left(-2w_{xx^{ \prime}}f(x)^{\top}f\left(x^{\prime}\right)+w_{x}w_{x^{\prime}}\left(f(x)^{ \top}f\left(x^{\prime}\right)\right)^{2}\right)\]

The form of \(\mathcal{L}_{\mathrm{SORL}}(f)\) is derived from plugging \(w_{xx^{\prime}}\) (defined in Eq. (1)) and \(w_{x}\). Full proof is in Appendix A. 

**Interpretation of \(\mathcal{L}_{\mathrm{SORL}}(f)\)**. At a high level, \(\mathcal{L}_{1}\) and \(\mathcal{L}_{2}\) push the embeddings of **positive pairs** to be closer while \(\mathcal{L}_{3}\), \(\mathcal{L}_{4}\) and \(\mathcal{L}_{5}\) pull away the embeddings of **negative pairs**. In particular, \(\mathcal{L}_{1}\) samples two random augmentation views of two images from labeled data with the **same** class label, and \(\mathcal{L}_{2}\) samples two views from the same image in \(\mathcal{X}\). For negative pairs, \(\mathcal{L}_{3}\) uses two augmentation views from two samples in \(\mathcal{X}_{l}\) with **any** class label. \(\mathcal{L}_{4}\) uses two views of one sample in \(\mathcal{X}_{l}\) and another one in \(\mathcal{X}\). \(\mathcal{L}_{5}\) uses two views from two random samples in \(\mathcal{X}\). This training objective, though bearing similarities to NSCL [64], operates within a distinct problem domain. Accordingly, we derive novel theoretical analysis uniquely tailored to our problem setting, which we present next.

## 4 Theoretical Analysis

So far we have presented a spectral approach for open-world semi-supervised learning based on graph factorization. Under this framework, we now formally analyze: _how does the labeling information shape the representations for known and novel classes?_

### An Illustrative Example

We consider a toy example that helps illustrate the core idea of our theoretical findings. Specifically, the example aims to distinguish 3D objects with different shapes, as shown in Figure 2. These images are generated by a 3D rendering software [31] with user-defined properties including colors, shape, size, position, etc. We are interested in contrasting the representations (in the form of singular vectors), when the label information is either incorporated in training or not.

**Data design.** Suppose the training samples come from three types, \(\mathcal{X}_{\overrightarrow{\Box}}\), \(\mathcal{X}_{\overrightarrow{\Box}}\), \(\mathcal{X}_{\overrightarrow{\Box}}\). Let \(\mathcal{X}_{\overrightarrow{\Box}}\) be the sample space with **known** class, and \(\mathcal{X}_{\overrightarrow{\Box}}\), \(\mathcal{X}_{\overrightarrow{\Box}}\) be the sample space with **novel** classes. Further, the two novel classes are constructed to have different relationships with the known class. Specifically, shares some similarity with \(\mathcal{X}_{\overrightarrow{\Box}}\) in color (red and blue); whereas another novel class \(\mathcal{X}_{\overrightarrow{\Box}}\) has no obvious similarity with the known class. Without any labeling information, it can be difficult to distinguish \(\mathcal{X}_{\overrightarrow{\Box}}\) from \(\mathcal{X}_{\overrightarrow{\Box}}\) since samples share common colors. We aim to verify the hypothesis that: _adding labeling information to \(\mathcal{X}_{\overrightarrow{\Box}}\)_(i.e., connecting \(\boxplus\) and \(\boxplus\)) has a larger (beneficial) impact to cluster \(\mathcal{X}_{\overrightarrow{\Box}}\) than \(\mathcal{X}_{\overrightarrow{\Box}}\)_.

Figure 2: An illustrative example for theoretical analysis. We consider a 6-node graph with one known class (cube) and two novel classes (sphere, cylinder). (a) The augmentation probabilities between nodes are defined by their color and shape in Eq. (7). (b) The adjacency matrix can then be calculated by Equations in Sec. 3.1 where we let \(\tau_{0}=0,\eta_{u}=6,\eta_{l}=4\). The calculation details are in Appendix B. The magnitude order follows \(\tau_{1}\gg\tau_{c}>\tau_{s}>0\).

**Augmentation graph.** Based on the data design, we formally define the augmentation graph, which encodes the probability of augmenting a source image \(\bar{x}\) to the augmented view \(x\):

\[\mathcal{T}\left(x\mid\bar{x}\right)=\left\{\begin{array}{ll}\tau_{1}&\text{ if color}(x)=\text{color}(\bar{x}),\text{shape}(x)=\text{shape}(\bar{x});\\ \tau_{c}&\text{if color}(x)=\text{color}(\bar{x}),\text{shape}(x)\neq\text{shape} (\bar{x});\\ \tau_{s}&\text{if color}(x)\neq\text{color}(\bar{x}),\text{shape}(x)=\text{shape} (\bar{x});\\ \tau_{0}&\text{if color}(x)\neq\text{color}(\bar{x}),\text{shape}(x)\neq\text{ shape}(\bar{x}).\end{array}\right.\] (7)

With Eq. (7) and the definition of the adjacency matrix in Section 3.1, we can derive the analytic form of \(A^{(u)}\) and \(A\), as shown in Figure 2(b). We refer readers to Appendix B for the detailed derivation. The two matrices allow us to contrast the connectivity changes in the graph, before and after the labeling information is added.**Insights.** We are primarily interested in analyzing the difference of the representation space derived from \(A^{(u)}\) and \(A\). We visualize the top-3 eigenvectors3 of the normalized adjacency matrix \(\tilde{A}^{(u)}\) and \(\tilde{A}\) in Figure 3(a), where the results are based on the magnitude order \(\tau_{1}\gg\tau_{c}>\tau_{s}>0\). Our key takeaway is: _adding labeling information to known class \(\mathcal{X}_{\overrightarrow{\bigoplus}}\) helps better distinguish the known class itself and the novel class \(\mathcal{X}_{\overrightarrow{\bigoplus}}\), which has a stronger connection/similarity with \(\mathcal{X}_{\overrightarrow{\bigoplus}}\)._

Footnote 3: When \(\tau_{1}\gg\tau_{c}>\tau_{s}>0\), the top-3 eigenvectors are almost equivalent to the feature embedding.

**Qualitative analysis.** Our theoretical insight can also be verified empirically, by learning representations on over 10,000 samples using the loss defined in Section 3.2. Due to the space limitation, we include experimental details in Appendix E.1. In Figure 3(b), we visualize the learned features through UMAP [43]. Indeed, we observe that samples become more concentrated around different shape classes after adding labeling information to the cube class.

### Main Theory

The toy example offers an important insight that the added labeled information is more helpful for the class with a stronger connection to the known class. In this section, we formalize this insight by extending the toy example to a more general setting. As a roadmap, we derive the result through three steps: **(1)** derive the closed-form solution of the learned representations; **(2)** define the clustering performance by the K-means measure; **(3)** contrast the resulting clustering performance before and after adding labels. We start by deriving the representations.

#### 4.2.1 Learned Representations in Analytic Form

**Representation without labels.** To obtain the representations, one can train the neural network \(f:\mathcal{X}\mapsto\mathbb{R}^{k}\) using the spectral loss defined in Equation 6. We assume that the optimizer is capable to obtain the representation \(Z^{(u)}\in\mathbb{R}^{N\times k}\) that minimizes the loss, where each row vector \(\mathbf{z}_{i}=f(x_{i})^{\top}\). Recall that Theorem 3.1 allows us to derive a closed-form solution for the learned feature space by the spectral decomposition of the adjacency matrix, which is \(\tilde{A}^{(u)}\) in the case without labeling information. Specifically, we have \(F_{k}^{(u)}=\sqrt{D^{(u)}}Z^{(u)}\), where \(F_{k}^{(u)}F_{k}^{(u)\top}\) contains the

Figure 3: Visualization of representation space for toy example. (a) Theoretically contrasting the feature formed by top-3 eigenvectors of \(\tilde{A}^{(u)}\) and \(\tilde{A}\) respectively. (b) UMAP visualization of the features learned without (left) and with labeled information (right). Details are in Appendix B (eigenvector calculation) and Appendix E.1 (visualization setting).

top-\(k\) components of \(\tilde{A}^{(u)}\)'s SVD decomposition and \(D^{(u)}\) is the diagonal matrix defined based on the row sum of \(A^{(u)}\). We further define the top-\(k\) singular vectors of \(\tilde{A}^{(u)}\) as \(V_{k}^{(u)}\in\mathbb{R}^{N\times k}\), so we have \(F_{k}^{(u)}=V_{k}^{(u)}\sqrt{\Sigma_{k}^{(u)}}\), where \(\Sigma_{k}^{(u)}\) is a diagonal matrix of the top-\(k\) singular values of \(\tilde{A}^{(u)}\). By equalizing the two forms of \(F_{k}^{(u)}\), the closed-formed solution of the learned feature space is given by \(Z^{(u)}=[D^{(u)}]^{-\frac{1}{2}}V_{k}^{(u)}\sqrt{\Sigma_{k}^{(u)}}\).

**Representation perturbation by adding labels.** We now analyze how the representation is "perturbed" as a result of adding label information. We consider \(|\mathcal{Y}_{l}|=1^{4}\) to facilitate a better understanding of our key insight. We can rewrite \(A\) in Eq. 3 as:

\[A(\delta)\triangleq\eta_{u}A^{(u)}+\delta\mathsf{I}^{\top},\]

where we replace \(\eta_{l}\) to \(\delta\) to be more apparent in representing the perturbation and define \(\mathsf{I}\in\mathbb{R}^{N},(\mathsf{I})_{x}=\mathbb{E}_{\bar{x}_{l}\sim \mathcal{P}_{l_{1}}}\mathcal{T}(x|\bar{x}_{l})\). Note that \(\mathsf{I}\) can be interpreted as the vector of "_the semantic connection for sample \(x\) to the labeled data_". One can easily extend to \(r\) classes by letting \(\mathsf{I}\in\mathbb{R}^{N\times r}\).

Here we treat the adjacency matrix as a function of the perturbation. In a similar manner as above, we can derive the normalized adjacency matrix \(\tilde{A}(\delta)\) and the feature representation \(Z(\delta)\) in closed form. The details are included in Appendix C.4.

#### 4.2.2 Evaluation Target

With the learned representations, we can evaluate their quality by the clustering performance. Our theoretical analysis of the clustering performance can well connect to empirical evaluation strategy in the literature [75] using \(K\)-means clustering accuracy/error. Formally, we define the ground-truth partition of clusters by \(\Pi=\{\pi_{1},\pi_{2},...,\pi_{C}\}\), where \(\pi_{i}\) is the set of samples' indices with underlying label \(y_{i}\) and \(C\) is the total number of classes (including both known and novel). We further let \(\bm{\mu}_{\pi}=\mathbb{E}_{i\in\pi}\mathbf{z}_{i}\) be the center of features in \(\pi\), and the average of all feature vectors be \(\bm{\mu}_{\Pi}=\mathbb{E}_{j\in[N]}\mathbf{z}_{j}\).

The clustering performance of K-means depends on two measurements: **Intra-class** measure and **Inter-class** measure. Specifically, we let the intra-class measure be the average Euclidean distance from the samples' feature to the corresponding cluster center and we measure the inter-class separation as the distances between cluster centers:

\[\mathcal{M}_{\text{intra-class}}(\Pi,Z)\triangleq\sum_{\pi\in\Pi}\sum_{i\in \pi}\left\|\mathbf{z}_{i}-\bm{\mu}_{\pi}\right\|^{2},\mathcal{M}_{\text{ inter-class}}(\Pi,Z)\triangleq\sum_{\pi\in\Pi}\left|\pi\right|\left\|\bm{\mu}_{ \pi}-\bm{\mu}_{\Pi}\right\|^{2}.\] (8)

Strong clustering results translate into low \(\mathcal{M}_{\text{intra-class}}\) and high \(\mathcal{M}_{\text{inter-class}}\). Thus we define the **K-means measure** as:

\[\mathcal{M}_{kms}(\Pi,Z)\triangleq\mathcal{M}_{\text{intra-class}}(\Pi,Z)/ \mathcal{M}_{\text{inter-class}}(\Pi,Z).\] (9)

We also formally show in Theorem 4.1 that the K-means clustering error5 is asymptotically equivalent to the K-means measure we defined above.

Footnote 4: To understand the perturbation by adding labels from more than one class, one can take the summation of the perturbation by each class.

Footnote 5: It is theoretically inconvenient to directly analyze the clustering error since it is a non-differentiable target.

**Theorem 4.1**.: _(**Relationship between the K-means measure and K-means error.**) We define the \(\xi_{\pi\to\pi^{\prime}}\) as the index set of samples that is from class division \(\pi\) however is closer to \(\bm{\mu}_{\pi^{\prime}}\) than \(\bm{\mu}_{\pi}\). In other word, \(\xi_{\pi\to\pi^{\prime}}=\{i:i\in\pi,\|\mathbf{z}_{i}-\bm{\mu}_{\pi}\|_{2}\geq \|\mathbf{z}_{i}-\bm{\mu}_{\pi^{\prime}}\|_{2}\}\). Assuming \(|\xi_{\pi\to\pi^{\prime}}|>0\), we define below the clustering error ratio from \(\pi\) to \(\pi^{\prime}\) as \(\mathcal{E}_{\pi\to\pi^{\prime}}\) and the overall cluster error ratio \(\mathcal{E}_{\Pi,Z}\) as the Harmonic Mean of \(\mathcal{E}_{\pi\to\pi^{\prime}}\) among all class pairs:_

\[\mathcal{E}_{\Pi,Z}=C(C-1)/\left(\sum_{\begin{subarray}{c}\pi\neq\pi^{\prime}\\ \pi,\pi^{\prime}\in\Pi\end{subarray}}\frac{1}{\mathcal{E}_{\pi\to\pi^{\prime} }}\right),\text{where }\mathcal{E}_{\pi\to\pi^{\prime}}=\frac{|\xi_{\pi\to\pi^{\prime}}|}{|\pi^{ \prime}|+|\pi|}.\]

_The K-means measure \(\mathcal{M}_{kms}(\Pi,Z)\) has the same order of the Harmonic Mean of the cluster error ratio between all cluster pairs with proof in Appendix C.3._

\[\mathcal{E}_{\Pi,Z}=O(\mathcal{M}_{kms}(\Pi,Z)).\]

[MISSING_PAGE_FAIL:8]

## 5 Empirical Validation of Theory

Beyond theoretical insights, we show empirically that SORL is effective on standard benchmark image classification datasets CIFAR-10/100 [35]. Following the seminal work ORCA [7], classes are divided into 50% known and 50% novel classes. We then use 50% of samples from the known classes as the labeled dataset, and the rest as the unlabeled set. We follow the evaluation strategy in [7] and report the following metrics: (1) classification accuracy on known classes, (2) clustering accuracy on the novel data, and (3) overall accuracy on all classes. More experiment details are in Appendix E.2.

**SORL achieves competitive performance.** Our proposed loss SORL is amenable to the theoretical understanding, which is our primary goal of this work. Beyond theory, we show that SORL is equally desirable in empirical performance. In particular, SORL displays competitive performance compared to existing methods, as evidenced in Table 1. Our comparison covers an extensive collection of very recent algorithms developed for this problem, including ORCA [7], GCD [69], and OpenCon [63]. We also compare methods in related problem domains: (1) Semi-Supervised Learning [21; 37; 62], (2) Novel Class Discovery [22; 82], (3) common representation learning method SimCLR [11]. In particular, on CIFAR-100, we improve upon the best baseline OpenCon by **3.4%** in terms of overall accuracy. Our result further validates that putting analysis on SORL is appealing for both theoretical and empirical reasons.

## 6 Broader Impact

From a theoretical perspective, our graph-theoretic framework can facilitate and deepen the understanding of other representation learning methods that commonly involve the notion of positive/negative pairs. In Appendix D, _we exemplify how our framework can be potentially generalized to other common contrastive loss functions_[11; 34; 68], and baseline methods that are tailored for the open-world semi-supervised learning problem (e.g., GCD [69], OpenCon [63]). Hence, we believe our theoretical framework has a broader utility and significance.

From a practical perspective, our work can directly impact and benefit many real-world applications, where unlabeled data are produced at an incredible rate today. Major companies exhibit a strong need for making their machine learning systems and services amendable for the open-world setting but lack fundamental and systematic knowledge. Hence, our research advances the understanding of open-world machine learning and helps the industry improve ML systems by discovering insights and structures from unlabeled data.

## 7 Related Work

**Semi-supervised learning.** Semi-supervised learning (SSL) is a classic problem in machine learning. SSL typically assumes the same class space between labeled and unlabeled data, and hence remains closed-world. A rich line of empirical works [9; 13; 21; 29; 37; 38; 39; 42; 48; 50; 53; 54; 74; 76; 78] and theoretical efforts [3; 46; 47; 51; 60; 61; 73] have been made to address this problem. An important class of SSL methods is to represent data as graphs and predict labels by aggregating proximal nodes' labels [1; 18; 80; 84; 85; 1; 1]. Different from classic SSL, we allow its semantic space to cover both known and novel classes. Accordingly, we contribute a graph-theoretic framework tailored to the open-world setting, and reveal new insights on how the labeled data can benefit the clustering performance on both known and novel classes.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{**CIFAR-10**} & \multicolumn{3}{c}{**CIFAR-100**} \\  & **All** & **Novel** & **Known** & **All** & **Novel** & **Known** \\ \hline
**FixMatch**[37] & 49.5 & 50.4 & 71.5 & 20.3 & 23.5 & 39.6 \\
**DS\({}^{\text{s}}\)L**[21] & 40.2 & 45.3 & 77.6 & 24.0 & 23.7 & 55.1 \\
**CGDL**[62] & 39.7 & 44.6 & 72.3 & 23.6 & 22.5 & 49.3 \\
**DTC**[22] & 38.3 & 39.5 & 53.9 & 18.3 & 22.9 & 31.3 \\
**RankStats**[82] & 82.9 & 81.0 & 86.6 & 23.1 & 28.4 & 36.4 \\
**SimCLR**[11] & 51.7 & 63.4 & 58.3 & 22.3 & 21.2 & 28.6 \\ \hline
**ORCA**[7] & \(88.3^{\pm 0.3}\) & \(87.5^{\pm 0.2}\) & \(89.9^{\pm 0.4}\) & \(47.2^{\pm 0.7}\) & \(41.0^{\pm 1.0}\) & \(66.7^{\pm 0.2}\) \\
**GCD**[69] & \(87.5^{\pm 0.5}\) & \(86.7^{\pm 0.4}\) & \(90.1^{\pm 0.4}\) & \(46.8^{\pm 0.5}\) & \(43.4^{\pm 0.7}\) & \(69.7^{\pm 0.4}\) \\
**OpenCon**[63] & \(90.4^{\pm 0.6}\) & \(91.1^{\pm 0.1}\) & \(89.3^{\pm 0.2}\) & \(52.7^{\pm 0.6}\) & \(47.8^{\pm 0.6}\) & \(69.1^{\pm 0.3}\) \\
**SORL (Ours)** & \(\textbf{93.5}^{\pm 1.0}\) & \(\textbf{92.5}^{\pm 0.1}\) & \(\textbf{94.0}^{\pm 0.2}\) & \(\textbf{56.1}^{\pm 0.3}\) & \(\textbf{52.0}^{\pm 0.2}\) & \(68.2^{\pm 0.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Main Results. Mean and std are estimated on five different runs. Baseline numbers are from [7; 63].

**Open-world semi-supervised learning**. The learning setting that considers both labeled and unlabeled data with a mixture of known and novel classes is first proposed in [7] and inspires a proliferation of follow-up works [49; 52; 63; 69; 81] advancing empirical success. Most works put emphasis on learning high-quality embeddings [49; 63; 69; 81]. In particular, Sun and Li [63] employ contrastive learning with both supervised and self-supervised signals, which aligns with our theoretical setup in Sec. 3.1. Different from prior works, our paper focuses on _advancing theoretical understanding_. To the best of our knowledge, we are the first to theoretically investigate the problem from a graph-theoretic perspective and provide a rigorous error bound.

**Spectral graph theory.** Spectral graph theory is a classic research problem [10; 14; 33; 40; 44; 70], which aims to partition the graph by studying the eigenspace of the adjacency matrix. The spectral graph theory is also widely applied in machine learning [1; 6; 45; 56; 58; 64; 86]. Recently, HaoChen et al. [23] derive a spectral contrastive loss from the factorization of the graph's adjacency matrix which facilitates theoretical study in unsupervised domain adaptation [24; 57]. In these works, the graph's formulation is exclusively based on unlabeled data. Sun et al. [64] later expanded this spectral contrastive loss approach to cater to learning environments that encompass both labeled data from known classes and unlabeled data from novel ones. In this paper, our adaptation of the loss function from [64] is tailored to address the open-world semi-supervised learning challenge, considering known class samples within unlabeled data.

**Theory for self-supervised learning.** A proliferation of works in self-supervised representation learning demonstrates the empirical success [5; 8; 11; 12; 23; 26; 68; 77] with the theoretical foundation by providing provable guarantees on the representations learned by contrastive learning for linear probing [4; 41; 55; 59; 66; 67]. From the graphic view, HaoChen et al. [23; 24], Shen et al. [57] model the pairwise relation by the augmentation probability and provided error analysis of the downstream tasks. The existing body of work has mostly focused on _unsupervised learning_. In this paper, we systematically investigate how the label information can change the representation manifold and affect the downstream clustering performance on both known and novel classes.

## 8 Conclusion

In this paper, we present a graph-theoretic framework for open-world semi-supervised learning. The framework facilitates the understanding of how representations change as a result of adding labeling information to the graph. Specifically, we learn representation through Spectral Open-world Representation Learning (SORL). Minimizing this objective is equivalent to factorizing the graph's adjacency matrix, which allows us to analyze the clustering error difference between having vs. excluding labeled data. Our main results suggest that the clustering error can be significantly reduced if the connectivity to the labeled data is stronger than their self-clusterability. Our framework is also empirically appealing to use since it achieves competitive performance on par with existing baselines. Nevertheless, we acknowledge two limitations to practical application within our theoretical construct:

* The augmentation graph serves as a potent theoretical tool for elucidating the success of modern representation learning methods. However, it is challenging to ensure that current augmentation strategies, such as cropping, color jittering, can transform two dissimilar images into identical ones.
* The utilization of Theorems 4.1 and 4.2 necessitates an explicit knowledge of the adjacency matrix of the augmentation graph, a requirement that can be intractable in practice.

In light of these limitations, we encourage further research to enhance the practicality of these theoretical findings. We also hope our framework and insights can inspire the broader representation learning community to understand the role of labeling prior.

## Acknowledgement

Research is supported by the AFOSR Young Investigator Program under award number FA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, Office of Naval Research under grant number N00014-23-1-2643, and faculty research awards/gifts from Google and Meta. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements either expressed or implied, of the sponsors. The authors would also like to thank Tengyu Ma, Xuefeng Du, and Yifei Ming for their helpful suggestions and feedback.

## References

* [1] Andreas Argyriou, Mark Herbster, and Massimiliano Pontil. Combining graph laplacians for semi-supervised learning. _Advances in Neural Information Processing Systems_, 18, 2005.
* [2] Haoyue Bai, Gregory Canal, Xuefeng Du, Jeongyeol Kwon, Robert D Nowak, and Yixuan Li. Feed two birds with one scone: Exploiting wild data for both out-of-distribution generalization and detection. In _International Conference on Machine Learning_, pages 1454-1471. PMLR, 2023.
* [3] Maria-Florina Balcan and Avrim Blum. A pac-style model for learning from labeled and unlabeled data. In _Colt_, pages 111-126. Springer, 2005.
* [4] Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods. _Advances in Neural Information Processing Systems_, 2022.
* [5] Adrien Bardes, Jean Ponce, and Yann Lecun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. In _ICLR 2022-10th International Conference on Learning Representations_, 2022.
* [6] Avrim Blum. Learning form labeled and unlabeled data using graph mincuts. In _Proc. 18th International Conference on Machine Learning, 2001_, 2001.
* [7] Kaidi Cao, Maria Brbic, and Jure Leskovec. Open-world semi-supervised learning. In _Proceedings of the International Conference on Learning Representations_, 2022.
* [8] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. _Proceedings of Advances in Neural Information Processing Systems_, 2020.
* [9] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien, editors. _Semi-Supervised Learning_. The MIT Press, 2006.
* [10] Jeff Cheeger. A lower bound for the smallest eigenvalue of the laplacian. In _Problems in analysis_, pages 195-200. Princeton University Press, 2015.
* [11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _Proceedings of the international conference on machine learning_, pages 1597-1607. PMLR, 2020.
* [12] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15750-15758, 2021.
* [13] Yanbei Chen, Xiatian Zhu, Wei Li, and Shaogang Gong. Semi-supervised learning under class distribution mismatch. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 3569-3576, 2020.
* [14] Fan RK Chung. _Spectral graph theory_, volume 92. American Mathematical Soc., 1997.
* [15] Xuefeng Du, Gabriel Gozum, Yifei Ming, and Yixuan Li. Siren: Shaping representations for detecting out-of-distribution objects. _Advances in Neural Information Processing Systems_, 35: 20434-20449, 2022.
* [16] Xuefeng Du, Yiyou Sun, Xiaojin Zhu, and Yixuan Li. Dream the impossible: Outlier imagination with diffusion models. _Advances in Neural Information Processing Systems_, 2023.
* [17] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1(3):211-218, 1936.
* [18] Rob Fergus, Yair Weiss, and Antonio Torralba. Semi-supervised learning in gigantic image collections. _Advances in neural information processing systems_, 22, 2009.

* [19] Enrico Fini, Enver Sangineto, Stephane Lathuiliere, Zhun Zhong, Moin Nabi, and Elisa Ricci. A unified objective for novel class discovery. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9284-9292, 2021.
* [20] Anne Greenbaum, Ren-cang Li, and Michael L Overton. First-order perturbation theory for eigenvalues and eigenvectors. _SIAM review_, 62(2):463-482, 2020.
* [21] Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. Safe deep semi-supervised learning for unseen-class unlabeled data. In _Proceedings of the international conference on machine learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 3897-3906. PMLR, 13-18 Jul 2020.
* [22] Kai Han, Andrea Vedaldi, and Andrew Zisserman. Learning to discover novel visual categories via deep transfer clustering. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019.
* [23] Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised deep learning with spectral contrastive loss. _Advances in Neural Information Processing Systems_, 34:5000-5011, 2021.
* [24] Jeff Z HaoChen, Colin Wei, Ananya Kumar, and Tengyu Ma. Beyond separability: Analyzing the linear transferability of contrastive representations to related subpopulations. _Advances in Neural Information Processing Systems_, 2022.
* [25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 9729-9738, 2020.
* [27] Yen-Chang Hsu, Zhaoyang Lv, and Zsolt Kira. Learning to cluster in order to transfer across domains and tasks. _Proceedings of the International Conference on Learning Representations_, 2018.
* [28] Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, and Zsolt Kira. Multi-class classification without multi-class labels. _Proceedings of the International Conference on Learning Representations_, 2019.
* [29] Junkai Huang, Chaowei Fang, Weikai Chen, Zhenhua Chai, Xiaolin Wei, Pengxu Wei, Liang Lin, and Guanbin Li. Trash to treasure: Harvesting odd data with cross-modal matching for open-set semi-supervised learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8310-8319, 2021.
* [30] Tony Jebara, Jun Wang, and Shih-Fu Chang. Graph construction and b-matching for semi-supervised learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 441-448, 2009.
* [31] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _CVPR_, 2017.
* [32] Antony Joseph and Bin Yu. Impact of regularization on spectral clustering. _The Annals of Statistics_, 44(4):1765-1791, 2016.
* [33] Ravi Kannan, Santosh Vempala, and Adrian Vetta. On clusterings: Good, bad and spectral. _Journal of the ACM (JACM)_, 51(3):497-515, 2004.
* [34] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _Advances in Neural Information Processing Systems_, 33:18661-18673, 2020.

* [35] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [36] Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* [37] Alex Kurakin, Chun-Liang Li, Colin Raffel, David Berthelot, Ekin Dogus Cubuk, Han Zhang, Kihyuk Sohn, Nicholas Carlini, and Zizhao Zhang. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _Proceedings of Advances in Neural Information Processing Systems_, 2020.
* [38] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. _Proceedings of the International Conference on Learning Representations_, 2017.
* [39] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on challenges in representation learning, ICML_, page 896, 2013.
* [40] James R Lee, Shayan Oveis Gharan, and Luca Trevisan. Multiway spectral partitioning and higher-order cheeger inequalities. _Journal of the ACM (JACM)_, 61(6):1-30, 2014.
* [41] Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps: Provable self-supervised learning. _Advances in Neural Information Processing Systems_, 34:309-323, 2021.
* [42] Wei Liu, Junfeng He, and Shih-Fu Chang. Large graph construction for scalable semi-supervised learning. In _Proceedings of the 27th international conference on machine learning (ICML-10)_, pages 679-686. Citeseer, 2010.
* [43] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold approximation and projection. _The Journal of Open Source Software_, 3(29):861, 2018.
* [44] Frank McSherry. Spectral partitioning of random graphs. In _Proceedings 42nd IEEE Symposium on Foundations of Computer Science_, pages 529-537. IEEE, 2001.
* [45] Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. _Advances in neural information processing systems_, 14, 2001.
* [46] Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. _Journal of Machine Learning Research_, 14(5), 2013.
* [47] Samet Oymak and Talha Cihad Gulcu. A theoretical characterization of semi-supervised learning with self-training for gaussian mixture models. In _International Conference on Artificial Intelligence and Statistics_, pages 3601-3609. PMLR, 2021.
* [48] Jongjin Park, Sukmin Yun, Jongheon Jeong, and Jinwoo Shin. Opencos: Contrastive semi-supervised learning for handling open-set unlabeled data. In _Computer Vision-ECCV 2022 Workshops: Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part II_, pages 134-149. Springer, 2023.
* [49] Nan Pu, Zhun Zhong, and Nicu Sebe. Dynamic conceptional contrastive learning for generalized category discovery. 2023.
* [50] Sylvestre-Alvise Rebuffi, Sebastien Ehrhardt, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Semi-supervised learning with scarce annotations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 762-763, 2020.
* [51] Philippe Rigollet. Generalization error bounds in semi-supervised classification under the cluster assumption. _Journal of Machine Learning Research_, 8(7), 2007.
* [52] Mamshad Nayeem Rizve, Navid Kardan, Salman Khan, Fahad Shahbaz Khan, and Mubarak Shah. Openldn: Learning to discover novel classes for open-world semi-supervised learning. _Proceedings of the European Conference on Computer Vision_, 2022.

* Saito et al. [2021] Kuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set consistency regularization for semi-supervised learning with outliers. _Proceedings of Advances in Neural Information Processing Systems_, 2021.
* Sajjadi et al. [2016] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformations and perturbations for deep semi-supervised learning. _Proceedings of Advances in Neural Information Processing Systems_, 29, 2016.
* Saunshi et al. [2019] Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandepparkar. A theoretical analysis of contrastive unsupervised representation learning. In _International Conference on Machine Learning_, pages 5628-5637. PMLR, 2019.
* Shaham et al. [2018] Uri Shaham, Kelly Stanton, Henry Li, Boaz Nadler, Ronen Basri, and Yuval Kluger. Spectralnet: Spectral clustering using deep neural networks. In _6th International Conference on Learning Representations, ICLR 2018_, 2018.
* Shen et al. [2022] Kendrick Shen, Robbie M Jones, Ananya Kumar, Sang Michael Xie, Jeff Z HaoChen, Tengyu Ma, and Percy Liang. Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In _International Conference on Machine Learning_, pages 19847-19878. PMLR, 2022.
* Shi and Malik [2000] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. _IEEE Transactions on pattern analysis and machine intelligence_, 22(8):888-905, 2000.
* Shi et al. [2023] Zhenmei Shi, Jiefeng Chen, Kunyang Li, Jayaram Raghuram, Xi Wu, Yingyu Liang, and Somesh Jha. The trade-off between universality and label efficiency of representations from contrastive learning. In _The Eleventh International Conference on Learning Representations_, 2023.
* Singh et al. [2008] Aarti Singh, Robert Nowak, and Jerry Zhu. Unlabeled data: Now it helps, now it doesn't. _Advances in neural information processing systems_, 21, 2008.
* Sokolovska et al. [2008] Nataliya Sokolovska, Olivier Cappe, and Francois Yvon. The asymptotics of semi-supervised learning in discriminative probabilistic models. In _Proceedings of the 25th international conference on Machine learning_, pages 984-991, 2008.
* Sun et al. [2020] Xin Sun, Zhenning Yang, Chi Zhang, Keck-Voon Ling, and Guohao Peng. Conditional gaussian distribution learning for open set recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 13480-13489, 2020.
* Sun and Li [2023] Yiyou Sun and Yixuan Li. Opencon: Open-world contrastive learning. _Transaction on Machine Learning Research_, 2023.
* Sun et al. [2023] Yiyou Sun, Zhenmei Shi, Yingyu Liang, and Yixuan Li. When and how does known class help discover unknown ones? Provable understanding through spectral analysis. In _Proceedings of the 40th International Conference on Machine Learning_, Proceedings of Machine Learning Research, pages 33014-33043. PMLR, 2023.
* Tao et al. [2023] Leitian Tao, Xuefeng Du, Xiaojin Zhu, and Yixuan Li. Non-parametric outlier synthesis. _International Conference on Learning Representations_, 2023.
* Tosh et al. [2021] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic posterior information to linear models. _J. Mach. Learn. Res._, 22:281-1, 2021.
* Tosh et al. [2021] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In _Algorithmic Learning Theory_, pages 1179-1206. PMLR, 2021.
* Van den Oord et al. [2018] Aaron Van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. _arXiv e-prints_, pages arXiv-1807, 2018.
* Vaze et al. [2022] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Generalized category discovery. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2022.

* [70] Ulrike Von Luxburg. A tutorial on spectral clustering. _Statistics and computing_, 17(4):395-416, 2007.
* [71] Fei Wang and Changshui Zhang. Label propagation through linear neighborhoods. In _Proceedings of the 23rd international conference on Machine learning_, pages 985-992, 2006.
* [72] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In _International Conference on Machine Learning_, pages 9929-9939. PMLR, 2020.
* [73] Larry Wasserman and John Lafferty. Statistical analysis of semi-supervised regression. _Advances in Neural Information Processing Systems_, 20, 2007.
* [74] Fan Yang, Kai Wu, Shuyi Zhang, Guannan Jiang, Yong Liu, Feng Zheng, Wei Zhang, Chengjie Wang, and Long Zeng. Class-aware contrastive semi-supervised learning. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2022.
* [75] Muli Yang, Yuehua Zhu, Jiaping Yu, Aming Wu, and Cheng Deng. Divide and conquer: Compositional experts for generalized novel class discovery. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14268-14277, 2022.
* [76] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum framework for open-set semi-supervised learning. In _Proceedings of the European Conference on Computer Vision_, pages 438-454. Springer, 2020.
* [77] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In _International Conference on Machine Learning_, pages 12310-12320. PMLR, 2021.
* [78] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1476-1485, 2019.
* [79] Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun, Xuefeng Du, Kaiyang Zhou, Wayne Zhang, et al. Openood v1. 5: Enhanced benchmark for out-of-distribution detection. _arXiv preprint arXiv:2306.09301_, 2023.
* [80] Kai Zhang, James T Kwok, and Bahram Parvin. Prototype vector machine for large scale semi-supervised learning. In _Proceedings of the 26th Annual International Conference on Machine Learning_, pages 1233-1240, 2009.
* [81] Sheng Zhang, Salman Khan, Zhiqiang Shen, Muzammal Naseer, Guangyi Chen, and Fahad Khan. Promptcal: Contrastive affinity learning via auxiliary prompts for generalized novel category discovery. 2022.
* [82] Bingchen Zhao and Kai Han. Novel visual category discovery with dual ranking statistics and mutual knowledge distillation. _Proceedings of Advances in Neural Information Processing Systems_, 34, 2021.
* [83] Zhun Zhong, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, and Nicu Sebe. Neighborhood contrastive learning for novel class discovery. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 10867-10875, 2021.
* [84] Dengyong Zhou, Thomas Hofmann, and Bernhard Scholkopf. Semi-supervised learning on directed graphs. _Advances in neural information processing systems_, 17, 2004.
* [85] Xiaojin Zhu. Learning from labeled and unlabeled data with label propagation. _Tech. Report_, 2002.
* [86] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In _Proceedings of the 20th International conference on Machine learning (ICML-03)_, pages 912-919, 2003.

**Appendix**

## Appendix A Technical Details of Spectral Open-world Representation Learning

**Theorem A.1**.: _(Recap of Theorem 3.1) We define \(\mathbf{f}_{x}=\sqrt{w_{x}}f(x)\) for some function \(f\). Recall \(\eta_{u},\eta_{l}\) are two hyper-parameters defined in Eq. (1). Then minimizing the loss function \(\mathcal{L}_{\mathrm{mf}}(F,A)\) is equivalent to minimizing the following loss function for \(f\), which we term **Spectral Open-world Representation Learning (SORL)**:_

\[\mathcal{L}_{\text{SORL}}(f) \triangleq-2\eta_{l}\mathcal{L}_{1}(f)-2\eta_{u}\mathcal{L}_{2}(f)\] (10) \[\quad+\eta_{l}^{2}\mathcal{L}_{3}(f)+2\eta_{l}\eta_{u}\mathcal{L }_{4}(f)+\eta_{u}^{2}\mathcal{L}_{5}(f),\]

_where_

\[\mathcal{L}_{1}(f)=\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E}} \limits_{\begin{subarray}{c}\bar{x}_{l}\sim\mathcal{P}_{l_{i}},\bar{x}^{ \prime}\sim\mathcal{P}_{l_{i}},\\ x\sim\mathcal{T}\left(:|\bar{x}_{l}\right),x^{\prime}\sim\mathcal{T}\left(:| \bar{x}^{\prime}_{l}\right)\end{subarray}}\left[f(x)^{\top}f\left(x^{+}\right) \right],\] \[\mathcal{L}_{2}(f)=\mathop{\mathbb{E}}\limits_{\begin{subarray}{ c}\bar{x}_{u}\sim\mathcal{P},\\ x\sim\mathcal{T}\left(:|\bar{x}_{u}\right),x^{\prime}\sim\mathcal{T}\left(:| \bar{x}_{u}\right)\end{subarray}}\left[f(x)^{\top}f\left(x^{+}\right)\right],\] \[\mathcal{L}_{3}(f)=\sum_{i,j\in\mathcal{Y}_{l}}\mathop{\mathbb{E} }\limits_{\begin{subarray}{c}\bar{x}_{l}\sim\mathcal{P}_{l_{i}},\bar{x}^{ \prime}\sim\mathcal{P}_{l_{j}},\\ x\sim\mathcal{T}\left(:|\bar{x}_{l}\right),x^{\prime}\sim\mathcal{T}\left(:| \bar{x}^{\prime}_{l}\right)\end{subarray}}\left[\left(f(x)^{\top}f\left(x^{-} \right)\right)^{2}\right],\] \[\mathcal{L}_{4}(f)=\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E} }\limits_{\begin{subarray}{c}\bar{x}_{l}\sim\mathcal{P}_{l_{i}},\bar{x}_{u} \sim\mathcal{P},\\ x\sim\mathcal{T}\left(:|\bar{x}_{l}\right),x^{\prime}\sim\mathcal{T}\left(:| \bar{x}_{u}\right)\end{subarray}}\left[\left(f(x)^{\top}f\left(x^{-}\right) \right)^{2}\right],\] \[\mathcal{L}_{5}(f)=\mathop{\mathbb{E}}\limits_{\begin{subarray}{ c}\bar{x}_{u}\sim\mathcal{P},\\ x\sim\mathcal{T}\left(:|\bar{x}_{u}\right),x^{\prime}\sim\mathcal{T}\left(:| \bar{x}^{\prime}_{u}\right)\end{subarray}}\left[\left(f(x)^{\top}f\left(x^{-} \right)\right)^{2}\right].\]

Proof.: We can expand \(\mathcal{L}_{\mathrm{mf}}(F,A)\) and obtain

\[\mathcal{L}_{\mathrm{mf}}(F,A)= \sum_{x,x^{\prime}\in\mathcal{X}}\left(\frac{w_{xx^{\prime}}}{ \sqrt{w_{x}w_{x^{\prime}}}}-\mathbf{f}_{x}^{\top}\mathbf{f}_{x^{\prime}}\right) ^{2}\] \[= \text{const}+\sum_{x,x^{\prime}\in\mathcal{X}}\left(-2w_{xx^{ \prime}}f(x)^{\top}f\left(x^{\prime}\right)+w_{x}w_{x^{\prime}}\left(f(x)^{ \top}f\left(x^{\prime}\right)\right)^{2}\right),\]

where \(\mathbf{f}_{x}=\sqrt{w_{x}}f(x)\) is a re-scaled version of \(f(x)\). At a high level, we follow the proof in [23], while the specific form of loss varies with the different definitions of positive/negative pairs. The form of \(\mathcal{L}_{\text{SORL}}(f)\) is derived from plugging \(w_{xx^{\prime}}\) and \(w_{x}\).

Recall that \(w_{xx^{\prime}}\) is defined by

\[w_{xx^{\prime}}=\eta_{l}\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E}}\limits_{ \bar{x}_{i}\sim\mathcal{P}_{l_{i}}}\mathop{\mathbb{E}}\nolimits_{\bar{x}^{ \prime}_{i}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x|\bar{x}_{l})\mathcal{T}\left(x ^{\prime}|\bar{x}^{\prime}_{l}\right)+\eta_{u}\mathbb{E}_{\bar{x}_{u}\sim \mathcal{P}}\mathcal{T}(x|\bar{x}_{u})\mathcal{T}\left(x^{\prime}|\bar{x}_{u} \right),\]

and \(w_{x}\) is given by

\[w_{x} =\sum_{x^{\prime}}w_{xx^{\prime}}\] \[=\eta_{l}\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E}}\nolimits_ {\bar{x}_{i}\sim\mathcal{P}_{l_{i}}}\mathop{\mathbb{E}}\nolimits_{\bar{x}^{ \prime}_{i}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x|\bar{x}_{l})\sum_{x^{\prime}} \mathcal{T}\left(x^{\prime}|\bar{x}^{\prime}_{l}\right)+\eta_{u}\mathbb{E}_{\bar{x} _{u}\sim\mathcal{P}}\mathcal{T}(x|\bar{x}_{u})\sum_{x^{\prime}}\mathcal{T}\left(x ^{\prime}|\bar{x}_{u}\right)\] \[=\eta_{l}\sum_{i\in\mathcal{Y}_{l}}\mathop{\mathbb{E}}\nolimits_{ \bar{x}_{i}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x|\bar{x}_{l})+\eta_{u}\mathbb{E}_{ \bar{x}_{u}\sim\mathcal{P}}\mathcal{T}(x|\bar{x}_{u}).\]

Plugging in \(w_{xx^{\prime}}\) we have,\[= -2\eta_{l}\mathcal{L}_{1}(f)-2\eta_{u}\mathcal{L}_{2}(f).\]

Plugging \(w_{x}\) and \(w_{x^{\prime}}\) we have,

\[\sum_{x,x^{\prime}\in\mathcal{X}}w_{x}w_{x^{\prime}}\left(f(x)^{ \top}f\left(x^{\prime}\right)\right)^{2}\] \[= \sum_{x,x^{\prime}\in\mathcal{X}}w_{x}w_{x^{-}}\left(f(x)^{\top} f\left(x^{-}\right)\right)^{2}\] \[= \sum_{x,x^{\prime}\in\mathcal{X}}\left(\eta_{l}\sum_{i\in \mathcal{Y}_{l}}\mathbb{E}_{\bar{x}_{l}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x| \bar{x}_{l})+\eta_{u}\mathbb{E}_{\bar{x}_{u}\sim\mathcal{P}}\mathcal{T}(x|\bar {x}_{u})\right)\] \[\qquad\cdot\left(\eta_{l}\sum_{j\in\mathcal{Y}_{l}}\mathbb{E}_{ \bar{x}^{\prime}_{l}\sim\mathcal{P}_{l_{j}}}\mathcal{T}(x^{-}|\bar{x}^{\prime }_{l})+\eta_{u}\mathbb{E}_{\bar{x}^{\prime}_{u}\sim\mathcal{P}}\mathcal{T}(x^ {-}|\bar{x}^{\prime}_{u})\right)\left(f(x)^{\top}f\left(x^{-}\right)\right)^{2}\] \[= \eta_{l}^{2}\sum_{x,x^{-}\in\mathcal{X}}\sum_{i\in\mathcal{Y}_{l }}\mathbb{E}_{\bar{x}_{l}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x|\bar{x}_{l}) \sum_{j\in\mathcal{Y}_{l}}\mathbb{E}_{\bar{x}^{\prime}_{l}\sim\mathcal{P}_{l_{ j}}}\mathcal{T}(x^{-}|\bar{x}^{\prime}_{l})\left(f(x)^{\top}f\left(x^{-} \right)\right)^{2}\] \[+2\eta_{u}\eta_{l}\sum_{x,x^{-}\in\mathcal{X}}\sum_{i\in\mathcal{ Y}_{l}}\mathbb{E}_{\bar{x}_{l}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x|\bar{x}_{l}) \mathbb{E}_{\bar{x}_{u}\sim\mathcal{P}}\mathcal{T}(x^{-}|\bar{x}_{u})\left(f(x )^{\top}f\left(x^{-}\right)\right)^{2}\] \[+\eta_{u}^{2}\sum_{x,x^{-}\in\mathcal{X}}\mathbb{E}_{\bar{x}_{u} \sim\mathcal{P}}\mathcal{T}(x|\bar{x}_{u})\mathbb{E}_{\bar{x}^{\prime}_{u} \sim\mathcal{P}}\mathcal{T}(x^{-}|\bar{x}^{\prime}_{u})\left(f(x)^{\top}f \left(x^{-}\right)\right)^{2}\] \[= \eta_{l}^{2}\sum_{i\in\mathcal{Y}_{l}}\sum_{j\in\mathcal{Y}_{l}} \mathbb{E}_{\bar{x}_{l}\sim\mathcal{P}_{l_{i}},\bar{x}^{\prime}_{l}\sim \mathcal{P}_{l_{j}}}\left[\left(f(x)^{\top}f\left(x^{-}\right)\right)^{2}\right]\] \[+2\eta_{u}\eta_{l}\sum_{i\in\mathcal{Y}_{l}}\mathbb{E}_{\bar{x}_{ l}\sim\mathcal{P}_{l_{i}},\bar{x}_{u}\sim\mathcal{P},}\left[\left(f(x)^{\top}f \left(x^{-}\right)\right)^{2}\right]\] \[+\eta_{u}^{2}\begin{matrix}\mathbb{E}_{\bar{x}_{u}\sim\mathcal{P},\bar{x}^{\prime}_{u}\sim\mathcal{P},}\\ x\sim\mathcal{T}(\cdot|\bar{x}_{u}),x^{-}\sim\mathcal{T}(\cdot|\bar{x}^{\prime}_{ u})\end{matrix}\] \[= \eta_{l}^{2}\mathcal{L}_{3}(f)+2\eta_{u}\eta_{l}\mathcal{L}_{4}(f )+\eta_{u}^{2}\mathcal{L}_{5}(f).\]Technical Details for Toy Example

### Calculation Details for Figure 2.

We first recap the toy example, which illustrates the core idea of our theoretical findings. Specifically, the example aims to distinguish 3D objects with different shapes, as shown in Figure 2. These images are generated by a 3D rendering software [31] with user-defined properties including colors, shape, size, position, etc.

**Data design.** Suppose the training samples come from three types, \(\mathcal{X}_{\overrightarrow{\Box}}\), \(\mathcal{X}_{\overrightarrow{\Box}}\), \(\mathcal{X}_{\overrightarrow{\Box}}\). Let \(\mathcal{X}_{\overrightarrow{\Box}}\) be the sample space with **known** class, and \(\mathcal{X}_{\overleftarrow{\Box}},\mathcal{X}_{\overleftarrow{\Box}}\) be the sample space with **novel** classes. Further, the two novel classes are constructed to have different relationships with the known class. Specifically, we construct the toy dataset with 6 elements as shown in Figure 4(a).

**Augmentation graph.** Based on the data design, we formally define the augmentation graph, which encodes the probability of augmenting a source image \(\bar{x}\) to the augmented view \(x\):

\[\mathcal{T}\left(x\mid\bar{x}\right)=\left\{\begin{array}{ll}\tau_{1}&\text {if color}(x)=\text{color}(\bar{x}),\text{shape}(x)=\text{shape}(\bar{x});\\ \tau_{c}&\text{if color}(x)=\text{color}(\bar{x}),\text{shape}(x)\neq\text{ shape}(\bar{x});\\ \tau_{s}&\text{if color}(x)\neq\text{color}(\bar{x}),\text{shape}(x)=\text{ shape}(\bar{x});\\ \tau_{0}&\text{if color}(x)\neq\text{color}(\bar{x}),\text{shape}(x)\neq\text{ shape}(\bar{x}).\end{array}\right.\] (11)

According to the definition above, the corresponding augmentation matrix \(T\) with each element formed by \(\mathcal{T}(\cdot\mid\cdot)\) is given in Figure 4(b). We proceed by showing the details to derive \(A^{(u)}\) and \(A\) using \(T\).

**Derivation details for \(A^{(u)}\) and \(A\).** Recall that each element of \(A^{(u)}\) is formed by \(w^{(u)}_{xx^{\prime}}=\mathbb{E}_{\bar{x}\sim\mathcal{P}}\mathcal{T}(x|\bar{x })\mathcal{T}\left(x^{\prime}|\bar{x}\right).\) In this toy example, one can then see that \(A^{(u)}=\frac{1}{6}TT^{\top}\) since augmentation matrix \(T\) is defined that each element \(T_{x\bar{x}}=\mathcal{T}(x|\bar{x})\). Note that \(T\) is explicitly given in Figure 4(b) and then if we let \(\eta_{u}=6\), we have the close-from:

\[\eta_{u}A^{(u)}=T^{2}=\left[\begin{array}{cccccc}\tau_{1}^{2}+\tau_{s}^{2}+ \tau_{c}^{2}&2\tau_{1}\tau_{s}&2\tau_{1}\tau_{c}&2\tau_{c}\tau_{s}&0&0\\ 2\tau_{1}\tau_{s}&\tau_{1}^{2}+\tau_{s}^{2}+\tau_{c}^{2}&2\tau_{c}\tau_{s}&2 \tau_{1}\tau_{c}&0&0\\ 2\tau_{1}\tau_{c}&2\tau_{c}\tau_{s}&\tau_{1}^{2}+\tau_{s}^{2}+\tau_{c}^{2}&2 \tau_{1}\tau_{s}&0&0\\ 2\tau_{c}\tau_{s}&2\tau_{1}\tau_{c}&2\tau_{1}\tau_{s}&\tau_{1}^{2}+\tau_{s}^{2 }+\tau_{c}^{2}&0&0\\ 0&0&0&0&2\tau_{1}^{2}&2\tau_{1}^{2}\\ 0&0&0&0&2\tau_{1}^{2}&2\tau_{1}^{2}\\ \end{array}\right].\]

Figure 4: An illustrative example for theoretical analysis. We consider a 6-node graph with one known class (cube) and two novel classes (sphere, cylinder). (a) The augmentation probabilities between nodes are defined by their color and shape in Eq. (11). (b) The augmentation matrices \(T\) derived by Eq. (11) where we let \(\tau_{0}=0\).

We then derive the second part \(A^{(l)}\) whose element is given by:

\[w^{(l)}_{xx^{\prime}}\triangleq\sum_{i\in\mathcal{Y}_{l}}\mathbb{E}_{\bar{x}_{l} \sim\mathcal{P}_{l_{i}}}\mathbb{E}_{\bar{x}^{\prime}_{l}\sim\mathcal{P}_{l_{i}}} \mathcal{T}(x|\bar{x}_{l})\mathcal{T}\left(x^{\prime}|\bar{x}^{\prime}_{l} \right).\]

Such a form can be simplified in Section 4 by defining \(\mathfrak{l}\in\mathbb{R}^{N},(\mathfrak{l})_{x}=\mathbb{E}_{\bar{x}_{l}\sim \mathcal{P}_{l_{1}}}\mathcal{T}(x|\bar{x}_{l})\) and by letting \(|\mathcal{Y}_{l}|=1\). In this toy example, the known class only has two elements, so \(\mathfrak{l}=\frac{1}{2}(T_{:,1}+T_{:,2})\) (average of \(T\)'s 1st & 2nd column), we then have:

\[A^{(l)}=\mathfrak{l}^{\top}=\frac{1}{4}\left[\begin{array}{cccc}\left( \tau_{1}+\tau_{s}\right)^{2}&\left(\tau_{1}+\tau_{s}\right)^{2}&\tau_{c}\left( \tau_{1}+\tau_{s}\right)&\tau_{c}\left(\tau_{1}+\tau_{s}\right)&0&0\\ \left(\tau_{1}+\tau_{s}\right)^{2}&\left(\tau_{1}+\tau_{s}\right)^{2}&\tau_{c }\left(\tau_{1}+\tau_{s}\right)&\tau_{c}\left(\tau_{1}+\tau_{s}\right)&0&0\\ \tau_{c}\left(\tau_{1}+\tau_{s}\right)&\tau_{c}\left(\tau_{1}+\tau_{s}\right)& \tau_{c}^{2}&\tau_{c}^{2}&0&0\\ \tau_{c}\left(\tau_{1}+\tau_{s}\right)&\tau_{c}\left(\tau_{1}+\tau_{s}\right)& \tau_{c}^{2}&\tau_{c}^{2}&0&0\\ 0&0&0&0&0&0\\ 0&0&0&0&0&0\end{array}\right].\]

Finally, if we let \(\eta_{l}=4\) and \(A=\eta_{u}A^{(u)}+\eta_{l}A^{(l)}\), we have the full results in Figure 2.

### Calculation Details for Figure 3.

In this section, we present the analysis of eigenvectors and their orders for toy examples shown in Figure 2. In Theorem B.1 we present the spectral analysis for the adjacency matrix with additional label information while in Theorem B.2, we show the spectral analysis for the unlabeled case.

**Theorem B.1**.: _Let_

\[\eta_{u}A^{(u)}=\left[\begin{array}{cccccc}\tau_{1}^{2}+\tau_{s}^{2}+\tau_ {c}^{2}&2\tau_{1}\tau_{s}&2\tau_{1}\tau_{c}&2\tau_{c}\tau_{s}&0&0\\ 2\tau_{1}\tau_{s}&\tau_{1}^{2}+\tau_{s}^{2}+\tau_{c}^{2}&2\tau_{c}\tau_{s}&2 \tau_{1}\tau_{c}&0&0\\ 2\tau_{1}\tau_{c}&2\tau_{c}\tau_{s}&\tau_{1}^{2}+\tau_{s}^{2}+\tau_{c}^{2}&2 \tau_{1}\tau_{s}&0&0\\ 2\tau_{c}\tau_{s}&2\tau_{1}\tau_{c}&2\tau_{1}\tau_{s}&\tau_{1}^{2}+\tau_{s}^{2 }+\tau_{c}^{2}&0&0\\ 0&0&0&0&2\tau_{1}^{2}&2\tau_{1}^{2}\\ 0&0&0&0&2\tau_{1}^{2}&2\tau_{1}^{2}\end{array}\right],\]

\[A=\eta_{u}A^{(u)}+\left[\begin{array}{cccccc}(\tau_{1}+\tau_{s})^{2}&(\tau_ {1}+\tau_{s})^{2}&\tau_{c}(\tau_{1}+\tau_{s})&\tau_{c}(\tau_{1}+\tau_{s})&0&0 \\ (\tau_{1}+\tau_{s})^{2}&(\tau_{1}+\tau_{s})^{2}&\tau_{c}(\tau_{1}+\tau_{s})& \tau_{c}(\tau_{1}+\tau_{s})&0&0\\ \tau_{c}(\tau_{1}+\tau_{s})&\tau_{c}(\tau_{1}+\tau_{s})&\tau_{c}^{2}&\tau_{c}^ {2}&0&0\\ \tau_{c}(\tau_{1}+\tau_{s})&\tau_{c}(\tau_{1}+\tau_{s})&\tau_{c}^{2}&\tau_{c}^ {2}&0&0\\ 0&0&0&0&0&0\end{array}\right],\]

_and we assume that \(1\gg\frac{\tau_{c}}{\tau_{1}}>\frac{\tau_{s}}{\tau_{1}}>0\), \(\frac{4}{9}\tau_{c}\leq\tau_{s}\leq\tau_{c}\) and \(\tau_{1}+\tau_{c}+\tau_{s}=1\)._

_Let \(\lambda_{1},\lambda_{2},\lambda_{3}\) and \(v_{1},v_{2},v_{3}\) be the largest three eigenvalues and their corresponding eigenvectors of \(D^{-\frac{1}{3}}AD^{-\frac{1}{2}}\), which is the normalized adjacency matrix of \(A\). Then the concrete form of \(\lambda_{1},\lambda_{2},\lambda_{3}\) and \(v_{1},v_{2},v_{3}\) can be approximately given by:_

\[\hat{\lambda}_{1}=1,\ \hat{\lambda}_{2}=1,\ \hat{\lambda}_{3}=1- \frac{16}{3}\frac{\tau_{c}}{\tau_{1}},\] \[\hat{v}_{1}=[0,0,0,0,1,1],\] \[\hat{v}_{2}=[\sqrt{3},\sqrt{3},1,1,0,0],\] \[\hat{v}_{3}=[1,1,-\sqrt{3},-\sqrt{3},0,0].\]

_Note that the approximation gap can be tightly bounded. Specifically, for \(i\in\{1,2,3\}\), we have \(|\lambda_{i}-\hat{\lambda}_{i}|\leq O((\frac{\tau_{c}}{\tau_{1}})^{2})\) and \(\|\sin(U,\hat{U})^{6}\|_{F}\leq O(\frac{\tau_{c}}{\tau_{1}})\), where \(U=[v_{1},v_{2},v_{3}],\hat{U}=[\hat{v}_{1},\hat{v}_{2},\hat{v}_{3}]\)._Proof.: By \(\tau_{1}+\tau_{c}+\tau_{s}=1\) and \(1\gg\frac{\tau_{c}}{\tau_{1}}>\frac{\tau_{s}}{\tau_{1}}>0\), we define the following equation which approximates the corresponding terms up to error \(O((\frac{\tau_{c}}{\tau_{1}})^{2})\):

\[A\approx\widehat{A}=\tau_{1}^{2}\left[\begin{array}{ccccc}2+2\frac{\tau_{s} }{\tau_{1}}&1+4\frac{\tau_{s}}{\tau_{1}}&3\frac{\tau_{c}}{\tau_{1}}&\frac{\tau _{s}}{\tau_{1}}&0&0\\ 1+4\frac{\tau_{1}}{\tau_{1}}&2+2\frac{\tau_{1}}{\tau_{1}}&\frac{\tau_{1}}{ \tau_{1}}&3\frac{\tau_{c}}{\tau_{1}}&0&0\\ 3\frac{\tau_{c}}{\tau_{1}}&\frac{\tau_{s}}{\tau_{1}}&1&2\frac{\tau_{s}}{\tau_{ 1}}&1&0&0\\ \frac{\tau_{s}}{\tau_{1}}&3\frac{\tau_{s}}{\tau_{1}}&2\frac{\tau_{s}}{\tau_{ 1}}&1&0&0\\ 0&0&0&0&2&2\end{array}\right].\]

\[D\approx\widehat{D}=\tau_{1}^{2}diag\left(\left[3\left(1+2\frac{\tau_{s}}{ \tau_{1}}+\frac{4}{3}\frac{\tau_{c}}{\tau_{1}}\right),3\left(1+2\frac{\tau_{s} }{\tau_{1}}+\frac{4}{3}\frac{\tau_{c}}{\tau_{1}}\right),1+2\frac{\tau_{s}}{ \tau_{1}}+4\frac{\tau_{c}}{\tau_{1}},1+2\frac{\tau_{s}}{\tau_{1}}+4\frac{\tau _{c}}{\tau_{1}},4,4\right]\right).\]

\[D^{-\frac{1}{2}}\approx\widehat{D^{-\frac{1}{2}}}=\frac{1}{\tau_{1}}diag \left(\left[\sqrt{3}\left(1-\frac{\tau_{s}}{\tau_{1}}-\frac{2}{3}\frac{\tau_{ c}}{\tau_{1}}\right),\sqrt{3}\left(1-\frac{\tau_{s}}{\tau_{1}}-\frac{2}{3}\frac{ \tau_{c}}{\tau_{1}}\right),1-\frac{\tau_{s}}{\tau_{1}}-2\frac{\tau_{c}}{\tau_{ 1}},1-\frac{\tau_{s}}{\tau_{1}}-2\frac{\tau_{c}}{\tau_{1}},2,2\right]\right).\]

\[D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\approx\widehat{D^{-\frac{1}{2}}}\widehat{ AD^{-\frac{1}{2}}}\]

And we have

\[\left\|D^{-\frac{1}{2}}AD^{-\frac{1}{2}}-\widehat{D^{-\frac{1}{2}}} \widehat{AD^{-\frac{1}{2}}}\right\|_{2}\] \[\leq \left\|D^{-\frac{1}{2}}AD^{-\frac{1}{2}}-\widehat{D^{-\frac{1}{2}} }\widehat{AD^{-\frac{1}{2}}}\right\|_{F}\] \[\leq O((\frac{\tau_{c}}{\tau_{1}})^{2}).\]

Let \(\hat{\lambda}_{a},\ldots,\hat{\lambda}_{f}\) be six eigenvalues of \(\widehat{D^{-\frac{1}{2}}}\widehat{AD^{-\frac{1}{2}}}\), and \(\hat{v}_{a},\ldots,\hat{v}_{f}\) be corresponding eigenvectors. By direct calculation we have

\[\hat{\lambda}_{a}=1,\ \ \hat{\lambda}_{b}=1,\ \ \hat{\lambda}_{c}=1-\frac{16}{3}\frac{ \tau_{c}}{\tau_{1}},\ \ \hat{\lambda}_{d}=0\]

and corresponding eigenvectors as

\[\hat{v}_{a}=[0,0,0,0,1,1],\] \[\hat{v}_{b}=[\sqrt{3},\sqrt{3},1,1,0,0],\] \[\hat{v}_{c}=[1,1,-\sqrt{3},-\sqrt{3},0,0],\] \[\hat{v}_{d}=[0,0,0,0,1,-1].\]

For the remaining two eigenvectors, by the symmetric property, they have the formula

\[\hat{v}_{e}=[\alpha(\frac{\tau_{s}}{\tau_{1}},\frac{\tau_{c}}{\tau _{1}}),-\alpha(\frac{\tau_{s}}{\tau_{1}},\frac{\tau_{c}}{\tau_{1}}),\beta( \frac{\tau_{s}}{\tau_{1}},\frac{\tau_{c}}{\tau_{1}}),-\beta(\frac{\tau_{s}}{ \tau_{1}},\frac{\tau_{c}}{\tau_{1}}),0,0],\] \[\hat{v}_{f}=[\beta(\frac{\tau_{s}}{\tau_{1}},\frac{\tau_{c}}{ \tau_{1}}),-\beta(\frac{\tau_{s}}{\tau_{1}},\frac{\tau_{c}}{\tau_{1}}),-\alpha( \frac{\tau_{s}}{\tau_{1}},\frac{\tau_{c}}{\tau_{1}}),\alpha(\frac{\tau_{s}}{ \tau_{1}},\frac{\tau_{c}}{\tau_{1}}),0,0],\]where \(\alpha,\beta\) are some real functions. Then, by solving

\[\widehat{D^{-\frac{1}{2}}}\widehat{AD^{-\frac{1}{2}}}\hat{v}_{e} =\hat{\lambda}_{e}\hat{v}_{e}\] \[\widehat{D^{-\frac{1}{2}}}\widehat{AD^{-\frac{1}{2}}}\hat{v}_{f} =\hat{\lambda}_{f}\hat{v}_{f},\]

we get

\[\hat{\lambda}_{e} =\frac{1}{9}\left(\sqrt{(3-12\frac{\tau_{s}}{\tau_{1}}-16\frac{ \tau_{c}}{\tau_{1}})^{2}+108(\frac{\tau_{c}}{\tau_{1}})^{2}}-24\frac{\tau_{s}} {\tau_{1}}-20\frac{\tau_{c}}{\tau_{1}}+6\right)\] \[\hat{\lambda}_{f} =\frac{1}{9}\left(-\sqrt{(3-12\frac{\tau_{s}}{\tau_{1}}-16\frac{ \tau_{c}}{\tau_{1}})^{2}+108(\frac{\tau_{c}}{\tau_{1}})^{2}}-24\frac{\tau_{s}} {\tau_{1}}-20\frac{\tau_{c}}{\tau_{1}}+6\right).\]

Now, we show that \(\hat{\lambda}_{c}>\hat{\lambda}_{e}\). By \(\frac{\tau_{c}}{\tau_{1}}\ll 1\) and \(\frac{4}{9}\tau_{c}\leq\tau_{s}\leq\tau_{c}\)

\[\hat{\lambda}_{c}\geq\hat{\lambda}_{e} \Leftrightarrow\;3+24\frac{\tau_{s}}{\tau_{1}}-28\frac{\tau_{c}}{ \tau_{1}}\geq\sqrt{(3-12\frac{\tau_{s}}{\tau_{1}}-16\frac{\tau_{c}}{\tau_{1}}) ^{2}+108(\frac{\tau_{c}}{\tau_{1}})^{2}}\] \[\Leftrightarrow\;36(\frac{\tau_{s}}{\tau_{1}})^{2}+35(\frac{\tau_{ c}}{\tau_{1}})^{2}-144\frac{\tau_{s}}{\tau_{1}}\frac{\tau_{c}}{\tau_{1}}+18 \frac{\tau_{s}}{\tau_{1}}-6\frac{\tau_{c}}{\tau_{1}}\geq 0.\]

Thus, we have \(1=\hat{\lambda}_{a}=\hat{\lambda}_{b}>\hat{\lambda}_{c}>\hat{\lambda}_{e}>\hat{ \lambda}_{f}>\hat{\lambda}_{d}=0\). Moreover, we also have

\[\hat{\lambda}_{c}-\hat{\lambda}_{e}= 1-\frac{1}{3}\frac{\tau_{c}}{\tau_{1}}-\frac{1}{9}\left(\sqrt{( 3-12\frac{\tau_{s}}{\tau_{1}}-16\frac{\tau_{c}}{\tau_{1}})^{2}+108(\frac{ \tau_{c}}{\tau_{1}})^{2}}-24\frac{\tau_{s}}{\tau_{1}}-20\frac{\tau_{c}}{\tau_{ 1}}+6\right)\] \[\geq \Omega\left(\frac{\tau_{c}}{\tau_{1}}\right).\]

Let \(\hat{\lambda}_{1}=\hat{\lambda}_{a},\hat{\lambda}_{2}=\hat{\lambda}_{b},\hat{ \lambda}_{3}=\hat{\lambda}_{c}\). Then, by Weyl's Theorem, for \(i\in\{1,2,3\}\), we have

\[|\lambda_{i}-\hat{\lambda}_{i}|\leq\left\|D^{-\frac{1}{2}}AD^{-\frac{1}{2}} \widehat{AD^{-\frac{1}{2}}}\right\|_{2}\leq O((\frac{\tau_{c}}{\tau_{1}})^{2}).\]

By Davis-Kahan theorem, we have

\[\|\sin(U,\hat{U})\|_{F}\leq\frac{O((\frac{\tau_{c}}{\tau_{1}})^{2})}{\Omega \left(\frac{\tau_{c}}{\tau_{1}}\right)}\leq O(\frac{\tau_{c}}{\tau_{1}}).\]

We finish the proof. 

**Theorem B.2**.: _Recall \(\eta_{u}A^{(u)}\) is defined in Theorem B.1. Assume \(1\gg\frac{\tau_{c}}{\tau_{1}}>\frac{\tau_{c}}{\tau_{1}}>0\) and \(\tau_{1}+\tau_{c}+\tau_{s}=1\). Let \(\lambda_{1}^{(u)},\lambda_{2}^{(u)},\lambda_{3}^{(u)}\) and \(v_{1}^{(u)},v_{2}^{(u)},v_{3}^{(u)}\) be the largest three eigenvalues and their corresponding eigenvectors of \(D^{(u)-\frac{1}{2}}(\eta_{u}A^{(u)})D^{(u)-\frac{1}{2}}\), which is the normalized adjacency matrix of \(\eta_{u}A^{(u)}\). Let_

\[\hat{\lambda}_{1}^{(u)} =1,\;\;\hat{\lambda}_{2}^{(u)}=1,\;\;\hat{\lambda}_{3}^{(u)}=1-4 \frac{\tau_{s}}{\tau_{1}},\] \[\hat{v}_{1}^{(u)} =[0,0,0,0,1,1],\] \[\hat{v}_{2}^{(u)} =[1,1,1,1,0,0],\] \[\hat{v}_{3}^{(u)} =[1,-1,1,-1,0,0].\]

_Let \(U^{(u)}=[v_{1}^{(u)},v_{2}^{(u)},v_{3}^{(u)}],\hat{U}^{(u)}=[\hat{v}_{1}^{(u) },\hat{v}_{2}^{(u)},\hat{v}_{3}^{(u)}]\). Then, for \(i\in\{1,2,3\}\), we have \(|\lambda_{i}^{(u)}-\hat{\lambda}_{i}^{(u)}|\leq O((\frac{\tau_{c}}{\tau_{1}})^{2})\) and \(\|\sin(U^{(u)},\hat{U}^{(u)})\|_{F}\leq O(\frac{\tau_{c}^{2}}{\tau_{1}(\tau_{c} -\tau_{s})})\)._

Proof.: Similar to the proof of Theorem B.1, up to error \(O((\frac{\tau_{c}}{\tau_{1}})^{2})\), we have the following equation,

\[\widehat{\eta_{u}A^{(u)}}=\tau_{1}^{2}\begin{bmatrix}1&2\frac{\tau_{s}}{\tau_{1} }&2\frac{\tau_{c}}{\tau_{1}}&0&0&0\\ 2\frac{\tau_{c}}{\tau_{1}}&1&0&2\frac{\tau_{c}}{\tau_{1}}&0&0\\ 0&2\frac{\tau_{c}}{\tau_{1}}&0&1&2\frac{\tau_{c}}{\tau_{1}}&0&0\\ 0&2\frac{\tau_{c}}{\tau_{1}}&2\frac{\tau_{s}}{\tau_{1}}&1&0&0\\ 0&0&0&0&2&2\\ 0&0&0&0&2&2\end{bmatrix}.\]\[\widehat{D^{(u)-\frac{1}{2}}}\widehat{\eta_{u}A^{(u)}}\widehat{D^{(u)-\frac{1}{2} }}=\left[\begin{array}{ccccc}1-2\frac{\tau_{s}}{\tau_{1}}-2\frac{\tau_{c}}{ \tau_{1}}&2\frac{\tau_{s}}{\tau_{1}}&0&0&0\\ 2\frac{\tau_{s}}{\tau_{1}}&1-2\frac{\tau_{s}}{\tau_{1}}-2\frac{\tau_{c}}{ \tau_{1}}&0&2\frac{\tau_{c}}{\tau_{1}}&0&0\\ 2\frac{\tau_{c}}{\tau_{1}}&0&1-2\frac{\tau_{s}}{\tau_{1}}&2\frac{\tau_{s}}{ \tau_{1}}&0&0\\ 0&2\frac{\tau_{c}}{\tau_{1}}&2\frac{\tau_{s}}{\tau_{1}}&1-2\frac{\tau_{s}}{ \tau_{1}}&2\frac{\tau_{s}}{\tau_{1}}&0&0\\ 0&0&0&0&\frac{1}{2}&\frac{1}{2}\\ 0&0&0&0&\frac{1}{2}&\frac{1}{2}\\ \end{array}\right].\]

Let \(\hat{\lambda}_{1}^{(u)},\ldots,\hat{\lambda}_{6}^{(u)}\) be six eigenvalue of \(\widehat{D^{(u)-\frac{1}{2}}}\widehat{\eta_{u}A^{(u)}}\widehat{D^{(u)-\frac{1} {2}}}\), and \(\hat{v}_{1}^{(u)},\ldots,\hat{v}_{6}^{(u)}\) be corresponding eigenvectors. By direct calculation we have

\[\hat{\lambda}_{1}^{(u)}=1,\ \ \hat{\lambda}_{2}^{(u)}=1,\ \ \hat{\lambda}_{3}^{(u)}=1-4\frac{\tau_{s}}{\tau_{1}},\ \ \hat{\lambda}_{4}^{(u)}=1-4\frac{\tau_{c}}{\tau_{1}},\ \ \hat{\lambda}_{5}^{(u)}=1-4\frac{\tau_{s}}{\tau_{1}}-4\frac{\tau_{c}}{\tau_{1}},\ \hat{\lambda}_{6}^{(u)}=0\]

and corresponding eigenvector as

\[\hat{v}_{1}^{(u)} =[0,0,0,0,1,1],\] \[\hat{v}_{2}^{(u)} =[1,1,1,1,0,0],\] \[\hat{v}_{3}^{(u)} =[1,-1,1,-1,0,0],\] \[\hat{v}_{4}^{(u)} =[1,1,-1,-1,0,0],\] \[\hat{v}_{5}^{(u)} =[1,-1,-1,1,0,0],\] \[\hat{v}_{6}^{(u)} =[0,0,0,0,1,-1].\]

Then, by Weyl's Theorem, for \(i\in\{1,2,3\}\), we have

\[|\lambda_{i}^{(u)}-\hat{\lambda}_{i}^{(u)}|\leq\left\|D^{(u)-\frac{1}{2}}\eta _{u}A^{(u)}D^{(u)-\frac{1}{2}}-\widehat{D^{(u)-\frac{1}{2}}}\widehat{\eta_{u}A ^{(u)}}\widehat{D^{(u)-\frac{1}{2}}}\right\|_{2}\leq O((\frac{\tau_{c}}{\tau_ {1}})^{2}).\]

By Davis-Kahan theorem, we have

\[\|\sin(U^{(u)},\hat{U}^{(u)})\|_{F}\leq\frac{O((\frac{\tau_{c}}{\tau_{1}})^{2 })}{4(\frac{\tau_{c}}{\tau_{1}}-\frac{\tau_{c}}{\tau_{1}})}\leq O(\frac{\tau_ {c}^{2}}{\tau_{1}(\tau_{c}-\tau_{s})}).\]

We finish the proof. \(\Box\)Technical Details for Main Theory

### Notation

We let \(\mathbf{1}_{n}\), \(\mathbf{0}_{n}\) be the \(n\)-dimensional vector with all 1 or 0 values respectively. \(\mathbf{1}_{m\times n}\), \(\mathbf{0}_{m\times n}\) are similarly defined for \(m\)-by-\(n\) matrix. \(I_{n}\) is the identity matrix with shape \(n\times n\). For any matrix \(V\), \(V_{(i,j)}\) indictes the value at \(i\)-th row and \(j\)-th column of \(V\). If the matrix is subscripted like \(V_{k}\), we use a comma in-between like \(V_{k,(i,j)}\). Similarly, \(\mathbf{v}_{(i)}\) and \(\mathbf{v}_{k,(i)}\) are the \(i\)-th value for vector \(\mathbf{v}\) and \(\mathbf{v}_{k}\) respectively. \([n]\) is used to abbreviate the set \(\{1,2,...,n\}\).

### Matrix Form of K-means and the Derivative

Recall that we defined the K-means clustering measure of features in Sec. 4:

\[\mathcal{M}_{kms}(\Pi,Z)=\sum_{\pi\in\Pi}\sum_{i\in\pi}\left\|\mathbf{z}_{i}- \boldsymbol{\mu}_{\pi}\right\|^{2}/\sum_{\pi\in\Pi}\left|\pi\right|\left\| \boldsymbol{\mu}_{\pi}-\boldsymbol{\mu}_{\Pi}\right\|^{2},\] (12)

where the numerator measures the intra-class distance:

\[\mathcal{M}_{intra}(\Pi,Z)=\sum_{\pi\in\Pi}\sum_{i\in\pi}\left\|\mathbf{z}_{i }-\boldsymbol{\mu}_{\pi}\right\|^{2},\] (13)

and the denominator measures the inter-class distance:

\[\mathcal{M}_{inter}(\Pi,Z)=\sum_{\pi\in\Pi}\left|\pi\right|\left\|\boldsymbol {\mu}_{\pi}-\boldsymbol{\mu}_{\Pi}\right\|^{2}.\] (14)

We will show next how to convert the intra-class and the inter-class measure into a matrix form, which is desirable for analysis.

Intra-class measure.Note that the \(K\)-means intra-class measure can be rewritten in a matrix form:

\[\mathcal{M}_{intra}(\Pi,Z)=\|Z-H_{\Pi}Z\|_{F}^{2},\]

where \(H_{\Pi}\) is a matrix to convert \(Z\) to mean vectors w.r.t clusters defined by \(\Pi\). Without losing the generality, we assume \(Z\) is ordered according to the partition in \(\Pi\) -- first \(|\pi_{1}|\) vectors are in \(\pi_{1}\), next \(|\pi_{2}|\) vectors are in \(\pi_{2}\), etc. Then \(H_{\Pi}\) is given by:

\[H_{\Pi}=\left[\begin{array}{cccc}\frac{1}{|\pi_{1}|}\mathbf{1}_{|\pi_{1}| \times|\pi_{1}|}&\mathbf{0}&\cdots&\mathbf{0}\\ \mathbf{0}&\frac{1}{|\pi_{2}|}\mathbf{1}_{|\pi_{2}|\times|\pi_{2}|}&\cdots& \mathbf{0}\\...&...&...&...\\ \mathbf{0}&\mathbf{0}&...&\frac{1}{|\pi_{k}|}\mathbf{1}_{|\pi_{k}|\times| \pi_{k}|}\end{array}\right].\]

Going further, we have:

\[\mathcal{M}_{intra}(\Pi,Z) =\|Z-H_{\Pi}Z\|_{F}^{2}\] \[=\mathrm{Tr}((I-H_{\Pi})^{2}ZZ^{\top})\] \[=\mathrm{Tr}((I-2H_{\Pi}+H_{\Pi}^{2})ZZ^{\top})\] \[=\mathrm{Tr}((I-H_{\Pi})ZZ^{\top}).\]

Inter-class measure.The inter-class measure can be equivalently given by:

\[\mathcal{M}_{inter}(\Pi,Z)=\|H_{\Pi}Z-\frac{1}{N}\mathbf{1}_{N\times N}Z\|_{F} ^{2},\]

where \(H_{\Pi}\) is defined as above. And we can also derive:

\[\mathcal{M}_{inter}(\Pi,Z) =\|H_{\Pi}Z-\frac{1}{N}\mathbf{1}_{N\times N}Z\|_{F}^{2}\] \[=\mathrm{Tr}((H_{\Pi}-\frac{1}{N}\mathbf{1}_{N\times N})^{2}ZZ^{ \top})\] \[=\mathrm{Tr}((H_{\Pi}^{2}-\frac{2}{N}H_{\Pi}\mathbf{1}_{N\times N} +\frac{1}{N^{2}}\mathbf{1}_{N\times N}^{2})ZZ^{\top})\] \[=\mathrm{Tr}((H_{\Pi}-\frac{1}{N}\mathbf{1}_{N\times N})ZZ^{\top }).\]

### K-means Measure Has the Same Order as K-means Error

**Theorem C.1**.: _(Recap of Theorem 4.1) We define the \(\xi_{\pi\to\pi^{\prime}}\) as the index of samples that is from class division \(\pi\) however is closer to \(\bm{\mu}_{\pi^{\prime}}\) than \(\bm{\mu}_{\pi^{\prime}}\). In other word, \(\xi_{\pi\to\pi^{\prime}}=\{i:i\in\pi,\|\mathbf{z}_{i}-\bm{\mu}_{\pi}\|_{2}\geq \|\mathbf{z}_{i}-\bm{\mu}_{\pi^{\prime}}\|_{2}\}\). Assuming \(|\xi_{\pi\to\pi^{\prime}}|>0\), we define below the clustering error ratio from \(\pi\) to \(\pi^{\prime}\) as \(\mathcal{E}_{\pi\to\pi^{\prime}}\) and the overall cluster error ratio \(\mathcal{E}_{\Pi,Z}\) as the **Harmonic Mean** of \(\mathcal{E}_{\pi\to\pi^{\prime}}\) among all class pairs:_

\[\mathcal{E}_{\Pi,Z}=C(C-1)/\left(\sum_{\begin{subarray}{c}\pi\neq\pi^{\prime} \\ \pi,\pi^{\prime}\in\Pi\end{subarray}}\frac{1}{\mathcal{E}_{\pi\to\pi^{\prime}} }\right),\text{where }\mathcal{E}_{\pi\to\pi^{\prime}}=\frac{|\xi_{\pi\to\pi^{\prime}}|}{|\pi^{ \prime}|+|\pi|}.\]

_The K-means measure \(\mathcal{M}_{kms}(\Pi,Z)\) has the same order as the Harmonic Mean of the cluster error ratio between all cluster pairs:_

\[\mathcal{E}_{\Pi,Z}=O(\mathcal{M}_{kms}(\Pi,Z)).\]

Proof.: We have the following inequality for \(i\in\xi_{\pi\to\pi^{\prime}}\):

\[4\|\mathbf{z}_{i}-\bm{\mu}_{\pi}\|_{2}^{2}\geq 2\|\mathbf{z}_{i}-\bm{\mu}_{ \pi}\|_{2}^{2}+2\|\mathbf{z}_{i}-\bm{\mu}_{\pi^{\prime}}\|_{2}^{2}\geq\|\bm{ \mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\|_{2}^{2}.\]

Then we have:

\[\mathcal{M}_{intra}(\Pi,Z) =\sum_{\pi\in\Pi}\sum_{i\in\pi}\|\mathbf{z}_{i}-\bm{\mu}_{\pi}\|_ {2}^{2}\] \[\geq\sum_{i\in\pi}\|\mathbf{z}_{i}-\bm{\mu}_{\pi}\|_{2}^{2}\] \[\geq\sum_{i\in\xi_{\pi\to\pi^{\prime}}}\|\mathbf{z}_{i}-\bm{\mu}_ {\pi}\|_{2}^{2}\] \[\geq\frac{1}{4}\sum_{i\in\xi_{\pi\to\pi^{\prime}}}\|\bm{\mu}_{\pi} -\bm{\mu}_{\pi^{\prime}}\|_{2}^{2}\] \[=\frac{1}{4}|\xi_{\pi\to\pi^{\prime}}|\bm{\|}_{\bm{\mu}_{\pi}}- \bm{\mu}_{\pi^{\prime}}\|_{2}^{2}.\]

Note that the inter-class measure can be decomposed into the summation of cluster center distances:

\[\mathcal{M}_{inter}(\Pi,Z) =\sum_{\pi\in\Pi}|\pi|\left\|\bm{\mu}_{\pi}-\bm{\mu}_{\Pi}\right\| _{2}^{2}\] \[=\sum_{\pi\in\Pi}\frac{|\pi|}{N^{2}}\left\|(\sum_{\pi^{\prime}\in \Pi}|\pi^{\prime}|)\bm{\mu}_{\pi}-\sum_{\pi^{\prime}\in\Pi}|\pi^{\prime}|\bm{ \mu}_{\pi^{\prime}}\right\|_{2}^{2}\] \[\leq\frac{C}{N^{2}}\sum_{\pi\in\Pi}|\pi|\sum_{\pi^{\prime}\in \Pi}|\pi^{\prime}|^{2}\left\|\bm{\mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\right\|_{2} ^{2}\] \[=\frac{C}{N^{2}}\sum_{\pi\neq\pi^{\prime}}|\pi||\pi^{\prime}|(| \pi^{\prime}|+|\pi|)\left\|\bm{\mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\right\|_{2} ^{2},\]

where \(\sum_{\pi\neq\pi^{\prime}}\) is enumerating over any two different class partitions in \(\Pi\). Combining together, we have:

\[C(C-1)/\left(\sum_{\pi\neq\pi^{\prime}}\frac{(|\pi^{\prime}|+|\pi |)}{|\xi_{\pi\to\pi^{\prime}}|}\right) =C(C-1)/\left(\sum_{\pi\neq\pi^{\prime}}\frac{(|\pi^{\prime}|+| \pi|)\|\bm{\mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\|_{2}^{2}}{|\xi_{\pi\to\pi^{ \prime}}|||\bm{\mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\|_{2}^{2}}\right)\] \[\leq C(C-1)/\left(\sum_{\pi\neq\pi^{\prime}}\frac{|\pi^{\prime}| \pi|(|\pi^{\prime}|+|\pi|)\|\bm{\mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\|_{2}^{2}}{N ^{2}|\xi_{\pi\to\pi^{\prime}}|||\bm{\mu}_{\pi}-\bm{\mu}_{\pi^{\prime}}\|_{2}^{2 }}\right)\] \[\leq C(C-1)/\left(\frac{\mathcal{M}_{inter}(\Pi,Z)}{4\mathcal{M} _{intra}(\Pi,Z)}\right)\] \[=O(\mathcal{M}_{kms}(\Pi,Z)).\]

### Proof of Theorem 4.2

We start by providing more details to supplement Sec. 4.2.1.

**Matrix perturbation by adding labels.** Recall that we define in Eq. 3 that the adjacency matrix is the unlabeled one \(A^{(u)}\) plus the perturbation of the label information \(A^{(l)}\):

\[A=\eta_{u}A^{(u)}+\eta_{l}A^{(l)}.\]

We study the perturbation from two aspects: (1) The direction of the perturbation which is given by \(A^{(l)}\), (2) The perturbation magnitude \(\eta_{l}\). We first consider the perturbation direction \(A^{(l)}\) and recall that we defined the concrete form in Eq. 2:

\[A^{(l)}_{xx^{\prime}}=w^{(l)}_{xx^{\prime}}\triangleq\sum_{i\in\mathcal{Y}_{l }}\mathbb{E}_{\bar{x}_{l}\sim\mathcal{P}_{l_{1}}}\mathbb{E}_{\bar{x}^{\prime}_ {l}\sim\mathcal{P}_{l_{i}}}\mathcal{T}(x|\bar{x}_{l})\mathcal{T}\left(x^{ \prime}|\bar{x}^{\prime}_{l}\right).\]

For simplicity, we consider \(|\mathcal{Y}_{l}|=1\) in this theoretical analysis. Then we observe that \(A^{(l)}_{xx^{\prime}}\) is a rank-1 matrix can be written as

\[A^{(l)}_{xx^{\prime}}=\mathfrak{l}^{\top},\]

where \(\mathfrak{l}\in\mathbb{R}^{N\times 1}\) with \((\mathfrak{l})_{x}=\mathbb{E}_{\bar{x}_{l}\sim\mathcal{P}_{l_{1}}}\mathcal{T} (x|\bar{x}_{l})\). And we define \(D_{l}\triangleq diag(\mathfrak{l})\).

**The perturbation function of representation.** We then consider a more generalized form for the adjacency matrix:

\[A(\delta)\triangleq\eta_{u}A^{(u)}+\delta\mathfrak{l}^{\top}.\]

where we treat the adjacency matrix as a function of the "labeling perturbation" degree \(\delta\). It is clear that \(A(0)=\eta_{u}A^{(u)}\) which is the scaled adjacency matrix for the unlabeled case and that \(A(\eta_{l})=A\). When we let the adjacency matrix be a function of \(\delta\), the normalized form and the derived feature representation should also be the function of \(\delta\). We proceed by defining these terms.

Without losing the generality, we let \(diag(\mathbf{1}_{N}^{\top}A(0))=I_{N}\) which means the node in the unlabeled graph has equal degree. We then have:

\[D(\delta)\triangleq diag(\mathbf{1}_{N}^{\top}A(\delta))=I_{N}+\delta D_{l}.\]

The normalized adjacency matrix is given by:

\[\tilde{A}(\delta)\triangleq D(\delta)^{-\frac{1}{2}}A(\delta)D(\delta)^{- \frac{1}{2}}.\]

For feature representation \(Z(\delta)\), it is derived from the top-\(k\) SVD components of \(\tilde{A}(\delta)\). Specifically, we have:

\[Z(\delta)Z(\delta)^{\top}=D(\delta)^{-\frac{1}{2}}\tilde{A}_{k}(\delta)D( \delta)^{-\frac{1}{2}}=D(\delta)^{-\frac{1}{2}}\sum_{j=1}^{k}\lambda_{j}( \delta)\Phi_{j}(\delta)D(\delta)^{-\frac{1}{2}},\]

where we define \(\tilde{A}_{k}(\delta)\) as the top-\(k\) SVD components of \(\tilde{A}(\delta)\) and can be further written as \(\tilde{A}_{k}(\delta)=\sum_{j=1}^{k}\lambda_{j}(\delta)\Phi_{j}(\delta)\). Here the \(\lambda_{j}(\delta)\) is the \(j\)-th singular value and \(\Phi_{j}(\delta)\) is the \(j\)-th singular projector (\(\Phi_{j}(\delta)=v_{j}(\delta)v_{j}(\delta)^{\top}\)) defined by the \(j\)-th singular vector \(v_{j}(\delta)\). **For brevity, when \(\delta=0\), we remove the suffix \((0)\)** since it is equivalent to the unperturbed version of notations. For example, we let

\[\tilde{A}(0)=\tilde{A}^{(u)},Z(0)=Z^{(u)},\lambda_{i}(0)=\lambda_{i}^{(u)},v_{ i}(0)=v_{i}^{(u)},\Phi_{i}(0)=\Phi_{i}^{(u)}.\]

**Theorem C.2**.: _(Recap of Th. 4.2) Denote \(V_{\partial}^{(u)}\in\mathbb{R}^{N\times(N-k)}\) as the null space of \(V_{k}^{(u)}\) and \(\tilde{A}_{k}^{(u)}=V_{k}^{(u)}\Sigma_{k}^{(u)}V_{k}^{(u)\top}\) as the rank-\(k\) approximation for \(\tilde{A}^{(u)}\). Given \(\delta,\eta_{1}>0\) and let \(\mathcal{G}_{k}\) as the spectral gap between \(k\)-th and \(k+1\)-th singular values of \(\tilde{A}^{(u)}\), we have:_

\[\Delta_{kms}(\delta)=\delta\eta_{1}\operatorname{Tr}\Big{(}\Upsilon\left(V_{k }^{(u)}V_{k}^{(u)\top}\mathfrak{l}^{\top}(I+V_{\partial}^{(u)}V_{\partial}^{(u) \top})-2\tilde{A}_{k}^{(u)}diag(\mathfrak{l})\right)\Big{)}+O(\frac{1}{ \mathcal{G}_{k}}+\delta^{2}),\]

_where \(diag(\cdot)\) converts the vector to the corresponding diagonal matrix and \(\Upsilon\in\mathbb{R}^{N\times N}\) is a matrix encoding the **ground-truth clustering structure** in the way that \(\Upsilon_{xx^{\prime}}>0\) if \(x\) and \(x^{\prime}\) has the same label and \(\Upsilon_{xx^{\prime}}<0\) otherwise._Proof.: As we shown in Sec C.2, we can now also write the K-means measure as the function of perturbation:

\[\mathcal{M}_{kms}(\delta)=\frac{\operatorname{Tr}((I-H_{\Pi})Z(\delta)Z(\delta)^ {\top})}{\operatorname{Tr}((H_{\Pi}-\frac{1}{N}\mathbf{1}_{N\times N})Z( \delta)Z(\delta)^{\top})}.\]

The proof is directly given by the following Lemma C.3. 

**Lemma C.3**.: _Let \(\eta_{1},\eta_{2}\) be two real values and \(\Upsilon=(1+\eta_{2})H_{\Pi}-I-\frac{\eta_{2}}{N}\mathbf{1}_{N}\mathbf{1}_{N}^ {\top}\). Let the spectrum gap \(\mathcal{G}_{k}=\frac{\lambda_{k}^{(u)}}{\lambda_{k+1}^{(u)}}\), we have the derivative of the K-means measure evaluated at \(\delta=0\):_

\[\left[\mathcal{M}_{kms}(\delta)\right]^{\prime}\Big{|}_{\delta=0}=-\eta_{1} \operatorname{Tr}\Big{(}\Upsilon\left(V_{k}^{(u)}V_{k}^{(u)\top}\mathfrak{l}^ {\top}-2\tilde{A}_{k}^{(u)}D_{l}+V_{k}^{(u)}V_{k}^{(u)\top}\mathfrak{l}^{\top }V_{\varnothing}^{(u)}V_{\varnothing}^{(u)\top}\right)\Big{)}+O(\frac{1}{ \mathcal{G}_{k}}).\]

The proof for Lemma C.3 is lengthy. We postpone it to Sec. C.6.

### Proof of Theorem 4.3

We start by showing the justification of the assumptions made in Theorem 4.3.

**Assumption C.4**.: We assume the spectral gap \(\mathcal{G}_{k}\) is large. Such an assumption is commonly used in theory works using spectral analysis [32, 57].

**Assumption C.5**.: We assume \(\mathfrak{l}\) lies in the linear span of \(V_{k}^{(u)}\). _i.e._, \(V_{k}^{(u)}V_{k}^{(u)\top}\mathfrak{l}=\mathfrak{l},V_{\varnothing}^{(u)\top} \mathfrak{l}=0\). The goal of this assumption is to simplify \((V_{k}^{(u)}V_{k}^{(u)\top}\mathfrak{l}^{\top}+V_{k}^{(u)}V_{k}^{(u)\top} \mathfrak{l}^{\top}\mathfrak{l}^{\top}V_{\varnothing}^{(u)}V_{\varnothing}^{( u)\top})\) to \(\mathfrak{l}^{\top}\).

**Assumption C.6**.: For any \(\pi_{c}\in\Pi\), \(\forall i,j\in\pi_{c},\mathfrak{l}_{(i)}=\mathfrak{l}_{\{j\}}=:\mathfrak{l}_ {\pi_{e}}\). Recall that the \(\mathfrak{l}_{(i)}\) means the connection between the \(i\)-th sample to the labeled data. Here we can view \(\mathfrak{l}_{\pi_{c}}\) as the _connection between class \(c\) to the labeled data_.

**Theorem C.7**.: _(Recap of Theorem 4.3.) With Assumption C.4, C.5 and C.6. Given \(\delta,\eta_{1},\eta_{2}>0\), we have:_

\[\Delta_{kms}(\delta)\geq\delta\eta_{1}\eta_{2}\sum_{\pi_{c}\in\Pi}|\pi_{c}| \mathfrak{l}_{\pi_{c}}\Delta_{\pi_{c}}(\delta),\]

_where_

\[\Delta_{\pi_{c}}(\delta)=(\mathfrak{l}_{\pi_{c}}-\frac{1}{N})-2(1-\frac{|\pi_ {c}|}{N})(\mathbb{E}_{i\in\pi_{c}}\mathbb{E}_{j\in\pi_{c}}\mathbf{z}_{i}^{ \top}\mathbf{z}_{j}-\mathbb{E}_{i\in\pi_{c}}\mathbb{E}_{j\notin\pi_{c}} \mathbf{z}_{i}^{\top}\mathbf{z}_{j}).\]

Proof.: The proof is directly given by Lemma C.8 and plugging the definition of \(\Delta_{kms}(\delta)\). 

**Lemma C.8**.: _With Assumption C.4 C.5 and C.6, we have the derivative of K-means measure with the upper bound:_

\[\left[\mathcal{M}_{kms}(\delta)\right]^{\prime}\Big{|}_{\delta=0}\leq-\eta_{1} \eta_{2}\sum_{\pi\in\Pi}|\mathfrak{l}_{\pi}\left((\mathfrak{l}_{\pi}-\frac{1}{ N})-2(\boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\pi}-\boldsymbol{\mu}_{\pi}^{ \top}\boldsymbol{\mu}_{\Pi})\right).\]

Proof.: By Assumption C.4 C.5 and C.6 and Theorem 4.2, we have

\[\frac{1}{\eta_{1}}[\mathcal{M}_{kms}(\delta)]^{\prime}\Big{|}_{ \delta=0} =-\operatorname{Tr}\Big{(}\Upsilon\left(V_{k}^{(u)}V_{k}^{(u) \top}\mathfrak{l}^{\top}-2\tilde{A}_{k}^{(u)}D_{l}\right)\Big{)}\] \[=-\operatorname{Tr}\Big{(}\Upsilon\left(\mathfrak{l}^{\top}-2 \tilde{A}_{k}^{(u)}D_{l}\right)\Big{)}\] \[=-\operatorname{Tr}\Big{(}\Big{(}(1+\eta_{2})H_{\Pi}-I-\frac{\eta _{2}}{N}\mathbf{1}_{N}\mathbf{1}_{N}^{\top}\Big{)}\left(\mathfrak{l}^{\top}-2 \tilde{A}_{k}^{(u)}D_{l}\right)\Big{)}\] \[=(1+\eta_{2})\mathcal{M}_{H}^{\prime}+\mathcal{M}_{I}^{\prime}+ \eta_{2}\mathcal{M}_{\mathbf{1}}^{\prime},\]where

\[\mathcal{M}^{\prime}_{H} =-\operatorname{Tr}\left(H_{\Pi}\left(\mathfrak{l}^{\toptop}-2\tilde {A}^{(u)}_{k}D_{l}\right)\right)\] \[=-\sum_{\pi\in\Pi}\left(|\pi|(\mathbb{E}_{i\in\pi}\mathfrak{l}_{(i )})^{2}-\frac{2}{|\pi|}\sum_{i\in\pi}\sum_{j\in\pi}\mathfrak{l}_{(i)}\tilde{A}^ {(u)}_{k,(i,j)}\right)\] \[=-\sum_{\pi\in\Pi}\left(|\pi|_{\pi}^{2}-2|\pi|\mathfrak{l}_{\pi} \mathbb{E}_{(i,j)\in\pi\times\pi}\mathbf{z}_{i}^{\top}\mathbf{z}_{j}\right)\] \[=-\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}(\mathfrak{l}_{\pi}-2 \boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\pi}),\]

\[\mathcal{M}^{\prime}_{I} =\operatorname{Tr}\left(\left(\mathfrak{l}^{\toptop}-2\tilde{A} ^{(u)}_{k}D_{l}\right)\right)\] \[=\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}(\mathfrak{l}_{\pi}-2 \mathbb{E}_{i\in\pi}\mathbf{z}_{i}^{\top}\mathbf{z}_{i}),\]

and

\[\mathcal{M}^{\prime}_{\mathbf{1}} =\operatorname{Tr}\left(\frac{1}{N}\mathbf{1}_{N}\mathbf{1}_{N}^ {\top}\left(\mathfrak{l}^{\toptop}-2\tilde{A}^{(u)}_{k}D_{l}\right)\right)\] \[=\frac{1}{N}-2\sum_{\pi\in\Pi}\sum_{i\in\pi}\mathfrak{l}_{(i)} \mathbb{E}_{j\in[N]}\mathbf{z}_{i}^{\top}\mathbf{z}_{j}\] \[=\frac{1}{N}-2\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}\boldsymbol{ \mu}_{\pi}^{\top}\boldsymbol{\mu}_{\Pi}.\]

We observe that

\[\mathcal{M}^{\prime}_{I}+\mathcal{M}^{\prime}_{H} =-\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}(\mathfrak{l}_{\pi}-2 \boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\pi})+\sum_{\pi\in\Pi}|\pi| \mathfrak{l}_{\pi}(\mathfrak{l}_{\pi}-2\mathbb{E}_{i\in\pi}\mathbf{z}_{i}^{ \top}\mathbf{z}_{i})\] \[=2\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}(\|\mathbb{E}_{i\in\pi} \mathbf{z}_{i}\|_{2}^{2}-\mathbb{E}_{i\in\pi}\|\mathbf{z}_{i}\|_{2}^{2})\] \[\leq 0,\]

where the last inequality is by Jensen's Inequality. We then have

\[\frac{1}{\eta_{1}\eta_{2}}[\mathcal{M}_{kms}(\delta)]^{\prime} \Big{|}_{\delta=0} \leq\mathcal{M}^{\prime}_{H}+\mathcal{M}^{\prime}_{\mathbf{1}}\] \[=-\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}(\mathfrak{l}_{\pi}-2 \boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\pi})+\frac{1}{N}-2\sum_{\pi \in\Pi}|\pi|\mathfrak{l}_{\pi}\boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\Pi}\] \[=\frac{1}{N}-\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}(\mathfrak{l} _{\pi}-2(\boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\pi}-\boldsymbol{\mu} _{\pi}^{\top}\boldsymbol{\mu}_{\Pi}))\] \[=-\sum_{\pi\in\Pi}|\pi|\mathfrak{l}_{\pi}((\mathfrak{l}_{\pi}- \frac{1}{N})-2(\boldsymbol{\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\pi}-\boldsymbol {\mu}_{\pi}^{\top}\boldsymbol{\mu}_{\Pi})).\]

### Proof of Lemma c.3

**Notation Recap:** We define \(\tilde{A}_{k}(\delta)\) as the top-\(k\) SVD components of \(\tilde{A}(\delta)\) and can be further written as \(\tilde{A}_{k}(\delta)=\sum_{j=1}^{k}\lambda_{j}(\delta)\Phi_{j}(\delta)\). Here the \(\lambda_{j}(\delta)\) is the \(j\)-th singular value and \(\Phi_{j}(\delta)\) is the \(j\)-th singular projector (\(\Phi_{j}(\delta)=v_{j}(\delta)v_{j}(\delta)^{\top}\)) defined by the \(j\)-th singular vector \(v_{j}(\delta)\). **For brevity, when \(\delta=0\), we remove the suffix \((0)\)** since it is equivalent to the unperturbed version of notations. For example, we let

\[\tilde{A}(0)=\tilde{A}^{(u)},Z(0)=Z^{(u)},\lambda_{i}(0)=\lambda_{i}^{(u)},v_{i }(0)=v_{i}^{(u)},\Phi_{i}(0)=\Phi_{i}^{(u)}.\]Proof.: By the derivative rule, we have,

\[\mathcal{M}^{\prime}_{kms}(\delta) =\frac{1}{\mathcal{M}_{inter}(\Pi,Z)}\mathcal{M}^{\prime}_{intra}( \delta)-\frac{\mathcal{M}_{intra}(\Pi,Z)}{\mathcal{M}_{inter}(\Pi,Z)^{2}} \mathcal{M}^{\prime}_{inter}(\delta)\] \[=\eta_{1}\mathcal{M}^{\prime}_{intra}(\delta)-\eta_{1}\eta_{2} \mathcal{M}^{\prime}_{inter}(\delta)\] \[=\eta_{1}\left(\operatorname{Tr}((I_{\Pi}-H_{\Pi})[Z(\delta)Z( \delta)^{\top}]^{\prime})-\eta_{2}\operatorname{Tr}((H_{\Pi}-\frac{1}{N}\mathbf{ 1}_{N\times N})[Z(\delta)Z(\delta)^{\top}]^{\prime})\right)\] \[=\eta_{1}\left(\operatorname{Tr}((I_{\Pi}+\frac{\eta_{2}}{N} \mathbf{1}_{N\times N}-(\eta_{2}+1)H_{\Pi})[Z(\delta)Z(\delta)^{\top}]^{\prime })\right)\] \[=-\eta_{1}\left(\operatorname{Tr}(\Upsilon[Z(\delta)Z(\delta)^{ \top}]^{\prime})\right)\] \[=-\eta_{1}\sum_{j=1}^{k}\operatorname{Tr}(\Upsilon[D(\delta)^{- \frac{1}{2}}\lambda_{j}(\delta)\Phi_{j}(\delta)D(\delta)^{-\frac{1}{2}}]^{ \prime}),\]

where we let \(\eta_{1}=\frac{1}{\mathcal{M}_{inter}(\Pi,Z)}\), \(\eta_{2}=\frac{\mathcal{M}_{intra}(\Pi,Z)}{\mathcal{M}_{inter}(\Pi,Z)}\) and \(\Upsilon=(1+\eta_{2})H_{\Pi}-I_{\Pi}-\frac{\eta_{2}}{N}\mathbf{1}_{N}\mathbf{ 1}_{N}^{\top}\). We proceed by showing the calculation of \([D(\delta)^{-\frac{1}{2}}]^{\prime}\), \([\lambda_{j}(\delta)]^{\prime}\) and \([\Phi_{j}(\delta)]^{\prime}\).

Since \(D(\delta)=I+\delta D_{l}\), then \([D(\delta)^{-\frac{1}{2}}]^{\prime}\Big{|}_{\delta=0}=-\frac{1}{2}D_{l}\). To calculate \([\lambda_{j}(\delta)]^{\prime}\) and \([\Phi_{j}(\delta)]^{\prime}\), we first need:

\[[\tilde{A}(\delta)]^{\prime}\Big{|}_{\delta=0} =[D(\delta)^{-\frac{1}{2}}A(\delta)D(\delta)^{-\frac{1}{2}}]^{\prime}\] \[=[D(\delta)^{-\frac{1}{2}}]^{\prime}\tilde{A}^{(u)}+[A(\delta)]^{ \prime}+\tilde{A}^{(u)}[D(\delta)^{-\frac{1}{2}}]^{\prime}\] \[=-\frac{1}{2}D_{l}\tilde{A}^{(u)}+\mathfrak{l}^{\top}-\frac{1}{2} \tilde{A}^{(u)}D_{l}.\]

Then, according to Equation (3) in [20], we have:

\[[\lambda_{j}(\delta)]^{\prime}\Big{|}_{\delta=0} =\operatorname{Tr}(\Phi_{j}^{(u)}[\tilde{A}(\delta)]^{\prime})\] \[=\operatorname{Tr}(\Phi_{j}^{(u)}(-\frac{1}{2}D_{l}\tilde{A}^{(u) }+\mathfrak{l}^{\top}-\frac{1}{2}\tilde{A}^{(u)}D_{l}))\] \[=\operatorname{Tr}((-\frac{\lambda_{j}^{(u)}}{2}D_{l}\Phi_{j}^{(u )}+\Phi_{j}^{(u)}\mathfrak{l}^{\top}-\frac{\lambda_{j}^{(u)}}{2}\Phi_{j}^{(u )}D_{l}))\] \[=\operatorname{Tr}(\Phi_{j}^{(u)}(\mathfrak{l}^{\top}-\lambda_{j} ^{(u)}D_{l})).\]

According to Equation (10) in [20], we have:

\[[\Phi_{j}(\delta)]^{\prime}\Big{|}_{\delta=0} =(\lambda_{j}^{(u)}I_{N}-\tilde{A}^{(u)})^{\dagger}[\tilde{A}( \delta)]^{\prime}\Phi_{j}^{(u)}+\Phi_{j}^{(u)}[\tilde{A}(\delta)]^{\prime}( \lambda_{j}^{(u)}I_{N}-\tilde{A}^{(u)})^{\dagger}\] \[=\sum_{i\neq j}^{N}\frac{1}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}( \Phi_{i}^{(u)}[\tilde{A}(\delta)]^{\prime}\Phi_{j}^{(u)}+\Phi_{j}^{(u)}[\tilde {A}(\delta)]^{\prime}\Phi_{i}^{(u)})\] \[=\sum_{i\neq j}^{N}\frac{1}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}( \Phi_{i}^{(u)}(-\frac{1}{2}D_{l}\tilde{A}^{(u)}+\mathfrak{l}^{\top}-\frac{1}{2} \tilde{A}^{(u)}D_{l})\Phi_{j}^{(u)}+\Phi_{j}^{(u)}(...)\Phi_{i}^{(u)})\] \[=\sum_{i\neq j}^{N}\frac{1}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}( \Phi_{i}^{(u)}(\mathfrak{l}^{\top}-\frac{\lambda_{j}^{(u)}+\lambda_{i}^{(u)}}{2 }D_{l})\Phi_{j}^{(u)}+\Phi_{j}^{(u)}(\mathfrak{l}^{\top}-\frac{\lambda_{j}^{(u) }+\lambda_{i}^{(u)}}{2}D_{l})\Phi_{i}^{(u)}).\]

Now we calculate the derivative of the \(K\)-means loss:\[\frac{1}{\eta_{1}}[\mathcal{M}_{kms}(\delta)]^{\prime}\Big{|}_{ \delta=0} =-\sum_{j=1}^{k}[\operatorname{Tr}(\Upsilon D(\delta)^{-\frac{1}{2}} \lambda_{j}(\delta)\Phi_{j}(\delta)D(\delta)^{-\frac{1}{2}})]^{\prime}\Big{|}_{ \delta=0}\] \[=-\sum_{j=1}^{k}\operatorname{Tr}\left(\Upsilon\left([D(\delta)^{ -\frac{1}{2}}]^{\prime}\lambda_{j}^{(u)}\Phi_{j}^{(u)}+\lambda_{j}^{(u)}\Phi_{ j}^{(u)}[D(\delta)^{-\frac{1}{2}}]^{\prime}+[\lambda_{j}(\delta)]^{\prime}\Phi_{ j}^{(u)}+\lambda_{j}^{(u)}[\Phi_{j}(\delta)]^{\prime}\right)\right)\] \[=\sum_{j=1}^{k}\operatorname{Tr}\left(\Upsilon\left(\frac{\lambda _{j}^{(u)}}{2}D_{i}\Phi_{j}^{(u)}+\frac{\lambda_{j}^{(u)}}{2}\Phi_{j}^{(u)}D_{ l}-[\lambda_{j}(\delta)]^{\prime}\Phi_{j}^{(u)}-\lambda_{j}^{(u)}[\Phi_{j}( \delta)]^{\prime}\right)\right)\] \[=\mathcal{M}_{a}^{\prime}+\mathcal{M}_{b}^{\prime}+\mathcal{M}_{c }^{\prime},\]

where

\[\mathcal{M}_{a}^{\prime}=\sum_{j=1}^{k}\frac{\lambda_{j}^{(u)}}{2} \operatorname{Tr}\left(\Upsilon\left(D_{l}\Phi_{j}^{(u)}+\Phi_{j}^{(u)}D_{l} \right)\right),\]

\[\mathcal{M}_{b}^{\prime} =-\sum_{j=1}^{k}\operatorname{Tr}\left(\Upsilon[\lambda_{j}( \delta)]^{\prime}\Phi_{j}^{(u)}\right)=-\sum_{j=1}^{k}\operatorname{Tr}\left(( \mathfrak{U}^{\top}-\lambda_{j}^{(u)}D_{l})\Phi_{j}^{(u)}\right) \operatorname{Tr}\left(\Upsilon\Phi_{j}^{(u)}\right)\] \[=-\sum_{j=1}^{k}\operatorname{Tr}\left((\mathfrak{U}^{\top}- \lambda_{j}^{(u)}D_{l})\Phi_{j}^{(u)}\Upsilon\Phi_{j}^{(u)}\right),\]

\[\mathcal{M}_{c}^{\prime} =-\sum_{j=1}^{k}\operatorname{Tr}\left(\Upsilon\lambda_{j}^{(u)}[ \Phi_{j}(\delta)]^{\prime}\right)\] \[=-\sum_{j=1}^{k}\operatorname{Tr}\left(\sum_{i\neq j}^{N}\frac{ \lambda_{j}^{(u)}}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}\left(\Upsilon\Phi_{i }^{(u)}(\mathfrak{U}^{\top}-\frac{\lambda_{j}^{(u)}+\lambda_{i}^{(u)}}{2}D_{ l})\Phi_{j}^{(u)}+\Upsilon\Phi_{j}^{(u)}(\mathfrak{U}^{\top}-\frac{\lambda_{j}^{(u)}+ \lambda_{i}^{(u)}}{2}D_{l})\Phi_{i}^{(u)}\right)\right)\] \[=-\sum_{j=1}^{k}\operatorname{Tr}\left(\sum_{i\neq j}^{N}\frac{ \lambda_{j}^{(u)}}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}\left((\Phi_{j}^{(u)} \Upsilon\Phi_{i}^{(u)}+\Phi_{i}^{(u)}\Upsilon\Phi_{j}^{(u)})(\mathfrak{U}^{ \top}-\frac{\lambda_{j}^{(u)}+\lambda_{i}^{(u)}}{2}D_{l})\right)\right)\] \[=-\sum_{j=1}^{k}\operatorname{Tr}\left(\sum_{i<j}\left(\frac{ \lambda_{j}^{(u)}}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}+\frac{\lambda_{i}^{(u )}}{\lambda_{i}^{(u)}-\lambda_{j}^{(u)}}\right)\left((\Phi_{j}^{(u)}\Upsilon \Phi_{i}^{(u)}+\Phi_{i}^{(u)}\Upsilon\Phi_{j}^{(u)})(\mathfrak{U}^{\top}-\frac {\lambda_{j}^{(u)}+\lambda_{i}^{(u)}}{2}D_{l})\right)\right)\] \[\quad-\sum_{j=1}^{k}\operatorname{Tr}\left(\sum_{i=k+1}^{N}\frac{ \lambda_{j}^{(u)}}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}\left((\Phi_{j}^{(u)} \Upsilon\Phi_{i}^{(u)}+\Phi_{i}^{(u)}\Upsilon\Phi_{j}^{(u)})(\mathfrak{U}^{ \top}-\frac{\lambda_{j}^{(u)}+\lambda_{i}^{(u)}}{2}D_{l})\right)\right)\]

[MISSING_PAGE_EMPTY:30]

We can represent \(\frac{\lambda_{j}^{(u)}}{\lambda_{j}^{(u)}-\lambda_{i}^{(u)}}=1+\sum_{p=1}^{\infty}( \frac{\lambda_{j}^{(u)}}{\lambda_{j}^{(u)}})^{p}\). Denote the residual term as :

\[\mathcal{M}_{e}^{\prime}=-\sum_{j=1}^{k}\sum_{i=k+1}^{\infty}\sum_{p=1}^{\infty }2(\frac{\lambda_{j}^{(u)}}{\lambda_{j}^{(u)}})^{p}{v_{i}^{(u)}}^{\top}\Upsilon v _{j}^{(u)}\cdot{v_{i}^{(u)}}^{\top}(\mathfrak{U}^{\top}-\lambda_{j}^{(u)}D_{l} )v_{j}^{(u)}=O(\frac{1}{\mathcal{G}_{k}}).\]

We then have:

\[\frac{1}{\eta_{1}}[\mathcal{M}_{kms-all}(\delta)]^{\prime}\Big{|} _{\delta=0}\] \[=-\operatorname{Tr}({V_{k}^{(u)}}^{\top}\Upsilon{V_{k}^{(u)}} \cdot{V_{k}^{(u)}}^{\top}\mathfrak{U}^{\top}{V_{k}^{(u)}})+2\operatorname{Tr }({V_{k}^{(u)}}^{\top}\Upsilon{V_{k}^{(u)}}\cdot\Sigma_{k}^{(u)}{V_{k}^{(u)}}^ {\top}D_{l}{V_{k}^{(u)}})\] \[\quad-2\operatorname{Tr}({V_{\varnothing}^{(u)}}^{\top}\Upsilon{V _{k}^{(u)}}^{\top}\mathfrak{U}^{\top}{V_{k}^{(u)}}^{\top}\mathfrak{U}^{\top}{ V_{\varnothing}^{(u)}})+2\operatorname{Tr}({V_{\varnothing}^{(u)}}^{\top}\Upsilon{V _{k}^{(u)}}\cdot\Sigma_{k}^{(u)}{V_{k}^{(u)}}^{\top}D_{l}{V_{\varnothing}^{(u)} })+\mathcal{M}_{e}^{\prime}\] \[=-\operatorname{Tr}(\Upsilon{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top} \mathfrak{U}^{\top}{V_{k}^{(u)}}^{\top})+2\operatorname{Tr}(\Upsilon\tilde{A }_{k}^{(u)}D_{l}{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top})\] \[\quad-2\operatorname{Tr}(\Upsilon{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top }\mathfrak{U}^{\top}({I}-{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top}))+2\operatorname{ Tr}(\Upsilon\tilde{A}_{k}^{(u)}D_{l}{I}({I}_{N}-{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top}))+ \mathcal{M}_{e}^{\prime}\] \[=-2\operatorname{Tr}(\Upsilon{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top} \mathfrak{U}^{\top})+2\operatorname{Tr}(\Upsilon\tilde{A}_{k}^{(u)}D_{l})+ \operatorname{Tr}(\Upsilon{V_{k}^{(u)}}{V_{k}^{(u)}}^{\top}\mathfrak{U}^{\top }{V_{k}^{(u)}}^{\top})+\mathcal{M}_{e}^{\prime}\] \[=-2\operatorname{Tr}\left(\Upsilon\left({V_{k}^{(u)}}{V_{k}^{(u)}} ^{\top}\mathfrak{U}^{\top}-\tilde{A}_{k}^{(u)}D_{l}-\frac{1}{2}{V_{k}^{(u)}}{V _{k}^{(u)}}^{\top}\mathfrak{U}^{\top}{V_{\varnothing}^{(u)}}^{\top}\right) \right)+\mathcal{M}_{e}^{\prime}\] \[=-\operatorname{Tr}\left(\Upsilon\left({V_{k}^{(u)}}{V_{k}^{(u)}} ^{\top}\mathfrak{U}^{\top}-2\tilde{A}_{k}^{(u)}D_{l}+{V_{k}^{(u)}}{V_{k}^{(u)}} ^{\top}\mathfrak{U}^{\top}{V_{\varnothing}}{V_{\varnothing}^{\top}}\right) \right)+O(\frac{1}{\mathcal{G}_{k}}).\]Analysis on Other Contrastive Losses

In this section, we discuss the extension of our graphic-theoretic analysis to one of the most common contrastive loss functions - SimCLR [11]. SimCLR loss is an extended version of InfoNCE loss [68] that achieves great empirical success and inspires a proliferation of follow-up works [5; 8; 12; 26; 34; 69; 77]. Specifically, SupCon [34] extends SimCLR to the supervised setting. GCD [69] and OpenCon [63] further leverage the SupCon and SimCLR losses, and are tailored to the open-world representation learning setting considering both labeled and unlabeled data.

At a high level, we consider a general form of the SimCLR and its extensions (including SupCon, GCD, OpenCon) as:

\[\mathcal{L}_{\text{gnl}}(f;\mathcal{P}_{+})=-\frac{1}{\tau}\underset{(x,x^{+}) \sim\mathcal{P}_{+}}{\mathbb{E}}\left[f(x)^{\top}f(x^{+})\right]\quad+ \underset{x\sim\mathcal{P}}{\mathbb{E}}\left[\log\left(\underset{\begin{subarray} {c}x^{\prime}\sim\mathcal{P}\\ x\neq x^{\prime}\end{subarray}}{e^{f(x^{\prime})^{\top}f(x)/\tau}}\right) \right],\] (15)

where we let the \(\mathcal{P}_{+}\) as the distribution of **positive pairs** defined in Section 3.1. In SimCLR [11], the positive pairs are purely sampled in the _unlabeled case (u)_ while SupCon [34] considers the _labeled case (l)_. With both labeled and unlabeled data, GCD [69] and OpenCon [63] sample positive pairs in both cases.

In this section, we investigate an alternative form that eases the theoretical analysis (also applied in [72]):

\[\widehat{\mathcal{L}}_{\text{gnl}}\left(f;\mathcal{P}_{+}\right)= -\frac{1}{\tau}\underset{(x,x^{+})\sim\mathcal{P}_{+}}{\mathbb{E} }\left[f(x)^{\top}f(x^{+})\right]\quad+\log\left(\underset{\begin{subarray}{ c}x,x^{\prime}\sim\mathcal{P}\\ x\neq x^{\prime}\end{subarray}}{e^{f(x^{\prime})^{\top}f(x)/\tau}}\right)\] (16) \[\geq \mathcal{L}_{\text{gnl}}\left(f;\mathcal{P}_{+}\right),\] (17)

which serves an upper bound of \(\mathcal{L}_{\text{gnl}}\left(f\right)\) according to Jensen's Inequality.

**A graph-theoretic view.** Recall in Section 3.1, we define the graph \(G(\mathcal{X},w)\) with vertex set \(\mathcal{X}\) and edge weights \(w\). Each entry of adjacency matrix \(A\) is given by \(w_{xx^{\prime}}\), which denotes the marginal probability of generating the pair for any two augmented data \(x,x^{\prime}\in\mathcal{X}\):

\[w_{xx^{\prime}}=\eta_{u}w_{xx^{\prime}}^{(u)}+\eta_{l}w_{xx^{\prime}}^{(l)},\]

and \(w_{x}\) measures the degree of node \(x\):

\[w_{x}=\sum_{x^{\prime}}w_{xx^{\prime}}.\]

One can view the difference between SimCLR and its variants in the following way: (1) SimCLR [11] corresponds to \(\eta_{l}=0\) when there is no labeled case; (2) SupCon [34] corresponds to \(\eta_{u}=0\) when only labeled case is considered. (3) GCD [69] and OpenCon [63] correspond to the cases when \(\eta_{u},\eta_{l}\) are both non-zero due to the availability of both labeled and unlabeled data.

With the define marginal probability of sampling positive pairs \(w_{xx^{\prime}}\) and the marginal probability of sampling a single sample \(w_{x}\), we have:

\[\widehat{\mathcal{L}}_{\text{gnl}}\left(Z;G(\mathcal{X},w)\right) =-\frac{1}{\tau}\sum_{x,x^{\prime}\in\mathcal{X}}w_{xx^{\prime}}f( x)^{\top}f\left(x^{\prime}\right)+\log\left(\sum_{\begin{subarray}{c}x,x^{ \prime}\in\mathcal{X}\\ x\neq x^{\prime}\end{subarray}}{w_{x}w_{x^{\prime}}e^{f(x^{\prime})^{\top}f(x) /\tau}}\right)\] \[=-\frac{1}{\tau}\operatorname{Tr}(Z^{\top}AZ)+\log\operatorname{ Tr}\left((D\mathbf{1}_{N}\mathbf{1}_{N}^{\top}D-D^{2})\exp(\frac{1}{\tau}ZZ^{T}) \right).\]

When \(\tau\) is large:\[\widehat{\mathcal{L}}_{\text{simclr}}\left(Z;G(\mathcal{X},w)\right) \approx-\frac{1}{\tau}\operatorname{Tr}(Z^{\top}AZ)+\log\operatorname {Tr}\left((D\mathbf{1}_{N}\mathbf{1}_{N}^{\top}D-D^{2})(\mathbf{1}_{N}\mathbf{ 1}_{N}^{\top}+\frac{1}{\tau}ZZ^{T})\right)\] \[=-\frac{1}{\tau}\operatorname{Tr}(Z^{\top}AZ)+\log(1+\frac{ \frac{1}{\tau}\operatorname{Tr}(Z^{\top}(D\mathbf{1}_{N}\mathbf{1}_{N}^{\top} D-D^{2})Z)}{\operatorname{Tr}(D)^{2}-\operatorname{Tr}(D^{2})})+\text{const}\] \[\approx-\frac{1}{\tau}\operatorname{Tr}(Z^{\top}AZ)+\frac{\frac{ 1}{\tau}\operatorname{Tr}(Z^{\top}(D\mathbf{1}_{N}\mathbf{1}_{N}^{\top}D-D^{2 })Z)}{\operatorname{Tr}(D)^{2}-\operatorname{Tr}(D^{2})}+\text{const}\] \[=-\frac{1}{\tau}\operatorname{Tr}\left(Z^{\top}(A-\frac{D \mathbf{1}_{N}\mathbf{1}_{N}^{\top}D-D^{2}}{\operatorname{Tr}(D)^{2}- \operatorname{Tr}(D^{2})})Z\right)+\text{const}.\]

If we further consider the constraint that the \(Z^{\top}Z=I\), minimizing \(\widehat{\mathcal{L}}_{\text{simclr}}\left(Z;G(\mathcal{X},w)\right)\) boils down to the eigenvalue problem such that \(Z\) is formed by the top-\(k\) eigenvectors of matrix \((A-\frac{D\mathbf{1}_{N}\mathbf{1}_{N}^{\top}D-D^{2}}{\operatorname{Tr}(D)^{2} -\operatorname{Tr}(D^{2})})\). Recall that our main analysis for Theorem 4.2 and Theorem 4.3 is based on the insight that the feature space is formed by the top-\(k\) eigenvectors of the normalized adjacency matrix \(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\). Viewed in this light, the same analysis could be applied to the SimCLR loss as well, which only differs in the concrete matrix form. We do not include the details in this paper but leave it as a future work.

Additional Experiments Details

### Experimental Details of Toy Example

**Recap of set up**. In Section 4.1 we consider a toy example that helps illustrate the core idea of our theoretical findings. Specifically, the example aims to cluster 3D objects of different colors and shapes, generated by a 3D rendering software [31] with user-defined properties including colors, shape, size, position, etc. Suppose the training samples come from three shapes, \(\mathcal{X}_{\bigodot}\), \(\mathcal{X}_{\bigodot}\), \(\mathcal{X}_{\bigodot}\). Let \(\mathcal{X}_{\bigodot}\) be the sample space with **known** class, and \(\mathcal{X}_{\bigodot}\), \(\mathcal{X}_{\bigodot}\) be the sample space with **novel** classes. Further, the two novel classes are constructed to have different relationships with the known class. Specifically, the toy dataset contains elements with 5 unique types:

\[\mathcal{X}=\mathcal{X}_{\bigodot}\ \cup\mathcal{X}_{\bigodot}\ \cup\mathcal{X}_{\bigodot},\]

where

\[\mathcal{X}_{\bigodot}=\{x_{\bigodot},x_{\bigodot}\ \},\]

\[\mathcal{X}_{\bigodot}=\{x_{\bigodot}\ \}.\]

**Experimental details for Figure 3(b)**. We rendered 2500 samples for each type of data. In total, we have 12500 samples. For known class \(\mathcal{X}_{\bigodot}\), we randomly select \(50\%\) as labeled data and treat the rest as unlabeled. For training, we use the same data augmentation strategy as in SimSiam [12]. We use ResNet18 and train the model for 40 epochs (sufficient for convergence) with a fixed learning rate of 0.005, using SORL defined in Eq. (6). We set \(\eta_{l}=0.04\) and \(\eta_{u}=1\), respectively. Our visualization is by PyTorch implementation of UMAP [43], with parameters (n_neighbors=30, min_dist=1.5, spread=2, metric=euclidean).

### Experimental Details for Benchmarks

**Hardware and software.** We run all experiments with Python 3.7 and PyTorch 1.7.1, using NVIDIA GeForce RTX 2080Ti and A6000 GPUs.

Training settings.For a fair comparison, we use ResNet-18 [25] as the backbone for all methods. Similar to [7], we pre-train the backbone using the unsupervised Spectral Contrastive Learning [23] for 1200 epochs. The configuration for the pre-training stage is consistent with [23]. Note that the pre-training stage does not incorporate any label information. At the training stage, we follow the same practice in [7, 63], and train our model \(f(\cdot)\) by only updating the parameters of the last block of ResNet. In addition, we add a trainable two-layer MLP projection head that projects the feature from the penultimate layer to an embedding space \(\mathbb{R}^{k}\) (\(k=1000\)). We use the same data augmentation strategies as SimSiam [12, 23]. For CIFAR-10, we set \(\eta_{l}=0.25,\eta_{u}=1\) with training epoch 100, and we evaluate using features extracted from the layer preceding the projection. For CIFAR-100, we set \(\eta_{l}=0.0225,\eta_{u}=3\) with 400 training epochs and assess based on the projection layer's features. We use SGD with momentum 0.9 as an optimizer with cosine annealing (lr=0.05), weight decay 5e-4, and batch size 512.

Evaluation settings.At the inference stage, we evaluate the performance in a transductive manner (evaluate on \(\mathcal{D}_{u}\)). We run a semi-supervised K-means algorithm as proposed in [69]. We follow the evaluation strategy in [7] and report the following metrics: (1) classification accuracy on known classes, (2) clustering accuracy on the novel data, and (3) overall accuracy on all classes. The accuracy of the novel classes is measured by solving an optimal assignment problem using the Hungarian algorithm [36]. When reporting accuracy on all classes, we solve optimal assignments using both known and novel classes.