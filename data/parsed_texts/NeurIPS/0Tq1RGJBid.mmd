# A Fast and Accurate Estimator for Large Scale Linear Model via Data Averaging

 Rui Wang

Center for Applied Statistics

and School of Statistics

Renmin University of China

Beijing 100872, China 446100240@qq.com

&Yanyan Ouyang

Center for Applied Statistics

and School of Statistics

Renmin University of China

Beijing 100872, China staoyyy@ruc.edu.cn

&Panpan Yu

NavInfo

Beijing 100094, China

yupanpan@navinfo.com

&Wangli Xu

Center for Applied Statistics

and School of Statistics

Renmin University of China

Beijing 100872, China wlxu@ruc.edu.cn

###### Abstract

This work is concerned with the estimation problem of linear model when the sample size is extremely large and the data dimension can vary with the sample size. In this setting, the least square estimator based on the full data is not feasible with limited computational resources. Many existing methods for this problem are based on the sketching technique which uses the sketched data to perform least square estimation. We derive fine-grained lower bounds of the conditional mean squared error for sketching methods. For sampling methods, our lower bound provides an attainable optimal convergence rate. Our result implies that when the dimension is large, there is hardly a sampling method can have a faster convergence rate than the uniform sampling method. To achieve a better statistical performance, we propose a new sketching method based on data averaging. The proposed method reduces the original data to a few averaged observations. These averaged observations still satisfy the linear model and are used to estimate the regression coefficients. The asymptotic behavior of the proposed estimation procedure is studied. Our theoretical results show that the proposed method can achieve a faster convergence rate than the optimal convergence rate for sampling methods. Theoretical and numerical results show that the proposed estimator has good statistical performance as well as low computational cost.

## 1 Introduction

Linear regression model is one of the simplest and the most fundamental models in statistics and machine learning. Suppose one collects independent and identically distributed (i.i.d.) observations \(\{Z_{i},y_{i}\}_{i=1}^{N}\), where \(Z_{i}\in\mathbb{R}^{p}\) is the vector of the predictors and \(y_{i}\in\mathbb{R}\) is the response. The linear model assumes

\[y_{i}=\beta_{0}+Z_{i}^{\top}\bm{\beta}_{1}+\varepsilon_{i},\quad i=1,\ldots,N,\] (1)

where \(\beta_{0}\in\mathbb{R}\), \(\bm{\beta}_{1}\in\mathbb{R}^{p}\) are the unknown coefficients and \(\varepsilon_{1},\ldots,\varepsilon_{N}\) are random variables representing noise. Let \(X_{i}=(1,Z_{i}^{\top})^{\top}\), \(\bm{\beta}=(\beta_{0},\bm{\beta}_{1}^{\top})^{\top}\), \(\mathbf{y}=(y_{1},\ldots,y_{N})^{\top}\) and \(\mathbf{X}=(X_{1},\ldots,X_{N})^{\top}\). The classical least square estimator equals \(\arg\min_{\bm{\beta}\in\mathbb{R}^{p+1}}\|\mathbf{y}-\mathbf{X}\bm{\beta}\|^ {2}\). If \(\mathbf{X}\) has full column rank, the least square estimator equals \((\sum_{i=1}^{N}X_{i}X_{i}^{\top})^{-1}\sum_{i=1}^{N}X_{i}y_{i}\) and the direct computation costs \(O(Np^{2})\)time. For large scale linear models with \(N\gg p\), the computing time \(O(Np^{2})\) of the exact least square estimator is not negligible. Faster estimators of \(\bm{\beta}\) can largely facilitate the practical data analysis pipelines.

Numerous research efforts have been devoted to the estimation problem for large scale linear model. Many existing work in this area can be understood as matrix sketching methods which explicitly or implicitly use matrix sketches as surrogates for the original observations to reduce the data size. Specifically, sketching methods solve the sketched least square problem

\[\min_{\bm{\beta}\in\mathbb{R}^{p+1}}\|\mathbf{O}^{\top}\mathbf{y}-\mathbf{O}^{ \top}\mathbf{X}\bm{\beta}\|^{2},\] (2)

where \(\mathbf{O}\in\mathbb{R}^{N\times n}\) is a sketching matrix with \(n\ll N\). The solution to the problem (2) is the least square estimator based on the reduced data \(\mathbf{O}^{\top}\mathbf{X}\in\mathbb{R}^{n\times(p+1)}\) and \(\mathbf{O}^{\top}\mathbf{y}\in\mathbb{R}^{n}\). Since \(n\ll N\), the sketched least square problem can be solved much faster than the least square estimator for the full data. In this paper, we only consider the case that \(\mathbf{O}\) is independent of \(\varepsilon_{1},\ldots,\varepsilon_{N}\). That is, \(\mathbf{O}\) may rely on \(\mathbf{X}\), but not rely on \(\mathbf{y}\). This guarantees that the solution to (2) is an unbiased estimator of \(\bm{\beta}\). Note that sampling methods are special cases of the sketching framework (2). In fact, for a sampling method, each column of \(\mathbf{O}\) is a vector whose elements are all \(0\) except one that equals \(1\). Sketching methods have been intensively researched in algorithmic aspect; see Mahoney (2010); Woodruff (2014); Drineas and Mahoney (2016) for reviews. Recently, the statistical aspect of sketching methods also draws much attention; see, e.g., Ma et al. (2015); Raskutti and Mahoney (2016); Wang et al. (2017); Dobriban and Liu (2019); Ma et al. (2020); Ahfock et al. (2021).

Probably the simplest sketching method is the uniform sampling method which randomly selects \(n\) observations with equal probability to form the reduced data. Recently, Pilanci and Wainwright (2016) provides a minimax lower bound for the mean squared prediction error of random sketching methods. Theorem 1 of Pilanci and Wainwright (2016) shows that for a large class of random sketching methods, including many existing data-oblivious sketching methods and sampling methods, the convergence rate of the mean squared prediction error can not be faster than the uniform sampling method. Hence it is a nontrivial task to construct a sketching method which has significantly better statistical performance than the uniform sampling method.

Recently, Wang et al. (2019) initiates the study of sampling methods based on extreme values. Motivated by the D-optimal criterion, Wang et al. (2019) proposed the information-based optimal subdata selection (IBOSS) algorithm which successively selects informative observations based on extreme values of variables. They showed that for fixed \(p\), the estimator produced by the IBOSS algorithm can have a faster convergence rate than the uniform sampling method. Meanwhile, the computation of the IBOSS algorithm can be completed within \(O(Np+np^{2})\) time which has the same order as the uniform sampling method if \(n=cN/p\) for some constant \(c>0\). Now the algorithm of Wang et al. (2019) has become the building block of some recent methods for large scale problems. For example, Wang (2019) proposed an algorithm which combines the algorithm of Wang et al. (2019) and the divide and conquer strategy. Cheng et al. (2020) extended the algorithm of Wang et al. (2019) to the logistic regression model. Existing asymptotic results for the IBOSS algorithm are obtained in the setting of fixed \(n\) and \(p\). At present, there is still a lack of theoretical understanding of the behavior of the IBOSS algorithm in the setting of varying \(n\) and \(p\).

The IBOSS algorithm is a sampling method and is therefore an instance of the sketching framework (2). Interestingly, the IBOSS algorithm can surpass the minimax lower bound of Pilanci and Wainwright (2016). In fact, a key condition for Theorem 1 of Pilanci and Wainwright (2016) does not hold for the IBOSS algorithm. Thus, the IBOSS algorithm is not restricted by the minimax bound of Pilanci and Wainwright (2016). This fact is detailed in Section 2. Note that there are many potential sketching methods which are not restricted by the minimax bound of Pilanci and Wainwright (2016). To give a more comprehensive understanding of the behavior of these sketching methods, we derive fine-grained lower bounds for the conditional mean squared error of the sketched least square estimators produced by (2) in the setting that \(Z_{i}\) is a standard normal random vector. In particular, our result provides a lower bound for any sampling method which may possibly rely on \(\mathbf{X}\) but does not rely on \(\mathbf{y}\). It turns out that if \(p\ll\log(N/n)\), then the optimal lower bound for sampling methods can have a faster convergence rate than the uniform sampling method. On the other hand, if \(\log(N/n)\ll p\), any sampling method can not largely surpass the uniform sampling method. Furthermore, we derive the asymptotic behavior of the IBOSS algorithm in the setting of varying \(n\) and \(p\). It turns out that under certain conditions, the IBOSS algorithm can achieve the optimal rate for sampling methods.

For large scale linear models, it is often the case that \(\log(N/n)\ll p\). In this case, any sampling method can not have a significantly better statistical performance than the uniform sampling method. Inspired by this phenomenon, we propose an alternative sketching method which can reduce the full data to just a few observations while the resulting estimator of \(\bm{\beta}\) may have smaller conditional mean squared error than sampling methods. The proposed method is based on data averaging. The main idea is to partition the observations into \(2p\) groups such that the averages of \(Z_{i}\) within groups are separated. The least square estimator based on \(2p\) averaged observations is used to estimate \(\bm{\beta}\). The computation of the proposed method can be completed within \(O(Np+p^{3})\) time. Our theoretical results show that the proposed method can have a faster convergence rate than any sampling methods with comparative computing time. Also, the proposed method reduces the full data to merely \(2p\) averaged observations. These averaged observations also satisfy the linear model (1) and have independent errors. Consequently, it is convenient to further compute other estimators or conduct statistical inferences using the reduced data. The good performance of the proposed estimator is also verified by simulation results and a real data example. Table 1 summarizes the theoretical performance of the proposed method and compare it with the ideal sampling method implied by Theorem 2 and the IBOSS algorithm.

The rest of the paper is organized as follows. Section 2 investigates lower bounds for the conditional mean squared error of the sketched least square estimators produced by (2). In Section 3, we propose a data averaging method to estimate \(\bm{\beta}\) and investigate its asymptotic behavior. Section 4 presents the simulation results briefly. Section 5 concludes the paper. The simulation results, a real data analysis and all proofs are deferred to the Supplement Material.

We close this section by introducing some notations and assumptions that will be used throughout the paper. For any real number \(w\), let \(\lfloor w\rfloor\) denote the largest integer not larger than \(w\). For any vector \(W\), let \(\|W\|\) denote the Euclidean norm of \(W\). For any matrix \(\mathbf{B}\), let \(\|\mathbf{B}\|\) and \(\|\mathbf{B}\|_{F}\) denote the operator norm and the Frobenious norm of \(\mathbf{B}\), respectively. Moreover, denote by \(\mathbf{B}_{:,j}\) the \(j\)th column of \(\mathbf{B}\). If \(\mathbf{B}\) is symmetric, denote by \(\lambda_{i}(\mathbf{B})\) the \(i\)th largest eigenvalue of \(\mathbf{B}\). In this paper, the symmetric matrices are equipped with Loewner partial order. That is, for two symmetric matrices \(\mathbf{B}_{1}\) and \(\mathbf{B}_{2}\), \(\mathbf{B}_{1}>\mathbf{B}_{2}\) if and only if \(\mathbf{B}_{1}-\mathbf{B}_{2}\) is positive definite. For a positive semidefinite matrix \(\mathbf{B}\), let \(\mathbf{B}^{1/2}\) denote a positive semidefinite matrix such that \((\mathbf{B}^{1/2})^{2}=\mathbf{B}\). For any set \(\mathscr{A}\), denote by \(\mathscr{A}^{\complement}\) its complement and \(\mathrm{Card}(\mathscr{A})\) its cardinality. Let \(\Phi(x)\) and \(\varphi(x)\) denote the distribution function and density function of the standard normal distribution, respectively. For random variables \(\xi\in\mathbb{R}\) and \(\eta>0\), \(\xi=o_{P}(\eta)\) means that \(\xi/\eta\) converges to \(0\) in probability, and \(\xi=O_{P}(\eta)\) means that \(\xi/\eta\) is bounded in probability.

Let \(N\) denote the size of full sample, \(p\) denote the dimension of covariates. Let \(\mathbf{Z}=(Z_{1},\ldots,Z_{N})^{\top}\) be an \(N\times p\) matrix of covariates. Denote by \(z_{i,j}\) the \(j\)th element of \(Z_{i}\), \(i=1,\ldots,N\), \(j=1,\ldots,p\). Let \(z_{(1),j}\leq\cdots\leq z_{(N),j}\) denote the order statistics of \(\{z_{i,j}\}_{i=1}^{N}\), \(j=1,\ldots,p\). The following assumption for the data distribution is assumed throughout the paper.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Methods & Reduced sample size & \(\mathrm{E}\left\{\|\hat{\bm{\beta}}_{\bm{\hat{1}}}-\bm{\beta}\|^{2}\mid \mathbf{Z}\right\}\) & Computing time \\ \hline ISM & \(n\) & \(O_{P}\left(\frac{p^{2}}{n(p+\log(\frac{N}{n}))}\right)\) & â€” \\ IBOSS & \(n\) & \(O_{P}\left(\frac{p^{2}}{n(p+\log(\frac{N}{n}))}\right)\) & \(O(Np+np^{2})\) \\ NEW & \(2p\) & \((1+o_{P}(1))\frac{p^{2}\sigma_{\varepsilon}^{2}}{2\log(2p)N}\) & \(O(Np+p^{3})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Theoretical performance of the ideal sampling method (abbreviated as ISM), the IBOSS algorithm and the proposed method when \(Z_{1}\sim\mathcal{N}(\mathbf{0}_{p},\mathbf{I}_{p})\). Assume that as \(N\to\infty\), \(p\to\infty\), \(p^{3}(\log(p))^{4}\log(N)/N\to\infty\), \(n=O(N^{\epsilon})\) for some \(0<\epsilon<1/2\) and \(p=O(n^{1/2-\epsilon^{\epsilon}})\) for some \(0<\epsilon^{\epsilon}<1/2\), \(\log(N/n)=O(p^{2})\). See Theorems 2, 3, 4. The reported computing time is under the assumption that the multiplication of an \(m\times n\) matrix and an \(n\times p\) matrix costs \(O(mnp)\) time, and the inversion of a \(p\times p\) matrix costs \(O(p^{3})\) time.

**Assumption 1**: _Suppose \(\{Z_{i},y_{i}\}_{i=1}^{N}\) are i.i.d. and satisfy the linear model (1), where \(\mathrm{E}(\varepsilon_{1})=0\), \(\mathrm{Var}(\varepsilon_{1})=\sigma_{\varepsilon}^{2}\) and \(\varepsilon_{1}\) is independent of \(Z_{1}\). Suppose \(Z_{1}\) has a density function \(f(z_{1,1},\ldots,z_{1,p})\) with respect to the Lebesgue measure on \(\mathbb{R}^{p}\), and \(\mathrm{E}(Z_{1})=\mu\), \(\mathrm{Cov}(Z_{1})=\mathbf{\Sigma}\) are finite. Suppose \(\mathbf{\Sigma}=(\sigma_{i,j})_{i,j=1}^{p}\) is positive definite. Suppose \(r>0\). As \(N\to\infty\), the dimension \(p\) is a function of \(N\), while the distribution of \((Z_{1},y_{1})\) only relies on \(p\). Finally, assume \(\sigma_{\varepsilon}^{2}\) is a constant which does not depend on \(N\)._

For simplicity, our notations suppress the dependence of \(p\) on \(N\), and the dependence of the distribution of \((Z_{1},y_{1})\) on \(p\).

## 2 Risk bounds for sketched least square estimators

Theorem 1 of Pilanci and Wainwright (2016) provides a minimax lower bound for the mean squared prediction error of random sketching methods. Their result implies that under certain conditions, there exists a constant \(C>0\) such that for any estimator \(\hat{\boldsymbol{\beta}}\) which only relies on \((\mathbf{O}^{\top}\mathbf{X},\mathbf{O}^{\top}\mathbf{y})\),

\[\sup_{\boldsymbol{\beta}\in\mathbb{R}^{p}}\mathrm{E}\{N^{-1}\|\mathbf{X}(\hat {\boldsymbol{\beta}}-\boldsymbol{\beta})\|^{2}\mid\mathbf{Z}\}\geq\frac{Cp}{n }\sigma_{\varepsilon}^{2}.\]

The optimal convergence rate \(p/n\) can be achieved by the least square estimator based on \(n\) uniformly selected observations.A key condition for the above result is that

\[\|\,\mathrm{E}(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top} \mid\mathbf{Z})\|\leq cn/N\] (3)

for some constant \(c>0\).

The result of Pilanci and Wainwright (2016) can be applied to general estimators based on \((\mathbf{O}^{\top}\mathbf{X},\mathbf{O}^{\top}\mathbf{y})\). In this paper, however, we focus on the least square estimator based on \((\mathbf{O}^{\top}\mathbf{X},\mathbf{O}^{\top}\mathbf{y})\). Let \(\hat{\boldsymbol{\beta}}_{\mathbf{O}}\) denote the solution to the sketched least square problem (2). In this paper, we use the conditional mean squared error \(\mathrm{E}\{\|\hat{\boldsymbol{\beta}}_{\mathbf{O}}-\boldsymbol{\beta}\|^{2} \mid\mathbf{Z}\}\) to measure the performance of \(\hat{\boldsymbol{\beta}}_{\mathbf{O}}\). The following theorem gives a lower bound for the conditional mean squared error of \(\hat{\boldsymbol{\beta}}_{\mathbf{O}}\).

**Theorem 1**: _Suppose Assumption 1 holds, \(Z\sim\mathcal{N}(\mathbf{0}_{p},\mathbf{I}_{p})\), the sketching matrix \(\mathbf{O}\) is an \(N\times n\) matrix with full column rank. Assume that \(\mathbf{O}\) is independent of \(\varepsilon_{1},\ldots,\varepsilon_{N}\) and with probability \(1\), \(\mathbf{O}^{\top}\mathbf{X}\) has full column rank. Suppose as \(N\to\infty\), \(p/N\to 0\). Then as \(N\to\infty\), \(\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{O}}-\boldsymbol{\beta}\|^ {2}\mid\mathbf{Z}\right\}\geq\)(1+o_{P}(1)) \(\|\mathrm{E}(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top} \mid\mathbf{Z})\|^{-1}\,\frac{p+1}{N}\sigma_{\varepsilon}^{2}.\)_

Theorem 1 gives an explicit characterization of the impact of \(\mathrm{E}(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top} \mid\mathbf{Z})\) on the lower bound of \(\mathrm{E}\{\|\hat{\boldsymbol{\beta}}_{\mathbf{O}}-\boldsymbol{\beta}\|^{2} \mid\mathbf{Z}\}\). Pilanci and Wainwright (2016) showed that the condition (3) is satisfied by many classical sketching methods. Under the conditions of Theorem 1, for sketching methods satisfying the condition (3), the convergence rate of \(\mathrm{E}\{\|\hat{\boldsymbol{\beta}}_{\mathbf{O}}-\boldsymbol{\beta}\|^{2} \mid\mathbf{Z}\}\) is lower bounded by \(p/n\), which is the convergence rate for the uniform sampling method. Thus, in order to achieve a faster convergence rate than the uniform sampling method, the condition (3) should be violated.

Many existing sketching methods are through sampling the observations. For sampling methods, \(\mathbf{O}\) is a column orthogonal matrix and each column of \(\mathbf{O}\) has a single nonzero element with value \(1\). Hence \(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top}\) is a diagonal matrix whose diagonal elements are zeros and ones. For the IBOSS algorithm of Wang et al. (2019), the selected observations are completely determined by \(\mathbf{X}\) and does not rely on additional randomness. Consequently \(\|\,\mathrm{E}(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top} \mid\mathbf{Z})\|=\|\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{ \top}\|=1\). In this case, the lower bound provided by Theorem 1 has rate \(p/N\) which is too loose. The following theorem gives a tighter lower bound of the mean squared error for sampling methods.

**Theorem 2**: _Suppose Assumption 1 holds, \(Z\sim\mathcal{N}(\mathbf{0}_{p},\mathbf{I}_{p})\), the sketching matrix \(\mathbf{O}\) is an \(N\times n\) matrix with full column rank. Assume that \(\mathbf{O}\) is independent of \(\varepsilon_{1},\ldots,\varepsilon_{N}\) and with probability \(1\), \(\mathbf{O}^{\top}\mathbf{X}\) has full column rank. Furthermore, suppose \(\mathrm{E}(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top} \mid\mathbf{Z})=\mathrm{diag}(d_{1},\ldots,d_{N})\). Let \(d_{\max}=\max_{i\in\{1,\ldots,N\}}d_{i}\) Then_

\[\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{O}}-\boldsymbol{\beta}\|^{2} \mid\mathbf{Z}\right\}\geq \frac{p^{2}}{6n\left(p+\log\left(\frac{Nd_{\max}}{n}\right) \right)+O_{P}(n)}\sigma_{\varepsilon}^{2}.\]If the matrix \(\mathrm{E}(\mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top}\mid \mathbf{Z})=\mathrm{diag}(d_{1},\dots,d_{N})\) is diagonal, then

\[d_{\max}=\max_{\alpha\in\mathbb{R}^{N},\|\alpha\|=1}\alpha^{\top}\,\mathrm{E}( \mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top}\mid\mathbf{Z}) \alpha\leq\mathrm{E}(\max_{\alpha\in\mathbb{R}^{N},\|\alpha\|=1}\alpha^{\top} \mathbf{O}(\mathbf{O}^{\top}\mathbf{O})^{-1}\mathbf{O}^{\top}\alpha\mid\mathbf{ Z})=1.\]

Under the conditions of Theorem 2, the optimal convergence rate for sampling methods is lower bounded by

\[\frac{p^{2}}{n\left(p+\log\left(\frac{N}{n}\right)\right)}.\] (4)

Note that if \(p\ll\log(N/n)\), then the rate (4) is faster than the uniform sampling method. Theorem 4 in Section 3.2 will show that under certain conditions, the method of Wang et al. (2019) can achieve the optimal rate (4).

It is worth mentioning that Theorems 1 and 2 are obtained under the condition \(Z_{i}\sim\mathcal{N}(\mathbf{0}_{p},\mathbf{I}_{p})\). Perhaps, these results can be extended to the case that \(Z_{i}\) has a general multivariate distribution. However, such results may not be valid if the distribution of \(Z_{i}\) has heavier tail than normal distribution. In fact, our numerical results imply that a faster convergence rate may be achieved when the distribution of \(Z_{i}\) has heavy tail.

## 3 An estimator via data averaging

In this section, we would like to propose a new sketching method which can hopefully have good statistical performance with low computational cost. To be simple, when considering computation time, it is understood that the multiplication of an \(m\times n\) matrix and \(n\times p\) matrix costs \(O(mnp)\) time, and the inversion of a \(p\times p\) matrix costs \(O(p^{3})\) time. Note that the computation time \(O(Np)\) is essential if each observation is accessed at least once, e.g., to be loaded into the memory. The sketched least square problem (2) involves \(n\) reduced observations and the direct computation costs \(O(np^{2}+p^{3})\) computation time. The computation time \(O(p^{3})\) comes from the inversion of a \((p+1)\times(p+1)\) matrix which is essential no matter how \(n\) is chosen. Hence the direct computation of any reasonable estimator which uses the information of the full data requires at least \(O(Np+p^{3})\) time. Thus, we restrict our attention to algorithms that can be completed within \(O(Np+p^{3})\) time.

To complete the computation within \(O(Np+p^{3})\) time, one needs to take \(n=O(N/p+p)\). Theorem 2 implies that if \(n=c_{1}N/p+c_{2}p\) where \(c_{1},c_{2}>0\) are constants, then the optimal convergence rate for sampling methods reduces to \(p/n\) which is equal to the convergence rate for the uniform sampling method. Also, for large \(N\), the reduced sample size \(n=c_{1}N/p+c_{2}p\) may still be large. To achieve a faster convergence rate and a better reduction of data, we would like to consider sketching methods other than sampling methods. This motivates us to propose a new data averaging method.

### Methodology

Let \(\mathscr{J}_{1},\dots,\mathscr{J}_{k}\subset\{1,\dots,N\}\) be \(k\) mutually disjoint index sets, each containing \(r\) indices, and \(\bigcup_{i=1}^{k}\mathscr{J}_{i}=\{1,\dots,N\}\). To use the information of the full data, we assume \(N=kr\). Let \(\bar{Z}_{j}=r^{-1}\sum_{i\in\mathscr{J}_{j}}Z_{i}\) and \(\bar{y}_{j}=r^{-1}\sum_{i\in\mathscr{J}_{j}}y_{i}\) be the averaged observation within the \(j\)th index set, \(j=1,\dots,k\). It can be seen that

\[\bar{y}_{j}=\beta_{0}+\bar{Z}_{j}^{\top}\boldsymbol{\beta}_{1}+\bar{\varepsilon }_{j},\]

where \(\bar{\varepsilon}_{j}=r^{-1}\sum_{i\in\mathscr{J}_{j}}\varepsilon_{i}\). Suppose that the choice of the index sets \(\mathscr{J}_{1},\dots,\mathscr{J}_{k}\) is based on the covariates \(\{Z_{i}\}_{i=1}^{N}\) and does not rely on the responses \(\{y_{i}\}_{i=1}^{N}\). Then \(\bar{\varepsilon}_{1},\dots,\bar{\varepsilon}_{k}\) are mutually independent and are independent of \(\{\bar{Z}_{j}\}_{j=1}^{k}\). Also, \(\bar{\varepsilon}_{j}\) has mean \(0\) and variance \(\sigma_{\varepsilon}^{2}/r\). Thus, the averaged observations also satisfy the linear model and one can estimate \(\boldsymbol{\beta}\) by the least square estimator base on \(k\) reduced observations as \(\hat{\boldsymbol{\beta}}=(\sum_{j=1}^{k}\bar{X}_{j}\bar{X}_{j}^{\top})^{-1}(\sum_ {j=1}^{k}\bar{X}_{j}\bar{y}_{j}),\) where \(\widehat{X}_{j}=r^{-1}\sum_{i\in\mathscr{J}_{j}}X_{i}\), \(j=1,\dots,k\). We would like to choose \(\mathscr{J}_{1},\dots,\mathscr{J}_{k}\) such that \(\hat{\boldsymbol{\beta}}\) is a fast and accurate estimator of \(\boldsymbol{\beta}\). Let \(\mathbf{H}=\sum_{\ell=1}^{k}(\bar{Z}_{\ell}-\bar{Z})(\bar{Z}_{\ell}-\bar{Z})^{\top}\) and \(\bar{Z}=N^{-1}\sum_{i=1}^{N}Z_{i}\). The conditional mean squared error of \(\bm{\beta}\) is

\[\mathrm{E}(\|\hat{\bm{\beta}}-\bm{\beta}\|^{2}\mid\mathbf{Z})= \frac{k\sigma_{\epsilon}^{2}}{N}\operatorname{tr}\left\{\left( \begin{array}{cc}k&\sum_{j=1}^{k}\bar{Z}_{j}^{\top}\\ \sum_{j=1}^{k}\bar{Z}_{j}&\sum_{j=1}^{k}\bar{Z}_{j}\bar{Z}_{j}^{\top}\end{array} \right)^{-1}\right\}\] \[= \frac{k\sigma_{\epsilon}^{2}}{N}\operatorname{tr}\left\{\left( \begin{smallmatrix}\frac{1}{k}+\bar{Z}^{\top}\mathbf{H}^{-1}\bar{Z}&-\bar{Z}^{ \top}\mathbf{H}^{-1}\\ -\mathbf{H}^{-1}\bar{Z}&\mathbf{H}^{-1}\end{smallmatrix}\right)\right\}\] \[= \left(k\left(\operatorname{tr}(\mathbf{H}^{-1})+\bar{Z}^{\top} \mathbf{H}^{-1}\bar{Z}\right)+1\right)\frac{\sigma_{\epsilon}^{2}}{N}.\]

In order to achieve a good statistical accuracy, we would like to choose the index sets such that \(\operatorname{tr}(\mathbf{H}^{-1})+\bar{Z}^{\top}\mathbf{H}^{-1}\bar{Z}\) is minimized.

First we consider the simplest case of \(p=1\). In this case, the matrix \(\mathbf{H}\) reduces to a real number. Since \(\bm{\beta}\in\mathbb{R}^{2}\), one needs at least two observations to estimate \(\bm{\beta}\). To achieve maximum reduction of data, we take \(k=2\). Then \(\mathbf{H}\) takes its maximum when \(\mathscr{J}_{1}=\{i\in\{1,\ldots,N\}:z_{i,1}\leq z_{(N/2),1}\}\) and \(\mathscr{J}_{2}=\{i\in\{1,\ldots,N\}:z_{i,1}\geq z_{(N/2+1),1}\}\). The least square estimator of \(\bm{\beta}\) based on the averaged observations is \(\hat{\bm{\beta}}=\left((\bar{Z}_{2}\bar{y}_{1}-\bar{Z}_{1}\bar{y}_{2})/(\bar{ Z}_{2}-\bar{Z}_{1}),(\bar{y}_{2}-\bar{y}_{1})/(\bar{Z}_{2}-\bar{Z}_{1})\right)^{\top}\). The above estimator \((\bar{y}_{2}-\bar{y}_{1})/(\bar{Z}_{2}-\bar{Z}_{1})\) of \(\bm{\beta}_{1}\) is considered in Barton and Casley (1958) as a quick estimate of \(\bm{\beta}_{1}\), which only considered the case of \(p=1\). To the best of our knowledge, no previous study generalized this estimator of Barton and Casley (1958) to the case \(p>1\).

For the general case of \(p\geq 1\), the exact minimizer of \(\mathrm{E}(\|\hat{\bm{\beta}}-\bm{\beta}\|^{2}\mid\mathbf{Z})\) may not be easy to obtain. A simpler criterion to choose the index sets is to maximize the trace \(\operatorname{tr}(\mathbf{H})=\sum_{\ell=1}^{k}\|\bar{Z}_{\ell}-\bar{Z}\|^{2}\). This problem is equivalent to minimizing \(\sum_{\ell=1}^{k}\sum_{i\in\mathscr{J}_{\ell}}\|Z_{i}-\bar{Z}_{\ell}\|^{2}\) and is an instance of the balanced \(k\)-means clustering problem; see Lin et al. (2019) and the references therein. Unfortunately, algorithms for the \(k\)-means clustering problem are computationally intensive. In fact, for the vanilla \(k\)-means algorithm, each iteration takes \(O(Npk)\) time which even exceeds the computing time of the least square estimator based on the full data. To achieve a balance between the statistical accuracy and the computing time, we deal with each variable in turn. We take \(k=2p\) and for \(j=1,\ldots,p\), we determine two index sets, namely \(\mathscr{L}_{r,j}\) and \(\mathscr{R}_{r,j}\), based on the \(j\)th variable. Hence the set \(\{1,\ldots,N\}\) is partitioned into \(2p\) index sets \(\mathscr{L}_{r,1},\ldots,\mathscr{L}_{r,p}\) and \(\mathscr{R}_{r,1},\ldots,\mathscr{R}_{r,p}\), each containing \(r=N/(2p)\) indices. The choice of these index sets is based on the following lower bound of \(\operatorname{tr}(\mathbf{H})\),

\[\operatorname{tr}(\mathbf{H})= \sum_{j=1}^{p}\sum_{\ell=1}^{p}\Bigg{\{}\Bigg{(}\frac{1}{r}\sum_{i \in\mathscr{L}_{r,\ell}}z_{i,j}-\frac{1}{N}\sum_{i=1}^{N}z_{i,j}\Bigg{)}^{2}+ \Bigg{(}\frac{1}{r}\sum_{i\in\mathscr{R}_{r,\ell}}z_{i,j}-\frac{1}{N}\sum_{i=1 }^{N}z_{i,j}\Bigg{)}^{2}\Bigg{\}}\] \[\geq \sum_{j=1}^{p}\Bigg{[}\sum_{\ell=1}^{j-1}\Bigg{\{}\Bigg{(}\frac{ 1}{r}\sum_{i\in\mathscr{L}_{r,\ell}}z_{i,j}-\frac{1}{N}\sum_{i=1}^{N}z_{i,j} \Bigg{)}^{2}+\Bigg{(}\frac{1}{r}\sum_{i\in\mathscr{R}_{r,\ell}}z_{i,j}-\frac{1 }{N}\sum_{i=1}^{N}z_{i,j}\Bigg{)}^{2}\Bigg{\}}\] \[\qquad+\Bigg{\{}\Bigg{(}\max(\tilde{z}_{j}-\frac{1}{r}\sum_{i\in \mathscr{L}_{r,j}}z_{i,j},0)\Bigg{)}^{2}+\Bigg{(}\max(\frac{1}{r}\sum_{i\in \mathscr{R}_{r,j}}z_{i,j}-\tilde{z}_{j},0)\Bigg{)}^{2}\Bigg{\}}\Bigg{]},\]

where \(\tilde{z}_{j}=(2r(p-j+1))^{-1}\sum_{i\notin\bigcup_{j=1}^{j-1}(\mathscr{L}_{r, \ell}\cup\mathscr{R}_{r,\ell})}z_{i,j}\). For \(j=1,\ldots,p\), we choose \(\mathscr{L}_{r,j}\) and \(\mathscr{R}_{r,j}\) to maximize the \(j\)th term of the above lower bound. Specifically, the first term of the above lower bound is \(\{\max(\tilde{z}_{1}-\sum_{i\in\mathscr{L}_{r,1}}z_{i,1}/r,0)\}^{2}+\{\max(\sum_{i \in\mathscr{R}_{r,1}}z_{i,1}/r-\tilde{z}_{1},0)\}^{2}\), which takes its maximum when \(\mathscr{L}_{r,1}=\{i\in\{1,\ldots,N\}:z_{i,1}\leq\gamma_{1,1}\}\) and \(\mathscr{R}_{r,1}=\{i\in\{1,\ldots,N\}:z_{i,1}\geq\gamma_{2,1}\}\) where \(\gamma_{1,1}=z_{(r),1}\) and \(\gamma_{2,1}=z_{(N-r+1),1}\). After obtaining the index sets \(\mathscr{L}_{r,1},\ldots,\mathscr{L}_{r,j-1}\) and \(\mathscr{R}_{r,1},\ldots,\mathscr{R}_{r,j-1}\), we choose \(\mathscr{L}_{r,j}\) and \(\mathscr{R}_{r,j}\) to maximize the \(j\)th term of the above lower bound, which is equivalent to maximizing \(\{\max(\tilde{z}_{j}-\sum_{i\in\mathscr{L}_{r,j}}z_{i,j}/r,0)\}^{2}+\{\max(\sum_{i \in\mathscr{R}_{r,j}}z_{i,j}/r-\tilde{z}_{j},0)\}^{2}\). Hence we choose \(\mathscr{L}_{r,j}\) and \(\mathscr{R}_{r,j}\) to be the indices of the remaining observations whose \(j\)th variable is no larger than \(\gamma_{1,j}\) and no less than \(\gamma_{2,j}\), respectively, where \(\gamma_{1,j}\) and \(\gamma_{2,j}\) are the \(r\)th smallest and the \(r\)th largest element of \(\{z_{i,j}:i\in\{1,\ldots,N\}\setminus(\bigcup_{\ell=1}^{j-1}(\mathscr{L}_{r, \ell}\cup\mathscr{R}_{r,\ell}))\}\), respectively. We average the observations within the groups \(\mathscr{L}_{r,1},\ldots,\mathscr{L}_{r,p}\) and \(\mathscr{R}_{r,1},\ldots,\mathscr{R}_{r,p}\). Finally, we use the least square estimator based on the \(2p\) averaged observations to estimate \(\bm{\beta}\). The proposed estimation procedure is summarized in Algorithm 1.

In Algorithm 1, our strategy to select the index sets \(\mathscr{L}_{r,j}\) and \(\mathscr{R}_{r,j}\) is closely related to the IBOSS algorithm of Wang et al. (2019). In fact, the index sets in Algorithm 1 is exactly the index sets selected by IBOSS algorithm with subdata size \(n:=N\). Of course, for IBOSS algorithm, taking \(n=N\) is unreasonable since the sample size is not reduced. In fact, for IBOSS algorithm, one needs to take \(n=O(N/p+p)\) to complete the computation within \(O(Np+p^{3})\) time. Thus, the selection procedures of the proposed method and the IBOSS algorithm have different behavior. Theorem 4 will show that under certain conditions, IBOSS can achieve the optimal convergence rate (4) among all sampling methods. We shall see that the statistical performance of Algorithm 1 is even better than the IBOSS algorithm.

Now we give an analysis of the computing time of Algorithm 1. Note that \(\gamma_{1,j}\) and \(\gamma_{2,j}\) are order statistics of no more than \(N\) elements. It is known that the selection of an order statistic among \(m\) elements can be completed within \(O(m)\) time even in the worst case; see Paterson (1996). Hence the computation of \(\gamma_{1,1},\ldots,\gamma_{1,p}\) and \(\gamma_{2,1},\ldots,\gamma_{2,p}\) can be completed within \(O(Np)\) time in total. It takes only one scan of the full data to compute the averaged observations, which takes \(O(Np)\) time. Finally, the computation of \(\hat{\boldsymbol{\beta}}_{\text{A}}\) based on \(2p\) averaged observations can be completed within \(O(p^{3})\) time. In summary, Algorithm 1 can be completed within \(O(Np+p^{3})\) time and reduces the full data to merely \(2p\) observations.

### Asymptotic results

Now we investigate the asymptotic behavior of the conditional mean squared error of \(\hat{\boldsymbol{\beta}}_{\text{A}}\). In our asymptotic results, we treat \(p\) as a function of \(N\), and \(N\) tends to infinity. Let \(Z=(z_{1},\ldots,z_{p})^{\top}\) be a random vector which is independent of \(\mathbf{Z}\) and \(\mathbf{y}\) and has the same distribution as \(Z_{1}\). The following theorem gives the exact limit of \(\mathrm{E}\{\|\hat{\boldsymbol{\beta}}_{\text{A}}-\boldsymbol{\beta}\|^{2}\mid \mathbf{Z}\}\) when \(Z\) is a standard normal random vector.

**Theorem 3**: _Suppose that Assumption 1 holds, \(r=N/(2p)\) is an integer, \(N>2p^{2}\), and \(Z\sim\mathcal{N}(\mathbf{0}_{p},\mathbf{I}_{p})\). Also suppose that as \(N\to\infty\), \(p\to\infty\) and \(p^{3}(\log(p))^{4}\log(N)/N\to 0\). Then as \(N\to\infty\),_

\[\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\text{A}}-\boldsymbol{\beta}\|^{ 2}\mid\mathbf{Z}\right\}=(1+o_{P}(1))\frac{p^{2}\sigma_{\varepsilon}^{2}}{2 \log(2p)N}.\]

Theorem 3 implies that when \(Z\) is a standard normal random vector, the conditional mean squared error of \(\hat{\boldsymbol{\beta}}_{\text{A}}\) has convergence rate \(p^{2}/(\log(2p)N)\). On the other hand, for sampling methods with \(n=c_{1}N/p+c_{2}p\) for constants \(c_{1},c_{2}>0\) such that the computing time may be comparable, Theorem 2 implies that the optimal convergence rate of the conditional mean squared error is \(p^{2}/N\). In this view, the convergence rate of the proposed estimator is faster than sampling methods for \(p\to\infty\).

The proposed algorithm is closely related to the IBOSS algorithm. We would like to derive the asymptotic behavior of the conditional mean squared error of \(\hat{\boldsymbol{\beta}}_{\text{I}}\). Theorem 6(i) of Wang et al. (2019) gives an asymptotic expression of the conditional covariance of \(\hat{\boldsymbol{\beta}}_{\text{I}}\) under the assumption that \(Z\) is normally distributed. It is implied that if \(n\) and \(p\) are fixed, then as \(N\to\infty\),

\[\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{l}}-\boldsymbol{\beta}\|^{2 }\mid\mathbf{Z}\right\}=(1+o_{P}(1))\left(\frac{p}{2\log(N)}\operatorname{tr}( \boldsymbol{\Sigma}^{-1}\operatorname{diag}(\boldsymbol{\Sigma})\boldsymbol{ \Sigma}^{-1})+1\right)\frac{\sigma_{\varepsilon}^{2}}{n}.\] (5)

Now we derive the fine-grained limiting behavior of \(\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{l}}-\boldsymbol{\beta}\|^ {2}\mid\mathbf{Z}\right\}\) for varying \(n\) and \(p\). Let \(\rho_{i,j}=\sigma_{i,j}/(\sigma_{i,i}\sigma_{j,j})^{1/2}\) denote the correlation coefficient between \(z_{i}\) and \(z_{j}\), \(i,j=1,\ldots,p\). Define

\[\alpha_{N}=\frac{p}{p+2\log\left(N/r\right)},\quad\mathbf{W}_{N}=\alpha_{N} \boldsymbol{\Sigma}+(1-\alpha_{N})\boldsymbol{\Sigma}\operatorname{diag}( \boldsymbol{\Sigma})^{-1}\boldsymbol{\Sigma}.\]

We have the following theorem.

**Theorem 4**: _Suppose Assumption 1 holds and \(Z\sim\mathcal{N}(\mu,\boldsymbol{\Sigma})\), \(r=n/(2p)\) is an integer, there exist constants \(C_{1},C_{2},C_{3}>0\) such that \(C_{1}<\lambda_{p}(\boldsymbol{\Sigma})<\lambda_{1}(\boldsymbol{\Sigma})<C_{2}\) and \(\|\mu\|<C_{3}\), there exists a constant \(0<\rho<1/\sqrt{2}\) such that \(\max_{1\leq i\leq j\leq p}|\rho_{i,j}|\leq\rho\), there exist \(\epsilon_{1},\epsilon_{2}\in(0,1)\) such that for sufficiently large \(N\), \(4r\leq N^{\epsilon_{1}}\), \(p\leq N^{\epsilon_{2}}\) and_

\[(1+2\epsilon_{2})^{1/2}|\rho|+\{(\epsilon_{1}+2\epsilon_{2})(1-\rho^{2})\}^{1 /2}<(1-\epsilon_{1})^{1/2}.\] (6)

_Furthermore, suppose as \(N\to\infty\),_

\[\frac{r}{N}\to 0\quad\text{and}\quad\frac{p^{2}(\log(n))^{4}}{\max\left(n,r \log\left(N/r\right)\right)}\to 0.\] (7)

_Then as \(N\to\infty\),_

\[\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{l}}-\boldsymbol{\beta}\|^ {2}\mid\mathbf{Z}\right\}=(1+o_{P}(1))\left(\alpha_{N}\operatorname{tr}( \mathbf{W}_{N}^{-1})+\alpha_{N}\mu^{\top}\mathbf{W}_{N}^{-1}\mu+1\right)\frac {\sigma_{\varepsilon}^{2}}{n}.\]

**Remark 1**: _If \(n\) and \(p\) are fixed, then the condition (6) is satisfied for sufficiently large \(N\). On the other hand, if \(\rho_{i,j}=0\) for all \(1\leq i<j\leq p\), that is, the variables are independent, then the condition (6) becomes \(\epsilon_{1}+\epsilon_{2}<1/2\). In this case, the condition (6) holds for \(n=O(N^{\epsilon})\) for some \(0<\epsilon<1/2\)._

**Remark 2**: _The condition (7) is satisfied if \(r/N\to 0\) and \(p=O(n^{1/2-\epsilon})\) for some \(\epsilon>0\). Also, the condition (7) can be satisfied for arbitrary \(n\), \(p\) provided \(N\) is sufficiently large._

Compared with Theorem 6(i) in Wang et al. [2019], our Theorem 4 gives a more comprehensive characterization of the asymptotics of \(\operatorname{Var}(\hat{\boldsymbol{\beta}}_{\mathbf{l}}\mid\mathbf{X})\). If \(\mu=\boldsymbol{0}_{p}\) and \(\alpha_{N}\to 0\), then Theorem 4 implies that

\[\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{l}}-\boldsymbol{\beta}\|^ {2}\mid\mathbf{Z}\right\}=(1+o_{P}(1))\left(\frac{p}{2\log\left(N/r\right)} \operatorname{tr}(\boldsymbol{\Sigma}^{-1}\operatorname{diag}(\boldsymbol{ \Sigma})\boldsymbol{\Sigma}^{-1})+1\right)\frac{\sigma_{\varepsilon}^{2}}{n}.\] (8)

If we further assume that \(\log(r)/\log(N)\to 0\), then the expressions (5) and (8) are equivalent. However, Theorem 4 implies that the expression (8) is not valid if \(\alpha_{N}\) does not converge to \(0\).

Now we consider the special case that \(Z\sim\mathcal{N}(\boldsymbol{0}_{p},\mathbf{I}_{p})\). In this case, Theorem 4 implies that if \(r/N\to 0\), \(n=O(N^{\epsilon})\) for some \(0<\epsilon<1/2\) and \(p=O(n^{1/2-\epsilon^{*}})\) for some \(0<\epsilon^{*}<1/2\), then \(\mathrm{E}\{\|\hat{\boldsymbol{\beta}}_{\mathbf{l}}-\boldsymbol{\beta}\|^{2} \mid\mathbf{Z}\}\) has convergence rate \((\alpha_{N}p+1)/n\). We have

\[\alpha_{N}=\frac{p}{p+2\log(2p)+2\log(N/n)}=O\left(\frac{p}{p+\log(N/n)}\right).\]

Hence if \(\log(N/n)=O(p^{2})\), then \(\mathrm{E}\left\{\|\hat{\boldsymbol{\beta}}_{\mathbf{l}}-\boldsymbol{\beta}\|^ {2}\mid\mathbf{Z}\right\}=O_{P}(p^{2}/(n(p+\log(N/n)))\) which matches (4). In this case, \(\hat{\boldsymbol{\beta}}_{\mathbf{l}}\) achieves the optimal rate of sampling methods given by Theorem 2, and hence the optimal rate given by Theorem 2 is tight.

Simulation results

In this section, we conduct simulations to examine the performance of the proposed estimator \(\hat{\bm{\beta}}_{\mathrm{A}}\). For comparison, the simulations also include the vanilla data averaging algorithm (abbreviated as VDA) where the full data is uniformly divided into \(k:=2p\) groups in random and the observations are averaged within groups, the least square estimator based on the uniform sampling method (abbreviated as UNI), the leverage score sampling estimator (abbreviated as LEV) as described in Ma et al. (2015), the sketched least square estimator based on the subsampled randomized Hadamard transform (abbreviated as SRHT), the estimator \(\hat{\bm{\beta}}_{\mathrm{I}}\) produced by the IBOSS algorithm, and the least square estimator based on the full data (abbreviated as FULL). The methods VDA, UNI, LEV, SRHT and IBOSS are instances of the sketching framework (2). For these three methods, we take \(n=N/p\). For SRHT, if \(N\) is a power of \(2\), then the sketching matrix \(\mathbf{O}=(\mathbf{PHD})^{\top}\), where \(\mathbf{P}\) is an \(n\times N\) matrix whose rows are uniformly sampled from the standard bases of \(\mathbb{R}^{N}\), \(\mathbf{H}\) is an \(N\times N\) Walsh-Hadamard matrix (see, e.g., Dobriban and Liu (2019) and \(\mathbf{D}\) is an \(N\times N\) diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables; and if \(N\) is not a power of \(2\), we pad zeros to the original data to make \(N\) reach a power of \(2\). The computation of the proposed method and the IBOSS estimator rely on certain order statistics. For these two methods, the algorithm SELECT of Floyd and Rivest (1975) is used to select the order statistics. The algorithms are implemented by C++. To be fair, for all algorithms, the estimators of \(\bm{\beta}\) are solved by Gaussian elimination. The simulations are performed on a CPU with 3.30 GHz.

The statistical performance of an estimator \(\hat{\bm{\beta}}\) of \(\bm{\beta}\) is evaluated by the empirical mean squared error based on \(100\) independent replications. Specifically, the empirical mean squared error is defined as \(100^{-1}\sum_{i=1}^{100}\|\hat{\bm{\beta}}^{(i)}-\bm{\beta}\|^{2}\), where \(\hat{\bm{\beta}}^{(i)}\) is the estimator in the \(i\)th replication. In all simulations, the ground truth of \(\bm{\beta}\) is a vector with all elements equal to \(1\). We consider two distributions of \(\varepsilon_{1}\): the normal distribution \(\varepsilon_{1}\sim\mathscr{N}(0,1)\) and the normalized chi-squared distribution \(\varepsilon_{1}\sim(\chi^{2}(1)-1)/\sqrt{2}\). We consider the following distributions of \(Z\).

* Case 1: \(\{z_{j}\}_{j=1}^{p}\) are i.i.d. with uniform distribution \(\text{Uniform}(0,1)\).
* Case 2: \(\{z_{j}\}_{j=1}^{p}\) are i.i.d. with normal distribution \(\mathscr{N}(0,1)\).
* Case 3: \(\{z_{j}\}_{j=1}^{p}\) are i.i.d. with lognormal distribution, that is, \(\log(z_{i})\sim\mathscr{N}(0,1)\).
* Case 4: \(\{z_{j}\}_{j=1}^{p}\) are i.i.d. with student \(t\) distribution with \(3\) degrees of freedom \(t_{3}\).
* Case 5: \(Z\sim\mathscr{N}(\mathbf{0}_{p},\bm{\Sigma})\), where the diagonal elements of \(\bm{\Sigma}\) all equals \(1\) and the off diagonal elements all equals \(0.5\).
* Case 6: \(Z\) is distributed as a mixture of \(\mathscr{N}(\mu,\bm{\Sigma})\) and \(\mathscr{N}(-\mu,\bm{\Sigma})\) where \(\mu\) has all \(1\) entries, \(\bm{\Sigma}\) is defined as in Case 5, and the mixing proportions of the two component distributions are both \(0.5\).

Table 2 and Tables A.1-A.3 in Supplementary Material list the empirical mean squared errors of various estimators, where the proposed method is referred to as NEW. Among the implemented methods, VDA, UNI and SRHT are data-oblivious sketching methods while NEW, LEV and IBOSS are data-aware sketching methods. It can be seen that VDA has the worst performance. This implies that the selection procedure is necessary for the proposed method. The simulation results show that UNI, SRHT, LEV have similar statistical performance. It can be seen that the proposed estimator can achieve substantial improvement over the competing sketching methods. Especially, the proposed method shows superiority when \(p\) is large.

We also evaluate the computing time for various algorithms. Table 3 lists the computing time for Case \(1\) with \(\epsilon_{1}\sim\mathscr{N}(0,1)\). Results for other settings are similar. It can be seen that the proposed method is slower than VDA and UNI. Compared with VDA and UNI, however, the proposed method has significantly better statistical performance and can achieve better data reduction. Compared with IBOSS, the proposed method has a comparable computing time but a much better statistical performance. In summary, the new estimator has good performance in both speed and statistical performance.

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_FAIL:11]