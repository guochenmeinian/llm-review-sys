# A Definition of Continual Reinforcement Learning

 David Abel

dmabel@google.com

Google DeepMind

&Andre Barreto

andrebarreto@google.com

Google DeepMind

&Benjamin Van Roy

benvanroy@google.com

Google DeepMind

&Doina Precup

doinap@google.com

Google DeepMind

&Hado van Hasselt

hado@google.com

Google DeepMind

&Satinder Singh

baveja@google.com

Google DeepMind

###### Abstract

In a standard view of the reinforcement learning problem, an agent's goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as _finding a solution_, rather than treating learning as _endless adaptation_. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that "never stop learning" through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.

## 1 Introduction

In _The Challenge of Reinforcement Learning_, Sutton states: "Part of the appeal of reinforcement learning is that it is in a sense the whole AI problem in a microcosm" [56]. Indeed, the problem facing an agent that learns to make better decisions from experience is at the heart of the study of Artificial Intelligence (AI). Yet, when we study the reinforcement learning (RL) problem, it is typical to restrict our focus in a number of ways. For instance, we often suppose that a complete description of the state of the environment is available to the agent, or that the interaction stream is subdivided into episodes. Beyond these standard restrictions, however, there is another significant assumption that constrains the usual framing of RL: We tend to concentrate on agents that learn to solve problems, rather than agents that learn forever. For example, consider an agent learning to play Go: Once the agent has discovered how to master the game, the task is complete, and the agent's learning can stop. This view of learning is often embedded in the standard formulation of RL, in which an agent interacts with a Markovian environment with the goal of efficiently identifying an optimal policy, at which point learning can cease.

But what if this is not the best way to model the RL problem? That is, instead of viewing learning as _finding a solution_, we can instead think of it as _endless adaptation_. This suggests study of the _continual_ reinforcement learning (CRL) problem [47; 48; 25; 27], as first explored in the thesis byRing [46], with close ties to supervised never-ending [10; 39; 43] and continual learning [47; 48; 26; 54; 41; 42; 49; 22; 30; 45; 4].

Despite the prominence of CRL, the community lacks a clean, general definition of this problem. It is critical to develop such a definition to promote research on CRL from a clear conceptual foundation, and to guide us in understanding and designing continual learning agents. To these ends, this paper is dedicated to carefully defining the CRL problem. Our definition is summarized as follows:

_The CRL Problem (Informal)_

_An RL problem is an instance of CRL if the best agents never stop learning._

The core of our definition is framed around two new insights that formalize the notion of "agents that never stop learning": (i) we can understand _every agent_ as implicitly searching over a set of history-based policies (Theorem 3.1), and (ii) _every agent_ will either continue this search forever, or eventually stop (Remark 3.2). We make these two insights rigorous through a pair of logical operators on agents that we call _generates_ and _reaches_ that provide a new mathematical language for characterizing agents. Using these tools, we then define CRL as any RL problem in which all of the best agents never stop their implicit search. We provide two motivating examples of CRL, illustrating that traditional multi-task RL and continual supervised learning are special cases of our definition. We further identify necessary properties of CRL (Theorem 4.1) and the new operators (Theorem 4.2, Theorem 4.3). Collectively, these definitions and insights formalize many intuitive concepts at the heart of continual learning, and open new research pathways surrounding continual learning agents.

## 2 Preliminaries

We first introduce key concepts and notation. Our conventions are inspired by Ring [46], the recent work by Dong et al. [16] and Lu et al. [32], as well as the literature on _general RL_ by Hutter [23; 24], Lattimore [28], Leike [29], Cohen et al. [12], and Majeed [36].

Notation.We let capital calligraphic letters denote sets (\(\mathcal{X}\)), lower case letters denote constants and functions (\(x\)), italic capital letters denote random variables (\(X\)), and blackboard capitals denote the natural and real numbers (\(\mathbb{N},\mathbb{R},\mathbb{N}_{0}=\mathbb{N}\cup\{0\}\)). Additionally, we let \(\Delta(\mathcal{X})\) denote the probability simplex over the set \(\mathcal{X}\). That is, the function \(p:\mathcal{X}\times\mathcal{Y}\rightarrow\Delta(\mathcal{Z})\) expresses a probability mass function \(p(\cdot\mid x,y)\), over \(\mathcal{Z}\), for each \(x\in\mathcal{X}\) and \(y\in\mathcal{Y}\). Lastly, we use \(\neg\) to denote logical negation, and we use \(\forall_{x\in\mathcal{X}}\) and \(\exists_{x\in\mathcal{X}}\) to express the universal and existential quantifiers over a set \(\mathcal{X}\).

### Agents and Environments

We begin by defining environments, agents, and related artifacts.

**Definition 2.1**.: _An agent-environment **interface** is a pair \((\mathcal{A},\mathcal{O})\) of countable sets \(\mathcal{A}\) and \(\mathcal{O}\) where \(|\mathcal{A}|\geq 2\) and \(|\mathcal{O}|\geq 1\)._

We refer to elements of \(\mathcal{A}\) as _actions_, denoted \(a\), and elements of \(\mathcal{O}\) as _observations_, denoted \(o\). Histories define the possible interactions between an agent and an environment that share an interface.

**Definition 2.2**.: _The **histories** with respect to interface \((\mathcal{A},\mathcal{O})\) are the set of sequences of action-observation pairs,_

\[\mathcal{H}=\bigcup_{t=0}^{\infty}\left(\mathcal{A}\times\mathcal{O}\right)^{t}.\] (2.1)

We refer to an individual element of \(\mathcal{H}\) as a _history_, denoted \(h\), and we let \(hh^{\prime}\) express the history resulting from the concatenation of any two histories \(h,h^{\prime}\in\mathcal{H}\). Furthermore, the set of histories of length \(t\in\mathbb{N}_{0}\) is defined as \(\mathcal{H}_{t}=\left(\mathcal{A}\times\mathcal{O}\right)^{t}\), and we use \(h_{t}\in\mathcal{H}_{t}\) to refer to a history containing \(t\) action-observation pairs, \(h_{t}=a_{0}o_{1}\ldots a_{t-1}o_{t}\), with \(h_{0}=\emptyset\) the empty history. An environment is then a function from the set of all environments, \(\mathcal{E}\), that produces observations given a history.

**Definition 2.3**.: _An **environment** with respect to interface (\(\mathcal{A},\mathcal{O}\)) is a function \(e:\mathcal{H}\times\mathcal{A}\rightarrow\Delta(\mathcal{O})\)._

This model of environments is general in that it can capture Markovian environments such as Markov decision processes (MDPs, Puterman, 2014) and partially observable MDPs (Cassandra et al., 1994), as well as both episodic and non-episodic settings. We next define an agent as follows.

**Definition 2.4**.: _An **agent** with respect to interface \((\mathcal{A},\mathcal{O})\) is a function \(\lambda:\mathcal{H}\to\Delta(\mathcal{A})\)._

We let \(\mathbb{A}\) denote the set of all agents, and let \(\Lambda\) denote any non-empty subset of \(\mathbb{A}\). This treatment of an agent captures the mathematical way experience gives rise to behavior, as in "agent functions" from work by Russell and Subramanian [50]. This is in contrast to a mechanistic account of agency as proposed by Dong et al. [16] and Sutton [58]. Further, note that Definition2.4 is precisely a history-based policy; we embrace the view that there is no real distinction between an agent and a policy, and will refer to all such functions as "agents" unless otherwise indicated.

### Realizable Histories

We will be especially interested in the histories that occur with non-zero probability as a result of the interaction between a particular agent and environment.

**Definition 2.5**.: _The **realizable histories** of a given agent-environment pair, \((\lambda,e)\), define the set of histories of any length that can occur with non-zero probability from the interaction of \(\lambda\) and \(e\),_

\[\mathcal{H}^{\lambda,e}=\vec{\mathcal{H}}=\bigcup_{t=0}^{\infty}\left\{h_{t }\in\mathcal{H}_{t}:\prod_{k=0}^{t-1}e(o_{k+1}\mid h_{k},a_{k})\lambda(a_{k} \mid h_{k})>0\right\}.\] (2.2)

Given a realizable history \(h\), we will refer to the realizable history _suffixes_, \(h^{\prime}\), which, when concatenated with \(h\), produce a realizable history \(hh^{\prime}\in\vec{\mathcal{H}}\).

**Definition 2.6**.: _The **realizable history suffixes** of a given \((\lambda,e)\) pair, relative to a history prefix \(h\in\mathcal{H}^{\lambda,e}\), define the set of histories that, when concatenated with prefix \(h\), remain realizable,_

\[\mathcal{H}^{\lambda,e}_{h}=\vec{\mathcal{H}}_{h}=\{h^{\prime}\in\mathcal{H}: hh^{\prime}\in\mathcal{H}^{\lambda,e}\}.\] (2.3)

We abbreviate \(\mathcal{H}^{\lambda,e}\) to \(\vec{\mathcal{H}}\), and \(\mathcal{H}^{\lambda,e}_{h}\) to \(\vec{\mathcal{H}}_{h}\), where \(\lambda\) and \(e\) are obscured for brevity.

### Reward, Performance, and the RL Problem

Supported by the arguments of Bowling et al. [7], we assume that all of the relevant goals or purposes of an agent are captured by a deterministic reward function (in line with the _reward hypothesis_[57]).

**Definition 2.7**.: _We call \(r:\mathcal{A}\times\mathcal{O}\to\mathbb{R}\) a **reward function**._

We remain agnostic to how the reward function is implemented; it could be a function inside of the agent, or the reward function's output could be a special scalar in each observation. Such commitments do not impact our framing. When we refer to an environment we will implicitly mean that a reward function has been selected as well. We remain agnostic to how reward is aggregated to determine performance, and instead adopt the function \(v\) defined as follows.

**Definition 2.8**.: _The **performance**, \(v:\mathcal{H}\times\mathbb{A}\times\mathcal{E}\to[\mathsf{v}_{\min},\mathsf{ v}_{\max}]\) is a bounded function for fixed constants \(\mathsf{v}_{\min},\mathsf{v}_{\max}\in\mathbb{R}\)._

The function \(v(\lambda,e\mid h)\) expresses some statistic of the received future random rewards produced by the interaction between \(\lambda\) and \(e\) following history \(h\), where we use \(v(\lambda,e)\) as shorthand for \(v(\lambda,e\mid h_{0})\). While we accommodate any \(v\) that satisfies the above definition, it may be useful to think of specific choices of \(v(\lambda,e\mid h_{t})\), such as the average reward,

\[\liminf_{k\to\infty}\frac{1}{k}\mathbb{E}_{\lambda,e}[R_{t}+\ldots+R_{t+k} \mid H_{t}=h_{t}],\] (2.4)

where \(\mathbb{E}_{\lambda,e}[\ \cdots\mid H_{t}=h_{t}]\) denotes expectation over the stochastic process induced by \(\lambda\) and \(e\) following history \(h_{t}\). Or, we might consider performance based on the expected discounted reward, \(v(\lambda,e\mid h_{t})=\mathbb{E}_{\lambda,e}[R_{t}+\gamma R_{t+1}+\ldots\mid H _{t}=h_{t}]\), where \(\gamma\in[0,1)\) is a discount factor.

The above components give rise to a simple definition of the RL problem.

**Definition 2.9**.: _An instance of the **RL problem** is defined by a tuple \((e,v,\Lambda)\) as follows_

\[\Lambda^{*}=\arg\max_{\lambda\in\Lambda}v(\lambda,e).\] (2.5)

This captures the RL problem facing an _agent designer_ that would like to identify an optimal agent (\(\lambda^{*}\in\Lambda^{*}\)) with respect to the performance (\(v\)), among the available agents (\(\Lambda\)), in a particular environment (\(e\)). We note that a simple extension of this definition of the RL problem might instead consider a set of environments (or similar alternatives).

Agent Operators: Generates and Reaches

We next introduce two new insights about agents, and the logical operators that formalize them:

1. Theorem 3.1: _Every agent_ can be understood as searching over another set of agents.
2. Remark 3.2: _Every agent_ will either continue their search forever, or eventually stop.

We make these insights precise by introducing a pair of logical operators on agents: (1) a set of agents _generates_ (Definition 3.4) another set of agents, and (2) a given agent _reaches_ (Definition 3.5) an agent set. Together, these operators enable us to define _learning_ as the implicit search process captured by the first insight, and _continual learning_ as the process of continuing this search indefinitely.

### Operator 1: An Agent Basis Generates an Agent Set.

The first operator is based on two complementary intuitions.

From the first perspective, an agent can be understood as _searching_ over a space of representable action-selection strategies. For instance, in an MDP, agents can be interpreted as searching over the space of policies (that is, the space of stochastic mappings from the MDP's state to action). It turns out this insight can be extended to any agent and any environment.

The second complementary intuition notes that, as agent designers, we often first identify the space of representable action-selection strategies of interest. Then, it is natural to design agents that search through this space. For instance, in designing an agent to interact with an MDP, we might be interested in policies representable by a neural network of a certain size and architecture. When we design agents, we then consider all agents (choices of loss function, optimizer, memory, and so on) that search through the space of assignments of weights to this particular neural network using standard methods like gradient descent. We codify these intuitions in the following definitions.

**Definition 3.1**.: _An **agent basis** (or simply, a basis), \(\Lambda_{\mathrm{B}}\subset\mathbb{A}\), is any non-empty subset of \(\mathbb{A}\)._

Notice that an agent basis is a choice of agent set, \(\Lambda\). We explicitly call out a basis with distinct notation (\(\Lambda_{\mathrm{B}}\)) as it serves an important role in the discussion that follows. For example, we next introduce _learning rules_ as functions that switch between elements of an agent basis for each history.

**Definition 3.2**.: _A **learning rule** over an agent basis \(\Lambda_{\mathrm{B}}\) is a function, \(\sigma:\mathcal{H}\rightarrow\Lambda_{\mathrm{B}}\), that selects a base agent for each history._

We let \(\mathbb{E}\) denote the set of all learning rules over \(\Lambda_{\mathrm{B}}\), and let \(\Sigma\) denote any non-empty subset of \(\mathbb{E}\). A learning rule is a mechanism for switching between the available base agents following each new experience. Notice that learning rules are deterministic; while a simple extension captures the stochastic case, we will see by Theorem 3.1 that the above is sufficiently general in a certain sense. We use \(\sigma(h)(h)\) to refer to the action distribution selected by the agent \(\lambda=\sigma(h)\) at any history \(h\).

**Definition 3.3**.: _Let \(\Sigma\) be a set of learning rules over some basis \(\Lambda_{\mathrm{B}}\), and let \(e\) be an environment. We say that a set \(\Lambda\) is **\(\mathbf{\Sigma}\)-generated** by \(\Lambda_{\mathrm{B}}\) in \(e\), denoted \(\Lambda_{\mathrm{B}}\upharpoonright_{\Sigma}\Lambda\), if and only if_

\[\forall_{\lambda\in\Lambda}\exists_{\sigma\in\Sigma}\forall_{h\in\tilde{ \mathcal{H}}}\ \ \lambda(h)=\sigma(h)(h).\] (3.1)

Thus, any choice of \(\Sigma\) together with a basis \(\Lambda_{\mathrm{B}}\) induces a family of agent sets whose elements can be understood as switching between the basis according to the rules prescribed by \(\Sigma\). We then say that a basis _generates_ an agent set in an environment if there exists a set of learning rules that switches between the basis elements to produce the agent set.

**Definition 3.4**.: _We say a basis \(\Lambda_{\mathrm{B}}\) generates \(\Lambda\) in \(e\), denoted \(\Lambda_{\mathrm{B}}\upharpoonright\Lambda\), if and only if_

\[\Lambda_{\mathrm{B}}\upharpoonright_{\Sigma}\Lambda.\] (3.2)

Intuitively, an agent basis \(\Lambda_{\mathrm{B}}\) generates another agent set \(\Lambda\) just when the agents in \(\Lambda\) can be understood as switching between the base agents. It is in this sense that we can understand agents as searching through a basis--an agent is just a particular sequence of history-conditioned switches over a basis. For instance, let us return to the example of a neural network: The agent basis might represent a specific multilayer perceptron, where each element of this basis is an assignment to the network's weights. The learning rules are different mechanisms that choose the next set of weights in response to experience (such as gradient descent). Together, the agent basis and the learning rules _generate_ the set of agents that search over choices of weights in reaction to experience. We present a cartoon visual of the generates operator in Figure 1(a).

Now, using the generates operator, we revisit and formalize the central insight of this section: Every agent can be understood as implicitly searching over an agent basis. We take this implicit search process to be the behavioral signature of learning.

**Theorem 3.1**.: _For any agent-environment pair (\(\lambda,e\)), there exists infinitely many choices of a basis, \(\Lambda_{\mathrm{B}}\), such that both (1) \(\lambda\notin\Lambda_{\mathrm{B}}\), and (2) \(\Lambda_{\mathrm{B}}\notin\{\lambda\}\)._

Due to space constraints, all proofs are deferred to Appendix B.

We require that \(\lambda\notin\Lambda_{\mathrm{B}}\) to ensure that the relevant bases are non-trivial generators of \(\{\lambda\}\). This theorem tells us that no matter the choice of agent or environment, we can view the agent as a series of history-conditioned switches between basis elements. In this sense, we can understand the agent _as if_1 it were carrying out a search over the elements of some \(\Lambda_{\mathrm{B}}\). We emphasize that there are infinitely many choices of such a basis to illustrate that there are many plausible interpretations of an agent's behavior--we return to this point throughout the paper.

Footnote 1: We use _as if_ in the sense of the positive economists, such as Friedman [19].

### Operator 2: An Agent Reaches a Basis.

Our second operator reflects properties of an agent's limiting behavior in relation to a basis. Given an agent and a basis that the agent searches through, what happens to the agent's search process in the limit: does the agent keep switching between elements of the basis, or does it eventually stop? For example, in an MDP, many agents of interest eventually stop their search on a choice of a fixed policy. We formally define this notion in terms of an agent _reaching_ a basis according to two modalities: an agent (i) _sometimes_ or (ii) _never_ reaches a basis.

**Definition 3.5**.: _We say agent \(\lambda\in\land\)**sometimes reaches \(\Lambda_{\mathrm{B}}\) in \(e\), denoted \(\lambda\narrowarrow\Lambda_{\mathrm{B}}\), if and only if_

\[\exists_{h\in\tilde{\mathcal{H}}}\exists_{\lambda_{\mathrm{B}}\in\Lambda_{ \mathrm{B}}}\forall_{h^{\prime}\in\tilde{\mathcal{H}}_{h}}\ \lambda(hh^{\prime})=\lambda_{\mathrm{B}}(hh^{\prime}).\] (3.3)

That is, for at least one realizable history, there is some base agent (\(\lambda_{\mathrm{B}}\)) that produces the same action distribution as \(\lambda\) forever after. This indicates that the agent can be understood as if it has stopped its search over the basis. We present a visual of sometimes reaches in Figure 1(b). By contrast, we say an agent _never_ reaches a basis just when it never becomes equivalent to a base agent.

**Definition 3.6**.: _We say agent \(\lambda\in\land\)**never reaches \(\Lambda_{\mathrm{B}}\) in \(e\), denoted \(\lambda\narrowarrow\Lambda_{\mathrm{B}}\), iff \(\neg(\lambda\narrowarrow\Lambda_{\mathrm{B}})\)._

Figure 1: A visual of the generates (left) and sometimes reaches (right) operators. **(a) Generates:** An agent basis, \(\Lambda_{\mathrm{B}}\), comprised of three base agents depicted by the triangle, circle, and square, generates a set \(\Lambda\) containing agents that can each be understood as switching between the base agents in the realizable histories of environment \(e\). **(b) Sometimes Reaches:** On the right, we visualize \(\lambda_{1}\in\Lambda\) generated by \(\Lambda_{\mathrm{B}}\) (from the figure on the left) to illustrate the concept of _sometimes reaches_. That is, the agent’s choice of action distribution at each history can be understood as switching between the three basis elements, and there is at least one history for which the agent stops switching—here, we show the agent settling on the choice of the blue triangle and never switching again.

[MISSING_PAGE_EMPTY:6]

### CRL Examples

We next detail two examples of CRL to provide further intuition.

Q-Learning in Switching MDPs.First we consider a simple instance of CRL based on the standard multi-task view of MDPs. In this setting, the agent repeatedly samples an MDP to interact with from a fixed but unknown distribution [64, 9, 2, 25, 20]. In particular, we make use of the switching MDP environment from Luketina et al. [33]. The environment \(e\) consists of a collection of \(n\) underlying MDPs, \(m_{1},\ldots,m_{n}\), with a shared action space and environment-state space. We refer to this environment-state space using observations, \(o\in\mathcal{O}\). The environment has a fixed constant positive probability of \(0.001\) to switch the underlying MDP, which yields different transition and reward functions until the next switch. The agent only observes each environment state \(o\in\mathcal{O}\), which does not reveal the identity of the active MDP. The rewards of each underlying MDP are structured so that each MDP has a unique optimal policy. We assume \(v\) is defined as the average reward, and the basis is the set of \(\epsilon\)-greedy policies over all \(Q(o,a)\) functions, for fixed \(\epsilon=0.15\). Consequently, the set of agents we generate, \(\Lambda_{\text{B}}\vcentcolon\Lambda\), consists of all agents that switch between these \(\epsilon\)-greedy policies.

Now, the components \((e,v,\Lambda,\Lambda_{\text{B}})\) have been defined, we can see that this is indeed an instance of CRL: None of the base agents can be optimal, as the moment that the environment switches its underlying MDP, we know that any previously optimal policy will no longer be optimal in the next MDP following the switch. Therefore, any agent that _converges_ (in that it reaches the basis \(\Lambda_{\text{B}}\)) cannot be optimal either for the same reason. We conclude that all optimal agents in \(\Lambda\) are continual learning agents relative to the basis \(\Lambda_{\text{B}}\).

We present a visual of this domain in Figure 2(a), and conduct a simple experiment contrasting the performance of \(\epsilon\)-greedy _continual_ Q-learning (blue) that uses a constant step-size parameter of \(\alpha=0.1\), with a _convergent_ Q-learning (green) that anneals its step size parameter over time to zero. Both use \(\epsilon=0.15\), and we set the number of underlying MDPs to \(n=10\). We present the average reward with 95% confidence intervals, averaged over 250 runs, in Figure 2(b). Since both variants of Q-learning can be viewed as searching over \(\Lambda_{\text{B}}\), the annealing variant that stops its search will under-perform compared to the continual approach. These results support the unsurprising conclusion that it is better to continue searching over the basis rather than converge in this setting.

Continual Supervised Learning.Second, we illustrate the power of our CRL definition to capture continual supervised learning. We adopt the problem setting studied by Mai et al. [35]. Let \(\mathcal{X}\) denote a set of objects to be labeled, each belonging to one of \(k\in\mathbb{N}\) classes. The observation space \(\mathcal{O}\) consists of pairs, \(o_{t}=(x_{t},y_{t})\), where \(x_{t}\in\mathcal{X}\) and \(y_{t}\in\mathcal{Y}\). Here, each \(x_{t}\) is an input object to be classified and \(y_{t}\) is the label for the previous input \(x_{t-1}\). Thus, \(\mathcal{O}=\mathcal{X}\times\mathcal{Y}\). We assume by convention that the initial

Figure 2: A visual of a grid world instance of the switching MDPs problem (left) [33], and results from an experiment contrasting continual learning and convergent Q-learning (right). The environment pictured contains \(n\) distinct MDPs. Each underlying MDP shares the same state space and action space, but varies in transition and reward functions, as indicated by the changing walls and rewarding locations (stars, circles, and fire). The results pictured on the right contrast continual Q-learning (with \(\alpha=0.1\)) with traditional Q-learning that anneals its step-size parameter to zero over time.

label \(y_{0}\) is irrelevant and can be ignored. The agent will observe a sequence of object-label pairs, \((x_{0},y_{0}),(x_{1},y_{1}),\ldots\), and the action space is a choice of label, \(\mathcal{A}=\{a_{1},\ldots,a_{k}\}\) where \(|\mathcal{Y}|=k\). The reward for each history \(h_{t}\) is \(+1\) if the agent's most recently predicted label is correct for the previous input, and \(-1\) otherwise:

\[r(a_{t-1}o_{t})=r(a_{t-1}y_{t})=\begin{cases}+1&a_{t-1}=y_{t},\\ -1&a_{t-1}=\text{otherwise}.\end{cases}\] (4.1)

Concretely, the continual learning setting studied by Mai et al. [35] supposes the learner will receive samples from a sequence of probability distributions, \(d_{0},d_{1},\ldots\), each supported over \(\mathcal{X}\times\mathcal{Y}\). The \((x,y)\in\mathcal{X}\times\mathcal{Y}\) pairs experienced by the learner are determined by the sequence of distributions. We capture this distributional shift in an environment \(e\) that shifts its probability distribution over \(\mathcal{O}\) depending on the history to match the sequence, \(d_{0},d_{1},\ldots\).

Now, is this an instance of CRL? To answer this question precisely, we need to select a \((\Lambda,\Lambda_{\text{B}})\) pair. We adopt the basis \(\Lambda_{\text{B}}=\{\lambda_{\text{B}}:x\mapsto y_{t},\forall_{y_{t}\in \mathcal{Y}}\}\) that contains each classifier that maps each object to each possible label. By the universal set of learning rules \(\Sigma\), this basis generates the set of all agents that search over classifiers. Now, our definition says the above is an instance of CRL just when every optimal agent never stops switching between classifiers, rather than stop their search on a fixed classifier. Consequently, if there is an optimal classifier in \(\Lambda_{\text{B}}\), then this will not be an instance of CRL. If, however, the environment imposes enough distributional shift (changing labels, adding mass to new elements, and so on), then the _only_ optimal agents will be those that always switch among the base classifiers, in which case the setting is an instance of CRL.

### Relationship to Other Views on Continual Learning

The spirit of continual learning has been an important part of machine learning research for decades, often appearing under the name of "lifelong learning" [63; 62; 53; 55; 51; 3; 4], "never-ending learning" [39; 43] with close ties to transfer-learning [61; 60], meta-learning [52; 17], as well as online learning and non-stationarity [5; 40; 13; 6; 31]. In a similar vein, the phrase "continuing tasks" is used in the classic RL textbook [59] to refer explicitly to cases when the interaction between agent and environment is not subdivided into episodes. Continual reinforcement learning was first posed in the thesis by Ring [46]. In later work [47; 48], Ring proposes a formal definition of the continual reinforcement learning problem--The emphasis of Ring's proposal is on the _generality_ of the environment: rather than assume that agents of interest will interact with an MDP, Ring suggests studying the unconstrained case in which an agent must maximize performance while only receiving a stream of observations as input. The environment or reward function, in this sense, may change over time or may be arbitrarily complex. This proposal is similar in spirit to _general RL_, studied by Hutter [24], Lattimore [28], Leike [29], and others [12; 37; 36] in which an agent interacts with an unconstrained environment. General RL inspires many aspects of our conception of CRL; for instance, our emphasis on history-dependence rather than environment-state comes directly from general RL. More recently, Khetarpal et al. [25] provide a comprehensive survey of the continual reinforcement learning literature. We encourage readers to explore this survey for a detailed history of the subject.2 In the survey, Khetarpal et al. propose a definition of the CRL problem that emphasizes the non-stationarity of the underlying process. In particular, in Khetarpal et al.'s definition, an agent interacts with a POMDP in which each of the individual components of the POMDP--such as the state space or reward function--are allowed to vary with time. We note that, as the environment model we study (Definition 2.3) is a function of history, it can capture time-indexed non-stationarity. In this sense, the same generality proposed by Khetarpal et al. and Ring is embraced and retained by our definition, but we add further precision to what is meant by _continual learning_ by centering around a mathematical definition of continual learning agents (Definition 4.1).

Footnote 2: For other surveys, see the recent survey on continual robot learning by Lesort et al. [30], a survey on continual learning with neural networks by Parisi et al. [42], a survey on transfer learning in RL by Taylor and Stone [60], and a survey on continual image classification by Mai et al. [35].

### Properties of CRL

Our formalism is intended to be a jumping off point for new lines of thinking around agents and continual learning. We defer much of our analysis and proofs to the appendix, and here focus on highlighting necessary properties of CRL.

**Theorem 4.1**.: _Every instance of CRL \((e,v,\Lambda,\Lambda_{\text{B}})\) necessarily satisfies the following properties:_

1. _If_ \(\Lambda\neq\Lambda_{\text{B}}\cup\Lambda^{\star}\)_, then there exists a_ \(\Lambda^{\prime}_{\text{B}}\) _such that (1)_ \(\Lambda^{\prime}_{\text{B}}\vDash\Lambda\)_, and (_2_)_ \((e,v,\Lambda,\Lambda^{\prime}_{\text{B}})\) _is not an instance of CRL._
2. _No element of_ \(\Lambda_{\text{B}}\) _is optimal:_ \(\Lambda_{\text{B}}\cap\Lambda^{\star}=\emptyset\)_._
3. _If_ \(|\Lambda|\) _is finite, there exists an agent set,_ \(\Lambda^{\circ}\)_, such that_ \(|\Lambda^{\circ}|<|\Lambda|\) _and_ \(\Lambda^{\circ}\vDash\Lambda\)_._
4. _If_ \(|\Lambda|\) _is infinite, there exists an agent set,_ \(\Lambda^{\circ}\)_, such that_ \(\Lambda^{\circ}\subset\Lambda\) _and_ \(\Lambda^{\circ}\vDash\Lambda\)_._

This theorem tells us several things. The first point of the theorem has peculiar implications. We see that as we change a single element (the basis \(\Lambda_{\text{B}}\)) of the tuple \((e,v,\Lambda_{\text{B}},\Lambda)\), the resulting problem can change from CRL to not CRL. By similar reasoning, an agent that is said to be a _continual learning agent_ according to Definition4.1 may not be a continual learner with respect to some other basis. We discuss this point further in the next paragraph. Point (2.) notes that no optimal strategy exists within the basis--instead, to be optimal, an agent must switch between basis elements indefinitely. As discussed previously, this fact encourages a departure in how we think about the RL problem: rather than focus on agents that can identify a single, fixed solution to a problem, CRL instead emphasizes designing agents that are effective at updating their behavior indefinitely. Points (3.) and (4.) show that \(\Lambda\) cannot be _minimal_. That is, there are necessarily some redundancies in the design space of the agents in CRL--this is expected, since we are always focusing on agents that search over the same agent basis. Lastly, it is worth calling attention to the fact that in the definition of CRL, we assume \(\Lambda_{\text{B}}\subset\Lambda\)--this suggests that in CRL, the agent basis is necessarily limited in some way. Consequently, the design space of agents \(\Lambda\) are _also_ limited in terms of what agents they can represent at any particular point in time. This limitation may come about due to a computational or memory budget, or by making use of a constrained set of learning rules. This suggests a deep connection between _bounded_ agents and the nature of continual learning, as explored further by Kumar et al. [27]. While these four points give an initial character of the CRL problem, we note that further exploration of the properties of CRL is an important direction for future work.

Canonical Agent Bases.It is worth pausing and reflecting on the concept of an agent basis. As presented, the basis is an arbitrary choice of a set of agents--consequently, point (1.) of Theorem4.1 may stand out as peculiar. From this perspective, it is reasonable to ask if the fact that our definition of CRL is basis-dependant renders it vacuous. We argue that this is not the case for two reasons. First, we conjecture that _any_ definition of continual learning that involves concepts like "learning" and "convergence" will have to sit on top of some reference object whose choice is arbitrary. Second, and more important, even though the mathematical construction allows for an easy change of basis, in practice the choice of basis is constrained by considerations like the availability of computational resources. It is often the case that the domain or problem of interest provides obvious choices of bases, or imposes constraints that force us as designers to restrict attention to a space of plausible bases or learning rules. For example, as discussed earlier, a choice of neural network architecture might comprise a basis--any assignment of weights is an element of the basis, and the learning rule \(\sigma\) is a mechanism for updating the active element of the basis (the parameters) in light of experience. In this case, the number of parameters of the network is constrained by what we can actually build, and the learning rule needs to be suitably efficient and well-behaved. We might again think of the learning rule \(\sigma\) as gradient descent, rather than a rule that can search through the basis in an unconstrained way. In this sense, the basis is not _arbitrary_. We as designers choose a class of functions to act as the relevant representations of behavior, often limited by resource constraints on memory or compute. Then, we use specific learning rules that have been carefully designed to react to experience in a desirable way--for instance, stochastic gradient descent updates the current choice of basis in the direction that would most improve performance. For these reasons, the choice of basis is not arbitrary, but instead reflects the ingredients involved in the design of agents as well as the constraints necessarily imposed by the environment.

### Properties of Generates and Reaches

Lastly, we summarize some of the basic properties of generates and reaches. Further analysis of generates, reaches, and their variations is provided in AppendixC.

**Theorem 4.2**.: _The following properties hold of the generates operator:_1. _Generates is transitive: For any triple_ \((\Lambda^{1},\Lambda^{2},\Lambda^{3})\) _and_ \(e\in\mathcal{E}\)_, if_ \(\Lambda^{1}\vDash\Lambda^{2}\) _and_ \(\Lambda^{2}\vDash\Lambda^{3}\)_, then_ \(\Lambda^{1}\vDash\Lambda^{3}\)_._
2. _Generates is not commutative: there exists a pair_ \((\Lambda^{1},\Lambda^{2})\) _and_ \(e\in\mathcal{E}\) _such that_ \(\Lambda^{1}\vDash\Lambda^{2}\)_, but_ \(\neg(\Lambda^{2}\vDash\Lambda^{1})\)_._
3. _For all_ \(\Lambda\) _and pair of agent bases_ \((\Lambda^{1}_{\text{B}},\Lambda^{2}_{\text{B}})\) _such that_ \(\Lambda^{1}_{\text{B}}\subseteq\Lambda^{2}_{\text{B}}\)_, if_ \(\Lambda^{1}_{\text{B}}\vDash\Lambda\)_, then_ \(\Lambda^{2}_{\text{B}}\vDash\Lambda\)_._
4. _For all_ \(\Lambda\) _and_ \(e\in\mathcal{E}\)_,_ \(\Lambda\vDash\Lambda\)_._
5. _The decision problem,_ _Given__\((e,\Lambda_{\text{B}},\Lambda)\)__,_ _output_ _True_ _iff_ \(\Lambda_{\text{B}}\vDash\Lambda\)_, is undecidable._

The fact that generates is transitive suggests that the basic tools of an agent set--paired with a set of learning rules--might be likened to an algebraic structure. We can draw a symmetry between an agent basis and the basis of a vector space: A vector space is comprised of all linear combinations of the basis, whereas \(\Lambda\) is comprised of all valid switches (according to the learning rules) between the base agents. However, the fact that generates is not commutative (by point 2.) raises a natural question: are there choices of learning rules under which generates is commutative? We suggest that a useful direction for future work can further explore an algebraic perspective on agents.

We find many similar properties hold of reaches.

**Theorem 4.3**.: _The following properties hold of the reaches operator:_

1. \(\rightsquigarrow\) _and_ \(\rightsquigarrow\) _are not transitive._
2. _"Sometimes reaches" is not commutative: there exists a pair_ \((\Lambda^{1},\Lambda^{2})\) _and_ \(e\in\mathcal{E}\) _such that_ \(\forall_{\lambda^{1}\in\Lambda^{1}}\ \ \lambda^{1}\rightsquigarrow\Lambda^{2}\)_, but_ \(\exists_{\lambda^{2}\in\Lambda^{2}}\ \ \lambda^{2}\rightsquigarrow\Lambda^{1}\)_._
3. _For all pairs_ \((\Lambda,e)\)_, if_ \(\lambda\in\Lambda\)_, then_ \(\lambda\rightsquigarrow\Lambda\)_._
4. _Every agent satisfies_ \(\lambda\rightsquigarrow\) _in every environment._
5. _The decision problem,_ _Given__\((e,\lambda,\Lambda)\)__,_ _output_ _True_ _iff_ \(\lambda\rightsquigarrow\Lambda\)_, is undecidable._

Many of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding whether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate that the majority of decision problems related to determining properties of arbitrary agent sets interacting with unconstrained environments will be undecidable, though it is still worth making these arguments carefully. Moreover, there may be interesting special cases in which these decision problems are decidable (and perhaps, efficiently so). We suggest that identifying these special cases and fleshing out their corresponding efficient algorithms is an interesting direction for future work.

## 5 Discussion

In this paper, we carefully develop a simple mathematical definition of the continual RL problem. We take this problem to be of central importance to AI as a field, and hope that these tools and perspectives can serve as an opportunity to think about CRL and its related artifacts more carefully. Our proposal is framed around two new insights about agents: (i) every agent can be understood as though it were searching over an agent basis (Theorem 3.1), and (ii) every agent, in the limit, will either sometimes or never stop this search (Remark 3.2). These two insights are formalized through the generates and reaches operators, which provide a rich toolkit for understanding agents in a new way--for example, we find straightforward definitions of a continual learning agent (Definition 4.1) and learning rules (Definition 3.2). We anticipate that further study of these operators and different families of learning rules can directly inform the design of new learning algorithms; for instance, we might characterize the family of _continual_ learning rules that are guaranteed to yield continual learning agents, and use this to guide the design of principled continual learning agents (in the spirit of continual backprop by Dohare et al. [14]). In future work, we intend to further explore connections between our formalism of continual learning and some of the phenomena at the heart of recent empirical continual learning studies, such as plasticity loss [34, 1, 15], in-context learning [8], and catastrophic forgetting [38, 18, 21, 26]. More generally, we hope that our definitions, analysis, and perspectives can help the community to think about continual reinforcement learning in a new light.

## Acknowledgements

The authors are grateful to Michael Bowling, Clare Lyle, Razvan Pascanu, and Georgios Piliouras for comments on a draft of the paper, as well as the anonymous NeurIPS reviewers that provided valuable feedback on the paper. The authors would further like to thank all of the 2023 Barbados RL Workshop participants and Elliot Catt, Will Dabney, Sebastian Flennerhag, Andras Gyorgy, Steven Hansen, Anna Harutyunyan, Mark Ho, Joe Marino, Joseph Modayil, Remi Munos, Evgenii Nikishin, Brendan O'Donoghue, Matt Overlan, Mark Rowland, Tom Schaul, Yannick Shroecker, Rich Sutton, Yunhao Tang, Shantanu Thakoor, and Zheng Wen for inspirational conversations.

## References

* [1] Zaheer Abbas, Rosie Zhao, Joseph Modayil, Adam White, and Marlos C Machado. Loss of plasticity in continual deep reinforcement learning. _arXiv preprint arXiv:2303.07507_, 2023.
* [2] David Abel, Yuu Jinnai, Yue Guo, George Konidaris, and Michael L. Littman. Policy and value transfer in lifelong reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, 2018.
* [3] Haitham Bou Ammar, Rasul Tutunov, and Eric Eaton. Safe policy search for lifelong reinforcement learning with sublinear regret. In _Proceedings of the International Conference on Machine Learning_, 2015.
* [4] Megan M Baker, Alexander New, Mario Aguilar-Simon, Ziad Al-Halah, Sebastien MR Arnold, Ese Ben-Iwhiwhu, Andrew P Brna, Ethan Brooks, Ryan C Brown, Zachary Daniels, et al. A domain-agnostic approach for characterization of lifelong learning systems. _Neural Networks_, 160:274-296, 2023.
* [5] Peter L Bartlett. Learning with a slowly changing distribution. In _Proceedings of the Annual Workshop on Computational Learning Theory_, 1992.
* [6] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary rewards. _Advances in Neural Information Processing Systems_, 2014.
* [7] Michael Bowling, John D. Martin, David Abel, and Will Dabney. Settling the reward hypothesis. In _Proceedings of the International Conference on Machine Learning_, 2023.
* [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in Neural Information Processing Systems_, 2020.
* [9] Emma Brunskill and Lihong Li. PAC-inspired option discovery in lifelong reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, 2014.
* [10] Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom Mitchell. Toward an architecture for never-ending language learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2010.
* [11] Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. Acting optimally in partially observable stochastic domains. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 1994.
* [12] Michael K Cohen, Elliot Catt, and Marcus Hutter. A strongly asymptotically optimal agent in general environments. _arXiv preprint arXiv:1903.01021_, 2019.
* [13] Travis Dick, Andras Gyorgy, and Csaba Szepesvari. Online learning in Markov decision processes with changing cost sequences. In _Proceedings of the International Conference on Machine Learning_, 2014.
* [14] Shibhansh Dohare, Richard S Sutton, and A Rupam Mahmood. Continual backprop: Stochastic gradient descent with persistent randomness. _arXiv preprint arXiv:2108.06325_, 2021.

* [15] Shibhansh Dohare, Juan Hernandez-Garcia, Parash Rahman, Richard Sutton, and A Rupam Mahmood. Loss of plasticity in deep continual learning. _arXiv preprint arXiv:2306.13812_, 2023.
* [16] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Simple agent, complex environment: Efficient reinforcement learning with agent states. _Journal of Machine Learning Research_, 23(255):1-54, 2022.
* [17] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _Proceedings of the International Conference on Machine Learning_, 2017.
* [18] Robert M French. Catastrophic forgetting in connectionist networks. _Trends in cognitive sciences_, 3(4):128-135, 1999.
* [19] Milton Friedman. _Essays in positive economics_. University of Chicago press, 1953.
* [20] Haotian Fu, Shangqun Yu, Michael Littman, and George Konidaris. Model-based lifelong reinforcement learning with Bayesian exploration. _Advances in Neural Information Processing Systems_, 2022.
* [21] Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investigation of catastrophic forgetting in gradient-based neural networks. _arXiv preprint arXiv:1312.6211_, 2013.
* [22] Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. _Trends in cognitive sciences_, 24(12):1028-1040, 2020.
* [23] Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity. _arXiv preprint cs/0004001_, 2000.
* [24] Marcus Hutter. _Universal artificial intelligence: Sequential decisions based on algorithmic probability_. Springer Science & Business Media, 2004.
* [25] Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement learning: A review and perspectives. _Journal of Artificial Intelligence Research_, 75:1401-1476, 2022.
* [26] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114(13):3521-3526, 2017.
* [27] Saurabh Kumar, Henrik Marklund, Ashish Rao, Yifan Zhu, Hong Jun Jeon, Yueyang Liu, and Benjamin Van Roy. Continual learning as computationally constrained reinforcement learning. _arXiv preprint arXiv:2307.04345_, 2023.
* [28] Tor Lattimore. _Theory of general reinforcement learning_. PhD thesis, The Australian National University, 2014.
* [29] Jan Leike. _Nonparametric general reinforcement learning_. PhD thesis, The Australian National University, 2016.
* [30] Timothee Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, and Natalia Diaz-Rodriguez. Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges. _Information fusion_, 58:52-68, 2020.
* [31] Yueyang Liu, Benjamin Van Roy, and Kuang Xu. A definition of non-stationary bandits. _arXiv preprint arXiv:2302.12202_, 2023.
* [32] Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi, Ian Osband, and Zheng Wen. Reinforcement learning, bit by bit. _Foundations and Trends in Machine Learning_, 16(6):733-865, 2023. ISSN 1935-8237.

* [33] Jelena Luketina, Sebastian Flennerhag, Yannick Schroecker, David Abel, Tom Zahavy, and Satinder Singh. Meta-gradients in non-stationary environments. In _Proceedings of the Conference on Lifelong Learning Agents_, 2022.
* [34] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Understanding plasticity in neural networks. In _Proceedings of the International Conference on Machine Learning_, 2023.
* [35] Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, and Scott Sanner. Online continual learning in image classification: An empirical survey. _Neurocomputing_, 469:28-51, 2022.
* [36] Sultan J Majeed. _Abstractions of general reinforcement Learning_. PhD thesis, The Australian National University, 2021.
* [37] Sultan Javed Majeed and Marcus Hutter. Performance guarantees for homomorphisms beyond Markov decision processes. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019.
* [38] Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of learning and motivation_, volume 24, pages 109-165. Elsevier, 1989.
* [39] Tom Mitchell, William Cohen, Estevam Hruschka, Partha Talukdar, Bishan Yang, Justin Betteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, Bryan Kisiel, et al. Never-ending learning. _Communications of the ACM_, 61(5):103-115, 2018.
* [40] Claire Monteleoni and Tommi Jaakkola. Online learning of non-stationary sequences. _Advances in Neural Information Processing Systems_, 16, 2003.
* [41] Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning. 2018.
* [42] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. _Neural networks_, 113:54-71, 2019.
* [43] Emmanouil Antonios Platanios, Abulhair Saparov, and Tom Mitchell. Jelly bean world: A testbed for never-ending learning. _arXiv preprint arXiv:2002.06306_, 2020.
* [44] Martin L Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. John Wiley & Sons, 2014.
* [45] Matthew Riemer, Sharath Chandra Raparthy, Ignacio Cases, Gopeshh Subbaraj, Maximilian Puelma Touzel, and Irina Rish. Continual learning in environments with polynomial mixing times. _Advances in Neural Information Processing Systems_, 2022.
* [46] Mark B Ring. _Continual learning in reinforcement environments_. PhD thesis, The University of Texas at Austin, 1994.
* [47] Mark B Ring. Child: A first step towards continual learning. _Machine Learning_, 28(1):77-104, 1997.
* [48] Mark B Ring. Toward a formal framework for continual learning. In _NeurIPS Workshop on Inductive Transfer_, 2005.
* [49] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory Wayne. Experience replay for continual learning. _Advances in Neural Information Processing Systems_, 2019.
* [50] Stuart J Russell and Devika Subramanian. Provably bounded-optimal agents. _Journal of Artificial Intelligence Research_, 2:575-609, 1994.
* [51] Paul Ruvolo and Eric Eaton. ELLA: An efficient lifelong learning algorithm. In _Proceedings of the International Conference on Machine Learning_, 2013.

* [52] Tom Schaul and Jurgen Schmidhuber. Metalearning. _Scholarpedia_, 5(6):4650, 2010.
* [53] Jurgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph. Reinforcement learning with self-modifying policies. In _Learning to Learn_, pages 293-309. Springer, 1998.
* [54] Jonathan Schwarz, Wojciech Czarnecki, Jelena Luketina, Agnieszka Grabska-Barwinska, Yee Whye Teh, Razvan Pascanu, and Raia Hadsell. Progress & compress: A scalable framework for continual learning. In _Proceedings of the International Conference on Machine Learning_, 2018.
* [55] Daniel L Silver. Machine lifelong learning: Challenges and benefits for artificial general intelligence. In _Proceedings of the Conference on Artificial General Intelligence_, 2011.
* [56] Richard S Sutton. Introduction: The challenge of reinforcement learning. In _Reinforcement Learning_, pages 1-3. Springer, 1992.
* [57] Richard S Sutton. The reward hypothesis, 2004. URL http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html.
* [58] Richard S Sutton. The quest for a common model of the intelligent decision maker. _arXiv preprint arXiv:2202.13252_, 2022.
* [59] Richard S Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2018.
* [60] Matthew E. Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. _Journal of Machine Learning Research_, 10(Jul):1633-1685, 2009.
* [61] Sebastian Thrun. Is learning the n-th thing any easier than learning the first? _Advances in Neural Information Processing Systems_, 1995.
* [62] Sebastian Thrun. Lifelong learning algorithms. _Learning to Learn_, 8:181-209, 1998.
* [63] Sebastian Thrun and Tom M Mitchell. Lifelong robot learning. _Robotics and autonomous systems_, 15(1-2):25-46, 1995.
* [64] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical Bayesian approach. In _Proceedings of the International Conference on Machine learning_, 2007.

## Appendix A Notation

We first provide a table summarizing all relevant notation.

## Appendix B Proofs of Presented Results

We next provide proofs of each result from the paper. Our proofs make use of some extra notation: we use as logical implication, and we use to denote the power set of any set \(\mathcal{X}\). Lastly, we use \(\forall_{\mathcal{A}\subseteq\mathcal{X}}\) and as shorthand for \(\forall_{\mathcal{A}\in\mathcal{P}(\mathcal{X})}\) and \(\exists_{\mathcal{A}\in\mathcal{P}(\mathcal{X})}\) respectively.

\begin{table}
\begin{tabular}{l l l} \hline \hline _Notation_ & _Meaning_ & _Definition_ \\ \hline \(\mathcal{A}\) & Actions & \\ \(\mathcal{O}\) & Observations & \\ \(\mathcal{H}_{l}\) & Length \(t\) histories & \(\mathcal{H}_{l}=(\mathcal{A}\times\mathcal{O})^{t}\) \\ \(\mathcal{H}\) & All histories & \(\mathcal{H}=\bigcup_{t=0}^{\infty}\mathcal{H}_{l}\) \\ \(h\) & A history & \(h\in\mathcal{H}\) \\ \(hh^{\prime}\) & History concatenation & \\ \(h_{l}\) & Length \(t\) history & \(h_{l}\in\mathcal{H}_{l}\) \\ \(\bar{\mathcal{H}}=\mathcal{H}^{\lambda,e}\) & Realizable histories & \(\bar{\mathcal{H}}=\bigcup_{t=0}^{\infty}\left\{h_{l}\in\mathcal{H}_{l}: \prod_{k=0}^{l-1}e(o_{k}\mid h_{k},a_{k})\lambda(a_{k}\mid h_{k})>0\right\}\) \\ \(\bar{\mathcal{H}}_{h}=\mathcal{H}_{h}^{\lambda,e}\) & Realizable history suffixes & \(\bar{\mathcal{H}}_{h}=\{h^{\prime}\in\mathcal{H}:hh^{\prime}\in\mathcal{H}^{ \lambda,e}\}\) \\ \(e\) & Environment & \(e:\mathcal{H}\times\mathcal{A}\rightarrow\Delta(\mathcal{A})\) \\ \(\mathcal{E}\) & Set of all environments & \\ \(\lambda\) & Agent & \(\lambda:\mathcal{H}\rightarrow\Delta(\mathcal{A})\) \\ \(\mathbb{A}\) & Set of all agents & \\ \(\Lambda\) & Set of agents & \(\Lambda\subseteq\mathbb{A}\) \\ \(\Lambda_{\text{B}}\) & Agent basis & \(\Lambda_{\text{B}}\subset\mathbb{A}\) \\ \(r\) & Reward function & \(r:\mathcal{A}\times\mathcal{O}\rightarrow\mathbb{R}\) \\ \(v\) & Performance & \(v:\mathcal{H}\times\mathbb{A}\times\mathcal{E}\rightarrow[\text{v}_{\text{ min}},\text{v}_{\text{max}}]\) \\ \(\sigma\) & Learning rule & \(\sigma:\mathcal{H}\rightarrow\Lambda_{\text{B}}\) \\ \(\bar{\Sigma}\) & Set of all learning rules & \\ \(\Sigma\) & Set of learning rules & \(\Sigma\subseteq\Sigma\) \\ \(\Lambda_{\text{B}}\not\in_{\Sigma}\Lambda\) & \(\Sigma\)-generates & \(\forall_{\Lambda}\exists_{o\in\Sigma}\forall_{h\in\bar{\mathcal{H}}}\ \lambda(h)=\sigma(h)(h)\) \\ \(\Lambda_{\text{B}}\not\in\Lambda\) & Generates & \(\exists_{\mathcal{C}\subseteq\Sigma}\ \Lambda_{\text{B}}\not\in_{\Sigma}\Lambda\) \\ \(\Lambda_{\text{B}}\models_{\Sigma}\Lambda\) & Universally \(\Sigma\)-generates & \(\forall_{\Lambda}\exists_{o\in\Sigma}\forall_{h\in\mathcal{H}}\ \lambda(h)=\sigma(h)(h)\) \\ \(\Lambda_{\text{B}}\models\Lambda\) & Universally generates & \(\exists_{\Sigma\subseteq\Sigma}\ \Lambda_{\text{B}}\models_{\Sigma}\Lambda\) \\ \(\lambda\not\hookrightarrow\Lambda_{\text{B}}\) & Sometimes reaches & \(\exists_{h\in\bar{\mathcal{H}}}\exists_{\lambda_{\text{B}}\in\Lambda_{\text{B}}} \forall_{h^{\prime}\in\bar{\mathcal{H}}_{h}}\ \lambda(hh^{\prime})=\lambda_{\text{B}}(hh^{\prime})\) \\ \(\lambda\not\hookrightarrow\Lambda_{\text{B}}\) & Never reaches & \(\neg(\lambda\hookrightarrow\lambda_{\text{B}})\) \\ \(\lambda\not\hookrightarrow\Lambda_{\text{B}}\) & Always reaches & \(\forall_{h\in\bar{\mathcal{H}}}\exists_{t\in\mathbb{N}_{0}}\forall_{h^{\prime} \in\bar{\mathcal{H}}_{h}^{\text{\tiny$e$}}\bar{\mathcal{H}}_{h}^{\text{\tiny$e$}} \bar{\mathcal{H}}_{h}^{\text{\tiny$e$}}\bar{\mathcal{H}}_{h}^{\text{\tiny$e$}} \bar{\mathcal{H}}_{h}^{\text{\tiny$e$}}\bar{\mathcal{H}}_{h}^{\text{\tiny$e$}} \exists_{\lambda_{\text{B}}}\in\Lambda_{\text{B}}}\forall_{h^{\prime}\in\bar{ \mathcal{H}}_{h}}\ \lambda(hh^{\text{\tiny$e$}}h^{\prime})=\lambda_{\text{B}}(hh^{\text{\tiny$e$}}h^{ \text{\tiny$e$}})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: A summary of notation.

### Section 3 Proofs

Our first result is from Section 3 of the paper.

**Theorem 3.1**.: _For any pair \((\lambda,e)\), there exists infinitely many choices of a basis, \(\Lambda_{\mathrm{B}}\), such that both (1) \(\lambda\notin\Lambda_{\mathrm{B}}\), and (2) \(\Lambda_{\mathrm{B}}\neq\{\lambda\}\)._

Proof of Theorem 3.1.: Choose a fixed but arbitrary pair \((\lambda,e)\). Then, enumerate the realizable histories, \(\mathcal{H}^{\lambda,e}\), and let \(h^{1}\) denote the first element of this enumeration, \(h^{2}\) the second, and so on. Then, we design a constructive procedure for a basis that, when repeatedly applied, induces an infinite enumeration of bases that satisfy the desired two properties. This constructive procedure for the \(k\)-th basis will contain \(k+1\) agents, where each agent is distinct from \(\lambda\), but will produces the same action as the agent every \(k+1\) elements of the history sequence, \(h^{1},h^{2},\ldots\).

For the first (\(k=1\)) basis, we construct two agents. The first, \(\lambda_{\mathrm{B}}^{1}\), chooses the same action distribution as \(\lambda\) on each even numbered history: \(\lambda_{\mathrm{B}}^{1}(h^{i})=\lambda(h^{i})\). Then, this agent will choose a different action distribution on the odd length histories: \(\lambda_{\mathrm{B}}^{1}(h^{i+1})\neq\lambda(h^{i+1})\), for \(i\) any even natural number. The second agent, \(\lambda_{\mathrm{B}}^{2}\) will do the opposite to \(\lambda_{\mathrm{B}}^{1}\): on each odd numbered history \(h^{i+1}\), \(\lambda_{\mathrm{B}}^{2}(h^{i+1})\neq\lambda(h^{i+1})\), but on every even numbered history, \(\lambda_{\mathrm{B}}^{2}(h^{i})=\lambda(h^{i})\).

Observe first that by construction, \(\lambda\neq\lambda_{\mathrm{B}}^{1}\), and \(\lambda\neq\lambda_{\mathrm{B}}^{2}\), since there exist histories where they choose different action distributions. Next, observe that the basis, \(\Lambda_{\mathrm{B}}=\{\lambda_{\mathrm{B}}^{1},\lambda_{\mathrm{B}}^{2}\}\), generates \(\{\lambda\}\) in \(e\) through the following set of learning rules, \(\Sigma\): given any realizable history, \(h\in\mathcal{H}^{\lambda,e}\), check whether the history has an even or odd numbered index in the enumeration. If odd, choose \(\lambda_{\mathrm{B}}^{1}\), and if even, choose \(\lambda_{\mathrm{B}}^{2}\).

More generally, this procedure can be applied for any \(k\):

\[\Lambda_{\mathrm{B}}^{k}=\{\lambda_{\mathrm{B}}^{1},\ldots,\lambda_{\mathrm{B }}^{k+1}\},\qquad\lambda_{\mathrm{B}}^{i}(h)=\begin{cases}\lambda(h)&[h]==i,\\ \neq\lambda(h)&\text{otherwise},\end{cases}\] (B.1)

where we use the notation \([h]==i\) to express the logical predicate asserting that the modulos of the index of \(h\) in the enumeration \(h^{1},h^{2},\ldots\) is equal to \(i\).

Further, \(\neq\lambda(h)\) simply refers to _any_ choice of action distribution that is unequal to \(\lambda(h)\). Thus, for all natural numbers \(k\geq 2\), we can construct a new basis consisting of \(k\) base agents that generates \(\lambda\) in \(e\), but does not contain the agent itself. This completes the argument. 

### Section 4 Proofs

We next present the proofs of results from Section 4.

#### b.2.1 Theorem 4.1: Properties of CRL

We begin with Theorem 4.1 that establishes basic properties of CRL.

**Theorem 4.1**.: _Every instance of CRL \((e,v,\Lambda,\Lambda_{\mathrm{B}})\) satisfies the following properties:_

1. _If_ \(\Lambda\neq\Lambda_{\mathrm{B}}\cup\Lambda^{*}\)_, there exists a_ \(\Lambda_{\mathrm{B}}^{\prime}\) _such that (1)_ \(\Lambda_{\mathrm{B}}^{\prime}\vDash\Lambda\)_, and (2)_ \((e,v,\Lambda,\Lambda_{\mathrm{B}}^{\prime})\) _is not an instance of CRL._
2. _No element of_ \(\Lambda_{\mathrm{B}}\) _is optimal:_ \(\Lambda_{\mathrm{B}}\cap\Lambda^{*}=\emptyset\)_._
3. _If_ \(|\Lambda|\) _is finite, there exists an agent set,_ \(\Lambda^{\circ}\) _such that_ \(|\Lambda^{\circ}|<|\Lambda|\) _and_ \(\Lambda^{\circ}\vDash\Lambda\)_._
4. _If_ \(|\Lambda|\) _is infinite, there exists an agent set,_ \(\Lambda^{\circ}\) _such that_ \(\Lambda^{\circ}\subset\Lambda\) _and_ \(\Lambda^{\circ}\vDash\Lambda\)_._We prove this result in the form of three lemmas, corresponding to each of the four points of the theorem (with the third lemma, Lemma B.3, covering both points 3. and 4.). Some of the lemmas make use of properties of generates and reaches that we establish later in Appendix C.

**Lemma B.1**.: _For all instances of CRL \((e,v,\Lambda,\Lambda_{\mathrm{B}})\), if \(\Lambda\neq\Lambda_{\mathrm{B}}\cup\Lambda^{\star}\), then there exists a choice \(\Lambda^{\prime}_{\mathrm{B}}\) such that (1) \(\Lambda^{\prime}_{\mathrm{B}}\neq\Lambda\), and (2) \((e,v,\Lambda,\Lambda^{\prime}_{\mathrm{B}})\) is not an instance of CRL._

Proof of Lemma b.1.: Recall that a tuple \((e,v,\Lambda,\Lambda_{\mathrm{B}})\) is CRL just when all of the optimal agents \(\Lambda^{\star}\) do not reach the basis. Then, the result holds as a straightforward consequence of two facts. First, we can always construct a new basis containing all of the optimal agents, \(\Lambda^{\circ}_{\mathrm{B}}=\Lambda_{\mathrm{B}}\cup\Lambda^{\star}\). Notice that \(\Lambda^{\circ}_{\mathrm{B}}\) still generates \(\Lambda\) by property three of Theorem 4.2. Further, since both \(\Lambda_{\mathrm{B}}\) and \(\Lambda^{\star}\) are each subsets of \(\Lambda\), and by assumption \(\Lambda\neq\Lambda_{\mathrm{B}}\cup\Lambda^{\star}\) (so there is at least one sub-optimal agent that is not in the basis), it follows that \(\Lambda^{\circ}_{\mathrm{B}}\subset\Lambda\). Second, by Proposition C.15, we know that every element \(\lambda^{\circ}_{\mathrm{B}}\in\Lambda^{\circ}_{\mathrm{B}}\) will always reach the basis, \(\lambda^{\circ}_{\mathrm{B}}\sqcap^{\diamond}\Lambda^{\circ}_{\mathrm{B}}\). Therefore, in the tuple \((e,v,\Lambda,\Lambda^{\circ}_{\mathrm{B}})\), each of the optimal agents will reach the basis, and therefore this is not an instance of CRL. 

**Lemma B.2**.: _No element of \(\Lambda_{\mathrm{B}}\) is optimal: \(\Lambda_{\mathrm{B}}\cap\Lambda^{\star}=\emptyset\)._

Proof of Lemma b.2.: The lemma follows as a combination of two facts.

First, recall that, by definition of CRL, each optimal agent \(\lambda\in\Lambda^{\star}\) satisfies \(\lambda^{\star}\not\to\Lambda_{\mathrm{B}}\).

Second, note that by Lemma B.11, we know that each \(\lambda_{\mathrm{B}}\in\Lambda_{\mathrm{B}}\) satisfies \(\lambda_{\mathrm{B}}\mathrel{\raisebox{-1.29pt}{\scalebox{0.8}{$\sim$}}} \Lambda_{\mathrm{B}}\).

Therefore, since sometimes reaches (\(\mathrel{\raisebox{-1.29pt}{\scalebox{0.8}{$\sim$}}}\)) and never reaches (\(\not\to\)) are negations of one another, we conclude that no basis element can be optimal. 

Before stating the next lemma, we note that points (3.) and (4.) of Theorem 4.1 are simply expansions of the definition of a _minimal_ agent set, which we define precisely in Definition C.4 and Definition C.5.

**Lemma B.3**.: _For any instance of CRL, \(\Lambda\) is not minimal._

Proof of Lemma b.3.: We first show that \(\Lambda\) cannot be minimal. To do so, we consider the cases where the rank (Definition C.3) of \(\Lambda\) is finite and infinite separately.

(Finite Rank \(\Lambda\).) If \(\mathrm{rank}(\Lambda)\) is finite and minimal, then it follows immediately that there is no agent set of smaller rank that generates \(\Lambda\). By consequence, since \(\Lambda_{\mathrm{B}}\subset\Lambda\) and \(\Lambda_{\mathrm{B}}\neq\Lambda\), we conclude that \(\Lambda\) cannot be minimal. \(\checkmark\)

(Infinite Rank \(\Lambda\).) If \(\mathrm{rank}(\Lambda)\) is infinite and minimal, then there is no proper subset of \(\Lambda\) that uniformly generates \(\Lambda\) by definition. By consequence, since \(\Lambda_{\mathrm{B}}\subset\Lambda\) and \(\Lambda_{\mathrm{B}}\neq\Lambda\), we conclude that \(\Lambda\) cannot be minimal. \(\checkmark\)

This completes the argument of both cases, and we conclude that for any instance of CRL, \(\Lambda\) is not minimal.

#### b.2.2 Theorem 4.2: Properties of Generates

Next, we prove basic properties of generates.

**Theorem 4.2**.: _The following properties hold of the generates operator:_

1. _Generates is transitive: For any triple_ \((\Lambda^{1},\Lambda^{2},\Lambda^{3})\) _and_ \(e\in\mathcal{E}\)_, if_ \(\Lambda^{1}\vDash\Lambda^{2}\) _and_ \(\Lambda^{2}\vDash\Lambda^{3}\)_, then_ \(\Lambda^{1}\vDash\Lambda^{3}\)_._
2. _Generates is not commutative: there exists a pair_ \((\Lambda^{1},\Lambda^{2})\) _and_ \(e\in\mathcal{E}\) _such that_ \(\Lambda^{1}\vDash\Lambda^{2}\)_, but_ \(\neg(\Lambda^{2}\vDash\Lambda^{1})\)_._
3. _For all_ \(\Lambda\) _and pair of agent bases_ \((\Lambda^{1}_{\mathbf{B}},\Lambda^{2}_{\mathbf{B}})\) _such that_ \(\Lambda^{1}_{\mathbf{B}}\subseteq\Lambda^{2}_{\mathbf{B}}\)_, if_ \(\Lambda^{1}_{\mathbf{B}}\vDash\Lambda\)_, then_ \(\Lambda^{2}_{\mathbf{B}}\vDash\Lambda\)_._
4. _For all_ \(\Lambda\) _and_ \(e\in\mathcal{E}\)_,_ \(\Lambda\vDash\Lambda\)_._
5. _The decision problem,_ _Given_ \((e,\Lambda_{\mathbf{B}},\Lambda)\)_,_ _output True iff_ \(\Lambda_{\mathbf{B}}\vDash\Lambda\)_, is undecidable._

The proof of this theorem is spread across the next five lemmas below.

The fact that generates is transitive suggests that the basic tools of an agent set--paired with a set of learning rules--might be likened to an algebraic structure. We can draw a symmetry between an agent basis and the basis of a vector space: A vector space is comprised of all linear combinations of the basis, whereas \(\Lambda\) is comprised of all valid switches (according to the learning rules) between the base agents. However, the fact that generates is not commutative (by point 2.) raises a natural question: are there choices of learning rules under which generates is commutative? An interesting direction for future work is to explore this style of algebraic analysis on agents.

**Lemma B.4**.: _Generates is transitive: For any triple \((\Lambda^{1},\Lambda^{2},\Lambda^{3})\) and \(e\in\mathcal{E}\), if \(\Lambda^{1}\vDash\Lambda^{2}\) and \(\Lambda^{2}\vDash\Lambda^{3}\), then \(\Lambda^{1}\vDash\Lambda^{3}\)._

_Proof of Lemma b.4._

Assume \(\Lambda^{1}\vDash\Lambda^{2}\) and \(\Lambda^{2}\vDash\Lambda^{3}\). Then, by Proposition C.4 and the definition of the generates operator, we know that

\[\forall_{\lambda^{2}\in\Lambda^{2}}\exists_{\sigma^{1}\in\mathbb{ E}^{1}}\forall_{h\in\mathcal{\bar{H}}}\ \lambda^{2}(h) =\sigma^{1}(h)(h),\] (B.2) \[\forall_{\lambda^{3}\in\Lambda^{3}}\exists_{\sigma^{2}\in \mathbb{E}^{2}}\forall_{h\in\mathcal{\bar{H}}}\ \lambda^{3}(h) =\sigma^{2}(h)(h),\] (B.3)

where \(\mathbb{E}^{1}\) and \(\mathbb{E}^{2}\) express the set of all learning rules over \(\Lambda^{1}\) and \(\Lambda^{2}\) respectively. By definition of a learning rule, \(\sigma\), we rewrite the above as follows,

\[\forall_{\lambda^{2}\in\Lambda^{2}}\forall_{h\in\mathcal{\bar{H} }}\exists_{\lambda^{1}\in\Lambda^{1}}\ \lambda^{2}(h) =\lambda^{1}(h),\] (B.4) \[\forall_{\lambda^{3}\in\Lambda^{3}}\forall_{h\in\mathcal{\bar{H} }}\exists_{\lambda^{2}\in\Lambda^{2}}\ \lambda^{3}(h) =\lambda^{2}(h).\] (B.5)

Then, consider a fixed but arbitrary \(\lambda^{3}\in\Lambda^{3}\). We construct a learning rule defined over \(\Lambda^{1}\) as \(\sigma^{1}:\mathcal{H}\to\Lambda^{1}\) that induces an equivalent agent as follows. For each realizable history, \(h\in\mathcal{\bar{H}}\), by Equation B.5 we know that there is an \(\lambda^{2}\) such that \(\lambda^{3}(h)=\lambda^{2}(h)\), and by Equation B.4, there is an \(\lambda^{1}\) such that \(\lambda^{2}(h)=\lambda^{1}(h)\). Then, set \(\sigma^{1}:h\mapsto\lambda^{1}\) such that \(\lambda^{1}(h)=\lambda^{2}(h)=\lambda^{3}(h)\)

Since \(h\) and \(\lambda^{3}\) were chosen arbitrarily, we conclude that

\[\forall_{\lambda^{3}\in\Lambda^{3}}\forall_{h\in\mathcal{\bar{H}}}\exists_{ \lambda^{1}\in\Lambda^{1}}\ \lambda^{3}(h)=\lambda^{1}(h).\]

But, by the definition of \(\Sigma\), this means there exists a learning rule such that

\[\forall_{\lambda^{3}\in\Lambda^{3}}\exists_{\sigma^{1}\in\mathbb{ E}^{1}}\forall_{h\in\mathcal{\bar{H}}}\ \lambda^{3}(h) =\sigma^{1}(h)(h).\]

This is exactly the definition of \(\Sigma\)-generation, and by Proposition C.4, we conclude \(\Lambda^{1}\vDash\Lambda^{3}\).

**Lemma B.5**.: _Generates is not commutative: there exists a pair \((\Lambda^{1},\Lambda^{2})\) and \(e\in\mathcal{E}\) such that \(\Lambda^{1}\upharpoonright\Lambda^{2}\), but \(\neg(\Lambda^{2}\upharpoonright\Lambda^{1})\)._

Proof of Lemma b.5.: The result follows from a simple counterexample: consider the pair

\[\Lambda^{1}=\{\lambda_{i}:h\mapsto a_{1}\},\qquad\Lambda^{2}=\{\lambda_{i}:h \mapsto a_{1},\lambda_{j}:h\mapsto a_{2}\}.\]

Note that since \(\lambda_{i}\) is in both sets, and \(\Lambda^{1}\) is a singleton, we know that \(\Lambda^{2}\upharpoonright\Lambda^{1}\) in any environment. But, by Proposition C.6, we know that \(\Lambda^{1}\) cannot generate \(\Lambda^{2}\). 

**Lemma B.6**.: _For all \(\Lambda\) and pair of agent bases \((\Lambda^{1}_{\mathsf{B}},\Lambda^{2}_{\mathsf{B}})\) such that \(\Lambda^{1}_{\mathsf{B}}\subseteq\Lambda^{2}_{\mathsf{B}}\), if \(\Lambda^{1}_{\mathsf{B}}\upharpoonright\Lambda\), then \(\Lambda^{2}_{\mathsf{B}}\upharpoonright\Lambda\)._

Proof of Lemma b.6.: The result follows as a natural consequence of the definition of generates. Recall that \(\Lambda^{1}_{\mathsf{B}}\upharpoonright\Lambda\) just when,

\[\exists_{\Sigma^{1}\subseteq\Sigma}\ \Lambda^{1}_{\mathsf{B}} \upharpoonright_{\Sigma^{1}}\ \Lambda\] (B.6) \[\equiv\exists_{\Sigma^{1}\subseteq\Sigma}\ \exists_{\partial^{1}\in\Sigma^{1}}\forall_{h\in\widetilde{\mathcal{H}}}\ \lambda(h)=\lambda^{\partial^{1}(h)}_{\mathsf{B}}(h),\] (B.7)

where again \(\lambda^{\partial^{1}(h)}_{\mathsf{B}}\in\Lambda^{1}_{\mathsf{B}}\) is the base agent chosen by \(\sigma^{1}(h)\). We use superscripts \(\Sigma^{1}\) and \(\sigma^{1}\) to signify that \(\sigma^{1}\) is defined relative to \(\Lambda^{1}_{\mathsf{B}}\), that is, \(\sigma^{1}:\mathcal{H}\rightarrow\Lambda^{1}_{\mathsf{B}}\in\Sigma^{1}\).

But, since \(\Lambda^{1}_{\mathsf{B}}\subseteq\Lambda^{2}_{\mathsf{B}}\), we can define \(\Sigma^{2}=\Sigma^{1}\) and ensure that \(\Lambda^{2}_{\mathsf{B}}\upharpoonright_{\Sigma^{2}}\Lambda\), since the agent basis \(\Lambda^{1}_{\mathsf{B}}\) was already sufficient to generate \(\Lambda\). Therefore, we conclude that \(\Lambda^{2}_{\mathsf{B}}\upharpoonright\Lambda\). 

**Lemma B.7**.: _For all \(\Lambda\) and \(e\in\mathcal{E}\), \(\mathbb{A}\upharpoonright\Lambda\)._

Proof of Lemma b.7.: This is a direct consequence of Proposition C.18. 

**Lemma B.8**.: _The decision problem, AgentsGenerate, Given \((e,\Lambda_{\mathsf{B}},\Lambda)\), output True iff \(\Lambda_{\mathsf{B}}\upharpoonright\Lambda\), is undecidable._

Proof of Lemma b.8.: We proceed as is typical of such results by reducing AgentsGenerate from the Halting Problem.

In particular, let \(m\) be a fixed but arbitrary Turing Machine, and \(w\) be a fixed but arbitrary input to be given to machine \(m\). Then, Halt defines the decision problem that outputs True iff \(m\) halts on input \(w\).

We construct an oracle for AgentsGenerate that can decide Halt as follows. Let \((\mathcal{A},\mathcal{O})\) be an interface where the observation space is comprised of all configurations of machine \(m\). Then, we consider a deterministic environment \(e\) that simply produces the next configuration of \(m\) when run on input \(w\), based on the current tape contents, the state of \(m\), and the location of the tape head. Note that all three of these elements are contained in a Turing Machine's configuration, and that a single configuration indicates whether the Turing Machine is in a halting state or not. Now, let the action space \(\mathcal{A}\) consist of two actions, \(\{a_{\text{no-op}},a_{\text{halt}}\}\). On execution of \(a_{\text{no-op}}\) no-op, the environment moves to the next configuration. On execution of \(a_{\text{halt}}\), the machine halts. That is, we restrict ourselves to the singleton agent set, \(\Lambda\), containingthe agent \(\lambda^{\circ}\) that outputs \(a_{\text{halt}}\) directly following the machine entering a halting configuration, and \(a_{\text{no-op}}\) otherwise:

\[\lambda^{\circ}:\,ha\omega\mapsto\begin{cases}a_{\text{halt}}&o\text{ is a halting configuration,}\\ a_{\text{no-op}}&\text{otherwise.}\end{cases},\qquad\Lambda=\{\lambda^{\circ}\}.\]

Using these ingredients, we take any instance of Halt, \((m,w)\), and consider the singleton agent basis: \(\Lambda^{\Gamma}_{\text{B}}=\{a_{\text{no-op}}\}\).

We make one query to our AgentsGenerate oracle, and ask: \(\Lambda^{1}_{\text{B}}\vDash\Lambda\). If it is True, then the histories realizable by \((\lambda^{\circ},e)\) pair ensure that the single agent in \(\Lambda\) never emits the \(a_{\text{halt}}\) action, and thus, \(m\) does not halt on \(w\). If it is False, then there _are_ realizable histories in \(e\) in which \(m\) halts on \(w\). We thus use the oracle's response directly to decide the given instance of Halt. 

#### b.2.3 Theorem 4.3: Properties of Reaches

We find many similar properties hold for reaches.

**Theorem 4.3**.: _The following properties hold of the reaches operator:_

1. \(\rightsquigarrow\) _and_ \(\rightsquigarrow\) _are not transitive._
2. _Sometimes reaches is not commutative: there exists a pair_ \((\Lambda^{1},\Lambda^{2})\) _and_ \(e\in\mathcal{E}\) _such that_ \(\forall_{\lambda^{1}\in\Lambda^{1}}\ \lambda^{1}\rightsquigarrow\Lambda^{2}\)_, but_ \(\exists_{\lambda^{2}\in\Lambda^{2}}\ \lambda^{2}\rightsquigarrow\Lambda^{1}\)_._
3. _For all pairs_ \((\Lambda,e)\)_, if_ \(\lambda\in\Lambda\)_, then_ \(\lambda\rightsquigarrow\Lambda\)_._
4. _Every agent satisfies_ \(\lambda\rightsquigarrow\Lambda\) _in every environment._
5. _The decision problem,_ _Given_ \((e,\lambda,\Lambda)\)_,_ _output True iff_ \(\lambda\rightsquigarrow\Lambda\)_, is undecidable._

Again, we prove this result through five lemmas that correspond to each of the above properties.

Many of these properties resemble those in Theorem 4.2. For instance, point (5.) shows that deciding whether a given agent sometimes reaches a basis in an environment is undecidable. We anticipate that the majority of decision problems related to determining properties of arbitrary agent sets will be undecidable, though it is still worth making these arguments carefully. Moreover, there may be interesting special cases in which these decision problems are decidable (and perhaps, efficiently so). Identifying these special cases and their corresponding efficient algorithms is another interesting direction for future work.

**Lemma B.9**.: \(\rightsquigarrow\) _and_ \(\rightsquigarrow\) _are not transitive._

Proof of Lemma b.9.: We construct two counterexamples, one for each of "sometimes reaches" (\(\rightsquigarrow\)) and "never reaches" (\(\rightsquigarrow\)).

Counterexample: Sometimes Reaches.To do so, we begin with a tuple \((e,\Lambda^{1},\Lambda^{2},\Lambda^{3})\) such that both

\[\forall_{\Lambda^{1}\in\Lambda^{1}}\ \lambda^{1}\rightsquigarrow\Lambda^{1}, \qquad\forall_{\Lambda^{2}\in\Lambda^{2}}\ \lambda^{2}\rightsquigarrow\Lambda^{2}.\]

We will show that there is an agent, \(\overline{\lambda}^{1}\in\Lambda^{1}\), such that \(\overline{\lambda}^{1}\rightsquigarrow\Lambda^{3}\), thus illustrating that sometimes reaches is not guaranteed to be transitive. The basic idea is that sometimes reaches only requires an agent stop its search on _one_ realizable history. So, \(\lambda^{1}\rightsquigarrow\Lambda^{2}\) might happen on some history \(h\), but each \(\lambda^{2}\in\Lambda^{2}\) might only reach \(\Lambda^{3}\) on an entirely different history. As a result, reaching \(\Lambda^{2}\) is not enough to ensure the agent also reaches \(\Lambda^{3}\).

In more detail, the agent sets of the counterexample are as follows. Let \(\mathcal{A}=\{a_{1},a_{2}\}\) and \(\mathcal{O}=\{o_{1},o_{2}\}\). Let \(\Lambda^{2}\) be all agents that, after ten timesteps, always take \(a_{2}\). \(\overline{\lambda}^{1}\) is simple: it 

[MISSING_PAGE_FAIL:21]

In particular, let \(m\) be a fixed but arbitrary Turing Machine, and \(w\) be a fixed but arbitrary input to be given to machine \(m\). Then, Halt defines the decision problem that outputs True iff \(m\) halts on input \(w\).

We construct an oracle for AgentReaches that can decide Halt as follows. Consider the same observation space used in the proof of Lemma B.8: Let \(\mathcal{O}\) be comprised of all configurations of machine \(m\). Then, sequences of observations are simply evolution of different Turing Machines processing possible inputs. We consider an action space, \(\mathcal{A}=\{a_{\text{halted}},a_{\text{not-yet}}\}\), where agents simply report whether the history so far contains a halting configuration.

Then, we consider a deterministic environment \(e\) that simply produces the next configuration of \(m\) when run on input \(w\), based on the current tape contents, the state of \(m\), and the location of the tape head. Note again that all three of these elements are contained in a Turing Machine's configuration.

Using these ingredients, we take any instance of Halt, \((m,w)\), and build the singleton agent set \(\Lambda_{\text{B}}\) containing only the agent \(\lambda_{\text{halted}}:h\mapsto a_{\text{halted}}\) that always reports the machine as having halted. We then consider whether the agent that outputs \(a_{\text{not-yet}}\) indefinitely until \(m\) reports halting, at which point the agent switches to \(a_{\text{halted}}\).

We make one query to our AgentReaches oracle, and ask: \(\lambda\leadsto\Lambda_{\text{B}}\). If it is True, then the branching agent eventually becomes equivalent to \(\lambda_{\text{halted}}\) in that they both indefinitely output \(a_{\text{halted}}\) on at least one realizable history. Since \(e\) is deterministic, we know this equivalence holds across all histories. If the query reports False, then there is no future in \(e\) in which \(m\) halts on \(w\), otherwise the agent would become equivalent to \(\lambda_{\text{halted}}\). We thus use the oracle's response directly to decide the given instance of Halt. 

## Appendix C Additional Analysis

Finally, we present a variety of additional results about agents and the generates and reaches operators.

### Additional Analysis: Generates

We first highlight simple properties of the generates operator. Many of our results build around the notion of _uniform generation_, a variant of the generates operator in which a basis generates an agent set in every environment. We define this operator precisely as follows.

**Definition C.1**.: _Let \(\Sigma\) be a set of learning rules over some basis \(\Lambda_{\text{B}}\). We say that a set \(\Lambda\) is **uniformly \(\Sigma\)-generated** by \(\Lambda_{\text{B}}\), denoted \(\Lambda_{\text{B}}\models_{\Sigma}\Lambda\), if and only if_

\[\forall_{\lambda\in\Lambda}\exists_{\sigma\in\Sigma}\forall_{h\in\mathcal{H}} \ \ \lambda(h)=\sigma(h)(h).\] (C.1)

**Definition C.2**.: _We say a basis \(\Lambda_{\text{B}}\)**uniformly generates**\(\Lambda\), denoted \(\Lambda_{\text{B}}\models\Lambda\), if and only if_

\[\exists_{\Sigma\subseteq\Sigma}\ \ \Lambda_{\text{B}}\models_{\Sigma}\Lambda.\] (C.2)

We will first show that uniform generation entails generation in a particular environment. As a consequence, when we prove that certain properties hold of uniform generation, we can typically also conclude that the properties hold for generation as well, though there is some subtlety as to when exactly this implication will allow results about \(\models\) to apply directly to.

**Proposition C.1**.: _For any \((\Lambda_{\text{B}},\Lambda)\) pair, if \(\Lambda_{\text{B}}\models\Lambda\), then for all \(e\in\mathcal{E}\), \(\Lambda_{\text{B}}\not\in\Lambda\)._

_Proof of Proposition C.1._

Recall that in the definition of uniform generation, \(\Lambda_{\text{B}}\models\Lambda\), we require,

\[\exists_{\Sigma\subseteq\Sigma}\forall_{\lambda\in\Lambda}\exists_{\sigma\in \Sigma}\forall_{h\in\mathcal{H}}\ \lambda(h)=\sigma(h)(h).\] (C.3)

Now, contrast this with generates with respect to a specific environment \(e\),

\[\exists_{\Sigma\subseteq\Sigma}\forall_{\lambda\in\Lambda}\exists_{\sigma\in \Sigma}\forall_{h\in\widehat{\mathcal{H}}}\ \lambda(h)=\sigma(h)(h).\] (C.4)The only difference in the definitions is that the set of histories quantified over is in the former, and in the latter.

Since for any choice of environment, we can conclude that when Equation C.3, it is also the case that Equation C.4 holds, too. Therefore, for any. 

We next show that the subset relation implies generation.

Any pair of agent sets, such that satisfies

(C.5)

Proof of Proposition C.2.: The result follows from the combination of two facts. First, that all agent sets generate themselves. That is, for arbitrary, we know that, since the trivial set of learning rules,

(C.6)

that never switches between agents is sufficient to generate the agent set.

Second, observe that removing an agent from the generated set has no effect on the generates operator. That is, for fixed but arbitrary. We see that, since is sufficient to generate, too. By inducting over all removals of agents from, we reach our conclusion. 

Next, we establish properties about the sets of learning rules that correspond to the generates operator.

For any such that, it holds that

(C.7)

Proof of Proposition C.3.: We proceed toward contradiction, and assume. Then, there is at least one learning rule that corresponds to two or more distinct agents in. Call this element, and without loss of generality let and be two distinct agents that are each generated by in the sense that,

(C.8)

for every. But, by the distinctness of and, there must exist a history in which. We now arrive at a contradiction as such a history cannot exist: By Equation C.8, we know that for all. 

We see that the universal learning rules,, is the strongest in the following sense.

For any basis and agent set, exactly one of the two following properties hold:

1. The agent basis uniformly generates under the set of all learning rules:.
2. There is no set of learning rules for which the basis -uniformly generates the agent set:

Proof of Proposition C.4.: The proof follows from the law of excluded middle. That is, for any set of learning rules, either it generates or it does not. If it does generate, by Lemma B.6 so does. By consequence, if does _not_ generate, neither do any of its subsets. 

Furthermore, uniform generation is also transitive.

**Theorem C.5**.: _Uniform generates is transitive: For any triple \((\Lambda^{1},\Lambda^{2},\Lambda^{3})\), if \(\Lambda^{1}\models\Lambda^{2}\) and \(\Lambda^{2}\models\Lambda^{3}\), then \(\Lambda^{1}\models\Lambda^{3}\)._

Proof of Theorem c.5.: Assume \(\Lambda^{1}\models\Lambda^{2}\) and \(\Lambda^{2}\models\Lambda^{3}\). Then, by Proposition C.4 and the definition of the uniform generates operator, we know that

\[\forall_{\lambda^{2}\in\Lambda^{2}}\exists_{\sigma^{1}\in\Sigma^ {1}}\forall_{h\in\mathcal{H}}\;\lambda^{2}(h)=\sigma^{1}(h)(h),\] (C.9) \[\forall_{\lambda^{3}\in\Lambda^{2}}\exists_{\sigma^{2}\in\Sigma^ {2}}\forall_{h\in\mathcal{H}}\;\lambda^{3}(h)=\sigma^{2}(h)(h),\] (C.10)

where \(\Sigma^{1}\) and \(\Sigma^{2}\) express the set of all learning rules over \(\Lambda^{1}\) and \(\Lambda^{2}\) respectively. By definition of a learning rule, \(\sigma\), we rewrite the above as follows,

\[\forall_{\lambda^{2}\in\Lambda^{2}}\forall_{h\in\mathcal{H}} \exists_{\lambda^{1}\in\Lambda^{1}}\;\lambda^{2}(h)=\lambda^{1}(h),\] (C.11) \[\forall_{\lambda^{3}\in\Lambda^{3}}\forall_{h\in\mathcal{H}} \exists_{\lambda^{2}\in\Lambda^{2}}\;\lambda^{3}(h)=\lambda^{2}(h).\] (C.12)

Then, consider a fixed but arbitrary \(\lambda^{3}\in\Lambda^{3}\). We construct a learning rule defined over \(\Lambda^{1}\) as \(\sigma^{1}:\mathcal{H}\to\Lambda^{1}\) that induces an equivalent agent as follows. For each history, \(h\in\mathcal{H}\), by Equation C.12 we know that there is an \(\lambda^{2}\) such that \(\lambda^{3}(h)=\lambda^{2}(h)\), and by Equation C.11, there is an \(\lambda^{1}\) such that \(\lambda^{2}(h)=\lambda^{1}(h)\). Then, set \(\sigma^{1}:h\mapsto\lambda^{1}\) such that \(\lambda^{1}(h)=\lambda^{2}(h)=\lambda^{3}(h)\). Since \(h\) and \(\lambda^{3}\) were chosen arbitrarily, we conclude that

\[\forall_{\lambda^{3}\in\Lambda^{3}}\forall_{h\in\mathcal{H}} \exists_{\lambda^{1}\in\Lambda^{1}}\;\lambda^{3}(h)=\lambda^{1}(h).\]

But, by the definition of \(\Sigma\), this means there exists a learning rule such that

\[\forall_{\lambda^{3}\in\Lambda^{3}}\exists_{\sigma^{1}\in\Sigma^ {1}}\forall_{h\in\mathcal{H}}\;\lambda^{3}(h)=\sigma^{1}(h)(h).\]

This is exactly the definition of \(\Sigma\)-uniform generation, and by Proposition C.4, we conclude \(\Lambda^{1}\models\Lambda^{3}\). 

Next, we show that a singleton basis only generates itself.

**Proposition C.6**.: _Any singleton basis, \(\Lambda_{\mathrm{B}}=\{\lambda\}\), only uniformly generates itself._

Proof of Proposition c.6.: Note that generates requires switching between base agents. With only a single agent, there cannot be any switching, and thus, the only agent that can be described as switching amongst the elements of the singleton set \(\Lambda_{\mathrm{B}}=\{\lambda\}\) is the set itself. 

#### c.1.1 Rank and Minimal Bases

As discussed in the paper, one natural reaction to the concept of an agent basis is to ask how we can justify different choices of a basis. And, if we cannot, then perhaps the concept of an agent basis is disruptive, rather than illuminating. In the main text, we suggest that in many situations, the choice of basis is made by the constraints imposed by the problem, such as the available memory. However, there are some objective properties of different bases that can help us to evaluate possible choices of a suitable basis. For instance, some bases are minimal in the sense that they cannot be made smaller while still retaining the same expressive power (that is, while generating the same agent sets). Identifying such minimal sets may be useful, as it is likely that there is good reason to consider only the most compressed agent bases.

To make these intuitions concrete, we introduce the _rank_ of an agent set.

**Definition C.3**.: _The **rank** of an agent set, \(\mathrm{rank}(\Lambda)\), is the size of the smallest agent basis that uniformly generates it:_

\[\mathrm{rank}(\Lambda)=\min_{\Lambda_{\mathrm{B}}\subset\Lambda}| \Lambda_{\mathrm{B}}|\qquad\text{s.t.}\qquad\Lambda_{\mathrm{B}}\models\Lambda.\] (C.13)For example, the agent set,

\[\Lambda=\{\lambda^{0}:h \mapsto a_{0},\] (C.14) \[\lambda^{1}:h \mapsto a_{1},\] \[\lambda^{2}:h \mapsto\begin{cases}a_{0}&|h|\bmod 2=0,\\ a_{1}&|h|\bmod 2=1,\end{cases}\] \[\},\]

has \(\operatorname{rank}(\Lambda)=2\), since the basis,

\[\Lambda_{\text{B}}=\{\lambda_{\text{B}}^{0}:h\mapsto a_{0},\;\lambda_{\text{ B}}^{1}:h\mapsto a_{1}\},\]

uniformly generates \(\Lambda\), and there is no size-one basis that uniformly generates \(\Lambda\) by Proposition C.6.

Using the notion of an agent set's rank, we now introduce the concept of a _minimal basis_. We suggest that minimal bases are particular important, as they contain no redundancy with respect to their expressive power. Concretely, we define a minimal basis in two slightly different ways depending on whether the basis has finite or infinite rank. In the finite case, we say a basis is minimal if there is no basis of lower rank that generates it.

**Definition C.4**.: _An agent basis \(\Lambda_{\text{B}}\) with finite rank is said to be **minimal** just when there is no smaller basis that generates it,_

\[\forall_{\Lambda_{\text{B}}^{\prime}\subseteq\Lambda}\;\;\Lambda_{\text{B}}^ {\prime}\models\Lambda_{\text{B}}\;\Rightarrow\;\operatorname{rank}( \Lambda_{\text{B}}^{\prime})\geq\operatorname{rank}(\Lambda_{\text{B}}).\] (C.15)

In the infinite case, as all infinite rank bases will have the same effective size, we instead consider a notion of minimiality based on whether any elements can be removed from the basis without changing its expressive power.

**Definition C.5**.: _An agent basis \(\Lambda_{\text{B}}\) with infinite rank is said to be **minimal** just when no proper subset of \(\Lambda_{\text{B}}\) uniformly generates \(\Lambda_{\text{B}}\)._

\[\forall_{\Lambda_{\text{B}}^{\prime}\subseteq\Lambda_{\text{B}}}\;\;\Lambda_{ \text{B}}^{\prime}\models\Lambda_{\text{B}}\;\Rightarrow\;\Lambda_{\text{B}}^ {\prime}=\Lambda_{\text{B}}.\] (C.16)

Notably, this way of looking at minimal bases will also apply to finite rank agent bases as a direct consequence of the definition of a minimal finite rank basis. However, we still provide both definitions, as a finite rank basis may not contain a subset that generates it, but there may exist a lower rank basis that generates it.

**Corollary C.7**.: _As a Corollary of Proposition C.2 and Definition C.4, for any minimal agent basis \(\Lambda_{\text{B}}\), there is no proper subset of \(\Lambda_{\text{B}}\) that generates \(\Lambda_{\text{B}}\)._

Regardless of whether an agent basis has finite or infinite rank, we say the basis is a minimal basis of an agent set \(\Lambda\) just when the basis uniformly generates \(\Lambda\) and the basis is minimal.

**Definition C.6**.: _For any \(\Lambda\), a **minimal basis of \(\Lambda\)** is any basis \(\Lambda_{\text{B}}\) that is both (1) minimal, and (2) \(\Lambda_{\text{B}}\models\Lambda\)._

A natural question arises as to whether the minimal basis of any agent set \(\Lambda\) is unique. We answer this question in the negative.

**Proposition C.8**.: _The minimal basis of a set of agents is not necessarily unique._

Proof of Proposition C.8.: To prove the claim, we construct an instance of an agent set with two distinct minimal bases. Let \(\mathcal{A}=\{a_{0},a_{1}\}\), and \(\mathcal{O}=\{o_{0}\}\). We consider the agent set containing four agents. The first two map every history to \(a_{0}\) and \(a_{1}\), respectively, while the second two alternate between and \(a_{1}\) depending on whether the history is of odd or even length:

\[\Lambda=\{\lambda^{0}:h \mapsto a_{0},\] (C.17) \[\lambda^{1}:h \mapsto a_{1},\] \[\lambda^{2}:h \mapsto\begin{cases}a_{0}&|h|\bmod 2=0,\\ a_{1}&|h|\bmod 2=1,\end{cases}\] \[\lambda^{3}:h \mapsto\begin{cases}a_{0}&|h|\bmod 2=1,\\ a_{1}&|h|\bmod 2=0,\end{cases}\] \[\}.\]

Note that there are two distinct subsets that each universally generate \(\Lambda\):

\[\Lambda^{0,1}_{\mathrm{B}}=\{\lambda^{0}_{\mathrm{B}},\lambda^{1}_{\mathrm{B} }\},\qquad\Lambda^{2,3}_{\mathrm{B}}=\{\lambda^{2}_{\mathrm{B}},\lambda^{3}_{ \mathrm{B}}\}.\] (C.18)

Next notice that there cannot be a singleton basis by Proposition C.6, and thus, both \(\Lambda^{0,1}_{\mathrm{B}}\) and \(\Lambda^{2,3}_{\mathrm{B}}\) satisfy (1) \(|\Lambda^{0,1}_{\mathrm{B}}|=|\Lambda^{2,3}_{\mathrm{B}}|=\mathrm{rank}(\Lambda)\), and (2) both \(\Lambda^{0,1}_{\mathrm{B}}\models\Lambda\), \(\Lambda^{2,3}_{\mathrm{B}}\models\Lambda\). 

Beyond the lack of redundancy of a basis, we may also be interested in their expressive power. For instance, if we compare two minimal bases, \(\Lambda^{1}_{\mathrm{B}}\) and \(\Lambda^{2}_{\mathrm{B}}\), how might we justify which to use? To address this question, we consider another desirable property of a basis: _universality_.

**Definition C.7**.: _An agent basis \(\Lambda_{\mathrm{B}}\) is **universal** if \(\Lambda_{\mathrm{B}}\models\mathbb{A}\)._

Clearly, it might be desirable to work with a universal basis, as doing so ensures that the set of agents we consider in our design space is as rich as possible. We next show that there is at least one natural basis that is both minimal and universal.

**Proposition C.9**.: _The basis,_

\[\Lambda^{\circ}_{\mathrm{B}}=\{\lambda:\mathcal{O}\to\Delta(\mathcal{H})\},\] (C.19)

_is a **minimal universal basis**:_

1. \(\Lambda^{\circ}_{\mathrm{B}}\models\mathbb{A}\)_: The basis uniformly generates the set of all agents._
2. \(\Lambda^{\circ}_{\mathrm{B}}\) _is minimal._

_Proof of Proposition C.9._

We prove each property separately.

_1. \(\Lambda^{\circ}_{\mathrm{B}}\models\Lambda\)_

First, we show that the basis is universal: \(\Lambda^{\circ}_{\mathrm{B}}\models\mathbb{A}\). Recall that this amounts to showing that,

\[\forall_{\lambda\in\mathbb{A}}\forall_{h\in\mathcal{H}}\exists_{\lambda^{ \prime}\in\Lambda^{\circ}_{\mathrm{B}}}\lambda(h)=\lambda^{\prime}(h).\] (C.20)

Let \(\lambda\in\mathbb{A}\) and \(h\in\mathcal{H}\) be fixed but arbitrary. Now, let us label the action distribution produced by \(\lambda(h)\) as \(p_{\lambda(h)}\). Let \(o\) refer to the last observation contained in \(h\) (or \(\emptyset\) if \(h=h_{0}=\emptyset\)). Now, construct the agent \(\lambda^{\circ}_{\mathrm{B}}:o\mapsto p_{\lambda(h)}\). By construction of \(\Lambda^{\circ}_{\mathrm{B}}\), this agent is guaranteed to be a member of \(\Lambda^{\circ}_{\mathrm{B}}\), and furthermore, we know that \(\lambda^{\circ}_{\mathrm{B}}\) produces the same output as \(\lambda\) on \(h\). Since both \(\lambda\) and \(h\) were chosen arbitrarily, the construction will work for any choice of \(\lambda\) and \(h\), and we conclude that at every history, there exists a basis agent \(\Lambda^{\circ}_{\mathrm{B}}\in\Lambda^{\circ}_{\mathrm{B}}\) that produces the same probability distribution over actions as any given agent. Thus, the first property holds. \(\checkmark\)

_2. \(\Lambda^{\circ}_{\mathrm{B}}\) is minimal._Second, we show that \(\Lambda_{\text{B}}^{\circ}\) is a minimal basis of \(\mathbb{A}\). Recall that since \(\text{rank}(\Lambda_{\text{B}}^{\circ})=\infty\), the definition of a minimal basis means that:

\[\mathbb{V}_{\Lambda_{\text{B}}\subseteq\Lambda_{\text{B}}^{\circ}}\ \Lambda_{\text{B}}\models\mathbb{A}\ \Rightarrow\ \Lambda_{\text{B}}=\Lambda_{\text{B}}^{\circ}.\] (C.21)

To do so, fix an arbitrary proper subset of \(\Lambda_{\text{B}}\in\mathscr{P}(\Lambda_{\text{B}}^{\circ})\). Notice that since \(\Lambda_{\text{B}}\) is a proper subset, there exists a non-empty set \(\overline{\Lambda_{\text{B}}}\) such that,

\[\Lambda_{\text{B}}\cup\overline{\Lambda_{\text{B}}}=\Lambda_{\text{B}}^{\circ}.\]

Now, we show that \(\Lambda_{\text{B}}\) cannot uniformly generate \(\mathbb{A}\) by constructing an agent from \(\overline{\Lambda_{\text{B}}}\). In particular, consider the first element of \(\overline{\Lambda_{\text{B}}}\), which, by construction of \(\Lambda_{\text{B}}^{\circ}\), is _some_ mapping from \(\mathcal{O}\) to a choice of probability distribution over \(\mathcal{A}\). Let us refer to this agent's output probability distribution over actions as \(\overline{p}\). Notice that there cannot exist an agent in \(\Lambda_{\text{B}}^{\circ}\) that chooses \(\overline{p}\), otherwise \(\Lambda_{\text{B}}\) would not be a proper subset of \(\Lambda_{\text{B}}^{\circ}\). Notice further that in the set of all agents, there are infinitely many agents that output \(\overline{p}\) in at least one history. We conclude that \(\Lambda_{\text{B}}\) cannot uniformly generate \(\mathbb{A}\), as it does not contain any base element that produces \(\overline{p}\). The set \(\overline{\Lambda_{\text{B}}}\) was chosen arbitrarily, and thus the claim holds for any proper subset of \(\Lambda_{\text{B}}^{\circ}\), and we conclude. \(\checkmark\)

This completes the proof of both statements. 

**Corollary C.10**.: _As a direct consequence of Proposition C.9, every universal basis has infinite rank._

#### c.1.2 Orthogonal and Parallel Agent Sets

Drawing inspiration from vector spaces, we introduce notions of _orthogonal_ and _parallel_ agent bases according to the agent sets they generate.

**Definition C.8**.: _A pair of agent bases (\(\Lambda_{\text{B}}^{1},\Lambda_{\text{B}}^{2}\)) are **orthogonal** if any pair \((\Lambda^{1},\Lambda^{2})\) they each uniformly generate_

\[\Lambda_{\text{B}}^{1}\models\Lambda^{1},\qquad\Lambda_{\text{B}}^{2}\models \Lambda^{2},\] (C.22)

_satisfy_

\[\Lambda^{1}\cap\Lambda^{2}=\emptyset.\] (C.23)

Naturally this definition can be modified to account for environment-relative generation (\(\mathbb{F}\)), or to be defined with respect to a particular set of learning rules in which case two bases are orthogonal with respect to the learning rule set just when they generate different agent sets under the given learning rules. As with the variants of the two operators, we believe the details of such formalisms are easy to produce.

A few properties hold of any pair of orthogonal bases.

**Proposition C.11**.: _If two bases \(\Lambda_{\text{B}}^{1},\Lambda_{\text{B}}^{2}\) are orthogonal, then the following properties hold:_

1. \(\Lambda_{\text{B}}^{1}\cap\Lambda_{\text{B}}^{2}=\emptyset\)_._
2. _Neither_ \(\Lambda_{\text{B}}^{1}\) _nor_ \(\Lambda_{\text{B}}^{2}\) _are universal._

_Proof of Proposition C.11._

We prove each property independently.

_1. \(\Lambda_{\text{B}}^{1}\cap\Lambda_{\text{B}}^{2}=\emptyset\)_

We proceed toward contradiction. That is, suppose that both \(\Lambda_{\text{B}}^{1}\) is orthogonal to \(\Lambda_{\text{B}}^{2}\), and that \(\Lambda_{\text{B}}^{1}\cap\Lambda_{\text{B}}^{2}\neq\emptyset\). Then, by the latter property, there is at least one agent that is an element of 

[MISSING_PAGE_FAIL:28]

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_EMPTY:30]

### Figure: Set Relations in CRL

Finally, in Figure 3 we present a visual depicting the set relations in CRL between an agent basis \(\Lambda_{\text{B}}\), an agent set it generates \(\Lambda\), and the three agent sets corresponding to those agents in \(\Lambda\) that (i) sometimes, (ii) never, or (iii) always reach the basis. First, we highlight that we visualize \(\Lambda_{\text{B}}\) as a subset of \(\Lambda\) since we define \(\Lambda_{\text{B}}\subset\Lambda\) in CRL (Definition 4.2). However, there can exist triples \((\Lambda_{\text{B}},\Lambda,e)\) such that \(\Lambda_{\text{B}}\not\in\Lambda\), but that \(\Lambda_{\text{B}}\) is _not_ a subset of \(\Lambda\). Such cases are slightly peculiar, since it means that the basis contains agents that cannot be expressed by the agent set \(\Lambda\). Such cases are not in line with our definition of CRL, so we instead opt to visualize \(\Lambda_{\text{B}}\) as a subset of \(\Lambda\). Next, notice that the basis is a subset of both the agents that always reach the basis and the agents that sometimes reach the basis--this follows directly from the combination of Proposition C.14 and point (3.) of Theorem 4.3. By similar reasoning from Proposition C.14, we know that the set of agents that always reaches \(\Lambda_{\text{B}}\) is a subset of the agents that sometimes reach the basis. Further, since sometimes and never reaches are negations of one another (Remark 3.2), observe that the two sets are disjoint, and together comprise the entirety of \(\Lambda\). Lastly, we know that the set of optimal agents, \(\Lambda^{\star}\), contains only agents that never reach the basis, and thus the set \(\Lambda^{\star}\) is disjoint from \(\Lambda_{\text{B}}\) and the set of agents that sometimes reach \(\Lambda_{\text{B}}\).

Figure 3: A depiction of the division of a set of agents \(\Lambda\) relative to a basis \(\Lambda_{\text{B}}\) through the reaches operator in CRL.