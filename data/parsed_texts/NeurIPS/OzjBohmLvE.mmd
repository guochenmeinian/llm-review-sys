# Achieving \(\mathcal{O}(\epsilon^{-1.5})\) Complexity in

Hessian/Jacobian-free Stochastic Bilevel Optimization

 Yifan Yang, Peiyao Xiao and Kaiyi Ji

Department of Computer Science and Engineering

University at Buffalo

Buffalo, NY 14260

{yyang99, peiyaoki, kaiyiji}@buffalo.edu

###### Abstract

In this paper, we revisit the bilevel optimization problem, in which the upper-level objective function is generally nonconvex and the lower-level objective function is strongly convex. Although this type of problem has been studied extensively, it still remains an open question how to achieve an \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity in Hessian/Jacobian-free stochastic bilevel optimization without any second-order derivative computation. To fill this gap, we propose a novel Hessian/Jacobian-free bilevel optimizer named FdeHBO, which features a simple fully single-loop structure, a projection-aided finite-difference Hessian/Jacobian-vector approximation, and momentum-based updates. Theoretically, we show that FdeHBO requires \(\mathcal{O}(\epsilon^{-1.5})\) iterations (each using \(\mathcal{O}(1)\) samples and only first-order gradient information) to find an \(\epsilon\)-accurate stationary point. As far as we know, this is the first Hessian/Jacobian-free method with an \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity for nonconvex-strongly-convex stochastic bilevel optimization.

## 1 Introduction

Bilevel optimization has drawn intensive attention due to its wide applications in meta-learning [18; 4; 50], hyperparameter optimization [18; 52; 14], reinforcement learning [35; 27], signal process [36; 16] and communication [31] and federated learning [59]. In this paper, we study the following stochastic bilevel optimization problem.

\[\min_{x\in\mathbb{R}^{p}}\Phi(x) =f(x,y^{*}(x)):=\mathbb{E}_{\xi}\left[f(x,y^{*}(x);\zeta)\right]\] \[\text{s.t. }y^{*}(x) =\operatorname*{arg\,min}_{y\in\mathbb{R}^{q}}g(x,y):=\mathbb{E} _{\zeta}\left[g(x,y^{*}(x);\zeta)\right]\] (1)

where the upper- and lower-level objective functions \(f(x,y)\) and \(g(x,y)\) take the expectation form w.r.t. the random variables \(\xi\) and \(\zeta\), and are jointly continuously differentiable. In this paper, we focus on the nonconvex-strongly-convex bilevel setting, where the lower-level function \(g(x,\cdot)\) is strongly convex and the upper-level function \(\Phi(x)\) is nonconvex. This class of bilevel problems has been studied extensively from the theoretical perspective in recent years. Among them, [19; 30; 3; 62] proposed bilevel approaches with a double-loop structure, which update \(x\) and \(y\) in a nested manner. Single-loop bilevel algorithms have also attracted significant attention recently [27; 62; 34; 25; 9; 40; 11] due to the simple updates on all variables simultaneously. Among them, the approaches in [62; 34; 25] have been shown to achieve an \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity, but with expensive evaluations of Hessian/Jacobian matrices or Hessian/Jacobian-vector products.

Hessian/Jacobian-free bilevel optimization has received increasing attention due to its high efficiency and feasibility in practical large-scale settings. In particular, [15; 48; 61] directly ignored thecomputation of all second-order derivatives. However, such eliminations may lead to performance degeneration [2; 13], and can vanish the hypergradient for bilevel problems with single-variable upper-level function, i.e., \(\Phi(x)=f(y^{*}(x))\). [56; 23] proposed zeroth-order approaches that approximate the hypergradient using only function values. These methods do not have a convergence rate guarantee. Recently, several Hessian/Jacobian-free bilevel algorithms were proposed by [42; 57; 53; 8] by reformulating the lower-level problem into the optimality-based constraints such as \(g(x,y)\leq\min_{y}g(x,y)\). However, these approaches all focus on the deterministic setting, and their extensions to the stochastic setting remain unclear. In the stochastic case, [58] proposed evolution strategies based bilevel method, which achieves a high sample complexity of \(\mathcal{O}(p^{2}\epsilon^{-2})\), where \(p\) is the problem dimension. Most recently, [37] proposed two fully first-order (i.e., Hessian/Jacobian-free) value-function-based stochastic bilevel optimizer named F\({}^{2}\)SA and its momentum-based version F\({}^{3}\)SA with a single-loop structure, which achieves sample complexities of \(\mathcal{O}(\epsilon^{-3.5})\) and \(\mathcal{O}(\epsilon^{-2.5})\), respectively. However, there is still a large gap of \(\epsilon^{-1}\), compared to the optimal complexity of \(\mathcal{O}(\epsilon^{-1.5})\). Then, an important open question, as recently proposed by [37], is:

* Can we achieve an \(\mathcal{O}(\epsilon^{-1.5})\) sample/gradient complexity for nonconvex-strongly-convex bilevel optimization using only first-order gradient information?

### Our Contributions

In this paper, we provide an affirmative answer to the above question by proposing a new Hessian/Jacobian-free stochastic bilevel optimizer named FdeHBO with three main features. First, FdeHBO takes the fully single-loop structure with momentum-based updates on three variables \(y,v\) and \(x\) for optimizing the lower-level objective, the linear system (LS) of the Hessian-inverse-vector approximation, and the upper-level objective, respectively. Second, FdeHBO contains only a single matrix-vector product at each iteration, which admits a simple first-order finite-difference estimation. Third, FdeHBO involves an auxiliary projection on \(v\) updates to ensure the boundedness of the Hessian-vector approximation error, the variance on momentum-based iterates, and the smoothness of the LS loss function. Our detailed contributions are summarized below.

* Theoretically, we show that FdeHBO achieves a sample/gradient complexity of \(\mathcal{O}(\epsilon^{-1.5})\) and an iteration complexity of \(\mathcal{O}(\epsilon^{-1.5})\) to achieve an \(\epsilon\)-accurate stationary point, both of which outperforms existing results by a large margin. As far as we know, this is the first-known method with an \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity for nonconvex-strongly-convex stochastic bilevel optimization using only first-order gradient information.
* Technically, we show that the auxiliary projection can provide more accurate iterates on \(v\) in solving the LS problem without affecting the overall convergence behavior, and in addition, provide a novel characterization of the gradient estimation error and the iterative progress during the \(v\) updates, as well as the impact of the \(y\) and \(v\) updates on the momentum-based hypergradient estimation, all of which do not exist in previous studies. In addition, the finite-different approximations make the unbiased assumptions in the momentum-based gradients no longer hold, and hence a more careful analysis is required.
* As a byproduct, we further propose a fully single-loop momentum-based method named FMBO in the small-dimensional case with matrix-vector-based hypergradient computations. Differently from existing momentum-based bilevel methods with \(\mathcal{O}(\log\frac{1}{\epsilon})\) Hessian-vector evaluations per iteration, FMBO contains only a single Hessian-vector computation per iteration with the same \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity.

We also want to emphasize our technical differences from previous works as below.

\begin{table}
\begin{tabular}{|l|l|l|l|c|} \hline Algorithm & Samples & Batch size & \# of iterations & Loops per iteration \\ \hline \hline PZOBO-S [58] & \(\widetilde{\mathcal{O}}(p^{2}\epsilon^{-3})\) & \(\mathcal{O}(\epsilon^{-1})\) & \(\widetilde{\mathcal{O}}(p^{2}\epsilon^{-2})\) & 2 \\ \hline F\({}^{2}\)SA [37] & \(\widetilde{\mathcal{O}}(\epsilon^{-3.5})\) & \(\mathcal{O}(1)\) & \(\widetilde{\mathcal{O}}(\epsilon^{-3.5})\) & 1 \\ \hline F\({}^{3}\)SA [37] & \(\widetilde{\mathcal{O}}(\epsilon^{-2.5})\) & \(\mathcal{O}(1)\) & \(\widetilde{\mathcal{O}}(\epsilon^{-2.5})\) & 1 \\ \hline FdeHBO (this paper) & \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) & \(\widetilde{\mathcal{O}}(1)\) & \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) & 1 \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of stochastic Hessian/Jacobian-free bilevel optimization algorithms.

**Comparison to existing momentum-based methods.** Previous momentum-based methods [62; 34] solve the linear system (LS) to a high accuracy of \(\mathcal{O}(\epsilon)\), whereas our algorithm includes a new estimation error by the single-step momentum update on LS, and this error is also correlated with the lower-level updating error and the hypergradient estimation error. In addition, due to the finite-difference approximation, the stochastic gradients in all three updates on \(y,v,x\) are no longer unbiased. Non-trivial efforts need to be taken to deal with such challenges and derive the optimal complexity.

**Comparison to existing fully single-loop methods.** The analysis of the single-step momentum update in solving the LS requires the smoothness of the LS loss function and the boundedness of LS gradient variance, both of which may not be satisfied. To this end, we include an auxiliary projection and show it not only guarantees these crucial properties, but also, in theory, provides an improved per-iteration progress. As a comparison, existing works on fully single-loop stochastic bilevel optimization such as SOBA/SABA [11] and FLSA [40] with a new time scale to update the LS problem often assume that the iterates on \(v\) are bounded during the process. We do not require such assumptions. In addition, an \(\mathcal{O}(\epsilon^{-1.5})\) complexity has not been established for fully single-loop bilevel algorithms yet.

### Related Work

**Bilevel optimization methods.** Bilevel optimization, which was first introduced by [6], has been studied for decades. By replacing the lower-level problem with its optimality conditions, [26; 20; 54; 55] reformulated the bilevel problem to the single-level problem. Gradient-based bilevel methods have shown great promise recently, which can be divided into approximate implicit differentiation (AID) [12; 49; 41; 3] and iterative differentiation (ITD) [47; 17; 15; 52; 21] based approaches. Recently, a bunch of stochastic bilevel algorithms has been proposed via Neumann series [9; 30], recursive momentum [62; 28; 25] and variance reduction [62; 11]. Theoretically, the convergence of bilevel optimization has been analyzed by [18; 52; 45; 19; 30; 27; 3; 11]. Among them, [29] provides the lower complexity bounds for deterministic bilevel optimization with (strongly-)convex upper-level functions. [25; 9; 62; 34] achieved the near-optimal sample complexity with second-order derivative computations. Some works studied deterministic bilevel optimization with convex or Polyak-Lojasiewicz (PL) lower-level problems via mixed gradient aggregation [51; 46; 39], log-barrier regularization [45], primal-dual method [57] and dynamic barrier [63]. More results and details can be found in the survey by [44].

**Hessian/Jacobian-free bilevel optimization.** Some Hessian/Jacobian-free bilevel optimization methods have been proposed recently by [58; 43; 15; 23; 56; 48]. Among them, FOMAML [15; 48] and MUMOMOAML [61] directly ignore the computation of all second-order derivatives. Several Hessian/Jacobian-free bilevel algorithms were proposed by [42; 57; 53; 8] by replacing the lower-level problem with the optimality conditions as the constraints. However, these approaches focus only on the deterministic setting. Recently, zeroth-order stochastic approaches have been proposed for the hypergradient estimation [56; 23; 58]. Theoretically, [58] analyzed the convergence rate for their method. [37] proposed fully first-order stochastic bilevel optimization algorithms based on the value-function-based lower-level problem reformulation. This paper proposes a new Hessian/Jacobian-free stochastic bilevel algorithm that for the first time achieves an \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity.

**Momentum-based bilevel approaches.** The recursive momentum technique was first introduced by [10; 60] for minimization problems to improve the SGD-based updates in theory and in practice. This technique has been incorporated in stochastic bilevel optimization [34; 9; 24; 25; 62]. These approaches involve either Hessian-inverse matrix computations or a subloop of a number of iterations in the Hessian-inverse-vector approximation. As a comparison, our proposed method takes the simpler fully single-loop structure, and only uses the first-order gradient information.

**Finite-difference matrix-vector approximation.** The finite-difference matrix-vector estimation has been studied extensively in the problems of escaping from saddle points [1][7] (some other related works can be found therein), neural architecture search (NAS) [43] and meta-learning [13]. However, such finite-different estimation can be sensitive to the selection of the smoothing constant, and may suffer from some numerical issues in practice [32][33], such as rounding errors. It is interesting but still open to developing a fully first-order stochastic bilevel optimizer without the finite-different matrix-vector estimation. We would like to leave it for future study.

## 2 Algorithms

In this section, we first describe the hypergradient computation in bilevel optimization, and then present the proposed Hessian/Jacobian-free bilevel method.

### Hypergradient Computation

One major challenge in bilevel optimization lies in computing the hypergradient \(\nabla\Phi(x)\) due to the implicit and complex dependence of the lower-level minimizer \(y^{*}\) on \(x\). To see this, if \(g\) is twice differentiable, \(\nabla_{y}g\) is continuously differentiable and the Hessian \(\nabla_{yy}^{2}g(x,y^{*}(x))\) is invertible, using the implicit function theorem (IFT) [22; 5], the hypergradient \(\nabla\Phi(x)\) takes the form of

\[\nabla\Phi(x)=\nabla_{x}f(x,y^{*})-\nabla_{xy}^{2}g(x,y^{*})\big{[}\nabla_{yy }^{2}g(x,y^{*})\big{]}^{-1}\nabla_{y}f(x,y^{*}).\] (2)

Note that the hypergradient in eq. (2) requires computing the exact solution \(y^{*}\) and the expensive Hessian inverse \([\nabla_{yy}^{2}g(x,y^{*})]^{-1}\). To approximate this hypergradient efficiently, we define the following (stochastic) hypergradient surrogates as

\[\bar{\nabla}f(x,y,v)= \nabla_{x}f(x,y)-\nabla_{xy}^{2}g(x,y)v,\] \[\bar{\nabla}f(x,y,v;\xi)= \nabla_{x}f(x,y;\xi)-\nabla_{xy}^{2}g(x,y;\xi)v,\] (3)

where \(v\in\mathbb{R}^{q}\) is an auxiliary vector to approximate the Hessian-inverse-vector product in eq. (2), and \(\bar{\nabla}f(x,y,v;\xi)\) can be regarded as a stochastic version of \(\bar{\nabla}f(x,y,v)\). Based on eq. (2), one needs to find an efficient estimate \(y\) of \(y^{*}\), e.g., via an iterative optimization procedure, as well as a feasible estimate \(v\) of the solution \(v^{*}=[\nabla_{yy}^{2}g(x,y)]^{-1}\nabla_{y}f(x,y)\) of a linear system (LS) (equivalently quadratic programming) whose generic loss function is given by

\[\text{(Linear system loss:)}\quad R(x,y,v)=\frac{1}{2}v^{T}\nabla_{yy}^{2}g(x,y)v-v^{T} \nabla_{y}f(x,y),\] (4)

where the gradient of \(R(x,y,v)\) w.r.t. \(v\) is given by

\[\nabla_{v}R(x,y,v)=\nabla_{yy}^{2}g(x,y)v-\nabla_{y}f(x,y).\] (5)

Similarly to eq. (2), we also define \(\nabla_{v}R(x,y,v;\psi)=\nabla_{yy}^{2}g(x,y;\psi)v-\nabla_{y}f(x,y;\psi)\) over any sample \(\psi\) as a stochastic version of \(\nabla_{v}R(x,y,v)\) in eq. (5). It can be seen from eq. (3), eq. (4) and eq. (5) that the updates on the LS system involve the Hessian- and Jacobian-vector products, which can be computationally intractable in the high-dimensional case. In the next section, we propose a novel stochastic Hessian/Jacobian-free bilevel algorithm.

### Hessian/Jacobian-free Bilevel Optimizer via Projection-aided Finite-difference Estimation

As shown in Algorithm 1, we propose a fully single-loop stochastic Hessian/Jacobian-free bilevel optimizer named FdeHBO via projection-aided finite-difference estimation. It can be seen that FdeHBO first minimizes the lower-level objective function \(g(x,y)\) w.r. \(y\) by running a single-step momentum-based update as \(y_{t+1}=y_{t}-\beta_{t}h_{t}^{g}\), where \(\beta_{t}\) is the stepsize and \(h_{t}^{g}\) is the momentum-based gradient estimator that takes the form of

\[h_{t}^{g}=\eta_{t}^{g}\nabla_{y}g(x_{t},y_{t};\zeta_{t})+(1-\eta_{t}^{g}) \big{(}h_{t-1}^{g}+\nabla_{y}g(x_{t},y_{t};\zeta_{t})-\nabla_{y}g(x_{t-1},y_{t -1};\zeta_{t})\big{)}\] (6)where \(\eta_{t}^{g}\in[0,1]\) is a tuning parameter. The next key step is to deal with the LS problem via solving the quadratic problem eq.4 as \(w_{t+1}=v_{t}-\lambda_{t}\widetilde{h}_{t}^{R}\), with the momentum-based gradient \(\widetilde{h}_{t}^{R}\) given by

\[\widetilde{h}_{t}^{R}= \eta_{t}^{R}\widetilde{\nabla}_{v}R(x_{t},y_{t},v_{t},\delta_{ \epsilon};\psi_{t})+(1-\eta_{t}^{R})\big{(}h_{t-1}^{R}+\widetilde{\nabla}_{v} R(x_{t},y_{t},v_{t},\delta_{\epsilon};\psi_{t})\] \[-\widetilde{\nabla}_{v}R(x_{t-1},y_{t-1},v_{t-1},\delta_{ \epsilon};\psi_{t})\big{)},\] (7)

where \(\widetilde{\nabla}_{v}R\) is a Hessian-free version of the LS gradient \(\nabla_{v}R\) in eq.5, given by

\[\text{(First-order LS gradient:)}\ \ \widetilde{\nabla}_{v}R(x_{t},y_{t},v_{t}, \delta_{\epsilon};\psi_{t})=\widetilde{H}(x_{t},y_{t},v_{t},\delta_{\epsilon} ;\psi_{t})-\nabla_{y}f(x_{t},y_{t};\psi_{t}).\] (8)

Note that in the above eq.8, \(\widetilde{H}(x_{t},y_{t},v_{t},\delta_{\epsilon};\psi_{t})\) is the finite-difference estimation of the Hessian-vector product \(\nabla_{yy}^{2}g(x_{t},y_{t};\psi_{t})v_{t}\), which takes the form of

\[\widetilde{H}(x_{t},y_{t},v_{t},\delta_{\epsilon};\psi_{t})=\frac{\nabla_{y}g( x_{t},y_{t}+\delta_{\epsilon}v_{t};\psi_{t})-\nabla_{y}g(x_{t},y_{t}-\delta_{ \epsilon}v_{t};\psi_{t})}{2\delta_{\epsilon}},\] (9)

where \(\delta_{\epsilon}>0\) is a small constant. Note that in eq.9, if the iterative \(v_{t}\) is unbounded, the approximation error between \(\widetilde{H}\) and \(\nabla_{yy}^{2}g(x_{t},y_{t};\psi_{t})v_{t}\) can be uncontrollable as well. We further prove lemma5 in appendixB that the bound of this gap relies on \(\|v_{t}\|\) and \(\delta\) but it is independent of the dimension of \(y_{t}\). To this end, after obtaining \(w_{t+1}\), our key step in line6 introduces an auxiliary projection on a ball (which can be generalized to any convex and bounded domain) with a radius of \(r_{v}\) as

\[\text{(Auxiliary projection)}\quad v_{t+1}=\begin{cases}w_{t+1},&\|w_{t+1}\| \leq r_{v};\\ \frac{r_{v}w_{t+1}}{\|w_{t+1}\|},&\|w_{t+1}\|>r_{v}.\end{cases}\]

This auxiliary projection guarantees the boundedness of \(v_{t},t=0,...,T-1\), which serves **three** important purposes. First, it ensures the smoothness of the LS loss function \(R(x,y,v)\) in eq.5 w.r.t. all \(x,y\) and \(v\), which is crucial in the convergence analysis of the momentum-based updates. Second, the boundedness of \(v_{t}\) also ensures that the estimation variance of the stochastic LS gradient \(\nabla_{v}R(x_{t},y_{t},v_{t};\psi_{t})\) does not explode. Third, it guarantees the error of the finite-difference Hessian-vector approximation to be sufficiently small with proper \(\delta_{\epsilon}\). We will show later that under a proper choice of the radius \(r_{v}\), this auxiliary projection provides better per-step progress, and the proposed algorithm achieves a stronger convergence performance. Finally, for the upper-level problem, the momentum-based hypergradient estimate \(\widetilde{h}_{t}^{f}\) is designed as

\[\widetilde{h}_{t}^{f}= \eta_{t}^{f}\widetilde{\nabla}f(x_{t},y_{t},v_{t},\delta_{ \epsilon};\bar{\xi}_{t})+(1-\eta_{t}^{f})\big{(}h_{t-1}^{f}+\widetilde{\nabla} f(x_{t},y_{t},v_{t},\delta_{\epsilon};\bar{\xi}_{t})\] \[-\widetilde{\nabla}f(x_{t-1},y_{t-1},v_{t-1},\delta_{\epsilon}; \bar{\xi}_{t})\big{)},\] (10)

where \(\widetilde{\nabla}f(x,y,v,\delta_{\epsilon};\bar{\xi}_{t})\) is the fully first-order hypergradient estimate evaluated at two consecutive iterates \((x_{t},y_{t},v_{t})\) and \((x_{t-1},y_{t-1},v_{t-1})\) is given by

\[\widetilde{\nabla}f(x,y,v,\delta_{\epsilon};\bar{\xi}_{t})= \nabla_{x}f(x,y;\bar{\xi}_{t})-\widetilde{J}(x,y,v,\delta_{\epsilon };\bar{\xi}_{t}),\]

and \(\widetilde{J}(x,y,v,\delta_{\epsilon};\bar{\xi}_{t})\) is the finite-difference Jacobian-vector approximation given by

\[\widetilde{J}(x,y,v,\delta_{\epsilon};\bar{\xi}_{t}):=\frac{\nabla_{x}g(x,y+ \delta_{\epsilon}v;\bar{\xi}_{t})-\nabla_{x}g(x,y-\delta_{\epsilon}v;\bar{\xi} _{t})}{2\delta_{\epsilon}}.\] (11)

Note that \(\widetilde{\nabla}_{v}R\) and \(\widetilde{\nabla}f\) are **biased** estimators of the gradients \(\nabla_{v}R\) and \(\bar{\nabla}f\), which further complicates the convergence analysis on the momentum-based updates because the conventional analysis on the recursive momentum requires the unbiased gradient estimation to ensure the variance reduction effect. By controlling the perturbation \(\delta_{\epsilon}\) properly, we will show that FdeHBO can achieve an \(\mathcal{O}(\epsilon^{-1.5})\) convergence and complexity performance without any second-order derivative computation.

### Extension to Small-Dimensional Case

As a byproduct of our proposed FdeHBO, we further propose a fully single-loop momentum-based bilevel optimizer (FMBO), which is more suitable in the small-dimensional case without finite-difference approximation. As shown in Algorithm2, FMBO first takes the same lower-level updateson \(y_{t}\) as in eq. (6). Then, it solves the LS problem as \(w_{t+1}=v_{t}-\lambda_{t}h_{t}^{R}\), where the momentum-based gradient estimator is given by

\[h_{t}^{R}= \eta_{t}^{R}\nabla_{v}R(x_{t},y_{t},v_{t};\psi_{t})+(1-\eta_{t}^{R} )\big{(}h_{t-1}^{g}+\nabla_{v}R(x_{t},y_{t},v_{t};\psi_{t})\] \[-\nabla_{v}R(x_{t-1},y_{t-1},v_{t-1};\psi_{t})\big{)},\] (12)

where differently from FdeHBO, we here use the precise gradient \(\nabla_{v}R\) without finite-difference approximation. Similarly to FdeHBO, we add an auxiliary projection on the \(v_{t}\) updates to ensure the LS smoothness and bounded variance. Finally, for the upper-level problem, we optimize \(x_{t}\) based on a momentum-based update as \(x_{t+1}=x_{t}-\alpha_{t}h_{t}^{f}\) with the hypergradient estimator

\[h_{t}^{f}= \eta_{t}^{f}\bar{\nabla}f(x_{t},y_{t},v_{t};\bar{\xi}_{t})+(1- \eta_{t}^{f})(h_{t-1}^{f}+\bar{\nabla}f(x_{t},y_{t},v_{t};\bar{\xi}_{t})-\bar {\nabla}f(x_{t-1},y_{t-1},v_{t-1};\bar{\xi}_{t}))\] (13)

where \(\eta_{t}^{f}\in[0,1]\) is a tuning parameter. Similarly, we directly use the hypergradient estimate in eq. (3) without the finite-difference estimation. We note that compared to existing momentum-based algorithms [62; 34] that contains \(\mathcal{O}(\log\frac{1}{\epsilon})\) steps in solving the LS problem, FMBO takes the fully single-loop structure with a single-step momentum-based acceleration on the LS updates.

## 3 Main Results

### Assumptions and Definitions

We make the following standard assumptions for the upper- and lower-level objective functions, as also adopted by [30; 9; 34]. The following assumption imposes the Lipschitz condition on the upper-level function \(f(x,y)\).

**Assumption 1**.: _For any \(x\in\mathbb{R}^{d_{x}}\) and \(y\in\mathbb{R}^{d_{y}}\), there exist positive constants \(L_{f_{x}}\), \(L_{f_{y}}\), \(C_{f_{x}}\) and \(C_{f_{y}}\) such that \(\nabla_{x}f(x,y)\) and \(\nabla_{y}f(x,y)\) are \(L_{f_{x}}\)- and \(L_{f_{y}}\)-Lipschitz continuous w.r.t. \((x,y)\), and \(\|\nabla_{x}f(x,y)\|^{2}\leq C_{f_{x}}\), \(\|\nabla_{y}f(x,y)\|^{2}\leq C_{f_{y}}\)._

The following assumption imposes the Lipschitz condition on the lower-level function \(g(x,y)\).

**Assumption 2**.: _For any \(x\in\mathbb{R}^{d_{x}}\) and \(y\in\mathbb{R}^{d_{y}}\), there exist positive constants \(\mu_{g}\), \(L_{g}\), \(L_{g_{xy}}\), \(L_{g_{yy}}\), \(C_{g_{xy}},C_{g_{yy}}\) such that_

* _Function_ \(g(x,y)\) _is twice continuously differentiable;_
* _Function_ \(g(x,\cdot)\) _is_ \(\mu_{g}\)_-strongly-convex;_
* _The derivatives_ \(\nabla_{y}g(x,y)\)_,_ \(\nabla_{xy}^{2}g(x,y)\) _and_ \(\nabla_{yy}^{2}g(x,y)\) _are_ \(L_{g}\)_-,_ \(L_{g_{xy}}\)_- and_ \(L_{g_{yy}}\)_-Lipschitz continuous w.r.t._ \((x,y)\)_;_
* \(\|\nabla_{xy}^{2}g(x,y)\|^{2}\leq C_{g_{xy}}\) _and_ \(\|\nabla_{yy}^{2}g(x,y)\|^{2}\leq C_{g_{yy}}\)_._

The following assumption is adopted for the stochastic functions \(f(x,y;\xi)\) and \(g(x,y;\zeta)\).

**Assumption 3**.: _Assumptions 1 and 2 hold for \(f(x,y;\xi)\) and \(g(x,y;\zeta)\) for \(\forall\,\xi\) and \(\zeta\). Moreover, we assume that there exist positive constants \(\sigma_{fx}\), \(\sigma_{y}\), \(\sigma_{g}\), \(\sigma_{g_{xy}}\) and \(\sigma_{g_{yy}}\) such that_

\[\mathbb{E}\left[\|\nabla_{x}f(x,y)-\nabla_{x}f(x,y;\xi)\|^{2} \right]\leq\sigma_{f_{x}}^{2},\quad\mathbb{E}\left[\|\nabla_{y}f(x,y)-\nabla_ {y}f(x,y;\xi)\|^{2}\right]\leq\sigma_{f_{y}}^{2},\] \[\mathbb{E}\left[\|\nabla_{y}g(x,y)-\nabla_{y}g(x,y;\zeta)\|^{2} \right]\leq\sigma_{g}^{2},\quad\quad\mathbb{E}\left[\|\nabla_{xy}^{2}g(x,y)- \nabla_{xy}^{2}g(x,y;\xi)\|^{2}\right]\leq\sigma_{g_{xy}}^{2},\] \[\mathbb{E}\left[\|\nabla_{yy}^{2}g(x,y)-\nabla_{yy}^{2}g(x,y; \xi)\|^{2}\right]\leq\sigma_{g_{yy}}^{2}.\]

**Definition 1**.: _We say \(\bar{x}\) is an \(\epsilon\)-accurate stationary point of a function \(\Phi(x)\) if \(\mathbb{E}\|\nabla\Phi(\bar{x})\|^{2}\leq\epsilon\), where \(\bar{x}\) is the output of an optimization algorithm._

### Convergence and Complexity Analysis of FdeHBO

We further provide the convergence analysis for the proposed Hessian/Jacobian-free FdeHBO algorithm. We first characterize several estimation properties of FdeHBO. Let \(e_{t}^{f}:=\widetilde{h}_{t}^{f}-\nabla f(x_{t},y_{t},v_{t})-\Delta(x_{t},y_{ t},v_{t})\) denote the hypergradient estimation error.

**Proposition 1**.: _Under Assumption 3, the iterates of the outer problem by Algorithm 1 satisfy_

\[\mathbb{E}\|e_{t+1}^{f}\|^{2}\leq \Big{[}(1-\eta_{t+1}^{f})^{2}+4L_{g_{xy}}r_{v}^{2}\delta_{\epsilon }\Big{]}\mathbb{E}\|e_{t}^{f}\|^{2}+4(\eta_{t+1}^{f})^{2}\sigma_{f}^{2}+\big{(} 4L_{g_{xy}}r_{v}^{2}\delta_{\epsilon}+16L_{g_{xy}}^{2}r_{v}^{4}\delta_{ \epsilon}^{2}\big{)}\] \[+6(1-\eta_{t+1}^{f})^{2}\Big{[}L_{F}^{2}\alpha_{t}^{2}\mathbb{E} \|\widetilde{h}_{t}^{f}\|^{2}+2L_{F}^{2}\beta_{t}^{2}\big{(}\mathbb{E}\|e_{t} ^{g}\|^{2}+\|\nabla_{y}g(x_{t},y_{t})\|^{2}\big{)}\] \[\qquad\qquad\qquad+2C_{g_{xy}}\lambda_{t}^{2}\big{(}\mathbb{E}\| e_{t}^{R}\|^{2}+L_{g}^{2}\mathbb{E}\|v_{t}-v_{t}^{*}\|^{2}\big{)}\Big{]},\]

_for all \(t\in\{0,...,T-1\}\) with \(L_{F}^{2}=2\big{(}L_{f_{x}}^{2}+L_{g_{xy}}^{2}r_{v}^{2}\big{)}\)._

The hypergradient estimator error \(\mathcal{O}(\mathbb{E}\|e_{t+1}^{f}\|^{2})\) contains three main components. The first term \([(1-\eta_{t+1}^{f})^{2}+4L_{g_{xy}}r_{v}^{2}\delta_{\epsilon}]\mathbb{E}\|e_{ t}^{f}\|^{2}\) indicates the per-iteration improvement induced by the momentum-based update, the error term \(\alpha_{t}^{2}\mathbb{E}\|h_{t}^{f}\|^{2}\) is caused by the \(x_{t}\) updates, the error term \(\mathcal{O}(\beta_{t}^{2}\mathbb{E}\|e_{t}^{g}\|^{2}+\|\nabla_{y}g(x_{t},y_{ t})\|^{2})\) is caused by solving the lower-level problem, and the new error term \(\mathcal{O}(\lambda_{t}^{2}\mathbb{E}\|e_{t}^{R}\|^{2}+L_{g}^{2}\|v_{t}-v_{t}^{ *}\|^{2})\) is induced by the one-step momentum update on the LS problem, which does not exist in previous momentum-based bilevel methods [62, 34, 25] that solve the LS problem to a high accuracy. Also note that the errors \(4L_{g_{xy}}r_{v}^{2}\delta_{\epsilon}\mathbb{E}\|e_{t}^{f}\|^{2}\) and \(4L_{g_{xy}}r_{v}^{2}\delta_{\epsilon}+16L_{g_{xy}}^{2}r_{v}^{4}\delta_{ \epsilon}^{2}\) are caused by the finite-difference approximation error. Fortunately, by choosing the perturbation level \(\delta_{\epsilon}\) in these two terms to be properly small, it can guarantee the descent factor \((1-\eta_{t+1}^{f})^{2}+4L_{g_{xy}}r_{v}^{2}\delta_{\epsilon}\) to be at an order of \((1-\mathcal{O}(\eta_{t+1}^{f}))^{2}\), and hence the momentum-based variance reduction effect is still applied.

**Proposition 2**.: _For \(\forall\,\psi\), define \(e_{t}^{R}:=\widetilde{h}_{t}^{R}-\nabla_{v}R(x_{t},y_{t},v_{t})\). Under Assumptions 1, 2, 3, we have_

\[\mathbb{E}\|e_{t+1}^{R}\|^{2}\leq \big{[}(1-\eta_{t+1}^{R})^{2}(1+96L_{g}^{4}\lambda_{t}^{2})+4L_{g_ {yy}}r_{v}^{2}\delta_{\epsilon}\big{]}\mathbb{E}\|e_{t}^{R}\|^{2}+\big{(}4L_{g_ {yy}}r_{v}^{2}\delta_{\epsilon}+8L_{g_{yy}}^{2}r_{v}^{4}\delta_{\epsilon}^{2} \big{)}\] \[+8(\eta_{t+1}^{R})^{2}(\sigma_{g_{yy}}^{2}r_{v}^{2}+\sigma_{f_{y} }^{2})+96(1-\eta_{t+1}^{R})^{2}L_{g}^{2}\lambda_{t}^{2}\left(\mathbb{E}\|e_{t} ^{R}\|^{2}+L_{g}^{2}\mathbb{E}\|v_{t}-v_{t}^{*}\|^{2}\right)\] \[+96(1-\eta_{t+1}^{R})^{2}\big{(}L_{g_{yy}}^{2}r_{v}^{2}+L_{f_{y} }^{2}\big{)}\Big{[}\alpha_{t}^{2}\mathbb{E}\|\widetilde{h}_{t}^{f}\|^{2}+2 \beta_{t}^{2}(\mathbb{E}\|e_{t}^{g}\|^{2}+\mathbb{E}\|\nabla_{y}g(x_{t},y_{t})\|^ {2})\Big{]}\]

_for all \(t\in\{0,1,...,T-1\}\)._

As shown in Proposition 2, the LS gradient estimation error \(e_{t+1}^{R}\) contains an iteratively improved error component \(\big{[}(1-\eta_{t+1}^{R})^{2}(1+96L_{g}^{4}\lambda_{t}^{2})+4L_{g_{yy}}r_{v}^{2} \delta_{\epsilon}\big{]}\mathbb{E}\|e_{t}^{R}\|^{2}\) for the stepsize \(\lambda_{t}\) and the approximation factor \(\delta_{\epsilon}\) sufficiently small, a finite-difference approximation error \(\mathcal{O}(\delta_{\epsilon})\) as well as an approximation error \(\mathcal{O}(\lambda_{t}^{2}\mathbb{E}\|v_{t}-v_{t}^{*}\|^{2})\) for solving the LS problem. The next step is to upper-bound \(\mathbb{E}\|v_{t}-v_{t}^{*}\|^{2}\).

**Proposition 3**.: _Under the Assumption 1, 2, the iterates of the LS problem by Algorithm 1 satisfy_

\[\mathbb{E}\|v_{t+1} -v_{t+1}^{*}\|^{2}\] \[\leq(1+\gamma_{t}^{\prime})\Big{(}1+\delta_{t}^{\prime}\Big{)} \Big{[}\Big{(}1-2\lambda_{t}\frac{(L_{g}+L_{g}^{3})\mu_{g}}{\mu_{g}+L_{g}}+ \lambda_{t}^{2}L_{g}^{2}\Big{)}\mathbb{E}\|v_{t}-v_{t}^{*}\|^{2}\Big{]}\] \[\quad+(1+\gamma_{t}^{\prime})\Big{(}1+\frac{1}{\delta_{t}^{ \prime}}\Big{)}\lambda_{t}^{2}\mathbb{E}\|e_{t}^{R}\|^{2}\] \[\quad+(1+\frac{1}{\gamma_{t}^{\prime}})\Big{(}\frac{2L_{f_{y}}^{ 2}}{\mu_{g}^{2}}+\frac{2C_{f_{y}}^{2}L_{g_{yy}}^{2}}{\mu_{g}^{4}}\Big{)} \left[\alpha_{t}^{2}\mathbb{E}\|\widetilde{h}_{t}^{f}\|^{2}+\beta_{t}^{2} \left(2\mathbb{E}\|e_{t}^{g}\|^{2}+2\mathbb{E}\|\nabla_{y}g(x_{t},_for all \(t\in\{0,...,T-1\}\) with some \(\gamma_{t}^{\prime}>0\) and \(\delta_{t}^{\prime}>0\)._

Based on the above important properties, we now provide the general convergence theorem for FdeHBO.

**Theorem 1**.: _Suppose Assumptions 1, 2, 3 and Lemma 3 are satisfied. Choose \(r_{v}\geq\frac{C_{f_{v}}}{\mu_{g}}\) and set_

\[\alpha_{t}=\frac{1}{(w+t)^{1/3}},\quad\beta_{t}=c_{\beta}\alpha_{t},\quad \lambda_{t}=c_{\lambda}\alpha_{t},\quad\eta_{t}^{f}=c_{\eta_{f}}\alpha_{t}^{2},\quad\eta_{t}^{R}=c_{\eta_{R}}\alpha_{t}^{2},\quad\eta_{t}^{g}=c_{\eta_{g}} \alpha_{t}^{2},\]

_and \(\delta_{\epsilon}\leq\frac{\min\{c_{\eta_{f}},c_{\eta_{R}}\}}{8(L_{g_{xy}}r_{v }^{2}(w+T-1)^{2/3})}\), where the constants \(w\), \(c_{\beta},c_{\lambda},c_{\eta_{f}},c_{\eta_{R}}\) and \(c_{\eta_{g}}\) are defined in eq. (67) in the appendix. Then, the iterates generated by Algorithm 1 satisfy_

\[\mathbb{E}\|\nabla\Phi\big{(}x_{a}(T)\big{)}\|^{2}\leq\widetilde{ \mathcal{O}}\bigg{(} \frac{\Phi(x_{0})-\Phi^{*}}{T^{2/3}}+\frac{\|y_{0}-y^{*}(x_{0})\|^{2}}{T^{ 2/3}}+\frac{\|v_{0}-v^{*}(x_{0},y_{0})\|^{2}}{T^{2/3}}\] \[+\frac{1}{T^{2/3}}+\frac{\sigma_{f}^{2}}{T^{2/3}}+\frac{\sigma_{g }^{2}}{T^{2/3}}+\frac{\sigma_{R}^{2}}{T^{2/3}}\bigg{)}.\]

**Corollary 1**.: _Under the same setting of Theorem 1, FdeHBO requires \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) samples and gradient evaluations, respectively, to achieve an \(\epsilon\)-accurate stationary point._

It can be seen from Corollary 1 that the proposed FdeHBO achieves an \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) sample complexity without any second-order derivative computation. As far as we know, this is the first Hessian/Jacobian-free stochastic bilevel optimizer with an \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) sample complexity.

### Convergence and Complexity Analysis of FMBO

In this section, we analyze the convergence and complexity of the simplified FMBO method.

**Theorem 2**.: _Suppose Assumptions 1, 2 and 3 are satisfied. Choose \(r_{v}\geq\frac{C_{f_{v}}}{\mu_{g}}\) and set parameters_

\[\alpha_{t}=\frac{1}{(w+t)^{1/3}},\quad\beta_{t}=c_{\beta}\alpha_{t},\quad \lambda_{t}=c_{\lambda}\alpha_{t},\]

\[\eta_{t}^{f}=c_{\eta_{f}}\alpha_{t}^{2},\quad\eta_{t}^{R}=c_{\eta_{R}}\alpha_{ t}^{2},\quad\eta_{t}^{g}=c_{\eta_{g}}\alpha_{t}^{2}\]

_where \(w\), \(c_{\beta},c_{\lambda},c_{\eta_{f}},c_{\eta_{R}}\) and \(c_{\eta_{g}}\) are defined in eq. (33) in the appendix. The iterates generated by Algorithm 2 satisfy_

\[\mathbb{E}\|\nabla\Phi(x_{a}(T))\|^{2}\leq \widetilde{\mathcal{O}}\Bigg{(}\frac{\Phi(x_{0})-\Phi^{*}}{T^{2/3 }}+\frac{\|y_{0}-y^{*}(x_{0})\|^{2}}{T^{2/3}}\] \[+\frac{\|v_{0}-v^{*}(x_{0},y_{0})\|^{2}}{T^{2/3}}+\frac{\sigma_{f }^{2}}{T^{2/3}}+\frac{\sigma_{g}^{2}}{T^{2/3}}+\frac{\sigma_{R}^{2}}{T^{2/3}} \Bigg{)}.\]

Theorem 2 shows that the proposed fully single-loop FMBO achieves a convergence rate of \(\frac{1}{T^{2/3}}\), which further yields the following complexity result.

**Corollary 2**.: _Under the same setting of Theorem 2, FMBO requires totally \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) data samples, gradient and matrix-vector evaluations, respectively, to achieve an \(\epsilon\)-accurate stationary point._

Corollary 2 shows that FMBO requires a total number \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) of data samples, which matches the best sample complexity in [34; 62; 28]. More importantly, each iteration of FMBO contains only one Hessian-vector computation due to the simple fully single-loop implementation, whereas other momentum-based approaches require \(\mathcal{O}(\log\frac{1}{\epsilon})\) Hessian-vector computations in a nested manner per iteration. Also, note that FMBO is the first fully single-loop bilevel optimizer that achieves the \(\widetilde{\mathcal{O}}(\epsilon^{-1.5})\) sample complexity.

## 4 Experiments

In this section, we test the performance of the proposed FdeHBO and FMBO on two applications: hyper-representation and data hyper-cleaning, respectively.

### Hyper-representation on MNIST Dataset

We now compare the performance of our Hessian/Jacobian-free FdeHBO with the relevant Hessian/Jacobian-free methods PZOBO-S [58], F\({}^{2}\)SA [37] and F\({}^{3}\)SA [37]. We perform the hyper-representation with the \(7\)-layer LeNet network [38], which aims to solve the following bilevel problem.

\[\min_{\lambda}L_{\nu}(\lambda):=\frac{1}{|S_{\nu}|}\sum_{(x_{i},y_{ i})\in S_{\nu}}L_{CE}(w^{*}(\lambda)f(\lambda;x_{i}),y_{i})\] \[s.t.\ \ w^{*}(\lambda)=\operatorname*{arg\,min}_{w}L_{in}(\lambda,w), \ \ \ L_{in}(\lambda,w):=\frac{1}{|S_{\tau}|}\sum_{(\tau,y_{i})\in S_{\tau}}L_{CE}( wf(\lambda,x_{i}),y_{i}),\]

where \(L_{CE}\) denotes the cross-entropy loss, \(S_{\nu}\) and \(S_{\tau}\) denote the training data and validation data, and \(f(\lambda;x_{i})\) denotes the features extracted from the data \(x_{i}\). More details of the experimental setups are specified in Appendix A.1.

As shown in Figure 1, our FdeHBO converges much faster and more stably than PZOBO-S, F\({}^{2}\)SA and F\({}^{3}\)SA, while achieving a higher training accuracy. This is consistent with our theoretical results, and validates the momentum-based approaches in reducing the variance during the entire training.

### Hyper-cleaning on MNIST Dataset

We compare the performance of our FMBO to various bilevel algorithms including AID-FP [21], reverse[17], SUSTAIN [34], MRBO and VRBO [62], BSA [19], stocBio [30], FSLA [40] and SOBA [11], on a low-dimensional data hyper-cleaning problem with a linear classifier on MNIST dataset, which takes the following formulation.

\[\min_{\lambda}L_{\nu}(\lambda,w^{*})=\frac{1}{|S_{\nu}|}\sum_{(x_{ i},y_{i})\in S_{\nu}}L_{CE}((w^{*})^{T}x_{i},y_{i})\] \[s.t.\ \ \ w^{*}=\operatorname*{arg\,min}_{w}L(\lambda,w):=\frac{1}{|S_ {\tau}|}\sum_{(x_{i},y_{i})\in S_{\tau}}\sigma(\lambda_{i})L_{CE}(w^{T}x_{i}, y_{i})+C\|w\|^{2},\] (14)

where \(L_{CE}\) denotes the cross-entropy loss, \(S_{\nu}\) and \(S_{\tau}\) denote the training data and validation data, whose sizes are set to 20000 and 5000, respectively, \(\lambda=\{\lambda_{i}\}_{i\in S_{\tau}}\) and \(C\) are the regularization parameters, and \(\sigma(\cdot)\) is the sigmoid function. AmIGO [3] is not included in the figures because it performs similarly to stocBio. The experimental details can be found in Appendix A.2.

As shown in Figure 2(a), FMBO, stocBio and AID-FP converge much faster and more stable than other algorithms. Compared to stocBio and AID-FP, FMBO achieves a lower training loss. This demonstrates the effectiveness of momentum-based variance reduction in finding more accurate iterates. It can be seen from Figure 2(b) that FMBO converges faster than existing fully single-loop FSLA and SOBA algorithms with a lower training loss.

## 5 Conclusion

In this paper, we propose a novel Hessian/Jacobian-free bilevel optimizer named FdeHBO. We show that FdeHBO achieves an \(\mathcal{O}(\epsilon^{-1.5})\) sample complexity, which outperforms existing algorithms of the

Figure 1: Comparison on hyper-representation with the LeNet neural network. Left plot: outer loss v.s. running time; right plot: accuracy v.s. running time.

same type by a large margin. Our experiments validate the theoretical results and the effectiveness of the proposed algorithms. We anticipate that the developed analysis will shed light on developing provable Hessian/Jacobian-free bilevel optimization algorithms and the proposed algorithms may be applied to other applications such as fair machine learning.

## Acknowledgement

The work is supported in part by NSF under grants 2326592 and 2311274.

## References

* [1] Z. Allen-Zhu and Y. Li. Neon2: Finding local minima via first-order oracles. _Advances in Neural Information Processing Systems_, 31, 2018.
* [2] A. Antoniou, H. Edwards, and A. Storkey. How to train your MAML. In _International Conference on Learning Representations (ICLR)_, 2019.
* [3] M. Arbel and J. Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In _International Conference on Learning Representations (ICLR)_, 2022.
* [4] L. Bertinetto, J. F. Henriques, P. Torr, and A. Vedaldi. Meta-learning with differentiable closed-form solvers. In _International Conference on Learning Representations (ICLR)_, 2018.
* [5] M. Blondel, Q. Berthet, M. Cuturi, R. Frostig, S. Hoyer, F. Llinares-Lopez, F. Pedregosa, and J.-P. Vert. Efficient and modular implicit differentiation. _arXiv preprint arXiv:2105.15183_, 2021.
* [6] J. Bracken and J. T. McGill. Mathematical programs with optimization problems in the constraints. _Operations Research_, 21(1):37-44, 1973.
* [7] Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Accelerated methods for nonconvex optimization. _SIAM Journal on Optimization_, 28(2):1751-1772, 2018.
* [8] L. Chen, J. Xu, and J. Zhang. On bilevel optimization without lower-level strong convexity. _arXiv preprint arXiv:2301.00712_, 2023.
* [9] T. Chen, Y. Sun, and W. Yin. A single-timescale stochastic bilevel optimization method. _arXiv preprint arXiv:2102.04671_, 2021.
* [10] A. Cutkosky and F. Orabona. Momentum-based variance reduction in non-convex SGD. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [11] M. Dagreou, P. Ablin, S. Vaiter, and T. Moreau. A framework for bilevel optimization that enables stochastic and global variance reduction algorithms. _arXiv preprint arXiv:2201.13409_, 2022.

Figure 2: (a) Comparison of different algorithms on data hyper-cleaning with noise \(p=0.1\). Left plot: test loss v.s. running time; right plot: train loss v.s. running time. (b) Comparison among different single-loop algorithms: training loss v.s. running time.

* [12] J. Domke. Generic methods for optimization-based modeling. In _Artificial Intelligence and Statistics (AISTATS)_, pages 318-326, 2012.
* [13] A. Fallah, A. Mokhtari, and A. Ozdaglar. On the convergence theory of gradient-based model-agnostic meta-learning algorithms. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 1082-1092. PMLR, 2020.
* [14] M. Feurer and F. Hutter. Hyperparameter optimization. In _Automated Machine Learning_, pages 3-33. Springer, Cham, 2019.
* [15] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _Proc. International Conference on Machine Learning (ICML)_, pages 1126-1135, 2017.
* [16] R. Flamary, A. Rakotomamonjy, and G. Gasso. Learning constrained task similarities in graphregularized multi-task learning. _Regularization, Optimization, Kernels, and Support Vector Machines_, page 103, 2014.
* [17] L. Franceschi, M. Donini, P. Frasconi, and M. Pontil. Forward and reverse gradient-based hyperparameter optimization. In _International Conference on Machine Learning (ICML)_, pages 1165-1173, 2017.
* [18] L. Franceschi, P. Frasconi, S. Salzo, R. Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In _International Conference on Machine Learning (ICML)_, pages 1568-1577, 2018.
* [19] S. Ghadimi and M. Wang. Approximation methods for bilevel programming. _arXiv preprint arXiv:1802.02246_, 2018.
* [20] S. Gould, B. Fernando, A. Cherian, P. Anderson, R. S. Cruz, and E. Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. _arXiv preprint arXiv:1607.05447_, 2016.
* [21] R. Grazzi, L. Franceschi, M. Pontil, and S. Salzo. On the iteration complexity of hypergradient computation. In _Proc. International Conference on Machine Learning (ICML)_, 2020.
* [22] A. Griewank and A. Walther. _Evaluating derivatives: principles and techniques of algorithmic differentiation_. SIAM, 2008.
* [23] B. Gu, G. Liu, Y. Zhang, X. Geng, and H. Huang. Optimizing large-scale hyperparameters via automated learning algorithm. _arXiv preprint arXiv:2102.09026_, 2021.
* [24] Z. Guo, Y. Xu, W. Yin, R. Jin, and T. Yang. On stochastic moving-average estimators for non-convex optimization. _arXiv preprint arXiv:2104.14840_, 2021.
* [25] Z. Guo and T. Yang. Randomized stochastic variance-reduced methods for stochastic bilevel optimization. _arXiv preprint arXiv:2105.02266_, 2021.
* [26] P. Hansen, B. Jaumard, and G. Savard. New branch-and-bound rules for linear bilevel programming. _SIAM Journal on Scientific and Statistical Computing_, 13(5):1194-1217, 1992.
* [27] M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic. _arXiv preprint arXiv:2007.05170_, 2020.
* [28] F. Huang and H. Huang. Bialam: Fast adaptive bilevel optimization methods. _arXiv preprint arXiv:2106.11396_, 2021.
* [29] K. Ji and Y. Liang. Lower bounds and accelerated algorithms for bilevel optimization. _arXiv preprint arXiv:2102.03926_, 2021.
* [30] K. Ji, J. Yang, and Y. Liang. Bilevel optimization: Convergence analysis and enhanced design. In _International Conference on Machine Learning (ICML)_, pages 4882-4892. PMLR, 2021.
* [31] K. Ji and L. Ying. Network utility maximization with general and unknown utility functions: A distributed, data-driven bilevel optimization approach. _Submitted_, 2022.

* [32] C. Jin, R. Ge, P. Netrapalli, S. M. Kakade, and M. I. Jordan. How to escape saddle points efficiently. In _International conference on machine learning_, pages 1724-1732. PMLR, 2017.
* [33] N. Jorge and J. W. Stephen. _Numerical optimization_. Spinger, 2006.
* [34] P. Khanduri, S. Zeng, M. Hong, H.-T. Wai, Z. Wang, and Z. Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:30271-30283, 2021.
* [35] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In _Advances in neural information processing systems (NeurIPS)_, pages 1008-1014, 2000.
* [36] G. Kunapuli, K. P. Bennett, J. Hu, and J.-S. Pang. Classification model selection via bilevel programming. _Optimization Methods & Software_, 23(4):475-489, 2008.
* [37] J. Kwon, D. Kwon, S. Wright, and R. Nowak. A fully first-order method for stochastic bilevel optimization. _arXiv preprint arXiv:2301.10945_, 2023.
* [38] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [39] J. Li, B. Gu, and H. Huang. Improved bilevel model: Fast and optimal algorithm with theoretical guarantee. _arXiv preprint arXiv:2009.00690_, 2020.
* [40] J. Li, B. Gu, and H. Huang. A fully single loop algorithm for bilevel optimization without hessian inverse. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7426-7434, 2022.
* [41] R. Liao, Y. Xiong, E. Fetaya, L. Zhang, K. Yoon, X. Pitkow, R. Urtasun, and R. Zemel. Reviving and improving recurrent back-propagation. In _Proc. International Conference on Machine Learning (ICML)_, 2018.
* [42] B. Liu, M. Ye, S. Wright, P. Stone, and Q. Liu. Bome! bilevel optimization made easy: A simple first-order approach. _Advances in Neural Information Processing Systems_, 35:17248-17262, 2022.
* [43] H. Liu, K. Simonyan, and Y. Yang. Darts: Differentiable architecture search. In _International Conference on Learning Representations (ICLR)_, 2018.
* [44] R. Liu, J. Gao, J. Zhang, D. Meng, and Z. Lin. Investigating bi-level optimization for learning and vision from a unified perspective: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2021.
* [45] R. Liu, X. Liu, X. Yuan, S. Zeng, and J. Zhang. A value-function-based interior-point method for non-convex bi-level optimization. In _International Conference on Machine Learning (ICML)_, 2021.
* [46] R. Liu, P. Mu, X. Yuan, S. Zeng, and J. Zhang. A generic first-order algorithmic framework for bi-level programming beyond lower-level singleton. In _International Conference on Machine Learning (ICML)_, 2020.
* [47] D. Maclaurin, D. Duvenaud, and R. Adams. Gradient-based hyperparameter optimization through reversible learning. In _International Conference on Machine Learning (ICML)_, pages 2113-2122, 2015.
* [48] A. Nichol, J. Achiam, and J. Schulman. On first-order meta-learning algorithms. _arXiv preprint arXiv:1803.02999_, 2018.
* [49] F. Pedregosa. Hyperparameter optimization with approximate gradient. In _International Conference on Machine Learning (ICML)_, pages 737-746, 2016.
* [50] A. Rajeswaran, C. Finn, S. M. Kakade, and S. Levine. Meta-learning with implicit gradients. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 113-124, 2019.

* [51] S. Sabach and S. Shtern. A first order method for solving convex bilevel optimization problems. _SIAM Journal on Optimization_, 27(2):640-660, 2017.
* [52] A. Shaban, C.-A. Cheng, N. Hatch, and B. Boots. Truncated back-propagation for bilevel optimization. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 1723-1732, 2019.
* [53] H. Shen and T. Chen. On penalty-based bilevel gradient descent method. _arXiv preprint arXiv:2302.05185_, 2023.
* [54] C. Shi, J. Lu, and G. Zhang. An extended kuhn-tucker approach for linear bilevel programming. _Applied Mathematics and Computation_, 162(1):51-63, 2005.
* [55] A. Sinha, P. Malo, and K. Deb. A review on bilevel optimization: from classical to evolutionary approaches and applications. _IEEE Transactions on Evolutionary Computation_, 22(2):276-295, 2017.
* [56] X. Song, W. Gao, Y. Yang, K. Choromanski, A. Pacchiano, and Y. Tang. ES-MAML: Simple Hessian-free meta learning. In _International Conference on Learning Representations (ICLR)_, 2019.
* [57] D. Sow, K. Ji, Z. Guan, and Y. Liang. A constrained optimization approach to bilevel optimization with multiple inner minima. _arXiv preprint arXiv:2203.01123_, 2022.
* [58] D. Sow, K. Ji, and Y. Liang. On the convergence theory for Hessian-free bilevel algorithms. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [59] D. A. Tarzanagh, M. Li, C. Thrampoulidis, and S. Oymak. Fednest: Federated bilevel, minimax, and compositional optimization. _arXiv preprint arXiv:2205.02215_, 2022.
* [60] Q. Tran-Dinh, N. H. Pham, D. T. Phan, and L. M. Nguyen. Hybrid stochastic gradient descent algorithms for stochastic nonconvex optimization. _arXiv preprint arXiv:1905.05920_, 2019.
* [61] R. Vuorio, S.-H. Sun, H. Hu, and J. J. Lim. Multimodal model-agnostic meta-learning via task-aware modulation. _Advances in Neural Information Processing Systems (NeurIPS)_, 32, 2019.
* [62] J. Yang, K. Ji, and Y. Liang. Provably faster algorithms for bilevel optimization. _Advances in Neural Information Processing Systems (NeurIPS)_, 34:13670-13682, 2021.
* [63] M. Ye, B. Liu, S. Wright, P. Stone, and Q. Liu. Bome! bilevel optimization made easy: A simple first-order approach. In _Conference on Neural Information Processing Systems (NeurIPS)_, page 1, 2022.