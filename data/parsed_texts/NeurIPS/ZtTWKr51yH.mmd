Constrained Adaptive Attack: Effective Adversarial Attack Against Deep Neural Networks for Tabular Data

 Thibault Simonetto

University of Luxembourg

 Luxembourg

thibault.simonetto@uni.lu

&Salah Ghamizi

LIST / RIKEN AIP

 Luxembourg

 salah.ghamizi@list.lu

&Maxime Cordy

University of Luxembourg

 Luxembourg

 maxime.cordy@uni.lu

###### Abstract

State-of-the-art deep learning models for tabular data have recently achieved acceptable performance to be deployed in industrial settings. However, the robustness of these models remains scarcely explored. Contrary to computer vision, there are no effective attacks to properly evaluate the adversarial robustness of deep tabular models due to intrinsic properties of tabular data, such as categorical features, immutability, and feature relationship constraints. To fill this gap, we first propose CAPGD, a gradient attack that overcomes the failures of existing gradient attacks with adaptive mechanisms. This new attack does not require parameter tuning and further degrades the accuracy, up to 81% points compared to the previous gradient attacks. Second, we design CAA, an efficient evasion attack that combines our CAPGD attack and MOEVA, the best search-based attack. We demonstrate the effectiveness of our attacks on five architectures and four critical use cases. Our empirical study demonstrates that CAA outperforms all existing attacks in 17 over the 20 settings, and leads to a drop in the accuracy by up to 96.1% points and 21.9% points compared to CAPGD and MOEVA respectively while being up to five times faster than MOEVA. Given the effectiveness and efficiency of our new attacks, we argue that they should become the minimal test for any new defense or robust architectures in tabular machine learning.

## 1 Introduction

Evasion attack is the process of slightly altering an original input into an _adversarial example_ designed to force a machine learning (ML) model to output a wrong decision. Robustness to adversarial examples is a problem of growing concern among the secure ML community, with over 10,000 publications on the subject since 2014 [7]. Recent studies also report real-world occurrences of evasion attacks, which demonstrate the importance of studying and defending against this phenomenon [20].

While research has studied the robustness of deep learning models in Computer Vision (CV) and Natural Language Processing (NLP) tasks, many real-world applications instead deal with tabular data, including in critical fields like finance, energy, and healthcare. If classical "shallow" models (e.g. random forests) have been the go-to solution to learn from tabular data [21], deep learning models are becoming competitive [5]. This raises anew the need to study the robustness of these models.

However, robustness assessment for tabular deep learning models brings a number of new challenges that previous solutions -- because they were originally designed for CV or NLP tasks -- do not consider. One such challenge is the fact that tabular data exhibit _feature constraints_, i.e. complex relationships and constraints across features. The satisfaction of these feature constraints can be a non-convex or even non-differentiable problem; this implies that established evasion attack algorithms relying on gradient computation do not create valid adversarial examples (i.e., constraint satisfying) [18]. Meanwhile, attacks designed for tabular data also ignore feature type constraints [3] or, in the best case, consider categorical features without feature relationships [40; 41; 4] and are evaluated on datasets that exclusively contain such features. This restricts their application to other domains that present heterogeneous feature types.

The only published evasion attacks that support feature constraints are _Constrained Projected Gradient Descent_ (CPGD) and _Multi-Objective Evolutionary Adversarial Attack_ (MOEVA) [35]. CPGD is an extension of the classical gradient-based PGD attack with a new loss function that encodes how far the generated examples are from satisfying the constraints. Although theoretically elegant and practically efficient, this attack suffers from a low success rate due to its difficulty to converge toward both model classification and constraint satisfaction [35]. Conversely, MOEVA is based on genetic algorithms. It offers an outstanding success rate compared to CPGD and works on shallow and deep learning models. However, it is computationally expensive and requires numerous hyper-parameters to be tuned (population size, mutation rate, generations, etc.). This prevents this attack from scaling to larger models and datasets.

Overall, research on adversarial robustness for tabular machine learning in general (and tabular deep learning in particular) is still in its infancy. This is in stark contrast to the abundant literature on adversarial robustness in CV [28] and NLP tasks [15]. Given this limited state of knowledge, the **objective** of this paper is to propose novel and effective attack methods for tabular models subject to feature constraints.

We hypothesize that gradient-based algorithms have not been explored adequately in [35] and that the introduction of dedicated adaptive mechanisms can outperform CPGD. To verify this, we design a new adaptive attack, named _Constrained Adaptive PGD_ (CAPGD), whose only free parameter is the number of iterations and that does not require additional parameter tuning (Section 4). We demonstrate that the different mechanisms we introduced in CAGPD contribute to improving the success rate of this attack compared to CPGD, by 81% points. Across all our datasets, the set of adversarial examples that CAPGD generates subsumes all the examples generated by any other gradient-based method. Furthermore, CAPGD is 75 times faster than MOEVA, while the latter reaches the highest success rate across all datasets.

These results motivate us to design _Constrained Adaptive Attack_ (CAA), an adaptive attack that combines our new gradient-based attack (CAPGD) with MOEVA for an increased success rate at a lower computational cost. Our experiments show that CAA reaches the highest success rate for all models/datasets we considered, except in one case where CAA is second-best. With this attack, we offer a strong baseline for future research on evasion attacks for tabular models, which should become the minimal test for robust tabular architectures and other defense mechanisms.

**Our contributions can be summarized as follows:**

1. We design a new parameter-free attack, CAPGD that introduces momentum and adaptive steps to effectively evade tabular models while enforcing the feature constraints. We show that CAPGD outperforms the other gradient-based attacks in terms of capability to generate valid (constraint-satisfying) adversarial examples.
2. We propose a new efficient and effective evasion attack (CAA) that combines gradient and search attacks to optimize both effectiveness and computational cost.
3. We evaluate CAA in a large-scale evaluation over four datasets, five architectures, and two training methods (standard and adversarial training). Our results show that CAA outperforms all other attacks and is up to 5 times more efficient.

## 2 Related work

### Tabular Deep Learning

Tabular data remains the most commonly used form of data [34], especially in critical applications such as medical diagnosis [38; 36], financial applications [18; 11; 8], user recommendation systems [46], cybersecurity [9; 1], and more. Improving the performance and robustness of tabular machine learning models for these applications is becoming critical as more ML-based solutions are cleared to be deployed in critical settings.

Borisov et al. [5] showed that traditional deep neural networks tend to yield less favorable results in handling tabular data when compared to more shallow machine learning methods, such as XGBoost. However, recent approaches like RLN [33] and TabNet [2] are catching up and even outperforming shallow models in some settings. We argue that DNNs for Tabular Data are sufficiently mature and competitive with shallow models and require therefore a thorough investigation of their safety and robustness. Our work is the first exhaustive study of these critical properties.

### Realistic Adversarial Examples

Initially applied to computer vision, adversarial examples have also been adapted and evaluated on tabular data. Ballet et al. [3] considered feature importance to craft the attacks, Mathov et al. [30] considered mutability, type, boundary, and data distribution constraints, Kireev et al. [24] suggested considering both the cost and benefit of perturbing each feature, and Simonetto et al. [35] introduced domain-constraints (relations between features) as a critical element of the attack.

While domain constraints satisfaction is essential for successful attacks, research on robustness for industrial settings (eg Ghamizi et al. [18] with a major bank) also demonstrated that imperceptibility remains important for critical systems with human-in-the-loop mechanisms, which could deflect attacks with manual checks from human operators. Imperceptibility is domain-specific, and multiple approaches have been suggested [3; 24; 16]. None of these approaches was confronted with human assessments or compared with each other, and in our study, we decided to use the most established \(L_{2}\) norm. Our algorithms and approaches are generalizable to further distance metrics and imperceptibility definitions.

Overall, except the work from Simonetto et al. [35], none of the existing attacks for tabular machine learning supports the feature relationships inherent to realistic tabular datasets, as summarized in Table 1. Nevertheless, we evaluate all the approaches that support continuous values and where a public implementation is available to confirm our claims: LowProFool, BF*, CPGD, and MOEVA.

## 3 Problem formulation

We formulate the problem of evasion attacks under constraints. We assume the attack to be untargeted (i.e. it aims to force misclassification in any incorrect class); the formulation for targeted attacks is similar and omitted for space reasons.

We denote by \(x\in\mathbb{R}^{d}\) an input example and by \(y\in\{1,\dots,C\}\) its correct label. Let \(h:\mathbb{R}^{d}\rightarrow\mathbb{R}^{C}\) be a classifier and \(h_{c_{k}}(x)\) the classification score that \(h\) outputs for input \(x\) to be in class \(c_{k}\). Let \(\Delta\subseteq\mathbb{R}^{d}\) be the space of allowed perturbations. Then, the objective of an evasion attack is to find a \(\delta\in\Delta\) such that \(argmax_{c\in\{1,\dots,C\}}h_{c}(x+\delta)\neq y\).

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline Attack & Supported features & \multicolumn{3}{c}{Supported constraints} \\  & & Categorical & Discrete & Relations \\ \hline
**LowProFool (LPF)**[3] & Continuous & No & No & No \\ Cartella et al. [8] & Continous, Discrete, Categorical & Yes & Yes & No \\ Gressel et al. [19] & Continous, Discrete, Categorical & Yes & Yes & No \\ Xu et al. [41] & Categorical & Yes & No & No \\ Wang et al. [40] & Categorical & Yes & No & No \\ Bao et al. [4] & Categorical & Yes & No & No \\
**BF*/BFS**[26; 25] & Continous, Discrete, Categorical & Yes & Yes & No \\ Mathov et al. [30] & Continous, Discrete, Categorical & Yes & Yes & No \\
**CPGD, MOEVA**[35] & Continous, Discrete, Categorical & Yes & Yes & Yes \\
**CAPGD, CAA (OURS)** & Continous, Discrete, Categorical & Yes & Yes & Yes \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evasion attacks for tabular machine learning. Attacks with a public implementation in bold.

In image classification, the set \(\Delta\) is typically chosen as the perturbations within some \(l_{p}\)-ball around \(x\), that is, \(\Delta_{p}=\{\delta\in\mathbb{R}^{d},||\delta||_{p}\leq\epsilon\}\) for a maximum perturbation threshold \(\epsilon\). This restriction aims at preserving the semantics of the original input by assuming that small enough perturbations will yield images that humans perceive the same as the original images and would therefore classify the perturbed input into the same class (while the classifier predicts another class). This also guarantees that the example remains meaningful, that is, \(x+\delta\) is not an image with random noise.

Tabular data are by nature different from images. They typically represent objects of the considered application domain (e.g. botnet traffic [10], financial transaction [18]). We denote by \(\varphi:Z\rightarrow\mathbb{R}^{d}\) the feature mapping function that maps objects of the problem space \(Z\) to a \(d\)-dimensional feature space defined by the feature set \(F=\{f_{1},f_{2},...f_{d}\}\). Each object \(z\in Z\) must inherently respect some natural condition to be valid (to be able to exist in reality). In the feature space, these conditions translate into a set of constraints on the feature values, which we denote by \(\Omega\). By construction, any input example \(x\) obtained from a real-world object \(z\) satisfies \(\Omega\), noted \(x\models\Omega\).

Thus, in the case of tabular data, we additionally require the perturbation \(\delta\) applied to \(x\) to yield a valid example \(x+\delta\) satisfying \(\Omega\), that is, \(\Delta_{p}(x)=\{\delta\in\mathbb{R}^{d}:||\delta||_{p}\leq\epsilon\wedge x+ \delta\models\Omega\}\).

To define the constraint language expressing \(\Omega\), we consider the four types of constraint introduced by Simonetto et al. [35]. These four constraint types cover all the constraints of the datasets in our empirical study. Hence, _immutability_ defines what features cannot be changed by an attacker; _boundaries_ defines upper / lower bounds for feature values; _type_ specifies a feature to take continuous, discrete, or categorical values; and _feature relationships_ capture numerical relations between features. Feature relationship constraints can be expressed with the following grammar:

\[\omega \coloneqq\omega_{1}\wedge\omega_{2}\mid\omega_{1}\lor\omega_{2} \mid\psi_{1}\succeq\psi_{2}\] (1) \[\psi \coloneqq c\mid f_{i}\mid\psi_{1}\oplus\psi_{2}\mid x_{i}\] (2)

Equation 1 means that a constraint formula \(\omega\) can either be an intersection (\(\land\)), or a union (\(\lor\)) of two other constraint formulae \(\omega_{1}\), \(\omega_{2}\), or \(\omega\) can be a comparison operator \(\succeq\in\{<,\leq,=,\neq,\geq,>\}\) between two values \(\psi_{1}\) and \(\psi_{2}\).

Equation 2 details the numeric expressions that are supported by the grammar. A numeric expression \(\psi\) can be constant \(c\), an operation \(\oplus\in\{+,-,*,/\}\) between two other numerical expressions \(\psi_{1}\) and \(\psi_{2}\), or a specific feature \(f_{i}\), or the \(i\)-th feature of the clean sample. The difference between \(f_{i}\) and \(x_{i}\) is that \(f_{i}\) corresponds to the current value of the evaluated example and \(x_{i}\) corresponds to its original value in the clean example.

Let's consider one complex constraint from the LCLD credit scoring use case: the term of the loan can only be 36 or 60 months and the number of open accounts is lower than the number of allowed accounts for this client. Such a constraint can be formally written as:

\[\omega_{1}=((f_{\text{term}}=36)\lor(f_{\text{term}}=60))\land(f_{\text{open\_ acc}}\leq f_{\text{total\_acc}})\] (3)

We provide other examples in Appendix A.1.

### Constrained Projected Gradient Descent

Constrained Projected Gradient Descent (CPGD) is an extension of the PGD attack [29] to generate adversarial examples satisfying constraints in tabular machine learning. It integrates constraint satisfaction into the loss function that PGD optimizes. This is achieved by translating each constraint \(\omega\) into a differentiable function \(penalty(x,\omega)\) that values to zero if \(x\models\omega\); otherwise, the function represents how far \(x\) is from satisfying \(\omega\). We follow the definition of Table 5 in Appendix A.1 to translate each construct of the constraints grammar into a penalty function.

For instance, the penalty function of \(\omega_{1}\) in Equation 3 is:

\[penalty(x,\omega_{1})=min(|f_{\text{term}}-36|,|f_{\text{term}}-60|)+max(0,f_{ \text{open\_acc}}-f_{\text{total\_acc}})\] (4)

Based on this, CPGD produces adversarial examples from an initial sample \(x_{orig}\) classified as \(y\) by iteratively computing:\[x^{(k+1)}=R_{\Omega}\Big{(}P_{\mathcal{S}}\big{(}x^{(k)}+\eta^{(k)}\nabla\mathcal{L }(x^{(k)},y,h,\Omega)\big{)}\Big{)}\] (5)

where \(x^{0}=x_{orig}\) (the original input), \(R_{\Omega}\) is a domain-specific repair operator [35], \(P_{\mathcal{S}}\) is the projection onto \(\mathcal{S}=\{x\in\mathbb{R}^{d},||x-x_{orig}||_{p}\leq\epsilon\}\), \(\nabla\mathcal{L}\) is the gradient of loss function \(\mathcal{L}\), defined as

\[\mathcal{L}(x,y,h,\Omega)=l(h(x),y)-\sum_{\omega_{i}\in\Omega}penalty(x,\omega _{i}).\] (6)

In the original CPGD implementation, the step size \(\eta^{(k)}\) follows a predefined decay schedule, \(\eta^{(k)}=\epsilon\times 10^{-(1+\lfloor k/\lfloor K/M\rfloor))}\), with \(M=7\), and \(K=max(k)\). \(\mathcal{L}^{\prime}(x)\) abbreviates \(\mathcal{L}(x,y,h,\Omega)\).

### Experimental settings

Our experiments are driven by the following datasets, models, and attack parameters. More details about the datasets and models are given in Appendix A.5.

DatasetsTo conduct our study, we selected tabular datasets that present feature constraints from their respective application domain. **URL**[22] is a dataset of legitimate and phishing URLs. With only 14 linear domain constraints and 63 features, it is the simplest of our empirical study. **LCLD**[17] is a credit-scoring dataset with non-linear constraints. The **WiDS**[27] dataset contains medical data on the survival of patients admitted to the ICU. It has only 30 linear domain constraints. The **CTU**[10] dataset reports legitimate and botnet traffic from CTU University. The challenge of this dataset lies in its large number of linear domain constraints (360). We detail the datasets in the Appendix A.4.

ArchitecturesWe evaluate five top-performing architectures from a recent survey on tabular ML [5]: **TabTransformer**[23] and **TabNet**[2] are transformer-based models. **RLN**[33] uses a regularization coefficient to minimize a counterfactual loss. **STG**[43] optimizes feature selection with stochastic gates, and **VIME**[44] relies on self-supervised learning. These architectures achieve performance equivalent to XGBoost, the best shallow machine learning model for our use cases.

Perturbation parametersWe use the L2-norm to measure the distance between original and perturbed inputs, because this norm is suitable for both numerical and categorical features. We set \(\epsilon\) to 0.5 for all datasets. Each dataset has a critical (negative) class, respectively phishing URLs, rejected loans, flagged botnets, and not surviving patients. Hence, we only attack clean examples from the critical class that are not already misclassified by the model and report robust accuracy of models.

Evaluation metricsWe measure the effectiveness of our attack using robust accuracy defined as the accuracy of valid examples generated by a given attack. If a clean example is misclassified, we do not perturb it. If the attack generates an invalid example, we consider it as correctly classified. We measure the efficiency of the attacks in computational time.

## 4 Our Constrained _Adaptive_ PGD

The relative lack of effectiveness of CPGD as reported in its original publication leads us to investigate the cause of these weaknesses. We investigate four factors that may affect the success rate of the attack: (1) we conjecture that the fixed step size and predefined decay in CPGD might be suboptimal because the choice of the step size is known to largely impact the effectiveness of gradient-based attacks [31]; (2) CPGD is unaware of the trend, i.e. it does not consider whether the optimization is evolving successfully and is not able to react to it; (3) CPGD does not check constraint satisfaction between the iterations, which could "lock" the algorithm into a part of the invalid data space; (4) CPGD starts with the original example, whereas classical gradient-based attacks often benefit from random initialization.

### CAPGD algorithm

We propose Constrained Adaptive PGD (CAPGD), a new constraint-aware gradient-based attack that aims to overcome the limitations of CPGD and improve its effectiveness. We detail CAPGD in Algorithm 1 in Appendix A.2, and summarize its components below.

Step size selectionWe introduce a step-size adaptation. We follow the exploration-exploitation principle by gradually reducing the gradient step [13]. However, unlike CPGD, this reduction does not follow a fixed schedule but is determined by the optimization trend. If the value of the loss function grows, we keep the same step size; otherwise, we halve it. That is, we start with a step \(\eta^{(0)}=2\epsilon\), and we identify checkpoints \(w_{0}=0,w_{1},...,w_{n}\) at which we decide whether it is necessary to halve the size of the current step. We halve the step size if any of the following two conditions holds:

1. Since the last checkpoint, we count how many cases since the last checkpoint \(w_{j-1}\) the update step has successfully increased \(\mathcal{L}^{\prime}\). The condition holds if the loss has increased for at least a fraction of \(\rho\) steps (we set \(\rho\) = 0.75): \[\sum_{i=w_{j-1}}^{w_{j}-1}\mathbf{1}_{\mathcal{L}^{\prime}(x^{(i+1)})> \mathcal{L}^{\prime}(x^{(i)})}<\rho\cdot(w_{j}-w_{j-1}).\] (7)
2. The step has not been reduced at the last checkpoint and the loss is less or equal to the loss of the last checkpoint: \[\eta^{(w_{j-1})}\equiv\eta^{(w_{j})}\wedge\mathcal{L}_{\text{ max}}^{(w_{j-1})}\equiv\mathcal{L}_{\text{max}}^{(w_{j})}\] (8) where \(\mathcal{L}^{\prime}(x)\) is the loss function, \(\mathcal{L}_{\text{max}}^{(w_{j})}\) is the highest loss value in the first \(j+1\) iterations.

Repair operatorWhile equality constraints are included in the penalty function, optimization alone does not achieve exact equality of feature values. Our new repair operator \(R_{\Omega}\) improved from [35] addresses this by setting the value of the left-hand side of an equation of the form \(f_{i}=\psi\) to match the evaluation of the right-hand side in each iteration. It maintains other dataset constraints such as bounds, mutability, and feature types but does not ensure other relational constraints are met. The operator can violate maximum perturbation constraints, yet at each iteration, the perturbation is corrected back within the allowed maximum. This approach has been shown to improve the success rate of CAPGD, as demonstrated by our ablation study in Table 8. We provide the algorithm in Appendix A.2.

Initial stateAs for initialization, we apply the attack from two initial states: the original example \(x_{orig}\) and a random example sampled from \(\mathcal{S}\) (the Lp-ball around \(x_{orig}\)). The goal behind this second initialization is to reduce the risk of being immediately locked into local optima that encompass only invalid examples. Our experiments later reveal the complementary of these two initializations.

Gradient stepFinally, we introduce in CAPGD a momentum [14]. Let \(\eta^{(k)}\) be the step size at iteration \(k\), then we first compute \(z^{(k+1)}\) before the updated example \(x^{(k+1)}\).

\[z^{(k+1)} =P_{\mathcal{S}}\big{(}x^{(k)}+\eta^{(k)}(\nabla\mathcal{L}^{ \prime}(x^{(k)})\big{)}\] (9) \[x^{(k+1)} =R_{\Omega}\Big{(}P_{\mathcal{S}}\big{(}x^{(k)}+\alpha\cdot(z^{(k +1)}-x^{(k)})+(1-\alpha)\cdot(x^{(k)}-x^{(k-1)})\big{)}\Big{)}\]

where \(\alpha\in[0,1]\) (we use \(\alpha=0.75\) following [13]) regulates the influence of the previous update on the current, and \(P_{\mathcal{S}}\) is the projection onto \(\mathcal{S}=\{x\in\mathbb{R}^{d},||x-x_{orig}||_{p}\leq\epsilon\}\).

### Comparison of CAPGD to gradient-based attacks

To evaluate the benefits of CAPGD, we compare it with CPGD as well as LowProFool, to the best of our knowledge, the only other public _gradient_ attack for tabular models that can be extended to support all types of features.

CAPGD is more successful than existing gradient attacks.In Table 2, we compare the robust accuracy across our four datasets and five architectures with CPGD, LowProFool, and CAPGD. CAPGD significantly outperforms CPGD and LowProFool. It decreases the robust accuracy on URL, LCLD, and WIDS datasets to as low as 10.9%, 0.2%, and 10.2% respectively.

The results also reveal that gradient attacks are ineffective on the CTU dataset. These results demonstrate that gradient-based attacks are not enough and motivate us to consider combining CAPGD with search-based attacks, as investigated in Section 5.

CAPGD subsumes all gradient attacks.We analyze in detail the original examples from which attacks could generate valid and successful adversarial examples. For each attack, we take the union of the sets of clean examples across 5 seeds. We generate the Venn diagram for CPGD, LowProFool, and CAPGD, for all datasets and model architectures. We sum the partition values in Figure 1. CAPGD generates adversarial examples for 6597 original examples from which none of the other gradient attacks could produce adversarial examples. In contrast, all successful adversarial examples by CPGD (132) and LowProFool (3) are also generated by CAPGD.

All components of CAPGD contribute to its effectiveness.We analyze in detail each of the components of CAPGD in Appendix B.1. These results and ablation studies confirm the complementarity of our new mechanisms and their contribution to the effectiveness of CAPGD.

## 5 CAA: an ensemble of gradient and search attacks

We next propose _Constrained Adaptive Attack_ (CAA), an effective and efficient ensemble of gradient- and search-based attacks. The idea underlying CAA is that gradient-based attacks for tabular data are more efficient but less successful than search-based attacks. Thus, CAA integrates the best search-based attacks from each family in a complementary way, such that we maximize the set of adversarial examples that can be generated.

### Design of CAA

Following our related work study, we consider the search attacks MOEVA and BF* Kulynych et al. [26], Kireev et al. [25]. As a first step, we compare in Figure 2 these two attacks in terms of the original examples for which they could generate successful adversarial examples. We also include CAPGD in this comparison, since we have shown that this attack subsumes the other gradient-based attacks. Our results reveal that CAPGD and MOEVA together subsume BF* except for 5 examples. Additionally, CAPGD and MOEVA are complementary, with CAPGD generating 477 unique examples and MOEVA 953. Overall, the combination of CAPGD with MOEVA yields the strongest method including only one gradient-based

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline DS & Model & Clean & LPF & CPGD & CAPGD \\ \hline \multirow{5}{*}{URL} & TabTr. & \(93.6\) & \(93.6\) & \(91.9\) & \(\mathbf{10.9}\) \\  & RLN & \(94.4\) & \(94.4\) & \(92.8\) & \(\mathbf{12.6}\) \\  & VIME & \(92.5\) & \(92.5\) & \(90.7\) & \(\mathbf{56.3}\) \\  & STG & \(93.3\) & \(93.3\) & \(93.3\) & \(\mathbf{72.6}\) \\  & TabNet & \(93.4\) & \(93.4\) & \(88.5\) & \(\mathbf{19.3}\) \\ \hline \multirow{5}{*}{LCLD} & TabTr. & \(69.5\) & \(69.2\) & \(69.5\) & \(\mathbf{27.1}\) \\  & RLN & \(68.3\) & \(68.3\) & \(68.3\) & \(\mathbf{0.2}\) \\  & VIME & \(67.0\) & \(67.0\) & \(67.0\) & \(\mathbf{2.6}\) \\  & STG & \(66.4\) & \(66.4\) & \(66.4\) & \(\mathbf{55.5}\) \\  & TabNet & \(67.4\) & \(67.4\) & \(67.4\) & \(\mathbf{6.3}\) \\ \hline \multirow{5}{*}{CTU} & TabTr. & \(\mathbf{95.3}\) & \(\mathbf{95.3}\) & \(\mathbf{95.3}\) & \(\mathbf{95.3}\) \\  & RLN & \(\mathbf{97.8}\) & \(\mathbf{97.8}\) & \(\mathbf{97.8}\) & \(\mathbf{97.8}\) \\  & VIME & \(\mathbf{95.1}\) & \(\mathbf{95.1}\) & \(\mathbf{95.1}\) & \(\mathbf{95.1}\) \\  & STG & \(\mathbf{95.3}\) & \(\mathbf{95.3}\) & \(\mathbf{95.3}\) & \(\mathbf{95.3}\) \\  & TabNet & \(\mathbf{96.1}\) & \(\mathbf{96.1}\) & \(\mathbf{96.1}\) \\ \hline \multirow{5}{*}{WIDS} & TabTr. & \(75.5\) & \(75.5\) & \(75.2\) & \(\mathbf{48.0}\) \\  & RLN & \(75.7\) & \(77.5\) & \(77.3\) & \(\mathbf{61.8}\) \\ \cline{1-1}  & VIME & \(72.3\) & \(72.3\) & \(71.5\) & \(\mathbf{51.4}\) \\ \cline{1-1}  & STG & \(77.6\) & \(77.6\) & \(77.5\) & \(\mathbf{65.1}\) \\ \cline{1-1}  & TabNet & \(79.7\) & \(79.7\) & \(76.0\) & \(\mathbf{10.2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Robust accuracy against CAPGD and SOTA gradient attacks. A lower robust accuracy means a more effective attack (lowest in bold).

Figure 1: Visualization of the complementarity of CAPGD, CPGD, and LowProFool with the number of successful adversarial examples.

Figure 2: Visualization of the complementarity of CAPGD, MOEVA, and BF* with the number of successful adversarial examples.

[MISSING_PAGE_FAIL:8]

CAPGD and MOEVA fail to generate adversarial examples on CTU for 2 out of 5 models. In Appendix B.3, we provide a possible explanation for this dataset by evaluating our attacks on different sub-sets of constraints with varying complexity.

### Impact of attack budget

We study the impact of the attacker's budget on the effectiveness of CAA, in terms of (i) maximum perturbation \(\epsilon\) and (ii) the number of iterations of its components. We focus on the CTU dataset, which models are the only ones to remain robust through our previous experiments. All the datasets are evaluated in Appendix B.2. Figure 3 reveals in (a) that the maximum perturbation distance \(\epsilon\) has little impact on the effectiveness of the attack. Increasing the number of iterations for the gradient attack component (b) does not have an impact on the success rate of CAA. Increasing the budget of the search attack component (c) significantly impacts the robustness of some models. While TabTransformer and STG remain robust, the robust accuracy of RLM and VIME drops below 0.4 when doubling the number of search iterations to 200, and to zero with 1000 search iterations.

### Impact of Adversarial training

We evaluate the effectiveness of our attack against models made robust with Madry's adversarial training (AT) [29], using examples generated by the PGD attack. We consider this defense because adversarial training-based methods were shown to be the only reliable defense against evasion attacks [37; 6]. In Table 4, we show the clean accuracy and the robust accuracy (against CAA) of the adversarially trained models (big numbers in Table 4). We also show the accuracy difference with the models trained with standard training (small numbers). In Appendix B.4, we evaluate additional defenses based on adversarial training and data augmentations.

Adversarial training can degrade clean and robust performance.As a preliminary check, we investigate whether adversarial training degrades the clean performance of the models. This is important to ensure that a non-increase of robust accuracy does not originate from clean performance degradation (instead of being due to CAA's strength). Our evaluation shows that adversarial training significantly degrades clean performance of the STG and Tabnet architectures. The accuracy of STG models drops to 15.6% and 62.6% for LCLD and WIDS respectively. As for Tabnet models, the clean accuracy drops to 0.0% (LCLD) and 0.2% (CTU). In all other cases, clean accuracy remains stable.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Dataset & Accuracy & TabTr. & RLN & VIME & STG & TabNet \\ \hline \multirow{2}{*}{URL} & Clean & \(93.9_{+0.3}\) & \(95.2_{+0.8}\) & \(93.4_{+0.9}\) & \(94.3_{+1.0}\) & \(99.5_{+6.1}\) \\  & CAA & \(56.7_{+47.8}\) & \(56.2_{+45.4}\) & \(69.8_{+20.3}\) & \(90.0_{+32.0}\) & \(91.8_{+80.8}\) \\ \hline \multirow{2}{*}{LCLD} & Clean & \(73.9_{+4.4}\) & \(69.5_{+1.2}\) & \(65.5_{-1.5}\) & \(15.6_{-50.8}\) & \(0.0_{-67.4}\) \\  & CAA & \(70.3_{+62.4}\) & \(63.0_{+63.0}\) & \(10.4_{+8.0}\) & \(12.1_{-41.5}\) & \(0.0_{-0.4}\) \\ \hline \multirow{2}{*}{CTU} & Clean & \(95.3_{+0.0}\) & \(97.3_{-0.5}\) & \(95.1_{+0.0}\) & \(95.1_{-0.2}\) & \(0.2_{-95.8}\) \\  & CAA & \(95.3_{+0.0}\) & \(97.1_{+3.0}\) & \(94.0_{+5.3}\) & \(95.1_{-0.2}\) & \(0.2_{+0.2}\) \\ \hline \multirow{2}{*}{WIDS} & Clean & \(77.3_{+1.8}\) & \(78.0_{+0.5}\) & \(72.1_{-0.2}\) & \(62.6_{-15.1}\) & \(98.4_{+18.6}\) \\  & CAA & \(65.1_{+19.2}\) & \(66.6_{+5.7}\) & \(52.1_{+1.8}\) & \(45.2_{-18.6}\) & \(58.4_{+53.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: CAA performances (XX+/-YY) against Madry adversarially trained model. XX refers to accuracy. YY is the difference between the accuracy of the adversarially trained model and standard training (cf. Table 3), such that ‘+’ means a higher accuracy for the adversarially trained model.

Figure 3: Impact of CAA budget on the robust accuracy for CTU dataset.

CAA remains effective against adversarial training for some architectures.The effectiveness of CAA against robust models is architecture- and dataset-dependent. The attack remains effective on VIME architecture applied to LCLD and WIDS, with robust accuracy as low as 10.1% and 52.2% respectively, as well as on RLN architecture on the WIDS dataset (66.6% robust accuracy and only +5.7% improvement compared to standard training). However, the robustness against CAA of Tabtransformer architecture is significantly improved on URL and LCLD datasets by respectively 47.8% and 62.4%, and marginally improved on WIDS dataset by 19.2%. Similarly, RLN robustness to CAA improves on URL (+45.4%) and LCLD (+63%).

## 6 Limitations

We identify three main limitations of our work.

_Marginal overhead of CAA:_ In scenarios where CAPGD struggles to attack tabular models, CAA can exhibit a computation overhead (<14%) compared to MOEVA. However, in 4 out of 5 evaluated settings, CAA is faster than MOEVA (up to 5 times).

_CAPGD effectiveness with complex constraints:_ CAPGD effectiveness drops when increasing the constraint's complexity in the number of constraints or the number of features involved in each constraint.

_Coherence of constraints:_ The mechanisms of CAA assume that the constraints definitions are sound. Incoherences between boundary constraints and feature relation constraints can lead to invalid adversarial examples with large \(\epsilon\) budgets.

## 7 Broader Impact

Our work proposes CAA, the most effective evasion attack against constrained tabular ML. We also provided for each dataset at least one combination of architecture combined with AT where CAA can be mitigated. We expect that our work will have a more positive impact by leading to improved defenses in the scarcely explored field of robust tabular ML.

## 8 Conclusion

In this work, we first propose CAPGD, a new parameter-free gradient attack for constrained tabular machine learning. We also design CAA, a new Constrained Adaptive Attack that combines the best gradient-based attack (CAPGD) and the best search-based attack (MOEVA). We evaluate our attacks over four datasets and five architectures and demonstrated that our new attacks outperform all previous attacks in terms of effectiveness and efficiency. We believe that our work is a springboard for further research on the robustness of tabular machine learning and to open multiple research perspectives on constrained tabular ML. We hope that CAA will contribute to a faster development of adversarial defenses and recommend it as part of a standard evaluation pipeline of new tabular machine models.

## Acknowledgments and Disclosure of Funding

This research was funded in whole, or in part, by the Luxembourg National Research Fund (FNR), grant reference NCER22/IS/16570468/NCER-FT and grant BRIDGES/2022/IS/17437536. This research was supported by BGL BNP Paribas Luxembourg. In particular, we would like to thank Anne Goujon and Andrey Boytsov for their support with the financial use case. The experiments presented in this paper were carried out using the HPC facilities of the University of Luxembourg [39] (see hpc.uni.lu).

## References

* Aghakhani et al. [2020] Hojjat Aghakhani, Fabio Gritti, Francesco Mecca, Martina Lindorfer, Stefano Ortolani, Davide Balzarotti, Giovanni Vigna, and Christopher Kruegel. 2020. When malware is packin' heat; limitsof machine learning classifiers based on static analysis features. In _Network and Distributed Systems Security (NDSS) Symposium 2020_.
* [2] Sercan O Arik and Tomas Pfister. 2021. Tabnet: Attentive interpretable tabular learning. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 35. 6679-6687.
* [3] Vincent Ballet, Jonathan Aigrain, Thibault Laugel, Pascal Frossard, Marcin Detyniecki, et al. 2019. Imperceptible Adversarial Attacks on Tabular Data. In _NeurIPS 2019 Workshop on Robust AI in Financial Services: Data, Fairness, Explainability, Trustworthiness and Privacy (Robust AI in FS 2019)_.
* [4] Hongyan Bao, Yufei Han, Yujun Zhou, Xin Gao, and Xiangliang Zhang. 2023. Towards efficient and domain-agnostic evasion attack with high-dimensional categorical inputs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 37. 6753-6761.
* [5] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2022. Deep Neural Networks and Tabular Data: A Survey. _IEEE Transactions on Neural Networks and Learning Systems_ (2022), 1-21. https://doi.org/10.1109/tnnls.2022.3229161
* [6] Nicholas Carlini. 2023. A LLM assisted exploitation of AI-Guardian. _arXiv preprint arXiv:2307.15008_ (2023).
* [7] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey Kurakin. 2019. On evaluating adversarial robustness. _arXiv preprint arXiv:1902.06705_ (2019).
* [8] Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke Yamaguchi, Toru Akishita, and Olivier Elshocht. 2021. Adversarial Attacks for Tabular Data: Application to Fraud Detection and Imbalanced Data. (2021).
* [9] Alesia Chernikova and Alina Oprea. 2019. Fence: Feasible evasion attacks on neural networks in constrained environments. _arXiv preprint arXiv:1909.10480_ (2019).
* [10] Alesia Chernikova and Alina Oprea. 2022. Fence: Feasible evasion attacks on neural networks in constrained environments. _ACM Transactions on Privacy and Security_ 25, 4 (2022), 1-34.
* [11] Jillian M Clements, Di Xu, Nooshin Yousefi, and Dmitry Efimov. 2020. Sequential Deep Learning for Credit Risk Monitoring with Tabular Financial Data. _arXiv preprint arXiv:2012.15330_ (2020).
* [12] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. 2020. Robustbench: a standardized adversarial robustness benchmark. _arXiv preprint arXiv:2010.09670_ (2020).
* [13] Francesco Croce and Matthias Hein. 2020. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _ICML_.
* [14] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. 2018. Boosting adversarial attacks with momentum. In _Proceedings of the IEEE conference on computer vision and pattern recognition_. 9185-9193.
* [15] Salijona Dymrishi, Salah Ghamizi, and Maxime Cordy. 2023. How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 8822-8836. https://doi.org/10.18653/v1/2023.acl-long.491
* [16] Salijona Dymrishi, Salah Ghamizi, Thibault Simonetto, Yves Le Traon, and Maxime Cordy. 2023. On the empirical effectiveness of unrealistic adversarial hardening against realistic adversarial attacks. In _2023 IEEE symposium on security and privacy (SP)_. IEEE, 1384-1400.

* George [2018] Nathan George. 2018. Lending Club loan data. https://www.kaggle.com/datasets/wordsforthewise/lending-club.
* Ghamizi et al. [2020] Salah Ghamizi, Maxime Cordy, Martin Gubri, Mike Papadakis, Andrey Boystov, Yves Le Traon, and Anne Goujon. 2020. Search-based adversarial testing and improvement of constrained credit scoring systems. In _Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering_. 1089-1100.
* Gressel et al. [2021] Gilad Gressel, Niranjan Hegde, Archana Sreekumar, Rishikumar Radhakrishnan, Kalyani Harikumar, Krishnashree Achuthan, et al. 2021. Feature importance guided attack: a model agnostic adversarial attack. _arXiv preprint arXiv:2106.14815_ (2021).
* Grosse et al. [2024] Kathrin Grosse, Lukas Bieringer, Tarek R. Besold, Battista Biggio, and Alexandre Alahi. 2024. When Your AI Becomes a Target: AI Security Incidents and Best Practices. _Proceedings of the AAAI Conference on Artificial Intelligence_ 38, 21 (Mar. 2024), 23041-23046. https://doi.org/10.1609/aaai.v38i21.30347
* Hancock and Khoshgoftaar [2020] John T Hancock and Taghi M Khoshgoftaar. 2020. Survey on categorical data for neural networks. _Journal of Big Data_ 7 (2020), 1-41.
* Hannousse and Yahiouche [2021] Abdelhakim Hannousse and Salima Yahiouche. 2021. Towards benchmark datasets for machine learning based website phishing detection: An experimental study. _Engineering Applications of Artificial Intelligence_ 104 (2021), 104347.
* Huang et al. [2020] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. 2020. Tabtransformer: Tabular data modeling using contextual embeddings. _arXiv preprint arXiv:2012.06678_ (2020).
* Kireev et al. [2022] Klim Kireev, Bogdan Kulynych, and Carmela Troncoso. 2022. Adversarial Robustness for Tabular Data through Cost and Utility Awareness. _arXiv preprint arXiv:2208.13058_ (2022).
* Kireev et al. [2023] Klim Kireev, Bogdan Kulynych, and Carmela Troncoso. 2023. Adversarial Robustness for Tabular Data through Cost and Utility Awareness. In _The Second Workshop on New Frontiers in Adversarial Machine Learning_.
* Kulynych et al. [2018] Bogdan Kulynych, Jamie Hayes, Nikita Samarin, and Carmela Troncoso. 2018. Evading classifiers in discrete domains with provable optimality guarantees. _arXiv preprint arXiv:1810.10939_ (2018).
* Lee et al. [2020] Meredith Lee, Jesse Raffa, Marzyeh Ghassemi, Tom Pollard, Sharada Kalanidhi, Omar Badawi, Karen Matthys, and Leo Anthony Celi. 2020. WiDS (Women in Data Science) Datathon 2020: ICU Mortality Prediction. PhysioNet.
* Long et al. [2022] Teng Long, Qi Gao, Lili Xu, and Zhangbing Zhou. 2022. A survey on adversarial attacks in computer vision: Taxonomy, visualization and future directions. _Computers & Security_ 121 (2022), 102847. https://doi.org/10.1016/j.cose.2022.102847
* Madry et al. [2017] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks. _arXiv preprint arXiv:1706.06083_ (2017).
* Mathov et al. [2022] Yael Mathov, Eden Levy, Ziv Katzir, Asaf Shabtai, and Yuval Elovici. 2022. Not all datasets are born equal: On heterogeneous tabular data and adversarial examples. _Knowledge-Based Systems_ 242 (2022), 108377.
* Mosbach et al. [2018] Marius Mosbach, Maksym Andriushchenko, Thomas Trost, Matthias Hein, and Dietrich Klakow. 2018. Logit pairing methods can fool gradient-based attacks. _NeurIPS 2018 Workshop on Security in Machine Learning_ (2018).
* Rebuffi et al. [2021] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy A Mann. 2021. Data augmentation can improve robustness. _Advances in Neural Information Processing Systems_ 34 (2021), 29935-29948.

* Shavitt and Segal (2018) Ira Shavitt and Eran Segal. 2018. Regularization learning networks: deep learning for tabular datasets. _Advances in Neural Information Processing Systems_ 31 (2018).
* Shwartz-Ziv and Armon (2021) Ravid Shwartz-Ziv and Amitai Armon. 2021. Tabular Data: Deep Learning is Not All You Need. _arXiv preprint arXiv:2106.03253_ (2021).
* Simonetto et al. (2022) Thibault Simonetto, Salijona Dymrishi, Salah Ghamizi, Maxime Cordy, and Yves Le Traon. 2022. A Unified Framework for Adversarial Attack and Defense in Constrained Feature Space. In _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_, Lud De Raedt (Ed.). International Joint Conferences on Artificial Intelligence Organization, 1313-1319. https://doi.org/10.24963/ijcai.2022/183 Main Track.
* Somani et al. (2021) Sulaiman Somani, Adam J Russak, Felix Richter, Shan Zhao, Akhil Vaid, Fayzan Chaudhry, Jessica K De Freitas, Nidhi Naik, Riccardo Miotto, Girish N Nadkarni, et al. 2021. Deep learning and the electrocardiogram: review of the current state-of-the-art. _EP Europace_ (2021).
* Tramer et al. (2020) Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. 2020. On adaptive attacks to adversarial example defenses. _Advances in Neural Information Processing Systems_ 33 (2020), 1633-1645.
* Ulmer et al. (2020) Dennis Ulmer, Lotta Meijerink, and Giovanni Cina. 2020. Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD Detection On Medical Tabular Data. In _Machine Learning for Health_. PMLR, 341-354.
* Varrette et al. (2022) S. Varrette, H. Cartiaux, S. Peter, E. Kieffer, T. Valette, and A. Olloh. 2022. Management of an Academic HPC & Research Computing Facility: The ULHPC Experience 2.0. In _Proc. of the 6th ACM High Performance Computing and Cluster Technologies Conf. (HPCCT 2022)_. Association for Computing Machinery (ACM), Fuzhou, China.
* Wang et al. (2020) Yutong Wang, Yufei Han, Hongyan Bao, Yun Shen, Fenglong Ma, Jin Li, and Xiangliang Zhang. 2020. Attackability characterization of adversarial evasion attack on discrete data. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 1415-1425.
* Xu et al. (2023) Han Xu, Pengfei He, Jie Ren, Yuxuan Wan, Zitao Liu, Hui Liu, and Jiliang Tang. 2023. Probabilistic categorical adversarial attack and adversarial training. In _International Conference on Machine Learning_. PMLR, 38428-38442.
* Xu et al. (2019) Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. 2019. Modeling tabular data using conditional GAN. In _Advances in Neural Information Processing Systems_, Vol. 33.
* Yamada et al. (2020) Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger. 2020. Feature Selection using Stochastic Gates. In _Proceedings of Machine Learning and Systems 2020_. 8952-8963.
* Yoon et al. (2020) Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela van der Schaar. 2020. Vime: Extending the success of self-and semi-supervised learning to tabular domain. _Advances in Neural Information Processing Systems_ 33 (2020), 11033-11043.
* Yun et al. (2019) Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. 2019. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 6023-6032.
* Zhang et al. (2019) Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based recommender system: A survey and new perspectives. _ACM Computing Surveys (CSUR)_ 52, 1 (2019), 1-38.

[MISSING_PAGE_FAIL:14]

[MISSING_PAGE_FAIL:15]

next attack is only executed if no attack has been successful, reducing the overall cost of CAA. At the end of CAA, we had the remaining candidates, for which we have not found adversarial examples to the output set of potentially adversarial examples \(X^{\prime}\), to ease the calculation of robust accuracy (e.g. in transferable settings).

The sub procedure \(is\_adv\) goes through all the examples \(X_{i}[k]\in X_{i}\) and adds \(True\) to the mask, if all of the following conditions hold:

* \(X_{i}[k]\) respects the domain constraints,
* \(X_{i}[k]\) classification by \(H\)is different from its true label \(Y_{c}[k]\),
* \(X_{i}[k]\) pertubation w.r.t to \(X_{c}[k]\) is lower or equal to \(\epsilon\).

### Datasets

Our dataset design followed the same protocol as Simonetto et al.[35]. We present in Table 6 the attributes of our datasets and the test performance achieved by each of the architectures.

Credit scoring - LCLD(licence: CC0: Public Domain) We engineer a dataset from the publicly available Lending Club Loan Data1. This dataset contains 151 features, and each example represents a loan that was accepted by the Lending Club. However, among these accepted loans, some are not repaid and charged off instead. Our goal is to predict, at the request time, whether the borrower will be repaid or charged off. This dataset has been studied by multiple practitioners on Kaggle. However, the original version of the dataset contains only raw data and to the extent of our knowledge, there is no featured engineered version commonly used. In particular, one shall be careful when reusing feature-engineered versions, as most of the versions proposed present data leakage in the training set that makes the prediction trivial. Therefore, we propose our own feature engineering. The original dataset contains 151 features. We remove the example for which the feature "loan status" is different from "Fully paid" or "Charged Off" as these represent the only final status of a loan: for other values, the outcome is still uncertain. For our binary classifier, a 'Fully paid" loan is represented as 0 and a "Charged Off" as 1. We start by removing all features that are not set for more than 30% of the examples in the training set. We also remove all features that are not available at loan request time, as this would introduce bias. We impute the features that are redundant (e.g. grade and sub-grade) or too granular (e.g. address) to be useful for classification. Finally, we use one-hot encoding for categorical features. We obtain 47 input features and one target feature. We split the dataset using random sampling stratified on the target class and obtained a training set of 915K examples and a testing set of 305K. They are both unbalanced, with only 20% of charged-off loans (class 1). We trained a neural network to classify accepted and rejected loans. It has 3 fully connected hidden layers with 64, 32, and 16 neurons.

Footnote 1: https://www.kaggle.com/wordsforthewise/lending-club

For each feature of this dataset, we define boundary constraints as the extremum value observed in the training set. We consider the 19 features that are under the control of the Lending Club as immutable. We identify 10 relationship constraints (3 linear, and 7 non-linear ones).

URL Phishing - ISCX-URL2016(license CC BY 4.0) Phishing attacks are usually used to conduct cyber fraud or identity theft. This kind of attack takes the form of a URL that reassembles a legitimate URL (e.g. user's favorite e-commerce platform) but redirects to a fraudulent website that asks the user for their personal or banking data. [22] extracted features from legitimate and fraudulent URLs

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline Dataset & \multicolumn{4}{c}{Properties} \\  & Task & Size & \# Features & Balance (\%) \\ \hline LCLD [17] & Credit Scoring & 1 220 092 & 28 & 80/20 \\ CTU-13 [10] & Botnet Detection & 198 128 & 756 & 99.3/0.7 \\ URL [22] & Phishing URL detection & 11 430 & 63 & 50/50 \\ WIDS [27] & ICU patient survival & 91 713 & 186 & 91.4/8.6 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The datasets evaluated in the empirical study, with the class imbalance of each dataset.

as well as external service-based features to build a classifier that can differentiate fraudulent URLs from legitimate ones. The feature extracted from the URL includes the number of special substrings such as "www", "&", ".", "&", "and", the length of the URL, the port, the appearance of a brand in the domain, in a subdomain or in the path, and the inclusion of "http" or "https". External service-based features include the Google index, the page rank, and the presence of the domain in the DNS records. The complete list of features is present in the reproduction package. [22] provide a dataset of 5715 legit and 5715 malicious URLs. We use 75% of the dataset for training and validation and the remaining 25% for testing and adversarial generation.

We extract a set of 14 relation constraints between the URL features. Among them, 7 are linear constraints (e.g. length of the hostname is less or equal to the length of the URL) and 7 are Boolean constraints of the type \(ifa>0\) then \(b>0\) (e.g. if the number of http \(>0\) then the number slash "/" \(>0\)).

Botnet attacks - CTU-13(license CC BY NC SA 4.0) This is a feature-engineered version of CTU-13 proposed by [9]. It includes a mix of legit and botnet traffic flows from the CTU University campus. Chernikova et al. aggregated the raw network data related to packets, duration, and bytes for each port from a list of commonly used ports. The dataset is made of 143K training examples and 55K testing examples, with 0.74% examples labeled in the botnet class (traffic that a botnet generates). Data have 756 features, including 432 mutable features. We identified two types of constraints that determine what feasible traffic data is. The first type concerns the number of connections and requires that an attacker cannot decrease it. The second type is inherent constraints in network communications (e.g. maximum packet size for TCP/UDP ports is 1500 bytes). In total, we identified 360 constraints.

WiDs(license: PhysioNet Restricted Health Data License 1.5.02) [27] dataset contains medical data on the survival of patients admitted to the ICU. The goal is to predict whether the patient will survive or die based on biological features (e.g. for triage). This very unbalanced dataset has 30 linear relation constraints.

Footnote 2: https://physionet.org/content/widsdatathon2020/view-license/1.0.0/

### Model architectures

Table 7 summarizes the family, model architecture, and hyperparameters tuned during the training of our models.

TabTransformeris a transformer-based model [23]. It uses self-attention to map the categorical features to an interpretable contextual embedding, and the paper claims this embedding improves the robustness of models to noisy inputs.

TabNetis another transformer-based model [2]. It uses multiple sub-networks that are used in sequence. At each decision step, it uses sequential attention to choose which features to reason. TabNet aggregates the outputs of each step to obtain the decision.

\begin{table}
\begin{tabular}{l l l} \hline \hline Family & Model & Hyperparameters \\ \hline Transformer & TabTransformer & \(hidden\_dim,n\_layers,\) \\  & \(learning\_rate,norm,\theta\) \\  & & \(n\_d,n\_steps,\) \\ Transformer & TabNet & \(\gamma,cat\_emb\_dim,n\_independent,\) \\  & & \(n\_shared,momentum,mask\_type\) \\  & & \(hidden\_dim,depth,\) \\ Regularization & RLN & \(heads,weight\_decay,\) \\  & & \(learning\_rate,dropout\) \\ Regularization & STG & \(hidden\_dims,learning\_rate,lam\) \\ Encoding & VIME & \(p_{m}\), \(\alpha\), \(K,\beta\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: The three model architectures of our study.

RLNor Regularization Learning Networks [33] uses an efficient hyperparameter tuning scheme in order to minimize a counterfactual loss. The authors train a regularization coefficient to weights in the neural network in order to lower the sensitivity and produce very sparse networks.

STGor Stochastic Gates [43] uses stochastic gates for feature selection in neural network estimation problems. The method is based on probabilistic relaxation of the \(l_{0}\) norm of features or the count of the number of selected features.

Vimeor Value Imputation for Mask Estimation [44] uses self and then semi-supervised learning through deep encoders and predictors.

### Attacks parameters

For existing attacks, we reuse the hyperparameters proposed in their respective papers. For LowPro-Fool, we use a small step size of \(\eta=0.001\), \(\lambda=8.5\) rade off factor between fooling the classifier and generating imperceptible adversarial example, and run the attack for \(N_{iter}=20,000\) iterations. All other gradient attacks run for \(N_{iter}=10\) iterations. The schedule of decreasing steps of CPGD uses \(M=7\). In CAPGD, we fix the checkpoints as \(w_{j}=\lceil p_{j}N_{\text{iter}}\rceil\leq N_{\text{iter}}\), with \(p_{j}\in[0,1]\) defined as \(p_{0}=0\), \(p_{1}=0.22\) and

\[p_{j+1}=p_{j}+\max\{p_{j}-p_{j-1}-0.03,0.06\}.\]

. The influence of the previous update on the current update is set to \(\alpha=0.75\), and \(\rho=0.75\) for the halving of the step. MOEVA runs for \(n_{gen}=100\) iterations generating \(n_{off}=100\) offspring per iteration. Among the offspring, \(n_{pop}=200\) survive and are used for mating in the next iteration. With BF*, we discretize numerical features in \(n_{bin}=150\) bins. We run the attack for a maximum of \(N_{iter}=100\) iterations. CAA applies CAPGD and MOEVA with the same parameters.

### Hardware

We run our experiments on an HPC cluster node with 32 cores and 64GB of RAM dedicated to our task. Each node consists of 2 AMD Epyc ROME 7H12 @ 2.6 GHz for a total of 128 cores with 256 GB of RAM.

### Reproduction package and availability

The source code, datasets and pre-trained models to reproduce the experiments of this paper are available with the submission. The source code will be available publicly upon acceptance under the MIT license or similar.

## Appendix B Additional results

### Components of CAPGD

All components of CAPGD contribute to its effectiveness.We conduct an ablation study on the components of CAPGD. We evaluate CAPGD without the repair operator at each iteration (NREP), without the initialization with clean example (NINI), without the initialization with random sampling (NRAN), and without the adaptive step (NADA). Table 8 reveals that removing a component of CAPGD reduces its effectiveness. Interestingly, not all components affect all datasets similarly. For instance, removing the repair at each gradient iteration only affects the LCLD datasets' success rate. For URL and WIDS, CAPGD-NREP remains in the confidence interval of CAPGD. Removing any other components always negatively affects CAPGD, up to an improvement of 32.1% of the robust accuracy for CAPGD-NADA on the WIDS dataset and TabNet model.

CAPGD components are complementary.None of the components of CAPGD negatively affects its capability of finding an adversarial example for a given clean example. In Figure 4, we analyze the coverage of each CAPGD variant A (Covering Attack) with regard to another variant B (Coverred Attack). For A and B, we compute the set of clean examples \(C_{A}\) and \(C_{B}\) on which the attacks are successful. The percentage in the heatmaps represents the proportion of \(C_{A}\cup C_{B}\) covered by \(C_{B}\), that is

\[coverage=\frac{|C_{B}|}{|C_{A}\cup C_{B}|}\]

where \(|C|\) is the cardinality of \(C\). Attack B subsumes A when \(coverage=1\). In practice, to avoid random effect, we run the attack for \(N=5\) seeds, and take the _union_ of clean examples on which we generate a successful example.

Figure 4 reveals that CAPGD subsumes all its variants with coverage over 99%, while none of the variants subsumes CAPGD. Therefore all components of CAPGD are necessary to obtain the strongest attack.

### Budget of attacker

In this section, we study the impact of CAA's budget on its effectiveness. We consider 3 budgets: the maximum perturbation \(\epsilon\) allowed, the number of iterations in the gradient attack CAPGD (without changing MOEVA's budget), and the number of iterations in the search attack MOEVA (without changing CAPGD's budget).

Figure 4: Visualization of the utility of CAA’s components. For attack A (respectively B) we compute the set of clean examples \(C_{A}\) (respectively \(C_{B}\))) on which the attack is successful. The percentage represents the proportion of the set \(C_{A}\cup C_{B}\) is covered by \(C_{B}\). CAPGD-NADA is CAPGD without adaptive step, CAPGD-NRAN is CAPGD without the random start, CAPGD-NINI is CAPGD without the clean example initialization and CAPGD NREP is CAPGD without repair at each iteration.

For each budget, we provide figures and detailed numerical results in tables, corresponding to the same experiment.

Maximum perturbation\(\epsilon\)Figure 5 (numerical results in Table 9) reveals that increasing the maximum perturbation \(\epsilon\) for CAA reduces the robust accuracy of the model in 16/20 cases.

Number of CAPGD iterationsFigure 6 (numerical results in Table 10) reveals that increasing the number of iterations for the gradient attack component has a limited impact on the success rate of CAA. The maximum drop of accuracy is 3.5% points between 10 and 100 iterations for WIDS/TabNet.

Number of MOEVA iterationsFigure 8 (numerical results in Table 11) reveals that increasing the number of iterations for the search attack component only reduces the robust accuracy in 4/20 cases (URL/VIME, URL/STG, CTU/VIME, and CTU/RLN).

We also observe that for TabTransformer and LCLD the robust accuracy increases with the number of search iterations. MOEVA is a multi-objective genetic algorithm. An inherent problem of multi-objective optimization is the trade-off between the objectives. If all solutions in the population are on the Pareto front, the algorithm must decide which solutions to discard for the next iteration, potentially discarding a valid adversarial example in our case.

Figure 7 shows the evolution of the success rate of MOEVA with the number of iterations in the same settings as in Figure 7(b) for TabTransformer. We find that the success rate reaches a maximum with 100 iterations. We argue that valid adversarial examples were discarded when the search continued to 1000 iterations. To confirm our hypothesis, we run the same experiment with a 10 times larger search population, such that more solutions are preserved at each iteration. We observe that in this setting, MOEVA converges slower (due to less selection pressure) but the success rate strictly increases with the number of generations. Increasing the population size also increases the execution time (by 3.4x in this case), due to the selection operator overhead.

Our approach CAA aims at minimizing the memory and computation overheads while maximizing the success rate, and CAA can be tuned to lead to lower robust accuracy with more iterations if the search space is expanded (for example with larger populations).

\begin{table}
\begin{tabular}{c l|c c c c c c} \hline \hline Dataset & Model & Clean & NREP & NINI & NRAN & NADA & CAPGD \\ \hline \multirow{6}{*}{URL} & TabTr. & \(93.6\) & \(\mathbf{10.9_{\pm 0.1}}\) & \(11.8_{\pm 0.3}\) & \(12.6_{\pm 0.0}\) & \(34.6_{\pm 0.4}\) & \(\mathbf{10.9_{\pm 0.1}}\) \\  & RLN & \(94.4\) & \(\mathbf{12.7_{\pm 0.2}}\) & \(14.9_{\pm 0.2}\) & \(14.8_{\pm 0.0}\) & \(30.2_{\pm 0.5}\) & \(\mathbf{12.6_{\pm 0.2}}\) \\  & VIME & \(92.5\) & \(\mathbf{56.3_{\pm 0.1}}\) & \(58.1_{\pm 0.3}\) & \(56.9_{\pm 0.0}\) & \(65.2_{\pm 0.1}\) & \(\mathbf{56.3_{\pm 0.1}}\) \\  & STG & \(93.3\) & \(\mathbf{72.6_{\pm 0.0}}\) & \(73.4_{\pm 0.2}\) & \(73.0_{\pm 0.0}\) & \(75.3_{\pm 0.1}\) & \(\mathbf{72.6_{\pm 0.0}}\) \\  & TabNet & \(93.4\) & \(\mathbf{19.2_{\pm 0.7}}\) & \(27.6_{\pm 0.8}\) & \(29.7_{\pm 0.0}\) & \(34.4_{\pm 0.3}\) & \(\mathbf{19.3_{\pm 0.6}}\) \\ \hline \multirow{6}{*}{LCLD} & TabTr. & \(69.5\) & \(38.3_{\pm 0.4}\) & \(38.0_{\pm 0.9}\) & \(38.0_{\pm 0.0}\) & \(44.4_{\pm 1.1}\) & \(\mathbf{27.1_{\pm 0.9}}\) \\  & RLN & \(68.3\) & \(5.3_{\pm 0.2}\) & \(1.4_{\pm 0.3}\) & \(1.6_{\pm 0.0}\) & \(1.1_{\pm 0.3}\) & \(\mathbf{0.2_{\pm 0.1}}\) \\  & VIME & \(67.0\) & \(17.9_{\pm 0.6}\) & \(7.1_{\pm 0.5}\) & \(7.3_{\pm 0.0}\) & \(3.6_{\pm 0.4}\) & \(\mathbf{2.6_{\pm 0.2}}\) \\  & STG & \(66.4\) & \(59.4_{\pm 0.1}\) & \(58.0_{\pm 0.3}\) & \(56.5_{\pm 0.0}\) & \(59.7_{\pm 0.2}\) & \(\mathbf{55.5_{\pm 0.2}}\) \\  & TabNet & \(67.4\) & \(30.4_{\pm 0.6}\) & \(33.1_{\pm 1.4}\) & \(10.8_{\pm 0.0}\) & \(7.3_{\pm 0.4}\) & \(\mathbf{6.3_{\pm 0.4}}\) \\ \hline \multirow{6}{*}{CTU} & TabTr. & \(\mathbf{95.3}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) \\  & RLN & \(\mathbf{97.8}\) & \(\mathbf{97.8_{\pm 0.0}}\) & \(\mathbf{97.8_{\pm 0.0}}\) & \(\mathbf{97.8_{\pm 0.0}}\) & \(\mathbf{97.8_{\pm 0.0}}\) & \(\mathbf{97.8_{\pm 0.0}}\) \\  & VIME & \(\mathbf{95.1}\) & \(\mathbf{95.1_{\pm 0.0}}\) & \(\mathbf{95.1_{\pm 0.0}}\) & \(\mathbf{95.1_{\pm 0.0}}\) & \(\mathbf{95.1_{\pm 0.0}}\) & \(\mathbf{95.1_{\pm 0.0}}\) \\  & STG & \(\mathbf{95.3}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) \\  & TabNet & \(\mathbf{96.1}\) & \(\mathbf{96.1_{\pm 0.0}}\) & \(\mathbf{96.1_{\pm 0.0}}\) & \(\mathbf{96.1_{\pm 0.0}}\) & \(\mathbf{96.1_{\pm 0.0}}\) & \(\mathbf{96.1_{\pm 0.0}}\) \\ \hline \multirow{6}{*}{WIDS} & TabTr. & \(75.5\) & \(\mathbf{48.2_{\pm 0.3}}\) & \(54.7_{\pm 0.9}\) & \(53.3_{\pm 0.0}\) & \(64.9_{\pm 0.6}\) & \(\mathbf{48.0_{\pm 0.3}}\) \\  & RLN & \(77.5\) & \(\mathbf{61.8_{\pm 0.3}}\) & \(66.3_{\pm 0.4}\) & \(63.7_{\pm 0.0}\) & \(72.3_{\pm 0.5}\) & \(\mathbf{61.8_{\pm 0.3}}\) \\ \cline{1-1}  & VIME & \(72.3\) & \(\mathbf{51.4_{\pm 0.3}}\) & \(55.6_{\pm 0.9}\) & \(54.1_{\pm 0.0}\) & \(62.9_{\pm 0.4}\) & \(\mathbf{51.4_{\pm 0.3}}\) \\ \cline{1-1}  & STG & \(77.6\) & \(\mathbf{65.1_{\pm 0.4}}\) & \(68.9_{\pm 0.3}\) & \(67.6_{\pm 0.0}\) & \(74.0_{\pm 0.2}\) & \(\mathbf{65.1_{\pm 0.4}}\) \\ \cline{1-1}  & TabNet & \(79.7\) & \(\mathbf{10.1_{\pm 0.5}}\) & \(20.7_{\pm 0.9}\) & \(17.5_{\pm 0.0}\) & \(42.3_{\pm 0.7}\) & \(\mathbf{10.2_{\pm 0.3}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation study: Robust accuracy for CAPGD and its variant without key components. The Clean column corresponds to the accuracy of the model on the subset of clean samples that we attack. A lower robust accuracy means a more effective attack. The lowest robust accuracy is in bold.

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline  & & \multicolumn{4}{c}{Maximum perturbation \(\epsilon\)} \\ Dataset & Model & 0.25 & 0.5 & 1.0 & 5.0 \\ \hline  & TabTr. & \(31.5_{\pm 0.1}\) & \(8.9_{\pm 0.2}\) & \(0.6_{\pm 0.1}\) & \(\mathbf{0.0}_{\pm 0.0}\) \\  & RLN & \(36.5_{\pm 0.4}\) & \(10.8_{\pm 0.2}\) & \(2.7_{\pm 0.2}\) & \(\mathbf{2.3}_{\pm 0.0}\) \\ URL & VIME & \(69.7_{\pm 0.2}\) & \(49.5_{\pm 0.5}\) & \(15.9_{\pm 0.1}\) & \(\mathbf{0.4}_{\pm 0.3}\) \\  & STG & \(81.4_{\pm 0.2}\) & \(58.0_{\pm 0.8}\) & \(16.1_{\pm 3.1}\) & \(\mathbf{0.0}_{\pm 0.0}\) \\  & TabNet & \(21.9_{\pm 0.7}\) & \(11.0_{\pm 0.5}\) & \(\mathbf{2.3}_{\pm 0.4}\) & \(\mathbf{1.6}_{\pm 0.3}\) \\ \hline  & TabTr. & \(23.8_{\pm 0.7}\) & \(7.9_{\pm 0.6}\) & \(\mathbf{2.2}_{\pm 0.3}\) & \(7.1_{\pm 0.9}\) \\  & RLN & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(0.1_{\pm 0.0}\) \\ LCLD & VIME & \(17.0_{\pm 0.2}\) & \(2.4_{\pm 0.1}\) & \(\mathbf{0.3}_{\pm 0.1}\) & \(2.2_{\pm 0.2}\) \\  & STG & \(59.5_{\pm 0.3}\) & \(53.6_{\pm 0.1}\) & \(\mathbf{38.5}_{\pm 0.2}\) & \(48.9_{\pm 0.6}\) \\  & TabNet & \(0.7_{\pm 0.2}\) & \(\mathbf{0.4}_{\pm 0.1}\) & \(\mathbf{0.4}_{\pm 0.1}\) & \(\mathbf{0.3}_{\pm 0.1}\) \\ \hline  & TabTr. & \(95.3_{\pm 0.0}\) & \(95.3_{\pm 0.0}\) & \(95.1_{\pm 0.2}\) & \(\mathbf{86.5}_{\pm 1.9}\) \\  & RLN & \(95.2_{\pm 0.3}\) & \(\mathbf{94.0}_{\pm 0.2}\) & \(\mathbf{94.0}_{\pm 0.2}\) & \(\mathbf{94.0}_{\pm 0.2}\) \\ CTU & VIME & \(56.1_{\pm 3.4}\) & \(\mathbf{40.8}_{\pm 4.7}\) & \(\mathbf{34.0}_{\pm 0.4}\) & \(\mathbf{34.0}_{\pm 0.0}\) \\  & STG & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) \\  & TabNet & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) \\ \hline  & TabTr. & \(61.9_{\pm 0.3}\) & \(45.9_{\pm 0.3}\) & \(22.6_{\pm 0.7}\) & \(\mathbf{2.6}_{\pm 0.5}\) \\  & RLN & \(69.8_{\pm 0.2}\) & \(60.9_{\pm 0.2}\) & \(41.5_{\pm 0.8}\) & \(\mathbf{2.2}_{\pm 0.4}\) \\ WIDS & VIME & \(61.4_{\pm 0.1}\) & \(50.3_{\pm 0.2}\) & \(27.8_{\pm 0.5}\) & \(\mathbf{8.4}_{\pm 0.5}\) \\  & STG & \(70.3_{\pm 0.1}\) & \(63.8_{\pm 0.2}\) & \(50.2_{\pm 0.1}\) & \(\mathbf{7.8}_{\pm 1.0}\) \\  & TabNet & \(29.4_{\pm 0.2}\) & \(5.3_{\pm 0.4}\) & \(\mathbf{1.9}_{\pm 0.4}\) & \(5.2_{\pm 0.5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Robust accuracy with CAA with varying maximum perturbation \(\epsilon\) budget. The lowest robust accuracy is in bold.

Figure 5: Robust accuracy with CAA with varying maximum perturbation \(\epsilon\) budget.

\begin{table}
\begin{tabular}{l l|r r r r} \hline \hline  & & \multicolumn{4}{c}{\# iterations CAPGD} \\ Dataset & Model & 5 & 10 & 20 & 100 \\ \hline  & TabTr. & \(11.4_{\pm 0.5}\) & \(8.9_{\pm 0.2}\) & \(\mathbf{8.4_{\pm 0.1}}\) & \(\mathbf{8.3_{\pm 0.1}}\) \\  & RLN & \(12.2_{\pm 0.4}\) & \(10.8_{\pm 0.2}\) & \(\mathbf{7.7_{\pm 0.1}}\) & \(\mathbf{7.9_{\pm 0.2}}\) \\ URL & VIME & \(52.3_{\pm 0.6}\) & \(49.5_{\pm 0.5}\) & \(47.0_{\pm 0.2}\) & \(\mathbf{46.3_{\pm 0.3}}\) \\  & STG & \(\mathbf{58.0_{\pm 0.8}}\) & \(\mathbf{58.0_{\pm 0.8}}\) & \(\mathbf{58.0_{\pm 0.8}}\) & \(\mathbf{58.0_{\pm 0.8}}\) \\  & TabNet & \(13.8_{\pm 0.4}\) & \(11.0_{\pm 0.5}\) & \(\mathbf{9.3_{\pm 0.3}}\) & \(10.5_{\pm 0.3}\) \\ \hline  & TabTr. & \(9.0_{\pm 0.5}\) & \(7.9_{\pm 0.6}\) & \(6.5_{\pm 0.4}\) & \(\mathbf{6.9_{\pm 0.5}}\) \\  & RLN & \(\mathbf{0.0_{\pm 0.0}}\) & \(\mathbf{0.0_{\pm 0.0}}\) & \(\mathbf{0.0_{\pm 0.0}}\) & \(\mathbf{0.0_{\pm 0.0}}\) \\ LCLD & VIME & \(3.2_{\pm 0.3}\) & \(2.4_{\pm 0.1}\) & \(\mathbf{1.2_{\pm 0.1}}\) & \(\mathbf{1.1_{\pm 0.0}}\) \\  & STG & \(54.5_{\pm 0.1}\) & \(\mathbf{53.6_{\pm 0.1}}\) & \(\mathbf{53.4_{\pm 0.2}}\) & \(\mathbf{53.6_{\pm 0.2}}\) \\  & TabNet & \(0.5_{\pm 0.2}\) & \(0.4_{\pm 0.1}\) & \(\mathbf{0.3_{\pm 0.1}}\) & \(\mathbf{0.1_{\pm 0.1}}\) \\ \hline  & TabTr. & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) \\  & RLN & \(\mathbf{94.0_{\pm 0.2}}\) & \(\mathbf{94.0_{\pm 0.2}}\) & \(\mathbf{94.0_{\pm 0.2}}\) & \(\mathbf{94.0_{\pm 0.2}}\) \\ CTU & VIME & \(\mathbf{40.8_{\pm 4.7}}\) & \(\mathbf{40.8_{\pm 4.7}}\) & \(\mathbf{40.8_{\pm 4.7}}\) & \(\mathbf{40.8_{\pm 4.7}}\) \\  & STG & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) & \(\mathbf{95.3_{\pm 0.0}}\) \\  & TabNet & \(\mathbf{0.0_{\pm 0.0}}\) & \(\mathbf{0.0_{\pm 0.0}}\) & \(\mathbf{0.0_{\pm 0.0}}\) & \(\mathbf{0.0_{\pm 0.0}}\) \\ \hline  & TabTr. & \(49.6_{\pm 0.2}\) & \(45.0_{\pm 0.3}\) & \(\mathbf{42.8_{\pm 0.3}}\) & \(\mathbf{42.4_{\pm 0.2}}\) \\  & RLN & \(64.1_{\pm 0.2}\) & \(60.9_{\pm 0.2}\) & \(58.8_{\pm 0.0}\) & \(\mathbf{57.8_{\pm 0.2}}\) \\ WIDS & VIME & \(52.9_{\pm 0.3}\) & \(50.3_{\pm 0.2}\) & \(48.3_{\pm 0.2}\) & \(\mathbf{47.7_{\pm 0.1}}\) \\  & STG & \(67.1_{\pm 0.1}\) & \(63.8_{\pm 0.2}\) & \(62.5_{\pm 0.2}\) & \(\mathbf{61.8_{\pm 0.1}}\) \\  & TabNet & \(8.9_{\pm 0.4}\) & \(5.3_{\pm 0.4}\) & \(3.3_{\pm 0.5}\) & \(\mathbf{1.6_{\pm 0.2}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Robust accuracy with CAA with varying gradient attack iterations in CAPGD. The lowest robust accuracy is in bold.

Figure 6: Robust accuracy with CAA with varying gradient attack iterations in CAPGD.

Figure 8: Robust accuracy with CAA with varying search attack iterations in MOEVA.

Figure 7: MOEVA success rate (LCLD - TabTransformer).

### Constraints complexity

In this section, we study the impact of the constraints' complexity on CAA's effectiveness.

CAPGD and MOEVA fail to generate adversarial examples on CTU for 2 out of 5 models. CTU dataset has a large number of constraints compared to the other datasets, and some are particularly challenging. We argue that some of these constraints hinder gradient attacks and are harder to optimize. To confirm our hypothesis and provide additional insights, we split the constraints of CTU based on their complexity. We consider two aspects of constraint complexity: the number of constraints to satisfy and the number of features involved in a single constraint.

We split the constraints of CTU as follows:

* **CG0** One constraint involving 90 features in the form of \(\sum F_{i}=\sum F_{j}\) where both sums represent the total number of sent packets.
* **CG1** One constraint involving 90 features in the form of \(\sum F_{i}=\sum F_{j}\) where both sums represent the total number of received packets.
* **CG2** 34 constraints in the form of \(BYTE/PACKETS\leq 1500\), to model the fact that each packet contains at most 1500 bytes.
* **CG3** 324 constraints in the form of \(A\leq B\) where A and B are statistical properties (min, max, sum) for each port, and direction.

First, we ran an ablation study, where we ignored one bucket of constraint at a time. Next, we studied the success rate when we considered each bucket separately. Finally, we reported the impact of the number of constraints to optimize from CG3, the largest bucket.

The results in Table 12 show that for gradient attacks, removing one type of constraint is not enough to improve the success rate. Constraints across multiple remaining categories are not satisfied. The individual bucket study confirms that only when considering constraints of type CG2 alone, CAPGD improves its success rate (in VIME and TabNet). When only considering CG3 constraints, reducing the number of constraints improves the success rate (by reducing robust accuracy from 95.3% when considering 100% of CG3 constraints to 84.5% and 43.0% respectively when considering 50% and 10% of the constraints).

\begin{table}
\begin{tabular}{l l|r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Model} & \multicolumn{4}{c}{\# iterations MOEVA} \\  & & 50 & 100 & 200 & 1000 \\ \hline \multirow{6}{*}{URL} & TabTr. & \(9.3_{\pm 0.0}\) & \(8.9_{\pm 0.2}\) & \(7.0_{\pm 0.2}\) & \(\mathbf{3.8}_{\pm 0.1}\) \\  & RLN & \(11.3_{\pm 0.2}\) & \(10.8_{\pm 0.2}\) & \(9.9_{\pm 0.1}\) & \(\mathbf{9.1}_{\pm 0.4}\) \\  & VIME & \(53.8_{\pm 0.2}\) & \(49.5_{\pm 0.5}\) & \(34.7_{\pm 0.4}\) & \(\mathbf{28.5}_{\pm 0.2}\) \\  & STG & \(64.5_{\pm 1.7}\) & \(58.0_{\pm 0.8}\) & \(48.2_{\pm 1.0}\) & \(\mathbf{43.7}_{\pm 0.8}\) \\  & TabNet & \(14.0_{\pm 0.2}\) & \(11.0_{\pm 0.5}\) & \(5.9_{\pm 0.6}\) & \(\mathbf{0.3}_{\pm 0.1}\) \\ \hline \multirow{6}{*}{LCDD} & TabTr. & \(10.6_{\pm 0.6}\) & \(\mathbf{7.9}_{\pm 0.6}\) & \(\mathbf{8.3}_{\pm 0.5}\) & \(10.5_{\pm 1.1}\) \\  & RLN & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.0}\) \\  & VIME & \(\mathbf{2.3}_{\pm 0.1}\) & \(\mathbf{2.4}_{\pm 0.1}\) & \(\mathbf{2.5}_{\pm 0.1}\) & \(2.6_{\pm 0.1}\) \\  & STG & \(\mathbf{53.7}_{\pm 0.1}\) & \(\mathbf{53.6}_{\pm 0.1}\) & \(\mathbf{53.5}_{\pm 0.2}\) & \(\mathbf{53.6}_{\pm 0.4}\) \\  & TabNet & \(0.8_{\pm 0.2}\) & \(\mathbf{0.4}_{\pm 0.1}\) & \(\mathbf{0.3}_{\pm 0.1}\) & \(\mathbf{0.7}_{\pm 0.8}\) \\ \hline \multirow{6}{*}{CTU} & TabTr. & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.1}_{\pm 0.2}\) \\  & RLN & \(95.8_{\pm 0.4}\) & \(94.0_{\pm 0.2}\) & \(24.1_{\pm 6.1}\) & \(\mathbf{0.0}_{\pm 0.1}\) \\  & VIME & \(76.0_{\pm 2.7}\) & \(40.8_{\pm 4.7}\) & \(6.0_{\pm 1.5}\) & \(\mathbf{0.2}_{\pm 0.0}\) \\  & STG & \(\mathbf{95.3}_{\pm 0.9}\) & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) & \(\mathbf{95.3}_{\pm 0.0}\) \\  & TabNet & \(\mathbf{0.1}_{\pm 0.1}\) & \(\mathbf{0.0}_{\pm 0.0}\) & \(\mathbf{0.0}_{\pm 0.1}\) & \(\mathbf{0.0}_{\pm 0.0}\) \\ \hline \multirow{6}{*}{WIDS} & TabTr. & \(46.5_{\pm 0.5}\) & \(45.9_{\pm 0.3}\) & \(44.8_{\pm 0.2}\) & \(\mathbf{41.2}_{\pm 0.5}\) \\  & RLN & \(61.7_{\pm 0.2}\) & \(60.9_{\pm 0.2}\) & \(60.2_{\pm 0.3}\) & \(\mathbf{56.7}_{\pm 0.4}\) \\ \cline{1-1}  & VIME & \(50.5_{\pm 0.2}\) & \(50.3_{\pm 0.2}\) & \(49.9_{\pm 0.2}\) & \(\mathbf{48.7}_{\pm 0.4}\) \\ \cline{1-1}  & STG & \(63.9_{\pm 0.2}\) & \(63.8_{\pm 0.2}\) & \(63.2_{\pm 0.3}\) & \(\mathbf{61.6}_{\pm 0.3}\) \\ \cline{1-1}  & TabNet & \(6.9_{\pm 0.4}\) & \(5.3_{\pm 0.4}\) & \(2.5_{\pm 0.4}\) & \(\mathbf{0.9}_{\pm 0.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Robust accuracy with CAA with varying search attack iterations in MOEVA. The lowest robust accuracy is in bold.

### Additional defenses

In addition to the application of adversarial training alone, we evaluate the robustness of the models when training with data augmentation. [32] showed that combining data augmentation with adversarial training can increase the robustness of models. We also observe in RobustBench [12], that top-performing models are trained with data augmentation. We consider two data augmentation techniques: Cutmix [45] and CT-GAN [42]. To train our model, at each batch, we use an equal proportion of clean, adversarial examples, data augmentation examples, and adversarial examples generated on top of data augmentation examples.

We evaluated CAA on these two defenses and report in Table 13 the robust accuracy of these defenses, compared to the robustness of the vanilla Madry AT on all our datasets and models. The results are the average over 5 seeds run to ensure reliable evaluation. Our experiments show that these new defenses can significantly improve the robustness of the models to CAA, but that our new attack remains effective for URL and LCLD datasets across all architectures, and for WIDS on TabTransformer and STG architectures.

\begin{table}
\begin{tabular}{l l r r r r r} \hline \hline  & & \multicolumn{5}{c}{Architecture} \\ \cline{3-8} Dataset & Training & RLN & STG & TabNet & TabTr & VIME \\ \hline \multirow{3}{*}{URL} & Adversarial Training & 56.2 & 90.0 & 91.8 & 56.7 & 69.8 \\  & Adv. Tr. + Cutmix & 60.8 & 42.7 & 89.7 & 40.3 & 68.6 \\  & Adv. Tr. + CT-GAN & 62.5 & 79.8 & 89.9 & 66.0 & 66.9 \\ \hline \multirow{3}{*}{LCLD} & Adversarial Training & 63.0 & 12.1 & 0.0 & 70.3 & 10.4 \\  & Adv. Tr. + Cutmix & 47.0 & 36.2 & 0.0 & 71.0 & 52.9 \\  & Adv. Tr. + CT-GAN & 54.3 & 81.2 & 0.0 & 78.5 & 76.8 \\ \hline \multirow{3}{*}{CTU} & Adversarial Training & 97.1 & 95.1 & 0.2 & 95.3 & 94.0 \\  & Adv. Tr. + Cutmix & 95.3 & 94.5 & 0.0 & 95.3 & 94.3 \\  & Adv. Tr. + CT-GAN & 96.7 & 96.0 & 100.0 & 94.4 & 100.0 \\ \hline \multirow{3}{*}{WIDS} & Adversarial Training & 66.6 & 45.2 & 58.4 & 65.1 & 52.1 \\  & Adv. Tr. + Cutmix & 59.9 & 41.2 & 37.4 & 50.8 & 43.5 \\ \cline{1-1}  & Adv. Tr. + CT-GAN & 100.0 & 73.8 & 100.0 & 68.1 & 100.0 \\ \hline \hline \end{tabular}
\end{table}
Table 13: CAA performances against Madry Adversarially Trained model, Adversarial Training + Cutmix and Adversarial Training + CT-GAN.

\begin{table}
\begin{tabular}{l l r r r r r r r r r} \hline \hline  & & \multicolumn{5}{c}{CAPGD} & \multicolumn{5}{c}{CAA} \\ \cline{3-11}  & Group & RLN & STG & TabNet & TabTr. & VIME & RLN & STG & TabNet & TabTr. & VIME \\ \hline \multirow{3}{*}{Ablation} & \(\Omega\setminus CG0\) & 97.8 & 95.3 & 96.1 & 95.3 & 95.1 & 94.0 & 95.3 & 0.0 & 95.3 & 40.8 \\  & \(\Omega\setminus CG1\) & 97.8 & 95.3 & 96.1 & 95.3 & 95.1 & 94.1 & 95.3 & 0.0 & 95.3 & 37.1 \\  & \(\Omega\setminus CG2\) & 97.8 & 95.3 & 96.1 & 95.3 & 95.1 & 93.9 & 95.3 & 0.1 & 95.3 & 40.8 \\  & \(\Omega\setminus CG3\) & 97.8 & 95.3 & 96.1 & 95.3 & 95.1 & 89.3 & 95.3 & 0.0 & 95.3 & 2.4 \\ \hline \multirow{3}{*}{Components} & CG0 & 97.8 & 95.3 & 96.1 & 95.3 & 95.1 & 91.6 & 95.3 & 0.0 & 95.2 & 2.8 \\  & CG1 & 97.8 & 95.3 & 96.1 & 95.3 & 95.1 & 91.1 & 95.3 & 0.0 & 95.2 & 1.9 \\  & CG2 & 75.3 & 95.3 & 31.4 & 94.3 & 0.0 & 72.0 & 95.3 & 0.0 & 94.3 & 0.0 \\  & CG3 & 97.2 & 95.3 & 95.3 & 95.3 & 95.1 & 92.7 & 95.3 & 0.0 & 95.3 & 19.0 \\ \hline \multirow{3}{*}{Percentage CG3} & \(10\%\) & 85.2 & 95.3 & 43.0 & 95.1 & 11.3 & 80.8 & 95.3 & 0.0 & 94.9 & 0.6 \\  & \(25\%\) & 93.4 & 95.3 & 59.9 & 95.3 & 37.8 & 87.1 & 95.3 & 0.0 & 95.3 & 2.5 \\ \cline{1-1}  & \(50\%\) & 94.9 & 95.3 & 84.5 & 95.3 & 93.2 & 88.6 & 95.3 & 0.0 & 95.3 & 8.3 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Robust accuracy with subset of constraints. \(\Omega\) is the complete set of constraints. CGX denotes the constraint group X. For CG2 and CG3, we evaluate with the entire group and on 10%, 25%, 50% selected randomly and averaged on 5 seeds.

### Generalization to shallow models

Shallow models and in particular tree-based models such as Random Forest (RF) XGBoost remain among the best models for tabular data on average [5]. We evaluate the robustness of these models against CAA in two settings: (1) direct attack where CAA (using its search component MOEVA) attacks directly the RF and XGBoOST models, and (2) transfer attacks, where we craft the examples on our deep learning (DL) models and evaluate them in the RF and XGBoOST models. Table 3 shows that (1) DL models of our study achieve comparable clean performance to the shallow models, (2) both RF and XGBoOST models are vulnerable to direct CAA attacks (down to 9.1% of robust accuracy on LCLD XGBoost), and (3) CAA attacks on DNN transfer to RF (down to 5.3% robust accuracy) and XGBoost (down to 9.4% robust accuracy) models.

\begin{table}
\begin{tabular}{l l r r r} \hline \hline Dataset & Model & Clean & Direct & Transferability \\ \hline \multirow{2}{*}{URL} & Random Forest & 96.2 & 52.7 & 72.4 \\  & XGBoost & 97.4 & 27.3 & 46.7 \\ \hline \multirow{2}{*}{LCLD} & Random Forest & 64.3 & 22.3 & 5.3 \\  & XGBoost & 68.3 & 9.1 & 9.4 \\ \hline \multirow{2}{*}{CTU} & Random Forest & 95.6 & 92.2 & 95.0 \\  & XGBoost & 97.3 & 76.3 & 97.1 \\ \hline \multirow{2}{*}{WIDS} & Random Forest & 52.2 & 5.2 & 14.1 \\  & XGBoost & 80.4 & 38.0 & 60.7 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Tree-based model robust accuracy in direct and transferability scenario (minimum robust accuracy over 5 neural networks).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper's contributions are two-folds: two new attacks (CAPGD and CAA) and an extensive study of five architectures and four datasets. These contributions match the structure of our paper and the figures and claims reported in the abstract and introduction match the figures of the experimental study. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation of CAPGD in Section 4.2, and the limitations of CAA against adversarial training in Section 5.4. Overall, we provided ablations studies for our new algorithms and analysis of the impact budget and computation ressources of our approaches. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]. Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We dedicate Section 3.2 and the Appendix A to present the experimental protocol and disclose all the information needed to reproduce all the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We submitted the source code, data and pre-trained models to reproduce the experiments in supplementary. As mentioned in Section A.8, the source code and pre-trained models will be publicly available upon acceptance under open-source licenses. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, the parameters used for our experiments are summarized in Section 3.2, detailed in Appendix A.6, and provided in source code. Furthermore, in Section 5.3 and Appendix B.2, we study the impact of the main attack parameters. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the 95% CI in all tables and figures when average results over random runs are needed in experiments. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report the exact hardware used for our experiment in Appendix A.7. The execution time is included in the main paper as part of our experiments in Table 3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We read and conform to the code of ethics and all laws and regulations in our jurisdiction. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]Justification: We have provided a "Broader Impact" section to explain the negative impacts of our new approach, and how by studying countermeasures in our work, we expect our research to bring more benefits to the defense community than the attackers. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Appendix A.4 cites the original creators of the datasets and provides their licences. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the algorithms of the attack in the Appendix to help understand our implementation. The reproduction package contains the code of all algorithms and experiments and is documented to guide all users. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects, nor any element that requires an Institutional Review Board (IRB) approval.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.