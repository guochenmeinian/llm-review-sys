# Towards the Transferability of Rewards Recovered

via Regularized Inverse Reinforcement Learning

 Andreas Schlaginhaufen

SYCAMORE, EPFL

andreas.schlaginhaufen@epfl.ch

&Maryam Kamgarpour

SYCAMORE, EPFL

maryam.kamgarpour@epfl.ch

###### Abstract

Inverse reinforcement learning (IRL) aims to infer a reward from expert demonstrations, motivated by the idea that the reward, rather than the policy, is the most succinct and transferable description of a task [20]. However, the reward corresponding to an optimal policy is not unique, making it unclear if an IRL-learned reward is transferable to new transition laws in the sense that its optimal policy aligns with the optimal policy corresponding to the expert's true reward. Past work has addressed this problem only under the assumption of full access to the expert's policy, guaranteeing transferability when learning from two experts with the same reward but different transition laws that satisfy a specific rank condition [17]. In this work, we show that the conditions developed under full access to the expert's policy cannot guarantee transferability in the more practical scenario where we have access only to demonstrations of the expert. Instead of a binary rank condition, we propose principal angles as a more refined measure of similarity and dissimilarity between transition laws. Based on this, we then establish two key results: 1) a sufficient condition for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws, and 2) a sufficient condition for transferability to local changes in the transition law when learning from a single expert. Furthermore, we also provide a probably approximately correct (PAC) algorithm and an end-to-end analysis for learning transferable rewards from demonstrations of multiple experts.

## 1 Introduction

Reinforcement learning (RL) has achieved remarkable success in various domains such as robotics [14], autonomous driving [15], or fine-tuning of large language models [21]. Despite these advances, a key challenge lies in designing appropriate reward functions that reflect the desired outcomes and align with human values. Misaligned rewards can lead to suboptimal behaviors [20], undermining the potential benefits of RL in practical scenarios. Inverse reinforcement learning (IRL), also known as inverse optimal control [18] or structural estimation [16], addresses this problem by inferring a reward from demonstrations of an expert acting optimally in a Markov decision process (MDP).

Compared to behavioral cloning [17], which directly fits a policy to the expert's demonstrations, IRL is believed to provide a more transferable description of the expert's task [20], as recovering the expert's underlying reward would enable us to train a policy in a new environment with different dynamics. However, it is also known that the reward corresponding to some optimal policy is not unique [20], making it difficult to recover the expert's true underlying reward. This raises the question: _Is a reward recovered via IRL transferable to a newenvironment in the sense that its optimal policy aligns with the expert's true reward?_ For example, in autonomous driving, could we effectively reuse a reward learned from demonstrations of one car in a given city to train or fine-tune a policy for another car in another city?

Ensuring transferability is challenging, as neither the optimal policy corresponding to a reward nor the reward corresponding to an optimal policy is unique. This leads to trivial solutions to the IRL problem, such as constant rewards that make all policies optimal. Common approaches to address this challenge include characterizing the entire set of rewards for which the expert is optimal (Metelli et al., 2021), or assuming the expert is optimal with respect to an entropy regularized RL problem (Ziebart, 2010), leading to many popular IRL and imitation learning algorithms (Ho and Ermon, 2016; Fu et al., 2017; Garg et al., 2021). Entropy regularization results in a unique and more uniform optimal policy, serving as a model for the expert's bounded rationality (Ortega et al., 2015).

In the entropy-regularized setting, several recent works study the set of rewards for which a given expert policy is optimal. In particular, Cao et al. (2021); Skalse et al. (2023) show that under entropy regularization, the expert's reward can be identified up to so-called potential shaping transformations (Ng et al., 1999). The authors of (Schlagnhaufen and Kamgarpour, 2023) extend this result to more general steep regularization. Furthermore, they show that to guarantee transferability to any transition law, the expert's reward needs to be identified up to a constant. The latter can be achieved either by restricting the reward class, e.g., to state-only rewards (Amin et al., 2017), or by learning from multiple experts with the same reward but different transition laws, given that a specific rank condition is satisfied (Cao et al., 2021; Rolland et al., 2022). However, the above results cannot be applied directly in practice, as they rely on having full access to the experts' policies, whereas in practice, we typically only have a finite set of demonstrations available.

ContributionsWe consider the framework of regularized IRL (Jeon et al., 2021) and address the transferability of rewards recovered from a finite set of expert demonstrations.

* We define a novel notion of transferability (Definition 3.1), to address the practical limitation of not having perfect access to the experts' policies. Furthermore, we show that when learning from finite data, the conditions developed under full access to the experts' policies are not sufficient to guarantee transferability (Example 3.3).
* Instead of a binary rank condition, we propose to use principal angles to characterize the similarity and dissimilarity between transition laws (Definition 3.8). Based on these principal angles, we then establish two key transferability results: 1) a guarantee for transferability to any transition laws when learning from at least two experts with sufficiently different transition laws (Theorem 3.10), and 2) a guarantee for transferability to local changes in the transition law when learning from a single expert (Theorem 3.11).
* Assuming oracle access to a probably approximately correct (PAC) algorithm for the forward RL problem, we provide a PAC algorithm for the IRL problem, which in \(\mathcal{O}(K^{2}/\hat{\varepsilon}^{2})\) steps recovers a reward for which, with high probability, all \(K\) experts are \(\hat{\varepsilon}\)-optimal (Theorem 4.1). Together with our results on transferability, this establishes end-to-end guarantees for learning transferable rewards from a finite set of expert demonstrations.
* We experimentally validate our results in a gridworld environment (Section 5).1

Footnote 1: The code is openly accessible at https://github.com/andrschl/transfer_irl.

## 2 Background

NotationGiven \(x\) and \(y\) in some Euclidean vector space \(\mathcal{V}\), we denote the \(p\)-norm by \(\|x\|_{p}\), the orthogonal projection onto a closed convex set \(\mathcal{X}\subset\mathcal{V}\) by \(\Pi_{\mathcal{X}}(x)=\arg\min_{y\in\mathcal{X}}\|x-y\|_{2}\), and the standard dot product by \(\langle x,y\rangle\). For a linear operator \(A\), we denote its image and rank by \(\operatorname{im}A\) and \(\operatorname{rank}A\), respectively. Given two sets \(\mathcal{X}\) and \(\mathcal{Y}\), we denote \(\mathcal{X}+\mathcal{Y}\) for their Minkowski sum and \(\mathcal{Y}^{\mathcal{X}}\) for the set of all functions mapping from \(\mathcal{X}\) to \(\mathcal{Y}\). Additionally, we denote \(\Delta_{\mathcal{X}}\) for the probability simplex over \(\mathcal{X}\) and \(\mathbbm{1}\) for the indicator function. The interior \(\operatorname{int}\mathcal{X}\), the relative interior \(\operatorname{relint}\mathcal{X}\), the relative boundary \(\operatorname{relbd}\mathcal{X}\), and the convex hull \(\operatorname{conv}\mathcal{X}\) of some set \(\mathcal{X}\) are defined in Appendix A, along with an overview of all other notations.

Regularized MDPsWe consider a regularized MDP (Geist et al., 2019) defined by a tuple \((\mathcal{S},\mathcal{A},P,\nu_{0},r,\gamma,h)\). Here, \(\mathcal{S}\) and \(\mathcal{A}\) represent finite state and action spaces with \(|\mathcal{S}|,|\mathcal{A}|>1\), \(\nu_{0}\in\Delta_{\mathcal{S}}\) the initial state distribution, \(P\in\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\) the transition law, \(r\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) the reward, and \(\gamma\in(0,1)\) the discount factor. Furthermore, \(h:\mathcal{X}\to\mathbb{R}\) is a strictly convex regularizer that is defined on a closed convex set \(\mathcal{X}\subseteq\mathbb{R}^{\mathcal{A}}\) with \(\operatorname{relint}\Delta_{\mathcal{A}}\subseteq\operatorname{int}\mathcal{ X}\). Starting from some initial state \(s_{0}\sim\nu_{0}\) the agent can at each step in time \(t\), choose an action \(a_{t}\in\mathcal{A}\), will arrive in state \(s_{t+1}\sim P(\cdot|s_{t},a_{t})\), and receives reward \(r(s_{t},a_{t})\). The goal is to find a Markov policy \(\pi\in\Delta_{\mathcal{A}}^{\mathcal{S}}\) maximizing the regularized objective \(\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t}\left[r(s_{t},a_{t})-h\left( \pi(\cdot|s_{t})\right)\right]\right]\). Following the classical linear programming approach to MDPs (Puterman, 2014), this can be cast equivalently as the convex optimization problem

\[\max_{\mu\in\mathcal{M}}J(r,\mu),\quad\text{with}\quad J(r,\mu):=\langle r, \mu\rangle-\bar{h}(\mu),\] (O-RL)

where \(\mathcal{M}\) denotes the set of occupancy measures, \(\mu^{\pi}(s,a):=(1-\gamma)\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty}\gamma^{t }\mathds{1}(s_{t}=s,a_{t}=a)\right]\), and we have \(\bar{h}(\mu):=\mathbb{E}_{(s,a)\sim\mu}\left[h(\pi^{\mu}(\cdot|s))\right]\), with \(\pi^{\mu}\) being the policy corresponding to \(\mu\) (see Appendix A). The set of occupancy measures is characterized by the Bellman flow constraints

\[\mathcal{M}=\left\{\mu\in\mathbb{R}_{+}^{\mathcal{S}\times\mathcal{A}}:(E- \gamma P)^{\top}\mu=(1-\gamma)\nu_{0}\right\}\subseteq\Delta_{\mathcal{S} \times\mathcal{A}},\]

where \(E:\mathbb{R}^{\mathcal{S}}\to\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) and \(P:\mathbb{R}^{\mathcal{S}}\to\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) are the linear operators mapping \(v\in\mathbb{R}^{\mathcal{S}}\) to \((Ev)(s,a)=v(s)\) and \((Pv)(s,a)=\sum_{s^{\prime}}P(s^{\prime}|s,a)v(s^{\prime})\), respectively.

Due to the strict convexity of \(h\), the regularized MDP problem has a unique optimal policy (Geist et al., 2019), hence guaranteeing the uniqueness of the optimal occupancy measure in (O-RL). In addition, we assume that the gradients of \(h\) become unbounded towards the relative boundary of the simplex as detailed in Assumption 2.1 below.

**Assumption 2.1** (Steep regularization).: Suppose that \(h:\mathcal{X}\to\mathbb{R}\) is differentiable in \(\operatorname{int}\mathcal{X}\) and that \(\lim_{l\to\infty}\left\|\nabla h(p_{l})\right\|_{2}=\infty\) if \(\left(p_{l}\right)_{l\in\mathbb{N}}\) is a sequence in \(\operatorname{int}\mathcal{X}\) converging to a point \(p\in\operatorname{relbd}\Delta_{\mathcal{A}}\).

Assumption 2.1 ensures that the optimal policy is non-vanishing, and together with Assumption 2.2 below, we also have that the optimal occupancy measure is non-vanishing.

**Assumption 2.2** (Exploration).: Let \(\nu(s):=\sum_{a}\mu(s,a)\geq\nu_{\text{min}}>0\) for any \(s\in\mathcal{S}\) and \(\mu\in\mathcal{M}\).

One way to guarantee Assumption 2.2 is to impose a lower bound on the initial state distribution \(\nu_{0}\). In the following, it will be convenient to denote the optimal solution to (O-RL) for the reward \(r\) as

\[\text{RL}(r):=\operatorname*{argmax}_{\mu\in\mathcal{M}}J(r,\mu),\]

and the suboptimality of some occupancy measure \(\mu\) for the reward \(r\) as

\[\mathsf{SubOpt}(r,\mu):=\max_{\mu^{\prime}\in\mathcal{M}}J(r,\mu^{\prime})-J(r,\mu).\] (1)

That is, \(\mu=\text{RL}(r)\) if and only if \(\mathsf{SubOpt}(r,\mu)=0\).

_Remark 2.3_.: As we aim to analyze the transferability of rewards to new transition laws \(P\in\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\), it will often be useful to explicitly specify the dependency on \(P\). We do so by adding a subscript - e.g. we write \(\mathcal{M}_{P}\), \(\text{RL}_{P}\), and \(\mathsf{SubOpt}_{P}\). However, for better readability, we drop these subscripts whenever there is no potential for confusion.

Inverse reinforcement learningGiven a dataset of trajectories sampled from an expert \(\mu^{\mathsf{E}}\) that is optimal for some reward \(r^{\mathsf{E}}\), the goal in IRL is to recover a reward \(\hat{r}\in\mathcal{R}\), within a predefined reward class \(\mathcal{R}\subseteq\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\), such that the expert is optimal for \(\hat{r}\). That is, ideally, we aim to find a reward in the feasible reward set

\[\text{IRL}(\mu^{\mathsf{E}}):=\left\{r\in\mathcal{R}:\mu^{\mathsf{E}}\in\text{RL} (r)\right\}.\] (2)

However, since we don't have direct access to the expert's policy but only to a finite set of demonstrations, the best we can hope for is an algorithm that with high probability outputs a reward \(\hat{r}\in\mathcal{R}\) such that \(\mathsf{SubOpt}(\hat{r},\mu^{\mathsf{E}})\) is small - i.e. an algorithm that is PAC (Syed and Schapire, 2007).

Reward equivalenceThe reward corresponding to an optimal occupancy is not unique. For example, all rewards in the affine subspace \(r+\mathcal{U}\), where \(\mathcal{U}:=\mathrm{im}(E-\gamma P)\) is the subspace of so-called potential shaping transformations, correspond to the same optimal occupancy measure (Ng et al., 1999). From a geometric perspective, the subspace \(\mathcal{U}=\mathrm{im}(E-\gamma P)\) lies perpendicular to the set of occupancy measures \(\mathcal{M}\). Therefore, adding an element of \(\mathcal{U}\) to the reward leaves the performance difference between any two occupancy measures invariant. Hence, it is often convenient to consider these rewards as equivalent (Kim et al., 2021) and to measure distances between rewards in the resulting quotient space. Given a linear subspace \(\mathcal{V}\subset\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\), the quotient space \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{V}\) is the set of all equivalence classes \([r]_{\mathcal{V}}:=\big{\{}r^{\prime}\in\mathbb{R}^{\mathcal{S}\times\mathcal{ A}}:r^{\prime}-r\in\mathcal{V}\big{\}}\), which is itself a vector space with addition and multiplication operation defined by \([r]_{\mathcal{V}}+[r^{\prime}]_{\mathcal{V}}=[r+r^{\prime}]_{\mathcal{V}}\) and \(c[r]_{\mathcal{V}}=[cr]_{\mathcal{V}}\) for \(c\in\mathbb{R}\). Intuitively, \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{V}\) is the vector space obtained by collapsing \(\mathcal{V}\) to zero, or in other words, it is isomorphic to the orthogonal complement of \(\mathcal{V}\). We endow \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{V}\) with the quotient norm \(\left\|[r]_{\mathcal{V}}\right\|_{2}:=\min_{v\in\mathcal{V}}\left\|r+v\right\| _{2}=\left\|\Pi_{v\downarrow^{\mathcal{V}}}\right\|_{2}\) and we say that \(r\) and \(r^{\prime}\) are close in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{V}\) if \(\left\|[r]_{\mathcal{V}}-[r^{\prime}]_{\mathcal{V}}\right\|_{2}\) is small. Moreover, the expert's reward is said to be identifiable up to some equivalence class \([\cdot]_{\mathcal{V}}\) if \(\mathsf{IRL}(\mu^{\mathsf{E}})\subseteq[r^{\mathsf{E}}]_{\mathcal{V}}\). In this paper, we will consider the equivalence relations induced by constant shifts, i.e., \(\mathcal{V}=\mathbf{1}:=\big{\{}r\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}: r(s,a)=c\in\mathbb{R}\big{\}}\), and by potential shaping transformations, i.e., \(\mathcal{V}=\mathcal{U}\). Note that since \(\mathbf{1}\) is a subspace of \(\mathcal{U}\) and \(\mathcal{U}\) is \(|\mathcal{S}|\)-dimensional, \([r]_{\mathbf{1}}\) is a strict subset of \([r]_{\mathcal{U}}\) whenever \(|\mathcal{S}|>1\).

## 3 Transferability

In this section, we present our main results on transferability in IRL. To this end, we first introduce the problem of learning \(\varepsilon\)-transferable rewards from multiple experts acting in different environments.

### Problem formulation

Let \(\mathcal{R}\subseteq\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) be a compact reward class, and suppose we are given access to \(K\) expert data sets,

\[\mathcal{D}_{k}^{\mathsf{E}}=\left\{\left(s_{0}^{k,i},a_{0}^{k,i},\ldots,s_{H^ {\mathsf{E}}-1}^{k,i},a_{H^{\mathsf{E}}-1}^{k,i}\right)\right\}_{i=0}^{N^{ \mathsf{E}}-1},\quad k=0,\ldots,K-1,\]

consisting of trajectories sampled independently from the experts \(\mu^{\mathsf{E}}_{P^{0}},\ldots,\mu^{\mathsf{E}}_{P^{K-1}}\). Each expert is optimal for the same unrevealed reward \(r^{\mathsf{E}}\in\mathcal{R}\), but under different transition laws, \(P^{0},\ldots,P^{K-1}\). Our goal is to recover a reward \(\hat{r}\in\mathcal{R}\) that is transferable across a set of transition laws \(\mathcal{P}\subseteq\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\). Specifically, the optimal occupancy measure corresponding to \(\hat{r}\) should remain approximately optimal for \(r^{\mathsf{E}}\) under every transition law in \(\mathcal{P}\). This yields the following definition of \(\varepsilon\)-transferability.

**Definition 3.1** (\(\varepsilon\)-transferability).: Fix some \(\varepsilon>0\). We say that \(\hat{r}\) is \(\varepsilon\)-transferable to some set of transition laws \(\mathcal{P}\subseteq\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\) if \(\mathsf{SubOpt}_{P}(r^{\mathsf{E}},\mathsf{IRL}_{P}(\hat{r}))\leq\varepsilon\) for all \(P\in\mathcal{P}\). We say that \(\hat{r}\) is exactly transferable to \(\mathcal{P}\) if it is \(\varepsilon\)-transferable to \(\mathcal{P}\) with \(\varepsilon=0\).

The error margin of \(\varepsilon\) is crucial, as exact transferability is unrealistic when learning from finite expert data. Moreover, note that Definition 3.1 is a definition of uniform transferability, as it requires \(\hat{r}\) to be \(\varepsilon\)-transferable to any \(P\in\mathcal{P}\) with the same fixed \(\varepsilon\). In the following, we will analyze the transferability of a reward \(\hat{r}\) for which all experts are \(\hat{\varepsilon}\)-optimal for some \(\hat{\varepsilon}>0\). That is,

\[\mathsf{SubOpt}_{P^{k}}(\hat{r},\mu^{\mathsf{E}}_{P^{k}})\leq\hat{\varepsilon},\quad k=0,\ldots,K-1.\] (3)

In particular, we aim to establish appropriate conditions for choosing \(\hat{\varepsilon}\) so as to guarantee \(\varepsilon\)-transferability to some set of transition laws \(\mathcal{P}\). In Section 4, we will then provide an IRL algorithm that, with high probability, outputs a reward \(\hat{r}\) such that (3) holds.

_Remark 3.2_.: As discussed in Appendix J, the assumption of perfect expert optimality with respect to \(r^{\mathsf{E}}\) can be relaxed to allow for a misspecification error. All our results remain applicable in this setting but include an additional error term due to the experts' suboptimality.

### Related work

Most previous work has focused on reward identifiability. For a single expert, Cao et al. (2021); Skalse et al. (2023); Schlaginhaufen and Kamgarpour (2023) show that under Assumption 2.1 (steepness)the feasible reward set (2) can be expressed as

\[\mathsf{IRL}(\mu^{\mathsf{E}})=\big{(}\nabla\bar{h}(\mu^{\mathsf{E}})+\mathcal{U} \big{)}\cap\mathcal{R}=[r^{\mathsf{E}}]_{\mathcal{U}}\cap\mathcal{R}.\] (4)

In other words, steepness ensures that the expert's reward is identifiable up to potential shaping. To identify the reward up to a constant, we can either restrict the reward class, e.g. to state-only rewards as explored by Amin et al. (2017), or learn from multiple experts (Cao et al., 2021; Rolland et al., 2022). In particular, when we are given access to two experts, \(\mu^{\mathsf{E}}_{P^{0}}\) and \(\mu^{\mathsf{E}}_{P^{1}}\), we can identify the experts' reward up to the intersection

\[\mathsf{IRL}_{P^{0}}(\mu^{\mathsf{E}}_{P^{0}})\cap\mathsf{IRL}_{P^{1}}(\mu^{ \mathsf{E}}_{P^{1}})=[r^{\mathsf{E}}]_{\mathcal{U}_{P^{0}}}\cap[r^{\mathsf{E}} ]_{\mathcal{U}_{P^{1}}}\cap\mathcal{R}=\big{(}r^{\mathsf{E}}+\mathcal{U}_{P^{ 0}}\cap\mathcal{U}_{P^{1}}\big{)}\cap\mathcal{R}.\]

That is, for the unrestricted reward class, \(\mathcal{R}=\mathbb{R}^{S\times\mathcal{A}}\), the reward is identifiable up to a constant if and only if \(\mathcal{U}_{P^{0}}\cap\mathcal{U}_{P^{1}}=\mathbf{1}\). Or equivalently, if and only if the rank condition

\[\mathrm{rank}\,\big{(}\big{[}E-\gamma P^{0},\qquad E-\gamma P^{1}\big{]}\big{)} =2|\mathcal{S}|-1,\] (5)

is satisfied (Rolland et al., 2022). Moreover, Schlaghinhutern and Kamgarpour (2023) show that identifying the expert's reward up to a constant is a necessary and sufficient condition for exact transferability to any full-dimensional set \(\mathcal{P}\subseteq\Delta^{\mathcal{S}\times\mathcal{A}}_{\mathcal{S}}\) (a set \(\mathcal{P}\) whose interior, with respect to the subspace topology on \(\Delta^{\mathcal{S}\times\mathcal{A}}_{\mathcal{S}}\)(Bourbaki, 1966), is non-empty).

LimitationsThe above results assume perfect access to the expert's policy, which isn't realistic. In practice, we can only learn a reward for which the experts are approximately optimal. In Example 3.3 below, we show that under approximate optimality of the experts, the learned reward can perform very poorly in a new environment, even if the rank condition in Equation (5) is satisfied.

**Example 3.3**.: We consider a two-state, two-action MDP with \(\mathcal{S}=\mathcal{A}=\{0,1\}\), uniform initial state distribution, discount rate \(\gamma=0.9\), and Shannon entropy regularization \(h=-\mathcal{H}\) (see Appendix C). Suppose the expert reward is \(r^{\mathsf{E}}(s,a)=1\{s=1\}\) and consider the transition laws, \(P^{0}\) and \(P^{1}\), defined by \(P^{0}(0|s,a)=0.75\) and \(P^{1}(0|s,a)=0.25+\beta\cdot 1\,\{s=0,a=0\}\) for some \(\beta\in[0,0.75]\). Also, consider the two experts \(\mu^{\mathsf{E}}_{P^{0}}=\mathsf{RL}_{P^{0}}(r^{\mathsf{E}})\) and \(\mu^{\mathsf{E}}_{P^{1}}=\mathsf{RL}_{P^{1}}(r^{\mathsf{E}})\), and suppose we recovered the reward \(\hat{r}(s,a)=-r^{\mathsf{E}}\). Then, as detailed in Appendix E, the following holds: 1) We have \(\mathsf{SubOpt}_{P^{0}}(\hat{r},\mu^{\mathsf{E}}_{P^{0}})=0\) and \(\mathsf{SubOpt}_{P^{1}}(\hat{r},\mu^{\mathsf{E}}_{P^{1}})=\mathcal{O}(\beta)\). That is, for small \(\beta\), the reward \(\hat{r}\) is a good solution to the IRL problem, as both experts are approximately optimal under \(\hat{r}\). 2) The rank condition (5) between \(P^{0}\) and \(P^{1}\) is satisfied for any \(\beta\geq 0\). 3) For a new transition law \(P\) defined by \(P(0|s,a)=1\,\{s=1,a=0\}\), we have \(\mathsf{SubOpt}_{P}(r^{\mathsf{E}},\mathsf{RL}_{P}(\hat{r}))\approx 4.81\), i.e. \(\mathsf{RL}_{P}(\hat{r})\) performs poorly under the experts' reward.

### Theoretical insights

To establish a sufficient condition for \(\varepsilon\)-transferability, our goal is to bound the suboptimality of an optimal occupancy measure, \(\mathsf{RL}(r)\), for some reward \(r^{\prime}\), in terms of reward distances measured in the quotient space \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{U}\). To this end, we first establish the relationship between the suboptimality in Equation (1) and the Bregman divergence corresponding to the occupancy measure regularization.

Bregman divergencesThe Bregman divergence (Teboulle, 1992) associated to \(\bar{h}\) is defined as

\[D_{\bar{h}}(\mu,\mu^{\prime})=\bar{h}(\mu)-\bar{h}(\mu^{\prime})-\langle\nabla \bar{h}(\mu^{\prime}),\mu-\mu^{\prime}\rangle.\]

**Proposition 3.4**.: _Under Assumptions 2.1 and 2.2, we have \(\mathsf{SubOpt}(r^{\prime},\mu)=D_{\bar{h}}(\mu,\mathsf{RL}(r^{\prime}))\) for any \(\mu\in\mathcal{M}\)._

Proposition 3.4 above demonstrates that the suboptimality of an occupancy measure \(\mu\) for the reward \(r^{\prime}\) coincides with the Bregman divergence between \(\mu\) and the optimal occupancy measure under \(r^{\prime}\). This generalizes (Mei et al., 2020, Lemma 26) from entropy regularization to any steeply regularized MDP. The proof is presented in Appendix D.6.

Reward approximationNext, we show that under strong convexity and local Lipschitz gradients, the Bregman divergence between two optimal occupancy measures is bounded in terms of reward distances in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{U}\).

**Assumption 3.5** (Regularity).: Suppose the following holds:

1. The regularizer \(\bar{h}\) is \(\eta\)-strongly convex over the set of occupancy measures \(\mathcal{M}\). That is, we have \[\bar{h}(\mu^{\prime})\geq\bar{h}(\mu)+\left\langle\nabla\bar{h}(\mu),\mu^{ \prime}-\mu\right\rangle+\frac{\eta}{2}\left\|\mu^{\prime}-\mu\right\|_{2}^{2},\quad\forall\mu,\mu^{\prime}\in\mathcal{M}.\]
2. The gradient \(\nabla\bar{h}\) is locally Lipschitz continuous over \(\operatorname{relint}\mathcal{M}\). That is, for any closed convex subset \(\mathcal{K}\subset\operatorname{relint}\mathcal{M}\) there exists \(L_{\mathcal{K}}>0\) such that \[\left\|\nabla\bar{h}(\mu)-\nabla\bar{h}(\mu^{\prime})\right\|_{2}\leq L_{ \mathcal{K}}\left\|\mu-\mu^{\prime}\right\|_{2},\quad\forall\mu,\mu^{\prime} \in\mathcal{K}.\]

We will show later that Assumption 3.5 is met for Shannon and Tsallis entropy regularization (see Proposition D.9). Under the above assumption, the following lemma establishes the desired upper and lower bound on the Bregman divergence between two optimal occupancy measures with respect to reward distances measured in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}\).

**Lemma 3.6**.: _Suppose Assumptions 2.1,2, and 3.5 hold, and let \(r,r^{\prime}\in\mathcal{R}\). Then, we have_

\[\frac{\sigma_{\mathcal{R}}}{2}\left\|[r]_{\mathcal{U}}-[r^{\prime}]_{ \mathcal{U}}\right\|_{2}^{2}\leq\mathsf{SubOpt}(r^{\prime},\mathsf{RL}(r))=D_ {\bar{h}}\left(\mathsf{RL}(r),\mathsf{RL}(r^{\prime})\right)\leq\frac{1}{2\eta }\left\|[r]_{\mathcal{U}}-[r^{\prime}]_{\mathcal{U}}\right\|_{2}^{2},\] (6)

_for some problem-dependent constant \(\sigma_{\mathcal{R}}>0\)._

_Remark 3.7_.: The proof of Lemma 3.6 hinges on the duality between equivalence classes of rewards and optimal occupancy measures (see Appendix B). The main idea is to leverage duality of Bregman divergences, and a dual smoothness and strong convexity result in Proposition D.7. A key challenge arises because, by Assumption 2.1, the regularizer cannot be globally smooth. This results in a problem-dependent dual strong convexity constant \(\sigma_{\mathcal{R}}\)(Goebel and Rockafellar, 2008). In Proposition D.9, we will provide a lower bound on \(\sigma_{\mathcal{R}}\) for the specific choices of Shannon and Tsallis entropy regularization. For more details, we refer to the full proof in Appendix D.6.

The above lemma has two key implications: First, the lower bound in (6) implies that if we recover a reward \(\hat{r}\) for which all experts are approximately optimal, then the distance between \(\hat{r}\) and \(r^{\mathsf{E}}\) can be bounded in the quotient spaces \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P^{\mathsf{E}}}\). Second, the upper bound shows that to control the performance of \(\mathsf{RL}_{P}(\hat{r})\) in a new environment \(P\), we need to tightly bound the distance between \(\hat{r}\) and \(r^{\mathsf{E}}\) in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P}\). As distances in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P}\) are bounded by distances in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\), this can be achieved by bounding the distance between \(\hat{r}\) and \(r^{\mathsf{E}}\) in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\). However, revisiting Example 3.3 in light of Lemma 3.6 shows that even though \(\hat{r}\) and \(r^{\mathsf{E}}\) are close in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P^{\mathsf{E}}}\), this does not guarantee their proximity in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\) and \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P}\).

**Example 3.3** (continued).: Recall the definition \(\mathcal{U}_{P^{\mathsf{E}}}=\operatorname{im}(E-\gamma P^{k})\). Given that in Example 3.3 we have \(\mathsf{SubOpt}_{P^{\mathsf{O}}}(\hat{r},\mu^{\mathsf{E}}_{P^{\mathsf{O}}})=0\) and \(\mathsf{SubOpt}_{P^{\mathsf{O}}}(\hat{r},\mu^{\mathsf{E}}_{P^{\mathsf{O}}})= \mathcal{O}(\beta)\), Lemma 3.6 ensures that \(\hat{r}\) and \(r^{\mathsf{E}}\) coincide in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P^{\mathsf{O}}}\), and for small \(\beta\), they are close in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P^{\mathsf{O}}}\). However, as illustrated in Figure1(a) this doesn't ensure that \(\hat{r}\) and \(r^{\mathsf{E}}\) are close in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\) and \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}_{P}\). In particular, it can be computed that \(\left\|[\hat{r}]_{\mathcal{U}_{P}}-[r^{\mathsf{E}}]_{\mathcal{U}_{P}}\right\|_ {2}\approx 1.51\), which by Lemma 3.6 explains the poor transferability to \(P\).

### Sufficient conditions for transferability

With Lemma 3.6 in place, we are set to present our results on \(\varepsilon\)-transferability. Example 3.3 indicates that a sufficient condition for learning transferable rewards from experts (\(K=2\)) should not rely solely on the binary rank condition (5), which only checks if \(\mathcal{U}_{P^{\mathsf{O}}}\cap\mathcal{U}_{P^{\mathsf{O}}}=\mathbf{1}\). Instead, we should consider the relative orientation between \(\mathcal{U}_{P^{\mathsf{O}}}\) and \(\mathcal{U}_{P^{\mathsf{O}}}\). To formalize this, we need to introduce the concept of principal angles between linear subspaces, as outlined in Definition 3.8 below.

**Definition 3.8** (Principal angles (Galantai, 2013)).: Let \(\mathcal{V},\mathcal{W}\subseteq\mathbb{R}^{n}\) be two subspaces of dimension \(m\leq n\). The principal angles \(0\leq\theta_{1}(\mathcal{V},\mathcal{W})\leq\ldots\leq\theta_{m}(\mathcal{V}, \mathcal{W})=:\theta_{\max}(\mathcal{V},\mathcal{W})\leq\pi/2\) between \(\mathcal{V}\) and \(\mathcal{W}\) are defined recursively via

\[\cos(\theta_{i}(\mathcal{V},\mathcal{W}))=\max_{v\in\mathcal{V},w\in\mathcal{W }}\left\langle v,w\right\rangle\text{ s.t. }\left\|v\right\|_{2}=\left\|w\right\|_{2}=1,\,\left\langle v,v_{j} \right\rangle=\left\langle w,w_{j}\right\rangle=0,\,j=1,\ldots,i-1,\]

where \(v_{j},w_{j}\) are the maximizers corresponding to the angle \(\theta_{j}\). For two transition laws \(P,P^{\prime}\), we define \(\theta_{i}(P,P^{\prime}):=\theta_{i}(\mathcal{U}_{P},\mathcal{U}_{P^{\prime}})\) and refer to \(\theta_{i}(P,P^{\prime})\) as the \(i\)-th principal angles between \(P\) and \(P^{\prime}\).

Principal angles are the natural generalization of angles between two lines or planes to higher dimensional subspaces. For principal angles between transition laws, we have the following proposition.

**Proposition 3.9**.: _Let \(P,P^{\prime}\in\Delta_{\mathcal{S}}^{S\times\mathcal{A}}\) and \(H_{\gamma}=1/(1-\gamma)\). Then, we have \(\theta_{1}(P,P^{\prime})=0\) and \(\sin\left(\theta_{\max}(P,P^{\prime})\right)\leq\gamma H_{\gamma}\sqrt{| \mathcal{S}|/|\mathcal{A}|}\left\|P-P^{\prime}\right\|\), where \(\left\|\cdot\right\|\) denotes the spectral norm._

The proof can be found in Appendix D.7. The above result shows that while the first principal angle between two transition laws is always zero, all principal angles are small if the transition laws are close to one another. In Example 3.3, we have \(\sin(\theta_{2}(P^{0},P^{1}))=\mathcal{O}(\beta)\), indicating that the second and in this case maximal principal angle is small when \(\beta\) is small (see Appendix E). The following result shows that when learning from two experts, the transferability error is directly controlled by the second principal angle between the experts' transition laws.

**Theorem 3.10**.: _Let \(K=2\), \(\theta_{2}(P^{0},P^{1})>0\), and suppose that Assumptions 2.1,2, and 3.5 hold. If \(\mathsf{SubOpt}_{P^{\mathsf{A}}}(\hat{r},\mu_{P^{\mathsf{A}}}^{\mathsf{E}})\leq \hat{\varepsilon}\) for \(k=0,1\), then \(\hat{r}\) is \(\varepsilon\)-transferable to \(\mathcal{P}=\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\) with_

\[\varepsilon=\hat{\varepsilon}/\left[\eta\sigma_{\mathcal{R}}\sin\left(\theta_ {2}(P^{0},P^{1})\left/2\right)^{2}\right].\]

_Sketch of proof._ The main idea of the proof is illustrated in Figure 1(b). First, it follows from Lemma 3.6 that \(\hat{r}\) and \(r^{\mathsf{E}}\) are \(\bar{\varepsilon}=\sqrt{2\hat{\varepsilon}/\sigma_{\mathcal{R}}}\)-close in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{U}_{P^{\mathsf{A}}}\) for \(k=0,1\), respectively. From Figure 1(b) we see - using basic trigonometry - that this implies that \(\hat{r}\) and \(r^{\mathsf{E}}\) are at least \(\Delta=\bar{\varepsilon}/\sin(\theta/2)\)-close in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathbb{1}\). As shown in the full proof in Appendix F, the relevant angle, \(\theta\), is the second principal angle \(\theta_{2}(P^{0},P^{1})\). The result then follows from the upper bound in Lemma 3.6. 

Some observations are in order. First, the above theorem shows that the larger the second principal angle between the two experts' transition laws, the better the recovered reward transfers to a new environment. Second, observe that \(\theta_{2}(P^{0},P^{1})>0\) is equivalent to the rank condition (5), as the second principal angle between two subspaces is non-zero if and only if their intersection is at most one-dimensional. Therefore, for exact transferability, Theorem 3.10 requires the rank condition (5) to be satisfied and \(\hat{\varepsilon}=0\), recovering the results by Cao et al. (2021); Rolland et al. (2022); Schlaginhaufen and Kamgarpour (2023). But in contrast to past results, Theorem 3.10 applies to more realistic scenarios, where \(\hat{\varepsilon}\) is merely small, not zero. Finally, we note that Theorem 3.10 can be trivially generalized to \(K\geq 2\) experts by replacing \(\theta_{2}(P^{0},P^{1})\) with the maximum of \(\theta_{2}(P^{k},P^{l})\) over \(0\leq k\leq l\leq K-1\). However, such bounds may be loose for \(K>2\), potentially leaving considerable room for improvement in this setting.

Local transferabilityWhen learning a reward \(\hat{r}\) from a single expert (\(K=1\)), Schlaginhaufen and Kamgarpour (2023) show that, without reducing the dimension of the reward class, \(\hat{r}\) cannot be exactly transferable to any neighborhood of the expert's transition law \(P_{0}\). However, Theorem 3.11 below shows that by allowing for an \(\varepsilon\) of error, we can guarantee transferability to a neighborhood of \(P_{0}\).

Figure 1: (\(a\)) illustrates the equivalence classes \([\hat{r}]_{\mathcal{U}}\) and \([r^{\mathsf{E}}]_{\mathcal{U}}\), corresponding to the transition laws \(P^{0},P^{1},P\) from Example 3.3, for a small \(\beta\), in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathbb{1}\). The blue lines correspond to \(P^{0}\), the red lines to \(P^{1}\), and the gray lines to \(P\). Furthermore, the shaded areas illustrate the approximation error around \([r^{\mathsf{E}}]_{\mathcal{U}_{P^{\mathsf{A}}}}\), as guaranteed by Lemma 3.6. (\(b\)) illustrates the uncertainty set for the recovered reward when learning from two experts, as discussed in the proof sketch of Theorem 3.10.

**Theorem 3.11**.: _Let \(K=1\), \(D:=\max_{r,r^{\prime}\in\mathcal{R}}\left\lVert r-r^{\prime}\right\rVert_{2}\), and suppose that Assumptions 2.1,2.2, and 3.5 hold. If \(\mathsf{SubOpt}_{P^{0}}(\hat{r},\mu^{\mathsf{E}})\leq\hat{\varepsilon}\), then \(\hat{r}\) is \(\varepsilon_{P}\)-transferable to \(P\in\Delta_{\mathcal{S}}^{S\times A}\) with_

\[\varepsilon_{P}=2\max\left\{2\hat{\varepsilon}/\sigma_{\mathcal{R}},D^{2}\sin \left(\theta_{\max}(P^{0},P)\right)^{2}\right\}/\eta.\]

The above theorem (which is proven in Appendix G) shows that the reward learned from a single expert transfers to transition laws that are sufficiently close to the expert's, where the closeness is measured in terms of the maximal principal angle. In other words, while a large second principal angle between two experts' transition laws, as per Theorem 3.10, ensures that the reward recovered from these two experts is transferable to arbitrary transition laws, a small largest principal angle between two transition laws ensures that a reward recovered in one environment can be successfully transferred to the other environment.

_Remark 3.12_.: As discussed in Appendix H, we can compute the principal angles using a singular value decomposition. Moreover, given estimates \(\hat{P}^{0},\hat{P}^{1}\) of the transition laws \(P^{0},P^{1}\), the error in the estimate of \(\sin\theta_{i}(P^{0},P^{1})\) scales with \(\mathcal{O}(\max\{||P^{0}-\hat{P}^{0}||,||P^{1}-\hat{P}^{1}||\})\).

RegularizersTo provide more insights about Theorems 3.10 and 3.11, we provide explicit values for the primal and dual strong convexity constants, \(\eta\) and \(\sigma_{\mathcal{R}}\), respectively. To this end, we focus on the Shannon entropy regularization \(h(p)=-\tau\mathcal{H}(p)\) and the Tsallis-1/2 entropy regularization \(h(p)=-\tau\mathcal{H}_{1/2}(p)\) as defined in Appendix C. While the Shannon entropy regularization is commonly used in IR (Ziebart, 2010; Ho and Ermon, 2016), the Tsallis-1/2 entropy is more often adopted in the multi-armed bandit literature Zimmert and Seldin (2021). Both regularizations satisfy Assumption 2.1 as well as Assumption 3.5 with the constants detailed in Proposition D.9 in the appendix. In general, the Tsallis entropy leads to a slightly smaller strong convexity constant \(\eta\), but avoids an exponential dependence on the effective horizon \(H_{\gamma}=1/(1-\gamma)\) in \(\sigma_{\mathcal{R}}\). Below, we summarize the implications of Proposition D.9 for \(\varepsilon\)-transferability of a reward \(\hat{r}\) recovered from two experts.

**Corollary 3.13**.: _Suppose the conditions in Theorem 3.10 hold. Furthermore, let \(H_{\gamma}:=1/(1-\gamma)\), \(R:=\max_{r\in\mathcal{R}}\left\lVert r\right\rVert_{\infty}\), \(D=\max_{r,r^{\prime}\in\mathcal{R}}\left\lVert r-r^{\prime}\right\rVert_{2}\), and \(\tau<D\). Then, for the Shannon entropy \(\hat{r}\) is \(\varepsilon\)-transferable to \(\mathcal{P}=\Delta_{\mathcal{S}}^{S\times A}\) with_

\[\varepsilon=\frac{2H_{\gamma}^{2}D|\mathcal{S}||\mathcal{A}|^{2+H_{\gamma}} \exp\left(\frac{2RH_{\gamma}}{\tau}\right)}{\nu_{\text{min}}^{2}\tau\sin \left(\theta_{2}(P^{0},P^{1})/2\right)^{2}}\hat{\varepsilon},\]

_and for the Tsallis entropy with_

\[\varepsilon=\frac{4\sqrt{2}H_{\gamma}^{5}D|\mathcal{S}||\mathcal{A}|^{2}\left( 2R/\tau+3\sqrt{|\mathcal{A}|}\right)^{3}}{\nu_{\text{min}}^{2}\tau\sin\left( \theta_{2}(P^{0},P^{1})/2\right)^{2}}\hat{\varepsilon}.\]

We observe that transferability generally becomes more challenging with decreasing regularization parameter \(\tau\), i.e. if the expert's policy becomes more deterministic. Furthermore, we see that it is easier to recover a transferable reward in a Tsallis entropy-regularized MDP. Corollary 3.13 also shows that the constant between \(\varepsilon\) and \(\hat{\varepsilon}\) tends to be large, meaning that we need to recover a reward for which the experts are \(\hat{\varepsilon}\)-optimal with a very small \(\hat{\varepsilon}\) to guarantee \(\varepsilon\)-transferability for a reasonable \(\varepsilon\). However, it's important to note that our results provide sufficient conditions for the worst case, and it remains for future work to determine under what conditions these constants can be improved.

_Remark 3.14_.: Our results in this section, especially Proposition 3.4 and Lemma 3.6, are critically relying on the steepness of the regularization (Assumption 2.1), which is essential to ensure that the expert's reward can be identified up to the equivalence class of potential shaping transformations. Although we can still upper bound the suboptimality \(\mathsf{SubOpt}(r^{\mathsf{E}},\mathsf{RL}(\hat{r}))\) in terms of the distance between \(\hat{r}\) to \(r^{\mathsf{E}}\) in \(\mathbb{R}^{S\times A}/\mathcal{U}\) without this assumption (see Proposition D.10), we no longer have a lower bound as in Lemma 3.6, which is essential for establishing closeness of \(\hat{r}\) and \(r^{\mathsf{E}}\) in \(\mathbb{R}^{S\times A}/\mathcal{U}\). Hence, we expect it to be difficult to obtain guarantees similar to those in Theorem 3.10 and 3.11 for the unregularized setting, without either reducing the dimension of the reward class (Amin et al., 2017) or making specific assumptions about the feasible reward sets (Metelli et al., 2021, Assumption 4.1).

## 4 Algorithm

To provide end-to-end guarantees for recovering transferable rewards from a finite set of expert demonstrations, we analyze the convergence and sample complexity (in terms of expert demonstrations) of an algorithm for recovering a reward for which all \(K\) experts are approximately optimal. To this end, we focus on the reward class \(\mathcal{R}=\left\{r\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}:\left\|r\right\| _{1}\leq 1\right\}\). Furthermore, we assume oracle access to a \((\varepsilon,\delta)\)-PAC algorithm for the forward problem (O-RL). That is, a polynomial-time algorithm, \(\mathsf{A}^{\varepsilon,\delta}\), that outputs a policy \(\pi=\mathsf{A}^{\varepsilon,\delta}(r)\) such that with probability at least \(1-\delta\) it holds that \(\mathsf{SubOpt}(r,\mu^{\pi})\leq\varepsilon\) (see e.g. (Lan, 2023) for a specific example). The key idea of our meta-algorithm is to learn a reward minimizing the sum of the suboptimalities of the \(K\) experts \(\mu_{P^{0}}^{\mathsf{E}},\ldots,\mu_{P^{K-1}}^{\mathsf{E}}\). This leads us to the following multi-expert IRL problem

\[\min_{r\in\mathcal{R}}\sum_{k=0}^{K-1}\mathsf{SubOpt}_{P^{k}}(r,\hat{\mu}_{ \mathcal{D}^{\mathsf{E}}_{k}}),\] (O-IRL)

where \(\hat{\mu}_{\mathcal{D}^{\mathsf{E}}_{k}}(s,a):=(1-\gamma)/N^{\mathsf{E}}\sum_ {i=0}^{N^{\mathsf{E}}-1}\sum_{t=0}^{H^{\mathsf{E}}-1}\gamma^{t}1\{s_{t}^{k,i}= s,a_{t}^{k,i}=a\}\) is an empirical expert occupancy measure. To solve Problem (O-IRL), we propose the projected gradient descent scheme as detailed in Algorithm 1 below, where \(\mathsf{rollout}_{P^{k}}(\pi,N,H)\) samples \(N\) independent trajectories of length \(H\) from policy \(\pi\). Using a stochastic online gradient descent analysis, Theorem 4.1 shows that any PAC algorithm for the forward problem yields a PAC algorithm for the inverse problem.

``` Input:\(\alpha,T,\{\mathcal{D}^{\mathsf{E}}_{k}\}_{k=0}^{K-1},N,H,\varepsilon_{\text{ opt}},\delta_{\text{opt}}\). Initialize:\(\pi\in\Delta^{\mathcal{S}}_{\mathcal{A}}\) and \(r\in\mathcal{R}\) arbitrarily. for\(i=0,\ldots,T-2\)do for\(k=0,\ldots,K-1\)do \(\pi_{k,t}=\mathsf{A}^{\varepsilon_{\text{opt}},\delta_{\text{opt}}}_{P^{k}}(r_{t})\)// Forward RL. \(\mathcal{D}_{k,t}=\mathsf{rollout}_{P^{k}}(\pi_{k,t},N,H)\)  end for \(g_{t}=\sum_{k=0}^{K-1}\left(\hat{\mu}_{\mathcal{D}_{k,t}}-\hat{\mu}_{\mathcal{ D}^{\mathsf{E}}_{k}}\right)\) \(r_{t+1}=\Pi_{\mathcal{R}}(r_{t}-\alpha g_{t})\)// Reward step.  end for Return:\(\hat{r}:=\frac{1}{T}\sum_{t=0}^{T-1}r_{t}\). ```

**Algorithm 1**Multi-expert IRL

**Theorem 4.1**.: _Suppose that \(N^{\mathsf{E}}=\Omega\big{(}K\log(|\mathcal{S}||\mathcal{A}|/\hat{\delta})/ \hat{\varepsilon}^{2}\big{)}\) and \(H^{\mathsf{E}}=\Omega\big{(}\log(K/\hat{\varepsilon})/\log(1/\gamma)\big{)}\). Running Algorithm 1 for \(T=\Omega\big{(}K^{2}/\hat{\varepsilon}^{2}\big{)}\) iterations with step-size \(\alpha=1/(K\sqrt{T})\), where \(\delta_{\text{opt}}=\mathcal{O}\big{(}\hat{\delta}\hat{\varepsilon}^{2}/K^{3} \big{)}\), \(\varepsilon_{\text{opt}}=\mathcal{O}(\hat{\varepsilon}/K)\), \(N=\Omega\big{(}K\log(K|\mathcal{S}||\mathcal{A}|/(\hat{\delta}\hat{\varepsilon}) )/\hat{\varepsilon}^{2}\big{)}\), and \(H=H^{\mathsf{E}}\), it holds with probability at least \(1-\hat{\delta}\) that \(\mathsf{SubOpt}_{P^{k}}(\hat{r},\mu_{P^{k}}^{\mathsf{E}})\leq\hat{\varepsilon},\;\;\text{for}\;k=0,\ldots,K-1\)._

The above result generalizes (Syed and Schapire, 2007, Theorem 2) by considering multiple experts and by proving convergence in terms of the expert suboptimality. We refer to Appendix I for the proof and the precise constants. Theorem 4.1 shows that with \(\Omega(K/\hat{\varepsilon}^{2})\) demonstrations of each expert, we recover in \(\Omega(K^{2}/\hat{\varepsilon}^{2})\) steps of Algorithm 1 a reward \(\hat{r}\) for which all experts are \(\hat{\varepsilon}\)-optimal. Together with Theorem 3.10 and 3.11, this provides a bound on the sample and time complexity of recovering in \(\varepsilon\)-transferable rewards in regularized IRL.

## 5 Experiments

To validate our results experimentally, we adopt a stochastic variant of the WindyGridworld environment (Sutton and Barto, 2018). In this environment, the agent moves to the intended grid cell with a probability of \((1-\beta)\) and is pushed one step further in the direction of the wind with a probability of \(\beta\). Using Algorithm 1, we recover a reward \(\hat{r}\) from demonstrations of two experts, both exposed to the same wind strength \(\beta\) but different wind directions - North and East. The experiments are repeated for a varying number of expert demonstrations \(N^{\mathsf{E}}\in\{10^{3},10^{4},10^{5},10^{6}\}\) and wind strengths \(\beta\in\{0.01,0.1,0.5,1.0\}\). We then test the transferability to two different environments: one with South wind, \(P^{\mathsf{South}}\), and a zero-wind environment with cyclically shifted actions, \(P^{\mathsf{Shifted}}\). Figure 2(a) shows that the second principal angle between the two experts' transition laws \(P_{0}\) and \(P_{1}\) increases with increasing wind strength. Moreover, Figure 2(b)-(d) show that both the closeness between \(\hat{r}\) and \(r^{\mathsf{E}}\) in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\) and the transferability to \(P^{\text{South}}\) and \(P^{\text{Shifted}}\) improve with a larger second principal angle, as expected from Theorem 3.10. For a more detailed discussion of the experiments we refer to Appendix K.

## 6 Conclusion

SummaryIn this paper, we investigated the transferability of rewards in regularized IRL. We showed that the conditions established under full access to the experts' policies do not guarantee transferability when learning a reward from a finite set of expert demonstrations. To address this issue, we proposed using principal angles as a more refined measure of the similarity and dissimilarity of transition laws. Assuming a strongly convex and locally smooth regularization, we then showed that if we recover a reward for which at least two experts are nearly optimal, and their environments are sufficiently different in terms of the second principal angle between their transition laws, then the recovered reward is universally transferable. Furthermore, we showed that if two environments are sufficiently similar in terms of the maximal principal angle between their transition laws, rewards learned in one environment can be effectively transferred to the other environment. Additionally, we provided explicit constants for the Shannon and Tsallis entropy, as well as a PAC algorithm for recovering a reward for which all experts are approximately optimal. As a result, we established end-to-end guarantees for learning transferable rewards in regularized IRL. Additionally, we experimentally validated our results through gridworld experiments.

Limitations and future workOur results provide only sufficient conditions for transferability. It would be valuable to investigate necessary conditions to check whether our bounds are tight. Furthermore, extending our analysis to lower-dimensional reward classes could reduce the complexity of learning transferable rewards. Although our paper focuses on discrete state and action spaces, an exciting avenue for future research would be to extend our results to continuous state and action spaces, which are more commonly encountered in practice. We expect that our proof methods can be generalized to this setting, but the analysis will be more intricate due to the infinite-dimensional reward and occupancy measure spaces. Finally, as our work is mainly theoretical, experimental validation on real-world applications could provide valuable insight into the practical aspects and challenges of transferability.

AcknowledgmentsAndreas Schlaginhaufen is funded by a PhD fellowship from the Swiss Data Science Center.

Figure 2: (_a_) shows the second principal angle between the experts, for varying wind strength \(\beta\). (_b_) shows the distance between \(\hat{r}\) and \(r^{\mathsf{E}}\) in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\) for a varying number of expert demonstrations \(N^{\mathsf{E}}\) and wind strength \(\beta\). (_c_) and (_d_) show the transferability to \(P^{\text{South}}\) and \(P^{\text{Shifted}}\) in terms of \(\mathsf{SubOpt}_{P^{\text{South}}}(r^{\mathsf{E}},\mathsf{RL}_{P^{\text{South} }}(\hat{r}))\) and \(\mathsf{SubOpt}_{P^{\text{Shifted}}}(r^{\mathsf{E}},\mathsf{RL}_{P^{\text{ Shifted}}}(\hat{r}))\), respectively. The circles indicate the median and the shaded areas the 0.2 and 0.8 quantiles over 10 independent realizations of the expert data.

## References

* Abbeel and Ng (2004) P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In _Proceedings of the twenty-first international conference on Machine learning_, 2004.
* Amin et al. (2017) K. Amin, N. Jiang, and S. Singh. Repeated inverse reinforcement learning. _Advances in neural information processing systems_, 2017.
* Beck (2017) A. Beck. _First-order methods in optimization_. SIAM, 2017.
* Bourbaki (1966) N. Bourbaki. _Elements of mathematics: General topology_. Hermann, 1966.
* Boyd and Vandenberghe (2004) S. P. Boyd and L. Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Cao et al. (2021) H. Cao, S. Cohen, and L. Szpruch. Identifiability in inverse reinforcement learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* Cover (1999) T. M. Cover. _Elements of information theory_. John Wiley & Sons, 1999.
* Dann et al. (2023) C. Dann, C.-Y. Wei, and J. Zimmert. Best of both worlds policy optimization. _arXiv preprint arXiv:2302.09408_, 2023.
* Drmac (2000) Z. Drmac. On principal angles between subspaces of euclidean space. _SIAM Journal on Matrix Analysis and Applications_, 2000.
* Fu et al. (2017) J. Fu, K. Luo, and S. Levine. Learning robust rewards with adversarial inverse reinforcement learning. _arXiv preprint arXiv:1710.11248_, 2017.
* Galantai (2013) A. Galantai. _Projectors and projection methods_. Springer Science & Business Media, 2013.
* Garg et al. (2021) D. Garg, S. Chakraborty, C. Cundy, J. Song, and S. Ermon. Iq-learn: Inverse soft-q learning for imitation. _Advances in Neural Information Processing Systems_, 2021.
* Geist et al. (2019) M. Geist, B. Scherrer, and O. Pietquin. A theory of regularized markov decision processes. In _International Conference on Machine Learning_. PMLR, 2019.
* Goebel and Rockafellar (2008) R. Goebel and R. T. Rockafellar. Local strong convexity and local lipschitz continuity of the gradient of convex functions. _Journal of Convex Analysis_, 2008.
* Ho and Ermon (2016) J. Ho and S. Ermon. Generative adversarial imitation learning. _Advances in neural information processing systems_, 2016.
* Hoeffding (1963) W. Hoeffding. Probability inequalities for sums of bounded random variables. _Journal of the American Statistical Association_, 1963.
* Hwangbo et al. (2019) J. Hwangbo, J. Lee, A. Dosovitskiy, D. Bellicoso, V. Tsounis, V. Koltun, and M. Hutter. Learning agile and dynamic motor skills for legged robots. _Science Robotics_, 2019.
* Jeon et al. (2021) W. Jeon, C.-Y. Su, P. Barde, T. Doan, D. Nowrouzezahrai, and J. Pineau. Regularized inverse reinforcement learning. In _International Conference on Learning Representations_, 2021.
* Ji-Guang (1987) S. Ji-Guang. Perturbation of angles between linear subspaces. _Journal of Computational Mathematics_, 1987.
* Kalman (1964) R. E. Kalman. When Is a Linear Control System Optimal? _Journal of Basic Engineering_, 1964.
* Kim et al. (2021) K. Kim, S. Garg, K. Shiragur, and S. Ermon. Reward identification in inverse reinforcement learning. In _Proceedings of the 38th International Conference on Machine Learning_, pages 5496-5505, 2021.
* Knyazev and Argentati (2002) A. V. Knyazev and M. E. Argentati. Principal angles between subspaces in an a-based scalar product: algorithms and perturbation estimates. _SIAM Journal on Scientific Computing_, 2002.
* Lan (2023) G. Lan. Policy mirror descent for reinforcement learning: Linear convergence, new sampling complexity, and generalized problem classes. _Mathematical programming_, 2023.
* Li et al. (2019)Y. Lu, J. Fu, G. Tucker, X. Pan, E. Bronstein, R. Roelofs, B. Sapp, B. White, A. Faust, S. Whiteson, et al. Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios. In _2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)_, 2023.
* Mei et al. (2020) J. Mei, C. Xiao, C. Szepesvari, and D. Schuurmans. On the global convergence rates of softmax policy gradient methods. In _International conference on machine learning_. PMLR, 2020.
* Metelli et al. (2021) A. M. Metelli, G. Ramponi, A. Concetti, and M. Restelli. Provably efficient learning of transferable rewards. In _International Conference on Machine Learning_. PMLR, 2021.
* Ng et al. (1999) A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In _Icml_, 1999.
* Ng et al. (2000) A. Y. Ng, S. Russell, et al. Algorithms for inverse reinforcement learning. In _Icml_, 2000.
* Ngo et al. (2022) R. Ngo, L. Chan, and S. Mindermann. The alignment problem from a deep learning perspective. _arXiv preprint arXiv:2209.00626_, 2022.
* Ortega et al. (2015) P. A. Ortega, D. A. Braun, J. Dyer, K.-E. Kim, and N. Tishby. Information-theoretic bounded rationality. _arXiv preprint arXiv:1512.06789_, 2015.
* Ouhamma and Kamgarpour (2023) R. Ouhamma and M. Kamgarpour. Learning nash equilibria in zero-sum markov games: A single time-scale algorithm under weak reachability. _arXiv preprint arXiv:2312.08008_, 2023.
* Penrose (1955) R. Penrose. A generalized inverse for matrices. In _Mathematical proceedings of the Cambridge philosophical society_. Cambridge University Press, 1955.
* Pomerleau (1988) D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. _Advances in neural information processing systems_, 1988.
* Puterman (2014) M. L. Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Rockafellar (1970) R. T. Rockafellar. _Convex Analysis_. Princeton University Press, 1970.
* Rockafellar and Wets (2009) R. T. Rockafellar and R. J.-B. Wets. _Variational analysis_. Springer Science & Business Media, 2009.
* Rolland et al. (2022) P. Rolland, L. Viano, N. Schurhoff, B. Nikolov, and V. Cevher. Identifiability and generalizability from multiple experts in inverse reinforcement learning. _Advances in Neural Information Processing Systems_, 2022.
* Rust (1994) J. Rust. Structural estimation of markov decision processes. _Handbook of econometrics_, pages 3081-3143, 1994.
* Schlaginhaufen and Kamgarpour (2023) A. Schlaginhaufen and M. Kamgarpour. Identifiability and generalizability in constrained inverse reinforcement learning. In _Proceedings of the 40th International Conference on Machine Learning_. PMLR, 2023.
* Skalse et al. (2023) J. M. V. Skalse, M. Farrugia-Roberts, S. Russell, A. Abate, and A. Gleave. Invariance in policy optimisation and partial identifiability in reward learning. In _International Conference on Machine Learning_. PMLR, 2023.
* Stiennon et al. (2020) N. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D. Amodei, and P. F. Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 2020.
* Sutton and Barto (2018) R. S. Sutton and A. G. Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Syed and Schapire (2007) U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. _Advances in neural information processing systems_, 2007.
* Teboulle (1992) M. Teboulle. Entropic proximal mappings with applications to nonlinear programming. _Mathematics of Operations Research_, 1992.
* Toshkov (1998)B. D. Ziebart. _Modeling purposeful adaptive behavior with the principle of maximum causal entropy_. Carnegie Mellon University, 2010.
* Zimmert and Seldin [2021] J. Zimmert and Y. Seldin. Tsallis-inf: An optimal algorithm for stochastic and adversarial bandits. _The Journal of Machine Learning Research_, 2021.
* Zinkevich [2003] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th international conference on machine learning (icml-03)_, 2003.

## Appendix

### Table of Contents

* A Notations
* B Conjugate duality in regularized IRL
* C Regularizers
* D Technical Lemmas
* D.1 Lipschitz continuity from policies to occupancy measures
* D.2 Strong convexity
* D.3 Lipschitz gradients
* D.4 Dual smoothness and strong convexity
* D.5 Regularity constants
* D.6 Suboptimality bounds
* D.7 Perturbation bounds
* E Proof of claim in Example 3.3
* F Proof of Theorem 3.10
* G Proof of Theorem 3.11
* H Estimating principal angles
* I Proof of Theorem 4.1
* J Suboptimal experts
* K Experimental details
Notations

OverviewHere, we provide an overview of some of the most important notations. However, every notation is defined when it is introduced as well.

Additional definitionsIn the following, we briefly recall some additional definitions. To this end, we denote \(\mathcal{B}(x,r):=\left\{x\in\mathbb{R}^{n}:\left\|x\right\|_{2}<r\right\}\) for an open ball of radius \(r\) with center \(x\).

**Definition A.1** (Interior).: The interior of a set \(\mathcal{X}\subseteq\mathbb{R}^{n}\) is defined as

\[\operatorname{int}\mathcal{X}:=\left\{x\in\mathcal{X}:\mathcal{B}(x,r) \subseteq\mathcal{X}\text{ for some }r>0\right\}.\]

**Definition A.2** (Affine hull).: The affine hull of a set \(\mathcal{X}\subseteq\mathbb{R}^{n}\) is defined as

\[\operatorname{aff}\mathcal{X}:=\left\{\theta_{1}x_{1}+\ldots+\theta_{k}x_{k}: x_{1},\ldots,x_{k}\in\mathcal{X},\theta_{1}+\ldots+\theta_{k}=1\right\}.\]

**Definition A.3** (Relative interior).: The relative interior of a set \(\mathcal{X}\subseteq\mathbb{R}^{n}\) is defined as

\[\operatorname{relint}\mathcal{X}:=\left\{x\in\mathcal{X}:\mathcal{B}(x,r) \cap\operatorname{aff}\mathcal{X}\subseteq\mathcal{X}\text{ for some }r>0\right\}.\]

**Definition A.4** (Relative boundary).: The relative boundary of a closed set \(\mathcal{X}\subseteq\mathbb{R}^{n}\) is defined as

\[\operatorname{relbd}\mathcal{X}:=\mathcal{X}\setminus\operatorname{relint} \mathcal{X}.\]

**Definition A.5** (Convex hull).: The convex hull of a set \(\mathcal{X}\subseteq\mathbb{R}^{n}\) is defined as

\[\operatorname{conv}\mathcal{X}:=\left\{\theta_{1}x_{1}+\ldots+\theta_{k}x_{k}: x_{1},\ldots,x_{k}\in\mathcal{X},\theta_{1}+\ldots+\theta_{k}=1,\theta_{i}\geq 0,i=1,\ldots,k\right\}.\]

\begin{table}
\begin{tabular}{l l l} \hline \hline \(\mathcal{Y}^{\mathcal{X}}\) & \(:=\) & set of functions \(f:\mathcal{X}\rightarrow\mathcal{Y}\) \\ \hline \(\Delta_{\mathcal{X}}\) & \(:=\) & probability simplex over some discrete set \(\mathcal{X}\) \\ \hline \(\mathcal{M}\) & \(:=\) & set of feasible occupancy measures \\ \hline \(\mathcal{R}\) & \(:=\) & \(\left\{r\in\mathbb{R}^{\mathcal{S}\times\mathcal{A}}:\left\|r\right\|_{1}\leq 1\right\}\), reward class \\ \hline \(R\) & \(:=\) & \(\max_{r\in\mathcal{R}}\left\|r\right\|_{\infty}\), reward bound \\ \hline \(D\) & \(:=\) & \(\max_{r,r\in\mathcal{R}}\left\|r-r^{\prime}\right\|_{2}\), diameter of the reward class \\ \hline \(H_{\gamma}\) & \(:=\) & \(1/(1-\gamma)\), effective horizon \\ \hline \(\nu(s)\) & \(:=\) & \(\sum_{a}\mu(s,a)\) \\ \hline \(\pi^{\mu}(a|s)\) & \(:=\) & \(\begin{cases}\mu(s,a)/\sum_{a^{\prime}}\mu(s,a^{\prime})=\mu(s,a)/\nu(s)&, \nu(s)>0\\ 1/|\mathcal{A}|\) (arbitrary) & \(,\)otherwise \\ \hline \(\pi_{s}\) & \(:=\) & \(\pi(\cdot|s)\) \\ \hline \(\bar{h}(\mu)\) & \(:=\) & \(\mathbb{E}_{(s,a)\sim\mu}\left[h(\pi_{s}^{\mu})\right]\) \\ \hline \(J(r,\mu)\) & \(:=\) & \(\langle r,\mu\rangle-\bar{h}(\mu)\) \\ \hline \(\mathsf{SubOpt}(r,\mu^{\prime})\) & \(:=\) & \(\max_{\mu\in\mathcal{M}}J(r,\mu)-J(r,\mu^{\prime})\) \\ \hline \(\mathsf{RL}(r)\) & \(:=\) & \(\operatorname{argmax}_{\mu\in\mathcal{M}}J(r,\mu)\), optimal occupancy measure for \(r\) \\ \hline \(\mathsf{IRL}(\mu^{\mathsf{E}})\) & \(:=\) & \(\left\{r\in\mathcal{R}:\mu^{\mathsf{E}}=\mathsf{RL}(r)\right\}\), feasible reward set for \(\mu^{\mathsf{E}}\) \\ \hline \(\mathsf{A}^{\varepsilon,\delta}(r)\) & \(:=\) & PAC RL algorithm outputting some \(\varepsilon\)-optimal policy with probability at least \(1-\delta\) \\ \hline \(\hat{\mu}_{\mathcal{D}}\) & \(:=\) & \(\frac{1-\gamma}{N}\sum_{i=0}^{N-1}\sum_{t=0}^{T-1}\gamma^{t}1\left\{s_{t}^{i}=s,a_{t}^{i}=a\right\}\), where \(\mathcal{D}=\left\{\left(s_{0}^{i},a_{0}^{i},\ldots,s_{H-1}^{i},a_{H-1}^{i} \right)\right\}_{i=0}^{N-1}\) \\ \hline \(E\) & \(:=\) & linear operator \(\mathbb{R}^{\mathcal{S}}\rightarrow\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) defined by \((Ef)(s,a)=f(s)\) for \(f\in\mathbb{R}^{\mathcal{S}}\) \\ \hline \(P\) & \(:=\) & linear operator \(\mathbb{R}^{\mathcal{S}}\rightarrow\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) defined by \((Pf)(s,a)=\sum_{s^{\prime}}P(s^{\prime}|s,a)f(s^{\prime})\) for \(f\in\mathbb{R}^{\mathcal{S}}\) \\ \hline \(\operatorname{im}\left(A\right)\) & \(:=\) & image of a linear operator \(A\) \\ \hline \(\mathcal{U}\) & \(:=\) & \(\operatorname{im}(E-\gamma P)\subset\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\), potential shaping subspace \\ \hline
**1** & \(:=\) & \(\left\{f=\text{constant}\right\}\subset\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\), constant subspace \\ \hline \(\mathcal{V}^{\perp}\) & \(:=\) & orthogonal complement of a linear subspace \(\mathcal{V}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notations.

Conjugate duality in regularized IRL

In this section, we first recall some background from convex analysis and then briefly discuss the duality between reward equivalence classes and optimal occupancy measures.

DefinitionsWe recall a few definitions related to convex functions. In convex analysis it is standard to consider extended real value functions \(f:\mathbb{R}^{n}\to\overline{\mathbb{R}}:=[-\infty,\infty]\), where convex functions defined on some subset \(\mathcal{X}\subset\mathbb{R}^{n}\) are extended over the entire space by setting their value to \(+\infty\) outside of their domain. The effective domain is defined as \(\operatorname{dom}f:=\{x:f(x)<\infty\}\), and a convex function \(f\) is said to be proper if \(f>-\infty\) and \(\operatorname{dom}f\neq\emptyset\). Furthermore, \(f\) is referred to as closed if its epigraph \(\{(x,y):x\in\operatorname{dom}f,y\geq f(x)\}\) is a closed set.2 In particular, \(f\) is closed if it is continuous on \(\operatorname{dom}f\) and \(\operatorname{dom}f\) is a closed set [Boyd and Vandenberghe, 2004]. Lastly, we recall two key concepts in convex analysis - the subdifferential and the convex conjugate of some convex function.

Footnote 2: A proper convex function is closed if and only if it is lower semi-continuous [Rockafellar, 1970].

Definition B.1 (Subdifferential).: A subgradient of \(f:\mathbb{R}^{n}\to\overline{\mathbb{R}}\) at some point \(x\in\mathbb{R}^{n}\) is a vector \(g\in\mathbb{R}^{n}\) such that \(f(x^{\prime})\geq f(x)+g^{\top}(x-x)\) for all \(x^{\prime}\in\mathcal{X}\). The subdifferential \(\partial f(x)\) at \(x\in\mathcal{X}\) is the set of all subgradients at \(x\), where \(\partial f(x)\) is defined to be empty if \(x\notin\operatorname{dom}f\).

Definition B.2 (Convex Conjugate).: The convex conjugate of \(f:\mathbb{R}^{n}\to\overline{\mathbb{R}}\) is the function \(f^{*}:\mathbb{R}^{n}\to\overline{\mathbb{R}}\) defined as

\[f^{*}(y)=\sup_{x}\langle y,x\rangle-f(x).\]

Key resultsNext, we list two key results from convex analysis.

Theorem B.3 ([Rockafellar, 1970]).: _A function \(f:\mathbb{R}^{n}\to\overline{\mathbb{R}}\) is differentiable at some point \(x\in\operatorname{dom}f\) if and only if \(\partial f(x)\) is singleton. In this case we have \(\partial f(x)=\{\nabla f(x)\}\)._

Theorem B.4 ([Rockafellar, 1970]).: _For any proper convex function \(f:\mathbb{R}^{n}\to\overline{\mathbb{R}}\) it holds_

\[f^{*}(y)=\langle y,x\rangle-f(x)\quad\iff\quad y\in\partial f(x).\]

_If additionally \(f\) is closed, then_

\[f^{*}(y)=\langle y,x\rangle-f(x)\quad\iff\quad y\in\partial f(x)\quad\iff \quad x\in\partial f^{*}(y).\]

Duality in IRLLet \(f:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\to\overline{\mathbb{R}}\) be given by \(f:=\bar{h}+\delta_{\mathcal{M}}\), where \(\delta_{\mathcal{M}}\) is a characteristic function defined as \(\delta_{\mathcal{M}}(\mu)=0\) if \(\mu\in\mathcal{M}\) and \(\delta_{\mathcal{M}}(\mu)=\infty\), otherwise. Since \(f\) is closed proper convex, Theorem B.3 and B.4 imply that for a strictly convex \(\bar{h}\) we have

\[\mathsf{RL}(r)=\nabla f^{*}(r)\quad\text{ and }\quad\mathsf{IRL}(\mu)=\partial f (\mu)\cap\mathcal{R}.\]

Additionally, under Assumption 2.1 and Slater's condition, which is ensured by Assumption 2.2, we have \(\partial f(\mu)=\nabla\bar{h}(\mu)+\mathcal{U}\)[Schlaginhaufen and Kamgarpour, 2023]. Hence, \(\mu\) is optimal for \(r\) if and only if \(r\in\left[\nabla\bar{h}(\mu)\right]_{\mathcal{U}}\). Therefore, there is a one-to-one mapping between elements of the quotient space \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{U}\), i.e. reward equivalence classes, and corresponding optimal occupancy measures in \(\mathcal{M}\). This mapping is given by

\[\nabla f^{*}:\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{U}\to\mathcal{ M},[r]_{\mathcal{U}}\mapsto\nabla f^{*}(r)=\mathsf{RL}(r),\]

and its inverse by

\[\partial f:\mathcal{M}\to\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{ U},\mu\mapsto\partial f(\mu)=\nabla\bar{h}(\mu)+\mathcal{U}.\]

## Appendix C Regularizers

We dedicate this section to discuss optimal policies in regularized MDPs and to recall their explicit form for Shannon and Tsallis entropy regularization.

Optimal policiesThroughout the appendix, it will convenient to use the notation \(\pi_{s}:=\pi(\cdot|s)\). Given some proper closed strongly convex policy regularizer \(h\), it can be shown [Geist et al., 2019] that the optimal policy, \(\pi^{*}\), satisfies

\[\pi_{s}^{*}=\nabla h^{*}(q_{s}^{*})=\operatorname*{argmax}_{\pi_{s}\in\Delta_{ \mathcal{A}}}\langle\pi_{s},q_{s}^{*}\rangle-h(\pi_{s}),\ \forall s\in\mathcal{S},\]

where \(q_{s}^{*}\in\mathbb{R}^{\mathcal{A}}\) is the optimal \(q\)-function defined via

\[q_{s}^{*}(a):=q^{*}(s,a):=\max_{\pi\in\Delta_{\mathcal{A}}^{S}}\mathbb{E}_{ \pi}\left[r(s_{t},a_{t})+\sum_{t\geq 1}\gamma^{t}\left[r(s_{t},a_{t})-h(\pi_{s}) \right]\middle|s_{0}=s,a_{0}=a\right].\]

Next, we discuss the explicit form of \(\pi_{s}^{*}=\nabla h^{*}(q_{s}^{*})\) for the specific cases of Shannon and Tsallis-\(1/2\) entropy regularization.

Shannon entropyFor some \(\tau>0\), we define the Shannon entropy regularizer as \(h:=-\tau\mathcal{H}\), where

\[\mathcal{H}(\pi_{s}):=-\sum_{a}\pi_{s}(a)\log\pi_{s}(a),\]

is the Shannon entropy satisfying \(0\leq\mathcal{H}\leq\log|\mathcal{A}|\). It can be shown that \(h\) is \(\tau\)-strongly convex with respect to \(\left\lVert\cdot\right\rVert_{1}\)[Cover, 1999], and the optimal policy satisfies [Geist et al., 2019]

\[\pi^{*}(a|s)=\frac{\exp\left(q^{*}(s,a)/\tau\right)}{\sum_{a^{\prime}}\exp \left(q^{*}(s,a^{\prime})/\tau\right)}.\]

Tsallis entropyFor some parameter \(\alpha\in\mathbb{R}\), the Tsallis entropy, \(\mathcal{H}_{\alpha}\), is defined as

\[\mathcal{H}_{\alpha}(\pi_{s}):=\frac{1}{\alpha-1}\left(1-\sum_{a}\pi_{s}(a)^{ \alpha}\right).\]

In the limit \(\alpha\to 1\), the Tsallis entropy equals the Shannon entropy defined above. However, in this paper, we use _Tsallis entropy_ to refer to the choice \(\alpha=1/2\), which is often adopted as regularization in multi-armed bandit and, more recently, policy optimization algorithms [Zimmert and Seldin, 2021, Dann et al., 2023]. That is, we consider \(h(\pi_{s})=-\tau\mathcal{H}_{1/2}(\pi_{s})=-2\tau\left(\sum_{a}\sqrt{\pi_{s}( a)}-1\right)\) for some \(\tau>0\). We have \(0\leq-h\leq 2\tau\left(\sqrt{|\mathcal{A}|}-1\right)\) and it can be shown that the optimal policy satisfies

\[\pi^{*}(a|s)=\left(\frac{\tau}{x_{s}-q^{*}(s,a)}\right)^{2},\]

where \(x_{s}\) is a normalization parameter such that \(\sum_{a}\pi^{*}(a|s)=1\)[Zimmert and Seldin, 2021]. Furthermore, since \(h\) has diagonal Hessian \(\nabla^{2}h(\pi_{s})_{a,a}=\tau/(2\pi_{s}(a)^{3/2})\), it is \(\tau/2\)-strongly convex with respect to \(\left\lVert\cdot\right\rVert_{2}\) and hence also \(\tau/(2|\mathcal{A}|)\)-strongly convex with respect to \(\left\lVert\cdot\right\rVert_{1}\).

## Appendix D Technical Lemmas

In this section, we show several new technical results that are required for the proofs of our main theorems.

### Lipschitz continuity from policies to occupancy measures

**Proposition D.1**.: _Let Assumption 2.2 hold. For any \(\mu_{1},\mu_{2}\in\mathcal{M}\), we have_

\[(1-\gamma)\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1}\leq\max_{s}\left\lVert \pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}}\right\rVert_{1}\leq\frac{2}{\nu_{\text{ min}}}\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1}.\]

Proof.: To show the first inequality, we decompose

\[\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1} \leq\sum_{s,a}|\nu_{1}(s)(\pi^{\mu_{1}}(a|s)-\pi^{\mu_{2}}(a|s))| +\sum_{s,a}|(\nu_{1}(s)-\nu_{2}(s))\pi^{\mu_{2}}(a|s)|\] \[\leq\max_{s}\left\lVert\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}} \right\rVert_{1}+\left\lVert\nu_{1}-\nu_{2}\right\rVert_{1},\]where we used the triangle and Holder's inequality. From the Bellman flow constraints,

\[\nu(s)=\gamma\sum_{s^{\prime},a^{\prime}}P(s|s^{\prime},a^{\prime})\mu(s^{\prime},a^{\prime})+(1-\gamma)\nu_{0}(s),\]

it follows that

\[\left\lVert\nu_{1}-\nu_{2}\right\rVert_{1} =\gamma\sum_{s}\left\lvert\sum_{s^{\prime},a^{\prime}}P(s|s^{ \prime},a^{\prime})(\mu_{1}(s^{\prime},a^{\prime})-\mu_{2}(s^{\prime},a^{ \prime}))\right\rvert\] \[\leq\gamma\sum_{s^{\prime},a^{\prime}}\underbrace{\sum_{s}P(s|s^{ \prime},a^{\prime})}_{=1}\left\lvert\mu_{1}(s^{\prime},a^{\prime})-\mu_{2}(s^{ \prime},a^{\prime})\right\rvert\] \[=\gamma\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1},\]

where we again used the triangle inequality. Hence, we have

\[\max_{s}\left\lVert\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}}\right\rVert_{1}\geq \left\lVert\mu_{1}-\mu_{2}\right\rVert_{1}-\left\lVert\nu_{1}-\nu_{2}\right \rVert_{1}\geq(1-\gamma)\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1}.\]

To show the second inequality, we use the reverse triangle inequality

\[\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1} \geq\sum_{s,a}\left\lvert\nu_{1}(s)(\pi^{\mu_{1}}(a|s)-\pi^{\mu_{2 }}(a|s))\right\rvert-\sum_{s,a}\left\lvert(\nu_{1}(s)-\nu_{2}(s))\pi^{\mu_{2}} (a|s)\right\rvert\] \[=\sum_{s}\nu_{1}(s)\left\lVert\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}} \right\rVert_{1}-\left\lVert\nu_{1}-\nu_{2}\right\rVert_{1},\] \[\geq\nu_{\text{min}}\max_{s}\left\lVert\pi_{s}^{\mu_{1}}-\pi_{s}^ {\mu_{2}}\right\rVert_{1}-\gamma\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1},\]

where in the last step we used again that \(\left\lVert\nu_{1}-\nu_{2}\right\rVert_{1}\leq\gamma\left\lVert\mu_{1}-\mu_{ 2}\right\rVert_{1}\). By rearranging terms we arrive at the desired inequality

\[\max_{s}\left\lVert\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}}\right\rVert_{1}\leq \frac{1+\gamma}{\nu_{\text{min}}}\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1} \leq\frac{2}{\nu_{\text{min}}}\left\lVert\mu_{1}-\mu_{2}\right\rVert_{1}.\]

### Strong convexity

Next, we show that strong convexity of the policy regularizer translates into strong convexity in the occupancy measure.

**Proposition D.2** (Strong convexity).: _Let Assumption 2.2 hold and suppose that \(h\) is \(\eta_{h}\)-strongly convex with respect to the \(\left\lVert\cdot\right\rVert_{1}\) norm. Then, \(\bar{h}\) is \(\eta_{h}\nu_{\text{min}}/H_{\gamma}^{2}\)-strongly convex with respect to \(\left\lVert\cdot\right\rVert_{1}\)._

Proof.: We need to show that for \(\alpha\in(0,1),\bar{\alpha}=1-\alpha\) and any two \(\mu_{1},\mu_{2}\in\mathcal{M}\) it holds that

\[\bar{h}(\alpha\mu_{1}+\bar{\alpha}\mu_{2})\leq\alpha\bar{h}(\mu_{1})+\bar{ \alpha}\bar{h}(\mu_{2})-\frac{\alpha\bar{\alpha}\eta}{2}\left\lVert\mu_{1}-\mu _{2}\right\rVert_{1}^{2},\]for \(\eta=\eta_{h}\nu_{\min}/H_{\gamma}^{2}\). To this end, we start similarly as in the proof of strict convexity by Schlaginhaufen and Kamgarpour (2023), but use \(\nu(s)\geq\nu_{\text{min}}\) and strong convexity of \(h\).

\[\bar{h}(\alpha\mu_{1}+\bar{\alpha}\mu_{2})\] (7) \[= \sum_{s}\left(\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)\right)h \left(\frac{\alpha\mu_{1}(s,\cdot)+\bar{\alpha}\mu_{2}(s,\cdot)}{\alpha\nu_{1} (s)+\bar{\alpha}\nu_{2}(s)}\right)\] \[= \sum_{s}\left(\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)\right)h \left(\frac{\alpha\mu_{1}(s,\cdot)}{\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)} \frac{\nu_{1}(s)}{\nu_{1}(s)}+\frac{\bar{\alpha}\mu_{2}(s,\cdot)}{\alpha\nu_{ 1}(s)+\bar{\alpha}\nu_{2}(s)}\frac{\nu_{2}(s)}{\nu_{2}(s)}\right)\] \[= \sum_{s}\left(\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)\right)h \left(\underbrace{\frac{\alpha\nu_{1}(s)}{\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2 }(s)}}_{\beta_{s}}\pi_{s}^{\mu_{1}}+\underbrace{\frac{\bar{\alpha}\nu_{2}(s)} {\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)}}_{1-\beta_{s}}\pi_{s}^{\mu_{2}}\right)\] \[\leq \sum_{s}\left(\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)\right) \left(\beta_{s}h\left(\pi_{s}^{\mu_{1}}\right)+(1-\beta_{s})h\left(\pi_{s}^{ \mu_{2}}\right)-\frac{\beta_{s}(1-\beta_{s})\eta_{h}}{2}\left\|\pi_{s}^{\mu_{ 1}}-\pi_{s}^{\mu_{2}}\right\|_{1}^{2}\right)\] \[= \sum_{s}\left(\alpha\nu_{1}(s)h\left(\pi_{s}^{\mu_{1}}\right)+ \bar{\alpha}\nu_{2}(s)h\left(\pi_{s}^{\mu_{2}}\right)-\frac{\alpha\bar{\alpha }\eta_{h}}{2}\frac{\nu_{1}(s)\nu_{2}(s)}{\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}( s)}\left\|\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}}\right\|_{1}^{2}\right).\]

From here on, we use that

\[\sum_{s}\frac{\nu_{1}(s)\nu_{2}(s)}{\alpha\nu_{1}(s)+\bar{\alpha} \nu_{2}(s)}\left\|\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}}\right\|_{1}^{2}\] \[=\sum_{s}\underbrace{\frac{\max\left\{\nu_{1}(s),\nu_{2}(s)\right\} }{\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)}}_{\geq 1}\underbrace{\min\left\{\nu_{1}(s),\nu_{2}(s) \right\}\left\|\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu_{2}}\right\|_{1}^{2}}_{\geq\nu_{ \text{min}}}\] \[\geq\sum_{s}\nu_{\text{min}}\left\|\pi_{s}^{\mu_{1}}-\pi_{s}^{\mu _{2}}\right\|_{1}^{2}\] \[\geq\nu_{\text{min}}/H_{\gamma}^{2}\left\|\mu_{1}-\mu_{2}\right\|_ {1}^{2}.\] (8)

where we used Proposition D.1 in the last step. Plugging the inequality (8) back into (7) concludes the proof. 

### Lipschitz gradients

In this section, we show how we can get bounds on the Lipschitz constant \(L_{\mathcal{K}}\). To this end, we first need to lower bound the optimal policies.

Policy lower boundsThe following proposition establishes a lower bound for optimal policies with Shannon and Tsallis entropy regularization.

**Proposition D.3**.: _Let \(H_{\gamma}=1/(1-\gamma)\) and \(r_{\max}:=\left\|r\right\|_{\infty}\). Then, we have the following lower bounds:_

1. _[label=)]_
2. _If_ \(h=-\tau\mathcal{H}\)_, then_ \(\pi^{*}(a|s)\geq\exp\left(-2r_{\max}H_{\gamma}/\tau\right)/|\mathcal{A}|^{1+H_ {\gamma}}\)_._
3. _If_ \(h=-\tau\mathcal{H}_{1/2}\)_, then_ \(\pi^{*}(a|s)\geq\left(\left(2r_{\max}/\tau+3\sqrt{|\mathcal{A}|}\right)H_{ \gamma}\right)^{-2}\)_._

Proof.: Recall the formula for the optimal policies in Appendix C.

_Part a):_ Since, \(-r_{\max}H_{\gamma}\leq q^{*}(s,a)\leq\left(r_{\max}+\tau\log|\mathcal{A}| \right)H_{\gamma}\), it holds that

\[\pi^{*}(a|s) \geq\frac{\exp\left(-r_{\max}H_{\gamma}/\tau\right)}{|\mathcal{A}| \exp\left(\left(r_{\max}+\tau\log|\mathcal{A}|\right)H_{\gamma}/\tau\right)}\] \[=\frac{\exp\left(-r_{\max}H_{\gamma}/\tau\right)}{|\mathcal{A}|^{1 +H_{\gamma}}\exp\left(r_{\max}H_{\gamma}/\tau\right)}=\frac{\exp\left(-2r_{ \max}H_{\gamma}/\tau\right)}{|\mathcal{A}|^{1+H_{\gamma}}}.\]_Part b)_: The proof of b) is similar to (Ouhamma and Kamgarpour, 2023, Lemma 8). However, our settings are slightly different. Recall that

\[\pi^{*}(a|s)=\left(\frac{\tau}{x_{s}-q^{*}(s,a)}\right)^{2}.\]

By Ouhamma and Kamgarpour (2023, Lemma 10) we have \(\tau\leq x_{s}-\left\lVert q_{s}^{*}\right\rVert_{\infty}\leq\tau\sqrt{| \mathcal{A}|}\). Furthermore, it holds that \(-r_{\max}H_{\gamma}\leq q^{*}(s,a)\leq\left(r_{\max}+2\tau\sqrt{|\mathcal{A}|} \right)H_{\gamma}\). Hence, we have

\[0<x_{s}-q^{*}(s,a)\leq\tau\sqrt{|\mathcal{A}|}+\left(r_{\max}+2\tau\sqrt{| \mathcal{A}|}\right)H_{\gamma}+r_{\max}H_{\gamma}\leq\left(2r_{\max}+3\tau \sqrt{|\mathcal{A}|}\right)H_{\gamma},\]

which yields the desired lower bound. 

We also highlight the following result, which shows that if the policy \(\pi^{\mu}\) is lower bounded on some set \(\mathcal{K}\subset\mathcal{M}\), then it is also lower bounded on its convex hull \(\operatorname{conv}\mathcal{K}\).

**Proposition D.4**.: _Suppose \(\mu=\alpha\mu_{1}+(1-\alpha)\mu_{2}\) with \(\alpha\in(0,1)\) and \(\mu_{1},\mu_{2}\in\mathcal{M}\). Then,_

\[\pi^{\mu}_{s}=\underbrace{\frac{\alpha\nu_{1}(s)}{\alpha\nu_{1}(s)+\bar{ \alpha}\nu_{2}(s)}}_{\widehat{\beta_{s}}}\pi^{\mu_{1}}_{s}+\underbrace{\frac{ \bar{\alpha}\nu_{2}(s)}{\alpha\nu_{1}(s)+\bar{\alpha}\nu_{2}(s)}}_{1-\widehat {\beta_{s}}}\pi^{\mu_{2}}_{s},\]

_where \(\beta_{s}\in(0,1)\)._

Proof.: The proof follows immediately from the proof of Proposition D.2. 

Hessian upper boundsIn the following, we establish upper bounds for the Hessians of the occupancy measure regularizations, \(\bar{h}\), resulting from Shannon and Tsallis entropy regularization of the policy. In particular, we aim to upper-bound the maximum norm of the Hessian in terms of the smallest entry of the policy.

**Proposition D.5**.: _Let Assumption 2.2 hold. Consider \(\mu\in\mathcal{M}\) and let \(\pi_{\text{min}}=\min_{s,a}\pi^{\mu}(a|s)>0\). Then, the Hessian of \(\bar{h}\) is upper bounded as follows:_

1. _If_ \(h=-\tau\mathcal{H}\)_, then_ \(\left\lVert\nabla^{2}\bar{h}(\mu)\right\rVert_{\infty}\leq\frac{\tau}{\nu_{ \text{min}}\pi_{\text{min}}}\)_._
2. _If_ \(h=-\tau\mathcal{H}_{1/2}\)_, then_ \(\left\lVert\nabla^{2}\bar{h}(\mu)\right\rVert_{\infty}\leq\frac{\tau}{\nu_{ \text{min}}\pi^{3/2}_{\text{min}}}\)_._

_Here, \(\left\lVert\cdot\right\rVert_{\infty}\) denotes the maximum norm \(\left\lVert A\right\rVert_{\infty}=\max_{ij}|A_{ij}|\)._

Proof.: As shown by Schlaginhaufen and Kamgarpour (2023, Proposition B.2), the gradient of \(\bar{h}\) satisfies

\[\nabla\bar{h}(\mu)(s,a)=h(\pi^{\mu}_{s})+\nabla h(\pi^{\mu}_{s})(a)-\langle \nabla h(\pi^{\mu}_{s}),\pi^{\mu}_{s}\rangle.\] (9)

Moreover, we have

\[\frac{\partial\pi^{\mu}(s,a)}{\partial\mu(s^{\prime},a^{\prime})}=\delta_{s,s^ {\prime}}\cdot\frac{\delta_{a,a^{\prime}}-\pi^{\mu}(a|s)}{\nu(s)}.\]

Using the above two formulas, we can calculate the Hessians explicitly.

_Part a)_: For the Shannon entropy it holds by (9) that \(\nabla\bar{h}(\mu)(s,a)=\tau\log\pi^{\mu}(a|s)\). Hence,

\[\frac{\partial^{2}\bar{h}(\mu)}{\partial\mu(s^{\prime},a^{\prime})\partial \mu(s,a)}=\tau\cdot\frac{1}{\pi^{\mu}(a|s)}\cdot\delta_{s,s^{\prime}}\cdot \frac{\delta_{a,a^{\prime}}-\pi^{\mu}(a|s)}{\nu(s)},\]

and

\[\left\lvert\nabla^{2}\bar{h}(\mu)_{(s^{\prime},a^{\prime}),(s,a)}\right\rvert =\left\lvert\frac{\partial^{2}\bar{h}(\mu)}{\partial\mu(s^{\prime},a^{\prime} )\partial\mu(s,a)}\right\rvert\leq\frac{\tau}{\nu_{\text{min}}\pi_{\text{min}}}.\]

_Part b)_: For the Tsallis entropy it holds by (9) that

\[\nabla\bar{h}(\mu)(s,a)=-\tau\left(\sum_{a^{\prime\prime}}\sqrt{\pi^{\mu}(a^{ \prime\prime}|s)}+\frac{1}{\sqrt{\pi^{\mu}(a|s)}}-2\right).\]Therefore, the second derivative is bounded as follows

\[\left|\frac{\partial^{2}\bar{h}(\mu)}{\partial\mu(s^{\prime},a^{ \prime})\partial\mu(s,a)}\right| =\left|-\tau\cdot\delta_{s,s^{\prime}}\cdot\left(\,\sum_{a^{\prime \prime}}\frac{1}{2\sqrt{\pi^{\mu}(a^{\prime\prime}|s)}}\frac{\delta_{a^{\prime },a^{\prime\prime}}-\pi^{\mu}(a^{\prime\prime}|s)}{\nu(s)}\right.\right.\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad-\left. \frac{1}{2\pi^{\mu}(a|s)^{3/2}}\frac{\delta_{a,a^{\prime}}-\pi^{\mu}(a|s)}{\nu (s)}\right)\right|\] \[=\frac{\tau\cdot\delta_{s,s^{\prime}}}{2\nu(s)}\left|\frac{1}{ \sqrt{\pi^{\mu}(a^{\prime}|s)}}-\sum_{a^{\prime\prime}}\sqrt{\pi^{\mu}(a^{ \prime\prime}|s)}-\frac{\delta_{a,a^{\prime}}}{\pi^{\mu}(a|s)^{3/2}}+\frac{1} {\sqrt{\pi^{\mu}(a|s)}}\right|\] \[\leq\frac{\tau}{2\nu_{\text{min}}}\left(\underbrace{\left|\frac{ 1}{\sqrt{\pi^{\mu}(a^{\prime}|s)}}-\sum_{a^{\prime\prime}}\sqrt{\pi^{\mu}(a^{ \prime\prime}|s)}\right|}_{(A)}+\underbrace{\left|\frac{\delta_{a,a^{\prime}}} {\pi^{\mu}(a|s)^{3/2}}-\frac{1}{\sqrt{\pi^{\mu}(a|s)}}\right|}_{(B)}\right).\]

Now, since \(\sum_{a}\sqrt{\pi^{\mu}(a|s)}\leq\sqrt{A}\leq 1/\sqrt{\pi_{\text{min}}}\), we have \((A)+(B)\leq 1/\sqrt{\pi_{\text{min}}}+1/\pi_{\text{min}}^{3/2}\leq 2/\pi_{ \text{min}}^{3/2}\), which yields the desired result

\[\left|\nabla^{2}\bar{h}(\mu)_{(s^{\prime},a^{\prime}),(s,a)}\right|\leq\tau/ \left(\nu_{\text{min}}\pi_{\text{min}}^{3/2}\right).\]

Lipschitz gradientsNext, we provide explicit Lipschitz constants \(L_{\mathcal{K}}\) for \(\nabla\bar{h}\) corresponding to Shannon and Tsallis entropy regularization.

**Proposition D.6** (Lipschitz gradients).: _Consider some closed convex set \(\mathcal{K}\subset\mathcal{M}\) and suppose \(\pi_{\text{min}}=\min_{\mu\in\mathcal{K}}\min_{s,a}\pi^{\mu}(a|s)>0\). Then, the gradient of \(\bar{h}\) is Lipschitz continuous over \(\mathcal{K}\) i.e._

\[\left\|\nabla\bar{h}(\mu_{1})-\nabla\bar{h}(\mu_{2})\right\|_{2}\leq L_{ \mathcal{K}}\left\|\mu_{1}-\mu_{2}\right\|_{2},\quad\forall\mu_{1},\mu_{2} \in\mathcal{K},\]

_where the respective Lipschitz constants are as follows:_

1. _If_ \(h=-\tau\mathcal{H}\)_, then_ \(L_{\mathcal{K}}=\tau|\mathcal{S}||\mathcal{A}|/(\nu_{\text{min}}\pi_{\text{min }})\)_._
2. _If_ \(h=-\tau\mathcal{H}_{1/2}\)_, then_ \(L_{\mathcal{K}}=\tau|\mathcal{S}||\mathcal{A}|/(\nu_{\text{min}}\pi_{\text{min }}^{3/2})\)_._

Proof.: Defining \(h=\mu_{2}-\mu_{1}\), Lipschitz continuity follows from

\[\left\|\nabla\bar{h}(\mu_{1})-\nabla\bar{h}(\mu_{2})\right\|_{2} \stackrel{{(i)}}{{\leq}} \sqrt{|\mathcal{S}||\mathcal{A}|}\left\|\nabla\bar{h}(\mu_{1})- \nabla\bar{h}(\mu_{2})\right\|_{\infty}\] \[\stackrel{{(ii)}}{{=}} \sqrt{|\mathcal{S}||\mathcal{A}|}\left\|\int_{0}^{1}\nabla^{2}\bar{ h}(\mu_{1}+th)hdt\right\|_{\infty}\] \[\stackrel{{(iii)}}{{\leq}} \sqrt{|\mathcal{S}||\mathcal{A}|}\int_{0}^{1}\left\|\nabla^{2}\bar{ h}(\mu_{1}+th)h\right\|_{\infty}dt\] \[\stackrel{{(iv)}}{{\leq}} \sqrt{|\mathcal{S}||\mathcal{A}|}\int_{0}^{1}\left\|\nabla^{2}\bar{ h}(\mu_{1}+th)\right\|_{\infty}\left\|h\right\|_{1}dt\] \[\stackrel{{(v)}}{{\leq}} \left|\mathcal{S}||\mathcal{A}|\max_{0\leq t\leq 1}\left\|\nabla^{2}\bar{ h}(\mu_{1}+th)\right\|_{\infty}\left\|\mu_{1}-\mu_{2}\right\|_{2}.\]

Here, we used in \((i)\) and \((v)\) that \(\left\|x\right\|_{1}\leq\sqrt{n}\left\|x\right\|_{2}\leq n\left\|x\right\|_{\infty}\) for \(x\in\mathbb{R}^{n}\), and in \((ii)\) we applied the fundamental theorem of calculus. Moreover, \((iii)\) follows from \(\left|\int f\right|\leq\int|f|\), and \((iv)\) from Holder's inequality. Now, by convexity \(\mu_{1},\mu_{2}\in\mathcal{K}\) implies that \(\mu_{1}+th\in\mathcal{K}\) for \(t\in[0,1]\). Hence, plugging in the upper bounds from Proposition D.5 concludes the proof.

### Dual smoothness and strong convexity

Next, we show that the convex conjugate, \(f^{*}\), of the extended real value function \(f:=\bar{h}+\delta_{\mathcal{M}}\) (see Appendix B) is - if understood as a mapping from \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}\) to \(\mathbb{R}\) - both smooth and strongly convex on \(\mathcal{R}\) with respect to the quotient norm. While it is well-known that global smoothness and strong convexity are dual properties (Rockafellar and West, 2009, Proposition 12.60), the key challenge for proving dual strong convexity is that \(\bar{h}\) has only locally Lipschitz gradients (see Proposition D.6). Proposition D.7 below shows that \(\eta\)-strong convexity of \(\bar{h}\) implies dual \(1/\eta\)-smoothness and locally Lipschitz gradients imply dual \(\sigma_{\mathcal{R}}\)-strong convexity on \(\mathcal{R}\) for some \(\sigma_{\mathcal{R}}>0\). Moreover, we provide explicit lower bounds on \(\sigma_{\mathcal{R}}\) for Shannon and Tsallis entropy retropary regularization.

**Proposition D.7**.: _Let \(f^{*}\) be the convex conjugate of \(f:=\bar{h}+\delta_{\mathcal{M}}\). Then, the following holds:_

1. _Suppose that_ \(\bar{h}\) _is_ \(\eta\)_-strongly convex over_ \(\mathcal{M}\)_, that is for all_ \(\mu,\mu^{\prime}\in\mathcal{M}\) _it holds that_ \[\bar{h}(\mu^{\prime})\geq\bar{h}(\mu)+\langle\nabla\bar{h}(\mu),\mu^{\prime}- \mu\rangle+\frac{\eta}{2}\left\|\mu-\mu^{\prime}\right\|_{2}^{2},\] _then we have for all_ \(r,r^{\prime}\in\mathbb{R}^{S\times\mathcal{A}}\) _that_ \[f^{*}(r^{\prime})\leq f(r)+\langle\nabla f^{*}(r),r^{\prime}-r\rangle+\frac{1} {2\eta}\left\|[r^{\prime}]_{\mathcal{U}}-[r]_{\mathcal{U}}\right\|_{2}^{2}.\] (10)
2. _Suppose that for any closed convex subset_ \(\mathcal{K}\subset\mathrm{relint}\,\mathcal{M}\)_, there is some_ \(L_{\mathcal{K}}>0\) _such that for all_ \(\mu,\mu^{\prime}\in\mathcal{K}\) _it holds that_ \[\left\|\nabla\bar{h}(\mu)-\nabla\bar{h}(\mu^{\prime})\right\|_{2}\leq L_{ \mathcal{K}}\left\|\mu-\mu^{\prime}\right\|_{2},\] _then we have for all_ \(r,r^{\prime}\in\mathcal{R}\) _that_ \[f^{*}(r^{\prime})\geq f^{*}(r)+\langle\nabla f^{*}(r),r^{\prime}-r\rangle+ \frac{\sigma_{\mathcal{R}}}{2}\left\|[r^{\prime}]_{\mathcal{U}}-[r]_{ \mathcal{U}}\right\|_{2}^{2},\] (11) _for some_ \(\sigma_{\mathcal{R}}>0\)_._
3. _Let_ \(H_{\gamma}:=1/(1-\gamma)\)_,_ \(R:=\max_{r\in\mathcal{R}}\left\|r\right\|_{\infty}\)_,_ \(D=\max_{r,r^{\prime}\in\mathcal{R}}\left\|r-r^{\prime}\right\|_{2}\)_, and suppose that_ \(\tau<D\)_, then for the Shannon entropy the inequality (_11_) holds with_ \[\sigma_{\mathcal{R}}=\frac{\nu_{\text{min}}\exp\left(\frac{-2RH_{\gamma}}{\tau }\right)}{2D|\mathcal{S}||\mathcal{A}|^{2+H_{\gamma}}},\] _and for the Tsallis entropy with_ \[\sigma_{\mathcal{R}}=\frac{\nu_{\text{min}}}{2\sqrt{2}D|\mathcal{S}||\mathcal{ A}|\left(\left(2R/\tau+3\sqrt{|\mathcal{A}|}\right)H_{\gamma}\right)^{3}}.\]

Proof.: _Part a):_ Since \(f\) is \(\eta\)-strongly convex with respect to \(\left\|\cdot\right\|_{2}\), the convex conjugate \(f^{*}\) is \(1/\eta\)-smooth with respect to the dual norm in \(\mathbb{R}^{S\times\mathcal{A}}/\mathcal{U}\)(Rockafellar and West, 2009, Proposition 12.60), which is equivalent to (10).

_Part b):_ The show b), we closely follow (Goebel and Rockafellar, 2008, Theorem 4.1), but we need to account for the quotient spaces. We define the sets \(\mathcal{K}=\nabla f^{*}(\mathcal{R})=\mathsf{RL}(\mathcal{R})\) and \(\mathcal{K}_{\epsilon}=\mathrm{conv}(\mathcal{K})+\epsilon(\mathcal{B}\cap \mathrm{aff}(\mathcal{M}))\), where \(\mathcal{B}\subset\mathbb{R}^{S\times\mathcal{A}}\) denotes the closed unit ball with respect to \(\left\|\cdot\right\|_{2}\) and \(\epsilon>0\) is chosen such that \(\mathcal{K}_{\epsilon}\subset\mathrm{relint}\,\mathcal{M}\). Moreover, we let \(L\) be the Lipschitz constant of \(\nabla\bar{h}\) over\(\mathcal{K}_{\epsilon}\). Now, consider \(r\in\mathcal{R}\) and \(\mu=\nabla f^{*}(r)\). Then, for any \(r^{\prime}\in\mathcal{R}\), we have

\[f^{*}(r^{\prime}) =\sup_{\bar{\mu}}\left[\langle r^{\prime},\bar{\mu}\rangle-f(\bar{ \mu})\right]\] \[\overset{(i)}{\geq}\sup_{\bar{\mu}\in\mathcal{K}_{\epsilon}} \left[\langle r^{\prime},\bar{\mu}\rangle-f(\bar{\mu})\right]\] \[\overset{(ii)}{\geq}\sup_{\bar{\mu}\in\mathcal{K}_{\epsilon}} \left[\langle r^{\prime},\bar{\mu}\rangle-f(\mu)-\langle r,\bar{\mu}-\mu \rangle-\frac{L}{2}\left\|\bar{\mu}-\mu\right\|_{2}^{2}\right]\] \[\overset{(iii)}{=}\langle r,\mu\rangle-f(\mu)+\sup_{\bar{\mu}\in \mathcal{K}_{\epsilon}}\left[\langle r^{\prime}-r,\bar{\mu}\rangle-\frac{L}{2} \left\|\bar{\mu}-\mu\right\|_{2}^{2}\right]\] \[\overset{(iv)}{=}f^{*}(r)+\sup_{\bar{\mu}\in\mathcal{K}_{ \epsilon}}\left[\langle r^{\prime}-r,\bar{\mu}\rangle-\frac{L}{2}\left\|\bar{ \mu}-\mu\right\|_{2}^{2}\right]\] \[\overset{(v)}{=}f^{*}(r)+\langle r^{\prime}-r,\mu\rangle+\sup_ {\bar{\mu}\in\mathcal{K}_{\epsilon}}\left[\langle r^{\prime}-r,\bar{\mu}-\mu \rangle-\frac{L}{2}\left\|\bar{\mu}-\mu\right\|_{2}^{2}\right].\]

Here, \((i)\) follows from \(\mathcal{K}_{\epsilon}\subset\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\), \((ii)\) from the fact that Lipschitz gradients imply that

\[f(\bar{\mu})\leq f(\mu)+\langle g,\bar{\mu}-\mu\rangle+\frac{L}{2}\left\| \bar{\mu}-\mu\right\|_{2}^{2},\]

for any \(g\in\partial f(\mu)\)[Beck, 2017, Lemma 5.7] and \(r\in\partial f(\mu)\) (see Theorem B.4). Moreover, \((iii)\) and \((v)\) follow from rearranging terms, and \((iv)\) from \(f^{*}(r)=\langle r,\mu\rangle-f(\mu)\). Now, consider any \(\alpha>0\) such that \(\sigma_{\mathcal{R}}=2(\alpha-L\alpha^{2}/2)>0\) and \(\alpha D\leq\epsilon\). Setting \(\bar{\mu}\in\mathcal{K}_{\epsilon}\) in the above supremum to \((\bar{\mu}-\mu)=\alpha\Pi_{\mathcal{U}^{\perp}}(r^{\prime}-r)\) yields the desired result

\[f^{*}(r^{\prime}) \geq f^{*}(r)+\langle r^{\prime}-r,\nabla f^{*}(r)\rangle+ \alpha\langle r^{\prime}-r,\Pi_{\mathcal{U}^{\perp}}(r^{\prime}-r)\rangle- \frac{L\alpha^{2}}{2}\left\|\Pi_{\mathcal{U}^{\perp}}(r^{\prime}-r)\right\|_{2 }^{2}\] \[=f^{*}(r)+\langle r^{\prime}-r,\nabla f^{*}(r)\rangle+\frac{ \sigma_{\mathcal{R}}}{2}\left\|\left[r^{\prime}\right]_{\mathcal{U}}-\left[r \right]_{\mathcal{U}}\right\|_{2}^{2}.\]

Note that we indeed have \(\bar{\mu}\in\mathcal{K}_{\epsilon}\) as \(\mu\in\mathcal{K}\) and \(\left\|\mu-\bar{\mu}\right\|_{2}\leq\alpha\left\|r-r^{\prime}\right\|_{2}\leq \alpha D\leq\epsilon\).

_Part c):_ To get an explicit constant for \(\sigma_{\mathcal{R}}\), we need to appropriately choose \(\epsilon\) and calculate the corresponding Lipschitz constant. To this end, we first recall that according to Proposition D.3 and D.4 policies corresponding to occupancy measures in \(\operatorname{conv}\mathcal{K}\) are, for Shannon and Tsallis entropy, lower bounded by

\[\pi_{\text{min, Sh}}=\frac{\exp\left(-2RH_{\gamma}/\tau\right)}{|\mathcal{A}| ^{1+H_{\gamma}}}\quad\text{and}\quad\pi_{\text{min, Ts}}=\left(\left(2R/\tau+3 \sqrt{|\mathcal{A}|}\right)H_{\gamma}\right)^{-2},\]

respectively. Furthermore, for any \(\mu\in\mathcal{K}_{\epsilon}\), we have by Proposition D.1 and equivalence of norms

\[\pi^{\mu}(a|s)\geq\pi_{\text{min}}-\frac{2\sqrt{|\mathcal{S}||\mathcal{A}|}}{ \nu_{\text{min}}}\epsilon=\pi^{\prime}_{\text{min}}.\]

Hence, by setting \(\epsilon=\nu_{\text{min}}\pi_{\text{min}}/(4\sqrt{|\mathcal{S}||\mathcal{A}|})\), we have \(\pi^{\mu}(a|s)\geq\pi^{\prime}_{\text{min}}=\pi_{\text{min}}/2\) for any \(\mu\in\mathcal{K}_{\epsilon}\). As for the Lipschitz constant over \(\mathcal{K}_{\epsilon}\), we have by Proposition D.6

\[L_{\text{Sh}}=\frac{\tau|\mathcal{S}||\mathcal{A}|}{\nu_{\text{min}}\pi^{ \prime}_{\text{min, Sh}}}\quad\text{and}\quad L_{\text{Ts}}=\frac{\tau|\mathcal{S}|| \mathcal{A}|}{\nu_{\text{min}}\pi^{\prime}_{\text{min, Ts}}},\] (13)

for the Shannon and Tsallis entropy, respectively. Now, we need to ensure that \(\alpha>0\) such that \(\sigma_{\mathcal{R}}=2(\alpha-L\alpha^{2}/2)>0\) and \(\alpha\leq\epsilon/D\). To that end, we set for both regularizations \(\alpha=\tau/(LD)\), which in light of (13) ensures that

\[\alpha=\frac{\tau}{LD}\leq\frac{\nu_{\text{min}}\pi^{\prime}_{\text{min}}}{D| \mathcal{S}||\mathcal{A}|}\leq\frac{\nu_{\text{min}}\pi^{\prime}_{\text{min}}}{2 D\sqrt{|\mathcal{S}||\mathcal{A}|}}=\frac{\epsilon}{D},\] (14)

for \(|\mathcal{S}|\), \(|\mathcal{A}|\geq 2\). Moreover, we get the dual strong convexity constant

\[\sigma_{\mathcal{R}}=2\left(\frac{\tau}{LD}-\frac{\tau^{2}}{2LD^{2}}\right)= \frac{2\tau}{LD}\left(1-\frac{\tau}{2D}\right),\]which is larger than \(\tau/LD\) for \(\tau<D\). We can therefore choose \(\sigma_{\mathcal{R}}=\tau/LD\) as a dual strong convexity constant. Plugging in the Lipschitz constants for the two regularizations yields

\[\sigma_{\mathcal{R},\text{Sh}}=\frac{\pi_{\text{min, Sh}}^{\prime}\nu_{\text{min}}}{D|\mathcal{S}||\mathcal{A}|}= \frac{\pi_{\text{min, Sh}}\nu_{\text{min}}}{2D|\mathcal{S}||\mathcal{A}|}=\frac{ \exp\left(-2RH_{\gamma}/\tau\right)\nu_{\text{min}}}{2D|\mathcal{S}||\mathcal{ A}|^{2+H_{\gamma}}},\]

for the Shannon entropy, and

\[\sigma_{\mathcal{R},\text{Ts}}=\frac{\pi_{\text{min, Ts}}^{\prime}\sfrac{3/2}{\nu_{\text{min}}}}{D|\mathcal{S}||\mathcal{A}|}= \frac{\pi_{\text{min, Ts}}^{3/2}\nu_{\text{min}}}{2\sqrt{2}D|\mathcal{S}|| \mathcal{A}|}=\frac{\nu_{\text{min}}}{2\sqrt{2}D|\mathcal{S}||\mathcal{A}| \left(\left(2R/\tau+3\sqrt{|\mathcal{A}|}\right)H_{\gamma}\right)^{3}},\]

for the Tsallis entropy. 

_Remark D.8_ (Large \(\tau\) regime).: Note that if \(\tau\geq 2D/\sqrt{|\mathcal{S}||\mathcal{A}|}\), we can set \(\sigma_{\mathcal{R}}=\alpha=1/L\), while still satisfying the condition (14). This leads to the strong convexity constants

\[\sigma_{\mathcal{R},\text{Sh}}=\frac{\exp\left(-2RH_{\gamma}/\tau\right)\nu_{ \text{min}}}{2\tau|\mathcal{S}||\mathcal{A}|^{2+H_{\gamma}}},\ \sigma_{\mathcal{R},\text{Ts}}=\frac{\nu_{\text{min}}}{2\sqrt{2}\tau| \mathcal{S}||\mathcal{A}|\left(\left(2R/\tau+3\sqrt{|\mathcal{A}|}\right)H_{ \gamma}\right)^{3}}.\]

### Regularity constants

In the following Proposition, we summarize the regularity constants for Shannon and Tsallis entropy regularization. We highlight that these constants are lower bounds for \(\eta\) and \(\sigma_{\mathcal{R}}\).

**Proposition D.9**.: _Let \(H_{\gamma}:=1/(1-\gamma)\), \(R:=\max_{r\in\mathcal{R}}\left\|r\right\|_{\infty}\), and \(D=\max_{r,r^{\prime}\in\mathcal{R}}\left\|r-r^{\prime}\right\|_{2}\). Suppose that \(\tau<D\), then for the Shannon entropy, Assumption 3.5 holds with_

\[\eta=\tau\nu_{\text{min}}/H_{\gamma}^{2}\quad\text{and}\quad\sigma_{\mathcal{ R}}=\frac{\exp\left(-2RH_{\gamma}/\tau\right)\nu_{\text{min}}}{2D|\mathcal{S}|| \mathcal{A}|^{2+H_{\gamma}}},\]

_and for the Tsallis entropy with_

\[\eta=\tau\nu_{\text{min}}/(2H_{\gamma}^{2}|\mathcal{A}|)\quad\text{and}\quad \sigma_{\mathcal{R}}=\frac{\nu_{\text{min}}}{2\sqrt{2}D|\mathcal{S}||\mathcal{ A}|\left(\left(2R/\tau+3\sqrt{|\mathcal{A}|}\right)H_{\gamma}\right)^{3}}.\]

Proof.: The derivation for \(\eta\) is given in Proposition D.2 and for \(\sigma_{\mathcal{R}}\) in Proposition D.7 above. 

### Suboptimality bounds

**Proposition 3.4**.: _Under Assumptions 2.1 and 2.2, we have \(\mathsf{SubOpt}(r^{\prime},\mu)=D_{\bar{h}}(\mu,\mathsf{RL}(r^{\prime}))\) for any \(\mu\in\mathcal{M}\)._

Proof.: Let \(\mu=\mathsf{RL}(r)\). We have

\[\mathsf{SubOpt}(r,\mu^{\prime}) =\langle r,\mu-\mu^{\prime}\rangle-\bar{h}(\mu)+\bar{h}(\mu^{ \prime})\] \[=\langle\nabla\bar{h}(\mu),\mu-\mu^{\prime}\rangle-\bar{h}(\mu)+ \bar{h}(\mu^{\prime})\] \[=D_{\bar{h}}(\mu^{\prime},\mu),\]

where the second equality holds, as by (4) we have \(r-\nabla\bar{h}(\mu)\in\mathcal{U}\), and \(\mu-\mu^{\prime}\in\mathcal{U}^{\perp}\). 

**Lemma 3.6**.: _Suppose Assumptions 2.1,2.2, and 3.5 hold, and let \(r,r^{\prime}\in\mathcal{R}\). Then, we have_

\[\frac{\sigma_{\mathcal{R}}}{2}\left\|[r]_{\mathcal{U}}-[r^{\prime}]_{\mathcal{ U}}\right\|_{2}^{2}\leq\mathsf{SubOpt}(r^{\prime},\mathsf{RL}(r))=D_{\bar{h}} \left(\mathsf{RL}(r),\mathsf{RL}(r^{\prime})\right)\leq\frac{1}{2\eta}\left\|[ r]_{\mathcal{U}}-[r^{\prime}]_{\mathcal{U}}\right\|_{2}^{2},\] (6)

_for some problem-dependent constant \(\sigma_{\mathcal{R}}>0\)._Proof.: Let \(f:=\overline{h}+\delta_{\mathcal{M}}\) and \(\mu=\mathsf{RL}(r),\mu^{\prime}=\mathsf{RL}(r^{\prime})\). We then have

\[D_{\overline{h}}(\mu,\mu^{\prime}) \stackrel{{(i)}}{{=}}f(\mu)-f(\mu^{\prime})-\langle r ^{\prime},\mu-\mu^{\prime}\rangle\] \[\stackrel{{(ii)}}{{=}}f^{*}(r^{\prime})-\langle r^{ \prime},\mu^{\prime}\rangle-f^{*}(r)+\langle r,\mu\rangle-\langle r^{\prime}, \mu-\mu^{\prime}\rangle\] \[\stackrel{{(iii)}}{{=}}f^{*}(r^{\prime})-f^{*}(r)- \langle r^{\prime}-r,\nabla f^{*}(r)\rangle=D_{f^{*}}(r^{\prime},r).\]

Here, \((i)\) follows from the definition of \(f\) and \(r^{\prime}\in[\nabla\overline{h}(\mu^{\prime})]_{\mathcal{U}}\), in \((ii)\) we use that \(f(\mu)=\langle r,\mu\rangle-f^{*}(r)\) and \(f(\mu^{\prime})=\langle r^{\prime},\mu^{\prime}\rangle-f^{*}(r^{\prime})\), and \((iii)\) follows from rearranging terms and \(\mu=\nabla f^{*}(r)\). The result then follows from dual strong convexity and smoothness as established in Proposition D.7. 

Note that without steep regularization it is impossible to lower bound the suboptimality in terms of reward distances in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathcal{U}\) (Proposition 3.4 doesn't hold). However, we still have the following upper bound.

**Proposition D.10**.: _Consider an arbitrary regularization and let \(\mu\in\mathsf{RL}(r),\mu^{\prime}\in\mathsf{RL}(r^{\prime})\). Then,_

\[\mathsf{SubOpt}(r,\mu^{\prime})\leq 2\left\|[r]_{\mathcal{U}}-[r^{\prime}]_{ \mathcal{U}}\right\|_{\infty}\leq 2\left\|[r]_{\mathcal{U}}-[r^{\prime}]_{ \mathcal{U}}\right\|_{2}.\]

Proof.: Let \(r^{\prime\prime}:=\operatorname*{argmin}_{\bar{r}\in[r^{\prime}]_{\mathcal{U} }}\left\|\bar{r}-r\right\|_{\infty}\), then the following holds

\[\mathsf{SubOpt}(r,\mu^{\prime}) =\max_{\mu\in\mathcal{M}}J(r,\mu)-J(r,\mu^{\prime})\] \[\stackrel{{(i)}}{{\leq}}\left|\max_{\mu\in\mathcal{ M}}J(r,\mu)-\max_{\mu\in\mathcal{M}}J(r^{\prime\prime},\mu)\right|+|J(r^{\prime \prime},\mu^{\prime})-J(r,\mu^{\prime})|\] \[\stackrel{{(ii)}}{{\leq}}\max_{\mu\in\mathcal{M}} \left|\langle r-r^{\prime\prime},\mu\rangle\right|+|\langle r-r^{\prime\prime},\mu^{\prime}\rangle|\] \[\stackrel{{(iii)}}{{\leq}}2\left\|r-r^{\prime\prime} \right\|_{\infty}\stackrel{{(iv)}}{{=}}2\left\|[r]_{\mathcal{U}}- [r^{\prime}]_{\mathcal{U}}\right\|_{\infty}\leq 2\left\|[r]_{\mathcal{U}}-[r^{\prime}]_{ \mathcal{U}}\right\|_{2}.\]

Here, \((i)\) follows from the triangle inequality and optimality of \(\mu^{\prime}\), \((ii)\) from \(\max f-\max g|\leq\max|f-g|\) and simplifying, \((iii)\) from Holder's inequality, and \((iv)\) from the definition of \(r^{\prime\prime}\) and the quotient norm. 

### Perturbation bounds

Next, we provide a bound quantifying the change in the quotient norm when changing the generating subspace.

**Proposition D.11**.: _Consider \(x,y\in\mathbb{R}^{n}\) and two subspaces \(\mathcal{V},\mathcal{W}\subset\mathbb{R}^{n}\) of dimension \(m<n\). Then,_

\[\left\|[x]_{\mathcal{W}}-[y]_{\mathcal{W}}\right\|_{2}\leq\left\|\Pi_{ \mathcal{W}}-\Pi_{\mathcal{V}}\right\|\cdot\left\|x-y\right\|_{2}+\left\|[x]_ {\mathcal{V}}-[y]_{\mathcal{V}}\right\|_{2},\]

_where \(\left\|\Pi_{\mathcal{W}}-\Pi_{\mathcal{V}}\right\|=\sin\left(\theta_{\max}( \mathcal{V},\mathcal{W})\right)\)._

Proof.: The result follows from the triangle inequality and the definition of the spectral norm:

\[\left\|[x]_{\mathcal{W}}-[y]_{\mathcal{W}}\right\|_{2} =\left\|\Pi_{\mathcal{W}^{\perp}}(x-y)\right\|_{2}\] \[=\left\|(\Pi_{\mathcal{W}^{\perp}}-\Pi_{\mathcal{V}^{\perp}})(x-y )+\Pi_{\mathcal{V}^{\perp}}(x-y)\right\|_{2}\] \[\leq\left\|(\Pi_{\mathcal{W}^{\perp}}-\Pi_{\mathcal{V}^{\perp}})(x -y)\right\|_{2}+\left\|\Pi_{\mathcal{V}^{\perp}}(x-y)\right\|_{2}\] \[=\left\|(\Pi_{\mathcal{W}}-\Pi_{\mathcal{V}})(x-y)\right\|_{2}+ \left\|[x]_{\mathcal{V}}-[y]_{\mathcal{V}}\right\|_{2}\] \[\leq\left\|\Pi_{\mathcal{W}}-\Pi_{\mathcal{V}}\right\|\cdot \left\|x-y\right\|_{2}+\left\|[x]_{\mathcal{V}}-[y]_{\mathcal{V}}\right\|_{2}.\]

Furthermore, for a proof of \(\left\|\Pi_{\mathcal{W}}-\Pi_{\mathcal{V}}\right\|=\sin\left(\theta_{\max}( \mathcal{V},\mathcal{W})\right)\) we refer to [10]. 

The following proposition shows that the maximal principal angle between two transition laws can be upper bounded by the spectral norm difference of the transition laws.

**Proposition 3.9**.: _Let \(P,P^{\prime}\in\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\) and \(H_{\gamma}=1/(1-\gamma)\). Then, we have \(\theta_{1}(P,P^{\prime})=0\) and \(\sin\left(\theta_{\max}(P,P^{\prime})\right)\leq\gamma H_{\gamma}\sqrt{| \mathcal{S}|/|\mathcal{A}|}\left\|P-P^{\prime}\right\|\), where \(\left\|\cdot\right\|\) denotes the spectral norm._Before proceeding with the proof of Proposition 3.9, we need the following technical result.

**Proposition D.12**.: _For any \(P\in\Delta_{\mathcal{S}\times\mathcal{A}}^{\mathcal{S}}\) the smallest singular value of \(E-\gamma P\) satisfies_

\[\sigma_{\min}\left(E-\gamma P\right)\geq\sqrt{|\mathcal{A}|/|\mathcal{S}|}(1- \gamma).\]

Proof.: The main idea of the proof is to use that \(\sigma_{\min}(A)=\min_{x\neq 0}\left\|Ax\right\|_{2}/\left\|x\right\|_{2}\) for any matrix \(A\in\mathbb{R}^{n\times m}\). We first lower bound \(\sigma_{\min}(I-\gamma P_{a})=\sigma_{\min}\left(I-\gamma(P_{a})^{\top}\right)\), where \(P_{a}\) denotes the state transition matrix under action \(a\). Let \(x\in\mathbb{R}^{S}\), then we have

\[\left\|\left(I-\gamma(P_{a})^{\top}\right)x\right\|_{2} \geq 1/\sqrt{|\mathcal{S}|}\left\|\left(I-\gamma(P_{a})^{\top} \right)x\right\|_{1}\] \[\geq 1/\sqrt{|\mathcal{S}|}\left(\left\|x\right\|_{1}-\gamma \left\|(P_{a})^{\top}x\right\|_{1}\right)\] \[\geq(1-\gamma)/\sqrt{|\mathcal{S}|}\left\|x\right\|_{2}\geq(1- \gamma)/\sqrt{|\mathcal{S}|}\left\|x\right\|_{2},\]

where the third inequality follows from

\[\left\|(P_{a})^{\top}x\right\|_{1}=\sum_{s}\left|\sum_{s^{\prime}}P_{a}(s|s^{ \prime})x(s^{\prime})\right|\leq\sum_{s}\sum_{s^{\prime}}P_{a}(s|s^{\prime}) \left|x(s^{\prime})\right|=\left\|x\right\|_{1}.\]

Hence, \(\sigma_{\min}(I-\gamma P_{a})\geq(1-\gamma)/\sqrt{|\mathcal{S}|}\). Therefore, we have for any \(x\in\mathbb{R}^{\mathcal{S}}\) that

\[\left\|(E-\gamma P)x\right\|_{2}^{2}=\sum_{a}\left\|(I-\gamma P_{a})x\right\|_ {2}^{2}\geq|\mathcal{A}|/|\mathcal{S}|(1-\gamma)^{2}\left\|x\right\|_{2}^{2},\]

which yields the desired result. 

We are now ready to prove Proposition 3.9.

Proof of Proposition 3.9.: The first principal angle \(\theta_{1}(P,P^{\prime})=0\) is zero as we always have \(\mathbf{1}\subseteq\mathcal{U}\). The bound on the maximal angle follows from a well-known perturbation result for orthogonal projections. Namely, if \(A,B\in\mathbb{R}^{n\times m}\) are matrices of the same rank and \(\Pi_{A},\Pi_{B}\) denote the orthogonal projections onto their column span, then we have [11, 12]

\[\left\|\Pi_{A}-\Pi_{B}\right\|\leq\min\left\{\left\|A^{\dagger}\right\|, \left\|B^{\dagger}\right\|\right\}\left\|A-B\right\|,\]

where \(A^{\dagger}\) denotes the Moore-Penrose inverse [10]. Recall that \(\mathcal{U}_{P}=\operatorname{im}(E-\gamma P)\), \(\sin\left(\theta_{\max}(P,P^{\prime})\right)=\left\|\Pi_{\mathcal{U}_{P}}-\Pi _{\mathcal{U}_{P^{\prime}}}\right\|\), and by Proposition D.12

\[\left\|(E-\gamma P)^{\dagger}\right\|=\left(\sigma_{\min}\left(E-\gamma P \right)\right)^{-1}\leq\sqrt{|\mathcal{S}|/|\mathcal{A}|}H_{\gamma}.\]

Therefore, we get

\[\sin\left(\theta_{\max}(P,P^{\prime})\right)\leq\sqrt{|\mathcal{S}|/|\mathcal{ A}|}\cdot\gamma\cdot H_{\gamma}\cdot\left\|P-P^{\prime}\right\|.\]

## Appendix E Proof of claim in Example 3.3

We recall Example 3.3 from the main paper.

**Example 3.3**.: We consider a two-state, two-action MDP with \(\mathcal{S}=\mathcal{A}=\{0,1\}\), uniform initial state distribution, discount rate \(\gamma=0.9\), and Shannon entropy regularization \(h=-\mathcal{H}\) (see Appendix C). Suppose the expert reward is \(r^{\mathsf{E}}(s,a)=1\{s=1\}\) and consider the transition laws, \(P^{0}\) and \(P^{1}\), defined by \(P^{0}(0|s,a)=0.75\) and \(P^{1}(0|s,a)=0.25+\beta\cdot 1\left\{s=0,a=0\right\}\) for some \(\beta\in[0,0.75]\). Also, consider the two experts \(\mu_{P^{0}}^{\mathsf{E}}=\mathsf{RL}_{P^{0}}(r^{\mathsf{E}})\) and \(\mu_{P^{1}}^{\mathsf{E}}=\mathsf{RL}_{P^{1}}(r^{\mathsf{E}})\), and suppose we recovered the reward \(\hat{r}(s,a)=-r^{\mathsf{E}}\). Then, the following holds: 1) We have \(\mathsf{SubOpt}_{P^{0}}(\hat{r},\mu_{P^{0}}^{\mathsf{E}})=0\) and \(\mathsf{SubOpt}_{P^{1}}(\hat{r},\mu_{P^{1}}^{\mathsf{E}})=\mathcal{O}(\beta)\). That is, for small \(\beta\), the reward \(\hat{r}\) is a good solution to the IRL problem, as both experts are approximately optimal under \(\hat{r}\). 2) The rank condition (5) between \(P^{0}\) and \(P^{1}\) is satisfied for any \(\beta>0\). 3) For a new transition law \(P\) defined by \(P(0|s,a)=1\)\(\{s=1,a=0\}\), we have \(\mathsf{SubOpt}_{P}(r^{\mathsf{E}},\mathsf{RL}_{P}(\hat{r}))\approx 4.81\), i.e. \(\mathsf{RL}_{P}(\hat{r})\) performs poorly under the experts' reward.

In the following we prove the claims 1. and 2., while 3. is computed via regularized dynamic programming (Geist et al., 2019).3 Furthermore, we illustrate the occupancy measure spaces corresponding to \(P^{0}\) and \(P^{1}\) for different \(\beta\) in Figure 3.

Footnote 3: The code is openly accessible at https://github.com/andrschl/irl_transferability.

1. Consider the transition law \(P^{\prime}\) defined by \(P^{\prime}(0|s,a)=0.25\). First, we observe that while \(\left\|P^{0}-P^{\prime}\right\|\) is large, the potential shaping spaces \(\mathcal{U}_{P^{0}}\) and \(\mathcal{U}_{P^{\prime}}\) coincide. To see this note that we have \(P^{\prime}(\cdot|s,a)=P^{0}(\cdot|s,a)+\Delta\), where \(\Delta=[-0.5,0.5]^{\top}\). Hence, we have for any \(x\in\mathbb{R}^{2}\) that \[(E-\gamma P^{\prime})x=(E-\gamma P^{0})x-\gamma\langle\Delta,x\rangle\mathbf{ 1}_{4}=(E-\gamma P^{0})\left(x-\frac{\gamma\langle\Delta,x\rangle}{1-\gamma} \mathbf{1}_{2}\right),\] where \(\mathbf{1}_{n}\) denotes the all-one vector in \(\mathbb{R}^{n}\). Therefore, \(\mathrm{span}(E-\gamma P^{\prime})=\mathrm{span}(E-\gamma P^{0})\). Moreover, we have \[\left\|P^{1}-P^{\prime}\right\|\leq\sqrt{\sum_{s,s^{\prime},a}(P^{1}(s^{ \prime}|s,a)-P^{\prime}(s^{\prime}|s,a))^{2}}=\sqrt{2}\beta.\]

Figure 3: The set of occupancy measures \(\mathcal{M}_{P^{0}}\) and \(\mathcal{M}_{P^{1}}\) are illustrated in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathbf{1}\cong\mathbf{1}^{\perp}\). For a two-state-two-action MDP, the set of occupancy measures is given by the intersection of a two-dimensional affine subspace (a plane in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathbf{1}\)) with the probability simplex in \(\mathbb{R}^{4}\) (a tetrahedron in \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}/\mathbf{1}\)). We see that for a small \(\beta\), the sets \(\mathcal{M}_{P^{0}}\) and \(\mathcal{M}_{P^{1}}\) are approximately parallel. That is, the angle between their normal vectors, which span the potential shaping spaces \(\mathcal{U}_{P^{0}}\) and \(\mathcal{U}_{P^{1}}\), is small. In contrast, for a large \(\beta\) the orientation of \(\mathcal{M}_{P^{0}}\) and \(\mathcal{M}_{P^{1}}\) is very different, resulting in a large angle between the corresponding normal vectors.

In light of Propositions D.10, D.11, and 3.9, this implies that \[\mathsf{SubOpt}_{P^{1}}(\hat{r},\mathsf{RL}_{P^{1}}(\mathsf{r}^{ \mathsf{E}})) \leq 2\left\|\left[\hat{r}\right]_{\mathcal{U}_{P^{1}}}-\left[ \mathsf{r}^{\mathsf{E}}\right]_{\mathcal{U}_{P^{1}}}\right\|_{2}\] \[\leq 2\left\|\left[\Pi_{\mathcal{U}_{P^{1}}}-\Pi_{\mathcal{U}_{P ^{\prime}}}\right]\left\|\left\|\hat{r}-\mathsf{r}^{\mathsf{E}}\right\|_{2}\right.\] \[\leq 2\gamma\cdot H_{\gamma}\cdot\left\|P-P^{\prime}\right\|\leq 2 \sqrt{2}\gamma\cdot H_{\gamma}\cdot\beta.\]
2. We need to show that \(P^{0}\) and \(P^{1}\) are satisfying the rank condition \[\operatorname{rank}\left(\left[E-\gamma P^{0},\qquad E-\gamma P^{1}\right] \right)=2|\mathcal{S}|-1.\] By the same reasoning as above, we can equivalently show the rank condition for the transition laws \({P^{0}}^{\prime},{P^{1}}^{\prime}\) defined by \({P^{0}}^{\prime}(0|s,a)=1\) and \({P^{1}}^{\prime}(0|s,a)=\beta\cdot\mathbf{1}\left\{s=0,a=0\right\}\). To this end, we choose the matrix representation \[E=\begin{bmatrix}I\\ I\end{bmatrix}\quad\text{and}\quad P=\begin{bmatrix}P_{a_{0}}\\ P_{a_{1}}\end{bmatrix},\] where \(I\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) is the identity matrix and \(P_{a_{0}},P_{a_{1}}\in\mathbb{R}^{|\mathcal{S}|\times|\mathcal{S}|}\) are the state transition matrices corresponding to the actions \(0,1\), respectively. Let \(C=\begin{bmatrix}E-\gamma{P^{0}}^{\prime},\qquad E-\gamma{P^{1}}^{\prime} \end{bmatrix}\). We have \[{P^{0}}^{\prime}=\begin{bmatrix}1&0\\ 1&0\\ 1&0\end{bmatrix}\quad\text{and}\quad{P^{1}}^{\prime}=\begin{bmatrix}\beta&1- \beta\\ 0&1\\ 0&1\\ 0&1\end{bmatrix},\] and \[C=\begin{bmatrix}1-\gamma&0&1-\beta\gamma&-\gamma+\beta\gamma\\ -\gamma&1&0&1-\gamma\\ 1-\gamma&0&1&-\gamma\\ -\gamma&1&0&1-\gamma\end{bmatrix}.\] It's straightforward to see that the vector \(\left[1\quad 1\quad-1\quad-1\right]^{\top}\) lies in the kernel of \(C\), but there is a \(3\times 3\) submatrix with non-zero determinant: \[\det\left(\begin{bmatrix}1-\gamma&0&1-\beta\gamma\\ -\gamma&1&0\\ 1-\gamma&0&1\end{bmatrix}\right)=1\cdot\left[(1-\gamma)-(1-\gamma)(1-\beta \gamma)\right]=\beta\gamma(1-\gamma)>0.\] In other words, we have \(\operatorname{rank}C=3\) for any \(\beta>0\).

## Appendix F Proof of Theorem 3.10

**Theorem 3.10**.: _Let \(K=2\), \(\theta_{2}(P^{0},P^{1})>0\), and suppose that Assumptions 2.1,2.2, and 3.5 hold. If \(\mathsf{SubOpt}_{P^{k}}(\hat{r},\mu_{P^{k}}^{\mathsf{E}})\leq\hat{\varepsilon}\) for \(k=0,1\), then \(\hat{r}\) is \(\varepsilon\)-transferable to \(\mathcal{P}=\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\) with_

\[\varepsilon=\hat{\varepsilon}/\left[\eta\sigma_{\mathcal{R}}\sin\left(\theta_ {2}(P^{0},P^{1})\left/2\right)^{2}\right].\]

The proof of Theorem 3.10 hinges on Lemma 3.6 and the following reward approximation result.

**Lemma F.1**.: _Let \(\left\|\left[\mathsf{r}^{\mathsf{E}}\right]_{\mathcal{U}_{P^{k}}}-\left[\hat{r }\right]_{\mathcal{U}_{P^{k}}}\right\|_{2}\leq\bar{\varepsilon}\) for \(k=0,1\). Then, if \(\theta_{2}(P^{0},P^{1})>0\), it holds that_

\[\left\|\left[\mathsf{r}^{\mathsf{E}}\right]_{\mathbf{1}}-\left[\hat{r} \right]_{\mathbf{1}}\right\|_{2}\leq\frac{\bar{\varepsilon}}{\sin\left(\theta_ {2}(P^{0},P^{1})/2\right)}.\]

Proof of Lemma F.1.: Throughout this proof, we will use the short-hand notation \(\mathcal{U}_{k}:=\mathcal{U}_{P^{k}}\) for \(k=0,1\). Recall that since \(\mathbf{1}\subseteq\mathcal{U}_{0}\cap\mathcal{U}_{1}\), we have \(\theta_{1}(\mathcal{U}_{0},\mathcal{U}_{1})=0\) and by assumption we also have \(\theta_{2}(\mathcal{U}_{0},\mathcal{U}_{1})>0\), which implies that \(\mathcal{U}_{0}\cap\mathcal{U}_{1}=\mathbf{1}\). Furthermore, since for \(k=0,1\) we can rewrite \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}\) as the orthogonal sum \(\mathbb{R}^{\mathcal{S}\times\mathcal{A}}=\mathcal{U}_{k}\cap\mathbf{1}^{ \perp}\oplus\mathcal{U}_{k}^{\perp}\oplus 1\), we can uniquely decompose \(\mathsf{r}^{\mathsf{E}}-\hat{r}\) into \(r^{\mathsf{E}}-\hat{r}=x_{k}+y_{k}+z\), where \(x_{k}\in\mathcal{U}_{k}\cap\mathbf{1}^{\perp}\), \(y_{k}\in\mathcal{U}_{k}^{\perp}\), \(z\in\mathbf{1}\), for \(k=0,1\). Then, it holds that \(x_{0}+y_{0}=x_{1}+y_{1}\). Since \(\left\|\left[\mathsf{r}^{\mathsf{E}}\right]_{P^{k}}-\left[\hat{r}\right]_{P^{k}} \right\|_{P^{k},2}=\left\|y_{k}\right\|_{2}\), the Assumption of Lemma F.1 impliesthat \(\left\|y_{k}\right\|_{2}\leq\bar{\varepsilon}\). For the \(2\)-distance between the equivalence classes \([r^{\mathsf{E}}]_{\mathbf{1}}\) and \([\hat{r}]_{\mathbf{1}}\) the Pythagorean theorem implies that

\[\left\|[r^{\mathsf{E}}]_{\mathbf{1}}-[\hat{r}]_{\mathbf{1}}\right\|_{\mathbf{1 },2}^{2}=\left\|x_{0}\right\|_{2}^{2}+\left\|y_{0}\right\|^{2}=\left\|x_{1} \right\|_{2}^{2}+\left\|y_{1}\right\|_{2}^{2}\leq\max_{\begin{subarray}{c}u_{k }\in\mathcal{U}_{k}\cap\mathbf{1}^{\perp},v_{k}\in\mathcal{U}_{k}^{\perp},\\ \left\|u_{k}\right\|_{2}=\left\|v_{k}\right\|_{2}=1,\\ \alpha_{k}\in\mathcal{U}_{1},\beta_{k}\in[0,\bar{\varepsilon}],k=0,1,\\ \alpha_{0}u_{0}\in\mathcal{H}_{0}\cap\alpha_{1}u_{1}+\beta_{1}v_{1},\end{subarray}} \alpha_{0}^{2}+\beta_{0}^{2},\] (27)

where the upper bound follows from \(x_{0}+y_{0}=x_{1}+y_{1}\) and \(\left\|y_{k}\right\|_{2}\leq\bar{\varepsilon}\). Next, we want to show that the maximum on the right-hand side of (27) is achieved for \(\beta_{0}=\beta_{1}=\bar{\varepsilon}\). To see this, note that taking inner products between \(u_{0}\) and \(u_{1}\), respectively, and the equation \(\alpha_{0}u_{0}+\beta_{0}v_{0}=\alpha_{1}u_{1}+\beta_{1}v_{1}\), we arrive at

\[\alpha_{0}=\alpha_{1}\langle u_{0},u_{1}\rangle+\beta_{1}\langle u_{0},v_{1} \rangle,\ \alpha_{1}=\alpha_{0}\langle u_{0},u_{1}\rangle+\beta_{0}\langle u_{1},v_{0}\rangle,\]

which is for any choice of \(\beta_{k},u_{k},v_{k},k=0,1\) an invertible linear system of equations for \(\alpha_{0},\alpha_{1}\) with the solutions

\[\alpha_{0}=\frac{\beta_{0}\langle u_{0},u_{1}\rangle\langle u_{1},v_{0} \rangle+\beta_{1}\langle u_{0},v_{1}\rangle}{1-\langle u_{0},u_{1}\rangle^{2} },\ \alpha_{1}=\frac{\beta_{1}\langle u_{1},u_{0}\rangle\langle u_{0},v_{1} \rangle+\beta_{0}\langle u_{1},v_{0}\rangle}{1-\langle u_{1},u_{0}\rangle^{2}}\]

where \(\langle u_{0},u_{1}\rangle<1\), due to \(\mathcal{U}_{0}\cap\mathcal{U}_{1}\cap\mathbf{1}^{\perp}=0\). As the sign of \(\langle u_{0},u_{1}\rangle\langle u_{1},v_{0}\rangle\) and \(\langle u_{0},v_{1}\rangle\) can be chosen arbitrarily by an appropriate choice of \(v_{0},v_{1}\), the objective in the right-hand-side of (27) is increasing in \(\beta_{0},\beta_{1}\) and hence the maximum is achieved for \(\beta_{0}=\beta_{1}=\bar{\varepsilon}\) and \(\alpha:=\alpha_{0}=\alpha_{1}=\frac{\bar{\varepsilon}\langle u_{0},v_{1} \rangle}{1-\langle u_{0},u_{1}\rangle}\). Therefore, it holds that

\[\left\|[r^{\mathsf{E}}]_{\mathbf{1}}-[\hat{r}]_{\mathbf{1}}\right\|_{\mathbf{1 },2}^{2}\leq\max_{\begin{subarray}{c}u_{k}\in\mathcal{U}_{k}\cap\mathbf{1}^{ \perp},v_{1}\in\mathcal{U}_{k}^{\perp},\\ \left\|u_{k}\right\|_{2}=\left\|v_{1}\right\|_{2}=1,k=0,1\end{subarray}}\bar{ \varepsilon}^{2}\left[1+\left(\frac{\langle u_{0},v_{1}\rangle}{1-\langle u _{0},u_{1}\rangle}\right)^{2}\right]\]

\[\stackrel{{(i)}}{{=}}\max_{\begin{subarray}{c}u_{0}\in\mathcal{U }_{0}\cap\mathbf{1}^{\perp},\\ \left\|u_{0}\right\|_{2}=1\end{subarray}}\bar{\varepsilon}^{2}\left[1+\left( \frac{\max_{v_{1}\in\mathcal{U}_{k}^{\perp},\left\|v_{1}\right\|_{2}=1}\langle u _{0},v_{1}\rangle}{1-\max_{u_{1}\in\mathcal{U}_{1}\cap\mathbf{1}^{\perp}, \left\|u_{1}\right\|_{2}=1}\langle u_{0},u_{1}\rangle}\right)^{2}\right]\]

\[\stackrel{{(ii)}}{{=}}\max_{\begin{subarray}{c}u_{0}\in\mathcal{U }_{0}\cap\mathbf{1}^{\perp},\\ \left\|u_{0}\right\|_{2}=1\end{subarray}}\bar{\varepsilon}^{2}\left[1+\left( \frac{\left\|\Pi_{\mathcal{U}_{k}^{\perp}}u_{0}\right\|_{2}^{2}}{1-\left\| \Pi_{\mathcal{U}_{1}\cap\mathbf{1}^{\perp}}u_{0}\right\|_{2}}\right)^{2}\right]\]

\[\stackrel{{(iii)}}{{=}}\max_{\begin{subarray}{c}u_{0}\in\mathcal{U }_{0}\cap\mathbf{1}^{\perp},\\ \left\|u_{0}\right\|_{2}=1\end{subarray}}\bar{\varepsilon}^{2}\left[1+\left( \frac{\sqrt{1-\left\|\Pi_{\mathcal{U}_{1}}u_{0}\right\|_{2}^{2}}}{1-\left\| \Pi_{\mathcal{U}_{1}\cap\mathbf{1}^{\perp}}u_{0}\right\|_{2}}\right)^{2}\right]\]

\[\stackrel{{(iv)}}{{=}}\max_{\begin{subarray}{c}u_{0}\in\mathcal{U }_{0}\cap\mathbf{1}^{\perp},\\ \left\|u_{0}\right\|_{2}=1\end{subarray}}\bar{\varepsilon}^{2}\left[1+\frac{1+ \left\|\Pi_{\mathcal{U}_{1}\cap\mathbf{1}^{\perp}}\cdot u_{0}\right\|_{2}}{1- \left\|\Pi_{\mathcal{U}_{1}\cap\mathbf{1}^{\perp}}\cdot u_{0}\right\|_{2}}\right]\]

\[\stackrel{{(vi)}}{{=}}\bar{\varepsilon}^{2}\frac{2}{\sin\left( \theta_{2}(\mathcal{U}_{0},\mathcal{U}_{1})/2\right)^{2}}.\]

Here, we took the maximum over \(u_{1},v_{1}\) in \((i)\), we used that \(\max_{v\in\mathcal{V},\left\|v\right\|_{2}=1}\langle v,u\rangle=\left\|\Pi_{ \mathcal{V}}u\right\|_{2}\) in \((ii)\), and \((iii)\) follows from the Pythagorean theorem. Furthermore, \((iv)\) follows from \(u_{0}\in\mathbf{1}^{\perp}\) and \((v)\) from simplifying. In \((vi)\) we then again use \(\max_{v\in\mathcal{V},\left\|v\right\|_{2}=1}\langle v,u\rangle=\left\|\Pi_{ \mathcal{V}}u\right\|_{2}\), the definition of the second principal angle (Definition 3.8), and the fact that the first principal vectors lie in \(\mathbf{1}\). Lastly, \((vii)\) follows from simplifying and \((viii)\) from \(\sin(x/2)^{2}=(1-\cos x)/2\)Proof of Theorem 3.10.: As mentioned in the proof sketch in the main paper, it follows from the lower bound in Lemma 3.6 that \(\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P^{k}}}-[\hat{r}]_{\mathcal{U}_{P^{k}}} \right\|_{2}\leq\sqrt{2\hat{\varepsilon}/\sigma_{\mathcal{R}}}\). In light of Lemma F.1, this implies that for any \(P\in\Delta_{\mathcal{S}\times\mathcal{A}}^{\mathcal{S}}\) we have

\[\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P}}-[\hat{r}]_{\mathcal{U}_{P}}\right\|_ {2}\leq\left\|[r^{\mathsf{E}}]_{\mathbf{1}}-[\hat{r}]_{\mathbf{1}}\right\|_{2} \leq\frac{\sqrt{2\hat{\varepsilon}/\sigma_{\mathcal{R}}}}{\sin\left(\theta_{2} (P^{0},P^{1})/2\right)}.\]

Hence, applying the upper bound in Lemma 3.6 yields

\[\mathsf{SubOpt}_{P}(r^{\mathsf{E}},\mathsf{RL}_{P}(\hat{r}))\leq\frac{1}{2 \eta}\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P}}-[\hat{r}]_{\mathcal{U}_{P}} \right\|_{2}^{2}\leq\frac{\hat{\varepsilon}}{\eta\sigma_{\mathcal{R}}\sin \left(\theta_{2}(P^{0},P^{1})/2\right)^{2}}.\]

## Appendix G Proof of Theorem 3.11

**Theorem 3.11**.: _Let \(K=1\), \(D:=\max_{r,r^{\prime}\in\mathcal{R}}\left\|r-r^{\prime}\right\|_{2}\), and suppose that Assumptions 2.1,2.2, and 3.5 hold. If \(\mathsf{SubOpt}_{P^{0}}(\hat{r},\mu^{\mathsf{E}})\leq\hat{\varepsilon}\), then \(\hat{r}\) is \(\varepsilon_{P}\)-transferable to \(P\in\Delta_{\mathcal{S}}^{\mathcal{S}\times\mathcal{A}}\) with_

\[\varepsilon_{P}=2\max\left\{2\hat{\varepsilon}/\sigma_{\mathcal{R}},D^{2}\sin \left(\theta_{\max}(P^{0},P)\right)^{2}\right\}/\eta.\]

Proof.: Similar to Theorem 3.10, it follows from Lemma 3.6 that \(\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P^{0}}}-[\hat{r}]_{\mathcal{U}_{P^{0}}} \right\|_{2}\leq\sqrt{2\hat{\varepsilon}/\sigma_{\mathcal{R}}}\). By Proposition D.11, we then have that

\[\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P}}-[\hat{r}]_{\mathcal{U}_ {P}}\right\|_{2} \leq\sin\left(\theta_{\max}(P,P^{0})\right)\left\|r^{\mathsf{E}}- \hat{r}\right\|_{2}+\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P^{0}}}-[\hat{r}]_{ \mathcal{U}_{P^{0}}}\right\|_{2}\] \[\leq\sin\left(\theta_{\max}(P,P^{0})\right)D+\sqrt{2\hat{ \varepsilon}/\sigma_{\mathcal{R}}}.\]

Hence, applying Lemma 3.6 again yields

\[\mathsf{SubOpt}_{P}\left(r^{\mathsf{E}},\mathsf{RL}_{P}(\hat{r})\right) \leq\frac{1}{2\eta}\left\|[r^{\mathsf{E}}]_{\mathcal{U}_{P}}-[ \hat{r}]_{\mathcal{U}_{P}}\right\|_{2}^{2}\] \[\leq\frac{\left(D\sin\left(\theta_{\max}(P,P^{0})\right)+\sqrt{2 \hat{\varepsilon}/\sigma_{\mathcal{R}}}\right)^{2}}{2\eta}\] \[\leq\frac{2\max\left\{D^{2}\sin\left(\theta_{\max}(P,P^{0}) \right)^{2},2\hat{\varepsilon}/\sigma_{\mathcal{R}}\right\}}{\eta}.\]

## Appendix H Estimating principal angles

Consider two full rank matrices \(A,B\in\mathbb{R}^{n\times m}\) and let the columns of \(U_{A},U_{B}\in\mathbb{R}^{n\times m}\) form an orthonormal basis of \(\mathcal{V}=\operatorname{im}A\) and \(\mathcal{W}=\operatorname{im}B\), respectively. Then, as discussed by [11] we have

\[\sigma_{i}=\cos(\theta_{i}(\mathcal{V},\mathcal{W})),i=1,\ldots,m,\]

where \(1\geq\sigma_{1}\geq\ldots\geq\sigma_{m}\geq 0\) denote the singular values of \(U_{A}^{\top}U_{B}\) sorted in decreasing order. Hence, given the transition matrices \(P^{0},P^{1}\), we can compute the principle angles \(\theta_{i}(P^{0},P^{1})\) by first computing orthonormal bases for the column spans of \(E-\gamma P_{i},i=1,2\), and then computing the singular values as described above.

Now, suppose that \(\hat{P}^{0},\hat{P}^{1}\) are empirical estimates of \(P^{0},P^{1}\), then we have by [11, Theorem 3.1] the following perturbation result

\[\max_{i}\left|\sin(\theta_{i}(P^{0},P^{1}))-\sin(\theta_{i}(\hat{ P}^{0},\hat{P}^{1}))\right| \leq\left\|\Pi_{\mathcal{U}_{P^{0}}}-\Pi_{\mathcal{U}_{P^{0}}} \right\|+\left\|\Pi_{\mathcal{U}_{P^{1}}}-\Pi_{\mathcal{U}_{P^{1}}}\right\|\] \[\leq\gamma H_{\gamma}\sqrt{|\mathcal{S}|/|\mathcal{A}|}\left(\left\| P^{0}-\hat{P}^{0}\right\|+\left\|P^{1}-\hat{P}^{1}\right\|\right),\]

where the last inequality follows from Propositions D.11 and 3.9. Hence, we can estimate \(\sin\theta_{i}(P^{0},P^{1})\) up to an error of \(\mathcal{O}\left(\max\left\{\left\|P^{0}-\hat{P}^{0}\right\|,\left\|P^{1}-\hat{ P}^{1}\right\|\right\}\right)\).

Proof of Theorem 4.1

**Theorem 4.1**.: _Suppose that \(N^{\mathsf{E}}=\Omega\big{(}K\log(|\mathcal{S}||\mathcal{A}|/\hat{\delta})/\hat{ \varepsilon}^{2}\big{)}\) and \(H^{\mathsf{E}}=\Omega\big{(}\log(K/\hat{\varepsilon})/\log(1/\gamma)\big{)}\). Running Algorithm 1 for \(T=\Omega\big{(}K^{2}/\hat{\varepsilon}^{2}\big{)}\) iterations with step-size \(\alpha=1/(K\sqrt{T})\), where \(\delta_{\text{opt}}=\mathcal{O}\big{(}\hat{\delta}\hat{\varepsilon}^{2}/K^{3} \big{)}\), \(\varepsilon_{\text{opt}}=\mathcal{O}(\hat{\varepsilon}/K)\), \(N=\Omega\big{(}K\log(K|\mathcal{S}||\mathcal{A}|/(\hat{\delta}\hat{\varepsilon }))/\hat{\varepsilon}^{2}\big{)}\), and \(H=H^{\mathsf{E}}\), it holds with probability at least \(1-\hat{\delta}\) that \(\mathsf{SubOpt}_{P^{h}}(\hat{r},\mu_{P^{k}}^{\mathsf{E}})\leq\hat{\varepsilon}, \;\text{ for }k=0,\ldots,K-1\)._

The proof of Theorem 4.1 is inspired by [20, Theorem 2]. However, in contrast to Syed and Schapire [2007], we consider the regularized problem with multiple experts, we use the suboptimality as the convergence metric, and we use a projected gradient descent update (instead of multiplicative weights). The proof hinges on Hoeffding's inequality and a regret bound for online gradient descent, which are provided in Theorem I.1 and I.2 below.

**Theorem I.1** (Hoeffding's inequality [16]).: _Let \(X_{0},\ldots,X_{M-1}\) be independent random variables with \(X_{l}\in[a,b]\) and let \(S_{M}:=X_{0}+\ldots+X_{M-1}\). Then,_

\[\Pr\left(|S_{M}-\mathbb{E}S_{M}|\geq c\right)\leq 2\exp\left(-\frac{2c^{2}}{M( b-a)^{2}}\right).\]

**Theorem I.2** (Online gradient descent [20]).: _Consider some bounded closed convex set \(\mathcal{X}\subset\mathbb{R}^{n}\) with \(D:=\max_{x,x^{\prime}\in\mathcal{X}}\left\|x-x^{\prime}\right\|_{2}\). Moreover, let \(\Pi_{\mathcal{X}}:\mathbb{R}^{n}\to\mathcal{X}\) be the orthogonal projection onto \(\mathcal{X}\). For any sequence of convex differentiable functions \(f_{0},\ldots,f_{T-1}:\mathcal{X}\to\mathbb{R}\) satisfying \(\max_{x\in\mathcal{X}}\left\|\nabla f_{t}(x)\right\|_{2}\leq G\), the online projected gradient descent update_

\[x_{t+1}\leftarrow\Pi_{\mathcal{X}}\left(x_{t}-\alpha\nabla f_{t}(x_{t})\right),\]

_with step-size \(\alpha=D/(G\sqrt{T})\) satisfies_

\[\sum_{t=0}^{T-1}f_{t}(x_{t})-\min_{x^{\prime}\in\mathcal{X}}\sum_{t=0}^{T-1}f_ {t}(x^{*})\leq DG\sqrt{T}.\]

Proof of Theorem 4.1.: The proof is in three steps. First, we use Hoeffding's inequality to prove concentration of the empirical occupancy measures around the true occupancy measures. Then, we use the union bound to upper bound the probability that any of our bounds fails to hold. Finally, we prove the convergence rate of Algorithm 1 using the regret bound in Theorem I.2.

_Step 1:_ Let \(\mathcal{D}=\left\{(s_{0},a_{0},\ldots,s_{H-1},a_{H-1})\right\}_{i=0}^{N-1}\) be sampled from some policy \(\pi^{\mu}\) and recall that the corresponding empirical occupancy measure is defined as

\[\hat{\mu}_{\mathcal{D}}(s,a)=\frac{1-\gamma}{N}\sum_{i=0}^{N-1}\sum_{t=0}^{H-1 }\gamma^{t}\mathbbm{1}\{s_{t}^{i}=s,a_{t}^{i}=a\}.\]

It will be convenient to define the truncated occupancy measure

\[\mu_{H}(s,a)=(1-\gamma)\sum_{t=0}^{H-1}\gamma^{t}\mathbbm{P}_{\nu_{0}}^{\pi^{ \mu}}\{s_{t}^{i}=s,a_{t}^{i}=a\}.\]

For \(K\) data sets \(\mathcal{D}_{1},\ldots,\mathcal{D}_{K}\) sampled from \(\pi^{\mu_{k}}\) we then have

\[\max_{r\in\mathcal{R}}\sum_{k=0}^{K-1}\langle r,\mu_{k}-\hat{\mu} _{\mathcal{D}_{k}}\rangle \stackrel{{(i)}}{{\leq}} \max_{r\in\mathcal{R}}\left\|r\right\|_{1}\left\|\sum_{k=0}^{K-1 }\left(\mu_{k}-\hat{\mu}_{\mathcal{D}_{k}}\right)\right\|_{\infty}\stackrel{{ (ii)}}{{\leq}}\left\|\sum_{k=0}^{K-1}\left(\mu_{k}-\hat{\mu}_{ \mathcal{D}_{k}}\right)\right\|_{\infty}\] \[\stackrel{{(iii)}}{{\leq}} \underbrace{\left\|\sum_{k=0}^{K-1}\left(\mu_{k}-\mu_{H,k} \right)\right\|_{\infty}}_{I_{1}}+\underbrace{\left\|\sum_{k=0}^{K-1}\left(\mu _{H,k}-\hat{\mu}_{\mathcal{D}_{k}}\right)\right\|_{\infty}}_{I_{2}},\]

where \((i)\) follows from Holder's inequality, \((ii)\) from our definition of \(\mathcal{R}\) as the 1-norm ball, and \((iii)\) from the triangle inequality. Since \(\left\|\mu-\mu_{H}\right\|_{\infty}\leq\gamma^{H}\), we have \(I_{1}\leq\gamma^{H}K\). Moreover, applying Hoeffding's inequality to the \(M=KN\) independent random variables

\[X_{kN+i}=\frac{1-\gamma}{N}\sum_{t=0}^{H-1}\gamma^{t}\mathbbm{1}\{s_{t}^{k,i}=s,a_{t}^{k,i}=a\},\;i\in[N],k\in[K],\]with \(X_{i}\in[0,1/N]\), we arrive at

\[\Pr\left(|S_{M}-\mathbb{E}S_{M}|\geq\varepsilon_{\text{stat}}/2\right)=\Pr\left( \left|\sum_{k=0}^{K-1}\hat{\mu}_{\mathcal{D}_{k}}(s,a)-\mu_{K,k}(s,a)\right| \geq\varepsilon_{\text{stat}}/2\right)\leq 2\exp\left(-\frac{\varepsilon_{\text{stat}}^{2}N} {2K}\right).\]

Hence, applying the union bound over all \(|\mathcal{S}||\mathcal{A}|\) components of the occupancy measure yields

\[\Pr(I_{2}<\varepsilon_{\text{stat}}/2)=1-\Pr(I_{2}\geq\varepsilon_{\text{stat }}/2)\geq 1-2|\mathcal{S}||\mathcal{A}|\exp\left(-\frac{\varepsilon_{\text{ stat}}^{2}N}{2K}\right).\]

Therefore, to ensure that with probability at least \(1-\delta_{\text{stat}}\) it holds that

\[\max_{r\in\mathcal{R}}\sum_{k=0}^{K-1}\left\langle r,\mu_{k}-\hat{\mu}_{ \mathcal{D}_{k}}\right\rangle\leq\varepsilon_{\text{stat}},\]

it suffices to choose

\[N\geq\frac{2K\log\left(2|\mathcal{S}||\mathcal{A}|/\delta_{\text{stat}}\right) }{\varepsilon_{\text{stat}}^{2}}\quad\text{and}\quad H\geq\frac{\log\left(2K /\varepsilon_{\text{stat}}\right)}{\log(1/\gamma)}.\]

This concentration result applies to both empirical occupancy measures generated from the expert data sets \(\mathcal{D}_{k}^{\mathsf{E}}\), as well as the data sets \(\mathcal{D}_{k,t}\) generated by Algorithm 1.

_Step 2:_ When analyzing Algorithm 1 there are three sources of stochasticity. The first two are due to the randomness in the data sets \(\mathcal{D}_{k}^{\mathsf{E}}\) and \(\mathcal{D}_{k,t}\), and the third is due to the randomness in the forward RL algorithm, \(\mathsf{A}_{p_{k}}^{\varepsilon_{\text{stat}},\delta_{\text{stat}}}\), that upon a query with the reward \(r_{t}\) outputs a policy \(\pi_{k,t}\) such that with probability at least \(1-\delta_{\text{opt}}\) it holds \(\mathsf{SubOpt}_{p^{k}}(r_{t},\mu^{\pi_{k,t}})\leq\varepsilon_{\text{opt}}\). Let's denote the event that \(\max_{r\in\mathcal{R}}\sum_{k=0}^{K-1}\left\langle r,\mu^{\mathsf{E}}_{P^{k}} -\hat{\mu}_{\mathcal{D}_{k}^{\mathsf{E}}}\right\rangle>\varepsilon_{\text{stat }}\) by \(\mathcal{E}_{\text{stat},t}\), the event that \(\max_{r\in\mathcal{R}}\sum_{k=0}^{K-1}\left\langle r,\mu^{\pi_{k,t}}-\hat{\mu} _{\mathcal{D}_{k,t}}\right\rangle>\varepsilon_{\text{stat}}\) by \(\mathcal{E}_{\text{stat},t}\). Moreover, let assume that \(\mathcal{E}_{\text{stat},\mathsf{E}}\) happens with probability at most \(\delta_{\text{stat},\mathsf{E}}\), \(\mathcal{E}_{\text{stat},t}\) happens with probability at most \(\delta_{\text{stat}}\), and \(\mathcal{E}_{\text{opt},k,t}\) happens with probability at most \(\delta_{\text{opt}}\). By union bound, the probability of the event

\[\mathcal{F}:=-\mathcal{E}_{\text{stat},\mathsf{E}}\wedge\bigwedge_{t=0}^{T-1}- \mathcal{E}_{\text{stat},t}\wedge\bigwedge_{t=0}^{T-1}\bigwedge_{k=0}^{K-1}- \mathcal{E}_{\text{opt},k,t},\]

that none of the above events happens is lower bounded by

\[\Pr\left(\mathcal{F}\right) =1-\Pr\left(\mathcal{E}_{\text{stat},\mathsf{E}}\vee\bigvee_{t=0} ^{T-1}\mathcal{E}_{\text{stat},t}\vee\bigvee_{t=0}^{T-1}\bigvee_{k=0}^{K-1} \mathcal{E}_{\text{opt},k,t}\right)\] \[\geq 1-\left(\Pr\left(\mathcal{E}_{\text{stat},\mathsf{E}}\right) +\sum_{t=0}^{T-1}\Pr\left(\mathcal{E}_{\text{stat},t}\right)+\sum_{t=0}^{T-1} \sum_{k=0}^{K-1}\Pr\left(\mathcal{E}_{\text{opt},k,t}\right)\right)\] \[\geq 1-\left(\delta_{\text{stat},\mathsf{E}}+T\delta_{\text{stat}}+ KT\delta_{\text{opt}}\right).\]

Hence, to ensure that \(\mathcal{F}\) happens with probability at least \(1-\hat{\delta}\), it suffices to choose

\[N \geq\frac{2K\log\left(6|\mathcal{S}||\mathcal{A}|/\hat{\delta} \right)}{\varepsilon_{\text{stat},\mathsf{E}}^{2}}\quad\text{and}\quad H\geq \frac{\log\left(2K/\varepsilon_{\text{stat},\mathsf{E}}\right)}{\log(1/ \gamma)},\] \[N_{t} \geq\frac{2K\log\left(6T|\mathcal{S}||\mathcal{A}|/\hat{\delta} \right)}{\varepsilon_{\text{stat}}^{2}}\quad\text{and}\quad\delta_{\text{ opt}}=\frac{\delta}{3KT}.\]

_Step 3:_ Note that we can bound \(\left\|g_{t}\right\|_{2}\leq\left\|g_{t}\right\|_{1}\leq\sum_{k=0}^{K-1} \left\|\hat{\mu}_{\mathcal{D}_{k}^{\mathsf{E}}}\right\|_{1}+\left\|\hat{\mu}_{k,t}\right\|_{1}\leq 2K=:G\) and the diameter of \(\mathcal{R}\) is \(D=2\). Hence, given that event \(\mathcal{F}\) happens, we can bound the suboptimalities of the \(K\) experts under the reward, \(\hat{r}\), recovered by Algorithm 1 with stepsize \(\alpha=D/(G\sqrt{T})\) as follows

\[\sum_{k=0}^{K-1}\mathsf{SubOpt}_{P^{k}}(\hat{r},\mu_{P^{k}}^{\mathsf{ E}})\] \[=\sum_{k=0}^{K-1}\left[\max_{\mu\in\mathcal{M}_{P^{k}}}\langle\hat {r},\mu-\mu_{P^{k}}^{\mathsf{E}}\rangle-\bar{h}(\mu)+\bar{h}(\mu_{P^{k}}^{ \mathsf{E}})\right]\] \[\stackrel{{(i)}}{{\leq}} \varepsilon_{\text{stat,\mathsf{E}}}+\sum_{k=0}^{K-1}\left[\max_{ \mu\in\mathcal{M}_{P^{k}}}\langle\hat{r},\mu-\hat{\mu}_{P^{k}_{k}}\rangle- \bar{h}(\mu)+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]\] \[\stackrel{{(ii)}}{{\leq}} \varepsilon_{\text{stat,\mathsf{E}}}+\sum_{k=0}^{K-1}\frac{1}{T} \sum_{t=0}^{T-1}\left[\max_{\mu\in\mathcal{M}_{P^{k}}}\langle r_{t},\mu-\hat{ \mu}_{P^{k}_{k}}\rangle-\bar{h}(\mu)+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]\] \[= \varepsilon_{\text{stat,\mathsf{E}}}+\frac{1}{T}\sum_{t=0}^{T-1} \sum_{k=0}^{K-1}\left[\max_{\mu\in\mathcal{M}_{P^{k}}}\langle r_{t},\mu-\hat{ \mu}_{P^{k}_{k}}\rangle-\bar{h}(\mu)+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]\] \[\stackrel{{(iii)}}{{\leq}} \varepsilon_{\text{stat,\mathsf{E}}}+K\varepsilon_{\text{opt}}+ \frac{1}{T}\sum_{t=0}^{T-1}\sum_{k=0}^{K-1}\left[\langle r_{t},\mu_{k,t}-\hat{ \mu}_{P^{k}_{k}}\rangle-\bar{h}(\mu_{k,t})+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]\] \[\stackrel{{(iv)}}{{\leq}} \varepsilon_{\text{stat,\mathsf{E}}}+K\varepsilon_{\text{opt}}+ \varepsilon_{\text{stat}}+\frac{1}{T}\sum_{t=0}^{T-1}\sum_{k=0}^{K-1}\left[ \langle r_{t},\hat{\mu}_{\mathcal{D}_{R,t}}-\hat{\mu}_{P^{k}_{k}}\rangle-\bar {h}(\mu_{k,t})+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]\] \[\stackrel{{(v)}}{{\leq}} \varepsilon_{\text{stat,\mathsf{E}}}+K\varepsilon_{\text{opt}}+ \varepsilon_{\text{stat}}+\frac{DG}{\sqrt{T}}+\min_{r\in\mathcal{R}}\frac{1}{T} \sum_{t=0}^{T-1}\sum_{k=0}^{K-1}\left[\langle r,\hat{\mu}_{\mathcal{D}_{R,t}}- \hat{\mu}_{P^{k}_{k}}\rangle-\bar{h}(\mu_{k,t})+\bar{h}(\mu_{P^{k}}^{\mathsf{ E}})\right]\] \[\stackrel{{(vi)}}{{\leq}} 2\varepsilon_{\text{stat,\mathsf{E}}}+K\varepsilon_{\text{opt}}+ 2\varepsilon_{\text{stat}}+\frac{DG}{\sqrt{T}}+\min_{r\in\mathcal{R}}\frac{1}{ T}\sum_{t=0}^{T-1}\sum_{k=0}^{K-1}\left[\langle r,\mu_{k,t}-\mu_{P^{k}}^{\mathsf{ E}}\rangle-\bar{h}(\mu_{k,t})+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]\] \[\stackrel{{(vii)}}{{\leq}} 2\varepsilon_{\text{stat,\mathsf{E}}}+K\varepsilon_{\text{opt}}+ 2\varepsilon_{\text{stat}}+\frac{DG}{\sqrt{T}}\] \[\qquad\qquad+\underbrace{\min_{r\in\mathcal{R}}\sum_{k=0}^{K-1} \left[\langle r,\bar{\mu}_{k}-\mu_{P^{k}}^{\mathsf{E}}\rangle-\bar{h}(\bar{\mu }_{k})+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\right]}_{\leq 0},\quad\text{with }\bar{\mu}_{k}:= \frac{1}{T}\sum_{t=0}^{T-1}\mu_{k,t},\] \[\stackrel{{(viii)}}{{\leq}} 2\varepsilon_{\text{stat,\mathsf{E}}}+K\varepsilon_{\text{opt}}+2 \varepsilon_{\text{stat}}+\frac{DG}{\sqrt{T}}=2\varepsilon_{\text{stat,\mathsf{E} }}+K\varepsilon_{\text{opt}}+2\varepsilon_{\text{stat}}+\frac{4K}{\sqrt{T}}.\]

Here, the inequalities \((i),(iv)\), and \((vi)\) follow from the concentration bound established in step 1. Moreover, inequality \((ii)\) holds since \(\hat{r}\mapsto\max_{\mu\in\mathcal{M}_{P^{k}}}\langle\hat{r},\mu-\mu_{P^{k}}^{ \mathsf{E}}\rangle-\bar{h}(\mu)+\bar{h}(\mu_{P^{k}}^{\mathsf{E}})\) is the pointwise maximum of affine functions and therefore convex. Furthermore, \((iii)\) follows from \(\varepsilon_{\text{opt}}\)-optimality of \(\mu_{k,t}\), \((v)\) from Theorem I.2, and \((vii)\) from concavity of the mapping \(\mu_{k,t}\mapsto\langle r,\mu_{k,t}-\mu_{P^{k}}^{\mathsf{E}}\rangle-\bar{h}(\mu _{k,t})\). Finally, \((viii)\) holds because all experts are optimal for the reward \(r^{\mathsf{E}}\). In conclusion, to ensure that with probability at least \(1-\hat{\delta}\) it holds that \(\mathsf{SubOpt}_{P^{k}}(\hat{r},\mu_{P^{k}}^{\mathsf{E}})\leq\sum_{k=0}^{K-1} \mathsf{SubOpt}_{P^{k}}(\hat{r},\mu_{P^{k}}^{\mathsf{E}})\leq\hat{\varepsilon}\) it suffices to choose \(T=\frac{256K^{2}}{\hat{\varepsilon}^{2}}\), \(\alpha=\frac{\hat{\varepsilon}}{16K^{2}}\), \(N=\frac{128K\log\left(6|\mathcal{S}||\mathcal{A}|/\delta\right)}{\hat{\varepsilon}^{ 2}}\), \(H=H_{t}=\frac{\log(16K/\hat{\varepsilon})}{\log(1/\gamma)}\), \(N_{t}=\frac{128K\log\left(1536K^{2}|\mathcal{S}||\mathcal{A}|/(\hat{\delta} \hat{\varepsilon}^{2})\right)}{\hat{\varepsilon}^{2}}\), \(\delta_{\text{opt}}=\frac{\hat{\delta}\hat{\varepsilon}^{2}}{768K^{3}}\), \(\varepsilon_{\text{opt}}=\frac{\hat{\varepsilon}}{4K}\). 

## Appendix J Suboptimal experts

In our problem formulation, we assumed that the \(K\) experts are optimal with respect to \(r^{\mathsf{E}}\), i.e. \(\mu_{P^{k}}^{\mathsf{E}}=\mathsf{RL}_{P^{k}}(r^{\mathsf{E}})\) for \(k=0,\ldots,K-1\). This assumption can be weakened by requiring that

\[\max_{r\in\mathcal{R}}\left|J(r,\mu_{P^{k}}^{\mathsf{E}})-J(r,\mathsf{RL}_{P^{k}}( r^{\mathsf{E}}))\right|\leq\varepsilon_{\text{mis}},\]where \(\varepsilon_{\text{mis}}\) is some misspecification error. The transferability results in Theorem 3.10 and 3.11 still apply whenever we recover a reward \(\hat{r}\) such that \(\mathsf{SubOpt}_{\text{$P^{k}$}}(\hat{r},\mathsf{RL}_{\text{$P^{k}$}}(r^{ \mathsf{E}}))\leq\hat{\varepsilon}\). Moreover, with a straightforward modification of the proof of Theorem 4.1, it follows that with high probability Algorithm 1 recovers a reward \(\hat{r}\) such that \(\mathsf{SubOpt}_{\text{$P^{k}$}}(\hat{r},\mathsf{RL}_{\text{$P^{k}$}}(r^{ \mathsf{E}}))\leq\hat{\varepsilon}+2K\varepsilon_{\text{mis}}\). Hence, our end-to-end transferability guarantees apply with \(\hat{\varepsilon}\leftarrow\hat{\varepsilon}+2K\varepsilon_{\text{mis}}\). However, \(\varepsilon_{\text{mis}}\) cannot be further reduced by collecting more samples from the expert or MDP.

## Appendix K Experimental details

SetupTo validate our results experimentally, we are using a stochastic adaption of the WindyGridworld environment [21].4 In particular, we consider a 6x6 grid with 4 actions (Up, Down, Left, Right), a wind direction (North, East, South, West), and a wind strength \(\beta\in[0,1]\). When the agent takes an action, with probability \((1-\beta)\), it moves to the intended grid cell, and with probability \(\beta\), the wind pushes the agent one step further in the direction of the wind. This means that the transition law is a convex combination of two laws: \((1-\beta)P^{\text{Gridworld}}+\beta P^{\text{Wind}}\), where \(P^{\text{Gridworld}}\) and \(P^{\text{Wind}}\) represent the transition laws for a deterministic Gridworld and a deterministic WindyGridworld. For our experiments, we then consider the pairs of expert transition laws \(P^{0}_{\beta}=(1-\beta)P^{\text{Gridworld}}+\beta P^{\text{North}}\) and \(P^{1}_{\beta}=(1-\beta)P^{\text{Gridworld}}+\beta P^{\text{East}}\) with \(\beta\) in \(\{0.01,0.1,0.5,1.0\}\). As shown in Figure 4(a), the second principal angle between \(P^{0}_{\beta}\) and \(P^{1}_{\beta}\), calculated using a singular value decomposition [17], increases as the wind strength \(\beta\) increases.

Footnote 4: All our experiments were carried out  within a day  on a MacBook Pro with an Apple M1 Pro chip and 32 GB of RAM.

Inverse reinforcement learningWe observed that under a small second principal angle, the recovered reward heavily depends on both the expert reward and the reward initialization. Hence, we sample 10 independent expert rewards, each generated by first sampling a random set of 10 state-action pairs and then randomly assigning a reward of \(\pm 1\). Using Shannon entropy regularization with \(\tau=0.3\), we then use soft policy iteration to get expert policies for each combination of expert reward and wind strength \(\beta\). For each of these expert policies, we then generate expert data sets with \(N^{\mathsf{E}}\in\{10^{3},10^{4},10^{5},10^{6}\}\) trajectories of length \(H=100\). Next, we run Algorithm 1, with soft policy iteration as a subroutine, for \(30^{\prime}000\) iterations, where rewards are initialized by sampling from a standard normal distribution. As a reward class, we choose the \(\left\lVert\cdot\right\rVert_{1}\)-ball with radius \(10^{3}\) (essentially unbounded), as a stepsize \(\alpha=0.05\) for the first \(15^{\prime}000\) iterations and \(\alpha=0.005\) for the second half. Moreover, we sample \(N=100\) new trajectories of horizon \(H=100\) at each gradient step. Figure 4(b) illustrates the distances between the recovered \(\hat{r}\) and the experts' reward \(r^{\mathsf{E}}\), measured in \(\mathbb{R}^{S\times\mathcal{A}}/\mathbf{1}\). It is evident that the recovered reward gets closer to the experts' reward as the number of expert demonstrations increases. Moreover, we observe that the recovered reward is closer to the experts' reward when the second principal angle between the experts is larger, as expected from Lemma F.1.

TransferabilityWe evaluate the transferability of the obtained reward by considering two new environments. First, a south wind setting \(P^{\text{South}}\) with wind strength \(\beta=1\), and second, a deterministic gridworld \(P^{\text{Shifted}}\), with cyclically shifted actions, i.e., Right\(\rightarrow\) Down, Up\(\rightarrow\) Right, Left\(\rightarrow\) Up, Down\(\rightarrow\) Left. In Figure 4(c) and (d), we illustrate the transferability in terms of \(\mathsf{SubOpt}_{\text{$P^{\text{South}}$}}(r^{\mathsf{E}},\mathsf{RL}_{\text{$P^{ \text{South}}$}}(\hat{r}))\) and \(\mathsf{SubOpt}_{\text{$P^{\text{Shifted}}$}}(r^{\mathsf{E}},\mathsf{RL}_{ \text{$P^{\text{Shifted}}$}}(\hat{r}))\), respectively. We observe that for both environments the transferability improves with a larger second principal angle, thus confirming our theoretical result in Theorem 3.10. The effect is even more pronounced for the shifted environment. While confirming our results, the experiments also reveal a high sample complexity in terms of expert demonstrations. This is to be expected, as IRL aims to match the expert's empirical occupancy measure, leading to overfitting when there are not enough demonstrations [14]. This issue can be mitigated by reducing the dimension of the reward class (see e.g. [1]).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: As mentioned in the abstract and introduction, our main contributions are sufficient conditions for learning transferable rewards, as given in Theorem 3.10 and 3.11. Furthermore, we also provide an algorithm and experiments in Section 4 and 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in the Limitations and future work paragraph in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Yes, all assumptions are stated in the main paper, and proofs are provided in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All details about our experiments are provided in Appendix K. Furthermore, we also provide the code attached as a.zip file. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All code (for experiments and computations in Example 3.3) is attached as a.zip file. Furthermore, we provide a README.md with instructions how to run the code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, we discuss the chosen stepsizes and parameters in Appendix K.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We repeated our experiments over 10 random seeds and provide quantile plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, the hardware details are specified as a footnote in Appendix K. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [NA]Justification: This work is purely theoretical and did not cause any harm to society. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This work is purely theoretical. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We don't work with any potentially harmful models or datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The experiments are based on our own codebase. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide our code as a.zip file. We will make it openly available on github later. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This research does not involve any human experts but only synthetically generated ones. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No humans are involved in this research. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.