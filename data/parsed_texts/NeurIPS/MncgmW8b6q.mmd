# Unsupervised Discovery of Formulas for Mathematical Constants

Michael Shalyt

Uri Seligmann

Equal contribution.

Rotem Elimelech

Ido Kaminer

Technion - Israel Institute of Technology, Haifa 3200003, Israel

shalyt@technion.ac.il, uri.seligmann@gmail.com

itaybe@campus.technion.ac.il, eofirdavid@gmail.com

rotem.eli@campus.technion.ac.il, kaminer@technion.ac.il

Iday Beit Halachmi

Ofir David

Rotem Elimelech

Ido Kaminer

Technion - Israel Institute of Technology, Haifa 3200003, Israel

shalyt@technion.ac.il, uri.seligmann@gmail.com

itaybe@campus.technion.ac.il, eofirdavid@gmail.com

rotem.eli@campus.technion.ac.il, kaminer@technion.ac.il

###### Abstract

Ongoing efforts that span over decades show a rise of AI methods for accelerating scientific discovery [11, 12, 13, 14], yet accelerating discovery in mathematics remains a persistent challenge for AI. Specifically, AI methods were not effective in creation of formulas for mathematical constants because each such formula must be correct for infinite digits of precision, with "near-true" formulas providing no insight toward the correct ones. Consequently, formula discovery lacks a clear distance metric needed to guide automated discovery in this realm. In this work, we propose a systematic methodology for categorization, characterization, and pattern identification of such formulas. The key to our methodology is introducing metrics based on the convergence dynamics of the formulas, rather than on the numerical value of the formula. These metrics enable the first automated clustering of mathematical formulas. We demonstrate this methodology on Polynomial Continued Fraction formulas, which are ubiquitous in their intrinsic connections to mathematical constants [1, 15, 16], and generalize many mathematical functions and structures. We test our methodology on a set of 1,768,900 such formulas, identifying many known formulas for mathematical constants, and discover previously unknown formulas for \(\pi\), \(\ln(2)\), Gauss', and Lemniscate's constants. The uncovered patterns enable a direct generalization of individual formulas to infinite families, unveiling rich mathematical structures. This success paves the way towards a generative model that creates formulas fulfilling specified mathematical properties, accelerating the rate of discovery of useful formulas.

## 2 Introduction

Historically, formulas of mathematical constants were a symbol of aesthetics and beauty. Continued fraction formulas such as those for the golden ratio \(\phi\) and for \(\tan(x)\)

\[1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\frac{1}{1+\cdots}}}}}=\phi\quad \frac{x}{1-\frac{x^{2}}{3-\frac{x^{2}}{5-\cdots}}}=\tan(x)\] (1)

enable calculating infinitely many digits for these constants. Discovering such formulas often leads to profound revelations regarding the properties and underlying structure of fundamental constants. For example, the continued fraction formula for \(\tan(x)\), shown in Eq. 1, was used by Johann Heinrich Lambert in the first proof of the irrationality of Pi [1][1]. Unfortunately, such formulas are notoriously hard to find on-demand, often relying on a mathematician's profound intuition. Part of the challenge is the lack of a well-defined 'distance', or ametric, between a formula and a given constant. i.e., there is no known way to tell whether a formula is nearly accurate. The formula either works, or it does not. In other fields of science, a prediction accurate to 1000 digits is precise enough for any practical need. However, in mathematics, if the 1001st digit is wrong, the formula is incorrect and gives no insight regarding a correct formula. This lack of a metric is a substantial hurdle both for human efforts and for automated analysis, as many methods for optimization such as gradient descent become unsuitable.

Recent efforts developed computer algorithms to discover a multitude of formula hypotheses for mathematical constants (Raayoni et al., 2021), even implementing the first large-scale distributed computation for such discoveries (Eilmelech et al., 2023), but they relied mostly on exhaustive search methods. These approaches complements earlier applications of algorithms for automated theorem proving (ATP) (such as computer proofs of hypergeometric identities (Petkovsek et al., 1996), Malarea (Urban, 2007), and Flyspeck (Kaliszyk and Urban, 2012)), and automated conjecture generation (ACG) (such as mechanical mathematics (Wang, 1960), the Automated Mathematician (Lenat, 1982), EURISKO (Lenat and Brown, 1984, Davis and Lenat, 1982), and Graffiti (Fajtlowicz, 1988)).

Here we propose a fundamentally new methodology for automated investigation and discovery of formulas for mathematical constants. We constructed a large dataset of continued fractions, and enriched it with metrics based on their convergence dynamics, which are found to embody fundamental information about each continued fraction. These dynamical metrics enable the identification and generalization of patterns within the dataset. Using the metrics, we develop a process of categorization and clustering (Fig. 1) of continued fractions that share similar values of their dynamical metrics. Analyzing each automatically identified cluster of formulas, we find that all its members often relate to the same mathematical constant, showing the value of the dynamical metrics for the discovery of new formulas and the internal structure of families of such formulas. This novel method of formula discovery allowed us to identify both previously known and completely new formulas for constants such as \(\pi\), \(\ln(2)\), \(\cot(1)\), the golden ratio, square roots of multiple integers, the Gauss constant, and the Lemniscate constant.

Figure 1: **Systematic clustering and labeling of formulas by dynamical metrics.** Our methodology analyzes Polynomial Continued Fractions (PCFs) in two main stages. **Clustering**: (a) Filter degenerate PCFs. (b) Evaluate PCFs and extract their dynamics-based metrics (section 3). (c) Choose the best few metrics and use them to cluster the data. **Labeling**: In every cluster, look for PCFs known in the literature and use them as anchors. (d) If anchors are found in the cluster, validate that they do not contradict, i.e., relate to different constants. (d.1) If all anchors are in agreement, choose a random subset of other points in the cluster and use PSLQ to validate that they also relate to the same constant. If the validation is successful, the cluster is labeled. If not, the cluster should be split. (d.2) If the anchors relate to different constants, the cluster should be split – return to step c for finer clustering of the data. When focusing on a specific cluster, the best metrics could be different than those for the full dataset. (e) If no anchor is found in a certain cluster, attempt to label by (e.1) choosing a small subset of PCFs in the cluster and running a PSLQ search for each of them against a large set of potential constants. If a connection is found, the cluster now has an anchor – return to step d. (e.2) If an anchor is still not found, attempt to connect a sample of data points within the cluster using PSLQ. If successful, conclude that the cluster is correct, but has no identified constant. Define a new label for that cluster. If PSLQ failed to connect points within the cluster, return to step c for finer clustering. If no further refinement is appropriate, flag the cluster for further analytical investigation.

As part of our analysis of metrics of continued fractions, we developed and applied the most complete classification of polynomial continued fractions known to date, detailed in Appendix B. This classification includes the prediction of whether the continued fraction converges directly based on its defining polynomials.

Traditional clustering methods attempt to relate data points by calculating distance metrics based on the parameters of these data points, e.g., the coefficients of the defining polynomials. The most common approaches (like SVM) rely on linear classification, while more advanced methods rely on non-linear kernel transformations - but usually use various functions calculated directly on the data parameters. In our dataset, each point is a continued fraction formula defined by the polynomials used to construct it. However, we find that it is not sufficient to use the parameters of the polynomials, and not even the numerical limit of the continued fraction. Instead, we find that it is the _dynamics_ of the continued fraction generated by these polynomials, rather than any direct function on their coefficients, which provides the most useful metrics for analysis. In other words, we find that the useful underlying metrics to extract from each data point are embedded within the intricate progression of the sequence created by the formula, rather than the explicit numerical value (limit) of that formula, or the coefficients defining it. Thus, in order to assess the distance between two polynomial continued fractions, and identify relations between such formulas, it is imperative to characterize the nuanced behavior of their sequences, analyzing trends in the convergence process of these sequences, spanning over numerous terms.

Some of the metrics we extract, such as the irrationality measure, are well-known in the mathematical community, yet were never considered for a large-scale classification effort. The evaluation of the irrationality measure is technically challenging for formulas whose limit is not known in advance (which is the vast majority). This challenge made it impossible to extract the irrationality of formulas for a large dataset. Consequently, we develop a new algorithm - the Blind-\(\delta\) algorithm (Section 3.4) - to enable the extraction of the irrationality measure of a continued fraction without prior knowledge of its limit. This algorithm allowed us to extract the irrationality measure for the entire dataset.

These advances provide the building blocks for our novel methodology for formula discovery. We cluster formulas by their 'closeness' to other formulas according to these new metrics, thus identifying promising formulas regardless of their numerical value (Fig. 1left). Once a candidate formula is found, we numerically validate it by calculating its value to a large precision and then identifying its relation to a mathematical constant. The "generate \(\Rightarrow\) validate" approach is inspired by works in AI-driven code generation (Ridnik et al., 2024) and problem solving in geometry (Trinh et al., 2024).

## 3 Methodology for Data-Driven Discovery

### Definitions

#### Polynomial Continued Fractions

In this work we chose to focus on polynomial continued fraction (PCF) formulas as our test case due to the combination of their simplicity and expressive power. PCFs relate to a wide range of mathematical fields, represent a variety of constants, are equivalent to infinite sums (Euler, 1748), and cover mathematical functions such as Bessel functions, trigonometric functions, integral families, widely used Taylor series, and generalized hypergeometric functions (Cuyt et al., 2008). Thus, studying PCFs can provide insight into a plethora of mathematical objects and applications.

A PCF at depth \(n\) is defined as:

\[a_{0}+\frac{b_{1}}{a_{1}+\frac{b_{2}}{\ddots+\frac{b_{n}}{a_{n}}}}=\frac{p_{n }}{q_{n}},\] (2)

where \(a_{n}=a(n)\) and \(b_{n}=b(n)\) are evaluations of polynomials with integer coefficients. The PCF value is the limit \(L=\lim\limits_{n\rightarrow\infty}\frac{p_{n}}{q_{n}}\) (when it exists). The converging sequence of rational numbers \(\frac{p_{n}}{q_{n}}\) provides an approximation of \(L\), which is known as a Diophantine approximation.

_The Irrationality Measure of a Number_While irrational numbers cannot be expressed using a simple quotient of integers, they can be approximated by them. Moreover, some approximations are "better" than others, and one way to evaluate their quality is by a quantity called the irrationality measure (Hardy et al., 1979).

We define the _irrationality measure of a sequence_\(\frac{p_{n}}{q_{n}}\to L\) as the limit \(\delta_{n}\rightarrow\delta\), with

\[\delta_{n}=\frac{-\log\left|L-\frac{p_{n}}{q_{n}}\right|}{\log|\tilde{q}_{n}|}- 1\quad,\quad\tilde{q}_{n}=\frac{q_{n}}{\gcd(p_{n},q_{n})}\] (3)

For every \(L\in\mathbb{R}\), the _irrationality measure of \(L\)_ is defined as the supremum of all possible \(\delta\) for which there is a sequence of distinct rational numbers \(\frac{p_{n}}{q_{n}}\to L\); \(\frac{p_{n}}{q_{n}}\neq L\) that satisfies

\[\left|L-\frac{p_{n}}{q_{n}}\right|<\frac{1}{q_{n}^{1+\delta}}.\] (4)

It is known that for irrational numbers this measure is \(\geq 1\) (Dirichlet theorem for Diophantine approximations), and for rationals it is \(0\).

Note that the irrationality measure of \(L\) is greater or equal to the irrationality measure of any specific sequence converging to the same \(L\). While the irrationality measure of a sequence can be any number \(\geq\) -1, the irrationality measure of its limit \(L\) is always either 0 or \(\geq\) 1 (Church, 2019).

### \(\delta\)-Predictor Formula

The classification of a large number of continued fraction formulas requires an efficient and accurate calculation of the irrationality measure \(\delta\) for each formula. This calculation is challenging because it depends on the asymptotic behavior of the converging sequence, and because \(\delta\) appears as an exponent of a large basis number. The \(\delta\)-Predictor formula that we present here provides a way around this challenge - requiring no specific knowledge about the convergence rate and trajectory, or even about the sequence limit itself:

\[\delta_{\mathrm{predicted}}=\lim_{n\rightarrow\infty}\frac{n\cdot\log\left| \frac{\lambda_{1}(n)}{\lambda_{2}(n)}\right|}{\log|\tilde{q}_{n}|}-1\] (5)

where \(\lambda_{1}(n)\) and \(\lambda_{2}(n)\) are the eigenvalues of the matrix \(\begin{pmatrix}0&b_{n}\\ 1&a_{n}\end{pmatrix}\), \(|\lambda_{1}(n)|>|\lambda_{2}(n)|\).

This formula extends a hypothesis made in a previous work (David et al., 2021), which was limited to PCFs with \(\deg(B)=2\deg(A)\) and with a \(\tilde{q}_{n}\) that grows exponentially. As we found in this work, Eq.5 works for any converging PCF. It was validated numerically and proven for the \(\deg(B)=2\deg(A)\) case in Appendix F. This formula helps estimate the irrationality measure, a critical dynamical metric for our work. Specifically, the asymptotic behavior of \(\tilde{q}_{n}\) and \(\nicefrac{{\lambda_{1}}}{{\lambda_{2}}}\) are still required for finding \(\delta_{\mathrm{predicted}}\), but they are usually easier to derive.

### Discovery of Formulas by Unsupervised Learning

Each PCF formula is defined by the polynomials that generate it. This work focuses on polynomials up to 2nd degree: \(a_{n}=A_{2}n^{2}+A_{1}n+A_{0}\), \(b_{n}=B_{2}n^{2}+B_{1}n+B_{0}\), with integer coefficients in the domain \(-5\leq A_{i}\leq 5\), \(-5\leq B_{i}\leq 5\). We removed the \(a=0\) and \(b=0\) cases, as they break the PCF structure, leaving us with 1,768,900 formulas. Some of these PCFs do not converge to a single limit, rendering their measured metrics meaningless (see Appendix B for the classification method we developed to predict PCF convergence). We filtered out all formulas that do not converge, providing the final filtered dataset of 1,543,926 formulas.

The conventional classification of the PCFs is by the coefficients of their polynomials \(a_{n},b_{n}\), \((A_{2},A_{1},A_{0},B_{2},B_{1},B_{0})\), and by their numerical limit \(L\), which we evaluate here at depth \(n=2000\).

Going beyond these conventional classification, our methodology relies on dynamics-based metrics calculated for each formula:

* The irrationality measure: for each PCF, we calculate \(\delta_{\mathrm{predicted}}\) (Eq.5) and compute \(\delta\) directly using the Blind-\(\delta\) algorithm (presented in section 3.4) at depth \(n=1000\). Fig.2a presents example \(\delta\) evaluations.

* The convergence rate dynamics, comprised of three parameters: for each PCF, we fit the approximation error \(\epsilon(n):=\left|\frac{p_{n}}{q_{n}}-L\right|\), which scales as \(\epsilon(n)\sim n!^{\eta}\cdot e^{\gamma n}\cdot n^{\beta}\) for large \(n\). We store the fitted \(\eta\) (factorial coefficient), \(\gamma\) (exponential coefficient), and \(\beta\) (polynomial coefficient). The process is detailed in Appendix A.
* The growth rate of \(\tilde{q}_{n}\), comprised of three parameters: For each PCF, we fit the denominator to \(\tilde{q}_{n}\sim n!^{\eta^{\prime}}\cdot e^{\gamma^{\prime}n}\cdot n^{\beta^ {\prime}}\) for large \(n\). We store the fitted parameters \(\eta^{\prime},\gamma^{\prime},\beta^{\prime}\).

Based on this set of metrics, we applied unsupervised clustering for unlabeled data (using the hierarchical density-based HDBSCAN algorithm (McInnes et al., 2017)). The clustering is the key component in the algorithm we developed (Fig.1), leading to the discovery of a variety of formulas and data patterns that exposed formula families (see sections 4.1, 4.2, 4.3, and 4.4 for selected results).

### The Blind-\(\delta\) Algorithm

The irrationality measure \(\delta\) of a PCF is of mathematical interest, and (as we will see in section 4) is a powerful dynamical metric. Unfortunately, evaluating \(\delta\) using Eq.3 requires knowing the series limit \(L\), making its estimation for a large set of unlabeled PCFs impractical.

The Blind-\(\delta\) algorithm was created in order to circumvent this limitation. Instead of inspecting the convergence behavior of \(\frac{p_{n}}{q_{n}}\to L\), we inspect the convergence behavior of \(\frac{p_{n}}{q_{n}}\to\frac{p_{m}}{q_{m}}\) for specific

Figure 2: **Dynamics-based metrics for formulas of mathematical constants.** Analysing the convergence of polynomial continued fraction (PCF) formulas provide _dynamical metrics_ that prove useful for their automated clustering and identification. (a) Irrationality measure \(\delta\) vs. PCF depth, for two example formulas of the constants \(\cot(1)\) and the Silver Ratio. The \(\delta\) of these constants is known to be \(1\) (green dashed lines). The blue dots show the numerical convergence of \(\delta\) (Eq.3) to the correct value. The red dots show the evaluated \(\delta\)-Predictor formula (Eq.5), following the numerical \(\delta\) very closely in the Silver Ratio formula, while taking a completely different (and much slower) trajectory in the \(\cot(1)\) formula; yet both converge to the correct value \(\delta=1\). For the purposes of clustering, \(\delta_{\mathrm{predicted}}\) was evaluated at \(n=10^{6}\), providing an accurate estimation for \(\delta\). (b) \(\delta_{n}\) (\(n=1000\)) vs. the limit value for PCFs in our dataset. While \(\delta\) values seem to follow a pattern, the limit value distribution does not contain relevant information (the higher density of PCFs near the Y axis arises from the small coefficients of the polynomials in our dataset). Our dataset contains \(913,056\) irrationality-proving formulas, most of which are not yet linked to any known constant. (c) Exponential growth coefficients of \(\tilde{q}_{n}\) and of \(\epsilon(n)\) for PCFs with \(\deg(B)=2\deg(A)\). Note the surprising “band-structures” that this view reveals. A few of the clusters have been identified, but the reason for the appearance of these “bands” and the properties of most clusters remain as open questions. (d) Example PCFs in the dataset that converge to a value close to the constant \(\cot(1)\) (\(\pm 10^{-5}\)) and yet are not related to it, showcasing the challenge of mathematical formula discovery. For visual clarity, error bars not shown. See Appendix A for a discussion regarding measurement errors.

\(m>n\). Given a rational approximation \(\frac{p_{n}}{q_{n}}\to L\), we approximate the error rate \(\epsilon(n)\) with

\[\left|\frac{p_{n}}{q_{n}}-\frac{p_{m}}{q_{m}}\right|=\epsilon(n)\cdot\left|1- \frac{\epsilon(m)}{\epsilon(n)}\right|.\]

If \(0<s<\left|1-\frac{\epsilon(m)}{\epsilon(n)}\right|<S\) is bounded away from zero and infinity for all \(n\) large enough, then the approximation of the Blind-\(\delta\) algorithm has the same convergence rate as Eq.3, bypassing the need to evaluate \(L\). Intuitively, this condition holds whenever \(|\epsilon(n)|\to 0\) fast enough, which is true for the vast majority of PCFs (see Appendix F for details).

Note that \(m\) has to grow with \(n\). In practice, our implementation of the algorithm in this work uses \(m=2n\), so in order to study \(\delta\) up to \(n=1000\), we use \(m=2000\).

### Choice of Metrics for Clustering

As part of the automated formula discovery flow we choose the best metric (for each step), in terms of representation power, which is measured by applying the Davies-Bouldin Index (Davies and Bouldin, 1979) on clustering using each metric individually. Table 1 shows results for a randomly chosen sample of 25K converging PCFs. Note the extremely poor performance of the PCF limit \(L\), in agreement with Fig.2b,d. This dimensionality reduction is important for efficiency during the clustering step and for better explainability. The former is because the dataset size grows exponentially with the PCF degree and with the magnitude of the polynomial coefficients.

Other metrics were tested. Some have been shown to have little to no representation power (e.g. \(p_{n}\) and \(q_{n}\), as defined in Eq.2, modulo various primes, their sign, their GCD etc.) while others show potential and are left for future study (e.g. the leading Fourier coefficients of the "noise" around the fit of \(\tilde{q}_{n}\)). A relatively small number of metrics were measured and used, which helped keep the results mathematically explainable. Nevertheless, the clustering using the metrics in Table 1 showcases the strength of our dynamical metrics approach.

## 4 Results

### Discovered Formulas for Mathematical Constants

The first step in validating the dynamical metrics approach is using basic heuristics on the metric space to find PCFs related to mathematical constants. There are some PCFs in the dataset that have a known irrational limit (like the examples in Eq.1 and the PCF family

\[\frac{B}{A+\frac{B}{A+\ \ddots}}=\frac{2B}{A+\sqrt{A^{2}+4B}}\]

for constant \(A\) and \(B\)), so we expected to find some of them. Through this test, we also found _previously unknown_ PCF formulas related to mathematical constants. Note that known mathematical formulas are both the anchors for labeling and a test set in our method.

A natural heuristic is inspecting PCFs with \(\delta\approx 1\), marking their limits as irrational. Another heuristic we used is focusing on PCFs with \(\eta^{\prime}\approx 0\), as it was a very strong indicator for mathematical constant

\begin{table}
\begin{tabular}{|l|c|c|} \hline Metric & \multicolumn{1}{c|}{Davies-Bouldin Index} \\ \hline Limit \(L\) & \multicolumn{1}{c|}{67.23} \\ \hline Irrationality measure \(\delta\) & \multicolumn{1}{c|}{1.11} \\ \hline Reduced denominator \(\tilde{q}_{n}\) growth factors & Exponential coefficient \(\gamma^{\prime}\) & 0.51 \\ \cline{2-3} \(\tilde{q}_{n}\sim n!^{\eta^{\prime}}\cdot e^{\gamma^{\prime}n}\cdot n^{\delta^ {\prime}}\) & Factorial coefficient \(\eta^{\prime}\) & 0.13 \\ \hline Error rate \(|e(n)|\) growth factors \(|e(n)|\sim n!^{\eta}\cdot e^{\gamma n}\cdot n^{\beta}\) & Exponential coefficient \(\gamma\) & 14.83 \\ \cline{2-3} \cline{1-2} \(n!^{\eta}\cdot e^{\gamma n}\cdot n^{\beta}\) & Factorial coefficient \(\eta\) & 0.77 \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of the representation power of the main dynamical metrics (lower is better). \(\beta\), \(\beta^{\prime}\) and \((A_{2},A_{1},A_{0},B_{2},B_{1},B_{0})\) provide little value for the initial clustering and are not shown.

formulas in a previous work [Elimlelech et al., 2023]. Combining the two gives a subset (see Fig.3a top left) that contains PCFs such as:

\[5+\frac{-10}{\ddots+\frac{-5n^{2}-5n}{5n+5+\ddots}}=2+\phi\qquad-3+\frac{1}{ \ddots+\frac{1}{-3+\ddots}}=\frac{-2}{\sqrt{13}-3}\] (6)

Removing the limitation on \(\tilde{q}_{n}\) growth rate, one can find the \(\cot(1)\) formula shown in Fig.2a:

\[1+\frac{-1}{\ddots+\frac{-1}{2n+1+\ddots}}=\cot(1)\] (7)

On the other hand, relaxing the limitation on \(\delta\), focusing only on \(\eta^{\prime}\approx 0\), a rich structure emerges (Fig.3b). Diving deeper into the B-dominated subset, we find formulas (Fig.3c) for the Gauss constant \(G_{GA}\)[Finch, 2003]:

\[4+\frac{6}{\ddots+\frac{4n^{2}+2n}{4+\ddots}}=\frac{2G_{GA}}{4G_{GA}-3}\qquad \quad 4+\frac{4}{\ddots+\frac{4n^{2}+2n-2}{4+\ddots}}=\frac{4G_{GA}-1}{3G_{GA}-2}\] (8)

Lemniscate constant \(L_{Lemniscate}\)[Finch, 2003]:

Figure 3: **Discovery of mathematical structures via analysis of dynamical metrics of formulas.** (a) Projecting the data on the \(\delta\) vs. \(\eta^{\prime}\) (\(\tilde{q}_{n}\) factorial coefficient) plane, it is easy to see the emerging subsets. We focus on PCFs with \(\eta^{\prime}\approx 0\), as a previous work [Elimlelech et al., 2023] indicated this as an important property. (b) Clustering in the \(\delta\) vs. \(\gamma^{\prime}\) (\(\tilde{q}_{n}\) exponential coefficient) plane shows examples of common properties within a cluster, like rationality or convergence to a specific constant (up to a linear fractional transformation). Focusing further on the \(\deg(B)>2\deg(A)\) cluster (as it is a clear anomaly in the \(\eta^{\prime}\approx 0\) subset), we used a PSLQ algorithm to identify links between these formulas and mathematical constants. This identification was feasible since a preliminary step identified a promising subset \(\sim 5,000\) times smaller than the initial dataset. (c) The result of this clustering and identification procedure is a structured arrangement of formulas that reveal a range of novel formulas related to constants such as \(\pi\), \(\ln(2)\), \(\sqrt{2}\), Gauss’ constant, and Lemniscate’s constant. (d) Keeping only PCFs with \(B_{2}=1\) we are left with a highly symmetrical “checkerboard pattern” of formulas for \(\pi\) and \(\ln(2)\), which was generalized into infinite formula families hypotheses (see section 4.3). Error bars not shown for visual clarity, see Appendix A for a discussion regarding measurement errors.

\[4+\frac{2}{\ddots+\frac{4n^{2}-2n}{4+\ddots}}=\frac{-6}{L_{Lemniscate}-4}\] (9)

As well as for second order roots, \(\pi\) and \(\ln(2)\) (see section 4.3). Note that unlike the formulas in Eq.6 and Eq.7, which are analytically proven, the formulas in Eq.8 and Eq.9 are (to the best of the authors' knowledge) _novel_. Their limits were numerically validated to a large precision, yet formal proofs for these formula hypotheses remain an open challenge.

It should be noted that usually in number theory, a bigger \(\delta\) is considered "good", whereas a smaller (often negative) \(\delta\) is considered "bad". We use \(\delta\) as a metric, without "judgment". These novel formulas (Eq.8, Eq.9 and the infinite family of formulas shown in section 4.3), which have the "bad" \(\delta\approx-1\), are a demonstration that our "non-judgmental" approach is successful.

### Clustering in Dynamics-Based Metric Latent Space

This section shows that clusters in the latent space of dynamics-based metrics are successful in grouping together different formulas in a way that exposes their shared properties, such as the mathematical constant to which they relate.

Looking at the top left cluster in Fig.3b (defined by \(\tilde{q}_{n}\) exponential coefficient \(<0.6\) and \(\delta>0.9\)), we recognize the canonical form of the golden ratio PCF (shown in Eq.1). This cluster also contains 21 additional PCFs, with different generating polynomials, some of higher degree. As it turns out, all of them are linear fractional transformations of \(\sqrt{5}\) (see Appendix C), which were labeled automatically by the formula discovery algorithm (Fig.1). Another example of property conservation within clusters is the rational cluster marked in green on Fig.3b. The limits of the PCFs in this subset are varied, and its spread is real (i.e., not only due to numerical imperfections). Yet, all the PCFs in this cluster converge to rationals - which is not directly measured by any of the latent space dimensions.

Fig.4 showcases a collection of clusters with shared properties. Using a set of 126 (mathematically unique) known anchor formulas, _441 novel mathematical formula hypotheses were automatically

Figure 4: **Automated Formula Discovery Results:** Showcasing the automated clustering and labeling of PCFs using a set of 126 known anchor formulas, connected to constants such as \(\pi,e,e^{2}\), \(C_{CF}\) (the continued fraction constant), the golden ratio \(\phi,\sqrt{2},\sqrt{3}\), and \(\sqrt{17}\). The clustering is visualized here via the 2 leading PCA components, revealing _441 novel, automatically discovered, mathematical formula hypotheses_. For visual clarity, error bars are not shown. See Appendix A for a discussion regarding measurement errors. See Appendix E for additional visualizations.

_discovered_. The constants which are related to the most new conjectures are: \(e^{2}\) (28 anchor formulas gave 178 new conjectures), \(\pi\) (39 anchor formulas gave 116 new conjectures), \(e\) (44 anchor formulas gave 80 new conjectures), and \(\sqrt{17}\) (1 anchor formula gave 55 new conjectures). Some of the novel formulas are equivalent to known PCFs (see Appendix C for a discussion about equivalence) while other formulas were analytically proven (see Appendix A.3 and Appendix G).

Note the multi-anchor clusters of \(e\) and \(e^{2}\), as well as the algebraic roots: these clusters failed to single out a specific constant, yet relate to constants of similar nature - suggesting meaningful clustering nevertheless. For the sake of visualization the algorithm stopped after the second iteration. In a standard run these multi-anchor clusters would have been separated via additional metrics.

### Detecting Patterns and Underlying Structure

As mentioned in section 4.1, the \(\mathrm{deg}(B)>2\mathrm{deg}(A)\), \(\eta^{\prime}\approx 0\) cluster, contains many formulas of interest (see Fig.3c and d). They were discovered via a PSLQ algorithm, identifying linear fractional relations between the limit values of PCFs in the subset and notable mathematical constants (such as \(\pi\) or \(e\)). This is a computationally heavy operation, and it would be challenging to run it on all 1.5M formulas in the data set. Yet by first identifying the promising clusters, we reduce the search space \(\sim 5,000\) times, allowing for a deeper inspection of each PCF.

Once the "checkerboard" pattern in Fig.3d was discovered, the hypothesis was expanded into 2 infinite families of PCFs with sub-exponential convergence relating to \(\pi\) and \(\ln(2)\):

* \(a_{n}=i+2j+1,b_{n}=n^{2}+(i+k)n\), with integers \(i,j\geq 0\), and \(k\in\{0,1\}\). This is expected to be related to \(\pi\) if \(k=1\), and to \(\ln(2)\) if \(k=0\) (in fact, this pattern can be generalized even further, into a novel 3-dimensional Conservative Matrix Field, provided in Appendix C. See [10] for the definition of Conservative Matrix Fields).

Another formula family was discovered via clustering in the \(\gamma\) vs. \(\gamma^{\prime}\) space. The algebraic roots subset (marked by a green circle in Fig.2c) was generalized into:

* \(a_{n}=-2n+j-1,b_{n}=-n^{2}+jn+k\) for integer \(j,k\) such that \(b_{n}\) has real roots that are not positive integers. This is expected to converge to a root of \(b_{n}\).

These are novel experimental results and mathematical hypotheses - awaiting proof.

### Higher Degree PCF Identification

Figure 5: Higher-degree PCF formulas for mathematical constants, overlaid on Fig.3b. Formulas that are equivalent to an existing element in the original dataset are marked with “o”, while “x” marks truly novel formulas. _The higher-degree PCFs fit, using the same dynamical metrics, into clusters trained on lower-degree PCFs, showing the ability to identify novel formulas despite being of mathematical forms not seen during training_.

The final validation for the dynamical metrics approach is to show its potential even on test sets of a different mathematical structure than the training set. For that purpose, the clusters created based on the original dataset were treated as a classifier, and a test set of higher-degree PCFs (up to 3\({}^{\text{rd}}\) degree \(A_{n}\) and 4\({}^{\text{th}}\) degree \(B_{n}\), coefficient range [-2,2]) was created, measured and classified.

Naturally, not all high-degree PCFs fit neatly into the existing clusters (as they represent constants that were not present in the original dataset), but some were correctly identified and labeled, discovering novel formulas in the process (see Fig.5 for a sample of the results).

## 5 Discussion and Outlook

This work marks an important step toward the vision of automated on-demand formula creation in mathematics. Going beyond all previous algorithms in this field, we connect the challenge of formula creation to robust ML methods. This methodology provides a wide variety of automatically generated formulas, including both previously known and previously unknown ones, exposing their underlying mathematical structure and enabling new proofs.

The next research step directly building on our methodology could help to finally reveal the complete intricate mathematical structure of PCFs. For example, starting with the "band-structure" found in Fig.2c, or with clusters of formulas with various structures representing the same mathematical constant. Further exploration of our conjectures from section 4 could have more impact on mathematics, perhaps achieving further generalizations and prescriptive formula generation.

The technique presented here can be applied to a larger scope of continued fractions and for completely different types of formulas. For more general continued fractions, dynamical metrics such as the numerical trajectories and the corresponding sequences of \(\delta\) (in addition to its asymptotic value) hold valuable information even in continued fractions that do not converge at all. We expect these dynamical metrics to provide a "fingerprint" for wider families of formulas and perhaps even for the mathematical constants themselves. This approach was directly applied in this work to higher polynomial degrees, larger polynomial coefficients, and can be expanded to continued fractions not based on polynomials. Looking beyond continued fractions, metrics that are derived from the dynamics of a numerical calculation of certain formulas are an especially good fit for automated computer-assisted investigations. Such metrics can be measured for a variety of mathematical structures, including ones whose evaluation is iterative or recursive, that are defined via an infinite sum, or any other process which produces rational approximants. Any such mathematical structure can be measured, clustered, and identified using the proposed method - treating the generating functions as a black box. We believe that such dynamical metrics can unveil patterns and underlying structures in broad fields of mathematics and in other areas of science.

To exemplify this universal concept, we looked into higher depth recursion relations, which are a promising research direction because little is known about their global structure, yet they are involved in several important conjectures. For example, the best rational approximation formula known for Euler's gamma constant is constructed via such a recursion relation [Aptekarev, 2009]. This family of formulas is broader than continued fractions, yet they can be described by the same metrics as PCFs. Another type of mathematical structure successfully analyzed using the same method is hypergeometric functions, showing the applicability of our measurement-clustering-generation approach to a bigger family of mathematical functions. This generalization can be useful in a wide variety of contexts, such as investigations of integral formulas (e.g., Beukers-type integrals [Beukers, 1979, Dougherty-Bliss et al., 2022, Brown and Zudilin, 2022]).

Our work was based on a limited-size dataset and on a small set of metrics. It would be intriguing to test the extracted conjectures on larger datasets, which can help reveal additional, more intricate, phenomena. Considering the success we had using a relatively small set of metrics, we would like to use an order-of-magnitude larger set of metrics and find what new predictions can be recovered. In fact, the creation and evaluation of the metrics themselves can be automated.

Taking a broader perspective, the methodology presented in this work can be seen as a general prescription for tackling scientific discovery challenges in mathematics and theoretical physics that rely on numerical evaluations and generalizations. Such an advance is especially exciting for such challenges that were considered in the past to require intuitive leaps of creativity.

#### Acknowledgments

This research received support through Schmidt Sciences, LLC.

## References

* Anderson (2004) David P Anderson. Boinc: A system for public-resource computing and storage. In _Fifth IEEE/ACM international workshop on grid computing_, pages 4-10. IEEE, 2004.
* Aptekarev (2009) A. I. Aptekarev. On linear forms containing the euler constant, 2009. URL https://arxiv.org/abs/0902.1768.
* Bailey et al. (2007) David Bailey, Jonathan Borwein, Neil Calkin, Russell Luke, Roland Girgensohn, and Victor Moll. _Experimental mathematics in action_. CRC press, 2007.
* Berggren et al. (2004) Lennart Berggren, Jonathan Borwein, Peter Borwein, and M Lambert. Memoire sur quelques proprietes remarquables des quantites transcendentes circulaires et logarithmiques. _Pi: A Source Book_, pages 129-140, 2004.
* Beukers (1979) Frits Beukers. A note on the irrationality of \(\zeta(2)\) and \(\zeta(3)\). _Bulletin of the London Mathematical Society_, 11(3):268-272, 1979.
* Bowman and McLaughlin (2002) Douglas Bowman and James McLaughlin. Polynomial continued fractions. _Acta Arith._, 103:329-342, 2002.
* Brown and Zudilin (2022) Francis Brown and Wadim Zudilin. On cellular rational approximations to \(\zeta(5)\). _arXiv preprint arXiv:2210.03391_, 2022.
* Buchberger et al. (2006) Bruno Buchberger, Adrian Craciun, Tudor Jebelean, Laura Kovacs, Temur Kutsia, Koji Nakagawa, Florina Piroi, Nikolaj Popov, Judit Robu, Markus Rosenkranz, et al. Theorema: Towards computer-aided mathematical theory exploration. _Journal of applied logic_, 4:470-504, 2006.
* Church (2019) Benjamin Church. Diophantine approximation and transcendence theory, 2019.
* Cuyt et al. (2008) Annie Cuyt, Vigdis Petersen, Brigitte Verdonk, Haakon Waadeland, and William B. Jones. _Handbook of continued fractions for special functions_. Springer Science & Business Media, 2008.
* David et al. (2021) Nadav Ben David, Guy Nimri, Uri Mendlovic, Yahel Manor, and Ido Kaminer. On the connection between irrationality measures and polynomial continued fractions. _arXiv preprint arXiv:2111.04468_, 2021.
* Davies et al. (2021) Alex Davies, Petar Velickovic, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomasev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andras Juhasz, et al. Advancing mathematics by guiding human intuition with AI. _Nature_, 600:70-74, 2021.
* Davies and Bouldin (1979) David L. Davies and Donald W. Bouldin. A cluster separation measure. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, PAMI-1(2):224-227, 1979. doi: 10.1109/TPAMI.1979.4766909.
* Davis and Lenat (1982) R. Davis and D.B. Lenat. _Knowledge-based Systems in Artificial Intelligence_. A McGraw-Hill advertising classic. McGraw-Hill International Book Company, 1982. ISBN 9780070155572. URL https://books.google.co.il/books?id=MpVQAAAAAAJ.
* Dougherty-Bliss et al. (2022) Robert Dougherty-Bliss, Christoph Koutschan, and Doron Zeilberger. Tweaking the beukers integrals in search of more miraculous irrationality proofs a la apery. _The Ramanujan Journal_, 58(3):973-994, 2022.
* Elimelech et al. (2023) Rotem Elimelech, Ofir David, Carlos De la Cruz Mengual, Rotem Kalisch, Wolfgang Berndt, Michael Shalyt, Mark Silberstein, Yaron Hadad, and Ido Kaminer. Algorithm-assisted discovery of an intrinsic order among mathematical constants. _arXiv preprint arXiv:2308.11829_, 2023.
* Elimelech et al. (2020)Leonhard Euler. _Introductio in analysin infinitorum_, volume 1,2. Apud Marcum-Michaelem Bousquet & Socios, 1748.
* Fajtlowicz [1988] Siemion Fajtlowicz. On conjectures of graffiti. In J. Akiyama, Y. Egawa, and H. Enomoto, editors, _Graph Theory and Applications_, volume 38 of _Annals of Discrete Mathematics_, pages 113-118. Elsevier, 1988. doi: https://doi.org/10.1016/S0167-5060(08)70776-3. URL https://www.sciencedirect.com/science/article/pii/S0167506008707763.
* Fawzi et al. [2022] Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, Mohammadamam Barekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Discovering faster matrix multiplication algorithms with reinforcement learning. _Nature_, 610:47-53, 2022.
* Finch [2003] Steven R. Finch. _Mathematical Constants_. Cambridge University Press, 2003.
* Hardy et al. [1979] Godfrey Harold Hardy, Edward Maitland Wright, et al. _An introduction to the theory of numbers_. Oxford university press, 1979.
* Harris et al. [2020] Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernandez del Rio, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.
* Kaliszyk and Urban [2012] Cezary Kaliszyk and Josef Urban. Learning-assisted automated reasoning with flyspeck. _CoRR_, abs/1211.7012, 2012. URL http://arxiv.org/abs/1211.7012.
* Lagarias [2013] Jeffrey C. Lagarias. Euler's constant: Euler's work and modern developments. _Bulletin of the American Mathematical Society_, 50, 2013. ISSN 02730979. doi: 10.1090/S0273-0979-2013-01423-X.
* Lambert [1768] Johann Heinrich Lambert. Memoires sur quelques proprietes remarquables des quantites transcendantes, circulaires et logarit humiques. _Memoires de l'Academie royale des sciences de Berlin_, pages 265-322, 1768.
* Laughlin and Wyshinski [2004] James Mc Laughlin and Nancy J Wyshinski. Real numbers with polynomial continued fraction expansions. _arXiv preprint math/0402462_, 2004.
* Lenat [1982] Douglas B. Lenat. The nature of heuristics. _Artificial Intelligence_, 19(2):189-249, 1982. ISSN 0004-3702. doi: https://doi.org/10.1016/0004-3702(82)90036-4. URL https://www.sciencedirect.com/science/article/pii/0004370282900364.
* Lenat and Brown [1984] Douglas B Lenat and John Seely Brown. Why am and eurisko appear to work. _Artificial intelligence_, 23(3):269-294, 1984.
* McInnes et al. [2017] Leland McInnes, John Healy, and Steve Astels. hdbscan: Hierarchical density based clustering. _Journal of Open Source Software_, 2(11):205, 2017. doi: 10.21105/joss.00205. URL https://doi.org/10.21105/joss.00205.
* Petkovsek et al. [1996] Marko Petkovsek, Herbert S. Wilf, and Doron Zeilberger. _A=B_. AK Peters Ltd, 1996.
* Raayoni et al. [2021] Gal Raayoni, Shahar Gottlieb, Yahel Manor, George Pisha, Yoav Harris, Uri Mendlovic, Doron Haviv, Yaron Hadad, and Ido Kaminer. Generating conjectures on fundamental constants with the ramanujan machine. _Nature_, 590, 2021. ISSN 14764687. doi: 10.1038/s41586-021-03229-4.
* Ridnik et al. [2024] Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation with alphacodium: From prompt engineering to flow engineering. _arXiv preprint arXiv:2401.08500_, 2024.
* Trinh et al. [2024] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. _Nature_, 625(7995):476-482, 2024.
* Urban [2007] Josef Urban. Malarea: a metasystem for automated reasoning in large theories. _ESARLT_, 257, 2007.
* Vedral et al. [2017]Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020. doi: 10.1038/s41592-019-0686-2.
* Wang (1960) H. Wang. Toward mechanical mathematics. _IBM Journal of Research and Development_, 4(1):2-22, 1960. doi: 10.1147/rd.41.0002.
* Wolfram et al. (2002) Stephen Wolfram et al. _A new kind of science_, volume 5. Wolfram media Champaign, 2002.

Numerical Measurements, Curve Fitting and Validation of Formulas

### Numerical Measurements

When characterizing PCFs, we use several metrics extracted from the dynamic behavior of the formula:

* The growth coefficients \(\eta,\gamma,\beta\) (of the form \(n!^{\eta}\cdot e^{\gamma n}\cdot n^{\beta}\)) of the convergence rate \(\epsilon(n)\).
* \(\tilde{q}_{n}\) (as defined in Eq.3) growth coefficients: \(\eta^{\prime},\gamma^{\prime}\) (of the form \(n!^{\eta^{\prime}}\cdot e^{\gamma^{\prime}n}\)).
* The \(\delta\) (as defined in Eq.3) calculated using the Blind-\(\delta\) Algorithm described in section 3.4.

To measure the growth coefficients of \(\tilde{q}_{n}\) and \(\epsilon(n)\), the values of \(\log\left(\epsilon(n)\right)\) (see section 3.4) and of \(\log\left(\tilde{q}_{n}\right)\) were evaluated up to depth 1000.

The most resource-intensive values that are generated are \(p_{n},q_{n}\) and \(\gcd(p_{n},q_{n})\) - all other values are calculated from them (and require less precision). For the worst case PCF this requires 36MB of memory (without optimizations) and \(\sim 1.9\) seconds of run time on a single core of a basic workstation, which translates to an upper cap of \(\sim 900\) hours for the whole data set. In practice we used a high power cluster with 64 cores, which ran each iteration of the measurements in \(\sim 8.5\) hours.

Once these values are calculated, using scipy (Virtanen et al., 2020) and numpy (Harris et al., 2020) a fit of the form \(\log\left(n!^{\eta}\cdot e^{\gamma n}\cdot n^{\beta}\right)\) was calculated for \(\tilde{q}_{n}\) and \(\epsilon(n)\), producing the dynamical metrics.

### Curve Fitting

A curve fit using 1000 points is a fairly heavy operation, unsuited for large scale investigations. Instead, we used an extreme down-sampling. Specifically, only 5 points were used for the fit.

One may justifiably wonder if 5 data points are sufficient to fit accurately enough the desired metrics.

A test comparing between a 5 data point fit and a 1000 data point fit was done. As the test set, 50 PCFs were randomly chosen out of each of 9 categories (450 total test cases). The categories were all combinations of \(\deg(a)=0,1,2\) and \(\deg(b)=0,1,2\). Focusing on the dominant coefficients (\(\gamma\) and \(\eta\)), for each case, a full (1000 point) fit was performed (producing \(\gamma_{f},\eta_{f}\)), and compared to the down sampled fit of 5 points (producing \(\gamma_{p},\eta_{p}\)). We tested 2 methods of choosing the 5 points, even (\(i=6,206,406,606,806\)) and logarithmic (\(i=6,125,250,500,1000\)). The relative error was then calculated (\(\frac{|\gamma_{p}-\gamma_{f}|}{|\gamma_{f}|}\) and \(\frac{|\eta_{p}-\eta_{f}|}{|\eta_{f}|}\)) for \(\epsilon(n)\) and \(\tilde{q}_{n}\). The relative errors were then averaged over the test set (results summarised in table 2) - showing the 5-point fit to be almost as good as the full 1000-point fit. In our measurements we use the logarithmic point distribution as it gives better results for most metrics.

Licences and versions of Python packages (used for curve fitting, clustering and large number mathematics):

Scipy (Version: 1.11.3) - BSD License (Copyright (c) 2001-2002 Enthought, Inc. 2003-2024, SciPy Developers. All rights reserved.)

gmpy2 (Version: 2.1.5) - GNU Lesser General Public License v3 or later

Numpy (Version 1.26.1)- BSD License (Copyright (c) 2005-2023, NumPy Developers. All rights reserved.)

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Behavior & \multicolumn{2}{c|}{\(\tilde{q}_{n}\)} & \multicolumn{2}{c|}{Convergence Rate} \\ \hline Coefficient & \(\gamma^{\prime}\) & \(\eta^{\prime}\) & \(\gamma\) & \(\eta\) \\ \hline Relative Error Average (even spread) & 0.0124 & 0.0007 & 0.0078 & 0.0005 \\ Relative Error Average (logarithmic spread) & 0.0175 & 0.0004 & 0.0024 & 0.00012 \\ \hline \end{tabular}
\end{table}
Table 2: Comparison between 1000 point fit results and 5 point fits results (even spread and logarithmic spread).

### Validation of Automatically Generated Formulas

After a PCF has been identified as potentially connected to a known constant \(C\), it undergoes a verification process in order to establish whether it indeed converges to a known mathematical expression involving \(C\). This process consists of several steps:

_1. Expression Identification Through PSLQ Analysis_

The PCF limit is computed and a targeted PSLQ search is used to identify an expression that matches the resulting digits and incorporates the constant \(C\).

_2. Expression Verification_

After a candidate expression has been identified, the PCF is computed to a greater depth. The resulting values are compared against the proposed expression to assess the accuracy of the match and verify convergence.

_3. Systematic Proof_

Once the expression has been verified, an automated analytical proof is attempted via the following steps.

Given two polynomials \(a(n)\) and \(b(n)\) defining a PCF, Euler's formula [10] states that when there exist polynomials \(h_{1}(n),h_{2}(n),f(n)\) such that \(b(n)=-h_{1}(n)h_{2}(n)\), \(f(n)a(n)=f(n-1)h_{1}(n)+f(n+1)h_{2}(n+1)\), the limit of the PCF equals:

\[\frac{f(-1)h_{1}(0)}{f(0)}+\frac{f(1)h_{2}(1)}{f(0)}\cdot\left(\sum_{k=0}^{ \infty}\frac{f(0)f(1)}{f(k)f(k+1)}\prod_{i=1}^{k}\frac{h_{1}(i)}{h_{2}(i+1)} \right)^{-1}\] (10)

The proof is then completed using known identities of infinite sums.

If the PCF's polynomials do not satisfy Euler's formula's conditions, the PCF is flagged for further examination.

For example, one may take the PCF defined by the polynomials \(a_{n}=-2\) and \(b_{n}=4n^{2}+4n+1\) which was labeled as related to \(\pi\). The PCF was numerically identified and validated as converging to \(1-\frac{3}{3-\frac{3\pi}{4}}\) Next, the polynomials \(h_{1},h_{2}\) and \(f\) were identified as \(2n+1,-(2n+1)\) and \(1\) respectively, which corresponds to the sum \(1-\frac{3}{\sum_{k=0}^{\infty}\prod_{n=1}^{k}\frac{-2n-1}{2\pi+3}}\). This infinite sum is known and was indeed proven to converge to \(1-\frac{3}{3-\frac{3\pi}{4}}\).

For the entire list of proven formulas, see table 5 in appendix G.

## Appendix B Classification of Continued Fractions

Not all PCFs converge. Clearly, if \(\frac{p_{n}}{q_{n}}\) does not have a well defined limit, then some of our numerically measured metrics lose their meaning. Though we had algorithmic safeguards to detect such cases and remove them from the analyzed set, it was valuable to identify a pattern and formulate a rule-set that predicts the convergence of a PCF.

For that purpose we turned to the matrix representation of a continued fraction to depth \(n\) (see Appendix F.1 for details):

\[\begin{bmatrix}p_{N-1}&p_{N}\\ q_{N-1}&q_{N}\end{bmatrix}=\prod_{n=1}^{N-1}\begin{bmatrix}0&b_{n}\\ 1&a_{n}\end{bmatrix}=\] \[\left(\prod_{n=1}^{N-1}a_{n-1}\right)\begin{bmatrix}1&0\\ 0&\frac{1}{a_{0}}\end{bmatrix}\begin{bmatrix}\prod_{n=1}^{N-1}\begin{bmatrix} 0&\frac{b_{n}}{a_{n}a_{n-1}}\\ 1&1\end{bmatrix}\right)\begin{bmatrix}1&0\\ 0&a_{N-1}\end{bmatrix}\]

Assuming \(a_{n}\neq 0\) for \(n\geq 0\).

Analyzing the eigenvalues of the matrices within the matrix product as \(n\to\infty\) allows for examining the asymptotic behavior of the continued fraction. Their characteristic polynomial is \(\lambda^{2}-\lambda-\frac{b_{n}}{a_{n}a_{n-1}}\)and we propose observing the discriminant of this polynomial - more specifically its dominant power of \(n\):

\[\Delta_{n}=1+\frac{4b_{n}}{a_{n}a_{n-1}}=C_{s}n^{s}+O(n^{s-1})\] (11)

Here we assume \(C_{s}\neq 0\) and \(s\) is some integer. Based on the data, we compiled table 3 as a summary of the conjectured behavior of any polynomial continued fraction based on \(s\) and \(C_{s}\).

We can further elaborate on the converging cases by discussing the conjectured rate of convergence. Usually, a PCF is expected to converge at a sub-exponential rate, but in the case of \(s=0,C_{0}>0\) it is expected to converge faster:

* If \(C_{0}\neq 1\) then the PCF will converge at an exponential rate, and the exact rate of convergence increases monotonically as \(C_{0}\to 1\), with a vertical asymptote at \(C_{0}=1\). The convergence rate is identical for \(C_{0}\) and \(\frac{1}{C_{0}}\).
* If \(C_{0}=1\) then the PCF will converge at a factorial rate. More specifically, if we find the second most dominant power \(\Delta_{n}=C_{0}+\frac{C_{t}}{n^{t}}+O(\frac{1}{n^{t+1}})\) for some \(C_{t}\neq 0\) and integer \(t>0\) then the precision will grow at a rate of \(O(n!t)\).

We used these rules (in conjunction with the measurements mentioned in section 3.3) to validate that all PCFs we analyze and cluster do converge and their measured metrics are well defined.

## Appendix C Discovering equivalence of continued fractions

Polynomial continued fractions use two polynomials \(a_{n}=a(n)\) and \(b_{n}=b(n)\) to generate a sequence of rationals \(p_{n}/q_{n}\). However, the same sequences with identical behaviour can be generated using more then one set of polynomials. By identifying transformations under which the dynamics of \(p_{n}/q_{n}\) remains invariant, we can formally prove equivalence between data points, validating the clustering power of the chosen metrics.

By rearranging the continued fraction definition, we can see how equivalent \(a_{n}\) and \(b_{n}\) series can arise:

\[a_{0}+\frac{b_{1}}{a_{1}+\frac{b_{2}}{a_{2}+\frac{b_{3}}{\ddots+\frac{b_{n}}{ a_{n}+\ddots}}}}=a_{0}\left(1+\frac{\frac{b_{1}}{a_{0}a_{1}}}{1+\frac{\frac{b_{2}}{a_{ 1}a_{2}}}{1+\frac{\frac{b_{3}}{a_{2}a_{3}}}{1+\frac{\frac{b_{3}}{a_{2}a_{3}}}{ \ddots+\frac{b_{n}}{a_{n}a_{n-1}}}{1+\ddots}}}}\right)=\frac{a_{0}c_{0}}{c_{0}} \left(1+\frac{\frac{b_{1}c_{0}c_{1}}{a_{0}c_{0}a_{1}c_{1}}}{1+\frac{\frac{b_{2 }c_{1}c_{2}}{a_{1}a_{2}c_{2}}}{1+\frac{\frac{b_{3}c_{2}c_{3}}{a_{2}a_{3}c_{3}}} \ddots+\frac{\frac{b_{n}c_{n-1}}{a_{n}c_{n-1}}}{a_{n}c_{n}a_{n-1}c_{n-1}}}}\right)\] (12)

Indeed, by defining a new pair of polynomials \(a^{\prime}_{n}=a_{n}c_{n};b^{\prime}_{n}=b_{n}c_{n}c_{n-1}\) we get an equivalent continued fraction which converges to \(\frac{c_{0}p_{n}}{q_{n}}\). Clearly, since the resulting sequence \(\frac{p^{\prime}_{n}}{q^{\prime}_{n}}\) is identical to the original one, it exhibits the same dynamics. We call this process "Inflation by \(c_{n}\)".

The metrics we are interested in are mostly not affected by a finite number of elements in the sequence. For example, both the convergence rate and \(\delta\) discuss an overall trend as \(n\) grows. Consequently, we can initiate the sequence at different values of \(n\neq 0\) without changing the latent parameters. When expressing these transformations as modification to the continued fraction definition, we see that the _limit_ of the continued fraction might change due to this shift in sequence initiation, but only by a rational fractional transform.

\begin{table}
\begin{tabular}{c l l} \hline Convergence & \(C_{s}>0\) & \(C_{s}<0\) \\ \hline ✗ & \(s\geq 3\) & \(s\geq 0\) \\ ✓ & \(s\leq 2\) & \(s\leq-1\) \\ \hline \end{tabular}
\end{table}
Table 3: Summary of PCF behavior characterized by \(s\) and \(C_{s}\) as defined in Eq.11.

\[a_{0}+\frac{b_{1}}{a_{1}+\frac{b_{2}}{a_{2}+\frac{b_{3}}{\ddots+\frac{b_{n}}{a_{n}+ \ddots}}}}=\frac{p_{n}}{q_{n}}\Rightarrow a_{1}+\frac{b_{2}}{a_{2}+\frac{b_{3}}{ \ddots+\frac{b_{n}}{a_{n}+\ddots}}}=\frac{b_{1}}{\frac{p_{n}}{q_{n}}-a_{0}}\]

For example, we consider the cluster of formulas related to the golden ratio shown in figure 2(b). A large portion of these PCFs stem from transforming the known formula for the golden ratio shown in Eq.1 via the methods aforementioned. The exact transformations are depicted in Table 4.

## Appendix D Scalability to Larger Datasets

The methodology can be scaled to larger data sets in multiple ways. For example, by extending the range of PCF coefficients from [-5, 5] to [-10, 10], the size of the data set increases by approximately 50 times. Additionally, by considering polynomials of higher degrees - such as advancing to third-degree \(a_{n}\) and fourth degree \(b_{n}\) with coefficients in [-5, 5] - the dataset size can be amplified by approximately 1,333 times.

To manage the substantial computational demands associated with these expansions, a three-fold solution is proposed.

First, a dynamically chosen measurement depth is implemented during evaluation, aiming for a fixed precision across all PCFs rather than maintaining a constant depth for all computations. This approach optimizes computational efficiency by adjusting the measurement depth based on the specific convergence rate of each PCF.

Second, using known equivalences of PCFs such as inflation (see Appendix C), it is possible to substantially decrease the effective size of the dataset that needs to be measured. In particular, when \(c_{n}=-1\), we observe that the sign of \(a_{n}\) does not affect the dynamics of the sequence - only flips the

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline \hline \(A_{n}\) & \(B_{n}\) & Limit & Transformation & Irrationality measure \(\delta\) \\ \hline \(1\) & \(1\) & \(\phi\) & Family’s canonical form & \(\delta=1.00168\) \\ \hline \(-1\) & \(1\) & \(\cdot\phi\) & Inflation by \(c_{n}=-1\) & \(\delta=1.00168\) \\ \hline \(2\) & \(4\) & \(2\phi\) & Inflation by \(c_{n}=2\) & \(\delta=1.00023\) \\ \hline \(-2\) & \(4\) & \(\cdot 2\phi\) & Inflation by \(c_{n}=-2\) & \(\delta=1.00023\) \\ \hline \(n+1\) & \(n(n+1)\) & \(\phi\) & Inflation by \(c_{n}=n+1\) & \(\delta=1.00168\) \\ \hline \(-(n+1)\) & \(n(n+1)\) & \(\cdot\phi\) & Inflation by \(c_{n}=-(n+1)\) & \(\delta=1.00168\) \\ \hline \(n+2\) & \((n+1)(n+2)\) & \(2\phi\) & Inflation by \(c_{n}=n+2\) & \(\delta=1.00023\) \\ \hline \(-(n+2)\) & \((n+1)(n+2)\) & \(\cdot 2\phi\) & Inflation by \(c_{n}=(-n+2)\) & \(\delta=1.00023\) \\ \hline \(2n+1\) & \((2n-1)(2n+1)\) & \(\phi\) & Inflation by \(c_{n}=(2n+1)\) & \(\delta=1.00168\) \\ \hline \(-(2n+1)\) & \((2n-1)(2n+1)\) & \(\cdot\phi\) & Inflation by \(c_{n}=(2n+1)\) & \(\delta=1.00168\) \\ \hline \(2(n+1)\) & \(4n(n+1)\) & \(\cdot 2\phi\) & Inflation by \(c_{n}=-2(n+1)\) & \(\delta=1.00023\) \\ \hline \(5\) & \(-5\) & \(\phi+2\) & Family’s canonical form & \(\delta=1.00168\) \\ \hline \(-5\) & \(-5\) & \(-(\phi+2)\) & Inflation by \(c_{n}=-1\) & \(\delta=1.00168\) \\ \hline \(5(n+1)\) & \(-5n(n+1)\) & \(\phi+2\) & Inflation by \(c_{n}=n+1\) & \(\delta=1.00168\) \\ \hline \(-5(n+1)\) & \(-5n(n+1)+0\) & \(-(\phi+2)\) & Inflation \(c_{n}=-(n+1)\) & \(\delta=1.00168\) \\ \hline \(n+2\) & \(n(n+3)\) & \((30\phi+6)/19\) & Family’s canonical form & \(\delta=0.96967\) \\ \hline \(-(n+2)\) & \(n(n+3)\) & \(-(30\phi+2)/19\) & Inflation by \(c_{n}=-1\) & \(\delta=0.96967\) \\ \hline \(n+3\) & \((n+1)(n+4)\) & \((30\phi+2)/11\) & Indent \(n\to n+1\) & \(\delta=0.97245\) \\ \hline \(-(n+3)\) & \((n+1)(n+4)\) & \(-(30\phi+2)/11\) & Indent \(n\to n+1\) and inflation by \(c_{n}=-1\) & \(\delta=0.97245\) \\ \hline \(n+3\) & \(n(n+5)\) & \((750\phi+240)/361\) & Family’s canonical form & \(\delta=0.95243\) \\ \hline \(-(n+3)\) & \(n(n+5)\) & \(-(750\phi+240)/361\) & Inflation by \(c_{n}=-1\) & \(\delta=0.95243\) \\ \hline \end{tabular}
\end{table}
Table 4: Continued fractions converging to linear fractional transformations of the golden ratio \(\phi\), found using the top left cluster of Figure 2(b). Numerous data points in this cluster exhibit identical sequence dynamics and are equivalent under the inflation and index indentation transformations. The equivalent data points create families of continued fractions in the cluster. Discrepancies between the calculated irrationality measure within the same family is ascribed to numerical inaccuracies, typically on the order \(0.001\). However, when comparing families, discrepancies in the irrationality measure rise to a magnitude of \(0.04\), suggesting potential deeper distinctions among these PCFs.

sign of the limit to \(-L\). For every PCF its inflation by -1 is also contained in the current data set, and clearly will have the same dynamics-based metrics. This equivalence single handedly de-facto cuts the size of the current data set by half (to 771,963 converging formulas).

Third, the inherently parallelizable nature of computing metrics for each formula is leveraged. The algorithm has been adapted and deployed within the Berkeley Open Infrastructure for Network Computing [1], enabling parallel computation across thousands of volunteer computers. Assuming a typical contribution of approximately 1,000 BOINC volunteer cores, processing the expanded data set requires about one month of computational time.

The approaches described enable the methodology to handle larger datasets efficiently, facilitating the analysis of more complex polynomial continued fractions within practical computational limits.

## Appendix E Clustering Visualizations

Fig.6 and Fig.7 show alternate clustering and 2D visualization approaches (in addition to Fig.4). The same set of anchors was used for all 3 versions. The tSNE algorithm (Fig. 6) provides a visualization that separates well between clusters, but loses out on explainability. In Fig.7 the opposite approach was taken - using only 2 dynamical metrics for clustering allows each cluster to be defined very clearly, but information from other dynamical metrics (which can be used for better cluster separation) is lost along the way.

Despite their differences, all 3 techniques point to similar results and conclusions, providing additional validation for our conclusions.

## Appendix F Analysis of the convergence rate

The growth rate for simple continued fraction or equivalently for constant linear recurrences is well understood, and usually boils down to the matrix defining the recurrence, and its eigenvalues. In our case, the coefficient in the recurrence also depend on \(n\), so their study is more involved, however the ideas are similar, which we now describe

Figure 6: Automated clustering and labeling of PCFs via a 2D tSNE with \(\text{perplexity}=10\). Clusters were verified via PSLQ relations between members of the cluster and known formulas. For visual clarity not all points are shown and error bars are not shown, see Appendix A for a discussion regarding measurement errors.

### Approximating the error rate

To find whether or not the sequence \(\frac{p_{n}}{q_{n}}\) converges and if so what is its convergence rate, we note the continued fraction formula

\[\frac{b\left(1\right)}{a\left(1\right)+\frac{b\left(2\right)}{a\left(2\right)+ \frac{b\left(3\right)}{\ddots+\frac{b\left(n-1\right)}{a\left(n-1\right)+0}}}}= \frac{p_{n}}{q_{n}},\]

can be rewritten in matrix form as

\[\begin{pmatrix}p_{n-1}&p_{n}\\ q_{n-1}&q_{n}\end{pmatrix}=\prod_{1}^{n-1}\begin{pmatrix}0&b\left(k\right)\\ 1&a\left(k\right)\end{pmatrix}.\]

In particular this implies that both \(p_{n}\) and \(q_{n}\) satisfy the same linear recurrence:

\[u_{n+1}=a\left(n\right)u_{n}+b\left(n\right)u_{n-1},\]

with initial conditions

\[\begin{pmatrix}p_{0}&p_{1}\\ q_{0}&q_{1}\end{pmatrix}=\begin{pmatrix}1&0\\ 0&1\end{pmatrix}.\]

Trying to determine if there is convergence, we use the Cauchy condition. For any \(m\geq n\) we have that

\[\frac{p_{m}}{q_{m}}-\frac{p_{n}}{q_{n}}=\sum_{n}^{m-1}\left(\frac{p_{k+1}}{q_ {k+1}}-\frac{p_{k}}{q_{k}}\right)=\sum_{n}^{m-1}\frac{p_{k+1}q_{k}-q_{k+1}p_{ k}}{q_{k}q_{k+1}}=-\sum_{n}^{m-1}\frac{\det\begin{pmatrix}p_{k}&p_{k+1}\\ q_{k}&q_{k+1}\end{pmatrix}}{q_{k}q_{k+1}}=-\sum_{n}^{m-1}\frac{\left(-1\right)^ {k}\prod_{j=1}^{k}b\left(j\right)}{q_{k}q_{k+1}}.\]

Figure 7: Automated clustering and labeling of PCFs via HDBSCAN. Only 2 dynamical metrics were used, \(\delta\) and \(\eta^{\prime}\) (as in Fig.3a), and yet the clustering is already informative. For visual clarity error bars are not shown, see Appendix A for a discussion regarding measurement errors.

**Corollary 1**.: The sequence \(\mathbb{K}_{1}^{\infty}\frac{b\left(n\right)}{a\left(n\right)}\) converges if and only if \(\sum_{1}^{\infty}\frac{\prod_{j=1}^{n}b\left(j\right)}{q_{k}q_{k+1}}\) converges, and to the same limit \(L\). More over, the convergence rate is

\[\epsilon\left(n\right):=\left|\frac{p_{n}}{q_{n}}-L\right|=\left|\sum_{n}^{ \infty}\frac{\left(-1\right)^{k}\prod_{j=1}^{k}b\left(j\right)}{q_{k}q_{k+1}} \right|.\]

This suggests that we should understand the growth rate of both \(q_{k}\) and \(\prod_{j=1}^{k}b\left(j\right)\).

**Remark 1**.: Note that the convergence and its rate might depend on the sign of \(\frac{\left(-1\right)^{k}\prod_{j=1}^{k}b\left(j\right)}{q_{k}q_{k+1}}\).

1. Suppose that \(\left|\frac{\left(-1\right)^{k}\prod_{j=1}^{k}b\left(j\right)}{q_{k}q_{k+1}} \right|=\frac{1}{k^{d}}\). If the signs do not alternate, then \(\left|\sum_{n}^{\infty}\frac{\left(-1\right)^{k}\prod_{j=1}^{k}b\left(j \right)}{q_{k}q_{k+1}}\right|=\sum_{n}^{\infty}\frac{1}{k^{d}}\). This diverge if \(d=1\) and has order of magnitude \(\frac{1}{k^{d-1}}\) for \(d>1\). However, with alternating signs we get the smaller bound \[\sum_{2n}^{\infty}\frac{\left(-1\right)^{k}}{k^{d}}=\sum_{n}^{\infty}\left( \frac{1}{\left(2k\right)^{d}}-\frac{1}{\left(2k+1\right)^{d}}\right)=\sum_{n} ^{\infty}\left(\frac{\left(2k+1\right)^{d}-\left(2k\right)^{d}}{\left(2k \right)^{d}\left(2k+1\right)^{d}}\right)\sim\sum_{n}^{\infty}\frac{d\left(2k \right)^{d-1}}{4k^{2d}}\sim\frac{1}{n^{d}}.\] Thus, it always converges and with better rates.
2. However, for faster converging sequences we do not expect alternating sign to affect the convergence rate. For example, if \(\left|\frac{\prod_{j=1}^{n-1}\left(-b\left(k\right)\right)}{q_{n}q_{n-1}} \right|=\lambda^{m}\) for some \(0<\lambda<1\), then with only positive signs the limit will be \(\frac{\lambda^{n}}{1+\lambda}\) while for alternating signs it will be \(\frac{\left(-\lambda\right)^{n}}{1+\lambda}\), so in any case the convergence rate is exponential.

### Growth rate of \(\prod_{k=1}^{m-1}\left|b\left(k\right)\right|\)

**Claim 1**.: Let \(b\left(x\right)\) be a polynomial of degree \(d\), with leading coefficient of absolute value \(B\). Then there exists a constant \(C>0\) such that for any integer \(N\) we have

\[\left(Ne\right)^{-C}\leq\prod_{k=1}^{N}\left|\frac{b\left(k\right)}{Bk^{d}} \right|\leq\left(Ne\right)^{C}.\]

**Proof 1**.: We may assume that the leading coefficient of \(b\) is positive. Writing \(b\left(x\right)=\sum_{0}^{d}b_{j}x^{j}\) with \(b_{d}=B\neq 0\), we want to approximate the product (of the absolute value) of

\[\tilde{b}\left(k\right)=1+\sum_{0}^{d-1}\frac{b_{j}}{B}\frac{1}{k^{d-j}}.\]

Hence, we can find an integer constant \(C_{0}\geq 1\) such that for all \(k\geq 1\) we have

\[\left(1-\frac{C_{0}}{k}\right)\leq\left|\tilde{b}\left(k\right)\right|\leq \left(1+\frac{C_{0}}{k}\right).\]

For all \(k\) large enough, all the expression above are positive, so we get

\[\ln\left(1-\frac{C_{0}}{k}\right)\leq\ln\left|\tilde{b}\left(k\right)\right| \leq\ln\left(1+\frac{C_{0}}{k}\right).\]

With the goal of summing up these expressions from \(1\) to infinity, we claim that there is some constant \(M>0\) such that for any \(C^{\prime}\in\mathbb{R}\), and \(2\left|C^{\prime}\right|\leq n<N\) we have that

\[\left|\sum_{n}^{N}\ln\left(1+\frac{C^{\prime}}{k}\right)-C^{\prime}\ln\left( \frac{N}{n-1}\right)\right|\leq M.\] (13)Given this claim we conclude that

\[-\left(C_{0}\ln\left(N\right)+\left[M-C_{0}\ln\left(2C_{0}\right)\right]\right) \leq\sum_{k=2C_{0}+1}^{N}\ln\left|\frac{b\left(k\right)}{Bk^{d}}\right|\leq C_{ 0}\ln\left(N\right)+\left[M-C_{0}\ln\left(2C_{0}\right)\right].\]

For another \(C\) large enough (independent of \(N\)), we can start the summation from \(k=1\) to get

\[-C\left(\ln\left(N\right)+1\right)\leq\sum_{k=1}^{N}\ln\left|\tilde{b}\left(k \right)\right|\leq C\left(\ln\left(N\right)+1\right).\]

Finally, exponenting it back we get the result we wanted:

\[\left(Ne\right)^{-C}\leq\prod_{k=1}^{N}\left|\tilde{b}\left(k\right)\right| \leq\left(Ne\right)^{C}.\]

We are left to prove Equation (13).

Using the Taylor expansion of \(\ln\left(1+x\right)\) for \(\left|x\right|\leq\frac{1}{2}\), we know that there is some large enough \(0<M_{0}\) such that

\[\left|\ln\left(1+x\right)-x\right|\leq M_{0}x^{2}.\]

It follows that for \(2\left|C^{\prime}\right|\leq n<N\) we have

\[\left|\sum_{k=n}^{N}\left(\ln\left(1+\frac{C^{\prime}}{k}\right)-\frac{C^{ \prime}}{k}\right)\right|\leq M_{0}C^{\prime 2}\sum_{n}^{N}\frac{1}{k^{2}}\leq M _{0}C^{\prime 2}\zeta\left(2\right).\]

In addition, we have that \(\left|\sum_{n}^{N}\frac{1}{k}-\int_{n-1}^{N}\frac{1}{x}\mathrm{d}x\right|\leq 1\), and

\[\int_{n-1}^{N}\frac{1}{x}\mathrm{d}x=\ln\left(\frac{N}{n-1}\right).\]

Therefore

\[\left|\sum_{n}^{N}\ln\left(1+\frac{C^{\prime}}{k}\right)-C^{\prime}\ln\left( \frac{N}{n-1}\right)\right|\leq\left|C^{\prime}\right|+M_{0}C^{\prime 2}\zeta \left(2\right)\]

is uniformly bounded.

### Growth rate of \(q_{n}\)

The sequence \(q_{n}\) satisfies the linear recurrence

\[q_{n+1}=a\left(n\right)q_{n}+b\left(n\right)q_{n-1},\]

or in matrix form

\[\left(q_{n},q_{n+1}\right)=\left(q_{n-1},q_{n}\right)\overbrace{\left(0 \quad b\left(n\right)\right)}^{M\left(n\right)}.\]

If both \(a\left(x\right),b\left(x\right)\) are constant, and therefore \(M=M\left(n\right)\) is a constant matrix, then this problem reduces to simply \(\left(q_{n},q_{n+1}\right)=\left(q_{0},q_{1}\right)M^{n}\). Its a standard exercise to approximate \(q_{n}\) using the eigenvectors decomposition of \(M\). However, in general not only \(M\left(n\right)\) is non-constant, its entries have different orders of magnitude.

Thus, we would like to move to an "equivalent" system where at the very least \(M\left(n\right)\) converges to some matrix \(M_{\infty}\), and then hope to show that the behavior of \(q_{n}\) can be read from the system with \(M_{\infty}^{n}\). This equivalent system will be built in two steps: first we "balance" the matrix, so its coordinates growth rate are the same, and then taking it outside as a scalar, the remaining sequence of matrices will converge.

#### f.3.1 Matrix balancing

This balancing is split into two cases according to the degrees of \(d_{a}=\deg\left(a\left(x\right)\right),d_{b}=\deg\left(b\left(x\right)\right)\).

Let \(d=\max\left\{d_{a},\frac{1}{2}d_{b}\right\}\) and denote by \(A,B\) the coefficients of \(x^{d},x^{2d}\) of \(a\left(x\right),b\left(x\right)\) respectively. Note that both \(A,B\) are either the corresponding leading coefficients or zero, depending on whether \(d_{a}=d\), respectively \(d_{b}=2d\). If \(2d_{a}<d_{b}\) and \(d_{b}\) is odd, then \(d_{a}<d=\frac{d_{b}}{2}\), and we still consider \(A\) to be zero. Regardless of the choice of \(d\), we see that at least one of \(A\) or \(B\) is not zero (and both if \(2d_{a}=d_{b}\), which we call a "balanced" PCF).

With this choice, taking \(\tilde{q}_{n}=\frac{q_{n}}{\left(n\right)^{d}}\), we obtain the linear recurrence

\[\tilde{q}_{n+1}=\frac{a\left(n\right)}{\left(n+1\right)^{d}}\tilde{q}_{n}+ \frac{b\left(n\right)}{\left(n\left(n+1\right)\right)^{d}}\tilde{q}_{n-1}.\]

Letting \(\tilde{a}\left(n\right)=\frac{a\left(n\right)}{\left(n+1\right)^{d}}\) and \(\tilde{b}\left(n\right)=\frac{b\left(n\right)}{\left(n\left(n+1\right) \right)^{d}}\), by our choice of \(d\) we see that the coefficient or the recurrence converge, and not both to zero:

\[\lim_{n\rightarrow\infty}\tilde{a}\left(n\right) =A\] \[\lim_{n\rightarrow\infty}\tilde{b}\left(n\right) =B.\]

Here too we can also write it in a matrix form, namely

\[\left(\tilde{q}_{n},\tilde{q}_{n+1}\right)=\left(\tilde{q}_{n-1},\tilde{q}_{n }\right)\begin{pmatrix}0&\tilde{b}\left(n\right)\\ 1&\tilde{a}\left(n\right)\end{pmatrix}.\]

We now have a limit matrix, and the dynamics of such a matrix is well known. If both eigenvalues are real which are distinct in absolute value, then we expect exponential convergence. If both are non real, and therefore complex conjugate we expect it to behave like a rotation, and therefore will not converge. In both of these cases, since the eigenvalues are distinct in the limit, this holds for almost all \(n\), so this behavior should hold in general.

In the discriminant zero, the situation is much more delicate, since we can converge to zero in many ways. For example, the discriminant along the way can be negative, positive or zero. In this notes we will restrict the study only to the two real eigenvalues with different absolute values.

#### f.3.2 Asymptotics of the continued fraction recurrence

The main goal of this section is to approximate the growth rate of a solution \(u_{n}\) to the recurrence

\[u_{n+1}=a_{n}u_{n}+b_{n}u_{n-1},\]

where both \(a_{n},b_{n}\) converge (and not both to zero) or in matrix form

\[\left(\begin{smallmatrix}v_{n}&v_{n+1}\end{smallmatrix}\right)=\left(\begin{smallmatrix} v_{n-1}&v_{n}\end{smallmatrix}\right)M_{n}\quad,\quad M_{n}=\left(\begin{smallmatrix} 0&b_{n}\\ 1&a_{n}\end{smallmatrix}\right),\]

where \(M_{n}\to M:=\begin{pmatrix}0&b\\ 1&a\end{pmatrix}\).

The first step is the standard conjugation to a simpler matrix. Indeed, if \(D=PMP^{-1}\) is simpler, e.g. diagonal, then \(D_{n}:=PM_{n}P^{-1}\to D\), and \(\prod_{1}^{n}M_{i}=P^{-1}\prod_{1}^{n}D_{i}P\), so we more or less need to understand \(\prod_{1}^{n}D_{i}\).

In the constant diagonal case \(D_{n}=\begin{pmatrix}\lambda_{1}&0\\ 0&\lambda_{2}\end{pmatrix}\) with \(\lambda_{1}>|\lambda_{2}|\), we expect that for almost every initial condition \(\left\|\left(\alpha_{1},\beta_{1}\right)D^{k}\right\|\sim\lambda_{1}^{k}\). This is true as long as the initial vector is not in \(\mathbb{R}\cdot e_{2}\), and we have similar behaviour for other type of matrices. When the \(D_{n}\) are not constant, we need to take a little bit more care. The image you should have in mind is the following:

Instead of the two eigenvectors being on the \(X\) and \(Y\) axes, they only converge to it, so we only know that they are somewhere inside the red and blue regions. Thus, to understand this system we first need a **separation condition** saying that these regions are disjoint. Assuming the \(X\)-axis is thepulling axis (larger eigenvalue), we will need at least one point outside the error region around the \(Y\) axis, which we call the **initial condition**. Once both these conditions hold, a standard investigation of diagonalizable product will show that the point's orbit converge towards the eigenvector in the \(X\)-region. As this region shrinks to \(X\) in the limit, we see that the limit of the orbit is there as well.

**Lemma 1**.: Suppose that \(D_{n}\to D\) where \(D=\left(\begin{smallmatrix}\lambda_{+}&0\\ 0&\lambda_{-}\end{smallmatrix}\right)\) with \(0\leq\left|\frac{\lambda_{-}}{\lambda_{+}}\right|<1\) and let \(\kappa_{n}=\frac{1}{\left|\lambda_{+}\right|}\max_{k\geq n}\left\|D_{k}-D \right\|_{\infty}\).

Fix some initial \(z_{1}\) and let \(z_{k}=\left(z_{1}\right)\prod_{1}^{k-1}D_{n}\). Assuming that for some \(n\) we have

* **Separation condition**: \(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+4\kappa_{n}<1\) and
* **Initial condition**: \(\left|z_{n}\right|<\frac{\mu_{n}+\sqrt{\left(\mu_{n}-2\kappa_{n}\right)\left( \mu_{n}+2\kappa_{n}\right)}}{2\kappa_{n}},\;\mu_{n}=1-\left|\frac{\lambda_{-}} {\lambda_{+}}\right|-2\kappa_{n}\).

Then \(\lim_{k\rightarrow\infty}\left|z_{k}\right|=0\).

**Remark 2**.: Note that \(\frac{\mu_{n}+\sqrt{\left(\mu_{n}-2\kappa_{n}\right)\left(\mu_{n}+2\kappa_{n} \right)}}{2\kappa_{n}}\sim\frac{1-\left|\lambda\right|}{\kappa_{n}}\rightarrow\infty\) as \(\kappa_{n}\to 0\), so this initial condition becomes easier to satisfy as \(n\rightarrow\infty\).

Proof.: First, proving the claim for \(\frac{1}{\lambda_{+}}D_{n}\) instead of \(D_{n}\), we may assume that the limit is \(D=\begin{pmatrix}1&0\\ 0&\lambda\end{pmatrix}\) where \(\lambda=\frac{\lambda_{-}}{\lambda_{+}}\).

Next, note that whenever \(\mu:=1-\left|\lambda\right|-2\kappa>2\kappa>0\), we have that \(\sqrt{\left(\mu-2\kappa\right)\left(\mu+2\kappa\right)}=\sqrt{\mu^{2}-4\kappa^ {2}}<\mu\). Setting

\[\nu_{\pm}\left(\kappa\right)=\frac{\mu_{\kappa}\pm\sqrt{\left(\mu_{\kappa}-2 \kappa\right)\left(\mu_{\kappa}+2\kappa\right)}}{2\kappa},\]

we get that \(0<\nu_{-}\left(\kappa\right)<\nu_{+}\left(\kappa\right)\) are real numbers, and the condition in the assumption is \(\left|z_{n}\right|<\nu_{+}\left(\kappa_{n}\right)\). Our main goal is to prove our process satisfies:

1. \(\left|z_{k}\right|<\nu_{+}\left(\kappa_{n}\right)\) for all \(k\geq n\) and,
2. We have \(\limsup_{k}\left|z_{k}\right|\leq\nu_{-}\left(\kappa_{n}\right).\)

Assuming these two steps are true, the full proof is not too far behind. Indeed, since \(D_{n}\to D\), the sequence \(\kappa_{n}:=\sup\limits_{k\geq n}\left\|D_{n}-D\right\|\) converges to zero, and note that as \(\kappa_{n}\to 0\) we get that \(\nu_{+}\left(\kappa_{n}\right)\nearrow\infty\) and \(\nu_{-}\left(\kappa_{n}\right)\searrow 0\). Assuming step \((1)\), for \(k\geq m\geq n\) we have \(\left|z_{k}\right|<\nu_{+}\left(\kappa_{n}\right)\leq\nu_{+}\left(\kappa_{m}\right)\), and by step \((2)\) we get that \(\limsup_{k}\left|z_{k}\right|\leq\nu_{-}\left(\kappa_{m}\right)\to 0\).

For the remaining of the proof, without loss of generality we may assume that \(n=1\) and just write \(\kappa,\mu\) instead of \(\kappa_{n},\mu_{n}\).

Figure 8: Convergence with variable coefficients

To prove these two steps, consider the change from \(z_{k}\) to \(z_{k+1}\). Writing \(D_{k}=\left(\begin{smallmatrix}1+\varepsilon_{1,1}&\varepsilon_{1,2}\\ \varepsilon_{2,1}&\lambda+\varepsilon_{2,2}\end{smallmatrix}\right)\), since \(z_{k+1}=\left(z_{k}\right)D_{k}\) and \(\left\|D-D_{k}\right\|_{\infty}\leq\kappa\), we get that

\[\left|z_{k+1}\right|=\left|\frac{\varepsilon_{1,2}+z_{k}\left(\lambda+ \varepsilon_{2,2}\right)}{\left(1+\varepsilon_{1,1}\right)+z_{k}\varepsilon_ {2,1}}\right|\leq\frac{\kappa+\left|z_{k}\right|\left(\left|\lambda\right|+ \kappa\right)}{1-\kappa-\kappa\left|z_{k}\right|}.\]

Note that the final denominator is positive, so that the last inequality is valid. Indeed, using the conditions of the claim we get

\[1-\kappa\left(1+\left|z_{k}\right|\right)\geq 1-\kappa\left(1+\frac{\mu+\sqrt{ \left(\mu-2\kappa\right)\left(\mu+2\kappa\right)}}{2\kappa}\right)>1-\left( \kappa+\mu\right)=\left|\lambda\right|+\kappa>0.\]

Thus, we can rewrite the inequality as

\[\left|z_{k+1}\right|\leq M_{\varepsilon}\left(\left|z_{k}\right|\right),\quad M _{\varepsilon}=\left(\begin{matrix}\left|\lambda\right|+\kappa&\kappa\\ -\kappa&1-\kappa\end{matrix}\right).\] (14)

The goal now is to show that if \(\left|z_{k}\right|\) is "large", then \(\left|z_{k+1}\right|\) is much smaller, and if \(\left|z_{k}\right|\) is small, then \(\left|z_{k+1}\right|\) cannot increase too much.

A simple computations shows that the eigenvalues of this matrix are

\[\gamma_{\pm}=\frac{\left|\lambda\right|+1\pm\sqrt{\left(\mu+2\kappa\right) \left(\mu-2\kappa\right)}}{2},\]

and since \(\sqrt{\left(\mu+2\kappa\right)\left(\mu-2\kappa\right)}\leq\mu\leq 1-\left| \lambda\right|\), we get that

\[\gamma_{+}>\gamma_{-}>0.\]

Finally, the corresponding (right) eigenvectors are

\[v_{\pm}=\left(\begin{matrix}\nu_{\mp}\\ 1\end{matrix}\right).\]

To simplify the notations, let us conjugate by the matrix \(T=\left(\begin{matrix}\nu_{+}&\nu_{-}\\ 1&1\end{matrix}\right)\) to obtain

\[T^{-1}M_{\varepsilon}T=\left(\begin{matrix}\gamma_{-}&0\\ 0&\gamma_{+}\end{matrix}\right).\]

Note that the Mobius map

\[T^{-1}\left(z\right):=\frac{1}{\nu_{+}-\nu_{-}}\left(\begin{matrix}1&-\nu_{-} \\ -1&\nu_{+}\end{matrix}\right)\left(z\right)=-\frac{z-\nu_{-}}{z-\nu_{+}}=-1+ \frac{\nu_{-}-\nu_{+}}{z-\nu_{+}}\]

sends \(\nu_{-}\mapsto 0\), \(\nu_{+}\mapsto\infty\) and \(0\mapsto-\frac{\nu_{-}}{\nu_{+}}<0\). In particular, it is monotone increasing on \([0,\nu_{+})\), so that our two steps from above are equivalent to

1. \(T^{-1}\left(\left|z_{k}\right|\right)\in[-\frac{\nu_{-}}{\nu_{+}},\infty)\),
2. \(\limsup_{k}T^{-1}\left(\left|z_{k}\right|\right)\in\left[-\frac{\nu_{-}}{\nu _{+}},0\right],\)

and the claim's original assumption is that \(T^{-1}\left(\left|z_{1}\right|\right)\in[-\frac{\nu_{-}}{\nu_{+}},\infty)\). However, now this claim is simple, since in these notations we get that

\[T^{-1}\left(M_{\varepsilon}\left(\left|z_{k}\right|\right)\right)=\left(T^{-1 }M_{\varepsilon}T\right)\left(T^{-1}\left(\left|z_{k}\right|\right)\right)= \frac{\gamma_{-}}{\gamma_{+}}\cdot T^{-1}\left(\left|z_{k}\right|\right),\]

and \(0<\frac{\gamma_{-}}{\gamma_{+}}<1\). Thus, if \(T^{-1}\left(\left|z_{k}\right|\right)\in[-\frac{\nu_{-}}{\nu_{+}},\infty)\), then so is \(T^{-1}\left(M_{\varepsilon}\left(\left|z_{k}\right|\right)\right)\in[-\frac{ \nu_{-}}{\nu_{+}},\infty)\), so by Equation (14) and the monotonicity of \(T\), we obtain that

\[T^{-1}\left(\left|z_{k+1}\right|\right)\leq\frac{\gamma_{-}}{\gamma_{+}}\cdot T ^{-1}\left(\left|z_{k}\right|\right),\]

which implies the two steps.

Returning back to the recursion, we get the following.

**Theorem 1**.: Suppose that we have a solution to the recurrence \(v_{n+1}=a_{n}v_{n}+b_{n}v_{n-1}\), where \(a_{n}\to a,b_{n}\to b\) and suppose that \(\lambda_{\pm}\) are the roots of \(x^{2}=ax+b\) with \(0\leq|\lambda_{-}|<\lambda_{+}\). Writing \(\kappa_{n}^{\prime}=\frac{1}{|\lambda_{+}|}\underset{k\geq n}{\max}\max\left\{ \left|a_{k}-a\right|,\left|b_{k}-b\right|\right\}\) and \(C\left(\lambda_{\pm}\right):=\frac{1+|\lambda_{+}|}{|\lambda_{+}-\lambda_{-}|}\), Assume that for some \(n\) we have

* **Separation condition**: \(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+4C\left(\lambda_{\pm}\right)\kappa _{n}^{\prime}<1\) and
* **Initial condition**: \(\left|\lambda_{-}-\frac{v_{n}}{v_{n-1}}\right|\geq C\left(\lambda_{\pm}\right) \kappa_{n}^{\prime}\frac{|\lambda_{+}-\lambda_{-}|}{1-\left|\frac{\lambda_{-} }{\lambda_{+}}\right|-4C\left(\lambda_{\pm}\right)\kappa_{n}^{\prime}},\)

Then

\[\frac{v_{n}}{v_{n-1}}\rightarrow\lambda_{+}.\]

**Proof 2**.: Set \(M_{n}=\left(\begin{smallmatrix}0&b_{n}\\ 1&a_{n}\end{smallmatrix}\right)\) and \(M=\left(\begin{smallmatrix}0&b\\ 1&a\end{smallmatrix}\right)\) as in the beginning of this section. With \(P=\left(\begin{smallmatrix}1&\lambda_{+}\\ 1&\lambda_{-}\end{smallmatrix}\right)\) and \(P^{-1}=\frac{1}{\lambda_{-}-\lambda_{+}}\left(\begin{smallmatrix}\lambda_{-}&- \lambda_{+}\\ -1&1\end{smallmatrix}\right)\) we have that \(D=PMP^{-1}=\left(\begin{smallmatrix}\lambda_{+}&0\\ 0&\lambda_{-}\end{smallmatrix}\right)\). We would like to apply Lemma 1 to the matrices \(D_{n}=PM_{n}P^{-1}\).

For the **separation condition** on the infinity norm, we have

\[\left\|PM_{n}P^{-1}-D\right\|_{\infty}=\left\|P\left(M_{n}-M\right)P^{-1} \right\|_{\infty}=\frac{1}{|\lambda_{-}-\lambda_{+}|}\left\|\left(\begin{smallmatrix} 1&\lambda_{+}\\ 1&\lambda_{-}\end{smallmatrix}\right)\left(\begin{smallmatrix}0&b_{n}-b\\ 0&a_{n}-a\end{smallmatrix}\right)\left(\begin{smallmatrix}\lambda_{-}&- \lambda_{+}\\ -1&1\end{smallmatrix}\right)\right\|_{\infty}\]

\[=\frac{1}{|\lambda_{-}-\lambda_{+}|}\left\|\left(\begin{smallmatrix}1&\lambda_{ +}\\ 1&\lambda_{-}\end{smallmatrix}\right)\left(\begin{smallmatrix}b-b_{n}&b_{n}-b\\ a-a_{n}&a_{n}-a\end{smallmatrix}\right)\right\|_{\infty}\leq\frac{\overbrace{1+ |\lambda_{+}|}^{C\left(\lambda_{\pm}\right)}}{|\lambda_{+}-\lambda_{-}|}\left\| M_{n}-M\right\|_{\infty}.\]

Thus, the separation condition of this theorem implies the separation condition of Lemma 1:

\[\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+4\kappa_{n}\leq\left|\frac{ \lambda_{-}}{\lambda_{+}}\right|+4C\left(\lambda_{\pm}\right)\frac{1}{|\lambda _{+}|}\max_{k\geq n}\left\|M_{n}-M\right\|_{\infty}<1\]

Next, for the **initial condition**, setting

\[\left(v_{k-1}\ v_{k}\right):=\left(v_{0}\ v_{1}\right)\left(\prod_{1}^{k-1}M_{ n}\right)=\left(v_{0}\ v_{1}\right)P^{-1}\left(\prod_{1}^{k-1}D_{n}\right)P\]

we have

\[\left(\alpha_{k},\beta_{k}\right)=\left(v_{0}\ v_{1}\right)P^{-1}\left(\prod_{ 1}^{k-1}D_{n}\right)=\left(v_{k-1},v_{k}\right)P^{-1}.\]

Setting \(z_{n}=\frac{\beta_{n}}{\alpha_{n}}\), we get that

\[\left|z_{n}\right|=\left|\frac{\beta_{n}}{\alpha_{n}}\right|=\left|\frac{- \lambda_{+}v_{n-1}+v_{n}}{\lambda_{-}v_{n-1}-v_{n}}\right|=\left|1+\frac{ \lambda_{+}-\lambda_{-}}{\lambda_{-}-\frac{v_{n}}{v_{n-1}}}\right|\leq 1+ \left|\frac{\lambda_{+}-\lambda_{-}}{\lambda_{-}-\frac{v_{n}}{v_{n-1}}}\right|= \left(\ast\right).\]

Using the assumption that \(\left|\lambda_{-}-\frac{v_{n}}{v_{n-1}}\right|\geq C\left(\lambda_{\pm}\right) \kappa_{n}^{\prime}\frac{|\lambda_{+}-\lambda_{-}|}{1-\left|\frac{\lambda_{-}}{ \lambda_{+}}\right|-4C\left(\lambda_{\pm}\right)\kappa_{n}^{\prime}}\geq \kappa_{n}\frac{|\lambda_{+}-\lambda_{-}|}{1-\left|\frac{\lambda_{-}}{\lambda_{ +}}\right|-4\kappa_{n}},\) we see that the expression above is

\[\left(\ast\right)\leq 1+\frac{|\lambda_{+}-\lambda_{-}|}{\kappa_{n}\frac{| \lambda_{+}-\lambda_{-}|}{1-\left|\frac{\lambda_{+}}{\lambda_{+}}\right|-4 \kappa_{n}}}=1+\frac{1-\left|\frac{\lambda_{-}}{\lambda_{+}}\right|-4\kappa_{ n}}{\kappa_{n}}=\frac{2\mu_{n}-2\kappa_{n}}{2\kappa_{n}}<\frac{\mu_{n}+\sqrt{ \left(\mu_{n}-2\kappa_{n}\right)\left(\mu_{n}+2\kappa_{n}\right)}}{2\kappa_{n} }.\]

This was the second condition needed for Lemma 1, so we can now conclude that

\[\left|1+\frac{\lambda_{+}-\lambda_{-}}{\lambda_{-}-\frac{v_{n}}{v_{n-1}}}\right|= \left|\frac{\beta_{n}}{\alpha_{n}}\right|\to 0\]

which implies that \(\frac{v_{n}}{v_{n-1}}\rightarrow\lambda_{+}\).

### Conclusion

We return now to the original problem with \(\alpha=\mathbb{K}_{1}^{\infty}\frac{b\left(n\right)}{a\left(n\right)}\) and assume that \(a\left(x\right),b\left(x\right)\) have degrees \(d_{a},d_{b}\). As mentioned before, we split our study into two cases:

#### The balanced case

Assume that \(d_{b}=2d_{a}=2d\), and let \(A,B\) be the leading coefficients of \(a\left(x\right),b\left(x\right)\) respectively.

In this case the limit matrix is \(M_{\infty}=\begin{pmatrix}0&B\\ 1&A\end{pmatrix}\), and we assume that the roots \(\lambda_{\pm}\) of \(x^{2}=Ax+B\) satisfy \(0<\left|\lambda_{-}\right|<\lambda_{+}\). Using Theorem 1 once the two conditions hold, we obtain

\[\frac{q_{n+1}}{q_{n}}\left(n+1\right)^{d}=\frac{q_{n+1}/\left(n+1\right)!^{d}} {q_{n}/n!^{d}}\rightarrow\lambda_{+},\]

implying that \(q_{n}=n!^{d}\lambda_{+}^{n}\exp\left(o\left(n\right)\right)\). As for the product of the \(b\left(k\right)\), using Claim 1 we have that

\[\prod_{k=1}^{N}\left|b\left(k\right)\right|=\exp\left(o\left(N\right)\right) \cdot B^{N}\cdot N!^{2d}.\]

Putting them together as in the error rate expression, we get :

\[\frac{\prod_{k=1}^{m-1}\left|b\left(k\right)\right|}{|q_{m-1}q_{m}|}=\frac{|B |^{m-1}\cdot\left(m-1\right)!^{2d}}{\left(m-1\right)!^{d}\left(m\right)!^{d} \lambda_{+}^{2m-1}}\exp\left(o\left(m\right)\right)=\left(\ast\right).\]

Note that \(|B|=\left|\det\left(M_{\infty}\right)\right|=\left|\lambda_{-}\lambda_{+}\right|\), so that the expression above is

\[\left(\ast\right)=\left|\lambda_{-}/\lambda_{+}\right|^{m}\cdot\exp\left(o \left(m\right)\right)=\exp\left(m\log\left|\lambda_{-}/\lambda_{+}\right|+o \left(m\right)\right).\]

Thus, for given \(\varepsilon>0\) where \(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+\varepsilon<1\), and for any \(m\) large enough we see that \(\left(\ast\right)\leq\left(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+ \varepsilon\right)^{m-1}\). We conclude that the error rate for all \(n\) large enough is bounded from above by

\[\left|\frac{p_{n}}{q_{n}}-\alpha\right|\leq\sum_{m=n+1}^{\infty}\frac{\prod_{ k=1}^{m-1}\left|b\left(k\right)\right|}{|q_{m-1}q_{m}|}\leq\sum_{m=n+1}^{\infty} \left(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+\varepsilon\right)^{m-1}= \left(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+\varepsilon\right)^{n} \frac{1}{1-\left(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|+\varepsilon \right)}.\]

It follows that

\[\ln\left|\frac{p_{n}}{q_{n}}-\alpha\right|\leq n\ln\left(\left(\left|\frac{ \lambda_{-}}{\lambda_{+}}\right|+\varepsilon\right)\right)-\ln\left(1-\left( \left|\frac{\lambda_{-}}{\lambda_{+}}\right|+\varepsilon\right)\right)\sim n \ln\left(\left|\frac{\lambda_{-}}{\lambda_{+}}\right|\right).\]

#### The unbalanced case

Suppose now that \(d_{b}<2d_{a}=2d\), so that \(B=\lim\limits_{n\rightarrow\infty}\frac{b\left(n\right)}{\left(n\left(n+1 \right)\right)^{d_{a}}}=0\). This time the two roots of \(x^{2}=Ax+0\) are \(\lambda=0,A\). If needed, we can use a simple continued fraction inflation \(\mathbb{K}_{1}^{\infty}\frac{\left(-1\right)^{2}b\left(n\right)}{\left(-1 \right)a\left(n\right)}\) and assume that \(A>0\). Using Theorem 1, if the two conditions hold, we obtain \(q_{n}=n!^{d}A^{n}\exp\left(o\left(n\right)\right)\).

Letting \(\hat{B}\) be the leading coefficient of \(b\left(x\right)\) in absolute value, Claim 1 implies that

\[\prod_{k=1}^{N}\left|b\left(k\right)\right|=\exp\left(o\left(N\right)\right) \cdot\hat{B}^{N}\cdot N!^{d_{b}}.\]

Again, together we obtain that

\[\frac{\prod_{k=1}^{m-1}\left|b\left(k\right)\right|}{|q_{m-1}q_{m}|}=\frac{ \hat{B}^{m-1}\cdot\left(m-1\right)!^{d_{b}}}{\left(m-1\right)!^{d}m!^{d}A^{2m- 1}}\exp\left(o\left(m\right)\right)=\frac{1}{\left(m-1\right)!^{2d_{a}-d_{b}}} \cdot\left(\frac{\hat{B}}{A^{2}}\right)^{m}\exp\left(o\left(m\right)\right)\]Similarly to the previous case, given \(\varepsilon>0\), and using the fact that \(2d_{a}-d_{b}\geq 1\), for all \(n\) large enough we obtain

\[\left|\frac{p_{n}}{q_{n}}-\alpha\right| \leq\sum_{m=n+1}^{\infty}\frac{\prod_{k=1}^{m-1}\left|b\left(k \right)\right|}{|q_{m-1}q_{m}|}\leq\sum_{m=n+1}^{\infty}\frac{1}{(m-1)!^{2d_{a }-d_{b}}}\cdot\left(\frac{\hat{B}}{A^{2}}+\varepsilon\right)^{m-1}\] \[=\frac{1}{n!^{2d_{a}-d_{b}}}\left(\frac{\hat{B}}{A^{2}}+\varepsilon \right)^{n}\sum_{m=0}^{\infty}\left(\frac{n!}{(n+m)!}\right)^{2d_{a}-d_{b}} \cdot\left(\frac{\hat{B}}{A^{2}}+\varepsilon\right)^{m}\] \[\leq\frac{1}{n!^{2d_{a}-d_{b}}}\left(\frac{\hat{B}}{A^{2}}+ \varepsilon\right)^{n}\left[\sum_{m=0}^{\infty}\left(\frac{1}{m!}\right)^{2d_ {a}-d_{b}}\cdot\left(\frac{\hat{B}}{A^{2}}+\varepsilon\right)^{m}\right].\]

The infinite sum in the last expression converges to some finite limit \(\tilde{C}\), so we conclude that

\[\ln\left|\frac{p_{n}}{q_{n}}-\alpha\right|\leq(d_{b}-2d_{a})\ln \left(n!\right)+n\ln\left|\frac{\hat{B}}{A^{2}}+\varepsilon\right|+\ln\left| \tilde{C}\right|\sim(d_{b}-2d_{a})\,n\cdot\ln\left|n\right|.\]

## Appendix G List of Proven Formulas

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \(A_{n}\) & \(B_{n}\) & \(h_{1}(n)\) & \(h_{2}(n)\) & \(f(n)\) & Known limit of resulting infinite sum \\ \hline \(\omega+1\) & \(n^{2}+\omega n\) & \(-n\) & \(n+\omega\) & \(1\) & \((\omega+1)/_{2}F_{1}(1,1,\omega+2,-1)\) \\ \hline \(5\) & \(n^{2}+4n\) & \(-n\) & \(n+4\) & \(1\) & \(-\frac{12}{131+192\log\left(2\right)}\) \\ \hline \(4\) & \(n^{2}+3n\) & \(-n\) & \(n+3\) & \(1\) & \(\frac{3}{24\log\left(2\right)-16}\) \\ \hline \(\omega\) & \((\omega n+1)^{2}\) & \(-\omega n-1\) & \(\omega n+1\) & \(1\) & \((\omega+1)/_{2}F_{1}(1,\frac{\omega+1}{\omega},\frac{2\omega+1}{\omega},-1)-1\) \\ \hline \(2\) & \(4n^{2}+4n+1\) & \(-2n-1\) & \(2n+1\) & \(1\) & \(\frac{n}{4-n}\) \\ \hline \(1\) & \(n^{2}+2n+1\) & \(-n-1\) & \(n+1\) & \(1\) & \(\frac{\log\left(2\right)}{1-\log\left(2\right)}\) \\ \hline \(\omega\) & \(n^{2}+(\omega+1)n+\omega\) & \(-n-1\) & \(n+\omega\) & \(1\) & \((\omega+1)/_{2}F_{1}(1,2,\omega+2,-1)-1\) \\ \hline \(3\) & \(n^{2}+4n+3\) & \(-n-1\) & \(n+3\) & \(1\) & \(\frac{4}{34-48\log\left(2\right)}-1\) \\ \hline \(2\) & \(n^{2}+3n+2\) & \(-n-1\) & \(n+2\) & \(1\) & \(\frac{3}{9-12\log\left(2\right)}-1\) \\ \hline \(5\) & \(n^{2}+2n\) & \(n\) & \(n+2\) & \(n+\frac{3}{2}\) & \(\frac{2}{17-24\log\left(2\right)}\) \\ \hline \(5\) & \(n^{2}+4n+3\) & \(-n-1\) & \(n+3\) & \(n+\frac{5}{2}\) & \(\frac{3(17-24\log\left(2\right))}{120\log\left(2\right)-83}\) \\ \hline \(4\) & \(n^{2}+n\) & \(-n\) & \(n+1\) & \(n+1\) & \(\frac{1}{3-4\log\left(2\right)}\) \\ \hline \(4\) & \(n^{2}+3n+2\) & \(-n-1\) & \(n+2\) & \(n+2\) & \(\frac{6-8\log\left(2\right)}{16\log\left(2\right)-11}\) \\ \hline \(4\) & \(4n^{2}+4n\) & \(-2n\) & \(2n+2\) & \(1\) & \(\frac{2}{2\log\left(2\right)-1}\) \\ \hline \(3\) & \(n^{2}\) & \(-n\) & \(n\) & \(n+\frac{1}{2}\) & \(\frac{1}{1-\log\left(2\right)}\) \\ \hline \(3\) & \(n^{2}+2n+1\) & \(-n-1\) & \(n+1\) & \(n+\frac{3}{2}\) & \(\frac{1-\log\left(2\right)}{3\log\left(2\right)-2}\) \\ \hline \(3\) & \(n^{2}+4n+4\) & \(-n-2\) & \(-n+2\) & \(n+\frac{5}{2}\) & \(\frac{4(3\log\left(2\right)-2)}{7-10\log\left(2\right)}\) \\ \hline \(1\) & \(n^{2}+4n+4\) & \(n+2\) & \(-n-2\) & \(1\) & \(\frac{2}{2\log\left(2\right)-1}-2\) \\ \hline \(4\) & \(4n^{2}-1\) & \(-2n+1\) & \(2n+1\) & \(1\) & \(\frac{\pi+2}{\pi-2}\) \\ \hline \(2\) & \(4n^{2}-4n-1\) & \(-2n+1\) & \(2n-1\) & \(1\) & \(\frac{4}{\pi}+1\) \\ \hline \end{tabular}
\end{table}
Table 5: This is a table showing the automatically generated conjectured formulas that were analytically proven. For the method of proving, see appendix A.3. Note that lines **1,4** and **7** are proven infinite families of formulas, generalized from the cases found in the dataset.

\begin{tabular}{|c|c|c|c|c|c|} \hline
5 & \(4n^{2}+2n-2\) & \(-2n-1\) & \(2n+2\) & \(1\) & \(\frac{22+12\sqrt{2}}{7}\) \\ \hline
5 & \(4n^{2}+2n\) & \(-2n-1\) & \(2n\) & \(n+\frac{3}{4}\) & \(3+2\sqrt{2}\) \\ \hline
3 & \(4n^{2}-2n\) & \(-2n+1\) & \(2n\) & \(1\) & \(2+\sqrt{2}\) \\ \hline \(2n^{2}+2n+1\) & \(-n^{4}\) & \(n^{2}\) & \(n^{2}\) & \(1\) & \(\frac{1}{\zeta(2)}\) \\ \hline \(2n+1\) & \(n^{4}\) & \(-n^{2}\) & \(n^{2}\) & \(1\) & \(\frac{2}{\zeta(2)}\) \\ \hline \(n^{2}+n+1\) & \(n^{4}+n^{2}+1\) & \(\phi(n^{2}+n+1)\) & \((\phi-1)(-n^{2}+n-1)\) & \(1\) & \(\phi\) \\ \hline \(2n^{2}+2n+2\) & \(n^{4}+n^{2}+1\) & \((\sqrt{2}+1)(n^{2}+n+1)\) & \((\sqrt{2}-1)(-n^{2}+n-1)\) & \(1\) & \(1+\sqrt{2}\) \\ \hline \(2n^{2}+2n+2\) & \(2n^{4}+2n^{2}+2\) & \((\sqrt{3}+1)(n^{2}+n+1)\) & \((\sqrt{3}-1)(-n^{2}+n-1)\) & \(1\) & \(1+\sqrt{3}\) \\ \hline \(-5\) & \(n^{2}+2n\) & \(-n\) & \(-n-2\) & \(n+\frac{3}{2}\)) & \(-\frac{5}{\frac{55}{2}-60\log(2)}\) \\ \hline \(-5\) & \(n^{2}+4n\) & \(n\) & \(-n-4\) & \(1\) & \(-\frac{5}{-\frac{55}{12}+80\log\log\left(2\right)}\) \\ \hline \(-4\) & \(n^{2}+n\) & \(n\) & \(-n-1\) & \(n+1\) & \(-\frac{4}{12-16\log\left(2\right)}\) \\ \hline \(-4\) & \(n^{2}+3n\) & \(n\) & \(-n-3\) & \(1\) & \(-\frac{4}{3+32\log\left(2\right)}\) \\ \hline \(-4\) & \(n^{2}+3n+2\) & \(n+1\) & \(-n-2\) & \(n+2\) & \(\frac{1}{2}-\frac{9}{2\left(-99+144\log\left(2\right)\right)}\) \\ \hline \(-4\) & \(n^{2}+5n+4\) & \(n+1\) & \(-n-4\) & \(1\) & \(1-\frac{5}{\frac{35}{32}-160\log\left(2\right)}\) \\ \hline \(-4\) & \(4n^{2}+4n\) & \(2n\) & \(-2n-2\) & \(1\) & \(-\frac{4}{-2+4\log\left(2\right)}\) \\ \hline \(-3\) & \(n^{2}\) & \(n\) & \(-n-2\) & \(1\) & \(-\frac{3}{3-3\log\left(2\right)}\) \\ \hline \(-3\) & \(n^{2}+2n\) & \(n\) & \(-n-2\) &

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, the main results, motivations and aspirational goals are included in the abstract (section 1) and introduction (section 2). Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations and required assumptions of the Blind-\(\delta\) Algorithm in section 3.4, the dynamical metric estimation via down-sampled curve fitting in Appendix A, and of PCF convergence and the delta-predictor formula (Eq.5) in Appendix F. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that are not acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Most of the results in this work are mathematical hypotheses (numerically validated) - presented without proof. The main theoretical result (Eq.5) is proven in Appendix F. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The data set used was explicitly defined, the algorithmic steps presented and numerical techniques described. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The git repository is linked in this camera-ready version. The data set is mathematical formulas and constants - so freely available to all. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In this work we are using unsupervised recurring clustering as the main machine learning method, and the results are mostly validated using the (test) subset of known PCF formulas for mathematical constants. This subset was included in the full data set but was not given any special treatment during clustering (as mentioned in section 4.1). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The main errors in this work originate in the dynamical metrics measurement stage. Error bars are not shown (for visual clarity), but these errors are discussed in Appendix A, and mentioned in every figure that shows measured dynamical metrics. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: There are no special or extreme resource requirements, so we did not focus on this question, but a brief discussion of required memory and runtime is in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that did not make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This work does not involve human participants, special data sets, does not have implications for the broader public (only for scientists) and cannot be used to do harm. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: Although our work is applied ML, its impact is on mathematical research, not society at large. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The methods in this paper do not pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The only existing assets that were used are open source Python packages, that were properly credited in Appendix A. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code is released as open source with a readme and documentation. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This research does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This research does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.