# Functional Equivalence and Path Connectivity

of Reducible Hyperbolic Tangent Networks

Matthew Farrugia-Roberts

School of Computing and Information Systems

The University of Melbourne

matthew@far.in.net

###### Abstract

Understanding the learning process of artificial neural networks requires clarifying the structure of the parameter space within which learning takes place. A neural network parameter's _functional equivalence class_ is the set of parameters implementing the same input-output function. For many architectures, almost all parameters have a simple and well-documented functional equivalence class. However, there is also a vanishing minority of _reducible_ parameters, with richer functional equivalence classes caused by redundancies among the network's units. In this paper, we give an algorithmic characterisation of unit redundancies and reducible functional equivalence classes for a single-hidden-layer hyperbolic tangent architecture. We show that such functional equivalence classes are piecewise-linear path-connected sets, and that for parameters with a majority of redundant units, the sets have a diameter of at most 7 linear segments.

## 1 Introduction

Deep learning algorithms construct neural networks through a local search in a high-dimensional parameter space. This search is guided by the shape of some loss landscape, which is in turn determined by the link between neural network parameters and their input-output functions. Thus, understanding the link between parameters and functions is key to understanding deep learning.

It is well known that neural network parameters often fail to uniquely determine an input-output function. For example, exchanging weights between two adjacent hidden units generally preserves functional equivalence (Hecht-Nielsen, 1990). For many architectures, almost all parameters have a simple class of functionally equivalent parameters. These classes have been characterised for multi-layer feed-forward architectures with various nonlinearities (e.g., Sussmann, 1992; Albertini et al., 1993; Kurkova and Kainen, 1994; Phuong and Lampert, 2020; Vlacic and Bolcskei, 2021).

However, all existing work on functional equivalence excludes from consideration certain measure zero sets of parameters, for which the functional equivalence classes may be richer. One such family of parameters is the so-called _reducible parameters_. These parameters display certain structural redundancies, such that the same function could be implemented with fewer hidden units (Sussmann, 1992; Vlacic and Bolcskei, 2021), leading to a richer functional equivalence class.

Despite their atypicality, reducible parameters may play an important role in deep learning. Learning exerts non-random selection pressure, so measure zero sets of parameters may still arise in practice, and indeed reducible parameters are appealing solutions due to parsimony (cf. Farrugia-Roberts, 2023). These parameters are also a source of information singularities (cf. Fukumizu, 1996), relevant to statistical theories of deep learning (Watanabe, 2009; Wei et al., 2022). Moreover, the structure of functional equivalence classes has implications for the local and global shape of the loss landscape, in ways that may influence learning dynamics _near_ reducible parameters.

In this paper, we study functional equivalence classes for single-hidden-layer networks with the hyperbolic tangent nonlinearity, building on the foundational work of Sussmann (1992) on reducibility in this setting. We offer the following theoretical contributions.

1. In Section 4, we give a formal algorithm producing a canonical representative parameter from any functional equivalence class, by systematically eliminating all sources of structural redundancy. This extends prior algorithms that only handle irreducible parameters.
2. In Section 5, we invert this canonicalisation algorithm to characterise the functional equivalence class of any parameter as a union of simple parameter manifolds. This characterisation extends the well-known result for irreducible parameters.
3. We show that in the reducible case, the functional equivalence class is a piecewise-linear path-connected set--that is, any two functionally equivalent reducible parameters are connected by a piecewise linear path comprising only equivalent parameters (Theorem 6.1).
4. We show that if a parameter has a high degree of reducibility (in particular, if the same function can be implemented using half of the available hidden units), then the number of linear segments required to connect any two equivalent parameters is at most 7 (Theorem 6.3).

While the single-hidden-layer hyperbolic tangent architecture is not immediately relevant to modern deep learning, it enables the first comprehensive analysis of neural network structural redundancy. Moreover, feed-forward layers are a fundamental building block of many modern architectures, and so our analysis is partially informative for such extensions. In Section 7 we discuss such extensions and others, as well as connections to other topics in deep learning including loss landscape analysis, model compression, and singular learning theory.

## 2 Related Work

Sussmann (1992) studied functional equivalence in single-hidden-layer hyperbolic tangent networks, showing that two irreducible parameters are functionally equivalent if and only if they are related by simple operations of exchanging and negating the weights of hidden units. This result was later extended to architectures with a broader class of nonlinearities (Albertini et al., 1993; Kurkova andainen, 1994), to architectures with multiple hidden layers (Fefferman and Markel, 1993; Fefferman, 1994), and to certain recurrent architectures (Albertini and Sontag, 1992, 1993a,b,c). More recently, similar results have been found for ReLU networks (Phuong and Lampert, 2020; Bona-Pellissier et al., 2021; Stock and Gribonval, 2022), and Vlacic and Bolcskei (2021, 2022) have generalised Sussmann's results to a very general class of architectures and nonlinearities. However, all of these results have come at the expense of excluding from consideration certain measure zero subsets of parameters with richer functional equivalence classes.

A similar line of work has documented the global symmetries of the parameter space--bulk transformations of the entire parameter space that preserve all implemented functions. The search for such symmetries was launched by Hecht-Nielsen (1990). Chen et al. (1993, also Chen and Hecht-Nielsen, 1991) showed that in the case of multi-layer hyperbolic tangent networks, all analytic symmetries are generated by unit exchanges and negations. Ruger and Ossen (1997) extended this result to additional sigmoidal nonlinearities. The analyticity condition excludes discontinuous symmetries acting selectively on, say, reducible parameters with richer equivalence classes (Chen et al., 1993).

Ruger and Ossen (1997) provide a canonicalisation algorithm. Their algorithm negates each hidden unit's weights until the bias is positive, and then sorts each hidden layer's units into non-descending order by bias weight. This algorithm is invariant precisely to the exchanges and negations mentioned above, but fails to properly canonicalise equivalent parameters that differ in more complex ways.

To our knowledge, there is one line of work bearing directly on the topic of the functional equivalence classes of reducible parameters. Fukumizu and Amari (2000) and Fukumizu et al. (2019) have catalogued methods of adding a single hidden unit to a neural network while preserving the network's function, and Simsek et al. (2021) have extended this work to consider the addition of multiple hidden units. Though derived under a distinct framing, it turns out that the subsets of parameter space accessible by such unit additions correspond to functional equivalence classes, similar to those we study (though in a slightly different architecture). We note these similarities, especially regarding our contributions (2) and (3), in Remarks 5.4 and 5.5 and Remark 6.2.

Preliminaries

We consider a family of fully-connected, feed-forward neural network architectures with a single input unit, a single biased output unit, and a single hidden layer of \(h\in\mathbb{N}\) biased hidden units with the hyperbolic tangent nonlinearity \(\tanh(z)=(e^{z}-e^{-z})/(e^{z}+e^{-z})\). Such an architecture has a parameter space \(\mathcal{W}_{h}=\mathbb{R}^{3h+1}\). Our results generalise directly to networks with multi-dimensional inputs and outputs, as detailed in Appendix A.

The weights and biases of the network's units are encoded in the parameter vector in the format \((a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)\in\mathcal{W}_{h}\) where for each hidden unit \(i=1,\ldots,h\) there is an _outgoing weight_\(a_{i}\in\mathbb{R}\), an _incoming weight_\(b_{i}\in\mathbb{R}\), and a _bias_\(c_{i}\in\mathbb{R}\), and \(d\in\mathbb{R}\) is an _output unit bias_. Thus each parameter \(w=(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)\in\mathcal{W}_{h}\) indexes a mathematical function \(f_{w}:\mathbb{R}\to\mathbb{R}\) defined as follows:

\[f_{w}(x)=d+\sum_{i=1}^{h}a_{i}\tanh(b_{i}x+c_{i}).\]

Two parameters \(w\in\mathcal{W}_{h},w^{\prime}\in\mathcal{W}_{h^{\prime}}\) are _functionally equivalent_ if and only if \(f_{w}=f_{w^{\prime}}\) as functions on \(\mathbb{R}\) (that is, \(\forall x\in\mathbb{R},f_{w}(x)=f_{w^{\prime}}(x)\)). Functional equivalence is of course an equivalence relation on \(\mathcal{W}_{h}\). Given a parameter \(w\in\mathcal{W}_{h}\), the _functional equivalence class_ of \(w\), denoted \(\mathfrak{F}[w]\), is the set of all parameters in \(\mathcal{W}_{h}\) that are functionally equivalent to \(w\):

\[\mathfrak{F}[w]=\{\,w^{\prime}\in\mathcal{W}_{h}\,|\,f_{w}=f_{w^{\prime}}\,\}.\]

For this family of architectures, the functional equivalence class of almost all parameters is a discrete set fully characterised by simple _unit negation and exchange transformations_\(\sigma_{i},\tau_{i,j}:\mathcal{W}_{h}\to\mathcal{W}_{h}\) for \(i,j=1,\ldots,h\), where

\[\sigma_{i}(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d) =(a_{1},b_{1},c_{1},\ldots,-a_{i},-b_{i},-c_{i},\ldots,a_{h},b_{h },c_{h},d)\] \[\tau_{i,j}(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d) =(a_{1},b_{1},c_{1},\ldots,c_{i-1},a_{j},b_{j},c_{j},a_{i+1},\] \[\ldots,c_{j-1},a_{i},b_{i},c_{i},a_{j+1},\ldots,a_{h},b_{h},c_{h},d).\]

More formally, these transformations generate the full functional equivalence class for all so-called irreducible parameters (Sussmann, 1992). A parameter \(w=(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)\in\mathcal{W}_{h}\) is _reducible_ if and only if it satisfies any of the following conditions (otherwise, \(w\) is _irreducible_):

1. \(a_{i}=0\) for some \(i\), or
2. \(b_{i}=0\) for some \(i\), or
3. \((b_{i},c_{i})=(b_{j},c_{j})\) for some \(i\neq j\), or
4. \((b_{i},c_{i})=(-b_{j},-c_{j})\) for some \(i\neq j\).

Sussmann (1992) also showed that in this family of architectures, reducibility corresponds to _non-minimality_: a parameter \(w\in\mathcal{W}_{h}\) is reducible if and only if \(w\) is functionally equivalent to some \(w^{\prime}\in\mathcal{W}_{h^{\prime}}\) with fewer hidden units \(h^{\prime}<h\). We define the _rank_ of \(w\), denoted \(\operatorname{rank}(w)\), as the minimum number of hidden units required to implement \(f_{w}\):

\[\operatorname{rank}(w)=\min\{\,h^{\prime}\in\mathbb{N}\,|\,\exists w^{\prime }\in\mathcal{W}_{h^{\prime}};\ f_{w}=f_{w^{\prime}}\,\}.\]

Finally, we make use of the following notions of connectivity for a set of parameters. Given a set \(W\subseteq\mathcal{W}_{h}\), define a _piecewise linear path in \(W\)_ as a continuous function \(\rho:[0,1]\to W\) comprising a finite number of linear segments. Two parameters \(w,w^{\prime}\in\mathcal{W}_{h}\) are _piecewise-linear path-connected in \(W\)_, denoted \(w\leftrightsquigarrow w^{\prime}\) (with \(W\) implicit), if there exists a piecewise linear path in \(W\) such that \(\rho(0)=w\) and \(\rho(1)=w^{\prime}\). Note that \(\leftrightsquigarrow\right\) is an equivalence relation on \(W\). A set \(W\subseteq\mathcal{W}_{h}\) is itself _piecewise-linear path-connected_ if and only if \(\leftrightsquigarrow\right\) is the full relation, that is, all pairs of parameters in \(W\) are piecewise linear path-connected in \(W\).

The _length_ of a piecewise linear path is the number of maximal linear segments comprising the path. The _distance_ between two piecewise linear path-connected parameters is the length of the shortest path connecting them. The _diameter_ of a piecewise linear path-connected set is the largest distance between any two parameters in the set.

## 4 Parameter Canonicalisation

A parameter _canonicalisation algorithm_ maps each parameter in a functional equivalence class to a consistent representative parameter within that class. A canonicalisation algorithm serves as a theoretical test of functional equivalence. In Section 5 we invert our canonicalisation algorithm to characterise the functional equivalence class. More practically, canonicalising parameters before measuring the distance between them yields a metric that is independent of functionally irrelevant details such as unit permutations.

Prior work has described canonicalisation algorithms for certain irreducible parameters (Ruger and Ossen, 1997); but when applied to functionally equivalent reducible parameters, such algorithms may fail to produce the same output. We introduce a canonicalisation algorithm that properly canonicalises both reducible and irreducible parameters, based on similar negation and sorting stages, combined with a novel _reduction_ stage. This stage effectively removes or 'zeroes out' redundant units through various operations that exploit the reducibility conditions. This process isolates a functionally equivalent but irreducible subparameter.

**Algorithm 4.1** (Parameter canonicalisation).: Given a parameter space \(\mathcal{W}_{h}\), proceed:

```
1:procedureCanonicalise(\(w=(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)\in\mathcal{W}_{h}\))
2:\(\triangleright\)Stage 1: Reduce the parameter; zeroing out redundant hidden units \(\triangleleft\)
3:\(Z\leftarrow\{\}\)\(\triangleright\)keep track of 'zeroed' units
4:while any of the following four conditions hold do
5:if for some hidden unit \(i\notin Z\), \(a_{i}=0\)then\(\triangleright\)reducibility condition (i)
6:\(b_{i},c_{i}\gets 0\)
7:\(Z\gets Z\cup\{i\}\)
8:elseif for some hidden unit \(i\notin Z\), \(b_{i}=0\)then\(\triangleright\) ----(ii)
9:\(d\gets d+a_{i}\tanh(c_{i})\)
10:\(a_{i},c_{i}\gets 0\)
11:\(Z\gets Z\cup\{i\}\)
12:elseif for some hidden units \(i,j\notin Z,i\neq j\), \((b_{i},c_{i})=(b_{j},c_{j})\)then\(\triangleright\) ----(iii)
13:\(a_{j}\gets a_{j}+a_{i}\)
14:\(a_{j},b_{i},c_{i}\gets 0\)
15:\(Z\gets Z\cup\{i\}\)
16:elseif for some hidden units \(i,j\notin Z,i\neq j\), \((b_{i},c_{i})=(-b_{j},-c_{j})\)then\(\triangleright\) ----(iv)
17:\(a_{j}\gets a_{j}-a_{i}\)
18:\(a_{i},b_{i},c_{i}\gets 0\)
19:\(Z\gets Z\cup\{i\}\)
20:endif
21:endwhile
22:\(\triangleright\)Stage 2: Negate the nonzero units to have positive incoming weights \(\triangleleft\)
23:for each hidden unit \(i\notin Z\)do
24:\(a_{i},b_{i},c_{i}\leftarrow\operatorname{sign}(b_{i})\cdot(a_{i},b_{i},c_{i})\)
25:endfor
26:\(\triangleright\)Stage 3: Sort the units by their incoming weights and biases \(\triangleleft\)
27:\(\pi\leftarrow\) a permutation sorting \(i=1,\ldots,h\) by decreasing \(b_{i}\), breaking ties with decreasing \(c_{i}\)
28:\(w\leftarrow(a_{\pi(1)},b_{\pi(1)},c_{\pi(1)},\ldots,a_{\pi(h)},b_{\pi(h)},c_{ \pi(h)},d)\)
29:\(\triangleright\)Now, \(w\) has been mutated into the canonical equivalent parameter \(\triangleleft\)
30:return\(w\)
31:endprocedure ```

The following theorem establishes the correctness of Algorithm 4.1.

**Theorem 4.2**.: _Let \(w,w^{\prime}\in\mathcal{W}_{h}\). Let \(v=\textsc{Canonicalise}(w)\) and \(v^{\prime}=\textsc{Canonicalise}(w^{\prime})\). Then (i) \(v\) is functionally equivalent to \(w\); and (ii) if \(w\) and \(w^{\prime}\) are functionally equivalent, then \(v=v^{\prime}\)._

Proof.: For (i), observe that \(f_{w}\) is maintained by each iteration of the loops in Stages 1 and 2, and by the permutation in Stage 3. For (ii), observe that Stage 1 isolates functionally equivalent _and irreducible_ subparameters \(u\in\mathcal{W}_{r}\) and \(u^{\prime}\in\mathcal{W}_{r^{\prime}}\) of the input parameters \(w\) and \(w^{\prime}\) (excluding the zeroed units). We have \(f_{u}=f_{w}=f_{w^{\prime}}=f_{u^{\prime}}\), so by the results of Sussmann (1992), \(r=r^{\prime}=\operatorname{rank}(w)\), and \(u\) and \(u^{\prime}\) are related by unit negation and exchange transformations. This remains true in the presence of the zero units. Stages 2 and 3 are invariant to precisely such transformations.

[MISSING_PAGE_FAIL:5]

Path Connectivity

In this section, we show that the reducible functional equivalence class is piecewise linear path-connected (Theorem 6.1), and, for parameters with rank at most half of the available number of hidden units, has diameter at most 7 linear segments (Theorem 6.3).

**Theorem 6.1**.: _Let \(w\in\mathcal{W}_{h}\). If \(w\) is reducible, then \(\mathfrak{G}[w]\) is piecewise linear path-connected._

Proof.: It suffices to show that each reducible parameter \(w\in\mathcal{W}_{h}\) is piecewise linear path-connected in \(\mathfrak{G}[w]\) to its canonical representative \(\textsc{Canonicalise}(w)\). The path construction proceeds by tracing the parameter's mutations in the course of execution of Algorithm 4.1. For each iteration of the loops in Stages 1 and 2, and for each transposition in the permutation in Stage 3, we construct a multi-segment sub-path. To describe these sub-paths, we denote the parameter at the beginning of each sub-path as \(w=(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)\), noting that this parameter is mutated throughout the algorithm, but is functionally equivalent to the original \(w\) at all of these intermediate points.

1. In each iteration of the Stage 1 loop, the construction depends on the chosen branch, as follows. Some examples are illustrated in Figure 1. 1. A direct path interpolating \(b_{i}\) and \(c_{i}\) to zero. 2. A two-segment path, interpolating \(a_{i}\) to zero and \(d\) to \(d+a_{i}\tanh(c_{i})\), then \(c_{i}\) to zero. 3. A two-segment path, interpolating \(a_{i}\) to zero and \(a_{j}\) to \(a_{j}+a_{i}\), then \(b_{i}\) and \(c_{i}\) to zero. 4. A two-segment path, interpolating \(a_{i}\) to zero and \(a_{j}\) to \(a_{j}-a_{i}\), then \(b_{i}\) and \(c_{i}\) to zero.

Since (the original) \(w\) is reducible, (the current) \(w\) must have gone through at least one iteration in Stage 1, and must have at least one _blank_ unit \(k\) with \(a_{k},b_{k},c_{k}=0\). From any such parameter \(w\), there is a three-segment path in \(\mathfrak{G}[w]\) that implements a _blank-exchange manoeuvre_ transferring the weights of another unit \(i\) to unit \(k\), and leaving \(a_{i},b_{i},c_{i}=0\): first interpolate \(b_{k}\) to \(b_{i}\) and \(c_{k}\) to \(c_{i}\); then interpolate \(a_{k}\) to \(a_{i}\) and \(a_{i}\) to zero; then interpolate \(b_{i}\) and \(c_{i}\) to zero. Likewise, there is a three-segment path that implements a _negative blank-exchange manoeuvre_, negating the weights as they are interpolated into the blank unit. With these manoeuvres noted, proceed:

1. In each iteration of the Stage 2 loop for which \(\mathrm{sign}(b_{i})=-1\), let \(k\) be a blank unit, and construct a six-segment path. First, blank-exchange unit \(i\) into unit \(k\). Then, negative blank-exchange unit \(k\) into unit \(i\). The net effect is to negate unit \(i\).
2. In Stage 3, construct a path for each segment in a decomposition of the permutation \(\pi\) as a product of transpositions. Consider the transposition \((i,j)\). If \(i\) or \(j\) is blank, simply blank-exchange them. If neither is blank, let \(k\) be a blank unit. Construct a nine-segment path, using three blank-exchange manoeuvres, using \(k\) as 'temporary storage' to implement the transposition: first blank-exchange units \(i\) and \(k\), then blank-exchange units \(i\) (now blank) and \(j\), then blank-exchange units \(j\) (now blank) and \(k\) (containing \(i\)'s original weights).

The resulting parameter is the canonical representative and it can be verified that each segment in each sub-path remains in \(\mathfrak{G}[w]\) as required. 

**Remark 6.2**.: Simsek et al. (2021, Theorem B.4) construct similar paths to show the connectivity of their expansion manifold (cf. Remark 5.5). They first connect reduced-form parameters using blank-exchange manoeuvres and then show inductively that each unit addition preserves connectivity.

Figure 1: Example paths constructed for each of the Stage 1 branches. Other dimensions held fixed.

**Theorem 6.3**.: _Let \(w\in\mathcal{W}_{h}\). If \(\mathrm{rank}(w)\leq\frac{h}{2}\), then \(\mathfrak{F}[w]\) has diameter at most \(7\)._

Proof.: Let \(w\in\mathcal{W}_{h}\) with \(\mathrm{rank}(w)=r\leq\frac{h}{2}\). Let \(w^{\prime}\in\mathfrak{F}[w]\). We construct a piecewise linear path from \(w\) to \(w^{\prime}\) with \(7\) segments. By Theorem 6.1, a path exists via the canonical representative parameter \(v=\textsc{Canonicalise}(w)\). However, this path has excessive length. We compress the length to \(7\) by exploiting the following opportunities to parallelise segments and 'cut corners'. These optimisation steps are illustrated in Figure 2.

1. Let the Stage 1 result from Algorithm 4.1 for \(w\) be denoted \(u\). Let the Stage 1 result for \(w^{\prime}\) be denoted \(u^{\prime}\). Instead of following the unit negation and exchange transformations from \(u\) to \(v\), and then back to \(u^{\prime}\), we transform \(u\) into \(u^{\prime}\) directly, not (necessarily) via \(v\).
2. We connect \(w\) to \(u\) using two segments, implementing all iterations of Stage 1 in parallel. The first segment shifts the outgoing weights from the blank units to the non-blank units and the output unit bias. The second segment interpolates the blank units' incoming weights and biases to zero. We apply the same optimisation to connect \(w^{\prime}\) and \(u^{\prime}\).
3. We connect \(u\) and \(u^{\prime}\) using two blank-exchange manoeuvres (6 segments), exploiting the majority of blank units as 'temporary storage'. First, we blank-exchange the non-blank units of \(u\) into blank units of \(u^{\prime}\), resulting in a parameter \(\bar{u}^{\prime}\) sharing no non-blank units with \(u^{\prime}\). Then, we (negative) blank-exchange those weights into the appropriate non-blank units of \(u^{\prime}\), implementing the unit negation and exchange transformations relating \(u\), \(\bar{u}^{\prime}\), and \(u^{\prime}\).
4. The manoeuvres in (b) and (c) begin and/or end by interpolating incoming weights and biases of blank units from and/or to zero, while the outgoing weights are zero. We combine adjacent beginning/end segments together, interpolating directly from the start to the end, without (necessarily) passing through zero. This results in the required seven-segment path, tracing the sequence of parameters \(w,w^{1},w^{2},\ldots,w^{6},w^{\prime}\in W_{h}\).

To describe the constructed path in detail, we introduce the following notation for the components of the key parameters \(w,w^{\prime},u,u^{\prime},w^{1},w^{2},\ldots,w^{6}\in\mathcal{W}_{h}\):

\[w =(a_{1}^{w},b_{1}^{v},c_{1}^{v},\ldots,a_{h}^{w},b_{h}^{w},c_{h}^ {w},d^{w}) u =(a_{1}^{u},b_{1}^{u},c_{1}^{u},\ldots,a_{h}^{u},b_{h}^{u},c_{h}^ {u},d^{u})\] \[w^{\prime} =(a_{1}^{w^{\prime}},b_{1}^{w^{\prime}},c_{1}^{w^{\prime}}, \ldots,a_{h}^{w^{\prime}},b_{h}^{w^{\prime}},c_{h}^{w^{\prime}},d^{w^{\prime}}) u^{\prime} =(a_{1}^{u^{\prime}},b_{1}^{u^{\prime}},c_{1}^{u^{\prime}},\ldots, a_{h}^{u^{\prime}},b_{h}^{u^{\prime}},c_{h}^{u^{\prime}},d^{u^{\prime}})\] \[w^{k} =(a_{1}^{k},b_{1}^{k},c_{1}^{k},\ldots,a_{h}^{k},b_{h}^{k},c_{h}^ {k},d^{k}) (k=1,\ldots,6).\]

Of the \(h\) units in \(u\), exactly \(h-r\) are blank--those in the set \(Z\) from \(\textsc{Canonicalise}(w)\). Denote the complement set of \(r\) non-blank units \(U=\{1,\ldots,h\}\setminus Z\). Likewise, define \(Z^{\prime}\) and \(U^{\prime}\) from \(u^{\prime}\).

Figure 2: A conceptual illustration of the four path optimisations, producing a seven-segment piecewise linear path of equivalent parameters in a high-dimensional parameter space. **(a)** Follow unit negation and exchange transformations directly between reduced parameters, not via the canonical parameter. **(b) & **(c)** Parallelise the reduction steps, and use the majority of blank units to parallelise the transformations. **(d)** Combine first/last segments of reduction and blank-exchange manoeuvres.

With notation clarified, we can now describe the key points \(w^{1},\ldots,w^{6}\) in detail, while showing that the entire path is contained within the functional equivalence class \(\mathfrak{G}[w]\).

1. The first segment interpolates each outgoing weight from \(a_{i}^{w}\) to \(a_{i}^{u}\), and interpolates the output bias from \(d^{w}\) to \(d^{u}\). That is, \(w^{1}=(a_{1}^{u},b_{1}^{w},c_{1}^{w},\ldots,a_{h}^{u},b_{h}^{w},c_{h}^{w},d^{u})\). To see that this segment is within \(\mathfrak{G}[w]\), observe that since the incoming weights and biases are unchanged between the two parameters, \(f_{tw^{1}+(1-t)w}(x)=tf_{w^{1}}(x)+(1-t)f_{w}(x)\) for \(x\in\mathbb{R}\) and \(t\in[0,1]\). To show that \(f_{w}=f_{w^{1}}\), we construct a function \(\tau:\{1,\ldots,h\}\to\{0,1,\ldots,h\}\) from identity following each iteration of Stage 1 of Canonicalise\((w)\): when the second branch is chosen, remap \(\tau(i)\) to \(0\); and when the third or fourth branch is chosen, for \(k\in\tau^{-1}[i]\) (including \(i\) itself), remap \(\tau(k)\) to \(j\). Moreover, we define a sign vector \(\sigma\in\{-1,+1\}^{h}\) where \(\sigma_{i}=-1\) if \(\operatorname{sign}(b_{i}^{w})=-1\), otherwise \(\sigma_{i}=+1\). Then: \[f_{w}(x) =d^{w}+\sum_{j=0}^{k}\sum_{i\in\tau^{-1}[j]}a_{i}^{w}\tanh(b_{i}^ {w}x+c_{i}^{w})\] \[=d^{w}+\sum_{i\in\tau^{-1}[0]}a_{i}^{w}\tanh(c_{i}^{w})+\sum_{j=1}^ {h}\left(\sum_{i\in\tau^{-1}[j]}\sigma_{j}\sigma_{i}a_{i}^{w}\right)\tanh(b_{j }^{w}x+c_{j}^{w})\] \[=d^{u}+\sum_{j=1}^{h}a_{j}^{u}\tanh(b_{j}^{w}x+c_{j}^{w})=f_{w^{1} }(x).\]
2. The second segment completes the reduction and begins the first blank-exchange manoeuvre to store the nonzero units in \(Z^{\prime}\). For \(i\in U\cap U^{\prime}\), pick distinct'storage' units \(j\in Z\cap Z^{\prime}\). There are enough, as \(r\leq\frac{h}{2}\) by assumption thus \(|U\cap U^{\prime}|=|U|-|Z\cap U^{\prime}|=r-|Z\cap U^{\prime}|\leq(h-r)-|Z\cap U ^{\prime}|=|Z^{\prime}|-|Z\cap U^{\prime}|=|Z^{\prime}\cap Z|\). Interpolate unit \(j\)'s incoming weight from \(b_{j}^{w}\) to \(b_{i}^{w}\) and interpolate its bias from \(c_{j}^{w}\) to \(c_{i}^{w}\). Meanwhile, for all other \(j\in Z\), interpolate the incoming weight and bias to zero. This segment is within \(\mathfrak{G}[w]\) as for \(j\in Z\), \(a_{j}^{1}=a_{j}^{u}=0\) by definition of \(Z\).
3. The third segment shifts the outgoing weights from the units in \(U\cap U^{\prime}\) to the units in \(Z\cap Z^{\prime}\) prepared in step (2). For \(i\in U\cap U^{\prime}\), pick the same storage unit \(j\) as in step (2). Interpolate unit \(j\)'s outgoing weight from \(a_{j}^{u}=0\) to \(a_{i}^{u}\) and interpolate unit \(i\)'s outgoing weight from \(a_{i}^{u}\) to zero. This segment is within \(\mathfrak{G}[w]\) as \(b_{i}^{2}=b_{j}^{2}\) and \(c_{i}^{2}=c_{j}^{2}\) by step (2).
4. The fourth segment completes the first blank-exchange manoeuvre and begins the second, to form the units of \(u^{\prime}\). For \(i\in U^{\prime}\), interpolate unit \(i\)'s incoming weight from \(b_{i}^{3}\) to \(b_{i}^{u^{\prime}}\) and interpolate its bias from \(c_{i}^{3}\) to \(c_{i}^{u^{\prime}}\). This segment is within \(\mathfrak{G}[w]\) because for \(i\in U^{\prime}\cap Z\), \(a_{i}^{3}=a_{i}^{u}=0\) by definition of \(Z\), and for \(i\in U^{\prime}\cap U\), \(a_{i}^{3}=0\) by step (3).
5. The fifth segment shifts the outgoing weights from the selected units in \(Z^{\prime}\) to the units in \(U^{\prime}\) prepared in step (4). We simply interpolate each unit \(i\)'s outgoing weight to \(a_{i}^{u^{\prime}}\). To see that the segment is within \(\mathfrak{G}[w]\), note that \(u\) and \(u^{\prime}\) are related by some unit negation and exchange transformations. Therefore, there is a correspondence between their sets of nonzero units, such that corresponding units have the same (or negated) incoming weights and biases. Due to steps (2)-(4) there are \(r\)'storage' units in \(w^{4}\) with the weights of the units of \(u\), and the correspondence extends to these storage units. Since the storage units are disjoint with \(U^{\prime}\), this fifth segment has the effect of interpolating the outgoing weight of each of the storage units \(j\in Z^{\prime}\) in \(w^{4}\) from \(a_{i}^{u}\) to zero (where \(i\) is as in step (3)), while interpolating the outgoing weight of its corresponding unit \(k\in U^{\prime}\) from zero to \(\pm a_{i}^{u}=a_{k}^{u^{\prime}}\) (where the sign depends on the unit negation transformations relating \(u\) and \(u^{\prime}\)).
6. The sixth segment completes the second blank-exchange manoeuvre and begins to reverse the reduction. For \(i\in Z^{\prime}\), interpolate unit \(i\)'s incoming weight from \(b_{i}^{5}\) to \(b_{i}^{w^{\prime}}\), and interpolate its bias from \(c_{i}^{5}\) to \(c_{i}^{w^{\prime}}\). This segment is within \(\mathfrak{G}[w]\) as for \(i\in Z^{\prime}\), \(a_{i}^{5}=a_{i}^{u^{\prime}}=0\) by definition of \(Z^{\prime}\).
7. The seventh segment, of course, interpolates from \(w^{6}\) to \(w^{\prime}\). To see that this segment is within \(\mathfrak{G}[w]\), note that by steps (5) and (6), \(w^{6}=(a_{1}^{u^{\prime}},b_{1}^{w^{\prime}},c_{1}^{w^{\prime}},\ldots,a_{h}^{ u^{\prime}},b_{h}^{w^{\prime}},c_{h}^{w^{\prime}},d^{u^{\prime}})\) (noting \(d^{u}=d^{u^{\prime}}\) since the output unit's bias is preserved by unit transformations). So the situation is the reverse of step (1), and a similar proof applies.

Discussion

In this paper, we have investigated functional equivalence classes for reducible single-hidden-layer hyperbolic tangent network parameters, and their connectivity properties. Recall that irreducible parameters have discrete functional equivalence classes described by simple unit negation and exchange transformations. In contrast, reducible functional equivalence classes form a complex union of manifolds, displaying the following rich qualitative structure:

* There is a central discrete constellation of _reduced-form_ parameters, each with maximally many blank units alongside an irreducible subparameter. These reduced-form parameters are related by unit negation and exchange transformations, like for irreducible parameters.
* Unlike in the irreducible case, these reduced-form parameters are connected by a network of piecewise linear paths. Namely, these are (negative) blank-exchange manoeuvres, and, when there are multiple blank units, simultaneous parallel blank-exchange manoeuvres.
* Various manifolds branch away from this central network, tracing in reverse the various reduction operations (optionally in parallel). Dually, these manifolds trace methods for _adding_ units (cf., Fukumizu and Amari, 2000; Fukumizu et al., 2019; Simsek et al., 2021).

Theorem 6.3 establishes that when there is a _majority_ of blank units, the diameter of the entire union of manifolds becomes a small constant number of linear segments. With fewer blank units it will sometimes require more blank-exchange manoeuvres to traverse the central network of reduced-form parameters. Future work could investigate efficient implementation of various permutations through parallel blank-exchange manoeuvres with varying numbers of blank units available.

Towards modern architectures.Single-hidden-layer hyperbolic tangent networks appear far from relevant to modern deep learning architectures. However, we expect our canonicalisation algorithm to partially generalise and some aspects of our connectivity findings to generalise as follows.

_Structural redundancy._ Observe that reducibility conditions (i)-(iii) apply to structural redundancies that are generic to every feed-forward layer within any architecture (units with zero, constant, or proportional output). Unit negation symmetries are characteristic of odd nonlinearities only, but other nonlinearities will exhibit their own affine symmetries that would play a similar role. In more sophisticated architectures, these basic sources of structural redundancy will sit alongside other sources such as interactions between layers, attention blocks, layer normalisation, and so on.

_Canonicalisation._ It follows that Algorithm 4.1 serves as a partial canonicalisation for more sophisticated architectures (with small modifications for alternative nonlinearities). Future works need only find and remove the _additional_ kinds of structural redundancy.

_Connectivity._ Similarly, additional sources of redundancy expand the functional equivalence class. While global connectivity properties may be lost in this process, individual paths will not be disturbed. We expect that our high-level qualitative finding will hold: that the more reducible a parameter is, the more intricately connected it is to functionally equivalent parameters.

Towards approximate structural redundancy.Our framework is built around the definition of functional equivalence, which requires exact equality of functions for all inputs. A more pragmatic definition would concern _approximate_ equality of functions for _relevant_ inputs. One step in this direction is studying proximity in parameter space to low-rank parameters, though detecting such proximity precisely is formally intractable (Farrugia-Roberts, 2023).

Functional equivalence and the loss landscape.Functionally equivalent parameters have equal loss. Continuous directions and piecewise linear paths within reducible functional equivalence classes therefore imply flat directions and equal-loss paths in the loss landscape. More broadly, the set of low- or zero-loss parameters is a union of functional equivalence classes. If some (very) reducible parameters obtain low loss (such as in the overparameterised setting) then the set of low-loss parameters contains rich functional equivalence classes as subsets.

Of course, having the same loss does not require functional equivalence. Indeed, Garipov et al. (2018) observe functional non-equivalence in low-loss paths. The exact relevance of reducible parameters to these topics remains to be clarified. Of special interest is the connection to theoretical work involving unit pruning (Kuditipudi et al., 2019) and permutation symmetries (Brea et al., 2019).

Canonicalisation and model compression.Our canonicalisation algorithm transforms a neural network parameter into another parameter that has the same input-output behaviour but effectively uses fewer units. This size reduction is not fundamental to the goals of canonicalisation (we could still achieve canonicalisation by producing a standard, maximally dense representation of each equivalent parameter). Nevertheless, Algorithm 4.1 performs (lossless) model compression as a side-effect (cf. Farrugia-Roberts, 2023).

In particular, our canonicalisation algorithm is reminiscent of a unit-based pruning technique (cf., e.g., Hoefler et al., 2021). However, there are a few salient differences. First, pruning algorithms are usually not designed to preserve exact functional equivalence on all inputs, but rather to accept small changes in outputs (on specific inputs). Second, unit-based pruning techniques usually select individual units for removal, and then continue training the network. Our canonicalisation algorithm instead considers operations that simultaneously remove a unit and perform a systematic adjustment to the remaining weights and biases to maintain the network's behaviour without any training.

Of interest, Casper et al. (2021) empirically studied a network pruning that finds units with weak outputs or pairs of units with correlated outputs, and then _eliminates_ or _merges_ these units and makes appropriate adjustments to approximately maintain performance. The elimination and merging operations bear a striking resemblance to the operations in Algorithm 4.1.

Reducible parameters and singular learning theory.When a reducible parameter is a critical point of the loss landscape, it is necessarily a _degenerate_ critical point (due to the continuum of equal-loss equivalent parameters nearby; or cf. Fukumizu, 1996). This places situations where learning encounters (or even approaches) reducible parameters within the domain of singular learning theory (cf. Watanabe, 2009; Wei et al., 2022).

The nature of the degeneracy depends on the exact ways in which it is reducible. For example, is the parameter at the intersection of multiple of the manifolds comprising the functional equivalence class (cf. Theorem 5.2)? The order of variation in directions away from the functional equivalence class also plays a role (cf. Lau et al., 2023). Future work could analyse the structural redundancy to find principled bounds on effective dimensionality.

## 8 Conclusion

Reducible parameters exhibit structural redundancy, in that the same input-output function could be implemented with a smaller network. While reducible parameters comprise a measure zero subset of the parameter space, their functional equivalence classes are much richer than those of irreducible parameters. Understanding these rich functional equivalence classes is important to understanding the nature of the loss landscape within which deep learning takes place.

We have taken the first step towards understanding functional equivalence beyond irreducible parameters by accounting for various kinds of structural redundancy in the setting of single-hidden-layer hyperbolic tangent networks. We offer an algorithmic characterisation of reducible functional equivalence classes and an investigation of their piecewise linear connectivity properties. We find in particular that the more redundancy is present in a parameter, the more intricately connected is its functional equivalence class.

We call for future work to seek out, catalogue, and thoroughly investigate sources of structural redundancy in more sophisticated neural network architectures; and to further investigate the role these parameters play in deep learning.

## Acknowledgements

Contributions (1), (2), and (3) also appear in MFR's minor thesis (Farrugia-Roberts, 2022, SS5). MFR was affiliated with the School of Computing and Information Systems at the University of Melbourne while completing this research, but is currently affiliated with the University of Cambridge.

We thank Daniel Murfet for helpful feedback during this research and on this manuscript.

MFR received financial support from the Melbourne School of Engineering Foundation Scholarship and the Long-Term Future Fund while completing this research.

## References

* Albertini and Sontag (1992) Francesca Albertini and Eduardo D. Sontag. For neural networks, function determines form. Technical Report SYCON-92-03, Rutgers Center for Systems and Control, **1992**. Expanded version of Albertini and Sontag (1993a). Cited on page 2.
* Albertini and Sontag (1993b) Francesca Albertini and Eduardo D. Sontag (1993b) Access via Crossref. Cited on pages 2 and 11.
* Albertini and Sontag (1993c) Francesca Albertini and Eduardo D. Sontag (1993c) Access via Crossref. Cited on pages 2, 11.
* Albertini and Sontag (1993d) Francesca Albertini and Eduardo D. Sontag (1993d) Francesca Albertini and Eduardo D. Sontag (1993d) Access via Francesca Albertini. Cited on page 2.
* Albertini and Sontag (1993e) Francesca Albertini and Eduardo D. Sontag (1993e) Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D. Sontag (193e) Access via Francesca Albertini and Eduardo D. Sontag (1993e) Access via Francesca Albertini and Eduardo D.

Kenji Fukumizu, Shoichiro Yamaguchi, Yoh-ichi Mototake, and Mirai Tanaka. Semi-flat minima and saddle points by embedding neural networks to overparameterization. In _Advances in Neural Information Processing Systems 32_, pages 13868-13876. Curran Associates, **2019**. Access via NeurIPS. Cited on pages 2, 5, and 9.
* Garipov et al. (2018) Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, and Andrew G. Wilson. Loss surfaces, mode connectivity, and fast ensembling of DNNs. In _Advances in Neural Information Processing Systems 31_, pages 8789-8798. Curran Associates, **2018**. Access via NeurIPS. Cited on page 9.
* E. Harzheim (2005)Ordered sets. Springer, **2005**. Access via Crossref. Cited on page 13.
* R. Hecht-Nielsen (1990)On the algebraic structure of feedforward network weight spaces. In _Advanced Neural Computers_, pages 129-135. North-Holland, Amsterdam, **1990**. Access via Crossref. Cited on pages 1 and 2.
* T. Hoefler, D. Alistarh, T. Ben-Nun, N. Dryden, and A. Peste (2021)Sparsity in deep learning: pruning and growth for efficient inference and training in neural networks. _Journal of Machine Learning Research_, 22(241):1-124, **2021**. Access via JMLR. Cited on page 10.
* V. Karkova and P. C. Kainen (1994)Functionally equivalent feedforward neural networks. _Neural Computation_, 6(3):543-558, **1994**. Access via Crossref. Cited on pages 1 and 2.
* R. Kuditipudi, X. Wang, H. Lee, Y. Zhang, Z. Li, W. Hu, R. Ge, and S. Arora (2019)Explaining landscape connectivity of low-cost solutions for multilayer nets. In _Advances in Neural Information Processing Systems 32_, pages 14601-14610. Curran Associates, **2019**. Access via NeurIPS. Cited on page 9.
* E. Lau, D. Murfet, and S. Wei (2023)Quantifying degeneracy in singular models via the learning coefficient. **2023**. Preprint arXiv:2308.12108 [stat.ML]. Cited on page 10.
* M. Phuong and C. H. Lampert (2020)Functional vs. parametric equivalence of ReLU networks. In _8th International Conference on Learning Representations_. OpenReview. Cited on pages 1 and 2.
* S. M. Ruger and A. Ossen (1997)The metric structure of weight space. _Neural Processing Letters_, 5(2):1-9, **1997**. Access via Crossref. Cited on pages 2 and 4.
* B. Simsek, F. Ged, A. Jacot, F. Spadaro, C. Hongler, W. Gerstner, and J. Brea (2021)Geometry of the loss landscape in overparameterized neural networks: symmetries and invariances. In _Proceedings of the 38th International Conference on Machine Learning_, pages 9722-9732. PMLR. Cited on pages 2, 5, and 9.
* P. Stock and R. Gribonval (2022)An embedding of ReLU networks and an analysis of their identifiability. _Constructive Approximation_, **2022**. Access via Crossref. Cited on page 2.
* H. J. Sussmann (1992)Uniqueness of the weights for minimal feedforward nets with a given input-output map. _Neural Networks_, 5(4):589-593, **1992**. Access via Crossref. Cited on pages 1, 2, 3, 4, 5, 13, 14, 15, and 16.
* V. Vlacic and H. Bolcskei (2021)Affine symmetries and neural network identifiability. _Advances in Mathematics_, 376:107485, **2021**. Access via Crossref. Cited on pages 1 and 2.
* V. Vlacic and H. Bolcskei (2022)Neural network identifiability for a family of sigmoidal nonlinearities. _Constructive Approximation_, 55(1):173-224, **2022**. Access via Crossref. Cited on page 2.
* S. Watanabe (2009)Algebraic geometry and statistical learning theory. Cambridge University Press, **2009**. Cited on pages 1 and 10.
* S. Wei, D. Murfet, M. Gong, H. Li, J. Gell-Redman, and T. Quella (2022)Deep learning is singular, and that's good. _IEEE Transactions on Neural Networks and Learning Systems_, **2022**. Access via Crossref. To appear in an upcoming volume. Cited on pages 1 and 10.

Generalising to multi-dimensional inputs and outputs

In this appendix, we consider a slightly more general family of architectures than that introduced in Section 3. Namely, we consider a family of fully-connected, feed-forward neural network architectures with \(n\in\mathbb{N}^{+}\) input units, \(m\in\mathbb{N}^{+}\) biased linear output units, and a single hidden layer of \(h\in\mathbb{N}\) biased hidden units with the hyperbolic tangent nonlinearity. With minor modifications, described in the remainder of this appendix, all definitions, algorithms, theorems, and proofs directly generalise from the case \(n=m=1\) to arbitrary \(n\) and \(m\).

Multi-dimensional architecture.Let \(n\in\mathbb{N}^{+}\), \(m\in\mathbb{N}^{+}\), and \(h\in\mathbb{N}\). Define the generalised parameter space \(\mathcal{W}_{h}^{n,m}=\mathbb{R}^{(n+m+1)h+m}\). The weights and biases of the network's units are encoded in the parameter vector in the format \((a_{1},b_{1},c_{1},\dots,a_{h},b_{h},c_{h},d)=w\in\mathcal{W}_{h}^{n,m}\) where for each hidden unit \(i=1,\dots,h\) there is an _outgoing weight vector_\(a_{i}\in\mathbb{R}^{m}\), an _incoming weight vector_\(b_{i}\in\mathbb{R}^{n}\), and a _bias_\(c_{i}\in\mathbb{R}\); and \(d\in\mathbb{R}^{m}\) is an _output unit bias vector_ containing one bias value for each output unit. This time, \(w\) indexes a multi-dimensional mathematical function \(f_{w}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\) defined as follows:

\[f_{w}(x)=d+\sum_{i=1}^{h}a_{i}\tanh(b_{i}\cdot x+c_{i}).\] (2)

Note that we use the same tuple notation and ordering \((a_{1},b_{1},c_{1},\dots,a_{h},b_{h},c_{h},d)\) but now the \(a_{i}\), the \(b_{i}\), and \(d\) all denote multi-component vectors. Accordingly, in Equation (2), \(b_{i}\) and \(x\) are now multiplied using the inner (dot) product, rather than scalar multiplication, since they are both vectors in \(\mathbb{R}^{n}\). Moreover, \(a_{i}\in\mathbb{R}^{m}\) as a vector is to be multiplied by the scalar \(\tanh(b_{i}\cdot x+c_{i})\). That is, the sum is over vectors of contributions to output units from each hidden unit.

To generalise the results of the main paper to this setting the first change necessary is to replace all mentions of scalar weights with these vectors of weights, and other similar changes such as reading the literal zero as vector zero where appropriate.

Signing and sorting incoming weight vectors.The lexicographic order on \(\mathbb{R}^{n}\), denoted \(\preceq\), is a relation such that for \(u,v\in\mathbb{R}^{n}\), \(u\preceq v\) if and only if \(u=v\) or, in the first index \(i=1,\dots,n\) where \(u\) and \(v\) differ, \(u_{i}<v_{i}\). From this definition we follow the usual conventions in defining \(\prec\), \(\succ\), and \(\succeq\). Finally, define the _lexicographic sign_ of \(v\in\mathbb{R}^{n}\), denoted \(\operatorname{sign}_{\operatorname{lex}}(v)\), as follows:

\[\operatorname{sign}_{\operatorname{lex}}(v)=\begin{cases}+1&(v\succ 0),\\ 0&(v=0),\\ -1&(v\prec 0).\end{cases}\]

The parameter canonicalisation algorithm and some of the other theorems and proofs make repeated use of the signs of incoming weight vectors. The lexicographic sign satisfies the requisite properties of the scalar sign function in these uses and so the second change necessary to generalising the results is to replace uses of \(\operatorname{sign}(\cdot)\) with uses of \(\operatorname{sign}_{\operatorname{lex}}(\cdot)\).

This lexicographic order relation is of course also a total order (see, e.g., Harzheim, 2005, Theorem 4.1.11). Therefore, it allows one to sort a list of vectors. Sorting units by decreasing incoming weights is a key step in Stage 3 of Algorithm 4.1, and so the third change necessary is to use decreasing lexicographic order (\(\succeq\)) in this stage.

Generalising Sussmann's equivalence theorem.The proofs in the main paper rely on the results of Sussmann (1992) on the equivalence between reducibility and non-minimality, and the fact that irreducible functionally equivalent parameters are related by unit negation and exchange transformations. Sussmann (1992) studied a setting with multiple input units but only a single output unit. Lemmas A.1 and A.2 generalise these results to the multi-output setting.1 The final necessary change to generalise the results in the main paper is to replace all references to Sussmann's results with references to Lemma A.1 or Lemma A.2.

The definitions of unit negation and exchange transformations, reducibility, and non-minimality all generalise to arbitrary \(n\) and \(m\) with the above-mentioned changes. These definitions are repeated here for convenience.

A _unit negation transformation_ is a function \(\sigma_{i}:\mathcal{W}_{h}^{n,m}\rightarrow\mathcal{W}_{h}^{n.m}\) for \(i=1,\ldots,h\), where

\[\sigma_{i}(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)=(a_{1},b_{1},c_{1}, \ldots,-a_{i},-b_{i},-c_{i},\ldots,a_{h},b_{h},c_{h},d).\]

A _unit exchange transformation_ is a function \(\tau_{i,j}:\mathcal{W}_{h}^{n,m}\rightarrow\mathcal{W}_{h}^{n,m}\) for \(i,j=1,\ldots,h\), where

\[\tau_{i,j}(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)=(a_{1},b_ {1},c_{1},\ldots,c_{i-1},a_{j},b_{j},c_{j},a_{i+1},\] \[\ldots,c_{j-1},a_{i},b_{i},c_{i},a_{j+1},\ldots,a_{h},b_{h},c_{h},d).\]

A parameter \(w=(a_{1},b_{1},c_{1},\ldots,a_{h},b_{h},c_{h},d)\in\mathcal{W}_{h}^{n,m}\) is _reducible_ if and only if it satisfies any of the following conditions (otherwise, \(w\) is _irreducible_):

1. \(a_{i}=0\) for some \(i\),
2. \(b_{i}=0\) for some \(i\),
3. \((b_{i},c_{i})=(b_{j},c_{j})\) for some \(i\neq j\), or
4. \((b_{i},c_{i})=(-b_{j},-c_{j})\) for some \(i\neq j\).

A parameter \(w\in\mathcal{W}_{h}^{n,m}\) is _non-minimal_ if and only if \(w\) is functionally equivalent to some \(w^{\prime}\in\mathcal{W}_{h^{\prime}}^{n,m}\) with fewer hidden units \(h^{\prime}<h\).

**Lemma A.1**.: _For \(w\in\mathcal{W}_{h}^{n,m}\), \(w\) is reducible if and only if \(w\) is non-minimal._

Proof.: (\(\Rightarrow\)): A smaller functionally equivalent parameter can be constructed as follows.

1. If \(a_{i}=0\) for some \(i\), then hidden unit \(i\) fails to contribute to the function. Construct a functionally equivalent parameter \(w^{\prime}\in\mathcal{W}_{h-1}^{n,m}\) with hidden unit \(i\) omitted: \[w^{\prime}=(a_{1},b_{1},c_{1},\ldots,a_{i-1},b_{i-1},c_{i-1},a_{i+1},b_{i+1}, c_{i+1},\ldots,a_{h},b_{h},c_{h},d).\]
2. If \(b_{i}=0\) for some \(i\), then hidden unit \(i\) contributes only a constant to the function. Construct a functionally equivalent parameter \(w^{\prime}\in\mathcal{W}_{h-1}^{n,m}\) with hidden unit \(i\) omitted and the output unit bias vector changed to compensate: \[w^{\prime}=(a_{1},b_{1},c_{1},\ldots,a_{i-1},b_{i-1},c_{i-1},a_{i+1},b_{i+1}, c_{i+1},\ldots,a_{h},b_{h},c_{h},d+a_{i}\tanh(c_{i})).\]
3. If \((b_{i},c_{i})=(b_{j},c_{j})\) for some \(i\neq j\), then hidden units \(i\) and \(j\) contribute proportionately. They can be combined into a single unit (say, \(j\)) with the same incoming weights and bias, and a combined outgoing weight vector. Construct a functionally equivalent parameter \(w^{\prime}\in\mathcal{W}_{h-1}^{n,m}\) accordingly: \[w^{\prime}=(a_{1},b_{1},c_{1},\ldots,c_{i-1},a_{i+1},\ldots,c_{j-1},a_{j}+a_{i },b_{j},c_{j},a_{j+1},\ldots,a_{h},b_{h},c_{h},d).\]
4. If \((b_{i},c_{i})=-(b_{j},c_{j})\) for some \(i\neq j\), then hidden units \(i\) and \(j\) contribute in negative proportion. Due to the odd property of \(\tanh\) they can be combined into a single unit (say, \(j\)) with incoming weight and bias vectors \((b_{j},c_{j})\) and a combined outgoing weight vector. Construct a new parameter \(w^{\prime}\in\mathcal{W}_{h-1}^{n,m}\) accordingly: \[w^{\prime}=(a_{1},b_{1},c_{1},\ldots,c_{i-1},a_{i+1},\ldots,c_{j-1},a_{j}-a_{i },b_{j},c_{j},a_{j+1},\ldots,a_{h},b_{h},c_{h},d).\]

In all cases, the new parameter \(w^{\prime}\in\mathcal{W}_{h-1}^{n,m}\) has \(f_{w^{\prime}}=f_{w}\), so \(w\) is non-minimal.

(\(\Leftarrow\)): We reduce to the single-output case and apply the result of Sussmann (1992) to show that \(w\) satisfies at least one of the reducibility conditions.

To reduce to the single-output case, we introduce some notation. From the function \(f_{w}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\) define a series of component functions \(f_{w}^{(1)},f_{w}^{(2)},\ldots,f_{w}^{(m)}:\mathbb{R}^{n}\rightarrow\mathbb{R}\) such that for \(x\in\mathbb{R}^{n}\),

\[f_{w}(x)=\left(f_{w}^{(1)}(x),f_{w}^{(2)}(x),\ldots,f_{w}^{(m)}(x)\right).\]Each of these component functions is a simple neural network function in an architecture with \(n\) input units and \(1\) output unit, corresponding to a subgraph of the connection graph of the original neural network, as illustrated in Figure 3.

Denote the corresponding (overlapping) subvectors of \(w\in\mathcal{W}_{h}^{n,m}\) as \(w_{(1)},\ldots,w_{(m)}\in\mathcal{W}_{h}^{n,1}\). That is, for \(\mu=1,\ldots,m\),

\[w_{(\mu)}=(a_{1,\mu},b_{1},c_{1},\ldots,a_{h,\mu},b_{h},c_{h},d_{\mu})\in \mathcal{W}_{h}^{n,1}.\]

Now, let \(w^{\prime}=(a^{\prime}_{1},b^{\prime}_{1},c^{\prime}_{1},\ldots,a^{\prime}_{h},b^{\prime}_{h},c^{\prime}_{h},d^{\prime})\in\mathcal{W}_{h^{\prime}}\) such that \(f_{w^{\prime}}=f_{w}\) where \(h^{\prime}\) is the smallest number of hidden units required to implement \(f_{w}\) (\(h^{\prime}<h\) by assumption of non-minimality). Apply the same decomposition to \(f_{w^{\prime}}\) to define \(f_{w^{\prime}}^{(1)},\ldots,f_{w^{\prime}}^{(m)}\), and to define \(w^{\prime}_{(1)},\ldots,w^{\prime}_{(m)}\in\mathcal{W}_{h^{\prime}}^{n,1}\).

Apply the results of Sussmann (1992) as follows. Since \(f_{w}=f_{w^{\prime}}\), \(f_{w}^{(\mu)}=f_{w^{\prime}}^{(\mu)}\) for \(\mu=1,\ldots,m\). It follows that for each \(w_{(\mu)}\), \(w^{\prime}_{(\mu)}\) is a functionally equivalent parameter using fewer units. Therefore, the reducibility conditions (in the special case of \(m=1\)) must hold for each \(w_{(\mu)}\)(Sussmann, 1992).

Since conditions (ii-iv) only depend on incoming weights and biases, if any of these conditions hold for any \(w_{(\mu)}\), then they must also hold for \(w\) itself (which shares the same incoming weights and biases), and the proof is complete. It remains only to consider the case in which conditions (ii-iv) fail to hold for any \(w_{(\mu)}\), and to show that condition (i) holds for \(w\) itself in this case.

We must introduce yet further notation. For \(i=1,\ldots,h\) denote by \(\varphi_{i}:\mathbb{R}^{n}\to\mathbb{R}\) the function \(\varphi_{i}(x)=\tanh(b_{i}x+c_{i})\). Similarly for \(j=1,\ldots,h^{\prime}\) denote by \(\psi_{j}:\mathbb{R}^{n}\to\mathbb{R}\) the function \(\psi_{j}(x)=\tanh(b^{\prime}_{j}x+c^{\prime}_{j})\). Then, since we have ruled out reducibility conditions (ii-iv) for \(w\), no \(\varphi_{i}\) is constant (ii) and no two are proportional (iii, iv). The same holds for the \(\psi_{j}\)--conditions (i-iv) do not hold for \(w^{\prime}_{(\mu)}\), since \(h^{\prime}\) was assumed to be minimal. Yet, for \(\mu=1,\ldots,m\), the linear combination of functions

\[d_{\mu}+\sum_{i=1}^{h}a_{i,\mu}\varphi_{i}-d^{\prime}_{\mu}-\sum_{j=1}^{h^{ \prime}}a^{\prime}_{j,\mu}\psi_{j}=f_{w}^{(\mu)}-f_{w^{\prime}}^{(\mu)}=0\]

yields the zero function. This linear combination remains when excluding those terms with \(a_{i,\mu}=0\) or \(a^{\prime}_{j,\mu}=0\). Applying the same reasoning as that in Sussmann (1992), due to the independence property of the hyperbolic tangent function (Sussmann, 1992, Lemma 3.1) the remaining terms must be in bijection, such that

\[\varphi_{i}=\pm\psi_{j}\] (3)

for some \(j\) with \(a^{\prime}_{j,\mu}\neq 0\) for each \(i\) with \(a_{i,\mu}\neq 0\).

To complete the proof, note that these relationships (3) between the units of \(w\) and \(w^{\prime}\) are independent of \(\mu\). However, the relationships are "exclusive" in the sense that no two \(\varphi_{i}\) can be proportional to the same \(\psi_{j}\), else they would also be proportional to each other (ruled out above). Since there are only \(h^{\prime}\) units \(\psi_{1},\ldots,\psi_{h^{\prime}}\), it follows that there must be one hidden unit \(i\) (actually at least \(h-h^{\prime}\) many units) for which \(a_{i,\mu}=0\) for all \(\mu=1,\ldots,m\) (allowing \(\varphi_{i}\) to avoid any such relationship). That is, \(a_{i}=(a_{i,1},\ldots,a_{i,m})=0\), satisfying condition (i) for \(w\) as required.

Figure 3: The connection graphs of the component functions of \(f_{w}\). Included units and weights are solid. The hidden units of each network share the same incoming weights (and biases, not shown).

**Lemma A.2**.: _Let \(w\in\mathcal{W}^{n,m}_{h}\) be irreducible, and let \(w^{\prime}\in\mathcal{W}^{n,m}_{h}\). If \(w\) and \(w^{\prime}\) are functionally equivalent then there exists a compositional chain of unit negation and exchange transformations, collectively a transformation \(T:\mathcal{W}^{n,m}_{h}\to\mathcal{W}^{n,m}_{h}\), such that \(w^{\prime}=T(w)\)._

Proof.: Once again, we reduce to the case \(m=1\) and appeal to Sussmann (1992).

Suppose \(w^{\prime}\in\mathfrak{F}[w]\). Introduce the same decomposition of the two neural networks as in the proof of Lemma A.1, namely, the component functions \(f^{(1)}_{w},\ldots,f^{(m)}_{w},f^{(1)}_{w^{\prime}},\ldots,f^{(m)}_{w^{\prime}}\) implemented by the parameter subvectors \(w_{(1)},\ldots,w_{(m)},w^{\prime}_{(1)},\ldots,w^{\prime}_{(m)}\in\mathcal{W }^{n,1}_{h}\) (cf. Figure 3).

For \(\mu=1,\ldots,m\), since \(f_{w}=f_{w^{\prime}}\), we have that \(f^{(\mu)}_{w}=f^{(\mu)}_{w^{\prime}}\). Now, \(w_{(\mu)}\) and \(w^{\prime}_{(\mu)}\) are not necessarily irreducible, but if they are reducible then it is only by condition (i), since \(w_{(\mu)}\) and \(w^{\prime}_{(\mu)}\) have the incoming weights and biases of \(w\) and \(w^{\prime}\) respectively (\(w\) is irreducible by assumption; \(w^{\prime}\) is irreducible because, with the same number of units as \(w\), it is necessarily minimal, and irreducibility follows by Lemma A.1). Remove such units with zero outgoing weight from \(w_{(\mu)}\) and \(w^{\prime}_{(\mu)}\) to produce new, functionally equivalent irreducible parameters \(u_{(\mu)},u^{\prime}_{(\mu)}\in\mathcal{W}^{n,1}_{\mathrm{rank}(w_{(\mu)})}\). Now by Sussmann (1992, Theorem 2.1) there exists a chain of unit negation and exchange transformations \(T_{\mu}\) such that \(u_{(\mu)}=T_{\mu}(u^{\prime}_{(\mu)})\).

For each \(\mu\), \(T_{\mu}\) implies a relationship between the units of \(w_{(\mu)}\) and \(w^{\prime}_{(\mu)}\) with nonzero outgoing weights, including possible negations and permutations of these units. This same relationship must hold between those units of \(w\) and \(w^{\prime}\) since they share incoming weights and biases with \(w_{(\mu)}\) and \(w^{\prime}_{(\mu)}\), and (since \(w\) is irreducible, conditions (ii-iv)) these incoming weights are nonzero and the incoming weight and bias vectors are absolutely distinct between units of the same parameter. Moreover, all units are involved in some such relationship because no unit of \(w\) or \(w^{\prime}\) can have zero outgoing weight vector by reducibility condition (i).

So, one can construct from these implied relationships a composition of unit negation and exchange transformations relating \(w\) and \(w^{\prime}\) as required.