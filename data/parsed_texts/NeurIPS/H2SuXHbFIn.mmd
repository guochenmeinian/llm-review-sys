# Tree-Based Diffusion Schrodinger Bridge

with Applications to Wasserstein Barycenters

 Maxence Noble

CMAP, CNRS, Ecole polytechnique,

Institut Polytechnique de Paris,

91120 Palaiseau, France

&Valentin De Bortoli

Computer Science Department,

ENS, CNRS, PSL University

Arnaud Doucet

Department of Statistics,

University of Oxford, UK

&Alain Oliviero Durmus

CMAP, CNRS, Ecole polytechnique,

Institut Polytechnique de Paris,

91120 Palaiseau, France

Corresponding author. Contact at: maxence.noble-bourillot@polytechnique.edu.

###### Abstract

Multi-marginal Optimal Transport (mOT), a generalization of OT, aims at minimizing the integral of a cost function with respect to a distribution with some prescribed marginals. In this paper, we consider an entropic version of mOT with a tree-structured quadratic cost, i.e., a function that can be written as a sum of pairwise cost functions between the nodes of a tree. To address this problem, we develop Tree-based Diffusion Schrodinger Bridge (TreeDSB), an extension of the Diffusion Schrodinger Bridge (DSB) algorithm. TreeDSB corresponds to a dynamic and continuous state-space counterpart of the multi-marginal Sinkhorn algorithm. A notable use case of our methodology is to compute Wasserstein barycenters which can be recast as the solution of a mOT problem on a star-shaped tree. We demonstrate that our methodology can be applied in high-dimensional settings such as image interpolation and Bayesian fusion.

## 1 Introduction

In the last decade, computational Optimal Transport (OT) has shown great success with applications in various fields such as biology (Schiebinger et al., 2019; Bunne et al., 2022), shape correspondence (Su et al., 2015; Feydy et al., 2017; Eisenberger et al., 2020), control theory (Bayraktar et al., 2018; Acciaio et al., 2019) and computer vision (Schmitz et al., 2018; Carion et al., 2020). While OT commonly seeks at computing the transport plan that minimizes the cost of moving between two distributions, it can naturally be extended to the multi-marginal setting (mOT) when considering several distributions. This extension of OT has notably been studied in quantum chemistry (Cotar et al., 2013), clustering (Cuturi and Doucet, 2014) and statistical inference (Srivastava et al., 2018). In particular, a popular application in unsupervised learning of mOT with Euclidean cost consists in computing the Wasserstein barycenter of a set of probability distributions (Agueh and Carlier, 2011; Benamou et al., 2015; Alvarez-Esteban et al., 2016; Peyre et al., 2019).

Interior point methods can be used to solve OT and mOT problems but they come with computational challenges (Pele and Werman, 2009). In order to mitigate these limitations, one often considers an _entropic regularization_ of OT, known as Entropic OT (EOT). This regularized formulation can be efficiently solved in discrete state-spaces using the celebrated _Sinkhorn_ algorithm (Cuturi, 2013; Knight, 2008; Sinkhorn and Knopp, 1967), which admits a continuous state-space counterpart referredto as the _Iterative Proportional Fitting_ (IPF) procedure (Fortet, 1940; Kullback, 1968; Ruschendorf, 1995). In the case of a quadratic cost, EOT is equivalent to the _static_ formulation of the Schrodinger Bridge (SB) problem (Schrodinger, 1932). Given a reference diffusion with finite time horizon \(T\) and two probability measures, solving SB amounts to finding the closest diffusion to the reference (in terms of Kullback-Leibler divergence on path spaces) with the given marginals at times \(t=0\) and \(t=T\). This framework naturally arises in stochastic control (Dai Pra, 1991) where one aims at controlling the marginal distribution of a stochastic process at a fixed time. Recently, De Bortoli et al. (2021) introduced Diffusion Schrodinger Bridge (DSB), an approximation of a _dynamic_ version of the IPF scheme on path spaces, see also Vargas et al. (2021); Chen et al. (2022). This methodology leverages advances in the field of denoising diffusion models (Song et al., 2021; Ho et al., 2020) in order to derive a scalable and efficient scheme to solve SB, and thus EOT.

Similarly to OT, mOT admits an entropic regularization (EmOT), which can be solved via a multi-marginal generalization of Sinkhorn/IPF algorithm (Benamou et al., 2015; Marino and Gerolin, 2020). Recently, Haasler et al. (2021) proposed an extension of the _static_ SB problem in _discrete_ state-space to any multi-marginal tree-based setting. They notably made the correspondence between this formulation and EmOT, when the cost function writes as the sum of interaction energies onto the given tree structure, and introduced an efficient version of Sinkhorn algorithm to solve it.

Motivations and contributions.In this work, we investigate the _continuous_ and _dynamic_ counterpart of the tree-based framework from Haasler et al. (2021). To be more specific, we present an extension of the static SB formulation in continuous state-space to any multi-marginal tree-based setting, referred to as TreeSB. Then, we establish the equivalence between TreeSB and a formulation of EmOT relying on a (quadratic) tree-structured cost function, analogously to Haasler et al. (2021). Inspired by DSB, we develop TreeDSB, a dynamic counterpart of the multi-marginal IPF (mIPF) to solve it, by operating on path spaces and using score-based diffusion techniques. To bridge gaps in literature, we prove the convergence of mIPF iterations in a _non-compact_ setting under mild assumptions, by extending results on IPF convergence (Ruschendorf, 1995). Finally, we illustrate our approach on examples of Wasserstein barycenters from statistical inference and image processing.

Although our approach can be applied to any tree, we focus on _star-shaped trees_. In this setting, we show that TreeSB reduces to a regularized Wasserstein barycenter problem. Our method comes with several benefits compared to existing works. First, it is out-of-sample, _i.e._, it does not require re-running the full procedure when given a new data point. Second, our formulation of the Wasserstein barycenter problem obtained from TreeSB allows us to avoid numerical issues of having to choose the regularization too small, see Section 5. Finally, to the best of our knowledge, this is the first methodology to extend ideas from diffusion-based models to the computation of Wasserstein barycenters. In particular, we believe that the idea of iterative refinement, _i.e._, solving the _dynamic_ counterpart of a _static_ problem, plays a key role in the efficiency and scalability of the method.

Notation.For any measurable space \((\mathsf{X},\mathcal{X})\), we denote by \(\mathscr{P}(\mathsf{X})\) the space of probability measures defined on \((\mathsf{X},\mathcal{X})\). Unless specified, \(\mathcal{X}\) is defined as the Borel sets on \(\mathsf{X}\). For any \(\hat{\ell}\in\mathbb{N}\), let \(\mathscr{P}^{(\hat{\ell})}=\mathscr{P}((\mathbb{R}^{d})^{\ell})\); we denote \(\mathscr{P}^{(1)}\) by \(\mathscr{P}\). Assume that \(\mathsf{X}=(\mathbb{R}^{d})^{\ell}\) for some \(\ell\in\mathbb{N}\). For any \(x\in\mathsf{X}\) and any \(m,n\in\{0,\ldots,\ell\}\) such that \(m\leq n\), let \(x_{m:n}=(x_{m},x_{m+1},\ldots,x_{n})\). Let \(\mathrm{Leb}\) be the Lebesgue measure. For any non-negative function \(f:\mathsf{X}\to\mathbb{R}_{+}\), such that \(\int_{\mathsf{X}}f\mathrm{d}\mathrm{Leb}<+\infty\), define \(\mathrm{H}(f)=-\int_{\mathsf{X}}f\log f\mathrm{d}\mathrm{Leb}\in(-\infty,+\infty]\). For any distribution \(\mu\in\mathscr{P}(\mathsf{X})\), we define the entropy of \(\mu\) as \(\mathrm{H}(\mu)=\mathrm{H}(\mathrm{d}\mu/\mathrm{d}\mathrm{Leb})\) if \(\mu\ll\mathrm{Leb}\) and \(\mathrm{H}(\mu)=+\infty\) otherwise. For any two arbitrary measures \(\mu\) and \(\nu\) defined on \((\mathsf{X},\mathcal{X})\), define the Kullback-Leibler divergence between \(\mu\) and \(\nu\) as \(\mathrm{KL}(\mu|\nu)=\int_{\mathsf{X}}\log(\mathrm{d}\mu/\mathrm{d}\nu)\mathrm{ d}\mu-\int_{\mathsf{X}}\mathrm{d}\mu+\int_{\mathsf{X}}\mathrm{d}\nu\) if \(\mu\ll\nu\) and \(\mathrm{KL}(\mu\mid\nu)=+\infty\) otherwise. For any \(T>0\), we denote by \(\mathrm{C}([0,T],\mathbb{R}^{d})\) the space of continuous functions from \([0,T]\) to \(\mathbb{R}^{d}\). For any path measure \(\mathbb{P}\in\mathscr{P}(\mathrm{C}([0,T],\mathbb{R}^{d}))\), we denote by \(\mathrm{Ext}(\mathbb{P})\in\mathscr{P}^{(2)}\) the coupling between the _extremal_ distributions of \(\mathbb{P}\), _i.e._, \(\mathrm{Ext}(\mathbb{P})=\mathbb{P}_{0,T}\). Note that, for a given coupling \(\pi_{0,T}\in\mathscr{P}^{(2)}\), there may exist several path measures \(\mathbb{P}\) verifying \(\mathrm{Ext}(\mathbb{P})=\pi_{0,T}\). For any undirected tree \(\mathsf{T}=(\mathsf{V},\mathsf{E})\) with vertices \(\mathsf{V}\) and edges \(\mathsf{E}\), we denote by \(\{v,v^{\prime}\}\) (or \(\{v^{\prime},v\}\)) the undirected edge between \(v\in\mathsf{V}\) and \(v^{\prime}\in\mathsf{V}\), if it exists. Given \(r\in\mathsf{V}\), we denote by \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\) the directed version of \(\mathsf{T}\) rooted in \(r\), where the directed edges \(\mathsf{E}_{r}\) are uniquely defined from the edges \(\mathsf{E}\), see Appendix B for further details. In this case, the edge linking \(v\in\mathsf{V}\) to \(v^{\prime}\in\mathsf{V}\) in \(\mathsf{T}_{r}\) is denoted by \((v,v^{\prime})\). Finally, for any integers \((n,K)\in\mathbb{N}\times\mathbb{N}^{*}\), we define \(n\bmod(K)\) as the the remainder of the Euclidean division of \(n\) by \(K\).

## 2 Background and setting

Multi-marginal optimal transport.Let \(\ell\in\mathbb{N}^{*}\). Given a cost function \(c:(\mathbb{R}^{d})^{\ell+1}\to\mathbb{R}\), a subset \(\mathsf{S}\subset\{0,\ldots,\ell\}\) and a family of probability measures \(\{\mu_{i}\}_{i\in\mathsf{S}}\in\mathscr{P}^{|S|}\), mOT consists in solving

\[\pi^{\star}=\arg\min\left\{\int c(x_{0:\ell})\mathrm{d}\pi(x_{0:\ell}):\pi\in \mathscr{P}^{(\ell+1)},\ \pi_{i}=\mu_{i}\,\forall i\in\mathsf{S}\right\},\] (mOT)

where \(\pi_{i}\) is the \(i\)-th marginal of \(\pi\), _i.e._, \(\pi_{i}(\mathsf{A})=\pi(\text{proj}_{i}^{-1}(\mathsf{A}))\) for any \(\mathsf{A}\in\mathcal{B}(\mathbb{R}^{d})\), with \(\text{proj}_{i}:x_{0:\ell}\mapsto x_{i}\). Given some weights \((w_{i})_{i\in\{1,\ldots,\ell\}}\in(\mathbb{R}_{+})^{\ell}\), the Wasserstein barycenter between the measures \(\{\mu_{i}\}_{i\in\mathsf{S}}\) is given by \(\pi_{0}^{\star}\) in (mOT), in the case where \(\mathsf{S}=\{1,\ldots,\ell\}\) and \(c(x_{0:\ell})=\sum_{i=1}^{\ell}w_{i}\|x_{0}-x_{i}\|^{2}\)(Peyre et al., 2019). In particular, when \(w_{i}=1/\ell\), the distribution \(\pi_{0}^{\star}\) can be regarded as the Frechet mean (Karcher, 2014) of the measures \(\{\mu_{i}\}_{i\in\mathsf{S}}\) for the Wasserstein distance of order 2. Similarly to OT, (mOT) can be relaxed using the following entropic regularization

\[\pi^{\star}=\arg\min\left\{\int c(x_{0:\ell})\mathrm{d}\pi(x_{0:\ell})+ \varepsilon\mathrm{KL}(\pi|\nu):\pi\in\mathscr{P}^{(\ell+1)},\ \pi_{i}=\mu_{i}\,\forall i\in \mathsf{S}\right\},\] (EmOT)

where \(\varepsilon>0\) is a hyperparameter and \(\nu\) is an arbitrary measure defined on \(((\mathbb{R}^{d})^{\ell+1},\mathcal{B}((\mathbb{R}^{d})^{\ell+1}))\).

Link with Schrodinger Bridge.We first recall the relationship between Schrodinger Bridge and EOT. Given \(T>0\), \(\mathbb{Q}\) a (reference) path measure, _i.e._, \(\mathbb{Q}\in\mathscr{P}(\mathrm{C}([0,T]\,,\mathbb{R}^{d}))\) and two measures \(\mu_{0},\mu_{1}\in\mathscr{P}(\mathbb{R}^{d})\), solving the SB problem amounts to finding the path measure \(\mathbb{P}^{\star}\) defined by

\[\mathbb{P}^{\star}=\arg\!\min\{\mathrm{KL}(\mathbb{P}|\mathbb{Q})\ :\ \mathbb{P}\in \mathscr{P}(\mathrm{C}([0,T]\,,\mathbb{R}^{d})),\ \mathbb{P}_{0}=\mu_{0},\ \mathbb{P}_{T}=\mu_{1}\}\.\] (SB)

If \(\mathbb{Q}\) is associated with a Stochastic Differential Equation (SDE)2, of the form \(\mathrm{d}\mathbf{X}_{t}=-a\mathbf{X}_{t}\mathrm{d}t+\mathrm{d}\mathbf{B}_{t}\), with \(a\geq 0\), then it can be shown, see (Leonard, 2014, Proposition 1) that \(\mathbb{P}^{\star}_{0,T}\) verifies

Footnote 2: We refer to Appendix C for details on solutions of SDEs and associated measures.

\[\mathbb{P}^{\star}_{0,T}=\arg\!\min\{\mathrm{KL}(\pi|\mathbb{Q}_{0,T})\ :\ \pi\in \mathscr{P}^{(2)},\ \pi_{0}=\mu_{0},\ \pi_{1}=\mu_{1}\}\.\] (static-SB)

This is called the _static_ formulation of SB. It can be shown that solving (static-SB) is equivalent to solving EOT with quadratic cost and regularization \(\varepsilon=2\sinh(aT)/a\) if \(a>0\), \(\varepsilon=2T\) if \(a=0\). Moreover, since \(\mathbb{P}^{\star}=\mathbb{P}^{\star}_{0,T}\otimes\mathbb{Q}_{|0,T}\), where \(\mathbb{Q}_{|0,T}\) is the measure \(\mathbb{Q}\) conditioned on initial and terminal conditions, solving the _dynamic_ problem (SB) is equivalent to solving (static-SB).

Similarly, (EmOT) can be easily rewritten in a _static_ multi-marginal SB fashion

\[\pi^{\star}=\arg\!\min\{\mathrm{KL}(\pi|\pi^{0})\ :\ \pi\in\mathscr{P}^{(\ell+1)},\ \pi_{i}=\mu_{i}\,\forall i\in\mathsf{S}\}\,\] (mSB-like)

with \((\mathrm{d}\pi^{0}/\mathrm{d}\mathrm{Leb})(x_{0:\ell})\propto\exp[-c(x_{0:\ell} )/\varepsilon](\mathrm{d}\nu/\mathrm{d}\mathrm{Leb})(x_{0:\ell})\), where \(\pi^{0}\) is the _reference_ measure.

Diffusion Schrodinger Bridge.Recently, De Bortoli et al. (2021) introduced Diffusion Schrodinger Bridge (DSB), a numerical scheme to solve (SB). It approximates the iterates of a _dynamic_ version of the _Iterative Proportional Fitting_ (IPF) scheme (Sinkhorn and Knopp, 1967; Knight, 2008; Peyre et al., 2019; Cuturi and Doucet, 2014), which can be described as follows: consider a sequence of path measures \((\mathbb{P}^{n})_{n\in\mathbb{N}}\) such that \(\mathbb{P}^{0}=\mathbb{Q}\) and for any \(n\in\mathbb{N}\)

\[\mathbb{P}^{2n+1}=\arg\!\min\{\mathrm{KL}(\mathbb{P}|\mathbb{P}^{2n})\ :\ \mathbb{P}_{T}=\mu_{1}\},\qquad\mathbb{P}^{2n+2}=\arg\!\min\{\mathrm{KL}( \mathbb{P}|\mathbb{P}^{2n+1})\ :\ \mathbb{P}_{0}=\mu_{0}\}\.\]

This procedure alternatively projects between the measures with fixed initial distribution and the ones with fixed terminal distribution. For the first iteration, we get that \(\mathbb{P}^{1}=\mu_{1}\otimes\mathbb{Q}_{|T}\). Assuming that \(\mathbb{Q}\) is given by \(\mathrm{d}\mathbf{X}_{t}=f_{t}(\mathbf{X}_{t})\mathrm{d}t+\mathrm{d}\mathbf{B}_ {t}\), with \(f:\ [0,T]\times\mathbb{R}^{d}\to\mathbb{R}^{d}\), then \(\mathbb{P}^{1}\) is associated with the _time-reversal_ of this SDE initialized at \(\mu_{1}\). The time-reversal of an SDE has been derived under mild assumptions on the drift and diffusion coefficients (Haussmann and Pardoux, 1986; Cattiaux et al., 2021). In this case, we have \((\mathbf{Y}_{T-t})_{t\in[0,1]}\sim\mathbb{P}^{1}\), with \(\mathbf{Y}_{0}\sim\mu_{1}\) and

\[\mathrm{d}\mathbf{Y}_{t}=\{-f_{T-t}(\mathbf{Y}_{t})+\nabla\log p_{T-t}(\mathbf{Y }_{t})\}\mathrm{d}t+\mathrm{d}\mathbf{B}_{t},\]

where \(p_{t}\) is the density of \(\mathbb{P}^{0}_{t}\) w.r.t. the Lebesgue measure. The score \(\nabla\log p_{t}\) is estimated using score matching techniques (Hyvarinen, 2005; Vincent, 2011). The first iterate of DSB, \(\mathbb{P}^{1}\), corresponds to a _denoising diffusion model_(Ho et al., 2020; Song et al., 2021). DSB iterates further and not only parameterizes the backward process but also the forward process. It can therefore be seen as a refinement of diffusion models drawing a bridge between generative modeling and optimal transport.

Tree-based framework.Consider an undirected tree \(\mathsf{T}=(\mathsf{V},\mathsf{E})\), with vertices \(\mathsf{V}\) and edges \(\mathsf{E}\), such that \(\mathsf{V}\) is identified with \(\{0,\ldots,\ell\}\). Inspired by Haasler et al. (2021), we restrict our study of (EmOT), to the case where the cost function \(c\) is the tree-structured _quadratic_ cost derived from \(\mathsf{T}\)

\[c(x_{0:\ell})=\sum_{\{v,v^{\prime}\}\in\mathsf{E}}w_{v,v^{\prime}}\|x_{v}-x_{v^ {\prime}}\|_{2}^{2}\,\] (1)

where \(w_{v,v^{\prime}}\) is a weight on the edge \(\{v,v^{\prime}\}\), which links \(v\) to \(v^{\prime}\) (and \(v^{\prime}\) to \(v\)). Furthermore, as in Haasler et al. (2021), we choose \(\mathsf{S}\), _i.e._, the set of vertices of \(\mathsf{T}\) with constrained marginals, to coincide with the _leaves_ of \(\mathsf{T}\). This framework recovers important applications, from Wasserstein barycenters to Wasserstein propagation, see Solomon et al. (2014, 2015). We emphasize that it differs from an OT problem defined on the space of graphs (Chen et al., 2016). Here, each node represents a probability measure (observed or to be inferred) and each edge represents a coupling between two distributions.

We consider an arbitrary vertex \(r\in\mathsf{V}\) and choose \(\nu\) in (EmOT) such that \((\mathrm{d}\nu/\mathrm{d}\mathrm{Leb})(x_{0:\ell})=\varphi_{r}(x_{r})\), where \(\varphi_{r}\) is a density defined on \(\mathbb{R}^{d}\). Due to the form of \(\nu\) and \(c\), the reference measure \(\pi^{0}\) in (mSB-like) is therefore a _probability_ distribution which factorizes along \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\), the directed version of \(\mathsf{T}\) rooted in \(r\). We refer to Appendix B for more details on the notion of directed trees. In this setting, (EmOT) is equivalent to the tree-based problem

\[\pi^{\star}=\mathrm{argmin}\{\mathrm{KL}(\pi|\pi^{0})\,:\ \pi\in \mathscr{P}^{(\mathsf{V}|)},\ \pi_{i}=\mu_{i}\,\forall i\in\mathsf{S}\}\,\] (TreeSB) \[\text{with}\quad\pi^{0}=\pi^{0}_{r}\bigotimes_{(v,v^{\prime})\in \mathsf{E}_{r}}\pi^{0}_{v^{\prime}|v}\,\] (2)

where \(\pi^{0}_{v^{\prime}|v}(\cdot\mid x_{v})=\mathrm{N}(x_{v},\varepsilon/(2w_{v,v^ {\prime}})\mathrm{I}_{d})\) and \(\pi^{0}_{r}\ll\mathrm{Leb}\) with density \(\varphi_{r}\). In a manner akin to Haasler et al. (2021), we thus establish, in _continuous_ state-space, the correspondence between (TreeSB), a _static_ tree-based version of SB, and a version of EmOT with tree-structured cost (1). In our work, we make the following assumption on the constrained marginals \(\{\mu_{i}\}_{i\in\mathsf{S}}\).

**A0**.: _For any \(i\in\mathsf{S}\), \(\mu_{i}\ll\mathrm{Leb}\) and \(\mathrm{H}(\mu_{i})<\infty\)._

In what follows, we define \(K\) as the number of leaves of \(\mathsf{T}\), denoting \(\mathsf{S}=\{i_{0},\ldots,i_{K-1}\}\), and define the horizon times \(T_{v,v^{\prime}}=\varepsilon/(2w_{v,v^{\prime}})\) for any \(\{v,v^{\prime}\}\in\mathsf{E}\). For any \(i_{k}\in\mathsf{S}\), we will denote by \(\mathsf{T}_{k}=(\mathsf{V},\mathsf{E}_{k})\) the directed version of \(\mathsf{T}\) rooted in the leaf \(i_{k}\). In the next section, we present our _dynamic_ method to solve (TreeSB), called _Tree-based Diffusion Schrodinger Bridge_.

## 3 Tree-based Diffusion Schrodinger Bridge

In this section, we present a method to solve (TreeSB) in the case where \(r\in\mathsf{S}\), _i.e._, \(r\) is a leaf of \(\mathsf{T}\). We refer to Appendix E for the extension to the case where \(r\in\mathsf{V}\backslash\mathsf{S}\). Without loss of generality, see Appendix E, we assume that \(r=i_{K-1}\) and choose \(\varphi_{r}=\mathrm{d}\mu_{i_{K-1}}/\mathrm{d}\mathrm{Leb}\), such that \(\pi^{0}_{i_{K-1}}=\mu_{i_{K-1}}\).

Dynamic approach to mIPF.In order to approximate solutions of (TreeSB), we consider the _multi-marginal_ extension of the IPF algorithm, denoted by mIPF. Namely, we define a sequence of probability distributions \((\pi^{n})_{n\in\mathbb{N}}\) such that for any \(n\in\mathbb{N}\)

\[\pi^{n+1}=\mathrm{argmin}\{\mathrm{KL}(\pi|\pi^{n})\,:\ \pi\in\mathscr{P}^{( \mathsf{V}|)},\ \pi_{i_{k_{n}+1}}=\mu_{i_{k_{n}+1}}\}\,\] (mIPF)

where \(k_{n}=(n-1)\bmod(K)\) and \((k_{n}+1)\) is identified with \(n\bmod(K)\). We define a _mIPF cycle_ as a sequence of \(K\) consecutive mIPF updates. In particular, each marginal constraint is considered exactly once during one mIPF cycle. In a practical setting, our main aim is to sample from the (mIPF) iterates at the lowest cost. Although these updates can be made explicit, see Marino and Gerolin (2020) for instance, direct sampling is unfeasible in practice when \(d\) is large. To overcome this limitation, we suggest to compute these iterates in a _dynamic_ fashion with equivalent path measures.

Since \(\pi^{0}\) factorizes along \(\mathsf{T}\), see (2), one can show that the iterates of (mIPF) also factorize along \(\mathsf{T}\), see Section 4. Since these iterates all have a constrained marginal, we obtain the following decomposition for any \(n\in\mathbb{N}\): \(\pi^{n}=\mu_{i_{k_{n}}}\otimes_{(v,v^{\prime})\in\mathsf{E}_{k_{n}}}\pi^{n}_{v^ {\prime}|v}\) where \(\mathsf{E}_{k_{n}}\) denotes the set of edges of the directed tree \(\mathsf{T}_{k_{n}}\). Then, our approach consists in computing _dynamic_ iterates, _i.e._, path measures, along the edges of \(\mathsf{T}\) that coincide on their extremal times with the _static_ iterates \((\pi^{n})_{n\in\mathbb{N}}\). Namely, for any \(n\in\mathbb{N}\), for any edge \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\), we define a path measure \(\mathbb{P}^{n}_{(v,v^{\prime})}\in\mathscr{P}(\mathrm{C}([0,T_{v,v^{\prime}}], \mathbb{R}^{d}))\) such that \(\mathrm{Ext}(\mathbb{P}^{n}_{(v,v^{\prime})})=\pi^{n}_{v,v^{\prime}}\), where \(\mathrm{Ext}(\mathbb{P}^{n}_{(v,v^{\prime})})\) stands for the joint distribution of \(\mathbb{P}^{n}_{(v,v^{\prime})}\) at times 0 and \(T_{v,v^{\prime}}\). In particular, it comes that \(\pi^{n}_{v^{\prime}|v}=\mathbb{P}^{n}_{(v,v^{\prime}),T_{v,v^{\prime}}|0}\). Using the tree-based form of the (mIPF) iterates, we can thus sample from \(\pi^{n}\) by (i) following the directed edges of \(\mathsf{T}_{k_{n}}\), (ii) diffusing along them the corresponding path measures \((\mathbb{P}^{n}_{(v,v^{\prime})})_{(v,v^{\prime})\in\mathsf{E}_{k_{n}}}\) and (iii) picking the samples on the vertices. When \(\mathsf{T}\) is a _bridge-shaped_ tree (2 vertices, 1 edge), it simply reduces to the dynamic reformulation of the IPF scheme. In what follows, we explain how to obtain our _dynamic_ sequence.

Definition of the dynamic iterates.We first compute the iterate \(\mathbb{P}^{0}\), corresponding to the dynamic version of \(\pi^{0}\) defined (2), in Proposition 1. Then, we build the following iterates by recursion on \(n\in\mathbb{N}\) and prove their well-posedness in Proposition 2.

**Proposition 1**.: _Let \(\mathsf{T}_{K-1}=(\mathsf{V},\mathsf{E}_{K-1})\), the directed tree associated with \(\mathsf{T}=(\mathsf{V},\mathsf{E})\) and root \(i_{K-1}\). Then, for any \((v,v^{\prime})\in\mathsf{E}_{K-1}\), there exists \(\mathbb{P}^{0}_{(v,v^{\prime})}\in\mathscr{P}(\mathrm{C}([0,T_{v,v^{\prime}}],\mathbb{R}^{d}))\) with \(\mathrm{Ext}(\mathbb{P}^{0}_{(v,v^{\prime})})=\pi^{0}_{(v,v^{\prime})}\) and such that \(\mathbb{P}^{0}_{(v,v^{\prime})[0}\) is the distribution of \((\mathbf{B}_{t})_{t\in[0,T_{v,v^{\prime}}]}\), recalling that \(T_{v,v^{\prime}}=\varepsilon/(2w_{v,v^{\prime}})\)._

Before deriving the dynamic counterpart of the (mIPF) iterates, we introduce several definitions. For any path measure \(\mathbb{P}\), we denote by \(\mathbb{P}^{R}\) the _time-reversal_ of \(\mathbb{P}\). For any directed tree and any vertex \(v\) of this tree, \(p(v)\) refers to the (unique) _parent_ of \(v\), and \(c(v)\) to the unique _child_ of \(v\) when it exists, see Appendix B for more details.

Let \(n\in\mathbb{N}\). Assume that we have defined the sequence of our dynamic iterates \((\mathbb{P}^{m}_{(v,v^{\prime})})_{(v,v^{\prime})\in\mathsf{E}_{k_{m}},m\leq n}\) up to stage \(n\).

Consider the path \(\mathsf{P}_{n}=\{(v_{j},v_{j+1})\}_{j=1}^{J}\) in the directed tree \(\mathsf{T}_{k_{n}}\) such that \(v_{1}=i_{k_{n}}\) and \(v_{J+1}=i_{k_{n}+1}\). In particular, for any \((v,v^{\prime})\in\mathsf{E}_{k_{n}+1}\), either \((v^{\prime},v)\in\mathsf{P}_{n}\) or \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\setminus\mathsf{P}_{n}\). This is illustrated in Figure 1 when \(\mathsf{V}=\{0,1,2,3,4\}\), \(S=\{2,3,4\}\), \(i_{k}=3\) and \(i_{k+1}=4\): in this case, \(\mathsf{P}=\{(3,1),(1,0),(0,4)\}\) and \((1,2)\) is the only edge common to \(\mathsf{E}_{k}\) and \(\mathsf{E}_{k+1}\).

Consider now the directed tree \(\mathsf{T}_{k_{n+1}}\). We define the \((n+1)\)-th iterate of our dynamic sequence by recursion on the edges of this tree, following the breadth-first order. In this order, \((i_{k_{n}+1},c(i_{k_{n}+1}))=(v_{J+1},v_{J})\) is the first edge considered.

First, we define \(\mathbb{P}^{n+1}_{(v_{J+1},v_{J})}=\mu_{i_{k_{n+1}}}\otimes(\mathbb{P}^{n}_{( v_{J},v_{J+1})})_{[0]}^{R}\). In the case of a _bridge-shaped_ tree, this is exactly the \((n+1)\)-th update described in DSB. Then, for any \((v,v^{\prime})\in\mathsf{E}_{k_{n}+1}\backslash\{(v_{J+1},v_{J})\}\),

1. either \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\backslash\mathsf{P}_{n}\), and we define \(\mathbb{P}^{n+1}_{(v,v^{\prime})}=\mathbb{P}^{n+1}_{(p(v),v),T_{p(v),v}}\otimes \mathbb{P}^{n}_{(v,v^{\prime})[0]}\),
2. or \((v^{\prime},v)\in\mathsf{P}_{n}\), and we define \(\mathbb{P}^{n+1}_{(v,v^{\prime})}=\mathbb{P}^{n+1}_{(p(v),v),T_{p(v),v}}\otimes (\mathbb{P}^{n}_{(v^{\prime},v)})_{[0]}^{R}\).

**Proposition 2**.: _Consider the sequence of dynamic iterates defined by (a) and (b). Then, for any \(n\in\mathbb{N}\) and any \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\), \(\mathbb{P}^{n}_{(v,v^{\prime})}\in\mathscr{P}(\mathrm{C}([0,T_{v,v^{\prime}}],\mathbb{R}^{d}))\) and we have \(\mathrm{Ext}(\mathbb{P}^{n}_{(v,v^{\prime})})=\pi^{n}_{(v,v^{\prime})}\)._

Proposition 2 highlights the equivalence between the (mIPF) iterates and our dynamic iterates. These path measures are defined iteratively, by following the updates (a) and (b) along the edges of \(\mathsf{T}\). The key observation here is that the computation of each dynamic iterate reduces to a sequence of updates (b) on a _path_ linking two leaves of \(\mathsf{T}\). We emphasize that our iterates could be similarly obtained by directly considering a dynamic formulation of (TreeSB) and introducing the formalism of deterministic time branching processes. We leave the study of this problem for future work. We now get into the details of our practical implementation, which relies on score-based methods.

Approximation of the dynamic iterates.The time-reversal operated in the update (b) can be computed explicitly, see Haussmann and Pardoux (1986) for instance. Indeed, assuming that \(\mathbb{P}^{n}_{(v^{\prime},v)}\) is associated with \(\mathrm{d}\mathbf{X}_{t}=f_{t,v^{\prime},v}(\mathbf{X}_{t})\mathrm{d}t+ \mathrm{d}\mathbf{B}_{t}\) with \(\mathbf{X}_{0}\sim\pi^{n}_{v^{\prime}}\), then, under mild conditions, its time-reversal \((\mathbb{P}^{n}_{(v^{\prime},v)})^{R}\) is associated with \(\mathrm{d}\mathbf{Y}_{t}=\{-f_{T-t,v^{\prime},v}+\nabla\log p_{v^{\prime},v,T-t} \}(\mathbf{Y}_{t})\mathrm{d}t+\mathrm{d}\mathbf{B}_{t}\) with \(\mathbf{Y}_{0}\sim\pi^{n+1}_{v}\), where \(p_{v^{\prime},v,t}\) is the density of \(\mathbb{P}^{n}_{(v^{\prime},v),t}\) w.r.t. the Lebesgue measure. The score \(\nabla\log p_{v^{\prime},v,T-t}\) can then be approximated using score-matching techniques (Hyvarinen, 2005; Vincent, 2011) which are now ubiquitous in diffusion models (Song et al., 2021) and used in DSB De Bortoli et al. (2021). Therefore, at iteration \((n+1)\), the update (b) is similar to the one of DSB _for each edge_ on the path joining \(i_{k_{n}}\) and \(i_{k_{n}+1}\). In practice, we parameterize the drifts \(f_{t,v,v^{\prime}}\) for any \(\{v,v^{\prime}\}\in\mathsf{E}\) with neural networks \(f_{t,\theta_{v,v^{\prime}}}\) and use the _mean-matching_ loss introduced by De Bortoli et al. (2021). Note that doing so, we obtain \(2|\mathsf{E}|\) neural networks. The whole procedure consisting in computing our dynamic iterates using the DSB framework is called _Tree-based Diffusion Schrodinger Bridge_ (TreeDSB) and is summarized in Algorithm 1.

Figure 1: Illustration of the change of root in a toy tree with 5 vertices.

**TreeDSB on a toy tree.** We consider a star-shaped tree with three leaves denoted \(\{1,2,3\}\) and its central node \(\{0\}\). Following (2), we define \(\pi^{0}\) with \(r=3\) and \(\varphi_{r}=(\mathrm{d}\mu_{3}/\mathrm{d}\mathrm{Leb})\). During the first iteration of TreeDSB, \(\mathsf{T}\) is rooted at vertex \(3\) and we compute samples from the _forward_ path \(\mathsf{P}_{0}=\{(3,0),(0,1)\}\) with Brownian motions, see Proposition 1, in order to learn the _backward_ path \(\{(1,0),(0,3)\}\). In the next iteration, we re-root the tree \(\mathsf{T}\) at vertex \(1\) and consider the _forward_ path \(\mathsf{P}_{1}=\{(1,0),(0,2)\}\), where the edges \((1,0)\) and \((0,2)\) are respectively given by the first iteration and the initialisation. This highlights that _TreeDSB does not require to update the whole tree_. The following iterations are done similarly. At each iteration \(n\in\mathbb{N}\), we sample from \(\pi^{n}\) by first sampling from \(\mu_{k_{n}}\) at leaf \(i_{k_{n}}\) and then following the parameterized SDEs on the directed edges of \(\mathsf{T}_{k_{n}}\).

## 4 Theoretical properties of mIPF

In this section, we study some of the theoretical properties of the _static_ iterates \((\pi^{n})_{n\in\mathbb{N}}\), that are equivalent to our _dynamic_ iterates according to Proposition 2. In the case where the cost function \(c\) is bounded in (EmOT), results of convergence of (mIPF) exist (Marino and Gerolin, 2020; Carlier, 2022). However, our setting does not satisfy their assumptions, since our transport cost is quadratic and the measures are defined on \(\mathbb{R}^{d}\). In what follows, we provide the first non-quantitative convergence results for (mIPF) in a _non-compact_ setting.

For the rest of the section, we consider a static formulation of the multi-marginal Schrodinger bridge problem which is more general than (TreeSB), defined as

\[\pi^{\star}=\operatorname{argmin}\{\mathrm{KL}(\pi|\pi^{0})\,:\,\,\pi\in \mathscr{P}^{(\ell+1)},\,\,\pi_{i}=\mu_{i}\,\,,\forall i\in\mathsf{S}\}\,\] (static-mSB)

where \(\mathsf{S}\subset\{0,\dots,\ell\}\), \(\pi^{0}\in\mathscr{P}\), \(\{\mu_{i}\}_{i\in\mathsf{S}}\in\mathscr{P}^{|\mathsf{S}|}\). We consider the following set of assumptions.

**A1**.: _There exists a family of measures \(\{\nu_{i}\}_{i\in\{0,\dots,\ell\}}\) defined on \((\mathbb{R}^{d},\mathcal{B}(\mathbb{R}^{d}))\) such that \(\pi^{0}\ll\bigotimes_{i=0}^{\ell}\nu_{i}\) with density \(h=\mathrm{d}\pi^{0}/(\mathrm{d}\bigotimes_{i=0}^{\ell}\nu_{i})\) and \(\mu_{i}\ll\nu_{i}\) with density \(r_{i}=\mathrm{d}\mu_{i}/\mathrm{d}\nu_{i}\) for any \(i\in\mathsf{S}\)._

**A2**.: \(\{\pi\in\mathscr{P}^{(\ell+1)}:\mathrm{KL}(\pi\mid\pi^{0})<\infty,\,\,\pi_{i} =\mu_{i},\,\,\forall i\in\mathsf{S}\}\neq\emptyset\) _._

**A3**.: _There exists a family of probability measures \(\{\tilde{\mu}_{j}\}_{j\in\{0,\dots,\ell\}\setminus\mathsf{S}}\) such that \(\pi^{0}\sim\tilde{\pi}^{0}\), where \(\tilde{\pi}^{0}=\bigotimes_{i\in\mathsf{S}}\mu_{i}\bigotimes_{j\in\{0,\dots, \ell\}\setminus\mathsf{S}}\tilde{\mu}_{j}\)._

Figure 2: Illustration of one mIPF cycle solved by TreeDSB for a toy star-shaped tree. At each iteration, our method learns the _backward_ stochastic process (dotted arrows) that goes from the target leaf (green-circled), corresponding to the constrained marginal, to the current root of the tree (red-circled) by using samples from the _forward_ stochastic process (solid arrows).

In particular, (static-mSB) recovers (TreeSB) by considering \(\nu_{i}=\mathrm{Leb}\) for any \(i\in\{0,\ldots,\ell\}\) and \(h(x_{0:\ell})=\varphi_{r}(x_{r})\exp[-c(x_{0:\ell})/\varepsilon]\) in \(\mathbf{A}1\). We detail in Appendix D how \(\mathbf{A}2\) and \(\mathbf{A}3\) can be met in (TreeSB). Under these assumptions, the multi-marginal Schrodinger Bridge exists.

**Proposition 3**.: _Assume \(\mathbf{A}1\) and \(\mathbf{A}2\). Then, there exists a unique solution \(\pi^{\star}\) to (static-mSB). In addition, assume \(\mathbf{A}3\). Then, there exists a family \(\{\psi^{\star}_{i}\}_{i\in\mathsf{S}}\) of measurable functions \(\psi^{\star}_{i}:\mathbb{R}^{d}\to\mathbb{R}\) such that_

\[(\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\psi^{ \star}_{i}]\quad\pi^{0}\text{-a.s.}\]

In order to establish the existence and uniqueness result of Proposition 3, we extend results from Nutz (2021) to the multi-marginal setting. A consequence of Proposition 3 is that the iterates of (mIPF) can be described using potentials.

**Corollary 4**.: _Assume \(\mathbf{A}1\), \(\mathbf{A}2\) and \(\mathbf{A}3\). Let \((\pi^{n})_{n\in\mathbb{N}}\) be the sequence given by (mIPF). Then, for any \(n\in\mathbb{N}^{\star}\) with \(k_{n}=(n-1)\,\mathrm{mod}(K)\) and \(q_{n}\in\mathbb{N}\) such that \(n=q_{n}K+k_{n}+1\), there exists a family of measurable functions \(\{\psi^{n_{n}+1}_{i_{0}},\ldots,\psi^{q_{n}+1}_{i_{k_{m}}},\psi^{q_{n}}_{i_{k_ {m+1}}},\ldots,\psi^{q_{n}}_{i_{K-1}}\}\) such that_

\[(\mathrm{d}\pi^{n}/\mathrm{d}\pi^{0})(x_{0:\ell})=\exp[\bigoplus_{j=0}^{k_{n} }\psi^{q_{n}+1}_{i_{j}}(x_{i_{j}})\bigoplus_{j=k_{n}+1}^{K-1}\psi^{q_{n}}_{i_{ j}}(x_{i_{j}})]\quad\pi^{0}\text{-a.s.}\]

In the tree-based setting, Corollary 4 explains why the (mIPF) iterations preserve the tree-based Markovian nature of \(\pi^{0}\). We now prove that the marginal \(\pi^{n}_{i}\) converges to \(\mu_{i}\) for any \(i\in\mathsf{S}\), as \(n\) goes to infinity, _i.e._, we have marginal convergence on the leaves of \(\mathsf{T}\).

**Proposition 5**.: _Assume \(\mathbf{A}1\) and \(\mathbf{A}2\). Let \((\pi^{n})_{n\in\mathbb{N}}\) be the sequence given by (mIPF). Then, we have \(\lim_{n\to\infty}\|\pi^{n}_{i}-\mu_{i}\|_{\mathrm{TV}}=0\) for any \(i\in\mathsf{S}\)._

The previous result does not ensure the convergence of \((\pi^{n})_{n\in\mathbb{N}}\) to the solution to (static-mSB). In particular, Proposition 5 does not provide the convergence of the marginals on the nodes \(v\in\mathsf{V}\setminus\mathsf{S}\), which is key to compute regularized Wasserstein barycenters with TreeDSB. Relying on additional assumptions, we now derive the convergence of (mIPF).

**A4**.: \(\bigoplus_{i\in\mathsf{S}}\mathrm{L}^{1}(\mu_{i})\subset\mathrm{L}^{1}(\pi^{ \star})\) _is closed._

**A5**.: _There exist \(\bar{c}\in(0,\infty)\) such that \(\exp(\psi^{n}_{i_{k}}-\psi^{n+1}_{i_{k}})\leq\bar{c}\), for any \(n\in\mathbb{N}\), any \(k\in\{0,\ldots,K-2\}\)._

These assumptions can be seen as multi-marginal extensions of the ones of Ruschendorf (1995), see Appendix D for a discussion and examples.

**Proposition 6**.: _Assume \(\mathbf{A}1\), \(\mathbf{A}2\), \(\mathbf{A}3\), \(\mathbf{A}4\) and \(\mathbf{A}5\). Let \((\pi^{n})_{n\in\mathbb{N}}\) be the sequence given by (mIPF). Then, we have \(\lim_{n\to\infty}\|\pi^{n}-\pi^{\star}\|_{\mathrm{TV}}=0\), where \(\pi^{\star}\) is given in Proposition 3._

To the best of our knowledge, Proposition 6 is the first convergence result of (mIPF) without assuming that the space is compact or that the cost is bounded. We highlight that traditional techniques to prove the convergence of IPF cannot be easily extended to the multi-marginal setting as pointed by Carlier (2022). In the case of bounded cost, quantitative results exist (Marino & Gerolin, 2020; Carlier, 2022). We leave the study of such results in the _unbounded_ cost setting for future work.

## 5 Application to Wasserstein barycenters

Although Algorithm 1 can be applied to trees \(\mathsf{T}\) with fixed marginals on the leaves, one case of particular interest is star-shaped trees, _i.e._, trees with a central node, denoted by index \(0\), and such that \(\mathsf{S}=\{1,\ldots,\ell\}\) (see Figure 2 for an illustration with \(\ell=3\)). In this section, we draw a link between (TreeSB) and regularized Wasserstein barycenters. We recall the definition of the Wasserstein distance of order \(2\) with \(\varepsilon\)-entropic regularization between \(\mu\) and \(\nu\)(Peyre et al., 2019, Chapter 4)

\[W^{2}_{2,\varepsilon}(\mu,\nu)=\inf\{\int\|x_{1}-x_{0}\|^{2}\mathrm{d}\pi(x_{0},x_{1})-\varepsilon\mathrm{H}(\pi):\pi\in\mathscr{P}^{(2)},\pi_{0}=\mu,\pi_{1}= \nu\}\.\] (3)

In this work, we consider the \((\ell\varepsilon,(\ell-1)\varepsilon)\)-doubly-regularized Wasserstein-2 barycenter problem (Chizat, 2023) defined as follows

\[\mu^{\star}_{\varepsilon}=\arg\min\{\sum_{i=1}^{\ell}w_{i}W^{2}_{2,\varepsilon/w_ {i}}(\mu,\mu_{i})+(\ell-1)\varepsilon\mathrm{H}(\mu):\mu\in\mathscr{P}\}\,\] (regWB)

where \((w_{i})_{i\in\{1,\ldots,\ell\}}\in(0,+\infty)^{\ell}\). The following proposition shows the equivalence between the barycenter problem (regWB) and the multi-marginal Schrodinger bridge problem (TreeSB) over \(\mathsf{T}\). In particular, it allows us to use TreeDSB to estimate the solution \(\mu^{\star}_{\varepsilon}\) of (regWB).

**Proposition 7**.: _Let \(\varepsilon>0\). Assume **A**0. Also assume that \(\mathsf{T}\) is a star-shaped tree with central node indexed by \(0\), and that the reference measure of \((\mathrm{TreeSB})\) defined in (2) verifies \(r=i_{K-1}\) and \(\varphi_{r}=\mathrm{d}\mu_{i_{K-1}}/\mathrm{d}\mathrm{Leb}>0\). Under **A**2, \((\mathrm{regWB})\) has a unique solution \(\pi_{0}^{\star}\), where \(\pi^{\star}\) solves \((\mathrm{TreeSB})\)._

The proof of this result is postponed to Appendix D. More generally, we show in Appendix D that, for any tree \(\mathsf{T}\), \((\mathrm{TreeSB})\) is equivalent to a regularized version of the Wasserstein propagation problem (Solomon et al., 2014, 2015). Moreover, we present in Appendix E an extension of Proposition 7 in the case where the chosen root \(r\) is not a leaf of \(\mathsf{T}\). We finally emphasize that the formulation of \((\mathrm{regWB})\) leads to a _minimization_ of the entropy of the barycenter. In particular, this allows us to choose \(\varepsilon\) reasonably large in TreeDSB, which is a stability advantage compared to other regularized methods which do not consider this further regularization.

## 6 Related work

Diffusion Schrodinger Bridge.Schrodinger Bridges (Schrodinger, 1932) have been extensively studied using tools from stochastic control and probability theory (Leonard, 2014; Dai Pra, 1991; Chen et al., 2021). More recently, algorithms were proposed to efficiently approximate such bridges in the context of machine learning. In particular, De Bortoli et al. (2021) proposed DSB while Vargas et al. (2021); Chen et al. (2022) developed related algorithms. In Chen et al. (2023), the authors study a multi-marginal version of DSB in a linear tree-based setting, where the set of observed nodes is the whole set of vertices. However, contrary to our setting, Chen et al. (2023) introduced a momentum variable. This allows for smoother trajectories which are desirable for single-cell trajectories applications and correspond to some spline interpolation in the space of probability measures (Chen et al., 2018). A general framework for tree-based static Schrodinger Bridges on discrete state-spaces was given in Haasler et al. (2021). In this work, we extend their formulation to a dynamic and continuous setting, see Appendix D for more a thorough comparison.

Wasserstein barycenters.The notion of Wasserstein barycenter was first introduced in Rabin et al. (2012) and then later studied in Agueh & Carlier (2011). The algorithms to solve this problem can be split into two families: the in-sample based approaches and the parametric ones. In-sample approaches require access to all the measures \(\mu_{i}\) which are assumed to be empirical measures (Cuturi & Doucet, 2014; Benamou et al., 2015; Solomon et al., 2015). Related to this class of algorithms is the semi-discrete approach, which aims at computing a Wasserstein barycenter between continuous distribution but rely on a discretization of the barycenter (Claici et al., 2018; Staib et al., 2017; Mi et al., 2020). Most recent approaches do not rely on a discrete representation of the barycenter, but instead parameterize it using neural networks. These approaches can be further split into two categories. First, _measure-based optimization_ approaches parameterize the measures using a neural network. This is the case of Cohen et al. (2020), where the barycenter is given by a generative model, which is then optimized. Fan et al. (2020) introduce an optimization procedure which relies on a _min-max-min_ problem using the framework of Makkuva et al. (2020). More recently, Korotin et al. (2022) considered a fixed point-based algorithm introduced in Alvarez-Esteban et al. (2016) to update a generative model parametrizing the barycenter. On the one hand, _potential-based methods_ rely on a dual formulation of the barycenter. Korotin et al. (2021) parameterized the dual potentials using Input Convex Neural Network and considered regularizing losses imposing conjugacy and congruency. On the other hand, Li et al. (2020) consider a dual version of the _regularized_ Wasserstein barycenter problem contrary to other works. Our approach applied to start-shaped trees also approximates a _regularized_ Wasserstein barycenter. However, contrary to Li et al. (2020), we do not consider a parameterization of the potentials in the _static_ setting but instead, parameterize the drift of an associated _dynamic_ formulation using Schrodinger bridges. To the best of our knowledge TreeDSB is the first approach leveraging DSB-like algorithms to compute Wasserstein barycenters.

## 7 Experiments

In our experiments3, we illustrate the performance of TreeDSB to compute entropic regularized Wasserstein barycenters for various tasks. We choose to compare our method with state-of-the-art regularized algorithms: fast free-support Wasserstein barycenter (fsWB) (Cuturi & Doucet, 2014), and continuous regularized Wasserstein barycenter (crWB) (Li et al., 2020). In all of our settings, we consider a star-shaped tree with \(K\) leaves and edge weights that are equal to \(1/K\), resulting in asequential training procedure over \(2K\) neural networks. The initial diffusion is always a Brownian motion parameterized as explained in Proposition 1. Hence, the time horizon on each edge is defined by \(T=K\varepsilon/2\). The order of the leaves is randomly shuffled between the mIPF cycles. We consider 50 steps for the time discretization on \([0,T]\). We refer to Appendix G for details on the choice of the schedule, the architecture of the neural networks and the settings of our experiments.

**Synthetic two dimensional datasets.** We first illustrate TreeDSB in a synthetic two dimensional setting. We consider three different datasets _Swiss-roll_ (vertex 0, starting node \(r\)), _Circle_ (vertex 2) and _Moons_ (vertex 3) and compute their Wasserstein barycenter (vertex 1) by running TreeDSB for 50 mIPF cycles with \(\varepsilon=0.1\). In Figure 3, we show the estimated densities of the datasets on the leaves of the tree (we emphasize that the distributions plotted on each leaf are generated from the central barycenter measure). In Figure 4, we observe the consistency between the barycenters generated from the different leaves. In Appendix G, we present additional results for this setting.

**Synthetic Gaussian datasets.** Next, we consider three independent Gaussian distributions with zero mean and random non-diagonal covariance matrices whose conditional number is less than 10, following Fan et al. (2020). In this case, the non-regularized barycenter can be exactly computed. To evaluate the performance of the algorithms, we use the Bures-Wasserstein Unexplained Variance Percentage (UVP), following (Korotin et al., 2021, Section 5). Given a target distribution \(\mu^{\star}\in\mathscr{P}\) and some approximation \(\mu\in\mathscr{P}\), we define

\[\mathrm{BW}^{2}_{2}\text{-}\mathrm{UVP}(\mu,\mu^{\star})=100\cdot 2\, \mathrm{BW}^{2}_{2}(\mu,\mu^{\star})/\,\mathrm{Var}(\mu^{\star})\%\,\]

where \(\mathrm{BW}^{2}_{2}(\mu,\mu^{\star})=W^{2}_{2}(\mathrm{N}(\mathbb{E}[\mu], \mathrm{Cov}(\mu)),\mathrm{N}(\mathbb{E}[\mu^{\star}],\mathrm{Cov}(\mu^{ \star}))\).

In this setting, we choose \(\mu^{\star}\) to be the non-regularized barycenter and assess the dependency w.r.t. the dimension of the algorithms using the \(\mathrm{BW}^{2}_{2}\text{-}\mathrm{UVP}\) metric. In Table 1, we compare ourselves with the two regularized methods Li et al. (2020) (\(\mathrm{L}_{2}\)-reg. equal to \(10^{-4}\)) and Cuturi and Doucet (2014). We run TreeDSB for 10 mIPF cycles with \(\varepsilon=0.1\). Bold numbers represent the best values up to statistical significance. While Li et al. (2020) and Cuturi and Doucet (2014) enjoy better performance in low dimensions (\(d=2\)), TreeDSB outperforms these methods as the dimension increases.

**MNIST Wasserstein barycenter.** We then turn to an image experiment using MNIST dataset (LeCun, 1998). Here, an image is not considered as a 2D-dimensional distribution as in Cuturi and Doucet (2014) and Li et al. (2020), but as a sample from a high-dimensional probability measure (\(d=784\)). We aim at computing a Wasserstein barycenter between the digits \(2\),\(4\) and \(6\). To do so, we

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & \(d=2\) & \(d=16\) & \(d=64\) & \(d=128\) & \(d=256\) \\ \hline fsWB (Cuturi and Doucet, 2014) & \(0.06_{\pm 0.01}\) & \(2.86_{\pm 0.06}\) & \(11.12_{\pm 0.06}\) & \(14.47_{\pm 0.07}\) & \(17.41_{\pm 0.05}\) \\ crWB (Li et al., 2020) & \(\mathbf{0.02_{\pm 0.01}}\) & \(\mathbf{1.52_{\pm 0.11}}\) & \(11.41_{\pm 0.73}\) & \(5.75_{\pm 0.02}\) & \(18.27_{\pm 0.54}\) \\ Tree DSB & \(\mathbf{0.63_{\pm 0.26}}\) & \(\mathbf{1.07_{\pm 0.58}}\) & \(\mathbf{1.39_{\pm 0.07}}\) & \(\mathbf{1.92_{\pm 0.02}}\) & \(\mathbf{2.62_{\pm 0.07}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Gaussian setting: comparison with the regularized methods crWB and fsWB.

Figure 4: From left to right: barycenter estimated from the leaves _Swiss-roll_, _Circle_ and _Moons_.

Figure 3: Estimated densities on the leaves.

run TreeDSB for 10 mlPF cycles with \(r\) that corresponds to the digit 6 and \(\varepsilon=0.5\). In Figure 5, we display samples from the estimated marginals on the leaves, to assess the reconstruction of the digits \(2\), \(4\) and \(6\), and samples from the barycenter, obtained by diffusing from the leaf corresponding to the digit \(6\). Our results prove the scalability of TreeDSB to the high-dimensional setting, compared to state-of-the-art regularized methods. Additional results on MNIST dataset are given in Appendix G.

Subset posterior aggregation.Finally, we evaluate TreeDSB in the context of _Bayesian fusion_(Srivastava et al., 2018), also called posterior aggregation. Given a Bayesian model and a dataset partitioned into several shards, this task aims at recovering the full data posterior distribution from the posterior distributions computed on each shard.

In particular, it has been proved that the barycenter of the subdataset posteriors is close to the full data posterior under mild assumptions (Srivastava et al., 2018). Here, we consider a logistic regression model applied to the wine dataset4(\(d=42\)) and proceed as follows. We first split this dataset into 3 subsets, with or without heterogeneity, and estimate the posterior parameters on each shard. Then, we draw samples from the obtained logistic distributions to define \(\mu_{1},\mu_{2},\mu_{3}\). Then, we compute the Wasserstein barycenter of these measures, and compare it to the posterior computed on the full dataset. As in the synthetic Gaussian experiment, we run TreeDSB for 10 mlPF cycles \(\varepsilon=0.1\) and we compare ourselves with Li et al. (2020) (\(\mathrm{L}_{2}\)-reg. equal to \(10^{-4}\)) and Cuturi and Doucet (2014). We evaluate the methods using the \(\mathrm{BW}_{2}^{2}\)-\(\mathrm{UVP}\) metric, where \(\mu^{\star}\) is the estimated full data posterior, and report the results in Table 2. In both settings, we observe that our method outperforms existing regularized methods to compute Wasserstein barycenters.

Footnote 4: https://archive.ics.uci.edu/ml/datasets/wine

Limitations.One of the main limitation of entropic regularized OT approach is that their behavior is usually badly conditioned as \(\varepsilon\to 0\). In our setting, we observe that if \(\varepsilon\), or equivalently \(T\), is too low then the algorithm becomes less stable as the training of the models slows down. In the future, we plan to mitigate this issue by incorporating fixed point techniques like the one used in Korotin et al. (2022). Finally, since our algorithm is based on DSB (De Bortoli et al., 2021), it suffers from the same limitations. In particular, training different neural networks iteratively incurs some bias in the SDE which is harmful for large number of mIPF iterations.

## 8 Discussion

In this paper, we introduced Tree-based Diffusion Schrodinger Bridge (TreeDSB) a scalable scheme to approximate solutions of entropic-regularized multi-marginal Optimal Transport (mOT) problems. Our methodology leverages tools from the diffusion model literature and extends Diffusion Schrodinger Bridge (De Bortoli et al., 2021). In particular, it approximates the iterates of the multi-marginal Iterative Proportional Fitting (mIPF) algorithm, for which we prove its convergence under mild assumptions. We illustrate the efficiency of TreeDSB for image processing and Bayesian fusion, using the link between mOT and Wasserstein barycenters. In future work, we would like to study quantitative convergence bounds for mIPF in the _unbounded_ cost setting. Another line of work would be to scale TreeDSB to higher dimensional problems building on recent developments in the diffusion model and flow matching community (Lipman et al., 2023; Pelucheti, 2023; Shi et al., 2023).

\begin{table}
\begin{tabular}{l c} \hline \hline Method & Without het. & With het. \\ \hline fsWB (Cuturi and Doucet, 2014) & \(12.95_{\pm 0.35}\) & \(14.43_{\pm 0.51}\) \\ crWB (Li et al., 2020) & \(20.66_{\pm 0.71}\) & \(23.06_{\pm 0.12}\) \\ Tree DSB & \(\mathbf{8.69_{\pm 0.12}}\) & \(\mathbf{8.90_{\pm 0.68}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Bayesian fusion setting: comparison with the regularized methods crWB and fsWB.

Figure 5: Samples from the estimated MNIST 2-4-6 marginals and from their Wasserstein barycenter.

## Acknowledgments

We thank James Thornton for the DSB codebase5 and useful discussions. AD acknowledges support from the Lagrange Mathematics and Computing Research Center. AD and MN would like to thank the Isaac Newton Institute for Mathematical Sciences for support and hospitality during the programme _The mathematical and statistical foundation of future data-driven engineering_ when work on this paper was undertaken. MN acknowledges funding from the grant SCAI (ANR-19-CHIA-0002).

Footnote 5: https://github.com/JTT94/diffusion_schrodinger_bridge

## References

* Acciaio et al. (2019) Acciaio, B., Backhoff-Veraguas, J., and Carmona, R. Extended mean field control problems: stochastic maximum principle and transport perspective. _SIAM journal on Control and Optimization_, 57(6):3666-3693, 2019.
* Agueh & Carlier (2011) Agueh, M. and Carlier, G. Barycenters in the Wasserstein space. _SIAM Journal on Mathematical Analysis_, 43(2):904-924, 2011.
* Alvarez-Esteban et al. (2016) Alvarez-Esteban, P. C., Del Barrio, E., Cuesta-Albertos, J., and Matran, C. A fixed-point approach to barycenters in Wasserstein space. _Journal of Mathematical Analysis and Applications_, 441(2):744-762, 2016.
* Bayraktar et al. (2018) Bayraktar, E., Cox, A. M., and Stoev, Y. Martingale optimal transport with stopping. _SIAM Journal on Control and Optimization_, 56(1):417-433, 2018.
* Benamou et al. (2015) Benamou, J.-D., Carlier, G., Cuturi, M., Nenna, L., and Peyre, G. Iterative Bregman projections for regularized transportation problems. _SIAM Journal on Scientific Computing_, 37(2):A1111-A1138, 2015.
* Bunne et al. (2022) Bunne, C., Papaxanthos, L., Krause, A., and Cuturi, M. Proximal optimal transport modeling of population dynamics. In _International Conference on Artificial Intelligence and Statistics_, pp. 6511-6528. PMLR, 2022.
* Carion et al. (2020) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko, S. End-to-end object detection with transformers. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16_, pp. 213-229. Springer, 2020.
* Carlier (2022) Carlier, G. On the linear convergence of the multimarginal Sinkhorn algorithm. _SIAM Journal on Optimization_, 32(2):786-794, 2022.
* Cattiaux et al. (2021) Cattiaux, P., Conforti, G., Gentil, I., and Leonard, C. Time reversal of diffusion processes under a finite entropy condition. _arXiv preprint arXiv:2104.07708_, 2021.
* Chen et al. (2022) Chen, T., Liu, G.-H., and Theodorou, E. A. Likelihood training of Schrodinger bridge using forward-backward SDEs theory. _International Conference on Learning Representations_, 2022.
* Chen et al. (2023) Chen, T., Liu, G.-H., Tao, M., and Theodorou, E. A. Deep momentum multi-marginal Schrodinger bridge. 2023. doi: 10.48550/ARXIV.2303.01751.
* Chen et al. (2016) Chen, Y., Georgiou, T., Pavon, M., and Tannenbaum, A. Robust transport over networks. _IEEE Transactions on Automatic Control_, 62(9):4675-4682, 2016.
* Chen et al. (2018) Chen, Y., Conforti, G., and Georgiou, T. T. Measure-valued spline curves: An optimal transport viewpoint. _SIAM Journal on Mathematical Analysis_, 50(6):5947-5968, 2018.
* Chen et al. (2021) Chen, Y., Georgiou, T. T., and Pavon, M. Optimal transport in systems and control. _Annual Review of Control, Robotics, and Autonomous Systems_, 4, 2021.
* Chizat (2023) Chizat, L. Doubly regularized entropic Wasserstein barycenters. _arXiv preprint arXiv:2303.11844_, 2023.
* Claici et al. (2018) Claici, S., Chien, E., and Solomon, J. Stochastic Wasserstein barycenters. In _International Conference on Machine Learning_, pp. 999-1008. PMLR, 2018.
* Cattin (2018)Cohen, S., Arbel, M., and Deisenroth, M. P. Estimating barycenters of measures in high dimensions. _arXiv preprint arXiv:2007.07105_, 2020.
* Cotar et al. (2013) Cotar, C., Friesecke, G., and Kluppelberg, C. Density functional theory and optimal transportation with Coulomb cost. _Communications on Pure and Applied Mathematics_, 66(4):548-599, 2013.
* Csiszar (1975) Csiszar, I. I-divergence geometry of probability distributions and minimization problems. _The Annals of Probability_, pp. 146-158, 1975.
* Cuturi (2013) Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in Neural Information Processing Systems_, 26, 2013.
* Cuturi & Doucet (2014) Cuturi, M. and Doucet, A. Fast computation of Wasserstein barycenters. In _International Conference on Machine Learning_, pp. 685-693. PMLR, 2014.
* Dai Pra (1991) Dai Pra, P. A stochastic control approach to reciprocal diffusion processes. _Applied Mathematics and Optimization_, 23(1):313-329, 1991.
* De Bortoli et al. (2021) De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. Diffusion Schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* Dupuis & Ellis (2011) Dupuis, P. and Ellis, R. S. _A weak convergence approach to the theory of large deviations_. John Wiley & Sons, 2011.
* Eisenberger et al. (2020) Eisenberger, M., Toker, A., Leal-Taixe, L., and Cremers, D. Deep shells: Unsupervised shape correspondence with optimal transport. _Advances in Neural Information Processing Systems_, 33:10491-10502, 2020.
* Fan et al. (2017) Fan, J., Taghvaei, A., and Chen, Y. Scalable computations of Wasserstein barycenter via input convex neural networks. _arXiv preprint arXiv:2007.04462_, 2020.
* Feydy et al. (2017) Feydy, J., Charlier, B., Vialard, F.-X., and Peyre, G. Optimal transport for diffeomorphic registration. In _Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I 20_, pp. 291-299. Springer, 2017.
* Flamary et al. (2021) Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel, L., Corenlos, A., Fatras, K., Fournier, N., et al. POT: Python optimal transport. _The Journal of Machine Learning Research_, 22(1):3571-3578, 2021.
* Fortet (1940) Fortet, R. Resolution d'un systeme d'equations de M. Schrodinger. _Journal de Mathematiques Pures et Appliquees_, 19(1-4):83-105, 1940.
* Haasler et al. (2021) Haasler, I., Ringh, A., Chen, Y., and Karlsson, J. Multimarginal Optimal Transport with a tree-structured cost and the Schrodinger bridge problem. _SIAM Journal on Control and Optimization_, 59(4):2428-2453, 2021.
* Haussmann & Pardoux (1986) Haussmann, U. G. and Pardoux, E. Time reversal of diffusions. _The Annals of Probability_, pp. 1188-1205, 1986.
* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hyvarinen (2005) Hyvarinen, A. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Karcher (2014) Karcher, H. Riemannian center of mass and so called Karcher mean. _arXiv preprint arXiv:1407.2087_, 2014.
* Kingma & Ba (2014) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Knight (2008) Knight, P. A. The Sinkhorn-Knopp algorithm: convergence and applications. _SIAM Journal on Matrix Analysis and Applications_, 30(1):261-275, 2008.
* Karcher (2014)Kober, H. A theorem on Banach spaces. _Compositio Mathematica_, 7:135-140, 1940.
* Koller and Friedman (2009) Koller, D. and Friedman, N. _Probabilistic Graphical Models: Principles and Techniques_. MIT press, 2009.
* Korotin et al. (2021) Korotin, A., Li, L., Solomon, J., and Burnaev, E. Continuous Wasserstein-2 barycenter estimation without minimax optimization. _arXiv preprint arXiv:2102.01752_, 2021.
* Korotin et al. (2022) Korotin, A., Egiazarian, V., Li, L., and Burnaev, E. Wasserstein iterative networks for barycenter estimation. _arXiv preprint arXiv:2201.12245_, 2022.
* Kullback (1968) Kullback, S. Probability densities with given marginals. _The Annals of Mathematical Statistics_, 39(4):1236-1243, 1968.
* LeCun (1998) LeCun, Y. The MNIST database of handwritten digits. _http://yann. lecun. com/exdb/mnist/_, 1998.
* Leonard (2014) Leonard, C. A survey of the Schrodinger problem and some of its connections with optimal transport. _Discrete & Continuous Dynamical Systems-A_, 34(4):1533-1574, 2014.
* Li et al. (2020) Li, L., Genevay, A., Yurochkin, M., and Solomon, J. M. Continuous regularized Wasserstein barycenters. _Advances in Neural Information Processing Systems_, 33:17755-17765, 2020.
* Lipman et al. (2023) Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. _International Conference on Learning Representations_, 2023.
* Luo and Tseng (1992) Luo, Z.-Q. and Tseng, P. On the convergence of the coordinate descent method for convex differentiable minimization. _Journal of Optimization Theory and Applications_, 72(1):7-35, 1992.
* Makkuva et al. (2020) Makkuva, A., Taghvaei, A., Oh, S., and Lee, J. Optimal transport mapping via input convex neural networks. In _International Conference on Machine Learning_, pp. 6672-6681. PMLR, 2020.
* Marino and Gerolin (2020) Marino, S. D. and Gerolin, A. An optimal transport approach for the Schrodinger bridge problem and convergence of Sinkhorn algorithm. _Journal of Scientific Computing_, 85(2):27, 2020.
* Mi et al. (2020) Mi, L., Yu, T., Bento, J., Zhang, W., Li, B., and Wang, Y. Variational Wasserstein barycenters for geometric clustering. _arXiv preprint arXiv:2002.10543_, 2020.
* Minsker et al. (2014) Minsker, S., Srivastava, S., Lin, L., and Dunson, D. Scalable and robust Bayesian inference via the median posterior. In _International Conference on Machine Learning_, pp. 1656-1664. PMLR, 2014.
* Nichol and Dhariwal (2021) Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pp. 8162-8171, 2021.
* Nutz (2021) Nutz, M. Introduction to entropic optimal transport, 2021.
* Pele and Werman (2009) Pele, O. and Werman, M. Fast and robust Earth mover's distances. In _2009 IEEE 12th International Conference on Computer Vision_, pp. 460-467. IEEE, 2009.
* Peluchetti (2023) Peluchetti, S. Diffusion bridge mixture transports, Schrodinger bridge problems and generative modeling. _arXiv preprint arXiv:2304.00917_, 2023.
* Peyre et al. (2019) Peyre, G., Cuturi, M., et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* Rabin et al. (2012) Rabin, J., Peyre, G., Delon, J., and Bernot, M. Wasserstein barycenter and its application to texture mixing. In _Scale Space and Variational Methods in Computer Vision: Third International Conference, SSVM 2011, Ein-Gedi, Israel, May 29-June 2, 2011, Revised Selected Papers 3_, pp. 435-446. Springer, 2012.
* Ruschendorf (1995) Ruschendorf, L. Convergence of the Iterative Proportional Fitting procedure. _The Annals of Statistics_, pp. 1160-1174, 1995.
* Schiebinger et al. (2019) Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu, S., Lin, S., Berube, P., et al. Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming. _Cell_, 176(4):928-943, 2019.
* Schiebinger et al. (2019)Schmitz, M. A., Heitz, M., Bonneel, N., Ngole, F., Coeurjolly, D., Cuturi, M., Peyre, G., and Starck, J.-L. Wasserstein dictionary learning: Optimal transport-based unsupervised nonlinear dictionary learning. _SIAM Journal on Imaging Sciences_, 11(1):643-678, 2018.
* Schrodinger (1932) Schrodinger, E. Sur la theorie relativiste de l'electron et l'interpretation de la mecanique quantique. _Annales de l'Institut Henri Poincare_, 2(4):269-310, 1932.
* Shi et al. (2023) Shi, Y., De Bortoli, V., Campbell, A., and Doucet, A. Diffusion Schrodinger bridge matching. _arXiv preprint arXiv:2303.16852_, 2023.
* Sinkhorn & Knopp (1967) Sinkhorn, R. and Knopp, P. Concerning nonnegative matrices and doubly stochastic matrices. _Pacific Journal of Mathematics_, 21(2):343-348, 1967.
* Solomon et al. (2014) Solomon, J., Rustamov, R., Guibas, L., and Butscher, A. Wasserstein propagation for semi-supervised learning. In _International Conference on Machine Learning_, pp. 306-314. PMLR, 2014.
* Solomon et al. (2015) Solomon, J., De Goes, F., Peyre, G., Cuturi, M., Butscher, A., Nguyen, A., Du, T., and Guibas, L. Convolutional Wasserstein distances: Efficient optimal transportation on geometric domains. _ACM Transactions on Graphics (ToG)_, 34(4):1-11, 2015.
* Song et al. (2021) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _International Conference on Learning Representations_, 2021.
* Srivastava et al. (2018) Srivastava, S., Li, C., and Dunson, D. B. Scalable Bayes via barycenter in Wasserstein space. _The Journal of Machine Learning Research_, 19(1):312-346, 2018.
* Staib et al. (2017) Staib, M., Claici, S., Solomon, J. M., and Jegelka, S. Parallel streaming Wasserstein barycenters. _Advances in Neural Information Processing Systems_, 30, 2017.
* Stroock & Varadhan (1997) Stroock, D. W. and Varadhan, S. S. _Multidimensional diffusion processes_, volume 233. Springer Science & Business Media, 1997.
* Su et al. (2015) Su, Z., Wang, Y., Shi, R., Zeng, W., Sun, J., Luo, F., and Gu, X. Optimal mass transport for shape matching and comparison. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 37(11):2246-2259, 2015.
* Valiente (2002) Valiente, G. _Algorithms on Trees and Graphs_, volume 112. Springer, 2002.
* Vargas et al. (2021) Vargas, F., Thodoroff, P., Lamacraft, A., and Lawrence, N. Solving Schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* Vincent (2011) Vincent, P. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.

## Appendix organization

First, additional notation is introduced in Appendix A. Then, we briefly recall some notions on undirected and directed trees in Appendix B. Similarly, martingale problems are introduced in Appendix C. The proofs of the main manuscript and additional theoretical results on Tree Schrodinger Bridges are given in Appendix D. Additional details on our consideration of the tree-based static SB problem are described in Appendix E. Details on the implementation of TreeDSB are given in Appendix F and the experiments are investigated in Appendix G.

## Appendix A Additional notation

For any finite set \(\mathsf{E}\), we equivalently refer to the cardinal of \(\mathsf{E}\) as \(\operatorname{card}(\mathsf{E})\) or \(|\mathsf{E}|\). Let \((\mathsf{X},\mathcal{X})\) be a measurable space. For any \(x\in(\mathbb{R}^{d})^{\ell+1}\) and any \(m\in\{0,\ldots,\ell\}\), let \(x_{-m}=(x_{0},\ldots,x_{m-1},x_{m+1},\ldots,x_{\ell})\). For any family of measures \(\{\nu_{j}\}_{j\in\{0,\ldots,\ell\}}\) defined on \((\mathsf{X},\mathcal{X})\) and any \(i\in\{0,\ldots,\ell\}\), let \(\nu_{-i}=\bigotimes_{j\in\{0,\ldots,\ell\}\setminus\{i\}}\nu_{j}\). Let \(I=\{i_{1},\ldots,i_{q}\}\subset\{1,\ldots,\ell\}\) and \(\mu\in\mathscr{P}^{(\ell)}\) such that \(\mu\ll\operatorname{Leb}\). We define \(I^{\mathsf{c}}=\{1,\ldots,\ell\}\backslash I\) and denote it by \(\{i^{\mathsf{c}}_{1},\ldots,i^{\mathsf{c}}_{q}\}\) where \(\bar{q}=\ell-q\). We denote the marginal of \(\mu\) along \(I\) by \(\mu_{I}\), _i.e._, \(\mu_{I}\in\mathscr{P}^{(q)}\) and we have for any \(\mathsf{A}\in\mathcal{B}((\mathbb{R}^{d})^{q})\), \(\mu_{I}(\mathsf{A})=\int_{\mathsf{X}}\mu(x)\prod_{j=1}^{q}\delta_{x_{j}}( \mathsf{A}_{j})\mathrm{d}x\). In addition, note that \(\mu_{I}\ll\operatorname{Leb}\). We denote the conditional distribution of \(\mu\) given \(I\) by \(\mu_{|I}(\cdot|\cdot)\), _i.e._, \(\mu_{|I}(\cdot|\cdot)\in\mathscr{P}^{(\bar{q})}\times(\mathbb{R}^{d})^{q}\) and we have for any \(y\in(\mathbb{R}^{d})^{q}\) and any \(\mathsf{A}\in\mathcal{B}((\mathbb{R}^{d})^{\bar{q}})\), \(\mu_{|I}(\mathsf{A}|y)=\int_{\mathsf{X}}\mu(x)/\mu_{I}(y)\prod_{j=1}^{q} \delta(x_{i_{j}}-y_{j})\prod_{j^{\prime}=1}^{\bar{q}}\delta_{x_{i^{\mathsf{c }}_{j^{\prime}}}}(\mathsf{A}_{j^{\prime}})\mathrm{d}x\). Remark that for any \(y\in(\mathbb{R}^{d})^{q}\), \(\mu_{|I}(\cdot|y)\ll\operatorname{Leb}\). For any subset \(\mathsf{J}\subset\mathsf{I}^{\mathsf{c}}\) with \(\operatorname{card}(\mathsf{J})=q_{\mathsf{J}}\), we also define \(\mu_{|\mathsf{J}|}(\cdot|\cdot)\in\mathscr{P}^{(q_{\mathsf{J}})}\times( \mathbb{R}^{d})^{q}\) such that for any \(y\in(\mathbb{R}^{d})^{q}\), \(\mu_{|\mathsf{J}|}(\cdot|y)=\{\mu_{|\mathsf{J}|}(\cdot|y)\}_{\mathsf{J}}\). For a collection of functions \(\{f_{i}\}_{i\in\mathsf{I}}\), with \(\mathsf{I}\subset\{1,\ldots,n\}\) and \(n\in\mathbb{N}\) such that \(f_{i}:\ \mathbb{R}^{d}\to\mathbb{R}\), we define \(\oplus_{i\in\mathsf{I}}f_{i}:(\mathbb{R}^{d})^{n}\to\mathbb{R}\) such that for any \(x=(x_{1},\ldots,x_{n})\in(\mathbb{R}^{d})^{n}\), \(\oplus_{i\in\mathsf{I}}f(x)=\sum_{i\in\mathsf{I}}f_{i}(x_{i})\).

## Appendix B Introduction to trees

**Undirected tree.** An undirected graph \(\mathsf{T}=(\mathsf{V},\mathsf{E})\), with vertices \(\mathsf{V}\) and edges \(\mathsf{E}\), is said to be an _undirected tree_ if it is _acyclic_ and _connected_(Valiente, 2002, Definition 1.19.). In particular, we have \(\operatorname{card}(\mathsf{E})=\operatorname{card}(\mathsf{V})-1\). The undirected edge between two nodes \(v_{1}\) and \(v_{2}\) is similarly denoted by \(\{v_{1},v_{2}\}\) or \(\{v_{2},v_{1}\}\). We say that \(\mathsf{T}^{\prime}=(\mathsf{V}^{\prime},\mathsf{E}^{\prime})\) is a _sub-tree_ of \(\mathsf{T}\) if \(\mathsf{T}^{\prime}\) is an undirected tree with vertices \(\mathsf{V}^{\prime}\subset\mathsf{V}\) and edges \(\mathsf{E}^{\prime}\subset\mathsf{E}\). For any vertex \(v\in\mathsf{V}\), we define the set of its _neighbours_\(\mathsf{N}_{v}\) as the set of vertices \(v^{\prime}\in\mathsf{V}\) such that \(\{v,v^{\prime}\}\in\mathsf{E}\). The integer \(\operatorname{card}(\mathsf{N}_{v})\) is referred to as the degree of \(v\). The vertices with degree \(1\) are called _leaves_, and we denote the set of leaves by \(\mathsf{V}_{\mathsf{L}}\subset\mathsf{V}\). The (unique) _path_ in \(\mathsf{T}\) between two vertices \(v\) and \(v^{\prime}\) is the sequence of two-by-two distinct edges \(\{\{v_{i},v_{i+1}\}\}_{i=1}^{n}\) (with \(n\geq 1\)) such that \(v_{k}=v_{k+1}\) for any \(k\in\{1,\ldots,n\}\) such that \(k=0\operatorname{mod}(2)\), \(v_{1}=v\) and \(v_{n+1}=v^{\prime}\). This path can be seen as a linear sub-tree of \(\mathsf{T}\), and we define \(n\) as the _length_ of this path. We say that \(\mathsf{T}\) is _weighted_ if there exists a map \(w:\mathsf{E}\mapsto\mathbb{R}_{+}\); in this case, \(w(\{v_{1},v_{2}\})\), or equivalently \(w(\{v_{2},v_{1}\})\) (also denoted by \(w_{v_{1},v_{2}}\) or \(w_{v_{2},v_{1}}\) ) is called the weight of the edge \(\{v_{1},v_{2}\}\). The tree \(\mathsf{T}\) is said to be _rooted_ in \(r\in\mathsf{V}\) if \(r\) defines a partial ordering \(\leq_{\mathsf{T},r}\subset\mathsf{V}\times\mathsf{V}\) such that for any \(v_{1},v_{2}\in\mathsf{V}\), \(v_{1}\leq_{\mathsf{T},r}v_{2}\) if the node \(v_{1}\) lies on the unique path between \(r\) and \(v_{2}\).

**Directed tree.** Consider a directed graph \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\) rooted in \(r\in\mathsf{V}\). Any directed edge \(e\in\mathsf{E}_{r}\) from \(v_{1}\in\mathsf{V}\) to \(v_{2}\in\mathsf{V}\) is denoted by \((v_{1},v_{2})\). \(\mathsf{T}_{r}\) is a said to be a _directed tree_ rooted in \(r\) if (i) the underlying undirected graph \(\mathsf{T}=(\mathsf{V},\mathsf{E})\) is an undirected tree rooted in \(r\) and (ii) any \((v_{1},v_{2})\in\mathsf{E}_{r}\) is directed according to the partial ordering \(\leq_{\mathsf{T},r}\), _i.e._, \(\{v_{1},v_{2}\}\in\mathsf{E}\) and \(v_{1}\leq_{\mathsf{T},r}v_{2}\). For any vertices \((v,v^{\prime})\in\mathsf{V}\times\mathsf{V}\) such that \(v\leq_{\mathsf{T},r}v^{\prime}\), the (unique) _path_ in \(\mathsf{T}_{r}\) from \(v\) to \(v^{\prime}\), denoted by \(\operatorname{path}_{\mathsf{T}_{r}}(v,v^{\prime})\), is defined as the directed version of the path in \(\mathsf{T}\) between \(v\) and \(v^{\prime}\) (viewed as a sub-tree of \(\mathsf{T}\)), which is rooted in \(v\). We say that \(\mathsf{T}_{r}\) is _weighted_, if \(\mathsf{T}\) is weighted and the edges of \(\mathsf{T}_{r}\) have the same weights as the corresponding undirected edges of \(\mathsf{T}\). For any \((v_{1},v_{2})\in\mathsf{E}_{r}\), we denote this weight by \(w_{v_{1},v_{2}}\). We say that \(\mathsf{T}_{r}\) is the (unique) _directed version_ of \(\mathsf{T}\) rooted in \(r\). It is endowed with a canonical vertex numbering \(\zeta:\mathsf{V}\to\{0,\ldots,\operatorname{card}(\mathsf{V})-1\}\), corresponding to a depth-first traversal of its nodes, starting from the root \(r\) (Valiente, 2002, Definition 3.1.). This numbering is consistent with the partial ordering on \(\mathsf{T}

For any vertices \((v_{1},v_{2})\in\mathsf{E}\times\mathsf{E}\) such that \(v_{1}\leq_{\mathsf{T},r}v_{2}\), \(\operatorname{path}_{\mathsf{T}_{r}}(v_{1},v_{2})\) corresponds to the ordered set of edges in \(\mathsf{E}_{r}\) which define the ordered path between two vertices \(v_{1}\) and \(v_{2}\). For any vertex \(v\in\mathsf{V}\), we define:

1. the set of its _children_\(\mathsf{C}_{v}\) as the set of vertices \(v^{\prime}\in\mathsf{V}\) such that \((v,v^{\prime})\in\mathsf{E}_{r}\). In particular, for any \(v\in\mathsf{V}_{L}\), the set of leaves, one has \(\mathsf{C}_{v}=\emptyset\).
2. its _parent_ as the unique vertex \(p(v)\) such that \((p(v),v)\in\mathsf{E}_{r}\), if \(v\neq r\) (the parent of the root is not defined).

Note that \(\mathsf{N}_{r}=\mathsf{C}_{r}\) and, for any vertex \(v\in\mathsf{V}\backslash\{r\}\), \(\mathsf{N}_{v}=\{p(v)\}\cup\mathsf{C}_{v}\).

**Definition 8** (Tree-structured directed Probabilistic Graphical Model (PGM)).: _Consider a directed tree \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\). The directed PGM induced by \(\mathsf{T}_{r}\)(Koller & Friedman, 2009, Definition 3.4.), denoted by \(\mathscr{P}_{\mathsf{T}_{r}}\), is the family of distributions \(\pi\in\mathscr{P}^{(|\mathsf{V}|)}\) which have a Markovian factorization along \(\mathsf{T}_{r}\), i.e.,_

\[\mathscr{P}_{\mathsf{T}_{r}}=\{\pi\in\mathscr{P}^{(|\mathsf{V}|)}:\pi=\pi_{r} \bigotimes_{(v,v^{\prime})\in\mathsf{E}_{r}}\pi_{v^{\prime}|v}\}\.\]

**Lemma 9**.: _Consider an undirected tree \(\mathsf{T}=(\mathsf{V},\mathsf{E})\). Let \((r,r^{\prime})\in\mathsf{V}\times\mathsf{V}\). Let \(\mathsf{T}^{\prime}\) be a sub-tree of \(\mathsf{T}\) with vertices \(\mathsf{V}^{\prime}\) such that \(r^{\prime}\in\mathsf{V}^{\prime}\). Denote by \(\mathsf{T}^{\prime}_{r}\), the directed version of \(\mathsf{T}^{\prime}\) rooted in \(r^{\prime}\). Then, for any \(\pi\in\mathscr{P}_{\mathsf{T}_{r}}\), we have \(\pi_{\mathsf{V}^{\prime}}\in\mathscr{P}_{\mathsf{T}^{\prime}_{r^{\prime}}}\)._

Proof.: Let \((r,r^{\prime})\in\mathsf{V}\times\mathsf{V}\). We denote by \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\), respectively \(\mathsf{T}_{r^{\prime}}=(\mathsf{V},\mathsf{E}_{r^{\prime}})\), the directed version of \(\mathsf{T}\) rooted in \(r\), respectively \(r^{\prime}\). We define the paths \(\mathsf{P}_{r,r^{\prime}}=\operatorname{path}_{\mathsf{T}_{r}}(r,r^{\prime}) \subset\mathsf{E}_{r}\) and \(\mathsf{P}_{r^{\prime},r}=\operatorname{path}_{\mathsf{T}_{r^{\prime}}}(r^{ \prime},r)\subset\mathsf{E}_{r^{\prime}}\). It is easy to see that

1. \(\mathsf{E}_{r}\backslash\mathsf{P}_{r,r^{\prime}}=\mathsf{E}_{r^{\prime}} \backslash\mathsf{P}_{r^{\prime},r}\),
2. \(\mathsf{P}_{r,r^{\prime}}=\{(v_{2},v_{1}):(v_{1},v_{2})\in\mathsf{P}_{r^{\prime},r}\}\),
3. \(\mathsf{P}_{r^{\prime},r}=\{(v_{2},v_{1}):(v_{1},v_{2})\in\mathsf{P}_{r,r^{ \prime}}\}\).

Let \(\pi\in\mathscr{P}_{\mathsf{T}_{r}}\). First note that for any \((v_{1},v_{2})\in\mathsf{E}_{r}\), we have by Bayes decomposition \(\pi_{v_{1}}\pi_{v_{2}|v_{1}}=\pi_{v_{2}}\pi_{v_{1}|v_{2}}=\pi_{v_{1},v_{2}}\). Then it comes

\[\pi =\pi_{r}\bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r}}\pi_{v_{2}|v_{1}}\] \[=\pi_{r}\bigotimes_{(v_{1},v_{2})\in\mathsf{P}_{r,r^{\prime}}} \pi_{v_{2}|v_{1}}\bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r}\backslash\mathsf{P}_ {r,r^{\prime}}}\pi_{v_{2}|v_{1}}\] \[=\pi_{r}\bigotimes_{(v_{2},v_{1})\in\mathsf{P}_{r^{\prime},r}}\pi _{v_{2}|v_{1}}\bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r^{\prime}}\backslash \mathsf{P}_{r^{\prime},r}}\pi_{v_{2}|v_{1}}\] \[=\pi_{r^{\prime}}\bigotimes_{(v_{1},v_{2})\in\mathsf{P}_{r^{\prime},r}}\pi_{v_{2}|v_{1}}\bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r^{\prime}}\backslash \mathsf{P}_{r^{\prime},r}}\pi_{v_{2}|v_{1}}\] \[=\pi_{r^{\prime}}\bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r^{\prime} }}\pi_{v_{2}|v_{1}}\,\]

and therefore, we have \(\pi\in\mathscr{P}_{\mathsf{T}_{r^{\prime}}}\).

Let \(\mathsf{T}^{\prime}\) be a sub-tree of \(\mathsf{T}\) with vertices \(\mathsf{V}^{\prime}\) such that \(r^{\prime}\in\mathsf{V}^{\prime}\). First note that \(\mathsf{E}^{\prime}_{r^{\prime}}\subset\mathsf{E}_{r^{\prime}}\). Using the previous computation, we have for any \(\mathsf{A}\in\mathcal{B}((\mathbb{R}^{d})^{|\mathsf{V}^{\prime}|})\),

\[\pi_{\mathsf{V}^{\prime}}(\mathsf{A}) =\int_{(\mathbb{R}^{d})^{|\mathsf{V}|}}\pi_{r^{\prime}}(x_{r^{ \prime}})\bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r^{\prime}}}\pi_{v_{2}|v_{1}}(x _{v_{2}}|x_{v_{1}})\prod_{v^{\prime}\in\mathsf{V}^{\prime}}\delta_{x_{v^{\prime}}}( \mathsf{A}_{v^{\prime}})\mathrm{d}x\] \[=\int_{(\mathbb{R}^{d})^{|\mathsf{V}|-|\mathsf{V}^{\prime}|}}\{ \pi_{r^{\prime}}(\mathsf{A}_{r^{\prime}})\bigotimes_{(v_{1},v_{2})\in\mathsf{E} ^{\prime}_{r^{\prime}}}\pi_{v_{2}|v_{1}}(\mathsf{A}_{v_{2}}|x_{v_{1}})\} \bigotimes_{(v_{1},v_{2})\in\mathsf{E}_{r^{\prime}}\backslash\mathsf{E}_{r^{\prime} }^{\prime}}\pi_{v_{2}|v_{1}}(x_{v_{2}}|x_{v_{1}})\mathrm{d}x_{\mathsf{V} \backslash\mathsf{V}^{\prime}}\] \[=\{\pi_{r^{\prime}}\bigotimes_{(v_{1},v_{2})\in\mathsf{E}^{ \prime}_{r^{\prime}}}\pi_{v_{2}|v_{1}}(\mathsf{A})\,\]

which proves that \(\pi_{\mathsf{V}^{\prime}}\in\mathscr{P}_{\mathsf{T}^{\prime}_{r^{\prime}}}\). 

Discretized undirected tree.Let \(N\geq 1\). Consider an undirected tree \(\mathsf{T}=(\mathsf{V},\mathsf{E})\) with weights \(w\). We say that \(\mathsf{T}^{(N)}=(\mathsf{V}^{(N)},\mathsf{E}^{(N)})\) is a \(N\)-discretized version of \(\mathsf{T}\) if it is an undirected tree with weights \(w^{(N)}\) such that

1. \(\mathsf{V}^{(N)}=\mathsf{V}\bigsqcup_{\begin{subarray}{c}e\in\mathsf{E}_{r} \\ k\in\{1,\ldots,N-1\}\end{subarray}}\{v_{e}^{k}\}\),* \(\mathsf{E}^{(N)}=\cup_{e\in\mathsf{E}}\cup_{k=0,\ldots,N-1}\left\{\{v_{e}^{k},v_{e} ^{k+1}\}\right\}\) with the convention that the vertices \(v_{e}^{N}\) and \(v_{e}^{N}\) are defined such that \(\{v_{e}^{0},v_{e}^{N}\}=e\),
* \(\sum_{e\in\mathrm{path}_{\mathrm{T}}(v,v^{\prime})}1/w_{e}^{(N)}=1/w_{v,v^{ \prime}}\), if \(\{v,v^{\prime}\}\in\mathsf{E}\).

Remark that the leaves of \(\mathsf{T}^{(N)}\) are exactly the original leaves of \(\mathsf{T}\) and that \(\mathsf{T}^{(1)}=\mathsf{T}\). The non-uniqueness of \(\mathsf{T}^{(N)}\) comes from the freedom of choice on the weights of its edges.

Discretized directed tree.Let \(N\geq 1\). Consider a directed tree \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\) rooted in \(r\in\mathsf{V}\) with weights \(w\). We say that \(\mathsf{T}_{r}^{(N)}=(\mathsf{V}^{(N)},\mathsf{E}_{r}^{(N)})\) is a \(N\)-discretized version of \(\mathsf{T}_{r}\) if it is the directed version of \(\mathsf{T}^{(N)}\) rooted in \(r\), where \(\mathsf{T}^{(N)}\) is a \(N\)-discretized version of the underlying undirected tree of \(\mathsf{T}_{r}\).

## Appendix C Background on martingale problems

In this section, we introduce the background on Stochastic Differential Equations (SDEs) and weak solutions of SDEs following the framework of (Stroock and Varadhan, 1997, Section 10.1, page 249). We recall that \(\mathrm{C}_{0}^{\infty}(\mathbb{R}^{d})\) is the space of infinitely differentiable real-valued functions which vanish at infinity. In addition, we have that \(\mathcal{S}_{+}^{d}\) is the space of \(d\times d\), symmetric, non-negative matrices.

**Definition 10**.: _Let \(T>0\) or \(T=+\infty\), \(\sigma:\;[0,T)\times\mathbb{R}^{d}\to\mathcal{S}_{+}^{d}\) and \(b:\;[0,T)\times\mathbb{R}^{d}\to\mathbb{R}^{d}\), locally bounded measurable functions. We define the infinitesimal generator, \(\mathcal{A}\), given for any \(f\in\mathrm{C}_{0}^{\infty}(\mathbb{R}^{d})\), \(t\in[0,T)\) and \(x\in\mathbb{R}^{d}\) by_

\[\mathcal{A}_{t}(f)(x)=\langle b_{t}(x),\nabla f(x)\rangle+\tfrac{1}{2} \langle\sigma_{t}(x)\sigma_{t}(x)^{\top},\nabla^{2}f(x)\rangle.\] (4)

_We say that a probability measure \(\mathbb{P}\) satisfies the martingale problem for \(\mathcal{A}\) if for any \(t\in[0,T)\) and \(f\in\mathrm{C}_{0}^{\infty}(\mathbb{R}^{d})\), we have that \((f(\mathbf{X}_{t})-\int_{0}^{t}\mathcal{A}_{s}(f)(\mathbf{X}_{s})\mathrm{d}s) _{s\in[0,t]}\) is a \(\mathbb{P}\)-martingale._

In the main document, see Section 2, we say that "a path measure \(\mathbb{P}\) is associated with \(\mathrm{d}\mathbf{X}_{t}=b(t,\mathbf{X}_{t})\mathrm{d}t+\sigma(t,\mathbf{X}_{ t})\mathrm{d}\mathbf{B}_{t}\) with \((\mathbf{B}_{t})_{t\geq 0}\) a \(d\)-dimensional Brownian motion" if \(\mathbb{P}\) solves the martingale problem associated with \(\mathcal{A}\) given by (4). Unless specified, we always assume that such a path measure exists and is unique. Below, we recall the following theorem, see (Stroock and Varadhan, 1997, Theorem 10.2.2), which gives sufficient conditions for the existence and uniqueness of solutions to the martingale problem.

**Theorem 11**.: _Assume that for any \(x\in\mathbb{R}^{d}\) we have_

\[\inf\{\langle\theta,\sigma\sigma^{\top}(s,x)\theta\rangle\,:\; \theta\in\mathbb{R}^{d},\;\|\theta\|=1,\;s\in[0,T]\}>0,\] \[\lim_{y\to x}\sup\{\|\sigma(s,x)-\sigma(s,y)\|\,:\;s\in[0,T]\}=0.\]

_In addition, assume that there exists \(C>0\) such that for any \(x\in\mathbb{R}^{d}\)_

\[\sup\{\|\sigma\sigma^{\top}(t,x)\|\,:\;s\in[0,T]\}+\sup\{\langle x,b(t,x) \rangle\,:\;s\in[0,T]\}\leq C(1+\|x\|^{2}).\]

_Then, there exists a unique solution to the martingale problem with initialization \(x_{0}\in\mathbb{R}^{d}\)._

## Appendix D Theoretical results on Tree Schrodinger Bridges

We respectively provide in Appendix D.1, Appendix D.2 and Appendix D.3 the proofs of the results of the main manuscript presented in Section 3, Section 4 and Section 5. Finally, we make a detailed comparison between our setting and the framework of Haasler et al. (2021) in Appendix D.4. In the rest of this section, we consider an undirected tree \(\mathsf{T}=(\mathsf{V},\mathsf{E})\), where \(|\mathsf{V}|=\ell+1\), and some subset \(\mathsf{S}\subset\mathsf{V}\) which we denote by \(\mathsf{S}=\{i_{0},\ldots,i_{K-1}\}\). We define \(\mathsf{S}^{c}=\mathsf{V}\backslash\mathsf{S}\).

### Proofs of Section 3

Proposition 1 is straightforward to obtain by combining the definition of the Brownian motion with the definition of \(\pi^{0}\) given in (2). The following lemma details the recursion relation between the (mIPF) iterates, which is key to prove Proposition 2.

[MISSING_PAGE_FAIL:18]

\((c(i_{k_{n}+1}),i_{k_{n}+1})\in\mathsf{E}_{k_{n}}\). Then, \(\mathbb{P}^{n+1}_{(i_{k_{n}+1},c(i_{k_{n}+1}))}\) is a well defined path measure on \([0,T]\). By definition of the (mIPF) sequence, we have \(\mu_{i_{k_{n}+1}}=\pi^{n+1}_{i_{k_{n}+1}}\). By recursion assumption, we also have that \(\operatorname{Ext}(\mathbb{P}^{n}_{(c(i_{k_{n}+1}),i_{k_{n}+1})})=\pi^{n}_{c(i _{k_{n}+1}),i_{k_{n}+1}}\). Hence, it comes that \((\mathbb{P}^{n}_{(c(i_{k_{n}+1}),i_{k_{n}+1})})^{R}_{T|0}=\pi^{n}_{c(i_{k_{n}+1 })|i_{k_{n}+1}}=\pi^{n+1}_{c(i_{k_{n}+1})|i_{k_{n}+1}}\), where the last equality comes from Lemma 12. Finally, we obtain that \(\operatorname{Ext}(\mathbb{P}^{n+1}_{(i_{k_{n}+1},c(i_{k_{n}+1}))})=\pi^{n+1}_ {i_{k_{n}+1},c(i_{k_{n}+1})}\), which proves the initialisation.

Assume now that \(\mathbb{P}^{n+1}\) is well defined and has the right properties, up to some edge in \(\mathsf{T}_{k_{n}+1}\). Consider the following edge, denoted by \((v,v^{\prime})\in\mathsf{E}_{k_{n}+1}\), in the breadth-first order. By edge recursion, we have that \(\operatorname{Ext}(\mathbb{P}^{n+1}_{(p(v),v)})=\pi^{n+1}_{p(v),v}\), and thus \(\mathbb{P}^{n+1}_{(p(v),v),T_{p(v),v}}=\pi^{n+1}_{v}\). Define the path \(\mathsf{P}_{n}=\operatorname{path}_{\mathsf{T}_{k_{n}}}(i_{k_{n}},i_{k_{n}+1})\). Then, we face two cases.

(i) Either \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\backslash\mathsf{P}_{n}\). Then, we have by (a) that

\[\mathbb{P}^{n+1}_{(v,v^{\prime})}=\mathbb{P}^{n+1}_{(p(v),v),T_{p(v),v}} \otimes\mathbb{P}^{n}_{(v,v^{\prime})|0}=\pi^{n+1}_{v}\otimes\mathbb{P}^{n}_{( v,v^{\prime})|0}\]

In particular, \(\mathbb{P}^{n+1}_{(v,v^{\prime})}\) is a well defined path measure on \([0,T_{v,v^{\prime}}]\). Since \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\), \(\operatorname{Ext}(\mathbb{P}^{n}_{(v,v^{\prime})})=\pi^{n}_{v,v^{\prime}}\) by recursion assumption. In particular, \(\mathbb{P}^{n}_{(v,v^{\prime}),T_{v,v^{\prime}}|0}=\pi^{n}_{v^{\prime}|v}=\pi^ {n+1}_{v^{\prime}|v}\) where the last equality comes from Lemma 12. We thus have \(\operatorname{Ext}(\mathbb{P}^{n+1}_{(v,v^{\prime})})=\pi^{n+1}_{v,v^{\prime}}\).

(ii) Or \((v^{\prime},v)\in\mathsf{P}_{n}\). Then, we have by (b) that

\[\mathbb{P}^{n+1}_{(v,v^{\prime})}=\mathbb{P}^{n+1}_{(p(v),v),T_{v,v^{\prime}}} \otimes(\mathbb{P}^{n}_{(v^{\prime},v)})^{R}_{0}=\pi^{n+1}_{v}\otimes(\mathbb{ P}^{n}_{(v^{\prime},v)})^{R}_{0}\]

In particular, \(\mathbb{P}^{n+1}_{(v,v^{\prime})}\) is a well defined path measure on \([0,T_{v,v^{\prime}}]\). Here, \((v^{\prime},v)\in\mathsf{E}_{k_{n}}\) and thus, \(\operatorname{Ext}(\mathbb{P}^{n}_{(v^{\prime},v)})=\pi^{n}_{v^{\prime},v}\) by recursion assumption. In particular, \((\mathbb{P}^{n}_{(v^{\prime},v)})^{R}_{T_{v^{\prime},v}|0}=\pi^{n}_{v^{\prime}|v }=\pi^{n+1}_{v^{\prime}|v}\) where the last equality comes from Lemma 12. We thus have \(\operatorname{Ext}(\mathbb{P}^{n+1}_{(v,v^{\prime})})=\pi^{n+1}_{v,v^{\prime}}\).

This achieves the recursion. 

### Proofs of Section 4

Remark on assumption A1.Although _A1 is not needed to establish the result of Proposition 3, Corollary 4 and Proposition 5_, it is however crucial in the proof of convergence of (mIPF) stated in Proposition 6. Nevertheless, we choose to keep A1 as an assumption in the statement of every theoretical result presented in Section 4 for sake of clarity.

Additional definitions.We define the set \(\mathscr{P}_{\mathsf{S}}=\cap_{i\in\mathsf{S}}\mathscr{P}_{i}\), where \(\mathscr{P}_{i}=\{\pi\in\mathscr{P}^{(\ell+1)}:\ \pi_{i}=\mu_{i}\}\), _i.e._, \(\mathscr{P}_{\mathsf{S}}\) is the set of all probability measures \(\pi\in\mathscr{P}^{(\ell+1)}\) which verify

\[\int_{(\mathbb{R}^{d})^{\ell+1}}f_{i}(x_{i})\mathrm{d}\pi(x_{0:\ell})=\int_{ \mathbb{R}^{d}}f_{i}(x_{i})\mathrm{d}\mu_{i}(x_{i})\,\]

for any family of bounded measurable functions \(\{f_{i}\}_{i\in\mathsf{S}}\in\mathrm{C}(\mathbb{R}^{d},\mathbb{R})^{K}\). Since \(\mathbb{R}^{d}\) is separable, there exists a dense family of functions \(\{f^{k}_{i}\}_{k\in\mathbb{N}^{*},i\in\mathsf{S}}\), with \(f^{k}_{i}\in\mathrm{L}^{\infty}(\mu_{i})\) for any \(k\in\mathbb{N}^{*}\) and any \(i\in\mathsf{S}\), such that \(\pi\in\mathscr{P}_{\mathsf{S}}\) if and only if

\[\int_{(\mathbb{R}^{d})^{\ell+1}}f^{k}_{i}(x_{i})\mathrm{d}\pi(x_{0:\ell})=\int_ {\mathbb{R}^{d}}f^{k}_{i}(x_{i})\mathrm{d}\mu_{i}(x_{i})\]

or equivalently, upon centering \(f^{k}_{i}\),

\[\int_{(\mathbb{R}^{d})^{\ell+1}}f^{k}_{i}(x_{i})\mathrm{d}\pi(x_{0:\ell})=0\.\]

In the rest of the section, we consider such family \(\{f^{k}_{i}\}_{k\in\mathbb{N}^{*},i\in\mathsf{S}}\).

For any \(n\in\mathbb{N}^{*}\), we also define \(\mathscr{P}^{n}_{\mathsf{S}}=\cap_{i\in\mathsf{S}}\mathscr{P}^{n}_{i}\), where \(\mathscr{P}^{n}_{i}=\{\pi\in\mathscr{P}^{(\ell+1)}:\int_{(\mathbb{R}^{d})^{\ell+1 }}f^{k}_{i}(x_{i})\mathrm{d}\pi(x_{0:\ell})=0,\ \forall k\in\{1,\ldots,n\}\}\). In particular, we have

\[\mathscr{P}_{\mathsf{S}}=\cap_{n\in\mathbb{N}^{*}}\mathscr{P}^{n}_{\mathsf{S}}\.\] (6)

Finally, (static-mSB) can be rewritten as

\[\pi^{\star}=\operatorname{argmin}\{\operatorname{KL}(\pi\mid\pi^{0}):\pi \in\mathscr{P}_{\mathsf{S}}\}\.\] (7)Proof of Proposition 3 and Corollary 4.In this part of the section, we present an extension of the theoretical results from Nutz (2021) to the multi-marginal setting. We first present two technical results, Lemma 13 and Lemma 14, which are respectively adapted from (Nutz, 2021, Lemma 2.10.) and (Nutz, 2021, Lemma 2.11.).

**Lemma 13**.: _Let \(\{\tilde{\mu}_{j}\}_{j\in\mathsf{S}^{c}}\) be a family of probability measures defined on \((\mathbb{R}^{d},\mathcal{B}(\mathbb{R}^{d}))\). We define \(\tilde{\pi}^{0}=\bigotimes_{i\in\mathsf{S}}\mu_{i}\,\bigotimes_{j\in\mathsf{S }^{c}}\tilde{\mu}_{j}\). Let \(\mathsf{A}\in\bigotimes_{m=0}^{\ell}\mathcal{B}(\mathbb{R}^{d})\) such that \(\tilde{\pi}^{0}(\mathsf{A})=1\). Then, for \(\tilde{\pi}^{0}\)-almost any \(x^{\star}\in\mathsf{A}\), there exists a family of sets \(\{\mathcal{X}^{0}_{m}\}_{m=0}^{\ell}\subset(\mathbb{R}^{d})^{\ell+1}\) such that_

1. \(\mu_{i}(\mathsf{X}^{0}_{i})=1\) _for any_ \(i\in\mathsf{S}\)_, and_ \(\tilde{\mu}_{j}(\mathsf{X}^{0}_{j})=1\) _for any_ \(j\in\mathsf{S}^{c}\)_,_
2. \(\mathsf{A}^{0}=\mathsf{A}\cap(\prod_{m=0}^{\ell}\mathsf{X}^{0}_{m})\) _satisfies_ \(x^{\star}\in\mathsf{A}^{0}\) _and_ \[(x^{\star}_{0},\ldots,x^{\star}_{m-1},x_{m},x^{\star}_{m+1},\ldots,x^{\star}_ {\ell})\in\mathsf{A}^{0},\forall x\in\mathsf{A}^{0},\forall m\in\{0,\ldots, \ell\}\.\]

Proof.: Consider such set \(\mathsf{A}\). We define for any \(m\in\{0,\ldots,\ell\}\) the set

\[\mathsf{X}_{m}=\{u\in\mathbb{R}^{d}:\tilde{\pi}^{0}_{-m}(\mathsf{A}^{u}_{m})=1 \}\,\]

where \(\mathsf{A}^{u}_{m}=\{y\in(\mathbb{R}^{d})^{\ell}:(y_{0},\ldots,y_{m-1},u,y_{m },\ldots,y_{\ell-1})\in\mathsf{A}\}\).

Take \(i\in\mathsf{S}\). Assume that \(\mu_{i}(\mathsf{X}_{i})<1\). We recall that \(\tilde{\pi}^{0}=\tilde{\pi}^{0}_{-i}\otimes\mu_{i}\). Using Fubini's theorem and that \(\int_{\mathsf{A}^{s_{i}}_{i}}\mathrm{d}\tilde{\pi}^{0}_{-i}(x_{-i})<1\) for any \(x_{i}\not\in\mathsf{X}_{i}\), we have

\[1=\tilde{\pi}^{0}(\mathsf{A}) =\int_{\mathsf{A}}\mathrm{d}\tilde{\pi}^{0}_{-i}(x_{-i})\otimes \mathrm{d}\mu_{i}(x_{i})\] \[=\int_{\mathbb{R}^{d}\big{\{}\mathsf{A}^{s_{i}}_{i}\mathrm{d} \tilde{\pi}^{0}_{-i}(x_{-i})\}\mathrm{d}\mu_{i}(x_{i})\] \[=\int_{\mathsf{X}_{i}}\{\int_{\mathsf{A}^{s_{i}}_{i}\mathrm{d} \tilde{\pi}^{0}_{-i}(x_{-i})\}\mathrm{d}\mu_{i}(x_{i})+\int_{\mathsf{X}^{c}_{i} }\{\int_{\mathsf{A}^{s_{i}}_{i}}\mathrm{d}\tilde{\pi}^{0}_{-i}(x_{-i})\} \mathrm{d}\mu_{i}(x_{i})\] \[<\mu_{i}(\mathsf{X}_{i})+\mu_{i}(\mathsf{X}^{c}_{i})=1\,\]

which is absurd. Therefore, we obtain \(\mu_{i}(\mathsf{X}_{i})=1\), and similarly, we have \(\tilde{\mu}_{j}(\mathsf{X}_{j})=1\) for any \(j\in\mathsf{S}^{c}\). For any \(y\in(\mathbb{R}^{d})^{\ell}\), any \(m\in\{0,\ldots,\ell\}\), we define the set

\[\bar{\mathsf{A}}^{y}_{m}=\{u\in\mathbb{R}^{d}:(y_{0},\ldots,y_{m-1},u,y_{m}, \ldots,y_{\ell-1})\in\mathsf{A}\}\.\]

Let \(i\in\mathsf{S}\). We have by Fubini's theorem

\[1=\tilde{\pi}^{0}(\mathsf{A}) =\int_{\mathsf{A}}\mathrm{d}\mu_{i}(x_{i})\otimes\mathrm{d}\tilde {\pi}^{0}_{-i}(x_{-i})\] \[=\int_{(\mathbb{R}^{d})^{\ell}\{\int_{\mathsf{A}^{s_{i}}_{i}} \mathrm{d}\mu_{i}(x_{i})\}\mathrm{d}\tilde{\pi}^{0}_{-i}(x_{-i})}\] \[=\int_{\prod_{m=0}^{\ell}\limits_{m\neq i}\mathsf{X}_{i}\{\int_{ \mathsf{A}^{s_{i}}_{i}}\mathrm{d}\mu_{i}(x_{i})\}\mathrm{d}\tilde{\pi}^{0}_{-i }(x_{-i})\,\]

where the last equality comes from the fact that \(\mu_{i}(\mathsf{X}_{i})=1\) for any \(i\in\mathsf{S}\), \(\tilde{\mu}_{j}(\mathsf{X}_{j})=1\) for any \(j\in\mathsf{S}^{c}\) and that \(\tilde{\pi}^{0}=\bigotimes_{i\in\mathsf{S}}\mu_{i}\bigotimes_{j\in\mathsf{S} ^{c}}\tilde{\mu}_{j}\). Consequently, there exists a measurable set \(\mathsf{A}_{-i}\subset\prod_{m=0}^{\ell}\mathsf{X}_{i}\) such that the following properties hold: (a) \(\mu_{i}(\tilde{\mathsf{A}}^{y}_{i})=1\) for any \(y\in\mathsf{A}_{-i}\), (b) \(\tilde{\pi}^{0}_{-i}(\mathsf{A}_{-i})=1\). Similarly, this result holds for any \(j\in\mathsf{S}^{c}\), _i.e._, there exists a measurable set \(\mathsf{A}_{-j}\subset\prod_{m\neq j}^{\ell}\mathsf{X}_{i}\) such that the following properties hold: (a) \(\tilde{\mu}_{j}(\tilde{\mathsf{A}}^{y}_{j})=1\) for any \(y\in\mathsf{A}_{-j}\), (b) \(\tilde{\pi}^{0}_{-j}(\mathsf{A}_{-j})=1\). We consider such sets \(\{\mathsf{A}_{-m}\}_{m=0}^{\ell}\) for the rest of the proof and finally define the set

\[\tilde{\mathsf{A}}=\cap_{m=0}^{\ell}\tilde{\mathsf{A}}_{m}\,\]

where \(\tilde{\mathsf{A}}_{m}=\mathsf{A}_{-m}\times\{u\in\tilde{\mathsf{A}}^{y}_{m}:y \in\mathsf{A}_{-m}\}\). By definition, we have \(\tilde{\mathsf{A}}\subset\mathsf{A}\cap\prod_{m=0}^{\ell}\mathsf{X}_{m}\), using the fact that \(\tilde{\mathsf{A}}_{m}\subset\mathsf{A}\) for any \(m\in\{0,\ldots,\ell\}\). In addition, for any \(i\in\mathsf{S}\), we get by Fubini's theorem

\[\tilde{\pi}^{0}(\tilde{\mathsf{A}}_{i})=\int_{\tilde{\mathsf{A}}_{i}}\mathrm{d} \mu_{i}(x_{i})\otimes\mathrm{d}\tilde{\pi}^{0}_{-i}(x_{-i})=\int_{\mathsf{A}_{-i }}\{\int_{\mathsf{A}^{s_{i}-i}_{i}}\mathrm{d}\mu_{i}(x_{i})\}\mathrm{d}\tilde{ \pi}^{0}_{-i}(x_{-i})=\tilde{\pi}^{0}_{-i}(\mathsf{A}_{-i})=1\,\]

and similarly, we get \(\tilde{\pi}^{0}(\tilde{\mathsf{A}}_{j})=1\) for any \(j\in\mathsf{S}^{c}\). We can deduce that \(\tilde{\pi}^{0}(\tilde{\mathsf{A}})=1\) since \(\tilde{\pi}^{0}(\tilde{\mathsf{A}}^{c})\leq\sum_{m=0}^{\ell}\tilde{\pi}^{0}( \tilde{\mathsf{A}}^{c}_{m})=0\).

Let \(x^{\star}\in\tilde{\mathsf{A}}\). In particular, \(x^{\star}\in\mathsf{A}\). We define the set \(\mathsf{A}^{0}=\mathsf{A}\cap(\prod_{m=0}^{\ell}\mathsf{X}^{0}_{m})\), where \(\mathsf{X}^{0}_{m}=\mathsf{X}_{m}\cap\bar{\mathsf{A}}_{m}^{x^{\star}_{m}}\) for any \(m\in\{0,\ldots,\ell\}\). We now establish the result of Lemma 13.

We first prove (a). Let \(i\in\mathsf{S}\). Since \(x^{\star}\in\tilde{\mathsf{A}}\), we have \(x^{\star}\in\tilde{\mathsf{A}}_{i}\) and therefore \(x^{\star}_{-i}\in\mathsf{A}_{-i}\). By definition of \(\mathsf{A}_{-i}\), we obtain that \(\mu_{i}(\bar{\mathsf{A}}_{i}^{x^{\star}_{-i}})=1\) and thus,

\[\mu_{i}(\{\mathsf{X}^{0}_{i}\}^{\mathsf{c}})\leq\mu_{i}(\mathsf{X}^{\mathsf{ c}}_{i})+\mu_{i}(\{\bar{\mathsf{A}}_{i}^{x^{\star}_{-i}}\}^{\mathsf{c}})=0,\]

which gives \(\mu_{i}(\mathsf{X}^{0}_{i})=1\), and similarly, we have \(\tilde{\mu}_{j}(\mathsf{X}^{0}_{j})=1\) for any \(j\in\mathsf{S}^{\mathsf{c}}\).

We now prove (b). Let \(m\in\{0,\ldots,\ell\}\). Since \(x^{\star}\in\tilde{\mathsf{A}}\subset\mathsf{A}\), we get \(x^{\star}_{m}\in\bar{\mathsf{A}}_{m}^{x^{\star}_{m}}\). Using that \(\tilde{\mathsf{A}}\subset\mathsf{A}\cap_{m=0}^{\ell}\mathsf{X}_{m}\), we get \(x^{\star}\in\mathsf{A}^{0}\). Let \(x\in\mathsf{A}^{0}\). We denote \(x^{m}=(x^{\star}_{0},\ldots,x^{\star}_{m-1},x_{m},x^{\star}_{m+1},\ldots,x^{ \star}_{\ell})\). We need to show that \(x^{m}\in\mathsf{A}\) and \(x^{m}\in\prod_{j=1}^{\ell}\mathsf{X}^{0}_{j}=\prod_{j=1}^{\ell}(\mathsf{X}_{j }\cap\bar{\mathsf{A}}_{j}^{x^{\star}_{-m}})\). First, since \(x^{\star}_{j}=x_{j}\) or \(x^{\star}_{j}\) for any \(j\in\{0,\ldots,\ell\}\), and \(x\in\mathsf{A}^{0}\) and \(x^{\star}\in\mathsf{A}^{0}\), we get that for any \(j\in\{0,\ldots,\ell\}\), \(x^{m}_{j}\in\mathsf{X}_{j}\). Similarly, for any \(j\in\{0,\ldots,\ell-1\}\), \(x^{m}_{j}\in\bar{\mathsf{A}}_{j}^{x^{\star}_{m}}\). Therefore, we get that \(x^{m}\in\prod_{j=1}^{\ell}(\mathsf{X}_{j}\cap\bar{\mathsf{A}}_{j}^{x^{\star}_ {-m}})\). Since \(x_{m}\in\mathsf{A}_{m}^{x^{\star}_{-m}}\) (because \(x\in\prod_{j=1}^{\ell}(\mathsf{X}_{j}\cap\bar{\mathsf{A}}_{j}^{x^{\star}_{-m}})\)), we get that \(x\in\mathsf{A}\), which concludes the proof. 

**Lemma 14**.: _Let \(\mathsf{A}^{0}\subset(\mathbb{R}^{d})^{\ell+1}\). For any \(m\in\{0,\ldots,\ell\}\), we denote \(\mathsf{X}^{0}_{m}=\mathrm{proj}_{m}(\mathsf{A}^{0})\). We make the following assumptions._

_(a) Assume there exists_ \(x^{\star}\in\mathsf{A}^{0}\) _such that for any_ \(x\in\mathsf{A}^{0}\)_, for any_ \(m\in\{0,\ldots,\ell\}\)_, we have_ \((x^{\star}_{0},\ldots,x^{\star}_{m-1},x_{m},x^{\star}_{m+1},\ldots,x^{\star}_{ \ell})\in\mathsf{A}^{0}\)_._

_(b) Assume there exists a family of functions_ \(\{\varphi^{n}_{i_{k}}\}_{n\in\mathbb{N}^{\star},k\in\{0,\ldots,K-1\}}\) _with_ \(\varphi^{n}_{i_{k}}:\mathsf{X}^{0}_{i_{k}}\to[-\infty,+\infty)\) _such that for any_ \(n\in\mathbb{N}^{\star}\) _and any_ \(k\in\{0,\ldots,K-2\}\)_, we have_ \(\varphi^{n}_{i_{k}}(x^{\star}_{i_{k}})=0\)_._

_(c) Denote_ \(F^{n}(x)=\sum_{k=0}^{K-1}\varphi^{n}_{i_{k}}(x_{i_{k}})\) _for any_ \(x\in\mathsf{A}^{0}\)_. Assume that for any_ \(x\in\mathsf{A}^{0}\)_,_ \(F(x)=\lim_{n\to\infty}F^{n}(x)\) _exists and is such that_ \(F(x)\in[-\infty,+\infty)\) _with_ \(F(x^{\star})\in\mathbb{R}\)_._

_Then, for any \(i\in\mathsf{S}\), for any \(x_{i}\in\mathsf{X}^{0}_{i}\), \(\varphi_{i}(x_{i})=\lim_{n\to\infty}\varphi^{n}_{i}(x_{i})\) exists and is such that \(\varphi_{i}(x_{i})\in[-\infty,+\infty)\)._

Proof.: Consider \(\mathsf{A}^{0}\subset(\mathbb{R}^{d})^{\ell+1}\) such that assumptions (a), (b) and (c) hold. Remark that we have \(F^{n}(x^{\star})=\varphi^{n}_{i_{K-1}}(x^{\star}_{i_{K-1}})\).

Let \(x\in\mathsf{A}^{0}\). We denote \(x^{m}=(x^{\star}_{1},\ldots,x^{\star}_{m-1},x_{m},x^{\star}_{m+1},\ldots,x^{ \star}_{t})\) for any \(m\in\{0,\ldots,\ell\}\). In particular, we have \(x^{m}\in\mathsf{A}^{0}\) by assumption (a). Let us define

\[\varphi_{i_{k}}(x_{i_{k}}) =F(x^{i_{k}})-F(x^{\star}),\quad\forall k\in\{0,\ldots,K-2\}\,\] \[\varphi_{i_{K-1}}(x_{i_{K-1}}) =F(x^{i_{K-1}})\.\]

Using assumption (c), we have \(\varphi_{i}(x_{i})\in[-\infty,+\infty)\) for any \(i\in\mathsf{S}\). Let \(k\in\{0,\ldots,K-2\}\). We have by definition of \(F^{n}\),

\[\varphi^{n}_{i_{k}}(x_{i_{k}}) =F^{n}(x^{i_{k}})-\sum_{\begin{subarray}{c}m=0\\ m\neq k\end{subarray}}^{K-1}\varphi^{n}_{i_{m}}(x^{\star}_{i_{m}})=F^{n}(x^{i_{ k}})-F^{n}(x^{\star})\,\]

where we used assumption (b) in the last equality. Since \(x^{i_{k}}\in\mathsf{A}^{0}\) and \(x^{\star}\in\mathsf{A}^{0}\), we have by assumption (c),

\[\lim_{n\to\infty}\varphi^{n}_{i_{k}}(x_{i_{k}}) =F(x^{i_{k}})-F(x^{\star})=\varphi_{i_{k}}(x_{i_{k}})\.\]

Furthermore, by combining the definition of \(F^{n}\) with assumption (b), we have

\[\lim_{n\to\infty}\varphi^{n}_{i_{K-1}}(x_{i_{K-1}}) =F(x^{i_{K-1}})=\varphi_{i_{K-1}}(x_{i_{K-1}}),\]

which concludes the proof.

In what follows, before proving Proposition 3, we respectively show in Proposition 15 and Proposition 16 how **A2** and **A3** can be satisfied in the case where \(\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\), as in (2), that is

\[\pi^{0}=\pi^{0}_{r}\bigotimes_{(v,v^{\prime})\in\mathsf{E}_{r}}\pi^{0}_{v^{ \prime}|v}.\]

**Proposition 15**.: _Let \(\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\). Assume that \(\pi^{0}_{r}=\mu_{r}\) if \(r\in\mathsf{S}\) or \(\pi^{0}_{r}=\mathrm{N}(m_{r},\sigma_{r}\mathrm{Id})\), with \(m_{r}\in\mathbb{R}^{d}\) and \(\sigma_{r}>0\) if \(r\in\mathsf{S}^{c}\). In addition, assume that for any \((v,v^{\prime})\in\mathsf{E}_{r}\), \(\pi^{0}_{v^{\prime}|v}(\cdot|x_{v})=\mathrm{N}(x_{v},\sigma_{v,v^{\prime}} \mathrm{Id})\) with \(\sigma_{v,v^{\prime}}>0\). Finally, assume that for any \(i\in\mathsf{S}\), \(\int_{\mathbb{R}^{d}}\|x\|^{2}\mathrm{d}\mu_{i}(x)<+\infty\) and \(\mathrm{H}(\mu_{i})<+\infty\). Then \(\mathbf{A2}\) is satisfied._

Proof.: Let \(\pi=\otimes_{i\in\mathsf{S}}\mu_{i}\otimes_{i\in\mathsf{S}^{c}}\nu_{i}\) with \(\nu_{i}\) any Gaussian measure with positive definite covariance matrix. First, we have that

\[\mathrm{KL}(\pi\mid\pi^{0})=\mathrm{KL}(\pi_{r}\mid\pi^{0}_{r})+\sum_{(v,v^{ \prime})\in\mathsf{E}_{r}}\int_{\mathbb{R}^{d}}\mathrm{KL}(\pi_{v^{\prime}|v} |\pi^{0}_{v^{\prime}|v})\mathrm{d}\pi_{v}\.\]

For any \((v,v^{\prime})\in\mathsf{E}_{r}\), there exists \(C_{v,v^{\prime}}\geq 0\) such that

\[\int_{\mathbb{R}^{d}}\mathrm{KL}(\pi_{v^{\prime}|v}|\pi^{0}_{v^{ \prime}|v})\mathrm{d}\pi_{v}\leq C_{v,v^{\prime}}-\mathrm{H}(\pi_{v^{\prime}}) +\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\|x_{v}-x_{v^{\prime}}\|^{2}/(2 \sigma^{2}_{v,v^{\prime}})\mathrm{d}\pi_{v}\otimes\pi_{v^{\prime}}(x_{v},x_{v^ {\prime}})\] \[\qquad\leq C_{v,v^{\prime}}-\mathrm{H}(\pi_{v^{\prime}})+(1/ \sigma^{2}_{v,v^{\prime}})\int_{\mathbb{R}^{d}}\|x_{v}\|^{2}\mathrm{d}\pi_{v}( x_{v})+(1/\sigma^{2}_{v,v^{\prime}})\int_{\mathbb{R}^{d}}\|x_{v^{\prime}}\|^{2} \mathrm{d}\pi_{v^{\prime}}(x_{v^{\prime}})<+\infty.\]

We conclude the proof upon remarking that \(\mathrm{KL}(\pi_{r}\mid\pi^{0}_{r})<+\infty\). 

**Proposition 16**.: _Let \(\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\). Assume that \(\pi^{0}_{r}=\mu_{r}\) if \(r\in\mathsf{S}\) or \(\pi^{0}_{r}=\mathrm{N}(m_{r},\sigma_{r}\mathrm{Id})\), with \(m_{r}\in\mathbb{R}^{d}\) and \(\sigma_{r}>0\) if \(r\in\mathsf{S}^{c}\). In addition, assume that for any \((v,v^{\prime})\in\mathsf{E}_{r}\), \(\pi^{0}_{v^{\prime}|v}(\cdot|x_{v})=\mathrm{N}(x_{v},\sigma_{v,v^{\prime}} \mathrm{Id})\) with \(\sigma_{v,v^{\prime}}>0\). Finally, assume that for any \(i\in\mathsf{S}\), \(\mu_{i}\) admits a positive density w.r.t. the Lebesgue measure. Then \(\mathbf{A}\) is satisfied._

Proof.: We have that \(\pi^{0}\) admits a positive density w.r.t the Lebesgue measure. Letting \(\tilde{\pi}^{0}=\bigotimes_{i\in\mathsf{S}}\mu_{i}\bigotimes_{j\in\mathsf{S}^{ c}}\tilde{\mu}_{j}\) where \(\tilde{\mu}_{j}\) which admits a positive density w.r.t. the Lebesgue measure for any \(j\in\mathsf{S}^{c}\), we get that \(\tilde{\pi}^{0}\) admits a positive density w.r.t. the Lebesgue measure and therefore \(\pi^{0}\sim\tilde{\pi}^{0}\), which concludes the proof. 

Using the preliminary results presented above, we are now ready to prove Proposition 3.

Proof of Proposition 3.: Assume \(\mathbf{A}\)1 and \(\mathbf{A}\)2. Since \(\mathscr{P}_{\mathsf{S}}\) is convex and closed in total-variation norm, there exists a probability distribution \(\pi^{\star}\) solution to (7), or equivalently to (static-mSB), by using \(\mathbf{A}\)2 with (Csiszar, 1975, Theorem 2.1.). Moreover, this solution is unique by strict convexity of \(\mathrm{KL}(\cdot\mid\pi^{0})\).

We now turn to the proof of existence of potentials defining \((\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})\), by adapting the arguments of (Nutz, 2021, Section 2.3.). Define \(\nu^{n}=\mathrm{argmin}\{\mathrm{KL}(\pi\mid\pi^{0}):\pi\in\mathscr{P}^{n}_{ \mathsf{S}}\}\) for any \(n\in\mathbb{N}^{\star}\). Since \(\{\mathscr{P}^{n}_{\mathsf{S}}\}_{n\in\mathbb{N}^{\star}}\subset\mathscr{P}^{( \ell+1)}\) is a decreasing sequence of sets that are convex and closed in total-variation norm such that (6) holds, we get from (Nutz, 2021, Proposition 1.17.) with \(\mathbf{A}\)2 that

\[\lim_{n\to\infty}\|\nu^{n}-\pi^{\star}\|_{\mathrm{TV}}=0\,\]

or equivalently

\[\lim_{n\to\infty}\|(\mathrm{d}\nu^{n}/\mathrm{d}\pi^{0})-(\mathrm{d}\pi^{ \star}/\mathrm{d}\pi^{0})\|_{\mathrm{L}^{1}(\pi^{0})}=0\.\] (8)

Following (Nutz, 2021, Example 1.18), there exists a family of bounded measurable functions \(\{\varphi^{n}_{i}\}_{n\in\mathbb{N}^{\star},i\in\mathsf{S}}\) with \(\varphi^{n}_{i}:\mathbb{R}^{d}\to\mathbb{R}\) such that for any \(n\in\mathbb{N}^{\star}\)

\[(\mathrm{d}\nu^{n}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\varphi^{n}_ {i}]\.\] (9)

We consider such family \(\{\varphi^{n}_{i}\}_{n\in\mathbb{N}^{\star},i\in\mathsf{S}}\) for the rest of the proof. By combining (8) and (9), we obtain, up to extraction,

\[(\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})=\lim_{n\to\infty}\exp[\bigoplus_{i\in \mathsf{S}}\varphi^{n}_{i}]\quad\pi^{0}\text{-a.s.}\.\] (10)We now define the following sets

\[\mathsf{A}^{\star} =\{x\in(\mathbb{R}^{d})^{\ell+1}:\lim_{n\to\infty}\bigoplus_{i\in \mathsf{S}}\varphi_{i}^{n}(x_{i})\in[-\infty,+\infty)\}\;,\] \[\mathsf{B}^{\star} =\{x\in(\mathbb{R}^{d})^{\ell+1}:\lim_{n\to\infty}\bigoplus_{i\in \mathsf{S}}\varphi_{i}^{n}(x_{i})>-\infty\}\subset\mathsf{A}^{\star}\]

Using (10), we have \(\pi^{0}(\mathsf{A}^{\star})=1\). Using **A3**, it comes \(\tilde{\pi}^{0}(\mathsf{A}^{\star})=1\). Moreover, we also get that \(\pi^{\star}(\mathsf{B}^{\star})=1\) by (10). Thus, it comes \(\pi^{0}(\mathsf{B}^{\star})>0\), and \(\tilde{\pi}^{0}(\mathsf{B}^{\star})>0\) using **A3**.

We then apply Lemma 13 to \(\tilde{\pi}^{0}\) and \(\mathsf{A}=\mathsf{A}^{\star}\). Since \(\tilde{\pi}^{0}(\mathsf{B}^{\star})>0\), it implies that there exists \(x^{\star}\in\mathsf{B}^{\star}\) and a measurable set \(\mathsf{A}^{0}\subset\mathsf{B}^{\star}\) verifying the properties (a) and (b). Following (Nutz, 2021, Corollary 2.12), we may assume without loss of generality in the statement of Lemma 13 that the sets \(\mathsf{X}^{0}_{m}\) are measurable with \(\prod_{m=0}^{\ell}\mathsf{X}^{0}_{m}\subset\mathsf{A}\). In this case, we obtain that \(\mu_{i}(\operatorname{proj}_{i}(\mathsf{A}^{0}))=1\) for any \(i\in\mathsf{S}\).

We now aim at applying Lemma 14 to the set \(\mathsf{A}^{0}\). Remark that \(\mathsf{A}^{0}\) directly satisfies assumption (a). For any \(n\in\mathbb{N}^{*}\), consider the following transformation of the functions \(\{\varphi_{i}^{n}\}_{i\in\mathsf{S}}\)

\[\varphi_{i_{k-1}}^{n} \leftarrow\varphi_{i_{k-1}}^{n}+\sum_{k=0}^{K-2}\varphi_{i_{k}}^ {n}(x_{i_{k}}^{\star})\;.\]

For any \(i\in\mathsf{S}\), we restrict \(\varphi_{i_{k}}^{n}\) to \(\mathsf{X}^{0}_{i_{k}}\), so that the family \(\{\varphi_{i}^{n}\}_{n\in\mathbb{N}^{*},i\in\mathsf{S}}\) now verifies assumption (b). Finally, since \(\mathsf{A}^{0}\subset\mathsf{A}^{\star}\) and \(x^{\star}\in\mathsf{B}^{\star}\), we directly obtain assumption (c).

Therefore, Lemma 14 may be applied. It provides us with the family of functions \(\{\varphi_{i}\}_{i\in\mathsf{S}}\) defined by \(\varphi_{i}:\mathsf{X}^{0}_{i}\to[-\infty,+\infty)\) with \(\varphi_{i}=\lim_{n\to\infty}\varphi_{i}^{n}\ \mu_{i}\)-a.s. for any \(i\in\mathsf{S}\). Since \(\mu_{i}(\operatorname{proj}_{i}(\mathsf{A}^{0}))=1\) for any \(i\in\mathsf{S}\), we may extend the functions \(\varphi_{i}\) to \(\mathbb{R}^{d}\). In particular, we can find a family of functions \(\{\psi_{i}^{\star}\}_{i\in\mathsf{S}}\) with \(\psi_{i}^{\star}:\mathbb{R}^{d}\to[-\infty,+\infty)\) such that \(\psi_{i}^{\star}=\varphi_{i}\ \mu_{i}\)-a.s. Note that these functions are measurable as limits of measurable functions.

Since \(\pi^{0}\sim\tilde{\pi}^{0}\) by **A3**, (10) turns into

\[(\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\psi_ {i}^{\star}]\quad\pi^{0}\text{-a.s.}\;.\] (11)

Finally, we show that the functions \(\psi_{i}^{\star}\) are \(\mu_{i}\)-a.s. finite. Let \(i\in\mathsf{S}\). Let us define \(\mathsf{A}_{i}=\{x_{i}\in\mathbb{R}^{d}:\psi_{i}^{\star}(x_{i})=-\infty\}\). Using (11), we obtain \((\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})(\mathsf{A}_{i}\times(\mathbb{R}^{d} )^{\ell})=0\). Since \(\pi_{i}^{\star}=\mu_{i}\), we have

\[\mu_{i}(\mathsf{A}_{i})=\pi^{\star}(\mathsf{A}_{i}\times(\mathbb{R}^{d})^{ \ell})=\int_{\mathsf{A}_{i}\times(\mathbb{R}^{d})^{\ell}}(\mathrm{d}\pi^{\star} /\mathrm{d}\pi^{0})\mathrm{d}\pi^{0}=0\;,\]

which gives the result. 

We now turn to the proof of Corollary 4, which states that the iterates of (mIPF) can be expressed via potentials, in the same manner as the solution \(\pi^{\star}\) to (static-mSB).

Proof of Corollary 4.: Assume **A1**, **A2** and **A3**. We prove the result of this corollary by recursion on \(n\in\mathbb{N}^{*}\). First take \(n=1\). In this case, the first iteration of (mIPF) is a multi-marginal SB problem of the form (static-mSB) where \(S=\{i_{0}\}\) with reference measure \(\pi^{0}\). Therefore, using **A2** and **A3**, we can apply Proposition 3 and obtain existence of \(\psi_{i_{0}}^{1}:\mathbb{R}^{d}\to\mathbb{R}\) such that

\[(\mathrm{d}\pi^{1}/\mathrm{d}\pi^{0})=\exp[\psi_{i_{0}}^{1}]\quad\pi^{0}\text{- a.s}\;.\]

By taking \(\psi_{i_{k}}^{0}=0\) for \(k\in\{1,\ldots,K-1\}\), we thus obtain the result at step \(n=1\).

Now assume that the result is verified for some \(n\in\mathbb{N}^{*}\), with \(k_{n}=(n-1)\ \mathrm{mod}(K)\). We define \(k_{n}+1=n\ \mathrm{mod}(K)\) and \(q_{n}\in\mathbb{N}\) as the quotient of the Euclidean division of \(n\) by \(K\). In this case, the \((n+1)\)-th iteration of (mIPF) is a multi-marginal SB problem of the form (static-mSB) where \(\mathsf{S}=\{i_{k_{n}+1}\}\) with reference measure \(\pi^{n}\). Using (13), we have that **A2** is satisfied for this new (static-mSB) problem. **A1** and **A3** are satisfied for this problem, given the form of \(\pi^{n}\). Therefore, we can apply Proposition 3 and obtain existence of \(\psi_{i_{k_{n+1}}}^{q_{n+1}}:\mathbb{R}^{d}\to\mathbb{R}\) such that

\[(\mathrm{d}\pi^{n+1}/\mathrm{d}\pi^{n})=\exp[\psi_{i_{k_{n}+1}}^{q_{n+1}}]\quad \pi^{n}\text{-a.s}\;.\] (12)

By assumption, we have that \(\pi^{n}\ll\pi^{0}\). Hence, we obtain \(\pi^{n+1}\ll\pi^{0}\) and thus,

\[(\mathrm{d}\pi^{n+1}/\mathrm{d}\pi^{0})=(\mathrm{d}\pi^{n+1}/\mathrm{d}\pi^{n})( \mathrm{d}\pi^{n}/\mathrm{d}\pi^{0})\quad\pi^{0}\text{-a.s}\;.\]

By combining (12) with the result of the recursion at step \(n\), we directly obtain the result at step \(n+1\), which achieves the proof.

Proofs of Proposition 5 and Proposition 6.In this part of the section, we establish the proofs of results related to the convergence of (mIPF), respectively Proposition 5 and Proposition 6, which can be seen as a natural extension of (Ruschendorf, 1995, Proposition 2.1.) and (Ruschendorf, 1995, Theorem 3.1.).

Proof of Proposition 5.Under \(\mathbf{A}1\) and \(\mathbf{A}2\), we obtain by Proposition 3 existence and uniqueness of a solution to (static-mSB), which we denote by \(\pi^{\star}\). Since \(\pi^{\star}\in\mathscr{P}_{\mathsf{S}}\), using recursively (Csiszar, 1975, Theorem 3.12.), the fact that \(\{\pi_{i_{k}}=\mu_{i_{k}}\;:\;\pi\in\mathscr{P}^{(|\mathsf{V}|)}\}\) is convex for any \(k\in\{0,\dots,K-1\}\) and (mIPF), we obtain

\[\operatorname{KL}(\pi^{\star}\mid\pi^{0})=\operatorname{KL}(\pi^{\star}\mid \pi^{n})+\sum_{i=1}^{n}\operatorname{KL}(\pi^{i}\mid\pi^{i-1})\;.\] (13)

Therefore, we have \(\sum_{i=1}^{\infty}\operatorname{KL}(\pi^{i}\mid\pi^{i-1})\leq\operatorname {KL}(\pi^{\star}\mid\pi^{0})<\infty\) and thus,

\[\lim_{i\to+\infty}\operatorname{KL}(\pi^{i}\mid\pi^{i-1})=0\;.\] (14)

Let \(n\in\mathbb{N}^{*}\) with \(n>2K\), \(k\in\{0,\dots,K-1\}\) and let \(q_{n}\in\mathbb{N}\) be the quotient of the Euclidean division of \(n-1\) by \(K\). We define \(n_{k}=q_{n}K+k+1\) with \((n_{k}-1)=k\bmod(K)\) if \(n_{k}\leq n\). Otherwise, we set \(n_{k}=(q_{n}-1)K+k+1\) with \((n_{k}-1)=k\bmod(K)\). Note that we always have \(|n-n_{k}|\leq 2K\). In particular, we have \(\pi_{i_{k}}^{n_{k}}=\mu_{i_{k}}\) by definition of (mIPF). Therefore, we obtain

\[\|\pi_{i_{k}}^{n}-\mu_{i_{k}}\|_{\mathrm{TV}} \leq\|\pi^{n}-\pi^{n_{k}}\|_{\mathrm{TV}}\] \[\leq\|\pi^{n}-\pi^{n-1}\|_{\mathrm{TV}}+\dots+\|\pi^{n_{k}+1}-\pi ^{n_{k}}\|_{\mathrm{TV}}\] (triangle inequality) \[\leq(2\operatorname{KL}(\pi^{n}\mid\pi^{n-1}))^{1/2}+\dots+(2 \operatorname{KL}(\pi^{n_{k}+1}\mid\pi^{n_{k}}))^{1/2}\;,\] (Pinsker's inequality)

where each term goes to 0 as \(n\to+\infty\) in the last inequality by (14), which achieves the proof. 

We now turn to the proof Proposition 6, which requires several preliminary technical results. For the rest of this section, we define, for any \(n\in\mathbb{N}\), \(q_{n}\) as the quotient of the Euclidean division of \(n-1\) by \(K\) (in particular, \(q_{0}=-1\)).

Schrodinger equations.Under \(\mathbf{A}1\), \(\mathbf{A}2\) and \(\mathbf{A}3\), we know from Proposition 3 that the unique solution \(\pi^{\star}\) to (static-mSB) can be \(\pi^{0}\)-a.s. written as \((\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\psi _{i}^{\star}]\), where \(\{\psi_{i}^{\star}\}_{i\in\mathsf{S}}\) are measurable potentials, referred to as _Schrodinger potentials_. These functions are determined by the fixed-point _Schrodinger equations_

\[\psi_{i}^{\star}(x_{i})=\log[r_{i}(x_{i})/\int_{(\mathbb{R}^{d})^{\ell}}\exp[ \sum_{j\in\mathsf{S}\setminus\{i\}}\psi_{j}^{\star}(x_{j})]h(x_{0:\ell}) \mathrm{d}\nu_{-i}(x_{-i})]\quad\mu_{i}\text{-a.s.},\quad\forall i\in\mathsf{S}\;,\]

which are obtained by marginalising \(\pi^{\star}\) along its constrained marginals. This family of potentials is not unique. Indeed, for any family of real numbers \(\{\lambda_{i_{k}}\}_{k\in\{0,\dots,K-2\}}\), we have

\[(\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\tilde {\psi}_{i}]\;,\]

where \(\tilde{\psi}_{i_{k}}=\psi_{i_{k}}^{\star}+\tilde{\lambda}_{i_{k}}\) for any \(k\in\{0,\dots,K-1\}\) with \(\tilde{\lambda}_{i_{k}}=\lambda_{i_{k}}\) if \(k\in\{0,\dots,K-2\}\) and \(\tilde{\lambda}_{i_{K-1}}=-\sum_{i=0}^{K-2}\lambda_{i_{k}}\).

Remark on the initialisation of (mIPF).Consider a probability measure \(\bar{\pi}^{0}\in\mathscr{P}^{(\ell+1)}\) of the form

\[(\mathrm{d}\bar{\pi}^{0}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\psi _{i}^{0}]\;,\] (15)

where \(\{\psi_{i}^{0}\}_{i\in\mathsf{S}}\) is a family of measurable potentials with \(\psi_{i}^{0}:\mathbb{R}^{d}\to\mathbb{R}\) such that \(\big{|}\int_{\mathbb{R}^{d}}\psi_{i}^{0}\mathrm{d}\mu_{i}\big{|}<\infty\) for any \(i\in\mathsf{S}\). Then, for any \(\pi\in\mathscr{P}_{\mathsf{S}}\), we have

\[\operatorname{KL}(\pi\mid\pi^{0})=\operatorname{KL}(\pi\mid\bar{\pi}^{0})+ \int_{(\mathbb{R}^{d})^{k}}\bigoplus_{i\in\mathsf{S}}\psi_{i}^{0}\mathrm{d} \pi=\operatorname{KL}(\pi\mid\bar{\pi}^{0})+\sum_{i\in\mathsf{S}}\int_{\mathbb{ R}^{d}}\psi_{i}^{0}\mathrm{d}\mu_{i}\;.\]

Hence, (static-mSB) is equivalent to the multi-marginal SB problem

\[\operatorname{argmin}\{\operatorname{KL}(\pi|\bar{\pi}^{0})\;:\;\pi\in\mathscr{ P}^{(\ell+1)},\;\pi_{i}=\mu_{i}\;,\forall i\in\mathsf{S}\}\;.\]

We refer to (Peyre et al., 2019, Proposition 4.2) for the EOT counterpart of this result. This means that the solutions of the multi-marginal Schrodinger Bridge problem are invariant by multiplication of the reference measure by potentials on the _fixed_ marginals. Consequently, the initialisation of the (mIPF) sequence may be chosen as \(\bar{\pi}^{0}\) instead of \(\pi^{0}\).

For sake of clarity, we now refer to the reference probability measure of (static-mSB) as \(\bar{\pi}\) or \(\pi^{-1}\) and to the initialisation of the (mIPF) iterates as \(\pi^{0}\).

Solving (mIPF) with potentials.To prove the convergence of the (mIPF) iterates to the solution \(\pi^{\star}\) given by Proposition 3, we first rewrite these iterates with potentials, following the form of \(\pi^{\star}\).

To do so, we recursively define the sequence of potentials \(\{\psi_{i}^{n}\}_{n\in\mathbb{N},i\in\mathsf{5}}\) by

\[\psi_{i_{0}}^{0}=\ldots=\psi_{i_{K-2}}^{0}=0\,\] (16) \[\psi_{i_{K-1}}^{0}(x_{i_{K-1}})=\log(r_{i_{K-1}}(x_{i_{K-1}})/ \int_{(\mathbb{R}^{d})^{\ell}}h(x_{0:\ell})\mathrm{d}\nu_{-i_{K-1}}(x_{-i_{K-1} }))\,\]

and for any \(n\in\mathbb{N}^{*}\) and \(k\in\{0,\ldots,K-1\}\)

\[\psi_{i_{k}}^{q_{n}+1}(x_{i_{k}})=\log[r_{i_{k}}(x_{i_{k}})/\int_ {(\mathbb{R}^{d})^{\ell}}\exp[\bigoplus_{\ell=0}^{k}\psi_{i_{\ell}}^{q_{n}+1}( x_{i_{\ell}})\bigoplus_{m=k+1}^{K-1}\psi_{i_{m}}^{q_{m}}(x_{i_{m}})]\] \[\qquad\qquad\qquad\qquad\times h(x_{0:\ell})\mathrm{d}\nu_{-i_{k }}(x_{-i_{k}})]\,\] (17)

recalling that \(q_{n}\) is the quotient of the Euclidean division of \(n-1\) by \(K\).

We now define the sequence of probability measures \(\{\pi^{n}\}_{n\in\mathbb{N}}\) by

\[\mathrm{d}\pi^{n}/\mathrm{d}\bar{\pi}=\exp[\bigoplus_{\ell=0}^{k_{n}}\psi_{i_ {\ell}}^{q_{n}+1}\bigoplus_{m=k_{n}+1}^{K-1}\psi_{i_{m}}^{q_{n}}],\;k_{n}=(n-1 )\bmod(K),\;n=q_{n}K+k_{n}+1.\] (18)

In particular, we have \((\mathrm{d}\pi^{0}/\mathrm{d}\bar{\pi})=\exp[\oplus_{\ell=0}^{K-1}\psi_{i_{ \ell}}^{0}]=\exp[\psi_{i_{K-1}}^{0}]\), and thus \(\int_{\mathbb{R}^{d}}\psi_{i_{K-1}}^{0}\mathrm{d}\mu_{i_{K-1}}=\mathrm{KL}(\mu _{i_{K-1}}\mid\bar{\pi}_{i_{K-1}})\). Consequently, \(\pi^{0}\) can be chosen as the initialisation of (mIPF), following the previous remark, if we assume that \(\mathrm{KL}(\mu_{i_{K-1}}\mid\bar{\pi}_{i_{K-1}})<\infty\). In (TreeSB) with \(r=i_{K-1}\), the latter assumption is directly verified since we choose \(\bar{\pi}_{i_{K-1}}=\mu_{i_{K-1}}\).

Let \(n\in\mathbb{N}\), with \(k_{n}=(n-1)\bmod(K)\), \(k_{n}+1=n\bmod(K)\). Using (16) and (17), we get that \(\pi_{i_{k_{n}}}^{n}=\mu_{i_{k_{n}}}\). Moreover, we have

\[\mathrm{d}\pi^{n}/\mathrm{d}\pi^{n-1}=\exp[\psi_{i_{k_{n}}}^{q_{n}+1}-\psi_{i_ {k_{n}}}^{q_{n}}]\,\] (19)

with the convention that \(\psi_{i_{K-1}}^{-1}=0\). In particular, we obtain that \(\pi_{|i_{k_{n}+1}}^{n+1}=\pi_{|i_{k_{n}+1}}^{n}\).

In conclusion, the sequence \(\{\pi^{n}\}_{n\in\mathbb{N}}\) defined in (18) verifies \(\pi^{n+1}=\mu_{i_{k_{n}+1}}\pi_{|i_{k_{n}+1}}^{n}\) for any \(n\in\mathbb{N}\). By decomposition property of the Kullback-Leibler divergence, this sequence solves (mIPF) with initialisation \(\pi^{0}\). _We consider such iterates in the following._

Since \(\pi_{i_{k_{n}}}^{n}=\mu_{i_{k_{n}}}\), we have that

\[\mathrm{KL}(\pi^{n}\mid\pi^{n-1})=\int_{\mathbb{R}^{d}}(\psi_{i_{k_{n}}}^{q_{n }+1}-\psi_{i_{k_{n}}}^{q_{n}})\mathrm{d}\mu_{i_{k_{n}}}\.\] (20)

Before proving a multi-marginal counterpart to (Ruschendorf, 1995, Lemma 4.1), we state and prove the following result.

**Proposition 17**.: _Let \(\pi_{0},\pi_{1}\) two probability measures on \(\mathbb{R}^{d}\) such that \(\pi_{0}\ll\pi_{1}\). Then, denoting \(f=\mathrm{d}\pi_{0}/\mathrm{d}\pi_{1}\), the following assertions are equivalent:_

* \(\mathrm{KL}(\pi_{0}\mid\pi_{1})<+\infty\)__
* \(\int_{\mathbb{R}^{d}}|\log(f)(x)|\mathrm{d}\pi_{0}(x)<+\infty\)__
* \(\int_{\mathbb{R}^{d}}\log(f)(x)\mathbbm{1}_{f(x)>1}\mathrm{d}\pi_{0}(x)<+\infty\)__

_If one of these conditions is satisfied then \(\int_{\mathbb{R}^{d}}|\log(f)(x)|\,\mathrm{d}\pi_{0}\leq\mathrm{KL}(\pi_{0} \mid\pi_{1})+2/\mathrm{e}\)._

Proof.: First, note that

\[\int_{\mathbb{R}^{d}}|\log(f)(x)|\mathbbm{1}_{f<1}\mathrm{d}\pi_{0}(x)\leq\int _{\mathbb{R}^{d}}|\log(f)(x)f(x)|\mathbbm{1}_{f<1}\mathrm{d}\pi_{1}(x)\leq 1/ \mathrm{e}\,\] (21)

where we have used that for any \(u\in[0,1]\), \(|u\log(u)|\leq 1/\mathrm{e}\). We have that (b) implies (c). Using the previous result we have that (c) implies (b). Hence (c) and (b) are equivalent. In addition, it is clear that (b) implies (a). Finally (this is more of a convention), we have that \(\mathrm{KL}(\pi_{0}\mid\pi_{1})=\int_{\mathbb{R}^{d}}\log(f)(x)\mathbbm{1}_{f( x)>1}\mathrm{d}\pi_{0}(x)+\int_{\mathbb{R}^{d}}\log(f)(x)\mathbbm{1}_{f(x)<1} \mathrm{d}\pi_{0}(x)<+\infty\). Using (21) this implies (c). Finally, we have

\[\int_{\mathbb{R}^{d}}|\log(f)(x)|\,\mathrm{d}\pi_{0}(x) =\int_{\mathbb{R}^{d}}\log(f)(x)\mathrm{d}\pi_{0}(x)-2\int_{\mathbb{ R}^{d}}\log(f)(x)\mathbbm{1}_{f(x)<1}\mathrm{d}\pi_{0}(x)\] \[\leq\mathrm{KL}(\pi_{0}\mid\pi_{1})+2/\mathrm{e},\]

which concludes the proof.

We begin with the following lemma which controls the integral of the potentials uniformly w.r.t. \(n\in\mathbb{N}\). It can be seen as the _multi-marginal_ counterpart of (Ruschendorf, 1995, Lemma 4.1).

**Lemma 18**.: _Assume \(\mathbf{A}4\). There exist \(\{c_{i}\}_{i\in\mathcal{S}}\in(0,+\infty)^{K}\) such that for any function \(f:(\mathbb{R}^{d})^{\ell+1}\to\mathbb{R}\) of the form \(f=\bigoplus_{i\in\mathcal{S}}f_{i}\), we have_

\[c_{i}\|f\|_{\mathrm{L}^{1}(\pi^{\star})}\geq\|f_{i}\|_{\mathrm{L}^{1}(\mu_{i}) },\quad\forall i\in\mathcal{S}\;.\] (22)

_For any \(n\in\mathbb{N}^{\star}\), we have_

1. \(\sum_{i\in\mathcal{S}}\int_{\mathbb{R}^{d}}\psi_{i}^{n}\mathrm{d}\mu_{i}\leq \mathrm{KL}(\pi^{\star}\mid\bar{\pi})<\infty\)_,_
2. \(\int_{(\mathbb{R}^{d})^{\ell+1}}(\bigoplus_{i\in\mathcal{S}}\psi_{i}^{\star}- \bigoplus_{i\in\mathcal{S}}\psi_{i}^{n})\mathrm{d}\pi^{\star}\leq\mathrm{KL}( \pi^{\star}\mid\bar{\pi})<\infty\)_,_
3. \(\sup_{n\in\mathbb{N}}\int_{\mathbb{R}^{d}}|\psi_{i}^{n}|\,\mathrm{d}\mu_{i}< \infty,\;\forall i\in\mathcal{S}\)_._

Proof.: First, we have that (22) is a direct consequence of (Kober, 1940, Theorem 1) and \(\mathbf{A}4\). Let us now prove (a). Using (20), we have

\[\begin{split}\sum_{m=0}^{Kn}\mathrm{KL}(\pi^{m}\mid\pi^{m-1})& =\sum_{\ell=0}^{n-1}\sum_{k=0}^{K-1}\mathrm{KL}(\pi^{\ell K+k+1} \mid\pi^{\ell K+k})+\mathrm{KL}(\pi^{0}\mid\pi^{-1})\\ &=\sum_{\ell=0}^{n-1}\sum_{i\in\mathcal{S}}\int_{\mathbb{R}^{d}}( \psi_{i}^{\ell+1}-\psi_{i}^{\ell})\mathrm{d}\mu_{i}+\int_{\mathbb{R}^{d}}( \psi_{i_{K-1}}^{0}-\psi_{i_{K-1}}^{-1})\mathrm{d}\mu_{i_{K-1}}\\ &=\sum_{i\in\mathcal{S}}\sum_{\ell=0}^{n-1}\int_{\mathbb{R}^{d}}( \psi_{i}^{\ell+1}-\psi_{i}^{\ell})\mathrm{d}\mu_{i}+\int_{\mathbb{R}^{d}}( \psi_{i_{K-1}}^{0}-\psi_{i_{K-1}}^{-1})\mathrm{d}\mu_{i_{K-1}}\\ &=\sum_{i\in\mathcal{S}}\int_{\mathbb{R}^{d}}(\psi_{i}^{n}-\psi_{ i}^{0})\mathrm{d}\mu_{i}+\int_{\mathbb{R}^{d}}(\psi_{i_{K-1}}^{0}-\psi_{i_{K-1}}^{ -1})\mathrm{d}\mu_{i_{K-1}}\\ &=\sum_{i\in\mathcal{S}}\int_{\mathbb{R}^{d}}\psi_{i}^{n} \mathrm{d}\mu_{i}\leq\mathrm{KL}(\pi^{\star}\mid\bar{\pi}).\;,\end{split}\]

where the last inequality follows the proof of Proposition 5.

Since the first term in the inequality of (b) is equal to \(\mathrm{KL}(\pi^{\star}\mid\pi^{nK})\), we obtain (b) using that \(\mathrm{KL}(\pi^{\star}\mid\pi^{nK})\leq\mathrm{KL}(\pi^{\star}\mid\bar{\pi})\) following the proof of Proposition 5.

Let us now prove (c). Since \(\mathrm{KL}(\pi^{\star}\mid\bar{\pi})<\infty\), using Proposition 17, we have that \(\bigoplus_{i\in\mathcal{S}}\psi_{i}^{\star}\in\mathrm{L}^{1}(\pi^{\star})\). From (b) and Proposition 17, we also get that \(\bigoplus_{i\in\mathcal{S}}(\psi_{i}^{\star}-\psi_{i}^{n})\in\mathrm{L}^{1}( \pi^{\star})\), and thus \(\int_{(\mathbb{R}^{d})^{\ell+1}}\big{|}\bigoplus_{i\in\mathcal{S}}(\psi_{i}^{ \star}-\psi_{i}^{n})\big{|}\,\mathrm{d}\pi^{\star}\leq C_{0}\) with \(C_{0}>0\). Therefore, we have

\[\int_{(\mathbb{R}^{d})^{\ell+1}}\big{|}\big{\!\bigoplus_{i\in\mathcal{S}}\psi_ {i}^{n}\big{|}\,\mathrm{d}\pi^{\star}\leq\int_{(\mathbb{R}^{d})^{\ell+1}}\big{|} \big{\!\bigoplus_{i\in\mathcal{S}}\psi_{i}^{\star}\big{|}\,\mathrm{d}\pi^{ \star}+\int_{(\mathbb{R}^{d})^{\ell+1}}\big{|}\big{\!\bigoplus_{i\in\mathcal{S} }(\psi_{i}^{\star}-\psi_{i}^{n})\big{|}\,\mathrm{d}\pi^{\star}\leq 2C_{0}\;.\]

Using (22), we conclude with \(\mathbf{A}4\) that for any \(i\in\mathcal{S}\), we have

\[\int_{\mathbb{R}^{d}}|\psi_{i}^{n}|\,\mathrm{d}\mu_{i}\leq 2c_{i}C_{0}\;,\]

which concludes the proof of (c). 

The next lemma gives an explicit expression for \(\mathrm{KL}(\pi^{n}\mid\bar{\pi})\). It can be seen as the _multi-marginal_ counterpart of (Ruschendorf, 1995, Lemma 4.2).

**Lemma 19**.: _For any \(n\in\mathbb{N}\), with \(k_{n}=(n-1)\bmod(K)\), we have_

\[\begin{split}\mathrm{KL}(\pi^{n}\mid\bar{\pi})=\int_{\mathbb{R}^{ d}}\psi_{i_{k_{n}}}^{q_{n}+1}\mathrm{d}\mu_{i_{k_{n}}}&+\sum_{\ell=0}^{ k_{n}-1}\int_{\mathbb{R}^{d}}\psi_{i_{\ell}}^{q_{n}+1}\mathrm{d}\pi_{i_{\ell}}^{q_{n}+1} \exp[\psi_{i_{\ell}}^{q_{n}+1}-\psi_{i_{\ell}}^{q_{n}+2}]\mathrm{d}\mu_{i_{ \ell}}\\ &+\sum_{m=k_{n}+1}^{K-1}\int_{\mathbb{R}^{d}}\psi_{i_{m}}^{q_{n}} \exp[\psi_{i_{m}}^{q_{n}}-\psi_{i_{m}}^{q_{n}+1}]\mathrm{d}\mu_{i_{m}}\;.\end{split}\]

Proof.: Let \(n\in\mathbb{N}\), with \(k_{n}=(n-1)\bmod(K)\). Using (18), we have

\[\begin{split}\mathrm{KL}(\pi^{n}\mid\bar{\pi})=\int_{\mathbb{R}^{ d}}\psi_{i_{k_{n}}}^{q_{n}+1}\mathrm{d}\mu_{i_{k_{n}}}+\sum_{\ell=0}^{k_{n}-1} \int_{\mathbb{R}^{d}}\psi_{i_{\ell}}^{q_{n}+1}\mathrm{d}\pi_{i_{\ell}}^{n}+ \sum_{m=k_{n}+1}^{K-1}\int_{\mathbb{R}^{d}}\psi_{i_{m}}^{q_{n}}\mathrm{d}\pi_{i_{ m}}^{n}\;.\end{split}\] (23)

Consider \(m\in\{k_{n}+1,\ldots,K-1\}\). Let \(m_{n}\) be the closest integer to \(n\) such that \(m_{n}>n\) and \(m=(m_{n}-1)\bmod(K)\). By (19), we have

\[\mathrm{d}\pi^{n}=\exp[\bigoplus_{j=k_{n}+1}^{m}\psi_{i_{j}}^{q_{n}}-\psi_{i_{j }}^{q_{n}+1}]\mathrm{d}\pi^{m_{n}}.\]

Using (19) recursively, we obtain

\[\mathrm{d}\pi_{i_{m}}^{n}=\exp[\psi_{i_{m}}^{q_{n}}-\psi_{i_{m}}^{q_{n}+1}] \mathrm{d}\pi_{i_{m}}^{m_{n}},\] (24)where we recall that \(\pi^{m_{n}}_{i_{m}}=\mu_{i_{m}}\).

Consider now \(\ell\in\{0,\ldots,k_{n}-1\}\). Let \(\ell_{n}\) be the closest integer to \(n\) such that \(\ell_{n}>n\) and \(\ell=(\ell_{n}-1)\bmod(K)\). By (19), we have

\[\mathrm{d}\pi^{n}=\exp[\bigoplus_{j=k_{n}+1}^{K-1}\{\psi^{q_{n}}_{i_{j}}-\psi^ {q_{n}+1}_{i_{j}}\}\bigoplus_{j^{\prime}=0}^{\ell}\{\psi^{q_{n}+1}_{i_{j^{ \prime}}}-\psi^{q_{n}+2}_{i_{j^{\prime}}}\}]\mathrm{d}\pi^{\ell_{n}},\]

and using (19) recursively, we obtain

\[\mathrm{d}\pi^{n}_{i_{\ell}}=\exp[\psi^{q_{n}+1}_{i_{\ell}}-\psi^{q_{n}+2}_{i_ {\ell}}]\mathrm{d}\pi^{\ell_{n}}_{i_{\ell}},\] (25)

where we recall that \(\pi^{\ell_{n}}_{i_{\ell}}=\mu_{i_{\ell}}\). We conclude the proof upon combining (23), (24) and (25). 

We are now ready to prove a _uniform integrability_ result which is the multi-marginal counterpart of (Ruschendorf, 1995, Lemma 4.4). Before stating Lemma 21, we prove the following well-known lemma. We recall that a sequence \((\Psi_{n})_{n\in\mathbb{N}}\) such that for any \(n\in\mathbb{N}\), \(\Psi_{n}\in\mathrm{L}^{1}(\mu)\), is _uniformly integrable_ w.r.t. \(\mu\) if \((\mathrm{i})\sup_{n\in\mathbb{N}}\int_{\mathbb{R}^{d}}|\Psi_{n}|\,\mathrm{d}\mu<+\infty\) and (ii) for any \(\varepsilon>0\), there exists \(K>0\) such that for any \(n\in\mathbb{N}\), \(\int_{\mathbb{B}(0,K)^{c}}|\Psi_{n}|\,\mathrm{d}\mu\leq\varepsilon\).

**Lemma 20**.: _Let \(f:\ \mathbb{R}\to\mathbb{R}\), convex and non-decreasing on \([A,+\infty)\) with \(A>0\) and \(\lim_{x\to+\infty}f(x)/x=+\infty\). Assume that \(\sup_{n\in\mathbb{N}}\int_{\mathbb{R}^{d}}f(|\Psi_{n}|)\mathrm{d}\mu<+\infty\). Then, \((\Psi_{n})_{n\in\mathbb{N}}\) is uniformly integrable w.r.t. \(\mu\)._

Proof.: Since \(f\) is convex, using Jensen's inequality, we get that \(\sup_{n\in\mathbb{N}}f(\int_{\mathbb{R}^{d}}|\Psi_{n}|\,\mathrm{d}\mu)<+\infty\) and since \(\lim_{x\to+\infty}f(x)/x=+\infty\) we have \(\sup_{n\in\mathbb{N}}\int_{\mathbb{R}^{d}}|\Psi_{n}|\,\mathrm{d}\mu<+\infty\). Let \(\varepsilon>0\), there exists \(K>0\) such that for any \(x>K\), \(x\leq\varepsilon f(x)/B\) with \(B=\sup_{n\in\mathbb{N}}\int_{\mathbb{R}^{d}}f(|\Psi_{n}|)\mathrm{d}\mu<+\infty\). Therefore, we have for any \(n\in\mathbb{N}\)

\[\int_{\mathbb{B}(0,K)^{c}}|\Psi_{n}|\,\mathrm{d}\mu\leq(\varepsilon/B)\int_{ \mathbb{B}(0,K)^{c}}f(|\Psi_{n}|)\mathrm{d}\mu\leq\varepsilon,\]

which concludes the proof. 

**Lemma 21**.: _Assume_ **A4** _and_ **A5**_. Then, \(\{\exp[\bigoplus_{i\in\mathsf{S}}\psi^{n}_{i}]\}_{n\in\mathbb{N}}\) is uniformly integrable w.r.t. \(\bar{\pi}\)._

Proof.: It is enough to show that the sequence \(\{f(\exp[\bigoplus_{i\in\mathsf{S}}\psi^{n}_{i})]\}_{n\in\mathbb{N}}\) is bounded in \(\mathrm{L}^{1}(\bar{\pi})\), where \(f:u\mapsto u\log(u)\) is continuous, convex and such that \(\lim_{u\to\infty}f(u)/u=+\infty\), see Lemma 20. Let \(n\in\mathbb{N}\). We have

\[\int_{(\mathbb{R}^{d})^{\ell+1}}f(\exp[\bigoplus_{i\in\mathsf{S}} \psi^{n}_{i})]\mathrm{d}\bar{\pi}=\mathrm{KL}(\pi^{nK}\mid\bar{\pi})\] \[\quad=\int_{\mathbb{R}^{d}}\psi^{n}_{i_{K-1}}\mathrm{d}\mu_{i_{K- 1}}+\sum_{k=0}^{K-2}\int_{\mathbb{R}^{d}}\psi^{n}_{i_{k}}\exp[\psi^{n}_{i_{k}}- \psi^{n+1}_{i_{k}}]\mathrm{d}\mu_{i_{k}}\] (Lemma 19) \[\quad=\sum_{k=0}^{K-1}\int_{\mathbb{R}^{d}}\psi^{n}_{i_{k}}\mathrm{ d}\mu_{i_{k}}+\sum_{k=0}^{K-2}\int_{\mathbb{R}^{d}}\psi^{n}_{i_{k}}\{\exp[\psi^{n}_{i_{ k}}-\psi^{n+1}_{i_{k}}]-1\}\mathrm{d}\mu_{i_{k}}\] \[\quad\leq\mathrm{KL}(\pi^{\star}\mid\bar{\pi})+(\bar{c}+1)\sum_{k= 0}^{K-2}\int_{\mathbb{R}^{d}}\psi^{n}_{i_{k}}\mathrm{d}\mu_{i_{k}}\] (Lemma 18- (a), **A5**) \[\quad\leq\mathrm{KL}(\pi^{\star}\mid\bar{\pi})+(\bar{c}+1)\sum_{k= 0}^{K-2}\sup_{n\in\mathbb{N}}\int_{\mathbb{R}^{d}}\bigl{|}\psi^{n}_{i_{k}} \bigr{|}\,\mathrm{d}\mu_{i_{k}}<\infty\.\] (Lemma 18- (c))

With the preliminary results stated above, we are now ready to prove Proposition 6.

Proof of Proposition 6.: Using **A4** and **A5**, we have, by Lemma 21, uniform integrability of \(\{\exp[\bigoplus_{i\in\mathsf{S}}\psi^{n}_{i}]\}_{n\in\mathbb{N}}\) in \(\mathrm{L}^{1}(\bar{\pi})\). Therefore, the sequence \(\{\pi^{nK}\}_{n\in\mathbb{N}}\) is relatively compact with respect to the weak topology of \(\sigma(\mathrm{L}^{1}(\bar{\pi}),\mathrm{L}^{\infty}(\bar{\pi}))\), denoted as the \(\tau\)-topology. We recall that \(\lim_{n\to\infty}\mathrm{KL}(\pi^{nK+1}\mid\pi^{nK})=0\). This implies that \(\{\pi^{nK+1}\}_{n\in\mathbb{N}}\) is also relatively \(\tau\)-compact. By trivial recursion, we obtain that the sequences \(\{\pi^{nK+k}\}_{n\in\mathbb{N}}\), where \(k\in\{2,\ldots,K-1\}\) are also relatively \(\tau\)-compact. Therefore, \(\{\pi^{n}\}_{n\in\mathbb{N}}\) is relatively \(\tau\)-compact and \(\tau\)-sequentially compact.

We consider an increasing function \(\Phi:\mathbb{N}\to\mathbb{N}\) such that \(\{\pi^{m}\}_{m\in\Phi(\mathbb{N})}\) is a \(\tau\)-convergent subsequence, and we denote by \(\bar{\pi}\) its limit for this topology. In particular, \(\bar{\pi}\in\mathscr{P}_{\mathsf{S}}\) by Proposition 5. We assume without loss of generality that \(\Phi(\mathbb{N})\subset K\mathbb{N}\).

Using the lower semi-continuity of the Kullback-Leibler divergence (Dupuis & Ellis, 2011, Lemma 1.4.3), we get

\[\operatorname{KL}(\tilde{\pi}\mid\bar{\pi})\leq\liminf\operatorname{KL}(\pi^{m} \mid\bar{\pi})\leq\limsup\operatorname{KL}(\pi^{m}\mid\bar{\pi})\.\]

Consider \(k\in\{0,\dots,K-2\}\). By (19), we have

\[\tfrac{\mathrm{d}\mu_{i_{k}}}{\mathrm{d}\pi_{i_{k}}^{nK+k}}=\tfrac{\mathrm{d} \pi_{i_{k}}^{nK+k+1}}{\mathrm{d}\pi_{i_{k}}^{nK+k}}=\tfrac{\mathrm{d}\pi^{nK+k+ 1}}{\mathrm{d}\pi^{nK+k}}=\exp[\psi_{i_{k}}^{n+1}-\psi_{i_{k}}^{n}]\,\]

and thus,

\[\|\mu_{i_{k}}-\pi_{i_{k}}^{nK+k}\|_{\mathrm{TV}}=(1/2)\int_{\mathbb{R}^{d}} \left|\mathrm{d}\pi_{i_{k}}^{nK+k}/\mathrm{d}\mu_{i_{k}}-1\right|\mathrm{d} \mu_{i_{k}}=(1/2)\int_{\mathbb{R}^{d}}\left|\exp[\psi_{i_{k}}^{n}-\psi_{i_{k} }^{n+1}]-1\right|\mathrm{d}\mu_{i_{k}}\.\]

With Proposition 5, we obtain that \(\{\exp[\psi_{i_{k}}^{n}-\psi_{i_{k}}^{n+1}]\}_{n\in\mathbb{N}}\) converges to 1 in \(\mathrm{L}^{1}(\mu_{i_{k}})\). In addition using the uniform integrability of \(\{\psi_{i_{k}}^{n}\}_{n\in\mathbb{N}}\) and **A5**, we get

\[\limsup\sup_{n\to+\infty}\int_{\mathbb{R}^{d}}\psi_{i_{k}}^{n}\exp[\psi_{i_{k} }^{n}-\psi_{i_{k}}^{n+1}]\mathrm{d}\mu_{i_{k}}=\limsup\sup_{n\to+\infty}\int_ {\mathbb{R}^{d}}\psi_{i_{k}}^{n}\mathrm{d}\mu_{i_{k}}\.\]

We denote \(m=K\ell\). Since \(\operatorname{KL}(\pi^{m}\mid\bar{\pi})=\int_{\mathbb{R}^{d}}\psi_{i_{K-1}}^{ \ell}\mathrm{d}\mu_{i_{K-1}}+\sum_{k=0}^{K-2}\int_{\mathbb{R}^{d}}\psi_{i_{k} }^{\ell}\exp[\psi_{i_{k}}^{\ell}-\psi_{i_{k}}^{\ell+1}]\mathrm{d}\mu_{i_{k}}\) by Lemma 19, we finally have

\[\operatorname{KL}(\tilde{\pi}\mid\bar{\pi})\leq\limsup\{\sum_{k=0}^{K-1}\int _{\mathbb{R}^{d}}\psi_{i_{k}}^{\ell}\mathrm{d}\mu_{i_{k}}\}\leq\operatorname{ KL}(\pi^{\star}\mid\bar{\pi})\]

where the last inequality comes from Lemma 18.

Since \(\tilde{\pi}_{i}=\mu_{i}\) for any \(i\in\mathsf{S}\), using Proposition 5, we have \(\tilde{\pi}=\pi^{\star}\) by uniqueness of \(\pi^{\star}\). Hence, \(\pi^{\star}\) is the only limit point of \(\{\pi^{n}\}_{n\in\mathbb{N}}\) in the \(\tau\)-topology. In particular, \(\operatorname{KL}(\pi^{n}\mid\bar{\pi})\to\operatorname{KL}(\pi^{\star}\mid \bar{\pi})\). Since \(\mathscr{P}_{\mathsf{S}}\) is convex, this last result implies \(\|\pi^{\star}-\pi^{n}\|_{\mathrm{TV}}\to 0\), see the proof of Theorem 2.1 in Csiszar (1975). 

We finish this section by highlighting that **A5** is stronger than (Ruscendorf, 1995, B1). A natural extension of the latter assumption would consist of having a guarantee on the \((K-1)\) first potentials given by (17), as presented below.

**A6**.: _There exist \(0<\underline{c}<\bar{c}\) such that for any \(k\in\{0,\dots,K-2\}\), we have \(\underline{c}\leq\exp(-\psi_{i_{k}}^{1})\leq\bar{c}\)._

Under **A6**, (Ruscendorf, 1995, Lemma 4.3) can be adapted as written below.

**Lemma 22**.: _Assume **A6**. Then, for any \(n\in\mathbb{N}^{*}\)_

1. _for any_ \(k\in\{0,\dots,K-2\}\)_, there exists_ \(\alpha_{n,k}\in\mathbb{N}\) _such that_ \[\underline{c}\cdot(\underline{c}/\bar{c})^{\alpha_{n,k}(K-2)}\leq\exp[\psi_{i _{k}}^{n-1}-\psi_{i_{k}}^{n}]\leq\bar{c}\cdot(\bar{c}/\underline{c})^{ \alpha_{n,k}(K-2)}\]
2. _there exists_ \(\alpha_{n,K-1}\in\mathbb{N}\) _such that_ \[1/\bar{c}^{K-1}\cdot(\underline{c}/\bar{c})^{\alpha_{n,K-1}(K-2)}\leq\exp[ \psi_{i_{K-1}}^{n-1}-\psi_{i_{K-1}}^{n}]\leq 1/\underline{c}^{K-1}\cdot(\bar{c}/ \underline{c})^{\alpha_{n,K-1}(K-2)}\]

_where \(\{\alpha_{n,k}\}_{n\in\mathbb{N}^{*},k\in\{0,\dots,K-1\}}\) is a strictly increasing sequence that can be explicitly defined._

Proof.: We prove the result by recursion on \(n\in\mathbb{N}^{*}\).

Take \(n=1\). Let \(k\in\{0,\dots,K-2\}\). We define \(\alpha_{1,k}=0\) and directly obtain (a) by **A5** since \(\psi_{i_{k}}^{0}=0\). Let us prove (b). We have by (17)

\[\exp[\psi_{i_{K-1}}^{0}-\psi_{i_{K-1}}^{1}] =\tfrac{\int_{\{\mathbb{R}^{d}\}}\exp[\bigoplus_{k=0}^{K-2}\psi_{ i_{k}}^{1}]h\mathrm{d}\nu_{-i_{K-1}}}{\int_{\{\mathbb{R}^{d}\}}\exp[\bigoplus_{k=0}^{K-2} \psi_{i_{k}}^{0}]h\mathrm{d}\nu_{-i_{K-1}}}\] \[=\tfrac{\int_{\{\mathbb{R}^{d}\}}\epsilon\exp[\bigoplus_{k=0}^{K-2} \{\psi_{i_{k}}^{1}-\psi_{i_{k}}^{0}\}+\bigoplus_{k=0}^{K-2}\psi_{i_{k}}^{0}]h \mathrm{d}\nu_{-i_{K-1}}}{\int_{\{\mathbb{R}^{d}\}}\epsilon\exp[\bigoplus_{k=0}^{K -2}\psi_{i_{k}}^{0}]h\mathrm{d}\nu_{-i_{K-1}}}\.\]

Using (a) at rank \(n=1\), we have

\[1/\bar{c}^{K-1}\leq\exp[\bigoplus_{k=0}^{K-2}\{\psi_{i_{k}}^{1}-\psi_{i_{k}}^{0} \}]\leq 1/\underline{c}^{K-1}\,\]and therefore, we obtain (b) by taking \(\alpha_{1,K-1}=0\). Let us assume that the result is verified for some \(n\in\mathbb{N}^{*}\). We have

\[\exp[\psi_{i_{0}}^{n}-\psi_{i_{0}}^{n+1}] =\frac{\int\exp[\bigoplus_{k=1}^{K-2}\psi_{i_{k}}^{n}]h\mathrm{d} \nu_{-i_{0}}}{\int\exp[\bigoplus_{k=1}^{K-1}\psi_{i_{k}}^{n-1}]h\mathrm{d}\nu_{ -i_{0}}}\] \[=\frac{\int\exp[\bigoplus_{k=1}^{K-2}\{\psi_{i_{k}}^{n}-\psi_{i_{ k}}^{n-1}\}\oplus\{\psi_{i_{K-1}}^{n-1}-\psi_{i_{K-1}}^{n-1}\}+\bigoplus_{k=1}^{K-1} \psi_{i_{k}}^{n-1}]h\mathrm{d}\nu_{-i_{0}}}{\int\exp[\bigoplus_{k=1}^{K-1}\psi _{i_{k}}^{n-1}]h\mathrm{d}\nu_{-i_{0}}}\]

Using (a) and (b) at rank \(n\), we have

\[1/\bar{c}^{K-2}\cdot(\underline{c}/\bar{c})^{(K-2)\sum_{k=1}^{K- 2}\alpha_{n,k}} \leq\exp[\bigoplus_{k=1}^{K-2}\{\psi_{i_{k}}^{n}-\psi_{i_{k}}^{n-1 }\}]\] \[\leq 1/\bar{c}^{K-2}\cdot(\bar{c}/\underline{c})^{(K-2)\sum_{k=1}^ {K-2}\alpha_{n,k}}\,\] \[\underline{c}^{K-1}\cdot(\underline{c}/\bar{c})^{\alpha_{n,K-1}( K-2)} \leq\exp[\psi_{i_{K-1}}^{n}-\psi_{i_{K-1}}^{n-1}]\leq\bar{c}^{K-1} \cdot(\bar{c}/\underline{c})^{\alpha_{n,K-1}(K-2)}\.\]

Therefore, we obtain

\[\underline{c}\cdot(\underline{c}/\bar{c})^{(K-2)\sum_{k=1}^{K- 1}\alpha_{n,k}} \leq\exp[\bigoplus_{k=1}^{K-2}\{\psi_{i_{k}}^{n}-\psi_{i_{k}}^{n-1 }\}\oplus\{\psi_{i_{K-1}}^{n}-\psi_{i_{K-1}}^{n-1}\}]\] \[\leq\bar{c}\cdot(\bar{c}/\underline{c})^{(K-2)\sum_{k=1}^{K-1} \alpha_{n,k}}\,\] \[\underline{c}\cdot(\underline{c}/\bar{c})^{(K-2)\sum_{k=1}^{K- 1}\alpha_{n,k}} \leq\exp[\psi_{i_{0}}^{n}-\psi_{i_{0}}^{n+1}]\leq\bar{c}\cdot( \bar{c}/\underline{c})^{(K-2)\sum_{k=1}^{K-1}\alpha_{n,k}}\.\]

Now, we define \(\alpha_{n+1,0}=\sum_{k=1}^{K-1}\alpha_{n,k}\) to obtain (a) for \(k=0\). Consider now \(k\in\{1,\ldots,K-2\}\). Following the same steps as above, we recursively define

\[\alpha_{n+1,k}=\sum_{j=0}^{k-1}\alpha_{n+1,j}+\sum_{j^{\prime}=k+1}^{K-1} \alpha_{n,j^{\prime}}\,\]

which gives (a) at rank \(n+1\). Let us now prove (b) at rank \(n+1\). We have

\[\exp[\psi_{i_{K-1}}^{n}-\psi_{i_{K-1}}^{n+1}] =\frac{\int\exp[\bigoplus_{k=0}^{K-2}\psi_{i_{k}}^{n+1}]h\mathrm{ d}\nu_{-i_{K-1}}}{\int\exp[\bigoplus_{k=0}^{K-2}\psi_{i_{k}}^{n}]h\mathrm{d}\nu_{-i_{ K-1}}}\] \[=\frac{\int\exp[\bigoplus_{k=0}^{K-2}\psi_{i_{k}}^{n+1}-\psi_{i_{ k}}^{n}]+\bigoplus_{k=0}^{K-2}\psi_{i_{k}}^{n}]h\mathrm{d}\nu_{-i_{k}}}{\int\exp[ \bigoplus_{k=0}^{K-2}\psi_{i_{k}}^{n}]h\mathrm{d}\nu_{-i_{K-1}}}\.\]

Using (a) at rank \(n+1\), we obtain

\[1/\bar{c}^{K-1}\cdot(\underline{c}/\bar{c})^{(K-2)\sum_{k=0}^{K- 2}\alpha_{n+1,k}} \leq\exp[\bigoplus_{k=0}^{K-2}\{\psi_{i_{k}}^{n+1}-\psi_{i_{k}}^{n}\}]\] \[\leq 1/\underline{c}^{K-1}\cdot(\bar{c}/\underline{c})^{(K-2)\sum_{ k=0}^{K-2}\alpha_{n+1,k}}\.\]

Therefore, by taking \(\alpha_{n+1,K-1}=\sum_{k=0}^{K-2}\alpha_{n+1,k}\), we obtain (b), which concludes the proof. 

Unfortunately, Lemma 22 only yields non-vacuous bounds in the case \(K=2\). Indeed, when \(K>2\), the sequence \(\{\alpha_{n,k}\}_{n\in\mathbb{N}^{*},k\in\{0,\ldots,K-1\}}\) leads to increase the bounds on the quantities \(\exp[\psi_{i_{k}}^{n-1}-\psi_{i_{k}}^{n}]\), which motivates the use of A5.

### Proof of Section 5

For the rest of this section, we consider the multi-marginal Schrodinger bridge problem given by (TreeSB) and establish in Proposition 24 the correspondence with the regularized Wasserstein propagation problem presented in Solomon et al. (2014, 2015). We first state a technical result.

**Lemma 23**.: _Let \(\varepsilon>0\). Assume that \(\pi^{0}\) is given by (2), where \(r\in\mathsf{V}\) is chosen arbitrarily. Then, for any \(\pi\in\mathscr{P}_{\mathsf{T}_{r}}\), we have_

\[\varepsilon\mathrm{KL}(\pi\mid\pi^{0})=\sum_{(v,v^{\prime})\in \mathsf{E}_{r}}\{w_{v,v^{\prime}}\mathbb{E}_{\pi_{v,v^{\prime}}}[\|X_{v}-X_{v^ {\prime}}\|^{2}]-\varepsilon\mathrm{H}(\pi_{v,v^{\prime}})\}\] \[\qquad\qquad+\varepsilon\sum_{v\in\mathsf{V}}\mathrm{card}( \mathrm{C}_{v})\mathrm{H}(\pi_{v})+\varepsilon\mathrm{KL}(\pi_{r}\mid\pi_{r}^{0})\,\]

_where we recall that \(\mathrm{C}_{v}=\{v^{\prime}\in\mathsf{V}:(v,v^{\prime})\in\mathsf{E}_{r}\}\)._Proof.: Since \(\pi,\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\), we obtain the following decomposition

\[\mathrm{KL}(\pi\mid\pi^{0})\] \[=\mathrm{KL}(\pi_{r}\mid\pi_{r}^{0})+\sum_{(v,v^{\prime})\in \mathsf{E}_{r}}\int_{\mathbb{R}^{d}}\mathrm{KL}(\pi_{v^{\prime}\mid v}(\cdot|x_{ v})\mid\pi_{v^{\prime}\mid v}^{0}(\cdot|x_{v}))\mathrm{d}\pi_{v}(x_{v})\] \[=\mathrm{KL}(\pi_{r}\mid\pi_{r}^{0})-\sum_{(v,v^{\prime})\in \mathsf{E}_{r}}\int_{\mathbb{R}^{d}\times\mathbb{R}^{d}}\log\pi_{v^{\prime}\mid v }^{0}\mathrm{d}\pi_{v,v^{\prime}}-\sum_{(v,v^{\prime})\in\mathsf{E}_{r}}\int_{ \mathbb{R}^{d}}\mathrm{H}(\pi_{v^{\prime}\mid v}(\cdot|x_{v}))\mathrm{d}\pi_{v }(x_{v})\.\]

We finally obtain the result by using the definition of \(\pi^{0}\) and noticing that \(\int_{\mathbb{R}^{d}}\mathrm{H}(\pi_{v^{\prime}\mid v}(\cdot|x_{v}))\mathrm{d }\pi_{v}(x_{v})=\mathrm{H}(\pi_{v,v^{\prime}})-\mathrm{H}(\pi_{v})\) for any \((v,v^{\prime})\in\mathsf{E}_{r}\). 

**Proposition 24**.: _Let \(\varepsilon>0\) and \(\mu_{0}\in\mathscr{P}\) such that \(\mu_{0}\ll\mathrm{Leb}\). Assume that \(\pi^{0}\) is given by (2), where \(r\in\mathsf{V}\) is chosen arbitrarily, and that \(\varphi_{r}=\mathrm{d}\mu_{0}/\mathrm{d}\mathrm{Leb}\). Also assume **A2**. Then, the set of marginals of the solution to (TreeSB) is exactly the solution to the entropic-regularized Wasserstein Propagation problem (Solomon et al., 2014, 2015) defined by_

\[\arg\min\{\sum_{(v,v^{\prime})\in\mathsf{E}_{r}}w_{v,v^{\prime}}W _{2,\varepsilon/w_{v,v^{\prime}}}^{2}(\nu_{v},\nu_{v^{\prime}})+\varepsilon \sum_{v\in\mathsf{V}}\mathrm{card}(\mathrm{C}_{v})\mathrm{H}(\nu_{v})+ \varepsilon\mathrm{KL}(\nu_{r}\mid\mu_{0}):\] (WP) \[\{\nu_{v}\}_{v\in\mathsf{V}}\in\mathscr{P}^{\ell+1},\ \nu_{i}=\mu_{i},\forall i\in\mathsf{S}\}\,\]

_where we recall that \(\mathrm{C}_{v}=\{v^{\prime}\in\mathsf{V}:(v,v^{\prime})\in\mathsf{E}_{r}\}\)._

Proof.: Assume that \(\pi^{0}\) is given by (2), where \(r\in\mathsf{V}\) is chosen arbitrarily, and that \(\varphi_{r}=\mathrm{d}\mu_{0}/\mathrm{d}\mathrm{Leb}\). In particular, we have \(\pi_{r}^{0}=\mu_{0}\). Moreover, it is clear that \(\pi^{0}\) verifies **A1**, and **A3** by Proposition 16.

Let \(\{\nu_{v}\}_{v\in\mathsf{V}}\in\mathscr{P}^{\ell+1}\) and \(\{\nu^{(v,v^{\prime})}\}_{(v,v^{\prime})\in\mathsf{E}_{r}}\in(\mathscr{P}^{( 2)})^{|\mathsf{E}_{r}|}\). We define

\[F(\{\nu_{v}\}) =\sum_{(v,v^{\prime})\in\mathsf{E}_{r}}w_{v,v^{\prime}}W_{2, \varepsilon/w_{v,v^{\prime}}}^{2}(\nu_{v},\nu_{v^{\prime}})+\varepsilon\sum_{ v\in\mathsf{V}}\mathrm{card}(\mathrm{C}_{v})\mathrm{H}(\nu_{v})+ \varepsilon\mathrm{KL}(\nu_{r}\mid\mu_{0})\,\] \[G(\nu_{r},\{\nu^{(v,v^{\prime})}\}) =\sum_{(v,v^{\prime})\in\mathsf{E}_{r}}\{w_{v,v^{\prime}}\mathbb{ E}_{\nu^{(v,v^{\prime})}}[\|X_{v}-X_{v^{\prime}}\|^{2}]-\varepsilon\mathrm{H}(\nu^{(v,v^{ \prime})})\}\] \[\quad+\varepsilon\sum_{(v,v^{\prime})\in\mathsf{E}_{r}}\mathrm{H}( \nu_{v}^{(v,v^{\prime})})+\varepsilon\mathrm{KL}(\nu_{r}\mid\mu_{0})\.\]

By definition of the regularized Wasserstein distance given in (3), we have for any \(\{\nu_{v}\}_{v\in\mathsf{V}}\in\mathscr{P}^{\ell+1}\)

\[F(\{\nu_{v}\})=\min\{G(\nu_{r},\{\nu^{(v,v^{\prime})}\}):\nu^{(v,v^{\prime})} \in\mathscr{P}^{(2)},\nu_{v}^{(v,v^{\prime})}=\nu_{v},\nu_{v^{\prime}}^{(v,v^{ \prime})}=\nu_{v^{\prime}},\forall(v,v^{\prime})\in\mathsf{E}_{r}\}\.\] (26)

In particular, we have \(F(\{\pi_{v}\})\leq G(\pi_{r},\{\pi_{v,v^{\prime}}\})\) for any \(\pi\in\mathscr{P}^{(\ell+1)}\). We now prove the result of Proposition 24 in two steps denoted by **Step 1** and **Step 2**.

Step 1.Let us not assume **A2** for now. In this case, we prove in **Step 1.a** and **Step 1.b** that solving (WP) is equivalent to solving a modified version of (TreeSB) given by

\[\pi^{\star}=\mathrm{argmin}\{\mathrm{KL}(\pi|\pi^{0})\,:\ \pi\in\mathscr{P}_{\mathsf{T}_{r}},\ \pi_{i}=\mu_{i}\,\forall i\in\mathsf{S}\}\.\] (T \[{}_{r}\] -TreeSB)

Remark that any solution to (TreeSB), but the converse result may not be true.

Step 1.a: (WP) \(\Longrightarrow\) (T\({}_{r}\)-TreeSB).Consider a solution \(\{\nu^{\star}_{v}\}_{v\in\mathsf{V}}\) to (WP). For any \((v,v^{\prime})\in\mathsf{E}_{r}\), \(W_{2,\varepsilon/w_{v,v^{\prime}}}^{2}(\nu^{\star}_{v},\nu^{\star}_{v^{\prime}})\) is well defined and thus, there exists \(\nu^{(v,v^{\prime})}\in\Pi(\nu^{\star}_{v},\nu^{\star}_{v^{\prime}})\) such that

\[\nu^{(v,v^{\prime})}\in\mathrm{argmin}\{\mathbb{E}_{r}[\|X_{v}-X_{v^{\prime}}\|^{ 2}]-(\varepsilon/w_{v,v^{\prime}})\mathrm{H}(\pi):\pi\in\Pi(\nu^{\star}_{v},\nu^{ \star}_{v^{\prime}})\}\.\] (27)

Using the gluging lemma, we build the probability measure \(\pi^{\star}=\nu^{\star}_{r}\prod_{(v,v^{\prime})\in\mathsf{E}_{r}}\nu^{(v,v^{ \prime})}_{v^{\prime}|v}\) such that (i) \(\pi^{\star}\in\mathscr{P}_{\mathsf{T}_{r}}\), and (ii) \(\pi^{\star}_{v,v^{\prime}}\) and \(\nu^{(v,v^{\prime})}\) have the same distribution for any \((v,v^{\prime})\in\mathsf{E}_{r}\). In particular, we have \(\pi^{\star}_{i}=\mu_{i}\) for any \(i\in\mathsf{S}\).

Let us show now that \(\pi^{\star}\) is a solution to (\(\mathsf{T}_{r}\)-TreeSB). Let \(\pi\in\mathscr{P}_{\mathsf{T}_{r}}\) such that \(\pi_{i}=\mu_{i}\) for any \(i\in\mathsf{S}\). We have

\[\epsilon\mathrm{KL}(\pi\mid\pi^{0}) =G(\pi_{r},\{\pi_{v,v^{\prime}}\}) \text{(Lemma~{}\ref{lem:T1})}\] \[\geq F(\{\pi_{v}\})\] \[\geq F(\{\nu^{\star}_{v}\}) \text{(definition of~{}}\nu^{\star})\] \[=G(\nu^{\star}_{r},\{\nu^{(v,v^{\prime})}\}) \text{(see~{}\eqref{eq:P1})}\] \[=G(\pi^{\star}_{r},\{\pi^{\star}_{(v,v^{\prime})}\}) \text{(definition of~{}}\pi^{\star})\] \[=\epsilon\mathrm{KL}(\pi^{\star}\mid\pi^{0})~{}. \text{(Lemma~{}\ref{lem:T1})}\]

Therefore, \(\pi^{\star}\) is a solution to (\(\mathsf{T}_{r}\)-TreeSB).

Step 1.b: (\(\mathsf{T}_{r}\)-TreeSB) \(\Longrightarrow\) (WP).Consider now a solution \(\pi^{\star}\) to (\(\mathsf{T}_{r}\)-TreeSB). Since \(\pi^{\star}\in\mathscr{P}_{\mathsf{T}_{r}}\), we have \(\pi^{\star}=\pi^{\star}_{r}\prod_{(v,v^{\prime})\in\mathsf{E}_{r}}\pi^{\star} _{v^{\prime}|v}\) and \(\pi^{\star}_{i}=\mu_{i}\) for any \(i\in\mathsf{S}\).

Let us show that \(\{\pi^{\star}_{v}\}_{v\in\mathsf{V}}\) is a solution to (WP). Let \(\{\nu_{v}\}_{v\in\mathsf{V}}\in\mathscr{P}^{\ell+1}\) such that \(\nu_{i}=\mu_{i}\) for any \(i\in\mathsf{S}\).

Let \(\{\nu^{(v,v^{\prime})}\}_{(v,v^{\prime})\in\mathsf{E}_{r}}\) be a family of probability measures such that \(\nu^{(v,v^{\prime})}\in\mathscr{P}^{(2)},\nu^{(v,v^{\prime})}_{v}=\nu_{v},\nu^ {(v,v^{\prime})}_{v^{\prime}}=\nu_{v^{\prime}}\) for any \((v,v^{\prime})\in\mathsf{E}_{r}\).

Using the gluing lemma, we build the probability measure \(\pi=\nu_{r}\prod_{(v,v^{\prime})\in\mathsf{E}_{r}}\nu^{(v,v^{\prime})}_{v^{ \prime}|v}\), such that (i) \(\pi\in\mathscr{P}_{\mathsf{T}_{r}}\) and (ii) \(\pi_{v,v^{\prime}}\) and \(\nu^{(v,v^{\prime})}\) have the same distribution for any \((v,v^{\prime})\in\mathsf{E}_{r}\). We have

\[\varepsilon\mathrm{KL}(\pi\mid\pi^{0}) =G(\pi_{r},\{\pi_{(v,v^{\prime})}\}) \text{(Lemma~{}\ref{lem:T1})}\] \[=G(\nu_{r},\{\nu^{(v,v^{\prime})}\}) \text{(definition of~{}}\pi)\] \[\geq\varepsilon\mathrm{KL}(\pi^{\star}\mid\pi^{0}) \text{(definition of~{}}\pi^{\star})\] \[=G(\pi^{\star}_{r},\{\pi^{\star}_{(v,v^{\prime})}\})~{}. \text{(Lemma~{}\ref{lem:T1})}\]

By taking the infimum in the previous inequality over the families \(\{\nu^{(v,v^{\prime})}\}_{(v,v^{\prime})\in\mathsf{E}_{r}}\), we obtain by (26) that

\[F(\{\nu_{v}\})\geq G(\pi^{\star}_{r},\{\pi^{\star}_{(v,v^{\prime})}\})\geq F( \{\pi^{\star}_{v}\}),\]

and therefore, \(\{\pi^{\star}_{v}\}_{v\in\mathsf{V}}\) is a solution to (WP).

Step 2.We now assume \(\mathbf{A2}\). By Proposition 3, there exists a unique solution \(\pi^{\star}\in\mathscr{P}^{(\ell+1)}\) to (TreeSB) such that we \(\pi^{0}\)-a.s. have \((\mathrm{d}\pi^{\star}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}}\psi^{ \star}_{i}]\), where \(\{\psi^{\star}_{i}\}_{i\in\mathsf{S}}\) are measurable potentials with \(\psi^{\star}:\mathbb{R}^{d}\to\mathbb{R}\). Since \(\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\), we also have \(\pi^{\star}\in\mathscr{P}_{\mathsf{T}_{r}}\), _i.e._, the potentials \(\{\psi^{\star}_{i}\}_{i\in\mathsf{S}}\) do not modify the Markovian nature of \(\pi^{0}\). Therefore, \(\pi^{\star}\) is also unique solution to (\(\mathsf{T}_{r}\)-TreeSB). Using the equivalence between (\(\mathsf{T}_{r}\)-TreeSB) and (WP) established in **Step 1**, we finally obtain the result of Proposition 24. 

In particular, Proposition 7 directly derives from Proposition 24 by taking \(r=i_{K-1}\) and \(\mu_{0}=\mu_{i_{K-1}}\).

### Comparison with Haasler et al. (2021)

In their work, Haasler et al. (2021) study the _static_ and _discrete-state_ counterpart of our approach. Given a state space \(\mathsf{X}\) such that \(|\mathsf{X}|=n+1\) with \(n\in\mathbb{N}\), they establish a correspondence between multi-marginal EOT with a general tree-based cost and discrete-time multi-marginal static Schrodinger bridge, and provide an efficient method to solve these problems. In this section, we provide details on their framework and give a precise comparison between our theory and their results.

To be coherent with the setting of Haasler et al. (2021), we adapt here some of our notation. Let us define \(\mathsf{Z}^{(q)}=\mathbb{R}^{(n+1)^{q}}_{+}\). For any \(q\in\mathbb{N}^{*}\), the set of probability measures on \(\mathsf{X}^{q}\) is defined as \(\mathscr{P}^{(q)}=\{M\in\mathsf{Z}^{(q)}:\langle M,\mathbf{1}\rangle=1\}\). We denote \(\mathscr{P}=\mathscr{P}^{(1)}\). For any tensors \(M,P\in\mathsf{Z}^{(q)}\), the Kullback-Leibler divergence between \(M\) and \(P\) is defined as \(\mathrm{KL}(M\mid P)=\langle M\log(M/P)-M+P,\mathbf{1}\rangle\) and the entropy of \(M\) is defined as \(\mathrm{H}(M)=-\mathrm{KL}(M\mid\mathbf{1})\), where the operations are meant componentwise. In the rest of the section, we consider an undirected tree \(\mathsf{T}=(\mathsf{V},\mathsf{E})\) with \(|\mathsf{V}|=\ell+1\) such that \(\mathsf{V}\) may be identified with \(\{0,\ldots,\ell\}\).

Details on the results of Haasler et al. (2021).In their paper, the authors consider a cost tensor \(C\in\mathsf{Z}^{(\ell+1)}\) that factorizes along \(\mathsf{T}\), _i.e._, for any \(\{j_{0},\ldots,j_{\ell}\}\) with for any \(i\in\{0,\ldots,\ell\}\), \(j_{i}\in\{0,\ldots,n\}\), we have

\[C_{j_{0},\ldots,j_{\ell}}=\sum_{(v,v^{\prime})\in\mathsf{E}}C_{j_{v},j_{v^{ \prime}}}^{\{v,v^{\prime}\}},\]

where \(C^{\{v,v^{\prime}\}}\in\mathsf{Z}^{(2)}\) is a cost matrix for transportation between the marginals at vertices \(v\) and \(v^{\prime}\), see (Haasler et al., 2021, Eq. (3.1)). In particular, this cost can be seen as the discrete counterpart of the tree-based cost introduced in (1) in the quadratic setting.

Given a subset \(\mathsf{S}\subset\mathsf{V}\) with \(|\mathsf{S}|=K\) and a set of marginals \(\{\mu_{i}\}_{i\in\mathsf{S}}\in\mathscr{P}^{K}\), Haasler et al. (2021) study the EOT problem associated to \(\mathsf{T}\), see (Haasler et al., 2021, Eq. (2.4)), which is given by

\[\operatorname{argmin}\{\langle C,M\rangle-\varepsilon\mathrm{H}(M):M\in \mathscr{P}^{(\ell+1)},\operatorname{proj}_{i}(M)=\mu_{i},\forall i\in \mathsf{S}\}\;.\] (discrete-EmOT)

This problem may be solved with Sinkhorn algorithm (Cuturi, 2013; Knight, 2008; Sinkhorn and Knopp, 1967), for which the authors provide an efficient implementation adapted to the tree-based setting, see (Haasler et al., 2021, Algorithm 3.1). Moreover, they state the convergence of their method in (Haasler et al., 2021, Theorem 3.5), as a direct consequence of the results presented in Luo and Tseng (1992).

In (Haasler et al., 2021, Section 4.2), it is assumed that \(\mathsf{S}\) corresponds to the set of the leaves of \(\mathsf{T}\), as we do, and it is shown an equivalence between (discrete-EmOT) and the discrete-state static SB problem stated in (Haasler et al., 2021, Eq 4.2), which is given by

\[\operatorname{argmin}\{\sum_{(v,v^{\prime})\in\mathsf{E}_{r}} \mathrm{KL}(M^{(v,v^{\prime})}\mid\operatorname{diag}(\nu_{v})A^{(v,v^{ \prime})}):\] (discrete-TreeSB) \[M^{(v,v^{\prime})}\in\mathscr{P}^{(2)},\{\nu_{v}\}_{v\in\mathsf{ V}}\in\mathscr{P}^{\ell+1},M^{(v,v^{\prime})}\mathbf{1}=\nu_{v},M^{(v,v^{ \prime})}{}^{\top}\mathbf{1}=\nu_{v^{\prime}},\;\nu_{i}=\mu_{i},\forall i\in \mathsf{S}\}\;,\]

where \(\mathsf{T}_{r}=(\mathsf{V},\mathsf{E}_{r})\) is the directed version of \(\mathsf{T}\) rooted in an arbitrary vertex \(r\in\mathsf{S}\), and \(A^{(v,v^{\prime})}=\exp(-C^{(v,v^{\prime})}/\varepsilon)\in\mathsf{Z}^{(2)}\) for any \((v,v^{\prime})\in\mathsf{E}_{r}\). Remark that \(A^{(v,v^{\prime})}\) may not necessarily be a transition probability matrix.

Finally, Haasler et al. (2021) provide two main numerical experiments. In (Haasler et al., 2021, Section 5.2), they consider a tree with 15 vertices, 14 edges and 8 leaves, combined to the state-space \(\mathsf{X}=\{0,1\}^{50\times 50}\), and solve the corresponding (discrete-EmOT) problem for the quadratic cost. In (Haasler et al., 2021, Section 6), they apply their methodology to estimate ensemble flows on a hidden Markov chain. Given \(\tau\in\mathbb{N}^{*}\), they consider a tree \(\mathsf{T}\) with \(\tau\) internal vertices (modeling the distribution of \(N\) agents at time \(t\in\{1,\ldots,\tau\}\)), that are linearly linked, and such that each of these vertices is independently linked to \(S\) leaves of \(\mathsf{T}\) (modeling observations at time \(t\in\{1,\ldots,\tau\}\)). In this setting, the state space is given by \(\mathsf{X}=\{1,\ldots,100\}^{N}\). They solve the formulation (discrete-TreeSB) where the reference measure is chosen as a random walk.

Comparison with our results.We now establish remarks on the main differences between our methodology and the work of Haasler et al. (2021).

First of all, the continuous state-space counterpart of (discrete-TreeSB) is given by

\[\operatorname{argmin}\{\mathrm{KL}(\pi\mid\pi^{0}):\pi\in\mathscr{P}_{ \mathsf{T}_{r}},\pi_{i}=\mu_{i},\forall i\in\mathsf{S}\}\;,\] (28)

where \(\pi^{0}\) is a reference measure which factorizes along \(\mathsf{T}_{r}\). In this case, \(\pi_{v,v^{\prime}}\), \(\pi_{v}\) and \(\pi_{v^{\prime}|v}^{0}\) in (28) respectively correspond to the continuous version of \(M^{(v,v^{\prime})}\), \(\nu_{v}\) and \(A^{(v,v^{\prime})}\) in (discrete-TreeSB). In contrast, our formulation of the multi-marginal Tree Schrodinger Bridge problem given in (TreeSB) is a minimization problem over all probability measures \(\pi\in\mathscr{P}^{(\ell+1)}\), and is not restricted to the distributions that admit a Markovian factorization along \(\mathsf{T}\) as in (28). Hence, our framework may be considered more general. Remark that under **A**1, **A**2 and **A**3, Proposition 3 states that (TreeSB) admits a unique solution \(\pi^{*}\ll\pi^{0}\) such that \((\mathrm{d}\pi^{*}/\mathrm{d}\pi^{0})\) can be written with potentials. Then, \(\pi^{*}\in\mathscr{P}_{\mathsf{T}_{r}}\) since \(\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\), and (TreeSB) is then equivalent to (28).

Furthermore, (EmOT) is more general than the continuous version of (discrete-EmOT), which we can recover by taking any measure \(\nu\) of the form \((\mathrm{d}\nu/\mathrm{d}\mathrm{Leb})=\exp[\bigoplus_{i\in\mathsf{S}}\varphi_{i}]\) in (EmOT), where \(\{\varphi_{i}\}_{i\in\mathsf{S}}\) is a family of potentials such that \(\big{|}\int_{\mathbb{R}^{d}}\varphi_{i}\mathrm{d}\mu_{i}\big{|}<\infty\) for any \(i\in\mathsf{S}\). As a consequence, our setting allows us to choose the root \(r\in\mathsf{V}\backslash\mathsf{S}\) for the SB problem, whereas Haasler et al. (2021) only consider the case where \(r\in\mathsf{S}\). In the latter case, we establish in Appendix E that \(r\) can be chosen arbitrarily, as stated by (Haasler et al., 2021, Corollary 4.3).

Finally, TreeDSB deeply differs from the framework of Haasler et al. (2021) due its _dynamic_ nature. Although we solve the same tree-based static SB problem (up to continuous/discrete state-space consideration), our approach consists in computing dynamic iterates (_i.e._, path measures) using diffusion-based methods instead of static iterates (_i.e._, distributions) using Sinkhorn algorithm. This paradigm is at the core of the DSB (De Bortoli et al., 2021) methodology, and offers an efficient approach to tackle high-dimensional settings, where Sinkhorn algorithm would fail.

Here, we present some advantages of the method proposed by Haasler et al. (2021) compared to ours. First, Haasler et al. (2021) may choose any kind of tree-based cost in practice, while our methodology only holds for the quadratic cost. This limitation is shared with all approaches based on the DSB (De Bortoli et al., 2021) methodology. Indeed, since the cost is determined by the reference path measure, we often choose quadratic costs associated with Brownian motions or Ornstein-Uhlenbeck processes. Moreover, Haasler et al. (2021) may consider various inhomogeneous (discrete) state spaces for the vertices of \(\mathsf{T}\), as presented in their numerical experiments. In our case, this approach is not compatible with our diffusion-based method. Finally, unlike Haasler et al. (2021), our method is not scalable with the number of vertices or edges in \(\mathsf{T}\) due to computational limits. This limitation is common to all multi-marginal approaches which rely on neural networks to parameterize the potential and/or the distributions of the multi-marginal OT method, see Li et al. (2020); Fan et al. (2020); Korotin et al. (2022, 2021) for instance.

## Appendix E Further results on TreeSB

Choice of the root \(r\) in (\(\mathrm{TreeSB}\)).We recall that the reference measure \(\pi^{0}\) considered in (TreeSB), which is defined in (2), verifies \(\pi^{0}\in\mathscr{P}_{\mathsf{T}_{r}}\) for some fixed root \(r\in\mathsf{V}\) and \(\pi^{0}_{r}\ll\mathrm{Leb}\) with density \(\varphi_{r}\). Moreover, we have \(\pi^{0}_{v^{\prime}|v}(\cdot\mid x_{v})=\mathrm{N}(x_{v},\varepsilon/(2w_{v,v^ {\prime}})\mathrm{I}_{d})\) for any \((v,v^{\prime})\in\mathsf{E}_{r}\), and thus, \(\pi^{0}\) is entirely determined by the choice of the root \(r\) and the density on the corresponding vertex \(\varphi_{r}\).

As presented in Appendix D.1, we recall that (TreeSB) is equivalent to any multi-marginal Tree-SB problem with a reference measure \(\bar{\pi}^{0}\) given by (15), _i.e._, \(\bar{\pi}^{0}\) writes as \((\mathrm{d}\bar{\pi}^{0}/\mathrm{d}\pi^{0})=\exp[\bigoplus_{i\in\mathsf{S}} \psi^{0}_{i}]\), where \(\{\psi^{0}_{i}\}_{i\in\mathsf{S}}\) is a family of measurable potentials with \(\psi^{0}_{i}:\mathbb{R}^{d}\to\mathbb{R}\) such that \(\big{|}\int_{\mathbb{R}^{d}}\psi^{0}_{i}\mathrm{d}\mu_{i}\big{|}<\infty\) for any \(i\in\mathsf{S}\). In the case where \(r\) is chosen as a leaf of \(\mathsf{T}\), this result implies that (TreeSB) is unchanged if

1. \(\varphi_{r}=\mathrm{d}\nu/\mathrm{d}\mathrm{Leb}\) where \(\nu\in\mathscr{P}\) is such that \(\mathrm{KL}(\mu_{r}|\nu)<\infty\),
2. \(r\) is replaced by \(r^{\prime}\in\mathsf{S}\), as long as \(\mathrm{H}(\mu_{r})<\infty\) and \(\mathrm{H}(\mu_{r^{\prime}})<\infty\).

Therefore, under \(\mathbf{A}0\), the setting chosen in Section 3 is equivalent to any other setting where \(r\) is arbitrarily chosen in \(\mathsf{S}\) and \(\varphi_{r}=\mathrm{d}\nu/\mathrm{d}\mathrm{Leb}\) where \(\mathrm{KL}(\mu_{r}|\nu)<\infty\).

Consider now the case where \(r\in\mathsf{S}^{c}\), _i.e._, \(r\) is not a leaf of \(\mathsf{T}\). Then, the choice of \(\varphi_{r}\) can not be made arbitrarily anymore, since it determines a further regularization on the \(r\)-th marginal of the solution to (TreeSB). In this setting, the sequence defined by (mIPF) is unchanged. Hence, TreeDSB proceeds in the same manner as presented in Section 3, except for the first iteration, which we detail now.

Let us define \(\mathsf{P}=\mathrm{path}_{\mathsf{T}_{i_{0}}}(i_{0},r)\), where \(\mathsf{T}_{i_{0}}=(\mathsf{V},\mathsf{E}_{i_{0}})\) is the directed version of \(\mathsf{T}\) rooted in \(i_{0}\). We recall that first iterate of (mIPF) is defined by

\[\pi^{1}=\mathrm{argmin}\{\mathrm{KL}(\pi\mid\pi^{0}):\pi\in\mathscr{P}^{(\ell+1 )},\pi_{i_{0}}=\mu_{i_{0}}\}\;.\]

Following the proof of Lemma 12, it is clear that

\[\pi^{1}=\mu_{i_{0}}\bigotimes_{(v,v^{\prime})\in\mathsf{P}}\pi^{0}_{v^{\prime}|v }\bigotimes_{(v,v^{\prime})\in\mathsf{E}_{i_{0}}\setminus\mathsf{P}}\pi^{0}_{v^ {\prime}|v}=\mu_{i_{0}}\bigotimes_{(v,v^{\prime})\in\mathsf{E}_{i_{0}}}\pi^{0}_{v^ {\prime}|v}\;,\]where we emphasize that \(\mathsf{P}=\{(v,v^{\prime})\in\mathsf{E}_{i_{0}}:(v^{\prime},v)\in\mathsf{E}_{r}\}\). Therefore, Proposition 2 still applies between \(r\) and \(i_{0}\), by considering \(r\) instead of \(i_{K-1}\). In practice, this means that the first iteration of TreeDSB consists in computing the time reversal of the path measures \(\mathbb{P}^{0}_{(v^{\prime},v)}\) for any \((v,v^{\prime})\in\mathsf{P}\).

Extension of the regularized Wasserstein barycenter problem (regWB).Consider the regularized Wasserstein-2 barycenter problem defined as follows

\[\mu_{e}^{\star}=\arg\min\{\sum_{i=1}^{\ell}w_{i}W_{2,\varepsilon/w_{i}}^{2}( \mu,\mu_{i})+\ell\varepsilon\mathrm{H}(\mu)+\varepsilon\mathrm{KL}(\mu\mid\mu _{0}):\mu\in\mathscr{P}\}\;,\qquad(\mu_{0}\text{-regWB})\]

where \((w_{i})_{i\in\{1,\ldots,\ell\}}\in(0,+\infty)^{\ell}\) and \(\mu_{0}\in\mathscr{P}\) is a reference measure. This formulation admits a further regularization compared to (regWB), which tends to make \(\mu_{e}^{\star}\) closer to \(\mu_{0}\). In particular, given a Wasserstein barycenter problem onto a star-shaped tree, the formulation (\(\mu_{0}\text{-regWB}\)) may be more adapted than (regWB) if we have an _a priori_ on the form of the regularized barycenter. In the case where \(\mu_{0}=\mathrm{N}(0,\sigma_{0}^{2}\mathrm{I}_{d})\), letting \(\sigma_{0}\to\infty\), we recover the \((\ell,(\ell-1)\varepsilon)\) doubly-regularized Wasserstein barycenter problem (regWB). In the same spirit as Proposition 7, we can derive the following result from Proposition 24, which proves that (\(\mu_{0}\text{-regWB}\)) can be solved with TreeDSB.

**Proposition 25**.: _Let \(\varepsilon>0\) and \(\mu_{0}\in\mathscr{P}\) such that \(\mu_{0}\ll\mathrm{Leb}\). Assume **A0**. Also assume that \(\mathsf{T}\) is a star-shaped tree with central node indexed by \(0\), and that the reference measure of (TreeSB) defined in (2) verifies \(r=0\) and \(\varphi_{r}=\mathrm{d}\mu_{0}/\mathrm{d}\mathrm{Leb}>0\). Under **A2**, (\(\mu_{0}\text{-regWB}\)) has a unique solution \(\pi_{0}^{\star}\), where \(\pi^{\star}\) is the solution to (TreeSB)._

Below, we provide practical guidelines to parameterize \(\mu_{0}\) when it is chosen as a Gaussian distribution.

Gaussian design of \(\mu_{0}\) in (\(\mu_{0}\text{-regWB}\)).Consider an undirected star-shaped tree \(\mathsf{T}\) with \(K+1\) vertices and leaves \(\{1,\ldots,K\}\). In order to incorporate the marginal constraints in the penalization brought by \(\mu_{0}\) when it is a Gaussian distribution, we set its mean to \(\sum_{i=1}^{K}\mathbb{E}[\mu_{i}]/K\) and its diagonal covariance matrix as \(\alpha\times(\sum_{i=1}^{K}\mathrm{diag}(\mathrm{Cov}[\mu_{i}])^{-1}/K)^{-1}\), where the inverse operation is component-wise and \(\alpha\) is a positive hyperparameter. This choice of variance helps to correctly explore the state-space at the very first iteration of TreeDSB, which is key to ensure numerical stability. In this setting, (TreeSB) verifies **A2** and **A3**, by Proposition 15 and Proposition 16. In particular, we use this approach for two of our experiments: synthetic Gaussian datasets and Bayesian fusion, see Appendix G.

## Appendix F Algorithmic techniques

Time discretization in TreeDSB.Denote \(k_{n}=(n-1)\bmod(K)\) for any \(n\in\mathbb{N}\). Let \(\mathsf{T}=(\mathsf{V},\mathsf{E})\) be a weighted undirected tree and consider the multi-marginal Schrodinger bridge problem (TreeSB) associated to this tree. We recall that for any \(\{v,v^{\prime}\}\in\mathsf{E}\), we define \(T_{v,v^{\prime}}=\varepsilon/(2w_{v,v^{\prime}})\).

Consider the path measures \(\{\mathbb{P}^{n}_{(v,v^{\prime})}\}_{n\in\mathbb{N},(v,v^{\prime})\in\mathsf{ E}_{k_{n}}}\) recursively defined by (a) and (b). By combining Proposition 1, Proposition 2 and results on time reversal theory (Haussmann & Pardoux, 1986), we obtain by recursion that for any \(n\in\mathbb{N}\), any \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\), \(\mathbb{P}^{n}_{(v,v^{\prime})}\) is associated with a Stochastic Differential Equation on \([0,T_{v,v^{\prime}}]\) given by

\[\mathrm{d}\mathbf{X}_{t}=f^{n}_{t,v,v^{\prime}}(\mathbf{X}_{t})\mathrm{d}t+ \mathrm{d}\mathbf{B}_{t},\quad\mathbf{X}_{0}\sim\pi_{v}^{n}\;.\] (29)

Let \(N\in\mathbb{N}^{*}\). In order to sample from the dynamics (29) at iteration \(n\in\mathbb{N}\), we consider its Euler-Maruyama discretization on \((N+1)\) time steps,

\[X_{m+1}=X_{m}+\gamma_{m+1}f^{n}_{t_{m},v,v^{\prime}}(X_{m})+\sqrt{\gamma_{m+1}} Z_{m+1},\quad X_{0}\sim\pi_{v}^{n}\;,\] (30)

where \(Z_{m}\sim\mathrm{N}(0,\mathrm{I}_{d})\) for any \(m\in\{1,\ldots,N\}\), \(t_{m}=\sum_{i=1}^{m}\gamma_{i}\), and \(\{\gamma_{m}\}_{m=1}^{N}\in(0,\infty)^{N}\) is a time schedule such that \(\sum_{m=1}^{N}\gamma_{m}=T_{v,v^{\prime}}\). This results in approximating the path measure \(\mathbb{P}^{n}_{(v,v^{\prime})}\) by the joint distribution \(\pi_{(v,v^{\prime})}^{n,N}\in\mathscr{P}^{(N+1)}\) defined by

\[\pi_{(v,v^{\prime})}^{n,N}=\pi_{v}^{n}\bigotimes_{m=0}^{N-1}\pi_{(v,v^{\prime} ),m+1|m}^{n,N}\;,\]

where \(\pi_{(v,v^{\prime}),m+1|m}^{n,N}(\cdot|x_{m})=\mathrm{N}(x_{m}+\gamma_{m+1}f^{ n}_{t_{m},v,v^{\prime}}(x_{m}),\gamma_{m+1}\mathrm{I}_{d})\) for any \(m\in\{0,\ldots,N-1\}\). If \(N\) is chosen large enough, then \(\pi_{(v,v^{\prime}),m}^{n,N}\) and \(\mathbb{P}^{n}_{(v,v^{\prime}),t_{m}}\) have approximately the same distribution for any \(m\in\{0,\ldots,N\}\). Consequently, \((\mathbb{P}_{(v,v^{\prime})}^{n})^{R}\) is naturally approximated by the joint distribution \(\tilde{\pi}_{(v,v^{\prime})}^{n,N}\in\mathscr{P}^{(N+1)}\) defined by

\[\tilde{\pi}_{(v,v^{\prime})}^{n,N}=\pi_{v}^{n}\bigotimes_{m=0}^{N-1}\pi_{(v,v^ {\prime}),N-m-1|N-m}^{n,N}\.\]

If \(N\) is chosen large enough, we obtain that

\[\pi_{(v,v^{\prime}),N-m-1|N-m}^{n,N}(\cdot|x_{N-m})\] \[\qquad=\mathrm{N}(x_{N-m}-\gamma_{N-m}f_{t_{N-m},v,v^{\prime}}^{n +1}(x_{N-m})+\gamma_{N-m}\nabla\log p_{v,v^{\prime},t_{N-m}}(x_{N-m}),\gamma_{ N-m}\mathrm{I}_{d})\,\]

where \(p_{v,v^{\prime},t}\) is the density of \(\mathbb{P}_{(v,v^{\prime}),t}^{n}\) w.r.t. the Lebesgue measure.

Following the construction of our dynamic iterates, we now explain how the sequence \(\{\pi_{(v,v^{\prime})}^{n}\}_{n\in\mathbb{N}^{*},(v,v^{\prime})\in\mathsf{E}_ {k_{n}}}\) is recursively defined. Let \(n\in\mathbb{N}\), \(k_{n}=(n-1)\bmod(K)\). Define the path \(\mathsf{P}_{n}=\mathrm{path}_{\Gamma_{i_{k}}}(i_{k_{n}},i_{k_{n}+1})\). Then, for any \((v,v^{\prime})\in\mathsf{E}_{k_{n}+1}\),

1. if \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\backslash\mathsf{P}_{n}\), then \(\pi_{(v,v^{\prime})}^{n+1,N}=\pi_{v}^{n+1}\bigotimes_{m=0}^{N-1}\pi_{(v,v^{ \prime}),m+1|m}^{n,N}\),
2. if \((v^{\prime},v)\in\mathsf{P}_{n}\), then \(\pi_{(v,v^{\prime})}^{n+1,N}=\pi_{v}^{n+1}\bigotimes_{m=0}^{N-1}\pi_{(v^{ \prime},v),N-m-1|N-m}^{n,N}\).

These computations may be obtained by considering the sequence given by (mIPF) to solve the multi-marginal Tree-SB problem associated to \(\mathsf{T}^{(N)}=(\mathsf{V}^{(N)},\mathsf{E}^{(N)})\), the \(N\)-discretized version of \(\mathsf{T}\) (see Appendix B) with weights \(w_{e_{m}}^{(N)}=2\gamma_{m}/\varepsilon\), which is given by

\[\pi^{*}=\mathrm{argmin}\{\mathrm{KL}(\pi|\pi^{0,N})\,:\,\pi\in\mathscr{P}^{( \mathsf{V}^{(N)})},\,\,\pi_{i}=\mu_{i}\,\forall i\in\mathsf{S}\}\,\]

with \(\pi^{0,N}=\pi_{r}^{0}\bigotimes_{(v,v^{\prime})\in\mathsf{E}_{r}}\pi_{(v,v^{ \prime}),1:N|0}^{0,N}\).

To approximate the IPF recursion given by (a) and (b), we use **on each edge** of \(\mathsf{T}\) the score-matching approach of De Bortoli et al. (2021), which avoids heavy computations of score approximations. The next proposition is direct adaptation of (De Bortoli et al., 2021, Proposition 3).

**Proposition 26**.: _Assume that for any \(n\in\mathbb{N}\), any \((v,v^{\prime})\in\mathsf{E}_{k_{n}}\) with \(k_{n}=(n-1)\bmod(K)\), we have_

\[\pi_{(v,v^{\prime}),m+1|m}^{n,N}(\cdot|x_{m})=\mathrm{N}(F_{m,v,v^{\prime}}^{ n}(x_{m}),\gamma_{m}\mathrm{I}_{d})\.\]

_Let \(n\in\mathbb{N}\). Consider the path \(\mathsf{P}_{n}=\mathrm{path}_{\Gamma_{i_{k_{n}}}}(i_{k_{n}},i_{k_{n}+1})\). Let \((v,v^{\prime})\in\mathsf{E}_{k_{n}+1}\). Define \(p^{n}=\pi_{(v,v^{\prime})}^{n,N}\) and \(m_{N}=N-m-1\). Then, if \((v^{\prime},v)\in\mathsf{P}_{n}\), we have_

\[F_{m,v,v^{\prime}}^{n+1}=\mathrm{argmin}_{\mathsf{F}\in\mathsf{L }^{2}(\mathbb{R}^{d},\mathbb{R}^{d})}\] (31) \[\quad\mathbb{E}_{p^{n}_{m_{N},m^{N+1}}}[\|\mathrm{F}(X_{m_{N}+1}) -(X_{m_{N}+1}+F_{m_{N},v^{\prime},v}^{n}(X_{m_{N}})-F_{m_{N},v^{\prime},v}^{ n}(X_{m_{N}+1}))\|^{2}],\]

_otherwise, we have \(F_{m,v,v^{\prime}}^{n+1}=F_{m,v,v^{\prime}}^{n}\)._

In practice, we use two neural networks per edge \(\{v,v^{\prime}\}\in\mathsf{E}\), one for each possible direction of the edge, such that \(F_{v,v^{\prime}}(\theta_{v,v^{\prime}}^{n},m,x)\approx F_{m,v,v^{\prime}}^{n}(x)\) and \(F_{v^{\prime},v}(\theta_{v^{\prime},v}^{n},m,x)\approx F_{m,v^{\prime},v}^{n}(x)\). For any \(\{v,v^{\prime}\}\in\mathsf{E}\), the parameter \(\theta_{v,v^{\prime}}^{n}\) is updated at iteration \(n\) via the score matching loss defined by (31) in Proposition 26 if \((v,v^{\prime})\in\mathrm{path}_{T_{i_{k_{n}}}}(i_{k_{n}},i_{k_{n}+1})\), see Algorithm 1.

## Appendix G Additional experimental results and details

The numerical experiments presented in Section 7 are obtained by our own Pytorch implementation, which is inspired from the code6 provided by De Bortoli et al. (2021). We first provide information on the general setting of our experiments in Appendix G.1, and then give details on each of them in Appendix G.2 along with additional results. We recall that a mIPF cycle is defined as a subset of \(K\) consecutive iterations of (mIPF) and that the order of the leaves given by \(\{i_{0},\ldots,i_{K-1}\}\) is randomly shuffled at each new mIPF cycle.

### General experimental setup

Implementation of Algorithm 1 in practice.Let \(n\in\mathbb{N}\), with \(k_{n}=(n-1)\bmod(K)\), \(k_{n}+1=n\bmod(K)\). Consider the path \(\mathsf{P}_{n}=\operatorname{path}_{\Gamma_{i_{k_{n}}}}(i_{k_{n}},i_{k_{n}+1})\). Assume that we are provided with a dataset \(\mathsf{D}_{i_{k_{n}}}\), which contains \(M\) samples from \(\pi^{n}_{i_{k_{n}}}\). Following Lines 7-9 in Algorithm 1, we apply processes (a) and (b) recursively on the edges \((v,v^{\prime})\in\mathsf{P}_{n}\).

1. Sampling step (Line 7). For any \(x_{0}\in\mathsf{D}_{v}\), we sample from the diffusion trajectory (30) given by the Euler Maruyama discretization of \(\mathbb{P}^{n}_{v,v^{\prime}}\) starting from \(x_{0}\). This gives us \(M\times N\) trajectory samples. We then store the last iterate of each trajectory in a new dataset \(\mathsf{D}_{v^{\prime}}\), which thus approximates \(\pi^{n}_{v^{\prime}}\).
2. Training step (Lines 8-9). In order to avoid heavy computation, we approximate the _mean-matching_ loss (31) by an unbiased estimator obtained by subsampling \(b\) elements from the _full_ trajectories computed in the sampling process, see (De Bortoli et al., 2021, Eq. (97)-(98)). Here, \(b\) refers to the _batch-size_ parameter of the neural networks. Then, we perform gradient descent to optimize the parameter \(\theta_{v^{\prime},v}\), which parameterizes the _backward_ drift on the edge \((v,v^{\prime})\).

To avoid any bias issue, the whole trajectories obtained at process (a) are refreshed at a certain frequency over the training iterations of the neural networks by once again simulating the diffusion (30). In our experiments, this refresh occurs each 500 iterations.

Setting of the time discretization.The number of time-steps \(N\) in the time discretization of the diffusions is chosen to be even and identical for each of the edges of the tree. Let \(\{v,v^{\prime}\}\in\mathsf{E}\). We now give details on the design of the time schedule \(\{\gamma_{k}\}_{k=1}^{N}\) related to the edge \(\{v,v^{\prime}\}\), see Appendix F. Following De Bortoli et al. (2021), we choose this sequence to be invariant by time reversal and consider \(\gamma_{k}=\gamma_{0}+(2k/N)(\bar{\gamma}-\gamma_{0})\) for any \(k\in\{0,\dots,N/2\}\) (the rest of the sequence being obtained by symmetry) where \(\gamma_{0}\) is a free parameter and \(\bar{\gamma}\) is determined by \(\sum_{k=1}^{N}\gamma_{k}=T_{v,v^{\prime}}\). In our experiments, we set \(N=50\) and \(\gamma_{0}=10^{\text{-}5}\).

Sampling improvement.In our code, we implemented the corrector scheme of Song et al. (2021) and the _probability flow_-based sampling approach detailed in (De Bortoli et al., 2021, Section H.3), but did not observe any significant improvement in our experiments using one of these techniques.

Choice of the architectures of the neural networks.In the case of the experiments related to synthetic datasets (two-dimensional toy datasets, Gaussian distributions) and to the subset posterior aggregation task, we implement the same architecture as presented in (De Bortoli et al., 2021, Figure 3). We refer to this model as "Basic Model" and detail it in Figure 6. In the "Basic Model", the Positiona-lEncoding block applies the sine transform described in Vaswani et al. (2017), with output dimension equal to 32, and each MLP Block represents a Multilayer Perceptron Network. In particular, MLPBlock (1a) has shape \((d,128,\max(256,2d))\), MLPBlock (1b) has shape \((32,128,\max(256,2d))\), and MLPBlock (2) has shape \((2\times\max(256,2d),\max(256,2d),\max(128,d),d)\), where \(d\) denotes the dimension of input data. We optimize the networks with ADAM (Kingma & Ba, 2014) with learning rate \(10^{-4}\) and momentum \(0.9\). For each of the networks, we set the batch size to 4,096 and the number of iterations to 10,000 for the synthetic datasets and 15,000 for the subset posterior aggregation task. Our experiments ran on 1 Intel Xeon CPU Gold 6230 20 cores @ 2.1 Ghz CPU.

In the case of the experiments related to MNIST dataset, we use a reduced UNET architecture based on Nichol & Dhariwal (2021), where we set the number of channels to 64 rather than 128. We implement an exponential moving average of network parameters across training iterations, with rate 0.999. We optimize the networks with ADAM (Kingma & Ba, 2014) with learning rate \(10^{-4}\) and momentum \(0.9\). Finally, we set the batch size to 256 and the number of training iterations to 30,000. Our experiments ran using 1 Nvidia A100.

Figure 6: Architecture of the “Basic Model”.

Details on regularized state-of the art methods.We run the fsWB algorithm (Cuturi and Doucet, 2014) with the implementation provided by Flamary et al. (2021). For each experiment, we run 100 Sinkhorn iterations with 1500 samples for each dataset (_i.e._, the maximum number of samples that it can generate) and set the regularization parameter \(\varepsilon\) to its lowest value such that the algorithm is stable. Finally, for sake of fairness with our method, we initialise the barycenter measure with \(\pi_{r}^{0}\) when solving the problem (\(\mu_{0}\)-regWB) for synthetic Gaussian datasets and Bayesian fusion. To run the crWB algorithm (Li et al., 2020), we use the code provided by the authors. We consider the quadratic regularization, which is shown to be empirically more stable than entropic regularization. Following Fan et al. (2020), we choose the potential networks to be fully connected neural networks with 3 hidden layers of shape \((\max(128,2d),\max(128,2d),\max(128,2d))\). The activation functions are ReLu. We optimize the networks with ADAM (Kingma and Ba, 2014) with learning rate \(10^{-4}\) for the subset posterior aggregation task and \(10^{-3}\) for the Gaussian experiment. Finally, we set the batch size to 4,096 and the number of training iterations to 50,000. We highlight that fsWB and crWB solve a regularized Wasserstein barycenter problem, which does not contain an additional _penalization_ term on the entropy of the barycenter, contrary to TreeDSB.

### Details on the experiments

Synthetic Gaussian datasets.For each dimension that we consider, we generate three different triplets of random non-diagonal covariance matrices whose condition number is less than 10. We then run the algorithms on each triplet and aggregate the obtained results. The Gaussian datasets contain 1,500 samples for fsWB, and 10,000 samples for crWB and TreeDSB. We run fsWB with the following settings \((d,\varepsilon)\in\{(2,0.1),(16,0.2),(64,0.5),(128,1.0),(256,2.0)\}\). We run TreeDSB for 10 mIPF cycles with regularization parameter \(\varepsilon=0.1\), starting from the central node initialized to a Gaussian distribution \(\mu_{0}\) chosen as detailed in Appendix F with \(\alpha=1\). Thus, we solve the regularized Wasserstein barycenter problem (\(\mu_{0}\)-regWB), which contains an additional regularization with respect to \(\mu_{0}\). This choice is justified, since the non-regularized barycenter is known to be a Gaussian distribution, and \(\mu_{0}\) can be seen as an _a priori_ for the regularized barycenter. For each of the three settings, we keep the best result among the \(30\) mIPF iterations. In this setting, TreeDSB and crWB have roughly the same training time.

Subset posterior aggregation.When considering a dataset splitted into several subdatasets, a common paradigm in bayesian inference consists in running Monte Carlo Markov Chain methods separately on these subdatasets, and then merge the obtained posteriors to recover the full posterior. The barycenter of these subdataset posteriors is proved to be close to the full data posterior under mild assumptions (Srivastava et al., 2018). In our setting, we consider the posterior aggregation problem for the logistic regression model associated to the wine dataset7 (d = 42) with 3 subdatasets. We consider here two splitting methods: (i) either, data is uniformly splitted between 3 subdatasets with respect to the label distribution, denoted by wine-homogeneous, or (ii) data is splitted with some heterogeneity according to a Dirichlet distribution whose parameter is randomly chosen, denoted by wine-heterogeneous. Following Korotin et al. (2021), we use the stochastic approximation trick so that the subset posterior samples do not vary consistently from the full posterior in covariance (Minsker et al., 2014). We implement the Unadjusted Langevin Algorithm (ULA) to sample from each subdataset posterior and from the full posterior. In each case, we run ULA for \(5.5\cdot 10^{6}\) iterations with a well chosen step-size, and obtain 9,900 samples after applying a _burn-in_ of order \(10\%\) and then a _thinning_ of size 500. We provide in Figure 7 some metrics which assess the quality of this sampling process. We recall that the the full posterior samples serve as ground truth in this experiment.

Footnote 7: https://archive.ics.uci.edu/ml/datasets/wine

The results presented in Table 2 were computed as follows. For fsWB, we first subsample 1,500 samples out of the 9,900 samples from each posterior, and then run the algorithm with \(\varepsilon=0.5\). We repeat three times this procedure and then aggregate the results. In the case of crWB and TreeDSB, we run the algorithms three times with various seeds. Similarly to the Gaussian setting, we run TreeDSB for 10 mIPF cycles with regularization parameter \(\varepsilon=0.1\). We start from the central node with a Gaussian distribution \(\mu_{0}\) chosen as detailed in Appendix F with \(\alpha=1\), and thus solve the barycenter formulation (\(\mu_{0}\)-regWB). For each of the three settings, we keep the best result among the \(30\) IPF iterations. In this setting, TreeDSB and crWB have roughly the same training time.

Synthetic two-dimensional datasets.In this setting, we consider three different datasets (_Swiss-roll_, _Circle_ and _Moons_) that each contain 10,000 samples. Since we do not have an _a priori_ on the shape of the barycenter between these datasets, we consider the regularized Wasserstein barycenter problem (regWB), _i.e._, \(r\) is chosen as a leaf and corresponds to one of the input datasets. We emphasize that this experiment is not intended to demonstrate the superiority of TreeDSB to compute 2D Wasserstein barycenters, but is rather meant to illustrate that (a) the marginals of the leaves are well recovered by the algorithm, see Figure 3, and that (b) the obtained barycenter is consistent when diffusing from the different leaves, see Figure 4. In all our experiments on 2D datasets, we observed that (a) was persistently verified without difficulty. In this section, we rather aim at illustrating (b) by providing additional results which assess the quality of the barycenter obtained by TreeDSB with respect to the choice of the starting leaf \(r\) and to the choice of the regularization parameter \(\varepsilon\).

To do so, we consider three different choices of regularization in TreeDSB: (i) \(\varepsilon=0.2\) (50 mIPF cycles), see Figure 8, (ii) \(\varepsilon=0.1\) (50 mIPF cycles), see Figure 9 and (iii) \(\varepsilon=0.05\) (60 mIPF cycles), see Figure 10. For each of these settings, we run TreeDSB with the starting leaf \(r\) chosen as _Swiss-roll_ (first row), _Circle_ (second row) or _Moons_ (third row), and display the final barycenter obtained by diffusing from _Swiss-roll_ (first column), _Circle_ (second column) and _Moons_ (third column). Note that the vertex 0 always corresponds to the starting leaf, the vertex 1 to the barycenter node and that Figure 4 corresponds to the first row of Figure 9.

We can make the following observations. First, the estimated barycenter is always coherent within each row, which assesses the convergence of our method. Then, for each value of \(\varepsilon\), the TreeDSB barycenter is rather consistent between the rows, _i.e._, the choice of the starting leaf does not have a meaningful impact on our method. Finally, as expected, we observe that the support of the barycenter is less and less diffuse as long as \(\varepsilon\) decreases.

Figure 7: Evaluation of the sampling process for wine-homogeneous (left) and wine-heterogeneous (right). We display the Autocorrelation function on 500 lags (above) and the evolution over the iterations of ULA of the negative log-likelihood (NLL) evaluated on each training dataset (below). In particular, the samples are decorrelated and the NLL has a satisfying profile.

Figure 8: Estimated 2D barycenter obtained by TreeDSB with \(\varepsilon=0.2\) (50 mIPF cycles). First row: starting from _Swiss-roll_. Second row: starting from _Circle_. Third row: starting from _Moons_.

Figure 9: Estimated 2D barycenter obtained by TreeDSB with \(\varepsilon=0.1\) (50 mIPF cycles). First row: starting from _Swiss-roll_. Second row: starting from _Circle_. Third row: starting from _Moons_.

For purpose of illustration, we provide in Figure 11 the barycenter obtained by state-of-the-art two-dimensional _in-sample_ methods that are available in POT library (Flamary et al., 2021): (i) non-regularized free-support Wasserstein barycenter (Cuturi & Doucet, 2014), (ii) entropic-regularized free-support Wasserstein barycenter (fsWB) with \(\varepsilon=0.5\)(Cuturi & Doucet, 2014) and (iii) entropic-regularized convolutional Wasserstein barycenter with \(\varepsilon=5.10^{-4}\)(Solomon et al., 2015), which is specifically designed for images. We notably observe that TreeDSB cannot capture the full complexity of the 2D barycenter compared to these methods. We infer that this gap comes from the _dynamic_ nature of TreeDSB, since increasing the number of training iterations per IPF iteration or improving the complexity of the neural networks did not bring any significant change in our results. Finally, we recall that the methods (i), (ii) and (iii) do not scale well with dimension, and have to be completely run again when new data samples are available, contrary to TreeDSB.

Figure 11: Estimated 2D barycenter obtained by _in-sample_ algorithms. From left to right: Cuturi & Doucet (2014) (non-regularized), Cuturi & Doucet (2014) (regularized), Solomon et al. (2015).

Figure 10: Estimated 2D barycenter obtained by TreeDSB with \(\varepsilon=0.05\) (60 mIPF cycles). First row: starting from _Swiss-roll_. Second row: starting from _Circle_. Third row: starting from _Moons_.

MNIST Wasserstein barycenter.This setting can be qualified as _high-dimensional_, since the data dimension is \(d=784\). Here, each digit dataset contains 1,000 samples. As in the two-dimensional setting, we do not have an _a priori_ on the shape of the barycenter between MNIST digits, and thus consider the formulation (regWB), where the root \(r\) is chosen as a leaf. We propose below several experiments to assess the scalability of TreeDSB to this setting.

Digits 0 and 1.In Figure 12, we report the results obtained by running TreeDSB on MNIST digits 0 and 1, for 15 mIPF cycles with \(\varepsilon=0.5\), starting from the leaf MNIST-0. We display 25 samples from the reconstructed MNIST-0 marginal (first column), from the reconstructed MNIST-1 marginal (fourth column), from the estimated barycenter by diffusing from MNIST-0 (second column) and diffusing from MNIST-1 (third column). We notably observe that the digits are well recovered and that the barycenter samples are consistent. We draw the reader's attention to the fact that TreeDSB showed numerical instability with a regularization value \(\varepsilon\) lower than \(0.5\). For purpose of illustration, we display in Figure 13 the Wasserstein barycenter obtained by _non-regularized_ methods from Fan et al. (2020) and Korotin et al. (2021), and by the _regularized_ approach from Li et al. (2020).

Digits 2,4 and 6.In Figure 14, we report the results obtained by running TreeDSB on MNIST digits 2,4 and 6, for 10 mIPF cycles with \(\varepsilon=0.5\). Here, we consider three settings which differ by the starting leaf \(r\) in the algorithm: MNIST-2 (first row), MNIST-4 (second row), or MNIST-6 (third row). For each of these settings, we display 30 samples from the estimated barycenter by diffusing from MNIST-2 (first column), diffusing from MNIST-4 (third column) and diffusing from MNIST-6 (third column). We notably observe a global consistency of the barycenter samples across the various settings. In Figure 15, we report the results obtained by running TreeDSB on MNIST digits 2,4 and 6, for 10 mIPF cycles with \(\varepsilon=0.2\), starting from MNIST-6. We display 30 samples from the reconstructed marginals (first row), from the estimated barycenter (second row) by diffusing from MNIST-2 (first column), diffusing from MNIST-4 (second column) and diffusing from MNIST-6 (third column). As expected, we observe less noisy barycenter samples compared to Figure 14, while still recovering MNIST digits.

Digits 0,1 and 4.In Figure 16, we report the results obtained by running TreeDSB on MNIST digits 0,1 and 4, for 10 mIPF cycles with \(\varepsilon=0.5\). We consider two settings which differ by the starting leaf \(r\) in the algorithm: MNIST-0 (second row) and MNIST-1 (first/third rows), for which we display samples from the reconstructed measures (first row). In Figure 17, we report the results obtained by running TreeDSB on MNIST digits 0,1 and 4, for 10 mIPF cycles with \(\varepsilon=0.2\). We consider two settings which differ by the starting leaf \(r\) in the algorithm: MNIST-0 (first/second row), for which display samples from the reconstructed measures (first row), and MNIST-1 (third row). For all of these settings, we display 30 samples from the estimated barycenter by diffusing from MNIST-0 (first column), diffusing from MNIST-1 (third column) and diffusing from MNIST-4 (third column). Similarly to the digits 2-4-6, we observe consistency within the barycenter samples, unconditionally to the starting leaf, and less noise as \(\varepsilon\) decreases. Note that the reconstructed MNIST digits are less truthful to the original datasets when \(\varepsilon\) is low.

Figure 12: Tree DSB results for MNIST digits 0 and 1, after 15 mIPF cycles with \(\varepsilon=0.5\).

Figure 13: From left to right: Fan et al. (2020), Korotin et al. (2021) and Li et al. (2020).

Figure 14: Tree DSB results for MNIST digits 2,4 and 6, after 10 mIPF cycles with \(\varepsilon=0.5\). First row: starting from MNIST-2. Second row: starting from MNIST-4. Third row: starting from MNIST-6.

Figure 15: Tree DSB results for MNIST digits 2,4 and 6, after 10 mIPF cycles with \(\varepsilon=0.2\), starting from MNIST-6. First row: samples from the reconstructed marginals. Second row: samples from the estimated barycenter.

Figure 16: Tree DSB results for MNIST digits 0,1 and 4, after 10 mIPF cycles with \(\varepsilon=0.5\). First row: samples from the reconstructed marginals, starting from MNIST-1. Second row: samples from the estimated barycenter, starting from MNIST-0. Third row: samples from the estimated barycenter, starting from MNIST-1.

Figure 17: Tree DSB results for MNIST digits 0,1 and 4, after 10 mIPF cycles with \(\varepsilon=0.2\). First row: samples from the reconstructed marginals, starting from MNIST-0. Second row: samples from the estimated barycenter, starting from MNIST-0. Third row: samples from the estimated barycenter, starting from MNIST-1.