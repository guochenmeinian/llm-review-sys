# Topological obstruction to the training of shallow ReLU neural networks

Marco Nurisso

Politecnico di Torino & CENTAI Institute

Torino, 10100 - ITALY

marco.nurisso@polito.it

Pierrick Leroy

Politecnico di Torino

Torino, 10100 - ITALY

pierrick.leroy@polito.it

Francesco Vaccarino

Politecnico di Torino

Torino, 10100 - ITALY

francesco.vaccarino@polito.it

###### Abstract

Studying the interplay between the geometry of the loss landscape and the optimization trajectories of simple neural networks is a fundamental step for understanding their behavior in more complex settings. This paper reveals the presence of topological obstruction in the loss landscape of shallow ReLU neural networks trained using gradient flow. We discuss how the homogeneous nature of the ReLU activation function constrains the training trajectories to lie on a product of quadric hypersurfaces whose shape depends on the particular initialization of the network's parameters. When the neural network's output is a single scalar, we prove that these quadrics can have multiple connected components, limiting the set of reachable parameters during training. We analytically compute the number of these components and discuss the possibility of mapping one to the other through neuron rescaling and permutation. In this simple setting, we find that the non-connectedness results in a topological obstruction, which, depending on the initialization, can make the global optimum unreachable. We validate this result with numerical experiments.

## 1 Introduction

Training a neural network consists of navigating the complex geometry of the loss landscape to reach one of its deepest valleys. Gradient descent and its variants are, by far, the most commonly used algorithms to perform this task. While technically correct, the standard picture of the parameter space as Euclidean space with the trajectory rolling down the loss's surface in the steepest direction towards a minimum is slightly misleading because different choices of parameters can be _observationally equivalent_ i.e. encode the same function [10]. The observational equivalence of parameters shape the loss landscape by imposing specific geometric structures on the parameter space. Minima are not isolated points but high-dimensional manifolds with complex geometry [17; 9; 42] and the loss function's gradients and Hessian are constrained to obey some specific laws [46; 27]. Gradient-based optimization methods, where the parameters are updated by performing discrete steps in the gradient's direction, are thus very much dependent on the symmetry-induced geometry [12; 29].

In this work, we provide a topological perspective on the constraints induced by some groups of network symmetries on the optimization trajectories. Topology is a field of mathematics that studies the properties of a space that are preserved under continuous deformations. Our main goal is to find and quantify in topological terms the impossibility of the training trajectories to freely explore the parameter space and get from any initialization to an optimal parameter. This idea is formalized inthe topological notion of _connectedness_ and, in particular, with the 0-th _Betti number_, which counts the number of _connected components_ the space is composed of. The presence, or the absence, of topological _obstructions_ in the parameter space does not depend on the particular loss function or the training data but is intrinsic to the interplay between the geometry and the topology of the parameter space under the action of groups of symmetries inducing observationally equivalent networks.

Main contributions.Our main contributions are the following.

1. We find that, for two-layer neural networks, the gradient flow trajectories lie on an invariant set, which can be factored as the product of quadric hypersurfaces.
2. We analytically compute its Betti numbers, i.e., the number of connected components, holes, and higher-dimensional cavities.
3. We find that the invariant set can be disconnected when the network's output dimension is 1, leading to a clear topological obstruction.
4. We find that the obstruction is caused by "pathological" neurons that cannot change the sign of their output weights when trained with gradient flow.
5. We discuss the relation between the invariant set and the network's symmetries, finding that if we consider permutations, the number of effective connected components scales linearly in the number of pathological neurons.
6. We perform numerical validations on controlled toy scenarios, displaying the effect of obstruction in practice.

## 2 Related work

A large body of work studies gradient flow and gradient descent optimization of one hidden layer networks with homogeneous activations. Convergence properties have been found for wide networks [37; 43] with bounded density at initialization [31]. The implicit regularization provided is studied under various assumptions on: orthogonal input data [4], initialization scale [30; 4], wide (overparameterized regime) and infinitely wide [6], linearly separable data [30; 45]. Deeper linear networks [24] have also been studied.

These works focus on proving convergence and understanding which (optimal) solution is found, whereas our work investigates the shape of the optimization space and focuses on cases where the optimum might not be reachable from a given initialization.

Closer to our work, Safran et al. [39] studies two-layer ReLU binary classifiers with single input and output, counting the number of their piecewise-linear components after training. Eberle et al. [13] focuses on the differential challenge posed by the ReLU activation function and studies properties like the uniqueness of the solution of a gradient flow differential equation for a given initialization.

ReLU activation is a nonnegative homogeneous function, meaning that particular weight rescalings do not change the neural network's function. This is at the heart of the counterargument to flatness measures made by Dinh et al. [10], which shows that the Hessian eigenvalues can be made arbitrarily large in this way. Neyshabur et al. [34] explores the effect such rescalings can have on the gradient, proposing a rescaling-invariant regularization, and Pittorino et al. [36] employs them to define invariant flatness measures. Generally speaking, neural networks possess symmetries [20], and symmetries influence the geometry of training. Du et al. [12] studies how symmetry leads ReLU networks to automatically balance the neurons' weights. Kunin et al. [27], Zhao et al. [51] study how it constrains the gradient and Hessian matrix, leading to conservation laws w.r.t. gradient flow and Tanaka et al. [46] leverages it to propose a network pruning scheme. Ziyin [52] studies general mirror-reflect symmetries of the loss function and their effect on the weights of the trained network. Other conserved quantities stem from batch normalization's scale invariance [23; 47]. The transition from gradient flow to finite step size gradient descent breaks the conservation laws, resulting in altered trajectories [14; 2; 27; 44].

Numerous works have explored the geometry and topology of the loss landscape to obtain insight into a neural network's training behavior. Motivated by the striking experimental observation that low loss points can be connected by simple curves [11; 18] or line segments [40; 16; 15], a large body of literature tries to understand this phenomenon of mode connectivity under the topological lens of the connectedness of the loss function's sublevel sets [17; 35; 26], especially for overparameterized neural networks [9; 8; 42]. Another line of work approaches the connectivity of minima from another point of view, studying the presence [50; 38; 48] or absence [28] of spurious minima, i.e. minima which are not global. Bucarelli et al. [5] analytically derives bounds on the sum of the Betti numbers of the loss landscape's sublevel set. Topological data analysis methods have also been exploited to numerically study the shape of the loss landscape [1; 22].

## 3 Setup and preliminaries

### One-hidden layer neural network

Unless otherwise stated, all vectors are column vectors, that is, \(x=(x_{1},\ldots,x_{d})^{\top}\in\mathbb{R}^{d}\cong\mathbb{R}^{d\times 1}\). Let us consider a two-layer neural network \(f(\cdot,\theta):\mathbb{R}^{d}\to\mathbb{R}^{e}\) specified by the function

\[f(x;\theta)=W^{(2)}\sigma(W^{(1)}x),\] (1)

where \(x\in\mathbb{R}^{d}\) is the input, \(\theta=(W^{(1)},W^{(2)})\) with \(W^{(1)}\in\mathbb{R}^{l\times d}\) and \(W^{(2)}\in\mathbb{R}^{e\times l}\) are the parameters, \(\sigma:\mathbb{R}\to\mathbb{R}\) is the component-wise activation function and \(l\) is the number of neurons in the hidden layer. Notice that we consider a network with no biases, as it allows us a discussion with lighter notation. The case with biases is discussed in Appendix E.

In this work, following [12], we focus on the case where \(\sigma\) is _homogeneous_, namely \(\sigma(x)=\sigma^{\prime}(x)\cdot x\) for every \(x\) and for every element of the sub-differential \(\sigma^{\prime}(x)\) if \(\sigma\) is non-differentiable at \(x\). The commonly used ReLU (\(\sigma(z)=\max\left\{z,0\right\}\)) and Leaky ReLU (\(\sigma(z)=\max\left\{z,\gamma\right\}\) with \(0\leq\gamma\leq 1\)) activation functions satisfy this property.

We call _parameter space_ the vector space \(\Theta=\left\{\theta=(W^{(1)},W^{(2)})\mid W^{(1)}\in\mathbb{R}^{l\times d},W ^{(2)}\in\mathbb{R}^{e\times l}\right\}\).

It will also be convenient to examine the single hidden neurons and their associated parameters for the following discussions.

**Proposition 1**.: _For the two-layer neural network defined in Equation (1). Let \(k=1,\ldots,l\), let \((e_{11},e_{12},\ldots,e_{ll})\) be the canonical basis of \(\mathbb{R}^{l\times l}\) and \(\Theta_{k}=\left\{\theta_{k}=(e_{kk}W^{(1)},W^{(2)}e_{kk})\mid(W^{(1)},W^{(2) })\in\Theta\right\}\subset\Theta\), then \(\Theta=\Theta_{1}\oplus\cdots\oplus\Theta_{l}\)._

Details of the proof are provided in Appendix A. Fixing \(k\in\left\{1,\ldots,l\right\}\), we can consider \(\Theta_{k}\) as the parameter space of the \(k\)-th hidden neuron, which consists of the inputs and output weights of neuron \(k\), namely the rows and columns of \(W^{(1)}\) and \(W^{(2)}\), respectively. For simplicity, when we work in \(\Theta_{k}\), we write \(W^{(1)}_{k}:=e_{kk}W^{(1)}\) and \(W^{(2)}_{k}:=W^{(2)}e_{kk}\). Interestingly, the decomposition of Proposition 1 only holds for two-layer neural networks and will be crucial to the formulations of this paper's results.

### Symmetries and observationally equivalent networks

It is well known that the properties of the activation function heavily influence the geometry of the parameter space \(\Theta\). The activation function's commutativity with some classes of transformations can result in the latter having no effect on the function implemented by the neural network. This means that, in general, the mapping from the parameter space to the hypothesis class of functions is not injective. Following the terminology in Dinh et al. [10], we say that two parameters \(\theta_{1},\theta_{2}\in\Theta\) are _observationally equivalent_, if they encode the same function \(f(\cdot;\theta_{1})=f(\cdot,\theta_{2})\) and write \(\theta_{1}\sim\theta_{2}\).

In the case of homogeneous activations (ReLU or Leaky ReLU), we describe two kinds of transformations that send a parameter \(\theta\) into an observationally equivalent one.

Neuron rescaling.The input weights of a hidden neuron can be rescaled by a positive scalar \(\alpha>0\) provided that its output weights are rescaled by the inverse \(\alpha^{-1}\) (top panel of Figure 0(a)). We formalize this as the action of the group \(\mathbb{R}_{+}\) of positive real numbers on \(\Theta_{k}\):

\[T\colon\mathbb{R}_{+}\times\Theta_{k} \to\Theta_{k}\] (2) \[(\alpha,\theta_{k}) \mapsto T_{\alpha}(\theta_{k})=\left(\alpha\cdot W^{(1)}_{k}, \frac{1}{\alpha}\cdot W^{(2)}_{k}\right).\]

This action can be naturally extended to the space of all parameters by considering the possibility of rescaling all hidden neurons simultaneously by different factors. If \(\alpha=(\alpha_{1},\ldots,\alpha_{l})\in\mathbb{R}_{+}^{l}\)

\[T_{\alpha}(\theta)=(\mathrm{diag}(\alpha)W^{(1)},W^{(2)}\mathrm{diag}(\alpha)^ {-1}).\] (3)Given that \(\sigma(az)=a\sigma(z)\) when \(a\in\mathbb{R}_{+}\), we see how \(\theta\sim T_{\alpha}(\theta)\).

We write \(T(\theta)\) to denote the orbit of a parameter \(\theta\) under the action of \(T\), i.e. the set of all parameters obtained from \(\theta\) by arbitrarily rescaling the neurons \(T(\theta)=\big{\{}T_{\alpha}(\theta):\ \alpha\in\mathbb{R}_{+}^{l}\big{\}}\).

Permutations of the neurons.Besides rescaling, we can obtain an observationally equivalent network by permuting the hidden neurons in such a way as to preserve their input and output weights (bottom panel of Figure 1a).

Given the symmetric group on \(l\) elements \(\mathfrak{S}_{l}\) of the permutations of \(\{1,\dots,l\}\), we write the action

\[P\colon\mathfrak{S}_{l}\times\Theta \rightarrow\Theta\] (4) \[(\pi,\theta) \mapsto P_{\pi}(\theta)=(R_{\pi}W^{(1)},W^{(2)}R_{\pi}^{\intercal}).\]

where \(R_{\pi}\) is the \(l\times l\) row-permutation matrix associated to the permutation \(\pi\).

Given that the activation function \(\sigma\) is applied component-wise, we have that it commutes with \(R_{\pi}\), namely

\[f(x;P_{\pi}(\theta))=W^{(2)}R_{\pi}^{\intercal}\sigma(R_{\pi}W^{(1)}x)=W^{(2) }R_{\pi}^{\intercal}R_{\pi}\sigma(W^{(1)}x)=W^{(2)}\sigma(W^{(1)}x)=f(x;\theta)\]

and thus \(P_{\pi}(\theta)\sim\theta\) because \(R_{\pi}^{\intercal}=(R_{\pi})^{-1}\).

Having defined these two actions, we say that \(\theta\) and \(\theta^{\prime}\) are _observationally equivalent by rescalings and permutations_ if \(\theta^{\prime}\) can be obtained from \(\theta\) by a finite sequence of actions of \(T\) and \(P\) or, equivalently thanks to Lemma 3 in the Appendix, if there exists a rescaling \(\alpha\) and a permutation \(\pi\) such that \(\theta^{\prime}=P_{\pi}\circ T_{\alpha}(\theta)\). In this case, we write \(\theta\overset{\mathrm{rp}}{\sim}\theta^{\prime}\).

### Conserved quantities and the invariant hyperquadrics

The presence of symmetries in the neural network's parameter-function map results in a specific geometric structure in the loss landscape. Let indeed \(D=\big{\{}(x_{i},y_{i})\in\mathbb{R}^{d}\times\mathbb{R}^{e}\big{\}}_{i=1}^{N}\) be a training set of \(N\) input-output pairs and fix a loss function \(L:\Theta\rightarrow\mathbb{R}\) which depends on the parameters only through the output of the neural network (1), that is

\[L(\theta)=\frac{1}{N}\sum_{i=1}^{N}\ell(f(x_{i};\theta),y_{i})\] (5)

where \(\ell:\mathbb{R}^{e}\times\mathbb{R}^{e}\rightarrow\mathbb{R}\) is differentiable. In this work, as empirical risk minimization, we consider the continuous time version of the gradient descent (GD) algorithm (with learning rate \(h>0\))

\[\theta_{t+1}=\theta_{t}-h\nabla_{\theta}L(\theta_{t})\] (6)

Figure 1: **a.** Depiction of the two group actions acting on the space of the network’s parameters: the neuron rescaling of Equation (2) (top) and the neuron permutation of Equation (4) (bottom). **b.** Depiction of the geometry of the parameter space induced by the rescaling invariance of ReLU networks. The dotted lines denote the orbits \(T(\theta)\) while the solid lines represent the invariant sets \(\mathcal{H}(c)\) associated with \(\theta\) and the one associated with its rescaled version \(\theta^{\prime}\). Notice how the gradient of the loss \(g(\theta)\) is tangent to \(\mathcal{H}(c)\) and orthogonal to \(T(\theta)\).

named _gradient flow_ (GF), and defined as

\[\frac{\mathrm{d}}{\mathrm{d}t}\theta(t)\in\epsilon-\nabla_{\theta}L(\theta(t)):=-g (\theta(t))\] (7)

where \(\nabla_{\theta}L(\theta(t))\) is the Clarke sub-differential [7] which takes into account the parameters \(\theta\) where \(L(\theta)\) is non-differentiable. Given that the loss function \(L\) depends on the parameters only through \(f\), its value at \(\theta\) must be constant over the orbit \(T(\theta)\). This, together with the fact that the gradient of a differentiable function at a point is orthogonal to the level set at that point, means that

\[g(\theta)\perp T(\theta)\] (8)

at any parameter \(\theta\) where \(L(\theta)\) is differentiable, as represented in Figure 0(b). This orthogonality condition constrains the possible values of the gradient and, by extension, the possible gradient flow trajectories. In particular, as proven in Liang et al. [29], Tanaka et al. [46], Equation (8) is equivalent to

\[\sum_{i=1}^{d}W^{(1)}_{ki}g^{(1)}_{ki}-\sum_{j=1}^{e}W^{(2)}_{jk}g^{(2)}_{jk}= 0\ \ \forall k=1,\ldots,l.\] (9)

For convenience of notation, we define, for \(k=1,\ldots,l\), the following bilinear forms on \(\Theta\), which help us describe the geometry induced by the rescaling symmetry. If \(\theta=(W^{(1)},W^{(2)})\) and \(\eta=(V^{(1)},V^{(2)})\), we define

\[\langle\!\langle\theta,\eta\rangle\!\rangle_{k}=\sum_{i=1}^{d}W^{(1)}_{ki}V^{ (1)}_{ki}-\sum_{j=1}^{e}W^{(2)}_{jk}V^{(2)}_{jk}\] (10)

which, notice, only depends on the \(k\)-th row of \(W^{(1)}\) and \(k\)-th column of \(W^{(2)}\) meaning that we can equivalently see it as a bilinear form on \(\Theta_{k}\). \(\Theta_{k}\), together with \(\langle\!\langle\cdot,\cdot\rangle\!\rangle_{k}\) is a _pseudo-Euclidean space_.

With the notation given by Equation (10), we see that Equation (9) can be simply rewritten as \(\langle\!\langle\theta,g(\theta)\rangle\!\rangle_{k}=0\) for every neuron \(k\). This condition, akin to orthogonality w.r.t. the bilinear form of Equation (10), implies that, under gradient flow optimization,

\[\frac{\mathrm{d}}{\mathrm{d}t}\langle\!\langle\theta,\theta\rangle\!\rangle_{ k}=2\langle\!\langle\dot{\theta},\theta\rangle\!\rangle_{k}=-2\langle\!\langle g (\theta),\theta\rangle\!\rangle_{k}=0\ \ \forall k=1,\ldots,l.\] (11)

This result, first obtained in Saxe et al. [41] for linear networks and discussed in Du et al. [12], Liang et al. [29], Kunin et al. [27], tells us that the rescaling symmetry results in the quantities \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}\) being conserved. This means that the difference between the Euclidean norm of the inputs and the outputs is constant for each neuron throughout the GF training trajectory. Moreover, under the condition of homogeneity of the activation function, Du et al. [12] proves that Equation (11) holds even at non-differentiable points of \(L\) and in the case of multiple layers.

Invariant sets.Assume that at the initialization \(\theta_{0}\) we have \(\langle\!\langle\theta_{0},\theta_{0}\rangle\!\rangle_{k}=c_{k}\), for all \(k\), then Equation (11) implies that the GF trajectory will lie on the set characterized by the system of equations \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}=c_{k}\) for \(k=1,\ldots,l\). This subset is mapped to itself under the GF dynamics by Equation (11) (see Figure 0(b)) and constitutes the main object of our study.

**Definition 1** (Invariant set).: _Given \(c=(c_{1},\ldots,c_{l})\), we call invariant set the subset \(\mathcal{H}(c)\subseteq\Theta\) given by the equations \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}=c_{k}\ \forall k=1,\ldots,l\)._

If we look at each single equation (i.e. to each hidden neuron), we see that Equation (11) can be written as

\[\sum_{i=1}^{d}\left(W^{(1)}_{ki}\right)^{2}-\sum_{j=1}^{e}\left(W^{(2)}_{jk} \right)^{2}=c_{k}\] (12)

which corresponds to a _hyperquadric_ (or quadric hypersurface) in \(\Theta_{k}\). We denote with \(\mathcal{Q}(c_{k})\subseteq\Theta_{k}\) this hypersurface and call it the _invariant hyperquadric_ associated to the \(k\)-th hidden neuron.

Here \(c_{k}\in\mathbb{R}\) takes the role of a label associated with the \(k\)-th hidden neuron, which, we see in the next section, plays a key role in specifying the shape of \(\mathcal{Q}(c_{k})\). Figure 1(a) shows how, for \(d=2\) and \(e=1\), \(\mathcal{Q}(c_{k})\) is an hyperboloid with 1 sheet (connected) if \(c_{k}>0\) and 2 sheets if \(c_{k}<0\).

Topology of the invariant set

As we discussed above, Equation (11) tells us that gradient flow trajectories can't explore the whole space \(\Theta\) but are constrained to lie on the invariant set \(\mathcal{H}(c)\). The values of \(c\), in turn, depend on the initialization and, we see from Equation (12), quantify the balance between the norms of input and output weights in every hidden neuron.

The goal of this section is to provide a topological characterization of \(\mathcal{H}(c)\) that can tell us something about the presence or absence of fundamental _obstructions_ to the network's training process. With obstruction, we mean the impossibility of a GF trajectory to travel freely from one point to the other in \(\mathcal{H}(c)\). We refer the reader to Appendix B for an essential overview of some of the topological concepts that we rely on in the next paragraphs.

Counting high-dimensional holes.Our topological characterization will be framed using _Betti numbers_. Betti numbers are well-known topological invariants given by a sequence of natural numbers that intuitively encode the number of higher-dimensional holes and cavities present in space. In particular, the 0-th Betti number of a space \(X\), \(\beta_{0}(X)\) corresponds to the number of connected components of \(X\) and thus will be fundamental for our goal of identifying obstructions.

The invariant set \(\mathcal{H}(c)\) is given as the set of solutions of \(l\) polynomial equations of degree 2 sharing no variables. Furthermore, in the setting of two-layer neural networks, we can leverage the fact that the parameter space can be decomposed into the parameter spaces of the hidden neurons. This, in turn, allows us to decompose the invariant set as the product of the neurons' invariant hyperquadrics, greatly simplifying our study.

**Lemma 1**.: _In a two-layer ReLU neural network, the invariant set \(\mathcal{H}(c)\) is homeomorphic to the Cartesian product of the hidden neurons' invariant hyperquadrics, that is_

\[\mathcal{H}(c)\equiv\mathcal{Q}(c_{1})\times\dots\times\mathcal{Q}(c_{l}).\] (13)

Lemma 1 tells us that we can understand the topology of \(\mathcal{H}(c)\) by studying independently its factors. Moreover, the hyperquadrics we encounter here are well-studied objects for which the next proposition (proven in Appendix D.2) gives a topological characterization.

**Proposition 2**.: _If \(c_{k}>0\), \(\mathcal{Q}(c_{k})\) is a topological manifold homeomorphic to \(\mathbb{R}^{e}\times S^{d-1}\). If \(c_{k}<0\), \(\mathcal{Q}(c_{k})\) is a topological manifold homeomorphic to \(\mathbb{R}^{d}\times S^{e-1}\). If \(c_{k}=0\), \(\mathcal{Q}(0)\) is a contractible space._

Leveraging the decomposition of Lemma 1 and the characterization of the factors given by Proposition 2, we can explicitly compute all the Betti numbers of the invariant set. We give the next result in terms of the _Poincare polynomial_ of \(\mathcal{H}(c)\), namely the polynomial whose coefficients are the Betti numbers (see Appendix B).

**Theorem 1**.: _Let \(l_{+},l_{-},l_{0}\) be the number of positive, negative, and zero components of \(c\), respectively. The Poincare polynomial of \(\mathcal{H}(c)\) is given by_

\[p_{\mathcal{H}(c)}(x)=(1+x^{d-1})^{l_{+}}(1+x^{e-1})^{l_{-}}.\] (14)

This result, which is proven in Appendix D.3, contains a wealth of topological information as it gives us the exact number of holes and cavities of any order, depending on the network's hyperparameters (\(d,e\)) and initialization (\(l_{+},l_{-}\)). In the rest of this work, we focus only on the 0-th Betti number as the non-connectedness of \(\mathcal{H}(c)\) provides a clear obstruction to the GF trajectories.

Connectedness of the invariant set.With regard to the connectedness of \(\mathcal{H}(c)\), we can leverage Theorem 1 to obtain the exact number of connected components.

**Corollary 1**.: _The \(0\)-th Betti number \(\beta_{0}\) of \(\mathcal{H}(c)\), corresponding to the number of its connected components, is given by_

\[\beta_{0}=\begin{cases}1&\text{if }d,e>1\\ 2^{l_{+}}&\text{if }d=1,e>1\\ 2^{l_{-}}&\text{if }d>1,e=1\\ 2^{l_{+}+l_{-}}&\text{if }d=1,e=1\end{cases}\] (15)

Proof.: This can be directly obtained from the coefficient of degree 0 of the Poincare polynomial obtained through Theorem 1.

What we see in Equation (15) is that in most cases, the invariant set is connected, and gradient flow has no topological limitations in exploring the whole of \(\mathcal{H}(c)\). Instead, when the hidden neurons have only one input or only one output, the space is fragmented into several components whose number scales exponentially in \(l_{+}\) or \(l_{-}\), respectively.

Let us focus on the more interesting case where \(d>1\) and \(e=1\).

**Corollary 2**.: _If the output of a two-layer ReLU neural network is a single scalar \(e=1\), its input has dimension \(d>1\), and the initial parameter \(\theta_{0}\) is such that \(\langle\!\langle\theta_{0},\theta_{0}\rangle\!\rangle_{k}<0\) for \(l_{-}>0\) hidden neurons, then the set \(\mathcal{H}(c)\) is disconnected and has \(2^{l_{-}}\) connected components._

This means that neurons initialized with the norm of their outgoing weight strictly greater than their incoming weights' norm are responsible for disconnecting the space. We now precisely identify which connected component a parameter \(\theta\) belongs to and clarify the meaning of the obstruction.

**Proposition 3**.: _Let \(e=1,d>1\), and \(\theta\in\mathcal{H}(c)\) with \(c\) such that \(c_{k_{1}},\ldots,c_{k_{l_{-}}}<0\) while \(c_{k}\geq 0\) for all other \(k\). Let \(W_{-}^{(2)}:=(W_{k_{1}}^{(2)},\ldots,W_{k_{l_{-}}}^{(2)})\in\mathbb{R}^{1\times l _{-}}\) be the row vector whose components are the components of \(W^{(2)}\in\mathbb{R}^{1\times l}\) associated to \(c_{k}<0\). Then the vector \(s(\theta)=(\operatorname{sign}(W_{k_{1}}^{(2)}),\ldots,\operatorname{sign}(W_ {k_{l_{-}}}^{(2)}))\) identifies uniquely the component \(\theta\) belongs to, namely: \(\theta\) and \(\theta^{\prime}\) belong to the same connected component of \(\mathcal{H}(c)\) if and only if \(s(\theta)=s(\theta^{\prime})\)._

Proposition 3, proven in Appendix D.4, implies that \(s(\theta)\) does not change when we move in \(C\) on a continuous curve such as the one given by gradient flow. This gives us an interesting interpretation of the topological obstruction: gradient flow cannot change the signs of the outgoing weights of the hidden neurons \(k\) such that \(c_{k}<0\) (see Appendix G for an intuitive explanation of the phenomenon). This same observation is also mentioned in Boursier and Flammarion [3]. Proposition 3 extends one of the results of Boursier et al. [4] which proves that the same also holds when \(c_{k}=0\) (balanced initialization).

By also considering Corollary 1, one obtains that a clever initialization of the parameters given by \(\langle\!\langle\theta_{0},\theta_{0}\rangle\!\rangle_{k}=c_{k}>0\)\(\forall k=1,\ldots,l\) can prevent the issue by ensuring the connectedness of the invariant set. We also find that under common initialization schemes such as Xavier [19] and Kaiming [21] the probability of having pathological neurons is negligible when the input dimension and number of hidden neurons is high (see Appendix F).

## 5 Taking symmetries into account

Corollary 2 states that neurons \(k\) such that \(c_{k}<0\) are "pathological", in the sense that they are responsible for disconnecting the invariant set into several components, whose number scales exponentially in the number of those neurons. This result gives us a grim picture of the possibility of actually

Figure 2: **a.** The invariant hyperquadric \(\mathcal{Q}(c_{k})\) of a neuron with two inputs (\(d=2\)) and one output (\(e=1\)) in the cases where \(c_{k}<0\) (left) and \(c_{k}>0\) (right). **b.** Depiction of the invariant set \(\mathcal{H}(c)\) in the case where \(l_{-}=2\) so that there are \(2^{l_{-}}=4\) connected components. \(C_{\pm\pi}\) denotes the connected component such that \(s=(\pm 1,\mp 1)\). The blue lines separate the different effective components of \(\mathcal{H}(c)\).

optimizing the neural network: if the initial parameter \(\theta_{0}\) is in a particular connected component and the global optimum \(\theta_{*}\) lies in another, then any gradient flow trajectory will not be able to reach \(\theta_{*}\) because it will be constrained in its connected component.

This result, however, provides us only with a partial picture of the parameter space's geometry. It is a priori possible that the training trajectory, moving in its connected component, reaches a parameter \(\zeta\), which itself is optimal as it is observationally equivalent to \(\theta_{*}\) (\(\zeta\sim\theta_{*}\)). In this case, the topological obstruction given by the non-connectedness would be only apparent.

To take this fact into account, we define the following notion.

**Definition 2** (Effective component).: _Let \(\theta\in\mathcal{H}(c)\) and \(C(\theta)\) be its connected component therein. We define its effective component \(\operatorname{Eff}(\theta)\) as the union of the connected component of all \(\theta^{\prime}\) such that \(\theta^{\prime}\stackrel{{\mathrm{Tp}}}{{\sim}}\theta\). So that \(\operatorname{Eff}(\theta):=\bigcup_{\theta^{\prime}\stackrel{{ \mathrm{Tp}}}{{\sim}}\theta}C(\theta^{\prime})\)._

Figure 1(b) gives a picture which clarifies the definition, showing a space with 4 connected components that has only 3 effective components. If the optimum \(\theta_{*}\) belongs to the same effective component as the initialization, then it is possible to reach a parameter that is observationally equivalent to it (through permutations and rescalings).

We present a useful result which tells us that the action of rescaling of Equation (3) can take any non-degenerate parameter \(\theta\in\mathcal{H}(c)\) to any other invariant set \(\mathcal{H}(c^{\prime})\) for every \(c^{\prime}\in\mathbb{R}^{l}\). This means that any invariant set can realize all the neural network's functions.

**Proposition 4**.: _For every \(c_{k}\in\mathbb{R}\) and for every \(\theta_{k}\in\Theta_{k}\) such that \(W_{k}^{(1)},W_{k}^{(2)}\neq 0\), there exists a unique \(\alpha_{k}\in\mathbb{R}_{+}\) such that \(T_{\alpha_{k}}(\theta_{k})\in\mathcal{Q}(c_{k})\). If \(W_{k}^{(1)}=0\) and \(W_{k}^{(2)}\neq 0\), then the same holds for every \(c_{k}<0\), while, if \(W_{k}^{(1)}\neq 0\) and \(W_{k}^{(2)}=0\), it holds for every \(c_{k}>0\)._

The proof can be found in Appendix D.5 with the formula of the specific \(\alpha\) which realizes the rescaling.

The following theorem leverages the power of Proposition 4 to give necessary and sufficient conditions for \(\theta\) and \(\theta^{\prime}\) to belong to the same effective component.

**Theorem 2**.: _Let \(d>1\) and \(e=1\). Let \(c\in\mathbb{R}^{l}\) and \(l_{-}\) be the number of neurons such that \(c_{k}<0\). Assume that \(l_{-}\geq 1\). Let \(C,C^{\prime}\subseteq\mathcal{H}(c)\) be two distinct connected components of \(\mathcal{H}(c)\) such that \(s(\theta)=s\), \(\forall\theta\in C,\) and \(s(\theta^{\prime})=s^{\prime}\), \(\forall\theta^{\prime}\in C^{\prime}\). Then, the following statements are equivalent:_

1. _for every_ \(\theta\in C\) _there exists_ \(\theta^{\prime}\in C^{\prime}\) _such that_ \(\theta\stackrel{{\mathrm{Tp}}}{{\sim}}\theta^{\prime}\)_;_
2. \(\sum\limits_{i=1}^{l_{-}}s_{i}=\sum\limits_{i=1}^{l_{-}}s_{i}^{\prime}\)__

The theorem, proven in Appendix D.6, tells us that, while connected components are identified by \(s\), the effective components are identified only by the values of \(\sum_{i}s_{i}\) or, equivalently, by the distribution of \(\pm 1\) in \(s\). Therefore, we find that the number of effective components scales much slower than the exponential growth of the number of connected components given by Corollary 1.

**Corollary 3**.: _The number of effective components of \(\mathcal{H}(c)\) is given by \(1+l_{-}\)._

Proof.: Theorem 2 tells us that two connected components \(C,C^{\prime}\) belong to the same effective component if and only if their associated sign vectors \(s,s^{\prime}\in\{-1,1\}^{l_{-}}\) have the same sum. The number of effective components will thus equal the number of different values that the sum \(\sum_{i=1}^{l_{-}}s_{i}\) can have. If \(s_{i}=1\)\(\forall i\) then \(\sum_{i=1}^{l_{-}}s=l_{-}\). Each switch of a component to \(-1\) decreases the sum's value by \(2\) until it reaches the minimum \(-l_{-}\). Therefore, the total number of values of the sum will be \(1+l_{-}\). 

## 6 Empirical Validation

Task, dataset, and model setup.We display here a toy example, showing how the initialization of the model can cause a topological obstruction, making the optimum unreachable.

We consider the function \(F(x_{1},x_{2})=-(x_{1}+x_{2})\), which will be our ground-truth. Next, we generate a dataset of 8000 points (\(x_{i},F(x_{i})\)) by sampling \(x_{i}\sim U(\left[0,1\right]^{2})\). Our model, depicted in Figure 2(a)is a one hidden layer neural network with 2 hidden neurons, ReLU activations and no biases. All the weights are initialized by independently sampling from \(U(\left[-\sqrt{2},\sqrt{2}\right])\). From the task and the network's architecture, it is clear that at least one of the output weights has to be negative to approximate \(F\) correctly.

To standardize our results, we apply the rescaling of Proposition 4 and relocate the initial parameters to an observationally equivalent one in the invariant set \(\mathcal{H}(c)\) with \(c_{k}\in\{-0.1,0.1\}\), controlling the sign of the weights on the last layer. We allow ourselves to do these two manipulations to control the experiments while only marginally modifying the network initialization, avoiding the introduction of massively unbalanced weights, which could change the dynamics, as shown in Neyshabur et al. [34]. Finally, we train the network using gradient descent on the MSE loss with a small learning rate of \(h=0.01\). This limits the variations of \(c_{k}\) values to less than one percent along training, giving us a good approximation of gradient flow.

Results.We initialize different models and collect all states and losses. First, when we initialize the model with an "unlucky" configuration, namely \(c=(-0.1,-0.1)\) (the space has 4 connected components) and \(s(\theta)=(+1,+1)\), we find that the trajectories are confined to the positive region of their invariant hyperquadric, resulting in a poor approximation of \(F\), as we can see in Figure 3b (top) and in the loss of Figure 3c. Instead, with an initial configuration such that \(c=(-0.1,+0.1)\) (2 connected components) and \(s(\theta)=(+1,+1)\), the model can leverage the connectedness of \(\mathcal{Q}(c_{2})\) to learn \(F\) by flipping the sign of the second neuron's output weight (Figure 3b bottom right).

A more realistic experiment.We present here a further experiment to show how the topological obstruction can be a hindrance in a more realistic setting. We consider a simple binary classification task on the well-known _breast cancer_ dataset [49], which we try to solve by fitting a one-layer ReLU neural network trained to minimize the BCE loss. We vary the number of hidden neurons \(l\) and, for each \(l\), we change the number of non-pathological neurons \(l_{+}\) (neurons with \(c_{k}>0\)) from 0 to \(l\). We repeat the experiment with 100 different random initializations and show how the model's average performance changes when the degree of disconnectedness of its invariant set is varied. The result, on the left panel of Figure 4, clearly shows the presence of a "gradient" in performance, where increasing the number of non-pathological neurons decreases the average value of the test loss after training. The right panel of Figure 4, moreover, shows how the impact of the obstruction depends on the number of non-pathological neurons and not on their fraction over the total number of hidden neurons.

Figure 3: Visualization of the experimental setup described in Section 6. **a.** The small 2-layer neural network architecture considered. **b.** The hidden neurons’ parameter spaces, together with the invariant hyperquadrics associated with hidden neurons 1 (left) and 2 (right), for an initialization with topological obstruction (top) and without it (bottom). The colored curves represent the gradient descent trajectories from initialization \(\theta_{k}(0)\) up to \(t_{*}=500\) optimization steps. **c.** The loss curves for the bad (obstructed) and good initializations.

## 7 Conclusions

In this paper, we have given analytical results that clarify the nature of the constraints imposed by gradient flow on the parameter space of a two-layer neural network with homogeneous activations. In the case of a single scalar output, which appears in tasks such as binary classification and scalar regression, we identified initial conditions that lead to a topological obstruction in the form of the parameter space's fragmentation into multiple connected components. This is caused by pathological neurons whose output weights cannot change their sign during training. Moreover, if one also considers the network's symmetries under permutations of the hidden neurons, we find that most of the connected components are equivalent. The number of effective components of the resulting space scales linearly with the number of pathological neurons, contrasting with the exponential growth of the number of connected components obtained without considering the permutation symmetries.

As shown in the last numerical experiment, the lack of non-pathological neurons hinders learning, even when the network's width is scaled. Our probabilistic analysis outlined in Appendix F, however, shows that with common initialization schemes, the probability of creating a pathological neuron decreases rapidly with increased inner layer width. Therefore, the combination of specific initialization schemes and a large number of hidden neurons (beyond the minimum required to solve a task) appears to make this obstruction unlikely in practice. This work describes a simple safeguard to avoid obstructions, which can, for instance, discourage the usage of initialization schemes that result in the proliferation of pathological neurons.

## 8 Limitations

The main limitation of the work is the network's architecture, which is limited to only one hidden layer. Considering multiple layers, we can still define rescalings and permutations and find invariant hyperquadrics for each hidden neuron. The issue emerges in the fact that these hyperquadrics are not "independent" anymore, and the invariant set cannot be factored into the product of the \(\mathcal{Q}(c_{k})\). This intuitively results from the fact that in the multi-layer case, each weight in the hidden layers is shared by two neurons.

The second limitation is that our study focuses on gradient flow optimization. This idealized situation doesn't take into account the fact that moderate step size of gradient descent and stochastic gradient descent can break the conservation of \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}\) and make the parameters drift away from the invariant set [2]. Moreover, popular optimizers like ADAM [25] update the parameters employing the gradients at previous iterations so their trajectories will not be constrained to lie on \(\mathcal{H}(c)\) as we defined it.

The inclusion of regularization terms in the loss function, such as \(\ell_{p}\) regularizations, also breaks the invariance to rescalings.

Figure 4: **Left.** Average test BCE loss of a two-layer ReLU neural network trained on the _breast cancer_ dataset over 100 different initializations for each pair (\(l,l_{+}\)), \(l=2,\ldots,9\) and \(l_{+}\leq l\), of numbers of hidden neurons and non-pathological neurons. **Right.** the y-axis displays the percentage of non-pathological neurons.

## Acknowledgements

M.N. acknowledges the project PNRR-NGEU, which has received funding from the MUR - DM 352/2022.

F.V. would like to thank the Isaac Newton Institute for Mathematical Sciences for the support and hospitality during the programme Hypergraphs: Theory and Applications when work on this paper was undertaken. This work was supported by: EPSRC Grant Number EP/V521929/1

This study was carried out within the FAIR - Future Artificial Intelligence Research and received funding from the European Union Next-GenerationEU (PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) - MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.3 - D.D. 1555 11/10/2022, PE00000013). This manuscript reflects only the authors' views and opinions; neither the European Union nor the European Commission can be considered responsible for them.

## References

* [1] Serguei Barannikov, Daria Voronkova, Ilya Trofimov, Alexander Korotin, Grigorii Sotnikov, and Evgeny Burnaev. Topological obstructions in neural networks learning. _arXiv preprint arXiv:2012.15834_, 2020.
* [2] David GT Barrett and Benoit Dherin. Implicit gradient regularization. _arXiv preprint arXiv:2009.11162_, 2020.
* [3] Etienne Boursier and Nicolas Flammarion. Early alignment in two-layer networks training is a two-edged sword. _arXiv preprint arXiv:2401.10791_, 2024.
* [4] Etienne Boursier, Loucas Pillaud-Vivien, and Nicolas Flammarion. Gradient flow dynamics of shallow relu networks for square loss and orthogonal inputs. _Advances in Neural Information Processing Systems_, 35:20105-20118, 2022.
* [5] Maria Sofia Bucarelli, Giuseppe Alessio D'Inverno, Monica Bianchini, Franco Scarselli, and Fabrizio Silvestri. A topological description of loss surfaces based on betti numbers. _arXiv preprint arXiv:2401.03824_, 2024.
* [6] Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on learning theory_, pages 1305-1338. PMLR, 2020.
* [7] Francis H Clarke, Yuri S Ledyaev, Ronald J Stern, and Peter R Wolenski. _Nonsmooth analysis and control theory_, volume 178. Springer Science & Business Media, 2008.
* [8] Y Cooper. The critical locus of overparameterized neural networks. _arXiv preprint arXiv:2005.04210_, 2020.
* [9] Yaim Cooper. The loss landscape of overparameterized neural networks. _arXiv preprint arXiv:1804.10200_, 2018.
* [10] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR, 2017.
* [11] Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht. Essentially no barriers in neural network energy landscape. In _International conference on machine learning_, pages 1309-1318. PMLR, 2018.
* [12] Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. _Advances in neural information processing systems_, 31, 2018.
* [13] Simon Eberle, Arnulf Jentzen, Adrian Riekert, and Georg S. Weiss. Existence, uniqueness, and convergence rates for gradient flows in the training of artificial neural networks with relu activation. _Electronic Research Archive_, 2023.

* Feng et al. [2019] Yuanyuan Feng, Tingran Gao, Lei Li, Jian-Guo Liu, and Yulong Lu. Uniform-in-time weak error analysis for stochastic gradient descent algorithms via diffusion approximation. _arXiv preprint arXiv:1902.00635_, 2019.
* Fort et al. [2020] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. _Advances in Neural Information Processing Systems_, 33:5850-5861, 2020.
* Frankle et al. [2020] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pages 3259-3269. PMLR, 2020.
* Freeman and Bruna [2016] C Daniel Freeman and Joan Bruna. Topology and geometry of half-rectified network optimization. _arXiv preprint arXiv:1611.01540_, 2016.
* Garipov et al. [2018] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* Glorot and Bengio [2010] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 249-256. JMLR Workshop and Conference Proceedings, 2010.
* Ghuch and Urbanke [2021] Grzegorz Ghuch and Rudiger Urbanke. Noether: The more things change, the more stay the same. _arXiv preprint arXiv:2104.05508_, 2021.
* He et al. [2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _Proceedings of the IEEE international conference on computer vision_, pages 1026-1034, 2015.
* Horoi et al. [2022] Stefan Horoi, Jessie Huang, Bastian Rieck, Guillaume Lajoie, Guy Wolf, and Smita Krishnaswamy. Exploring the geometry and topology of neural network loss landscapes. In _International Symposium on Intelligent Data Analysis_, pages 171-184. Springer, 2022.
* Ioffe and Szegedy [2015] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456. pmlr, 2015.
* Ji and Telgarsky [2019] Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HJflg30qKX.
* Kingma and Ba [2014] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _CoRR_, abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID:6628106.
* Kuditipudi et al. [2019] Rohith Kuditipudi, Xiang Wang, Holden Lee, Yi Zhang, Zhiyuan Li, Wei Hu, Rong Ge, and Sanjeev Arora. Explaining landscape connectivity of low-cost solutions for multilayer nets. _Advances in neural information processing systems_, 32, 2019.
* Kunin et al. [2020] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. _arXiv preprint arXiv:2012.04728_, 2020.
* Liang et al. [2018] Shiyu Liang, Ruoyu Sun, Yixuan Li, and Rayadurgam Srikant. Understanding the loss surface of neural networks for binary classification. In _International Conference on Machine Learning_, pages 2835-2843. PMLR, 2018.
* Liang et al. [2019] Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-rao metric, geometry, and complexity of neural networks. In _The 22nd international conference on artificial intelligence and statistics_, pages 888-896. PMLR, 2019.
* Lyu et al. [2021] Kaifeng Lyu, Zhiyuan Li, Runzhe Wang, and Sanjeev Arora. Gradient descent on two-layer nets: Margin maximization and simplicity bias. _Advances in Neural Information Processing Systems_, 34:12978-12991, 2021.

* Mei et al. [2018] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* Munkres [2018] James R Munkres. _Elements of algebraic topology_. CRC press, 2018.
* Munkres [2000] J.R. Munkres. _Topology_. Featured Titles for Topology. Prentice Hall, Incorporated, 2000. ISBN 9780131816299.
* Neyshabur et al. [2015] Behnam Neyshabur, Russ R Salakhutdinov, and Nati Srebro. Path-sgd: Path-normalized optimization in deep neural networks. _Advances in neural information processing systems_, 28, 2015.
* Nguyen [2019] Quynh Nguyen. On connected sublevel sets in deep learning. In _International conference on machine learning_, pages 4790-4799. PMLR, 2019.
* Pittorino et al. [2022] Fabrizio Pittorino, Antonio Ferraro, Gabriele Perugini, Christoph Feinauer, Carlo Baldassi, and Riccardo Zecchina. Deep networks on toroids: removing symmetries reveals the structure of flat regions in the landscape geometry. In _International Conference on Machine Learning_, pages 17759-17781. PMLR, 2022.
* Rotskoff and Vanden-Eijnden [2022] Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. _Communications on Pure and Applied Mathematics_, 75(9):1889-1935, 2022.
* Safran and Shamir [2018] Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks. In _International conference on machine learning_, pages 4433-4441. PMLR, 2018.
* Safran et al. [2022] Itay Safran, Gal Vardi, and Jason D Lee. On the effective number of linear regions in shallow univariate relu networks: Convergence guarantees and implicit bias. _Advances in Neural Information Processing Systems_, 35:32667-32679, 2022.
* Sagun et al. [2017] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis of the hessian of over-parametrized neural networks. _arXiv preprint arXiv:1706.04454_, 2017.
* Saxe et al. [2013] Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. _arXiv preprint arXiv:1312.6120_, 2013.
* Simsek et al. [2021] Berlin Simsek, Francois Ged, Arthur Jacot, Francesco Spadaro, Clement Hongler, Wulfram Gerstner, and Johanni Brea. Geometry of the loss landscape in overparameterized neural networks: Symmetries and invariances. In _International Conference on Machine Learning_, pages 9722-9732. PMLR, 2021.
* Sirignano and Spiliopoulos [2020] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of large numbers. _SIAM Journal on Applied Mathematics_, 80(2):725-752, 2020.
* Smith et al. [2021] Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit regularization in stochastic gradient descent. _arXiv preprint arXiv:2101.12176_, 2021.
* Soudry et al. [2018] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. _Journal of Machine Learning Research_, 19(70):1-57, 2018.
* Tanaka et al. [2020] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. _Advances in neural information processing systems_, 33:6377-6389, 2020.
* Van Laarhoven [2017] Twan Van Laarhoven. L2 regularization versus batch and weight normalization. _arXiv preprint arXiv:1706.05350_, 2017.
* Venturi et al. [2019] Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural network optimization landscapes. _Journal of Machine Learning Research_, 20(133):1-34, 2019.
* Wolberg et al. [1995] William Wolberg, Olvi Mangasarian, Nick Street, and W Street. Breast cancer wisconsin (diagnostic). _UCI Machine Learning Repository_, 10:C5DW2B, 1995.

* [50] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad local minima in neural networks. _arXiv preprint arXiv:1802.03487_, 2018.
* [51] Bo Zhao, Iordan Ganev, Robin Walters, Rose Yu, and Nima Dehmamy. Symmetries, flat minima, and the conserved quantities of gradient flow. _arXiv preprint arXiv:2210.17216_, 2022.
* [52] Liu Ziyin. Symmetry induces structure and constraint of learning. In _Forty-first International Conference on Machine Learning_, 2024.

## Appendix A Parameter spaces

Let \((e_{11},e_{12},\ldots,e_{ll})\) and \((e_{1},\ldots,e_{l})\) be the canonical bases of the spaces \(\mathbb{R}^{l\times l}\) and \(\mathbb{R}^{l}\), respectively and

\[\Theta_{k}=\left\{(e_{kk}W^{(1)},W^{(2)}e_{kk})\mid(W^{(1)},W^{(2)})\in\Theta \right\}\subset\Theta.\]

\(\Theta_{k}\), notice, is the subspace of \(\Theta\) consisting of the weight matrices \(W^{(1)}\) with null rows except for the \(k\)-th one, and weight matrices \(W^{(2)}\) with null columns except for the \(k\)-th one. We can check that \(\Theta=\Theta_{1}\oplus\cdots\oplus\Theta_{l}\), because, if \(I_{l}\) is the \(l\times l\) identity matrix, \(\sum e_{kk}=I_{l}\) so that

\[(W^{(1)},W^{(2)})=\sum_{k}(W^{(1)}_{k},W^{(2)}_{k})=\sum_{k}(e_{kk}W^{(1)},W^{ (2)}e_{kk}).\]

Moreover, \(\Theta\cong\Theta_{1}\times\cdots\times\Theta_{l}\) via the linear isomorphism

\[\theta=(W^{(1)},W^{(2)})\leftrightarrow(\theta_{k})_{k=1}^{l}=\left((e_{kk}W^ {(1)},W^{(2)}e_{kk})\right)_{k=1}^{l}.\]

This, we see, is equivalent to decomposing the neural network of Equation (1) into the computations of the single hidden neurons. Indeed, let \(f(x;\theta_{k}):=f(x,(e_{kk}W^{(1)},W^{(2)}e_{kk}))\), then, considering that \(e_{kk}e_{kk}=e_{kk}\) and that \(\sigma(e_{kk}v)=e_{kk}\sigma(v)\), it holds that

\[f(x;\theta_{k})=W^{(2)}e_{kk}\sigma(W^{(1)}x)\ \forall k=1,\ldots,l.\]

Therefore \(\sum_{k}f(x;\theta_{k})=f(x;\theta)\).

## Appendix B Primer on topology

Here, we recall some basic facts about the topology required to understand the paper's results. A self-consistent introduction is outside this work's scope, so we refer the interested reader to more complete expositions in Munkres [33, 32].

Topological manifold.An \(n\)-dimensional _topological manifold_ is a topological space \(X\) which locally looks like the Euclidean space \(\mathbb{R}^{n}\). More formally, for each \(p\in X\), there exists a neighbourhood \(U\) of \(p\) and a homeomorphism mapping \(U\) to an open subset of \(\mathbb{R}^{n}\).

Contractible space.A topological space \(X\) is _contractible_ if it can continuously deform to a point \(p\in X\). This means that there exists a continuous map

\[F:X\times[0,1]\to X\]

such that \(F(x,0)=x\) and \(F(x,1)=p\) for every \(x\in X\).

Betti numbers.Betti numbers formalize the notion of the hole in a topological space and extend it to describe higher-dimensional cavities. The general idea is that one can associate a sequence of Abelian groups named _homology groups_ to any space \(X\), which encodes rich information about the higher-dimensional cavities in \(X\). For what we are concerned here, the rank of the \(k\)-th homology group is called the \(k\)-th _Betti number_\(\beta_{k}(X)\). \(\beta_{k}(X)\) counts the number of \(k\)-dimensional holes in the space: \(\beta_{0}(X)\) count the number of connected components, \(\beta_{1}(X)\) the number of "circular" holes and \(\beta_{2}(X)\) the number of voids or cavities.

A contractible space \(X\) is connected and cannot have any holes, and thus its Betti numbers are \(\beta_{0}(X)=1\) and \(\beta_{i}(X)=0\ \forall i>0\).

Betti numbers are _topological invariants_, meaning they are preserved when a space is transformed via a homeomorphism, namely a bijective, continuous map with continuous inverse.

Poincare polynomials.The _Poincare polynomial_ of a topological space \(X\) is the polynomial whose \(k\)-th coefficient is given by the \(k\)-th Betti number

\[p_{X}(x)=\beta_{0}(X)+\beta_{1}(X)x+\beta_{2}(X)x^{2}+\ldots.\]Kunneth formula.Kunneth's theorem describes assembling the homology groups of a Cartesian product of spaces \(X\times Y\) from the homology groups of the factors \(X,Y\). One of its corollaries tells us that if we care only about Betti numbers, a simple relation holds between the Poincare polynomials of \(X\times Y\) and the ones of \(X\) and \(Y\), namely,

\[p_{X\times Y}(x)=p_{X}(x)p_{Y}(x).\]

Types of connectedness.In topology, there are several kinds of connectedness. Two of them are particularly important for this work.

**1.)** A topological space \(X\) is _connected_ if it cannot be divided into two disjoint non-empty open sets. If it is not connected, the connected component of a point \(x\in X\) is given by the union of all connected subsets of \(X\) which contain \(x\).

A topological space equal to the Cartesian product of two spaces \(X=Y\times Z\) is connected if and only if \(Y\) and \(Z\) are both connected.

**2.)** A topological space is _path-connected_ if, for every pair of points \(x,y\in X\), there exists a continuous curve \(\gamma:[0,1]\to X\) such that \(\gamma(0)=x,\;\gamma(1)=y\). The _path-component_ of \(x\) is the set of all \(y\in X\) such that a continuous curve exists connecting \(x\) to \(y\).

This second notion is more relevant to our setting, where we care about the possible destinations of the optimization trajectories.

Path-connectedness implies connectedness, but not the opposite. There are situations, however, where these two notions are equivalent. For example, when \(X\) is a topological manifold, \(X\) is connected if and only if \(X\) is path connected.

Notice that the 0-th Betti number \(\beta_{0}(X)\) counts the number of connected components but, in general, not the number of path components. With Lemma 2, we prove that these two notions are equivalent for our object of study.

## Appendix C Extra propositions and lemmas

**Lemma 2**.: _The invariant set \(\mathcal{H}(c)\) is connected if and only if it is path connected._

Proof.: Lemma 1 tells us that \(\mathcal{H}(c)\cong\mathcal{Q}(c_{1})\times\cdots\times\mathcal{Q}(c_{l})\).

Let us focus on a particular \(\mathcal{Q}(c_{k})\).

When \(c_{k}\neq 0\), Proposition 2 tells us that \(\mathcal{Q}(c_{k})\) is a topological manifold, and thus, it is connected if and only if it is path connected.

When \(c_{k}=0\), \(\mathcal{Q}(0)\) is not a topological manifold but contractible, implying that it is connected. Let us prove that it is also path-connected.

Let \(\theta_{k},\theta^{\prime}_{k}\in\mathcal{Q}(0)\) and define the curve \(\gamma:[0,1]\to\mathcal{Q}(0)\)

\[\gamma_{k;\theta}(t)=t\cdot\theta_{k}\]

such that \(\gamma_{k;\theta}(0)=\theta,\;\gamma_{k;\theta}(1)=0\). \(\gamma_{k;\theta}(t)\in\mathcal{Q}(0)\) for every \(t\in[0,1]\) because

\[\llangle\gamma_{k;\theta}(t),\gamma_{k;\theta}(t)\rr\lambda_{k}=t\llangle \theta,\theta\rr\lambda_{k}=0.\]

Therefore, the segment from \(\theta_{k}\) to \(0\) belongs to \(\mathcal{Q}(0)\).

A continuous curve from \(\theta_{k}\) to \(\theta^{\prime}_{k}\) can be thus obtained by

\[\gamma_{k;\theta^{\prime}}\gamma_{k;\theta}(t):=\begin{cases}\gamma_{k;\theta} (2t)&\text{if }t\in[0,\frac{1}{2}]\\ \gamma_{k;\theta^{\prime}}(2-2t)&\text{if }t\in[\frac{1}{2},1]\end{cases}\]

which is continuous because \(\gamma_{k;\theta}(1)=\gamma_{k;\theta^{\prime}}(1)=0\). Therefore \(\mathcal{Q}(0)\) is path connected.

Finally, if \(\mathcal{H}(c)\) is connected, then all of its factors \(\mathcal{Q}(c_{k})\) are connected, which, in turn, is true if and only if they are path-connected. A product of path-connected space is again path-connected, and therefore \(\mathcal{H}(c)\) is path-connected. The other implication is true because path-connectedness implies connectedness, thus concluding the proof.

**Lemma 3** (Interchange of rescalings and permutations).: _Let \(\theta\in\Theta\), \(\alpha\in\mathbb{R}_{+}^{l}\) and \(\pi\in\mathfrak{S}_{l}\), then, if \(\tilde{\alpha}=R_{\pi^{-1}}(\alpha)\)_

\[T_{\alpha}P_{\pi}(\theta)=P_{\pi}T_{\tilde{\alpha}}(\theta).\] (16)

Proof.: Given that

\[T_{\alpha}P_{\pi}(\theta)=(\mathrm{diag}(\alpha)R_{\pi}W^{(1)},W^{(2)}R_{\pi} ^{\top}\mathrm{diag}(\alpha)^{-1})\]

we need to prove that \(\mathrm{diag}(\alpha)R_{\pi}=R_{\pi}\mathrm{diag}(\tilde{\alpha})\).

\[(\mathrm{diag}(\alpha)R_{\pi})_{ij}=\sum_{k=1}^{l}\mathrm{diag}(\alpha)_{ik}(R _{\pi})_{kj}=\alpha_{i}(R_{\pi})_{ij}=\begin{cases}\alpha_{i}&\text{if }j=\pi(i)\\ 0&\text{otherwise}\end{cases}.\]

Let us pick a generic \(\tilde{\alpha}\in\mathbb{R}_{+}^{l}\).

\[(R_{\pi}\mathrm{diag}(\tilde{\alpha}))_{ij}=\sum_{k=1}^{l}(R_{\pi})_{ik} \mathrm{diag}(\tilde{\alpha})_{kj}=(R_{\pi})_{ij}\tilde{\alpha}_{j}=\begin{cases} \tilde{\alpha}_{j}&\text{if }j=\pi(i)\\ 0&\text{otherwise}\end{cases}.\]

Let us consider the inverse permutation \(\pi^{-1}\) so that \(\pi^{-1}(j)=i\) if \(\pi(i)=j\). Then, if \(\tilde{\alpha}=R_{\pi^{-1}}\alpha\),

\[\tilde{\alpha}_{j}=\alpha_{\pi^{-1}(j)}=\alpha_{i}\]

and thus we get that \(\mathrm{diag}(\alpha)R_{\pi}=R_{\pi}\mathrm{diag}(\tilde{\alpha})\). 

## Appendix D Proofs

### Proof of Lemma 1

Proof.: Proposition 1 tells us that the invariant set can be decomposed as the direct sum of the single hidden neurons' parameter spaces. This means that, for every \(\theta\in\Theta\), there exist unique \(\theta_{1}\in\Theta_{1},\ldots,\theta_{l}\in\Theta_{l}\) such that

\[\theta=\theta_{1}+\theta_{2}+\cdots+\theta_{l}.\]

Therefore, we have a linear isomorphism \(\varphi:\Theta_{1}\times\cdots\times\Theta_{k}\to\Theta\)

\[\varphi:(\theta_{1},\ldots,\theta_{l})\mapsto\theta_{1}+\cdots+\theta_{l}=\theta.\]

The invariant set is a subset of \(\Theta\), which is given as the set of solutions of \(l\) equations \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}=c_{k}\ k=1,\ldots,l\). Notice that each of these equations involves a set of variables that appear only in that particular equation. These variables are exactly the ones which belong to \(\Theta_{k}\). In fact \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}=\langle\!\langle\theta_{k}, \theta_{k}\rangle\!\rangle_{k}\).

Therefore, given \(\theta_{1}\in\mathcal{Q}(c_{1}),\ldots,\theta_{l}\in\mathcal{Q}(c_{l})\) we have that

\[\varphi(\theta_{1},\ldots,\theta_{l})=\sum_{k=1}^{l}\theta_{k}\in\mathcal{H}(c).\]

On the opposite, given \(\theta\in\mathcal{H}(c)\) we have that

\[\varphi^{-1}(\theta)=(\theta_{1},\ldots,\theta_{l})\in\mathcal{Q}(c_{1})\times \cdots\times\mathcal{Q}(c_{l}).\]

Therefore, \(\mathcal{H}(c)\) is in bijection with \(\mathcal{Q}(c_{1})\times\cdots\times\mathcal{Q}(c_{l})\) through \(\varphi\) which, being a linear isomorphism, implies also that \(\mathcal{H}(c)\) and \(\mathcal{Q}(c_{1})\times\cdots\times\mathcal{Q}(c_{l})\) are homeomorphic. 

### Proof of Proposition 2

Proof.: Let us consider the three cases separately. If \(c_{k}>0\), \(\mathcal{Q}(c_{k})\) is defined by the equation

\[\sum_{i=1}^{d}\left(W_{ki}^{(1)}\right)^{2}-\sum_{j=1}^{e}\left(W_{jk}^{(2)} \right)^{2}=c_{k}\iff\left\|W_{k}^{(1)}\right\|_{F}^{2}-\left\|W_{k}^{(2)} \right\|_{F}^{2}=c_{k},\]where \(\left\lvert\cdot\right\rvert_{F}\) is the Frobenius norm of a matrix, i.e., the square root of the sum of the squares of its elements. This can be rewritten as

\[\left\lVert W_{k}^{(1)}\right\rVert_{F}=\sqrt{c_{k}+\left\lVert W_{k}^{(2)} \right\rVert_{F}^{2}}\] (17)

where \(\sqrt{c_{k}+\left\lVert W_{k}^{(2)}\right\rVert_{F}^{2}}>0\) because \(c_{k}>0\). We define the map \(h:\mathcal{Q}(c_{k})\to S^{d-1}\times\mathbb{R}^{e}\) as

\[h\left(W_{k}^{(1)},W_{k}^{(2)}\right)=\left(\frac{W_{k}^{(1)}}{\sqrt{c_{k}+ \left\lVert W_{k}^{(2)}\right\rVert_{F}^{2}}},W_{k}^{(2)}\right)\] (18)

where, notice, the first component belongs to the sphere \(S^{d-1}\) because of Equation (17) and \(W_{k}^{(2)}\in\mathbb{R}^{e}\). This map is bijective, differentiable and has the following inverse \(h^{-1}:S^{d-1}\times\mathbb{R}^{e}\to\mathcal{Q}(_{k})\)

\[h^{-1}(u,x)=(\sqrt{c_{k}+\left\lVert v\right\rVert_{F}^{2}}\;u,x)\]

which is differentiable. Therefore, \(h\) is a diffeomorphism from \(\mathcal{Q}(c_{k})\) to \(S^{d-1}\times\mathbb{R}^{e}\).

If \(c_{k}<0\), we write the equation of \(\mathcal{Q}(c_{k})\) as

\[\left\lVert W_{k}^{(2)}\right\rVert_{2}=\sqrt{-c_{k}+\left\lVert W_{k}^{(1)} \right\rVert_{2}^{2}}\]

where \(W_{k}^{(1)}\) and \(W_{k}^{(2)}\) have switched their role to guarantee the term on the right to be positive. The diffeomorphism is now built analogously to Equation (18) as a map \(h:\mathcal{Q}(c_{k})\to S^{e-1}\times\mathbb{R}^{d}\).

If \(c_{k}=0\), we prove that \(\mathcal{Q}(0)\) is a contractible space. To do that, we exhibit a homotopy equivalence between \(\mathcal{Q}(0)\) and the point \(0\), i.e. a continuous map \(p:[0,1]\times\mathcal{Q}(0)\to\mathcal{Q}(0)\) such that \(p(0,\theta_{k})=\theta_{k}\) and \(p(1,\theta_{k})=0\;\forall\theta_{k}\in\mathcal{Q}(0)\). The map is defined in the following way:

\[p(\lambda,\theta_{k})=(1-\lambda)\theta_{k}.\]

This is continuous and well-defined because

\[\langle\!\langle p(\lambda,\theta_{k}),p(\lambda,\theta_{k})\rangle\!\rangle_{ k}=\langle\!\langle(1-\lambda)\theta_{k},(1-\lambda)\theta_{k}\rangle\!\rangle_{ k}=(1-\lambda)^{2}\langle\!\langle\theta_{k},\theta_{k}\rangle\!\rangle_{k}=0,\]

meaning that \(p(\lambda,\theta_{k})\in\mathcal{Q}(0)\) for every \(\theta_{k}\in\mathcal{Q}(0)\) and for every \(\lambda\in[0,1]\). 

### Proof of Theorem 1

Proof.: An implication of the Kunneth formula is that the Poincare polynomial of the Cartesian product of two spaces is equal to the product of their Poincare polynomials:

\[p_{X\times Y}(x)=p_{X}(x)p_{Y}(y).\]

Starting from Proposition 2, we can apply this result to \(\mathcal{Q}(c_{k})\).

\[p_{\mathcal{Q}(c_{k})}=\begin{cases}p_{\mathbb{R}^{e}}(x)p_{S^{d-1}}(x)&\text {if }c_{k}>0\\ p_{\mathbb{R}^{d}}(x)p_{S^{e-1}}(x)&\text{if }c_{k}<0\\ 1&\text{if }c_{k}=0\end{cases}\] (19)

because a contractible space has 1 connected component and all of its other Betti numbers equal to zero.

Moreover, we know that \(\mathbb{R}^{n}\) is contractible for any \(n\) and its Poincare polynomial is \(p_{\mathbb{R}^{n}}(x)=1\). The Poincare polynomial of the sphere \(S^{n}\) is given by \(p_{S^{n}}(x)=1+x^{n}\).

Equation (19) becomes

\[p_{\mathcal{Q}(c_{k})}=\begin{cases}1+x^{d-1}&\text{if }c_{k}>0\\ 1+x^{e-1}&\text{if }c_{k}<0\\ 1&\text{if }c_{k}=0\end{cases}.\] (20)

Given that Lemma 1 tells us that \(\mathcal{H}(c)\) can be factored into the product of the \(\mathcal{Q}(c_{k})\), we apply Kunneth formula and find that

\[p_{\mathcal{H}(c)}(x)=p_{\mathcal{Q}(c_{1})}(x)\cdots p_{\mathcal{Q}(c_{1})}(x )=(1+x^{d-1})^{l_{+}}(1+x^{e-1})^{l_{-}}.\] (21)

### Proof of Proposition 3

Proof.: In the following, we exploit Lemma 2 and use the terminology connected and path-connected interchangeably.

Let us first prove that \(s(\theta)=s(\theta^{\prime})\) means that \(\theta\) and \(\theta^{\prime}\) belong to the same connected component. We do this by explicitly building a continuous curve \(\delta:[0,1]\to\mathcal{H}(c)\) such that \(\delta(0)=\theta\) and \(\delta(1)=\theta^{\prime}\).

Let us proceed by leveraging the homeomorphism from \(\mathcal{H}(c)\) and \(\mathcal{Q}(c_{1})\times\dots\times\mathcal{Q}(c_{l})\) and consider the different components of \(\delta\) in the invariant hyperquadrics associated to each neuron \(\delta=(\delta_{1},\dots,\delta_{l})\).

If \(c_{k}\geq 0\), we know that \(\mathcal{Q}(c_{k})\) is path-connected and therefore we fix \(\delta_{k}(t)\) to any continuous curve in \(\mathcal{Q}(c_{k})\) such that \(\delta_{k}(0)=\theta_{k}\) and \(\delta_{k}(1)=\theta_{k}^{\prime}\).

If \(c_{k}<0\) for \(k\in K:=\{k_{1},\dots,k_{l_{-}}\}\subseteq\{1,\dots,l\}\), we define the curve \(\gamma_{i;\theta}:[0,1]\to\mathcal{Q}(c_{k_{i}})\) with

\[\gamma_{i;\theta}(t)=\left((1-t)W^{(1)}_{k_{i}1},\dots,(1-t)W^{(1)}_{k_{i}l},s (\theta)_{i}\sqrt{-c_{k_{i}}+(1-t)^{2}\norm{W^{(1)}_{k_{i}}}^{2}}\right)\]

for every \(i=1,\dots,l_{-}\).

\(\gamma_{i;\theta}\) is a continuous curve which connects the point \(\gamma_{i;\theta}(0)=\theta_{k_{i}}\) with \(\gamma_{i;\theta}(1)=(0,s(\theta)_{i}\sqrt{-c_{k_{i}}})\).

If we define \(\bar{\gamma}_{i;\theta}(t):=\gamma_{i;\theta}(1-t)\), which is the same curve as \(\gamma_{i;\theta}\) but traversed in the opposite direction, we can define the curve

\[\delta_{k_{i}}(t)=\bar{\gamma}_{i;\theta^{\prime}}\gamma_{i;\theta}(t)\]

i.e. the curve which travels on \(\gamma_{i;\theta}\) for \(t\in[0,\frac{1}{2}]\) and on \(\bar{\gamma}_{i;\theta^{\prime}}\) for \(t\in[\frac{1}{2},1]\), for \(i=1,\dots,l_{-}\).

Notice now that \(\delta_{k_{i}}(0)=\theta_{k_{i}}\) and \(\delta_{k_{i}}(1)=\theta_{k_{i}}^{\prime}\). Moreover \(\delta_{k_{i}}\) is continuous because

\[\gamma_{i;\theta}(1)=(0,s(\theta)_{i}\sqrt{-c_{k_{i}}})=(0,s(\theta^{\prime}) _{i}\sqrt{-c_{k_{i}}})=\bar{\gamma}_{i;\theta^{\prime}}(0)\]

under the hypothesis that \(s(\theta)=s(\theta^{\prime})\).

Finally, we found a continuous curve \(\delta=(\delta_{1},\dots,\delta_{l})\) such that \(\delta(t)\in\mathcal{H}(c)\)\(\forall t\in[0,1]\) and \(\delta(0)=\theta,\ \delta(1)=\theta^{\prime}\). Therefore, \(\theta\) and \(\theta^{\prime}\) belong to the same connected component.

Let us now prove that if \(\theta\) and \(\theta^{\prime}\) belong to the same connected component, then \(s(\theta)=s(\theta^{\prime})\).

Let \(\gamma:[0,1]\to\mathcal{H}(c)\) be a continuous curve in \(\mathcal{H}(c)\) such that \(\gamma(0)=\theta\) and \(\gamma(1)=\theta^{\prime}\).

For each \(k\in K=\{k_{1},\dots,k_{l_{-}}\}\) such that \(c_{k}<0\), we know that \(\gamma_{k}(t)\in\mathcal{Q}(c_{k})\) means that

\[\gamma_{k}(t)=\left(\gamma_{k1}^{(1)}(t),\dots,\gamma_{kd}^{(1)}(t),\underbrace {s_{k}(t)\sqrt{-c_{k}+\norm{\gamma_{k}^{(1)}}_{F}^{2}}}_{\gamma_{k}^{(2)}(t)}\right)\]

for some function \(s_{k}(t)\in\{-1,1\}\) such that \(s_{k}(0)=s(\theta)_{k}\) and \(s_{k}(1)=s(\theta^{\prime})_{k}\).

Assume, by contradiction, that \(s(\theta)_{k}=-s(\theta^{\prime})_{k}\). Assume also that \(s(\theta)_{k}=+1\) and \(s(\theta^{\prime})_{k}=-1\).

Notice that \(c_{k}<0\) implies that \(p(t):=\sqrt{-c_{k}+\norm{\gamma_{k}^{(1)}}_{F}^{2}}>0\).

The function \(\gamma_{k}^{(2)}(t)=s_{k}(t)p(t)\), then, is a continuous function such that \(\gamma_{k}^{(2)}(0)>0\) and \(\gamma_{k}^{(2)}(1)<0\) and thus, by the intermediate value theorem, there exists \(t_{*}\in(0,1)\) such that \(\gamma_{k}^{(2)}(t_{*})=0\).

But \(\gamma_{k}^{(2)}(t)\neq 0\) for every \(t\), as \(s_{k}(t)\in\{-1,1\}\) and \(\sqrt{-c_{k}+\norm{\gamma_{k}^{(1)}}_{F}^{2}}>0\).

Repeating the argument for \(s(\theta)_{k}=-1\) we then prove by contradiction that \(s(\theta)_{k}=s(\theta^{\prime})_{k}\) for all \(k\in K\), thus concluding the proof. 

### Proof of Proposition 4

Proof.: We have by Equation (10) and Equation (2):

\[\langle\!\langle T_{\alpha}(\theta),T_{\alpha}(\theta)\rangle\!\rangle_{k}-c_{k }=0\iff\alpha_{k}^{2}\sum_{i=1}^{d}(W^{(1)}_{ki})^{2}-\frac{1}{\alpha_{k}^{2} }\sum_{j=1}^{e}(W^{(2)}_{jk})^{2}-c_{k}=0\]By renaming \(A=\sum\limits_{i=1}^{d}(W_{ki}^{(1)})^{2}=\left\|W_{k}^{(1)}\right\|_{F}^{2}\), \(C=\sum\limits_{j=1}^{e}(W_{jk}^{(2)})^{2}=\left\|W_{k}^{(2)}\right\|_{F}^{2}\) and multiplying by \(\alpha_{k}^{2}>0\) we have:

\[A\alpha_{k}^{4}-c_{k}\alpha_{k}^{2}-C=0\] (22)

Solving for \(\alpha_{k}^{2}\) gives us:

\[\Delta=c_{k}^{2}+4AC\geq 4AC>0\]

\[\alpha_{k}^{2}=\frac{c_{k}\pm\sqrt{\Delta}}{2A}\]

Given that we want \(\alpha_{k}>0\), we discard the negative solution. The other is positive because \(\Delta>c_{k}^{2}\) and thus \(\sqrt{\Delta}>|c_{k}|\).

\[\alpha_{k}=\pm\sqrt{\frac{c_{k}+\sqrt{\Delta}}{2A}}\]

Of which we keep the positive solution only, with its full expression being:

\[\alpha_{k}=\sqrt{\frac{c_{k}+\sqrt{c_{k}^{2}+4\left\|W_{k}^{(1)}\right\|_{F}^ {2}\left\|W_{k}^{(2)}\right\|_{F}^{2}}}{2\left\|W_{k}^{(1)}\right\|_{F}^{2}}}.\] (23)

Hence, if \(\alpha=(\alpha_{1},\dots,\alpha_{l})\) with \(\alpha_{k}\) given by Equation (23), we get that \(\langle\!\langle T_{\alpha}(\theta),T_{\alpha}(\theta)\rangle\!\rangle_{k}=c_ {k}\ \forall k=1,\dots,l\).

Let us consider now the pathological cases \(W_{k}^{(1)}=0\) or \(W_{k}^{(2)}=0\).

If \(W_{k}^{(1)}=0,W_{k}^{(2)}\neq 0\) then \(A=0,C\neq 0\). Therefore, we have that Equation (22) becomes

\[-c_{k}\alpha_{k}^{2}-C=0\]

which has solutions if and only if \(c_{k}<0\). In that case \(\alpha_{k}=\frac{\left\|W_{k}^{(2)}\right\|_{F}}{\sqrt{-c_{k}}}\). This means that a hidden neuron with zero input weights and nonzero output weights can be rescaled only to the invariant hyperquadrics with \(c_{k}<0\).

If \(W_{k}^{(2)}=0,W_{k}^{(1)}\neq 0\) then \(C=0,A\neq 0\). Therefore, we have that Equation (22) becomes

\[\alpha_{k}^{2}A-c_{k}=0\]

which has solutions if and only if \(c_{k}>0\). In that case \(\alpha_{k}=\frac{\sqrt{c_{k}}}{\left\|W_{k}^{(1)}\right\|_{F}}\). This means that a hidden neuron with zero output weights and nonzero input weights can be rescaled only to the invariant hyperquadrics with \(c_{k}>0\).

If \(W_{k}^{(1)}=0\) and \(W_{k}^{(2)}=0\), then \(\theta_{k}=0\in\mathcal{Q}(0)\) and it cannot be rescaled to any other invariant hyperquadric.

### Proof of Theorem 2

Proof.: Let us first prove that, if for every \(\theta\in C\) there exists a \(\theta^{\prime}\in C^{\prime}\) such that \(\theta\overset{\text{rp}}{\sim}\theta^{\prime}\) then \(\sum\limits_{i=1}^{l}s(\theta)_{i}=\sum\limits_{i=1}^{l}s(\theta^{\prime})_{i}\).

First, Lemma 3 tells us that we can interchange rescaling and permutation if we permute the rescaling factors accordingly. This means we can reduce any composite action of rescalings and permutations to the action of a single rescaling and a single permutation.

Let \(\theta\in C\) and \(\theta^{\prime}\in C^{\prime}\) such that \(\theta\overset{\text{rp}}{\sim}\theta^{\prime}\). Then there exist \(\alpha\in\mathbb{R}^{l}_{+}\) and \(\pi\in\mathfrak{S}_{l}\) such that

\[T_{\alpha}P_{\pi}(\theta)=\theta^{\prime}.\] (24)

This means that \(T_{\alpha}P_{\pi}(\theta)\) and \(\theta^{\prime}\) belong to the same invariant set and, specifically, to the same connected component. Therefore,

\[s(T_{\alpha}P_{\pi}(\theta))=s(\theta^{\prime}).\]

Notice that

\[s(T_{\alpha}P_{\pi}(\theta))=s(P_{\pi}(\theta))\]

because \(T_{\alpha}\) does not change the sign of \(W^{(2)}\) as it acts by scaling it by positive factors. Let us focus on

\[s(P_{\pi}(\theta))=\operatorname{sign}((W^{(2)}R_{\pi}^{\intercal})_{-}).\]

If the neurons of \(\theta\) such that \(c_{k}<0\) are indexed by \(k_{1},k_{2},\ldots,k_{l_{-}}\), we will have that the neurons of \(P_{\pi}(\theta)\) such that \(c_{k}<0\) are indexed by \(\pi(k_{1}),\pi(k_{2}),\ldots,\pi(k_{l_{-}})\). Therefore

\[s(P_{\pi}(\theta))_{i}=\operatorname{sign}(W^{(2)}_{\pi(k_{i})})=(s(\theta)R_ {\pi_{-}}^{\intercal})_{i}\]

for some permutation \(\pi_{-}\in\mathfrak{S}_{l_{-}}\). Therefore, Equation (24) implies that

\[s(\theta^{\prime})=s(P_{\pi}(\theta))=s(\theta)R_{\pi_{-}}^{\intercal}.\]

The action of rescaling and permutation can only reshuffle the label \(s\) of the connected component. This means that

\[s(\theta)R_{\pi_{-}}^{\intercal}=s(\theta^{\prime})\implies\sum_{i=1}^{l_{-} }(s(\theta)R_{\pi_{-}}^{\intercal})_{i}=\sum_{i=1}^{l_{-}}s(\theta^{\prime})_{ i}\implies\sum_{i=1}^{l_{-}}s(\theta)_{i}=\sum_{i=1}^{l_{-}}s(\theta^{\prime})_{i}.\]

Let us now prove the other implication. Let \(s,s^{\prime}\in\mathbb{R}^{l_{-}}\) such that \(\sum\limits_{i=1}^{l_{-}}s_{i}=\sum\limits_{i=1}^{l_{-}}s_{i}^{\prime}\).

Given that their sum is equal, \(s\) and \(s^{\prime}\) have the same number of \(+1\) and \(-1\) and thus there exists a permutation \(\pi_{-}\in\mathfrak{S}_{l_{-}}\) such that \(s^{\prime}=sR_{\pi_{-}}^{\intercal}\).

Let \(\pi\in\mathfrak{S}_{l}\) be the permutation which permutes the neurons such that \(c_{k}<0\) according to \(\pi_{-}\) and leaves the others fixed. In this way \(s(P_{\pi}(\theta))=sR_{\pi_{-}}^{\intercal}=s^{\prime}\).

\(P_{\pi}(\theta)\), however, doesn't belong to \(\mathcal{H}(c)\) but to another invariant set given by \(\mathcal{H}(R_{\pi}c)\).

Applying Proposition 4 we can find a rescaling \(\alpha=\alpha(\pi)\in\mathbb{R}^{l}_{+}\) such that \(T_{\alpha}P_{\pi}(\theta)\in\mathcal{H}(c)\).

Since neurons such that \(c_{k}\geq 0\), are left unchanged by the permutation, we can apply the proposition and we rescale them with \(\alpha_{k}=1\). The permuted neurons are the ones such that \(c_{k}<0\), namely the ones whose weights satisfy \(\left\|W^{(1)}_{k}\right\|_{F}^{2}-\left\|W^{(2)}_{k}\right\|_{F}^{2}<0\), meaning that \(W^{(2)}_{k}\neq 0\).

As noted above, the action of the rescaling doesn't change the sign vector, and thus

\[s(T_{\alpha}P_{\pi}(\theta))=s(P_{\pi}(\theta))=s^{\prime}.\]

If we name \(\theta^{\prime}:=T_{\alpha}P_{\pi}(\theta)\) this result means that we found a \(\theta^{\prime}\overset{\text{rp}}{\sim}\theta\) such that \(\theta^{\prime}\in C^{\prime}\), thus concluding the proof. 

## Appendix E Including biases

Let us consider the case where we include biases. The resulting two-layer neural network can be written as

\[f(x;\theta)=W^{(2)}\sigma(W^{(1)}x+b^{(1)})+b^{(2)},\] (25)

where \(b^{(1)}\in\mathbb{R}^{l}\) and \(b^{(2)}\in\mathbb{R}^{e}\).

To work with this extended set of parameters, we re-define the space

\[\Theta=\left\{\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})\right\}\]and the single hidden neuron spaces

\[\Theta_{k}=\left\{\theta_{k}=(e_{kk}W_{k}^{(1)},e_{kk}b_{k}^{(1)},W_{k}^{(2)}e_{ kk})\right\}\]

where the second bias term \(b^{(2)}\) does not appear because it is not directly involved with the computations of the hidden neurons. This means that we can write

\[\Theta\cong\Theta_{1}\times\cdots\times\Theta_{l}\times\mathbb{R}^{e}\]

where \(\mathbb{R}^{e}\) is included to describe the parameters in \(b^{(2)}\).

The neuron rescaling action now acts on the biases \(b^{(1)}\) as well as the weights:

\[T :\mathbb{R}_{+}\times\Theta_{k}\rightarrow\Theta_{k}\] (26) \[(\alpha,\theta_{k})\mapsto T_{\alpha}(\theta_{k})=(\alpha W_{k}^ {(1)},\alpha b_{k}^{(1)},\frac{1}{\alpha}W_{k}^{(2)})\]

and can be extended to the whole space of parameters

\[T :\mathbb{R}_{+}^{l}\times\Theta \rightarrow \Theta\] (27) \[(\alpha,\theta) \mapsto T_{\alpha}(\theta)=(\mathrm{diag}(\alpha)W^{(1)}, \mathrm{diag}(\alpha)b^{(1)},W^{(2)}\mathrm{diag}(\alpha)^{-1},b^{(2)}).\]

Once again, we find that \(T_{\alpha}\theta\sim\theta\).

In this more general case, we can rewrite the bilinear form to include the biases. If \(\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})\) and \(\eta=(V^{(1)},p^{(1)},V^{(2)},p^{(2)})\), we define

\[\langle\!\langle\theta,\eta\rangle\!\rangle_{k}=\sum_{i=1}^{d}W_{ki}^{(1)}V_{ ki}^{(1)}+b_{k}^{(1)}p_{k}^{(1)}-\sum_{j=1}^{e}W_{jk}^{(2)}V_{jk}^{(2)}\] (28)

and see that, once gradient flow optimization, we have a conservation condition like the one of Equation (9)

\[\langle\!\langle\theta(t),\theta(t)\rangle\!\rangle_{k}=c_{k}\forall t>0\ \forall k=1,\ldots,l.\]

Once again, we call \(\mathcal{Q}(c_{k})\) the hypersurface of \(\Theta_{k}\) which satisfies the equation \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}=c_{k}\) and \(\mathcal{H}(c_{k})\) the set in \(\Theta\) defined by \(\langle\!\langle\theta,\theta\rangle\!\rangle_{k}=c_{k}\ \forall k=1,\ldots,l\).

With this in mind, it is not hard to extend the results of Proposition 2 and Theorem 1 which turn out to be slightly modified.

**Proposition 5**.: _If \(c_{k}>0\), \(\mathcal{Q}(c_{k})\) is a topological manifold homeomorphic to \(\mathbb{R}^{e}\times S^{d}\). If \(c_{k}<0\), \(\mathcal{Q}(c_{k})\) is a topological manifold homeomorphic to \(\mathbb{R}^{d}\times S^{e-1}\). If \(c_{k}=0\), \(\mathcal{Q}(0)\) is contractible._

In this case, we can factor the space of parameters \(\mathcal{H}(c)\) as

\[\mathcal{H}(c)\cong\mathcal{Q}(c_{1})\times\cdots\times\mathcal{Q}(c_{l}) \times\mathbb{R}^{e},\]

where the last factor is due to the freedom in choosing the values of \(b^{(2)}\).

**Proposition 6**.: _Let \(c_{k}\neq 0\ \forall k=1,\ldots,l\). Let \(l_{+},l_{-},l_{0}\) be the number of positive, negative and zero elements of \(c\), respectively. The Poincare polynomial of \(\mathcal{H}(c)\) is given by_

\[p_{\mathcal{H}(c)}(x)=(1+x^{d})^{l_{+}}(1+x^{e-1})^{l_{-}}\] (29)

**Corollary 4**.: _The \(0\)-th Betti number \(\beta_{0}(c)\) of \(\mathcal{H}(c)\), corresponding to the number of its connected components, is given by_

\[\beta_{0}(c)=\begin{cases}1&\text{if }e>1\\ 2^{l_{-}}&\text{if }e=1\end{cases}\] (30)

The result we obtain is similar to Corollary 1 although slightly modified by the fact that having a single input neuron does not cause \(\mathcal{H}(c)\) to become disconnected anymore. In the case of \(e=1\), therefore, the picture presented in the main text is left unchanged.

## Appendix F Probability of obstruction

Let us consider the following question: _what is the probability of having a disconnected invariant set given a realistic initialization?_

Consider a one-layer ReLU neural network with \(e=1\) and assume that the weights are sampled independently of one another from a normal distribution \(W_{ki}^{(1)}\sim\mathcal{N}(0,\sigma_{1}^{2})\ \forall k,i\,\ W_{k}^{(2)}\sim \mathcal{N}(0,\sigma_{2}^{2})\ \forall k,j\). From Corollary 2 we know that the invariant set \(\mathcal{H}(c)\) will be disconnected if and only if there exists a hidden neuron satisfying \(\sum_{i=1}^{d}(W_{ki}^{(1)})^{2}<(W_{k}^{(2)})^{2}\). Given independence of the initial weight sampling, this probability can be computed as

\[\mathbb{P}[\text{obstruction}]=1-\mathbb{P}[\sum_{i=1}^{d}(W_{ki}^{(1)})^{2}>( W_{k}^{(2)})^{2}]^{l}=1-(F_{1,d}(d\sigma_{1}^{2}/\sigma_{2}^{2}))^{l},\]

where \(F\) is the cumulative distribution function of the Fisher-Snedecor distribution.

Having obtained this general expression, we can specify it to two common initialization schemes.

* We obtain _Kaiming initialization_[21] with \(\sigma_{1}^{2}=2/d,\sigma_{2}^{2}=2/l\) resulting in \(\mathbb{P}[\text{obstruction}]=1-F_{1,d}(l)^{l}\).
* We obtain _Xavier normal initialization_[19] with \(\sigma_{1}^{2}=2/(d+l),\sigma_{2}^{2}=2/(1+l)\) resulting in \(\mathbb{P}[\text{obstruction}]=1-F_{1,d}(\frac{d+l}{d+l})^{l}\).

We plot these two expressions in Figure 5. We can see how, for large values of \(d\), the probability of obstruction quickly falls to 0 for any number of hidden neurons. Instead, we see an opposite trend for small values of \(d\): the probability of disconnectedness grows with \(l\). Moreover, it is interesting to notice that the region of high obstruction probability is much larger for Xavier initialization than for Kaiming initialization, further showing why the latter is preferred when working with ReLU networks.

## Appendix G Intuition on the occurrence of obstruction

We can give some intuition on why there is no obstruction for multiple outputs. First, we consider a single hidden neuron \(k\), with \(d\) incoming weights and a single output \(e=1\). If the neuron is pathological, we have that

\[\sum_{i=1}^{d}\left(W_{ki}^{(1)}\right)^{2}<\left(W_{1k}^{(2)}\right)^{2}.\]

Since the weights \(W_{ij}^{(a)}(t)\) are continuous curves in time, for \(W_{1k}^{(2)}\) to change sign, its value needs to pass through 0 but, under the condition above, this cannot happen its square is always positive.

Figure 5: Probability of the topological obstruction as a function of the number of input \(d\) and hidden \(l\) neurons, when the initial weights are sampled with Xavier normal (left) and Kaiming normal (right) initialization schemes.

Consider now multiple outputs \(e>1\), resulting in the conservation condition being

\[\sum_{i=1}^{d}\left(W_{ki}^{(1)}\right)^{2}<\sum_{j=1}^{e}\left(W_{jk}^{(2)} \right)^{2}.\]

Now, any component \(W_{jk}^{(2)}\) can change sign by passing through \(0\) because the other components can compensate for it by increasing their magnitude to keep the condition satisfied.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All the claims in the abstract are supported by analytical results and numerical experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our analysis in Section 8. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All the assumptions of the theoretical results are clearly specified and the complete proofs are written in Appendix D. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The "Task, dataset and model setup." paragraph of Section 6 clearly explains our experimental setup. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Given the simplicity of our setup, we think that releasing the code would be unnecessary. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the experiment's details are explained in Section 6. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our simple experiment has the goal of displaying the meaning of the analytical results and, thus, we think it doesn't require error bars. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Our experiment was performed on a simple laptop. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We reviewd the NeurIPS Code of Ethics and confirm that we conform to it in every repect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Being a purely theoretical work, we believe that it doesn't havesignificantt social impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We release no data or models that have a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We use no existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We release no new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.