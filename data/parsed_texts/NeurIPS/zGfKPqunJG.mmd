# \(E^{3}\): Exploring Embodied Emotion Through

A Large-Scale Egocentric Video Dataset

 Wang Lin\({}^{1,\cdot}\), Yueying Feng\({}^{1,\cdot}\), Wenkang Han\({}^{1,\cdot}\), Tao Jin\({}^{1}\), Zhou Zhao\({}^{1}\),

**Fei Wu\({}^{1}\), Chang Yao\({}^{1}\), Jingyuan Chen\({}^{1,\dagger}\),**

Zhejiang University\({}^{1}\)

{linwanglu, yueyingf, jint_zju, zhaozhou,

wufei, changy, jingyuanchen,}@zju.edu.cn

Equal Contribution.Corresponding Author.

###### Abstract

Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior. Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically. To address this gap, this paper presents \(E^{3}\) for Exploring Embodied Emotion, the first massive first-person view video dataset. \(E^{3}\) contains more than \(50\) hours of video, capturing \(8\) different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we define \(4\) core benchmark tasks - emotion recognition, emotion classification, emotion localization, and emotion reasoning - supported by more than \(80\)k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in first-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that \(E^{3}\) can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents. Project page: https://exploring-embodied-emotion-official.github.io.

## 1 Introduction

Embodied Agents [16], which are capable of simulating human interactions by perceiving the environment and executing corresponding behaviors, have facilitated a wide range of applications [53, 55, 56, 58]. Emotion analysis [39], as a key component of understanding human behavior, is crucial for improving the naturalness and effectiveness of human-computer interaction. To enhance the accuracy of emotional interactions in real-world scenarios, embodied agents necessitate large and diverse datasets for training and validating their algorithms.

Although current emotion analysis techniques have made significant progress in a variety of applications, most works [90, 98, 105] focus on analyzing emotions from a _third-person view_, which involves assessing emotional states from an external observer's standpoint. However, for embodied agents, understanding emotions from a _first-person view_ (FPV) or an _egocentric_ perspective is crucial [60]. In FPV, agents perceive and respond to emotions through their own perceptual system, which is morealigned with human experience. Analyzing emotions from an FPV can provide agents with more personalized and direct emotional information, enabling them to generate more accurate responses.

The goal of this work is to advance research on emotion analysis from an FPV and provide solid support for the emotion interaction capabilities of embodied agents. Our primary contribution is the creation of the dataset \(E^{3}\): a large-scale collection of FPV videos that capture people's activities and emotions in daily life, as shown in Figure 1. The dataset contains \(50\) hours of videos covering \(8\) emotion types in multiple languages including Mandarin, Cantonese, and English. The camera wearers include people of different genders, ages, and occupations, while the video backgrounds cover diverse daily scenes such as home, office, city, and countryside. Meanwhile, the dataset also includes unedited videos and post-edited videos, aiming to support a wider and more robust scenario for emotion analysis. In addition to RGB videos, all data samples are equipped with full multimodal information, including video titles, audio, audio transcribed text, etc., allowing combining multiple modalities together for emotion analysis. As shown in Table 1, \(E^{3}\) addresses a significant deficiency in emotion analysis in egocentric videos by enhancing scale, detail, diversity, and robustness.

Our second contribution involves the introduction of four benchmark tasks as shown in Figure 1 that constitute the core components of embodied emotion analysis - including:

* **Emotion Recognition**: determining the overall emotion tendency of the video.
* **Emotion Classification**: identifying the specific emotion category of each person in the video, including the camera wearer.
* **Emotion Localization**: locating the start and end times of emotion based on a given textual query like _"when does the girl in black feel happy"_.
* **Emotion Reasoning**: reasoning the cause of the emotion in the context based on a given textual query like _"why is the girl in black afraid"_.

To facilitate these tasks, the dataset is equipped with over \(20\)k annotations derived from extensive manual labor totaling more than \(4,000\) hours. These annotations cover semantic and temporal labels, as well as emotion strength, query texts, and human explanations for emotions.

Our third contribution is the construction of a video understanding framework, named **Emotion-LlaMa**, designed for embodied emotion analysis. FPV video analysis requires a comprehensive understanding of the camera wearer's physical environment and the ability to interpret emotions within a human context. Given that the camera wearer is often not visible within the frame, it is essential to integrate multiple modalities in order to fully understand the video content. However, existing video understanding models [7, 32, 45, 108] are typically trained on third-person view

Figure 1: **Examples in \(E^{3}\). \(E^{3}\) collects massive first-person view videos in which the camera wearer (denoted as \(C\)) actively engages in the video activities. Each video in the dataset is annotated manually with fine-grained labels to support four embodied emotion analysis tasks.**

datasets and are primarily designed for activity recognition, emphasizing visual cues while ignoring the acoustic modality. To bridge this gap, our Emotion-LlaMa incorporates an audio branch that captures information such as tone and rhythm, which can be instrumental in emotion analysis, particularly in discerning the emotions of the camera wearer.

We fine-tune Emotion-LlaMa and existing models on \(E^{3}\). Experiments demonstrate the superiority of Emotion-LlaMa and sets the first benchmark for embodied emotion analysis. We expect \(E^{3}\) can pave the way for building embodied agents capable of seamlessly learning human emotions. These agents will be empowered with empathy and seamlessly integrate into human daily interactions.

## 2 Related Work

**Egocentric Video Datasets.** Over the past decade, multimodal understanding [33; 48; 49; 64] has received much attention, numerous egocentric datasets [9; 11; 41; 67] have been developed, presenting a variety of challenging research topics [6; 11; 20; 35; 41]. However, due to the high cost associated with collecting egocentric videos, previous datasets have typically been small-scale and focused on specific domains. Several prominent egocentric datasets [12; 19; 46; 67] have demonstrated the potential of first-person views for action recognition, particularly in hand-object interactions. Conversely, larger egocentric datasets have primarily focused on kitchen environments [13; 44]. Recently, the release of a substantial egocentric video dataset, Ego4D [22], comprising \(3,670\) hours of video content across various indoor and outdoor settings, has attracted attention. However, these videos often lack emphasis on human dialogue and facial expressions, focusing more on actions. This lack of interpersonal communication and emotion expression makes them unsuitable for emotion analysis. In contrast, \(E^{3}\) features diverse scenes with rich human interactions and emotion expressions, bridging the gap in FPV emotional datasets.

**Emotional Analysis Datasets.** Emotion analysis datasets serve as a crucial foundation for advancing research in emotion computing and human-computer interaction. These datasets [37; 68; 97; 101] are commonly sourced from movies, customer reviews, video, and speech video. One of the pioneers in this field is the IEMOCAP [4] dataset, offering a comprehensive multimodal collection for emotion analysis. The DEAP [37] dataset builds upon this by incorporating EEG signals, enabling the study of physiological reactions to emotional stimuli. The MOSI [101] dataset and its successors, such as CMU-MOSEI [102], have further contributed by sourcing data from YouTube videos, annotated with emotional intensity scores, thus facilitating the analysis of emotional expressions in natural settings. The introduction of cross-lingual datasets like CMU-MOSEAS [100], which includes languages such as Spanish(ES), Portuguese(PT), German(DE), and French(FR), has also expanded the scope of emotion analysis research to better accommodate a diverse and global user base.

\begin{table}
\begin{tabular}{l l l l l l l l l} \hline \hline
**Dataset** & **Domain** & **Dur(hrs)** & **flabels** & **Modality** & **Language** & **Emotion?** & **Ego?** & **Example** \\ \hline EmoDB[89] & speech & 03:00 & 800 & a & DE & ✓ & ✗ \\ Large Movie[51] & movie & - & 25,000 & t & EN & ✗ & ✗ \\ SeMANNE[54] & dialogue & 06:30 & 80 & v,a & EN & ✓ & ✗ \\ HUMAINE[84] & diverse & 04:11 & 50 & v,a & various & ✓ & ✗ \\ YouTube[57] & diverse & 00:29 & 300 & v,a & various & ✗ & ✗ \\ SST[80] & movie & - & 11:855 & 1 & EN & ✗ & ✗ \\ ICT-MMMO[96] & movie & 13:58 & 340 & v,a,t & EN & ✗ & ✗ \\ RECOAL[76] & dialogue & 03:50 & 46 & v,a & FR & ✗ & ✗ \\ MOUD[65] & review & 00:59 & 400 & v,a & ES & ✗ & ✗ \\ AFEW[14] & movie & 02:28 & 1,645 & v,a & various & ✓ & ✗ \\ MOSI[101] & diverse & 02:36 & 2,199 & v,a & EN & ✓ & ✗ \\ MOSEI[102] & diverse & 65:53 & 23,453 & v,a & EN & ✓ & ✗ \\ SEWA[38] & adverts & 04:39 & 538 & v,a & EN,DE,EL & ✓ & ✗ \\ CH-SIMS[99] & adverts & - & 2281 & v,a & CH & ✓ & ✗ \\ \hline Disneyworld[18] & disneyland & 42:00 & 15,000 & v,a & EN & ✗ & ✓ \\ EGTEA Gaze[19] & diverse & 28:00 & - & v,a,t & various & ✗ & ✓ \\ BEOID[8] & diverse & - & - & v,a,t & EN & ✗ & ✓ \\ Charades-Ego[78] & home & 34:00 & 30,000 & v,a & EN & ✗ & ✓ \\ EPIC[10] & kitchen & 100:00 & 90,000 & v,a,t & EN & ✗ & ✓ \\ Ego-4D[22] & diverse & 3025:00 & 74000 & v,a,t & various & ✗ & ✓ \\ \hline \(E^{3}\)(Ours) & **diverse** & **71:41** & **81,248** & **v,a,t** & **various** & ✓ & ✓ & 1st-person view \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of our proposed \(E^{3}\) dataset with mainstream emotion analysis datasets (top) and egocentric datasets (bottom).

Despite these advancements, current datasets present limitations, particularly for applications in robotics and augmented reality where inputs are in first-person view. While existing datasets typically feature third-person perspectives, capturing activities without the camera wearer's involvement, thereby impeding the development of embodied agent empathy. Furthermore, exiting datasets focus on emotion recognition and categorization, neglecting the moments and causes of emotions. This oversight results in a superficial understanding of emotion in existing models, indicating a need for datasets that capture first-person experiences and provide deeper insights into the context of emotions.

## 3 \(E^{3}\) for Exploring Embodied Emotion

The dataset, denoted as \(E^{3}\), serves as a comprehensive resource for multiple emotion analysis tasks. It comprises a collection of \(21,998\) videos, each ranging from 10 to 30 seconds in duration, accompanied by human annotations for emotion analysis. This section delves into the details of our data collection and annotation procedures.

### Data Collection and Preprocessing

Videos that satisfy embodied emotion analysis should have both 1) a first-person perspective recording the social activities of the videemaker, and 2) a clear expression of emotion. The videos are collected from 3 popular websites: YouTube, BiliBili and Douyin, with a focus on video blogs (vlogs) that document the daily lives of the individuals behind the camera. These vlogs are typically tagged with the hashtag _#vlog_. The videos in our dataset were recorded in a variety of settings and with different equipment. Some users utilized high-quality microphones and cameras, while others opted for less professional recording devices. The distance between the camera and the user varied, as did the background and lighting conditions. All videos are maintained in their original resolution and saved in MP4 format. The length of the videos ranges from 3 to 5 minutes.

To facilitate the annotation process, we segment the videos into shorter clips through shot detection, typically ranging from 10 to 30 seconds. These clips encapsulate the entire event and serve as the fundamental unit for analysis. Note that the segmentation is seamless, allowing researchers to easily reassemble the clips for long-term video analysis. Moreover, in cases where the videos do not contain subtitles, we employ Whisper [73] for speech recognition. This technology enables us to generate subtitles for the videos, aiding annotators in their labeling process.

### Annotation Construction

We utilize a third-party vendor to annotate our data (see more details in Appendix). Prior to the annotation process, human annotators go through a strict training phase. We employed a total of \(26\) people, including \(14\) females and \(12\) males, and a variety of educational backgrounds to cover most human understanding of emotions. The entire annotation process took more than \(5,500\) hours.

**Stage 1: Emotion Recognition Annotation.** In this stage, the annotator first filters videos that do not meet the embodied emotion analysis, such as videos that are not in FPV or where the camera wearer is not involved in the activity. Then they are asked to determine whether the emotion of the whole video is positive or negative with an emotion intensity score [101], which is defined from strongly negative to strongly positive with a linear scale from \(-3\) to \(+3\). Considering that the judgment of emotion intensity is very subjective, we employ \(3\) annotators to annotate the same video, and their average scores are rounded as the final annotation. Finally, videos with an emotion intensity of \(0\), _i.e.,_ videos with no recognizable emotion, are also excluded from further analysis.

**Stage 2: Emotion Classification Annotation.** Following the completion of Stage 1, the annotator proceeds to further annotate the emotions exhibited by individuals in the video. A total of \(8\) emotion categories were designed, with markers denoting emotion intensity ranging from weak to strong (1 to 3). Six of these emotion categories, namely _(Happy, Sad, Angry, Scared, Disgusted, and Surprised)_ are from Ekman [17]. The other two emotions _{Shy, and Sarcastic}_ are identified as hidden emotions based on feedback from annotators. Hidden emotions pose a significant challenge in emotion analysis tasks, as they require contextual analysis for accurate interpretation. To the best of our knowledge, our dataset is the first to include hidden emotions in emotion analysis.

Each video typically involves multiple characters, including the camera wearer and other individuals who appear in the video. To facilitate emotion analysis of these characters, we opt to distinguish them based on their physical attributes, such as "woman with glasses", rather than using specific identity nouns like _mother_ or _boyfriend_.

**Stage 3: Emotion Localization Annotation.** In this stage, the annotators identify the specific moments when emotions begin and end for each character based on the categorization of emotions assigned to each character in Stage 2. The start and end times of these emotions are pinpointed by paying attention to the corresponding sounds or expressions made by the characters. It is important to note that emotions from multiple characters may overlap during the same time period, as they are actively engaged in a real-time socialization activity.

**Stage 4: Emotion Reasoning Annotation.** In this stage, the annotator is tasked with analyzing the specific reasons behind each emotion exhibited by the characters, based on the contextual information provided. To formulate the question text, a standardized template of "_why [person] [emotion]?_" is used. This helps guide the annotator in identifying the underlying reasons for the character's emotions. In the answer text, the annotator is required to objectively describe the event that triggered the emotion and explain the relationship between this event and the character's emotional response. To ensure the accuracy and quality of the answer text, a separate group of \(10\) annotators will review it for spelling mistakes and grammatical errors.

### Dataset Analysis

**Statistics.**\(E^{3}\) is the first egocentric video emotion analysis dataset. Table 1 illustrates a comparison between \(E^{3}\) and other existing video datasets. \(E^{3}\) introduces a unique perspective by providing first-person video data, where the camera wearer actively participates in emotion-related activities rather than observing from a distance. This dataset offers a more direct view of the facial expressions and body movements of individuals involved in interactions, thereby enhancing the training of empathetic embodied agents. In terms of annotated labels, \(E^{3}\) contains \(4\) tasks with a total of \(80\)k annotations, making it the largest video dataset for emotion analysis. Furthermore, \(E^{3}\) features diverse linguistic content, including _Mandarin_, _English_, as well as languages such as _Cantonese_, _Japanese_, and _Korean_ that are not present in other datasets.

Compared to existing FPV video datasets, \(E^{3}\) bridges a crucial gap in the field of emotion analysis for embodied agents. While both datasets offer comprehensive multimodal information, \(E^{3}\) focuses more on social interactions, whereas the first-person video dataset focuses more on actions. As a result, the acoustic (tone of voice, emotional expressions like _crying_ and _laughing_) and textual (dialogues) modalities in \(E^{3}\) contain richer information.

**Distribution of Videos and Answer Texts.** In total, we have annotated \(18\)k videos and over \(70\)k labels. Figure 2(a) shows the distribution of video durations and answer lengths. The average duration of videos is \(11.5\)s and the average length of answer texts is \(18\) words. Notably, the average video duration of \(E^{3}\) is longer compared to existing emotion analysis datasets, which typically have an average video duration of \(6.2\)s. This indicates that our videos contain more emotional context information. Additionally, Figure 2(a) illustrates the distribution of video scenes, with \(E^{3}\) covering a wide range of daily scenes such as living rooms, streets, and offices.

**Distribution of Emotions.** Figure 2(b) shows the distribution of emotion timestamps and video topics. Most emotions happen within the \(2\)nd-\(5\)th seconds of the video, and the average duration of emotions is \(2.62\)s. Specifically, the shortest duration is observed for the emotion _surprised_ at \(1.68\)s,

Figure 2: Visualization of static analysis of \(E^{3}\): **(a)** Distribution of video duration, answer length and scenes; **(b)** Distribution of emotions timestamps and topics; **(c)** Distribution of emotion categories.

while emotions like _happy_ and _sarcastic_ tend to last longer, averaging \(2.71\)s and \(3.39\)s, respectively. Moreover, the video content of \(E^{3}\) encompasses a variety of topics including _family_, _food_, and _shopping_. Figure 2(c) displays the distribution of the \(8\) emotions in the video clips of \(E^{3}\). On average, each video clip contains \(1.32\) emotions. The emotion _happy_ is the most frequently occurring emotion (40.55%), while _disgusted_ is the least prevalent emotion (2.13%). Additionally, each emotion was assigned an intensity level ranging from \(1\) to \(3\), with the majority of emotions exhibiting an intensity of \(1\), totaling at \(10,953\).

### Privacy Protection

When constructing \(E^{3}\), we follow the principles of privacy protection and copyright respect. We obtain explicit consent from video owners and ensure that they understand the use of personal information (_e.g._, portrait, voice). We comply with the data protection regulations of _YouTube_, _Douyin_, and _Bililil_, providing transparency to participants and guaranteeing their right to withdraw consent. We collect only necessary data and ensure strict data usage boundaries to support research while protecting privacy. Further details can be found in Appendix.

## 4 Methods

We first briefly review the structure of existing multimodal Large Language models (MLLMs) for video understanding. Subsequently, we introduce **Emotion-LlaMa**, a novel model integrates the acoustic modality to enhance the emotion comprehension of embodied videos.

**Preliminary.** MiniGPT4-Video [1] is a significant advancement in the field of MLLMs tailored for video understanding. Building upon the foundation established by LlaMa [86], MiniGPT4-Video utilizes EVA-CLIP [83] to align visual modality with textual modality and maps them to the language space in LlaMa using a linear layer. Notably, MiniGPT4-Video optimizes efficiency by consolidating 4 adjacent visual tokens in each frame into a single token, thereby reducing the overall number of tokens. By combining visual token of each sampled frame with the corresponding textual token from frame subtitle, MiniGPT4-Video enables LlaMa to better interpret video content.

**Emotion-LlaMa.** In this study, we propose Emotion-LlaMa, an extended model based on MiniGPT4-Video, aiming to analyze emotions in an FPV video. It is important to note that relying solely on visual modality for embodied emotion analysis may be insufficient for several reasons. First, the camera wearers are often not visible in the frame, which limits the visual cues available for emotion inference. Second, there exists a strong correlation between audio cues such as pitch and rhythm and emotional expression. Therefore, it is essential to incorporate acoustic modality into the emotion analysis process. To address this, we have incorporated an audio feature branch into MiniGPT4-Video. Specifically, we employ an advanced audio encoder, AudioMAE [30], to convert audio inputs to audio features. Then these audio features are mapped to the language space of LLMs through an audio linear layer. In this way, Emotion-LlaMa can combine multimodal information to comprehensively analyze emotions in FPV videos.

## 5 Benchmarks

In this section, we present a challenging suite of benchmark tasks. The four benchmark tasks include 1) embodied emotion recognition; 2) embodied emotion categorization; 3) embodied emotion localization; and 4) embodied emotion reasoning. Each task and the specialized baselines are

Figure 3: **Overview of Emotion-LlaMa.**

described below. Additionally, detailed implementation instructions and qualitative examples are included in Appendix.

### Embodied Emotion Recognition

**Motivation.** The ability to recognize emotions is crucial for embodied agents, as it enables them to understand the emotional tone of interactions. By being able to perceive the emotional tone of ongoing activities, these agents can provide responses that are sensitive to the emotional context, showing empathy and enhancing the user experience. This capability is essential for creating emotionally intelligent interactions, which in turn improves the relationship between the user and the agent.

**Task Definition.** Embodied emotion recognition is formulated as a classification task. Given an FPV video, the model is required to predict the overall emotional tendency of the video. Emotions are categorized into positive and negative groups, with intensities ranging from strongly negative (-\(3\)) to strongly positive (+\(3\)). The dataset consists of \(8912\) samples for training, \(874\) for validation, and \(891\) for testing. The effectiveness of the model is evaluated using average accuracy (Acc) and F1 score.

**Experimental Results.** As shown in Table 2, the first part presents the results of training task-specific models: MMIM [27], CENET [91], and ALMT [103] on \(E^{3}\). The second part presents the zero-shot and fine-tuning (highlighted in grey) results of MLLMs. The experimental results indicate that task-specific models exhibit lower emotion recognition capabilities compared to the zero-shot recognition capabilities of MLLMs, likely due to their limited parameter amount. However, upon fine-tuning, there is a noticeable improvement in the performance of MLLMs. Specifically, VAST [7] outperforms MINI [1] in both zero-shot and fine-tuning scenarios. Our Emotion-LlaMa model, after fine-tuning, achieves the highest performance, highlighting the advantages of incorporating audio features.

### Embodied Emotion Classification

**Motivation.** Unlike emotion recognition, which provides an overall emotional assessment, embodied emotion classification focuses on identifying specific emotions of each individual. This detailed approach is essential for applications requiring precise emotional discernment, such as psychotherapy [23; 26; 95; 88] and interactive chatbots [5; 70; 87]. By being able to classify emotions on an individual level, embodied agents can provide personalized responses that are finely tuned to the specific contextual cues of the interaction.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Methods & R1,U@0.3 & R1,U@0.5 & R1,U@0.7 & R5,U@0.3 & R5,U@0.5 & R5,U@0.7 & mIoU \\ \hline MSAT[104] & 31.74 & 15.69 & 7.11 & 66.05 & 39.71 & 19.73 & 25.41 \\ r2-tuning[50] & 33.62 & 17.18 & 7.61 & 69.57 & 38.53 & 19.26 & 21.73 \\ \hline VTG-LLM[25] & 15.46 & 7.73 & 1.95 & - & - & - & 12.01 \\ VTG-LLM[25] & 28.68 & 17.61 & 5.14 & - & - & - & 19.42 \\ TimeChat[75] & 18.28 & 6.87 & 1.84 & - & - & - & 14.94 \\ TimeChat[75] & 30.31 & 17.67 & 5.76 & - & - & - & 20.58 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparing different approaches on **embodied emotion localization** task. ‘-’ indicates that LLMs based models cannot generate multiple localization candidates simultaneously.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Methods & Acc & F1 & Methods & happy & angry & swriped & shny & sarcate & sad & disguest & scared & avg \\ \hline MMIM & 73.12 & 75.51 & & & & & & & \\ CENET & 75.60 & 77.17 & & & & & & & \\ ALMT & 77.11 & 78.72 & & & & & & & \\ \hline MALMM & 47.82 & 61.33 & & & & & & & \\ MALMM & 53.50 & 67.02 & & & & & & & \\ VAST & 81.50 & 91.33 & & & & & & & \\ VAST & 84.92 & 92.71 & & & & & & & \\ MINI & 80.91 & 90.52 & & & & & & & \\ MINI & 83.83 & 92.64 & & & & & & & \\ \hline Ours & **85.93** & **93.70** & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of **embodied emotion recognition** task.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Methods & happy & angry & swriped & shny & sarcate & sad & disguest & scared & avg \\ \hline MMIM & 44.63 & 37.26 & 35.79 & 9.80 & 6.42 & 11.57 & 10.17 & 3.32 & 37.76 \\ CENET & 56.85 & 40.52 & 32.83 & 5.26 & 7.01 & 3.77 & 7.01 & 6.74 & 43.30 \\ CTEN & 52.72 & 40.14 & **40.10** & 6.59 & 9.41 & 6.54 & 5.56 & 10.20 & 42.37 \\ \hline MALMM & 57.37 & 1.72 & 14.55 & 8.00 & 2.38 & 7.29 & 0 & 0 & 35.44 \\ MALMM & 3.58 & 16.48 & 31.81 & 5.48 & 5.33 & 6.06 & **12.50** & 2.53 & 18.10 \\ VAST & 53.51 & 53.13 & 6.85 & 5.34 & 0 & 16.67 & 0 & 3.85 & 33.51 \\ VAST & 63.27 & 40.15 & 3.88 & 8.96 & 8.53 & 3.39 & 11.76 & 11.24 & 43.55 \\ MINI & 80.91 & 90.52 & & & & & & & \\ MINI & 83.83 & 92.64 & & & & & & & \\ \hline Ours & **85.93** & **93.70** & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparing different approaches on **embodied emotion classification** task. Fine-tuning methods are highlighted in grey.

**Task Definition.** Embodied emotion classification is defined as a closed-set question-and-answer task. Given an FPV video and a query text specifying a person (including the camera wearer), the model is tasked to classify the person's emotion. The model is expected to assign one of eight common emotion categories to the person, each of which is associated with an intensity score ranging from \(1\) to \(3\). The dataset comprises \(8912\) training samples, \(900\) for validation, and \(1472\) for testing. The evaluation metrics include the F1 score for each class and the overall Micro-F1 score.

**Experimental Results.** We first evaluate several task-specific models: DFAN [71], VAANet [107] and CTEN [106]. As shown in Table 3, previous task-specific models outperform zero-shot MLLMs but are inferior to fine-tuned MLLMs on the average F1 score. After fine-tuning, MLLMs have enhanced their ability to perceive the emotion of the person in video. Emotion-LlaMa performs almost on par with task-specific models in terms of F1 score. It is worth noting that the Micro-F1 score of MALMM decreased after fine-tuning, due to its long-term memory bank storing reference histories that contradict the predicted target emotions.

### Embodied Emotion Localization

**Motivation.** This benchmark aims to identify the precise moments when specific emotions occur in FPV videos. Grasping the timing of emotional occurrences is essential for crafting systems capable of delivering timely and contextually appropriate interventions or responses. This capability is especially important for applications such as interactive storytelling [2, 66, 81], mental health monitoring [31, 79, 85], and assistive technologies [36, 72, 92], where accurately identifying emotional cues can significantly enhance user engagement and the effectiveness of the system.

**Task Definition.** Embodied emotion localization is defined as a regression task. Given an FPV video and a text query specifying a character and an emotion, the model is asked to predict the start and end times of that emotion in the video. The dataset used for this benchmark is the same as that described in Section 5.2. The evaluation metrics are "mIoU" which indicates the mean intersection over union and "R\(n\),U@\(m\)" which represents the percentage of testing samples that have at least one of the top-\(n\) results with IoU larger than \(m\). More details can be found in Appendix.

**Experimental Results.** As shown in Table 4, the experimental results indicate that directly adapting current localization models, such as MSAT [104] and r2-tuning [50], does not yield satisfactory results, due to the gap between embodied emotion localization and existing activity localization tasks. The majority of MLLMs process video frames by sampling and encoding them into visual tokens, which often leads to a lack of temporal awareness necessary for localization and hinders their application to this task. Therefore, we fine-tune VTG-LLM [25] and TimeChat [75] which specifically design timestamp knowledge encoders for localization. While these methods may initially perform less effectively in zero-shot scenarios compared to task-specific models, they have shown competitive results after fine-tuning, showcasing their potential for emotion localization. In addition, we find that VTG-LLM and TimeChat tend to output longer intervals during zero-shot inference compared to inference after fine-tuning. This is mainly because they use action localization datasets in the pre-training stage, which have longer action time intervals.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Methods & IC & DO & CU & TUC & Bleu-3 & Bleu-4 & Rough-L & Cider \\ \hline HCRN & 0.77 & 1.24 & 1.29 & 0.56 & 8.64 & 5.10 & 27.86 & 17.99 \\ UMT & 0.85 & 1.35 & 1.36 & 0.66 & 8.70 & 5.11 & 28.79 & 18.20 \\ MGN & 0.88 & 1.41 & 1.55 & 0.67 & 9.01 & 5.43 & 28.91 & 18.27 \\ \hline MALMM & 1.31 & 1.32 & 1.74 & 1.51 & 3.12 & 2.98 & 9.74 & 10.67 \\ MALMM & 1.41 & 1.83 & 1.95 & 1.59 & 18.82 & 13.87 & 32.86 & 59.21 \\ VAST & 0.98 & 1.30 & 1.56 & 1.27 & 8.06 & 5.06 & 22.46 & 15.67 \\ VAST & 2.00 & 2.07 & 2.51 & 1.87 & 19.75 & 15.61 & 36.14 & 75.51 \\ MINI & 2.05 & 2.02 & 2.44 & 2.03 & 4.29 & 3.00 & 19.01 & 19.18 \\ \hline MINI & 2.22 & 2.23 & 2.63 & 2.06 & 18.62 & 14.95 & 34.68 & 81.22 \\ \hline Ours & **2.39** & **2.39** & **2.77** & **2.18** & **20.64** & **15.99** & **37.20** & **84.06** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparing different approaches on **embodied emotion reasoning** task. Our evaluation employs GPT, assessing responses for information correctness (IC), detailed orientation (DO), contextual understanding (CU), and temporal understanding consistency (TUC), alongside standard text quality metrics.

[MISSING_PAGE_FAIL:9]

## References

* [1] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal lms for video understanding with interleaved visual-textual tokens. _arXiv preprint arXiv:2404.03413_, 2024.
* [2] Alexandr BONTA. _Development of a multi-agent system for enhanced interactive storytelling_. PhD thesis, Universitatea Tehnica a Moldovei, 2024.
* [3] Elizabeth Broadbent, Mark Billinghurst, Samantha G Boardman, and P Murali Doraiswamy. Enhancing social connectedness with companion robots using ai. _Science Robotics_, 8(80):eadi6347, 2023.
* [4] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. Iemocap: Interactive emotional dyadic motion capture database. _Language resources and evaluation_, 42:335-359, 2008.
* [5] Mark Anthony Camilleri and Ciro Troise. Live support by chatbots with artificial intelligence: A future research agenda. _Service Business_, 17(1):61-80, 2023.
* [6] Chien-Yi Chang, De-An Huang, Danfei Xu, Ehsan Adeli, Li Fei-Fei, and Juan Carlos Niebles. Procedure planning in instructional videos. In _European Conference on Computer Vision_, pages 334-350. Springer, 2020.
* [7] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: A vision-audio-subtitle-text omni-modality foundation model and dataset. _Advances in Neural Information Processing Systems_, 36, 2024.
* [8] D Damen, T Leelasawassuk, O Haines, M Wray, D Moltisanti, A Calway, and W Mayol-Cuevas. Bristol egocentric object interactions dataset. _URL: http://people. cs. bris. ac. uk/ damen/BEOID/index. htm_, 2016.
* [9] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In _Proceedings of the European conference on computer vision (ECCV)_, pages 720-736, 2018.
* [10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. The epic-kitchens dataset: Collection, challenges and baselines. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(11):4125-4141, 2020.
* [11] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100. _International Journal of Computer Vision_, pages 1-23, 2022.
* [12] Dima Damen, Teesid Leelasawassuk, Osian Haines, Andrew Calway, and Walterio W Mayol-Cuevas. You-do, i-learn: Discovering task relevant objects and their modes of interaction from multi-user egocentric video. In _BMVC_, volume 2, page 3. Citeseer, 2014.
* [13] Fernando De la Torre, Jessica Hodgins, Adam Bargteil, Xavier Martin, Justin Macey, Alex Collado, and Pep Beltran. Guide to the carnegie mellon university multimodal activity (cmu-mmac) database. 2009.
* [14] Abhinav Dhall, OV Ramana Murthy, Roland Goecke, Jyoti Joshi, and Tom Gedeon. Video and image based emotion recognition challenges in the wild: Emotiw 2015. In _Proceedings of the 2015 ACM on international conference on multimodal interaction_, pages 423-426, 2015.
* [15] Gilbert Dizon. Affordances and constraints of intelligent personal assistants for second-language learning. _RELC Journal_, 54(3):848-855, 2023.
* [16] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. A survey of embodied ai: From simulators to research tasks. _IEEE Transactions on Emerging Topics in Computational Intelligence_, 6(2):230-244, 2022.

* Ekman et al. [1980] Paul Ekman, Wallace V Freisen, and Sonia Ancoli. Facial signs of emotional experience. _Journal of personality and social psychology_, 39(6):1125, 1980.
* Fathi et al. [2012] Alircza Fathi, Jessica K Hodgins, and James M Rehg. Social interactions: A first-person perspective. In _2012 IEEE Conference on Computer Vision and Pattern Recognition_, pages 1226-1233. IEEE, 2012.
* Fathi et al. [2012] Alireza Fathi, Yin Li, and James M Rehg. Learning to recognize daily actions using gaze. In _Computer Vision-ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12_, pages 314-327. Springer, 2012.
* Furnari and Farinella [2020] Antonino Furnari and Giovanni Maria Farinella. Rolling-unrolling lstms for action anticipation from first-person video. _IEEE transactions on pattern analysis and machine intelligence_, 43(11):4021-4036, 2020.
* Gonulal [2023] Talip Gonulal. Investigating efl learners' humorous interactions with an intelligent personal assistant. _Interactive Learning Environments_, 31(7):4521-4534, 2023.
* Grauman et al. [2022] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18995-19012, 2022.
* Grunder et al. [2024] Gerhard Grunder, Manuela Brand, Lea J Mertens, Henrik Jungaberle, Laura Kartner, Dennis J Scharf, Moritz Spangemacher, and Max Wolff. Treatment with psychedelics is psychotherapy: beyond reductionism. _The Lancet Psychiatry_, 11(3):231-236, 2024.
* Guo and Luo [2023] Wenshan Guo and Qiangqiang Luo. Investigating the impact of intelligent personal assistants on the purchase intentions of generation z consumers: The moderating role of brand credibility. _Journal of Retailing and Consumer Services_, 73:103353, 2023.
* Guo et al. [2024] Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, and Bo Zhao. Vtg-llm: Integrating timestamp knowledge into video l1ms for enhanced video temporal grounding. _arXiv preprint arXiv:2405.13382_, 2024.
* Haber et al. [2024] Yuval Haber, Inbar Levkovich, Dorit Hadar-Shoval, and Zohar Elyoseph. The artificial third: a broad view of the effects of introducing generative artificial intelligence on psychotherapy. _JMIR Mental Health_, 11:e54781, 2024.
* Han et al. [2021] Wei Han, Hui Chen, and Soujanya Poria. Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 9180-9192, 2021.
* He et al. [2024] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. _arXiv preprint arXiv:2404.05726_, 2024.
* Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Huang et al. [2022] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. In _NeurIPS_, 2022.
* Jayanthi et al. [2024] S Jayanthi, V Priyadharshini, V Kirithiga, and S Premalatha. Mental health status monitoring for people with autism spectrum disorder using machine learning. _International Journal of Information Technology_, 16(1):43-51, 2024.
* Jin et al. [2023] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. _arXiv preprint arXiv:2311.08046_, 2023.

* [33] Tao Jin, Wang Lin, Ye Wang, Linjun Li, Xize Cheng, and Zhou Zhao. Rethinking the multimodal correlation of multimodal sequential learning via generalizable attentional results alignment. In _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5247-5265, 2024.
* [34] Eunbin Kang and Youn Ah Kang. Counseling chatbot design: The effect of anthropomorphic chatbot characteristics on user self-disclosure and companionship. _International Journal of Human-Computer Interaction_, 40(11):2781-2795, 2024.
* [35] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual temporal binding for egocentric action recognition. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 5492-5501, 2019.
* [36] Alex Khang. _Medical Robotics and AI-Assisted Diagnostics for a High-Tech Healthcare Industry_. IGI Global, 2024.
* [37] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological signals. _IEEE transactions on affective computing_, 3(1):18-31, 2011.
* [38] Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn Schuller, et al. Sewa db: A rich database for audio-visual emotion and sentiment research in the wild. _IEEE transactions on pattern analysis and machine intelligence_, 43(3):1022-1040, 2019.
* [39] Songning Lai, Xifeng Hu, Haoxuan Xu, Zhaoxia Ren, and Zhi Liu. Multimodal sentiment analysis: A survey. _Displays_, page 102563, 2023.
* [40] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9972-9981, 2020.
* [41] Yong Jae Lee, Joydeep Ghosh, and Kristen Grauman. Discovering important people and objects for egocentric video summarization. In _2012 IEEE conference on computer vision and pattern recognition_, pages 1346-1353. IEEE, 2012.
* [42] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models, 2023.
* [43] Linjun Li, Tao Jin, Wang Lin, Hao Jiang, Wenwen Pan, Jian Wang, Shuwen Xiao, Yan Xia, Weihao Jiang, and Zhou Zhao. Multi-granularity relational attention network for audio-visual question answering. _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.
* [44] Yin Li, Miao Liu, and James M Rehg. In the eye of beholder: Joint learning of gaze and actions in first person video. In _Proceedings of the European conference on computer vision (ECCV)_, pages 619-635, 2018.
* [45] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_, 2023.
* [46] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 7083-7093, 2019.
* [47] Wang Lin, Jingyuan Chen, Jiaxin Shi, Yichen Zhu, Chen Liang, Junzhong Miao, Tao Jin, Zhou Zhao, Fei Wu, Shuicheng Yan, et al. Non-confusing generation of customized concepts in diffusion models. _arXiv preprint arXiv:2405.06914_, 2024.
* [48] Wang Lin, Tao Jin, Wenwen Pan, Linjun Li, Xize Cheng, Ye Wang, and Zhou Zhao. Tavt: Towards transferable audio-visual text generation. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 14983-14999, 2023.

* [49] Wang Lin, Tao Jin, Ye Wang, Wenwen Pan, Linjun Li, Xize Cheng, and Zhou Zhao. Exploring group video captioning with efficient relational approximation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 15281-15290, 2023.
* [50] Ye Liu, Jixuan He, Wanhua Li, Junsik Kim, Donglai Wei, Hanspeter Pfister, and Chang Wen Chen. \(\bm{\Omega}\)-tuning: Efficient image-to-video transfer learning for video temporal grounding. _arXiv preprint arXiv:2404.00801_, 2024.
* [51] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies_, pages 142-150, 2011.
* [52] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. _arXiv preprint arXiv:2306.05424_, 2023.
* [53] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Tingfan Wu, Jay Vakil, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? _Advances in Neural Information Processing Systems_, 36, 2024.
* [54] Gary McKeown, Michel Valstar, Roddy Cowie, Maja Pantic, and Marc Schroder. The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. _IEEE transactions on affective computing_, 3(1):5-17, 2011.
* [55] Bahar Memarian and Tenzin Doleck. Embodied ai in education: A review on the body, environment, and mind. _Education and Information Technologies_, 29(1):895-916, 2024.
* [56] Shrestha Mohanty, Negar Arabzadeh, Julia Kiseleva, Artem Zholus, Milagro Teruel, Ahmed Awadallah, Yuxuan Sun, Kavya Srinet, and Arthur Szlam. Transforming human-centered ai collaboration: Redefining embodied agents capabilities through interactive grounded language instructions. _arXiv preprint arXiv:2305.10783_, 2023.
* [57] Louis-Philippe Morency, Rada Mihalcea, and Payal Doshi. Towards multimodal sentiment analysis: Harvesting opinions from the web. In _Proceedings of the 13th international conference on multimodal interfaces_, pages 169-176, 2011.
* [58] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. _Advances in Neural Information Processing Systems_, 36, 2024.
* [59] Zahra Norouzi, Fatemeh Amirkhani, and Saeedeh Babai. Counselor bots as mental healthcare assistants and some ethical challenges. In _Social Robots in Social Institutions_, pages 100-109. IOS Press, 2023.
* [60] Alexander Obaigbena, Oluwaseun Augustine Lottu, Ejike David Ugwuanyi, Boma Sonimitiem Jacks, Enoch Oluwademillade Sodiya, and Obinna Donald Daraojimba. Ai and human-robot interaction: A review of recent advances and challenges. _GSC Advanced Research and Reviews_, 18(2):321-330, 2024.
* [61] Kaihang Pan, Zhaoyu Fan, Juncheng Li, Qifan Yu, Hao Fei, Siliang Tang, Richang Hong, Hanwang Zhang, and Qianru Sun. Towards unified multimodal editing with enhanced knowledge collaboration. _arXiv preprint arXiv:2409.19872_, 2024.
* [62] Kaihang Pan, Juncheng Li, Hongye Song, Hao Fei, Wei Ji, Shuo Zhang, Jun Lin, Xiaozhong Liu, and Siliang Tang. Controlretiever: Harnessing the power of instructions for controllable retrieval. _arXiv preprint arXiv:2308.10025_, 2023.
* [63] Kaihang Pan, Juncheng Li, Hongye Song, Jun Lin, Xiaozhong Liu, and Siliang Tang. Self-supervised meta-prompt learning with meta-gradient regularization for few-shot generalization. _arXiv preprint arXiv:2303.12314_, 2023.

* [64] Kaihang Pan, Siliang Tang, Juncheng Li, Zhaoyu Fan, Wei Chow, Shuicheng Yan, Tat-Seng Chua, Yueting Zhuang, and Hanwang Zhang. Auto-encoding morph-tokens for multimodal llm. _arXiv preprint arXiv:2405.01926_, 2024.
* [65] Veronica Perez-Rosas, Rada Mihalcea, and Louis-Philippe Morency. Utterance-level multimodal sentiment analysis. In _Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 973-982, 2013.
* [66] Marina PETICI. _The integration of Theory of Mind into intelligent agents for enhanced interactive storytelling_. PhD thesis, Universitatea Tehnica a Moldovei, 2024.
* [67] Hamed Pirsiavash and Deva Ramanan. Detecting activities of daily living in first-person camera views. In _2012 IEEE conference on computer vision and pattern recognition_, pages 2847-2854. IEEE, 2012.
* [68] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations. _arXiv preprint arXiv:1810.02508_, 2018.
* [69] D Povey, A Ghoshal, G Boulianne, L Burget, O Glembek, N Goel, M Hannemann, P Motlicek, Y Qian, P Schwarz, et al. The kaldi speech recognition toolkit. inieee 2011 workshop on automatic speech recognition and understanding. ieee signal processing society, dec. 2011. _IEEE Catalog No.: CFP1ISRW-USB_.
* [70] Fawaz Qasem, Mukhtar Ghaleb, Hassan Saleh Mahdi, Ahmed Al Khateeb, and Hind Al Fadda. Dialog chatbot as an interactive online tool in enhancing esp vocabulary learning. _Saudi Journal of Language Studies_, 3(2):76-86, 2023.
* [71] Haonan Qiu, Liang He, and Feng Wang. Dual focus attention network for video emotion recognition. In _2020 IEEE International Conference on Multimedia and Expo (ICME)_, pages 1-6. IEEE, 2020.
* [72] Riaz Qureshi, Daniel Shaughnessy, Kayden AR Gill, Karen A Robinson, Tianjing Li, and Eitan Agai. Are chatgpt and large language models "the answer" to bringing us closer to systematic review automation? _Systematic Reviews_, 12(1):72, 2023.
* [73] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In _International Conference on Machine Learning_, pages 28492-28518. PMLR, 2023.
* [74] Luca Ragno, Alberto Borboni, Federica Vannetti, Cinzia Amici, and Nicoletta Cusano. Application of social robots in healthcare: Review on characteristics, requirements, technical solutions. _Sensors_, 23(15):6820, 2023.
* [75] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive multimodal large language model for long video understanding. _arXiv preprint arXiv:2312.02051_, 2023.
* [76] Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne. Introducing the recola multimodal corpus of remote collaborative and affective interactions. In _2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)_, pages 1-8. IEEE, 2013.
* [77] Juhi Shah, Ali Ayub, Chrystopher L Nehaniv, and Kerstin Dautenhahn. Where is my phone? towards developing an episodic memory model for companion robots to track users' salient objects. In _Companion of the 2023 ACM/IEEE International Conference on Human-Robot Interaction_, pages 621-624, 2023.
* [78] Gunnar A Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: A large-scale dataset of paired third and first person videos. _arXiv preprint arXiv:1804.09626_, 2018.
* [79] Om P Singh. Artificial intelligence in the era of chatgpt-opportunities and challenges in mental health care. _Indian Journal of Psychiatry_, 65(3):297-298, 2023.

* [80] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* [81] Stacey Spiegel and Hua Wang. Exploring climate science in the metaverse: Interactive story-telling in immersive environments for deep learning and public engagement. In _Storytelling to Accelerate Climate Solutions_, pages 365-378. Springer, 2024.
* [82] Aseem Srivastava, Ishan Pandey, Md Shad Akhtar, and Tanmoy Chakraborty. Response-act guided reinforced dialogue generation for mental health counseling. In _Proceedings of the ACM Web Conference 2023_, pages 1118-1129, 2023.
* [83] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for clip at scale. _arXiv preprint arXiv:2303.15389_, 2023.
* [84] Xiaofan Sun, Jeroen Lichtenauer, Michel Valstar, Anton Nijholt, and Maja Pantic. A multimodal database for mimicry analysis. In _Affective Computing and Intelligent Interaction: 4th International Conference, ACII 2011, Memphis, TN, USA, October 9-12, 2011, Proceedings, Part 14_, pages 367-376. Springer, 2011.
* [85] Anja Thieme, Maryann Hanratty, Maria Lyons, Jorge Palacios, Rita Faia Marques, Cecily Morrison, and Gavin Doherty. Designing human-centered ai for mental health: Developing clinically relevant applications for online cbt treatment. _ACM Transactions on Computer-Human Interaction_, 30(2):1-50, 2023.
* [86] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [87] Wan-Hsiu Sunny Tsai and Ching-Hua Chuan. Humanizing chatbots for interactive marketing. _The Palgrave handbook of interactive marketing_, pages 255-273, 2023.
* [88] Katherine E Venturo-Conerly, Rachel Reynolds, Malia Clark, Olivia M Fitzpatrick, and John R Weisz. Personalizing youth psychotherapy: A scoping review of decision-making in modular treatments. _Clinical Psychology: Science and Practice_, 30(1):45, 2023.
* [89] Martin Vondra and Robert Vich. Recognition of emotions in german speech using gaussian mixture models. In _Multimodal Signals: Cognitive and Algorithmic Issues: COST Action 2102 and euCognition International School Vietri sul Mare, Italy, April 21-26, 2008 Revised Selected and Invited Papers_, pages 256-263. Springer, 2009.
* [90] Di Wang, Xutong Guo, Yumin Tian, Jinhui Liu, LiHuo He, and Xuemei Luo. Tetfn: A text enhanced transformer fusion network for multimodal sentiment analysis. _Pattern Recognition_, 136:109259, 2023.
* [91] Di Wang, Shuai Liu, Quan Wang, Yumin Tian, Lihuo He, and Xinbo Gao. Cross-modal enhancement network for multimodal sentiment analysis. _IEEE Transactions on Multimedia_, pages 1-13, 2022.
* [92] Ye Wang, Wang Lin, Shengyu Zhang, Tao Jin, Linjun Li, Xize Cheng, and Zhou Zhao. Weakly-supervised spoken video grounding via semantic interaction learning. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 10914-10932, 2023.
* [93] Zehan Wang, Ziang Zhang, Xize Cheng, Rongjie Huang, Luping Liu, Zhenhui Ye, Haifeng Huang, Yang Zhao, Tao Jin, Peng Gao, et al. Freebind: Free lunch in unified multimodal space via knowledge fusion. In _Forty-first International Conference on Machine Learning_.
* [94] Zehan Wang, Ziang Zhang, Hang Zhang, Luping Liu, Rongjie Huang, Xize Cheng, Hengshuang Zhao, and Zhou Zhao. Omnibind: Large-scale omni multimodal representation via binding spaces. _arXiv preprint arXiv:2407.11895_, 2024.

* [95] Henny A Westra and Alyssa A Di Bartolomeo. Developing expertise in psychotherapy: The case for process coding as clinical training. _American Psychologist_, 79(2):163, 2024.
* [96] Martin Wollmer, Felix Weninger, Tobias Knaup, Bjorn Schuller, Congkai Sun, Kenji Sagae, and Louis-Philippe Morency. Youtube movie reviews: Sentiment analysis in an audio-visual context. _IEEE Intelligent Systems_, 28(3):46-53, 2013.
* [97] Nan Xu, Wenji Mao, and Guandan Chen. Multi-interactive memory network for aspect based multimodal sentiment analysis. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 371-378, 2019.
* [98] Guanghao Yin, Yuanyuan Liu, Tengfei Liu, Haoyu Zhang, Fang Fang, Chang Tang, and Liangxiao Jiang. Token-disentangling mutual transformer for multimodal emotion recognition. _Engineering Applications of Artificial Intelligence_, 133:108348, 2024.
* [99] Wenmeng Yu, Hua Xu, Fangyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, and Kaicheng Yang. Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality. In _Proceedings of the 58th annual meeting of the association for computational linguistics_, pages 3718-3727, 2020.
* [100] Amir Zadeh, Yan Sheng Cao, Simon Hessner, Paul Pu Liang, Soujanya Poria, and Louis-Philippe Morency. Cmu-moseas: A multimodal language dataset for spanish, portuguese, german and french. In _Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing_, volume 2020, page 1801. NIH Public Access, 2020.
* [101] Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis-Philippe Morency. Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos. _arXiv preprint arXiv:1606.06259_, 2016.
* [102] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2236-2246, 2018.
* [103] Haoyu Zhang, Yu Wang, Guanghao Yin, Kejun Liu, Yuanyuan Liu, and Tianshu Yu. Learning language-guided adaptive hyper-modality representation for multimodal sentiment analysis. In _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pages 756-767. Association for Computational Linguistics, 2023.
* [104] Mingxing Zhang, Yang Yang, Xinghan Chen, Yanli Ji, Xing Xu, Jingjing Li, and Heng Tao Shen. Multi-stage aggregated transformer network for temporal language localization in videos. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12669-12678, 2021.
* [105] Rui Zhang, Chengrong Xue, Qingfu Qi, Liyuan Lin, Jing Zhang, and Lun Zhang. Bimodal fusion network with multi-head attention for multimodal sentiment analysis. _Applied Sciences_, 13(3):1915, 2023.
* [106] Zhicheng Zhang, Lijuan Wang, and Jufeng Yang. Weakly supervised video emotion detection and prediction via cross-modal temporal erasing network. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18888-18897, 2023.
* [107] Sicheng Zhao, Yunsheng Ma, Yang Gu, Jufeng Yang, Tengfei Xing, Pengfei Xu, Runbo Hu, Hua Chai, and Kurt Keutzer. An end-to-end visual-audio attention network for emotion recognition in user-generated videos. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 303-311, 2020.
* [108] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, et al. Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment. _arXiv preprint arXiv:2310.01852_, 2023.