[MISSING_PAGE_FAIL:1]

is an existing classifier for the dataset, it must be discarded. A recent approach proposes to learn a post-hoc rejector on top of a pretrained classifier via surrogate loss functions [48].

In this work, to learn rejectors we shift from a loss function perspective to a _distributional_ perspective. Given a model and a corresponding loss function, we find a distribution where the model and loss _performs "best"_ and compare it against the data input distribution to make rejection decisions (see Fig. 1 and Algorithm 1 with equations boxed). As such, the set of rejectors that we propose creates a rejection decision by considering the _density ratio_[67] between a "best" case (idealized) distribution and the data distribution, which can be thresholded by different values \(\tau\) to provide different accuracy vs rejection percentage trade-offs. To learn these density ratios for rejection, we consider a risk minimization problem which is regularized by \(\varphi\)-divergences [1, 19]. We study a particular type of \(\varphi\)-divergences as our regularizer: the family of \(\alpha\)-divergences which generalizes the KL-divergence. To this end, one of our core contributions in this work is providing various methods for constructing and approximating idealized distributions, in particular those constructed by \(\alpha\)-divergences.

The idealized distributions that we consider are connected to adversarial distributions examined in _Distributionally Robust Optimization_ (DRO) [63] and the distributions learned in _Generalized Variational Inference_ (GVI) [42]; and, as such, the closed formed solutions found for our idealized distribution have utility outside of rejection. Furthermore, when utilizing the KL-divergence and Bayes optimal models, we recover the well known optimal rejection policies, _i.e._, Chow's rule [15, 72]. Our rejectors are then examined empirically on 6 different datasets and under label noise corruption.

In summary, our contributions are the following:

* We present a new framework for learning with rejection involving the density ratios of idealized distributions which mirrors the distributions learned in DRO and GVI;
* We show that rejection policies learned in our framework can theoretically recover optimal rejection policies, _i.e._, Chow's rule;
* We derive optimal idealized distributions generated from \(\alpha\)-divergences;
* We present a set of simplifying assumptions such that our framework can be utilized in practice for post-hoc rejection and verify this empirically.

## 2 Preliminaries

NotationLet \(\mathcal{X}\) be a domain of inputs and \(\mathcal{Y}\) be output targets. We primarily consider the case where \(\mathcal{Y}\) is a finite set of \(N\) labels \([N]\), where \([N]\doteq\{0,\ldots,N-1\}\). We also consider output domains \(\mathcal{Y}^{\prime}\) which is not necessarily the same as the output data \(\mathcal{Y}\), _e.g._, class probabilities estimates in classification. Denote the underlying (usually unknown) input distribution as \(\mathrm{P}_{\mathrm{x},\mathrm{y}}\in\mathcal{D}(\mathcal{X}\times\mathcal{Y})\). The marginals of a joint distribution are denoted by subscript, _e.g._, \(\mathrm{P}_{\mathcal{Y}}\in\mathcal{D}(\mathcal{Y})\) for the marginal on the label space. We denote conditional distributions, _e.g._, \(\mathrm{P}_{\mathrm{x}|\mathrm{y}}\). For marginals on \(\mathcal{D}(\mathcal{X})\), the subscripting of \(\mathrm{x}\) is implicit with \(\mathrm{P}=\mathrm{P}_{\mathrm{x}}\). The empirical distributions of \(\mathrm{P}\) are denoted by \(\hat{\mathrm{P}}_{N}\), with \(N\) denoting the number of samples used to generate it. Denote the Iverson bracket \([\![p]\!]=1\) if the predicate \(p\) is true and \([\![p]\!]=0\) otherwise [43]. The maximum \([z]_{+}\doteq\max\{0,z\}\) is shorthanded.

```
0: Model \(h\); Divergence \(\alpha\geq 1\); \(\mathrm{Reg.}\ \lambda>0\); Threshold \(\tau\in[0,1]\).
1: Calibrate \(h\) if needed.
2: Calculate density ratio \(\rho^{\alpha}_{\lambda}\) (either Corollary 4.1 or 4.6).
3: Normalize \(\rho^{\alpha}_{\lambda}\) with Monte-Carlo or Eq. (17).
4: Calculate rejector \(r_{\tau}\) via Def. 3.2.
4:\(r_{\tau}\)
```

**Algorithm 1** Density-Ratio Rejection

Learning with RejectionWe first recall the standard _risk minimization_ setting for learning. Suppose that we are given a (point-wise) loss function \(\ell\colon\mathcal{Y}\times\mathcal{Y}^{\prime}\to\mathbb{R}_{\geq 0}\) which measures the level of

Figure 1: An _idealized distribution_\(\mathbb{Q}\) is learned to _minimizes_ the loss of a model. We then compare \(\mathbb{Q}\) with the original data distribution \(\mathrm{P}\) via a density ratio \(\rho=\mathrm{d}\mathbb{Q}/\mathrm{d}\mathrm{P}\). A rejection criteria is defined via threshold value \(\tau\).

[MISSING_PAGE_FAIL:3]

For GVI, we generalize a number of quantities. For instance, the 'loss' considered can be altered from the log-likelihood \(\log p(z\mid\theta)\) to an alternative loss function over samples \(\{z_{i}\}\). The divergence function \(\mathrm{KL}(\mathrm{P}\parallel\mathrm{Q})\) can also be altered to change the notion of distributional distance. Furthermore, the set of distributions being minimized \(\mathcal{D}(\Theta)\) can also be altered to, _e.g._, reduce the computational cost of the minimization problem. One can thus alter Eq. (4) to give the following:

\[\operatorname*{argmin}_{\mathrm{Q}\in\mathcal{Q}}\quad L(\mathrm{Q})+\lambda \cdot D(\mathrm{P}\parallel\mathrm{Q}), \tag{5}\]

where \(\lambda>0\) and \(L\), \(D\), and \(\mathcal{Q}\) denote the generalized loss, divergence, and set of distribution, respectively. Here \(\lambda\) seeks to act as a regularization constant which can be tuned.

In our work, we will consider a GVI problem which changes the loss function and divergence to define an entropy regularized risk minimization problem. Our loss function corresponds to the learning setting. For the change in divergence, we consider \(\varphi\)-divergence [1; 19], which are otherwise referred to as \(f\)-divergences [62] or the Csiszar divergence [18].

**Definition 2.2**.: Let \(\varphi\colon\mathbb{R}\to(-\infty,\infty]\) be a convex lower semi-continuous function with \(\varphi(1)=0\) then the corresponding \(\varphi\)-divergence over non-negative measures for \(\mathrm{P},\mathrm{Q}\) is defined as:

\[D_{\varphi}(\mathrm{P}\parallel\mathrm{Q})\doteq\int_{\mathcal{X}}\varphi \left(\frac{\mathrm{d}\mathrm{Q}}{\mathrm{dP}}\right)\mathrm{dP},\quad\text{ if }\mathrm{Q}\ll\mathrm{P};\qquad\text{otherwise},\quad D_{\varphi}(\mathrm{P} \parallel\mathrm{Q})=+\infty. \tag{6}\]

We note that for \(\varphi\)-divergences, the regularization constant in Eq. (5) can be absorbed into the \(\varphi\)-divergence generator, _i.e._, \(\lambda\cdot D_{\varphi}(\mathrm{P}\parallel\mathrm{Q})=D_{\lambda\cdot\varphi }(\mathrm{P}\parallel\mathrm{Q})\).

Distributionally Robust OptimizationA related piece of literature is Distributionally Robust Optimization (DRO) [63], where the goal is to find a distribution that maximizes (or minimizes) the expectation of a function from a prescribed _uncertainty_ set. Popular candidates for these uncertainty sets include all distributions that are a certain radius away from a fixed distribution by some divergence. \(\varphi\)-divergences have been used to define such uncertainty set [8; 22; 45]. Given radius \(\varepsilon>0\), define \(B^{\varphi}_{\tau}(\mathrm{P})\doteq\{\mathrm{Q}\in\mathcal{D}(\mathcal{X} \times\mathcal{Y}):D_{\varphi}(\mathrm{P}\parallel\mathrm{Q})<\varepsilon\}\). Given a point-wise loss function \(\ell\colon\mathcal{Y}\times\mathcal{Y}^{\prime}\to\mathbb{R}\), DRO alters risk minimization, as per Eq. (1), to solve the following optimization problem:

\[\min_{h\in\mathcal{H}}\;\max_{\mathrm{Q},x\in B_{\varepsilon}(\mathrm{P})}\; \mathbb{E}_{\bar{\mathrm{Q}}_{x,y}}\left[\ell(\mathsf{Y},h(\mathsf{X}))\right]. \tag{7}\]

The max over \(\bar{\mathrm{Q}}\) is typically over the target space \(\mathcal{Y}\). That is, \(\bar{\mathrm{Q}}(x,y)=\mathrm{Q}(y)\cdot\mathrm{P}(x\mid y)\) and the max is adversarial over the marginal label distribution [74]. Note that converting the \(\varepsilon\)-ball constraint into a Lagrange multiplier, the inner optimization over \(\mathrm{Q}\) in Eq. (7) mirrors Eq. (5). The connection between GVI and adversarial robustness has been previously noted [37].

Typically in DRO and related learning settings, the construction of the adversarial distribution defined by the inner maximization problem is implicitly solved. For example, when \(\varphi\) is twice differentiable, it has been shown that the inner maximization can be reduced to a variance regularization expression [22; 21]; whereas other choices of divergences such as kernel Maximum Mean Discrepancy (MMD) yields kernel regularization [66] and Integral Probability Metrics (IPM) correspond to general regularizers [36]. Another popular choice is the Wasserstein distance which has shown strong connections to point-wise adversarial robustness [10; 11; 64].

The aforementioned work, however, seek only to find the value of the inner maximization in Eq. (7) without considering the form the optimal adversarial distribution takes.

## 3 Rejection via Idealized Distributions

We propose learning rejection functions \(r\colon\mathcal{X}\to\{0,1\}\) by comparing data distributions \(\mathrm{P}\) to a learned idealized distribution \(\mathrm{Q}\) (as per Fig. 1). An idealized distribution (w.r.t. model \(h\)) is a distribution which when taken as data results in low risk (per Eq. (1)). \(\mathrm{Q}\) are idealized rather than 'ideal' as they do not solely rely on a model's performance, but are also regularized by their distance from the data distribution \(\mathrm{P}\). Formally, we define our idealized distribution via a GVI minimization problem.

**Definition 3.1**.: Given a data distribution \(\mathrm{P}_{\mathrm{x},\mathrm{y}}\in\mathcal{D}(\mathcal{X}\times\mathcal{Y})\) and a \(\varphi\)-divergence, an _idealized distribution_\(\mathrm{Q}\in\mathcal{D}(\mathcal{X})\) for a fixed model \(h\) and loss \(\ell\) is a distribution given by

\[\operatorname*{arginf}_{\mathrm{Q}\in\mathcal{D}(\mathcal{X})}\;L(\mathrm{Q}) +\lambda\cdot D_{\varphi}(\mathrm{P}\parallel\mathrm{Q}), \tag{8}\]

where \(L(\mathrm{Q})\doteq\mathbb{E}_{\mathrm{Q}}[L^{\prime}(\mathsf{X})]\) and \(L^{\prime}(x)\doteq\mathbb{E}_{\mathrm{P}_{\mathrm{y}|x}}\left[\ellGiven the objective of Eq. (8), an idealized distribution \(Q\) will have high mass when \(L^{\prime}(x)\) is small and low mass when \(L^{\prime}(x)\) is large. The \(\varphi\)-divergence regularization term prevents the idealized distributions from collapsing to a point mass. Indeed, without regularization the distance from \(P\), idealized distributions would simply be Dirac deltas at values of \(x\in\mathcal{X}\) which minimize \(L^{\prime}(x)\).

With an idealized distribution \(Q\), a rejection can be made via the density ratio [67] w.r.t. \(P\).

**Definition 3.2**.: Given a data distribution \(P\) and an idealized distribution \(Q\ll P\), the \(\tau\)-ratio-rejector w.r.t. \(Q\) is given by \(r_{\tau}(x)\doteq\llbracket\rho(x)\leq\tau\rrbracket\), where \(\rho(x)\doteq\mathrm{d}Q/\mathrm{d}P(x)\).

Definition 3.2 aims to reject inputs where the idealized rejection distribution has lower mass than the original data distribution. Given Definition 3.1 for idealized distribution, small values of \(\rho(x)\) corresponds to regions of the input space \(\mathcal{X}\) where having lower data probability would decrease the expected risk of the model. Note that we do not reject on regions with high density ratio \(\rho\) as \(L^{\prime}(x)\) would be necessarily small or the likelihood of occurrence \(P(x)\) would be relatively small. We restrict the value of \(\tau\) to \((0,1]\) to ensure that rejection is focused regions where \(L^{\prime}(x)\) is high with high probability w.r.t. \(P(x)\) -- further noting that \(\tau=0\) always rejects.

Although Definitions 3.1 and 3.2 suggests that we should learn distributions \(Q\) (and \(P\)) separately to make our rejection decision, in practice, we can learn the density ratio \(\mathrm{d}Q/\mathrm{d}P\) directly. Indeed, through the definition of Definition 2.2, we have that equivalent minimization problem:

\[\operatorname*{argmin}_{\rho\colon\mathcal{X}\to\mathbb{R}_{+}}\ \ \mathbb{E}_{P_{\mathrm{x},\mathcal{Y}}}\left[\rho(X)\cdot\ell(Y,h(X))+\lambda\cdot \varphi(\rho(X))\right];\qquad\mathrm{s.t.}\ \ \ \mathbb{E}_{P}\left[\rho(X)\right]=1. \tag{9}\]

Such an equivalence has been utilized in DRO previously [23, Proof of Theorem 1]. Notice that the learning density ratio \(\rho\) in Eq. (9) is analogous to the acceptor \(1-r(x)\) in Eq. (2). Indeed, ignoring the normalization constraint \(\mathbb{E}_{P}\left[\rho(X)\right]=1\), by restricting \(\rho(x)\in\{0,1\}\), letting \(\lambda=c\), and letting \(\varphi(z)=\llbracket z=0\rrbracket\), the objective function of Eq. (9) can be reduced to:

\[\mathbb{E}_{P_{\mathrm{x},\mathcal{Y}}}\left[\rho(X)\cdot\ell(Y,h(X))\right]+c \cdot P\left(\rho(X)=0\right). \tag{10}\]

Given the restriction of \(\rho\) to binary outputs \(\{0,1\}\) (and Definition 3.2), we have that \(r_{\tau}(x)=1-\rho(x)\). As such, Eq. (10) in this setting is equivalent to the minimization of \(r\) in Eq. (2) (with \(\mathcal{R}\) as all possible functions \(\mathcal{X}\to\{0,1\}\)). Through the specific selection of \(\varphi\) and restriction of \(r\), we have shown that rejection via idealized distributions generalizes the typical learning with rejection objective Eq. (2).

By utilizing form of \(\varphi\)-divergences, we find the form of the idealized rejection distributions \(Q\) and their corresponding density ratio \(\rho\) used for rejection.

**Theorem 3.3**.: _Given Definition 3.1, the optimal density ratio function \(\rho\) of Eq. (9) is of the form,_

\[\rho_{\lambda}^{\varphi}(x)=(\varphi^{\prime})^{-1}\left(\frac{a(x)-L^{\prime} (x)+b}{\lambda}\right), \tag{11}\]

_where \(a(x)\) are Lagrange multipliers to ensure non-negativity \(\rho_{\lambda}^{\varphi}(\cdot)\geq 0\); and \(b\) is a Lagrange multiplier to ensure the constraint \(\mathbb{E}_{P}\left[\rho_{\lambda}^{\varphi}(X)\right]=1\) is satisfied. Furthermore, the optimal idealized rejection distribution is given by: \(Q^{\varphi}(x)=P(x)\cdot\rho_{\lambda}^{\varphi}(x)\)._

Taking \(h\) as the Bayes posterior \(h^{\star}\), \(L^{\prime}\) becomes a function of the ground truth posterior \(\Pr(\boldsymbol{\mathsf{Y}}\mid X=x)\). Hence taking the output \(h\) as a neural network plugin estimate of the underlying true posterior \(\Pr(\boldsymbol{\mathsf{Y}}\mid X=x)\) (see Section 4.3) yields an approach similar to softmax response, _i.e._, rejection based on the output of \(h\) when it outputs softmax probabilities [29]. Theorem 3.3 presents a general approach to generating rejectors from these plugin estimates, as a function of \(\ell\) and \(\varphi\).

Connections to GVI and DROThe formulation and solutions to the optimization of idealized distributions has several connections to GVI and DRO. In contrast to the setting of GVI, Eqs. (4) and (5), the support of the idealized distributions being learned in Definition 3.1 is w.r.t. inputs \(\mathcal{X}\) instead of parameters. Furthermore, the inner maximization of the DRO optimization problem, Eq. (7), seeks to solve a similar form of optimization. For idealized distributions, the maximization is switched to minimization and we consider a distribution over inputs \(\mathcal{X}\) instead of targets \(\mathcal{Y}\). Indeed, notice that the explicit inner optimization of DRO (in Eq. (7)) over \(Q\) can be expressed as the following via the Fan's minimax Theorem [25, Theorem 2]:

\[\sup_{\lambda>0}\ \inf_{Q_{\lambda}\in\mathcal{D}(\mathcal{X})}\ \ -L(Q_{ \lambda})+\lambda\cdot(D_{\varphi}(Q_{\lambda}\parallel P)-\varepsilon). \tag{12}\]Notably, the loss \(L(\mathrm{Q})\) in Eq. (7) can be simply negated to make the optimization over \(\mathrm{Q}\) in Eq. (12) equivalent to DRO Eq. (8) (noting the only requirement for Eq. (7) to Eq. (12) is that \(L(\mathrm{Q})\) is a linear functional of \(\mathrm{Q}\)). This shows that switching the sign of the loss function changes idealized distributions of Definition 3.1 to DRO adversarial distributions. Indeed, through the connection between our idealized distributions and DRO adversarial distributions, the distributions \(\mathrm{Q}^{\varphi}(x)\) will have the same form as the optimal rejection distributions implicitly learned in DRO.

**Corollary 3.4**.: _Suppose \(\mathrm{Q}_{\lambda}^{\varphi}\) denotes the optimal idealized distribution in Theorem 3.3 (switching \(L(\mathrm{Q})\) to \(-L(\mathrm{Q})\)) for a fixed \(\lambda>0\). Further let \(\lambda^{\star}\in\operatorname*{arginf}_{\lambda>0}\left\{-L(\mathrm{Q}_{ \lambda}^{\varphi})+\lambda\cdot(D_{\varphi}(\mathrm{Q}_{\lambda}^{\varphi} \parallel\mathrm{P})-\varepsilon)\right\}\). Then the optimal adversarial distribution in the inner minimization for DRO (Eq. (7)) is \(\mathrm{Q}_{\lambda^{\star}}^{\varphi}\)._

As such, the various optimal idealized distributions (w.r.t. Definition 3.1) for rejection we will present in the sequel can be directly used to obtain the optimal adversarial distributions for DRO.

## 4 Learning Idealized Distributions

In the following section, we explore optimal closed-form density ratio rejectors. We first examine the easiest example -- the KL-divergence -- and then consider the more general \(\alpha\)-divergences.

### KL-Divergence

Let us first consider the KL-divergence [2] for constructing density ratio rejectors via Theorem 3.3.

**Corollary 4.1**.: _Let \(\varphi(z)=z\log z-z+1\) and \(\lambda>0\). The optimal density ratio of Definition 3.1 is,_

\[\rho_{\lambda}^{\mathrm{KL}}(x)=\frac{1}{Z}\cdot\exp\left(\frac{-L^{\prime}(x) }{\lambda}\right),\qquad\text{where }Z=\mathbb{E}\mathrm{P}\left[\exp\left( \frac{-L^{\prime}(x)}{\lambda}\right)\right]. \tag{13}\]

One will notice that \(\rho_{\lambda}^{\mathrm{KL}}\) corresponds to an exponential tilt [24] of \(\mathrm{P}\) to yield a _Gibbs distribution_\(\mathrm{Q}^{\mathrm{KL}}\). The KL density ratio rejectors are significant in a few ways. First, we obtain a closed-form solution due to the properties of '\(\log\)' and complementary slackness, _i.e._, \(a(\cdot)=0\). Secondly, due to the properties of \(\exp\), the normalization term \(b\) has a closed form solution given by the typical log-normalizer term of exponential families.

Another notable property of utilizing a KL idealized distribution is that it recovers the previously mentioned optimal rejection policies for classical modelling with rejection via cost penalty.

**Theorem 4.2** (Informal).: _Given the CPE setting Theorem 2.1, if \(h=h^{\star}\) is optimal, then there exists a \(r_{\tau}^{\mathrm{KL}}\) which is equivalent to the optimal rejectors in Theorem 2.1._

Theorem 4.2 states that the KL density rejectors (with correctly specified \(\lambda\) and \(\tau\)) with optimal predictors \(h^{\star}\) recovers the optimal rejectors of the typical rejection setting, _i.e._, Chow's rule.

Until now, we have implicitly assumed that the true data distribution \(\mathrm{P}\) is accessible. In practice, we only have access empirical \(\hat{\mathrm{P}}_{N}\), defining subsequent rejectors \(\hat{\rho}_{\lambda,N}\). We show that for the KL rejector, \(\hat{\mathrm{P}}_{N}\) is enough given the following generalization bound.

**Theorem 4.3**.: _Assume we have bounded loss \(|\ell(\cdot,\cdot)|\leq B\) for \(B>0\), \(\hat{\mathrm{P}}_{N}\subset\mathrm{P}\) with h.p., and \(\mathcal{T}\subset\mathrm{Supp}(\mathrm{P})\). Suppose \(M=|\mathcal{T}|<+\infty\), then with probability \(1-\delta\), we have that_

\[\sup_{x\in\mathcal{T}}\left|\rho_{\lambda}^{\mathrm{KL}}(x)-\hat{\rho}_{ \lambda,N}^{\mathrm{KL}}(x)\right|\leq C\cdot\sqrt{\frac{2}{N}\log\left(\frac {2M}{\delta}\right)},\]

_where \(C=\exp\left(B/\lambda\right)^{3}\cdot\sinh\left(B/\lambda\right)\)._

Looking at Theorem 4.3, essentially we pay a price in generalization \(M\) for each element \(x\in\mathcal{T}\) we are testing for rejection. For generalization, it is useful to consider how \(N,M\) changes our rate in Theorem 4.3. If we assume that the test set \(\mathcal{T}\) is small in comparison to the \(N\) samples used to generate empirical distribution \(\hat{\mathrm{P}}\), then the \(\mathcal{O}(1/\sqrt{N})\) rate will dominate. A concrete example of this case is when \(|\mathcal{X}|\) is finite. A less advantaged scenario is when \(M\approx N\), yielding \(\mathcal{O}(\log(N)/\sqrt{N})\) -- the scenario where we test approximately the same number of data points as that used to learn the rejector. This rate will still decrease with \(N\to\infty\), although with a \(\log N\) price.

### Alpha-Divergences

Although the general case of finding idealized rejection distributions for \(\varphi\)-divergences is difficult, we examine a specific generalization of the KL case, the \(\alpha\)-divergences [3].

**Definition 4.4**.: For \(\alpha\in\mathbb{R}\), the \(\alpha\)-divergence \(D_{\alpha}\) is defined as the \(\varphi_{\alpha}\)-divergence, where

\[\varphi_{\alpha}(z)\operatorname{\doteq}\begin{cases}\frac{4}{1-\alpha^{2}} \left(1-z^{\frac{1+\alpha}{2}}\right)-\frac{2}{1-\alpha}(z-1)&\text{if }\alpha \neq\pm 1\\ -\log z+(z-1)&\text{if }\alpha=-1\\ z\log z-(z-1)&\text{if }\alpha=1\end{cases}.\]

We further define \(\psi_{\alpha}\), where \(\psi_{\alpha}(z)=z^{\frac{1-\alpha}{2}}\) when \(\alpha\neq 1\) and \(\psi_{\alpha}(z)=\log z\) when \(\alpha=1\).

Note that taking \(\alpha=1\) recover the KL-divergence. The \(\alpha\)-divergence covers a wide range of divergences including the Pearson \(\chi^{2}\) divergence \((\alpha=3)\). For the density ratio, \(\alpha\)-divergences with \(\alpha\neq 1\) (_i.e._ not KL) can be characterized as the following.

**Theorem 4.5**.: _Let \(\alpha\neq 1\) and \(\lambda>0\). For \(\varphi_{\alpha}\), the optimal density ratio rejector \(\rho_{\lambda}^{\alpha}(x)\) is,_

\[\rho_{\lambda}^{\alpha}(x)=\psi_{\alpha}^{-1}\left(\frac{2}{\alpha-1}\cdot \left(a(x)-\frac{L^{\prime}(x)}{\lambda}+b\right)^{-1}\right) \tag{14}\]

\(a(x)\) _are Lagrange multipliers for positivity; and \(b\) is a Lagrange multiplier for normalization._

One major downside of using general \(\varphi\)-divergences is that solving the Lagrange multipliers for the idealized rejection distribution is often difficult. Indeed, the "\(\log\)" and "\(\exp\)" ensures non-negativity of the idealized distribution when the input data \(\mathrm{P}\) is in the interior of the simplex; and also provides a convenient normalization calculation. For \(\alpha\)-divergences, the non-negative Lagrange multipliers \(a(\cdot)\) can be directly solved given certain conditions.

**Corollary 4.6**.: _Suppose \(\alpha>1\) and \(\lambda>0\), then Eq. (14) simplifies to,_

\[\rho_{\lambda}^{\alpha}(x)=\left[\left(\frac{\alpha-1}{2}\cdot\left(b-\frac{L^ {\prime}(x)}{\lambda}\right)\right)^{\frac{2}{\alpha-1}}\right]_{+}, \tag{15}\]

_where we take non-integer powers of negative values as 0._

On the other hand, for \(\alpha\leq-1\), \(D_{\alpha}(\mathrm{P}\parallel\mathrm{Q})=\infty\) whenever \(\mathrm{Q}\) is on the boundary whenever \(\mathrm{P}\) is not on the boundary [3, Section 3.4.1]. As such, we can partially simplify Eq. (14) for \(\alpha\leq-1\).

**Corollary 4.7**.: _Suppose \(\alpha\leq-1\), \(\lambda>0\), and \(\mathrm{P}\) lies in the simplex interior, then \(a(\cdot)=0\) in Eq. (14)._

Both Corollaries 4.6 and 4.7 can provide a unique rejector policy than the KL-divergence variant. Corollary 4.7 can provide a similar effect when \(a(x)\neq 0\) for all \(x\). Nevertheless, having to determine which inputs \(a(x)\neq 0\) and solving these values are difficult in practice. As such, we focus on \(\alpha>0\). If there are values of \(L^{\prime}(x)\) with high risk, the \(\max\) will flatten these inputs to \(0\) in Corollary 4.6. However, if the original model \(h\) performs well and \(L^{\prime}(x)\) is relatively small for all \(x\), then it is possible that the \(\max\) is not utilized. In such a case, the \(\alpha\)-divergence rejectors can end up being similar -- this follows the fact that (as \(\varphi_{\alpha}^{\prime\prime}(1)>0\)) locally all \(\alpha\)-divergences will be similar to the \(\chi^{2}\) / \((\alpha=3)\)-divergence [56, Theorem 7.20]. This ultimately results in \(\alpha\)-divergences being similar to, _e.g._, Chow's rule when \(h_{y}(x)\approx\mathrm{Pr}(\mathsf{V}=y\mid\mathsf{X}=x)\) in classification via Theorem 2.1.

Among the \(\alpha>0\) cases, we examine the \(\chi^{2}\)-divergence (\(\alpha=3\)) which results in closed form for bounded loss functions and sufficient large regularizing parameter \(\lambda\).

**Corollary 4.8**.: _Suppose that \(\ell(\cdot,\cdot)\leq B\) and \(\lambda>\max_{x}L^{\prime}(x)-\mathbb{E}_{\mathrm{P}}[L^{\prime}(x)]\). Then,_

\[\rho_{\lambda}^{\alpha=3}(x)=1+\frac{\mathbb{E}_{\mathrm{P}}[L^{\prime}(x)]-L ^{\prime}(x)}{\lambda}. \tag{16}\]

The condition on \(\lambda\) for Corollary 4.8 is equivalent to rescaling bounded loss functions \(\ell\). Indeed, by fixing \(\lambda=1\), we can achieve a similar Theorem with suitable rescaling of \(\ell\). Nevertheless, Eq. (16) provides a convenient form to allow for generalization bounds to be established.

**Theorem 4.9**.: _Assume we have bounded loss \(|\ell(\cdot,\cdot)|\leq B\) for \(B>0\), \(\lambda>2B\), \(\hat{\mathrm{P}}_{N}\subset\mathrm{P}\) with h.p., and \(\mathcal{T}\subset\mathrm{Supp}(\mathrm{P})\). Suppose \(M=|\mathcal{T}|<+\infty\), then with probability \(1-\delta\), we have that_

\[\sup_{x\in\mathcal{T}}\left|\rho_{\lambda}^{\alpha=3}(x)-\hat{\rho}_{\lambda}^ {\alpha=3}(x)\right|\leq\frac{B}{\lambda}\cdot\sqrt{\frac{2}{N}\log\left(\frac {2M}{\delta}\right)}.\]

Notice that Theorem 4.9's sample complexity is equivalent to Theorem 4.3 up to constant multiplication. Hence, the analysis of Theorem 4.3 regarding the scales of \(N,M\) hold for Theorem 4.9.

A question pertaining to DRO is what would be the generalization capabilities of the corresponding adversarial distributions \(\hat{\mathrm{Q}}_{\lambda,N}\doteq\hat{\mathrm{P}}_{N}\cdot\hat{\rho}_{\lambda,N}\) (through Corollary 3.4). On a finite domain, via Theorems 4.3 and 4.9 and a simple triangle inequality, one can immediately bound the total variation \(\mathrm{TV}(\hat{\mathrm{Q}}_{\lambda,N},\mathrm{Q}_{\lambda})\leq\mathcal{O} (1/\sqrt{N})\), see Appendix M for further details.

### Practical Rejection

We consider practical concerns for utilizing the KL-or (\(\alpha\) > 1)-divergence case, Eqs. (13) and (15), for post-hoc rejection. To do so, we need to estimate the loss \(L^{\prime}(x)\) and reject normalizer \(Z\) or \(b\).

**Loss**: The former is tricky, we require an estimate to evaluate \(L^{\prime}(x)=\mathbb{E}_{\mathrm{P}_{\mathrm{y}|\mathrm{x}}}\left[\ell( \mathsf{Y},h(x))\right]\) over any possible \(x\in\mathcal{X}\) to allow us to make a rejection decision. Implicitly, this requires us to have a high quality estimate of \(\mathrm{P}_{\mathsf{y}|\mathrm{x}}\). In a general learning setting, this can be difficult to obtain -- in fact it is just the overall objective that we are trying to learning, _i.e._, predicting a target \(y\) given an input \(x\). However, in the case of CPE (classification), it is not unreasonable to obtain an _calibrated estimate_ of \(\mathrm{P}_{\mathsf{y}|\mathrm{x}}\) via the classifier \(h\colon\mathcal{X}\to\mathcal{D}(\mathcal{Y})\)[55]. In Section 5, we utilize temperature scaling to calibrate the neural networks we learn to provide such an estimate [32]. Hence, we set \(L^{\prime}(x)=\mathbb{E}_{\mathsf{Y}\sim h(x)}\left[\ell(\mathsf{Y},h(x))\right]\). For proper CPE loss functions, \(\mathbb{E}_{\mathsf{Y}\sim h(x)}\left[\ell(\mathsf{Y},h(x))\right]\) acts as a generalized entropy function. As such, the rejectors Eqs. (13) and (15) act as functions over said generalized entropy functions. It should be noted that simply considering the softmax outputs of neural networks for probability estimation have seen prior success [29; 39]. This is equivalent to taking the 0-1-loss function [15; 35; 28] and taking a plugin estimate of probabilities via the neural network's output. The study of using plugin estimates have also been explored in the model cascade literature [40].

**Normalization**: For the latter, we utilize a sample based estimate (over the dataset used to train the rejector) of \(\mathbb{E}_{\mathrm{P}}\left[\rho_{\lambda}^{\varphi}(\mathsf{X})\right]\) is utilized to solve the normalizers \(Z\) and \(b\). In the case of the KL-divergence rejector, this is all that is required due to the convenient function form of the Gibbs distribution, _i.e._, the normalizer \(Z\) can be simply estimated by a sample mean. However, for \(\alpha>1\)-divergences \(b\) needs to be found to determine the normalization. Practically, we find \(b\) through bisection search of

\[\mathbb{E}_{\mathrm{P}}\left[\left(\frac{\alpha-1}{2}\cdot\left(b-\frac{L^{ \prime}(x)}{\lambda}\right)\right)^{\frac{2}{\alpha-1}}\right]_{+}-1=0. \tag{17}\]

This practically works as the optimization \(b\) is over a single dimension. Furthermore, we can have that \(b>\min_{x}\left\{L^{\prime}(x)/\lambda\right\}\). As an upper bound over the possible values of \(b\), we utilize a heuristic where we multiple the corresponding maximum of \(L^{\prime}(x)/\lambda\) with a constant.

**Threshold \(\tau\)**: In addition to learning the density ratio, an additional consideration is how to tune \(\tau\) in the rejection decision, Definition 3.2. Given a fixed density estimator \(\rho\), change \(\tau\) amounts to changing the rate of rejection. We note that this problem is not limited to our density ratio rejectors, where approaches with rejectors \(r\in\mathcal{R}\) via a (surrogate) minimization of Eq. (2) may be required multiple rounds of training with different rejection costs \(c\) to find an accept rate of rejection. In our case, we have a fixed \(\rho\) which allows easy tuning of \(\tau\) given a validation dataset, similar to other confidence based rejection approaches, _e.g._, tuning a threshold for the margin of a classifier [7].

## 5 Experiments

In this section, we evaluate our distributional approach to rejection across a number of datasets. In particular, we consider the standard classification setting with an addition setting where uniform label noise is introduced [4; 30]. For our density ratio rejectors, we evaluate the KL-divergence basedrejector (Corollary 4.1) and (\(\alpha\)=3)-based rejectors (Corollary 4.6) with 50 equidistant \(\tau\in(0,1]\) values. For our tests, we fix \(\lambda=1\). Throughout our evaluation, we assume that a neural network (NN) model without rejection is accessible for all (applicable) approaches. For our density ratio rejectors, we utilize the log-loss, practical considerations in Section 4.3, and Algorithm 1.2

Footnote 2: Our rejectors code public at: [https://github.com/alexandersoen/density-ratio-rejection](https://github.com/alexandersoen/density-ratio-rejection).

To evaluate our density ratio rejectors and baselines, we compare accuracy and acceptance coverage. Accuracy corresponds to the 0-1 loss in Eq. (1) and the acceptance coverage is the percentage of non-rejections in the test set. Ideally, a rejector smoothly trade-offs between accuracy and acceptance, _i.e._, a higher accuracy can be achieved by decreasing the acceptance coverage by rejecting more data. We study this trade-off by examining multiple cut-off values \(\tau\) and rejection costs \(c\).

Dataset and BaselinesWe consider 6 multiclass classification datasets. For tabular datasets, we consider the gas drift dataset [68] and the human activity recognition (HAR) dataset [5]. Each of these datasets consists of 6 classes to predict. Furthermore, we utilize a two hidden layer MLP NN model for these datasets. We consider the MNIST image dataset [46] (10 classes), where we utilize a convolutional NN. Additionally, we consider 3 larger image datasets with ResNet-18 architecture [33]: CIFAR-10 [44] (10 classes); and OrgMNIST / OrganSMNIST (11 classes) and OctMNIST (4 classes) from the MedMNIST collection [69, 70]. These prediction models are trained utilizing the standard logistic / log-loss without rejection and then are calibrated via temperature scaling [32]. For each of these datasets, we utilize both clean and noisy variants. For the noisy variant, we flip the class labels of the train set with a rate of 25%. We note that the test set is clean in both cases. All evaluation uses 5-fold cross validation. All implementation use PyTorch and training was done on a p3.2xlarge AWS instance.

We consider 4 different baseline for comparison. Each is trained with 50 equidistant costs \(c,\tau\in[0,0.5)\), except on OctMNIST which uses 10 equidistant costs (selecting \(c,\tau\) discussed in Appendix P). One baseline used corresponds to a modification of [49]'s cross-entropy surrogate approach (DEFER) originally used for the learning to defer literature (see [14, Appendix A.2]). This approach treats the rejection option as a separate class and solves a \(|\mathcal{Y}|+1\) classification problem. A generalization of DEFER is considered which utilizes generalized cross-entropy [75] as its surrogate loss (GCE) [13]. We also consider a cost-sensitive classification reduction (CSS) of the classification with rejection problem [14] utilizing the sigmoid loss function. The aforementioned 3 baselines all learn a model with rejection simultaneously, _i.e._, a pretrained model cannot be utilized. We also consider a two stage predictor-rejector approach (PredRej) which learns a rejector from a pretrained classifier [48].

ResultsTable 1 presents a tabular summary of accuracy and coverage values when targeting 80% coverage values; and Fig. 2 presents a summary plot of the acceptance coverage versus model accuracy after rejection, focused around the 60% to 100% coverage region. This plot is limited to HAR, Gas Drift, and MNIST due to space limitations however the deferred datasets show curves where the density ratio rejector dominate with better accuracy and coverage in the plotted region, with corresponding extended plots for all datasets in Appendix Q. Over all folds for MNIST our density ratio rejectors take approximately \(\approx 1/2\) hour to fit. A single baseline (fixed \(c\)) takes upwards of 2 hour for a single fold. Overall, given that the underlying model is calibrated, we find that our

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline  & & Base & KL-Rej & (\(\alpha\)=3)-Rej & PredRej & CSS & DEFER & GCE \\ \hline \multirow{8}{*}{CIFER} & HAR & 97.38 [100] & 99.93 [81] & **99.93**[82] & 98.86 [85] & 99.58 [81] & 99.44 [80] & 99.31 [82] \\  & Gas Drift & 94.10 [100] & **99.16**[80] & **99.16**[80] & 98.12 [80] & 98.68 [80] & 98.06 [80] & 97.62 [80] \\  & MNIST & 98.55 [100] & 99.93 [87] & 99.93 [88] & 99.18 [74] & **99.95**[83] & 99.93 [80] & 99.85 [80] \\  & CIFAR-10 & 90.20 [100] & **97.22**[80] & 97.71 [81] & 91.40 [74] & 95.45 [81] & 93.72 [81] & 94.25 [81] \\  & OrganMNIST & 89.10 [100] & **96.55**[80] & 96.52 [80] & 93.79 [82] & 94.49 [80] & 93.47 [80] & 93.68 [80] \\  & OctMNIST & 91.93 [100] & 97.08 [81] & **97.18**[80] & 93.43 [86] & 95.40 [81] & 94.66 [85] & 94.91 [87] \\ \hline \multirow{8}{*}{CIFER} & HAR & 96.51 [100] & 98.56 [81] & 98.56 [81] & 97.22 [80] & 97.82 [81] & 97.78 [69] & **98.85**[80] \\  & Gas Drift & 93.84 [100] & 97.30 [80] & 97.28 [80] & 95.87 [82] & 98.71 [80] & **99.02**[77] & 97.52 [75] \\ \cline{1-1}  & MNIST & 97.88 [100] & 99.89 [80] & 99.89 [81] & 98.00 [93] & 99.94 [80] & 99.93 [81] & **99.95**[81] \\ \cline{1-1}  & CIFAR-10 & 85.31 [100] & 92.25 [82] & **92.50**[81] & 85.84 [88] & 89.58 [82] & 90.93 [80] & 92.22 [81] \\ \cline{1-1}  & OrganMNIST & 89.10 [100] & 95.86 [82] & **96.29**[81] & 93.40 [84] & 96.74 [81] & 94.67 [80] & 94.48 [80] \\ \cline{1-1}  & OctMNIST & 91.89 [100] & **97.17**[80] & 97.10 [81] & 93.42 [83] & 95.49 [81] & 94.08 [89] & 94.63 [78] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of rejection methods over all baselines and datasets targeting 80% coverage. Each cell reports the “accuracy [coverage]” values, **bold** for most accurate, and s.t.d. reported in Appendix.

density ratio rejector are either competitive or superior to the baselines. One might notice that the aforementioned baselines do not or provide poor trade-offs for coverage values \(>95\%\) (as per Fig. 2). Indeed, to achieve rejection with high coverage (without architecture tuning), approaches which 'wrap' a base classifier seem preferable, _i.e._, PredRej and our density ratios rejectors. Even at lower coverage targets (80%), Table 1 shows that density-ratio methods are comparable or superior in the more complex datasets of CIFAR-10 and the MedMNIST collection. If large models are allowed to be used for the rejector -- as per the MNIST case -- CSS, DEFER, and GCE can provide superior accuracy vs acceptance coverage trade-offs (noisy MNIST). However, this is not always true as per CIFAR-10 where all approaches are similarly effected by noise; or OctMNIST and OrgMNIST where approaches only slightly change with noise. The latter appears to be a consequence of label noise not effecting the Base classifier's accuracy (using the larger ResNet-18 architecture), as per Table 1.

Among the approaches which 'wrap' the base classifier \(h\), we find that these approaches have higher variance ranges than the other approaches. In particular, the randomness of the base model potentially magnifies the randomness after rejection. The variance range of the base model tends to increase as the noise increases (additional ranges of noise for HAR and Gas Drift are presented in the Appendix). The influence on rejection is unsurprising as these 'wrapping' approaches predict via a composition of the original model (and hence inherits its randomness across folds). In general, our density ratio rejector outperforms PredRej. However, it should be noted that PredRej does not require a calibrated classifier. Among the density ratio rejectors, between KL and \(\alpha=3\), the only variation is in coverage region that the \(\tau\in(0,1]\) threshold covers. This follows from the fact that for similar distributions, \(\varphi\)-divergences act similarly (56, Theorem 7.20). We find this pattern holds for other values of \(\alpha\).

## 6 Limitations and Conclusions

We propose a new framework for rejection by learning idealized density ratios. Our proposed rejection framework links typically explored classification with rejection to generalized variational inference and distributionally robust optimization. It should be noted that although we have focused on classification, \(L^{\prime}(\mathrm{Q})\) could in theory be replaced by any other loss functions. In this sense, one could adapt this approach to other learning tasks such as regression, discussed in Appendix N. Furthermore, although we have focused on \(\varphi\)-divergences, there are many alternative ways idealized distribution can be constructed, _e.g._, integral probability metrics (51, 9). One limitation of our distributional way of rejecting is the reliance on approximating \(\mathrm{P}[\mathsf{Y}\mid\mathsf{X}]\) with model \(h\). In future work, one may seek to approximate the density ratio \(\rho\) by explicitly learning densities \(\mathrm{Q}\) and \(\mathrm{P}\) or via gradient based methods (for the latter, see Appendix O).

Figure 2: Accuracy vs coverage plots across select datasets and all approaches, with 50 equidistant \(\tau\in(0,1]\) and \(c\in[0,0.5)\) values (sorted by coverage). The black horizontal line depicts base models trained without rejection. Missing approaches in the plots indicates that the model rejects more than 60% of test points or has accuracy below the base model. Shaded region indicates \(\pm 1\) s.t.d. region.

## Acknowledgments and Disclosure of Funding

We thank the reviewers and the area chair for the various suggestions which improved the paper. A majority of the work was done whilst AS was interning at Amazon.

## References

* [1] Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distribution from another. _Journal of the Royal Statistical Society: Series B (Methodological)_, 28(1):131-142, 1966.
* [2] S.-I. Amari and H. Nagaoka. _Methods of Information Geometry_. Oxford University Press, 2000.
* [3] Shun-ichi Amari. _Information geometry and its applications_, volume 194. Springer, 2016.
* [4] Dana Angluin and Philip Laird. Learning from noisy examples. _Machine learning_, 2:343-370, 1988.
* [5] Davide Anguita, Alessandro Ghio, Luca Oneto, Xavier Parra, Jorge Luis Reyes-Ortiz, et al. A public domain dataset for human activity recognition using smartphones. In _Esann_, volume 3, page 3, 2013.
* [6] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
* [7] Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss. _Journal of Machine Learning Research_, 9(8), 2008.
* [8] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. _Management Science_, 59(2):341-357, 2013.
* [9] Jeremiah Birrell, Paul Dupuis, Markos A Katsoulakis, Yannis Pantazis, and Luc Rey-Bellet. \((f,\Gamma)\)-Divergences: interpolating between \(f\)-divergences and integral probability metrics. _The Journal of Machine Learning Research_, 23(1):1816-1885, 2022.
* [10] Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport. _Mathematics of Operations Research_, 44(2):565-600, 2019.
* [11] Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. _Journal of Applied Probability_, 56(3):830-857, 2019.
* [12] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration Inequalities: A Nonasymptotic Theory of Independence_. Oxford University Press, 2013.
* [13] Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo An, Gang Niu, and Masashi Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. _Advances in Neural Information Processing Systems_, 35:521-534, 2022.
* [14] Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification with rejection based on cost-sensitive classification. In _International Conference on Machine Learning_, pages 1507-1517, 2021.
* [15] C Chow. On optimum recognition error and reject tradeoff. _IEEE Transactions on information theory_, 16(1):41-46, 1970.
* [16] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. _Advances in Neural Information Processing Systems_, 29, 2016.
* [17] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In _Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27_, pages 67-82. Springer, 2016.
* [18] Imre Csiszar. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. _Magyer Tud. Akad. Mat. Kutato Int. Koezol._, 8:85-108, 1964.

* [19] Imre Csiszar. Information-type measures of difference of probability distributions and indirect observation. _studia scientiarum Mathematicarum Hungarica_, 2:229-318, 1967.
* [20] Imre Csiszar, Paul C Shields, et al. Information theory and statistics: A tutorial. _Foundations and Trends(r) in Communications and Information Theory_, 1(4):417-528, 2004.
* [21] John Duchi and Hongseok Namkoong. Variance-based regularization with convex objectives. _The Journal of Machine Learning Research_, 20(1):2450-2504, 2019.
* [22] John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. _Mathematics of Operations Research_, 46(3):946-969, 2021.
* [23] Krishnamurthy Dj Dvijotham, Jamie Hayes, Borja Balle, Zico Kolter, Chongli Qin, Andras Gyorgy, Kai Xiao, Sven Gowal, and Pushmeet Kohli. A framework for robustness certification of smoothed classifiers using f-divergences. In _International Conference on Learning Representations_, 2019.
* [24] Bradley Efron. _Exponential families in theory and practice_. Cambridge University Press, 2022.
* [25] Ky Fan. Minimax theorems. _Proceedings of the National Academy of Sciences of the United States of America_, 39(1):42, 1953.
* [26] Leo Feng, Mohamed Osama Ahmed, Hossein Hajimirsadeghi, and Amir H. Abdi. Towards better selective classification. In _The Eleventh International Conference on Learning Representations_, 2023.
* [27] Vojtech Franc and Daniel Prusa. On discriminative learning of prediction uncertainty. In _International Conference on Machine Learning_, pages 1963-1971, 2019.
* [28] Vojtech Franc, Daniel Prusa, and Vaclav Voracek. Optimal strategies for reject option classifiers. _Journal of Machine Learning Research_, 24(11):1-49, 2023.
* [29] Yonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. _Advances in neural information processing systems_, 30, 2017.
* [30] Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise. _Neurocomputing_, 160:93-107, 2015.
* [31] Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, and Stephane Canu. Support vector machines with a reject option. _Advances in neural information processing systems_, 21, 2008.
* [32] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330, 2017.
* [33] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [34] Kilian Hendrickx, Lorenzo Perini, Dries Van der Plas, Wannes Meert, and Jesse Davis. Machine learning with a reject option: A survey. _Machine Learning_, 113(5):3073-3110, 2024.
* [35] Radu Herbei and Marten H Wegkamp. Classification with reject option. _The Canadian Journal of Statistics/La Revue Canadienne de Statistique_, pages 709-721, 2006.
* [36] Hisham Husain. Distributional robustness with ipms and links to regularization and gans. _Advances in Neural Information Processing Systems_, 33:11816-11827, 2020.
* [37] Hisham Husain and Jeremias Knoblauch. Adversarial interpretation of bayesian inference. In _International Conference on Algorithmic Learning Theory_, pages 553-572, 2022.
* [38] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International conference on machine learning_, pages 448-456, 2015.

* [39] Paul F Jaeger, Carsten Tim Luth, Lukas Klein, and Till J. Bungert. A call to reflect on evaluation practices for failure detection in image classification. In _The Eleventh International Conference on Learning Representations_, 2023.
* [40] Wittawat Jitkrittum, Neha Gupta, Aditya K Menon, Harikrishna Narasimhan, Ankit Rawat, and Sanjiv Kumar. When does confidence-based cascade deferral suffice? _Advances in Neural Information Processing Systems_, 36, 2024.
* [41] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In _International Conference on Learning Representations_, 2014.
* [42] Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on bayes' rule: Reviewing and generalizing variational inference. _The Journal of Machine Learning Research_, 23(1):5789-5897, 2022.
* [43] Donald E Knuth. Two notes on notation. _The American Mathematical Monthly_, 99(5):403-422, 1992.
* [44] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Toronto, ON, Canada, 2009.
* [45] Henry Lam. Robust sensitivity analysis for stochastic systems. _Mathematics of Operations Research_, 41(4):1248-1275, 2016.
* [46] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. _ATT Labs [Online]_, 2, 2010.
* [47] Naresh Manwani, Kalpit Desai, Sanand Sasidharan, and Ramasubramanian Sundararajan. Double ramp loss based reject option classifier. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_, pages 151-163. Springer, 2015.
* [48] Anqi Mao, Mehryar Mohri, and Yutao Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. In _International Conference on Algorithmic Learning Theory_, pages 822-867, 2024.
* [49] Hussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. In _International Conference on Machine Learning_, pages 7076-7087, 2020.
* [50] Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and David Sontag. Who should predict? exact algorithms for learning to defer to humans. In _International Conference on Artificial Intelligence and Statistics_, pages 10520-10545, 2023.
* [51] Alfred Muller. Integral probability metrics and their generating classes of functions. _Advances in Applied Probability_, 29(2):429-443, 1997.
* [52] Harikrishna Narasimhan, Wittawat Jitkrittum, Aditya K Menon, Ankit Rawat, and Sanjiv Kumar. Post-hoc estimators for learning to defer to an expert. _Advances in Neural Information Processing Systems_, 35:29292-29304, 2022.
* [53] Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of multiclass classification with rejection. _Advances in Neural Information Processing Systems_, 32, 2019.
* [54] Tadeusz Pietraszek. Optimizing abstaining classifiers using roc analysis. In _Proceedings of the 22nd international conference on Machine learning_, pages 665-672, 2005.
* [55] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in large margin classifiers_, 10(3):61-74, 1999.
* [56] Yury Polyanskiy and Yihong Wu. _Information theory: From coding to learning_. Cambridge university press, 2024.
* [57] Andrea Pugnana and Salvatore Ruggieri. A model-agnostic heuristics for selective classification. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9461-9469, 2023.

* [58] Andrea Pugnana, Lorenzo Perini, Jesse Davis, and Salvatore Ruggieri. Deep neural network benchmarks for selective classification. _Journal of Data-centric Machine Learning Research_, 2024.
* [59] H. G. Ramaswamy, Ambuj Tewari, and Shivani Agarwal. Consistent algorithms for multiclass classification with an abstain option. _Electronic Journal of Statistics_, 12:530-554, 2018.
* [60] Mark D Reid and Robert C Williamson. Composite binary losses. _The Journal of Machine Learning Research_, 11:2387-2422, 2010.
* [61] Mark D Reid and Robert C Williamson. Information, divergence and risk for binary experiments. _Journal of Machine Learning Research_, 12:731-817, 2011.
* [62] Igal Sason and Sergio Verdu. \(f\)-divergence inequalities. _IEEE Transactions on Information Theory_, 62(11):5973-6006, 2016.
* [63] Herbert E Scarf. A min-max solution of an inventory problem. Technical report, RAND CORP SANTA MONICA CALIF, 1957.
* [64] Aman Sinha, Hongseok Namkoong, and John Duchi. Certifiable distributional robustness with principled adversarial training. In _International Conference on Learning Representations_, 2018.
* [65] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _The journal of machine learning research_, 15(1):1929-1958, 2014.
* [66] Matthew Staib and Stefanie Jegelka. Distributionally robust optimization and generalization in kernel methods. In _Advances in Neural Information Processing Systems_, pages 9131-9141, 2019.
* [67] Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori. _Density ratio estimation in machine learning_. Cambridge University Press, 2012.
* [68] Alexander Vergara, Shankar Vembu, Tuba Ayhan, Margaret A Ryan, Margie L Homer, and Ramon Huerta. Chemical gas sensor drift compensation using classifier ensembles. _Sensors and Actuators B: Chemical_, 166:320-329, 2012.
* [69] Jiancheng Yang, Rui Shi, and Bingbing Ni. Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis. In _2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)_, pages 191-195. IEEE, 2021.
* [70] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. _Scientific Data_, 10(1):41, 2023.
* [71] Ming Yuan and Marten Wegkamp. Classification methods with reject option based on convex risk minimization. _Journal of Machine Learning Research_, 11(1), 2010.
* [72] Ahmed Zaoui, Christophe Denis, and Mohamed Hebiri. Regression with reject option and application to knn. _Advances in Neural Information Processing Systems_, 33:20073-20082, 2020.
* [73] Arnold Zellner. Optimal information processing and bayes's theorem. _American Statistician_, pages 278-280, 1988.
* [74] Jingzhao Zhang, Aditya Krishna Menon, Andreas Veit, Srinadh Bhojanapalli, Sanjiv Kumar, and Suvrit Sra. Coping with label shift via distributionally robust optimisation. In _International Conference on Learning Representations_, 2021.
* [75] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. _Advances in neural information processing systems_, 31, 2018.

Supplementary Material

This is the Supplementary Material to Paper "Rejection via Learning Density Ratios". To differentiate with the numberings in the main file, the numbering of Theorems is letter-based (A, B,...).

## Table of contents

**Proof**

[MISSING_PAGE_POST]

**Deferred Content**

\(\hookrightarrow\) Appendix L: Broader Impact \(\underline{\phantom{\rule{1.0pt}{1.0pt}}}\) Pg 22

\(\hookrightarrow\) Appendix M: Distribution Generalization Bounds \(\underline{\phantom{\rule{1.0pt}{1.0pt}}}\) Pg 22

\(\hookrightarrow\) Appendix N: Rejection for Regression \(\underline{\phantom{\rule{1.0pt}{1.0pt}}}\) Pg 23

\(\hookrightarrow\) Appendix O: Gradient of Density Ratio Objective \(\underline{\phantom{\rule{1.0pt}{1.0pt}}}\) Pg 24

\(\hookrightarrow\) Appendix P: Finding the Best Rejection Cost \(\underline{\phantom{\rule{1.0pt}{1.0pt}}}\) Pg 24

\(\hookrightarrow\) Appendix Q: Additional Experimental Details \(\underline{\phantom{\rule{1.0pt}{1.0pt}}}\) Pg 25

## Appendix A Proof of Theorem 2.1

Proof.: We first rewrite the coverage probability as an expectation:

\[\mathbb{E}_{\mathrm{P}}[(1-r(\mathsf{X}))\cdot\ell(\mathsf{Y},h(\mathsf{X}))]+c \cdot\Pr[r(\mathsf{X})=1]=\mathbb{E}_{\mathrm{P}}[(1-r(\mathsf{X}))\cdot\ell( \mathsf{Y},h(\mathsf{X}))+c\cdot r(\mathsf{X})].\]

We note that point-wise, the _proper_ loss \(\ell\) is minimized by taking the Bayes optimal classifier \(\eta^{\star}(x)=\Pr[\mathsf{Y}=+1\mid\mathsf{X}=x]\). Thus taking the \(\operatorname{argmin}\) over all possible CPE classifiers, \(h^{\star}=\eta^{\star}\). We note that the point-wise risk taken by the Bayes optimal classifier is typical denoted as the Bayes point-wise risk \(\underline{L}(x)=\mathbb{E}_{\mathrm{P}_{\mathrm{y}|x=x}}[\ell(\mathsf{Y},\eta ^{\star}(x))]\)[60, 61].

As such, we are left to minimize \(r\) over,

\[\mathbb{E}_{\mathrm{P}}[(1-r(\mathsf{X}))\cdot\ell(\mathsf{Y}, \eta^{\star}(\mathsf{X}))+c\cdot r(\mathsf{X})]\] \[=\mathbb{E}_{\mathrm{P}_{\mathrm{x}}}\mathbb{E}_{\mathrm{P}_{ \mathrm{y}|x=x}}[(1-r(\mathsf{X}))\cdot\ell(\mathsf{Y},\eta^{\star}(\mathsf{X} ))+c\cdot r(\mathsf{X})]\] \[=\mathbb{E}_{\mathrm{P}_{\mathrm{x}}}[(1-r(\mathsf{X}))\cdot \underline{L}(\mathsf{X})+c\cdot r(\mathsf{X})]\] \[=\mathbb{E}_{\mathrm{P}_{\mathrm{x}}}[\underline{L}(\mathsf{X})]+ \mathbb{E}_{\mathrm{P}_{\mathrm{x}}}\left[r(\mathsf{X})\cdot(c-\underline{L}( \mathsf{X}))\right].\]

Thus, we immediately get the optimal \(r^{\star}(x)=[\underline{L}(x)\geq c]\). 

## Appendix B Proof of Theorem 3.3

Proof.: We use the theory of Lagrange multipliers to make different constraints explicit optimization problems.

Let us first consider the reduction from learning explicit distributions Eq. (8) to density ratios Eq. (9). First note that the objective Eq. (8) can be written as follows:

\[L(\mathrm{Q})+\lambda\cdot D_{\varphi}(\mathrm{P}\parallel\mathrm{ Q})\] \[=\int L^{\prime}(x)\mathrm{d}\mathrm{Q}(x)+\lambda\cdot\int \varphi\left(\frac{\mathrm{d}\mathrm{Q}}{\mathrm{dP}}\right)\mathrm{dP}(x)\] \[=\mathbb{E}_{\mathrm{P}}\left[L^{\prime}(x)\cdot\frac{\mathrm{d} \mathrm{Q}}{\mathrm{dP}}(x)+\lambda\cdot\varphi\left(\frac{\mathrm{d}\mathrm{Q }}{\mathrm{dP}}(x)\right)\right]\]

The reduction to Eq. (9) now follows from a reduction from minimizing over \(\mathrm{Q}\) to \(\mathrm{d}\mathrm{Q}/\mathrm{dP}\), noting that \(\mathrm{Q}\) is restricted to be on the simplex. As such, the simplex constraints are transfered to:

\[\frac{\mathrm{d}\mathrm{Q}}{\mathrm{dP}}\geq 0\quad\text{and}\quad\int\mathrm{dP }(x)\cdot\frac{\mathrm{d}\mathrm{Q}}{\mathrm{dP}}(x)=\mathbb{E}_{\mathrm{P}} \left[\frac{\mathrm{d}\mathrm{Q}}{\mathrm{dP}}(\mathsf{X})\right]=1,\]

where the former is the non-negativity of simplex elements and the latter is the normalization requirement. Hence, taking \(\rho=\rho_{\lambda}\doteq\frac{\mathrm{d}\mathrm{Q}}{\mathrm{dP}}\) completes the reduction. (we remove the subscript "\(\lambda\)" for the rest of the proof)

As such we have the optimization problem in Eq. (9), where we will convert the constraints into Lagrange multipliers, defining,

\[\mathcal{L}(\rho;a,b) =\mathbb{E}_{\mathrm{P}}\left[\rho(\mathsf{X})\cdot L^{\prime}( \mathsf{X})+\lambda\cdot\varphi(\rho(\mathsf{X}))\right]-\int a^{\prime}(x) \rho(x)\mathrm{d}x+b\cdot(1-\mathbb{E}_{\mathrm{P}}[\rho(\mathsf{X})])\] \[=\mathbb{E}_{\mathrm{P}}\left[\rho(\mathsf{X})\cdot L^{\prime}( \mathsf{X})+\lambda\cdot\varphi\left(\rho(\mathsf{X})\right)-a(\mathsf{X}) \cdot\rho(\mathsf{X})-b\cdot\rho(\mathsf{X})\right]+b,\]

where \(a(x)=a^{\prime}(x)\cdot\mathrm{P}(x)\).

We can obtain the first order optimality conditions by taking the functional derivative. Suppose that \(\delta>0\) and \(h\colon\mathcal{X}\to\mathbb{R}\) is any function. The functional derivative is given by,

\[\left.\frac{\mathrm{d}}{\mathrm{d}\delta}\mathcal{L}(\rho+\delta \cdot h;a,b)\right|_{\delta=0}\] \[=\mathbb{E}_{\mathrm{P}}\left[h(\mathsf{X})\cdot(L^{\prime}( \mathsf{X})+\lambda\cdot\varphi^{\prime}\left(\rho(\mathsf{X})+\delta\cdot h (Thus, the first order condition is,

\[0=\frac{\mathrm{d}}{\mathrm{d}\delta}\mathcal{L}(\rho+\delta\cdot h;a,b)\Bigg{|}_{ \delta=0}=\mathbb{E}_{\mathrm{P}}\left[h(\mathsf{X})\cdot(L^{\prime}(\mathsf{X})+ \lambda\cdot\varphi^{\prime}\left(\rho(\mathsf{X})\right)-a(\mathsf{X})-b) \right],\]

for all \(h\colon\mathcal{X}\to\mathbb{R}\). As the condition must hold for all \(h\) for an optimal \(\rho^{\star}\), we have that for all \(x\in\mathcal{X}\),

\[0 =L^{\prime}(x)+\lambda\cdot\varphi^{\prime}\left(\rho^{\star}(x) \right)-a(x)-b\] \[\iff \varphi^{\prime}\left(\rho^{\star}(x)\right) =\frac{b+a(x)-L^{\prime}(x)}{\lambda}\] \[\iff \rho^{\star}(x) =(\varphi^{\prime})^{-1}\left(\frac{b-L^{\prime}(x)+a(x)}{ \lambda}\right).\]

As required. 

## Appendix C Proof of Corollary 3.4

Proof.: First notice that the inner maximization in the DRO optimization Eq. (7) can be simplified as follows:

\[\sup_{\mathrm{Q}\in B_{\star}(\mathrm{P})}L(\mathrm{Q}) =-\left(\inf_{\mathrm{Q}\in B_{\star}(\mathrm{P})}-L(\mathrm{Q})\right)\] \[=-\left(\inf_{\mathrm{Q}\in\mathcal{D}(\mathcal{X})}\sup_{ \lambda\geq 0}-L(\mathrm{Q}_{\lambda})-\lambda\cdot(\varepsilon-D_{\varphi}( \mathrm{P}\parallel\mathrm{Q}_{\lambda}))\right)\] \[=-\left(\sup_{\lambda\geq 0}\inf_{\mathrm{Q}_{\lambda}\in \mathcal{D}(\mathcal{X})}-L(\mathrm{Q}_{\lambda})-\lambda\cdot(\varepsilon-D_ {\varphi}(\mathrm{P}\parallel\mathrm{Q}_{\lambda}))\right),\]

where the last inequality follows from Fan's minimax Theorem [25, Theorem 2] noting that \(\mathrm{Q}\mapsto L(\mathrm{Q}_{\lambda})\) is linear and the selected \(\varphi\) per Definition 2.2 makes \(\mathrm{Q}\mapsto D(\mathrm{P}\parallel\mathrm{Q}_{\lambda})\) a convex lower semi-continuous function.

Now notice that the inner minimization of this simplification is exactly our idealized rejection distribution objective Eq. (8) when we negate the loss. As such, noticing that solutions to Eq. (8) are exactly given by \(\mathrm{P}\cdot\rho\) yields the result after optimizing for the 'arguments' in the above DRO objective for both \(\lambda\) and \(\mathrm{Q}_{\lambda}\). 

## Appendix D Proof of Corollary 4.1

We defer the proof of Corollary 4.1 to Appendix G, which covers any \(\alpha\) including KL (\(\alpha=1\)).

## Appendix E Proof of Theorem 4.2

We breakdown Theorem 4.2 into two sub-theorems Theorem E.1 for each of the settings.

**Theorem E.1**.: _Given the binary CPE setting presented in Theorem 2.1 and that we are given the optimal classifier \(h^{\star}(x)=\mathrm{P}(\mathsf{Y}=1\mid\mathsf{X}=x)\), for any \(\lambda>0\) there exists a \(\tau>0\) such that the \(r^{\mathrm{KL}}_{\tau}\) rejector generated the optimal density ratio in Corollary 4.1 is equivalent to the optimal rejector in Theorem 2.1._

The proofs of each are similar. We first make the observation that the rejector function \(r^{\mathrm{KL}}_{\tau}\) can be simplified as follows:

\[r^{\mathrm{KL}}_{\tau}=\left[L^{\prime}(x)\leq-\lambda\left(\log Z+\log\tau \right)\right],\]

Now we note that given a fixed \(\lambda>0\) (which also fixes \(Z\)), the RHS term has a one-to-one mapping from \(\mathbb{R}_{+}\) to \(\mathbb{R}\). Thus all that is to verify is that thresholding \(L^{\prime}(x)\) is equivalent to the rejectors of Theorems 2.1 and N.1.

Proof.: For the CPE case, from assumptions, we have that \(L^{\prime}(x)=\mathbb{E}_{\mathsf{Y}\sim\mathrm{P}(\mathsf{Y}\mid\mathsf{X}=x )}[\ell(\mathsf{Y},h^{\star}(x))]=\mathbb{E}_{\mathsf{Y}\sim h^{\star}( \mathsf{X}=x)}[\ell(\mathsf{Y},h^{\star}(x))\text{ as }h^{\star}\text{ is optimal. Thus thresholding used in }r^{\mathrm{KL}}_{\tau}\) is equivalent to thresholding \(L^{\prime}(x)\) by keeping \(\lambda\) fixed and changing \(\tau\) appropriately.

Proof of Theorem 4.3

To prove the theorem, we will be using the standard Hoeffding's inequality [12].

**Theorem F.1** (Hoeffding's Inequality [12, Theorem 2.8]).: _Let \(\mathsf{X}_{1},\ldots,\mathsf{X}_{n}\) be independent random variables such that \(\mathsf{X}_{i}\) takes values in \([a_{i},b_{i}]\) almost surely for all \(i\leq n\). Defining \(\mathsf{X}=\sum_{i}\mathsf{X}_{i}-\mathbb{E}X_{i}\), then for every \(t>0\),_

\[\Pr\left(\mathsf{X}\geq t\right)\leq\exp\left(-\frac{2t^{2}}{\sum_{i=1}^{n}(b_ {i}-a_{i})^{2}}\right).\]

Proof.: Let us denote \(Z\) to be the normalizer with the true expectation \(\mathbb{E}_{\mathrm{P}}\) and \(\hat{Z}\) to be the normalizer with the empirical expectation \(\mathbb{E}_{\hat{\mathrm{P}}}\).

As \(\ell\) is bounded, we take note of the following bounds,

\[\exp(-B/\lambda)\leq\max\left\{\exp\left(\frac{-L^{\prime}(x)}{\lambda}\right),Z,\hat{Z}\right\}\leq\exp(B/\lambda).\]

This can be simply verified by taking the smallest and largest values of the \(\exp\).

Now we simply have

\[|\rho^{\text{KL}}(x)-\hat{\rho}^{\text{KL}}(x)| =\exp\left(\frac{-L^{\prime}(x)}{\lambda}\right)\cdot\left|\frac{ 1}{Z}-\frac{1}{\hat{Z}}\right|\] \[\leq\exp\left(\frac{B}{\lambda}\right)\cdot\left|\frac{1}{Z}-\frac {1}{\hat{Z}}\right|\] \[=\exp\left(\frac{B}{\lambda}\right)\cdot\frac{|Z-\hat{Z}|}{|Z \cdot\hat{Z}|}\] \[\leq\exp\left(\frac{B}{\lambda}\right)^{3}\cdot|Z-\hat{Z}|.\]

Notice that \(|Z-\hat{Z}|\) can be bounded via concentration inequality on (bounded) random variable \(\exp\left(\frac{-L^{\prime}(\mathsf{X})}{\lambda}\right)\).

Thus, we have that by Theorem F.1

\[\Pr\left(\left|\mathbb{E}_{\mathrm{P}}\left[\exp\left(\frac{-L^{\prime}(x)}{ \lambda}\right)\right]-\mathbb{E}_{\mathrm{P}}\left[\exp\left(\frac{-L^{\prime }(x)}{\lambda}\right)\right]\right|>t\right)\leq 2\exp\left(\frac{-2\cdot N\cdot t^{2} }{(b-a)^{2}}\right),\]

where \((b-a)^{2}=(\exp(B/\lambda)-\exp(-B/\lambda))^{2}=4\sinh^{2}(B/\lambda)\).

Taking a union bound over \(\mathcal{T}\), we have that

\[\Pr\left(\exists x\in\mathcal{T}:\left|\mathbb{E}_{\mathrm{P}}\left[\exp\left( \frac{-L^{\prime}(x)}{\lambda}\right)\right]-\mathbb{E}_{\mathrm{P}}\left[ \exp\left(\frac{-L^{\prime}(x)}{\lambda}\right)\right]\right|>t\right)\leq 2M \exp\left(\frac{-N\cdot t^{2}}{2\sinh^{2}(B/\lambda)}\right).\]

Thus taking \(t=\sinh(B/\lambda)\cdot\sqrt{\frac{2}{N}\cdot\log\left(\frac{2M}{\delta}\right)}\), we have that with probability \(1-\delta\) for \(\delta>0\), for any \(x\in\mathcal{T}\),

\[|\rho^{\text{KL}}(x)-\hat{\rho}^{\text{KL}}(x)| \leq\exp\left(\frac{B}{\lambda}\right)^{3}\cdot|Z-\hat{Z}|\] \[\leq\exp\left(\frac{B}{\lambda}\right)^{3}\cdot\sinh(B/\lambda) \cdot\sqrt{\frac{2}{N}\cdot\log\left(\frac{2M}{\delta}\right)}.\]

As required.

Proof of Theorem 4.5

Before proving the theorems, we first state some basic properties of \(\psi_{\alpha}\).

**Lemma G.1**.: _For \(\alpha\neq-1\), \(\psi_{\alpha}(u\cdot v)=\psi_{\alpha}(u)\cdot\psi_{\alpha}(v)\) and \(\psi_{\alpha}(1/v)=1/\psi_{\alpha}(v)\). Furthermore, these statements hold when \(\psi_{\alpha}\) is replaced with \(\psi_{\alpha}^{-1}\)._

**Lemma G.2**.: \[\psi_{\alpha}^{-1}(z)=\begin{cases}z^{\frac{2}{1-\alpha}}&\text{if }\alpha\neq 1\\ \exp(z)&\text{otherwise}\end{cases}.\] (18)

The above statements follows directly from definition and simple calculation. The next statement directly connects \(\varphi_{\alpha}^{\prime}\) to \(\psi_{\alpha}\).

**Lemma G.3**.: _For \(\alpha\neq 1\),_

\[(\varphi_{\alpha}^{\prime})(z)=\frac{2}{\alpha-1}\cdot\psi_{\alpha}(1/z). \tag{19}\]

Proof.: We prove this via cases.

\(\bullet\)\(\alpha=-1\):

\(\varphi_{-1}^{\prime}(z)=\frac{\mathrm{d}}{\mathrm{d}z}(-\log z)=-z^{-1}=-1 \cdot z^{-\frac{1-\alpha}{2}}=\frac{2}{\alpha-1}\cdot\psi_{\alpha}(1/z).\)

\(\bullet\)\(\alpha\neq\pm 1\):

\(\varphi_{\alpha}^{\prime}(z)=\frac{\mathrm{d}}{\mathrm{d}z}\left(\frac{4}{1- \alpha^{2}}\cdot\left(1-z^{\frac{1+\alpha}{2}}\right)\right)=-\frac{2}{1- \alpha}z^{\frac{\alpha-1}{2}}=-\frac{2}{1-\alpha}\cdot z^{-\frac{1-\alpha}{2}} =\frac{2}{\alpha-1}\cdot\psi_{\alpha}(1/z)\)

We now define the following constants depending on \(\alpha\):

\[c_{\alpha}=\begin{cases}\psi_{\alpha}^{-1}\left(-\frac{2}{1-\alpha}\right)& \text{if }\alpha\neq 1\\ \psi_{\alpha}^{-1}(-1)=\exp(-1)&\text{otherwise}.\end{cases} \tag{20}\]

**Lemma G.4**.: _For \(\alpha\neq 1\),_

\[(\varphi_{\alpha}^{\prime})^{-1}(z)=c_{\alpha}\cdot\frac{1}{\psi_{\alpha}^{-1} (z)}. \tag{21}\]

Proof.: We prove this via cases.

\(\bullet\)\(\alpha=-1\):

\(\varphi_{-1}^{\prime}(z)=\frac{\mathrm{d}}{\mathrm{d}z}(-\log z)=-z^{-1}=-1 \cdot z^{-\frac{1-\alpha}{2}}=-1\cdot\psi_{\alpha}(1/z).\)

Then,

\((\varphi_{-1}^{\prime})^{-1}=\frac{1}{\psi_{\alpha}^{-1}\left(-1\cdot z\right) }=\psi_{\alpha}^{-1}\left(\frac{1}{-1\cdot z}\right)=\psi_{\alpha}^{-1}\left( -1\right)\cdot\frac{1}{\psi_{\alpha}^{-1}(z)}.=c_{\alpha}\cdot\frac{1}{\psi_{ \alpha}^{-1}\left(z\right)}.\)

\(\bullet\)\(\alpha\neq\pm 1\):

\(\varphi_{\alpha}^{\prime}(z)=\frac{\mathrm{d}}{\mathrm{d}z}\left(\frac{4}{1- \alpha^{2}}\cdot\left(1-z^{\frac{1+\alpha}{2}}\right)\right)=-\frac{2}{1- \alpha}z^{\frac{n-1}{2}}=-\frac{2}{1-\alpha}\cdot z^{-\frac{1-\alpha}{2}}=- \frac{2}{1-\alpha}\cdot\psi_{\alpha}(1/z)\)

Then,

\((\varphi_{\alpha}^{\prime})^{-1}=\frac{1}{\psi_{\alpha}^{-1}\left(-\frac{1- \alpha}{2}\cdot z\right)}=\frac{1}{\psi_{\alpha}^{-1}\left(-\frac{1-\alpha}{2 }\right)\cdot\psi_{\alpha}^{-1}\left(z\right)}=\psi_{\alpha}^{-1}\left(- \frac{2}{1-\alpha}\right)\cdot\frac{1}{\psi_{\alpha}^{-1}\left(z\right)}=c_{ \alpha}\cdot\frac{1}{\psi_{\alpha}^{-1}\left(z\right)}\)

**Lemma G.5**.: _For \(\alpha=1\),_

\[(\varphi^{\prime}_{\alpha})^{-1}(z)=c_{\alpha}\cdot\psi_{\alpha}^{-1}(z). \tag{22}\]

Proof.: \(\bullet\quad\alpha=1\):

\[\varphi^{\prime}_{1}(z)=\frac{\mathrm{d}}{\mathrm{d}z}\left(z\log z\right)=\log z +1\]

Then,

\[(\varphi^{\prime}_{-1})^{-1}=\exp(z-1)=\exp(-1)\cdot\exp(z)=\psi_{1}^{-1}(-1) \cdot\psi_{1}^{-1}(z)=c_{\alpha}\cdot\psi_{\alpha}^{-1}(z)\]

Thus now via Lemmas G.4 and G.5, we can prove the Theorems.

Proof of Corollary 4.1 and Theorem 4.5.: The proof follows from utilizing either Lemmas G.4 and G.5 in conjunction with Theorem 3.3.

\(\bullet\quad\alpha=1\): We have that,

\[\rho^{\varphi}_{\lambda}(x) =(\varphi^{\prime})^{-1}\left(\frac{a(x)-L^{\prime}(x)+b}{\lambda }\right)\] \[=c_{\alpha}\cdot\psi_{\alpha}^{-1}\left(\frac{a(x)-L^{\prime}(x)+ b}{\lambda}\right)\] \[=\exp\left(\frac{a(x)-L^{\prime}(x)+b-1}{\lambda}\right).\]

As \(\rho^{\varphi}_{\lambda}(x)\) for \(\alpha=1\) is already positive by '\(\mathrm{exp}\)', by complementary slackness, the Lagrange multipliers \(a(\cdot)=0\). Hence, we can further simplify the above,

\[\rho^{\varphi}_{\lambda}(x)=\exp\left(\frac{-L^{\prime}(x)+b-\lambda}{\lambda} \right).\]

The normalizer then can be easily calculated, renaming \(b^{\prime}=b-\lambda\), we simplify have that

\[\rho^{\varphi}_{\lambda}(x) =\exp\left(\frac{-L^{\prime}(x)+b^{\prime}}{\lambda}\right)\] \[=\frac{1}{\exp(-b^{\prime}/\lambda)}\cdot\exp\left(\frac{-L^{ \prime}(x)}{\lambda}\right),\]

which by normalization condition and setting \(Z\doteq\exp(-b^{\prime}/\lambda)\),

\[1=\mathbb{E}\left[\frac{1}{Z}\cdot\exp\left(\frac{-L^{\prime}(x)}{\lambda} \right)\right]\]

\[\Longleftrightarrow\quad Z=\mathbb{E}\left[\exp\left(\frac{-L^{\prime}(x)}{ \lambda}\right)\right],\]

which completes the case (Corollary 4.1).

\(\bullet\quad\alpha\neq 1\): We have that,

\[\rho^{\varphi}_{\lambda}(x) =(\varphi^{\prime})^{-1}\left(\frac{a(x)-L^{\prime}(x)+b}{\lambda }\right)\] \[=c_{\alpha}\cdot\left(\psi_{\alpha}^{-1}\left(\frac{a(x)-L^{ \prime}(x)+b}{\lambda}\right)\right)^{-1}\] \[\stackrel{{(a)}}{{=}}\psi_{\alpha}^{-1}\left(\frac{2 }{\alpha-1}\right)\cdot\left(\psi_{\alpha}^{-1}\left(\frac{a(x)-L^{\prime}(x) +b}{\lambda}\right)\right)^{-1}\] \[=\psi_{\alpha}^{-1}\left(\frac{2}{\alpha-1}\right)\cdot\psi_{ \alpha}^{-1}\left(\left(\frac{a(x)-L^{\prime}(x)+b}{\lambda}\right)^{-1}\right)\] \[=\psi_{\alpha}^{-1}\left(\frac{2}{\alpha-1}\cdot\left(\frac{a(x) -L^{\prime}(x)+b}{\lambda}\right)^{-1}\right),\]

where \((a)\) we exploit the that \((\psi_{\alpha}^{-1}(z))^{-1}=\psi_{\alpha}^{-1}(1/z)\) via Lemma G.1.

Proof of Corollary 4.6

Proof.: First we simplify the density ratio rejector.

\[\rho_{\lambda}^{\alpha}(x) =\psi_{\alpha}^{-1}\left(\frac{2}{\alpha-1}\cdot\left(a(x)-\frac{L^ {\prime}(x)}{\lambda}+b\right)^{-1}\right)\] \[=\left(\frac{\alpha-1}{2}\cdot\left(a(x)-\frac{L^{\prime}(x)}{ \lambda}+b\right)\right)^{\frac{m-1}{2}}.\]

We suppose that it is possible for

\[\left(\frac{\alpha-1}{2}\cdot\left(a(x)-\frac{L^{\prime}(x)}{\lambda}+b\right) \right)^{\frac{\alpha-1}{2}}<0\]

for values of \(a(x)\), \(b\), and \(\lambda\). Otherwise, \(\alpha(x)=0\) and we are done due to the above equation's non-negativity.

Let \(x\in\mathcal{X}\) be arbitrary. Suppose that \(a(x)=0\). Then by complementary slackness, we have that

\[\left(\frac{\alpha-1}{2}\cdot\left(-\frac{L^{\prime}(x)}{\lambda}+b\right) \right)^{\frac{\alpha-1}{2}}>0\iff b-\frac{L^{\prime}(x)}{\lambda}>0.\]

By contra-positive, we have that \(b-L^{\prime}(x)/\lambda\leq 0\) implies that \(a(x)\neq 0\). By prime feasibility, in this case we also have \(\rho(x)=0\). We can solve either case by using the maximum as stated. 

## Appendix I Proof of Corollary 4.7

Proof of Corollary 4.6.: The proof directly follows from a property of the \(\alpha\)-divergence when one of the measure have disjoint support. From [3] we have.

**Theorem I.1** ([3, Section 3.4.1 (4)]).: _For \(\alpha\)-divergences, we have that_

1. _For_ \(\alpha\leq-1\)_,_ \(D_{\alpha}(\mathrm{P}\parallel\mathrm{Q})=\infty\) _when_ \(\mathrm{P}(x)\neq 0\) _and_ \(\mathrm{Q}(x)=0\) _for some_ \(x\in\mathcal{X}\)_._

This result immediately gives the result, as otherwise the objective function is \(\infty\). 

## Appendix J Proof of Corollary 4.8

Proof.: We first note simplifying \(\rho_{\lambda}^{\alpha=3}\) via Corollary 4.6 yields:

\[\rho_{\lambda}^{\alpha=3}(x)=\max\left\{0,b-\frac{L^{\prime}(x)}{\lambda} \right\}.\]

Thus our goal is to solve \(b\) to give \(\mathbb{E}_{\mathrm{P}}[\rho_{\lambda}^{\alpha=3}(\mathsf{X})]=1\).

Now, consider the following,

\[1+\frac{\mathbb{E}_{\mathrm{P}}\left[L^{\prime}(\mathsf{X})\right]}{\lambda}- \frac{L^{\prime}(x)}{\lambda}>0\iff\lambda>L^{\prime}(x)-\mathbb{E}_{\mathrm{P }}\left[L^{\prime}(\mathsf{X})\right],\]

where the latter holds uniformly for all \(x\) from assumptions on \(\lambda\).

Thus setting \(b=1+\frac{\mathbb{E}_{\mathrm{P}}\left[L^{\prime}(\mathsf{X})\right]}{\lambda}\), we simplify

\[\int\max\left\{0,1+\frac{\mathbb{E}_{\mathrm{P}}\left[L^{\prime} (\mathsf{X})\right]}{\lambda}-\frac{L^{\prime}(\mathsf{X})}{\lambda}\right\} \mathrm{dP}(x)\] \[=\int 1+\frac{\mathbb{E}_{\mathrm{P}}\left[L^{\prime}(\mathsf{X}) \right]}{\lambda}-\frac{L^{\prime}(\mathsf{X})}{\lambda}\mathrm{dP}(x)\] \[=1.\]

As such, \(b=1+\frac{\mathbb{E}_{\mathrm{P}}\left[L^{\prime}(\mathsf{X})\right]}{\lambda}\) solves the required normalization. Substituting \(b\) back into \(\rho_{\lambda}^{\alpha=3}(x)\) yields the Theorem.

Proof of Theorem 4.9

Proof.: First we note that for any meas \(\mathrm{R}\), \(\lambda>2B\) implies that \(\lambda>L^{\prime}(x)-\mathbb{E}_{\mathrm{R}}\left[L^{\prime}(\mathsf{X})\right]\) (taking largest and smallest values of \(\ell\).

As such, for both \(\mathrm{P},\hat{\mathrm{P}}\) have closed forms Corollary 4.8

Thus, the bound can be simply shown to have,

\[|\rho_{\lambda}^{\alpha=3}(x)-\hat{\rho}_{\lambda}^{\alpha=3}(x)|=\frac{1}{ \lambda}\cdot\left|\mathbb{E}_{\mathrm{P}}\left[L^{\prime}(\mathsf{X})\right] -\mathbb{E}_{\hat{\mathrm{P}}}\left[L^{\prime}(\mathsf{X})\right]\right|.\]

Thus by Hoeffding's inequality Theorem F.1 and union bound (see for instance the proof of Theorem 4.3), setting \(t=B\cdot\sqrt{\frac{2}{N}\log\left(\frac{2M}{\delta}\right)}\), we have that with probability \(1-\delta\) for all \(x\in\mathcal{T}\),

\[|\rho_{\lambda}^{\alpha=3}(x)-\hat{\rho}_{\lambda}^{\alpha=3}(x)|\leq\frac{B}{ \lambda}\cdot\sqrt{\frac{2}{N}\log\left(\frac{2M}{\delta}\right)}.\]

As required. 

## Appendix L Broader Impact

The paper presents work which reinterprets the classification with rejection problem in terms of learning distributions and density ratios. Beyond advancing Machine Learning in general, potential societal consequences include enhancing the understanding of the rejection paradigm and, consequentially, the human-in-the-loop paradigm. The general rejection setting aims to prevent models from making prediction when they are not confident, which can have societal significance when deployed in high stakes real life scenarios -- allowing for human intervention.

## Appendix M Distribution Generalization Bounds

In the following, we seek to provide generalization bounds on \(\hat{\mathrm{Q}}_{\lambda,N}\doteq\hat{\mathrm{P}}_{N}\cdot\hat{\rho}_{\lambda,N}\). That is, one seeks to know that as \(N\to\infty\) how and if \(\hat{\mathrm{Q}}_{\lambda,N}\to\mathrm{Q}\). A natural measure of distance for probability measures is _total variation_[20],

\[\mathrm{TV}(\mathrm{P},\mathrm{Q})\doteq\frac{1}{2}\cdot\|\mathrm{P}-\mathrm{Q }\|_{1}=\int|\mathrm{Q}(x)-\mathrm{P}(x)|\;\mathrm{d}x.\]

Let us also define

\[\|\mathrm{P}-\mathrm{Q}\|_{\infty}=\sup_{x\in\mathcal{X}}|\mathrm{Q}(x)- \mathrm{P}(x)|.\]

One immediately gets a rate if we assume that \(|\mathcal{X}|\) is finite and we have a bound for \(\hat{\rho}_{\lambda,N}\).

**Theorem M.1**.: _Suppose that \(|\mathcal{X}|=M<\infty\) is finite and bounded density ratio \(|\hat{\rho}_{\lambda,N}|\leq B^{\prime}\) for \(B^{\prime}>0\). Then, if we have that with probability \(1-\delta\) that,_

\[\|\rho(x)-\hat{\rho}_{N,\lambda}(x)\|_{\infty}\leq C\cdot\left(\sqrt{\frac{1}{ N}\cdot\log\frac{2M}{\delta}+C^{\prime}}\right),\]

_for some \(C,C^{\prime}>0\), then we have that_

\[\mathrm{TV}(\mathrm{Q},\hat{\mathrm{Q}}_{\lambda,N})=\mathcal{O}\left(\sqrt{ \frac{M^{2}\log M}{N}}\right). \tag{23}\]

Proof.: Noting that the empirical distribution is a sum of Dirac deltas \(\hat{\mathrm{P}}_{N}(x)=\sum_{i\in[N]}\delta(x-x_{i})\), we can establish a simple bound via a consequence of Hoeffding's Theorem Theorem F.1 (also noting that \(\mathrm{P}=\mathbb{E}\hat{\mathrm{P}}_{N}\)). We have that,

\[\Pr(|\mathrm{P}(x)-\hat{\mathrm{P}}_{N}(x)|\geq t)\leq 2\exp\left(-2Nt^{2} \right).\]Thus,

\[\Pr(\forall x\in\mathscr{X}:|\mathrm{P}_{N}(x)-\hat{\mathrm{P}}_{N}(x)|\geq t)\leq 2 M\exp\left(-2Nt^{2}\right).\]

Setting \(t=\sqrt{\frac{1}{2N}\log\frac{2M}{\delta}}\), we have with probability \(1-\delta\)

\[\|\mathrm{P}-\hat{\mathrm{P}}_{N}\|_{\infty}\leq\sqrt{\frac{1}{2N}\log\frac{2M} {\delta}}.\]

We now consider the following:

\[\|\mathrm{Q}-\hat{\mathrm{Q}}_{\lambda,N}\|_{\infty} =\|\mathrm{P}\cdot\rho-\hat{\mathrm{P}}_{N}\cdot\hat{\rho}_{ \lambda,N}\|_{\infty}\] \[=\|\mathrm{P}\cdot(\rho-\rho_{\lambda,N})+(\mathrm{P}-\hat{ \mathrm{P}}_{N})\cdot\hat{\rho}_{\lambda,N}\|_{\infty}\] \[\leq\|\mathrm{P}\cdot(\rho-\hat{\rho}_{\lambda,N})\|_{\infty}+\| (\mathrm{P}-\hat{\mathrm{P}}_{N})\cdot\hat{\rho}_{\lambda,N}\|_{\infty}\] \[\leq\|(\rho-\hat{\rho}_{\lambda,N})\|_{\infty}+B^{\prime}\cdot\| (\mathrm{P}-\hat{\mathrm{P}}_{N})\|_{\infty}.\]

Taking a union bound of the above inequality and our assumption, we have that, for some \(C>0\), with probability \(1-\delta\),

\[\|\mathrm{Q}-\hat{\mathrm{Q}}_{\lambda,N}\|_{\infty} \leq\|(\rho-\hat{\rho}_{\lambda,N})\|_{\infty}+B^{\prime}\cdot\| (\mathrm{P}-\hat{\mathrm{P}}_{N})\|_{\infty}\] \[\leq C\cdot\sqrt{\frac{1}{N}\log\frac{4M}{\delta}+C^{\prime}}+B^{ \prime}\cdot\sqrt{\frac{1}{2N}\log\frac{4M}{\delta}}\] \[=\mathscr{O}\left(\sqrt{\frac{\log M}{N}}\right).\]

Converting the bound to TV amounts to simply summing over \(\mathscr{X}\), which gives \(\mathrm{TV}(\mathrm{Q},\hat{\mathrm{Q}}_{\lambda,N})=\mathscr{O}(\sqrt{(M^{2} \log M)/N})\), as required. 

Notably, with appropriate assumptions, Theorems 4.3 and 4.9 can be used with Theorem M.1 to get bounds for \(\hat{\mathrm{Q}}_{\lambda,N}^{\mathrm{KL}}\) and \(\hat{\mathrm{Q}}_{\lambda,N}^{\alpha=3}\).

## Appendix N Rejection for Regression

In the main-text of the paper we have focused on classification. However, many of the idea discussed can be extended for the regression setting. For instance, similar to Chow's Rule in Theorem 2.1 we can express the regression equivalent to the optimal solution of Eq. (2).

**Theorem N.1** (Optimal Regression Rejection [72]).: _Let us consider the regression setting, where \(\mathfrak{y}=\mathfrak{y}^{\prime}=\mathbb{R}\) and \(\ell(y,y^{\prime})=\frac{1}{2}(y-y^{\prime})^{2}\). Then w.r.t. Eq. (2), the optimal model is given by \(h^{\star}(x)=\mathbb{E}_{\mathrm{P}}[\mathsf{Y}\mid\mathsf{X}=x]\) and the optimal rejector is given by \(r^{\star}(x)=\llbracket\sigma^{2}(x)\leq c\rrbracket\), where \(\sigma^{2}(x)\) is the conditional variance of \(\mathsf{Y}\) given \(\mathsf{X}=x\)._

For regression, there is no clear analogous notion of output "confidence score" unless the model explicitly outputs probabilities. Indeed, rejection method for regression explicitly requires the estimation of the target variable's conditional variance [72].

Similar to the CPE case, our KL density ratio rejector can provide a rejection policy equivalent to the typical case.

**Theorem N.2**.: _Given the regression setting presented in Theorem N.1 and that we are given optimal regressor \(h^{\star}(x)=\mathbb{E}_{\mathrm{P}}[\mathsf{Y}\mid\mathsf{X}=x]\), for any \(\lambda>0\) there exists a \(\tau>0\) such that the \(r^{\mathrm{KL}}_{\tau}\) rejector generated the optimal density ratio in Corollary 4.1 is equivalent to the optimal rejector in Theorem N.1._

Proof.: The proof follows similarly to the proof of Theorem E.1. The regression case follows almost identically, by noticing that \(L^{\prime}(x)=\sigma^{2}(x)\) is the variance. This is similar to the proof of Theorem N.1[72].

Despite the equivalence, there is a difficult in using the density ratio rejectors, as per the closed form equations of Section 4, for regression. Estimating \(L^{\prime}(x)=\sigma^{2}(x)\) is challenging. Unlike classification where learning calibrated classifiers has a variety of approaches, learning a regression model which explicitly outputs probabilities is quite difficult. As such, approximating \(\mathrm{P}(\mathsf{Y}\mid\mathsf{X}=x)\) with the model \(h(x)\) cannot be done.

## Appendix O Gradient of Density Ratio Objective

As an alternative to the closed-form rejectors explored in the main-text, one may want to explore a method to learn \(\rho\) iteratively. We consider the gradients of the optimization problem in Eq. (9). In practice, we found that we were unable to learn such rejectors via taking gradient updates and thus leave a concrete implementation of the idea for future work.

The idea comes from utilizing the "variational" aspect of the GVI formulation (which was not explored in the main-text). We suppose that the rejectors we are interested in come from a parameterized family. In particular, we consider the self-normalizing family \(\rho_{\vartheta}/Z_{\vartheta}\), where \(Z_{\vartheta}=\mathbb{E}_{\mathrm{P}}[\pi_{\vartheta}(\mathsf{X})]\) normalizes the rejector such that \(\mathbb{E}_{\mathrm{P}}[\rho_{\vartheta}(\mathsf{X})]=1\). Having the \(Z_{\vartheta}\) normalizing term means that the constraint in Eq. (9) is satisfied for any \(\vartheta\). The only constraint that we must have for \(\pi_{\vartheta}\) is non-negativity, _i.e._, \(\pi_{\vartheta}\) is a neural network with exponential last activation functions from \(\mathcal{X}\to\mathbb{R}_{+}\). By setting a parametric form of \(\rho_{\vartheta}\), we implicitly restrict the set of idealized distributions to \(\mathrm{Q}_{\vartheta}=\mathrm{P}\cdot\rho_{\vartheta}\). The gradients of such a parametric form can be calculated as follows.

**Corollary O.1**.: _Let \(\rho_{\vartheta}=\pi_{\vartheta}(x)/Z_{\vartheta}\). Then the gradient of Eq. (9) w.r.t. \(\vartheta\) is given by,_

\[\mathbb{E}_{\mathrm{P}_{\mathrm{x},y}}\left[\nabla_{\vartheta}\left(\frac{\pi_ {\vartheta}(\mathsf{X})}{Z_{\vartheta}}\right)\cdot\left(\varphi^{\prime} \left(\frac{\pi_{\vartheta}(\mathsf{X})}{Z_{\vartheta}}\right)+\lambda\cdot \ell(\mathsf{Y},h(\mathsf{X}))\right)\right]. \tag{24}\]

Proof.: The proof is immediate from differentiation of Eq. (9). 

An alternative form of Eq. (24) can be found by noticing that \(\nabla f=f\cdot\nabla\log f\). This provides an expression in terms of the log density ratio.

\[\mathbb{E}_{\mathrm{P}_{\mathrm{x},y}}\left[\frac{\pi_{\vartheta}(\mathsf{X}) }{Z_{\vartheta}}\cdot\nabla_{\vartheta}\log\left(\frac{\pi_{\vartheta}( \mathsf{X})}{Z_{\vartheta}}\right)\cdot\left(\varphi^{\prime}\left(\frac{\pi_ {\vartheta}(\mathsf{X})}{Z_{\vartheta}}\right)+\lambda\cdot\ell(\mathsf{Y},h( \mathsf{X}))\right)\right].\]

One will notice that the gradient is in-fact the log-likelihood of the idealized distribution: noting \(\mathrm{P}\) free of \(\vartheta\), we have \(\nabla_{\vartheta}\log(\pi_{\vartheta}(\mathsf{X})/Z_{\vartheta})=\nabla_{ \vartheta}\log(\mathrm{P}(\mathsf{X})\cdot\pi_{\vartheta}(\mathsf{X})/Z_{ \vartheta})=\nabla_{\vartheta}\log\mathrm{Q}_{\vartheta}\). As such, the gradient Eq. (24) is equivalent to the gradient of a weighted log-likelihood.

Despite the potential nice form of the gradient, we found that learning rejector through this was not possible. One limiting factor of computing such gradients is that we need to estimate \(Z_{\vartheta}\) at each gradient calculation, _i.e._, this must happen whenever \(\vartheta\) changes. This can be particularly costly when \(\mathcal{X}\) is high-dimensional. Secondly, we suspect that the model capacity of \(\pi_{\vartheta}\) was not sufficient: we only tested on simple neural networks and convolutions neural networks, mirroring the architecture of the base classifiers used in our experimental setting.

## Appendix P Finding the Best Rejection Cost

Both the baselines and density ratio approaches evaluated in Section 5 require a tuning of hyperparameters \(c,\tau\). Practically, we are more interested in the rejection rate \(\mathrm{P}[r(\mathsf{X})=1]\) rather than the abstract choices of \(c,\tau\). Hence, one often wishes to select \(c,\tau\) according to a coverage constraint.

One of the simplest choices of selecting \(c,\tau\) is to utilize a calibration set and approximate the rejection rate / coverage on this data. Our experiments in Section 5 mirrors this in Tables 1 and 1 by treating the final test set as the calibration data.

Other works in the literature utilize more sophisticated methods for deriving coverage constraints. [29] picks threshold values which provide a guarantee on a coverage-modified risk requirement. Another approach [57] does not require an additional calibration set and instead fits threshold values \(\tau\) by exploiting stacking confidences and (stratified) cross-fitting. Although one could consider such approaches for all baselines, including those which utilize the cost-model of rejection via \(c\), one will have to pay a price in retraining the learned rejector \(r\) for each instance of the hyperparameter \(c\) that is searched. As a result, approaches which can trade-off accuracy and coverage via a threshold variable \(\tau\) (rather than an expensive retraining) are computationally preferable when exhaustively searching for a good \(c,\tau\).

We leave more sophisticated methods for selecting \(\tau\) for density ratio rejectors for future work.

## Appendix Q Additional Experimental Details

### Training Settings

The neural network architecture used to train the base classifiers and baselines are almost identical. For the baseline approaches which have output dimension which is different than the output of the original neural network, we modify the last linear layer of the base classifier's architecture to fit the baseline's requirements, _e.g._, adding an additional output dimension for rejection in DEFER. Our architectures utilize batch normalization [38] and dropout [65] in a variety of places. Training settings are mostly identical, with some baselines requiring slight changes.

The base model's architecture is as follows.

* HAR (CC BY 4.0): We utilize a two hidden layer neural network with batch normalization. Both hidden layer is \(64\) neurons and the activation function is the sigmoid function. We take a 64 batch size, 40 training epochs and a 0.0001 learning rate.
* Gas Drift (CC BY 4.0): We utilize a two hidden layer neural network with batch normalization. Both hidden layers are \(64\) neurons and the activation function is the sigmoid function. We take a 64 batch size, 40 training epochs and a 0.0001 learning rate.
* MNIST (CC BY-SA 3.0): We utilize a convolutional neural network with two convolutional layers and two linear layers. The architecture follows directly from the MNIST example for PyTorch. We utilize the sigmoid function activation function. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.
* CIFAR-10 (CC BY 4.0): We utilize a ResNet-18 classifier. Random cropping and horizontal flipping data augmentation is utilized in training. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.
* OrgMNIST (CC BY 4.0): We utilize a ResNet-18 classifier. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.
* OctMNIST (CC BY 4.0): We utilize a ResNet-18 classifier. We take a 256 batch size, 40 training epochs and a 0.0001 learning rate.

For CSS, as noted in [53], batch normalization is needed at the final layer to stabilize training.

All training utilizes the Adam [41] optimizer.

All datasets we consider are in the public domain, _e.g._, UCI [6].

### Extended Table and Plots

Table 1 presents an extended version of Table 1 over coverage targets of 80% and 90%. The standard deviation is additionally reported. One can see the observation from the main text are consistent with this extended table.

Plots Figs. 1 to 3 show Fig. 2 over and extend region of acceptable coverage percentages. In addition, we include a larger range of noise rates for HAR and Gas Drift. For MNIST, we explore a larger range of noise rates for MNIST in Fig. 3 for our density ratio rejectors. We find that the findings in the main text are extended to these additional noise rates. In particular, we find that the our density ratio rejectors can be competitive with the various baseline approaches. We find that our density ratio approaches can have more fine-grained trade-offs at higher ranges of acceptance coverage. This is an important region where the budge for rejection may be low (and only a few examples, _e.g._\(<10\%\), can be rejected). Indeed, the baseline approaches which do not 'wrap' a base model require a lower _maximum acceptance coverage_ as the noise rate increases (the approaches require a higher rejection % for any type of rejection). Nevertheless, we do see a downside of the density ratio approach: the 

[MISSING_PAGE_EMPTY:26]

[MISSING_PAGE_FAIL:27]

size setting can be useful in the related learning with deferral setting [52], where having a small model to defer to a larger model is needed.

The results are reported in Figs. 8 and 9. We can see that in this setting, PredRej and our density ratio approaches are more competitive. This might indicate that for simple base models, approaches which 'wrap' a base model for rejection can be quite effective (especially in higher coverage regimes). In general, it seems with this smaller architecture regime, the 'non-wrapping' baseline approaches only provide rejection options when the acceptance coverage is lower than \(70\%\).

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Dataset** & BaseClf & \(\alpha-\mathrm{Rej}\) & DEFER & GCE & CSS & \(\mathrm{PredRej}\) \\ \hline HAR & 40,647 & 40,648 & 40,646 & 40,711 & 40,711 & 80,968 \\ Gas Drift & 12,935 & 12,936 & 12,934 & 12,999 & 12,999 & 25,544 \\ MNIST & 1,199,883 & 1,199,884 & 1,200,138 & 1,200,011 & 1,200,267 & 2,398,860 \\ CIFAR-10 & 21,282,123 & 21,282,124 & 21,283,146 & 21,282,635 & 21,283,659 & 42,560,652 \\ OctMNIST & 11,169,733 & 11,169,734 & 11,170,756 & 11,170,245 & 11,171,269 & 22,338,950 \\ OrganMNIST & 11,173,324 & 11,173,325 & 11,174,347 & 11,173,836 & 11,174,860 & 22,342,541 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Default parameter sizes of experiments.

Figure 2: Extended plots for Gas Drift of Fig. 2.

### Parameter sweeps over \(\alpha\) and \(\lambda\)

The following shows parameter sweeps over \(\alpha\) and \(\lambda\) for our density ratio rejectors. These are given by Figs. 1 and 2 respectively. We find that increasing \(\alpha\) compresses the trade-off curve from both sides. While decrease \(\lambda\) extends the trade-off curve on the left side.

Figure 3: Extended plots for MNIST of Fig. 2.

Figure 4: Deferred extended plots for CIFAR-10 of Fig. 2.

Figure 6: Deferred extended plots for OctMNIST of Fig. 2.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Dataset** & BaseClf & \(\alpha-\mathrm{Rej}\) & DEFER & GCE & CSS & \(\mathrm{PredRej}\) \\ \hline HAR & 36,487 & 36,488 & 36,486 & 36,551 & 36,551 & 72,648 \\ Gas Drift & 8,775 & 8,776 & 8,774 & 8,839 & 8,839 & 17,224 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Alternative parameter sizes of experiments.

Figure 5: Deferred extended plots for OctMNIST of Fig. 2.

Figure 7: MNIST with different noises for density ratio approaches.

Figure 8: Plots for HAR with smaller models.

Figure X: \(\alpha\) parameter sweep over dataset + noise combinations. S.t.d. shade is not utilized, but instead end points of the trade-off curves for \(\tau\in(0,1]\) are shown via vertical bars

Figure XI: \(\lambda\) parameter sweep over dataset + noise combinations for KL rejector. S.t.d. shade is not utilized, but instead end points of the trade-off curves for \(\tau\in(0,1]\) are shown via vertical bars

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Claims are justified via formal results and experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are appropriately discussed throughout the paper. Furthermore, the last section of the main-text also explicitly discusses limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Assumptions are stated and complete proofs are provided in Appendix. 1. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? 1. Answer: [Yes] 2. Justification: Detailed settings of experiments are provided in Appendix. Code is also included. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: Code and scripts to create small scale experiments are provided. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All included in the main-text and also the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars are given in plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We state the AWS resources utilized and estimated total compute time. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Research conforms with the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We provide a broader impact section in the Appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No release. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Baselines and data are correctly credited (with licenses given in the Appendix). Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA]  Justification: No new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing and research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.