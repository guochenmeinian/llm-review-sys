# Efficient Multi-task LLM Quantization and Serving

for Multiple LoRA Adapters

 Yifei Xia

Peking University

yifeixia@stu.pku.edu.cn

&Fangcheng Fu

Peking University

ccchengff@pku.edu.cn

&Wentao Zhang

Peking University

wentao.zhang@pku.edu.cn

&Jiawei Jiang

Wuhan University

jiawei.jiang@whu.edu.cn

&Bin Cui

Peking University

bin.cui@pku.edu.cn

###### Abstract

With the remarkable achievements of large language models (LLMs), the demand for fine-tuning and deploying LLMs in various downstream tasks has garnered widespread interest. Parameter-efficient fine-tuning techniques represented by LoRA and model quantization techniques represented by GPTQ and AWQ are of paramount significance. However, although these techniques have been widely adopted in single-task scenarios, research is scarce in multi-task scenarios. To be specific, we find that mainstream quantization methods would prevent the base LLM from being shared among tasks, so current LLM serving systems are infeasible to integrate LLM quantization with multiple LoRA adapters to achieve memory-efficient multi-task serving. Moreover, existing LLM serving systems lack support for dynamic task addition and overlook the workload differences among tasks, leading to inefficiencies in multi-task scenarios.

This work proposes _LoRA-Inlaid_, an efficient multi-task LLM serving system. On the one hand, _LoRA-Inlaid_ designs a flexible and efficient multi-task quantization algorithm (MLGPTQ) that facilitates the sharing of a single quantized model for multiple LoRA adapters, which significantly reduces the memory consumption for model deployment. Meanwhile, it supports adding LoRA adapters for new tasks on the fly, without sacrificing the stability of online services. On the other hand, _LoRA-Inlaid_ develops a novel multi-task scheduling algorithm guided by output length prediction and grouping among different tasks, which effectively shrinks the memory consumption and avoids frequent switching of LoRA adapters. Empirical results verify that _LoRA-Inlaid_ outperforms existing state-of-the-art LLM serving systems by up to 1.58\(\times\) in terms of throughput, 1.76\(\times\) in terms of average latency, 2\(\times\) in terms of job completion time, and 10\(\times\) in terms of SLO Attainment, while maintaining the same level of model quality.

## 1 Introduction

Large language models (LLMs) have demonstrated impressive effectiveness in various domains [26, 30, 31, 39], and the demand of deploying LLMs in downstream tasks continues to grow [4, 10,20, 24, 34, 43, 44, 46, 47]. Given the explosive increase in model size and the limitations of hardware resources, "parameter-efficient fine-tuning" (PEFT) and "quantization-then-deployment" have become the most common pathways for deploying LLMs in downstream tasks [50]. On the one hand, PEFT techniques, represented by LoRA (Low-Rank Adaptation) [16], only train small-scale adapters to adapt the base model to a specific task, significantly reducing the cost of model fine-tuning. On the other hand, low-bit quantization techniques like GPTQ and AWQ [12, 22] can substantially reduce the memory requirements of model deployment and alleviate memory access overhead during inference, while maintaining model quality.

Although mainstream LLM serving systems like vLLM and TensorRT-LLM [19, 29] have integrated support for the quantized deployment of fine-tuned models, these systems focus on single-task serving scenarios. With the rising demand for various downstream tasks, efficiently supporting multi-task servicing scenarios has become increasingly crucial. This has led to the emergence of multi-task serving systems supporting multiple LoRA adapters concurrently, such as S-LoRA and Punicia [5, 35]. These systems share a unified base model across different tasks and activate different LoRA adapters based on the incoming requests, enabling the simultaneous processing of multiple tasks in a single batch. However, in multi-task scenarios, existing systems still face three major challenges.

First, existing multi-task serving systems cannot effectively incorporate mainstream model quantization methods such as GPTQ and AWQ. Specifically, these quantization methods require calibration of numerical distributions using task-specific datasets, and the quantization process for each task necessitates activating the corresponding LoRA adapter. Consequently, the base models after quantization are divergent across different tasks, and thus it is infeasible to share a unified quantized model. This limitation leads to performance deficiencies or even unavailability in resource-constrained scenarios.

Second, in practical multi-task serving scenarios, it would be necessary to add new tasks in real time. However, existing systems only support a static number of tasks and are incapable of dynamically adding LoRA adapters. More importantly, after a quantized model is deployed, current solutions do not support any subsequent quantization and deployment for new tasks without affecting the existing tasks. In contrast, adding new tasks typically requires suspending and restarting the serving process, which severely harms the stability and robustness of online services.

Third, incoming requests for different tasks inevitably have workload variations (such as request length, processing time, etc.) and require loading different LoRA adapters for processing. Existing systems overlook these issues during the scheduling for multi-task requests, and thus necessitate loading a large number of adapters in a single scheduling step as well as frequently switching adapters between adjacent scheduling steps, leading to significant efficiency degradation.

To address these challenges, we develop _LoRA-Inlaid_, a resource-efficient and high-performance system for multi-task LLM serving. The main contributions of this paper are as follows.

To begin with, we propose an innovative multi-task quantization algorithm termed MLGPTQ (**M**ulti-**LoRA **GPTQ**), which utilizes multi-task data to perform joint quantization on the base model. This allows the quantized base model to be shared across multiple tasks. In addition, it supports incremental quantization for newly added tasks without impacting the performance of online services.

Subsequently, we introduce a novel multi-task scheduling strategy based on output length prediction and grouping. This effectively reduces memory consumption and memory swapping overhead in multi-task scenarios, significantly enhancing overall system performance.

Based on these two techniques, we develop a brand new multi-task LLM serving system, namely _LoRA-Inlaid_. As shown in Table 1, _LoRA-Inlaid_ integrates multi-task quantization, enables dynamic task addition, and employs the multi-task scheduling strategy, achieving high-performance and flexible multi-task LLM serving in resource-constrained environments.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline System & Multi-task & Multi-task & Dynamic Task & Multi-task \\  & Serving & Quantization & Addition & Scheduling \\ \hline vLLM [19] \& TensorRT-LLM [29] & ✗ & ✗ & ✗ & ✗ \\ S-LoRA [35] \& Punicia [5] & ✓ & ✗ & ✗ & ✗ \\ _LoRA-Inlaid_ (this work) & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of supported features of different LLM serving systemsFinally, experimental results demonstrate that, compared to existing systems, _LoRA-Inlaid_ can increase throughput by up to 1.58\(\times\), reduce average latency and job completion time by up to 1.76\(\times\) and 2\(\times\), improve SLO Attainment by up to 10\(\times\), and support larger-scale language models under the same resource constraints, all while maintaining nearly the same level of model quality.

## 2 Background and Related Works

Low-Rank Adaptation.LoRA [16], short for Low-Rank Adaptation, is one of the most widely used parameter-efficient fine-tuning (PEFT) techniques. Unlike full-parameter fine-tuning, LoRA fine-tunes only a small adapter, which consists of much fewer parameters than the base model, significantly reducing the training cost. The key idea behind LoRA is that the fine-tuning process should only introduce small changes to the weight matrix of the base model (denoted by \(\mathbf{W}\in\mathbb{R}^{m\times n}\)), so we can learn two small, low-rank matrices (denoted by \(\mathbf{A}\in\mathbb{R}^{r\times n},\mathbf{B}\in\mathbb{R}^{m\times r}\) where \(r\ll m,n\)), and approximate such changes with the product of two matrices (i.e., \(\Delta\mathbf{W}\approx\mathbf{BA}\)).

Low-bit Quantization.Low-bit quantization [7; 8; 12; 22; 23; 42] shrinks the model size effectively and thus reduces the memory requirement when deploying the model. In addition, it usually helps to improve efficiency by decreasing the memory access overhead of the model weights. Consequently, it has been widely adopted in LLM serving. There are various quantization paradigms, with post-training quantization (PTQ) being among the most popular ones. Typically, PTQ computes \(X_{\text{INT}}=\texttt{Round}(\alpha\;\texttt{Clip}(X_{R}/\alpha,Q_{\text{ min}},Q_{\text{max}}))\), where \(X_{R}\) represents the real-valued parameters before quantization, \(X_{\text{INT}}\) represents the parameters after quantization to integers, \(Q_{min}\) and \(Q_{max}\) denote the minimum and maximum values of the quantization range, and \(\alpha\) represents the scaling factor. Various PTQ methods calculate the quantization knobs like \(\alpha\) with diverse approaches or implement different approximation methods. While mainstream PTQ methods (e.g., GPTQ [12], AWQ [22]) have a common ground that they need to calibrate the numerical distribution based on a small task-specific dataset (a.k.a. _the calibration set_), since numerous studies have revealed the accuracy after quantization with dataset calibration is usually significantly higher than that without dataset calibration [17]. Therefore, this paper focuses on quantization with dataset calibration.

Scheduling in LLM Serving.With the explosive applications of LLMs, more and more studies try to evolve the scheduling strategies in LLM serving for better performance. Early systems like FasterTransformer [28] rely on request-level scheduling. Notably, Yu et al. [45] introduced Orca, the first iteration-level scheduling with first-come-first-serve (FCFS) order for better batching. Building on this, mainstream LLM serving systems leverage various batching approaches, such as continuous batching in vLLM [19] and in-flight batching in TensorRT-LLM [29]. FastServe [40] takes the semi-information of requests (e.g., input length, processed time, etc.) into account and tries to minimize average job completion time. However, none of these scheduling strategies consider the characteristics of multi-task scenarios, as we will discuss in SS3.3.

Multi-task Serving Systems.Since the LoRA fine-tuning technique keeps the base model unaltered, it is feasible to share the same base model across multiple LoRA adapters, so that we can serve requests from multiple tasks within a single batch. Punica [5] and S-LoRA [35] are two notable multi-task serving systems, putting forward the initial efforts to support multi-task LLM serving with multiple LoRA adapters. Specific optimization techniques are proposed. For instance, the Segmented Gather Matrix-Vector (SGMV) kernel is developed to enhance memory and computation efficiency when processing requests from different tasks together. In addition, to allocate more GPU memory to intermediate results (typically, KV cache), existing systems maintain the LoRA adapters in CPU memory and only preserve a relatively small number of LoRA adapters in GPU memory. When a LoRA adapter outside GPU memory is needed, it is necessary to perform memory swapping between the CPU and GPU memory.

## 3 _LoRA-Inlaid_

The overview of _LoRA-Inlaid_ is depicted in Figure 1. Given an LLM with multiple LoRA adapters for various downstream tasks, _LoRA-Inlaid_ initiates a joint quantization process (SS3.1), which produces a unified quantized base model that can be shared across the adapters. During the online serving, if new tasks are to be included on the fly, _LoRA-Inlaid_ facilitates a dynamic task addition process (SS3.2) that efficiently conducts incremental re-quantization and seamlessly deploys the added tasks.

Furthermore, _LoRA-Inlaid_ employs a multi-task scheduling strategy (SS3.3) that takes the workload differences into account for better efficiency.

### Multi-task Joint Quantization

As introduced in SS2, mainstream quantization methods require task-specific datasets for calibration. In addition, they mostly follow the _Forward-Aggregate Info-Modify Weight-Quant_ paradigm in Figure 2. This paradigm first simulates the activation distribution for a given task through _Forward_ propagation and _Aggregates Information_ of this specific task. Subsequently, it uses the aggregated information to _Modify_ model _Weights_ to adapt to the task. Finally, the quantization knobs like scales \(\alpha\) are calculated based on the modified weights to _Quan_rize the base model.

However, in multi-task scenarios, since different tasks should provide diverse calibration sets and necessitate unique LoRA adapters for computation, the quantized models of different tasks are inevitably divergent. Intuitively, if we wish to tweak existing quantization methods to make the quantized model shareable across tasks, we should quantize the model without any LoRA adapters. In addition, we should either (i) quantize the model without calibration or (ii) quantize the model with a mixed calibration set consisting of the datasets from all tasks.

However, these approaches fail to accurately capture the unique numerical distribution of each task, and suffer from severe accuracy loss (as evaluated in SS4.2). Below we first elaborate on the reason why these approaches fail with the widely used GPTQ [12] and then propose our solution1.

Footnote 1: Note that the idea of our solution can also be applied to other quantization algorithms which follow the same paradigm (like AWQ) since the drawbacks exist generally. Considering that the choice of backbone algorithm is orthogonal to our goal, we focus on GPTQ in this work due to its widespread adaption, and present how to adapt our solution to AWQ in Appendix D.

**Drawbacks of GPTQ in multi-task scenarios.** Directly applying GPTQ in multi-task scenarios has the following drawbacks. First, as aforementioned, GPTQ can only quantize the model without any LoRA adapters, which is infeasible to accurately capture the correct activation information for multiple tasks during _Forward_. Second, in _Aggregate Info_, since the calibration sets from all tasks are mixed, GPTQ simply accumulates the information from different tasks into one Hessian matrix, making each task's specific information diluted and losing the emphasis on critical information from different tasks. Third, in _Modify Weight_, GPTQ relies on the naive, mix-aggregated Hessian matrix, overlooking the varying importance across tasks, which results in suboptimal outcomes. These drawbacks make the direct application of GPTQ in multi-task scenarios ineffective.

**Our MLGPTQ (Multi-LoRA GPTQ) Algorithm** To address these drawbacks, we propose a multi-task quantization algorithm termed MLGPTQ. Our algorithm enables joint quantization of multiple tasks to retain only one quantized base model, while effectively maintaining the model accuracy by capturing the numerical distributions of all tasks. The goal of MLGPTQ is to minimize the errors of activations before and after quantization, i.e.,

\[\operatorname*{arg\,min}_{Q(\mathbf{W})}||\sum_{t=1}^{T}((\mathbf{W}+\mathbf{ B}_{t}\mathbf{A}_{t})\mathbf{X}_{t}-(Q(\mathbf{W})+\mathbf{B}_{t}\mathbf{A}_{t}) \mathbf{X}_{t})||_{2}^{2},\] (1)

Figure 1: Design overview of _LoRA-Inlaid_. The workflow is labeled with numbers in the diagram. 1 _Quaníze and Deploy_ indicates the initiation of the server performing the multi-task quantization and deploying the quantized model and LoRA online. 2 _Schedule_ involves utilizing a multi-task scheduling strategy for 3 _Inference_. If a new task is detected, it invokes 4 _Add Task_ to dynamically add the new task without interruping the ongoing services.

where \(T\) denotes the number of tasks, \(\mathbf{A}_{t}\) and \(\mathbf{B}_{t}\) are the low-rank adapter matrices of the \(t\)-th task, \(\mathbf{X}_{t}\) is the input of \(t\)-th task, and \(\mathbf{W}\) and \(Q(\mathbf{W})\) denote the original and quantized weights of a layer.

As shown in Figure 2, During _Forward_, MLGPTQ loads the corresponding LoRA adapters based on each task, accurately computing the activations. In _Aggregate Info_, unlike GPTQ's naive mix-aggregation that disrupts task-specific information, MLGPTQ derives the max-aggregation to solve the objective in Eq. 1 (the derivation can be found in the Appendix A), which has the following form:

\[\nabla\mathbf{W}=-\frac{w_{q}-Q(w_{q})}{(\mathbf{H}_{t}^{-1})_{qq}}\mathbf{H} _{t}^{-1}e_{q},\text{ where }t^{*}=\operatorname*{arg\,max}_{t\in\{1,2,\cdots,T\}}( \mathbf{H}_{t}^{-1})_{qq},\] (2)

where \(\mathbf{H}_{t}\) denotes the Hessian matrix of the \(t\)-th task,\(w_{q}\) is the \(q\)-th parameter in \(\mathbf{W}\). To be formal, there are primarily two steps in _Aggregate Info_. First, it calculates the Hessian matrix information for each task individually (i.e., compute \(\{\mathbf{H}_{t}^{-1}\}_{t=1}^{T}\)) Second, it aggregates the most important information from each one into a max-aggregated Hessian matrix (i.e., \(\mathbf{H}_{tmp}=\texttt{MaxAgg}(\{\mathbf{H}_{t}^{-1}\}_{t=1}^{T})\)). In _Modify Weight_, MLGPTQ utilizes the max-aggregated Hessian matrix to adjust the weights according to Eq. 2. Finally in _Quant_, we utilize the modified weights for quantization. Due to space constraints, we only present the core concept of MLGPTQ here. Interested readers are referred to Appendix A for a complete derivation as well as the detailed algorithm.

### Dynamic Task Addition

In real-world online services, there is a need for dynamic task addition (i.e., adding new LoRA adapters). In single-task scenarios, adding new tasks typically requires launching more services with extra hardware resources, which does not affect the services for existing tasks. In multi-task scenarios, there would be interference since all tasks share the same base model. However, we find that none of the existing multi-task serving systems address this problem, lacking a proper solution.

Nevertheless, adding new LoRA adapters on the fly in _LoRA-Inlaid_ is inherently far from trivial since the multi-task quantization poses two challenges: _(1. Unseen Distributions)_ Since the MLGPTQ algorithm is invoked before the new tasks are involved, the quantized model has not captured the distribution information about the new tasks, making it infeasible to work with the new LoRA adapters directly. _(2. Serving Interruption)_ Directly re-quantizing the model requires a substantial amount of memory, so it necessitates pausing the ongoing serving for a while to reserve available space for re-quantization, harming the stability of online services. To support dynamic task addition in multi-task scenarios, _LoRA-Inlaid_ tackles these two obstacles, respectively.

To capture the information of new tasks, a naive solution is to perform full quantization once there are new tasks. Denote \(T_{1},T_{2}\) as numbers of existing and new tasks, respectively. The naive solution runs the two steps of _Aggregate Info_ above with \(T=T_{1}+T_{2}\). However, this leads to redundant computation of \(\{\mathbf{H}_{t}^{-1}\}_{t=1}^{T}\). In addition, given the commutative property of the \(\texttt{max-aggregation}\) operation, we have \(\texttt{MaxAgg}(\{\mathbf{H}_{t}^{-1}\}_{t=1}^{T})=\texttt{MaxAgg}(\{ \mathbf{MaxAgg}(\{\mathbf{H}_{t}^{-1}\}_{t=1}^{T})_{t=1}^{T_{2}}\},\texttt{ MaxAgg}(\{\mathbf{H}_{t}^{-1}\}_{t=T_{1}+1}^{T_{2}})\), where the first term \(\texttt{MaxAgg}(\{\mathbf{H}_{t}^{-1}\}_{t=1}^{T_{1}})\) has already been computed as \(\mathbf{H}_{tmp}\) in the previous quantization. Inspired by this, _LoRA-Inlaid_ caches \(\mathbf{H}_{tmp}\) so that the incremental quantization can be done as follows. In _Forward_, it capture the activation information of new task \(T_{1}+1,\cdots,T_{2}\). In _Aggregate

Figure 2: Process of MLGPTQ vs GPTQ. Both MLGPTQ and GPTQ follow the _Forward-Aggregate Info-Modify Weight-Quant_ paradigm. MLGPTQ primarily improves the first three steps, aiming to better gather and highlight critical information for all tasks.

_Info_, it computes the Hessian matrices for new tasks \(\{\mathbf{H}_{t}^{-1}\}_{t=T_{1}+1}^{T_{2}}\), and then max-aggregates the \(T_{2}+1\) matrices (i.e., \(\{\mathbf{H}_{t}^{-1}\}_{t=T_{1}+1}^{T_{2}}\) and the cached \(\mathbf{H}_{tmp}^{( cached)}\)). At last, it performs _Modify Weight_ and _Quant_, as introduced in SS3.1. By doing so, incremental quantization with \(T_{2}\) tasks is identical to full quantization with \(T_{1}+T_{2}\) tasks, while avoiding redundant computation.

To avoid halting the ongoing services, _LoRA-Inlaid_ spawns a background thread for incremental quantization. Moreover, it is done in a layer-by-layer manner to reduce the memory consumption -- for each (unquantized) model weight, we load it from CPU memory to GPU memory, perform incremental quantization, remove it from GPU memory, and proceed to the next model weight. The IO between CPU-GPU is overlapped with computation. Thus, _LoRA-Inlaid_ supports seamless task addition on the fly and has very little influence on the ongoing services, as evaluated in SS4.4.

Putting them together, _LoRA-Inlaid_ develops an asynchronous, layer-wise re-quantization mechanism, which accomplishes incremental quantization with the new tasks and cached Hessian matrices asynchronously, without interrupting the serving.

### Multi-task Scheduling

Despite extensive research on scheduling strategies for LLM serving, these approaches primarily focus on single-task serving, leaving the unique characteristics in the multi-task scenarios neglected. Below we analyze two limitations of existing scheduling strategies in multi-task serving. Besides, due to the space constraint, we briefly introduce the corresponding solutions in _LoRA-Inlaid_, while leaving the details of our multi-task scheduling algorithm in Appendix B.

_Limitation 1: Divergent Output Length Distributions Leading to High Average Completion Time._ As shown in Figure 4, the distributions of input and output lengths vary significantly across different tasks, while requests of the same task exhibit clustering effects. Current strategies mainly rely on semi-information (e.g., input length, processed time, etc.) to make the scheduling decisions, but do not consider the information of output length since it is not the prior knowledge. Intuitively, this may work fine for single-task scenarios where the vast majority of requests fall within the same workload and thus the clustering effect exists. However, it is unsuitable for multi-task scenarios due to the divergent output length distributions across different tasks. Eventually, we find that existing scheduling strategies suffer from heavy performance degradation when applied to multi-task serving.

_Solution 1: Scheduling Guided by Output Length Prediction._ Existing research has shown that the output lengths can be accurately predicted by a small, distilled model given the requests [49]. Inspired by this, we leverage a number of small models, to predict the output lengths of incoming requests. Particularly, upon receiving a new request, we predict its output length on CPU using a small model (255MB). Note that the output length prediction takes about 16 milliseconds for one request on CPU, while it takes about 200 milliseconds or more to finish the inference of one request on GPU. Hence, we can completely overlap the prediction, without occupying any GPU computing resources. Based on the predictions, we employ a Shortest Remaining Time First (SRTF) scheduling, which prioritizes requests with the shortest remaining processing time and has been proven to minimize the average completion time in the field of job scheduling [37].

_Limitation 2: Excessive Tasks Involved in each Step Leading to Expensive Memory Access Overhead._ Due to the randomness and dynamicity of request arrivals, multiple tasks are to be scheduled in each step. However, owing to the lack of consideration upon the task for each request, existing scheduling

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

of tasks, and it encounters out-of-memory (OOM) errors in several cases. In contrast, _LoRA-Inlaid_ supports all cases well. More importantly, since _LoRA-Inlaid_ is able to reserve more memory for intermediate results (e.g., KV cache) in serving, it achieves higher performance than the baselines. For instance, _LoRA-Inlaid_ surpasses S-LoRA by 26.5%, 31.3%, 24.1% on average, and up to 58.1%, 76.3%, and 99.9%, in terms of throughput, latency, and JCT, respectively.

**SLO Attainment.** We also assess the SLO Attainment under different serving loads by varying the request rates and maximum request lengths. The results are shown in Figure 7. In short, compared to S-LoRA and vLLM, _LoRA-Inlaid_ improves the SLO Attainment by 3.9\(\times\), 8.5\(\times\) on average, and up to 10\(\times\), 38\(\times\), respectively. Furthermore, we observe that as the request rate or maximum sequence length increases, S-LoRA and vLLM experience a steep decline in SLO Attainment while _LoRA-Inlaid_ does not. This demonstrates the excellent adaptability of _LoRA-Inlaid_ to various serving loads.

### More Experiments

**Scalability.** We investigate the scalability w.r.t. number of tasks. As shown in Table 3, vLLM suffers from significant performance decline, dropping by 56%-73% when the number of tasks increases from 2 to 4, and eventually encountering out-of-memory (OOM) errors when the number of tasks reaches 5. In contrast, under all experimented request rates, the throughput of _LoRA-Inlaid_ hardly declines, even with 1000 tasks served simultaneously. S-LoRA also supports a large number of tasks, while _LoRA-Inlaid_ consistently achieves better performance across all kinds of workloads.

**Ablation Studies of Multi-task Scheduling and Multi-task Quantization.** We compare different scheduling strategies on _LoRA-Inlaid_. The results are shown in the left of Figure 8. "Ours (w/o group)", "Ours (w/o prediction)" and "Ours (w/o SRTF)" represent three variants of our multi-task scheduling strategy without task grouping, without output length prediction and without the prediction-based SRTF, respectively. "FIFO" is the strategy adopted in S-LoRA and vLLM, and "Skip-join

Figure 6: System performance in terms of throughput (higher is better), latency (lower is better), and JCT (lower is better) under various request rates (\(x\)-axis) and numbers of tasks (\(T\)).

Figure 7: SLO Attainment (higher is better) under various serving loads (RTX 4090).

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|} \hline
**Task num** & \multicolumn{2}{c|}{**2**} & \multicolumn{2}{c|}{**3**} & \multicolumn{2}{c|}{**4**} & \multicolumn{2}{c|}{**5**} & \multicolumn{2}{c|}{—} & \multicolumn{2}{c|}{**100**} & \multicolumn{2}{c|}{**1000**} \\ \hline
**Rags rate** & **5** & **10** & **20** & **5** & **10** & **20** & **5** & **10** & **20** & **5** & **10** & **20** & **–** & **5** & **10** & **20** & **5** & **10** & **20** \\ \hline _LoRA-Inlaid_ & 3.89 & 4.70 & 4.86 & 3.78 & 4.66 & 4.81 & 3.82 & 4.77 & 4.89 & 3.71 & 4.61 & 4.73 & — & 3.60 & 4.25 & 4.58 & 3.42 & 4.02 & 4.22 \\ \hline S-LoRA & 2.93 & 3.45 & 3.51 & 2.97 & 3.86 & 3.54 & 2.91 & 3.36 & 3.58 & 2.97 & 3.40 & 3.55 & -2.87 & 3.35 & 3.36 & 2.78 & 3.26 & 3.28 \\ \hline vLLM & 1.77 & 2.46 & 2.98 & 1.02 & 1.68 & 2.27 & 0.77 & 0.76 & 0.80 & OOM & OOM & OOM & — & OOM & OOM & OOM & OOM & OOM \\ \hline \end{tabular}
\end{table}
Table 3: Scalability comparison in terms of throughput (reqs/s, higher is better) under different request rates and number of LoRA adapters (LLAMA2-7B@RTX 4090).

MLFQ" represents the strategy in FastServe [40]. It is evident that our multi-task scheduling strategy achieves the best performance in terms of SLO Attainment. The designs of task grouping, output length prediction, and SRTF increase the SLO Attainment by 1.16\(\times\), 1.23\(\times\) and 2.27\(\times\) on average, respectively. We also explore the individual impact of multi-task quantization as shown in the right of Figure 8. Specificically, we consider a variant of _LoRA-Inlaid_, which disables quantization (i.e., the served model is not quantized), denoted as "Ours (w/o quant)". The results show that multi-task quantization brings 39% improvement ("Ours" vs. "Ours (w/o quant)") when serving the 7B model. Additionally, without quantization, it will lead to OOM when serving the 13B model.

**Dynamic Task Addition.** We evaluate the ability of dynamic task addition in _LoRA-Inlaid_ by adding 1, 5, and 10 tasks to a heavily loaded service on the fly. The results in Figure 9 show that the throughput undergoes 10%-13% of degradation during the task addition, regardless of the number of tasks added. This is worthwhile given that the online service need not be interrupted. Meanwhile, to evaluate the time consumption of dynamic task addition, we conducted an experiment where there are 5 tasks in the ongoing service and another 5 tasks need to be added. We measured the time cost of three approaches: "Full Quant", which halts the serving and performs full quantization with 10 tasks, "Incr Quant offline", an offline variant (which halts the serving) of our incremental quantization on the 5 new tasks without layer-by-layer quantization, and "Incr Quant", our incremental quantization with the 5 new tasks, which works concurrently with the ongoing service. As shown in Table 4, by avoiding the redundant computation, the time cost of forward process and calculation of Hess matrix can be reduced greatly, accelerating quantization. Moreover, although the layer-by-layer mechanism slows down the quantization by 1.26 \(\times\) due to the extra IO, it reduces the memory greatly and does not halt the serving. These empirical results validate the flexibility and robustness of _LoRA-Inlaid_ for multi-task serving.

## 5 Conclusion and Limitations

In this work, we focused on LLM serving in multi-task scenarios and developed a multi-LoRA task serving system, namely _LoRA-Inlaid_. On one hand, we designed a flexible and efficient dynamic multi-task quantization algorithm that supports the joint quantization of models for multiple tasks, significantly reducing the memory requirements for model deployment. We also facilitated real-time dynamic task addition, enhancing the stability and flexibility of online services. On the other hand, we introduced a novel multi-task scheduling strategy based on output length prediction and grouping, effectively resolving the issues of high memory overhead and frequent memory swapping when applying existing strategies in multi-task scenarios. Extensive experiments demonstrated that _LoRA-Inlaid_ significantly outperforms existing LLM serving systems.

Despite the effectiveness of _LoRA-Inlaid_, it still has several limitations. First, our quantization does not detect the existence of malicious or poisoning tasks, which might be intentionally crafted to harm the other tasks. Second, our scheduling does not consider the fairness among tasks (e.g., balancing the total numbers of output tokens for all tasks), which may be essential for shared service platforms. Third, it only supports language tasks while requiring some system re-designs for multi-modal tasks. We wish to leave the exploration of these issues as future works.

## Acknowledgments and Disclosure of Funding

This work is supported by National Natural Science Foundation of China (U22B2037, U23B2048), China National Postdoctoral Program for Innovative Talents (BX20230012), China Postdoctoral Science Foundation (2024M750103), Beijing Natural Science Foundation (4244080), research grant No. SH-2024JK29, the Fund of Kunpeng and Ascend Center of Excellence (Peking University), and High-performance Computing Platform of Peking University. Fangcheng Fu and Bin Cui are the corresponding authors.

## References

* [1] malicious-600k. https://huggingface.co/datasets/bggapaditya/malicious-600k, 2023.
* [2] Medical_mmlu. https://huggingface.co/datasets/medalpaca/medical_meadow_mmmlu, 2023.
* [3] Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In _Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization_, 2005.
* [4] Hong Chen, Kang Yuan, Yanjun Huang, Lulu Guo, Yulei Wang, and Jie Chen. Feedback is all you need: from chatgpt to autonomous driving. _Sci. China Inf. Sci._, 66(6), 2023.
* [5] Lequun Chen, Zihao Ye, Yongji Wu, Danyang Zhuo, Luis Ceze, and Arvind Krishnamurthy. Punica: Multi-tenant lora serving. _CoRR_, abs/2310.18547, 2023.
* [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _CoRR_, abs/2110.14168, 2021.
* [7] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3.int8(): 8-bit matrix multiplication for transformers at scale. _Advances in Neural Information Processing Systems (NeurIPS 2022)_, 2022.
* [8] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _Advances in Neural Information Processing Systems (NeurIPS 2023)_, 2023.
* [9] George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In _Proceedings of the Second International Conference on Human Language Technology Research (HLT 2002)_, 2002.
* [10] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. _arXiv preprint arXiv:1808.09381_, 2018.
* [11] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning. _Advances in Neural Information Processing Systems (NeurIPS 2022)_, 2022.
* [12] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: accurate post-training quantization for generative pre-trained transformers. _CoRR_, abs/2210.17323, 2022.
* [13] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.
* [14] Tahmid Hasan, Abhik Bhattacharjee, Md Saiful Rahman, Shafiq Joty Zadeh, Eduard Hovy, Naeemul Hoque, and Hamdy Mubarak. Xlsum: A large-scale multilingual abstractive summarization dataset. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP 2021)_, 2021.

* [15] Cunchen Hu, Heyang Huang, Liangliang Xu, Xusheng Chen, Jiang Xu, Shuang Chen, Hao Feng, Chenxi Wang, Sa Wang, Yungang Bao, Ninghui Sun, and Yizhou Shan. Inference without interference: Disaggregate LLM inference for mixed downstream workloads. _CoRR_, abs/2401.11181, 2024.
* [16] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In _International Conference on Learning Representations 2022 (ICLR 2022)_, 2022.
* [17] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. Accurate post training quantization with small calibration sets. In _International Conference on Machine Learning (ICML 2021)_, 2021.
* [18] William JF Keenan. Sacre bleu: faith, fashion and freedom: Marist foundation garments 1817-1862. In _Materializing Religion_, pages 132-153. Routledge, 2017.
* [19] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the 29th Symposium on Operating Systems Principles (SOSP 2023)_, pages 611-626, 2023.
* [20] Zhi Lei, Guixian Zhang, Lijuan Wu, Kui Zhang, and Rongjiao Liang. A multi-level mesh mutual attention model for visual question answering. _Data Sci. Eng._, 7(4):339-353, 2022.
* [21] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In _Text summarization branches out: Proceedings of the ACL-04 workshop_, pages 74-81, 2004.
* [22] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. AWQ: activation-aware weight quantization for LLM compression and acceleration. _CoRR_, abs/2306.00978, 2023.
* [23] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. _arXiv preprint arXiv:2310.16836_, 2023.
* [24] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. _arXiv preprint arXiv:1908.08345_, 2019.
* [25] Meta. Llama 2: Advancing ai through collaboration. https://about.fb.com/news/2023/07/llama-2/, July 2023. Accessed: 2024-04-21.
* [26] Meta. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta.com/blog/meta-llama-3/, 2024.
* [27] Xupeng Miao, Xiaonan Nie, Hailin Zhang, Tong Zhao, and Bin Cui. Hetu: a highly efficient automatic parallel distributed deep learning system. _Sci. China Inf. Sci._, 66(1), 2023.
* [28] NVIDIA. Fastertransformer: A fast and memory-efficient library for transformer models. https://github.com/NVIDIA/FasterTransformer, 2023. Accessed: 2024-05-17.
* [29] NVIDIA. Tensorrt-llm. https://github.com/NVIDIA/TensorRT-LLM, 2023.
* [30] OpenAI. ChatGPT: Optimizing Language Models for Dialogue, 2022. https://openai.com/blog/chatgpt.
* [31] OpenAI. GPT-4 technical report. _CoRR_, abs/2303.08774, 2023.
* [32] Abhay K. Parekh and Robert G. Gallager. A generalized processor sharing approach to flow control in integrated services networks: the single-node case. _IEEE/ACM Trans. Netw._, 1(3):344-357, 1993.
* [33] Muhammad Raza. Service level objectives (slos) explained, 2018. Accessed: 2024-05-17.
* [34] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. _arXiv preprint arXiv:2004.13637_, 2020.

* [35] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, Joseph E. Gonzalez, and Ion Stoica. S-lora: Serving thousands of concurrent lora adapters. _CoRR_, abs/2311.03285, 2023.
* [36] Xian-He Sun and Xiaoyang Lu. The memory-bounded speedup model and its impacts in computing. _J. Comput. Sci. Technol._, 38(1):64-79, 2023.
* [37] Andrew S. Tanenbaum and Herbert Bos. _Modern Operating Systems_. Pearson Education, 4 edition, 2014. Includes comprehensive coverage of various scheduling algorithms, including Shortest Remaining Time First (SRTF).
* [38] Jorg Tiedemann. Parallel data, tools and interfaces in opus. In _Lrec_, volume 2012, pages 2214-2218. Citeseer, 2012.
* [39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Annual Conference on Neural Information Processing Systems 2017 (NeurIPS 2017)_, 2017.
* [40] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu, and Xin Jin. Fast distributed inference serving for large language models. _arXiv preprint arXiv:2305.05920_, 2023.
* [41] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation, 2016.
* [42] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In _International Conference on Machine Learning (ICML 2023)_, 2023.
* [43] Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang. Improving BERT fine-tuning via self-ensemble and self-distillation. _J. Comput. Sci. Technol._, 38(4):853-866, 2023.
* [44] Yuemei Xu, Han Cao, Wanze Du, and Wenqing Wang. A survey of cross-lingual sentiment analysis: Methodologies, models and evaluations. _Data Sci. Eng._, 7(3):279-299, 2022.
* [45] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. Orca: A distributed serving system for transformer-based generative models. In _16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 521-538, 2022.
* [46] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In _International conference on machine learning (ICML 2020)_, 2020.
* [47] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. _arXiv preprint arXiv:1911.00536_, 2019.
* [48] Yilun Zhao, Zhenting Qi, Linyong Nan, Boyu Mi, Yixin Liu, Weijin Zou, Simeng Han, Xiangru Tang, Yumo Xu, Arman Cohan, et al. Qtsumm: A new benchmark for query-focused table summarization. _arXiv preprint arXiv:2305.14303_, 2023.
* [49] Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, and Yang You. Response length perception and sequence scheduling: An llm-empowered llm inference pipeline. _Advances in Neural Information Processing Systems (NeurIPS 2023)_, 2023.
* [50] Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. Pets: A unified framework for parameter-efficient transformers serving. In _2022 USENIX Annual Technical Conference (USENIX ATC 22)_, pages 489-504, 2022.

Details of MLGPTQ

### Derivation of MLGPTQ

MLGPTQ is based on GPTQ, adapting the corresponding LoRA adapter according to the activation values of the input data to minimize the error in activation values before and after quantization. For only one task, GPTQ aims to solve the following problem:

\[\operatorname*{arg\,min}_{Q(\mathbf{W}_{l})}||\mathbf{W}_{l}\mathbf{X}_{l}-Q( \mathbf{W}_{l})\mathbf{X}_{l}||_{2}^{2},\] (3)

where \(\mathbf{W}_{l}\) represents the weight of \(l\)-th layer in the base model, \(Q(\mathbf{W}_{l})\) represents the quantized version of \(\mathbf{W}_{l}\), \(\mathbf{X}_{l}\) is the input of \(l\)-th layer. In other words, the objective of GPTQ is to find a \(Q(\mathbf{W}_{l})\) for each layer's weight \(\mathbf{W}_{l}\) through layer-wise quantization, in order to minimize the changes in activation values. Since we are discussing quantization within a single layer, we will omit the subscript \(l\) for simplicity in the rest of this section.

As proved by [11], solving Eq. 3 can be transformed into solving Eq. 4 as follows.

\[\operatorname*{arg\,min}_{Q(\mathbf{W})}E\coloneqq\sum_{i=1}^{d_{\text{max}}} \left\|\mathbf{W}_{i,:}\mathbf{X}-Q(\mathbf{W})_{i,:}\mathbf{X}\right\|_{2}^{2}.\] (4)

Since the model has converged through training before quantization, existing works generally assume the model has reached a local minimum. Thus, when we add a small adjustment \(\nabla\mathbf{W}\) to the parameter \(\mathbf{W}\), according to Taylor expansion, we have

\[\nabla E=\left(\frac{\partial E}{\partial\mathbf{W}}\right)^{T}\nabla\mathbf{ W}+\frac{1}{2}\nabla\mathbf{W}^{T}\cdot\mathbf{H}\cdot\nabla\mathbf{W}+O( \left\|\nabla\mathbf{W}\right\|^{3}),\] (5)

where \(\mathbf{H}=\partial^{2}E/\partial\mathbf{W}^{2}\) represents the Hessian matrix. Again, since the model has converged, existing works generally assume its first-order partial derivative is close to zero and thus negligible. By neglecting the first-order partial derivative and higher-order terms, we have

\[\nabla E=\frac{1}{2}\nabla\mathbf{W}^{T}\cdot\mathbf{H}\cdot\nabla\mathbf{W}.\] (6)

Recall that our goal is to quantize \(\mathbf{W}\) to \(Q(\mathbf{W})\). Denote \(\nabla w_{q}=Q(w_{q})-w_{q}\), where \(w_{q}\) represents the \(q\)-th element of \(\mathbf{W}\). Then, the problem to solve can be re-written as

\[\operatorname*{arg\,min}_{q}\left\{\operatorname*{arg\,min}_{\nabla\mathbf{W}} \left(\frac{1}{2}\nabla\mathbf{W}^{T}\cdot\mathbf{H}\cdot\nabla\mathbf{W} \right)\left[e_{q}^{T}\nabla\mathbf{W}+w_{q}=Q(w_{q})\right]\right\},\] (7)

where \(e_{q}\) represents a unit vector with a value of 1 at position \(q\) and 0 elsewhere. Since this is a constrained convex optimization problem, based on the method of Lagrange multipliers, it is necessary to solve the following equation:

\[L=\frac{1}{2}\nabla\mathbf{W}^{T}\cdot\mathbf{H}\cdot\nabla\mathbf{W}+\lambda (e_{q}^{T}\nabla\mathbf{W}+w_{q}-Q(w_{q})).\] (8)

By taking the partial derivatives of \(\nabla W\) and \(\lambda\), and setting them to zero to find the steady-state solution, we have

\[\begin{cases}\frac{1}{2}(\mathbf{H}+\mathbf{H}^{T})\nabla\mathbf{W}+\lambda e _{q}=0\\ e_{q}^{T}\nabla\mathbf{W}+w_{q}-Q(w_{q})=0\end{cases}\] (9)

Solving this, we get

\[\lambda=\frac{w_{q}-Q(w_{q})}{(\mathbf{H}^{-1})_{qq}},\] (10)

\[\nabla\mathbf{W}=-\frac{w_{q}-Q(w_{q})}{(\mathbf{H}^{-1})_{qq}}\mathbf{H}^{-1 }e_{q},\] (11)

\[\nabla E=\frac{(w_{q}-Q(w_{q}))^{2}}{2(\mathbf{H}^{-1})_{qq}},\] (12)

where \((\mathbf{H}^{-1})_{qq}\) represents the value at the diagonal position \((q,q)\) of \(\mathbf{H}^{-1}\), which is the inverse of the Hessian matrix.

For the Hessian matrix, we say that \(\mathbf{H}=2\mathbf{X}\mathbf{X}^{T}\) by proving the following Lemma.

**Lemma A.1**.: _Let \(a\) be a \(1\times n\) row vector and \(X\) be an \(n\times m\) matrix. The Hessian matrix of the quadratic form \(\|aX\|_{2}^{2}\) is \(2XX^{T}\)._

Proof.: Let \(a=[a_{1},a_{2},\ldots,a_{n}]\) be a \(1\times n\) row vector, and let \(X=[x_{ij}]\) be an \(n\times m\) matrix.

Let's denote \(y=aX\). Then, \(y\) is a \(1\times m\) row vector with elements \(y_{j}\) defined as

\[y_{j}=\sum_{i=1}^{n}a_{i}x_{ij}.\]

Thus, we have

\[\|aX\|_{2}^{2}=(aX)\cdot(aX)^{T}=\sum_{j=1}^{m}y_{j}^{2}=\sum_{j=1}^{m}\left( \sum_{i=1}^{n}a_{i}x_{ij}\right)^{2}.\]

We can expand this expression and re-write it as a quadratic form, i.e.,

\[\|aX\|_{2}^{2}=\sum_{j=1}^{m}\sum_{i=1}^{n}\sum_{k=1}^{n}a_{i}a_{k}x_{ij}x_{kj}.\]

To find the Hessian matrix of this quadratic form, we treat it as a quadratic form in \(a\). Let \(Q\) be the coefficient matrix of this quadratic form.

\[\|aX\|_{2}^{2}=aQa^{T}.\]

The \((i,k)\) element of \(Q\) is given by

\[Q_{ik}=\sum_{j=1}^{m}x_{ij}x_{kj}.\]

Thus, the matrix \(Q\) can be written as

\[Q=XX^{T}.\]

And the Hessian matrix is twice \(Q\):

\[H=2Q=2XX^{T}.\]

Therefore, the Hessian matrix of \(\|aX\|_{2}^{2}\) is \(2XX^{T}\), which completes the proof. 

GPTQ quantizes the weight \(\mathbf{W}\) row by row. For each row, according to Eq. 12 mentioned above, it finds the minimum \(w_{q}\) that leads to an increase in the loss function due to quantization, then calculates scales via \(\alpha=\frac{max(\mathbf{W}_{i,j})-max(\mathbf{W}_{i,j})}{Q_{max}}\), performs quantization using \(\alpha\), and finally update the remaining values using Eq. 11. This process repeats until all parameters have been updated.

MLGPTQ considers the scenario of quantization for multiple tasks. During the forward propagation process, it dynamically loads the corresponding LoRA adapter for each task to simulate the correct activation values for the respective tasks. Consequently, the problem we need to solve is as follows.

\[\operatorname*{arg\,min}_{Q(\mathbf{W})}\|\sum_{t=1}^{T}((\mathbf{W}+ \mathbf{B}_{t}\mathbf{A}_{t})\mathbf{X}_{t}-(Q(\mathbf{W})+\mathbf{B}_{t} \mathbf{A}_{t})\mathbf{X}_{t})\|_{2}^{2},\] (13)

where \(T\) denotes the number of tasks, \(\mathbf{A}_{t}\) and \(\mathbf{B}_{t}\) are the low-rank adapter matrices of the \(t\)-th task, \(\mathbf{X}_{t}\) is the input of \(t\)-th task, and \(\mathbf{W}\) and \(Q(\mathbf{W})\) denote the original and quantized weights of a layer.

Denote \(\widetilde{\mathbf{W}_{t}}\coloneqq\mathbf{W}+\mathbf{B}_{t}\mathbf{A}_{t}\), then the problem is re-written as

\[\operatorname*{arg\,min}_{Q(\mathbf{W})}\sum_{i=1}^{d_{\text{max}}}\left\| \sum_{t=1}^{T}(\widetilde{\mathbf{W}_{t}}_{i,:}\mathbf{X}_{t}-Q(\widetilde{ \mathbf{W}_{t}})_{i,:}\mathbf{X}_{t})\right\|_{2}^{2}.\] (14)Same as GPTQ, we could get \(T\) Hessian matrix \(\mathbf{H}_{t}\), where \(t\in[1,T]\). To minimize the objective function, we can obtain the updating formulas for \(\mathbf{W}\) and \(E\) in MLGPTQ:

\[\nabla\mathbf{W}=-\frac{w_{q}-Q(w_{q})}{(\mathbf{H}_{t^{*}}^{-1})_{qq}}\mathbf{ H}_{t^{*}}^{-1}e_{q},\nabla E=\frac{(w_{q}-Q(w_{q}))^{2}}{2\left((\mathbf{H}_{t^{*} }^{-1})_{qq}\right)},\text{ where }t^{*}=\operatorname*{arg\,max}_{t\in[1,T]} \left((\mathbf{H}_{t}^{-1})_{qq}\right).\] (15)

The updating method described here leads to the max-aggregation method proposed in SS 3.1, which always selects the Hessian matrix of the task that minimizes \(\nabla E\) for updating, ultimately reducing the overall error and thus better-guiding parameter updates.

### Pseudocode of MLGPTQ

We provide the pseudocode of MLGPTQ in Algorithm 1, and we also highlight the differences compared to directly applying GPTQ to multi-task scenarios, which is termed GPTQ_trex_ed in our work (i.e., with a mixed calibration set and without any LoRA adapters).

Due to the high complexity and numerical instability of the process described in Appendix A.1, we leverage the following optimizations to accelerate the quantization, partly inspired by the practical implementation of GPTQ [12].

**Random Order Optimization.** GPTQ requires updating weights in the order that produces the smallest quantization error \(\nabla E\). For \(\mathbf{W}\in\mathbb{R}^{m\times n}\), the complexity of GPTQ is \(O(mn^{3})\). However, using a random order achieves similar results and facilitates GPU parallel optimization [12].

**Batch Processing.** Since weight updates between different columns of the same matrix \(\mathbf{W}\) are non-redundant, we use batch processing and delayed updates, with 128 columns processed at a time, to enhance computation speed.

**Cholesky Decomposition.** Using numerically stable Cholesky decomposition to pre-compute the necessary information increases computational stability.

```
1:\(\{\mathbf{X}_{t}\leftarrow(\mathbf{W}+\mathbf{B}_{t}\mathbf{A}_{t})\mathbf{X}_ {t}\}\)\(\triangleright\) The inputs of different tasks of this layer
2:\(\mathbf{H}_{t}^{-1}\leftarrow\text{Cholesky}((2\mathbf{X}_{t}\mathbf{X}_{t}^{T}+ \lambda\mathbf{I}))^{T},\text{ for }\forall t\in[1,T]\)
3:\(\mathbf{H}^{-1}\leftarrow\sum_{1}^{T}\text{Cholesky}((2\mathbf{X}_{mseed} \mathbf{X}_{mseed}^{T}+\lambda\mathbf{I})^{-1})^{T}\)\(\triangleright\) Cal inv of Hessian matrix
4:\(\mathbf{H}_{tmp}\leftarrow\text{zero\_like}(\mathbf{H}_{t}^{-1})\)\(\triangleright\) Init the tmp Hessian matrix
5:\(\mathbf{Q}\gets 0_{d_{\text{\tiny{low}}}\times d_{\text{\tiny{cl}}}}\)\(\triangleright\) Store quantized results
6:\(\mathbf{E}\leftarrow{0_{d_{\text{\tiny{low}}}\times B}}\)\(\triangleright\) Store quantization errors in blocks
7:for\(i\gets 0,B,2B,\ldots\)do
8:for\(j\gets i,\ldots,i+B-1\)do
9:\(\mathbf{Q}_{\cdot,j}\leftarrow\text{quant}(\mathbf{W}_{\cdot,j})\)\(\triangleright\) Column-wise quantization
10:\(\mathbf{E}_{\cdot,j\leftarrow i}\leftarrow(\mathbf{W}_{\cdot,j}-\mathbf{Q}_{ \cdot,j})/\text{max}_{t\in[1,T]}\left((\mathbf{H}_{t}^{-1})_{jj}\right)\)\(\triangleright\) Update quantization error
11:\(\mathbf{(H}_{tmp})_{\cdot,\delta}\leftarrow(\mathbf{H}_{t}^{-1})_{\cdot, \delta}\text{ where }t^{*}=\operatorname*{arg\,max}_{t\in[1,T]}\left((\mathbf{H}_{t}^{-1})_{s \delta}\right),\text{ for }s\in[j,i+B-1]\)
12:\(\mathbf{W}_{\cdot,j:(i+B)}\leftarrow\mathbf{W}_{\cdot,j:(i+B)}-\mathbf{E}_{ \cdot,j-1}\cdot(\mathbf{H}_{tmp})_{\cdot,j:(i+B)}\)\(\triangleright\) Update weights in current block
13:endfor
14:\(\mathbf{W}_{\cdot,(i+B)}\leftarrow\mathbf{W}_{\cdot,(i+B)}-\mathbf{E}\cdot( \mathbf{H}_{tmp})_{\cdot,(i+B),(i+B)}\)\(\triangleright\) Update weights in remaining blocks
15:\(\mathbf{W}_{\cdot,(i+B)}\leftarrow\mathbf{W}_{\cdot,(i+B)}-\mathbf{E}\cdot( \mathbf{H}^{-1})_{i,(i+B),(i+B)}\)\(\triangleright\) Update weights in remaining blocks
16:endfor ```

**Algorithm 1** Routines of MLGPTQ and GPTQ_trex_ed to quantize one layer in multi-task scenarios.

Details of our Multi-task Scheduling Strategy

Our multi-task scheduling strategy is based on task grouping and prediction-based SRTF. Compared to other scheduling strategies, it is well-suited for multi-task scenarios, achieving excellent results. The pseudocode is shown in Algorithm 2, with the helper functions \(generate\_new\_batch\) and \(schedule\_new\_batch\) in Algorithm 3 and 4. There are four queues maintaining different requests in our system: 1\(prefill\_reqs\): requests that have not yet entered the prefill stage (i.e., new requests that have not yet been served). 2\(decoding\_reqs\): requests in the decoding stage. 3\(hungry\_prefill\_reqs\): requests in a starving state that have not yet been served. 4\(hungry\_decoding\_reqs\): requests in a starving state in the decoding stage.

First, we check if \(running\_batch\) is empty (line 7) to determine whether the system is in a cold start phase (i.e., the previous scheduling step did not have any requests to serve). If yes, we perform a cold start by calling \(generate\_new\_batch\) to get a new batch to schedule from \(prefill\_reqs\) (line 8). If \(new\_batch\) is not empty, we proceed to \(prefill\) with \(new\_batch\) (line 10-11). Otherwise, the system remains idle since there are no requests (line 13). If \(running\_batch\) is not empty, we check if we have consecutively executed \(max\_cont\_decode\) decoding steps (line 16). If yes, we call \(generate\_new\_batch\) to schedule new requests from \(prefill\_reqs\) and \(hungry\_prefill\_reqs\) (line 17-20). Otherwise, we check if the continuous scheduling for a batch has reached a pre-defined threshold \(max\_cont\_decode\_one\_batch\) (line 22). If yes, we call \(schedule\_new\_batch\) to schedule new requests in the decoding stage from \(decoding\_reqs\) and \(hungry\_decoding\_reqs\) according to the SRTF and grouping strategy (line 23-25). Otherwise, we continue processing the current \(running\_batch\) (line 27-28). This two-level threshold strategy effectively reduces the frequent swapping of LoRA adapters and KV caches due to frequent batch switching. Additionally, by adjusting the threshold size, we can ensure the immediacy and flexibility of scheduling.

```
0: Four queues: \(prefill\_reqs\), \(decoding\_reqs\), \(hungry\_prefill\_reqs\), \(hungry\_decoding\_reqs\)
1:\(running\_batch\leftarrow\emptyset\)
2:\(max\_cont\_decode\leftarrow\) Threshold value for decoding
3:\(max\_cont\_decode\_one\_batch\leftarrow\) Threshold value for decoding one batch
4:\(decode\_count\gets 0\)
5:\(decode\_count\_one\_batch\gets 0\)
6:while not terminated do
7:if\(running\_batch\) is empty then
8:\(new\_batch\gets generate\_new\_batch(prefill\_reqs)\)
9:if\(new\_batch\neq\emptyset\)then
10: Perform \(prefill(new\_batch)\)
11:\(decoding\_reqs\gets decoding\_reqs+new\_batch\)
12:else
13: Keep idle
14:endif
15:else
16:if\(decode\_count\geq max\_cont\_decode\)then
17:\(new\_batch\gets generate\_new\_batch(prefill\_reqs,hungry\_prefill\_reqs)\)
18: Perform \(prefill(new\_batch)\)
19:\(decoding\_reqs\gets decoding\_reqs+new\_batch\)
20:\(decode\_count\gets 0\)
21:else
22:if\(decode\_count\_one\_batch\geq max\_cont\_decode\_one\_batch\)then
23:\(new\_batch\gets schedule\_new\_batch(decoding\_reqs,hungry\_decoding\_reqs)\)
24: Perform \(decode(running\_batch)\)
25:\(decode\_count\_one\_batch\gets 0\)
26:else
27: Perform \(decode(running\_batch)\)
28:\(decode\_count\_one\_batch\gets decode\_count\_one\_batch+1\)
29:endif
30:\(decode\_count\gets decode\_count+1\)
31:endif
32:endif
33:endif while ```
1:functiongenerate_new_batch(\(prefill\_reqs\), \(hungry\_prefill\_reqs\))
2: Sort \(prefill\_reqs\) by \(len(prompt)+predict\_output\_len\) ascending
3: Sort \(hungry\_prefill\_reqs\) by \(waiting\_time\) descending, then \(len(prompt)+predict\_output\_len\) ascending
4:\(new\_batch\leftarrow\emptyset\)
5:for each \(req\) in \(hungry\_prefill\_reqs\)do
6:if can_add_req(\(req\), \(new\_batch\)) and meet_max_lora(\(req\), \(new\_batch\)) then
7:\(new\_batch\).append(\(req\)); \(hungry\_prefill\_reqs\).remove(\(req\))
8:endif
9:endfor
10:for each \(req\) in \(prefill\_reqs\)do
11:if can_add_req(\(req\), \(new\_batch\)) and meet_max_lora(\(req\), \(new\_batch\)) then
12:\(new\_batch\).append(\(req\)); \(prefill\_reqs\).remove(\(req\))
13:endif
14:endfor
15:if\(new\_batch\) not full then
16:for each \(req\) in \(hungry\_prefill\_reqs+prefill\_reqs\)do
17:if can_add_req(\(req\), \(new\_batch\)) then
18:\(new\_batch\).append(\(req\)); \(prefill\_reqs\).remove(\(req\))
19:endif
20:endfor
21:endif
22:for each \(req\) in \(prefill\_reqs\) and \(hungry\_prefill\_reqs\)do
23:\(req\).waiting\(\_time\gets req\).waiting\(\_time+1\)
24:endfor
25:for each \(req\) in \(prefill\_reqs\)do
26:if\(req\).waiting\(\_time\geq\) Threshold then
27:\(prefill\_reqs\).remove(\(req\)); \(hungry\_prefill\_reqs\).append(\(req\))
28:endif
29:endfor
30:return\(new\_batch\)
31:endfunction ```

**Algorithm 4** The utility function \(schedule\_new\_batch\) for Algorithm 2.

```
1:functionschedule_new_batch(\(running\_batch\_decoding\_reqs\),\(hungry\_decoding\_reqs\))
2: Sort \(decoding\_reqs\) by \(predict\_output\_len-len(output)\) ascending
3: Sort \(hungry\_decoding\_reqs\) by \(waiting\_time\) descending, then \(predict\_output\_len-len(output)\) ascending
4:\(new\_batch\leftarrow\emptyset\)
5:for each \(req\) in \(hungry\_decoding\_reqs\)do
6:if can_add_req(\(req\), \(new\_batch\)) and \(req\).\(lora\in running\_batch\) then
7:\(new\_batch\).append(\(req\)); \(hungry\_decoding\_reqs\).remove(\(req\))
8:endif
9:endfor
10:for each \(req\) in \(decoding\_reqs\)do
11:if can_add_req(\(req\), \(new\_batch\)) and \(req\).\(lora\in running\_batch\) then
12:\(new\_batch\).append(\(req\)); \(decoding\_reqs\).remove(\(req\))
13:endif
14:endif
15:endif
16:if\(new\_batch\) not full then
17:for each \(req\) in \(hungry\_decoding\_reqs+decoding\_reqs\)do
18:if can_add_req(\(req\), \(new\_batch\)) then
19:\(new\_batch\).append(\(req\)); \(decoding\_reqs\).remove(\(req\))
20:endif
21:endfor
22:endif
23:for each \(req\) in \(decoding\_reqs\)do
24:\(req\).waiting\(\_time\gets req\).waiting\(\_time+1\)
25:endfor
26:return\(new\_batch\)
27:endfunction ```

**Algorithm 5** The utility function \(generate\_new\_batch\) for Algorithm 2.

More Experimental Details

### Experimental Environment

The GPU platforms for evaluation are shown in Table 5. The RTX 3090 platform is equipped with Intel(R) Core(TM) i9-10900X CPU @ 3.70GHz and 256GB host memory, while the RTX 4090 platform is equipped with Intel(R) Xeon(R) Gold 6330 CPU @ 2.00GH and 512GB host memory.

### Summary of Evaluated Tasks

We primarily select 12 major datasets for testing, covering tasks such as machine translation, text summarization, table summarization, code generation, math QA, medical QA and malicious detection. For each task, we use open-source models from Hugging Face2 that have been fine-tuned using the training set of the corresponding dataset and evaluated on the test set. The summary is presented in Table 6.

Footnote 2: https://huggingface.co/kaichuup

For the machine translation tasks of different languages, we consider the classic bilingual translation dataset OPUS [38], specifically choosing six translation tasks: French-to-English, Czech-to-English, Indonesian-to-English, Vietnamese-to-English, Danish-to-English, and Swedish-to-English, considering the diversity of languages.

For the text translation task, we consider the XLSum dataset [14], a comprehensive and diverse dataset comprising 1.35 million professionally annotated text-summary pairs extracted from the BBC.

For the table summarization task, we consider the QTSumm dataset [48], which is a large-scale dataset for query-centric summarization tasks on tabular data. It contains 7,111 human-annotated query-summary pairs and 2,934 tables covering various topics.

For the code generation task, we consider the tiny-codes dataset [13]. This dataset consists of 16 million short and clear code snippets, aiding LLM models in learning how to reason using both natural and programming languages. The dataset includes a variety of programming languages such as Python, TypeScript, JavaScript, Ruby, Julia, Rust, C++, Bash, Java, C#, and Go.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset Name & Abbreviation & Avg. Input & Avg. Output & \multirow{2}{*}{Type of Task} \\  & & Length & Length & \\ \hline OPUS-French-English & trans-fr & 121 & 105 & Machine Translation \\ OPUS-Czech-English & trans-cs & 47 & 47 & Machine Translation \\ OPUS- Indonesian-English & trans-id & 47 & 38 & Machine Translation \\ OPUS-Vietnamese-English & trans-nl & 72 & 65 & Machine Translation \\ OPUS-Danish-English & trans-da & 72 & 71 & Machine Translation \\ OPUS-Swedish-English & trans-sw & 64 & 65 & Machine Translation \\ XLSum & xlsum & 2595 & 125 & Text Summarization \\ QTSumm & QTSum & 1350 & 339 & Table Summarization \\ tiny-codes & tiny-codes & 328 & 1890 & Code Generation \\ gsm8k & GSM8k & 240 & 293 & Math Question Answer \\ medical\_meadow\_mmmlu & med-qa & 367 & 1 & Medical Question Answer \\ malicious-600k & malicious & 59 & 1 & Malicious Detection \\ \hline \hline \end{tabular}
\end{table}
Table 6: Dataset Summary

\begin{table}
\begin{tabular}{c c c} \hline \hline Platform & RTX 4090 & RTX 3090 \\ \hline GPU cores & 16384 & 10496 \\ GFLOPS & 51640 & 35580 \\ Memory Capacity & 24 GB & 24 GB \\ Memory Access Bandwidth & 1024 GB/s & 936 GB/s \\ \hline \hline \end{tabular}
\end{table}
Table 5: Experimental GPU platforms in detail.

For the math QA task, we choose the GSM8k (Grade School Math 8K) dataset [6], which consists of 8.5K high quality linguistically diverse grade school math word problems. The dataset was created to support the task of question answering on basic mathematical problems that require multi-step reasoning.

For the medical QA task, we choose medical_meadow_mmmlu [2], which contains 3.79k medical multiple choice question.

For malicious detection task, we choose malicious-600k [1], which contains 641k URLs to determine whether they are malicious.

### Summary of Metrics

(1) **SacreBLEU**[18], represented as S_BLEU, is a classic machine translation evaluation standard. It scores by comparing the n-gram overlap between the machine translation output and one or more reference translations, while also considering a brevity penalty to prevent favoring overly short translation outputs.

(2) **rouge1**[21], represented as ROUGE1, calculates the overlap ratio of words between the machine-generated text and the reference text, i.e., the unigram (single word) match rate.

(3) **rouge2**[21], represented as ROUGE2, similar to rouge1, indicates the bigram (two-word sequence) match rate.

(4) **nist_mt**[9], represented as NIST_MT, is an improvement on BLEU that gives higher weight to less common words, encouraging diversity and accuracy in translation.

(5) **meteor**[3], abbreviated as METEOR, calculates the score based on the harmonic mean of precision and recall and introduces a penalty factor to penalize excessive mismatches.

(6) **google_bleu**[41], abbreviated as G_BLEU, is an improvement on BLEU that adjusts for smoothing methods, n-gram weights, and brevity penalties to optimize its performance.

## Appendix D Multi-LoRA AWQ Migration

```
1:\(\{\mathbf{X}^{t}\}_{t=1}^{T}\), ratio_search_space\(\triangleright\) The inputs of different tasks of this layer
2:\(\mathbf{\overline{M}^{t}}\leftarrow(\mathbf{W}+\mathbf{B}^{t}\mathbf{A}^{t}) \mathbf{\overline{X}^{t}}\)\(\mathbf{\overline{M}_{\text{nucd}}}\leftarrow\sum_{1}^{T}\mathbf{W}\mathbf{X}^{t}\)\(\triangleright\) Forward to get monitoring matrix
3:\(\mathbf{W}_{\text{mean}}\leftarrow\mathbf{W}.\text{mean}(0)\)
4:\(\mathbf{X}_{\text{mean}}^{t}\leftarrow\mathbf{X}^{t}.\text{mean}(0)\) for \(\forall t\in[1,T]\)\(\mathbf{X}_{\text{mean}}\leftarrow\{\mathbf{X}^{t}\}_{t=1}^{T}.\text{mean}(0)\)\(\triangleright\) Aggregate Info: Per-channel mean of \(\mathbf{X}\)
5:\(\text{best\_s}\leftarrow\text{None}\)
6:\(\text{min\_errors}\leftarrow\infty\cdot\mathbf{1}_{\text{dim}(\mathbf{X}^{t} \cdot\text{shape-1})}\)\(\text{min\_error}\leftarrow\infty\)\(\triangleright\) Initialize minimum error(s)
7:for\(\text{ratio in ratio\_search\_space}\)do
8:\(s_{t}\leftarrow\left(\frac{\mathbf{X}_{\text{mean}}^{t},\text{max}(0)}{ \mathbf{W}_{\text{mean}},\text{max}(1-\text{ratio})}\right)\) for \(\forall t\in[1,T]\)\(\triangleright\) Aggregate Info: Calculate \(s\)
9:\(\mathbf{W}_{\text{scaled}}^{t}\leftarrow\mathbf{W}\cdot s_{t}\)\(\mathbf{\overline{W}_{\text{scaled}}}\leftarrow\mathbf{W}\cdot s\)\(\triangleright\) Scale \(\mathbf{W}\)
10:\(\mathbf{X}_{\text{scaled}}^{t}\leftarrow\mathbf{X}^{t}/s_{t}\)\(\mathbf{X}_{\text{scaled}}\leftarrow\{\mathbf{X}^{t}\}_{t=1}^{T}/s\)\(\triangleright\) Scale \(\mathbf{X}\)
11:\(\text{errs}_{t}\leftarrow\|\mathbf{M}^{t}-(\alpha\) (round (clamp \(\left(\mathbf{W}_{\text{scaled}}^{t}/\alpha,\text{min\_val},\text{max\_val} \right)\)))+\mathbf{B}^{t}\mathbf{A}^{t})\mathbf{X}_{\text{scaled}}^{t}\|_{2 \text{columns}}\) ```

**Algorithm 5** Routines of \(\mathbf{\overline{M}LAWQ}\) and \(\mathbf{\overline{A}WO_{\text{nucd}}}\) to quantize on layer in multi-task scenarios.

Our work centers on GPTQ due to its widespread use, but our solution can also adapt to AWQ. We presented the differences between MLAWQ and AWQ\({}_{\text{nucd}}\) in Alg 5. As introduced in SS3.1, most quantization methods follow the **Forward-Aggregate Info-Modify Weight-Quant** paradigm. In essence, AWQ\({}_{\text{nucd}}\) smooths outliers by multiplying weights with a smoothing factor, best_s, to minimize per-channel quart error:

* In **Forward**, the input is multiplied by the weights to create an unquantized monitor, guiding min error quantization (line 1).
* In **Aggregate Info**, the average of all samples and weights is calculated for each channel (lines 2&3) to determine smoothing factor \(s\) (line 7). \(\mathbf{W}\) and \(\mathbf{X}\) are smoothed to remove outliers (lines 8&9). Then, smoothed \(\mathbf{W}\) is pseudo-quantized (quantize-then-dequantize to simulate round loss) and compared to the unquantized monitor for quantization error (line 10). This process iterates over various ratios (line 6), selecting the factor with the smallest error as best_s (line 11). Then, this best_s is used to **Modify the weight** (line 12), followed by the **Quant** (line 13) process using the modified weight.

The drawbacks discussed in SS3.1 also exist for AWQ\({}_{\text{nucd}}\) in multi-task quantization:

* **Forward** (line 1): It can't pass LoRA adapters during activation distribution simulation, causing quantization bias during inference.
* **Aggregate Info**: It uses \(X_{\text{mean}}=X.\text{mean}(0)\), a naive mixed average of multi-tasks' info. Since each task affects each channel differently, simply averaging blurs distributions, ignoring individual effects.

As explained in SS3.1 and shown in Fig 2, MLGPTQ mainly improves the first three steps to tackle GPTQ's issues in multi-task scenarios. Similarly, we can fix AWQ's issues to create a better multi-task quantization algorithm MLAWQ:* **Forward**: MLAWQ loads corresponding LoRA adapter for each task to participate in forward propagation, accurately simulating real activation distribution (line 1).
* **Aggregate Info**: Instead of mixing and averaging features of each column across all tasks to compute \(s\), MLAWQ computes the average for each task separately to get \(s_{i}\) (line 3&7). Then it calculates quantization error for each column rather than the entire matrix (line 10). If the \(i\)-th task results in the smallest quantization error for the \(j\)-th column, it sets best_s \([j]=s_{i}[j]\) (line 11). This approach allows optimal error minimization, showing each task's individual effect on different channels, enhancing **Aggregate Info** (lines 3&6-11), and improving the **Modify Weight** (line 12) and **Quant** (line 13) processes.

In summary, our work identifies common drawbacks of current single-task quantization methods in multi-task scenarios. By addressing these issues, we can develop more precise multi-task quantization algorithms.

Broader Impact and Future Work

This work proposes _LoRA-Inlaid_, a brand new LLM serving system for the multi-task scenario. _LoRA-Inlaid_ is featured with a series of innovations, specifically the multi-task joint quantization algorithm, the dynamic task addition mechanism, and the multi-task scheduling strategy. Considering the booming research and applications of LLMs in various downstream tasks, we believe _LoRA-Inlaid_ has the potential to gain widespread adoption and shed light on the high-performance and resource-efficient system designs for follow-up works in the field of LLM serving. However, there are several issues that _LoRA-Inlaid_ does not consider currently.

On the one hand, the multi-task quantization algorithm in _LoRA-Inlaid_ does not involve the detection of malicious or poisoning tasks that may bring negative impacts on the other tasks. One of the most typical use cases of _LoRA-Inlaid_ is the personalization of LLMs, where clients can upload their data to create personalized LoRA adapters using the same base model (or directly upload their self-tuned LoRA adapters). The server is responsible for serving requests from all these clients using the proposed _LoRA-Inlaid_ system. Fortunately, these LoRA adapters are independently manufactured, so we can apply malicious detection to them individually. For instance, the server can prepare a rich set of evaluations to assess the security risks of each LoRA adapter, including violence, discrimination, unlawful responses, etc. If any LoRA adapters fail to pass the evaluation, the server can reject serving them.

On the other hand, the multi-task scheduling strategy in _LoRA-Inlaid_ ignores the fairness among different tasks (e.g., controlling the number of output tokens to be close), which may make our work ineffective under some settings. To measure fairness among tasks, we can compute a weighted combination of numbers of input and output tokens for each task. This is because the prefilling and decoding phases in LLM inference have different workload characteristics [15](also why these tokens differ in online service pricing). Then, we can borrow the idea of Weighted Fair Queueing (WFQ) [32] for scheduling different tasks. We wish to address these issues in the future.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 3 and Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 5 and Appendix E. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Appendix A.1, Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 4.1, Appendix C.1, Appendix C.2, and we have provided the link to source code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

[MISSING_PAGE_FAIL:26]

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Section 4 and Appendix C.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Appendix E. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Not applicable. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: Not applicable. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.