# Stochastic Extragradient with Flip-Flop Shuffling & Anchoring: Provable Improvements

Jiseok Chae

Department of Mathematical Sciences

KAIST

Daejeon, Republic of Korea

jsch@kaist.ac.kr

&Chulhee Yun

Kim Jaechul Graduate School of AI

KAIST

Seoul, Republic of Korea

chulhee.yun@kaist.ac.kr

&Donghwan Kim

Department of Mathematical Sciences

KAIST

Daejeon, Republic of Korea

donghwankim@kaist.ac.kr

###### Abstract

In minimax optimization, the extragradient (EG) method has been extensively studied because it outperforms the gradient descent-ascent method in convex-concave (C-C) problems. Yet, stochastic EG (SEG) has seen limited success in C-C problems, especially for unconstrained cases. Motivated by the recent progress of shuffling-based stochastic methods, we investigate the convergence of _shuffling-based SEG_ in unconstrained _finite-sum_ minimax problems, in search of convergent shuffling-based SEG. Our analysis reveals that both random reshuffling and the recently proposed flip-flop shuffling alone can suffer divergence in C-C problems. However, with an additional simple trick called anchoring, we develop the _SEG with flip-flop anchoring_ (SEG-FFA) method which successfully converges in C-C problems. We also show upper and lower bounds in the strongly-convex-strongly-concave setting, demonstrating that SEG-FFA has a provably faster convergence rate compared to other shuffling-based methods.

## 1 Introduction

Minimax problems with a finite-sum structure, which are optimization problems of the form

\[\min_{\bm{x}}\max_{\bm{y}}\ f(\bm{x},\bm{y})\coloneqq\frac{1}{n}\sum_{i=1}^{n} f_{i}(\bm{x},\bm{y}),\] (1)

can be found in many interesting applications, such as generative adversarial networks [19], refining diffusion models [28], adversarial training [37], optimal transport based generative models [48], multi-agent reinforcement learning [53], and so on. Deterministic methods for minimax problems, such as _gradient descent-ascent_ (GDA) [3] and _extragradient_ (EG) [29], have been extensively studied in the literature. It is though known that, unlike _gradient descent_ (GD) for minimization problems, GDA may diverge even when \(f\) is convex on \(\bm{x}\) and concave on \(\bm{y}\). On the other hand, EG employs a two-step update procedure, named _extrapolation_ and _update_ steps (see Section 2 for details), which allows it to find an optimum under this _convex-concave_ setting [29, 51], and moreover, attains a convergence rate faster than GDA [4] when \(f\) is strongly convex on \(\bm{x}\) and strongly concave on \(\bm{y}\).

In contrast, attempts to construct stochastic variants of these algorithms have not been so fruitful. When \(f\) is convex-concave, _stochastic gradient descent-ascent_ (SGDA) clearly may diverge, just asin the deterministic GDA. To make matters worse, _stochastic extragradient_ (SEG) methods have also had limited success on unconstrained convex-concave problems. As we elaborate in Section 2 in more detail, existing versions of SEG and their analyses have limitations that hinder its application to general unconstrained finite-sum convex-concave problems, requiring additional assumptions such as bounded domain, increasing batch size, convex-concavity of each component \(f_{i}\), uniformly bounded gradient variance, and/or absence of convergence rates.

In the context of finite-sum optimization, most of the theoretical studies on stochastic methods have long been based on the _with-replacement_ sampling scheme, where an index \(i(t)\) is independently and uniformly sampled among \(\{1,\ldots,n\}\) at each iteration \(t\). Such a sampling scheme is relatively easy to theoretically analyze, because the sampled \(f_{i(t)}\) is an unbiased estimator of the full objective function \(f\). In practice, however, inspired by the empirical observations of faster convergence in finite-sum minimization [8; 47], the _without-replacement_ sampling schemes have been the de facto standard. Among them, the most popular is the _random reshuffling_ (RR) scheme, where in every _epoch_ consisting of \(n\) iterations, the indices are chosen exactly once in a randomly shuffled order.

This gap between theory and practice in minimization problems is being closed by the recent breakthroughs in stochastic gradient descent (SGD), namely that SGD with RR leads to a provably faster convergence compared to with-replacement SGD when the number of epochs is large enough [1; 35; 39; 41; 55; 56]. This has motivated further studies on finding other shuffling-based sampling schemes that can improve upon RR, resulting in the discoveries such as the _flip-flop_ scheme [46] and _gradient balancing_ (GraB) [11; 32]. The flip-flop scheme is a particularly simple yet interesting modification of RR with improved rates in _quadratic_ problems: a random permutation is used twice in a single epoch (i.e., two passes over \(n\) components in an epoch), but the order is reversed in the second pass.

The aforesaid progress in minimization also triggered the study of stochastic minimax methods with shuffling. Similar to minimization problems, SGDA with RR indeed converges faster than the with-replacement SGDA, under assumptions such as strongly-convex-strongly-concave objectives [15] or \(f\) satisfying the Polyak-Lojasiewicz condition [13]. Despite the superiority of EG over GDA, the _SEG with shuffling_ has not been shown to have a solid theoretical advantage over the SGDA with shuffling yet. This motivated us to study the following question:

_Can shuffling schemes provide convergence guarantees for SEG, improved upon SGDA with shuffling, in unconstrained finite-sum (strongly-)convex-(strongly-)concave settings?_

There are two types of SEG: _same-sample_ SEG, where a sample chosen is used both for the extrapolation step and the update step, and _independent-sample_ SEG, where two independently chosen samples are used in each step. We will particularly focus on the same-sample SEG because it combines more naturally with shuffling-based schemes than independent-sample SEG. Therefore, to be more specific, we are interested in developing shuffling-based variants of _same-sample_ SEG in unconstrained finite-sum minimax problems with minimal modifications to the algorithm. We show that **(a)** in convex-concave settings, our new method reaches an optimum with a guarantee on the rate of convergence, overcoming the limitations of existing results; **(b)** in strongly-convex-strongly-concave settings, the method converges faster than other SGDA/SEG variants.

### Our Contributions

In this paper, we study various same-sample SEG algorithms under different shuffling schemes, and propose the _stochastic extragradient with flip-flop anchoring_ (\(\mathsf{SEG-FFA}\)) method, which is SEG amended with the techniques of _flip-flop_ shuffling scheme and _anchoring_. Here, by _anchoring_ we refer to a step of taking a convex combination between the initial and final iterates of an epoch, resembling the celebrated _Krasnosel'skii-Mann iteration_[30; 33] as we discuss in Section 5. With such minimal modifications to SEG, we show that \(\mathsf{SEG-FFA}\) achieves provably improved convergence guarantees. More precisely, our contributions can be listed as follows (see Table 1 for a summary). For clarity, we use \(\mathsf{SEG-US}\) to refer to with-replacement SEG, where US stands for uniform sampling.

* We first study the same-sample versions of \(\mathsf{SEG-US}\), SEG with RR (\(\mathsf{SEG-RR}\)), and SEG with flip-flop (\(\mathsf{SEG-FF}\)). We show that they all can diverge when \(f\) is convex-concave,1by constructing an explicit counterexample (Theorem 4.1). This shows that shuffling alone _cannot_ fix the divergence issue of SEG-US.
* We next investigate the underlying cause for the nonconvergence of SEG-US, SEG-RR, and SEG-FF. In particular, we identify that either they fail to match the update equation of the reference method EG beyond _first-order_ Taylor expansion terms, or attempting to match both the _first-_ and _second-order_ Taylor expansion terms results in divergence (Proposition 5.2).
* By adopting a simple technique of _anchoring_ on top of flip-flop shuffling, we devise our algorithm SEG-FFA, whose epoch-wise update deterministically matches EG up to second-order Taylor expansion terms (Proposition 5.3). We prove that SEG-FFA enjoys improved convergence guarantees, as anticipated by our design principle. Most importantly, we show that SEG-FFA achieves a convergence rate of \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{K^{1/3}}})\) when \(f\) is convex-concave, where \(K\) denotes the number of epochs. This is in stark contrast to other baseline algorithms that diverge under this setting (see the last column of Table 1).
* Moreover, we show that when \(f\) is strongly-convex-strongly-concave, SEG-FFA achieves a convergence rate of \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{nK^{4}}})\) (Theorem 5.5). In addition, by proving \(\Omega(\nicefrac{{1}}{{nK^{3}}})\) lower bounds for the convergence rates of SGDA-RR and SEG-RR under the same setting (Theorem 5.6), we show that SEG-FFA has a _provable advantage_ over these baseline algorithms.

## 2 Related Works

Extragradient and EG+Extragradient (EG) method [29] is a widely used minimax optimization method, well-known for resolving the nonconvergence issue of GDA on convex-concave problems. In this paper, we also consider EG+ [17], which is a generalization of EG. The update rule of EG+ is defined, for stepsizes \(\{\eta_{1,k}\}_{k\geq 0}\) and \(\{\eta_{2,k}\}_{k\geq 0}\), as

\[\begin{cases}\bm{u}^{k}\leftarrow\bm{x}^{k}-\eta_{1,k}\nabla_{\bm{x}}\,f(\bm {x}^{k},\bm{y}^{k})\\ \bm{v}^{k}\leftarrow\bm{y}^{k}+\eta_{1,k}\nabla_{\bm{y}}\,f(\bm{x}^{k},\bm{y}^ {k})\end{cases},\qquad\quad\begin{cases}\bm{x}^{k+1}\leftarrow\bm{x}^{k}-\eta_ {2,k}\nabla_{\bm{x}}\,f(\bm{u}^{k},\bm{v}^{k})\\ \bm{y}^{k+1}\leftarrow\bm{y}^{k}+\eta_{2,k}\nabla_{\bm{y}}\,f(\bm{u}^{k},\bm{v }^{k})\end{cases}.\] (2)

The first step is called the _extrapolation step_, and the second step is called the _update step_. If \(f\) is convex-concave, Diakonikolas et al. [17] show that EG+ reaches an optimum when \(\eta_{1,k}\geq\eta_{2,k}\). In particular, when \(\eta_{1,k}=\eta_{2,k}\), we recover the standard EG by Korpelevich [29].

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline  & \multicolumn{2}{c|}{Strongly-Convex-Strongly-Concave} & \multicolumn{2}{c}{Convex-Concave} \\ \cline{2-5} Method & Upper Bound & Lower Bound & Upper Bound & Lower Bound \\ \hline SGDA-US & \(\mathcal{O}(\frac{1}{k})\)[31] & \(\Omega(\frac{1}{k})\)[13] & N/A & \(\Omega(1)\) (as SGA) \\ SEG-US & \(\mathcal{O}(\frac{1}{k})\)[20] & \(\Omega(\frac{1}{k})\)[6] & N/A\({}^{\dagger\dagger}\) & \(\Omega(1)\) (Timb. 4.1) \\ \hline SGDA-RR & \(\tilde{\mathcal{O}}(\frac{1}{nk^{2}})\)[15] & \(\Omega(\frac{1}{nk^{3}})\) (Timb. 5.6) & N/A & \(\Omega(1)\) (as SGA) \\ SEG-RR & \(\tilde{\mathcal{O}}(\frac{1}{nk^{2}})\)[18] & \(\Omega(\frac{1}{nk^{3}})\) (Timb. 5.6) & \(\mathcal{O}(\frac{1}{nk^{3}})\)[7] (18) & \(\Omega(1)\) (Timb. 4.1) \\ SEG-FF & \(\tilde{\mathcal{O}}(\frac{1}{nk^{2}})\) (Timb. F.5) & \(-\) & N/A & \(\Omega(1)\) (Timb. 4.1) \\ \hline SEG-FFA & \(\tilde{\mathcal{O}}(\frac{1}{nk^{4}})\) (Timb. 5.5) & \(-\) & \(\tilde{\mathcal{O}}(\frac{1}{k^{1/3}})\) (Timb. 5.4) & \(-\) \\ \hline \hline \end{tabular}

* [17, 20] show upper bounds for SEG-US, but they require increasing batch sizes as well as other assumptions (see Appendix B.1).
* [25] shows that _independent-sample_ SEG-US converges for stepsizes \(\alpha_{k}\), \(\beta_{t}\) decaying at different rates, but gives no conv. rate.
* Unfortunately, the proof of this convergence bound in this recent AISTATS 2024 paper seems to be incorrect: see Appendix B.4.

\end{table}
Table 1: Summary of upper/lower convergence rate bounds of _same-sample_ SEG for unconstrained finite-sum minimax problems, without requiring increasing batch size, convex-concavity of each component, and uniformly bounded gradient variance. Pseudocode of algorithms can be found in Appendix A. We only display terms that become dominant for sufficiently large \(T\) and \(K\). To compare the with-replacement versions (-US) against shuffling-based versions, one can substitute \(T=nK\). The optimality measure used for SC-SC problems is \(\mathbb{E}[\|\hat{\bm{z}}-\bm{z}^{*}\|^{2}]\) for the last iterate \(\hat{\bm{z}}\). For C-C problems, we consider \(\min_{t=0,\ldots,T}\mathbb{E}[\|\bm{Fz}_{t}\|^{2}]\) for with-replacement methods and \(\min_{k=0,\ldots,K}\mathbb{E}[\|\bm{Fz}_{0}^{k}\|^{2}]\) for shuffling-based methods.

Stochastic Variants of ExtragradientIn (2), if the _stochastic estimators_ of \(\nabla_{\bm{x}}\,f\) and \(\nabla_{\bm{y}}\,f\) are used instead of the gradients themselves, we get the standard SEG. If an estimator chosen is used for both the extrapolation and the update steps, we get the _same-sample_ SEG, which we focus on in this paper; see Appendix A for the pseudocode.

While EG improves upon GDA, unfortunately, SEG has not been able to show a clear advantage over SGDA. On one hand, analyses of SEG on strongly-convex-strongly-concave problems have shown some success; see, _e.g._, [18; 20]. Yet, on the other hand, for general unconstrained convex-concave problems, to the best of our knowledge, the existing stochastic variants of EG and their analyses face several limitations.23 Assumptions commonly imposed in the existing literature include: **(i)** the domain is bounded, either explicitly or implicitly [27; 36], **(ii)** one must increase the batch size to achieve convergence [9; 17; 20],4 and **(iii)** each component \(f_{i}\) is convex-concave [20; 36], and **(iv)** the components have uniformly bounded gradient variance [9; 17; 42]. For further details, see Appendix B.1 and Table 2 therein. Notably, Hsieh et al. [25] prove convergence of the _independent-sample_ SEG without these four restrictions, but the result lacks an explicit convergence rate.

Footnote 2: Most of these results are carried out assuming access to a stochastic oracle of \(f\), which indeed subsumes the finite-sum setting as a special case. However, it seems unlikely that these limitations of the existing studies will be easily resolved by simply narrowing the focus down to the finite-sum setting; see Appendix B.3.

Footnote 3: Recently, Emmanouilidis et al. [18] claimed the convergence of SEG-RR in the convex-concave setting. Unfortunately, however, there seems to be a flaw in their proof. We defer a discussion on this to Appendix B.4.

Footnote 4: In fact, for the methods studied in [17; 20] it is possible to show that increasing the batch size is _strictly necessary and unavoidable_ for convergence; see Appendix H.2.

Our proposed SEG-FFA overcomes all the aforementioned limitations, and reaches an optimum with an explicit rate in unconstrained convex-concave problems, under relatively mild conditions. The readers may also refer to [7] for a comprehensive overview on this topic.

Meanwhile, under the finite-sum setting, variance reduction schemes have also been considered, achieving some promising results [2; 10]. Yet, although theoretically appealing, variance reduction is less widely used in practice due to their curiously inferior performance in training neural networks [16]. On top of this practical issue, variance reduction techniques share the aforementioned limitation (ii), as accessing full gradients can be viewed as increasing the batch size. In contrast, our main goal in this paper is to study how a carefully chosen sampling scheme, with minimal modifications to the algorithm, can improve the convergence of SEG without the need for increased batch size; therefore, we believe that our work is not directly comparable to variance reduction-based EG.

Taylor Expansion Matching and Convergence GuaranteesIt has been repeatedly reported that the convergence of an optimization method is deeply related to the degree to which the Taylor expansion (with respect to the step size) of its update equation matches with that of an already known convergent method. For example, Mokhtari et al. [38] observed that the advantage of EG over GDA comes from the Taylor expansion of update equations of EG matching that of the _proximal point_ (PP) method [34] up to second-order terms, whereas GDA matches PP only up to first-order terms.

The advantages of the shuffling scheme over the with-replacement sampling can be explained in a similar way. One key property of shuffling-based methods is that, while the individual estimators are biased as they are dependent to other estimators within the same epoch, the overall stochastic error across the epoch decreases dramatically compared to using \(n\) independent unbiased estimators. For instance, in SGD with RR [1] and in SGDA with RR [15], the overall progress made within each epoch exactly matches their deterministic counterparts up to the first-order, leaving an error as small as \(\mathcal{O}(\eta^{2})\), where \(\eta\) is the stepsize. Rajput et al. [46] observed that, when each component functions are convex quadratics, then using flip-flop on SGD can reduce the error further to \(\mathcal{O}(\eta^{3})\), resulting in an even faster convergence. As we further elaborate in Section 5, the motivation behind our design principle of SEG-FFA is also based on this line of observations.

## 3 Notations and Problem Settings

Let \([n]\subset\mathbb{Z}\) denote the set \(\{1,\ldots,n\}\). The set of all permutations on \([n]\) will be denoted by \(\mathcal{S}_{n}\). For the finite-sum minimax problem (1), we denote the _saddle gradient_ operators by

\[\bm{F}(\,\cdot\,)\coloneqq\begin{bmatrix}\nabla_{\bm{x}}\,f(\,\cdot\,)\\ -\nabla_{\bm{y}}\,f(\,\cdot\,)\end{bmatrix},\;\;\bm{F}_{i}(\,\cdot\,)\coloneqq \begin{bmatrix}\nabla_{\bm{x}}\,f_{i}(\,\cdot\,)\\ -\nabla_{\bm{y}}\,f_{i}(\,\cdot\,)\end{bmatrix},\quad i\in[n].\]The derivative of an operator will be denoted with a prefix \(D\). For example, the derivative of \(\bm{F}\) is denoted by \(D\bm{F}\). Often a single vector will be used to denote the minimization and the maximization variable at once. For instance, for \(\bm{z}\in\mathbb{R}^{d_{1}+d_{2}}\) which is a concatenation of \(\bm{x}\in\mathbb{R}^{d_{1}}\) and \(\bm{y}\in\mathbb{R}^{d_{2}}\), we simply write \(\bm{F}\bm{z}\) to denote \(\bm{F}(\bm{x},\bm{y})\).

It is well known that, if \(f\) is \(\mu\)-strongly convex on \(\bm{x}\) and \(\mu\)-strongly concave on \(\bm{y}\) for some \(\mu>0\) (respectively, \(\mu=0\)), then its saddle gradient \(\bm{F}\) is \(\mu\)-strongly monotone (respectively, monotone), in the following sense. For a proof of this standard fact, see, _e.g._, [22].

**Assumption 3.1** (Monotonicity & Strong Monotonicity).: For \(\mu>0\), we say that an operator \(\bm{F}\) is \(\mu\)-strongly monotone if, for any \(\bm{z},\bm{w}\in\mathbb{R}^{d_{1}+d_{2}}\), it holds that

\[\left\langle\bm{F}\bm{z}-\bm{F}\bm{w},\bm{z}-\bm{w}\right\rangle\geq\mu\left\| \bm{z}-\bm{w}\right\|^{2}.\] (3)

If (3) holds for \(\mu=0\), then we say that \(\bm{F}\) is monotone.

Thus, from now on, we will use the term _strongly monotone_ (respectively, _monotone_) problems rather than strongly-convex-strongly-concave (respectively, convex-concave) problems. Notice that we only assume that the full saddle gradient \(\bm{F}\) is (strongly) monotone, not the individual \(\bm{F}_{i}\)'s.

In addition, we remark that our convergence analysis under the monotonicity of \(\bm{F}\), Theorem 5.4, in fact requires only a relaxed version of monotonicity, known as _star_-monotonicity. This condition imposes the inequality (3) with \(\mu=0\), but only when \(\bm{w}=\bm{z}^{*}\), where \(\bm{z}^{*}\) is a point such that \(\bm{F}\bm{z}^{*}=\bm{0}\). This relaxation allows for a certain degree of nonconvex-nonconcavity in \(f\). For a more detailed discussion on the star-monotonicity condition, see Appendix G.1.

Other three underlying assumptions we make on the problem (1) can be listed as follows.

**Assumption 3.2** (Existence of an Optimal Solution).: An optimal solution of the problem (1), which is a point we denote by \(\bm{z}^{*}=(\bm{x}^{*},\bm{y}^{*})\) that satisfies

\[f(\bm{x}^{*},\bm{y})\leq f(\bm{x}^{*},\bm{y}^{*})\leq f(\bm{x},\bm{y}^{*})\]

for any \(\bm{x}\in\mathbb{R}^{d_{1}}\) and \(\bm{y}\in\mathbb{R}^{d_{2}}\), exists in \(\mathbb{R}^{d_{1}+d_{2}}\).

Because the problem is unconstrained and \(f\) is convex-concave, a point \(\bm{z}^{*}\) is an optimum if and only if \(\bm{F}\bm{z}^{*}=\bm{0}\). For strongly monotone problems, Assumption 3.2 is not explicitly required, as it is guaranteed _a priori_[5, Proposition 22.11]. For monotone problems, we explicitly impose Assumption 3.2 in order to exclude pathological problems such as \(f(x,y)=x-y\).

**Assumption 3.3** (Smoothness).: Each \(f_{i}\) is \(L\)-smooth, and each \(\bm{F}_{i}\) is \(M\)-smooth. That is, for any \(\bm{z},\bm{w}\in\mathbb{R}^{d_{1}+d_{2}}\),

* \(\left\|\bm{F}_{i}\bm{z}-\bm{F}_{i}\bm{w}\right\|\leq L\left\|\bm{z}-\bm{w}\right\|\),
* \(\left\|D\bm{F}_{i}\bm{z}-D\bm{F}_{i}\bm{w}\right\|\leq M\left\|\bm{z}-\bm{w}\right\|\).

It is worth mentioning that the gradient operator \(\bm{F}_{i}\) arising from a quadratic function \(f_{i}\) is \(M\)-smooth with \(M=0\). Notice also that, by the finite-sum structure \(\bm{F}=\frac{1}{n}\sum_{i=1}^{n}\bm{F}_{i}\), it is clear that Assumption 3.3 implies \(f\) being \(L\)-smooth and \(\bm{F}\) being \(M\)-smooth.

The \(L\)-smoothness assumption on the objective functions is standard in the optimization literature, while the \(M\)-smoothness assumption on the saddle gradients may look less standard. This smoothness assumption on the saddle gradient, in other words the Lipschitz Hessian condition, for analyzing SEG-FFA stems from the analysis of the flip-flop sampling scheme [46]. In particular, this is needed for bounding the high-order error terms between the (deterministic) EG and SEG-FFA in Section 5.1. The existing analysis of flip-flop sampling [46] is limited to quadratic functions that trivially have \(0\)-Lipschitz Hessians (\(M=0\)), so our analysis is a step forward.

**Assumption 3.4** (Component Variance).: There exist constants \(\rho\geq 0\) and \(\sigma\geq 0\) such that

\[\frac{1}{n}\sum_{i=1}^{n}\,\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2} \leq(\rho\left\|\bm{F}\bm{z}\right\|+\sigma)^{2}\qquad\forall\bm{z}.\] (4)

For strongly monotone problems, Assumption 3.4 is not explicitly required, because it can be obtained as a consequence of the preceding assumptions: see Lemma C.9. Nevertheless, for convenience, we will keep the notations \(\rho\) and \(\sigma\) as in (4) for the strongly monotone setting as well.

In many existing works on stochastic optimization methods for minimax problems, Assumption 3.4 with \(\rho=0\) is imposed. This _uniform_ bound on the variance simplifies the convergence analyses, but it is also fairly restrictive especially in the unconstrained settings. Already for bilinear finite-sum minimax problems \(f(\bm{x},\bm{y})=\frac{1}{n}\sum_{i=1}^{n}\bm{x}^{\top}\bm{B}_{i}\bm{y}\), one can easily check that setting \(\rho=0\) forces the matrices \(\bm{B}_{i}\) to be exactly equal to each other. For machine learning applications, it has been also reported that the assumption with \(\rho=0\) often fails to hold [7]. Therefore, allowing the variance to grow with the gradient \(\bm{F}\bm{z}\) makes the assumption much more realistic.

The Lipschitz Hessian condition and the component variance assumption for monotone problems may still look rather strong. We leave the study on how one can relax such assumptions to prove _upper_ bounds for convergence rates as an interesting future direction. On the other hand, while our lower bound results in Theorems 4.1 and 5.6 are derived under those strong assumptions, they still serve as lower bound results also for larger function classes that do not have those assumptions. In other words, the value of those results are not limited because of those assumptions being imposed.

## 4 Shuffling Alone Is Not Enough

Under the settings we have discussed, we study the SEG with shuffling-based sampling schemes. First we describe the precise methods of our consideration, namely the \(\mathsf{SEG}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{FF}\).

For \(k\geq 0\), in the beginning of an epoch, a random permutation \(\tau_{k}\) is sampled from a uniform distribution over \(\mathcal{S}_{n}\). Then, for \(n\) iterations, we use each of the component functions once, in the order determined by \(\tau_{k}\). That is, for \(i=0,1,\ldots,n-1\) we do

\[\bm{w}_{i}^{k} \leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(i+1)}\bm{z}_ {i}^{k},\] (5) \[\bm{z}_{i+1}^{k} \leftarrow\bm{z}_{i}^{k}-\beta_{k}\bm{F}_{\tau_{k}(i+1)}\bm{w}_{i} ^{k},\]

for some stepsizes \(\alpha_{k}\) and \(\beta_{k}\). In case of \(\mathsf{SEG}\)-\(\mathsf{RR}\), the epoch is completed here, and we set \(\bm{z}_{0}^{k+1}\leftarrow\bm{z}_{n}^{k}\) as the initial point for the next epoch.

In case of \(\mathsf{SEG}\)-\(\mathsf{FF}\), we additionally perform \(n\) more iterations in the epoch, as proposed in Rajput et al. [46]. In these additional iterations, the component functions are each used once more, but in the reverse order. That is, for \(i=n,n+1,\ldots,2n-1\), we do

\[\bm{w}_{i}^{k} \leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(2n-i)}\bm{z}_ {i}^{k},\] (6) \[\bm{z}_{i+1}^{k} \leftarrow\bm{z}_{i}^{k}-\beta_{k}\bm{F}_{\tau_{k}(2n-i)}\bm{w}_ {i}^{k}.\]

Then we set \(\bm{z}_{0}^{k+1}\leftarrow\bm{z}_{2n}^{k}\) as the initial point for the next epoch. The full pseudocode of these methods can be found in Appendix A.

When \(\bm{F}\) is strongly monotone, it is possible to show that both \(\mathsf{SEG}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{FF}\) indeed provide speed-up over \(\mathsf{SEG}\)-\(\mathsf{US}\). The well-known rate of \(\mathsf{SEG}\)-\(\mathsf{US}\) under strong monotonicity of \(\bm{F}\) is \(\Theta(\nicefrac{{1}}{{1}}/{{T}})\), where \(T\) is the total number of iterations [6, 20]. Translating this rate to our shuffling-based setting, where there are \(\Theta(n)\) iterations per epoch, this rate amounts to \(\Theta(\nicefrac{{1}}{{nK}})\). Recently, Emmanouilidis et al. [18] have shown that \(\mathsf{SEG}\)-\(\mathsf{RR}\), under the same setting as ours, attains a convergence rate of \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{nK^{2}}})\), on par with the rate of \(\mathsf{SGDA}\)-\(\mathsf{RR}\)[15]. In Appendix F, we also show that \(\mathsf{SEG}\)-\(\mathsf{FF}\) attains a similar rate of convergence.

However, it turns out that the benefit of shuffling does not extend further beyond the strongly monotone setting. In fact, when \(\bm{F}\) is merely monotone, then in the worst case, \(\mathsf{SEG}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{FF}\) suffers from nonconvergence, just as in the case of \(\mathsf{SEG}\)-\(\mathsf{US}\).

**Theorem 4.1**.: _For \(n=2\), there exists a minimax problem with \(f(x,y)=\frac{1}{2}\sum_{j=1}^{2}f_{i}(x,y)\) having a monotone \(\bm{F}\), consisting of \(L\)-smooth quadratic \(f_{i}\)'s satisfying Assumption 3.4 with \((\rho,\sigma)=(1,0)\), such that \(\mathsf{SEG}\)-\(\mathsf{US}\), \(\mathsf{SEG}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{FF}\) diverge in expectation for any positive stepsizes._

We provide the explicit counterexample and the proof of divergence in Appendix H.1. Note that Theorem 4.1 and its proof in Appendix H.1 imply that \(\min_{t=0,\ldots,T}\mathbb{E}[\left\lVert\bm{F}\bm{z}_{t}\right\rVert^{2}]= \Omega(1)\) for \(\mathsf{SEG}\)-\(\mathsf{US}\) and \(\min_{k=0,\ldots,K}\mathbb{E}[\left\lVert\bm{F}\bm{z}_{0}^{k}\right\rVert^{2}]= \Omega(1)\) for \(\mathsf{SEG}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{FF}\), as summarized in Table 1.

SEG-FFA: SEG with Flip-Flop Anchoring

In this section, we investigate the underlying cause for nonconvergence of SEG-RR and SEG-FF from the perspective of how accurately they match the convergent EG or PP methods in terms of the Taylor expansions of updates. We then propose adding a simple _anchoring_ step at the end of each epoch of SEG-FF. It turns out that adding the _anchoring_ step, which is a step of taking a convex combination of an iterate with a previously computed iterate, reduces the stochastic noise and leads to a method with improved convergence properties.

### Design Principle: Second-Order Matching

As observed by [38], the key feature of EG behind its superior convergence properties compared to GDA is its update rule closely resembling PP, while the "error" of GDA as an approximation of PP is so large that it hinders convergence. The difference between the updates of EG and PP, in the Taylor expansion, is as small as \(\mathcal{O}(\eta^{3})\) per iteration, where \(\eta\) is the stepsize. On the other hand, GDA and PP show a difference of \(\mathcal{O}(\eta^{2})\), and this greater "error" explains why GDA diverges while EG and PP converge. Of course, EG and PP are not the only two algorithms that converge in the monotone setting; let us recall the update rule of EG+ method [17], and Taylor-expand it as the following:

\[\begin{split}\bm{z}^{+}&\coloneqq\bm{z}-\eta_{2} \bm{F}(\bm{z}-\eta_{1}\bm{F}\bm{z})\\ &=\bm{z}-\eta_{2}\bm{F}\bm{z}+\eta_{1}\eta_{2}D\bm{F}(\bm{z})\bm {F}\bm{z}+O(\eta_{1}^{2}\eta_{2}).\end{split}\] (7)

EG+ is known to converge for unconstrained monotone problems if \(\eta_{1}\geq\eta_{2}\). When \(\eta_{1}=\eta_{2}\), it recovers EG and matches PP up to second-order terms.

Based on these observations, we now state our key principle for designing a convergent version of SEG: _second-order matching_. We would like to choose proper stepsizes, sampling scheme, and anchoring scheme so that our without-replacement SEG can _deterministically_ match the update equation of a convergent algorithm (EG/PP or EG+) up to the \(O(\eta^{2})\) terms (i.e., _second-order_ terms in the Taylor expansion), thereby satisfying a small \(\mathcal{O}(\eta^{3})\) approximation error. We show that **(a)** this _second-order matching_ can be achieved with _flip-flop anchoring_, but not solely by permutation-based sampling such as RR and flip-flop (without anchoring), and **(b)** second-order matching indeed grants convergence for monotone problems. In particular, we demonstrate that

1. SEG-RR suffers a poor approximation error of \(\mathcal{O}(\eta^{2})\) as an approximation of EG/EG+.
2. SEG-FF can match EG+ up to second-order terms, but it results in a choice of stepsizes (\(\eta_{2}=2\eta_{1}\)) that make EG+ diverge (Proposition 5.2).
3. SEG-FFA, the method we propose, matches EG up to second-order terms to get an error of \(\mathcal{O}(\eta^{3})\) (Proposition 5.3), achieving convergence in monotone problems (Theorem 5.4).

To this end, let us consider a general form of SEG that incorporates any arbitrary sampling scheme. More precisely, in the \(k\)-th "epoch" consisted of \(N\) iterations, the components are chosen in the order of \(\bm{T}_{0}^{k},\bm{T}_{1}^{k},\cdots,\bm{T}_{N-1}^{k}\), where \(\bm{T}_{i}^{k}\in\{\bm{F}_{1},\ldots,\bm{F}_{n}\}\) for each \(i\). For our purpose, we assume that \(N\) is some multiple of \(n\) (e.g., \(N=n\) for SEG-RR, \(N=2n\) for SEG-FF). Then, given \(\alpha\) and \(\beta\) we perform SEG updates, for \(i=0,1,\ldots,N-1\),

\[\begin{split}\bm{w}_{i}^{k}&\leftarrow\bm{z}_{i}^{k} -\alpha\bm{T}_{i}^{k}\bm{z}_{i}^{k},\\ \bm{z}_{i+1}^{k}&\leftarrow\bm{z}_{i}^{k}-\beta\bm{T} _{i}^{k}\bm{w}_{i}^{k}.\end{split}\] (8)

#### 5.1.1 Necessity of Flip-Flop Sampling

The general method in (8) that sets the initial point for the next epoch as \(\bm{z}_{0}^{k+1}\leftarrow\bm{z}_{N}^{k}\) satisfies the following property.

**Proposition 5.1**.: _Suppose that Assumption 3.3 holds. For some \(\bm{\epsilon}_{N}^{k}=o\left((\alpha+\beta)^{2}\right)\), it holds that_

\[\bm{z}_{0}^{k+1}=\bm{z}_{0}^{k}-\beta\sum_{j=0}^{N-1}\bm{T}_{j}^{k}\bm{z}_{0}^{ k}+\alpha\beta\sum_{j=0}^{N-1}D\bm{T}_{j}^{k}(\bm{z}_{0}^{k})\bm{T}_{j}^{k}\bm{z}_{0}^{ k}+\beta^{2}\sum_{i<j}D\bm{T}_{j}^{k}(\bm{z}_{0}^{k})\bm{T}_{i}^{k}\bm{z}_{0}^{k}+ \bm{\epsilon}_{N}^{k}.\] (9)See Appendix D.1 for the proof. To make (7) and (9) match up to the second-order, both the equations

\[\frac{\eta_{2}}{n}\sum_{j=1}^{n}\bm{F}_{i}\bm{z}_{0}^{k}=\beta\sum_{ j=0}^{N-1}\bm{T}_{j}^{k}\bm{z}_{0}^{k}\qquad\text{ and }\] (10) \[\frac{\eta_{1}\eta_{2}}{n^{2}}\Big{(}\sum_{j=1}^{n}D\bm{F}_{j}(\bm {z}_{0}^{k})\bm{F}_{j}\bm{z}_{0}^{k}+\sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0}^{k}) \bm{F}_{i}\bm{z}_{0}^{k}\Big{)}=\alpha\beta\sum_{j=0}^{N-1}D\bm{T}_{j}^{k}(\bm{ z}_{0}^{k})\bm{T}_{j}^{k}\bm{z}_{0}^{k}+\beta^{2}\sum_{i<j}D\bm{T}_{j}^{k}(\bm{z}_{0}^ {k})\bm{T}_{i}^{k}\bm{z}_{0}^{k}\] (11)

must hold. Clearly, without-replacement sampling will make (10) hold. However, it is easy to check that random reshuffling falls short of making (11) hold. This is because, if RR is used, then \(\bm{T}_{0}^{k},\bm{T}_{1}^{k},\ldots,\bm{T}_{n-1}^{k}\) is nothing but a reordering of \(\bm{F}_{1},\ldots,\bm{F}_{n}\) into \(\bm{F}_{\tau(1)},\ldots,\bm{F}_{\tau(n)}\), so the RHS of (11) can only contain terms \(D\bm{F}_{\tau(j)}(\bm{z}_{0}^{k})\bm{F}_{\tau(i)}\bm{z}_{0}^{k}\) with \(i\leq j\). This observation motivates the use of flip-flop sampling, because choosing \(\bm{T}_{i}^{k}=\bm{T}_{2n-1-i}^{k}\) lets all the required terms \(D\bm{F}_{j}(\bm{z}_{0}^{k})\bm{F}_{i}\bm{z}_{0}^{k}\) to appear in the RHS of (11).

#### 5.1.2 Designing SEG-Ffa

Flip-flop does resolve the aforesaid issue, but still another complication remains for plain SEG-FF.

**Proposition 5.2**.: _Suppose we use flip-flop sampling (without anchoring). In order to make (10) and (11) hold, we must choose \(\beta=\nicefrac{{\eta_{1}}}{{n}}\) and \(\alpha=\nicefrac{{\beta}}{{2}}\). However, this leads to \(\eta_{2}=2\eta_{1}\), which is the set of parameters that fails to make EG+ converge._

For the proof, see Appendix D.2. This shows that a modification is necessary to develop a stochastic method that achieves second-order matching to _convergent_ EG/EG+ methods.

We thus propose to add an _anchoring_ step:

\[\bm{z}_{0}^{k+1}\leftarrow\tfrac{1}{2}\left(\bm{z}_{N}^{k}+\bm{z}_{0}^{k} \right),\] (12)

after finishing the \(N\) updates (8), instead of \(\bm{z}_{0}^{k+1}\leftarrow\bm{z}_{N}^{k}\). This is our _Stochastic ExtraGradient with Flip-Flop Anchoring_ (SEG-FFA) method, named after the design of combining the flip-flop sampling scheme and the anchoring step. We note that this idea of taking a convex combination has originally appeared in the Krasnosel'skii-Mann iteration [30, 33], and also under the name of _Lookahead_ methods [12, 43]. This slightly differs from the more widely used _Halpern iteration_[23] based anchoring (_cf._[54]), which would have used the initial point \(\bm{z}_{0}^{0}\) instead of \(\bm{z}_{0}^{k}\) in (12).

This anchoring step changes (9) accordingly, and essentially amounts to dividing the right-hand sides of (10) and (11) each by \(2\) (see Appendix D for the detailed derivations). We show that choosing \(\alpha=\nicefrac{{\beta}}{{2}}\) in fact leads to the second-order matching to EG, i.e., EG+ with \(\eta_{1}=\eta_{2}\).

**Proposition 5.3**.: _Suppose that Assumptions 3.3 and 3.4 hold. Then, for \(\beta_{k}=\eta\) and \(\alpha_{k}=\nicefrac{{\beta_{k}}}{{2}}\), SEG-FFA becomes an approximation of EG with error at most \(\mathcal{O}(\eta^{3})\). In other words, we achieve_

\[\left\|\bm{z}_{0}^{k}-\eta n\bm{F}(\bm{z}_{0}^{k}-\eta n\bm{F}\bm{z}_{0}^{k})- \bm{z}_{0}^{k+1}\right\|=\mathcal{O}(\eta^{3}).\]

In other words, adding the anchoring step allows us to get a method that well approximates the convergent EG with an error as small as \(\mathcal{O}(\eta^{3})\). For a more in-depth discussion, see Appendix E.

### Convergence Analysis of SEG-Ffa

As a result of the second-order matching, we obtain SEG-FFA, a stochastic method that has an error of \(\mathcal{O}(\eta^{3})\) as an approximation of EG. Achieving this order of magnitude for the approximation error turns out to be the key to the exact convergence to an optimum under the monotone setting.

**Theorem 5.4**.: _Suppose that \(\bm{F}\) is (star-)monotone, Assumptions 3.2, 3.3, and 3.4 hold, and we are running SEG-FFA. Then, for any \(K\geq 1\), by choosing the stepsizes sufficiently small and decaying as \(\beta_{k}=\mathcal{O}(\nicefrac{{1}}{{k^{1/3}}}\log k)\) and \(\alpha_{k}=\nicefrac{{\beta_{k}}}{{2}}\), the iterates generated by SEG-FFA achieves the bound_

\[\min_{k=0,1,\ldots,K}\mathbb{E}\left\|\bm{F}\bm{z}_{0}^{k}\right\|^{2}= \mathcal{O}\left(\frac{(\log K)^{2}}{K^{1/3}}\right).\]For the full statement of the theorem and its proof, see Appendix G. We note that, although Theorem 5.4, and also Theorem 5.5 below, are stated specifically for \(\mathsf{SEG}\)-\(\mathsf{FFA}\), our analyses show that both theorems can be applied to any method that achieves the second-order matching in terms of Proposition 5.3.

The reduced error also shows a gain in the rate of convergence under the strongly monotone setting. This aligns with the intuition that error hinders convergence, hence having a smaller error is beneficial.

**Theorem 5.5**.: _Suppose that \(\bm{F}\) is \(\mu\)-strongly monotone with \(\mu>0\) and Assumption 3.3 holds. Then, there exists a choice of \(\eta>0\) such that, when \(\mathsf{SEG}\)-\(\mathsf{FFA}\) is run for \(K\) epochs with constant stepsizes \(\beta_{k}=\eta\) and \(\alpha_{k}=\nicefrac{{\eta}}{{2}}\), for some constant \(\omega\) independent of \(\eta\), the iterates generated by \(\mathsf{SEG}\)-\(\mathsf{FFA}\) achieves the bound_

\[\mathbb{E}\left\|\bm{z}_{0}^{K}-\bm{z}^{*}\right\|^{2}\leq\exp\left(-\frac{1} {2}\mu\omega nK\right)\left\|\bm{z}_{0}^{0}-\bm{z}^{*}\right\|^{2}+\mathcal{O }\left(\frac{\left(\log(n^{1/4}K)\right)^{4}}{nK^{4}}\right).\]

Theorem 5.5 actually stems from a unified analysis that encompasses all the shuffling-based SEG methods introduced in this paper, including \(\mathsf{SEG}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{FF}\). See Appendix F for the details.

Notice the exponent \(4\) of the number of epochs \(K\) in the convergence rate, which is twice as large as the exponent \(2\) of \(\mathsf{SGDA}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{RR}\). In fact, this gain in the rate of convergence turns out to be fundamental. As we show in the following theorem, the theoretical lower bounds of convergence for \(\mathsf{SGDA}\)-\(\mathsf{RR}\) and \(\mathsf{SEG}\)-\(\mathsf{RR}\) with constant stepsize are both \(\Omega(\nicefrac{{1}}{{n}}K^{3})\). This exhibits that there is a _provable gap_ between those methods and \(\mathsf{SEG}\)-\(\mathsf{FFA}\), which attains \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{n}}K^{4})\).

**Theorem 5.6**.: _Suppose \(n\geq 2\). For both \(\mathsf{SGDA}\)-\(\mathsf{RR}\) with constant stepsize \(\alpha_{k}=\alpha>0\) and \(\mathsf{SEG}\)-\(\mathsf{RR}\) with constant stepsize \(\alpha_{k}=\alpha>0\), \(\beta_{k}=\beta>0\), there exists a \(\mu\)-strongly monotone minimax problem \(f(\bm{z})=\frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{z})\) with \(\mu>0\) such that regardless of stepsizes, we have_

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}-\bm{z}^{*}\right\|^{2}\right]=\begin{cases} \Omega\left(\frac{\sigma^{2}}{L\mu nK}\right)&\text{ if }K\leq L/\mu,\\ \Omega\left(\frac{L\sigma^{2}}{\mu^{3}nK^{3}}\right)&\text{ if }K>L/\mu.\end{cases}\]

Proof.: The full statement and the proof are presented in Appendix H.3. 

## 6 Experiments

We consider randomly generated quadratic problems of the form

\[\min_{\bm{x}\in\mathbb{R}^{d_{x}}}\max_{\bm{y}\in\mathbb{R}^{d_{y}}}\ \frac{1}{n}\sum_{i=1}^{n}\ \begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}^{\!\!\top}\!\!\begin{bmatrix}\bm{A}_{i}&\bm{B}_{i}\\ \bm{B}_{i}^{\top}&-\bm{C}_{i}\end{bmatrix}\!\!\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}-\bm{t}_{i}^{\top}\!\!\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}.\] (13)

In particular, we sample the random components so that the full objective is either monotone or strongly monotone, respectively, while each of the components may be nonmonotone. For the exact descriptions on how we constructed the problems, see Appendix I.1.

Monotone CaseWe ran the experiment on \(5\) random instances of (13) with the stepsizes scheduled as \(\eta_{k}=\nicefrac{{\eta_{0}}}{{(1+k/10)^{0.34}}}\) where \(\eta_{0}=\min\{0.01,\frac{1}{L}\}\) for \(\mathsf{SEG}\)-\(\mathsf{FFA}\), and \(\alpha_{k}=\beta_{k}=\eta_{k}\) for \(\mathsf{SEG}\)-\(\mathsf{US}\), \(\mathsf{SEG}\)-\(\mathsf{RR}\), and \(\mathsf{SEG}\)-\(\mathsf{FF}\). The exponent \(0.34\) is to ensure a sufficient decay rate required by Theorem 5.4, and the convergence of \(\mathsf{SEG}\)-\(\mathsf{FFA}\) under such a stepsize scheduling is validated in Remark G.5. The value of \(\eta_{0}\) is, however, a heuristically determined small number. The results of the geometric mean over the \(5\) runs are plotted in Figure 1. As expected by our theory, \(\mathsf{SEG}\)-\(\mathsf{FFA}\) successfully shows convergence, while all of \(\mathsf{SEG}\)-\(\mathsf{FF}\), \(\mathsf{SEG}\)-\(\mathsf{RR}\), and \(\mathsf{SEG}\)-\(\mathsf{US}\) diverge in the long run.

Strongly monotone caseAlong with the variants of SEG, we also compare the performances of \(\mathsf{SGDA}\)-\(\mathsf{RR}\) and \(\mathsf{SGDA}\)-\(\mathsf{US}\). We ran the experiment on \(5\) random instances of (13) with stepsizes \(\eta_{k}=0.001\), and the results are plotted in Figure 1. Additional results obtained from using other stepsizes can be found in Appendix I.4. We again observe an agreement between the empirical results and our theory; \(\mathsf{SEG}\)-\(\mathsf{FFA}\) eventually finds the point with the smallest gradient norm among the methods that are considered.

Further additional experiments and ablation studies we have conducted can be found in Appendix I.

## 7 Conclusion

We proposed SEG-FFA, a new stochastic variant of EG that uses flip-flop sampling and anchoring. While being a minimal modification from the vanilla SEG, SEG-FFA attains the crucial "second-order matching" property to the deterministic EG, leading to a two-fold improved convergence. On one hand, SEG-FFA reaches an optimum in the monotone setting, unlike many baseline methods such as SEG-US, SEG-RR, and SEG-FF that diverge. Moreover, in the strongly monotone setting, SEG-FFA shows a faster convergence with a provable gap from the other methods.

An interesting future direction would be to extend our work to more general nonconvex-nonconcave problems, further exploring the potentials of the second-order matching technique. It would also be appealing to further study whether it is possible to devise a new method that achieves second-order (or higher) matching without the anchoring step, potentially enhancing our understanding of the effectiveness of the matching technique.

## Acknowledgments and Disclosure of Funding

This work was supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2019-NR040050). JC and DK acknowledge support from the NRF grant (No. RS-2022-NR071715) funded by the Korea government (MSIT), and the Samsung Science & Technology Foundation grant (No. SSTF-BA2101-02). CY acknowledges support from the NRF grant (No. RS-2023-00211352) funded by the Korea government (MSIT).

## References

* [1] Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffling: optimal rates without component convexity and large epoch requirements. _Advances in Neural Information Processing Systems_, 33:17526-17535, 2020.
* [2] Ahmet Alacaoglu and Yura Malitsky. Stochastic variance reduction for variational inequality methods. In _Conference on Learning Theory_, pages 778-816. PMLR, 2022.
* [3] Kenneth J. Arrow and Leonid Hurwicz. Reduction of constrained maxima to saddle-point problems. In _Proc. Third Berkeley Symp. on Math. Statist. and Prob._, volume 5, pages 1-20, 1956. Univ. of Calif. Press.
* [4] Waiss Azizian, Ioannis Mitliagkas, Simon Lacoste-Julien, and Gauthier Gidel. A tight and unified analysis of gradient-based methods for a whole spectrum of differentiable games. In

Figure 1: Experimental results on the (left) monotone and (right) strongly monotone examples, comparing the variants of SEG. For a fair comparison, we take the number of passes over the full dataset as the abscissae. In other words, we plot \(\|\bm{F}\bm{z}_{0}^{2}/2\|^{2}\|/\|\bm{F}\bm{z}_{0}^{2}\|^{2}\) for SEG-FFA and SEG-FF, as they pass through the whole dataset twice every epoch, and \(\|\bm{F}\bm{z}_{0}^{2}/\|\bm{F}\bm{z}_{0}^{2}\|^{2}\) for the other methods, as they pass once every epoch.

International Conference on Artificial Intelligence and Statistics_, pages 2863-2873. PMLR, 2020.
* [5] Heinz H. Bauschke and Patrick L. Combettes. _Convex Analysis and Monotone Operator Theory in Hilbert Spaces_. Springer, 2nd edition, 2017.
* [6] Aleksandr Beznosikov, Valentin Samokhin, and Alexander Gasnikov. Distributed saddle-point problems: Lower bounds, near-optimal and robust algorithms. _arXiv preprint arXiv:2010.13112_, 2020.
* [7] Aleksandr Beznosikov, Boris Polyak, Eduard Gorbunov, Dmitry Kovalev, and Alexander Gasnikov. Smooth monotone stochastic variational inequalities and saddle point problems: A survey. _European Mathematical Society Magazine_, 127:15-28, 2023.
* [8] Leon Bottou. Curiously fast convergence of some stochastic gradient descent algorithms. In _Proceedings of the symposium on learning and data science, Paris_, volume 8, pages 2624-2633. Citeseer, 2009.
* [9] Xufeng Cai, Chaobing Song, Cristobal Guzman, and Jelena Diakonikolas. Stochastic Halpern iteration with variance reduction for stochastic monotone inclusions. _Advances in Neural Information Processing Systems_, 35:24766-24779, 2022.
* [10] Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian. Variance reduction for matrix games. _Advances in Neural Information Processing Systems_, 32, 2019.
* [11] Jaeyoung Cha, Jaewook Lee, and Chulhee Yun. Tighter lower bounds for shuffling SGD: Random permutations and beyond. In _International Conference on Machine Learning_, pages 3855-3912. PMLR, 2023.
* [12] Tatjana Chavdarova, Matteo Pagliardini, Sebastian U Stich, Francois Fleuret, and Martin Jaggi. Taming GANs with Lookahead-minmax. In _The Ninth International Conference on Learning Representations_, 2021.
* [13] Hanseul Cho and Chulhee Yun. SGDA with shuffling: faster convergence for nonconvex-PL minimax optimization. In _The Eleventh International Conference on Learning Representations_, 2023.
* [14] Sayantan Choudhury, Eduard Gorbunov, and Nicolas Loizou. Single-call stochastic extragradient methods for structured non-monotone variational inequalities: Improved analysis under weaker conditions. _Advances in Neural Information Processing Systems_, 36:64918-64956, 2023.
* [15] Aniket Das, Bernhard Scholkopf, and Michael Muehlebach. Sampling without replacement leads to faster rates in finite-sum minimax optimization. _Advances in Neural Information Processing Systems_, 35:6749-6762, 2022.
* [16] Aaron Defazio and Leon Bottou. On the ineffectiveness of variance reduced optimization for deep learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [17] Jelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 2746-2754. PMLR, 2021.
* [18] Konstantinos Emmanouilidis, Rene Vidal, and Nicolas Loizou. Stochastic extragradient with random reshuffling: Improved convergence for variational inequalities. In _International Conference on Artificial Intelligence and Statistics_, pages 3682-3690. PMLR, 2024.
* [19] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.
* [20] Eduard Gorbunov, Hugo Berard, Gauthier Gidel, and Nicolas Loizou. Stochastic extragradient: General analysis and improved rates. In _International Conference on Artificial Intelligence and Statistics_, pages 7865-7901. PMLR, 2022.

* [21] Eduard Gorbunov, Nicolas Loizou, and Gauthier Gidel. Extragradient method: \(O(1/K)\) last-iterate convergence for monotone variational inequalities and connections with cocoercivity. In _International Conference on Artificial Intelligence and Statistics_, pages 366-402. PMLR, 2022.
* [22] Benjamin Grimmer, Haihao Lu, Pratik Worah, and Vahab Mirrokni. The landscape of the proximal point method for nonconvex-nonconcave minimax optimization. _Mathematical Programming_, 201(1-2):373-407, 2023.
* [23] Benjamin Halpern. Fixed points of nonexpanding maps. _Bulletin of the American Mathematical Society_, 73(6):957-961, 1967.
* [24] Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernandez del Rio, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, September 2020.
* [25] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggressively, update conservatively: Stochastic extragradient methods with variable stepsize scaling. _Advances in Neural Information Processing Systems_, 33:16223-16234, 2020.
* [26] J. D. Hunter. Matplotlib: A 2d graphics environment. _Computing in Science & Engineering_, 9 (3):90-95, 2007.
* [27] Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with stochastic mirror-prox algorithm. _Stochastic Systems_, 1(1):17-58, 2011.
* [28] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ODE trajectory of diffusion. In _International Conference on Learning Representations_, 2024.
* [29] Galina M. Korpelevich. The extragradient method for finding saddle points and other problems. _Matecon_, 12:747-756, 1976.
* [30] M. A. Krasnosel'skii. Two remarks on the method of successive approximations. _Uspekhi Matematicheskikh Nauk_, 10:123-127, 1955.
* [31] Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon Lacoste-Julien. Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity. _Advances in Neural Information Processing Systems_, 34:19095-19108, 2021.
* [32] Yucheng Lu, Wentao Guo, and Christopher De Sa. GraB: Finding provably better data permutations than random reshuffling. _Advances in Neural Information Processing Systems_, 35:8969-8981, 2022.
* [33] W. Robert Mann. Mean value methods in iteration. _Proceedings of the American Mathematical Society_, 4(3):506-510, 1953.
* [34] Bernard Martinet. Regularisation d'inequations variationelles par approximations succesives. _Revue Francaise d'informatique et de Recherche operationelle_, 1970.
* [35] Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis with vast improvements. _Advances in Neural Information Processing Systems_, 33:17309-17320, 2020.
* [36] Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtarik, and Yura Malitsky. Revisiting stochastic extragradient. In _International Conference on Artificial Intelligence and Statistics_, pages 4573-4582. PMLR, 2020.
* [37] Aleksander Madry, Aleksandar Makelov, Ludwig Schmdit, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.

* [38] Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach. In _International Conference on Artificial Intelligence and Statistics_, pages 1497-1507. PMLR, 2020.
* [39] Dheeraj Nagaraj, Prateek Jain, and Praneeth Netrapalli. SGD without replacement: Sharper rates for general smooth convex functions. In _International Conference on Machine Learning_, pages 4703-4711. PMLR, 2019.
* [40] Yurii Nesterov. _Lectures on convex optimization_, volume 137 of _Springer Optimization and Its Applications_. Springer, second edition, 2018.
* [41] Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten Van Dijk. A unified convergence analysis for shuffling-type gradient methods. _The Journal of Machine Learning Research_, 22(1):9397-9440, 2021.
* [42] Thomas Pethick, Olivier Fercoq, Puya Latafat, Panagiotis Patrinos, and Volkan Cevher. Solving stochastic weak Minty variational inequalities without increasing batch size. In _International Conference on Learning Representations_, 2023.
* [43] Thomas Pethick, Wanyun Xie, and Volkan Cevher. Stable nonconvex-nonconcave training via linear interpolation. _Advances in Neural Information Processing Systems_, 37, 2023.
* [44] L. D. Popov. A modification of the Arrow-Hurwitz method of search for saddle points. _Matematicheskie Zametki_, 28(5):777-784, 1980.
* [45] Shashank Rajput, Anant Gupta, and Dimitris Papailiopoulos. Closing the convergence gap of SGD without replacement. In _International Conference on Machine Learning_, pages 7964-7973. PMLR, 2020.
* [46] Shashank Rajput, Kangwook Lee, and Dimitris Papailiopoulos. Permutation-based SGD: Is random optimal? In _International Conference on Learning Representations_, 2022.
* [47] Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for large-scale matrix completion. _Mathematical Programming Computation_, 5(2):201-226, 2013.
* [48] Litu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In _International Conference on Learning Representations_, 2022.
* [49] Itay Safran and Ohad Shamir. How good is SGD with random shuffling? In _Conference on Learning Theory_, pages 3250-3284. PMLR, 2020.
* [50] Itay Safran and Ohad Shamir. Random shuffling beats SGD only after many epochs on ill-conditioned problems. _Advances in Neural Information Processing Systems_, 34:15151-15161, 2021.
* [51] Mikhail V. Solodov and Benar F. Svaiter. A hybrid approximate extragradient-proximal point algorithm using the enlargement of a maximal monotone operator. _Set-Valued Analysis_, 7(4):323-345, 1999.
* [52] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, CJ Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020.
* [53] Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong. Multi-agent reinforcement learning via double averaging primal-dual optimization. _Advances in Neural Information Processing Systems_, 31, 2018.

* [54] TaeHo Yoon and Ernest K. Ryu. Accelerated Algorithms for Smooth Convex-Concave Minimax Problems with \(\mathcal{O}(1/k^{2})\) Rate on Squared Gradient Norm. In _International Conference on Machine Learning_, pages 12098-12109. PMLR, 2021.
* [55] Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffle SGD be better than reshuffling SGD and GD? In _Conference on Learning Theory_, pages 4653-4658. PMLR, 2021.
* [56] Chulhee Yun, Shashank Rajput, and Suvrit Sra. Minibatch vs local SGD with shuffling: Tight convergence bounds and beyond. In _International Conference on Learning Representations_, 2022.

###### Contents

* 1 Introduction
	* 1.1 Our Contributions
* 2 Related Works
* 3 Notations and Problem Settings
* 4 Shuffling Alone Is Not Enough
* 5 SEG-FFA: SEG with Flip-Flop Anchoring
	* 5.1 Design Principle: Second-Order Matching
		* 5.1.1 Necessity of Flip-Flop Sampling
		* 5.1.2 Designing SEG-FFA
	* 5.2 Convergence Analysis of SEG-FFA
* 6 Experiments
* 7 Conclusion
* A Pseudocode of the Algorithms
* B Further Details and Discussions on the Related Works
* B.1 A Summary of the Limitations of the Existing Works in the Monotone Setting
* B.2 On the Assumptions Made by Gorbunov et al. [20]
* B.3 Finite Sum Structure vs. General Stochastic Setting
* B.4 On the Claimed Convergence of SEG-RR in the Monotone Setting by Emmanouilidis et al. [18]
* C Useful Lemmata
* D Missing Proofs for Section 5
* D.1 Unravelling the Recurrence of the Generalized SEG in (8) and (12)
* D.2 Insufficiency of Only Using Flip-Flop Sampling
* E Within-Epoch Error Analysis for Upper Bounds
* E.1 Auxiliary Lemmata
* E.2 Upper Bounds of the Within-Epoch Errors
* E.2.1 Proof of Equation (31) for SEG-FFA
* E.2.2 Proof of Equation (32) for SEG-FFA
* E.2.3 Upper Bounds of the Within-Epoch Errors for SEG-FF
* E.2.4 Upper Bounds of the Within-Epoch Errors for SEG-RR
* F Convergence Bounds in the Strongly Monotone Setting

* G Convergence Rate of SEG-FFA in the Monotone Setting
* G.1 Star-monotonicity
* G.2 Convergence Analysis of SEG-FFA in the (Star-)Monotone Setting
* H Proof of Lower Bounds
* H.1 Proof of the Divergence of SEG-US, SEG-RR and SEG-FF
* H.2 Proof of Limited Convergence of SEG-US in Monotone Cases
* H.3 Proof of SGDA-RR and SEG-RR Lower Bounds
* H.3.1 Existing Lower Bound for SGD-RR
* H.3.2 Proof of Lower Bound for SGDA-RR
* H.3.3 Proof of Lower Bound for SEG-RR
* I Additional Experiments
	* I.1 Problem Constructions for Experiments in Section 6
	* I.2 Monotone Case & Ablation Study on the Anchoring Step
	* I.3 Monotone Case: Comparison with Hsieh et al. [25]
	* I.4 Strongly Monotone Case Again, with Various Stepsizes
Pseudocode of the Algorithms

We present the pseudocode of the algorithms we consider in this paper in Algorithms 2, 3 and 4, with the pseudocode of the with-replacement stochastic methods in Algorithm 1.

``` Input: The number of components \(n\); stepsize sequences \(\{\alpha_{t}\}_{t\geq 0}\) and \(\{\beta_{t}\}_{t\geq 0}\) Initialize:\(\bm{z}_{0}\in\mathbb{R}^{d_{1}+d_{2}}\) for\(t=0,1,\ldots\)do \(\text{sample }i(t)\) uniformly from \(\{1,\ldots,n\}\) ifSGDA-US then \(\bm{z}_{t+1}\leftarrow\bm{z}_{t}-\alpha_{t}\bm{F}_{i(t)}\bm{z}_{t}\) elseifSEG-US then \(\bm{w}_{t}\leftarrow\bm{z}_{t}-\alpha_{t}\bm{F}_{i(t)}\bm{z}_{t}\) \(\bm{z}_{t+1}\leftarrow\bm{z}_{t}-\beta_{t}\bm{F}_{i(t)}\bm{w}_{t}\) endif endfor ```

**Algorithm 1**SEG-US / SGDA-US

``` Input: The number of components \(n\); stepsize sequences \(\{\alpha_{k}\}_{k\geq 0}\) and \(\{\beta_{k}\}_{k\geq 0}\) Initialize:\(\bm{z}_{0}^{0}\in\mathbb{R}^{d_{1}+d_{2}}\) for\(k=0,1,\ldots\)do \(\text{sample }\tau_{k}\) uniformly from \(\mathcal{S}_{n}\) for\(i=0\)to\(n-1\)do ifSGDA-RR then \(\bm{z}_{i+1}^{k}\leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(i+1)}\bm{z}_ {i}^{k}\) elseifSEG-RR then \(\bm{w}_{i}^{k}\leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(i+1)}\bm{z}_ {i}^{k}\) \(\bm{z}_{i+1}^{k}\leftarrow\bm{z}_{i}^{k}-\beta_{k}\bm{F}_{\tau_{k}(i+1)}\bm{w} _{i}^{k}\) endif endfor \(\bm{z}_{0}^{k+1}\leftarrow\bm{z}_{n}^{k}\) endfor ```

**Algorithm 2**SEG-RR / SGDA-RR

``` Input: The number of components \(n\); stepsize sequences \(\{\alpha_{k}\}_{k\geq 0}\) and \(\{\beta_{k}\}_{k\geq 0}\) Initialize:\(\bm{z}_{0}^{0}\in\mathbb{R}^{d_{1}+d_{2}}\) for\(k=0,1,\ldots\)do \(\bm{w}_{i}^{k}\leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(i+1)}\bm{z}_ {i}^{k}\) \(\bm{z}_{i+1}^{k}\leftarrow\bm{z}_{i}^{k}-\beta_{k}\bm{F}_{\tau_{k}(i+1)}\bm{w} _{i}^{k}\) endfor for\(i=n\)to\(2n-1\)do \(\bm{w}_{i}^{k}\leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(2n-i)}\bm{z}_ {i}^{k}\) \(\bm{z}_{i+1}^{k}\leftarrow\bm{z}_{i}^{k}-\beta_{k}\bm{F}_{\tau_{k}(2n-i)}\bm{w} _{i}^{k}\) endfor \(\bm{z}_{0}^{k+1}\leftarrow\bm{z}_{2n}^{k}\) endfor ```

**Algorithm 3**SEG-FF

``` Input: The number of components \(n\); stepsize sequences \(\{\alpha_{k}\}_{k\geq 0}\) and \(\{\beta_{k}\}_{k\geq 0}\) Initialize:\(\bm{z}_{0}^{0}\in\mathbb{R}^{d_{1}+d_{2}}\) for\(k=0,1,\ldots\)do \(\bm{w}_{i}^{k}\leftarrow\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(i+1)}\bm{z}_ {i}^{k}\) \(\bm{z}_{i+1}^{k}

## Appendix B Further Details and Discussions on the Related Works

### A Summary of the Limitations of the Existing Works in the Monotone Setting

In Table 2, we have summarized the settings considered in each of the previous works on stochastic variants of EG discussed in Section 2, and compare them with our settings. Please note that we focus on the monotone \(\bm{F}\) setting in the table. Entries that are worth further discussions are marked, with the corresponding explanations below.

(*)The methods proposed in these works are not stochastic variants of EG in a strict sense. The method introduced by Cai et al. [9] is rather a hybrid of EG and the Halpern iteration [23], while the method by Choudhury et al. [14] is a stochastic version of the so-called _optimistic gradient_ method [44]. Hence, determining whether these methods fall into the category of same-sample methods or not is unnecessary. Nonetheless, as these works focus on solving a similar problem to ours, we include them as references.

(\(\dagger\))Under the assumptions that Gorbunov et al. [20] make in their paper, one can show that each of the components must necessarily be (star-)monotone when the full \(\bm{F}\) is (star-)monotone. For further explanations on why this is the case, see the following Appendix B.2.

(\(\ddagger\))Yet, to be precise, what Gorbunov et al. [20] have shown in the monotone case is that SEG-US can find an optimal solution if we _increase_ the batch size each iteration. If the batch size is fixed as a constant, then they were only able to show that the iterates will be _bounded_ in the (star-)monotone setting. In particular, they did not provide a guarantee that the iterates will be necessarily convergent.

In fact, as we demonstrate with an explicit counterexample in Appendix H.2, if we do not increase the batch size each iteration, then it is possible to show that SEG-US in the worst case will _never_

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline  & same & required & bounded & uniform & monotone \\  & sample? & batch size & domain? & gradient variance? & components? \\ \hline Ours &  & constantly 1 &  &  &  \\ \hline Cai et al. [9]\({}^{*}\) & N/A & increasing &  &  &  \\ \hline Choudhury et al. [14]\({}^{*}\) & N/A & increasing &  &  &  \\ \hline Diakonikolas et al. [17] &  & increasing\({}^{\ddagger}\) &  &  &  \\ \hline Gorbunov et al. [20] &  & increasing\({}^{\ddagger}\) &  &  & (star-)\({}^{\dagger}\) \\ \cline{2-6}  &  & increasing\({}^{\ddagger}\) &  &  &  \\ \hline Hsieh et al. [25]\({}^{\lx@sectionsign}\) &  & constant &  &  &  \\ \hline Juditsky et al. [27] &  & constant &  &  &  \\ \hline Mishchenko et al. [36] &  & constant &  &  &  \\ \hline Pethick et al. [42] &  & constant &  &  &  \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of the underlying settings between ours and the existing works.

converge to an optimal point. This nonconvergence result in fact holds for any SEG-US whose extrapolation and update stepsizes differ by a constant factor. Hence, it not only applies to [20], but also to [17].

(\(\lx@sectionsign\))Hsieh et al. [25] show that _independent-sample_ SEG-US converges for stepsizes \(\alpha_{t},\beta_{t}\) decaying at certain different rates, but gives no convergence rates.

(\(\lx@paragraphsign\))Mishchenko et al. [36] assume a uniformly bounded gradient variance in the strongly monotone case. In the monotone case, the bound they derived depends on the supremum of the gradient variance over the domain that is under consideration. Hence, in the monotone case, either the domain has to be (implicitly) bounded, or the uniform gradient variance assumption should be imposed.

### On the Assumptions Made by Gorbunov et al. [20]

We would like to first clarify that in [20], the requirement to increase the batch size is utilized only in the monotone setting: see, _e.g._, Corollary E.4 therein.

Gorbunov et al. [20] use a generalized notion of \(\mu\)-strong monotonicity, namely the _\(\mu\)-quasi strong monotonicity_, which requires the operator \(\bm{F}\) to satisfy

\[\left\langle\bm{F}\bm{z},\bm{z}-\bm{z}^{*}\right\rangle\geq\mu\left\|\bm{z}- \bm{z}^{*}\right\|^{2}.\] (14)

In the notion of \(\mu\)-quasi strong monotonicity they also allow \(\mu\leq 0\). In particular, if (14) holds with \(\mu=0\), then \(\bm{F}\) is called a _star-monotone_ operator. In Appendix G.1 we further discuss on star-monotone operators.

Meanwhile, let us further elaborate on why in the (star-)monotone setting, the assumptions made by the authors of [20] lead to each component being star-monotone. In their work the authors require, as equation (10) therein, that

\[\frac{1}{n}\sum_{i:\mu_{i}\geq 0}\mu_{i}+\frac{4}{n}\sum_{i:\mu_{i}<0}\mu_{i} \geq 0.\] (15)

Observe that this amounts to

\[\mu\coloneqq\frac{1}{n}\sum_{i=1}^{n}\mu_{i}\geq-\frac{3}{n}\sum_{i:\mu_{i}<0 }\mu_{i}=\frac{3}{n}\sum_{i:\mu_{i}<0}\left|\mu_{i}\right|.\] (16)

However, if any of \(\mu_{i}\) is strictly negative, then the rightmost sum in (16) becomes strictly positive, hence cannot be less than or equal to \(\mu\) if \(\mu=0\). Therefore, the only possible case is when the rightmost sum is an empty sum. In other words, (15) can hold with \(\mu=0\) only when \(\mu_{i}\geq 0\) for all \(i\), so that each \(\bm{F}_{i}\) is star-monotone. We would like to remind the readers that our analyses, on the other hand, do not have any restrictions on the individual components.

### Finite Sum Structure vs. General Stochastic Setting

The works mentioned in Section 2 usually assume that we have access to a stochastic oracle that returns a stochastic estimator of \(\bm{F}\). Indeed, having a finite sum structure is a special case of having a stochastic oracle, as each \(\bm{F}_{i}\) can be seen as an estimator of \(\bm{F}\). One might then ask whether assuming the finite sum structure can help the works mentioned in Section 2 overcome the mentioned limitations. We strongly believe that this is not the case. Recall Theorem 4.1, where we have constructed an explicit counterexample that SEG-US, SEG-RR, and SEG-FF all diverge. Because the set of problems with a finite sum structure is a subset of the set of problems with a stochastic oracle, the (counter-)example in Theorem 4.1 also works as an example that displays the nonconvergence of SEG-US, SEG-RR, and SEG-FF in the general stochastic setting. That is, a variant of SEG that only modifies the stepsizes and/or the sampling scheme into a without-replacement based one will suffer from nonconvergence, due to the counterexample in Theorem 4.1. It is also true that there are some methods that cannot exactly be classified as one of SEG-US, SEG-RR, or SEG-FF, but this counterexample demonstrates that, unless explicitly proven otherwise, there is not a good reason to believe that the existing convergence analyses will be easily extended beyond the assumptions they are each based on.

### On the Claimed Convergence of SEG-RR in the Monotone Setting by Emmanouilidis et al. [18]

Recently, a paper focusing on the study of SEG-RR[18] has been published. As we have briefly introduced in Table 1 with a discussion in Section 4, the authors have established a convergence rate \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{nK^{2}}})\) of SEG-RR in the strongly monotone setting, using an independent analysis of ours.

On the other hand, the authors of [18] furthermore claim that SEG-RR is capable of finding an optimum in the monotone setting, which is seemingly contradictory to our analyses. We assert that this is not the case, as their proof, at least in their AISTATS 2024 version, seems to have a flaw.

In establishing equation (85) in [18], the authors claim that the inequality

\[\frac{1}{K}\sum_{k=0}^{K}\frac{1}{G^{k}}\operatorname{\mathbb{E}}\left[\| \boldsymbol{F}(\boldsymbol{z}_{0}^{k})\|\right]\geq\operatorname{\mathbb{E}} \left[\left\|\boldsymbol{F}\left(\frac{1}{K}\sum_{k=0}^{K}\frac{1}{G^{k}} \boldsymbol{z}_{0}^{k}\right)\right\|^{2}\right]\]

holds by Jensen's inequality, where \(G\geq 6\) is a fixed constant. However, Jensen's inequality cannot be applied here, because not only \(\|\boldsymbol{F}(\cdot)\|^{2}\) is possibly nonconvex, but also the weights multiplied to the iterates, namely \(\nicefrac{{1}}{{k}}G^{k}\), do not sum up to \(1\). Hence, the "averaged" iterate is not in the form of a convex combination. So, even if \(\|\boldsymbol{F}(\cdot)\|^{2}\) was convex, if we were to properly apply Jensen's inequality, at least the averaged iterate should be multiplied by \(\frac{1}{\sum_{k=0}^{K}1/G^{k}}\) instead of \(\frac{1}{K}\). Yet then, the sum \(\sum_{k=0}^{K}\frac{1}{G^{k}}\leq\frac{G}{G-1}\) is bounded above by a constant independent of \(K\), and the right hand side of the equation right above (85) in [18] shall no longer be divided by \(K\). Therefore, their claimed convergence is unobtainable.

We would also like to remark that the linear decay rate of \(1/G^{k}\) can make the series \(\sum_{k=0}^{\infty}\frac{1}{G^{k}}\operatorname{\mathbb{E}}\|\boldsymbol{F}( \boldsymbol{z}_{0}^{k})\|^{2}\) convergent even when \(\operatorname{\mathbb{E}}[\|\boldsymbol{F}(\boldsymbol{z}_{0}^{k})\|^{2}]\) grows exponentially as \(k\to\infty\), as long as its rate of exponential growth is less than \(G\). In particular, once their (85) is corrected, there is no contradiction with our divergence result in Theorem 4.1.

## Appendix C Useful Lemmata

**Lemma C.1** (Polarization identity).: _For any two vectors \(\boldsymbol{a}\) and \(\boldsymbol{b}\), it holds that_

\[2\left\langle\boldsymbol{a},\boldsymbol{b}\right\rangle =\|\boldsymbol{a}\|^{2}+\|\boldsymbol{b}\|^{2}-\|\boldsymbol{a}- \boldsymbol{b}\|^{2}\] \[=\|\boldsymbol{a}+\boldsymbol{b}\|^{2}-\|\boldsymbol{a}\|^{2}-\| \boldsymbol{b}\|^{2}\,.\]

Proof.: The identities are immediate from \(\|\boldsymbol{a}\pm\boldsymbol{b}\|^{2}=\|\boldsymbol{a}\|^{2}\pm 2\left\langle \boldsymbol{a},\boldsymbol{b}\right\rangle+\|\boldsymbol{b}\|^{2}\). 

**Lemma C.2** (Weighted AM-GM inequality).: _For any \(\gamma>0\) and two vectors \(\boldsymbol{a}\) and \(\boldsymbol{b}\) in \(\mathbb{R}^{d}\),_

\[2\left|\left\langle\boldsymbol{a},\boldsymbol{b}\right\rangle\right|\leq\gamma \left\|\boldsymbol{a}\right\|^{2}+\frac{1}{\gamma}\left\|\boldsymbol{b}\right\| ^{2}.\]

Proof.: Notice that

\[2\left|\left\langle\boldsymbol{a},\boldsymbol{b}\right\rangle\right| \leq 2\left(|a_{1}b_{1}|+\cdots+|a_{d}b_{d}|\right)\] \[\leq\left(\gamma a_{1}^{2}+\frac{b_{1}^{2}}{\gamma}\right)+ \cdots+\left(\gamma a_{d}^{2}+\frac{b_{d}^{2}}{\gamma}\right)=\gamma\left\| \boldsymbol{a}\right\|^{2}+\frac{1}{\gamma}\left\|\boldsymbol{b}\right\|^{2}.\qed\]

**Lemma C.3** (Young's inequality).: _For any \(\gamma>0\) and two vectors \(\boldsymbol{a}\) and \(\boldsymbol{b}\),_

\[\left\|\boldsymbol{a}+\boldsymbol{b}\right\|^{2}\leq\left(1+\gamma\right) \left\|\boldsymbol{a}\right\|^{2}+\left(1+\frac{1}{\gamma}\right)\left\| \boldsymbol{b}\right\|^{2}.\] (17)

_In particular, as a special case where \(\gamma=1\), it holds that_

\[\left\|\boldsymbol{a}+\boldsymbol{b}\right\|^{2}\leq 2\left\|\boldsymbol{a} \right\|^{2}+2\left\|\boldsymbol{b}\right\|^{2}.\] (18)

Proof.: The left hand side of (17) is \(\left\|\boldsymbol{a}\right\|^{2}+2\left\langle\boldsymbol{a},\boldsymbol{b} \right\rangle+\left\|\boldsymbol{b}\right\|^{2}\). Applying Lemma C.2 then suffices.

**Lemma C.4**.: _For any two vectors \(\bm{a}\) and \(\bm{b}\), it holds that_

\[\left\|\bm{a}-\bm{b}\right\|^{2}\geq\frac{1}{2}\left\|\bm{a}\right\|^{2}-\left\| \bm{b}\right\|^{2}.\]

Proof.: From (18) it follows that

\[\left\|\bm{a}\right\|^{2}=\left\|\left(\bm{a}-\bm{b}\right)+\bm{b}\right\|^{2} \leq 2\left\|\bm{a}-\bm{b}\right\|^{2}+2\left\|\bm{b}\right\|^{2}.\]

Simply rearranging the terms gives us the result. 

**Lemma C.5** (Generalized Young's inequality).: _For any nonnegative scalars \(p_{1},\ldots,p_{n}\) such that \(p_{1}+\cdots+p_{n}=1\) and vectors \(\bm{a}_{1},\ldots,\bm{a}_{n}\), it holds that_

\[\left\|p_{1}\bm{a}_{1}+\cdots+p_{n}\bm{a}_{n}\right\|^{2}\leq p_{1}\left\|\bm {a}_{1}\right\|^{2}+\cdots+p_{n}\left\|\bm{a}_{n}\right\|^{2}.\]

_In particular, setting \(p_{1}=\cdots=p_{n}=\frac{1}{n}\) and multiplying both sides by \(n^{2}\) yields_

\[\left\|\bm{a}_{1}+\cdots+\bm{a}_{n}\right\|^{2}\leq n\left(\left\|\bm{a}_{1} \right\|^{2}+\cdots+\left\|\bm{a}_{n}\right\|^{2}\right).\]

Proof.: We use induction on \(n\). If \(n=1\) then \(p_{1}=1\), so there is nothing to show. For the inductive step, suppose that the statement holds for some \(n\geq 1\). Say we are given nonnegative scalars \(p_{1},\ldots,p_{n+1}\) such that \(p_{1}+\cdots+p_{n+1}=1\), and vectors \(\bm{a}_{1},\ldots,\bm{a}_{n+1}\). For the moment, suppose that \(p_{n+1}<1\). Applying Lemma C.3 with \(\gamma=\frac{p_{n+1}}{1-p_{n+1}}\) and using the induction hypothesis, we get

\[\left\|p_{1}\bm{a}_{1}+\cdots+p_{n}\bm{a}_{n}+p_{n+1}\bm{a}_{n+1} \right\|^{2}\] \[\qquad\leq\frac{1}{1-p_{n+1}}\left\|p_{1}\bm{a}_{1}+\cdots+p_{n} \bm{a}_{n}\right\|^{2}+\frac{1}{p_{n+1}}\left\|p_{n+1}\bm{a}_{n+1}\right\|^{2}\] \[\qquad=\left(1-p_{n+1}\right)\left\|\frac{p_{1}}{1-p_{n+1}}\bm{a} _{1}+\cdots+\frac{p_{n}}{1-p_{n+1}}\bm{a}_{n}\right\|^{2}+p_{n+1}\left\|\bm{a} _{n+1}\right\|^{2}\] \[\qquad\leq p_{1}\left\|\bm{a}_{1}\right\|^{2}+\cdots+p_{n}\left\| \bm{a}_{n}\right\|^{2}+p_{n+1}\left\|\bm{a}_{n+1}\right\|^{2}\]

where in last line we used that \(p_{1}+\cdots+p_{n}=1-p_{n+1}\). Now, if \(p_{n+1}=1\), then we must have \(p_{1}=\cdots=p_{n}=0\), so the claimed inequality holds in this case also. This completes the proof. 

**Lemma C.6**.: _Suppose that \(\bm{F}\) is \(M\)-smooth. Then for any \(\bm{z}\) and \(\bm{w}\) it holds that_

\[\left\|\bm{F}\bm{w}-\bm{F}\bm{z}-D\bm{F}(\bm{z})(\bm{w}-\bm{z})\right\|\leq \frac{M}{2}\left\|\bm{w}-\bm{z}\right\|^{2}.\]

Proof.: The proof closely follows the arguments used for Lemma 1.2.4 in [40], by replacing the gradients therein by saddle gradients. The fundamental theorem of calculus with the \(M\)-smoothness of \(\bm{F}\) gives us

\[\left\|\bm{F}\bm{w}-\bm{F}\bm{z}-D\bm{F}(\bm{z})(\bm{w}-\bm{z})\right\| =\left\|\int_{0}^{1}D\bm{F}(\bm{z}+t(\bm{w}-\bm{z}))\,\mathrm{d}t \left(\bm{w}-\bm{z}\right)-D\bm{F}(\bm{z})(\bm{w}-\bm{z})\right\|\] \[\leq\left\|\bm{w}-\bm{z}\right\|\int_{0}^{1}\left\|D\bm{F}(\bm{z} +t(\bm{w}-\bm{z}))-D\bm{F}(\bm{z})\right\|\,\mathrm{d}t\] \[\leq\left\|\bm{w}-\bm{z}\right\|\int_{0}^{1}Mt\left\|\bm{w}-\bm{z }\right\|\,\mathrm{d}t\] \[=\frac{M}{2}\left\|\bm{w}-\bm{z}\right\|^{2}.\qed\]

**Lemma C.7**.: _Let \(\bm{F}\) be a \(\mu\)-strongly monotone operator. Let \(\bm{z}^{*}\) be a point such that \(\bm{F}\bm{z}^{*}=\bm{0}\), and let \(\eta>0\). Then, for any point \(\bm{z}\) in the domain of \(\bm{F}\) and \(\bm{w}:=\bm{z}-\eta\bm{F}\bm{z}\), it holds that_

\[\left\langle\bm{F}\bm{w},\bm{w}-\bm{z}^{*}\right\rangle\geq\frac{\mu}{2}\left\| \bm{z}-\bm{z}^{*}\right\|^{2}-\eta^{2}\mu\left\|\bm{F}\bm{z}\right\|^{2}.\]Proof.: By the \(\mu\)-strong monotonicity of \(\bm{F}\) and Lemma C.4 it holds that

\[\left\langle\bm{F}\bm{w},\bm{w}-\bm{z}^{*}\right\rangle \geq\mu\left\|\bm{w}-\bm{z}^{*}\right\|^{2}\] \[=\mu\left\|\bm{z}-\eta\bm{F}\bm{z}-\bm{z}^{*}\right\|^{2}\] \[\geq\frac{\mu}{2}\left\|\bm{z}-\bm{z}^{*}\right\|^{2}-\mu\left\| \eta\bm{F}\bm{z}\right\|^{2}\]

so we are done. 

The following lemma generalizes Lemma 3.2 in [21] shown for monotone \(\bm{F}\) to \(\mu\)-strongly monotone \(\bm{F}\) with \(\mu>0\).

**Lemma C.8**.: _Let \(\bm{F}\) be a \(\mu\)-strongly monotone \(L\)-Lipschitz operator, and let \(\bm{z}\) be any point in the domain of \(\bm{F}\). Then for any \(0<\eta<\frac{1}{L\sqrt{2}}\), it holds that_

\[\left\|\bm{F}(\bm{z}-\eta\bm{F}(\bm{z}-\eta\bm{F}\bm{z}))\right\|^{2}\leq \left(1-\frac{2\eta\mu}{5}\right)\left\|\bm{F}\bm{z}\right\|^{2}.\]

Proof.: For convenience, let us define \(\bm{w}\coloneqq\bm{z}-\eta\bm{F}\bm{z}\) and \(\bm{z}^{+}\coloneqq\bm{z}-\eta\bm{F}(\bm{z}-\eta\bm{F}\bm{z})=\bm{z}-\eta\bm {F}\bm{w}\). Because \(\bm{F}\) is \(\mu\)-strongly monotone, we have

\[\mu\left\|\bm{z}^{+}-\bm{z}\right\|^{2} \leq\left\langle\bm{F}\bm{z}^{+}-\bm{F}\bm{z},\bm{z}^{+}-\bm{z}\right\rangle\] (19) \[=\eta\left\langle\bm{F}\bm{z}-\bm{F}\bm{z}^{+},\bm{F}\bm{w} \right\rangle.\]

Also from the \(\mu\)-strong monotonicity of \(\bm{F}\) we get

\[\mu\left\|\bm{w}-\bm{z}^{+}\right\|^{2} \leq\left\langle\bm{F}\bm{w}-\bm{F}\bm{z}^{+},\bm{w}-\bm{z}^{+}\right\rangle\] (20) \[=\eta\left\langle\bm{F}\bm{w}-\bm{F}\bm{z}^{+},\bm{F}\bm{w}-\bm{ F}\bm{z}\right\rangle.\]

Meanwhile, from the \(L\)-Lipschitzness of \(\bm{F}\) we have

\[\left\|\bm{F}\bm{w}-\bm{F}\bm{z}^{+}\right\|^{2}\leq\eta^{2}L^{2}\left\|\bm{ F}\bm{w}-\bm{F}\bm{z}\right\|^{2}.\] (21)

Summing up the inequalities (19), (20), (21) with weights \(\nicefrac{{2}}{{\eta}}\), \(\nicefrac{{1}}{{2\eta}}\), and \(\nicefrac{{3}}{{2}}\) respectively, we obtain

\[\frac{\mu}{\eta}\left(2\left\|\bm{z}^{+}-\bm{z}\right\|^{2}+\frac {1}{2}\left\|\bm{w}-\bm{z}^{+}\right\|^{2}\right)+\frac{3}{2}\left\|\bm{F}\bm {w}-\bm{F}\bm{z}^{+}\right\|^{2}\] \[\qquad\leq 2\left\langle\bm{F}\bm{z}-\bm{F}\bm{z}^{+},\bm{F}\bm{w} \right\rangle+\frac{1}{2}\left\langle\bm{F}\bm{w}-\bm{F}\bm{z}^{+},\bm{F}\bm{ w}-\bm{F}\bm{z}\right\rangle+\frac{3\eta^{2}L^{2}}{2}\left\|\bm{F}\bm{w}-\bm{F} \bm{z}\right\|^{2}.\]

From this inequality, we can exactly follow the arguments used in the proof of Lemma D.4 in [21] to derive that

\[\frac{\mu}{\eta}\left(2\left\|\bm{z}^{+}-\bm{z}\right\|^{2}+\frac{1}{2}\left\| \bm{w}-\bm{z}^{+}\right\|^{2}\right)+\left\|\bm{F}\bm{z}^{+}\right\|^{2}\leq \left\|\bm{F}\bm{z}\right\|^{2}.\] (22)

Meanwhile, Young's inequality (Lemma C.3) tells us that

\[\eta^{2}\left\|\bm{F}\bm{z}\right\|^{2}=\left\|\bm{w}-\bm{z}\right\|^{2}\leq \left(1+\frac{1}{4}\right)\left\|\bm{w}-\bm{z}^{+}\right\|^{2}+\left(1+4 \right)\left\|\bm{z}^{+}-\bm{z}\right\|^{2}.\]

Using this to lower bound the left hand side of (22), we get that

\[\frac{2\eta\mu}{5}\left\|\bm{F}\bm{z}\right\|^{2}+\left\|\bm{F}\bm{z}^{+} \right\|^{2}\leq\left\|\bm{F}\bm{z}\right\|^{2}.\]

It remains to simply rearrange the terms. 

**Lemma C.9**.: _Suppose that \(\bm{F}_{i}\) is \(L\)-Lipschitz for all \(i=1,\ldots,n\), and that \(\bm{F}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\bm{F}_{i}\) is \(\mu\)-strongly monotone with \(\mu>0\). Define \(\kappa\coloneqq\nicefrac{{L}}{{\mu}}\) and \(\sigma_{\star}^{2}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\left\|\bm{F}_{i}\bm{z}^{*} \right\|^{2}\). Then, for any \(\bm{z}\in\mathbb{R}^{d_{1}+d_{2}}\) it holds that_

\[\frac{1}{n}\sum_{i=1}^{n}\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2}\leq \left(\sqrt{3(1+\kappa^{2})}\left\|\bm{F}\bm{z}\right\|+\sqrt{3}\sigma_{\star} \right)^{2}.\]Proof.: For any \(\bm{z},\bm{w}\in\mathbb{R}^{d_{1}+d_{2}}\), as Assumption 3.1 holds with \(\mu>0\), by Cauchy-Schwarz inequality

\[\mu\left\|\bm{z}-\bm{w}\right\|^{2}\leq\left\langle\bm{F}\bm{z}-\bm{F}\bm{w}, \bm{z}-\bm{w}\right\rangle\leq\left\|\bm{F}\bm{z}-\bm{F}\bm{w}\right\|\left\| \bm{z}-\bm{w}\right\|,\]

and as a consequence, \(\left\|\bm{z}-\bm{w}\right\|\leq\nicefrac{{1}}{{\mu}}\left\|\bm{F}\bm{z}-\bm{ F}\bm{w}\right\|\). Thus, for any \(i\in[n]\), it holds that

\[\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2} \leq 3\left\|\bm{F}_{i}\bm{z}-\bm{F}_{i}\bm{z}^{*}\right\|^{2}+3 \left\|\bm{F}\bm{z}-\bm{F}\bm{z}^{*}\right\|^{2}+3\left\|\bm{F}_{i}\bm{z}^{* }-\bm{F}\bm{z}^{*}\right\|^{2}\] \[\leq 3L^{2}\left\|\bm{z}-\bm{z}^{*}\right\|^{2}+3\left\|\bm{F} \bm{z}-\bm{F}\bm{z}^{*}\right\|^{2}+3\left\|\bm{F}_{i}\bm{z}^{*}\right\|^{2}\] \[\leq 3\left(\frac{L^{2}}{\mu^{2}}+1\right)\left\|\bm{F}\bm{z}- \bm{F}\bm{z}^{*}\right\|^{2}+3\left\|\bm{F}_{i}\bm{z}^{*}\right\|^{2}\] \[=3(1+\kappa^{2})\left\|\bm{F}\bm{z}\right\|^{2}+3\left\|\bm{F}_{i }\bm{z}^{*}\right\|^{2}.\]

Summing this inequality over \(i=1,\ldots,n\) and then dividing by \(n\) leads to

\[\frac{1}{n}\sum_{i=1}^{n}\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2} \leq 3(1+\kappa^{2})\left\|\bm{F}\bm{z}\right\|^{2}+\frac{3}{n} \sum_{i=1}^{n}\left\|\bm{F}_{i}\bm{z}^{*}\right\|^{2}\] \[=3(1+\kappa^{2})\left\|\bm{F}\bm{z}\right\|^{2}+3\sigma_{*}^{2}.\]

The conclusion follows from the basic inequality \(a^{2}+b^{2}\leq(a+b)^{2}\) which holds for any \(a,b\geq 0\). 

**Lemma C.10** (Nonexpansiveness of the EG operator).: _Let \(\bm{F}\) be a monotone \(L\)-Lipschitz operator, and \(\bm{z}^{*}\) be a point such that \(\bm{F}\bm{z}^{*}=\bm{0}\). Then, for any point \(\bm{z}\) in the domain of \(\bm{F}\) and \(\eta>0\),_

\[\left\|\bm{z}-\eta\bm{F}(\bm{z}-\eta\bm{F}\bm{z})-\bm{z}^{*}\right\|^{2}\leq \left\|\bm{z}-\bm{z}^{*}\right\|^{2}-\eta^{2}(1-\eta^{2}L^{2})\left\|\bm{F} \bm{z}\right\|^{2}.\]

Proof.: This classical result dates back to the original paper on EG by Korpelevich [29]. Here, for completeness, we replicate the proof using our notations.

Expanding the left hand side of the inequality stated, we obtain

\[\left\|\bm{z}-\eta\bm{F}(\bm{z}-\eta\bm{F}\bm{z})-\bm{z}^{*}\right\|^{2} =\left\|\bm{z}-\bm{z}^{*}\right\|^{2}-2\left\langle\eta\bm{F}( \bm{z}-\eta\bm{F}\bm{z}),\bm{z}-\bm{z}^{*}\right\rangle+\left\|\eta\bm{F}( \bm{z}-\eta\bm{F}\bm{z})\right\|^{2}\] \[=\left\|\bm{z}-\bm{z}^{*}\right\|^{2}-2\eta\left\langle\bm{F}(\bm {z}-\eta\bm{F}\bm{z}),\bm{z}-\eta\bm{F}\bm{z}-\bm{z}^{*}\right\rangle\] \[\qquad\qquad\qquad-2\eta^{2}\left\langle\bm{F}(\bm{z}-\eta\bm{F} \bm{z}),\bm{F}\bm{z}\right\rangle+\eta^{2}\left\|\bm{F}(\bm{z}-\eta\bm{F}\bm{z })\right\|^{2}.\] (23)

For the first inner product term, we have

\[-2\eta\left\langle\bm{F}(\bm{z}-\eta\bm{F}\bm{z}),\bm{z}-\eta\bm{F}\bm{z}-\bm {z}^{*}\right\rangle\leq 0\]

because \(\bm{F}\) is monotone. For the second inner product term, we use the polarization identity (Lemma C.1) and the \(L\)-Lipschitzness of \(\bm{F}\) to get

\[-2\left\langle\bm{F}(\bm{z}-\eta\bm{F}\bm{z}),\bm{F}\bm{z}\right\rangle =\left\|\bm{F}(\bm{z}-\eta\bm{F}\bm{z})-\bm{F}\bm{z}\right\|^{2} -\left\|\bm{F}(\bm{z}-\eta\bm{F}\bm{z})\right\|^{2}-\left\|\bm{F}\bm{z}\right\|^ {2}\] \[\leq L^{2}\left\|-\eta\bm{F}\bm{z}\right\|^{2}-\left\|\bm{F}(\bm{z} -\eta\bm{F}\bm{z})\right\|^{2}-\left\|\bm{F}\bm{z}\right\|^{2}\] \[=-(1-\eta^{2}L^{2})\left\|\bm{F}\bm{z}\right\|^{2}-\left\|\bm{F}( \bm{z}-\eta\bm{F}\bm{z})\right\|^{2}.\]

Applying these two bounds on (23) completes the proof. 

**Lemma C.11**.: _Let \(\{a_{k}\}_{k\geq 0}\), \(\{b_{k}\}_{k\geq 0}\), \(\{c_{k}\}_{k\geq 0}\), and \(\{d_{k}\}_{k\geq 0}\) be sequences of nonnegative numbers satisfying the recurrence relation_

\[b_{k}\leq(1+a_{k})d_{k}-d_{k+1}+c_{k}\qquad\forall k\geq 0.\]

_Then for any \(k\geq 0\) it holds that_

\[d_{k+1}+\sum_{j=0}^{k}b_{j}\leq\left(\prod_{j=0}^{k}(1+a_{j})\right)\left(d_{0 }+\sum_{j=0}^{k}c_{j}\right).\]Proof.: Because \(a_{k}\geq 0\), it suffices to show that

\[\sum_{j=0}^{k}(b_{j}-c_{j})\prod_{i=j+1}^{k}(1+a_{i})\leq-d_{k+1}+d_{0}\prod_{j=0 }^{k}(1+a_{j}),\] (24)

as this implies

\[\sum_{j=0}^{k}b_{j} \leq\sum_{j=0}^{k}b_{j}\prod_{i=j+1}^{k}(1+a_{i})\] \[\leq\left(\sum_{j=0}^{k}c_{j}\prod_{i=j+1}^{k}(1+a_{i})\right)-d_ {k+1}+d_{0}\prod_{j=0}^{k}(1+a_{j})\] \[\leq-d_{k+1}+\left(d_{0}+\sum_{j=0}^{k}c_{k}\right)\prod_{j=0}^{k }(1+a_{j}).\]

So, we show that (24) holds, by induction on \(k\). For the base case \(k=0\), the recurrence relation tells us that

\[b_{0}-c_{0}\leq(1+a_{0})d_{0}-d_{1}\]

which is exactly (24) when \(k=0\). Now suppose that (24) holds for some \(k\geq 0\). Using the induction hypothesis and the recurrence relation we get

\[\sum_{j=0}^{k+1}(b_{j}-c_{j})\prod_{i=j+1}^{k+1}(1+a_{i}) =b_{k+1}-c_{k+1}+(1+a_{k+1})\left(\sum_{j=0}^{k}(b_{j}-c_{j})\prod _{i=j+1}^{k}(1+a_{i})\right)\] \[\leq b_{k+1}-c_{k+1}-(1+a_{k+1})d_{k+1}+d_{0}\prod_{j=0}^{k+1}(1+a _{j})\] \[\leq-d_{k+2}+d_{0}\prod_{j=0}^{k+1}(1+a_{j}).\]

This shows that (24) holds also for \(k+1\), and we are done. 

The subsequent lemma is technical, but it can be derived from elementary calculus.

**Lemma C.12**.: _For any \(K\geq 1\),_

\[\sum_{k=2}^{K+2}\frac{1}{k^{2/3}(\log k)^{2}}\geq\frac{(K+3)^{1/3}}{(\log(K+3 ))^{2}}.\]

Proof.: Consider the function \(h(x)\coloneqq\frac{1}{x^{2/3}(\log x)^{2}}\) over the interval \([2,K+3]\). As

\[h^{\prime}(x)=-\frac{2}{x^{5/3}(\log x)^{3}}-\frac{2}{3x^{5/3}(\log x)^{2}}<0,\]

\(h\) is decreasing. Hence, an upper Riemann sum becomes an upper bound for the integral, so we have

\[\sum_{k=2}^{K+2}\frac{1}{k^{2/3}(\log k)^{2}}\geq\int_{2}^{K+3}\frac{1}{x^{2/3 }(\log x)^{2}}\,\mathrm{d}x.\] (25)

Now consider a function \(g:[1,\infty)\to\mathbb{R}\), defined as

\[g(y)\coloneqq\int_{2}^{y+3}\frac{1}{x^{2/3}(\log x)^{2}}\,\mathrm{d}x-\frac{( y+3)^{1/3}}{(\log(y+3))^{2}}.\]

Differentiating, we get

\[g^{\prime}(y)=\frac{2}{(y+3)^{2/3}(\log(y+3))^{3}}+\frac{2}{3(y+3)^{2/3}(\log( y+3))^{2}}>0\]whenever \(y\geq 1\). That is, \(g\) is increasing on \(y\geq 1\). We then show that \(g(1)\geq 0\). To this end, let us begin with observing that

\[h^{\prime\prime}(x)=\frac{6}{x^{8/3}(\log x)^{4}}+\frac{14}{3x^{8/3}(\log x)^{3} }+\frac{10}{9x^{8/3}(\log x)^{2}}>0,\]

from which we get that \(h\) is convex. In particular, it holds that

\[\int_{2}^{3}h(x)\,\mathrm{d}x\geq\int_{2}^{3}h^{\prime}\left(\frac{5}{2} \right)\left(x-\frac{5}{2}\right)+h\left(\frac{5}{2}\right)\,\mathrm{d}x=h \left(\frac{5}{2}\right),\]

and similarly, \(\int_{3}^{4}h(x)\,\mathrm{d}x\geq h(\nicefrac{{7}}{{2}})\). Thus we indeed have

\[g(1) =\int_{2}^{4}h(x)\,\mathrm{d}x-\frac{4^{1/3}}{(\log 4)^{2}}\] \[\geq\frac{1}{(5/2)^{2/3}(\log(5/2))^{2}}+\frac{1}{(7/2)^{2/3}( \log(7/2))^{2}}-\frac{4^{1/3}}{(\log 4)^{2}}\ \geq 0.\]

Recalling that \(g\) is increasing, we have \(g(K)\geq g(1)\geq 0\) for all \(K\geq 1\). This, with (25), implies that

\[\sum_{k=2}^{K+2}\frac{1}{k^{2/3}(\log k)^{2}}\geq\int_{2}^{K+3}\frac{1}{x^{2/3 }(\log x)^{2}}\,\mathrm{d}x\geq\frac{(K+3)^{1/3}}{(\log(K+3))^{2}}\]

holds whenever \(K\geq 1\), which is exactly the claimed. 

## Appendix D Missing Proofs for Section 5

### Unravelling the Recurrence of the Generalized SEG in (8) and (12)

In Section 5.1, we considered the method where, in a single epoch (hence omitting all superscripts that are used to denote the epoch number for convenience), the iterates are generated following the recurrence

\[\bm{w}_{i} =\bm{z}_{i}-\alpha\bm{T}_{i}\bm{z}_{i}\] (26) \[\bm{z}_{i+1} =\bm{z}_{i}-\beta\bm{T}_{i}\bm{w}_{i}\]

for \(i=0,1,\ldots,N-1\), where each \(\bm{T}_{i}\) are sampled from the set \(\{\bm{F}_{1},\ldots,\bm{F}_{n}\}\), and an additional anchoring step

\[\bm{z}^{\sharp}\coloneqq\frac{\bm{z}_{N}+\theta\bm{z}_{0}}{1+\theta}\] (27)

is performed so that \(\bm{z}^{\sharp}\) is used as the initial point of the next epoch. Notice that (27) is a generalized anchoring step that incorporates all the settings we are considering, as the versions of SEG where anchoring is not used correspond to taking \(\theta=0\), and the anchoring step (12) that is used in SEG-FFA corresponds to taking \(\theta=1\). In this section we would like to prove the following statement regarding this update rule.

**Proposition D.1** (Proposition 5.1).: _It holds that_

\[\bm{z}^{\sharp}=\bm{z}_{0}-\frac{\beta}{1+\theta}\sum_{j=0}^{N-1}\bm{T}_{j} \bm{z}_{0}+\frac{\alpha\beta}{1+\theta}\sum_{j=0}^{N-1}D\bm{T}_{j}(\bm{z}_{0}) \bm{T}_{j}\bm{z}_{0}+\frac{\beta^{2}}{1+\theta}\sum_{0\leq i<j\leq N-1}D\bm{T }_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}+\frac{\bm{\epsilon}_{N}}{1+\theta}\] (28)

_for some \(\bm{\epsilon}_{N}=o\left((\alpha+\beta)^{2}\right)\)._

Proof.: Equation (28) immediately follows from Proposition D.2, with (30) giving us the precise definition of \(\bm{\epsilon}_{N}\). To show that \(\bm{\epsilon}_{N}=o\left((\alpha+\beta)^{2}\right)\), we begin with noting that both \(\|\bm{z}_{j}-\bm{z}_{0}\|\) and \(\|\bm{w}_{j}-\bm{z}_{0}\|\) are of \(\mathcal{O}(\alpha+\beta)\), because both \(\bm{z}_{j}\) and \(\bm{w}_{j}\) are obtained from \(\bm{z}_{0}\) by performing at most \(j\) updates following (26). Thus, the first term in the right hand side of (30) is of \(\mathcal{O}(\beta(\alpha+\beta)^{2})\) by Lemma C.6, and the remaining terms are of \(\mathcal{O}((\alpha+\beta)^{3})\) by the \(L\)-smoothness of the operators \(\bm{F}_{1},\ldots,\bm{F}_{n}\).

**Proposition D.2**.: _For any \(i=0,1,\ldots,N\), it holds that_

\[\bm{z}_{i}=\bm{z}_{0}-\beta\sum_{j=0}^{i-1}\bm{T}_{j}\bm{z}_{0}+\alpha\beta\sum_{ j=0}^{i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+\beta^{2}\sum_{0\leq k<j \leq i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{k}\bm{z}_{0}+\bm{\epsilon}_{i}\] (29)

_where we denote_

\[\begin{split}\bm{\epsilon}_{i}\coloneqq&-\beta \sum_{j=0}^{i-1}\Bigl{(}\bm{T}_{j}\bm{w}_{j}-\bm{T}_{j}\bm{z}_{0}-D\bm{T}_{j}( \bm{z}_{0})(\bm{w}_{j}-\bm{z}_{0})\Bigr{)}\\ &+\alpha\beta\sum_{j=0}^{i-1}D\bm{T}_{j}(\bm{z}_{0})(\bm{T}_{j} \bm{z}_{j}-\bm{T}_{j}\bm{z}_{0})+\beta^{2}\sum_{j=0}^{i-1}D\bm{T}_{j}(\bm{z}_{ 0})\sum_{k=0}^{j-1}(\bm{T}_{k}\bm{w}_{k}-\bm{T}_{k}\bm{z}_{0}).\end{split}\] (30)

Proof.: We use induction on \(i\). There is nothing to show for the base case \(i=0\). Now, suppose that (29) and (30) hold for some \(i<N\), and write

\[\bm{z}_{i+1} =\bm{z}_{i}-\beta\bm{T}_{i}\bm{w}_{i}\] \[=\bm{z}_{i}-\beta\bm{T}_{i}\bm{z}_{0}-\beta D\bm{T}_{i}(\bm{z}_{ 0})(\bm{w}_{i}-\bm{z}_{0})-\beta\Bigl{(}\bm{T}_{i}\bm{w}_{i}-\bm{T}_{i}\bm{z}_ {0}-D\bm{T}_{i}(\bm{z}_{0})(\bm{w}_{i}-\bm{z}_{0})\Bigr{)}.\]

Here, notice that by the update rule we have

\[\bm{w}_{i} =\bm{z}_{i}-\alpha\bm{T}_{i}\bm{z}_{i}\] \[=\bm{z}_{0}-\beta\sum_{j=0}^{i-1}\bm{T}_{j}\bm{w}_{j}-\alpha\bm{T }_{i}\bm{z}_{i}.\]

Using this identity and the induction hypothesis we get

\[\bm{z}_{i+1}=\bm{z}_{0}-\beta\sum_{j=0}^{i-1}\bm{T}_{j}\bm{z}_{0} +\alpha\beta\sum_{j=0}^{i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+\beta ^{2}\sum_{0\leq k<j\leq i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{k}\bm{z}_{0}+\bm{ \epsilon}_{i}\] \[\qquad-\beta\bm{T}_{i}\bm{z}_{0}-\beta D\bm{T}_{i}(\bm{z}_{0}) \left(-\beta\sum_{j=0}^{i-1}\bm{T}_{j}\bm{w}_{j}-\alpha\bm{T}_{i}\bm{z}_{i}\right)\] \[\qquad-\beta\Bigl{(}\bm{T}_{i}\bm{w}_{i}-\bm{T}_{i}\bm{z}_{0}-D \bm{T}_{i}(\bm{z}_{0})(\bm{w}_{i}-\bm{z}_{0})\Bigr{)}\] \[=\bm{z}_{0}-\beta\sum_{j=0}^{i-1}\bm{T}_{j}\bm{z}_{0}+\alpha\beta \sum_{j=0}^{i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+\beta^{2}\sum_{0 \leq k<j\leq i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{k}\bm{z}_{0}+\bm{\epsilon}_{i}\] \[\qquad-\beta\bm{T}_{i}\bm{z}_{0}+\beta^{2}D\bm{T}_{i}(\bm{z}_{0}) \sum_{j=0}^{i-1}\bm{T}_{j}\bm{z}_{0}+\beta^{2}D\bm{T}_{i}(\bm{z}_{0})\sum_{j =0}^{i-1}(\bm{T}_{j}\bm{w}_{j}-\bm{T}_{j}\bm{z}_{0})\] \[\qquad+\alpha\beta D\bm{T}_{i}(\bm{z}_{0})(\bm{T}_{i}\bm{z}_{i}- \bm{T}_{i}\bm{z}_{0})+\alpha\beta D\bm{T}_{i}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\] \[\qquad-\beta\Bigl{(}\bm{T}_{i}\bm{w}_{i}-\bm{T}_{i}\bm{z}_{0}-D \bm{T}_{i}(\bm{z}_{0})(\bm{w}_{i}-\bm{z}_{0})\Bigr{)}\] \[=\bm{z}_{0}-\beta\sum_{j=0}^{i}\bm{T}_{j}\bm{z}_{0}+\alpha\beta \sum_{j=0}^{i}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+\beta^{2}\sum_{0 \leq k<j\leq i}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{k}\bm{z}_{0}+\bm{\epsilon}_{i}\] \[\qquad+\beta^{2}D\bm{T}_{i}(\bm{z}_{0})\sum_{j=0}^{i-1}(\bm{T}_{j }\bm{w}_{j}-\bm{T}_{j}\bm{z}_{0})+\alpha\beta D\bm{T}_{i}(\bm{z}_{0})(\bm{T}_{ i}\bm{z}_{i}-\bm{T}_{i}\bm{z}_{0})\] \[\qquad-\beta\Bigl{(}\bm{T}_{i}\bm{w}_{i}-\bm{T}_{i}\bm{z}_{0}-D \bm{T}_{i}(\bm{z}_{0})(\bm{w}_{i}-\bm{z}_{0})\Bigr{)}\] \[=\bm{z}_{0}-\beta\sum_{j=0}^{i}\bm{T}_{j}\bm{z}_{0}+\alpha\beta \sum_{j=0}^{i}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+\beta^{2}\sum_{0 \leq k<j\leq i}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{k}\bm{z}_{0}+\bm{\epsilon}_{i+1}\]

which asserts that (29) also holds for \(i+1\).

### Insufficiency of Only Using Flip-Flop Sampling

Here we prove the following.

**Proposition D.3** (Proposition 5.2).: _Suppose we use flip-flop sampling only. In order to make (10) and (11) hold, we must choose \(\beta=\nicefrac{{n}}{{n}}\) and \(\alpha=\nicefrac{{\beta}}{{2}}\). However, this leads to \(\eta_{2}=2\eta_{1}\), which is the set of parameters that fails to make EG+ converge._

Proof.: Suppose that we have already established the upcoming Lemma D.4. Then, we can see by setting \(\theta=0\) in the result of Lemma D.4 that for (11) to hold, the following system of equations should be satisfied:

\[\begin{cases}\eta_{1}\eta_{2}=2n^{2}\beta^{2},\\ \eta_{1}\eta_{2}=n^{2}(2\alpha\beta+\beta^{2}),\\ \eta_{2}=2n\beta.\end{cases}\]

Solving this system of equations, we get \(\eta_{1}=n\beta\), \(\eta_{2}=2n\beta\), and \(\alpha=\nicefrac{{\beta}}{{2}}\).

For the latter part of the statement on the divergence of EG+ with \(\eta_{2}=2\eta_{1}\), consider the \((1+1)\)-dimensional bilinear problem

\[\min_{x}\max_{y}\ xy\]

whose unique optimum is \(\bm{z}^{\star}=(0,0)\). A simple computation shows that

\[\bm{F}\bm{z}=\begin{bmatrix}0&1\\ -1&0\end{bmatrix}\bm{z}.\]

Consequently, for any \(\eta>0\), the update rule of EG+ with \(\eta_{1}=\eta\) and \(\eta_{2}=2\eta\) amounts to

\[\bm{z}^{+}=\bm{z}-2\eta\bm{F}(\bm{z}-\eta\bm{F}\bm{z})=\begin{bmatrix}1-2\eta^ {2}&-2\eta\\ 2\eta&1-2\eta^{2}\end{bmatrix}\bm{z}.\]

It follows that

\[\left\|\bm{z}^{+}-\bm{z}^{\star}\right\|^{2} =\left\|\begin{bmatrix}1-2\eta^{2}&-2\eta\\ 2\eta&1-2\eta^{2}\end{bmatrix}\begin{bmatrix}x\\ y\end{bmatrix}\right\|^{2}\] \[=\left((1-2\eta^{2})x-2\eta y\right)^{2}+\left(2\eta x+(1-2\eta^ {2})y\right)^{2}\] \[=(1+4\eta^{4})(x^{2}+y^{2})\] \[=(1+4\eta^{4})\left\|\bm{z}-\bm{z}^{\star}\right\|^{2}.\]

Therefore, the distance from the optimal solution strictly increases every iterate. 

It remains to actually prove Lemma D.4.

**Lemma D.4**.: _When flip-flop sampling is used with the generalized anchoring step (27), it holds that_

\[\frac{\alpha\beta}{1+\theta}\sum_{j=0}^{N-1}D\bm{T}_{j}(\bm{z}_{0} )\bm{T}_{j}\bm{z}_{0}+\frac{\beta^{2}}{1+\theta}\sum_{0\leq i<j\leq N-1}D\bm{ T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\] \[\qquad=\frac{2\alpha\beta+\beta^{2}}{1+\theta}\sum_{j=1}^{n}D\bm {F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\frac{2\beta^{2}}{1+\theta}\sum_{i\neq j }D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i}\bm{z}_{0}.\]

Proof.: As we are using flip-flop sampling, we have \(N=2n\), and it is clear that

\[\sum_{j=0}^{N-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}=2\sum_{j=1}^{n}D\bm {F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}.\]

For the second term, as \(\bm{T}_{i}=\bm{T}_{2n-1-i}\), we have

\[\sum_{0\leq i<j\leq 2n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}=\sum_{0 \leq i<j\leq n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}+\sum_{n\leq i<j \leq 2n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\]\[+\sum_{i=0}^{n-1}\sum_{j=i+1}^{n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}+ \sum_{i=0}^{n-1}\sum_{j=0}^{n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\] \[+\sum_{i=0}^{n-1}D\bm{T}_{2n-1-i}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\] \[=\sum_{0\leq i<j\leq n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{ 0}+\sum_{0\leq j<i\leq n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\] \[+\sum_{i=0}^{n-1}\sum_{j=i+1}^{n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_ {i}\bm{z}_{0}+\sum_{i=0}^{n-1}\sum_{j=0}^{i-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i }\bm{z}_{0}\] \[=2\sum_{0\leq i<j\leq n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_ {0}+2\sum_{0\leq j<i\leq n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}\] \[\quad+\sum_{i=0}^{n-1}D\bm{T}_{i}(\bm{z}_{0})\bm{T}_{i}\bm{z}_{0}.\]

The claimed identity can be obtained by taking the weighted sum of the two results. 

## Appendix E Within-Epoch Error Analysis for Upper Bounds

All the upper bounds for SEG-RR, SEG-FF, and SEG-FFA in this paper are established by following the two steps below.

The first step is to decompose the cumulative updates made within an epoch by using the method into a sum of an exact EG update and a within-epoch error term, which we denote by \(\bm{r}^{k}\). In particular, we show that the error term \(\bm{r}^{k}\) occurring from any of SEG-RR, SEG-FF, and SEG-FFA can be expressed in a specific unified form (described in Theorem E.1). This will be the main focus of this section.

The second step is establishing a convergence rate that can be applied to _any_ method whose update can be decomposed into a sum of an exact EG update and an error term that is of the specific unified form mentioned above. By doing so, the convergence rates of SEG-RR, SEG-FF, and SEG-FFA will automatically follow as special cases of the general convergence result. This step will be dealt in Appendices F and G.

To this end, for any of SEG-RR, SEG-FF, and SEG-FFA, let us decompose the cumulative updates made within an epoch into a sum of an exact EG update and a within-epoch error term \(\bm{r}^{k}\), as

\[\bm{z}_{0}^{k+1}=\bm{z}_{0}^{k}-\eta_{k}n\bm{F}(\bm{z}_{0}^{k}-\eta_{k}n\bm{F} \bm{z}_{0}^{k})+\bm{r}^{k}.\]

The quality of the method will depend on how small the "noise" term \(\bm{r}^{k}\) is, as the noise will in general hinder the convergence. As mentioned above, it turns out that, regardless of the method that is in use, the noise term can be bounded in a unified format, as follows.

**Theorem E.1**.: _Suppose that Assumptions 3.3 and 3.4 hold. Then, for each of SEG-RR, SEG-FF, and SEG-FFA, there exists a choice of stepsizes that makes the following hold: for an exponent \(a\) that depends on the method, there exist constants \(C_{\dagger}\), \(D_{\dagger}\), \(V_{\dagger}\), \(C_{2}\), \(D_{\bm{2}}\), and \(V_{\bm{2}}\), all independent of \(\eta_{k}\) and \(n\), such that the error term \(\bm{r}^{k}\) satisfies a deterministic bound_

\[\left\|\bm{r}^{k}\right\|\leq\eta_{k}^{a}n^{a}C_{\dagger}\left\|\bm{F}\bm{z}_{ 0}^{k}\right\|+\eta_{k}^{a}n^{a}D_{\dagger}\left\|\bm{F}\bm{z}_{0}^{k}\right\|^ {2}+\eta_{k}^{a}n^{a}V_{\dagger}\] (31)

_and a bound that holds on expectation_

\[\mathbb{E}\left[\left\|\bm{r}\right\|^{2}\middle|\bm{z}_{0}^{k}\right]\leq\eta _{k}^{2a}n^{2a}C_{\dagger}\left\|\bm{F}\bm{z}_{0}^{k}\right\|^{2}+\eta_{k}^{2a }n^{2a}D_{\dagger}\left\|\bm{F}\bm{z}_{0}^{k}\right\|^{4}+\eta_{k}^{2a}n^{2a-1 }V_{\bm{2}}.\] (32)

_Furthermore, the exponent is \(a=2\) for SEG-RR and SEG-FF, and \(a=3\) for SEG-FFA._In other words, SEG-FFA has an error that is an order of magnitude smaller than other methods. Thus, it is now intuitively clear that SEG-FFA should have an advantage in the convergence. The proof of Theorem E.1 is quite long and technical, so we defer it to Appendix E.2.

Within the remaining of this section only, although it is an abuse of notation, for convenience we will write \(\bm{F}_{i}\) to denote the saddle gradient of the component function chosen in the \(i^{\text{th}}\) iteration. More precisely, for indices \(i=0,1,\ldots,n-1\) we denote \(\bm{F}_{\tau(i+1)}\) by \(\bm{F}_{i}\). Similarly, in cases of considering SEG-FF or SEG-FFA, for \(i\geq n\) we denote \(\bm{F}_{\tau(2n-i)}\) by \(\bm{F}_{i}\). Also, we omit the superscripts and subscripts denoting the epoch number \(k\) unless strictly necessary, as all the iterates that we consider will be from the same epoch.

Let us reformulate the update rule (8) into

\[\bm{w}_{i} =\bm{z}_{i}-\xi\eta\bm{F}_{i}\bm{z}_{i},\] (33) \[\bm{z}_{i+1} =\bm{z}_{i}-\eta\bm{F}_{i}\bm{w}_{i}.\]

Note that \(\xi=\nicefrac{{1}}{{2}}\) for SEG-FFA, and \(\xi=1\) for SEG-RR and SEG-FF.

### Auxiliary Lemmata

For \(j=1,\ldots,2n\) we define

\[\bm{g}_{j} \coloneqq\sum_{i=0}^{j-1}\bm{F}_{i}\bm{z}_{0},\] (34) \[\delta_{j} \coloneqq\left\lVert\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\rVert,\] (35) \[\Sigma_{j} \coloneqq\sum_{i=1}^{j}\delta_{i},\] (36) \[\Psi_{j} \coloneqq\sum_{i=1}^{j}\delta_{i}^{2}.\] (37)

We set \(\Sigma_{0}=\Psi_{0}=0\), as they are empty sums. Notice that \(\delta_{j}\) is a random variable that depends on the permutation \(\tau\).

Meanwhile, by triangle inequality it is immediate that

\[\left\lVert\bm{g}_{j}\right\rVert\leq j\left\lVert\bm{F}\bm{z}_{0}\right\rVert +\delta_{j},\]

and by Young's inequality it holds that

\[\left\lVert\bm{g}_{j}\right\rVert^{2}\leq 2j^{2}\left\lVert\bm{F}\bm{z}_{0} \right\rVert^{2}+2\delta_{j}^{2}.\]

**Lemma E.2**.: _For any index \(i\geq 1\), it holds that_

\[\left\lVert\bm{z}_{i}-\bm{z}_{0}\right\rVert \leq\eta\left(1+\xi\eta L\right)\left\lVert\bm{g}_{i}\right\rVert\] (38) \[\qquad+\eta^{2}L\left(2\xi+2\xi\eta L+\xi^{2}\eta^{2}L^{2}\right) \sum_{\ell=0}^{i-2}\left(1+\eta L+\xi\eta^{2}L^{2}\right)^{i-\ell-2}\left\lVert \bm{g}_{\ell+1}\right\rVert,\] \[\left\lVert\bm{w}_{i}-\bm{z}_{0}\right\rVert \leq\xi\eta\left\lVert\bm{g}_{i+1}\right\rVert+\xi\eta\left((1- \xi^{-1})+2\eta L+\xi\eta^{2}L^{2}\right)\left\lVert\bm{g}_{i}\right\rVert\] \[\qquad+\eta(1+\xi\eta L)\left(2\xi\eta L+2\xi\eta^{2}L^{2}+\xi^{2 }\eta^{3}L^{3}\right)\sum_{\ell=0}^{i-2}\left(1+\eta L+\xi\eta^{2}L^{2}\right) ^{i-\ell-2}\left\lVert\bm{g}_{\ell+1}\right\rVert.\]

Proof.: By the fundamental theorem of calculus for line integrals and the update rule (33), we have

\[\bm{w}_{i} =\bm{z}_{i}-\xi\eta\bm{F}_{i}\bm{z}_{i}\] \[=\bm{z}_{i}-\xi\eta\bm{F}_{i}\bm{z}_{0}-\xi\eta(\bm{F}_{i}\bm{z}_ {i}-\bm{F}_{i}\bm{z}_{0})\] \[=\bm{z}_{i}-\xi\eta\bm{F}_{i}\bm{z}_{0}-\xi\eta\int_{0}^{1}D\bm{F} _{i}(\bm{z}_{0}+t(\bm{z}_{i}-\bm{z}_{0}))\,\mathrm{d}t\left(\bm{z}_{i}-\bm{z}_{ 0}\right)\]and similarly

\[\begin{split}\bm{z}_{i+1}&=\bm{z}_{i}-\eta\bm{F}_{i} \bm{w}_{i}\\ &=\bm{z}_{i}-\eta\bm{F}_{i}\bm{z}_{0}-\eta(\bm{F}_{i}\bm{w}_{i}- \bm{F}_{i}\bm{z}_{0})\\ &=\bm{z}_{i}-\eta\bm{F}_{i}\bm{z}_{0}-\eta\int_{0}^{1}D\bm{F}_{i }(\bm{z}_{0}+t(\bm{w}_{i}-\bm{z}_{0}))\,\mathrm{d}t\left(\bm{w}_{i}-\bm{z}_{0 }\right).\end{split}\] (40)

Hence, by defining

\[\begin{split}\bm{A}_{i}&\coloneqq\int_{0}^{1}D\bm{F} _{i}(\bm{z}_{0}+t(\bm{z}_{i}-\bm{z}_{0}))\,\mathrm{d}t\\ \bm{B}_{i}&\coloneqq\int_{0}^{1}D\bm{F}_{i}(\bm{z}_ {0}+t(\bm{w}_{i}-\bm{z}_{0}))\,\mathrm{d}t\end{split}\] (41)

the update rule can be rewritten using these quantities as

\[\bm{w}_{i} =\bm{z}_{i}-\xi\eta\bm{F}_{i}\bm{z}_{0}-\xi\eta\bm{A}_{i}(\bm{z} _{i}-\bm{z}_{0}),\] (42) \[\bm{z}_{i+1} =\bm{z}_{i}-\eta\bm{F}_{i}\bm{z}_{0}-\eta\bm{B}_{i}(\bm{w}_{i}- \bm{z}_{0}).\] (43)

Subtracting \(\bm{z}_{0}\) from both sides of (42) we get

\[\begin{split}\bm{w}_{i}-\bm{z}_{0}&=\bm{z}_{i}-\bm {z}_{0}-\xi\eta\bm{F}_{i}\bm{z}_{0}-\xi\eta\bm{A}_{i}(\bm{z}_{i}-\bm{z}_{0})\\ &=\left(\bm{I}-\xi\eta\bm{A}_{i}\right)(\bm{z}_{i}-\bm{z}_{0})- \xi\eta\bm{F}_{i}\bm{z}_{0},\end{split}\] (44)

and plugging this into (43) gives us

\[\begin{split}\bm{z}_{i+1}-\bm{z}_{0}&=\bm{z}_{i}- \bm{z}_{0}-\eta\bm{F}_{i}\bm{z}_{0}-\eta\bm{B}_{i}(\bm{w}_{i}-\bm{z}_{0})\\ &=\bm{z}_{i}-\bm{z}_{0}-\eta\bm{F}_{i}\bm{z}_{0}-\eta\bm{B}_{i} \left(\left(\bm{I}-\xi\eta\bm{A}_{i}\right)(\bm{z}_{i}-\bm{z}_{0})-\xi\eta\bm{ F}_{i}\bm{z}_{0}\right)\\ &=\left(\bm{I}-\eta\bm{B}_{i}+\xi\eta^{2}\bm{B}_{i}\bm{A}_{i} \right)(\bm{z}_{i}-\bm{z}_{0})-\eta\left(\bm{I}-\xi\eta\bm{B}_{i}\right)\bm{F} _{i}\bm{z}_{0}.\end{split}\] (45)

For convenience let us define

\[\begin{split}\bm{C}_{i}&\coloneqq\bm{I}-\eta\bm{B}_ {i}+\xi\eta^{2}\bm{B}_{i}\bm{A}_{i},\\ \bm{P}_{i,\ell}&\coloneqq\bm{C}_{i}\bm{C}_{i-1}\ldots \bm{C}_{\ell+2}\bm{C}_{\ell+1}\end{split}\]

and \(\bm{P}_{i,\,i}\coloneqq\bm{I}\) as it denotes an empty product. Observe that for any \(j\) we have

\[\left\|\bm{C}_{j}\right\|=\left\|\bm{I}-\eta\bm{B}_{i}+\xi\eta^{2}\bm{B}_{i} \bm{A}_{i}\right\|\leq 1+\eta L+\xi\eta^{2}L^{2}.\] (46)

Also note that for any \(\ell\) it holds that

\[\begin{split}(\bm{I}-\xi\eta\bm{B}_{\ell+1})-\bm{C}_{\ell+1} \left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\\ &\quad=\left(\bm{I}-\xi\eta\bm{B}_{\ell+1}\right)-\left(\bm{I}- \eta\bm{B}_{\ell+1}+\xi\eta^{2}\bm{B}_{\ell+1}\bm{A}_{\ell+1}\right)\left(\bm {I}-\xi\eta\bm{B}_{\ell}\right)\\ &\quad=\xi\eta(\bm{B}_{\ell+1}+\bm{B}_{\ell})-\xi\eta^{2}\bm{B}_{ \ell+1}(\bm{A}_{\ell+1}+\bm{B}_{\ell})+\xi^{2}\eta^{3}\bm{B}_{\ell+1}\bm{A}_{ \ell+1}\bm{B}_{\ell}\end{split}\]

and hence

\[\left\|(\bm{I}-\xi\eta\bm{B}_{\ell+1})-\bm{C}_{\ell+1}\left(\bm{I}-\xi\eta\bm{ B}_{\ell}\right)\right\|\leq 2\xi\eta L+2\xi\eta^{2}L^{2}+\xi^{2}\eta^{3}L^{3}.\] (47)

Unravelling the recurrence relation (45) we get

\[\begin{split}\bm{z}_{i+1}-\bm{z}_{0}&=\bm{C}_{i}( \bm{z}_{i}-\bm{z}_{0})-\eta\left(\bm{I}-\xi\eta\bm{B}_{i}\right)\bm{F}_{i}\bm{z} _{0}\\ &=\bm{C}_{i}\big{(}\bm{C}_{i-1}(\bm{z}_{i-1}-\bm{z}_{0})-\eta \left(\bm{I}-\xi\eta\bm{B}_{i-1}\right)\bm{F}_{i-1}\bm{z}_{0}\big{)}-\eta \left(\bm{I}-\xi\eta\bm{B}_{i}\right)\bm{F}_{i}\bm{z}_{0}\\ &=\bm{P}_{i,i-2}(\bm{z}_{i-1}-\bm{z}_{0})-\eta\sum_{\ell=i-1}^{i} \bm{P}_{i,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\bm{F}_{\ell}\bm{z}_{0} \\ &=\bm{P}_{i,i-2}\big{(}\bm{C}_{i-2}(\bm{z}_{i-2}-\bm{z}_{0})-\eta \left(\bm{I}-\xi\eta\bm{B}_{i-2}\right)\bm{F}_{i-2}\bm{z}_{0}\big{)}-\eta \sum_{\ell=i-1}^{i}\bm{P}_{i,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\bm{F }_{\ell}\bm{z}_{0}\\ &=\bm{P}_{i,i-3}(\bm{z}_{i-2}-\bm{z}_{0})-\eta\sum_{\ell=i-2}^{i} \bm{P}_{i,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\bm{F}_{\ell}\bm{z}_{0} \\ &\quad\vdots\\ &=\bm{P}_{i,-1}(\bm{z}_{0}-\bm{z}_{0})-\eta\sum_{\ell=0}^{i}\bm{ P}_{i,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\bm{F}_{\ell}\bm{z}_{0}\end{split}\]and therefore

\[\bm{z}_{i}-\bm{z}_{0}=-\eta\sum_{\ell=0}^{i-1}\bm{P}_{i-1,\ell}\left(\bm{I}-\xi \eta\bm{B}_{\ell}\right)\bm{F}_{\ell}\bm{z}_{0}.\] (47)

In order to compute the bound for \(\left\|\bm{z}_{i}-\bm{z}_{0}\right\|\), we use summation by parts to get

\[\frac{1}{\eta}(\bm{z}_{0}-\bm{z}_{i}) =\sum_{\ell=0}^{i-1}\bm{P}_{i-1,\ell}\left(\bm{I}-\xi\eta\bm{B}_{ \ell}\right)\bm{F}_{\ell}\bm{z}_{0}\] \[=\bm{P}_{i-1,i-1}\left(\bm{I}-\xi\eta\bm{B}_{i-1}\right)\sum_{ \ell=0}^{i-1}\bm{F}_{\ell}\bm{z}_{0}\] \[\qquad-\sum_{\ell=0}^{i-2}\left(\bm{P}_{i-1,\ell+1}\left(\bm{I}- \xi\eta\bm{B}_{\ell+1}\right)-\bm{P}_{i-1,\ell}\left(\bm{I}-\xi\eta\bm{B}_{ \ell}\right)\right)\sum_{j=0}^{\ell}\bm{F}_{\ell}\bm{z}_{0}\] \[=\left(\bm{I}-\xi\eta\bm{B}_{i-1}\right)\bm{g}_{i}-\sum_{\ell=0} ^{i-2}\left(\bm{P}_{i-1,\ell+1}\left(\bm{I}-\xi\eta\bm{B}_{\ell+1}\right)-\bm {P}_{i-1,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\right)\bm{g}_{\ell+1}.\]

Here, observe that

\[\bm{P}_{i-1,\ell+1} \left(\bm{I}-\xi\eta\bm{B}_{\ell+1}\right)-\bm{P}_{i-1,\ell} \left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\] \[=\bm{C}_{i-1}\bm{C}_{i-2}\ldots\bm{C}_{\ell+2}\left(\left(\bm{I} -\xi\eta\bm{B}_{\ell+1}\right)-\bm{C}_{\ell+1}\left(\bm{I}-\xi\eta\bm{B}_{ \ell}\right)\right)\]

so by using (45) and (46) we obtain

\[\left\|\bm{P}_{i-1,\ell+1}\left(\bm{I}-\xi\eta\bm{B}_{\ell+1} \right)-\bm{P}_{i-1,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\right\|\] \[\qquad\leq\left(2\xi\eta L+2\xi\eta^{2}L^{2}+\xi^{2}\eta^{3}L^{3} \right)\left(1+\eta L+\xi\eta^{2}L^{2}\right)^{i-\ell-2}.\]

Therefore, we conclude that

\[\left\|\bm{z}_{i}-\bm{z}_{0}\right\|\leq\eta\left(1+\xi\eta L\right)\left\|\bm {g}_{i}\right\|+\eta^{2}L\left(2\xi+2\xi\eta L+\xi^{2}\eta^{2}L^{2}\right)\sum _{\ell=0}^{i-2}\left(1+\eta L+\xi\eta^{2}L^{2}\right)^{i-\ell-2}\left\|\bm{g}_ {\ell+1}\right\|.\]

Meanwhile, substituting (47) back to (43) gives us

\[\bm{w}_{i}-\bm{z}_{0}=-\xi\eta\bm{F}_{i}\bm{z}_{0}-\eta\sum_{\ell=0}^{i-1} \left(\bm{I}-\xi\eta\bm{A}_{i}\right)\bm{P}_{i-1,\ell}\left(\bm{I}-\xi\eta\bm {B}_{\ell}\right)\bm{F}_{\ell}\bm{z}_{0}.\] (48)

For \(\ell<i\) let us define

\[\bm{R}_{i,\ell} \coloneqq\xi^{-1}\left(\bm{I}-\xi\eta\bm{A}_{i}\right)\bm{P}_{i-1,\ell}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\] \[=\xi^{-1}\left(\bm{I}-\xi\eta\bm{A}_{i}\right)\bm{C}_{i-1}\bm{C}_ {i-2}\ldots\bm{C}_{\ell+2}\bm{C}_{\ell+1}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\]

and for convenience \(\bm{R}_{i,i}\coloneqq\bm{I}\) so that (48) can be rewritten as

\[\frac{1}{\xi\eta}(\bm{z}_{0}-\bm{w}_{i})=\sum_{\ell=0}^{i}\bm{R}_{i,\ell}\bm{F }_{\ell}\bm{z}_{0}.\] (49)

Applying summation by parts on the above, we obtain

\[\frac{1}{\xi\eta}(\bm{z}_{0}-\bm{w}_{i}) =\bm{R}_{i,i}\sum_{\ell=0}^{i}\bm{F}_{\ell}\bm{z}_{0}-\sum_{\ell =0}^{i-1}(\bm{R}_{i,\ell+1}-\bm{R}_{i,\ell})\sum_{j=0}^{\ell}\bm{F}_{j}\bm{z}_{0}\] \[=\bm{g}_{i+1}-\sum_{\ell=0}^{i-1}(\bm{R}_{i,\ell+1}-\bm{R}_{i, \ell})\bm{g}_{\ell+1}\]

and as a consequence we get

\[\frac{1}{\xi\eta}\left\|\bm{w}_{i}-\bm{z}_{0}\right\|\leq\left\|\bm{g}_{i+1} \right\|+\sum_{\ell=0}^{i-1}\left\|\bm{R}_{i,\ell+1}-\bm{R}_{i,\ell}\right\| \left\|\bm{g}_{\ell+1}\right\|.\] (50)It remains to bound \(\|\bm{R}_{i,\ell+1}-\bm{R}_{i,\ell}\|\). For the special case where \(\ell=i-1\), a direct computation leads to

\[\bm{R}_{i,i}-\bm{R}_{i,i-1} =\bm{I}-\xi^{-1}\left(\bm{I}-\xi\eta\bm{A}_{i}\right)\left(\bm{I}- \xi\eta\bm{B}_{i-1}\right)\] \[=(1-\xi^{-1})\bm{I}+\eta\bm{A}_{i}+\eta\bm{B}_{i-1}-\xi\eta^{2} \bm{A}_{i}\bm{B}_{i-1}\]

and thus we have

\[\|\bm{R}_{i,i}-\bm{R}_{i,i-1}\|\leq(1-\xi^{-1})+2\eta L+\xi\eta^{2}L^{2}.\] (51)

For the other cases; that is, when \(\ell<i-1\), we have

\[\bm{R}_{i,\ell+1}-\bm{R}_{i,\ell}=\xi^{-1}\left(\bm{I}-\xi\eta\bm{A}_{i} \right)\bm{C}_{i-1}\bm{C}_{i-2}\ldots\bm{C}_{\ell+2}\left((\bm{I}-\xi\eta\bm{ B}_{\ell+1})-\bm{C}_{\ell+1}\left(\bm{I}-\xi\eta\bm{B}_{\ell}\right)\right)\]

so by using (45) and (46) we get the bound

\[\|\bm{R}_{i,\ell+1}-\bm{R}_{i,\ell}\|\leq\xi^{-1}(1+\xi\eta L)\left(2\xi\eta L +2\xi\eta^{2}L^{2}+\xi^{2}\eta^{3}L^{3}\right)\left(1+\eta L+\xi\eta^{2}L^{2} \right)^{i-\ell-2}.\] (52)

Applying (51) and (52) on (50) gives the bound for \(\|\bm{w}_{i}-\bm{z}_{0}\|\). 

**Proposition E.3**.: _Suppose that SEG-FFA is used, \(\eta<\frac{1}{nL}\), and let \(\nu\coloneqq 1+\frac{1}{2n}\). Then for any \(i=1,\ldots,2n-1\) we have the bounds_

\[\|\bm{z}_{i}-\bm{z}_{0}\|\leq\left(\eta\nu i+\frac{\eta\nu^{2}e^{ 2}i(i-1)}{2n}\right)\|\bm{F}\bm{z}_{0}\|+\eta\nu\delta_{i}+\eta^{2}L\nu^{2}e^{ 2}\Sigma_{i-1},\] \[\|\bm{w}_{i}-\bm{z}_{0}\|\leq\frac{\eta}{2}\left(1+2\nu^{2}i+ \frac{\nu^{3}e^{2}i(i-1)}{n}\right)\|\bm{F}\bm{z}_{0}\|+\frac{\eta}{2}\delta_{ i+1}+\frac{\eta(2\nu^{2}-1)}{2}\delta_{i}+\eta^{2}L\nu^{3}e^{2}\Sigma_{i-1},\] \[\|\bm{w}_{i}-\bm{z}_{0}\|^{2}\leq\left(\frac{3\eta^{2}(i+1)^{2}}{ 2}+\frac{3\eta^{2}(2\nu^{2}-1)^{2}i^{2}}{2}+\frac{\eta^{2}\nu^{6}e^{4}i(i-1)^ {2}(2i-1)}{n^{2}}\right)\|\bm{F}\bm{z}_{0}\|^{2}\] \[\qquad\qquad\qquad+\frac{3\eta^{2}}{2}\delta_{i+1}^{2}+\frac{3 \eta^{2}(2\nu^{2}-1)^{2}}{2}\delta_{i}^{2}+\frac{6\eta^{2}\nu^{6}e^{4}(i-1)}{ n^{2}}\Psi_{i-1}.\]

Proof.: Using elementary calculus one can show that \(x\mapsto(1+\frac{1}{x}+\frac{1}{2x^{2}})^{x}\) increases on \(x>0\) and is bounded above by \(e\). Hence for all \(0\leq\ell<i\leq 2n\) we have

\[\left(1+\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{i-\ell-2}\leq\left(1+\frac{1}{ n}+\frac{1}{2n^{2}}\right)^{2n}\leq e^{2}.\]

Applying the definitions (35) and (36) on (38) and then substituting \(\xi=\nicefrac{{1}}{{2}}\) we get

\[\|\bm{z}_{i}-\bm{z}_{0}\|\leq\eta\left(1+\frac{\eta L}{2}\right)\| \bm{g}_{i}\|+\eta^{2}L\left(1+\frac{\eta L}{2}\right)^{2}\sum_{\ell=0}^{i-2} \left(1+\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{i-\ell-2}\|\bm{g}_{\ell+1}\|\] \[\leq\eta\nu\left(i\left\|\bm{F}\bm{z}_{0}\right\|+\delta_{i} \right)+\eta^{2}L\nu^{2}\sum_{\ell=0}^{i-2}e^{2}\left((\ell+1)\left\|\bm{F}\bm{ z}_{0}\right\|+\delta_{\ell+1}\right)\] \[\leq\eta\nu\left(i\left\|\bm{F}\bm{z}_{0}\right\|+\delta_{i} \right)+\frac{\eta\nu^{2}e^{2}i(i-1)}{2n}\left\|\bm{F}\bm{z}_{0}\right\|+\eta^ {2}L\nu^{2}e^{2}\Sigma_{i-1}.\]

Similarly, from (39) we get

\[\|\bm{w}_{i}-\bm{z}_{0}\|\leq\frac{\eta}{2}\left\|\bm{g}_{i+1}\right\|+\frac{ \eta}{2}\left(1+2\eta L+\frac{\eta^{2}L^{2}}{2}\right)\|\bm{g}_{i}\|\] \[\qquad\qquad+\eta^{2}L\left(1+\frac{\eta L}{2}\right)^{3}\sum_{ \ell=0}^{i-2}\left(1+\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{i-\ell-2}\|\bm{g}_ {\ell+1}\|\] \[\leq\frac{\eta}{2}\left((i+1)\left\|\bm{F}\bm{z}_{0}\right\|+ \delta_{i+1}\right)+\frac{\eta}{2}\left(1+\frac{2}{n}+\frac{1}{2n^{2}}\right) \left(i\left\|\bm{F}\bm{z}_{0}\right\|+\delta_{i}\right)\] \[\qquad\qquad+\eta^{2}L\nu^{3}\sum_{\ell=0}^{i-2}e^{2}\left((\ell+ 1)\left\|\bm{F}\bm{z}_{0}\right\|+\delta_{\ell+1}\right)\] \[\leq\frac{\eta}{2}(1+2i\nu^{2})\left\|\bm{F}\bm{z}_{0}\right\|+ \frac{\eta\nu^{3}e^{2}i(i-1)}{2n}\left\|\bm{F}\bm{z}_{0}\right\|+\frac{\eta}{2} \delta_{i+1}+\frac{\eta(2\nu^{2}-1)}{2}\delta_{i}+\eta^{2}L\nu^{3}e^{2}\Sigma_ {i-1}.\]Finally, applying generalized Young's inequality on (39) we get

\[\left\|\bm{w}_{i}-\bm{z}_{0}\right\|^{2} \leq\frac{3\eta^{2}}{4}\left\|\bm{g}_{i+1}\right\|^{2}+\frac{3\eta^ {2}}{4}\left(1+2\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{2}\left\|\bm{g}_{i} \right\|^{2}\] \[\qquad+3\left(\eta^{2}L\left(1+\frac{\eta L}{2}\right)^{3}\sum_{ \ell=0}^{i-2}\left(1+\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{i-\ell-2}\left\| \bm{g}_{\ell+1}\right\|\right)^{2}.\]

Using generalized Young's inequality once more on the last term gives us

\[3\left(\eta^{2}L\left(1+\frac{\eta L}{2}\right)^{3}\sum_{\ell=0} ^{i-2}\left(1+\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{i-\ell-2}\left\|\bm{g}_{ \ell+1}\right\|\right)^{2} \leq 3\left(\frac{\eta\nu^{3}e^{2}}{n}\sum_{\ell=0}^{i-2}\left\| \bm{g}_{\ell+1}\right\|\right)^{2}\] \[\leq\frac{3\eta^{2}\nu^{6}e^{4}(i-1)}{n^{2}}\sum_{\ell=0}^{i-2} \left\|\bm{g}_{\ell+1}\right\|^{2}.\]

Plugging this back yields

\[\left\|\bm{w}_{i}-\bm{z}_{0}\right\|^{2} \leq\frac{3\eta^{2}}{4}\left\|\bm{g}_{i+1}\right\|^{2}+\frac{3 \eta^{2}}{4}\left(1+2\eta L+\frac{\eta^{2}L^{2}}{2}\right)^{2}\left\|\bm{g}_{ i}\right\|^{2}+\frac{3\eta^{2}\nu^{6}e^{4}(i-1)}{n^{2}}\sum_{\ell=0}^{i-2}\left\| \bm{g}_{\ell+1}\right\|^{2}\] \[\leq\frac{3\eta^{2}}{4}\left(2(i+1)^{2}\left\|\bm{F}\bm{z}_{0} \right\|^{2}+2\delta_{i+1}^{2}\right)+\frac{3\eta^{2}}{4}\left(2\nu^{2}-1 \right)^{2}\left(2i^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+2\delta_{i}^{2}\right)\] \[\qquad+\frac{3\eta^{2}\nu^{6}e^{4}(i-1)}{n^{2}}\sum_{\ell=0}^{i-2 }\left(2(\ell+1)^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+2\delta_{\ell+1}^{2}\right)\] \[\leq\frac{3\eta^{2}}{2}\left((i+1)^{2}\left\|\bm{F}\bm{z}_{0} \right\|^{2}+\delta_{i+1}^{2}\right)+\frac{3\eta^{2}}{2}\left(2\nu^{2}-1 \right)^{2}\left(i^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\delta_{i}^{2}\right)\] \[\qquad+\frac{\eta^{2}\nu^{6}e^{4}i(i-1)^{2}(2i-1)}{n^{2}}\left\| \bm{F}\bm{z}_{0}\right\|^{2}+\frac{6\eta^{2}\nu^{6}e^{4}(i-1)}{n^{2}}\Psi_{i-1}.\]

Now the claimed inequalities can be obtained simply by rearranging the terms appropriately. 

**Proposition E.4**.: _Suppose that either SEG-RR or SEG-FF is used with \(\alpha=\beta=\eta<\frac{1}{nL}\), and let \(\tilde{\nu}:=1+\frac{1}{n}\). Then for any \(i=1,\ldots,2n-1\) we have the bounds_

\[\left\|\bm{z}_{i}-\bm{z}_{0}\right\| \leq\left(\eta\tilde{\nu}i+\frac{16\eta\tilde{\nu}^{2}i(i-1)}{n} \right)\left\|\bm{F}\bm{z}_{0}\right\|+\eta\tilde{\nu}\delta_{i}+32\eta^{2}L \tilde{\nu}^{2}\Sigma_{i-1},\] \[\left\|\bm{w}_{i}-\bm{z}_{0}\right\| \leq\eta\left(1+i\tilde{\nu}^{2}+\frac{16\tilde{\nu}^{3}i(i-1)}{ n}\right)\left\|\bm{F}\bm{z}_{0}\right\|+\eta\delta_{i+1}+\eta(\tilde{\nu}^{2}-1) \delta_{i}+32\eta^{2}L\tilde{\nu}^{3}\Sigma_{i-1},\] \[\left\|\bm{w}_{i}-\bm{z}_{0}\right\|^{2} \leq\left(6\eta^{2}(i+1)^{2}+\frac{6\eta^{2}\left(1+\tilde{\nu} \right)^{2}i^{2}}{n^{2}}+\frac{1024\eta^{2}\tilde{\nu}^{6}i(i-1)^{2}(2i-1)}{n^ {2}}\right)\left\|\bm{F}\bm{z}_{0}\right\|^{2}\] \[\qquad+6\eta^{2}\delta_{i+1}^{2}+\frac{6\eta^{2}\left(1+\tilde{\nu }\right)^{2}}{n^{2}}\delta_{i}^{2}+\frac{6144\eta^{2}\tilde{\nu}^{6}(i-1)}{n^ {2}}\Psi_{i-1}.\]

Proof.: One can verify that \(x\mapsto(1+\frac{4}{3x})^{x}\) increases on \(x\geq 3\) and is bounded above by \(e^{4/3}<4\). With noting that \((1+\frac{1}{1}+\frac{1}{1^{2}})^{1}=3<4\), \((1+\frac{1}{2}+\frac{1}{2^{2}})^{2}=\frac{49}{16}<4\), and \(1+\frac{1}{x}+\frac{1}{x^{2}}\leq 1+\frac{4}{3x}\) whenever \(x\geq 3\), we see that for all \(0\leq\ell<i\leq 2n\) it holds that

\[\left(1+\eta L+\eta^{2}L^{2}\right)^{i-\ell-2}\leq\left(1+\frac{1}{n}+\frac{1}{ n^{2}}\right)^{2n}\leq 4^{2}=16.\]

Also, we have

\[2+2\eta L+\eta^{2}L^{2}\leq 2+\frac{2}{n}+\frac{1}{n^{2}}=1+\tilde{\nu}^{2}\leq 2 \tilde{\nu}^{2}.\]Applying the definitions (35) and (36) on (38) and then substituting \(\xi=1\) we get

\[\|\bm{z}_{i}-\bm{z}_{0}\| \leq\eta\left(1+\eta L\right)\|\bm{g}_{i}\|+\eta^{2}L\left(2+2\eta L +\eta^{2}L^{2}\right)\sum_{\ell=0}^{i-2}\left(1+\eta L+\eta^{2}L^{2}\right)^{i- \ell-2}\|\bm{g}_{\ell+1}\|\] \[\leq\eta\tilde{\nu}\left(i\left\|\bm{F}\bm{z}_{0}\right\|+\delta_ {i}\right)+2\eta^{2}L\tilde{\nu}^{2}\sum_{\ell=0}^{i-2}16\left(\left(\ell+1 \right)\|\bm{F}\bm{z}_{0}\|+\delta_{\ell+1}\right)\] \[\leq\eta\tilde{\nu}\left(i\left\|\bm{F}\bm{z}_{0}\right\|+\delta_ {i}\right)+\frac{16\eta\tilde{\nu}^{2}i(i-1)}{n}\left\|\bm{F}\bm{z}_{0}\right\| +32\eta^{2}L\tilde{\nu}^{2}\Sigma_{i-1}.\]

Similarly, from (39) we get

\[\|\bm{w}_{i}-\bm{z}_{0}\| \leq\eta\left\|\bm{g}_{i+1}\right\|+\eta^{2}L\left(2+\eta L \right)\|\bm{g}_{i}\|\] \[\qquad+\eta^{2}L(1+\eta L)\left(2+2\eta L+\eta^{2}L^{2}\right) \sum_{\ell=0}^{i-2}\left(1+\eta L+\eta^{2}L^{2}\right)^{i-\ell-2}\left\|\bm{g} _{\ell+1}\right\|.\] \[\leq\eta\left(\left(i+1\right)\|\bm{F}\bm{z}_{0}\right\|+\delta_ {i+1}\right)+\frac{\eta}{n}\left(2+\frac{1}{n}\right)\left(i\left\|\bm{F}\bm{z }_{0}\right\|+\delta_{i}\right)\] \[\qquad+\eta^{2}L\tilde{\nu}\left(2\tilde{\nu}^{2}\right)\sum_{ \ell=0}^{i-2}16\left(\ell\left\|\bm{F}\bm{z}_{0}\right\|+\delta_{\ell}\right).\] \[\leq\eta(1+i\tilde{\nu}^{2})\left\|\bm{F}\bm{z}_{0}\right\|+\frac {16\eta\tilde{\nu}^{3}i(i-1)}{n}\left\|\bm{F}\bm{z}_{0}\right\|+\eta\delta_{i +1}+\eta(\tilde{\nu}^{2}-1)\delta_{i}+32\eta^{2}L\tilde{\nu}^{3}\Sigma_{i-1}.\]

Finally, applying Young's inequality on (39) we get

\[\|\bm{w}_{i}-\bm{z}_{0}\|^{2} \leq 3\eta^{2}\left\|\bm{g}_{i+1}\right\|^{2}+\frac{3\eta^{2} \left(2+\eta L\right)^{2}}{n^{2}}\left\|\bm{g}_{i}\right\|\] \[\qquad+3\left(\eta^{2}L(1+\eta L)\left(2+2\eta L+\eta^{2}L^{2} \right)\sum_{\ell=0}^{i-2}\left(1+\eta L+\eta^{2}L^{2}\right)^{i-\ell-2}\left\| \bm{g}_{\ell+1}\right\|\right)^{2}\] \[\leq 3\eta^{2}\left\|\bm{g}_{i+1}\right\|^{2}+\frac{3\eta^{2} \left(2+\eta L\right)^{2}}{n^{2}}\left\|\bm{g}_{i}\right\|+3\left(\frac{2\eta \tilde{\nu}^{3}}{n}\sum_{\ell=0}^{i-2}16\left\|\bm{g}_{\ell+1}\right\|\right) ^{2}.\]

Using Young's inequality once more on the last term gives us

\[3\left(\frac{32\eta\tilde{\nu}^{3}}{n}\sum_{\ell=0}^{i-2}\left\|\bm{g}_{\ell +1}\right\|\right)^{2}\leq\frac{3072\eta^{2}\tilde{\nu}^{6}(i-1)}{n^{2}}\sum_ {\ell=0}^{i-2}\left\|\bm{g}_{\ell+1}\right\|^{2}.\]

Plugging this back yields

\[\left\|\bm{w}_{i}-\bm{z}_{0}\right\|^{2} \leq 3\eta^{2}\left\|\bm{g}_{i+1}\right\|^{2}+\frac{3\eta^{2} \left(2+\eta L\right)^{2}}{n^{2}}\left\|\bm{g}_{i}\right\|+\frac{3072\eta^{2} \tilde{\nu}^{6}(i-1)}{n^{2}}\sum_{\ell=0}^{i-2}\left\|\bm{g}_{\ell+1}\right\| ^{2}\] \[\leq 6\eta^{2}\left(\left(i+1\right)^{2}\left\|\bm{F}\bm{z}_{0} \right\|^{2}+\delta_{i+1}^{2}\right)+\frac{6\eta^{2}\left(2+\eta L\right)^{2}}{ n^{2}}\left(i^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\delta_{i}^{2}\right)\] \[\qquad+\frac{6144\eta^{2}\tilde{\nu}^{6}(i-1)}{n^{2}}\sum_{\ell=0 }^{i-2}\left(\left(\ell+1\right)^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+ \delta_{\ell+1}^{2}\right)\] \[\leq 6\eta^{2}\left(\left(i+1\right)^{2}\left\|\bm{F}\bm{z}_{0} \right\|^{2}+\delta_{i+1}^{2}\right)+\frac{6\eta^{2}\left(1+\tilde{\nu}\right) ^{2}}{n^{2}}\left(i^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\delta_{i}^{2}\right)\] \[\qquad+\frac{1024\eta^{2}\tilde{\nu}^{6}i(i-1)^{2}(2i-1)}{n^{2}} \left\|\bm{F}\bm{z}_{0}\right\|^{2}+\frac{6144\eta^{2}\tilde{\nu}^{6}(i-1)}{n^{ 2}}\Psi_{i-1}.\]

Now the claimed inequalities can be obtained simply by rearranging the terms appropriately. 

Let us now derive the upper bounds for the quantities related to \(\delta_{j}\) and \(\Sigma_{j}\), defined in (35) and (36) respectively, using the upper bound of the variance of saddle gradients (4).

**Lemma E.5**.: _For any \(j=1,\ldots,2n\), it deterministically holds that_

\[\delta_{j}\leq n(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma).\] (53)

Proof.: For any set of indices \(\mathcal{J}\subset\{0,\ldots,n-1\}\), by Assumption 3.4 it holds that

\[\sum_{i\in\mathcal{J}}\left\|\bm{F}_{i}\bm{z}_{0}-\bm{F}\bm{z}_{0}\right\|^{2} \leq\sum_{i=0}^{n-1}\left\|\bm{F}_{i}\bm{z}_{0}-\bm{F}\bm{z}_{0}\right\|^{2} \leq n(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}.\]

Hence, for any \(j=1,\ldots,n\) we have

\[\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2} =\left\|\sum_{i=0}^{j-1}\bm{F}_{i}\bm{z}_{0}-j\bm{F}\bm{z}_{0} \right\|^{2}\] \[\leq j\sum_{i=0}^{j-1}\left\|\bm{F}_{i}\bm{z}_{0}-\bm{F}\bm{z}_{ 0}\right\|^{2}\] \[\leq jn(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}\] \[\leq n^{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2},\]

and for any \(j=n+1,\ldots,2n\) we have

\[\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2} =\left\|\sum_{i=0}^{n-1}\bm{F}_{i}\bm{z}_{0}+\sum_{i=n}^{j-1}\bm{ F}_{i}\bm{z}_{0}-j\bm{F}\bm{z}_{0}\right\|^{2}\] (54) \[=\left\|\sum_{i=n}^{j-1}\bm{F}_{i}\bm{z}_{0}-(j-n)\bm{F}\bm{z}_{0 }\right\|^{2}\] \[=\left\|\sum_{i=2n-j}^{n-1}\bm{F}_{i}\bm{z}_{0}-(j-n)\bm{F}\bm{z} _{0}\right\|^{2}\] \[\leq(j-n)\sum_{i=0}^{j-1}\left\|\bm{F}_{i}\bm{z}_{0}-\bm{F}\bm{z} _{0}\right\|^{2}\] \[\leq n^{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}.\]

Therefore, in any case we have

\[\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2}\leq n^{2}(\rho\left\|\bm{F} \bm{z}_{0}\right\|+\sigma)^{2}.\]

Taking square roots on both sides gives us the desired bound. 

**Lemma E.6**.: _For any \(j=1,\ldots,2n\), it holds that_

\[\mathbb{E}_{\tau}[\delta_{j}^{2}]\leq\frac{n(\rho\left\|\bm{F}\bm{z}_{0} \right\|+\sigma)^{2}}{2},\] (55)

Proof.: If \(n=1\) then the left hand side is always \(0\), so there is nothing to show. So, we may assume that \(n\geq 2\). Then, for any \(j=1,\ldots,n\), using Lemma 1 in [35] we obtain

\[\mathbb{E}_{\tau}\left\|\frac{1}{j}\bm{g}_{j}-\bm{F}\bm{z}_{0}\right\|^{2} \leq\frac{n-j}{j(n-1)}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}.\]

Multiplying both sides by \(j^{2}\) and applying AM-GM inequality leads to

\[\mathbb{E}_{\tau}\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2} \leq\frac{j(n-j)}{n-1}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}\leq \frac{n^{2}}{4(n-1)}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}\leq\frac{ n}{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}.\]

Meanwhile, for \(j=n+1,\ldots,2n\), following the first few steps in (54) we get

\[\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2}=\left\|\sum_{i=2n-j}^{n-1}\bm{ F}_{i}\bm{z}_{0}-(j-n)\bm{F}\bm{z}_{0}\right\|^{2}\]Here, once more applying Lemma 1 of [35], we get

\[\mathbb{E}_{\tau}\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2} =\mathbb{E}_{\tau}\left\|\sum_{i=2n-j}^{n-1}\bm{F}_{i}\bm{z}_{0}-(j -n)\bm{F}\bm{z}_{0}\right\|^{2}\] \[=(j-n)^{2}\,\mathbb{E}_{\tau}\left\|\frac{1}{j-n}\sum_{i=2n-j}^{n- 1}\bm{F}_{i}\bm{z}_{0}-\bm{F}\bm{z}_{0}\right\|^{2}\] \[\leq(j-n)^{2}\cdot\frac{n-(j-n)}{(j-n)(n-1)}(\rho\left\|\bm{F} \bm{z}_{0}\right\|+\sigma)^{2}\] \[\leq\frac{(j-n)(2n-j)}{n-1}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+ \sigma)^{2}.\]

Using AM-GM inequality on the last line gives us

\[\mathbb{E}_{\tau}\left\|\bm{g}_{j}-j\bm{F}\bm{z}_{0}\right\|^{2} \leq\frac{(j-n)(2n-j)}{n-1}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+ \sigma)^{2}\leq\frac{n^{2}}{4(n-1)}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma )^{2}\leq\frac{n}{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}.\]

Thus, for any case, we have (55). 

**Lemma E.7**.: _For any \(k,\ell\in\{0,1,\ldots,2n\}\), it holds that_

\[\mathbb{E}_{\tau}[\Sigma_{k}\Sigma_{\ell}]\leq\frac{k\ell n(\rho \left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}.\] (56)

Proof.: Expanding the product \(\Sigma_{k}\Sigma_{\ell}\) and writing in terms of \(\delta\), we get

\[\Sigma_{k}\Sigma_{\ell}=\left(\sum_{i=1}^{k}\delta_{i}\right) \left(\sum_{j=1}^{\ell}\delta_{j}\right) =\sum_{i=1}^{k}\sum_{j=1}^{\ell}\delta_{i}\delta_{j}\] \[\leq\sum_{i=1}^{k}\sum_{j=1}^{\ell}\frac{\delta_{i}^{2}+\delta_{j }^{2}}{2}\]

where the last line follows from the AM-GM inequality. Taking the expectation with respect to \(\tau\) and using the bound from Lemma E.6, we obtain

\[\mathbb{E}_{\tau}[\Sigma_{k}\Sigma_{\ell}] \leq\frac{1}{2}\sum_{i=1}^{k}\sum_{j=1}^{\ell}\left(\mathbb{E}_{ \tau}[\delta_{i}^{2}]+\mathbb{E}_{\tau}[\delta_{j}^{2}]\right)\] \[\leq\frac{1}{2}\sum_{i=1}^{k}\sum_{j=1}^{\ell}\left(\frac{n(\rho \left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}+\frac{n(\rho\left\|\bm{F}\bm{z }_{0}\right\|+\sigma)^{2}}{2}\right)\] \[=\frac{k\ell n(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}\]

which is exactly the claimed. 

**Lemma E.8**.: _For any \(k,\ell\in\{0,1,\ldots,2n\}\), it holds that_

\[\mathbb{E}_{\tau}\left[\left(\sum_{i=1}^{k}\Sigma_{i}\right)\left(\sum_{j=1}^{ \ell}\Sigma_{j}\right)\right]\leq\frac{k(k+1)\ell(\ell+1)n(\rho\left\|\bm{F} \bm{z}_{0}\right\|+\sigma)^{2}}{8}.\] (57)Proof.: Expanding the product in the left hand side of (57) and applying (56), we get

\[\mathbb{E}_{\tau}\left[\left(\sum_{i=1}^{k}\Sigma_{i}\right)\left( \sum_{j=1}^{\ell}\Sigma_{j}\right)\right]=\mathbb{E}_{\tau}\left[\sum_{i=1}^{k} \sum_{j=1}^{\ell}\Sigma_{i}\Sigma_{j}\right] =\sum_{i=1}^{k}\sum_{j=1}^{\ell}\mathbb{E}_{\tau}[\Sigma_{i} \Sigma_{j}]\] \[\leq\sum_{i=1}^{k}\sum_{j=1}^{\ell}\frac{ijn(\rho\left\|\bm{F} \bm{z}_{0}\right\|+\sigma)^{2}}{2}\] \[\leq\frac{k(k+1)\ell(\ell+1)n(\rho\left\|\bm{F}\bm{z}_{0}\right\| +\sigma)^{2}}{8}.\qed\]

### Upper Bounds of the Within-Epoch Errors

The full proof of Theorem E.1 is quite long and technical, so we divide it into several parts. First we show that (31) and (32) holds with \(a=3\) when \(\mathsf{SEG-FFA}\) is in use. Then we show that Theorem E.1 also holds for \(\mathsf{SEG-FF}\) in Appendix E.2.3, and for \(\mathsf{SEG-RR}\) in Appendix E.2.4.

Throughout the remaining of this section, we always assume that the variance of the saddle gradients satisfies (4).

#### e.2.1 Proof of Equation (31) for \(\mathsf{SEG-FFA}\)

In this section we prove the following.

**Theorem E.9**.: _Say we use \(\mathsf{SEG-FFA}\). Then, as long as the stepsize used in an epoch satisfies \(\eta<\frac{1}{nL}\), it holds that_

\[\left\|\bm{r}\right\|\leq\eta^{3}n^{3}C_{\bm{\mathsf{IA}}}\left\|\bm{F}\bm{z}_ {0}\right\|+\eta^{3}n^{3}D_{\bm{\mathsf{IA}}}\left\|\bm{F}\bm{z}_{0}\right\|^{ 2}+\eta^{3}n^{3}V_{\bm{\mathsf{IA}}}\] (58)

_for constants_

\[C_{\bm{\mathsf{IA}}} \coloneqq L^{2}\left(\frac{1}{2}\left(1+\frac{2e^{2}}{3}\right)+ \frac{6+e^{2}}{3}+15\rho\right),\] (59) \[D_{\bm{\mathsf{IA}}} \coloneqq M\left(\frac{83}{4}+\frac{24e^{4}}{5}+\rho^{2}\left( \frac{243}{16}+27e^{4}\right)\right),\] (60) \[V_{\bm{\mathsf{IA}}} \coloneqq M\sigma^{2}\left(\frac{243}{16}+27e^{4}\right)+15L^{2}\sigma.\] (61)

We first list the intermediate results. The actual proof of Theorem E.9 is in page 43, at the end of this section.

**Proposition E.10**.: _For using \(\mathsf{SEG-FFA}\), the within-epoch update \(\bm{z^{\sharp}}\) as given by (12) satisfies_

\[\bm{z^{\sharp}}=\bm{z}_{0}-n\eta\bm{F}(\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0})+\bm{r}\]

_where we denote_

\[\bm{r} \coloneqq n\eta\bm{F}(\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0})-n\eta\bm{F}\bm{z}_ {0}+n^{2}\eta^{2}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}\] (62a) \[\quad-\frac{\eta}{2}\sum_{j=0}^{2n-1}\Bigl{(}\bm{F}_{j}\bm{w}_{j}- \bm{F}_{j}\bm{z}_{0}-D\bm{F}_{j}(\bm{z}_{0})(\bm{w}_{j}-\bm{z}_{0})\Bigr{)}\] (62b) \[\quad+\frac{\eta^{2}}{4}\sum_{j=0}^{2n-1}D\bm{F}_{j}(\bm{z}_{0})( \bm{F}_{j}\bm{z}_{j}-\bm{F}_{j}\bm{z}_{0})\] (62c) \[\quad+\frac{\eta^{2}}{2}\sum_{j=0}^{2n-1}D\bm{F}_{j}(\bm{z}_{0}) \sum_{k=0}^{j-1}(\bm{F}_{k}\bm{w}_{k}-\bm{F}_{k}\bm{z}_{0}).\] (62d)

Proof.: Setting \(\alpha=\nicefrac{{\eta}}{{2}}\), \(\beta=\eta\), and \(\theta=1\) in (28), we get

\[\bm{z}^{\sharp}=\bm{z}_{0}-\frac{\eta}{2}\sum_{j=0}^{2n-1}\bm{F}_{j}\bm{z}_{0} +\frac{\eta^{2}}{4}\sum_{j=0}^{2n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0} +\frac{\eta^{2}}{2}\sum_{0\leq k<j\leq 2n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0} +\frac{1}{2}\bm{\epsilon}_{2n}\] (63)where \(\epsilon_{2n}\) is defined as in (30). Recall that \(\bm{F}_{i}=\bm{F}_{2n-1-i}\) for all \(i=0,1,\ldots,2n-1\), and moreover, \(\sum_{i=0}^{n-1}\bm{F}_{i}=\sum_{i=n}^{2n-1}\bm{F}_{i}=n\bm{F}\). Thus, the first sum in the above is equal to \(2n\bm{F}\bm{z}_{0}\), and the second sum is equal to \(2\sum_{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}\). For the last sum, observe that

\[\sum_{0\leq k<j\leq 2n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0} =\sum_{0\leq k<j\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0} +\sum_{n\leq k<j\leq 2n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}\] \[+\sum_{\begin{subarray}{c}0\leq k\leq n-1\\ n\leq j\leq 2n-1\end{subarray}}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}\] \[=\sum_{0\leq k<j\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_ {0} +\sum_{n-1\geq k>j\geq 0}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}\] \[+\sum_{\begin{subarray}{c}0\leq k\leq n-1\\ n-1\geq j\geq 0\end{subarray}}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}\] \[=2\sum_{k\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}+\sum _{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}.\]

Hence, (63) is equivalent to

\[\bm{z}^{\sharp} =\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0}+\frac{\eta^{2}}{2}\sum_{j=0}^{n -1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\frac{\eta^{2}}{2}\sum_{0\leq k <j\leq 2n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}+\frac{1}{2}\bm{ \epsilon}_{2n}\] \[=\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0}+\eta^{2}\sum_{j=0}^{n-1}D\bm{F} _{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\eta^{2}\sum_{k\neq j}D\bm{F}_{j}(\bm{z} _{0})\bm{F}_{k}\bm{z}_{0}+\frac{1}{2}\bm{\epsilon}_{2n}\] \[=\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0}+\eta^{2}\left(\sum_{j=0}^{n-1}D \bm{F}_{j}(\bm{z}_{0})\right)\left(\sum_{j=0}^{n-1}\bm{F}_{j}\bm{z}_{0}\right) +\frac{1}{2}\bm{\epsilon}_{2n}\] \[=\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0}+n^{2}\eta^{2}D\bm{F}(\bm{z}_{0}) \bm{F}\bm{z}_{0}+\frac{1}{2}\bm{\epsilon}_{2n}.\]

Observing that the terms (62b), (62c), and (62d) add up to \(\frac{1}{2}\bm{\epsilon}_{2n}\) completes the proof. 

**Proposition E.11**.: _Suppose that \(\eta<\frac{1}{nL}\), and let \(\nu\coloneqq 1+\frac{1}{2n}\). Then the noise term satisfies the bound_

\[\left\|\bm{r}\right\|\leq\eta^{3}n^{3} \,L^{2}\left\|\bm{F}\bm{z}_{0}\right\|\left(\frac{1}{2n}\left(1+ \frac{2e^{2}}{3}\right)+\frac{4\nu+e^{2}}{3}\right)\] \[+\eta^{3}n^{3}M\left\|\bm{F}\bm{z}_{0}\right\|^{2}\left(\frac{1}{ 2}+4\nu^{4}+\frac{16\nu e^{4}}{5}\right)\] \[+\frac{3\eta^{3}M}{8}\left(\Psi_{2n}+(2\nu^{2}-1)^{2}\Psi_{2n-1}+ \frac{4\nu^{6}e^{4}}{n^{2}}\sum_{j=1}^{2n-2}j\Psi_{j}\right)\] \[+\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+\frac{\eta^{3}L^{2} \nu^{2}(1+\eta Le^{2})}{2}\sum_{j=1}^{2n-2}\Sigma_{j}+\frac{\eta^{4}L^{3}\nu^ {3}e^{2}}{2}\sum_{k=1}^{2n-2}(2n-k-1)\Sigma_{k-1}.\]

Proof.: We bound each line in equation (62). For (62a), we use Lemma C.6 to get

\[\left\|n\eta\bm{F}(\bm{z}_{0}-n\eta\bm{F}\bm{z}_{0})-n\eta\bm{F} \bm{z}_{0}+n^{2}\eta^{2}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}\right\| \leq\frac{n\eta M}{2}\left\|-n\eta\bm{F}\bm{z}_{0}\right\|^{2}\] \[=\frac{n^{3}\eta^{3}M}{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}.\]

[MISSING_PAGE_FAIL:39]

For (62c), we use \(L\)-smoothness of \(\bm{F}_{j}\), and also the fact that it implies \(\left\|D\bm{F}_{j}(\bm{z}_{0})\right\|\leq L\), to get

\[\left\|\frac{\eta^{2}}{4}\sum_{j=0}^{2n-1}D\bm{F}_{j}(\bm{z}_{0})( \bm{F}_{j}\bm{z}_{j}-\bm{F}_{j}\bm{z}_{0})\right\|\] \[\qquad\leq\frac{\eta^{2}}{4}\sum_{j=0}^{2n-1}\left\|D\bm{F}_{j}( \bm{z}_{0})\right\|\left\|\bm{F}_{j}\bm{z}_{j}-\bm{F}_{j}\bm{z}_{0}\right\|\] \[\qquad\leq\frac{\eta^{2}L^{2}}{4}\sum_{j=0}^{2n-1}\left\|\bm{z}_ {j}-\bm{z}_{0}\right\|\] \[\qquad\leq\frac{\eta^{2}L^{2}}{4}\sum_{j=1}^{2n-1}\left(\left( \eta\nu j+\frac{\eta\nu^{2}e^{2}j(j-1)}{2n}\right)\left\|\bm{F}\bm{z}_{0} \right\|+\eta\nu\delta_{j}+\eta^{2}L\nu^{2}e^{2}\Sigma_{j-1}\right)\] \[\qquad\qquad+\frac{\eta^{3}L^{2}\nu}{4}\Sigma_{2n-1}+\frac{\eta^ {4}L^{3}\nu^{2}e^{2}}{4}\sum_{j=1}^{2n-1}\Sigma_{j-1}\] \[\qquad\leq\frac{\eta^{3}n^{2}L^{2}}{2}\left(1+\frac{2e^{2}}{3} \right)\left\|\bm{F}\bm{z}_{0}\right\|+\frac{\eta^{3}L^{2}\nu}{4}\Sigma_{2n-1} +\frac{\eta^{4}L^{3}\nu^{2}e^{2}}{4}\sum_{j=1}^{2n-2}\Sigma_{j}.\]

By the same logic, each summand in (62d) with \(j>0\) can be bounded as

\[\left\|D\bm{F}_{j}(\bm{z}_{0})\sum_{k=0}^{j-1}(\bm{F}_{k}\bm{w}_{ k}-\bm{F}_{k}\bm{z}_{0})\right\|\] \[\qquad\leq\left\|D\bm{F}_{j}(\bm{z}_{0})\right\|\sum_{k=0}^{j-1} \left\|\bm{F}_{k}\bm{w}_{k}-\bm{F}_{k}\bm{z}_{0}\right\|\] \[\qquad\leq L^{2}\sum_{k=0}^{j-1}\left\|\bm{w}_{k}-\bm{z}_{0}\right\|\] \[\qquad\leq L^{2}\left(\frac{\eta}{2}\left\|\bm{F}\bm{z}_{0} \right\|+\frac{\eta}{2}\delta_{1}\right)\] \[\qquad\qquad+L^{2}\sum_{k=1}^{j-1}\frac{\eta}{2}\left(1+2\nu^{2} k+\frac{\nu^{3}e^{2}k(k-1)}{n}\right)\left\|\bm{F}\bm{z}_{0}\right\|\] \[\qquad\qquad+L^{2}\sum_{k=1}^{j-1}\left(\frac{\eta}{2}\delta_{k+1 }+\frac{\eta(2\nu^{2}-1)}{2}\delta_{k}+\eta^{2}L\nu^{3}e^{2}\Sigma_{k-1}\right)\] \[\qquad=\frac{\eta L^{2}}{2}\left(\left\|\bm{F}\bm{z}_{0}\right\| +\delta_{1}\right)+\frac{\eta L^{2}}{2}\left(j-1+\nu^{2}j(j-1)+\frac{\nu^{3} e^{2}j(j-1)(j-2)}{3n}\right)\left\|\bm{F}\bm{z}_{0}\right\|\] \[\qquad\qquad+\frac{\eta L^{2}}{2}(\Sigma_{j}-\delta_{1})+\frac{ \eta L^{2}(2\nu^{2}-1)}{2}\Sigma_{j-1}+\eta^{2}L^{3}\nu^{3}e^{2}\sum_{k=1}^{j -1}\Sigma_{k-1}\] \[\qquad=\frac{\eta L^{2}}{2}\left(j+\nu^{2}j(j-1)+\frac{\nu^{3}e^ {2}j(j-1)(j-2)}{3n}\right)\left\|\bm{F}\bm{z}_{0}\right\|\] \[\qquad\qquad+\frac{\eta L^{2}}{2}\Sigma_{j}+\frac{\eta L^{2}(2 \nu^{2}-1)}{2}\Sigma_{j-1}+\eta^{2}L^{3}\nu^{3}e^{2}\sum_{k=1}^{j-1}\Sigma_{k-1},\]and when \(j=0\) the sum with respect to \(k\) becomes an empty sum. Thus, (62d) in total satisfies the bound

\[\left\|\frac{\eta^{2}}{2}\sum_{j=0}^{2n-1}D\bm{F}_{j}(\bm{z}_{0}) \sum_{k=0}^{j-1}(\bm{F}_{k}\bm{w}_{k}-\bm{F}_{k}\bm{z}_{0})\right\|\] \[\qquad\leq\frac{\eta^{2}}{2}\sum_{j=0}^{2n-1}\left\|D\bm{F}_{j}( \bm{z}_{0})\sum_{k=0}^{j-1}(\bm{F}_{k}\bm{w}_{k}-\bm{F}_{k}\bm{z}_{0})\right\|\] \[\qquad\leq\frac{\eta^{3}L^{2}}{4}\sum_{j=1}^{2n-1}\left(j+\nu^{2} j(j-1)+\frac{\nu^{3}e^{2}j(j-1)(j-2)}{3n}\right)\|\bm{F}\bm{z}_{0}\|\] \[\qquad\qquad+\frac{\eta^{2}}{2}\sum_{j=1}^{2n-1}\left(\frac{\eta L ^{2}}{2}\Sigma_{j}+\frac{\eta L^{2}(2\nu^{2}-1)}{2}\Sigma_{j-1}+\eta^{2}L^{3} \nu^{3}e^{2}\sum_{k=1}^{j-1}\Sigma_{k-1}\right)\] \[\qquad=\frac{\eta^{3}L^{2}}{4}\left(n(2n-1)+\frac{4\nu^{2}n(n-1) (2n-1)}{3}+\frac{\nu^{3}e^{2}(n-1)(2n-1)(2n-3)}{3}\right)\|\bm{F}\bm{z}_{0}\|\] \[\qquad\qquad+\frac{\eta^{3}L^{2}}{4}\sum_{j=1}^{2n-1}\Sigma_{j}+ \frac{\eta^{3}L^{2}(2\nu^{2}-1)}{4}\sum_{j=1}^{2n-1}\Sigma_{j-1}+\frac{\eta^{4 }L^{3}\nu^{3}e^{2}}{2}\sum_{j=1}^{2n-1}\sum_{k=1}^{j-1}\Sigma_{k-1}\] \[\qquad\leq\frac{\eta^{3}L^{2}}{2}\left(n^{2}+\frac{4n^{3}}{3}+ \frac{2e^{2}n^{3}}{3}\right)\|\bm{F}\bm{z}_{0}\|\] \[\qquad\qquad+\frac{\eta^{3}L^{2}}{4}\sum_{j=1}^{2n-1}\Sigma_{j}+ \frac{\eta^{3}L^{2}(2\nu^{2}-1)}{4}\sum_{j=1}^{2n-2}\Sigma_{j}+\frac{\eta^{4} L^{3}\nu^{3}e^{2}}{2}\sum_{k=1}^{2n-2}\sum_{j=k+1}^{2n-1}\Sigma_{k-1}\] \[\qquad\leq\eta^{3}n^{3}L^{2}\left(\frac{4\nu+e^{2}}{3}\right)\| \bm{F}\bm{z}_{0}\|\] \[\qquad\qquad+\frac{\eta^{3}L^{2}}{4}\Sigma_{2n-1}+\frac{\eta^{3} L^{2}\nu^{2}}{2}\sum_{j=1}^{2n-2}\Sigma_{j}+\frac{\eta^{4}L^{3}\nu^{3}e^{2}}{2} \sum_{k=1}^{2n-2}(2n-k-1)\Sigma_{k-1}.\]

Simply collecting all the inequalities and rearranging the terms leads to the claimed bound. 

Before we proceed, let us write

\[X_{1} \coloneqq\frac{3\eta^{3}M}{8}\left(\Psi_{2n}+(2\nu^{2}-1)^{2} \Psi_{2n-1}+\frac{4\nu^{6}e^{4}}{n^{2}}\sum_{j=1}^{2n-2}j\Psi_{j}\right),\] (64) \[X_{2} \coloneqq\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+\frac{\eta^ {3}L^{2}\nu^{2}(1+\eta Le^{2})}{2}\sum_{j=1}^{2n-2}\Sigma_{j}+\frac{\eta^{4}L ^{3}\nu^{3}e^{2}}{2}\sum_{k=1}^{2n-2}(2n-k-1)\Sigma_{k-1}\] (65)

so that the bound on \(\|\bm{r}\|\) obtained in Proposition E.11 can be written as

\[\|\bm{r}\|\leq\eta^{3}n^{3} \,L^{2}\left\|\bm{F}\bm{z}_{0}\right\|\left(\frac{1}{2n}\left(1+ \frac{2e^{2}}{3}\right)+\frac{4\nu+e^{2}}{3}\right)\] (66) \[+\eta^{3}n^{3}M\left\|\bm{F}\bm{z}_{0}\right\|^{2}\left(\frac{1}{ 2}+4\nu^{4}+\frac{16\nu e^{4}}{5}\right)\] \[+X_{1}+X_{2}.\]

**Theorem E.12**.: _Suppose that \(\eta<\frac{1}{nL}\), and let \(\nu\coloneqq 1+\frac{1}{2n}\). Then the noise term deterministically satisfies the bound_

\[\|\bm{r}\|\leq\eta^{3}n^{3}L^{2}\left\|\bm{F}\bm{z}_{0}\right\|\left( \frac{1}{2n}\left(1+\frac{2e^{2}}{3}\right)+\frac{4\nu+e^{2}}{3}+10\nu\rho\right)\] \[\qquad\qquad+\eta^{3}n^{3}M\left\|\bm{F}\bm{z}_{0}\right\|^{2} \left(\frac{1}{2}+4\nu^{4}+\frac{16\nu e^{4}}{5}+\rho^{2}\left(3\nu^{4}+8\nu^{ 3}e^{4}\right)\right)\] \[\qquad\qquad+\eta^{3}n^{3}M\sigma^{2}\left(3\nu^{4}+8\nu^{3}e^{4 }\right)+10\nu\eta^{3}n^{3}L^{2}\sigma.\]

Proof.: From (36), (37), and Lemma E.5, it holds that

\[\Sigma_{j} =\sum_{i=1}^{j}\delta_{i}\leq jn(\rho\left\|\bm{F}\bm{z}_{0}\right\| +\sigma),\] (67) \[\Psi_{j} =\sum_{i=1}^{j}\delta_{i}^{2}\leq jn^{2}(\rho\left\|\bm{F}\bm{z}_ {0}\right\|+\sigma)^{2}.\] (68)

Plugging the bound for \(\Psi_{j}\) into (64) we get

\[X_{1} \leq\frac{3\eta^{3}M}{8}\Bigg{(}2n^{3}(\rho\left\|\bm{F}\bm{z}_{0 }\right\|+\sigma)^{2}+(2\nu^{2}-1)^{2}(2n-1)n^{2}(\rho\left\|\bm{F}\bm{z}_{0} \right\|+\sigma)^{2}+4\nu^{6}e^{4}\sum_{j=1}^{2n-2}j^{2}(\rho\left\|\bm{F}\bm{ z}_{0}\right\|+\sigma)^{2}\Bigg{)}\] \[=\frac{3\eta^{3}M}{8}\left(\left(2n^{3}+(2\nu^{2}-1)^{2}(2n-1)n^{ 2}\right)+\frac{4\nu^{6}e^{4}}{3}(n-1)(2n-1)(4n-3)\right)(\rho\left\|\bm{F}\bm {z}_{0}\right\|+\sigma)^{2}\] \[\leq\frac{3\eta^{3}M}{8}\left(4\nu^{4}n^{3}+\frac{32\nu^{3}e^{4} n^{3}}{3}\right)(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}\] \[=\frac{\eta^{3}n^{3}M(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma) ^{2}}{2}\left(3\nu^{4}+8\nu^{3}e^{4}\right).\]

By Young's inequality, it holds that

\[\frac{(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}\leq\rho^{2}\left\| \bm{F}\bm{z}_{0}\right\|^{2}+\sigma^{2},\]

from which we get

\[X_{1}\leq\eta^{3}n^{3}M\rho^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}\left(3\nu^{ 4}+8\nu^{3}e^{4}\right)+\eta^{3}n^{3}M\sigma^{2}\left(3\nu^{4}+8\nu^{3}e^{4} \right).\] (69)

Meanwhile, plugging the bound for \(\Sigma_{j}\) into (65) we get

\[X_{2} \leq\frac{\eta^{3}L^{2}(\nu+1)}{4}(2n-1)n(\rho\left\|\bm{F}\bm{z}_ {0}\right\|+\sigma)+\frac{\eta^{3}L^{2}\nu^{2}(1+\eta Le^{2})}{2}\sum_{j=1}^{ 2n-2}jn(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)\] \[\qquad\qquad+\frac{\eta^{4}L^{3}\nu^{3}e^{2}}{2}\sum_{k=1}^{2n-2} (2n-k-1)(k-1)n(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)\] \[=\frac{\eta^{3}L^{2}(\nu+1)}{4}(2n-1)n(\rho\left\|\bm{F}\bm{z}_{0} \right\|+\sigma)+\frac{\eta^{3}L^{2}\nu^{2}(1+\eta Le^{2})}{2}(n-1)(2n-1)n( \rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)\] \[\qquad\qquad+\frac{\eta^{4}L^{3}\nu^{3}e^{2}}{6}\left(-3+11n-12n ^{2}+4n^{3}\right)n(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)\] \[\leq\eta^{3}L^{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma) \left(n^{2}+(1+\eta Le^{2})n^{3}+\frac{2\eta Le^{2}}{3}n^{4}\right)\] \[\leq\eta^{3}n^{3}L^{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma) \left(\frac{1}{n}+1+\frac{e^{2}}{n}+\frac{2e^{2}}{3}\right)\]

where in the last line we used that \(\eta<\frac{1}{nL}\). Because the inequality

\[\frac{1}{n}+1+\frac{e^{2}}{n}+\frac{2e^{2}}{3}\leq 10\nu\]holds for all \(n\geq 1\), continuing from above we obtain

\[\begin{split} X_{2}&\leq 10\nu\eta^{3}n^{3}L^{2}(\rho \left\|\bm{F}\bm{z}_{0}\right\|+\sigma)\\ &\leq 10\nu\eta^{3}n^{3}L^{2}\rho\left\|\bm{F}\bm{z}_{0}\right\|+10 \nu\eta^{3}n^{3}L^{2}\sigma.\end{split}\] (70)

Rearranging (66) with applying the bounds (69) and (70) gives us the claimed result. 

Proof of Theorem e.9.: As \(n\geq 1\), we notice that \(\nicefrac{{1}}{{n}}\leq 1\) and \(\nu\leq\nicefrac{{3}}{{2}}\) where \(\nu=1+\frac{1}{2n}\) following the notation of Theorem E.12. Then the bound (58) is immediate from Theorem E.12. 

#### e.2.2 Proof of Equation (32) for SEG-FFA

In this section, we prove the following.

**Theorem E.13**.: _Say we use SEG-FFA Then, as long as the stepsize used in an epoch satisfies \(\eta<\frac{1}{nL}\), it holds that_

\[\mathbb{E}\left[\left\|\bm{r}\right\|^{2}\left|\bm{z}_{0}\right|\right]\leq \eta^{6}n^{6}C_{\bm{\mathcal{Z}}\bm{\mathcal{A}}}\left\|\bm{F}\bm{z}_{0} \right\|^{2}+\eta^{6}n^{6}D_{\bm{\mathcal{Z}}\bm{\mathcal{A}}}\left\|\bm{F} \bm{z}_{0}\right\|^{4}+\eta^{6}n^{5}V_{\bm{\mathcal{Z}}\bm{\mathcal{A}}}\] (71)

_for constants_

\[C_{\bm{\mathcal{Z}}\bm{\mathcal{A}}} \coloneqq 4L^{4}\left(\left(\frac{1}{2}\left(1+\frac{2e^{2}}{3} \right)+\frac{6+e^{2}}{3}\right)^{2}+36\rho^{2}e^{4}\right),\] (72) \[D_{\bm{\mathcal{Z}}\bm{\mathcal{A}}} \coloneqq 4M^{2}\left(\left(\frac{83}{4}+\frac{24e^{4}}{5} \right)^{2}+\rho^{4}\left(\frac{243}{16}+27e^{4}\right)^{2}\right),\] (73) \[V_{\bm{\mathcal{Z}}\bm{\mathcal{A}}} \coloneqq 4M^{2}\sigma^{4}\left(\frac{243}{16}+27e^{4}\right)^{2}+144e^ {4}L^{4}\sigma^{2}.\] (74)

Proof.: The bound is then immediate from the following Theorem E.14, as \(n\geq 1\) implies \(\nicefrac{{1}}{{n}}\leq 1\) and \(\nu\leq\nicefrac{{3}}{{2}}\) for \(\nu\) defined in the statement of Theorem E.14. 

**Theorem E.14**.: _Suppose that \(\eta<\frac{1}{nL}\), and let \(\nu\coloneqq 1+\frac{1}{2n}\). Then, in expectation, the noise term satisfies the bound_

\[\mathbb{E}\left[\left\|\bm{r}\right\|^{2}\left|\bm{z}_{0}\right| \right]\leq 4\eta^{6}n^{6}L^{4}\left\|\bm{F}\bm{z}_{0}\right\|^{2}\left( \left(\frac{1}{2n}\left(1+\frac{2e^{2}}{3}\right)+\frac{4\nu+e^{2}}{3}\right) ^{2}+\frac{36\rho^{2}e^{4}}{n}\right)\] \[\qquad+4\eta^{6}n^{6}M^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{4} \left(\left(\frac{1}{2}+4\nu^{4}+\frac{16\nu e^{4}}{5}\right)^{2}+\frac{\rho^ {4}\left(3\nu^{4}+8\nu^{3}e^{4}\right)^{2}}{n}\right)\] \[\qquad+4\eta^{6}n^{5}M^{2}\sigma^{4}\left(3\nu^{4}+8\nu^{3}e^{4} \right)^{2}+144e^{4}\eta^{6}n^{5}L^{4}\sigma^{2}.\]

Proof.: Notice that, when conditioned on \(\bm{z}_{0}\), the only source of randomness included in \(\Psi_{j}\) is the random permutation \(\tau\) selected for the epoch. Hence, we can use Lemma E.6 to get

\[\mathbb{E}\left[\Psi_{j}\left|\bm{z}_{0}\right|=\mathbb{E}\left[\sum_{i=1}^{j} \delta_{i}^{2}\left|\bm{z}_{0}\right]=\sum_{i=1}^{j}\mathbb{E}\left[\delta_{i }^{2}\left|\bm{z}_{0}\right|\right]\leq\frac{jn(\rho\left\|\bm{F}\bm{z}_{0} \right\|+\sigma)^{2}}{2}.\]

Applying Young's inequality on (66) we get

\[\begin{split}\left\|\bm{r}\right\|^{2}\leq 4\eta^{6}n^{6}L^{4} \left\|\bm{F}\bm{z}_{0}\right\|^{2}\left(\frac{1}{2n}\left(1+\frac{2e^{2}}{3} \right)+\frac{4\nu+e^{2}}{3}\right)^{2}\\ +4\eta^{6}n^{6}M^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{4}\left(\frac {1}{2}+4\nu^{4}+\frac{16\nu e^{4}}{5}\right)^{2}\\ +4X_{1}^{2}+4X_{2}^{2}.\end{split}\] (75)

When conditioned on \(\bm{z}_{0}\), the first two lines are not random quantities. Thus, it suffices to derive the bounds for \(\mathbb{E}\left[X_{i}^{2}\left|\bm{z}_{0}\right|\right]\), \(i=1,2\).

Recall that the bound (69) on \(X_{1}\) holds _deterministically_. Hence, it holds that

\[\mathbb{E}\left[X_{1}^{2}\,|\,\bm{z}_{0}\right] \leq\mathbb{E}\left[X_{1}\left(\eta^{3}n^{3}M\rho^{2}\left\|\bm{F} \bm{z}_{0}\right\|^{2}\left(3\nu^{4}+8\nu^{3}e^{4}\right)+\eta^{3}n^{3}M\sigma ^{2}\left(3\nu^{4}+8\nu^{3}e^{4}\right)\right)\Big{|}\,\bm{z}_{0}\right]\] \[=\eta^{3}n^{3}M\left(3\nu^{4}+8\nu^{3}e^{4}\right)\left(\rho^{2} \left\|\bm{F}\bm{z}_{0}\right\|^{2}+\sigma^{2}\right)\mathbb{E}\left[X_{1} \,|\,\bm{z}_{0}\right].\]

Now, to compute \(\mathbb{E}\left[X_{1}\,|\,\bm{z}_{0}\right]\), we apply the linearity of expectation on (64) to get

\[\mathbb{E}\left[X_{1}\,|\,\bm{z}_{0}\right]\] \[\leq\frac{3\eta^{3}M}{8}\Bigg{(}n^{2}(\rho\left\|\bm{F}\bm{z}_{0 }\right\|+\sigma)^{2}+\frac{(2\nu^{2}-1)^{2}(2n-1)n(\rho\left\|\bm{F}\bm{z}_{ 0}\right\|+\sigma)^{2}}{2}+\frac{4\nu^{6}e^{4}}{n^{2}}\sum_{j=1}^{2n-2}\frac{ j^{2}n(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}\Bigg{)}\] \[=\frac{3\eta^{3}M}{8}\left(\frac{2n^{2}+(2\nu^{2}-1)^{2}(2n^{2}- n)}{2}+\frac{2\nu^{6}e^{4}(n-1)(2n-1)(4n-3)}{3n}\right)(\rho\left\|\bm{F}\bm{z}_{0 }\right\|+\sigma)^{2}\] \[\leq\frac{3\eta^{3}M}{8}\left(2\nu^{4}n^{2}+\frac{16\nu^{3}e^{4} n^{2}}{3}\right)(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}\] \[=\frac{\eta^{3}n^{2}M(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma) ^{2}}{4}\left(3\nu^{4}+8\nu^{3}e^{4}\right).\]

Young's inequality gives us the bound

\[\frac{(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}\leq\rho^{2}\left\| \bm{F}\bm{z}_{0}\right\|^{2}+\sigma^{2}\] (76)

which, with the inequality derived above, leads to

\[\mathbb{E}\left[X_{1}\,|\,\bm{z}_{0}\right] \leq\frac{\eta^{3}n^{2}M}{2}\left(3\nu^{4}+8\nu^{3}e^{4}\right) \left(\rho^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\sigma^{2}\right).\]

As a consequence, with using Young's inequality once again, we obtain

\[\mathbb{E}\left[X_{1}^{2}\,|\,\bm{z}_{0}\right] \leq\frac{\eta^{6}n^{5}M^{2}}{2}\left(3\nu^{4}+8\nu^{3}e^{4} \right)^{2}\left(\rho^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\sigma^{2}\right) ^{2}\] (77) \[\leq\eta^{6}n^{5}M^{2}\left(3\nu^{4}+8\nu^{3}e^{4}\right)^{2} \left(\rho^{4}\left\|\bm{F}\bm{z}_{0}\right\|^{4}+\sigma^{4}\right).\]

To get the bound of \(\mathbb{E}\left[X_{2}^{2}\,|\,\bm{z}_{0}\right]\), we begin by using

\[\eta L\nu^{2}(2n-k-1) \leq\left(1+\frac{1}{2n}\right)^{2}\frac{2n-k-1}{n}\] \[=-\frac{k}{4n^{3}}-\frac{k}{n^{2}}-\frac{k}{n}-\frac{1}{4n^{3}}- \frac{1}{2n^{2}}+\frac{1}{n}+2\,\leq\,2,\]

which holds for all \(1\leq k\leq 2n-2\), to (65) to obtain

\[X_{2} \leq\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+\frac{\eta^{3}L^ {2}\nu^{2}(1+\eta Le^{2})}{2}\sum_{j=1}^{2n-2}\Sigma_{j}+\frac{\eta^{3}L^{2} \nu^{3}e^{2}}{2}\sum_{k=1}^{2n-2}\frac{2n-k-1}{n}\Sigma_{k-1}\] \[\leq\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+\frac{\eta^{3}L^ {2}\nu^{2}(1+\eta Le^{2})}{2}\sum_{j=1}^{2n-2}\Sigma_{j}+\eta^{3}L^{2}\nu e^{ 2}\sum_{k=1}^{2n-2}\Sigma_{k-1}\] \[\leq\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+\left(\frac{\eta ^{3}L^{2}\nu^{2}(1+\eta Le^{2})}{2}+\eta^{3}L^{2}\nu e^{2}\right)\sum_{j=1}^{2n -2}\Sigma_{j}\] \[\leq\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+3\eta^{3}L^{2}e^{ 2}\sum_{j=1}^{2n-2}\Sigma_{j}.\]Then we directly square both sides and expand them to get

\[X_{2}^{2} \leq\left(\frac{\eta^{3}L^{2}(\nu+1)}{4}\Sigma_{2n-1}+3\eta^{3}L^{2} e^{2}\sum_{j=1}^{2n-2}\Sigma_{j}\right)^{2}\] \[=\frac{\eta^{6}L^{4}(\nu+1)^{2}}{16}\Sigma_{2n-1}^{2}+9\eta^{6}L^{ 4}e^{4}\left(\sum_{j=1}^{2n-2}\Sigma_{j}\right)^{2}+\frac{3\eta^{6}L^{4}e^{2}( \nu+1)}{2}\sum_{j=1}^{2n-2}\Sigma_{2n-1}\Sigma_{j}.\]

Here, using Lemma E.7 and Lemma E.8 on the right hand side leads to

\[\mathbb{E}\left[X_{2}^{2}\,\middle|\,\bm{z}_{0}\right] \leq\frac{\eta^{6}L^{4}(\nu+1)^{2}n(2n-1)^{2}(\rho\left\|\bm{F} \bm{z}_{0}\right\|+\sigma)^{2}}{32}+\frac{9\eta^{6}L^{4}e^{4}n(2n-2)^{2}(2n- 1)^{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{8}\] \[\qquad+\frac{3\eta^{6}L^{4}e^{2}(\nu+1)}{2}\sum_{j=1}^{2n-2} \frac{jn(2n-1)(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{2}\] \[\leq\frac{\eta^{6}L^{4}(\nu+1)^{2}n(2n-1)^{2}(\rho\left\|\bm{F} \bm{z}_{0}\right\|+\sigma)^{2}}{32}+\frac{9\eta^{6}L^{4}e^{4}n(2n-2)^{2}(2n- 1)^{2}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{8}\] \[\qquad+\frac{3\eta^{6}L^{4}e^{2}(\nu+1)n(n-1)(2n-1)^{2}(\rho\left\| \bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{4}\] \[\leq\frac{\eta^{6}L^{4}n^{3}(\rho\left\|\bm{F}\bm{z}_{0}\right\| +\sigma)^{2}}{2}+\frac{9\eta^{6}L^{4}e^{4}n(2n-2)^{2}(2n-1)^{2}(\rho\left\| \bm{F}\bm{z}_{0}\right\|+\sigma)^{2}}{8}\] \[\qquad+6\eta^{6}L^{4}e^{2}n^{3}(n-1)(\rho\left\|\bm{F}\bm{z}_{0} \right\|+\sigma)^{2}\] \[=\eta^{6}L^{4}\left(\frac{n^{3}}{2}+\frac{9e^{4}n(2n-2)^{2}(2n-1) ^{2}}{8}+6e^{2}n^{3}(n-1)\right)(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)^{2}\] \[\leq 18e^{4}\eta^{6}L^{4}n^{5}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+ \sigma)^{2}.\]

As a consequence, with using (76) once again, we obtain

\[\mathbb{E}\left[X_{2}^{2}\,\middle|\,\bm{z}_{0}\right]\leq 36e^{4}\eta^{6}L^{4}n^{5 }\left(\rho^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\sigma^{2}\right).\] (78)

Taking the conditional expectation on (75), applying the bounds (77) and (78), and then rearranging the terms leads to the claimed inequality. 

#### e.2.3 Upper Bounds of the Within-Epoch Errors for SEG-Ff

**Theorem E.15**.: _Say we use SEG-FF with \(\alpha=\beta=\nicefrac{{\eta}}{{2}}\). Then, as long as the stepsize used in an epoch satisfies \(\eta<\frac{1}{nL}\), it holds that_

\[\left\|\bm{r}\right\| \leq\eta^{2}n^{2}C_{\text{1F}}\left\|\bm{F}\bm{z}_{0}\right\|+ \eta^{2}n^{2}D_{\text{1F}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{2}n^{2}V_{ \text{1F}}\] \[\mathbb{E}\left[\left\|\bm{r}\right\|^{2}\,\middle|\,\bm{z}_{0}\right] \leq\eta^{4}n^{4}C_{\text{2F}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+ \eta^{4}n^{4}D_{\text{2F}}\left\|\bm{F}\bm{z}_{0}\right\|^{4}+\eta^{4}n^{3}V_{ \text{2F}}\]

_for constants \(C_{\text{1F}}\), \(D_{\text{1F}}\), \(V_{\text{1F}}\), \(C_{\text{2F}}\), \(D_{\text{2F}}\), and \(V_{\text{2F}}\) to be determined later in (81) and (82)._

Proof.: As we have discussed in Section 5.1, we already know that aiming to achieve \(\mathcal{O}(\eta^{3})\) error without anchoring is futile. Instead, we show that error of magnitude \(\mathcal{O}(\eta^{2})\) is possible with the chosen stepsizes.

By Proposition D.2 and Lemma D.4 we have For any \(i=0,1,\ldots,N\), it holds that

\[\bm{z}_{2n} =\bm{z}_{0}-\frac{\eta}{2}\sum_{j=0}^{2n-1}\bm{T}_{j}\bm{z}_{0}+ \frac{\eta^{2}}{4}\sum_{j=0}^{2n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+ \frac{\eta^{2}}{4}\sum_{0\leq k<j\leq 2n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{k}\bm{z}_{0}+\bm{ \epsilon}_{2n}\] \[=\bm{z}_{0}-\eta\sum_{j=0}^{n-1}\bm{F}_{j}\bm{z}_{0}+\frac{3\eta^{ 2}}{4}\sum_{j=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\frac{\eta^{2}}{ 2}\sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i}\bm{z}_{0}+\bm{\epsilon}_{2n}\] \[=\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0}+\eta^{2}n^{2}D\bm{F}(\bm{z}_{0}) \bm{F}\bm{z}_{0}-\frac{\eta^{2}}{4}\sum_{j=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j }\bm{z}_{0}-\frac{\eta^{2}}{2}\sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i} \bm{z}_{0}+\bm{\epsilon}_{2n}\]where we denote

\[\begin{split}\bm{\epsilon}_{2n}\coloneqq&-\frac{\eta}{2} \sum_{j=0}^{2n-1}\left(\bm{F}_{j}\bm{w}_{j}-\bm{F}_{j}\bm{z}_{0}-D\bm{F}_{j}( \bm{z}_{0})(\bm{w}_{j}-\bm{z}_{0})\right)\\ &\quad+\frac{\eta^{2}}{4}\sum_{j=0}^{2n-1}D\bm{F}_{j}(\bm{z}_{0}) (\bm{F}_{j}\bm{z}_{j}-\bm{F}_{j}\bm{z}_{0})+\frac{\eta^{2}}{4}\sum_{j=0}^{2n-1 }D\bm{F}_{j}(\bm{z}_{0})\sum_{k=0}^{j-1}(\bm{F}_{k}\bm{w}_{k}-\bm{F}_{k}\bm{z}_ {0}).\end{split}\] (79)

Comparing \(\bm{z}_{2n}\) to a point that would have been the result of a deterministic EG update with stepsize \(\eta n\) we get

\[\begin{split}\bm{z}_{2n}-(\bm{z}_{0}-\eta n\bm{F}(\bm{z}_{0}- \eta n\bm{F}\bm{z}_{0}))=\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0})-\eta n \bm{F}\bm{z}_{0}+\eta^{2}n^{2}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}+\bm{ \epsilon}_{2n}\\ -\frac{\eta^{2}}{4}\sum_{j=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{ j}\bm{z}_{0}-\frac{\eta^{2}}{2}\sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i} \bm{z}_{0}.\end{split}\]

Let us define

\[\tilde{\bm{r}}\coloneqq\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0})-\eta n \bm{F}\bm{z}_{0}+\eta^{2}n^{2}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}+\bm{\epsilon }_{2n}.\] (80)

Noticing the resemblence between (62) and the equations in (79) and (80), we can repeat the same reasoning used for Theorem E.9 and Theorem E.13, but with replacing the bounds given by Proposition E.3 to those in Proposition E.4 (and plugging in \(\nicefrac{{\eta}}{{2}}\) in place of \(\eta\) in the statement of Proposition E.4) to conclude that

\[\begin{split}\|\tilde{\bm{r}}\|&\leq\eta^{3}n^{3} \tilde{C}_{\mathsf{IA}}\left\|\bm{F}\bm{z}_{0}\right\|+\eta^{3}n^{3}\tilde{D}_{ \mathsf{IA}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{3}n^{3}\tilde{V}_{ \mathsf{IA}}\\ \mathbb{E}\left[\left\|\tilde{\bm{r}}\right\|^{2}\Big{|}\bm{z}_{0 }\right]&\leq\eta^{6}n^{6}\tilde{C}_{\mathsf{IA}}\left\|\bm{F}\bm{z }_{0}\right\|^{2}+\eta^{6}n^{6}\tilde{D}_{\mathsf{IA}}\left\|\bm{F}\bm{z}_{0} \right\|^{4}+\eta^{6}n^{5}\tilde{V}_{\mathsf{ZA}}\end{split}\]

for some constants \(\tilde{C}_{\mathsf{IA}}\), \(\tilde{D}_{\mathsf{IA}}\), \(\tilde{V}_{\mathsf{IA}}\), \(\tilde{C}_{\mathsf{ZA}}\), \(\tilde{D}_{\mathsf{ZA}}\), and \(\tilde{V}_{\mathsf{ZA}}\). Meanwhile, we also have

\[\begin{split}\left\|\frac{\eta^{2}}{4}\sum_{j=1}^{n}D\bm{F}_{j}( \bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\frac{\eta^{2}}{2}\sum_{i\neq j}D\bm{F}_{j}( \bm{z}_{0})\bm{F}_{i}\bm{z}_{0}\right\|\\ &\quad=\left\|\frac{\eta^{2}n^{2}}{2}D\bm{F}(\bm{z}_{0})\bm{F} \bm{z}_{0}-\frac{\eta^{2}}{4}\sum_{j=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j} \bm{z}_{0}\right\|\\ &\quad\leq\frac{\eta^{2}n^{2}}{2}\left\|D\bm{F}(\bm{z}_{0})\right\| \left\|\bm{F}\bm{z}_{0}\right\|+\frac{\eta^{2}}{4}\sum_{j=1}^{n}\left\|D\bm{F} _{j}(\bm{z}_{0})\right\|\left\|\bm{F}_{j}\bm{z}_{0}\right\|\\ &\quad\leq\frac{\eta^{2}n^{2}}{2}L\left\|\bm{F}\bm{z}_{0}\right\| +\frac{\eta^{2}}{4}\sum_{j=1}^{n}L\left(\left\|\bm{F}_{j}\bm{z}_{0}-\bm{F}\bm{z }_{0}\right\|+\left\|\bm{F}\bm{z}_{0}\right\|\right)\\ &\quad\leq\frac{\eta^{2}(n^{2}+n)L}{2}\left\|\bm{F}\bm{z}_{0} \right\|+\frac{\eta^{2}L}{4}\sum_{j=1}^{n}\left\|\bm{F}_{j}\bm{z}_{0}-\bm{F} \bm{z}_{0}\right\|\\ &\quad\leq\eta^{2}n^{2}L\left\|\bm{F}\bm{z}_{0}\right\|+\frac{ \eta^{2}L}{4}\left(\sum_{j=1}^{n}\left\|\bm{F}_{j}\bm{z}_{0}-\bm{F}\bm{z}_{0} \right\|^{2}\right)^{1/2}\left(\sum_{j=1}^{n}1\right)^{1/2}\\ &\quad=\eta^{2}n^{2}L\left\|\bm{F}\bm{z}_{0}\right\|+\frac{\eta^{2 }nL}{4}(\rho\left\|\bm{F}\bm{z}_{0}\right\|+\sigma)\end{split}\]

where in the second to the last line we used the Cauchy-Schwarz inequality. Therefore, as \(\eta\leq\nicefrac{{1}}{{nL}}\), we conclude that

\[\left\|\bm{z}_{2n}-(\bm{z}_{0}-\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0})) \right\|\leq\eta^{2}n^{2}C_{\mathsf{IF}}\left\|\bm{F}\bm{z}_{0}\right\|+\eta^{2 }n^{2}D_{\mathsf{IF}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{2}n^{2}V_{ \mathsf{IF}}\]

for constants

\[C_{\mathsf{IF}}=L+\frac{\rho L}{4}+\frac{\tilde{C}_{\mathsf{IA}}}{L},\quad D_{ \mathsf{IF}}=\frac{\tilde{D}_{\mathsf{IA}}}{L},\quad V_{\mathsf{IF}}=\frac{ \sigma L}{4}+\frac{\tilde{V}_{\mathsf{IA}}}{L}.\] (81)Moreover, using Young's inequality, we see that

\[\left\|\frac{\eta^{2}}{4}\sum_{j=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_ {j}\bm{z}_{0}+\frac{\eta^{2}}{2}\sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i} \bm{z}_{0}\right\|^{2}\] \[\qquad\leq 3\eta^{4}n^{4}L^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+ \frac{3\eta^{4}n^{2}L^{2}}{16}\rho^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\frac {3\eta^{4}n^{2}L^{2}}{16}\sigma^{2},\]

so we also conclude that

\[\mathbb{E}\left[\left\|\bm{z}_{2n}-\left(\bm{z}_{0}-\eta n\bm{F}(\bm{z}_{0}- \eta n\bm{F}\bm{z}_{0})\right)\right\|^{2}\Big{|}\bm{z}_{0}\right]\leq\eta^{ 4}n^{4}C_{\text{2F}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{4}n^{4}D_{\text {2F}}\left\|\bm{F}\bm{z}_{0}\right\|^{4}+\eta^{4}n^{3}V_{\text{2F}}\]

holds for constants

\[C_{\text{2F}}=6L^{2}+\frac{3\rho^{2}L^{2}}{8}+\frac{2\tilde{C}_{\text{2A}}}{L ^{2}},\quad D_{\text{2F}}=\frac{2\tilde{D}_{\text{1A}}}{L^{2}},\quad V_{\text {2F}}=\frac{3\sigma^{2}L^{2}}{8}+\frac{2\tilde{V}_{\text{1A}}}{L^{2}}.\] (82)

#### e.2.4 Upper Bounds of the Within-Epoch Errors for SEG-RR

**Theorem E.16**.: _Say we use SEG-RR with \(\alpha=\beta=\eta\). Then, as long as the stepsize used in an epoch satisfies \(\eta<\frac{1}{nL}\), it holds that_

\[\left\|\bm{r}\right\| \leq\eta^{2}n^{2}C_{\text{1R}}\left\|\bm{F}\bm{z}_{0}\right\|+ \eta^{2}n^{2}D_{\text{1R}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{2}n^{2}V_{ \text{1R}}\] \[\mathbb{E}\left[\left\|\bm{r}\right\|^{2}\Big{|}\bm{z}_{0}\right] \leq\eta^{4}n^{4}C_{\text{2R}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+ \eta^{4}n^{4}D_{\text{2R}}\left\|\bm{F}\bm{z}_{0}\right\|^{4}+\eta^{4}n^{3}V_{ \text{2R}}\]

_for constants \(C_{\text{1R}}\), \(D_{\text{1R}}\), \(V_{\text{1R}}\), \(C_{\text{2R}}\), \(D_{\text{2R}}\), and \(V_{\text{2R}}\) to be determined later in (86) and (87)._

Proof.: As we have discussed in Section 5.1, we already know that aiming to achieve \(\mathcal{O}(\eta^{3})\) error with only using random reshuffling is futile. Instead, we show that error of magnitude \(\mathcal{O}(\eta^{2})\) is possible with the chosen stepsizes.

By Proposition D.2 and Lemma D.4 we have For any \(i=0,1,\ldots,N\), it holds that

\[\bm{z}_{n} =\bm{z}_{0}-\eta\sum_{j=0}^{n-1}\bm{F}_{j}\bm{z}_{0}+\eta^{2} \sum_{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\eta^{2}\sum_{0\leq k <j\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0}+\bm{\epsilon}_{n}\] \[=\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0}+\eta^{2}n^{2}D\bm{F}(\bm{z}_{0 })\bm{F}\bm{z}_{0}-\eta^{2}\sum_{0\leq j<k\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{ F}_{k}\bm{z}_{0}+\bm{\epsilon}_{n}\]

where we denote

\[\bm{\epsilon}_{n}\coloneqq -\eta\sum_{j=0}^{n-1}\Bigl{(}\bm{F}_{j}\bm{w}_{j}-\bm{F}_{j}\bm{z }_{0}-D\bm{F}_{j}(\bm{z}_{0})(\bm{w}_{j}-\bm{z}_{0})\Bigr{)}\] (83) \[\quad+\eta^{2}\sum_{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0})(\bm{F}_{j} \bm{z}_{j}-\bm{F}_{j}\bm{z}_{0})+\eta^{2}\sum_{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0 })\sum_{k=0}^{j-1}(\bm{F}_{k}\bm{w}_{k}-\bm{F}_{k}\bm{z}_{0}).\]

Comparing \(\bm{z}_{n}\) to a point that would have been the result of a deterministic EG update with stepsize \(\eta n\) we get

\[\bm{z}_{n}-(\bm{z}_{0}-\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_ {0}))=\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0})-\eta n\bm{F}\bm{z}_{0}+ \eta^{2}n^{2}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}+\bm{\epsilon}_{n}\] \[\qquad\qquad\qquad-\eta^{2}\sum_{0\leq j<k\leq n-1}D\bm{F}_{j}(\bm {z}_{0})\bm{F}_{k}\bm{z}_{0}.\]

Let us define

\[\tilde{\bm{r}}\coloneqq\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0})-\eta n \bm{F}\bm{z}_{0}+\eta^{2}n^{2}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}+\bm{ \epsilon}_{n}.\] (84)

Comparing the sums (62b)-(62d) to (83), we can repeat the same reasoning used for Theorem E.9 and Theorem E.13, but with replacing the bounds given by Proposition E.3 to those in Proposition E.4, to conclude that

\[\left\|\tilde{\bm{r}}\right\| \leq\eta^{3}n^{3}\tilde{C}_{\text{1A}}\left\|\bm{F}\bm{z}_{0} \right\|+\eta^{3}n^{3}\tilde{D}_{\text{1A}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+ \eta^{3}n^{3}\tilde{V}_{\text{1A}}\] \[\mathbb{E}\left[\left\|\tilde{\bm{r}}\right\|^{2}\Big{|}\bm{z}_{0 }\right] \leq\eta^{6}n^{6}\tilde{C}_{\text{2A}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+ \eta^{6}n^{6}\tilde{D}_{\text{2A}}\left\|\bm{F}\bm{z}_{0}\right\|^{4}+\eta^{6}n^{5} \tilde{V}_{\text{2A}}\]for some constants \(\hat{C}_{\text{1A}}\), \(\hat{D}_{\text{1A}}\), \(\hat{V}_{\text{1A}}\), \(\hat{C}_{\text{2A}}\), \(\hat{D}_{\text{2A}}\), and \(\hat{V}_{\text{2A}}\). Meanwhile, we also have

\[\sum_{0\leq j<k\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k}\bm{z}_{0} =\sum_{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0})(n\bm{F}\bm{z}_{0}-\bm{g}_ {j+1})\] \[=\sum_{j=0}^{n-1}(n-j-1)D\bm{F}_{j}(\bm{z}_{0})\bm{F}\bm{z}_{0}- \sum_{j=0}^{n-1}D\bm{F}_{j}(\bm{z}_{0})(\bm{g}_{j+1}-(j+1)\bm{F}\bm{z}_{0})\]

which leads to

\[\left\|\sum_{0\leq j<k\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k} \bm{z}_{0}\right\| \leq\sum_{j=0}^{n-1}(n-j-1)L\left\|\bm{F}\bm{z}_{0}\right\|+L \sum_{j=0}^{n-1}\delta_{j+1}\] (85) \[\leq\frac{n^{2}L}{2}\left\|\bm{F}\bm{z}_{0}\right\|+L\sum_{j=0}^ {n-1}\delta_{j+1}.\]

Therefore, from \(\eta\leq\nicefrac{{1}}{{nL}}\) and Lemma E.5, on one hand we obtain

\[\left\|\bm{z}_{n}-(\bm{z}_{0}-\eta n\bm{F}(\bm{z}_{0}-\eta n\bm{F}\bm{z}_{0}) )\right\|\leq\eta^{2}n^{2}C_{\text{1R}}\left\|\bm{F}\bm{z}_{0}\right\|+\eta^{ 2}n^{2}D_{\text{1R}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{2}n^{2}V_{\text {1R}}\]

for constants

\[C_{\text{1R}}=\frac{L}{2}+\rho L+\frac{\hat{C}_{\text{1A}}}{L},\quad D_{\text{ 1R}}=\frac{\hat{D}_{\text{1A}}}{L},\quad V_{\text{1R}}=\sigma L+\frac{\hat{V}_ {\text{1A}}}{L}.\] (86)

On the other hand, applying Young's inequality on (85) we get

\[\left\|\sum_{0\leq j<k\leq n-1}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{k} \bm{z}_{0}\right\|^{2} \leq n^{4}L^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+2L^{2}\left( \sum_{j=0}^{n-1}\delta_{j+1}\right)^{2}\] \[\leq n^{4}L^{2}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+2nL^{2}\sum_{ j=1}^{n}\delta_{j}^{2}.\]

Taking the expectation conditioned on \(\bm{z}_{0}\) and applying Lemma E.6, we conclude that

\[\mathbb{E}\left[\left\|\bm{z}_{2n}-(\bm{z}_{0}-\eta n\bm{F}(\bm{z}_{0}-\eta n \bm{F}\bm{z}_{0}))\right\|^{2}\Big{|}\bm{z}_{0}\right]\leq\eta^{4}n^{4}C_{ \text{2R}}\left\|\bm{F}\bm{z}_{0}\right\|^{2}+\eta^{4}n^{4}D_{\text{2R}}\left\| \bm{F}\bm{z}_{0}\right\|^{4}+\eta^{4}n^{3}V_{\text{2R}}\]

holds for constants

\[C_{\text{2R}}=2L^{2}+4\rho^{2}L^{2}+\frac{2\hat{C}_{\text{2A}}}{L^{2}},\quad D _{\text{2R}}=\frac{2\hat{D}_{\text{2A}}}{L^{2}},\quad V_{\text{2R}}=4\sigma^{ 2}L^{2}+\frac{2\hat{V}_{\text{2A}}}{L^{2}}.\] (87)

## Appendix F Convergence Bounds in the Strongly Monotone Setting

In this section, we focus only on the iterates \(\{\bm{z}_{0}^{k}\}_{k\geq 0}\). So, we omit the subscript \(0\) unless necessary, and simply write \(\bm{z}^{k}\) instead of \(\bm{z}_{0}^{k}\).

### Unified Analysis of the Upper Bounds for Shuffling-Based SEG Methods

When \(\bm{F}\) is \(\mu\)-strongly monotone with \(\mu>0\), all of SEG-RR, SEG-FF, and SEG-FFA do not diverge. In fact, it is possible to establish the following unified analysis of the methods.

**Theorem F.1** (Theorem F.5, simplified).: _Suppose that \(\bm{F}\) is \(\mu\)-strongly monotone with \(\mu>0\), Assumption 3.3 holds, and an optimization method whose within-epoch error satisfies (31) and (32) for some constant \(a>0\) is run for \(K\) epochs. Then, for a sufficiently small constant \(\omega\) that does not depend on \(K\), we achieve the bound_

\[\mathbb{E}\left\|\bm{z}^{K}-\bm{z}^{*}\right\|^{2}\leq\exp\left(-\frac{1}{2} \mu\omega nK\right)\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\tilde{\mathcal{O}} \left(\frac{1}{nK^{2a-2}}\right).\]The goal of this section is to prove this theorem, whose precise statement is in Theorem F.5. As the polynomial decay will dominate the exponential decay for large enough \(K\), the bound we get is essentially \(\tilde{\mathcal{O}}\left(\nicefrac{{1}}{{nK^{2a-2}}}\right)\). Recall that for \(\mathsf{SEG-FF}\) and \(\mathsf{SEG-RR}\) we have \(a=2\) (by Theorems E.15 and E.16) which leads to an upper bound of \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{nK^{2}}})\), whereas for \(\mathsf{SEG-FFA}\) we have \(a=3\) (by Theorems E.9 and E.13) which gives an upper bound of \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{nK^{4}}})\).

As also mentioned in the beginning of Appendix E, for any of \(\mathsf{SEG-RR}\), \(\mathsf{SEG-FF}\), and \(\mathsf{SEG-FFA}\), we can decompose the update across the epoch into a deterministic EG update plus a noise. In this section, letting \(\bm{w}_{\dagger}^{k}\coloneqq\bm{z}^{k}-\eta_{k}n\bm{F}\bm{z}^{k}\), we define \(\widehat{\bm{F}}^{k}\) by the relation \(\eta_{k}n\widehat{\bm{F}}^{k}=\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}+\bm{r}^{k}\) so that

\[\bm{z}^{k+1}=\bm{z}^{k}-\eta_{k}n\widehat{\bm{F}}^{k}.\] (88)

**Proposition F.2**.: _Let \(\bm{F}\) be \(\mu\)-strongly monotone with \(\mu>0\). Then, for any \(\eta_{k}>0\), it holds that_

\[\eta_{k}^{2}n^{2} \left(1-\frac{3}{2}\mu\eta_{k}n-\left(1+\frac{1}{2}\mu\eta_{k}n \right)\eta_{k}^{2}n^{2}L^{2}\right)\left\|\bm{F}\bm{z}^{k}\right\|^{2}\] (89) \[\leq\left(1-\frac{1}{2}\mu\eta_{k}n\right)\left\|\bm{z}^{k}-\bm{z }^{*}\right\|^{2}-\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}+\frac{2+\mu\eta_ {k}n}{\mu\eta_{k}n}\left\|\bm{r}^{k}\right\|^{2}.\]

Proof.: From (88), using Lemma C.7 we get

\[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2} =\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\left\langle\eta_{k}n \widehat{\bm{F}}^{k},\bm{z}^{k}-\bm{z}^{*}\right\rangle+\left\|\eta_{k}n \widehat{\bm{F}}^{k}\right\|^{2}\] \[=\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\eta_{k}n\left\langle \bm{F}\bm{w}_{\dagger}^{k},\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\rangle-2\eta _{k}^{2}n^{2}\left\langle\bm{F}\bm{w}_{\dagger}^{k},\bm{F}\bm{z}^{k}\right\rangle\] \[\qquad\qquad\qquad\qquad-2\left\langle\bm{r}^{k},\bm{z}^{k}-\bm{ z}^{*}\right\rangle+\left\|\eta_{k}n\widehat{\bm{F}}^{k}\right\|^{2}\] \[\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\mu\eta_{k}n\left\| \bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\eta_{k}^{2}n^{2}\left\langle\bm{F}\bm{w}_{ \dagger}^{k},\bm{F}\bm{z}^{k}\right\rangle\] \[\qquad\qquad\qquad\qquad-2\left\langle\bm{r}^{k},\bm{z}^{k}-\bm {z}^{*}\right\rangle+\left\|\eta_{k}n\widehat{\bm{F}}^{k}\right\|^{2}+2\mu \eta_{k}^{3}n^{3}\left\|\bm{F}\bm{z}^{k}\right\|^{2}.\]

Meanwhile, using the polarization identity (Lemma C.1) and the \(L\)-smoothness of \(\bm{F}\) we get

\[-2\left\langle\bm{F}\bm{w}_{\dagger}^{k},\bm{F}\bm{z}^{k}\right\rangle =\left\|\bm{F}\bm{w}_{\dagger}^{k}-\bm{F}\bm{z}^{k}\right\|^{2}- \left\|\bm{F}\bm{w}_{\dagger}^{k}\right\|^{2}-\left\|\bm{F}\bm{z}^{k}\right\|^ {2}\] \[\leq L^{2}\left\|\bm{w}_{\dagger}^{k}-\bm{z}^{k}\right\|^{2}-\left\| \bm{F}\bm{w}_{\dagger}^{k}\right\|^{2}-\left\|\bm{F}\bm{z}^{k}\right\|^{2}\] \[\leq-(1-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k}\right\|^{2} -\left\|\bm{F}\bm{w}_{\dagger}^{k}\right\|^{2}.\]

Combining the two inequalities and using the definition of \(\widehat{\bm{F}}\) we obtain

\[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2} \leq(1-\mu\eta_{k}n)\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\eta _{k}^{2}n^{2}(1-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k}\right\|^{2}-\eta _{k}^{2}n^{2}\left\|\bm{F}\bm{w}_{\dagger}^{k}\right\|^{2}\] \[\qquad\qquad\qquad\qquad-2\left\langle\bm{r}^{k},\bm{z}^{k}-\bm {z}^{*}\right\rangle+\left\|\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}+\bm{r}^{k} \right\|^{2}+2\mu\eta_{k}^{3}n^{3}\left\|\bm{F}\bm{z}^{k}\right\|^{2}\] \[\leq(1-\mu\eta_{k}n)\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\eta _{k}^{2}n^{2}(1-2\mu\eta_{k}n-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k} \right\|^{2}\] \[\leq(1-\mu\eta_{k}n)\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\eta _{k}^{2}n^{2}(1-2\mu\eta_{k}n-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k} \right\|^{2}\] \[\qquad\qquad\qquad\qquad-2\left\langle\bm{r}^{k},\bm{z}^{k}-\bm {r}_{k}n\bm{F}\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\rangle+\left\|\bm{r}^{k} \right\|^{2}.\]

Let us consider the inner product term in the last line above. By Lemma C.2 and the nonexpansiveness of the EG update (Lemma C.10), for any \(\gamma_{k}>0\) we have

\[-2\left\langle\bm{r}^{k},\bm{z}^{k}-\eta_{k}n\bm{F}\bm{w}_{\dagger}^ {k}-\bm{z}^{*}\right\rangle \leq\frac{1}{\gamma_{k}}\left\|\bm{r}^{k}\right\|^{2}+\gamma_{k} \left\|\bm{z}^{k}-\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\|^{2}\] \[\leq\frac{1}{\gamma_{k}}\left\|\bm{r}^{k}\right\|^{2}+\gamma_{k} \left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\gamma_{k}\eta_{k}^{2}n^{2}(1-\eta _{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k}\right\|^{2}.\]Plugging this back we get

\[\begin{split}\eta_{k}^{2}& n^{2}(1+\gamma_{k}-2\mu\eta_{k }n-(1+\gamma_{k})\eta_{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k}\right\|^{2}\\ &\leq(1+\gamma_{k}-\mu\eta_{k}n)\left\|\bm{z}^{k}-\bm{z}^{*} \right\|^{2}-\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}+\left(1+\frac{1}{ \gamma_{k}}\right)\left\|\bm{r}^{k}\right\|^{2}.\end{split}\] (90)

Choosing \(\gamma_{k}=\frac{\mu\eta_{k}n}{2}\) completes the proof. 

**Proposition F.3**.: _Let \(\bm{F}\) be a \(\mu\)-strongly monotone and \(L\)-Lipschitz operator. Then, whenever \(\eta_{k}\leq\frac{1}{nL\sqrt{2}}\), it holds that_

Proof.: Let \(\bm{z}_{\uparrow}^{k+1}\coloneqq\bm{z}^{k}-\eta_{k}n\bm{F}(\bm{z}^{k}-\eta_{k }n\bm{F}\bm{z}^{k})\), so that we have \(\left\|\bm{z}^{k+1}-\bm{z}_{\uparrow}^{k+1}\right\|=\left\|\bm{r}^{k}\right\|\). Then, the \(L\)-smoothness of \(\bm{F}\) and Lemma C.8 implies

\[\begin{split}\left\|\bm{F}\bm{z}^{k+1}\right\|& \leq\left\|\bm{F}\bm{z}^{k+1}-\bm{F}\bm{z}_{\uparrow}^{k+1} \right\|+\left\|\bm{F}\bm{z}_{\uparrow}^{k+1}\right\|\\ &\leq L\left\|\bm{z}^{k+1}-\bm{z}_{\uparrow}^{k+1}\right\|+\left\| \bm{F}\bm{z}_{\uparrow}^{k+1}\right\|\\ &\leq L\left\|\bm{r}^{k}\right\|+\left(1-\frac{2\mu\eta_{k}n}{5} \right)^{1/2}\left\|\bm{F}\bm{z}^{k}\right\|\\ &\leq L\left\|\bm{r}^{k}\right\|+\left(1-\frac{\mu\eta_{k}n}{5} \right)\left\|\bm{F}\bm{z}^{k}\right\|\end{split}\]

where in the last line we apply a simple inequality \(1-2x\leq(1-x)^{2}\) which holds for all \(x\in\mathbb{R}\). 

**Lemma F.4**.: _Suppose that (31) holds. Say we use a constant stepsize \(\eta_{k}=\eta\), where \(\eta\) satisfies \(\eta\leq\frac{1}{nL\sqrt{2}}\) and_

\[\eta^{a-1}n^{a-1}\leq\frac{1}{10}\min\left\{\frac{1}{L^{2}},\frac{\mu}{L(C_{1 }+D_{1}(\left\|\bm{F}\bm{z}_{0}\right\|+\nicefrac{{V_{1}}}{{\mu L}}))}\right\}.\] (91)

_Then for any \(k=0,1,\ldots\), the following two inequalities both hold:_

\[\left\|\bm{F}\bm{z}^{k+1}\right\| \leq\left(1-\frac{\mu\eta n}{10}\right)\left\|\bm{F}\bm{z}^{k} \right\|+\eta^{a}n^{a}LV_{1},\] (92) \[\left\|\bm{F}\bm{z}^{k}\right\| \leq\left\|\bm{F}\bm{z}^{0}\right\|+\frac{V_{t}}{\mu L}.\] (93)

Proof.: For the case \(k=0\), the inequality (93) clearly holds. For the remaining cases, we use strong induction on \(k\). More precisely, assuming that (93) holds for all \(0,1,\ldots,k\), we will show that (92) holds, and from that the inequality

\[\left\|\bm{F}\bm{z}^{k+1}\right\|\leq\left\|\bm{F}\bm{z}^{0}\right\|+\frac{V_{ 1}}{\mu L}\] (94)

follows. To this end, let us begin from noting that Proposition F.3, (31), and the induction hypothesis (93) implies

\[\begin{split}\left\|\bm{F}\bm{z}^{k+1}\right\|& \leq\left(1-\frac{\mu\eta n}{5}\right)\left\|\bm{F}\bm{z}^{k} \right\|+\eta^{a}n^{a}L\left(C_{1}\left\|\bm{F}\bm{z}^{k}\right\|+D_{1}\left\| \bm{F}\bm{z}^{k}\right\|^{2}+V_{1}\right)\\ &\leq\left(1-\frac{\mu\eta n}{5}+\eta^{a}n^{a}LC_{1}+\eta^{a}n^{ a}LD_{1}\left(\left\|\bm{F}\bm{z}^{0}\right\|+\frac{V_{1}}{\mu L}\right) \right)\left\|\bm{F}\bm{z}^{k}\right\|+\eta^{a}n^{a}LV_{1}.\end{split}\] (95)

Here, from the choice of the stepsize (91), we have

\[\eta^{a}n^{a}LC_{1}+\eta^{a}n^{a}LD_{1}\left(\left\|\bm{F}\bm{z}^{0}\right\|+ \frac{V_{1}}{\mu L}\right)\leq\frac{\mu\eta n}{10}.\]Hence, from (95) we get

\[\left\|\bm{F}\bm{z}^{k+1}\right\|\leq\left(1-\frac{\mu\eta n}{10}\right)\left\| \bm{F}\bm{z}^{k}\right\|+\eta^{a}n^{a}LV_{1}.\]

which is exactly (92). Now, considering that we are assuming (93) holds for all \(0,1,\ldots,k\), we must also have (92) for all \(0,1,\ldots,k\). Thus we can unravel the recurrence to get

\[\left\|\bm{F}\bm{z}^{k+1}\right\| \leq\left(1-\frac{\mu\eta n}{10}\right)\left\|\bm{F}\bm{z}^{k} \right\|+\eta^{a}n^{a}LV_{1}\] \[\leq\left(1-\frac{\mu\eta n}{10}\right)^{2}\left\|\bm{F}\bm{z}^{ k-1}\right\|+\left(1-\frac{\mu\eta n}{10}\right)\eta^{a}n^{a}LV_{1}+\eta^{a}n^{a} LV_{1}\] \[\leq\ldots\] \[\leq\left(1-\frac{\mu\eta n}{10}\right)^{k+1}\left\|\bm{F}\bm{z} ^{0}\right\|+\eta^{a}n^{a}LV_{1}\sum_{j=0}^{k}\left(1-\frac{\mu\eta n}{10} \right)^{j}\] (96) \[\leq\left\|\bm{F}\bm{z}^{0}\right\|+\frac{\eta^{a}n^{a}LV_{1}}{1 -\left(1-\frac{\mu\eta n}{10}\right)}\] \[=\left\|\bm{F}\bm{z}^{0}\right\|+\frac{10\eta^{a-1}n^{a-1}LV_{1} }{\mu}.\]

As (91) also implies \(10\eta^{a-1}n^{a-1}L\leq\nicefrac{{1}}{{L}}\), we obtain (94), as claimed. This completes the proof. 

**Theorem F.5** (Theorem F.1).: _Suppose that \(\bm{F}\) is \(\mu\)-strongly monotone with \(\mu>0\), Assumption 3.3 holds, and an optimization method whose within-epoch error satisfies (31) and (32) for some constant \(a>0\) is run for \(K\) epochs. Let us define a constant_

\[\varPhi\coloneqq C_{2}+D_{2}\left(\left\|\bm{F}\bm{z}^{0}\right\|+\frac{V_{ 1}}{\mu L}\right)^{2}.\]

_Say we use a constant stepsize \(\eta_{k}=\eta\), where \(\eta\) is chosen as_

\[\eta=\min \left\{\frac{2}{5nL},\right.\] (97a) \[\left.\frac{1}{n(10L^{2})^{1/(a-1)}},\right.\] (97b) \[\left.\frac{\mu^{1/(a-1)}}{n(10L(C_{1}+D_{1}(\left\|\bm{F}\bm{z}_{ 0}\right\|+\nicefrac{{V_{1}}}{{\mu L}})))^{1/(a-1)}},\right.\] (97c) \[\left.\frac{1}{(12\varPhi/\mu)^{1/(2a-3)}n},\right.\] (97d) \[\left.\frac{4(a-1)\log(n^{1/(2a-2)}K)}{\mu nK}\right\}.\] (97e)

_Then for \(\omega\) denoting the minimum among (97a)-(97d), it holds that_

\[\mathbb{E}\left\|\bm{z}^{K}-\bm{z}^{\star}\right\|^{2}\leq\exp\left(-\frac{1} {2}\mu\omega nK\right)\left\|\bm{z}^{0}-\bm{z}^{\star}\right\|^{2}+\mathcal{O }\left(\frac{\left(\log(n^{1/(2a-2)}K)\right)^{2a-2}}{nK^{2a-2}}\right).\] (98)

As a reminder, for SEG-FF and SEG-RR we have \(a=2\), and for SEG-FFA we have \(a=3\).

Proof.: Notice that (97b) and (97c) together implies (91), and that \(\eta_{k}=\eta\leq\frac{2}{5nL}\leq\frac{1}{nL\sqrt{2}}<\frac{1}{nL}\). So, we can utilize (32) and Lemma F.4 to get

\[\mathbb{E}\left\|\bm{r}\right\|^{2}\left|\bm{z}^{k}\right\| \leq\eta^{2a}n^{2a}C_{2}\left\|\bm{F}\bm{z}^{k}\right\|^{2}+\eta^{ 2a}n^{2a}D_{2}\left\|\bm{F}\bm{z}^{k}\right\|^{4}+\eta^{2a}n^{2a-1}V_{2}\] \[\leq\eta^{2a}n^{2a}C_{2}\left\|\bm{F}\bm{z}^{k}\right\|^{2}+\eta^ {2a}n^{2a}D_{2}\left(\left\|\bm{F}\bm{z}^{0}\right\|+\frac{V_{1}}{\mu L}\right) ^{2}\left\|\bm{F}\bm{z}^{k}\right\|^{2}+\eta^{2a}n^{2a-1}V_{2}\] \[=\eta^{2a}n^{2a}\varPhi\left\|\bm{F}\bm{z}^{k}\right\|^{2}+\eta^ {2a}n^{2a-1}V_{2}.\]Taking the conditional expectation on (89) and applying the bound just derived, we obtain

\[\eta^{2}n^{2}\left(1-\frac{3}{2}\mu\eta n-\left(1+\frac{1}{2}\mu\eta n \right)\eta^{2}n^{2}L^{2}\right)\left\|\boldsymbol{F}\boldsymbol{z}^{k}\right\| ^{2}\] \[\leq\left(1-\frac{1}{2}\mu\eta n\right)\left\|\boldsymbol{z}^{k}- \boldsymbol{z}^{*}\right\|^{2}-\mathbb{E}\left[\left\|\boldsymbol{z}^{k+1}- \boldsymbol{z}^{*}\right\|^{2}\Big{|}\boldsymbol{z}^{k}\right]\] \[+\frac{2+\mu\eta n}{\mu}\left(\eta^{2a-1}n^{2a-1}\Phi\left\| \boldsymbol{F}\boldsymbol{z}^{k}\right\|^{2}+\eta^{2a-1}n^{2a-2}V_{2}\right).\]

A simple rearrangement of the terms leads to

\[\eta^{2}n^{2} \left(1-\frac{3}{2}\mu\eta n-\left(1+\frac{1}{2}\mu\eta n\right) \eta^{2}n^{2}L^{2}-\frac{2+\mu\eta n}{\mu}\cdot\eta^{2a-3}n^{2a-3}\Phi\right) \left\|\boldsymbol{F}\boldsymbol{z}^{k}\right\|^{2}\] (99) \[\leq\left(1-\frac{1}{2}\mu\eta n\right)\left\|\boldsymbol{z}^{k} -\boldsymbol{z}^{*}\right\|^{2}-\mathbb{E}\left[\left\|\boldsymbol{z}^{k+1}- \boldsymbol{z}^{*}\right\|^{2}\Big{|}\boldsymbol{z}^{k}\right]+\frac{2+\mu \eta n}{\mu}\cdot\eta^{2a-1}n^{2a-2}V_{2}.\]

Notice that by assuming (97a) and (97d), it holds that

\[\frac{3}{2}\mu\eta n+\left(1+\frac{1}{2}\mu\eta n\right)\eta^{2}n^{2}L^{2}+ \frac{2+\mu\eta n}{\mu}\cdot\eta^{2a-3}n^{2a-3}\Phi\]

so we can guarantee that the left hand side of (99) is nonnegative. It then follows that

Applying the law of total expectation, from the above we obtain

\[\mathbb{E}\left\|\boldsymbol{z}^{k+1}-\boldsymbol{z}^{*}\right\|^{2}\leq \left(1-\frac{1}{2}\mu\eta n\right)\mathbb{E}\left\|\boldsymbol{z}^{k}- \boldsymbol{z}^{*}\right\|^{2}+\frac{2+\mu\eta n}{\mu}\cdot\eta^{2a-1}n^{2a- 2}V_{2}.\]

We can now unravel this recurrence over \(k=0,1,\ldots,K-1\) as done in (96) to get

\[\mathbb{E}\left\|\boldsymbol{z}^{K}-\boldsymbol{z}^{*}\right\|^{2} \leq\left(1-\frac{1}{2}\mu\eta n\right)\mathbb{E}\left\| \boldsymbol{z}^{K-1}-\boldsymbol{z}^{*}\right\|^{2}+\frac{2+\mu\eta n}{\mu} \cdot\eta^{2a-1}n^{2a-2}V_{2}\] \[\leq\ldots\] \[\leq\left(1-\frac{1}{2}\mu\eta n\right)^{K}\left\|\boldsymbol{z }^{0}-\boldsymbol{z}^{*}\right\|^{2}+\frac{2+\mu\eta n}{\mu}\cdot\eta^{2a-1}n ^{2a-2}V_{2}\sum_{j=0}^{K-1}\left(1-\frac{1}{2}\mu\eta n\right)^{j}\] \[\leq\exp\left(-\frac{1}{2}\mu\eta nK\right)\left\|\boldsymbol{z }^{0}-\boldsymbol{z}^{*}\right\|^{2}+\frac{24}{5\mu^{2}}\cdot\eta^{2a-2}n^{2a -3}V_{2}\]

where in the last line we used the basic inequality \(1+x\leq e^{x}\) which holds for all \(x\in\mathbb{R}\). With the choice of the stepsize (97e), we arrive at

\[\mathbb{E}\left\|\boldsymbol{z}^{K}-\boldsymbol{z}^{*}\right\|^{2}\leq\exp \left(-\frac{1}{2}\mu\eta nK\right)\left\|\boldsymbol{z}^{0}-\boldsymbol{z}^{* }\right\|^{2}+\frac{24\cdot(4a-4)^{2a-2}V_{2}}{5\mu^{2a}}\cdot\frac{\left( \log(n^{1/(2a-2)}K)\right)^{2a-2}}{nK^{2a-2}}.\] (100)

Now, recall that \(\eta\) is chosen to be the smallest one among (97a)-(97e). Notice that the options (97a)-(97d) are independent with respect to \(K\), and (97e) is the only one that depends on \(K\). Let us consider these two cases separately.

* \(\eta\) is chosen to be the minimum among (97a)-(97d). This is the case where we have \(\eta=\omega\). Notice that the constant \(\omega\) that does not depend on \(K\). The inequality (100) then takes the form \[\mathbb{E}\left\|\boldsymbol{z}^{K}-\boldsymbol{z}^{*}\right\|^{2}\leq\exp \left(-\frac{\mu\omega nK}{2}\right)\left\|\boldsymbol{z}^{0}-\boldsymbol{z}^ {*}\right\|^{2}+\mathcal{O}\left(\frac{\left(\log(n^{1/(2a-2)}K)\right)^{2a-2} }{nK^{2a-2}}\right).\]2. \(\eta\) is chosen to be (97e), that is, \(\eta=\frac{4(a-1)\log(n^{1/(2a-2)}K)}{\mu nK}\). In this case, the exponential factor of the first term in the right hand side of (100) reduces to \[\exp\left(-\frac{1}{2}\mu\eta nK\right)=\frac{1}{nK^{2a-2}}.\] Thus, the second term in (100) dominates the first term, and in total (100) becomes \[\mathbb{E}\left\|\bm{z}^{K}-\bm{z}^{\star}\right\|^{2}=\mathcal{O}\left(\frac{ \left(\log(n^{1/(2a-2)}K)\right)^{2a-2}}{nK^{2a-2}}\right).\]

Therefore, in both cases we have

\[\mathbb{E}\left\|\bm{z}^{K}-\bm{z}^{\star}\right\|^{2}\leq\exp\left(-\frac{1}{ 2}\mu\omega nK\right)\left\|\bm{z}^{0}-\bm{z}^{\star}\right\|^{2}+\mathcal{O} \left(\frac{\left(\log(n^{1/(2a-2)}K)\right)^{2a-2}}{nK^{2a-2}}\right)\]

which is exactly (98). This completes the proof. 

_Remark F.6_.: To compare the convergence rate of SEG-FFA in the strongly monotone setting with that of SEG-RR by Emmanouilidis et al. [18] more in depth, let us make an estimation on the size of \(\omega\) appearing in Theorem F.5 when \(a=3\).

To this end, we need estimates on the constants \(C_{\tt 1A}\), \(D_{\tt 1A}\), \(V_{\tt 1A}\), \(C_{\tt 2A}\), and \(D_{\tt 2A}\). From their definitions in (59)-(61), (72), and (73) we have \(C_{\tt 1A}\asymp L^{2}\), \(D_{\tt 1A}\asymp M\), \(V_{\tt 1A}\asymp M+L^{2}\), \(C_{\tt 2A}\asymp L^{4}\), and \(D_{\tt 2A}\asymp M^{2}\). In general, there is not a direct relation between \(L\) and \(M\). For example, recall that if all components are quadratic, then \(M=0\). Meanwhile, Gorbunov et al. [21] has argued that \(M\) can be much larger than \(L\) in certain cases, by providing an example where \(M\asymp L^{3/2}\). For our purposes, however, let us allow \(M\) to be even as large as \(M\asymp L^{2}\), so that the situation is simplified into \(C_{\tt 1A}\asymp D_{\tt 1A}\asymp V_{\tt 1A}\asymp L^{2}\) and \(C_{\tt 2A}\asymp D_{\tt 2A}\asymp L^{4}\).

Then, we get the estimate of (97c),

\[\frac{\mu^{1/2}}{n(10L(C_{\tt 1A}+D_{\tt 1A}(\left\|\bm{F}\bm{z}_{0}\right\|+V _{\tt 1A}/\mu L)))^{1/2}}\asymp\frac{\mu}{nL^{2}}.\]

Meanwhile, as for the constant \(\varPhi\) it holds that

\[\varPhi=C_{\tt 2A}+D_{\tt 2A}\left(\left\|\bm{F}\bm{z}^{0}\right\|+\frac{V_{ \tt 1A}}{\mu L}\right)^{2}\asymp\frac{L^{6}}{\mu^{2}},\]

for (97d) we have

\[\frac{1}{(12\varPhi/\mu)^{1/3}n}\asymp\frac{\mu}{nL^{2}}.\]

As (97a) while (97b) are both \(\Theta(\nicefrac{{1}}{{nL}})\) and \(\mu\leq L\), we essentially have \(\omega\asymp\nicefrac{{\mu}}{{nL^{2}}}\). Or equivalently, for some \(b=\Theta(1)\), the convergence rate (98) reads

\[\mathbb{E}\left\|\bm{z}^{K}-\bm{z}^{\star}\right\|^{2}\leq\exp\left(-\frac{b \mu^{2}K}{L^{2}}\right)\left\|\bm{z}^{0}-\bm{z}^{\star}\right\|^{2}+\mathcal{O }\left(\frac{\left(\log(n^{1/4}K)\right)^{4}}{nK^{4}}\right).\] (101)

On the other hand, Theorem 2.1 of [18] states that, for some \(b^{\prime}=\Theta(1)\), SEG-RR exhibits a rate of

\[\mathbb{E}\left\|\bm{z}^{K}-\bm{z}^{\star}\right\|^{2}\leq\exp\left(-\frac{b^{ \prime}\mu^{2}K}{L^{2}}\right)\left\|\bm{z}^{0}-\bm{z}^{\star}\right\|^{2}+ \mathcal{O}\left(\frac{\left(\log(n^{1/(2a-2)}K)\right)^{2a-2}}{nK^{2}}\right).\] (102)

Comparing (101) with (102), the exponents in the exponentially decaying term are of the same order of \(-\frac{\mu^{2}K}{L^{2}}\), so SEG-FFA having a faster polynomially decaying term \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{nK^{4}}})\) enjoys an improved convergence rate.

Convergence Rate of SEG-FFA in the Monotone Setting

### Star-monotonicity

Notice that we only used Assumptions 3.3 and 3.4 in deriving the results in Appendices D and E, and in particular, the monotonicity assumption on \(\bm{F}\) was not necessary. Moreover, among the lemmata listed in Appendix C, Lemma C.10 is the only one that possibly uses the (non-strongly) monotone assumption, but that lemma is not used in this section.

In fact, as it turns out in Appendix G.2, in the convergence analysis of SEG-FFA, we need not fully exploit the inequality (3) provided by the monotonicity assumption. Rather, all the results on the performance of SEG-FFA can be established with only assuming the following condition (which has been also briefly mentioned in Appendix B.2).

**Assumption G.1** (Star-monotonicity).: Given an operator \(\bm{F}\) with a point \(\bm{z}^{*}\in\mathbb{R}^{d_{1}+d_{2}}\) such that \(\bm{F}\bm{z}^{*}=\bm{0}\), we say that \(\bm{F}\) is star-monotone if, for any \(\bm{z}\in\mathbb{R}^{d_{1}+d_{2}}\), it holds that

\[\left\langle\bm{F}\bm{z},\bm{z}-\bm{z}^{*}\right\rangle\geq 0.\] (103)

Monotone and strongly-monotone operators are clearly star-monotone, as they satisfy (3). On the other hand, there exist operators that are star-monotone but not monotone: see, _e.g._, [31, Appendix A.6].

Recall that when \(\bm{F}\) is monotone, Assumption 3.2 is equivalent to assuming the existence of a point \(\bm{z}^{*}\) that satisfies \(\bm{F}\bm{z}^{*}=\bm{0}\). Hence, after simply replacing the optimality condition in Assumption 3.2 with \(\bm{F}\bm{z}^{*}=\bm{0}\), our convergence analyses not only will show that our SEG-FFA finds an optimum on monotone problems, but also that it can be also used to find stationary points in "star-monotone" problems, allowing the objective function \(f\) to be nonconvex-nonconcave.

Star-monotonicity is also known as the _variational stability condition_[25], and has much been studied in the literature. For further details on star-monotonicity, we refer to [25, 31] and the references therein.

### Convergence Analysis of SEG-FFA in the (Star-)Monotone Setting

Let us in particular consider SEG-FFA. As in the previous section, we focus only on the iterates \(\{\bm{z}_{0}^{k}\}_{k\geq 0}\), so again, we omit the subscript \(0\) unless necessary, and simply write \(\bm{z}^{k}\) instead of \(\bm{z}_{0}^{k}\).

Decompose the update across the epoch into a deterministic EG update plus a noise, as

\[\bm{w}_{\dagger}^{k} \coloneqq\bm{z}^{k}-\eta_{k}n\bm{F}\bm{z}^{k},\] (104) \[\bm{z}^{k+1} =\bm{z}^{k}-\eta_{k}n\widehat{\bm{F}}^{k}.\]

for \(\widehat{\bm{F}}^{k}\) defined by the equation

\[\eta_{k}n\widehat{\bm{F}}^{k}=\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}+\bm{r}^{k}.\] (105)

**Lemma G.2**.: _Let \(\bm{F}\) be a (star-)monotone operator with a point \(\bm{z}^{*}\) that satisfies \(\bm{F}\bm{z}^{*}=\bm{0}\), and suppose that Assumption 3.3 holds. Then for any \(\eta_{k}>0\) and \(\gamma_{k}>0\), it holds that_

\[0\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\frac{1}{1+\gamma_{k}}\left\| \bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}-\eta_{k}^{2}n^{2}(1-\eta_{k}^{2}n^{2}L^{2} )\left\|\bm{F}\bm{z}^{k}\right\|^{2}+\frac{1}{\gamma_{k}}\left\|\bm{r}^{k} \right\|^{2}.\] (106)

Proof.: By (104) and (105) we get

\[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2} =\left\|\bm{z}^{k}-\eta_{k}n\widehat{\bm{F}}^{k}-\bm{z}^{*}\right\| ^{2}\] \[=\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\left\langle\eta_{k}n \widehat{\bm{F}}^{k},\bm{z}^{k}-\bm{z}^{*}\right\rangle+\left\|\eta_{k}n \widehat{\bm{F}}^{k}\right\|^{2}\] \[=\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\left\langle\eta_{k}n \bm{F}\bm{w}_{\dagger}^{k},\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\rangle-2 \left\langle\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k},\bm{z}^{k}-\bm{w}_{\dagger}^{ k}\right\rangle-2\left\langle\bm{r}^{k},\bm{z}^{k}-\bm{z}^{*}\right\rangle\] \[\qquad\qquad+\left\|\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}\right\|^{2 }+2\left\langle\bm{r}^{k},\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}\right\rangle+ \left\|\bm{r}^{k}\right\|^{2}\] \[=\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\eta_{k}n\left\langle \bm{F}\bm{w}_{\dagger}^{k},\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\rangle-2 \left\langle\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k},\eta_{k}n\bm{F}\bm{z}^{k}\right\rangle\] \[\qquad\qquad+\left\|\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}\right\|^{2 }-2\left\langle\bm{r}^{k},\bm{z}^{k}-\eta_{k}n\bm{F}\bm{w}_{\dagger}^{k}-\bm{z} ^{*}\right\rangle+\left\|\bm{r}^{k}\right\|^{2}\]\[=\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\eta_{k}n\left\langle\bm{Fw} _{\dagger}^{k},\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\rangle-2\eta_{k}^{2}n^{2} \left\langle\bm{Fw}_{\dagger}^{k},\bm{F}\bm{z}^{k}\right\rangle\] \[\qquad\qquad+\eta_{k}^{2}n^{2}\left\|\bm{Fw}_{\dagger}^{k}\right\| ^{2}-2\left\langle\bm{r}^{k},\bm{z}^{k+1}-\bm{z}^{*}\right\rangle-\left\|\bm{ r}^{k}\right\|^{2}.\]

We now bound the inner products. On one hand, by the polarization identity (Lemma C.1) and the \(L\)-smoothness of \(f\), we have

\[-2\left\langle\bm{Fw}_{\dagger}^{k},\bm{F}\bm{z}^{k}\right\rangle =\left\|\bm{Fw}_{\dagger}^{k}-\bm{F}\bm{z}^{k}\right\|^{2}-\left\| \bm{Fw}_{\dagger}^{k}\right\|^{2}-\left\|\bm{Fz}^{k}\right\|^{2}\] \[\leq L^{2}\left\|-\eta_{k}n\bm{F}\bm{z}^{k}\right\|^{2}-\left\| \bm{Fw}_{\dagger}^{k}\right\|^{2}-\left\|\bm{Fz}^{k}\right\|^{2}\] \[=-(1-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{Fz}^{k}\right\|^{2}-\left\| \bm{Fw}_{\dagger}^{k}\right\|^{2}.\]

On the other hand, by the weighted AM-GM inequality (Lemma C.2), for any number \(a_{k}\in(0,1)\) it holds that

\[-2\left\langle\bm{r}^{k},\bm{z}^{k+1}-\bm{z}^{*}\right\rangle\leq\frac{1}{a_{ k}}\left\|\bm{r}^{k}\right\|^{2}+a_{k}\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}.\]

Using these two bounds, we get

\[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2} \leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-2\eta_{k}n\left\langle \bm{Fw}_{\dagger}^{k},\bm{w}_{\dagger}^{k}-\bm{z}^{*}\right\rangle-\eta_{k}^{2 }n^{2}(1-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{Fz}^{k}\right\|^{2}\] (107) \[\qquad-\eta_{k}^{2}n^{2}\left\|\bm{Fw}_{\dagger}^{k}\right\|^{2}+ \eta_{k}^{2}n^{2}\left\|\bm{Fw}_{\dagger}^{k}\right\|^{2}+a_{k}\left\|\bm{z}^ {k+1}-\bm{z}^{*}\right\|^{2}+\left(\frac{1}{a_{k}}-1\right)\left\|\bm{r}^{k} \right\|^{2}.\]

Choosing \(a_{k}=\frac{\gamma_{k}}{1+\gamma_{k}}\) and rearranging the terms, we obtain

\[2\eta_{k}n\left\langle\bm{Fw}_{\dagger}^{k},\bm{w}_{\dagger}^{k} -\bm{z}^{*}\right\rangle\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\frac{1} {1+\gamma_{k}}\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}\] (108) \[\qquad\qquad\qquad-\eta_{k}^{2}n^{2}(1-\eta_{k}^{2}n^{2}L^{2}) \left\|\bm{Fz}^{k}\right\|^{2}+\frac{1}{\gamma_{k}}\left\|\bm{r}^{k}\right\|^ {2}.\]

The left hand side of (108) is nonnegative by the star-monotonicity of \(\bm{F}\) (104), and the claimed inequality follows. 

Now we show that choosing the appropriate stepsizes leads to \(\left\|\bm{Fz}^{k}\right\|\) being bounded uniformly over \(k\).

**Proposition G.3**.: _Let \(\bm{F}\) be a (star-)monotone operator with a point \(\bm{z}^{*}\) that satisfies \(\bm{Fz}^{*}=\bm{0}\), and suppose that Assumptions 3.3 and 3.4 hold. Say we are using SEG-FFA, or any optimization method whose within-epoch error satisfies (58) and (71). Let the sequence of stepsizes \(\{\eta_{k}\}_{k\geq 0}\) be nonincreasing, with_

\[S\coloneqq\sum_{k=0}^{\infty}\eta_{k}^{3}n^{3}L^{3}<\infty.\] (109)

_Suppose that initial stepsize \(\eta_{0}\) is chosen sufficiently small so that_

\[\eta_{0}^{2}n^{2}L^{2}+\frac{3\eta_{0}nC_{\bm{1}\bm{A}}^{2}}{L^{3}}+\frac{3 \eta_{0}nD_{\bm{1}\bm{A}}^{2}}{L}\cdot e^{S}\left(\left\|\bm{z}^{0}-\bm{z}^{* }\right\|^{2}+\frac{6SV_{\bm{1}\bm{A}}^{2}}{L^{6}}\right)\leq 1\] (110)

_for constants \(C_{\bm{1}\bm{A}}\), \(D_{\bm{1}\bm{A}}\) and \(V_{\bm{1}\bm{A}}\) defined in (59)-(61). Then for all \(k\geq 0\),_

\[\left\|\bm{Fz}^{k}\right\|^{2}\leq e^{S}L^{2}\left(\left\|\bm{z}^{0}-\bm{z}^{* }\right\|^{2}+\frac{6SV_{\bm{1}\bm{A}}^{2}}{L^{6}}\right).\] (111)

Proof.: We use induction on \(k\), to establish a stronger inequality

\[\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}\leq e^{S}\left(\left\|\bm{z}^{0}-\bm{z} ^{*}\right\|^{2}+\frac{6SV_{\bm{1}\bm{A}}^{2}}{L^{6}}\right).\] (112)

To see that (112) indeed implies (111), notice that by the \(L\)-smoothness of \(f\) it holds that

\[\left\|\bm{Fz}^{k}\right\|^{2}=\left\|\bm{Fz}^{k}-\bm{Fz}^{*}\right\|^{2}\leq L ^{2}\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}.\]For the case when \(k=0\), as \(S>0\), it is clear that (111) holds. Now suppose that (111) holds for some \(k\geq 0\). Applying Young's inequality on (31) leads to

\[\left\|\bm{r}^{k}\right\|^{2}\leq 3\eta_{k}^{6}n^{6}\left(C_{\mathsf{IA}}^{2} \left\|\bm{F}\bm{z}^{k}\right\|^{2}+D_{\mathsf{IA}}^{2}\left\|\bm{F}\bm{z}^{k} \right\|^{4}+V_{\mathsf{IA}}^{2}\right).\]

Applying this bound on \(\left\|\bm{r}^{k}\right\|^{2}\) on (106), we obtain

\[\eta_{k}^{2}n^{2}\left(1-\eta_{k}^{2}n^{2}L^{2}-\frac{3\eta_{k}^{ 4}n^{4}C_{\mathsf{IA}}^{2}}{\gamma_{k}}-\frac{3\eta_{k}^{4}n^{4}D_{\mathsf{IA}} ^{2}}{\gamma_{k}}\left\|\bm{F}\bm{z}^{k}\right\|^{2}\right)\left\|\bm{F}\bm{z}^ {k}\right\|^{2}\] (112) \[\qquad\qquad\qquad\qquad\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^ {2}-\frac{1}{1+\gamma_{k}}\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}+\frac{3 \eta_{k}^{6}n^{6}V_{\mathsf{IA}}^{2}}{\gamma_{k}}.\]

Choose \(\gamma_{k}=\eta_{k}^{3}n^{3}L^{3}\). Notice that (109) implies \(\eta_{0}nL\leq 1\), henceforth \(\eta_{k}\leq\eta_{0}\leq\nicefrac{{1}}{{nL}}\). This, with the induction hypothesis (110), implies

\[\eta_{k}^{2}n^{2}L^{2}+\frac{3\eta_{k}^{4}n^{4}C_{\mathsf{IA}}^{2} }{\gamma_{k}}+\frac{3\eta_{k}^{4}n^{4}D_{\mathsf{IA}}^{2}}{\gamma_{k}}\left\| \bm{F}\bm{z}^{k}\right\|^{2}\] \[\qquad=\eta_{k}^{2}n^{2}L^{2}+\frac{3\eta_{k}nC_{\mathsf{IA}}^{2} }{L^{3}}+\frac{3\eta_{k}nD_{\mathsf{IA}}^{2}}{L^{3}}\left\|\bm{F}\bm{z}^{k} \right\|^{2}\] \[\qquad\leq\eta_{0}^{2}n^{2}L^{2}+\frac{3\eta_{0}nC_{\mathsf{IA}}^ {2}}{L^{3}}+\frac{3\eta_{0}nD_{\mathsf{IA}}^{2}}{L^{3}}\left\|\bm{F}\bm{z}^{k} \right\|^{2}\] \[\qquad\leq\eta_{0}^{2}n^{2}L^{2}+\frac{3\eta_{0}nC_{\mathsf{IA}}^ {2}}{L^{3}}+\frac{3\eta_{0}nD_{\mathsf{IA}}^{2}}{L^{3}}\cdot e^{S}L^{2}\left( \left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\frac{6SV_{\mathsf{IA}}^{2}}{L^{6}}\right)\] \[\qquad\leq 1.\]

That is, the left hand side of (112) becomes nonnegative. Then it is immediate from (112) that

\[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2} \leq\left(1+\gamma_{k}\right)\left\|\bm{z}^{k}-\bm{z}^{*}\right\| ^{2}+\frac{3\eta_{k}^{6}n^{6}\left(1+\gamma_{k}\right)V_{\mathsf{IA}}^{2}}{ \gamma_{k}}\] \[\leq\left(1+\eta_{k}^{3}n^{3}L^{3}\right)\left\|\bm{z}^{k}-\bm{z} ^{*}\right\|^{2}+\frac{6\eta_{k}^{3}n^{3}V_{\mathsf{IA}}^{2}}{L^{3}}.\]

Using Lemma C.11 to unravel this recurrence relation, we obtain

\[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2} \leq\left(\prod_{j=0}^{k}\left(1+\eta_{j}^{3}n^{3}L^{3}\right) \right)\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\sum_{j=0}^{k}\frac{6 \eta_{j}^{3}n^{3}V_{\mathsf{IA}}^{2}}{L^{3}}\right)\] \[\leq e^{S\sum_{j=0}^{k}\eta_{j}^{3}n^{3}L^{3}}\left(\left\|\bm{z} ^{0}-\bm{z}^{*}\right\|^{2}+\frac{6V_{\mathsf{IA}}^{2}}{L^{6}}\sum_{j=0}^{k} \eta_{j}^{3}n^{3}L^{3}\right)\] \[\leq e^{S}\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\frac{6SV _{\mathsf{IA}}^{2}}{L^{6}}\right)\]

which shows that (111) also holds when \(k\) is replaced by \(k+1\). This completes the proof. 

**Theorem G.4** (Formal version of Theorem 5.4).: _Let \(\bm{F}\) be a (star-)monotone operator with a point \(\bm{z}^{*}\) that satisfies \(\bm{F}\bm{z}^{*}=\bm{0}\), and suppose that Assumptions 3.3 and 3.4 hold. Say that we are using \(\mathsf{SEG}\)-\(\mathsf{FFA}\), or any optimization method whose within-epoch error satisfies (58) and (71), with \(\beta_{k}=\eta_{k}=\frac{\eta_{0}\sqrt[3]{2}\log 2}{(k+2)^{1/3}\log(k+2)}\) and \(\alpha_{k}=\nicefrac{{\beta_{k}}}{{2}}\) for \(k=0,1,\ldots\), where, for \(S\coloneqq\sum_{k=0}^{\infty}\eta_{k}^{3}n^{3}L^{3}\), the initial stepsize \(\eta_{0}\) is chosen so that_

\[\eta_{0}^{2}n^{2}L^{2}+\frac{3\eta_{0}nC_{\mathsf{IA}}^{2}}{L^{3}}+\frac{3\eta_{ 0}nD_{\mathsf{IA}}^{2}}{L}\cdot e^{S}\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^ {2}+\frac{6SV_{\mathsf{IA}}^{2}}{L^{6}}\right)\leq 1\] (113)

_for constants \(C_{\mathsf{IA}}\), \(D_{\mathsf{IA}}\) and \(V_{\mathsf{IA}}\) defined in (59)-(61), and there exists a positive constant \(\lambda>0\) such that_

\[\eta_{0}^{2}n^{2}L^{2}+\frac{\eta_{0}nC_{\mathsf{ZA}}}{L^{3}}+\frac{\eta_{0}nD_{ \mathsf{ZA}}}{L}\cdot e^{S}\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\frac{6 SV_{\mathsf{IA}}^{2}}{L^{6}}\right)\leq 1-\lambda\] (114)_for constants \(C_{\text{2A}}\), \(D_{\text{2A}}\), and \(V_{\text{2A}}\) defined in (72)-(74). Then for any \(K\geq 1\), it holds that_

\[\min_{k=0,1,\ldots,K}\mathbb{E}\left\|\bm{F}\bm{z}^{k}\right\|^{2}\leq\frac{( \log(K+3))^{2}}{(K+3)^{1/3}}\cdot\left(\frac{\left\|\bm{z}^{0}-\bm{z}^{*} \right\|^{2}+\frac{3V_{\text{2A}}}{nL^{2}}}{\lambda e^{-3/2}(\sqrt[3]{2}\log 2)^{ 2}}\eta_{0}^{2}n^{2}\right).\] (115)

Proof.: As the sequence of stepsizes \(\{\eta_{k}\}_{k\geq 0}\) is nonincreasing and (113) asserts that \(\eta_{0}\leq\nicefrac{{1}}{{nL}}\), we can use the bounds established in Theorem E.9 and Theorem E.13. Also, the premises required for Proposition G.3 are also satisfied, so the bound (110) holds.

Setting \(\gamma_{k}=\eta_{k}^{3}n^{3}L^{3}\) in (106) and then taking the conditional expectation given \(\bm{z}^{k}\), with using (71) and (114), we obtain

\[0 \leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\frac{1}{1+\gamma_{ k}}\,\mathbb{E}\left[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}\left|\bm{z}^{k} \right]-\eta_{k}^{2}n^{2}(1-\eta_{k}^{2}n^{2}L^{2})\left\|\bm{F}\bm{z}^{k} \right\|^{2}+\frac{1}{\gamma_{k}}\,\mathbb{E}\left[\left\|\bm{r}^{k}\right\| ^{2}\left|\bm{z}^{k}\right]\right.\] \[\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\frac{1}{1+\gamma_{ k}}\,\mathbb{E}\left[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}\left|\bm{z}^{k} \right]\right.\] \[\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\frac{1}{1+\gamma_{ k}}\,\mathbb{E}\left[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}\left|\bm{z}^{k} \right]\right.\] \[\leq\left\|\bm{z}^{k}-\bm{z}^{*}\right\|^{2}-\frac{1}{1+\gamma_{ k}}\,\mathbb{E}\left[\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}\left|\bm{z}^{k} \right]-\lambda\eta_{k}^{2}n^{2}\left\|\bm{F}\bm{z}^{k}\right\|^{2}+\frac{\eta _{k}^{3}n^{2}V_{\text{2A}}}{L^{3}}.\]

By the law of total expectation, and that \(\gamma_{k}=\eta_{k}^{3}n^{3}L^{3}<1\), from the above we get

\[(1+\gamma_{k})\lambda\eta_{k}^{2}n^{2}\,\mathbb{E}\left\|\bm{F} \bm{z}^{k}\right\|^{2} \leq(1+\gamma_{k})\,\mathbb{E}\left\|\bm{z}^{k}-\bm{z}^{*} \right\|^{2}-\mathbb{E}\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}+\frac{(1+ \gamma_{k})\eta_{k}^{3}n^{2}V_{\text{2A}}}{L^{3}}\] \[\leq(1+\gamma_{k})\,\mathbb{E}\left\|\bm{z}^{k}-\bm{z}^{*}\right\| ^{2}-\mathbb{E}\left\|\bm{z}^{k+1}-\bm{z}^{*}\right\|^{2}+\frac{2\eta_{k}^{3}n ^{2}V_{\text{2A}}}{L^{3}}.\]

This recurrence can be unraveled using Lemma C.11, giving us

\[\mathbb{E}\left\|\bm{z}^{K+1}-\bm{z}^{*}\right\|^{2}+\sum_{k=0}^{ K}(1+\gamma_{k})\lambda\eta_{j}^{2}n^{2}\,\mathbb{E}\left\|\bm{F}\bm{z}^{k} \right\|^{2}\] (116) \[\qquad\leq\left(\prod_{k=0}^{K}(1+\gamma_{k})\right)\left(\left\| \bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\sum_{k=0}^{K}\frac{2\eta_{k}^{3}n^{2}V_{ \text{2A}}}{L^{3}}\right).\]

For the left hand side of (116), we have

\[\mathbb{E}\left\|\bm{z}^{K+1}-\bm{z}^{*}\right\|^{2}+\sum_{k=0}^{ K}(1+\gamma_{k})\lambda\eta_{k}^{2}n^{2}\,\mathbb{E}\left\|\bm{F}\bm{z}^{k} \right\|^{2} \geq\lambda\sum_{k=0}^{K}\eta_{k}^{2}n^{2}\,\mathbb{E}\left\|\bm{F} \bm{z}^{k}\right\|^{2}\] (117) \[\geq\lambda\min_{k=0,1,\ldots,K}\mathbb{E}\left\|\bm{F}\bm{z}^{k} \right\|^{2}\,\sum_{k=0}^{K}\eta_{k}^{2}n^{2}.\]

From Lemma C.12, we know that whenever \(K\geq 1\),

\[\sum_{k=0}^{K}\eta_{k}^{2}n^{2} =\eta_{0}^{2}n^{2}(\sqrt[3]{2}\log 2)^{2}\sum_{k=0}^{K}\frac{1}{(k+2)^{2/3}( \log(k+2))^{2}}\] \[\geq\eta_{0}^{2}n^{2}(\sqrt[3]{2}\log 2)^{2}\cdot\frac{(K+3)^{1/3}}{( \log(K+3))^{2}}.\]Meanwhile, as \(x\mapsto\frac{2(\log 2)^{3}}{(x+2)(\log(x+2))^{3}}\) is a decreasing function, we have

\[\sum_{k=0}^{\infty}\frac{2(\log 2)^{3}}{(k+2)(\log(k+2))^{3}} \leq 1+\frac{2(\log 2)^{3}}{3(\log 3)^{3}}+\int_{1}^{\infty}\frac{2( \log 2)^{3}}{(x+2)(\log(x+2))^{3}}\,\mathrm{d}x\] \[\leq 1+\frac{2(\log 2)^{3}}{3(\log 3)^{3}}+\frac{(\log 2)^{3}}{( \log 3)^{2}}\quad\leq\frac{3}{2}\]

and thus

\[S=\sum_{k=0}^{\infty}\eta_{k}^{3}n^{3}L^{3}=\eta_{0}^{3}n^{3}L^{3}\sum_{k=0}^ {\infty}\frac{2(\log 2)^{3}}{(k+2)(\log(k+2))^{3}}\leq\frac{3}{2}\eta_{0}^{3}n^{3}L ^{3}\leq\frac{3}{2}.\]

Thus, for the right hand side of (116), it holds that

\[\left(\prod_{k=0}^{K}(1+\gamma_{k})\right)\left(\left\|\bm{z}^{0 }-\bm{z}^{*}\right\|^{2}+\sum_{k=0}^{K}\frac{2\eta_{k}^{3}n^{2}V_{\mathsf{ZA} }}{L^{3}}\right) \leq e^{\sum_{k=0}^{K}\gamma_{k}}\left(\left\|\bm{z}^{0}-\bm{z}^{ *}\right\|^{2}+\sum_{k=0}^{K}\frac{2\eta_{k}^{3}n^{2}V_{\mathsf{ZA}}}{L^{3}}\right)\] \[\leq e^{S}\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\frac{2 SV_{\mathsf{ZA}}}{nL^{6}}\right)\] \[\leq e^{3/2}\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\frac{ 3V_{\mathsf{ZA}}}{nL^{6}}\right).\]

Therefore, from (116) we get

\[\lambda\eta_{0}^{2}n^{2}(\sqrt[3]{2}\log 2)^{2}\cdot\frac{(K+3)^{1/3}}{(\log(K+3 ))^{2}}\cdot\min_{k=0,1,\ldots,K}\mathbb{E}\left\|\bm{F}\bm{z}^{k}\right\|^{2} \leq e^{3/2}\left(\left\|\bm{z}^{0}-\bm{z}^{*}\right\|^{2}+\frac{3V_{\mathsf{ EA}}}{nL^{6}}\right).\]

Simply rearranging the terms gives us the desired inequality. 

_Remark G.5_.: While \(\eta_{0}\) should be chosen so that both (113) and (114) hold, in practice, there is a way to circumvent this complication. Notice that in deriving the upper bound (118) of the right hand side of (116), it suffices to have \(\eta_{k}\leq\frac{\eta_{0}\sqrt[3]{2}\log 2}{(k+2)^{1/3}\log(k+2)}\), and the lower bound (117) of the left hand side holds for any \(\eta_{k}\geq 0\). In other words, if we have had chosen \(\eta_{k}=\Theta\left(\nicefrac{{1}}{{k}}\right)^{q}\) for \(q>\frac{1}{3}\) so that \(S=\sum_{k=0}^{\infty}\eta_{k}^{3}n^{3}L^{3}<\infty\), as long as \(\eta_{0}\) satisfies (113) and (114), we would still have obtained the inequality

\[\min_{k=0,1,\ldots,K}\mathbb{E}\left\|\bm{F}\bm{z}^{k}\right\|^{2}\leq\frac{ e^{S}}{\lambda n^{2}\sum_{k=0}^{K}\eta_{k}^{2}}\left(\left\|\bm{z}^{0}-\bm{z}^{*} \right\|^{2}+\frac{2SV_{\mathsf{ZA}}}{nL^{6}}\right).\] (119)

In particular, if we additionally assume that \(q<\frac{1}{2}\) then

\[\sum_{k=0}^{K}\eta_{k}^{2}\asymp\sum_{k=1}^{K}\frac{1}{k^{2q}}\asymp K^{1-2q},\]

so from (119) we would have obtained the convergence rate

\[\min_{k=0,1,\ldots,K}\mathbb{E}\left\|\bm{F}\bm{z}^{k}\right\|^{2}=\mathcal{O} \left(\frac{1}{K^{1-2q}}\right).\] (120)

We now claim that, if one accepts a slight sacrifice of the convergence rate from \(\tilde{\mathcal{O}}(\nicefrac{{1}}{{K^{1/3}}})\) to \(\mathcal{O}\left(\nicefrac{{1}}{{K^{1-2q}}}\right)\) for \(\nicefrac{{1}}{{3}}<q<\nicefrac{{1}}{{2}}\), one can simply choose the stepsizes as \(\eta_{k}=\nicefrac{{\eta_{0}}}{{k}}+\nicefrac{{1}}{{q}}\) for a sufficiently small \(\eta_{00}\). To see why this is the case, let us fix \(\eta_{0}\) to be a number that satisfies the inequalities (113) and (114). Then, because \(\nicefrac{{\eta_{00}}}{{k}}+\nicefrac{{1}}{{q}}=o\left(\frac{1}{(k+2)^{1/3} \log(k+2)}\right)\), there will exist a nonnegative integer \(k_{0}\) such that \(\eta_{k}\leq\frac{\eta_{0}\sqrt[3]{2}\log 2}{(k+2)^{1/3}\log(k+2)}\) for all \(k\geq k_{0}\). So, by ignoring the first \(k_{0}\) terms if necessary--that is, considering as if the \(k_{0}\)th iteration is the \(0\)th iteration--it follows from the discussions made above in obtaining (120) that we get the rate of convergence \(\mathcal{O}\left(\nicefrac{{1}}{{K^{1-2q}}}\right)\).

This discussion also justifies the choice of stepsizes \(\eta_{k}=\Theta\left(\nicefrac{{1}}{{(1+k/10)^{0.34}}}\right)\) used in the experiments for the monotone setting.

Proof of Lower Bounds

### Proof of the Divergence of SEG-US, SEG-RR and SEG-FF

We prove the divergence of SEG-US, SEG-RR and SEG-FF in each proposition below, using the same worst-case problem for \(n=2\). These constitute the proof of Theorem 4.1.

**Proposition H.1** (Part of Theorem 4.1).: _For \(n=2\), there exists a convex-concave minimax problem \(f(x,y)=\frac{1}{2}\sum_{j=1}^{2}f_{i}(x,y)\) having a monotone \(\bm{F}\), consisting of \(L\)-smooth quadratic \(f_{i}\)'s satisfying Assumption 3.4 with \((\rho,\sigma)=(1,0)\) such that SEG-US diverges in expectation for any choice of stepsizes \(\{\alpha_{t}\}_{t\geq 0}\) and \(\{\beta_{t}\}_{t\geq 0}\). That is, for all \(t\geq 0\),_

\[\mathbb{E}\left[\left\|\bm{z}_{t+1}\right\|^{2}\right]>\mathbb{E}\left[\left\| \bm{z}_{t}\right\|^{2}\right],\quad\mathbb{E}\left[\left\|\bm{F}\bm{z}_{t+1} \right\|^{2}\right]>\mathbb{E}\left[\left\|\bm{F}\bm{z}_{t}\right\|^{2}\right].\]

Proof.: We consider the case of

\[f_{1}(x,y)=-\frac{L}{4}x^{2}+\frac{L}{2}xy-\frac{L}{4}y^{2},\] \[f_{2}(x,y)=\frac{L}{4}x^{2}+\frac{L}{2}xy+\frac{L}{4}y^{2},\]

which result in a bilinear (and hence convex-concave) objective function

\[f(x,y)=\frac{1}{2}\sum_{i=1}^{2}f_{i}(x,y)=\frac{L}{2}xy.\] (121)

One can quickly check from the definitions of the component functions \(f_{1}\) and \(f_{2}\) that the corresponding saddle gradient operators are given as

\[\bm{F}_{1}\bm{z}=\underbrace{\begin{bmatrix}-L/2&L/2\\ -L/2&L/2\end{bmatrix}}_{:=\bm{A}_{1}}\bm{z},\quad\bm{F}_{2}\bm{z}=\underbrace{ \begin{bmatrix}L/2&L/2\\ -L/2&-L/2\end{bmatrix}}_{:=\bm{A}_{2}}\bm{z},\quad\bm{F}\bm{z}=\begin{bmatrix} 0&L/2\\ -L/2&0\end{bmatrix}\bm{z}\]

where \(\bm{z}=(x,y)\in\mathbb{R}^{2}\). From the fact that \(\left\|\bm{A}_{i}\right\|\leq L\) for all \(i\)'s, we can confirm that \(f_{i}\)'s are indeed \(L\)-smooth. As for Assumption 3.4, we can verify that

\[\frac{1}{2}\sum_{i=1}^{2}\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2}= \frac{L^{2}}{4}\left\|\bm{z}\right\|^{2}=\left\|\bm{F}\bm{z}\right\|^{2},\]

thus proving that our example \(f\) indeed satisfies Assumption 3.4 with \((\rho,\sigma)=(1,0)\).

We now proceed to show that for this particular worst-case example \(f\), SEG-US diverges in expectation. For \(t\geq 0\), the \((t+1)\)-th iteration of SEG-US starts at \(\bm{z}_{t}\), and the algorithm uniformly chooses an index \(i(t)\) from \([n]\). The algorithm then makes an update

\[\bm{w}_{t} =\bm{z}_{t}-\alpha_{t}\bm{F}_{i(t)}\bm{z}_{t},\] \[\bm{z}_{t+1} =\bm{z}_{t}-\beta_{t}\bm{F}_{i(t)}\bm{w}_{t}.\]

In our worst-case example \(f\), the updates can be compactly written as

\[\bm{z}_{t+1}=(\bm{I}-\beta_{t}\bm{A}_{i(t)}+\alpha_{t}\beta_{t}\bm{A}_{i(t)}^{ 2})\bm{z}_{t}.\]

Since we have \(n=2\), the update can be summarized as

\[\bm{z}_{t+1}=\begin{cases}(\bm{I}-\beta_{t}\bm{A}_{1}+\alpha_{t}\beta_{t}\bm{A }_{1}^{2})\bm{z}_{t}&\text{with probability }1/2,\\ (\bm{I}-\beta_{t}\bm{A}_{2}+\alpha_{t}\beta_{t}\bm{A}_{2}^{2})\bm{z}_{t}&\text{ with probability }1/2.\end{cases}\]

By the definition of \(\bm{A}_{1}\) and \(\bm{A}_{2}\) and using \(\bm{A}_{1}^{2}=\bm{A}_{2}^{2}=\bm{0}\), we can verify that

\[\bm{N}_{1} \coloneqq\bm{I}-\beta_{t}\bm{A}_{1}+\alpha_{t}\beta_{t}\bm{A}_{ 1}^{2}=\begin{bmatrix}1+\frac{\beta_{t}L}{2}&-\frac{\beta_{t}L}{2}\\ \frac{\beta_{t}L}{2}&1-\frac{\beta_{t}L}{2}\end{bmatrix},\] \[\bm{N}_{2} \coloneqq\bm{I}-\beta_{t}\bm{A}_{2}+\alpha_{t}\beta_{t}\bm{A}_{ 2}^{2}=\begin{bmatrix}1-\frac{\beta_{t}L}{2}&-\frac{\beta_{t}L}{2}\\ \frac{\beta_{t}L}{2}&1+\frac{\beta_{t}L}{2}\end{bmatrix}.\]From this, we notice that the expectation of \(\left\|\bm{z}_{t+1}\right\|^{2}\) conditional on \(\bm{z}_{t}\) reads

\[\mathbb{E}\left[\left\|\bm{z}_{t+1}\right\|^{2}\Big{|}\bm{z}_{t}\right]=\bm{z}_{ t}^{\top}\left(\frac{\bm{N}_{1}^{\top}\bm{N}_{1}+\bm{N}_{2}^{\top}\bm{N}_{2}}{2} \right)\bm{z}_{t}.\]

Working out the calculations, we can check that

\[\frac{\bm{N}_{1}^{\top}\bm{N}_{1}+\bm{N}_{2}^{\top}\bm{N}_{2}}{2}=\begin{bmatrix} 1+\frac{\beta_{t}^{2}L^{2}}{2}&0\\ 0&1+\frac{\beta_{t}^{2}L^{2}}{2}\end{bmatrix},\]

thus resulting in

\[\mathbb{E}\left[\left\|\bm{z}_{t+1}\right\|^{2}\Big{|}\bm{z}_{t}\right]=\left( 1+\frac{\beta_{t}^{2}L^{2}}{2}\right)\left\|\bm{z}_{t}\right\|^{2}.\]

Since this holds for all \(t\geq 0\), SEG-US diverges in expectation, for any positive stepsizes \(\{\alpha_{t}\}_{t\geq 0}\) and \(\{\beta_{t}\}_{t\geq 0}\). The statement on \(\left\|\bm{F}\bm{z}_{t}\right\|\) follows by realizing that \(\left\|\bm{F}\bm{z}\right\|=\frac{L}{2}\left\|\bm{z}\right\|\). 

**Proposition H.2** (Part of Theorem 4.1).: _For \(n=2\), there exists a convex-concave minimax problem \(f(x,y)=\frac{1}{2}\sum_{i=1}^{2}f_{i}(x,y)\) having a monotone \(\bm{F}\), consisting of \(L\)-smooth quadratic \(f_{i}\)'s satisfying Assumption 3.4 with \((\rho,\sigma)=(1,0)\) such that SEG-RR diverges in expectation for any choice of stepsizes \(\{\alpha_{k}\}_{k\geq 0}\) and \(\{\beta_{k}\}_{k\geq 0}\). That is, for any \(k\geq 0\),_

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{k+1}\right\|^{2}\right]>\mathbb{E}\left[ \left\|\bm{z}_{0}^{k}\right\|^{2}\right],\quad\mathbb{E}\left[\left\|\bm{F} \bm{z}_{0}^{k+1}\right\|^{2}\right]>\mathbb{E}\left[\left\|\bm{F}\bm{z}_{0}^{k }\right\|^{2}\right].\]

Proof.: The proof uses the same example as Proposition H.1, outlined in (121). We show that for this particular worst-case example \(f\), SEG-RR diverges in expectation. For \(k\geq 0\), the \((k+1)\)-th epoch of SEG-RR starts at \(\bm{z}_{0}^{k}\), and the algorithm randomly chooses a permutation \(\tau_{k}:[n]\to[n]\). The algorithm then goes through a series of updates

\[\bm{w}_{i}^{k} =\bm{z}_{i}^{k}-\alpha_{k}\bm{F}_{\tau_{k}(i+1)}\bm{z}_{i}^{k},\] \[\bm{z}_{i+1}^{k} =\bm{z}_{i}^{k}-\beta_{k}\bm{F}_{\tau_{k}(i+1)}\bm{w}_{i}^{k},\]

for \(i=0,\dots,n-1\). In our worst-case example \(f\), the updates can be compactly written as

\[\bm{z}_{i+1}^{k}=(\bm{I}-\beta_{k}\bm{A}_{\tau_{k}(i+1)}+\alpha_{k}\beta_{k} \bm{A}_{\tau_{k}(i+1)}^{2})\bm{z}_{i}^{k}.\]

Since we have \(n=2\) and there are only two possible permutations, the updates over an epoch can be summarized as

\[\bm{z}_{0}^{k+1}=\bm{z}_{n}^{k}=\begin{cases}(\bm{I}-\beta_{k}\bm{A}_{1}+ \alpha_{k}\beta_{k}\bm{A}_{1}^{2})(\bm{I}-\beta_{k}\bm{A}_{2}+\alpha_{k}\beta _{k}\bm{A}_{2}^{2})\bm{z}_{0}^{k}&\text{with probability $1/2$,}\\ (\bm{I}-\beta_{k}\bm{A}_{2}+\alpha_{k}\beta_{k}\bm{A}_{2}^{2})(\bm{I}-\beta_{k} \bm{A}_{1}+\alpha_{k}\beta_{k}\bm{A}_{1}^{2})\bm{z}_{0}^{k}&\text{with probability $1/2$.}\end{cases}\]

By the definition of \(\bm{A}_{1}\) and \(\bm{A}_{2}\) and using \(\bm{A}_{1}^{2}=\bm{A}_{2}^{2}=\bm{0}\), we can verify that

\[\bm{M}_{1} \coloneqq(\bm{I}-\beta_{k}\bm{A}_{1}+\alpha_{k}\beta_{k}\bm{A}_ {1}^{2})(\bm{I}-\beta_{k}\bm{A}_{2}+\alpha_{k}\beta_{k}\bm{A}_{2}^{2})\!=\! \begin{bmatrix}1-\frac{\beta_{k}^{2}L^{2}}{2}&-\beta_{k}L-\frac{\beta_{k}^{2} L^{2}}{2}\\ \beta_{k}L-\frac{\beta_{k}^{2}L^{2}}{2}&1-\frac{\beta_{k}^{2}L^{2}}{2}\end{bmatrix},\] (122) \[\bm{M}_{2} \coloneqq(\bm{I}-\beta_{k}\bm{A}_{2}+\alpha_{k}\beta_{k}\bm{A}_{ 2}^{2})(\bm{I}-\beta_{k}\bm{A}_{1}+\alpha_{k}\beta_{k}\bm{A}_{1}^{2})\!=\! \begin{bmatrix}1-\frac{\beta_{k}^{2}L^{2}}{2}&-\beta_{k}L+\frac{\beta_{k}^{2}L^{2} }{2}\\ \beta_{k}L+\frac{\beta_{k}^{2}L^{2}}{2}&1-\frac{\beta_{k}^{2}L^{2}}{2}\end{bmatrix}.\] (123)

From this, we notice that the expectation of \(\left\|\bm{z}_{0}^{k+1}\right\|^{2}\) conditional on \(\bm{z}_{0}^{k}\) reads

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{k+1}\right\|^{2}\Big{|}\bm{z}_{0}^{k}\right] =(\bm{z}_{0}^{k})^{\top}\left(\frac{\bm{M}_{1}^{\top}\bm{M}_{1}+\bm{M}_{2}^{ \top}\bm{M}_{2}}{2}\right)\bm{z}_{0}^{k}.\]

Working out the calculations, we can check that

\[\frac{\bm{M}_{1}^{\top}\bm{M}_{1}+\bm{M}_{2}^{\top}\bm{M}_{2}}{2}=\begin{bmatrix} 1+\frac{\beta_{t}^{4}L^{4}}{2}&0\\ 0&1+\frac{\beta_{t}^{4}L^{4}}{2}\end{bmatrix},\]

thus resulting in

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{k+1}\right\|^{2}\Big{|}\bm{z}_{0}^{k}\right]= \left(1+\frac{\beta_{k}^{4}L^{4}}{2}\right)\left\|\bm{z}_{0}^{k}\right\|^{2}.\]

Since this holds for all \(k\geq 0\), SEG-RR diverges in expectation, for any positive stepsizes \(\{\alpha_{k}\}_{k\geq 0}\) and \(\{\beta_{k}\}_{k\geq 0}\). The statement on \(\left\|\bm{F}\bm{z}_{0}^{k}\right\|\) follows by realizing that \(\left\|\bm{F}\bm{z}\right\|=\frac{L}{2}\left\|\bm{z}\right\|\).

[MISSING_PAGE_FAIL:61]

\((0,\sigma)\) such that SEG-US with any positive stepsizes \(\{\alpha_{t}\}_{t\geq 0}\) and \(\{\beta_{t}\}_{t\geq 0}\) satisfying \(\beta_{t}=\gamma\alpha_{t}\) for \(\gamma>0\) cannot converge beyond a certain fixed constant \(\Omega(\sigma^{2})\). More concretely, for any \(t\geq 0\),_

\[\mathbb{E}\left[\left\|\bm{F}\bm{z}_{t}\right\|^{2}\right]\geq\min\left\{\left\| \bm{F}\bm{z}_{0}\right\|^{2},\frac{\gamma\sigma^{2}}{2}\right\}\]

_regardless of the stepsizes. This holds for both **same-sample** and **independent-sample SEG-US**._

Proof.: We consider the case of

\[f_{1}(x,y) =Lxy+\nu x-\nu y,\] \[f_{2}(x,y) =Lxy-\nu x+\nu y,\]

which results in a bilinear (and hence convex-concave) objective function

\[f(x,y)=\frac{1}{2}\sum_{i=1}^{2}f_{i}(x,y)=Lxy.\]

One can quickly check from the definitions of the component functions \(f_{1}\) and \(f_{2}\) that the corresponding saddle gradient operators are given as

\[\bm{F}_{1}\bm{z}=\underbrace{\begin{bmatrix}0&L\\ -L&0\end{bmatrix}}_{:=\bm{A}}\bm{z}+\nu\bm{1},\quad\bm{F}_{2}\bm{z}=\bm{A}\bm {z}-\nu\bm{1},\quad\bm{F}\bm{z}=\bm{A}\bm{z},\]

where \(\bm{z}=(x,y)\in\mathbb{R}^{2}\). From the fact that \(\left\|\bm{A}\right\|\leq L\), we can confirm that \(f_{i}\)'s are indeed \(L\)-smooth. As for Assumption 3.4, we can verify that

\[\frac{1}{2}\sum_{i=1}^{2}\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2}= \frac{1}{2}\sum_{i=1}^{2}2\nu^{2}=2\nu^{2}.\]

Therefore, by choosing \(\nu^{2}=\frac{\sigma^{2}}{2}\), our example \(f\) indeed satisfies Assumption 3.4 with \((\rho,\sigma)=(0,\sigma)\).

The proof is outlined as follows. For the example constructed above, we will calculate the \(\mathbb{E}[\left\|\bm{z}_{t+1}\right\|^{2}]\) and show that the expectation is identical for both same-sample and independent sample versions of SEG-US. We will then show that the update on the expected squared distance to equilibrium \(\mathbb{E}[\left\|\bm{z}_{t+1}\right\|^{2}]\) for given \(\bm{z}_{t}\) can only belong to two categories: either \(\left\|\bm{z}_{t}\right\|^{2}\leq\mathbb{E}[\left\|\bm{z}_{t+1}\right\|^{2}]\) (expected squared distance increases) or \(\left\|\bm{z}_{t}\right\|^{2}\geq\mathbb{E}[\left\|\bm{z}_{t+1}\right\|^{2}] \geq\frac{\gamma\sigma^{2}}{2L}\) (expected squared distance shrinks but is bounded from below by a constant). Since the two cases hold for any \(t\geq 0\) and any choices of \(\alpha_{t}\) and \(\beta_{t}=\gamma\alpha_{t}\), we show that the "convergence" can happen only up to a neighborhood of equilibrium.

At iteration \(t\), SEG-US samples component indices \(i(t),j(t)\in\{1,2\}\) for its extrapolation step and update step, respectively. In the independent-sample version \(i(t)\) and \(j(t)\) are independently sampled from \(\text{Unif}(\{1,2\})\), and in the same-sample version \(i(t)\) is sampled uniformly at random and \(j(t)\) is set to be equal to \(i(t)\). With the indices sampled as above, SEG-US then makes an update

\[\bm{w}_{t} =\bm{z}_{t}-\alpha_{t}\bm{F}_{i(t)}\bm{z}_{t},\] \[\bm{z}_{t+1} =\bm{z}_{t}-\beta_{t}\bm{F}_{j(t)}\bm{w}_{t}.\]

In our worst-case example \(f\), the updates can be written as

\[\bm{w}_{t} =\bm{z}_{t}-\alpha_{t}\bm{A}\bm{z}_{t}-s_{i(t)}\alpha_{t}\nu\bm{1}\] \[=(\bm{I}-\alpha_{t}\bm{A})\bm{z}_{t}-s_{i(t)}\alpha_{t}\nu\bm{1}\] \[\bm{z}_{t+1} =\bm{z}_{t}-\beta_{t}\bm{A}\bm{w}_{t}-s_{j(t)}\beta_{t}\nu\bm{1}\] \[=(\bm{I}-\beta_{t}\bm{A}+\alpha_{t}\beta_{t}\bm{A}^{2})\bm{z}_{t }+s_{i(t)}\alpha_{t}\beta_{t}\nu\bm{A}\bm{1}-s_{j(t)}\beta_{t}\nu\bm{1},\]

where we defined \(s_{1}=+1\) and \(s_{2}=-1\) for simplicity of notation.

We now calculate the expected value of \(\left\|\bm{z}_{t+1}\right\|^{2}\).

\[\left\|\bm{z}_{t+1}\right\|^{2}= \left\|(\bm{I}-\beta_{t}\bm{A}+\alpha_{t}\beta_{t}\bm{A}^{2})\bm{ z}_{t}\right\|^{2}+\alpha_{t}^{2}\beta_{t}^{2}\nu^{2}\left\|\bm{A}\bm{1}\right\|^{2}+ \beta_{t}^{2}\nu^{2}\left\|\bm{1}\right\|^{2}\] \[+2s_{i(t)}\alpha_{t}\beta_{t}\nu(\left(\bm{I}-\beta_{t}\bm{A}+ \alpha_{t}\beta_{t}\bm{A}^{2}\right)\bm{z}_{t},\bm{A}\bm{1})-2s_{j(t)}\beta_{t }\nu(\left(\bm{I}-\beta_{t}\bm{A}+\alpha_{t}\beta_{t}\bm{A}^{2}\right)\bm{z}_{ t},\bm{1})\] \[-2s_{i(t)}s_{j(t)}\alpha_{t}\beta_{t}^{2}\nu^{2}\langle\bm{A}\bm{ 1},\bm{1}\rangle.\]For the independent-sample case, since \(s_{i(t)}\) and \(s_{j(t)}\) are independent mean-zero random variables,

\[\mathbb{E}_{i(t),j(t)}[\left\|\boldsymbol{z}_{t+1}\right\|^{2}]=\left\|(\boldsymbol {I}-\beta_{t}\boldsymbol{A}+\alpha_{t}\beta_{t}\boldsymbol{A}^{2})\boldsymbol {z}_{t}\right\|^{2}+\alpha_{t}^{2}\beta_{t}^{2}\nu^{2}\left\|\boldsymbol{A} \boldsymbol{1}\right\|^{2}+\beta_{t}^{2}\nu^{2}\left\|\boldsymbol{1}\right\|^ {2}.\] (124)

In the same-sample case, \(s_{i(t)}=s_{j(t)}\) is a mean-zero random variable, so

\[\mathbb{E}_{i(t)}[\left\|\boldsymbol{z}_{t+1}\right\|^{2}]=\left\|(\boldsymbol {I}-\beta_{t}\boldsymbol{A}+\alpha_{t}\beta_{t}\boldsymbol{A}^{2})\boldsymbol {z}_{t}\right\|^{2}+\alpha_{t}^{2}\beta_{t}^{2}\nu^{2}\left\|\boldsymbol{A} \boldsymbol{1}\right\|^{2}+\beta_{t}^{2}\nu^{2}\left\|\boldsymbol{1}\right\|^ {2}-2\alpha_{t}\beta_{t}^{2}\nu^{2}\langle\boldsymbol{A}\boldsymbol{1}, \boldsymbol{1}\rangle,\]

but once we realize that \(\langle\boldsymbol{A}\boldsymbol{1},\boldsymbol{1}\rangle=0\), the expectation becomes identical to (124); hence, the rest of the analysis is the same for the two versions.

We now expand and arrange the RHS of (124). It is easy to check that

\[(\boldsymbol{I}-\beta_{t}\boldsymbol{A}+\alpha_{t}\beta_{t} \boldsymbol{A}^{2})\boldsymbol{z}_{t}=\begin{bmatrix}1-\alpha_{t}\beta_{t}L^{ 2}&-\beta_{t}L\\ \beta_{t}L&1-\alpha_{t}\beta_{t}L^{2}\end{bmatrix}\begin{bmatrix}x_{t}\\ y_{t}\end{bmatrix}=\begin{bmatrix}(1-\alpha_{t}\beta_{t}L^{2})x_{t}-\beta_{t}Ly _{t}\\ \beta_{t}Lx_{t}+(1-\alpha_{t}\beta_{t}L^{2})y_{t}\end{bmatrix}\]

and hence

\[\left\|(\boldsymbol{I}-\beta_{t}\boldsymbol{A}+\alpha_{t}\beta_{t }\boldsymbol{A}^{2})\boldsymbol{z}_{t}\right\|^{2} =\left((1-\alpha_{t}\beta_{t}L^{2})^{2}+\beta_{t}^{2}L^{2}\right) \left\|\boldsymbol{z}_{t}\right\|^{2}\] \[=\left(1-2\alpha_{t}\beta_{t}L^{2}+\beta_{t}^{2}L^{2}(1+\alpha_{t }^{2}L^{2})\right)\left\|\boldsymbol{z}_{t}\right\|^{2}.\]

From this, we get

\[\mathbb{E}[\left\|\boldsymbol{z}_{t+1}\right\|^{2}] =\left\|\boldsymbol{z}_{t}\right\|^{2}-\left(2\alpha_{t}\beta_{t }L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})\right)\left\|\boldsymbol{z}_ {t}\right\|^{2}+2\alpha_{t}^{2}\beta_{t}^{2}L^{2}\nu^{2}+2\beta_{t}^{2}\nu^{2}\] \[=\left\|\boldsymbol{z}_{t}\right\|^{2}-\left(2\alpha_{t}\beta_{t }L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})\right)\left\|\boldsymbol{z}_ {t}\right\|^{2}+\beta_{t}^{2}\sigma^{2}(1+\alpha_{t}^{2}L^{2}),\]

where we used the choice \(\nu_{2}=\frac{\sigma^{2}}{2}\) as above.

The rest of the proof proceeds as follows: we show that, regardless of \(t\geq 0\) and the choices of \(\alpha_{t}\) and \(\beta_{t}=\gamma\alpha_{t}\), the expected value of \(\left\|\boldsymbol{z}_{t+1}\right\|^{2}\) given \(\boldsymbol{z}_{t}\) can be categorized into only two cases:

1. \(\left\|\boldsymbol{z}_{t}\right\|^{2}\leq\mathbb{E}[\left\|\boldsymbol{z}_{t+1 }\right\|^{2}]\). That is, the iterate moves away from the equilibrium in expectation.
2. \(\left\|\boldsymbol{z}_{t}\right\|^{2}\geq\mathbb{E}[\left\|\boldsymbol{z}_{t+1 }\right\|^{2}]\geq\frac{\gamma\sigma^{2}}{2L^{2}}\). That is, the expected squared distance shrinks but is lower bounded by a certain constant independent of the stepsizes.

Showing this immediately finishes the proof, because there is no way that any \(\mathbb{E}[\left\|\boldsymbol{z}_{t}\right\|^{2}]\) can get smaller than \(\min\{\left\|\boldsymbol{z}_{0}\right\|^{2},\frac{\gamma\sigma^{2}}{2L^{2}}\}\), and \(\left\|\boldsymbol{F}\boldsymbol{z}\right\|=L\left\|\boldsymbol{z}\right\|\) for our example \(f\).

The remaining proof is simple, by noticing that \(\left\|\boldsymbol{z}_{t}\right\|^{2}\leq\mathbb{E}[\left\|\boldsymbol{z}_{t+1 }\right\|^{2}]\) is equivalent to

\[\left(2\alpha_{t}\beta_{t}L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2}) \right)\left\|\boldsymbol{z}_{t}\right\|^{2}\leq\beta_{t}^{2}\sigma^{2}(1+ \alpha_{t}^{2}L^{2}).\] (125)

Hence, if \(\alpha_{t}\), \(\beta_{t}\), and \(\boldsymbol{z}_{t}\) satisfies (125), we belong to the first category. Otherwise, we are in the second category, for which we need to additionally show \(\mathbb{E}[\left\|\boldsymbol{z}_{t+1}\right\|^{2}]\geq\frac{\gamma\sigma^{2}}{2L^ {2}}\). When the inequality (125) is satisfied with the opposite sign, we must have \(2\alpha_{t}\beta_{t}L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})>0\) and

\[\left\|\boldsymbol{z}_{t}\right\|^{2}\geq\frac{\beta_{t}^{2}\sigma^{2}(1+ \alpha_{t}^{2}L^{2})}{2\alpha_{t}\beta_{t}L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t} ^{2}L^{2})}.\]

Also, notice that

\[2\alpha_{t}\beta_{t}L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})=1-\left((1- \alpha_{t}\beta_{t}L^{2})^{2}+\beta_{t}^{2}L^{2}\right)<1.\]

Using \(2\alpha_{t}\beta_{t}L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})\in(0,1)\) and substituting the lower bound on \(\left\|\boldsymbol{z}_{t}\right\|^{2}\) into the update equation, we find that

\[\mathbb{E}[\left\|\boldsymbol{z}_{t+1}\right\|^{2}] =\left\|\boldsymbol{z}_{t}\right\|^{2}-\left(2\alpha_{t}\beta_{t }L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})\right)\left\|\boldsymbol{z}_{t} \right\|^{2}+\beta_{t}^{2}\sigma^{2}(1+\alpha_{t}^{2}L^{2})\] \[\geq\frac{\beta_{t}^{2}\sigma^{2}(1+\alpha_{t}^{2}L^{2})}{2\alpha_ {t}\beta_{t}L^{2}-\beta_{t}^{2}L^{2}(1+\alpha_{t}^{2}L^{2})}=\frac{1}{L^{2} \left(\frac{2\alpha_{t}}{\beta_{t}\sigma^{2}(1+\alpha_{t}^{2}L^{2})}-1\right)}\]

Lastly, substituting \(\beta_{t}=\gamma\alpha_{t}\) into the RHS gives

\[\mathbb{E}[\left\|\boldsymbol{z}_{t+1}\right\|^{2}]\geq\frac{1}{L^{2}\left( \frac{2}{\gamma\sigma^{2}(1+\alpha_{t}^{2}L^{2})}-1\right)}\geq\frac{ \gamma\sigma^{2}}{2L^{2}}.\]

This finishes the proof.

_Remark H.5_.: We remark that, while Theorem H.4 successfully shows that SEG-US as studied in [17, 20] cannot converge to an optimal point unless the batch size is increased every iteration, it does not contradict the (almost sure) convergence result of independent-sample SEG by Hsieh et al. [25]. Indeed, in [25], the stepsizes \(\{\alpha_{t}\}_{t\geq 0}\) and \(\{\beta_{t}\}_{t\geq 0}\) are chosen so that they decay to \(0\) with a _different_ rate and hence the corresponding ratio \(\gamma\) approaches \(0\), while Theorem H.4 considers the case where \(\alpha_{t}\) and \(\beta_{t}\) differ by a constant factor \(\gamma\).

### Proof of SGDA-RR and SEG-RR Lower Bounds

**Theorem H.6**.: _Suppose \(n\geq 2\) and \(L,\mu>0\) satisfies \(L/\mu\geq 2\). There exists a \(\mu\)-strongly-convex-strongly-concave minimax problem \(f(\bm{z})=\frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{z})\) consisting of \(L\)-smooth quadratic \(f_{i}\)'s satisfying Assumption 3.4 with \((\rho,\sigma)=(0,\sigma)\) and initialization \(\bm{z}_{0}^{0}\) such that SEG-RR with any constant stepsize \(\alpha_{k}=\alpha>0\), \(\beta_{k}=\beta>0\) satisfies_

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}-\bm{z}^{\star}\right\|^{2}\right]=\begin{cases} \Omega\left(\frac{\sigma^{2}}{L\mu nK}\right)&\text{ if }K\leq L/\mu,\\ \Omega\left(\frac{L\sigma^{2}}{\mu^{2}nK^{3}}\right)&\text{ if }K>L/\mu.\end{cases}\]

_where \(\bm{z}^{\star}\) is the unique equilibrium point of \(f\). For a similar choice of problem \(f\) (this time with \((\rho,\sigma)=(1,\sigma)\)), SGDA-RR with any constant stepsize \(\alpha_{k}=\alpha>0\) satisfies_

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}-\bm{z}^{\star}\right\|^{2}\right]= \begin{cases}\Omega\left(\frac{\sigma^{2}}{L\mu nK}\right)&\text{ if }K\leq L/\mu,\\ \Omega\left(\frac{\sigma^{2}}{\mu^{2}n^{2}K^{2}}+\frac{L\sigma^{2}}{\mu^{3}nK ^{3}}\right)&\text{ if }K>L/\mu.\end{cases}\]

_Remark H.7_.: In Theorem H.6, we adopt techniques from the existing lower bounds for SGD-RR to prove lower bounds for the minimax algorithms SGDA-RR and SEG-RR. In the literature, there are two types of lower bounds for SGD-RR when \(K\gtrsim L/\mu\): namely, \(\Omega(\frac{1}{n^{2}K^{2}}+\frac{1}{nK^{3}})\) bounds for strongly convex _quadratic_ functions [49, 50] and \(\Omega(\frac{1}{nK^{2}})\) bounds for strongly convex _non-quadratic_ functions [45, 11, 56]. Upper bounds that match the lower bounds in \(n\) and \(K\) are also known, indicating that SGD-RR is one of the rare examples of minimization algorithms whose tight convergence rates for quadratic vs. non-quadratic functions differ, within the narrow scope of strongly convex and smooth functions. While it is tempting to aim for a tighter \(\Omega(\frac{1}{nK^{2}})\) lower bound for our algorithms of interest, we note that the existing \(\Omega(\frac{1}{nK^{2}})\) bounds for SGD-RR are proven for piecewise-quadratic functions whose Hessian is _discontinuous_. Since the discontinuous Hessian violates our Assumption 3.3, we instead adhere to the quadratic case to prove lower bounds \(\Omega(\frac{1}{nK^{3}})\) for both SGDA-RR and SEG-RR (when \(K\geq L/\mu\)). These bounds may not be the tightest possible (since they are restricted to quadratics), but they still suffice to demonstrate that SEG-FFA is provably superior to both SGDA-RR and SEG-RR.

#### h.3.1 Existing Lower Bound for SGD-RR

For the proof of lower bounds for SGDA-RR and SEG-RR, we utilize the results and techniques from the lower bounds proven for SGD-RR; thus, it would be profitable to summarize the existing result.

In case of SGD-RR, it is known from Theorem 2 of Safran and Shamir [50] that there exists a minimization problem \(g(\bm{x})\) such that SGD-RR satisfies a lower bound of \(\Omega(\frac{1}{n^{2}K^{2}}+\frac{1}{nK^{3}})\) for large enough values of \(K\). We rewrite the theorem in a version in accordance with our notation and assumptions:

**Theorem H.8** (Theorem 2 of Safran and Shamir [50]).: _For any \(n\geq 2\) and \(L,\mu>0\) satisfying \(L/\mu\geq 2\), there exists a \(\mu\)-strongly convex minimization problem \(g(\bm{x})=\frac{1}{n}\sum_{i=1}^{n}g_{i}(\bm{x})\) consisting of \(L\)-smooth quadratic \(g_{i}\)'s satisfying Assumption 3.4 with \((\rho,\sigma)=(1,\sigma)\) such that SGD-RR using any constant stepsize \(\alpha_{k}=\alpha>0\) satisfies_

\[\mathbb{E}\left[\left\|\bm{x}_{0}^{K}-\bm{x}^{\star}\right\|^{2}\right]= \Omega\left(\frac{\sigma^{2}}{L\mu nK}\cdot\min\left\{1,\frac{L}{\mu nK}+\frac {L^{2}}{\mu^{2}K^{2}}\right\}\right).\]

The statement is equivalent to saying that for SGD-RR with constant stepsize \(\alpha>0\), the bound \(\Omega(\frac{\sigma^{2}}{L\mu nK})\) holds for \(K\lesssim L/\mu\) and \(\Omega(\frac{\sigma^{2}}{\mu^{2}n^{2}K^{2}}+\frac{L\sigma^{2}}{\mu^{3}nK^{3}})\) for \(K\gtrsim L/\mu\).

The function \(g=\frac{1}{n}\sum_{i=1}^{n}g_{i}\) used in the theorem is defined by the following component functions:

\[g_{i}(\bm{x})=g_{i}(x_{1},x_{2},x_{3})\coloneqq\frac{\mu}{2}x_{1}^{2}+\frac{L}{2 }x_{2}^{2}+\begin{cases}\frac{\sigma}{2}x_{2}+\frac{L}{2}x_{3}^{2}+\frac{\sigma }{2}x_{3}&i\leq\frac{n}{2},\\ -\frac{\sigma}{2}x_{2}-\frac{\sigma}{2}x_{3}&i>\frac{n}{2},\end{cases}\] (126)

thus making the objective function

\[g(x_{1},x_{2},x_{3})\coloneqq\frac{\mu}{2}x_{1}^{2}+\frac{L}{2}x_{2}^{2}+\frac {L}{4}x_{3}^{2}.\]

One can notice that the linear terms in \(g_{i}\) (126) change signs depending on \(i\leq\frac{n}{2}\) or not, and handling these sign flips is the key to the proof of the lower bound.

#### h.3.2 Proof of Lower Bound for Sgda-Rr

For the Sgda-Rr lower bound, we consider the following minimax optimization problem:

\[f(\bm{x},y) =\frac{1}{n}\sum_{i=1}^{n}f_{i}(\bm{x},y),\text{ where }\bm{x} \in\mathbb{R}^{3},\ y\in\mathbb{R},\] (127) \[f_{i}(\bm{x},y) =g_{i}(\bm{x})-\frac{\mu}{2}y^{2},\]

where \(g_{i}\)'s are from (126). We need to first check if the problem instance satisfies the assumptions listed in the theorem statement. Since \(f(\bm{x},y)=g(\bm{x})-\frac{\mu}{2}y^{2}\) and \(g\) is a \(\mu\)-strongly convex function, \(f\) is \(\mu\)-strongly-convex-strongly-concave as claimed. Also, it is easy to check from the definition of \(g_{i}\) that each component \(f_{i}(\bm{x},y)\) is \(L\)-smooth quadratic.

Lastly, to verify Assumption 3.4, we first define \(s_{1},\ldots,s_{n}\) as \(s_{i}=1\) for \(i\leq\frac{n}{2}\) and \(s_{i}=0\) for \(i>\frac{n}{2}\). Using this notation, The function \(g_{i}\) can be compactly written as the following:

\[g_{i}(x_{1},x_{2},x_{3})=\frac{\mu}{2}x_{1}^{2}+\frac{L}{2}x_{2}^{2}+\frac{ \sigma}{2}(2s_{i}-1)x_{2}+\frac{L}{2}s_{i}x_{3}^{2}+\frac{\sigma}{2}(2s_{i}-1 )x_{3}.\]

Therefore, the saddle gradient operators \(\bm{F}_{i}\) of \(f_{i}\) and \(\bm{F}\) of \(f\) evaluate to

\[\bm{F}_{i}\bm{z}\coloneqq\begin{bmatrix}\nabla g_{i}(\bm{x})\\ \mu y\end{bmatrix}=\begin{bmatrix}\mu x_{1}\\ Lx_{2}+\frac{\sigma}{2}(2s_{i}-1)\\ Ls_{i}x_{3}+\frac{\sigma}{2}(2s_{i}-1)\\ \mu y\end{bmatrix},\quad\bm{F}\bm{z}=\begin{bmatrix}\mu x_{1}\\ Lx_{2}\\ \frac{L}{2}x_{3}\\ \mu y\end{bmatrix},\]

which in turn yields

\[\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2}=\frac{\sigma^{2}}{4}+\left( \frac{L}{2}x_{3}+\frac{\sigma}{2}\right)^{2}\leq\left(\frac{L}{2}|x_{3}|+\sigma \right)^{2}\leq\left(\left\|\bm{F}\bm{z}\right\|+\sigma\right)^{2}\]

for all \(i=1,\ldots,n\). This confirms that the function \(f=\frac{1}{n}\sum_{i}f_{i}\) satisfies Assumption 3.4 with \((\rho,\sigma)=(1,\sigma)\).

If we run Sgda-Rr on this problem, the updates on \(\bm{x}\) done by Sgda-Rr is exactly identical to what Sgd-Rr would perform for the minimization problem \(g(\bm{x})=\frac{1}{n}\sum_{i}g_{i}(\bm{x})\) with the same choices of random permutations. Therefore, after \(K\) epochs of Sgda-Rr, it follows from Theorem H.8 that

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}-\bm{z}^{*}\right\|^{2}\right]\geq \mathbb{E}\left[\left\|\bm{x}_{0}^{K}-\bm{x}^{*}\right\|^{2}\right]=\Omega \left(\frac{\sigma^{2}}{L\mu nK}\cdot\min\left\{1,\frac{L}{\mu nK}+\frac{L^{2} }{\mu^{2}K^{2}}\right\}\right),\]

which is in fact a tighter lower bound for Sgda-Rr than what is stated in Theorem H.6. This finishes the proof.

#### h.3.3 Proof of Lower Bound for Seg-Rr

In this subsection, we prove the lower bound for SEG-Rr. We will first define a new problem instance \(f\) to be used here, and verify that the assumptions in the theorem statement are indeed satisfied by this new \(f\). We will then spell out the update equation of SEG-Rr for this example, which will serve as a basis for the case analysis that follows: we will divide the choices of stepsizes\(\alpha,\beta>0\) to four regimes and prove a lower bound for each of them. Combining the regimes will result in the desired lower bound.

For SEG-RR, we use a slightly different problem from (127). This time, we consider

\[\begin{split} f(\bm{x},y)&=\frac{1}{n}\sum_{i=1}^{n}f_ {i}(\bm{x},y),\text{ where }\bm{x}\in\mathbb{R}^{2},\ y\in\mathbb{R},\\ f_{i}(\bm{x},y)&=\frac{L}{2}x_{1}^{2}+\frac{L}{4} x_{2}^{2}+\sigma(2s_{i}-1)x_{2}-\frac{\mu}{2}y^{2},\end{split}\] (128)

where \(s_{i}=1\) for \(i\leq\frac{n}{2}\) and \(s_{i}=0\) for \(i>\frac{n}{2}\), as defined above.

We first check if the problem (128) satisfies the assumptions in the theorem statement. Since

\[f(\bm{x},y)=\frac{L}{2}x_{1}^{2}+\frac{L}{4}x_{2}^{2}-\frac{\mu}{2}y^{2}\]

and \(L/2\geq\mu\) by assumption, \(f\) is \(\mu\)-strongly-convex-strongly-concave. Also, it is straightforward to see that each \(f_{i}\) is an \(L\)-smooth quadratic function. It is left to check Assumption 3.4. The saddle gradient operators \(\bm{F}_{i}\) of \(f_{i}\) and \(\bm{F}\) of \(f\) evaluate to

\[\bm{F}_{i}\bm{z}=\begin{bmatrix}Lx_{1}\\ \frac{L}{2}x_{2}+\sigma(2s_{i}-1)\\ \mu y\end{bmatrix},\quad\bm{F}\bm{z}=\begin{bmatrix}Lx_{1}\\ \frac{L}{2}x_{2}\\ \mu y\end{bmatrix},\]

which in turn yields

\[\left\|\bm{F}_{i}\bm{z}-\bm{F}\bm{z}\right\|^{2}=\sigma^{2},\]

for all \(i=1,\ldots,n\). This confirms that the function \(f=\frac{1}{n}\sum_{i}f_{i}\) satisfies Assumption 3.4 with \((\rho,\sigma)=(0,\sigma)\), as required by the theorem.

For \(k\geq 0\), the \((k+1)\)-th epoch of SEG-RR starts at \(\bm{z}_{0}^{k}=(\bm{x}_{0}^{k},y_{0}^{k})\) and the algorithm chooses a random permutation \(\tau_{k}\). The algorithm then goes through a series of updates

\[\bm{w}_{i}^{k} =\bm{z}_{i}^{k}-\alpha\bm{F}_{\tau_{k}(i+1)}\bm{z}_{i}^{k},\] \[\bm{z}_{i+1}^{k} =\bm{z}_{i}^{k}-\beta\bm{F}_{\tau_{k}(i+1)}\bm{w}_{i}^{k},\]

for \(i=0,\ldots,n-1\). For our example \(f\) (128), it can be checked that a single iteration by SEG-RR reads

\[\bm{z}_{i+1}^{k}=\begin{bmatrix}x_{i+1,1}^{k}\\ x_{i+1,2}^{k}\\ y_{i+1}^{k}\end{bmatrix}=\begin{bmatrix}(1-\beta L+\alpha\beta L^{2})x_{i,1} ^{k}\\ (1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4})x_{i,2}^{k}-\beta\sigma(1- \frac{\alpha L}{2})(2s_{\tau_{k}(i+1)}-1)\\ (1-\beta\mu+\alpha\beta\mu^{2})y_{i}^{k}\end{bmatrix}.\]

Aggregating the SEG-RR updates over an entire epoch (\(i=0,\ldots,n-1\)) results in

\[x_{0,1}^{k+1} =(1-\beta L+\alpha\beta L^{2})^{n}x_{0,1}^{k},\] \[x_{0,2}^{k+1} =\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{n} x_{0,2}^{k}-\beta\sigma\left(1-\frac{\alpha L}{2}\right)\underbrace{\sum_{i=1}^{n}(2s_ {\tau_{k}(i)}-1)\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^ {n-i}}_{=:\,\Phi},\]

\[y_{0}^{k+1} =(1-\beta\mu+\alpha\beta\mu^{2})^{n}y_{0}^{k}.\]

We will now square both sides of these equations above and take expectations over \(\tau_{k}\). In doing so, there is a useful identity:

\[\mathbb{E}[\Phi]=\sum_{i=1}^{n}\mathbb{E}[2s_{\tau_{k}(i)}-1]\left(1-\frac{ \beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{n-i}=0.\]

Also, it is worth mentioning that \(\tau_{k}\) is independent of \(\bm{z}_{0}^{k}=(x_{0,1}^{k},x_{0,2}^{k},y_{0}^{k})\). Using these facts, we can arrange the terms to obtain

\[(x_{0,1}^{k+1})^{2} =(1-\beta L+\alpha\beta L^{2})^{2n}(x_{0,1}^{k})^{2},\] (129) \[\mathbb{E}[(x_{0,2}^{k+1})^{2}] =\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2n} \mathbb{E}[(x_{0,2}^{k})^{2}]+\beta^{2}\sigma^{2}\left(1-\frac{\alpha L}{2} \right)^{2}\mathbb{E}[\Phi^{2}],\] (130) \[(y_{0}^{k+1})^{2} =(1-\beta\mu+\alpha\beta\mu^{2})^{2n}(y_{0}^{k})^{2}.\] (131)Based on these three per-epoch update equations above, we now divide the choices of SEG-RR stepsizes \(\alpha,\beta>0\) into the following four cases and handle them separately:

1. \(\alpha>\frac{1}{L}\), in which case we show that SEG-RR makes \((x_{0,1}^{k+1})^{2}>(x_{0,1}^{k})^{2}\) hold deterministically, so that if we initialize at \(x_{0,1}^{0}=\frac{\sigma}{\sqrt{L\mu}}\) then we have \[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]\geq(x_{0,1}^{K})^{2 }>(x_{0,1}^{0})^{2}=\frac{\sigma^{2}}{L\mu}.\]
2. \(\alpha\leq\frac{1}{L}\) and \(\beta\leq\frac{1}{\mu nK}\), in which case we show that SEG-RR initialized at \(y_{0}^{0}=\frac{\sigma}{\sqrt{L\mu}}\) suffers \[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]=\Omega\left(\frac{ \sigma^{2}}{L\mu}\right),\]
3. \(\alpha\leq\frac{1}{L}\) and \(\frac{1}{\mu nK}<\beta<\frac{1}{nL}\), in which case we show that SEG-RR initialized at \(x_{0,2}^{0}=0\) suffers \[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]=\Omega\left(\frac{ L\sigma^{2}}{\mu^{3}nK^{3}}\right),\]
4. \(\alpha\leq\frac{1}{L}\), \(\beta>\frac{1}{\mu nK}\), and \(\beta\geq\frac{1}{nL}\) in which case we show that SEG-RR initialized at \(x_{0,2}^{0}=0\) suffers \[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]=\Omega\left(\frac{ \sigma^{2}}{L\mu nK}\right).\]

Notice that the third case \(\frac{1}{\mu nK}<\beta<\frac{1}{nL}\) only makes sense when \(K>L/\mu\); otherwise, the third case just disappears. Hence, for the "large epoch" regime where \(K>L/\mu\), the third case achieves the minimum error possible, so it holds that

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]=\Omega\left(\frac{L \sigma^{2}}{\mu^{3}nK^{3}}\right).\]

For the "small epoch" regime (\(K\leq L/\mu\)), the third case does not exist and the fourth case achieves the minimum, so

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]=\Omega\left(\frac{ \sigma^{2}}{L\mu nK}\right).\]

Combining the two cases yields the desired lower bound in the theorem statement. It now remains to carry out the case analysis.

Case 1: \(\alpha>\frac{1}{L}\).For this case, we use (129) to prove divergence. Notice from \(\alpha>\frac{1}{L}\) that

\[1-\beta L+\alpha\beta L^{2}=1+\beta L(\alpha L-1)>1,\]

regardless of \(\beta>0\). Hence, from (129), we get

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]\geq(x_{0,1}^{K})^{2} >(x_{0,1}^{0})^{2}.\]

If we initialize at \(x_{0,1}^{0}=\frac{\sigma}{\sqrt{L\mu}}\), then this proves

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]\geq\frac{\sigma^{2}}{ L\mu}.\]

Case 2: \(\alpha\leq\frac{1}{L}\) and \(\beta\leq\frac{1}{\mu nK}\).For this case, we employ (131) to show that the "contraction rate" is too small to make enough "progress." Notice from our stepsizes that

\[1-\beta\mu+\alpha\beta\mu^{2}\geq 1-\beta\mu\geq 1-\frac{1}{nK}\geq 0.\]

Applying this inequality to (131), we have

\[(y_{0}^{k+1})^{2}\geq\left(1-\frac{1}{nK}\right)^{2n}(y_{0}^{k})^{2},\]which in turn means that the progress over \(K\) epoch is bounded from below by

\[(y_{0}^{K})^{2}\geq\left(1-\frac{1}{nK}\right)^{2nK}(y_{0}^{0})^{2}\geq\frac{(y_ {0}^{0})^{2}}{16},\]

where we used our assumption that \(n\geq 2\) and \(K\geq 1\). Hence, if our initialization was given as \(y_{0}^{0}=\frac{\sigma}{\sqrt{L\mu}}\), then this proves

\[\mathbb{E}\left[\left\|\mathbf{z}_{0}^{K}\right\|^{2}\right]\geq(y_{0}^{K})^{ 2}\geq\frac{(y_{0}^{0})^{2}}{16}=\Omega\left(\frac{\sigma^{2}}{L\mu}\right).\]

Case 3:\(\alpha\leq\frac{1}{L}\) and \(\frac{1}{\mu nK}<\beta<\frac{1}{nL}\).For stepsizes in this interval, we use (130) to derive the desired bound. Here, it is important to characterize a lower bound on the quantity

\[\mathbb{E}[\Phi^{2}]:=\mathbb{E}\left[\left(\sum_{i=1}^{n}(2s_{\tau_{k}(i)}-1 )\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{n-i}\right)^{ 2}\right].\]

To this end, we can use a lemma from Safran and Shamir [49], stated below:

**Lemma H.9** (Lemma 1 of Safran and Shamir [49]).: _Let \(\pi_{1},\ldots,\pi_{n}\) (for even \(n\)) be a random permutation of \((1,1,\ldots,1,-1,-1,\ldots,-1)\) where both \(1\) and \(-1\) appear exactly \(n/2\) times. Then there is a numerical constant \(c>0\) such that for any \(\nu>0\),_

\[\mathbb{E}\left[\left(\sum_{i=1}^{n}\pi_{i}(1-\nu)^{n-i}\right)^{2}\right] \geq c\cdot\min\left\{1+\frac{1}{\nu},n^{3}\nu^{2}\right\}.\]

One can notice that Lemma H.9 is directly applicable to \(\mathbb{E}[\Phi^{2}]\), with \(\nu\leftarrow\frac{\beta L}{2}-\frac{\alpha\beta L^{2}}{4}\). Since

\[\nu=\frac{\beta L}{2}-\frac{\alpha\beta L^{2}}{4}\leq\frac{\beta L}{2}\leq \frac{1}{2n},\]

we have \(n^{3}\nu^{2}\leq\frac{1}{8\nu}\), thereby

\[\min\left\{1+\frac{1}{\nu},n^{3}\nu^{2}\right\}\geq\min\left\{\frac{1}{\nu}, n^{3}\nu^{2}\right\}=n^{3}\nu^{2}.\]

Therefore, Lemma H.9 gives

\[\mathbb{E}[\Phi^{2}]\geq cn^{3}\left(\frac{\beta L}{2}-\frac{\alpha\beta L^{ 2}}{4}\right)^{2}=\frac{c\beta^{2}n^{3}L^{2}}{4}\left(1-\frac{\alpha L}{2} \right)^{2}\geq\frac{c\beta^{2}n^{3}L^{2}}{16},\] (132)

where the last inequality used \(\alpha\leq\frac{1}{L}\). Applying (132) to (130) and also using \((1-\frac{\alpha L}{2})^{2}\geq\frac{1}{4}\),

\[\mathbb{E}[(x_{0,2}^{k+1})^{2}]\geq\left(1-\frac{\beta L}{2}+\frac{\alpha \beta L^{2}}{4}\right)^{2n}\mathbb{E}[(x_{0,2}^{k})^{2}]+\frac{c\beta^{4}n^{3} L^{2}\sigma^{2}}{64}.\]

Unrolling the inequality for \(k=0,\ldots,K-1\) gives

\[\mathbb{E}\left[(x_{0,2}^{K})^{2}\right] \geq\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right) ^{2nK}(x_{0,2}^{0})^{2}+\frac{c\beta^{4}n^{3}L^{2}\sigma^{2}}{64}\sum_{j=0}^{ K-1}\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2nj}\] \[=\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2 nK}(x_{0,2}^{0})^{2}+\frac{c\beta^{4}n^{3}L^{2}\sigma^{2}}{64}\cdot\frac{1- \left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2nK}}{1-\left( 1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2n}}.\]

Now note that our initialization \(x_{0,2}^{0}\) can be set to zero, which eliminates the need to think about the first term in the RHS. It is now left to bound the second term. First, by the stepsize range \(\alpha\leq\frac{1}{L}\), \(\beta>\frac{1}{\mu nK}\) and our assumption \(L/\mu\geq 2\), we have

\[\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2nK}\leq\left(1- \frac{\beta L}{4}\right)^{2nK}\leq\left(1-\frac{L}{4\mu nK}\right)^{2nK}\leq e ^{-\frac{L}{2n}}\leq e^{-1}.\]Next, by Bernoulli's inequality

\[\left(1-\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2n}\geq\left(1- \frac{\beta L}{2}\right)^{2n}\geq 1-\beta nL>0.\]

Plugging in the two inequalities to above, we obtain

\[\mathbb{E}\left[(x_{0,2}^{K})^{2}\right] \geq\frac{c\beta^{4}n^{3}L^{2}\sigma^{2}}{64}\cdot\frac{1-\left(1 -\frac{\beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2nK}}{1-\left(1-\frac{ \beta L}{2}+\frac{\alpha\beta L^{2}}{4}\right)^{2n}}\] \[\geq\frac{c\beta^{4}n^{3}L^{2}\sigma^{2}}{64}\cdot\frac{1-e^{-1}} {1-(1-\beta nL)}=c^{\prime}\beta^{3}n^{2}L\sigma^{2}\]

for a numerical constant \(c^{\prime}>0\). Plugging in the lower bound \(\beta>\frac{1}{\mu nK}\) yields

\[\mathbb{E}\left[\left\|\bm{z}_{0}^{K}\right\|^{2}\right]\geq\mathbb{E}\left[( x_{0,2}^{K})^{2}\right]=\Omega\left(\frac{L\sigma^{2}}{\mu^{3}nK^{3}}\right).\]

Case 4: \(\alpha\leq\frac{1}{L}\), \(\beta>\frac{1}{\mu nK}\), and \(\beta\geq\frac{1}{nL}\).We again use (130). By noticing that the initialization \(x_{0,2}^{0}=0\), we can unroll (130) for \(k=0,\ldots,K-1\) to get

\[\mathbb{E}\left[(x_{0,2}^{K})^{2}\right]\geq\frac{\beta^{2}\sigma^{2}}{4} \mathbb{E}[\Phi^{2}]\sum_{j=0}^{K-1}\left(1-\frac{\beta L}{2}+\frac{\alpha \beta L^{2}}{4}\right)^{2nj}\geq\frac{\beta^{2}\sigma^{2}}{4}\mathbb{E}[\Phi^{ 2}],\] (133)

where the last inequality holds regardless of \(\beta\) because each summand with \(j\geq 1\) is nonnegative. We then invoke Lemma H.9 to lower bound \(\mathbb{E}[\Phi^{2}]\), again with \(\nu\leftarrow\frac{\beta L}{2}-\frac{\alpha\beta L^{2}}{4}\). Since

\[\nu=\frac{\beta L}{2}-\frac{\alpha\beta L^{2}}{4}\geq\frac{\beta L}{4}\geq \frac{1}{4n},\]

we have \(n^{3}\nu^{2}\geq\frac{1}{64\nu}\), thereby

\[\min\left\{1+\frac{1}{\nu},n^{3}\nu^{2}\right\}\geq\min\left\{\frac{1}{\nu}, n^{3}\nu^{2}\right\}\geq\frac{1}{64\nu}.\]

Therefore, Lemma H.9 gives

\[\mathbb{E}[\Phi^{2}]\geq\frac{c}{64\nu}=\frac{c}{32\beta L}\cdot\frac{1}{1- \frac{\alpha L}{2}}\geq\frac{c}{32\beta L}.\] (134)

Combining (134) with (133) gives

\[\mathbb{E}\left[(x_{0,2}^{K})^{2}\right]\geq\frac{c\beta\sigma^{2}}{128L}= \Omega\left(\frac{\sigma^{2}}{L\mu nK}\right),\]

where the last step used \(\beta>\frac{1}{\mu nK}\). This finishes the case analysis, hence the proof of Theorem H.6.

## Appendix I Additional Experiments

To evaluate our algorithm SEG-FFA as well as other baseline algorithms, we conduct numerical experiments on monotone and strongly monotone problems. Specifically, as we have mentioned in Section 6, we consider random quadratic problems of the form

\[\min_{\bm{x}\in\mathbb{R}^{d_{x}}}\max_{\bm{y}\in\mathbb{R}^{d_{y}}}\ \frac{1}{n}\sum_{i=1}^{n}\ \begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}^{\top}\begin{bmatrix}\bm{A}_{i}&\bm{B}_{i}\\ \bm{B}_{i}^{\top}&-\bm{C}_{i}\end{bmatrix}\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}-\bm{t}_{i}^{\top}\begin{bmatrix}\bm{x}\\ \bm{y}\end{bmatrix}.\]

We choose \(d_{x}=d_{y}=20\) and \(n=40\) for all the experiments. Numerical computations are done using NumPy[24] and SciPy[52], and the plots are drawn using Matplotlib[26].

### Problem Constructions for Experiments in Section 6

For an experiment for the monotone case, the random components are sampled as follows. We choose \(\bm{B}_{i}\) so that each element is an i.i.d. sample from a uniform distribution over the interval \([0,1]\), and \(\bm{t}_{i}\) so that each element is an i.i.d. sample from a standard normal distribution. We chose \(\bm{A}_{i}\) to be diagonal matrices in the following procedure: for each \(j=1,\ldots,20\) we randomly chose a subset \(\mathcal{I}_{j}\) of \(\frac{n}{2}=20\) indices from \([n]=\{1,\ldots,40\}\), and set the \((j,j)\)-entry of \(\bm{A}_{i}\) to be

\[(\bm{A}_{i})_{j,j}=\begin{cases}2&\text{if }i\in\mathcal{I}_{j}\\ -2&\text{otherwise}\end{cases}.\]

We repeat the exact same procedure for \(\bm{C}_{i}\) as well. Notice that \(\sum_{i=1}^{n}\bm{A}_{i}=\sum_{i=1}^{n}\bm{C}_{i}=\bm{0}\) by design. Hence, each of the component functions will be a nonconvex-nonconcave quadratic function in general, but the objective function itself becomes a convex-concave function.

For the experiment in the strongly monotone case, we sample \(\bm{B}_{i}\) and \(\bm{t}_{i}\) in the same way as in the monotone case, but we use different choices of \(\bm{A}_{i}\) and \(\bm{C}_{i}\) to ensure the objective function to be strongly-convex-strongly-concave. In particular, for each \(i=1,\ldots,n\), we sample \(\bm{A}_{i}\) by computing \(\bm{A}_{i}=\bm{Q}_{i}\bm{D}_{i}\bm{Q}_{i}^{\top}\), where \(\bm{D}_{i}\) is a random diagonal matrix whose diagonal entries are i.i.d. samples from a uniform distribution over the interval \([\frac{1}{2},1]\), and \(\bm{Q}_{i}\) is a random orthogonal matrix obtained by computing a _QR_ decomposition of a \(20\times 20\) random matrix whose elements are i.i.d. samples from a standard normal distribution. We sample \(\bm{C}_{i}\) by the exact same method.

### Monotone Case & Ablation Study on the Anchoring Step

In Section 6, we compared the empirical performance of various SEGs, namely SEG-FFA, SEG-FF, SEG-FR, and SEG-US. Here, as an ablation study on the anchoring technique, we additionally compare _SEG-RRA_ and _SEG-USA_, which are each SEG-RR and SEG-US with an additional anchoring step, respectively. For these two methods, we take the anchoring step after every \(n\) iterations. We ran those methods on the same \(5\) random instances used in Section 6. For both SEG-RRA and SEG-USA, we ran the method with two different stepsize choices, namely \(\alpha_{k}=\beta_{k}=\eta_{k}\) (inspired by the stepsize used in deterministic EG) and \(\alpha_{k}=\nicefrac{{\beta_{k}}}{{2}}=\nicefrac{{\eta_{k}}}{{2}}\) (the stepsize used for SEG-FFA) where we again set \(\eta_{k}=\nicefrac{{\eta_{0}}}{{(1+k/10)^{0.34}}}\) with \(\eta_{0}=\min\{0.01,\frac{1}{L}\}\).

The results are plotted in Figure 2. As SEG-RRA and SEG-USA are designed to take one pass per epoch, for those methods, we compute the ratio \(\frac{\left\|\bm{F}\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{ \bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z }_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0} ^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{ \bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{ \bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{ \bm{z}_{0}^{\bm{z}}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z }}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}}_{0}^{\bm{z}_{0}^{\bm{z}_{0} ^{\bm{z}}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}^{\bm{z}_{0}}^{\bm{z}_{0}^{\bm{z }}_{0}^{\bm{z}_{0}}^{\bm{z}_{0}}^{\bm{z}_{0}}^{\bm{z}_{0}}}^{\bm{z}_{0}}}^{\bm{z }_{0}}}^{\bm{z}_{0}}}^{\bm{z}_{0}}}^{\bm{z}_{0}}}^{\bm{z}_{0}}^{\bm{z}_{0}}^{\bm{z }_{0}}^{\bm{z}_{0}}^{\bm{z}_{0}}^{\bm{z}_{0}}}}\) where \(t\) denotes the number of passes, and plot the geometric mean over the \(5\) runs.

From the performance of SEG-RRA with \(\alpha_{k}=\beta_{k}\) and the two variants of SEG-USA, it is possible to observe that adding the anchoring step does improve the performance of the method up to a certain level, but it alone does not fully resolve the nonconvergence issue. On the other hand, quite interestingly, SEG-RRA with \(\alpha_{k}=\nicefrac{{\beta_{k}}}{{2}}\) shows a hint of convergence. While its performance is slightly worse compared to SEG-FFA, it is nonetheless still notable as it is the only other method from SEG-FFA that seems to be capable of converging to an optimum.

We conjecture that this intriguing performance of SEG-RRA with \(\alpha_{k}=\nicefrac{{\beta_{k}}}{{2}}\) is because it achieves an _"expected"_ second order matching to the (deterministic) EG. Indeed, following the notations of Proposition D.1, one can deduce from Proposition D.1 that using SEG-RRA with \(\alpha=\nicefrac{{\beta}}{{2}}\) will result in an epoch-level update of

\[\bm{z}^{\sharp}=\bm{z}_{0}-\frac{\beta}{2}\sum_{j=0}^{n-1}\bm{T}_{j}\bm{z}_{0}+ \frac{\beta^{2}}{4}\sum_{j=0}^{n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{j}\bm{z}_{0}+ \frac{\beta^{2}}{2}\sum_{0\leq i<j\leq n-1}D\bm{T}_{j}(\bm{z}_{0})\bm{T}_{i}\bm {z}_{0}+\frac{\bm{\epsilon}_{n}}{2}\] (135)

with \(\bm{\epsilon}_{n}=o\left(\beta^{2}\right)\). Here, notice that \((\bm{T}_{0},\bm{T}_{1},\ldots,\bm{T}_{n-1})=(\bm{F}_{\tau(1)},\bm{F}_{\tau(2)},\ldots,\bm{F}_{\tau(n)})\) for some randomly chosen permutation \(\tau\in\mathcal{S}_{n}\). Now, observe that for any two distinct \(i,j\in[n]\), there are exactly \(\frac{n!}{2}\) permutations in \(\mathcal{S}_{n}\) such that \(i\) comes before \(j\) in the sequence \(\tau(1),\tau(2),\ldots,\tau(n)\), and also exactly \(\frac{n!}{2}\) permutations such that \(j\) comes before \(i\). Thus, in taking the expectation over therandomness of choosing the permutation \(\tau\), we get

\[\mathbb{E}_{\tau}\left[\sum_{0\leq i<j\leq n-1}D\bm{T}_{j}(\bm{z}_{0} )\bm{T}_{i}\bm{z}_{0}\right] =\mathbb{E}_{\tau}\left[\sum_{1\leq i<j\leq n}D\bm{F}_{\tau(j)}(\bm {z}_{0})\bm{F}_{\tau(i)}\bm{z}_{0}\right]\] \[=\frac{1}{n!}\sum_{\tau\in\mathcal{S}_{n}}\sum_{1\leq i<j\leq n}D \bm{F}_{\tau(j)}(\bm{z}_{0})\bm{F}_{\tau(i)}\bm{z}_{0}\] \[=\frac{1}{2}\sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i}\bm{z} _{0},\]

where in getting the third line we have used the previously made observation that for any fixed \(i\) and \(j\) with \(i\neq j\), the term \(D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i}\bm{z}_{0}\) appears exactly \(\frac{n!}{2}\) times in the sum on the second line. Hence, taking the expectation with respect to the random permutation on (135) we get

\[\mathbb{E}_{\tau}\left[\bm{z}^{\sharp}\right] =\bm{z}_{0}-\frac{n\beta}{2}\bm{F}\bm{z}_{0}+\frac{\beta^{2}}{4} \sum_{j=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{j}\bm{z}_{0}+\frac{\beta^{2}}{4} \sum_{i\neq j}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i}\bm{z}_{0}+\frac{1}{2}\,\mathbb{ E}_{\tau}\left[\bm{\epsilon}_{n}\right]\] \[=\bm{z}_{0}-\frac{n\beta}{2}\bm{F}\bm{z}_{0}+\frac{\beta^{2}}{4} \sum_{j=1}^{n}\sum_{i=1}^{n}D\bm{F}_{j}(\bm{z}_{0})\bm{F}_{i}\bm{z}_{0}+\frac{ 1}{2}\,\mathbb{E}_{\tau}\left[\bm{\epsilon}_{n}\right]\] \[=\bm{z}_{0}-\frac{n\beta}{2}\bm{F}\bm{z}_{0}+\frac{n^{2}\beta^{2} }{4}D\bm{F}(\bm{z}_{0})\bm{F}\bm{z}_{0}+\frac{1}{2}\,\mathbb{E}_{\tau}\left[ \bm{\epsilon}_{n}\right].\]

Comparing this to (7) when \(\eta_{1}=\eta_{2}=\nicefrac{{n\beta}}{{2}}\), we indeed see that the update rule of SEG-RRA with \(\alpha=\nicefrac{{\beta}}{{2}}\) achieves a second-order matching _on expectation_ to the (deterministic) EG update with stepsize \(\nicefrac{{n\beta}}{{2}}\).

We also conjecture that the relatively worse performance of SEG-RRA with \(\alpha=\nicefrac{{\beta}}{{2}}\) compared to SEG-FFA is because the error over an epoch is \(O(\eta^{3})\) only on expectation, and thus the actual error occurring in each epoch can be larger than \(O(\eta^{3})\). Unfortunately, our convergence analysis on SEG-FFA relies on the error over an epoch being \(O(\eta^{3})\)_deterministically_ (_cf._ Proposition 5.3), hence cannot be directly applied to SEG-RRA with \(\alpha=\nicefrac{{\beta}}{{2}}\). We leave the search for a theoretical explanation on this alluring performance of SEG-RRA with \(\alpha=\nicefrac{{\beta}}{{2}}\) as a stimulating direction for future work.

### Monotone Case: Comparison with Hsieh et al. [25]

Let us also compare the performance of SEG-FFA with the _independent-sample_ double stepsize SEG (_DSEG_) by Hsieh et al. [25]. Writing in terms of the finite-sum structure, the update rule of DSEG

Figure 2: Experimental results in the monotone example, comparing the performance of SEG-RRA and SEG-USA with the results displayed in Figure 1. Because SEG-FFA and SEG-FF use two passes per epoch, for those two methods, we plot \(\nicefrac{{\|\bm{F}\bm{z}_{0}^{\sharp}/2\|^{2}}}{\|\bm{F}\bm{z}_{0}^{\sharp}\|^ {2}}/\nicefrac{{\|\bm{F}\bm{z}_{0}^{\flat}\|^{2}}}{{2}}\).

can be written as

\[\bm{w}^{k} \leftarrow\bm{z}^{k}-\eta_{1,k}\bm{F}_{i(1,k)}\bm{z}^{k}\] \[\bm{z}^{k+1} \leftarrow\bm{z}^{k}-\eta_{2,k}\bm{F}_{i(2,k)}\bm{w}^{k}\]

where \(i(1,k)\) and \(i(2,k)\) are random indices that are independently drawn from \([n]\) for each \(k\). The stepsizes are chosen in the form of \(\eta_{1,k}=\Theta(\nicefrac{{1}}{{k^{\tau_{1}}}})\) and \(\eta_{2,k}=\Theta(\nicefrac{{1}}{{k^{\tau_{2}}}})\), where setting \(r_{1}\leq r_{2}\) is the key point of DSEG. Two choices of the exponent pair \((r_{1},r_{2})\) proposed in [25] are \((1/3,2/3)\) for general monotone problems and \((0,1)\) exclusively for the case when \(\bm{F}\) is affine.

We again use the same component functions as in the previous experiment. The setup for running SEG-FFA are kept the same. For DSEG, we use the default choices suggested by Hsieh et al. [25], namely \(\eta_{1,k}=\nicefrac{{\gamma_{0}}}{{(k+19)^{\gamma_{1}}}}\) and \(\eta_{2,k}=\nicefrac{{\eta_{0}}}{{(k+19)^{\gamma_{2}}}}\), where \((\gamma_{0},\eta_{0})=(1,0.1)\) for the bilinear case with \((r_{1},r_{2})=(0,1)\) and \((\gamma_{0},\eta_{0})=(0.1,0.05)\) for the general case with \((r_{1},r_{2})=(1/3,2/3)\).

The results are displayed in Figure 3, where the details on how the plots are drawn are the same as Figure 2. Here we can clearly see that SEG-FFA outperforms both versions of DSEG.

### Strongly Monotone Case Again, with Various Stepsizes

We also ran the experiment on strongly monotone problems described in Section 6, but with changing the stepsizes. We tested six different values of \(\eta_{k}\); we have tested with \(\eta_{k}=a\times 10^{b}\) where \(a\in\{1,2,5\}\) and \(b\in\{-4,-3\}\). Notice that the case \(\eta_{k}=10^{-3}\) is exactly the experiment conducted in Section 6.

The results are plotted in Figure 4. The overall details are the same as described in Section 6, as the only difference is the stepsize choice. We can observe that, while the initial speed of convergence may not be the fastest depending on the stepsize, SEG-FFA is always the method that eventually finds the point with the smallest gradient. In other words, as predicted by our theoretical analyses, the supremacy of SEG-FFA is in general not affected by the choice of the stepsize, as long as the chosen stepsize is reasonably small.

Figure 3: Experimental results in the monotone example, comparing SEG-FFA and the methods proposed by Hsieh et al. [25]. By the same reason as in Figure 2, we plot \(\nicefrac{{\|\bm{F}\bm{z}_{0}^{1/2}\|^{2}}}{{\|\bm{F}\bm{z}_{0}^{0}\|^{2}}}\) for SEG-FFA only.

Figure 4: Experimental results on the strongly monotone problems with different stepsizes. Notice that Figure 4d is exactly the plot that is included in Section 6. The only difference between the experiments conducted is the choice of the stepsize.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and the introduction well summarizes our theoretical results and the problem settings we are considering. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: While we do not have a separate "Limitations" sections, in Section 3 we thoroughly discuss about the assumptions we have imposed. The paper is highly theoretical, hence the other factors listed in the guidelines below are either not applicable to this paper, or apparent from the statements of the theorems/lemma/propositions and the discussions that follow. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Section 3 is devoted for the discussions on the assumptions. Full proofs of the theorems/lemma/propositions can be found in the appendices. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Appendix I, we provide full explanations on how the experiments have been conducted. We have also submitted the exact code that we used for our experiments as a supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have submitted the exact code that we used for our experiments as a supplemental material, so that it becomes revealed to the public once our paper gets accepted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The overall settings are discussed in Appendix I. The code we submit along with the paper is an exact copy of the one we used in the reported experiments, so the details not included in the paper shall be found in the code itself. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Our paper is mainly theoretical, and the experiments are to demonstrate that our analyses are correct. Hence, we claim that error bars or information about the statistical significance are not necessary, and rather, the interpretations we made regarding our experiments in the relevant section(s) are enough. Guidelines: ** The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: The experiments are numerical validations of our theoretical analyses using simple quadratic functions, so they should be executable on any modern computer with a reasonable CPU. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read through the Code of Ethics, but due to the theoretical nature of the paper, there are no risks regarding ethical issues. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [NA] Justification: There are no societal impacts of this paper, as it is a theory paper. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper is highly theoretical, hence poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Packages used in the experiments, NumPy, SciPy, and Matplotlib, are cited. No existing data nor models are used. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper provides novel theoretical results rather than datasets or models, hence this question is not applicable. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.