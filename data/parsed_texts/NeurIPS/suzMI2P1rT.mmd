# CEIL: Generalized Contextual Imitation Learning

 Jinxin Liu\({}^{1,2}\) Li He\({}^{1}\)1  Yachen Kang\({}^{1,2}\) Zifeng Zhuang\({}^{1,2}\)

**Donglin Wang\({}^{1,4}\) Huazhe Xu\({}^{3,5,6}\)**

\({}^{1}\)Westlake University \({}^{2}\)Zhejiang University \({}^{3}\)Tsinghua University \({}^{4}\)Westlake Institute for Advanced Study \({}^{5}\)Shanghai Qi Zhi Institute \({}^{6}\)Shanghai AI Lab

Equal contributions. Corresponding author: Donglin Wang <wangdonglin@westlake.edu.cn>

###### Abstract

In this paper, we present **C**ont**E**xtual **I**mitation **L**earning (CEIL), a general and broadly applicable algorithm for imitation learning (IL). Inspired by the formulation of hindsight information matching, we derive CEIL by explicitly learning a hindsight embedding function together with a contextual policy using the hindsight embeddings. To achieve the expert matching objective for IL, we advocate for optimizing a contextual variable such that it biases the contextual policy towards mimicking expert behaviors. Beyond the typical learning from demonstrations (LfD) setting, CEIL is a generalist that can be effectively applied to multiple settings including: 1) learning from observations (LfO), 2) offline IL, 3) cross-domain IL (mismatched experts), and 4) one-shot IL settings. Empirically, we evaluate CEIL on the popular MuJoCo tasks (online) and the D4RL dataset (offline). Compared to prior state-of-the-art baselines, we show that CEIL is more sample-efficient in most online IL tasks and achieves better or competitive performances in offline tasks.

## 1 Introduction

Imitation learning (IL) allows agents to learn from expert demonstrations. Initially developed with a supervised learning paradigm [58; 63], IL can be extended and reformulated with a general expert matching objective, which aims to generate policies that produce trajectories with low distributional distances to expert demonstrations [30]. This formulation allows IL to be extended to various new settings: 1) online IL where interactions with the environment are allowed, 2) learning from observations (LfO) where expert actions are absent, 3) offline IL where agents learn from limited expert data and a fixed dataset of sub-optimal and reward-free experience, 4) cross-domain IL where the expert demonstrations come from another domain (_i.e._, environment) that has different transition dynamics, and 5) one-shot IL which expects to recover the expert behaviors when only one expert trajectory is observed for a new IL task.

Modern IL algorithms introduce various designs or mathematical principles to cater to the expert matching objective in a specific scenario. For example, the LfO setting requires particular considerations regarding the absent expert actions, _e.g._, learning an inverse dynamics function [5; 65]. Besides, out-of-distribution issues in offline IL require specialized modifications to the learning objective, such as introducing additional policy/value regularization [32; 72]. However, such a methodology, designing an individual formulation for each IL setting, makes it difficult to scale up a specific IL algorithm to more complex tasks beyond its original IL setting, _e.g._, online IL methods often suffer severe performance degradation in offline IL settings. Furthermore, realistic IL tasks are often not subject to a particular IL setting but consist of a mixture of them. For example, we may have accessto both demonstrations and observation-only data in offline robot tasks; however, it could require significant effort to adapt several specialized methods to leverage such mixed/hybrid data. Hence, a problem naturally arises: _How can we accommodate various design requirements of different IL settings with a general and practically ready-to-deploy IL formulation?_

Hindsight information matching, a task-relabeling paradigm in reinforcement learning (RL), views control tasks as analogous to a general sequence modeling problem, with the goal to produce a sequence of actions that induces high returns [12]. Its generality and simplicity enable it to be extended to both online and offline settings [17; 42]. In its original RL context, an agent directly uses known extrinsic rewards to bias the hindsight information towards task-related behaviors. However, when we attempt to retain its generality in IL tasks, how to bias the hindsight towards expert behaviors remains a significant barrier as the extrinsic rewards are missing.

To design a general IL formulation and tackle the above problems, we propose **C**ont**E**xtual **I**mitation **L**earning (CEIL), which readily incorporates the hindsight information matching principle within a bi-level expert matching objective. In the inner-level optimization, we explicitly learn a hindsight embedding function to deal with the challenges of unknown rewards. In the outer-level optimization, we perform IL expert matching via inferring an optimal embedding (_i.e._, hindsight embedding biasing), replacing the naive reward biasing in hindsight. Intuitively, we find that such a bi-level objective results in a spectrum of expert matching objectives from the embedding space to the trajectory space. To shed light on the applicability and generality of CEIL, we instantiate CEIL to various IL settings, including online/offline IL, LfD/LfO, cross-domain IL, and one-shot IL settings.

In summary, this paper makes the following contributions: 1) We propose a bi-level expert matching objective ContExtual Imitation Learning (CEIL), inheriting the spirit of hindsight information matching, which decouples the learning policy into a contextual policy and an optimal embedding. 2) CEIL exhibits high generality and adaptability and can be instantiated over a range of IL tasks. 3) Empirically, we conduct extensive empirical analyses showing that CEIL is more sample-efficient in online IL and achieves better or competitive results in offline IL tasks.

## 2 Related Work

Recent advances in decision-making have led to rapid progress in IL settings (Table 1), from typical learning from demonstrations (LfD) to learning from observations (LfO) [7; 9; 35; 54; 62; 66], from online IL to offline IL [11; 15; 33; 53; 73], and from single-domain IL to cross-domain IL [34; 48; 56; 68]. Targeting a specific IL setting, individual works have shown their impressive

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Expert data} & \multicolumn{2}{c}{Task setting} & \multicolumn{2}{c}{Cross-domain} & \multicolumn{1}{c}{One-shot} \\ \cline{2-7}  & LfD & LfO & Online & Offline & & \\ \hline S-on-LfD [9; 13; 21; 30; 38; 52; 57; 61; 77] & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ S-on-LfO [7; 54; 65; 66; 75] & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ S-off-LfD [19; 32; 33; 39; 55; 70; 72; 73] & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ S-off-LfO [78] & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ C-on-LfD [18; 69; 79] & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ C-on-LfO [20; 25; 26; 48; 59; 60] & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ C-off-LfD [34] & ✓ & ✗ & ✗ & ✓ & ✗ & ✗ \\ C-off-LfO [56; 68] & ✓ & ✓ & ✗ & ✓ & ✗ & ✗ \\ \hline S-on/off-LfO [28] & ✓ & ✓ & ✓ & ✓ & ✗ & ✗ \\ Online one-shot [14; 16; 40] & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ Offline one-shot [24; 71] & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ \\ \hline
**CEIL (ours)** & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: A coarse summary of IL methods demonstrating 1) different expert data modalities they can handle (learning from _demonstrations_ or _observations_), 2) disparate task settings they consider (learning from _online_ environment interactions or pre-collected _offline_ static dataset), 3) the specific _cross-domain_ setting they assume (the transition dynamics between the learning environment and that of the expert behaviors are different), and 4) the unique _one-shot_ merit they desire (the learned policy is capable of one-shot transfer to new imitation tasks). We highlight that our contextual imitation learning (CEIL) method can naturally be applied to all the above IL settings.

ability to solve the exact IL setting. However, it is hard to retrain their performance in new unprepared IL settings. In light of this, it is tempting to consider how we can design a general and broadly applicable IL method. Indeed, a number of prior works have studied part of the above IL settings, such as offline LFO [78], cross-domain LFO [48; 60], and cross-domain offline IL [56]. While such works demonstrate the feasibility of tackling multiple IL settings, they still rely on standard online/offline RL algorithmic advances to improve performance [25; 32; 44; 47; 50; 51; 55; 72; 76]. Our objective diverges from these works, as we strive to minimize the reliance on the RL pipeline by replacing it with a simple supervision objective, thus avoiding the dependence on the choice of RL algorithms.

Our approach to IL is most closely related to prior hindsight information-matching methods [2; 8; 24; 49], both learning a contextual policy and using a contextual variable to guide policy improvement. However, these prior methods typically require additional mechanisms to work well, such as extrinsic rewards in online RL [4; 42; 64] or a handcrafted target return in offline RL [12; 17]. Our method does not require explicit handling of these components. By explicitly learning an embedding space for both expert and suboptimal behaviors, we can bias the contextual policy with an inferred optimal embedding (contextual variable), thus avoiding the need for explicit reward biasing in prior works. Our method also differs from most prior offline transformer-based RL/IL algorithms that explicitly model a long sequence of transitions [10; 12; 31; 36; 43; 71]. We find that simple fully-connected networks can also elicit useful embeddings and guide expert behaviors when conditioned on a well-calibrated embedding. In the context of the recently proposed prompt-tuning paradigm in large language tasks or multi-modal tasks [27; 45; 74], our method can be interpreted as a combination of IL and prompting-tuning, with the main motivation that we tune the prompt (the optimal contextual variable) with an expert matching objective in IL settings.

## 3 Background

Before discussing our method, we briefly introduce the background for IL, including learning from demonstrations (LfD), learning from observations (LfO), online IL, offline IL, and cross-domain settings in Section 3.1, and introduce the hindsight information matching in Section 3.2.

### Imitation Learning

Consider a control task formulated as a discrete-time Markov decision process (MDP)2\(\mathcal{M}=\{\mathcal{S},\mathcal{A},\mathcal{T},r,\gamma,p_{0}\}\), where \(\mathcal{S}\) is the state (observation) space, \(\mathcal{A}\) is the action space, \(\mathcal{T}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}\) is the transition dynamics function, \(r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the reward function, \(\gamma\) is the discount factor, and \(p_{0}\) is the distribution of initial states. The goal in a reinforcement learning (RL) control task is to learn a policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s})\) maximizing the expected sum of discounted rewards \(\mathbb{E}_{\pi_{\theta}(\boldsymbol{\tau})}\left[\sum_{t=0}^{T-1}\gamma^{t}r (\mathbf{s}_{t},\mathbf{a}_{t})\right]\), where \(\boldsymbol{\tau}:=\{\mathbf{s}_{0},\mathbf{a}_{0},\cdots,\mathbf{s}_{T-1}, \mathbf{a}_{T-1}\}\) denotes the trajectory and the generated trajectory distribution \(\pi_{\theta}(\boldsymbol{\tau})=p_{0}(\mathbf{s}_{0})\pi_{\theta}(\mathbf{a} _{0}|\mathbf{s}_{0})\prod_{t=1}^{T-1}\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{ t})\mathcal{T}(\mathbf{s}_{t}|\mathbf{s}_{t-1},\mathbf{a}_{t-1})\).

Footnote 2: In this paper, we use environment and MDP interchangeably, and use state and observation interchangeably.

In IL, the ground truth reward function (_i.e._, \(r\) in \(\mathcal{M}\)) is not observed. Instead, we have access to a set of demonstrations (or observations) \(\{\boldsymbol{\tau}|\boldsymbol{\tau}\sim\pi_{E}(\boldsymbol{\tau})\}\) that are collected by an unknown expert policy \(\pi_{E}(\mathbf{a}|\mathbf{s})\). The goal of IL tasks is to recover a policy that matches the corresponding expert policy. From the mathematical perspective, IL achieves the plain expert matching objective by minimizing the divergence of trajectory distributions between the learner and the expert:

\[\min_{\pi_{\theta}}\;D(\pi_{\theta}(\boldsymbol{\tau}),\pi_{E}(\boldsymbol{ \tau})),\] (1)

where \(D\) is a distance measure. Meanwhile, we emphasize that the given expert data \(\{\boldsymbol{\tau}|\boldsymbol{\tau}\sim\pi_{E}(\boldsymbol{\tau})\}\) may not contain the corresponding expert actions. Thus, in this work, we consider two IL cases where the given expert data \(\boldsymbol{\tau}\) consists of a set of state-action demonstrations \(\{(\mathbf{s}_{t},\mathbf{a}_{t},\mathbf{s}_{t+1})\}\) (learning from demonstrations, LfD), as well as a set of state-only transitions \(\{(\mathbf{s}_{t},\mathbf{s}_{t+1})\}\) (learning from observations, LfO) _When it is clear from context, we abuse notation \(\pi_{E}(\boldsymbol{\tau})\) to denote both demonstrations in LfD and observations in LfO for simplicity._

Besides, we can also divide IL settings into two orthogonal categories: online IL and offline IL. In online IL, the learning policy \(\pi_{\theta}\) can interact with the environment and generate online trajectories \(\boldsymbol{\tau}\sim\pi_{\theta}(\boldsymbol{\tau})\). In offline IL, the agent cannot interact with the environment but has access to an offlinestatic dataset \(\{\bm{\tau}|\bm{\tau}\sim\pi_{\beta}(\bm{\tau})\}\), collected by some unknown (sub-optimal) behavior policies \(\pi_{\beta}\). By leveraging the offline data \(\{\pi_{\beta}(\bm{\tau})\}\cup\{\pi_{E}(\bm{\tau})\}\) without any interactions with the environment, the goal of offline IL is to learn a policy recovering the expert behaviors (demonstrations or observations) generated by \(\pi_{E}\). Note that, in contrast to the typical offline RL problem [46], the offline data \(\{\pi_{\beta}(\bm{\tau})\}\) in offline IL does not contains any reward signal.

**Cross-domain IL.** Beyond the above two IL branches (online/offline and LfD/LfO), we can also divide IL into: 1) single-domain IL and 2) cross-domain IL, where 1) the single-domain IL assumes that the expert behaviors are collected in the same MDP in which the learning policy is to be learned, and 2) the cross-domain IL studies how to imitate expert behaviors when discrepancies exist between the expert and the learning MDPs (_e.g._, differing in their transition dynamics or morphologies).

### Hindsight Information Matching

In typical goal-conditioned RL problems, hindsight experience replay (HER) [3] proposes to leverage the rich repository of the failed experiences by replacing the desired (true) goals of training trajectories with the achieved goals of the failed experiences:

\[\mathtt{Alg}(\pi_{\theta};\bm{g},\bm{\tau}_{\bm{g}})\rightarrow\mathtt{Alg}( \pi_{\theta};f_{\text{HER}}(\bm{\tau}_{\bm{g}}),\bm{\tau}_{\bm{g}}),\]

where the learner \(\mathtt{Alg}(\pi_{\theta};\cdot,\cdot)\) could be any RL methods, \(\bm{\tau}_{\bm{g}}\sim\pi_{\theta}(\bm{\tau}_{\bm{g}}|\bm{g})\) denotes the trajectory generated by a goal-conditioned policy \(\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t},\bm{g})\), and \(f_{\text{HER}}\) denotes a pre-defined (hindsight information extraction) function, _e.g._, returning the last state in trajectory \(\bm{\tau}_{\bm{g}}\).

HER can also be applied to the (single-goal) reward-driven online/offline RL tasks, setting the return (sum of the discounted rewards) of a trajectory as an implicit goal for the corresponding trajectory. Thus, _we can reformulate the (single-goal) reward-driven RL task_, learning policy \(\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t})\) that maximize the return, _as a multi-goal RL task_, learning a return-conditioned policy \(\pi_{\theta}(\mathbf{a}_{t}|\mathbf{s}_{t},\cdot)\) that maximize the following log-likelihood:

\[\max_{\pi_{\theta}}\ \mathbb{E}_{\mathcal{D}(\bm{\tau})}\left[\log\pi_{ \theta}(\mathbf{a}|\mathbf{s},f_{\text{R}}(\bm{\tau}))\right],\] (2)

where \(f_{\text{R}}(\bm{\tau})\) denotes the return of trajectory \(\bm{\tau}\). At test, we can then condition the contextual policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\cdot)\) on a desired target return. In offline RL, the empirical distribution \(\mathcal{D}(\bm{\tau})\) in Equation 2 can be naturally set as the offline data distribution; in online RL, \(\mathcal{D}(\bm{\tau})\) can be set as the replay/experience buffer, and will be updated and biased towards trajectories that have high expected returns.

Intuitively, biasing the sampling distribution (\(\mathcal{D}(\bm{\tau})\) towards higher returns) leads to _an implicit policy improvement operation_. However, such an operator is non-trivial to obtain in the IL problem, where we do not have access to a pre-defined function \(f_{\text{R}}(\bm{\tau})\) to bias the learning policy towards recovering the given expert data \(\{\pi_{E}(\bm{\tau})\}\) (demonstrations or observations).

## 4 Method

In this section, we will formulate IL as a bi-level optimization problem, which will allow us to derive our method, contextual imitation learning (CEIL). Instead of attempting to train the learning policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s})\) with the plain expert matching objective (Equation 1), our approach introduces an additional contextual variable \(\mathbf{z}\) for a contextual IL policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\cdot)\). The main idea of CEIL is to learn a contextual policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z})\) and an optimal contextual variable \(\mathbf{z}^{*}\) such that the given expert data (demonstrations in LfD or observations in LfO) can be recovered by the learned \(\mathbf{z}^{*}\)-conditioned policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z}^{*})\). We begin by describing the overall framework of CEIL in Section 4.1, and make a connection between CEIL and the plain expert matching objective in Section 4.2, which leads to a practical implementation under various IL settings in Section 4.3.

### Contextual Imitation Learning (CEIL)

Motivated by the hindsight information matching in online/offline RL (Section 3.2), we propose to learn a general hindsight embedding function \(f_{\phi}\), which encodes trajectory \(\bm{\tau}\) (with window size \(T\)) into a latent variable \(\mathbf{z}\in\mathcal{Z}\), \(|\mathcal{Z}|\ll T*|\mathcal{S}|\). Formally, we learn the embedding function \(f_{\phi}\) and a corresponding contextual policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z})\) by minimizing the trajectory self-consistency loss:

\[\pi_{\theta},f_{\phi}=\min_{\pi_{\theta},f_{\phi}}\ -\mathbb{E}_{\mathcal{D}(\bm{ \tau})}\left[\log\pi_{\theta}(\bm{\tau}|f_{\phi}(\bm{\tau}))\right]=\min_{\pi_{ \theta},f_{\phi}}\ -\mathbb{E}_{\bm{\tau}\sim\mathcal{D}(\bm{\tau})}\mathbb{E}_{(\mathbf{s}, \mathbf{a})\sim\bm{\tau}}\left[\log\pi_{\theta}(\mathbf{a}|\mathbf{s},f_{\phi}( \bm{\tau}))\right],\] (3)where in the online setting, we sample trajectory \(\bm{\tau}\) from buffer \(\mathcal{D}(\bm{\tau})\), known as the experience replay buffer in online RL; in the offline setting, we sample trajectory \(\bm{\tau}\) directly from the given offline data.

If we can ensure that the learned contextual policy \(\pi_{\theta}\) and the embedding function \(f_{\phi}\) are accurate on the empirical data \(\mathcal{D}(\bm{\tau})\), then we can convert the IL policy optimization objective (in Equation 1) into a bi-level expert matching objective:

\[\min_{\bm{\mathrm{z}}^{*}}\;D(\pi_{\theta}(\bm{\tau}|\bm{\mathrm{z }}^{*}),\pi_{E}(\bm{\tau})),\] (4) \[\text{s.t. }\pi_{\theta},f_{\phi}=\min_{\pi_{\theta},f_{\phi}}- \mathbb{E}_{\mathcal{D}(\bm{\tau})}\left[\log\pi_{\theta}(\bm{\tau}|f_{\phi}( \bm{\tau}))\right]-\mathcal{R}(f_{\phi}),\;\;\text{and}\;\;\bm{\mathrm{z}}^{*} \in f_{\phi}\circ\text{supp}(\mathcal{D}),\] (5)

where \(\mathcal{R}(f_{\phi})\) is an added regularization over the embedding function (we will elaborate on it later), and \(\text{supp}(\mathcal{D})\) denotes the support of the trajectory distribution \(\{\bm{\tau}|\mathcal{D}(\bm{\tau})>0\}\). Here \(f_{\phi}\) is employed to map the trajectory space to the latent variable space (\(\mathcal{Z}\)). Intuitively, by optimizing Equation 4, we expect the induced trajectory distribution of the learned \(\pi_{\theta}(\bm{\mathrm{a}}|\bm{\mathrm{s}},\bm{\mathrm{z}}^{*})\) will match that of the expert. However, in the offline IL setting, the contextual policy can not interact with the environment. If we directly optimize the expert matching objective (Equation 4), such an objective can easily exploit generalization errors in the contextual policy model to infer a mistakenly overestimated \(\bm{\mathrm{z}}^{*}\) that achieves low expert-matching loss but does not preserve the trajectory self-consistency (Equation 3). Therefore, we formalize CEIL into a bi-level optimization problem, where, in Equation 5, we explicitly constrain the inferred \(\bm{\mathrm{z}}^{*}\) lies in the (\(f_{\phi}\)-mapped) support of the training trajectory distribution.

At a high level, CEIL decouples the learning policy into two parts: an expressive contextual policy \(\pi_{\theta}(\bm{\mathrm{a}}|\bm{\mathrm{s}},\cdot)\) and an optimal contextual variable \(\bm{\mathrm{z}}^{*}\). By comparing CEIL with the plain expert matching objective, \(\min_{\pi_{\theta}}D(\pi_{\theta}(\bm{\tau}),\pi_{E}(\bm{\tau}))\), in Equation 1, we highlight two merits: 1) CEIL's expert matching loss (Equation 4) does not account for updating \(\pi_{\theta}\) and is only incentivized to update the low-dimensional latent variable \(\bm{\mathrm{z}}^{*}\), which enjoys efficient parameter learning similar to the prompt tuning in large language models [74], and 2) we learn \(\pi_{\theta}\) by simply performing supervised regression (Equation 5), which is more stable compared to vanilla inverse-RL/adversarial-IL methods.

### Connection to the Plain Expert Matching Objective

To gain more insight into Equation 4 that captures the quality of IL (the degree of similarity to the expert data), we define \(D(\cdot,\cdot)\) as the sum of reverse KL and forward KL divergence3, _i.e._, \(D(q,p)=D_{\text{KL}}(q\|p)+D_{\text{KL}}(p\|q)\), and derive an alternative form for Equation 4:

Footnote 3: \(D_{\text{KL}}(p\|q):=\mathbb{E}_{p(\bm{\mathrm{x}})}\left[\log\frac{p(\bm{ \mathrm{x}})}{q(\bm{\mathrm{x}})}\right]\) denotes the (forward) KL divergences. It is well known that reverse KL ensures that the learned distribution is mode-seeking and forward KL exhibits a mode-covering behavior [37]. For analysis purposes, here we define \(D(\cdot,\cdot)\) as the sum of reverse KL and forward KL, and set the weights of both reverse KL and forward KL to 1.

\[\arg\min_{\bm{\mathrm{z}}^{*}}\;D(\pi_{\theta}(\bm{\tau}|\bm{\mathrm{z}}^{*}), \pi_{E}(\bm{\tau}))=\arg\max_{\bm{\mathrm{z}}^{*}}\;\underbrace{\mathcal{I}( \bm{\mathrm{z}}^{*};\bm{\mathrm{\tau}}_{E})-\mathcal{I}(\bm{\mathrm{z}}^{*}; \bm{\mathrm{\tau}}_{\theta})}_{\mathcal{J}_{\text{nn}}}-\underbrace{D(\pi_{ \theta}(\bm{\tau}),\pi_{E}(\bm{\tau}))}_{\mathcal{J}_{\text{D}}},\] (6)

where \(\mathcal{I}(\bm{\mathrm{x}};\bm{\mathrm{y}})\) denotes the mutual information (MI) between \(\bm{\mathrm{x}}\) and \(\bm{\mathrm{y}}\), which measures the predictive power of \(\bm{\mathrm{y}}\) on \(\bm{\mathrm{x}}\) (or vice-versa), the latent variables are defined as \(\bm{\mathrm{\tau}}_{E}:=\bm{\mathrm{\tau}}\sim\pi_{E}(\bm{\mathrm{\tau}})\), \(\bm{\mathrm{\tau}}_{\theta}:=\bm{\mathrm{\tau}}\sim p(\bm{\mathrm{z}}^{*})\pi_ {\theta}(\bm{\mathrm{\tau}}|\bm{\mathrm{z}}^{*})\), and \(\pi_{\theta}(\bm{\mathrm{\tau}})=\mathbb{E}_{\bm{\mathrm{z}}^{*}}\left[\pi_{ \theta}(\bm{\mathrm{\tau}}|\bm{\mathrm{z}}^{*})\right]\).

Intuitively, the second term \(\mathcal{J}_{D}\) on RHS of Equation 6 is similar to the plain expert matching objective in Equation 1, except that here we optimize a latent variable \(\bm{\mathrm{z}}^{*}\) over this objective. Regarding the MI terms \(\mathcal{J}_{\text{MI}}\), we can interpret the maximization over \(\mathcal{J}_{\text{MI}}\) as an implicit policy improvement, which incentivizes the optimal latent variable \(\bm{\mathrm{z}}^{*}\) for having high predictive power of the expert data \(\bm{\mathrm{\tau}}_{E}\) and having low predictive power of the non-expert data \(\bm{\mathrm{\tau}}_{\theta}\).

Further, we can rewrite the MI term (\(\mathcal{J}_{\text{MI}}\) in Equation 6) in terms of the learned embedding function \(f_{\phi}\), yielding an approximate embedding inference objective \(\mathcal{J}_{\text{MI}(f_{\phi})}\):

\[\mathcal{J}_{\text{MI}} =\mathbb{E}_{\pi_{E}(\bm{\mathrm{z}}^{*},\bm{\mathrm{\tau}}_{E})} \log p(\bm{\mathrm{z}}^{*}|\bm{\mathrm{\tau}}_{E})-\mathbb{E}_{\pi_{\theta}( \bm{\mathrm{z}}^{*},\bm{\mathrm{\tau}}_{\theta})}\log p(\bm{\mathrm{z}}^{*}| \bm{\mathrm{\tau}}_{\theta})\] \[\approx\mathbb{E}_{p(\bm{\mathrm{z}}^{*})\pi_{E}(\bm{\mathrm{\tau} }_{E})\pi_{\theta}(\bm{\mathrm{\tau}}_{\theta}|\bm{\mathrm{z}}^{*})}\left[-\| \bm{\mathrm{z}}^{*}-f_{\phi}(\bm{\mathrm{\tau}}_{E})\|^{2}+\|\bm{\mathrm{z}}^{*} -f_{\phi}(\bm{\mathrm{\tau}}_{\theta})\|^{2}\right]\triangleq\mathcal{J}_{\text{ MI}(f_{\phi})},\]

where we approximate the logarithmic predictive power of \(\bm{\mathrm{z}}^{*}\) on \(\bm{\mathrm{\tau}}\) with \(-\|\bm{\mathrm{z}}^{*}-f_{\phi}(\bm{\mathrm{\tau}})\|^{2}\), by taking advantage of the learned embedding function \(f_{\phi}\) in Equation 5.

By maximizing \(\mathcal{J}_{\text{MI}(f_{\phi})}\), the learned optimal \(\mathbf{z}^{*}\) will be induced to converge towards the embeddings of expert data and avoid trivial solutions (as shown in Figure 1). Intuitively, \(\mathcal{J}_{\text{MI}(f_{\phi})}\) can also be thought of as an instantiation of contrastive loss, which manifests two facets we consider significant in IL: 1) the "anchor" variable4\(\mathbf{z}^{*}\) is unknown and must be estimated, and 2) it is necessary to ensure that the estimated \(\mathbf{z}^{*}\) lies in the support set of training distribution, as specified by the support constraints in Equation 5.

Footnote 4: The triplet contrastive loss enforces the distance between the anchor and the positive to be smaller than that between the anchor and the negative. Thus, we can view \(\mathbf{z}^{*}\) in \(\mathcal{J}_{\text{MI}(f_{\phi})}\) as an instance of the anchor.

In summary, by comparing \(\mathcal{J}_{\text{MI}(f_{\phi})}\) and \(\mathcal{J}_{D}\), we can observe that \(\mathcal{J}_{\text{MI}(f_{\phi})}\) actually encourages expert matching in the embedding space, while \(\mathcal{J}_{D}\) encourages expert matching in the original trajectory space. In the next section, we will see that such an embedding-level expert matching objective naturally lends itself to cross-domain IL settings.

### Practical Implementation

In this section, we describe how we can convert the bi-level IL problem above (Equations 4 and 5) into a feasible online/offline IL objective and discuss some practical implementation details in LfO, offline IL, cross-domain IL, and one-shot IL settings (see more details5 in Appendix 9.3).

Footnote 5: Our code will be released at https://github.com/wechto/GeneralizedCEIL.

As shown in Algorithm 1 (best viewed in colors), CEIL alternates between solving the bi-level problem with respect to the support constraint (Line 3 for online IL or Line 7 for offline IL), the trajectory self-consistency loss (Line 5), and the optimal embedding inference (Line 6).

To satisfy the support constraint in Equation 5, for online IL (Line 3), we directly roll out the \(z^{*}\)-conditioned policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z}^{*})\) in the environment; for offline IL (Line 7), we minimize a simple regularization6 over \(\mathbf{z}^{*}\), bearing a close resemblance to the one used in TD3+BC [23]:

Footnote 6: In other words, the offline support constraint in Equation 5 is achieved through minimizing \(\mathcal{R}(\mathbf{z}^{*})\).

\[\mathcal{R}(\mathbf{z}^{*})=\min\left(\|\mathbf{z}^{*}-f_{\bar{\phi}}(\bm{ \tau}_{E})\|^{2},\|\mathbf{z}^{*}-f_{\bar{\phi}}(\bm{\tau}_{D})\|^{2}\right), \ \ \bm{\tau}_{E}:=\bm{\tau}\sim\pi_{E}(\bm{\tau}),\ \bm{\tau}_{D}:=\bm{\tau}\sim \mathcal{D}(\bm{\tau}),\] (7)

where we apply a stop-gradient operation to \(f_{\bar{\phi}}\). To ensure the optimal embedding inference (\(\max_{\mathbf{z}^{*}}\mathcal{J}_{\text{MI}(f_{\phi})}-\mathcal{J}_{D}\)) retaining the flexibility of seeking \(\mathbf{z}^{*}\) across different instances of \(f_{\phi}\), we jointly update the optimal embedding \(\mathbf{z}^{*}\) and the embedding function \(f_{\phi}\) with

\[\max_{\mathbf{z}^{*},f_{\phi}}\mathcal{J}_{\text{MI}(f_{\phi})}-\alpha\mathcal{ J}_{D},\] (8)

where we use \(\alpha\) to control the weight on \(\mathcal{J}_{D}\).

**LfO.** In the LfO setting, as expert actions are missing, we apply our expert matching objective only over the observations. Note that even though expert data contains no actions in LfO, we can still leverage a large number of suboptimal actions presented in online/offline \(\mathcal{D}(\bm{\tau})\). Thus, we can learn the contextual policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z})\) using the buffer data in online IL or the offline data in offline IL, much owing to the fact that we do not directly use the plain expert matching objective to update \(\pi_{\theta}\).

**Cross-domain IL.** Cross-domain IL considers the case in which the expert's and learning agent's MDPs are different. Due to the domain shift, the plain idea of \(\min\mathcal{J}_{D}\) may not be a sufficient proxy for the expert matching objective, as there may never exist a trajectory (in the learning MDP) that matches the given expert data. Thus, we can set (the weight of \(\mathcal{J}_{D}\)) \(\alpha\) to \(0\).

Further, to make embedding function \(f_{\phi}\) useful for guiding the expert matching in latent space (_i.e._, \(\max\mathcal{J}_{\text{MI}(f_{\phi})}\)), we encourage \(f_{\phi}\) to capture the task-relevant embeddings and ignore the domain-specific factors. To do so, we generate a set of pseudo-random transitions \(\{\bm{\tau}_{E^{\prime}}\}\) by independently sampling trajectories from expert data \(\{\pi_{E}(\bm{\tau}_{E})\}\) and adding random noise over these sampled trajectories, _i.e._, \(\bm{\tau}_{E^{\prime}}=\bm{\tau}_{E}+\text{noise}\). Then, we couple each trajectory \(\bm{\tau}\) in \(\{\bm{\tau}_{E}\}\cup\{\bm{\tau}_{E^{\prime}}\}\) with a label \(\mathbf{n}\in\{\bm{0},\bm{1}\}\), indicating whether it is noised, and then generate a new set of \(\{(\bm{\tau},\mathbf{n})\}\), where \(\bm{\tau}\in\{\bm{\tau}_{E}\}\cup\{\bm{\tau}_{E^{\prime}}\}\) and \(\mathbf{n}\in\{\bm{0},\bm{1}\}\). Thus, we can set the regularization \(\mathcal{R}(f_{\phi})\) in Equation 5 to be:

\[\mathcal{R}(f_{\phi})=\mathcal{I}(f_{\phi}(\bm{\tau});\mathbf{n}).\] (9)

Intuitively, maximizing \(\mathcal{R}(f_{\phi})\) encourages embeddings to be domain-agnostic and task-relevant: \(f_{\phi}(\bm{\tau}_{E})\) has high predictive power over expert data (\(\mathbf{n}=0\)) and low that over noised data (\(\mathbf{n}=\bm{1}\)).

**One-shot IL.** Benefiting from the separate design of the contextual policy learning and the optimal embedding inference, CEIL also enjoys another advantage -- one-shot generalization to new IL tasks. For new IL tasks, given the corresponding expert data \(\bm{\tau}_{\text{new}}\), we can use the learned embedding function \(f_{\phi}\) to generate a corresponding latent embedding \(\mathbf{z}_{\text{new}}\). When conditioning on such an embedding, we can directly roll out \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z}_{\text{new}})\) to recover the one-shot expert behavior.

## 5 Experiments

In this section, we conduct experiments across a variety of IL problem domains: single/cross-domain IL, online/offline IL, and LfD/LfO IL settings. By arranging and combining these IL domains, we obtain 8 IL tasks in all: _S-on-LfD_, _S-on-Lf0_, _S-off-Lf0_, _S-off-Lf0_, _C-on-LfD_, _C-on-Lf0_, _C-off-Lf0_, and _C-off-Lf0_, where S/C denotes single/cross-domain IL, on/off denotes online/offline IL, and LfD/LfO denote learning from demonstrations/observations respectively. Moreover, we also verify the scalability of CEIL on the challenging one-shot IL setting.

Our experiments are conducted in four popular MuJoCo environments: Hopper-v2 (Hop.), HalfCheetah-v2 (Hal.), Walker2d-v2 (Wal.), and Ant-v2 (Ant.). In the single-domain IL setting, we train a SAC policy in each environment and use the learned expert policy to collect expert trajectories (demonstrations/observations). To investigate the cross-domain IL setting, we assume the two domains (learning MDP and the expert-data collecting MDP) have the same state space and action space, while they have different transition dynamics. To achieve this, we modify the torso length of the MuJoCo agents (see details in Appendix 9.2). Then, for each modified agent, we train a separate expert policy and collect expert trajectories. For the offline IL setting, we directly take the reward-free D4RL [22] as the offline dataset, replacing the online rollout experience in the online IL setting.

### Evaluation Results

To demonstrate the versatility of the CEIL idea, we collect 20 expert trajectories (demonstrations in LfD or state-only observations in LfO) for each environment and compare CEIL to GAIL [30], AIRL [21], SQIL [61], IQ-Learn [28], ValueDICE [41], GAIfO [66], ORIL [78], DemoDICE [39], and SMODICE [56] (see their implementation details in Appendix 9.4). Note that these baseline methods cannot be applied to all the IL task settings (_S/C-on/off-LfD/LfO_), thus we only provide experimental comparisons with compatible baselines in each IL setting.

**Online IL.** In Figure 2, we provide the return (cumulative rewards) curves of our method and baselines on 4 online IL settings: _S-on-LfD_ (_top-left_), _S-on-LfO_ (_top-right_), _C-on-LfD_ (_bottom-left_), and _C-on-LfO_ (_bottom-right_) settings. As can be seen, CEIL quickly achieves expert-level performance in _S-on-LfD_. When extended to _S-on-LfO_, CEIL also yields better sample efficiency compared to baselines. Further, considering the complex cross-domain setting, we can see those baselines SQILand IQ-Learn (in _C-on-LfD_ and _C-on-LfO_) suffer from the domain mismatch, leading to performance degradation at late stages of training, while CEIL can still achieve robust performance.

**Offline IL.** Next, we evaluate CEIL on the other 4 offline IL settings: _S-off-LfD_, _S-off-LfO_, _C-off-LfD_, and _C-off-LfO_. In Table 2, we provide the normalized return of our method and baseline methods on reward-free D4RL [22] medium (m), medium-replay (mr), and medium-expert (me) datasets. We can

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline \multicolumn{2}{c}{\multirow{2}{*}{Offline IL settings}} & \multicolumn{3}{c}{Hopper-v2} & \multicolumn{3}{c}{Halfcheetah-v2} & \multicolumn{3}{c}{Walker2d-v2} & \multicolumn{3}{c}{Ant-v2} & \multirow{2}{*}{sum} \\ \cline{3-3} \cline{5-16}  & m & m & m & m & m & m & m & m & m & m & m & m & m \\ \hline \multirow{8}{*}{**C-on-LfO**} & OMIL (TD3+BC) & 50.9 & 22.1 & 72.7 & 44.7 & 30.2 & 87.5 & 47.1 & 26.7 & 102.6 & 46.5 & 31.4 & 61.9 & 624.3 \\  & SQIL (TD3+BC) & 32.6 & 60.6 & 25.5 & 13.2 & 25.3 & 14.4 & 25.6 & 15.6 & 8.0 & 63.6 & 58.4 & 44.3 & 387.1 \\  & IQ-Learn & 21.3 & 19.9 & 24.9 & 5.0 & 7.5 & 7.5 & 22.3 & 19.6 & 18.5 & 38.4 & 24.3 & 55.3 & 264.5 \\  & ValueDICE & 73.8 & 83.6 & 50.8 & 1.9 & 2.4 & 32.2 & 24.6 & 26.4 & 44.1 & 79.1 & 82.4 & 75.2 & 547.5 \\  & DemoDICE & 54.8 & 32.7 & 65.4 & 42.8 & 37.0 & 55.6 & 68.1 & 39.7 & 95.0 & 85.6 & 69.0 & 108.8 & 754.6 \\  & SMODICE & 56.1 & 28.7 & 68.0 & 42.7 & 37.7 & 66.9 & 66.2 & 40.7 & 58.2 & 87.4 & 69.9 & 113.4 & 735.9 \\  & **CEIL (ours)** & 110.4 & 103.0 & 106.8 & 40.0 & 30.3 & 63.9 & 118.6 & 110.8 & 117.0 & 126.3 & 122.0 & 114.3 & **1163.5** \\ \hline \multirow{8}{*}{**C-on-LfO**} & OMIL (TD3+BC) & 43.4 & 25.7 & 73.0 & 44.9 & 2.4 & 81.8 & 58.9 & 16.8 & 78.2 & 33.7 & 29.6 & 67.1 & 555.4 \\  & SMODICE & **54.5** & 26.4 & 73.7 & 42.7 & 37.9 & 66.2 & 60.6 & 38.5 & 70.9 & 85.7 & 68.3 & 116.3 & 741.7 \\  & **CEIL (ours)** & 54.2 & **51.4** & **90.4** & 43.5 & 40.1 & 47.7 & 78.5 & 20.5 & 110.0 & 97.0 & 67.8 & 120.5 & **821.7** \\ \hline \multirow{8}{*}{**C-on-LfO**} & OMIL (TD3+BC) & 52.8 & 27.6 & 46.5 & 38.3 & 8.0 & **74.0** & 25.3 & 28.4 & 26.3 & 26.0 & 17.6 & 11.9 & 382.6 \\  & SQL (TD3+BC) & 34.4 & 19.1 & 11.4 & 19.2 & 25.1 & 19.9 & 15.8 & 16.5 & 8.8 & 21.8 & 23.2 & 21.2 & 236.2 \\  & IQ-Learn & 37.3 & 35.4 & 25.9 & 27.4 & 27.1 & 31.2 & 27.7 & 22.2 & 31.7 & 63.7 & 63.3 & 55.8 & 448.8 \\  & ValueDICE & 22.0 & 18.3 & 18.9 & 14.0 & 11.7 & 8.7 & 11.5 & 10.0 & 8.6 & 24.1 & 21.4 & 19.2 & 188.4 \\  & DemoDICE & 52.9 & 15.2 & 77.2 & 42.8 & 38.9 & 53.8 & 58.4 & 26.4 & 77.8 & 87.8 & 69.3 & 114.9 & 715.6 \\  & SMODICE & 55.4 & 21.4 & 71.2 & 42.7 & 38.0 & 64.6 & 86.4 & 34.2 & 80.4 & 87.4 & 70.4 & 115.7 & 749.7 \\  & **CEIL (ours)** & 58.4 & 39.8 & 81.6 & 42.6 & 38.3 & 46.6 & 76.5 & 21.1 & 81.1 & 91.6 & 88.0 & 115.3 & **780.9** \\ \hline \multirow{8}{*}{**C-on-LfO**} & OMIL (TD3+BC) & 55.5 & 18.2 & 55.5 & 40.6 & 2.9 & **73.0** & 26.9 & 19.4 & 22.7 & 11.2 & 21.3 & 10.8 & 358.0 \\  & SMODICE & 53.7 & 18.3 & 64.2 & 42.6 & 38.0 & 63.0 & 68.9 & **37.5** & 60.7 & 87.5 & **75.1** & 115.0 & **724.4** \\ \cline{1-1}  & **CEIL (ours)** & 44.7 & **44.2** & 48.2 & 42.4 & 36.5 & 46.9 & **76.2** & 31.7 & **77.0** & **95.0** & 71.0 & 112.7 & **727.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Normalized scores (averaged over 30 trails for each task) on 4 offline IL settings: _S-off-LfD_, _S-off-LfO_, _C-off-LfO_, and _C-off-LfO_. Scores within two points of the maximum score are highlighted

Figure 3: Ablating (**a**, **b**) the number of expert demonstrations and (**c**, **d**) the trajectory window size.

Figure 2: Return curves on 4 online IL settings: (**a**) _S-on-LfD_, (**b**) _S-on-LfO_, (**c**) _C-on-LfD_, and (**d**) _C-on-LfO_, where the shaded area represents a 95% confidence interval over 30 trails. Note that baselines cannot be applied to all the IL task settings, thus we only provide comparisons with compatible baselines (two separate legends).

observe that CEIL achieves a significant improvement over the baseline methods in both _S-off-LfD_ and _S-off-LfD_ settings. Compared to the state-of-the-art offline baselines, CEIL also shows competitive results on the challenging cross-domain offline IL settings (_C-off-LfD_ and _C-off-LfO_).

**One-shot IL.** Then, we explore CEIL on the one-shot IL tasks, where we expect CEIL can adapt its behavior to new IL tasks given only one trajectory for each task (mismatched MDP, see Appendix 9.2).

We first pre-train an embedding function and a contextual policy in the training domain (online/offline IL), then infer a new contextual variable and evaluate it on the new task. To facilitate comparison to baselines, we similarly pre-train a policy network (using baselines) and run BC on top of the pre-trained policy by using the provided demonstration. Consequently, such a baseline+BC procedure cannot be applied to the (one-shot) LfO tasks. The results in Table 3 show that baseline+BC struggles to transfer their expertise to new tasks. Benefiting from the hindsight framework, CEIL shows better one-shot transfer learning performance on 7 out of 8 one-shot LfD tasks and retains higher scalability and generality for both one-shot LfD and LfO IL tasks.

### Analysis of CEIL

**Hybrid IL settings.** In real-world, many IL tasks do not correspond to one specific IL setting, and instead consist of a hybrid of several IL settings, each of which passes a portion of task-relevant information to the IL agent. For example, we can provide the agent with both demonstrations and state-only observations and, in some cases, cross-domain demonstrations (S-LfD+S-LfO+C-LfD).

To examine the versatility of CEIL, we collect a separate expert trajectory for each of the four offline IL settings, and study CEIL's performance under hybrid IL settings. As shown in Table 4, we can see that by adding new expert behaviors on top of LfD, even when carrying relatively less supervision (_e.g._, actions are absent in LfO), CEIL can still improve the performance.

**Varying the number of demonstrations.** In Figure 3 (a, b), we study the effect of the number of expert demonstrations on CEIL's performance. Empirically, we reduce the number of training demonstrations from 20 to 1, and report the normalized returns at 1M training steps. We can observe that across both online and offline (D4RL *-medium) IL settings, CEIL shows more robust performance with respect to different numbers of demonstrations compared to baseline methods.

**Varying the window size of trajectory.** Next we assess the effect of the trajectory window size (_i.e._, the length of trajectory \(\bm{\tau}\) used for the embedding function \(f_{\phi}\) in Equation 3). In Figure 3 (b, c), we ablate the number of the window size in 4 LfD IL instantiations. We can see that across a range of window sizes, CEIL remains stable and achieves expert-level performance.

\begin{table}
\begin{tabular}{l l c c c} \hline \hline
**Hybrid offline IL settings** & Hop. & Hal. & Wal. & Ant. \\ \hline S-LfD & 29.4 & 69.9 & 42.8 & 84.9 \\ S-LfD + S-LfO & 30.4 & 68.6 & 42.3 & **91.6** \\ S-LfD + S-LfO + C-LfD & 30.7 & 71.7 & 42.9 & 89.2 \\ S-LfD + S-LfO + C-LfD & 58.6 & 79.6 & 43.7 & 98.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: The normalized results of CEIL, showing that CEIL can consistently digest useful (task-relevant) information and boost its performance, even under a hybrid of offline IL settings.

Figure 4: **Ablation studies on the optimization of \(f_{\phi}\) (_ablating \(f_{\phi}\)_) and the objective of \(\mathcal{J}_{\text{MH}}\) (_ablating \(\mathcal{J}_{\text{MH}}\)_), where the shaded area represents 95% CIs over 5 trails. See ablation results for offline IL tasks in Table 5.**

**Ablation studies on the optimization of \(f_{\phi}\) and the objective of \(\mathcal{J}_{\text{MI}}\).** In Figure 4 and Table 5, we carried out ablation experiments on the loss of \(f_{\phi}\) and \(\mathcal{J}_{\text{MI}}\) in both online IL and offline IL settings. We can see that ablating the \(f_{\phi}\) loss (optimizing with Equation 5) does degrade the performance in both online and offline IL tasks, demonstrating the effectiveness of optimizing with Equation 8. Intuitively, Equation 8 encourages the embedding function to be task-relevant, and thus we use the expert matching loss to update \(f_{\phi}\). We can also see that ablating \(\mathcal{J}_{\text{MI}}\) does lead to degraded performance, further verifying the effectiveness of our expert matching objective in the latent space.

## 6 Conclusion

In this paper, we present CEIL, a novel and general Imitation Learning framework applicable to a wide range of IL settings, including _CS-on/off-LfD/Lf0_ and few-shot IL settings. This is achieved by explicitly decoupling the imitation policy into 1) a contextual policy, learned with the self-supervised hindsight information matching objective, and 2) a latent variable, inferred by performing the IL expert matching objective. Compared to prior baselines, our results show that CEIL is more sample-efficient in most of the online IL tasks and achieves better or competitive performances in offline tasks.

**Limitations and future work.** Our primary aim behind this work is to develop a simple and scalable IL method. We believe that CEIL makes an important step in that direction. Admittedly, we also find some limitations of CEIL: 1) Offline results generally outperform online results, especially in the LfO setting. The main reason is that CEIL lacks explicit exploration bounds, thus future work could explore the exploration ability of online CEIL. 2) The trajectory self-consistency cannot be applied to cross-embodiment agents once the two embodiments/domains have different state spaces or action spaces. Considering such a cross-embodiment setting, a typical approach is to serialize state/action from different modalities into a flat sequence of tokens. We also remark that CEIL is compatible with such a tokenization approach, and thus suitable for IL tasks with different action/state spaces. Thus, we encourage the future exploration of generalized IL methods across different embodiments.

## Acknowledgments and Disclosure of Funding

We sincerely thank the anonymous reviewers for their insightful suggestions. This work was supported by the National Science and Technology Innovation 2030 - Major Project (Grant No. 2022ZD0208800), and NSFC General Program (Grant No. 62176215).

## References

* Agarwal et al. [2021] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_, 34:29304-29320, 2021.
* Ajay et al. [2022] Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? _arXiv preprint arXiv:2211.15657_, 2022.
* Andrychowicz et al. [2017] Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. _Advances in neural information processing systems_, 30, 2017.

\begin{table}
\begin{tabular}{c l c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Hopper-v2} & \multicolumn{3}{c}{HalfCheetah-v2} & \multicolumn{3}{c}{Walker2d-v2} & \multicolumn{3}{c}{Ant-v2} & \multirow{2}{*}{sum} \\ \cline{3-14} \multicolumn{1}{c}{\multirow{-2}{*}{Offline IL settings}} & \multicolumn{2}{c}{m} & \multicolumn{1}{c}{mr} & \multicolumn{1}{c}{me} & \multicolumn{1}{c}{m} & \multicolumn{1}{c}{mr} & \multicolumn{1}{c}{me} & \multicolumn{1}{c}{m} & \multicolumn{1}{c}{mr} & \multicolumn{1}{c}{me} & \multicolumn{1}{c}{m} & \multicolumn{1}{c}{mr} & \multicolumn{1}{c}{me} \\ \hline \multirow{4}{*}{**CELEL (_abating_\(f_{\phi}\))**} & CELEL (_abating_\(f_{\phi}\)) & 97.9 & 92.5 & 99.3 & 41.3 & 30.3 & 66.7 & 103.6 & 88.1 & 114.4 & 97.6 & 98.4 & 100.7 & 1030.8 \\  & CEIL (_abating_\(f_{\text{MI}}\)) & 83.2 & 89.0 & 98.7 & 27.1 & 28.3 & 53.5 & 107.4 & 68.0 & 75.6 & 116.9 & 97.8 & 105.9 & 951.4 \\  & CEIL & 110.4 & 103.0 & 106.8 & 40.0 & 30.3 & 63.9 & 118.6 & 110.8 & 117.0 & 126.3 & 122.0 & 114.3 & **1163.5** \\ \hline \multirow{4}{*}{**CELEL (_abating_\(f_{\phi}\))**} & CELEL (_abating_\(f_{\phi}\)) & 51.5 & 41.1 & 83.3 & 43.8 & 40.1 & 63.7 & 76.3 & 20.3 & 103.0 & 78.0 & 52.5 & 105.5 & 759.2 \\  & CEIL (_abating_\(f_{\text{MI}}\)) & 54.3 & 44.9 & 84.7 & 42.2 & 39.9 & 51.6 & 77.4 & 22.7 & 94.0 & 92.1 & 67.9 & 118.4 & 792.0 \\ \cline{1-1}  & CEIL & 54.2 & 51.4 & 90.4 & 43.5 & 40.1 & 47.7 & 78.5 & 20.5 & 110.0 & 97.0 & 67.8 & 120.5 & **821.7** \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Ablation studies on the optimization of \(f_{\phi}\) (_abating \(f_{\phi}\)_) and the objective of \(\mathcal{J}_{\text{MI}}\) (_abating \(\mathcal{J}_{\text{MI}}\)_), where scores (averaged over 5 trails for each task) within two points of the maximum score are highlighted.*** [4] Kai Arulkumaran, Dylan R Ashley, Jurgen Schmidhuber, and Rupesh K Srivastava. All you need is supervised learning: From imitation learning to meta-rl with upside down rl. _arXiv preprint arXiv:2202.11960_, 2022.
* [5] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. _Advances in Neural Information Processing Systems_, 35:24639-24654, 2022.
* [6] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. _arXiv preprint arXiv:1801.04062_, 2018.
* [7] Damian Boborzi, Christoph-Nikolas Straehle, Jens S Buchner, and Lars Mikelsons. Imitation learning by state-only distribution matching. _arXiv preprint arXiv:2202.04332_, 2022.
* [8] David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan Bruna. When does return-conditioned supervised learning work for offline reinforcement learning? _arXiv preprint arXiv:2206.01079_, 2022.
* [9] Daniel S Brown, Wonjoon Goo, and Scott Niekum. Better-than-demonstrator imitation learning via automatically-ranked demonstrations. In _Conference on robot learning_, pages 330-359. PMLR, 2020.
* [10] Micah Carroll, Orr Paradise, Jessy Lin, Raluca Georgescu, Mingfei Sun, David Bignell, Stephanie Milani, Katja Hofmann, Matthew Hausknecht, Anca Dragan, et al. Unimask: Unified inference in sequential decision problems. _arXiv preprint arXiv:2211.10869_, 2022.
* [11] Jonathan Chang, Masatoshi Uehara, Dhruv Sreenivas, Rahul Kidambi, and Wen Sun. Mitigating covariate shift in imitation learning via offline data with partial coverage. _Advances in Neural Information Processing Systems_, 34:965-979, 2021.
* [12] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [13] Robert Dadashi, Leonard Hussenot, Matthieu Geist, and Olivier Pietquin. Primal wasserstein imitation learning. _arXiv preprint arXiv:2006.04678_, 2020.
* [14] Christopher R Dance, Julien Perez, and Theo Cachet. Conditioned reinforcement learning for few-shot imitation. In _International Conference on Machine Learning_, pages 2376-2387. PMLR, 2021.
* [15] Branton DeMoss, Paul Duckworth, Nick Hawes, and Ingmar Posner. Ditto: Offline imitation learning with world models. _arXiv preprint arXiv:2302.03086_, 2023.
* [16] Yan Duan, Marcin Andrychowicz, Bradly Stadie, OpenAI Jonathan Ho, Jonas Schneider, Ilya Sutskever, Pieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. _Advances in neural information processing systems_, 30, 2017.
* [17] Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline rl via supervised learning? _arXiv preprint arXiv:2112.10751_, 2021.
* [18] Arnaud Fickinger, Samuel Cohen, Stuart Russell, and Brandon Amos. Cross-domain imitation learning via optimal transport. _arXiv preprint arXiv:2110.03684_, 2021.
* [19] Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In _Conference on Robot Learning_, pages 158-168. PMLR, 2022.
* [20] Tim Franzmeyer, Philip HS Torr, and Joao F Henriques. Learn what matters: cross-domain imitation learning with task-relevant embeddings. _arXiv preprint arXiv:2209.12093_, 2022.

* [21] Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse reinforcement learning. _arXiv preprint arXiv:1710.11248_, 2017.
* [22] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* [23] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* [24] Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline hindsight information matching. _arXiv preprint arXiv:2111.10364_, 2021.
* [25] Tanmay Gangwani and Jian Peng. State-only imitation with transition dynamics mismatch. _arXiv preprint arXiv:2002.11879_, 2020.
* [26] Tanmay Gangwani, Yuan Zhou, and Jian Peng. Imitation learning from observations under transition model disparity. _arXiv preprint arXiv:2204.11446_, 2022.
* [27] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. _arXiv preprint arXiv:2110.04544_, 2021.
* [28] Divyansh Garg, Shuvam Chakraborty, Chris Cundy, Jiaming Song, and Stefano Ermon. Iq-learn: Inverse soft-q learning for imitation. _Advances in Neural Information Processing Systems_, 34:4028-4039, 2021.
* [29] Adam Gleave, Mohammad Taufeeque, Juan Rocamonde, Erik Jenner, Steven H. Wang, Sam Toyer, Maximilian Ernestus, Nora Belrose, Scott Emmons, and Stuart Russell. imitation: Clean imitation learning implementations, 2022.
* [30] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. _Advances in neural information processing systems_, 29, 2016.
* [31] Michael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence modeling problem. _Advances in neural information processing systems_, 34:1273-1286, 2021.
* [32] Firas Jarboui and Vianney Perchet. Offline inverse reinforcement learning. _arXiv preprint arXiv:2106.05068_, 2021.
* [33] Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. Strictly batch imitation learning by energy-based distribution matching. _Advances in Neural Information Processing Systems_, 33:7354-7365, 2020.
* [34] Shengyi Jiang, Jingcheng Pang, and Yang Yu. Offline imitation learning with a misspecified simulator. _Advances in neural information processing systems_, 33:8510-8520, 2020.
* [35] Kshitij Judah, Alan Fern, Prasad Tadepalli, and Robby Goetschalckx. Imitation learning with demonstrations and shaping rewards. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 28, 2014.
* [36] Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, and Donglin Wang. Beyond reward: Offline preference-guided policy optimization. _arXiv preprint arXiv:2305.16217_, 2023.
* [37] Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa. Imitation learning as f-divergence minimization. In _International Workshop on the Algorithmic Foundations of Robotics_, pages 313-329. Springer, 2020.
* [38] Liyiming Ke, Sanjiban Choudhury, Matt Barnes, Wen Sun, Gilwoo Lee, and Siddhartha Srinivasa. Imitation learning as f-divergence minimization. In _Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14_, pages 313-329. Springer International Publishing, 2021.

* [39] Geon-Hyeong Kim, Seokin Seo, Jongmin Lee, Wonseok Jeon, HyeongJoo Hwang, Hongseok Yang, and Kee-Eung Kim. Demodice: Offline imitation learning with supplementary imperfect demonstrations. In _International Conference on Learning Representations_, 2022.
* [40] Kuno Kim, Yihong Gu, Jiaming Song, Shengjia Zhao, and Stefano Ermon. Domain adaptive imitation learning. In _International Conference on Machine Learning_, pages 5286-5295. PMLR, 2020.
* [41] Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching. _arXiv preprint arXiv:1912.05032_, 2019.
* [42] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. _arXiv preprint arXiv:1912.13465_, 2019.
* [43] Yao Lai, Jinxin Liu, Zhentao Tang, Bin Wang, HAO Jianye, and Ping Luo. Chipformer: Transferable chip placement via offline decision transformer. _ICML_, 2023. URL https://openreview.net/pdf?id=j0miEURw87.
* [44] Youngwoon Lee, Andrew Szot, Shao-Hua Sun, and Joseph J Lim. Generalizable imitation learning from observation via inferring goal proximity. _Advances in neural information processing systems_, 34:16118-16130, 2021.
* [45] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021.
* [46] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* [47] Yunzhu Li, Jiaming Song, and Stefano Ermon. Infogail: Interpretable imitation learning from visual demonstrations. _Advances in Neural Information Processing Systems_, 30, 2017.
* [48] Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su. State alignment-based imitation learning. _arXiv preprint arXiv:1911.10947_, 2019.
* [49] Jinxin Liu, Donglin Wang, Qiangxing Tian, and Zhengyu Chen. Learn goal-conditioned policy with intrinsic motivation for deep reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7558-7566, 2022.
* [50] Jinxin Liu, Hongyin Zhang, and Donglin Wang. Dara: Dynamics-aware reward augmentation in offline reinforcement learning. _arXiv preprint arXiv:2203.06662_, 2022.
* [51] Jinxin Liu, Ziqi Zhang, Zhenyu Wei, Zifeng Zhuang, Yachen Kang, Sibo Gai, and Donglin Wang. Beyond odd state actions: Supported cross-domain offline reinforcement learning. _arXiv preprint arXiv:2306.12755_, 2023.
* [52] Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. _arXiv preprint arXiv:2004.09395_, 2020.
* [53] Minghuan Liu, Hanye Zhao, Zhengyu Yang, Jian Shen, Weinan Zhang, Li Zhao, and Tie-Yan Liu. Curriculum offline imitating learning. _Advances in Neural Information Processing Systems_, 34:6266-6277, 2021.
* [54] YuXuan Liu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Imitation from observation: Learning to imitate behaviors from raw video via context translation. In _2018 IEEE International Conference on Robotics and Automation (ICRA)_, pages 1118-1125. IEEE, 2018.
* [55] Yicheng Luo, Zhengyao Jiang, Samuel Cohen, Edward Grefenstette, and Marc Peter Deisenroth. Optimal transport for offline imitation learning. _arXiv preprint arXiv:2303.13971_, 2023.
* [56] Yecheng Jason Ma, Andrew Shen, Dinesh Jayaraman, and Osbert Bastani. Smodice: Versatile offline imitation learning via state occupancy matching. _arXiv e-prints_, pages arXiv-2202, 2022.

* [57] Tianwei Ni, Harshit Sikchi, Yufei Wang, Tejus Gupta, Lisa Lee, and Ben Eysenbach. f-irl: Inverse reinforcement learning via state marginal matching. In _Conference on Robot Learning_, pages 529-551. PMLR, 2021.
* [58] Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. _Neural computation_, 3(1):88-97, 1991.
* [59] Yiwen Qiu, Jialong Wu, Zhangjie Cao, and Mingsheng Long. Out-of-dynamics imitation learning from multimodal demonstrations. In _Conference on Robot Learning_, pages 1071-1080. PMLR, 2023.
* [60] Dripta S Raychaudhuri, Sujoy Paul, Jeroen Vanbaar, and Amit K Roy-Chowdhury. Cross-domain imitation from observations. In _International Conference on Machine Learning_, pages 8902-8912. PMLR, 2021.
* [61] Siddharth Reddy, Anca D Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement learning with sparse rewards. _arXiv preprint arXiv:1905.11108_, 2019.
* [62] Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 627-635. JMLR Workshop and Conference Proceedings, 2011.
* [63] Stefan Schaal. Learning from demonstration. _Advances in neural information processing systems_, 9, 1996.
* [64] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Jaskowski, and Jurgen Schmidhuber. Training agents using upside-down reinforcement learning. _arXiv preprint arXiv:1912.02877_, 2019.
* [65] Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. _arXiv preprint arXiv:1805.01954_, 2018.
* [66] Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation. _arXiv preprint arXiv:1807.06158_, 2018.
* [67] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.
* [68] Luca Viano, Yu-Ting Huang, Parameswaran Kamalaruban, Craig Innes, Subramanian Ramamoorthy, and Adrian Weller. Robust learning from observation with model misspecification. _arXiv preprint arXiv:2202.06003_, 2022.
* [69] Tianyu Wang, Nikhil Karnwal, and Nikolay Atanasov. Latent policies for adversarial imitation learning. _arXiv preprint arXiv:2206.11299_, 2022.
* [70] Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin. Discriminator-weighted offline imitation learning from suboptimal demonstrations. In _International Conference on Machine Learning_, pages 24725-24742. PMLR, 2022.
* [71] Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan. Prompting decision transformer for few-shot policy generalization. In _International Conference on Machine Learning_, pages 24631-24645. PMLR, 2022.
* [72] Sheng Yue, Guanbo Wang, Wei Shao, Zhaofeng Zhang, Sen Lin, Ju Ren, and Junshan Zhang. Clare: Conservative model-based reward learning for offline inverse reinforcement learning. _arXiv preprint arXiv:2302.04782_, 2023.
* [73] Wenjia Zhang, Haoran Xu, Haoyi Niu, Peng Cheng, Ming Li, Heming Zhang, Guyue Zhou, and Xianyuan Zhan. Discriminator-guided model-based offline imitation learning. In _Conference on Robot Learning_, pages 1266-1276. PMLR, 2023.
* [74] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. _International Journal of Computer Vision_, 130(9):2337-2348, 2022.

* [75] Zhuangdi Zhu, Kaixiang Lin, Bo Dai, and Jiayu Zhou. Off-policy imitation learning from observations. _Advances in Neural Information Processing Systems_, 33:12402-12413, 2020.
* [76] Zifeng Zhuang, Kun Lei, Jinxin Liu, Donglin Wang, and Yilang Guo. Behavior proximal policy optimization. _arXiv preprint arXiv:2302.11312_, 2023.
* [77] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In _Aaai_, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.
* [78] Konrad Zolna, Alexander Novikov, Ksenia Konyushkova, Caglar Gulcehre, Ziyu Wang, Yusuf Aytar, Misha Denil, Nando de Freitas, and Scott Reed. Offline learning from demonstrations and unlabeled experience. _arXiv preprint arXiv:2011.13885_, 2020.
* [79] Konrad Zolna, Scott Reed, Alexander Novikov, Sergio Gomez Colmenarejo, David Budden, Serkan Cabi, Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learning. In _Conference on Robot Learning_, pages 247-263. PMLR, 2021.

## 7 Additional Derivation

(_Repeat from the main paper._) To gain more insight into Equation 4 that captures the quality of IL (the degree of similarity to the expert data), we define \(D(\cdot,\cdot)\) as the sum of reverse KL and forward KL divergence, _i.e._, \(D(q,p)=D_{\text{KL}}(q\|p)+D_{\text{KL}}(p\|q)\), and derive an alternative form for Equation 4:

\[\arg\min_{\mathbf{z}^{*}}\;D(\pi_{\theta}(\boldsymbol{\tau}|\mathbf{z}^{*}), \pi_{E}(\boldsymbol{\tau}))=\arg\max_{\mathbf{z}^{*}}\;\underbrace{\mathcal{I }(\mathbf{z}^{*};\boldsymbol{\tau}_{E})-\mathcal{I}(\mathbf{z}^{*};\boldsymbol {\tau}_{\theta})}_{\mathcal{J}_{\text{ML}}}-\underbrace{D_{\text{KL}}(\pi_{ \theta}(\boldsymbol{\tau}),\pi_{E}(\boldsymbol{\tau}))}_{\mathcal{J}_{D}},\]

where \(\mathcal{I}(\mathbf{x};\mathbf{y})\) denotes the mutual information (MI) between \(\mathbf{x}\) and \(\mathbf{y}\), which measures the predictive power of \(\mathbf{y}\) on \(\mathbf{x}\) (or vice-versa), the latent variables are defined as \(\boldsymbol{\tau}_{E}:=\boldsymbol{\tau}\sim\pi_{E}(\boldsymbol{\tau})\), \(\boldsymbol{\tau}_{\theta}:=\boldsymbol{\tau}\sim p(\mathbf{z}^{*})\pi_{ \theta}(\boldsymbol{\tau}|\mathbf{z}^{*})\), and \(\pi_{\theta}(\boldsymbol{\tau})=\mathbb{E}_{\mathbf{z}^{*}}\left[\pi_{ \theta}(\boldsymbol{\tau}|\mathbf{z}^{*})\right]\).

Below is our derivation:

\[\min_{\mathbf{z}^{*}}\;D(\pi_{\theta}(\boldsymbol{\tau}|\mathbf{z}^ {*}),\pi_{E}(\boldsymbol{\tau}))\] \[=\min_{\mathbf{z}^{*}}\;\mathbb{E}_{p(\mathbf{z}^{*})}\left[D_{ \text{KL}}(\pi_{\theta}(\boldsymbol{\tau}|\mathbf{z}^{*})\|\pi_{E}( \boldsymbol{\tau}))+D_{\text{KL}}(\pi_{E}(\boldsymbol{\tau})\|\pi_{\theta}( \boldsymbol{\tau}|\mathbf{z}^{*}))\right]\] \[=\min_{\mathbf{z}^{*}}\;\mathbb{E}_{p(\mathbf{z}^{*})\pi_{ \theta}(\boldsymbol{\tau}|\mathbf{z}^{*})}\left[\log\pi_{\theta}(\boldsymbol{ \tau}|\mathbf{z}^{*})-\log\pi_{E}(\boldsymbol{\tau})\right]\] \[\qquad\qquad+\mathbb{E}_{p(\mathbf{z}^{*})\pi_{E}(\boldsymbol{ \tau})}\left[\log\frac{p(\mathbf{z}^{*}|\boldsymbol{\tau})\pi_{\theta}( \boldsymbol{\tau})}{p(\mathbf{z}^{*})}-\log\pi_{E}(\boldsymbol{\tau})\right]\] \[=\min_{\mathbf{z}^{*}}\;\mathbb{E}_{p(\mathbf{z}^{*})\pi_{ \theta}(\boldsymbol{\tau}|\mathbf{z}^{*})}\left[\log\frac{p(\mathbf{z}^{*}| \boldsymbol{\tau})\pi_{\theta}(\boldsymbol{\tau})}{p(\mathbf{z}^{*})}-\log \pi_{E}(\boldsymbol{\tau})\right]\] \[=\max_{\mathbf{z}^{*}}\;\mathcal{I}(\mathbf{z}^{*};\boldsymbol{ \tau}_{E})-\mathcal{I}(\mathbf{z}^{*};\boldsymbol{\tau}_{\theta})-D(\pi_{ \theta}(\boldsymbol{\tau}),\pi_{E}(\boldsymbol{\tau})),\]

where \(\boldsymbol{\tau}_{E}:=\boldsymbol{\tau}\sim\pi_{E}(\boldsymbol{\tau})\), \(\boldsymbol{\tau}_{\theta}:=\boldsymbol{\tau}\sim p(\mathbf{z}^{*})\pi_{ \theta}(\boldsymbol{\tau}|\mathbf{z}^{*})\).

## 8 More Comparisons and Ablation Studies

### Offline Comparison on D4RL Expert Domain Dataset

In Table 6, we provide the normalized return of our method and baseline methods on the reward-free D4RL [22] expert dataset. Consistently, we can observe that CEIL achieves a significant improvement over the baseline methods in both _S-off-IfD_ and _S-off-IfO_ settings. Compared to the state-of-the-art offline IL baselines, CEIL also shows competitive results on the challenging cross-domain offline IL settings (_C-off-IfD_ and _C-off-IfO_).

### Generalizability on Cross-domain Offline IL Settings

In the standard cross-domain IL setting, the goal is to extract expert-relevant information from the mismatched expert demonstrations/observations (expert domain) and to mimic such expert behaviors in the training environment (training domain). Thus, we validate the performance of the learned policy in the training environment (_i.e._, the environment where the offline data was collected). Here, we also study the generalizability of the learned policy by evaluating the learned policy in the expert environment (_i.e._, the environment where the mismatched expert data was collected). We provide the normalized scores (_evaluated in the expert domain_) in Table 7. We can find that across a range of cross-domain offline IL tasks, CEIL consistently demonstrates better (zero-shot) generalizability compared to baselines.

### Ablating the Cross-domain Regularization

We now conduct ablation studies to evaluate the importance of cross-domain regularization in Equation 9 (in the main paper). In Figure 5, we provide the performance improvement when we

\begin{table}
\begin{tabular}{l l r r r r r} \hline \hline  & & hop & hal & wal & ant & \\  & & expert & expert & expert & sum \\ \hline  & ORIL (TD3+BC) & 97.5 & 91.8 & 14.5 & 76.8 & 280.6 \\  & SQIL (TD3+BC) & 25.5 & 14.4 & 8.0 & 44.3 & 92.1 \\  & IQ-Learn & 37.3 & 9.9 & 46.6 & 85.9 & 179.7 \\ _S-off-LfD_ & ValueDICE & 65.6 & 2.9 & 28.2 & 90.5 & 187.1 \\  & DemoDICE & 107.3 & 87.1 & 104.8 & 114.2 & 413.3 \\  & SMODICE & 111.0 & 93.5 & 108.2 & 122.0 & **434.7** \\  & CEIL & 106.0 & 96.0 & 115.6 & 117.8 & **455.4** \\ \hline  & ORIL (TD3+BC) & 64.2 & 92.1 & 12.2 & 44.3 & 212.8 \\ _S-off-LfD_ & SMODICE & 111.3 & 93.7 & 108.0 & 122.0 & **435.0** \\  & CEIL & 103.3 & 96.8 & 110.0 & 126.4 & **436.5** \\ \hline  & ORIL (TD3+BC) & 24.4 & 78.3 & 29.3 & 32.1 & 164.1 \\  & SQIL (TD3+BC) & 12.2 & 19.9 & 8.8 & 21.2 & 62.0 \\  & IQ-Learn & 25.9 & 31.2 & 31.7 & 55.8 & 144.6 \\ _C-off-LfD_ & ValueDICE & 18.6 & 9.8 & 8.3 & 22.3 & 59.0 \\  & DemoDICE & 111.5 & 88.7 & 107.9 & 122.5 & 430.6 \\  & SMODICE & 111.1 & 93.8 & 108.2 & 120.9 & **434.0** \\  & CEIL & 105.8 & 97.1 & 108.6 & 112.2 & 423.7 \\ \hline  & ORIL (TD3+BC) & 22.5 & 76.6 & 11.2 & 28.2 & 138.6 \\ _C-off-LfO_ & SMODICE & 111.2 & 93.7 & 108.1 & 117.7 & 430.7 \\  & CEIL & 113.0 & 90.1 & 108.7 & 125.2 & **437.0** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Normalized scores (averaged over 30 trails for each task) on D4RL expert dataset. Scores within two points of the maximum score are highlighted. hop: Hopper-v2. hal: HalfCheetah-v2. wal: Walker2d-v2. ant: Ant-v2.

\begin{table}
\begin{tabular}{l l r r r r r r r r r} \hline \hline  & & \multicolumn{4}{c}{Hopper-v2} & \multicolumn{4}{c}{HalfCheetah-v2} & \multicolumn{1}{c}{sum} \\ \cline{3-10}  & & m & mr & me & e & m & mr & me & e & \\ \hline  & ORIL (TD3+BC) & 74.7 & 16.7 & 45.0 & 21.4 & 2.2 & 0.8 & -0.3 & -2.2 & 158.3 \\  & SQIL (TD3+BC) & 33.6 & 21.6 & 14.5 & 14.5 & 18.2 & 7.5 & 20.9 & 20.9 & 151.8 \\  & IQ-Learn & 11.8 & 9.7 & 17.1 & 17.1 & 7.7 & 7.8 & 9.5 & 9.5 & 90.2 \\  & ValueDICE & 49.5 & 24.2 & 55.7 & 49.3 & 32.2 & 32.9 & 38.7 & 28.7 & 311.2 \\  & DemoDICE & 83.2 & 31.5 & 81.6 & 28.5 & 0.9 & -1.1 & -1.7 & -2.4 & 220.6 \\  & SMODICE & 80.1 & 26.1 & 78.0 & 54.3 & 2.8 & -1.0 & 1.0 & -2.3 & 239.1 \\  & CEIL & 87.4 & 74.3 & 81.2 & 82.4 & 44.0 & 30.4 & 25.0 & 17.1 & **441.9** \\ \hline  & ORIL (TD3+BC) & 62.3 & 18.7 & 57.0 & 28.2 & 0.2 & 1.1 & -0.3 & -2.3 & 165.0 \\ _C-off-LfD_ & SMODICE & 77.6 & 22.5 & 80.2 & 71.0 & 2.0 & -0.9 & 0.8 & -2.3 & 250.9 \\  & CEIL & 56.4 & 58.6 & 56.7 & 65.2 & 5.5 & 36.5 & 5.0 & 5.0 & **288.7** \\ \hline  & & \multicolumn{4}{c}{Walker2d-v2} & \multicolumn{4}{c}{Ant-v2} & \multicolumn{1}{c}{sum} \\ \cline{3-10}  & & m & mr & me & e & m & mr & me & e & sum \\ \hline  & ORIL (TD3+BC) & 22.0 & 24.5 & 23.9 & 33.1 & 16.0 & 18.6 & 2.5 & 0.4 & 141.0 \\  & SQIL (TD3+BC) & 32.4 & 14.9 & 10.3 & 10.3 & 71.4 & 63.6 & 60.1 & 60.1 & 323.1 \\  & IQ-Learn & 8.4 & 5.0 & 10.2 & 10.2 & 19.4 & 18.4 & 16.1 & 16.1 & 103.8 \\ _C-off-LfD_ & ValueDICE & 31.7 & 21.9 & 22.9 & 27.7 & 70.5 & 68.5 & 69.3 & 68.5 & 380.9 \\  & DemoDICE & 12.8 & 31.5 & 12.9 & 86.9 & 15.7 & 24.2 & 2.3 & 1.4 & 187.7 \\  & SMODICE & 43.6 & 16.1 & 62.0 & 85.3 & 23.7 & 22.9 & 2.3 & -5.9 & 249.9 \\  & CEIL & 102.8 & 94.8 & 101.9 & 100.7 & 82.0 & 77.0 & 76.4 & 79.8 & **715.3** \\ \hline  & ORIL (TD3+BC) & 22.4 & 15.2 & 17.8 & 12.6 & 13.6 & 20.7 & 5.5 & -6.2 & 101.6 \\ _C-off-LfO_ & SMODICE & 42.4 & 17.0 & 55.5 & 88.7 & 15.7 & 22.6 & 2.5 & -6.3 & 238.1 \\  & CEIL & 67.9 & 12.0 & 68.4 & 50.8 & 31.7 & 57.0 & 18.0 & -1.9 & **304.0** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Normalized scores (_evaluated on the expert dataset_ over 30 trails for each task) on 2 cross-domain offline IL settings: _C-off-LfD_ and _C-off-LfO_. Scores within two points of the maximum score are highlighted. m: medium. mr: medium-replay. me: medium-expert. e: expert.

able the cross-domain regularization in two cross-domain offline IL tasks (_C-off-LfD_ and _C-off-LfO_). We can find that in 26 out of 32 cross-domain tasks, ablating the regularization can cause performance to decrease (negative performance improvement), thus verifying the benefits of encouraging task-relevant embeddings.

Figure 5: Normalized performance improvement (left: _C-off-LfD_, right: _C-off-LfO_) when we ablate the cross-domain regularization (Equation 9 in the main paper) in cross-domain IL settings. We can observe the general trend (in 26 out of 32 tasks) that ablating the cross-domain regularization causes negative performance improvement. hop: Hopper-v2. hal: HalfCheetah-v2. wal: Walker2d-v2. ant: Ant-v2. m: medium. me: medium-expert. mr: medium-replay. e: expert.

Figure 6: Aggregate median, IQM, mean, and optimality gap over 16 offline IL tasks. Higher median, higher IQM, and higher mean and lower optimality gap are better. The shaded bar shows 95% stratified bootstrap confidence intervals. We can see that CEIL achieves consistently better performance across a wide range of offline IL settings.

[MISSING_PAGE_FAIL:19]

### Limitation (Failure Modes in Online LfO Setting)

Meanwhile, we find that in the online LfO settings, CEIL's performance deteriorates severely on a few tasks, as shown in Figure 7 (Walker2d). In LfD (either on single-domain or on cross-domain IL) settings, CEIL can consistently achieve expert-level performance, but when migrating to LfO settings, CEIL suffers collapsing performance under the same number of environmental interactions. We believe that this is due to the lack of expert actions in LfO settings, which causes the agent to stay in the collapsed state region and therefore deteriorates performance. Thus, we believe a rich direction for future research is to explore the online exploration ability.

## 9 Implementation Details

### Imitation Learning Tasks

In our paper, we conduct experiments across a variety of IL problem domains: single/cross-domain IL, online/offline IL, and LfD/LfO IL settings. By arranging and combining these IL domains, we obtain 8 IL tasks in all: _S-on-LfD_, _S-on-LfO_, _S-off-LfD_, _S-off-LfO_, _C-on-LfD_, _C-on-LfO_, _C-off-LfD_, and _C-off-LfO_, where S/C denotes single/cross-domain IL, on/off denotes online/offline IL, and LfD/LfO denote learning from demonstrations/observations respectively.

_S-on-LfD._ We have access to a limited number of expert demonstrations and an online interactive training environment. The goal of _S-on-LfD_ is to learn an optimal policy that mimics the provided demonstrations in the training environment.

_S-on-LfO._ We have access to a limited number of expert observations (state-only demonstrations) and an online interactive training environment. The goal of _S-on-LfO_ is to learn an optimal policy that mimics the provided observations in the training environment.

_S-off-LfD._ We have access to a limited number of expert demonstrations and a large amount of pre-collected offline (reward-free) data. The goal of _S-off-LfD_ is to learn an optimal policy that mimics the provided demonstrations in the environment in which the offline data was collected. Note that here _the environment_ that was used to collect the expert demonstrations and _the environment_ that was used to collect the offline data are the same environment.

_S-off-LfO._ We have access to a limited number of expert observations and a large amount of pre-collected offline (reward-free) data. The goal of _S-off-LfO_ is to learn an optimal policy that mimics the provided observations in the environment in which the offline data was collected. Note that here _the environment_ that was used to collect the expert observations and _the environment_ that was used to collect the offline data are the same environment.

_C-on-LfD._ We have access to a limited number of expert demonstrations and an online interactive training environment. The goal of _C-on-LfD_ is to learn an optimal policy that mimics the provided demonstrations in the training environment. Note that here _the environment_ that was used to collect the expert demonstrations and _the online training environment_ are _not the same environment_.

_C-on-LfO._ We have access to a limited number of expert observations and a large amount of pre-collected offline (reward-free) data. The goal of _C-off-LfD_ is to learn an optimal policy that mimics the provided demonstrations in the environment in which the offline data was collected. Note that here _the environment_ that was used to collect the expert demonstrations and _the environment_ that was used to collect the offline data are _not the same environment_.

_C-off-LfO._ We have access to a limited number of expert demonstrations and a large amount of pre-collected offline (reward-free) data. The goal of _C-off-LfO_ is to learn an optimal policy that mimics the provided demonstrations in the environment in which the offline data was collected. Note that here _the environment_ that was used to collect the expert demonstrations and _the environment_ that was used to collect the offline data are _not the same environment_.

_C-off-LfO._ We have access to a limited number of expert observations and a large amount of pre-collected offline (reward-free) data. The goal of _C-off-LfO_ is to learn an optimal policy that mimics the provided demonstrations in the environment in which the offline data was collected. Note that here _the environment_ that was used to collect the expert demonstrations and _the environment_ that was used to collect the offline data are _not the same environment_.

_C-off-LfO._ We have access to a limited number of expert observations and a large amount of pre-collected offline (reward-free) data. The goal of _C-off-LfD_ is to learn an optimal policy that mimics the provided demonstrations in the environment in which the offline data was collected. Note that here _the environment_ that was used to collect the expert demonstrations and _the environment_ that was used to collect the offline data are _not the same environment_.

### Online IL Environments, Offline IL Datasets, and One-shot tasks

Our experiments are conducted in four popular MuJoCo environments (Figure 8): Hopper-v2, HalfCheetah-v2, Walker2d-v2, and Ant-v2. For offline IL tasks, we take the standard (reward-free) D4RL dataset [22] (medium, medium-replay, medium-expert, and expert domains) as the offline dataset. For cross-domain (online/offline) IL tasks, we collect the expert behaviors (demonstrations or observations) on a modified MuJoCo environment. Specifically, we change the height of the agent's torso (as shown in Figure 8). We refer the reader to our code submission, which includes our modified MuJoCo assets. For one-shot IL tasks, we train the policy only in the single-domain IL settings (_S-on-LfD_, _S-on-LfO_, _S-off-LfD_, and _S-off-LfO_). Then we collect only one expert trajectory in the modified MuJoCo environment, and roll out the fine-tuned/inferred policy in the modified environment to test the one-shot performance.

**Collecting expert behaviors.** In our implementation, we use the publicly available rlkit7 implementation of SAC to learn an expert policy and use the learned policy to collect expert behaviors (demonstrations in LfD or observations in LfO).

Footnote 7: https://github.com/rail-berkeley/rlkit.

### CEIL Implementation Details

**Trajectory self-consistency loss.** To learn the embedding function \(f_{\phi}\) and a corresponding contextual policy \(\pi_{\theta}(\mathbf{a}|\mathbf{s},\mathbf{z})\), we minimize the following trajectory self-consistency loss:

\[\pi_{\theta},f_{\phi}=\min_{\pi_{\phi},f_{\phi}}\;-\mathbb{E}_{\mathbf{\tau} _{1:T}\sim\mathcal{D}(\mathbf{\tau}_{1:T})}\mathbb{E}_{(\mathbf{s},\mathbf{a })\sim\mathbf{\tau}_{1:T}}\left[\log\pi_{\theta}(\mathbf{a}|\mathbf{s},f_{ \phi}(\mathbf{\tau}_{1:T}))\right],\]

where \(\mathbf{\tau}_{1:T}\) denotes a trajectory segment with window size of \(T\). In the online setting, we sample trajectory \(\bm{\tau}\) from the experience replay buffer \(\mathcal{D}(\bm{\tau})\); in the offline setting, we sample trajectory \(\bm{\tau}\) directly from the given offline data \(\mathcal{D}(\bm{\tau})\). Meanwhile, if we can access the expert actions (_i.e._, LfD settings), we also incorporate the expert demonstrations into the empirical expectation (_i.e._, storing the expert demonstrations into the online/offline experience \(\mathcal{D}(\bm{\tau})\)).

In our implementation, we use a 4-layer MLP (with ReLU activation) to encode the trajectory \(\bm{\tau}_{1:T}\) and a 4-layer MLP (with ReLU activation ) to predict the action respectively. To regularize the learning of the encoder function \(f_{\phi}\), we additionally introduce a decoder network (4-layer MLP with ReLU activation) \(\pi^{\prime}_{\theta}(\mathbf{s}^{\prime}|\mathbf{s},f_{\phi}(\bm{\tau}_{1:T}))\) to predict the next states: \(\min_{\pi^{\prime}_{\theta},f_{\phi}}\;-\mathbb{E}_{\mathbf{\tau}_{1:T}\sim \mathcal{D}(\mathbf{\tau}_{1:T})}\mathbb{E}_{(\mathbf{s},\mathbf{a},\mathbf{ s}^{\prime})\sim\mathbf{\tau}_{1:T}}\left[\log\pi^{\prime}_{\theta}( \mathbf{s}^{\prime}|\mathbf{s},f_{\phi}(\bm{\tau}_{1:T}))\right]\). Further, to circumvent issues of "posterior collapse" [67], we encourage learning quantized latent embeddings. In a similar spirit to VQ-VAE [67], we incorporate ideas from vector quantization (VQ) and introduce the following regularization: \(\min_{f_{\phi}}||\text{sg}[z_{e}(\bm{\tau}_{1:T})]-e||^{2}+||z_{e}(\bm{\tau}_{1 :T})-\text{sg}[e]||^{2}\), where \(e\) is a dictionary of vector quantization embeddings (we set the size of this embedding dictionary to be 4096), \(z_{e}(\bm{\tau}_{1:T})\) is defined as the nearest dictionary embedding to \(f_{\phi}(\bm{\tau}_{1:T})\), and \(\text{sg}[\cdot]\) denotes the stop-gradient operator.

**Out-level embedding inference.** In Section 4.2 (main paper), we approximate \(\mathcal{J}_{\text{MI}}\) with \(\mathcal{J}_{\text{MI}(f_{\phi})}\triangleq\mathbb{E}_{p(\bm{\tau}^{*})\pi_{ E}(\bm{\tau}_{E})\pi_{\theta}(\bm{\tau}_{\theta}|\mathbf{s}^{*})}\left[-\|\bm{z}^{*}-f_{ \phi}(\bm{\tau}_{E})\|^{2}+\|\bm{z}^{*}-f_{\phi}(\bm{\tau}_{\theta})\|^{2}\right]\), where we replace the mutual information with \(-\|\bm{z}^{*}-f_{\phi}(\bm{\tau})\|^{2}\) by leveraging the learned embedding function \(f_{\phi}\). Empirically, we find that we can ignore the second loss \(\|\bm{z}^{*}-f_{\phi}(\bm{\tau}_{\theta})\|^{2}\), and directly conduct outer-level embedding inference with \(\max_{\bm{z}^{*},f_{\phi}}\mathbb{E}_{p(\bm{z}^{*})\pi_{E}(\bm{\tau}_{E})} \left[-\|\bm{z}^{*}-f_{\phi}(\bm{\tau}_{E})\|^{2}\right]\). Meanwhile, this simplification makes the support constraints (\(\mathcal{R}(\bm{z}^{*})\) in Equation 7 in the main paper) for the offline OOD issues naturally satisfied, since \(\max_{\bm{z}^{*}}\mathbb{E}_{p(\bm{z}^{*})\pi_{E}(\bm{\tau}_{E})}\left[-\|\bm{ z}^{*}-f_{\phi}(\bm{\tau}_{E})\|^{2}\right]\) and \(\min_{\bm{z}^{*}}\mathcal{R}(\bm{z}^{*})\) are equivalent.

**Cross-domain IL regularization.** To encourage \(f_{\phi}\) to capture the task-relevant embeddings and ignore the domain-specific factors, we set the regularization \(\mathcal{R}(f_{\phi})\) in Equation 5 to be:

Figure 8: MuJoCo environments and our modified versions. From left to right: Ant-v2, HalfCheetah-v2, Hopper-v2, Walker2d-v2, our modified Ant-v2, our modified HalfCheetah-v2, our modified Hopper-v2, and our modified Walker2d-v2.

\(\mathcal{R}(f_{\phi})=\mathcal{I}(f_{\phi}(\bm{\tau});\mathbf{n})\), where we couple each trajectory \(\bm{\tau}\) in \(\{\bm{\tau}_{E}\}\cup\{\bm{\tau}_{E^{\prime}}\}\) with a label \(\mathbf{n}\in\{\bm{0},\bm{1}\}\), indicating whether it is noised. In our implementation, we apply MINE [6] to estimate the mutual information and conduct encoder regularization. Specifically, we estimate \(\mathcal{I}(\mathbf{z};\mathbf{n})\) with \(\hat{\mathcal{I}}(\mathbf{z};\mathbf{n}):=\sup_{\delta}\mathbb{E}_{p(\mathbf{ z},\mathbf{n})}\left[f_{\delta}(\mathbf{z},\mathbf{n})\right]-\log\mathbb{E}_{p( \mathbf{z})p(\mathbf{n})}\left[\exp\left(f_{\delta}(\mathbf{z},\mathbf{n}) \right)\right]\) and regularize the encoder \(f_{\phi}\) with \(\max_{f_{\phi}}\hat{\mathcal{I}}(f_{\phi}(\bm{\tau});\mathbf{n})\), where we model \(f_{\delta}\) with a 4-layer MLP (using ReLU activations).

**Hyper-parameters.** In Table 9, we list the hyper-parameters used in the experiments. For the size of the embedding dictionary, we selected it from a range of [512, 1024, 2048, 4096]. We found 4096 to almost uniformly attain good performance across IL tasks, thus selecting it as the default. For the size of the embedding dimension, we tried four values [4, 8, 16, 32] and selected 16 as the default. For the trajectory window size, we tried five values [2, 4, 8, 16, 32] but we did not observe a significant difference in performance across these values. Thus we selected 2 as the default value. For the learning rate scheduler, we tried the default Pytorch scheduler and CosineAnnealingWarmRestarts, and found CosineAnnealingWarmRestarts enables better results (thus we selected it). For other hyperparameters, they are consistent with the default values of most RL implementations, e.g. learning rate 3e-4 and the MLP policy.

### Baselines Implementation Details

We summarize our code-bases of our baseline implementations in Table 10 and describe each baseline as follows:

**Generative Adversarial Imitation Learning (GAIL).** GAIL [30] is a GAN-based online LfD method that trains a policy (generator) to confuse a discriminator trained to distinguish between generated transitions and expert transitions. While the goal of the discriminator is to maximize the

\begin{table}
\begin{tabular}{l l} \hline \hline Baselines & Code-bases \\ \hline GAIL, GAIfO, AIRL & https://github.com/HumanCompatibleAI/imitation \\ SAIL & https://github.com/FangchenLiu/SAIL \\ IQ-Learn, SQIL & https://github.com/Div99/IQ-Learn \\ ValueDICE & https://github.com/google-research/google-research/tree/master/value_dice \\ DemoDICE & https://github.com/KAIST-AILab/imitation-dice \\ SMODICE, ORIL & https://github.com/JasonMa2016/SMODICE \\ \hline \hline \end{tabular}
\end{table}
Table 10: Baseline methods and their code-bases.

\begin{table}
\begin{tabular}{l l} \hline \hline Parameter & Value \\ \hline size of the embedding dictionary & 4096 \\ size of the embedding dimension & 16 \\ trajectory window size & 2 \\ \hline encoder: optimizer & Adam \\ encoder: learning rate & 3e-4 \\ encoder: learning rate scheduler & CosineAnnealingWarmRestarts(T\_0 = 1000,T\_mult=1, eta\_min=1e-5) \\ encoder: number of hidden layers & 4 \\ encoder: number of hidden units per layer & 512 \\ encoder: nonlinearity & ReLU \\ \hline policy: optimizer & Adam \\ policy: learning rate & 3e-4 \\ policy: learning rate scheduler & CosineAnnealingWarmRestarts(T\_0 = 1000,T\_mult=1, eta\_min=1e-5) \\ policy: number of hidden layers & 4 \\ policy: number of hidden units per layer & 512 \\ policy: nonlinearity & ReLU \\ \hline decoder: optimizer & Adam \\ decoder: learning rate scheduler & CosineAnnealingWarmRestarts(T\_0 = 1000,T\_mult=1, eta\_min=1e-5) \\ decoder: number of hidden layers & 4 \\ decoder: number of hidden units per layer & 512 \\ decoder: nonlinearity & ReLU \\ \hline \hline \end{tabular}
\end{table}
Table 9: CEIL hyper-parameters.

objective below, the policy is optimized via an RL algorithm to match the expert occupancy measure (minimize the objective below):

\[\mathcal{J}(\pi,D)=\mathbb{E}_{\pi}\left[\log(D(s,a))\right]+\mathbb{E}_{\pi_{E}} \left[1-\log(D(s,a))\right]-\lambda H(\pi).\]

We used the implementation by Gleave et al. [29] on the GitHub page8, where there are two modifications introduced with respect to the original paper: 1) a higher output of the discriminator represents better, 2) PPO is used to optimize the policy instead of TRPO.

Footnote 8: https://github.com/HumanCompatibleAI/imitation

**Generative Adversarial Imitation from Observations (GAIfO).** GAIfO [66] is an online LfO method that applies the principle of GAIL and utilizes a state-only discriminator to judge whether the generated trajectory matches the expert trajectory in terms of states. We provide the objective of GAIfO as follows:

\[\mathcal{J}(\pi,D)=\mathbb{E}_{\pi}\left[\log(D(s,s^{\prime}))\right]+\mathbb{ E}_{\pi_{E}}\left[1-\log(D(s,s^{\prime}))\right]-\lambda H(\pi).\]

Based on the implementation of GAIL, we implement GAIfO by changing the input of the discriminator to state transitions.

**Adversarial Inverse Reinforcement Learning (AIRL).** AIRL [21] is an online LfD/LfO method using an adversarial learning framework similar to GAIL. It modifies the form of the discriminator to explicitly disentangle the task-relevant information from the transition dynamics. To make the policy more generalized and less sensitive to dynamics, AIRL proposes to learn a parameterized reward function using the output of the discriminator:

\[f_{\theta,\phi}(s,a,s^{\prime}) =g_{\theta}(s,a)+\lambda h_{\phi}(s^{\prime})-h_{\phi}(s),\] \[D_{\theta,\phi}(s,a,s^{\prime}) =\frac{\exp(\not{f}_{\theta,\phi}(s,a,s^{\prime}))}{\exp(\not{f}_ {\theta,\phi}(s,a,s^{\prime}))+\pi(a|s)}.\]

Similarly to GAIL, we used the code provided by Gleave et al. [29], and the RL algorithm is also PPO.

**State Alignment-based Imitation Learning (SAIL).** SAIL [48] is an online LfO method capable of solving cross-domain tasks. SAIL aims to minimize the divergence between the policy rollout and the expert trajectory from both local and global perspectives: 1) locally, a KL divergence between the policy action and the action predicted by a state planner and an inverse dynamics model, 2) globally, a Wasserstein divergence of state occupancy between the policy and the expert. The policy is optimized using:

\[\mathcal{J}(\pi) =-D_{\mathcal{W}}(\pi(s)\|\pi_{E}(s))-\lambda D_{KL}(\pi(\cdot|s_{ t})\|\pi_{E}(\cdot|s_{t}))\] \[=\mathbb{E}_{\pi(s_{t},a_{t},s_{t+1})}\bigg{(}\sum_{t=1}^{T} \frac{D(s_{t+1})-\mathbb{E}_{\pi_{E}(s)}D(s)}{T}\bigg{)}-\lambda D_{KL}\bigg{(} \pi\big{(}\cdot|s_{t}\big{)}\|g_{\text{inv}}\big{(}\cdot|s_{t},f(s_{t})\big{)} \bigg{)},\]

where \(D\) is a state-based discriminator trained via \(\mathcal{J}(D)=\mathbb{E}_{\pi_{E}}\left[D(s)\right]-\mathbb{E}_{\pi}\left[D (s)\right]\), \(f\) is the pretrained VAE-based state planner, and \(g_{\text{inv}}\) is the inverse dynamics model trained by supervised regression.

In the online setting, we use the official implementation published by the authors9, where SAIL is optimized using PPO with the reward definition: \(r(s_{t},s_{t+1})=\frac{1}{T}\left[D(s_{t+1})-\mathbb{E}_{\pi_{E}(s)}D(s)\right]\). Besides, we further implement SAIL in the offline setting by using TD3+BC [23] to maximize the reward defined above.

Footnote 9: https://github.com/FangchenLiu/SAIL

In our experiments, we empirically discover that SAIL is computationally expensive. While SAIL is able to learn tasks in the typical IL setting (_S-on-LfD_), our early experimental results find that SAIL(TD3+BC) with heavy hyperparameter tuning failed on the offline setting. This indicates that SAIL is rather sensitive to the dataset composition, which also coincides with the results gathered in Ma et al. [56]. Thus, we do not include SAIL in our comparison results.

**Soft-Q Imitation Learning (SQIL).** SQIL [61] is a simple but effective single-domain LfD IL algorithm that is easy to implement with both online and offline Q-learning algorithms. The main idea of SQIL is to give sparse rewards (+1) only to those expert transitions and zero rewards (0) to those experiences in the replay buffer. The Q-function of SQIL is updated using the squared soft Bellman Error:

\[\delta^{2}(\mathcal{D},r)\triangleq\frac{1}{|\mathcal{D}|}\sum_{(s,a,s^{\prime}) \in\mathcal{D}}\bigg{(}Q(s,a)-\Big{(}r+\gamma\log\big{(}\sum_{a^{\prime}\in \mathcal{A}}\exp(Q(s^{\prime},a^{\prime}))\big{)}\Big{)}\bigg{)}^{2}.\]

The overall objective of the Q-function is to maximize the following objective:

\[\mathcal{J}(Q)=-\delta^{2}(\mathcal{D}_{E},1)-\delta^{2}(\mathcal{D}_{\pi},0).\]

In our experiments, the online imitation policy is optimized using SAC which is also used in the original paper. To make a fair comparison among the offline IL baselines, the offline policy is optimized via TD3+BC.

**Offline Reinforced Imitation Learning (ORIL).** ORIL [78] is an offline single-domain IL method that solves both LfD and LfO tasks. To relax the hard-label assumption (like the sparse rewards made in SQIL), ORIL treats the experiences stored in the replay buffer as unlabelled data that could potentially include both successful and failed trajectories. More specifically, ORIL aims to train a reward function to distinguish between the expert and the suboptimal data without explicitly knowing the negative labels. By incorporating Positive-unlabeled learning (PU-learning), the objective of the reward model can be written as follows (for the LfD setting):

\[\mathcal{J}(R)=\eta\mathbb{E}_{\pi_{E}(s,a)}\left[\log(R(s,a))\right]+\mathbb{ E}_{\pi(s,a)}\left[\log(1-R(s,a))\right]-\eta\mathbb{E}_{\pi_{E}(s,a)}\left[ \log(1-R(s,a))\right],\]

where \(\eta\) is the relative proportion of the expert data and we set it as 0.5 throughout our experiments. In the original paper, the policy learning algorithm of ORIL is Critic Regularized Regression (CRR), while in this paper, we implemented ORIL using TD3+BC for fair comparisons. Besides, we adapted ORIL to the LfO setting by learning a state-only reward function:

\[\mathcal{J}(R)=\eta\mathbb{E}_{\pi_{E}(s,s^{\prime})}\left[\log(R(s,s^{\prime} ))\right]+\mathbb{E}_{\pi(s,s^{\prime})}\left[\log(1-R(s,s^{\prime}))\right]- \eta\mathbb{E}_{\pi_{E}(s,s^{\prime})}\left[\log(1-R(s,s^{\prime}))\right].\]

**Inverse soft-Q learning (IQ-Learn).** IQ-Learn [28] is an IRL-based method that can solve IL tasks in the online/offline and LfD/LfO settings. It proposes to directly learn a Q-function from demonstrations and avoid the intermediate step of reward learning. Unlike GAIL optimizing a min-max objective defined in the reward-policy space, IQ-Learn solves the expert matching problem directly in the policy-Q space. The Q-function is trained to maximize the objective:

\[\mathbb{E}_{\pi_{E}(s,a,s^{\prime})}\left[Q(s,a)-\gamma V^{\pi}(s^{\prime}) \right]-\mathbb{E}_{\pi(s,a,s^{\prime})}\left[Q(s,a)-\gamma V^{\pi}(s^{\prime })\right]-\psi(r),\]

where \(V^{\pi}(s)\triangleq\mathbb{E}_{a\sim\pi(\cdot|s)}\left[Q(s,a)-\log\pi(a|s)\right]\), \(\psi(r)\) is a regularization term calculated over the expert distribution. Then, the policy is learned by SAC.

We use the code provided in the official IQ-learn repository10 and reproduce the online-LfD results reported in the original paper. For online tasks, we empirically find that penalizing the Q-value on the initial states gives the best and most stabilized performance. The learning objective of the Q-function for the online tasks is:

Footnote 10: https://github.com/Div99/IQ-Learn

\[\mathcal{J}(Q)=\mathbb{E}_{\pi_{E}(s,a,s^{\prime})}\left[Q(s,a)-\gamma V^{ \pi}(s^{\prime})\right]-(1-\gamma)\mathbb{E}_{\rho_{0}}\left[V^{\pi}(s_{0}) \right]-\psi(r).\]

In the offline setting, we find that using the above objective easily leads to an overfitting issue, causing collapsed performance. Thus, we follow the instruction provided in the paper and only penalize the expert samples:

\[\mathcal{J}(Q) =\mathbb{E}_{\pi_{E}(s,a,s^{\prime})}\left[Q(s,a)-\gamma V^{\pi}(s ^{\prime})\right]-\mathbb{E}_{\pi_{E}(s,a,s^{\prime})}\left[V^{\pi}(s)-\gamma V ^{\pi}(s^{\prime})\right]-\psi(r)\] \[=\mathbb{E}_{\pi_{E}(s,a,s^{\prime})}\left[Q(s,a)-V^{\pi}(s) \right]-\psi(r).\]

**Imitation Learning via Off-Policy Distribution Matching (ValueDICE).** ValueDICE [41] is a DICE-based11 LfD algorithm which minimizes the divergence of state-action distributions between the policy and the expert. In contrast to the state-conditional distribution of actions \(\pi(\cdot|s)\) used in the above methods, the state-action distribution, \(d^{\pi}(s,a):\mathcal{S}\times\mathcal{A}\to[0,1]\), can uniquely characterize a one-to-one correspondence,

\[d^{\pi}(s,a)\triangleq(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\textbf{Pr}(s_{t}=s,a_{t}=a\mid s_{0}\sim\rho_{0},a_{t}\sim\pi(s_{t}),s_{t+1}\sim P(s_{t},a_{t})).\]

Thus, the plain expert matching objective can be reformulated and expressed in the Donsker-Varadhan representation:

\[\mathcal{J}(\pi) =-D_{KL}(d^{\pi}(s,a)\|d^{\pi_{E}}(s,a))\] \[=\min_{x:\mathcal{S}\times\mathcal{A}\to\mathbb{R}}\log\mathbb{E}_ {(s,a)\sim d^{\pi_{E}}}\left[\exp(x(s,a))\right]-\mathbb{E}_{(s,a)\sim d^{\pi} }\left[x(s,a)\right].\]

The objective above can be expanded further by defining \(x(s,a)=v(s,a)-\mathcal{B}^{\pi}v(s,a)\) and using a zero-reward Bellman operator \(\mathcal{B}^{\pi}\) to derive the following (adversarial) objective:

\[\mathcal{J}_{DICE}(\pi,v)=\log\mathbb{E}_{(s,a)\sim d^{\pi_{E}}}\left[\exp \left(v(s,a)-\mathcal{B}^{\pi}v(s,a)\right)\right]-(1-\gamma)\mathbb{E}_{s_{0 }\sim\rho_{0},a_{0}\sim\pi\left(\cdot\mid s_{0}\right)}\left[v(s_{0},a_{0}) \right].\]

We use the official Tensorflow implementation12 in our experiments. In the online setting, the rollouts collected are used as an additional replay regularization. The overall objective in the online setting is:

Footnote 12: https://github.com/google-research/google-research/tree/master/value_dice

\[\mathcal{J}_{DICE}^{mix}(\pi,v)\] \[=-D_{KL}\big{(}(1-\alpha)d^{\pi}(s,a)+\alpha d^{RB}(s,a)\|(1- \alpha)d^{\pi_{E}}(s,a)+\alpha d^{RB}(s,a)\big{)}\] \[=\log\mathbb{E}_{(s,a)\sim d^{mix}}\left[\exp\left(v(s,a)- \mathcal{B}^{\pi}v(s,a)\right)\right]-(1-\alpha)(1-\gamma)\,\mathbb{E}_{s_{0 }\sim\rho_{0},\;a_{0}\sim\pi\left(\cdot\mid s_{0}\right)}\left[v(s_{0},a_{0})\right]\] \[-\alpha\,\mathbb{E}_{(s,a)\sim d^{RB}}\left[v(s,a)-\mathcal{B}^{ \pi}v(s,a)\right],\]

where \(d^{mix}\triangleq(1-\alpha)d^{\pi_{E}}+\alpha d^{RB}\) and \(\alpha\) is a non-negative regularization coefficient (we set \(\alpha\) as 0.1 following the specification of the paper).

In the offline setting, ValueDICE only differs in the source of sampling data. We change the online replay buffer to the offline pre-collected dataset.

**Offline Imitation Learning with Supplementary Imperfect Demonstrations (DemoDICE).**

DemoDICE [39] is a DICE-based offline LfD method that assumes to have access to an offline dataset collected by a behavior policy \(\pi_{\beta}\). Using this supplementary dataset, the expert matching objective of DemoDICE is instantiated over ValueDICE:

\[-D_{KL}(d^{\pi}(s,a)\|d^{\pi_{E}}(s,a))-\alpha D_{KL}(d^{\pi}(s,a)\|d^{\pi_{ \beta}}(s,a)),\]

where \(\alpha\) is a positive weight for the constraint.

The above optimization objective can be transformed into three tractable components: 1) a reward function \(r(s,a)\) derived by pre-training a binary discriminator \(D:\mathcal{S}\times\mathcal{A}\to[0,1]\):

\[r(s,a) =-\log(\frac{1}{D^{*}(s,a)}-1),\] \[D^{*}(s,a) =\operatorname*{arg\,max}_{D}=\mathbb{E}_{d^{\pi_{E}}}\left[\log D (s,a)\right]+\mathbb{E}_{d^{\pi_{\beta}}}\left[\log(1-D(s,a))\right],\]

2) a value function optimization objective:

\[\mathcal{J}(v)=-(1-\gamma)\mathbb{E}_{s\sim\rho_{0}}\left[v(s)\right]-(1+ \alpha)\log\mathbb{E}_{(s,a)\sim d^{\pi_{\beta}}}\left[\exp(\frac{r(s,a)+ \mathbb{E}_{s^{\prime}\sim P(s,a)}(v(s^{\prime}))-v(s)}{1+\alpha})\right],\]

and 3) a policy optimization step:

\[\mathcal{J}(\pi) =\mathbb{E}_{(s,a)\sim d^{\pi_{\beta}}}\left[v^{*}(s,a)\log\pi(a|s )\right],\] \[v^{*}(s,a) =\operatorname*{arg\,max}_{v}\mathcal{J}(v).\]

We report the offline results using the official Tensorflow implementation13.

Footnote 13: https://github.com/KAIST-AILab/imitation-dice

**State Matching Offline Distribution Correction Estimation (SMODICE).** SMODICE [56] proposes to solve offline IL tasks in LfO and cross-domain settings and it optimizes the following state occupancy objective:

\[-D_{KL}(d^{\pi}(s)\|d^{\pi_{E}}(s)).\]

To incorporate the offline dataset, SMODICE derives an f-divergence regularized state-occupancy objective:

\[\mathbb{E}_{s\sim d^{\pi}(s)}\left[\log(\frac{d^{\pi_{\beta}}(s)}{d^{\pi_{E}}( s)})\right]+-D_{f}(d^{\pi}(s,a)\|d^{\pi_{\beta}}(s,a)).\]

Intuitively, the first term can be interpreted as matching the offline states towards the expert states, while the second regularization term constrains the policy close to the offline distribution of state-action occupancy. Similarly, we can divide the objective into three steps: 1) deriving a state-based reward by learning a state-based discriminator:

\[r(s,a) =-\log(\frac{1}{D^{*}(s)}-1),\] \[D^{*}(s,a) =\operatorname*{arg\,max}_{D}=\mathbb{E}_{d^{\pi_{E}}}\left[\log D (s)\right]+\mathbb{E}_{d^{\pi_{\beta}}}\left[\log(1-D(s))\right],\]

2) learning a value function using the learned reward:

\[\mathcal{J}(v)=-(1-\gamma)\mathbb{E}_{s\sim\rho_{0}}\left[v(s) \right]-\log\mathbb{E}_{(s,a)\sim d^{\pi_{\beta}}}\left[f_{*}(r(s,a)+\mathbb{ E}_{s^{\prime}\sim P(s,a)}(v(s^{\prime}))-v(s))\right],\]

and 3) training the policy via weighted regression:

\[\mathcal{J}(\pi) =\mathbb{E}_{(s,a)\sim d^{\pi_{\beta}}}\left[f^{\prime}_{*}(r(s,a )+\mathbb{E}_{s^{\prime}\sim P(s,a)}(v^{*}(s^{\prime}))-v^{*}(s))\log\pi(a|s) \right],\] \[v^{*}(s,a) =\operatorname*{arg\,max}_{v}\mathcal{J}(v),\]

where \(f_{*}\) is the Fenchel conjugate of f-divergence (please refer to Ma et al. [56] for more details).

We conduct experiments using the official Pytorch implementation 14, where the f-divergence used is \(\mathcal{X}^{2}\)-divergence. On the LfD tasks, we change the input of the discriminator to state-action pairs.

Footnote 14: https://github.com/JasonMa2016/SMODICE