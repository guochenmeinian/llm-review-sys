# PAC Learning Linear Thresholds

from Label Proportions

 Anand Brahmbhatt

Google Research India

anandpareshb@google.com

&Rishi Saket

Google Research India

rishisaket@google.com

&Aravindan Raghuveer

Google Research India

araghuveer@google.com

###### Abstract

Learning from label proportions (LLP) is a generalization of supervised learning in which the training data is available as sets or _bags_ of feature-vectors (instances) along with the average instance-label of each bag. The goal is to train a good instance classifier. While most previous works on LLP have focused on training models on such training data, computational learnability of LLP was only recently explored by [25, 26] who showed worst case intractability of properly learning _linear threshold functions_ (LTFs) from label proportions. However, their work did not rule out efficient algorithms for this problem on natural distributions.

In this work we show that it is indeed possible to efficiently learn LTFs using LTFs when given access to random bags of some label proportion in which feature-vectors are, conditioned on their labels, independently sampled from a Gaussian distribution \(N(\bm{\mu},\bm{\Sigma})\). Our work shows that a certain matrix - formed using covariances of the differences of feature-vectors sampled from the bags with and without replacement - necessarily has its principal component, after a transformation, in the direction of the normal vector of the LTF. Our algorithm estimates the means and covariance matrices using subgaussian concentration bounds which we show can be applied to efficiently sample bags for approximating the normal direction. Using this in conjunction with novel generalization error bounds in the bag setting, we show that a low error hypothesis LTF can be identified. For some special cases of the \(N(\bm{0},\mathbf{I})\) distribution we provide a simpler mean estimation based algorithm. We include an experimental evaluation of our learning algorithms along with a comparison with those of [25, 26] and random LTFs, demonstrating the effectiveness of our techniques.

+
Footnote †: \(*\) – equal contribution

## 1 Introduction

In _learning from label proportions_ (LLP), the training data is aggregated into sets or _bags_ of feature-vectors (instances). For each bag we are given its constituent feature-vectors along with only the sum or average of their labels The goal is a to obtain a _good_ instance-level classifier - one that minimizes the classification error on a test set of instances or bags. In this work we study the LLP learnability over Gaussian distributions of linear threshold functions (LTFs), also called _linear classifiers_ or _halfspaces_, given by \(f(\mathbf{x})=\mathsf{pos}\left(\mathbf{r}^{\mathsf{T}}\mathbf{x}+c\right)\) where \(\mathsf{pos}(a):=1\) if \(a>0\) and \(0\) otherwise.

The _probably approximately correct_ (PAC) model of [29] states that a _concept_ class \(\mathcal{C}\) of \(\{0,1\}\)-valued functions can be learnt by a _hypothesis_ class \(\mathcal{H}\) if there is an algorithm to efficiently obtain, using iid samples from a distribution on \((\mathbf{x},f(\mathbf{x}))\), a hypothesis \(h\in\mathcal{H}\) of arbitrarily high accuracy on that distribution, for any unknown \(f\in\mathcal{C}\). If \(\mathcal{H}=\mathcal{C}\) we say that \(\mathcal{C}\) is _properly_ learnable, fore.g. LTFs are known to be properly learnable using linear programming ([3]). This notion can be extended to the LLP setting - which for brevity we call PAC-LLP - as follows: distribution \(D\) is over bags and their label proportions \((B,\sigma(B,f))\) where \(B=\{\mathbf{x}_{1},\ldots,\mathbf{x}_{q}\}\) is a bag of feature vectors and \(\sigma(B,f)=\text{Avg}\{f(\mathbf{x})\;\mid\;\mathbf{x}\in B\}\). A bag \((B,\sigma(B,f))\) is said to be _satisfied_ by \(h\) iff \(\sigma(B,h)=\sigma(B,f)\), and the accuracy of \(h\) is the fraction of bags satisfied by it.

With the above notion of PAC-LLP, [25] studied the learnability of LTFs and rather disturbingly showed that for any constant \(\varepsilon>0\) it is NP-hard to PAC-LLP learn an LTF using an LTF which satisfies \((1/2+\varepsilon)\)-fraction of the bags when all bags are of size at most \(2\), which was subsequently strengthened to a \((4/9+\varepsilon)\)-factor hardness by [26] who also proved \((1/q+\varepsilon)\)-factor hardness when the bag size is at most \(q\), for any \(q\geq 3\). This is in contrast to the supervised learning (i.e, with unit-sized bags) in which an LTF can be efficiently learnt by an LTF using linear programming. On the algorithmic side, [25] gave a _semi-definite programming_ (SDP) based algorithm to find an LTF satisfying \((2/5)\)-fraction of bags of size \(\leq 2\), which was extended by [26] to a fairly involved SDP yielding \((1/12)\)-approximation for bags of size \(\leq 3\), while no non-trivial algorithms for bags of size \(>3\) are known. These results show that PAC-LLP learning LTFs using LTFs is intractable on _hard_ bag distributions, and even the non-trivial algorithms for bag sizes \(\leq 3\) are via complicated convex programming techniques. A natural question therefore, is whether the problem is tractable on natural distributions that may arise out of real world scenarios.

We answer the above question in the affirmative when the feature-vectors are distributed according to some (unknown) Gaussian distribution \(\mathcal{D}=N(\bm{\mu},\bm{\Sigma})\) in \(d\)-dimensions. Gaussian distributions are ubiquitous in machine learning and in many applications the input data distribution is modeled as multivariate Gaussians, and several previous works [8; 30] have studied learnability in Gaussian distributions. An unknown target LTF is given by \(f(\mathbf{x}):=\mathsf{pos}\left(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x}+c_{*}\right)\) where \(\|\mathbf{r}_{*}\|_{2}=1\). Let \(\mathcal{D}_{a}\) be the distribution of \(\mathbf{x}\leftarrow\mathcal{D}\) conditioned on \(f(\mathbf{x})=a\), for \(a\in\{0,1\}\). Using this we formalize the notion of a distribution \(\mathcal{O}\) on bags of size \(q\) and average label \(k/q\): a random bag \(B\) sampled from \(\mathcal{O}\) consists of \(k\) id samples from \(\mathcal{D}_{1}\) and \((q-k)\) iid samples from \(\mathcal{D}_{0}\). The case of \(k\in\{0,q\}\) is uninteresting as all instances in such bags are either labeled \(0\) or \(1\) and traditional PAC-learning for LTFs can be employed directly. Unlike [25; 26] our objective is to directly maximize the instance-level level accuracy on \(\mathcal{D}\). With this setup we informally describe our main result.

**Our PAC-LLP LTF Learner** (Informal): Assuming mild conditions on \(\bm{\Sigma},\bm{\mu}\) and \(c_{*}\), for any \(q,k\in\mathbb{Z}^{+}\) s.t. \(1\leq k\leq q-1\) and \(\varepsilon,\delta>0\), there is an algorithm that samples at most \(m\) bags from \(\mathcal{O}\) and runs in time \(O(t+m)\) and with probability \(1-\delta\) produces an LTF \(h\) s.t.

\(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq\varepsilon\) if \(k\neq q/2\), and

\(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq\varepsilon\) or \(\Pr[f(\mathbf{x})\neq(1-h(\mathbf{x}))]\leq\varepsilon\) if \(k=q/2\),

where \(t,m\) are fixed polynomials in \(d,q,(1/\varepsilon),\log(1/\delta)\). We also obtain a more efficient algorithm when \(k\neq q/2\), \(\bm{\mu}=\bm{0}\), \(c_{*}=0\) and \(\bm{\Sigma}=\mathbf{I}\). The ambiguity in the case of \(k=q/2\) is inherent since bags of label proportion \(1/2\) consistent with an LTF \(f(\mathbf{x})\) are also consistent with \((1-f(\mathbf{x}))\).

**Remark 1.1** (Mixtures of \((q,k)\)).: _The training data could consist of bags of different sizes and label proportions, however typically the the maximum size of bags is bounded by (say) \(Q\), and in a large enough sample we would have at least \((1/Q^{2})\)-fraction of bags of a particular size and label proportion and we can apply our PAC-LLP LTF Learner above to that subsample._

### Related Work

The LLP problem is motivated by many real applications where labels are available not for each feature-vector but only as the average labels of bags of feature-vectors. This may occur because of privacy and legal ([24; 33])) reasons, supervision cost ([5]) or lack of labeling instrumentation ([10]). Previous works ([9; 15; 20; 24]) on LLP have applied techniques such as such as clustering, and linear classifiers and MCMC. Specifically for LLP, assuming class conditional independence of bags, [23] gave an algorithm to learn an exponential generative model, which was further generalized by [22]. On the other hand, the work of [34] proposed a novel _proportional_ SVM based algorithms which optimized the SVM loss over instance-labels which were constrained by bag-level loss w.r.t the given label-proportions. Subsequently, approaches based on deep neural nets for large-scale and multi-class data ([18; 11; 19; 21]), as well as bag pre-processing techniques ([28; 27]) have been developed. Recently, [4; 6] have proposed model training methods for either random or curated bags.

The LLP framework (as an analogue of PAC learning) was first formalized in the work of [35]. They bounded the generalization error of a trained classifier when taking the (bag, label-proportion)-pairsas instances sampled iid from some distribution. Their loss function was different - a weaker notion than the strict bag satisfaction predicate of [25, 26]. A single-bag variant - _class ratio estimation_ - of LLP was studied by [13] in which learning LTFs has a simple algorithm (see Appendix G). Nevertheless, the study of computational learning in the LLP framework has been fairly limited, apart from the works of [25, 26] whose results of learning LTFs in the LLP setting have been described earlier in this section.

In the fully supervised setting [3] showed that LTFs can be learnt using LTFs via linear programming without any distributional assumptions. Adversarial label noise makes the problem NP-hard to approximate beyond the trivial \(\nicefrac{{(1/2)}}{{2}}\)-factor even using constant degree polynomial thresholds as hypothesis ([12, 14, 2]). However, under distributional assumptions a series of results ([16, 17, 1, 7]) have given efficient algorithms to learn adversarially noisy LTFs.

Next, Sec. 1.2 mathematically defines our problem statement. Sec. 1.3 states the main results of this paper. Sec. 1.4 provides an overview of our techniques. Sec. 2 mentions some preliminary results which are used in our proofs. Sec. 3 defines and analyses a subroutine which we use in all our algorithms. Sec. 4 provides a complete proof for one of our main results. Sec 5 gives brief proof sketches of our other results. Sec. 6 mentions some experiments which support of our results.

### Problem Definition

**Definition 1.2** (Bag Oracle).: _Given distribution \(\mathcal{D}\) over \(\mathbb{R}^{d}\) and a target concept \(f:\mathbb{R}\to\{0,1\}\), the bag oracle for size \(q\) and label proportion \(k/q\) (\(1\leq k\leq q-1\)), denoted by \(\mathsf{Ex}(f,\mathcal{D},q,k)\), generates a bag \(\{\mathbf{x}^{(i)}\}_{i=1}^{q}\) such that \(\mathbf{x}^{(i)}\) is independently sampled from_ (i)_\(\mathcal{D}_{f,1}\) for \(i=\{1,\ldots,k\}\), and_ (ii)_\(\mathcal{D}_{f,0}\) _for \(i=\{k+1,\ldots,q\}\), where \(\mathcal{D}_{f,a}\) is \(\mathbf{x}\leftarrow\mathcal{D}\) conditioned on \(f(\mathbf{x})=a\), for \(a\in\{0,1\}\)._

### Our results

We first state our result (proved in Appendix A) for the case of standard \(d\)-dimensional Gaussian distribution \(N(\mathbf{0},\mathbf{I})\), homogeneous target LTF and unbalanced bags.

**Theorem 1.3**.: _For \(q>2\) and \(k\in\{1,\ldots,q-1\}\) s.t. \(k\neq q/2\) and LTF \(f(\mathbf{x}):=\mathsf{pos}(\mathbf{r}_{\star}^{\mathsf{T}}\mathbf{x})\), there is an algorithm that samples \(m\) iid bags from \(\mathsf{Ex}(f,N(\mathbf{0},\mathbf{I}),q,k)\) and runs in \(O(m)\) time to produce a hypothesis \(h(\mathbf{x}):=\mathsf{pos}(\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{x})\) s.t. w.p. at least \(1-\delta\) over the sampling, \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq\varepsilon\), for any \(\varepsilon,\delta>0\), when \(m\geq O\left((d/\varepsilon^{2})\log(d/\delta)\right)\)._

The above algorithm is based on estimating the mean of the bag vectors, which unfortunately does not work when \(k=q/2\) or for a general covariance matrix \(\mathbf{\Sigma}\). We instead use a covariance estimation based approach - albeit with a worse running time - for our next result which is proved in Sec. 4. \(\lambda_{\min}\) and \(\lambda_{\max}\) denote the minimum and maximum eigenvalues of the covariance matrix \(\mathbf{\Sigma}\).

**Theorem 1.4**.: _For \(q>2\), \(k\in\{1,\ldots,q-1\}\), \(f(\mathbf{x}):=\mathsf{pos}(\mathbf{r}_{\star}^{\mathsf{T}}\mathbf{x})\), and positive definite \(\mathbf{\Sigma}\) there is an algorithm that samples \(m\) iid bags from \(\mathsf{Ex}(f,N(\mathbf{0},\mathbf{\Sigma}),q,k)\) and runs in \(\mathsf{poly}(m)\) time to produce a hypothesis \(h(\mathbf{x}):=\mathsf{pos}(\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{x})\) s.t. w.p. at least \(1-\delta\) over the sampling_

* _if_ \(k\neq q/2\)_,_ \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq\varepsilon\)_, and_
* _if_ \(k=q/2\)_,_ \(\min\{\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})],\Pr_{\mathcal{D}}[f( \mathbf{x})\neq\tilde{h}(\mathbf{x})]\}\leq\varepsilon\)_, where_ \(\tilde{h}(\mathbf{x}):=\mathsf{pos}(-\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{x})\)__

_for any \(\varepsilon,\delta>0\), when \(m\geq O((d/\varepsilon^{4})\log(d/\delta)(\lambda_{\max}/\lambda_{\min})^{6}q ^{8})\)._

Our general result stated below (proved in Appendix C), extends our algorithmic methods to the case of non-centered Gaussian space and non-homogeneous LTFs.

**Theorem 1.5**.: _For \(q>2\), \(k\in\{1,\ldots,q-1\}\), \(f(\mathbf{x}):=\mathsf{pos}(\mathbf{r}_{\star}^{\mathsf{T}}\mathbf{x}+c_{\star})\), and positive definite \(\mathbf{\Sigma}\) there is an algorithm that samples \(m\) iid bags from \(\mathsf{Ex}(f,N(\mathbf{\mu},\mathbf{\Sigma}),q,k)\) and runs in \(\mathsf{poly}(m)\) time to produce a hypothesis \(h(\mathbf{x}):=\mathsf{pos}(\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{x}+\hat{c})\) s.t. w.p. at least \(1-\delta\) over the sampling_

* _if_ \(k\neq q/2\)_,_ \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq\varepsilon\)_, and_
* _if_ \(k=q/2\)_,_ \(\min\left\{\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})],\Pr_{\mathcal{D}}[ f(\mathbf{x})\neq\tilde{h}(\mathbf{x})]\right\}\leq\varepsilon\)_, and_

_for any \(\varepsilon,\delta>0\), when \(m\geq O\left((d/\varepsilon^{4})\frac{O(\ell^{2})}{(\Phi(\ell)(1-\Phi(\ell)))^{ 2}}\log(d/\delta)\left(\frac{\lambda_{\max}}{\lambda_{\min}}\right)^{4}\left( \frac{\sqrt{\lambda_{\max}}+\|\boldsymbol{\mu}\|_{2}}{\sqrt{\lambda_{\min}}} \right)^{4}q^{8}\right)\) where \(\Phi(.)\) is the standard Gaussian cdf and \(\ell=-\frac{c_{\star}+\mathbf{r}_{\star}^{\mathsf{T}}\boldsymbol{\mu}}{\| \mathbf{\Sigma}^{1/2}\mathbf{r}_{\star}\|_{2}}\)_The value of \(\hat{\mathbf{r}}\) output by our algorithms is a close estimate of \(\mathbf{r}_{*}\) (or possibly \(-\mathbf{r}_{*}\) in the case of balanced bags). Note that our algorithms do not require knowledge of \(\boldsymbol{\mu}\) or \(\boldsymbol{\Sigma}\), and only the derived parameters in Thms. 1.4 and 1.5 are used for the sample complexity bounds. They are based on the certain properties of the empirical mean-vectors and covariance matrices formed by sampling vectors or pairs of vectors from random bags of the bag oracle. An empirical mean based approach has been previously developed by [23] in the LLP setting to estimate the parameters of an exponential generative model, when bag distributions satisfy the so-called class conditioned independence.e., given its label, the feature-vector distribution is same for all the bags. These techniques were extended by [22] to linear classifiers with loss functions satisfying certain smoothness conditions. While the bag oracle in our setup satisfies such conditioned independence, we aim to minimize the instance classification error on which the techniques of [23, 22] are not applicable.

For the case when \(q=1\) (ordinary classification), the sample complexity is \(O(d/\varepsilon\log(d/\delta))\) as one can solve a linear program to obtain an LTF and then use uniform convergence to bound the generalization error. The sample complexity expressions in Theorems 1.3, 1.4 and 1.5 have the same dependence on \(d\) and \(\delta\). However, they have higher powers of \(1/\varepsilon\). They also include other parameters like the bag size (\(q\)), condition number of \(\boldsymbol{\Sigma}\) (\(\lambda_{\max}/\lambda_{\min}\)) and the normalized distance of mean of the Gaussian to the LTF (\(l\)). The origins and significance of these discrepancies are discussed in Sec. 1.4.

### Our Techniques

**Theorem 1.3**: **Case \(N(\mathbf{0},\mathbf{I})\), \(f(\mathbf{x})=\mathsf{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x})\), \(k\neq q/2\).** Assume that \(k>q/2\). A randomly sampled bag with label proportion \(k/q\) has \(k\) vectors iid sampled from the positive side of the separating hyperplane passing through origin, and \((q-k)\) iid sampled from its negative side. It is easy to see that the expected sum of the vectors vanishes in all directions orthogonal to the normal vector \(\mathbf{r}_{*}\), and in the direction of \(\mathbf{r}_{*}\) it has a constant magnitude. The case of \(k<q/2\) is analogous with the direction of the expectation opposite to \(\mathbf{r}_{*}\). Sampling a sufficient number of bags and a random vector from each of them, and taking their normalized expectation (negating if \(k<q/2\)) yields the a close estimate \(\hat{\mathbf{r}}\) of \(\mathbf{r}_{*}\), which in turn implies low classification error. The sample complexity is the same as that for mean estimation bag-vectors (see Section 3) and thus the power of \(1/\varepsilon\) is \(2\).

This simple approach however does not work when \(r=q/2\), in which case the expectation vanishes completely, or for general Gaussian distributions which (even if centered) could be skewed in arbitrary directions. We present our variance based method to handle these cases.

**Theorem 1.4**: **Case \(N(\mathbf{0},\boldsymbol{\Sigma})\), \(f(\mathbf{x})=\mathsf{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x})\).** To convey the main idea of our approach, consider two different ways of sampling two feature-vectors from a random bag of the oracle. The first way is to sample two feature-vectors \(\mathbf{Z}_{1},\mathbf{Z}_{2}\) independently and u.a.r from a random bag. In this case, the probability that they have different labels (given by \(f\)) is \(2k(q-k)/q^{2}\). The second way is to sample a random _pair_\(\tilde{\mathbf{Z}}_{1},\tilde{\mathbf{Z}}_{2}\) of feature-vectors i.e., without replacement. In this case, the probability of different labels is \(2k(q-k)/(q(q-1))\) which is strictly greater than \(2k(q-k)/q^{2}\). Since the labels are given by thresholding in the direction of \(\mathbf{r}_{*}\), this suggests that the variance of \((\tilde{\mathbf{Z}}_{1}-\tilde{\mathbf{Z}}_{2})\) w.r.t. that of \((\mathbf{Z}_{1}-\mathbf{Z}_{2})\) is maximized in the direction of \(\mathbf{r}_{*}\). Indeed, let be the _pair_ covariance matrix and let be the _bag_ covariance matrix. Then we show that \(\pm\mathbf{r}_{*}=\text{argmax}_{\mathbf{r}}\rho(\mathbf{r})\) where \(\rho(\mathbf{r}):=\frac{\mathbf{r}^{\mathsf{T}}\boldsymbol{\Sigma}_{D}\mathbf{ r}}{\mathbf{r}^{\mathsf{T}}\boldsymbol{\Sigma}_{B}\mathbf{r}}\). A simple transformation gives us that

\[\pm\mathbf{r}_{*}=\boldsymbol{\Sigma}_{B}^{-1/2}\mathsf{PrincipalEigenVector}( \boldsymbol{\Sigma}_{B}^{-1/2}\boldsymbol{\Sigma}_{D}\boldsymbol{\Sigma}_{B}^ {-1/2})\] (1)

This suggests the following algorithm: sample enough bags to construct the corresponding empirical estimates \(\hat{\boldsymbol{\Sigma}}_{D}\) and \(\hat{\boldsymbol{\Sigma}}_{B}\) and then compute the empirical proxy of the RHS of (1). We show that using close enough empirical estimates w.h.p the algorithm computes a vector \(\hat{\mathbf{r}}\) s.t. one of \(\pm\hat{\mathbf{r}}\) is close to \(\mathbf{r}_{*}\), and via a geometric stability argument this implies that one of \(\pm\hat{\mathbf{r}}\) yields an LTF that has small instance-level classification error.

At this point, if \(k=q/2\), there is no way to identify the correct solution from \(\pm\hat{\mathbf{r}}\), since a balanced bag, if consistent with an LTF, is also consistent with its complement. On the other hand, if \(k\neq q/2\) we can obtain the correct solution as follows. It is easy to show that since \(f\) is homogeneous and the instance distribution is a centered Gaussian, the measure of \(\{\mathbf{x}\mid f(\mathbf{x})=a\}\) is \(1/2\) for \(a=\{0,1\}\). Thus, one of \(h(\mathbf{x}):=\mathsf{pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{x})\), \(\tilde{h}(\mathbf{x})=\mathsf{pos}(-\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{x})\) will have a high bag satisfaction accuracy. Thus, a large enough sample of bags can be used to identify one of \(h,\tilde{h}\) having a high bag satisfaction accuracy. Lastly, we use a novel generalization error bound (see below) to show that the identified LTF also has a high instance classification accuracy.

The algorithm incurs a sample complexity of \(O\left(\left(q\lambda_{\max}/(\varepsilon\lambda_{\min})\right)^{4}\right)\) to estimate \(\bm{\Sigma}_{D}\) and \(\bm{\Sigma}_{B}\) accurately so that the distance between \(\hat{\mathbf{r}}\) (or \(-\hat{\mathbf{r}}\)) and \(\mathbf{r}_{*}\) is sufficiently bounded. (see Lemmas 3.1 and 4.1). In fact, the distance is required to be \(O\left((\varepsilon/q)\sqrt{\lambda_{\min}/\lambda_{\max}}\right)\) to translate the geometric bound into an \(\varepsilon\)-miscalification error bound, thereby incurring a further factor of \(O\left(q^{4}(\lambda_{\max}/\lambda_{\min})^{2}\right)\) in the sample complexity (see Lemma 2.3). The higher powers in the dependencies are mainly due to the second moment estimates which degrade with larger values of \((\lambda_{\max}/\lambda_{\min})\). Note that the sample complexity explicitly depends on the bag size \(q\) (and not just the label proportion \(k/q\)). This is because the probability of of sampling (without replacement from a bag) a pair of differently labeled feature-vectors is \(2(k/q)(1-k/q)/(1-1/q)\). Keeping \(k/q\) the same, this probability decreases with increasing bag size, thereby increasing the sample complexity for larger bags.

**Theorem 1.5**: **Case \(N(\bm{\mu},\bm{\Sigma})\), \(f(\mathbf{x})=\mathsf{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x}+c_{*})\).** We show that (1) also holds in this case, and therefore we use a similar approach of empirically estimating the pair and bag covariance matrices solving (1) works in principle. However, there are complications, in particular the presence of \(\bm{\mu}\) and \(c_{*}\) degrades the error bounds in the analysis, thus increasing the sample complexity of the algorithm. This is because the measures of \(\{\mathbf{x}\mid f(\mathbf{x})=a\}\) for \(a=\{0,1\}\) could be highly skewed if \(\|\mu\|_{2}\) and/or \(|c_{*}|\) is large. Moreover, the spectral algorithm only gives a solution \(\pm\hat{\mathbf{r}}\) for \(\mathbf{r}_{*}\). An additional step is required to obtain an estimate of \(c_{*}\). This we accomplish using the following procedure which, given a sample of \(s\) bags and any \(\mathbf{r}\) outputs a \(\hat{c}\) which has the following property: if \(s^{*}=\max_{c}\{\text{no. of bags satisfied by }\mathsf{pos}(\mathbf{r}^{\mathsf{T}} \mathbf{x}+c)\}\), then \(\hat{c}\) will satisfy at least \(s^{*}-1\) bags. This is done by ordering the values \(\mathbf{r}^{\mathsf{T}}\mathbf{x}\) of the vectors \(\mathbf{x}\) within each bag in decreasing order, and then constructing set of the \(k\)th values of each bag. Out of these \(s\) values, the one which taken as \(c\) in \(\mathsf{pos}(\mathbf{r}^{\mathsf{T}}\mathbf{x}+c)\) satisfies the most bags, is chosen to be \(\hat{c}\).

Due to the non-homogeneity of the Gaussian distribution and the LTF \(f\), the application of Lemma 2.3 instead incurs an \(O\left(q^{4}((\sqrt{\lambda_{\max}}+\|\bm{\mu}\|_{2})/\sqrt{\lambda_{\min}})^{ 4}\right)\) factor in the sample complexity, while a factor of \(O(\ell^{2})\) is towards the estimation of \(\bm{\Sigma}_{B}\) and \(\bm{\Sigma}_{D}\). Note that \(\ell\) is the distance from \(\bm{\mu}\) to the hyperplane of \(f\) normalized by the stretch induced by \(\bm{\Sigma}\), and thus a larger value of \(\ell\) implies a lower density near the hyperplane leading to an increased sample complexity. Lastly, a further blowup by \(1/(\Phi(\ell)(1-\Phi(\ell)))^{2}\) comes from bounding the sample error from geometric bound between \(\hat{\mathbf{r}}\) and \(\mathbf{r}_{*}\), and is required for a sufficiently accurate approximation of \(c_{*}\).

**Generalization Error Bounds.** We prove (Thm. 2.2) bounds on the generalization of the error of a hypothesis LTF \(h\) in satisfying sampled bags to its distributional instance-level error. Using this, we are able to distinguish (for \(k\neq q/2\)) between the two possible solutions our principal component algorithm yields - the one which satisfies more of the sampled bags has w.h.p. low instance-level error. For proving these bounds, the first step is to use a bag-level generalization error bound shown by [26] using the techniques of [35]. Next, we show that low distributional bag satisfaction error by \(h\) implies low instance level error. This involves a fairly combinatorial analysis of two independent binomial random variables formed from the incorrectly classified labels within a random bag. Essentially, unless \(h\) closely aligns with \(f\) at the instance level, with significant probability there will be an imbalance in these two random variables leading to \(h\) not satisfying the bag.

**Subgaussian concentration bounds.** The standard estimation bounds for Gaussians are not directly applicable in our case, since the random vector sampled from a random bag is biased according to its label given by \(f\), and is therefore not a Gaussian vector. To obtain sample complexity bounds linear in \(\log(1/\delta)\) we use subgaussian concentration bounds for mean and covariance estimation ([32, 31]). For this, we show \(O(\ell)\) bound on the expectation and subgaussian norm of the thresholded Gaussian given by \(\{g\sim N(0,1)\mid\ g>\ell\}\) for some \(\ell>0\). The random vectors of interest to us are (in a transformed space) distributed as a combination of thresholded Gaussians in one of the coordinates, and \(N(0,1)\) in the rest. We show that they satisfy the \(O(\ell)\) bound on their subgaussian norm and admit the corresponding subgaussian Hoeffding (for empirical mean) and empirical covariance concentration bounds. Based on this, in Sec. 3 we abstract out the procedure used in our learning algorithms for obtaining the relevant mean and covariance estimates.

**Experiments.** We include in Sec. 6 an experimental evaluation of our learning algorithms along with a comparison of with those of [25, 26] and random LTFs, demonstrating the effectiveness of our techniques.

## 2 Preliminaries

We begin with some useful linear algebraic notions. Let \(\lambda_{\max}(\mathbf{A})\) and \(\lambda_{\min}(\mathbf{A})\) denote the maximum and minimum eigenvalue of a real symmetric matrix \(\mathbf{A}\). The _operator norm_\(\|\mathbf{A}\|_{2}:=\max_{\|\mathbf{x}\|_{2}=1}\|\mathbf{Ax}\|_{2}\) for such matrices is given by \(\lambda_{\max}(\mathbf{A})\).

We shall restrict our attention to symmetric _positive definite_ (p.d.) matrices \(\mathbf{A}\) which satisfy \(\mathbf{x}^{\mathsf{T}}\mathbf{Ax}>0\) for all non-zero vectors \(\mathbf{x}\), implying that \(\lambda_{\min}(\mathbf{A})>0\) and \(\mathbf{A}^{-1}\) exists and is symmetric p.d. as well. Further, for such matrices \(\mathbf{A}\), \(\mathbf{A}^{1/2}\) is well defined to be the unique symmetric p.d. matrix \(\mathbf{B}\) satisfying \(\mathbf{BB}=\mathbf{A}\). The eigenvalues of \(\mathbf{A}^{1/2}\) are the square-roots of those of \(\mathbf{A}\). We have the following lemma which is proved in Appendix B.6.

**Lemma 2.1**.: _Let \(\mathbf{A}\) and \(\mathbf{B}\) be symmetric p.d. matrices such that \(\|\mathbf{A}-\mathbf{B}\|\leq\varepsilon_{1}\|\mathbf{A}\|_{2}\). Let \(\mathbf{r}_{1},\mathbf{r}_{2}\in\mathbb{R}^{d}\) be two unit vectors such that \(\|\mathbf{r}_{1}-\mathbf{r}_{2}\|_{2}\leq\varepsilon_{2}\). Then, \(\left\|\frac{\mathbf{Ar}_{1}}{\|\mathbf{Ar}_{1}\|_{2}}-\frac{\mathbf{Br}_{2}} {\|\mathbf{Br}_{2}\|_{2}}\right\|_{2}\leq 4\frac{\lambda_{\min}(\mathbf{A})}{ \lambda_{\min}(\mathbf{A})}(\varepsilon_{2}+\varepsilon_{1})\) when \(\frac{\lambda_{\max}(\mathbf{A})}{\lambda_{\min}(\mathbf{A})}(\varepsilon_{2}+ \varepsilon_{1})\leq\frac{1}{2}\)._

**Bag Oracle and related statistics.** Let \(\mathcal{O}:=\mathsf{Ex}(f,\mathcal{D},q,k)\) be any bag oracle with \(k\in\{1,\ldots,q-1\}\) for an LTF \(f(\mathbf{x}):=\mathbf{r}_{\ast}^{\mathsf{T}}\mathbf{x}+c_{\ast}\) in \(d\)-dimensions, and let \(\mathcal{M}\) be a collection of \(m\) bags sampled iid from the oracle. Define for any hypothesis LTF \(h\),

\[\mathsf{BagErr}_{\mathsf{ oracle}}(h,f,\mathcal{D},q,k):=\Pr_{B\gets \mathcal{O}}\left[\text{Avg}\{h(\mathbf{x})\;\left|\;\mathbf{x}\in B\right. \right\}\neq k/q\right],\text{ and,}\] (2) \[\mathsf{BagErr}_{\mathsf{ sample}}(h,\mathcal{M}):=\left|\{B\in\mathcal{M}\;\left|\;\text{Avg}\{h( \mathbf{x})\;\left|\;\mathbf{x}\in B\right.\right\}\neq k/q\}\right|/m.\] (3)

We define the following statistical quantities related to \(\mathcal{O}\). Let \(\mathbf{X}\) be a random feature-vector sampled uniformly from a random bag sampled from \(\mathcal{O}\). Let,

\[\boldsymbol{\mu}_{B}:=\mathbb{E}[\mathbf{X}]\qquad\text{ and,}\qquad\boldsymbol{\Sigma}_{B}:=\mathbb{E}\left[\left(\mathbf{X}- \boldsymbol{\mu}_{B}\right)\left(\mathbf{X}-\boldsymbol{\mu}_{B}\right)^{ \mathsf{T}}\right]=\text{Var}[\mathbf{X}].\] (4)

Now, let \(\mathbf{Z}=\mathbf{X}_{1}-\mathbf{X}_{2}\) where \((\mathbf{X}_{1},\mathbf{X}_{2})\) are a random pair of feature-vectors sampled (without replacement) from a random bag sampled from \(\mathcal{O}\). Clearly \(\mathbb{E}[\mathbf{Z}]=\mathbf{0}\). Define

\[\boldsymbol{\Sigma}_{D}:=\mathbb{E}\left[\mathbf{Z}\mathbf{Z}^{\mathsf{T}} \right]=\text{Var}[\mathbf{Z}].\] (5)

**Generalization and stability bounds.** We prove in Appendix D.1 the following generalization bound from bag classification error to instance classification error.

**Theorem 2.2**.: _For any \(\varepsilon<1/4q\) if \(\mathsf{BagErr}_{\mathsf{ sample}}(h,\mathcal{M})\leq\varepsilon\) then,_

(i) _if \(k\neq q/2\), \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq 4\varepsilon\), and_

(ii) _if \(k=q/2\), \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq 4\varepsilon\) or \(\Pr[f(\mathbf{x})\neq(1-h(\mathbf{x}))]\leq 4\varepsilon\), w.p. \(1-\delta\), when \(m\geq C_{0}d\left(\log q+\log(1/\delta)\right)/\varepsilon^{2}\), for any \(\delta>0\) and absolute constant \(C_{0}>0\)._

In some cases we directly obtain geometric bounds on the hypothesis classifier and the following lemma (proved in Appendix D.2) allows us to straightaway bound the classification error.

**Lemma 2.3**.: _Suppose \(\|\mathbf{r}-\hat{\mathbf{r}}\|_{2}\leq\varepsilon_{1}\) for unit vectors \(\mathbf{r},\hat{\mathbf{r}}\). Then, \(\Pr\left[\text{pos}\left(\mathbf{r}^{T}\mathbf{X}+c\right)\neq\text{pos}\left( \hat{\mathbf{r}}^{T}\mathbf{X}+c\right)\right]\leq\varepsilon\) where \(\varepsilon=\varepsilon_{1}(c_{0}\sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}}+c_ {1}\|\boldsymbol{\mu}\|_{2}/\sqrt{\lambda_{\text{min}}})\) for some absolute constants \(c_{0},c_{1}>0\) and \(\lambda_{\text{max}}\),\(\lambda_{\text{min}}\) are the maximum and minimum eigenvalues of \(\boldsymbol{\Sigma}\) respectively._

## 3 Bag distribution statistics estimation

We provide the following estimator for \(\boldsymbol{\mu}_{B},\boldsymbol{\Sigma}_{B}\) and \(\boldsymbol{\Sigma}_{D}\) defined in (4) and (5). We have the following lemma - which follows from the subgaussian distribution based mean and covariance concentration bounds shown for thresholded Gaussians (see Appendix E) - whose proof is given in Appendix E.3.

**Lemma 3.1**.: _If \(m\geq O\left((d/\varepsilon^{2})O(\ell^{2})\log(d/\delta)\right)\) where \(\ell\) is as given in Lemma E.13 then Algorithm 1 returns \(\tilde{\boldsymbol{\mu}}_{B},\hat{\boldsymbol{\Sigma}}_{B},\hat{\boldsymbol{ \Sigma}}_{D}\) such that \(\|\tilde{\boldsymbol{\mu}}_{B}-\boldsymbol{\mu}_{B}\|_{2}\leq\varepsilon\sqrt{ \lambda_{\text{max}}}/2\), \(\|\hat{\boldsymbol{\Sigma}}_{B}-\boldsymbol{\Sigma}_{B}\|_{2}\leq\varepsilon\lambda_ {\text{max}}\), and \(\|\hat{\boldsymbol{\Sigma}}_{D}-\boldsymbol{\Sigma}_{D}\|_{2}\leq\varepsilon \lambda_{\text{max}}\), w.p. at least \(1-\delta\), for any \(\varepsilon,\delta>0\). Here \(\lambda_{\text{max}}\) is the maximum eigenvalue of \(\boldsymbol{\Sigma}\)._

## 4 Proof of Theorem 1.4

For the setting of Theorem 1.4, we provide Algorithm 2. It uses as a subroutine a polynomial time procedure \(\mathsf{PrincipalEigenVector}\) for the principal eigen-vector of a symmetric matrix, and first computes two LTFs given by a normal vector and its negation, returning the one that has lower error on a sampled collection of bags.

```
0:\(\mathcal{O}=\mathsf{Ex}(f,\mathcal{D}=N(\mathbf{0},\mathbf{\Sigma}),q,k),m,s\), where \(f(\mathbf{x})=\mathsf{pos}\left(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x}\right)\), \(\|\mathbf{r}_{*}\|_{2}=1\).
1. Compute \(\hat{\mathbf{\Sigma}}_{B},\hat{\mathbf{\Sigma}}_{D}\) using \(\mathsf{MeanCosEstimator}\) with \(m\) samples.
2. \(\bar{\mathbf{r}}=\hat{\mathbf{\Sigma}}_{B}^{-1/2}\mathsf{PrincipalEigenVector}( \hat{\mathbf{\Sigma}}_{B}^{-1/2}\hat{\mathbf{\Sigma}}_{D}\hat{\mathbf{\Sigma}} _{B}^{-1/2})\) if \(\hat{\mathbf{\Sigma}}_{B}^{-1/2}\) exists, else exit.
3. Let \(\hat{\mathbf{r}}=\bar{\mathbf{r}}/\|\mathbf{r}\|_{2}\). 4. If \(k=q/2\)return:\(h=\mathsf{pos}\left(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{X}\right)\), else
4. Let \(\hat{h}=\mathsf{pos}\left(-\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{X}\right)\).
5. Sample a collection \(\mathcal{M}\) of \(s\) bags from \(\mathcal{O}\).
6. Return\(h^{*}\in\{h,\tilde{h}\}\) which has lower \(\mathsf{BagErr}_{\text{sample}}(h^{*},\mathcal{M})\). ```

**Algorithm 2** PAC Learner for no-offset LTFs over \(N(\mathbf{0},\mathbf{\Sigma})\)

**Lemma 4.1**.: _For any \(\varepsilon,\delta\in(0,1)\), if \(m\geq O((d/\varepsilon^{4})\log(d/\delta)(\lambda_{\text{max}}/\lambda_{ \text{min}})^{4}q^{4})\), then \(\hat{\mathbf{r}}\) computed in Step 3 of Alg. 2 satisfies \(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{* }\|_{2}\}\leq\varepsilon\), w.p. \(1-\delta/2\)._

The above, whose proof is deferred to Sec. 4.1, is used in conjunction with the following lemma.

**Lemma 4.2**.: _Let \(k\neq q/2\), \(\varepsilon,\delta\in(0,1)\) and suppose \(\hat{\mathbf{r}}\) computed in Step 3 of Alg. 2 satisfies \(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{* }\|_{2}\}\leq\varepsilon\), Then, with \(s\geq O\left(d(\log q+\log(1/\delta))/\varepsilon^{2}\right)\), \(h^{*}\) in Step. 3.c satisfies \(\Pr_{\mathcal{D}}\left[h^{*}(\mathbf{x})\neq f(\mathbf{x})\right]\leq 16c_{0}q \varepsilon\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}\) w.p. \(1-\delta/2\), where constant \(c_{0}>0\) is from Lem. 2.3._

With the above we complete the proof of Theorem 1.4 as follows.

Proof.: (of Theorem 1.4) Let the parameters \(\delta,\varepsilon\) be as given in the statement of the theorem.

For \(k=q/2\), we use \(O(\varepsilon\sqrt{\lambda_{\text{min}}/\lambda_{\text{max}}})\) for the error bound in Lemma 4.1 thereby taking \(m=O((d/\varepsilon^{4})\log(d/\delta)(\lambda_{\text{max}}/\lambda_{\text{min} })^{6}q^{4})\) in Alg. 2, so that Lemma 4.1 along with Lemma 2.3 yields the desired misclassification error bound of \(\varepsilon\) for one of \(h\), \(\tilde{h}\).

For \(k\neq q/2\), we use \(O(\varepsilon\sqrt{\lambda_{\text{min}}/\lambda_{\text{max}}}/q)\) for the error bound in Lemma 4.1. Taking \(m=O((d/\varepsilon^{4})\log(d/\delta)(\lambda_{\text{max}}/\lambda_{\text{min} })^{6}q^{8})\) in Alg. 2 we obtain the following bound: \(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{* }\|_{2}\}\leq\varepsilon\sqrt{\lambda_{\text{min}}/\lambda_{\text{max}}}/(16c_{ 0}q)\) with probability \(1-\delta/2\). Using \(s\geq O\left(d(\log q+\log(1/\delta))q^{2}\frac{\lambda_{\text{max}}}{ \varepsilon^{2}\lambda_{\text{min}}}\right)\), Lemma 4.2 yields the desired misclassification error bound of \(\varepsilon\) on \(h^{*}\) w.p. \(1-\delta\). 

Proof.: (of Lemma 4.2) Applying Lemma 2.3 we obtain that at least one of \(h\), \(\tilde{h}\) has an instance misclassification error of at most \(O(\varepsilon\sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}})\). WLOG assume that \(h\) satisfies this error bound i.e., \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq c_{0}\varepsilon\sqrt{ \lambda_{\text{max}}/\lambda_{\text{min}}}\eqeqcolon\varepsilon^{\prime}\). Since the separating hyperplane of the LTFpasses through the origin, and \(\mathcal{D}=N(\mathbf{0},\mathbf{\Sigma})\) is centered, \(\Pr_{\mathcal{D}}[f(\mathbf{x})=1]=\Pr_{\mathcal{D}}[f(\mathbf{x})=0]=1/2\). Thus,

\[\Pr_{\mathcal{D}}[h(x)\neq f(x)\;\mid\;f(\mathbf{x})=1],\Pr_{\mathcal{D}}[[h(x) \neq f(x)\;\mid\;f(\mathbf{x})=0]\leq 2\varepsilon^{\prime}.\]

Therefore, the probability that a random bag from the oracle contains a feature vector on which \(f\) and \(h\) disagree is at most \(2q\varepsilon^{\prime}\). Applying the Chernoff bound (see Appendix B.1) we obtain that with probability at least \(1-\delta/6\), \(\mathsf{BagErr}_{\text{sample}}(h,\mathcal{M})\leq 4q\varepsilon^{\prime}\). Therefore, in Step 3.c. \(h^{*}\) satisfies \(\mathsf{BagErr}_{\text{sample}}(h^{*},\mathcal{M})\leq 4q\varepsilon^{\prime}\)

On the other hand, applying Theorem 2.2, except with probability \(\delta/3\), \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h^{*}(\mathbf{x})]\leq 16q\varepsilon^{\prime}= 16c_{0}q\varepsilon\sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}}\). Therefore, except with probability \(\delta/2\), the bound in Lemma 4.2 holds. 

### Proof of Lemma 4.1

We define and bound a few useful quantities depending on \(k,q,\lambda_{\text{min}}\) and \(\lambda_{\text{max}}\) using \(1\leq k\leq q-1\).

**Definition 4.3**.: _Define, (i) \(\kappa_{1}:=\left(\frac{2k}{q}-1\right)^{2}\frac{2}{\pi}\) so that \(0\leq\kappa_{1}\leq 2/\pi\), (ii) \(\kappa_{2}:=\frac{1}{q-1}\frac{k}{q}\left(1-\frac{k}{q}\right)\frac{16}{\pi}\) so that \(\frac{16}{\pi q^{2}}\leq\kappa_{2}\leq\frac{4}{\pi(q-1)}\), (iii) \(\kappa_{3}:=\frac{\kappa_{2}}{1-\kappa_{1}}\) so that \(\frac{16}{\pi q^{2}}\leq\kappa_{3}\leq\frac{4}{(\pi-2)(q-1)}\), and (iv) \(\theta:=\frac{2\lambda_{\text{max}}}{\lambda_{\text{min}}}\left(\frac{1}{2- \max(0,2\kappa_{1}-\kappa_{2})}+\frac{1}{1-\kappa_{1}}\right)\) so that \(\frac{3\lambda_{\text{min}}}{\lambda_{\text{min}}}\leq\theta\leq\frac{3 \lambda_{\text{max}}}{(1-2/\pi)\lambda_{\text{min}}}\)._

For the analysis we begin by showing in the following lemma that \(\hat{\mathbf{r}}\) in the algorithms is indeed \(\pm\mathbf{r}_{*}\) if the covariance estimates were the actual covariances.

**Lemma 4.4**.: _The ratio \(\rho(\mathbf{r}):=\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{D}\mathbf{r}/\mathbf{ r}^{\mathsf{T}}\mathbf{\Sigma}_{B}\mathbf{r}\) is maximized when \(\mathbf{r}=\pm\mathbf{r}_{*}\). Moreover,_

\[\rho(\mathbf{r})=2+\frac{\gamma(\mathbf{r})^{2}\kappa_{2}}{1-\gamma(\mathbf{r} )^{2}\kappa_{1}}\qquad\text{ where }\qquad\gamma(\mathbf{r}):=\frac{\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma} \mathbf{r}_{*}}{\sqrt{\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}\mathbf{r}}\sqrt{ \mathbf{r}^{\mathsf{T}}_{*}\mathbf{\Sigma}\mathbf{r}_{*}}}\text{ and }\]

\[\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{B}\mathbf{r}=\mathbf{r}^{\mathsf{T}} \mathbf{\Sigma}\mathbf{r}(1-\gamma(\mathbf{r})^{2}\kappa_{1}),\quad\mathbf{r }^{\mathsf{T}}\mathbf{\Sigma}_{D}\mathbf{r}=\mathbf{r}^{\mathsf{T}}\mathbf{ \Sigma}\mathbf{r}(2-\gamma(\mathbf{r})^{2}(2\kappa_{1}-\kappa_{2}))\]

Proof.: Let \(\mathbf{\Gamma}:=\mathbf{\Sigma}^{1/2}\), then \(\mathbf{X}\sim N(\mathbf{0},\mathbf{\Sigma})\Leftrightarrow\mathbf{X}=\mathbf{ \Gamma}\mathbf{Z}\) where \(\mathbf{Z}\sim N(\mathbf{0},\mathbf{I})\). Further, \(\mathsf{pos}\left(\mathbf{r}^{\mathsf{T}}\mathbf{X}\right)=\mathsf{pos}\left( \mathbf{u}^{\mathsf{T}}\mathbf{Z}\right)\) where \(\mathbf{u}=\mathbf{\Gamma}\mathbf{r}/\|\mathbf{\Gamma}\mathbf{r}\|_{2}\). Using this, we can let \(\mathbf{X}_{B}=\mathbf{\Gamma}\mathbf{Z}_{B}\) as a random feature-vector sampled uniformly from a random bag sampled from \(\mathcal{O}\). Also, let \(\mathbf{X}_{D}=\mathbf{\Gamma}\mathbf{Z}_{D}\) be the difference of two random feature vectors sampled uniformly without replacement from a random bag sampled from \(\mathcal{O}\). Observe that the ratio \(\rho(\mathbf{r})=\operatorname{Var}[\mathbf{r}^{\mathsf{T}}\mathbf{X}_{D}]/ \operatorname{Var}[\mathbf{r}^{\mathsf{T}}\mathbf{X}_{B}]=\operatorname{Var}[ \mathbf{u}^{\mathsf{T}}\mathbf{Z}_{D}]/\operatorname{Var}[\mathbf{u}^{\mathsf{ T}}\mathbf{Z}_{B}]\).

Let \(\mathbf{u}_{*}:=\mathbf{\Gamma}\mathbf{r}_{*}/\|\mathbf{\Gamma}\mathbf{r}_{*}\|_{2}\), and \(g^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}\) which is \(N(0,1)\). For \(a\in\{0,1\}\), let \(\mathbf{Z}_{a}\) be \(\mathbf{Z}\) conditioned on \(\mathsf{pos}\left(\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}\right)=a\). Let \(g_{a}^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}_{a}\), \(a\in\{0,1\}\), be the half normal distributions satisfying \(\mathbb{E}[(g_{a}^{*})^{2}]=1\) and \(\mathbb{E}[g_{a}^{*}]=(-1)^{1-a}\sqrt{2/\pi}\). With this setup, letting \(g_{B}^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}_{B}\) and \(g_{D}^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}_{D}\) we obtain (using Lemma B.2 in Appendix B.2)

\[\operatorname{Var}[g_{B}^{*}]=1-\kappa_{1},\quad\operatorname{Var}[g_{D}^{*}]=2( 1-\kappa_{1})+\kappa_{2}\]

Now let \(\tilde{\mathbf{u}}\) be a unit vector orthogonal to \(\mathbf{u}_{*}\). Let \(\tilde{g}=\tilde{\mathbf{u}}^{\mathsf{T}}\mathbf{Z}\) be \(N(0,1)\). Also, let \(\tilde{g}_{a}=\tilde{\mathbf{u}}^{\mathsf{T}}\mathbf{Z}_{a}\) for \(a\in\{0,1\}\). Since \(\mathbf{Z}_{a}\) are given by conditioning \(\mathbf{Z}\) only along \(\mathbf{u}_{*}\), \(\tilde{g}_{a}\sim N(0,1)\) for \(a\in\{0,1\}\). In particular, the component along \(\tilde{u}\) of \(\mathbf{Z}_{B}\) (call it \(\tilde{g}_{B}\)) is \(N(0,1)\) and that of \(\mathbf{Z}_{D}\) (call it \(\tilde{g}_{D}\)) is the difference of two iid \(N(0,1)\) variables. Thus, \(\operatorname{Var}[\tilde{g}_{B}]=1\) and \(\operatorname{Var}[\tilde{g}_{D}]=2\). Moreover, due to orthogonality all these gaussian variables corresponding to \(\tilde{\mathbf{u}}\) are independent of those corresponding to \(\mathbf{u}_{*}\) defined earlier. Now let \(\mathbf{u}=\alpha\mathbf{u}_{*}+\beta\tilde{\mathbf{u}}\), where \(\beta=\sqrt{1-\alpha^{2}}\) be any unit vector. From the above we have,

\[\frac{\operatorname{Var}\left[\mathbf{u}^{\mathsf{T}}\mathbf{Z}_{D} \right]}{\operatorname{Var}\left[\mathbf{u}^{\mathsf{T}}\mathbf{Z}_{B}\right]}= \frac{\operatorname{Var}\left[\alpha g_{D}^{*}+\beta\tilde{g}_{D}\right]}{ \operatorname{Var}\left[\alpha g_{B}^{*}+\beta\tilde{g}_{B}\right]}=\frac{\alpha^ {2}\operatorname{Var}\left[g_{D}^{*}\right]+\beta^{2}\operatorname{Var}\left[ \tilde{g}_{D}\right]}{\alpha^{2}\operatorname{Var}\left[g_{B}^{*}\right]+\beta^{2} \operatorname{Var}\left[\tilde{g}_{B}\right]}= \frac{2\alpha^{2}(1-\kappa_{1})+\alpha^{2}\kappa_{2}+2\beta^{2}}{\alpha^{2}(1- \kappa_{1})+\beta^{2}}\] \[= 2+\frac{\alpha^{2}\kappa_{2}}{1-\alpha^{2}\kappa_{1}}\] (6)

where the last equality uses \(\beta=\sqrt{1-\alpha^{2}}\). Letting \(\mathbf{u}=\mathbf{\Gamma}\mathbf{r}/\|\mathbf{\Gamma}\mathbf{r}\|_{2}\) we obtain that \(\alpha=\frac{\langle\mathbf{\Gamma}\mathbf{r},\mathbf

**Lemma 4.5**.: \(\underset{\|\mathbf{r}\|_{2}=1}{\text{argmax}}\rho(\mathbf{r})=\mathbf{\Sigma}_{B }^{-1/2}\mathsf{PrincipalEigenVector}(\mathbf{\Sigma}_{B}^{-1/2}\mathbf{\Sigma}_{ D}\mathbf{\Sigma}_{B}^{-1/2})\)__

Proof.: This follows directly from its generalization in Appendix B.5. 

We now complete the proof of Lemma 4.1 (with \(\delta\) instead of \(\delta/2\) for convenience). By Lemma 3.1, taking \(m\geq O\left((d/\varepsilon_{1}^{2})\log(d/\delta)\right)\) ensures that \(\|\mathbf{E}_{B}\|_{2}\leq\varepsilon_{1}\lambda_{\text{max}}\) and \(\|\mathbf{E}_{D}\|_{2}\leq\varepsilon_{1}\lambda_{\text{max}}\) w.p. at least \(1-\delta\) where \(\mathbf{E}_{B}=\mathbf{\tilde{\Sigma}}_{B}-\mathbf{\Sigma}_{B}\) and \(\mathbf{E}_{D}=\mathbf{\hat{\Sigma}}_{D}-\mathbf{\Sigma}_{D}\). We start by defining \(\rho(\mathbf{r}):=\frac{\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{B}\mathbf{r}} {\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{D}\mathbf{r}}\) which is the equivalent of \(\rho\) using the estimated matrices. Observe that it can be written as \(\rho(\mathbf{r})=\frac{\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{B}\mathbf{r}+ \mathbf{r}^{\mathsf{T}}\mathbf{E}_{B}\mathbf{r}}{\mathbf{r}^{\mathsf{T}} \mathbf{\Sigma}_{D}\mathbf{r}+\mathbf{r}^{\mathsf{T}}\mathbf{E}_{D}\mathbf{r}}\). Using these we can obtain the following bound on \(\hat{\rho}\): for any \(\mathbf{r}\in\mathbb{R}^{d}\), \(|\hat{\rho}(\mathbf{r})-\rho(\mathbf{r})|\leq\theta\varepsilon_{1}|\rho( \mathbf{r})|\) w.p. at least \(1-\delta\) (*) as long as \(\varepsilon_{1}\leq\frac{(1-\kappa_{1})}{2}\frac{\lambda_{\text{min}}}{\lambda_ {\text{max}}}\), which we shall ensure (see Appendix B.4).

For convenience we denote the normalized projection of any vector \(\mathbf{r}\) as \(\tilde{\mathbf{r}}:=\frac{\mathbf{\Sigma}^{1/2}\mathbf{r}}{\|\mathbf{\Sigma}^ {1/2}\mathbf{r}\|_{2}}\). Now let \(\tilde{\mathbf{r}}\in\mathbb{R}^{d}\) be a unit vector such that \(\min\{\|\tilde{\mathbf{r}}-\tilde{\mathbf{r}}_{*}\|_{2},\|\tilde{\mathbf{r}}+ \tilde{\mathbf{r}}_{*}\|_{2}\}\geq\varepsilon_{2}\). Hence, using the definitions from Lemma 4.4, \(|\gamma(\mathbf{r})|\leq 1-\varepsilon_{2}^{2}/2\) while \(\gamma(\mathbf{r}_{*})=1\) which implies \(\rho(\mathbf{r}_{*})-\rho(\mathbf{r})\geq\kappa_{3}\varepsilon_{2}^{2}/2\). Note that \(\rho(\mathbf{r})\leq\rho(\mathbf{r}_{*})=2+\kappa_{3}\). Choosing \(\varepsilon_{1}<\frac{\kappa_{3}}{4\theta(2+\kappa_{3})}\varepsilon_{2}^{2}\), we obtain that \(\rho(\mathbf{r}_{*})(1-\theta\varepsilon_{1})>\rho(\mathbf{r})(1+\theta \varepsilon_{1})\). Using this along with the bound (*) we obtain that w.p. at least \(1-\delta\), \(\hat{\rho}(\mathbf{r}_{*})>\hat{\rho}(\mathbf{r})\) when \(\varepsilon_{2}>0\). Since our algorithm returns \(\tilde{\mathbf{r}}\) as the maximizer of \(\hat{\rho}\), w.p. at least \(1-\delta\) we get \(\min\{\|\tilde{\mathbf{r}}-\tilde{\mathbf{r}}_{*}\|_{2},\|\tilde{\mathbf{r}}+ \tilde{\mathbf{r}}_{*}\|_{2}\}\leq\varepsilon_{2}\). Using Lemma 2.1, \(\min\{\|\tilde{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\tilde{\mathbf{r}}+\mathbf{ r}_{*}\|_{2}\}\leq 4\sqrt{\frac{\lambda_{\text{min}}}{\lambda_{\text{min}}}} \varepsilon_{2}\). Substituting \(\varepsilon_{2}=\frac{\varepsilon}{4}\sqrt{\frac{\lambda_{\text{min}}}{\lambda_ {\text{max}}}}\), \(\|\mathbf{r}-\mathbf{r}_{*}\|_{2}\leq\varepsilon\) w.p. at least \(1-\delta\). The conditions on \(\varepsilon_{1}\) are satisfied by taking it to be \(\leq O\left(\frac{\kappa_{3}\varepsilon^{2}\lambda_{\text{min}}}{\theta(2+ \kappa_{3})\lambda_{\text{max}}}\right)\), and thus we can take \(m\geq O\left((d/\varepsilon^{4})\log(d/\delta)\left(\frac{\lambda_{\text{max}} }{\lambda_{\text{min}}}\right)^{2}\theta^{2}\left(\frac{2+\kappa_{3}}{\kappa_{3 }}\right)^{2}\right)=O\left((d/\varepsilon^{4})\log(d/\delta)\left(\frac{\lambda _{\text{max}}}{\lambda_{\text{min}}}\right)^{4}q^{4}\right)\), using Defn. 4.3. This completes the proof.

## 5 Proof Sketches for Theorems 1.3 and 1.5

**Theorem 1.3**.: **Case \(N(\mathbf{0},\mathbf{I})\), \(f(\mathbf{x})=\mathsf{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x})\), \(k\neq q/2\).** The algorithm (Algorithm 3) and the proof is in Appendix A. We argue that a vector sampled uniformly at random from a bag is distributed as \(\omega\mathbf{X}_{1}+(1-\omega)\mathbf{X}_{0}\) where \(\mathbf{X}_{a}\sim N(\mathbf{0},\mathbf{I})\) conditioned on \(f(\mathbf{X}_{a})=a\) and \(\omega\) is an independent \(\{0,1\}-\)Bernoulli r.v. s.t. \(p(\omega=1)=k/q\). This along with the fact that uncorrelated Gaussians are independent, allows us to show that the expectation is \(\mathbf{0}\) in any direction orthogonal to \(\mathbf{r}_{*}\) and to compute the expectation in the direction of \(\mathbf{r}_{*}\). We then use Lemma 3.1 to get the sample complexity expression.

**Theorem 1.5**.: **Case \(N(\mathbf{\mu},\mathbf{\Sigma})\), \(f(\mathbf{x})=\mathsf{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x}+c_{*})\).** The algorithm (Algorithm 4) and the detailed proof is given in Appendix C. We start by generalizing the high probability geometric error bound in Lemma 4.1 to this case (Lemma C.1 proven in Appendix C.1) and appropriately generalize \(\kappa_{1},\kappa_{2},\kappa_{3}\) and \(\theta\). The rest of the proof is similar to Section 4.1. An extra factor of \(O(\ell^{2})\) is introduced to the sample complexity from Lemma 3.1. Next, assuming the geometric bound, we give a high probability bound on the generalization error in Lemma C.2. In this analysis, we bound the \(\mathsf{BagErr}_{\text{sample}}(h,\mathcal{M})\) where \(h(\mathbf{x})=\mathsf{pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{x}+c_{*})\) with \(\hat{\mathbf{r}}\) being the geometric estimate of \(\mathbf{r}_{*}\) and \(\mathcal{M}\) is a sample of bags. This introducing the dependencies on \((\sqrt{\lambda_{\max}}+\|\mathbf{\mu}\|_{2})/\sqrt{\lambda_{\min}}\) as well as on \(\Phi(\ell)\). Our subroutine to find \(\hat{c}\) ensures that \(h^{*}(\mathbf{x})=\mathsf{pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{x}+\hat{c})\) satisfies \(\mathsf{BagErr}_{\text{sample}}(h^{*},\mathcal{M})\geq\mathsf{BagErr}_{ \text{sample}}(h,\mathcal{M})\). We then use Theorem 2.2 to bound the generalization error of \(h^{*}\). Lemmas C.1 C.2 together imply Theorem 1.5.

## 6 Experimental Results

General Gaussian.We empirically evaluate our algorithmic technique on centered and general Gaussian distributions for learning homogeneous LTFs. For homogeneous LTFs the general case algorithm (Alg. 4 in Appendix C) boils down to Alg. 2 in Sec. 4. The experimental LLP datasets are created using samples from both balanced as well as unbalanced bag oracles. In particular, for dimension \(d\in\{10,50\}\), and each pair \((q,k)\in\{(2,1),(3,1),(10,5),(10,8),(50,25),(50,35)\}\)and \(m=\) we create 25 datasets as follows: for each dataset (i) sample a random unit vector \(\mathbf{r}^{*}\) and let \(f(\mathbf{x}):=\mathsf{pos}\left(\mathbf{r}^{*\mathsf{T}}\mathbf{x}\right)\), (ii) sample \(\boldsymbol{\mu}\) and \(\boldsymbol{\Sigma}\) randomly (see Appendix F for details), (iii) sample \(m=2000\) training bags from \(\mathsf{Ex}(f,N(\boldsymbol{\mu},\boldsymbol{\Sigma}),q,k)\), (iv) sample 1000 test instances \((\mathbf{x},f(\mathbf{x}))\), \(\mathbf{x}\gets N(\boldsymbol{\mu},\boldsymbol{\Sigma})\). We fix \(\boldsymbol{\mu}=\mathbf{0}\) for the centered Gaussian case.

For comparison we include the random LTF algorithm in which we sample 100 random LTFs and return the one that satisfies the most bags. In addition, we evaluate the Algorithm of [25] on \((q,k)=(2,1)\), and the Algorithm of [26] on \((q,k)=(3,1)\). We measure the accuracy of each method on the test set of each dataset. The algorithms of [25, 26] are considerably slower and we use 200 training bags for them. The results for centered Gaussian are in Table 0(a) and for the general Gaussian are in Table 0(b). We observe that our algorithms perform significantly better in terms of accuracy than the comparative methods in all the bag distribution settings. Further, our algorithms have significantly lower error bounds (see Appendix F).

Notice in Tables 0(a) and 0(b) that the test accuracy for Algorithm 2 decreases with an increase in \(q\) and \(d\). This is consistent with the sample complexity expressions in Thm. 1.4 and Thm. 1.5. Also, notice that the test accuracy for Algorithm 2 for general Gaussian (Table 0(b)) is usually lesser than the same for centered Gaussian (Table 0(a)). This supports the theoretical result that the sample complexity increases with the increase in \(l\).

Appendix F has additional details and further experiments for the \(N(\mathbf{0},\mathbf{I})\) with homogeneous LTFs, \(N(\boldsymbol{\mu},\boldsymbol{\Sigma})\) with non-homogeneous LTFs as well as on noisy label distributions.

## 7 Conclusion and Future work

Our work shows that LTFs can be efficiently properly learnt in the LLP setting from random bags with given label proportion whose feature-vectors are sampled independently from a Gaussian space, conditioned on their underlying labels. For the simple case of \(N(\mathbf{0},\mathbf{I})\) distribution and bags with unbalanced labels we provide a mean estimation based algorithm. For the general scenarios we develop a more sophisticated approach using the principal component of a matrix formed from certain covariance matrices. To resolve the ambiguity between the obtained solutions we employ novel generalization error bounds from bag satisfaction to instance classification. We also show that subgaussian concentration bounds are applicable on the thresholded Gaussians, yielding efficient sample complexity bounds. Our experimental results validate the performance guarantees of our algorithmic techniques.

In future work, classes of distributions other than Gaussian could be similarly investigated. Classifiers other than LTFs are also interesting to study in the LLP setting.

## References

* [1] P. Awasthi, M. F. Balcan, and P. M. Long. The power of localization for efficiently learning linear separators with noise. _J. ACM_, 63(6):50:1-50:27, 2017.

\begin{table}
\begin{tabular}{r r r r r r} \hline \hline \(d\) & \(q\) & \(k\) & A2 & R & \(\boldsymbol{\mathrm{S}}\) \\

[MISSING_PAGE_POST]

 \hline \hline \end{tabular}
\end{table}
Table 1: Algorithm A2 vs. rand. LTF (R) vs SDP algorithms (S)* Bhattacharyya et al. [2018] A. Bhattacharyya, S. Ghoshal, and R. Saket. Hardness of learning noisy halfspaces using polynomial thresholds. In _Proc. COLT_, volume 75, pages 876-917. PMLR, 2018. URL http://proceedings.mlr.press/v75/bhattacharyya18a.html.
* Blumer et al. [1989] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the vapnik-chervonenkis dimension. _J. ACM_, 36(4):929-965, 1989.
* Busa-Fekete et al. [2023] Robert Istvan Busa-Fekete, Heejin Choi, Travis Dick, Claudio Gentile, and Andres Munoz medina. Easy learning from label proportions. _arXiv_, 2023. URL https://arxiv.org/abs/2302.03115.
* Chen et al. [2004] L. Chen, Z. Huang, and R. Ramakrishnan. Cost-based labeling of groups of mass spectra. In _Proc. ACM SIGMOD International Conference on Management of Data_, pages 167-178, 2004.
* Chen et al. [2023] Lin Chen, Thomas Fu, Amin Karbasi, and Vahab Mirrokni. Learning from aggregated data: Curated bags versus random bags. _arXiv_, 2023. URL https://arxiv.org/abs/2305.09557.
* Daniely [2015] Amit Daniely. A PTAS for agnostically learning halfspaces. In _Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015_, volume 40 of _JMLR Workshop and Conference Proceedings_, pages 484-502, 2015.
* Dasgupta [1999] S. Dasgupta. Learning mixtures of Gaussians. In _FOCS_, pages 634-644, 1999.
* de Freitas and Kuck [2005] N. de Freitas and H. Kuck. Learning about individuals from group statistics. In _Proc. UAI_, pages 332-339, 2005.
* Dery et al. [2017] L. M. Dery, B. Nachman, F. Rubbo, and A. Schwartzman. Weakly supervised classification in high energy physics. _Journal of High Energy Physics_, 2017(5):1-11, 2017.
* Dulac-Arnold et al. [2019] G. Dulac-Arnold, N. Zeghidour, M. Cuturi, L. Beyer, and J. P. Vert. Deep multi-class learning from label proportions. _CoRR_, abs/1905.12909, 2019. URL http://arxiv.org/abs/1905.12909.
* Feldman et al. [2009] V. Feldman, P. Gopalan, S. Khot, and A. K. Ponnuswami. On agnostic learning of parities, monomials, and halfspaces. _SIAM J. Comput._, 39(2):606-645, 2009.
* Fish and Reyzin [2020] Benjamin Fish and Lev Reyzin. On the complexity of learning a class ratio from unlabeled data. _J. Artif. Intell. Res._, 69:1333-1349, 2020.
* Guruswami and Raghavendra [2006] V. Guruswami and P. Raghavendra. Hardness of learning halfspaces with noise. In _Proc. FOCS_, pages 543-552, 2006.
* Hernandez-Gonzalez et al. [2013] J. Hernandez-Gonzalez, I. Inza, and J. A. Lozano. Learning bayesian network classifiers from label proportions. _Pattern Recognit._, 46(12):3425-3440, 2013.
* Kalai et al. [2005] A. T. Kalai, A. R. Klivans, Y. Mansour, and R. A. Servedio. Agnostically learning halfspaces. In _46th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2005), 23-25 October 2005, Pittsburgh, PA, USA, Proceedings_, pages 11-20. IEEE Computer Society, 2005.
* Klivans et al. [2009] A. R. Klivans, P. M. Long, and R. A. Servedio. Learning halfspaces with malicious noise. _J. Mach. Learn. Res._, 10:2715-2740, 2009.
* Kotzias et al. [2015] D. Kotzias, M. Denil, N. de Freitas, and P. Smyth. From group to individual labels using deep features. In _Proc. SIGKDD_, pages 597-606, 2015.
* Liu et al. [2019] J. Liu, B. Wang, Z. Qi, Y. Tian, and Y. Shi. Learning from label proportions with generative adversarial networks. In _Proc. NeurIPS_, pages 7167-7177, 2019.
* Musicant et al. [2007] D. R. Musicant, J. M. Christensen, and J. F. Olson. Supervised learning by training on aggregate outputs. In _Proc. ICDM_, pages 252-261. IEEE Computer Society, 2007.
* Nandy et al. [2022] J. Nandy, R. Saket, P. Jain, J. Chauhan, B. Ravindran, and A. Raghuveer. Domain-agnostic contrastive representations for learning from label proportions. In _Proc. CIKM_, pages 1542-1551, 2022.

* Patrini et al. [2014] G. Patrini, R. Nock, T. S. Caetano, and P. Rivera. (almost) no label no cry. In _Proc. Advances in Neural Information Processing Systems_, pages 190-198, 2014.
* Quadrianto et al. [2009] N. Quadrianto, A. J. Smola, T. S. Caetano, and Q. V. Le. Estimating labels from label proportions. _J. Mach. Learn. Res._, 10:2349-2374, 2009.
* Rueping [2010] S. Rueping. SVM classifier estimation from group probabilities. In _Proc. ICML_, pages 911-918, 2010.
* Saket [2021] R. Saket. Learnability of linear thresholds from label proportions. In _Proc. NeurIPS_, 2021. URL https://openreview.net/forum?id=5BnaKeEwuYk.
* Saket [2022] R. Saket. Algorithms and hardness for learning linear thresholds from label proportions. In _Proc. NeurIPS_, 2022. URL https://openreview.net/forum?id=4LZo68TuF-4.
* Saket et al. [2022] Rishi Saket, Aravindan Raghuveer, and Balaraman Ravindran. On combining bags to better learn from label proportions. In _AISTATS_, volume 151 of _Proceedings of Machine Learning Research_, pages 5913-5927. PMLR, 2022. URL https://proceedings.mlr.press/v151/saket22a.html.
* Scott and Zhang [2020] C. Scott and J. Zhang. Learning from label proportions: A mutual contamination framework. In _Proc. NeurIPS_, 2020.
* Valiant [1984] Leslie G. Valiant. A theory of the learnable. _Commun. ACM_, 27(11):1134-1142, 1984.
* Vempala [2010] S. Vempala. Learning convex concepts from gaussian distributions with PCA. In _FOCS_, pages 124-130, 2010.
* Vershynin [2012] R. Vershynin. How close is the sample covariance matrix to the actual covariance matrix? _J. Theor. Probab._, 25:655-686, 2012.
* Vershynin [2018] Roman Vershynin. _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2018. doi: 10.1017/9781108231596.
* Wojtusiak et al. [2011] J. Wojtusiak, K. Irvin, A. Birerdinc, and A. V. Baranova. Using published medical results and non-homogenous data in rule learning. In _Proc. International Conference on Machine Learning and Applications and Workshops_, volume 2, pages 84-89. IEEE, 2011.
* Yu et al. [2013] F. X. Yu, D. Liu, S. Kumar, T. Jebara, and S. F. Chang. \(\propto\)SVM for learning with label proportions. In _Proc. ICML_, volume 28, pages 504-512, 2013.
* Yu et al. [2014] F. X. Yu, K. Choromanski, S. Kumar, T. Jebara, and S. F. Chang. On learning from label proportions. _CoRR_, abs/1402.5902, 2014. URL http://arxiv.org/abs/1402.5902.

Proof of Theorem 1.3

For the setting of Theorem 1.3 we provide Algorithm 3.

``` \(\mathsf{Ex}(f,\mathcal{D},q,k),m\), where \(f(\mathbf{x})=\mathsf{pos}\left(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x}\right)\), \(\|\mathbf{r}_{*}\|_{2}=1\), \(k\neq q/2\).
1. Compute \(\hat{\boldsymbol{\mu}}_{B}\) using \(\mathsf{MeanCosEstimator}\) with \(m\) samples.
2. If \(k>q/2\)Return:\(\hat{\boldsymbol{\mu}}_{B}/\|\hat{\boldsymbol{\mu}}_{B}\|_{2}\) else Return:\(-\hat{\boldsymbol{\mu}}_{B}/\|\hat{\boldsymbol{\mu}}_{B}\|_{2}\) ```

**Algorithm 3** PAC Learner for homogenous LTFs from unbalanced bags over \(N(\mathbf{0},\mathbf{I})\)

Define \(\mathbf{X}_{a}\sim N(\mathbf{0},\mathbf{I})\) conditioned on \(f(\mathbf{X}_{a})=a\) for \(a\in\{0,1\}\), and let \(\mathbf{X}_{B}\) denotes the random feature-vector u.a.r sampled from a randomly sampled bag. By the definition of the bag oracle, \(\mathbf{X}_{B}:=\omega\mathbf{X}_{1}+(1-\omega)\mathbf{X}_{0}\), where \(\omega\) is an independent \(\{0,1\}\)-Bernoulli r.v. s.t. \(p(\omega=1)=k/q\). Let \(g^{*}:=\mathbf{r}_{*}^{\mathsf{T}}\mathbf{X}_{B}\). \(g^{*}\sim N(0,1)\) and let \(g_{a}^{*}=\mathbf{r}_{*}^{\mathsf{T}}\mathbf{X}_{a}\), \(a\in\{0,1\}\). Since \(\mathbf{r}_{*}\) is a unit vector, \(g_{a}^{*}\) is a half-gaussian and by direct integration we obtain \(\mathbb{E}\left[g_{a}^{*}\right]=(-1)^{1-a}\sqrt{2/\pi}\) for \(a\in\{0,1\}\). Thus,

\[\mathbb{E}[g^{*}]=\frac{k}{q}\mathbb{E}[g_{1}^{*}]+\left(1-\frac{k}{q}\right) \mathbb{E}[g_{0}^{*}]=\eta(q,k):=\left(\frac{2k}{q}-1\right)\sqrt{\frac{2}{ \pi}},0\leq\eta(q,k)\leq 1\]

On the other hand, let \(g^{\perp}=\mathbf{r}^{\mathsf{T}}\mathbf{X}\) s.t. \(\mathbf{r}^{\mathsf{T}}\mathbf{r}_{*}=0\) and \(\|\mathbf{r}\|_{2}=1\), and let \(g_{a}^{\perp}=\mathbf{r}^{\mathsf{T}}\mathbf{X}_{a}\), \(a\in\{0,1\}\). Since \(\mathbf{X}_{a}\) (\(a\in\{0,1\}\)) is given by conditioning a standard Gaussian vector only on the component in the direction of \(\mathbf{r}_{*}\), its component along any direction orthogonal to \(\mathbf{r}_{*}\) is a one-dimensional standard Gaussian. Therefore, \(g_{a}^{\perp}\) are iid \(N(0,1)\) (\(a\in\{0,1\}\)), and so is \(g^{\perp}\), implying \(\mathbb{E}\left[g^{\perp}\right]=0\). Thus, the value of \(\mathbf{r}^{\mathsf{T}}\mathbb{E}\left[\mathbf{X}_{B}\right]\) is (i) \(\eta(q,k)\) if \(\mathbf{r}=\mathbf{r}_{*}\), and (ii) \(0\) if \(\mathbf{r}\perp\mathbf{r}_{*}\). In other words, \(\boldsymbol{\mu}_{B}=\mathbb{E}\left[\mathbf{X}_{B}\right]=\eta(q,k)\mathbf{r }_{*}\), and \(\|\boldsymbol{\mu}_{B}\|_{2}=|\eta(q,k)|\). Hence, if \(\eta(q,k)>0\) then \(\mathbf{r}_{*}=\boldsymbol{\mu}_{B}/\|\boldsymbol{\mu}_{B}\|_{2}\) else \(\mathbf{r}_{*}=-\boldsymbol{\mu}_{B}/\|\boldsymbol{\mu}_{B}\|_{2}\) ```

With \(m=O\left(\eta(q,k)^{2}(d/\varepsilon^{2})\log(d/\delta)\right)=O\left((d/ \varepsilon^{2})\log(d/\delta)\right)\), the following lemma along with Lemma 2.3 completes the proof of Theorem 1.3.

**Lemma A.1**.: _Algorithm 3 returns a normal vector \(\hat{\mathbf{r}}\) such that \(\|\mathbf{r}_{*}-\hat{\mathbf{r}}\|_{2}\leq\varepsilon\) w.p. at least \(1-\delta\) when \(m\geq O\left((d/\varepsilon^{2})\log(d/\delta)\right)\) for \(\varepsilon,\delta>0\)._

Proof.: First, we can assume \(\varepsilon\leq 2\) since the distance between two unit vectors is at most \(2\) and therefore the lemma is trivially true for \(\varepsilon>2\). By Lemma 3.1, taking \(m\geq=O\left((d/\varepsilon^{2})\log(d/\delta)\right)=O\left(\eta(q,k)^{2}(d/ \varepsilon^{2})\log(d/\delta)\right)\) ensures \(\|\hat{\boldsymbol{\mu}}_{B}-\boldsymbol{\mu}_{B}\|_{2}\leq\varepsilon\| \boldsymbol{\mu}_{B}\|/4=\varepsilon\|\boldsymbol{\mu}_{B}\|_{2}/4\) w.p. \(1-\delta\). Therefore, by triangle inequality, \(\|\hat{\boldsymbol{\mu}}_{B}\|_{2}-\|\boldsymbol{\mu}_{B}\|_{2}\leq\varepsilon\| \boldsymbol{\mu}_{B}\|_{2}/4\Rightarrow\|\hat{\boldsymbol{\mu}}_{B}\|_{2}/\| \boldsymbol{\mu}_{B}\|_{2}\in[1-\frac{\varepsilon}{4},1+\frac{\varepsilon}{4}]\). Now, \(\mathbf{r}_{*}=\mathsf{sign}(\eta(q,k))\boldsymbol{\mu}_{B}/\|\boldsymbol{\mu}_{ B}\|_{2}\) and the algorithm returns \(\hat{\mathbf{r}}:=\mathsf{sign}(\eta(q,k))\hat{\boldsymbol{\mu}}_{B}/\|\hat{ \boldsymbol{\mu}}_{B}\|_{2}\).

\[\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2}=\left\|\frac{\hat{ \boldsymbol{\mu}}_{B}}{\|\hat{\boldsymbol{\mu}}_{B}\|_{2}}-\frac{\boldsymbol{\mu} _{B}}{\|\boldsymbol{\mu}_{B}\|_{2}}\right\|_{2} \leq\frac{\|\hat{\boldsymbol{\mu}}_{B}\|\boldsymbol{\mu}_{B}\|_{2}- \boldsymbol{\mu}_{B}\|\boldsymbol{\mu}_{B}\|_{2}+\boldsymbol{\mu}_{B}\| \boldsymbol{\mu}_{B}\|_{2}-\boldsymbol{\mu}_{B}\|\hat{\boldsymbol{\mu}}_{B}\|_{2} \big{\|}_{2}}{\|\hat{\boldsymbol{\mu}}_{B}\|_{2}\|\boldsymbol{\mu}_{B}\|_{2}}\] \[\leq\frac{\|\hat{\boldsymbol{\mu}}_{B}-\boldsymbol{\mu}_{B}\|_{2} \|\boldsymbol{\mu}_{B}\|_{2}+\|\boldsymbol{\mu}_{B}\|_{2}\|\boldsymbol{\mu}_{B} \|_{2}-\|\hat{\boldsymbol{\mu}}_{B}\|_{2}}{\|\hat{\boldsymbol{\mu}}_{B}\|_{2}\| \boldsymbol{\mu}_{B}\|_{2}}\] \[\leq\frac{\frac{\varepsilon}{2}\|\boldsymbol{\mu}_{B}\|_{2}}{\| \boldsymbol{\mu}_{B}\|_{2}-\frac{\varepsilon}{4}\|\boldsymbol{\mu}_{B}\|_{2}} \leq\frac{2\varepsilon}{4-\varepsilon}\leq\varepsilon\ \ \ \text{for}\ \varepsilon\leq 2.\]

## Appendix B Useful Tools

### Chernoff Bound

We state the well known Chernoff Bound.

**Theorem B.1**.: _Let \(X_{1},\ldots,X_{n}\) be iid \(\{0,1\}\)-valued random variables. Let \(S\) be their sum and \(\mu=\mathbb{E}[S]\). Then, for any \(\delta>0\),_

\[\Pr\left[S\geq(1+\delta)\mu\right]\leq\exp(-\delta^{2}\mu/(2+\delta)).\]

### Relationship between \(\bm{\Sigma}_{B}\) and \(\bm{\Sigma}_{D}\)

**Lemma B.2**.: _If \(\bm{\Sigma}_{B},\bm{\Sigma}_{D}\) are as defined in the preliminaries and \(\mathbf{X}_{a}:=\mathbf{X}|f(\mathbf{X})=a\), then_

\[\bm{\Sigma}_{D}=2\bm{\Sigma}_{B}+\frac{2}{q-1}\left(\frac{k}{q}\right)\left(1- \frac{k}{q}\right)(\mathbb{E}[\mathbf{X}_{1}]-\mathbb{E}[\mathbf{X}_{0}])( \mathbb{E}[\mathbf{X}_{1}]-\mathbb{E}[\mathbf{X}_{0}])^{T}\]

Proof.: Let \(\mathbf{X}_{B}\) be a random-feature vector sampled uniformly from a random bag sampled from \(\mathcal{O}\). Hence, with probability \(k/q\) it is sampled from \(\mathbf{X}_{0}\). Hence,

\[\bm{\mu}_{B}=\mathbb{E}[\mathbf{X}_{B}]=\frac{k}{q}\mathbb{E}[\mathbf{X}_{1}] +\left(1-\frac{k}{q}\right)\mathbb{E}[\mathbf{X}_{0}]\]

\[\bm{\Sigma}_{B}=\mathbb{E}[\mathbf{X}_{B}\mathbf{X}_{B}^{T}]-\mathbb{E}[ \mathbf{X}_{B}]\mathbb{E}[\mathbf{X}_{B}]^{T}\text{ where }\mathbb{E}[ \mathbf{X}\mathbf{X}^{T}]=\frac{k}{q}\mathbb{E}[\mathbf{X}_{1}\mathbf{X}_{1}^ {T}]+\left(1-\frac{k}{q}\right)\mathbb{E}[\mathbf{X}_{0}\mathbf{X}_{0}^{T}]\]

Let \(\mathbf{X}_{D}=\mathbf{X}-\mathbf{X}^{\prime}\) where \((\mathbf{X},\mathbf{X}^{\prime})\) are a random pair of feature-vectors sampled (without replacement) from a random bag sampled from \(\mathcal{O}\). Hence, with probability \(\binom{k}{2}/\binom{q}{2}\) it is the difference of two vectors sampled independently from \(\mathbf{X}_{1}\), with probability \(\binom{q-k}{2}/\binom{q}{2}\) it is the difference of two vectors sampled independently from \(\mathbf{X}_{0}\), with probability \(k(q-k)/2\binom{q}{2}\), it is the difference of one vector sampled from \(X_{1}\) and another sampled independently from \(X_{0}\) and with probability \(k(q-k)/2\binom{q}{2}\), it is the difference of one vector sampled from \(X_{0}\) and another sampled independently from \(X_{1}\). Then,

\[\bm{\Sigma}_{D}=\mathbb{E}[\mathbf{X}_{D}\mathbf{X}_{D}^{T}] =\frac{1}{\binom{q}{2}}\bigg{[}\binom{k}{2}\mathbb{E}[(\mathbf{X }_{1}-\mathbf{X}_{1}^{\prime})(\mathbf{X}_{1}-\mathbf{X}_{1}^{\prime})^{T}]+ \binom{q-k}{2}\mathbb{E}[(\mathbf{X}_{0}-\mathbf{X}_{0}^{\prime})(\mathbf{X} _{0}-\mathbf{X}_{0}^{\prime})^{T}]\] \[+\frac{k(q-k)}{2}\mathbb{E}[(\mathbf{X}_{1}-\mathbf{X}_{0}^{ \prime})(\mathbf{X}_{1}-\mathbf{X}_{0}^{\prime})^{T}]+\frac{k(q-k)}{2}\mathbb{ E}[(\mathbf{X}_{0}-\mathbf{X}_{1}^{\prime})(\mathbf{X}_{0}-\mathbf{X}_{1}^{ \prime})^{T}]\bigg{]}\]

Due to independence, we obtain for \(a\in\{0,1\}\)

\[\mathbb{E}[(\mathbf{X}_{a}-\mathbf{X}_{a}^{\prime})(\mathbf{X}_{ a}-\mathbf{X}_{a}^{\prime})^{T}]=2\mathbb{E}[\mathbf{X}_{a}\mathbf{X}_{a}^{T}]-2 \mathbb{E}[\mathbf{X}_{a}]\mathbb{E}[\mathbf{X}_{a}]^{T}\] \[\mathbb{E}[(\mathbf{X}_{a}-\mathbf{X}_{1-a}^{\prime})(\mathbf{X} _{a}-\mathbf{X}_{1-a}^{\prime})^{T}]=\mathbb{E}[\mathbf{X}_{1}\mathbf{X}_{1}^ {T}]+\mathbb{E}[\mathbf{X}_{0}\mathbf{X}_{0}^{T}]-\mathbb{E}[\mathbf{X}_{1}] \mathbb{E}[\mathbf{X}_{0}]^{T}-\mathbb{E}[\mathbf{X}_{0}]\mathbb{E}[\mathbf{X}_ {1}]^{T}\]

Hence,

\[\bm{\Sigma}_{D} =\frac{1}{\binom{q}{2}}\bigg{[}\left(2\binom{k}{2}+k(q-k)\right) \mathbb{E}[\mathbf{X}_{1}\mathbf{X}_{1}^{T}]+\left(2\binom{q-k}{2}+k(q-k) \right)\mathbb{E}[\mathbf{X}_{0}\mathbf{X}_{0}^{T}]\] \[-\left(k(k-1)\mathbb{E}[\mathbf{X}_{1}]\mathbb{E}[\mathbf{X}_{ 1}]^{T}+(q-k)(q-k-1)\mathbb{E}[\mathbf{X}_{0}]\mathbb{E}[\mathbf{X}_{0}]^{T}\right)\] \[+\left(k(q-k)\left(\mathbb{E}[\mathbf{X}_{1}]\mathbb{E}[\mathbf{ X}_{0}]^{T}+\mathbb{E}[\mathbf{X}_{0}]\mathbb{E}[\mathbf{X}_{1}]^{T}\right) \right)\bigg{]}\]

Simplifying \(\bm{\Sigma}_{D}-2\bm{\Sigma}_{B}\), we get

\[\bm{\Sigma}_{D}-2\bm{\Sigma}_{B}=\frac{2}{q-1}\left(\frac{k}{q}\right)\left(1- \frac{k}{q}\right)\left[(\mathbb{E}[\mathbf{X}_{1}]-\mathbb{E}[\mathbf{X}_{0} ])(\mathbb{E}[\mathbf{X}_{1}]-\mathbb{E}[\mathbf{X}_{0}])^{T}\right]\]

### Bound on \(\gamma(\mathbf{r})\) when \(\mathbf{r}^{T}\bm{\Sigma}_{B}\mathbf{r}_{*}=0\)

**Lemma B.3**.: _If \(\mathbf{r}^{T}\bm{\Sigma}_{B}\mathbf{r}_{*}=0\) then \(\gamma(\mathbf{r})\leq 1-\left(\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}} \right)^{2}\frac{1-\max(0,\kappa_{1}(q,k,\ell))}{1-\min(0,\kappa_{1}(q,k, \ell))}\). Further if \(\ell=0\), then \(|\gamma(\mathbf{r})|\leq 1-\left(\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}} \right)^{2}(1-\kappa_{1}(q,k))\)._

Proof.: We begin by observing the for any \(\mathbf{r}\in\mathbb{R}^{d},\|\bm{\Sigma}^{1/2}\mathbf{r}\|_{2}=1\), \(\text{Var}[\mathbf{r}^{T}\mathbf{Z}_{B}]=1-\kappa_{1}(q,k,\ell)\). Hence, \(\lambda_{\text{min}}(\bm{\Sigma}_{B})\geq(1-\max(0,\kappa_{1}(q,k,\ell)))\lambda_ {\text{min}}\) and \(\lambda_{\text{max}}(\bm{\Sigma}_{B})\leq(1-\min(0,\kappa_{1}(q,k,\ell)))\lambda_ {\text{max}}\).

Thus, by Cautchy-Schwartz inequality,

\[\|\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\|_{2}\leq\sqrt{1-\min(0, \kappa_{1}(q,k,\ell))}\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}\] \[\|\bm{\Sigma}^{1/2}\bm{\Sigma}_{B}^{-1/2}\|_{2}\leq\frac{1}{\sqrt{ 1-\max(0,\kappa_{1}(q,k,\ell))}}\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{ \text{min}}}}\]

Define \(\tilde{\mathbf{r}}=\frac{\bm{\Sigma}^{1/2}\mathbf{r}}{\|\bm{\Sigma}^{1/2} \mathbf{r}\|_{2}}\) for any \(\mathbf{r}\in\mathbb{R}^{d}\). Observe that \(\gamma(\mathbf{r})=\tilde{\mathbf{r}}^{T}\tilde{\mathbf{r}}_{*}\). Now since \(\mathbf{r}^{T}\bm{\Sigma}_{B}\mathbf{r}_{*}=0\), by substitution \((\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r}})^{T}(\bm{\Sigma}_{ B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r}}_{*})=0\). Thus, using Cautchy-Schwartz again

\[\|\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r}}-\bm{ \Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r}}_{*}\|_{2} =\sqrt{\|\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r} }\|_{2}^{2}+\|\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r}}_{*}\|_ {2}^{2}}\] \[\geq\frac{\sqrt{\|\tilde{\mathbf{r}}\|_{2}^{2}+\|\tilde{\mathbf{ r}}_{*}\|_{2}^{2}}}{\|\bm{\Sigma}^{1/2}\bm{\Sigma}_{B}^{-1/2}\|_{2}}\geq\sqrt{2(1- \max(0,\kappa_{1}(q,k,\ell)))}\sqrt{\frac{\lambda_{\text{min}}}{\lambda_{\text {max}}}}\]

Again using Cautchy-Schwartz,

\[\|\tilde{\mathbf{r}}-\tilde{\mathbf{r}}_{*}\|_{2} \geq\frac{\|\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{ \mathbf{r}}-\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\tilde{\mathbf{r}}_{*}\|_ {2}}{\|\bm{\Sigma}_{B}^{1/2}\bm{\Sigma}^{-1/2}\|_{2}}\] \[\geq\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}}\sqrt{\frac {2(1-\max(0,\kappa_{1}(q,k,\ell)))}{1-\min(0,\kappa_{1}(q,k,\ell))}}\]

Thus,

\[\gamma(\mathbf{r})=\tilde{\mathbf{r}}^{T}\tilde{\mathbf{r}}_{*} \leq 1-\left(\frac{\lambda_{\text{min}}}{\lambda_{\text{max}}} \right)^{2}\frac{1-\max(0,\kappa_{1}(q,k,\ell))}{1-\min(0,\kappa_{1}(q,k,\ell ))}\]

If \(\ell=0\), then \(\kappa_{1}(q,k,0)=\kappa_{1}(q,k)\geq 0\). Hence,

\[\gamma(\mathbf{r})\leq 1-\left(\frac{\lambda_{\text{min}}}{\lambda_{\text{ max}}}\right)^{2}(1-\kappa_{1}(q,k))\]

### Bounding error in \(\hat{\rho}\)

**Lemma B.4**.: _If \(\|\bm{E}_{B}\|_{2}\leq\varepsilon\lambda_{\text{max}}\) and \(\|\bm{E}_{D}\|_{2}\leq\varepsilon\lambda_{\text{max}}\) where \(\bm{E}_{B}=\hat{\bm{\Sigma}}_{B}-\hat{\bm{\Sigma}}_{B}\) and \(\bm{E}_{D}=\hat{\bm{\Sigma}}_{D}-\hat{\bm{\Sigma}}_{D}\) then,_

\[|\hat{\rho}(\mathbf{r})-\rho(\mathbf{r})|\leq|\rho(\mathbf{r})|\theta\varepsilon\]

_for \(\theta\) as defined in Defn. 4.3 or Defn. C.3._

Proof.: Let \(a(\mathbf{r}):=\mathbf{r}^{\mathsf{T}}\bm{\Sigma}_{D}\mathbf{r},b(\mathbf{r}):= \mathbf{r}^{\mathsf{T}}\bm{\Sigma}_{B}\mathbf{r},e(\mathbf{r}):=\mathbf{r}^{ \mathsf{T}}\bm{E}_{D}\mathbf{r},f(\mathbf{r}):=\mathbf{r}^{\mathsf{T}}\bm{E}_{ B}\mathbf{r}\). Using the bounds on the spectral norms of \(\bm{E}_{B}\) and \(\bm{E}_{D}\), we get that \(|e(\mathbf{r})|\leq\varepsilon\lambda_{\text{max}}\) and \(|f(\mathbf{r})|\leq\varepsilon\lambda_{\text{max}}\). Also, using variances in Lemma C.4, \(a(\mathbf{r})\geq\lambda_{\text{min}}(2-\max(0,2\kappa_{1}-\kappa_{2}))\geq 0\) and \(b(\mathbf{r})\geq\lambda_{\text{min}}(1-\max(0,\kappa_{1}))\geq 0\). Hence, we can conclude that

\[\left|\frac{e(\mathbf{r})}{a(\mathbf{r})}\right|\leq\varepsilon\frac{\lambda_{ \text{max}}}{\lambda_{\text{min}}}\left(\frac{1}{2-\max(0,2\kappa_{1}-\kappa_{2 })}\right)\text{ and }\left|\frac{f(\mathbf{r})}{b(\mathbf{r})}\right|\leq\varepsilon\frac{\lambda_ {\text{max}}}{\lambda_{\text{min}}}\left(\frac{1}{1-\max(0,\kappa_{1})}\right)\]

Observe that \(\hat{\rho}(\mathbf{r})/\rho(\mathbf{r})=\frac{1+\epsilon(\mathbf{r})/a(\mathbf{ r})}{1+b(\mathbf{r})/f(\mathbf{r})}\). Hence,

\[\frac{1-\varepsilon\frac{\lambda_{\text{min}}}{\lambda_{\text{min}}}\left(\frac{1 }{2-\max(0,2\kappa_{1}-\kappa_{2})}\right)}{1+\varepsilon\frac{\lambda_{\text{ min}}}{\lambda_{\text{min}}}\left(\frac{1}{1-\max(0,\kappa_{1})}\right)}\leq\frac{\hat{ \rho}(\mathbf{r})}{\rho(\mathbf{r})}\leq\frac{1+\varepsilon\frac{\lambda_{\text{ max}}}{\lambda_{\text{min}}}\left(\frac{1}{2-\max(0,2\kappa_{1}-\kappa_{2})} \right)}{1-\varepsilon\frac{\lambda_{\text{min}}^{\text{max}}}{\lambda_{\text{ min}}}\left(\frac{1}{1-\max(0,\kappa_{1})}\right)}\]

Now, whenever \(\varepsilon\leq\frac{1-\max(0,\kappa_{1})}{2}\frac{\lambda_{\text{min}}}{\lambda_{ \text{max}}}\), \(1-\varepsilon\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}\left(\frac{1}{1-\max(0, \kappa_{1})}\right)\geq 1/2\) and,

\(1+\varepsilon\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}\left(\frac{1}{1- \max(0,\kappa_{1})}\right)\leq 3/2\). Thus, we obtain that

\[\left|\frac{\hat{\rho}(\mathbf{r})-\rho(\mathbf{r})}{\rho(\mathbf{r})}\right| \leq\theta\varepsilon\]

where \(\theta\) is as defined in Defn. C.3. If we substitute \(\ell=0\), we get that \(\kappa_{1}\geq 0\) and we get \(\theta\) as defined in Defn. 4.3.

### Ratio maximisation as a PCA problem

**Theorem B.5**.: _If \(\mathbf{A}\) and \(\mathbf{B}\) are positive definite matrices, then for all \(\mathbf{r}\in\mathbb{R}^{d}\)_

1. \(\frac{\mathbf{r}^{\mathsf{T}}\mathbf{A}\mathbf{r}}{\mathbf{r}^{\mathsf{T}} \mathbf{B}\mathbf{r}}=\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{B}^{-1/2}\mathbf{ A}\mathbf{B}^{-1/2}\tilde{\mathbf{r}}\)_,_ \(\tilde{\mathbf{r}}=\frac{\mathbf{B}^{1/2}\mathbf{r}}{\|\mathbf{B}^{1/2} \mathbf{r}\|_{2}}\)__
2. \(\underset{\|\mathbf{r}\|_{2}=1}{\operatorname{argmax}}\frac{\mathbf{r}^{ \mathsf{T}}\mathbf{A}\mathbf{r}}{\mathbf{r}^{\mathsf{T}}\mathbf{B}\mathbf{r}}= \frac{\mathbf{B}^{-1/2}\mathbf{r}^{*}}{\|\mathbf{B}^{-1/2}\tilde{\mathbf{r}}^{ *}\|_{2}}\) _where_ \(\tilde{\mathbf{r}}^{*}=\underset{\|\tilde{\mathbf{r}}\|_{2}=1}{\operatorname {argmax}}\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{B}^{-1/2}\mathbf{A}\mathbf{B}^{ -1/2}\tilde{\mathbf{r}}\)__

Proof.: The first statement comes from the substitution. Now, \(\frac{\mathbf{r}^{\mathsf{T}}\mathbf{A}\mathbf{r}}{\mathbf{r}^{\mathsf{T}} \mathbf{B}\mathbf{r}}\) is homogeneous in \(\mathbf{r}\). Let \(a\simeq b\Rightarrow a=kb,k\in\mathbb{R}\)

\[\underset{\|\mathbf{r}\|_{2}=1}{\operatorname{argmax}}\frac{ \mathbf{r}^{\mathsf{T}}\mathbf{A}\mathbf{r}}{\mathbf{r}^{\mathsf{T}}\mathbf{B} \mathbf{r}} \simeq\underset{\mathbf{r}}{\operatorname{argmax}}\frac{\mathbf{r}^{ \mathsf{T}}\mathbf{A}\mathbf{r}}{\mathbf{r}^{\mathsf{T}}\mathbf{B}\mathbf{r}}\] \[\simeq\underset{\mathbf{B}^{-1/2}\mathbf{r}}{\operatorname{argmax}} \frac{\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{B}^{-1/2}\mathbf{A}\mathbf{B}^{-1/ 2}\tilde{\mathbf{r}}}{\|\tilde{\mathbf{r}}\|_{2}^{2}}\] \[\simeq\mathbf{B}^{-1/2}\underset{\|\tilde{\mathbf{r}}\|_{2}=1}{ \operatorname{argmax}}\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{B}^{-1/2} \mathbf{A}\mathbf{B}^{-1/2}\tilde{\mathbf{r}}\]

Hence, \(\underset{\|\mathbf{r}\|_{2}=1}{\operatorname{argmax}}\frac{\mathbf{r}^{ \mathsf{T}}\mathbf{A}\mathbf{r}}{\mathbf{r}^{\mathsf{T}}\mathbf{B}\mathbf{r}}= \frac{\mathbf{B}^{-1/2}\tilde{\mathbf{r}}^{*}}{\|\mathbf{B}^{-1/2}\mathbf{r} ^{*}\|_{2}}\) where \(\tilde{\mathbf{r}}^{*}=\underset{\|\tilde{\mathbf{r}}\|_{2}=1}{\operatorname {argmax}}\tilde{\mathbf{r}}^{\mathsf{T}}\mathbf{B}^{-1/2}\mathbf{A}\mathbf{B}^{ -1/2}\tilde{\mathbf{r}}\) 

### Proof of Lemma 2.1

Proof.: Let \(\mathbf{r}_{1}^{\prime}=\mathbf{A}\mathbf{r}_{1}\) and \(\mathbf{r}_{2}^{\prime}=\mathbf{B}\mathbf{r}_{2}\). As \(\mathbf{r}_{1}\) and \(\mathbf{r}_{2}\) are unit vectors, \(\frac{1}{\|\mathbf{A}^{-1}\|_{2}}\leq\|\mathbf{r}_{1}^{\prime}\|_{2}\leq\| \mathbf{A}\|_{2}\).

\[\|\mathbf{r}_{1}^{\prime}-\mathbf{r}_{2}^{\prime}\|_{2} =\|\mathbf{A}\mathbf{r}_{1}-\mathbf{A}\mathbf{r}_{2}+\mathbf{A} \mathbf{r}_{2}-\mathbf{B}\mathbf{r}_{2}\|_{2}\] \[\leq\|\mathbf{A}\|_{2}\|\mathbf{r}_{1}-\mathbf{r}_{2}\|_{2}+\| \mathbf{A}-\mathbf{B}\|_{2}\|\mathbf{r}_{2}\|_{2}\] \[\leq\|\mathbf{A}\|_{2}(\varepsilon_{2}+\varepsilon_{1})\]

\[\left\|\frac{\mathbf{r}_{1}^{\prime}}{\|\mathbf{r}_{1}^{\prime}\|_{2}}- \frac{\mathbf{r}_{2}^{\prime}}{\|\mathbf{r}_{2}^{\prime}\|_{2}}\right\|_{2}= \frac{\||\mathbf{r}_{2}^{\prime}\|_{2}\mathbf{r}_{1}^{\prime}-\|\mathbf{r}_{1}^ {\prime}\|_{2}\mathbf{r}_{2}^{\prime}\|_{2}}{\|\mathbf{r}_{1}^{\prime}\|_{2} \|\mathbf{r}_{2}^{\prime}\|_{2}}=\frac{\||\mathbf{r}_{2}^{\prime}\|_{2} \mathbf{r}_{1}^{\prime}-\|\mathbf{r}_{1}^{\prime}\|_{2}\mathbf{r}_{1}^{\prime}+ \|\mathbf{r}_{1}^{\prime}\|_{2}\mathbf{r}_{1}^{\prime}-\|\mathbf{r}_{1}^{ \prime}\|_{2}\mathbf{r}_{2}^{\prime}\|_{2}}{\|\mathbf{r}_{1}^{\prime}\|_{2}\| \mathbf{r}_{2}^{\prime}\|_{2}}\] \[\leq\frac{\|\mathbf{r}_{2}^{\prime}\|_{2}-\|\mathbf{r}_{1}^{\prime }\|_{2}\|\|\mathbf{r}_{1}^{\prime}\|_{2}+\|\mathbf{r}_{1}^{\prime}\|_{2}\| \mathbf{r}_{1}^{\prime}-\mathbf{r}_{2}^{\prime}\|_{2}}{\|\mathbf{r}_{1}^{ \prime}\|_{2}^{\prime}\|_{2}^{\prime}\|_{2}}\leq\frac{2(\|\mathbf{A}\|_{2}( \varepsilon_{2}+\varepsilon_{1}))}{\frac{1}{\|\mathbf{A}^{-1}\|_{2}}-\|\mathbf{A} \|_{2}(\varepsilon_{2}+\varepsilon_{1})}\] \[\leq\frac{2\frac{\lambda_{\min}(A)}{\lambda_{\min}(A)}(\varepsilon _{2}+\varepsilon_{1}))}{1-\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}(\varepsilon _{2}+\varepsilon_{1})}\leq\frac{4\lambda_{\max}(A)}{\lambda_{\min}(A)}( \varepsilon_{2}+\varepsilon_{1})\text{ if }\frac{\lambda_{\max}(A)}{\lambda_{\min}(A)}(\varepsilon_{2}+ \varepsilon_{1})\leq\frac{1}{2}\]

## Appendix C Proof of Theorem 1.5

For the setting of Theorem 1.5, we provide Algorithm 4. It uses as a subroutine a polynomial time procedure \(\mathsf{PrincipalEigenVector}\) for the principal eigen-vector of a symmetric matrix.

For notation, let \(\mathbf{\Gamma}:=\mathbf{\Sigma}^{1/2}\), then by Lemma E.13, we can write our linear threshold function \(\mathsf{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{X}+c_{*})=\mathsf{pos}( \mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}-\ell)\) where \(\mathbf{Z}\sim N(\mathbf{0},\mathbf{I})\) where \(\ell:=-\frac{c_{*}+\mathbf{r}_{*}^{\mathsf{T}}\mathbf{\mu}}{\|\mathbf{\Gamma} \mathbf{r}_{*}\|_{2}}\), \(\mathbf{u}_{*}:=\mathbf{\Gamma}\mathbf{r}_{*}/\|\mathbf{\Gamma}\mathbf{r}_{*}\|_{2}\). For any \(\mathbf{r}\in\mathbb{R}^{d}\), define \(\mathbf{u}:=\mathbf{\Gamma}\mathbf{r}/\|\mathbf{\Gamma}\mathbf{r}\|_{2}\). Let \(\phi\) and \(\Phi\) be the standard gaussian pdf and cdf respectively.

The following geometric bound is obtained by the algorithm.

**Lemma C.1**.: _For any \(\varepsilon,\delta\in(0,1)\), if \(m\geq O\left((d/\varepsilon^{4})O(\ell^{2})\log(d/\delta)\left(\frac{\lambda_{ \max}}{\lambda_{\min}}\right)^{4}q^{4}\right)\), then \(\hat{\mathbf{r}}\) computed in Step 3 of Alg. 4 satisfies \(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{*}\|_{2}\}\leq\varepsilon\), w.p. \(1-\delta/2\)._The above lemma, whose proof is deferred to Sec. C.1, is used in conjunction with the following lemma.

**Lemma C.2**.: _Let \(\varepsilon,\delta\in(0,1)\) and suppose that \(\hat{\mathbf{r}}\) computed in Step 3 of Alg. 4 satisfies if \(k\neq q/2\)\(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{*}\|_{2 }\}\leq\varepsilon\),. Then, with \(s\geq O\left(d(\log q+\log(1/\delta))/\varepsilon^{2}\right)\), \(h^{*}\) computed in Step 4.c or Step 5.d satisfies_

\[\Pr_{\mathcal{D}}\left[h^{*}(\mathbf{x})\neq f(\mathbf{x})\right]\leq\frac{8q \varepsilon}{\Phi(\ell)(1-\Phi(\ell))}\left(c_{0}\sqrt{\frac{\lambda_{\text{ max}}}{\lambda_{\text{min}}}}+c_{1}\frac{\|\mu\|_{2}}{\sqrt{\lambda_{\text{ min}}}}\right)\]

_and if \(k=q/2\),_

\[\min(\Pr_{\mathcal{D}}\left[h^{*}(\mathbf{x})\neq f(\mathbf{x})\right],\Pr_{ \mathcal{D}}\left[(1-h^{*}(\mathbf{x}))\neq f(\mathbf{x})\right])\leq\frac{8q \varepsilon}{\Phi(\ell)(1-\Phi(\ell))}\left(c_{0}\sqrt{\frac{\lambda_{\text{ max}}}{\lambda_{\text{min}}}}+c_{1}\frac{\|\mu\|_{2}}{\sqrt{\lambda_{\text{ min}}}}\right)\]

_w.p. \(1-\delta/2\), where \(c_{0},c_{1}\) are the constants from Lemma 2.3._

With the above we complete the proof of Theorem 1.5 as follows.

Proof.: (of Theorem 1.5) Let the parameters \(\delta,\varepsilon\) be as given in the statement of the theorem. We use for the ground in Lemma C.1. We now take \(m\) to be of \(m=O\left((d/\varepsilon^{4})\frac{O(\ell^{2})}{(\Phi(\ell)(1-\Phi(\ell)))^{2} }\log(d/\delta)\left(\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}\right) ^{4}\left(\frac{\sqrt{\lambda_{\text{max}}}+\|\mu\|_{2}}{\sqrt{\lambda_{\text {min}}}}\right)^{4}q^{8}\right)\) in Alg. 4 we obtain the following bound: \(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{* }\|_{2}\}\leq\varepsilon\frac{(\Phi(\ell)(1-\Phi(\ell)))\sqrt{\lambda_{\text {min}}}}{(\ell\infty\sqrt{\lambda_{\text{max}}}+c_{1}\|\mu\|_{2})}\) with probability \(1-\delta/2\). Using \(s\geq O\left(d(\log(q)+\log(1/\delta))\frac{q^{2}(\sqrt{\lambda_{\text{max}} }+\|\mu\|_{2})^{2}}{\varepsilon^{2}(\Phi(\ell)(1-\Phi(\ell))^{2}\lambda_{ \text{min}}}\right)\), Lemma C.2 yields the desired misclassification error bound of \(\varepsilon\) on \(h^{*}\) w.p. \(1-\delta\). 

Proof.: (of Lemma C.2) Define \(h(\mathbf{x}):=\text{pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{X}+c_{*})\) and \(\tilde{h}(\mathbf{x}):=\text{pos}(-\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{X}+c_ {*})\). Applying Lemma 2.3, we obtain that at least one of \(h\), \(\tilde{h}\) has an instance misclassification error of at most \(O(\varepsilon(\sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}}+\|\bm{\mu}\|_{2}/ \lambda_{\text{min}}))\). WLOG assume that \(h\) satisfies this error bound i.e., \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\leq\varepsilon(c_{0}\sqrt{ \lambda_{\text{max}}/\lambda_{\text{min}}}+c_{1}\|\bm{\mu}\|_{2}/\sqrt{\lambda_ {\text{min}}})=:\varepsilon^{\prime}\). Note that, \(\Pr_{\mathcal{D}}[f(\mathbf{x})=1]=\Phi(\ell),\Pr_{\mathcal{D}}[f(\mathbf{x}) =0]=1-\Phi(\ell)\). Thus,

\[\Pr_{\mathcal{D}}[h(x)\neq f(x)\;\mid\;f(\mathbf{x})=1]\leq\varepsilon^{\prime }/\Phi(\ell),\qquad\Pr_{\mathcal{D}}[[h(x)\neq f(x)\;\mid\;f(\mathbf{x})=0] \leq\varepsilon^{\prime}/(1-\Phi(\ell)).\]

Therefore, taking a union bound we get that the probability that a random bag from the oracle contains a feature vector on which \(f\) and \(h\) disagree is at most \(q\varepsilon^{\prime}/\Phi(\ell)(1-\Phi(\ell))\). Applying Chernoff bound (see Appendix B.1) we obtain that with probability at least \(1-\delta/6\), \(\mathsf{BagErr}_{\text{sample}}(h,\mathcal{M})\leq 2q\varepsilon^{\prime}/ \Phi(\ell)(1-\Phi(\ell))\). Therefore, \(h\) satisfies \(\mathsf{BagErr}_{\text{sample}}(h^{*},\mathcal{M})\leq 2q\varepsilon^{\prime}/ \Phi(\ell)(1-\Phi(\ell))\). Hence, there exists at least one \(h_{j}(\mathbf{x})=\mathsf{pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{x}+\hat{ \mathbf{r}}^{\mathsf{T}}\mathbf{x}_{j}^{(k)})\) will satisfy \(\mathsf{BagErr}_{\text{sample}}(h_{j},\mathcal{M})\leq 2q\varepsilon^{ \prime}/\Phi(\ell)(1-\Phi(\ell))\). Since, \(h^{*}(\mathbf{x})\) has the minimum sample bag error among all \(h_{j}(\mathbf{x})\), \(\mathsf{BagErr}_{\text{sample}}(h^{*},\mathcal{M})\leq 2q\varepsilon^{ \prime}/\Phi(\ell)(1-\Phi(\ell))\).

On the other hand, applying Theorem 2.2, except with probability \(\delta/3\), \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h^{*}(\mathbf{x})]\leq 8q\varepsilon^{\prime}/ \Phi(\ell)(1-\Phi(\ell))=\frac{8q\varepsilon}{\Phi(\ell)(1-\Phi(\ell))}(c_{0} \sqrt{\lambda_{\text{max}}/\lambda_{\text{min}}}+c_{1}\|\bm{\mu}\|_{2}/\sqrt{ \lambda_{\text{min}}})\) if \(k\neq q/2\) and \(\min(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h^{*}(\mathbf{x})],\Pr_{\mathcal{D}} [f(\mathbf{x})\neq(1-h^{*}(\mathbf{x}))])\leq 8q\varepsilon^{\prime}/\Phi(\ell)(1- \Phi(\ell))=\frac{8q\varepsilon}{\Phi(\ell)(1-\Phi(\ell))}(c_{0}\sqrt{\lambda_ {\text{max}}/\lambda_{\text{min}}}+c_{1}\|\bm{\mu}\|_{2}/\sqrt{\lambda_{\text{ min}}})\) if \(k=q/2\). Therefore, except with probability \(\delta/2\), the bound in Lemma C.2 holds. 

### Proof of Lemma C.1

We use these to generalize a few quantities we had defined earlier. We obtain their bounds using E.2,

**Definition C.3**.: _Define:_

\[\kappa_{1}:=\left(\frac{\phi(\ell)\left(\frac{k}{q}-(1-\Phi(\ell) )\right)}{\Phi(\ell)(1-\Phi(\ell))}\right)^{2}-\frac{\ell\phi(\ell)\left(\frac {k}{q}-(1-\Phi(\ell))\right)}{\Phi(\ell)(1-\Phi(\ell))},\qquad\kappa_{1}\geq- \ell^{2}/4\] \[\kappa_{2}:=\frac{2}{q-1}\frac{k}{q}\left(1-\frac{k}{q}\right) \left(\frac{\phi(\ell)}{\Phi(\ell)(1-\Phi(\ell))}\right)^{2},\qquad\kappa_{2} \geq 2\ell^{2}/q^{2}\qquad\text{when }\ell>1\] \[\kappa_{3}:=\frac{\kappa_{2}}{(1-\kappa_{1})(1-\max(0,\kappa_{1} ))},\qquad\kappa_{3}\geq\frac{2\ell^{2}}{q^{2}(1+\ell^{2}/4)}\qquad\text{when }\ell>1\] \[\theta:=\frac{2\lambda_{\text{max}}}{\lambda_{\text{min}}}\left( \frac{1}{2-\max(0,2\kappa_{1}-\kappa_{2})}+\frac{1}{1-\max(0,\kappa_{1})} \right),\quad\frac{3\lambda_{\text{max}}}{\lambda_{\text{min}}}\leq\theta\leq \frac{3\lambda_{\text{max}}}{(1-K_{2})\lambda_{\text{min}}}\]

_Where \(K_{i}\)'s are some finite functions of \(K\)._

Similar to Lemma 4.4, we again show in the following lemma that \(\hat{\mathbf{r}}\) in the algorithms is indeed \(\pm\mathbf{r}_{*}\) if the covariance estimates were the actual covariances.

**Lemma C.4**.: _The ratio \(\rho(\mathbf{r}):=\mathbf{r}^{\mathsf{T}}\bm{\Sigma}_{D}\mathbf{r}/\mathbf{r}^{ \mathsf{T}}\bm{\Sigma}_{B}\mathbf{r}\) is maximized when \(\mathbf{r}=\pm\mathbf{r}_{*}\). Moreover,_

\[\rho(\mathbf{r})=2+\frac{\gamma(\mathbf{r})^{2}\kappa_{2}}{1-\gamma(\mathbf{r})^ {2}\kappa_{1}}\;\;\text{where }\gamma(\mathbf{r}):=\frac{\mathbf{r}^{\mathsf{T}}\bm{\Sigma}\mathbf{r}_{*}}{ \sqrt{\mathbf{r}^{\mathsf{T}}\bm{\Sigma}\mathbf{r}}\sqrt{\mathbf{r}_{*}^{ \mathsf{T}}\bm{\Sigma}\mathbf{r}_{*}}}\;\text{and}\]

\[\mathbf{r}^{\mathsf{T}}\bm{\Sigma}_{B}\mathbf{r}=\mathbf{r}^{\mathsf{T}}\bm{ \Sigma}\mathbf{r}(1-\gamma(\mathbf{r})^{2}\kappa_{1}),\quad\mathbf{r}^{\mathsf{ T}}\bm{\Sigma}_{D}\mathbf{r}=\mathbf{r}^{\mathsf{T}}\bm{\Sigma}\mathbf{r}(2- \gamma(\mathbf{r})^{2}(2\kappa_{1}-\kappa_{2}))\]

Proof.: Using the transformations to \(\mathbf{Z}\), we can let \(\mathbf{X}_{B}=\bm{\Gamma}\mathbf{Z}_{B}\) as a random feature-vector sampled uniformly from a random bag sampled from \(\mathcal{O}\). Also, let \(\mathbf{X}_{D}=\bm{\Gamma}\mathbf{Z}_{D}\) be the difference of two random feature vectors sampled uniformly without replacement from a random bag sampled from \(\mathcal{O}\). Observe that the ratio \(\rho(\mathbf{r})=\mathrm{Var}[\mathbf{r}^{\mathsf{T}}\mathbf{X}_{D}]/\mathrm{ Var}[\mathbf{r}^{\mathsf{T}}\mathbf{X}_{B}]=\mathrm{Var}[\mathbf{u}^{\mathsf{T}} \mathbf{Z}_{D}]/\mathrm{Var}[\mathbf{u}^{\mathsf{T}}\mathbf{Z}_{B}]\).

Let \(g^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}\) which is \(N(0,1)\). For \(a\in\{0,1\}\), let \(\mathbf{Z}_{a}\) be \(\mathbf{Z}\) conditioned on \(\mathsf{pos}\left(\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}-\ell\right)=a\). Let \(g_{*}^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}_{a}\), \(a\in\{0,1\}\). \(g_{0}^{*}\) lower-tailed one-sided truncated normal distributions truncated at \(\ell\) and \(g_{*}^{*}\) upper-tailed one-sided truncated normal distributions truncated at \(\ell\). Hence, \(\mathbb{E}[g_{1}^{*}]=\phi(\ell)/(1-\Phi(\ell))\), \(\mathbb{E}[g_{0}^{*}]=-\phi(\ell)/\Phi(\ell)\), \(\mathbb{E}[g_{1}^{*2}]=1+\ell\phi(\ell)/(1-\Phi(\ell))\), \(\mathbb{E}[g_{0}^{*2}]=1-\ell\phi(\ell)/\Phi(\ell)\).

With this setup, letting \(g_{B}^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}_{B}\) and \(g_{D}^{*}:=\mathbf{u}_{*}^{\mathsf{T}}\mathbf{Z}_{D}\) we obtain (using Lemma B.2 in Appendix B.2)

\[\mathrm{Var}[g_{B}^{*}]=1-\kappa_{1},\quad\mathrm{Var}[g_{D}^{*}]=2(1-\kappa_{ 1})+\kappa_{2}\]

Now let \(\tilde{\mathbf{u}}\) be a unit vector orthogonal to \(\mathbf{u}_{*}\). Let \(\tilde{g}=\tilde{\mathbf{u}}^{\mathsf{T}}\mathbf{Z}\) be \(N(0,1)\). Also, let \(\tilde{g}_{a}=\tilde{\mathbf{u}}^{\mathsf{T}}\mathbf{Z}_{a}\) for \(a\in\{0,1\}\). Since \(\mathbf{Z}_{a}\) are given by conditioning \(\mathbf{Z}\) only along \(\mathbf{u}_{*}\), \(\tilde{g}_{a}\sim N(0,1)\) for \(a\in\{0,1\}\). In particular, the component along \(\tilde{u}\) of \(\mathbf{Z}_{B}\) (call it \(\tilde{g}_{B}\)) is \(N(0,1)\) and that of \(\mathbf{Z}_{D}\) (call it \(\tilde{g}_{D}\)) is the difference of two iid \(N(0,1)\) variables. Thus, \(\mathrm{Var}[\tilde{g}_{B}]=1\) and \(\mathrm{Var}[\tilde{g}_{D}]=2\). Moreover, due to orthogonality all these gaussian variables corresponding to \(\tilde{\mathbf{u}}\) are independent of those corresponding to \(\mathbf{u}_{*}\) defined earlier.

Now let \(\mathbf{u}=\alpha\mathbf{u}_{*}+\beta\tilde{\mathbf{u}}\), where \(\beta=\sqrt{1-\alpha^{2}}\) be any unit vector. From the above we have,

\[\frac{\mathrm{Var}\left[\mathbf{u}^{\mathsf{T}}\mathbf{Z}_{D} \right]}{\mathrm{Var}\left[\mathbf{u}^{\mathsf{T}}\mathbf{Z}_{B}\right]} = \frac{\mathrm{Var}\left[\alpha g_{D}^{*}+\beta\tilde{g}_{D}\right] }{\mathrm{Var}\left[\alpha g_{B}^{*}+\beta\tilde{g}_{B}\right]} = \frac{\alpha^{2}\,\mathrm{Var}\left[g_{D}^{*}\right]+\beta^{2}\, \mathrm{Var}\left[\tilde{g}_{D}\right]}{\alpha^{2}\,\mathrm{Var}\left[g_{B}^{ *}\right]+\beta^{2}\,\mathrm{Var}\left[\tilde{g}_{B}\right]}\] (7) \[= \frac{2\alpha^{2}(1-\kappa_{1})+\alpha^{2}\kappa_{2}+2\beta^{2}} {\alpha^{2}(1-\kappa_{1})+\beta^{2}}\] \[= 2+\frac{\alpha^{2}\kappa_{2}}{1-\alpha^{2}\kappa_{1}}\]

where the last equality uses \(\beta=\sqrt{1-\alpha^{2}}\). Letting \(\mathbf{u}=\mathbf{\Gamma}\mathbf{r}/\|\mathbf{\Gamma}\mathbf{r}\|_{2}\) we obtain that \(\alpha=\frac{\langle\mathbf{r}_{\mathsf{r}},\mathbf{\Gamma}\mathbf{r}_{*} \rangle}{\|\mathbf{\Gamma}\mathbf{r}\|_{2}\|\mathbf{\Gamma}\mathbf{r}_{*}\|_{ 2}}=\gamma(\mathbf{r})\) completing the proof. 

Lemma 4.5 shows that ratio maximization can be treated as an eigenvalue decomposition problem of the matrix \(\mathbf{\Sigma}_{B}^{-1/2}\mathbf{\Sigma}_{D}\mathbf{\Sigma}_{B}^{-1/2}\). We now prove that the error in the estimate of \(\hat{\mathbf{r}}\) given to us by the algorithm is bounded if the error in the covariance estimates are bounded. The sample complexity of computing these estimates gives the sample complexity of our algorithm.

We now complete the proof of Lemma C.1 (with \(\delta\) instead of \(\delta/2\) for convenience). By Lemma 3.1, taking \(m\geq O\left((d/\varepsilon_{1}^{2})O(\ell^{2})\log(d/\delta)\right)\) ensures that \(\|\mathbf{E}_{B}\|_{2}\leq\varepsilon_{1}\lambda_{\text{max}}\) and \(\|\mathbf{E}_{D}\|_{2}\leq\varepsilon_{1}\lambda_{\text{max}}\) w.p. at least \(1-\delta\) where \(\mathbf{E}_{B}=\hat{\mathbf{\Sigma}}_{B}-\mathbf{\Sigma}_{B}\) and \(\mathbf{E}_{D}=\hat{\mathbf{\Sigma}}_{D}-\mathbf{\Sigma}_{D}\). We start by defining \(\hat{\rho}(\mathbf{r}):=\frac{\mathbf{r}^{\mathsf{T}}\hat{\mathbf{\Sigma}}_{D} \mathbf{r}}{\mathbf{r}^{\mathsf{T}}\hat{\mathbf{\Sigma}}_{D}\mathbf{r}}\) which is the equivalent of \(\rho\) using the estimated matrices. Observe that it can be written as \(\hat{\rho}(\mathbf{r})=\frac{\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{\mathbf{r} }\mathbf{r}+\mathbf{r}^{\mathsf{T}}\mathbf{E}_{B}\mathbf{r}}{\mathbf{r}^{ \mathsf{T}}\mathbf{\Sigma}_{\mathbf{D}}\mathbf{r}+\mathbf{r}^{\mathsf{T}}\mathbf {E}_{D}\mathbf{r}}\). Using these we can obtain the following bound on \(\hat{\rho}\): for any \(\mathbf{r}\in\mathbb{R}^{d}\), \(|\hat{\rho}(\mathbf{r})-\rho(\mathbf{r})|\leq\varrho\varepsilon_{1}|\rho( \mathbf{r})|\) w.p. at least \(1-\delta\) (*) as long as \(\varepsilon_{1}\leq\frac{(1-\max(0,\kappa_{1}))}{2}\frac{\lambda_{\text{min}}}{ \lambda_{\text{max}}}\), which we shall ensure (see Appendix B.4).

For convenience we denote the normalized projection of any vector \(\mathbf{r}\) as \(\tilde{\mathbf{r}}:=\frac{\mathbf{\Sigma}^{1/2}\mathbf{r}}{\|\mathbf{\Sigma}^{ 1/2}\mathbf{r}\|_{2}}\). Now let \(\tilde{\mathbf{r}}\in\mathbb{R}^{d}\) be a unit vector such that \(\min\{\|\tilde{\mathbf{r}}-\tilde{\mathbf{r}}_{*}\|_{2},\|\tilde{\mathbf{r}}+ \tilde{\mathbf{r}}_{*}\|_{2}\}\geq\varepsilon_{2}\). Hence, using the definitions from Lemma C.4, \(|\gamma(\mathbf{r})|\leq 1-\varepsilon_{2}^{2}/2\) while \(\gamma(\mathbf{r}_{*})=1\) which implies \(\rho(\mathbf{r}_{*})-\rho(\mathbf{r}_{*})\geq\kappa_{3}\varepsilon_{2}^{2}/2\). Note that \(\rho(\mathbf{r})\leq\rho(\mathbf{r}_{*})=2+\kappa_{3}(1-\max(0,\kappa_{1}))\). Choosing \(\varepsilon_{1}<\frac{\kappa_{3}}{2\theta(2+\kappa_{3}(1-\max(0,\kappa_{1}))} \varepsilon_{2}^{2}\), we obtain that \(\rho(\mathbf{r}_{*})(1-\theta\varepsilon_{1})>\rho(\mathbf{r})(1+\theta \varepsilon_{1})\). Using this along with the bound (*) we obtain that w.p. at least \(1-\delta\), \(\hat{\rho}(\mathbf{r}_{*})>\hat{\rho}(\mathbf{r})\) when \(\varepsilon_{2}>0\). Since our algorithm returns \(\tilde{\mathbf{r}}\) as the maximizer of \(\hat{\rho}\), w.p. at least \(1-\delta\) we get \(\min\{\|\tilde{\mathbf{r}}-\tilde{\mathbf{r}}_{*}\|_{2},\|\tilde{\mathbf{r}}+ \tilde{\mathbf{r}}_{*}\|_{2}\}\leq\varepsilon_{2}\). Using Lemma 2.1, \(\min\{\|\tilde{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{*} \|_{2}\}\leq 4\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}\varepsilon_{2}\). Substituting \(\varepsilon_{2}=\frac{\varepsilon}{\delta}\sqrt{\frac{\lambda_{\text{min}}}{ \lambda_{\text{max}}}}\), \(\|\mathbf{r}-\mathbf{r}_{*}\|_{2}\leq\varepsilon\) w.p. at least \(1-\delta\). The conditions on \(\varepsilon_{1}\) are satisfied by taking it to be \(O\left(\frac{\kappa_{3}\varepsilon^{2}\lambda_{\text{min}}}{\theta(2+\kappa_{3}(1- \max(0,\kappa_{1}))\lambda_{\text{max}}}\right)\), and thus we can take \(m\geq O\left((d/\varepsilon^{4})O(\ell^{2})\log(d/\delta)\left(\frac{\lambda_{ \text{max}}}{\lambda_{\text{min}}}\right)^{2}\theta^{2}\left(\frac{2+\kappa_{3 }(1-\max(0,\kappa_{1}))}{\kappa_{3}}\right)^{2}\right)=O\left((d/\varepsilon^{4})O( \ell^{2})\log(d/\delta)\left(\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}} \right)^{4}q^{4}\right)\) since \(1/\kappa_{3}\leq q^{2}(1+\ell^{2}/4)/\ell^{2}\) whenever \(\ell>1\)(Defn. C.3). This completes the proof of Lemma C.1.

## Appendix D Generalization error Bounds

We show that if a hypothesis LTF \(h\) satisfies close to \(1\) fraction of sufficient number of bags sampled from a bag oracle then with high probability \(h\) is a good approximator for the target LTF \(f\) (or its complement in the case of balanced bags). The first step is to prove a generalization bound from the sample bag-level accuracy to the oracle bag-level accuracy.

**Theorem D.1**.: _Let \(\mathcal{O}:=\mathsf{Ex}(f,\mathcal{D},q,k)\) be any bag oracle for an LTF \(f\) in \(d\)-dimensions, and let \(\mathcal{M}\) be a collection of \(m\) bags sampled iid from the oracle. Then, there is an absolute constant \(C_{0}\leq 1000\) s.t. w.p. at least \(1-\delta\),_

\[\mathsf{BagErr}_{\mathsf{oracle}}(h,f,\mathcal{D},q,k)\leq\mathsf{BagErr}_{ \mathsf{sample}}(h,\mathcal{M})+\varepsilon\] (8)

_when \(m\geq C_{0}d\left(\log q+\log(1/\delta)\right)/\varepsilon^{2}\), for any \(\delta,\varepsilon>0\)._

Proof.: The proof follows from the arguments similar to the ones used in Appendix M of [26] to prove bag satisfaction generalization bounds. Consider a bag loss function of the form \(\ell(\phi^{\zeta}(h,B),(\phi^{\zeta}(f,B))\), where \(\phi^{\zeta}(g,B):=\zeta(\mathbf{y}_{g,B})\) where \(\mathbf{y}_{g,B}\) is the vector of \((g(\mathbf{x}))_{\mathbf{x}\in B}\), for \(\zeta:\{0,1\}^{q}\to\mathbb{R}\). The result of [35] showed generalization error bounds when (i) \(\zeta\) is \(1\)-Lipschitz w.r.t. to \(\infty\)-norm, and (ii) \(\ell\) is \(1\)-Lipschitz in the first coordinate. We can thus apply their results using the bound of \((d+1)\) on the VC-dimension of LTFs in \(d\)-dimensions to show that the above bound on \(m\) holds (with \(C_{0}/8\) instead of \(C_{0}\)) for the generalization of the following bag error:

\[\left|\gamma(B,f,t)-\gamma(B,h,t)\right|,\]

where

\[\gamma(B,g,t):=\begin{cases}0&\text{if }\sum_{\mathbf{x}\in B}g(\mathbf{x}) \leq t\\ 1&\text{otherwise.}\end{cases}\] (9)

for \(t\in\{0,\ldots,q-1\}\). We can bound our bag satisfaction error by the sum of the generalization errors of \(\left|\gamma(B,f,k)-\gamma(B,h,k)\right|\), and \(\left|\gamma(B,f,k-1)-\gamma(B,h,k-1)\right|\), which can each be bounded by \(\varepsilon/2\) thus completing the proof. 

Next we show that if the oracle-level bag accuracy of \(h\) is high then this translates to \(h\) is being a low error instance-level approximator for the target LTF \(f\) (or its complement in the case of balanced bags). With the setup as used in the previous theorem, let us define define the regions \(S_{a}:=\{\mathbf{x}\text{ s.t. }f(\mathbf{x})=a\}\) for \(a\in\{0,1\}\), and \(S_{ab}:=\{\mathbf{x}\text{ s.t. }f(\mathbf{x})=a,h(\mathbf{x})=b\}\). Let \(\mu\) be the measure induced by \(\mathcal{D}\), \(\mu_{a}\) and \(\mu_{ab}\) be the respectively conditional measures induced on \(S_{a}\) and \(S_{ab}\). The oracle \(\mathcal{O}\) for a random bag \(B\), samples \(k\) points iid from \((S_{1},\mu_{1})\) and \((q-k)\) points from \((S_{0},\mu_{0})\).

we have the following theorem.

**Theorem D.2**.: _Suppose \(k\in\{1,\ldots,q\}\), and \(0<\mathsf{BagAcc}_{\mathsf{oracle}}(h,f,\mathcal{D},q,k)\leq\varepsilon^{ \prime}<1/(4q)\), then_

1. \(\Pr_{\mathcal{D}}\bigl{[}f(\mathbf{x})\neq h(\mathbf{x})\bigr{]}\leq\varepsilon\) _if_ \(k\neq q/2\)_, and_
2. \(\Pr_{\mathcal{D}}\bigl{[}f(\mathbf{x})\neq h(\mathbf{x})\bigr{]}\leq\varepsilon\) _or_ \(\Pr[f(\mathbf{x})\neq(1-h(\mathbf{x}))]\leq\varepsilon\)_, if_ \(k=q/2\)_,_

_where \(\varepsilon=4\varepsilon^{\prime}\)._

Before we prove the above them, we need the following lemma bounding the probability that two independent binomial random variables take the same value. Let \(\text{Binomial}(n,p)\) be the sum of \(n\) iid \(\{0,1\}\) random variables each with expectation \(p\).

**Lemma D.3**.: _Let \(u,v\in\{1,\ldots,q\}\), and \(X_{1}\sim\text{Binomial}(u,p_{1})\) and \(X_{2}\sim\text{Binomial}(v,p_{2})\) be independent binomial distributions for some probabilities \(p_{1}\) and \(p_{2}\). Let \(p^{*}=\min\{\max\{p_{1},(1-p_{1})\},\max\{p_{2},(1-p_{2})\}\}\), then, \(\Pr\left[X_{1}\neq X_{2}\right]\geq 1-\sqrt{p^{*}}\)._

Proof.: We first begin by bounding a binomial coefficient by the sum of its adjacent binomial coefficients. Let \(n\geq 1\) and \(n>r>0\). We begin with the standard identity and proceed further:

\[\begin{pmatrix}n\\ r\end{pmatrix} = \begin{pmatrix}n-1\\ r\end{pmatrix}+\begin{pmatrix}n-1\\ r-1\end{pmatrix}\] (10) \[\leq \frac{n}{r+1}\binom{n-1}{r}+\frac{n}{n-r+1}\binom{n-1}{r-1}\] (11) \[= \binom{n}{r+1}+\binom{n}{r-1}\] (12)Moreover, it is trivially true that \(\binom{n}{0}\leq\binom{n}{1}\) and \(\binom{n}{n}\leq\binom{n}{n-1}\), therefore (11) holds even for \(r\in\{0,n\}\) whenever the binomial coefficients exist. Now, for some probability \(p\) define \(\nu(p,n,r):=\binom{n}{r}p^{r}(1-p)^{1-r}\) which is pdf at \(r\) of Binomial\((n,p)\). The above implies the following:

\[\nu(p,n,r)\leq p^{\prime}\left(\nu(p,n,r-1)+\nu(p,n,r+1)\right)\] (12)

where \(p^{\prime}=\max\{p/(1-p),(1-p)/p\}\). Using the fact that \(\nu(p,n,r-1)+\nu(p,n,r)+\nu(p,n,r+1)\leq 1\), we obtain that

\[(1/p^{\prime})\nu(p,n,r)+\nu(p,n,r)\leq 1\] (13) \[\Rightarrow \nu(p,n,r)\leq(p^{\prime}/1+p^{\prime})=\max\{p,1-p\}\]

The above allows us to complete the proof of the lemma as follows.

\[\Pr\left[X_{1}=X_{2}\right]\] (14) \[= \sum_{r=0}^{\min\{u,v\}}\nu(p_{1},u,r)\nu(p_{2},v,r)\] \[\leq \left(\sum_{r=0}^{u}\nu(p_{1},u,r)^{2}\right)^{\frac{1}{2}}\left( \sum_{r=0}^{v}\nu(p_{2},v,r)^{2}\right)^{\frac{1}{2}}\] \[\leq \left(\max_{r}\{\nu(p_{1},u,r)\}\sum_{r=0}^{u}\nu(p_{1},u,r) \right)^{\frac{1}{2}}\] \[\cdot\left(\max_{r}\{\nu(p_{2},v,r)\}\sum_{r=0}^{u}\nu(p_{2},v,r) \right)^{\frac{1}{2}}\] \[\leq (\max\{p_{1},1-p_{1}\})^{\frac{1}{2}}(\max\{p_{2},1-p_{2}\})^{ \frac{1}{2}}\] \[\leq \sqrt{p^{*}},\]

where we use Cauchy-Schwarz for the first inequality. 

Proof.: (of Theorem D.2) We now consider three cases, for each one we shall prove points (i) and (ii) of the theorem.

_Case \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\geq 1-\varepsilon\)._ This condition means that \(\mu(S_{10})+\mu(S_{01})\geq 1-\varepsilon\), which implies that at least one of \(\mu_{1}(S_{10}),\mu_{0}(S_{01})\) is \(\geq 1-\varepsilon\). Assume that \(\mu_{0}(S_{01})\geq 1-\varepsilon\) (the other case is analogous).

Consider unbalanced bags i.e., \(k\neq q/2\). Now in case that \(\mu_{1}(S_{10})\geq 1-\varepsilon\), all the points sampled from \(S_{1}\) are sampled from \(S_{10}\) w.p. \((1-k\varepsilon)\) and those sampled from \(S_{0}\) are all sampled from \(S_{01}\) w.p. \((1-(q-k)\varepsilon)\). Therefore, with probability at least \((1-q\varepsilon)\) all the points are sampled from \(S_{10}\cup S_{01}\). Since \(k\neq q/2\) this implies that \(h\) does not satisfy the random bags with probability at least \((1-q\varepsilon)>\varepsilon^{\prime}\). If \(\mu_{1}(S_{10})\leq\varepsilon\), we can show with similar arguments that with probability \(\geq(1-q\varepsilon)>\varepsilon^{\prime}\) no points are sampled from \(S_{10}\) and \(q-k\) points are sampled from \(S_{01}\) which means (since \(k\geq 1\)) that \(h\) does not satisfy the bag. Finally, let \(\mu_{1}(S_{10})\in(\varepsilon,1-\varepsilon)\). In this case, the number of points sampled from \(S_{10}\) is distributed as Binomial\((k,\mu_{1}(S_{10}))\) and those sampled from \(S_{01}\) is independently distributed as Binomial\((k,\mu_{0}(S_{01}))\). If these two numbers are different then \(h\) does not satisfy the bag. We can apply Lemma D.3 and using the bounds on \(\mu_{1}(S_{10})\), we get that \(p^{*}\leq 1-\varepsilon\). Therefore, the probability of \(h\) not satisfying a randomly sampled bag is at least

\[1-\sqrt{1-\varepsilon}\geq\frac{(1-(1-\varepsilon))}{1+\sqrt{1-\varepsilon}} \geq\varepsilon/2>\varepsilon^{\prime}.\] (15)

For balanced bags, the case \(\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]\geq 1-\varepsilon\) implies that \(\Pr[f(\mathbf{x})\neq(1-h(\mathbf{x}))]\leq\varepsilon\), so the condition (ii) of the theorem holds.

_Case \(\varepsilon<\Pr_{\mathcal{D}}[f(\mathbf{x})\neq h(\mathbf{x})]<1-\varepsilon\)._ This case violates the conditions (i) and (ii). First observe that, \(\mu_{1}(S_{10})\) and \(\mu_{0}(S_{01})\) both cannot be \(\geq(1-\varepsilon)\) or \(\leq\varepsilon\), otherwise \(\mu(S_{10})+\mu(S_{01})\) is either \(\geq 1-\varepsilon\) or \(\leq\varepsilon\) violating the assumption of this case. For the subcase that \(\mu_{1}(S_{10})\leq\varepsilon\) and \(\mu_{0}(S_{01})\geq(1-\varepsilon)\)we can show using arguments similar to the previous case that w.p. \(\geq(1-q\varepsilon)\) no points are sampled from \(S_{10}\) and \(q-k\) points are sampled from \(S_{01}\), and thus, \(h\) will not satisfy a random bag with probability at least \((1-q\varepsilon)>\varepsilon^{\prime}\). The subcase when \(\mu_{1}(S_{01})\leq\varepsilon\) and \(\mu_{0}(S_{10})\geq(1-\varepsilon)\) is analogous. Finally, we have the subcase that \(\mu_{1}(S_{10})\) or \(\mu_{0}(S_{01})\) both lie in the range \((\varepsilon,1-\varepsilon)\). Now, the number of points sampled from \(S_{10}\) is distributed as Binomial\((k,\mu_{1}(S_{10}))\) and those sampled from \(S_{01}\) is independently distributed as Binomial\((k,\mu_{0}(S_{01}))\). If these two numbers are different then \(h\) does not satisfy the bag. We can apply Lemma D.3 and using the bounds on one of \(\mu_{1}(S_{10})\) or \(\mu_{0}(S_{01})\), we get that \(p^{*}\leq 1-\varepsilon\). Therefore, the probability of \(h\) not satisfying a randomly sampled bag is at least \(1-\sqrt{1-\varepsilon}\geq\varepsilon/2>\varepsilon^{\prime}\), using (15).

### Proof of Theorem 2.2

The proof follows directly from Theorems D.1 and D.2.

### Proof of Lemma 2.3

Proof.: We have,

\[\Pr[\mathsf{pos}(\mathbf{r}^{\mathsf{T}}\mathbf{X}+c)\neq\mathsf{ pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{X}+c)] =\Pr[\mathsf{pos}(\mathbf{r}^{\mathsf{T}}\tilde{\mathbf{X}}+\| \mathbf{\Gamma}\mathbf{r}\|_{2}\zeta)\neq\mathsf{pos}(\hat{\mathbf{r}}^{ \mathsf{T}}\tilde{\mathbf{X}}+\|\mathbf{\Gamma}\hat{\mathbf{r}}\|_{2}\hat{ \zeta})]\] \[=\Pr[\mathsf{pos}(\mathbf{a}^{\mathsf{T}}\mathbf{Z}+\zeta)\neq \mathsf{pos}(\hat{\mathbf{a}}^{\mathsf{T}}\mathbf{Z}+\hat{\zeta})]\] (16)

where \(\mathbf{\Gamma}=\mathbf{\Sigma}^{1/2}\), \(\mathbf{a}=\mathbf{\Gamma}\mathbf{r}/\|\mathbf{\Gamma}\mathbf{r}\|_{2}\) and \(\hat{\mathbf{a}}=\mathbf{\Gamma}\hat{\mathbf{r}}/\|\mathbf{\Gamma}\hat{ \mathbf{r}}\|_{2}\) and \(\mathbf{Z}=\mathbf{\Gamma}^{-1}\tilde{\mathbf{X}}\sim N(\mathbf{0},\mathbf{I})\) and \(\tilde{\mathbf{X}}+\boldsymbol{\mu}=\mathbf{X}\sim N(\boldsymbol{\mu},\mathbf{ \Sigma})\), \(\zeta=(c+\mathbf{r}^{\mathsf{T}}\boldsymbol{\mu})/\|\mathbf{\Gamma}\mathbf{r}\|_ {2}\) and \(\hat{\zeta}=(c+\hat{\mathbf{r}}^{\mathsf{T}}\boldsymbol{\mu})/\|\mathbf{\Gamma }\hat{\mathbf{r}}\|_{2}\). By Lemma 2.1, \(\|\mathbf{a}-\hat{\mathbf{a}}\|_{2}\leq 4\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min} }}}\varepsilon\).

Now, the RHS of (16) can be bounded as,

\[\Pr[\mathsf{pos}(\mathbf{a}^{\mathsf{T}}\mathbf{Z}+\zeta)\neq \mathsf{pos}(\mathbf{a}^{\mathsf{T}}\mathbf{Z}+\hat{\zeta})]+\Pr[\mathsf{pos} (\mathbf{a}^{\mathsf{T}}\mathbf{Z}+\hat{\zeta})\neq\mathsf{pos}(\hat{\mathbf{ a}}^{\mathsf{T}}\mathbf{Z}+\hat{\zeta})]\] (17)

Now, \(g:=\mathbf{a}^{\mathsf{T}}\mathbf{Z}\sim N(0,1)\). Thus, the first term in the above is bounded by the probability that \(g\) lies in a range of length \(|\zeta-\hat{\zeta}|=\|\boldsymbol{\mu}\|_{2}\|\mathbf{r}-\hat{\mathbf{r}}\|_{ 2}/\|\mathbf{\Gamma}\mathbf{r}\|_{2}\leq\varepsilon\|\boldsymbol{\mu}\|_{2}/ \sqrt{\lambda_{\text{min}}}\). This probability is at most \(\varepsilon\|\boldsymbol{\mu}\|_{2}/\sqrt{2\pi\lambda_{\text{min}}}\).

For the second term, that is at most the probability that \(\mathsf{pos}(\mathbf{a}^{\mathsf{T}}\mathbf{Z})\neq\mathsf{pos}(\hat{\mathbf{ a}}^{\mathsf{T}}\mathbf{Z})\). Now \(\|\hat{\mathbf{a}}-\mathbf{a}\|_{2}\leq 4\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{ \text{min}}}}\varepsilon\Rightarrow\angle\hat{\mathbf{a}},\mathbf{a}\leq\pi 4\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{ \text{min}}}}\varepsilon\Rightarrow\Pr[\mathsf{pos}(\mathbf{a}^{\mathsf{T}} \mathbf{Z})\neq\mathsf{pos}(\hat{\mathbf{a}}^{\mathsf{T}}\mathbf{Z})]\leq 2\sqrt{ \frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}\varepsilon\). Hence,

\[\Pr[\mathsf{pos}(\mathbf{r}^{\mathsf{T}}\mathbf{X}+c)\neq \mathsf{pos}(\hat{\mathbf{r}}^{\mathsf{T}}\mathbf{X}+c)]\leq\varepsilon\left( 2\sqrt{\frac{\lambda_{\text{max}}}{\lambda_{\text{min}}}}+\frac{\|\boldsymbol{ \mu}\|_{2}}{\sqrt{2\pi\lambda_{\text{min}}}}\right)\]

## Appendix E Subgaussian concentration with thresholded Gaussian random variables

Let \(\Phi(.)\) be the standard Gaussian cdf i.e., \(\Phi(t):=\Pr_{X\sim N(0,1)}\left[X\geq t\right]\). We also define \(\overline{\Phi}(t):=\Pr\left[X>t\right]=1-\Phi(t)\). We begin by defining the subgaussian norm of a random variable.

**Definition E.1**.: _The subgaussian norm of a random variable \(X\) denoted by \(\|X\|_{\psi_{2}}\) and is defined as: \(\|X\|_{\psi_{2}}:=\inf\{t>0:\mathbb{E}\left[\exp\left(X^{2}/t^{2}\right) \right]\leq 2\}\). Further, there is an absolute constant \(K_{0}\) such that \(\|X\|_{\psi_{2}}\leq K_{0}K\) if \(X\) satisfies,_

\[\Pr\left[|X|\geq t\right]\leq 2\text{exp}\left(-t^{2}/K^{2}\right),\qquad\text{ for all }t\geq 0.\] (18)

Let \(X\sim N(0,1)\). It is easy to see that \(\mathbb{E}\left[\exp(X^{2}/2^{2})\right]=(1/\sqrt{2\pi})\int_{-\infty}^{\infty} \text{exp}(-x^{2}/4)dx=\sqrt{2}(1/\sqrt{2\pi})\int_{-\infty}^{\infty}\text{ exp}(-z^{2}/2)dz=\sqrt{2}\). Thus, \(X\sim N(0,1)\) is subgaussian with subgaussian norm \(\leq 2\). In the analysis of this section, we shall use the following proposition from [32].

**Proposition E.2** (Prop. 2.1.2 of [32]).: _Let \(X\sim N(0,1)\). Then, for any \(t>0\),_

\[\left(\frac{1}{t}-\frac{1}{t^{3}}\right)\frac{1}{\sqrt{2\pi}}\text{exp}\left(-t^ {2}/2\right)\leq\overline{\Phi}(t)\leq\frac{1}{t\sqrt{2\pi}}\text{exp}\left(-t ^{2}/2\right).\]

_In particular for \(t\geq 1\),_

\[\overline{\Phi}(t)\leq\frac{1}{\sqrt{2\pi}}\text{exp}\left(-t^{2}/2\right).\]

Using the above, and by symmetry it is easy to see that for \(t\geq 1\), \(\Pr\left[\left|X\right|>t\right]\leq\left(\sqrt{2/\pi}\right)\text{exp}\left(-t ^{2}/2\right)\leq 2\cdot\text{exp}\left(-t^{2}/2\right)\). On the other hand, \(2\cdot\text{exp}\left(-t^{2}/2\right)\geq 1\) for \(0\leq t<1\). Thus,

\[\Pr\left[\left|X\right|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/2\right), \quad\forall\;t\geq 0.\] (19)

Consider the normal distribution conditioned on a threshold defined by letting \(\mathcal{D}_{\ell}\) be the distribution of \(\{X\sim N(0,1)\;\mid\;X>\ell\}\). We shall show that \(\tilde{X}\sim\mathcal{D}_{\ell}\) is a subgaussian random variable. Let us handle the (relatively) easy case of \(\ell\leq 0\) first.

**Lemma E.3**.: _Let \(\tilde{X}\sim\mathcal{D}_{\ell}\) for some \(\ell\leq 0\). Then, \(\Pr\left[\left|\tilde{X}\right|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/2\right)\), for any \(t>0\)._

Proof.: Let \(E\) be the event that \(\tilde{X}\geq 0\). Conditioned on \(E\), \(\tilde{X}\) is distributed as \(\left|X\right|\) where \(X\sim N(0,1)\) and (19) implies that

\[\Pr\left[\left|\tilde{X}\right|>t\;\mid\;E\right]\leq 2\cdot\text{exp}\left(-t^ {2}/2\right),\quad\forall\;t>0.\]

On the other hand, conditioned on \(\overline{E}\), \(\tilde{X}\) is sampled as \(-\left|Z\right|\) where \(\{Z\sim N(0,1)\;\mid\;\left|Z\right|\leq-\ell=\left|\ell\right|\}\). Thus,

\[\Pr\left[\left|\tilde{X}\right|>t\;\mid\;\overline{E}\right]=\Pr_{Z\sim N(0,1 )}\left[\left|Z\right|>t\;\mid\;\left|Z\right|\leq\left|\ell\right|\right].\] (20)

Now, if \(t>\left|\ell\right|\) then \(\Pr\left[\left|\tilde{X}\right|>t\;\mid\;\overline{E}\right]=0\), otherwise if \(t\leq\ell\) the LHS of (20) is,

\[\Pr_{Z\sim N(0,1)}\left[\left|Z\right|>t\;\mid\;\left|Z\right| \leq\left|\ell\right|\right]=1-\Pr\left[\left|Z\right|\leq t\;\mid\;\left|Z \right|\leq\left|\ell\right|\right] =1-\frac{\Pr\left[\left|Z\right|\leq t\right]}{\Pr\left[\left|Z \right|\leq\ell\right]}\] \[\leq 1-\Pr\left[\left|Z\right|\leq t\right]=\Pr\left[\left|Z\right|>t\right]\]

which is bounded by \(2\cdot\text{exp}\left(-t^{2}/2\right)\) and combining the probability bounds conditioned on \(E\) and \(\overline{E}\) we complete the proof. 

The case of \(\ell>0\) is proved below.

**Lemma E.4**.: _Let \(\tilde{X}\sim\mathcal{D}_{\ell}\) for some \(\ell>0\). Then, \(\Pr\left[\left|\tilde{X}\right|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/K_{1}^{2}\right)\), for any \(t>0\), where \(K_{1}=\max\{\sqrt{20},|\ell|\sqrt{10}\}\)._

Proof.: Let us first explicitly define the pdf of \(\tilde{X}\) as:

\[f_{\mathcal{D}_{\ell}}(x)=\begin{cases}0&\text{ if }x\leq\ell\\ f_{N(0,1)}(x)/\overline{\Phi}(\ell)&\text{ otherwise,}\end{cases}\] (21)

where \(f_{N(0,1)}\) is the pdf of \(N(0,1)\). We prove this in two cases.

_Case \(\ell\leq 2\)._ In this case, one can use the lower bound from Prop. E.2 to show that \(\overline{\Phi}(\ell)\geq\overline{\Phi}(2)>1/50\) by explicit calculation. Thus, \(f_{\mathcal{D}_{\ell}}(x)\leq 50f_{N(0,1)}(x)\) for \(x>0\). We can now obtain the desired bound as follows. Letting \(X\sim N(0,1)\), for any \(t>0\),

\[\Pr\left[\left|\tilde{X}\right|>t\right] = \Pr\left[\tilde{X}>t\right]\] (22) \[\leq 50\Pr\left[X>t\right]\leq 50\Pr\left[\left|X\right|>t\right]\leq 1 00\text{exp}\left(-t^{2}/2\right)\]using (19). Now, it is easy to check that for \(t\geq 3\),

\[\frac{\text{exp}\left(-\frac{t^{2}}{20}\right)}{\text{exp}\left(- \frac{t^{2}}{2}\right)}\geq\text{exp}\left(9\cdot\left(\frac{1}{2}-\frac{1}{20} \right)\right)\geq\text{exp}(4)>50\] \[\Rightarrow 2\cdot\text{exp}\left(-\frac{t^{2}}{20}\right)>100\cdot \text{exp}\left(-\frac{t^{2}}{2}\right)\] (23)

On the other hand, for \(0\leq t<3\), \(2\text{exp}\left(-t^{2}/20\right)>1\). Thus,

\[\Pr\left[\left|\tilde{X}\right|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/20\right)\]

_Case \(\ell>2\)._ In this case using the easily verifiable facts that hold for \(\ell>2\):

* \((1/\ell-1/\ell^{3})>1/(2\ell)\) and
* \(\text{exp}\left(-3\ell^{2}/2\right)\leq\frac{1}{2\ell\sqrt{2\pi}}\),

Prop. E.2 yields

\[\overline{\Phi}(\ell)\geq\left(\frac{1}{\ell}-\frac{1}{\ell^{3}}\right)\frac {1}{\sqrt{2\pi}}\text{exp}\left(-\ell^{2}/2\right)\geq\frac{1}{2\ell\sqrt{2 \pi}}\text{exp}\left(-\ell^{2}/2\right)\geq\text{exp}\left(-2\ell^{2}\right).\] (24)

Observe that \(\Pr\left[\left|\tilde{X}\right|>t\right]=\Pr_{Z\sim N(0,1)}\left[Z>t\,\mid\,Z \geq\ell\right]\). If \(t<\ell\), then this probability vanishes. Otherwise \(t\geq\ell>2\), and from Prop. E.2, \(\Pr[Z>t]\leq\text{exp}(-t^{2}/2)\) and therefore \(\Pr\left[\left|\tilde{X}\right|>t\right]\) can be bounded by

\[\leq\frac{\Pr[Z>t]}{\Pr[Z\geq\ell]}\leq 2\overline{\Phi}(\ell)^{-1}\text{ exp}\left(-t^{2}/2\right)\leq 2\text{exp}\left(-t^{2}/2+2\ell^{2}\right)\] (25)

using (24). Now, if \(t^{2}=5\ell^{2}+\kappa\) for some \(\kappa\geq 0\), then using \(\ell>2\) we have

\[-\frac{t^{2}}{2}+2\ell^{2}=-\frac{\ell^{2}+\kappa}{2}\leq-1-\frac{\kappa}{5 \ell^{2}}=-\frac{5\ell^{2}+\kappa}{5\ell^{2}}\leq-\frac{t^{2}}{5\ell^{2}}\leq -\frac{t^{2}}{10\ell^{2}}.\] (26)

Therefore, \(2\text{exp}\left(-t^{2}/2+2\ell^{2}\right)\leq 2\text{exp}\left(-t^{2}/(10 \ell^{2})\right)\) for \(t^{2}\geq 5\ell^{2}\). On the other hand,

\[2\text{exp}\left(-t^{2}/(10\ell^{2})\right)>2e^{-1/2}>1,\qquad\text{ when }t^{2}<5\ell^{2}.\]

Thus, in this case the following holds for all \(t>0\):

\[\Pr\left[\left|\tilde{X}\right|>t\right]\leq 2\text{exp}\left(-t^{2}/(10\ell^{2})\right)\] (27)

completing the proof. 

The above results also apply to "complements" of the thresholded Gaussians. In particular, let \(\overline{\mathcal{D}}_{\ell}\) be the distribution of \(\{X\sim N(0,1)\,\mid\,X\leq\ell\Leftrightarrow-X\geq-\ell\}\) which is equivalently \(\{-X\sim N(0,1)\,\mid\,X\geq-\ell\}\) to which the above analysis can be directly be applied. This yields, that if \(\tilde{X}\sim\overline{\mathcal{D}}_{\ell}\) then for any \(t>0\),

\[\Pr\left[\left|\tilde{X}\right|>t\right] \leq 2\cdot\text{exp}\left(-t^{2}/2\right), \text{if }\ell\geq 0,\] (28) \[\Pr\left[\left|\tilde{X}\right|>t\right] \leq 2\cdot\text{exp}\left(-t^{2}/K_{1}^{2}\right), \text{if }\ell<0,\] (29)

where \(K_{1}=\max\{\sqrt{20},|\ell|\sqrt{10}\}\).

### Bag mean and covariance estimation error bounds

In this section we shall be concerned with random variables \(\tilde{X}\) which are sampled from \(\mathcal{D}_{\ell}\) with probability \(p\) and from \(\overline{\mathcal{D}}_{\ell}\) with probability \((1-p)\). Let us denote this distribution by \(\hat{\mathcal{D}}(p,\ell)\). Using Lemmas E.3, E.4 and (28), (29), we obtain the following lemma.

**Lemma E.5**.: _Let \(\tilde{X}\sim\hat{\mathcal{D}}(p,\ell)\) for some \(p\in(0,1)\). Then for any \(t>0\),_

\[\Pr\left[|\tilde{X}|>t\right]\leq 2\cdot\text{\rm exp}\left(-t^{2}/K_{1}^{2}\right)\] (30)

_where \(K_{1}=\max\{\sqrt{20},|\ell|\sqrt{10}\}\). In particular, \(\|\tilde{X}\|_{\psi_{2}}=O(|\ell|)\)._

We however, shall also require similar bounds for the mean-zero version of such distributions. To begin with we state an easy lemma bounding the mean of \(\tilde{X}\).

**Lemma E.6**.: _Let \(\tilde{X}\sim\hat{\mathcal{D}}(p,\ell)\) for some \(p\in(0,1)\). Then, \(\left|\mathbb{E}\left[\tilde{X}\right]\right|\leq\gamma_{\ell}=\max\{\gamma_{0 },2\ell\}\), where \(\gamma_{0}>0\) is some constant._

Proof.: Let us consider the case of \(\ell>0\) (and \(\ell\leq 0\) follows analogously). When \(\ell<2\), it is easy to see that desired expectation is \(O(1)\). Further, the expectation over \(\overline{\mathcal{D}}_{\ell}\) is \(O(\ell)\) for any \(\ell>0\), since it is a convex combination of the expectation of a half gaussian which has \(O(1)\) expectation, and a gaussian truncated from below at \(0\) and above at \(\ell\), which has \(O(\ell)\) expectation. To complete the argument we need to bound the expectation over \(\mathcal{D}_{\ell}\). Using (21) and \((1/\sqrt{2\pi})\int_{\ell}^{\infty}x\,\text{\rm exp}(-x^{2}/2)\,dx=(1/\sqrt{2 \pi})\text{\rm exp}(-\ell^{2}/2)\), we obtain \(\mathbb{E}_{X\sim\mathcal{D}_{\ell}}\left[X\right]=(1/\sqrt{2\pi})\text{\rm exp }(-\ell^{2}/2)\overline{\Phi}(\ell)^{-1}\) and the lower bound from Prop. E.2 along with \(\ell\geq 2\) yields an upper bound of \(2\ell\) on the expectation. 

With the setup as in Lemma E.5 define \(\tilde{X}:=\tilde{X}-\mathbb{E}\left[\tilde{X}\right]\). Clearly, \(\mathbb{E}\left[\tilde{X}\right]=0\). Further,

\[\left|\tilde{X}\right|>t\Rightarrow\left|\tilde{X}\right|+\left|\mathbb{E} \left[\tilde{X}\right]\right|>t\Rightarrow\left|\tilde{X}\right|>t-\gamma_{ \ell}.\]

Therefore,

\[\Pr\left[|\tilde{X}|>t\right]\leq\Pr\left[|\tilde{X}|>t-\gamma_{\ell}\right] \leq 2\cdot\text{\rm exp}\left(-(t-\gamma_{\ell})^{2}/K_{1}^{2}\right)\] (31)

Let \(K_{2}=\max\{2K_{1},\sqrt{2}\gamma_{\ell}\}\). Now,

\[t\geq 2\gamma_{\ell}\Rightarrow|t|\leq 2|t-\gamma_{\ell}| \Rightarrow \frac{t^{2}}{4K_{1}^{2}}\leq\frac{(t-\gamma_{\ell})^{2}}{K_{1}^{ 2}}\] (32) \[\Rightarrow 2\cdot\text{\rm exp}\left(-(t-\gamma_{\ell})^{2}/K_{1}^{2}\right) \leq 2\cdot\text{\rm exp}\left(-t^{2}/K_{2}^{2}\right)\]

On the other hand, when \(0\leq t<2\gamma_{\ell}\), \(t^{2}/K_{2}^{2}\geq 1/2\), and thus

\[2\cdot\text{\rm exp}\left(-t^{2}/K_{2}^{2}\right)\geq 2e^{-1/2}>1.\]

Thus,

\[\Pr\left[|\tilde{X}|>t\right]\leq 2\cdot\text{\rm exp}\left(-t^{2}/K_{2}^{2}\right)\] (33)

for all \(t>0\), where \(K_{2}=O(|\ell|)\).

#### e.1.1 Concentration of mean estimate using Hoeffding's Bound

Let us first state the Hoeffding's concentration bound for subgaussian random variables.

**Theorem E.7** (Theorem 2.6.2 of [32]).: _Let \(X_{1},\ldots,X_{N}\) be independent, mean-zero, sub-gaussian random variables. Then, for every \(\varepsilon\geq 0\),_

\[\Pr\left[\left|\frac{\sum_{i=1}^{N}X_{i}}{N}\right|\geq\varepsilon\right]\leq 2 \cdot\text{\rm exp}\left(\frac{-c\varepsilon^{2}N^{2}}{\sum_{i=1}^{N}\|X_{i}\|_ {\psi_{2}}^{2}}\right),\] (34)

_where \(c>0\) is some absolute constant._For the rest of this section we shall fix \(\ell\in\mathbb{R}\) and \(p\in(0,1)\). Consider vector valued random variable \(\mathbf{X}=(X^{(1)},\ldots,X^{(d)})\) with independent coordinates where

* \(X^{(1)}=\tilde{X}-\mathbb{E}[\tilde{X}]\) where \(\tilde{X}\sim\hat{\mathcal{D}}(p,\ell)\). From the previous subsection, we have \(\|X^{(1)}\|_{\psi_{2}}=O(|\ell|)\).
* For \(i=2,\ldots,d\), \(X^{(i)}\sim N(0,1)\) and therefore \(\|X^{(i)}\|_{\psi_{2}}=O(1)\).

Using the above bounds on the subgaussian norms, and applying Theorem E.7 to bound the error in each coordinate by \(\varepsilon/\sqrt{d}\) and taking a union bound we obtain the following lemma.

**Lemma E.8**.: _Let \(\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\) be \(N\) iid samples of \(\mathbf{X}\). Then for every \(\varepsilon\geq 0\),_

\[\Pr\left[\left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}}{N}\right\|_{2}\geq \varepsilon\right]\leq 2\cdot\text{\rm exp}\left(\frac{-c_{0}\varepsilon^{2}N}{dO( \ell^{2})}\right)+2(d-1)\cdot\text{\rm exp}\left(-c_{0}\varepsilon^{2}N/d\right)\] (35)

_for some absolute constant \(c_{0}>0\). In particular, if \(N>O\left((d/\varepsilon^{2})O(\ell^{2})\log(d/\delta)\right)\),_

\[\Pr\left[\left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}}{N}\right\|_{2}\geq \varepsilon\right]\leq\delta,\]

_for any \(\delta>0\)._

#### e.1.2 Concentration of covariance estimate

Consider the vector random variable \(\mathbf{X}\) defined in the previous subsection. It is mean-zero and so by Defn. 3.4.1 and Lemma 3.4.2 of [32],

\[\text{\rm sup}_{\mathbf{x}\in S^{d-1}}\|\langle\mathbf{x},\mathbf{X}\rangle\|_ {\psi_{2}}=O(\ell),\] (36)

using the bounds on the subgaussian norms of the coordinates of \(\mathbf{X}\) given in the previous subsection. Using this we can directly apply Proposition 2.1 of [31] to obtain the following lemma.

**Lemma E.9**.: _Let \(\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\) be \(N\) iid samples of \(\mathbf{X}\), then if \(N>O\left((d/\varepsilon^{2})O(\ell^{2})\log(1/\delta)\right)\),_

\[\Pr\left[\left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{T}} }{N}-\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\mathsf{T}}\right]\right\|_{2}\geq \varepsilon\right]\leq\delta,\] (37)

_for any \(\varepsilon,\delta>0\)._

#### e.1.3 Mean and covariance estimate bounds for non-centered vector r.v.s

**Distribution \(\mathcal{D}_{\text{\rm asymvec}}(p,\ell)\).** We revisit the definition of \(\mathbf{X}\) in Sec. E.1.1 and instead define distribution \(\mathcal{D}_{\text{\rm asymvec}}(p,\ell)\) over \(\mathbf{Z}=(Z^{(1)},\ldots,Z^{(d)})\) with independent coordinates by taking \(Z^{(1)}=\tilde{X}\) where \(\tilde{X}\sim\hat{\mathcal{D}}(p,\ell)\) and for \(i=2,\ldots,d\), \(Z^{(i)}\sim N(0,1)\).

Clearly, \(\mathbf{X}\) = \(\mathbf{Z}-\mathbb{E}[\mathbf{Z}]\). For convenience, we shall use the following notation:

\[\boldsymbol{\mu}_{Z} :=\mathbb{E}[\mathbf{Z}]\] and, \[\hat{\boldsymbol{\mu}}_{Z} :=\frac{\sum_{i=1}^{N}\mathbf{Z}_{i}}{N},\] (38) \[\boldsymbol{\Sigma}_{Z} :=\mathbb{E}[(\mathbf{Z}-\boldsymbol{\mu}_{Z})(\mathbf{Z}- \boldsymbol{\mu}_{Z})^{\mathsf{T}}]\] and, \[\hat{\boldsymbol{\Sigma}}_{Z} :=\frac{\sum_{i=1}^{N}(\mathbf{Z}_{i}-\hat{\boldsymbol{\mu}}_{Z} )(\mathbf{Z}_{i}-\hat{\boldsymbol{\mu}}_{Z})^{\mathsf{T}}}{N},\] (39)

where \(\mathbf{Z}_{i}\) is an iid sample of \(\mathbf{Z}\) and \(\mathbf{X}_{i}=\mathbf{Z}_{i}-\boldsymbol{\mu}_{Z}\), for \(i=1,\ldots,N\). We have the following lemma.

**Lemma E.10**.: _For any \(\varepsilon,\delta\in(0,1)\), if \(N>O\left((d/\varepsilon^{2})O(\ell^{2})\log(d/\delta)\right)\), then w.p. \(1-\delta\) the following hold simultaneously,_

\[\|\hat{\boldsymbol{\mu}}_{Z}-\boldsymbol{\mu}_{Z}\|_{2}\leq \varepsilon/2,\] (40) \[\|\hat{\boldsymbol{\Sigma}}_{Z}-\boldsymbol{\Sigma}_{Z}\|_{2}\leq\varepsilon\] (41)Proof.: We begin by applying Lemmas E.8 and E.9 to \(\mathbf{X}\) and the iid samples \(\mathbf{X}_{1},\ldots,\mathbf{X}_{N}\) so that their conditions hold with \(\varepsilon/2\) and \(\delta/2\). Taking a union bound we get that the following simultaneously hold with probability at least \(1-\delta\):

\[\left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}}{N}\right\|_{2}\leq \varepsilon/2,\] (42) \[\left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{ T}}}{N}-\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\mathsf{T}}\right]\right\|_{2}\leq \varepsilon/2.\] (43)

By the definitions above, (42) directly implies (40).

Now, observe that \(\mathbf{\Sigma}_{Z}=\mathbb{E}[\mathbf{X}\mathbf{X}^{\mathsf{T}}]\). On the other hand, letting \(\boldsymbol{\zeta}:=\hat{\boldsymbol{\mu}}_{Z}-\boldsymbol{\mu}_{Z}=(\sum_{i= 1}^{N}\mathbf{X}_{i})/N\) we simplify \(\hat{\mathbf{\Sigma}}_{Z}\) as,

\[\frac{\sum_{i=1}^{N}(\mathbf{Z}_{i}-\hat{\boldsymbol{\mu}}_{Z})( \mathbf{Z}_{i}-\hat{\boldsymbol{\mu}}_{Z})^{\mathsf{T}}}{N} = \frac{\sum_{i=1}^{N}(\mathbf{X}_{i}-\boldsymbol{\zeta})(\mathbf{ X}_{i}-\boldsymbol{\zeta})^{\mathsf{T}}}{N}\] (44) \[= \frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{T}}- \mathbf{X}_{i}\boldsymbol{\zeta}^{\mathsf{T}}-\boldsymbol{\zeta}\mathbf{X}_{i} ^{\mathsf{T}}+\boldsymbol{\zeta}\boldsymbol{\zeta}^{\mathsf{T}}}{N}\] \[= \frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{T}}}{N} -2\boldsymbol{\zeta}\boldsymbol{\zeta}^{\mathsf{T}}+\boldsymbol{\zeta} \boldsymbol{\zeta}^{\mathsf{T}}\] \[= \frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{T}}}{N} -\boldsymbol{\zeta}\boldsymbol{\zeta}^{\mathsf{T}}\]

Thus, the LHS of (41) is at most,

\[\left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{ T}}}{N}-\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\mathsf{T}}\right]-\boldsymbol{\zeta} \boldsymbol{\zeta}^{\mathsf{T}}\right\|_{2} \leq \left\|\frac{\sum_{i=1}^{N}\mathbf{X}_{i}\mathbf{X}_{i}^{\mathsf{ T}}}{N}-\mathbb{E}\left[\mathbf{X}\mathbf{X}^{\mathsf{T}}\right]\right\|_{2}+ \left\|\boldsymbol{\zeta}\boldsymbol{\zeta}^{\mathsf{T}}\right\|_{2}\] (45) \[\leq \varepsilon/2+\varepsilon^{2}/4\leq\varepsilon,\]

since we have shown that \(\left\|\boldsymbol{\zeta}\right\|_{2}=\left\|\hat{\boldsymbol{\mu}}_{Z}- \boldsymbol{\mu}_{Z}\right\|_{2}\leq\varepsilon/2\). 

Finally, we prove a version of the above lemma under a symmteric psd transformation. Let \(\mathbf{A}\) be a psd matrix s.t. \(\lambda_{\text{max}}\) is the maximum eigenvalue of \(\mathbf{A}^{2}=\mathbf{A}\mathbf{A}\) i.e., \(\sqrt{\lambda_{\text{max}}}\) is the maximum eigenvalue of \(\mathbf{A}\). Then, if we define \(\tilde{\mathbf{Z}}:=\mathbf{A}\mathbf{Z}\) and \(\tilde{\mathbf{Z}}_{i}\) as iid samples of \(\tilde{\mathbf{Z}}\), \(i=1,\ldots,N\) and analogous to (38) and (39), define \(\boldsymbol{\mu}_{\tilde{\boldsymbol{\mu}}_{\tilde{\boldsymbol{Z}}}}\), \(\boldsymbol{\hat{\boldsymbol{\mu}}_{\tilde{\boldsymbol{Z}}}}\), \(\boldsymbol{\Sigma}_{\tilde{\boldsymbol{Z}}}\) and \(\hat{\mathbf{\Sigma}}_{\tilde{\boldsymbol{Z}}}\), we have the following lemma which follows directly from Lemma E.10 the \(\sqrt{\lambda_{\text{max}}}\) upper bound on the operator norm of \(\mathbf{A}\).

**Lemma E.11**.: _For any \(\varepsilon,\delta\in(0,1)\), if \(N>O\left((d/\varepsilon^{2})O(\ell^{2})\log(d/\delta)\right)\), then w.p. \(1-\delta\) the following hold simultaneously,_

\[\|\hat{\boldsymbol{\mu}}_{\tilde{\boldsymbol{Z}}}-\boldsymbol{\mu }_{\tilde{\boldsymbol{Z}}}\|_{2}\leq\varepsilon\sqrt{\lambda_{\text{max}}}/2,\] (46) \[\|\hat{\mathbf{\Sigma}}_{\tilde{\boldsymbol{Z}}}-\boldsymbol{ \Sigma}_{\tilde{\boldsymbol{Z}}}\|_{2}\leq\varepsilon\lambda_{\text{max}}\] (47)

### Estimating Covariance of differences

First we begin this subsection with a simple observation. If \(X\) and \(Y\) are two random variables such that,

\[\Pr\left[|X|>t\right],\Pr\left[|Y|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/K_{1 }^{2}\right),\quad\forall t>0,\] (48)

then \(X-Y\) is a random variable such that,

\[\Pr\left[|X-Y|>t\right]\leq\Pr\left[|X|>t/2\right]+\Pr\left[|Y|>t/2\right] \leq 4\cdot\text{exp}\left(-t^{2}/(2K_{1})^{2}\right).\]

It is easy to see that,

\[4\cdot\text{exp}\left(-t^{2}/(2K_{1})^{2}\right)\leq 2\cdot\text{exp}\left(-t^{2}/ (4K_{1})^{2}\right),\]

when \(t^{2}\geq 8K_{1}^{2}\). On the other hand, when \(t^{2}<8K_{1}^{2}\), \(2\cdot\text{exp}\left(-t^{2}/(4K_{1})^{2}\right)>1\). Thus,

\[\Pr\left[|X-Y|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/(4K_{1})^{2}\right)\] (49)

In this subsection shall consider \(\tilde{X}\sim\mathcal{D}(p,q,\ell)\) to be defined as follows, for some \(\ell\in\mathbb{R}\), \(p,q\in[0,1)\) s.t. \(p+q<1\). \(\tilde{X}=U-V\) where:* with probability \(p\), \(U\sim\mathcal{D}_{\ell}\) and \(V\sim\mathcal{D}_{\ell}\) independently,
* with probability \(q\), \(U\sim\overline{\mathcal{D}}_{\ell}\) and \(V\sim\overline{\mathcal{D}}_{\ell}\) independently,
* with probability \((1-p-q)/2\), \(U\sim\mathcal{D}_{\ell}\) and \(V\sim\overline{\mathcal{D}}_{\ell}\) independently,
* with probability \((1-p-q)/2\), \(U\sim\overline{\mathcal{D}}_{\ell}\) and \(V\sim\mathcal{D}_{\ell}\) independently.

From the above it is clear that \(\mathbb{E}[\tilde{X}]=0\). Further, from (49) and Lemmas E.3, E.4 and (28), (29),

\[\Pr\left[|\tilde{X}|>t\right]\leq 2\cdot\text{exp}\left(-t^{2}/K_{1}^{2}\right)\] (50)

for \(t>0\), where \(K_{1}=\max\{4\sqrt{20},4|\ell|\sqrt{10}\}\). In particular, \(\|\tilde{X}\|_{\psi_{2}}=O(\ell)\).

Let us now define a distribution \(\mathcal{D}_{\text{diffve}}(p,q,\ell)\) vector valued random variable with independent coordinates \(\mathbf{X}=(X^{(1)},\ldots,X^{(d)})\), where

* \(X^{(1)}\sim\mathcal{D}(p,q,\ell)\), for some \(\ell\in\mathbb{R}\), \(p,q\in[0,1)\) s.t. \(p+q<1\).
* \(\mathbf{X}^{(j)}\) is the difference of two iid \(N(0,1)\) random variables, for \(j=2,\ldots,d\). In particular, the subgaussian norm of these coordinates is \(O(1)\).

From, the above it is clear that \(\mathbb{E}[\mathbf{X}]=\mathbf{0}\), and by Defn. 3.4.1 and Lemma 3.4.2 of [32], (36) is applicable to \(\mathbf{X}\) as defined above. Thus, letting \(\hat{\mathbf{X}}:=\mathbf{A}\mathbf{X}\), where \(\mathbf{A}\) is as used in the previous subsection we have,

**Lemma E.12**.: _For \(\mathbf{X}\) defined above, the statement of Lemma E.9 is applicable, and (37) implies that the following holds:_

\[\left\|\frac{\sum_{i=1}^{N}\hat{\mathbf{X}}_{i}\hat{\mathbf{X}}_{i}^{\mathsf{ T}}}{N}-\mathbb{E}\left[\hat{\mathbf{X}}\hat{\mathbf{X}}^{\mathsf{T}}\right] \right\|_{2}\leq\varepsilon\lambda_{\max}\] (51)

_with probability \(1-\delta\)._

### Proof of Lemma 3.1

Our proof shall utilize the following normalization of LTFs in a Gaussian space.

**Lemma E.13**.: _Suppose \(f(\mathbf{X})=\mathsf{pos}\left(\mathbf{r}^{\mathsf{T}}\mathbf{X}+c\right)\), where \(\|\mathbf{r}\|_{2}>0\) and \(\mathbf{X}\sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})\) s.t. \(\boldsymbol{\Sigma}\) is positive definite. Let \(\boldsymbol{\Gamma}:=\boldsymbol{\Sigma}^{1/2}\) be symmetric p.d., and \(\mathbf{U}\) be any orthonormal transformation satisfying \(\mathbf{U}\mathbf{r}/\|\mathbf{r}\|_{2}=\mathbf{e}_{1}\) where \(\mathbf{e}_{1}\) is the vector with \(1\) in the first coordinate and \(0\) in the rest. Then, letting \(\mathbf{Z}\sim N(\mathbf{0},\mathbf{I})\) so that \(\mathbf{X}=\boldsymbol{\Gamma}\mathbf{U}^{\mathsf{T}}\mathbf{Z}+\boldsymbol{\mu}\),_

\[f(\mathbf{X})=\mathsf{pos}\left(\mathbf{r}^{\mathsf{T}}\mathbf{X}+c\right)= \mathsf{pos}\left(\mathbf{e}_{1}^{\mathsf{T}}\mathbf{Z}+\ell\right)\] (52)

_where \(\ell=\left(\mathbf{r}^{\mathsf{T}}\boldsymbol{\mu}+c\right)/\|\boldsymbol{ \Gamma}\mathbf{r}\|_{2}\)._

Proof.: We have \(\mathbf{X}=\hat{\mathbf{X}}+\boldsymbol{\mu}\) where \(\hat{\mathbf{X}}\sim N(\mathbf{0},\boldsymbol{\Sigma})\). Thus, \(\hat{\mathbf{X}}=\boldsymbol{\Gamma}\mathbf{U}^{\mathsf{T}}\mathbf{Z}\Rightarrow \mathbf{Z}=\mathbf{U}\boldsymbol{\Gamma}^{-1}\hat{\mathbf{X}}\), using \(\mathbf{U}^{\mathsf{T}}=\mathbf{U}^{-1}\). Now, \(f(\mathbf{X})\) can be written as

\[\mathsf{pos}\left(\frac{\mathbf{r}^{\mathsf{T}}(\hat{\mathbf{X}}+ \boldsymbol{\mu})+c}{\|\boldsymbol{\Gamma}\mathbf{r}\|_{2}}\right) = \mathsf{pos}\left(\frac{\mathbf{r}^{\mathsf{T}}\boldsymbol{\Gamma} \mathbf{U}^{\mathsf{T}}\mathbf{U}\boldsymbol{\Gamma}^{-1}\hat{\mathbf{X}}+ \mathbf{r}^{\mathsf{T}}\boldsymbol{\mu}+c}{\|\boldsymbol{\Gamma}\mathbf{r}\|_{2}}\right)\] (53) \[= \mathsf{pos}\left(\frac{(\mathbf{U}\mathbf{\Gamma}\mathbf{r})^{ \mathsf{T}}\mathbf{Z}}{\|\boldsymbol{\Gamma}\mathbf{r}\|_{2}}+\frac{\mathbf{r}^{ \mathsf{T}}\boldsymbol{\mu}+c}{\|\boldsymbol{\Gamma}\mathbf{r}\|_{2}}\right)\] \[= \mathsf{pos}\left(\mathbf{e}_{1}^{\mathsf{T}}\mathbf{Z}+\frac{ \mathbf{r}^{\mathsf{T}}\boldsymbol{\mu}+c}{\|\boldsymbol{\Gamma}\mathbf{r}\|_{2 }}\right)\]

which completes the proof. 

Proof.: (of Lemma 3.1) Using the normalization in Lemma E.13, we can write \(\mathbf{X}=\mathbf{A}\mathbf{Z}+\boldsymbol{\mu}\) where \(\mathbf{A}=\boldsymbol{\Sigma}^{1/2}\mathbf{U}^{\mathsf{T}}\) such that \(\mathbf{X}\sim N(\boldsymbol{\mu},\boldsymbol{\Sigma})\equiv\mathbf{Z}\sim N( \mathbf{0},\mathbf{I})\). From the condition in (52) we can write the samples in Step 2 of Alg. 1 as \(\mathbf{x}_{i}=\mathbf{A}\mathbf{z}_{i}+\boldsymbol{\mu}\) where \(\mathbf{z}_{i}\) are sampled from \(\mathcal{D}_{\text{asymwee}}(k/q,-\ell)\) (see Sec.

E.1.3) for \(\ell\) as given in Lemma E.13. Note that the maximum eigenvalue of \(\mathbf{A}^{2}\) is \(\lambda_{\text{max}}\) which is the maximum eigenvalue of \(\bm{\Sigma}\). Thus, one can apply Lemma E.11 to \(\hat{\bm{\mu}}_{B}\) and \(\hat{\bm{\Sigma}}_{B}\).

Further, the difference vectors sampled in Step 6 can be written as \(\overline{\mathbf{x}}_{i}=\mathbf{A}\bar{\mathbf{z}}_{i}\) where \(\bar{\mathbf{z}}_{i}\) are sampled from \(\mathcal{D}_{\text{diffvec}}(p,p^{\prime},-\ell)\) (see Sec. E.2) where \(p=\binom{k}{2}/\binom{q}{2}\) is the probability of sampling a pair of \(1\)-labeled feature-vectors from a bag, and \(p^{\prime}=\binom{q-k}{2}/\binom{q}{2}\) is that of sampling a pair of \(0\)-labeled feature-vectors. Thus, one can apply Lemma E.12 to \(\hat{\bm{\Sigma}}_{D}\).

Using both the above applications with the error probability \(\delta/2\) and using union bound we complete the proof. 

## Appendix F Experimental Details and Results

### Implementation Details

The implementations of the algorithms in this paper (Algs. 2, 3, 4) and of the random LTF algorithm are in python using numpy libraries. The code for the SDP algorithms of [25, 26] for bag sizes \(2\) and \(3\) is adapted from the publicly available codebase1. The experimental code was executed on a 16-core CPU and 128 GB RAM machine running linux in a standard python environment.

Footnote 1: https://github.com/google-research/google-research/tree/master/Algorithms_and_Hardness_for_Learning_Linear_Thresholds_from_Label_Proportions (license included in the repository)

### Experimental Results

In the following \(d\) denotes the dimension of the feature-vectors, \(q\) the size of the bags, with \(k/q\in(0,1)\) the bag label proportion, and \(m\) be the number of sampled bags in the training dataset. The instance-level test set is of size 1000 in all the experiments, and the reported metric is the accuracy over the test set.

**Standard Gaussian without LTF offset.** Here we primarily wish to evaluate the Algorithm 3 using unbalanced bag oracles such that \(k\neq q/2\). For \(d\in\{10,50\}\), \((q,k)\in\{(3,1),(10,8),(50,35)\}\) and \(m\in\{100,500,2000\}\) we create 25 datasets. In each LLP dataset, we (i) sample a random unit vector \(\mathbf{r}^{*}\) and let \(f(\mathbf{x}):=\text{pos}\left(\mathbf{r}^{*}\mathbf{T}\mathbf{x}\right)\), (ii) sample \(m\) training bags from \(\mathsf{Ex}(f,N(\mathbf{0},\mathbf{I}),q,k)\), (iii) sample 1000 test instances \((\mathbf{x},f(\mathbf{x}))\), \(\mathbf{x}\gets N(\mathbf{0},\mathbf{I})\). We also evaluate the Algorithm 2 on these datasets. For comparison we have the random LTF algorithm (R) in which we sample 100 random LTFs and return the one that performs best on the training set. The results are reported in Table 5. We also evaluate the SDP algorithm (S) in [26] for \((q,k)=(3,1)\) using \(m\in\{50,100,200\}\) (since the SDP algorithms do not scale to larger number of bags) whose results are reported in Table 6.

Table 2 reports a concise set of comparative scores of Algorithms 2, 3 and random LTF (R) using 2000 bags and of the SDP algorithms (S) with 200 bags.

**Centered and general Gaussian.** Here we evaluate Algorithms 2 and 4, and we have both balanced as well as unbalanced bag oracles. In particular, for \(d\in\{10,50\}\), \((q,k)\in\{(2,1),(3,1),(10,5),(10,8),(50,25),(50,35)\}\) and \(m\in\{100,500,2000\}\) we create 25 datasets similar to the previous case, except that for each dataset we first sample \(\bm{\mu}\) and \(\bm{\Sigma}\) and use \(N(\bm{0},\bm{\Sigma})\) for sampling feature-vectors in the centered Gaussian case and use \(N(\bm{\mu},\bm{\Sigma})\) for sampling feature-vectors in the general Gaussian case. We perform the following set of experiments in each case. For the cases when bags are balanced, i.e. \((q,k)\in\{(2,1),(10,5),(50,25)\}\), for each our Algorithms 2 and 4 we evaluate their two possible solutions on the test data and report the better number.

* **With LTF offset.** We sample \((\mathbf{r}_{*},c_{*})\) and create a dataset using \(\text{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x}+c_{*})\) as the labeling function. Table 11 reports the test accuracy scores for Algorithm 4 and random LTF (R) with \(m=\{100,500,2000\}\) for centered and general Gaussians. Table 12 reports the corresponding scores for the SDP algorithm (S) [25, 26] with \(m=\{50,100,2000\}\) and \((q,k)\in\{(2,1),(3,1)\}\). Table 4 provides concise comparative scores with \(m=2000\) for Algorithm 4 and random LTF and \(m=200\) for the SDP algorithm (S).
* **Without LTF offset.** We sample an \(\mathbf{r}_{*}\) and create a dataset using \(\text{pos}(\mathbf{r}_{*}^{\mathsf{T}}\mathbf{x})\) as the labeling function. Table 7 reports the test accuracy scores for Algorithm 2 and random LTF (R)

[MISSING_PAGE_FAIL:30]

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(d\) & \(m\) & S \\ \hline
10 & 50 & 67.86\({}_{\pm 6.49}\) \\
10 & 100 & 66.25\({}_{\pm 5.87}\) \\
10 & 200 & 68.04\({}_{\pm 6.82}\) \\
50 & 50 & 58.09\({}_{\pm 3.24}\) \\
50 & 100 & 56.46\({}_{\pm 1.83}\) \\
50 & 200 & 57.55\({}_{\pm 3.27}\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: SDP Algorithm (S) on standard gaussian feature vectors

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \(d\) & \(q\) & \(k\) & A4 & R & S \\ \hline

[MISSING_PAGE_POST]

 \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of Algorithm A4, rand. LTF (R) and SDP algorithms (S) with offset

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \(d\) & \(q\) & \(k\) & \(m\) & A3 & A2 & R \\ \hline

[MISSING_PAGE_POST]

 \hline \hline \end{tabular}
\end{table}
Table 5: Our algorithms A1 and A2 vs. rand. LTF (R) on \(N(\mathbf{0},\mathbf{I})\) feature-vectors.

\begin{table}

\end{table}
Table 7: Our algorithms A2 vs. rand. LTF (R) without offset

\begin{table}

\end{table}
Table 8: SDP Algorithm S without offset

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_EMPTY:35]

Class ratio estimation for LTFs

The work of [13] studies the problem of matching the classifier label proportion using a single sampled bag which they call _class-ratio_ (CR) learning as distinct from LLP. Indeed, in LLP the goal is to learn an accurate instance-level classifier from multiple sampled bags, whereas CR-learning does not guarantee instance-level performance. Further, similar to Prop. 18 of [13], CR learning LTFs over Gaussians is easy: for a bag \(B=\{\mathbf{x}^{(i)}\}_{i=1}^{n}\) of iid Gaussian points, a random unit vector \(\mathbf{r}\) has distinct inner products \(\{s_{i}:=\mathbf{r}^{\mathsf{T}}\mathbf{x}^{(i)}\}_{i=1}^{n}\) with probability 1. The LTFs \(\{\text{pos}\left(\mathbf{r}^{\mathsf{T}}\mathbf{x}-s\right)\ |\ s\in\{-\infty,s_{1}, \ldots,s_{n}\}\}\) achieve all possible target label proportions \(\{j/n\}_{j=0}^{n}\), and one can then apply the generalization error bound in Thm. 4 of [13].

## Appendix H Analysis of a Mixture of Label Sums

**Definition H.1** (Mixed Bag Oracle).: _Given a set of bag oracles \(\text{\sc Ex}(f,\mathcal{D},q,k)\) for \(k\in\{0,\ldots,q\}\) and \(\mathbf{p}=(p_{0},\ldots,p_{q})\in\Delta^{q}\) where \(\Delta^{q}\) is a \(q\)-simplex, a mixed bag oracle \(\text{\sc Ex}(f,\mathcal{D},q,\mathbf{p})\) samples a bag size \(k\) from \(\text{\sc Multinoulli}(\mathbf{p})\) distribution2 and then samples a bag from \(\text{\sc Ex}(f,\mathcal{D},q,k)\)._

Footnote 2: Section 2.3.2 (p. 35) of ‘Machine Learning: A Probabilistic Perspective’ (by K. Murphy)

Let \(\mathbf{\Sigma}_{D}\) be the covariance matrix of difference of a pair of vectors sampled u.a.r without replacement from \(\text{\sc Ex}(f,\mathcal{D},q,\mathbf{p})\) and \(\mathbf{\Sigma}_{B}\) be the covariance matrix of vectors sampled u.a.r from \(\text{\sc Ex}(\hat{f},\mathcal{D},q,\mathbf{p})\). If \(\mathbf{\Sigma}_{Dk}\) is the covariance matrix of difference of a pair of vectors sampled u.a.r without replacement from \(\text{\sc Ex}(f,\mathcal{D},q,k)\) and \(\mathbf{\Sigma}_{Bk}\) be the covariance matrix of vectors sampled u.a.r from \(\text{\sc Ex}(f,\mathcal{D},q,k)\) then we have the following

\[\mathbf{\Sigma}_{B}=\sum_{k=0}^{q}p_{k}^{2}\mathbf{\Sigma}_{Bk} \mathbf{\Sigma}_{D}=\sum_{k=0}^{q}p_{k}^{2}\mathbf{\Sigma}_{Dk}\] (54)

Using the above, we prove the following geometric error bound which is analogous to Lemma 4.1.

**Lemma H.2**.: _For any \(\varepsilon,\delta\in(0,1)\), if \(m\geq O\left((d/\varepsilon^{4})\log(d/\delta)(\lambda_{\text{max}}/\lambda_{ \text{min}})^{4}q^{4}\left(1/\sum_{k=1}^{q-1}p_{k}^{2}\right)^{2}\right)\), then \(\hat{\mathbf{r}}\) computed in Step 3 of Alg. 2 satisfies \(\min\{\|\hat{\mathbf{r}}-\mathbf{r}_{*}\|_{2},\|\hat{\mathbf{r}}+\mathbf{r}_{*} \|_{2}\}\leq\varepsilon,\text{w.p. }1-\delta/2\)._

### Proof of Lemma H.2

We define and bound the following useful quantities based on \(q\), \(\lambda_{\text{max}}\), \(\lambda_{\text{min}}\) and \(\mathbf{p}\).

**Definition H.3**.: _Define, (i) \(\kappa_{1}(k):=\left(\frac{2k}{q}-1\right)^{2}\frac{2}{\pi}\) so that \(0\leq\kappa_{1}(k)\leq 2/\pi\), (ii) \(\kappa_{2}(k):=\frac{1}{q-1}\frac{k}{q}\left(1-\frac{k}{q}\right)\frac{16}{\pi}\) so that \(\frac{16}{\pi q^{2}}\leq\kappa_{2}(k)\leq\frac{4}{\pi(q-1)}\) whenever \(1\leq k\leq q-1\), (iii) \(\kappa_{3}(\mathbf{p}):=\frac{\sum_{k=0}^{q}(p_{k}^{2}\kappa_{2}(k)}{\sum_{k=0 }^{q}p_{k}^{2}-\sum_{k=0}^{q}p_{k}^{2}\kappa_{1}(k)}\) so that \(\frac{16}{\pi q^{2}\sum_{k=0}^{q-1}p_{k}^{2}}\leq\kappa_{3}(\mathbf{p})\), and (iv) \(\theta(\mathbf{p}):=\frac{2\lambda_{\text{max}}}{\lambda_{\text{min}}}\left( \frac{2\lambda_{\text{max}}}{\sum_{k=0}^{q}p_{k}^{2}-\max(0,2\sum_{k=0}^{q} \kappa_{1}(k)-\sum_{k=0}^{q}p_{k}^{2}\kappa_{2}(k))}+\frac{1}{\sum_{k=0}^{q}p_ {k}^{2}-\sum_{k=0}^{q}p_{k}^{2}\kappa_{1}(k)}\right)\) so that \(\theta(\mathbf{p})\leq\frac{3\lambda_{\text{max}}}{(1-2/\pi)\lambda_{\text{min} }(\sum_{k=0}^{q}p_{k}^{2})}\)._

**Lemma H.4**.: _The ratio \(\rho(\mathbf{r}):=\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{D}\mathbf{r}/\mathbf{ r}^{\mathsf{T}}\mathbf{\Sigma}_{B}\mathbf{r}\) is maximized when \(\mathbf{r}=\pm\mathbf{r}_{*}\). Moreover,_

\[\rho(\mathbf{r})=2+\frac{\gamma(\mathbf{r})^{2}\sum_{k=0}^{q}p_{k}^{2}\kappa_{2}( k)}{\sum_{k=0}^{q}p_{k}^{2}-\gamma(\mathbf{r})^{2}\sum_{k=0}^{q}p_{k}^{2}\kappa_{1}(k)} \qquad\text{where}\qquad\gamma(\mathbf{r}):=\frac{\mathbf{r}^{\mathsf{T}} \mathbf{\Sigma}\mathbf{r}_{*}}{\sqrt{\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma} \mathbf{r}}\sqrt{\mathbf{r}^{\mathsf{T}}_{*}\mathbf{\Sigma}\mathbf{r}_{*}}}\text { and }\]

\[\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{B}\mathbf{r}=\mathbf{r}^{\mathsf{T}} \mathbf{\Sigma}\mathbf{r}\left(\sum_{k=0}^{q}p_{k}^{2}-\gamma(\mathbf{r})^{2} \sum_{k=0}^{q}p_{k}^{2}\kappa_{1}(k)\right),\]

\[\mathbf{r}^{\mathsf{T}}\mathbf{\Sigma}_{D}\mathbf{r}=\mathbf{r}^{\mathsf{T}} \mathbf{\Sigma}\mathbf{r}\left(2\sum_{k=0}^{q}p_{k}^{2}-2\gamma(\mathbf{r})^{2} \sum_{k=0}^{q}p_{k}^{2}\kappa_{1}(k)+\gamma(\mathbf{r})^{2}\sum_{k=0}^{q}p_{k}^{2 }\kappa_{2}(k)\right)\]

[MISSING_PAGE_EMPTY:37]