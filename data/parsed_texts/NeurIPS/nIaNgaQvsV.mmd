# PromptRestorer: A Prompting Image Restoration Method with Degradation Perception

Cong Wang\({}^{1}\), Jinshan Pan\({}^{2}\), Wei Wang\({}^{3}\), Jiangxin Dong\({}^{2}\), Mengzhu Wang\({}^{4}\),

**Yakun Ju\({}^{1}\), Junyang Chen\({}^{5}\)**

\({}^{1}\)The Hong Kong Polytechnic University, \({}^{2}\)Nanjing University of Science and Technology,

\({}^{3}\)Dalian University of Technology, \({}^{4}\)Hebei University of Technology, \({}^{5}\)Shenzhen University

These authors equally contribute to this work.

###### Abstract

We show that raw degradation features can effectively guide deep restoration models, providing accurate degradation priors to facilitate better restoration. While networks that do not consider them for restoration forget gradually degradation during the learning process, model capacity is severely hindered. To address this, we propose a **Prompt**ing image **Restorer**, termed as **PromptRestorer**. Specifically, PromptRestorer contains two branches: a restoration branch and a prompting branch. The former is used to restore images, while the latter perceives degradation priors to prompt the restoration branch with reliable perceived content to guide the restoration process for better recovery. To better perceive the degradation which is extracted by a pre-trained model from given degradation observations, we propose a prompting degradation perception modulator, which adequately considers the characters of the self-attention mechanism and pixel-wise modulation, to better perceive the degradation priors from global and local perspectives. To control the propagation of the perceived content for the restoration branch, we propose gated degradation perception propagation, enabling the restoration branch to adaptively learn more useful features for better recovery. Extensive experimental results show that our PromptRestorer achieves state-of-the-art results on 4 image restoration tasks, including image deraining, deblurring, dehazing, and desnowing.

## 1 Introduction

Image restoration aims to recover clear high-quality images from given degraded ones. It is highly ill-posed since only degraded images can be exploited, statistical observations are thus required to well-pose the problems [37, 67, 66, 41, 42]. Although conventional approaches can recover images to some extent, they typically involve solving optimization algorithms that are difficult due to the non-convexity and non-smooth problems. Additionally, the observations may not always hold, which can cause algorithms to fail.

With the emergence of convolutional neural networks (CNNs) [48] and Transformers [22, 45], which perform well at implicitly learning the priors from large-scale data, learning-based methods have dominated recent image restoration tasks and achieved impressive performance [81, 51, 103, 72, 61, 93, 33, 13]. However, these methods are usually built without explicitly considering the specific degradation information, which accordingly limits model capacity (**Case 1** in Fig. 1). An alternative approach is to design a conditional branch to learn additional information to provide the restoration network with useful content for modulation [30, 17, 34, 35] (**Case 2** in Fig. 1). However, we note that while conditional branches in these models are learnable, they may not effectively provide degradation information, as the optimizable parameters result in gradually clearer features during the learning process, leading to the degradation vanishing which accordingly limits model performance.

Recently, prompt learning has been shown an effective tool to improve model performance by designing various prompts [115; 98; 104; 23; 114; 46; 53; 43]. The prompt usually serves as the guidance tool to correct the networks toward better results [28]. However, prompt learning still keeps a margin for image restoration, and existing prompts may not be suitable for image restoration since they cannot effectively model degradation priors well. Hence, we ask: _Is there a reasonable prompting manner to correct degraded image restoration networks to facilitate better recovery?_

_The answer is in the riddle_. This paper proposes the **PromptRestorer**, a **Prompt**ing image **Restorer**, to overcome degradation vanishing in image restoration via promoting by exploring degradation input itself for better restoration (**Case 3** in Fig. 1). Our idea is simple: we directly exploit the raw degraded features extracted by a pre-trained model from the degraded inputs to generate more reliable prompting content to guide image restoration. Raw degraded features preserve accurately degraded information, which can consistently prompt the restoration network with accurate degraded priors, enabling the restoration network to perceive the degradation for better restoration. Hence, we design the PromptRestorer, which consists of two branches: (a) the restoration branch and (b) the prompting branch. The former is used to restore images and the latter is used to generate reliable prompting features to guide the restoration network for better restoration. To better perceive the degradation, we propose a **Prompting**Degradation **P**erception **M**odulator (**PromptDPM**), which consists of **G**lobal **P**rompting **P**erceptor (**G2P**) and **L**ocal **P**rompting **P**erceptor (**L2P**). The G2P adequately exploits the self-attention mechanism to form global prompting attention, while the L2P considers the pixel-level perception to build local prompting content. To control the propagation of perceived features in the restoration branch, we propose **G**ated **D**egradation Perception **P**ropagation (**GDP**), enabling the restoration network to adaptively learn more useful features to facilitate better restoration.

The main contributions of this work are summarized below:

* We propose PromptRestorer, which is the first approach to our knowledge that takes advantage of the prompting learning for general image restoration by considering raw degradation features in restoration, enabling the restoration model to overcome degradation vanishing while consistently retaining the degradation priors to facilitate better restoration.
* We propose a prompting degradation perception modulator that is used to perceive degradation from global and local perspectives, which is able to provide the restoration network

Figure 1: **(a)** compares different restoration frameworks. Unlike existing approaches that are built within the architectures such as **Cases 1-2**, which are unable to memorize the degradation well during the learning process, we propose a prompting method (**Case 3**) that directly exploits raw degradation features extracted by a pre-trained model from the given degradation observations to guide restoration. In **(b)**, we observe that both **Cases 1-2** outperform our method in early iterations, as they effectively memorize degraded information. However, both **Cases 1-2** experience degradation vanishing with further iterations (better demonstrated in Sec. 4.3), while our prompting method persists in guiding the restoration network with accurate degradation priors, accordingly producing better restoration quality. In **(c)**, visual performance demonstrates that our prompting method recovers sharper images. Quantitative results are reported in Tab. 5.

with more reliable perceived content learned from the degradation priors, enabling it to better guide the restoration process.
* We propose gated degradation perception propagation that exploits a gating mechanism to control the propagation of the perceived features, enabling the model to adaptively learn more useful features for better image restoration.

Fig. 1 summarises framework comparisons, and their learning curves and visual performance. Deeper analysis and discussion about them are provided in Sec. 4.2.

## 2 Related Work

In this section, we review image restoration, conditional modulation, and prompt learning.

**Image Restoration.** Recently, CNN-based architectures [110; 112; 103; 4; 24; 102; 89; 91; 87; 19; 88; 117] and Transformer-based models [96; 56; 49; 12; 93; 93] have been shown to outperform conventional restoration approaches [37; 83; 64; 47; 7; 79]. These learning-based methods usually adopt U-Net architectures [50; 18; 103; 100; 1; 93; 108; 101], which have been demonstrated the effectiveness because of hierarchical multi-scale representation and effective learning between shallow and deeper layers by skip connection [111; 59; 102; 31]. We refer the readers to recent excellent literature reviews on image restoration [5; 54; 82], which summarise the main designs in deep image restoration models.

Although these models have achieved promising performance, they do not explicitly take degradation into consideration for model design which is vital for restoration, limiting the model capacity.

**Conditional Modulation.** Conditional modulation usually involves implicitly modulating the additional content to guide the restoration [30; 10; 16; 17; 36; 35; 34; 60; 57; 92]. These approaches usually contain two branches: a basic network and a conditional network. The conditional network provides additional information to guide the basic network for restoration via spatial feature transform (SFT) [92]. Among these methods, blur kernel [30], semantics [92], and degraded input [57; 16] which serve as the additional conditions are broadly known.

The learnable nature of the conditional network in these models does not effectively provide degradation information for the basic network. As parameters are optimized in the learning process, features become gradually clear.

**Prompt Learning.** Prompt learning methods have been studied broadly in natural language processing (NLP) [75; 78; 8]. Due to high effectiveness, prompt learning is recently used in vision-related tasks [115; 98; 104; 23; 114; 46; 53; 43; 71; 29; 40; 86; 28]. In vision prompt learning, many works seek useful prompts to correct task networks toward better performance [28].

Although prompt learning has shown promise in various vision tasks, it still keeps a margin in general image restoration. This paper proposes an effective prompting method, enabling the restoration model to overcome the degradation vanishing in the learning process for better restoration.

## 3 PromptRestorer

Our goal aims to overcome degradation vanishing and better perceive degradation in deep restoration models to improve image recovery quality. To achieve this, we introduce a prompting strategy that helps the model consistently memorize degradation information, enabling it to prompt restoration with better degradation for better restoration. To better perceive degradation, we propose the **Prompting********************(**Degradation **P**ereception **M**odulator (**PromptDPD**), which can provide more reliable perceived content to guide the restoration network. To control the propagation of the perceived content, we propose the **G**ated **D**egradation Perception **P**ropagation (**GDP**), enabling the restoration network to adaptively learn more useful features for better restoration.

### Overall Pipeline

Fig. 2 shows the framework of our PromptRestorer, which contains two branches: (a) the restoration branch and (b) the prompting branch. The restoration branch is used to restore images, where each block is prompted by the prompting branch. The prompting branch first generates the accurate degradation feature extracted by a pre-trained model, and then the feature is to prompt the restoration branch, enabling the restoration branch to better perceive the degradation prior for better recovery.

**Restoration Branch.** Given a degraded input image \(\mathbf{I}\in\mathbb{R}^{H\times W\times 3}\), we first applies a \(3\times 3\) convolution as the feature extraction to obtain low-level embeddings \(\mathbf{X}_{0}\in\mathbb{R}^{H\times W\times C}\); where \(H\times W\) denotes the spatial dimension and \(C\) is the number of channels. Next, the shallow features \(\mathbf{X}_{0}\) gradually are hierarchically encoded into deep features \(\mathbf{X}_{l}\in\mathbb{R}^{\frac{H}{l}\times\frac{N^{l}}{l}\times lC}\). After encoding the degraded input into low-resolution latent features \(\mathbf{X}_{3}\in\mathbb{R}^{\frac{H}{l}\times\frac{N^{l}}{3}\times 3C}\), the decoder progressively recovers the high-resolution representations. Finally, a reconstruction layer which contains 4 Transformer blocks as the refinement followed by a \(3\times 3\) convolution is applied to decoded features to generate residual image \(\mathbf{S}\in\mathbb{R}^{H\times W\times 3}\) to which degraded image is added to obtain the restored output image: \(\mathbf{\hat{H}}=\mathbf{I}+\mathbf{S}\). Both encoder and decoder at \(l\)- level consist of multiple **C**ontinuous **G**ated **T**ransformers (**CGT**) with expanding channel capacity. To help better recovery, the encoder features are concatenated with the decoder features via skip connections [74] by \(1\times\)1 convolutions.

**Prompting Branch.** The prompting branch, as shown in Fig. 2(b), aims to generate and perceive degradation features and then provide useful guidance content for the restoration branch. We note VQGAN [25] has been demonstrated that it can generate high-quality images while representing the features of input images. However, it tends to damage image structure after vector quantization [116, 32]. To avoid this problem, we only exploit the encoder of pre-trained VQGAN to represent the deep features of the degraded inputs. We first use the pretrained encoder to extract degraded features \(\mathbf{Y}_{l}\in\mathbb{R}^{\frac{H}{l}\times\frac{N^{l}}{l}\times lC}\); where \(l\) denotes the \(l\)- level layer in the pre-trained encoder. Then, the degraded features are exploited to generate reliable prompting content to prompt the restoration branch by PromptDPM (see Sec. 3.2). The generated prompting content is transmitted to each CGT to guide the restoration branch.

**Continuous Gated Transformers.** CGT exploits the perceived features from PromptDPM to provide the Transformer block with more reliable content to overcome degradation vanishing to facilitate better restoration. Each CGT consists of three Transformer blocks (Fig. 2(c)) with residual connections [38] and the input in each block is gated by GDP (see Sec. 3.3) to control the propagation of perceived features. Let \(\mathcal{P}\), \(\mathcal{G}\), and \(\mathcal{T}\) respectively denote the operations of PromptDPM (expressed in (2)), GDP (expressed in (7)), and Transformer, the features flow in \(k^{th}\) block in one CGT at \(l\)- level encoder/decoder, which can be expressed as:

\[\mathbf{X}_{k}=\mathcal{T}(\mathbf{G}_{k-1});\mathbf{G}_{k-1}=\mathcal{G}\big{(} \mathbf{X}_{k-1},\mathbf{P}_{l});\mathbf{P}_{l}=\mathcal{P}(\mathbf{X}_{k-1}, \mathbf{Y}_{l}),\] (1)

Figure 2: Overall pipeline of our **PromptRestorer**. PromptRestorer contains two branches: **(a)** the restoration branch and **(b)** the prompting branch. The restoration branch is used to restore images, where each block **(c)** in CGT is prompted by the prompting branch. The prompting branch first generates precise degradation features extracted by a pre-trained model from degradation observations, then these features prompt the restoration branch to facilitate better restoration via PromptDPM **(d)**.

where \(\mathbf{X}_{k}\) means the output of \(k^{th}\) Transformer block in one CGT, especially \(\mathbf{X}_{0}\) is the downsampled/upsampled features at \((l-1)\)- level encoder/decoder; \(\mathbf{P}_{l}\) refers to the generated features of PromptDPM at \(l\)-level layer; \(\mathbf{G}_{k-1}\) means the gated features between \(\mathbf{X}_{k-1}\) and \(\mathbf{P}_{l}\), which serves as the input of \(k^{th}\) Transformer block. Each Transformer block consists of multi-head attention [101] followed by an improved ConvNeXt [62] as the feed-forward network (see Fig. 2(c)).

### Prompting Degradation Perception Modulator

To better perceive the degradation to prompt the restoration network with more reliable perceived content from the degradation priors, we propose the PromptDPM (see Fig. 2(d)). The PromptDPM consists of 1) **G**lobal **P**rompting **P**erceptor (**G2P**, introduced in Sec. 3.2.1) and 2) **L**ocal **P**rompting **P**erceptor (**L2P**, introduced in Sec. 3.2.2) to respectively perceive the degradation from global and local perspectives, enabling to generate more useful content to guide the restoration branch. From a restoration tensor \(\mathbf{X}\in\mathbb{R}^{H\times W\times\hat{C}}\) and a degradation tensor \(\mathbf{Y}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\), we prompt \(\mathbf{X}\) with \(\mathbf{Y}\):

\[\mathcal{P}(\mathbf{X},\mathbf{Y})=W_{p}\Big{(}\mathcal{C}\big{[}\Psi^{ \mathbf{global}}(\mathbf{X},\mathbf{Y}),\Psi^{\mathbf{local}}(\mathbf{X}, \mathbf{Y})\big{]}\Big{)}+\mathbf{X},\] (2)

where \(\Psi^{\mathbf{global}}(\cdot,\cdot)\) and \(\Psi^{\mathbf{local}}(\cdot,\cdot)\) respectively denote the operations of G2P and L2P; \(\mathcal{C}[\cdot,\cdot]\) means the concatenation at channel dimension; \(W_{p}(\cdot)\) refers to the \(1\times 1\) point-wise convolution.

#### 3.2.1 Global Prompting Perceptor

The G2P, shown in Fig. 3(a), fully exploits the self-attention mechanism to form the global prompting attention induced by the degraded features. The G2P contains the global perception attention followed by an improved ConvNeXt [62]. Our global perception attention consists of 1) **Q**uery-**I**nduced **A**ttention (**Q**-**InAtt**) and 2) **K**ey-**V**alue-**I**nduced **A**ttention (**KV**-**InAtt**). The Q-InAtt considers re-forming the query vector induced by degradation features to build a representative query to perform attention, while the KV-InAtt re-considers key and value vectors induced by other degradation counterparts to search for more similar content with the restoration query. From a layer normalized restoration tensor \(\mathbf{X}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\), our G2P first generates _restoration query_ (**Q**), _key_ (**K**), and _value_ (**V**) projections from the restoration features. It is achieved by applying \(1{\times}1\) convolutions to aggregate pixel-wise cross-channel context followed by \(3{\times}3\) depth-wise convolutions \(W_{d}(\cdot)\) to encode channel-wise spatial context, yielding \(\mathbf{Q}{=}W_{d}W_{p}\mathbf{X}\), \(\mathbf{K}{=}W_{d}W_{p}\mathbf{X}\), and \(\mathbf{V}{=}W_{d}W_{p}\mathbf{X}\). Meanwhile, we similarly convert the degradation tensor \(\mathbf{Y}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\) into _degradation query_ (\(\widetilde{\mathbf{Q}}\)), _key_ (\(\widetilde{\mathbf{K}}\)), and _value_ (\(\widetilde{\mathbf{V}}\)) projections: \(\widetilde{\mathbf{Q}}{=}W_{d}W_{p}\mathbf{Y}\), \(\widetilde{\mathbf{K}}{=}W_{d}W_{p}\mathbf{Y}\), and \(\widetilde{\mathbf{V}}{=}W_{d}W_{p}\mathbf{Y}\). Then, we respectively conduct Q-InAtt and KV-InAtt:

\[\mathbf{A}_{\text{Q-InAtt}}=\mathcal{A}_{\text{Q-InAtt}}\Big{(}W_{p}\big{(} \mathcal{C}[\mathbf{Q},\widetilde{\mathbf{Q}}]\big{)},\mathbf{K},\mathbf{V} \Big{)};\mathbf{A}_{\text{KV-InAtt}}=\mathcal{A}_{\text{KV-InAtt}}\Big{(} \mathbf{Q},W_{p}\big{(}\mathcal{C}[\mathbf{K},\widetilde{\mathbf{K}}]\big{)},W_{p}(\mathcal{C}[\mathbf{V},\widetilde{\mathbf{V}}]\big{)}\Big{)},\] (3)

where \(\mathcal{A}_{(\cdot)}\left(\hat{\mathbf{Q}},\hat{\mathbf{K}},\hat{\mathbf{V}} \right)=\hat{\mathbf{V}}\cdot\text{Softmax}\left(\hat{\mathbf{K}}\cdot\hat{ \mathbf{Q}}/\alpha\right)\); Here, \(\alpha\) is a learnable scaling parameter to control

Figure 3: **(a)** Global Prompting Perceptor (**G2P**); **(b)** Local Prompting Perceptor (**L2P**).

the magnitude of the dot product of \(\mathbf{K}\) and \(\mathbf{\hat{Q}}\) before applying the softmax function. Similar to the conventional multi-head SA [22], we divide the number of channels into 'heads' and learn separate attention maps. Then two induced attentions are fused and followed by an improved ConvNeXt:

\[\mathbf{A}^{{}^{\prime}}=W_{p}\big{(}\mathcal{C}[\mathbf{A}_{\text{Q-InAn}}, \mathbf{A}_{\text{KV-InAn}}]\big{)}+\mathbf{X};\mathbf{A}=W_{p}W_{d}\phi W_{p}W_ {d}\big{(}LN(\mathbf{A}^{{}^{\prime}})\big{)}+\mathbf{A}^{{}^{\prime}},\] (4)

where the \(W_{p}W_{d}\phi W_{p}W_{d}(\cdot)\) means the improved ConvNeXt shown in the latter of Fig. 2(c); \(LN(\cdot)\) means the operation of layer normalization [6].

#### 3.2.2 Local Prompting Perceptor

The L2P, as shown in Fig. 3(b), adequately considers the pixel-level degradation perception, enabling to better perceive degradation from spatially neighboring pixel positions. The L2P consists of a local perception modulator followed by a separable depth-level convolution. The local perception modulator contains two core components: 1) **Deg**radation-**Induced **Band** (**Deg**-**InBan**) and 2) **Rest**oration-**Induced **Band** (**Res**-**InBan**). The former is achieved by exploiting the degradation features to induce spatially useful content from restoration content to guide restoration gating fusion, while the latter utilizes the deep restoration features to induce more useful features from another degradation counterpart to form the degradation gating. Given the degradation tensor \(\mathbf{Y}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\), we first exploit the point-wise convolution and \(3\times 3\) depth-wise convolution to encode two _degradation_ projections, yielding \(\mathbf{\tilde{Q}}\)=\(W_{d}^{Q}W_{p}^{Q}\mathbf{Y}\) and \(\mathbf{\tilde{K}}\)=\(W_{d}^{K}W_{p}^{K}\mathbf{Y}\). Meanwhile, the restoration tensor \(\mathbf{X}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\) are also encoded into two _restoration_ projections: \(\mathbf{Q}\)=\(W_{d}^{Q}W_{p}^{Q}\mathbf{X}\) and \(\mathbf{K}\)=\(W_{d}^{K}W_{p}^{K}\mathbf{X}\). Then, we respectively conduct Deg-InBan and Res-InBan:

\[\mathbf{Z}_{\text{Deg-InBan}}=\sigma\Big{(}W_{p}\phi W_{d}(\mathcal{C}[ \mathbf{\tilde{Q}},\mathbf{Q}])\Big{)}\odot\mathbf{K};\,\mathbf{Z}_{\text{ Res-InBan}}=\mathbf{\tilde{Q}}\odot\sigma\Big{(}W_{\phi}W_{d}(\mathcal{C}[ \mathbf{\tilde{K}},\mathbf{K}])\Big{)},\] (5)

where \(\sigma(\cdot)\) denotes the sigmoid function that controls the gating level. Then, the perceived features in the two bands are fused via concatenation and \(1\times 1\) convolution and followed by a depth-level separable convolution \(W_{p}\phi W_{d}(\cdot)\):

\[\mathbf{Z}^{{}^{\prime}}=W_{p}\big{(}\mathcal{C}[\mathbf{Z}_{\text{Deg-InBan} },\mathbf{Z}_{\text{Res-InBan}}]\big{)}+\mathbf{X};\mathbf{Z}=W_{p}\phi W_{d} (\mathbf{Z}^{{}^{\prime}})+\mathbf{Z}^{{}^{\prime}}.\] (6)

### Gated Degradation Perception Propagation

The GDP aims to control the propagation of the perceived degradation, enabling to adaptively learn more useful features in Transformer blocks to facilitate better restoration. Given the output restoration tensor \(\mathbf{X}_{k-1}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\) of \((k-1)^{th}\) Transformer block in one CGT and the perceived tensor \(\mathbf{P}_{l}\in\mathbb{R}^{\hat{H}\times\hat{W}\times\hat{C}}\) which is the output feature of one PromptDPM at \(l\)- level, the input of \(k^{th}\) Transformer block can be obtained by gating the \(\mathbf{X}_{k-1}\) with \(\mathbf{P}_{l}\) by \(1\times 1\) convolution and gated control function sigmoid \(\sigma(\cdot)\) with residual learning [39]:

\[\mathcal{G}\big{(}\mathbf{X}_{k-1},\mathbf{P}_{l}\big{)}=\sigma(W_{p}\mathbf{P} _{l})\odot\mathbf{X}_{k-1}+\mathbf{X}_{k-1}.\] (7)

### Learning Strategy

To train the network, two objective loss functions are adopted, including image reconstruction loss (\(\mathcal{L}_{i}\)) for pixel recovery and frequency loss (\(\mathcal{L}_{f}\)) for detail enhancement [18]:

\[\mathcal{L}=\mathcal{L}_{i}+\lambda\mathcal{L}_{f},\text{where }\mathcal{L}_{i}=\| \mathbf{\hat{H}}-\mathbf{H}\|_{1};\;\mathcal{L}_{f}=\|\mathcal{F}(\mathbf{\hat {H}})-\mathcal{F}(\mathbf{H})\|_{1},\] (8)

where \(\mathbf{H}\) denotes the ground truth image; \(\mathcal{F}\) denotes the Fast Fourier transform; \(\lambda\) is a weight that is empirically set to be 0.1.

## 4 Experiment

We evaluate PromptRestorer on benchmarks for 4 image restoration tasks: **(a)** deraining, **(b)** deblurring, **(c)** desnowing, and **(d)** dehazing. We train separate models for different image restoration tasks. Our PromptRestorer employs a 3-level encoder-decoder. From level-1 to level-3, the number of CGT is \([2,3,6]\), attention heads are \([2,4,8]\), and number of channels is \([48,96,192]\). The expanding channel capacity factor \(\beta\) is 4. For downsampling and upsampling, we adopt pixel-unshuffle and pixel-shuffle [77], respectively. We train models with AdamW optimizer with the initial learning rate \(3e^{-4}\) gradually reduced to \(1e^{-6}\) with the cosine annealing [63]. The patch size is set as \(256\times 256\).

[MISSING_PAGE_FAIL:7]

**Image Dehazing Results.** We perform the image dehazing experiments on both synthetic benchmark RESIDE SOTS-Indoor [52], and real-world hazy benchmarks Dense-Haze [2] and NH-Haze [3]. Tab. 3 summarise the quantitative results. Compared to the recent works DeHamer [33] and MAXIM [84], our method receives \(4.33\) dB and \(5.91\) dB PSNR gains on the SOTS-Indoor, respectively. On the real-world benchmark NH-Haze [3], our PromptRestorer can achieve \(0.7203\) of the SSIM result, which is a new record and significantly outperforms current state-of-the-art approaches DeHamer [33]. The results on both synthetic and real-world benchmarks have demonstrated the effectiveness of our PromptRestorer on the image dehazing task. Fig. 6 shows the visual results, where our PromptRestorer is more effective in removing haze than other methods.

**Image Desnowing Results.** For the image desnowing task, we compare our PromptRestorer on the CSD [15], SRRS [14], and Snow100K [61] datasets with existing state-of-the-art methods [61, 14, 15, 13, 85]. We also compare recent Transformer-based general image restoration approaches Restormer [101] and Uformer [93]. As shown in Tab. 4, our PromptRestorer yields a \(2.05\) dB PSNR improvement over the state-of-the-art approach [101] on the CSD benchmark [15]. The visual results in Fig. 7 show that our PromptRestorer is able to remove spatially varying snowflakes than competitors.

\begin{table}
\begin{tabular}{l|l|l l l l l l l l} \hline \hline
**Benchmark** & **Metrics** & **DempNet [61]** & **JSTASR [14]** & **HDCW-Net [15]** & **TransWeather [85]** & **MSP-Former [13]** & **Ufomer [94]** & **Restorer [101]** & **PromptRestorer** \\ \hline \multirow{2}{*}{**SSD (2009)**[15]} & PSNR \(\uparrow\) & 20.13 & 27.96 & 29.96 & 31.76 & 33.75 & 33.80 & **35.43** & **37.48** \\  & SSIM \(\uparrow\) & 0.81 & 0.88 & 0.91 & 0.93 & 0.96 & 0.96 & **0.97** & **0.99** \\ \hline \multirow{2}{*}{**SRES (2009)**[14]} & PSNR \(\uparrow\) & 20.38 & 25.82 & 27.78 & 28.29 & 30.76 & 30.12 & **32.24** & **33.99** \\  & SSIM \(\uparrow\) & 0.84 & 0.89 & 0.92 & 0.92 & 0.95 & 0.96 & **0.96** & **0.96** & **0.99** \\ \hline \multirow{2}{*}{**Snow100K (2000)**[61]} & PSNR \(\uparrow\) & 30.50 & 23.12 & 31.54 & 31.82 & 31.43 & 33.81 & **34.67** & **36.62** \\  & SSIM \(\uparrow\) & 0.94 & 0.86 & 0.95 & 0.95 & **0.96** & 0.94 & 0.95 & **0.97** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Image desnowing** results on CSD (2000) [65], SRRS (2000) [76], and Snow100K (2000) [73]. Our PromptRestorer achieves the best metrics on all datasets on the image desnowing problem.

\begin{table}
\begin{tabular}{l|l|l l l l l l l l l l l} \hline \hline
**Benchmark** & **Metrics** & **DempNet [61]** & **JSTASR [14]** & **HDCW-Net [15]** & **TransWeather [85]** & **MSP-Former [13]** & **Ufomer [94]** & **Restorer [101]** & **PromptRestorer** \\ \hline \multirow{2}{*}{**SSD (2009)**[15]} & PSNR \(\uparrow\) & 20.13 & 27.96 & 29.96 & 31.76 & 33.75 & 33.80 & **35.43** & **37.48** \\  & SSIM \(\uparrow\) & 0.81 & 0.88 & 0.91 & 0.93 & 0.96 & 0.96 & **0.97** & **0.99** \\ \hline \multirow{2}{*}{**SRES (2009)**[14]} & PSNR \(\uparrow\) & 20.38 & 25.82 & 27.78 & 28.29 & 30.76 & 30.12 & **32.24** & **33.99** \\  & SSIM \(\uparrow\) & 0.84 & 0.89 & 0.92 & 0.92 & 0.95 & 0.95 & 0.96 & **0.96** & **0.96** & **0.99** \\ \hline \multirow{2}{*}{**Snow100K (2000)**[61]} & PSNR \(\uparrow\) & 30.50 & 23.12 & 31.54 & 31.82 & 31.43 & 33.81 & **34.67** & **36.62** \\  & SSIM \(\uparrow\) & 0.94 & 0.86 & 0.95 & 0.95 & 0.96 & 0.94 & 0.95 & **0.97** \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Image desnowing** results on CSD (2000) [65], SRRS (2000) [76], and Snow100K (2000) [73]. Our PromptRestorer achieves the best metrics on all datasets on the image desnowing problem.

Figure 6: **Image dehazing** example on SOTS-Indoor [52].

Figure 7: **Image desnowing** example on CSD (2000) [15].

### Analysis and Discussion

For ablation experiments, following [84; 20], we train the image deblurring model on GoPro dataset [65] for \(1000\) epochs only and set the number of Transformer in each CGT is 1. Params mean the number of learnable parameters. Testing is performed on the GoPro testing dataset [65]. FLOPs are computed on image size \(256{\times}256\). Next, we describe the influence of each component individually.

**Effect on Prompting.** The core design of our PromptRestorer is the 'prompting', which exploits a pre-trained model to extract raw degradation features from the degraded observations and then generate perceived content to guide the restoration branch (i.e., **Case 3** in Fig. 1). Compared to existing frameworks such as **Cases 1-2** in Fig. 1, our proposed prompting strategy shows superior performance, as demonstrated in Tab. 5. Our method achieved \(0.877\) dB gains compared to **Case 1**, and \(0.369\) dB higher than **Case 2**. Interestingly, the learnable condition branch in **Case 2**2, despite consuming more FLOPs and Params, results in worse performance than ours. Our approach directly exploits raw degradation features to prompt restoration with persistent degradation priors to facilitate better recovery. Fig. 1(c) shows two examples, where our model that exploits raw degradation features as prompting generates sharper and clearer images.

**Effect on PromptDPM.** We analyze the impact of PromptDPM on restoration quality in Tab. 6 by disabling one component at a time. Each model in L2P and G2P consumes similar Params and FLOPs, while our full model achieves the best performance. Disabling L2P or G2P results in a decrease in performance by \(0.196\) dB and \(0.318\) dB, respectively. These experiments conclusively demonstrate the effectiveness of each component in L2P and G2P for restoration.

**Effect on GDP.** To understand the impact of GDP, we disable it to compare with full model in Tab. 7. Note that the computational cost of the GDP is negligible compared to disabling it as it only involves a 1\({\times}\)1 convolution and sigmoid function for the gating mechanism, while it leads to a gain of \(0.091\) dB. This finding highlights the significance of controlling the propagation of the perceived degradation features.

### Visualization Understanding for Degradation Vanishing

To emphasize the understanding of degradation vanishing, we visualize the features learned in the condition/prompting branches to better comprehend the learned status of these branches in Fig. 8. Notably, both **Cases 1-2** exhibit sharper results in later iterations compared to earlier ones, which fail to provide the restoration branch with sufficient degraded information and cause the restoration

\begin{table}
\begin{tabular}{l|c|c c} \hline \hline Experiment & PSNR & FLOPs (G) & Params (M) \\ \hline w/o L2P & 30.819 & 148.34 & 12.60 \\ w/o Res-InBan & 30.952 & 153.39 & 12.97 \\ w/o Deg-InBan & 30.964 & 153.39 & 12.97 \\ \hline Full (_Ours_) & **31.015** & 157.04 & 13.24 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Ablation experiments on PromptDPM.**

\begin{table}
\begin{tabular}{l|c|c c} \hline \hline Experiment & PSNR & FLOPs (G) & Params (M) \\ \hline w/o GDP & 30.924 & 154.60 & 12.95 \\ \hline w/ GDP (_Ours_) & **31.015** & 157.04 & 13.24 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Effect on GDP.** Our GDP which controls the degradation propagation is effective.

\begin{table}
\begin{tabular}{l|c|c c} \hline \hline Case in Fig. 1 & PSNR & FLOPs (G) & Params (M) \\ \hline
1 & 30.138 & 105.58 & 10.16 \\
2 & 30.646 & 157.04 & 16.77 \\ \hline
3 (_Ours_) & **31.015** & 157.04 & 13.24 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Effect on prompting.** Our method that directly exploits the raw degradation to prompt restoration performs better.

models to not perceive the degradation well, thereby hindering the model capacity. In contrast, as the restoration branch needs to adapt perceived features from the PromptDPM which is to perceive the raw degradation features from inputs, our model (**Case 3**) initially exhibits inferior performance (around 20K iterations) as shown in Fig. 1(b). However, with better adaptation to the degradation information after more iterations, the prompting branch can better prompt the restoration branch consistently with more reliable perceived content learned from the raw degradation, enabling our restoration branch to overcome degradation vanishing and improve restoration quality, as shown in Fig. 1(c).

## 5 Concluding Remarks

In this paper, we investigate the degradation vanishing in the learning process for image restoration. To solve this problem, we have proposed the PromptRestorer which explores the raw degradation features extracted by a pre-trained model from the given degraded observations to guide the restoration process to facilitate better recovery. Extensive experiments have demonstrated that our PromptRestorer favors against state-of-the-art approaches on 4 restoration tasks, including image deraining, deblurring, dehazing, and desnowing.

## References

* [1] A. Abuolaim and M. S. Brown. Defocus deblurring using dual-pixel data. In _ECCV_, 2020.
* [2] C. O. Ancuti, C. Ancuti, M. Sbert, and R. Timofte. Dense-haze: A benchmark for image dehazing with dense-haze and haze-free images. In _ICIP_, pages 1014-1018, 2019.
* [3] C. O. Ancuti, C. Ancuti, and R. Timofte. Nh-haze: An image dehazing benchmark with non-homogeneous hazy and haze-free images. In _CVPR workshops_, pages 444-445, 2020.
* [4] S. Anwar and N. Barnes. Densely residual laplacian super-resolution. _TPAMI_, 2020.
* [5] S. Anwar, S. Khan, and N. Barnes. A deep journey into super-resolution: A survey. _ACM Computing Surveys_, 2019.
* [6] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. _arXiv:1607.06450_, 2016.
* [7] D. Berman, T. Treibitz, and S. Avidan. Non-local image dehazing. In _CVPR_, pages 1674-1682, 2016.
* [8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. _arXiv:2005.14165_, 2020.
* [9] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet: An end-to-end system for single image haze removal. _IEEE TIP_, 25(11):5187-5198, 2016.
* [10] H. Cai, J. He, Y. Qiao, and C. Dong. Toward interactive modulation for photo-realistic image restoration. In _CVPR Workshops_, pages 294-303, 2021.
* [11] C. Chen, X. Shi, Y. Qin, X. Li, X. Han, T. Yang, and S. Guo. Real-world blind super-resolution via feature matching with implicit high-resolution priors. In _ACM MM_, pages 1329-1338, 2022.
* [12] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao. Pre-trained image processing transformer. In _CVPR_, 2021.

Figure 8: **Visualization**. We show the average features over the channel dimension in the condition/prompting branches for the second example in Fig. 1(c). We obtain GT/degraded features by inputting GT/degraded images into the pre-trained VQGAN. As single-branch models (**Case 1** in Fig. 1) do not have condition branches, we visualize the last layer in the 1-level encoder for reference.

* [13] S. Chen, T. Ye, Y. Liu, T. Liao, Y. Ye, and E. Chen. Msp-former: Multi-scale projection transformer for single image desnowing. _arXiv preprint arXiv:2207.05621_, 2022.
* [14] W.-T. Chen, H.-Y. Fang, J.-J. Ding, C.-C. Tsai, and S.-Y. Kuo. Jstasr: Joint size and transparency-aware snow removal algorithm based on modified partial convolution and veiling effect removal. In _ECCV_, pages 754-770, 2020.
* [15] W.-T. Chen, H.-Y. Fang, C.-L. Hsieh, C.-C. Tsai, I. Chen, J.-J. Ding, S.-Y. Kuo, et al. All snow removed: Single image desnowing algorithm using hierarchical dual-tree complex wavelet representation and contradict channel loss. In _ICCV_, pages 4196-4205, 2021.
* [16] X. Chen, Y. Liu, Z. Zhang, Y. Qiao, and C. Dong. Hdrunet: Single image hdr reconstruction with denoising and dequantization. In _CVPR Workshops_, pages 354-363, 2021.
* [17] X. Chen, Z. Zhang, J. S. Ren, L. Tian, Y. Qiao, and C. Dong. A new journey from sdrtv to hdrtv. In _ICCV_, pages 4500-4509, 2021.
* [18] S.-J. Cho, S.-W. Ji, J.-P. Hong, S.-W. Jung, and S.-J. Ko. Rethinking coarse-to-fine approach in single image deblurring. In _ICCV_, 2021.
* [19] X. Cui, C. Wang, D. Ren, Y. Chen, and P. Zhu. Semi-supervised image deraining using knowledge distillation. _IEEE TCSVT_, 32(12):8327-8341, 2022.
* [20] Y. Cui, Y. Tao, Z. Bing, W. Ren, X. Gao, X. Cao, K. Huang, and A. Knoll. Selective frequency network for image restoration. In _ICLR_, 2023.
* [21] H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M. Yang. Multi-scale boosted dehazing network with dense feature fusion. In _CVPR_, pages 2154-2164, 2020.
* [22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [23] Y. Du, F. Wei, Z. Zhang, M. Shi, Y. Gao, and G. Li. Learning to prompt for open-vocabulary object detection with vision-language model. In _CVPR_, pages 14064-14073, 2022.
* [24] A. Dudhane, S. W. Zamir, S. Khan, F. Khan, and M.-H. Yang. Burst image restoration and enhancement. In _CVPR_, 2022.
* [25] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In _CVPR_, pages 12873-12883, 2021.
* [26] X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley. Clearing the skies: A deep network architecture for single-image rain removal. _TIP_, 2017.
* [27] X. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley. Removing rain from single images via a deep detail network. In _CVPR_, 2017.
* [28] Y. Gan, X. Ma, Y. Lou, Y. Bai, R. Zhang, N. Shi, and L. Luo. Decorate the newcomers: Visual domain prompt for continual test time adaptation. In _AAAI_, 2023.
* [29] P. Gao, S. Geng, R. Zhang, T. Ma, R. Fang, Y. Zhang, H. Li, and Y. Qiao. Clip-adapter: Better vision-language models with feature adapters. _arXiv preprint arXiv:2110.04544_, 2021.
* [30] J. Gu, H. Lu, W. Zuo, and C. Dong. Blind super-resolution with iterative kernel correction. In _CVPR_, 2019.
* [31] S. Gu, Y. Li, L. V. Gool, and R. Timofte. Self-guided network for fast image denoising. In _ICCV_, 2019.
* [32] Y. Gu, X. Wang, L. Xie, C. Dong, G. Li, Y. Shan, and M. Cheng. VQFR: blind face restoration with vector-quantized dictionary and parallel decoder. In _ECCV_, pages 126-143, 2022.
* [33] C.-L. Guo, Q. Yan, S. Anwar, R. Cong, W. Ren, and C. Li. Image dehazing transformer with transmission-aware 3d position embedding. In _CVPR_, pages 5812-5820, 2022.
* [34] J. He, C. Dong, Y. Liu, and Y. Qiao. Interactive multi-dimension modulation for image restoration. _TPAMI_, 44(12):9363-9379, 2022.

* [35] J. He, C. Dong, and Y. Qiao. Interactive multi-dimension modulation with dynamic controllable residual learning for image restoration. In A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, editors, _ECCV_, volume 12365, pages 53-68, 2020.
* [36] J. He, Y. Liu, Y. Qiao, and C. Dong. Conditional sequential modulation for efficient global image retouching. In _ECCV_, volume 12358, pages 679-695. Springer, 2020.
* [37] K. He, J. Sun, and X. Tang. Single image haze removal using dark channel prior. _TPAMI_, 2010.
* [38] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [39] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [40] R. Herzig, O. Abramovich, E. Ben-Avraham, A. Arbelle, L. Karlinsky, A. Shamir, T. Darrell, and A. Globerson. Promptonomyvit: Multi-task prompt learning improves video transformers using synthetic scene data. _CoRR_, abs/2212.04821, 2022.
* [41] Z. Hu, S. Cho, J. Wang, and M.-H. Yang. Deblurring low-light images with light streaks. In _CVPR_, 2014.
* [42] J.-B. Huang, A. Singh, and N. Ahuja. Single image super-resolution from transformed self-exemplars. In _CVPR_, 2015.
* [43] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim. Visual prompt tuning. In _ECCV_, pages 709-727, 2022.
* [44] K. Jiang, Z. Wang, P. Yi, B. Huang, Y. Luo, J. Ma, and J. Jiang. Multi-scale progressive fusion network for single image deraining. In _CVPR_, 2020.
* [45] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah. Transformers in vision: A survey. _arXiv:2101.01169_, 2021.
* [46] M. U. Khattak, H. A. Rasheed, M. Maaz, S. Khan, and F. S. Khan. Maple: Multi-modal prompt learning. _CoRR_, abs/2210.03117, 2022.
* [47] J. Kopf, B. Neubert, B. Chen, M. Cohen, D. Cohen-Or, O. Deussen, M. Uyttendaele, and D. Lischinski. Deep photo: Model-based photograph enhancement and viewing. _ACM TOG_, 2008.
* [48] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In _NIPS_, 2012.
* [49] M. Kumar, D. Weissenborn, and N. Kalchbrenner. Colorization transformer. In _ICLR_, 2021.
* [50] O. Kupyn, T. Martyniuk, J. Wu, and Z. Wang. DeblurGAN-v2: Deblurring (orders-of-magnitude) faster and better. In _ICCV_, 2019.
* [51] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng. Aod-net: All-in-one dehazing network. In _ICCV_, pages 4780-4788, 2017.
* [52] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and Z. Wang. Benchmarking single-image dehazing and beyond. _TIP_, 28(1):492-505, 2019.
* [53] M. Li, L. Chen, Y. Duan, Z. Hu, J. Feng, J. Zhou, and J. Lu. Bridge-prompt: Towards ordinal action understanding in instructional videos. In _CVPR_, pages 19880-19889, 2022.
* [54] S. Li, I. B. Araujo, W. Ren, Z. Wang, E. K. Tokuda, R. H. Junior, R. Cesar-Junior, J. Zhang, X. Guo, and X. Cao. Single image deraining: A comprehensive benchmark analysis. In _CVPR_, 2019.
* [55] X. Li, J. Wu, Z. Lin, H. Liu, and H. Zha. Recurrent squeeze-and-excitation context aggregation net for single image deraining. In _ECCV_, 2018.
* [56] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte. SwinIR: Image restoration using swin transformer. In _ICCV Workshops_, 2021.
* [57] X. Liu, J. Hu, X. Chen, and C. Dong. Udc-unet: Under-display camera image restoration via u-shape dynamic network. In L. Karlinsky, T. Michaeli, and K. Nishino, editors, _ECCV Workshops_, volume 13805, pages 113-129, 2022.
* [58] X. Liu, Y. Ma, Z. Shi, and J. Chen. Gridehazenet: Attention-based multi-scale network for image dehazing. In _ICCV_, pages 7313-7322, 2019.

* [59] X. Liu, M. Suganuma, Z. Sun, and T. Okatani. Dual residual networks leveraging the potential of paired operations for image restoration. In _CVPR_, 2019.
* [60] Y. Liu, J. He, X. Chen, Z. Zhang, H. Zhao, C. Dong, and Y. Qiao. Very lightweight photo retouching network with conditional sequential modulation. _TMM_, 2022.
* [61] Y.-F. Liu, D.-W. Jaw, S.-C. Huang, and J.-N. Hwang. Desnownet: Context-aware deep network for snow removal. _TIP_, 27(6):3064-3073, 2018.
* [62] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie. A convnet for the 2020s. In _CVPR_, pages 11976-11986, 2022.
* [63] I. Loshchilov and F. Hutter. SGDR: Stochastic gradient descent with warm restarts. In _ICLR_, 2017.
* [64] T. Michaeli and M. Irani. Nonparametric blind super-resolution. In _ICCV_, 2013.
* [65] S. Nah, T. Hyun Kim, and K. Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In _CVPR_, 2017.
* [66] J. Pan, Z. Hu, Z. Su, and M.-H. Yang. \(l_{0}\) -regularized intensity and gradient prior for deblurring text images and beyond. _TPAMI_, 39(2):342-355, 2017.
* [67] J. Pan, D. Sun, H. Pfister, and M.-H. Yang. Blind image deblurring using dark channel prior. In _CVPR_, 2016.
* [68] D. Park, D. U. Kang, J. Kim, and S. Y. Chun. Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training. In _ECCV_, 2020.
* [69] K. Purohit, M. Suin, A. Rajagopalan, and V. N. Boddeti. Spatially-adaptive image restoration using distortion-guided networks. In _ICCV_, 2021.
* [70] X. Qin, Z. Wang, Y. Bai, X. Xie, and H. Jia. Ffa-net: Feature fusion attention network for single image dehazing. In _AAAI_, volume 34, pages 11908-11915, 2020.
* [71] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.
* [72] D. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng. Progressive image deraining networks: A better and simpler baseline. In _CVPR_, 2019.
* [73] J. Rim, H. Lee, J. Won, and S. Cho. Real-world blur dataset for learning and benchmarking deblurring algorithms. In _ECCV_, 2020.
* [74] O. Ronneberger, P. Fischer, and T. Brox. U-Net: convolutional networks for biomedical image segmentation. In _MICCAI_, 2015.
* [75] T. Schick and H. Schutze. Exploiting cloze-questions for few-shot text classification and natural language inference. In _EACL_, pages 255-269, 2021.
* [76] Z. Shen, W. Wang, X. Lu, J. Shen, H. Ling, T. Xu, and L. Shao. Human-aware motion deblurring. In _ICCV_, 2019.
* [77] W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In _CVPR_, 2016.
* [78] T. Shin, Y. Razeghi, R. L. L. IV, E. Wallace, and S. Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In _EMNLP_, pages 4222-4235, 2020.
* [79] H. Singh, A. Kumar, L. K. Balyan, and G. K. Singh. A novel optimally gamma corrected intensity span maximization approach for dark image enhancement. In _DSP_, pages 1-5, 2017.
* [80] M. Suin, K. Purohit, and A. N. Rajagopalan. Spatially-attentive patch-hierarchical network for adaptive motion deblurring. In _CVPR_, 2020.
* [81] X. Tao, H. Gao, X. Shen, J. Wang, and J. Jia. Scale-recurrent network for deep image deblurring. In _CVPR_, 2018.

* [82] C. Tian, L. Fei, W. Zheng, Y. Xu, W. Zuo, and C.-W. Lin. Deep learning on image denoising: An overview. _Neural Networks_, 2020.
* [83] R. Timofte, V. De Smet, and L. Van Gool. Anchored neighborhood regression for fast example-based super-resolution. In _ICCV_, 2013.
* [84] Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li. Maxim: Multi-axis mlp for image processing. In _CVPR_, pages 5769-5780, 2022.
* [85] J. M. J. Valanarasu, R. Yasarla, and V. M. Patel. Transweather: Transformer-based restoration of images degraded by adverse weather conditions. In _CVPR_, pages 2353-2363, 2022.
* [86] C. Wang, J. Pan, W. Lin, J. Dong, and X.-M. Wu. Selfpromer: Self-prompt dehazing transformers with depth-consistency. _arXiv preprint arXiv:2303.07033_, 2023.
* [87] C. Wang, J. Pan, and X. Wu. Online-updated high-order collaborative networks for single image deraining. In _AAAI_, pages 2406-2413, 2022.
* [88] C. Wang, Y. Wu, Z. Su, and J. Chen. Joint self-attention and scale-aggregation for self-calibrated deraining network. In _ACM MM_, pages 2517-2525, 2020.
* [89] C. Wang, X. Xing, Y. Wu, Z. Su, and J. Chen. DCSFN: deep cross-scale fusion network for single image rain removal. In _ACM MM_, pages 1643-1651. ACM, 2020.
* [90] C. Wang, X. Xing, Y. Wu, Z. Su, and J. Chen. DCSFN: deep cross-scale fusion network for single image rain removal. In _ACM MM_, pages 1643-1651, 2020.
* [91] C. Wang, H. Zhu, W. Fan, X. Wu, and J. Chen. Single image rain removal using recurrent scale-guide networks. _Neurocomputing_, 467:242-255, 2022.
* [92] X. Wang, K. Yu, C. Dong, and C. C. Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In _CVPR_, 2018.
* [93] Z. Wang, X. Cun, J. Bao, and J. Liu. Uformer: A general u-shaped transformer for image restoration. _arXiv:2106.03106_, 2021.
* [94] Z. Wang, X. Cun, J. Bao, W. Zhou, J. Liu, and H. Li. Uformer: A general u-shaped transformer for image restoration. In _CVPR_, pages 17683-17693, 2022.
* [95] W. Wei, D. Meng, Q. Zhao, Z. Xu, and Y. Wu. Semi-supervised transfer learning for image rain removal. In _CVPR_, 2019.
* [96] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo. Learning texture transformer network for image super-resolution. In _CVPR_, 2020.
* [97] W. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan. Deep joint rain detection and removal from a single image. In _CVPR_, 2017.
* [98] Y. Yao, A. Zhang, Z. Zhang, Z. Liu, T. Chua, and M. Sun. CPT: colorful prompt tuning for pre-trained vision-language models. _CoRR_, abs/2109.11797, 2021.
* [99] R. Yasarla and V. M. Patel. Uncertainty guided multi-scale residual learning-using a cycle spinning cnn for single image de-raining. In _CVPR_, 2019.
* [100] Z. Yue, Q. Zhao, L. Zhang, and D. Meng. Dual adversarial network: Toward real-world noise removal and noise generation. In _ECCV_, 2020.
* [101] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang. Restormer: Efficient transformer for high-resolution image restoration. In _CVPR_, pages 5718-5729, 2022.
* [102] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang, and L. Shao. Learning enriched features for real image restoration and enhancement. In _ECCV_, 2020.
* [103] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang, and L. Shao. Multi-stage progressive image restoration. In _CVPR_, 2021.
* [104] Y. Zang, W. Li, K. Zhou, C. Huang, and C. C. Loy. Unified vision and language prompt learning. _CoRR_, abs/2210.07225, 2022.

* [105] H. Zhang, Y. Dai, H. Li, and P. Koniusz. Deep stacked hierarchical multi-patch network for image deblurring. In _CVPR_, 2019.
* [106] H. Zhang and V. M. Patel. Density-aware single image de-raining using a multi-stream dense network. In _CVPR_, 2018.
* [107] H. Zhang, V. Sindagi, and V. M. Patel. Image de-raining using a conditional generative adversarial network. _TCSVT_, 2019.
* [108] K. Zhang, Y. Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte. Plug-and-play image restoration with deep denoiser prior. _TPAMI_, 2021.
* [109] K. Zhang, W. Luo, Y. Zhong, L. Ma, B. Stenger, W. Liu, and H. Li. Deblurring by realistic blurring. In _CVPR_, 2020.
* [110] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. Image super-resolution using very deep residual channel attention networks. In _ECCV_, 2018.
* [111] Y. Zhang, K. Li, K. Li, B. Zhong, and Y. Fu. Residual non-local attention networks for image restoration. In _ICLR_, 2019.
* [112] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual dense network for image restoration. _TPAMI_, 2020.
* [113] Z. Zheng, W. Ren, X. Cao, X. Hu, T. Wang, F. Song, and X. Jia. Ultra-high-definition image dehazing via multi-guided bilateral learning. In _CVPR_, pages 16185-16194, 2021.
* [114] Z. Zheng, X. Yue, K. Wang, and Y. You. Prompt vision transformer for domain generalization. _CoRR_, abs/2208.08914, 2022.
* [115] K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Conditional prompt learning for vision-language models. In _CVPR_, pages 16795-16804, 2022.
* [116] S. Zhou, K. C. K. Chan, C. Li, and C. C. Loy. Towards robust blind face restoration with codebook lookup transformer. In _NeurIPS_, 2022.
* [117] H. Zhu, C. Wang, Y. Zhang, Z. Su, and G. Zhao. Physical model guided deep image deraining. In _ICME_, 2020.