# MomentDiff: Generative Video Moment Retrieval

from Random to Real

 Pandeng Li\({}^{1}\), Chen-Wei Xie\({}^{2}\), Hongtao Xie\({}^{1}\), Liming Zhao\({}^{2}\),

**Lei Zhang\({}^{1}\), Yun Zheng\({}^{2}\), Deli Zhao\({}^{2}\), Yongdong Zhang\({}^{1}\)**

\({}^{1}\) University of Science and Technology of China, Hefei, China

\({}^{2}\) Alibaba Group

lpd@mail.ustc.edu.cn, {htxie, leizh23, zhyd73}@ustc.edu.cn

{eniac.xcw, lingchen.zlm, zhengyun.zy}@alibaba-inc.com

zhaodeli@gmail.com

Interns at Alibaba GroupCorresponding author

###### Abstract

Video moment retrieval pursues an efficient and generalized solution to identify the specific temporal segments within an untrimmed video that correspond to a given language description. To achieve this goal, we provide a generative diffusion-based framework called MomentDiff, which simulates a typical human retrieval process from random browsing to gradual localization. Specifically, we first diffuse the real span to random noise, and learn to denoise the random noise to the original span with the guidance of similarity between text and video. This allows the model to learn a mapping from arbitrary random locations to real moments, enabling the ability to locate segments from random initialization. Once trained, MomentDiff could sample random temporal segments as initial guesses and iteratively refine them to generate an accurate temporal boundary. Different from discriminative works (_e.g.,_ based on learnable proposals or queries), MomentDiff with random initialized spans could resist the temporal location biases from datasets. To evaluate the influence of the temporal location biases, we propose two "anti-bias" datasets with location distribution shifts, named Charades-STA-Len and Charades-STA-Mom. The experimental results demonstrate that our efficient framework consistently outperforms state-of-the-art methods on three public benchmarks, and exhibits better generalization and robustness on the proposed anti-bias datasets. The code, model, and anti-bias evaluation datasets are available at https://github.com/IMCCretrieval/MomentDiff.

## 1 Introduction

Video understanding [1, 2, 3, 4, 5, 6, 7, 8, 9] is a crucial problem in machine learning [10, 11, 12, 13, 14, 15, 16], which covers various video analysis tasks, such as video classification and action detection. But both tasks above are limited to predicting predefined action categories. A more natural and elaborate video understanding process is the ability for machines to match human language descriptions to specific activity segments in a complex video. Hence, a series of studies [17, 18, 19, 20, 21, 22] are conducted on Video Moment Retrieval (VMR), with the aim of identifying the moment boundaries ( _i.e.,_ the start and end time) within a given video that best semantically correspond to the text query.

As shown in Fig. 1(a), early works address the VMR task by designing predefined dense video proposals ( _i.e.,_ sliding windows [23, 24, 25], anchors [26] and 2D map [27] ). Then, the prediction segment is determined based on the maximum similarity score between dense proposals and thequery text. However, these methods have a large redundancy of proposals and the numbers of positive and negative proposals are unbalanced, which limits the learning efficiency [28]. To deal with this problem, a series of VMR works [29, 30, 31, 32, 33, 34, 35, 36] have recently emerged, mainly discussing how to reduce the number of proposals and improve the quality of proposals. Among them, a promising scheme (Fig. 1(b)) is to use sparse and learnable proposals [34, 35, 36] or queries [37, 38] (_i.e.,_ soft proposals) to model the statistics of the entire dataset and adaptively predict video segments. However, these proposal-learnable methods rely on a few specific proposals or queries to fit the location distribution of ground truth moments. For example, these proposals or queries may tend to focus on video segments where locations in the dataset occur more often (_i.e.,_ yellow highlights in Fig. 1(b)). Thus, these methods potentially disregard significant events that transpire in out-of-sample situations. Recent studies [39, 40] indicate that VMR models [35, 27, 32] may exploit the location biases present in dataset annotations [23], while downplaying multimodal interaction content. This leads to the limited generalization of the model, especially in real-world scenarios with location distribution shifts.

To tackle the above issues, we propose a generative perspective for the VMR task. As shown in Fig. 1 (c) and (d), given an untrimmed video and the corresponding text query, we first introduce several random spans as the initial prediction, then employ a diffusion-based denoiser to iteratively refine the random spans by conditioning on similarity relations between the text query and video frames. A heuristic explanation of our method is that, it can be viewed as a way for humans to quickly retrieve moments of interest in a video. Specifically, given an unseen video, instead of watching the entire video from beginning to end (which is too slow), humans may first glance through random contents to identify a rough location, and finally iteratively focus on key semantic moments and generate temporal coordinates. In this way, we do not rely on distribution-specific proposals or queries (as mentioned in the above discriminative approaches) and exhibit more generalization and robustness (Tab. 5 and Fig. 4) when the ground truth location distributions of training and test sets are different.

To implement our idea, we introduce a generative diffusion-based framework, named MomentDiff. Firstly, MomentDiff extracts feature embeddings for both the input text query and video frames. Subsequently, these text and video embeddings are fed into a similarity-aware condition generator. This generator modulates the video embeddings with text embeddings to produce text-video fusion embeddings. The fusion embeddings contain rich semantic information about the similarity relations between the text query and each video frame, so we can use them as a guide to help us generate predictions. Finally, we develop a Video Moment Denoiser (VMD) that enhances noise perception and enables efficient generation with only a small number of random spans and flexible embedding learning. Specifically, VMD directly maps randomly initialized spans into the multimodal space,

Figure 1: (a) Proposal-predefined methods. Yellow highlights in the timeline represent frequently occurring moments in the dataset. (b) Proposal-learnable methods. (c) Our generative method. (d) We model the VMR task as a process of gradually generating real temporal span from random noise.

taking them as input together with noise intensities. Then, VMD iteratively refines spans according to the similarity relations of fusion embeddings, thereby generating true spans from random to real.

Our main contributions are summarized as follows. 1) To the best of our knowledge, we are the first to tackle video moment retrieval from a generative perspective, which does not rely on predefined or learnable proposals and mitigates temporal location biases from datasets. 2) We propose a new framework, MomentDiff, which utilizes diffusion models to iteratively denoise random spans to the correct results. 3) We propose two "anti-bias" datasets with location distribution shifts to evaluate the influence of location biases, named Charades-STA-Len and Charades-STA-Mom. Extensive experiments demonstrate that MomentDiff is more efficient and transferable than state-of-the-art methods on three public datasets and two anti-bias datasets.

## 2 Related Work

**Video Moment Retrieval.** Video moment retrieval [32; 31; 41; 42; 33; 43; 44; 45; 46] is a newly researched subject that emphasizes retrieving correlated moments in a video, given a natural language query. Pioneering works are proposal-based approaches, which employ a "proposal-rank" two-stage pipeline. Early methods [23; 25; 26; 27; 32] usually use handcrafted predefined proposals to retrieve moments. For example, CTRL [23] and MCN [25] aim to generate video proposals by using sliding windows of different scales. TGN [26] emphasizes temporal information and develops multi-scale candidate solutions through predefined anchors. 2DTAN [27] designs a 2D temporal map to enumerate proposals. However, these dense proposals introduce redundant computation with a large number of negative samples [28; 36]. Therefore, two types of methods are proposed: 1) Proposal-free methods [17; 31; 47] do not use any proposals and are developed to directly regress start and end boundary values or probabilities based on ground-truth segments. These methods are usually much faster than proposal-based methods. 2) Proposal-learnable methods that use proposal prediction networks [34; 35; 36] or learnable queries [37; 38] to model dataset statistics and adaptively predict video segments. QSPN [35] and APGN [34] adaptively obtain discriminative proposals without handcrafted design. LPNet [36] uses learnable proposals to alleviate the redundant calculations in dense proposals. MomentDETR [37] can predict multiple segments using learnable queries. Since proposal-learnable methods adopt a two-stage prediction [34; 35; 36] or implicit iterative [37] design, the performance is often better than that of proposal-free methods. However, proposal-learnable methods explicitly fit the location distribution of target moments. Thus, models are likely to be inclined to learn location bias in datasets [39; 40], resulting in limited generalization. We make no assumptions about the location and instead use random inputs to alleviate this problem.

**Diffusion models.** Diffusion Models [48; 49; 50; 51] are inspired by stochastic diffusion processes in non-equilibrium thermodynamics. The model first defines a Markov chain of diffusion steps to slowly add random noise to the data, and then learns the reverse diffusion process to construct the desired data samples from the noise. The diffusion-based generation has achieved disruptive achievements in tasks such as vision generation [52; 53; 54; 55; 56; 57; 58] and text generation [59]. Motivated by their great success in generative tasks [60], diffusion models have been used in image perception tasks such as object detection [61] and image segmentation [62]. However, diffusion models are less explored for video-text perception tasks. This paper models similarity-aware multimodal information as coarse-grained cues, which can guide the diffusion model to generate the correct moment boundary from random noise in a gradual manner. Unlike DiffusionDet [61], we avoid a large number of Region of Interest (ROI) features and do not require additional post-processing techniques. To our knowledge, this is the first study to adapt the diffusion model for video moment retrieval.

## 3 Method

In this section, we first define the problem in Sec. 3.1, introduce our framework in Sec. 3.2, and describe the inference process in Sec. 3.3.

### Problem Definition

Suppose an untrimmed video \(\mathcal{V}=\{\bm{v}_{i}\}_{i=1}^{N_{v}}\) is associated with a natural text description \(\mathcal{T}=\{\bm{t}_{i}\}_{i=1}^{N_{t}}\), where \(N_{v}\) and \(N_{t}\) represent the frame number and word number, respectively. Under this notation definition, Video Moment Retrieval (VMR) aims to learn a model \(\bm{\Omega}\) to effectively predict the moment \(\hat{\bm{x}}_{0}=(\hat{\bm{c}}_{0},\hat{\bm{w}}_{0})\) that is most relevant to the given text description: \(\hat{\bm{x}}_{0}=\bm{\Omega}(\mathcal{T},\mathcal{V})\), where \(\hat{\bm{c}}_{0}\) and \(\hat{\bm{w}}_{0}\) represent the center time and duration length of the temporal moments, _i.e.,_ predicted spans.

### The MomentDiff Framework

Fig. 2 sheds light on the generation modeling architecture of our proposed MomentDiff. Concretely, we first extract frame-level and word-level features by utilizing pre-trained video and text backbone networks. Afterward, we employ a similarity-aware condition generator to interact text and visual features into fusion embeddings. Finally, combined with the fusion embeddings, the video moment denoiser can progressively produce accurate temporal targets from random noise.

#### 3.2.1 Visual and Textual Representations.

Before performing multimodal interaction, we should convert the raw data into a continuous feature space. To demonstrate the generality of our model, we use three distinct visual extractors [32; 44] to obtain video features \(\mathcal{V}\): 1) 2D visual encoder, the VGG model [63]. 2) 3D visual encoder, the C3D model [64]. 3) Cross-modal pre-train encoder, the CLIP visual model [65]. However, due to the absence of temporal information in CLIP global features, we additionally employ the SlowFast model [66] to extract features, which concatenate CLIP features. Besides, to take full advantage of the video information [38], we try to incorporate audio features, which are extracted using a pre-trained PANN model [67]. To obtain text features, we try two feature extractors: the Glove model [68] and the CLIP textual model to extract 300-d and 512-d text features \(\mathcal{T}\), respectively.

#### 3.2.2 Similarity-aware Condition Generator

Unlike generation tasks [69] that focus on the veracity and diversity of results, the key to the VMR task is to fully understand the video and sentence information and to mine the similarities between text queries and video segments. To this end, we need to provide multimodal information to cue the denoising network to learn the implicit relationships in the multimodal space.

A natural idea is to interact and aggregate information between video and text sequences with a multilayer Transformer [70]. Specifically, we first use two multilayer perceptron (MLP) networks to map feature sequences into the common multimodal space: \(\bm{V}\in\mathbb{R}^{N_{v}\times D}\) and \(\bm{T}\in\mathbb{R}^{N_{t}\times D}\), where \(D\) is the embedding dimension. Then, we employ two cross-attention layers to perform interactions between multiple modalities, where video embeddings \(\bm{V}\) are projected as the query \(\bm{Q}_{v}\), text embeddings \(\bm{T}\) are projected as key \(\bm{K}_{t}\) and value \(\bm{V}_{t}\): \(\bm{\hat{V}}=\operatorname{softmax}\left(\bm{Q}_{v}\bm{K}_{t}^{T}\right)\bm{V }_{t}+\bm{Q}_{v},\) where \(\bm{\hat{V}}\in\mathbb{R}^{N_{v}\times D}\). To help the model better understand the video sequence relations, we feed \(\bm{\hat{V}}\) into a 2-layer self-attention network, and the final similarity-aware fusion embedding is \(\bm{F}=\operatorname{softmax}\left(\bm{Q}_{\hat{v}}\bm{K}_{\hat{v}}^{T}\right) \bm{V}_{\hat{v}}+\bm{Q}_{\hat{v}}\), where \(\bm{Q}_{\hat{v}},\bm{K}_{\hat{v}},\bm{V}_{\hat{v}}\) is the matrix obtained from \(\bm{\hat{V}}\) after three different projections respectively.

In the span generation process, even for the same video, the correct video segments corresponding to different text queries are very different. Since the fusion embedding \(\bm{F}\) serves as the input condition of the denoiser, the quality of \(\bm{F}\) directly affects the denoising process. To learn similarity relations for \(\bm{F}\) in the multimodal space, we design the similarity loss \(\mathcal{L}_{sim}\), which contains the pointwise cross entropy loss and the pairwise margin loss:

\[\mathcal{L}_{sim}=-\frac{1}{N_{v}}\sum_{i=1}\bm{y}_{i}*log(\bm{s}_{i})+(1-\bm {y}_{i})*log(1-\bm{s}_{i})+\frac{1}{N_{s}}\sum_{j=1}\max\left(0,\beta+\bm{s}_{ n_{j}}-\bm{s}_{p_{j}}\right),\] (1)

Figure 2: Our MomentDiff framework, which includes a Similarity-aware Condition Generator (SCG) and a Video Moment Denoiser (VMD). The diffusion process is conducted progressively in VMD.

where \(\bm{s}\in\mathbb{R}^{N_{v}}\) is the similarity score, which is obtained by predicting the fusion embedding \(\bm{F}\) through the MLP network. \(\bm{y}\in\mathbb{R}^{N_{v}}\) is the similarity label, where \(\bm{y}_{i}=1\) if the \(i\)-th frame is within the ground truth temporal moment and \(\bm{y}_{i}=0\) otherwise. \(\bm{s}_{p_{j}}\) and \(\bm{s}_{n_{j}}\) are the randomly sampled positive and negative frames. \(N_{s}\) is the number of samples and the margin \(\beta=0.2\). Although \(\mathcal{L}_{sim}\) may only help the fusion embedding retain some coarse-grained similarity semantics, this still provides indispensable multimodal information for the denoiser.

#### 3.2.3 Video Moment Denoiser

Recent works [39, 40] have revealed that previous models [35, 27, 32] may rely on the presence of location bias in annotations to achieve seemingly good predictions. To alleviate this problem, instead of improving distribution-specific proposals or queries, we use random location spans to iteratively obtain real spans from a generative perspective. In this section, we first introduce the principle of the forward and reverse processes in diffusion models. Then, we build the diffusion generation process in the video moment denoiser with model distribution \(p_{\theta}(\bm{x}_{0})\) to learn the data distribution \(q(\bm{x}_{0})\).

**Forward process.** During training, we first construct a forward process that corrupts real segment spans \(\bm{x}_{0}\sim q(\bm{x}_{0})\) to noisy data \(\bm{x}_{m}\), where \(m\) is the noisy intensity. Specifically, the Gaussian noise process of any two consecutive intensities [49] can be defined as: \(q\left(\bm{x}_{m}\mid\bm{x}_{m-1}\right)=\mathcal{N}\left(\bm{x}_{m};\sqrt{1- \beta_{m}}\bm{x}_{m-1},\beta_{m}\text{I}\right),\) where \(\beta\) is the variance schedule. In this way, \(\bm{x}_{m}\) can be constructed by \(\bm{x}_{0}\): \(q\left(\bm{x}_{1:m}\mid\bm{x}_{0}\right)=\prod_{i=1}^{m}q\left(\bm{x}_{i}\mid \bm{x}_{i-1}\right).\) Benefiting from the reparameterization technique, the final forward process is simplified to:

\[\bm{x}_{m}=\sqrt{\bar{\alpha}_{m}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}_{m}}\bm{ \epsilon}_{m},\] (2)

where the noise \(\bm{\epsilon}_{m}\sim\mathcal{N}(0,\text{I})\) and \(\bar{\alpha}_{m}=\prod_{i=1}^{m}(1-\beta_{i})\).

**Reverse process.** The denoising process is learning to remove noise asymptotically from \(\bm{x}_{m}\) to \(\bm{x}_{0}\), and its traditional single-step process can be defined as:

\[p_{\theta}\left(\bm{x}_{m-1}\mid\bm{x}_{m}\right)=\mathcal{N}\left(\bm{x}_{m- 1};\bm{\mu}_{\theta}\left(\bm{x}_{m},m\right),\sigma_{m}^{2}\bm{I}\right)\] (3)

where \(\sigma_{m}^{2}\)is associated with \(\beta_{m}\) and \(\bm{\mu}_{\theta}\left(\bm{x}_{m},m\right)\) is the predicted mean. In this paper, we train the Video Moment Denoiser (VMD) to reverse this process. The difference is that we predict spans from the VMD network \(f_{\theta}(\bm{x}_{m},m,\bm{F})\) instead of \(\bm{\mu}_{\theta}\left(\bm{x}_{m},m\right)\).

**Denoiser network.** As shown in Fig. 3, the VMD network mainly consists of 2-layer cross-attention Transformer layers. Next, we walk through how VMD works step by step. For clarity, the input span and output prediction presented below are a single vector.

\(\bm{\Theta}\)**Span normalization.** Unlike generation tasks, our ground-truth temporal span \(\bm{x}_{0}\) is defined by two parameters \(c_{0}\) and \(\bm{w}_{0}\) that have been normalized to \([0,1]\), where \(\bm{c}_{0}\) and \(\bm{w}_{0}\) are the center and length of the span \(\bm{x}_{0}\). Therefore, in the above forward process, we need to extend its scale to \([-\lambda,\lambda]\) to stay close to the Gaussian distribution [71, 61]. After the noise addition is completed, we need to clamp \(\bm{x}_{m}\) to \([-\lambda,\lambda]\) and then transform the range to \([0,1]\): \(\bm{x}_{m}=(\bm{x}_{m}/\lambda+1)/2\), where \(\lambda=2\).

\(\bm{\Theta}\)**Span embedding.** To model the data distribution in multimodal space, we directly project the discrete span to the embedding space through the Fully Connected (FC) layer: \(\bm{x}_{m}^{\prime}=\text{FC}(\bm{x}_{m})\in\mathbb{R}^{D}\). Compared to constructing ROI features in DiffusionDet [61], linear projection is very flexible and decoupled from conditional information (_i.e.,_ fusion embeddings), avoiding more redundancy.

\(\bm{\Theta}\)**Intensity-aware attention.** The denoiser needs to understand the added noise intensity \(m\) during denoising, so we design the intensity-aware attention to perceive the intensity magnitude explicitly. In Fig. 3, we use sinusoidal mapping for the noise intensity \(m\) to obtain \(\bm{e}_{m}\in\mathbb{R}^{D}\) in the multimodal space and add it to the span embedding. We project \(\bm{x}_{m}^{\prime}+\bm{e}_{m}\) as query embedding and the positional embedding \(\bm{pos}_{m}\in\mathbb{R}^{D}\) is obtained by sinusoidal mapping of \(\bm{x}_{m}\). We can obtain the input query: \(\bm{Q}_{m}=\text{Concat}(\text{Proj}(\bm{x}_{m}^{\prime}+\bm{e}_{m}),\bm{pos}_{ m})\). Similarly, The input key is \(\bm{K}_{f}=\text{Concat}(\text{Proj}(\bm{F}),\bm{pos}_{f})\) and the input value is \(\bm{V}_{f}=\text{Proj}(\bm{F})\), where \(\text{Proj}(\cdot)\) is the projection function and \(\bm{pos}_{f}\in\mathbb{R}^{D}\) is the standard position embedding in Transformer [70]. Thus, the intensity-aware attention is:

\[\bm{Q}_{m}=\text{softmax}\left(\bm{Q}_{m}\bm{K}_{f}^{\text{T}}\right)\bm{V}_{f} +\bm{Q}_{m}.\] (4)

Figure 3: Video moment denoiser. For simplicity, we only draw the intensity-aware attention structure that is different from the general Transformer.

**Denoising training.** Finally, the generated transformer output is transformed into predicted spans \(\bm{\hat{x}}_{m-1}=(\bm{\hat{c}}_{m-1},\bm{\hat{w}}_{m-1})\) and confidence scores \(\bm{\hat{z}}_{m-1}\), which are implemented through a simple FC layer, respectively. Following [71], the network prediction should be as close to ground truth \(\bm{x}_{0}\) as possible. In addition, inspired by [37; 72], we define the denoising loss as:

\[\mathcal{L}_{\text{vmr}}\left(\bm{x}_{0},f_{\theta}(\bm{x}_{m},m,\bm{F}) \right)=\lambda_{\text{L1}}\left\|\bm{x}_{0}-\bm{\hat{x}}_{m-1}\right\|+\lambda _{\text{iou}}\,\mathcal{L}_{\text{iou}}\,\left(\bm{x}_{0},\bm{\hat{x}}_{m-1} \right)+\lambda_{\text{ce}}\mathcal{L}_{ce}(\bm{\hat{z}}_{m-1}),\] (5)

where \(\lambda_{\text{L1}}\), \(\lambda_{\text{iou}}\) and \(\lambda_{\text{ce}}\) are hyperparameters, \(\mathcal{L}_{\text{iou}}\) is a generalized IoU loss [73], \(\mathcal{L}_{\text{ce}}\) is a cross-entropy loss. Note that the above procedure is a simplification of training. Considering that there may be more than one ground truth span in the dataset [37], we set the number of input and output spans to \(N_{r}\). For the input, apart from the ground truth, the extra spans are padded with random noise. For the output, we calculate the matching cost of each predicted span and ground truth according to \(\mathcal{L}_{vmr}\) (_i.e.,_ the Hungarian match [72]), and find the span with the smallest cost to calculate the loss. In \(\mathcal{L}_{\text{ce}}\), we set the confidence label to 1 for the best predicted span and 0 for the remaining spans.

### Inference

After training, MomentDiff can be applied to generate temporal moments for video-text pairs including unseen pairs during training. Specifically, we randomly sample noise \(\bm{\hat{x}}_{m}\) from a Gaussian distribution \(\mathcal{N}(0,\text{I})\), the model can remove noise according to the update rule of diffusion models [69]:

\[\bm{\hat{x}}_{m-1}= \sqrt{\bar{\alpha}_{m-1}}f_{\theta}(\bm{\hat{x}}_{m},m,\bm{F})+ \sqrt{1-\bar{\alpha}_{m-1}-\sigma_{m}^{2}}\frac{\bm{\hat{x}}_{m}-\sqrt{\bar{ \alpha}_{m}}f_{\theta}(\bm{\hat{x}}_{m},m,\bm{F})}{\sqrt{1-\bar{\alpha}_{m}}} +\sigma_{m}\bm{\epsilon}_{m}.\] (6)

As shown in Fig 1(d), we iterate this process continuously to obtain \(\bm{\hat{x}}_{0}\) from coarse to fine. Note that in the last step we directly use \(f_{\theta}(\bm{\hat{x}}_{1},1,\bm{F})\) as \(\bm{\hat{x}}_{0}\). In \(\bm{\hat{x}}_{0}\), we choose the span with the highest confidence score in \(\bm{\hat{z}}_{0}\) as the final prediction. To reduce inference overhead, we do not employ any post-processing techniques, such as box renewal in DiffusionDet [61] and self-condition [71].

## 4 Experiments

### Datasets, Metrics and Implementation Details

**Datasets.** We evaluate the efficacy of our model by conducting experiments on three representative datasets: Charades-STA [23], QVHighlights [37] and TACoS [74]. The reason is that the above three datasets exhibit diversity. Charades-STA comprises intricate daily human activities. QVHighlights contains a broad spectrum of themes, ranging from everyday activities and travel in lifestyle vlogs to social and political events in news videos. TACoS mainly presents long-form videos featuring culinary activities. The training and testing divisions are consistent with existing methods [28; 38].

**Metrics.** To make fair comparisons, we adopt the same evaluation metrics as those used in previous works [38; 37; 23; 75; 29; 19; 27], namely R1@n, MAP@n, and MAP\({}_{avg}\). Specifically, R1@n is defined as the percentage of testing queries that have at least one correct retrieved moment (with an intersection over union (IoU) greater than n) within the top-1 results. Similarly, MAP@n is defined as the mean average precision with an IoU greater than n, while MAP\({}_{avg}\) is determined as the average MAP@n across multiple IoU thresholds [0.5: 0.05: 0.95].

**Implementation details.** For a fair comparison [44; 20; 30], we freeze the video encoder and text encoder and use only the extracted features. For VGG [63], C3D [64] or SlowFast+CLIP (SF+C) [66; 65], we extract video features every 1/6s, 1s or 2s. So the frame number \(N_{v}\) is related to the length of the video, while the max text length \(N_{t}\) is set to 32. Note that since the videos in TACoS are long, we uniformly sample the video frame features and set the max frame number \(N_{v}\) to 100 in TACoS. We set the hidden size \(D=256\) in all Transformer layers. In SCG, we also use a variant of the pairwise margin loss called InfoNCE loss [76]. The number of random spans \(N_{r}\) is set to 10 for QVHighlights, 5 for Charades-STA and TACoS. We use the cosine schedule for \(\beta\). For all datasets, we optimize MomentDiff for 100 epochs on one NVIDIA Tesla A100 GPU, employ Adam optimizer [77] with 1e-4 weight decay and fix the batch size as 32. The learning rate is set to 1e-4. By default, the loss hyperparameters \(\lambda_{\text{L1}}=10\), \(\lambda_{\text{iou}}=1\) and \(\lambda_{\text{ce}}=4\). The weight values for \(\mathcal{L}_{sim}\) and \(\mathcal{L}_{vmr}\) are 4 and 1. To speed up the sampling process during inference, we follow DDIM [69] and iterate 50 times.

### Performance Comparisons

**Comparison with state-of-the-art methods.** To prove the effectiveness of MomentDiff, we compare the retrieval performance with 17 discriminative VMR methods. Tab. 1, Tab. 2, and Tab. 3 show the R1@n, MAP@n, and MAP\({}_{avg}\) results on Charades-STA, QVHighlights and TACoS. Compared with SOTA methods [28; 38; 75; 44; 30; 37], MomentDiff achieves significant improvements on Charades-STA regardless of whether 2D features (VGG), multimodal features (VGG+A), 3D features (C3D), or multimodal pre-trained features (SF+C) are used. This proves that MomentDiff is a universal generative VMR method. In the other two datasets (QVHighlights and TACoS), we still have highly competitive results. Specifically, compared to MomentDETR [37], MomentDiff obtains 2.35%, 3.86%, and 13.13% average gains in R1@0.5 on three datasets. It is worth noting that TACoS contains long videos of cooking events where different events are only slightly different in terms of cookware, food and other items. The learnable queries in MomentDETR may not cope well with such fine-grained dynamic changes. We attribute the great advantage of MomentDiff over these methods to fully exploiting similarity-aware condition information and progressive refinement denoising.

**Transfer experiments.** To explore the location bias problem, we first organize the Out of Distribution (OOD) experiment following [82], which repartitions Charades-STA [23] and ActivityNet-Captions [83] datasets according to moment annotation density values [82]. In Tab. 4, we exceed

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{5}{c}{QVHighlights} \\ \cline{2-5}  & R1@0.5 & R1@0.7 & MAP@0.5 & MAP@0.75 & MAP\({}_{avg}\) \\ \hline MCN [25] & 11.41 & 2.72 & 24.94 & 8.22 & 10.67 \\ CAL [80] & 25.49 & 11.54 & 23.40 & 7.65 & 9.89 \\ XML [81] & 41.83 & 30.35 & 44.63 & 31.73 & 32.14 \\ XML+ [81] & 46.69 & 33.46 & 47.89 & 34.67 & 34.90 \\ MDE* [37] & 53.56 & 34.09 & 53.97 & 28.65 & 29.39 \\ MomentDiff & **57.42** & **39.66** & **54.02** & **35.73** & **35.95** \\ UMT* [38] & 56.26 & 40.31 & 52.77 & 36.82 & 35.79 \\ MomentDiff* [82] & **58.21** & **41.48** & **54.57** & **37.21** & **36.84** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparisons (%) on QVHighlights Table 3: Performance comparisons (%) with SF+C video features and CLIP text features. ”+” de-on TACoS. We adopt C3D features to notes that we re-implement the method with only segment encode videos. MDE is the abbreviation moment labels. ”+” stands for using audio data. MDE is the of MomentDETR [37].

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{5}{c}{QVHighlights} \\ \cline{2-5}  & R1@0.5 & R1@0.7 & MAP@0.5 & MAP@0.75 & MAP\({}_{avg}\) \\ \hline MCN [25] & 11.41 & 2.72 & 24.94 & 8.22 & 10.67 \\ CAL [80] & 25.49 & 11.54 & 23.40 & 7.65 & 9.89 \\ XML [81] & 41.83 & 30.35 & 44.63 & 31.73 & 32.14 \\ XML+ [81] & 46.69 & 33.46 & 47.89 & 34.67 & 34.90 \\ MDE* [37] & 53.56 & 34.09 & 53.97 & 28.65 & 29.39 \\ MomentDiff & **57.42** & **39.66** & **54.02** & **35.73** & **35.95** \\ UMT* [38] & 56.26 & 40.31 & 52.77 & 36.82 & 35.79 \\ MomentDiff* [82] & **58.21** & **41.48** & **54.57** & **37.21** & **36.84** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance comparisons (%) on the Charades-STA dataset. ”\(\star\)” denotes that we re-implement the method under the same training scheme. ”A” stands for using audio data.

[MISSING_PAGE_FAIL:8]

**Span embedding type.** Regarding the way discrete spans are mapped to the embedding space, we compare the ROI strategy [61] with our linear projection (FC) in Tab. 6(a). For the ROI strategy, we slice the fusion embeddings \(\bm{F}\) corresponding to random spans, followed by mean pooling on the sliced features. Tab. 6(a) shows that ROI does not work well. This may be due to two points: 1) ROI is a hard projection strategy, while the importance of each video frame is quite different. FC is similar to soft ROI, and its process can be trained end-to-end. 2) FC is decoupled from \(\bm{F}\), which allows the model to focus on modeling the diffusion process and avoid over-dependence on \(\bm{F}\).

**Scale \(\lambda\).**\(\lambda\) is the signal-to-noise ratio [61] of the diffusion process, and its effect is shown in Tab. 6(b). We find that the effect of larger \(\lambda\) drops significantly, which may be due to the lack of more hard samples for denoising training when the proportion of noise is small, resulting in poor generalization.

**Video Moment Denoiser (VMD) and noise intensity \(m\).** In Tab. 6(c), we first remove the denoiser and the diffusion process (w/o VMD). After training with the same losses, we find that predicting with only fusion embeddings \(\bm{F}\) leads to a drastic drop in results, which reveals the effectiveness of denoising training. Then we remove the noise intensity \(m\) (w/o \(m\)), and the result is reduced by 5.16% on R1@0.5. This shows that explicitly aggregating noise intensity with random spans improves noise modeling. Combined with VMD and \(m\), the diffusion mechanism can fully understand the data distribution and generate the real span from coarse to fine.

**Loss designs.** In Tab. 6(d), we show the impact of loss functions. In \(\mathcal{L}_{sim}\), we use pointwise and pairwise constraints to guide token-wise interactions between multimodal features, while ensuring reliable conditions for subsequent denoising. In \(\mathcal{L}_{curr}\), the model can learn to accurately localize exact segments. Adequate multimodal interaction and denoising training procedures are complementary.

**Span number.** In Tab. 6(e), we only need 5 random spans to achieve good results. Unlike object detection [61], the number of correct video segments corresponding to text query is small. Therefore, a large number of random inputs may make the model difficult to train and deteriorate the performance.

\begin{table}

\end{table}
Table 6: Ablation study (%) on the Charades-STA dataset with SF+C video features and CLIP text features. We report R1@0.5, R1@0.7 and MAP\({}_{avg}\). Default settings are marked in blue.

Figure 5: Visualization of the diffusion process on Charades-STA (Left) and QVHighlights (Right). For clarity, we show 3 random spans (\(\bm{x}_{50}\)) with Gaussian initialization, and progressively get the top-1 result (\(\bm{x}_{0}\)) according to the confidence score. Green box: right segment. MDE: MomentDETR [37].

**Model performance vs. speed.** In Tab. 6(f), we explore the effects of different diffusion steps. When step=2, good results and fast inference speed have been achieved. Subsequent iterations can improve the results of high IoU (_i.e.,_ R1@0.7), which shows the natural advantages of diffusion models.

### Qualitative Results

We show two examples of the diffusion process in Fig. 5. We can find that the retrieved moments by MomentDiff are closer to the ground truth than those by MomentDETR. The diffusion process can gradually reveal the similarity between text and video frames, thus achieving better results. Besides, the final predictions corresponding to spans with multiple random initial locations are close to the ground truth. This shows that our model achieves a mapping from arbitrary locations to real segments.

## 5 Limitation and Conclusion

**Limitation.** Compared to existing methods [27; 37], the diffusion process requires multiple rounds of iterations, which may affect the inference speed. As shown in Tab. 6(f), we reduce the number of iterations, with only a small sacrifice in performance. In practical usage, we suggest choosing a reasonable step number for a better trade-off between performance and speed.

**Conclusion.** This paper proposes a novel generative video moment retrieval framework, MomentDiff, which simulates a typical human retrieval style via diffusion models. Benefiting from the denoising diffusion process from random noise to temporal span, we achieve the refinement of prediction results and alleviate the location bias problem existing in discriminative methods. MomentDiff demonstrates efficiency and generalization on multiple diverse and anti-bias datasets. We aim to stimulate further research on video moment retrieval by addressing the inadequacies in the framework design, and firmly believe that this work provides fundamental insights into the multimodal domain.

## 6 Acknowledgement

This work is supported by the National Key Research and Development Program of China (2022YFB3104700), the National Nature Science Foundation of China (62121002, 62022076, 62232006).

## References

* [1] Han, T., W. Xie, A. Zisserman. Self-supervised co-training for video representation learning. In _NeurIPS_, pages 5679-5690. 2020.
* [2] Chen, Y., Y. Tsai, M. Yang. End-to-end multi-modal video temporal grounding. In _NeurIPS_, pages 28442-28453. 2021.
* [3] Kim, M., H. Kwon, C. Wang, et al. Relational self-attention: What's missing in attention for video understanding. In _NeurIPS_, pages 8046-8059. 2021.
* [4] Xie, C.-W., S. Sun, X. Xiong, et al. RA-CLIP: Retrieval augmented contrastive language-image pre-training. In _CVPR_, pages 19265-19274. 2023.
* [5] Zhao, L., K. Zheng, Y. Zheng, et al. RLEG: vision-language representation learning with diffusion-based embedding generation. In _ICML_, pages 42247-42258. 2023.
* [6] Jin, P., H. Li, Z. Cheng, et al. Text-video retrieval with disentangled conceptualization and set-to-set alignment. In _IJCAI_. 2023.
* [7] Ging, S., M. Zolfaghari, H. Pirsiavash, et al. Coot: Cooperative hierarchical transformer for video-text representation learning. In _NeurIPS_, pages 22605-22618. 2020.
* [8] Li, P., C.-W. Xie, L. Zhao, et al. Progressive spatio-temporal prototype matching for text-video retrieval. In _ICCV_, pages 4100-4110. 2023.
* [9] Li, P., H. Xie, J. Ge, et al. Dual-stream knowledge-preserving hashing for unsupervised video retrieval. In _ECCV_, pages 181-197. 2022.

* [10] Miech, A., J.-B. Alayrac, L. Smaira, et al. End-to-end learning of visual representations from uncurated instructional videos. In _CVPR_, pages 9879-9889. 2020.
* [11] Jin, P., J. Huang, F. Liu, et al. Expectation-maximization contrastive learning for compact video-and-language representations. In _NeurIPS_, pages 30291-30306. 2022.
* [12] Jin, P., H. Li, Z. Cheng, et al. Diffusionret: Generative text-video retrieval with diffusion model. In _ICCV_. 2023.
* [13] Jin, P., J. Huang, P. Xiong, et al. Video-text as game players: Hierarchical banzhaf interaction for cross-modal representation learning. In _CVPR_, pages 2472-2482. 2023.
* [14] Li, P., H. Xie, S. Min, et al. Deep fourier ranking quantization for semi-supervised image retrieval. _TIP_, pages 5909-5922, 2022.
* [15] Zhang, A., H. Fei, Y. Yao, et al. Transfer visual prompt generator across lms. In _NeurIPS_. 2023.
* [16] Li, P., Y. Li, H. Xie, et al. Neighborhood-adaptive structure augmented metric learning. In _AAAI_, pages 1367-1375. 2022.
* [17] Yuan, Y., T. Mei, W. Zhu. To find where you talk: Temporal sentence localization in video with attention based location regression. In _AAAI_, pages 9159-9166. 2019.
* [18] Cao, M., T. Yang, J. Weng, et al. Locvtp: Video-text pre-training for temporal localization. In _ECCV_, vol. 13686, pages 38-56. 2022.
* [19] Zhang, M., Y. Yang, X. Chen, et al. Multi-stage aggregated transformer network for temporal language localization in videos. In _CVPR_, pages 12669-12678. 2021.
* [20] Soldan, M., A. Pardo, J. L. Alcazar, et al. MAD: A scalable dataset for language grounding in videos from movie audio descriptions. In _CVPR_, pages 5016-5025. 2022.
* [21] Kim, D., J. Park, J. Lee, et al. Language-free training for zero-shot video grounding. In _WACV_, pages 2539-2548. 2023.
* [22] Moon, W., S. Hyun, S. Park, et al. Query-dependent video representation for moment retrieval and highlight detection. In _CVPR_. 2023.
* [23] Gao, J., C. Sun, Z. Yang, et al. Tall: Temporal activity localization via language query. In _ICCV_, pages 5267-5275. 2017.
* [24] Liu, M., X. Wang, L. Nie, et al. Attentive moment retrieval in videos. In _SIGIR_, pages 15-24. 2018.
* [25] Anne Hendricks, L., O. Wang, E. Shechtman, et al. Localizing moments in video with natural language. In _ICCV_, pages 5803-5812. 2017.
* [26] Chen, J., X. Chen, L. Ma, et al. Temporally grounding natural sentence in video. In _EMNLP_, pages 162-171. 2018.
* [27] Zhang, S., H. Peng, J. Fu, et al. Learning 2d temporal adjacent networks for moment localization with natural language. In _AAAI_, pages 12870-12877. 2020.
* [28] Wang, Z., L. Wang, T. Wu, et al. Negative sample matters: A renaissance of metric learning for temporal grounding. In _AAAI_, pages 2613-2623. 2022.
* [29] Zhao, Y., Z. Zhao, Z. Zhang, et al. Cascaded prediction network via segment tree for temporal video grounding. In _CVPR_, pages 4197-4206. 2021.
* [30] Zhang, H., A. Sun, W. Jing, et al. Span-based localizing network for natural language video localization. In _ACL_, pages 6543-6554. 2020.
* [31] Zeng, R., H. Xu, W. Huang, et al. Dense regression network for video grounding. In _CVPR_, pages 10287-10296. 2020.

* [32] Yuan, Y., L. Ma, J. Wang, et al. Semantic conditioned dynamic modulation for temporal sentence grounding in videos. _NeurIPS_, 32, 2019.
* [33] Zhang, D., X. Dai, X. Wang, et al. MAN: moment alignment network for natural language moment retrieval via iterative graph adjustment. In _CVPR_, pages 1247-1257. 2019.
* [34] Liu, D., X. Qu, J. Dong, et al. Adaptive proposal generation network for temporal sentence localization in videos. In _EMNLP_, pages 9292-9301. 2021.
* [35] Xu, H., K. He, B. A. Plummer, et al. Multilevel language and vision integration for text-to-clip retrieval. In _AAAI_, pages 9062-9069. 2019.
* [36] Xiao, S., L. Chen, J. Shao, et al. Natural language video localization with learnable moment proposals. In _EMNLP_, pages 4008-4017. 2021.
* [37] Lei, J., T. L. Berg, M. Bansal. Qvhighlights: Detecting moments and highlights in videos via natural language queries. In _NeurIPS_. 2021.
* [38] Liu, Y., S. Li, Y. Wu, et al. UMT: unified multi-modal transformers for joint video moment retrieval and highlight detection. In _CVPR_, pages 3032-3041. 2022.
* [39] Otani, M., Y. Nakashima, E. Rahtu, et al. Uncovering hidden challenges in query-based video moment retrieval. In _BMVC_. 2020.
* [40] Yang, X., F. Feng, W. Ji, et al. Deconfounded video moment retrieval with causal intervention. In _SIGIR_, page 1-10. 2021.
* [41] Nan, G., R. Qiao, Y. Xiao, et al. Interventional video grounding with dual contrastive learning. In _CVPR_, pages 2765-2775. 2021.
* [42] Gao, J., C. Xu. Fast video moment retrieval. In _ICCV_, pages 1523-1532. 2021.
* [43] Wang, H., Z.-J. Zha, L. Li, et al. Structured multi-level interaction network for video moment localization via language query. In _CVPR_. 2021.
* [44] Liu, D., X. Qu, J. Dong, et al. Context-aware biaffine localizing network for temporal sentence grounding. In _CVPR_, pages 11235-11244. 2021.
* [45] Zhang, H., A. Sun, W. Jing, et al. Towards debiasing temporal sentence grounding in video. _arXiv preprint arXiv:2111.04321_, 2021.
* [46] Zeng, Y., D. Cao, X. Wei, et al. Multi-modal relational graph for cross-modal video moment retrieval. In _CVPR_, pages 2215-2224. 2021.
* [47] Mun, J., M. Cho, B. Han. Local-global video-text interactions for temporal grounding. In _CVPR_, pages 10807-10816. 2020.
* [48] Sohl-Dickstein, J., E. A. Weiss, N. Maheswaranathan, et al. Deep unsupervised learning using nonequilibrium thermodynamics. In _ICML_, vol. 37, pages 2256-2265. 2015.
* [49] Ho, J., A. Jain, P. Abbeel. Denoising diffusion probabilistic models. In _NeurIPS_. 2020.
* [50] Austin, J., D. D. Johnson, J. Ho, et al. Structured denoising diffusion models in discrete state-spaces. In _NeurIPS_, pages 17981-17993. 2021.
* [51] Ho, J., T. Salimans, A. A. Gritsenko, et al. Video diffusion models. In _NeurIPS_. 2022.
* [52] Gu, S., D. Chen, J. Bao, et al. Vector quantized diffusion model for text-to-image synthesis. In _CVPR_, pages 10686-10696. 2022.
* [53] Hao, S., K. Han, S. Zhao, et al. ViCo: Detail-preserving visual condition for personalized text-to-image generation. _arXiv preprint arXiv:2306.00971_, 2023.
* [54] Xue, Z., G. Song, Q. Guo, et al. Raphael: Text-to-image generation via large mixture of diffusion paths. _arXiv preprint arXiv:2305.18295_, 2023.

* [55] Liu, Z., R. Feng, K. Zhu, et al. Cones: Concept neurons in diffusion models for customized generation. _arXiv preprint arXiv:2303.05125_, 2023.
* [56] Chen, X., L. Huang, Y. Liu, et al. Anydoor: Zero-shot object-level image customization. _arXiv preprint arXiv:2307.09481_, 2023.
* [57] Zhao, S., D. Chen, Y.-C. Chen, et al. Uni-ControlNet: All-in-one control to text-to-image diffusion models. In _NeurIPS_. 2023.
* [58] Wang, X., H. Yuan, S. Zhang, et al. VideoComposer: Compositional video synthesis with motion controllability. In _NeurIPS_. 2023.
* [59] Li, X. L., J. Thickstun, I. Gulrajani, et al. Diffusion-lm improves controllable text generation. In _NeurIPS_. 2022.
* [60] Popov, V., I. Vovk, V. Gogoryan, et al. Grad-tts: A diffusion probabilistic model for text-to-speech. In _ICML_, vol. 139, pages 8599-8608. 2021.
* [61] Chen, S., P. Sun, Y. Song, et al. DiffusionDet: Diffusion model for object detection. In _ICCV_, pages 19830-19843. 2023.
* [62] Liang, C., W. Wang, J. Miao, et al. GMMSeg: Gaussian mixture based generative semantic segmentation models. In _NeurIPS_. 2022.
* [63] Simonyan, K., A. Zisserman. Very deep convolutional networks for large-scale image recognition. In _ICLR_. 2015.
* [64] Tran, D., L. D. Bourdev, R. Fergus, et al. Learning spatiotemporal features with 3d convolutional networks. In _ICCV_, pages 4489-4497. 2015.
* [65] Radford, A., J. W. Kim, C. Hallacy, et al. Learning transferable visual models from natural language supervision. In _ICML_, vol. 139, pages 8748-8763. 2021.
* [66] Feichtenhofer, C., H. Fan, J. Malik, et al. Slowfast networks for video recognition. In _ICCV_, pages 6201-6210. IEEE, 2019.
* [67] Kong, Q., Y. Cao, T. Iqbal, et al. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. _IEEE Trans Audio Speech Lang Process_, 2020.
* [68] Pennington, J., R. Socher, C. D. Manning. Glove: Global vectors for word representation. In _EMNLP_, pages 1532-1543. 2014.
* [69] Song, J., C. Meng, S. Ermon. Denoising diffusion implicit models. In _ICLR_. 2021.
* [70] Vaswani, A., N. Shazeer, N. Parmar, et al. Attention is all you need. In _NeurIPS_. 2017.
* [71] Chen, T., R. Zhang, G. E. Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. In _ICLR_. 2023.
* [72] Carion, N., F. Massa, G. Synnaeve, et al. End-to-end object detection with transformers. In _ECCV_, vol. 12346, pages 213-229. 2020.
* [73] Rezatofighi, H., N. Tsoi, J. Gwak, et al. Generalized intersection over union: A metric and a loss for bounding box regression. In _CVPR_, pages 658-666. 2019.
* [74] Regneri, M., M. Rohrbach, D. Wetzel, et al. Grounding action descriptions in videos. _TACL_, 1:25-36, 2013.
* [75] Gao, J., X. Sun, M. Xu, et al. Relation-aware video reading comprehension for temporal language grounding. In _EMNLP_, pages 3978-3988. 2021.
* [76] Chen, T., S. Kornblith, M. Norouzi, et al. A simple framework for contrastive learning of visual representations. In _ICML_. 2020.
* [77] Kingma, D. P., J. Ba. Adam: A method for stochastic optimization. In _ICLR_. 2015.

* [78] Rodriguez-Opazo, C., E. Marrese-Taylor, B. Fernando, et al. DORi: Discovering object relationships for moment localization of a natural language query in a video. In _WACV_. 2021.
* [79] Lu, C., L. Chen, C. Tan, et al. Debug: A dense bottom-up grounding approach for natural language video localization. In _EMNLP_. 2019.
* [80] Escorcia, V., M. Soldan, J. Sivic, et al. Temporal localization of moments in video collections with natural language. _arXiv preprint arXiv:1907.12763_, 2019.
* [81] Lei, J., L. Yu, T. L. Berg, et al. TVR: A large-scale dataset for video-subtitle moment retrieval. In _ECCV_, pages 447-463. 2020.
* [82] Yuan, Y., X. Lan, X. Wang, et al. A closer look at temporal sentence grounding in videos: Dataset and metric. In _HUMA_. 2021.
* [83] Krishna, R., K. Hata, F. Ren, et al. Dense-captioning events in videos. In _ICCV_, pages 706-715. 2017.
* [84] Paszke, A., S. Gross, F. Massa, et al. Pytorch: An imperative style, high-performance deep learning library. In _NeurIPS_. 2019.

This supplementary material provides more details of our MomentDiff framework:

1. Implementation details.
2. Inference efficiency of MomentDiff.
3. More experiment results.
4. Broader impacts.

## Appendix A Implementation details

### Datasets

**Public datasets. Charades-STA**[23] serves as a benchmark dataset for the video moment retrieval task and is built upon the Charades dataset, originally collected for video action recognition and video captioning. The Charades-STA dataset comprises 6,672 videos and 16,128 video-query pairs, allocated for training (12,408 pairs) and testing (3,720 pairs). On average, the videos in this dataset have a duration of 29.76 seconds. Each video is annotated with an average of 2.4 moments, with each moment lasting approximately 8.2 seconds. **QVHighlights**[37] contains 10,148 videos, each 150 seconds long and annotated with at least one text query describing its relevant content. These videos are from three main categories, daily vlogs, travel vlogs, and news events. On average, there are approximately 1.8 non-overlapping moments per query, annotated on 2s non-overlapping clips. The dataset contains a total of 10,310 queries with 18,367 annotated moments. The training set, validation set and test set include 7,218, 1,550 and 1,542 video-text pairs, respectively. **TACoS**[74] is compiled specifically for video moment retrieval and dense video captioning tasks. It is comprised of 127 videos that depict cooking activities, with an average duration of 4.79 minutes. TACoS contains a total of 18,818 video-query pairs. In comparison to the Charades-STA dataset, TACoS has more video segments that are temporally annotated with queries per video. On average, each video contains 148 queries. Additionally, the TACoS dataset is known for its difficulty, as the queries it contains are limited to only a few seconds or even just a few frames. To ensure impartial comparisons, we use the same dataset split [31], which consists of 10,146, 4,589, and 4,083 video-query pairs for the training, validation, and testing sets, respectively. **ActivityNet-Captions**[83] comprises of 20,000 videos and 100,000 descriptions that encompass a wide range of contexts. Similar to [27], we designate val 1 as the validation set and val 2 as the testing set. The dataset contains 37,417, 17,505, and 17,031 pairs of moments and corresponding sentences for training, validation, and testing respectively.

**Anti-bias datasets.** To explore the location bias problem, we construct two anti-bias datasets with location distribution shifts based on the Charades-STA dataset. In video moment retrieval, length and position are important parameters of spans.

Therefore, we first investigate the effect of span length \(\bm{w}_{0}\). As shown in Tab. 7, in the training set of Charades-STA-Len, we collect 9,307 video-text pairs with span length \(\bm{w}_{0}\leq 10s\) and 2,326 video-text pairs with \(\bm{w}_{0}>10s\), accounting for 80% and 20% of the total training set. In contrast, in the test set, we select 197 video-text pairs with \(\bm{w}_{0}\leq 10s\) and 788 video-text pairs with \(\bm{w}_{0}>10s\), accounting for 20% and 80% of the total test set.

Then, we design the dataset Charades-STA-Mom based on the span's end time \(\bm{c}_{0}+\bm{w}_{0}/2\) and start time \(\bm{c}_{0}-\bm{w}_{0}/2\). In the training set of Charades-STA-Mom, we collect 5,330 video-text pairs with \(\bm{c}_{0}+\bm{w}_{0}/2\leq 15s\) and 1,332 video-text pairs with \(\bm{c}_{0}-\bm{w}_{0}/2>15s\), accounting for 80% and 20% of the total training set. In contrast, in the test set, we select 259 video-text pairs with \(\bm{c}_{0}+\bm{w}_{0}/2\leq 15s\) and 788 video-text pairs with \(\bm{c}_{0}-\bm{w}_{0}/2>15s\), accounting for 20% and 80% of the total test set.

\begin{table}
\begin{tabular}{l c c|c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{Charades-STA-Len} & \multicolumn{3}{c}{Charades-STA-Mom} \\  & \(\bm{w}_{0}\leq 10s\) & \(\bm{w}_{0}>10s\) & Total & \(\bm{c}_{0}+\bm{w}_{0}/2\leq 15s\) & \(\bm{c}_{0}-\bm{w}_{0}/2>15s\) & Total \\ \hline Training & 9307 & 2326 & 11633 & 5330 & 1332 & 6662 \\ Test & 197 & 788 & 985 & 259 & 1038 & 1297 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Training and test sets on two anti-bias datasets.

```
#Videofeatures:v_feats\(\in\mathbb{R}^{N_{v}\times D}\)
#Textfeatures:t_feats\(\in\mathbb{R}^{N_{l}\times D}\)
#Groundtruthspans:gt_spans:[*,2]
#alpha_cumprod(m):cumulativeproductof\(\alpha_{i}\)
#Fully-connectedlayer:FC()
#VideoMomentDenoiser:VMD() deftrain(v_feats,t_feats,gt_spans):  #Similarity-awareConditionGenerator  f_feats=SCG(v_feats,t_feats)#\(N_{v}\times D\)
#Spannormalization  ps=pad_spans(gt_spans)#Padgt_spansto[\(N_{r},2\)]  ps=(ps*2-1)*\(\lambda\)#Signalscaling  m=randint(0,M)#Noiseintensity  noi=normal(mean=0,std=1)#Noise  ps_m=sqrt(alpha_cumprod(m))*ps+  sqrt(1-alpha_cumprod(m))*noi#Noisyspan  ps_m=(ps_m/\(\lambda\)+1)/2#Normalization
#Spanembedding  ps_emb_m=FC(ps_m)
#Intensity-awareattention  output_m=VMD(ps_emb_m,m,f_feats)#Outputembedding
#Denoisingtraining  hat_ps_m=Span_pred(output_m)#Predictedspan  hat_cs_m=Score_pred(output_m)#Confidencescore  #Computingloss  loss=loss_sim(f_feats)+loss_vmr(hat_ps_m,hat_cs_m,gt_spans)  returnloss ```

**Algorithm 1**MomentDiff Training in a PyTorch-like style.

### Pseudo Code of MomentDiff

Algorithm 1 provides the pseudo-code of MomentDiff Training in a PyTorch-like style.

The inference procedure of MomentDiff is a denoising sampling process from noise to temporal spans. Starting from spans sampled in Gaussian distribution, the model progressively refines its predictions, as shown in Algorithm 2.

## Appendix B Inference Efficiency of MomentDiff

**Inference time.** Inference efficiency is critical for machine learning models. We test 2DTAN [27], MMN [28], MomentDETR [37] and MomentDiff on the Pytorch framework [84] in Tab. 8. We test all models with one NVIDIA Tesla A100 GPU.

Compared with 2DTAN [27] and MMN [28], MomentDiff (Step=1) not only achieves the best results on recall, but also improves the inference speed by 5-7 times. This is because 2DTAN and MMN predefine a large number of proposals, which may be redundant and increase computational overhead. Compared to MomentDETR [37], MomentDiff (Step=10) achieves better results with similar inference time. The possible reasons are that we adopt fewer random spans, very simple network structures, and avoid post-processing.

## Appendix C More Experiment Results

### Experiment on ActivityNet-Captions

The results of our method on ActivityNet-Captions are shown in Tab. 9. We record the 1 epoch training time and the inference time for all test samples in one NVIDIA Tesla A100 GPU. Our model has been improved compared with baseline (MomentDETR). Compared to SOTAs, we still have competitive results. Compared with MMN, our method is 25 times faster in training time and 7.24 times faster in testing time.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{Charades-STA} & Inference time \\  & R1@0.5 & R1@0.7 & MAP\({}_{avg}\) & (second) \\ \hline
2DTAN [27] & 41.34 & 23.91 & 29.26 & 42.18 \\ MMN [28] & 46.93 & 27.07 & 31.58 & 53.42 \\ MomentDETR [37] & 50.54 & 28.01 & 29.87 & 12.42 \\ \hline MomentDiff (Step=1) & 49.17 & 26.39 & 29.12 & 7.56 \\ MomentDiff (Step=2) & 50.81 & 27.84 & 31.27 & 8.23 \\ MomentDiff (Step=10) & **52.36** & 28.08 & **31.75** & 11.01 \\ MomentDiff (Step=50) & 51.94 & 28.25 & 31.66 & 20.74 \\ MomentDiff (Step=100) & 52.21 & **28.84** & 31.01 & 34.35 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The inference time of 2DTAN [27], MMN [28], MomentDETR [37] and MomentDiff on Charades-STA with VGG video features and Glove text features. We report R1@0.5, R1@0.7 and MAP\({}_{avg}\). Default settings are marked in blue.

### Error bars

Fig. 6 shows the performance fluctuation of the model on the Charades-STA dataset. We use different random seeds (seed= 2023, 2022, 2021, 2020, 2019) and different features (VGG, Glove; C3D, Glove; SF+C, C;) to organize experiments. This shows that the model always converges and achieves stable results for different initializations. This phenomenon demonstrates the ability of the model to learn to generate real spans from arbitrary random spans.

### Ablation study

**Different sampling strategies.** Denoising Diffusion Probabilistic Models (DDPM) [49] and Denoising Diffusion Implicit Models (DDIM) [69] are popular and classic diffusion models. We show the results of both strategies in Tab. 10(a). We find that DDIM and DDPM perform similarly, but DDIM samples faster. Therefore we adopt DDIM as the default technology.

**Effect of the schedule of \(\beta\).** The schedule of \(\beta\) determines the weighting ratio of different intensity noises. In Tab. 10(b), we find that the cosine schedule works better in our experiment. The cosine schedule makes the noisy spans change slowly at the beginning and end during the diffusion process, and the generation effect is more stable. So we set the cosine schedule as the default.

**Effect of the Box Renewal.** Box Renewal is a post-processing technique in DiffusionDet [61]. Tab. 10(c) shows that Box Renewal can indeed slightly improve the results. To keep the inference process as simple as possible, we do not use Box Renewal by default.

**Effect of the batch size.** As shown in Tab. 10(d), we set the batch size to 32 to achieve the best results.

**Effect of the layer number.** In Tab. 10(e), we show the effect of the layer number of Similarity-aware Condition Generator (SCG) and Video Moment Denoiser (VMD). The default setting (2+2:2): SCG contains 2 cross-attention and 2 self-attention layers and VMD contains 2 cross-attention layers. The default setting works best.

**Effect of the weight value on \(\mathcal{L}_{sim}\).** We set the weight values of \(\mathcal{L}_{sim}\) and \(\mathcal{L}_{vmr}\) to 4 and 1, respectively. Keeping the weight value of \(\mathcal{L}_{vmr}\) unchanged, we organize the weight influence experiment of \(\mathcal{L}_{sim}\), as shown in Tab. 10(f). The default setting works best.

**Effect of the number of spans on R1@0.5.** Our training and inference are decoupled. Our simple framework allows us to input any number of random noises. Tab. 10(g) shows that there is a slight improvement when testing with more noise boxes.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Method & R1@0.3 & R1@0.5 & R1@0.7 & MAP\({}_{avg}\) & Training time & Inference time \\
2DTAN [27] & 59.92 & 44.63 & 27.53 & 27.26 & 1.1h & 523.74s \\ MMN [28] & **65.21** & **48.26** & **28.95** & **28.74** & 1.5h & 662.12s \\ MomentDETR [37] & 61.87 & 43.19 & 25.74 & 25.63 & **0.05h** & **52.72s** \\ MomentDiff & 62.79 & 46.52 & 28.43 & 28.19 & 0.06h & 91.43s \\ \hline \hline \end{tabular}
\end{table}
Table 9: Performance comparisons (%) on ActivityNet-Captions with C3D video features and Glove text features.

Figure 6: Performance fluctuations (%) corresponding to different features and multiple random seeds on the Charades-STA dataset.

## Appendix D Broader Impacts

First, our work does not involve private data. Second, we believe that AI is a double-edged sword, and our model is no exception. For example, when users or websites use our model, only natural language is needed to locate video moments and collect desired video material, which improves the productivity of society. However, this may have a negative impact if the natural language entered by the user contains words related to violence, pornography, etc. We will consider these scenarios and implement a more secure VMR model.

\begin{table}

\end{table}
Table 10: Ablation study (%) on the Charades-STA dataset with SF+C video features and CLIP text features. We report R1@0.5, R1@0.7 and MAP\({}_{avg}\). Default settings are marked in blue.