# Small Total-Cost Constraints in Contextual Bandits

with Knapsacks, with Application to Fairness

 Evgenii Chzhen  Christophe Giraud

Universite Paris-Saclay, CNRS, Laboratoire de mathematiques d'Orsay, 91405, Orsay, France

{evgenii.chzhen, christophe.giraud}@universite-paris-saclay.fr

 Zhen Li

BNP Paribas Corporate and Institutional Banking, 20 boulevard des Italiens, 75009 Paris, France

zhen.li@bnpparibas.com

Gilles Stoltz

Universite Paris-Saclay, CNRS, Laboratoire de mathematiques d'Orsay, 91405, Orsay, France

HEC Paris, 78351 Jouy-en-Josas, France

gilles.stoltz@universite-paris-saclay.fr, stoltz@hec.fr

###### Abstract

We consider contextual bandit problems with knapsacks [CBwK], a problem where at each round, a scalar reward is obtained and vector-valued costs are suffered. The learner aims to maximize the cumulative rewards while ensuring that the cumulative costs are lower than some predetermined cost constraints. We assume that contexts come from a continuous set, that costs can be signed, and that the expected reward and cost functions, while unknown, may be uniformly estimated--a typical assumption in the literature. In this setting, total cost constraints had so far to be at least of order \(T^{3/4}\), where \(T\) is the number of rounds, and were even typically assumed to depend linearly on \(T\). We are however motivated to use CBwK to impose a fairness constraint of equalized average costs between groups: the budget associated with the corresponding cost constraints should be as close as possible to the natural deviations, of order \(\sqrt{T}\). To that end, we introduce a dual strategy based on projected-gradient-descent updates, that is able to deal with total-cost constraints of the order of \(\sqrt{T}\) up to poly-logarithmic terms. This strategy is more direct and simpler than existing strategies in the literature. It relies on a careful, adaptive, tuning of the step size.

## 1 Setting, literature review, and main contributions

We consider contextual bandits with knapsacks [CBwK], a setting where at each round \(t\geqslant 1\), the learner, after observing some context \(\bm{x}_{t}\in\mathcal{X}\), where \(\mathcal{X}\subseteq\mathbb{R}^{n}\), picks an action \(a_{t}\in\mathcal{A}\) in a finite set \(\mathcal{A}\). We do not impose the existence of a null-cost action. Contexts are independently drawn according to a distribution \(\nu\). The learner may pick \(a_{t}\) at random according to a probability distribution, denoted by \(\bm{\pi}_{t}(\bm{x}_{t})=\big{(}\pi_{t,a}(\bm{x}_{t})\big{)}_{a\in\mathcal{A}}\) for consistency with the notion of policy defined later in Section 2. The action \(a_{t}\) played leads to some scalar reward \(r_{t}\in[0,1]\) and some signed vector-valued cost \(\bm{c}_{t}\in[-1,1]^{d}\). Actually, \(r_{t}\) and \(\bm{c}_{t}\) are generated independently at random in a way such that the conditional expectations of \(r_{t}\) and \(\bm{c}_{t}\) given the past, \(\bm{x}_{t}\), and \(a_{t}\), equal \(r(\bm{x}_{t},a_{t})\) and \(\bm{c}(\bm{x}_{t},a_{t})\), respectively. We denoted here by \(r:\mathcal{X}\times\mathcal{A}\to[0,1]\) and \(\bm{c}=(c_{1},\ldots,c_{d}):\mathcal{X}\times\mathcal{A}\to[-1,1]^{d}\) the unknown expected-reward and expected-cost functions. The modeling and the estimation of

**Know parameters:** finite action set \(\mathcal{A}\); context set \(\mathcal{X}\subseteq\mathbb{R}^{n}\); number \(T\) of rounds; vector of average cost constraints \(\bm{B}\in[0,1]^{d}\)

**Unknown parameters:** context distribution \(\nu\) on \(\mathcal{X}\); scalar expected-reward function \(r:\mathcal{X}\times\mathcal{A}\to[0,1]\) and vector-valued expected-cost function \(\bm{c}:\mathcal{X}\times\mathcal{A}\to[-1,1]^{d}\), both enjoying some modeling allowing for estimation, see Section 2.2

**For rounds \(t=1,2,3,\dots,T\)**:

1. Context \(\bm{x}_{t}\sim\nu\) is drawn independently of the past;
2. Learner observes \(\bm{x}_{t}\) and picks an action \(a_{t}\in\mathcal{A}\), possibly at random according to a distribution \(\bm{\pi}_{t}(\bm{x}_{t})=\big{(}\pi_{t,a}(\bm{x}_{t})\big{)}_{a\in\mathcal{A}}\);
3. Learner gets a reward \(r_{t}\in[0,1]\) with conditional expectation \(r(\bm{x}_{t},a_{t})\) and suffers constraints \(\bm{c}_{t}\) with conditional expectations \(\bm{c}(\bm{x}_{t},a_{t})\).

**Goal:** Maximize \(\sum_{t\leqslant T}r_{t}\) while ensuring \(\sum_{t\leqslant T}\bm{c}_{t}\leqslant T\bm{B}\)

and \(\bm{c}\) are discussed in Section 2.2. The aim of the learner is to maximize the sum of rewards (or equivalently, to minimize the regret defined in Section 2) while controlling cumulative costs. More precisely, we denote by \(\bm{B}=(B_{1},\dots,B_{d})\in[0,1]^{d}\) the normalized (i.e., average per-instance) cost constraints, which may depend on the coordinates. The number \(T\) of rounds is known, and the learner must play in a way such that \(\bm{c}_{1}+\dots+\bm{c}_{T}\leqslant T\bm{B}\). The example of Section 2.1 illustrates how this setting allows for controlling costs in absolute values. The setting described is summarized in Box A.

**Overview of the literature review.** Contextual bandits with knapsacks [CBwK] is a setting that is a combination of the problems of contextual bandits (where only rewards are obtained, and no costs: see, among others, Lattimore and Szepesvari, 2020, Chapter 18 for a survey of this rich area) and of bandits with knapsacks (without contexts, as initially introduced by Badanidiyuru et al., 2013, 2018). The first approaches to CBwK (Badanidiyuru et al., 2014, Agrawal et al., 2016) relied on no specific modeling of the rewards and costs, and made the problem tractable by using as a benchmark a finite set of so-called static policies (but picking this set is uneasy, as noted by Agrawal and Devanur, 2016). Virtually all subsequent approaches to CBwK thus introduced structural assumptions in one way or the other. The simplest modelings are linear dependencies of expected rewards and costs on the contexts (Agrawal and Devanur, 2016) and a logistic conversion model (Li and Stoltz, 2022). The problem of CBwK attracted attention in recent past: we discuss and contrast the contributions by Chohlas-Wood et al. (2021); Slivkins et al. (2022); Han et al. (2022); Ai et al. (2022); Li and Stoltz (2022) after stating our main contributions.

**Overview of our main contributions.** Each contribution calls or allows for the next one.

**1.** This article revisits the CBwK approach by Chohlas-Wood et al. (2021) to a problem of fair spending of resources between groups while maximizing some total utility. Fairness constraints require to deal with signed costs and with possibly small total-cost constraints (typically close to \(T^{1/2}\)), while having no null-cost action at disposal.

**2.** We therefore provide a new CBwK strategy dealing with these three issues, the most significant being handling cost constraint \(T\bm{B}\) as small as \(T^{1/2+\varepsilon}\), breaking the state-of-the-art \(T^{3/4}\) barrier for CBwK with continuous contexts, while preserving attractive computational efficiency.

**3.** This new strategy is a direct, simple, explicit, dual strategy, relying on projected-gradient-descent updates (instead of using existing general algorithms as subroutines). We may perform an ad hoc analysis. The latter leads to refined regret bounds, in terms of the norms of some optimal dual variables. We also discuss the optimality of the results achieved by offering a proof scheme for problem-dependent (not only minimax) regret lower bounds.

### Detailed literature review

We now contrast our main contributions to the existing literature.

**CBwK and fairness: missing tools.** In the setting of Chohlas-Wood et al. (2021), there is a budget constraint on the total spendings on top of the wish of sharing out these spendings among groups (possibly favoring to some maximal extent some groups that are more in need). However,Chohlas-Wood et al. (2021) incorporated the fairness constraints in the reward function (through some Lagrangian penalty) instead of doing so in the vector-cost function, which does not seem the most natural way to proceed. One reason is that (see a discussion below) the total cost constraints \(T\bm{B}\) were so far typically assumed to be linear, which is undesirable in fairness applications, where one wishes that \(T\bm{B}\) is sublinear--and actually, is sometimes as close as possible to the natural \(\sqrt{T}\) deviations suffered even in a perfectly fair random allocation scheme. Also, there is no null-cost action and costs are signed--two twists to the typical settings of CBwK that only Slivkins et al. (2022) and our approach deal with, to the best of our knowledge. We describe our alternative modeling of this fairness problem in Section 2.1.

**Orders of magnitude for total-cost constraints \(T\bm{B}\).** The literature so far mostly considered linear total-cost constraints \(T\bm{B}\) (see, e.g., Han et al., 2022 or Ai et al., 2022 among recent references), with two series of exceptions: (i) the primal strategy by Li and Stoltz (2022) handling total-cost contraints of the order of \(\sqrt{T}\) up to poly-logarithmic terms but critically assuming the finiteness of the context set \(\mathcal{X}\); and (ii) the two-stage dual strategies of Agrawal and Devanur (2016); Han et al. (2022) handling \(T^{3/4}\) total-cost constraints. These two-stage strategies use \(\sqrt{T}\) preliminary rounds to learn some key hyperparameter \(Z\) and then transform strategies that work with linear total-cost constraints into strategies that may handle total-cost constraints larger than \(T^{3/4}\); see discussions after Lemma 2. In contrast, we provide a general theory of small cost constraints for continuous context sets, while obtaining similar regret bounds as the literature does: for some components \(j\), we may have \(TB_{j}\ll T^{3/4}\) (actually, any rate \(TB_{j}\gg T^{1/2+\varepsilon}\) is suitable for a sublinear regret), while for other components \(k\), the total-cost constraints \(TB_{k}\) may be linearly large.

**Typical CBwK strategies: primal approaches suffer from severe limitations.** CBwK is typically solved through dual approaches, as the latter basically rely on learning a finite-dimensional parameter given by the optimal dual variables \(\bm{\lambda}^{\star}\) (see the beginning of Section 3), while primal approaches rely on learning the distribution \(\nu\) of the contexts in \(\mathbb{L}^{1}\)-distance. This may be achieved in some cases, like finiteness of the context set \(\mathcal{X}\)(Li and Stoltz, 2022; Ai et al., 2022) or, at the cost of degraded regret bounds and under additional assumptions on \(\nu\) when resorting to \(\mathbb{L}^{1}\)-density-estimation techniques (Ai et al., 2022). On top of these strong limitations, such primal approaches are also typically computationally inefficient as they require the computation of expectations over estimates of \(\nu\).

**Typical CBwK strategies: dual approaches are modular and somewhat indirect.** Typical dual approaches in CBwK take two general forms. One, illustrated in Slivkins et al. (2022) (extending the non-contextual approach by Immorlica et al., 2019), takes a game-theoretic approach by identifying the primal-dual formulation (4) as some minimax equilibrium, which may be learned by separate regret minimization of a learner picking actions \(a_{t}\) and an opponent picking dual variables \(\bm{\lambda}_{t}\). The second approach (Agrawal and Devanur, 2016; Han et al., 2022) learns more directly the \(\bm{\lambda}_{t}\) via some online convex optimization algorithm fed with the suffered costs \(\bm{c}_{t}\); this second approach however requires, at the stage of picking \(a_{t}\), a suitable bound \(Z\) on the norms of the \(\bm{\lambda}_{t}\).

The dual strategies discussed above are modular and indirect as they all consist of using as building blocks some general-purpose strategies. We rather propose a more direct dual approach, tailored to our needs. (We also believe that it is simpler and more elegant.) Our strategy picks the arm \(a_{t}\) that maximizes the Lagrangian penalization of rewards by costs through the current \(\bm{\lambda}_{t-1}\) (see Step 2 in Box B), as also proposed by Agrawal and Devanur (2016) (while Han et al., 2022 add some randomization to this step), and then, performs some direct, explicit, projected-gradient-descent update on \(\bm{\lambda}_{t-1}\) to obtain \(\bm{\lambda}_{t}\), with step size \(\gamma\). We carefully and explicitly tune \(\gamma\) (in a sequential fashion, via regimes) to achieve our goals. Such fine-tuning is more difficult to perform with approaches relying on the black-box use of subroutines given by existing general-purpose strategies.

**Regret (upper and lower) bounds typically proposed in the CBwK literature.** A final advantage of our more explicit strategy is that we master each piece of its analysis: while not needing a null-cost action, we provide refined regret bounds that go beyond the typical \((\textsc{opt}(r,\bm{c},\bm{B})/\min\bm{B})\sqrt{T}\) bound offered by the literature so far (see, among others, Agrawal and Devanur, 2016; Li and Stoltz, 2022; Han et al., 2022), where \(\textsc{opt}(r,\bm{c},\bm{B})\) denotes the expected reward achieved by the optimal static policy (see Section 2 for definitions). Namely, our bounds are of the order of \(\|\bm{\lambda}^{\star}\|\sqrt{T}\), where \(\bm{\lambda}^{\star}\) is the optimal dual variable; we relate the norm of the latter to quantities of the form \(\big{(}\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c},\alpha\bm{B}) \big{)}/\big{(}(1-\alpha)\min\bm{B}\big{)}\), for some \(\alpha<1\). Again, we may do so without the existence of a null-cost action, but when one such action is available, we may take \(\alpha=0\) in the interpretation of the bound. We also offer a proof scheme for lower bounds in Section 4 and explain why \(\|\bm{\lambda}^{\star}\|\sqrt{T}\) appears as the correct target. We compare therein our problem-dependent lower bound-approach to the minimax ones of Han et al. (2022) and Slivkins et al. (2022).

### Outline

In Section 2, we further describe the problem of CBwK (by defining the regret and recalling how \(r\) and \(\bm{c}\) may be estimated) and state our motivating example of fairness. Section 3 is devoted to our new dual strategy, which we analyze first with a fixed step size \(\gamma\), for which we move next to an adaptive version, and whose bounds we finally discuss. Section 4 offers a proof scheme for regret lower bounds and lists some limitations of our approach, mostly relative to optimality.

## 2 Further description of the setting, and statement of our motivating example

We define a static policy as a mapping \(\bm{\pi}:\mathcal{X}\to\mathcal{P}(\mathcal{A})\), where \(\mathcal{P}(\mathcal{A})\) denotes the set of probability distributions over \(\mathcal{A}\). We denote by \(\bm{\pi}=(\pi_{a})_{a\in\mathcal{A}}\) the components of \(\bm{\pi}\). We let \(\mathbb{E}_{\bm{X}\sim\nu}\) indicate that the expectation is taken over the random variable \(\bm{X}\) with distribution \(\nu\). In the sequel, the inequalities \(\leqslant\) or \(\geqslant\) between vectors will mean pointwise satisfaction of the corresponding inequalities.

**Assumption 1**.: _The contextual bandit problem with knapsacks \((r,\bm{c},\bm{B}^{\prime},\nu)\) is feasible if there exists a stationary policy \(\bm{\pi}\) such that_

\[\mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi _{a}(\bm{X})\Bigg{]}\leqslant\bm{B}^{\prime}\,.\]

We denote the average cost constraints by \(\bm{B}^{\prime}\) in the assumption above as in Section 3, we will actually require feasibility for average cost constraints \(\bm{B}^{\prime}<\bm{B}\), not just for \(\bm{B}\).

In a feasible problem \((r,\bm{c},\bm{B}^{\prime},\nu)\), the optimal static policy \(\bm{\pi}^{\star}\) is defined as the policy \(\bm{\pi}:\mathcal{X}\to\mathcal{P}(\mathcal{A})\) achieving

\[\textsc{opt}(r,\bm{c},\bm{B}^{\prime})=\sup_{\bm{\pi}}\Bigg{\{}\mathbb{E}_{ \bm{X}\sim\nu}\Bigg{[}\sum_{a\in\mathcal{A}}r(\bm{X},a)\,\pi_{a}(\bm{X})\Bigg{]} :\ \ \mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi_{ a}(\bm{X})\Bigg{]}\leqslant\bm{B}^{\prime}\Bigg{\}}.\] (1)

Maximizing the sum of the \(r_{t}\) amounts to minimizing the regret \(R_{T}=T\,\textsc{opt}(r,\bm{c},\bm{B}^{\prime})-\sum_{t=1}^{T}r_{t}\,\).

### Motivating example: fairness--equalized average costs between groups

This example is inspired from Chohlas-Wood et al. (2021) (who did not provide a strategy to minimize regret), and features a total budget constraint \(TB_{\text{total}}\) together with fairness constraints on how the budget is spent among finitely many subgroups, whose set is denoted by \(\mathcal{G}\). We assume that the contexts \(\bm{x}_{t}\) include the group index \(\operatorname{gr}(\bm{x}_{t})\in\mathcal{G}\), which means, in our setting, that the learner accesses this possibly sensitive attribute before making the predictions (the so-called awareness framework in the fairness literature). Each context-action pair \((\bm{x},a)\in\mathcal{X}\times\mathcal{A}\), on top of leading to some expected reward \(r(\bm{x},a)\), also corresponds to some spendings \(c_{\text{spd}}(\bm{x},a)\). We recall that we denoted that \(r_{t}\) and \(c_{t}\) (here, this is a scalar for now) the individual payoffs and costs obtained at round \(t\); they have conditional expectations \(r(\bm{x}_{t},a_{t})\) and \(c_{\text{spd}}(\bm{x}_{t},a_{t})\). We want to trade off the differences in average spendings among groups with the total reward achieved: larger differences lead to larger total rewards but generate feelings of inequity or of public money not optimally used. (Chohlas-Wood et al. (2021) consider a situation where unfavored groups should get slightly more spendings than favored groups.)

Formally, we denote by \(\operatorname{gr}(\bm{x})\in\mathcal{G}\) the group to which a given context \(\bm{x}\) belongs and introduce a tolerance factor \(\tau\), which may depend on \(T\). We issue a simplifying assumption: while the distribution \(\nu\) of the contexts is complex and is unknown, the respective proportions \(\gamma_{g}=\nu\big{\{}\bm{x}:\operatorname{gr}(\bm{x})=g\big{\}}\) of the sub-groups may be known.1 The total-budget and fairness constraints then read:

Footnote 1: This simplification is not unheard of in the fairness literature; see, for instance, Chzhen et al. (2021). It amounts to having a reasonable knowledge of the breakdown of the population into subgroups. We use this assumption to have an easy rewriting of the fairness constraints in the setting of Box A. The knowledge of the \(\gamma_{g}\) is key for determining the average-constraint vector \(\bm{B}\).

\[\sum_{t=1}^{T}c_{t}\leqslant TB_{\text{total}}\qquad\text{and}\qquad\forall g \in\mathcal{G},\qquad\Bigg{|}\frac{1}{T\gamma_{g}}\sum_{t=1}^{T}c_{t}\,\mathds{1 }_{\{\operatorname{gr}(\bm{x}_{t})=g\}}-\frac{1}{T}\sum_{t=1}^{T}c_{t}\Bigg{|} \leqslant\tau\,,\]which corresponds, in the setting of Box A, to the following vector-valued expected-cost function \(\bm{c}\), with \(d=1+2|\mathcal{G}|\) components and with values in \([-1,1]^{1+2|\mathcal{G}|}\):

\[\left(c_{\text{\tiny{gold}}}(\bm{x},a),\ \left(c_{\text{\tiny{gold}}}(\bm{x},a) \mathds{1}_{\{\mathbb{g}(\bm{x})=g\}}-\gamma_{g}c_{\text{\tiny{gold}}}(\bm{x},a),\ \gamma_{g}c_{\text{\tiny{gold}}}(\bm{x},a)-c_{\text{\tiny{gold}}}(\bm{x},a) \mathds{1}_{\{g(\bm{x})=g\}}\right)_{g\in\mathcal{G}}\right)\]

as well as to the average-constraint vector \(\bm{B}=\left(B_{\text{\tiny{total}}},\ \left(\gamma_{g}\,\tau,\gamma_{g}\, \tau\right)_{g\in\mathcal{G}}\right)\).

The regimes we have in mind, and that correspond to the public-policy issues reported by Chohlas-Wood et al. (2021), are that the per-instance budget \(B_{\text{\tiny{total}}}\) is larger than a positive constant, i.e., a constant fraction of the \(T\) individuals may benefit from some costly action(s), while the fairness threshold \(\tau\) must be small, and even, as small as possible. Because of central-limit-theorem fluctuations, a minimal value of the order of \(1/\sqrt{T}\), or slightly larger (up to logarithmic terms, say), has to be considered for \(\tau\). The salient point is that the components of \(\bm{B}\) may therefore be of different orders of magnitude, some of them being as small as \(1/\sqrt{T}\), up to logarithmic terms. More details on this example and numerical simulations about it are provided in Section 5 and Appendix G.

### Modelings for, and estimation of, expected-reward and expected-cost functions

As is common in the literature (see Han et al., 2022 and Slivkins et al., 2022, who use the terminology of regression oracles), we sequentially estimate the functions \(r\) and \(\bm{c}\) and assume that we may do so in some uniform way, e.g., because of the underlying structure assumed. Note that the estimation procedure of Assumption 2 below does not force any specific choice of actions, it is able to exploit actions \(a_{t}\) picked by the main strategy (this is what the "otherwise" is related to therein). We denote by \(\widehat{r}_{t}:\mathcal{X}\times\mathcal{A}\to[0,1]\) and \(\widehat{\bm{c}}_{t}:\mathcal{X}\times\mathcal{A}\to[-1,1]^{d}\) the estimations built based on \((\bm{x}_{s},a_{s},r_{s},\bm{c}_{s})_{s\leqslant t}\), for \(t\geqslant 1\). We also assume that initial estimations \(\widehat{r}_{0}\) and \(\widehat{\bm{c}}_{0}\), based on no data, are available. We denote by \(\bm{1}\) the column-vector \((1,\ldots,1)\).

**Assumption 2**.: _There exists an estimation procedure such that for all individual sequences of contexts \(\bm{x}_{1},\bm{x}_{2},\ldots\) and of actions \(a_{1},a_{2},\ldots\) played otherwise, there exist known error functions \(\varepsilon_{t}:\mathcal{X}\times\mathcal{A}\times(0,1]\to[0,1]\), relying each on the pairs \((\bm{x}_{s},a_{s})_{s\leqslant t}\), where \(t=0,1,2,\ldots\), ensuring that for all \(\delta\in(0,1]\), with probability at least \(1-\delta\),_

\[\forall 0\leqslant t\leqslant T,\ \ \forall\bm{x}\in \mathcal{X},\ \ \forall a\in\mathcal{A}, \left|\widehat{r}_{t}(\bm{x},a)-r(\bm{x},a)\right| \leqslant\varepsilon_{t}(\bm{x},a,\delta)\] \[\text{and} \left|\widehat{\bm{c}}_{t}(\bm{x},a)-c(\bm{x},a)\right| \leqslant\varepsilon_{t}(\bm{x},a,\delta)\,\bm{1}\,,\]

_where we assume in addition that \(\ \ \beta_{T,\delta}\overset{\text{def}}{=}\sum_{t=1}^{T} \varepsilon_{t-1}(\bm{x}_{t},a_{t},\delta)=O\bigg{(}\sqrt{T}\ln\frac{T}{\delta }\bigg{)}\,.\)_

This assumption is satisfied at least for the two modelings discussed below, which are both of the form: for known transfer functions \(\bm{\varphi}\), link functions \(\Phi\), and normalization function \(\eta\), for some unknown finite-dimensional parameters \(\bm{\mu}_{\star}\), \(\bm{\theta}_{\star,1},\ldots,\bm{\theta}_{\star,d}\),

\[\forall\bm{x}\in\mathcal{X},\ \ \forall a\in\mathcal{A}, r(\bm{x},a)=\eta_{r}(a,\bm{x})\ \Phi_{r}\big{(}\bm{\varphi}_{r}(\bm{x},a)^{\mathbb{T}}\bm{\mu}_{\star}\big{)}\,,\] \[\text{and for all }1\leqslant i\leqslant d, c_{i}(\bm{x},a)=\eta_{c,i}(a,\bm{x})\ \Phi_{c,i}\big{(}\bm{\varphi}_{c,i}(\bm{x},a)^{\mathbb{T}}\bm{\theta}_{\star, i}\big{)}\,.\]

See also the exposition by Han et al. (2022, Section 3.3).

Based on Assumption 2 and given the ranges \([0,1]\) for rewards and \([-1,1]^{d}\) for costs, we may now define the following (clipped) upper- and lower-confidence bounds : given a confidence level \(1-\delta\) where \(\delta\in(0,1]\), for all \(t\leqslant T\), for all \(\bm{x}\in\mathcal{X}\) and \(a\in\mathcal{A}\),

\[\widehat{r}_{\delta,t}^{\text{\tiny{sub}}}(\bm{x},a)\overset{\text{def}}{=} \operatorname{clip}\left[\widehat{r}_{t}(\bm{x},a)+\varepsilon_{t}(\bm{x},a, \delta)\right]_{0}^{1}\ \text{ and }\ \widehat{\bm{c}}_{\delta,t}^{\text{\tiny{lb}}}(\bm{x},a)\overset{ \text{def}}{=}\operatorname{clip}\left[\widehat{\bm{c}}_{t}(\bm{x},a)- \varepsilon_{t}(\bm{x},a,\delta)\bm{1}\right]_{-1}^{1}\,,\] (2)

where we define clipping by \(\operatorname{clip}[x]_{\ell}^{u}=\min\bigl{\{}\max\{x,\ell\},\,u\bigr{\}}\) for a scalar value \(x\in\mathbb{R}\) and lower and upper bounds \(\ell\) and \(u\), and apply clipping component-wise to vectors. Under Assumption 2, we have that for all \(\delta\in(0,1]\), with probability at least \(1-\delta\),

\[\forall 0\leqslant t\leqslant T,\ \ \forall\bm{x}\in \mathcal{X},\ \ \forall a\in\mathcal{A}, r(\bm{x},a)\leqslant\widehat{r}_{\delta,t}^{\text{\tiny{ sub}}}(\bm{x},a)\leqslant r(\bm{x},a)+2\varepsilon_{t}(\bm{x},a,\delta)\] (3) \[\text{and} \bm{c}(\bm{x},a)-2\varepsilon_{t}(\bm{x},a,\delta)\bm{1}\leqslant \widehat{\bm{c}}_{\delta,t}^{\text{\tiny{lb}}}(\bm{x})\leqslant\bm{c}(\bm{x},a)\,.\]

Doing so, we have optimistic estimates of rewards (they are larger than the actual expected rewards) and of costs (they are smaller than the actual expected costs) for all actions \(a\), while for actions \(a_{t+1}\) played, and only these, we will also use the control from the other side, which will lead to manageable sums of \(\varepsilon_{t}(\bm{x}_{t+1},a_{t+1},\delta)\), given the control on \(\beta_{T,\delta}\) by Assumption 2.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

\[R_{T}\leqslant\widetilde{\mathcal{O}}\Big{(}\big{(}1+\|\bm{\lambda}_{\bm{B}-b_{T} \bm{1}}^{\star}\|\big{)}\sqrt{T}\Big{)}\qquad\text{and}\qquad\sum_{t\leqslant T} \bm{c}_{t}\leqslant T\bm{B}\,,\]

_where a fully closed-form, non-asymptotic, regret bound is stated in (19) in Appendix C._

A complete proof may be found in Appendix C. Its structure is the following; all statements hold with high probability. We denote by \(\ell_{k}\) the realized lengths of the regime. First, the number of regimes is bounded by noting that if regime \(K=\mathrm{i}\log\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\) is achieved, then by Lemma 1 and the choice of \(M_{T,\delta,K}\), the stopping condition of regime \(K\) will never be met; thus, at most \(K+1\) regimes take place. We also prove that \(\mathrm{i}\log\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\leqslant K_{T}= \mathrm{i}\mathrm{i}\mathrm{o}g\,T\). Second, on each regime \(k\), the difference of cumulated costs to \(\ell_{k}(\bm{B}-b_{T}\bm{1})\) is smaller than \(M_{T,\delta,k}\) by design, so that the total cumulative costs are smaller than \(T(\bm{B}-b_{T}\bm{1})\) plus something of the order of \((K_{T}+1)\,M_{T,\delta,K_{T}}\), which is a quantity that only depends on \(T\), \(\delta\), \(d\), and not on the unknown \(\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\), and that we take for \(T\,b_{T}\). Third, we similarly sum the regret bounds of Lemma 2 over the regimes: keeping in mind that \(b_{T}=\widetilde{\mathcal{O}}(\sqrt{T})\), we have sums of the form

\[\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\sum_{k\leqslant K}\big{(}b_{T} \ell_{k}+\sqrt{T}\big{)}\leqslant\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star} \|\,\widetilde{\mathcal{O}}\big{(}\sqrt{T}\big{)}\quad\text{and}\quad\sum_{k \leqslant K}\gamma_{k}T\leqslant\frac{2^{K+1}}{\sqrt{T}}\leqslant\frac{4\| \bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|}{\sqrt{T}}\,.\]

### Discussion of the obtained regret bounds

The regret bound of Theorem 1 features a multiplicative factor \(\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\) instead of the typical factor \(\textsc{opt}(r,\bm{c},\bm{B})/\min\bm{B}\) proposed by the literature (see, e.g., Agrawal and Devanur, 2016; Li and Stoltz, 2022; Han et al., 2022). In view of the lower bound proof scheme discussed in Section 4, this bound \(\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\,\sqrt{T}\) seems the optimal bound. We thus now provide bounds on \(\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\) that have some natural interpretation. We recall that feasibility was defined in Assumption 1. The elementary proofs of Lemma 3 and Corollary 1 are based on (4) and may be found in Appendix D.

**Lemma 3**.: _Let \(b\in[0,\min\bm{B})\) and let \(\bm{0}\leqslant\widetilde{\bm{B}}<\!\bm{B}-b\bm{1}\). If the contextual bandit problem with knapsacks \((r,\bm{c},\bm{B}^{\prime},\nu)\) is feasible for some \(\bm{B}^{\prime}<\widetilde{\bm{B}}\), then_

\[\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|\leqslant\frac{\textsc{opt}(r,\bm{c},\bm{B}-b\bm{1})-\textsc{opt}(r,\bm{c},\widetilde{\bm{B}})}{\min\big{(}\bm{B}- b\bm{1}-\widetilde{\bm{B}}\big{)}}\,.\]

**Corollary 1**.: _When there exists a null-cost action, \(\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|\leqslant 2\,\frac{\textsc{opt}(r,\bm{c},\bm{B})- \textsc{opt}(r,\bm{c},\bm{0})}{\min\bm{B}}\) for all \(b\in[0,\min\bm{B}/2]\)._

The bounds provided by Lemma 3 and Corollary 1 must be discussed in each specific case. They are null (as expected) when the average cost constraints \(\bm{B}\) are large enough so that costs do not constrain the choice of actions; this was not the case with the typical \(\textsc{opt}(r,\bm{c},\bm{B})/\min\bm{B}\) bounds of the literature. Another typical example is the following.

**Example 1**.: _Consider a problem featuring a baseline action \(a_{\text{\rm null}}\) with some positive expected rewards and no cost, and additional actions with larger rewards and scalar expected costs \(c(\bm{x},a)\geqslant\alpha>0\). Denote by \(B\) the average cost constraint. Then actions \(a\neq a_{\text{\rm null}}\) may only be played at most \(B/\alpha\) times on average, and therefore, \(\textsc{opt}(r,c,B)-\textsc{opt}(r,c,0)\leqslant B/\alpha\). In particular, the bound stated in Corollary 1 may be further upper bounded by \(1/(2\alpha)\), which is fully independent of \(T\) and \(B\); i.e., a \(\sqrt{T}\)-regret only is suffered in Theorem 1._

## 4 Optimality of the upper bounds, and limitations

The main limitations to our work are relative, in our eyes, to the optimality of the bounds achieved--up to one limitation, already discussed in Section 2.1: our fairness example is intrinsically limited to the awareness set-up, where the learner has access to the group index before making the predictions.

**A proof scheme for problem-dependent lower bounds.** Results offering lower bounds on the regret for CBwK are scarce in the literature. They typically refer to some minimax regret, and either do so by indicating that lower bounds for CBwK must be larger in a minimax sense than the ones obtained for non-contextual bandits with knapsacks (see, e.g., Agrawal and Devanur, 2016; Li and Stoltz, 2022), or by leveraging a minimax regret lower bound for a subroutine of the strategy used (Slivkins et al., 2022). We rather focus on problem-dependent regret bounds but only offer a proof scheme, to be made more formal--see Appendix E.1. Interestingly, this proof scheme follows closely the analysis of a primal strategy performed in Appendix F. First, an argument based on the law of the iterated logarithm shows the necessity of a margin \(b_{T}\) of order \(1/\sqrt{T}\) to satisfy the total \(T\bm{B}\) cost constraints. This imposes that only \(T\textsc{Opt}(r,\bm{c},\bm{B}-b_{T}\bm{1})\) is targeted, thus the regret is at least of order

\[T\big{(}\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c},\bm{B}-b_{T}\bm{1 })\big{)}+\sqrt{T}\,,\qquad\text{i.e.,}\qquad\big{(}1+\|\lambda^{\star}_{\bm{B }}\|\big{)}\sqrt{T}\,.\]

This matches the bound of Theorem 1, but with \(\lambda^{\star}_{\bm{B}-b_{T}\bm{1}}\) replaced by \(\lambda^{\star}_{\bm{B}}\).

We now turn to the list of limitations pertaining to the optimality of our bounds.

**Limitation 1: Large constants.** First, we must mention that in the bounds of Lemmas 1 and 2, we did not try to optimize the constants but targeted readability. The resulting values in Theorem 1, namely, the recommended choice of \(b_{T}\) and the closed-form bound (19) on the regret, therefore involve constants that are much larger than needed.

**Limitation 2: Not capturing possibly fast regret rates in some cases.** Second, a careful inspection of our proof scheme for lower bounds shows that it only works in the absence of a null-cost action. When such a null-cost action exists and costs are non-negative, Example 1 shows that costly actions are played at most \(B/\alpha\) times on average. If the null-cost action has also a null reward, the costly actions are the only ones generating some stochasticity, thus, we expect that the \(\sqrt{T}\) factors (coming, among others, from a version of Bernstein's inequality) be replaced by \(\sqrt{B/\alpha}\) factors. As a consequence, in the setting of Example 1, bounds growing much slower than \(\sqrt{T}\) should be achievable, see the discussion in Appendix E.1.

**Limitation 3: \(\sqrt{T}\) regret not captured in case of softer constraints.** For the primal strategy discussed in Appendix F, we could prove a general \(\sqrt{T}\) regret bound in the case where total costs smaller than \(T\bm{B}+\widetilde{\mathcal{O}}\big{(}\sqrt{T}\big{)}\) are allowed. We were unable to do so for our dual strategy.

## 5 Brief overview of the numerical experiments performed

In Appendix G, we implement a specific version of the motivating example of Section 2.1, as proposed by Chohlas-Wood et al. (2021) in the public repository https://github.com/stanford-policylab/learning-to-be-fair. Fairness costs together with spendings related to rideshare assistance or transportation vouchers are considered. We report below the performance of the strategies considered only in terms of average rewards and rideshare costs, and for the fairness threshold \(\tau=10^{-7}\). The Box B strategies with fixed step sizes may fail to control the budget when \(\gamma\) is too large: we observe this for \(\gamma=0.01\). We see overall a tradeoff between larger average rewards and larger average costs. The Box C strategy navigates between three regimes, \(\gamma=0.01\) to start (regime \(k=0\)), then \(\gamma=0.02\) (regime \(k=1\)), and finally \(\gamma=0.04\) (regime \(k=2\)), which it sticks to. It overall adaptively finds a decent tuning of \(\gamma\).

## Acknowledgments and Disclosure of Funding

Evgenii Chzhen, Christophe Giraud, Zhen Li, and Gilles Stoltz have no direct funding to acknowledge other than the salaries paid by their employers, CNRS, Universite Paris-Saclay, BNP Paribas, and HEC Paris. They have no competing interests to declare.

## References

* Abbasi-Yadkori et al. (2011) Y. Abbasi-Yadkori, D. Pal, and C. Szepesvari. Improved algorithms for linear stochastic bandits. In _Advances in Neural Information Processing Systems (NeurIPS'11)_, volume 24, 2011.
* Agrawal and Devanur (2016) S. Agrawal and N. Devanur. Linear contextual bandits with knapsacks. In _Advances in Neural Information Processing Systems (NeurIPS'16)_, volume 29, 2016.
* Agrawal et al. (2016) S. Agrawal, N.R. Devanur, and L. Li. An efficient algorithm for contextual bandits with knapsacks, and an extension to concave objectives. In _Proceedings of the 29th Annual Conference on Learning Theory (COLT'16)_, volume PMLR:49, pages 4-18, 2016.
* Ai et al. (2022) R. Ai, Z. Chen, X. Deng, Y. Pan, C. Wang, and M. Yang. On the re-solving heuristic for (binary) contextual bandits with knapsacks, November 2022. Preprint, arXiv:2211.13952.
* Badanidiyuru et al. (2013) A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with knapsacks. In _IEEE 54th Annual Symposium on Foundations of Computer Science (FOCS'13)_, pages 207-216, 2013. Latest extended version available at arXiv:1305.2545, dated September 2017.
* Badanidiyuru et al. (2014) A. Badanidiyuru, J. Langford, and A. Slivkins. Resourceful contextual bandits. In _Proceedings of the 27th Conference on Learning Theory (COLT'14)_, volume PMLR:35, pages 1109-1134, 2014.
* Badanidiyuru et al. (2018) A. Badanidiyuru, R. Kleinberg, and A. Slivkins. Bandits with global convex constraints and objective. _Journal of the ACM_, 65(3):1-55, 2018.
* Cesa-Bianchi et al. (2005) N. Cesa-Bianchi, G. Lugosi, and G. Stoltz. Minimizing regret with label-efficient prediction. _IEEE Transactions on Information Theory_, 51:2152-2162, 2005.
* Chohlas-Wood et al. (2021) A. Chohlas-Wood, M. Coots, H. Zhu, E. Brunskill, and S. Goel. Learning to be fair: A consequentialist approach to equitable decision-making, 2021. Preprint, arXiv:2109.08792.
* Chzhen et al. (2021) E. Chzhen, C. Giraud, and G. Stoltz. A unified approach to fair online learning via Blackwell approachability. In _Advances in Neural Information Processing Systems (NeurIPS'21)_, volume 34, 2021.
* Devroye and Gyorfi (1985) L. Devroye and L. Gyorfi. _Nonparametric Density Estimation: The \(\mathbb{L}_{1}\) View_. John Wiley & Sons, 1985.
* Faury et al. (2020) L. Faury, M. Abeille, C. Calauzenes, and O. Fercoq. Improved optimistic algorithms for logistic bandits. In _Proceedings of the 37th International Conference on Machine Learning (ICML'20)_, volume PMLR:119, pages 3052-3060, 2020.
* Filippi et al. (2010) S. Filippi, O. Cappe, A. Garivier, and C. Szepesvari. Parametric bandits: The generalized linear case. In _Advances in Neural Information Processing Systems (NeurIPS'10)_, volume 23, 2010.
* Freedman (1975) David A Freedman. On tail probabilities for martingales. _The Annals of Probability_, pages 100-118, 1975.
* Han et al. (2022) Y. Han, J. Zeng, Y. Wangy, Y. Xiang, and J. Zhang. Optimal contextual bandits with knapsacks under realizability via regression oracles, October 2022. Preprint, arXiv:2210.11834.
* Immorlica et al. (2019) N. Immorlica, K.A. Sankararaman, R. Schapire, and A. Slivkins. Adversarial bandits with knapsacks. In _2019 IEEE 60th Annual Symposium on Foundations of Computer Science (FOCS'19)_, pages 202-219, 2019.
* Lattimore and Szepesvari (2020) T. Lattimore and C. Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* Li and Stoltz (2022) Z. Li and G. Stoltz. Contextual bandits with knapsacks for a conversion model. In _Advances in Neural Information Processing Systems (NeurIPS'22)_, volume 35, 2022.
* Luenberger (1969) D. Luenberger. _Optimization by Vector Space Methods_. Series in Decision and Control. John Wiley & Sons, 1969.
* Slivkins et al. (2022) A. Slivkins, K.A. Sankararaman, and D.J. Foster. Contextual bandits with packing and covering constraints: A modular Lagrangian approach via regression, November 2022. Preprint, arXiv:2211.07484.
* Slivkins et al. (2020)A.B. Tsybakov. _Introduction to Nonparametric Estimation_. Springer Series in Statistics. Springer, 2008.
* Zinkevich [2003] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th International Conference on Machine Learning (ICML'03)_, pages 928-935, 2003.

Proof of the strong duality (4)

In this section, we explain why the equalities (4) hold when the problem \((r,\bm{c},\bm{B}^{\prime},\nu)\) is feasible for some \(\bm{B}^{\prime}<\bm{B}\). We restate first these inequalities for the convenience of the reader:

\[\textsc{opt}(r,\bm{c},\bm{B}) =\sup_{\bm{\pi}:\mathcal{X}\to\mathcal{P}(\bm{\mathcal{A}})}\,\inf _{\bm{\lambda}\geq\bm{0}}\,\mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}\sum_{a\in \mathcal{A}}r(\bm{X},a)\,\pi_{a}(\bm{X})+\left\langle\bm{\lambda},\,\bm{B}-\sum _{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi_{a}(\bm{X})\right\rangle\Bigg{]}\] \[\stackrel{{(*)}}{{=}} \min_{\bm{\lambda}\geq\bm{0}}\,\,\sup_{\bm{\pi}:\mathcal{X}\to \mathcal{P}(\bm{\mathcal{A}})}\,\mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}\sum_{a\in \mathcal{A}}\pi_{a}(\bm{X})\Big{(}r(\bm{X},a)-\big{\langle}\bm{c}(\bm{X},a)- \bm{B},\,\bm{\lambda}\big{\rangle}\Big{)}\Bigg{]}\] \[=\min_{\bm{\lambda}\geq\bm{0}}\,\,\mathbb{E}_{\bm{X}\sim\nu} \Bigg{[}\max_{a\in\mathcal{A}}\Big{\{}r(\bm{X},a)-\big{\langle}\bm{c}(\bm{X},a )-\bm{B},\,\bm{\lambda}\big{\rangle}\Big{\}}\Bigg{]}.\]

We deal here with a linear program, with primal variables in functional space \(\mathcal{X}\to\mathbb{R}^{\mathcal{A}}\) and dual variables in the finite-dimensional space \(\mathbb{R}^{d}\).

The first and third equalities are straightforward. The first equality holds because in \(\textsc{opt}(r,\bm{c},\bm{B})\), we want to consider only policies with expected cost smaller than or equal to \(\bm{B}\); the infimum over \(\lambda\geq\bm{0}\) equals \(-\infty\) for policies with expected costs strictly larger than \(\bm{B}\) and is achieved at \(\bm{\lambda}=0\) otherwise. The third inequality follows from identifying that for a given \(\bm{\lambda}\), the best policy may be defined pointwise as the argument of the maximum written in the expectation. Thus, only the middle equality (\(\star\)) deserves a proof. We obtain it by applying a general theorem of strong duality (which requires feasibility for slightly smaller cost constraints).

Statement of a general theorem of strong duality.We restate a result extracted from the monograph by Luenberger (1969). It relies on the dual functional \(\varphi\), whose expression we recall below.

**Theorem 2** (stated as Theorem 1 in Section 8.6, page 224 in Luenberger, 1969).: _Let \(f\) be a real-valued convex functional defined on a convex subset \(\Omega\) of a vector space \(X\), and let \(G\) be a convex mapping of \(X\) into a normed space \(Z\). Suppose there exists an \(x_{1}\) such that \(G(x_{1})<\theta\) and that \(\mu_{0}=\inf\bigl{\{}f(x):\ G(x)\leqslant\theta,\ x\in\Omega\bigr{\}}\) is finite. Then_

\[\inf_{\begin{subarray}{c}(x)\leqslant\theta\\ x\in\Omega\end{subarray}}f(x)=\max_{z^{*}\geqslant\theta}\,\varphi(z^{*})\]

_and the maximum on the right is achieved by some \(z^{*}_{0}\geqslant\theta\)._

The dual function \(\varphi\) is defined as follows (Luenberger, 1969, pages 215, 223, and 224, which we quote). We denote by \(P\) a positive convex cone \(P\subset Z\), and let \(P^{*}=\bigl{\{}z^{*}\in Z^{*}:\ \forall\,z\in P,\ \langle z,\,z^{*}\rangle \geqslant 0\bigr{\}}\) be its corresponding positive cone in the dual space \(Z^{\star}\). The dual functional is defined, for each element \(z^{*}\in P^{\star}\), as

\[\varphi(z^{*})=\inf_{x\in\Omega}\bigl{\{}f(x)+\langle G(x),\,z^{*}\rangle \bigr{\}}\,.\]

Application.To prove the middle equality (\(\star\)), we apply Theorem 2 with the vector space \(X\) of all functions \(\mathcal{X}\to\mathbb{R}^{d}\), its convex subset \(\Omega\) formed by functions \(\mathcal{X}\to\mathcal{P}(\mathcal{A})\), and note that the function \(f\) is actually linear in our situation:

\[f(\pi)=-\mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}\sum_{a\in\mathcal{A}}r(\bm{X},a)\, \pi_{a}(\bm{X})\Bigg{]}\,.\]

(We take a \(-\) sign to match the form of the minimization problem over \(\pi\) in Theorem 2.) We take \(Z=\mathbb{R}^{d}=Z^{\star}\) with positive cones \(P=P^{\star}=[0,+\infty)^{d}\). The mapping \(G\) is a linear mapping from \(X\) to \(Z=\mathbb{R}^{d}\):

\[G(\pi)=\mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a )\,\pi_{a}(\bm{X})\Bigg{]}-\bm{B}\,,\]

and we take \(\theta=\bm{0}\). We have that \(\mu_{0}=\textsc{opt}(r,\bm{c},\bm{B})\) is indeed finite. We note that the condition "\((r,\bm{c},\bm{B}^{\prime},\nu)\) is feasible for some \(\bm{B}^{\prime}<\bm{B}\)" is required to apply the theorem. The result follows from noting that we exactly have, for \(z=\bm{\lambda}\in P^{\star}\),

\[\varphi(\bm{\lambda})=\inf_{\bm{\pi}:\mathcal{X}\to\mathcal{P}(\mathcal{A})}\, \,\mathbb{E}_{\bm{X}\sim\nu}\Bigg{[}-\sum_{a\in\mathcal{A}}r(\bm{X},a)\,\pi_{a} (\bm{X})+\sum_{a\in\mathcal{A}}\big{\langle}\bm{c}(\bm{X},a)-\bm{B},\,\bm{ \lambda}\big{\rangle}\pi_{a}(\bm{X})\Bigg{]}\,.\]Proofs of Lemmas 1 and 2

In both proofs, we will use the following deviation inequalities, holding on an event \(\mathcal{E}_{\text{H-Ax}}\cap\mathcal{E}_{\beta}\) of probability at least \(1-\delta/2\), where the event \(\mathcal{E}_{\beta}\) is defined in Assumption 2 with a confidence level \(1-\delta/4\) and the event \(\mathcal{E}_{\text{H-Ax}}\) is defined below: on \(\mathcal{E}_{\text{H-Ax}}\cap\mathcal{E}_{\beta}\),

\[\forall\,1\leqslant t\leqslant T, \sum_{\tau=1}^{t}\bm{c}_{\tau}\leqslant\big{(}\alpha_{t,\delta/4} +2\beta_{t,\delta/4}\big{)}\bm{1}+\sum_{\tau=1}^{t}\widehat{\bm{c}}_{\delta/4, \tau-1}^{\text{\,kb}}(\bm{x}_{\tau},a_{\tau})\] (7) \[\text{and}\qquad\sum_{\tau=1}^{t}r_{\tau}\geqslant-\big{(}\alpha _{t,\delta/4}+2\beta_{t,\delta/4}\big{)}+\sum_{\tau=1}^{t}\widehat{r}_{\delta/4,\tau-1}^{\text{\,wb}}(\bm{x}_{\tau},a_{\tau})\,,\] (8)

where \(\beta_{t,\delta/4}\) is also defined in Assumption 2 and

\[\alpha_{t,\delta/4}=\sqrt{2t\ln\frac{2(d+1)T}{\delta/4}}\,.\]

These inequalities come, on the one hand, by the Hoeffding-Azuma inequality applied \((d+1)T\) times on the range \([-1,1]\): it ensures that on an event \(\mathcal{E}_{\text{H-Ax}}\) with probability at least \(1-\delta/4\), for all \(1\leqslant t\leqslant T\),

\[\left\|\sum_{\tau=1}^{t}\bm{c}_{\tau}-\sum_{\tau=1}^{t}\bm{c}(\bm{x}_{\tau},a _{\tau})\right\|_{\infty}\leqslant\alpha_{t,\delta/4}\,\bm{1}\qquad\text{and} \qquad\left|\sum_{\tau=1}^{t}r_{\tau}-\sum_{\tau=1}^{t}r(\bm{x}_{\tau},a_{ \tau})\right|\leqslant\alpha_{t,\delta/4}\]

(where we denoted by \(\|\,\cdot\,\|_{\infty}\) the supremum norm). On the other hand, Assumption 2 and the clipping (3) entail, in particular, that on an event \(\mathcal{E}_{\beta}\) of probability at least \(1-\delta/4\), for all \(1\leqslant t\leqslant T\),

\[\sum_{\tau=1}^{t}\bm{c}(\bm{x}_{\tau},a_{\tau})\leqslant\sum_{ \tau=1}^{t}\big{(}\widehat{\bm{c}}_{\delta/4,\tau-1}^{\text{\,kb}}(\bm{x}_{ \tau},a_{\tau})+2\varepsilon_{\tau-1}(\bm{x}_{\tau},a_{\tau},\delta/4)\,\bm{1} \big{)} =2\beta_{t,\delta/4}\,\bm{1}+\sum_{\tau=1}^{t}\widehat{\bm{c}}_{ \delta/4,\tau-1}^{\text{\,kb}}(\bm{x}_{\tau},a_{\tau})\] \[\text{and (similarly)}\qquad\sum_{\tau=1}^{t}r(\bm{x}_{\tau},a_{ \tau}) \geqslant-2\beta_{t,\delta/4}+\sum_{\tau=1}^{t}\widehat{r}_{\delta/4, \tau-1}^{\text{\,wb}}(\bm{x}_{\tau},a_{\tau})\,.\]

### Proof of Lemma 1

For \(\tau\geqslant 1\), by definition of \(\bm{\lambda}_{\tau}\) in Box B,

\[\bm{\lambda}_{\tau}-\bm{\lambda}_{\tau-1} =\Big{(}\bm{\lambda}_{\tau-1}+\gamma\left(\widehat{\bm{c}}_{ \delta/4,\tau-1}^{\text{\,kb}}(\bm{x}_{\tau},a_{\tau})-(\bm{B}-b\bm{1})\right) \Big{)}_{+}-\bm{\lambda}_{\tau-1}\] \[\geqslant\gamma\left(\widehat{\bm{c}}_{\delta/4,\tau-1}^{\text{\, kb}}(\bm{x}_{\tau},a_{\tau})-(\bm{B}-b\bm{1})\right).\]

For \(t\geqslant 1\), as \(\bm{\lambda}_{0}=\bm{0}\) and \(\bm{\lambda}_{t}\geqslant\bm{0}\), we get, after telescoping and taking non-negative parts,

\[\bm{\lambda}_{t}\geqslant\gamma\left(\sum_{\tau=1}^{t}\big{(} \widehat{\bm{c}}_{\delta/4,\tau-1}^{\text{\,kb}}(\bm{x}_{\tau},a_{\tau})-(\bm{ B}-b\bm{1})\big{)}\right)_{+},\] \[\text{thus}\qquad\left\|\left(\sum_{\tau=1}^{t}\big{(}\widehat{ \bm{c}}_{\delta/4,\tau-1}^{\text{\,kb}}(\bm{x}_{\tau},a_{\tau})-(\bm{B}-b\bm{1 })\big{)}\right)_{+}\right\|\leqslant\frac{\|\bm{\lambda}_{t}\|}{\gamma}\,.\] (9)

Up to the deviation terms (7), \(\|\bm{\lambda}_{t}\|/\gamma\) bounds how larger the cost constraints till round \(t\) are from \(t(\bm{B}-b\bm{1})\). Most of the rest of the proof, namely, the Steps 1-4 below, thus focus on upper bounding the \(\|\bm{\lambda}_{t}\|\), while Step 5 will collect all bounds together and conclude.

Step 1: Gradient-descent analysis.We introduce

\[\Delta\widehat{\bm{c}}_{\tau}=\widehat{\bm{c}}_{\delta/4,\tau-1}^{\text{\,kb}}( \bm{x}_{\tau},a_{\tau})-(\bm{B}-b\bm{1})\] (10)

and prove the following deterministic inequality: for all \(1\leqslant t\leqslant T\),

\[\forall\,\bm{\lambda}\geqslant\bm{0},\qquad\|\bm{\lambda}_{t}-\bm{\lambda}\| ^{2}\leqslant\|\bm{\lambda}\|^{2}+4d\,\gamma^{2}t+2\gamma\sum_{\tau=1}^{t} \bigl{\langle}\Delta\widehat{\bm{c}}_{\tau},\,\bm{\lambda}_{\tau-1}-\bm{ \lambda}\bigr{\rangle}\,.\] (11)To do so, we proceed as is classical in (projected) gradient-descent analyses; see, e.g., Zinkevich (2003). Namely, for all \(1\leqslant\tau\leqslant T\),

\[2\big{\langle}-\Delta\widehat{\bm{c}}_{\tau},\,\bm{\lambda}_{\tau- 1}-\bm{\lambda}\big{\rangle} =\frac{1}{\gamma}\Big{(}\|\bm{\lambda}_{\tau-1}-\bm{\lambda}\|^{2} +\big{\|}\,\gamma\Delta\widehat{\bm{c}}_{\tau}\,\big{\|}^{2}-\big{\|}\,\bm{ \lambda}_{\tau-1}-\bm{\lambda}+\gamma\Delta\widehat{\bm{c}}_{\tau}\,\big{\|}^{ 2}\Big{)}\] \[=\gamma\big{\|}\,\Delta\widehat{\bm{c}}_{\tau}\,\big{\|}^{2}+ \frac{1}{\gamma}\Big{(}\|\bm{\lambda}_{\tau-1}-\bm{\lambda}\|^{2}-\big{\|}\, \bm{\lambda}_{\tau-1}+\gamma\Delta\widehat{\bm{c}}_{\tau}-\bm{\lambda}\big{\|} ^{2}\Big{)}\] \[\leqslant\gamma\big{\|}\,\Delta\widehat{\bm{c}}_{\tau}\,\big{\|}^ {2}+\frac{1}{\gamma}\Big{(}\|\bm{\lambda}_{\tau-1}-\bm{\lambda}\|^{2}-\|\bm{ \lambda}_{\tau}-\bm{\lambda}\|^{2}\Big{)}\,,\]

where the inequality comes from the definition \(\bm{\lambda}_{\tau}=\big{(}\bm{\lambda}_{\tau-1}+\gamma\Delta\widehat{\bm{c}}_ {\tau}\big{)}_{+}\) and the fact that

\[\forall\,x\in\mathbb{R},\ \ \forall\,y\geqslant 0,\qquad\big{|}(x)_{+}-y \big{|}\leqslant|x-y|\]

(which may be proved by distinguishing the cases \(x\leqslant 0\) and \(x\geqslant 0\)).

We note that

\[\bm{B}-b\bm{1}\in[0,1]^{d}\quad\text{and}\quad\widehat{\bm{c}}_{\delta/4,\tau- 1}^{\,\text{kb}}(\bm{x}_{\tau},a_{\tau})\in[-1,1]^{d}\,,\qquad\text{so that}\qquad\big{\|}\,\Delta\widehat{\bm{c}}_{\tau}\,\big{\|}^{2} \leqslant 4d\,.\]

Collecting all bounds above, we get, after summation and telescoping,

\[\sum_{\tau=1}^{t}\big{\langle}-\Delta\widehat{\bm{c}}_{\tau},\,\bm{\lambda}_{ \tau-1}-\bm{\lambda}\big{\rangle}\leqslant 2d\,\gamma t+\frac{1}{2\gamma} \Big{(}\|\bm{\lambda}_{0}-\bm{\lambda}\|^{2}-\|\bm{\lambda}_{t}-\bm{\lambda} \|^{2}\Big{)}\,.\]

Rearranging and substituting \(\bm{\lambda}_{0}=\bm{0}\) yields the claimed inequality (11).

Step 2: Relating estimated costs (and rewards) to true conditional expectations.In this part, we upper bound the right-hand side of (11) by showing that on the event \(\mathcal{E}_{\beta}\),

\[\forall\,\bm{\lambda}\geqslant\bm{0},\quad\forall\,1\leqslant t \leqslant T,\] \[\sum_{\tau=1}^{t}\big{\langle}\Delta\widehat{\bm{c}}_{\tau},\,\bm{ \lambda}_{\tau-1}-\bm{\lambda}\big{\rangle} \leqslant 2\big{(}1+\|\bm{\lambda}\|_{1}\big{)}\beta_{t,\delta/4}+ \sum_{\tau=1}^{t}\big{(}g_{\tau}(\bm{\lambda})-g_{\tau}(\bm{\lambda}_{\tau-1} )\big{)}\,,\] (12) \[\text{where}\qquad g_{\tau}(\bm{\lambda})=\max_{a\in\mathcal{A}} \Bigl{\{}r(\bm{x}_{\tau},a)-\big{\langle}\bm{c}(\bm{x}_{\tau},a)-(\bm{B}-b \bm{1}),\,\bm{\lambda}\big{\rangle}\Bigr{\}}\]

and where we recall that \(\|\cdot\|_{1}\) denotes the \(\ell_{1}\)-norm.

Adding and subtracting \(\widehat{r}_{\delta/4,\tau-1}^{\,\text{sb}}(\bm{x}_{\tau},a_{\tau})\), here, we deal with

\[\sum_{\tau=1}^{t}\bigl{\langle}\Delta\widehat{\bm{c}}_{\tau},\, \bm{\lambda}_{\tau-1}-\bm{\lambda}\bigr{\rangle} =\sum_{\tau=1}^{t}\Bigl{(}\widehat{r}_{\delta/4,\tau-1}^{\,\text{sb }}(\bm{x}_{\tau},a_{\tau})-\big{\langle}\widehat{\bm{c}}_{\delta/4,\tau-1}^{ \,\text{kb}}(\bm{x}_{\tau},a_{\tau})-(\bm{B}-b\bm{1}),\,\bm{\lambda}\big{\rangle} \Bigr{)}\] \[\quad-\sum_{\tau=1}^{t}\Bigl{(}\widehat{r}_{\delta/4,\tau-1}^{\, \text{sb}}(\bm{x}_{\tau},a_{\tau})-\big{\langle}\widehat{\bm{c}}_{\delta/4, \tau-1}^{\,\text{kb}}(\bm{x}_{\tau},a_{\tau})-(\bm{B}-b\bm{1}),\,\bm{\lambda}_ {\tau-1}\big{\rangle}\Bigr{)}\,.\]

Now, for each \(\tau\), by (3) and the fact that \(\bm{\lambda}\geqslant\bm{0}\),

\[\widehat{r}_{\delta/4,\tau-1}^{\,\text{sb}}(\bm{x}_{\tau},a_{ \tau})-\big{\langle}\widehat{\bm{c}}_{\delta/4,\tau-1}^{\,\text{kb}}(\bm{x}_{ \tau},a_{\tau})-(\bm{B}-b\bm{1}),\,\bm{\lambda}\big{\rangle}\] \[\leqslant r(\bm{x}_{\tau},a_{\tau})+2\varepsilon_{\tau-1}(\bm{x}_ {\tau},a_{\tau},\delta/4)-\big{\langle}\bm{c}(\bm{x}_{\tau},a_{\tau})-(\bm{B}-b \bm{1}),\,\bm{\lambda}\big{\rangle}+2\varepsilon_{\tau-1}(\bm{x}_{\tau},a_{ \tau},\delta/4)\,\|\bm{\lambda}\|_{1}\] \[\leqslant g_{\tau}(\bm{\lambda})+2\big{(}1+\|\bm{\lambda}\|_{1} \big{)}\varepsilon_{\tau-1}(\bm{x}_{\tau},a_{\tau},\delta/4)\,.\]

On the other hand, by definition of \(a_{\tau}\) for the equality, and then by the other inequalities in (3), for each \(\tau\), and the fact that \(\bm{\lambda}_{\tau-1}\geqslant\bm{0}\),

\[\widehat{r}_{\delta/4,\tau-1}^{\,\text{sb}}(\bm{x}_{\tau},a_{ \tau})-\big{\langle}\widehat{\bm{c}}_{\delta/4,\tau-1}^{\,\text{kb}}(\bm{x}_{ \tau},a_{\tau})-(\bm{B}-b\bm{1}),\,\bm{\lambda}_{\tau-1}\big{\rangle}\] (13) \[=\max_{a\in\mathcal{A}}\Bigl{\{}\widehat{r}_{\delta/4,\tau-1}^{\, \text{sb}}(\bm{x}_{\tau},a)-\big{\langle}\widehat{\bm{c}}_{\delta/4,\tau-1}^{\, \text{kb}}(\bm{x}_{\tau},a)-(\bm{B}-b\bm{1}),\,\bm{\lambda}_{\tau-1}\big{\rangle} \Bigr{\}}\] \[\geqslant\max_{a\in\mathcal{A}}\Bigl{\{}r(\bm{x}_{\tau},a)-\big{\langle} \bm{c}(\bm{x}_{\tau},a)-(\bm{B}-b\bm{1}),\,\bm{\lambda}_{\tau-1}\big{\rangle} \Bigr{\}}=g_{\tau}(\bm{\lambda}_{\tau-1})\,.\]

Collecting the two series of bounds concludes this part.

Step 3: Application of a Bernstein-Freedman inequality.We recall that we denoted by \(\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star}\) the optimal dual variable in (4) for \(\textsc{opt}(r,\boldsymbol{c},\boldsymbol{B}-b\boldsymbol{1})\); it exists because we assumed feasibility of a problem with average cost constraints \(\boldsymbol{B}^{\prime}<\boldsymbol{B}-b\boldsymbol{1}\).

We now upper bound the sum appearing in the right hand side of (12) at \(\boldsymbol{\lambda}=\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star}\) by showing that on an event \(\mathcal{E}_{\textsc{Bern-c}}\) of probability at least \(1-\delta/4\), for all \(1\leqslant t\leqslant T\),

\[\sum_{\tau=1}^{t}\bigl{(}g_{\tau}(\boldsymbol{\lambda}_{ \boldsymbol{B}-b\boldsymbol{1}}^{\star})-g_{\tau}(\boldsymbol{\lambda}_{\tau -1})\bigr{)}\leqslant(1+2\Lambda_{t})\sqrt{2t\ln\frac{T^{2}}{\delta/4}}+2K_{t} \ln\frac{T^{2}}{\delta/4}\,,\] (14) \[\text{where}\qquad\Lambda_{t}=\max_{1\leqslant\tau\leqslant t}\| \boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star}-\boldsymbol{ \lambda}_{\tau-1}\|_{1}\qquad\text{and}\qquad K_{t}=4\bigl{(}1+\|\boldsymbol {\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star}\|_{1}+2d\gamma t\bigr{)}\,.\]

We will do so by applying a version of the Bernstein-Freedman inequality for martingales stated in Cesa-Bianchi et al. (2005, Corollary 16) involving the sum of the conditional variances (and not only a deterministic bound thereon); it is obtained via peeling based on the classic version of Bernstein's inequality (Freedman, 1975). We restate it here for the convenience of the reader (after applying some simple boundings).

**Lemma 4** (a version of the Bernstein-Freedman inequality by Cesa-Bianchi et al., 2005).: _Let \(X_{1},\,X_{2},\,\ldots\) be a martingale difference with respect to the filtration \(\mathcal{F}=(\mathcal{F}_{s})_{s\geqslant 0}\) and with increments bounded in absolute values by \(K\). For all \(t\geqslant 1\), let_

\[\mathfrak{S}_{t}=\sum_{\tau=1}^{t}\mathbb{E}\bigl{[}X_{\tau}^{2}\,\big{|}\, \mathcal{F}_{\tau-1}\bigr{]}\]

_denote the sum of the conditional variances of the first \(t\) increments. Then, for all \(\delta\in(0,1)\) and all \(t\geqslant 1\), with probability at least \(1-\delta\),_

\[\sum_{\tau=1}^{t}X_{\tau}\leqslant\sqrt{2\mathfrak{S}_{t}\,\ln\frac{t}{\delta }}+2K\ln\frac{t}{\delta}\,.\]

We introduce, for all \(\boldsymbol{\lambda}\geqslant\boldsymbol{0}\), the common expectation of the \(g_{\tau}(\boldsymbol{\lambda})\), namely,

\[G(\boldsymbol{\lambda})=\mathbb{E}\bigl{[}g_{\tau}(\boldsymbol{\lambda})\bigr{]} =\mathbb{E}_{\boldsymbol{X}\sim\nu}\biggl{[}\max_{a\in\mathcal{A}}\Bigl{\{}r( \boldsymbol{X},a)-\bigl{\langle}\boldsymbol{c}(\boldsymbol{X},a)-( \boldsymbol{B}-b\boldsymbol{1}),\,\boldsymbol{\lambda}\bigr{\rangle}\Bigr{\}} \biggr{]}\,,\]

and consider the martingale increments

\[X_{\tau}=\bigl{(}g_{\tau}(\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1 }}^{\star})-g_{\tau}(\boldsymbol{\lambda}_{\tau-1})\bigr{)}-\bigl{(}G( \boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star})-G(\boldsymbol {\lambda}_{\tau-1})\bigr{)}\,.\]

As \(\boldsymbol{B}-b\boldsymbol{1}\in[0,1]^{d}\) and \(\boldsymbol{c}\) takes values in \([-1,1]^{d}\), for all \(\boldsymbol{x}\in\mathcal{X}\), all \(a\in\mathcal{A}\), and all \(\boldsymbol{v}\in\mathbb{R}^{d}\), the quantities \(r(\boldsymbol{x},a)-\bigl{\langle}\boldsymbol{c}(\boldsymbol{x},a)-(\boldsymbol {B}-b\boldsymbol{1}),\,\boldsymbol{v}\bigr{\rangle}\) take absolute values smaller than \(1+2\|\boldsymbol{v}\|_{1}\). Using that a difference of maxima is smaller than the maximum of the differences, we get, in particular,

\[\bigl{|}g_{\tau}(\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{ \star})-g_{\tau}(\boldsymbol{\lambda}_{\tau-1})\bigr{|} \leqslant 1+2\|\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{ \star}-\boldsymbol{\lambda}_{\tau-1}\|_{1}\quad\text{a.s.}\] (15) \[\text{and}\qquad\bigl{|}G(\boldsymbol{\lambda}_{\boldsymbol{B}-b \boldsymbol{1}}^{\star})-G(\boldsymbol{\lambda}_{\tau-1})\bigr{|} \leqslant 1+2\|\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{ \star}-\boldsymbol{\lambda}_{\tau-1}\|_{1}\,.\]

Now, given the update step in Step 3 of Box B, we have the deterministic bound \(\|\boldsymbol{\lambda}_{t}\|_{1}\leqslant 2d\gamma t\) for all \(1\leqslant t\leqslant T\). Therefore, by a triangle inequality, the martingale increments are bounded in absolute values by \(K_{t}\), as

\[2\Bigl{(}1+2\|\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star}\|_ {1}+1+2\max_{\tau\leqslant t}\|\boldsymbol{\lambda}_{\tau-1}\|_{1}\Bigr{)} \leqslant 4\bigl{(}1+\|\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{ \star}\|_{1}+2d\gamma t\bigr{)}=K_{t}\,.\]

The conditional variance of \(X_{\tau}\) is smaller than the squared half-width of the conditional range (Popoviciu's inequality on variances); in particular, (15) thus entails

\[\mathfrak{S}_{t}\leqslant\sum_{\tau=1}^{t}\bigl{(}1+2\|\boldsymbol{\lambda}_{ \boldsymbol{B}-b\boldsymbol{1}}^{\star}-\boldsymbol{\lambda}_{\tau-1}\|_{1} \bigr{)}^{2}\leqslant(1+2\Lambda_{t})^{2}t\,.\]

We now get the claimed inequalities (14) first by noting that by (4), for all \(\tau\leqslant T\),

\[G(\boldsymbol{\lambda}_{\boldsymbol{B}-b\boldsymbol{1}}^{\star})-G(\boldsymbol {\lambda}_{\tau-1})\leqslant 0\,,\]

and second, by applying Lemma 4 for each \(1\leqslant t\leqslant T\), using a confidence level \(\delta/(4T)\). By a union bound, this indeed defines an event \(\mathcal{E}_{\textsc{Bern-c}}\) of probability at least \(1-\delta/4\).

Step 4: Induction to bound the \(\Lambda_{t}\).In this step, we show by induction that, with high probability, the norms \(\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{t}\|\) satisfy the bound (18) stated below.

To do so, we combine the outcomes of Steps 1-3 and obtain that on the event \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{berm-c}}\), for all \(1\leqslant t\leqslant T\),

\[\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{t}\|^{2}\] \[\leqslant\|\bm{\lambda}_{B-b1}^{\star}\|^{2}+4d\,\gamma^{2}t+2 \gamma\left(2\big{(}1+\|\bm{\lambda}_{B-b1}^{\star}\|_{1}\big{)}\beta_{t,\delta/ 4}+\sum_{\tau=1}^{t}\big{(}g_{\tau}(\bm{\lambda}_{B-b1}^{\star})-g_{\tau}(\bm{ \lambda}_{\tau-1})\big{)}\right)\] \[\leqslant\|\bm{\lambda}_{B-b1}^{\star}\|^{2}+4d\,\gamma^{2}t+4 \gamma\big{(}1+\|\bm{\lambda}_{B-b1}^{\star}\|_{1}\big{)}\beta_{t,\delta/4}+2 \gamma(1+2\Lambda_{t})\sqrt{2t\ln\frac{T^{2}}{\delta/4}}+4\gamma K_{t}\ln \frac{T^{2}}{\delta/4}\,,\]

where we recall that norms not indexed by a subscript are Euclidean norms, and

\[\Lambda_{t}=\max_{1\leqslant\tau\leqslant t}\|\bm{\lambda}_{B-b1}^{\star}- \bm{\lambda}_{\tau-1}\|_{1}\qquad\text{and}\qquad K_{t}=4\big{(}1+\|\bm{ \lambda}_{B-b1}^{\star}\|_{1}+2d\gamma t\big{)}\,.\]

We upper bound \(\|\bm{\lambda}_{B-b1}^{\star}\|_{1}\) and \(\Lambda_{t}\) in terms of Euclidean norms,

\[\|\bm{\lambda}_{B-b1}^{\star}\|_{1}\leqslant\sqrt{d}\,\|\bm{\lambda}_{B-b1}^ {\star}\|\qquad\text{and}\qquad\Lambda_{t}\leqslant\sqrt{d}\,\max_{1\leqslant \tau\leqslant t}\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{\tau-1}\|\,,\]

perform some crude boundings like \(\sqrt{t}\leqslant\sqrt{T}\) and \(\beta_{t,\delta/4}\leqslant\beta_{T,\delta/4}\), and obtain the following induction relationship: on the event \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{berm-c}}\), for all \(1\leqslant t\leqslant T\),

\[\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{t}\|^{2}\leqslant A+B\,t+C\,\max_ {1\leqslant\tau\leqslant t}\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{\tau-1 }\|\,,\] (16)

where

\[A =\|\bm{\lambda}_{B-b1}^{\star}\|^{2}+\gamma\left(4\big{(}1+\sqrt{ d}\,\|\bm{\lambda}_{B-b1}^{\star}\|\big{)}\beta_{T,\delta/4}+2\sqrt{2T\ln\frac{T^{2}} {\delta/4}}+16\big{(}1+\sqrt{d}\,\|\bm{\lambda}_{B-b1}^{\star}\|\big{)}\ln \frac{T^{2}}{\delta/4}\right),\] \[B =4d\gamma^{2}+4\times 4\times 2d\gamma^{2}\text{ln}\,\frac{T^{2}} {\delta/4}=\left(4+32\ln\frac{T^{2}}{\delta/4}\right)d\gamma^{2}\leqslant 36\,d \gamma^{2}\ln\frac{T^{2}}{\delta/4}\,,\] \[C =4\gamma\sqrt{2dT\ln\frac{T^{2}}{\delta/4}}\,.\]

We now show that (16) implies that on \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{berm-c}}\),

\[\forall\,0\leqslant t\leqslant T,\qquad\|\bm{\lambda}_{B-b1}^{\star}-\bm{ \lambda}_{t}\|\leqslant M\overset{\text{def}}{=}\frac{C}{2}+\sqrt{A+BT+\frac{C ^{2}}{4}}\,.\] (17)

Indeed, for \(t=0\), given that \(\bm{\lambda}_{0}=\bm{0}\), we have \(\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{t}\|=\|\bm{\lambda}_{B-b1}^{\star} \|\leqslant\sqrt{A}\). Now, if the bound (17) is satisfied for all \(0\leqslant\tau\leqslant t\), where \(0\leqslant t\leqslant T-1\), then (16) implies that

\[\|\bm{\lambda}_{B-b1}^{\star}-\bm{\lambda}_{t+1}\|\leqslant A+B\,(t+1)+CM \leqslant A+B\,T+CM\leqslant M^{2}\,,\]

where the final inequality follows from the fact that (by definition of \(M\), and this explains how we picked \(M\))

\[M^{2}-CM=\left(M-\frac{C}{2}\right)^{\!\!2}+\frac{C^{2}}{4}=A+BT\,.\]

Below, we will make repeated uses of \(\sqrt{x+y}\leqslant\sqrt{x}+\sqrt{y}\), of \(xy\leqslant 2(x^{2}+y^{2})\), and of \(\sqrt{x}\leqslant 1+x\), for \(x,y\geqslant 0\). From (17), we conclude that on the event \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{berm-c}}\),

\[\forall\,0\leqslant t\leqslant T,\qquad\|\bm{\lambda}_{B-b1}^{ \star}-\bm{\lambda}_{t}\| \leqslant\sqrt{A}+\sqrt{BT}+C\] \[=\sqrt{A}+6\gamma\sqrt{dT\ln\frac{T^{2}}{\delta/4}}+4\gamma\sqrt{2dT \ln\frac{T^{2}}{\delta/4}}\leqslant\sqrt{A}+6\gamma\beta_{T,\delta/4}^{\prime}\,,\]

where we denoted

\[\beta_{T,\delta/4}^{\prime}=\max\left\{\beta_{T,\delta/4},\ 2\sqrt{dT\ln\frac{T^{2}}{ \delta/4}}\right\}\geqslant\ln\frac{T^{2}}{\delta/4}\,.\]For the sake of readability, we may further bound \(\sqrt{A}\) as follows (in some crude way):

\[\sqrt{A} \leqslant\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|+\sqrt{\gamma} \sqrt{4\big{(}1+\sqrt{d}\,\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|\big{)}\beta_ {T,\delta/4}+2\sqrt{2T\ln\frac{T^{2}}{\delta/4}}+16\big{(}1+\sqrt{d}\,\|\bm{ \lambda}_{\bm{B}-b\bm{1}}^{\star}\|\big{)}\ln\frac{T^{2}}{\delta/4}\] \[\leqslant\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|+\sqrt{\gamma} \sqrt{22\beta_{T,\delta/4}^{\prime}+20\sqrt{d}\,\|\bm{\lambda}_{\bm{B}-b\bm{1} }^{\star}\|\beta_{T,\delta/4}^{\prime}}\] \[\leqslant\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|+1+6\gamma \beta_{T,\delta/4}^{\prime}+\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|+5\gamma \sqrt{d}\,\beta_{T,\delta/4}^{\prime}\,,\]

where we used the facts that \(\sqrt{20xy}=2\sqrt{5xy}\leqslant x+5y\) and \(\sqrt{22x}=2\sqrt{22x/4}\leqslant 1+6x\).

All in all, we proved that on the event \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{\tiny Bern-e}}\),

\[\forall\,0\leqslant t\leqslant T,\qquad\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star }-\bm{\lambda}_{t}\|\leqslant 2\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|+17 \gamma\sqrt{d}\,\beta_{T,\delta/4}^{\prime}+1\,.\] (18)

Step 5: Conclusion.We combine the bound (18) with the bound (9) of Step 1 and the bound (7): on the intersection of events \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{\tiny Bern-e}}\cap\mathcal{E}_{\text {\tiny H-Ar}}\), which has a probability at least \(1-3\delta/4\), for all \(1\leqslant t\leqslant T\),

\[\left\|\left(\sum_{\tau=1}^{t}\bm{c}_{\tau}-t(\bm{B}-b\bm{1}) \right)_{+}\right\| \leqslant\sqrt{d}\big{(}\alpha_{T,\delta/4}+2\beta_{T,\delta/4} \big{)}+\frac{\|\bm{\lambda}_{t}\|}{\gamma}\] \[\leqslant\sqrt{d}\big{(}\alpha_{T,\delta/4}+2\beta_{T,\delta/4} \big{)}+\frac{\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|}{\gamma}+\frac{\|\bm{ \lambda}_{\bm{B}-b\bm{1}}^{\star}-\bm{\lambda}_{t}\|}{\gamma}\] \[\leqslant\frac{3\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|+1}{ \gamma}+\sqrt{d}\big{(}\alpha_{T,\delta/4}+19\beta_{T,\delta/4}^{\prime}\big{)}\,.\]

This entails in particular the stated result, given the definition of \(\Upsilon_{T,\delta}\) as \(\max\bigl{\{}\beta_{T,\delta/4}^{\prime},\,\alpha_{T,\delta/4}\bigr{\}}\).

### Proof of Lemma 2

The proof is similar to (but much simpler and shorter than) the one of Lemma 1 and borrows some of its arguments. We use throughout this section the notation introduced therein; we also define a new event \(\mathcal{E}_{\text{\tiny Bern-\tau}}\) of probability at least \(1-\delta/4\).

We start from (8) and introduce the same \(\Delta\widehat{\bm{c}}_{t}\) quantity as in (10): on the event \(\mathcal{E}_{\text{\tiny H-Ar}}\cap\mathcal{E}_{\beta}\), for all \(1\leqslant t\leqslant T\),

\[\sum_{\tau=1}^{t}r_{\tau} \geqslant-\big{(}\alpha_{t,\delta/4}+2\beta_{t,\delta/4}\big{)}+ \sum_{\tau=1}^{t}\widehat{r}_{\delta/4,\tau-1}^{\text{\tiny sub}}(\bm{x}_{ \tau},a_{\tau})\] \[\geqslant-\big{(}\alpha_{t,\delta/4}+2\beta_{t,\delta/4}\big{)}+ \sum_{\tau=1}^{t}\Bigl{(}\widehat{r}_{\delta/4,\tau-1}^{\text{\tiny sub}}(\bm{x} _{\tau},a_{\tau})-\bigl{\langle}\Delta\widehat{\bm{c}}_{\tau},\,\bm{\lambda}_{ \tau-1}\bigr{\rangle}\Bigr{)}+\sum_{\tau=1}^{t}\bigl{\langle}\Delta\widehat{\bm {c}}_{\tau},\,\bm{\lambda}_{\tau-1}\bigr{\rangle}\,.\]

On the one hand, the result (11) with \(\bm{\lambda}=\bm{0}\) exactly states that

\[\sum_{\tau=1}^{t}\bigl{\langle}\Delta\widehat{\bm{c}}_{\tau},\,\bm{\lambda}_{ \tau-1}\bigr{\rangle}\geqslant\frac{\|\bm{\lambda}_{t}\|^{2}}{2\gamma}-2d\, \gamma t\geqslant-2d\,\gamma t\,.\]

On the other hand, the result (13) states that on \(\mathcal{E}_{\beta}\), for all \(1\leqslant\tau\leqslant T\),

\[\widehat{r}_{\delta/4,\tau-1}^{\text{\tiny sub}}(\bm{x}_{\tau},a_{ \tau})-\bigl{\langle}\Delta\widehat{\bm{c}}_{\tau},\,\bm{\lambda}_{\tau-1} \bigr{\rangle} =\widehat{r}_{\delta/4,\tau-1}^{\text{\tiny sub}}(\bm{x}_{\tau},a_{ \tau})-\bigl{\langle}\widehat{\bm{c}}_{\delta/4,\tau-1}^{\text{\tiny sub}}(\bm{x }_{\tau},a_{\tau})-(\bm{B}-b\bm{1}),\,\bm{\lambda}_{\tau-1}\bigr{\rangle}\] \[\geqslant g_{\tau}(\bm{\lambda}_{\tau-1})\,.\]

A similar application of Lemma 4 as in the proof of Lemma 1 shows that on a new event \(\mathcal{E}_{\text{\tiny Bern-\tau}}\) of probability at least \(1-\delta/4\), for all \(1\leqslant t\leqslant T\),

\[\sum_{\tau=1}^{t}g_{\tau}(\bm{\lambda}_{\tau-1})\geqslant\sum_{\tau=1}^{t}G( \bm{\lambda}_{\tau-1})-\sqrt{2\sum_{\tau=1}^{t}\bigl{(}1+2\|\bm{\lambda}_{\tau- 1}\|_{1}\bigr{)}^{2}\!\ln\frac{T^{2}}{\delta/4}}-8(1+2\gamma t)\ln\frac{T^{2}} {\delta/4}\,.\]We relate \(\ell^{1}\)-norms to Euclidean norms, resort to a triangle inequality, and substitute (18) to get that on \(\mathcal{E}_{\text{Bem-c}}\), for all \(1\leqslant t\leqslant T\),

\[-\sqrt{2\sum_{\tau=1}^{t}\bigl{(}1+2\|\bm{\lambda}_{\tau-1}\|_{1} \bigr{)}^{2}\!\ln\frac{T^{2}}{\delta/4}}\geqslant-\biggl{(}1+2\sqrt{d}\max_{0 \leqslant\tau\leqslant t-1}\|\bm{\lambda}_{\tau-1}\|\biggr{)}\sqrt{2t\ln\frac{ T^{2}}{\delta/4}}\\ \geqslant-\biggl{(}1+2\sqrt{d}\,\|\bm{\lambda}_{\bm{B}-b1}^{\star }\|+2\sqrt{d}\max_{0\leqslant\tau\leqslant t-1}\|\bm{\lambda}_{\bm{B}-b1}^{ \star}-\bm{\lambda}_{\tau-1}\|\biggr{)}\sqrt{2t\ln\frac{T^{2}}{\delta/4}}\\ \geqslant-\biggl{(}8\sqrt{d}\,\|\bm{\lambda}_{\bm{B}-b1}^{\star }\|+34\gamma d\,\beta_{T,\delta/4}^{\prime}+2\sqrt{d}+1\biggr{)}\sqrt{2t\ln \frac{T^{2}}{\delta/4}}\\ \geqslant-\biggl{(}8\sqrt{d}\,\|\bm{\lambda}_{\bm{B}-b1}^{\star }\|+34\gamma d\,\beta_{T,\delta/4}^{\prime}+4\sqrt{d}\biggr{)}\sqrt{2t\ln\frac {T^{2}}{\delta/4}}\\ \geqslant-6\|\bm{\lambda}_{\bm{B}-b1}^{\star}\|\,\beta_{T,\delta/4 }^{\prime}-25\gamma\sqrt{d}\,\bigl{(}\beta_{T,\delta/4}^{\prime}\bigr{)}^{2}-2 \sqrt{2}\,\beta_{T,\delta/4}^{\prime}\\ \geqslant-6\|\bm{\lambda}_{\bm{B}-b1}^{\star}\|\,\beta_{T,\delta/4 }^{\prime}-28\gamma\sqrt{d}\,\bigl{(}\beta_{T,\delta/4}^{\prime}\bigr{)}^{2}\,,\]

where we performed some crude boundings using the definition of \(1\leqslant\beta_{t,\delta/4}^{\prime}\leqslant\beta_{T,\delta/4}^{\prime}\). We also note that

\[8(1+2\gamma t)\ln\frac{T^{2}}{\delta/4}\leqslant 8\ln\frac{T^{2}}{\delta/4}+4 \gamma\bigl{(}\beta_{T,\delta/4}^{\prime}\bigr{)}^{2}\,.\]

By (4), we have \(\text{\sc opt}(r,\bm{c},\bm{B}-b\bm{1})=G(\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star} )\leqslant G(\bm{\lambda})\), for all \(\bm{\lambda}\geqslant\bm{0}\). Also,

\[G(\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star})+b\,\|\bm{\lambda}_{\bm{ B}-b\bm{1}}^{\star}\|_{1}\] \[=\mathbb{E}_{\bm{X}\sim\nu}\biggl{[}\max_{a\in\mathcal{A}}\Bigl{\{} r(\bm{X},a)-\bigl{\langle}\bm{c}(\bm{X},a)-(\bm{B}-b\bm{1}),\,\bm{\lambda}_{ \bm{B}-b\bm{1}}^{\star}\bigr{\rangle}\Bigr{\}}\biggr{]}+b\,\|\bm{\lambda}_{\bm {B}-b\bm{1}}^{\star}\|_{1}\] \[=\mathbb{E}_{\bm{X}\sim\nu}\biggl{[}\max_{a\in\mathcal{A}}\Bigl{\{} r(\bm{X},a)-\bigl{\langle}\bm{c}(\bm{X},a)-\bm{B},\,\bm{\lambda}_{\bm{B}-b \bm{1}}^{\star}\bigr{\rangle}\Bigr{\}}\biggr{]}\geqslant\text{\sc opt}(r,\bm{ c},\bm{B})\,,\]

where we used again (4). In particular,

\[\sum_{\tau=1}^{t}G(\bm{\lambda}_{\tau-1})\geqslant t\,\text{\sc opt}(r,\bm{c},\bm{B})-t\,b\,\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|_{1}\geqslant t\, \text{\sc opt}(r,\bm{c},\bm{B})-t\,b\sqrt{d}\,\|\bm{\lambda}_{\bm{B}-b\bm{1}}^ {\star}\|_{1}\,.\]

Collecting all bounds above and using the definition of \(\Upsilon_{T,\delta}\) as \(\max\bigl{\{}\beta_{T,\delta/4}^{\prime},\,\alpha_{T,\delta/4}\bigr{\}}\) and the fact that \(dt\leqslant(\Upsilon_{T,\delta})^{2}\), we proved the following. On \(\mathcal{E}_{\text{\sc H-\sc H}}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\text {\sc Hem-c}}\cap\mathcal{E}_{\text{\sc Hem-r}}\), which is indeed an event with probability at least \(1-\delta\), for all \(1\leqslant t\leqslant T\),

\[\sum_{\tau=1}^{t}r_{\tau}\] \[\geqslant-3\Upsilon_{T,\delta}-2d\,\gamma t+\sum_{\tau=1}^{t}G( \bm{\lambda}_{\tau-1})-\sqrt{2\sum_{t=1}^{T}\bigl{(}1+2\|\bm{\lambda}_{t-1}\|_ {1}\bigr{)}^{2}\!\ln\frac{T^{2}}{\delta/4}}-8\ln\frac{T^{2}}{\delta/4}-4\gamma \bigl{(}\beta_{T,\delta/4}^{\prime}\bigr{)}^{2}\] \[\geqslant t\,\text{\sc opt}(r,\bm{c},\bm{B})-\|\bm{\lambda}_{\bm {B}-b\bm{1}}^{\star}\|\Bigl{(}t\,b\sqrt{d}+6\Upsilon_{T,\delta}\Bigr{)}-36 \gamma\sqrt{d}\,\bigl{(}\Upsilon_{T,\delta}\bigr{)}^{2}-8\ln\frac{T^{2}}{ \delta/4}\,.\]

This entails in particular the stated result.

## Appendix C Proof of Theorem 1

The proof is divided into three steps: on a favorable event \(\mathcal{E}_{\text{meta}}\) of probability at least \(1-\delta\),

(i) we bound by \(\operatorname{ilog}T\) the index of the last regime achieved in Box C;

[MISSING_PAGE_EMPTY:20]

Therefore, the bound of Lemma 1 entails that on \(\mathcal{E}_{\text{meta}}\), for all \(t\geqslant T_{K}\),

\[\left\|\left(\sum_{\tau=T_{K}}^{t}\bm{c}_{\tau}-\left(t-T_{k}+1 \right)\left(\bm{B}-b_{T}\bm{1}\right)\right)_{+}\right\| \leqslant\frac{1+3\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|}{ \gamma_{K}}+20\sqrt{d}\,\Upsilon_{T,\delta/(K+2)^{2}}\] \[\leqslant 4\sqrt{T}+20\sqrt{d}\,\Upsilon_{T,\delta/(K+2)^{2}}=M_{T,\delta,K}\,.\]

This is exactly the contrary of the stopping condition of regime \(K\): the latter thus cannot be broken. In some bounds, we will further bound \(\operatorname{ilog}\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\) by \(\operatorname{ilog}T\): this holds because the assumption of \((\bm{B}-2b_{T}\bm{1})\)-feasibility entails, by Lemma 3 and as opt is always smaller than \(1\), the crude bound

\[\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\leqslant\frac{\textsc{opt}(r, \bm{c},\bm{B}-b_{T}\bm{1})-\textsc{opt}\big{(}r,\bm{c},\bm{B}-(3/2)b_{T}\bm{1 }\big{)}}{b_{T}/2}\leqslant\frac{2}{b_{T}}\leqslant\frac{\sqrt{T}}{7} \leqslant T\,,\]

where we used that \(b_{T}\geqslant 14/\sqrt{T}\) given its definition. (Of course, sharper but more complex bounds could be obtained; however, they would only improve logarithmic terms in the bound, which we do not try to optimize anyway.)

### Step 2: Bounding the cumulative costs

We still denote by \(K\) the index of the last regime and recall that \(K\leqslant\operatorname{ilog}\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\| \leqslant\operatorname{ilog}(T)\), and that for \(k\geqslant 0\), regime \(k\) starts at \(T_{k}\) and stops at \(T_{k+1}-1\). By convention, \(T_{0}=1\) and \(T_{K+1}=T+1\).

By the very definition of the stopping condition of regime \(k\geqslant 0\),

\[\left\|\left(\sum_{t=T_{k}}^{T_{k+1}-2}\bm{c}_{t}-\left(T_{k+1}-T_{k}-1\right) \left(\bm{B}-b_{T}\bm{1}\right)\right)_{+}\right\|\leqslant M_{T,\delta,k}\,.\]

For rounds of the form \(t=T_{k+1}-1\), we bound the Euclidean norm of \(\bm{c}_{t}-(\bm{B}-b_{T}\bm{1})\) by \(2\sqrt{d}\). Therefore, by a triangle inequality, satisfied both by the non-negative part \((\,\cdot\,)_{+}\) and the norm \(\|\,\cdot\,\|\) functions, we have

\[\left\|\left(\sum_{t=1}^{T}\bm{c}_{t}-T\left(\bm{B}-b_{T}\bm{1} \right)\right)_{+}\right\|\] \[\leqslant(K+1)\big{(}M_{T,\delta,\operatorname{ilog}T}+2\sqrt{d} \big{)}\leqslant T\,b_{T}\,,\]

where we used the fact that \(M_{T,\delta,k}\) increases with \(k\), the bound \(K\leqslant\operatorname{ilog}T\) proved in Step 1 and holding on the event \(\mathcal{E}_{\text{meta}}\), as well as the definition of \(b_{T}\). Therefore, on \(\mathcal{E}_{\text{meta}}\), no component of

\[\left(\sum_{t=1}^{T}\bm{c}_{t}-T\left(\bm{B}-b_{T}\bm{1}\right)\right)_{+}\]

can be larger than \(T\,b_{T}\), which yields the desired control \(\sum_{t=1}^{T}\bm{c}_{t}\leqslant T\bm{B}\).

### Step 3: Computing the associated regret bound

The total regret is the sum of the regrets suffered over each regime:

\[R_{T}=\sum_{k=0}^{K}\left(\left(T_{k+1}-T_{k}\right)\textsc{opt}(r,\bm{c},\bm {B})-\sum_{t=T_{k}}^{T_{k+1}-1}r_{t}\right).\]On the favorable event \(\mathcal{E}_{\text{meta}}\), the bound of Lemma 2 holds in particular at the end of each regime; i.e., given the parameters \(\gamma_{k}=2^{k}/\sqrt{T}\) and \(\delta/\big{(}4(k+2)^{2}\big{)}\) to run the Box B strategy in regime \(k\in\{0,\ldots,K\}\), it holds on \(\mathcal{E}_{\text{meta}}\) that

\[(T_{k+1}-T_{k})\,\textsc{opt}(r,\bm{c},\bm{B})-\sum_{t=T_{k}}^{T_ {k+1}-1}r_{t}\leqslant\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\Big{(}(T_{k +1}-T_{k})\,b_{T}\sqrt{d}+6\Upsilon_{T,\delta/(k+2)^{2}}\Big{)}\\ +36\sqrt{d}\,\big{(}\Upsilon_{T,\delta/(k+2)^{2}}\big{)}^{2}\, \frac{2^{k}}{\sqrt{T}}+8\ln\frac{T^{2}}{\delta/\big{(}4(k+2)^{2}\big{)}}\,.\]

We now sum the above bounds and use the (in)equalities \(\Upsilon_{T,\delta/(k+2)^{2}}\leqslant\Upsilon_{T,\delta/(K+2)^{2}}\),

\[\sum_{k=0}^{K}(T_{k+1}-T_{k})\,b_{T}=T\,b_{T}\,,\qquad\text{and}\qquad\sum_{k =0}^{K}2^{k}\leqslant 2^{K+1}\leqslant 2^{1+\mathrm{i}\log\|\bm{\lambda}_{\bm{B}-b_{T} \bm{1}}^{\star}\|}\leqslant 4\,\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\]

to get

\[R_{T}\leqslant\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\Big{(} T\,b_{T}\sqrt{d}+6K\Upsilon_{T,\delta/(K+2)^{2}}\Big{)}+144\sqrt{d}\,\|\bm{ \lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\frac{\big{(}\Upsilon_{T,\delta/(K+2)^ {2}}\big{)}^{2}}{\sqrt{T}}\\ +8K\ln\frac{T^{2}}{\delta/\big{(}4(K+2)^{2}\big{)}}\,.\]

The final regret bound is achieved by substituting the inequality \(K\leqslant\mathrm{i}\log T\) proved in Step 1:

\[R_{T}\leqslant\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\left( 144\sqrt{d}\,\frac{\big{(}\Upsilon_{T,\delta/(2+\mathrm{i}\log T)^{2}}\big{)} ^{2}}{\sqrt{T}}+T\,b_{T}\sqrt{d}+6\Upsilon_{T,\delta/(2+\mathrm{i}\log T)^{2} }\,\mathrm{i}\log T\right)\\ +8\ln\frac{T^{2}}{\delta/\big{(}4(2+\mathrm{i}\log T)^{2}\big{)} }\,\mathrm{i}\log T\,.\] (19)

The order of magnitude is \(\sqrt{T}\), up to poly-logarithmic terms, for the quantity \(\Upsilon_{T,\delta/(2+\mathrm{i}\log T)^{2}}\), thus for \(M_{T,\delta,\mathrm{i}\log T}\), thus for \(T\,b_{T}\), therefore, the order of magnitude in \(\sqrt{T}\) of the above bound is, up to poly-logarithmic terms,

\[\big{(}1+\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\big{)}\sqrt{T},\]

as claimed.

## Appendix D Proofs of Lemma 3 and Corollary 1

Proof of Lemma 3.: For \(\bm{\lambda}\geqslant\bm{0}\) and \(\bm{C}\in[0,1]^{d}\), we denote

\[L(\bm{\lambda},\bm{C})=\mathbb{E}_{\bm{X}\sim\bm{v}}\bigg{[}\max_{a\in\mathcal{ A}}\Bigl{\{}r(\bm{X},a)-\big{\langle}\bm{c}(\bm{X},a)-\bm{C},\,\bm{\lambda} \big{\rangle}\Bigr{\}}\bigg{]}\]

so that by (4) and the feasibility assumption, we have, at least for \(\bm{C}=\widetilde{\bm{B}}\) and \(\bm{C}=\bm{B}-b\bm{1}\):

\[\textsc{opt}(r,\bm{c},\bm{C})=\min_{\bm{\lambda}\geqslant\bm{0}}L(\bm{ \lambda},\bm{C})=L(\bm{\lambda}_{\bm{C}}^{\star},\bm{C})\,.\]

The function \(L\) is linear in \(\bm{C}\), so that

\[\textsc{opt}\big{(}r,\bm{c},\widetilde{\bm{B}}\big{)}=L\big{(} \lambda_{\widetilde{\bm{B}}}^{\star},\widetilde{\bm{B}}\big{)}\leqslant L \big{(}\lambda_{\bm{B}-b\bm{1}}^{\star},\widetilde{\bm{B}}\big{)} =L\big{(}\lambda_{\bm{B}-b\bm{1}}^{\star},\bm{B}-b\bm{1}\big{)}- \big{\langle}\lambda_{\bm{B}-b\bm{1}}^{\star},\,\bm{B}-b\bm{1}-\widetilde{\bm{ B}}\big{\rangle}\\ =\textsc{opt}\big{(}r,\bm{c},\bm{B}-b\bm{1}\big{)}-\big{\langle} \lambda_{\bm{B}-b\bm{1}}^{\star},\,\bm{B}-b\bm{1}-\widetilde{\bm{B}}\big{\rangle}\,.\]

The result follows from substituting

\[\big{\langle}\lambda_{\bm{B}-b\bm{1}}^{\star},\,\bm{B}-b\bm{1}-\widetilde{\bm{ B}}\big{\rangle}\geqslant\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|_{1}\,\min \bigl{(}\bm{B}-b\bm{1}-\widetilde{\bm{B}}\big{)}\geqslant\|\bm{\lambda}_{\bm{B} -b\bm{1}}^{\star}\|\,\min\bigl{(}\bm{B}-b\bm{1}-\widetilde{\bm{B}}\big{)}\]

and from rearranging the inequality thus obtained. 

Proof of Corollary 1.: We apply Lemma 3 with \(\widetilde{\bm{B}}=\varepsilon\bm{1}\) for some \(\varepsilon>0\) sufficiently small and obtain

\[\|\bm{\lambda}_{\bm{B}-b\bm{1}}^{\star}\|\leqslant\frac{\textsc{opt}(r,\bm{c}, \bm{B}-b\bm{1})-\textsc{opt}\big{(}r,\bm{c},\varepsilon\bm{1}\big{)}}{\min \bigl{(}\bm{B}-(b+\varepsilon)\bm{1}\big{)}}\,.\]

We conclude by substituting \(\textsc{opt}(r,\bm{c},\bm{B}-b\bm{1})\leqslant\textsc{opt}(r,\bm{c},\bm{B})\) and \(\textsc{opt}\big{(}r,\bm{c},\varepsilon\bm{1}\big{)}\geqslant\textsc{opt} \big{(}r,\bm{c},\bm{0}\big{)}\) as well as \(\min(\bm{B}-b\bm{1})\geqslant\min\bm{B}/2\), and by letting \(\varepsilon\to 0\)Additional (sketch of) results concerning optimality

We detail here two series of claims made in Section 4.

### A proof scheme for problem-dependent lower bounds

We provide the proof scheme for proving the problem-dependent lower bound of order \(\big{(}1+\|\lambda_{\bm{B}}^{\star}\|\big{)}\,\sqrt{T}\) announced in Section 4.

Step 0: Considering strict cost constraints.Our aim, as described in Box A, is to make sure that with high probability the cumulative costs are smaller than \(T\,\bm{B}\). If we considered softer constraints, of the form \(T\,\bm{B}+\tilde{\mathcal{O}}\big{(}\sqrt{T}\big{)}\), then \(\tilde{\mathcal{O}}\big{(}\sqrt{T}\big{)}\) regret bounds would be possible (see Appendix F); i.e., the factor \(\|\bm{\lambda}_{\bm{B}-b_{T}\bm{1}}^{\star}\|\) of Theorem 1 could be replaced by a constant. Thus, lower bounds are only interesting in the case of hard constraints stated in Box A.

Step 1: Necessity of a margin \(b_{T}\) of order \(1/\sqrt{T}\).First, a classical lemma in CBwK (see, e.g., Agrawal and Devanur, 2016, Lemma 1) indicates that a sequence of adaptive cannot perform better than an optimal static policy. Denote by \(\pi_{\bm{B}^{\prime}}^{\star}\) a (quasi-)optimal policy for the average cost constraints \(\bm{B}^{\prime}\). Provided that costs are truly random (i.e., do not stem from Dirac distributions, which in particular, does not cover the cases where there is a null-cost action, see Limitation 2 in Section 4), then the law of iterated logarithm shows that when playing \(\pi_{\bm{B}^{\prime}}^{\star}\) at each round, the cumulative costs must (almost-surely, as \(T\to+\infty\)) be larger than \(T\,\bm{B}^{\prime}\) plus a positive term of the order of \(\sqrt{T\ln\ln T}\). Therefore, to meet the hard constraints, one should pick \(\bm{B}^{\prime}\) of the form \(\bm{B}-b_{T}\bm{1}\), where \(b_{T}\) is of order \(1/\sqrt{T}\) up to logarithmic terms.

Step 2: Consequences in terms of regret.Therefore, the largest average reward a strategy may target is \(\textsc{opt}(r,\bm{c},\bm{B}-b_{T}\bm{1})\). Deviations of the order \(\sqrt{T}\) are also bound to happen. Therefore, up to logarithmic terms, the regret lower bound is approximatively larger than something of the order

\[T\big{(}\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c},\bm{B}-b_{T}\bm{1} )\big{)}+\sqrt{T}\,.\]

Now, an argument similar to the one used in the proof of Lemma 3 shows that

\[\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c},\bm{B}-b_{T}\bm{1}) \geqslant L(\lambda_{\bm{B}}^{\star},\bm{B})-L(\lambda_{\bm{B}}^{\star},\bm{B }-b_{T}\bm{1})=b_{T}\,\|\lambda_{\bm{B}}^{\star}\|\,.\]

All in all, the regret lower bound is thus approximatively larger than something of the order of

\[\big{(}1+\|\lambda_{\bm{B}}^{\star}\|\big{)}\,\sqrt{T}\,.\]

This matches the form of the bound of Theorem 1, but the dual vector \(\lambda_{\bm{B}-b_{T}\bm{1}}^{\star}\) present in the upper bound is replaced by \(\lambda_{\bm{B}}^{\star}\) in our lower bound.

### Faster rates may be achievable in some specific cases

We explain here why, in some specific cases, faster rates would be achievable--with \(\sqrt{T}\) in the regret bound of Theorem 1 replaced by \(\sqrt{TB}\) for a problem with scalar costs and total-cost constraints \(B\).

Indeed, consider a problem similar to Example 1: with scalar costs, total-cost constraint \(B>0\), featuring a baseline action \(a_{\text{null}}\) with null cost but also null reward, and additional actions with larger rewards and expected costs \(c(\bm{x},a)\geqslant\alpha>0\). Let \(N_{T}\) denotes the number of times non-null cost actions are played within the first \(T\) rounds. Deviation inequalities have it that

\[\sum_{t=1}^{T}c_{t}\geqslant\alpha N_{T}-\tilde{O}\Big{(}\sqrt{N_{T}}\Big{)}\,,\]

where we recall that \(\tilde{O}\) is up to poly-logarithmic factors; as a consequence, the total-cost constraint enforces that

\[N_{T}\leqslant\frac{TB}{\alpha}+\tilde{O}\Bigg{(}\sqrt{\frac{TB}{\alpha}} \Bigg{)}\,.\]In particular, the margin \(b_{T}\) only needs to be of order \(\sqrt{N_{T}}/T\), i.e., \(\sqrt{B/(\alpha T)}\), instead of \(1/\sqrt{T}\).

Similarly, since at most \(N_{T}\) non-null actions are played, the regret should be lower bounded by something of the order of

\[T\big{(}\textsc{opt}(r,c,B)-\textsc{opt}(r,c,B-b_{T})\big{)}+\sqrt{N_{T}}\,,\]

where, as in Step 2 above,

\[\big{(}\textsc{opt}(r,c,B)-\textsc{opt}(r,c,B-b_{T})\big{)}\geq b_{T}| \lambda^{\star}_{B}|=\tilde{O}\Bigg{(}\sqrt{\frac{B}{\alpha T}}\Bigg{)}\ |\lambda^{\star}_{B}|.\]

This suggests a lower bound on the regret of the order (up to poly-logarithmic factors) of

\[\big{(}|\lambda^{\star}_{B}|+1\big{)}\,\sqrt{TB/\alpha}\qquad\text{instead of}\qquad\big{(}1+|\lambda^{\star}_{B}|\big{)}\,\sqrt{T}\,.\]

The difference between the two bounds is significant when \(B\) is small, i.e., \(B\ll 1\). While proving a matching upper bound with the Box B and Box C strategies looks a bit tricky, we feel that it must be possible to do so with the primal approach of Appendix F, at least when the context space \(\mathcal{X}\) is finite. If our intuition holds, then it would be possible to get an upper bound

\[R_{T}\leqslant\tilde{O}\Big{(}Tb_{T}\big{(}1+|\lambda^{\star}_{ B-b_{T}}|\big{)}\Big{)}\\ =\tilde{O}\Bigg{(}\sqrt{\frac{TB}{\alpha}}\left(1+\frac{\textsc{ opt}(r,c,B)-\textsc{opt}(r,c,0)}{B}\right)\Bigg{)}=\tilde{O}\Big{(}\sqrt{TB/ \alpha^{3}}\Big{)}\,,\]

where the last bound follows from \(\textsc{opt}(r,c,B)-\textsc{opt}(r,c,0)\leqslant B/\alpha\), as explained in Example 1.

## Appendix F Primal strategy

This section studies the primal strategy stated in Box D, which, at every round, solves an approximation of the primal optimization problem (1). The key issue in running such a primal approach is estimating \(\nu\), see comments after Notation 1; this primal approach is essentially worth for the case of finite context sets \(\mathcal{X}\). The aim of this section is threefold:

1. In Appendix F.1, we provide a theory of "soft" constraints, when total-cost deviations from \(T\bm{B}\) of order \(\sqrt{T}\) up to logarithmic terms are allowed; at least when \(\mathcal{X}\) is a finite set, the regret bound then becomes proportional to \(\sqrt{T}\) up to logarithmic terms.
2. In Appendix F.2, we revisit and extend the results by Li and Stoltz (2022). The extension consists of dealing with possibly signed constraints, and the revisited analysis (of the same strategy as in Li and Stoltz, 2022) consists in not directly dealing with KKT constraints (which, in addition, imposed the finiteness of \(\mathcal{X}\)) but in only relating optimization problems--defined with the true \(r\) and \(\bm{c}\) or estimates thereof. We also offer a modular approach and separate the error terms coming from estimating \(\nu\) and from estimating \(r\) and \(\bm{c}\).
3. In Appendix F.3, we generalize the results of Appendix F.2, which rely on the existence of a null-cost action, and get guarantees that correspond quite exactly to the combination of Theorem 1 and the interpretation thereof offered by Lemma 3, at least in the case of a finite \(\mathcal{X}\). Actually, in our research path, we had first obtained these primal results, before trying to obtain them in a more general case of a continuous \(\mathcal{X}\), by resorting to a dual strategy. The proof technique in Appendix F.3 also inspired our approach to proving problem-dependent lower bounds presented in Appendix E.1.

Throughout this appendix, we will assume that the context distribution \(\nu\) can be estimated in some way. We provide examples and pointers below.

**Notation 1**.: _Fix \(\delta\in(0,1)\). We denote by \(\widehat{\nu}_{\delta,t}\) a sequence of estimators of \(\nu\), each constructed on the contexts \(\bm{x}_{1},\dots,\bm{x}_{t}\), and by \(\xi_{t,\delta}\) a sequence of estimation errors such that, with probability at least \(1-\delta\), for all bounded functions \(f:\mathcal{X}\to[-1,1]\),_

\[\Big{|}\mathbb{E}_{\bm{X}\sim\nu}\big{[}f(\bm{X})\big{]}-\mathbb{E}_{\bm{X} \sim\widehat{\nu}_{\delta,t}}\big{[}f(\bm{X})\big{]}\Big{|}\leqslant\xi_{t, \delta}\,.\]

_We also denote \(\Xi_{T,\delta}=\sum_{t=1}^{T}\xi_{t-1,\delta}\,.\)_

[MISSING_PAGE_FAIL:25]

densities. To control the latter with uniform convergence rates, so as to obtain deviation terms \(\xi_{t,\delta}\) only depending on \(t\) and \(\delta\), heavy assumptions on the model to which \(\nu\) belongs are in order, e.g., some Holderian regularity, and uniform estimation rates obtained degrade with the ambient dimension \(n\); they are generally much slower than \(1/\sqrt{t}\). The total error terms \(\Xi_{T,\delta}\) then prevent the bounds stated below in Proposition 1 and Theorems 3 and 4 from sharing the same orders of magnitude than the bounds proved with our dual approach in Theorem 1 and interpreted in Section 3.3. On this topic, see also (Ai et al., 2022, Section 4.1) for the estimation rates as well as a similar description in Han et al. (2022, end of Section 1) of the limitation of the primal approach in CBwK due to the estimation of densities. General references on density estimations are the monographs by Devroye and Gyorfi (1985) in \(\mathbb{L}^{1}\) and Tsybakov (2008) in \(\mathbb{L}^{2}\).

Note that in the dual approach, the knowledge of (the possibly complex) \(\nu\) is replaced by the knowledge of the (finite-dimensional) optimal dual variables \(\bm{\lambda}_{\bm{B}}^{*}\in\mathbb{R}^{d}\), which is easier to learn. This explains the fundamental efficiency of the dual approach compared to the primal approach.

### Analysis with "soft" constraints

We first provide an analysis for a version of the Box D strategy that may possibly breach the total-cost constraints \(T\bm{B}\). More precisely, we allow deviations to \(T\bm{B}\) of the order of \(\sqrt{T}\) times poly-log factors: this is what we refer to as "soft" constraints. Our result is that the regret may then be bounded by a quantity of order \(\sqrt{T}\) times poly-log factors, at least when \(\mathcal{X}\) is finite.

We do so for two reasons: first, because we do not think that this is a well-known result, and second, for pedagogic reasons, as the proof for "hard" constraints follows from adapting the proof scheme for soft constraints (see Appendix F.2).

**Proposition 1** (soft constraints).: _Fix \(\delta\in(0,1)\). Under Assumption 2 and with Notation (1), the strategy of Box D, run with \(\delta/4\) and positive slacks \(b_{t}=\xi_{t-1,\delta/4}\), ensures that with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\bm{c}_{t}\leqslant T\bm{B}+\big{(}2\alpha_{T,\delta/4}+\beta_{ T,\delta/4}+2\Xi_{T,\delta/4}\big{)}\bm{1}\qquad\text{and}\qquad R_{T} \leqslant 2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+2\Xi_{T,\delta/4}\,,\]

_where \(\alpha_{T,\delta/4}=\sqrt{2T\ln\bigl{(}(d+1)/(\delta/4)\bigr{)}}\)._

In particular, when \(\mathcal{X}\) is finite, the deviation terms \(2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+2\Xi_{T,\delta/4}\) are of order \(\sqrt{T}\) up to poly-logarithmic terms, so that the bound of Proposition 1 reads: with high-probability,

\[\sum_{t=1}^{T}\bm{c}_{t}\leqslant T\bm{B}+\widetilde{\mathcal{O}}\bigl{(} \sqrt{T}\bigr{)}\qquad\text{and}\qquad R_{T}\leqslant\widetilde{\mathcal{O}} \bigl{(}\sqrt{T}\bigr{)}\,.\]

Put differently, soft-constraint satisfaction allows for \(\widetilde{\mathcal{O}}\bigl{(}\sqrt{T}\bigr{)}\) regret bounds when \(\mathcal{X}\) is finite.

Proof.: As in the proofs of Lemmas 1 and 2 in Appendix B, we consider four events, each of probability at least \(1-\delta/4\): two events \(\mathcal{E}_{\text{H-A/2}}\) and \(\mathcal{E}_{\text{H-A/2}}\), defined below, following from applications of the Hoeffding-Azuma inequality, the favorable event \(\mathcal{E}_{\text{TVD}}\) of Notation 1 with \(\delta/4\), and the favorable event \(\mathcal{E}_{\beta}\) of Assumption 2 with \(\delta/4\). Namely, given the value for \(\alpha_{T,\delta/4}\) proposed in the statement (note this value is slightly different from the one considered in Appendix B), we have, on the one hand, on \(\mathcal{E}_{\text{H-A/2}}\),

\[\sum_{t=1}^{T}\bm{c}_{t}\leqslant\alpha_{T,\delta/4}\bm{1}+\sum_{t=1}^{T}\bm{c} (\bm{x}_{t},a_{t})\qquad\text{and}\qquad\sum_{t=1}^{T}r_{t}\geqslant-\alpha_{ T,\delta/4}+\sum_{t=1}^{T}r(\bm{x}_{t},a_{t})\,,\] (20)

and on the other hand, on \(\mathcal{E}_{\text{H-A/2}}\),

\[\sum_{t=1}^{T}\widehat{c}_{\delta/4,t-1}^{\text{kb}}(\bm{x}_{t}, a_{t})\leqslant\alpha_{T,\delta/4}\bm{1}+\sum_{t=1}^{T}\mathbb{E}_{\bm{X}\sim\nu} \Biggl{[}\sum_{a\in\mathcal{A}}\widehat{\bm{c}}_{\delta/4,t-1}^{\text{kb}}( \bm{X},a)\,\pi_{t,a}(\bm{X})\Biggr{]}\] \[\text{and}\qquad\sum_{t=1}^{T}\widehat{r}_{\delta/4,t-1}^{\text{ ub}}(\bm{x}_{t},a_{t})\geqslant-\alpha_{T,\delta/4}+\sum_{t=1}^{T}\mathbb{E}_{\bm{X} \sim\nu}\Biggl{[}\sum_{a\in\mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\text{vb}}( \bm{X},a)\,\pi_{t,a}(\bm{X})\Biggr{]}\,.\] (21)The first two inequalities are obtained by considering conditional expectations with respect to the past, \(\bm{x}_{t}\) and \(a_{t}\), while the second two inequalities follow from taking conditional expectations with respect to the past and \(\bm{x}_{t}\); we crucially use that, by definition, \(\bm{x}_{t}\sim\nu\) is independent from \(\bm{\pi}_{t}\) and the estimates \(\widehat{r}_{\delta/4,t-1}^{\text{\,bb}}\) and \(\widehat{\bm{c}}_{\delta/4,t-1}^{\text{\,bb}}\).

We first note that by \(\bm{B}\)-feasibility of the problem, by (3), and by Notation (1), a policy \(\bm{\pi}_{t}\) satisfying the constraints stated in Box D exists at each round \(t\geqslant 1\) on the event \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{\sc tVD}}\). Indeed, denoting by \(\bm{\pi}^{\text{\,feas}}\) such a \(\bm{B}\)-feasible policy, this existence follows from the inequalities

\[\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}}\Bigg{[} \sum_{a\in\mathcal{A}}\widehat{\bm{c}}_{\delta/4,t-1}^{\text{\,bb}}(\bm{X},a) \,\pi_{a}^{\text{\,res}}(\bm{X})\Bigg{]} \leqslant\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}} \Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi_{a}^{\text{\,res}}(\bm{X}) \Bigg{]}\] \[\leqslant\xi_{t-1,\delta/4}+\underbrace{\mathbb{E}_{\bm{X}\sim \nu}\Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi_{a}^{\text{\,res}}(\bm {X})\Bigg{]}}_{\leqslant\bm{B}}\] (22)

and from the choice \(b_{t}=\xi_{t-1,\delta/4}\).

Cost-wise, we then successively have, by (20), then (3) and Assumption 2, and finally (21) and Notation (1), on the event \(\mathcal{E}_{\text{\sc H-A-1}}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{ \sc H-A-2}}\cap\mathcal{E}_{\text{\sc tVD}}\) of probability at least \(1-\delta\),

\[\sum_{t=1}^{T}\bm{c}_{t} \leqslant\alpha_{T,\delta/4}\bm{1}+\sum_{t=1}^{T}\bm{c}(\bm{x}_ {t},a_{t})\] \[\leqslant\big{(}\alpha_{T,\delta/4}+\beta_{T,\delta/4}\big{)}\bm{ 1}+\sum_{t=1}^{T}\widehat{\bm{c}}_{\delta/4,t-1}^{\text{\,bb}}(\bm{x}_{t},a_{ t})\] \[\leqslant\big{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}\big{)} \bm{1}+\sum_{t=1}^{T}\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}} \Bigg{[}\sum_{a\in\mathcal{A}}\widehat{\bm{c}}_{\delta/4,t-1}^{\text{\,bb}}( \bm{X},a)\,\pi_{t,a}(\bm{X})\Bigg{]}\] \[\leqslant\big{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+\Xi_{T, \delta/4}\big{)}\bm{1}+\sum_{t=1}^{T}\underbrace{\mathbb{E}_{\bm{X}\sim \widetilde{\nu}_{\delta/4,t-1}}\Bigg{[}\sum_{a\in\mathcal{A}}\widehat{\bm{c}} _{\delta/4,t-1}^{\text{\,bb}}(\bm{X},a)\,\pi_{t,a}(\bm{X})\Bigg{]}}_{ \leqslant\bm{B}+b_{t}\bm{1}},\]

where the inequalities \(\leqslant\bm{B}+b_{t}\bm{1}\) follow from the definition of \(\bm{\pi}_{t}\) in Box D. Substituting the choice \(b_{t}=\xi_{t-1,\delta/4}\), we thus proved that on \(\mathcal{E}_{\text{\sc H-A-1}}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{ \sc H-A-2}}\cap\mathcal{E}_{\text{\sc tVD}}\),

\[\sum_{t=1}^{T}\bm{c}_{t}\leqslant T\bm{B}+\big{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+2\Xi_{T,\delta/4}\big{)}\bm{1}\,,\]

as claimed.

The control for rewards mimics the steps above for costs (and then resorts to an additional argument). We obtain first that on \(\mathcal{E}_{\text{\sc H-A-1}}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{ \sc H-A-2}}\cap\mathcal{E}_{\text{\sc tVD}}\),

\[\sum_{t=1}^{T}r_{t}\geqslant-\big{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+ \Xi_{T,\delta/4}\big{)}+\sum_{t=1}^{T}\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{ \delta/4,t-1}}\Bigg{[}\sum_{a\in\mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\text {\,ab}}(\bm{X},a)\,\pi_{t,a}(\bm{X})\Bigg{]}\,.\]

We denote by \(\pi_{\bm{B}}^{\star}\) an optimal static policy for the average cost constraints \(\bm{B}\)--when it exists, e.g., by (4), as soon as the problem is feasible for some \(\bm{B}^{\prime}<\bm{B}\); otherwise, we take a static policy achieving \(\text{\sc opt}(r,\bm{c},\bm{B})\) up to some small \(e>0\), which we let vanish in a final step of the proof. As in (22), we have, on \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{\sc tVD}}\),

\[\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}}\Bigg{[}\sum_ {a\in\mathcal{A}}\widehat{\bm{c}}_{\delta/4,t-1}^{\text{\,bb}}(\bm{X},a)\,\pi_ {\bm{B},a}^{\star}(\bm{X})\Bigg{]} \leqslant\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}} \Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi_{\bm{B},a}^{\star}(\bm{X}) \Bigg{]}\] \[\leqslant\xi_{t-1,\delta/4}\bm{1}+\underbrace{\mathbb{E}_{\bm{X} \sim\nu}\Bigg{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi_{\bm{B},a}^{\star}( \bm{X})\Bigg{]}}_{\leqslant\bm{B}},\]where the \(\leqslant\bm{B}\) inequality follows from the definition of \(\bm{\pi}_{\bm{B}}^{\star}\). Thanks to the choice \(b_{t}=\xi_{t-1,\delta/4}\), we have, by definition of \(\bm{\pi}_{t}\) as some optimal policy in Box D and on \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{\sc VD}}\),

\[\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}}\Bigg{[}\sum_{a\in \mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\text{\sc ub}}(\bm{X},a)\,\pi_{t,a}( \bm{X})\Bigg{]} \geqslant\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}} \Bigg{[}\sum_{a\in\mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\text{\sc ub}}(\bm{ X},a)\,\pi_{\bm{B},a}^{\star}(\bm{X})\Bigg{]}\,.\]

Again by (3) and Notation (1), we have, on \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{\sc VD}}\),

\[\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}}\Bigg{[} \sum_{a\in\mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\text{\sc ub}}(\bm{X},a)\, \pi_{\bm{B},a}^{\star}(\bm{X})\Bigg{]} \geqslant\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}} \Bigg{[}\sum_{a\in\mathcal{A}}r(\bm{X},a)\,\pi_{\bm{B},a}^{\star}(\bm{X}) \Bigg{]}\] \[\geqslant-\xi_{t-1,\delta/4}+\underbrace{\mathbb{E}_{\bm{X}\sim \nu}\Bigg{[}\sum_{a\in\mathcal{A}}r(\bm{X},a)\,\pi_{\bm{B},a}^{\star}(\bm{X}) \Bigg{]}}_{=\text{\sc opt}(\bm{r},\bm{c},\bm{B})}.\]

Collecting all bounds, we proved that on \(\mathcal{E}_{\text{\sc H-A}l}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{ \sc H-A}2}\cap\mathcal{E}_{\text{\sc VD}}\),

\[\sum_{t=1}^{T}r_{t}\geqslant T\text{\sc opt}(r,\bm{c},\bm{B})-\left(2\alpha_{ T,\delta/4}+\beta_{T,\delta/4}+2\Xi_{T,\delta/4}\right),\]

which corresponds to the claimed regret bound. 

### Analysis with "hard" constraints and a null-cost action

We now turn our attention to the main kind of result that we want to achieve: when constraints must be strictly satisfied--which we refer to as "hard" constraints. For the sake of simplicity, we do so for now in the presence of a null-cost action; Appendix F.3 explains how the analysis may be generalized to cases without such null-cost actions.

The following result corresponds to the combination of Theorem 1 with Corollary 1, and also, to Li and Stoltz (2022, main result: Theorem 2).

**Theorem 3** (hard constraints).: _Fix \(\delta\in(0,1)\). We consider the strategy of Box D, run with \(\delta/4\) and negative slacks all equal to_

\[b_{t}\equiv-\overline{\Delta}_{T,\delta/4}\,,\qquad\text{where}\qquad\overline {\Delta}_{T,\delta/4}\overset{\text{\rm def}}{=}\frac{2\alpha_{T,\delta/4}+ \beta_{T,\delta/4}+\Xi_{T,\delta/4}}{T}\]

_and \(\alpha_{T,\delta/4}=\sqrt{2T\ln\bigl{(}(d+1)/(\delta/4)\bigr{)}}\). Assume that a null-cost action exists, that \(\overline{\Delta}_{T,\delta/4}<\min\bm{B}\), that Assumption 2 holds, and use Notation (1). Then, with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\bm{c}_{t}\leqslant T\bm{B}\quad\text{ and }\quad R_{T} \leqslant\bigl{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+2\Xi_{T,\delta/4} \bigr{)}\left(1+\frac{\text{\sc opt}(r,\bm{c},\bm{B})-\text{\sc opt}(r,\bm{c },\bm{0})}{\min\bm{B}}\right).\]

Proof.: We explain how the proof of Proposition 1 may be adapted. We first justify the existence at each round \(t\geqslant 1\) of a policy \(\bm{\pi}_{t}\) satisfying the cost constraints stated in Box D: for the null-cost action \(a_{\text{\sc null}}\), we may impose that, for all \(\bm{x}\in\mathcal{X}\), all \(t\geqslant 0\), and all \(\delta\in(0,1)\),

\[\widehat{\bm{c}}_{t}(\bm{x},a_{\text{\sc null}})=\bm{0}\quad\text{and}\quad \varepsilon_{t}(\bm{x},a_{\text{\sc null}},\delta)=0\,,\qquad\text{so that}\quad\widehat{\bm{c}}_{\delta,t}^{\text{\sc lb}}(\bm{x},a_{ \text{\sc null}})=\bm{0}\ \ \text{a.s.};\]

this shows that at least the static policy \(\bm{\pi}^{\text{\sc null}}\) always playing \(a_{\text{\sc null}}\) satisfying the defining cost-constraints for \(\bm{\pi}_{t}\). (Alternatively, we may note that the policy \(\overline{\bm{\pi}}_{t}\) defined below also satisfies the cost constraints stated in Box D, on the high-probability event considered.)

We then handle total-cost constraints similarly as in the proof of Proposition 1 and obtain that on the event \(\mathcal{E}_{\text{\sc H-A}l}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\text{ \sc H-A}2}\cap\mathcal{E}_{\text{\sc VD}}\) of probability at least \(1-\delta\),

\[\sum_{t=1}^{T}\bm{c}_{t}\leqslant\bigl{(}2\alpha_{T,\delta/4}+\beta_{T, \delta/4}+\Xi_{T,\delta/4}\bigr{)}\bm{1}+\sum_{t=1}^{T}\underbrace{\mathbb{E}_ {\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}}\Bigg{[}\sum_{a\in\mathcal{A}} \widehat{c}_{\delta/4,t-1}^{\text{\sc kb}}(\bm{X},a)\,\pi_{t,a}(\bm{X})\Bigg{]} }_{\leqslant\bm{B}-\overline{\Delta}_{T,\delta/4}\bm{1}}\leqslant T\bm{B}\,,\]where this time, the slacks \(b_{t}=-\overline{\Delta}_{T,\delta/4}\) are negative and were set to cancel out the positive deviation terms. Similarly, on \(\mathcal{E}_{\textsc{H-Ax1}}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\textsc{H-Ax2 }}\cap\mathcal{E}_{\textsc{TVD}}\),

\[\sum_{t=1}^{T}r_{t}\geqslant-T\overline{\Delta}_{T,\delta/4}+\sum_{t=1}^{T} \mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}}\!\left[\sum_{a\in\mathcal{ A}}\widehat{r}_{\delta/4,t-1}^{\textsc{wb}}(\bm{X},a)\,\pi_{t,a}(\bm{X})\right].\] (23)

Now, the main modification to the proof of Proposition 1 is that (with its notation) we rather consider the policy

\[\overline{\bm{\pi}}_{t}=(1-w_{t})\bm{\pi}_{\bm{B}}^{\star}+w_{t}\bm{\pi}^{ \text{null}}\,,\qquad\text{where}\quad w_{t}=\min\!\left\{\frac{\overline{ \Delta}_{T,\delta/4}+\xi_{t-1,\delta/4}}{\min\bm{B}},\ 1\right\}.\]

As in (22), we see that this policy satisfies, on \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\textsc{TVD}}\):

\[\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}}\!\left[\sum_ {a\in\mathcal{A}}\widehat{\bm{c}}_{\delta/4,t-1}^{\textsc{bb}}(\bm{X},a)\, \overline{\pi}_{t,a}(\bm{X})\right] \leqslant\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}} \!\left[\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\overline{\pi}_{t,a}(\bm{X})\right]\] \[\leqslant\xi_{t-1,\delta/4}\bm{1}+\underbrace{\mathbb{E}_{\bm{X} \sim\nu}\!\left[\sum_{a\in\mathcal{A}}\!\bm{c}(\bm{X},a)\,\overline{\pi}_{t,a} (\bm{X})\right]}_{\leqslant(1-w_{t})\bm{B}}.\]

By using \(\bm{B}\geqslant(\min\bm{B})\bm{1}\) and since we assumed that \(\overline{\Delta}_{T,\delta/4}<\min\bm{B}\), we may continue the series of inequalities as follows:

\[\xi_{t-1,\delta/4}\bm{1}+(1-w_{t})\bm{B} \leqslant\bm{B}+\xi_{t-1,\delta/4}\bm{1}-w_{t}(\min\bm{B})\bm{1}\] \[=\bm{B}-\min\!\left\{\overline{\Delta}_{T,\delta/4},\ \min\bm{B}-\xi_{t-1,\delta/4}\right\}\bm{1}\] \[\leqslant\bm{B}-\min\!\left\{\overline{\Delta}_{T,\delta/4},\ \overline{\Delta}_{T,\delta/4}-\xi_{t-1,\delta/4}\right\}\bm{1}=\bm{B}+b_{t} \bm{1}\,,\]

meaning that \(\overline{\bm{\pi}}_{t}\) is a policy satisfying the constraints stated in Step 2 of Box D on round \(t\geqslant 1\). The consequence is that, first by definition of \(\bm{\pi}_{t}\) as a maximizer and then by the optimistic estimates (3) and Notation (1), on \(\mathcal{E}_{\beta}\cap\mathcal{E}_{\textsc{TVD}}\),

\[\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}}\!\left[\sum_{ a\in\mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\textsc{wb}}(\bm{X},a)\,\pi_{t,a}( \bm{X})\right]\] \[\geqslant\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}}\! \left[\sum_{a\in\mathcal{A}}\widehat{r}_{\delta/4,t-1}^{\textsc{wb}}(\bm{X},a )\,\overline{\pi}_{t,a}(\bm{X})\right]\] \[\geqslant-\xi_{t-1,\delta/4}+\mathbb{E}_{\bm{X}\sim\nu}\!\left[ \sum_{a\in\mathcal{A}}r(\bm{X},a)\,\overline{\pi}_{t,a}(\bm{X})\right]\] \[=-\xi_{t-1,\delta/4}+(1-w_{t})\textsc{opt}(r,\bm{c},\bm{B})+w_{t }\textsc{opt}(r,\bm{c},\bm{0})\] \[=\textsc{opt}(r,\bm{c},\bm{B})-\xi_{t-1,\delta/4}-\min\!\left\{ \frac{\overline{\Delta}_{T,\delta/4}+\xi_{t-1,\delta/4}}{\min\bm{B}},\ 1\right\}\left(\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c},\bm{0}) \right),\]

where \(\textsc{opt}(r,\bm{c},\bm{0})\) refers to the expect reward achieved by the null-cost action \(a_{\text{null}}\). After combination with (23) and summation, we get that on \(\mathcal{E}_{\textsc{H-Ax1}}\cap\mathcal{E}_{\beta}\cap\mathcal{E}_{\textsc{ H-Ax2}}\cap\mathcal{E}_{\textsc{TVD}}\):

\[\sum_{t=1}^{T}r_{t}-T\textsc{opt}(r,\bm{c},\bm{B})\] \[\geqslant-T\overline{\Delta}_{T,\delta/4}-\Xi_{T,\delta/4}-\sum_{ t=1}^{T}\min\!\left\{\frac{\overline{\Delta}_{T,\delta/4}+\xi_{t-1,\delta/4}}{ \min\bm{B}},\ 1\right\}\left(\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c},\bm{0})\right)\] \[\geqslant-\!\left(T\overline{\Delta}_{T,\delta/4}+\Xi_{T,\delta/4 }\right)\left(1+\frac{\textsc{opt}(r,\bm{c},\bm{B})-\textsc{opt}(r,\bm{c}, \bm{0})}{\min\bm{B}}\right),\]

which corresponds to the stated regret bound.

### General analysis with "hard" constraints

We finally generalize Theorem 3 and get a result corresponding to Theorem 1 combined with Lemma 3.

Here, we "mix" the slacks \(+\xi_{t-1,\delta/4}\) and \(-\overline{\Delta}_{T,\delta/4}\) of Appendices F.1 and F.2, with a slight modification of \(\overline{\Delta}_{T,\delta/4}\) to compensate for the \(\xi_{t-1,\delta/4}\): we rather have a \(2\Xi_{T,\delta/4}\) term in the numerator of \(\overline{\Delta}_{T,\delta/4}\), compared to the \(\Xi_{T,\delta/4}\) term in the numerator of \(\overline{\Delta}_{T,\delta/4}\).

**Theorem 4** (hard constraints).: _Fix \(\delta\in(0,1)\). We consider the strategy of Box D, run with \(\delta/4\) and signed slacks (depending on \(t\geqslant 1\))_

\[b_{t}=-\overline{\Delta}^{\prime}_{T,\delta/4}+\xi_{t-1,\delta/4}\qquad\text{ where}\qquad\overline{\Delta}^{\prime}_{T,\delta/4}=\frac{2\alpha_{T,\delta/4}+\beta_{T, \delta/4}+2\Xi_{T,\delta/4}}{T}\]

_and \(\alpha_{T,\delta/4}=\sqrt{2T\ln\bigl{(}(d+1)/(\delta/4)\bigr{)}}\). Assume that the problem is \((\bm{B}-\bm{m})\)-feasible for some \(\bm{m}\) that does not need to be known by the strategy, with \(\bm{B}-\bm{m}\leqslant\bm{B}-\overline{\Delta}^{\prime}_{T,\delta/4}\bm{1}\). Then, under Assumption 2 and with Notation (1), with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}\bm{c}_{t} \leqslant T\bm{B}\] \[\text{and}\qquad R_{T} \leqslant\bigl{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+2\Xi_{T,\delta/4}\bigr{)}\left(1+\frac{\texttt{opt}(r,\bm{c},\bm{B})-\texttt{opt}(r, \bm{c},\bm{B}-\bm{m})}{\min\bm{m}}\right)\,.\]

Proof.: We rather sketch the differences to the proofs of Theorem 3 and Proposition 1. We denote by \(\mathcal{E}_{\text{all}}\) the event of probability at least \(1-\delta\) obtained as the intersection of four convenient events of probability each at least \(1-\delta/4\). All inequalities below hold on \(\mathcal{E}_{\text{all}}\) but for the sake of brevity, we will not highlight this fact each time.

We denote by \(\bm{\pi}^{\text{feas}}\) a (quasi-)optimal static policy among the ones achieving expected costs smaller than \(\bm{B}-\bm{m}\); it therefore ensures that its expected costs are smaller than \(\bm{B}-\bm{m}\) and achieves an expected reward of \(\texttt{opt}(r,\bm{c},\bm{B}-\bm{m})-e\), possibly up to a small factor \(e\geqslant 0\) which we let vanish. We note that for each round \(t\geqslant 1\),

\[\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta,t-1}}\Biggl{[}\sum_ {a\in\mathcal{A}}\widehat{\bm{c}}^{\text{\,kb}}_{\delta,t-1}(\bm{X},a)\,\pi^{ \text{feas}}_{a}(\bm{X})\Biggr{]} \leqslant\xi_{t-1,\delta/T}\bm{1}+\mathbb{E}_{\bm{X}\sim\nu} \Biggl{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\pi^{\text{feas}}_{a}(\bm{X}) \Biggr{]}\] \[\leqslant\xi_{t-1,\delta/T}\bm{1}+\bm{B}-\bm{m}\bm{1}\leqslant\bm {B}+b_{t}\bm{1}\,,\]

given the definition \(b_{t}=-\overline{\Delta}^{\prime}_{T,\delta/4}+\xi_{t-1,\delta/4}\) and the assumption \(\bm{B}-\bm{m}\leqslant\bm{B}-\overline{\Delta}^{\prime}_{T,\delta/4}\bm{1}\). Thus, on \(\mathcal{E}_{\text{all}}\), the strategy \(\bm{\pi}_{t}\) is indeed defined at each round \(t\geqslant 1\) by the optimization problem stated in Step 2 of Box D (and not in an arbitrary manner).

Cost-wise, we thus have

\[\sum_{t=1}^{T}\bm{c}_{t} \leqslant\bigl{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+\Xi_{ T,\delta/4}\bigr{)}\bm{1}+\sum_{t=1}^{T}\overbrace{\mathbb{E}_{\bm{X}\sim \widetilde{\nu}_{\delta/4,t-1}}\Biggl{[}\sum_{a\in\mathcal{A}}\widehat{\bm{c} }^{\text{\,kb}}_{\delta/4,t-1}(\bm{X},a)\,\pi_{t,a}(\bm{X})\Biggr{]}}^{\leqslant T \bm{B}+\bigl{(}2\alpha_{T,\delta/4}+\beta_{T,\delta/4}+\Xi_{T,\delta/4}\bigr{)} \bm{1}-T\overline{\Delta}^{\prime}_{T,\delta/4}+\Xi_{T,\delta/4}=T\bm{B}\,,\]

where the final equality follows from the definition of \(\overline{\Delta}^{\prime}_{T,\delta/4}\), which involves a \(2\Xi_{T,\delta/4}\) term in its numerator.

Reward-wise, we introduce for each \(t\geqslant 1\),

\[\overline{\bm{\pi}}_{t}=(1-w_{t})\bm{\pi}^{\star}_{\bm{B}}+w_{t}\bm{\pi}^{ \text{feas}}\,,\qquad\text{where}\quad w_{t}=\min\Biggl{\{}\frac{\overline{ \Delta}^{\prime}_{T,\delta/4}}{\min\bm{m}},\ 1\Biggr{\}}\,,\]

whose empirical expected cost at round \(t\geqslant 1\) is smaller than

\[\mathbb{E}_{\bm{X}\sim\widetilde{\nu}_{\delta/4,t-1}}\Biggl{[} \sum_{a\in\mathcal{A}}\widehat{\bm{c}}^{\text{\,kb}}_{\delta/4,t-1}(\bm{X},a)\, \overline{\pi}_{t,a}(\bm{X})\Biggr{]} \leqslant\xi_{t-1,\delta/4}\bm{1}+\mathbb{E}_{\bm{X}\sim\nu} \Biggl{[}\sum_{a\in\mathcal{A}}\bm{c}(\bm{X},a)\,\overline{\pi}_{t,a}(\bm{X}) \Biggr{]}\] \[\leqslant\xi_{t-1,\delta/4}\bm{1}+(1-w_{t})\bm{B}+w_{t}(\bm{B}-\bm {m})\] \[=\bm{B}+\xi_{t-1,\delta/4}\bm{1}-w_{t}\bm{m}\,.\]By using \(\bm{m}\geqslant(\min\bm{m})\bm{1}\) and thanks to the assumption \(\bm{B}-\bm{m}\leqslant\bm{B}-\overline{\Delta}^{\prime}_{T,\delta/4}\bm{1}\), which can be equivalently formulated as \(\min\bm{m}\geqslant\overline{\Delta}^{\prime}_{T,\delta/4}\), we may continue this series of inequalities by

\[\bm{B}+\xi_{t-1,\delta/4}\bm{1}-w_{t}\bm{m} \leqslant\bm{B}-\min\Bigl{\{}\overline{\Delta}^{\prime}_{T, \delta/4}-\xi_{t-1,\delta/4},\ \min\bm{m}-\xi_{t-1,\delta/4}\Bigr{\}}\,\bm{1}\] \[\leqslant\bm{B}-\min\Bigl{\{}\overline{\Delta}^{\prime}_{T, \delta/4}-\xi_{t-1,\delta/4},\ \overline{\Delta}^{\prime}_{T,\delta/4}-\xi_{t-1,\delta/4}\Bigr{\}}\,\bm{1}= \bm{B}+b_{t}\bm{1}\,,\]

meaning that \(\overline{\bm{\pi}}_{t}\) is a policy satisfying the constraints stated in Step 2 of Box D on round \(t\geqslant 1\). In particular, by definition of \(\bm{\pi}_{t}\) as a maximizer,

\[\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}}\Biggl{[}\sum_ {a\in\mathcal{A}}\widehat{r}^{\text{wb}}_{\delta/4,t-1}(\bm{X},a)\,\pi_{t,a}( \bm{X})\Biggr{]}\\ \geqslant\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{\delta/4,t-1}} \Biggl{[}\sum_{a\in\mathcal{A}}\widehat{r}^{\text{wb}}_{\delta/4,t-1}(\bm{X},a )\,\overline{\pi}_{t,a}(\bm{X})\Biggr{]}\\ \geqslant-\xi_{t-1,\delta/4}+\mathbb{E}_{\bm{X}\sim\nu}\Biggl{[} \sum_{a\in\mathcal{A}}r(\bm{X},a)\,\overline{\pi}_{t,a}(\bm{X})\Biggr{]}\\ \geqslant-\xi_{t-1,\delta/4}+\bigl{(}(1-w_{t})\text{opt}(r,\bm{c} \,\bm{B})+w_{t}\text{opt}(r,\bm{c}\,\bm{B}-\bm{m})\bigr{)}\\ =\text{opt}(r,\bm{c}\,\bm{B})-\min\Biggl{\{}\frac{\overline{ \Delta}^{\prime}_{T,\delta/4}}{\min\bm{m}},\ 1\Biggr{\}}\left(\text{opt}(r,\bm{c}\,\bm{B})-\text{opt}(r,\bm{c}\,\bm{B}-\bm {m})\right)-\xi_{t-1,\delta/4}\,.\] (24)

Finally, by combining (23) and (24),

\[\sum_{t=1}^{T}r_{t} \geqslant-\bigl{(}\overline{2\alpha_{T,\delta/4}+\beta_{T,\delta/ 4}+\Xi_{T,\delta/4}}\bigr{)}\bm{1}+\sum_{t=1}^{T}\mathbb{E}_{\bm{X}\sim\widehat {\nu}_{\delta/4,t-1}}\Biggl{[}\sum_{a\in\mathcal{A}}\widehat{r}^{\text{wb}}_{ \delta/4,t-1}(\bm{X},a)\,\pi_{t,a}(\bm{X})\Biggr{]}\\ \geqslant T\,\text{opt}(r,\bm{c}\,\bm{B})+\Xi_{T,\delta/4}-T \overline{\Delta}^{\prime}_{T,\delta/4}\left(1+\frac{\text{opt}(r,\bm{c}\, \bm{B})-\text{opt}(r,\bm{c}\,\bm{B}-\bm{m})}{\min\bm{m}}\right)-\Xi_{T,\delta /4},\]

as claimed. 

## Appendix G Numerical experiments: full description

This appendix reports numerical simulations performed on simulated data with the motivating example described in Chohlas-Wood et al. (2021) and alluded at in Section 2.1. These simulations are for the sake of illustration only.

A brief summary of the applicative background in AI for justice is the following. The learner wants to maximize the total number of appearances to court for people of concern. To achieve this goal, the learner is able to provide, or not, some transportation assistance: rideshare assistance (the highest level of help), or a transit voucher (a more modest level of help). There are of course budget limits on these assistance means, and the learner also wants to control how (un)fair the assistance policy is, in terms of subgroups of the population, while maximizing the total number of appearances. Some subgroups take a better advantage of assistance to appear in court, thus, without the fairness control, all assistance would go to these groups. The fairness costs described in Section 2.1 force the learner to perform some tradeoff between spending all assistance on most reactive subgroups and spending it equally among subgroups.

Outline of this appendix.We first recall the experimental setting of Chohlas-Wood et al. (2021)--in particular, how contexts, rewards, and costs are generated in their simulations (we cannot replicate their study on real data, which is not accessible). We then specify the strategies we implemented and how we tuned them--this includes describing the estimation procedure discussed in Section 2.2. We finally report the performance observed, in terms of (average) rewards and costs.

[MISSING_PAGE_FAIL:32]

where \(\lambda^{\text{logistic}}\geqslant 0\) is a regularization factor. We define as \(\widehat{r}_{t}(\bm{x},a)=\Phi\big{(}\bm{\varphi}(\bm{x},a)^{\mathsf{T}}\widehat {\bm{\mu}}_{t}\big{)}\) the estimated expected rewards, and the associated uniform estimation errors (see Assumption 2) are of the form

\[\varepsilon_{t}(\bm{x},a,\delta)=C_{\delta}\big{(}1+\ln t\big{)} \sqrt{\bm{\varphi}(a,\bm{x})^{\mathsf{T}}V_{t}^{-1}\bm{\varphi}(a,\bm{x})}\] \[\text{where}\qquad V_{t}=\sum_{s=1}^{t}\bm{\varphi}(a_{s},\bm{x}_ {s})\,\bm{\varphi}(a_{s},\bm{x}_{s})^{\mathsf{T}}+\lambda^{\text{logistic}} \mathrm{I}_{5}\,,\]

where \(\mathrm{I}_{5}\) is the \(5\times 5\) identity matrix.

In our simulations, we tested a range of values and picked (in hindsight) the well-performing values \(C_{\delta}=0.025\) and \(\lambda^{\text{logistic}}=0\).

### Strategies implemented in this numerical study

We run all these strategies not with the total-cost constraints

\[\bm{B}=(0.05,\,0.2,\,\tau,\,\tau,\,\tau)\,,\qquad\text{where}\qquad\tau\in\{10 ^{-7},\,0.025\}\,,\]

but take a margin \(b=0.005\) and use \(\bm{B}^{\prime}=\bm{B}-(b,b,0,0,0)\) instead. This is a slightly different way of taking some margin on the average cost constraint: we do so because we are not aiming for a strict respect of the fairness constraints but rather want to report the level of violation on it. Again, we tried a range of values for \(b\) (between \(0.001\) to \(0.01\)) and this value of \(0.005\) led to a good balance between (lack of) total-cost constraint violations and rewards.

Performance of optimal static policies.We use \(\textsc{opt}(r,\bm{c},\bm{B})\) as the benchmark in the definition of regret; our methodology also reveals that \(\textsc{opt}(r,\bm{c},\bm{B}^{\prime})\) is another benchmark, see the discussion in Section 4. We report both values on our graphs. To compute them, we proceed as follows, e.g., for \(\bm{B}\). As computing directly the minimum stated in (4) is difficult, even when fully knowing the distribution \(\nu\), we compute \(100\) estimates

\[\textsc{opt}^{(j)}(r,\bm{c},\bm{B})\,,\qquad\text{which we average out into}\qquad\widehat{\textsc{opt}}(r,\bm{c},\bm{B})=\frac{1}{100}\sum_{j=1}^{100} \textsc{opt}^{(j)}(r,\bm{c},\bm{B})\,.\]

For each \(j\), we sample \(S=10,\!000\) contexts from the distribution \(\nu\), and denote by \(\widehat{\nu}_{S}^{(j)}\) the associated empirical distribution; we then solve numerically the problem (4) with \(\nu\) replaced by this empirical distribution:

\[\textsc{opt}^{(j)}(r,\bm{c},\bm{B})=\min_{\bm{\lambda}\geqslant 0}\ \mathbb{E}_{\bm{X}\sim\widehat{\nu}_{S}^{(j)}}\bigg{[}\!\max_{a\in\mathcal{A}} \!\Big{\{}r(\bm{X},a)-\big{\langle}\bm{c}(\bm{X},a)-\bm{B},\,\bm{\lambda} \big{\rangle}\Big{\}}\bigg{]}\,.\]

Mixed policy knowing \(\bm{\lambda}_{\bm{B}^{\prime}}^{\star}\) but estimating \(r\).For the sake of illustration, we report the performance of a policy that would have oracle knowledge of \(\bm{\lambda}_{\bm{B}^{\prime}}^{\star}\), which is a finite-dimensional parameter that summarizes \(\nu\), but would ignore \(r\), i.e., the underlying logistic model. That is, this mixed policy would pick, at each round,

\[\max_{a\in\mathcal{A}}\!\Big{\{}\widehat{r}_{\delta,t-1}^{\textsc{wb}}(\bm{x }_{t},a)-\big{\langle}\widehat{\bm{c}}_{\delta,t-1}^{\textsc{bb}}(\bm{x}_{t},a) -\bm{B}^{\prime},\,\bm{\lambda}_{\bm{B}^{\prime}}^{\star}\big{\rangle}\Big{\}}\,,\]

(and would omit the \(\bm{\lambda}\) update in Box B).

To compute (an approximation of) \(\bm{\lambda}_{\bm{B}^{\prime}}^{\star}\), we proceed \(100\) times as described below to compute estimates

\[\bm{\lambda}_{\bm{B}^{\prime}}^{\star,(j)}\,,\qquad\text{which we average out into}\qquad\widehat{\bm{\lambda}}_{\bm{B}^{\prime}}^{\star}=\frac{1}{100}\sum_{j=1}^{100}\bm{ \lambda}_{\bm{B}^{\prime}}^{\star,(j)}\,,\]

where we noted that the numerical values obtained for the \(\bm{\lambda}_{\bm{B}^{\prime}}^{\star,(j)}\) are rather similar. With the notation above, for each \(j\), we sample \(S=10,\!000\) contexts from the distribution \(\nu\), and solve

\[\bm{\lambda}_{\bm{B}^{\prime}}^{\star,(j)}\in\operatorname*{arg\,min}_{\bm{ \lambda}\geqslant\bm{0}}\mathbb{E}_{\bm{X}\sim\widehat{\nu}_{S}^{(j)}}\bigg{[} \!\max_{a\in\mathcal{A}}\!\Big{\{}r(\bm{X},a)-\big{\langle}\bm{c}(\bm{X},a)- \bm{B},\,\bm{\lambda}\big{\rangle}\Big{\}}\bigg{]}\,.\]

(These estimations are independent from the estimations used to compute the opt values, i.e., we use different seeds.)PGD \(\gamma\).We refer to the Box B strategy as PGD \(\gamma\), for which we report the performance for values \(\gamma\in\{0.01,\,0.02,\,0.04,\,0.05,\,0.1\}\).

We also implemented the Box C strategy, with an alternative value of \(M_{T,\delta,k}\), given that the one exhibited based on the theoretical analysis was too conservative, so that no regime break occurred. We resort to a value of the same order of magnitude:

\[M^{\prime}_{T,\delta,k}=c\,d\sqrt{T\ln\bigl{(}T(k+2)\bigr{)}}\,,\]

for a numerical constant \(c\) that we set to 0.01 in our simulations. We call this strategy PGD Adaptive in Figure 1 and Table 1.

### Outcomes of the simulations

We take \(T=10{,}000\) individuals (instead of \(T=1{,}000\) as in the code by Chohlas-Wood et al., 2021) and set initial \(50\) rounds as a warm start for strategies (mostly because of the logistic estimation). We were limited by the computational power (see Appendix G.4) and could only perform \(N=100\) simulations for each (instance of each) strategy. We report averages (strong lines in the graphs) as well as \(\pm 2\) times standard errors (shaded areas in the graphs or values in parentheses in the table).

Graphs.In the first line of graphs in Figure 1, we report the average rewards (over the \(N\) runs) of the strategy under scrutiny as a function of the sample size. More precisely, with obvious notation, we plot

\[t\mapsto\frac{1}{N}\sum_{n=1}^{N}\left(\frac{1}{t}\sum_{s=1}^{t}r_{s}^{(n)} \right)\,.\]

We report the values of \(\textsc{opt}(r,\bm{c},\bm{B})\) and \(\textsc{opt}(r,\bm{c},\bm{B}^{\prime})\) as dashed horizontal lines.

In the second and third lines of graphs in Figure 1, we report the average costs suffered; again, with obvious notation, we plot

\[t\mapsto\frac{1}{N}\sum_{n=1}^{N}\left(\frac{1}{t}\sum_{s=1}^{t}\mathds{1}_{ \{a_{s}^{(n)}=a_{\text{ride}}\}}\right)\qquad\text{and}\qquad t\mapsto\frac{1 }{N}\sum_{n=1}^{N}\left(\frac{1}{t}\sum_{s=1}^{t}\mathds{1}_{\{a_{s}^{(n)}=a_ {\text{weber}}\}}\right)\,.\]

We include the average budget constraints \(B_{\text{ride}}=0.05\) and \(B_{\text{voucher}}=0.20\) as dashed horizontal lines.

Finally, the fourth line of the graphs in Figure 1 reports the fairness costs; we average their absolute values and draw, again with obvious notation,

\[t\mapsto\frac{1}{N}\sum_{n=1}^{N}\left(\frac{1}{4}\sum_{a\in\{a_{\text{ride}},a_{\text{voucher}}\}}\sum_{g\in\{0,1\}}\left|\frac{1}{t}\sum_{s=1}^{t}\Bigl{(} 2\mathds{1}_{\{a_{s}^{(n)}=a\}}\mathds{1}_{\{\textsc{gr}(\bm{x}_{s}^{(n)})=g \}}-\mathds{1}_{\{a_{s}^{(n)}=a\}}\Bigr{)}\right|\right)\,.\]

We include the fairness tolerance \(\tau\) as a dashed horizontal line.

Table.We also report the performance of the strategies at the final round \(T\) in following table, with \(\pm 2\) times standard errors in parentheses. We note that the performance of the mixed policy is poor, in particular in terms of fairness costs, which is why we omitted it on the graphs, to keep them readable.

Comments.When \(\gamma\) is well set, i.e., large enough (see the bound of Lemma 1), the PGD strategies control spending costs thanks to targeting \(\bm{B}^{\prime}\) instead of \(\bm{B}\). We observe that for \(\gamma=0.01\), the PGD strategy does not control the rideshare costs, but for all larger values of \(\gamma\), the associated strategies control the three costs considered. The average rewards achieved are coherent with the average spendings: the smaller the average spendings, the smaller the average rewards. There is some lag: the strategies tuned with \(\gamma\) parameters in \(\{0.04,\,0.05,\,0.1\}\) could use costly actions more. We also observe that fairness costs remain under the target limits.

The mixed policy does not success in controlling the costs, in particular, the fairness costs. While the optimal dual variables \(\bm{\lambda}_{\bm{B}^{\prime}}^{\star}\) summarize the distribution \(\nu\), it seems that \(\bm{\lambda}_{\bm{B}^{\prime}}^{\star}\) has to be used with care: only with the exact values \(r\) and \(\bm{c}\), and not with estimates. On the contrary, the PGD strategies of Box B are more stable, as the dual variables are learned based also on the estimated reward and cost functions.

Figure 1: Performance of the PGD strategies.

Finally, we note that in our experiments, the regimes in PGD Adaptive strategy typically covered range from \(k=0\) (corresponding to \(\gamma_{0}=1/\sqrt{T}=0.01\)) to \(k=2\) (corresponding to \(\gamma_{2}=2^{2}/\sqrt{T}=0.04\)). The PGD Adaptive strategy performs well and is (only) outperformed by the PDG strategy with a fixed \(\gamma=0.02\) (of course difficult to pick in advance). In particular, the PGD Adaptive strategy controls costs, and does so by switching to larger step sizes when needed.

### Computation time and environment

As requested by the NeurIPS checklist, we provide details on the computation time and environment. Our experiments were ran on the following hardware environment: no GPU was required, CPU is \(3.3\) GHz 8 Cores with total of 16 threads, and RAM is \(32\) GB \(4800\) MHz DDR5. We ran \(100\) simulations with \(10\) different seeds on parallel each time. In the setting and for the data described above, the average time spend on each algorithm for a single run was inferior to \(10\) minutes.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Average rewards & Rideshare costs & Voucher costs & Fairness costs \\ \hline \multicolumn{5}{c}{Fairness tolerance \(\tau=10^{-7}\)} \\ \hline \(\texttt{opt}(r,\bm{c},\bm{B})\) & 0.4688 (0.0002) & & & \\ \(\texttt{opt}(r,\bm{c},\bm{B}^{\prime})\) & 0.4648 (0.0002) & & & \\ PGD \(\gamma=0.01\) & 0.4651 (0.0002) & 0.0519 (\(\textless{}0.0001\)) & 0.1984 (\(\textless{}0.0001\)) & 0.0006 (\(\textless{}0.0001\)) \\ PGD \(\gamma=0.02\) & 0.4613 (0.0002) & 0.0492 (\(\textless{}0.0001\)) & 0.1967 (0.0004) & 0.0004 (\(\textless{}0.0001\)) \\ PGD \(\gamma=0.04\) & 0.4571 (0.0002) & 0.0479 (\(\textless{}0.0001\)) & 0.1962 (0.0002) & 0.0004 (\(\textless{}0.0001\)) \\ PGD \(\gamma=0.05\) & 0.4554 (0.0002) & 0.0476 (\(\textless{}0.0001\)) & 0.1961 (0.0002) & 0.0003 (\(\textless{}0.0001\)) \\ PGD \(\gamma=0.1\) & 0.4502 (0.0002) & 0.0471 (\(\textless{}0.0001\)) & 0.196 (0.0002) & 0.0003 (\(\textless{}0.0001\)) \\ PGD Adaptive & 0.4581 (0.0002) & 0.0498 (0.0002) & 0.1971 (0.0002) & 0.0005 (\(\textless{}0.0001\)) \\ Mixed Policy & 0.4402 (0.0056) & 0.0499 (0.0058) & 0.1056 (0.017) & 0.0411 (0.0052) \\ \hline \multicolumn{5}{c}{Fairness tolerance \(\tau=0.025\)} \\ \hline \(\texttt{opt}(r,\bm{c},\bm{B})\) & 0.4731 (0.0002) & & & \\ \(\texttt{opt}(r,\bm{c},\bm{B}^{\prime})\) & 0.4691 (0.0002) & & & \\ PGD \(\gamma=0.01\) & 0.4698 (0.0002) & 0.0518 (0.0002) & 0.1983 (\(\textless{}0.0001\)) & 0.0246 (0.0002) \\ PGD \(\gamma=0.02\) & 0.4663 (0.0002) & 0.0492 (\(\textless{}0.0001\)) & 0.1966 (0.0006) & 0.0242 (0.0002) \\ PGD \(\gamma=0.04\) & 0.4621 (0.0004) & 0.0478 (\(\textless{}0.0001\)) & 0.1958 (0.001) & 0.0223 (0.0004) \\ PGD \(\gamma=0.05\) & 0.4604 (0.0004) & 0.0476 (\(\textless{}0.0001\)) & 0.1955 (0.0014) & 0.0208 (0.0004) \\ PGD \(\gamma=0.1\) & 0.4538 (0.0002) & 0.0471 (\(\textless{}0.0001\)) & 0.1958 (0.0004) & 0.0128 (0.0004) \\ PGD Adaptive & 0.4634 (0.0002) & 0.0499 (0.0002) & 0.1972 (0.0002) & 0.0228 (0.0002) \\ Mixed Policy & 0.4466 (0.0054) & 0.0566 (0.0052) & 0.1053 (0.0164) & 0.0473 (0.0054) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Performance of the strategies as computed at the final round \(T\).