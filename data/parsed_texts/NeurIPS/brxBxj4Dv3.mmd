# NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise

 Zhonghao Wang\({}^{1}\), Danyu Sun\({}^{1}\), Sheng Zhou\({}^{1}\)1, Haobo Wang\({}^{1}\),

**Jiapei Fan\({}^{2}\), Longtao Huang\({}^{2}\), Jiajun Bu\({}^{1}\)**

\({}^{1}\)Zhejiang Key Laboratory of Accessible Perception and Intelligent Systems,

Collage of Computer Science, Zhejiang University \({}^{2}\)Alibaba Group

{wangzhonghao, zhousheng_zju, wanghaobo, bjj}@zju.edu.cn,

danyu.21@intl.zju.edu.cn,

{jiapei.fjp, kaiyang.hlt}@alibaba-inc.com

Sheng Zhou is the Corresponding Author

###### Abstract

Graph Neural Networks (GNNs) exhibit strong potential in node classification tasks through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, _label noise_ is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.

## 1 Introduction

Many complex real-world systems can be represented as graph-structured data, including the citation network [19], biological networks [7], traffic networks [6], and social networks [9]. Graph Neural Networks (GNNs) have demonstrated substantial effectiveness in modeling graph data through a message-passing process that aggregates information from neighboring nodes [5]. Among the numerous applications of GNNs, node classification is the most thoroughly studied task, where GNNs are trained with the explicit assistance of semi-supervised node labels [1].

Although GNNs have achieved success, their performance in semi-supervised graph learning tasks is highly dependent on precise node labels, which are difficult to obtain in real-world scenarios [1]. For instance, in online social networks, the process of manually labeling millions of users is costly, and the labels often depend on unreliable user input [18]. Furthermore, graph data is vulnerable to adversarial label-flipping attacks [31]. Consequently, label noise is widespread in graph data. Research has demonstrated that label noise can significantly reduce the generalizability of machine learning models on computer vision and natural language processing scenarios [21]. In GNNs, themessage-passing mechanism can further exacerbate this negative impact by propagating incorrect supervision from mislabeled nodes throughout the graph, leading to substantial results [18].

To address this challenge, an intuitive solution is to draw on the success of previous Learning with Label Noise (LLN) strategies and apply them to GNNs. However, these approaches are not always applicable to graph learning tasks due to the non-i.i.d nature, sparse labeling of graph data, and message-passing mechanism of GNNs[1]. All these factors make GNNs vulnerable to label noise and hinder traditional LLN methods from being directly applied to graph learning tasks.

In recent years, researchers have developed a series of Graph Neural Networks under Label Noise (GLN) methods to achieve robust graph learning in the presence of label noise. These methods succeeded greatly by adopting Loss regularization [14; 31; 11; 2], Robust training strategy [24], Graph structure augmentation [1; 18; 32], and contrastive learning [30; 10]. Despite the researcher's claim of the robustness of their proposed GLN methods, the comprehensive benchmark for evaluating these methods remains absent, bringing out the following problems: _1) Existing works utilize different datasets, noise types, rates, data splitting, and processing strategies, which makes it challenging to achieve a fair comparison. 2) Existing work lacks an empirical understanding regarding the impact of the graph structure itself on label noise--a critical distinction between LLN and GLN. 3) No existing work has thoroughly examined the applicability of traditional LLN methods to graph learning problems._ These problems hinder us from gaining a comprehensive understanding of the progress in this field.

In this research, we present NoisyGL -- the first comprehensive benchmark for graph neural networks under label noise. Our benchmark includes seventeen representative methods: ten GLN methods to assess their effectiveness and robustness on graphs with noisy labels, and seven LLN methods to evaluate their applicability in graph learning tasks. We employ standardized backbones and APIs, consistent data splitting, and processing strategies to ensure a fair comparison and allow users to construct their models or datasets with minimal effort easily. Besides performance and robustness evaluations, our benchmark supports multidimensional analysis, enabling researchers to explore the time efficiency of different methods and understand the influence of graph structure on the handling of label noise.

Through extensive experiments, we have the following key findings: 1) Simply applying LLN methods can't significantly improve GNNs' robustness to label noise. 2) Existing GLN methods can alleviate label noise in their applicable scenarios. 3) Pair noise is the most harmful label noise due to its misleading impact. 4) Negative effects of label noise can spread through the graph structure, especially in sparse graphs. 5) GLN methods involving graph structure augmentation effectively mitigate the spread effect of label noise. Our contributions can be summarized as follows:

* **Perform an in-depth review of the current research challenge.** In our study, we revisited and scrutinized the entire progression of GLN. We discovered that the lack of a thorough benchmark in this domain significantly hinders a deeper understanding.
* **Provide a comprehensive and user-friendly benchmark.** We present NoisyGL, the first comprehensive benchmark for GLN. In this benchmark, we have selected and implemented a variety of LLN and GLN methods and evaluated them across eight commonly used datasets under uniform experimental settings. Our benchmark library is available to the public on GitHub, intending to aid future research efforts.
* **Highlight the key findings and future opportunities.** Our study has resulted in several crucial findings that have the potential to greatly advance this field.

## 2 Formulations and Background

**Notations.** Consider a graph denoted by \(\mathcal{G}=\{\mathcal{V},\mathcal{E}\}\), where \(\mathcal{V}\) is the set of \(N\) nodes and \(\mathcal{E}\) is the set of edges. \(\mathbf{A}\in\mathbb{R}^{N\times N}\) is the adjacency matrix and \(\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},\cdots\mathbf{x}_{N}]\in\mathbb{R}^ {N\times d}\) denotes node features matrix with dimension \(d\). Each node has a ground truth label, the set of which is denoted by \(\mathcal{Y}=\{y_{1}^{*},y_{2}^{*},\cdots,y_{N}^{*}\}\). We focus on the semi-supervised node classification problem, where only a small set of nodes \(\mathcal{V}_{L}\) has assigned labels for training procedure, denoted as \(\mathcal{Y}_{L}=\{y_{1}^{*},y_{2}^{*},\cdots,y_{l}^{*}\}\), where \(l\) is the number of labeled nodes. The rest of them are unlabeled nodes, denoted as \(\mathcal{V}_{U}=\mathcal{V}-\mathcal{V}_{L}\). Given \(\mathbf{X}\) and \(\mathbf{A}\), the goal of node classification is to train a classifier \(f_{\theta}:(\mathbf{X},\mathbf{A})\rightarrow\hat{\mathbf{Y}}^{N\times c}=\{ \hat{\mathbf{y}}_{1},\hat{\mathbf{y}}_{2},\cdots,\hat{\mathbf{y}}_{N}\}\) by minimizing \(\mathcal{L}(f_{\theta}(\mathbf{X},\mathbf{A}),\mathcal{Y}_{L})\), where \(c\) is the number of classes, \(\mathcal{L}\) is a loss function that measures the difference between the predicted labels and the ground truth labels. Typically \(f_{\theta}\) is a well-designed Graph Neural Network(GNN). In this way, according to the Empirical Risk Minimization (ERM) principle, the well-trained classifier \(f_{\theta^{*}}\) can generalize on unseen data \(\mathcal{V}_{U}\).

However, the accessible labels \(\mathcal{Y}_{L}\) can be corrupted by label noise in the real world, reducing the generalization ability of \(f_{\theta^{*}}\). We denote the observed noisy labels as \(\mathcal{Y}_{N}=\{\widetilde{y}_{1},\widetilde{y}_{2},\cdots\widetilde{y}_{l}\}\) and \(\mathcal{Y}_{L}\) is their corresponding true labels. Typically, we consider two types of label noise, and here are their definitions:

**Uniform noise [21]** or symmetric noise assumes that the true label has a probability of \(\epsilon\in(0,1)\) to be uniformly flipped to another class. Formally, for \(\forall_{j\neq i}\), we have \(p(\widetilde{y}=j|y^{*}=i)=\frac{\epsilon}{c-1}\), where \(c\) represents the number of classes.

**Pair noise [28]** or pair flipping. Assumes that the true label can only be flipped to its corresponding pair class with a probability \(\epsilon\). Formally, we have \(p(\widetilde{y}=y_{p}|y^{*}=y_{c})=\epsilon\) and \(\forall_{j\neq y_{p},y_{c}}p(\widetilde{y}=j|y^{*}=y_{c})=0\), where \(y_{p}\) is the corresponding pair class of \(y_{c}\).

The transition patterns of pair noise and uniform noise are illustrated in the Appendix. B.1. It is important to note that these noise types assume that the transition probability depends only on the observed and true labels, and is independent of instances. In real-world scenarios, label noise can be much more complex. We focus on the most frequently used noise types, leaving the investigation of the other noise types for future studies.

## 3 Benchmark Design

### Datasets and Implementations

**Datasets.** We selected 8 node classification datasets widely used among different studies on graph label noise. These selected datasets come from different domains and exhibit different characteristics, enabling us to evaluate the generalizability of existing methods across a range of scenarios. Specifically, we use three classic citation datasets [19], namely Cora, Citeseer, Pubmed, and one author collaboration network DBLP [15], as well as two representative product co-purchase network datasets Amazon-Computers and Amazon-Photo [20]. Additionally, to analyze the model performance on heterophilous graphs, we include two representative social media network datasets BlogCatalog and Flickr [26]. We present detailed introductions to these datasets in Appendix C.1.

The splitting methods for training, validation, and test sets of the same dataset in different tasks are not always consistent. This necessitates a unified dataset splitting in our work to achieve fair comparisons. For three citation datasets, i.e. Cora Citeseer and Pubmed, we follow the most commonly used split in [31; 24; 11; 8]. For the author collaboration network DBLP, we follow the split as [1; 10]. For two co-purchase datasets Amazon-Computers and Amazon-Photo, we follow the split as [24]. For the social network datasets BlogCatalog and Flickr, we use the same split as [18]. In this study, we assume that the labels of both the training set and validation set have been affected by label noise. A clean test set is used to evaluate the model's performance.

**Label Corruption.** In each experiment, we first generate a label transition probability matrix based on the given noise rate and the definition of noise. Then, for each clean label in the training and validation set, we draw a noisy label from a categorical distribution according to its corresponding transition probability. These noisy labels are used in the subsequent training procedure.

**Implementations** We consider a collection of state-of-the-art GLN algorithms, including NRGNN [1], RTGNN [18], CP [31], D-GNN [14], RCNGLN [32], CLNode [24], PIGNN [2], UnionNET [11], CGNN [30], and CRGNN [10]; and a set of well-designed LLN methods, including two loss correction methods Forward and Backward correction [16], two robust loss functions APL [13] and SCE [22], two multi-network learning methods Coteaching [4] and JoCoR [23], and one noise adaptation layer method S-model [3]. We have rigorously reproduced all methods according to their papers and source code. More details about these algorithms and implementations can be found in the Appendix C.2.

### Research Questions

In this study, we aim to answer the following research questions:

**RQ1: Can LLN methods be applied directly to graph learning tasks?**

**Motivation.** While recent studies have suggested that applying traditional Learning with Label Noise (LLN) methods directly to graph learning tasks may not yield the best results [1], a comprehensive analysis of this issue is still lacking. We aim to investigate the suitability of existing LLN methods for graph learning and understand the underlying reasons. By tackling this question, we can gain a clearer insight into the unique challenges posed by graph label noise and identify which LLN techniques remain effective in graph learning contexts.

**Experiment Design.** To investigate this question, we select various LLN methods referenced in the Section 3.1 and implement them on the GCN[8] backbone using unified hyper-parameters. We then perform node classification experiments on the most frequently used datasets, evaluating their effectiveness under various types and levels of label noise. For each method and dataset, we record the mean accuracy metrics and standard deviations over 10 runs. Data splitting is performed randomly with a consistent ratio. By comparing the performance of these LLN methods with GCN, we determine whether they enhance the robustness of the backbone.

**RQ2: How much progress has been made by existing GLN methods?**

**Motivation.** While numerous GLN methods have been introduced in the literature, previous studies have used varied datasets, data splits, and preprocessing techniques, complicating the fair comparison of these methods' performance. Furthermore, we notice that the majority of existing approaches have been tested on homophily graphs, leading to concerns about their relevance to heterophily graphs, which are also commonly encountered in practice. By investigating this issue, we seek to determine if current GLN methods effectively address graph label noise and to identify their shortcomings.

**Experiment Design.** To address this question, we select and implement many advanced GLN methods as described in Section 3.1. We then assess the performance of these methods using uniform datasets and experimental settings. For each method and dataset, we record the mean test accuracy and the standard deviation across 10 runs. Since many of these GLN methods use GCN as their foundation, we compare their performance with GCN to evaluate their robustness to label noise.

**RQ3: Are existing GLN methods computationally efficient?**

**Motivation.** The efficiency of GNNs in terms of computation is crucial for their use in real-world applications, and considering label noise can lead to higher computational expenses. While previous research has deeply investigated the accuracy, generalization, and robustness of the GLN method, it has failed to address the computational efficiency of these approaches. Therefore, it is important to evaluate the computational efficiency of different methods.

**Experiment design.** To answer this question, we recorded the runtime and test accuracy of various methods on different datasets under \(30\%\) uniform noise. Specifically, for each method, we conducted 10 experiments for each method on each dataset. In each experiment, we measured the time when the model achieved the best accuracy on the validation set, considering it as the total runtime for that method. Through these experiments, we can assess whether the GLN methods strike a balance between computational efficiency and test accuracy.

**RQ4: Are existing GLN methods sensitive to noise rate?**

**Motivation.** Previous studies utilize different noise rates, making it difficult to fairly compare the performance of various methods. Therefore, it is essential to assess different methods using a consistent set of noise rates and to verify if existing GLN methods maintain stable performance across different noise levels.

**Experiment Design.** To investigate this question, we assess the performance of several GLN methods over varying noise levels using the same datasets and noise types. Specifically, we introduce label contamination with pair noise and uniform noise at rates of \(10\%\), \(20\%\), \(30\%\), \(40\%\), and \(50\%\), while using clean labels as a baseline. We then train the GLN methods on these datasets following the experimental settings described in RQ2 and record the mean test accuracy and standard deviation from 10 runs. This evaluation allows us to determine the robustness of each method.

**RQ5: Are existing GLN methods robust to different types of label noise?**

[MISSING_PAGE_FAIL:5]

[MISSING_PAGE_FAIL:6]

GCN on the Cora dataset and an astounding 2945.8 times longer on the DBLP dataset. Moreover, RNCGLN runs out of memory on the PubMed dataset, underscoring its inefficiency in memory usage. On the other hand, while the NRGNN method also consumes more time than GCN, it achieves a reasonable trade-off between performance and computational efficiency across both datasets. Detailed experimental results can be found in Appendix A.

#### 4 (RQ4) Most GLN methods can't ensure a high performance under severe noise.

Figure 2 depicts the performance of different GLN methods on the DBLP dataset under various types and levels of label noise. We observe that, in general, as the noise level increases, the test accuracy of each method decreases. This decrease is most pronounced for pair noise, where the test accuracy of all methods almost halves at \(50\%\) pair noise. Additionally, we noticed that RTGNN maintains relatively stable performance under uniform noise. Moreover, two methods, NRGNN and PIGNN, show better results than the baseline GNN over different noise levels and types on the DBLP dataset. Detailed experimental results are provided in Appendix A.

#### 5 (RQ5) Pair noise is more harmful to graph learning.

In our experiments, we consistently observed that pair noise poses the most significant threat to the generalization ability of models. We have an explanation for this finding: Recall the definition provided in Section 2. For uniform noise, the true label has a chance to flip to any other class, incorrect parameter updates caused by mislabeled instances can be partially compensated by other mislabeled instances. Pair noise, however, restricts the flipping class to a specific pair class. For classifiers, this type of pair flipping can be more misleading. After being fully trained, the classifier is more likely to over-fit the pair class. This becomes particularly harmful when node features propagate through message-passing mechanisms, which can lead to a more similar embedding within local neighbors and thus make them have a similar probability of being misclassified to their corresponding pair class. To validate our hypothesis, we conducted an empirical study. Specifically, we recorded the misleading train accuracy of five methods (including 1 GCN baseline, 2 LLN and 2 GLN) on four datasets under \(50\%\) pair and uniform noise. Here the misleading train accuracy represents the model's accuracy in making incorrect predictions to the misclassified classes. The experimental results (shown in Table 1) demonstrate that pair noise has the greatest impact, leading the model to overfit predict the mislabeled classes across different methods and datasets. Detailed experimental results are available in Appendix A.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Dataset (Avg. \# Degree) & Noise type & GCN & JCoRoR & APL & NRGNN & CLNode \\ \hline \multirow{3}{*}{**Cora (3.90)**} & \(50\%\) **Uniform noise** & \(78.49\pm 9.87\) & \(70.82\pm 3.87\) & \(77.29\pm 12.35\) & \(16.16\pm 7.00\) & \(68.46\pm 17.64\) \\  & \(50\%\) **Pair Noise** & \(94.53\pm 5.51\) & \(84.90\pm 3.49\) & \(92.93\pm 6.46\) & \(64.05\pm 12.02\) & \(88.03\pm 6.22\) \\  & \(50\%\) **Uniform noise** & \(98.78\pm 1.15\) & \(82.64\pm 5.51\) & \(93.21\pm 5.13\) & \(32.74\pm 6.88\) & \(79.54\pm 15.19\) \\  & \(50\%\) **Pair Noise** & \(98.54\pm 2.46\) & \(87.54\pm 5.49\) & \(96.19\pm 3.85\) & \(60.74\pm 12.91\) & \(82.23\pm 8.78\) \\
**A-Computers (35.76)** & \(50\%\) **Uniform noise** & \(19.68\pm 6.59\) & \(19.02\pm 5.02\) & \(23.49\pm 6.44\) & \(13.36\pm 2.89\) & \(15.27\pm 5.00\) \\  & \(50\%\) **Pair Noise** & \(75.30\pm 7.91\) & \(66.85\pm 5.91\) & \(72.42\pm 9.90\) & \(45.17\pm 8.59\) & \(64.34\pm 11.66\) \\
**Blogcatalog (66.11)** & \(50\%\) **Uniform noise** & \(29.13\pm 9.95\) & \(27.73\pm 13.31\) & \(24.23\pm 16.14\) & \(7.93\pm 2.97\) & \(31.67\pm 9.33\) \\  & \(50\%\) **Pair Noise** & \(72.50\pm 15.54\) & \(64.25\pm 7.95\) & \(73.39\pm 13.65\) & \(56.92\pm 10.29\) & \(64.70\pm 11.64\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Misleading train accuracy of different methods under pair and uniform noise (10 Runs)

Figure 4: Time consumption and Test accuracy of different GLN methods on Cora and DBLP under \(30\%\) uniform noise (10 Runs).

(RQ6) Graph structure can amplify the negative effect of label noise.From the experimental results in Table 2, we observed that in the sparse graph (Cora), AUIS and AUU exhibit a significant decrease compared to AUCS. Taking the performance of GCN on the Cora dataset as an example, this decrease is \(36.17\%\) and \(10.85\%\), respectively. These results highlight the importance of proper supervision of neighboring nodes with correct annotations. Proper supervision of neighboring nodes with correct annotations significantly improves the classification accuracy of unlabeled nodes, while incorrect supervision of neighboring nodes severely reduces the classification accuracy of these nodes, even worse than when no neighborhood supervision is applied. Besides, our investigation also highlights the effectiveness of graph structure augmentation methods in mitigating the spread effect of label noise. According to Table 2, three methods, i.e. NRGNN, RTGNN, and RNCGLN, exhibit the smallest decrease in AUIS compared to AUCS and AUU among all methods. This indicates that they can effectively mitigate the spread effect of label noise. This phenomenon is even more pronounced in sparse graphs like Cora. One possible explanation can be easily drawn from the previous findings: The additional graph structure learning measures they adopted can lead to a denser graph structure used for predictions during the up-sampling process. Consequently, the classifier can rely on more references from the neighborhood, reducing its dependence on a small number of incorrectly labeled samples. Detailed experimental results are available in Appendix A.

(RQ6) Sparse graphs are more vulnerable to the spread effect of label noise.From Table 2 we see that the propagation effect of label noise can be very severe on sparse graphs with a relatively low average degree, like Cora, Citeseer, Pubmed, and DBLP, but not on dense graphs such as Amazon-Computers, Amazon-Photos, Blogcatalog and Flickr. The explanation for this observation is that unlabeled nodes on sparse graphs usually have only a limited number of annotated nodes in their neighborhood available for training. The prediction results of unlabeled nodes rely heavily on the annotated nodes in their neighborhood. However, if these nodes are incorrectly labeled, it will lead to erroneous learning of the embedding for the unlabeled nodes. In contrast, for dense graphs, the neighborhood of unlabeled nodes contains many annotated nodes that can serve as references. As a result, the classifier model is more likely to find correct supervision from these annotated nodes. This hypothesis is further supported by empirical evidence from Table 1, where we observe that compared to sparse graphs (such as Cora, Citeseer, and Pubmed), GCN is less susceptible to misleading on dense graphs like Blogcatalog and Amazon-Computers with a high average degree. Detailed experimental results are available in Appendix A.

## 5 Future directions

Based on the experimental results and analysis, we present several potential directions for the further development of the GLN.

**Designing widely applicable GLN approaches.** Our observations in finding (r) reveal that most existing GLN methods cannot ensure consistently high performance across all scenarios. To address this problem, we need to explore three key questions: 1) What are the common properties of different graph datasets? 2) How can these common properties be utilized to enhance the robustness of GNNs against label noise? Our finding (r) indicated that enhancing graph structures can reduce the spread of label noise in graphs with varying densities, leading to the third question: 3) If identifying common properties is challenging, can we unify these features through data augmentation?

**Designing GLN approaches for various graph learning tasks.** Previous studies on GLN have predominantly focused on node classification tasks. However, the field of graph learning includes

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline
**Dataset (Avg. \# Degree)** & Records & GCN & NRGNN & RTGNN & CP & CLNode & RNCGLN \\ \hline \multirow{3}{*}{**Cora (3.90)**} & **AUCS** & \(80.76\pm 2.95\) & \(83.11\pm 3.16\) & \(75.53\pm 4.80\) & \(80.85\pm 3.46\) & \(76.77\pm 3.30\) & \(77.06\pm 3.30\) \\  & **AUU** & \(71.99\pm 3.44\) & \(81.33\pm 2.25\) & \(73.44\pm 4.95\) & \(72.32\pm 4.45\) & \(67.11\pm 5.11\) & \(75.36\pm 3.33\) \\  & **AUUS** & \(51.55\pm 6.53\) & \(78.81\pm 5.94\) & \(69.50\pm 8.06\) & \(51.46\pm 12.99\) & \(43.86\pm 7.48\) & \(72.92\pm 4.66\) \\ \cline{2-8}  & **AUCS** & \(92.21\pm 2.44\) & \(85.62\pm 2.96\) & \(89.98\pm 2.10\) & \(90.74\pm 2.98\) & \(93.08\pm 1.97\) & \(75.71\pm 6.59\) \\ \cline{2-8}  & **AUU** & \(89.76\pm 2.84\) & \(84.29\pm 2.79\) & \(89.19\pm 2.13\) & \(88.85\pm 3.20\) & \(91.15\pm 2.33\) & \(74.40\pm 6.73\) \\ \cline{2-8}  & **AUIS** & \(87.01\pm 4.72\) & \(82.46\pm 5.86\) & \(88.84\pm 4.76\) & \(86.34\pm 3.99\) & \(88.71\pm 3.40\) & \(71.08\pm 7.89\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: AUCS, AUU, AUIS of different methods on Cora and Amazon-Photos under \(30\%\) uniform noise (10 Runs)other important tasks such as link prediction, edge property prediction, and graph classification. However, there is limited work on graph classification [27] and graph transfer learning [29] in the presence of label noise. Overall, research in other areas of graph learning, beyond node classification, is still in its early stages, and warrants further attention and exploration.

**Considering other types of label noise in graph learning.** Previous studies of GLN have mainly focused on pair noise and uniform noise. These noise types are instance-independent, assuming that the label corruption process is conditionally independent of node features when the true labels are given [21]. However, there exists another type of label noise--instance-dependent label noise--that is more realistic. In this case, the corruption probability depends on both the node features and the observed labels. However, none of the previous GLN studies have investigated this problem. Furthermore, unlike traditional machine learning tasks, graph learning involves additional graph structure, so the label noise model on graphs may also depend on graph topology. These issues are worth investigating, as they are more likely to occur in real-world scenarios.

## 6 Conclusions and Future work

In this research, we present NoisyGL, the first comprehensive benchmark designed for Graph Neural Networks under Label Noise (GLN) conditions. NoisyGL includes 7 prominent LLN and 10 GLN methods, allowing the community to fairly evaluate their effectiveness and robustness across various datasets. By using standardized backbones and APIs, consistent data splitting, and processing strategies, NoisyGL ensures a fair comparison and allows users to easily construct their own models or datasets with minimal effort. From this benchmark, we extract several key insights that are highly promising for the progression of this evolving field: Firstly, we point out that simply applying LLN methods cannot significantly improve the robustness of GNNs to label noise. Secondly, we found that existing GLN methods can alleviate label noise in their own applicable scenarios. In particular, pair noise emerges as the most harmful label noise due to its misleading effects. Finally, we discovered that negative effects of label noise can spread through the graph structure, especially in sparse graphs, and graph structure augmentation proves to be effective in mitigating the spread effect of label noise.

**Border Impacts and Limitations.** As NoisyGL provides a comprehensive benchmark for GNNs under label noise, we aim to attract more attention on the quality of graph data from the graph learning community, including the topology, node attributes and labels. However, NoisyGL also has some limitations that we aim to address in future work. Firstly, we aim to include a broader range of datasets to evaluate methods in different scenarios. While our current datasets are predominantly homogeneous graphs, we recognize that most GLN methods struggle with heterogeneous graphs, such as the Flickr network. Secondly, we hope to implement more GLN methods to gain a deeper understanding of the progress in the field. We will continuously update our repository to keep track of the latest advances in the field. We are also open to any suggestions and contributions that will improve the usability and effectiveness of our benchmark.

## 7 Acknowledgement

This work is supported by the National Natural Science Foundation of China (62106221, 62372408), Zhejiang Provincial Natural Science Foundation of China (Grant No: LTGG23F030005), Ningbo Natural Science Foundation (Grant No: 2022J183).

## References

* [1] Enyan Dai, Charu Aggarwal, and Suhang Wang. Nrgnn: Learning a label noise resistant graph neural network on sparsely and noisily labeled graphs. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pages 227-236, 2021.
* [2] Xuefeng Du, Tian Bian, Yu Rong, Bo Han, Tongliang Liu, Tingyang Xu, Wenbing Huang, Yixuan Li, and Junzhou Huang. Noise-robust graph learning by estimating and leveraging pairwise interactions. _arXiv preprint arXiv:2106.07451_, 2021.
* [3] Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation layer. In _International conference on learning representations_, 2022.
* [4] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _Advances in neural information processing systems_, 31, 2018.
* [5] Adrian Javaloy, Pablo Sanchez-Martin, Amit Levi, and Isabel Valera. Learnable graph convolutional attention networks. _arXiv preprint arXiv:2211.11853_, 2022.
* [6] Weiwei Jiang and Jiayun Luo. Graph neural network for traffic forecasting: A survey. _Expert Systems with Applications_, 207:117921, 2022.
* [7] Jeremy Kawahara, Colin J Brown, Steven P Miller, Brian G Booth, Vann Chau, Ruth E Grunau, Jill G Zwicker, and Ghassan Hamarneh. Brainnetcnn: Convolutional neural networks for brain networks; towards predicting neurodevelopment. _NeuroImage_, 146:1038-1049, 2017.
* [8] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [9] Jure Leskovec and Julian Mcauley. Learning to discover social circles in ego networks. _Advances in neural information processing systems_, 25, 2012.
* [10] Xianxian Li, Qiyu Li, Haodong Qian, Jinyan Wang, et al. Contrastive learning of graphs under label noise. _Neural Networks_, 172:106113, 2024.
* [11] Yayong Li, Jie Yin, and Ling Chen. Unified robust training for graph neural networks against label noise. In _Pacific-Asia Conference on Knowledge Discovery and Data Mining_, pages 528-540. Springer, 2021.
* [12] Yuwen Li, Miao Xiong, and Bryan Hooi. Graphcleaner: Detecting mislabelled samples in popular graph learning benchmarks. In _International Conference on Machine Learning_, pages 20195-20209. PMLR, 2023.
* [13] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In _International conference on machine learning_, pages 6543-6553. PMLR, 2020.
* [14] Hoang NT, Choong Jun Jin, and Tsuyoshi Murata. Learning graph neural networks with noisy labels. _arXiv preprint arXiv:1905.01591_, 2019.
* [15] Shirui Pan, Jia Wu, Xingquan Zhu, Chengqi Zhang, and Yang Wang. Tri-party deep network representation. In _International Joint Conference on Artificial Intelligence 2016_, pages 1895-1901. Association for the Advancement of Artificial Intelligence (AAAI), 2016.
* [16] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1944-1952, 2017.
* [17] Oleg Platonov, Denis Kuzenedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A critical look at the evaluation of gnns under heterophily: Are we really making progress? _arXiv preprint arXiv:2302.11640_, 2023.
* [18] Siyi Qian, Haochao Ying, Renjun Hu, Jingbo Zhou, Jintai Chen, Danny Z Chen, and Jian Wu. Robust training of graph neural networks via noise governance. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 607-615, 2023.
* [19] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.

* [20] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of graph neural network evaluation. _arXiv preprint arXiv:1811.05868_, 2018.
* [21] Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. _IEEE transactions on neural networks and learning systems_, 2022.
* [22] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 322-330, 2019.
* [23] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint training method with co-regularization. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 13726-13735, 2020.
* [24] Xiaowen Wei, Xiuwen Gong, Yibing Zhan, Bo Du, Yong Luo, and Wenbin Hu. Clnode: Curriculum learning for node classification. In _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, pages 670-678, 2023.
* [25] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [26] Renchi Yang, Jieming Shi, Xiaokui Xiao, Yin Yang, Juncheng Liu, and Sourav S Bhowmick. Scaling attributed network embedding to massive graphs. _Proceedings of the VLDB Endowment_, 14(1):37-49, 2020.
* [27] Nan Yin, Li Shen, Mengzhu Wang, Xiao Luo, Zhigang Luo, and Dacheng Tao. Omg: Towards effective graph classification against label noise. _IEEE Transactions on Knowledge and Data Engineering_, 35(12):12873-12886, 2023.
* [28] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In _International conference on machine learning_, pages 7164-7173. PMLR, 2019.
* [29] Jingyang Yuan, Xiao Luo, Yifang Qin, Zhengyang Mao, Wei Ju, and Ming Zhang. Alex: Towards effective graph transfer learning with noisy labels. In _Proceedings of the 31st ACM international conference on multimedia_, pages 3647-3656, 2023.
* [30] Jingyang Yuan, Xiao Luo, Yifang Qin, Yusheng Zhao, Wei Ju, and Ming Zhang. Learning on graphs under label noise. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.
* [31] Mengmei Zhang, Linmei Hu, Chuan Shi, and Xiao Wang. Adversarial label-flipping attack and defense for graph neural networks. In _2020 IEEE International Conference on Data Mining (ICDM)_, pages 791-800. IEEE, 2020.
* [32] Yonghua Zhu, Lei Feng, Zhenyun Deng, Yang Chen, Robert Amor, and Michael Witbrock. Robust node classification on graph data with graph and label noise. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 17220-17227, Mar. 2024.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] See Section 6. 3. Did you discuss any potential negative societal impacts of your work? [No] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)...

1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix D 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 3, Appendix D and our project URL. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] See Section 4 and AppendixA. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See Appendix D.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] See Appendix E. 2. Did you mention the license of the assets? [Yes] See Appendix E. 3. Did you include any new assets either in the supplemental material or as a URL? [No] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See Appendix E. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]

[MISSING_PAGE_EMPTY:13]

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Dataset & Noise type & GCN & S-model & Cotenching & JCoR & APL & SCE & Forward & Backward \\ \hline \multirow{9}{*}{**A-Photos**} & \(0\%\)**clean** & \(91.82\pm 0.09\) & \(92.05\pm 0.51\) & \(89.66\pm 2.21\) & \(77.14\pm 7.63\) & \(89.92\pm 1.74\) & \(91.04\pm 1.24\) & \(69.14\pm 12.01\) & \(68.34\pm 34.09\) \\  & \(100\%\) **pair** & \(88.83\pm 1.42\) & \(89.71\pm 1.50\) & \(89.47\pm 1.80\) & \(74.03\pm 7.45\) & \(88.08\pm 2.07\) & \(89.78\pm 1.41\) & \(68.64\pm 14.55\) & \(50.42\pm 39.364\) \\  & \(20\%\) **pair** & \(85.74\pm 2.86\) & \(85.47\pm 2.91\) & \(86.73\pm 3.25\) & \(76.04\pm 8.12\) & \(87.07\pm 2.30\) & \(86.74\pm 3.11\) & \(69.64\pm 16.08\) & \(61.26\pm 23.32\) \\  & \(30\%\) **pair** & \(79.26\pm 4.79\) & \(80.06\pm 6.25\) & \(74.56\pm 7.56\) & \(66.88\pm 8.62\) & \(83.93\pm 6.90\) & \(81.57\pm 4.96\) & \(67.29\pm 11.43\) & \(51.90\pm 25.85\) \\  & \(40\%\) **pair** & \(64.83\pm 6.28\) & \(60.56\pm 5.80\) & \(62.22\pm 1.18\) & \(72.38\pm 3.82\) & \(65.89\pm 11.22\) & \(64.30\pm 5.90\) & \(54.24\pm 10.74\) & \(41.41\pm 19.25\) \\  & \(50\%\) **pair** & \(44.87\pm 1.49\) & \(4.21\pm 14.45\) & \(41.36\pm 15.48\) & \(42.11\pm 13.98\) & \(45.28\pm 14.30\) & \(44.26\pm 13.12\) & \(42.07\pm 15.02\) & \(27.34\pm 18.84\) \\  & \(10\%\) **uniform** & \(89.42\pm 1.61\) & \(96.64\pm 1.48\) & \(89.91\pm 1.32\) & \(75.82\pm 7.95\) & \(89.52\pm 1.51\) & \(90.24\pm 0.91\) & \(77.44\pm 1.42\) & \(70.45\pm 34.88\) \\
**A-Photos** & \(20\%\) **uniform** & \(88.02\pm 1.99\) & \(89.53\pm 2.39\) & \(88.61\pm 2.08\) & \(76.59\pm 1.90\) & \(88.40\pm 1.17\) & \(89.11\pm 1.14\) & \(64.41\pm 15.05\) & \(62.42\pm 31.57\) \\  & \(30\%\) **uniform** & \(84.86\pm 3.27\) & \(83.42\pm 4.17\) & \(82.78\pm 7.85\) & \(72.82\pm 8.42\) & \(86.17\pm 3.17\) & \(85.85\pm 4.42\) & \(65.13\pm 11.73\) & \(59.57\pm 39.25\) \\  & \(40\%\) **uniform** & \(80.02\pm 4.79\) & \(81.37\pm 6.43\) & \(80.47\pm 5.60\) & \(71.60\pm 8.55\) & \(80.38\pm 6.35\) & \(83.03\pm 6.75\) & \(65.63\pm 14.04\) & \(46.17\pm 26.38\) \\  & \(50\%\) **uniform** & \(78.15\pm 5.60\) & \(76.35\pm 5.90\) & \(73.10\pm 7.06\) & \(64.04\pm 10.51\) & \(76.17\pm 6.26\) & \(79.18\pm 4.07\) & \(53.00\pm 12.74\) & \(24.97\pm 15.83\) \\  & \(10\%\) **random** & \(88.05\pm 1.46\) & \(87.42\pm 3.45\) & \(88.20\pm 1.14\) & \(77.06\pm 4.26\) & \(88.88\pm 11.17\) & \(87.42\pm 20.27\) & \(72.67\pm 12.38\) & \(74.74\pm 22.32\) \\  & \(20\%\) **random** & \(86.82\pm 1.58\) & \(84.68\pm 6.86\) & \(86.72\pm 1.53\) & \(80.27\pm 7.36\) & \(88.24\pm 9.00\) & \(86.19\pm 2.79\) & \(61.02\pm 15.40\) & \(56.67\pm 33.49\) \\  & \(30\%\) **random** & \(82.23\pm 4.37\) & \(81.22\pm 3.38\) & \(78.74\pm 24.47\) & \(79.51\pm 7.69\) & \(85.27\pm 2.51\) & \(82.83\pm 5.31\) & \(63.60\pm 14.20\) & \(54.90\pm 33.14\) \\  & \(10\%\) **random** & \(76.32\pm 6.09\) & \(78.89\pm 3.95\) & \(76.05\pm 12.73\) & \(67.10\pm 10.96\) & \(80.74\pm 5.22\) & \(80.12\pm 10.41\) & \(61.81\pm 13.76\) & \(45.52\pm 25.57\) \\  & \(50\%\) **random** & \(70.69\pm 6.24\) & \(77.62\pm 6.32\) & \(68.05\pm 11.26\) & \(65.34\pm 8.04\) & \(69.47\pm 11.71\) & \(77.50\pm 5.23\) & \(79.77\pm 8.19\) & \(72.13\pm 15.82\) \\ \hline \multirow{9}{*}{**DRLP**} & \(0\%\)**clean** & \(77.03\pm 0.35\) & \(77.11\pm 0.29\) & \(71.62\pm 2.47\) & \(76.15\pm 0.22\) & \(77.06\pm 0.44\) & \(78.03\pm 0.30\) & \(77.33\pm 0.21\) & \(77.30\pm 0.24\) \\  & \(10\%\) **pair** & \(74.04\pm 1.58\) & \(74.33\pm 1.48\) & \(69.18\pm 4.03\) & \(73.22\pm 1.84\) & \(74.44\pm 1.00\) & \(74.89\pm 1.43\) & \(74.45\pm 1.74\) & \(74.53\pm 1.66\) \\  & \(20\%\) **pair** & \(70.11\pm 1.40\) & \(70.25\pm 1.45\) & \(65.89\pm 2.42\) & \(70.57\pm 2.89\) & \(70.69\pm 1.84\) & \(71.15\pm 16.08\) & \(70.83\pm 1.61\) & \(70.84\pm 1.61\) \\  & \(30\%\) **pair** & \(62.56\pm 2.39\) & \(62.02\pm 2.48\) & \(56.98\pm 2.37\) & \(62.59\pm 4.95\) & \(64.25\pm 2.77\) & \(63.86\pm 3.44\) & \(63.92\pm 25.85\) & \(65.95\pm 2.98\) \\  & \(60\%\) **pair** & \(52.16\pm 7.86\) & \(52.5

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Dataset & Records & GCN & S-model & Cotexching & JoCoR & APL & SCE & Forward & Backward \\ \hline \multirow{6}{*}{**Cora**} & **ACLT** & \(98.53\pm 0.93\) & \(98.79\pm 1.34\) & \(98.81\pm 1.13\) & \(95.00\pm 3.52\) & \(97.50\pm 2.09\) & \(98.33\pm 1.65\) & \(98.70\pm 0.96\) & \(98.70\pm 0.96\) \\  & **ALT** & \(33.77\pm 9.40\) & \(30.92\pm 6.95\) & \(11.87\pm 5.23\) & \(41.59\pm 1.04\) & \(38.66\pm 12.00\) & \(22.90\pm 13.41\) & \(31.99\pm 9.71\) & \(31.19\pm 9.71\) \\  & **AUCS** & \(80.76\pm 2.95\) & \(81.12\pm 3.10\) & \(77.33\pm 3.31\) & \(80.20\pm 3.30\) & \(81.11\pm 3.16\) & \(79.75\pm 3.35\) & \(81.46\pm 3.10\) & \(81.66\pm 3.10\) \\  & **AUC** & \(71.99\pm 3.44\) & \(72.04\pm 3.97\) & \(64.88\pm 6.03\) & \(72.51\pm 5.82\) & \(74.14\pm 3.98\) & \(69.34\pm 4.94\) & \(72.89\pm 4.95\) & \(72.94\pm 4.07\) \\  & **AUIS** & \(51.55\pm 6.53\) & \(51.12\pm 7.11\) & \(34.72\pm 8.61\) & \(55.40\pm 11.08\) & \(56.25\pm 7.93\) & \(47.42\pm 8.07\) & \(53.65\pm 6.87\) & \(53.70\pm 7.44\) \\  & **Time** & \(0.18\pm 0.29\) & \(0.10\pm 0.03\) & \(2.58\pm 1.02\) & \(2.36\pm 1.46\) & \(0.13\pm 0.03\) & \(1.16\pm 1.03\) & \(1.75\pm 0.16\) & \(1.71\pm 0.13\) \\ \hline \multirow{6}{*}{**Citeseer**} & **ACLT** & \(99.18\pm 0.96\) & \(99.42\pm 0.83\) & \(98.95\pm 1.02\) & \(90.11\pm 2.50\) & \(98.23\pm 2.02\) & \(99.88\pm 0.37\) & \(99.05\pm 1.48\) & \(98.93\pm 1.44\) \\  & **ALT** & \(1.39\pm 2.65\) & \(1.68\pm 2.64\) & \(1.98\pm 1.87\) & \(12.05\pm 7.35\) & \(31.00\pm 3.39\) & \(1.75\pm 1.51\) & \(2.48\pm 2.70\) & \(2.48\pm 2.70\) \\  & **AUCS** & \(74.70\pm 3.49\) & \(74.55\pm 3.03\) & \(72.39\pm 3.22\) & \(71.53\pm 3.06\) & \(75.44\pm 3.33\) & \(74.12\pm 3.32\) & \(75.00\pm 2.56\) & \(75.02\pm 3.09\) \\  & **AUCS** & \(75.50\pm 3.60\) & \(57.19\pm 3.42\) & \(56.25\pm 4.86\) & \(57.80\pm 2.90\) & \(57.81\pm 3.87\) & \(57.19\pm 3.49\) & \(58.75\pm 3.13\) & \(58.96\pm 3.19\) \\  & **AUIS** & \(16.45\pm 5.60\) & \(15.37\pm 5.67\) & \(19.02\pm 6.67\) & \(18.53\pm 7.35\) & \(16.30\pm 4.76\) & \(16.80\pm 5.65\) & \(20.57\pm 6.05\) & \(20.91\pm 5.57\) \\  & **Time** & \(0.55\pm 0.44\) & \(0.74\pm 0.69\) & \(2.26\pm 1.23\) & \(1.54\pm 1.25\) & \(0.94\pm 1.59\) & \(3.79\pm 1.47\) & \(2.19\pm 0.77\) & \(2.40\pm 0.86\) \\ \hline \multirow{6}{*}{**Pubmed**} & **ACLT** & \(97.08\pm 2.61\) & \(96.79\pm 2.89\) & \(97.67\pm 2.50\) & \(84.79\pm 4.05\) & \(96.41\pm 2.41\) & \(89.38\pm 12.10\) & \(97.55\pm 3.24\) & \(97.55\pm 3.24\) \\  & **ALT** & \(30.01\pm 1.53\) & \(25.21\pm 8.01\) & \(18.08\pm 12.42\) & \(30.72\pm 10.11\) & \(28.95\pm 1.11\) & \(35.51\pm 15.07\) & \(12.98\pm 1.48\) & \(12.66\pm 13.81\) \\  & **AUCS** & \(69.49\pm 4.71\) & \(70.11\pm 7.61\) & \(73.03\pm 8.08\) & \(60.31\pm 11.59\) & \(71.10\pm 8.60\) & \(68.16\pm 5.19\) & \(71.22\pm 5.73\) & \(71.72\pm 5.73\) \\  & **AUUS** & \(59.47\pm 7.87\) & \(59.47\pm 6.61\) & \(60.53\pm 9.37\) & \(72.63\pm 10.23\) & \(61.05\pm 7.53\) & \(59.47\pm 7.46\) & \(59.47\pm 7.87\) & \(59.47\pm 7.87\) \\  & **AUIS** & \(27.06\pm 21.05\) & \(25.95\pm 20.31\) & \(34.80\pm 28.88\) & \(72.82\pm 20.72\) & \(28.97\pm 18.29\) & \(41.27\pm 24.98\) & \(22.86\pm 15.84\) & \(22.86\pm 15.84\) \\  & **Time** & \(0.21\pm 0.06\) & \(0.35\pm 0.30\) & \(2.25\pm 1.33\) & \(2.22\pm 1.51\) & \(0.35\pm 0.13\) & \(4.13\pm 1.97\) & \(2.12\pm 0.74\) & \(2.05\pm 0.65\) \\ \hline \multirow{6}{*}{**A-Computers**} & **ACLT** & \(91.05\pm 3.19\) & \(92.33\pm 2.22\) & \(86.33\pm 3.46\) & \(84.85\pm 3.60\) & \(92.49\pm 4.26\) & \(91.04\pm 4.03\) & \(51.54\pm 15.75\) & \(37.62\pm 39.19\) \\  & **ALT** & \(68.90\pm 9.98\) & \(74.61\pm 4.68\) & \(67.96\pm 5.67\) & \(64.28\pm 8.40\) & \(72.38\pm 9.26\) & \(72.56\pm 5.99\) & \(45.95\pm 14.45\) & \(31.87\pm 27.46\) \\  & **AUUS** & \(83.43\pm 2.44\) & \(83.97\pm 2.99\) & \(80.30\pm 4.47\) & \(74.94\pm 8.12\) & \(84.59\pm 2.65\) & \(84.00\pm 3.22\) & \(47.10\pm 11.11\) & \(38.38\pm 35.61\) \\  & **AUC** & \(81.40\pm 3.76\) & \(82.77\pm 3.19\) & \(78.96\pm 4.28\)

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_FAIL:18]

Additional experiment result figures

### Transition patterns

In this work, we primarily consider two types of label noise: pair noise and uniform noise, as defined in Section 2. Additionally, our code implementation supports random noise. Unlike the fixed transition patterns of the first two types, the label transition pattern for random noise is generated randomly. Below are examples of their label transition probabilities.

### Test accuracy of different methods under different noise rate

In Section 4, we investigated the performance of different LLN and GLN methods, here are additional experimental results.

Figure A1: Label transition probability under \(30\%\) uniform noise, pair noise and random noise, respectively.

Figure A3: Test accuracy of LLN and GLN methods on Citeseer dataset under different rate of pair and uniform noise, respectively (10 Runs).

Figure A5: Test accuracy of LLN and GLN methods on Amazon-Computers dataset under different rate of pair and uniform noise, respectively (10 Runs).

Figure A7: Test accuracy of LLN and GLN methods on Blogcatalog dataset under different rate of pair and uniform noise, respectively (10 Runs).

[MISSING_PAGE_EMPTY:23]

## Appendix C Additional details of the Benchmark

### Datasets

**Cora, Citeseer and Pubmed**[19] are citation networks that most commonly used in previous graph learning under label noise studies [1, 31, 24, 11, 18]. Each node represents a paper and each edge represents citation relationship between papers. Node features are 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The label of each node is its category of research topic.

**Amazon-Computers and Amazon-Photo**[20] are co-purchase graphs extracted from Amazon, where each node represents a product, edges represent the co-purchased relationships between products. Features are bag-of-words vectors extracted from product reviews, labels of each node is its corresponding product category. These datasets were frequently used in robust graph learning under label noise studies [24, 12].

**Amazon-Ratings**[17] is derived from the Amazon product co-purchasing network metadata from SNAP Datasets. In this dataset, nodes represent products, while edges indicate products frequently bought together. Node features are calculated as the average of FastText embeddings for words in the product descriptions. The product ratings are categorized into five distinct classes as labels.

**Roman-Empire**[17] is derived from the Roman Empire article on English Wikipedia. The text was obtained from the English Wikipedia dump dated 2022.03.01. Each node in the dataset's graph represents a (non-unique) word in the text. Node features are calculated by FastText embeddings. Two words are connected if they follow each other in the text or are linked in the dependency tree of a sentence. Nodes are labeled based on their syntactic roles and identified using spaCy. The 17 most frequent roles are considered unique classes, while others are grouped into the 18th class.

**DBLP**[15] is an author collaboration network in computer science, each node represents a document and edges represent their citation links. Features are word vectors and labels are category of research topic. This dataset was used in study [1].

**Blogcatalog**[26] is a social network formed by an online community, each node is a blogger and edges represent their relationships. The features of each node are derived from the keywords present in their blog descriptions, and the labels are selected from a collection of established categories that reflect the bloggers' interests. This dataset was used in study [18].

**Flickr**[26] is a platform where users can share videos and images. User can follow each others thus form a social network. The feature of each node are generated from the user-specified tags, and labels represent the groups they have joined.

### Algorithms

#### c.2.1 Graph Neural Networks with Label Noise

**NRGNN**[1] believe that since the labels on the graph are sparse, the falsely labeled nodes may affect the unlabeled nodes in its neighborhood, which make it difficult to receive supervision from correctly labeled nodes. To address these issues, NRGNN first connects nodes with similar features to create a refined graph. Based on this refined graph, precise pseudo-labels are generated, allowing unlabeled samples to receive more supervision from correctly labeled samples, thereby reducing the impact of noisy labels.

**RTGNN**[18] followed the work of NRGNN. The authors point out that although NRGNN emphasizes providing additional supervision for unlabeled nodes through link prediction, it does not distinguish between incorrectly labeled and correctly labeled nodes. Instead, it merely connects nodes with similar features indiscriminately, which may lead to the spread of influence from incorrectly labeled nodes. To solve this problem, the authors propose RTGNN, which, building based on NRGNN, uses the small-loss criterion from Co-teaching [4] to further distinguish between trustworthy and untrustworthy nodes, and corrects the labels of some untrustworthy nodes, mitigates some of the shortcomings of NRGNN.

**CP**[31] studied the impact of adversarial label-flipping attacks on the generalization ability of Graph Convolutional Networks (GCNs). To counteract label-flipping attacks, the authors proposed a defense framework named CP, which uses community labels as high-level signals to guide the node classification task. The CP framework includes a constraint with community information to prevent overfitting to the flipped noisy labels. The use of community labels is motivated by their similarity to the output of GCNs.

**D-GNN**[14] obtains a label noise robust Graph Neural Network by adopting backward loss correction [16] on GIN [25] backbone, which estimates the unbiased loss on clean labels.

**RNCGLN**[32] aim to simultaneously mitigate graph and label noise issues. To achieve this, it first use graph contrastive loss to conduct local graph learning, and adopt multi-head self-attention mechanism to learn node representation from a global perspective. Then utilize pseudo graphs and pseudo labels to deal with graph noise and label noise, respectively.

**CLNode**[24] adopt a curriculum learning strategy to mitigate the impact of label noise. To be specific, it first utilize a multi-perspective difficulty measurer to accurately measure the quality of training nodes. Then employ a training scheduler that selects appropriate training nodes to train GNN in each epoch based on the measured qualities. The authors demonstrated this method enhances the robustness of backbone GNN to label noise.

**PIGNN**[2] enhances the GNN's resistance to label noise by introducing additional pair-wise labels. The motivation is pair-wise labels are more robust than node-wise labels. In authors' definition, a pair interaction label is 1 if the nodes have the same label, and 0 otherwise, and they designed a PI label estimation method based on the similarity of node embeddings. During training, the estimated PI label serves as the confidence level for the node classifier's predictions, thereby constraining the training process of the node classifier. This method performs well on homophilic graphs but poorly on heterophilic graphs.

Figure A10: Timeline of GLN research. Existing GLN methods can be categorized into Loss regularization, Robust training strategy, Graph structure augmentation and Contrastive learning.

**Union-NET**[11] tries to limit the gradient passing process of mislabeled samples through neighborhood labeling, like a kind of neighborhood voting with node representation similarity weighting. A GNNs first generates node representations and predicted labels. Context nodes are then aggregated using random walks, and an attention mechanism calculates class probability distributions. This guides a reweighting scheme to minimize the impact of noisy labels. Labels are corrected by aligning them with the most consistent context labels, and a KL-divergence loss maintains alignment with the prior distribution. The training involves pre-training the GNN and updating model parameters with a combined loss function, ensuring robust training and effective label correction.

**GNN**[30] addresses label noise in GNNs by combining neighborhood-based label correction and contrastive learning. It utilizes message passing neural networks to update node representations, integrating graph contrastive learning for consistent representations across augmented graph views. Finally, CGNN employs an MLP for prediction distributions and iteratively corrects noisy labels by comparing them with their neighbors and choosing the most labels.

**CR-GNN**[10] introduces contrastive learning to enhance GNNs robustness in the face of sparse and noisy node labels. Through techniques like feature masking and edge dropping, CR-GNN preserves node semantics while generating augmented views. Contrastive loss captures local structural information and mitigates noisy label effects, while dynamic cross-entropy loss addresses overfitting and adversarial vulnerabilities. Also, cross-space consistency ensures semantic alignment between embeddings.

#### c.2.2 Learning with Label Noise methods

**S-model**[3] adds a noise adaptation layer that models the transition pattern of noisy labels on true labels. In the training procedure, this layer is parameterized by bias terms and allows the network to learn both the classifier and noise model simultaneously. In the test procedure, the noise adaptation layer is removed, which enables the network to predict true labels more effectively.

**Co-teaching**[4] works by simultaneously training two deep neural networks (DNNs), each of which selects a certain number of small-loss samples from them and passes these samples to the other for further training. It assume that mislabeling typically leads to larger losses and thus is less likely to be selected, and then each network selects the samples that perform best on its own with lower loss. This peer-to-peer training mechanism helps to reduce the effect of noisy labels, as both networks focus on more reliable data.

**JoCoR**[23] utilizes consistency maximization to deal with the noisy labels. Instead of using hard sampling, two different classifiers are made to converge in their predictions through explicit regularization. Specifically, the two classifiers are trained by a joint loss function to minimize the differences between them. During the training process, these two classifiers update their parameters at the same time and are jointly trained by means of a pseudo-twin network. The loss function consists of a supervised learning loss and a contrast loss, where the contrast loss is used to maximize the agreement between the two classifiers.

**SCE**[22] enhances the robustness of a model in the presence of noisy labels by combining a noise tolerance term with the standard cross entropy (CCE) loss. Inspired by the Kullback-Leibler scattering symmetry, SCE incorporates the reverse cross entropy loss, a noise tolerance term, and combines it with the standard CCE loss to improve the model's ability to tolerate noisy data. This approach not only retains the advantages of the CCE loss, but also significantly improves the generalization performance in noisy environments through symmetry processing and noise tolerance.

**Forward correction**[16] corrects the sample loss by linearly combining the softmax output of the DNN before applying the loss function. During the forward propagation process, the estimated label transfer probability is multiplied with the softmax output to obtain the corrected loss value. In this way, the softmax output of each sample is first combined with the corresponding transfer probability, and then the loss function is applied, which improves the robustness of the model in noisy labeling environments.

**Backward correction**[16] adjusts the loss for each sample by multiplying the estimated label transfer probability with the output of the specified DNN. The learning of the label transfer probability is decoupled from the learning of the model, and the label transfer matrix is first approximated using the softmax output of the DNN in the uncorrected loss case. Then, when retraining the DNN, the original loss is corrected based on the estimated matrix. The correction loss is computed by linearly combiningthe loss values for each observable label, where the coefficients are the transfer probabilities from each observable label to the target label.

## Appendix D Package

We have developed an open-source software package NoisyGL, which provides a comprehensive and unbiased platform for evaluating GLN algorithms and advancing future research. The code structure of NoisyGL is well-designed to ensure fair experimental setups for different algorithms, easy reproducibility of experimental results, and support for flexible assembly of models for experiments. NoisyGL includes the following key modules. The Config module consists of the files that define the necessary hyperparameters and settings. The Dataset module is used to load datasets, and the label Contaminator modifies the raw data to create a contaminated graph. The Base-predictor serves as the base class for various reproduced LLN/GLN predictors, and the LLN/GLN Predictor evaluates the contaminated graph to predict performance.

As shown in the Figure A11, the code structure is well-organized to ensure fair experimental settings across algorithms, easy reproduction of experimental results, and convenient trials on flexibly assembled models. Given a specific dataset and config file, a solver will return the learned structure and the task performance. For more details and updated features, please refer to our GitHub repository.

**General Experimental Settings.** We endeavor to follow the original implementations of the various GLN methods in their associated papers or source code. To this end, we integrate the different options into a standardized framework as shown in Figure. In this way, we can ensure consistency and comparability of experiments, allowing the performance of different GLN methods to be fairly evaluated on the same platform. We run most experiments on NVIDIA Geforce RTX 3090 GPU with 24 GB memory, the out-of-memory error during the training is reported as N/A in Appendix A. For the two large datasets, Amazon Ratings and Roman Empire, we run these experiments on NVIDIA A100 with 80GB memory.

**Hyperparameter.** We performed manual hyperparameter tuning to ensure an unbiased evaluation of these GLN methods. The hyperparameter search space for all methods is shown in Table A8. For details on the meaning of these hyperparameters, please refer to their original papers. Through exhaustive tuning and setup, we strive for the best performance of each method under different configurations, thus ensuring the accuracy and fairness of the evaluation.

Figure A11: The structure of NoisyGL. The Raw data is processed by the Label Contaminator to introduce label noise, resulting in a Contaminated Graph. This contaminated graph, along with the Method config, is then input to the LLN/GLN Predictor, which evaluates performance metrics based on the specified method configuration.

\begin{table}
\begin{tabular}{l|l|l|l} \hline
**Algorithm** & **Hyper-parameter** & **Search Space** \\ \hline \multirow{4}{*}{General Settings} & learning rate & 1e-1, 5e-2, 1e-2, 5e-3, 1e-3, 5e-4, 1e-4, 5e-5 \\  & weight decay & 5e-2, 5e-3, 5e-4, 5e-5 \\  & layer number & 2, 3, 4, 5 \\  & hidden size & 16, 32, 64, 128 \\ \hline \multirow{2}{*}{NRGNN [1]} & \(\alpha\) & 0.01, 0.02, 0.03 \\  & \(\beta\) & 0.01, 0.1, 1, 10 \\ \hline \multirow{2}{*}{RTGNN [18]} & \(\tau\) & 0, 0.05, 0.1, 0.2 \\  & \(\lambda\) & 0.05, 0.1, 0.2 \\  & \(\alpha\) & 0.03, 0.1, 0.3, 1 \\ \hline LAFAK/CP [31] & \(\lambda\) & 0.1, 0.2, 0.3 \\ \hline \multirow{2}{*}{CLNode [24]} & \(\lambda_{0}\) & 0.25, 0.5, 0.75 \\  & \(T\) & 50, 100, 150 \\ \hline PIGNN [2] & N/A & N/A \\ \hline DGNN [14] & N/A & N/A \\ \hline RNCGLN [32] & \(\alpha\) & \(10^{-3}\), \(10^{-2}\),..., \(10^{3}\) \\ \hline \multirow{2}{*}{UnionNET [11]} & \(\alpha\) & 0.1, 0.5, 1.0 \\  & \(\beta\) & 0.1, 0.5, 1.0 \\ \hline \multirow{2}{*}{CGNN [30]} & \(\gamma\) & 0.6, 0.7, 0.8, 0.9, 0.95 \\  & \(\omega\) & 0.6, 0.7, 0.8, 0.9, 0.95 \\ \hline \multirow{2}{*}{CRGNN [10]} & \(\alpha\) & 0.1, 0.2, 0.3,..., 1 \\  & \(\beta\) & 0.1, 0.2, 0.3,..., 1 \\ \hline \end{tabular}
\end{table}
Table A8: Hyper-parameter search space of all implemented GLN methods.

Reproducibility

All of NoisyGL's experimental results are highly reproducible. We provide more detailed information on the following aspects to ensure the reproducibility of the experiments.

**Accessibility.** You can access all datasets, algorithm implementations, and experimental configurations in our open source project https://github.com/eaglelab-zju/NoisyGL without a personal request.

**Dataset.** The datasets used are publicly available. The Cora, Citeseer, and Pubmed datasets are accessible online and are used under the Creative Commons 4.0 license. The BlogCatalog and Flickr datasets were originally published by [26] and further processed in subsequent studies. To the best of our knowledge, these datasets do not have a specific license. The DBLP dataset can be found in [15] and is released under the MIT license. All of these datasets are licensed by the authors for academic research and do not contain any personally identifiable information or offensive content.

**Documentation and uses.** We've dedicated ourselves to providing users with comprehensive documentation, guaranteeing a smooth experience with our library. Our code includes ample comments to enhance readability. Furthermore, we furnish all essential files to replicate experimental outcomes, which also serve as illustrative guides on library utilization. Running the code is straightforward; users need only execute the '.py' files with specified arguments like data, method, and GPU.

**License.** We use an MIT license for our open-sourced project.

**Code maintenance.** We are dedicated to maintaining our code through continuous updates, actively engaging with user feedback, and addressing any issues promptly. Additionally, we are eager to receive contributions from the community to improve our library and benchmark algorithms. However, we will uphold rigorous version control measures to uphold reproducibility standards during maintenance procedures.