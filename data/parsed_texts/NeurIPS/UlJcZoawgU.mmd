# Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman

 Jiarui Feng\({}^{1}\)  Lecheng Kong\({}^{1}\)  Hao Liu\({}^{1}\)  Dacheng Tao\({}^{2}\)  Fuhai Li\({}^{1}\)

**Muhan Zhang\({}^{3}\) Yixin Chen\({}^{1}\)**

{feng.jiarui, jerry.kong, liuhao, fuhai.li, ychen25}@wustl.edu,

dacheng.tao@gmail.com, muhan@pku.edu.cn

\({}^{1}\)Washington University in St. Louis \({}^{2}\)JD Explore Academy \({}^{3}\)Peking University

Corresponding author

###### Abstract

Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by \(k\)-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) \(k\)-WL/FWL requires at least \(O(n^{k})\) space complexity, which is impractical for large graphs even when \(k=3\); (2) The design space of \(k\)-WL/FWL is rigid, with the only adjustable hyper-parameter being \(k\). To tackle the first limitation, we propose an extension, \((k,t)\)-FWL. We theoretically prove that even if we fix the space complexity to \(O(n^{k})\) (for any \(k\geq 2\)) in \((k,t)\)-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose \(k\)-FWL+, which considers any equivariant set as neighbors instead of all nodes, thereby greatly expanding the design space of \(k\)-FWL. Combining these two modifications results in a flexible and powerful framework \((k,t)\)-FWL+. We demonstrate \((k,t)\)-FWL+ can implement most existing models with matching expressiveness. We then introduce an instance of \((k,t)\)-FWL+ called Neighborhood\({}^{2}\)-FWL (N\({}^{2}\)-FWL), which is practically and theoretically sound. We prove that N\({}^{2}\)-FWL is no less powerful than 3-WL, and can encode many substructures while only requiring \(O(n^{2})\) space. Finally, we design its neural version named N\({}^{2}\)-GNN and evaluate its performance on various tasks. N\({}^{2}\)-GNN achieves record-breaking results on ZINC-Subset (**0.059**) outperforming previous SOTA results by 10.6%. Moreover, N\({}^{2}\)-GNN achieves new SOTA results on the BREC dataset (**71.8%**) among all existing high-expressive GNN methods.

## 1 Introduction

In recent years, graph neural networks (GNNs) have become one of the most popular and powerful methods for graph representation learning, following a message passing framework [1; 2; 3; 4]. However, the expressive power of message passing GNNs is bounded by the one-dimensional Weisfeiler-Lehman (1-WL) test [5; 6]. As a result, numerous efforts have been made to design GNNs with higher expressive power. We provide a more detailed discussion in Section 5.

Several works have drawn inspiration from the \(k\)-dimensional Weisfeiler-Lehman (\(k\)-WL) or Folklore Weisfeiler-Lehman (\(k\)-FWL) test [7] and developed corresponding neural versions [8; 9; 10; 6]. However, \(k\)-WL/FWL has two inherent limitations. First, while the expressive power increaseswith higher values of \(k\), the space and time complexity also grows exponentially, requiring \(O(n^{k})\) space complexity and \(O(n^{k+1})\) time complexity, which makes it impractical even when \(k=3\). Thus, the question arises: **Can we retain high expressiveness without exploding both time and space complexities?** Second, the design space of WL-based algorithms is rigid, with the only adjustable hyper-parameter being \(k\). However, there is a significant gap in expressive power even between consecutive values of \(k\), making it hard to fine-tune the tradeoffs. Moreover, increasing the expressive power does not necessarily translate into better real-world performance, as it may lead to overfitting [8; 11]. Although some works try to tackle this problem [8; 10], there is still limited understanding of **how to expand the design space of the original \(k\)-FWL to a broader space** that enables us to identify the most appropriate instance to match the complexity of real-world tasks.

To tackle the first limitation, we notice that \(k\)-FWL and \(k\)-WL have the same space complexity but \(k\)-FWL can achieve the same expressive power as (\(k\)+1)-WL. We found the key component that allows \(k\)-FWL to have stronger power is the tuple aggregation style. Enlightened by this observation, we propose \((k,t)\)-FWL, which extends the tuple aggregation style in \(k\)-FWL. Specifically, in the original \(k\)-FWL, a neighbor of a \(k\)-tuple is defined by iteratively replacing its \(i\)-th element with a node \(u\), and \(u\) traverses all nodes in the graph. In \((k,t)\)-FWL, we extend a single node \(u\) to a \(t\)-tuple of nodes and carefully design a replacement scheme to insert the \(t\)-tuple into a \(k\)-tuple to define its neighbor. We demonstrate that even with a fixed space complexity of \(O(n^{k})\) (for any \(k\geq 2\)), \((k,t)\)-FWL can construct an expressive hierarchy capable of solving the graph isomorphism problem. To deal with the second limitation, we revisit the definition of neighborhood in \(k\)-FWL. Inspired by previous works [8; 12] which consider only local neighbors (i.e., \(u\) must be connected to the \(k\)-tuple) instead of global neighbors in \(k\)-WL/FWL, we find that the neighborhood (i.e., which \(u\) are used to construct a \(k\)-tuple's neighbors) can actually be extended to any equivariant set related to the \(k\)-tuple, resulting in \(k\)-FWL+. Combining the two modifications leads to a novel and powerful FWL-based algorithm \((k,t)\)-FWL+. \((k,t)\)-FWL+ is highly flexible and can be used to design different versions to fit the complexity of real-world tasks. Based on the proposed \((k,t)\)-FWL+ framework, we implement many different instances that are closely related to existing powerful GNN/WL models, further demonstrating the flexibility of \((k,t)\)-FWL+.

Finally, we propose an instance of \((2,2)\)-FWL+ named **Neighborhood\({}^{2}\)-FWL** that is both theoretically expressive and practically powerful. It considers the local neighbors of both two nodes in a 2-tuple. Despite having a space complexity of \(O(n^{2})\), which is lower than 3-WL's \(O(n^{3})\) space complexity, this instance can still partially outperform 3-WL and is able to count many substructures. We implement a neural version named **Neighborhood\({}^{2}\)-GNN (N\({}^{2}\)-GNN)** and evaluate its performance on various synthetic and real-world datasets. Our results demonstrate that N\({}^{2}\)-GNN outperforms existing SOTA methods across most tasks. Particularly, it achieves **0.059** in ZINC-Subset, surpassing existing state-of-the-art models by significant margins. Meanwhile, it achieves **71.8%** on the BREC dataset, the new SOTA among all existing high-expressive GNN methods.

## 2 Preliminaries

**Notations.** Let \(\{\cdot\}\) denote a set, \(\{\!\!\{\cdot\}\!\}\) denote a multiset (a set that allows repetition), and \((\cdot)\) denote a tuple. As usual, let \([n]=\{1,2,\ldots,n\}\). Let \(G=(V(G),E(G),l_{G})\) be an undirected, colored graph, where \(V(G)=[n]\) is the node set with \(n\) nodes, \(E(G)\subseteq V(G)\times V(G)\) is the edge set, and \(l_{G}\colon V(G)\to C\) is the graph coloring function with \(C=\{c_{1},\ldots,c_{d}\}\) denote a set of \(d\) distinct colors. Let \(\mathcal{N}_{k}(v)\) denote a set of nodes within \(k\) hops of node \(v\) and \(Q_{k}(v)\) denote the \(k\)-th hop neighbors of node \(v\) and we have \(\mathcal{N}_{k}(v)=\bigcup_{i=0}^{k}Q_{i}(v)\). Let \(\text{SPD}(u,v)\) denote the shortest path distance between \(u\) and \(v\). We use \(x_{v}\in\mathbb{R}^{d_{u}}\) to denote attributes of node \(v\in V(G)\) and \(e_{uv}\in\mathbb{R}^{d_{v}}\) to denote attributes of edge \((u,v)\in E(G)\). They are usually the one-hot encoding of the node and edge color respectively. We say that two graphs \(G\) and \(H\) are _isomorphic_ (denoted as \(G\simeq H\)) if there exists a bijection \(\varphi\colon V(G)\to V(H)\) such that \(\forall u,v\in V(G),(u,v)\in E(G)\Leftrightarrow(\varphi(u),\varphi(v))\in E (H)\) and \(\forall v\in V(G),l_{G}(v)=l_{H}(\varphi(v))\). Denote \(V(G)^{k}\) the set of \(k\)-tuples of vertices and \(\mathbf{v}=(v_{1},\ldots,v_{k})\in V(G)^{k}\) a \(k\)-tuple of vertices. Let \(S_{n}\) denote the permutation group of \([n]\) and \(g\in S_{n}:[n]\rightarrow[n]\) be a particular permutation. When a permutation \(g\in S_{n}\) operates on any target \(X\), we denote it by \(g\cdot X\). Particularly, a permutation operating on an edge set \(E(G)\) is \(g\cdot E(G)=\{(g(u),g(v))|(u,v)\in E(G)\}\). A permutation operating on a \(k\)-tuple \(\mathbf{v}\) is \(g\cdot\mathbf{v}=(g(v_{1}),\ldots,g(v_{k}))\). A permutation operating on a graph is \(g\cdot G=(g\cdot V(G),g\cdot E(G),g\cdot l_{G})\)\(k\)-dimensional Weisfeiler-Lehman test. The \(k\)-dimensional Weisfeiler-Lehman (\(k\)-WL) test is a family of algorithms used to test graph isomorphism. There are two variants of the \(k\)-WL test: \(k\)-WL and \(k\)-FWL (Folklore WL). We first describe the procedure of 1-WL, which is also called the color refinement algorithm [7]. Let \(\mathcal{C}^{0}_{1wl}(v)=l_{G}(v)\) be the initial color of node \(v\in V(G)\). At the \(l\)-th iteration, 1-WL updates the color of each node using the following equation:

\[\mathcal{C}^{l}_{1wl}(v)=\text{HASH}\left(\mathcal{C}^{l-1}_{1wl}(v),\{\!\! \{\mathcal{C}^{l-1}_{1wl}(u)|u\in Q_{1}(v)\}\!\}\right).\] (1)

After the algorithm converges, a color histogram is constructed using the colors assigned to all nodes. If the color histogram is different for two graphs, then the two graphs are non-isomorphic. However, if the color histogram is the same for two graphs, they can still be non-isomorphic.

The \(k\)-WL and \(k\)-FWL, for \(k\geq 2\), are generalizations of the 1-WL, which do not color individual nodes but node tuples \(\mathbf{v}\in V(G)^{k}\). Let \(\mathbf{v}_{w/j}\) be a \(k\)-tuple obtained by replacing the \(j\)-th element of \(\mathbf{v}\) with \(w\). That is \(\mathbf{v}_{w/j}=(v_{1},\ldots,v_{j-1},w,v_{j+1},\ldots,v_{k})\). The main difference between \(k\)-WL and \(k\)-FWL lies in their aggregation way. For \(k\)-WL, the set of \(j\)-th neighbors of tuple \(\mathbf{v}\) is denoted as \(Q_{j}(\mathbf{v})=\{\mathbf{v}_{w/j}|w\in V(G)\}\), \(j\in[k]\). Instead, the \(w\)-neighbor of tuple \(\mathbf{v}\) for \(k\)-FWL is denoted as \(Q_{w}^{F}(\mathbf{v})=\left(\mathbf{v}_{w/1},\mathbf{v}_{w/2},\ldots,\mathbf{ v}_{w/k}\right)\). Let \(\mathcal{C}^{0}_{kwl}(\mathbf{v})=\mathcal{C}^{0}_{kfwl}(\mathbf{v})\) be the initial color for \(k\)-WL and \(k\)-FWL, respectively. They are usually the isomorphism types of tuple \(\mathbf{v}\). At the \(l\)-th iteration, \(k\)-WL and \(k\)-FWL update the color of each tuple according to the following equations:

\[\text{\bf WL:}\quad\mathcal{C}^{l}_{kwl}(\mathbf{v})=\text{HASH} \left(\mathcal{C}^{l-1}_{kwl}(\mathbf{v}),\left(\{\!\!\{\mathcal{C}^{l-1}_{kwl }(\mathbf{u})|\mathbf{u}\in Q_{j}(\mathbf{v})\}\!\}\right|j\in[k]\right)\right),\] (2) \[\text{\bf FWL:}\quad\mathcal{C}^{l}_{kfwl}(\mathbf{v})=\text{HASH }\left(\mathcal{C}^{l-1}_{kfwl}(\mathbf{v}),\{\!\!\{\mathcal{C}^{l-1}_{kfwl} (\mathbf{u})|\mathbf{u}\in Q_{w}^{F}(\mathbf{v})\}\!\!\}\left|w\in V(G)\right. \!\!\!\}\right).\] (3)

The procedure described above is repeated until convergence, resulting in a color histogram of the graph that can be compared with the histograms of other graphs. Prior research has shown that (\(k\)+1)-WL and (\(k\)+1)-FWL are strictly more powerful than \(k\)-WL and \(k\)-FWL, respectively, except in the case where 1-WL is equivalent to 2-WL. Additionally, it has been shown that \(k\)-FWL is equivalent to (\(k\)+1)-WL [13; 14; 6]. We leave the additional discussion on \(k\)-WL/FWL in Appendix A.

**Message passing neural networks.** Message Passing Neural Networks (MPNNs) are a type of GNNs that update node representations by iteratively aggregating information from their direct neighbors. Let \(h^{l}_{v}\) be the output of MPNNs of node \(v\in V(G)\) after \(l\) layers and we let \(h^{0}_{v}=x_{v}\). At \(l\)-th layer, the representation is updated by:

\[h^{l}_{v}=\mathbf{U}^{l}\left(h^{l-1}_{v},\{\!\!\{m^{l}_{vu}|u\in Q_{1}(v)\} \!\!\}\right),\quad m^{l}_{vu}=\mathbf{M}^{l}\left(h^{l-1}_{v},h^{l-1}_{u},e_{ vu}\right),\] (4)

where \(\mathbf{U}^{l}\) and \(\mathbf{M}^{l}\) are learnable update and message functions, usually parameterized by multi-layer perceptrons (MLPs). After \(L\) layers, MPNNs output the final representation \(h^{L}_{v}\) for all nodes \(v\in V(G)\). The graph-level representation is obtained by:

\[h_{G}=\mathbf{R}(\{\!\!\{h^{L}_{v}|v\in V(G)\}\!\!\}),\] (5)

where \(\mathbf{R}\) is a readout function. The expressive power of MPNNs is at most as powerful as 1-WL [5; 6].

## 3 Rethinking and extending the Folklore Weisfeiler-Lehman test

### Rethinking and extending the aggregation style in \(k\)-Fwl

Increasing \(k\) can increase the expressive power of \(k\)-FWL for distinguishing graph structures. However, increasing \(k\) brings significant memory costs, as \(k\)-FWL requires \(O(n^{k})\) memory. It becomes impractical even when \(k=3\)[9]. Therefore, it is natural to ask:

_Can we achieve higher expressive power without exploding the memory cost?_

To achieve this goal, we first notice that \(k\)-FWL has a higher expressive power than \(k\)-WL but only requires the same \(O(n^{k})\) memory cost. To understand why, let's use 2-WL and 2-FWL as examples and rewrite Equation (2) and Equation (3):

**2-WL:**: \(\mathcal{C}^{l}_{2wl}(v_{1},v_{2})=\text{HASH}\left(\mathcal{C}^{l-1}_{2wl}(v_{1},v_{2}),\{\!\!\{\mathcal{C}^{l-1}_{2wl}(v_{1},w)|w\in V(G)\}\!\!\},\{\!\!\{ \mathcal{C}^{l-1}_{2wl}(w,v_{2})|w\in V(G)\}\!\!\}\right),\)
**2-FWL:**: \(\mathcal{C}^{l}_{2fwl}(v_{1},v_{2})=\text{HASH}\left(\mathcal{C}^{l-1}_{2fwl}(v_{1},v_{2}),\{\!\!\{\mathcal{C}^{l-1}_{2fwl}(v_{1},w),\mathcal{C}^{l-1}_{2fwl}(w,v_ {2})\}\!\}\right)|w\in V(G)\}\!\!\}\right),\)where \(\mathcal{C}^{l}_{2wl}(v_{1},v_{2})\) and \(\mathcal{C}^{l}_{2fwl}(v_{1},v_{2})\) are the color of tuple \((v_{1},v_{2})\) at iteration \(l\) for 2-WL and 2-FWL, respectively. We can see the key difference between 2-WL and 2-FWL is that: in 2-FWL, a tuple of color \((\mathcal{C}^{l-1}_{2fwl}(v_{1},w),\mathcal{C}^{l-1}_{2fwl}(w,v_{2}))\) is aggregated. While in 2-WL, the colors of nodes are considered in separate multisets. To understand why the first aggregation is more powerful, we can further rewrite the update equation of 2-FWL as follows:

\[\mathcal{C}^{l}_{2fwl}(v_{1},v_{2})=\text{HASH}\left(\mathbb{\{}\Big{(} \mathcal{C}^{l-1}_{2fwl}(v_{1},v_{2}),\mathcal{C}^{l-1}_{2fwl}(v_{1},w), \mathcal{C}^{l-1}_{2fwl}(w,v_{2})\Big{)}\,|w\in V(G)\mathbb{\}}\right).\]

It is easy to verify that the above equation is equivalent to the original one. This means that 2-FWL updates the color of tuple \((v_{1},v_{2})\) by aggregating different tuples of \(((v_{1},v_{2}),(v_{1},w),(w,v_{2}))\), which can be viewed as aggregating information of 3-tuples \((v_{1},v_{2},w)\). For example, \(\text{HASH}(\mathcal{C}^{0}_{2fwl}(v_{1},v_{2}),\mathcal{C}^{0}_{2fwl}(v_{1},w),\mathcal{C}^{0}_{2fwl}(w,v_{2}))\) can fully recover the isomorphism type of tuple \((v_{1},v_{2},w)\) used in 3-WL. The above observations can be easily extended to \(k\)-FWL. The key insight here is that **tuple-style aggregation is the key to lifting the expressive power of \(k\)-FWL**. Since aggregating \(k\)-tuple can boost the expressive power of \(k\)-FWL to be equivalent to (\(k\)+1)-WL without the increase of space complexity, we may wonder, can we further extend the size of the tuple?

We first introduce the neighborhood tuple to extend the original \(w\)-neighbor used in \(k\)-FWL. Let \(Q^{F}_{\mathbf{w}}(\mathbf{v})\) denote a neighborhood tuple related to \(k\)-tuple \(\mathbf{v}\) and \(t\)-tuple \(\mathbf{w}\). The neighborhood tuple contains all possible results such that we sequentially select \(m\in[0,1,\ldots,min(k,t)]\) elements from \(\mathbf{w}\) to replace \(m\) elements in \(\mathbf{v}\). For each \(m\), we first select all possible combinations of \(m\)-tuples (no repeated elements) from tuple \(\mathbf{w}\) with a pre-defined order to form a new tuple, denoted as \(P_{m}(\mathbf{w})\). At the same time, we select all possible combinations of \(m\) indices from \(\mathbf{i}=(1,2,\ldots,k)\) with a pre-defined order to form another new tuple \(P_{m}(\mathbf{i})\). Finally, we iterate all \((\mathbf{w}^{\prime},\mathbf{i}^{\prime})\in P_{m}(\mathbf{w})\times P_{m}( \mathbf{i})\) to get a \(|P_{m}(\mathbf{w})||P_{m}(\mathbf{i})|\)-sub-tuple, where for each \((\mathbf{w}^{\prime},\mathbf{i}^{\prime})\) we replace the elements with indices \(\mathbf{i}^{\prime}\) in the original \(\mathbf{v}\) with \(\mathbf{w}^{\prime}\). The final neighborhood tuple is the concatenation of all sub-tuples. Note that the pre-defined orders can be flexible as long as they are consistent for any \(k\) and \(t\).

Here is an example of a possible construction of a neighborhood tuple when \(k=t=2\). Let \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\) denote the neighborhood tuple. In this case, we need to enumerate \(m\) in \([0,1,2]\). First, when \(m=0\), we select 0 elements from \((w_{1},w_{2})\) to replace \((v_{1},v_{2})\), resulting in a sub-tuple with only one element \(((v_{1},v_{2}))\). Next, when \(m=1\). we sequentially select \(w_{1}\) and \(w_{2}\), resulting in \(P_{1}(\mathbf{w})=(w_{1},w_{2})\). Similarly, we can get \(P_{1}(\mathbf{i})=(2,1)\). Thus, the final sub-tuple is \(((v_{1},w_{1}),(v_{1},w_{2}),(w_{1},v_{2}),(w_{2},v_{2}))\). The case of \(m=2\) is simple and we have the result sub-tuple be \(((w_{1},w_{2}))\). By concatenating all three sub-tuples, we have \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})=((v_{1},v_{2}),(v_{1},w_{1}),(v_{1},w_{2}),( w_{1},v_{2}),(w_{2},v_{2}),(w_{1},w_{2}))\). The construction procedure is shown in Figure 1. By default, we adopt the above order in \(Q^{F}_{\mathbf{w}}(\mathbf{v})\) in the rest of the paper if \(k=t=2\). We leave a more formal definition of the neighborhood tuple \(Q^{F}_{\mathbf{w}}(\mathbf{v})\) in Appendix A.

With the neighborhood tuple, we are ready to introduce \((k,t)\)**-FWL**, which extends \(k\)-FWL by extending the tuple size in the update function. Let \(\mathcal{C}^{l}_{ktfwl}(\mathbf{v})\) be the color of tuple \(\mathbf{v}\) at iteration \(l\) for \((k,t)\)-FWL, the update function of \((k,t)\)-FWL is:

\[(k,t)\text{-FWL:}\quad\mathcal{C}^{l}_{ktfwl}(\mathbf{v})=\text{HASH}( \mathcal{C}^{l-1}_{ktfwl}(\mathbf{v}),\mathbb{\{}\Big{(}\mathcal{C}^{l-1}_{ktfw }(\mathbf{u})|\mathbf{u}\in Q^{F}_{\mathbf{w}}(\mathbf{v})\Big{)}\,|\mathbf{w} \in V^{t}(G)\mathbb{\}}_{t}),\] (6)

Figure 1: Illustration of the construction of neighborhood tuple \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\) in \((2,2)\)-FWL. We sequentially select 0, 1, 2 elements from \((w_{1},w_{2})\) to replace 0, 1, 2 elements in \((v_{1},v_{2})\), resulting in three sub-tuple of length 1, 4, 1, respectively. The final neighborhood tuple is the concatenation of three sub-tuples. We can easily recover high-order graph structures from the constructed neighborhood tuple with the isomorphism type of 2-tuples.

where \(\{\!\{\cdot\}\!\}_{t}\) is hierarchical multiset over \(t\)-tuples. In a hierarchical multiset \(\{\!\{\!\mathbf{v}|\mathbf{v}\in V^{t}(G)\!\}\!\}_{t}\), elements are grouped hierarchically according to the node order of the tuple. For example, to construct \(\{\!\{\!(v_{1},v_{2},v_{3})|(v_{1},v_{2},v_{3})\in V^{3}(G)\!\}\!\}_{3}\) from \(\{\!\{\!(v_{1},v_{2},v_{3})|(v_{1},v_{2},v_{3})\in V^{3}(G)\!\}\!\}\), we first group together all elements with the same \(v_{2}\) and \(v_{3}\). That is, \(\forall v_{2},v_{3}\in V(G)\), we denote \(t(v_{2},v_{3})=\{\!\{\!(v_{1},v_{2},v_{3})|v_{1}\in V(G)\!\}\!\}\) as the grouped result. Next, use the similar procedure, we have \(\forall v_{3}\in V(G)\), \(t(v_{3})=\{\!\{\!t(v_{2},v_{3})|v_{2}\in V(G)\!\}\!\}\). Finally, we group all possible \(v_{3}\in V(G)\) to get \(\{\!\{\!(v_{1},v_{2},v_{3})|(v_{1},v_{2},v_{3})\in V^{3}(G)\!\}\!\}_{3}=\{\!\{ \!t(v_{3})|v_{3}\in V(G)\!\}\!\}\).

It is easy to see \((k,1)\)-FWL is equivalent to \(k\)-FWL under our definition of \(Q_{\mathbf{w}}^{F}(\mathbf{v})\). Further, as we only need to maintain representations of all \(k\)-tuples. \((k,t)\)-FWL has a fixed space complexity of \(O(n^{k})\). Here we show the expressive power of \((k,t)\)-FWL.

**Proposition 3.1**.: _For \(k\geq 2\) and \(t\geq 1\), if \(t\geq n-k\), \((k,t)\)-FWL can solve the graph isomorphism problems with the size of the graph less than or equal to \(n\)._

**Theorem 3.2**.: _For \(k\geq 2\), \(t\geq 1\), \((k,t)\)-FWL is at most as powerful as \((k+t)\)-WL. In particular, \((k,1)\)-FWL is as powerful as \((k+1)\)-WL._

**Proposition 3.3**.: _For \(k\geq 2\), \(t\geq 1\), \((k,t+1)\)-FWL is strictly more powerful than \((k,t)\)-FWL; \((k+1,t)\)-FWL is strictly more powerful than \((k,t)\)-FWL._

We leave all formal proofs and complexity analysis in Appendix B. Briefly speaking, even if we fix the size of \(k\), \((k,t)\)-FWL can still construct an expressive hierarchy by varying \(t\). Further, if \(t\) is large enough, \((k,t)\)-FWL can actually enumerate all possible combinations of tuples with the size of the graph, and thus equivalent to the relational pooling on graph [15]. It is worth noting that the size of \(Q_{\mathbf{w}}^{F}(\mathbf{v})\) will grow exponentially with an increase in the size of \(t\). However, the key contribution of \((k,t)\)-FWL is that even when \(k=2\), \((k,t)\)-FWL can still construct an expressive hierarchy for solving graph isomorphism problems. Therefore, high-order embedding may not be necessary for building high-expressivity WL algorithms. Note that our \((k,t)\)-FWL is also different from subgraph GNNs such as \(k,t\)-WL [16] and \(l\)-OSAN [17], where \(l\)-tuples are labeled independently to enable learning \(k\)-tuple representations in all \(l\)-tuples' subgraphs, resulting in \(O(n^{k+l})\) space complexity.

### Rethinking and extending the aggregation scope of \(k\)-Fwl

Another problem of \(k\)-FWL is its limited design space, as the only adjustable hyperparameter is \(k\). It is well known that there is a huge gap in expressive power even if we increase from \(k\) to \(k+1\). For example, 1-WL cannot count any cycle even with a length of 3, but 2-FWL can already count up to 7-cycle [18, 19, 20]. Moreover, increasing the expressive power does not always bring better performance when designing the corresponding neural version as it quickly leads to overfitting [6, 9, 11]. Therefore, we ask another question:

_Can we extend the \(k\)-FWL to a more flexible and fine-grained design space?_

To address this issue, we identify that the inflexibility of \(k\)-FWL's design space arises from the definition of the neighbor used in the aggregation step. Unlike 1-WL, \(k\)-FWL lacks the concept of local neighbors and instead requires the aggregation of all \(|V(G)|\) global neighbors to update the color of each tuple \(\mathbf{v}\). Recently, some works have extended \(k\)-WL by incorporating local information [8, 12]. Inspired by previous works, we find that the definition of neighbor can actually be much more flexible than just considering local neighbors or global neighbors. Specifically, for each \(k\)-tuple \(\mathbf{v}\) in graph \(G\), we define equivariant set \(ES(\mathbf{v})\) to be the neighbors set of tuple \(\mathbf{v}\) and propose \(k\)**-FWL+**.

**Definition 3.4**.: _An **equivariant set**\(ES(\mathbf{v})\) is a set of nodes related to \(\mathbf{v}\) and equivariant given the permutation \(g\in S_{n}\). That is, \(\forall w\in ES(\mathbf{v})\) in graph \(G\) implies \(g(w)\in ES(g\cdot\mathbf{v})\) in graph \(g\cdot G\)._

Some nature equivariant sets \(ES(v)\) including \(V(G)\), \(\mathcal{N}_{k}(v)\), and \(Q_{k}(v)\), etc. Let \(\mathcal{C}_{kfwl+}^{l}(\mathbf{v})\) be the color of tuple \(\mathbf{v}\) at iteration \(l\) for \(k\)-FWL+, we have:

\[k\text{-FWL+:}\quad\mathcal{C}_{kfwl+}^{l}(\mathbf{v})=\text{HASH}(\mathcal{C} _{kfwl+}^{l-1}(\mathbf{v}),\mathbb{\{}\!\{\!\Big{(}\mathcal{C}_{kfwl+}^{l-1}( \mathbf{u})|\mathbf{u}\in Q_{w}^{F}(\mathbf{v})\Big{)}\,|w\in ES(\mathbf{v}) \!\}\!\}).\] (7)

The key of \(k\)-FWL+ is that the equivariant set \(ES(\mathbf{v})\) can be any set of nodes as long as it is equivariant to any permutation \(g\in S_{n}\). For example, if \(ES(\mathbf{v})=V(G)\), the \(k\)-FWL+ will reduce to the original \(k\)-FWL. Instead, if \(ES(\mathbf{v})=\bigcup_{i=0}^{k}\mathcal{N}_{1}(v_{i})\), it becomes the localized \(k\)-FWL [12]. We can also design other more innovative equivariant sets \(ES(\mathbf{v})\). For example, denote \(\mathcal{SP}(\mathbf{v})\) to be a set that contains all nodes in the shortest paths between any \(v_{i},v_{j}\in\mathbf{v}\). It is still a valid equivariant set of tuple \(\mathbf{v}\). We can see that the design space of \(k\)-FWL+ is much broader than \(k\)-FWL.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

connected node sets. However, these approaches can still be impractical despite careful design. Our work hugely extends the design space of FWL-based methods and demonstrates that the resulting model can be expressive even with a lower space complexity cost.

**Permutation equivariant and invariant networks**. This line of research aims to leverage the permutation equivariant and invariant properties of graphs and devise corresponding architectures. For example, Maron et al. [35] proposed \(k\)-IGNs, which employ permutational equivariant linear layers with point-wise activation functions to the adjacency matrix. \(k\)-IGNs have been proven to have the same expressive power as \(k\)-WL [9; 36; 37]. Another approach utilizes the Reynold operator to consider all possible permutations on either global [15] or local [20] levels. However, they consider all possible permutations in order to achieve universality, which may lead to overfitting in real-world tasks and are more valuable from a theoretical perspective. In contrast, our framework is highly flexible and can be tailored to fit the complexity of real-world tasks.

**Feature-based methods**. Feature-based methods aim to incorporate graph-related features that cannot be computed by MPNNs. For instance, some methods add random features to nodes to break the symmetry [38; 39]. GSN [40] introduces substructure counting features into MPNNs. GD-WL [41] applies general distance features to enhance the expressiveness of MPNNs and shows it can solve the graph biconnectivity problem. GDGNN [23] integrates the geodesic information between node pairs and achieves a great balance between performance and efficiency. ESC-GNN [42] employs subgraph features to simulate subgraph GNNs without running them explicitly. Puny et al. [43] computes graph polynomial features and injects them into PPGN to achieve greater expressive power than 3-WL within \(O(n^{2})\) space. Although feature-based methods are highly efficient, they often suffer from overfitting in real-world tasks and produce suboptimal results.

## 6 Experiments

In this section, we conduct various experiments on synthetic and real-world tasks to verify the effectiveness of N\({}^{2}\)-GNN. The details of all experiments can be found in Appendix E. Additional experimental results and ablation studies are included in Appendix F. The source code is provided in https://github.com/JiaruiFeng/N2GNN.

### Expressive power

**Datasets**. To verify the expressive power of N\({}^{2}\)-GNN, we select four synthetic datasets, all containing graphs that cannot be distinguished by 1-WL/MPNNs. The datasets we selected are (1) EXP [38], which consists of 600 pairs of non-isomorphic graphs that 1-WL fails to distinguish; (2) CSL [15], which contains 150 4-regular graphs divided into 10 isomorphism classes; and (3) SR25 [44], which contains 15 non-isomorphic strongly regular graphs, each has 25 nodes. Even 3-WL is unable to distinguish between these graphs. (4) BREC [45], which contains 400 non-isomorphic graph pairs that range from 1-WL-indistinguishable to 4-WL-indistinguishable.

**Models**. For EXP, CSL, and SR25, we only report results for N\({}^{2}\)-GNN. For BREC, we compare N\({}^{2}\)-GNN with I\({}^{2}\)-GNN [19] as it is the current SOTA on BREC among all GNN methods.

**Results**. We report results in Table 1 and Table 2. For EXP and CSL, we report the average accuracy for 10-time cross-validation; For SR25, we report single-time accuracy. We can see N\({}^{2}\)-GNN achieves perfect results on all three datasets, empirically verifying its expressive power. Notably, N\({}^{2}\)-GNN is able to distinguish strongly regular graphs. This empirically verified Corollary 4.2. Moreover, N\({}^{2}\)-GNN achieves the best result on the BREC dataset among all GNNs methods, surpassing the previous SOTA, I\({}^{2}\)-GNN (See the complete comparison and further discussion in Appendix F). Despite this improved performance, N\({}^{2}\)-GNN only requires at most \(O(n^{2})\) space complexity, which is less than 3-WL. This further demonstrates the superiority of N\({}^{2}\)-GNN.

\begin{table}
\begin{tabular}{l c c c} \hline \hline Datasets & EXP & CSL & SR25 \\ \hline N\({}^{2}\)-GNN & 100 & 100 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Expressive power verification (Accuracy).

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{Basic Graphs (60)} & \multicolumn{2}{c}{Regular Graphs (140)} & \multicolumn{2}{c}{Extension Graphs (100)} & \multicolumn{2}{c}{Cf Graphs (100)} & \multicolumn{2}{c}{Total (400)} \\ \cline{2-9} Model & Number & Accuracy & Number & Accuracy & Number & Accuracy & Number & Accuracy & Number & Accuracy \\ \hline I\({}^{2}\)-GNN & 60 & 100\% & 100 & 71.4\% & 100 & 100\% & 21 & 21\% & 281 & 70.2\% \\ N\({}^{2}\)-GNN & 60 & 100\% & 100 & 71.4\% & 100 & 100\% & 27 & 27\% & **287** & **71.8\%** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Pair distinguishing accuracies on BREC

### Substructure counting

**Datasets.** To verify the substructure counting power of N\({}^{2}\)-GNN, we select the synthetic dataset from [25; 19]. The dataset consists of 5000 randomly generated graphs from different distributions, which are split into training, validation, and test sets with a ratio of 0.3/0.2/0.5. The task is to perform node-level counting regression. Specifically, we choose tailed-triangle, chordal cycles, 4-cliques, 4-paths, triangle-rectangle, 3-cycles, 4-cycles, 5-cycles, and 6-cycles as target substructures.

**Models**. We compare N\({}^{2}\)-GNN with the following baselines: Identity-aware GNN (ID-GNN) [28], Nested GNN (NGNN) [24], GNN-AK+ [25], PPGN [9], I\({}^{2}\)-GNN [19]. Results for all baselines are reported from [19].

**Results**. We report the results for counting substructures in Table 3. All results are the average normalized test MAE of three runs with different random seeds. We colored all results that are less than 0.01, which is an indication of successful counting, the same as in [19]. We can see that N\({}^{2}\)-GNN achieves MAE less than 0.01 among all different substructures/cycles, which empirically verified Theorem 4.3. Specifically, N\({}^{2}\)-GNN achieves the best result on 4-path counting and comparable performance to I\({}^{2}\)-GNN on most substructures. Moreover, N\({}^{2}\)-GNN can count 4-clique extremely well, which is theoretically and empirically infeasible for 3-WL [42] and PPGN [9].

### Molecular properties prediction

**Datasets**. To evaluate the performance of N\({}^{2}\)-GNN on real-world tasks, we select two popular molecular graphs datasets: QM9 [46; 50] and ZINC [51]. QM9 dataset contains over 130K molecules with 12 different molecular properties as the target regression task. The dataset is split into training, validation, and test sets with a ratio of 0.8/0.1/0.1. The ZINC dataset has two variants: ZINC-subset (12k graphs) and ZINC-full (250k graphs), and the task is graph regression. The training, validation, and test splits for the ZINC datasets are provided.

**Models**. For QM9, we report baseline results of DTNN and MPNN from [46]. We further adopt PPGN [9], NGNNs [24], KP-GIN\(\prime\)[22], I\({}^{2}\)-GNN [19]. We report results of PPGN, NGNN, and KP-GIN\(\prime\) from [22] and results of I\({}^{2}\)-GNN from [19]. For ZINC, the baseline including CIN [47], \(\delta\)-2-GNN [8], KC-SetGNN [10], PPGN [9], Graphormer-GD [41], GPS [48], Specformer [49], NGNN [24], GNN-AK-ctx [25], ESAN [34], SUN [27], KP-GIN\(\prime\)[22], I\({}^{2}\)-GNN [19], SSWL+ [12]. We report results of all baselines from [12; 43; 10; 48; 49; 19].

\begin{table}
\begin{tabular}{l|c c c c c c|c} \hline \hline Target & DTNN [46] & MPNN [46] & PPGN [9] & NGNN [24] & KP-GIN\({}^{2}\)[22] & I\({}^{2}\)-GNN [19] & N\({}^{2}\)-GNN \\ \hline \(\mu\) & 0.244 & 0.358 & **0.231** & 0.433 & 0.358 & 0.428 & 0.333 \\ \(\alpha\) & 0.95 & 0.89 & 0.382 & 0.265 & 0.233 & 0.230 & **0.193** \\ \(\varepsilon_{\text{HOMO}}\) & 0.00388 & 0.00541 & 0.00276 & 0.00279 & 0.00240 & 0.00261 & **0.00217** \\ \(\varepsilon_{\text{LUMO}}\) & 0.00512 & 0.00623 & 0.00287 & 0.00276 & 0.00236 & 0.00267 & **0.00210** \\ \(\Delta\varepsilon\) & 0.0112 & 0.0066 & 0.00406 & 0.00390 & 0.00333 & 0.00380 & **0.00304** \\ \(\langle R^{2}\rangle\) & 17.0 & 28.5 & 16.7 & 20.1 & 16.51 & 18.64 & **14.47** \\ \(Z\)PVE & 0.00172 & 0.00216 & 0.00064 & 0.00015 & 0.00017 & 0.00014 & **0.00013** \\ \(U_{0}\) & 2.43 & 2.05 & 0.234 & 0.205 & 0.0682 & 0.211 & **0.0247** \\ \(U\) & 2.43 & 2.00 & 0.234 & 0.200 & 0.0696 & 0.206 & **0.0315** \\ \(H\) & 2.43 & 2.02 & 0.229 & 0.249 & 0.0641 & 0.269 & **0.0182** \\ \(G\) & 2.43 & 2.02 & 0.238 & 0.253 & 0.0484 & 0.261 & **0.0178** \\ \(C_{v}\) & 0.27 & 0.42 & 0.184 & 0.0811 & 0.0869 & **0.0730** & 0.0760 \\ \hline \hline \end{tabular}
\end{table}
Table 4: MAE results on QM9 (smaller the better).

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline Target & ID-GNN [28] & NGNN [24] & GIN-AK+ [25] & PPGN [9] & I\({}^{2}\)-GNN [19] & N\({}^{2}\)-GNN \\ \hline Tailed Triangle & 0.1053 & 0.1044 & 0.0043 & 0.0026 & 0.0011 & 0.0025 \\ Chordal Cycle & 0.0454 & 0.0392 & 0.0112 & 0.0015 & 0.0010 & 0.0019 \\
4-Clique & 0.0026 & 0.0045 & 0.0049 & 0.1646 & 0.0003 & 0.0005 \\
4-Path & 0.0273 & 0.0244 & 0.0075 & 0.0041 & 0.0041 & 0.0042 \\ Tri.-Rec. & 0.0628 & 0.0729 & 0.1311 & 0.0144 & 0.0013 & 0.0055 \\
3-Cycles & 0.0006 & 0.0003 & 0.0004 & 0.0003 & 0.0003 & 0.0002 \\
4-Cycles & 0.0022 & 0.0013 & 0.0041 & 0.0009 & 0.0016 & 0.0024 \\
5-Cycles & 0.0490 & 0.0402 & 0.0133 & 0.0036 & 0.0028 & 0.0039 \\
6-Cycles & 0.0495 & 0.0439 & 0.0238 & 0.0071 & 0.0082 & 0.0075 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation on Counting Substructures (norm MAE), cells with MAE less than 0.01 are colored.

## 7 Limitations, future works, and conclusions

**Limitations:** For \((k,t)\)-FWL, although we can control the space complexity by fixing the \(k\), the time complexity of \((k,t)\)-FWL will grow exponentially with the increase of \(t\) (as discussed in Appendix B). Therefore, the resulting model can still be impractical when \(t\) is large. We believe this result is evidence of the "no free lunch" in developing a more expressive GNN model. For \((k,t)\)-FWL+, although we demonstrate its capability by implementing many instances corresponding to different existing GNN models, it is still unclear what is the whole space of \((k,t)\)-FWL+, especially the space of \(ES(\mathbf{v})\). Moreover, the quantitative expressiveness analysis of \((k,t)\)-FWL+ is still unexplored. Finally, for N\({}^{2}\)-GNN, the practical complexity can still be unbearable, especially for dense graphs if the aggregation is implemented in a parallel way, as we need to aggregate many more neighbors for each tuple than normal MPNN. It also introduces the optimization issue on N\({}^{2}\)-GNN, which makes it hard to achieve its theoretical expressiveness. In Appendix B, C, and D, we provide more detailed discussion on the limitation of \((k,t)\)-FWL, \((k,t)\)-FWL+, and N\({}^{2}\)-GNN, respectively.

**Future works**: There are several directions that are worth further exploration. First, can we characterize the whole space of \(ES(\mathbf{v})\) and thus can theoretically and quantitatively analyze the expressive power of \((k,t)\)-FWL+. Further, how to design advanced model architecture to achieve the theoretical power of \((k,t)\)-FWL+. Finally, can we provide a systematic implementation code package and practical usage of different downstream tasks such that researchers can easily develop an instance of \((k,t)\)-FWL+ that can best fit their downstream tasks? We leave these to our future work.

**Conclusions:** In this work, we propose \((k,t)\)-FWL+, a flexible and powerful extension of \(k\)-FWL. First, \((k,t)\)-FWL+ expands the tuple aggregation style in \(k\)-FWL. We theoretically prove that given any fixed pace complexity of \(O(n^{k})\), \((k,t)\)-FWL+ can still construct an expressiveness hierarchy up to solving the graph isomorphism problem. Second, \((k,t)\)-FWL+ extends the global neighborhood definition in \(k\)-FWL to any equivariant set, which enables a finer-grained design space. We show that the \((k,t)\)-FWL+ framework can implement many existing powerful GNN models with matching expressiveness. We further implement an instance named N\({}^{2}\)-GNN which is both practically powerful and theoretically expressive. Theoretically, it partially outperforms 3-WL but only requires \(O(n^{2})\) space. Empirically, it achieves new SOTA on BREC, ZINC-Subset, and ZINC-Full datasets. We envision \((k,t)\)-FWL+ can serve as a promising general framework for designing highly effective GNNs tailored to real-world problems.

## 8 Acknowledgments

Jiarui Feng, Lecheng Kong, Hao Liu, and Yixin Chen are supported by NSF grant CBE-2225809. Muhan Zhang is partially supported by the National Natural Science Foundation of China (62276003) and Alibaba Innovative Research Program.

\begin{table}
\begin{tabular}{l|c|c c} \hline \hline Model & \# Param & \begin{tabular}{c} ZINC-Subset \\ Test MAE \\ \end{tabular} & 
\begin{tabular}{c} ZINC-Full \\ Test MAE \\ \end{tabular} \\ \hline CIN [47] & -100k & 0.079 \(\pm\) 0.006 & **0.022 \(\pm\) 0.002** \\ \hline \(\delta\)-2-GNN [8] & - & - & 0.042 \(\pm\) 0.003 \\ KC-SetGNN [10] & - & 0.075 \(\pm\) 0.003 & - \\ PPGN [9] & - & 0.079 \(\pm\) 0.005 & 0.022 \(\pm\) 0.003 \\ \hline GD-WL [41] & 503k & 0.081 \(\pm\) 0.009 & 0.025 \(\pm\) 0.004 \\ GPS [48] & 424k & 0.070 \(\pm\) 0.004 & - \\ Spectrometer [49] & -500k & 0.066 \(\pm\) 0.003 & - \\ \hline NGNN [24] & -500k & 0.111 \(\pm\) 0.003 & 0.029 \(\pm\) 0.001 \\ GNN-AK [25] & -500k & 0.093 \(\pm\) 0.002 & - \\ ESAN [34] & 446k & 0.097 \(\pm\) 0.006 & 0.025 \(\pm\) 0.003 \\ SUN [27] & 526k & 0.083 \(\pm\) 0.003 & 0.024 \(\pm\) 0.003 \\ KP-GIN\({}^{2}\)[22] & 489k & 0.093 \(\pm\) 0.007 & - \\ I\({}^{2}\)-GNN [19] & - & 0.083 \(\pm\) 0.001 & 0.023 \(\pm\) 0.001 \\ SSWL+ \{1.2\} & 387k & 0.070 \(\pm\) 0.005 & **0.022 \(\pm\) 0.002** \\ \hline N\({}^{2}\)-GNN & 316k/414k & **0.059 \(\pm\) 0.002** & **0.022 \(\pm\) 0.002** \\ \hline \hline \end{tabular}
\end{table}
Table 5: MAE results on ZINC (smaller the better).

## References

* Kipf and Welling [2017] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations_, 2017. URL https://openreview.net/forum?id=SJU4ayYgl.
* Hamilton et al. [2017] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In _Advances in Neural Information Processing Systems_, pages 1025-1035, 2017.
* Velickovic et al. [2018] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=rJXMpkCZ.
* Gilmer et al. [2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* Xu et al. [2018] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2018.
* Morris et al. [2019] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, pages 4602-4609, 2019.
* Weisfeiler and Leman [1968] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. _NTI, Series_, 2(9):12-16, 1968.
* Morris et al. [2020] Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and leman go sparse: Towards scalable higher-order graph embeddings. _Advances in neural information processing systems_, 33:21824-21840, 2020.
* Maron et al. [2019] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _Advances in neural information processing systems_, 32, 2019.
* Zhao et al. [2022] Lingxiao Zhao, Neil Shah, and Leman Akoglu. A practical, progressively-expressive GNN. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=WBV9Z6qpA8x.
* Morris et al. [2023] Christopher Morris, Floris Geerts, Jan Tonshoff, and Martin Grohe. WL meet VC. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 25275-25302. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/morris23a.html.
* Zhang et al. [2023] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph GNNs via subgraph weisfeiler-lehman tests. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 41019-41077. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/zhang23k.html.
* Cai et al. [1992] Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables for graph identification. _Combinatorica_, 12(4):389-410, 1992.
* Grohe [2017] Martin Grohe. _Descriptive Complexity, Canonisation, and Definable Graph Structure Theory_. Lecture Notes in Logic. Cambridge University Press, 2017. doi: 10.1017/9781139028868.
* Murphy et al. [2019] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 4663-4673, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/v97/murphy19a.html.

* Zhou et al. [2023] Cai Zhou, Xiyuan Wang, and Muhan Zhang. From relational pooling to subgraph GNNs: A universal framework for more expressive graph neural networks. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 42742-42768. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/zhou23n.html.
* Qian et al. [2022] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=w0QoqmU79vJ.
* Arvind et al. [2020] V. Arvind, Frank Fuhlbruck, Johannes Kobler, and Oleg Verbitsky. On Weisfeiler-Leman invariance: Subgraph counts and related graph properties. _Journal of Computer and System Sciences_, 113:42-59, November 2020. ISSN 0022-0000. doi: 10.1016/j.jcss.2020.04.003. URL https://www.sciencedirect.com/science/article/pii/S0022000020300386.
* Huang et al. [2023] Yinan Huang, Xingang Peng, Jianzhu Ma, and Muhan Zhang. Boosting the cycle counting power of graph neural networks with is*2S-GNNs. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=kDSmxOspsXQ.
* Chen et al. [2020] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? _Advances in neural information processing systems_, 33:10383-10395, 2020.
* Wijesinghe and Wang [2022] Asiri Wijesinghe and Qing Wang. A new perspective on "how graph neural networks go beyond weisfeiler-lehman?". In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=uxgg9o7b1_3.
* Feng et al. [2022] Jiarui Feng, Yixin Chen, Fuhai Li, Anindya Sarkar, and Muhan Zhang. How powerful are k-hop message passing graph neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=nN3aVR0sxGd.
* Kong et al. [2022] Lecheng Kong, Yixin Chen, and Muhan Zhang. Geodesic graph neural network for efficient graph representation learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=6pC50tP7eBx.
* Zhang and Li [2021] Muhan Zhang and Pan Li. Nested graph neural networks. _Advances in Neural Information Processing Systems_, 34:15734-15747, 2021.
* Zhao et al. [2022] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any GNN with local structure awareness. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Nspk_WYKoEH.
* 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 8523-8527, 2021. doi: 10.1109/ICASSP39728.2021.9414015.
* Frasca et al. [2022] Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron. Understanding and extending subgraph GNNs by rethinking their symmetries. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=sc7bBHAmcN.
* You et al. [2021] Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 10737-10745, 2021.
* Li et al. [2020] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. _Advances in Neural Information Processing Systems_, 33, 2020.

* [30] Pal Andras Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. DropGNN: Random dropouts increase the expressiveness of graph neural networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=fPQojkIVSq8.
* [31] Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=ZKb24mebI91.
* [32] Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=Hcr9mgBG6ds.
* [33] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In _Advances in Neural Information Processing Systems_, pages 5165-5175, 2018.
* [34] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=dFbKQaRk15w.
* [35] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=Syx72jC9tm.
* [36] Waiss Azizian and marc lelarge. Expressive power of invariant and equivariant graph neural networks. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=lxHgXYN4bwl.
* [37] Floris Geerts. The expressive power of kth-order invariant graph networks. _ArXiv_, abs/2007.12035, 2020.
* [38] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In _Proceedings of the Thirtieth International Joint Conference on Artifical Intelligence (IJCAI)_, 2021.
* [39] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. In _Proceedings of the 2021 SIAM International Conference on Data Mining, SDM_, 2021.
* [40] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting, 2021. URL https://openreview.net/forum?id=LTOKSFnQDWF.
* [41] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of GNNs via graph biconnectivity. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=r9hNv76KoT3.
* [42] Zuoyu Yan, Junru Zhou, Liangcai Gao, Zhi Tang, and Muhan Zhang. Efficiently counting substructures by subgraph gnns without running gnn on subgraphs, 2023.
* [43] Omri Puny, Derek Lim, Bobak T. Kiani, Haggai Maron, and Yaron Lipman. Equivariant polynomials for graph neural networks, 2023.
* [44] Muhammet Balcilar, Pierre Heroux, Benoit Guizere, Pascal Vasseur, Sebastien Adam, and Paul Honeine. Breaking the limits of message passing graph neural networks. In _Proceedings of the 38th International Conference on Machine Learning (ICML)_, 2021.
* [45] Yanbo Wang and Muhan Zhang. Towards better evaluation of gnn expressiveness with brec dataset, 2023.

* Wu et al. [2018] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* Bodnar et al. [2021] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yu Guang Wang, Pietro Lio, Guido Montufar, and Michael M. Bronstein. Weisfeiler and lehman go cellular: CW networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL https://openreview.net/forum?id=uWPZCMVtsSG.
* Rampasek et al. [2022] Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. _Advances in Neural Information Processing Systems_, 35, 2022.
* Bo et al. [2023] Deyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral graph neural networks meet transformers. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=OptSt3oyJa1.
* Ramakrishnan et al. [2014] Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. _Scientific data_, 1(1):1-7, 2014.
* Dwivedi et al. [2020] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _arXiv preprint arXiv:2003.00982_, 2020.
* Fey and Lenssen [2019] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In _ICLR Workshop on Representation Learning on Graphs and Manifolds_, 2019.

## Appendix

### Table of Contents

* A Additional preliminaries
* A.1 Additional definitions
* A.2 More on Weisfeiler-Lehman test and Folklore Weisfeiler-Lehman test
* B Additional discussion on \((k,t)\)-FWL
* B.1 Detailed proofs for \((k,t)\)-FWL
* B.2 Complexity analysis
* B.3 Limitations
* C Additional discussion on \((k,t)\)-FWL+
* C.1 Detailed proofs for \((k,t)\)-FWL+
* C.2 Discussion on the complexity
* C.3 Limitations
* D Additional discussion on N\({}^{2}\)-FWL and N\({}^{2}\)-GNN
* D.1 Detailed proofs for N\({}^{2}\)-FWL
* D.2 Complexity analysis
* D.3 Model implementation details
* D.4 Limitations
* E Experimental details
* E.1 Dataset statistics
* E.2 Experimental details
* F Additional experimental results
* F.1 Ablation studies for \((k,t)\)-FWL+
* F.2 Practical complexity of N\({}^{2}\)-GNN
* F.3 Additional discussion on BREC experiment
* F.4 Ablation studies for N\({}^{2}\)-GNN
Additional preliminaries

In this section, we provide additional preliminaries.

### Additional definitions

**Definition A.1**.: When we say we **select a \(t\)-tuple**\(\mathbf{w}^{\prime}=(w^{\prime}_{1},w^{\prime}_{2},\cdots,w^{\prime}_{t})\) from a \(p\)-tuple \(\mathbf{v}=(v_{1},v_{2},\cdots,v_{p})\) (\(p\geq t\)), it means that we select \(t\) unique indices \((i_{1},i_{2},\cdots,i_{t})\) from \([p]\) such that \(i_{1}<i_{2}<\cdots<i_{t}\). Next, to construct \(\mathbf{w}^{\prime}\), we let \(w^{\prime}_{1}=v_{i_{1}},\ldots,w^{\prime}_{t}=v_{i_{t}}\). When we say we **select all possible \(t\)-tuple**\(\mathbf{w}^{\prime}\) from a \(p\)-tuple \(\mathbf{v}\), we enumerate all possible \(t\) indices and construct \(t\)-tuples correspondingly. There are \(\left(\begin{array}{c}p\\ t\end{array}\right)\) different selection results and we denote \(\tilde{P}_{t}(\mathbf{v})\) as the set of all possible selections. Finally, when we say **select all possible \(t\)-tuple**\(\mathbf{w}^{\prime}\) from a \(p\)-tuple \(\mathbf{v}\)**with order**, we further assume that the selection process is taken with a predefined order such that the sequence of selected \(t\) indices will be the same if we repeat the selection. For example, we can always assume we start from indices \((1,2,\ldots,t)\). Next, fix other indices and enumerate all possible \(i_{t}\) in ascending order. Then, increase \(i_{t-1}\) by 1 and repeat the procedure. If we select all possible 2-tuple from \((v_{1},v_{2},v_{3})\), the result is \(((v_{1},v_{2}),(v_{1},v_{3}),(v_{2},v_{3}))\). We denote \(P_{t}(\mathbf{v})\) as the tuple of the selection result.

**Definition A.2**.: **Neighborhood tuple**\(Q^{F}_{\mathbf{w}}(\mathbf{v})\) is a generalization of neighbor set \(Q^{F}_{w}(\mathbf{v})\) in \(k\)-FWL. Briefly speaking, the neighborhood tuple enumerates all possible combinations of elements in \(\mathbf{v}\) and \(\mathbf{w}\) follows a pre-defined order. Suppose the length of the tuple is \(k\) and \(t\) for \(\mathbf{v}\) and \(\mathbf{w}\), respectively. To construct the neighborhood tuple \(Q^{F}_{\mathbf{w}}(\mathbf{v})\), we enumerate \(m\) from 0 to \(min(k,t)\). For each \(m\), we construct a sub-tuple, and the final neighborhood tuple \(Q^{F}_{\mathbf{w}}(\mathbf{v})\) is the concatenation of all sub-tuples. Given a particular \(m\), the sub-tuple is constructed by selecting all possible \(m\)-tuple \(\mathbf{w}^{\prime}=(w^{\prime}_{1},\cdots,w^{\prime}_{m})\) from \(t\)-tuple \(\mathbf{w}\) with order to construct a tuple \(P_{m}(\mathbf{w})\). At the same time, we select all possible \(m\) indices \(\mathbf{i}^{\prime}=(i^{\prime}_{1},\ldots,i^{\prime}_{m})\) from \(\mathbf{i}=(1,2,\ldots,k)\) with order to construct another tuple \(P_{m}(\mathbf{i})\). Finally, for each \((\mathbf{w}^{\prime},\mathbf{i}^{\prime})\in P_{m}(\mathbf{w})\times P_{m}( \mathbf{i})\), we add a tuple to the resulted sub-tuple by replacing \(v_{i^{\prime}_{e}}\) with \(w^{\prime}_{o}\) for \(o=1,2,\ldots,m\). The neighborhood tuple is constructed by concatenating all sub-tuples. The length of the neighborhood tuple is \(\sum_{m=0}^{min(k,t)}\left(\begin{array}{c}k\\ m\end{array}\right)\times\left(\begin{array}{c}t\\ m\end{array}\right)\).

**Definition A.3**.: Let \(F_{1}\) and \(F_{2}\) be two color refinement algorithms, and denote \(\mathcal{C}_{i}(G),i\in\{1,2\}\) as the color histogram computed by \(F_{i}\) for graph \(G\) after convergence. We say:

* \(F_{1}\) is **more powerful** than \(F_{2}\), denoted as \(F_{2}\preceq F_{1}\), if for any pair of graph \(G\) and \(H\), \(\mathcal{C}_{1}(G)=\mathcal{C}_{1}(H)\) also implies \(\mathcal{C}_{2}(G)=\mathcal{C}_{2}(H)\).
* \(F_{1}\) is **as powerful as \(F_{2}\)**, denoted as \(F_{1}\simeq F_{2}\), if we have \(F_{1}\preceq F_{2}\) and \(F_{2}\preceq F_{1}\).
* \(F_{1}\) is **strictly more powerful** than \(F_{2}\), denoted as \(F_{2}\prec F_{1}\), if we have \(F_{2}\preceq F_{1}\) and there exist graphs \(G\) and \(H\) such that \(\mathcal{C}_{1}(G)\neq\mathcal{C}_{1}(H)\) and \(\mathcal{C}_{2}(G)=\mathcal{C}_{2}(H)\).
* \(F_{1}\) and \(F_{2}\) are **incomparable**, denoted as \(F_{1}\nsim F_{2}\), if neither \(F_{1}\preceq F_{2}\) nor \(F_{2}\preceq F_{1}\) holds.

_Remark A.4_.: To prove that \(F_{1}\) is more powerful than \(F_{2}\), one way is to show that the color of node/tuple cannot be further refined by \(F_{2}\) if the color output from \(F_{1}\) is already stable (convergence). This was also indicated in Remark B.2 in [12]. Note that the initial color of \(F_{1}\) should be at least as powerful as the initial color of \(F_{2}\). Namely, if we use the initial color to compute the color histogram, we have \(F_{2}\preceq F_{1}\).

### More on Weisfeiler-Lehman test and Folklore Weisfeiler-Lehman test

First, we restate the color update equation of both \(k\)-WL and \(k\)-FWL.

\[\text{WL:}\quad\mathcal{C}^{l}_{kwl}(\mathbf{v})=\text{HASH}\left( \mathcal{C}^{l-1}_{kwl}(\mathbf{v}),\left(\llbracket\mathcal{C}^{l-1}_{kwl}( \mathbf{u})|\mathbf{u}\in Q_{j}(\mathbf{v})\rrbracket\,j\in[k]\right)\right),\] \[\text{FWL:}\quad\mathcal{C}^{l}_{kfwl}(\mathbf{v})=\text{HASH} \left(\mathcal{C}^{l-1}_{kfwl}(\mathbf{v}),\llbracket\Big{(}\mathcal{C}^{l-1}_{ kfwl}(\mathbf{u})|\mathbf{u}\in Q^{F}_{w}(\mathbf{v})\Big{)}\,|w\in V(G) \rrbracket\right).\]

The first step of both \(k\)-WL and \(k\)-FWL is to encoder the isomorphic type of tuple \(\mathbf{v}=(v_{1},v_{2},...,v_{k})\) as initial color such that for any two tuples \(\mathbf{u},\mathbf{v}\in v^{k}(G)\), \(\mathcal{C}^{0}_{kfwl}(\mathbf{v})=\mathcal{C}^{0}_{kfwl}(\mathbf{u})\) and \(\mathcal{C}^{0}_{kwl}(\mathbf{v})=\mathcal{C}^{0}_{kfwl}(\mathbf{u})\).

\(\mathcal{C}^{0}_{kwl}(\mathbf{u})\) if and only if the ordered subgraphs induced by two tuples have the same isomorphism type. Namely, \(\forall i,j\in[k]\) (1) \(l_{G}(v_{i})=l_{G}(u_{i})\) and (2) \((v_{i},v_{j})\in E(G)\iff(u_{i},u_{j})\in E(G)\). Another thing is that the HSAH here is an injective function. As long as the input is different, the output of HASH is different. This will serve as a basic concept in the later proof. The algorithm converge means that \(\forall\mathbf{v}\in V^{k}(G)\), we have \(\mathcal{C}^{l+1}_{kwl}(\mathbf{v})=\mathcal{C}^{l}_{kwl}(\mathbf{v})\) or \(\mathcal{C}^{l+1}_{kful}(\mathbf{v})=\mathcal{C}^{l}_{kfwl}(\mathbf{v})\). Denote the convergent iteration as \(\infty\). After the algorithm convergence, the color histogram can be computed by another injective function:

\[\mathcal{C}_{kfwl}(G)=\text{HASH}(\{\!\!\{\mathcal{C}^{\infty}_{kfwl}(\mathbf{ v})|\mathbf{v}\in V^{k}(G)\!\!\}\!\}).\] (12)

Here we use \(k\)-FWL as an example, but the procedure for other color refinement algorithms is the same.

## Appendix B Additional discussion on \((k,t)\)-Fwl

### Detailed proofs for \((k,t)\)-Fwl

In this section, we provide all detailed proofs related to \((k,t)\)-FWL. Let \(\mathbf{p}=(\mathbf{v},\mathbf{w})\) be a new tuple that is the concatenation of a \(k\)-tuple \(\mathbf{v}\) and a \(t\)-tuple \(\mathbf{w}\). Let \(\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p})\) be the isomorphic type of tuple \(\mathbf{p}\). We start to prove the expressive power of \((k,t)\)-FWL with the first lemma:

**Lemma B.1**.: _Suppose \(k\geq 2\) and \(t\geq 1\). Given two graphs \(G\), \(H\), two tuples \(\mathbf{v}_{1}\in V^{k}(G),\!\mathbf{v}_{2}\in V^{k}(H)\), and another two tuples \(\mathbf{w}_{1}\in V^{t}(G),\!\mathbf{w}_{2}\in V^{t}(H)\). Let \(\mathbf{p}_{1}=(\mathbf{v}_{1},\mathbf{w}_{1})\), \(\mathbf{p}_{2}=(\mathbf{v}_{2},\mathbf{w}_{2})\). We have \(\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{0}_{(k+t)wl}(\mathbf{ p}_{2})\iff\left(\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{1})|\mathbf{u}_{1}\in Q ^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\right)=\left(\mathcal{C}^{0}_{ktfwl}( \mathbf{u}_{2})|\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\right)\)._

Proof.: First, we prove the forward direction. That is, if \(\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{0}_{(k+t)wl}(\mathbf{ p}_{2})\), we have \(\left(\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{1})|\mathbf{u}_{1}\in Q^{F}_{ \mathbf{w}_{1}}(\mathbf{v}_{1})\right)=\left(\mathcal{C}^{0}_{ktfwl}(\mathbf{ u}_{2})|\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\right)\). Recall the definition of the initial color, if \(\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{0}_{(k+t)wl}(\mathbf{ p}_{2})\), we must have the ordered subgraph induced by \(\mathbf{p}_{1}\) is isomorphic to the ordered subgraph induced by \(\mathbf{p}_{2}\). Namely, \(\forall i,j\in[k+t]\), \((p_{1i},p_{1j})\in E(G)\iff(p_{2i},p_{2j})\in E(H)\) and \(l_{G}(p_{1i})=l_{G}(p_{2i})\). Next, given Definition A.2, we know that for any neighborhood tuple, there exists a single indices mapping function \(O\) that given \(\mathbf{p}=(\mathbf{v},\mathbf{w})\), for any \(\mathbf{u}\in Q^{F}_{\mathbf{w}}(\mathbf{v})\), we have \(O(\mathbf{u},Q^{F}_{\mathbf{w}}(\mathbf{v}))=(o_{1},\ldots,o_{k})\) such that \((u_{1},\ldots,u_{k})=(p_{o_{1}},\ldots,p_{o_{k}})\). Further, the construction of the neighborhood tuple follows the same ordering rule no matter the input. This means that, for both \(\mathbf{u}_{1}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\), as long as \(\mathbf{u}_{1}\) and \(\mathbf{u}_{2}\) are in the same position of neighborhood tuple \(Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\), we must have \(O(\mathbf{u}_{1},Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1}))=O(\mathbf{u}_{2},Q^{F }_{\mathbf{w}_{2}}(\mathbf{v}_{2}))=(o_{1},\ldots,o_{k})\). Finally, as for any \(o_{i}\) and \(o_{j}\), we have \((p_{1o_{i}},p_{1o_{j}})\in E(G)\iff(p_{2o_{i}},p_{2o_{j}})\in E(H)\), \(l_{G}(p_{1o_{i}})=l_{G}(p_{2o_{i}})\), and \(l_{G}(p_{1o_{j}})=l_{G}(p_{2o_{j}})\), we conclude that \(\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{1})=\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{2})\). As it is the same for any \(\mathbf{u}_{1}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\), it must holds that \(\left(\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{1})|\mathbf{u}_{1}\in Q^{F}_{\mathbf{ w}_{1}}(\mathbf{v}_{1})\right)=\left(\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{2})| \mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\right)\). The proof of the backward direction is similar to the forward direction. As for any \(i,j\in[k+t]\), we can find \(p_{1i},p_{1j}\) exist in some \(\mathbf{u}_{1}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(p_{2i},p_{2j}\) in the corresponding \(\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\). Since we have \(\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{1})=\mathcal{C}^{0}_{ktfwl}(\mathbf{u}_{2})\), we can easily conclude that \((p_{1i},p_{1j})\in E(G)\iff(p_{2i},p_{2j})\in E(G)\), \(l_{G}(p_{1i})=l_{G}(p_{2i})\), and \(l_{G}(p_{1j})=l_{G}(p_{2j})\). This concludes the proof. 

Lemma B.1 indicates that \((k,t)\)-FWL can recover the isomorphism type of high-order tuple, which is impossible in the original \(k\)-FWL. Therefore, as long as \(t\) is large enough, we can recover the isomorphism type of the whole graph. Further, Lemma B.1 also implies that we can resort to high-order tuples to prove the expressive power of \((k,t)\)-FWL. Define the \(H_{t}(\mathbf{v})=\{(\mathbf{v},\mathbf{w})|\mathbf{w}\in V^{t}(G)\}\), we slightly rewrite Equation (6):

\[\mathcal{C}^{l}_{ktfwl}(\mathbf{p})=\text{HASH}\left(\left(\mathcal{C}^{l-1}_{ ktfwl}(\mathbf{u})|\mathbf{u}\in Q^{F}_{\mathbf{w}}(\mathbf{v})\right)\right),\] (13) \[\mathcal{C}^{l}_{ktfwl}(\mathbf{v})=\text{HASH}\left(\mathcal{C}^{l-1 }_{ktfwl}(\mathbf{v}),\{\!\!\{\mathcal{C}^{l}_{ktfwl}(\mathbf{p})|\mathbf{p}\in H _{t}(\mathbf{v})\!\!\}\!\}_{t}\right).\] (14)It is easy to verify the rewritten will not affect the expressive power of \((k,t)\)-FWL. Now, we are ready to prove the Proposition 3.1 in the main paper. We restate it here:

**Proposition B.2**.: _For \(k\geq 2\) and \(t\geq 1\), if \(t\geq n-k\), \((k,t)\)-FWL can solve the graph isomorphism problems with the size of the graph less than or equal to \(n\)._

Proof.: Here we show that with only 1 iteration, \((k,t)\)-FWL can solve the graph isomorphism problem with the size of graph \(n\leq t+k\). Given Lemma B.1, for any \(\mathbf{v}\in V^{k}(G)\), we have \(\mathcal{C}^{1}_{ktfwl}(\mathbf{p}_{1})=\mathcal{C}^{1}_{ktfwl}(\mathbf{p}_ {2})\iff\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{0}_{(k+t)wl}( \mathbf{p}_{2})\) for any \(\mathbf{p}_{1},\mathbf{p}_{2}\in H_{t}(\mathbf{v})\). Therefore \(\{\!\!\{\mathcal{C}^{1}_{ktfwl}(\mathbf{p})|\mathbf{p}\in H_{t}(\mathbf{v})\}\!\!\}\) is equivalent to \(\{\!\!\{\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p})|\mathbf{p}\in H_{t}(\mathbf{v})\}\!\!\}\), which is essentially the multiset of isomorphism type of \(\mathbf{p}\in H_{t}(\mathbf{v})\). According to Equation (14), \(\mathcal{C}^{1}_{ktfwl}(\mathbf{v})\) can injectively encode this information. Next, the color histogram of a graph is computed by \(\mathcal{C}_{ktfwl}(G)=\text{HASH}(\{\!\!\{\mathcal{C}^{1}_{ktfwl}(\mathbf{v} )|\mathbf{v}\in V^{k}(G)\}\!\!\})\). It is easy to verify that \(\mathcal{C}_{ktfwl}(G)\) can encode the isomorphism type of all \((k+t)\)-tuple \(\mathbf{p}\in V^{k+t}(G)\). Then, if the graph has size \(n\leq k+t\), \(\mathcal{C}_{ktfwl}(G)\) can give different color histograms for any non-isomorphic graph pairs. This concludes the proof. 

Before we prove the relationship between \((k,t)\)-FWL and \((k+t)\)-WL, we first show some properties of \((k+t)\)-WL. Let \(\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{p})\) be the stable color of tuple \(\mathbf{p}\in V^{k+t}(G)\) output from \((k+t)\)-WL. We first show that:

**Lemma B.3**.: _Suppose \(k\geq 2\), \(t\geq 1\). Given a graph \(G\), for any \(\mathbf{v}_{1},\mathbf{v}_{2}\in V^{k}(G)\), \(\mathbf{w}_{1},\mathbf{w}_{2}\in V^{t}(G)\), \(\mathbf{p}_{1}=(\mathbf{v}_{1},\mathbf{w}_{1}),\mathbf{p}_{2}=(\mathbf{v}_{2 },\mathbf{w}_{2})\), \(\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{\infty}_{(k+t)wl} (\mathbf{p}_{2})\) implies that \(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{q}_{1})|\mathbf{q}_{1}\in H_{t }(\mathbf{v}_{1})\}\!\!\}_{t}=\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{ q}_{2})|\mathbf{q}_{2}\in H_{t}(\mathbf{v}_{2})\}\!\!\}_{t}\)._

Proof.: Let \(\mathbf{p}_{1}=(p_{11},\ldots,p_{1(k+t)}),\mathbf{p}_{2}=(p_{21},\ldots,p_{2( k+t)})\). As we have \(\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{\infty}_{(k+t)wl}( \mathbf{p}_{2})\), given it is the stable color, we must have \(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}((p_{11},\ldots,p_{1k},w,p_{1(k+2)}, \ldots,p_{1(k+t)}))|w\in V(G)\}\!\!\}=\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}((p _{21},\ldots,p_{2k},w,p_{2(k+2)},\ldots,p_{2(k+t)}))|w\in V(G)\}\!\!\}\). Otherwise, by doing one more iteration, we will have \(\mathcal{C}^{\infty+1}_{(k+t)wl}(\mathbf{p}_{1})\neq\mathcal{C}^{\infty+1}_{( k+t)wl}(\mathbf{p}_{2})\), which is a contradiction. Further, we can do another unrolling to have \(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}((p_{11},\ldots,p_{1k},w_{1},w_{2}, \ldots,p_{1(k+t)}))|w_{2}\in V(G)\}\!\!\}|w_{1}\in V(G)\}\!\!\}=\{\{\!\!\{ \mathcal{C}^{\infty}_{(k+t)wl}((p_{21},\ldots,p_{2k},w_{1},w_{2},\ldots,p_{2(k+ t)}))|w_{2}\in V(G)\}\!\!\}|w_{1}\in V(G)\}\!\!\}\). This implies that \(\{\!\!\{\!\mathcal{C}^{\infty}_{(k+t)wl}((p_{11},\ldots,p_{1k},w_{1},w_{2}, \ldots,p_{1(k+t)}))|(w_{1},w_{2})\in V^{2}(G)\}\!\!\}_{2}=\{\!\!\{\mathcal{C}^{ \infty}_{(k+t)wl}((p_{21},\ldots,p_{2k},w_{1},w_{2},\ldots,p_{2(k+t)}))|(w_{1},w_{2})\in V^{2}(G)\}\!\!\}_{2}\) Therefore, by iteratively doing the unrolling, we must have \(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}((p_{11},\ldots,p_{1k},w_{1},\ldots,w_{t})) |(w_{1},\ldots,w_{t})\in V^{t}(G)\}\!\!\}_{t}=\{\!\!\{\mathcal{C}^{\infty}_{(k+t) wl}((p_{21},\ldots,p_{2k},w_{1},\ldots,w_{t}))|(w_{1},\ldots,w_{t})\in V^{t}(G)\}\!\!\}_{t}\), which is exactly \(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{q}_{1})|\mathbf{q}_{1}\in H_{t }(\mathbf{v}_{1})\}\!\!\}_{t}=\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{ q}_{2})|\mathbf{q}_{2}\in H_{t}(\mathbf{v}_{2})\}\!\!\}_{t}\). This concludes the proof. 

Lemma B.3 indicates that if the \((k,t)\)-WL converges, the hierarchical set will not increase the expressive power of \((k+t)\)-WL. Based on this observation, we can slightly rewrite the equation for computing the graph color histogram of \((k+t)\)-WL:

\[\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{v}) =\text{HASH}(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{p})| \mathbf{p}\in H_{t}(\mathbf{v})\}\!\!\}_{t}),\] (15) \[\mathcal{C}^{\infty}_{(k+t)wl}(G) =\text{HASH}(\{\!\!\{\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{v})| \mathbf{v}\in V^{k}(G)\}\!\!\}),\] (16)

where \(\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{v})\) is the stable color of \(k\)-tuple \(v\). Here we change the computation of graph color histogram by first hierarchically pooling all tuples in \(H_{t}(\mathbf{v})\) to get a color for \(\mathbf{v}\) and then pooling all tuples \(\mathbf{v}\in V^{k}(G)\). It is straightforward that the rewritten will not change the expressive power of \((k+t)\)-WL given Lemma B.3. Equation (15) and Equation (16) will be used in the latter proofs. Not surprisingly, we have similar result for \((k,t)\)-FWL:

**Lemma B.4**.: _Suppose \(k\geq 2\), \(t\geq 1\). Given a graph \(G\), for any \(\mathbf{v}_{1},\mathbf{v}_{2}\in V^{k}(G)\), \(\mathbf{w}_{1},\mathbf{w}_{2}\in V^{t}(G)\), \(\mathbf{p}_{1}=(\mathbf{v}_{1},\mathbf{w}_{1}),\mathbf{p}_{2}=(\mathbf{v}_{2}, \mathbf{w}_{2})\), \(\mathcal{C}^{\infty}_{ktfwl}(\mathbf{p}_{1})=\mathcal{C}^{\infty}_{ktfwl}( \mathbf{p}_{2})\) implies that \(\{\!\!\{\mathcal{C}^{\infty}_{ktfwl}(\mathbf{q}_{1})|\mathbf{q}_{1}\in H_{t }(\mathbf{v}_{1})\}\!\!\}_{t}=\{\!\!\{\mathcal{C}^{\infty}_{ktfwl}(\mathbf{ q}_{2})|\mathbf{q}_{2}\in H_{t}(\mathbf{v}_{2})\}\!\!\}_{t}\)Proof.: Given the definition of the neighborhood tuple, we must have \(\mathbf{v}_{1}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(\mathbf{v}_{2}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{2})\). This means we also have \(\mathcal{C}^{\infty}_{k\mathit{fwl}}(\mathbf{v}_{1})=\mathcal{C}^{\infty}_{ k\mathit{fwl}}(\mathbf{v}_{2})\). Otherwise, by doing one more iteration of \((k,t)\)-FWL, we will get different colors for \(\mathbf{p}_{1}\) and \(\mathbf{p}_{2}\), which is a contradiction. \(\mathcal{C}^{\infty}_{k\mathit{fwl}}(\mathbf{v}_{1})=\mathcal{C}^{\infty}_{ k\mathit{fwl}}(\mathbf{v}_{2})\) directly implies that \(\{\!\!\{\mathcal{C}^{\infty}_{k\mathit{fwl}}(\mathbf{q}_{1})|\mathbf{q}_{1} \in H_{t}(\mathbf{v}_{1})\}\!\!\}_{t}=\{\!\!\{\mathcal{C}^{\infty}_{k\mathit{ fwl}}(\mathbf{q}_{2})|\mathbf{q}_{2}\in H_{t}(\mathbf{v}_{2})\}\!\!\}_{t}\) given Equation (14) and thus conclude the proof. 

Next, we show another property of \((k+t)\)-WL. Denote \(I_{k+t}\) as a permutation group operates on the indices of a \(k+t\)-tuple. For \(\sigma\in I_{k+t}\), we denote it operates on a \(k+t\)-tuple \(\mathbf{p}=(p_{1},\ldots,p_{k+t})\) as \(\sigma\cdot\mathbf{p}=(p_{\sigma(1)},\ldots,p_{\sigma(k+t)})\). For example, for a \(\sigma\in I_{3}\) with \(\sigma(1)=2,\sigma(2)=1,\sigma(3)=3\), we have \(\sigma\cdot(p_{1},p_{2},p_{3})=(p_{2},p_{1},p_{3})\).

**Lemma B.5**.: _Given two graphs \(G\) and \(H\), for any \(\mathbf{p}_{1}=(p_{11},\ldots,p_{1(k+t)})\in V^{k+t}(G)\), \(\mathbf{p}_{2}=(p_{21},\ldots,p_{2(k+t)})\in V^{k+t}(H)\) and a permutation \(\sigma\in I_{k+t}\) operates on the indices of tuples, \(\mathcal{C}^{l}_{(k+t)\mathit{wl}}(\mathbf{p}_{1})=\mathcal{C}^{l}_{(k+t)wl}( \mathbf{p}_{2})\) implies that \(\mathcal{C}^{l}_{(k+t)wl}(\sigma\cdot\mathbf{p}_{1})=\mathcal{C}^{l}_{(k+t)wl }(\sigma\cdot\mathbf{p}_{2})\)._

Proof.: We prove it by induction on \(l\). At iteration \(0\), \(\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{0}_{(k+t)wl}(\mathbf{p} _{2})\) means the ordered subgraph induced by \(\mathbf{p}_{1}\) is isomorphic to the ordered subgraph induced by \(\mathbf{p}_{2}\). As the permutation is operating on the indices, it will not change the nodes in the tuple but only change the order of the tuple. As we use the same permutation \(\sigma\) for both \(\mathbf{p}_{1}\) and \(\mathbf{p}_{2}\), it is easy to verify \(\mathcal{C}^{0}_{(k+t)wl}(\sigma\cdot\mathbf{p}_{1})=\mathcal{C}^{0}_{(k+t)wl }(\sigma\cdot\mathbf{p}_{2})\). Suppose it follows for \(1,\ldots,l-1\). At iteration \(l\), if \(\mathcal{C}^{l}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{l}_{(k+t)wl}(\mathbf{p} _{2})\), we have:

\[\{\!\!\{\mathcal{C}^{l-1}_{(k+t)wl}(\mathbf{p}_{1,w/j})|w\in V(G)\}\!\!\}=\{\! \!\{\mathcal{C}^{l-1}_{(k+t)wl}(\mathbf{p}_{2,w/j})|w\in V(H)\}\!\!\},\quad \forall j\in[k+t].\]

Then, since the statement follows at iteration \(l-1\), we must have

\[\{\!\!\{\mathcal{C}^{l-1}_{(k+t)wl}((\sigma\cdot\mathbf{p}_{1})_{w/j})|w\in V (G)\}\!\!\}=\{\!\!\{\mathcal{C}^{l-1}_{(k+t)wl}((\sigma\cdot\mathbf{p}_{2})_{ w/j})|w\in V(H)\}\!\!\},\quad\forall j\in[k+t].\]

The above equation directly implies that \(\mathcal{C}^{l}_{(k+t)wl}(\sigma\cdot\mathbf{p}_{1})=\mathcal{C}^{l}_{(k+t)wl }(\sigma\cdot\mathbf{p}_{2})\). This concludes the proof. 

With all conclusions at hand, we can start to prove the main proposition in this section.

**Proposition B.6**.: _Suppose \(k\geq 2\), \(t\geq 1\). Given any two graphs \(G\) and \(H\). If \(\mathcal{C}^{\infty}_{(k+t)wl}(G)=\mathcal{C}^{\infty}_{(k+t)wl}(H)\), then for any \(\mathbf{v}_{1}\in V^{k}(G)\), \(\mathbf{v}_{2}\in V^{k}(H)\), \(\mathbf{w}_{1}\in V^{t}(G)\), \(\mathbf{w}_{2}\in V^{t}(H)\), \(\mathbf{p}_{1}=(\mathbf{v}_{1},\mathbf{w}_{1})\), \(\mathbf{p}_{2}=(\mathbf{v}_{2},\mathbf{w}_{2})\) with \(\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{p}_{1})=\mathcal{C}^{\infty}_{(k+t)wl}( \mathbf{p}_{2})\), we also have \(\mathcal{C}^{\infty}_{k\mathit{fwl}}(\mathbf{p}_{1})=\mathcal{C}^{\infty}_{k \mathit{fwl}}(\mathbf{p}_{2})\)._

Proof.: As indicated in Remark A.4, this can be proved by doing one iteration of \((k,t)\)-FWL using the stable color computed by the \(k+t\)-WL as the initial color. If the color is not refined anymore, we can conclude it. However, since there is no stable color of low dimensional tuples in original \((k+t)\)-WL, we resort to Equation (15) for help:

\[\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{v}_{i})=\text{HASH}(\{\!\!\{\mathcal{C }^{\infty}_{(k+t)wl}(\mathbf{q})|\mathbf{q}\in H_{t}(\mathbf{v}_{i})\}\!\!\}_{t}), \quad i=1,2.\] (17)

Using Lemma B.3, we must have:

\[\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{v}_{1})=\mathcal{C}^{\infty}_{(k+t)wt}( \mathbf{v}_{2}).\] (18)

Further, given the definition of the neighborhood tuple, any \(\mathbf{u}_{1}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) is a sub-tuple of \(\mathbf{p}_{1}\), which means that we can find a permutation \(\sigma\in I_{[k+t]}\), such that the first \(k\) position of \(\sigma\cdot\mathbf{p}_{1}\) is exactly \(\mathbf{u}_{1}\). As the order of the neighborhood tuple is predefined and the same for both \(Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\), the first \(k\) position of \(\sigma\cdot\mathbf{p}_{2}\) will be \(\mathbf{u}_{2}\), as long as \(\mathbf{u}_{1}\) and \(\mathbf{u}_{2}\) is at the same position in the neighborhood tuple. Therefore, for any \(\mathbf{u}_{1}\in Q^{F}_{\mathbf{w}_{1}}(\mathbf{v}_{1})\) and \(\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2})\) at the same position \(o\) in the neighborhood tuple, we can find a permutation \(\sigma_{o}\in I_{[k+t]}\) to achieve it. Then, given Lemma B.5, we have \(\mathcal{C}^{\infty}_{(k+t)wl}(\sigma_{o}\cdot\mathbf{p}_{1})=\mathcal{C}^{ \infty}_{(k+t)wl}(\sigma_{o}\cdot\mathbf{p}_{2})\), which indicates \(\mathcal{C}^{\infty}_{(k+t)wt}(\mathbf{u}_{1})=\mathcal{C}^{\infty}_{(k+t)wl}( \mathbf{u}_{2})\) by using Lemma B.3. This means:

\[\left(\mathcal{C}^{\infty}_{(k+t)wl}(\mathbf{u}_{1})|\mathbf{u}_{1}\in Q^{F}_{ \mathbf{w}_{1}}(\mathbf{v}_{1})\right)=\left(\mathcal{C}^{\infty}_{(k+t)wl}( \mathbf{u}_{2})|\mathbf{u}_{2}\in Q^{F}_{\mathbf{w}_{2}}(\mathbf{v}_{2}) \right).\] (19)

[MISSING_PAGE_FAIL:20]

Finally, given Equation (20), we also have \(\mathcal{C}^{l-1}_{ktfwl}(\mathbf{v}_{1})=\mathcal{C}^{l-1}_{ktfwl}(\mathbf{v}_{2})\), which implies \(\mathcal{C}^{l}_{ktfwl}(\mathbf{v}_{1})=\mathcal{C}^{l}_{ktfwl}(\mathbf{v}_{2})\). This concludes the proof.

For the second part, the procedure is similar, as for any \(\mathbf{v}\in V^{k}(G)\), we can find at least one \(\mathbf{v}^{\prime}\in V^{k+1}(G)\) that contains all nodes and persevere the order of \(\mathbf{v}\). Further, given a \(\mathbf{w}\in V^{t}(G)\), for any \(\mathbf{u}\in Q^{\mathcal{F}}_{\mathbf{w}}(\mathbf{v})\), we can find at least one \(\mathbf{u}^{\prime}\in Q^{\mathcal{F}}_{\mathbf{w}}(\mathbf{v}^{\prime})\) that contains all nodes and persevere the order of \(\mathbf{u}\). Then, an induction on \(l\) can be performed and we omit the details.

Next, we show a useful property of \((k,t)\)-FWL.

**Lemma B.9**.: _For \(k\geq 2\), \(t\geq 1\), given a graph \(G\), for any \(\mathbf{v}_{1},\mathbf{v}_{2}\in V^{k}(G)\), \(\mathcal{C}^{\infty}_{ktfwl}(\mathbf{v}_{1})=\mathcal{C}^{\infty}_{ktfwl}( \mathbf{v}_{2})\) only if \(\forall i,j\in[k]\), \(\text{SPD}(v_{1i},v_{1j})=\text{SPD}(v_{2i},v_{2j})\)._

Proof.: Based on Lemma B.8, we only need to show it is true for \((k,1)\)-FWL, which is the original \(k\)-FWL. At the iteration 0, \(\mathcal{C}^{0}_{k1fwl}(\mathbf{v}_{1})=\mathcal{C}^{0}_{k1fwl}(\mathbf{v}_{2})\) if and only if the induced subgraph of \(\mathbf{v}_{1}\) and \(\mathbf{v}_{2}\) are isomorphic. Therefore, \(\forall i,j\in[k]\), if \(\text{SPD}(v_{1i},v_{1j})=0\) or \(\text{SPD}(v_{1i},v_{1j})=1\), we must have \(\text{SPD}(v_{2i},v_{2j})=0\) or \(\text{SPD}(v_{2i},v_{2j})=1\), respectively. After the first iteration, \(\forall i,j\in[k]\), if there exist at least one \(w_{1}\in V(G)\) such that \((v_{1i},w_{1}),(w_{1},v_{1j})\in E(G)\) but no such \(w_{2}\in V(G)\) exist with \((v_{2i},w_{2}),(w_{2},v_{2j})\in E(G)\), \((k,1)\)-FWL will output different color for \(\mathbf{v}_{1}\) and \(\mathbf{v}_{2}\), given the color update equation of \((k,1)\)-FWL. This means \(\mathcal{C}^{1}_{k1fwl}(\mathbf{v}_{1})=\mathcal{C}^{1}_{k1fwl}(\mathbf{v}_{2})\) implies that for \(\forall i,j\in[k]\), if \(\text{SPD}(v_{1i},v_{1j})=2\), we must have \(\text{SPD}(v_{2i},v_{2j})=2\). Similarly, after the second iteration, \((k,1)\)-FWL will output different color for \(\mathbf{v}_{1}\) and \(\mathbf{v}_{2}\) if there exist at least one \(w_{1}\in V(G)\) with \(\text{SPD}(v_{1i},w_{1})=2\) and \(\text{SPD}(w_{1},v_{1j})=1\) but no such \(w_{2}\in V(G)\) exists for \(\mathbf{v}_{2}\). Therefore, we can conclude that \(l\) iteration of \((k,1)\)-FWL can generate different color for \(\mathbf{v}_{1}\) and \(\mathbf{v}_{2}\) if there exist some \(i,j\in[k]\) with \(\text{SPD}(v_{1i},v_{1j})=d\) but \(\text{SPD}(v_{2i},v_{2j})\neq d\) (\(d\leq l+1\)). Therefore, by performing enough iterations, \((k,1)\)-FWL can compute the shortest path distance between any pair of nodes in a \(k\)-tuple and it is easy to verify \(\mathcal{C}^{\infty}_{ktfwl}(\mathbf{v}_{1})=\mathcal{C}^{\infty}_{ktfwl}( \mathbf{v}_{2})\) only if \(\forall i,j\in[k]\), \(\text{SPD}(v_{1i},v_{1j})=\text{SPD}(v_{2i},v_{2j})\). This concludes the proof. 

Next, we introduce Cai-Furer-Immermman (CFI) graphs [13]. CFI graphs are a family of graphs that \(k+1\)-WL can differentiate while \(k\)-WL cannot. Here we first state the construction of CFI graphs [8].

**Construction.** Let \(K_{k+1}\) denote a complete graph on \(k+1\) nodes (without self-loop). Nodes in \(K_{k+1}\) are labeled from 0 to \(k\). Let \(E(v)\) denote the set of edges incident to \(v\) in \(K_{k+1}\). We have \(|E(v)|=k\) for all \(v\in V(K_{k+1})\). Then, we define the graph \(G_{k}\) as follows:

1. For the node set \(V(G_{k})\), we add 1. \((v,S)\) for each \(v\) in \(V(K_{k+1})\) and for each _even_ subset \(S\subseteq E(v)\); 2. two nodes \(e^{1}\) and \(e^{0}\) for each edge \(e\in E(K_{k+1})\).
2. For the edge set \(E(G_{k})\), we add 1. an edge \((e^{0},e^{1})\) for each \(e\in E(K_{k+1})\); 2. an edge between \((v,S)\) and \(e^{1}\) if \(v\in e\) and \(e\in S\); 3. an edge between \((v,S)\) and \(e^{0}\) if \(v\in e\) and \(e\notin S\).

Further, we construct a companion graph \(H_{k}\) in a similar way to \(G_{k}\), except that: in Step 1(a), for the vertex 0 \(0\in V(K_{k+1})\), we choose all _odd_ subsets of \(E(0)\). It is easy to verify that both \(G_{k}\) and \(H_{k}\) have \((k+1)\cdot 2^{k-1}+\left(\begin{array}{c}k\\ 2\end{array}\right)\cdot 2\) nodes.

We say a set \(S\) of nodes forms a _distance-two-clique_ if the distance between any two vertices in \(S\) is exactly two.

**Lemma B.10**.: _The following holds for the graphs \(G_{k}\) and \(H_{k}\) defined above._

* _There exists a distance-two-clique of size_ \((k+1)\) _in_ \(G_{k}\)_._
* _There does not exist a distance-two-clique of size_ \((k+1)\) _inside_ \(H_{k}\)Proof.: Please see the Lemma 8 in [8] for detailed proof. 

Next, we show that \((k,t)\)-FWL cannot distinguish \(G_{k+t}\) and \(H_{k+t}\) but \((k+1,t)\)-FWL and \((k,t+1)\)-FWL can.

**Lemma B.11**.: _For \(k\geq 2\), \(t\geq 1\), \((k,t)\)-FWL cannot distinguish \(G_{k+t}\) and \(H_{k+t}\)._

Proof.: Given Theorem 3.2, \((k,t)\)-FWL is at most as powerful as \((k+t)\)-WL. As we know \((k+t)\)-WL cannot distinguish \(G_{k+t}\) and \(H_{k+t}\), we conclude that \((k,t)\)-FWL cannot distinguish between them neither. 

**Lemma B.12**.: _For \(k\geq 2\), \(t\geq 1\), \((k+1,t)\)-FWL and \((k,t+1)\)-FWL can distinguish \(G_{k+t}\) and \(H_{k+t}\)._

Proof.: The proof idea is to show both \((k+1,t)\)-FWL and \((k,t+1)\)-FWL are powerful enough to detect distance-two-cliques of size \((k+1)\), which is sufficient to show that \((k+1,t)\)-FWL and \((k,t+1)\)-FWL can distinguish between \(G_{k+t}\) and \(H_{k+t}\). Given Lemma B.9, we know that both \((k+1,t)\)-FWL and \((k,t+1)\)-FWL can compute the distance between any two nodes in a \(k+1\)-tuple and \(k\)-tuple, respectively. We first discuss \((k+1,t)\)-FWL. In \(G_{k}\), there exist a \(k+t+1\)-tuple \(\mathbf{p}=(\mathbf{v},\mathbf{w})\) with \(\mathbf{v}\in V^{k+1}(G_{k})\) and \(\mathbf{w}\in V^{t}(G_{k})\) that exactly form a distance-two-clique. Based on Equation 13, there exists one \(\mathbf{p}\in V^{k+t+1}(G_{k})\) such that \(\mathcal{C}^{l}_{(k+1)tfwl}(\mathbf{p})\) encodes the distance between any pair of nodes in \(\mathbf{p}\) is \(2\). However, there is no such \(\mathbf{p}\) in \(H_{k}\). This is sufficient to show that \((k+1,t)\)-FWL can distinguish \(G_{k}\) and \(H_{k}\). The proof for \((k,t+1)\)-FWL is similar, as \((k,t+1)\)-FWL can also encode pairwise distance information for any \(k+t+1\)-tuple, thus detecting distance-two-clique with the size of \(k+t+1\). This concludes the proof. 

Now, we are ready to prove Proposition 3.3 in the main paper. We restate it here:

**Proposition B.13**.: _For \(k\geq 2\), \(t\geq 1\), \((k,t+1)\)-FWL is strictly more powerful than \((k,t)\)-FWL; \((k+1,t)\)-FWL is strictly more powerful than \((k,t)\)-FWL._

Proof.: It is clear the proposition holds given Lemma B.8, Lemma B.12, and Definition A.3. 

### Complexity analysis

For \((k,t)\)-FWL, the space complexity is \(O(n^{k})\) as we only need to store representations of all \(k\)-tuples. For time complexity, if \(k\) and \(t\) are relatively small, to update the color of each \(k\)-tuple, \((k,t)\)-FWL needs to aggregate all \(V^{t}(G)\) possible neighborhood tuples, resulting in \(O(n^{k+t})\) time complexity. If both \(k\) and \(t\) are large enough (related to \(n\)), the time complexity further increases to \(O(n^{k+t}\cdot m\cdot(\frac{d^{t}}{m!})^{2})\), where \(m=min(k,t)\) and \(q=max(k,t)\).

### Limitations

As discussed in the above section, although \((k,t)\)-FWL can have a fixed space complexity of \(O(n^{k})\), the time complexity will grow exponentially if we increase the \(t\). Further, the increase of the \(t\) will also make the length of the neighborhood tuple increase. This makes the practical usage of \((k,t)\)-FWL limited (This will be hugely alleviated by \((k,t)\)-FWL+). This is evidence of the "no free lunch" issue in developing an expressive GNN model. However, we do believe there are many repeat computations in the \((k,t)\)-FWL that can be safely removed without the hurt of the expressive power. We leave it to our future work.

## Appendix C Additional discussion on \((k,t)\)-FWL+

### Detailed proofs for \((k,t)\)-FWL+

In this section, we provide all detailed proofs for \((k,t)\)-FWL+. Specifically, we will use \((k,t)\)-FWL+ to implement many existing powerful GNN/WL models with the best matching expressive power. To simplify notations and make it more clear, let \(\tilde{\mathcal{C}}\) denote the color output by \((k,t)\)-FWL+, \(\tilde{\mathcal{C}}\) denote the color output by any existing model we want to compare. Further, denote \(\hat{\mathcal{C}}^{l}(v_{1},v_{2})\) as the color of tuple \((v_{1},v_{2})\) at iteration \(l\) if \(k=2\).

**SLFWL(2)**[12]: We prove Proposition 3.5 in the main paper. We restate it here:

**Proposition C.1**.: _Let \(t=1\), \(k=2\), and \(ES(\mathbf{v})=\mathcal{N}_{1}(v_{1})\cup\mathcal{N}_{1}(v_{2})\), the corresponding \((k,t)\)-FWL+ instance is equivalent to SLFWL(2) [12] and strictly more powerful than any existing node-based subgraph GNNs._

Proof.: Given \(t=1\), \(k=2\), and \(ES(\mathbf{v})=\mathcal{N}_{1}(v_{1})\cup\mathcal{N}_{1}(v_{2})\), the color update equation of \((k,t)\)-FWL+ can be written as:

\[\hat{\mathcal{C}}^{l}(v_{1},v_{2})=\text{HASH}(\hat{\mathcal{C}}^{l-1}(v_{1}, v_{2}),\{\hskip-1.0pt\big{(}\hat{\mathcal{C}}^{l-1}(v_{1},w),\hat{\mathcal{C}}^{l-1} (w,v_{2})\big{)}\hskip-1.0pt|w\in\mathcal{N}_{1}(v_{1})\cup\mathcal{N}_{1}(v_ {2}))\hskip-1.0pt\}).\] (24)

We can see Equation (24) exactly matches SLFWL(2) stated in [12] and trivially this instance is as powerful as SLFWL(2). Further, given Theorem 7.1 in [12], we conclude that this instance is strictly more powerful than all existing node-based subgraph GNNs. 

\(\delta\)**-k-LWL**: The aggregation scheme of \(\delta\)-k-LWL can be written as:

\[\hat{\mathcal{C}}^{l}(\mathbf{v})=\text{HASH}(\hat{\mathcal{C}}^{l-1}( \mathbf{v}),(\{\hskip-1.0pt\big{[}\hat{\mathcal{C}}^{l-1}(\mathbf{u})|\mathbf{ u}\in\tilde{Q}_{j}(\mathbf{v})\hskip-1.0pt\big{]}\hskip-1.0pt|j\in[k])),\] (25)

where \(Q_{j}(\mathbf{v})=\{\mathbf{v}_{w/j}|w\in Q_{1}(v_{j})\}\). Next, we prove Proposition 3.6 in the main paper. We restate it here:

**Proposition C.2**.: _Let \(t=1\), \(k=k\), and \(ES(\mathbf{v})=\bigcup_{i=1}^{k}Q_{1}(v_{i})\), the corresponding \((k,t)\)-FWL+ instance is more powerful than \(\delta\)-k-LWL [8]._

Proof.: Given \(t=1\), \(k=k\), and \(ES(\mathbf{v})=\bigcup_{i=1}^{k}Q_{1}(v_{i})\), the color update equation of \((k,t)\)-FWL+ can be written as:

\[\hat{\mathcal{C}}^{l}(\mathbf{v})=\text{HASH}(\hat{\mathcal{C}}^{l-1}( \mathbf{v}),\{\hskip-1.0pt\big{(}\hat{\mathcal{C}}^{l-1}(\mathbf{u})|\mathbf{ u}\in Q_{w}^{F}(\mathbf{v})\big{)}|w\in\bigcup_{i=1}^{k}Q_{1}(v_{i}) \hskip-1.0pt\}).\] (26)

Given two \(k\)-tuple \(\mathbf{v}_{1},\mathbf{v}_{2}\in V^{k}(G)\), it is easy to see that for any node \(w\in V(G)\), \(\hat{\mathcal{C}}^{0}(\mathbf{v}_{1,w/j})\) will be different to \(\hat{\mathcal{C}}^{0}(\mathbf{v}_{2,w/j})\) as long as there exist any \(v_{1i}\) and \(v_{2i}\) such that \(v_{1i}\) is a neighbor of node \(w\) but \(v_{2i}\) is not for any \(i\in[k],i\neq j\). This means that for any \(\mathbf{u}\in Q_{w}^{F}(\mathbf{v})\), \(\hat{\mathcal{C}}^{0}(\mathbf{u})\) can injectively encode whether \(w\) is a neighbor to all other nodes in tuples. Further, as \(\bigcup_{i=1}^{k}Q_{1}(v_{i})\) includes neighbors of all nodes in tuple \(\mathbf{v}\), we can injectively encode \((\{\hskip-1.0pt\big{[}\hat{\mathcal{C}}^{l}(\mathbf{v}_{w/j})|w\in Q_{1}(v_{j })\hskip-1.0pt\big{]}\hskip-1.0pt|j\in[k]\hskip-1.0pt\})\) for any \(l\) by \(\{\hskip-1.0pt\big{(}\hat{\mathcal{C}}^{l-1}(\mathbf{u})|\mathbf{u}\in Q_{w}^{F} (\mathbf{v})\big{)}|w\in\bigcup_{i=1}^{k}Q_{1}(v_{i})\hskip-1.0pt\}\). Now, given any two graphs \(G\), \(H\) with \(\hat{\mathcal{C}}^{\infty}(G)=\hat{\mathcal{C}}^{\infty}(H)\). For any \(\mathbf{v}_{1}\in V^{k}(G)\) and \(\mathbf{v}_{2}\in V^{k}(H)\) with \(\hat{\mathcal{C}}^{\infty}(\mathbf{v}_{1})=\hat{\mathcal{C}}^{\infty}( \mathbf{v}_{2})\), we must have \(\{\hskip-1.0pt\big{[}\hat{\mathcal{C}}^{\infty}(\mathbf{v}_{1,w/j})|w\in Q_{ 1}(v_{1j})\hskip-1.0pt\}=\{\hskip-1.0pt\big{[}\hat{\mathcal{C}}^{\infty}( \mathbf{v}_{2,w/j})|w\in Q_{1}(v_{2j})\hskip-1.0pt\}\) for any \(j\in[k]\) given the above statement. Then, it is easy to verify that we cannot further refine the color of tuple \(\mathbf{v}_{1}\) and \(\mathbf{v}_{2}\) if we use the stable color of \((k,t)\)-FWL+ as initial color to perform another iteration of \(\delta\)-k-LWL, so as the final graph color histogram. Therefore, we conclude that \((k,t)\)-FWL+ is more powerful than \(\delta\)-k-LWL. 

**GraphSNN**[21]: The GNN aggregation scheme of GraphSNN can be written as:

\[\hat{\mathcal{C}}^{l}(v_{1},v_{1})=\text{HASH}(\hat{\mathcal{C}}^{l-1}(v_{1},v_ {1}),\{\hskip-1.0pt\big{[}\hat{\mathcal{C}}^{l-1}(v_{1},w),\hat{\mathcal{C}}^{l -1}(w,w)\big{)}\hskip-1.0pt|w\in Q_{1}(v_{1})\hskip-1.0pt\},\] (27)

\[\hat{\mathcal{C}}^{l}(v_{1},v_{2})=\text{HASH}(\frac{|E_{v_{1}v_{2}}|V_{v_{1}v_{2 }}|^{\lambda}}{|V_{v_{1}v_{2}}||V_{v_{1}v_{2}}-1|}),\] (28)

where \(|E_{v_{1}v_{2}}|\), \(|V_{v_{1}v_{2}}|\) are the number of edges and nodes in the overlapping 1-hop subgraph between node \(v_{1}\) and \(v_{2}\), \(\lambda\) is a constant. Next, we prove Proposition 3.7 in the main paper. We restate it here:

**Proposition C.3**.: _Let \(t=2\), \(k=2\), and \(ES^{2}(\mathbf{v})=(Q_{1}(v_{1})\cap Q_{1}(v_{2}))\times(Q_{1}(v_{1})\cap Q_{1}(v_ {2}))\), the corresponding \((k,t)\)-FWL+ instance is more powerful than GraphSNN [21]._

[MISSING_PAGE_EMPTY:24]

Proof.: We prove it by contradiction. First, if \(v_{12}=v_{11}\) or \(v_{12}=w_{12}\) we must have \(v_{22}=v_{21}\) or \(v_{22}=w_{22}\), respectively, as the initial color of \((v_{1},v_{1},w_{2})\) and \((v_{1},w_{2},w_{2})\) are different from other nodes in the subgraph. Further it is easy to show that \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{11},w_{12})=\tilde{\mathcal{C}}^{\infty }(v_{21},v_{21},w_{22})\iff\tilde{\mathcal{C}}^{\infty}(v_{11},w_{12},w_{12})= \tilde{\mathcal{C}}^{\infty}(v_{21},w_{22},w_{22})\) as they are uniquely labeled. Second, if \(v_{12}\neq v_{11},w_{12}\) and \(v_{22}\neq v_{21},w_{22}\), we can leverage the fact that the information of tuple \((v_{1},v_{1},w_{2})\) and \((v_{1},w_{2},w_{2})\) can be passed to other nodes injectively through message passing. Suppose we have \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{12},w_{12})=\tilde{\mathcal{C}}^{ \infty}(v_{21},v_{22},w_{22})\), if \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{11},w_{12})\neq\tilde{\mathcal{C}}^{ \infty}(v_{21},v_{21},w_{22})\), by doing another \(m\) iterations (\(m\) is large enough), this discrepancy must be reflected by \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{12},w_{12})\) and \(\tilde{\mathcal{C}}^{\infty}(v_{21},v_{22},w_{22})\), respectively. Therefore, we must have \(\tilde{\mathcal{C}}^{\infty+m}(v_{11},v_{12},w_{12})\neq\tilde{\mathcal{C}}^{ \infty+m}(v_{21},v_{22},w_{22})\), which is a contradiction to the stable color. The case of \((v_{1},w_{2},w_{2})\) is similar to the first one. This concludes the proof. 

**Lemma C.5**.: _Given two graphs \(G\), \(H\), for any two tuples \((v_{11},v_{12})\in V^{2}(G)\), \((v_{21},v_{22})\in V^{2}(H)\), \(w_{12}\in Q_{1}(v_{11})\), and \(w_{22}\in Q_{1}(v_{21})\), we have \(\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{11}\in V(G))\} \!\!\}=\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{21},w_{21},w_{22})|w_{21}\in V (H)\}\!\!\}\) if \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{11},w_{12})=\tilde{\mathcal{C}}^{ \infty}(v_{21},v_{21},w_{22})\) and \(\tilde{\mathcal{C}}^{\infty}(v_{11},w_{12},w_{12})=\tilde{\mathcal{C}}^{ \infty}(v_{21},w_{22},w_{22})\)._

Proof.: This lemma is an extension of Lemma B.6 in [12]. First, iven Lemma B.6 in [12], it is easy to conclude that we have \(\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{11}\in Q_{k_{1} }(v_{11})\cap Q_{k_{2}}(w_{12})\}\!\!\}=\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v _{21},w_{21},w_{22})|w_{21}\in Q_{k_{1}}(v_{21})\cap Q_{k_{2}}(w_{22})\}\!\}\) for any \(k_{1},k_{2}\in\mathbb{N}\). This directly implies \(\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{11}\in V(G))\} \!\!\}=\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{21},w_{21},w_{22})|w_{21}\in V (H)\}\!\!\}\). This concludes the proof. 

**Lemma C.6**.: _Given two graphs \(G\), \(H\), for any two tuples \((v_{11},v_{12})\in V^{2}(G)\), \((v_{21},v_{22})\in V^{2}(H)\), \(w_{12}\in Q_{1}(v_{11})\), and \(w_{22}\in Q_{1}(v_{21})\), we have \(\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{11}\in V(G))\} \!\!\}=\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{21},w_{21},w_{22})|w_{21}\in V (H)\}\!\!\}\) if \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{12},w_{12})=\tilde{\mathcal{C}}^{ \infty}(v_{21},v_{22},w_{22})\)._

Proof.: Given Lemma C.4, if \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{12},w_{12})=\tilde{\mathcal{C}}^{ \infty}(v_{21},v_{22},w_{22})\), we have \(\tilde{\mathcal{C}}^{\infty}(v_{11},v_{11},w_{12})=\tilde{\mathcal{C}}^{ \infty}(v_{21},v_{21},w_{22})\) and \(\tilde{\mathcal{C}}^{\infty}(v_{11},w_{21},w_{21})=\tilde{\mathcal{C}}^{ \infty}(v_{21},w_{22},w_{22})\). Next, given Lemma C.5, we have \(\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{11}\in V(G))\} \!\}=\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{21},w_{21},w_{22})|w_{21}\in V (H)\}\!\}\). This concludes the proof. 

**Lemma C.7**.: _For any two graphs \(G\), \(H\) and two nodes \(v_{11}\in V(G)\) and \(v_{21}\in V(H)\), we have \(\{\!\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{11}\in V(G)\}\!\!\} \!\}=\{\!\!\{\!\{\!\{\tilde{\mathcal{C}}^{\infty}(v_{21},w_{21},w_{22})|w_{21}\in V (H)\}\!\!\}\!\}\) if \(\{\!\!\{\!\{\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{12}\in Q_{1}(v_{11}) \}\!\!\}\!\}=\{\!\!\{\!\{\!\{\!\tilde{\mathcal{C}}^{\infty}(v_{21},w_{21},w_{22} )|w_{22}\in Q_{1}(v_{21})\}\!\!\}\!\}\) if \(\{\!\!\{\!\{\!\{\!\tilde{\mathcal{C}}^{\infty}(v_{11},w_{11},w_{12})|w_{12}\in Q_{1} (v_{11})\}\!\!\}\!\}\!\}\!\!\}\!\!However, edge-based subgraph GNNs use 3-tuple representations instead of 2-tuples in the \((k,t)\)-FWL+, which makes the comparison not trivial. Therefore, we slightly rewrite the above equation:

\[\hat{\mathcal{C}}^{l}(v_{1},v_{2},w_{1},w_{2})=\text{HASH}(\hat{ \mathcal{C}}^{l-1}(v_{1},v_{2},w_{1},w_{2}),\Big{(}\hat{\mathcal{C}}^{l-1}(u_{1 },u_{2})|(u_{1},u_{2})\in Q_{(w_{1},w_{2})}^{F}(v_{1},v_{2})\Big{)}),\] (33) \[\hat{\mathcal{C}}^{l}(v_{1},v_{2},w_{2})=\text{HASH}(\hat{ \mathcal{C}}^{l-1}(v_{1},v_{2},w_{2}),\{\hskip-1.0pt[\hat{\mathcal{C}}^{l}(v_{ 1},v_{2},w_{1},w_{2})|w_{1}\in Q_{1}(v_{2})\hskip-1.0pt]\hskip-1.0pt\}),\] (34) \[\hat{\mathcal{C}}^{l}(v_{1},v_{2})=\text{HASH}(\hat{\mathcal{C}}^ {l-1}(v_{1},v_{2}),\{\hskip-1.0pt[\hat{\mathcal{C}}^{l}(v_{1},v_{2},w_{2})|w_{ 2}\in Q_{1}(v_{1})\hskip-1.0pt]\hskip-1.0pt\}),\] (35)

where we let \(\hat{\mathcal{C}}^{0}(v_{1},v_{2},w_{2})\) and \(\hat{\mathcal{C}}^{0}(v_{1},v_{2},w_{1},w_{2})\) be the same for any \((v_{1},v_{2})\in V^{2}(G)\), \(w_{1}\in Q_{1}(v_{2})\), and \(w_{2}\in Q_{1}(v_{1})\). It is easy to verify that the above equations the equivalent to the original one. Now, we prove a strong lemma:

**Lemma C.8**.: _For any two graphs \(G\), \(H\), given two tuples \((v_{11},v_{12})\in V^{2}(G)\) and \((v_{21},v_{22})\in V^{2}(H)\), for any \((w_{11},w_{12})\in\mathcal{Q}_{1}(v_{12})\times\mathcal{Q}_{1}(v_{11})\) and \((w_{21},w_{22})\in\mathcal{Q}_{1}(v_{22})\times\mathcal{Q}_{1}(v_{21})\), we have \(\hat{\mathcal{C}}^{l-1}(v_{11},w_{11},w_{12})=\hat{\mathcal{C}}^{l-1}(v_{21},w _{21},w_{22})\) if \(\hat{\mathcal{C}}^{l}(v_{11},v_{12},w_{11},w_{12})=\hat{\mathcal{C}}^{l}(v_{2 1},v_{22},w_{21},w_{22})\) for any \(l\geq 1\)._

Proof.: We prove it by induction on \(l\). The base case is easy to verify given Lemma B.1 and the definition of initial color in edge-based subgraph GNNs.

Now suppose it is true for \(l=2,\ldots,l-1\). At iteration \(l\), if \(\hat{\mathcal{C}}^{l}(v_{11},v_{12},w_{11},w_{12})=\hat{\mathcal{C}}^{l}(v_{2 1},v_{22},w_{21},w_{22})\), given Equation 33, we must have:

\[\hat{\mathcal{C}}^{l-1}(v_{11},v_{12},w_{11},w_{12}) =\hat{\mathcal{C}}^{l-1}(v_{21},v_{22},w_{21},w_{22}),\] (36) \[\hat{\mathcal{C}}^{l-1}(u_{11},u_{12}) =\hat{\mathcal{C}}^{l-1}(u_{21},u_{22}),\] (37)

for any \((u_{11},u_{12})\in Q_{(w_{11},w_{12})}^{F}(v_{11},v_{12})\) and \((u_{21},u_{22})\in Q_{(w_{21},w_{22})}^{F}(v_{21},v_{22})\) at the same position. Particularly, we have \((v_{11},w_{11})\in Q_{(w_{11},w_{12})}^{F}(v_{11},v_{12})\) and \((v_{21},w_{21})\in Q_{(w_{21},w_{22})}^{F}(v_{21},v_{22})\) and they are in the same position. Given Equation (35), we have:

\[\{\hskip-1.0pt[\hat{\mathcal{C}}^{l-1}(v_{11},w_{11},y_{12})|y_{12}\in Q_{1}( v_{11})\hskip-1.0pt]\hskip-1.0pt\}=\{\hskip-1.0pt[\hat{\mathcal{C}}^{l-1}(v_{21},w _{21},y_{22})|y_{22}\in Q_{1}(v_{21})\hskip-1.0pt]\hskip-1.0pt\}.\] (38)

Equation (38) further indicates that for any \(y_{12}\in Q_{1}(v_{11})\), we can find a \(y_{22}\in Q_{1}(v_{21})\) such that

\[\hat{\mathcal{C}}^{l-1}(v_{11},w_{11},y_{12})=\hat{\mathcal{C}}^{l-1}(v_{21},w _{21},y_{22}).\] (39)

Next, given Equation (34), we can conclude that:

\[\{\hskip-1.0pt[\hat{\mathcal{C}}^{l-1}(v_{11},w_{11},x_{11},y_{12})|x_{11}\in Q _{1}(w_{11})\hskip-1.0pt]\hskip-1.0pt\}=\{\hskip-1.0pt[\hat{\mathcal{C}}^{l-1}( v_{21},w_{21},x_{21},y_{12})|x_{21}\in Q_{1}(w_{21})\hskip-1.0pt]\hskip-1.0pt\}.\] (40)

Since the statement follows at iteration \(l-1\), we have \(\{\hskip-1.0pt[\hat{\mathcal{C}}^{l-2}(v_{11},x_{11},y_{12})|x_{11}\in Q_{1}(w_{ 11})\hskip-1.0pt]\hskip-1.0pt\}=\{\hskip-1.0pt[\hat{\mathcal{C}}^{l-2}(v_{21}, x_{21},y_{12})|x_{21}\in Q_{1}(w_{21})\hskip-1.0pt]\hskip-1.0pt\}\). We also have \(\hat{\mathcal{C}}^{l-2}(v_{11},w_{11},y_{12})=\hat{\mathcal{C}}^{l-2}(v_{21},w_{21}, y_{22})\) given Equation (39). This means that we must have \(\hat{\mathcal{C}}^{l-1}(v_{11},w_{11},y_{12})=\hat{\mathcal{C}}^{l-1}(v_{21},w_{21}, y_{22})\) based on Equation (30). Finally, as \(w_{12}\) is one of \(y_{12}\) and \(w_{22}\) is one of \(y_{22}\) and it is one such pair that holds the statement at iteration \(l-1\). It must be the case that they are also the pair at iteration \(l\). This means we have \(\hat{\mathcal{C}}^{l-1}(v_{11},w_{11},w_{12})=\hat{\mathcal{C}}^{l-1}(v_{21},w_{2 1},w_{22})\), which concludes the proof. 

Now we prove Proposition 3.8 in the main paper. We restate it here:

**Proposition C.9**.: _Let \(t=2\), \(k=2\), and \(ES^{2}(\mathbf{v})=Q_{1}(v_{2})\times Q_{1}(v_{1})\), the corresponding \((k,t)\)-FWL+ instance is more powerful than edge-based subgraph GNNs like I\({}^{2}\)-GNN [19]._

Proof.: Note that the initial color of edge-based subgraph GNNs is weaker than \((k,t)\)-FWL+ given Lemma C.8, but it is still sufficient to show that the stable color from \((k,t)\)-FWL+ cannot be further refined by edge-based subgraph GNNs. For any two graphs \(G\) and \(H\) with \(\hat{\mathcal{C}}^{\infty}(G)=\hat{\mathcal{C}}^{\infty}(H)\), it is easy to see that for any \((v_{11},v_{12})\in V^{2}(G)\) and \(w_{12}\in Q_{1}(v_{11})\), we can find a \((v_{21},v_{22})\in V^{2}(H)\) and \(w_{22}\in Q_{1}(v_{21})\) with \(\hat{\mathcal{C}}^{\infty}(v_{11},v_{12},w_{12})=\hat{\mathcal{C}}^{\infty}(v_{2 1},v_{22},w_{22})\). Then, given Equation (34), we have \(\{\hskip-1.0pt[\hat{\mathcal{C}}^{\infty}(v_{11},v_{12},w_{11},w_{12})|w_{11} \in Q_{1}(v_{12})\hskip-1.0pt]\hskip-1.0pt\}=\{\hskip-1.0pt[\hat{\mathcal{C}}^{ \infty}(v_{21},v_{22},w_{21

[MISSING_PAGE_FAIL:27]

[MISSING_PAGE_FAIL:28]

Additional discussion on N\({}^{2}\)-FWL and N\({}^{2}\)-Gnn

### Detailed proofs for N\({}^{2}\)-Fwl

In this section, we provide all detailed proofs for N\({}^{2}\)-FWL. To make it clear, we denote \(\hat{\mathcal{C}}^{l}(v_{1},v_{2})\) as the color of tuple \((v_{1},v_{2})\) at iteration \(l\) for N\({}^{2}\)-FWL. We rewrite Equation 9 as:

\[\begin{split}\hat{\mathcal{C}}^{l}(v_{1},v_{2})=& \text{HASH}(\hat{\mathcal{C}}^{l-1}(v_{1},v_{2}),\{\hskip-1.0pt\text{f} \hskip-1.0pt\Big{(}\hat{\mathcal{C}}^{l-1}(u_{1},u_{2})|(u_{1},u_{2}\in Q^{F}_{ (w_{1},w_{2})}(v_{1},v_{2})\Big{)}\] (46) \[(w_{1},w_{2})\in(\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{ 1}))\cap(\mathcal{N}_{h}(v_{1})\cap\mathcal{N}_{h}(v_{2}))^{2}\hskip-1.0pt \text{f}\hskip-1.0pt\}).\end{split}\]

We start with the first lemma:

**Lemma D.1**.: _Given \(h\) large enough, N\({}^{2}\)-FWL is more powerful than SLFWL(2) [12]._

Proof.: Given \(h\) large enough, it is clearly that the neighbor of N\({}^{2}\)-FWL becomes \(\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1})\). For any \((w_{1},w_{2})\in\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1})\), it is easy to verify that \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\) contains all tuples in the aggregation of SLFWL(2). Further, the elements in \(\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1})\) includes all elements in \(Q_{1}(v_{2})\cup Q_{1}(v_{1})\). Therefore, it is easy to prove that N\({}^{2}\)-FWL is more powerful than SLFWL(2) by doing induction on \(l\). Here we omit the detailed proof. 

**Lemma D.2**.: _Given \(h\) large enough, N\({}^{2}\)-FWL is more powerful than edge-based subgraph GNNs._

Proof.: Given \(h\) large enough, it is clear that the neighbor of N\({}^{2}\)-FWL becomes \(\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1})\). The N\({}^{2}\)-FWL exactly matches the instance in Proposition 3.8 with additional root nodes. As the initial feature of root tuples(\((v_{1},v_{1})\), \((v_{2},v_{2})\)) are different from other tuples, its information will not affect other neighbors. This concludes the proof. 

It is intuitive to conjecture that N\({}^{2}\)-FWL with the number of \(h\) is more powerful than edge-based subgraph GNNs with \(h\)-hop subgraph and we leave the detailed proof in the future. Now, we can prove Theorem 4.1 in the main paper. We restate it here:

**Theorem D.3**.: _Given \(h\) is large enough, N\({}^{2}\)-FWL is more powerful than SLFWL(2) [12] and edge-based subgraph GNNs._

Proof.: Lemma D.1 and Lemma D.2 directly implies the theorem. This concludes the proof. 

Next, we prove Theorem 4.3 in the main paper. We restate it here:

**Theorem D.4**.: _N\({}^{2}\)-FWL can count up to (1) 6-cycles; (2) all connected graphlets with size 4; (3) 4-paths at node level._

Proof.: Given Theorem 4.1, N\({}^{2}\)-FWL is more powerful than I\({}^{2}\)-GNN [19]. Given Theorem 4.1, 4.2, 4.3 in [19], I\({}^{2}\)-GNN [19] is able to count up to (1) 6-cycles; (2) all connected graphlets with size 4; (3) 4-paths at node level. This concludes the proof. 

Then, we prove Corollary 4.2 in the main paper. We restate it here:

**Corollary D.5**.: _N\({}^{2}\)-FWL is strictly more powerful than all existing node-based subgraph GNNs and no less powerful than 3-WL._

Proof.: Given Theorem 4.1, this corollary can be directly concluded given Theorem 7.1 in [12] and Proposition 4.1 in [19]. But we further give an example to show that even with \(h=1\), N\({}^{2}\)-FWL is already powerful enough to distinguish some non-isomorphic strongly regular graph pairs that 3-WL cannot distinguish. When \(h=1\), the neighbors of N\({}^{2}\)-FWL is \((\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1}))\cap(\mathcal{N}_{1}(v_{1 })\cap\mathcal{N}_{1}(v_{2}))^{2}\). Consider the Shrikhande graph and 4\(\times\)4 Rook's graph in Figure 2. It is well known this pair of graphs cannot be distinguished by 3-WL. First, look at node \(v_{1}\) and \(v_{2}\) in Shrikhande graph, \(\mathcal{N}_{1}(v_{1})\cap\mathcal{N}_{1}(v_{2})\) includes two green nodes, \(v_{1}\), and \(v_{2}\). This means that \((w_{1},w_{2})\in(\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1}))\cap( \mathcal{N}_{1}(v_{1})\cap\mathcal{N}_{1}(v_{2}))^{2}\) contains a pair of \((w_{1},w_{2})\) such that \(w_{1}\) is one green node and \(w_{2}\) is the other. Given Definition A.2, \((w_{1},w_{2})\in Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\). This means that the information of \(\hat{\mathcal{C}}^{0}(w_{1},w_{2})\) will be encoded into \(\hat{\mathcal{C}}^{1}(v_{1},v_{2})\) and we have \((w_{1},w_{2})\) is not an edge. Similarly, in 4 \(\times\) 4 Rook's graph, \(\mathcal{N}_{1}(v_{1})\cap\mathcal{N}_{1}(v_{2})\) includes two green nodes, \(v_{1}\), and \(v_{2}\). Thus, \(\hat{\mathcal{C}}^{0}(w_{1},w_{2})\) will be encoded into \(\hat{\mathcal{C}}^{1}(v_{1},v_{2})\). However, \((w_{1},w_{2})\) here is an edge, which will have a different isomorphism type than \((w_{1},w_{2})\) in the Shrikhande graph, thus resulting in different \(\hat{\mathcal{C}}^{1}(v_{1},v_{2})\) and further graph color histogram. This concludes the proof. 

Finally, we prove Proposition 4.4 in the main paper. We restate it here:

**Proposition D.6**.: _If \(\mathbf{U}^{l}\), \(\mathbf{M}^{l}\), and \(\mathbf{R}\) are injective functions, there exist a N\({}^{2}\)-GNN instance that can be as powerful as N\({}^{2}\)-FWL._

Proof.: If \(\mathbf{U}^{l}\), \(\mathbf{M}^{l}\), and \(\mathbf{R}\) are injective, they are exactly the same as HASH used in N\({}^{2}\)-FWL. Therefore, it is easy to prove it by doing induction on \(l\) to show that if N\({}^{2}\)-GNN outputs the same color for two tuples, N\({}^{2}\)-FWL also does for any \(l\geq 0\) and vice versa. We omit the detailed proof here. 

### Complexity analysis

In this section, we provide a complexity analysis of N\({}^{2}\)-GNN. Let \(m\) denote the number of edges in the graph and \(d\) denote the average degree of the graph. First, we consider \(h\) is large enough, where the neighbor of N\({}^{2}\)-GNN becomes \(\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1})\). As N\({}^{2}\)-GNN is an instance of \((2,2)\)-FWL, it is straightforward that the space complexity of N\({}^{2}\)-GNN is upper-bounded by \(O(n^{2})\). Further, each tuple needs to aggregate \((d+1)^{2}\) different neighbors, which results in a total time complexity of \(O(n^{2}d^{2})=O(m^{2})\). Notes that the time complexity of N\({}^{2}\)-GNN is the same as edge-based subgraph GNNs. However, the space complexity of N\({}^{2}\)-GNN is \(O(n^{2})\), which is far less than edge-based subgraph GNNs with a \(O(nm)\) space complexity.

If we consider a pratical \(h\), the neighbors of N\({}^{2}\)-GNN is \((\mathcal{N}_{1}(v_{2})\times\mathcal{N}_{1}(v_{1}))\cap(\mathcal{N}_{h}(v_{1 })\cap\mathcal{N}_{h}(v_{2}))^{2}\). This means, only \((w_{1},w_{2})\) that within the overlapping \(h\)-hop subgraph of \(v_{1}\) and \(v_{2}\) can send message to \(v_{1},v_{2}\). Therefore, the time complexity is bounded by \(O(nd^{h+2})\), which is still the same as edge-based subgraph GNNs with \(h\)-hop subgraph selection policy. Further, it is easy to verify that only tuple \((v_{1},v_{2})\) with SPD\((v_{1},v_{2})\leq h\) can update its representation. Thus, in memory-sensitive tasks, we can only keep representation for tuples that can be updated, resulting in a space complexity of \(O(nd^{h})\).

### Model implementation details

Here we provide the details about model implementation. As N\({}^{2}\)-GNN is an instance of \((2,2)\)-FWL+, we are dealing with a tuple of size 6 in the aggregation process. Note that for any tuple \((v_{1},v_{2})\), \(Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\) will also include itself. However, it does not increase the expressive power as

Figure 2: The Shrikhande graph and 4 \(\times\) 4 Rook’s graph.

we already encode its information. Therefore, in the implementation, we remove it from the tuple. Thus, we implement the neighbor tuple as follows: for any 2-tuple \((v_{1},v_{2})\), the message from each neighbor \((w_{1},w_{2})\in N^{2}(v_{1},v_{2})\) is defined as \(((v_{1},w_{1}),(v_{1},w_{2}),(w_{1},v_{2}),(w_{2},v_{2}),(w_{1},w_{2}))\). Note the order of tuples is flexible as long as it is uniform across different neighbors, according to Definition A.2. We implement N\({}^{2}\)-GNN in a memory-saving way. To make it more clear, we slightly redefine some notations. Let \(h^{l}(v_{1},v_{2})\) be the representation of tuple \((v_{1},v_{2})\), \(m^{l}_{(v_{1},v_{2})}(w_{1},w_{2})\) be the message from the neighbor \((w_{1},w_{2})\), \(N^{2}(v_{1},v_{2})\) be the set of all neighborhoods of \((v_{1},v_{2})\) in N\({}^{2}\)-GNN with the output embedding hidden size of \(hs\). We let the initial feature of tuple \(h^{0}(v_{1},v_{2})=\text{MLP}(l_{G}(v_{2})+\text{SPD}(v_{1},v_{2}))\). The message function is implemented as:

\[u^{l}_{i}(v_{1},v_{2})=\text{LIN}_{1}(h^{l-1}(v_{1},v_{2}))*\text {Tanh}(\mathbf{W}_{i}),\] (47) \[m^{l}_{(v_{1},v_{2})}(w_{1},w_{2})=\text{LIN}_{2}(\text{CON}(u^{ l}_{1}(v_{1},w_{1}),u^{l}_{2}(v_{1},w_{2}),u^{l}_{3}(w_{1},v_{2}),u^{l}_{4}(w_{2},v _{2}),u^{l}_{5}(w_{1},w_{2}))),\] (48)

where \(\text{LIN}_{1}\) is a linear layer project the embedding from hidden size \(h\) to a lower value \(d\), named as inner size, \(\mathbf{W}_{i}\in R^{d},i\in[5]\) are learnable weight vector with size \(d\) and \(*\) is element-wise product, CON is the concatenation, and \(\text{LIN}_{2}\) is another linear layer project the concatenated embedding from size \(5*d\) back to hidden size \(h\). It is easy to validate that this implementation is injective to tuple input. Next, the update function \(\mathbf{U}^{l}\) is implemented as:

\[h^{l}(v_{1},v_{2})=\text{MLP}^{l}\left((1+\epsilon^{l})*h^{l-1}(v_{1},v_{2})+ \sum_{(w_{1},w_{2})\in N^{2}(v_{1},v_{2})}m^{l}_{(v_{1},v_{2})}(w_{1},w_{2}) \right),\] (49)

where \(\epsilon^{l}\) is a learnable scalar and \(\text{MLP}^{l}\) is a one-hidden-layer MLP with hidden size and output size equal to \(h\) and activation function be \(\text{ReLu}\). Note that in the implementation we do not consider the hierarchical multiset but directly pooling all neighbors together. We observe it already achieves superior results in all real-world tasks and is more parameter-saving.

As discussed in [12; 27], for each \(h^{l-1}(v_{1},v_{2})\), add the representation of \(h^{l-1}(v_{2},v_{2})\) can boost the performance in real-world tasks. Therefore, we adopt a similar way for all real-world experiments:

\[a^{l}(v_{1},v_{2})=\text{MLP}^{l}_{a}\left((1+\epsilon^{l}_{a})* h^{l-1}(v_{1},v_{2})+h^{l-1}(v_{2},v_{2})\right),\] (50) \[\hat{h}^{l}(v_{1},v_{2})=h^{l}(v_{1},v_{2})+a^{l}(v_{1},v_{2}).\] (51)

If add this additional aggregation, we will use the \(\hat{h}^{l}(v_{1},v_{2})\) instead of \(h^{l}(v_{1},v_{2})\) as the output representation. We conjecture this works as virtual nodes in real-work tasks, which allows a better mixture of node features and structure information. As indicated by [12], adding this term will not increase the expressive power from the theoretical side. Finally, the readout function is implemented in a hierarchical way:

\[h(v_{1})=\text{MLP}\left(\text{POOL}_{1}(\{\!\{h^{L}(v_{1},w)|w \in V(G)\}\!\})\right),\] (52) \[h_{G}=\text{POOL}_{2}\left(\{\!\{h(w)|w\in V(G)\}\!\}\right),\] (53)

where \(\text{POOL}_{1}\) and \(\text{POOL}_{2}\) are two different pooling function.

In addition, we adopted two implementation strategies for N\({}^{2}\)-GNN. The first strategy is to keep all \(O(n^{2})\) tuples even if it has a distance greater than \(h\). Notes that it will not aggregate information from other tuples but is only updated in Equation (50) and Equation (51). We observe it has great performance in real-world tasks combined with Equation (52) and Equation (53). The second implementation is used for memory-sensitive tasks. We remove all tuples that have a distance greater than \(h\). We refer to the first strategy as the dense version and the second strategy as the sparse version. We will specify which version we used in our experiments later on. The code is implemented using PyG [52]. We provide open-sourced code in https://github.com/JiaruiFeng/N2GNN for reproducing all results reported in the main paper.

### Limitations

Although N\({}^{2}\)-GNN has efficient theoretical space complexity, the empirical space complexity can still be high as we need to keep neighbors index for all tuple \((u_{1},u_{2})\in Q^{F}_{(w_{1},w_{2})}(v_{1},v_{2})\). This introducesa constant factor (5 in the implementation) on the saving edge index and corresponding gradients during training (see Table 9 and Table 10 for the practical time and memory comparison). It is still worth studying how to improve the empirical space complexity of N\({}^{2}\)-GNN. Further, in the current implementation, we didn't consider the hierarchical set but directly pooled all elements together to save the parameters and computational cost. Additionally, each tuple needs to aggregate many more neighbors than normal MPNNs, which makes the sum aggregation hard to achieve injectiveness during the aggregation, introducing the optimization issue. These hinder the model from achieving its theoretical power. Finally, although N\({}^{2}\)-GNN has \(O(nd^{h+2})\) time complexity, which is practical for most real-world tasks, the complexity still goes exponentially if the graph is dense (like a strongly regular graph). But as the design space of \((k,t)\)-FWL+ is extremely flexible, it is easy to design more practical and expressive instances suitable for dense graphs.

## Appendix E Experimental details

This section presents additional information on the experimental settings.

### Dataset statistics

### Experimental details

In this section, we provide all the experimental details. First, we discuss some general experimental settings that, if not specified further, are consistent across all experiments.

**Model**. We adopt POOL\({}_{1}\) as summation and POOL\({}_{2}\) as mean pooling in Equation (52) and Equation (53). We set the \(\epsilon^{l}\) in Equation (49), \(\epsilon^{l}_{a}\) in Equation (50) to be 0.0 and fix it during training and inference. At each MLP, we add batch normalization.

**Training**. For all experiments, we use Adam as the optimizer and ReduceLRonPlateau as the learning rate scheduler. We set l2 weight decay to 0.0 across all experiments. All experiments are conducted on a single NVIDIA A100 80GB GPU.

#### e.2.1 Graph structures counting

**Model**. For the substructure counting dataset, we adopt a similar setting as I\({}^{2}\)-GNN [19]. Specifically, we set the number of hop \(h\) to be 1, 2, 2, 3, 2, 2, 1, 4, 2 for 3-cycles, 4-cycles, 5-cycles, 6-cycles, tailed triangles, chordal cycles, 4-cliques, 4-paths, and triangle-rectangle respectively. For all substructures, we set the number of layers to 5, the hidden size \(h\) to 96, and the inner size \(d\) to 32. In the substructure count dataset, we remove all normalization layers. Finally, we use the sparse version of data preprocessing.

**Training**. The training/validation/test splitting ratio is 0.3/0.2/0.5. The initial learning rate is 0.001 and the minimum learning rate is 1e-5. The patience and factor of the scheduler are 10 and 0.9 respectively. The batch size is set to 256 and the number of epochs is 2000. For all substructures, we run the experiments 3 times and report the mean results on the test dataset.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Dataset & \#Graphs & Avg. \#Nodes & Avg. \#Edges & Task type \\ \hline EXP & 1200 & 44.4 & 110.2 & Graph classification \\ CSL & 150 & 41.0 & 164.0 & Graph classification \\ SR25 & 15 & 25.0 & 300.0 & Graph classification \\ BREC & 800 & 34.8 & 143.4 & Pair distinguishment \\ Counting & 5,000 & 18.8 & 31.3 & Node regression \\ QM9 & 130,831 & 18.0 & 18.7 & Graph regression \\ ZINC-12k & 12,000 & 23.2 & 24.9 & Graph regression \\ ZINC-250k & 249,456 & 23.1 & 24.9 & Graph regression \\ \hline \hline \end{tabular}
\end{table}
Table 6: Dataset statistics.

#### e.2.2 Zinc

**Model**. The setting is the same for both ZINC-Subset and ZINC-Full except for the inner size. We set the number of hop \(h\) to be 3. We set the number of layers to 6, and the hidden size \(h\) to 96. The inner size \(d\) is 20 for the ZINC-subset and 48 for the ZINC-full.

**Training**. The initial learning rate is 0.001 and the minimum learning rate is 1e-6. The patience and factor of the scheduler are 20 and 0.5 respectively. The batch size is set to 128 and the number of epochs is 500. For both ZINC-Subset and ZINC-Full, we run experiments 10 times and report the mean results and standard deviation on the test dataset.

#### e.2.3 Qm9

**Model**. For all targets, we adopt the same setting. We set the number of hop \(h\) to be 3. We set the number of layers to 6, the hidden size \(h\) to 120, and the inner size \(d\) to 32. We further add the resistance distance feature into the model, similar to the implementation in Feng et al. [22], Huang et al. [19].

**Training**. The initial learning rate is 0.001. The patience and factor of the scheduler are 5 and 0.9 respectively. The batch size is set to 128 and the number of epochs is 350. we run experiments 1 time and report the test result with the model of the best validation result.

#### e.2.4 Csl

**Model**: We set the number of hop \(h\) to be 4, the number of layers to be 4, the hidden size \(h\) to 48, and the inner size \(d\) to 16. We use the sparse version of data preprocessing and we do not add root aggregation (Equation (50)-(51)).

**Training**. The initial learning rate is 0.001 and the minimum learning rate is 1e-6. The patience and factor of the scheduler are 20 and 0.5 respectively. The batch size is set to 32 and the number of epochs is 80. We use 10-fold cross-validation and report the average results.

#### e.2.5 Sr25

**Model**: We set the number of hop \(h\) to be 1, the number of layers to be 6, the hidden size \(h\) to 64, and the inner size \(d\) to 16. We use the sparse version of data preprocessing and we do not add root aggregation (Equation (50)-(51)). Further, the normalization layer in SR25 is set to Layer normalization to avoid test/training mismatch.

**Training**. The initial learning rate is 0.001 and the minimum learning rate is 1e-6. The patience and factor of the scheduler are 200 and 0.5 respectively. The batch size is set to 15 and the number of epochs is 800. We report the single-time results.

#### e.2.6 Exp

**Model**: We set the number of hop \(h\) to be 3, the number of layers to be 4, the hidden size to 48, and the inner size \(d\) to 24. We use the sparse version of data preprocessing and we do not add root aggregation (Equation (50)-(51)).

**Training**. The initial learning rate is 0.001 and the minimum learning rate is 1e-6. The patience and factor of the scheduler are 20 and 0.5 respectively. The batch size is set to 32 and the number of epochs is 200. We use 10-fold cross-validation and report the average results.

#### e.2.7 Brec

**Model**: We set the number of hop \(h\) to be 8, the number of layers to be 4, the hidden size to 64, and the inner size \(d\) to 32. Meanwhile, the output channel is set to 16 for computing the similarity. We use the sparse version of data preprocessing and we do not add root aggregation (Equation (50)-(51)).

**Training**. We follow the same training and evaluation procedure as described by BREC [45]. The initial learning rate is 0.001 and the weight decay is set to 0.0001. The batch size is set to 4 and the number of epochs is 20. Due to the resource limitation, we only ran the experiment on 340 graph pairs (removed 20 4-vertex-condition graphs and 40 CFI graphs), which follow the same protocol as the reported result for \(\text{I}^{2}\)-GNN.

Additional experimental results

### Ablation studies for \((k,t)\)-FWL+

In this section, we provide an additional ablation study on \((2,t)\)-FWL+ by varying different \(t\) and \(ES\). We select two important \(ES\) mentioned in our paper--\(Q_{1}(v)\) and \(\mathcal{SP}(v_{1},v_{2})\). We conduct experiments on both Expressiveness verification datasets and counting datasets. The experiment setup is the same as what is discussed in the above section. The results can be seen in Table 7 and Table 8. Notes that we find the performance of \(\mathcal{SP}\) or \(\mathcal{SP}\times\mathcal{SP}\) on counting dataset is similar to MPNN and thus omit it in Table 8.

We can see that as long as \(t=2\), we can have a perfect performance on SR25 no matter if we use \(\mathcal{SP}\) or \(Q_{1}(v_{1})\), this aligns with our theoretical results. Further, \(\mathcal{SP}(v_{1},v_{2})\times Q_{1}(v_{1})\) achieve great results on all counting tasks and expressiveness verification. This indicates the \(\mathcal{SP}\) can also be a great choice in the practical scenario.

### Practical complexity of N\({}^{2}\)-GNN

In this section, we provide practical complexity analysis for N\({}^{2}\)-GNN. Here we use the cycle counting datasets with a similar parameter size for all models and a batch size of 256. To ensure a fair comparison, in N\({}^{2}\)-GNN, we use the sparse version for data preprocessing (which is true for all compared models), and set the number of workers as 0 and random seed as 1 for all models. The experiments are run on a single TeslaV100 32GB GPU. We report the training time, maximum training memory, inference time, and maximum inference memory usage during inference for both \(h=1\) and \(h=2\) for an average of 20 epochs. The results is shown in Table 9 and Table 10.

We can see that the empirical memory usage of N\({}^{2}\)-GNN is much less than \(I^{2}\)-GNN when \(h=1\) and comparable when \(h=2\). We conjecture the reason why N\({}^{2}\)-GNN require more training memory than I\({}^{2}\)-GNN when \(h=2\) is that when \(h\) increase, each tuple will need to aggregate much more neighbors than each node in I\({}^{2}\)-GNN and each aggregation require a constant size (5 in N\({}^{2}\)-GNN) of more memory for saving the gradients. Meanwhile, As we mentioned in limitations (Appendix D), to enjoy some extent of parallelism, the current implementation of \(N^{2}\)-GNN needs to save all neighbor indices, which introduces a constant factor of more memory on saving. But the inference memory of N\({}^{2}\)-GNN is less than I\({}^{2}\)-GNN in both \(h=1\) and \(h=2\). The training time and inference time of N\({}^{2}\)-GNN are also more efficient than I\({}^{2}\)-GNN. However, currently, the time and memory cost by N\({}^{2}\)-GNN are higher than node-based subgraph GNNs. We do want to highlight several points: (1) By changing the way of implementation, \(N^{2}\)-GNN can be implemented strictly within \(O(n^{2})\) space. Therefore, the merit of \(N^{2}\)-GNN still holds. (2) The experiment is conducted on the same parameter budget level. However, in real-world tasks, \(N^{2}\)-GNN needs much fewer parameters to achieve SOTA results (Table 5). (3) In this work, we not only aim to propose a new model but also mean to introduce a new flexible framework, \((k,t)\)-FWL+. The empirical success of \(N^{2}\)-GNN demonstrates the great potential of this framework for designing new expressive GNN variants.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Datasets & \(t=1,Q_{i}(v_{i})\) & \(t=1,Q_{i}(v_{i})\cup Q_{i}(v_{i})\) & \(t=1,\mathcal{SP}(v_{1},v_{2})\) & \(t=2,\mathcal{SP}(v_{1},v_{2})\times Q_{i}(v_{1})\) & \(t=2,\mathcal{SP}(v_{1},v_{2})\times\mathcal{SP}(v_{1},v_{2})\) \\ \hline EXP & 100 & 100 & 100 & 100 & 100 \\ CSL & 100 & 100 & 100 & 100 & 100 \\ SR25 & 6.67 & 6.67 & 6.67 & 100 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Expressive power verification (Accuracy) for different \((2,t)\)-FWL+.

\begin{table}
\begin{tabular}{l|c c c|c} \hline \hline Target & \(t=1,Q_{i}(v_{1})\) & \(t=1,Q_{i}(v_{1})\cup Q_{i}(v_{2})\) & \(t=2,\mathcal{SP}(v_{1},v_{2})\times Q_{i}(v_{1})\) & N\({}^{2}\)-GNN \\ \hline \hline Tailed Triangle & 0.0033 & 0.0031 & 0.0022 & 0.0025 \\ Chordal Cycle & 0.0019 & 0.0017 & 0.0021 & 0.0019 \\
4-Clique & 0.0013 & 0.0014 & 0.0016 & 0.0005 \\
4-Path & 0.0046 & 0.0043 & 0.0076 & 0.0042 \\ Tri.-Rec. & 0.0043 & 0.0053 & 0.0081 & 0.0055 \\
3-Cycles & 0.0005 & 0.0006 & 0.0004 & 0.0002 \\
4-Cycles & 0.0027 & 0.0022 & 0.0037 & 0.0024 \\
5-Cycles & 0.0051 & 0.0042 & 0.0037 & 0.0039 \\
6-Cycles & 0.0113 & 0.0097 & 0.0097 & 0.0075 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Evaluation of different \((2,t)\)-FWL+ variants on Counting Substructures (norm MAE), cells with MAE **greater** than 0.01 are colored.

[MISSING_PAGE_FAIL:35]

ZINC-Subset and QM9, respectively. The results are shown in Table 12. First, we can see adding Equation (50)-(51) indeed improve the performance across different tasks. Second, by explicitly keeping \(O(n^{2})\) embedding the performance, of N\({}^{2}\)-GNN on ZINC-Subset improved from 0.079 to 0.059. This may indicate that even though the two models have the same expressive power from the theoretical side, adding additional feature embedding can still boost the performance significantly in real-world tasks as it allows a better mixture of both feature and structure information. Note that these two components were used in many previous models like ESAN [43], SUN [27], and SSWL+ [12], etc. However, our model still outperforms them by a large margin. This indicates that the expressive power of the model is still the most critical ingredient, adding these additional components is just a way to unleash all the potential of the model for real-world tasks.

In the second ablation study, we evaluate the effect of the number of hop \(h\). We vary \(h\) from 1 to 4 and results are shown in Table 13. We can see an increase of \(h\) can boost the performance of the model, which has already been verified in previous subgraph-based methods. Particularly, for ZINC-Subset, we find that the third hop is critical to the performance. However, if we continue to increase the number of hops, the performance drops, which is a sign of overfitting.

\begin{table}
\begin{tabular}{l|c c} \hline \hline Model design & ZINC-Subset \(\downarrow\) & QM9 (\(C_{v}\)) \(\downarrow\) \\ \hline Full & **0.059 \(\pm\) 0.002** & **0.0760** \\ \hline w/o \(h^{\prime}(v_{2},v_{2})\) & 0.064 \(\pm\) 0.003 & 0.0811 \\ sparse & 0.079 \(\pm\) 0.003 & 0.0876 \\ w/o \(h^{\prime}(v_{2},v_{2})\) + sparse & 0.084 \(\pm\) 0.003 & 0.0847 \\ \hline \hline \end{tabular} 
\begin{tabular}{l|c c} \hline \hline \(h\) in N\({}^{2}\)-GNN & ZINC-Subset \(\downarrow\) & QM9 (\(C_{v}\)) \(\downarrow\) \\ \hline \(h\)=1 & 0.150 \(\pm\) 0.009 & 0.0823 \\ \(h\)=2 & 0.127 \(\pm\) 0.005 & 0.0808 \\ \(h\)=3 & **0.059 \(\pm\) 0.002** & **0.0760** \\ \(h\)=4 & 0.063 \(\pm\) 0.004 & 0.0826 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Ablation study of \(h\).