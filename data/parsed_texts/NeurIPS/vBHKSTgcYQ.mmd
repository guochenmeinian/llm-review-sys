# An Exploration-by-Optimization Approach

to Best of Both Worlds in Linear Bandits

 Shinji Ito

NEC Corporation, RIKEN AIP

i-shinji@nec.com &Kei Takemura

NEC Corporation

kei_takemura@nec.com

###### Abstract

In this paper, we consider how to construct best-of-both-worlds linear bandit algorithms that achieve nearly optimal performance for both stochastic and adversarial environments. For this purpose, we show that a natural approach referred to as exploration by optimization (Lattimore and Szepesvari, 2020b) works well. Specifically, an algorithm constructed using this approach achieves \(O(d\sqrt{T\log T})\)-regret in adversarial environments and \(O(\frac{d^{2}\log T}{\Delta_{\min}})\)-regret in stochastic environments. Symbols \(d\), \(T\) and \(\Delta_{\min}\) here represent the dimensionality of the action set, the time horizon, and the minimum sub-optimality gap, respectively. We also show that this algorithm has even better theoretical guarantees for important special cases including the multi-armed bandit problem and multitask bandits.

## 1 Introduction

The linear bandit problem (Abernethy et al., 2008) is a fundamental sequential decision-making problem in which a player chooses a \(d\)-dimensional vector \(a_{t}\) from a given _action set_\(\mathcal{A}\subseteq\mathbb{R}^{d}\) and incurs loss \(f_{t}\). The expectation of \(f_{t}\) is assumed to be expressed as an inner product of \(\ell_{t}\) and \(a_{t}\) with an unknown _loss vector_\(\ell_{t}\in\mathbb{R}^{d}\) in each round \(t=1,2,\ldots,T\). This paper focuses on the (expected) regret defined as \(R_{T}=\max_{a^{*}\in\mathcal{A}}\mathbf{E}[\sum_{t=1}^{T}\left\langle\ell_{t},a-a^{*}\right\rangle]\). It is known that the order of achievable regret can vary greatly depending on the structure of the environment that generates losses. For example, in stochastic environments, i.e., if the loss vector \(\ell_{t}\) is fixed at \(\ell^{*}\) for all \(t\), there exists an algorithm that achieves regret of \(O(\log T)\). By way of contrast, for adversarial environments in which the loss vector changes arbitrarily, the optimal regret bound is \(\Theta(\sqrt{T})\)(Abernethy et al., 2008; Bubeck et al., 2012). While this clearly suggests the importance of choosing an algorithm that is well-suited to the environment, in real-world applications it is often quite difficult, if not virtually impossible, to know the type of environment in advance.

One promising approach to this difficulty is to apply _best-of-both-worlds (BOBW)_ algorithms (Bubeck and Slivkins, 2012) that work nearly optimally both for stochastic and adversarial environments. The first BOBW algorithm for linear bandit problems was proposed by Lee et al. (2021); it achieves \(O(\sqrt{dT\log(T)\log(T|\mathcal{A}|)})\)-regret in adversarial environments and \(O(c^{*}\log(T|\mathcal{A}|)\log T)\)-regret in stochastic environments, where \(c^{*}=c(\mathcal{A},\ell^{*})\) represents an instance-dependent quantity characterizing the instance-optimal asymptotic bound for stochastic environments (Lattimore and Szepesvari, 2017). This algorithm by Lee et al. (2021) also has the notable advantage that these regret guarantees hold with high probability. On the other hand, their regret bound for stochastic environments includes an extra \(\log T\) factor.1 This issue of extra \(\log T\) factors has been resolved in a recent study by Dann et al. (2023). They have proposed algorithms achieving regret bounds with optimal dependency on \(T\), which are summarized here in Table 1. Their regret bounds for stochastic environments dependon the _minimum sub-optimality gap_\(\Delta_{\min}\). Note that we have \(c^{*}\leq O(d/\Delta_{\min})\)(Lee et al., 2021, Lemma 16) and that this inequality can be arbitrarily loose (Lattimore and Szepesvari, 2017). The study by Dann et al. (2023) involves the proposal of a general reduction technique, which has the remarkable advantage of being applicable to a variety of sequential decision-making problems in addition to linear bandit problems, though it does tend to complicate the structure and implementation of the algorithms.

The main contribution of this paper is to show that BOBW algorithms can be constructed via a (conceptally) simple approach referred to as _exploration by optimization_(EXO) (Lattimore and Szepesvari, 2020; Lattimore and Gyorgy, 2021; Foster et al., 2022). In this approach, we update the _reference distribution_\(q_{t}\) using the exponential weight method and then compute sampling distribution \(p_{t}\) for action \(a_{t}\) (by modifying \(q_{t}\) moderately) and loss estimator \(g_{t}\) by solving an optimization problem so that an upper bound on regret is minimized. As shown by Lattimore and Gyorgy (2021) and Foster et al. (2022), this natural approach has a connection to both the information ratio (Russo and Van Roy, 2014) and the decision-estimation coefficient (Foster et al., 2021), and it achieves nearly optimal worst-case regret bounds for some online decision-making problems. These existing studies on EXO, however, focus on adversarial environments, and not much has yet been shown about the potential of its working for stochastic environments. In this paper, we show that the EXO approach also works for stochastic environments to achieve \(O(\log T)\)-regret.

The regret bounds with the EXO approach and in existing studies are summarized in Table 1. This paper shows the regret bounds of \(O(\sqrt{\alpha dT\log T})\) in adversarial environments and of \(O(\beta d\log T)\) in stochastic environments, where \(\alpha\) is a parameter that depends on \(\mathcal{A}\) and \(\beta\) is a parameter depending on \(\mathcal{A}\) and \(\ell^{*}\). Bounds on \(\alpha\) and \(\beta\) shown in this paper are summarized in Table 2. For general action sets, the EXO approach reproduces the regret bound shown by Dann et al. (2023, Corollary 7). Further, for some special cases, such as multi-armed bandits and hypercube linear bandits, it achieves improved regret bounds that are tight in stochastic regimes parametrized by the sub-optimality gap \(\Delta_{\min}\).

To show BOBW regret bounds, we consider here the EXO approach combined with the _continuous_ exponential weights method (Hoeven et al., 2018), in which we update reference distributions \(q_{t}\) over a convex set. In our regret analysis of the exponential weight, the ratio of the variance of the sampling distribution \(p_{t}\) to the variance of \(q_{t}\) plays an important role: the larger the former, the better the guarantee obtained. To show \(O(\log T)\)-regret bounds for stochastic environments, we exploit the fact that \(q_{t}\) is a _log-concave distribution_(Lovasz and Vempala, 2007). Intuitively speaking, as any log-concave distribution has most of its weight concentrated around its mean (see, e.g., (Lovasz and Vempala, 2007, Lemma 5.17)), we may choose sampling distribution \(p_{t}\) so that it has a larger variance than \(q_{t}\) which leads to improved regret bounds.

\begin{table}
\begin{tabular}{l l l} \hline \hline Reference & Adversarial & Stochastic \\ \hline Lattimore and Szepesvari (2017) & & \(c^{*}\log T+o(\log T)\) \\ Bubeck et al. (2012) & \(O(\sqrt{dT\log(|\mathcal{A}|)})\) & \(O\left(c^{*}\log(T|\mathcal{A}|)\log T\right)\) \\ Lee et al. (2021) & \(O\left(\sqrt{dT\log(T)\log(T|\mathcal{A}|)}\right)\) & \(O\left(c^{*}\log(T|\mathcal{A}|)\log T\right)\) \\ Dann et al. (2023, Cor.7) & \(O\left(\sqrt{d^{2}T\log T}\right)\) & \(O\left(\frac{d^{2}\log T}{\Delta_{\min}}\right)\) \\ Dann et al. (2023, Cor.12) & \(O\left(\sqrt{dT\log(|\mathcal{A}|)}\right)\) & \(O\left(\frac{d\log(|\mathcal{A}|)\log T}{\Delta_{\min}}\right)\) \\
**[This work, Theorem 1]** & \(O\left(\sqrt{\alpha dT\log T}\right)\) & \(O\left(\beta d\log T\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Regret bounds for linear bandits

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Setting & Action set & \(\alpha\) & \(\beta\) \\ \hline Corollary 1 & General & \(\mathcal{A}\subseteq\mathbb{R}^{d}\) & \(O\left(d\right)\) & \(O\left(d/\Delta_{\min}\right)\) \\ Corollary 2 & Multi-armed & \(\mathcal{A}=\mathcal{A}_{d}:=\{e_{1},\ldots,e_{d}\}\) & \(O\left(1\right)\) & \(O\left(1/\Delta_{\min}\right)\) \\ Corollary 3 & Hypercube & \(\mathcal{A}=\{0,1\}^{d}\) & \(O\left(d\right)\) & \(O\left(1/\Delta_{\min}\right)\) \\ Corollary 3 & Multitask & \(\mathcal{A}=\mathcal{A}_{d_{1}}\times\cdots\times\mathcal{A}_{d_{m}}\) & \(O\left(m\right)\) & \(O\left(1/\Delta_{\min}\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Bounds on parameters \(\alpha\) and \(\beta\)Our regret analysis uses the _self-bounding technique_(Wei and Luo, 2018; Zimmert and Seldin, 2021) to show regret bounds for stochastic environments. One advantage of this technique is that a regret bound also leads to _stochastic environments with adversarial corruptions_ or _corrupted stochastic environments_(Lykouris et al., 2018; Li et al., 2019). As with existing algorithms based on the self-bounding technique, our algorithm achieves a regret bound of \(O(\alpha dT\log T+\sqrt{C\alpha dT\log T})\) for corrupted stochastic environments with the corruption level \(C\geq 0\). More generally, these regret bounds are achieved for environments in _adversarial regimes with self-bounding constraints_(Zimmert and Seldin, 2021), details of which are provided here in Section 3.

Another contribution of this paper is to establish a connection between the EXO approach and the SCRiBLe algorithm (Abernethy et al., 2008, 2012) (and its extensions (Dann et al., 2023; Ito and Takemura, 2023)). The SCRiBLe algorithm uses a follow-the-regularized-leader (FTRL) method with self-concordant barrier regularization to select a point in the action set, rather than a probability distribution. Abernethy et al. (2008) have shown that this approach achieves \(O(\sqrt{d^{3}T})\)-regret bounds for adversarial environments. Recently, Dann et al. (2023) and Ito and Takemura (2023) have shown that it can also achieve \(O(\frac{d^{3}\log T}{\Delta_{\min}})\)-regret in stochastic environments, through modification of the sampling distributions and loss estimators. To better understand the connection between these studies and the EXO approach, we propose a variant that we refer to as mean-oriented EXO and which includes EXO with the continuous exponential weight method as a special case. Given this correspondence, existing SCRiBLe-type algorithms can be interpreted as EXO-based methods in which the optimization is roughly solved with approximation ratios \(\geq\Omega(d)\).

The results of this paper establish a connection between the framework of exploration by optimization and best-of-both-worlds regret guarantees. We would like to emphasize that there is potential to extend this result to a broader class of sequential decision-making beyond linear bandits, such as partial monitoring problems and episodic MDPs. In fact, the EXO framework can be applied to a general model of sequential decision-making with structured observation (Foster et al., 2022) and is known to have a connection to several key concepts in sequential decision-making, such as the Decision-Estimation Coefficients (Foster et al., 2021) and information-directed sampling (Lattimore and Gyorgy, 2021; Russo and Van Roy, 2014), which has been extensively applied to partial monitoring (Lattimore and Szepesvari, 2020) and linear partial monitoring (Kirschner et al., 2020). Building further relationships between these generic frameworks and BOBW regret analysis will be an important topic to work on in the future.

## 2 Related work

For adversarial linear bandit problems, various approaches have been considered, including algorithms based on self-concordant barriers (SCRiBLe) (Abernethy et al., 2008, 2012; Rakhlin and Sridharan, 2013), discrete exponential weights (Geometric Hedge) (Dani et al., 2008; Bubeck et al., 2012; Cesa-Bianchi and Lugosi, 2012), and continuous exponential weights (continuous Geometric Hedge) (Hazan and Karnin, 2016; Bubeck and Eldan, 2019), all of which can be interpreted as methods in the FTRL framework. We can see that continuous exponential weight methods can be interpreted as an FTRL approach with an entropic barrier, a \(d\)-self-concordant barrier (Chewi, 2021), which implies a close relationship between the SCRiBLe algorithm and the continuous Geometric Hedge, as Bubeck and Eldan (2019) have noted. One key difference lies in how the sampling distribution \(p_{t}\) is chosen; the SCRiBLe algorithm employs a sampling scheme supported on the Dikin ellipsoid while the continuous Geometric Hedge designs \(p_{t}\) by mixing the reference distribution \(q_{t}\) with some exploration basis (e.g., a G-optimal design distribution). As a result of this difference, the latter can achieve an \(O(d^{-1/2})\)-times better regret bound than the former. Recent studies by Dann et al. (2023, Theorem 3) and Ito and Takemura (2023) have proposed new sampling schemes based on Dikin ellipsoid sampling to achieve BOBW regret bounds. This paper takes these approaches a step further, determining a sampling distribution itself on the basis of an optimization problem for minimizing regret, which leads to improved performance.

The self-bounding technique (Wei and Luo, 2018; Zimmert and Seldin, 2021) is known as a promising approach for designing and analyzing BOBW algorithms. The greatest impetus for the widespread use of this technique is considered to come from the Tsallis-INF algorithm (Zimmert et al., 2019) for the multi-armed bandit problem. Since the study on Tsallis-INF, FTRL methods with Tsallis-entropy regularizer (with some modification) have been used successfully in a variety of sequential decision-making problems with limited (structured) observation, including combinatorial semi-bandits (Zimmert et al., 2019), bandits with switching costs (Rouyer et al., 2021; Amir et al., 2022), bandits with delayed feedback (Masoudian et al., 2022), online learning with feedback graphs (Erez and Koren, 2021; Rouyer et al., 2022). dueling bandits (Saha and Gaillard, 2022), and episodic MDPs (Jin and Luo, 2020; Jin et al., 2021; Dann et al., 2023a). On the other hand, a few (rather limited number of) studies have constructed BOBW algorithms using regularizer functions other than the Tsallis entropy. For example, Ito (2021) proposed a BOBW multi-armed bandit algorithm by using log-barrier regularization equipped with adaptive learning rates. More recently, Shannon-entropy regularization with some specific adjustment methods for learning rates has been shown to provide BOBW regret guarantees for several problems including online learning with feedback graphs (Ito et al., 2022b), partial monitoring (Tsuchiya et al., 2023), and linear bandits (Kong et al., 2023). It seems that these Shannon-entropy-based methods have the strength of being versatile while these have the weakness that their regret bounds for stochastic environments include extra \(O(\log T)\)-factors. The approach in this paper may appear to be related to the previous studies using the Shannon entropy (Ito et al., 2022b; Tsuchiya et al., 2023; Kong et al., 2023) as it employs exponential weight methods, but it is more related to the studies using log-barrier regularization (Ito, 2021). In fact, both log barriers and continuous exponential weight can be captured in the category of self-concordant barriers as we will discuss in Sections 5 and 6. These also have in common the type of regret bounds: \(O(\log T)\) in stochastic environments and \(O(\sqrt{T\log T})\) in adversarial environments.

## 3 Problem setting

In linear bandit problems, the player is given an _action set_\(\mathcal{A}\subseteq\mathbb{R}^{d}\), which is a closed bounded set of \(d\)-dimensional vectors. Without loss of generality, we assume that \(\mathcal{A}\) spans all vectors in \(\mathbb{R}^{d}\). In each round \(t\), the environment chooses a _loss vector_\(\ell_{t}\subseteq\mathbb{R}^{d}\) while the player selects an action \(a_{t}\in\mathcal{A}\) without knowing \(\ell_{t}\). The player then gets feedback of \(f_{t}\in[-1,1]\) that is drawn from a distribution \(M_{a_{t}}\), where the series of distributions \(M=(M_{a})_{a\in\mathcal{A}}\) is in \(\mathcal{M}_{\ell_{t}}:=\{(M_{a})_{a\in\mathcal{A}}|\forall a\in\mathcal{A}, \ \mathbf{E}_{f\sim M_{a}}[f]=\langle\ell_{t},a\rangle\}\). The condition that the support of \(M_{a}\) is included in \([-1,1]\) for all \(a\in\mathcal{A}\) implies that \(\langle\ell_{t},a\rangle\in[-1,1]\) holds for all \(a\in\mathcal{A}\) and \(t\in[T]\). In other words, it is assumed that \(\ell_{t}\in\mathcal{L}\) for any \(t\), where we define \(\mathcal{L}\subseteq\mathbb{R}^{d}\) by \(\mathcal{L}=\{\ell\in\mathbb{R}^{d}\mid\forall a\in\mathcal{A},\ |\ \langle\ell,a\rangle\mid\leq 1\}\). The performance of the player is evaluated by means of _regret_ defined as

\[R_{T}(a^{*})=\mathbf{E}\left[\sum_{t=1}^{T}\left\langle\ell_{t},a_{t}\right\rangle -\sum_{t=1}^{T}\left\langle\ell_{t},a^{*}\right\rangle\right],\quad R_{T}= \sup_{a^{*}\in\mathcal{A}}R_{T}(a^{*}).\] (1)

Regimes of environmentsIn _stochastic environments_, \(\ell_{t}\) is assumed to be round-invariant, i.e., there exists \(\ell^{*}\in\mathcal{L}\) such that \(\ell_{t}=\ell^{*}\) holds for all \(t\). In _adversarial environments_, \(\ell_{t}\) can be chosen in an adversarial manner depending on the history so far \(((\ell_{s},a_{s}))_{s=1}^{t-1}\). _Stochastic environments with adversarial corruptions_ correspond to an intermediate setting between these two types of environments. These environments are parametrized by the _corruption level_\(C\geq 0\), in which the loss vectors \((\ell_{t})\) are assumed to satisfy \(\mathbf{E}\left[\sum_{t=1}^{T}\max_{a\in\mathcal{A}}|\left\langle\ell_{t}- \ell^{*},a\right\rangle|\right]\leq C\). To capture these settings of environments, we define an _adversarial regime with a \((\ell^{*},a^{*},C,T)\)-self-bounding constraint_(Zimmert and Seldin, 2021) in which the environments choose losses so that

\[R_{T}(a^{*})\geq\mathbf{E}\left[\sum_{t=1}^{T}\left\langle\ell^{*},a_{t}-a^{* }\right\rangle\right]-C\] (2)

holds for any algorithms. This regime includes (corrupted) stochastic environments. In fact, any corrupted stochastic environments with the true loss \(\ell^{*}\in\mathcal{L}\) and the corruption level \(C\geq 0\) are included in an adversarial regime with a \((\ell^{*},a^{*},C,T)\)-self-bounding constraint. Our regret analysis for (corrupted) stochastic environments relies only on the condition of (2), as in some existing studies (Zimmert and Seldin, 2021; Dann et al., 2023b).

Reduction to convex action setLet \(\mathcal{X}=\operatorname{conv}(\mathcal{A})\) represent the convex hull of \(\mathcal{A}\). We may consider the action set \(\mathcal{X}\) instead of \(\mathcal{A}\), i.e., if we can construct an algorithm \(\mathtt{Alg}_{\mathcal{X}}\) for the problem with action set \(\mathcal{X}\), we can convert it to an algorithm \(\mathtt{Alg}_{\mathcal{A}}\) for the problem with \(\mathcal{A}\). In fact, for any output \(a_{t}^{\prime}\in\mathcal{X}\) of \(\mathtt{Alg}_{\mathcal{X}}\), we can pick \(a_{t}\in\mathcal{A}\) so that \(\mathbf{E}[a_{t}|a_{t}^{\prime}]=a_{t}^{\prime}\).2 When defining \(a_{t}\) as the output of \(\mathtt{Alg}_{\mathcal{A}}\), we have \(\mathbf{E}[f_{t}|a_{t}^{\prime}]=\mathbf{E}[\left\langle\ell_{t},a_{t}\right \rangle|a_{t}^{\prime}]=\left\langle\ell_{t},a_{t}^{\prime}\right\rangle\), which means that the expectation of the feedback \(f_{t}\) given \(a_{t}^{\prime}\) is indeed a linear function in \(a_{t}^{\prime}\) and that the regret for \(\mathtt{Alg}_{\mathcal{A}}\) coincides with that for \(\mathtt{Alg}_{\mathcal{X}}\).

Footnote 2: Such a distribution of \(a_{t}\) given \(a_{t}^{\prime}\) can be computed efficiently e.g., by the algorithm given in [19, Corollary 14.1g] or [17].

## 4 Exploration by optimization

### Algorithm framework and regret analysis

Let \(\mathcal{P}(\mathcal{X})\) denote the set of all distributions over \(\mathcal{X}\). For any distribution \(p\in\mathcal{P}(\mathbb{R}^{d})\), denote \(\mu(p)=\mathbf{E}_{x\sim p}[x{x}^{\top}]\in\mathbb{R}^{d}\), \(H(p)=\mathbf{E}_{x\sim p}[x{x}^{\top}]\in\mathbb{R}^{d\times d}\), and \(V(p)=H(p)-\mu(p)\mu(p)^{\top}\in\mathbb{R}^{d\times d}\). We consider algorithms that choose \(a_{t}\) following a _sampling distribution_\(p_{t}\in\mathcal{P}(\mathcal{X})\) in each round \(t\). To determine \(p_{t}\), we compute an (approximately) unbiased estimator \(g_{t}(x;a_{t},f_{t})\) of the loss function \(\left\langle\ell_{t},x\right\rangle\). Let \(\mathcal{G}\) be the set of all estimators \(g\) that are linear in the first variable, i.e., \(\mathcal{G}=\{g:\mathcal{X}\times\mathcal{X}\times[-1,1]\to\mathbb{R}\mid \exists h:\mathcal{X}\times[-1,1]\to\mathbb{R}^{d},\;g(x;a,f)=\left\langle h(a,f),x\right\rangle\}\). For any distribution \(p\in\mathcal{P}(\mathcal{X})\), let \(\mathcal{G}^{\mathrm{u}}(p)\subseteq\mathcal{G}\) represent the set of all unbiased estimators:

\[\mathcal{G}^{\mathrm{u}}(p)=\left\{g\in\mathcal{G}\mid\forall\ell\in \mathcal{L},\forall x\in\mathcal{X},\forall M\in\mathcal{M}_{\ell},\quad \underset{a\sim p,f\sim M_{a}}{\mathbf{E}}[g(x;a,f)]=\left\langle\ell,x\right\rangle \right\}.\] (3)

A typical choice of \(g\) given \(p\) is the estimator defined as follows:

\[g(x;a,f)=\left\langle h(a,f),x\right\rangle,\quad\text{where}\quad h(a,f)=f \cdot(H(p))^{-1}a,\] (4)

where \(H(p)\) is assumed to be non-singular. This is an unbiased estimator, i.e., \(g\) defined by (4) is an element of \(\mathcal{G}^{\mathrm{u}}\). In fact, if \(M\in\mathcal{M}_{\ell}\), we have \(\mathbf{E}_{a\sim p,f\sim M_{a}}[h(a,f)]=\mathbf{E}_{a\sim p}[(H(p))^{-1}aa^{ \top}\ell]=(H(p))^{-1}H(p)\ell=\ell\).

Using \(\{g_{s}\}_{s=1}^{t}\), we set a _reference distribution_\(q_{t}\) over \(\mathcal{X}\) by the continuous exponential weights method as follows: \(q_{t}(x)\propto q_{0}(x)\exp(-\eta_{t}\sum_{s=1}^{t-1}g_{s}(x;a_{s},f_{s}))\), where \(q_{0}\) is an arbitrary initial distribution over \(\mathcal{X}\) and \(\eta_{t}\) is a learning rate such that \(\eta_{1}\geq\eta_{2}\geq\cdots\geq\eta_{T}>0\). We may choose \(\eta_{t}\) depending on the historical actions and observations so far \(((a_{s},f_{s}))_{s=1}^{t-1}\). Let \(D_{\mathrm{KL}}(q^{\prime},q)\) represent the Kullback-Leibler divergence of \(q^{\prime}\) from \(q\). Note that the exponential weights method can be interpreted as a follow-the-regularized-leader approach over \(\mathcal{P}\) with the regularizer function \(q\mapsto D_{\mathrm{KL}}(q,a_{0})\), i.e., the distribution \(q_{t}\) is a solution to the following optimization problem:

\[q_{t}\in\operatorname*{arg\,min}_{q\in\mathcal{P}(\mathcal{X})}\left\{ \underset{x\sim q}{\mathbf{E}}\left[\sum_{s=1}^{t-1}g_{s}(x;a_{s},f_{s})\right] +\frac{1}{\eta_{t}}\,D_{\mathrm{KL}}(q,q_{0})\right\}.\] (5)

We may determine the sampling distribution \(p_{t}\) on the basis of \(q_{t}\) in an arbitrary way. The regret is then bounded as follows:

**Lemma 1**.: _For any distribution \(p^{*}\in\mathcal{P}(\mathcal{X})\) such that \(D_{\mathrm{KL}}(p^{*},q_{0})<\infty\), the expected value of the regret \(R_{T}(a^{*})\) for \(a^{*}\sim p^{*}\) is bounded by_

\[\mathbf{E}\left[\sum_{t=1}^{T}\left(\left\langle\ell_{t},\mu(p_{t})-\mu(q_{t}) \right\rangle+\mathrm{bias}(g_{t},p_{t},q_{t},\ell_{t})+\frac{1}{\eta_{t}} \mathrm{stab}(\eta_{t}g_{t}(\cdot;a_{t},f_{t}),q_{t})\right)+\frac{D_{ \mathrm{KL}}(p^{*},q_{0})}{\eta_{T+1}}\right],\]

_where_ \[\mathrm{bias}(g,p,q,\ell)=\sup_{a^{*}\in\mathcal{X},M\in\mathcal{ M}_{\ell}}\left\{\underset{a\sim p,x\sim q,f\sim M_{a}}{\mathbf{E}}[ \left\langle\ell,x-a^{*}\right\rangle-g(a^{*};a,f)+g(x;a,f)]\right\},\] \[\mathrm{stab}(g,q)=\sup_{q^{\prime}\in\mathcal{P}(\mathcal{X})} \left\{\underset{x\sim q}{\mathbf{E}}[g(x)]-\underset{x\sim q^{\prime}}{ \mathbf{E}}[g(x)]-D_{\mathrm{KL}}(q^{\prime},q)\right\}.\] (6)

This follows from the standard analysis for exponential update methods or FTRL (see, e.g., [11], Exercise 28.12), and all omitted proofs can be found in the appendix. For the case of the exponential weights, the _stability term_\(\mathrm{stab}(g,q)\) can be bounded as

\[\mathrm{stab}(g,q)\leq\inf_{\bar{g}\in\mathbf{E}}\underset{x\sim q}{\mathbf{E} }\left[\phi(g(x)-\bar{g})\right],\quad\text{where}\quad\phi(y)=\exp(-y)+y-1.\] (7)The _bias term_\(\mathrm{bias}(g,p,q,\ell)\) corresponds to the bias of the estimator \(g(\cdot;a,f)\) for the fucntion \(\langle\ell,\cdot\rangle\), under the condition that \(a\sim p\), and \(\mathbf{E}[f|a]=\langle a,\ell\rangle\). We can easily see that \(g\in\mathcal{G}^{\mathrm{u}}(p)\) implies \(\mathrm{bias}(g,p,q,\ell)=0\) for all \(q\in\mathcal{P}(\mathcal{X})\) and \(\ell\in\mathcal{L}\).

For any reference distribution \(q\in\mathcal{P}(\mathcal{X})\) and learning rate \(\eta>0\), we define \(\Lambda_{q,\eta}(p,g)\) by

\[\Lambda_{q,\eta}(p,g)=\sup_{\ell\in\mathcal{L},M\in\mathcal{M}_{\ell}}\left\{ \langle\ell,\mu(p)-\mu(q)\rangle+\mathrm{bias}(g,p,q,\ell)+\frac{1}{\eta} \mathop{\mathbf{E}}_{a\sim p,f\sim M_{a}}\left[\mathrm{stab}(\eta g(\cdot;a,f),q)\right]\right\}.\]

From Lemma 1, the regret is bounded as

\[\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[R_{T}(a^{*})\right]\leq\mathop{ \mathbf{E}}\left[\sum_{t=1}^{T}\Lambda_{q_{t},\eta_{t}}(p_{t},g_{t})+\frac{1}{ \eta_{T+1}}\,D_{\mathrm{KL}}(p^{*},q_{0})\right].\] (8)

In the _exploration-by-optimization (EXO)_ approach (Lattimore and Szepesvari, 2020, 2021), we choose \(g_{t}\) and \(p_{t}\) so that the value of \(\Lambda_{q_{t},\eta_{t}}(g_{t},p_{t})\) is as small as possible, i.e., we consider the following optimization problem:

\[\text{Minimize}\quad\Lambda_{q_{t},\eta_{t}}(p,g)\quad\text{ subject to}\quad p\in\mathcal{P}(\mathcal{X}),\;g\in\mathcal{G}.\] (9)

We denote the optimal value of this problem by \(\Lambda_{q_{t},\eta_{t}}^{*}\).

**Remark 1**.: In most linear bandit algorithms based on exponential weights (Dani et al., 2008, Bubeck et al., 2012, Cesa-Bianchi and Lugosi, 2012, Hazan and Karnin, 2016, Besbes et al., 2019), \(p_{t}\) is defined as \(q_{t}\) mixed with a small component of _exploration basis_\(\pi_{0}\) and \(g_{t}\) is given by (4) with \(p=p_{t}\). The exploration basis \(\pi_{0}\) here is a distribution over \(\mathcal{X}\) such that \(G(\pi_{0}):=\sup_{a\in\mathcal{X}}a^{\top}H(\pi_{0})a\) is bounded. It is known that any action set \(\mathcal{X}\subseteq\mathbb{R}^{d}\) admits an exploration basis \(\pi_{0}\) such that \(G(\pi_{0})\leq d\), which is called the G-optimal design or the John's ellipsoid exploration basis. Given such a \(\pi_{0}\), we set \(p_{t}=(1-\gamma_{t})q_{t}+\gamma_{t}\pi_{0}\) with some \(\gamma_{t}\in(0,1)\). Under the condition of \(\eta_{t}=\Omega(1/d)\) and \(\gamma_{t}=\Theta(d\eta_{t})\), we have \(\Lambda_{q_{t},\eta_{t}}(p_{t},g_{t})=O(\gamma_{t}+\eta_{t}d)=O(\eta_{t}d)\), which implies that \(\Lambda_{q_{t},\eta_{t}}^{*}=O(\eta_{t}d)\) holds for any \(q\in\mathcal{P}(\mathcal{X})\). Hence, from (8), if \(D_{\mathrm{KL}}(p^{*},q_{0})\leq B\), by setting \(\eta_{t}=\eta=\Theta(\sqrt{B/(dT)})\), we obtain \(\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}[R_{T}(a^{*})]=O(\sqrt{BdT})\).

### Sufficient condition for best-of-both-worlds regret guarantee

In this section, we show that a certain type of upper bounds on \(\Lambda_{q,\eta}^{*}\) that depend on reference distribution \(q\) will lead to best-of-both-worlds regret bounds. In the following, we set the initial distribution \(q_{0}\) to be a uniform distribution over \(\mathcal{X}\). Denote \(\Delta_{\ell^{*}}(x)=\langle\ell^{*},x-a^{*}\rangle\) for any \(\ell^{*}\in\mathcal{L}\). The following lemma provides a sufficient condition for best-of-both-worlds regret guarantees:

**Theorem 1**.: _Suppose that there exist \(\alpha>0,\beta>0\) and \(\eta_{0}>0\) such that_

\[\Lambda_{q_{t},\eta}^{*}\leq\eta\cdot\min\{\alpha,\beta\cdot\Delta_{\ell^{*}} (\mu(q_{t}))\}\] (10)

_for all \(t\) and \(\eta\leq\eta_{0}\). Then, an algorithm based on the EXO approach achieves the following regret bounds simultaneously: (i) In adversarial environments, we have \(R_{T}=O\left(\sqrt{\alpha dT\log T}+\frac{d\log T}{\eta_{0}}\right).\) (ii) In environments satisfying (2), we have \(R_{T}=O\left(\left(\beta+\frac{1}{\eta_{0}}\right)d\log T+\sqrt{C\beta d\log T }\right)\)._

In the proof of this theorem, we consider the algorithm that chooses \((p_{t},g_{t})\) to be an optimal solution to (9) and that sets learning rates \(\eta_{t}\) by \(\eta_{t}=\min\left\{\eta_{0},\sqrt{\frac{d\log T}{\alpha+\sum_{s=1}^{t-1}\eta_ {s}^{*}\Lambda_{q_{s},\eta_{s}}^{*}}}\right\}\). A complete proof is given in the appendix.

**Remark 2**.: In the implementation of the algorithm, we do not necessarily need to solve the optimization problem (9) exactly. In fact, to achieve the regret upper bounds in Theorem 1, it is sufficient to compute \(p_{t},g_{t}\), and \(z_{t}>0\) that satisfy \(\eta_{t}^{-1}\Lambda_{q_{t},\eta_{t}}(p_{t},g_{t})\leq z_{t}\leq\min\{\alpha, \beta\cdot\Delta_{\ell^{*}}(\mu(q_{t}))\}\) for some \(\alpha>0\) and \(\beta>0\), under the assumption of \(\eta_{t}\leq\eta_{0}\). In this case, setting \(\eta_{t}=\min\{\eta_{0},\sqrt{d\log T}/\sqrt{\alpha+\sum_{s=1}^{t-1}z_{s}}\}\) will work.

### Regret bounds for concrete examples

In this section, we will see how we can bound \(\Lambda^{*}_{q_{t},\eta_{t}}\). For \(q\in\mathcal{P}(\mathcal{X})\), define \(\omega(q),\omega^{\prime}(q)\geq 0\) by

\[\omega(q) =\min_{p\in\mathcal{P}(\mathcal{X}):\mu(p)=\mu(q)}\{H(p)^{-1} \bullet V(q)\},\] (11) \[\omega^{\prime}(q) =\min_{p\in\mathcal{P}(\mathcal{X}):\mu(p)=\mu(q)}\min\{y>0\mid d \cdot V(q)\preceq y\cdot H(p)\}.\] (12)

Note that \(\omega(q)\leq\omega^{\prime}(q)\) holds. In fact, from the definition of \(\omega^{\prime}(q)\), there exists \(p\in\mathcal{P}(\mathcal{X})\) satisfying \(\mu(p)=\mu(q)\) and \(d\cdot V(q)\preceq\omega^{\prime}(q)\cdot H(p)\). For such a distribution \(p\), we have \(H(p)^{-1}\bullet V(q)=\operatorname{tr}(H(p)^{-1/2}V(q)H(p)^{-1/2})\leq \operatorname{tr}(d^{-1}\cdot\omega^{\prime}(q)\cdot I)=\omega^{\prime}(q)\), which implies that \(\omega(q)\leq\omega^{\prime}(q)\).

Using these, we can construct a bound \(\Lambda^{*}_{q,\eta}\) as the following lemma provides:

**Lemma 2**.: _Suppose that \(q\) is a log-concave distribution. If \(\eta\leq 1/d\), we then have \(\Lambda^{*}_{q,\eta}=O(\eta\omega(q))\). If \(\eta\leq 1\), we then have \(\Lambda^{*}_{q,\eta}=O(\eta\omega^{\prime}(q))\)._

Bounds on \(\Lambda^{*}_{q,\eta}\) in this lemma can be achieved by \(p=\gamma\tilde{p}+(1-\gamma)\pi_{0}\) and \(g\) defined by (4), where \(\tilde{p}\) is the minimizer on the right-hand side of (11) or (12), \(\pi_{0}\) is a G-optimal design for \(\mathcal{X}\), and some mixing weight parameter \(\gamma\in(0,1)\). The proof of this lemma is given in the appendix.

General action setWe can provide an upper bound on \(\omega(q)\) by exploiting the property of _log-concave distributions_. A distribution \(q\in\mathcal{P}(\mathcal{X})\) is called a log-concave distribution if it has a distribution function that is proportional to \(\exp(-h(x))\) for some convex function \(h\). Note that the reference distribution \(q_{t}\) is a log-concave function.

**Lemma 3**.: _Suppose that \(q\) is a log-concave distribution. Let \(\ell^{*}\in\mathcal{L}\) be an arbitrary loss vector such that \(a^{*}\in\operatorname*{arg\,min}_{a\in\mathcal{A}}\left\langle\ell^{*},a\right\rangle\) is unique. Denote \(\Delta_{\ell^{*},\min}=\min_{a\in\mathcal{A}\setminus\{a^{*}\}}\Delta_{\ell^ {*}}(a)\). We then have \(\omega^{\prime}(q)=O\left(d\cdot\min\left\{1,\Delta_{\ell^{*}}(\mu(q))/\Delta_ {\ell^{*},\min}\right\}\right)\)._

We here provide a proof sketch for this lemma. We consider a distribution \(p\in\mathcal{P}(\mathcal{A})\) of \(a\in\mathcal{A}\) generated by the following procedure: (i) Pick \(x\in\mathcal{X}\) following \(q\). (ii) Let \(\xi\) be an arbitrary distribution over \(\mathcal{A}\) such that \(\mu(\xi)=x\). In other words, compute \(\xi:\mathcal{A}\rightarrow\mathbb{R}_{\geq 0}\) such that \(\sum_{a^{\prime}\in\mathcal{A}}\xi(a^{\prime})=1\) and \(\sum_{a^{\prime}\in\mathcal{A}}\xi(a^{\prime})a^{\prime}=x\). (iii) Pick \(a\in\mathcal{A}\) following \(\xi\). It is then clear that \(\mu(p)=\mu(q)\) holds. Further, it follows from the assumption that \(q\) is a log-concave distribution that \((1-p(a^{*}))u^{\top}V(p)u=\Omega(u^{\top}V(q)u)\) holds for any \(a^{*}\in\mathcal{A}\) and any \(u\in\mathbb{R}^{d}\setminus\{0\}\), which implies \(d\cdot V(q)\preceq O((1-p(a^{*}))d)\cdot V(p)\preceq O((1-p(a^{*}))d)H(p)\). Finally, since we have \((1-p(a^{*}))\leq\Delta_{\ell^{*}}(\mu(p))/\Delta_{\ell^{*},\min}\)- the bound on \(\omega^{\prime}(q)\) in Lemma 3 follows. A full proof can be found in the appendix.

From Lemmas 2 and 3, we obtain the following bounds on \(\alpha\) and \(\beta\):

**Corollary 1**.: _For arbitrary action set \(\mathcal{A}\), the condition (10) in Theorem 1 holds with \(\alpha=O(d),\beta=O(d/\Delta_{\ell^{*},\min})\), and \(\eta_{0}=O(1)\)._

Hence, there exists an algorithm that achieves \(R_{T}=O(d\sqrt{T\log T})\) in adversarial environments and \(R_{T}=O(\frac{d^{2}}{\Delta_{\ell^{*},\min}}\log T+\sqrt{\frac{Cd^{2}}{\Delta_{ \ell^{*},\min}}\log T})\) in stochastically constrained adversarial environments.

Multi-armed banditsSuppose a problem setting in which the action set is the canonical bases: \(\mathcal{A}=\mathcal{A}_{d}:=\{e_{1},e_{2},\ldots,e_{d}\}=\{a\in\{0,1\}^{d}\mid \|a\|_{1}=1\}\). For this action set, we have the following bound on \(\omega(q)\):

**Lemma 4**.: _If \(\mathcal{A}=\mathcal{A}_{d}\), for any log-concave distribution \(q\), \(\omega(q)\) defined in (11) is bounded as \(\omega(q)\leq O\left(\sum_{i=1}^{d}\mu_{i}(q)(1-\mu_{i}(q))\right)\leq O\left( \min_{1\leq i\leq d}\{1-\mu_{i}(q)\}\right)\)._

Let \(\ell^{*}\in\mathcal{L}\) be an a loss vector such that \(e_{i^{*}}\in\operatorname*{arg\,min}_{1\leq i\leq d}\ell^{*}_{i}\) is unique. We then have \(\Delta_{\ell^{*}}(\mu)=\sum_{i\neq i^{*}}\Delta_{\ell^{*}}(e_{i})\mu_{i}\geq \Delta_{\ell^{*},\min}(1-\mu_{i^{*}})\), which, combined with Lemma 4, implies \(\omega(q)\leq\Delta_{\ell^{*}}(\mu(q))/\Delta_{\ell^{*},\min}\). From this and Lemma 2, we have the following bounds on \(\alpha\) and \(\beta\):

**Corollary 2**.: _For the multi-armed bandit problem \(\mathcal{A}=\mathcal{A}_{d}\), the condition (10) in Theorem 1 holds with \(\alpha=O(1)\), \(\beta=O(1/\Delta_{\ell^{*},\min})\) and \(\eta_{0}=O(1/d)\)._Product-space action setsSuppose that \(\mathcal{A}\subseteq\mathbb{R}^{d}\) is expressed as a product space of \(m\) sets, i.e., \(\mathcal{A}=\mathcal{A}^{(1)}\times\mathcal{A}^{(2)}\times\cdots\times\mathcal{ A}^{(m)}\), where \(\mathcal{A}^{(j)}\subseteq\mathbb{R}^{d_{j}}\) for each \(j\) and \(\sum_{j=1}^{m}d_{j}=d\). Denote \(\mathcal{X}^{(j)}=\mathrm{conv}(\mathcal{A}^{(j)})\). This setting is referred to as the _multi-task bandit_ problem (Cesa-Bianchi and Lugosi, 2012), in which the player chooses \(a_{t}^{(j)}\in\mathcal{A}^{(j)}\) for each \(j\) in parallel, and then gets only a single feedback corresponding to the sum of the losses for all \(m\) actions. For such an action set, probability distribution \(q_{t}\) given by continuous exponential weights can be expressed as a product measure of \(m\) log-concave measures \(q_{t}^{(j)}\in\mathcal{P}(\mathcal{X}^{(j)})\). In fact, if a probability density function is expressed as \(q(x)=c\exp(\langle\eta L,x\rangle)\) for some \(c>0\)\(\eta>0\), and \(L=(L^{(1)},\ldots,L^{(m)})\in\mathbb{R}^{d}\), it can be decomposed as \(q(x)=c\exp(\sum_{j=1}^{m}\langle\eta L^{(j)},x^{(j)}\rangle)=\prod_{j}^{m}c^{ (j)}\exp(\langle\eta L^{(j)},x^{(j)}\rangle)=\prod_{j}^{m}q^{(j)}(x^{(j)})\), where \(x^{(j)}\in\mathcal{X}^{(j)}\) and \(c^{(j)}=\int_{x\in\mathcal{X}^{(j)}}\exp(\langle\eta L^{(j)},x^{(j)}\rangle) \mathrm{d}x\). This fact leads to the following bounds on \(\omega(q)\) and \(\Lambda_{q,\eta}^{*}\):

**Lemma 5**.: _If \(\mathcal{A}=\mathcal{A}^{(1)}\times\cdots\times\mathcal{A}^{(m)}\) and \(q\in\mathcal{P}(\mathcal{X})\) can be expressed as a product measure of \(q^{(j)}\in\mathcal{P}(\mathcal{X}^{(j)})\) for \(j=1,\ldots,m\), we have \(\omega(q)\leq\sum_{j=1}^{m}\omega(q^{(j)})\)._

**Corollary 3**.: _If \(\mathcal{A}=\mathcal{A}_{d_{1}}\times\cdots\times\mathcal{A}_{d_{m}}\), the condition (10) in Theorem 1 holds with \(\alpha=O(m)\), \(\beta=O(1/\Delta_{\ell^{*},\min})\), and \(\eta_{0}=O(1/d)\). If \(\mathcal{A}=\{0,1\}^{d}\), the condition (10) in Theorem 1 holds with \(\alpha=O(d)\), \(\beta=O(1/\Delta_{\ell^{*},\min})\), and \(\eta_{0}=O(1/d)\)._

### Computational complexity

At this time, it is not known if there is an efficient algorithm to solve the optimization problems (9). However, it is worth noting that the optimization problem corresponding to the right-hand side of (11) is a convex optimization problem. Further, as the minimum can be achieved by \(p\in\mathcal{P}(\mathcal{A})\), we can reduce the problem into a convex optimization problem over \(|\mathcal{A}|\)-dimensional space, which can be solved efficiently if \(|\mathcal{A}|\) is not very large.

Note that it is not always necessary to solve the optimization problem (9) exactly, as mentioned in Remark 2. For example, we can implement an algorithm achieving regret bounds of Corollary 1 given a separation oracle (or equivalently, a linear optimization oracle) for \(cA\), without solving (9) exactly. Indeed, to achieve the regret bounds of Corollary 1, it suffices to find \(p\) and \(g\) such that \(\Lambda_{q_{t},\eta_{t}}(p,g)\) is bounded by the RHS of (10). The construction of such \(p\) and \(g\) is provided in the proof of Lemmas 2 and 3, which can be performed using a separation oracle for \(\mathcal{A}\). In fact, we can obtain samples from \(p\) by using the techniques for log-concave sampling (e.g., (Lovasz and Vempala, 2007)) and for computing convex combination expression (cf. Caratheodory's theorem for convex hull and (Schrijver, 1998, Corollary 14.1gL)). However, the analysis of log-concave sampling and calculations of \(H(p)\) (which is required for constructing \(g\)) including consideration of calculation errors can be highly complicated, and the computational cost can be very large, although on the order of polynomials (e.g., (Hazan and Karnin, 2016, Corollary 6.2)).

## 5 Mean-oriented FTRL and SCRiBLe

For linear bandit problems, we have an alternative approach that we referred to as _mean-oriented FTRL_, in which we compute a _reference mean_\(w_{t}\) given as follows:

\[w_{t}\in\operatorname*{arg\,min}_{w\in\mathcal{X}}\left\{\left(\sum_{s=1}^{t-1 }h_{s}(a_{s},f_{s}),w\right)+\frac{1}{\eta_{t}}\psi(w)\right\},\] (13)

where \(h_{s}(a_{s},f_{s})\) is an estimator of \(\ell_{s}\) and \(\psi\) is a convex regularization function over \(\mathcal{X}\) such that \(\min_{x\in\mathcal{X}}\psi(x)=0\). Let \(D_{\psi}(x^{\prime},x)\) denote the Bregman divergence associated with \(\psi\): \(D_{\psi}(x^{\prime},x)=\psi(x^{\prime})-\psi(x)-\langle\nabla\psi(x),x^{\prime}-x\rangle\). This approach ensures the following regret bound:

**Lemma 6**.: _For any point \(x^{*}\in\mathcal{X}\), we have_

\[R_{T}(x^{*}) \leq\mathbf{E}\left[\sum_{t=1}^{T}\left(\langle\ell_{t},\mu(p_{t}) -w_{t}\rangle+\mathrm{bias}(h_{t},p_{t},w_{t},\ell_{t})+\frac{\mathrm{stab}_{ \psi}(\eta_{t}h_{t}(a_{t},f_{t}),w_{t})}{\eta_{t}}\right)+\frac{\psi(x^{*})}{ \eta_{T+1}}\right],\] \[\text{where}\quad\mathrm{bias}(h,p,w,\ell)=\sup_{a^{*}\in \mathcal{X},M\in\mathcal{M}_{t}}\left\{\underset{a\sim p,f\sim M_{a}}{\mathbf{ E}}[\langle\ell-h(a,f),w-a^{*}\rangle]\right\},\] \[\mathrm{stab}_{\psi}(\hat{\ell},w) =\sup_{w^{\prime}\in\mathcal{X}}\left\{\left\langle\hat{\ell},w- w^{\prime}\right\rangle-D_{\psi}(w^{\prime},w)\right\}.\] (14)

One example of linear bandit algorithms based on mean-oriented FTRL is the SCRiBLe algorithm (Abernethy et al., 2008, 2012), which employs _self-concordant barriers_ as regularizer functions.

We can also consider an EXO approach for mean-oriented FTRL. For any reference point \(w\in\mathcal{X}\) and learning rate \(\eta>0\), define

\[\Lambda_{w,\eta}(p,h)=\sup_{\ell\in\mathcal{L},M\in\mathcal{M}_{t}}\left\{ \langle\ell,\mu(p)-w\rangle+\mathrm{bias}(h,p,w,\ell)+\frac{1}{\eta}\underset {a\sim p,f\sim M_{a}}{\mathbf{E}}[\mathrm{stab}_{\psi}(\eta h(a,f),q)]\right\}\]

and \(\Lambda^{*}_{w,\eta}=\inf_{p\in\mathcal{P}(\mathcal{X}),\hat{\ell}:\mathcal{ X}\times\mathbb{R}\rightarrow\mathbb{R}^{d}}\Lambda_{w,\eta}(p,\hat{\ell})\).

This achieves the following BOBW regret bounds, under assumptions on \(\Lambda^{*}_{w,\eta}\):

**Lemma 7**.: _Suppose that \(\psi\) is a \(\nu\)-self-concordant barrier. Suppose that there exist \(\alpha>0,\beta>0\) and \(\eta_{0}>0\) such that_

\[\Lambda^{*}_{w_{t},\eta}\leq\eta\cdot\min\{\alpha,\beta\cdot\Delta_{\ell^{*}}( w_{t})\}\] (15)

_for all \(t\) and \(\eta\leq\eta_{0}\). Then an algorithm based on the EXO approach achieves the following regret bounds simultaneously: (i) In adversarial environments, we have \(R_{T}=O\left(\sqrt{\alpha\nu T\log T}+\frac{\nu\log T}{\eta_{0}}\right).\) (ii) In environments satisfying (2), we have \(R_{T}(a^{*})=O\left(\left(\beta+\frac{1}{\eta_{0}}\right)\nu\log T+\sqrt{C \beta\nu\log T}\right).\)_

### Connection to the SCRiBLe algorithm and variants of it

Abernethy et al. (2008, 2012) have proposed a sampling scheme \(w_{t}\mapsto p_{t}\) supported on the Dikin ellipsoid, which leads to \(\Lambda_{w_{t},\eta_{t}}(p_{t},h_{t})=O(\eta_{t}d^{2})\) with an appropriately defined unbiased estimator \(h_{t}\). This algorithm referred to as SCRiBLe hence achieves \(R_{T}=O(\sqrt{d^{2}\nu T\log T})\), the adversarial regret bound in Lemma 7 with \(\alpha=O(d^{2})\). Recent studies by Dann et al. (2023, Theorem 3) and Ito and Takemura (2023) have shown that a modified sampling scheme based on the Dikin ellipsoid sampling achieves \(\Lambda_{w_{t},\eta_{t}}(p_{t},h_{t})=O\left(\eta_{t}d^{2}\cdot\min\{1,\Delta_ {\ell^{*}}(w_{t})/\Delta_{\ell^{*},\min}\}\right)\). Consequently, these modified algorithms achieve BOBW regret bounds in Lemma 7 with \(\alpha=O(d^{2})\) and \(\beta=O(d^{2}/\Delta_{\ell^{*},\min})\). These results suggest that the following lemma holds:

**Lemma 8** (Dann et al., 2023, Ito and Takemura, 2023).: _If \(\psi\) is a self-concordant barrier, (15) holds with \(\alpha=O(d^{2})\), \(\beta=O(d^{2}/\Delta_{\ell^{*},\min})\) and \(\eta_{0}=O(1/d)\)._

### Connection to EXO with continuous exponential weights

The entropic barrier is a \(\nu\)-self-concordant barrier with \(\nu\leq d\)(Chewi, 2021), for which a definition and properties are given, e.g., in (Bubeck and Eldan, 2019). We can see that mean-oriented EXO with entropic-barrier regularization coincides with the EXO approach with exponential weights. In fact, if we compute \(w_{t}\) using (13) with \(\psi\) the entropic barrier, from the definition of the entropic barrier, \(w_{t}\) is equal to the mean of the distribution \(q_{t}\) given by the continuous exponential weights. Given this correspondence, we can see that bounds on \(\Lambda^{*}_{w_{t},\eta}\) follow immediately from Corollary 1:

**Lemma 9**.: _If \(\psi\) is the entropic barrier for \(\mathcal{X}\), (15) holds with \(\alpha=O(d)\), \(\beta=O(d/\Delta_{\ell^{*},\min})\) and \(\eta_{0}=O(1)\)._

It is worth noting that these bounds are \(O(1/d)\)-times better than the results for Lemma 8 that follow from previous studies.

## 6 Limitations and future work

A limitation of this work is that the regret bounds for (corrupted) stochastic environments require the assumption that the optimal arm \(a^{*}\in\operatorname*{arg\,min}_{a\in\mathcal{A}}\left<\ell^{*},a\right>\) is unique. While this assumption is common in analyses of best-of-both-worlds algorithms based on the self-bounding technique (Zimmert and Seldin, 2021; Dam et al., 2023b), a limited number of studies on the multi-armed bandit problem (Ito, 2021; Jin et al., 2023) have removed this uniqueness assumption by careful analyses of regret bounds for follow-the-regularized-leader methods. As these analyses rely on structures specific to the multi-armed bandit problems, extending them to other problem settings does not seem trivial. Extending this analysis technique to linear bandits and removing the uniqueness assumptions will be an important future challenge.

Another future direction is to work toward a tighter regret bound of \(O(c^{*}\log T)\) in stochastic environments while satisfying BOBW regret guarantee, where \(c^{*}\) is an instance-dependent quantity that characterizes the optimal asymptotic bound (see, e.g., (Lattimore and Szepesvari, 2017; Corollary 2)). As \(c^{*}\leq O(d/\Delta_{\min})\)(Lee et al., 2021, Lemma 16.) and the gap between these two quantities can be arbitrarily large (Lattimore and Szepesvari, 2017, Example 4), our bounds depending on \(1/\Delta_{\min}\) can be far from tight. In the multi-armed bandit problem, a special case of linear bandits, the quantity \(c^{*}\) can be expressed as \(c^{*}=\sum_{a\in\mathcal{A}:\Delta_{\ell^{*}}(a)>2}\frac{2}{\Delta_{\ell^{*}} (a)}\), and known FTRL-type BOBW algorithms with \(O(c^{*}\log T)\)-regret bounds employs the Tsallis entropy with \(\Theta(1/\sqrt{t})\)-learning rates: \(\psi_{t}(w)=-\sqrt{t}\sum_{i=1}^{d}\sqrt{w_{i}}\)(Zimmert and Seldin, 2021) or logarithmic barriers with entry-wise adaptive learning rates: \(\psi_{t}(w)=-\sum_{i=1}^{d}\frac{1}{\eta_{i}}\log w_{i}\)(Ito, 2021; Ito et al., 2022). The latter would be more closely related to our results in this paper as logarithmic barriers are examples of self-concordant barriers. A major difference between the approach in this paper and the one by Ito (2021) is that the former only considers regularizer functions expressed as a scalar multiple of a fixed function, while the latter even changes the _shape_ of regularizer functions for each round. Such a more flexible adaptive regularization may be necessary for pursuing tighter regret bounds. Another promising approach is to extend the Tsallis-entropy approach to linear bandits while the connection between the Tsallis-INF (Zimmert and Seldin, 2021) and this paper may be somewhat tenuous as the Tsallis entropy is not a self-concordant barrier.

## References

* Abernethy et al. (2008) J. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. In _Conference on Learning Theory_, 2008.
* Abernethy et al. (2012) J. D. Abernethy, E. Hazan, and A. Rakhlin. Interior-point methods for full-information and bandit online learning. _IEEE Transactions on Information Theory_, 58(7):4164-4175, 2012.
* Amir et al. (2022) I. Amir, G. Azov, T. Koren, and R. Livni. Better best of both worlds bounds for bandits with switching costs. _Advances in Neural Information Processing Systems_, 35, 2022.
* Besbes et al. (2019) O. Besbes, Y. Gur, and A. Zeevi. Optimal exploration-exploitation in a multi-armed bandit problem with non-stationary rewards. _Stochastic Systems_, 9(4):319-337, 2019.
* Bubeck and Eldan (2019) S. Bubeck and R. Eldan. The entropic barrier: Exponential families, log-concave geometry, and self-concordance. _Mathematics of Operations Research_, 44(1):264-276, 2019.
* Bubeck and Slivkins (2012) S. Bubeck and A. Slivkins. The best of both worlds: Stochastic and adversarial bandits. In _Conference on Learning Theory_, pages 42-1. PMLR, 2012.
* Bubeck et al. (2012) S. Bubeck, N. Cesa-Bianchi, and S. M. Kakade. Towards minimax policies for online linear optimization with bandit feedback. In _Conference on Learning Theory_, pages 41-1. JMLR Workshop and Conference Proceedings, 2012.
* Cesa-Bianchi and Lugosi (2012) N. Cesa-Bianchi and G. Lugosi. Combinatorial bandits. _Journal of Computer and System Sciences_, 78(5):1404-1422, 2012.
* Chewi (2021) S. Chewi. The entropic barrier is \(n\)-self-concordant. _arXiv preprint arXiv:2112.10947_, 2021.
* Dani et al. (2008) V. Dani, S. M. Kakade, and T. P. Hayes. The price of bandit information for online optimization. In _Advances in Neural Information Processing Systems_, pages 345-352, 2008.
* D'Auria et al. (2012)C. Dann, C.-Y. Wei, and J. Zimmert. Best of both worlds policy optimization. In _International Conference on Machine Learning_, pages 6968-7008. PMLR, 2023a.
* Dann et al. [2023b] C. Dann, C.-Y. Wei, and J. Zimmert. A blackbox approach to best of both worlds in bandits and beyond. In _Conference on Learning Theory_, pages 5503-5570. PMLR, 2023b.
* Erez and Koren [2021] L. Erez and T. Koren. Towards best-of-all-worlds online learning with feedback graphs. _Advances in Neural Information Processing Systems_, 34, 2021.
* Foster et al. [2021] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* Foster et al. [2022] D. J. Foster, A. Rakhlin, A. Sekhari, and K. Sridharan. On the complexity of adversarial decision making. In _Advances in Neural Information Processing Systems_, 2022.
* Hazan and Karnin [2016] E. Hazan and Z. Karnin. Volumetric spanners: an efficient exploration basis for learning. _The Journal of Machine Learning Research_, 17(1):4062-4095, 2016.
* Hoeven et al. [2018] D. Hoeven, T. Erven, and W. Kotlowski. The many faces of exponential weights in online learning. In _Conference On Learning Theory_, pages 2067-2092. PMLR, 2018.
* Ito [2021] S. Ito. Parameter-free multi-armed bandit algorithms with hybrid data-dependent regret bounds. In _Conference on Learning Theory_, pages 2552-2583. PMLR, 2021.
* Ito and Takemura [2023] S. Ito and K. Takemura. Best-of-three-worlds linear bandit algorithm with variance-adaptive regret bounds. In _Conference on Learning Theory_, pages 2653-2677. PMLR, 2023.
* Ito et al. [2020] S. Ito, S. Hirahara, T. Soma, and Y. Yoshida. Tight first-and second-order regret bounds for adversarial linear bandits. In _Advances in Neural Information Processing Systems_, volume 33, 2020.
* Ito et al. [2022a] S. Ito, T. Tsuchiya, and J. Honda. Adversarially robust multi-armed bandit algorithm with variance-dependent regret bounds. In _Conference on Learning Theory_, pages 1421-1422. PMLR, 2022a.
* Ito et al. [2022b] S. Ito, T. Tsuchiya, and J. Honda. Nearly optimal best-of-both-worlds algorithms for online learning with feedback graphs. In _Advances in Neural Information Processing Systems_, volume 35, 2022b.
* Jin and Luo [2020] T. Jin and H. Luo. Simultaneously learning stochastic and adversarial episodic mdps with known transition. _Advances in Neural Information Processing Systems_, 33:16557-16566, 2020.
* Jin et al. [2021] T. Jin, L. Huang, and H. Luo. The best of both worlds: stochastic and adversarial episodic mdps with unknown transition. _Advances in Neural Information Processing Systems_, 34, 2021.
* Jin et al. [2023] T. Jin, J. Liu, and H. Luo. Improved best-of-both-worlds guarantees for multi-armed bandits: Ftrl with general regularizers and multiple optimal arms. _arXiv preprint arXiv:2302.13534_, 2023.
* Kirschner et al. [2020] J. Kirschner, T. Lattimore, and A. Krause. Information directed sampling for linear partial monitoring. In _Conference on Learning Theory_, pages 2328-2369. PMLR, 2020.
* Kong et al. [2023] F. Kong, C. Zhao, and S. Li. Best-of-three-worlds analysis for linear bandits with follow-the-regularized-leader algorithm. In _Proceedings of Thirty Sixth Conference on Learning Theory_, pages 657-673. PMLR, 2023.
* Lattimore and Gyorgy [2021] T. Lattimore and A. Gyorgy. Mirror descent and the information ratio. In _Conference on Learning Theory_, pages 2965-2992. PMLR, 2021.
* Lattimore and Szepesvari [2017] T. Lattimore and C. Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.
* Lattimore and Szepesvari [2020a] T. Lattimore and C. Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020a.
* Lattimore and Szepesvari [2020b] T. Lattimore and C. Szepesvari. Exploration by optimisation in partial monitoring. In _Conference on Learning Theory_, pages 2488-2515. PMLR, 2020b.
* Lee et al. [2021] C.-W. Lee, H. Luo, C.-Y. Wei, M. Zhang, and X. Zhang. Achieving near instance-optimality and minimax-optimality in stochastic and adversarial linear bandits simultaneously. In _International Conference on Machine Learning_, pages 6142-6151. PMLR, 2021.
* Liu et al. [2021]Y. Li, E. Y. Lou, and L. Shan. Stochastic linear optimization with adversarial corruption. _arXiv preprint arXiv:1909.02109_, 2019.
* Lovasz and Vempala (2007) L. Lovasz and S. Vempala. The geometry of logconcave functions and sampling algorithms. _Random Structures & Algorithms_, 30(3):307-358, 2007.
* Lykouris et al. (2018) T. Lykouris, V. Mirrokni, and R. Paes Leme. Stochastic bandits robust to adversarial corruptions. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 114-122, 2018.
* Masoudian et al. (2022) S. Masoudian, J. Zimmert, and Y. Seldin. A best-of-both-worlds algorithm for bandits with delayed feedback. _Advances in Neural Information Processing Systems_, 35, 2022.
* Mirrokni et al. (2017) V. Mirrokni, R. P. Leme, A. Vladu, and S. C.-w. Wong. Tight bounds for approximate caratheodory and beyond. In _International Conference on Machine Learning_, pages 2440-2448. PMLR, 2017.
* Rakhlin and Sridharan (2013) A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In _Conference on Learning Theory_, pages 993-1019. PMLR, 2013.
* Rouyer et al. (2021) C. Rouyer, Y. Seldin, and N. Cesa-Bianchi. An algorithm for stochastic and adversarial bandits with switching costs. In _International Conference on Machine Learning_, pages 9127-9135. PMLR, 2021.
* Rouyer et al. (2022) C. Rouyer, D. van der Hoeven, N. Cesa-Bianchi, and Y. Seldin. A near-optimal best-of-both-worlds algorithm for online learning with feedback graphs. _Advances in Neural Information Processing Systems_, 35, 2022.
* Russo and Van Roy (2014) D. Russo and B. Van Roy. Learning to optimize via information-directed sampling. _Advances in Neural Information Processing Systems_, 27, 2014.
* Saha and Gaillard (2022) A. Saha and P. Gaillard. Versatile dueling bandits: Best-of-both world analyses for learning from relative preferences. In _International Conference on Machine Learning_, pages 19011-19026. PMLR, 2022.
* Schrijver (1998) A. Schrijver. _Theory of Linear and Integer Programming_. John Wiley & Sons, 1998.
* Tsuchiya et al. (2023) T. Tsuchiya, S. Ito, and J. Honda. Best-of-both-worlds algorithms for partial monitoring. In _International Conference on Algorithmic Learning Theory_, pages 1484-1515. PMLR, 2023.
* Wei and Luo (2018) C.-Y. Wei and H. Luo. More adaptive algorithms for adversarial bandits. In _Conference On Learning Theory_, pages 1263-1291. PMLR, 2018.
* Zimmert and Seldin (2021) J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. _Journal of Machine Learning Research_, 22(28):1-49, 2021.
* Zimmert et al. (2019) J. Zimmert, H. Luo, and C.-Y. Wei. Beating stochastic and adversarial semi-bandits optimally and simultaneously. In _International Conference on Machine Learning_, pages 7683-7692. PMLR, 2019.

Notes on notation and assumption

For any two real vectors \(x=(x_{1},\ldots,x_{d})\in\mathbb{R}^{d}\) and \(y=(y_{1},\ldots,y_{d})\in\mathbb{R}^{d}\), \(\langle x,y\rangle\) denotes the the inner product of \(x\) and \(y\), i.e., \(\langle x,y\rangle=\sum_{i=1}^{d}x_{i}y_{i}\). For any real vector \(x=(x_{1},\ldots,x_{d})\in\mathbb{R}^{d}\), let \(\|x\|_{1}\) denote the \(L_{1}\)-norm of \(x\), i.e., \(\|x\|_{1}=\sum_{i=1}^{d}|x_{i}|\). For any real symmetric matrix \(X\), let \(\operatorname{tr}(X)\) denote the trace of \(X\). For any two real symmetric matrices \(X\) and \(Y\), we denote \(X\preceq Y\) (or \(Y\succeq X\)) if and only if \(Y-X\) is a positive semi-definite matrix. For any two real symmetric matrices \(X\) and \(Y\), \(X\bullet Y\) denotes the Frobenius inner product of \(X\) and \(Y\), i.e., \(X\bullet Y=\operatorname{tr}(AB)\).

Without loss of generality, we may assume that \(\mathcal{A}\) spans all vectors in \(\mathbb{R}^{d}\). Indeed, if not, we can make \(\mathcal{A}\) make full-dimensional by ignoring redundant dimensions. Under this assumption, distributions \(q\) given by exponential weight methods have non-singular variance-covariance matrices \(V(q)\) (and thus also \(H(q)\)). This fact is used in some places in this paper without notice.

## Appendix B Omitted proofs

### Lemmas on log-concave distributions

**Lemma 10** (Theorem 5.2 in (Lovasz and Vempala, 2007)).: _If \(X\in\mathbb{R}^{n}\) is a random variable following a log-concave distribution, for any constant matrix \(M\in\mathbb{R}^{m\times n}\), \(MX\) follows a log-concave distribution as well._

**Lemma 11** (Lemmas 5.4 and 5.5 (a) in (Lovasz and Vempala, 2007)).: _If \(p\) is a one-dimensional log-concave distribution, we have_

\[\Pr_{X\sim\alpha}[X\leq\mu(p)]\geq\frac{1}{\mathrm{e}}.\] (16)

_Further, if \(V(p)=1\), we have_

\[\Pr_{X\sim p}[a\leq X\leq b]\leq|b-a|\] (17)

_for any \(a,b\in\mathbb{R}\)._

**Lemma 12**.: _For any one dimensional log-concave distribution \(p\) over a one-dimensional segment \([L,R]\subseteq\mathbb{R}\), we have_

\[V(p)\leq\mathrm{e}^{2}\min\left\{(\mu(p)-L)^{2},(R-\mu(p))^{2}\right\}.\] (18)

Proof.: From Lemma 11, we have

\[\frac{1}{\mathrm{e}}\leq\Pr_{X\sim p}[X\leq\mu(p)]=\Pr_{X\sim p}\left[\frac{L} {\sqrt{V(p)}}\leq\frac{X}{\sqrt{V(p)}}\leq\frac{\mu(p)}{\sqrt{V(p)}}\right] \leq\frac{\mu(p)-L}{\sqrt{V(p)}},\] (19)

where the first and the second inequalities follow from (16) and (17), respectively. We hence have \(V(p)\leq\mathrm{e}^{2}(\mu(p)-L)^{2}\). By considering \(-X\) instead of \(X\), we can show \(V(p)\leq\mathrm{e}^{2}(R-\mu(p))^{2}\) in a similar way. 

**Lemma 13**.: _Suppose \(\phi\) is defined as in (7). If \(y\) follows a log-concave distribution over \(\mathbb{R}\) and if \(s=\sqrt{\mathbf{E}[y^{2}]}\leq\sqrt{1/2}\), we have_

\[\mathbf{E}[\phi(y)]\leq s^{2}+\frac{\exp(3-s^{-1})}{1-\exp(1-s^{-1})}=O\left( s^{2}\right).\] (20)

Proof.: See, e.g., the proof of (Ito et al., 2020, Lemma 5).

### Proof of Lemma 1

Proof.: From the definition of the regret, we have

\[\mathop{\mathbf{E}}_{\alpha^{*}\sim p^{*}}\left[R_{T}(a^{*})\right] =\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[\sum_{t=1}^{T}\mathop{ \mathbf{E}}_{a\sim p_{t}}\left[\langle\ell_{t},a-a^{*}\rangle\right]\right]\] \[=\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[\sum_{t=1}^{T}\left( \mathop{\mathbf{E}}_{x\sim q_{t}}\left[\langle\ell_{t},x-a^{*}\rangle\right]+ \langle\ell_{t},\mu(q_{t})-\mu(p_{t})\rangle\right)\right].\] (21)

From the definition of bias given in (6), the part of \(\mathop{\mathbf{E}}_{x\sim q_{t}}\left[\langle\ell_{t},x-a^{*}\rangle\right]\) can be bounded as follows:

\[\mathop{\mathbf{E}}_{x\sim q_{t}}\left[\langle\ell_{t},x-a^{*} \rangle\right]\] \[=\mathop{\mathbf{E}}_{a_{t},f_{t}}\left[\mathop{\mathbf{E}}_{x \sim q_{t}}\left[\langle\ell_{t},x-a^{*}\rangle-g_{t}(a^{*};a_{t},f_{t})+g_{t} (x;a_{t},f_{t})\right]+\mathop{\mathbf{E}}_{x\sim q_{t}}\left[g_{t}(x;a_{t},f_ {t})-g_{t}(a^{*};a_{t},f_{t})\right]\right]\] \[\leq\mathrm{bias}(g_{t},p_{t},q_{t},\ell_{t})+\mathop{\mathbf{E} }_{a_{t},f_{t}}\left[\mathop{\mathbf{E}}_{x\sim q_{t}}\left[g_{t}(x;a_{t},f_{t })-g_{t}(a^{*};a_{t},f_{t})\right]\right].\] (22)

From this and (21), we have

\[\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[R_{T}(a^{*})\right] \leq\mathop{\mathbf{E}}\left[\sum_{t=1}^{T}\left(\langle\ell_{t},\mu(q_{t})-\mu(p_{t})\rangle+\mathrm{bias}(g_{t},p_{t},q_{t},\ell_{t})\rangle\right)\right]\] \[\quad+\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[\mathop{\mathbf{ E}}\left[\sum_{t=1}^{T}\mathop{\mathbf{E}}_{x\sim q_{t}}\left[g_{t}(x;a_{t},f_{t })-g_{t}(a^{*};a_{t},f_{t})\right]\right]\right].\] (23)

By a standard analysis of the exponential weight method, we have

\[\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[\sum_{t=1}^{T}\mathop{\mathbf{E}} _{x\sim q_{t}}\left[g_{t}(x;a_{t},f_{t})-g_{t}(a^{*};a_{t},f_{t})\right]\right] \leq\sum_{t=1}^{T}\frac{1}{\eta_{t}}\mathrm{stab}(\eta_{t}g_{t}(\cdot;a_{t},f_ {t}),q_{t})+\frac{1}{\eta_{T+1}}\,D_{\mathrm{KL}}(p^{*},q_{0}).\] (24)

To show (24), we denote \(G_{t}(p)=\mathop{\mathbf{E}}_{a\sim p}[g_{t}(a;a_{t},f_{t})]\) in the following for notational simplicity. From (5), we have

\[\mathop{\mathbf{E}}_{a^{*}\sim p^{*}}\left[\sum_{t=1}^{T}g_{t}( a^{*};a_{t},f_{t})\right]+\frac{1}{\eta_{T+1}}\,D_{\mathrm{KL}}(p^{*},q_{0})\] \[=\sum_{t=1}^{T}G_{t}(p^{*})+\frac{1}{\eta_{T+1}}\,D_{\mathrm{KL}} (p^{*},q_{0})\] \[\geq\sum_{t=1}^{T}G_{t}(q_{T+1})+\frac{1}{\eta_{T+1}}\,D_{\mathrm{ KL}}(q_{T+1},q_{0})\] \[\geq\sum_{t=1}^{T-1}G_{t}(q_{T+1})+\frac{1}{\eta_{T}}\,D_{\mathrm{ KL}}(q_{T+1},q_{0})+G_{T}(q_{T+1})\] \[=\sum_{t=1}^{T-1}G_{t}(q_{T})+\frac{1}{\eta_{T}}\,D_{\mathrm{KL}} (q_{T},q_{0})+\frac{1}{\eta_{T}}\,D_{\mathrm{KL}}(q_{T+1},q_{T})+G_{T}(q_{T+1})\] \[\geq\cdots\geq\sum_{t=1}^{T}\left(\frac{1}{\eta_{t}}\,D_{\mathrm{ KL}}(q_{t+1},q_{t})+G_{t}(q_{t+1})\right),\] (25)

where the first inequality follows from (5) and the second inequality follows from \(\eta_{T+1}\leq\eta_{T}\) and the fact that the values of the KL divergence are nonnegative. The second equality follows from the definition of \(q_{T}(x)\); As \(q_{T}(x)\) can be expressed as \(q_{T}(x)=\kappa_{T}\exp(-\eta_{T}\hat{g}_{T}(x))q_{0}(x)\) for some

[MISSING_PAGE_FAIL:15]

where the last inequality holds since \(-\ell^{*}\in\mathcal{L}\) for any \(\ell^{*}\in\mathcal{L}\), which follows from the definition of \(\mathcal{L}\).

We then, hence, have

\[R_{T}^{\prime\prime} :=\mathbf{E}\left[\sum_{t=1}^{T}\Delta_{\ell^{*}}(\mu(q_{t})) \right]=\mathbf{E}\left[\sum_{t=1}^{T}\left(\Delta_{\ell^{*}}(\mu(p_{t}))+ \langle\ell^{*},\mu(q_{t})-\mu(p_{t})\rangle\right)\right]\] \[\leq\mathbf{E}\left[\sum_{t=1}^{T}\Delta_{\ell^{*}}(\mu(p_{t})) \right]+\mathbf{E}\left[\sum_{t=1}^{T}\Lambda_{q_{t},\eta_{t}}^{*}\right]\leq R _{T}+C+R_{T}^{\prime}\leq 2R_{T}^{\prime}+C.\] (31)

From this, we have

\[R_{T}^{\prime}\leq 3\,\mathbf{E}\left[\sqrt{d\log T\left(\alpha+\beta R_{T}^ {\prime\prime}\right)}\right]+\frac{d\log T}{\eta_{0}}+1\leq 3\,\mathbf{E} \left[\sqrt{d\log T\left(\alpha+\beta(2R_{T}^{\prime}+C)\right)}\right]+\frac{ d\log T}{\eta_{0}}+1\]

which implies that \(R_{T}=O\left(\left(\beta+\frac{1}{\eta_{0}}\right)d\log T+\sqrt{C\beta d\log T}\right)\) holds. 

### Proof of Lemma 2

Proof.: We first see that \(\Lambda_{q,\eta}^{*}=O(1)\) holds for any \(q\in\mathcal{P}(\mathcal{X})\) and \(\eta>0\). In fact, by setting \(p=q\) and letting \(g\in\mathcal{G}\) to be the zero function (\(g(x;a,f)=0\) for all \(x,a\) and \(f\)), we obtain \(\Lambda_{q,\eta}(p,g)=\sup_{\ell\in\mathcal{L}}\mathrm{bias}(g,p,q,\ell)= \sup_{\ell\in\mathcal{L},x\in\mathcal{X},a^{*}\in\mathcal{X}}\left(\ell,x-a^{*} \right)\leq 2\).

Let us consider the case in which \(\eta\leq 1/d\) holds. As \(\Lambda_{q,\eta}^{*}=O(1)\) for any \(\eta\) and \(q\), we have \(\Lambda_{q,\eta}^{*}=O(\eta\omega(q))\) if \(\eta\omega(q)=\Omega(1)\). In the following, we assume \(\eta\omega(q)\leq 1/8\). Let \(\tilde{p}\in\mathcal{P}(\mathcal{X})\) be a distribution such that \(\mu(\tilde{p})=\mu(q)\) and \(w(q)=H(\tilde{p})^{-1}\bullet V(q)\). Let \(\pi_{0}\in\mathcal{P}(\mathcal{X})\) be a distribution such that

\[a^{\top}H(\pi_{0})^{-1}a\leq d\] (32)

holds for any \(a\in\mathcal{A}\). Note that such a distribution always exists (see, e.g., [13, Theorem 21.1]). Define \(p\) by

\[p=(1-\gamma)\tilde{p}+\gamma\pi_{0}\] (33)

with \(\gamma=4\eta\omega(q)\leq 1/2\). With this \(p\), we define \(g\) by (4). Then, as we have \(\mathrm{bias}(g,p,q,\ell)=0\), we have

\[\Lambda_{q,\eta}(p,g)\leq\sup_{\ell\in\mathcal{A}}\left\{\langle\ell,\mu(p)- \mu(q)\rangle\right\}+\frac{1}{\eta}\sup_{\ell\in[-1,1]}\underset{a\sim p}{ \mathbf{E}}[\mathrm{stab}(\eta g(\cdot;a,f),q)].\] (34)

As the definition of \(p\) implies \(\mu(p)=(1-\gamma)\mu(\tilde{p})+\gamma\mu(\pi_{0})=(1-\gamma)\mu(q)+\gamma\mu (\pi_{0})\), it holds for any \(\ell\in\mathcal{L}\) that

\[\langle\ell,\mu(p)-\mu(q)\rangle=\langle\ell,(1-\gamma)\mu(q)+\gamma\mu(\pi_{ 0})-\mu(q)\rangle=\gamma\,\langle\ell,\gamma\mu(\pi_{0})-\mu(q)\rangle\leq 2\gamma.\] (35)

Further, from (7) and (4), we have

\[\mathrm{stab}(\eta g(\cdot;a,f),q) \leq\underset{x\sim q}{\mathbf{E}}\left[\phi(\eta g(x;a,f)-\eta g (\mu(q);a,f))\right]\] \[=\underset{x\sim q}{\mathbf{E}}\left[\phi\left(\eta f\cdot \left\langle(H(p))^{-1}a,x-\mu(q)\right\rangle\right)\right]\] (36)

for any fixed \(a\in\mathcal{A}\) and \(f\in[-1,1]\). Let \(r\) represent the distribution of \(f\cdot\left\langle(H(p))^{-1}a,x-\mu(q)\right\rangle\) for \(x\sim q\) with any fixed \(a\in\mathcal{A}\) and \(f\in[-1,1]\). Then, as \(q\) is a log-concave distribution, from Lemma 10, \(r\) is a log-concave distribution as well. Further, we have

\[H(r) =\underset{y\sim r}{\mathbf{E}}[y^{2}]=f^{2}\,\underset{x\sim q} {\mathbf{E}}\left[\left(\langle(H(p))^{-1}a,x-\mu(q)\rangle\right)^{2}\right]\] \[=f^{2}a^{\top}(H(p))^{-1}\,\underset{x\sim q}{\mathbf{E}}\left[( x-\mu(q))(x-\mu(q))^{\top}\right](H(p))^{-1}a\] \[=f^{2}a^{\top}(H(p))^{-1}V(q)(H(p))^{-1}a\leq a^{\top}(H(p))^{-1} V(q)(H(p))^{-1}a.\] (37)

Since we have

\[\mathrm{tr}(H(p)^{-1/2}V(q)H(p)^{-1/2}) =V(q)\bullet H(p)^{-1}=V(q)\bullet((1-\gamma)H(\tilde{p})+\gamma H (\pi_{0}))^{-1}\] \[\leq V(q)\bullet((1-\gamma)H(\tilde{p}))^{-1}=\frac{1}{1-\gamma} \omega(q),\] (38)we have

\[H(p)^{-1/2}V(q)H(p)^{-1/2}\preceq\frac{\omega(q)}{1-\gamma}I_{d}\] (39)

where \(I_{d}\) represents the identity matrix of size \(d\). Hence, we have

\[H(r) \leq a^{\top}(H(p))^{-1/2}\left(\frac{\omega(q)}{1-\gamma}I_{d} \right)(H(p))^{-1/2}a\leq\frac{\omega(q)}{1-\gamma}a^{\top}(H(p))^{-1}a\] \[=\frac{\omega(q)}{1-\gamma}a^{\top}((1-\gamma)H(\tilde{p})+\gamma H (\pi_{0}))^{-1}a\leq\frac{\omega(q)}{1-\gamma}a^{\top}(\gamma H(\pi_{0}))^{-1}a\] \[=\frac{\omega(q)}{(1-\gamma)\gamma}a^{\top}H(\pi_{0})^{-1}a\leq \frac{\omega(q)d}{(1-\gamma)\gamma}\leq 2\frac{\omega(q)d}{\gamma}=\frac{d}{2 \eta}.\] (40)

This implies that \(\eta^{2}H(r)\leq d\eta/2\leq 1/2\). Hence, from Lemma 13, we have

\[\mathrm{stab}(\eta g(\cdot;a,f),q)\leq\underset{y\sim r}{\mathbf{ E}}\left[\phi(\eta y)\right]=O\left(\eta^{2}H(r)\right)=O\left(\eta^{2}a^{\top}(H(p))^{-1}V (q)(H(p))^{-1}a\right).\] (41)

Hence, we have

\[\sup_{f\in[-1,1]}\underset{a\sim p}{\mathbf{E}}\left[\mathrm{stab }(\eta g(\cdot;a,f),q)\right]=O\left(\eta^{2}\underset{a\sim p}{\mathbf{E}} \left[a^{\top}(H(p))^{-1}V(q)(H(p))^{-1}a\right]\right)\] \[=O\left(\eta^{2}\mathrm{tr}\left(\underset{a\sim p}{\mathbf{E}} \left[aa^{\top}\right](H(p))^{-1}V(q)(H(p))^{-1}\right)\right)=O\left(\eta^{2 }V(q)\bullet(H(p))^{-1}\right)\] \[\leq O\left(\frac{\eta^{2}}{1-\gamma}V(q)\bullet(H(\tilde{p}))^{ -1}\right)=O\left(\frac{\eta^{2}\omega(q)}{1-\gamma}\right)=O\left(\eta^{2} \omega(q)\right).\] (42)

Combining this with (34) and (35), we obtain

\[\Lambda_{q,\eta}(p,g)\leq 2\gamma+\frac{1}{\eta}\sup_{f\in[-1,1]} \underset{a\sim p}{\mathbf{E}}\left[\mathrm{stab}(\eta g(\cdot;a,f),q)\right] =O\left(\gamma+\eta\omega(q)\right)=O\left(\eta\omega(q)\right),\] (43)

which implies that \(\Lambda_{q,\eta}^{*}=O(\eta\omega(q))\).

We next consider the case in which \(\eta\leq 1\) holds. As we have \(\Lambda_{q,\eta}^{*}=O(\eta\omega^{\prime}(q))\) if \(\eta\omega^{\prime}(q)>1/8\) (since \(\Lambda_{q,\eta}^{*}=O(1)\) for any \(q\) and \(\eta\)), we assume \(\eta\omega^{\prime}(q)\leq 1/8\). We set \(p\) by (33) where \(\tilde{p}\) is the minimizer in the right-hand side of (12) and \(\gamma=4\eta\omega^{\prime}(q)\leq 1/2\). We set \(g\) by (4). Then, (34) and (35) can be shown in a similar way to in the first half of this proof. As we have

\[dV(q)\preceq\omega^{\prime}(q)H(\tilde{p})\preceq\frac{\omega^{ \prime}(q)}{1-\gamma}H(p),\] (44)

we have

\[H(p)^{-1/2}V(q)H(p)^{-1/2}\preceq\frac{\omega^{\prime}(q)}{(1- \gamma)d}I_{d}.\] (45)

From this, if we define \(r\) in a similar way to in the first half of this proof, we have \(H(r)\leq 1/(2\eta)\). We hence have \(\eta^{2}H(r)\leq\eta/2\leq 1/2\) under the assumption of \(\eta\leq 1\). We can then apply Lemma 13 to obtain the following:

\[\sup_{f\in[-1,1]}\underset{a\sim p}{\mathbf{E}}\left[\mathrm{ stab}(\eta g(\cdot;a,f),q)\right]\leq O\left(\frac{\eta^{2}}{1-\gamma}V(q)\bullet(H( \tilde{p}))^{-1}\right)\] \[\leq O\left(\frac{\eta^{2}}{1-\gamma}\frac{\omega^{\prime}(q)}{ d}H(\tilde{p})\bullet(H(\tilde{p}))^{-1}\right)=O\left(\frac{\eta^{2}\omega^{ \prime}(q)}{1-\gamma}\right)=O\left(\eta^{2}\omega^{\prime}(q)\right),\] (46)

where the first inequality follows from a similar argument as in (42). Combining this with (34), (35), and \(\gamma=4\eta\omega^{\prime}(q)\), we obtain \(\Lambda_{q,\eta}(p,g)=O(\eta\omega^{\prime}(q))\), which implies \(\Lambda_{q,\eta}^{*}=O(\eta\omega^{\prime}(q))\). \(\square\)

### Proof of Lemma 3

Proof of Lemma 3.We will show that

\[\text{there exists}\quad p\in\mathcal{P}(\mathcal{X})\quad\text{such that}\quad\mu(p)=\mu(q)\quad\text{and}\quad V(q)\preceq z\cdot V(p)\] (47)

with \(z=O(1,\Delta_{\ell^{*}}(\mu(q))/\Delta_{\ell^{*},\min})\). As we have \(H(p)=V(p)+\mu(p)\mu(p)^{\top}\succeq V(p)\), (47) leads to \(d\cdot V(q)\preceq dz\cdot V(p)\preceq dz\cdot H(p)\), which implies \(\omega^{\prime}(q)\leq dz\). Hence, to prove the lemma, it is sufficient to show that (47) holds with \(z=O(1,\Delta_{\ell^{*}}(\mu(q))/\Delta_{\ell^{*},\min})\). By considering the case of \(p=q\), we can easily see that (47) holds with \(z=O(1)\). for any \(q\). In the following, we show that (47) holds with \(z=O\left(\frac{\Delta_{\ell^{*}}(\mu(q))}{\Delta_{\ell^{*},\min}}\right)\).

Without loss of generality, we may assume \(\mu(q)=0\), via a variable transformation \(x\gets x-\mu(q)\). In fact, such a transformation preserves \(V(q)\) and \(V(p)\). For any \(x\in\mathcal{X}\), there exists \(\xi_{x}=(\xi_{x}(a))_{a\in\mathcal{A}}\in\mathcal{P}(\mathcal{A})\) such that \(\mu(\xi_{x})=\sum_{a\in\mathcal{A}}\xi_{x}(a)=x\). For any \((\xi_{x})_{x\in\mathcal{X}}\), define \(p\in\mathcal{P}(\mathcal{A})\) by \(p(a)=\mathbf{E}_{x\sim q}[\xi_{x}(a)]\). We then have \(\mu(p)=\mathbf{E}_{a\sim p}[x]=\mathbf{E}_{x\sim q}[\mathbf{E}_{a\sim\xi_{s}} [a]]=\mathbf{E}_{x\sim q}[x]=\mu(q)=0\). Fix an arbitrary non-zero vector \(u\in\mathbb{R}^{d}\setminus\{0\}\) and an arbitrary \(a^{*}\in\mathcal{A}\). Denote \(\varepsilon=1-\Pr_{a\sim p}[a=a^{*}]\).

Set \(v=u/\sqrt{u^{\top}V(q)u}\). Let \(p_{v}\) and \(q_{v}\) denote the distributions of \(\langle v,a\rangle\) for \(a\sim p\) and \(\langle v,x\rangle\) for \(x\sim q\), respectively. We then have \(V(q_{v})=v^{\top}V(q)v=u^{\top}V(q)u/(u^{\top}V(q)u)=1\). Without loss of generality, we assume that \(\langle v,a^{*}\rangle\geq\langle v,\mu(q)\rangle=\mu(q_{v})=0\). (If not, we redefine \(v=-u/\sqrt{u^{\top}V(q)u}\).) Since \(q\) is a log-concave distribution, from Lemma 10, \(q_{v}\) is a log-concave distribution as well. Hence, from Lemma 11, we have

\[\Pr_{X\sim q_{v}}\left[X\leq-\frac{1}{2\mathrm{e}}\right]=\Pr_{X\sim q_{v}} \left[X\leq 0\right]-\Pr_{X\sim q_{v}}\left[-\frac{1}{2\mathrm{e}}<X\leq 0 \right]\geq\frac{1}{\mathrm{e}}-\frac{1}{2\mathrm{e}}=\frac{1}{2\mathrm{e}},\] (48)

where the inequality follows from (16) and (17).

We hence have

\[V(p_{v}) =\mathop{\mathbf{E}}_{a\sim p}[\langle\langle v,a\rangle\rangle^ {2}]=\mathop{\mathbf{E}}_{x\sim q}\left[\mathop{\mathbf{E}}_{a\sim\xi_{x}} \left[\langle\langle v,a\rangle\rangle^{2}\right]\right]\] \[\geq\mathop{\mathbf{E}}_{x\sim q}\left[\mathop{\mathbf{E}}_{a \sim\xi_{x}}\left[\langle\langle v,a\rangle\rangle^{2}\right]|\langle v,x \rangle\leq-\frac{1}{2\mathrm{e}}\right]\cdot\Pr_{X\sim q_{v}}\left[X\leq- \frac{1}{2\mathrm{e}}\right]\] \[\geq\frac{1}{2\mathrm{e}}\cdot\mathop{\mathbf{E}}_{x\sim q} \left[\mathop{\mathbf{E}}_{a\sim\xi_{x}}\left[\langle\langle v,a\rangle\rangle ^{2}\right]|\langle v,x\rangle\leq-\frac{1}{2\mathrm{e}}\right],\] (49)

where the last inequality follows from (48). Suppose \(\langle v,x\rangle\leq-\frac{1}{2\mathrm{e}}\). Since \(\mu(\xi_{x})=x\), we have

\[-\frac{1}{2\mathrm{e}}\geq\langle v,x\rangle=\xi_{x}(a^{*})\left\langle v,a^{ *}\right\rangle+\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}\xi_{x}(a)\left\langle v,a\right\rangle\geq\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}\xi_{x}(a)\left\langle v,a\right\rangle.\] (50)

We hence have

\[\mathop{\mathbf{E}}_{a\sim\xi_{x}}\left[\langle\langle v,a\rangle\rangle^{2} \right]\geq\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}\xi_{x}(a)(\langle v,a \rangle)^{2}=(1-\xi_{x}(a^{*}))\frac{\sum_{a\in\mathcal{A}\setminus\{a^{*}\}} \xi_{x}(a)(\langle v,a\rangle)^{2}}{\sum_{a\in\mathcal{A}\setminus\{a^{*}\}} \xi_{x}(a)}\]

\[\geq(1-\xi_{x}(a^{*}))\left(\frac{\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}\xi_{ x}(a)\left\langle v,a\right\rangle}{\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}\xi_{x}(a)} \right)^{2}=\frac{\left(\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}\xi_{x}(a) \left\langle v,a\right\rangle\right)^{2}}{1-\xi_{x}(a^{*})}\geq\frac{1}{4 \mathrm{e}^{2}(1-\xi_{x}(a^{*}))},\]

where the second inequality comes from Jensen's inequality, and the last inequality follows from (50). From this and (49), we have

\[V(p_{v})\geq\frac{1}{8\mathrm{e}^{3}}\mathop{\mathbf{E}}_{x\sim q}\left[\frac{1 }{1-\xi_{x}(a^{*})}|\left\langle v,x\right\rangle\leq-\frac{1}{2\mathrm{e}} \right]\geq\frac{1}{8\mathrm{e}^{3}}\cdot\frac{1}{\mathop{\mathbf{E}}_{x\sim q }\left[1-\xi_{x}(a^{*})|\left\langle v,x\right\rangle\leq-\frac{1}{2\mathrm{e}} \right]}.\] (51)

We further have

\[\varepsilon =\mathop{\mathbf{E}}_{x\sim q}\left[1-\xi_{x}(a^{*})\right]\geq \mathop{\mathbf{E}}_{x\sim q}\left[1-\xi_{x}(a^{*})|\left\langle v,x\right\rangle \leq-\frac{1}{2\mathrm{e}}\right]\cdot\Pr_{x\sim q}\left[\langle v,x\rangle \leq-\frac{1}{2\mathrm{e}}\right]\] \[\geq\frac{1}{2\mathrm{e}}\mathop{\mathbf{E}}_{x\sim q}\left[1-\xi_ {x}(a^{*})|\left\langle v,x\right\rangle\leq-\frac{1}{2\mathrm{e}}\right],\] (52)where the last inequality follows from (48). Combining this with (51), we obtain

\[V(p_{v})\geq\frac{1}{16\mathrm{e}^{4}\varepsilon}.\] (53)

As it follows from the definition of \(v\) that

\[V(p_{v})=v^{\top}V(p)v=\frac{u^{\top}V(p)u}{u^{\top}V(q)u},\] (54)

we have

\[16\mathrm{e}^{4}\varepsilon\cdot u^{\top}V(p)u\geq u^{\top}V(q)u.\] (55)

Since this holds for all \(u\in\mathbb{R}^{d}\setminus\{0\}\), we have

\[16\mathrm{e}^{4}\varepsilon\cdot V(p)\succeq V(q)\] (56)

Letting \(a^{*}\in\operatorname*{arg\,min}_{a\in\mathcal{A}}\left\langle\ell^{*},a\right\rangle\), we obtain

\[\Delta_{\ell^{*}}(\mu(q)) =\Delta_{\ell^{*}}(\mu(p))=\sum_{a\in\mathcal{A}}p(a)\Delta_{\ell ^{*}}(a)\] \[\geq\sum_{a\in\mathcal{A}\setminus\{a^{*}\}}p(a)\Delta_{\ell^{*},\min}=\Delta_{\ell^{*},\min}\cdot(1-p(a^{*}))=\Delta_{\ell^{*},\min}\cdot\varepsilon\] (57)

Combining this with (56), we obtain

\[16\mathrm{e}^{4}\frac{\Delta_{\ell^{*}}(\mu(q))}{\Delta_{\ell^{*},\min}}\cdot V (p)\succeq V(q),\] (58)

which completes the proof.

### Proof of Lemma 4

Proof.: Let \(q\) be a log-concave distribution over \(\mathcal{X}=\{x\in[0,1]^{d}\mid\|x\|_{1}=1\}\). If \(x=(x_{1},\ldots,x_{d})\sim q\), from Lemma 10, \(x_{i}\) for each \(i\) also follows a log-concave distribution over \([0,1]\). Hence, from Lemma 12, the variance of \(x_{i}\) ( i.e., the \((i,i)\) entry of \(V(q)\) ) is bounded as

\[[V(q)]_{i,i}\leq\mathrm{e}^{2}\min\{\mu_{i}(q)^{2},(1-\mu_{i}(q))^{2}\}\leq 4 \mathrm{e}^{2}\mu_{i}(q)^{2}(1-\mu_{i}(q))^{2}\] (59)

Consider the distribution \(p\in\mathcal{P}(\mathcal{A})\) such that \(\Pr_{a\sim p}[a=e_{i}]=\mu(q)\). We then have \(\mu_{i}(p)=\mu_{i}(q)\) and that \(H(p)\) is a diagonal matrix with diagonal entries \((\mu_{i}(q))_{i=1}^{d}\). Hence, from (59), we have

\[V(q)\bullet(H(p))^{-1} =\sum_{i=1}^{d}[V(q)]_{i,i}(\mu_{i}(q))^{-1}\leq 4\mathrm{e}^{2} \sum_{i=1}^{d}\mu_{i}(q)(1-\mu_{i}(q))^{2}\] \[\leq 4\mathrm{e}^{2}\sum_{i=1}^{d}\mu_{i}(q)(1-\mu_{i}(q)),\] (60)

which implies that \(\omega(q)=O\left(\sum_{i=1}^{d}\mu_{i}(q)(1-\mu_{i}(q))\right)\). Further, as we have \(\sum_{i=1}^{d}\mu_{i}(q)=1\), it holds for any \(i^{*}\in\{1,\ldots,d\}\) that

\[\sum_{i=1}^{d}\mu_{i}(q)(1-\mu_{i}(q)) =\mu_{i^{*}}(q)(1-\mu_{i^{*}}(q))+\sum_{i\neq i^{*}}\mu_{i}(q)(1- \mu_{i}(q))\] \[\leq 1-\mu_{i^{*}}(q)+\sum_{i\neq i^{*}}\mu_{i}(q)=2\left(1-\mu_{ i^{*}}(q)\right),\] (61)

which complete the proof. 

### Proof of Corollary 2

Proof.: From Lemma 4 and the fact that \(\sum_{i=1}^{d}\mu_{i}(q)=1\), it is clear that \(\omega(q)=O(1)\). Let \(\ell^{*}\in\mathcal{L}\) be an a loss vector such that \(e_{i^{*}}\in\operatorname*{arg\,min}_{1\leq i\leq d}\ell_{i}^{*}\) is unique. We then have \(\Delta_{\ell^{*}}(\mu(q))=\sum_{i\neq i^{*}}\Delta_{\ell^{*}}(e_{i})\mu_{i}\geq \Delta_{\ell^{*},\min}(1-\mu_{i^{*}}(q))\). Hence, from Lemma 4, we have \(\omega(q)=O\left(1-\mu_{i^{*}}(q)\right)\leq O\left(\Delta_{\ell^{*}}(\mu(q)) /\Delta_{\ell^{*},\min}\right)\). From this and Lemma 2, (10) holds with \(\alpha=O(1)\), \(\beta=O(\frac{1}{\Delta_{\ell^{*},\min}})\), and \(\eta_{0}=O(1/d)\).

### Proof of Lemma 5

Proof.: For each \(j\in\{1,\ldots,m\}\), let \(p^{(j)}\in\mathcal{P}(\mathcal{X}^{(j)})\) be such that \(\mu(p^{(j)})=\mu(q^{(j)})\) and \(\omega(q^{(j)})=V(q^{(j)})\bullet(H(p^{(j)}))^{-1}\). Let \(p\in\mathcal{P}(\mathcal{X})\) be the product measure of \(p^{(1)},\ldots,p^{(m)}\). As \(p\) is the product measure of \(q^{(1)},\ldots,q^{(m)}\), \(V(q)\in\mathbb{R}^{d\times d}\) is a block diagonal matrix with submatrices \(V(q^{(1)}),\ldots,V(q^{(m)})\). Similarly, \(H(p)\in\mathbb{R}^{d\times d}\) is a block diagonal matrix with submatrices \(H(p^{(1)}),\ldots,H(p^{(m)})\). We hence have \(V(q)\bullet(H(p))^{-1}=\sum_{j=1}^{m}V(q^{(j)})\bullet(H(p^{(j)}))^{-1}=\sum_{ j=1}^{m}\omega(q^{(j)}),\) which implies \(\omega(q)\leq\sum_{j=1}^{m}\omega(q^{(j)})\). 

### Proof of Corollary 3

Proof.: Suppose \(\mathcal{A}\) is given by \(\mathcal{A}=\mathcal{A}_{d_{1}}\times\cdots\times\mathcal{A}_{d_{m}}\). For any \(\ell=(\ell^{(1)},\ell^{(2)},\ldots,\ell^{(m)})\in\mathcal{L}\) and \(x=(x^{(1)},x^{(2)},\ldots,x^{(m)})\in\mathcal{X}\), where \(\ell^{(j)}\in\mathbb{R}^{d_{j}}\) and \(x^{(j)}\in\mathcal{X}^{(j)}\) for each \(j\), denote \(\Delta_{\ell^{(j)}}^{(j)}(x^{(j)})=\left<\ell^{(j)},x^{(j)}\right>-\min_{a^{(j )}\in\mathcal{A}^{(j)}}\left<\ell^{(j)},a^{(j)}\right>=\left<\ell^{(j)},x^{(j) }-a^{*(j)}\right>\). We then have \(\Delta_{\ell^{(j)}}^{(j)}(x^{(j)})\). Let \(\ell\) be a loss vector such that \(a^{*}=(a^{*(1)},a^{*(2)},\ldots,a^{*(m)})\in\arg\min_{a\in\mathcal{A}}\left< \ell,a\right>\) is unique. We then have \(\Delta_{\ell,\min}=\min_{1\leq j\leq m}\Delta_{\ell^{(j)},\min}^{(j)}\).

If \(q\in\mathcal{X}\) is a product measure of \(q_{j}\in\mathcal{P}(\mathcal{X}^{(j)})\), from Lemmas 4 and 5, we have

\[\omega(q) \leq O\left(\sum_{j=1}^{d}\omega(q^{(j)})\right)\leq O\left( \sum_{j=1}^{m}\min\left\{1,\frac{\Delta_{\ell^{*(j)}}^{(j)}(\mu(q^{(j)}))}{ \Delta_{\ell^{*(j)},\min}}\right\}\right)\] \[\leq O\left(\min\left\{m,\sum_{j=1}^{m}\frac{\Delta_{\ell^{*(j)}} ^{(j)}(\mu(q^{(j)}))}{\Delta_{\ell^{*(j)},\min}}\right\}\right)\leq O\left( \min\left\{m,\sum_{j=1}^{m}\frac{\Delta_{\ell^{*(j)}}^{(j)}(\mu(q^{(j)}))}{ \Delta_{\ell^{*},\min}}\right\}\right)\] \[=O\left(\min\left\{m,\frac{\Delta_{\ell^{*}}^{(j)}(\mu(q))}{ \Delta_{\ell^{*},\min}}\right\}\right).\] (62)

This means that (10) holds with \(\alpha=O(m)\) and \(\beta=O(\frac{1}{\Delta_{\ell^{*},\min}})\).

The case of \(\mathcal{A}=\{0,1\}^{d}\) can be interpreted as a problem instance in which \(\mathcal{A}\) is a direct product of \(d\) copies of \(\mathcal{A}_{2}\). Hence, we can see that (10) holds with \(\alpha=O(d)\) and \(\beta=O(\frac{1}{\Delta_{\ell^{*},\min}})\). 

### Proof of Lemma 6

Proof.: From the definition of the regret, we have

\[R_{T}(a^{*}) =\sum_{t=1}^{T}\mathop{\mathbf{E}}_{a\sim p_{t}}\left[\left<\ell _{t},a-a^{*}\right>\right]\] \[=\sum_{t=1}^{T}\left(\left<\ell_{t},w_{t}-a^{*}\right>+\left<\ell _{t},w_{t}-\mu(p_{t})\right>\right).\] (63)

From the definition of \(\mathrm{bias}\) given in (14), the part of \(\left<\ell_{t},w_{t}-a^{*}\right>\) can be bounded as follows:

\[\left<\ell_{t},w_{t}-a^{*}\right> =\mathop{\mathbf{E}}_{a_{t},f_{t}}\left[\left<\ell_{t}-h_{t}(a_{t},f_{t}),w_{t}-a^{*}\right>+\left<h_{t}(a_{t},f_{t}),w_{t}-a^{*}\right>\right]\] \[\leq\mathrm{bias}(h_{t},p_{t},w_{t},\ell_{t})+\mathop{\mathbf{E} }_{a_{t},f_{t}}\left[h_{t}(a_{t},f_{t}),w_{t}-a^{*}\right].\] (64)

From this and (63), we have

\[R_{T}(a^{*})\leq\mathop{\mathbf{E}}\left[\sum_{t=1}^{T}\left(\left<\ell_{t},w_ {t}-\mu(p_{t})\right>+\mathrm{bias}(h_{t},p_{t},w_{t},\ell_{t})\right)+\sum_{t =1}^{T}\left<h_{t}(a_{t},f_{t}),w_{t}-a^{*}\right>\right].\] (65)

By a standard analysis of the FTRL framework (see, e.g., [15, Exercise 28.12]), we have

\[\sum_{t=1}^{T}\left<h_{t}(a_{t},f_{t}),w_{t}-a^{*}\right>\leq\sum_{t=1}^{T} \frac{1}{\eta_{t}}\mathrm{stab}_{\psi}(\eta_{t}h_{t}(a_{t},f_{t}),w_{t})+\frac{ 1}{\eta_{T+1}}\psi(a^{*}).\] (66)By combining (65) and (66), we obtain the bound of (14).