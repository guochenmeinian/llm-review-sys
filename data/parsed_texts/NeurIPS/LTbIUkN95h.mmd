Sample Efficient Reinforcement Learning in Mixed Systems through Augmented Samples and Its Applications to Queueing Networks

 Honghao Wei

Washington State University

honghao.wei@wsu.edu

&Xin Liu

ShanghaiTech University

liuxin7@shanghaitech.edu.cn

&Weina Wang

Carnegie Mellon University

weinaw@cs.cmu.edu

&Lei Ying

University of Michigan, Ann Arbor

leiying@umich.edu

###### Abstract

This paper considers a class of reinforcement learning problems, which involve systems with two types of states: stochastic and pseudo-stochastic. In such systems, stochastic states follow a stochastic transition kernel while the transitions of pseudo-stochastic states are deterministic _given_ the stochastic states/transitions. We refer to such systems as mixed systems, which are widely used in various applications, including manufacturing systems, communication networks, and queueing networks. We propose a sample efficient RL method that accelerates learning by generating augmented data samples. The proposed algorithm is data-driven and learns the policy from data samples from both real and augmented samples. This method significantly improves learning by reducing the sample complexity such that the dataset only needs to have sufficient coverage of the stochastic states. We analyze the sample complexity of the proposed method under Fitted Q Iteration (FQI) and demonstrate that the optimality gap decreases as \(\tilde{\mathcal{O}}(\sqrt{1/n}+\sqrt{1/m}),\) where \(n\) is the number of real samples and \(m\) is the number of augmented samples per real sample. It is important to note that without augmented samples, the optimality gap is \(\tilde{\mathcal{O}}(1)\) due to insufficient data coverage of the pseudo-stochastic states. Our experimental results on multiple queueing network applications confirm that the proposed method indeed significantly accelerates learning in both deep Q-learning and deep policy gradient.

## 1 Introduction

Reinforcement learning (RL) algorithms have recently achieved superhuman performance in gaming, such as AlphaGo (Silver et al., 2017), and AlphaStar (Vinyals et al., 2019), under the premise that vast amounts of training data can be collected. However, collecting data in real-world could be expensive and time-consuming applications such as clinical trials and autonomous driving, posing a significant challenge to extending the success of RL to broader applications. In this paper, we are interested in sample efficient algorithms for a class of problems in which environments (also called systems in this paper) include both stochastic and deterministic transitions, resulting in two types of states: stochastic states and pseudo-stochastic states. For example, in a queueing system, customers arrive and depart, following certain stochastic processes, but the evolution of the queues given arrivals and departures are deterministic. We call these kinds of systems mixed systems. Mixed systems are common in various RL applications, including data centers, ride-sharing systems, and communication networks,which can be modeled as queueing networks so as mixed systems. The broad application of mixed systems makes it crucial to explore whether current RL approaches are efficient in learning optimal policies for such systems. However, existing RL approaches are often not sample efficient due to the curse of dimensionality. For instance, in a queueing system, the state space (i.e. the queue lengths) is unbounded, which means that a tremendous amount of data samples are required to adequately cover the state space in order to learn a near-optimal policy. To highlight the challenge, in a recent paper (Dai and Gluzman, 2022), a 96-core processor with \(1,510\) GB of RAM was used to train a queueing policy for a queueing network with only a few nodes.

This paper proposes a new approach to handling the curse of dimensionality based on augmented samples. Our approach is based on two observations:

* When dealing with a mixed system that has a large state space, model-based approaches are incredibly challenging to use. For example, as of today, it remains impossible to analytically determine the optimal policy for a queueing network. On the other hand, using a deep neural network can be regarded as a numerical method for solving a large-scale optimization problem. Therefore, a data-driven, neural network-based solution could be a much more efficient approach.
* It is well-known that training deep learning is data-hungry. While, in principle, model-free approaches can be directly used for mixed systems, they are likely to be very inefficient in sample complexity for mixed systems whose state space is large. In this paper, we will utilize the knowledge of deterministic transitions to generate new data samples by augmenting existing real samples. In other words, we can generalize real samples to a large (even infinite) number of samples that cover the unobserved pseudo-stochastic states. These samples are equally useful for training the neural network as the real samples.

Based on the two observations above, we consider a mixed system that includes two types of states, stochastic states and pseudo-stochastic states, where the transitions of the stochastic states are driven by a stochastic kernel, and the transitions of the pseudo-states are deterministic and conditioned on the current and next stochastic states. We comment that without conditioning the stochastic states, the pseudo-stochastic states become stochastic. The distributions of stochastic states and pseudo-stochastic states are correlated, which makes the problem different from MDPs with exogenous inputs.

With this state separation, we propose an augmented sample generator (ASG). The sample generator generates virtual samples from real ones while keeping stochastic states and augmenting the pseudo-stochastic states. Both the real samples and virtual samples are then used to train the deep neural networks, e.g., Q-networks or policy networks.

We analyze the sample complexity of the proposed approach for mixed systems under Fitted Q Iteration (FQI Ernst et al. (2005)) which is equivalent to DQN (Mnih et al., 2015) for tabular setting. Specifically, we consider the scenario where the size of pseudo-stochastic state space is much larger than that of stochastic state space, and the set of available real data samples does not provide sufficient coverage of the joint state space. This is the situation where the proposed approach is expected to be particularly advantageous. Our analysis demonstrates that the proposed approach yields a significant improvement in the convergence rate for tabular settings. In particular, by generating \(m\) virtual samples for each real sample, the optimality gap between the learned policy and the optimal policy decreases as \(\tilde{\mathcal{O}}(\sqrt{1/n}+\sqrt{1/m}),\) where \(n\) is the number of real samples. Note that without augmented samples, the error is \(\tilde{\mathcal{O}}(1)\) due to the lack of data coverage of the pseudo-stochastic states, which reduces to \(\tilde{\mathcal{O}}(\sqrt{1/n})\) when we generate at least \(n\) augmented samples for each real sample. We also would like to emphasize that \(\omega(\sqrt{1/n})\) is also the fundamental lower bound due to the coverage of the stochastic states.

### Related Work

Deep reinforcement learning with augmented samples is not new. However, the approach in this paper has fundamental differences compared with existing works in the literature, which we will explain in detail.

**Dyna and its variants:** The first group of related methods is Dyna (Sutton, 1988) and its extensions. Dyna-type algorithms are an architecture that integrates learning and planning for speeding up learning or policy convergence for Q-learning. However, our proposed method differs fundamentally from these approaches in several aspects: \((1)\) Dyna-type algorithms are model-based methods and need to estimate the system model which will be used to sample transitions. On the contrary, ASG does not require such an estimation and instead leverages the underlying dynamics of the system to generate many augmented samples from one real sample. \((2)\) Dyna is limited to making additional updates only for states that have been observed previously, whereas ASG has the potential to update all pseudo-stochastic states, including those that have not yet been explored. \((3)\) Since Dyna is a model-based approach, the memory complexity is \(|\mathcal{S}|^{2}|\mathcal{X}|^{2}|\mathcal{A}|\), where \(\mathcal{S}\) is the stochastic state space, \(\mathcal{X}\) is the pseudo-stochastic state space and \(\mathcal{A}\) is the action space. ASG employs a replay buffer only for real data samples, which typically requires far less memory space. Moreover, some Dyna-type approaches like Dyna-Q\({}^{+}\)(Sutton, 1988), Linear Dyna (Sutton et al., 2012), and Dyna-Q\((\lambda)\)(Yao et al., 2009), require more computational resources to search the entire state space for updating the order of priorities.

Dyna-type approaches have been successfully used in model-based online reinforcement learning for policy optimization, including several state-of-the-art algorithms, including ME-TRPO (Kurutach et al., 2018), SLBO (Luo et al., 2019), MB-MPO (Clavera et al., 2018), MBPO (Janner et al., 2019), MOPO (Yu et al., 2020). As we already mentioned above, ASG is fundamentally different from these model-based approaches. The key differences between Dyna-type algorithms and ASG are summarized in Table 1.

**Data augmentation:** It has been shown that data augmentation can be used to efficiently train RL algorithms. Laskin et al. (2020) showed that simple data augmentation mechanisms such as cropping, flipping, and rotating can significantly improve the performance of RL algorithms. Fan et al. (2021) used weak and strong image augmentations to learn a robust policy. The images are weakly or strongly distorted to make sure the learned representation is robust. However, these methods rely solely on image augmentation, and none of them consider the particular structure of the mixed system. The purpose of augmentation and distortion is to improve robustness and generalization. Our approach is to use virtual samples, which are as good as real samples to learn an optimal policy. The purpose of introducing the virtual samples is to improve the data coverage for learning the optimal policy.

Encoding symmetries into the designs of neural networks to enforce translation, rotation, and reflection equivariant convolutions of images have also been proposed in deep learning, like G-Convolution (Cohen and Welling, 2016a), Steerable CNN (Cohen and Welling, 2016b), and E(2)-Steerable CNNs (Weiler and Cesa, 2019). Mondal et al. (2020); Wang et al. (2022) investigated the use of Equivariant DQN to train RL agents. van der Pol et al. (2020) imposed symmetries to construct equivariant network layers, i.e., imposing physics into the neural network structures. Lin et al. (2020) used symmetry to generate feasible virtual trajectories for training robotic manipulation to accelerate learning. Our approach relies on the structure of mixed systems to generate virtual samples for learning an optimal policy and can incorporate much more general knowledge than symmetry.

In addition, existing works on low-dimensional state and action representations in RL are also related to our research focus on representing the MDP in a low-dimensional latent space to reduce the complexity of the problem. For example, Modi et al. (2021); Agarwal et al. (2020) studied provably representation learning for low-rank MDPs. Misra et al. (2020); Du et al. (2019) investigated the sample complexity and practical learning in Block MDPs. There are also practical algorithms (Ota et al., 2020; Sekar et al., 2020; Machado et al., 2020; Pathak et al., 2017) with non-linear function approximation from the deep reinforcement learning literature. Although the mixed model studied in this paper can be viewed as an MDP with a low-dimensional stochastic transition kernel, the state space of the MDP does not have a low-dimensional structure, which makes the problem fundamentally different from low-dimensional representations in RL.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Algorithm} & **estimate** & **update** & **computational** & **store** & **convergence** \\  & **model?** & **unseen states?** & **efficient?** & **all transitions?** & **analysis?** \\ \hline Dyna-type & ✓ & ✗ & ✗ & ✗ & ✗ \\ \hline ASG & ✗ & ✓ & ✓ & ✓ & ✓ \\ \hline \end{tabular}
\end{table}
Table 1: Dyna-type v.s. ASG

**Factored MDP:** A factored MDP (Kearns and Koller, 1999) is an MDP whose reward function and transition kernel exhibit some conditional independence structure. In a factored MDP, the state space is factored into a set of variables or features, and the action space is factored into a set of actions or control variables. This factorization simplifies the representation and allows for more efficient computation and decision-making. While factored MDPs and mixed systems share some similarities, they are actually quite different. In a factored MDP, state variables are grouped into sets, allowing for the factoring of rewards that depend on specific state subsets, and localized (or factorized) transition probabilities. In a mixed system, the cost/reward function is _not_ assumed to be factored. Furthermore, although the transition probabilities of stochastic states are localized, depending only on stochastic states and actions, the transitions of pseudo-stochastic states generally depend on the full state vector and are _not_ factorized. However, these transitions are _deterministic_, which allows us to generate augmented samples. Given the fundamental differences between the two, the analysis and the results are quite different despite the high-level similarity. Additionally, for the batch offline RL setting without enough coverage used in our paper, there are no existing approaches, including factored MDPs, that guarantee performance in this scenario.

**MDPs with exogenous inputs:** A recent paper considers MDP with exogenous inputs (Sinclair et al., 2022), where the system has endogenous states and exogenous states. The fundamental difference between exogenous-state/endogenous state versus stochastic-state/pseudo-stochastic-state is that the evolution of the exogenous state is independent of the endogenous state, while the evolution of the stochastic state depends on the action. Since the action is chosen based on the pseudo-stochastic state, the evolution of a stochastic state, therefore, depends on the pseudo-stochastic state.

The most significant difference between this paper and existing works is that we propose the concept of mixed systems and mixed system models. Based on the mixed system models, we develop principled approaches to generate virtual data samples, which are as informative for learning as real samples and enhance RL algorithms with much lower sample complexity. Equally significant, we provide sample complexity analysis that theoretically quantifies the sample complexity improvement under the proposed approach and explains the reason that principled data augmentation improves learning in RL.

## 2 Mixed Systems and Mixed System Models

We consider a discrete-time Markov decision process \(M=(\mathcal{S}\times\mathcal{X},\mathcal{A},P,R,\gamma,\eta_{0}),\) whose state space is \(\mathcal{S}\times\mathcal{X}\), where \(\mathcal{S}\) is the set of stochastic states and \(\mathcal{X}\) is the set of pseudo-stochastic states, and both \(\mathcal{S}\) and \(\mathcal{X}\) are assumed to be finite. Further, \(\mathcal{A}\) is a finite action space, \(R:\mathcal{S}\times\mathcal{X}\times\mathcal{A}\rightarrow[0,r_{\max}]\) is a deterministic and known cost function, and \(P:\mathcal{S}\times\mathcal{X}\times\mathcal{A}\rightarrow\Delta(\mathcal{S} \times\mathcal{X})\) is the transition kernel (where \(\Delta(\cdot)\) is the probability simplex).

The transition kernel \(P\) is specified in the following way. The transitions of the stochastic state follow a stochastic transition kernel. In particular, the transition of the stochastic state \(S_{t}\) at time \(t\) can be represented as

\[p_{ij}(a)=P(S_{t+1}=j|S_{t}=i,a_{t}=a).\] (1)

We assume that this transition is independent of the pseudo-stochastic state \(X_{t}\) (and everything else at or before time \(t\)). The transition of the pseudo-stochastic state is then deterministic, governed by a function \(g\) as follows:

\[X_{t+1}=g(S_{t},X_{t},a_{t},S_{t+1}).\] (2)

Our system \(M\) is said to be a _mixed system_ because it includes both stochastic and deterministic transitions. Our mixed system model then consists of the stochastic transition kernel and the deterministic transition function \(g\).

We focus on discounted costs in this paper. Given a mixed system, the value function of a policy \(\pi\) is defined as

\[V^{\pi}(s,x):=\mathbb{E}\left[\sum_{h=0}^{\infty}\gamma^{h}R(s_{h},x_{h},a_{h })\right|(s_{0},x_{0})=(s,x),a_{h}=\pi(s_{h},x_{h})\right],\] (3)

where \(\gamma\) is the discount factor, and the expectation is taken over the transitions of stochastic states and randomness in the policy. Let \(v^{\pi}=\mathbb{E}_{(s_{0},x_{0})\sim\eta_{0}}[V^{\pi}(s_{0},x_{0})]\), i.e., the expected cost when the initial distribution of the state is \(\eta_{0}\). Let \(v^{*}=\min_{\pi}v^{*}\).

The \(Q\)-value function of a policy \(\pi\) is defined as

\[Q^{\pi}(s,x,a):=\mathbb{E}\left[\left.\sum_{h=0}^{\infty}\gamma^{h}R(s_{h},x_{h},a _{h})\right|(s_{0},x_{0})=(s,x),a_{0}=a,a_{h}=\pi(s_{h},x_{h})\right].\] (4)

Let \(Q\) be the optimal \(Q\)-value function. Then the Bellman equation can be written as

\[Q(s,x,a)=R(s,x,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}\left[\min_{a ^{\prime}}Q(s^{\prime},g(s,x,a,s^{\prime}),a^{\prime})\right].\] (5)

Note that the value function and the \(Q\)-value function both take values in \([0,V_{\max}]\) for some finite \(V_{\max}\) under any policy, due to the assumption that the cost is in \([0,r_{\max}]\).

### Example: A Wireless Downlink Network

To better illustrate the structure of mixed systems. Consider the example of a wireless downlink network shown in Fig. 1 with three mobile users. We model it as a discrete-time system such that \(\Lambda_{t}(i)\) is the number of newly arrived data packets for mobile \(i\) at the beginning of time slot \(t,\) and \(O_{t}(i)\) is the number of packets that can be transmitted to mobile \(i\) during time slot \(t\) if mobile \(i\) is scheduled.

The base station can transmit to one and only one mobile during each time slot, and let \(A_{t}\in\{1,2,3\}\) denote the mobile scheduled at time slot \(t.\) The length of queue \(i\) evolves as

\[q_{t+1}(i)=\left(q_{t}(i)+\Lambda_{t}(i)-O_{t}(i)\mathbb{I}(A_{t}=i)\right)^{ +},\] (6)

where \(\mathbb{I}(\cdot)\) is the indicator function and \((\cdot)^{+}=\max\{\cdot,0\}.\)

To minimize the total queue lengths, the problem can be formulated as an RL problem such that the state is \((\mathbf{\Lambda}_{t},\mathbf{O}_{t},\mathbf{q}_{t}),\) the action is \(A_{t},\) and the cost is \(\sum_{i}q_{t}(i).\)

We can see that in this problem, \(S_{t}:=(\mathbf{\Lambda}_{t},\mathbf{O}_{t})\) is the stochastic state and \(\mathbf{q}_{t}\) is the pseudo-stochastic state. In general, the state space of the stochastic states is bounded, e.g., both \(\mathbf{\Lambda}_{t}\) and \(\mathbf{O}_{t}\) may be Bernoulli random variables, but the pseudo-stochastic state such as \(\mathbf{q}_{t}\) is large or even unbounded. Therefore, while the distributions of the stochastic states can be learned with a limited number of samples, learning the distribution of the queue lengths, even under a given policy, will require orders of magnitude more samples. Therefore, vanilla versions of model-free approaches that do not distinguish between stochastic and pseudo-stochastic states require an unnecessary amount of data samples and are not efficient.

Therefore, we propose a sample-efficient, data-driven approach based on deep RL and augmented samples. Augmented samples guarantee enough coverage based on a limited number of real data samples. Note that given \(\mathbf{\Lambda}_{t},\)\(\mathbf{O}_{t}\) and \(A_{t},\) we can generate one-slot queue evolution starting from any queue length.

## 3 Sample Efficient Algorithms for Mixed Systems

### Augmented Sample Generator (ASG)

We first introduce the Augmented Sample Generator (ASG, Algorithm 1), which augments a dataset by generating virtual samples. In particular, ASG takes as input a dataset \(D\), an integer \(m,\) and a probability distribution \(\beta(\cdot)\) over the pseudo-stochastic states in \(\mathcal{X}\). For each sample \((s,x,a,r,s^{\prime},x^{\prime})\in D,\) we first sample a pseudo-stochastic state \(\hat{x}\) from \(\beta(\cdot)\), and then construct a virtual sample \((s,\hat{x},a,\hat{r},s^{\prime},\hat{x}^{\prime})\), where \(\hat{r}=R(s,\hat{x},a)\) and \(\hat{x}^{\prime}=g(s,\hat{x},a,s^{\prime})\). Note that we are able to do this since we assume that functions \(R\) and \(g\) are given in our applications. For each sample in \(D\), we repeat this procedure \(m\) times independently to generate \(m\) virtual samples. Taking the downlink wireless network as an example, where \(S_{t}:=(\mathbf{\tilde{\Lambda}_{t}},\mathbf{O_{t}}),X_{t}:=\mathbf{q_{t}},\) assume that at some timeslot \(t,\) one of the real samples is

\[(s,x,a,r,s^{\prime},x^{\prime}):=((\{(3,4,5),\{1,2,0\}),\{4,6,6\},1,16,(\{(2,3,4\},\{2,2,0\})\},\{6,10,11\})\,,\]

Figure 1: A Downlink Wireless Network

where the queue length \(\mathbf{q_{t+1}}\) is calculated according to Eq. (6). Using ASG, we generate two pseudo-stochastic states (queue length) \(\hat{\mathbf{x}}_{1}=(1,2,3),\hat{\mathbf{x}}_{2}=(0,2,1)\), then we are able to obtain two virtual samples

\[(s,\hat{x}_{1},a,\hat{r}_{1},s^{\prime},\hat{x}_{1}^{\prime}):=(( \{3,4,5\},\{1,2,0\}),\{1,2,3\},1,6,(\{2,3,4\},\{2,2,0\}),\{3,6,8\})\,,\] \[(s,\hat{x}_{2},a,\hat{r}_{2},s^{\prime},\hat{x}_{2}^{\prime}):=(( \{3,4,5\},\{1,2,0\}),\{0,2,1\},1,3,(\{2,3,4\},\{2,2,0\}),\{2,6,6\})\,.\]

Note that all the virtual samples represent true transitions of the mixed system if the queue lengths were \(\mathbf{\hat{x}}_{1}\) and \(\mathbf{\hat{x}}_{2}\).

```
1Input: A dataset \(D=\{(s,x,a,r,s^{\prime},x^{\prime})\}\), a positive integer \(m\), a distribution \(\beta(\cdot)\) on \(\mathcal{X}\) ;
2Initialize virtual sample dataset: \(D^{\prime}=\emptyset\) ;
3foreach sample \((s,x,a,r,s^{\prime},x^{\prime})\in D\)do
4 Sample \(m\) virtual pseudo-stochastic states \(\hat{x}_{1},\hat{x}_{2},\ldots,\hat{x}_{m}\) from \(\beta(\cdot)\) independently;
5 Compute \(\hat{r}_{i}=R(s,\hat{x}_{i},a)\) and \(\hat{x}_{i}^{\prime}=g(s,\hat{x}_{i},a,s^{\prime})\) for \(i=1,2,\ldots,m\);
6 Add \(m\) virtual samples to virtual dataset: \(\{(s,\hat{x}_{i},a,\hat{r}_{i},s^{\prime},\hat{x}_{i}^{\prime}),i=1,2,\ldots, m\}\to D^{\prime}\);
7Output:\(D^{\prime}\cup D\) ; ```

**Algorithm 1**Augmented Sample Generator (ASG)

### Batch FQI with ASG

We now formally present our algorithm, Batch FQI with ASG (Algorithm 2), for mixed systems. As in a typical setup for batch reinforcement learning (Chen and Jiang, 2019), we assume that we have a dataset \(D\) with \(n\) samples. The samples are i.i.d. and the distribution of each sample \((s,x,a,r,s^{\prime},x^{\prime})\) is specified by a distribution \(\mu\in\Delta(\mathcal{S}\times\mathcal{X}\times\mathcal{A})\) for \((s,x,a)\), \(r=R(s,x,a)\), \(s^{\prime}\sim P(\cdot|s,a)\), and \(x^{\prime}=g(s,x,a,s^{\prime})\). The distribution \(\mu\) is unknown to the agent.

Our algorithm follows the framework of a batch FQI algorithm. We consider a class of candidate value-functions \(\mathcal{F}\in(\mathcal{S}\times\mathcal{X}\times\mathcal{A}\rightarrow[0,V_ {\max}])\). We focus on the scenario where the number of pseudo-stochastic states, \(|\mathcal{X}|\), is far more than that of stochastic states, \(|\mathcal{S}|\), and the dataset \(D\) does not provide sufficient coverage of the pseudo-stochastic states (\(n<|\mathcal{X}|\)).

The goal is to compute a near-optimal policy from the given dataset \(D\), via finding an \(f\in\mathcal{F}\) that approximates the optimal Q-function \(Q\). The algorithm runs in episodes. For each episode \(k\), it first generates an augmented dataset \(D_{k}\) using ASG\((D,m,\beta_{k})\), where \(\beta_{k}\) is some distribution of the pseudo-stochastic states satisfies that, for each typical pseudo-stochastic state, it can be generated with at least probability \(\sigma_{1}\). We assume that the pseudo-stochastic states will not transition to atypical states, such as extremely large values (possibly infinite) in the queuing example, under a reasonable policy. Details can be found in Assumption A.4 in the supplemental material. We remark that we can also use the same \(\beta\) for all the episodes, and in practice, we can simply adopt the estimation of the pseudo-stochastic states (i.e., with Gaussian distribution) as \(\beta_{k}\). The algorithm then computes \(f_{k}\) as the minimizer that minimizes the squared loss regression objective over \(\mathcal{F}\). The complete algorithm is given in Algorithm 2.

### Guarantee and Analysis

The guarantee of Algorithm 2 is summarized below.

**Theorem 1**.: _We assume the data coverage and completeness assumptions (details in the section A in the supplemental material). Given a dataset \(D=\{s,x,a,r,s^{\prime},x^{\prime}\}\) with \(n\) samples, w.p. at least \(1-\delta\), the output policy of Algorithm 2 after \(k\) iterations, \(\pi_{f_{k}}\), satisfies_

\[v^{\pi_{f_{k}}}-v^{*}\leq\frac{2}{(1-\gamma)^{2}}\Bigg{(} \sqrt{\frac{(m+1)C}{m\sigma_{1}}\left(5\left(\frac{1}{n}+\frac{1}{m} \right)V_{\max}^{2}\log\left(\frac{nK|\mathcal{F}|^{2}}{\delta}\right)+\frac{3 \delta V_{\max}^{2}}{n}\right)}\] \[+\sqrt{c_{0}\sigma_{2}}V_{\max}+\gamma^{k}(1-\gamma)V_{\max} \Bigg{)},\] (7)

_where \(C\) is a constant related to the data coverage assumption A.3, and \(c_{0},\sigma_{1},\sigma_{2}\) are constants defined in Assumption A.4 to ensure a typical set of reasonable typical pseudo-stochastic states._Theorem 1 shows that ASG significantly improves the (real) sample complexity, i.e., the number of real data samples needed. When the tail of pseudo-stochastic states decays fast, e.g. an exponential tails link in a typical queueing network, we can choose a sampling distribution \(\beta\) that guarantees that \(\sigma_{2}=\tilde{\mathcal{O}}(1/n)\) and \(\sigma_{1}=\Omega(1/\log n).\) Therefore, for sufficiently large \(k\) and sufficiently small \(\sigma_{2},\) the first term is the dominating term. As \(m\) increases from 1 to \(n,\) the first term of the bound decreases from \(\tilde{\mathcal{O}}(1)\) to \(\tilde{\mathcal{O}}(\sqrt{1/n}).\) Remark that we cannot establish a convergence result without using ASG (when \(m=0\)) since using the dataset \(D\) alone doesn't have a sufficient data coverage guarantee, which is critical for batch RL. Due to the page limit, we only present the outline behind our analysis. The detailed proof is deferred to the supplemental material.

**Proof Outline:**

1. First we will show that using performance difference lemma (Kakade and Langford, 2002) it is sufficient by bounding \(\|f_{k}-Q^{*}\|_{2,\xi},\) where \(\xi\) is some distribution.
2. Then, by considering how rare the distribution of the pseudo-stochastic states is under \(\xi\) and then classify the pseudo-stochastic states into typical and atypical sets based on a threshold \(\sigma_{2}\) on the distribution. Along with the augmented data generator under \(\beta_{k}(\cdot).\) Then later we show that the term \(\|f_{k}-Q^{*}\|_{2,\xi}\) can be bounded by \(\mathcal{O}\left(\|f_{k}-\mathcal{T}f_{k-1}\|_{2,\mu_{\beta_{k}}}+\gamma\|f_ {k+1}-Q^{*}\|_{2,\xi^{\prime}}+\sqrt{\sigma_{2}}V_{\max}\right),\) where \(\mu_{\beta_{k}}\) is the data distribution after augmenting virtual samples. The first two terms are related to the typical set given that a sufficient coverage of the data after data augmentation. The last term is easy to obtain by considering the atypical set.
3. Therefore, it is obvious that if we can have a bound on \(\|f_{k}-\mathcal{T}f_{k-1}\|_{2,\mu_{\beta_{k}}}\) which is independent of the episode \(k,\) we can prove the final result by expanding \(\|f_{k}-Q^{*}\|_{2,\xi}\) iteratively for \(k\) times. We finally show that the term \(\|f_{k}-\mathcal{T}f_{k-1}\|_{2,\mu_{\beta_{k}}}\) can indeed be bounded by using the FQI minimizer at each episode and a concentration bound on the offline dataset after augmenting virtual samples.

### Extension to the case when \(|\mathcal{X}|\) is infinite

Theorem 1 assumes the number of the pseudo-stochastic states is finite. The result can be generalized to infinite pseudo-stochastic state space \(|\mathcal{X}|\) if we make the following additional assumption:

**Assumption 1**.: _For the typical set \(\mathcal{B}\) of pseudo-stochastic states (formally defined in Assumption A.4 in the supplemental material), for any \(s,a\in\mathcal{S}\times\mathcal{A},f\in\mathcal{F},\) if \(x\in\mathcal{B},\) then \(f(s,x,a)\leq V_{\max}\) otherwise if \(x\notin\mathcal{B},\) we have \(|f(s,x,a)-Q^{*}(s,x,a)|\leq V_{\max}.\) Furthermore, for any given \(f\in\mathcal{F},(s,x),x\in\mathcal{B},\) we have \(|V_{f}(s^{\prime},x^{\prime})-V_{f}(s^{\prime\prime},x^{\prime\prime})|\leq V _{\max},\) where \(x^{\prime}=g(s,x,\pi_{f},s^{\prime}),x^{\prime\prime}=g(s,x,\pi_{f},s^{\prime \prime}).\)_

The first part in assumption 1 means that the function \(f\in\mathcal{F}\) is always bounded when \(x\in\mathcal{B}.\) Otherwise, the difference between \(f\) and the optimal \(Q-\)function \(Q^{*}\) is bounded, which indicates that although \(f\) could be extremely large or even unbounded for the pseudo-stochastic states, including the infinite case, in the atypical set, it is also true for the optimal \(Q-\)function \(Q^{*}\). Therefore the difference between \(f\) and \(Q^{*}\) is assumed to be bounded, which is reasonable in practice. The second part states that for any given \(x\in\mathcal{B},\) the difference between the value functions of any possible next transitions \((s^{\prime},x^{\prime\prime})\) and \((s^{\prime\prime},x^{\prime\prime})\) is always bounded, which is also true in general especially in queuing literature since the changing of queue lengths between two timeslots is small.

Assumption 1 is easy to be satisfied in a typical queueing system (e.g. the wireless downlink network in Section 2.1). If the control/scheduling policy is a stabilizing policy, then the queues have exponential tails, i.e., the probability decays exponentially as queue length increases, and the policy can drive very large queues to bounded values exponentially fast. Therefore, if all functions in \(F\) are from stabilizing policies, Assumption 1 holds because any policy in \(F\) including the optimal policy can reduce the queue lengths exponentially fast (in expectation) and the difference in \(Q\)-functions therefore is bounded.

Under the additional Assumption 1, we show that the same order of the result is achievable. Details can be found in Section A.2 in the supplemental material.

## 4 Experiments

To evaluate the performance of our approach, we compared our results with several baselines in extensive queuing environments. In our simulations, when augmenting virtual samples, we only require that the augmented samples are valid samples for which the action from the real sample remains to be feasible.

### The Criss-Cross Network

The criss-cross network is shown in Fig. 1(a), which consists of two servers and three classes of jobs. Each job class is queued in a separate buffer if not served immediately. Each server can only serve one job at a time. Server \(1\) can process jobs of both class \(1\) and class \(3;\) server \(2\) can only process jobs of class \(2.\) Class \(3\) jobs become class \(2\) jobs after being processed, and class \(2\) and class \(3\) jobs leave the system after being processed. The service time of a class \(i\) job is exponentially distributed with mean \(m_{i}.\) The service rate of a class \(i\) job is defined to be \(\mu_{i}:=1/m_{i},\) and jobs of class \(1\) and class \(3\) arrive to the system following the Poisson processes with rates \(\lambda_{1}\) and \(\lambda_{3}.\) To make sure the load can be supportable (i.e. queues can be stabilized), we assume \(\lambda_{1}m_{1}+\lambda_{3}m_{3}<1\) and \(\lambda_{1}m_{2}<1.\) In the simulation, we consider the imbalanced medium traffic regime, where \(\mu_{1}=\mu_{3}=2,\mu_{2}=1.5,\lambda_{1}=\lambda_{3}=0.6.\)

All queues are first-in-first-out (FIFO). Let \(e_{t}(i)\in\{-1,0,+1\}\) denote the state of class \(i\) jobs at time \(t,\) where \(-1\) means a departure, \(+1\) means a arrival and \(0\) denotes no changes for job \(i.\) Then \(e_{t}=(e_{t}(1),e_{t}(2),e_{t}(3))\) be the event happens at time \(t\) in the system, which is the stochastic state in this mixed system. Let \(q_{t}=(q_{t}(1),q_{t}(2),q_{t}(3))\) be the vector of queue lengths at time \(t,\) where \(q_{t}(i)\) is the number of class \(i\) jobs in the system. Obviously, \(q_{t}\) is the pseudo-stochastic state in the system, which can be derived from \((q_{t-1},e_{t-1})\) and action \(a_{t}.\) We combine ASG with \(Q-\)learning and compare it with several baselines: (i) the vanilla Q-learning, (ii) a proximal policy optimization (PPO) based algorithm proposed in Dai and Gluzman (2022) designed for queuing networks, (iii) a random policy, and (iv) a priority policy such that the server always serves class \(1\) jobs when its queue length is not empty, otherwise it serves class \(3\) jobs. Simulation results in Fig. 1(d) demonstrate that using ASG significantly improves the performance, and achieves the performance of the optimal policy, which was obtained by solving a large-scale MDP problem, after only \(4\) training episode (\(4k\) training steps). All other policies are far away from the optimal policy, and barely learn anything, as we mentioned that the state space is quite large and is not even sufficiently explored after \(4k\) training steps. To further demonstrate the fundamental difference between ASG and Dyna-type algorithms, we also compared ASQ with the model-based approach: Q-learning with Dyna. We can observe that Dyna also fails to learn a good policy since, as we mentioned before, ASG is built for improving sample efficiency with augmented samples that represent _true_ transitions for possible all the pseudo-stochastic states, but dyna-type algorithms can only generate addition samples from an estimated model, which are limited by the samples that have been seen before.

### Two-Phase Criss-Cross Network:

To further evaluate the power of ASG beyond the tabular setting, We combine ASG with Deep Q-network (Mnih et al., 2015) named DQN-ASG and evaluate it on a more complicated variant of the Criss-Cross network presented in Fig. 2b, where class \(3\) jobs have two phases. Class \(3\) jobs at phase \(1\) become phase \(2\) after being processed with probability \(1-p,\) leave the system with probability \(p;\) and class \(3\) jobs at phase \(2\) leave the system after completed processing. Results in Fig. 2 indicate that combining ASG with DQN also has a significant improvement on the performance, and our method only needs \(100\) episodes to achieve a near-optimal policy. We also remark here that the training time for _DQN-ASG_ is three times faster than the baseline (\(4\) hours v.s. \(12\) hours). In the simulation, all the parameters are set to be the same as those in the previous case, and we use \(p=0.8\). Both the criss-cross and two-phase criss-cross networks are continuous-time Markov chains (CTMCs). We use the standard uniformization (Puterman, 2014; Serfozo, 1979) approach to simulate them using discrete-time Markov chains (DTMCs).

We remark here that the two-phase Criss-Cross network cannot be modeled as MDPs with exogenous inputs. Note that the phase a job is in is a stochastic state, which depends on the action (whether a job is selected for service). Since the scheduling action depends on the queue lengths, the evolution of stochastic states depends on the pseudo-stochastic states (the queue lengths). However, the exogenous input in an MDP with exogenous inputs has to be an observable random process independent of the action. Therefore, the current phase of a job of the phases Criss-Cross network is not an exogenous input and the system is not an MDP with exogenous inputs.

### Wireless Networks

In addition to the criss-cross networks, we also evaluate our approach on the downlink network described in Section 2.1 with three mobiles. We let the arrivals are Poisson with rates \(\mathbf{\Lambda}=\{2,4,3\},\) and the channel rates \(\mathbf{O}\) to be \(\{12,12,12\}.\) We use the Max-Weight (Tassiulas and Ephremides, 1992) algorithm, a throughput-optimal and heavy-traffic delay optimal algorithm, as the baseline. As shown in Fig. 2f, the learned policy outperforms Max-Weight. To the best of our knowledge, no efficient RL algorithms exist that can outperform Max-Weight.

### Additional Simulations on the criss-cross network

To further verify the performance of our algorithm on more scenarios, we first consider a more general criss-cross network where the size of both class \(1\) and class \(3\) jobs can vary over time. Then all class \(1\) and class \(3\) jobs need multiple timeslots to be processed instead of \(1\). In particular, when a new job of either class \(1\) or \(2\) joins the system, the job size of such job is chosen uniformly. The environment in Fig. 2b can be seen as a special case where class \(1,\) and \(3\) have a fixed job size \(1\) and \(2,\) respectively. The performances of different cases are presented in Fig. 3. Our approach still obtains the best result.

Figure 2: Performance on queuing systems

The details of the parameters can be found in section B.2. We do not include the optimal solution since, for the general case, the state space is quite large, and using value iteration to obtain the optimal solution is very time-consuming. The priority policy already provides a reasonable good baseline.

### Additional Simulations on the Wireless Network

We also evaluate the performance of our algorithm on the wireless network (Fig. 1) under different sets of arrival rates. The results of average queue length compared with those of Max-Weight are summarized in Table 2. We can observe that ASG performs better than Max-Weight in all the cases. Simulation results on a more complicated two-phase wireless network can be found in the supplemental material (Section B.3).

### Combining ASG with Policy Gradient-type Algorithm

We also investigate the possibilities of using ASG in policy gradient-type algorithms (i.e., TD3 (Fu et al., 2018), SAC (Haarnoja et al., 2018)) and compare our approach with one of the state-of-art algorithms MBPO (Janner et al., 2019). Our approach still achieves the best performance. Due to page limit, the detailed simulation and algorithm are deferred to Section B in the supplemental material.

## 5 Conclusions

In this work, we considered RL problems for mixed systems which have two types of states: stochastic states and pseudo-stochastic states. We proposed a sample efficient RL approach that accelerates learning by augmenting virtual data samples. We theoretically quantified the (real) sample complexity gain by reducing the optimality gap from \(\tilde{\mathcal{O}}(1)\) to \(\tilde{\mathcal{O}}(1/\sqrt{n})\). Experimental studies further confirmed the effectiveness of our approach on several queuing systems. The possible limitation of using ASG is that it introduces more computation during each learning iteration when generating virtual samples, though it is not obvious in our experiments.

## References

* Agarwal et al. (2020) Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. (2020). Flambe: Structural complexity and representation learning of low rank mdps. In _Advances Neural Information Processing Systems (NeurIPS)_, page 20095-20107. Curran Associates Inc.
* Chen and Jiang (2019) Chen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning. In _Int. Conf. Machine Learning (ICML)_, pages 1042-1051. PMLR.
* Chen et al. (2019)

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Arrival Rates & Service Rates & Max-Weight & DQN-ASG \\ \hline \(\{2,3,4\}\) & \(\{12,12,12\}\) & \(10.979\) & \(\mathbf{10.403}\) \\ \hline \(\{1,7,2\}\) & \(\{12,12,12\}\) & \(13.811\) & \(\mathbf{13.273}\) \\ \hline \(\{2,2,6\}\) & \(\{12,12,12\}\) & \(13.774\) & \(\mathbf{13.137}\) \\ \hline \(\{3,1,5\}\) & \(\{12,12,12\}\) & \(10.895\) & \(\mathbf{10.091}\) \\ \hline \end{tabular}
\end{table}
Table 2: Best testing average queue length after training for \(120k\) steps

Figure 3: Performance on Criss-Cross network with general job size

Clavera, I., Rothfuss, J., Schulman, J., Fujita, Y., Asfour, T., and Abbeel, P. (2018). Model-based reinforcement learning via meta-policy optimization. _CoRR_, abs/1809.05214.
* Cohen and Welling (2016a) Cohen, T. and Welling, M. (2016a). Group equivariant convolutional networks. In _Int. Conf. Machine Learning (ICML)_, pages 2990-2999. PMLR.
* Cohen and Welling (2016b) Cohen, T. S. and Welling, M. (2016b). Steerable cnns. _arXiv preprint arXiv:1612.08498_.
* Dai and Gluzman (2022) Dai, J. G. and Gluzman, M. (2022). Queueing network controls via deep reinforcement learning. _Stochastic Systems_, 12(1):30-67.
* Du et al. (2019) Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J. (2019). Provably efficient rl with rich observations via latent state decoding. In _Int. Conf. Machine Learning (ICML)_, pages 1665-1674. PMLR.
* Ernst et al. (2005) Ernst, D., Geurts, P., and Wehenkel, L. (2005). Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6:503-556.
* Fan et al. (2021) Fan, L., Wang, G., Huang, D.-A., Yu, Z., Fei-Fei, L., Zhu, Y., and Anandkumar, A. (2021). SECANT: self-expert cloning for zero-shot generalization of visual policies. In _Int. Conf. Machine Learning (ICML)_.
* Fu et al. (2018) Fu, L., Fu, X., Zhang, Z., Xu, Z., Wu, X., Wang, X., and Lu, S. (2018). Joint optimization of multicast energy in delay-constrained mobile wireless networks. _IEEE/ACM Transactions on Networking_, 26(1):633-646.
* Haarnoja et al. (2018) Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. (2018). Soft Actor-Critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _Int. Conf. Machine Learning (ICML)_, pages 1861-1870.
* Janner et al. (2019) Janner, M., Fu, J., Zhang, M., and Levine, S. (2019). When to trust your model: Model-based policy optimization. In _Advances Neural Information Processing Systems (NeurIPS)_, volume 32.
* Kakade and Langford (2002) Kakade, S. and Langford, J. (2002). Approximately optimal approximate reinforcement learning. In _Int. Conf. Machine Learning (ICML)_, pages 267-274.
* Kearns and Koller (1999) Kearns, M. and Koller, D. (1999). Efficient reinforcement learning in factored mdps. In _Proc. Int. Joint Conf. Artif. Intell. Org (IJCAI)_, volume 16, pages 740-747.
* Kurutach et al. (2018) Kurutach, T., Clavera, I., Duan, Y., Tamar, A., and Abbeel, P. (2018). Model-ensemble trust-region policy optimization. _arXiv preprint arXiv:1802.10592_.
* Laskin et al. (2020) Laskin, M., Lee, K., Stooke, A., Pinto, L., Abbeel, P., and Srinivas, A. (2020). Reinforcement learning with augmented data. In _Advances Neural Information Processing Systems (NeurIPS)_. Curran Associates Inc.
* Lin et al. (2020) Lin, Y., Huang, J., Zimmer, M., Guan, Y., Rojas, J., and Weng, P. (2020). Invariant transform experience replay: Data augmentation for deep reinforcement learning. _IEEE Robotics and Automation Letters_, 5(4):6615-6622.
* Luo et al. (2019) Luo, Y., Xu, H., Li, Y., Tian, Y., Darrell, T., and Ma, T. (2019). Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees. In _Int. Conf. on Learning Representations (ICLR)_.
* Machado et al. (2020) Machado, M. C., Bellemare, M. G., and Bowling, M. (2020). Count-based exploration with the successor representation. In _AAAI Conf. Artificial Intelligence_, pages 5125-5133.
* Misra et al. (2020) Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J. (2020). Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In _Int. Conf. Machine Learning (ICML)_, pages 6961-6971. PMLR.
* Mnih et al. (2015a) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., and Ostrovski, G. (2015a). Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533.
* Mnih et al. (2015b)Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., et al. (2015b). Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533.
* Modi et al. (2021) Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2021). Model-free representation learning and exploration in low-rank mdps. _arXiv preprint arXiv:2102.07035_.
* Mondal et al. (2020) Mondal, A. K., Nair, P., and Siddiqi, K. (2020). Group equivariant deep reinforcement learning. _ICML 2020 Worshop on Inductive Biases, Invariances and Generalization in RL_.
* Ota et al. (2020) Ota, K., Oiki, T., Jha, D., Mariyama, T., and Nikovski, D. (2020). Can increasing input dimensionality improve deep reinforcement learning? In _Int. Conf. Machine Learning (ICML)_, pages 7424-7433. PMLR.
* Pathak et al. (2017) Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. In _Int. Conf. Machine Learning (ICML)_, pages 2778-2787. PMLR.
* Puterman (2014) Puterman, M. L. (2014). _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons.
* Sekar et al. (2020) Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., and Pathak, D. (2020). Planning to explore via self-supervised world models. In _Int. Conf. Machine Learning (ICML)_, pages 8583-8592. PMLR.
* Serfozo (1979) Serfozo, R. F. (1979). An equivalence between continuous and discrete time markov decision processes. _Operations Research_, 27(3):616-620.
* Silver et al. (2017) Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A., et al. (2017). Mastering the game of go without human knowledge. _nature_, 550(7676):354-359.
* Sinclair et al. (2022) Sinclair, S. R., Frujeri, F., Cheng, C.-A., and Swaminathan, A. (2022). Hindsight learning for mdps with exogenous inputs. _arXiv preprint arXiv:2207.06272_.
* Sutton (1988) Sutton, R. S. (1988). Learning to predict by the methods of temporal differences. _Machine learning_, 3(1):9-44.
* Sutton et al. (2012) Sutton, R. S., Szepesvari, C., Geramifard, A., and Bowling, M. P. (2012). Dyna-style planning with linear function approximation and prioritized sweeping. _arXiv preprint arXiv:1206.3285_.
* Tassiulas and Ephremides (1992) Tassiulas, L. and Ephremides, A. (1992). Stability properties of constrained queueing systems and scheduling policies for maximum throughput in multihop radio networks. _IEEE Trans. Autom. Control_, 37:1936-1948.
* van der Pol et al. (2020) van der Pol, E., Worrall, D., van Hoof, H., Oliehoek, F., and Welling, M. (2020). MDP homomorphic networks: Group symmetries in reinforcement learning. In _Advances Neural Information Processing Systems (NeurIPS)_, volume 33, pages 4199-4210.
* Vinyals et al. (2019) Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354.
* Wang et al. (2022) Wang, D., Walters, R., and Platt, R. (2022). \(\mathrm{SO}(2)\)-equivariant reinforcement learning. In _iclr_.
* Weiler and Cesa (2019) Weiler, M. and Cesa, G. (2019). General e(2)-equivariant steerable cnns. In _Advances Neural Information Processing Systems (NeurIPS)_, volume 32, page 14357-14368. Curran Associates, Inc.
* Yao et al. (2009) Yao, H., Bhatnagar, S., and Diao, D. (2009). Multi-step linear dyna-style planning. In _Advances Neural Information Processing Systems (NeurIPS)_, NIPS'09, page 2187-2195, Red Hook, NY, USA. Curran Associates Inc.
* Yu et al. (2020) Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J. Y., Levine, S., Finn, C., and Ma, T. (2020). Mopo: Model-based offline policy optimization. _Advances Neural Information Processing Systems (NeurIPS)_, 33:14129-14142.

**Supplementary Material**

## Appendix A Proof of Theorem 1

In this section, we present the proof of Theorem 1. We first introduce and recall some necessary notations and assumptions. Then, we present some auxiliary lemmas and their proofs. Finally, we combine the lemmas to prove the main result.

**Notations:** Define \(P(\nu)\) as distribution over states such that \((s^{\prime},x^{\prime})\sim P(\nu)\Leftrightarrow(s,x,a)\sim\nu,s^{\prime}\sim P (s^{\prime}|s,a),x^{\prime}=g(s,a,s^{\prime}).\) In other words, it is the distribution of the next state if the state action pair follows \(\nu.\) For \(f:\mathcal{S}\times\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R},\nu\in \Delta(\mathcal{S}\times\mathcal{X}\times\mathcal{A}),\) where \(\Delta(\cdot)\) is the probability simplex, and \(p>1,\) define \(\|f\|_{p,\nu}=\left(\mathbb{E}_{(s,x,a)\sim\nu}[|f(s,x,a)|^{p}]\right)^{1/p}.\) Define \(\pi_{f,f^{\prime}}(s,x):=\arg\min_{a\in\mathcal{A}}\min\{f(s,x,a),f^{\prime}( s,x,a)\}.\)

Recall that in Alg. 2, at iteration \(k,\)

\[\mathcal{L}_{\hat{D}_{k}}(f;f_{k-1})= \frac{1}{|\hat{D}_{k}|}\sum_{(s,x,a,r,s^{\prime},x^{\prime})\in \hat{D}_{k}}(f(s,x,a)-r-\gamma V_{f_{k-1}}(s^{\prime},x^{\prime}))^{2},\]

where

\[f_{k}:\,=\arg\min_{f\in\mathcal{F}}\mathcal{L}_{\hat{D}_{k}}(f;f_{k-1})\quad \text{and}\quad V_{f_{k}}(s,x):=\min_{a\in\mathcal{A}}f_{k}(s,x,a).\]

**Assumptions:**

**Assumption A.1**.: _(Realizability) For the optimal policy, \(Q^{*}\in\mathcal{F}.\)_

**Assumption A.2**.: _(Completeness) For the policy \(\pi\) to be evaluated, \(\forall f\in\mathcal{F},\mathcal{T}f\in\mathcal{F},\) where \(\mathcal{T}:\mathbb{R}^{\mathcal{S}\times\mathcal{X}\times\mathcal{A}} \rightarrow\mathbb{R}^{\mathcal{S}\times\mathcal{X}\times\mathcal{A}}\) is the Bellman update operator, \(\forall f:\)_

\[(\mathcal{T}f)(s,x,a):=R(s,x,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)} [V_{f}(s^{\prime},x^{\prime}=g(s,x,a,s^{\prime}))].\]

We say a distribution \(\nu\in\Delta(\mathcal{S}\times\mathcal{A})\) is admissible in MDP \((\mathcal{S},\mathcal{X},\mathcal{A},P,R,\gamma),\) if there exist \(h\geq 0\) and a policy \(\pi\) such that \(\nu(s,a)=\sum_{x\in\mathcal{X}}\Pr(s_{h}=s,x_{h}=x,a_{h}=a|s_{0},x_{0},\pi).\) The following assumption is imposed to limit the distribution shift. Note that "admissible" is defined on the stochastic state and action. Later, we will also abuse the notation and call \(\nu\in\Delta(\mathcal{S}\times\mathcal{X}\times\mathcal{A})\) is admissible if \(\nu(s,a)=\sum_{x}\nu(s,x,a)\) is admissible.

**Assumption A.3**.: _For a data distribution \(\mu,\) we assume that there exists \(C<\infty\) such that for any admissible \(v\) and any \((s,a)\in\mathcal{S}\times\mathcal{A},\)_

\[\frac{\nu(s,a)}{\mu(s,a)}\leq C,\]

_where \(\nu(s,a)=\sum_{x\in\mathcal{X}}\nu(s,x,a)\) and \(\mu(s,a)=\sum_{x\in\mathcal{X}}\mu(s,x,a).\)_

Assumptions A.2-A.3 are standard assumptions in batch reinforcement learning (Chen and Jiang, 2019). However, in assumption A.3, we only require the data coverage of stochastic states which is the fundamental difference.

**Assumption A.4**.: _We assume that there exists a set \(\mathcal{B}\) of typical pseudo-stochastic states such that the distributions \(\beta_{k}(x),k=1,\ldots,K\) used for augmenting virtual samples satisfy \(\beta_{k}(x)\geq\sigma_{1},\forall x\in\mathcal{B}.\) We also assume that the marginal distribution over using a reasonable policy \(\pi,\) that is, \(d_{\eta_{0}}^{\pi}(s,x):=(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t-1}\Pr(s_{t}=s,x _{t}=x|(s_{0},x_{0})\sim\eta_{0},\pi)\) satisfies \(\sum_{s\in\mathcal{S}}\sum_{x\in\mathcal{X},x\not\in\mathcal{B}}d_{\eta_{0}}^{ \pi}(s,x)\leq\sigma_{2},\) where \(\eta_{0}\) is the initial distribution. Furthermore, if we have for a distribution \(\eta\) of the states satisfying \(\sum_{s\in\mathcal{S}}\sum_{x\in\mathcal{X},x\not\in\mathcal{B}}\eta(s,x)\leq \sigma_{2},\) then under any reasonable policy \(\pi,\) the marginal distribution \(\eta_{h}^{\pi}(s,x):=\Pr(s_{h}=s,x_{h}=x|(s_{0},x_{0})\sim\eta,\pi)\) satisfies \(\sum_{s\in\mathcal{S}}\sum_{x\in\mathcal{X},x\not\in\mathcal{B}}\eta_{h}^{\pi}(s,x)\leq c_{0}\sigma_{2},\forall h>0,\) for some constant \(c_{0}\geq 1.\) In particular, all the policies at each iteration, the optimal policy and their joint policies are assumed to be reasonable policies._

We remark that in a queuing network, under any stable policy, the queue distribution has an exponential tail; in other words, large queue lengths occur with a small probability. In such a case, we can use a uniform distribution for pseudo-stochastic states in set \(\mathcal{B}\) to guarantee that \(\sigma_{1}=\Theta\left(\frac{1}{\log(1/\sigma_{2})}\right).\) Therefore, if we choose \(\sigma_{2}=\frac{1}{n},\) then \(\sigma_{1}=\frac{1}{\log n}.\)

### Auxiliary Lemmas

In the following lemma, we will show that when all admissible distributions are not far away from the data distribution \(\mu\) over stochastic state \(\mathcal{S}\) and action \(\mathcal{A}\), we can have a good coverage of \(\mathcal{S}\times\mathcal{X}\times\mathcal{A}\) by generating virtual samples.

For a given dataset \(D\) of size \(|D|=n\) and data distribution \(\mu,\) let \(\bar{\mu}_{\beta}\) denote the expected distribution of the the state action pair \((s,x,a)\) in the combined dataset after using Algorithm 2 with a virtual sample distribution \(\beta(x).\)

**Lemma A.1**.: _Given a virtual sample generating distribution \(\beta(x)\) of the pseudo-stochastic state, if \(\beta(x)\geq\sigma_{1},\forall x\in\mathcal{B}.\) Then given any admissible distribution \(\nu,\) then under Assumptions A.3, we have for any \((s,x,a)\in\mathcal{S}\times\mathcal{X}\times\mathcal{A},x\in\mathcal{B},\)_

\[\frac{\nu(s,x,a)}{\bar{\mu}_{\beta}(s,x,a)}\leq\frac{(m+1)C}{m\sigma_{1}},\]

_where_

\[\bar{\mu}_{\beta}(s,x,a)= \mu(s,x,a)\frac{n}{nm+n}+\sum_{\hat{x}\in\mathcal{X}}\mu(s,\hat{x },a)\left(\beta(x)\frac{nm}{nm+n}\right)\] (8)

Proof.: Given the data distribution \(\mu,\) we know that the real samples are drawn according to \(\mu(s,x,a).\) Then

\[\frac{\nu(s,x,a)}{\bar{\mu}_{\beta}(s,x,a)}\leq \frac{\sum_{\hat{x}\in\mathcal{X}}\nu(s,\hat{x},a)}{\bar{\mu}_{ \beta}(s,x,a)}=\frac{\sum_{\hat{x}\in\mathcal{X}}\nu(s,\hat{x},a)}{\sum_{\hat{ x}\in\mathcal{X}}\mu(s,\hat{x},a)}\times\frac{\sum_{\hat{x}\in\mathcal{X}}\mu(s, \hat{x},a)}{\bar{\mu}_{\beta}(s,x,a)}\] \[= \frac{\nu(s,a)}{\mu(s,a)}\times\frac{\sum_{\hat{x}\in\mathcal{X} }\mu(s,\hat{x},a)}{\bar{\mu}_{\beta}(s,x,a)}\] \[\leq_{(1)}C\times\frac{\mu(s,a)}{\bar{\mu}_{\beta}(s,x,a)}\] \[= {}_{(2)}C\left(\frac{\mu(s,a)}{\frac{\mu(s,x,a)}{m+1}+\frac{m}{m+1 }\cdot\sum_{\hat{x}\in\mathcal{X}}\mu(s,\hat{x},a)\beta(x)}\right)\] \[\leq C\left(\frac{(m+1)\mu(s,a)}{m\sum_{\hat{x}\in\mathcal{X}}\mu(s, \hat{x},a)\beta(x)}\right)\] \[\leq C\cdot\frac{m+1}{m}\cdot\frac{1}{\sigma_{1}},\]

where the inequality \((1)\) holds because of Assumption A.3, the equality \((2)\) holds by substituting equation (8) and the last inequality is true because the fact that \(\beta(x)\geq\sigma_{1},\forall x\in\mathcal{B}.\) 

The next lemma transforms the norm in terms of distribution \(\nu\) to distribution \(\bar{\mu}_{\beta}\) (Eq. (8)).

**Lemma A.2**.: _Let \(\nu\) be any admissible distribution, \(\bar{\mu}_{\beta}\) denote the new data distribution defined in Eq. (8) after generating virtual samples with \(\beta(x).\) If \(\beta(x)\geq\sigma_{1},\forall x\in\mathcal{B},\) then under Assumption A.3, for any function \(f:\mathcal{S}\times\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R},\) we have \(\|f\|_{2,\nu}\leq\sqrt{\frac{m+1}{m}\frac{C}{\sigma_{1}}}\|f\|_{2,\bar{\mu}_{ \beta}},\) where_

\[\|f\|_{2,\nu}=\left(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times \mathcal{A},x\in\mathcal{B}}|f(s,x,a)|^{2}\nu(s,x,a)\right)^{1/2}.\]

Proof.: For any function \(f,\) we have

\[\|f\|_{2,\nu}=\left(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times \mathcal{A},x\in\mathcal{B}}|f(s,x,a)|^{2}\nu(s,x,a)\right)^{1/2}\]\[\|\mathcal{T}f^{\prime}-Q^{*}\|_{2,\nu}^{2}=\|\mathcal{T}^{*}f^{ \prime}-\mathcal{T}Q^{*}\|_{2,\nu}^{2}=\mathbb{E}_{(s,x,a)\sim\nu}\left[\left(( \mathcal{T}f^{\prime})(s,x,a)-(\mathcal{T}Q^{*})(s,x,a)\right)^{2}\right]\] \[= \mathbb{E}_{(s,x,a)\sim\nu}\left[\left(\gamma\mathbb{E}_{s^{\prime }\sim P\cdot|s,a}\right)\left[V_{f^{\prime}}(s^{\prime},g(s,x,a,s^{\prime}))-V ^{*}(s^{\prime},g(s,x,a,s^{\prime}))\right)\right]^{2}\right]\] \[\leq \gamma^{2}\mathbb{E}_{(s,x,a)\sim\nu,s^{\prime}\sim P\cdot(\cdot |s,a)}\left[\left(V_{f^{\prime}}(s^{\prime},g(s,x,a,s^{\prime}))-V^{*}(s^{ \prime},g(s,x,a,s^{\prime}))\right)^{2}\right]\] \[= \gamma^{2}\mathbb{E}_{s^{\prime}\sim P(\nu)}\left[\left(V_{f^{ \prime}}(s^{\prime},g(s,x,a,s^{\prime}))-V^{*}(s^{\prime},g(s,x,a,s^{\prime}) )\right)^{2}\right]\] \[= \gamma^{2}\|V_{f^{\prime}}-V^{*}\|_{2,P(\nu)}^{2},\]

and the last inequality holds due to Lemma A.3.

**Lemma A.5**.: _For a given data sample \((s,x,a,r,s^{\prime},a^{\prime})\) generated from a data distribution \(\mu,\) such that \((s,x,a)\sim\mu,s^{\prime}\sim P(\cdot|s,a),x^{\prime}=g(s,x,a,s^{\prime}),\) for any \(f,f^{\prime}\in\mathcal{F},\) define \(V_{f}(s,x)=\min_{a^{\prime}}f(s,x,a^{\prime}),\) then_

\[\mathbb{E}\left[\left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime },x^{\prime})\right)^{2}\right]\] \[= \|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+\gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{\prime})|s,x,a)]\] (11)

Proof.: \[\mathbb{E}\left[\left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime },x^{\prime})\right)^{2}\right]\] \[= \mathbb{E}\left[\left(f(s,x,a)-\left(\mathcal{T}f^{\prime}\right) (s,x,a)+\left(\mathcal{T}f^{\prime}\right)(s,x,a)-\left(r+\gamma V_{f^{\prime }}(s^{\prime},x^{\prime})\right)\right)^{2}\right]\] \[= \mathbb{E}\left[\left(f(s,x,a)-\left(\mathcal{T}f^{\prime}\right) (s,x,a)\right)\left(\left(\mathcal{T}f^{\prime}\right)(s,x,a)-\left(r+\gamma V _{f^{\prime}}(s^{\prime},x^{\prime})\right)\right)\right]\] \[= \mathbb{E}\left[\left(f(s,x,a)-\left(\mathcal{T}f^{\prime}\right) (s,x,a)\right)^{2}\right]+\gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{ f^{\prime}}(s^{\prime},x^{\prime})|s,x,a)]\] \[= \|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+\gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{\prime})|s,x,a)],\]

where the equation \((1)=0\) because that condition on \((s,x,a),\) we have \(f\) and \(V_{f^{\prime}}\) are independent. 

**Lemma A.6**.: _Under Algorithm 2, at iteration \(k,\) we have_

\[\mathcal{L}_{\hat{\mu}_{\hat{\mu}_{k}}}(f;f^{\prime})-\mathcal{L}_{\hat{\mu}_ {\hat{\mu}_{\hat{\mu}_{k}}}}(\mathcal{T}f;f^{\prime})=\|f-\mathcal{T}f^{\prime }\|_{2,\hat{\mu}_{\hat{\mu}_{\hat{\mu}_{k}}}}^{2},\] (12)

_where \(\mathcal{L}_{\hat{\mu}_{\hat{\mu}_{k}}}(f;f^{\prime})=\mathbb{E}[\mathcal{L}_ {\hat{D}_{k}}(f;f^{\prime})]\)._

Proof.: Recall that \(\hat{D}_{k}=D\cup D_{k}\) and \(|\hat{D}_{k}|=nm+n\). The expectation is w.r.t. the random draw of the dataset \(D\) and the random generation of dataset \(D_{k}\) with virtual sample distribution \(\beta_{k}\). We know that

\[\mathcal{L}_{\hat{D}_{k}}(f;f^{\prime})= \frac{1}{|\hat{D}_{k}|}\sum_{(s,x,a,r,s^{\prime},x^{\prime})\in D} \left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime},x^{\prime})\right)^{2}\] \[+\frac{1}{|\hat{D}_{k}|}\sum_{(s,x,a,r,s^{\prime},x^{\prime})\in D _{k}}\left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime},x^{\prime})\right)^{2}\]

Let \(\mathcal{M}_{k}^{(s,x,a,s^{\prime},x^{\prime})}\) denote the set of virtual samples that are associated with the real sample \((s,x,a,s^{\prime},x^{\prime})\) at iteration \(k\). Then

\[\mathcal{L}_{\hat{\mu}_{\hat{\mu}_{\hat{\mu}_{\hat{\mu}_{k}}}}}(f; f^{\prime}):=\mathbb{E}[\mathcal{L}_{\hat{D}_{k}}(f;f^{\prime})]\] \[= \frac{n}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+ \gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{ \prime})|s,x,a)]\right)\] (by using Lemma \[A.5\] ) \[+\frac{1}{nm+n}\mathbb{E}\left[\sum_{(s,x,a,r,s^{\prime},x^{ \prime})\in D}\sum_{(s,\bar{x},a,\bar{r},\bar{s}^{\prime},\bar{x}^{\prime})\in \mathcal{M}_{k}^{(s,x,a,r,s^{\prime},x^{\prime})}}\left(f(s,\bar{x},a)-\bar{r} -\gamma V_{f^{\prime}}(\bar{s}^{\prime},\bar{x}^{\prime})\right)^{2}\right]\] \[= \frac{n}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+ \gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{ \prime})|s,x,a)]\right)\] \[+ \frac{1}{nm+n}\mathbb{E}\left[\sum_{(s,x,a,r,s^{\prime},x^{ \prime})\in D}\mathbb{E}\left[\sum_{(s,\bar{x},a,\bar{r},\bar{s}^{\prime},\bar {x}^{\prime})\in\mathcal{M}_{k}^{(s,x,a,r,s^{\prime},x^{\prime})}}\left(f(s, \bar{x},a)-\bar{r}-\gamma V_{f^{\prime}}(\bar{s}^{\prime},\bar{x}^{\prime}) \right)^{2}\right|s,x,a,r,s^{\prime},x^{\prime}\right]\right]\] \[= \frac{n}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+ \gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{ \prime})|s,x,a)]\right)\]\[+\frac{m}{nm+n}\mathbb{E}\left[\sum_{(s,x,a,r,s^{\prime},x^{\prime}) \in D}\sum_{\bar{x}\in\mathcal{X}}\beta_{k}(\bar{x})\left(f(s,\bar{x},a)-\bar{r }-\gamma V_{f^{\prime}}(\bar{s}^{\prime},\bar{x}^{\prime})\right)^{2}\right]\] \[= \frac{n}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+\gamma ^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{\prime })|s,x,a)]\right)\] \[+ \frac{mn}{nm+n}\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times \mathcal{A}}\mu(s,x,a)\sum_{\bar{x}\in\mathcal{X}}\beta_{k}(\bar{x})\left(f(s,\bar{x},a)-\bar{r}-\gamma V_{f^{\prime}}(\bar{s}^{\prime},\bar{x}^{\prime}) \right)^{2}\] \[= \frac{n}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+ \gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{ \prime})|s,x,a)]\right)\] \[+ \frac{mn}{nm+n}\sum_{(\bar{x},\bar{x},a)\in\mathcal{S}\times \mathcal{X}\times\mathcal{A}}\mu(s,a)\beta_{k}(\bar{x})\left(f(s,\bar{x},a)- \bar{r}-\gamma V_{f^{\prime}}(\bar{s}^{\prime},\bar{x}^{\prime})\right)^{2}\] \[= \frac{n}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+\gamma ^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{ \prime})|s,x,a)]\right)\] \[+ \frac{nm}{nm+n}\left(\|f-\mathcal{T}f^{\prime}\|_{2,\mu_{k}}^{2}+ \gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu_{k}}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{\prime})|s,x,a)]\right),\qquad\text{(by using Lemma~{}\ref{lem:mnp})}\]

where \(\bar{r}=R(s,\bar{x},a),\mu_{k}(s,x,a)=\sum_{x^{\prime}\in\mathcal{X}}\mu(s,x^ {\prime},a)\beta_{k}(x)=\mu(s,a)\beta_{k}(x).\) Since we have \(\bar{\mu}_{\beta_{k}}(s,x,a)=\frac{1}{m+1}\mu(s,x,a)+\frac{m}{m+1}\mu(s,a) \beta_{k}(x).\)

Therefore,

\[\mathcal{L}_{\hat{\mu}_{\beta_{k}}}(f;f^{\prime})-\mathcal{L}_{\hat{\mu}_{ \beta_{k}}}(\mathcal{T}f^{\prime};f^{\prime})=\|f-\mathcal{T}f^{\prime}\|_{2, \bar{\mu}_{\beta_{k}}}^{2}.\]

The next lemma shows an upper bound on \(\|f_{k+1}-\mathcal{T}f_{k}\|_{2,\bar{\mu}_{\beta_{k}}}^{2}.\)

**Lemma A.7**.: _Given the MDP \(M=(\mathcal{S},\mathcal{X},P,R,\gamma),\) we assume that the \(Q-\)function classes \(\mathcal{F}\) satisfies \(\forall f\in\mathcal{F},\mathcal{T}f\in\mathcal{F}.\) The dataset \(D\) is generated as: \((s,x,a)\sim\mu,r=R(s,x,a),s^{\prime}\sim P(\cdot|s,a),x^{\prime}=g(s,x,a,s^{ \prime}),\) and the new dataset \(\hat{D}_{k}=D\cup D_{k}\) is generated by following Alg. 1 with virtual sample generating distribution \(\beta_{k}(x)\) at \(k\)th iteration. Then with probability at least \(1-\delta,\forall f\in\mathcal{F},\) and \(k=0,\ldots,K\) we hvac_

\[\|f_{k+1}-\mathcal{T}f_{k}\|_{2,\bar{\mu}_{\beta_{k}}}^{2}\leq 5\left(\frac{1}{n }+\frac{1}{m}\right)V_{\max}^{2}\log(nK|\mathcal{F}|^{2}/\delta)+\frac{3\delta V _{\max}^{2}}{n}\] (13)

Proof.: Using Lemma A.6 we know that

\[\|f-\mathcal{T}f^{\prime}\|_{2,\bar{\mu}_{\beta_{k}}}^{2}=\mathcal{L}_{\hat{ \mu}_{\beta_{k}}}(f;f^{\prime})-\mathcal{L}_{\hat{\mu}_{\beta_{k}}}(\mathcal{T} f;f^{\prime}).\]

Then it is sufficient to bound \(\|f-\mathcal{T}f^{\prime}\|_{2,\bar{\mu}_{\beta_{k}}}^{2}\) by bounding

\[\mathcal{L}_{\hat{\mu}_{\beta_{k}}}(f;f^{\prime})-\mathcal{L}_{\hat{\mu}_{ \beta_{k}}}(\mathcal{T}f;f^{\prime})=\mathbb{E}[\mathcal{L}_{\hat{D}_{k}}(f;f^ {\prime})-\mathcal{L}_{\hat{D}_{k}}(\mathcal{T}f;f^{\prime})].\]

For any \(f,f^{\prime},\) recall that

\[\mathcal{L}_{\hat{D}_{k}}(f;f^{\prime})= \underbrace{\frac{1}{|\hat{D}_{k}|}\sum_{(s,x,a,r,s^{\prime},x^{ \prime})\in D}\left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime},x^{\prime}) \right)^{2}}_{\mathcal{L}_{D}(f;f^{\prime})}\] \[+\underbrace{\frac{1}{|\hat{D}_{k}|}\sum_{(s,x,a,r,s^{\prime},x^{ \prime})\in D_{k}}\left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime},x^{\prime}) \right)^{2}}_{\mathcal{L}_{D_{k}}(f;f^{\prime})}.\]

For any \(f,f^{\prime}\) define

\[Y(f;f^{\prime}):= (f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime},x^{\prime}))^{2}-( \mathcal{T}f^{\prime}(s,x,a)-r-\gamma V_{f^{\prime}}(s^{\prime},x^{\prime}))^{2}\]

Then for each \((s,x,a,s^{\prime},x^{\prime})\in D,\) we get i.i.d. variables \(Y_{1}(f;f^{\prime}),\ldots,Y_{n}(f;f^{\prime})\).

We also define

\[X_{i}(f;f^{\prime}):= (f(s_{i},\hat{x}_{i},a_{i})-\hat{r}_{i}-\gamma V_{f^{\prime}}({s_{i} }^{\prime},\hat{x_{i}}^{\prime}))^{2}-(\mathcal{T}f^{\prime}(s_{i},\hat{x}_{i},a _{i})-\hat{r}_{i}-\gamma V_{f^{\prime}}({s_{i}}^{\prime},\hat{x_{i}}^{\prime}))^ {2},\]

where \((s_{i},\hat{x}_{i},a_{i},\hat{r}_{i},s_{i}^{\prime},\hat{x}_{i}^{\prime})\) is an augmented sample based on the \(i\)th real sample \((s_{i},x_{i},a_{i},r_{i},s_{i}^{\prime})\). Denote the \(m\) i.i.d virtual samples by \(X_{i_{1}}(f;f^{\prime}),\ldots,X_{i_{m}}(f;f^{\prime})\). Therefore

\[\mathcal{L}_{\hat{D}_{k}}(f;f^{\prime})-\mathcal{L}_{\hat{D}_{k}}(\mathcal{T} f^{\prime};f^{\prime})=\frac{n}{nm+n}\times\frac{1}{n}\sum_{i=1}^{n}Y_{i}(f;f^{ \prime})+\frac{nm}{nm+n}\times\frac{1}{nm}\sum_{i=1}^{n}\sum_{j=1}^{m}X_{i_{j} }(f;f^{\prime}).\] (14)

Taking the expectations on both sides, we obtain for any \(f,f^{\prime}\in\mathcal{F},\)

\[\mathcal{L}_{\hat{\mu}_{\beta_{k}}}(f;f^{\prime})-\mathcal{L}_{\hat{\mu}_{ \beta_{k}}}(\mathcal{T}f^{\prime};f^{\prime})=\frac{n}{nm+n}\mathbb{E}\left[Y( f;f^{\prime})\right]+\frac{nm}{nm+n}\times\frac{1}{n}\mathbb{E}\left[\sum_{i=1}^{n}X_ {i}(f;f^{\prime})\right]\]

We need to introduce \(\frac{1}{n}\sum_{i=1}^{n}Y_{i}(f;f^{\prime})\) and \(\frac{1}{m}\sum_{i=1}^{n}\sum_{j=1}^{m}X_{i_{j}}(f;f^{\prime})\) to bound the above terms. For the first term, we know that the variance of \(Y\) can be bounded by:

\[\text{Var}(Y(f;f^{\prime}))\leq \mathbb{E}[Y(f;f^{\prime})^{2}]\] \[= \mathbb{E}\big{[}\big{(}(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{ \prime},x^{\prime}))^{2}-(\mathcal{T}f^{\prime}(s,x,a)-r-\gamma V_{f}(s^{ \prime},x^{\prime}))^{2}\big{)}^{2}\big{]}\] \[= \mathbb{E}\left[\big{(}f(s,x,a)-\mathcal{T}f^{\prime}(s,x,a)\big{)} ^{2}\left(f(s,x,a)+\mathcal{T}f^{\prime}(s,x,a)-2r-2\gamma V_{f^{\prime}}(s^{ \prime},x^{\prime})\right)^{2}\right]\] \[\leq 4V_{\max}^{2}\mathbb{E}\left[\big{(}f(s,x,a)-\mathcal{T}f^{ \prime}(s,x,a)\big{)}^{2}\right]\] \[= 4V_{\max}^{2}\|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}\] \[= 4V_{\max}^{2}\mathbb{E}[Y(f;f^{\prime})],\] (15)

where the last equality is true because

\[\mathbb{E}[Y(f;f^{\prime})]=\mathbb{E}[\mathcal{L}_{D}(f;f^{ \prime})]-\mathbb{E}[\mathcal{L}_{D}(\mathcal{T}f^{\prime};f^{\prime})]\] \[= \|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}+\gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^{\prime})|s,x,a]\] \[-\|\mathcal{T}f^{\prime}-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}- \gamma^{2}\mathbb{E}_{(s,x,a)\sim\mu}[\text{Var}(V_{f^{\prime}}(s^{\prime},x^ {\prime})|s,x,a]\] (using Lemma A.5 ) \[= \|f-\mathcal{T}f^{\prime}\|_{2,\mu}^{2}.\]

Then by applying Bernstein's inequality, together with a union bound over all \(f,f^{\prime}\in\mathcal{F},\) we obtain with probability \(1-\delta\) we have

\[\mathbb{E}\left[Y(f;f^{\prime})\right]-\frac{1}{n}\sum_{i=1}^{n}Y _{i}(f;f^{\prime})\leq \sqrt{\frac{2\text{Var}(Y(f;f^{\prime}))\log(|\mathcal{F}|^{2}/ \delta)}{n}}+\frac{4V_{\max}^{2}\log(|\mathcal{F}|^{2}/\delta)}{3n}\] \[\leq \sqrt{\frac{8V_{\max}^{2}\mathbb{E}[Y(f;f^{\prime})]\log(|\mathcal{ F}|^{2}/\delta)}{n}}+\frac{4V_{\max}^{2}\log(|\mathcal{F}|^{2}/\delta)}{3n}\] (16)

For the second term, note that for any given \(i\)th sample \((s_{i},x_{i},a_{i},s_{i}^{\prime},x_{i}^{\prime})\) all the variables \(\{X_{i_{j}}\}\) are i.i.d. Then following a similar argument, then for all \(f,f^{\prime}\in\mathcal{F},\) we have with probability at least \(1-\delta/n,\)

\[\mathbb{E}[X_{i}(f;f^{\prime})|s_{i},x_{i},a_{i}]-\frac{1}{m}\sum_ {j=1}^{m}X_{i_{j}}(f;f^{\prime})\] \[\leq \sqrt{\frac{8V_{\max}^{2}\mathbb{E}[X_{i}(f;f^{\prime})|s_{i},x_{ i},a_{i}]\log(n|\mathcal{F}|^{\prime}/\delta)}{m}}+\frac{4V_{\max}^{2}\log(n| \mathcal{F}|^{2}/\delta)}{3m}\]

Then it is easy to obtain that we have for all \(f,f^{\prime}\in\mathcal{F},\)

\[\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}\left[X_{i}(f;f^{\prime})-\frac{1}{m}\sum_{j=1 }^{m}X_{i_{j}}(f;f^{\prime})\right]\]

[MISSING_PAGE_FAIL:19]

### Proof of Theorem 1

Now we are ready to show the main theorem. Given a dataset \(D\). After generating virtual samples \(D_{k}\) we get a new combined dataset \(\hat{D}_{k}=D\cup D_{k}\) at each iteration \(k\) with virtual sample generating distribution \(\beta_{k}(x).\) We first have

\[v^{\pi_{f_{k}}}-v^{*}= \frac{1}{1-\gamma}\mathbb{E}_{(s,x)\sim d_{\eta_{0}}^{\pi_{f_{k}}} (s,x)}\left[Q^{*}(s,x,\pi_{f_{k}})-V^{*}(s,x)\right]\] \[\leq \frac{1}{1-\gamma}\mathbb{E}_{(s,x)\sim d_{\eta_{0}}^{\pi_{f_{k}} }(s,x)}\left[Q^{*}(s,x,\pi_{f_{k}})-f_{k}(s,x,\pi_{f_{k}})+f_{k}(s,x,\pi^{*})-V ^{*}(s,x)\right]\] \[\leq \frac{1}{1-\gamma}\left(\left\|Q^{*}-f_{k}\right\|_{1,d_{\eta_{0} }^{\pi_{f_{k}}}(s,x)\times\pi_{f_{k}}}+\left\|Q^{*}-f_{k}\right\|_{1,d_{\eta_{ 0}}^{\pi_{f_{k}}}(s,x)\times\pi^{*}}\right)\] \[\leq \frac{1}{1-\gamma}\left(\left\|Q^{*}-f_{k}\right\|_{2,d_{\eta_{0} }^{\pi_{f_{k}}}(s,x)\times\pi_{f_{k}}}+\left\|Q^{*}-f_{k}\right\|_{2,d_{\eta_{ 0}}^{\pi_{f_{k}}}(s,x)\times\pi^{*}}\right),\] (20)

where the first equality follows from the performance difference lemma (Kakade and Langford, 2002), the first inequality holds because \(\pi_{f_{k}}\in\arg\min_{a}f_{k}(s,x,a)\) and the last inequality is true by using the fact that for any vector \(a=(a_{1},\ldots,a_{n})\) and a valid distribution \(d=(d_{1},\ldots,d_{n}),\sum_{i}d_{i}=1\)

\[\|a\|_{1,d}= \sum_{i}|a_{i}|d_{i}=\sum_{i}|a_{i}|\sqrt{d_{i}}\sqrt{d_{i}}\] (Cauchy-Schwarz inequality) \[\leq \sqrt{\sum_{i}d_{i}}\times\sqrt{\sum_{i}|a_{i}|^{2}d_{i}}=\|a\|_ {2,d}.\]

According to Assumption A.4, we know that \(\sum_{s\in\mathcal{S}}\sum_{x\in\mathcal{X},x\notin\mathcal{B}}d_{\eta_{0}}^{ \pi_{f_{k}}}(s,x)\leq\sigma_{2},\) which implies that \(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times\mathcal{A},x\notin\mathcal{ B}}\{d_{\eta_{0}}^{\pi_{f_{k}}}(s,x)\times\pi^{*}\}(s,x,a)\leq\sigma_{2},\) and \(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times\mathcal{A},x\notin \mathcal{B}}\{d_{\eta_{0}}^{\pi_{f_{k}}}(s,x)\times\pi_{f_{k}}\}(s,x,a)\leq \sigma_{2}.\) Define \(\xi=\{d_{\eta_{0}}^{\pi_{f_{k}}}(s,x)\times\pi_{f_{k}}\}(s,x,a),\) then we have

\[\|f_{k}-Q^{*}\|_{2,\xi}=\left(\sum_{(s,x,a)\in\mathcal{S}\times \mathcal{X}\times\mathcal{A}}|f_{k}(s,x,a)-Q^{*}(s,x,a)|^{2}\xi(s,x,a)\right)^{ 1/2}\] \[\leq \left(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times\mathcal{ A},x\notin\mathcal{B}}|f_{k}(s,x,a)-Q^{*}(s,x,a)|^{2}\xi(s,x,a)\right)^{1/2}\] \[+\left(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times \mathcal{A},x\notin\mathcal{B}}|f_{k}(s,x,a)-Q^{*}(s,x,a)|^{2}\xi(s,x,a)\right) ^{1/2}\] \[\leq \|f_{k}-Q^{*}\|_{2,\xi}+\sqrt{\sigma_{2}}V_{\max}\] \[\leq \sqrt{\frac{(m+1)C}{m\sigma_{1}}}\|f_{k}-\mathcal{T}f_{k-1}\|_{2, \tilde{\mu}_{\tilde{\mu}_{k}}}+\gamma\|f_{k-1}-Q^{*}\|_{2,P(\xi)\times\pi_{f_{ k-1},Q^{*}}}+\sqrt{\sigma_{2}}V_{\max},\] (21)

where the first inequality holds because \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) (\(a\geq 0,b\geq 0\)) and the last inequality comes from Lemma A.4.

By using LemmaA.7 we have with at least probability \(1-\delta\)

\[\|f_{k}-\mathcal{T}f_{k-1}\|_{2,\tilde{\mu}_{\tilde{\beta}_{k}}}^{2}\leq\|f_{k }-\mathcal{T}f_{k-1}\|_{2,\tilde{\mu}_{\tilde{\mu}_{k}}}^{2}\leq\epsilon_{1}\] (22)

where \(\epsilon_{1}=5\left(\frac{1}{n}+\frac{1}{m}\right)V_{\max}^{2}\log(nK|\mathcal{ F}|^{2}/\delta)+\frac{3\delta V_{\max}^{2}}{n}.\) Therefore, we obtain

\[\|f_{k}-Q^{*}\|_{2,\xi}\leq\gamma\|f_{k-1}-Q^{*}\|_{2,P(\xi)\times\pi_{f_{k-1},Q^{*}}}+\sqrt{\frac{(m+1)C\epsilon_{1}}{m\sigma_{1}}}+\sqrt{\sigma_{2}}V_{ \max}.\] (23)

Now define \(\xi^{\prime}=P(\xi)\times\pi_{f_{k-1},Q^{*}}.\) Then based on Assumption A.4, it is easy to obtain \(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times\mathcal{A},x\notin \mathcal{B}}\xi^{\prime}(s,x,a)\leq c_{0}\sigma_{2}.\) Note that the distribution \(\xi^{\prime\prime}=P(\xi^{\prime})\times\pi_{f_{k-2},Q^{*}}\) still satisfies \(\sum_{(s,x,a)\in\mathcal{S}\times\mathcal{X}\times\mathcal{A},x\notin \mathcal{B}}\xi^{\prime\prime}(s,x,a)\leq c_{0}\sigma_{2}\) according to Assumption A.4, because \(\pi_{f_{k-1}},\pi_{f_{k-2}}\) and \(Q^{*}\) are all assumed to be reasonable policies.

Repeating the expansion above for \(k\) times, we have

\[\|f_{k}-Q^{*}\|_{2,\xi}\leq\frac{1-\gamma^{k}}{1-\gamma}\left(\sqrt{ \frac{(m+1)C\epsilon_{1}}{m\sigma_{1}}}+\sqrt{c_{0}\sigma_{2}}V_{\max}\right)+ \gamma^{k}V_{\max}.\]

All the above analyses are still applied to the case when \(\xi=\{d_{n_{0}}^{\pi_{f_{k}}}(s,x)\times\pi^{*}\}\). Therefore, it is straightforward to obtain

\[v^{*}-v^{\pi_{f_{k}}}\leq\frac{2}{(1-\gamma)^{2}}\left(\sqrt{ \frac{(m+1)C\epsilon_{1}}{m\sigma_{1}}}+\sqrt{c_{0}\sigma_{2}}V_{\max}+\gamma^ {k}(1-\gamma)V_{\max}\right).\] (24)

Substituting \(\epsilon_{1}\) completes the proof.

### Extension of Theorem 1

We repeat the assumption for extending our main results to the case when \(|\mathcal{X}|\) can be infinite such that \(f(s,x,a)\) may not be bounded by \(V_{\max}\).

**Repeat of Assumption 1:** For the typical set \(\mathcal{B}\) of pseudo-stochastic states (defined in Assumption A.4), for any \(s,a\in\mathcal{S}\times\mathcal{A},f\in\mathcal{F}\), if \(x\in\mathcal{B}\), then \(f(s,x,a)\leq V_{\max}\) otherwise if \(x\notin\mathcal{B}\), we have \(|f(s,x,a)-Q^{*}(s,x,a)|\leq V_{\max}\). Furthermore, for any given \(f\in\mathcal{F},(s,x),x\in\mathcal{B}\), we have \(|V_{f}(s^{\prime},x^{\prime})-V_{f}(s^{\prime\prime},x^{\prime\prime})|\leq V _{\max}\), where \(x^{\prime}=g(s,x,\pi_{f},s^{\prime}),x^{\prime\prime}=g(s,x,\pi_{f},s^{\prime \prime})\).

There are two places we need to pay attention to: \((1):\) a bound on \(\|f_{0}-Q^{*}\|_{2,P(\xi)\times\pi_{f_{0},Q^{*}}}\), \((2):\) a bound on the variance of \(Y\) as shown in Eq. (15). For the first case, it automatically holds due to assumption 1. For the second term, we first have

\[\text{Var}(Y(f;f^{\prime}))\leq\mathbb{E}[Y(f;f^{\prime})^{2}]\] \[= \mathbb{E}[\left(\left(f(s,x,a)-r-\gamma V_{f^{\prime}}(s^{ \prime},x^{\prime})\right)^{2}-\left(\mathcal{T}f^{\prime}(s,x,a)-r-\gamma V_ {f}(s^{\prime},x^{\prime})\right)^{2}\right)^{2}]\] \[= \mathbb{E}\left[\left(f(s,x,a)-\mathcal{T}f^{\prime}(s,x,a) \right)^{2}\left(f(s,x,a)+\mathcal{T}f^{\prime}(s,x,a)-2r-2\gamma V_{f^{ \prime}}(s^{\prime},x^{\prime})\right)^{2}\right]\] \[= \mathbb{E}\left[\left(f(s,x,a)-\mathcal{T}f^{\prime}(s,x,a) \right)^{2}\left(f(s,x,a)-\mathcal{T}f^{\prime}(s,x,a)+2\mathcal{T}f^{\prime} (s,x,a)-2r-2\gamma V_{f^{\prime}}(s^{\prime},x^{\prime})\right)^{2}\right].\]

We also know that

\[\left(f(s,x,a)-\mathcal{T}f^{\prime}(s,x,a)+2\mathcal{T}f^{\prime }(s,x,a)-2r-2\gamma V_{f^{\prime}}(s^{\prime},x^{\prime})\right)^{2}\] \[\leq 2(f(s,x,a)-\mathcal{T}f^{\prime}(s,x,a))^{2}+2(2\mathcal{T}f^{ \prime}(s,x,a)-2r-2\gamma V_{f^{\prime}}(s^{\prime},x^{\prime}))^{2}\] \[= 2(f(s,x,a)+Q^{*}(s,x,a)-Q^{*}(s,x,a)-\mathcal{T}f^{\prime}(s,x,a ))^{2}+8\gamma^{2}(\mathbb{E}[V_{f^{\prime}}^{\prime}(\hat{s},\hat{x})|s,x,a]- V_{f^{\prime}}(s^{\prime},x^{\prime}))^{2}\] \[\leq 16V_{\max}^{2}.\]

Therefore, we have \(\text{Var}(Y(f;f^{\prime}))\leq 16V_{\max}^{2}\mathbb{E}[Y(f;f^{\prime})]\).

Then we can obtain a similar result of the same order, which only differs for some constant \(\tilde{c}\) such that

\[v^{*}-v^{\pi_{f_{k}}}\leq\frac{2\tilde{c}}{(1-\gamma)^{2}}\left( \sqrt{\frac{(m+1)C\epsilon_{1}}{m\sigma_{1}}}+\sqrt{c_{0}\sigma_{2}}V_{\max}+ \gamma^{k}(1-\gamma)V_{\max}\right).\] (25)

## Appendix B Additional Simulations

### Combining PSG with Policy Gradient-type algorithms

In this section, we investigate the possibilities of using ASG in policy gradient-type algorithms. In particular, we use ASG in the phase of policy evaluation. An algorithm (SAC-ASG) that incorporates ASG into SAC is presented in Alg. 3. We also compare our algorithm SAC-ASG with state-of-art Dyna-type model-based approaches, i.e., MBPO (Janner et al., 2019) on the phases criss-cross network environment (Fig. 1(b)). The simulation results are shown in Fig.4. We can observe that the performance of our approach is significantly better than the baselines'. We also would like to emphasize that the training time of our approach is much less than that of MBPO (\(4\) hours v.s. \(3\) days).

### Details of the Environment in Section 4.4

In this section, we summarize the detailed parameters used in section 4.4 in Table 3.

### Two-phase Wireless Network

The two-phase wireless network is a modified version of the downlink network in Section 2.1, which cannot be modeled as an MDP with exogenous inputs. In this example, each packet has two phases. When a packet from a mobile is scheduled, if it is in phase \(1\), it leaves with probability \(0.2\) or moves from phase \(1\) to phase \(2\); and if it is in phase \(2\), it leaves the system. We considered Poisson arrivals

Figure 4: Performance on the Two-phase Criss-Cross Network

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Setting & Arrival Rates & Service Rates & Job Size Range \\ \hline \((a)\) & \(\{0.6,0.6\}\) & \(\{2,1.5,1.5\}\) & \(2\) \\ \hline \((b)\) & \(\{0.6,0.6\}\) & \(\{7,3.5,7\}\) & \(5\) \\ \hline \((c)\) & \(\{0.6,0.6\}\) & \(\{2.5,4.5,2.5\}\) & \(5\) \\ \hline \end{tabular}
\end{table}
Table 3: Detailed Environment Parameterswith rates \(\Lambda=\{2,4,3\}\), and the channel rates \(O=\{12,12,12\}\). As the result shown in Figure. 5 ASG outperforms Max-weight in this environment as well.

## Appendix C Simulation Details

In our simulations, the reward function is the total queue length (or a scaled version: divided by \(20\) i the wireless networks). We used Gaussian distributions fitted with real data samples as \(\beta_{k}\), for each real sample, we generated \(50\) augmented samples and repeated the process \(10\) times. We use a simple two layer neural network with hidden size \(128\) with learning rate \(0.0003\). The batch size used in all the simulations is \(256\).

### Experimental settings

For all the simulations, We used a single NVIDIA GeForce RTX 2080 Super with AMD Ryzen 7 3700 \(8\)-Core Processor.

Figure 5: Performance on the Two-phase Downlink Wireless Network