# Debias Coarsely, Sample Conditionally:

Statistical Downscaling through Optimal Transport

and Probabilistic Diffusion Models

 Zhong Yi Wan

Google Research

Mountain View, CA 94043, USA

wanzy@google.com

&Ricardo Baptista

California Institute of Technology

Pasadena, CA 91106, USA

rsb@caltech.edu

&Yi-fan Chen

Google Research

Mountain View, CA 94043, USA

yifanchen@google.com

&John Roberts Anderson

Google Research

Mountain View, CA 94043, USA

janders@google.com

&Anudhyan Boral

Google Research

Mountain View, CA 94043, USA

anudhyan@google.com

&Fei Sha

Google Research

Mountain View, CA 94043, USA

fsha@google.com

&Leonardo Zepeda-Nunez

Google Research

Mountain View, CA 94043, USA

lzepedanunez@google.com

Equal contribution

###### Abstract

We introduce a two-stage probabilistic framework for statistical downscaling _using unpaired data_. Statistical downscaling seeks a probabilistic map to transform low-resolution data from a _biased_ coarse-grained numerical scheme to high-resolution data that is consistent with a high-fidelity scheme. Our framework tackles the problem by composing two transformations: (i) a debiasing step via an optimal transport map, and (ii) an upsampling step achieved by a probabilistic diffusion model with _a posteriori_ conditional sampling. This approach characterizes a conditional distribution _without needing paired data_, and faithfully recovers relevant physical statistics from biased samples. We demonstrate the utility of the proposed approach on one- and two-dimensional fluid flow problems, which are representative of the core difficulties present in numerical simulations of weather and climate. Our method produces realistic high-resolution outputs from low-resolution inputs, by upsampling resolutions of \(8\times\) and \(16\times\). Moreover, our procedure correctly matches the statistics of physical quantities, even when the low-frequency content of the inputs and outputs do not match, a crucial but difficult-to-satisfy assumption needed by current state-of-the-art alternatives. Code for this work is available at: [https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/probabilistic_diffusion](https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/probabilistic_diffusion).

Introduction

Statistical downscaling is crucial to understanding and correlating simulations of complex dynamical systems at multiple resolutions. For example, in climate modeling, the computational complexity of general circulation models (GCMs) [4] grows rapidly with resolution. This severely limits the resolution of long-running climate simulations. Consequently, accurate predictions (as in forecasting localized, regional and short-term weather conditions) need to be _downscaled_ from coarser lower-resolution models' outputs. This is a challenging task: coarser models do not resolve small-scale dynamics, thus creating bias [16; 69; 84]. They also lack the necessary physical details (for instance, regional weather depends heavily on local topography) to be of practical use for regional or local climate impact studies [33; 36], such as the prediction or risk assessment of extreme flooding [35; 44], heat waves [59], or wildfires [1].

At the most abstract level, _statistical downscaling_[81; 82] learns a map from low- to high-resolution data. However, it has several unique challenges. First, unlike supervised machine learning (ML), there is _no natural pairing of samples_ from the low-resolution model (such as climate models [23]) with samples from higher-resolution ones (such as weather models that assimilate observations [40]). Even in simplified cases of idealized fluids problems, one cannot naively align the simulations in time, due to the chaotic behavior of the models: two simulations with very close initial conditions will diverge rapidly. Several recent studies in climate sciences have relied on synthetically generated paired datasets. The synthesis process, however, requires accessing both low- and high-resolution models and either (re)running costly high-resolution models while respecting the physical quantities in the low-resolution simulations [25; 43] or (re)running low-resolution models with additional terms nudging the outputs towards high-resolution trajectories [12]. In short, requiring data in correspondence for training severely limits the potential applicability of supervised ML methodologies in practice, despite their promising results [37; 39; 60; 66; 38].

Second, unlike the setting of (image) super-resolution [26], in which an ML model learns the (pseudo) inverse of a downsampling operator [13; 78], downscaling additionally needs to correct the bias. This difference is depicted in Fig. 1(a). Super-resolution can be recast as frequency extrapolation [10], in which the model reconstructs high-frequency contents, while matching the low-frequency contents of a low-resolution input. However, the restriction of the target high-resolution data may not match the distribution of the low-resolution data in Fourier space [49]. Therefore, debiasing is necessary to correct the Fourier spectrum of the low-resolution input to render it admissible for the target distribution (moving solid red to solid blue lines with the dashed blue extrapolation in Fig. 1). Debiasing allows us to address the crucial yet challenging prerequisite of aligning the low-frequency statistics between the low- and high-resolution datasets.

Given these two difficulties, statistical downscaling should be more naturally framed as matching two probability distributions linked by an unknown map; such a map emerges from both distributions representing the same underlying physical system, albeit with different characterizations of the system's statistics at multiple spatial and temporal resolutions. The core challenge is then: _how do we structure the downscaling map so that the (probabilistic) matching can effectively remediate the bias introduced by the coarser, i.e., the low-resolution, data distribution?_

Thus, the main idea behind our work is to introduce a debiasing step so that the debiased (yet, still coarser) distribution is closer to the target distribution of the high-resolution data. This step results in an intermediate representation for the data that preserves the correct statistics needed in the follow-up step of upsampling to yield the high-resolution distribution. In contrast to recent works on distribution matching for unpaired image-to-image translation [86] and climate modeling [32], the additional structure our work imposes on learning the mapping prevents the bias in the low-resolution data from polluting the upsampling step. We review those approaches in SS2 and compare to them in SS4.

Concretely, we propose a new probabilistic formulation for the downscaling problem that handles _unpaired data_ directly, based on a factorization of the unknown map linking both low- and high-resolution distributions. This factorization is depicted in Fig. 1(b). By appropriately restricting the maps in the factorization, we rewrite the downscaling map as the composition of two procedures: a debiasing step performed using an optimal transport map [21], which _couples the data distributions_ and corrects the biases of the low-resolution snapshots; followed by an upsampling step performed using conditional probabilistic diffusion models, which have produced state-of-the-art results for image synthesis and flow construction [5; 52; 71; 73].

We showcase the performance of our framework on idealized fluids problems that exhibit the same core difficulty present in atmospheric flows. We show that our framework is able to generate realistic snapshots that are faithful to the physical statistics, while outperforming several baselines.

## 2 Related work

The most direct approach to upsampling low-resolution data is to learn a low- to high-resolution mapping via paired data when it is possible to collect such data. For complex dynamical systems, several methods carefully manipulate high- and low-resolution models, either by nudging or by enforcing boundary conditions, to produce paired data without introducing spectral biases [12; 25]. Alternatively, if one has strong prior knowledge about the process of downsampling, optimization methods can solve an inverse problem to directly estimate the high-resolution data, leveraging prior assumptions such as sparsity in compressive sensing [9; 10] or translation invariance [42].

In our setting, there is no straightforward way to obtain paired data due to the nature of the problem (i.e., turbulent flows, with characteristically different statistics across a large span of spatio-temporal scales). In the weather and climate literature (see [79] for an extensive overview), prior knowledge can be exploited to downscale specific variables [81]. One of the most predominant methods of this type is bias-correction spatial disaggregation (BCSD), which combines traditional spline interpolation with a quantile matching bias correction [56], and linear models [41]. Recently, several studies have used ML to downscale physical quantities such as precipitation [78], but without quantifying the prediction uncertainty. Yet, a generally applicable method to downscale arbitrary variables is lacking.

Another difficulty is to remove the bias in the low resolution data. This is an instance of domain adaptation, a topic popularly studied in computer vision. Recent work has used generative models such as GANs and diffusion models to bridge the gap between two domains [5; 7; 14; 58; 60; 62; 68; 74; 83; 85]. A popular domain alignment method that was used in [32] for downscaling weather data is AlignFlow [34]. This approach learns normalizing flows for source and target data of the same dimension, and uses their common latent space to move across domains. The advantage of those methods is that they do not require training data from two domains in correspondence. Many of those approaches are related to optimal transport (OT), a rigorous mathematical framework for learning maps between two domains without paired data [80]. Recent computational advances in OT for discrete (i.e., empirical) measures [21; 64] have resulted in a wide set of methods for domain adaptation [20; 31]. Despite their empirical success with careful choices of regularization, their use alone for high-dimensional images has remained limited [61].

Our work uses diffusion models to perform upsampling after a debiasing step implemented with OT. We avoid common issues from GANs [75] and flow-based methods [54], which include over-smoothing, mode collapse and large model footprints [24; 52]. Also, due to the debiasing map, which matches the low-frequency content in distribution (see Fig. 1(a)), we do not need to explicitly impose that the low-frequency power spectra of the two datasets match like some competing methods do [5]. Compared to formulations that perform upsampling and debiasing simultaneously [5; 78], our

Figure 1: (a) Upsampling (super-resolution) as frequency extrapolation in the Fourier domain. The model extrapolates low-frequency content to higher-frequencies (dashed blue). The debiasing map corrects the biased low-frequency content (solid red). (b) Illustration of the proposed framework where \(\mathcal{X}\) is the space of high-resolution data, \(\mathcal{Y}\) is the space of low-resolution data, \(C\) is an _unknown nonlinear_ map linking \(\mathcal{X}\) and \(\mathcal{Y}\), \(C^{\prime}\) is a _known linear_ downsampling map, \(\mathcal{Y}^{\prime}\) is an intermediate (low-resolution) space induced by the image of \(C^{\prime}\), and \(T\) is an invertible debiasing map such that \(C\) can be factorized as \(T^{-1}\circ C^{\prime}\). The conditional probabilities \(p(x|C^{\prime}x=y^{\prime})\) are used for the probabilistic upsampling procedure.

framework performs these two tasks separately, by only training (and independently validating) a single probabilistic diffusion model for the high-resolution data once. This allows us to quickly assess different modeling choices, such as the linear downsampling map, by combining the diffusion model with different debiasing maps. Lastly, in comparison to other two-stage approaches [5; 32], debiasing is conducted at low-resolutions, which is less expensive as it is performed on a much smaller space, and more efficient as it is not hampered from spurious biases introduced by interpolation techniques.

## 3 Methodology

SetupWe consider two spaces: the high-fidelity, high-resolution space \(\mathcal{X}=\mathbb{R}^{d}\) and the low-fidelity, low-resolution space \(\mathcal{Y}=\mathbb{R}^{d^{\prime}}\), where we suppose that \(d>d^{\prime}\). We model the elements \(X\in\mathcal{X}\) and \(Y\in\mathcal{Y}\) as random variables with marginal distributions, \(\mu_{X}\) and \(\mu_{Y}\), respectively. In addition, we suppose there is a statistical model relating the \(X\) and \(Y\) variables via \(C\colon\mathcal{X}\to\mathcal{Y}\), an unknown and possibly nonlinear, downsampling map. See Fig. 1(b) for a diagram.

Given an observed realization \(\bar{y}\in\mathcal{Y}\), which we refer to as a _snapshot_, we formulate downscaling as the problem of sampling from the conditional probability distribution \(p(x|E_{\bar{y}})\) for the event \(E_{\bar{y}}:=\{x\in\mathcal{X}\,|\,C(x)=\bar{y}\}\), which we denote by \(p(x|C(x)=\bar{y})\). Our objective is to sample this distribution given only access to marginal samples of \(X\) and \(Y\).

Main ideaIn general, downscaling is an ill-posed problem given that the joint distribution of \(X\) and \(Y\) is not prescribed by a known statistical model. Therefore, we seek an approximation to \(C\) so the statistical properties of \(X\) are preserved given samples of \(\mu_{Y}\). In particular, such a map should satisfy \(C_{\sharp}\mu_{X}=\mu_{Y}\), where \(C_{\sharp}\mu_{X}\) denotes the push-forward measure of \(\mu_{X}\) through \(C\).

In this work, we impose a structured ansatz to approximate \(C\). Specifically, we _factorize_ the map \(C\) as the composition of a known and linear _downsampling map_\(C^{\prime}\), and an invertible _debiasing map_\(T\):

\[C=T^{-1}\circ C^{\prime},\quad\text{such that}\quad(T^{-1}\circ C^{\prime})_{ \sharp}\mu_{X}=\mu_{Y}, \tag{1}\]

or alternatively, \(C^{\prime}_{\sharp}\mu_{X}=T_{\sharp}\mu_{Y}\). This factorization decouples and explicitly addresses two entangled goals in downscaling: debiasing and upsampling. We discuss the advantage of such factorization, after sketching how \(C^{\prime}\) and \(T\) are implemented.

The range of the downsampling map \(C^{\prime}\colon\mathcal{X}\to\mathcal{Y}^{\prime}\) defines an _intermediate_ space \(\mathcal{Y}^{\prime}=\mathbb{R}^{d^{\prime}}\) of high-fidelity low-resolution samples with measure \(\mu_{Y^{\prime}}\). Moreover, the joint space \(\mathcal{X}\times\mathcal{Y}^{\prime}\) is built by projecting samples of \(X\) into \(\mathcal{Y}^{\prime}\), i.e., \((x,y^{\prime})=(x,C^{\prime}x)\in\mathcal{X}\times\mathcal{Y}^{\prime}\); see Fig. 1(b). Using these spaces, we decompose the domain adaptation problem into the following three sub-problems:

1. _High-resolution prior_: Estimate the marginal density \(p(x)\);
2. _Conditional modeling_: For the joint variables \(X\times Y^{\prime}\), approximate \(p(x|C^{\prime}x=y^{\prime})\);
3. _Debiasing_: Compute a transport map such that \(T_{\sharp}\mu_{Y}=C^{\prime}_{\sharp}\mu_{X}\).

For the first sub-problem, we train an _unconditional_ model to approximate \(\mu_{X}\), or \(p(x)\), as explained in SS3.1. For the second sub-problem, we leverage the prior model and \(y^{\prime}\in\mathcal{Y}^{\prime}\) to build a model for _a posteriori_ conditional sampling of \(p(x|C^{\prime}x=y^{\prime})\), which allows us to upsample snapshots from \(\mathcal{Y}^{\prime}\) to \(\mathcal{X}\), as explained in SS3.2. For the third sub-problem, we use domain adaptation to shift the resulting model from the source domain \(\mathcal{X}\times\mathcal{Y}^{\prime}\) to the target domain \(\mathcal{X}\times\mathcal{Y}\), for which there is no labeled data. For such a task, we build a transport map \(T:\mathcal{Y}\to\mathcal{Y}^{\prime}\) satisfying the condition that \(T_{\sharp}\mu_{Y}=\mu_{Y^{\prime}}=C^{\prime}_{\sharp}\mu_{X}\). This map is found by solving an optimal transport problem, which we explain in SS3.3.

Lastly, we merge the solutions to the sub-problems to arrive at our core downscaling methodology, which is summarized in Alg. 1. In particular, given a low-fidelity and low-resolution sample \(\overline{y}\), we use the optimal transport map \(T\) to project the sample to the high-fidelity space \(\overline{y}^{\prime}=T(\overline{y})\) and use the conditional model to sample \(p(x|C^{\prime}x=\overline{y}^{\prime})\). The resulting samples are contained in the high-fidelity and high-resolution space.

The factorization in Eq. (1) has several advantages. We do not require a cycle-consistency type of loss [34; 86]: the consistency condition is automatically enforced by Eq. (1) and the conditional sampling. By using a linear downsampling map \(C^{\prime}\), it is trivial to create the intermediate space \(\mathcal{Y}^{\prime}\), while rendering the conditional sampling tractable: conditional sampling with a nonlinear map is often more expensive and it requires more involved tuning [17; 18]. The factorization also allows us to compute the debiasing map in a considerably lower dimensional space, which conveniently requires less data to cover the full distribution, and fewer iterations to find the optimal map [21].

### High-resolution prior

To approximate the prior of the high-resolution snapshots we use a probabilistic diffusion model, which is known to avoid several drawbacks of other generative models used for super-resolution [52], while providing greater flexibility for _a posteriori_ conditioning [17; 30; 46; 47].

Intuitively, diffusion-based generative models involves iteratively transforming samples from an initial noise distribution \(p_{T}\) into ones from the target data distribution \(p_{0}=p_{\text{data}}\). Noise is removed sequentially such that samples follow a family of marginal distributions \(p_{t}(x_{t};\sigma_{t})\) for decreasing diffusion times \(t\) and noise levels \(\sigma_{t}\). Conveniently, such distributions are given by a forward noising process that is described by the stochastic differential equation (SDE) [45; 73]

\[dx_{t}=f(x_{t},t)dt+g(x_{t},t)dW_{t}, \tag{2}\]

with drift \(f\), diffusion coefficient \(g\), and the standard Wiener process \(W_{t}\). Following [45], we set

\[f(x_{t},t)=f(t)x_{t}:=\frac{\dot{s}_{t}}{s_{t}}x_{t},\qquad\text{and}\qquad g( x_{t},t)=g(t):=s_{t}\sqrt{2\dot{\sigma}_{t}\sigma_{t}}. \tag{3}\]

Solving the SDE in Eq. (2) forward in time with an initial condition \(x_{0}\) leads to the Gaussian perturbation kernel \(p(x_{t}|x_{0})=\mathcal{N}(x_{t};s_{t}x_{0},s_{t}^{2}\sigma_{t}^{2}\mathbf{I})\). Integrating the kernel over the data distribution \(p_{0}(x_{0})=p_{\text{data}}\), we obtain the marginal distribution \(p_{t}(x_{t})\) at any \(t\). As such, one may prescribe the profiles of \(s_{t}\) and \(\sigma_{t}\) so that \(p_{0}=p_{\text{data}}\) (with \(s_{0}=1,\sigma_{0}=0\)), and more importantly

\[p_{T}(x_{T})\approx\mathcal{N}(x_{T};0,s_{T}^{2}\sigma_{T}^{2}\mathbf{I}), \tag{4}\]

i.e., the distribution at the terminal time \(T\) becomes indistinguishable from an isotropic, zero-mean Gaussian. To sample from \(p_{\text{data}}\), we utilize the fact that the reverse-time SDE

\[dx_{t}=\big{[}f(t)x_{t}-g(t)^{2}\nabla_{x_{t}}\log p_{t}(x_{t})\big{]}dt+g(t) dW_{t}, \tag{5}\]

has the same marginals as Eq. (2). Thus, by solving Eq. (5) backwards using Eq. (4) as the final condition at time \(T\), we obtain samples from \(p_{\text{data}}\) at \(t=0\).

Therefore, the problem is reduced to estimating the _score function_\(\nabla_{x_{t}}\log p_{t}(x_{t})\) resulting from \(p_{\text{data}}\) and the prescribed diffusion schedule \((s_{t},\sigma_{t})\). We adopt the denoising formulation in [45] and learn a neural network \(D_{\theta}(x_{0}+\varepsilon_{t},\sigma_{t})\), where \(\theta\) denotes the network parameters. The learning seeks to minimize the \(L_{2}\)-error in predicting the true sample \(x_{0}\) given a noise level \(\sigma_{t}\) and the sample noised with \(\varepsilon_{t}=\sigma_{t}\varepsilon\) where \(\varepsilon\) is drawn from a standard Gaussian. The score can then be readily obtained from the denoiser \(D_{\theta}\) via the asymptotic relation (i.e., Tweedie's formula [29])

\[\nabla_{x_{t}}\log p_{t}(x_{t})\approx\frac{D_{\theta}(\hat{x}_{t},\sigma_{t })-\hat{x}_{t}}{s_{t}\sigma_{t}^{2}},\qquad\hat{x}_{t}=x_{t}/s_{t}. \tag{6}\]

### _A posteriori_ conditioning via post-processed denoiser

We seek to super-resolve a low-resolution snapshot \(\bar{y}^{\prime}\in\mathcal{Y}^{\prime}\) to a high-resolution one by leveraging the high-resolution prior modeled by the diffusion model introduced above. Abstractly, our goal is to sample from \(p(x_{0}|E^{\prime}_{\bar{y}^{\prime}})\), where \(E^{\prime}_{\bar{y}^{\prime}}=\{x_{0}:C^{\prime}x_{0}\!=\!\bar{y}^{\prime}\}\). Following [30], this may be approximated by modifying the learned denoiser \(D_{\theta}\) at _inference time_ (see Appendix A for more details):

\[\tilde{D}_{\theta}(\hat{x}_{t},\sigma_{t})=(C^{\prime})^{\dagger}\bar{y}^{ \prime}+(I-VV^{T})\left[D_{\theta}(\hat{x}_{t},\sigma_{t})-\alpha\nabla_{\hat{ x}_{t}}\|C^{\prime}D_{\theta}(\hat{x}_{t},\sigma_{t})-\bar{y}^{\prime}\|^{2} \right], \tag{7}\]where \((C^{\prime})^{\dagger}=V\Sigma^{-1}U^{T}\) is the pseudo-inverse of \(C^{\prime}\) based on its singular value decomposition (SVD) \(C^{\prime}=U\Sigma V^{T}\), and \(\alpha\) is a hyperparameter that is empirically tuned. The \(\tilde{D}_{\theta}\) defined in Eq. (7) directly replaces \(D_{\theta}\) in Eq. (6) to construct a conditional score function \(\nabla_{x_{t}}\log p_{t}(x_{t}|E^{\prime}_{\bar{y}^{\prime}})\) that facilitates the sampling of \(p(x_{0}|E^{\prime}_{\bar{y}^{\prime}})\) using the reverse-time SDE in Eq. (5).

### Debiasing via optimal transport

In order to upsample a biased low-resolution data \(\overline{y}\in\mathcal{Y}\), we first seek to find a mapping \(T\) such that \(\overline{y}^{\prime}=T(\overline{y})\in\mathcal{Y}^{\prime}\) is a representative sample from the distribution of unbiased low-resolution data. Among the infinitely many maps that satisfy this condition, the framework of optimal transport (OT) selects a map by minimizing an integrated transportation distance based on the cost function \(c\colon\mathcal{Y}\times\mathcal{Y}^{\prime}\to\mathbb{R}^{+}\). The function \(c(y,y^{\prime})\) defines the cost of moving one unit of probability mass from \(y^{\prime}\) to \(y\). By treating \(Y,Y^{\prime}\) as random variables on \(\mathcal{Y},\mathcal{Y}^{\prime}\) with measures \(\mu_{Y},\mu_{Y^{\prime}}\), respectively, the OT map is given by the solution to the Monge problem

\[\min_{T}\left\{\int c(y,T(y))d\mu_{Y}(y):T_{\sharp}\mu_{Y}=\mu_{Y^{\prime}} \right\}. \tag{8}\]

In practice, directly solving the Monge problem is hard and may not even admit a solution [80]. One common relaxation of Eq. (8) is to seek a joint distribution, known as a coupling or transport plan, which relates the underlying random variables [80]. A valid plan is a probability measure \(\gamma\) on \(\mathcal{Y}\times\mathcal{Y}^{\prime}\) with marginals \(\mu_{Y}\) and \(\mu_{Y^{\prime}}\). To efficiently estimate the plan when the \(c\) is the quadratic cost (i.e., \(c(y,y^{\prime})=\frac{1}{2}\|y-y^{\prime}\|^{2}\)), we solve the entropy regularized problem

\[\inf_{\gamma\in\Pi(\mu_{Y^{\prime}},\mu_{Y^{\prime}})}\int\frac{1}{2}\|y-y^{ \prime}\|^{2}d\gamma(y,y^{\prime})+\epsilon D_{\text{KL}}(\gamma||\mu_{Y^{ \prime}}\otimes\mu_{Y}), \tag{9}\]

where \(D_{\text{KL}}\) denotes the KL divergence, and \(\epsilon>0\) is a small regularization parameter, using the Sinkhorn's algorithm [21], which leverages the structure of the optimal plan to solve Eq. (9) with small runtime complexity [3]. The solution to Eq. (9) is the transport plan \(\gamma_{\epsilon}\in\Pi(\mu,\nu)\) given by

\[\gamma_{\epsilon}(y,y^{\prime})=\exp\left((f_{\epsilon}(y)+g_{\epsilon}(y^{ \prime})-\frac{1}{2}\|y-y^{\prime}\|^{2})/\epsilon\right)d\mu_{Y}(y)d\mu_{Y^{ \prime}}(y^{\prime}), \tag{10}\]

in terms of potential functions \(f_{\epsilon},g_{\epsilon}\) that are chosen to satisfy the marginal constraints. After finding these potentials, we can approximate the transport map using the barycentric projection \(T_{\gamma}(y)=\mathbb{E}_{\gamma}[Y^{\prime}|Y=y]\), for a plan \(\gamma\in\Pi(\mu_{Y},\mu_{Y^{\prime}})\)[2]. For the plan in Eq. (10), the map is given by

\[T_{\gamma_{\epsilon}}(y)=\frac{\int y^{\prime}e^{(g_{\epsilon}(y^{\prime})- \frac{1}{2}\|y-y^{\prime}\|^{2})/\epsilon}d\mu_{Y}(y^{\prime})}{\int e^{(g_{ \epsilon}(y^{\prime})-\frac{1}{2}\|y-y^{\prime}\|^{2})/\epsilon}d\mu_{Y}(y^{ \prime})}. \tag{11}\]

In this work, we estimate the potential functions \(f_{\epsilon},g_{\epsilon}\) from samples, i.e., empirical approximations of the measures \(\mu_{Y},\mu_{Y^{\prime}}\). Plugging in the estimated potentials in Eq. (11) defines an approximate transport map to push forward samples of \(\mu_{Y}\) to \(\mu_{Y^{\prime}}\). More details on the estimation of the OT map are provided in Appendix H.

A core advantage of this methodology is that it provides us with the flexibility of changing the cost function \(c\) in Eq. (8), and embed it with structural biases that one wishes to preserve in the push-forward distribution. Such direction is left for future work.

## 4 Numerical experiments

### Data and setup

We showcase the efficacy and performance of the proposed approach on one- and two-dimensional fluid flow problems that are representative of the core difficulties present in numerical simulations of weather and climate. We consider the one-dimensional Kuramoto-Sivashinski (KS) equation and the two-dimensional Navier-Stokes (NS) equation under Kolmogorov forcing (details in Appendix F) in periodic domains. The low-fidelity (LF), low-resolution (LR) data (\(\mathcal{Y}\) in Fig. 1(b)) is generated using a finite volume discretization in space [51] and a fractional discretization in time, while the high-fidelity(HF), high-resolution (HR) data (\(\mathcal{X}\) in Fig. 1(b)) is simulated using a spectral discretization in space with an implicit-explicit scheme in time. Both schemes are implemented with jax-cfd and its finite-volume and spectral toolboxes [28, 48] respectively. After generating the HF data in HR, we run the LF solver using a spatial discretization that is \(8\times\) coarser (in each dimension) with permissible time steps. For NS, we additionally create a \(16\times\) coarser LFLR dataset by further downsampling by a factor of two the \(8\times\) LFLR data. See Appendix F for further details.

For both systems, the datasets consist of long trajectories generated with random initial conditions2, which are sufficiently downsampled in time to ensure that consecutive samples are decorrelated. We stress once more that even when the grids and time stamps of both methods are aligned, there is _no pointwise correspondence_ between elements of \(\mathcal{X}\) and \(\mathcal{Y}\). This arises from the different modeling biases inherent to the LF and HF solvers, which inevitably disrupt any short-term correspondence over the long time horizon in a strongly nonlinear dynamical setting.

Footnote 2: The presence of global attractors in both systems renders the exact initial conditions unimportant. It also guarantees sufficient coverage of the target distributions sampling from long trajectories.

Finally, we create the intermediate space \(\mathcal{Y}^{\prime}\) in Fig. 1(b) by downsampling the HFHR data with a simple selection mask3 (i.e., the map \(C^{\prime}\)). This creates the new HFLR dataset \(\mathcal{Y}^{\prime}\) with the same resolution as \(\mathcal{Y}\), but with the low-frequency bias structure of \(\mathcal{X}\) induced by the push-forward of \(C^{\prime}\).

Footnote 3: It is worth noting that careful consideration should be given to the choice of \(C^{\prime}\) to avoid introducing aliasing, as this can potentially make the downscaling task more challenging.

**Baselines and definitions.** We define the following ablating variants of our proposed method

* Unconditional diffusion sampling (_UncondDfn_).
* Diffusion sampling conditioned on LFLR data without OT correction (_Raw cDfn_).
* [_Main_] Diffusion sampling conditioned on OT-corrected (HFLR) data (_OT+cDfn_).

We additionally consider the following baselines to benchmark our method:

* Cubic interpolation approximating HR target using local third-order splines (_Cubic_).
* Vision transformer (_ViT_) [27] based deterministic super-resolution model.
* Bias correction and statistical disaggregation (_BCSD_), involving upsampling with cubic interpolation, followed by a quantile-matching debiasing step.
* CycleGAN, which is adapted from [86] to enable learning transformations between spaces of different dimensions (_cycGAN_).
* ClimAlign (adapted from [32]), in which the input is upsampled using cubic interpolation, and the debiasing step is performed using AlignFlow [34] (_ClimAlign_).

The first two baselines require paired data and, therefore, learn the upsampling map \(\mathcal{Y}^{\prime}\rightarrow\mathcal{X}\) (i.e., HFLR to HFHR) and are composed with OT debiasing as factorized baselines. BCSD is a common approach used in the climate literature. The last two baselines present end-to-end alternatives and are trained directly on unpaired LFLR and HFHR samples. Further information about the implemented baselines can be found in Appendix D.

**OT training.** To learn the transport map in Eq. (11), we solve the entropic OT problem in Eq. (9) with \(\epsilon=0.001\) using a Sinkhorn [21] iteration with Anderson acceleration and parallel updates. We use \(90,000\) i.i.d. samples of \(Y\in\mathcal{Y}\) and \(Y^{\prime}\in\mathcal{Y}^{\prime}\), and perform \(5000\) iterations. Implementations are based on the ott-jax library [22].

**Denoiser training and conditional sampling.** The denoiser \(D_{\theta}\) is parametrized with a standard U-Net architecture similar to the one used in [67]. We additionally incorporate the preconditioning technique proposed in [45]. For \(s_{t}\) and \(\sigma_{t}\) schedules, we employ the variance-preserving (VP) scheme originally introduced in [73]. Furthermore, we adopt a data augmentation procedure to increase the effective training data size by taking advantage of the translation symmetries in the studied systems.

Samples are generated by solving the SDE based on the post-processed denoiser \(\tilde{D}_{\theta}\) using the Euler-Maruyama scheme with exponential time steps, i.e., \(\{t_{i}\}\) is set such that \(\sigma(t_{i})=\sigma_{\text{max}}(\sigma_{\text{min}}/\sigma_{\text{max}})^{i/N}\) for \(i=\{0,...,N\}\). The number of steps used, \(N\), vary between systems and downscaling factors. More details regarding denoiser training and sampling are included in Appendix B.

**Metrics.** To quantitatively assess the quality of the resulting snapshots we compare a number of physical and statistical properties of the snapshots: (i) the energy spectrum, which measures the energy in each Fourier mode and thereby providing insights into the similarity between the generated and reference samples, (ii) a spatial covariance metric, which characterizes the spatial correlations within the snapshots, (iii) the KL-divergence (KLD) of the kernel density estimation for each point, which serves as a measure for the local structures (iv) the maximum mean discrepancy (MMD), and (v) the empirical Wasserstein-1 metric (Wass1). We present (i) below and leave the rest described in Appendix C as they are commonly used in the context of probabilistic modeling.

The energy spectrum is defined4 as

Footnote 4: This definition is applied to each sample and averaged to obtain the metric (same for MELR below).

\[E(k)=\sum_{|\underline{k}|=k}|\hat{u}(\underline{k})|^{2}=\sum_{|\underline{k} |=k}\left|\sum_{i}u(x_{i})\exp(-j2\pi\underline{k}\cdot x_{i}/L)\right|^{2} \tag{12}\]

where \(u\) is a snapshot system state, and \(k\) is the magnitude of the wave-number (wave-vector in 2D) \(\underline{k}\). To assess the overall consistency of the spectrum between the generated and reference samples using a single scalar measure, we consider the mean energy log ratio (MELR):

\[\text{MELR}=\sum_{k}w_{k}\left|\log\left(E_{\text{pred}}(k)/E_{\text{ref}}(k) \right)\right|, \tag{13}\]

where \(w_{k}\) represents the weight assigned to each \(k\). We further define \(w_{k}^{\text{unweighted}}=1/\text{card}(k)\) and \(w_{k}^{\text{weighted}}=E_{\text{ref}}(k)/\sum_{k}E_{\text{ref}}(k)\). The latter skews more towards high-energy/low-frequency modes.

### Main results

**Effective debiasing via optimal transport.** Table 1 shows that the OT map effectively corrects the statistical biases in the LF snapshots for all three experiments considered. Significant improvements are observed across all metrics, demonstrating that the OT map approximately achieves \(C_{\sharp}^{\prime}\mu_{X}\approx T_{\sharp}\mu_{Y}\) as elaborated in SS3 (extra comparisons are included in Appendix H).

Indeed, the OT correction proves crucial for the success of our subsequent conditional sampling procedure: the unconditional diffusion samples may not have the correct energy spectrum (see

\begin{table}
\begin{tabular}{|l|c c|c c|c c|} \cline{2-7} \multicolumn{1}{c|}{} & \multicolumn{2}{c|}{KS \(8\times\)} & \multicolumn{2}{c|}{NS \(8\times\)} & \multicolumn{2}{c|}{NS \(16\times\)} \\ \hline
**Metric** & LFLR & OT-corrected & LFLR & OT-corrected & LFLR & OT-corrected \\ \hline covRMSE \(\downarrow\) & \(0.343\) & \(\mathbf{0.081}\) & \(0.458\) & \(\mathbf{0.083}\) & \(0.477\) & \(\mathbf{0.079}\) \\ MELRu \(\downarrow\) & \(0.201\) & \(\mathbf{0.020}\) & \(1.254\) & \(\mathbf{0.013}\) & \(0.600\) & \(\mathbf{0.016}\) \\ MELRw \(\downarrow\) & \(0.144\) & \(\mathbf{0.020}\) & \(0.196\) & \(\mathbf{0.026}\) & \(0.200\) & \(\mathbf{0.025}\) \\ KLD \(\downarrow\) & \(1.464\) & \(\mathbf{0.018}\) & \(29.30\) & \(\mathbf{0.033}\) & \(12.26\) & \(\mathbf{0.017}\) \\ \hline \end{tabular}
\end{table}
Table 1: Metrics of the LFLR source and OT-corrected samples for KS and NS. The precise metric definitions are provided in Appendix C.

Figure 2: (a) KS samples generated with diffusion model conditioned on LR information with and without OT correction applied, (b) empirical probability density function for relevant LR and HR samples in KS and (c) mode-wise log energy ratios with respect to the true samples (Eq. (13) without weighted sum) at \(8\times\) downscaling for NS.

_UncondDfn_ in Fig. 2(c), i.e. suffering from _color shifts_ - a known problem for score-based diffusion models [72, 15]. The conditioning on OT corrected data serves as a sparse anchor which draws the diffusion trajectories to the correct statistics at sampling time. In fact, when conditioned on uncorrected data, the bias effectively pollutes the statistics of the samples (_Raw cDfn_ in Table 2). Fig. 2(b) shows that the same pollution is present for the KS case, despite the unconditional sampler being unbiased.

In Appendix E, we present additional ablation studies that demonstrate the importance of OT correction in the factorized benchmarks.

**Comparison vs. factorized alternatives.** Fig. 3 displays NS samples generated by all benchmarked methods. Qualitatively, our method is able to provide highly realistic small-scale features. In comparison, we observe that _Cubic_ expectedly yields the lowest quality results; the deterministic _ViT_ produces samples with color shift and excessive smoothing, especially at \(16\times\) downscaling factor.

Quantitatively, our method outperforms all competitors in terms of MELR and KLD metrics in the NS tasks, while demonstrating consistently good performance in both \(8\times\) and \(16\times\) downscaling, despite the lack of recognizable features in the uncorrected LR data (Fig. 3(a) bottom) in the latter case. Other baselines, on the other hand, experience a significant performance drop. This showcases the value of having an unconditional prior to rely on when the conditioning provides limited information.

**Comparison vs. end-to-end downscaling.** Although the _cycGAN_ baseline is capable of generating high-quality samples at \(8\times\) downscaling (albeit with some smoothing) reflecting competitive metrics, we encountered persistent stability issues during training, particularly in the \(16\times\) downscaling case.

**Diffusion samples exhibit ample variability.** Due to the probabilistic nature of our approach, we can observe from Table 2 that the OT-conditioned diffusion model provides some variability in the downscaling task, which increases when the downscaling factor increases. This variability provides a measure of uncertainty quantification in the generated snapshots as a result of the consistent formulation of our approach on probability spaces.

## 5 Conclusion

We introduced a two-stage probabilistic framework for the statistical downscaling problem. The framework performs a debiasing step to correct the low-frequency statistics, followed by an upsampling step using a conditional diffusion model. We demonstrate that when applied to idealized physical fluids, our method provides high-resolution samples whose statistics are physically correct, even when there is a mismatch in the low-frequency energy spectra between the low- and high

Figure 3: Example showing the vorticity field of samples debiased and super-resolved using different techniques at \(8\times\) (top row) and \(16\times\) (bottom row) downscaling factors. From left to right: **(a)** LR snapshots produced by the **low-fidelity solver** (input \(\bar{y}\) of Alg. 1), **(b) OT-corrected** snapshots (\(\bar{y}^{\prime}\) in line \(1\) of Alg. 1), **(c) BCSD** applied to LR snapshots, **(d)** snapshots downscaled with **cycle-GAN** directly from LR snapshots, **(e) ClimAlign** applied to LR snapshots, **(f) cubic interpolation** of the OT-corrected snapshots, **(g)** deterministic upsample of the OT-corrected snapshots with **ViT**, **(h) diffusion sample conditioned on the OT-corrected snapshots** (output \(\bar{x}\) in Alg. 1, ours), and **(i)** two **true HR samples** in the training data with the closest Euclidean distance to the OT-corrected generated sample. The \(16\times\) source is the same as the \(8\times\) source but further downsampled by a factor of two. OT maps are computed independently between resolutions.

resolution data distributions. We have shown that our method is competitive and outperforms several commonly used alternative methods.

Future work will consider fine-tuning transport maps by adapting the map to the goal of conditional sampling, and introducing physically-motivated cost functions in the debiasing map. Moreover, we will address current limitations of the methodology, such as the high-computational complexity of learning OT-maps that scales quadratically with the size of the training set, and investigate the model's robustness to added noise in the collected samples as is found in weather and climate datasets. We will also further develop this methodology to cover other downscaling setups such as perfect prognosis [57] and spatio-temporal downscaling.

## Broader impact

Statistical downscaling is important to weather and climate modeling. In this work, we propose a new method for improving the accuracy of high-resolution forecasts (on which risk assessment would be made) from low resolution climate modeling. Weather and climate research and other scientific communities in computational fluid dynamics will benefit from this work for its potential to reduce computational costs. We do not believe this research will disadvantage anyone.

## Acknowledgments

The authors would like to sincerely thank Toby Bischoff, Katherine Deck, Nikola Kovachki, Andrew Stuart and Hongkai Zheng for many insightful and inspiring discussions that were vital to this work. RB gratefully acknowledges support from the Air Force Office of Scientific Research MURI on "Machine Learning and Physics-Based Modeling and Simulation" (award FA9550-20-1-0358), and a Department of Defense (DoD) Vannevar Bush Faculty Fellowship (award N00014-22-1-2790).

## References

* Abatzoglou and Brown [2012] J. T. Abatzoglou and T. J. Brown. A comparison of statistical downscaling methods suited for wildfire applications. _International Journal of Climatology_, 32(5):772-780, 2012. doi: [https://doi.org/10.1002/joc.2312](https://doi.org/10.1002/joc.2312). URL [https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/joc.2312](https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/joc.2312).

\begin{table}
\begin{tabular}{l c c c c c c c} \hline Model & Var & covRMSE\(\downarrow\) & MELRu\(\downarrow\) & MELRw\(\downarrow\) & KLD\(\downarrow\) & Wass1\(\downarrow\) & MMD\(\downarrow\) \\ \hline \multicolumn{8}{l}{**8\(\times\) downscale**} \\ \hline BCSD & 0 & 0.31 & 0.67 & 0.25 & 2.19 & **0.23** & 0.10 \\ \hline cycGAN & 0 & 0.15 & 0.08 & 0.05 & 1.62 & 0.32 & 0.08 \\ \hline ClimAlign & 0 & 2.19 & 0.64 & 0.45 & 64.37 & 2.77 & 0.53 \\ \hline Raw+cDfn & 0.27 & 0.46 & 0.79 & 0.37 & 73.16 & 1.04 & 0.42 \\ \hline OT+Cubic & 0 & **0.12** & 0.52 & 0.06 & 1.46 & 0.42 & 0.10 \\ \hline OT+ViT & 0 & 0.43 & 0.38 & 0.18 & 1.72 & 1.11 & 0.31 \\ \hline (ours) OT+cDfn & 0.36 & **0.12** & **0.06** & **0.02** & **1.40** & 0.26 & **0.07** \\ \hline \multicolumn{8}{l}{**16\(\times\) downscale**} \\ \hline BCSD & 0 & 0.34 & 0.67 & 0.25 & 2.17 & **0.21** & 0.11 \\ \hline cycGAN & 0 & 0.32 & 1.14 & 0.28 & 2.05 & 0.48 & 0.13 \\ \hline ClimAlign & 0 & 2.53 & 0.81 & 0.50 & 77.51 & 3.15 & 0.55 \\ \hline Raw+cDfn & 1.07 & 0.46 & 0.54 & 0.30 & 93.87 & 0.99 & 0.39 \\ \hline OT+Cubic & 0 & 0.25 & 0.55 & 0.13 & 7.30 & 0.85 & 0.20 \\ \hline OT+ViT & 0 & 0.14 & 1.38 & 0.09 & 1.67 & 0.32 & **0.07** \\ \hline (ours) OT+cDfn & 1.56 & **0.12** & **0.05** & **0.02** & **0.83** & 0.29 & **0.07** \\ \hline \end{tabular}
\end{table}
Table 2: Evaluation of downscaling methods for NS. The best metric values are highlighted **in bold**. Precise metric definitions (except MELR, given by Eq. (13)) are included in Appendix C.

* Agueh and Carlier [2011] M. Agueh and G. Carlier. Barycenters in the wasserstein space. _SIAM Journal on Mathematical Analysis_, 43(2):904-924, 2011. doi: 10.1137/100805741. URL [https://doi.org/10.1137/100805741](https://doi.org/10.1137/100805741).
* Altschuler et al. [2017] J. Altschuler, J. Niles-Weed, and P. Rigollet. Near-linear time approximation algorithms for optimal transport via sinkhorn iteration. _Advances in neural information processing systems_, 30, 2017.
* Balaji et al. [2022] V. Balaji, F. Couvreux, J. Deshayes, J. Gautrais, F. Hourdin, and C. Rio. Are general circulation models obsolete? _Proceedings of the National Academy of Sciences_, 119(47):e2202075119, 2022. doi: 10.1073/pnas.2202075119. URL [https://www.pnas.org/doi/abs/10.1073/pnas.2202075119](https://www.pnas.org/doi/abs/10.1073/pnas.2202075119).
* Bischoff and Deck [2023] T. Bischoff and K. Deck. Unpaired downscaling of fluid flows with diffusion bridges. _arXiv preprint arXiv:2305.01822_, 2023.
* Boffetta and Ecke [2012] G. Boffetta and R. E. Ecke. Two-Dimensional Turbulence. _Annual Review of Fluid Mechanics_, 44, January 2012. ISSN 0066-4189, 1545-4479. doi: 10.1146/annurev-fluid-120710-101240.
* Bortoli et al. [2021] V. D. Bortoli, J. Thornton, J. Heng, and A. Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=9BnCuiXB0ty](https://openreview.net/forum?id=9BnCuiXB0ty).
* Brenier [1991] Y. Brenier. Polar factorization and monotone rearrangement of vector-valued functions. _Communications on pure and applied mathematics_, 44(4):375-417, 1991.
* Candes and Fernandez-Granda [2013] E. J. Candes and C. Fernandez-Granda. Super-resolution from noisy data. _Journal of Fourier Analysis and Applications_, 19(6):1229-1254, Aug. 2013. doi: 10.1007/s00041-013-9292-3. URL [https://doi.org/10.1007/s00041-013-9292-3](https://doi.org/10.1007/s00041-013-9292-3).
* Candes and Fernandez-Granda [2014] E. J. Candes and C. Fernandez-Granda. Towards a mathematical theory of super-resolution. _Communications on Pure and Applied Mathematics_, 67(6):906-956, 2014. doi: [https://doi.org/10.1002/cpa.21455](https://doi.org/10.1002/cpa.21455). URL [https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21455](https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21455).
* Canuto et al. [2007] C. G. Canuto, M. Y. Hussaini, A. M. Quarteroni, and T. A. Zang. _Spectral Methods: Evolution to Complex Geometries and Applications to Fluid Dynamics (Scientific Computation)_. Springer-Verlag, Berlin, Heidelberg, 2007. ISBN 3540307273.
* Charalampopoulos et al. [2023] A.-T. Charalampopoulos, S. Zhang, B. Harrop, L.-y. R. Leung, and T. Sapsis. Statistics of extreme events in coarse-scale climate simulations via machine learning correction operators trained on nudged datasets. _arXiv preprint arXiv:2304.02117_, 2023.
* Cheng et al. [2020] J. Cheng, Q. Kuang, C. Shen, J. Liu, X. Tan, and W. Liu. Reslap: Generating high-resolution climate prediction through image super-resolution. _IEEE Access_, 8:39623-39634, 2020. doi: 10.1109/ACCESS.2020.2974785.
* Choi et al. [2021] J. Choi, S. Kim, Y. Jeong, Y. Gwon, and S. Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. _arXiv preprint arXiv:2108.02938_, 2021.
* Choi et al. [2022] J. Choi, J. Lee, C. Shin, S. Kim, H. Kim, and S. Yoon. Perception prioritized training of diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11472-11481, 2022.
* Christensen et al. [2008] J. H. Christensen, F. Boberg, O. B. Christensen, and P. Lucas-Picher. On the need for bias correction of regional climate change projections of temperature and precipitation. _Geophysical Research Letters_, 35(20), 2008. doi: [https://doi.org/10.1029/2008GL035694](https://doi.org/10.1029/2008GL035694). URL [https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2008GL035694](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2008GL035694).
* Chung et al. [2022] H. Chung, J. Kim, M. T. Mccann, M. L. Klasky, and J. C. Ye. Diffusion posterior sampling for general noisy inverse problems. _arXiv preprint arXiv:2209.14687_, 2022.
* Chung et al. [2022] H. Chung, B. Sim, D. Ryu, and J. C. Ye. Improving diffusion models for inverse problems using manifold constraints. _arXiv preprint arXiv:2206.00941_, 2022.
* Cooley and Tukey [1965] J. W. Cooley and J. W. Tukey. An algorithm for the machine calculation of complex Fourier series. _Math. Comput._, 19(90):297-301, 1965. ISSN 00255718, 10886842. URL [http://www.jstor.org/stable/2003354](http://www.jstor.org/stable/2003354).

* Courty et al. [2017] N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. _Advances in neural information processing systems_, 30, 2017.
* Cuturi [2013] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in neural information processing systems_, 26, 2013.
* Cuturi et al. [2022] M. Cuturi, L. Meng-Papaxanthos, Y. Tian, C. Bunne, G. Davis, and O. Teboul. Optimal transport tools (ott): A jax toolbox for all things wasserstein. _arXiv preprint arXiv:2201.12324_, 2022.
* Danabasoglu et al. [2019] G. Danabasoglu, J.-F. Lamarque, J. Bacmeister, D. A. Bailey, A. K. DuVivier, J. Edwards, L. K. Emmons, J. Fasullo, R. Garcia, A. Gettelman, C. Hannay, M. M. Holland, W. G. Large, P. H. Lauritzen, D. M. Lawrence, J. T. M. Lenaerts, K. Lindsay, W. H. Lipscomb, M. J. Mills, R. Neale, K. W. Oleson, B. Otto-Bliesner, A. S. Phillips, W. Sacks, S. Tilmes, L. van Kampenhout, M. Verstrestein, A. Bertini, J. Dennis, C. Deser, C. Fischer, B. Fox-Kemper, J. E. Kay, D. Kinnison, P. J. Kusner, V. E. Larson, M. C. Long, S. Mickelson, J. K. Moore, E. Nienhouse, L. Polvani, P. J. Rasch, and W. G. Strand. The community earth system model version 2 (cesm2). _Journal of Advances in Modeling Earth Systems_, 12(2):e2019MS001916, 2020. doi: [https://doi.org/10.1029/2019MS001916](https://doi.org/10.1029/2019MS001916). URL [https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001916](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019MS001916). e2019MS001916 2019MS001916.
* Dhariwal and Nichol [2021] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* Dixon et al. [2016] K. W. Dixon, J. R. Lanzante, M. J. Nath, K. Hayhoe, A. Stoner, A. Radhakrishnan, V. Balaji, and C. F. Gaitan. Evaluating the stationarity assumption in statistically downscaled climate projections: is past performance an indicator of future results? _Climatic Change_, 135(3-4):395-408, 2016.
* ECCV 2014_, pages 184-199, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10593-2.
* Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL [https://openreview.net/forum?id=YicbFdNTTY](https://openreview.net/forum?id=YicbFdNTTY).
* Dresdner et al. [2022] G. Dresdner, D. Kochkov, P. Norgaard, L. Zepeda-Nunez, J. A. Smith, M. P. Brenner, and S. Hoyer. Learning to correct spectral methods for simulating turbulent flows, 2022.
* Efron [2011] B. Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* Finzi et al. [2023] M. A. Finzi, A. Boral, A. G. Wilson, F. Sha, and L. Zepeda-Nunez. User-defined event sampling and uncertainty quantification in diffusion models for physical dynamical systems. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 10136-10152. PMLR, 23-29 Jul 2023. URL [https://proceedings.mlr.press/v202/finzi23a.html](https://proceedings.mlr.press/v202/finzi23a.html).
* Flamary et al. [2016] R. Flamary, N. Courty, D. Tuia, and A. Rakotomamonjy. Optimal transport for domain adaptation. _IEEE Trans. Pattern Anal. Mach. Intell_, 1, 2016.
* Groenke et al. [2021] B. Groenke, L. Madaus, and C. Monteleoni. Climalign: Unsupervised statistical downscaling of climate variables via normalizing flows. In _Proceedings of the 10th International Conference on Climate Informatics_, CI2020, page 60-66, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450388481. doi: 10.1145/3429309.3429318. URL [https://doi.org/10.1145/3429309.3429318](https://doi.org/10.1145/3429309.3429318).
* 303, 1991. doi: [https://doi.org/10.1175/1520-0442](https://doi.org/10.1175/1520-0442)(1991)004<0286:TUOGGCM>2.0.CO;2. URL [https://journals.ametsoc.org/view/journals/clim/4/3/1520-0442_1991_004_0286_tuogcm_2_0_co_2.xml](https://journals.ametsoc.org/view/journals/clim/4/3/1520-0442_1991_004_0286_tuogcm_2_0_co_2.xml).
* Grover et al. [2019] A. Grover, C. Chute, R. Shu, Z. Cao, and S. Ermon. Alignflow: Cycle consistent learning from multiple domains via normalizing flows, 2019. URL [https://openreview.net/forum?id=S11NELLKuN](https://openreview.net/forum?id=S11NELLKuN).

* Gutmann et al. [2014] E. Gutmann, T. Pruitt, M. P. Clark, L. Brekke, J. R. Arnold, D. A. Raff, and R. M. Rasmussen. An intercomparison of statistical downscaling methods used for water resource assessments in the united states. _Water Resources Research_, 50(9):7167-7186, 2014. doi: [https://doi.org/10.1002/2014WR015559](https://doi.org/10.1002/2014WR015559). URL [https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014WR015559](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014WR015559).
* Hall [2014] A. Hall. Projecting regional change. _Science_, 346(6216):1461-1462, 2014. doi: 10.1126/science.aaa0629. URL [https://www.science.org/doi/abs/10.1126/science.aaa0629](https://www.science.org/doi/abs/10.1126/science.aaa0629).
* Hammoud et al. [2022] M. A. E. R. Hammoud, E. S. Titi, I. Hoteit, and O. Knio. Cdanet: A physics-informed deep neural network for downscaling fluid flows. _Journal of Advances in Modeling Earth Systems_, 14(12):e2022MS003051, 2022.
* Harder et al. [2022] P. Harder, Q. Yang, V. Ramesh, P. Sattigeri, A. Hernandez-Garcia, C. Watson, D. Szwarcman, and D. Rolnick. Generating physically-consistent high-resolution climate data with hard-constrained neural networks. _arXiv preprint arXiv:2208.05424_, 2022.
* Harris et al. [2022] L. Harris, A. T. McRae, M. Chantry, P. D. Dueben, and T. N. Palmer. A generative deep learning approach to stochastic downscaling of precipitation forecasts. _Journal of Advances in Modeling Earth Systems_, 14(10):e2022MS003120, 2022.
* Hersbach et al. [2020] H. Hersbach, B. Bell, P. Berrisford, S. Hirahara, A. Horanyi, J. Munoz-Sabater, J. Nicolas, C. Peubey, R. Radu, D. Schepers, A. Simmons, C. Soci, S. Abdalla, X. Abellan, G. Balsamo, P. Bechtold, G. Biavati, J. Bidlot, M. Bonavita, G. De Chiara, P. Dahlgren, D. Dee, M. Diamantakis, R. Dragani, J. Flemming, R. Forbes, M. Fuentes, A. Geer, L. Haimberger, S. Healy, R. J. Hogan, E. Holm, M. Janiskova, S. Keeley, P. Laloyaux, P. Lopez, C. Lupu, G. Radnoti, P. de Rosnay, I. Rozum, F. Vamborg, S. Villaume, and J.-N. Thepaut. The era5 global reanalysis. _Quarterly Journal of the Royal Meteorological Society_, 146(730):1999-2049, 2020. doi: [https://doi.org/10.1002/qj.3803](https://doi.org/10.1002/qj.3803). URL [https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3803](https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3803).
* Hessami et al. [2008] M. Hessami, P. Gachon, T. B. Ouarda, and A. St-Hilaire. Automated regression-based statistical downscaling tool. _Environmental Modelling and Software_, 23(6):813-834, 2008. ISSN 1364-8152. doi: [https://doi.org/10.1016/j.envsoft.2007.10.004](https://doi.org/10.1016/j.envsoft.2007.10.004). URL [https://www.sciencedirect.com/science/article/pii/S1364815207001867](https://www.sciencedirect.com/science/article/pii/S1364815207001867).
* Hua and Sarkar [1991] Y. Hua and T. K. Sarkar. On SVD for estimating generalized eigenvalues of singular matrix pencil in noise. _IEEE Trans. Sig. Proc._, 39(4):892-900, April 1991. ISSN 1941-0476. doi: 10.1109/78.80911.
* Huang et al. [2020] X. Huang, D. L. Swain, and A. D. Hall. Future precipitation increase from very high resolution ensemble downscaling of extreme atmospheric river storms in california. _Science Advances_, 6(29):eaba1323, 2020. doi: 10.1126/sciadv.aba1323. URL [https://www.science.org/doi/abs/10.1126/sciadv.aba1323](https://www.science.org/doi/abs/10.1126/sciadv.aba1323).
* Hwang and Graham [2014] S. Hwang and W. D. Graham. Assessment of alternative methods for statistically downscaling daily gcm precipitation outputs to simulate regional streamflow. _JAWRA Journal of the American Water Resources Association_, 50(4):1010-1032, 2014. doi: [https://doi.org/10.1111/jawr.12154](https://doi.org/10.1111/jawr.12154). URL [https://onlinelibrary.wiley.com/doi/abs/10.1111/jawr.12154](https://onlinelibrary.wiley.com/doi/abs/10.1111/jawr.12154).
* Karras et al. [2022] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative models. _arXiv preprint arXiv:2206.00364_, 2022.
* Kawar et al. [2021] B. Kawar, G. Vaksman, and M. Elad. SNIPS: Solving noisy inverse problems stochastically. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=pBK0X_dxYAN](https://openreview.net/forum?id=pBK0X_dxYAN).
* Kawar et al. [2022] B. Kawar, M. Elad, S. Ermon, and J. Song. Denoising diffusion restoration models. In _ICLR Workshop on Deep Generative Models for Highly Structured Data_, 2022. URL [https://openreview.net/forum?id=BExXihVOvWq](https://openreview.net/forum?id=BExXihVOvWq).
* Kochkov et al. [2021] D. Kochkov, J. A. Smith, A. Alieva, Q. Wang, M. P. Brenner, and S. Hoyer. Machine learning-accelerated computational fluid dynamics. _Proc. Natl. Acad. Sci. U. S. A._, 118(21), May 2021. URL [https://www.pnas.org/content/118/21/e2101784118](https://www.pnas.org/content/118/21/e2101784118).
* Kolmogorov [1962] A. N. Kolmogorov. A refinement of previous hypotheses concerning the local structure of turbulence in a viscous incompressible fluid at high reynolds number. _Journal of Fluid Mechanics_, 13(1):82-85, 1962.

* Korotin et al. [2019] A. Korotin, V. Egiazarian, A. Asadulaev, A. Safin, and E. Burnaev. Wasserstein-2 generative networks. _arXiv preprint arXiv:1909.13082_, 2019.
* LeVeque [2002] R. J. LeVeque. _Finite Volume Methods for Hyperbolic Problems_. Cambridge Texts in Applied Mathematics. Cambridge University Press, 2002. doi: 10.1017/CBO9780511791253.
* Li et al. [2022] H. Li, Y. Yang, M. Chang, S. Chen, H. Feng, Z. Xu, Q. Li, and Y. Chen. SRdiff: Single image super-resolution with diffusion probabilistic models. _Neurocomputing_, 479:47-59, 2022. ISSN 0925-2312. doi: [https://doi.org/10.1016/j.neucom.2022.01.029](https://doi.org/10.1016/j.neucom.2022.01.029). URL [https://www.sciencedirect.com/science/article/pii/S0925231222000522](https://www.sciencedirect.com/science/article/pii/S0925231222000522).
* Lorenz [2005] E. N. Lorenz. Designing chaotic models. _Journal of the atmospheric sciences_, 62(5):1574-1587, 2005.
* Lugmayr et al. [2020] A. Lugmayr, M. Danelljan, L. Van Gool, and R. Timofte. SRFlow: Learning the super-resolution space with normalizing flow. In _ECCV_, 2020.
* Makkuva et al. [2020] A. Makkuva, A. Taghvaei, S. Oh, and J. Lee. Optimal transport mapping via input convex neural networks. In _International Conference on Machine Learning_, pages 6672-6681. PMLR, 2020.
* 2143, 2013. doi: [https://doi.org/10.1175/JCLI-D-12-00821.1](https://doi.org/10.1175/JCLI-D-12-00821.1). URL [https://journals.ametsoc.org/view/journals/clim/26/6/jcli-d-12-00821.1.xml](https://journals.ametsoc.org/view/journals/clim/26/6/jcli-d-12-00821.1.xml).
* Maraun and Widmann [2018] D. Maraun and M. Widmann. _Perfect Prognosis_, page 141-169. Cambridge University Press, 2018. doi: 10.1017/9781107588783.012.
* Meng et al. [2021] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* Naveena et al. [2022] N. Naveena, G. C. Satyanarayana, N. Umakanth, M. C. Rao, B. Avinash, J. Jaswanth, and M. S. S. Reddy. Statistical downscaling in maximum temperature future climatology. _AIP Conference Proceedings_, 2357(1), 05 2022. ISSN 0094-243X. doi: 10.1063/5.0081087. URL [https://doi.org/10.1063/5.0081087.030026](https://doi.org/10.1063/5.0081087.030026).
* Pan et al. [2021] B. Pan, G. J. Anderson, A. Goncalves, D. D. Lucas, C. J. Bonfils, J. Lee, Y. Tian, and H.-Y. Ma. Learning to correct climate projection biases. _Journal of Advances in Modeling Earth Systems_, 13(10):e2021MS002509, 2021.
* Papadakis [2015] N. Papadakis. _Optimal transport for image processing_. PhD thesis, Universite de Bordeaux, 2015.
* Park et al. [2020] T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu. Contrastive learning for unpaired image-to-image translation. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part IX 16_, pages 319-345. Springer, 2020.
* Perrot et al. [2016] M. Perrot, N. Courty, R. Flamary, and A. Habrard. Mapping estimation for discrete optimal transport. _Advances in Neural Information Processing Systems_, 29, 2016.
* Peyre et al. [2019] G. Peyre, M. Cuturi, et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* Pooladian and Niles-Weed [2021] A.-A. Pooladian and J. Niles-Weed. Entropic estimation of optimal transport maps. _arXiv preprint arXiv:2109.12004_, 2021.
* Price and Rasp [2022] I. Price and S. Rasp. Increasing the accuracy and resolution of precipitation forecasts using deep generative models. In _International conference on artificial intelligence and statistics_, pages 10555-10571. PMLR, 2022.
* Saharia et al. [2022] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. _Advances in Neural Information Processing Systems_, 35:36479-36494, 2022.
* Sasaki et al. [2021] H. Sasaki, C. G. Willcocks, and T. P. Breckon. Unit-ddpm: Unpaired image translation with denoising diffusion probabilistic models. _arXiv preprint arXiv:2104.05358_, 2021.
* Schneider et al. [2017] T. Schneider, J. Teixeira, C. S. Bretherton, F. Brient, K. G. Pressel, C. Schar, and A. P. Siebesma. Climate goals and computing the future of clouds. _Nature Climate Change_, 7(1):3-5, 2017.

* Scott [2015] D. W. Scott. _Multivariate density estimation: theory, practice, and visualization_. John Wiley & Sons, 2015.
* Shu et al. [2023] D. Shu, Z. Li, and A. B. Farimani. A physics-informed diffusion model for high-fidelity flow field reconstruction. _Journal of Computational Physics_, 478:111972, 2023.
* Song and Ermon [2020] Y. Song and S. Ermon. Improved techniques for training score-based generative models. _Advances in neural information processing systems_, 33:12438-12448, 2020.
* Song et al. [2020] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2020.
* Su et al. [2023] X. Su, J. Song, C. Meng, and S. Ermon. Dual diffusion implicit bridges for image-to-image translation. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=5HLoTVGDe](https://openreview.net/forum?id=5HLoTVGDe).
* Tian et al. [2022] C. Tian, X. Zhang, J. C.-W. Lin, W. Zuo, and Y. Zhang. Generative adversarial networks for image super-resolution: A survey. _arXiv preprint arXiv:2204.13620_, 2022.
* Trefethen [2000] L. N. Trefethen. _Spectral Methods in MATLAB_. Society for industrial and applied mathematics (SIAM), 2000.
* Trigila and Tabak [2016] G. Trigila and E. G. Tabak. Data-driven optimal transport. _Communications on Pure and Applied Mathematics_, 69(4):613-648, 2016.
* Vandal et al. [2017] T. Vandal, E. Kodra, S. Ganguly, A. Michaelis, R. Nemani, and A. R. Ganguly. Deepsd: Generating high resolution climate change projections through single image super-resolution. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '17, page 1663-1672, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983.3098004. URL [https://doi.org/10.1145/3097983.3098004](https://doi.org/10.1145/3097983.3098004).
* Vandal et al. [2019] T. Vandal, E. Kodra, and A. R. Ganguly. Intercomparison of machine learning methods for statistical downscaling: The case of daily and extreme precipitation. _Theor. Appl. Climatol._, 137:557---570, 2019.
* Villani [2009] C. Villani. _Optimal transport: old and new_. Springer Berlin Heidelberg, 2009.
* Wilby et al. [1998] R. L. Wilby, T. M. L. Wigley, D. Conway, P. D. Jones, B. C. Hewitson, J. Main, and D. S. Wilks. Statistical downscaling of general circulation model output: A comparison of methods. _Water Resources Research_, 34(11):2995-3008, 1998. doi: [https://doi.org/10.1029/98WR02577](https://doi.org/10.1029/98WR02577). URL [https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/98WR02577](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/98WR02577).
* Wilby et al. [2006] R. L. Wilby, P. Whitehead, A. Wade, D. Butterfield, R. Davis, and G. Watts. Integrated modelling of climate change impacts on water resources and quality in a lowland catchment: River kennet, uk. _J. Hydrol._, 330(1-2):204-220, 2006.
* Wu and De la Torre [2022] C. H. Wu and F. De la Torre. Unifying diffusion models' latent space, with applications to cyclediffusion and guidance. _arXiv preprint arXiv:2210.05559_, 2022.
* Zelinka et al. [2020] M. D. Zelinka, T. A. Myers, D. T. McCoy, S. Po-Chedley, P. M. Caldwell, P. Ceppi, S. A. Klein, and K. E. Taylor. Causes of higher climate sensitivity in cmip6 models. _Geophysical Research Letters_, 47(1):e2019GL085782, 2020. doi: [https://doi.org/10.1029/2019GL085782](https://doi.org/10.1029/2019GL085782). URL [https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019GL085782](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019GL085782). e2019GL085782.
* Zhao et al. [2022] M. Zhao, F. Bao, C. Li, and J. Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. _arXiv preprint arXiv:2207.06635_, 2022.
* Zhu et al. [2017] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _2017 IEEE International Conference on Computer Vision (ICCV)_, pages 2242-2251, 2017. doi: 10.1109/ICCV.2017.244.