# Fine-Grained Human Feedback Gives Better Rewards for Language Model Training

 Zeqiu Wu\({}^{1}\)1  Yushi Hu\({}^{1}\)1  Weijia Shi\({}^{1}\)  Nouha Dziri\({}^{2}\)  Alane Suhr\({}^{3}\)

**Prithviraj Ammanabrolu\({}^{45}\)  Noah A. Smith\({}^{1}\)2  Mari Ostendorf\({}^{1}\)  Hannaneh Hajishirzi\({}^{1}\)\({}^{2}\)**

\({}^{1}\)University of Washington \({}^{2}\)Allen Institute for Artificial Intelligence

\({}^{3}\)University of California, Berkeley \({}^{4}\)University of California, San Diego \({}^{5}\)MosaicML

Equal contribution. Correspondence to <Zeqiu Wu: zeqiuw1@uw.edu>, <Yushi Hu: yushihu@uw.edu>

Footnote 1: footnotemark:

###### Abstract

Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. Reinforcement learning from human feedback (RLHF)--where human preference judgments on LM outputs are transformed into a learning signal--has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce **Fine-Grained RLHF**, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with such reward functions leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.

## 1 Introduction

State-of-the-art AI is built on pre-trained language models that are then trained through interaction with humans [29, 28, 9], with a combination of supervised learning and reinforcement learning. Incorporating human feedback into the process of language model (LM) training has been shown as effective to reduce false, toxic and other undesired model generation outputs [29, 3, 2, 33, 10]. Many of these studies adopt reinforcement learning from human feedback (RLHF) [46], a framework that converts human feedback into an effective LM training signal to reach these goals. Specifically, humans are presented with two or more outputs and asked to select one or rank them, and this signal is then used to train a reward model, which computes a single scalar reward for each LM-generated sequence. The LM is then trained with RL to optimize the reward it receives (from the reward model).

Such a reward provides a relatively sparse training signal, especially for tasks that require the generation of long-form text--making RLHF in such domains unreliable [33]. Furthermore, previous research [8, 18, 11, 45, 25, 43] into automated evaluation of generated text shows that it can be challenging for human annotators to reliably compare the overall quality of two or more model outputs when the outputs contain a mixture of diverse undesired behaviors. They demonstrate how categorizing and localizing model errors (i.e., fine-grained evaluation) provides explicit insights about which part of the model output has what type of problem. We thus ask the question: how can we improve rewards for LM training via RLHF by using more _fine-grained human feedback_?In this paper, we propose that humans give fine-grained feedback to LM output, associating _categories_ of undesired behavior (e.g., false or irrelevant generations) and a text span at a _density_ (e.g., sentence or sub-sentence-level). To enable LMs to learn from such fine-grained feedback, we introduce the Fine-Grained Rlhf framework. As shown in Figure 1, we first use collected human feedback to train fine-grained reward models such that each of them focuses on one _category_ and provides rewards at the _density_ associated with that category. We then integrate these reward models into Proximal Policy Optimization (PPO) [37], a commonly used RL algorithm for training LMs with preference-based human feedback (SS2).

We conduct experiments on two language generation tasks--detoxification [12] (SS3) and long-form question answering (QA) [39] (SS4). For detoxification, toxicity is the only error category and we explore learning with a dense reward. We adopt Perspective[1], a widely used language toxicity detection model trained on millions of human annotations, as our reward model. We use it to calculate a fine-grained reward after the generation of every sentence. Our experimental results show the efficacy and data efficiency of training models with dense reward compared to a holistic sequence-level reward, supported by automatic evaluation results.

With experiments on long-form QA, we aim to examine training models with fine-grained rewards at the two granularity dimensions (density and error category), for which we construct a long-form QA dataset, QA-Feedback, along with our collected human feedback. We carefully develop a pipeline to collect fine-grained human feedback on three error categories at different density levels: i) irrelevance, repetition, or incoherence (sub-sentence), ii) incorrect or unverifiable facts (sentence), and iii) incomplete information (whole sequence; see Figure 1). Our experimental results show improved results in each error category by learning with such fine-grained feedback, supported by both automatic and human evaluation results. In a scenario with multiple reward models representing different error types, we also show Fine-Grained Rlhf allows us to combine reward models with different weights and thus control the model training process towards a customized combination of desired behaviors.

Figure 1: Comparison of **(a) RL with human preference** and **(b) our Fine-Grained Rlhf** on long-form QA. Different from (a), which collects human preferences on the overall quality of LM outputs, we ask annotators to mark which part of an output contains what type(s) of errors. We train a fine-grained reward model for each type of error and optimize LM against these reward models. In this example, we provide a relevance reward and a factuality reward after each sentence is generated. There is also a holistic information completeness reward after the whole text is generated.

Fine-Grained RLHF

We introduce Fine-Grained RLHF, a framework that enables us to train fine-grained reward functions for generation outputs across different feedback types. We first define the RL environment and learning algorithm. Then we define the fine-grained reward models and describe how to incorporate the fine-grained reward model(s) into an RL algorithm, in contrast to previous RLHF studies that only consider a single reward.

**Environment: language generation as a MDP.** We focus on language generation tasks. For each task, we are given a set of task input prompts \(D=\{x^{n}\}_{n=1}^{N}\). We follow [33] to define language generation as a Markov Decision Process (MDP) \(\langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,T_{max}\rangle\) with a finite vocabulary \(\mathcal{V}\). Each MDP episode starts with a sampled prompt \(x=(x_{1},x_{2},\ldots,x_{l})\) with \(x_{i}\in\mathcal{V}\), and ends when the current time step exceeds \(T_{max}\) or an end of sequence token is generated. \(\mathcal{S}\) is the state space and \(s_{0}=(x_{1},x_{2},\ldots,x_{l})\in\mathcal{S}\) is the initial state. An action in the environment \(a_{t}\in\mathcal{A}\) is a generated token (by the policy LM model \(P_{\theta}\)) at time \(t\) from \(\mathcal{V}\) (\(a_{0}\) is the begin sequence token). The transition function \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta\mathcal{S}\) appends \(a_{t}\) at the end of the state \(s_{t}=(x_{1},x_{2},\ldots,x_{l},a_{0},a_{1},\ldots,a_{t-1})\). This process continues until the end time step \(T\leq T_{max}\) is reached, which gives a generated sequence \(y=(a_{1},\ldots,a_{T})\). A reward function \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), which comes from the reward model(s) in Fine-Grained RLHF, provides dense rewards before and when \(T\) is reached. \(P_{\theta}\) can be initialized with a pre-trained language model, and sometimes also with supervised fine-tuning on task-specific demonstrations. The reward function is defined later.

**Learning algorithm: proximal policy optimization (PPO).** PPO [37] is an actor-critic RL algorithm that is widely used in previous RLHF work to optimize the policy model against a reward model of human feedback. It uses a value model \(V_{\psi}(s_{t})\) to estimate the value of state \(s_{t}\), and optimizes the policy model with a PPO clipped surrogate training objective. The advantage \(A_{t}\) at timestep \(t\) is estimated by a generalized advantage estimation function [36]: \(A_{t}=\sum_{t^{\prime}=t}^{T}(\gamma\lambda)^{t^{\prime}-t}(r_{t^{\prime}}+ \gamma V_{\psi}(s_{t^{\prime}+1})-V_{\psi}(s_{t^{\prime}}))\), with \(\gamma\) as a hyperparameter and \(\lambda\) as the discounting factor for rewards. \(r_{t}\) is the reward assigned to \(a_{t}\), which in our case is acquired using one or multiple learned reward models. The value model \(V_{\psi}(s_{t})\) is optimized with an expected squared-error loss with the value target as \(V^{\text{targ}}(s_{t})=\sum_{t^{\prime}=t}^{T-1}\gamma^{t^{\prime}-t}r_{t^{ \prime}}+\gamma^{T-t}V_{\psi_{\text{old}}}(s_{T})\), where \(V_{\psi_{\text{old}}}\) is the lagging value model. Finally, PPO is trained to optimize both policy (\(P_{\theta}\)) and value (\(V_{\psi}\)) models with their respective objectives. No reward model is being optimized during PPO training. See Appendix B for more details.

**Fine-grained reward models.** Previous RLHF work adopts a holistic reward model \(R_{\phi}\) that maps input prompt \(x\) and generated output \(y\) to a single scalar reward representing its overall quality (Figure 1(a)). This single scalar reward is only assigned to the final token in the generated sequence, \(a_{T}\). Formally, \(r_{t}=R_{\phi}(x,y)\) if \(t=T\) and 0 otherwise.

In contrast, we consider a reward function that is derived from one or multiple _fine-grained_ reward models that (1) provide rewards densely (i.e., for subsequences of the generated output), and (2) compute rewards on distinct categories of undesired behaviors (e.g., false or repetitive generation), where each category is associated with an individual reward model.

For a fine-grained reward model \(R_{\phi_{k}}\) that gives feedback on error category \(C_{k}\), we first segment \(y\) into \(L_{k}\) segments \((y^{k}_{1},y^{k}_{2},\ldots,y^{k}_{L_{k}})\) corresponding to the density (e.g., sentence-level) of \(R_{\phi_{k}}\), where each segment \(y^{k}_{j}\) ends at timestep \(T^{k}_{j}\). \(R_{\phi_{k}}\) outputs a reward \(R_{\phi_{k}}(x,y,j)\) for each segment \(y^{k}_{j}\) given \(x\) and \(y\) as the input, which is assigned to the final token in \(y^{k}_{j}\). Additionally, to ensure the fluency of generated outputs, we follow [41] to add an approximate KL divergence penalty to each token \(a_{t}\) with a weight \(\beta\), that is not backpropagated through during training. Formally, assuming that we have \(K\) fine-grained reward models that represent different error categories, we will have a combined reward function for each token \(a_{t}\) as:

\[r_{t}=\sum_{k=1}^{K}\sum_{j=1}^{L_{k}}\left(\mathbbm{1}(t=T^{k}_{j})\,w_{k}\,R_ {\phi_{k}}(x,y,j)\right)-\beta\log\frac{P_{\theta}(a_{t}\mid s_{t})}{P_{\theta_ {\text{old}}}(a_{t}\mid s_{t})}\] (1)

where \(w_{k}\in\mathbb{R}\) is a weight assigned to reward model \(R_{\phi_{k}}\). Then we follow the same PPO training algorithm to optimize the policy model. We discuss how we define and train fine-grained reward models for the detoxification and long-form QA task in our experiments in SS 3 and SS 4 respectively.

## 3 Task 1: Detoxification

The task of detoxification aims to reduce the toxicity in the model generation \(y\) when given a prompt \(x\). Toxicity is the only undesired behavior in this task, and we aim to explore learning with a dense reward in comparison to a _single_ holistic reward. We conduct our experiments on RealToxicityPrompts, a dataset of 100K sentence-level prompts derived from the web that are known to easily elicit problematic generations in GPT-2 [31]. Using a dense sentence-level fine-grained reward, we demonstrate that **our fine-grained reward exhibits greater sample efficiency compared to a holistic reward**, achieving lower toxicity with fewer training steps while maintaining better fluency (SS3.1).

**Holistic reward for (non-)Toxicity.** We use the Perspective API [1] as our reward model, which is widely used for language toxicity detection and is trained with millions of examples gathered from several online platforms and annotated by human annotators for toxicity. That means we use an off-policy reward model that is not trained on outputs from \(P_{\theta_{init}}\). The API outputs a score between 0 (non-toxic) and 1 (toxic). Given the entire model output \(y\), the holistic reward for RL is \(1-\)Perspective(\(y\)).

**Sentence-level (fine-grained) reward for (non-)Toxicity.** To calculate the _fine-grained reward_, we query the API after the model generates each sentence instead of generating the full sequence. For each generated sentence \(y_{j}\), we assign Perspective(\([y_{1},\dots,y_{j-1}]\)) - Perspective(\([y_{1},\dots,y_{j}]\)) as the sentence reward (i.e., how much toxicity is changed from generating \(y_{j}\)). Since there is only one error category, we omit the category superscript, using \(y_{j}\) to denote the \(j^{th}\) segment (e.g., sentence) in \(y\).

### Experiments

**Implementation details.** We follow previous work [17; 21] and use GPT-2 large model as the initial policy model \(P_{\theta_{init}}\). During both the exploration stage in RL training and inference, we use nucleus sampling decoding with \(p\) = 0.9 and temperature = 1.0. The generation length limit is set to 48. The value model used during RL training is initialized with GPT-2-base due to GPU memory constraint. We report RL training parameters in Appendix B. All scores are averaged over 3 independent runs.

**Compared systems and evaluation.** We report the performance of **Fine-Grained RLhf**, RLHF with holistic reward (**Hol. RLHF**), and the state-of-the-art controlled generation approaches **GeDi**[17] and **Dexperts**[21]. We follow previous work [17; 21] to report the toxicity score calculated on each full generation sequence from the Perplexity API, as well as other commonly used metrics for RealToxicityPrompts, including n-gram diversity and GPT-2 XL perplexity (PPL) as a proxy for fluency. The lower the perplexity, the more fluent the generated text. The toxicity score is reported as the _maximum_ score among 4 sampled model outputs, averaged over all test input prompts. Other metrics are reported as the _average_ score of the same 4 samples.

**Main results.** Table 1 shows the experimental results on the RealToxicityPrompts test set. Fine-Grained RLhf with sentence-level fine-grained reward attains the lowest toxicity and perplexity among all methods, while maintaining a similar level of diversity.

**Sample efficiency analysis.** Figure 2 shows the max toxicity and average perplexity on the development set during training. Fine-Grained RLhf has the toxicity drop much faster while keeping a low-level perplexity. This shows that learning from denser fine-grained reward is more sample efficient than holistic reward. One explanation is that fine-grained reward locates where the toxiccontent is, which is a stronger training signal compared with a scalar reward for the whole text. The cost is that we have to query the reward model more times per example.

## 4 Task 2: Long-Form Question Answering (QA)

Long-form QA requires an LM to generate a textual response to a question with a comprehensive answer and explanation. To examine learning with fine-grained rewards at the two granularity dimensions (error category and density), we collect QA-Feedback (SS4.1), a long-form QA dataset annotated with human feedback on LM-generated responses. We define three error categories at different density levels and train a reward model for each (SS4.2). We describe the experimental setup in SS4.3. Both human and automatic evaluation show that Fine-Grained Rlhf outperforms preference-based RLHF and supervised fine-tuning models on all error categories (SS4.4). We then show that adjusting the weights of fine-grained reward models during RL training leads to distinct behaviors in LM generation, allowing us to customize the LM for users with different needs (SS4.5). Finally, we conduct an in-depth analysis of the fine-grained reward models, revealing that they compete against each other, and provide an analysis of their impact on the resulting policy model.

### QA-Feedback: Long Form QA with Human Feedback

QA-Feedback is based on ASQA [39], a dataset that focuses on answering ambiguous factoid questions [26] in an open-domain setting. We use their provided oracle knowledge contexts to reformulate the task into a reading comprehension setting: given the input \(x\) that contains a question \(q\) and a set of knowledge passages \(P=\{p_{1},\ldots,p_{|P|}\}\), generate a long-form response \(y\). On average, there are roughly 65 words in each gold response. Since ASQA does not release the test set, we create our own train/development/test data split from the original train and development sets. We name our newly constructed data, along with collected human feedback (discussed next), QA-Feedback. Overall, we have 3,853 training, 500 development, and 948 test examples (details in Appendix C).

**Initial policy and fine-grained human feedback.** Before collecting human feedback, we follow [33] to initialize the policy model with supervised fine-tuning on a small set of examples. Specifically, we use 1K training examples to supervise fine-tuning of T5-large (the original baseline for ASQA) [32] to get \(P_{\theta_{init}}\). We name this initial policy model **SFT**. We then sample outputs from SFT for the remaining training and development examples and collect _fine-grained_ human feedback in three error categories-- \(\boxed{C_{1}}\): **irrelevance**, repetition**, or incoherence**: \(\boxed{C_{2}}\): **incorrect** or unverifiable facts** based on knowledge passages; and \(\boxed{C_{3}}\): **incomplete information**. The collected feedback instances are then used as the training and development examples for training reward models. For each task prompt \(x\), we only collect fine-grained feedback for _one_ model output. Our data collection has IRB approval and is deemed exempt.

We instruct workers to identify any error in each model output \(y=(a_{1},\ldots,a_{T})\), marking the span of text associated with each identified error type. Formally, we define the set of user-annotated feedback for a task prompt \(x\) and model output \(y\) as \(\mathcal{F}=\{f_{i}\}\) where each \(f_{i}=\langle c_{i},b_{i},e_{i}\rangle\) represents the user-identified span \((a_{b_{i}},\ldots,a_{e_{i}})\) of the error category \(C_{c_{i}}\), where \(c_{i}\in\{1,2,3\}\). Importantly, we impose three restrictions in the annotation: (1) error spans of category \(C_{1}\) or \(C_{2}\) should not overlap with each other; (2) only spans that do not have error \(C_{1}\) need to be assessed as containing error \(C_{2}\) or not; (3) \(C_{3}\) can only apply to whole output sequences. Additionally, we ask workers to mark passage sentences that contain missing information if a \(C_{3}\) error is annotated. We also ask workers to rewrite \(y\) into a corrected version \(y^{\prime}\) that addresses all annotated feedback \(\mathcal{F}\). Details about the feedback collection interface, instructions, and quality control are in Appendix C.

To analyze human-human agreement, a subset of 300 examples receive annotations from two distinct workers. We observe that while exact agreement in error span boundaries is low, workers achieve reasonably high agreement on whether a sub-sentence contains \(C_{1}\) and whether a sentence contains \(C_{2}\).2 Therefore, we decide to have the density for error type \(C_{1}\), \(C_{2}\), and \(C_{3}\) as sub-sentence, sentence and full sequence. We provide more data analysis including human agreement in Appendix C.

Footnote 2: We use spaCy [15] to segment generated model outputs into sentences. We then split sentences into sub-sentences using a comma or semicolon.

**Preference-based human feedback.** For comparison purposes, we follow [29] to separately collect pairwise _human preferences_ from the same group of workers. We sample 4 model outputs for each prompt \(x\), which gives 6 pairs of model outputs. We ask the workers to indicate pairwise preferences(ties are allowed) based on all errors they can find in each model output. They are not asked to explicitly annotate these errors.

**Annotation details.** On average, both annotation tasks of fine-grained and preference feedback for one question take a worker about 6 minutes to finish. In contrast, [39] report that they spend about 15 minutes to label a human-written response for each question, which is much more time-consuming than our feedback annotation. On average, we pay $1.65 per example for both tasks, leading to $16.50 hourly pay for our workers. We include details of the pay structure in Appendix C. We observe that human annotators can reach a higher agreement in each aspect of fine-grained feedback compared to pairwise comparisons because the feedback definitions are more concrete.

### Fine-Grained Reward Models

We train three separate reward models \(R_{\phi_{1}}\), \(R_{\phi_{2}}\), and \(R_{\phi_{3}}\) for \(C_{1}\), \(C_{2}\), and \(C_{3}\) error categories respectively with a density of sub-sentence, sentence, and full sequence, respectively. Since reward models provide scalar reward scores and do not perform generation, we use the encoder-only Longformer-base [4] as our backbone model to handle long input sequences (more details of each reward model are in Appendix D).

\(\boldsymbol{C_{1}}\)**:**Irrelevance, repetition, or incoherence\(R_{\phi_{1}}\) targets to predict whether each sub-sentence in \(y\) contains a \(C_{1}\) type error. We denote \(y=(y_{1}^{1},\ldots,y_{L_{1}}^{1})\), where \(y_{j}^{1}\) is the \(j\)th segment at \(R_{\phi_{1}}\)'s density (i.e., sub-sentence), with \(L_{1}\) segments in total. We add a 2-class token-level classification layer (a single feed-forward layer) on the top of the Longformer encoder. The model input has the format of "question: \(q\) answer: [sep] \(y_{1}^{1}\) [sep] \(y_{2}^{1}\)...", and we take the classification output at each [sep] token to indicate whether the following \(y_{j}^{1}\) contains a \(C_{1}\) error. We do not add passages in the model input because, intuitively, the detection of \(C_{1}\) errors does not depend on them. To train \(R_{\phi_{1}}\), we apply a token-level classification loss to each [sep] token before \(y_{j}^{1}\), where its gold label \(g_{j}\) is "has error" if there is a \(f_{i}\in\mathcal{F}\) that has \((a_{b_{i}},\ldots,a_{e_{i}})\) overlapped with \(y_{j}^{1}\) and \(c_{i}=1\), and "no error" otherwise. When \(R_{\phi_{1}}\) provides a reward during RL training as in Eq. 1, we read a reward \(R_{\phi_{1}}(x,y,j)\) for every \(y_{j}^{1}\) given \(x\) and \(y\). We define \(R_{\phi_{1}}(x,y,j)=+1\) if \(R_{\phi_{1}}\) predicts "no error" for \(y_{j}^{1}\) and \(-1\) otherwise.

\(\boldsymbol{C_{2}}\)**:**Incorrect or unverifiable facts.**\(R_{\phi_{2}}\) is developed for detecting a \(C_{2}\) error at the sentence level in a similar way. The model input has the format of "question: \(q\) context: \(p_{1}\)\(p_{2}\)\(\ldots\)answer: [sep] \(y_{1}^{2}\) [sep] \(y_{2}^{2}\)...", where \(p\)'s denotes the grounding passages and \(y_{j}^{2}\) represents the \(j\)th sentence. We train \(R_{\phi_{2}}\) similarly to \(R_{\phi_{1}}\), with one exception: as we instruct the workers not to annotate a \(C_{2}\) error for a span that is already labeled as containing a \(C_{1}\) error, we do not calculate loss on sentences that are labeled as containing \(C_{1}\) but not \(C_{2}\) during \(R_{\phi_{2}}\) training.

\(\boldsymbol{C_{3}}\)**:**Incomplete information.**\(R_{\phi_{3}}\) is trained to measure the information completeness of \(y\), at the full sequence level. Motivated by [19], \(R_{\phi_{3}}\) predicts a single scalar reward and is trained with a pairwise comparison loss [29]:

\[\mathcal{L}_{r}(\phi)=-\mathbb{E}_{(x,\bar{y}_{p},\bar{y}_{l})\sim D_{p}} \Big{[}\log\big{(}\sigma(R_{\phi_{3}}(x,\bar{y}_{p})-R_{\phi_{3}}(x,\bar{y}_{ l}))\big{)}\Big{]}\] (2)

where \(R_{\phi_{3}}(x,y)\) is the scalar output of the reward model for input \(x\) and output \(y\); \(\bar{y}_{p}\) and \(\bar{y}_{l}\) are sampled from the same input \(x\), and \(\bar{y}_{p}\) has less missed information compared with \(\bar{y}_{l}\); \(D_{p}\) contains the pairwise comparisons bootstraped from human feedback on \(C_{3}\) errors (see details in Appendix D).

**Preference-based reward model.** The preference-based reward model is trained in a similar way to \(R_{\phi_{3}}\), with \(\bar{y}_{p}\) representing the human preferred response against \(\bar{y}_{l}\) in the loss function Eq. 2. It outputs a scalar score for the given \(x\) and \(y\) that represents the overall response quality.

### Experimental Setup

**Compared systems.** We compare our proposed method, **Fine-Grained RLhf** with the initial T5 policy model trained with 1K examples (**SFT**) and RLHF with holistic preference-based rewards (**Preference RLHF**). The reward models used in RLHF experiments are trained on 2.8K examples with annotated feedback (but no gold human response). For analysis, we also use the human gold responses of all training examples to finetune a fully supervised T5 model (**SFT-Full**). Notice that SFT-Full requires much higher annotation cost because it takes longer (15 minutes per example [39]) for annotators to draft long-form responses.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

the outputs become extremely long and the _comp._ reward is extremely high. We observe the outputs find the model is copying a lot of content from the passages. When the _fact._ reward model (\(\,R_{\phi_{2}}\,\)) is removed, the _rel._ reward becomes the highest. We observe that the LM tends to answer the question directly and not reference the passages, which causes a lot of hallucinations. When the _comp._ reward model (\(\overline{R_{\phi_{\mathrm{a}}}}\,\)) is removed, the outputs are concise and factual but not providing all relevant information to the question. Thus, it has lower information completeness and Rouge score compared with the LM trained with all reward models.

**Reward model performance.** We report and analyze the performance of each reward model in predicting its corresponding error category. The _rel._ reward model \(R_{\phi_{1}}\,\) has a binary classification accuracy of 69.6, and an F1 score (for the "has error" class) of 68.5 on model-generated sub-sentences from the development set. We sample 20 sub-sentences where \(R_{\phi_{1}}\,\) predicts the opposite of the human label, and observe that all of them either 1) contain relevant auxiliary information and are marked as "no error" by humans, or 2) are marked as irrelevant by humans but provide closely related background information to the question. In other words, \(R_{\phi_{1}}\,\) is mostly struggling with predicting the relevance of auxiliary information, and it rarely fails to predict a direct answer as "no error".

The _fact._ reward model \(R_{\phi_{2}}\,\) has an accuracy of 77.8 and an F1 score of 67.5. We sample 20 sentences where \(R_{\phi_{2}}\,\) makes a prediction mistake and we observe that the mistakes often happen when the generated sentence is highly abstractive instead of directly copying information from the passage. We also observe that more than 80% of human labeled factual errors occur when the model generates a direct answer (not auxiliary information) that contains hallucinated information or a random entity from a passage. We notice that \(R_{\phi_{2}}\,\) correctly captures more than 80% of such errors.

The _comp._ reward model \(R_{\phi_{3}}\,\) has an accuracy of 70.9 in pairwise comparison. In contrast, the preference-based reward model only reaches an accuracy of 57.2. This helps confirm our intuition that assessing long-form generation outputs holistically can be more ambiguous and subjective than evaluating the outputs with a focus on a specific undesired behavior type.

**Comparison with ChatGPT responses.** We experiment with answering the questions with ChatGPT. To familiarize ChatGPT with the style of our LFQA task, we prompt it with the task instruction and a single random QA example (due to length limitation). ChatGPT achieves a RougeLSum score of 40.92 on the test set, which is much lower than our models. We do not use our trained reward models to evaluate ChatGPT outputs because reward models trained on T5-large may not generalize well to ChatGPT. We instead manually inspect the ChatGPT responses, and observe that they are mostly concise and factual, yet lack the auxiliary information necessary to clarify ambiguous questions. Qualitative examples are in Appendix A. This shows the difficulty for ChatGPT in learning user-desired behaviors through simple prompting.

## 5 Related Work

**Reinforcement learning from human feedback (RLHF).** RLHF [46; 42; 29] aims to optimize the policy language model to generate content that is desired by human. This framework has been explored to improve the model performance on a variety of natural language processing tasks such as text summarization [40], instruction following [29], question answering [24; 27] and reducing harmfulness [3; 2; 22; 10]. Most of these studies collect human preferences over pairs of model outputs on one or a set of desired attributes, in order to train a reward model to assign a holistic score for a generation output during RL training. [13] trains separate reward models that assign scores for different desired attributes, but still uses a single reward that combines scores from all reward models. In contrast, we explore RLHF with fine-grained reward models trained on human feedback where each reward model provides dense reward after every small text segment for a specific type of desired behavior. [30] explores using intermediate rewards to improves LM performance on reasoning tasks.

**Learning from human feedback in NLP.** There also exists work that explores non-RL methods to learn from human feedback. [44] trains a reward model that predicts a single score for each model output and selects samples with the highest reward scores for supervised fine-tuning. [38; 14; 42] train a conversational model to predict both the response and a binary user satisfaction score in order to improve the response generation. Besides such numerical human feedback, natural language (NL) human feedback has also been explored. [23; 6] collect and store NL human feedback in a feedback memory for the model to retrieve and then perform the end task conditioning on the retrieved feedback. [5; 35; 34] use a refinement model to refine model outputs conditioning on NL human feedback and then use a reward model to select the best refined outputs for supervised fine-tuning. Methods for using a reward model to guide LM generation towards desired behaviors at inference time [21; 7] can complement our work that aims to improve the LM during training. [16] also explores incorporating human feedback into LM pre-training.

## 6 Discussion

**Annotation Costs.** It is important to note that the fine-grained human feedback used for training our fine-grained reward models does _not_ incur a greater cost than holistic human preference. As outlined in SS 4.2, our observations reveal that annotators require a substantial amount of time to compare two lengthy text outputs. For the long-form QA task, both fine-grained feedback and preference-based feedback takes approximately 6 minutes per sample for an annotator.

### Broader Impacts

We propose the Fine-Grained Rlhf framework that can incorporate multiple reward models to provide dense rewards for RL training, which leads to LM outputs that are optimized towards such rewards. Our framework can be applied to any text generation task, thereby enhancing LM performance by offering more nuanced guidance than holistic feedback. The key advantages of the Fine-Grained Rlhf framework are two-fold:

**Flexibility.** Our framework significantly expands the versatility of reward models for RLHF. For example, future work involving fact-checking, sentiment classification, toxicity detection, among others, can all be incorporated within this framework. LMs can be trained against all these reward models via Fine-Grained Rlhf.

**Controllability.** Having multiple reward models that stand for different feedback types allows the end user to exert greater control over RL training (e.g., through different combinations of reward model weights; see details in SS 4.5). This leads to customized model behaviors, a benefit particularly valuable for applications like educational tools where model personalization is crucial.

### Limitations and Future Work

One major limitation of our framework comes from the additional compute cost of getting _fine-grained_ rewards, compared to RLHF with a holistic reward. For instance, in the detoxification task, we need to make multiple Perspective API calls for each model output depending on how many sentences are generated, while RLHF with a holistic reward only requires one. In the long-form QA task, we need to calculate a dense reward from multiple reward models, which takes more compute time and GPU memory than a single reward model.

Another limitation is that different tasks may have different definitions of fine-grained feedback in terms of the feedback types and the density level of each type. Therefore, defining feedback that is well-suited for a task and training reward models accordingly requires non-trivial manual effort.

Finally, in this work, we carefully control the quality of annotated feedback, which is then used to train reward models for RL. In practice, when a deployed model is released to the public, end users don't always give clean feedback. Therefore, how to obtain effective learning signals from noisy human feedback in the wild still needs further investigation.

Some other interesting questions to explore in the future include: 1) Can we obtain fine-grained feedback from LMs like GPT-4 instead of humans to improve model performance and reduce annotation costs? 2) How can other non-RL approaches of using human feedback such as controlled generation during inference time complement Fine-Grained Rlhf? 3) How would fine-grained reward and value model sizes affect policy model performance during RL training?

## 7 Conclusion

In this work, we introduce Fine-Grained Rlhf, a framework that enables LMs to learn from multiple fine-grained reward models trained from human feedback, where each reward model detects a specific error category and provides dense rewards. We conduct experimental analysis on two text generation tasks to illustrate the performance gain of Fine-Grained Rlhf than RLHF over holistic rewards, supported by both automatic and human evaluation. Furthermore, we show that an LM can be customized for specific needs using different combinations of fine-grained reward models.

## Acknowledgments

We thank Jiacheng Liu for sharing the standard PPO training code, and Yizhong Wang for providing insights during early discussions of the project. We also thank UW TIAL members for participating in our pilot feedback annotation. We extend our thanks to UW NLP members who provided insights or feedback to our project. Lastly, we especially thank all our AMT workers for helping us annotate the high quality feedback data. This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA) under Contract No. FA8650-23-C-7316. This work was also funded in part by the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), NSF IIS-2044660, and ONR N00014-18-1-2826. The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.

## References

* [1] Perspective API, https://github.com/conversationai/perspectiveapi.
* [2] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [3] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* [4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. _arXiv preprint arXiv:2004.05150_, 2020.
* [5] Angelica Chen, Jeremy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural language feedback. _arXiv preprint arXiv:2303.16749_, 2023.
* [6] Bhavana Dalvi Mishra, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9465-9480, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* [7] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled text generation. In _International Conference on Learning Representations_, 2020.
* [8] Yao Dou, Maxwell Forbes, Rik Koncel-Kedziorski, Noah A. Smith, and Yejin Choi. Is GPT-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 7250-7274, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [9] Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, and Tushar Khot. Chain-of-thoughht hub: A continuous effort to measure large language models' reasoning performance. _arXiv preprint arXiv:2305.17306_, 2023.
* [10] Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamille Lukositute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for moral self-correction in large language models. _arXiv preprint arXiv:2302.07459_, 2023.
* [11] Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. _arXiv preprint arXiv:2305.14627_, 2023.
* [12] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. Real-ToxicityPrompts: Evaluating neural toxic degeneration in language models. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 3356-3369, Online, November 2020. Association for Computational Linguistics.

* [13] Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. _arXiv preprint arXiv:2209.14375_, 2022.
* [14] Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 3667-3684, Florence, Italy, July 2019. Association for Computational Linguistics.
* [15] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. spaCy: Industrial-strength Natural Language Processing in Python. 2020.
* [16] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher L Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining language models with human preferences. _arXiv preprint arXiv:2302.08582_, 2023.
* [17] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafiq Joty, Richard Socher, and Nazneen Fatema Rajani. GeDi: Generative discriminator guided sequence generation. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 4929-4952, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [18] Kalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. LongEval: Guidelines for human evaluation of faithfulness in long-form summarization. In _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 1650-1669, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics.
* [19] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. _arXiv preprint arXiv:1909.03087_, 2019.
* [20] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
* [21] Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. DEXerts: Decoding-time controlled text generation with experts and anti-experts. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 6691-6706, Online, August 2021. Association for Computational Linguistics.
* [22] Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. In _Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS)_, 2022.
* [23] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve GPT-3 after deployment. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 2833-2861, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* [24] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support answers with verified quotes. _arXiv preprint arXiv:2203.11147_, 2022.
* [25] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. _arXiv preprint arXiv:2305.14251_, 2023.
* [26] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. AmbigQA: Answering ambiguous open-domain questions. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 5783-5797, Online, November 2020. Association for Computational Linguistics.

* [27] Reiichiro Nakano, Jacob Hilton, S. Arun Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback. _ArXiv_, abs/2112.09332, 2021.
* [28] OpenAI. Gpt-4 technical report, 2023.
* [29] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022.
* [30] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations, 2023.
* [31] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* [33] Rajkumar Ramamurthy*, Prithviraj Ammanabrolu*, Kiante Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization. In _International Conference on Learning Representations (ICLR)_, 2023.
* [34] Jeremy Scheurer, Jon Ander Campos, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with natural language feedback. _arXiv preprint arXiv:2204.14146_, 2022.
* [35] Jeremy Scheurer, Jon Ander Campos, Tomasz Korbak, Jun Shern Chan, Angelica Chen, Kyunghyun Cho, and Ethan Perez. Training language models with language feedback at scale, 2023.
* [36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In _Proceedings of the International Conference on Learning Representations (ICLR)_, 2016.
* [37] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [38] Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and Jing Xu. When life gives you lemons, make cherryade: Converting feedback from bad responses into good labels. _arXiv preprint arXiv:2210.15893_, 2022.
* [39] Ivan Stelmakh, Yi Luan, Zhuwan Dhingra, and Ming-Wei Chang. ASQA: Factoid questions meet long-form answers. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 8273-8288, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.
* [40] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan J. Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. _ArXiv_, abs/2009.01325, 2020.
* [41] Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. Recursively summarizing books with human feedback. _arXiv preprint arXiv:2109.10862_, 2021.
* [42] Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, and Jason Weston. Learning new skills after deployment: Improving open-domain internet-driven dialogue with human feedback. _arXiv preprint arXiv:2208.03270_, 2022.

* [43] Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. Instructscore: Towards explainable text generation evaluation with automatic feedback. _arXiv preprint arXiv:2305.14282_, 2023.
* [44] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears, 2023.
* [45] Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. Automatic evaluation of attribution by large language models. _arXiv preprint arXiv:2305.06311_, 2023.
* [46] Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _ArXiv_, abs/1909.08593, 2019.

## Appendix

* [1] A Qualitative Examples for Long-Form QA
* A.1 Examples on LM Customization
* A.2 Examples on LM Errors
* A.3 Examples on Reward Model Ablation
* A.4 Comparison with ChatGPT responses
* [2] B Algorithm and Training Details of Fine-Grained Rlhf
* B.1 PPO with Fine-Grained Rewards
* B.2 Implementation Details
* [3] C Long-Form QA Data and Human Feedback Annotation
* C.1 Data Construction
* C.2 Human Feedback Annotation
* C.3 Analysis of Collected Fine-Grained Feedback
* [4] D Long-Form QA Reward Model Training DetailsQualitative Examples for Long-Form QA

### Examples on LM Customization

As discussed in SS 4.5, we can modify the weight of each fine-grained reward model during RL training to get LM with different behaviors. Here, we explore three configurations of reward model weights and name them'short','medium', and 'long' based on the LM's average generation length. The'short' configuration generates concise and short responses, while the 'long' configuration generates detailed and long responses. Table 6 demonstrates the different behaviors of our customized LMs. Given the same question, each LM generates different amount of auxiliary information in the response.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Question:** & When did the French join revolution on colonists’ side? \\ \hline
**Passages:** & **Article Title: France in the American Revolutionary War** \\  & French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments of money and material to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no alies. Finance’s help is considered a vital and decisive contribution to the United States’ victory against the British. As a cost of participation in the war, France accumulated over 1 billion lives in debt. After its defeat in the Seven Years’ War in 1763, France lost its vast holdings in North America. Meanwhile, the American colonists and the British government began to fight over whether Parliament in London or the colonial assemblies had primary responsibility for taxation. As part of that conflict, the colonists organized the Boston Tea Party in response to a tax on tea. The British government responded by passing the Inderable Acts, which included the closing of Boston Harbor and the revocation of Massachusetts’s colonial charter. This conflict exacerbated tensions further. The ideological conflict escalated into open warfare in 1775, at which point the American patrols revolved against British rule. France, who had been rebuilding their Navy and other forces, saw this as an opportunity to seriously weaken her perennial enemy. \\ \hline
**Article Title: France in the American Revolutionary War** \\  & France bitterly resented its loss in the Seven Years’ War and sought revenge. It also wanted to strategically weaken Britain. Following the Declaration of Independence, the American Revolution was well received by both the general population and the aristracency in France. The Revolution was perceived as the incarnation of the Enlightment Spirit against the "English tyranny." Benjamin Franklin traveled to France in December 1776 in order to fully the nation’s support, and he was welcomed with great enthusiasm. At first, French support was covert: French agents sent the Purtids military and (predominantly gunpowder) through a company called ‘Rodriguez Hortalez or Compagnie”, beginning in the spring of 1776. Estimates place the percentage of French supplied arms to the Americans in the Saratoga campaign at up to 90%. By 1777, over five million ‘lives” of aid had been sent to the American rebk. Motivated by the prospect of glory in battle or animated by the science ideals of liberty and republicansim, volunteers like Pierre Charles L’Efafant joined the American army. The most famous was Lafayette, a charming young aristocrat who defied the king’s order and enlisted in 1777 are 20. He became an aide to George Washington and a combat general. More importantly, he solidified a favorable American view of France. \\
**Article Title: Treaty of Alliance (1778)** \\  & On March 17, 1778, four days after a French ambassador informed the British government that France had officially recognized the United States as an independent nation with the signing of the Treaty of Amity and Commerce and the Treaty of Alliance, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary War. French entry into the war would lead to further escalation of the war when Spain entered the fight against Britain as France’s ally, after the signing of the Treaty of Aranjuge on April 12, 1779, and again in December 1780 when Britain declared war on the Dutch Republic after serving a Dutch merchant ship they claimed was carrying contradanded to France during the Affair of Fidelity and Bylandt. After the signing of the treaty French supplies of arms, ammunition, and uniforms proved vital for the Continental Army, while their actions in the Anglo French War of 1778-83 in the West Indies and elsewhere forced Britain to redeploy troops and naval units away from the North American colonies to secure their holdings. \\ \hline \multicolumn{2}{l}{**LM outputs with different reward model configurations:**} \\
**Short:** & French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments of money and matreiel to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies. \\
**Medium:** & French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments of money and matreiel to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies. On March 17, 1778, four days after a French ambassador informed the British government that France had officially recognized the United States as an independent nation, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary War. \\
**Long:** & French involvement in the American Revolutionary War began in 1775, when France, a rival of the British Empire, secretly shipped supplies to the Continental Army. A Treaty of Alliance in 1778 soon followed, which led to shipments of money and matreiel to the United States. Subsequently, the Spanish Empire and the Dutch Republic also began to send assistance, leaving the British Empire with no allies. On March 17, 1778, four days after a French ambassador informed the British government that France had officially recognized the United States as an independent nation with the signing of the Treaty of Amity and Commerce and the Treaty of Alliance, Great Britain declared war on France, thereby engaging that nation in the American Revolutionary War. \\ \hline \hline \end{tabular}
\end{table}
Table 6: Qualitative examples on LM customization.

[MISSING_PAGE_FAIL:17]

## 6 Conclusion

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Question:** & Where does the flat iron cut of meat come from? \\ \hline
**Passages:** & **Article Title: Flat iron steak** \\  & Flat iron steak (US), butters’ steak (UK), or oyster blade steak (Australia and New Zealand) is a cut of steak cut with the grain from the shoulder of the animal. This produces a flavorful cut that is a bit tough because it contains a gristly fascia membrane unless removed. Some restaurants offer it on their menu, often at lower price than the more popular rib-eye and strip steaks of the same grade. This is used, in some places, as a means of selling a less expensive cut from the same animal, for example Kobe beef. This cut of steak is from the shoulder of a beef animal. It is located adjacent to the heart of the shoulder clot, under the seven or paddle bone (shoulder blade or scapula). The steak encompasses the infraspinatus muscles of beef, and one may see this displayed in some butcher shops and meat markets as a “top blade” roast. Anatomically, the muscle forms the dorsal part of the rotator cuff of the steer. This cut is anatomically distinct from the shoulder tender, which lies directly below it and is the teres major. Flat iron steaks usually have a significant amount of marbling. \\ \hline
**LM outputs** \\
**SFT:** & Flat iron steak is a cut of steak cut with the grain from the shoulder of the animal. This produces a flavorful cut that is a bit tough because it contains a gristly fascia membrane unless removed. Some restaurants offer it on their menu, often at lower price than the more popular rib-eye and strip steaks of the same grade. This is used, in some places, as a means of selling a less expensive cut from the same animal, for example Kobe beef. \\
**Pref.** & Flat iron steak is a cut of steak cut with the grain from the shoulder of the animal. This produces a flavorful cut that is a bit tough because it contains a gristly fascia membrane unless removed. Some restaurants offer it on their menu, often at lower price than the more popular rib-eye and strip steaks of the same grade. This is used, in some places, as a means of selling a less expensive cut from the same animal, for example Kobe beef. \\
**Fine-** & Flat iron steak is a cut of steak cut with the grain from the shoulder of the animal. This is located adjacent to the heart of the shoulder clot, under the seven or paddle bone (shoulder blade or scapula). This steak encompasses the infraspinatus muscles of beef, and one may see this displayed in some butcher shops and meat markets as a “top blade roast.” \\ \hline \hline \end{tabular}
\end{table}
Table 8: Qualitative examples on another question. Here we highlight the information in blue that is covered by the LM trained with Fine-Grained Rlhf but missed (\(\underline{C_{3}}\): incomplete information error) by SFT and Pref.RLHF.

[MISSING_PAGE_FAIL:19]

**Question:** When did the Rams go to St Louis?

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Question:** & Who gang the song I’m Coming Out? \\ \hline
**Passages:** & **Article Title: I’m Coming Out** \\  & There’s also been a cover by Marcia Hines. It was featured as a cover for the video game “Karaoke Revolution” as well. In more recent times, the song was sampled in Keyshia Cole’s remake of “Tm Coming Out” (featuring Iggy Azalea) for the original soundtrack of the 2014 movie The Other Woman, starting Cameron Diaz, Kate Upton and Leslie Mann. The song is sampled in “Break Your Heart Right Back” from Arima Grande’s sophome album”My Everything” (2014). This song was also covered for the soundtrack to the 2016 movie “Trolls”. In 1997, German comedian Hella von Sinnen sampled “Tm Coming Out” for her single “Mein Coming Out”, using the term “coming out” in the context of coming out of the closet, or revealing one’s homosevainality. In 2003, American singer America covered “Tm Coming Out” for the soundtrack to the film “Maid in Manhattan”. He version was released as a single in select European countries and Australia. The final single version to be released was the Loren Dawson remix. The original single version (which is similar to the original) can be found on both CD singles and the original motion picture soundtrack to “Maid in Manhattan”. Rosis’ version appears on the film’s soundtrack album as well. \\
**Article Title: I’m Coming Out** \\  & \({}^{\prime}\)Tm Coming Out” is a song recorded by American singer Diana Ross. It was written and produced by Chic members Bernard Edwards and Nile Rodgers, and released in August 22, 1980 as the second single from Rosis’ self-titled tenth album “Diana” (1980). In 1979, Ross commissioned Chile founds Nile Rodgers and Bernard Edwards to create material for a new album after taking her daughters to see the band in concert, following the success of Rosis’s final Ashford \& Simpson-composed and produced. LP” The Ross. “Roders got the idea for “Tm Coming Out” after noticing three different drag queues dressed as Diana Ross at a New York club called the GG Barum Room. The lyrics hold an additional meaning to Ross, as she was leaving Motown Records and “coming out” from under Berry Gordy’s thumb. According to the BBC4 documentary “How to Make It in the Music Business”, Rosis loved the record until she put it in front of Franike Crocker, who pointed out that ‘Tm Coming Out’ is what homosevainals use to announce their homosevainality; she ran back to the studio in tears and demanded to know why Rodgers wanted to ruin her career. The song was another hit from the album, peaking at number five on the U.S. \\ \hline
**Comparison between our output and ChatGPT output** \\ \hline
**ChatGPT:** & The songs \({}^{\prime}\)Tm Coming Out” was recorded by American singer Diana Ross. It was written and produced by Chic members Bernard Edwards and Nile Rodgers, and released in August 22, 1980 as the second single from Rosis’ self-titled tenth album Diana. In 2003, American singer America covered “Tm Coming Out” for the soundtrack to the film Maid in Manhattan. Her version was released as a single in select European countries and Australia. The final single version to be released was the Loren Dawson remix. \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparison between ChatGPT outputs and Fine-Grained Rlff outputs.

Algorithm and Training Details of Fine-Grained RLhf

### PPO with Fine-Grained Rewards

The algorithm below shows in detail how PPO updates the policy LM \(P_{\theta}\) and the value model \(V_{\psi}\) with \(K\) fine-grained reward models \(R_{\phi_{k}}\).

**Input** initial policy model \(P_{\theta_{\text{min}}}\); initial value model \(V_{\psi_{\text{min}}}\); \(K\) reward models \(R_{\phi_{k}}\) trained from human feedback; task prompts \(\mathcal{D}\); hyperparameters \(\gamma\), \(\lambda\), \(\epsilon\), \(\beta\)\(\triangleright\) SS 2

```
1:policy model \(P_{\theta}\gets P_{\theta_{\text{min}}}\), value model \(V_{\psi}\gets V_{\psi_{\text{min}}}\)
2:for step = 1,..., M do
3: Sample a batch \(\mathcal{D}_{b}\) from \(\mathcal{D}\)
4: Sample output sequence \(y^{n}\sim P_{\theta}(\cdot\mid x^{n})\) for each prompt \(x^{n}\in\mathcal{D}_{b}\)
5: Compute rewards \(\{r_{t}^{n}\}_{t=1}^{|y^{n}|}\) for each sampled output \(y^{n}\) by running \(R_{\phi_{k}}\)\(\triangleright\) Eq. 1
6: Compute advantages \(\{A_{t}\}_{t=1}^{|y^{n}|}\) and value targets \(\{V^{\text{arg}}(s_{t})\}_{t=1}^{|y^{n}|}\) for each \(y^{n}\) with \(V_{\psi}\)
7:for PPO iteration = 1,..., \(\mu\)do
8: Update the policy model by maximizing the PPO clipped surrogate objective: \[\theta\leftarrow\arg\max_{\theta}\frac{1}{|\mathcal{D}_{b}|}\sum_{n=1}^{| \mathcal{D}_{b}|}\frac{1}{|y^{n}|}\sum_{t=1}^{|y^{n}|}\min\left(\frac{P_{\theta }(a_{t}\mid s_{t})}{P_{\theta_{\text{all}}}(a_{t}\mid s_{t})}A_{t},\,\text{ clip}(v_{t},\,1-\varepsilon,\,1+\varepsilon)A_{t}\right)\]
9: Update the value model by minimizing a square-error objective: \[\psi\leftarrow\arg\min_{\psi}\frac{1}{|\mathcal{D}_{b}|}\sum_{n=1}^{| \mathcal{D}_{b}|}\frac{1}{|y^{n}|}\sum_{t=1}^{|y^{n}|}\left(V_{\psi}(s_{t})-V^ {\text{arg}}(s_{t})\right)^{2}\]

**Output \(P_{\theta}\)**

**Algorithm 1** Fine-Grained Reinforcement Learning from Human Feedback (Fine-Grained RLhf)

### Implementation Details

**Model architectures.** For the detoxification experiments, the policy model is initialized with GPT2-large [31], and the value model is initialized with GPT2-base. For the long-form QA experiments, the policy model is initialized with a supervised fine-tuned T5-large [32], and the value model is initialized with T5-base. This design follows InstructGPT [29], which uses a larger (175B) policy model, and smaller value and reward (6B) models.

**Training details on detoxification.** For both the holistic reward baseline and the sentence-level (fine-grained) reward, we do a hyper-parameter search with the same set of hyper-parameters. For training, we run 200K episodes. The batch size (number of episodes per card during training) is 64. We use Adam optimizer with a linear learning rate scheduler and 10 warmup steps. We perform a hyper-parameter grid-search for peak learning rate \(\in\{5e-6,1e-5,2e-5\}\), KL coefficient \(\beta\in\{0.1,0.2,0.3\}\), discounting factor \(\lambda\in\{0.95,0.97,0.99\}\), and the frequency of exploration (number of sampled outputs) \(\in\{2,4,8\}\). We find that the higher the KL coefficient, the lower the perplexity, and the higher toxicity. This is consistent with findings from previous RLHF studies ([29], [33]). For a fair comparison, we eventually choose a set of parameters that achieve a similar level of perplexity for both reward models. The optimal set of hyper-parameters for holistic reward is \(\beta=0.3,\lambda=0.99\). For sentence-level reward \(\beta=0.1,\lambda=0.95\). The learning rate is \(1e-5\), and the exploration frequency is \(4\) for both experiments. We choose the checkpoint with the lowest validation set toxicity for evaluation. Regarding computation time, we use \(2\times\) 80G NVIDIA A100 GPU for training, and the run time is about 22 hours.

**Training details on long-form QA.** We conduct a similar hyper-parameter grid search as our detoxification experiments. For long-Form QA, the input length limit is 1024, and the output length limit is 200. Notice that this is much longer than detoxification, so we use a smaller batch size and fewer training episodes. We experiment with multiple combinations of reward model weights. Fixing \(w_{2}=0.5\) (factuality reward weight), we perform a grid search on \(w_{1},w_{3}\in[0.0,0.5]\)We eventually choose \(w_{1}=0.3,w_{2}=0.5,w_{3}=0.3\), which reaches a balance between three reward models and allows all three rewards to increase during training. For training, the batch size (number of episodes per card during training) is 32. We use Adam optimizer with a linear learning rate scheduler and 100 warmup steps. We perform a hyper-parameter grid-search for peak learning rate \(\in\{5e-6,1e-5,2e-5\}\), KL coefficient \(\beta\in\{0.1,0.2,0.3\}\), discounting factor \(\lambda\in\{0.95,0.97,0.99\}\), and the frequency of exploration \(\in\{2,4,8\}\). The optimal set of hyper-parameters for Pref. RLHF is \(\beta=0.2,\lambda=0.99\). For Fine-Grained Rlhf, \(\beta=0.3,\lambda=0.95\). The learning rate is \(1e-5\), and the exploration frequency is \(4\) for both experiments. we run 80K episodes, which is approximately 5 epochs. We choose the checkpoint with the highest validation reward for evaluation. Regarding computation time, we use \(2\times\) 80G NVIDIA A100 GPU for training, and the run time is about 15 hours.

**A note on the error bars.** All results we report in the paper are from 3 independent runs. The scores reported are all averaged across all runs. The error bars are represented as the shades behind each training curve in our figures. It shows the standard error across three runs.

## Appendix C Long-Form QA Data and Human Feedback Annotation

### Data Construction

ASQA [39] is a long-form QA dataset that focuses on answering ambiguous factoid questions in an _open-domain_ setting that requires passage retrieval from a given Wikipedia passage corpus. We reformulate it into a _reading comprehension_ setting: given the input \(x\) that contains a question \(q\) and a set of knowledge passages \(P=\{p_{1},...,p_{|P|}\}\), generate a long-form response \(y\). To construct \(P\) for each input \(x\), we use the oracle knowledge contexts provided by ASQA for each \(x\), that are text snippets from the passage corpus. We use BM254 to map each knowledge context (text snippet) to the closest passage from the passage corpus and use the resulting passages as \(P\). Our train and dev examples come from the original ASQA train set and our test examples are the original ASQA dev examples.

Footnote 4: https://github.com/castorini/pyserini

Footnote 5: https://www.mturk.com/

### Human Feedback Annotation

**Fine-grained feedback.** As discussed in SS 4.1, we first use 1K randomly sampled training examples to train a T5-large based supervised model **SFT** as the initial policy model \(P_{\theta_{init}}\). Then we collect feedback on sampled outputs from SFT for the remaining 2,853 training examples and the 500 development examples, using the Amazon Mechanical Turk platform.5

Footnote 5: We see very few "incoherence” errors (1%), so the majority of labeled errors are from the other four categories during annotation.

Figure 5 shows the fine-grained human feedback annotation interface with an example from QA-Feedback. In addition to the task input--question \(q\) and oracle passages \(P\), we also provide a human-written response from ASQA to the worker as reference. However, it is important to note that, in practice, the annotation of our fine-grained feedback should not require the human-written response. The only purpose for us to provide the gold response is to have our workers follow the same question interpretation and expected response of the workers who annotate for ASQA, such that our experimental comparison with supervised models (**SFT** and **SFT-Full**; details in SS 4.3) is fair. However, we still instruct our workers to strictly use the given passages for checking factual errors. For each span error, we ask the worker to select one out of 5 categories shown in Figure 6 (left).6 However, we collapse these 5 categories into two categories (\(C_{1}\) and \(C_{2}\) mentioned in SS 4.1) based on whether the error detection depends on the passages or not. When workers mark passage sentences as containing missing information, we instruct them to categorize each sentence as missing "answer", "major auxiliary information" or "minor auxiliary information," as shown in Figure 6 (right). Our instruction to the worker is provided in Figure 8.

Footnote 6: We see very few "incoherence” errors (1%), so the majority of labeled errors are from the other four categories during annotation.

**Quality control.** Before feedback collection, we design a qualification task to select qualified workers for this feedback annotation task. The qualification task consists of 5 questions with their corresponding passages and model outputs for the workers to annotate. We manually review about 70 submissions of the qualification task and select 15 workers whose annotation is marked by us as of high quality. Throughout the actual feedback annotation process, we constantly monitor the annotated data and send feedback to workers.

**Preference-based feedback.** For comparison purposes, we follow [29] to collect pairwise human preferences from the same group of workers we select from the qualification task. We sample four model outputs for each prompt \(x\), which gives 6 pairs of model outputs. Similarly, we provide the worker with the human-written response and ask the workers to indicate pairwise preferences (ties are allowed) based on all errors they can find each model output. Figure 7 shows the preference-based human feedback annotation interface with an example from QA-Feedback.

**Pay structure.** We pay a base rate of $1.5 per example for annotating fine-grained or preference feedback. If the example consists of \(\geq 3\) passages to read, we assign an additional $0.3 bonus to the example. On average, we pay roughly $1.65 per example for both tasks, which gives an $16.5 hourly pay for our workers.

### Analysis of Collected Fine-Grained Feedback

Overall, among all _error spans_ we collect, 76% of them are \(C_{1}\) errors and the remaining 24% are \(C_{2}\) errors. However, it is important to note that we instruct workers to label \(C_{2}\) errors only at places that don't have a \(C_{1}\) error. 75% examples are labeled as being incomplete; i.e., containing missing information that can be found in the given passages (\(C_{3}\)). Among all marked passage sentences that contain missing information, 31%, 42% and 27% are missing answer, major auxiliary information and minor auxiliary information respectively.

To analyze human-human agreement, a subset of 300 examples receive annotations from two distinct workers. We observe that while the exact agreement in error span boundaries is low, workers achieve reasonably high agreement on whether a sub-sentence contains \(C_{1}\) (reach an agreement for 83% of all sub-sentences) and whether a sentence contains \(C_{2}\) (92%). 7 The agreement on whether a model output contains complete information or not (\(C_{3}\)) is 85%. Therefore, we decide to have the density for error type \(C_{1}\), \(C_{2}\), and \(C_{3}\) as sub-sentence, sentence and full sequence.

Footnote 7: We use spaCy [15] to segment generated model outputs into sentences. We then split sentences into sub-sentences using a comma or semicolon.

## Appendix D Long-Form QA Reward Model Training Details

We train reward models with the 2,835 training examples with feedback collected and select the best model for each error category based on the their performance on the development set. The batch size and training epochs are 24 and 50 for \(R_{\phi_{1}}\) and \(R_{\phi_{2}}\). Each training is run on a single 80G NVIDIA A100 GPU, taking 1 and 2 hours for training \(R_{\phi_{1}}\) and \(R_{\phi_{2}}\) respectively.8 The batch size and training epochs are 12 (per GPU) and 30 for \(R_{\phi_{3}}\) and the preference-based reward model. Each training is run on \(2\times\) 80G NVIDIA A100 GPU and takes 2 hours. We use Adam optimizer with a linear learning rate scheduler for all reward model training. For each reward model, we search the learning rate over \(\{5e^{-6},1e^{-5},5e^{-5}\}\), weight decay over \(\{0.001,0.01\}\), and warm-up step ratio over \(\{0.1,0.2\}\) based on the dev set performance. Specifically, we use the model for \(R_{\phi_{1}}\) and \(R_{\phi_{2}}\) that achieve the best binary classification accuracy. For \(R_{\phi_{3}}\) and the preference-based reward model, we select the model that achieves the best pairwise comparison accuracy. We also provide more training details for each reward model below.

Footnote 8: Note that training \(R_{\phi_{1}}\) takes shorter time as its input does not contain passages.

\(R_{\phi_{1}}\) **for \(C_{1}\): Irrelevance, repetition, or incoherence.** To train the reward model \(R_{\phi_{1}}\) that detects error of irrelevance, repetition, or incoherence, we apply a token-level classification loss to each [sep] token before \(y^{1}_{j}\), where its gold label \(g_{j}\) is "has error" if there is a \(f_{i}\in\mathcal{F}\) that has \(a_{b_{i},\dots,e_{i}}\) overlapped with \(y^{1}_{j}\) and \(c_{i}=1\), and "no error" otherwise. We observe that most of the spans marked as error type \(C_{1}\) that are shorter than 5 words usually carry very little information or are annotated as a result of workers being very careful or strict. Therefore, we filter out such short spans before constructing training examples for \(R_{\phi_{1}}\). Overall, we get 7379 and 8059 sub-sentences with the "has error" and "no error" label respectively.

\(R_{\phi_{2}}\) for \(C_{2}\): Incorrect or unverifiable facts.** We train \(R_{\phi_{2}}\) in a similar way as how we train \(R_{\phi_{1}}\). Instead of predicting the error for each sub-sentence, \(R_{\phi_{2}}\) is trained to predict at the sentence level (i.e., \(y_{j}^{2}\) is the \(j^{th}\) sentence in \(y\)). Since workers do not annotate \(C_{2}\) error for spans that are already labeled as having \(C_{1}\) error, in order to avoid false negatives in training \(R_{\phi_{2}}\), we do not provide gold label nor calculate loss for sentences that only contain \(C_{1}\) error from training. In other words, all sentences that contain a \(C_{2}\) error has the gold label "has error" and sentences that contain no error has the gold label "no error". Overall, we get 1600 and 3411 sentences with the "has error" and "no error" label respectively.

\(R_{\phi_{2}}\) for \(C_{3}\): Incomplete information.** Instead of casting this as a classification task, \(R_{\phi_{3}}\) predicts a single scalar reward given \(x\) and \(y\) and is trained with a pairwise comparison loss [29]. This is motivated by early work [19] that shows the better reliability of pairwise comparison than error classification when assessing a full generation sequence. To construct training data for \(R_{\phi_{3}}\), we bootstrap pairwise comparisons from the corrected model output \(y^{\prime}\) as follows. We first map each sub-sentence in \(y^{\prime}\) to a passage sentence in \(P\) that has a sub-string with the highest token-level F1 score with the sub-sentence,9 and denote all mapped sentences as \(S\). We then sample four responses from SFT, for each we do the same sentence mapping to get a set of passages sentences \(\mathcal{S}^{\prime}\). We calculate \(score(y)=|\mathcal{S}^{\prime}\cap\mathcal{S}|/|\mathcal{S}|\) as the information completeness score for each model response \(y\). We follow [29] to pair up sampled responses for \(q\) and denote each sampled response pair as (\(\bar{y}_{p}\), \(\bar{y}_{l}\)), where \(score(\bar{y}_{p})>score(\bar{y}_{l})\). We drop the pairs where \(score(\bar{y}_{p})=score(\bar{y}_{l})\). Then we follow [29] to train \(R_{\phi_{3}}\) with the loss function in Eq. 2. We have a total number of 6821 pair examples in training.

Footnote 9: We manually review 50 mapped passage sentences and find over 90% of them are correctly mapped, which indicates frequent extractive behaviors from \(P_{\theta_{initial}}\).

**Preference-based reward model.** The preference-based reward model is trained in a similar way as \(R_{\phi_{3}}\), with \(\bar{y}_{p}\) representing the human preferred response against \(\bar{y}_{l}\) in the loss function Eq. 2. We drop the pairs where a tie is indicated. We have a total number of 14981 pair examples in training.

Figure 5: Fine-grained feedback annotation interface.

Figure 6: Error types in the fine-grained feedback annotation interface.

Figure 7: Preference feedback annotation interface. The task input is omitted (same as in Figure 5).

In each task, **you will be given** a question, a set of Wikipedia passages (with their article title provided), a reference response, and a model-predicted response. Each passage is presented as a sequence of sentences (title is indexed as sentence #o). **Your goal is to** i) label mistakes made in the model prediction and _2)_ make corrective edits to the model prediction based on your labeled mistakes.

**Important Definitions**: _An ideal_ response to a question should provide both _answer(s)_ that directly responds to the question and _crucial auxiliary information_ for better comprehension of the answer(s). We consider auxiliary information as _crucial_ if it is used in the reference response. Additionally, all information in _an ideal_ response should be _factually consistent with (i.e., grounded in) the passages_. Note that the reference response is written by a human with potentially different grounding passages, and thus, you might find _answers_ that can be found in the passages but are not included in the reference, which are _STILL_ expected to be in the model prediction. On the other hand, answers in the reference that cannot be found in or verifiable by the passages are _NOT_ expected to be in the model prediction. _To conclude, all answers_ are expected in the model prediction _IFAND ONLY IP_'it can be found in the passages. _Crucial auxiliary information_ is expected in the model prediction _IFAND ONLY IP_'it can be found in both the reference response and the passages.

Here are the detailed annotation steps:

_STEP :_ - Read the question and label mistakes made in a model-predicted response. As explained above, leverage the reference, **BUT** rely on the passages. Decide the mistake type and follow detailed instructions as follows. We encourage you to use CTRL+F/CMD+F for navigating the reference and passages using keywords.

* **Erroneous Span (i.e., substring)**: Highlight each span in the model prediction that contains one of the following errors. Label each span _as short as possible_*
* and make sure each labeled span only contain _one single_ information piece. You will be prompted to select the error type and provide an explanation if apply. For each span, label _only one_ error type. If a span contains multiple errors, select the error type based on the order below (e.g., if a span contains "irrelevant" information, ignore any "inconsistent fact" it contains).
* **[Irrelevant]**: The span contains "irrelevant" information (e.g., neither an answer nor crucial auxiliary information, defined in the first \(2\) sentences in "Important Definitions"). To detect "irrelevant" errors, you do not need to consider whether the information is factually consistent with the passages or not.
* **[Repetitive]**: The span repeats information in its previous text. Provide the previous text being repeated (_as short as possible_). _Note that_ a "repetitive" span should still be marked even if its previous text being repeated contains an factual or coherence issue (defined below). However, if the previous text is "irrelevant", it should be marked as "irrelevant" too.
* **[Inconsistent Fact]**: The span is factually inconsistent with the passages. Enter the passage id and sentence id(s) as evidence. _Note that_ if you find multiple evidences in the passages, mark only one of them. The need for multiple passage ids usually indicates that you should separate the error into multiple ones (due to multiple information pieces).
* **[Unverifiable Fact]**: The span is factually unverifiable (i.e., not mentioned in any passage), after _carefully_ checking all passages. Common sense (e.g., "a bigle" has two wheels") doesn't need to be verified. However, do not count knowledge only commonly known in a specific region/community as commonsense. This can be subjective, and simply follow your best judgment.
* **[Incoherent]**: The span contains major grammar error (ignore minor typos), is uninterpretable, contradicts to common sense, or is not coherent with its context.
* **Missing Information**: Identify information that is expected but missing in the model prediction. Check "_Important Definitions_" above to see how to identify such information. Classify _each piece_ of missing information as **[Missing Answer]** or **[Missing Major/Minor Auxiliary Information]**, and enter the passage id and sentence id(s) as evidence. Mark the missing auxiliary information as _major_ if you think the information is indeed helpful for understanding the answer. Otherwise (e.g., a bit off-topic), mark it as _minor_. Simply follow your best judgment. Follow the same "_Note that_" rule under "_[Inconsistent Fact]_" above. _STEP :_- Correct the model prediction to address your labeled mistakes in STEP 1. **Instead of** copying and pasting the reference into the box, make minimal local edits on the original prediction. Make sure the corrected version is fluent and free of typos. In rare cases when you find no useful information in passages for answering the question, you can leave the box as blank if necessary.

Figure 8: Fine-grained feedback annotation instructions.