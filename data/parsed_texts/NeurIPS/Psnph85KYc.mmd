# Interpretable Graph Networks

Formulate Universal Algebra Conjectures

 Francesco Giannini\({}^{*}\)

CINI, Italy

francesco.giannini@unisi.it &Stefano Fioravanti\({}^{*}\)

Universita di Siena, Italy, JKU Linz, Austria Italy

stefano.fioravanti@unisi.it &Oguzhan Keskin

University of Cambridge, UK

ok313@cam.ac.uk &Alisia Maria Lupidi

University of Cambridge, UK

aml201@cam.ac.uk &Lucie Charlotte Magister

University of Cambridge, UK

lcm67@cam.ac.uk &Pietro Lio

University of Cambridge, UK

pl219@cam.ac.uk &Pietro Barbiero\({}^{*}\)

Universita della Svizzera Italiana, CH

University of Cambridge, UK

barbip@usi.ch

###### Abstract

The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)--one of the fields laying the foundations of modern mathematics--is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.

## 1 Introduction

Universal Algebra (UA, (6)) is one of the foundational fields of modern Mathematics with possible deep impact in all mathematical disciplines, but the complexity of studying abstract algebraic structures hinders scientific progress and discourages many academics. Recently, the emergence of powerful AI technologies empowered researchers to investigate intricate mathematical problems which eluded traditional approaches for decades, leading to the solution of open problems (e.g., (24)) and discovery of new conjectures (e.g., (7)). Yet, universal algebra currently remains an un-investigated realm for AI, primarily for two reasons (i) first

Figure 1: Interpretable graph networks support universal algebra research.

UA deals with infinite objects or even classes of abstract objects, which pose unique challenges for conventional AI techniques, (ii) secondly the field commonly relies on deterministic algorithms, utilized to construct finite models or serve as deterministic theorem provers, such as "mace4" and "prover9". In this sense, we hope that our paper could represent a guidance to further explore the study of UA's problems with AI, e.g. by integrating existing systems, like these mentioned theorem provers, with our methodology, which may suggest novel conjectures, in a unique scheme.

Footnote 1: https://www.cs.unm.edu/~mccune/prover9/.

Universal algebra studies algebraic structures from an abstract perspective. Interestingly, several UA conjectures equivalently characterize algebraic properties using equations or graphs (16). In theory, studying UA properties as graphs would enable the use of powerful AI techniques, such as Graph Neural Networks (GNN, (39)), which excel on graph-structured data. However, two factors currently limit scientific progress. First, the _absence of benchmark datasets_ suitable for machine learning prevents widespread application of AI to UA. Second, _GNNs' opaque reasoning_ obstructs human understanding of their decision process (38). Compounding the issue of GNNs' limited transparency, GNN explainability methods mostly rely on brittle and untrustworthy local/post-hoc methods [(14; 27; 28; 38; 45)] or pre-defined subgraphs for explanations [(2; 42)], which are often unknown in UA.

**Contributions.** In this work, we investigate universal algebra's conjectures through AI (Figure 1). Our work includes three significant contributions. First, we propose a novel algorithm that generates a dataset suitable for training AI models based on an UA equational conjecture. Second, we generate and release the first-ever universal algebra's dataset compatible with AI, which contains more than \(29,000\) lattices and the labels of \(5\) key properties i.e., modularity, distributivity, semi-distributivity, join semi-distributivity, and meet semi-distributivity. And third, we introduce a novel neural layer that makes GNNs fully interpretable, according to Rudin's (38) notion of interpretability. The results of our experiments demonstrate that interpretable GNNs (iGNNs): (i) enhance GNN interpretability without sacrificing task accuracy, (ii) strongly generalize when trained to predict universal algebra's properties, (iii) generate simple concept-based explanations that empirically validate existing conjectures, and (iv) identify subgraphs which could be relevant for the formulation of novel conjectures. As a consequence, our findings demonstrate the potentiality of AI methods for investigating UA problems.

## 2 Background

Universal Algebra is a branch of mathematics studying general and abstract algebraic structures. _Algebraic structures_ are typically represented as ordered pairs \(\mathbf{A}=(A,F)\), consisting of a non-empty set \(A\) and a collection of operations \(F\) defined on the set. UA aims to identify algebraic properties (often in equational form) shared by various mathematical systems. In particular, _varieties_ are classes of algebraic structures sharing a common set of identities, which enable the study of algebraic systems based on their common properties. Prominent instances of varieties that have been extensively studied across various academic fields encompass Groups, Rings, Boolean Algebras, Fields, and many others. A particularly relevant variety of algebras are Lattices (details in Appendix A.3), which are often studied for their connection with logical structures.

**Definition 2.1**.: A _lattice_\(\mathbf{L}\) is an algebraic structure composed by a non-empty set \(L\) and two binary operations \(\vee\) and \(\wedge\), satisfying the commutativity, associativity, idempotency, and absorption axioms.

Equivalently a lattice can be characterized as a partially ordered set in which every pair of elements has a _supremum_\((\vee)\) and an _infimum_\((\wedge)\) (cf. Appendix A.3). Lattices also have formal representations as graphs via _Hasse diagrams_\((L,E)\) (e.g., Figure 2), where each node \(x\in L\) is a lattice element, and directed2 edges \((x,y)\in E\subseteq L\times L\) represent the ordering relation, such that if \((x,y)\in E\) then \(x\leq_{L}y\) in the ordering of the lattice. A _sublattice_\(\mathbf{L}^{\prime}\) of a lattice \(\mathbf{L}\) is a lattice such that \(L^{\prime}\subseteq L\) and \(\mathbf{L}^{\prime}\) preserves the original order (the "essential structure") of \(L\), i.e. for all \(x,y\in L^{\prime}\) then \(x\leq_{L^{\prime}}y\) if and only if \(x\leq_{L}y\). The foundational work by Birkhoff (5), Dedekind (9), and Jonsson (18) played a significant role in discovering that some significant varieties of lattices can be characterized through the omission of one or more lattices. Specifically, a variety \(\mathcal{V}\) of lattices is said to _omit_ a lattice \(\mathbf{L}\) if the latter cannot be identified as a sublattice of any lattice in \(\mathcal{V}\). A parallel line of work in UA

Figure 2: Hasse diagrams.

[MISSING_PAGE_FAIL:3]

on large graph structures without explicitly checking them. Using Algorithm 1, we generated the first large-scale AI-compatible datasets of lattices containing more than \(29,000\) graphs and the labels of \(5\) key properties of lattice (quasi-)varieties i.e., modularity, distributivity, semi-distributivity, join semi-distributivity, and meet semi-distributivity, whose definitions can be found in Appendix A.

Scalability and Complexity. The scalability of the dataset generation process is related to two main points: (i) dealing with the exponential growth of the number of lattices, (ii) how to feasibly generate a lattice of a finite amount of elements. (i) Since the number of lattices definable on a set \(L=\{0,...n-1\}\) increases exponentially with \(n\), it is not feasible to realize a dataset containing all the lattices of \(n\) elements. However, this is also unnecessary for the scope of this work, which aims at automatically identifying (small) topological patterns responsible for the failure of certain algebraic properties. In addition, the majority of the graphs generated are isomorphic, thus not particularly informative for our task. Because of this, we only generate a portion of all the lattices up to a certain value \(M>0\), and then take a fixed number of samples \(n_{s}\) up to \(N\) (we take \(M=8\), \(n_{s}=20\), \(N=50\) in the paper). (ii) The construction of a lattice with \(n\) elements is briefly sketched in Algorithm 1. We refer to Appendix B for the technical details on the design of the used functions.

### Interpretable Graph Networks (iGNNs)

In this section, we design an interpretable graph network (iGNN, Figure 3) that satisfies the notion of "interpretability" introduced by Rudin (38). According to this definition, a machine learning (ML) system is interpretable if and only if (1) its inputs are semantically meaningful, and (2) its model inference is simple for humans to understand (e.g., sparse and/or symbolic). This definition covers ML systems that take tabular datasets or sets of concepts as inputs, and (piece-wise) linear models such as logistic regression or decision trees. To achieve this goal in GNNs, we introduce an interpretable graph layer that learns semantically meaningful concepts and uses them as inputs for a simple linear classification layer. We then show how this layer can be included into existing architectures or into hierarchical iGNNs, which consist of a sequence of interpretable graph layers. We notice that in devising our approach, we preferred to rely on linear classifiers as they are fully differentiable, hence allowing us to realize a fully interpretable and differentiable model from the input to the classification head. However,in practice any interpretable and differentiable classifier could be used in place of this linear layer.

#### 3.2.1 Interpretable Graph Layer

The interpretable graph layer (Figure 3) serves three main functions: message passing, concept generation, and task predictions. The first step of the interpretable graph layer involves a standard message passing operation (Eq. 1 right), which aggregates information from node neighbors. This operation enables to share and process relational information across nodes and it represents the basis of any GNN layer.

Node-level concepts.An interpretable concept space is the first step towards interpretability. Following Ghorbani et al. (10), a relevant concept is a "high-level human-understandable unit of

Figure 3: An interpretable graph layer (i) aggregates node features with message passing, (ii) generates a node-level concept space with a hard Gumbel-Softmax activation \(\Theta\), (iii) generates a graph-level concept space with an interpretable permutation invariant pooling function \(\boxplus\) on node-level concepts, and (iv) predicts a task label with an interpretable classifier \(f\) using graph-level concepts.

information" shared by input samples and thus identifiable with clustering techniques. Message passing algorithms do cluster node embeddings based on the structure of node neighborhoods, as observed by Magister et al. (28). However, the real-valued large embedding representations \(\mathbf{h}_{i}\in\mathbb{R}^{q},q\in\mathbb{N}\) generated by message passing can be challenging for humans to interpret. To address this, we use a hard Gumbel-Softmax activation \(\Theta:\mathbb{R}^{q}\mapsto\{0,1\}^{q}\), following Azzolin et al. (2):

\[\mathbf{c}_{i}=\Theta\big{(}\mathbf{h}_{i}\big{)}\qquad\mathbf{h}_{i}=\phi \Big{(}\mathbf{x}_{i},\bigoplus_{j\in N_{i}}\psi(\mathbf{x}_{i},\mathbf{x}_{j} )\Big{)}\] (1)

where \(\psi\) is a learnable function ignoring or assuming constant input features and \(\phi\) is a learnable function aggregating information from a node neighborhood \(N_{i}\), and \(\oplus\) is a permutation invariant aggregation function (such as sum or mean). During the forward pass, the Gumbel-Softmax activation \(\Theta\) produces a one-hot encoded representation of each node embedding. Since nodes sharing the same neighborhood have similar embeddings \(\mathbf{h}_{i}\) due to message passing, they will also have the same one-hot vector \(\mathbf{c}_{i}\) due to the Gumbel-Softmax, and vice versa - we can then interpret nodes having the same one-hot concept \(\mathbf{c}_{i}\) as nodes having similar embeddings \(\mathbf{h}_{i}\) and thus sharing a similar neighborhood. More formally, we can assign a semantic meaning to a reference concept \(\gamma\in\{0,1\}^{q}\) by visualizing concept prototypes corresponding to the inverse images of a node concept vector. In practice, we can consider a subset of the input lattices \(\Gamma\) corresponding to the node's (\(p\)-hop) neighborhood covered by message passing:

\[\Gamma(\gamma,p)=\Big{\{}\mathbf{L}^{(i,p)}\mid i\in L\land\;\mathbf{L}\in \mathcal{D}\land\;\mathbf{c}_{i}=\gamma\Big{\}}\] (2)

where \(\mathcal{D}\) is the set of all training lattices, and \(\mathbf{L}^{(i,p)}\) is the graph corresponding to the \(p\)-hop neighborhood (\(p\in\{1,\ldots,|L|\}\)) of the node \(i\in L\), as suggested by Ghorbani et al. (10); Magister et al. (28). This way, by visualizing concept prototypes as subgraph neighborhoods, the meaning of the concept representation becomes easily interpretable to humans (Figure 3), aiding in the understanding of the reasoning process of the network.

**Example 3.1** (Interpreting node-level concepts).: Consider the problem of classifying distributive lattices with a simplified dataset including \(\mathbf{N}_{5}\) and \(\mathbf{M}_{3}\) only, and where each node has a constant feature \(x_{i}=1\). As these two lattices only have nodes with 2 or 3 neighbours, one layer of message passing will then generate only two types of node embeddings e.g., \(\mathbf{h}_{II}=[0.2,-0.4,0.3]\) for nodes with a 2-nodes neighborhood (e.g., ), and \(\mathbf{h}_{III}=[0.6,0.2,-0.1]\) for nodes with a 3-nodes neighborhood (e.g., ). As a consequence, the Gumbel-Softmax will only generate two possible concept vectors e.g., \(\mathbf{c}_{II}=[0,0,1]\) and \(\mathbf{c}_{III}=[1,0,0]\). Hence, for instance the concept belongs to \(\mathbf{c}_{II}\), while \(\mathcal{C}\) belongs to \(\mathbf{c}_{III}\).

Graph-level concept embeddings.To generate a graph-level concept space in the interpretable graph layer, we can utilize the node-level concept space produced by the Gumbel-Softmax. Normally, graph-level embeddings are generated by applying a permutation invariant aggregation function on node embeddings. However, in iGNNs we restrict the options to (piece-wise) linear permutation invariant functions in order to follow our interpretability requirements dictated by Rudin (38). This restriction still includes common options such as max or sum pooling. Max pooling can easily be interpreted by taking the component-wise max over the one-hot encoded concept vectors \(\mathbf{c}_{i}\). After max pooling, the graph-level concept vector has a value of \(1\) at the \(k\)-th index if and only if at least one node activates the \(k\)-th concept i.e., \(\exists i\in L,\mathbf{c}_{ik}=1\). Similarly, we can interpret the output of a sum pooling: a graph-level concept vector takes a value \(v\in\mathbb{N}\) at the \(k\)-th index after sum pooling if and only if there are exactly \(v\) nodes activating the \(k\)-th concept i.e., \(\exists i_{0},\ldots,i_{v}\in L,\mathbf{c}_{ik}=1\).

**Example 3.2** (Interpreting graph-level concepts).: Following Example 3.1, let us use sum pooling to generate graph-level concepts. For an \(\mathbf{N}_{5}\) graph, we have 5 nodes with exactly the same 2-node neighborhood. Therefore, sum pooling generates a graph-level embedding \([0,0,5]\), which certifies that we have 5 nodes of the same type e.g.,. For an \(\mathbf{M}_{3}\) graph, the top and bottom nodes have a 3-node neighborhood e.g.,, while the middle nodes have a 2-node neighborhood e.g.,. This means that sum pooling generates a graph-level embedding \([2,0,3]\), certifying that we have 2 nodes of type  and 3 nodes of type.

Interpretable classifier.To prioritize the identification of relevant concepts, we use a classifier to predict the task labels using the concept representations. A black-box classifier like a multi-layer perceptron (36) would not be ideal as it could compromise the interpretability of our model, so instead we use an interpretable linear classifier such as a single-layer network (20). This allows for a completely interpretable and differentiable model from the input to the classification head, as the input representations of the classifier are interpretable concepts and the classifier is a simple linear model which is intrinsically interpretable as discussed by Rudin (38). In fact, the weights of the perceptron can be used to identify which concepts are most relevant for the classification task. Hence, the resulting model can be used not only for classification, but also to interpret and understand the problem at hand.

#### 3.2.2 Interpretable architectures

The interpretable graph layer can be used to instantiate different types of iGNNs. One approach is to plug this layer as the last message passing layer of a standard GNN architecture:

\[\hat{y}=f\Big{(}\boxplus_{i\in K}~{}~{}\Big{(}\Theta\Big{(}\phi^{( K)}\Big{(}\mathbf{h}_{i}^{(K-1)},\bigoplus_{j\in N_{i}}\psi^{(K)}(\mathbf{h}_{i}^ {(K-1)},\mathbf{h}_{j}^{(K-1)})\Big{)}\Big{)}\Big{)}\Big{)}\Big{)}\Big{)}\] (3) \[\mathbf{h}_{i}^{(l)}=\phi^{(l)}\Big{(}\mathbf{h}_{i}^{(l-1)}, \bigoplus_{j\in N_{i}}\psi^{(l)}(\mathbf{h}_{i}^{(l-1)},\mathbf{h}_{j}^{(l-1) })\Big{)}\quad l=1,\dots,K\] (4)

where \(f\) is an interpretable classifier (e.g., single-layer network), \(\boxplus\) is an interpretable piece-wise linear and permutation-invariant function (such as max or sum), \(\Theta\) is a Gumbel-Softmax hard activation function, and \(\mathbf{h}_{i}^{0}=\mathbf{x}_{i}\). In this way, we can interpret the first part of the network as a feature extractor generating well-clustered latent representations from which concepts can be extracted. This approach is useful when we only care about the most complex neighborhoods/concepts. Another approach is to generate a hierarchical transparent architecture where each GNN layer is interpretable:

\[\hat{y}^{(l)}=f\Big{(}\boxplus_{i\in K}\Big{(}\Theta\Big{(}\mathbf{h}_{j}^{(l) }\Big{)}\Big{)}\Big{)}\qquad l=1,\dots,K\] (5)

In this case, we can interpret every single layer of our model with concepts of increasing complexity. The concepts extracted from the first layer represent subgraphs corresponding to the \(1\)-hop neighborhood of a node, those extracted at the second layer will correspond to \(2\)-hop neighborhoods, and so on. These hierarchical iGNNs can be useful to get insights into concepts with different granularities. By analyzing the concepts extracted at each layer, we gain a better understanding of the GNN inference and of the importance of different (sub)graph structures for the classification task.

#### 3.2.3 Training

For the classification layer, the choice of the activation and loss functions for iGNNs depends on the nature of the task at hand and does not affect their interpretability. For classification tasks, we use standard activation functions such as softmax or sigmoid, along with standard loss functions like cross-entropy. For hierarchical iGNNs (HiGNNs), we apply the same loss function at each layer of the concept hierarchy, as their layered architecture enables intermediate supervisions. This ensures that each layer is doing its best to extract the most relevant concepts to solve the task. Internal losses can also be weighted differently to prioritize the formation of optimal concepts of a specific size, allowing the HiGNN to learn in a progressive and efficient way.

## 4 Experimental Analysis

### Research questions

In this section we analyze the following research questions:

* Can GNNs generalize when trained to predict universal algebra's properties? Can interpretable GNNs generalize as well?
* Do interpretable GNNs concepts empirically validate universal algebra's conjectures? How can concept-based explanations suggest novel conjectures?

### Setup

Baselines.For our comparative study, we evaluate the performance of iGNNs and their hierarchical version against equivalent GNN models (i.e., having the same hyperparameters such as number layers, training epochs, and learning rate). For vanilla GNNs we resort to common practice replacing the Gumbel-Softmax with a standard leaky ReLU activation. We exclude from our main baselines prototype or concept-based GNNs pre-defining graph structures for explanations, as for most datasets these structures are unknown. Appendix C covers implementation details. We show more extensive results including local and post-hoc explanations in Appendix G.

Evaluation.We employ three quantitative metrics to assess a model's generalization and interpretability. We use the Area Under the Receiver Operating Characteristic (AUC ROC) curve to assess task generalization. We evaluate generalization under two different conditions: with independently and identically distributed train/test splits, and out-of-distribution by training on graphs up to eight nodes, while testing on graphs with more than eight nodes ("strong generalization" (41)). We further assess generalization under binary and multilabel settings (classifying 5 properties of a lattice at the same time). To evaluate interpretability, we use standard metrics such as completeness (44) and fidelity (37). Completeness4 assesses the quality of the concept space on a global scale using an interpretable model to map concepts to tasks, while fidelity measures the difference in predictions obtained with an interpretable surrogate model and the original model. Finally, we evaluate the meaningfulness of our concept-based explanations by visualizing and comparing the generated concepts with ground truth lattices like e.g. \(\mathbf{M}_{3}\) and \(\mathbf{N}_{5}\), whose omission is known to be significant for modular and distributive properties. All metrics in our evaluation, across all experiments, are computed on test sets using \(5\) random seeds, and reported using the mean and \(95\%\) confidence interval.

Footnote 4: We assess the recall of the completeness as the datasets are very unbalanced towards the negative label.

## 5 Key Findings

iGNNs improve interpretability without sacrificing task accuracy (Figure 4).Our experimental evaluation reveals that interpretable GNNs are able to strike a balance between completeness and fidelity, two crucial metrics that are used to assess generalization-interpretability trade-offs (37). We observe that the multilabel classification scenario, which requires models to learn a more varied and diverse set of concepts, is the most challenging and results in the lowest completeness scores on average. We also notice that the more challenging out-of-distribution scenario results in the lowest completeness and fidelity scores across all datasets. More importantly, our findings indicate that iGNNs achieve optimal fidelity scores, as their classification layer consists of a simple linear function of the learnt concepts which is intrinsically interpretable (38). On the contrary, interpretable surrogate models of black-box GNNs exhibit, as expected, lower fidelity scores, confirming analogous observations in the explainable AI literature [37; 38]. In practice, this discrepancy between the original black-box predictions and the predictions obtained with an interpretable surrogate model questions the actual usefulness of black-boxes when interpretable alternatives achieve similar results in solving the problem at hand, as extensively discussed by Rudin (38). Overall, these results demonstrate how concept spaces are highly informative to solve universal algebra's tasks and how the interpretable graph layer may improve GNNs' interpretability without sacrificing task accuracy. We refer the reader to Appendix E for detailed discussion on quantitative analysis of concept space obtained by iGNNs under different generalization settings with comparisons to their black-box counterparts.

GNNs strongly generalize on universal algebra's tasks (Figure 5).Our experimental findings demonstrate the strong generalization capabilities of GNNs across the universal algebra tasks we designed. Indeed, we stress GNNs test generalization abilities by training the models on graphs of size up to \(n\) (with \(n\) ranging from \(5\) to \(8\)), and evaluating their performance on much larger graphs of size up to \(50\). We designed this challenging experiment in order to understand the limits and robustness of interpretable GNNs when facing a significant data distribution shift from training to test data. Remarkably, iGNNs exhibit robust generalization abilities (similar to their black-box counterparts) when trained on graphs up to size \(8\) and tested on larger graphs. This evidence confirms the hypothesis that interpretable models can deliver reliable and interpretable predictions, as suggested by Rudin(38). However, we observe that black-box GNNs slightly outperform iGNNs when trained on even smaller lattices. We hypothesize that this is due to the more constrained architecture of iGNNs, which imposes tighter bounds on their expressiveness when compared to standard black-box GNNs. Notably, training with graphs of size up to \(5\) or \(6\) significantly diminishes GNNs generalization in the tasks we designed. We hypothesize that this is due to the scarcity of non-distributive and non-modular lattices during training, but it may also suggest that some patterns of size \(7\) and \(8\) might be quite relevant to generalize to larger graphs. Unfortunately, running generalization experiments with \(n\leq 4\) was not possible since all such lattices trivially omitted \(\mathbf{\hat{N}}_{5}\) and \(\mathbf{M}_{3}\). It is worth mentioning that GNNs performed well even in the challenging multilabel case, where they had to learn a wider and more diverse set of concepts and tasks. In all experiments, we observe a plateau of the AUC ROC scores for \(n=8\), thus suggesting that a training set including graphs of this size might be sufficient to learn the relevant patterns allowing the generalization to larger lattice varieties. For detailed numerical results across all tasks, we refer the reader to Table 1 in Appendix D. Overall, these results emphasize the potential of GNNs in addressing complex problems in universal algebra, providing an effective tool to handle lattices that are difficult to analyze manually with pen and paper.

### Interpretability

**Concept-based explanations empirically validate universal algebra's conjectures (Figure 6).** We present empirical evidence to support the validity of theorems 2.3 and 2.4 by examining the concepts generated for modular and distributive tasks. For this investigation we leverage the interpretable structure of iGNNs. Similarly to Ribeiro et al. (37), we visualize in Figure 6 the weights of our trained linear classifier representing the relevance of each concept. We remark that the visualization is limited to the (top-\(5\)) most negative weights, as we are interested in those concepts that negatively affect the prediction of a property. In the same plot, we also show the prototype of each concept represented by the 2-hop neighbor-

Figure 4: Accuracy-interpretability trade-off in terms of concept completeness (accuracy) and model fidelity (interpretability). iGNNs attain optimal fidelity as model inference is inherently interpretable, outmatching equivalent black-box GNNs. All models attain similar results in terms of completeness.

Figure 5: Strong generalization performance with respect to the maximum number of nodes used in training.

Figure 6: Ranking of relevant clusters of lattices (x-axis) according to the interpretable GNN linear classifier weights (y-axis, the lower the more relevant the cluster). \(\mathbf{N}_{5}\) is always the most important sub-lattice to omit for modularity, while both \(\mathbf{M}_{3}\) and \(\mathbf{N}_{5}\) are relevant for distributivity, thus validating theorems 2.3 and 2.4.

[MISSING_PAGE_FAIL:9]

properties on graphs, while non-structural properties may require the adoption of other kinds of (interpretable) models.

Broader impact and perspectives.AI techniques are becoming increasingly popular for solving previously intractable mathematical problems and proposing new conjectures [(23; 26; 7; 12)]. However, the use of modern AI methods in universal algebra was a novel and unexplored field until the development of the approach presented in this paper. To this end, our method uses interpretable graph networks to suggest graph structures that characterize relevant algebraic properties of lattices. With our approach, we empirically validated Dedekind [(9)] and Birkhoff [(5)] theorems on distributive and modular lattices, by recovering relevant lattices. This approach can be readily extended--beyond equational properties determined by the omission of a sublattice in a variety [(43)]--to any structural property of lattices, including the characterization of congruence lattices of algebraic varieties [(1; 21; 31; 43)]. Our methodology can also be applied (beyond universal algebra) to investigate (almost) any mathematical property that can be topologically characterized on a graph, such as the classes of graphs/diagrams with a fixed set of polymorphisms [(25; 3; 32)].

Conclusion.This paper presents the first-ever AI-assisted approach to investigate equational and topological conjectures in the field of universal algebra. To this end, we present a novel algorithm to generate datasets suitable for AI models to study equational properties of lattice varieties. While topological representations would enable the use of graph neural networks, the limited transparency and brittle explainability of these models hinder their use in validating existing conjectures or proposing new ones. For this reason, we introduce a novel neural layer to build fully interpretable graph networks to analyze the generated datasets. The results of our experiments demonstrate that interpretable graph networks: enhance interpretability without sacrificing task accuracy, strongly generalize when predicting universal algebra's properties, generate simple explanations that empirically validate existing conjectures, and identify subgraphs suggesting the formulation of novel conjectures. These promising results demonstrate the potential of our methodology, opening the doors of universal algebra to AI with far-reaching impact across all mathematical disciplines.

This paper was supported by TAILOR and by HumanE-AI-Net projects funded by EU Horizon 2020 research and innovation programme under GA No 952215 and No 952026, respectively. This paper has been also supported by the Austrian Science Fund FWF project P33878 "Equations in Universal Algebra" and the European Union's Horizon 2020 research and innovation programme under grant agreement No 848077. This project has also received funding from the European Union's Horizon-MSCA-2021 research and innovation program under grant agreement No 101073307.

## References

* [1] Paolo Agliano, Stefano Bartali, and Stefano Fioravanti. On Freese's technique. _International Journal of Algebra and Computation_, 2023. doi: https://doi.org/10.1142/S0218196723500601. URL https://arxiv.org/abs/2302.11452.
* [2] Steve Azzolin, Antonio Longa, Pietro Barbiero, Pietro Lio, and Andrea Passerini. Global explainability of gnns via logic combination of learned concepts. _arXiv preprint arXiv:2210.07147_, 2022.
* [3] Libor Barto, Marcin Kozik, and Todd Niven. The csp dichotomy holds for digraphs with no sources and no sinks (a positive answer to a conjecture of bang-jensen and hell). _SIAM Journal on Computing_, 38(5):1782-1802, 2009.
* [4] Joel Berman and Pawel Idziak. Counting finite algebras in the post varieties. _International Journal of Algebra and Computation_, 10(03):323-337, 2000.
* [5] Garrett Birkhoff. On the structure of abstract algebras. In _Mathematical proceedings of the Cambridge philosophical society_, volume 31, pages 433-454. Cambridge University Press, 1935.
* [6] Stanley Burris and H. P. Sankappanavar. _A Course in Universal Algebra_. Springer, 1981.

* Davies et al. [2021] Alex Davies, Petar Velickovic, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomasev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andras Juhasz, Marc Lackenby, Geordie Williamson, Demis Hassabis, and Pushmeet Kohli. Advancing mathematics by guiding human intuition with AI. _Nature_, 600(7887):70-74, December 2021. doi: 10.1038/s41586-021-04086-x. URL https://doi.org/10.1038/s41586-021-04086-x.
* Day [1969] Alan Day. A characterization of modularity for congruence lattices of algebras*. _Canadian Mathematical Bulletin_, 12(2):167-173, 1969. doi: 10.4153/CMB-1969-016-6.
* Dedekind [1900] Richard Dedekind. Uber die von drei Moduln erzeugte Dualgruppe. _Math. Ann._, 1900.
* Ghorbani et al. [2019] Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based explanations. _Advances in Neural Information Processing Systems_, 32, 2019.
* Gilmer et al. [2017] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International conference on machine learning_, pages 1263-1272. PMLR, 2017.
* He [2022] Yang-Hui He. Machine-learning mathematical structures. _International Journal of Data Science in the Mathematical Sciences_, pages 1-25, 2022.
* Heitzig and Reinhold [2002] Jobst Heitzig and Jurgen Reinhold. Counting finite lattices. _Algebra universalis_, 48(1):43-53, 2002.
* Huang et al. [2022] Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable model explanations for graph neural networks. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* Hyland and Power [2007] Martin Hyland and John Power. The category theoretic understanding of universal algebra: Lawvere theories and monads. _Electronic Notes in Theoretical Computer Science_, 172:437-458, 2007. ISSN 1571-0661. doi: https://doi.org/10.1016/j.entcs.2007.02.019. URL https://www.sciencedirect.com/science/article/pii/S1571066107000874. Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin.
* Jipsen and Rose [1992] Peter Jipsen and Henry Rose. _Varieties of Lattices_. Springer Berlin, 1992.
* Jonsson [1967] Bjarni Jonsson. Algebras whose congruence lattices are distributive. _MATHEMATICA SCANDINAVICA_, 21:110-121, Dec. 1967. doi: 10.7146/math.scand.a-10850. URL https://www.mscand.dk/article/view/10850.
* Jonsson [1953] Bjarni Jonsson. On the representation of lattices. _Mathematica Scandinavica_, 1953.
* Jonsson and Rival [1978] Bjarni Jonsson and Ivan Rival. Lattice varieties covering the smallest non-modular lattice variety. _Pacific J. Math._, Volume 82, 1978.
* Karimboyevich and Nematullayevich [2022] Samandarov Erkaboy Karimboyevich and Abdurakhmonov Olim Nematullayevich. Single layer artificial neural network: Perceptron. _European Multidisciplinary Journal of Modern Science_, 5:230-238, Apr. 2022. URL https://emjms.academicjournal.io/index.php/emjms/article/view/253.
* Kearnes and Kiss [2013] Keith Kearnes and Emil Kiss. The shape of congruence lattices. _Memoirs of the American Math. Soc._, 2013.
* Keskin et al. [2023] Oguzhan Keskin, Alisia Maria Lupidi, Stefano Fioravanti, Lucie Charlotte Magister, Pietro Barbiero, Pietro Lio, and Francesco Giannini. Bridging equational properties and patterns on graphs: an ai-based approach. In _Topological, Algebraic and Geometric Learning Workshops 2023_, pages 156-168. PMLR, 2023.
* Lample and Charton [2019] Guillaume Lample and Francois Charton. Deep learning for symbolic mathematics. In _International Conference on Learning Representations_.
* Lample and Charton [2019] Guillaume Lample and Francois Charton. Deep learning for symbolic mathematics. _arXiv preprint arXiv:1912.01412_, 2019.

* [25] Barto Libor and Kozik Marcin. Absorbing Subalgebras, Cyclic Terms, and the Constraint Satisfaction Problem. _Logical Methods in Computer Science_, Volume 8, Issue 1, 2012.
* [26] Donald W Loveland. _Automated theorem proving: A logical basis_. Elsevier, 2016.
* [27] Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Parameterized explainer for graph neural network. _Advances in neural information processing systems_, 33:19620-19631, 2020.
* [28] Lucie Charlotte Magister, Dmitry Kazhdan, Vikash Singh, and Pietro Lio. Geexplainer: Human-in-the-loop concept-based explanations for graph neural networks. _arXiv preprint arXiv:2107.11889_, 2021.
* [29] Lucie Charlotte Magister, Pietro Barbiero, Dmitry Kazhdan, Federico Siciliano, Gabriele Ciravegna, Fabrizio Silvestri, Mateja Jamnik, and Pietro Lio. Encoding concepts in graph neural networks. _arXiv preprint arXiv:2207.13586_, 2022.
* [30] Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In _International Conference on Machine Learning_, pages 15524-15543. PMLR, 2022.
* [31] J. B. Nation. Varieties whose congruences satisfy certain lattice identities. _Algebra Universalis_, 1974.
* [32] Miroslav Olsak. The local loop lemma. _Algebra universalis_, 2020.
* [33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _arXiv preprint arXiv:1912.01703_, 2019.
* [34] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learning in python. _the Journal of machine Learning research_, 12:2825-2830, 2011.
* [35] Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10772-10781, 2019.
* [36] Marius-Constantin Popescu, Valentina Balas, Liliana Perescu-Popescu, and Nikos Mastorakis. Multilayer perceptron and neural networks. _WSEAS Transactions on Circuits and Systems_, 8, 07 2009.
* [37] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.
* [38] Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. _Nature machine intelligence_, 1(5):206-215, 2019.
* [39] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. _IEEE transactions on neural networks_, 20(1):61-80, 2008.
* [40] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. _arXiv preprint arXiv:1312.6034_, 2013.
* [41] Petar Velickovic, Rex Ying, Matilde Padovano, Raia Hadsell, and Charles Blundell. Neural execution of graph algorithms. In _International Conference on Learning Representations_, 2019.
* [42] Minh Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. _Advances in neural information processing systems_, 33:12225-12235, 2020.
* [43] Philip M Whitman. Free lattices. _Annals of Mathematics_, pages 325-330, 1941.

* [44] Chih-Kuan Yeh, Been Kim, Sercan Arik, Chun-Liang Li, Tomas Pfister, and Pradeep Ravikumar. On completeness-aware concept-based explanations in deep neural networks. _Advances in Neural Information Processing Systems_, 33:20554-20565, 2020.
* [45] Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks, 2019.
* [46] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Cheekong Lee. Protgnn: Towards self-explaining graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 9127-9135, 2022.

Algebra definitions

### Formal defintions for Universal Algebra

Universal algebra is the field of mathematics that studies algebraic structures, which are defined as a set \(A\) along with its own collection of operations. In this section, we recall some basic definitions and theorems from Burris and Sankappanavar [6]; Day [8]; Jonnson [17], about elements of universal algebra and lattice theory.

**Definition A.1**.: **N-ary function** For a non-empty set \(A\) and \(n\) non-negative integer we define \(A^{0}=\{\emptyset\}\) and, for \(n>0\), \(A^{n}\) is the set of n-tuples of elements from \(A\). An \(n\)-ary _operation_ on \(A\) is any function \(f\) from \(A^{n}\) to \(A\); \(n\) is the _arity_ of \(f\). An operation \(f\) on \(A\) is called an \(n\)-ary operation if its arity is \(n\).

**Definition A.2**.: **Algebraic Structure** An _algebra_\(\mathbf{A}\) is a pair \((A,F)\) where \(A\) is a non-empty set called _universe_ and \(F\) is a set of finitary operations on \(A\).

Apart from the operations on \(A\), an algebra is further defined by axioms, that in the particular case of universal algebras are in the form of identities.

**Definition A.3**.: A _lattice_\(\mathbf{L}\) is an algebraic structure composed by a non-empty set \(L\) and two binary operations \(\vee\) and \(\wedge\) satisfying the following axioms and their duals obtained exchanging \(\vee\) and \(\wedge\):

\[x\lor y\approx y\lor x\] (commutativity) \[x\vee(y\lor z)\approx(x\lor y)\] (associativity) \[x\lor x\approx x\] (idempotency) \[x\approx x\vee(x\wedge y)\] (absorption)

**Theorem A.4** ((6)).: _A partially ordered set \(L\) is a lattice if and only if for every \(a,b\in L\) both supremum and infimum of \(\{a,b\}\) exist (in \(L\)) with \(a\lor b\) being the supremum and \(a\wedge b\) the infimum._

**Definition A.5**.: Let \(\mathbf{L}\) be a lattice. Then \(\mathbf{L}\) is _modular_ (_distributive_, \(\vee\)_-semi-distributive_, \(\wedge\)_-semi-distributive_) if it satisfies the following:

\[x\leq y\to x\vee(y\wedge z)\approx y\wedge(x\lor z)\] (modularity) \[x\vee(y\wedge z)\approx(x\lor y)\wedge(x\lor z)\] (distributivity) \[x\lor y\approx x\lor z\to x\vee(y\wedge z)\approx x\lor y\] (\[\vee\] semi-distributivity) \[x\wedge y\approx x\wedge z\to x\wedge(y\lor z)\approx x\wedge y\] (\[\wedge\] semi-distributivity).

Furthermore a lattice \(\mathbf{L}\) is semi-distributive if is both \(\vee\)-semi-distributive and \(\wedge\)-semi-distributive

Proof.: By assuming \(x\leq y\), we have \(x\lor y=y\). Hence, from the distributive property we get:

\[x\vee(y\wedge z)\approx(x\lor y)\wedge(x\lor z)\approx y\wedge(x\lor z)\]

Figure 8: \(\mathbf{N}_{5}\), a non-modular non-distributive and \(\mathbf{M}_{3}\), a modular non-distributive lattice.

**Definition A.7**.: **Congruence Lattice**

An _equivalence relation_ on a set \(A\) is a binary relation \(\sim\) that satisfies three properties: reflexivity, symmetry, and transitivity.

Reflexivity: For every element \(a\) in \(A\), \(a\) is related to itself, denoted as \(a\sim a\);

Symmetry: For any elements \(a\) and \(b\) in \(A\), if \(a\sim b\), then \(b\sim a\);

Transitivity: For any elements \(a\), \(b\), and \(c\) in \(A\), if \(a\sim b\) and \(b\sim c\), then \(a\sim c\).

In other words, an equivalence relation partitions the set \(A\) into subsets, called _equivalence classes_, such that elements within the same class are equivalent to each other under the relation \(\sim\).

Let \(\mathbf{A}\) be an algebra. A _congruence_\(\theta\) of \(\mathbf{A}\) is a equivalent relation on \(A\), that is compatible with the operations of \(\mathbf{A}\). Formally, for every \(n\)-ary operation \(f\) of \(\mathbf{A}\): if \((a_{1},b_{1}),(a_{2},b_{2}),\ldots,(a_{n},b_{n})\in\theta\), then \((f(a_{1},a_{2},\ldots,a_{n}),f(b_{1},b_{2},\ldots,b_{n}))\in\theta\). For every algebra \(\mathbf{A}\) on the set \(A\), the identity relation on \(A\), and \(A\times A\) are trivial congruences. An algebra with no other congruences is called _simple_. Let \(\mathrm{Con}(\mathbf{A})\) be the set of congruences on the algebra \(\mathbf{A}\). Since congruences are closed under intersection, we can define a meet operation: \(\wedge:\mathrm{Con}(\mathbf{A})\times\mathrm{Con}(\mathbf{A})\rightarrow \mathrm{Con}(\mathbf{A})\) by simply taking the intersection of the congruences \(E_{1}\wedge E_{2}=E_{1}\cap E_{2}\). Congruences are not closed under union, however we can define the following closure operator of a binary relation \(E\), with respect to a fixed algebra \(\mathbf{A}\), such that its image is congruence: \(\langle E\rangle_{\mathbf{A}}=\bigcap\{F\in\mathrm{Con}(\mathbf{A})\mid E\subseteq F\}\). Note that the closure of a binary relation is a congruence and thus depends on the operations in \(\mathbf{A}\), not just on the base set. Now define \(\vee:\mathrm{Con}(\mathbf{A})\times\mathrm{Con}(\mathbf{A})\rightarrow \mathrm{Con}(\mathbf{A})\) as \(E_{1}\lor E_{2}=\langle E_{1}\cup E_{2}\rangle_{\mathbf{A}}\). For every algebra \(\mathbf{A}\), \((\mathrm{Con}(\mathbf{A}),\wedge,\vee)\) with the two operations defined above forms a lattice, called the _congruence lattice_ of \(\mathbf{A}\).

A _type_\(\mathcal{F}\) is defined as a set of operation symbols along with their respective arities. Each operation symbol represents a specific operation that can be performed on the elements of the algebraic system. To refer to the specific operation performed by a given symbol \(f\) on an algebra \(\mathbf{A}\) of type \(\mathcal{F}\), we denote it as \(f^{\mathbf{A}}\). This notation allows us to differentiate and access the particular operation carried out by \(f\) within the context of \(\mathbf{A}\).

**Definition A.8**.: **Subalgebra** Let \(\mathbf{A}\) and \(\mathbf{B}\) be two algebras of the same type. Then \(\mathbf{B}\) is a _subalgebra_ of \(\mathbf{A}\) if \(B\subseteq A\) and every fundamental operation of \(\mathbf{B}\) is the restriction of the corresponding operation of \(\mathbf{A}\), i.e., for each function symbol \(f\), \(f^{\mathbf{B}}\) is \(f^{\mathbf{A}}\) restricted to \(\mathbf{B}\).

**Definition A.9**.: **Homomorphic image** Suppose \(\mathbf{A}\) and \(\mathbf{B}\) are two algebras of the same type \(\mathcal{F}\), i.e. for each operation of \(\mathbf{A}\), there exists a corresponding operation \(\mathbf{B}\) with the same arity, and vice versa. A mapping \(\alpha:A\to B\) is called a _homomorphism_ from \(\mathbf{A}\) to \(\mathbf{B}\) if

\[\alpha f^{\mathbf{A}}(a_{1},\ldots,a_{n})=f^{\mathbf{B}}(\alpha a_{1},\ldots, \alpha a_{n})\]

for each n-ary \(f\) in \(\mathcal{F}\) and each sequence \(a_{1},\ldots,a_{n}\) from \(\mathbf{A}\). If, in addition, the mapping \(\alpha\) is onto then \(\mathbf{B}\) is said to be a _homomorphic image_ of \(\mathbf{A}\).

**Definition A.10**.: **Direct product** Let \(\mathbf{A}_{1}\) and \(\mathbf{A}_{2}\) be two algebras of the same type \(\mathcal{F}\). We define the direct product \(\mathbf{A}_{1}\times\mathbf{A}_{2}\) to be the algebra whose universe is the set \(A_{1}\times A_{2}\), and such that for \(f\in\mathcal{F}\) and \(a_{i}\in A_{1}\), \(a^{\prime}_{i}\in A_{2}\), \(1\leq i\leq n\),

\[f^{\mathbf{A}_{1}\times\mathbf{A}_{2}}(\langle a_{1},a^{\prime}_{1}\rangle, \ldots,\langle a_{n},a^{\prime}_{n}\rangle=\langle f^{\mathbf{A}_{1}}(a_{1}, \ldots,a_{n}),f^{\mathbf{A}_{2}}(a^{\prime}_{1},\ldots,a^{\prime}_{n})\rangle\]

The collection of algebraic structures defined by equational laws are called varieties. (15)

**Definition A.11**.: **Variety** A nonempty class K of algebras of type \(\mathcal{F}\) is called a _variety_ if it is closed under subalgebras, homomorphic images, and direct products.

## Appendix B Algorithm 1 details

In the following, we report some technical details on how the dataset generator sketched in Algorithm 1 is actually implemented. First, notice that a brute-force approach is infeasible for large lattices, as given a set of \(n\) nodes, the number of binary relations on this set is \(2^{n^{2}}\). To cope with this issue, firstfor each candidate lattice \(\mathbf{L}\) we consider a squared \(n\times n\) matrix \(\leq_{L}\) representing its order that has 1 value in a position \((i,j)\) if and only if the element \(i\) is less or equal to the element \(j\) in \(\mathbf{L}\) (i.e. \(i\leq_{L}j\)) and 0 otherwise. Then we constraint each matrix to have 1 in the diagonal (reflexivity), 0 in each \((i,j)\) with \(j<i\), where "\(<\)" denotes denotes the order on \(\mathbb{N}\) (this choice both prunes the majority of isomorphic lattices and yields anti-symmetric). All the other pairs of elements \((i,j)\) can either be such that \(i\leq_{L}j\) or incomparable (i.e. \(i\not\leq_{L}j\) and \(j\not\leq_{L}i\)), and we consider all these possible cases. Finally, we apply matrix multiplications to get a transitive closure of the order relation (convergence guaranteed in at most \(n-2\) steps) and hence \(\leq_{L}\) represents a partial order. To assure that the order represents a lattice, we have to check that each pair of elements \((i,j)\) admits a (unique!) infimum and supremum. This step and checking lattice equational properties are implemented tensorially to leverage GPU quicker computations, hence being particularly advantageous when the dimensions of the lattices is such that the computational cost of Algorithm 1 surpasses the overhead of GPU communication. Finally, we notice that even avoiding the isomorphic lattices, for \(n=18\) there are around 165Bn non-isomorphic different lattices (13). This is why we sampled a fixed number of lattices as \(n\) increases, e.g. 20 samples for cardinality after a certain threshold, instead of keeping generating all the possible lattices for each value of \(n\), which is not particularly relevant for our task. Whereas it allows us to study the strong generalization capability of GNNs trained on e.g. up to \(n=8\) nodes and then evaluated on lattices of higher dimensions, e.g. \(n=50\) nodes (see Figure 5). Notice that checking e.g. the distributivity for \(n=50\) is deterministic but requires checking \(50^{5}\) identities.

Running time. The running time of the algorithm increases polynomially in the size \(n\) of the given lattice (it is \(O(n^{3})\) for checking each of the equational properties, e.g. distributivity/modularity, and \(O(n^{4})\) to check if a candidate binary relation is actually a lattice). In a machine with a single quad-core CPU, it requires 20 minutes to generate all the lattices up to \(N=8\) and sampling \(n_{s}=20\) lattices for \(n\in[9,50]\) (the dataset we used in the paper).

## Appendix C Baselines' details

In practice, we train all models using eight message passing layers and different embedding sizes ranging from \(16\) to \(64\). We train all models for \(200\) epochs with a learning rate of \(0.001\). For interpretable models, we set the Gumbel-Softmax temperature to the default value of 1 and the activation behavior to "hard," which generates one-hot encoded embeddings in the forward pass, but computes the gradients using the soft scores. For the hierarchical model, we set the internal loss weight to \(0.1\) (to score it roughly \(10\%\) less w.r.t. the main loss). Overall, our selection of baselines aims at embracing a wide set of training setups and architectures to assess the effectiveness and versatility of GNNs for analyzing lattice properties in universal algebra. To demonstrate the robustness of our approach, we implemented different types of message passing layers, including graph convolution and GIN.

## Appendix D Generalization results details

Here we report the raw numbers for the weak and strong generalization results reported as a figure in the main paper. The results are obtained by setting maximum lattice size to 8 in training and using lattices of size 9 or larger during evaluation. All models provide near perfect performance for binary classification and perform slightly worse but still very competitively for multi-label classification. This demonstrates the strong generalization capability of GNNs for universal algebra tasks, and may be an ideal starting point to finding new relevant patterns in UA properties.

In Table 1 we also include a quantitative comparison with GSAT (30), and observe that GSAT and iGNNs obtain comparable results in terms of task generalization and concept completeness (cf. Table 4) when trained on the proposed UA's tasks.

## Appendix E Concept completeness and purity

Our experimental results (Tables 2 & 4) demonstrate that interpretable GNNs produce concepts with high completeness and low purity, which are standard quantitative metrics used to evaluate the quality of concept-based approaches. Completeness score is the accuracy of a classifier, such as decision tree, which takes concepts as inputs and predicts a label. Purity score is the number of graph edits, such as node/edge addition/eliminations, necessary to match two graphs in a cluster. A concept space is said to be pure if the purity score is zero.

We employ decision tree as the classifier, but compute recall instead of accuracy to calculate completeness score since the datasets are heavily unbalanced towards the negative labels. We compute purity scores for each cluster and report the average of those scores as the final purity score. Our approach achieves at least 73% and up to 87% recall, which shows that our interpretable models consistently avoid false negatives in the abundance of negative labels. We obtain around 3-4 purity scores, which suggests that our interpretable models extract relatively pure concept spaces in the presence of large lattices.

Furthermore, the hierarchical structure of interpretable GNNs enables us to evaluate the quality of intermediate concepts layer by layer. This hierarchy provides insights into why we may need more layers, and it can be used as a valuable tool to find the optimal setup and tune the size of the architecture. Additionally, it can also be used to compare the quality of concepts at different layers of the network. To that end, we compare the purity scores of the concept spaces obtained by the second layer and the final layer of HiGNN. As shown in Table 3, deeper layers may produce higher quality concepts for distributivity and join semi-distributivity whereas earlier layers may result in more reliable concepts for the remaining properties. Overall, these results quantitatively assess and validate the high quality of the concepts learned by the interpretable GNNs, highlighting the effectiveness of this approach for learning and analyzing the algebraic structures.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c} \hline  & \multicolumn{3}{c|}{**weak CPHTERS**} & \multicolumn{3}{c}{**strong CPHTERS**} \\  & **GCExplainer** & **GSAT** & **IGNN** & **GCExplainer** & **GSAT** & **IGNN** \\ \hline
**Distributive** & \(98.0\pm 0.04\) & \(96.73\pm 0.44\) & \(96.3\pm 0.12\) & \(99.45\pm 0.06\) & \(99.51\pm 0.29\) & \(99.26\pm 0.30\) & \(99.44\pm 0.05\) & \(99.22\pm 0.04\) \\
**Join Semi Distributive** & \(99.49\pm 0.02\) & \(98.33\pm 0.06\) & \(98.31\pm 0.15\) & \(98.28\pm 0.04\) & \(99.87\pm 0.15\) & \(97.76\pm 0.07\) & \(97.50\pm 0.14\) & \(97.48\pm 0.14\) \\
**Meet Semi Distributive** & \(99.52\pm 0.04\) & \(98.36\pm 0.04\) & \(98.19\pm 0.06\) & \(98.25\pm 0.08\) & \(98.90\pm 0.03\) & \(97.85\pm 0.10\) & \(97.18\pm 0.14\) & \(96.98\pm 0.37\) \\
**Modular** & \(99.77\pm 0.02\) & \(96.62\pm 0.31\) & \(99.18\pm 0.11\) & \(99.35\pm 0.09\) & \(99.32\pm 0.22\) & \(99.63\pm 0.31\) & \(99.21\pm 0.14\) & \(99.11\pm 0.22\) \\
**Semi Distributive** & \(99.66\pm 0.03\) & \(98.76\pm 0.04\) & \(98.57\pm 0.02\) & \(98.05\pm 0.06\) & \(99.19\pm 0.04\) & \(98.14\pm 0.12\) & \(97.28\pm 0.48\) & \(96.88\pm 0.47\) \\
**Multi Label** & \(99.60\pm 0.02\) & \(95.90\pm 0.52\) & \(96.32\pm 0.34\) & \(94.98\pm 0.50\) & \(98.62\pm 0.43\) & \(94.45\pm 0.38\) & \(95.29\pm 0.55\) & \(95.27\pm 0.32\) \\ \hline \end{tabular}
\end{table}
Table 1: Generalization performance of different graph neural models in solving universal algebra’s tasks. Values represents the mean and the standard error of the mean of the area under the receiver operating curve (AUCROC, %).

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline  & \multicolumn{3}{c|}{**weak CPHTERS**} & \multicolumn{3}{c}{**strong CPHTERS**} \\  & **GCExplainer** & **IGNN** & **HGNN** & **GCExplainer** & **IGNN** & **HGNN** \\ \hline
**Distributive** & \(3.30\pm 0.36\) & \(3.64\pm 0.30\) & \(3.09\pm 0.56\) & \(3.29\pm 0.38\) & \(4.00\pm 0.77\) & \(4.15\pm 0.67\) \\
**Join Semi Distributive** & \(2.38\pm 0.37\) & \(3.96\pm 0.51\) & \(3.74\pm 0.62\) & \(3.45\pm 0.34\) & \(3.98\pm 0.68\) & \(4.29\pm 0.61\) \\
**Meet Semi Distributive** & \(3.24\pm 0.63\) & \(3.55\pm 0.62\) & \(3.39\pm 0.29\) & \(3.36\pm 0.32\) & \(4.25\pm 0.39\) & \(4.97\pm 0.44\) \\
**Modular** & \(2.31\pm 0.35\) & \(3.50\pm 0.46\) & \(4.44\pm 0.56\) & \(3.14\pm 0.24\) & \(3.19\pm 0.11\) & \(4.25\pm 0.69\) \\
**Semi Distributive** & \(2.84\pm 0.51\) & \(3.70\pm 0.54\) & \(4.11\pm 0.46\) & \(3.70\pm 0.55\) & \(3.92\pm 0.28\) & \(4.08\pm 0.85\) \\ \hline \end{tabular}
\end{table}
Table 2: Concept purity scores of graph neural models in solving universal algebra’s tasks. Lower is better.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline  & \multicolumn{3}{c|}{**weak CPHTERS**} & \multicolumn{3}{c}{**strong CPHTERS**} \\  & **GCExplainer** & **IGNN** & **HGNN** & **GCExplainer** & **IGNN** & **HGNN** \\ \hline
**Distributive** & \(3.36\pm 0.43\) & \(3.09\pm 0.56\) & \(4.66\pm 0.98\) & \(4.15\pm 0.67\) \\
**Join Semi Distributive** & \(4.25\pm 0.69\) & \(3.74\pm 0.62\) & \(4.30\pm 0.39\) & \(4.29\pm 0.61\) \\
**Meet Semi Distributive** & \(3.64\pm 0.39\) & \(3.39\pm 0.29\) & \(4.41\pm 0.27\) & \(4.97\pm 0.44\) \\
**Modular** & \(3.89\pm 0.63\) & \(4.44\pm 0.56\) & \(4.19\pm 0.56\) & \(4.25\pm 0.69\) \\
**Semi Distributive** & \(3.55\pm 0.58\) & \(4.11\pm 0.46\) & \(3.16\pm 0.59\) & \(4.08\pm 0.85\) \\ \hline \end{tabular}
\end{table}
Table 3: Concept purity scores of different layers of HiGNN. Lower is better.

## Appendix F Concept visualization

Figure 9 visualizes 18 randomly sampled graph concepts (out of the 7896 graph concepts represented by different graph encodings) following the visualization procedure introduced by (28). The figure shows for each concept an example of four (randomly sampled) graphs having the same concept label in the 7-th layer of the hierarchical iGNN trained on the multilabel dataset. Graphs belonging to the same concept show a coherency in their structure and similar patterns. These patterns represent the knowledge extracted and discovered by the hierarchical iGNN.

Figure 9: Examples of graph concepts.

Explanations of post-hoc explainers

We compared our Explainable Hierarchical GNN against a standard explainer (namely GNNExplainer (45)) to further support our results. GNNExplainer is the first general, model-agnostic approach for providing interpretable explanations for predictions of any GNN-based model on any graph-based machine learning task and it is widely used in the scientific community as one of the staple explainers in GNN's XAI. In this particular setting, GNNExplainer was configured as follows: model-wise explanation on multiclass-node level classification task, with HiGNN as the model of choice, and GNNExplainer as the desired algorithm, trained for 200 epochs. The explainer takes as input a single graph in the dataset and outputs and explanation for its classification. GNNExplainer will enforce a classification based on the presence or omission of \(M_{3}\) and/or \(N_{5}\) and it is possible to visualize the subgraph that lead to this classification by leveraging the visualize_graph function. By doing this, we retrieve the following visualizations:

On the right, the substructure identified as \(N5\) by GNNExplainer which lead to the classification of said graph as non modular and non distributive. On the right, in green \(M3\). Our hierarchical model arrives to the same conclusions as the standard explainer but can also be augmented with a standard explainer.

## Appendix H Code, Licences, Resources

LibrariesFor our experiments, we implemented all baselines and methods in Python 3.7 and relied upon open-source libraries such as PyTorch 1.11 (33) (BSD license) and Scikit-learn (34) (BSD license). To produce the plots seen in this paper, we made use of Matplotlib 3.5 (BSD license). We will release all of the code required to recreate our experiments in an MIT-licensed public repository.

ResourcesAll of our experiments were run on a private machine with 8 Intel(R) Xeon(R) Gold 5218 CPUs (2.30GHz), 64GB of RAM, and 2 Quadro RTX 8000 Nvidia GPUs. We estimate that approximately 100-GPU hours were required to complete all of our experiments.

Figure 10: Visualizations obtained with GNNExplainer on weak distributive generalization (on the left) and strong multiclass generalization (on the right)