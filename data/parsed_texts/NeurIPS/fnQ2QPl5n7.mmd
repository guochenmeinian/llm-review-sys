# GUARD: A Safe Reinforcement Learning Benchmark

 Weiye Zhao

Robotics Institute

Carnegie Mellon University

weijyezha@andrew.cmu.edu

&Rui Chen

Robotics Institute

Carnegie Mellon University

ruic3@andrew.cmu.edu

&Yifan Sun

Robotics Institute

Carnegie Mellon University

yifansu2@andrew.cmu.edu

&Ruixuan Liu

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

twei2@andrew.cmu.edu

&Changliu Liu

Robotics Institute

Carnegie Mellon University

cliu6@andrew.cmu.edu

&Rui Chen

Robotics Institute

Carnegie Mellon University

ruic3@andrew.cmu.edu

&Yifan Sun

Robotics Institute

Carnegie Mellon University

yifansu2@andrew.cmu.edu

&Ruixuan Liu

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

twei2@andrew.cmu.edu

&Changliu Liu

Robotics Institute

Carnegie Mellon University

cliu6@andrew.cmu.edu

&Ruixuan Liu

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Changliu Liu

Robotics Institute

Carnegie Mellon University

twei2@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

cliu6@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute
Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu
&Tianhao Wei

Robotics Institute

Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute
Carnegie Mellon University

ruixuan1@andrew.cmu.edu

&Tianhao Wei

Robotics Institute

Cardifferent applications, ensuring a reliable transition from theory to practice. A benchmark includes 1) algorithms for comparison; 2) environments to evaluate algorithms; 3) a set of evaluation metrics, etc. There are benchmarks for unconfined RL and some safe RL, but not comprehensive enough (Duan et al., 2016; Brockman et al., 2016; Ellenberger, 2018-2019; Yu et al., 2019; Osband et al., 2020; Tunyasuvunakool et al., 2020; Dulac-Arnold et al., 2020; Zhang et al., 2022a).

To create a robust safe RL benchmark, we identify three essential pillars. Firstly, the benchmark must be **generalized**, accommodating diverse agents, tasks, and safety constraints. Real-world applications involve various agent types (e.g., drones, robot arms) with distinct complexities, such as different control degrees-of-freedom (DOF) and interaction modes (e.g., 2D planar or 3D spatial motion). The performance of algorithms is influenced by several factors, including variations in robots (such as observation and action space dimensions), tasks (interactive or non-interactive, 2D or 3D), and safety constraints (number, trespassibility, movability, and motion space). Therefore, providing a comprehensive environment to test the generalizability of safe RL algorithms is crucial.

Secondly, the benchmark should be **unified**, overcoming discrepancies in experiment setups prevalent in the emerging safe RL literature. A unified platform ensures consistent evaluation of different algorithms in controlled environments, promoting reliable performance comparison. Lastly, the benchmark must be **extensible**, allowing researchers to integrate new algorithms and extend setups to address evolving challenges. Given the ongoing progress in safe RL, the benchmark should incorporate major existing works and adapt to advancements. By encompassing these pillars, the benchmark provides a solid foundation for addressing these open problems in safe RL research.

In light of the above-mentioned pillars, this paper introduces GUARD, a **G**eneralized **U**nified **SA**fe **R**einforcement Learning **D**evelopment Benchmark. In particular, GUARD is developed based upon the Safety Gym (Ray et al., 2019), SafeRL-Kit (Zhang et al., 2022a) and SpinningUp (Achiam, 2018). Unlike existing benchmarks, GUARD pushes the boundary beyond the limit by significantly extending the algorithms in comparison, types of agents and tasks, and safety constraint specifications. The contributions of this paper are as follows:

1. **Generalized benchmark with a wide range of agents.** GUARD genuinely supports **11** different agents, covering the majority of real robot types.
2. **Generalized benchmark with a wide range of tasks.** GUARD genuinely supports **7** different task specifications, which can be combined to represent most real robot tasks.
3. **Generalized benchmark with a wide range of safety constraints.** GUARD genuinely supports **8** different safety constraint specifications. The included constraint options comprehensively cover the safety requirements that would encounter in real-world applications.
4. **Unified benchmarking platform with comprehensive coverage of safe RL algorithms.** Guard implements **8** state-of-the-art safe RL algorithms following a unified code structure.
5. **Highly customizable benchmarking platform.** GUARD features a modularized design that enables effortless customization of new testing suites with self-customizable agents, tasks, and constraints. The algorithms in GUARD are self-contained, with a consistent structure and independent implementations, ensuring clean code organization and eliminating dependencies between different algorithms. This self-contained structure greatly facilitates the seamless integration of new algorithms for further extensions.

## 2 Related Work

Open-source Libraries for Reinforcement Learning AlgorithmsOpen-source RL libraries are code bases that implement representative RL algorithms for efficient deployment and comparison. They often serve as backbones for developing new RL algorithms, greatly facilitating RL research. We divide existing libraries into two categories: (a) safety-oriented RL libraries that support safe RL algorithms, and (b) general RL libraries that do not. Among safety-oriented libraries, Safety Gym (Ray et al., 2019) is the most famous one with highly configurable tasks and constraints but only supports three safe RL methods. SafeRL-Kit (Zhang et al., 2022) supports five safe RL methods while missing some key methods such as CPO (Achiam et al., 2017). Bullet-Safety-Gym (Gronauer, 2022) supports CPO but is limited in overall safe RL support at totally four methods. Compared to the above libraries, our proposed GUARD doubles the support at eight methods in total, covering a wider spectrum of general safe RL research. General RL libraries, on the other hand, can be summarized according to their backend into PyTorch (Achiam, 2018; Weng et al., 2022; Raffin et al., 2021; Liang et al., 2018), Tensorflow (Dhariwal et al., 2017; Hill et al., 2018), Jax (Castro et al., 2018; Hoffman et al., 2020), and Keras (Plappert, 2016). In particular, SpinningUp (Achiam, 2018) serves as the major backbone of our GUARD benchmark on the safety-agnostic RL portion.

Benchmark Platform for Safe RL AlgorithmsTo facilitate safe RL research, the benchmark platform should support a wide range of task objectives, constraints, and agent types. Among existing work, the most representative one is Safety Gym (Ray et al., 2019) which is highly configurable. However, Safety Gym is limited in agent types in that it does not support high-dimensional agents (e.g., drone and arm) and lacks tasks with complex interactions (e.g., chase and defense). Moreover, Safety Gym only supports naive contact dynamics (e.g., touch and snap) instead of more realistic cases (e.g., objects bouncing off upon contact) in contact-rich tasks. Safe Control Gym (Yuan et al., 2022) is another open-source platform that supports very simple dynamics (i.e., cartpole, 1D/2D quadrotors) and only supports navigation tasks. Finally, Bullet Safety Gym (Gronauer, 2022) provides high-fidelity agents, but the types of agents are limited, and they only consider navigation tasks. Compared to the above platforms, our GUARD supports a much wider range of task objectives (e.g., 3D reaching, chase and defense) with a much larger variety of eight agents including high-dimensional ones such as drones, arms, ants, and walkers.

## 3 Preliminaries

Markov Decision ProcessAn Markov Decision Process (MDP) is specified by a tuple \((\mathcal{S},\mathcal{A},\gamma,\mathcal{R},P,\rho)\), where \(\mathcal{S}\) is the state space, and \(\mathcal{A}\) is the control space, \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the reward function, \(0\leq\gamma<1\) is the discount factor, \(\rho:\mathcal{S}\rightarrow[0,1]\) is the starting state distribution, and \(P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]\) is the transition probability function (where \(P(s^{\prime}|s,a)\) is the probability of transitioning to state \(s^{\prime}\) given that the previous state was \(s\) and the agent took action \(a\) at state \(s\)). A stationary policy \(\pi:\mathcal{S}\rightarrow\mathcal{P}(\mathcal{A})\) is a map from states to a probability distribution over actions, with \(\pi(a|s)\) denoting the probability of selecting action \(a\) in state \(s\). We denote the set of all stationary policies by \(\Pi\). Suppose the policy is parameterized by \(\theta\); policy search algorithms search for the optimal policy within a set \(\Pi_{\theta}\subset\Pi\) of parameterized policies.

The solution of the MDP is a policy \(\pi\) that maximizes the performance measure \(\mathcal{J}(\pi)\) computed via the discounted sum of reward:

\[\mathcal{J}(\pi)=\mathbb{E}_{\tau\sim\pi}\left[\sum_{t=0}^{\infty}\gamma^{t }\mathcal{R}(s_{t},a_{t},s_{t+1})\right],\] (1)

where \(\tau=[s_{0},a_{0},s_{1},\cdots]\) is the state and control trajectory, and \(\tau\sim\pi\) is shorthand for that the distribution over trajectories depends on \(\pi:s_{0}\sim\mu,a_{t}\sim\pi(\cdot|s_{t}),s_{t+1}\sim P(\cdot|s_{t},a_{t})\). Let \(R(\tau)\doteq\sum_{t=0}^{\infty}\gamma^{t}\mathcal{R}(s_{t},a_{t},s_{t+1})\) be the discounted return of a trajectory. We define the on-policy value function as \(V^{\pi}(s)\doteq\mathbb{E}_{\tau\sim\pi}[R(\tau)|s_{0}=s]\), the on-policy action-value function as \(Q^{\pi}(s,a)\doteq\mathbb{E}_{\tau\sim\pi}[R(\tau)|s_{0}=s,a_{0}=a]\), and the advantage function as \(A^{\pi}(s,a)\doteq Q^{\pi}(s,a)-V^{\pi}(s)\).

Constrained Markov Decision ProcessA constrained Markov Decision Process (CMDP) is an MDP augmented with constraints that restrict the set of allowable policies. Specifically, CMDP introduces a set of cost functions, \(C_{1},C_{2},\cdots,C_{m}\), where \(C_{i}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow\mathbb{R}\) maps the state action transition tuple into a cost value. Similar to (1), we denote \(\mathcal{J}_{C_{i}}(\pi)=\mathbb{E}_{\tau\sim\pi}[\sum_{t=0}^{\infty}\gamma^{t} C_{i}(s_{t},a_{t},s_{t+1})]\) as the cost measure for policy \(\pi\) with respect to the cost function \(C_{i}\). Hence, the set of feasible stationary policies for CMDP is then defined as \(\Pi_{C}=\{\pi\in\Pi\big{|}\ \forall i,\mathcal{J}_{C_{i}}(\pi)\leq d_{i}\}\), where \(d_{i}\in\mathbb{R}\). In CMDP, the objective is to select a feasible stationary policy \(\pi\) that maximizes the performance:\(\underset{\pi\in\Pi_{\theta}\cap\Gamma_{C}}{\max}\mathcal{J}(\pi)\). Lastly, we define on-policy value, action-value, and advantage functions for the cost as \(V_{C_{i}}^{\pi},Q_{C_{i}}^{\pi}\) and \(A_{C_{i}}^{\pi}\), which as analogous to \(V^{\pi}\), \(Q^{\pi}\), and \(A^{\pi}\), with \(C_{i}\) replacing \(R\).

## 4 GUARD Safe RL Library

### Overall Implementation

GUARD contains the latest methods that can achieve safe RL: (i) end-to-end safe RL algorithms including CPO (Achiam et al., 2017), TRPO-Lagrangian (Bohez et al., 2019), TRPO-FAC (Ma et al., 2021), TRPO-IPO (Liu et al., 2020), and PCPO (Yang et al., 2020); and (ii) hierarchical safe RL algorithms including TRPO-SL (TRPO-Safety Layer) (Dalal et al., 2018) and TRPO-USL (TRPO-Unrolling Safety Layer) (Zhang et al., 2022). We also include TRPO (Schulman et al., 2015) as an unconstrained RL baseline. Note that GUARD only considers model-free approaches which rely less on assumptions than model-based ones. We highlight the benefits of our algorithm implementations in GUARD:

* GUARD comprehensively covers a **wide range of algorithms** that enforce safety in both hierarchical and end-to-end structures. Hierarchical methods maintain a separate safety layer, while end-to-end methods solve the constrained learning problem as a whole.
* GUARD provides a **fair comparison among safety components** by equipping every algorithm with the same reward-oriented RL backbone (i.e., TRPO (Schulman et al., 2015)), implementation (i.e., MLP policies with (Gulman et al., 2015; Gulman et al., 2015) hidden layers and tanh activation), and training procedures. Hence, all algorithms inherit the performance guarantee of TRPO.
* GUARD is implemented in PyTorch with a clean structure where every algorithm is self-contained, enabling **fast customization and development** of new safe RL algorithms. GUARD also comes with unified logging and plotting utilities which makes analysis easy.

### Unconstrained RL

TRPOWe include TRPO (Schulman et al., 2015) since it is state-of-the-art and several safe RL algorithms are based on it. TRPO is an unconstrained RL algorithm and only maximizes performance \(\mathcal{J}\). The key idea behind TRPO is to iteratively update the policy within a local range (trust region) of the most recent version \(\pi_{k}\). Mathematically, TRPO updates policy via

\[\pi_{k+1}=\underset{\pi\in\Pi_{\theta}}{\text{argmax}}\,\mathcal{J}(\pi) \quad\text{ s.t.}\,\mathcal{D}_{KL}(\pi,\pi_{k})\leq\delta,\] (2)

where \(\mathcal{D}_{KL}\) is Kullback-Leibler (KL) divergence, \(\delta>0\) and the set \(\{\pi\in\Pi_{\theta}\ :\ \mathcal{D}_{KL}(\pi,\pi_{k})\leq\delta\}\) is called the _trust region_. To solve (2), TRPO applies Taylor expansion to the objective and constraint at \(\pi_{k}\) to the first and second order, respectively. That results in an approximate optimization with linear objective and quadratic constraints (LOQC). TRPO guarantees a worst-case performance degradation.

### End-to-End Safe RL

CPOConstrained Policy Optimizaiton (CPO) (Achiam et al., 2017) handles CMDP by extending TRPO. Similar to TRPO, CPO also performs local policy updates in a trust region. Different from TRPO, CPO additionally requires \(\pi_{k+1}\) to be constrained by \(\Pi_{\theta}\cap\Pi_{C}\). For practical implementation, CPO replaces the objective and constraints with surrogate functions (advantage functions), which can easily be estimated from samples collected on \(\pi_{k}\), formally:

\[\pi_{k+1}=\underset{\pi\in\Pi_{\theta}}{\text{argmax}}\,\underset{s\sim d ^{\pi_{k}}}{\mathbb{E}}[A^{\pi_{k}}(s,a)]\] (3) \[\text{s.t.}\ \ \mathcal{D}_{KL}(\pi,\pi_{k})\leq\delta,\ \ \ \mathcal{J}_{C_{i}}(\pi_{k})+\frac{1}{1-\gamma}\underset{a\sim\pi}{\mathbb{E}} \Bigg{[}A_{C_{i}}^{\pi_{k}}(s,a)\Bigg{]}\leq d_{i},i=1,\cdots,m.\]where \(d^{\pi_{k}}\doteq(1-\gamma)\sum_{t=0}^{H}\gamma^{t}P(s_{t}=s|\pi_{k})\) is the discounted state distribution. Following TRPO, CPO also performs Taylor expansion on the objective and constraints, resulting in a Linear Objective with Linear and Quadratic Constraints (LOLQC). CPO inherits the worst-case performance degradation guarantee from TRPO and has a worst-case cost violation guarantee.

PcpoProjection-based Constrained Policy Optimization (PCPO) (Yang et al., 2020) is proposed based on CPO, where PCPO first maximizes reward using a trust region optimization method without any constraints, then PCPO reconciles the constraint violation (if any) by projecting the policy back onto the constraint set. Policy update then follows an analytical solution:

\[\pi_{k+1}=\pi_{k}+\sqrt{\frac{2\delta}{g^{\top}H^{-1}g}}H^{-1}g-\max\bigg{(}0, \frac{\sqrt{\frac{2\delta}{g^{\top}H^{-1}g}}g_{c}^{\top}H^{-1}g+b}{g_{c}^{\top }L^{-1}g_{c}}\bigg{)}L^{-1}g_{c}\] (4)

where \(g_{c}\) is the gradient of the cost advantage function, \(g\) is the gradient of the reward advantage function, \(H\) is the Hessian of the KL divergence constraint, \(b\) is the constraint violation of the policy \(\pi_{k}\), \(L=\mathcal{I}\) for \(L_{2}\) norm projection, and \(L=H\) for KL divergence projection. PCPO provides a lower bound on reward improvement and an upper bound on constraint violation.

TRPO-Lagrangian methods solve constrained optimization by transforming hard constraints into soft constraints in the form of penalties for violations. Given the objective \(\mathcal{J}(\pi)\) and constraints \(\{\mathcal{J}_{C_{i}}(\pi)\leq d_{i}\}_{i}\), TRPO-Lagrangian (Bohez et al., 2019) first constructs the dual problem

\[\underset{\forall i,\lambda_{i}\geq 0}{\text{max}}\underset{\pi\in\Pi_{ \theta}}{\text{min}}-\mathcal{J}(\pi)+\sum_{i}\lambda_{i}(\mathcal{J}_{C_{i}} (\pi)-d_{i}).\] (5)

The update of \(\theta\) is done via a trust region update with the objective of (2) replaced by that of (5) while fixing \(\lambda_{i}\). The update of \(\lambda_{i}\) is done via standard gradient ascend. Note that TRPO-Lagrangian does not have a theoretical guarantee for constraint satisfaction.

Trepo-FacInspired by Lagrangian methods and aiming at enforcing state-wise constraints (e.g., preventing state from stepping into infeasible parts in the state space), Feasible Actor Critic (FAC) (Ma et al., 2021) introduces a multiplier (dual variable) network. Via an alternative update procedure similar to that for (5), TRPO-FAC solves the _statewise_ Lagrangian objective:

\[\underset{\forall i,\xi_{i}}{\text{max}}\underset{\pi\in\Pi_{ \theta}}{\text{min}}-\mathcal{J}(\pi)+\sum_{i}\mathbb{E}_{s\sim d^{\pi_{k}}} \left[\lambda_{\xi_{i}}(s)(\mathcal{J}_{C_{i}}(\pi)-d_{i})\right],\] (6)

where \(\lambda_{\xi_{i}}(s)\) is a parameterized Lagrangian multiplier network and is parameterized by \(\xi_{i}\) for the \(i\)-th constraint. Note that TRPO-FAC does not have a theoretical guarantee for constraint satisfaction.

Trepo-IpoTrepo (Liu et al., 2020) incorporates constraints by augmenting the optimization objective in (2) with logarithmic barrier functions, inspired by the interior-point method (Boyd and Vandenberghe, 2004). Ideally, the augmented objective is \(I(\mathcal{J}_{C_{i}}(\pi)-d_{i})=0\) if \(\mathcal{J}_{C_{i}}(\pi)-d_{i}\leq 0\) or \(-\infty\) otherwise. Intuitively, that enforces the constraints since the violation penalty would be \(-\infty\). To make the objective differentiable, \(I(\cdot)\) is approximated by \(\phi(x)=\log(-x)/t\) where \(t>0\) is a hyperparameter. Then TRPO-IPO solves (2) with the objective replaced by \(\mathcal{J}_{\mathrm{IPO}}(\pi)=\mathcal{J}(\pi)+\sum_{i}\phi(\mathcal{J}_{C_{ i}}(x)-d_{i})\). TRPO-IPO does not have theoretical guarantees for constraint satisfaction.

### Hierarchical Safe RL

Safety Layer(Dalal et al., 2018), added on top of the original policy network, conducts a quadratic-programming-based constrained optimization to project reference action into the nearest safe action. Mathematically:

\[a_{t}^{safe}=\underset{a}{\text{argmin}}\frac{1}{2}\|a-a_{t}^{ ref}\|^{2}\quad\mathbf{s.t.}\quad\forall i,\bar{g}_{\varphi_{i}}(s_{t})^{\top}a+C_{i}(s_{t -1},a_{t-1},s_{t})\leq d_{i}\] (7)

where \(a_{t}^{ref}\sim\pi_{k}(\cdot|s_{t})\), and \(\bar{g}_{\varphi_{i}}(s_{t})^{\top}a_{t}+C_{i}(s_{t-1},a_{t-1},s_{t})\approx C _{i}(s_{t},a_{t},s_{t+1})\) is a \(\varphi\) parameterized linear model. If there's only one constraint, (7) has a closed-form solution.

UslUnrolling Safety Layer (USL) (Zhang et al., 2022b) is proposed to project the reference action into safe action via gradient-based correction. Specifically, USL iteratively updates the learned \(Q_{C}(s,a)\) function with the samples collected during training. With step size \(\eta\) and normalization factor \(\mathcal{Z}\), USL performs gradient descent as \(a_{t}^{safe}=a_{t}^{ref}-\frac{\eta}{\mathcal{Z}}\cdot\frac{\partial}{ \partial a_{t}^{ref}}\big{[}Q_{C}(s_{t},a_{t}^{ref})-d\big{]}\).

## 5 GUARD Testing Suite

### Robot Options

In GUARD testing suite, the agent (in the form of a robot) perceives the world through sensors and interacts with the world through actuators. Robots are specified through MuJoCo XML files. The suite is equipped with **8** types of pre-made robots that we use in our benchmark environments as whison in Figure 1. The action space of the robots are continuous, and linearly scaled to [-1, +1].

**Swimmer** consist of three links and two joints. Each joint connects two links to form a linear chain. Swimmer can move around by applying **2** torques on the joints.

**Ant** is a quadrupedal robot composed a torso and four legs. Each of the four legs has a hip joint and a knee joint; and can move around by applying **8** torques to the joints.

**Walker** is a bipedal robot that consists of four main parts - a torso, two thighs, two legs, and two feet. Different from the knee joints and the ankle joints, each of the hip joints has three hinges in the \(x\), \(y\) and \(z\) coordinates to help turning. With the torso height fixed, Walker can move around by controlling **10** joint torques.

**Humanoid** is also a bipedal robot that has a torso with a pair of legs and arms. Each leg of Humanoid consists of two joints (no ankle joint). Since we mainly focus on the navigation ability of the robots in designed tasks, the arm joints of Humanoid are fixed, which enables Humanoid to move around by only controlling **6** torques.

**Hopper** is a one-legged robot that consists of four main parts - a torso, a thigh, a leg, and a single foot. Similar to Walker, Hopper can move around by controlling **5** joint torques.

**Arm3** is designed to simulate a fixed three-joint robot arm. Arm is equipped with multiple sensors on each links in order to fully observe the environment. By controlling **3** joint torques, Arm can move its end effector around with high flexibility.

**Arm6** is designed to simulate a robot manipulator with a fixed base and six joints. Similar to Arm3, Arm6 can move its end effector around by controlling **6** torques.

**Drone** is designed to simulate a quadrotor. The interaction between the quadrotor and the air is simulated by applying four external forces on each of the propellers. The external forces are set to balance the gravity when the control action is zero. Drone can move in 3D space by applying **4** additional control forces on the propellers.

### Task Options

We categorize robot tasks in two ways: (i) interactive versus non-interactive tasks, and (ii) 2D space versus 3D space tasks. 2D space tasks constrain agents to a planar space, while 3D space tasks do not. Non-interactive tasks primarily involve achieving a target state (e.g., trajectory tracking) while

Figure 1: Robots of our environments.

interactive tasks (e.g., human-robot collaboration and unstructured object pickup) necessitate contact or non-contact interactions between the robot and humans or movable objects, rendering them more challenging. On a variety of tasks that cover different situations, GUARD facilitates a thorough evaluation of safe RL algorithms via the following tasks. See Table 17 for more information.

**Goal** (Figure 1(a)) requires the robot navigating towards a series of 2D or 3D goal positions. Upon reaching a goal, the location is randomly reset. The task provides a sparse reward upon goal achievement and a dense reward for making progress toward the goal.

**Push** (Figure 1(b)) requires the robot pushing a ball toward different goal positions. The task includes a sparse reward for the ball reaching the goal circle and a dense reward that encourages the agent to approach both the ball and the goal. Unlike pushing a box in Safety Gym, it is more challenging to push a ball since the ball can roll away and the contact dynamics are more complex.

**Chase** (Figure 1(c)) requires the robot tracking multiple dynamic targets. Those targets continuously move away from the robot at a slow speed. The dense reward component provides a bonus for minimizing the distance between the robot and the targets. The targets are constrained to a circular area. A 3D version of this task is also available, where the targets move within a restricted 3D space. Detailed dynamics of the targets is described in Appendix A.5.1.

**Defense** (Figure 1(d)) requires the robot to prevent dynamic targets from entering a protected circle area. The targets will head straight toward the protected area or avoid the robot if the robot gets too close. Dense reward component provides a bonus for increasing the cumulative distance between the targets and the protected area. Detailed dynamics of the targets is described in Appendix A.5.2.

### Constraint Options

We classify constraints based on various factors: **trespassibility**: whether constraints are trespassable or untrespassable. Trespassable constraints allow violations without causing any changes to the robot's behaviors, and vice versa. (ii) **movability**: whether they are immovable, passively movable, or actively movable; and (iii) **motion space**: whether they pertain to 2D or 3D environments. To cover a comprehensive range of constraint configurations, we introduce additional constraint types via expanding Safety Gym. Please refer to Table 18 for all configurable constraints.

**3D Hazards** (Figure 2(a)) are dangerous 3D areas to avoid. These are floating spheres that are trespassable, and the robot is penalized for entering them.

**Ghosts** (Figure 2(b)) are dangerous areas to avoid. Different from hazards, ghosts always move toward the robot slowly, represented by circles on the ground. Ghosts can be either trespassable or untrespassable. The robot is penalized for touching the untrespassable ghosts and entering the trespassable ghosts. Moreover, ghosts can be configured to start chasing the robot when the distance from the robot is larger than some threshold. This feature together with the adjustable velocity allows users to design the ghosts with different aggressiveness. Detailed dynamics of the targets is described in Appendix A.5.3.

**3D Ghosts** (Figure 2(c)) are dangerous 3D areas to avoid. These are floating spheres as 3D version of ghosts, sharing the similar behaviour with ghosts.

Figure 2: Tasks of our environments.

[MISSING_PAGE_FAIL:8]

Figure 4: Comparison of results from four representative tasks. (a) to (d) cover four robots on the goal task. (e) shows the performance of a task with ghosts. (f) to (h) cover three different tasks with the point robot.

## References

* Mnih et al. [2013] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_, 2013.
* Zhao et al. [2019a] Weiye Zhao, Yang Liu, Xiaoming Zhao, J Qiu, and Jian Peng. Approximation gradient error variance reduced optimization. In _Workshop on Reinforcement Learning in Games (RLG) at The Thirty-Third AAAI Conference on Artificial Intelligence_, 2019a.
* Silver et al. [2018] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* OpenAI [2019] OpenAI, ;., Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, Rafal Jozefowicz, Scott Gray, Catherine Olsson, Jakub Pachocki, Michael Petrov, Henrique P. d. O. Pinto, Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang. Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_, 2019.
* Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, Wojtek Czarnecki, Andrew Dudzik, Aja Huang, Petko Georgiev, Richard Powell, Timo Ewalds, Dan Horgan, Manuel Kroiss, Ivo Danihelka, John Agapiou, Junhyuk Oh, Valentin Dalibard, David Choi, Laurent Sifre, Yury Sulsky, Sasha Vezhnevets, James Molloy, Trevor Cai, David Budden, Tom Paine, Caglar Gulcehre, Ziyu Wang, Tobias Pfaff, Toby Pohlen, Dani Yogatama, Julia Cohen, Katrina McKinney, Oliver Smith, Tom Schaul, Timothy Lillicrap, Chris Apps, Koray Kavukcuoglu, Demis Hassabis, and David Silver. AlphaStar: Mastering the Real-Time Strategy Game StarCraft II. https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/, 2019.
* Zhao et al. [2019b] Wei-Ye Zhao, Xi-Ya Guan, Yang Liu, Xiaoming Zhao, and Jian Peng. Stochastic variance reduction for deep q-learning. _arXiv preprint arXiv:1905.08152_, 2019b.
* Popov et al. [2017] Ivaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland Hafner, Gabriel Barth-Maron, Matej Vecerik, Thomas Lampe, Yuval Tassa, Tom Erez, and Martin Riedmiller. Data-efficient deep reinforcement learning for dexterous manipulation. _arXiv preprint arXiv:1704.03073_, 2017.
* Zhao et al. [2022a] Weiye Zhao, Suqin He, and Changliu Liu. Provably safe tolerance estimation for robot arms via sum-of-squares programming. _IEEE Control Systems Letters_, 6:3439-3444, 2022a.
* Chen et al. [2023] Rui Chen, Alvin Shek, and Changliu Liu. Robust and context-aware real-time collaborative robot handling via dynamic gesture commands. _IEEE Robotics and Automation Letters_, 2023.
* Agostinelli et al. [2019] Forest Agostinelli, Stephen McAleer, Alexander Shmakov, and Pierre Baldi. Solving the rubik's cube with deep reinforcement learning and search. _Nature Machine Intelligence_, pages 1-8, 2019.
* Shek et al. [2022] Alvin Shek, Rui Chen, and Changliu Liu. Learning from physical human feedback: An object-centric one-shot adaptation method. _arXiv preprint arXiv:2203.04951_, 2022.
* Zhao et al. [2020a] Wei-Ye Zhao, Suqin He, Chengtao Wen, and Changliu Liu. Contact-rich trajectory generation in confined environments using iterative convex optimization. In _Dynamic Systems and Control Conference_, volume 84287, page V002T31A002. American Society of Mechanical Engineers, 2020a.
* Noren et al. [2021] Charles Noren, Weiye Zhao, and Changliu Liu. Safe adaptation with multiplicative uncertainties using robust safe set algorithm. _IFAC-PapersOnLine_, 54(20):360-365, 2021.
* Noren et al. [2021]David Isele, Alireza Nakhaei, and Kikuo Fujimura. Safe reinforcement learning on autonomous vehicles. _arXiv preprint arXiv:1910.00399_, 2019.
* Kiran et al. [2022] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A. Al Sallab, Senthil Yogamani, and Patrick Perez. Deep reinforcement learning for autonomous driving: A survey. _IEEE Transactions on Intelligent Transportation Systems_, 23(6):4909-4926, 2022.
* Gu et al. [2022a] Shangding Gu, Guang Chen, Lijun Zhang, Jing Hou, Yingbai Hu, and Alois Knoll. Constrained reinforcement learning for vehicle motion planning with topological reachability analysis. _Robotics_, 11(4), 2022a.
* Kober et al. [2013] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* Brunke et al. [2022] Lukas Brunke, Melissa Greeff, Adam W. Hall, Zhaocong Yuan, Siqi Zhou, Jacopo Panerati, and Angela P. Schoellig. Safe learning in robotics: From learning-based control to safe reinforcement learning. _Annual Review of Control, Robotics, and Autonomous Systems_, 5(1):411-444, 2022.
* Zhao et al. [2022b] Weiye Zhao, Tairan He, Tianhao Wei, Simin Liu, and Changliu Liu. Safety index synthesis via sum-of-squares programming. _arXiv preprint arXiv:2209.09134_, 2022b.
* Zhao et al. [2020b] Weiye Zhao, Liting Sun, Changliu Liu, and Masayoshi Tomizuka. Experimental evaluation of human motion prediction toward safe and efficient human robot collaboration. In _2020 American Control Conference (ACC)_, pages 4349-4354. IEEE, 2020b.
* Sun et al. [2023] Yifan Sun, Weiye Zhao, and Changliu Liu. Hybrid task constrained planner for robot manipulator in confined environment. _arXiv preprint arXiv:2304.09260_, 2023.
* Cheng et al. [2019] Yujiao Cheng, Weiye Zhao, Changliu Liu, and Masayoshi Tomizuka. Human motion prediction using semi-adaptable neural networks. In _2019 American Control Conference (ACC)_, pages 4884-4890. IEEE, 2019.
* Garcia and Fernandez [2015] Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning. _Journal of Machine Learning Research_, 16(1):1437-1480, 2015.
* Gu et al. [2022b] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, Yaodong Yang, and Alois Knoll. A review of safe reinforcement learning: Methods, theory and applications. _arXiv preprint arXiv:2205.10330_, 2022b.
* Zhao et al. [2023] Weiye Zhao, Tairan He, Rui Chen, Tianhao Wei, and Changliu Liu. State-wise safe reinforcement learning: A survey. _The 32nd International Joint Conference on Artificial Intelligence (IJCAI)_, 2023.
* Zhao et al. [2022c] Weiye Zhao, Tairan He, and Changliu Liu. Probabilistic safeguard for reinforcement learning using safety index guided gaussian process models. _arXiv preprint arXiv:2210.01041_, 2022c.
* He et al. [2023a] Suqin He, Weiye Zhao, Chuxiong Hu, Yu Zhu, and Changliu Liu. A hierarchical long short term safety framework for efficient robot manipulation under uncertainty. _Robotics and Computer-Integrated Manufacturing_, 82:102522, 2023a.
* Wei et al. [2022] Tianhao Wei, Shucheng Kang, Weiye Zhao, and Changliu Liu. Persistently feasible robust safe control by safety index synthesis and convex semi-infinite programming. _IEEE Control Systems Letters_, 2022.
* Zhao et al. [2021] Weiye Zhao, Tairan He, and Changliu Liu. Model-free safe control for zero-violation reinforcement learning. In _5th Annual Conference on Robot Learning_, 2021.
* He et al. [2023b] Tairan He, Weiye Zhao, and Changliu Liu. Autocost: Evolving intrinsic cost for zero-violation reinforcement learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 2023b.
* He et al. [2023c]Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep reinforcement learning for continuous control. In _International conference on machine learning_, pages 1329-1338. PMLR, 2016.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Ellenberger [2018] Benjamin Ellenberger. Pybullet gymperium. https://github.com/benelot/pybullet-gym, 2018-2019.
* Yu et al. [2019] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In _Conference on Robot Learning (CoRL)_, 2019.
* Osband et al. [2020] Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepesvari, Satinder Singh, Benjamin Van Roy, Richard Sutton, David Silver, and Hado van Hasselt. Behaviour suite for reinforcement learning. In _International Conference on Learning Representations_, 2020.
* Tunyasuvunakool et al. [2020] Saran Tunyasuvunakool, Alistair Muldal, Yotam Doron, Siqi Liu, Steven Bohez, Josh Merel, Tom Erez, Timothy Lillicrap, Nicolas Heess, and Yuval Tassa. dm_control: Software and tasks for continuous control. _Software Impacts_, 6:100022, 2020.
* Dulac-Arnold et al. [2020] Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges of real-world reinforcement learning. 2020.
* Zhang et al. [2022a] Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, and Xueqian Wang. Saferl-kit: Evaluating efficient reinforcement learning methods for safe autonomous driving. _arXiv preprint arXiv:2206.08528_, 2022a.
* Ray et al. [2019] Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement learning. _CoRR_, abs/1910.01708, 2019.
* Achiam [2018] Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.
* Achiam et al. [2017a] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pages 22-31. PMLR, 2017a.
* Gronauer [2022] Sven Gronauer. Bullet-safety-gym: A framework for constrained reinforcement learning. Technical report, TUM Department of Electrical and Computer Engineering, Jan 2022.
* Weng et al. [2022] Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Yi Su, Hang Su, and Jun Zhu. Tianshou: A highly modularized deep reinforcement learning library. _Journal of Machine Learning Research_, 23(267):1-6, 2022. URL http://jmlr.org/papers/v23/21-1127.html.
* Raffin et al. [2021] Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. _Journal of Machine Learning Research_, 22(268):1-8, 2021. URL http://jmlr.org/papers/v22/20-1364.html.
* Liang et al. [2018] Eric Liang, Richard Liaw, Robert Nishihara, Philipp Moritz, Roy Fox, Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica. RLlib: Abstractions for distributed reinforcement learning. In _International Conference on Machine Learning (ICML)_, 2018.
* Dhariwal et al. [2017] Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https://github.com/openai/baselines, 2017.

* Hill et al. [2018] Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore, Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/hill-a/stable-baselines, 2018.
* Castro et al. [2018] Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare. Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http://arxiv.org/abs/1812.06110.
* Hoffman et al. [2020] Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stanczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, Leonard Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani, Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson, Abe Friesen, Ruba Haroun, Alex Novikov, Sergio Gomez Colmenarejo, Serkan Cabi, Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando de Freitas. Acme: A research framework for distributed reinforcement learning. _arXiv preprint arXiv:2006.00979_, 2020. URL https://arxiv.org/abs/2006.00979.
* Plappert [2016] Matthias Plappert. keras-rl. https://github.com/keras-rl/keras-rl, 2016.
* Yuan et al. [2022] Zhaocong Yuan, Adam W. Hall, Siqi Zhou, Lukas Brunke, Melissa Greeff, Jacopo Panerati, and Angela P. Schoellig. Safe-control-gym: A unified benchmark suite for safe learning-based control and reinforcement learning in robotics. _IEEE Robotics and Automation Letters_, 7(4):11142-11149, 2022. doi: 10.1109/LRA.2022.3196132.
* Bohez et al. [2019] Steven Bohez, Abbas Abdolmaleki, Michael Neunert, Jonas Buchli, Nicolas Heess, and Raia Hadsell. Value constrained model-free continuous control. _arXiv preprint arXiv:1902.04623_, 2019.
* Ma et al. [2021] Haitong Ma, Yang Guan, Shengbo Eben Li, Xiangteng Zhang, Sifa Zheng, and Jianyu Chen. Feasible actor-critic: Constrained reinforcement learning for ensuring statewise safety. _arXiv preprint arXiv:2105.10682_, 2021.
* Liu et al. [2020] Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under constraints. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 4940-4947, 2020.
* Yang et al. [2020] Tsung-Yen Yang, Justinian Rosca, Karthik Narasimhan, and Peter J Ramadge. Projection-based constrained policy optimization. _arXiv preprint arXiv:2010.03152_, 2020.
* Dalal et al. [2018] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. _CoRR_, abs/1801.08757, 2018.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Achiam et al. [2017b] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International Conference on Machine Learning_, pages 22-31. PMLR, 2017b.
* Boyd and Vandenberghe [2004] Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Zhang et al. [2022b] Linrui Zhang, Qin Zhang, Li Shen, Bo Yuan, Xueqian Wang, and Dacheng Tao. Evaluating model-free reinforcement learning toward safety-critical tasks. _arXiv preprint arXiv:2212.05727_, 2022b.