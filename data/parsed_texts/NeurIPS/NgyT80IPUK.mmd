# Matrix Denoising with Doubly Heteroscedastic Noise: Fundamental Limits and Optimal Spectral Methods

Yihan Zhang

Institute of Science and Technology Austria

zephyr.z798@gmail.com &Marco Mondelli

Institute of Science and Technology Austria

marco.mondelli@ist.ac.at

###### Abstract

We study the matrix denoising problem of estimating the singular vectors of a rank-\(1\) signal corrupted by noise with both column and row correlations. Existing works are either unable to pinpoint the exact asymptotic estimation error or, when they do so, the resulting approaches (e.g., based on whitening or singular value shrinkage) remain vastly suboptimal. On top of this, most of the literature has focused on the special case of estimating the left singular vector of the signal when the noise only possesses row correlation (one-sided heteroscedasticity). In contrast, our work establishes the information-theoretic and algorithmic limits of matrix denoising with doubly heteroscedastic noise. We characterize the exact asymptotic minimum mean square error, and design a novel spectral estimator with rigorous optimality guarantees: under a technical condition, it attains positive correlation with the signals whenever information-theoretically possible and, for one-sided heteroscedasticity, it also achieves the Bayes-optimal error. Numerical experiments demonstrate the significant advantage of our theoretically principled method with the state of the art. The proofs draw connections with statistical physics and approximate message passing, departing drastically from standard random matrix theory techniques.

## 1 Introduction

Matrix denoising is a central primitive in statistics and machine learning, and the problem is to recover a signal \(X\in\mathbb{R}^{n\times d}\) from an observation \(A=X+W\) corrupted by additive noise \(W\). This finds applications across multiple domains of sciences, e.g., imaging [24, 63], biology [14, 46] and astronomy [70, 5]. When \(X\) has low rank and \(W\) i.i.d. entries, \(A\) is the standard model for principal component analysis, typically referred to as the Johnstone spiked covariance model [42]. When \(n,d\) are both large and proportional, which corresponds to the most sample-efficient regime, its Bayes-optimal limits are well understood [52], and it has been established how to achieve them efficiently [56]. Minimax/non-asymptotic guarantees are also available in special cases, such as sparse PCA [18], Gaussian mixtures [72] and certain joint scalings of \((n,d)\)[57].

However, in most applications, noise is highly structured and correlated, thereby calling for more realistic assumptions on \(W\) than having i.i.d. entries. A recent line of work addresses this concern by studying matrix denoising with heteroscedastic noise [1, 69, 33, 44, 26], resting on two basic ideas: whitening and singular value shrinkage. Whitening refers to multiplying the data matrix by the square root of the inverse covariance, in order to reduce the model to one with i.i.d. noise; and singular value shrinkage retains the singular vectors of the data while deflating the singular values to correct for the noise. Though the exact asymptotic performance of these algorithms has been derived [69, 33, 44, 26], their optimality is yet to be determined from a Bayesian standpoint. In fact, we will prove that whitening and shrinkage are _not_ the correct way to approach Bayes optimality.

Main contributions.We focus on the prototypical model \(A=X+W\), where \(X=\frac{\lambda}{n}{u^{*}}{v^{*}}^{\top}\) is a rank-\(1\) signal, \(\lambda\) is the signal-to-noise ratio (SNR), and \(W=\Xi^{1/2}\widetilde{W}\Sigma^{1/2}\) is doubly heterogeneous noise. Here \(u^{*},v^{*}\) follow i.i.d. priors; \(\widetilde{W}\) contains i.i.d. Gaussian entries; the covariance matrices \(\Xi,\Sigma\) capture column and row correlations; and we consider the typical high-dimensional regime in which \(n,d\) are both large and proportional. Our main results are summarized below.

1. We design an efficient spectral estimator to recover \(u^{*},v^{*}\), and we provide a precise asymptotic analysis of its performance, see Theorem 5.1. This estimator is given by the top singular vectors of a matrix obtained by carefully pre-processing \(A\), see (5.3).
2. When the priors of \(u^{*},v^{*}\) are standard Gaussian, we show in Corollary 5.2 that the spectral estimator above is optimal in the following sense: _(i)_ under a technical condition, it achieves the _optimal weak recovery threshold_, namely its mean square error is non-trivial as soon as this is information-theoretically possible; _(ii)_ it achieves the _Bayes-optimal error_ for \(u^{*}\) (resp. \(v^{*}\)) when \(\Xi\) (resp. \(\Sigma\)) is the identity. These optimality guarantees follow from rigorously obtaining the asymptotic minimum mean square error (MMSE) for the estimation of the whitened signals \(\Xi^{-1/2}u^{*}\) and \(\Sigma^{-1/2}v^{*}\), see Theorem 4.2.

Our spectral estimator only involves matrix multiplication and computing principal singular vectors. Practically, this can be efficiently done using standard SVD algorithms or power iteration [48]. For both one-sided and double heteroscedasticity, numerical experiments in Figures 2 and 3 show significant advantage of our spectral estimator for moderate SNRs over HeteroPCA [76] and shrinkage-based methods, i.e., Whiten-Shrink-reColor [44; 45], OptShrink [59], and ScreeNOT [27].

Proof techniques.We take a completely different route from classical approaches in statistics and random matrix theory (e.g., whitening and shrinkage), and instead exploit tools from statistical physics and the theory of approximate message passing. In particular, the MMSE for the whitened signals \(\Xi^{-1/2}u^{*},\Sigma^{-1/2}v^{*}\) is obtained via an interpolation argument [10; 52; 53]. This result allows us to derive the weak recovery threshold for estimating the true signals \(u^{*},v^{*}\). Moreover, for one-sided heteroscedasticity, this MMSE coincides with that for estimating the true signal on the homoscedastic side. Evaluating the Bayes-optimal estimators requires solving high-dimensional integrals that are computationally intractable. To circumvent this issue, we propose an efficient spectral method that still enjoys optimality guarantees. Its design and analysis draw connections with a family of iterative algorithms called approximate message passing (AMP) [11; 32]. All our results are mathematically rigorous, with the only technical condition being "(5.1) implies \(\sigma_{2}^{*}<1\)" in Theorem 5.1 that we only managed to verify numerically, but not analytically; see Remark 5.1.

## 2 Related work

Research on matrix denoising in the homoscedastic case (\(\Xi=I_{n},\Sigma=I_{d}\)) has a rich history, and in random matrix theory properties of the spectrum and eigenspaces of \(A\) have been studied exhaustively. Most prominently, the BBP phase transition phenomenon [4] (and its finite-sample counterpart [60]) unveils a threshold of the SNR \(\lambda\) above which a pair of outlier singular value and singular vector emerge. Under i.i.d. priors, the asymptotic Bayes-optimal estimation error has been derived [52; 53], rigorously justifying predictions from statistical physics [47]. The proof uses the interpolation method due to Guerra [35], originally developed in the context of mean-field spin glasses. Besides low-rank matrix estimation, this method (including its adaptive variant [10] and the Aizenman-Sims-Starr scheme [2]) has also been applied to a range of problems, including spiked tensor estimation [49], generalized linear models [9], stochastic block models [74] and group synchronization [73].

Moving to the heteroscedastic case, an active line of work concerns optimal singular value shrinkage methods [44; 33; 45; 69; 59; 26]. These methods can be regarded as a special family of rotationally invariant estimators, which apply a univariate function \(\eta\colon\mathbb{R}_{\geq 0}\to\mathbb{R}\) to each empirical singular value. An example widely employed by practitioners is the thresholding function \(\eta_{\theta}(y)=y\mathds{1}\{y>\theta\}\)[27]. In the presence of noise heteroscedasticity, most of these results are based on whitening [43]. Another model of noise heterogeneity common in the literature takes \(W=\widetilde{W}\circ\Delta^{0/2}\), where \(\widetilde{W}\) has i.i.d. Gaussian entries, \(\Delta\) is a deterministic block matrix with fixed (i.e., constant with respect to \(n,d\)) number of blocks, and \(\circ\) denotes the element-wise product. This means that the entries of the noise are independent but non-identically distributed, and they follow the variance profile \(\Delta\). The corresponding low-rank perturbation \(A\), known as a spiked inhomogeneous matrix, has attracted attention from both the information-theoretic [12; 66; 36] and the algorithmic sides [38; 50; 61].

Spiked inhomogeneous matrices have some connections with the model considered in this paper: if \(\Delta\) has rank \(1\), such \(A\) can be realized by taking \(\Xi,\Sigma\) to be diagonal with suitable block structures. Finally, non-asymptotic results for the heteroscedastic and the inhomogeneous models have been derived in varying generality in [76; 82; 23; 1; 17]. We highlight that our paper is the _first_ to establish information-theoretic and algorithmic limits for doubly heteroscedastic noise.

Our characterization of the spectral estimator relies on an AMP algorithm that converges to it by performing power iteration. AMP refers to a family of iterative procedures, whose performance in the high-dimensional limit is precisely characterized by a low-dimensional deterministic recursion called state evolution [11; 15]. Originally introduced for compressed sensing [28], AMP algorithms have been developed for various settings, including low-rank estimation [56; 31; 6] and inference in generalized linear models [64; 65; 71]. Beyond statistical estimation, AMP proves its versatility as both an efficient algorithm and a proof technique for studying e.g. posterior sampling [58], spectral universality [30], first order methods with random data [22], mismatched estimation [8], spectral estimators for generalized linear models [79; 80] and their combination with linear estimators [54].

## 3 Problem setup

Consider the following rank-\(1\) rectangular matrix estimation problem with doubly heteroscedastic noise where we observe

\[A=\frac{\lambda}{n}u^{*}{v^{*}}^{\top}+W\in\mathbb{R}^{n\times d},\] (3.1)

and aim to estimate \(u^{*},v^{*}\). The following assumptions are imposed throughout the paper. The dimensions \(n,d\to\infty\) obey the proportional scaling \(n/d\to\delta\in(0,\infty)\), where \(\delta\) is the aspect ratio. The SNR \(\lambda\in[0,\infty)\) is a known constant (relative to \(n,d\)). The signals \((u^{*},v^{*})\sim P^{\otimes n}\otimes Q^{\otimes d}\) have i.i.d. priors, where \(P,Q\) are distributions on \(\mathbb{R}\) with mean \(0\) and variance \(1\). The unknown noise matrix has the form \(W=\Xi^{1/2}\widetilde{W}\Sigma^{1/2}\in\mathbb{R}^{n\times d}\), with \(\widetilde{W}_{i,j}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N }(0,1/n)\) independent of \((u^{*},v^{*})\). The covariances \(\Xi\in\mathbb{R}^{n\times n},\Sigma\in\mathbb{R}^{d\times d}\) are known, deterministic,1 strictly positive definite and satisfy

Footnote 1: All our results hold verbatim if \(\Xi,\Sigma\) are random matrices independent of each other and of \(u^{*},v^{*},\widetilde{W}\).

\[\lim_{n\to\infty}\frac{1}{n}\operatorname{Tr}(\Xi)=\lim_{d\to\infty}\frac{1} {d}\operatorname{Tr}(\Sigma)=1.\] (3.2)

Their empirical spectral distributions (ESD) converge (as \(n,d\to\infty\) s.t. \(n/d\to\infty\)) weakly to the laws of the random variables \(\overline{\Xi}\) and \(\overline{\Sigma}\). Furthermore, \(\left\|\Xi\right\|_{2},\left\|\Sigma\right\|_{2}\) are uniformly bounded over \(d\). The supports of \(\overline{\Xi},\overline{\Sigma}\) are compact subsets of \((0,\infty)\). For all \(\varepsilon>0\), there exists \(d_{0}\in\mathbb{N}\) s.t. for all \(d\geq d_{0}\),

\[\operatorname{supp}(\operatorname{ESD}(\Xi))\subset\operatorname{supp}( \overline{\Xi})+[-\varepsilon,\varepsilon],\quad\operatorname{supp}( \operatorname{ESD}(\Sigma))\subset\operatorname{supp}(\overline{\Sigma})+[- \varepsilon,\varepsilon].\] (3.3)

The trace assumption (3.2) on the covariances is for normalization purposes since the values of the traces, if not \(1\), can be absorbed into \(\lambda\). The support assumption (3.3) excludes outliers in the spectra of covariances which may contribute to undesirable spikes in \(A\)[69].

## 4 Information-theoretic limits

For mathematical convenience, in this section, we switch to an equivalent rescaled model

\[Y\coloneqq\sqrt{n}A=\sqrt{\frac{\gamma}{n}}u^{*}{v^{*}}^{\top}+\Xi^{1/2}Z \Sigma^{1/2}\in\mathbb{R}^{n\times d},\] (4.1)

where \(\gamma\coloneqq\lambda^{2}\) and \(Z=\sqrt{n}\widetilde{W}\) contains i.i.d. elements \(Z_{i,j}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N}(0,1)\). Abusing terminology, we refer to \(\gamma\) as the SNR of \(Y\). Define also \(\alpha\coloneqq 1/\delta\in(0,\infty)\) so that \(d/n\to\alpha\). The scaling of the parameters in (4.1) turns out to be more convenient for presenting the results in this section. Results for \(Y\) can be easily translated to \(A\) by a change of variables.

Let \(\widetilde{u}^{*}\coloneqq\Xi^{-1/2}u^{*}\) and \(\widetilde{v}^{*}\coloneqq\Sigma^{-1/2}v^{*}\) denote the whitened signals. The main result of this section is Theorem 4.2, which characterizes the performance of the matrix minimum mean square error (MMSE) associated to the estimation of \(\widetilde{u}^{*}(\widetilde{v}^{*})^{\top},\widetilde{u}^{*}(\widetilde{u}^{*})^ {\top}\) and \(\widetilde{v}^{*}(\widetilde{v}^{*})^{\top}\), via the corresponding Bayes-optimal estimators:

\[\mathrm{MMSE}_{n}(\gamma) \coloneqq\frac{1}{nd}\mathbb{E}\Big{[}\big{\|}\widetilde{u}^{*}( \widetilde{v}^{*})^{\top}-\mathbb{E}\big{[}\widetilde{u}^{*}(\widetilde{v}^{* })^{\top}\big{|}\,Y\big{]}\big{\|}_{\mathrm{F}}^{2}\Big{]},\] (4.2) \[\mathrm{MMSE}_{n}^{u}(\gamma) \coloneqq\frac{1}{n^{2}}\mathbb{E}\Big{[}\big{\|}\widetilde{u}^{* }(\widetilde{u}^{*})^{\top}-\mathbb{E}\big{[}\widetilde{u}(\widetilde{u}^{*})^ {\top}\big{|}\,Y\big{]}\big{\|}_{\mathrm{F}}^{2}\Big{]},\] (4.3) \[\mathrm{MMSE}_{n}^{v}(\gamma) \coloneqq\frac{1}{d^{2}}\mathbb{E}\Big{[}\big{\|}\widetilde{v}^{* }(\widetilde{v}^{*})^{\top}-\mathbb{E}\big{[}\widetilde{v}^{*}(\widetilde{v}^ {*})^{\top}\big{|}\,Y\big{]}\big{\|}_{\mathrm{F}}^{2}\Big{]}.\] (4.4)

Our characterization involves a pair of parameters \((q_{u}^{*},q_{v}^{*})\in\mathbb{R}^{2}_{\geq 0}\) defined as the largest solution to

\[q_{u}=\mathbb{E}\Bigg{[}\frac{\alpha\gamma q_{v}\underline{\Xi}^{-2}}{1+ \alpha\gamma q_{v}\underline{\Xi}^{-1}}\Bigg{]},\qquad q_{v}=\mathbb{E}\Bigg{[} \frac{\gamma q_{u}\underline{\Sigma}^{-2}}{1+\gamma q_{u}\underline{\Sigma}^ {-1}}\Bigg{]}.\] (4.5)

Here and throughout the paper, all expectations involving \(\underline{\Xi},\overline{\Sigma}\) are computed as integrals against the limiting spectral distributions of \(\Xi,\Sigma\).

The proposition below, proved in Appendix A, justifies the existence of the solution to (4.5) and identifies when a non-trivial solution emerges.

**Proposition 4.1**.: _The fixed point equation (4.5) always has a trivial solution \((0,0)\). There exists a non-trivial solution \((q_{u}^{*},q_{v}^{*})\in\mathbb{R}^{2}_{>0}\) if and only if_

\[\alpha\gamma^{2}\mathbb{E}\Big{[}\underline{\Sigma}^{-2}\Big{]}\mathbb{E} \Big{[}\overline{\Xi}^{-2}\Big{]}>1,\] (4.6)

_in which case the non-trivial solution is unique._

We are now ready to state our main result on the MMSE.

**Theorem 4.2**.: _Assume \(P=Q=\mathcal{N}(0,1)\). For almost every \(\gamma>0\),_

\[\lim_{n\to\infty}\mathrm{MMSE}_{n}(\gamma)=\mathbb{E}\Big{[} \overline{\Xi}^{-1}\Big{]}\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]}-q_{u }^{*}q_{v}^{*},\] (4.7) \[\lim_{n\to\infty}\mathrm{MMSE}_{n}^{u}(\gamma)=\mathbb{E}\Big{[} \overline{\Xi}^{-1}\Big{]}^{2}-q_{u}^{*\,2},\qquad\lim_{n\to\infty}\mathrm{ MMSE}_{n}^{v}(\gamma)=\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]}^{2}-q_{v}^{*\,2}.\] (4.8)

We note that

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\big{\|}\widetilde{u}^{*}( \widetilde{v}^{*})^{\top}\big{\|}_{\mathrm{F}}^{2}\Big{]}=\lim_{n\to\infty} \frac{1}{nd}\mathbb{E}\Big{[}\big{\|}\widetilde{u}^{*}\big{\|}_{2}^{2}\Big{]} \mathbb{E}\Big{[}\big{\|}\widetilde{v}^{*}\big{\|}_{2}^{2}\Big{]}=\mathbb{E} \Big{[}\overline{\Xi}^{-1}\Big{]}\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]},\] (4.9)

where the last step follows from Proposition G.2. This quantity represents the trivial error in the estimation of \(\widetilde{u}^{*}(\widetilde{v}^{*})^{\top}\), which is achieved by the all-0 estimator. Analogous considerations hold for \(\widetilde{u}^{*}(\widetilde{u}^{*})^{\top}\) and \(\widetilde{v}^{*}(\widetilde{v}^{*})^{\top}\), for which the trivial estimation error is \(\mathbb{E}\Big{[}\overline{\Xi}^{-1}\Big{]}^{2}\) and \(\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]}^{2}\), respectively. Thus, Proposition 4.1 and Theorem 4.2 identify (4.6) as the condition for non-trivial estimation, and the smallest \(\gamma\) that satisfies (4.6) gives the _weak recovery threshold_.

We show below that the weak recovery threshold is the same for the estimation of the true signals \(u^{*}{v^{*}}^{\top},u^{*}{u^{*}}^{\top}\) and \(v^{*}{v^{*}}^{\top}\). In this case, since the signal priors are Gaussian, using the same passages as in (4.9) one has that the trivial estimation error for \(u^{*}{v^{*}}^{\top},u^{*}{u^{*}}^{\top}\) and \(v^{*}{v^{*}}^{\top}\) is always equal to \(1\).

**Corollary 4.3**.: _Assume \(P=Q=\mathcal{N}(0,1)\). The MMSE associated to the estimation of \(u^{*}{v^{*}}^{\top}\) is non-trivial, i.e,_

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\Big{\|}{u^{*}{v^{*}}^{\top}}- \mathbb{E}\Big{[}{u^{*}{v^{*}}^{\top}}\Big{|}\,Y\Big{]}\Big{\|}_{\mathrm{F}}^ {2}\Big{]}<1\] (4.10)

_if and only if (4.6) holds. The same result holds for the MMSE of \({u^{*}{u^{*}}^{\top}}\) and \({v^{*}{v^{*}}^{\top}}\)._

Proof strategy.To derive the characterizations in Theorem 4.2, we write the posterior distribution of \(u^{*},v^{*}\) given \(Y\) in a Gibbs form, i.e., its density is the exponential of a Hamiltonian normalized by a partition function. The interpolation argument relates the log-partition function (also referred to as the 'free energy') of the posterior to that of the posteriors of two Gaussian location models. Since i.i.d. Gaussianity is key to this approach, the challenge is to handle noise covariances. Our idea is 

[MISSING_PAGE_FAIL:5]

_Remark 4.2_ (Gaussian priors).: Theorem 4.4 crucially relies on having Gaussian priors \(P,Q\). This assumption is mainly used to derive single-letter (i.e., dimension-free) expressions of the free energy of the vector models in (4.17) which, under Gaussian priors, are nothing but Gaussian integrals. The free energy, and hence the MMSE, are expected to be sensitive to the priors. Indeed, this is already the case in the homoscedastic setting \(\Xi=I_{n},\Sigma=I_{d}\)[52]. An extension towards general i.i.d. priors is a challenging open problem and, in fact, without posing additional assumptions on \(\Xi,\Sigma\), it is unclear whether a single-letter expression for free energy and MMSE is possible.

At this point, the MMSE can be derived from the above characterization of free energy. Indeed, let

\[\mathcal{D}(\alpha)\coloneqq\{\gamma>0:\mathcal{F}\text{ has a unique maximizer }(q_{u}^{*},q_{v}^{*})\text{ over }\mathcal{C}(\gamma,\alpha)\}.\]

The envelope theorem [51, Corollary 4] ensures that \(\mathcal{D}(\alpha)\) is equal to \(\mathbb{R}_{>0}\) up to a countable set. Using algebraic relations between free energy and MMSE, we prove (4.7) and (4.8) for all \(\gamma\in\mathcal{D}(\alpha)\) (and, thus, for almost every \(\gamma>0\)). Then, using the Nishimori identity (Proposition G.4) and the fact that the ESDs of \(\Xi,\Sigma\) are upper and lower bounded by constants independent of \(n\) and \(d\), Corollary 4.3 also follows. The formal arguments are contained in Appendix D.

## 5 Spectral estimator

This section introduces a spectral estimator that meets the weak recovery threshold and, for one-sided heteroscedasticity, attains the Bayes-optimal error. Suppose that the following condition holds

\[\frac{\lambda^{4}}{\delta}\mathbb{E}\Big{[}\overline{\Sigma}^{-2}\Big{]} \mathbb{E}\Big{[}\overline{\Xi}^{-2}\Big{]}>1,\] (5.1)

which is equivalent to (4.6). Under this condition, the fixed point equations (4.5) have a unique pair of positive solutions \((q_{u}^{*},q_{v}^{*})\). For convenience, we also define the rescalings \(\mu^{*}\coloneqq\lambda q_{v}^{*}/\delta,\nu^{*}\coloneqq\lambda q_{u}^{*}\), and the auxiliary quantities

\[b^{*}\coloneqq\frac{1}{\delta}\mathbb{E}\bigg{[}\frac{\lambda}{\lambda\nu^{* }+\overline{\Sigma}}\bigg{]},\qquad c^{*}\coloneqq\mathbb{E}\bigg{[}\frac{ \lambda}{\lambda\mu^{*}+\overline{\Xi}}\bigg{]}.\] (5.2)

Now, we pre-process the data matrix \(A\) as

\[A^{*}\coloneqq\lambda(\lambda(\mu^{*}+b^{*})I_{n}+\Xi)^{-1/2}\Xi^{-1/2}A \Sigma^{-1/2}(\lambda(\nu^{*}+c^{*})I_{d}+\Sigma)^{-1/2},\] (5.3)

from which we obtain the spectral estimators

\[\widehat{u} \coloneqq\eta_{u}\sqrt{n}\frac{\Xi^{1/2}(\lambda(\mu^{*}+b^{*})I _{n}+\Xi)^{-1/2}(\lambda\mu^{*}I_{n}+\Xi)u_{1}(A^{*})}{\big{\|}\Xi^{1/2}( \lambda(\mu^{*}+b^{*})I_{n}+\Xi)^{-1/2}(\lambda\mu^{*}I_{n}+\Xi)u_{1}(A^{*}) \big{\|}_{2}},\] (5.4a) \[\widehat{v} \coloneqq\eta_{v}\sqrt{d}\frac{\Sigma^{1/2}(\lambda(\nu^{*}+c^{*} )I_{d}+\Sigma)^{-1/2}(\lambda\nu^{*}I_{d}+\Sigma)v_{1}(A^{*})}{\big{\|}\Sigma^{ 1/2}(\lambda(\nu^{*}+c^{*})I_{d}+\Sigma)^{-1/2}(\lambda\nu^{*}I_{d}+\Sigma)v_{ 1}(A^{*})\big{\|}_{2}},\] (5.4b)

where \(u_{1}(\cdot)/v_{1}(\cdot)\) denote the top left/right singular vectors and

\[\eta_{u}\coloneqq\sqrt{\frac{\lambda\mu^{*}}{\lambda\mu^{*}+1}},\qquad\eta_{v }\coloneqq\sqrt{\frac{\lambda\nu^{*}}{\lambda\nu^{*}+1}}.\] (5.5)

Note that \(\eta_{u},\eta_{v}>0\), provided that (5.1) holds. The pre-processing of \(A\) in (5.3) and the form of the spectral estimators in (5.4) come from the derivation of a suitable AMP algorithm, and they are discussed at the end of the section. We finally defer to Appendix E.3 the definition of the scalar quantity \(\sigma_{2}^{*}\) obtained via a fixed point equation depending only on \(\overline{\Xi},\overline{\Sigma},\lambda,\delta\), see (E.26) for details.

Our main result, Theorem 5.1, shows that, under the criticality condition (5.1), the matrix \(A^{*}\) exhibits a spectral gap between the top two singular values, and it characterizes the performance of the spectral estimators in (5.4), proving that they achieve weak recovery of \(u^{*}\) and \(v^{*}\), respectively.

**Theorem 5.1**.: _Suppose that (5.1) holds and that, for any \(c>0\),_

\[\lim_{\beta\downarrow s}\mathbb{E}\bigg{[}\frac{\overline{\Sigma}^{*}}{\beta-c \overline{\Sigma}^{*}}\bigg{]}=\lim_{\beta\downarrow s}\mathbb{E}\left[\left( \frac{\overline{\Sigma}^{*}}{\beta-c\overline{\Sigma}^{*}}\right)^{2} \right]=\infty,\quad\lim_{\alpha\downarrow\sup\operatorname{supp}(\overline{ \Xi}^{*})}\mathbb{E}\left[\frac{\overline{\Xi}^{*}}{\alpha-\overline{\Xi}^{*}} \right]=\infty,\] (5.6)_where \(\overline{\Xi}^{*}\coloneqq\frac{\lambda}{\lambda(a^{*}+b^{*})+\Xi}\). \(\overline{\Sigma}^{*}\coloneqq\frac{\lambda}{\lambda(a^{*}+c^{*})+\Sigma}\) and \(s\coloneqq c\cdot\sup\mathrm{supp}(\overline{\Sigma}^{*})\). Let \(A^{*},\widehat{u},\widehat{v},\sigma_{2}^{*}\) be defined in (5.3), (5.4) and (E.26), and \(\sigma_{i}(A^{*})\) denote the \(i\)-th largest singular value of \(A^{*}\). Then, if \(\sigma_{2}^{*}<1\), the following limits hold in probability:_

\[\lim_{n\to\infty}\sigma_{1}(A^{*})=1>\sigma_{2}^{*}=\lim_{n\to \infty}\sigma_{2}(A^{*}),\] (5.7) \[\lim_{n\to\infty}\frac{|\langle\widehat{u},u^{*}\rangle|}{\| \widehat{u}\|_{2}\|u^{*}\|_{2}}=\eta_{u},\quad\lim_{d\to\infty}\frac{|\langle \widehat{v},v^{*}\rangle|}{\|\widehat{v}\|_{2}\|v^{*}\|_{2}}=\eta_{v}\] (5.8) \[\lim_{n\to\infty}\frac{1}{n^{2}}\Big{\|}u^{*}{u^{*}}^{\top}- \widehat{u}\widehat{u}^{\top}\Big{\|}_{\mathrm{F}}^{2}=1-\eta_{u}^{4},\quad \lim_{d\to\infty}\frac{1}{d^{2}}\Big{\|}{v^{*}{v^{*}}^{\top}}-\widehat{v} \widehat{v}^{\top}\Big{\|}_{\mathrm{F}}^{2}=1-\eta_{v}^{4},\] (5.9) \[\lim_{n\to\infty}\frac{1}{nd}\Big{\|}{u^{*}{v^{*}}^{\top}}- \widehat{u}\widehat{v}^{\top}\Big{\|}_{\mathrm{F}}^{2}=1-\eta_{u}^{2}\eta_{v} ^{2}.\] (5.10)

_Remark 5.1_ (Assumptions).: To guarantee a spectral gap for \(A^{*}\) and the weak recoverability of \(u^{*},v^{*}\) via the proposed spectral method, we also require the algebraic condition \(\sigma_{2}^{*}<1\). We conjecture that this condition is implied by (5.1), and we have verified that this is the case in all our numerical experiments (see Figure 1 for two concrete examples). The additional assumption (5.6) is a mild regularity condition on the covariances. It ensures that the densities of \(\overline{\Xi}^{*},\overline{\Sigma}^{*}\) decay sufficiently slowly at the edges of the support, so that \(\sigma_{2}^{*}\) is well-posed [79].

_Remark 5.2_ (Signal priors).: Theorem 5.1 does not require the prior distributions \(P,Q\) to be Gaussian, and it is valid for any i.i.d. prior with mean \(0\) and variance \(1\).

On the one hand, Corollary 4.3 shows that, if (5.1) is violated, the problem is information-theoretically impossible, i.e., no estimator achieves non-trivial error. On the other hand, Theorem 5.1 exhibits a pair of estimators that achieves non-trivial error as soon as (5.1) holds - under the additional assumption \(\sigma_{2}^{*}<1\) which we conjecture to be implied by (5.1). Thus, the spectral method in (5.4) is optimal in terms of weak recovery threshold. Though such estimators do not attain the optimal error, when both priors are Gaussian and \(\Xi=I_{n}\), \(\widehat{u}\widehat{u}^{\top}\) is the Bayes-optimal estimate for \(u^{*}{u^{*}}^{\top}\).

**Corollary 5.2**.: _Assume \(P=Q=\mathcal{N}(0,1)\), and consider the setting of Theorem 5.1 with the additional assumption \(\Xi=I_{n}\). Then, \(\eta_{u}=\sqrt{q_{u^{*}}^{*}}\), i.e., \(\widehat{u}\widehat{u}^{\top}\) achieves the MMSE for \(u^{*}{u^{*}}^{\top}\)._

The claim readily follows by noting that, when \(\Xi=I_{n}\), the first equation in (4.5) becomes

\[q_{u}^{*}=\frac{\alpha\gamma q_{v}^{*}}{1+\alpha\gamma q_{v}^{*}}=\frac{( \lambda^{2}/\delta)(\delta{\mu^{*}}/\lambda)}{1+(\lambda^{2}/\delta)(\delta{ \mu^{*}}/\lambda)}=\frac{\lambda{\mu^{*}}}{1+\lambda{\mu^{*}}}=\eta_{u}^{2},\]

where the last equality is by the definition (5.5) of \(\eta_{u}\). Let us highlight that, even if \(\Xi=I_{n}\), \(\widehat{u}\) still makes non-trivial use of the other covariance \(\Sigma^{1/2}\). At the information-theoretic level, this is reflected by the fact that \(\Sigma^{1/2}\) enters \(q_{u}^{*}\) through the fixed point equations (4.5). Therefore, even though the matrix model in (4.1) is equivalent to a pair of uncorrelated vector models in (4.17) in the sense of the free energy, the tasks of estimating \(u^{*}\) and \(v^{*}\) cannot be decoupled.

Figure 1: Top two singular values of \(A^{*}\) in (5.3), where \(d=4000,\delta=4\) and each simulation is averaged over \(10\) i.i.d. trials. The singular values computed experimentally (‘sim’ in the legends and \(\times\) in the plots) closely match our theoretical prediction in (5.7) (‘thy’ in the legends and solid curves with the same color in the plots). The threshold \(\lambda^{*}\) is such that equality holds in (5.1). We note that the green curve corresponding to \(\sigma_{2}^{*}\) is smaller than \(1\) for \(\lambda>\lambda^{*}\), i.e., when (5.1) holds.

[MISSING_PAGE_FAIL:8]

\[v^{t+1}=\Sigma^{-1}A^{\top}\Xi^{-1}\widetilde{u}^{t}-c_{t}\Sigma^{-1}\widetilde{v }^{t},\quad\widetilde{v}^{t+1}=f^{*}_{t+1}(v^{t+1}),\quad b_{t+1}=\frac{1}{n} \operatorname{Tr}((\nabla f^{*}_{t+1}(v^{t+1}))\Sigma^{-1}),\]

where \(\nabla\) denotes the Jacobian matrix, and the functions \(g^{*}_{t},f^{*}_{t+1}\) are specified below in (5.12). As common in AMP algorithms, the iterates (5.11) are accompanied with a state evolution which accurately tracks their behavior via a simple deterministic recursion: the joint empirical distribution of \((u^{*},v^{*},u^{t},v^{t+1})\) converges to the random variables \((U^{*},V^{*},U_{t},V_{t+1})\), see Proposition E.1 for a formal statement and the recursive description of the laws of such random variables. Then, the name 'Bayes-AMP' is motivated by the fact that \(g^{*}_{t},f^{*}_{t+1}\) are the posterior-mean denoisers given by

\[g^{*}_{t}(u)\coloneqq\mathbb{E}[U^{*}\,|\,U_{t}=u],\qquad f^{*}_{t+1}(v) \coloneqq\mathbb{E}[V^{*}\,|\,V_{t+1}=v].\] (5.12)

Remarkably, Bayes-AMP operates on \(\Xi^{-1}A\Sigma^{-1}\), as opposed to the widely adopted ansatz of considering the whitened matrix \(\Xi^{-1/2}A\Sigma^{-1/2}\). The advantage of operating on \(\Xi^{-1}A\Sigma^{-1}\) is that the fixed point of the corresponding state evolution matches the extremizers of the free energy in (4.5). This would _not_ be the case if Bayes-AMP used the whitening \(\Xi^{-1/2}A\Sigma^{-1/2}\). Indeed, one can repeat the analysis of an AMP that operates on \(\Xi^{-1/2}A\Sigma^{-1/2}\). The fixed point equations of the resulting state evolution do not match the information-theoretically optimal one in (4.5). In particular, the weak recovery threshold coming out of this approach is strictly larger than the optimal one in (4.6), as long as at least one of \(\Xi,\Sigma\) is not a multiple of the identity. Since these derivations led to suboptimal results, the details were left out from the paper.

The design of Bayes-AMP and the proof of its state evolution follow a two-step reduction detailed in Appendix F. Using a change of variables, we show in Appendix F.2 that Bayes-AMP can be realized by an auxiliary AMP with non-separable denoising functions (meaning that \(g_{t},f_{t+1}\) cannot be written as univariate functions applied component-wise) operating on \(\Xi^{-1/2}A\Sigma^{-1/2}=\frac{\lambda}{n}\widetilde{u}^{*}(\widetilde{v}^{* })^{\top}+\widetilde{W}\). Then, in Appendix F.1 we simulate the auxiliary AMP using a standard AMP operating on the i.i.d. Gaussian matrix \(\widetilde{W}\), whose state evolution has been established in [13, 34].

However, Bayes-AMP by itself is not a practical algorithm since it needs a warm start, i.e., an initialization that achieves non-trivial error. Thus, the _second step_ is to design a spectral estimator that solves the fixed point equation of Bayes-AMP, which turns out to be an eigen-equation for \(A^{*}\).

To offer the readers an intuition on how the spectral estimators arise from Bayes-AMP, we now heuristically derive the form (5.3) of \(A^{*}\) and the expression (5.4) of the spectral estimator. To do so, we note that the large-\(n\) limits of \(c_{t},b_{t+1}\) coincide with the auxiliary quantities \(c^{*},b^{*}\) defined in (5.2).

Figure 4: Spectra of \(A\) and \(A^{*}\) averaged over \(10\) i.i.d. trials, where \(d=4000,\delta=4\). An outlier singular value emerges in the spectrum of \(A^{*}\) due to the pre-processing on \(A\).

Furthermore, when the priors of \(u^{*},v^{*}\) are Gaussian, (5.12) reduces to

\[g^{*}_{t}(u)=\lambda(\lambda\mu^{*}\Xi^{-1}+I_{n})^{-1}u,\qquad f^{*}_{t+1}(v)= \lambda(\lambda\nu^{*}\Sigma^{-1}+I_{d})^{-1}v,\]

where we recall that \(\mu^{*}=\lambda q^{*}_{v}/\delta\) and \(\nu^{*}=\lambda q^{*}_{u}\) are rescalings of the non-trivial solution \((q^{*}_{u},q^{*}_{v})\) of (4.5). Denoting by \(u,v\) the fixed points of the iteration (5.11), after some manipulations we have

\[\mathfrak{g}(\Xi)u=A^{*}\mathfrak{f}(\Sigma)v,\qquad\mathfrak{f}(\Sigma)v=A^{* ^{*}}\,\mathfrak{g}(\Xi)u,\]

where \(A^{*}\) is given in (5.3) and

\[\mathfrak{g}(\Xi) \coloneqq\sqrt{\lambda}(\lambda(\nu^{*}+b^{*})I_{n}+\Xi)^{1/2}( \lambda\mu^{*}I_{n}+\Xi)^{-1}\Xi^{1/2},\] \[\mathfrak{f}(\Sigma) \coloneqq\sqrt{\lambda}(\lambda(\nu^{*}+c^{*})I_{d}+\Sigma)^{1/ 2}(\lambda\nu^{*}I_{d}+\Sigma)^{-1}\Sigma^{1/2}.\]

This suggests that \(A^{*}\) has top singular value equal to \(1\) and \((\mathfrak{g}(\Xi)u,\mathfrak{f}(\Sigma)v)\) are aligned with the corresponding singular vectors \((u_{1}(A^{*}),v_{1}(A^{*}))\). Moreover, state evolution implies that the distribution of the fixed point \((u,v)\) is close to that of

\[(\mu^{*}\Xi^{-1}u^{*}+\sqrt{\mu^{*}/\lambda}w_{u},\nu^{*}\Sigma^{-1}v^{*}+ \sqrt{\nu^{*}/\lambda}w_{v}),\]

with \((w_{u},w_{v})\sim\mathcal{N}(0_{n},\Xi^{-1})\otimes\mathcal{N}(0_{d},\Sigma^{ -1})\) independent of \(u^{*},v^{*}\). Thus, to obtain estimates of \((u^{*},v^{*})\), we take \((\Xi\mathfrak{g}(\Xi)^{-1}u_{1}(A^{*}),\Sigma\mathfrak{f}(\Sigma)^{-1}v_{1}( A^{*}))\) and suitably rescale their norm, which leads to the expressions in (5.4). More details on the above heuristics are discussed in Appendix E.2.

The most outstanding step remains to make the heuristics rigorous. This involves proving that \(\Xi u^{t},\Sigma v^{t+1}\) are aligned with the proposed spectral estimator, which allows for a performance characterization via state evolution. The formal argument is carried out in Appendix E.4.

## 6 Concluding remarks

In this work, we establish information-theoretic limits and propose an efficient spectral method with optimality guarantees, for matrix estimation with doubly heteroscedastic noise. On the one hand, under Gaussian priors, we give a rigorous characterization of the MMSE; on the other hand, we present a spectral estimator that _(i)_ achieves the information-theoretic weak recovery threshold, and _(ii)_ is Bayes-optimal for the estimation of one of the signals, when the noise is heteroscedastic only on the other side. While our analysis focuses on rank-1 estimation, we expect that all results admit proper extensions to rank-\(r\) signals, where \(r\) is a constant independent on \(n,d\).

The design and analysis of the spectral estimator draws connections with approximate message passing and, along the way, we introduce a Bayes-AMP algorithm which could be of independent interest. In this paper, we employ Bayes-AMP solely as a proof technique. However, one could use the spectral method designed here as an initialization of Bayes-AMP itself, after suitably correcting its iterates. This strategy has been successfully carried out for i.i.d. Gaussian noise in [56] and for rotationally invariant noise in [55; 81]. Bayes-AMP is well equipped to exploit signal priors more informative than the Gaussian one, and AMP algorithms are known to achieve the information-theoretically optimal estimation error for low-rank matrix inference [56; 6]. Nevertheless, we point out two obstacles towards doing so in the presence of doubly heteroscedastic noise. First, for general priors, establishing the information-theoretic limits remains a challenging open problem, and it is unclear whether a low-dimensional characterization of the free energy (and, hence, of the MMSE) is possible. Second, even for Gaussian priors, Bayes-AMP reduces to the proposed spectral estimator, which is not Bayes-optimal for the general case of doubly heteroscedastic noise.

Finally, the proposed spectral estimator makes non-trivial use of the covariances \(\Xi,\Sigma\), which are assumed to be known. When such matrices possess additional structure - e.g., they are sparse [21], their inverses are sparse [19] or they are circulant or Toeplitz [75] - their consistent estimation is possible, see also the survey [20]. However, in general, \(\Xi,\Sigma\) cannot be consistently estimated from the data when \(n\) and \(d\) grow proportionally. Thus, a challenging open problem is to construct estimators that retain comparable performance without knowing the noise covariances. The paper [33] addresses the challenge of unknown covariances by considering a modified model where one additionally observes an independent copy of noise. The statistician can then estimate the covariance from the noise-only observation and use it as a surrogate of the true covariance for estimating the signals from the spiked model. It is possible to derive similar results in the doubly heteroskedastic setting considered in our paper. If the covariances are completely unknown, then our model (with Gaussian priors) is equivalent to a spiked matrix model with a certain bi-rotationally invariant noise. This problem is expected to exhibit rather different behaviors than when covariances are known, see [7; 29] for recent progress on understanding the statistical and computational limits for such models.

## Acknowledgments and Disclosure of Funding

YZ thanks Shashank Vatedka for discussions at the early stage of this project. MM thanks Jean Barbier for sharing his insights into the interpolation argument. This research is partially supported by the 2019 Lopez-Loreta Prize and by the Interdisciplinary Projects Committee (IPC) at the Institute of Science and Technology Austria (ISTA). This work was done in part while the authors were visiting the Simons Institute for the Theory of Computing.

## References

* [1] Joshua Agterberg, Zachary Lubberts, and Carey E. Priebe. Entrywise estimation of singular vectors of low-rank matrices with heteroskedasticity and dependence. _IEEE Trans. Inform. Theory_, 68(7):4618-4650, 2022.
* [2] Michael Aizenman, Robert Sims, and Shannon L. Starr. Extended variational principle for the sherrington-kirkpatrick spin-glass model. _Phys. Rev. B_, 68:214403, Dec 2003.
* [3] Z. D. Bai and Y. Q. Yin. Limit of the smallest eigenvalue of a large-dimensional sample covariance matrix. _Ann. Probab._, 21(3):1275-1294, 1993.
* [4] Jinho Baik, Gerard Ben Arous, and Sandrine Peche. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices. _Ann. Probab._, 33(5):1643-1697, 2005.
* [5] Stephen Bailey. Principal component analysis with noisy and/or missing data. _Publications of the Astronomical Society of the Pacific_, 124(919):1015, sep 2012.
* [6] Jean Barbier, Francesco Camilli, Marco Mondelli, and Manuel Saenz. Fundamental limits in structured principal component analysis and how to reach them. _Proc. Natl. Acad. Sci. USA_, 120(30):Paper No. e2302028120, 7, 2023.
* [7] Jean Barbier, Francesco Camilli, Marco Mondelli, and Yizhou Xu. Information limits and thouless-anderson-palmer equations for spiked matrix models with structured noise. _CoRR_, abs/2405.20993, 2024.
* [8] Jean Barbier, TianQi Hou, Marco Mondelli, and Manuel Saenz. The price of ignorance: how much does it cost to forget noise structure in low-rank matrix estimation? In _Advances in Neural Information Processing Systems_, volume 35, pages 36733-36747, 2022.
* [9] Jean Barbier, Florent Krzakala, Nicolas Macris, Leo Miolane, and Lenka Zdeborova. Optimal errors and phase transitions in high-dimensional generalized linear models. _Proc. Natl. Acad. Sci. USA_, 116(12):5451-5460, 2019.
* [10] Jean Barbier and Nicolas Macris. The adaptive interpolation method: a simple scheme to prove replica formulas in Bayesian inference. _Probab. Theory Related Fields_, 174(3-4):1133-1185, 2019.
* [11] Mohsen Bayati and Andrea Montanari. The dynamics of message passing on dense graphs, with applications to compressed sensing. _IEEE Trans. Inform. Theory_, 57(2):764-785, 2011.
* [12] Joshua K. Behne and Galen Reeves. Fundamental limits for rank-one matrix estimation with groupwise heteroskedasticity. In _International Conference on Artificial Intelligence and Statistics_, pages 8650-8672, 2022.
* [13] Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen. State evolution for approximate message passing with non-separable functions. _Inf. Inference_, 9(1):33-79, 2020.
* [14] Tejal Bhamre, Teng Zhang, and Amit Singer. Denoising and covariance estimation of single particle cryo-em images. _Journal of Structural Biology_, 195(1):72-81, 2016.
* [15] Erwin Bolthausen. An iterative construction of solutions of the TAP equations for the Sherrington-Kirkpatrick model. _Comm. Math. Phys._, 325(1):333-366, 2014.
* [16] Stephane Boucheron, Gabor Lugosi, and Pascal Massart. _Concentration inequalities_. Oxford University Press, Oxford, 2013.

* [17] T. Tony Cai, Rungang Han, and Anru R. Zhang. On the non-asymptotic concentration of heteroskedastic Wishart-type matrix. _Electron. J. Probab._, 27:Paper No. 29, 40, 2022.
* [18] T. Tony Cai, Zongming Ma, and Yihong Wu. Sparse PCA: optimal rates and adaptive estimation. _Ann. Statist._, 41(6):3074-3110, 2013.
* [19] T. Tony Cai, Zhao Ren, and Harrison H. Zhou. Optimal rates of convergence for estimating Toeplitz covariance matrices. _Probab. Theory Related Fields_, 156(1-2):101-143, 2013.
* [20] T. Tony Cai, Zhao Ren, and Harrison H. Zhou. Estimating structured high-dimensional covariance and precision matrices: optimal rates and adaptive estimation. _Electron. J. Stat._, 10(1):1-59, 2016.
* [21] T. Tony Cai and Harrison H. Zhou. Optimal rates of convergence for sparse covariance matrix estimation. _Ann. Statist._, 40(5):2389-2420, 2012.
* [22] Michael Celentano, Chen Cheng, and Andrea Montanari. The high-dimensional asymptotics of first order methods with random data. _arXiv preprint arXiv:2112.07572_, 2021.
* [23] Chen Cheng, Yuting Wei, and Yuxin Chen. Tackling small eigen-gaps: fine-grained eigenvector estimation and inference under heteroscedastic noise. _IEEE Trans. Inform. Theory_, 67(11):7380-7419, 2021.
* [24] Lucilio Cordero-Grande, Daan Christiaens, Jana Hutter, Anthony N. Price, and Jo V. Hajnal. Complex diffusion-weighted image estimation via matrix recovery under general noise models. _NeuroImage_, 200:391-404, 2019.
* [25] Romain Couillet and Walid Hachem. Analysis of the limiting spectral measure of large random matrices of the separable covariance type. _Random Matrices Theory Appl._, 3(4):1450016, 23, 2014.
* [26] Xiucai Ding, Yun Li, and Fan Yang. Eigenvector distributions and optimal shrinkage estimators for large covariance and precision matrices. _arXiv preprint arXiv:2404.14751_, 2024.
* [27] David Donoho, Matan Gavish, and Elad Romanov. _ScreeNOT_: exact MSE-optimal singular value thresholding in correlated noise. _Ann. Statist._, 51(1):122-148, 2023.
* [28] David L. Donoho, Arian Maleki, and Andrea Montanari. Message passing algorithms for compressed sensing. _Proceedings of the National Academy of Sciences_, 106:18914-18919, 2009.
* [29] Rishabh Dudeja, Songbin Liu, and Junjie Ma. Optimality of approximate message passing algorithms for spiked matrix models with rotationally invariant noise. _CoRR_, abs/2405.18081, 2024.
* [30] Rishabh Dudeja, Subhabrata Sen, and Yue M Lu. Spectral universality of regularized linear regression with nearly deterministic sensing matrices. _IEEE Transactions on Information Theory_, 2024.
* [31] Zhou Fan. Approximate message passing algorithms for rotationally invariant matrices. _The Annals of Statistics_, 50(1):197-224, 2022.
* [32] Oliver Y Feng, Ramji Venkataramanan, Cynthia Rush, Richard J Samworth, et al. A unifying tutorial on approximate message passing. _Foundations and Trends(r) in Machine Learning_, 15(4):335-536, 2022.
* [33] Matan Gavish, William Leeb, and Elad Romanov. Matrix denoising with partial noise statistics: optimal singular value shrinkage of spiked F-matrices. _Inf. Inference_, 12(3):Paper No. iaad028, 46, 2023.
* [34] Cedric Gerbelot and Raphael Berthier. Graph-based approximate message passing iterations. _Inf. Inference_, 12(4):Paper No. iaad020, 67, 2023.
* [35] Francesco Guerra. Broken replica symmetry bounds in the mean field spin glass model. _Comm. Math. Phys._, 233(1):1-12, 2003.

* [36] Alice Guionnet, Justin Ko, Florent Krzakala, and Lenka Zdeborova. Low-rank matrix estimation with inhomogeneous noise. _arXiv preprint arXiv:2208.05918_, 2022.
* [37] Philip Hartman. _Ordinary differential equations_, volume 38. Society for Industrial and Applied Mathematics (SIAM), Philadelphia, PA, 2002.
* [38] David Hong, Fan Yang, Jeffrey A. Fessler, and Laura Balzano. Optimally weighted PCA for high-dimensional heteroscedastic data. _SIAM J. Math. Data Sci._, 5(1):222-250, 2023.
* [39] Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis testing for high-dimensional regression. _J. Mach. Learn. Res._, 15:2869-2909, 2014.
* [40] Adel Javanmard and Andrea Montanari. Hypothesis testing in high-dimensional regression under the Gaussian random design model: asymptotic theory. _IEEE Trans. Inform. Theory_, 60(10):6522-6554, 2014.
* [41] Adel Javanmard and Andrea Montanari. Debiasing the Lasso: optimal sample size for Gaussian designs. _Ann. Statist._, 46(6A):2593-2622, 2018.
* [42] Iain M. Johnstone. On the distribution of the largest eigenvalue in principal components analysis. _Ann. Statist._, 29(2):295-327, 2001.
* [43] Boris Landa, Thomas T. C. K. Zhang, and Yuval Kluger. Biwhitening reveals the rank of a count matrix. _SIAM J. Math. Data Sci._, 4(4):1420-1446, 2022.
* [44] William Leeb and Elad Romanov. Optimal spectral shrinkage and PCA with heteroscedastic noise. _IEEE Trans. Inform. Theory_, 67(5):3009-3037, 2021.
* [45] William E. Leeb. Matrix denoising for weighted loss functions and heterogeneous signals. _SIAM J. Math. Data Sci._, 3(3):987-1012, 2021.
* [46] Jeffrey T. Leek. Asymptotic conditional singular value decomposition for high-dimensional genomic data. _Biometrics_, 67(2):344-352, 2011.
* [47] Thibault Lesieur, Florent Krzakala, and Lenka Zdeborova. Mmse of probabilistic low-rank matrix estimation: Universality with respect to the output channel. In _2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 680-687, 2015.
* [48] Yue M. Lu. Householder dice: a matrix-free algorithm for simulating dynamics on Gaussian and random orthogonal ensembles. _IEEE Trans. Inform. Theory_, 67(12):8264-8272, 2021.
* [49] Clement Luneau, Jean Barbier, and Nicolas Macris. Mutual information for low-rank even-order symmetric tensor estimation. _Inf. Inference_, 10(4):1167-1207, 2021.
* [50] Pierre Mergny, Justin Ko, and Florent Krzakala. Spectral phase transition and optimal pca in block-structured spiked models. _arXiv preprint arXiv:2403.03695_, 2024.
* [51] Paul Milgrom and Ilya Segal. Envelope theorems for arbitrary choice sets. _Econometrica_, 70(2):583-601, 2002.
* [52] Leo Miolane. Fundamental limits of low-rank matrix estimation: the non-symmetric case. _arXiv preprint arXiv:1702.00473_, 2017.
* ENS PARIS ; Inria Paris, June 2019.
* [54] Marco Mondelli, Christos Thrampoulidis, and Ramji Venkataramanan. Optimal combination of linear and spectral estimators for generalized linear models. _Foundations of Computational Mathematics_, pages 1-54, 2021.
* [55] Marco Mondelli and Ramji Venkataramanan. Pca initialization for approximate message passing in rotationally invariant models. In _Advances in Neural Information Processing Systems_, volume 34, pages 29616-29629, 2021.

* [56] Andrea Montanari and Ramji Venkataramanan. Estimation of low-rank matrices via approximate message passing. _Ann. Statist._, 49(1):321-345, 2021.
* [57] Andrea Montanari and Yuchen Wu. Fundamental limits of low-rank matrix estimation with diverging aspect ratios. _arXiv preprint arXiv:2211.00488_, 2022.
* [58] Andrea Montanari and Yuchen Wu. Posterior sampling from the spiked models via diffusion processes. _arXiv preprint arXiv:2304.11449_, 2023.
* [59] Raj Rao Nadakuditi. OptShrink: an algorithm for improved low-rank signal matrix denoising by optimal, data-driven singular value shrinkage. _IEEE Trans. Inform. Theory_, 60(5):3002-3018, 2014.
* [60] Boaz Nadler. Finite sample approximation results for principal component analysis: a matrix perturbation approach. _Ann. Statist._, 36(6):2791-2817, 2008.
* [61] Aleksandr Pak, Justin Ko, and Florent Krzakala. Optimal algorithms for the inhomogeneous spiked wigner model. In _Advances in Neural Information Processing Systems_, volume 36, pages 76409-76424, 2023.
* [62] Dmitry Panchenko. _The Sherrington-Kirkpatrick model_. Springer Monographs in Mathematics. Springer, New York, 2013.
* [63] Henrik Pedersen, Sebastian Kozerke, Steffen Ringgaard, Kay Nehrke, and Won Yong Kim. k-t pca: Temporally constrained k-t blast reconstruction using principal component analysis. _Magnetic Resonance in Medicine_, 62(3):706-716, 2009.
* [64] S. Rangan. Generalized approximate message passing for estimation with random linear mixing. In _IEEE International Symposium on Information Theory (ISIT)_, 2011.
* [65] Sundeep Rangan, Philip Schniter, and Alyson K Fletcher. Vector approximate message passing. _IEEE Transactions on Information Theory_, 65(10):6664-6684, 2019.
* [66] Galen Reeves. Information-theoretic limits for the matrix tensor product. _IEEE Journal on Selected Areas in Information Theory_, 1(3):777-798, 2020.
* [67] R. Tyrrell Rockafellar. _Convex analysis_. Princeton Landmarks in Mathematics. Princeton University Press, Princeton, NJ, 1997.
* [68] Charles M. Stein. Estimation of the mean of a multivariate normal distribution. _The Annals of Statistics_, 9(6):1135-1151, 1981.
* [69] Pei-Chun Su and Hau-Tieng Wu. Data-driven optimal shrinkage of singular values under high-dimensional noise with separable covariance structure. _arXiv preprint arXiv:2207.03466_, 2022.
* [70] O. Tamuz, T. Mazeh, and S. Zucker. Correcting systematic effects in a large set of photometric light curves. _Monthly Notices of the Royal Astronomical Society_, 356(4):1466-1470, 02 2005.
* [71] Ramji Venkataramanan, Kevin Kogler, and Marco Mondelli. Estimation in rotationally invariant generalized linear models via approximate message passing. In _International Conference on Machine Learning (ICML)_, 2022.
* [72] Yihong Wu and Harrison H. Zhou. Randomly initialized EM algorithm for two-component Gaussian mixture achieves near optimality in \(O(\sqrt{n})\) iterations. _Math. Stat. Learn._, 4(3-4):143-220, 2021.
* [73] Kaylee Y Yang, Timothy LH Wee, and Zhou Fan. Asymptotic mutual information in quadratic estimation problems over compact groups. _arXiv preprint arXiv:2404.10169_, 2024.
* [74] Xiaodong Yang, Buyu Lin, and Subhabrata Sen. Fundamental limits of community detection from multi-view data: multi-layer, dynamic and partially labeled block models. _arXiv preprint arXiv:2401.08167_, 2024.

* [75] Ming Yuan. High dimensional inverse covariance matrix estimation via linear programming. _J. Mach. Learn. Res._, 11:2261-2286, 2010.
* [76] Anru R. Zhang, T. Tony Cai, and Yihong Wu. Heteroskedastic PCA: algorithm, optimality, and applications. _Ann. Statist._, 50(1):53-80, 2022.
* [77] Cun-Hui Zhang and Stephanie S. Zhang. Confidence intervals for low dimensional parameters in high dimensional linear models. _J. R. Stat. Soc. Ser. B. Stat. Methodol._, 76(1):217-242, 2014.
* [78] Lixin Zhang. _Spectral analysis of large dimentional random matrices_. PhD thesis, National University of Singapore, 2007.
* [79] Yihan Zhang, Hong Chang Ji, Ramji Venkataramanan, and Marco Mondelli. Spectral estimators for structured generalized linear models via approximate message passing. _arXiv preprint arXiv:2308.14507_, 2023.
* [80] Yihan Zhang, Marco Mondelli, and Ramji Venkataramanan. Precise asymptotics for spectral methods in mixed generalized linear models. _arXiv preprint arXiv:2211.11368_, 2022.
* [81] Xinyi Zhong, Tianhao Wang, and Zhou Fan. Approximate message passing for orthogonally invariant ensembles: Multivariate non-linearities and spectral initialization. _arXiv preprint arXiv:2110.02318_, 2021.
* [82] Yuchen Zhou and Yuxin Chen. Deflated heteropca: Overcoming the curse of ill-conditioning in heteroskedastic pca. _arXiv preprint arXiv:2303.06198_, 2023.

Notation.All vectors are column vectors. The singular values of a matrix \(A\in\mathbb{R}^{n\times d}\) (where \(n\geq d\) without loss of generality) are denoted by \(\sigma_{1}(A)\geq\cdots\geq\sigma_{d}(A)\geq 0\) and the corresponding left/right singular vectors are denoted by \(u_{1}(A),\cdots,u_{d}(A)\in\mathbb{S}^{n-1}\) and \(v_{1}(A),\cdots,v_{d}(A)\in\mathbb{S}^{d-1}\). The (real) eigenvalues of a symmetric matrix \(B\in\mathbb{R}^{d\times d}\) are denoted by \(\lambda_{1}(B)\geq\cdots\geq\lambda_{d}(B)\) and the corresponding eigenvectors are denoted by \(v_{1}(B),\cdots,v_{d}(B)\in\mathbb{S}^{d-1}\) (which will not be confused with the right singular vectors, whenever they are different, since we will never talk about both simultaneously for a square asymmetric matrix). We generally put overlines on capital letters to indicate a scalar random variable, e.g., \(\overline{X}\in\mathbb{R}\), whose support is denoted by \(\mathrm{supp}(\overline{X})\). The limit/liminf/limsup in probability are denoted by \(\mathrm{p}\)-\(\mathrm{lim}\), \(\mathrm{p}\)-\(\mathrm{lim}\)irl, \(\mathrm{p}\)-\(\mathrm{limsup}\). The product distribution whose \(i\)-th (\(i\in[k]\)) marginal is given by \(P_{i}\) is denoted by \(P_{1}\otimes\cdots\otimes P_{k}\), with the shorthand \(P^{\otimes k}\) when all \(P_{i}\)'s are equal to \(P\). The gradient of \(f\colon\mathbb{R}^{n}\to\mathbb{R}\), or with abuse of notation, the Jacobian matrix of \(F\colon\mathbb{R}^{n}\to\mathbb{R}^{d}\) are denoted by \(\nabla f\in\mathbb{R}^{n},\nabla F\in\mathbb{R}^{d\times n}\). The partial derivative of \(f(x_{1},\cdots,x_{n})\) with respect to \(x_{i}\) is denoted by either \(\frac{\partial}{\partial x_{i}}f(x_{1},\cdots,x_{n})\) or \(\partial_{i}f(x_{1},\cdots,x_{n})\). All \(\log\) and \(\exp\) are to the base \(e\). We use the standard notation of \(\sup(S),\inf(S)\) for a subset \(S\subset\mathbb{R}\). We generally use \(C>0\) to denote a sufficiently large constant independent of \(n,d\). Its dependence on other parameters will be specified, though its value may change across passages. We use the standard big O notation.

## Appendix A Proof of Proposition 4.1

We eliminate \(q_{u}\) and write a fixed point equation only involving \(q_{v}\):

\[q_{v}=\mathbb{E}\Bigg{[}\frac{\gamma\mathbb{E}\Big{[}\frac{\alpha\gamma q_{u} \frac{\Xi^{-2}}{\alpha\gamma q_{v}\frac{\Xi^{-2}}{\Xi^{-1}}+1}\Big{]}\overline {\Sigma}^{-2}}}{\gamma\mathbb{E}\Big{[}\frac{\alpha\gamma q_{v}\frac{\Xi^{-2} }{\alpha\gamma q_{v}\frac{\Xi^{-1}}{\Xi^{-1}}+1}\Big{]}\overline{\Sigma}^{-1} +1}}\Bigg{]}.\]

Denote the RHS by \(f(q_{v})\). Recall that we are only interested in non-negative solutions \((q_{u},q_{v})\). So let us restrict attention on \(f\) to the domain \(\mathbb{R}_{\geq 0}\). We have \(f(0)=0\) and

\[f^{\prime}(q_{u}) =\mathbb{E}\Bigg{[}\frac{\gamma\overline{\Sigma}^{-2}}{\left( \gamma\mathbb{E}\Big{[}\frac{\alpha\gamma q_{u}\frac{\Xi^{-2}}{\alpha\gamma q _{v}\frac{\Xi^{-2}}{\Xi^{-1}}+1}\Big{]}\overline{\Sigma}^{-1}+1}\Big{]}^{2} }\mathbb{E}\Bigg{[}\frac{\alpha\gamma\overline{\Xi}^{-2}}{\left(\alpha\gamma q _{v}\overline{\Xi}^{-1}+1\right)^{2}}\Bigg{]}>0,\] \[f^{\prime}(0) =\alpha\gamma^{2}\mathbb{E}\Big{[}\overline{\Sigma}^{-2}\Big{]} \mathbb{E}\Big{[}\overline{\Xi}^{-2}\Big{]},\] \[f^{\prime\prime}(q_{v}) =-2\left(\mathbb{E}\Bigg{[}\frac{\alpha\gamma\overline{\Xi}^{-2} }{\left(\alpha\gamma q_{v}\overline{\Xi}^{-1}+1\right)^{2}}\Bigg{]}^{2} \mathbb{E}\Bigg{[}\frac{\gamma^{2}\overline{\Sigma}^{-3}}{\left(\gamma \mathbb{E}\Big{[}\frac{\alpha\gamma q_{v}\frac{\Xi^{-2}}{\alpha\gamma q_{v} \overline{\Xi}^{-1}}+1}\Big{]}\overline{\Sigma}^{-1}+1\right)^{3}}\Bigg{]}\right.\] \[\qquad\qquad\qquad\left.+\mathbb{E}\Bigg{[}\frac{\gamma\mathbb{E}^ {-2}}{\left(\gamma\mathbb{E}\Big{[}\frac{\alpha\gamma q_{v}\frac{\Xi^{-2}}{ \alpha\gamma q_{v}\overline{\Xi}^{-1}}+1}\Big{]}\overline{\Sigma}^{-1}+1\right) ^{2}}\Bigg{]}\mathbb{E}\Bigg{[}\frac{\alpha^{2}\gamma^{2}\overline{\Xi}^{-3} }{\left(\alpha\gamma q_{v}\overline{\Xi}^{-1}+1\right)^{3}}\Bigg{]}\Bigg{)}<0,\] \[\lim_{q_{v}\to\infty}f(q_{v}) =\mathbb{E}\Bigg{[}\frac{\gamma\mathbb{E}\Big{[}\overline{\Xi}^{- 1}\Big{]}\overline{\Sigma}^{-2}}{\gamma\mathbb{E}\Big{[}\overline{\Xi}^{-1} \Big{]}\overline{\Sigma}^{-1}+1}\Bigg{]}\in(0,\infty).\]

It then becomes evident that a non-trivial fixed point \(q_{v}>0\) exists if and only if \(f^{\prime}(0)>1\) and in this case, the non-trivial fixed point is unique.

Finally, by the first equation in (4.5), there is a non-trivial fixed point \(q_{u}\) if and only if there is a non-trivial fixed point \(q_{v}\), which completes the proof.

## Appendix B Auxiliary Gaussian channel

We formally introduce here the auxiliary model mentioned in Section 4. Consider a Gaussian channel with blocklength \(n\), input \(x^{*}\), output \(Y\), anisotropic Gaussian noise \(\Sigma^{1/2}Z\) and SNR \(\gamma\):

\[Y=\sqrt{\gamma}x^{*}+\Sigma^{1/2}Z\in\mathbb{R}^{n},\] (B.1)where

\[(x^{*},Z)\sim P^{\otimes n}\otimes\mathcal{N}(0_{n},I_{n}).\]

By similar derivations as in Section 4, the posterior distribution of \(x^{*}\) given \(Y\) can be written as

\[\mathrm{d}P(x\,|\,Y)=\frac{1}{Z_{n}(\gamma)}\exp(H_{n}(x))\,\mathrm{d}P^{ \otimes n}(x),\]

where the Hamiltonian and the partition function are

\[H_{n}(x) \coloneqq\gamma{x^{*}}^{\top}\Sigma^{-1}x+\sqrt{\gamma}Z^{\top} \Sigma^{-1/2}x-\frac{\gamma}{2}x^{\top}\Sigma^{-1}x,\] \[Z_{n}(\gamma) \coloneqq\int_{\mathbb{R}^{n}}\exp(H_{n}(x))\,\mathrm{d}P^{ \otimes n}(x).\]

Define the free energy as

\[F_{n}(\gamma)\coloneqq\frac{1}{n}\mathbb{E}[\log Z_{n}(\gamma)].\]

With \(P=\mathcal{N}(0,1)\), \(Z_{n}(\gamma)\) becomes a Gaussian integral that can be computed as below using Proposition G.1:

\[Z_{n}(\gamma)=\frac{1}{\sqrt{\det(\gamma\Sigma^{-1}+I_{n})}}\exp \biggl{(}\frac{1}{2}\Bigl{(}\gamma\Sigma^{-1}x^{*}+\sqrt{\gamma}\Sigma^{-1/2} Z\Bigr{)}^{\top}\bigl{(}\gamma\Sigma^{-1}+I_{n}\bigr{)}^{-1}\Bigl{(}\gamma \Sigma^{-1}x^{*}+\sqrt{\gamma}\Sigma^{-1/2}Z\Bigr{)}\biggr{)}.\]

Therefore, by Proposition G.2,

\[\operatorname*{p-lim}_{n\to\infty}F_{n}(\gamma) =-\frac{1}{2}\mathbb{E}\Bigl{[}\log\Bigl{(}\gamma\overline{\Sigma }^{-1}+1\Bigr{)}\Bigr{]}+\frac{1}{2}\gamma^{2}\mathbb{E}\biggl{[}\overline{ \Sigma}^{-2}\Bigl{(}\gamma\overline{\Sigma}^{-1}+1\Bigr{)}^{-1}\biggr{]}+\frac {1}{2}\gamma\mathbb{E}\biggl{[}\overline{\Sigma}^{-1}\Bigl{(}\gamma\overline{ \Sigma}^{-1}+1\Bigr{)}^{-1}\biggr{]}\] \[=\frac{1}{2}\Bigl{(}\gamma\mathbb{E}\biggl{[}\overline{\Sigma}^{ -1}\Bigr{]}-\mathbb{E}\Bigl{[}\log\Bigl{(}1+\gamma\overline{\Sigma}^{-1} \Bigr{)}\Bigr{]}\Bigr{)}.\] (B.2)

The above functional is nothing but \(\psi_{\overline{\Sigma}}(\gamma)\) introduced in (4.15) which will play an important role in characterizing the free energy of the original model (4.1).

## Appendix C Proof of Theorem 4.4

Before diving into the proof, we make further notation adjustments for the ease of applying the interpolation argument. Specifically, we will henceforth assume \(\gamma=1\) by incorporating the actual value of \(\gamma\) into the prior distributions \(P,Q\),

\[\int_{\mathbb{R}}x^{2}\,\mathrm{d}P(x)=\gamma,\quad\int_{\mathbb{R}}x^{2}\, \mathrm{d}Q(x)=1.\]

This is obviously equivalent to the previous setting. So we can drop the dependence on \(\gamma\) and write \(\mathrm{MMSE}_{n},\mathcal{Z}_{n},\mathcal{F}_{n}\) for \(\mathrm{MMSE}_{n}(\gamma)\), \(\mathcal{Z}_{n}(\gamma)\), \(\mathcal{F}_{n}(\gamma)\) defined in (4.2), (4.13) and (4.14).

We will also assume that \(\Xi,\Sigma\) are diagonal. This is without loss of generality since the Gaussianity of \(P,Q,\widetilde{W}\) ensures that both the prior distributions and the noise matrix are rotationally invariant. Furthermore, we truncate \(P,Q\) so that they are supported on \([-K,K]\) for a constant \(K>0\). The approximation error in the free energy due to truncation can be made arbitrarily small if \(K\) is sufficiently large, since the free energy is pseudo-Lipschitz in the prior distribution with respect to the Wasserstein-\(2\) metric.

The proof follows an interpolation argument [52, 10, 53] with suitable modifications to take care of the noise heteroscedasticity featured by the covariances \(\Xi,\Sigma\). To start with, define the interpolating models:

\[Y_{t} \coloneqq\sqrt{\frac{1-t}{n}}{u^{*}{v^{*}}^{\top}}+\Xi^{1/2}Z \Sigma^{1/2}\in\mathbb{R}^{n\times d},\] \[Y_{t}^{u} \coloneqq\sqrt{\alpha q_{1}(t)}u^{*}+\Xi^{1/2}Z^{u}\in\mathbb{R}^ {n},\]\[Y_{t}^{v}\coloneqq\sqrt{q_{2}(t)}v^{*}+\Sigma^{1/2}Z^{v}\in\mathbb{R}^{d},\]

where \(q_{1}(t),q_{2}(t)\geq 0\) are to be determined and

\[(u^{*},v^{*},Z,Z^{u},Z^{v})\sim P^{\otimes n}\otimes Q^{\otimes d}\otimes \mathcal{N}(0_{nd},I_{nd})\otimes\mathcal{N}(0_{n},I_{n})\otimes\mathcal{N}(0 _{d},I_{d}).\] (C.1)

By definition, \(Y_{0}=Y\) is the model that we would like to understand, and \(Y_{t}^{u},Y_{t}^{v}\) are instances of Gaussian channels in (B.1) whose free energy we have already understood (see (B.2)). The idea is that \(Y_{t}\) serves as a path parametrized by \(t\in[0,1]\) from the original model \(Y\) to the target models \((Y_{1}^{u},Y_{1}^{v})\). The crux of the interpolation argument lies in showing that \(Y_{t}\) and \((Y_{t}^{u},Y_{t}^{v})\) are equivalent (at the level of free energy) along the path.

To study the interpolating models \((Y_{t},Y_{t}^{u},Y_{t}^{v})\), define the Hamiltonian

\[\begin{split}\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_{1},q_{2})&\coloneqq\sqrt{\frac{1-t}{n}}\widetilde{u}^{\top}Z \widetilde{v}+\frac{1-t}{n}\widetilde{u}^{\top}\widetilde{u}^{*}\widetilde{v}^ {\top}\widetilde{v}^{*}-\frac{1-t}{2n}\|\widetilde{u}\|_{2}^{2}\|\widetilde{v }\|_{2}^{2}\\ &\quad+\alpha q_{1}\widetilde{u}^{\top}\widetilde{u}^{*}+\sqrt{ \alpha q_{1}}\widetilde{u}^{\top}Z^{u}-\frac{\alpha q_{1}}{2}\|\widetilde{u} \|_{2}^{2}\\ &\quad+q_{2}\widetilde{v}^{\top}\widetilde{v}^{*}+\sqrt{q_{2} }\widetilde{v}^{\top}Z^{v}-\frac{q_{2}}{2}\|\widetilde{v}\|_{2}^{2}.\end{split}\] (C.2)

Then the posterior distribution of \((u^{*},v^{*})\) given \((Y_{t},Y_{t}^{u},Y_{t}^{v})\) is

\[\begin{split}\mathrm{d}P(u,v\,|\,Y_{t},Y_{t}^{u},Y_{t}^{v})= \frac{1}{\mathcal{Z}_{n,t}}\exp&\Big{(}\mathcal{H}_{n,t}(\Xi^{- 1/2}u,\Sigma^{-1/2}v;q_{1}(t),q_{2}(t))\Big{)}\,\mathrm{d}P^{\otimes n}(u)\, \mathrm{d}Q^{\otimes d}(v).\end{split}\] (C.3)

Let the partition function be

\[\mathcal{Z}_{n,t} \coloneqq\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{n}}\exp& \Big{(}\mathcal{H}_{n,t}(\Xi^{-1/2}u,\Sigma^{-1/2}v;q_{1}(t),q_{2}(t))\Big{)} \,\mathrm{d}P^{\otimes n}(u)\,\mathrm{d}Q^{\otimes d}(v)\] \[=\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{n}}\exp&( \mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_{1}(t),q_{2}(t)))\,\mathrm{d }\widetilde{P}(\widetilde{u})\,\mathrm{d}\widetilde{Q}(\widetilde{v}).\] (C.4)

Define the free energy as

\[f_{n}(t)\coloneqq\frac{1}{n}\mathbb{E}[\log\mathcal{Z}_{n,t}].\] (C.5)

The Gibbs bracket \(\left\langle\cdot\right\rangle_{n,t}\) denotes the expectation with respect to the posterior distribution in (C.3):

\[\left\langle g(\widetilde{u},\widetilde{v})\right\rangle_{n,t}\coloneqq\frac{ 1}{\mathcal{Z}_{n,t}}\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{n}}g(\widetilde{u },\widetilde{v})\exp(\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_{1}(t),q _{2}(t)))\,\mathrm{d}\widetilde{P}(\widetilde{u})\,\mathrm{d}\widetilde{Q}( \widetilde{v}),\] (C.6)

for any \(g\colon\mathbb{R}^{n}\times\mathbb{R}^{d}\to\mathbb{R}\) such that the expectation exists. That is,

\[\left\langle g(\widetilde{u},\widetilde{v})\right\rangle_{n,t}=\mathbb{E}[g( \widetilde{u}^{*},\widetilde{v}^{*})\,|\,Y_{t},Y_{t}^{u},Y_{t}^{v}],\]

where we recall the notation

\[\widetilde{u}^{*}\coloneqq\Xi^{-1/2}u^{*},\qquad\widetilde{v}^{*}\coloneqq \Sigma^{-1/2}v^{*}.\] (C.7)

We will also use the notation \(\left\langle\cdot\right\rangle_{n}\) for the Gibbs bracket with respect to the original posterior \(\mathrm{d}P(u,v\,|\,Y)\) in (4.11).

**Lemma C.1**.: _Consider \(f_{n}(t)\) defined in (C.5) with \(t\in\{0,1\}\). Assume that \(q_{1}(0),q_{2}(0)\) satisfy_

\[q_{1}(0)\geq 0,\quad q_{2}(0)\geq 0,\quad\lim_{n\to\infty}q_{1}(0)=\lim_{n\to \infty}q_{2}(0)=0.\]

_Then we have_

\[f_{n}(0) =\mathcal{F}_{n}+\mathcal{O}(q_{1}(0)+q_{2}(0)),\] (C.8) \[\lim_{n\to\infty}f_{n}(1) =\psi_{\overline{\Xi}}(\alpha q_{1}(1)\gamma)+\alpha\psi_{ \overline{\Xi}}(q_{2}(1))+o_{K},\] (C.9)

_where \(\lim_{K\to\infty}o_{K}=0\)._Proof.: To show the first statement (C.8), let us control \(f_{n}(0)-\mathcal{F}_{n}\). Denoting

\[\mathcal{H}^{\prime}_{n,t}(\widetilde{u},\widetilde{v};q_{1},q_{2})\coloneqq \alpha q_{1}\widetilde{u}^{\top}\widetilde{u}^{*}+\sqrt{\alpha q_{1}} \widetilde{u}^{\top}Z^{u}-\frac{\alpha q_{1}}{2}\|\widetilde{u}\|_{2}^{2}+q_ {2}\widetilde{v}^{\top}\widetilde{v}^{*}+\sqrt{q_{2}v}^{\top}Z^{v}-\frac{q_{2 }}{2}\|\widetilde{v}\|_{2}^{2}\]

and recalling the Gibbs bracket notation \(\left\langle\cdot\right\rangle_{n}\), we have

(C.10)

where the outer expectation is over all randomness in \(u^{*},v^{*},Z^{u},Z^{v}\). The second equality above follows since

\[\mathcal{H}_{n,0}(\widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0))=\mathcal{H}_ {n}(\widetilde{u},\widetilde{v})+\mathcal{H}^{\prime}_{n,0}(\widetilde{u}, \widetilde{v};q_{1}(0),q_{2}(0)).\]

We will derive double-sided bounds on \(f_{n}(0)-\mathcal{F}_{n}\).

To upper bound it, use Jensen's inequality \(\mathbb{E}[\log(\cdot)]\leq\log\mathbb{E}[\cdot]\) on the partial expectation over \(Z^{u},Z^{v}\) in (C.10):

\[f_{n}(0)-\mathcal{F}_{n} \leq\frac{1}{n}\mathop{\mathbb{E}}_{u^{*},v^{*}}\biggl{[}\log \mathop{\mathbb{E}}_{Z^{u},Z^{v}}\Bigl{[}\left\langle\exp\bigl{(}\mathcal{H}^ {\prime}_{n,0}(\widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0))\bigr{)}\right\rangle _{n}\biggr{]}\biggr{]}\] \[=\frac{1}{n}\mathop{\mathbb{E}}_{u^{*},v^{*}}\biggl{[}\log \biggl{\langle}\mathop{\mathbb{E}}_{Z^{u},Z^{v}}\bigl{[}\exp\bigl{(}\mathcal{H }^{\prime}_{n,0}(\widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0))\bigr{)} \bigr{]}\biggr{\rangle}_{n}\biggr{]},\]

where the equality is legit since \(\left\langle\cdot\right\rangle_{n}\) does not depend on \(Z^{u},Z^{v}\). By the Gaussian integral formula (Proposition G.1), the inner expectation equals

Replacing the Gibbs bracket with \(\max\), we obtain an upper bound:

\[f_{n}(0)-\mathcal{F}_{n} \leq\frac{1}{n}\mathop{\mathbb{E}}_{u^{*},v^{*}}\biggl{[}\log \max_{(\widetilde{u},\widetilde{v})\in\Xi^{-1/2}[-K,K]^{n}\times\Sigma^{-1/2}[ -K,K]^{d}}\exp\bigl{(}\alpha q_{1}(0)\widetilde{u}^{\top}\widetilde{u}^{*}+q_ {2}(0)\widetilde{v}^{\top}\widetilde{v}^{*}\bigr{)}\biggr{]}\] \[\leq\alpha q_{1}(0)\|\Xi\|_{2}^{-1}K^{2}+q_{2}(0)\|\Sigma\|_{2}^ {-1}K^{2}\frac{d}{n}\] \[\leq\frac{2\alpha q_{1}(0)K^{2}}{\inf\operatorname{supp}(\overline {\Xi})}+\frac{2\alpha q_{2}(0)K^{2}}{\inf\operatorname{supp}(\overline{\Xi})},\] (C.11)

where the last inequality holds for all sufficiently large \(n\) by (3.3) and \(n\asymp d\).

To lower bound \(f_{n}-\mathcal{F}_{n}\), we use Jensen's inequality again but this time on the Gibbs bracket in (C.10):

\[f_{n}(0)-\mathcal{F}_{n}\geq\frac{1}{n}\mathbb{E}\Bigl{[}\left\langle\mathcal{ H}^{\prime}_{n,0}(\widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0))\right\rangle_{n} \Bigr{]}=\frac{1}{n}\mathop{\mathbb{E}}_{u^{*},v^{*}}\biggl{[}\left\langle \mathop{\mathbb{E}}_{Z^{u},Z^{v}}\bigl{[}\mathcal{H}^{\prime}_{n,0}( \widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0))\bigr{]}\right\rangle_{n}\biggr{]}.\] (C.12)

Since \((Z^{u},Z^{v})\sim\mathcal{N}(0_{n},I_{n})\otimes\mathcal{N}(0_{d},I_{d})\), the inner expectation equals

\[\mathop{\mathbb{E}}_{Z^{u},Z^{v}}\bigl{[}\mathcal{H}^{\prime}_{n,0}( \widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0))\bigr{]}=\alpha q_{1}(0) \widetilde{u}^{\top}\widetilde{u}^{*}-\frac{\alpha q_{1}(0)}{2}\|\widetilde{u }\|_{2}^{2}+q_{2}(0)\widetilde{v}^{\top}\widetilde{v}^{*}-\frac{q_{2}(0)}{2} \|\widetilde{v}\|_{2}^{2}.\]

So

\[\max_{(\widetilde{u},\widetilde{v})\in\Xi^{-1/2}[-K,K]^{n} \times\Sigma^{-1/2}[-K,K]^{d}}\biggl{|}\mathop{\mathbb{E}}_{Z^{u},Z^{v}} \bigl{[}\mathcal{H}^{\prime}_{n,0}(\widetilde{u},\widetilde{v};q_{1}(0),q_{2} (0))\bigr{]}\biggr{|}\\ \leq\frac{3}{2}\alpha q_{1}(0)\|\Xi\|_{2}^{-1}nK^{2}+\frac{3}{2}q_ {2}(0)\|\Sigma\|_{2}^{-1}dK^{2}.\]

Using this, we obtain a lower bound on \(f_{n}-\mathcal{F}_{n}\) by replacing the Gibbs bracket on the RHS of (C.12) with \(-\max\lvert\cdot\rvert\):

\[f_{n}(0)-\mathcal{F}_{n}\geq-\frac{1}{n}\mathop{\mathbb{E}}_{u^{*},v^{*}} \biggl{[}\max_{(\widetilde{u},\widetilde{v})\in\Xi^{-1/2}[-K,K]^{n}\times \Sigma^{-1/2}[-K,K]^{d}}\biggl{|}\mathop{\mathbb{E}}_{Z^{u},Z^{v}}\bigl{[} \mathcal{H}^{\prime}_{n,0}(\widetilde{u},\widetilde{v};q_{1}(0),q_{2}(0)) \bigr{]}\biggr{|}\biggr{]}\]\[\geq-\frac{3}{2}\alpha q_{1}(0)\|\Xi\|_{2}^{-1}K^{2}-\frac{3}{2}q_{2} (0)\|\Sigma\|_{2}^{-1}K^{2}\frac{d}{n}\] \[\geq-\frac{3\alpha q_{1}(0)K^{2}}{\inf\operatorname{supp}(\overline {\Xi})}-\frac{3\alpha q_{2}(0)K^{2}}{\inf\operatorname{supp}(\overline{\Sigma} )},\] (C.13)

where the last inequality holds for all sufficiently large \(n\) by (3.3) and \(n\asymp d\). Combining (C.11) and (C.13) gives the first result (C.8).

We then prove the second statement (C.9). Since \(f_{n}\) is pseudo-Lipschitz as a function of the priors, up to a term \(o_{K}\) that vanishes as \(K\to\infty\) uniformly over \(n\), it suffices to ignore the truncation at \(K\) and assume \(P=\mathcal{N}(0,\gamma),Q=\mathcal{N}(0,1)\). From the definition (C.4) of \(\mathcal{Z}_{n,t}\), we have

\[\mathcal{Z}_{n,1}=\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{n}} \exp\Bigl{(}\mathcal{H}^{\prime}_{n,1}(\Xi^{-1/2}u,\Sigma^{-1/2}v;q_{1}(1),q _{2}(1))\Bigr{)}\,\mathrm{d}P^{\otimes n}(u)\,\mathrm{d}Q^{\otimes d}(v)\] \[=\sqrt{\frac{\det(\Xi)}{\gamma^{n}\det(\Xi/\gamma+\alpha q_{1}(1 )I_{n})}}\] \[\times\exp\biggl{(}\frac{1}{2}\Bigl{(}\alpha q_{1}(1)\widetilde{ u}^{*}+\sqrt{\alpha q_{1}(1)}Z^{u}\Bigr{)}^{\top}(\Xi/\gamma+\alpha q_{1}(1)I_{ n})^{-1}\Bigl{(}\alpha q_{1}(1)\widetilde{u}^{*}+\sqrt{\alpha q_{1}(1)}Z^{u} \Bigr{)}\biggr{)}\] \[\times\sqrt{\frac{\det(\Sigma)}{\det(\Sigma+q_{2}(1)I_{d})}}\exp \biggl{(}\frac{1}{2}\Bigl{(}q_{2}(1)\widetilde{v}^{*}+\sqrt{q_{2}(1)}Z^{v} \Bigr{)}^{\top}(\Sigma+q_{2}(1)I_{d})^{-1}\Bigl{(}q_{2}(1)\widetilde{v}^{*}+ \sqrt{q_{2}(1)}Z^{v}\Bigr{)}\biggr{)},\]

where in the second equality, we use the Gaussian integral formula (Proposition G.1). Therefore

\[\lim_{n\to\infty}f_{n}(1) =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}[\log\mathcal{Z}_{n,1}]\] \[=\frac{1}{2}\alpha q_{1}(1)\gamma\mathbb{E}\Bigl{[}\overline{\Xi} ^{-1}\Bigr{]}-\frac{1}{2}\mathbb{E}\Bigl{[}\log\Bigl{(}1+\alpha q_{1}(1) \gamma\overline{\Xi}^{-1}\Bigr{)}\Bigr{]}\] \[\quad+\frac{\alpha}{2}q_{2}(1)\mathbb{E}\Bigl{[}\overline{\Sigma} ^{-1}\Bigr{]}-\frac{\alpha}{2}\mathbb{E}\Bigl{[}\log\Bigl{(}1+q_{2}(1) \overline{\Sigma}^{-1}\Bigr{)}\Bigr{]},\]

verifying the identity (C.9). In the second equality, we have used Proposition G.2. 

**Lemma C.2** (Free energy variation).: _For all \(t\in(0,1)\),_

(C.14)

Proof.: From the definitions (C.4) and (C.5), we compute

\[f^{\prime}_{n}(t) =\frac{1}{n}\mathbb{E}\biggl{[}\frac{1}{\mathcal{Z}_{n,t}}\frac{ \partial}{\partial t}\mathcal{Z}_{n,t}\biggr{]}\] \[=\frac{1}{n}\mathbb{E}\biggl{[}\frac{1}{\mathcal{Z}_{n,t}}\int_{ \mathbb{R}^{d}}\int_{\mathbb{R}^{n}}\biggl{(}\frac{\partial}{\partial t} \mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_{1}(t),q_{2}(t))\biggr{)}\exp (\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_{1}(t),q_{2}(t)))\,\mathrm{d }\widetilde{P}(\widetilde{u})\,\mathrm{d}\widetilde{Q}(\widetilde{v})\biggr{]}\] \[=\frac{1}{n}\mathbb{E}\Biggl{[}\left\langle\frac{\partial}{ \partial t}\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_{1}(t),q_{2}(t)) \right\rangle_{n,t}\Biggr{]}.\]

By the definition (C.2), the time derivative of the Hamiltonian is

\[\frac{\partial}{\partial t}\mathcal{H}_{n,t}(\widetilde{u}, \widetilde{v};q_{1}(t),q_{2}(t)) =-\frac{1}{2\sqrt{(1-t)n}}\widetilde{u}^{\top}Z\widetilde{v}- \frac{1}{n}\widetilde{u}^{\top}\widetilde{u}^{*}\widetilde{v}^{\top}\widetilde{v }^{*}+\frac{1}{2n}\|\widetilde{u}\|_{2}^{2}\|\widetilde{v}\|_{2}^{2}\] \[\quad+\alpha q^{\prime}_{1}(t)\widetilde{u}^{\top}\widetilde{u}^{* }+\frac{\sqrt{\alpha}}{2\sqrt{q_{1}(t)}}q^{\prime}_{1}(t)\widetilde{u}^{\top} Z^{u}-\frac{\alpha}{2}q^{\prime}_{1}(t)\|\widetilde{u}\|_{2}^{2}\] (C.15) \[\quad+q^{\prime}_{2}(t)\widetilde{v}^{\top}\widetilde{v}^{*}+ \frac{1}{2\sqrt{q_{2}(t)}}q^{\prime}_{2}(t)\widetilde{v}^{\top}Z^{v}-\frac{1}{ 2}q^{\prime}_{2}(t)\|\widetilde{v}\|_{2}^{2}.\]The expectation of \(\left\langle\partial_{t}\mathcal{H}_{n,t}\right\rangle_{n,t}\) can be computed using the Stein's lemma (Proposition G.7). Indeed, let us consider the term

\[\begin{split}\mathbb{E}\Big{[}\big{\langle}\widetilde{u}^{\top}Z \widetilde{v}\big{\rangle}_{n,t}\Big{]}&=\sum_{i=1}^{n}\sum_{j=1}^ {d}\mathbb{E}\Big{[}\big{\langle}\widetilde{u}_{i}\widetilde{v}_{j}\big{\rangle} _{n,t}Z_{i,j}\Big{]}=\sum_{i=1}^{n}\sum_{j=1}^{d}\mathbb{E}\Bigg{[}\frac{ \partial\langle\widetilde{u}_{i}\widetilde{v}_{j}\rangle_{n,t}}{\partial Z_{i, j}}\Bigg{]}\\ &=\sum_{i=1}^{n}\sum_{j=1}^{d}\sqrt{\frac{1-t}{n}}\mathbb{E} \Big{[}\big{\langle}\widetilde{u}_{i}^{2}\widetilde{v}_{j}^{2}\big{\rangle}_{n,t}\Big{]}-\sum_{i=1}^{n}\sum_{j=1}^{d}\sqrt{\frac{1-t}{n}}\mathbb{E}\Big{[} \big{\langle}\widetilde{u}_{i}\widetilde{v}_{j}\big{\rangle}_{n,t}^{2}\Big{]} \\ &=\sqrt{\frac{1-t}{n}}\mathbb{E}\bigg{[}\big{\langle}\|\widetilde {u}\|_{2}^{2}\|\widetilde{v}\|_{2}^{2}\big{\rangle}_{n,t}\bigg{]}-\sqrt{\frac{ 1-t}{n}}\mathbb{E}\Big{[}\big{\langle}\widetilde{u}^{\top}\widetilde{u}^{*} \widetilde{v}^{\top}\widetilde{v}^{*}\big{\rangle}_{n,t}\Big{]},\end{split}\] (C.16)

where the last step is by the Nishimori identity (Proposition G.4). So the first line of (C.15) upon taken the Gibbs bracket and the expectation becomes

\[-\frac{1}{2n}\mathbb{E}\Big{[}\big{\langle}\widetilde{u}^{\top}\widetilde{u}^ {*}\widetilde{v}^{\top}\widetilde{v}^{*}\big{\rangle}_{n,t}\Big{]}.\]

Similar cancellations happen for the second and third lines of (C.15). Putting them together, we obtain

\[f_{n}^{\prime}(t)=-\frac{1}{2n^{2}}\mathbb{E}\Big{[}\big{\langle}\widetilde{ u}^{\top}\widetilde{u}^{*}\widetilde{v}^{\top}\widetilde{v}^{*}\big{\rangle}_{n,t}\Big{]}+\frac{\alpha}{2n}q_{1}^{\prime}(t)\mathbb{E}\Big{[}\big{\langle} \widetilde{u}^{\top}\widetilde{u}^{*}\big{\rangle}_{n,t}\Big{]}+\frac{1}{2n}q _{2}^{\prime}(t)\mathbb{E}\Big{[}\big{\langle}\widetilde{v}^{\top}\widetilde{ v}^{*}\big{\rangle}_{n,t}\Big{]},\]

which is the same as (C.14) with the parentheses opened up. 

In what follows, our strategy is:

1. Show that \(\big{\langle}\widetilde{u}^{\top}\widetilde{u}^{*}\big{\rangle}_{n,t}\) is concentrated around its mean \(\mathbb{E}\Big{[}\big{\langle}\widetilde{u}^{\top}\widetilde{u}^{*}\big{\rangle} _{n,t}\Big{]}\);
2. Choose \(q_{2}(t)\) to be the solution to \[q_{2}^{\prime}(t)=\frac{1}{n}\mathbb{E}\Big{[}\big{\langle}\widetilde{u}^{\top }\widetilde{u}^{*}\big{\rangle}_{n,t}\Big{]}.\]

Once Items 1 and 2 are done, we then have

\[\mathcal{F}_{n} \approx f_{n}(0)=f_{n}(1)-\int_{0}^{1}f_{n}^{\prime}(t)\,\mathrm{d}t\] \[\approx\psi_{\overline{\Xi}}(\alpha q_{1}(1)\gamma)+\alpha\psi_{ \overline{\Xi}}(q_{2}(1))-\int_{0}^{1}\frac{\alpha}{2}q_{1}^{\prime}(t)q_{2}^{ \prime}(t)\,\mathrm{d}t\] \[\quad+\int_{0}^{1}\frac{1}{2}\mathbb{E}\Bigg{[}\bigg{\langle} \bigg{\langle}\frac{\widetilde{u}^{\top}\widetilde{u}^{*}}{n}-q_{2}^{\prime}(t) \bigg{\rangle}\bigg{(}\frac{\widetilde{v}^{\top}\widetilde{v}^{*}}{n}-\alpha q _{1}^{\prime}(t)\bigg{)}\bigg{\rangle}_{n,t}\Bigg{]}\,\mathrm{d}t\] \[\approx\psi_{\overline{\Xi}}(\alpha q_{1}(1)\gamma)+\alpha\psi_{ \overline{\Sigma}}(q_{2}(1))-\int_{0}^{1}\frac{\alpha}{2}q_{1}^{\prime}(t)q_{2 }^{\prime}(t)\,\mathrm{d}t,\]

where the first line above uses (C.8) in Lemma C.1; the second line uses (C.9) in Lemma C.1 and Lemma C.2; the third line uses Items 1 and 2. This will almost lead to the desired characterization of the free energy \(\mathcal{F}_{n}\) in Theorem 4.4:

\[\sup_{q_{v}\geq 0}\inf_{q_{u}\geq 0}\psi_{\overline{\Xi}}(\alpha\gamma q_{v})+ \alpha\psi_{\overline{\Sigma}}(q_{u})-\frac{\alpha}{2}q_{u}q_{v}.\]

Consider the function

\[\phi_{t}(q_{1},q_{2})=\frac{1}{n}\log\mathcal{Z}_{n,t}.\]

Note that \(\phi_{t}(q_{1},q_{2})\) also depends on \(n,u^{*},v^{*},Z,Z^{u},Z^{v}\), and \(\mathbb{E}[\phi_{t}(q_{1},q_{2})]=f_{n}(t)\) where the expectation is over \((u^{*},v^{*},Z,Z^{u},Z^{v})\) distributed according to (C.1).

**Lemma C.3** (Free energy concentration).: _Fix a constant \(M>0\). There exists a constant \(C>0\) depending only on \(K,M,\alpha,\overline{\Xi},\overline{\Sigma}\) such that for any \(t\in[0,1]\), \(0\leq q_{1}(t),q_{2}(t)\leq M\) and sufficiently large \(n\),_

\[\mathbb{E}[\|\phi_{t}(q_{1},q_{2})-\mathbb{E}[\phi_{t}(q_{1},q_{2})]\|]\leq \frac{C}{\sqrt{n}}.\]

Proof.: Fix \(u^{*},v^{*}\). Consider \(\phi_{t}(q_{1},q_{2})\) as a function of \((Z,Z^{u},Z^{v})\). Then

\[\left\|\nabla_{(Z,Z^{u},Z^{v})}\phi_{t}(q_{1},q_{2})\right\|_{2}^ {2}\] \[=\|\nabla_{Z}\phi_{t}(q_{1},q_{2})\|_{2}^{2}+\left\|\nabla_{Z^{u} }\phi_{t}(q_{1},q_{2})\right\|_{2}^{2}+\left\|\nabla_{Z^{v}}\phi_{t}(q_{1},q_ {2})\right\|_{2}^{2}\] \[=\sum_{i=1}^{n}\sum_{j=1}^{d}\left(\frac{1}{n}\mathbb{E}\!\left[ \left\langle\sqrt{\frac{1-t}{n}}\widetilde{u}_{i}\widetilde{v}_{j}\right\rangle _{n,t}\right]\right)^{2}+\sum_{i=1}^{n}\!\left(\frac{1}{n}\mathbb{E}\!\left[ \left\langle\sqrt{\alpha q_{1}}\widetilde{u}_{i}\right\rangle_{n,t}\right] \right)^{2}+\sum_{j=1}^{d}\!\left(\frac{1}{n}\mathbb{E}\!\left[\left\langle \sqrt{q_{2}}\widetilde{v}_{j}\right\rangle_{n,t}\right]\right)^{2}\] \[\leq\frac{1}{n^{3}}\mathbb{E}\!\left[\left\langle\|\widetilde{u} \|_{2}^{2}\|\widetilde{v}\|_{2}^{2}\right\rangle_{n,t}\right]+\frac{\alpha q_{ 1}}{n^{2}}\mathbb{E}\!\left[\left\langle\|\widetilde{u}\|_{2}^{2}\right\rangle _{n,t}\right]+\frac{q_{2}}{n^{2}}\mathbb{E}\!\left[\left\langle\|\widetilde{v} \|_{2}^{2}\right\rangle_{n,t}\right]\] \[\leq\frac{\alpha}{n}\|\Xi\|_{2}^{-1}\|\Sigma\|_{2}^{-1}K^{4}+ \frac{\alpha}{n}M\|\Xi\|_{2}^{-1}K^{2}+\frac{\alpha}{n}M\|\Sigma\|_{2}^{-1}K ^{2}\leq\frac{C}{n},\]

where \(C>0\) is a constant depending only on \(\alpha,M,\overline{\Xi},\overline{\Sigma},K\). The penultimate line is by Cauchy-Schwarz and the last line holds for all sufficiently large \(n\) by (3.3) and \(n\asymp d\). Then by the Gaussian Poincare inequality (Proposition G.8),

\[\mathbb{E}_{Z,Z^{u},Z^{v}}\!\left[\left|\phi_{t}(q_{1},q_{2})-\mathbb{E}_{Z,Z ^{u},Z^{v}}[\phi_{t}(q_{1},q_{2})]\right|\right]\leq\sqrt{\operatorname{Var} _{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1},q_{2})]}\leq\frac{C}{\sqrt{n}}.\] (C.17)

The above result holds for any fixed \(u^{*},v^{*}\). We then verify that \(\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1},q_{2})]\) has bounded difference as a function of \(u^{*},v^{*}\). We do so by bounding the derivatives of \(\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1},q_{2})]\) with respect to \(u_{i}^{*},v_{j}^{*}\) for any \(i\in[n],j\in[d]\). We have

\[\frac{\partial}{\partial u_{i}^{*}}\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1},q _{2})]=\frac{1}{n}\,\mathbb{E}_{Z,Z^{u},Z^{v}}\!\left[\left\langle\frac{ \partial}{\partial u_{i}^{*}}\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};q_ {1},q_{2})\right\rangle_{n,t}\right]\!,\] (C.18)

and a similar expression holds for the derivative with respect to \(v_{j}^{*}\). Recall the definition of \(\mathcal{H}_{n,t}\) from (C.2). We have

\[\frac{\partial}{\partial u_{i}^{*}}\mathcal{H}_{n,t}(\Xi^{-1/2}u,\Sigma^{-1/2} v;q_{1},q_{2})=\frac{1-t}{n}v^{\top}\Sigma^{-1}v^{*}(\Xi^{-1})_{i,i}u_{i}+ \alpha q_{1}(\Xi^{-1})_{i,i}u_{i},\]

where we have used the fact that \(\Xi\) is diagonal. Therefore,

\[\left|\frac{\partial}{\partial u_{i}^{*}}\mathcal{H}_{n,t}(\Xi^{-1/2}u,\Sigma^ {-1/2}v;q_{1},q_{2})\right|\leq\frac{1}{n}\cdot dK^{2}\|\Sigma\|_{2}^{-1}\cdot \|\Xi\|_{2}^{-1}K+\alpha q_{1}\cdot\|\Xi\|_{2}^{-1}K\leq C,\]

where \(C>0\) is a constant depending only on \(\alpha,M,K,\overline{\Xi},\overline{\Sigma}\). The last inequality holds for all sufficiently large \(n\). A similar bound holds for \(\frac{\partial}{\partial v_{j}^{*}}\mathcal{H}_{n,t}(\Xi^{-1/2}u,\Sigma^{-1/2} v;q_{1},q_{2})\). This, by (C.18), implies that \(\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1},q_{2})]\) as a function of \((u^{*},v^{*})\) satisfies the bounded difference property with \(c_{i}=C/n\) (see Proposition G.9). So by Proposition G.9,

\[\mathbb{E}_{u^{*},v^{*}}\!\left[\left|\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1 },q_{2})]-\mathbb{E}_{u^{*},v^{*}}\!\left[\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q _{1},q_{2})]\right]\right|\right]\leq\sqrt{\operatorname{Var}_{u^{*},v^{*}}\! \left[\mathbb{E}_{Z,Z^{u},Z^{v}}[\phi_{t}(q_{1},q_{2})]\right]}\leq\frac{C}{ \sqrt{n}}.\] (C.19)

Finally using (C.17) and (C.19) and the triangle inequality,

\[\mathbb{E}[|\phi_{t}(q_{1},q_{2})-\mathbb{E}[\phi_{t}(q_{1},q_{2}) ]|]\] \[\leq\frac{C}{\sqrt{n}},\] concluding the proof.

Suppose \(a,b\geq 0\) are constants. With \(q_{1}=s_{n}a^{2},q_{2}=s_{n}b^{2}\), we can write \(\mathcal{H}_{n,t}\) in (C.2) as

\[\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v};s_{n}a^{2},s_{n}b^{2})=\mathcal{H }_{n,t}(\widetilde{u},\widetilde{v})+\mathcal{H}_{n,a}^{u}(\widetilde{u})+ \mathcal{H}_{n,b}^{v}(\widetilde{v}),\]

where

\[\mathcal{H}_{n,t}(\widetilde{u},\widetilde{v}) \coloneqq\sqrt{\frac{1-t}{n}}\widetilde{u}^{\top}Z\widetilde{v}+ \frac{1-t}{n}\widetilde{u}^{\top}\widetilde{u}^{\top}\widetilde{u}^{\top} \widetilde{v}^{\top}\widetilde{v}^{\ast}-\frac{1-t}{2n}\|\widetilde{u}\|_{2}^ {2}\|\widetilde{v}\widetilde{v}\|_{2}^{2},\] \[\mathcal{H}_{n,a}^{u}(\widetilde{u}) \coloneqq\alpha s_{n}a^{2}\widetilde{u}^{\top}\widetilde{u}^{ \ast}+a\sqrt{\alpha s_{n}}\widetilde{u}^{\top}Z^{u}-\frac{\alpha s_{n}a^{2}}{2 }\|\widetilde{u}\|_{2}^{2},\] \[\mathcal{H}_{n,b}^{v}(\widetilde{v}) \coloneqq s_{n}b^{2}\widetilde{v}^{\top}\widetilde{v}^{\ast}+b \sqrt{s_{n}}\widetilde{v}^{\top}Z^{v}-\frac{s_{n}b^{2}}{2}\|\widetilde{v}\|_ {2}^{2}.\]

Fix \(A\geq 2\). Define

\[\phi^{u}(a) =\frac{1}{ns_{n}}\log\int_{\mathbb{R}^{n}}\exp\bigl{(}\mathcal{ H}_{n,a}^{u}(\widetilde{u})\bigr{)}\,\mathrm{d}\widetilde{P}(\widetilde{u}), \phi^{v}(b) =\frac{1}{ns_{n}}\log\int_{\mathbb{R}^{d}}\exp\bigl{(}\mathcal{H}_ {n,b}^{v}(\widetilde{v})\bigr{)}\,\mathrm{d}\widetilde{Q}(\widetilde{v}),\] \[\xi_{n}^{u}(s_{n}) \coloneqq\sup_{1/2\leq a\leq A+1/2}\mathbb{E}[|\phi^{u}(a)- \mathbb{E}[\phi^{u}(a)]|], \xi_{n}^{v}(s_{n}) \coloneqq\sup_{1/2\leq b\leq A+1/2}\mathbb{E}[|\phi^{v}(b)- \mathbb{E}[\phi^{v}(b)]|],\] (C.20) \[\Upsilon^{u}(\widetilde{u}) \coloneqq\frac{1}{ns_{n}}\frac{\partial}{\partial a}\mathcal{H}_ {n,a}^{u}(\widetilde{u}), \Upsilon^{v}(\widetilde{v}) \coloneqq\frac{1}{ns_{n}}\frac{\partial}{\partial b}\mathcal{H}_ {n,b}^{b}(\widetilde{v}).\]

Denote by \(\left\langle g(u^{\ast},v^{\ast})\right\rangle_{n,a,b}\) the conditional expectation of \(g(u^{\ast},v^{\ast})\) given

\[Y_{t},\quad Y_{t}^{u}(a)\coloneqq a\sqrt{\alpha s_{n}}u^{\ast}+\Xi^{1/2}Z^{u},\quad Y_{t}^{v}(b)\coloneqq b\sqrt{s_{n}}v^{\ast}+\Sigma^{1/2}Z^{v},\] (C.21)

where the expectation is with respect to the distribution in (C.1).

**Corollary C.4**.: _Let \(s_{n}=n^{-1/32}\) and \(A\leq\sqrt{M/s_{n}}-1/2\) for a constant \(M>0\) independent of \(n\). Then there exists a constant \(C>0\) depending only on \(K,\alpha,\overline{\Xi}\) such that for all sufficiently large \(n\),_

\[\xi_{n}^{u}(s_{n})\leq\frac{C}{s_{n}\sqrt{n}}.\]

Proof.: Note that if \(b=0,t=1\), it holds that \(\mathcal{H}_{n,1}(\widetilde{u},\widetilde{v};s_{n}a^{2},0)=\mathcal{H}_{n,a} ^{u}(\widetilde{u})\) and \(\phi_{1}(s_{n}a^{2},0)=s_{n}\phi^{u}(a)\). The conclusion then follows immediately from Lemma C.3 since by the assumption on \(A\), \(q_{1}=s_{n}a^{2}\in[0,M]\) for any \(1/2\leq a\leq A+1/2\). 

**Lemma C.5**.: _Let \(s_{n}=n^{-1/32}\). For all \(A\geq 2\),_

\[\frac{1}{A-1}\int_{1}^{A}\mathbb{E}\biggl{[}\left\langle\left|\Upsilon^{u}( \widetilde{u})-\mathbb{E}\Bigl{[}\left\langle\Upsilon^{u}(\widetilde{u}) \right\rangle_{n,a,b}\right]\right|\Bigr{\rangle}_{n,a,b}\biggr{]}\,\mathrm{d}a \leq C\biggl{(}\frac{1}{\sqrt{ns_{n}}}+\sqrt{\xi_{n}^{u}(s_{n})}\biggr{)},\]

_where \(C>0\) only depends on \(\alpha,\overline{\Xi},K\)._

Proof.: By the triangle inequality,

\[\mathbb{E}\biggl{[}\left\langle\left|\Upsilon^{u}(\widetilde{u}) -\mathbb{E}\Bigl{[}\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b }\right]\right|\Bigr{\rangle}_{n,a,b}\biggr{]} \leq\mathbb{E}\biggl{[}\left\langle\left|\Upsilon^{u}(\widetilde {u})-\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b}\right| \Bigr{\rangle}_{n,a,b}\right]\] \[\quad+\mathbb{E}\biggl{[}\left\langle\left|\left\langle\Upsilon^ {u}(\widetilde{u})\right\rangle_{n,a,b}-\mathbb{E}\Bigl{[}\left\langle\Upsilon^ {u}(\widetilde{u})\right\rangle_{n,a,b}\right]\right|\Bigr{\rangle}_{n,a,b} \biggr{]}.\] (C.22)

We will bound the two terms on the RHS separately. We first bound

\[\frac{1}{A-1}\int_{1}^{A}\mathbb{E}\biggl{[}\left\langle\left| \Upsilon^{u}(\widetilde{u})-\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle _{n,a,b}\right|\Bigr{\rangle}_{n,a,b}\right]\mathrm{d}a\] \[\leq\left(\frac{1}{A-1}\int_{1}^{A}\mathbb{E}\Biggl{[}\left\langle \left(\Upsilon^{u}(\widetilde{u})-\left\langle\Upsilon^{u}(\widetilde{u}) \right\rangle_{n,a,b}\right)^{2}\right\rangle_{n,a,b}\Biggr{]}\,\mathrm{d}a \right)^{1/2}.\] (C.23)The first two derivatives of \(\phi^{u}\) are

\[(\phi^{u})^{\prime}(a) =\frac{1}{ns_{n}}\frac{\int_{\mathbb{R}^{n}}\exp\bigl{(}\mathcal{H}_{ n,a}^{u}(\widetilde{u})\bigr{)}\frac{\partial}{\partial a}\mathcal{H}_{n,a}^{u}( \widetilde{u})\,\mathrm{d}\widetilde{P}(\widetilde{u})}{\int_{\mathbb{R}^{n}} \exp\bigl{(}\mathcal{H}_{n,a}^{u}(\widetilde{u})\bigr{)}\,\mathrm{d} \widetilde{P}(\widetilde{u})}=\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle _{n,a,b},\] (C.24a) \[(\phi^{u})^{\prime\prime}(a)\] \[=ns_{n}\Bigl{[}\left\langle\Upsilon^{u}(\widetilde{u})^{2} \right\rangle_{n,a,b}-\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b}^{2}\Bigr{]}+\frac{\alpha}{n}\biggl{[}2\langle\widetilde{u}^{\top} \widetilde{u}^{*}\rangle_{n,a,b}-\left\langle\left\|\widetilde{u}\right\|_{2 }^{2}\right\rangle_{n,a,b}\biggr{]}.\] (C.24b)

Since there exists \(C>0\) depending only on \(\alpha,\overline{\Xi},K\) such that for all sufficiently large \(n\),

\[\left|\frac{\alpha}{n}\biggl{[}2\langle\widetilde{u}^{\top}\widetilde{u}^{*} \rangle_{n,a,b}-\left\langle\left\|\widetilde{u}\right\|_{2}^{2}\right\rangle_ {n,a,b}\biggr{]}\right|\leq\frac{4\alpha K^{2}}{\inf\mathrm{supp}(\overline{ \Xi})}\rightleftharpoons:C,\] (C.25)

the second result (C.24b) above implies that for all sufficiently large \(n\),

\[\left\langle\left(\Upsilon^{u}(\widetilde{u})-\left\langle\Upsilon^{u}( \widetilde{u})\right\rangle_{n,a,b}\right)^{2}\right\rangle_{n,a,b}\leq\frac{ 1}{ns_{n}}((\phi^{u})^{\prime\prime}(a)+C).\]

Consequently,

\[\int_{1}^{A}\mathbb{E}\Biggl{[}\left\langle\left(\Upsilon^{u}(\widetilde{u})- \left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b}\right)^{2}\right\rangle _{n,a,b}\Biggr{]}\,\mathrm{d}a\leq\frac{1}{ns_{n}}(\mathbb{E}[(\phi^{u})^{ \prime}(A)]-\mathbb{E}[(\phi^{u})^{\prime}(1)]+C(A-1)).\] (C.26)

To proceed, we compute for any \(a\),

\[\mathbb{E}[(\phi^{u})^{\prime}(a)]=\mathbb{E}\Bigl{[}\left\langle\Upsilon^{u }(\widetilde{u})\right\rangle_{n,a,b}\Bigr{]},\] (C.27)

which is by (C.24a). By definition,

\[\Upsilon^{u}(\widetilde{u})=\frac{1}{n}\Bigl{(}2\alpha a\widetilde{u}^{\top} \widetilde{u}^{*}+\sqrt{\alpha/s_{n}}\widetilde{u}^{\top}Z^{u}-\alpha a\| \widetilde{u}\|_{2}^{2}\Bigr{)},\]

whose expectation is therefore given by

\[\mathbb{E}\Bigl{[}\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b }\Bigr{]}=\frac{2\alpha a}{n}\mathbb{E}\Bigl{[}\left\langle\widetilde{u}^{\top }\widetilde{u}^{*}\right\rangle_{n,a,b}\Bigr{]}+\frac{\sqrt{\alpha}}{n\sqrt{s _{n}}}\mathbb{E}\Bigl{[}\left\langle\widetilde{u}^{\top}Z^{u}\right\rangle_{n,a,b}\Bigr{]}-\frac{\alpha a}{n}\mathbb{E}\biggl{[}\left\langle\left\| \widetilde{u}\right\|_{2}^{2}\right\rangle_{n,a,b}\biggr{]}.\] (C.28)

Using Stein's lemma (Proposition G.7), the middle term is equal to

\[\frac{\sqrt{\alpha}}{n\sqrt{s_{n}}}\mathbb{E}\Bigl{[}\left\langle \widetilde{u}^{\top}Z^{u}\right\rangle_{n,a,b}\Bigr{]} =\frac{\sqrt{\alpha}}{n\sqrt{s_{n}}}\sum_{i=1}^{n}\mathbb{E}\Bigl{[} Z_{i}^{u}\langle\widetilde{u}_{i}\rangle_{n,a,b}\Bigr{]}=\frac{\sqrt{\alpha}}{n \sqrt{s_{n}}}\sum_{i=1}^{n}\mathbb{E}\biggl{[}\frac{\partial}{\partial Z_{i}^{u }}\langle\widetilde{u}_{i}\rangle_{n,a,b}\biggr{]}\] \[=\frac{a\alpha}{n}\mathbb{E}\biggl{[}\left\langle\left\|\widetilde {u}\right\|_{2}^{2}\right\rangle_{n,a,b}\biggr{]}-\frac{a\alpha}{n}\mathbb{E} \Bigl{[}\left\langle\widetilde{u}^{\top}\widetilde{u}^{*}\right\rangle_{n,a,b} \Bigr{]}.\]

Therefore,

\[\mathbb{E}\Bigl{[}\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b }\Bigr{]}=\frac{\alpha a}{n}\mathbb{E}\Bigl{[}\left\langle\widetilde{u}^{\top }\widetilde{u}^{*}\right\rangle_{n,a,b}\Bigr{]},\] (C.29)

and

\[\Bigl{|}\mathbb{E}\Bigl{[}\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{ n,a,b}\Bigr{]}\Bigr{|}\leq aC,\] (C.30)

where \(C\) depends only on \(\alpha,\overline{\Xi},K\).

Using the last inequality, we can further upper bound the RHS of (C.26) by \(\frac{CA}{ns_{n}}\) for some \(C\) depending only on \(\alpha,\overline{\Xi},K\). Putting this back to (C.23), we get

\[\frac{1}{A-1}\int_{1}^{A}\mathbb{E}\biggl{[}\left\langle\left|\Upsilon^{u}( \widetilde{u})-\left\langle\Upsilon^{u}(\widetilde{u})\right\rangle_{n,a,b} \right|\right\rangle_{n,a,b}\biggr{]}\,\mathrm{d}a\leq\sqrt{\frac{CA}{ns_{n}}(A- 1)}\leq\sqrt{\frac{2C}{ns_{n}}},\] (C.31)since \(A\geq 2\).

Now it remains to bound the second term on the RHS of (C.22) which can be written as

\[\mathbb{E}\bigg{[}\Big{\langle}\Big{|}\langle\Upsilon^{u}(\widetilde{ u})\rangle_{n,a,b}-\mathbb{E}\Big{[}\big{(}\Upsilon^{u}(\widetilde{u})\big{)}_{n,a,b }\Big{]}\Big{|}\Big{\rangle}_{n,a,b}\bigg{]}=\mathbb{E}[|(\phi^{u})^{\prime}(a )-\mathbb{E}[(\phi^{u})^{\prime}(a)]|]\] (C.32)

using (C.24a). To further bound the RHS, consider the following two functions

\[a\mapsto\phi^{u}(a)+\frac{2\alpha K^{2}a^{2}}{\inf\operatorname{supp}( \overline{\Xi})},\quad a\mapsto\mathbb{E}[\phi^{u}(a)]+\frac{2\alpha K^{2}a^{ 2}}{\inf\operatorname{supp}(\overline{\Xi})}.\]

They are both differentiable and convex for all sufficiently large \(n\) since their second derivatives are non-negative by (C.25) and (C.24b). Applying Proposition G.10 with the above two functions, taking the expectation and using the triangle inequality, we have that for any \(1\leq a\leq A,0<a^{\prime}\leq 1/2\),

\[\mathbb{E}[|(\phi^{u})^{\prime}(a)-\mathbb{E}[(\phi^{u})^{\prime }(a)]|]\leq\mathbb{E}[(\phi^{u})^{\prime}(a+a^{\prime})]-\mathbb{E}[(\phi^{u })^{\prime}(a-a^{\prime})]+3\xi_{n}^{u}(s_{n})/a^{\prime}+\frac{8\alpha K^{2} a^{\prime}}{\inf\operatorname{supp}(\overline{\Xi})}.\] (C.33)

Then

\[\int_{1}^{A}\mathbb{E}[(\phi^{u})^{\prime}(a+a^{\prime})]- \mathbb{E}[(\phi^{u})^{\prime}(a-a^{\prime})]\,\mathrm{d}a\] \[=(\mathbb{E}[\phi^{u}(A+a^{\prime})]-\mathbb{E}[\phi^{u}(1+a^{ \prime})])-(\mathbb{E}[\phi^{u}(A-a^{\prime})]-\mathbb{E}[\phi^{u}(1-a^{ \prime})])\] \[=(\mathbb{E}[\phi^{u}(A+a^{\prime})]-\mathbb{E}[\phi^{u}(A-a^{ \prime})])-(\mathbb{E}[\phi^{u}(1+a^{\prime})]-\mathbb{E}[\phi^{u}(1-a^{ \prime})])\] \[=\int_{-a^{\prime}}^{a^{\prime}}\mathbb{E}[(\phi^{u})^{\prime}(A+ a)]\,\mathrm{d}a-\int_{-a^{\prime}}^{a^{\prime}}\mathbb{E}[(\phi^{u})^{\prime}(1+a)]\, \mathrm{d}a\] \[\leq 4a^{\prime}(A+a^{\prime})C\leq 8a^{\prime}AC,\]

where the last step is by (C.27) and (C.30), and \(C\) depends only on \(\alpha,\overline{\Xi},K\). Using this in (C.33), we obtain

\[\int_{1}^{A}\mathbb{E}[|(\phi^{u})^{\prime}(a)-\mathbb{E}[(\phi^ {u})^{\prime}(a)]|]\,\mathrm{d}a\leq CA(a^{\prime}+\xi_{n}^{u}(s_{n})/a^{ \prime}),\]

and the RHS is minimized by \(a^{\prime}=\sqrt{\xi_{n}^{u}(s_{n})}\) which lies in the interval \((0,1/2]\) for all sufficiently large \(n\) due to Corollary C.4. Using this result in (C.32) and integrating over \(a\), we have

\[\frac{1}{A-1}\int_{1}^{A}\mathbb{E}\bigg{[}\Big{\langle}\Big{|} \langle\Upsilon^{u}(\widetilde{u})\rangle_{n,a,b}-\mathbb{E}\Big{[}\langle \Upsilon^{u}(\widetilde{u})\rangle_{n,a,b}\Big{]}\Big{|}\Big{\rangle}_{n,a,b }\bigg{]}\,\mathrm{d}a\leq\frac{CA}{A-1}\cdot 2\sqrt{\xi_{n}^{u}(s_{n})}\leq 4C\sqrt{\xi_{n}^{u}(s_{n })},\] (C.34)

since \(A\geq 2\).

Finally, combining (C.22), (C.31) and (C.34) proves the lemma. 

**Lemma C.6**.: _There exists \(C>0\) depending only on \(\alpha,K,\overline{\Xi}\) such that for any \(A\geq 2\),_

\[\frac{1}{A-1}\int_{1}^{A}\frac{1}{n^{2}}\mathbb{E}\Bigg{[}\Bigg{\langle} \bigg{(}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}-\mathbb{E}\bigg{[} \Big{\langle}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}\bigg{)}^{2}\Bigg{\rangle}_{n,a,b}\Bigg{]}\,\mathrm{d}a\leq C\bigg{(} \frac{1}{\sqrt{ns}_{n}}+\sqrt{\xi_{n}^{u}(s_{n})}\bigg{)},\]

_where \(\widetilde{u}^{(1)}=\Xi^{-1/2}u^{(1)},\widetilde{u}^{(2)}=\Xi^{-1/2}u^{(2)}\) with \(u^{(1)},u^{(2)}\) being two i.i.d. copies from the conditional law of \(u^{*},v^{*}\) given (C.21)._

Proof.: Since \(\operatorname{supp}(P),\operatorname{supp}(Q),\lVert\Xi\rVert_{2}^{-1}\) are all bounded, by the triangle inequality,

\[\frac{1}{n}\Big{|}\mathbb{E}\bigg{[}\Big{\langle}\Upsilon^{u}( \widetilde{u}^{(1)})(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle} _{n,a,b}\bigg{]}-\mathbb{E}\Big{[}\langle\Upsilon^{u}(\widetilde{u})\rangle_{n,a,b}\Big{]}\mathbb{E}\bigg{[}\Big{\langle}(\widetilde{u}^{(1)})^{\top} \widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}\bigg{|}\] (C.35) \[\leq\lVert\Xi\rVert_{2}^{-1}K^{2}\mathbb{E}\bigg{[}\Big{\langle} \Big{|}\Upsilon^{u}(\widetilde{u})-\mathbb{E}\Big{[}\langle\Upsilon^{u}( \widetilde{u})\rangle_{n,a,b}\Big{]}\Big{|}\Big{\rangle}_{n,a,b}\bigg{]}.\] (C.36)On the other hand, let us compute (C.35) on the LHS of the above inequality. Recall from (C.29) in the proof of Lemma C.5 that

\[\mathbb{E}\bigg{[}\Big{\langle}\Upsilon^{u}(\widetilde{u}^{(1)})\Big{\rangle}_{n,a,b}\bigg{]}=\frac{\alpha a}{n}\mathbb{E}\bigg{[}\Big{\langle}(\widetilde{u} ^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}.\] (C.37)

Similar to (C.28), we have

\[\mathbb{E}\bigg{[}\Big{\langle}\Upsilon^{u}(\widetilde{u}^{(1)}) (\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}= \frac{2\alpha a}{n}\mathbb{E}\bigg{[}\Big{\langle}(\widetilde{u}^{(1)})^{\top }\widetilde{u}^{*}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle} _{n,a,b}\bigg{]}\\ +\frac{\sqrt{\alpha}}{n\sqrt{s_{n}}}\mathbb{E}\bigg{[}\Big{\langle} (\widetilde{u}^{(1)})^{\top}Z^{u}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{ (2)}\Big{\rangle}_{n,a,b}\bigg{]}-\frac{\alpha a}{n}\mathbb{E}\bigg{[}\bigg{ \langle}\left\|\widetilde{u}^{(1)}\right\|_{2}^{2}(\widetilde{u}^{(1)})^{\top }\widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}.\]

Using Stein's lemma and following similar derivations leading to (C.29), the second term on the RHS equals

\[\frac{\sqrt{\alpha}}{n\sqrt{s_{n}}}\mathbb{E}\bigg{[}\Big{\langle} (\widetilde{u}^{(1)})^{\top}Z^{u}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{ (2)}\Big{\rangle}_{n,a,b}\bigg{]}=\frac{a\alpha}{n}\mathbb{E}\Bigg{[}\bigg{ \langle}\left\|\widetilde{u}^{(1)}\right\|_{2}^{2}(\widetilde{u}^{(1)})^{\top }\widetilde{u}^{(2)}\bigg{\rangle}_{n,a,b}\bigg{]}\\ -\frac{2a\alpha}{n}\mathbb{E}\bigg{[}\Big{\langle}(\widetilde{u} ^{(1)})^{\top}\widetilde{u}^{(3)}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2 )}\Big{\rangle}_{n,a,b}\bigg{]}+\frac{a\alpha}{n}\mathbb{E}\Bigg{[}\bigg{\langle} \Big{(}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{)}^{2}\bigg{\rangle} _{n,a,b}\bigg{]}.\]

Therefore,

\[\mathbb{E}\bigg{[}\Big{\langle}\Upsilon^{u}(\widetilde{u}^{(1)})(\widetilde{u }^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}=\frac{\alpha a }{n}\mathbb{E}\Bigg{[}\bigg{\langle}\Big{(}(\widetilde{u}^{(1)})^{\top} \widetilde{u}^{(2)}\Big{)}^{2}\bigg{\rangle}_{n,a,b}\bigg{]}.\] (C.38)

Putting (C.37) and (C.38) together and using Nishimori identity (Proposition G.4), we have that (C.35) equals

\[\frac{\alpha a}{n^{2}}\bigg{|}\mathbb{E}\Bigg{[}\bigg{\langle} \Big{(}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{)}^{2}\bigg{\rangle} _{n,a,b}\Bigg{]}-\mathbb{E}\bigg{[}\Big{\langle}(\widetilde{u}^{(1)})^{\top} \widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}^{2}\Bigg{|}\\ =\frac{\alpha a}{n^{2}}\bigg{|}\mathbb{E}\Bigg{[}\Bigg{\langle} \bigg{(}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}-\mathbb{E}\bigg{[} \Big{\langle}(\widetilde{u}^{(1)})^{\top}\widetilde{u}^{(2)}\Big{\rangle}_{n,a,b}\bigg{]}\bigg{)}^{2}\bigg{\rangle}_{n,a,b}\Bigg{]}\Bigg{|}.\]

So by the inequality (C.36), for any \(a\geq 1\),

Integrating over \(a\in[1,A]\) and invoking Lemma C.5 concludes the proof. 

**Lemma C.7** (Overlap concentration).: _Let \(R_{1},R_{2}\colon[0,1]\times\mathbb{R}_{>0}^{2}\to\mathbb{R}_{\geq 0}\) be two continuous bounded functions such that their partial derivatives with respect to the second and third arguments are continuous and non-negative. Let \(s_{n}=n^{-1/32}\). For \((\varepsilon_{1},\varepsilon_{2})\in[1,2]^{2}\), slightly abusing notation, let \(q_{1}(\cdot,\varepsilon_{1},\varepsilon_{2}),q_{2}(\cdot,\varepsilon_{1}, \varepsilon_{2})\) be the unique solution to_

\[\begin{cases}q_{1}(0)=s_{n}\varepsilon_{1}\\ q_{2}(0)=s_{n}\varepsilon_{2}\end{cases},\qquad\begin{cases}q_{1}^{\prime}(t)=R_ {1}(t,q_{1}(t),q_{2}(t))\\ q_{2}^{\prime}(t)=R_{2}(t,q_{1}(t),q_{2}(t))\end{cases}.\] (C.39)

_Then there exists a constant \(C>0\) depending only on \(K\), \(\alpha\), \(\|R_{1}\|_{\infty}\), \(\|R_{2}\|_{\infty}\), \(\overline{\Xi}\) such that for every \(t\in[0,1]\),_

\[\int_{1}^{2}\int_{1}^{2}\frac{1}{n^{2}}\mathbb{E}\Bigg{[}\bigg{\langle}\left( \widetilde{u}^{\top}\widetilde{u}^{*}-\mathbb{E}\Big{[}\langle\widetilde{u}^{ \top}\widetilde{u}^{*}\rangle_{n,t}\Big{]}\right)^{2}\bigg{\rangle}_{n,t} \bigg{]}\,\mathrm{d}\varepsilon_{1}\,\mathrm{d}\varepsilon_{2}\leq Cn^{-1/8}.\]Proof.: The existence and uniqueness of the solution to the Cauchy problem (C.39) is a direct consequence of the Cauchy-Lipschitz theorem [37, Theorem 3.1, Chapter V]. For any \(t\in[0,1]\), the function \(Q_{t}(\varepsilon_{1},\varepsilon_{2})=(q_{1}(t,\varepsilon_{1},\varepsilon_{2}),q_{2}(t,\varepsilon_{1},\varepsilon_{2}))\) is a \(C^{1}\)-diffeomorphism. Its Jacobian determinant is given by the Liouville formula [37, Corollary 3.1, Chapter V] and can be lower bounded as

\[J(\varepsilon_{1},\varepsilon_{2}) \coloneqq\det(\nabla Q_{t}(\varepsilon_{1},\varepsilon_{2}))\] \[=s_{n}^{2}\exp\biggl{(}\int_{0}^{t}\partial_{2}R_{1}(s,Q_{s}( \varepsilon_{1},\varepsilon_{2}))\,\mathrm{d}s+\int_{0}^{t}\partial_{3}R_{2} (s,Q_{s}(\varepsilon_{1},\varepsilon_{2}))\,\mathrm{d}s\biggr{)}\geq s_{n}^{ 2},\] (C.40)

since the partial derivatives are non-negative by assumptions.

We then view the RHS below as a function of \(q_{1},q_{2}\) and denote it by

\[V(q_{1},q_{2})=\mathbb{E}\Biggl{[}\left\langle\left(\widetilde{u}^{\top} \widetilde{u}^{*}-\mathbb{E}\Bigl{[}\left\langle\widetilde{u}^{\top} \widetilde{u}^{*}\right\rangle_{n,t}\Bigr{]}\right)^{2}\right\rangle_{n,t} \Biggr{]}.\] (C.41)

Denote \(\Omega_{t}=Q_{t}([1,2]^{2})/s_{n}\) and \(M\coloneqq\max\{\left\|R_{1}\right\|_{\infty},\left\|R_{2}\right\|_{\infty}\} +2\). Since \(R_{1},R_{2}\geq 0\) by assumptions, \(q_{1},q_{2}\) are non-decreasing in \(t\) by (C.39). So for \(i\in\{1,2\}\), and any \(t\in[0,1]\) and \((\varepsilon_{1},\varepsilon_{2})\in[1,2]^{2}\),

\[q_{i}(t,\varepsilon_{1},\varepsilon_{2})/s_{n} \geq q_{i}(0,\varepsilon_{1},\varepsilon_{2})/s_{n}=\varepsilon_{i }\geq 1,\] \[q_{i}(t,\varepsilon_{1},\varepsilon_{2})/s_{n} \leq q_{i}(1,\varepsilon_{1},\varepsilon_{2})/s_{n}=s_{n}^{-1} \int_{0}^{1}q_{i}^{\prime}(t,\varepsilon_{1},\varepsilon_{2})\,\mathrm{d}t+s_ {n}^{-1}q_{i}(0,\varepsilon_{1},\varepsilon_{2})\leq s_{n}^{-1}(\left\|R_{i} \right\|_{\infty}+2).\]

We obtain the relation \(\Omega_{t}\subset[1,M/s_{n}]^{2}\) for any \(t\in[0,1]\).

Next, using the change of variable \((r_{1},r_{2})=Q_{t}(\varepsilon_{1},\varepsilon_{2})/s_{n}\), we have

\[\int_{1}^{2}\int_{1}^{2}\frac{1}{n^{2}}\mathbb{E}\Biggl{[}\left\langle \left(\widetilde{u}^{\top}\widetilde{u}^{*}-\mathbb{E}\Bigl{[}\left\langle \widetilde{u}^{\top}\widetilde{u}^{*}\right\rangle_{n,t}\Bigr{]}\right)^{2} \right\rangle_{n,t}\Biggr{]}\,\mathrm{d}\varepsilon_{1}\,\mathrm{d}\varepsilon _{2}\] \[=\frac{1}{n^{2}}\int_{1}^{2}\int_{1}^{2}V(q_{1}(t,\varepsilon_{1},\varepsilon_{2}),q_{2}(t,\varepsilon_{1},\varepsilon_{2}))\,\mathrm{d} \varepsilon_{1}\,\mathrm{d}\varepsilon_{2}\] \[=\frac{1}{n^{2}}\int_{\Omega_{t}}\frac{V(s_{n}r_{1},s_{n}r_{2})s_ {n}^{2}}{J(Q_{t}^{-1}(s_{n}r_{1},s_{n}r_{2}))}\,\mathrm{d}(r_{1},r_{2})\] \[\leq\frac{1}{n^{2}}\int_{1}^{M/s_{n}}\int_{1}^{M/s_{n}}V(s_{n}r_{1 },s_{n}r_{2})\,\mathrm{d}r_{1}\,\mathrm{d}r_{2},\] (C.42)

where the last step is by (C.40). Further applying the change of variable \(r_{1}=a^{2}\), we have that for all \(r_{2}\geq 1\),

\[\frac{1}{n^{2}}\int_{1}^{M/s_{n}}V(s_{n}r_{1},s_{n}r_{2})\,\mathrm{d}r_{1}= \frac{1}{n^{2}}\int_{1}^{\sqrt{M/s_{n}}}V(s_{n}a^{2},s_{n}r_{2})2a\,\mathrm{d}a \leq\frac{2\sqrt{M/s_{n}}}{n^{2}}\int_{1}^{\sqrt{M/s_{n}}}V(s_{n}a^{2},s_{n}r_ {2})\,\mathrm{d}a.\] (C.43)

Recalling \(V\) from (C.41), we recognize that

\[V(s_{n}a^{2},s_{n}r_{2}) =\mathbb{E}\Biggl{[}\left\langle\left(\widetilde{u}^{\top} \widetilde{u}^{*}-\mathbb{E}\Bigl{[}\left\langle\widetilde{u}^{\top} \widetilde{u}^{*}\right\rangle_{n,a,\sqrt{r_{2}}}\Bigr{]}\right)^{2}\right\rangle _{n,a,\sqrt{r_{2}}}\Biggr{]}\] \[=\mathbb{E}\Biggl{[}\left\langle\left(\widetilde{u}^{(1)})^{\top }\widetilde{u}^{(2)}-\mathbb{E}\biggl{[}\left\langle(\widetilde{u}^{(1)})^{ \top}\widetilde{u}^{(2)}\right\rangle_{n,a,\sqrt{r_{2}}}\biggr{]}\right\rangle ^{2}\right\rangle_{n,a,\sqrt{r_{2}}}\Biggr{]},\]

where \(\left\langle\cdot\right\rangle_{n,a,b}\) is defined in (C.21) and the second equality is by Nishimori identity (Proposition G.4).

Since \(\sqrt{M/s_{n}}\geq 2\) for all sufficiently large \(n\), applying Lemma C.6, we get

\[\frac{1}{\sqrt{M/s_{n}}-1}\int_{1}^{\sqrt{M/s_{n}}}\frac{1}{n^{2}}V(s_{n}a^{2}, s_{n}r_{2})\,\mathrm{d}a\leq C\biggl{(}\frac{1}{\sqrt{ns_{n}}}+\sqrt{\xi_{n}^{u}(s_{n})} \biggr{)},\]where \(C>0\) depends only on \(\alpha,K,\overline{\Xi}\) and \(\xi_{n}^{n}(s_{n})\) is given in (C.20) with \(A=\sqrt{M/s_{n}}\). Using this back in (C.43) and then in (C.42), we obtain

\[\int_{1}^{2}\int_{1}^{2}\frac{1}{n^{2}}\mathbb{E}\Bigg{[}\bigg{\langle}\left( \widetilde{u}^{\top}\widetilde{u}^{*}-\mathbb{E}\Big{[}\langle\widetilde{u}^{ \top}\widetilde{u}^{*}\rangle_{n,t}\Big{]}\right)^{2}\bigg{\rangle}_{n,t}\Bigg{]} \,\mathrm{d}\varepsilon_{1}\,\mathrm{d}\varepsilon_{2}\leq 2C(M/s_{n})^{2}(1/ \sqrt{ns_{n}}+\sqrt{\xi_{n}^{u}(s_{n})}).\]

Corollary C.4 guarantees that \(\xi_{n}^{u}(s_{n})\leq\frac{C}{s_{n}\sqrt{n}}\) for some \(C>0\) depending only on \(\alpha,K,\overline{\Xi}\). By the choice of \(s_{n}\), we can finally upper bound (up to a positive constant depending only on \(\alpha,K,\overline{\Xi},M\)) the RHS above by

\[\frac{1}{s_{n}^{2}}\Bigg{(}\frac{1}{\sqrt{ns_{n}}}+\frac{1}{\sqrt{s_{n}\sqrt{n }}}\Bigg{)}\leq\frac{1}{s_{n}^{2.5}}\cdot\frac{2}{n^{1/4}}=2\cdot n^{-11/64} \leq 2\cdot n^{-1/8},\]

for all sufficiently large \(n\), which completes the proof. 

Recalling \(\langle\cdot\rangle_{n,t}\) defined in (C.6), let us identify \(\mathbb{E}\Big{[}\langle\widetilde{u}^{\top}\widetilde{u}^{*}\rangle_{n,t} \Big{]}\) as a function of \((t,q_{1},q_{2})\):

\[\frac{1}{n}\mathbb{E}\Big{[}\langle\widetilde{u}^{\top}\widetilde{u}^{*} \rangle_{n,t}\Big{]}=\Delta(t,q_{1},q_{2}).\] (C.44)

Note that \(\Delta\) is continuous, non-negative (by Nishimori identity) on \([0,1]\times\mathbb{R}_{\geq 0}^{2}\) and bounded by \(K^{2}\). Its partial derivatives with respect to the second and third arguments are continuous and non-negative, since the correlation between \(\widetilde{u}^{*}\) and \(\langle\widetilde{u}\rangle_{n,t}\) is a non-decreasing function of the SNRs \(q_{1},q_{2}\).

**Lemma C.8** (Fundamental sum rule).: _In the setting of Lemma C.7, for \((\varepsilon_{1},\varepsilon_{2})\in[1,2]^{2}\), let \(q_{1}(t,\varepsilon_{1},\varepsilon_{2}),q_{2}(t,\varepsilon_{1},\varepsilon_ {2})\) be the solution to (C.39) with \(R_{2}=\Delta\) defined in (C.44). Then we have_

\[\mathcal{F}_{n}=\int_{1}^{2}\int_{1}^{2}\int_{0}^{1}\psi_{\overline{\Xi}}( \alpha\gamma q_{1}(1,\varepsilon_{1},\varepsilon_{2}))+\alpha\psi_{\overline {\Sigma}}(q_{2}(1,\varepsilon_{1},\varepsilon_{2}))-\frac{\alpha}{2}q_{1}^{ \prime}(t,\varepsilon_{1},\varepsilon_{2})q_{2}^{\prime}(t,\varepsilon_{1}, \varepsilon_{2})\,\mathrm{d}t\,\mathrm{d}\varepsilon_{1}\,\mathrm{d} \varepsilon_{2}+o(1).\]

Proof.: Fix \((\varepsilon_{1},\varepsilon_{2})\in[1,2]^{2}\). By the choice \(R_{2}=\Delta\),

\[q_{2}^{\prime}(t,\varepsilon_{1},\varepsilon_{2})=\Delta(t,q_{1}(t, \varepsilon_{1},\varepsilon_{2}),q_{2}(t,\varepsilon_{1},\varepsilon_{2}))= \frac{1}{n}\mathbb{E}\Big{[}\langle\widetilde{u}^{\top}\widetilde{u}^{*} \rangle_{n,t}\Big{]}.\]

Plugging this into (C.14), integrating over \((\varepsilon_{1},\varepsilon_{2})\in[1,2]^{2}\) and applying Lemma C.7, we have

\[f_{n}^{\prime}(t)=\frac{\alpha}{2}q_{1}^{\prime}(t)q_{2}^{\prime}(t)+o(1),\]

where \(o(1)\to 0\) as \(n\to\infty\), uniformly over \(t\). By Lemma C.1, we conclude

\[\mathcal{F}_{n} =\int_{1}^{2}\int_{1}^{2}f_{n}(0)\,\mathrm{d}\varepsilon_{1}\, \mathrm{d}\varepsilon_{2}+o(1)=\int_{1}^{2}\int_{1}^{2}\biggl{(}f_{n}(1)-\int_{ 0}^{1}f_{n}^{\prime}(t)\,\mathrm{d}t\biggr{)}\,\mathrm{d}\varepsilon_{1}\, \mathrm{d}\varepsilon_{2}+o(1)\] \[=\int_{1}^{2}\int_{1}^{2}\int_{0}^{1}\psi_{\overline{\Xi}}( \alpha\gamma q_{1}(1,\varepsilon_{1},\varepsilon_{2}))+\alpha\psi_{\overline {\Sigma}}(q_{2}(1,\varepsilon_{1},\varepsilon_{2}))-\frac{\alpha}{2}q_{1}^{ \prime}(t,\varepsilon_{1},\varepsilon_{2})q_{2}^{\prime}(t,\varepsilon_{1}, \varepsilon_{2})\,\mathrm{d}t\,\mathrm{d}\varepsilon_{1}\,\mathrm{d} \varepsilon_{2}+o(1),\]

as desired. Here the first and last equalities are by (C.8) and (C.9), respectively. 

Finally, we prove a pair of matching upper and lower bounds, completing the proof of Theorem 4.4.

**Lemma C.9** (Lower bound).: _Let \(s_{n}=n^{-1/32}\) and \(R_{2}=\Delta\). Then_

\[\liminf_{n\to\infty}\mathcal{F}_{n}\geq\sup_{q_{v}\geq 0}\inf_{q_{u}\geq 0} \mathcal{F}(q_{u},q_{v}).\]

Proof.: Fix an arbitrary \(q_{v}\geq 0\). Let \(R_{1}=q_{v}\). Then \(q_{1}(t,\varepsilon_{1},\varepsilon_{2})=s_{n}\varepsilon_{1}+tq_{v}\) and Lemma C.8 gives

\[\mathcal{F}_{n} =\int_{1}^{2}\int_{1}^{2}\int_{0}^{1}\psi_{\overline{\Xi}}(\alpha \gamma(s_{n}\varepsilon_{1}+q_{v}))+\alpha\psi_{\overline{\Sigma}}(q_{2}(1, \varepsilon_{1},\varepsilon_{2}))-\frac{\alpha}{2}q_{v}q_{2}^{\prime}(t, \varepsilon_{1},\varepsilon_{2})\,\mathrm{d}t\,\mathrm{d}\varepsilon_{1}\, \mathrm{d}\varepsilon_{2}+o(1)\] \[=\int_{1}^{2}\int_{1}^{2}\psi_{\overline{\Xi}}(\alpha\gamma q_{v})+ \alpha\psi_{\overline{\Sigma}}(q_{2}(1,\varepsilon_{1},\varepsilon_{2}))-\frac{ \alpha}{2}q_{v}q_{2}(1,\varepsilon_{1},\varepsilon_{2})\,\mathrm{d}\varepsilon_{1} \,\mathrm{d}\varepsilon_{2}+o(1)\] \[\geq\inf_{q_{2}\geq 0}\psi_{\overline{\Xi}}(\alpha\gamma q_{v})+ \alpha\psi_{\overline{\Sigma}}(q_{2})-\frac{\alpha}{2}q_{v}q_{2}+o(1)=\inf_{q_{u} \geq 0}\mathcal{F}(q_{u},q_{v})+o(1),\]

where the second line holds since \(\psi_{\overline{\Xi}}\) is Lipschitz and \(q_{2}(0,\varepsilon_{1},\varepsilon_{2})=s_{n}\varepsilon_{2}=o(1)\). This completes the proof since the above lower bound holds for all \(q_{v}\geq 0\)

**Lemma C.10** (Upper bound).: _Let \(s_{n}=n^{-1/32}\) and \(R_{2}=\Delta\). Then_

\[\limsup_{n\to\infty}\mathcal{F}_{n}\leq\sup_{q_{v}\geq 0}\inf_{q_{u}\geq 0 }\mathcal{F}(q_{u},q_{v}).\]

Proof.: We apply Lemma C.8 with

\[R_{1}(t,q_{1},q_{2})=2\alpha\psi_{\overline{\Sigma}}^{\prime}( \Delta(t,q_{1},q_{2})).\] (C.45)

Since \(\psi_{\overline{\Xi}}\) is Lipschitz and convex,

\[\psi_{\overline{\Xi}}(\alpha\gamma q_{1}(1,\varepsilon_{1}, \varepsilon_{2})) =\psi_{\overline{\Xi}}(\alpha\gamma(q_{1}(1,\varepsilon_{1}, \varepsilon_{2})-q_{1}(0,\varepsilon_{1},\varepsilon_{2})))+o(1)=\psi_{ \overline{\Xi}}\bigg{(}\alpha\gamma\int_{0}^{1}q_{1}^{\prime}(t,\varepsilon _{1},\varepsilon_{2})\,\mathrm{d}t\bigg{)}+o(1)\] \[\leq\int_{0}^{1}\psi_{\overline{\Xi}}(\alpha\gamma q_{1}^{\prime }(t,\varepsilon_{1},\varepsilon_{2}))\,\mathrm{d}t+o(1),\]

and similarly

\[\psi_{\overline{\Sigma}}(q_{2}(1,\varepsilon_{1},\varepsilon_{2}))\leq\int_ {0}^{1}\psi_{\overline{\Sigma}}(q_{2}^{\prime}(t,\varepsilon_{1},\varepsilon _{2}))\,\mathrm{d}t+o(1).\]

Now Lemma C.8 implies

\[\mathcal{F}_{n} \leq\int_{1}^{2}\int_{1}^{2}\int_{0}^{1}\psi_{\overline{\Xi}}( \alpha\gamma q_{1}^{\prime}(t,\varepsilon_{1},\varepsilon_{2}))+\alpha\psi_{ \overline{\Sigma}}(q_{2}^{\prime}(t,\varepsilon_{1},\varepsilon_{2}))-\frac{ \alpha}{2}q_{1}^{\prime}(t,\varepsilon_{1},\varepsilon_{2})q_{2}^{\prime}(t, \varepsilon_{1},\varepsilon_{2})\,\mathrm{d}t\,\mathrm{d}\varepsilon_{1}\, \mathrm{d}\varepsilon_{2}+o(1)\] \[=\int_{1}^{2}\int_{1}^{2}\int_{0}^{1}\mathcal{G}(q_{2}^{\prime}(t, \varepsilon_{1},\varepsilon_{2}),q_{1}^{\prime}(t,\varepsilon_{1},\varepsilon _{2}))\,\mathrm{d}t\,\mathrm{d}\varepsilon_{1}\,\mathrm{d}\varepsilon_{2}+o(1),\]

where

\[\mathcal{G}(q_{u},q_{v})\coloneqq\psi_{\overline{\Xi}}(\alpha\gamma q_{v})+ \alpha\psi_{\overline{\Sigma}}(q_{u})-\frac{\alpha}{2}q_{u}q_{v}.\]

With the choice of \(R_{1}\) in (C.45) and \(R_{2}=\Delta\), the ODE in Lemma C.7 gives

\[q_{1}^{\prime}(t,\varepsilon_{1},\varepsilon_{2})=2\alpha\psi_{\overline{ \Sigma}}^{\prime}(q_{2}^{\prime}(t,\varepsilon_{1},\varepsilon_{2})),\]

which corresponds to the criticality condition of \(\mathcal{G}\) with respect to \(q_{u}\):

\[\partial_{1}\mathcal{G}(q_{2}^{\prime}(t,\varepsilon_{1},\varepsilon_{2}),q_{ 1}^{\prime}(t,\varepsilon_{1},\varepsilon_{2}))=0.\]

Since \(\psi_{\overline{\Sigma}}\) is convex and \(-\frac{\alpha}{2}q_{u}q_{v}\) is linear in \(q_{2}\), we have that \(\mathcal{G}\) is convex in \(q_{u}\). Therefore

\[\mathcal{G}(q_{2}^{\prime}(t,\varepsilon_{1},\varepsilon_{2}),q_{1}^{\prime}(t,\varepsilon_{1},\varepsilon_{2}))=\inf_{q_{u}\geq 0}\mathcal{G}(q_{u},q_{1}^{ \prime}(t,\varepsilon_{1},\varepsilon_{2}))=\inf_{q_{u}\geq 0}\mathcal{F}(q_{u},q_{1}^ {\prime}(t,\varepsilon_{1},\varepsilon_{2}))\leq\sup_{q_{v}\geq 0}\inf_{q_{u} \geq 0}\mathcal{F}(q_{u},q_{v}),\]

which completes the proof. 

## Appendix D Proofs of Theorem 4.2 and Corollary 4.3

### Proof of (4.7)

We compute the derivative of \(\mathcal{F}_{n}(\gamma)\):

\[\mathcal{F}_{n}^{\prime}(\gamma) =\frac{1}{n}\mathbb{E}\bigg{[}\frac{\mathcal{Z}_{n}^{\prime}( \gamma)}{\mathcal{Z}_{n}(\gamma)}\bigg{]}=\frac{1}{n}\mathbb{E}\bigg{[} \bigg{\langle}\frac{1}{2\sqrt{\gamma n}}\widetilde{u}^{\top}\,Z\widetilde{v}+ \frac{1}{n}\widetilde{u}^{\top}\widetilde{u}^{\ast}\widetilde{v}^{\top} \widetilde{v}^{\ast}-\frac{1}{2n}\widetilde{u}^{\top}\widetilde{u}\widetilde{v }^{\top}\widetilde{v}\bigg{\rangle}_{n}\bigg{]}\] \[=\frac{1}{2n^{2}}\mathbb{E}\big{[}\big{\langle}\widetilde{u}^{ \top}\widetilde{u}^{\ast}\widetilde{v}^{\top}\widetilde{v}^{\ast}\big{\rangle}_ {n}\big{]},\] (D.1)

where the last step follows similar calculations in (C.16). Since \(\mathcal{F}_{n}(\gamma)\to\sup_{q_{v}}\inf_{q_{u}}\mathcal{F}(q_{u},q_{v})\) as \(n\to\infty\), we have \(\mathcal{F}_{n}^{\prime}(\gamma)\to\frac{\partial}{\partial\gamma}\sup_{q_{v}} \inf_{q_{u}}\mathcal{F}(q_{u},q_{v})\). To compute the RHS, note that

\[\sup_{q_{v}\geq 0}\inf_{q_{u}\geq 0}\mathcal{F}(q_{u},q_{v})=\sup_{q_{v} \geq 0}\bigg{\{}\psi_{\overline{\Xi}}(\alpha\gamma q_{v})+\inf_{q_{u}\geq 0}\Big{\{} \alpha\psi_{\overline{\Sigma}}(\gamma q_{u})-\frac{\alpha}{2}\gamma q_{u}q_{v} \Big{\}}\bigg{\}},\]and the value of the infimum does not depend on \(\gamma\). Therefore, we have

\[\frac{\partial}{\partial\gamma}\sup_{q_{v}\geq 0}\inf_{q_{u}\geq 0}\mathcal{F}(q_{u},q _{v})=\psi^{\prime}_{\overline{\Xi}}(\alpha\gamma q^{*}_{v})\alpha q^{*}_{v}= \frac{\alpha q^{*}_{u}q^{*}_{v}}{2},\] (D.2)

where first equality is by the envelope theorem from [51, Corollary 4] and the last equality follows since the extremizers \(q^{*}_{u},q^{*}_{v}\) solve a pair of equations in (4.16).

On the other hand, we relate \(\mathcal{F}^{\prime}_{n}(\gamma)\) to the MMSE (4.2) as follows:

\[\mathrm{MMSE}_{n}(\gamma) =\frac{1}{nd}\mathbb{E}\Big{[}\big{\|}\widetilde{u}^{*}( \widetilde{v}^{*})^{\top}-\big{\langle}\widetilde{u}\widetilde{v}^{\top} \big{\rangle}_{n}\big{\|}_{\mathrm{F}}^{2}\Big{]}=\frac{1}{nd}\mathbb{E}\Big{[} \big{\|}\widetilde{u}^{*}\big{\|}_{2}^{2}\big{\|}\widetilde{v}^{*}\big{\|}_{ 2}^{2}+\big{\|}\big{\langle}\widetilde{u}\widetilde{v}^{\top}\big{\rangle}_ {n}\big{\|}_{\mathrm{F}}^{2}-2(\widetilde{u}^{*})^{\top}\big{\langle} \widetilde{u}\widetilde{v}^{\top}\big{\rangle}_{n}\widetilde{v}^{*}\Big{]}\] \[=\frac{\mathrm{Tr}(\Xi^{-1})}{n}\frac{\mathrm{Tr}(\Sigma^{-1})}{ d}-\frac{1}{nd}\mathbb{E}\big{[}\big{\langle}\widetilde{u}^{\top} \widetilde{u}^{*}\widetilde{v}^{\top}\widetilde{v}^{*}\big{\rangle}_{n}\big{]},\] (D.3)

where the last step is by Nishimori identity (Proposition G.4). Combining the above with (D.1) and (D.2), we conclude

\[\mathrm{MMSE}_{n}(\gamma)\to\mathbb{E}\Big{[}\overline{\Xi}^{-1}\Big{]} \mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]}-q^{*}_{u}q^{*}_{v},\]

as claimed.

### Proof of (4.8)

Recall \(Y\) from (4.1) and define for some \(\gamma^{\prime}\geq 0\),

\[Y^{\prime}\coloneqq\sqrt{\frac{\gamma^{\prime}}{n}}{u^{*}}{u^{*}}^{\top}+\Xi^ {1/2}Z^{\prime}\Xi^{1/2},\] (D.4)

where \(Z^{\prime}\in\mathbb{R}^{n\times n}\) is a symmetric random matrix independent of \(u^{*},v^{*}\) with \(Z^{\prime}_{i,i}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N}(0,2)\) and \(Z^{\prime}_{i,j}\stackrel{{\mathrm{i.i.d.}}}{{\sim}}\mathcal{N}(0,1)\) for all \(1\leq i<j\leq n\). By similar derivations as before, the free energy associated with \((Y,Y^{\prime})\) is given by

\[\mathsf{F}_{n}(\gamma,\gamma^{\prime})=\frac{1}{n}\mathbb{E} \Bigg{[}\log\int_{\mathbb{R}^{d}}\int_{\mathbb{R}^{n}}\exp\bigg{(}\mathcal{H} _{n}(\Xi^{-1/2}u,\Sigma^{-1/2}v)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+\frac{1}{2} \sqrt{\frac{\gamma^{\prime}}{n}}{u^{\top}}\Xi^{-1}Y^{\prime}\Xi^{-1}u-\frac{ \gamma^{\prime}}{4n}({u^{\top}}\Xi^{-1}u)^{2}\bigg{)}\,\mathrm{d}P^{\otimes n }(u)\,\mathrm{d}Q^{\otimes n}(v)\Bigg{]},\]

where \(\mathcal{H}_{n}\) is given in (4.12). Denote by \(\langle\!\langle\cdot\rangle\!\rangle_{n}\) the Gibbs bracket with respect to the corresponding Hamiltonian. Let

\[\mathsf{F}(\gamma,\gamma^{\prime})\coloneqq\sup_{q_{u},q_{v}\geq 0}\frac{ \gamma^{\prime}}{4}q_{u}^{2}+\frac{\alpha\gamma}{2}q_{u}q_{v}-\psi^{*}_{ \overline{\Xi}}\Big{(}\frac{q_{u}}{2}\Big{)}-\alpha\psi^{*}_{\overline{\Xi}} \Big{(}\frac{q_{v}}{2}\Big{)},\] (D.5)

where \(f^{*}\) denotes the monotone conjugate of a convex non-decreasing function \(f\colon\mathbb{R}_{\geq 0}\to\mathbb{R}\); see Definition G.1. Basic properties of monotone conjugate can be found in [67, SS12].

The following lemma, proved in Appendix D.3, characterizes the high-dimensional limit of \(\mathsf{F}_{n}(\gamma,\gamma^{\prime})\).

**Lemma D.1**.: _For all \(\gamma,\gamma^{\prime}\geq 0\),_

\[\lim_{n\to\infty}\mathsf{F}_{n}(\gamma,\gamma^{\prime})=\mathsf{F}(\gamma, \gamma^{\prime}),\]

Let us show how (4.8) can be derived from Lemma D.1.

Proof of (4.8).: Let

\[\mathrm{MMSE}^{u}_{n}(\gamma,\gamma^{\prime})\coloneqq\frac{1}{n^{2}} \mathbb{E}\Big{[}\big{\|}\widetilde{u}^{*}(\widetilde{u}^{*})^{\top}-\mathbb{E} \big{[}\widetilde{u}^{*}(\widetilde{u}^{*})^{\top}\big{|}\,Y,Y^{\prime}\big{]} \big{\|}_{\mathrm{F}}^{2}\Big{]}.\]

Following similar derivations as in Appendix D.1, one can verify the following two identities:

\[\partial_{2}\mathsf{F}_{n}(\gamma,\gamma^{\prime})=\frac{1}{4n^{2}}\mathbb{E} \Big{[}\big{\langle}\big{(}\widetilde{u}^{\top}\widetilde{u}^{*}\big{)}^{2} \big{\rangle}\!\rangle_{n}\Big{]},\quad\mathrm{MMSE}^{u}_{n}(\gamma,\gamma^{ \prime})=\frac{\mathrm{Tr}(\Xi^{-1})^{2}}{n^{2}}-\frac{1}{n^{2}}\mathbb{E} \Big{[}\big{\langle}\big{(}\widetilde{u}^{\top}\widetilde{u}^{*}\big{)}^{2} \big{\rangle}\!\rangle_{n}\Big{]}.\] (D.6)Therefore, the MMSE in (4.3) can be written as

\[\mathrm{MMSE}^{u}_{n}(\gamma)=\lim_{\gamma^{\prime}\downarrow 0}\mathrm{ MMSE}^{u}_{n}(\gamma,\gamma^{\prime})=\frac{\mathrm{Tr}(\Xi^{-1})^{2}}{n^{2}}-4 \lim_{\gamma^{\prime}\downarrow 0}\partial_{2}\mathsf{F}_{n}(\gamma,\gamma^{ \prime}).\]

By Lemma D.1 and Proposition G.11,

\[\limsup_{n\to\infty}\lim_{\gamma^{\prime}\downarrow 0}\partial_{2}\mathsf{F}_{n}( \gamma,\gamma^{\prime})\leq\lim_{\gamma^{\prime}\downarrow 0}\partial_{2} \mathsf{F}(\gamma,\gamma^{\prime}).\]

The envelope theorem from [51, Corollary 4] allows us to compute the RHS:

\[\lim_{\gamma^{\prime}\downarrow 0}\partial_{2}\mathsf{F}(\gamma,\gamma^{ \prime})=\frac{{q_{u}^{*}}^{2}}{4},\]

where \((q_{u}^{*},q_{v}^{*})\) are the maximizer of

\[\begin{split}\sup_{q_{u},q_{v}\geq 0}\frac{\alpha}{2}\gamma q_{u} q_{v}-\psi^{*}_{\Xi}\Big{(}\frac{q_{u}}{2}\Big{)}-\alpha\psi^{*}_{\Xi} \Big{(}\frac{q_{v}}{2}\Big{)}&=\sup_{(q_{u},q_{v})\in\mathcal{C }(\gamma,\alpha)}\psi_{\Xi}(\alpha\gamma q_{v})+\alpha\psi_{\Xi}(\gamma q_{u}) -\frac{\alpha\gamma}{2}q_{u}q_{v},\end{split}\]

where the equality is by Proposition G.12. Putting the above together, we have

\[\begin{split}\limsup_{n\to\infty}\frac{1}{n^{2}}\mathbb{E}\Big{[} \Big{\langle}\left(\widetilde{u}^{\top}\widetilde{u}^{*}\right)^{2}\Big{\rangle} _{n}\Big{]}&=\limsup_{n\to\infty}\lim_{\gamma^{\prime}\downarrow 0} \frac{1}{n^{2}}\mathbb{E}\Big{[}\Big{\langle}\left(\widetilde{u}^{\top} \widetilde{u}^{*}\right)^{2}\Big{\rangle}_{n}\Big{]}\leq{q_{u}^{*}}^{2},\\ \liminf_{n\to\infty}\mathrm{MMSE}^{u}_{n}(\gamma)& \geq\mathbb{E}\Big{[}\Xi^{-1}\Big{]}^{2}-{q_{u}^{*}}^{2}.\end{split}\] (D.7)

By a symmetric argument, we also have

\[\limsup_{d\to\infty}\lim_{\gamma^{\prime}\downarrow 0}\frac{1}{d^{2}} \mathbb{E}\Big{[}\Big{\langle}\left(\widetilde{v}^{\top}\widetilde{v}^{*} \right)^{2}\Big{\rangle}_{n}\Big{]}\leq{q_{v}^{*}}^{2}.\] (D.8)

To find an upper bound on \(\mathrm{MMSE}^{u}_{n}(\gamma)\), note that by (4.7) and (D.3):

This combined with (D.7) and (D.8) implies

\[\lim_{n\to\infty}\frac{1}{n^{2}}\mathbb{E}\Big{[}\Big{\langle}\left(\widetilde {u}^{\top}\widetilde{u}^{*}\right)^{2}\Big{\rangle}_{n}\Big{]}={q_{u}^{*}}^{2 },\quad\lim_{d\to\infty}\frac{1}{d^{2}}\mathbb{E}\Big{[}\Big{\langle}\left( \widetilde{v}^{\top}\widetilde{v}^{*}\right)^{2}\Big{\rangle}_{n}\Big{]}={q_ {v}^{*}}^{2},\]

which concludes the proof in view of the relation

\[\mathrm{MMSE}^{u}_{n}(\gamma)=\frac{\mathrm{Tr}(\Xi^{-1})^{2}}{n^{2}}-\frac{1 }{n^{2}}\mathbb{E}\Big{[}\Big{\langle}\left(\widetilde{u}^{\top}\widetilde{u} ^{*}\right)^{2}\Big{\rangle}_{n}\Big{]}.\qed\]

### Proof of Lemma d.1

We assume \(\gamma=\gamma^{\prime}=1\) by formally absorbing them into \(P,Q\):

\[\int_{\mathbb{R}}x^{2}\,\mathrm{d}P(x)=\sqrt{\gamma^{\prime}},\quad\int_{ \mathbb{R}}x^{2}\,\mathrm{d}Q(x)=\frac{\gamma}{\sqrt{\gamma^{\prime}}},\]

so that we can drop the dependence on \(\gamma,\gamma^{\prime}\) in notation such as \(\mathsf{F}_{n},\mathsf{F}\). We then truncate \(P,Q\) at a sufficiently large constant \(K>0\) so that they have bounded supports.

Recall \(Y\) from (4.1) and define for \(r\geq 0\), \(Y^{\prime\prime}\coloneqq\sqrt{r}u^{*}+\Xi^{1/2}Z^{\prime\prime}\) where \(Z^{\prime\prime}\sim\mathcal{N}(0_{n},I_{n})\) is independent of everything else. The free energy \(\widetilde{\mathsf{F}}_{n}\) associated with \((Y,Y^{\prime\prime})\) is

\[\widetilde{\mathsf{F}}_{n}=\frac{1}{n}\mathbb{E}\bigg{[}\log\int_{\mathbb{R}^{d} }\int_{\mathbb{R}^{n}}\exp\Bigl{(}\mathcal{H}_{n}(\Xi^{-1/2}u,\Sigma^{-1/2}v)+ru ^{\top}\Xi^{-1}u^{*}+\sqrt{r}u^{\top}\Xi^{-1/2}Z^{\prime\prime}-\frac{r}{2}u^ {\top}\Xi^{-1}u\Bigr{)}\,\mathrm{d}P^{\otimes n}(u)\,\mathrm{d}Q^{\otimes d }(v)\bigg{]},\]

where \(\mathcal{H}_{n}\) is given in (4.12). A straightforward adaptation of the proof of Theorem 4.4 yields a characterization of the limit of \(\widetilde{\mathsf{F}}_{n}\). Let

\[\widetilde{\mathsf{F}}(r)\coloneqq\sup_{q_{v}\geq 0}\inf_{q_{u}\geq 0}\psi_{\Xi} \Big{(}\sqrt{\gamma^{\prime}}(\alpha q_{v}+r)\Big{)}+\alpha\psi_{\Xi}\bigg{(} \frac{\gamma}{\sqrt{\gamma^{\prime}}}q_{u}\bigg{)}-\frac{\alpha}{2}q_{u}q_{v}.\] (D.9)

[MISSING_PAGE_FAIL:32]

By Lemma D.2, this implies the lower bound

\[\liminf_{n\to\infty}\mathsf{F}_{n}\geq\sup_{r\geq 0}\widetilde{\mathsf{F}}(r)- \frac{r^{2}}{4}.\]

Next we show a matching upper bound. Let \(\varepsilon\in[1,2]\) and slightly abusing notation, denote by \(r(t;\varepsilon)\) the solution to

\[r^{\prime}(t)=\frac{1}{n}\mathbb{E}\Big{[}\big{\langle}\!\left\langle\widetilde {u}^{\top}\widetilde{u}^{*}\right\rangle\!\big{\rangle}_{n,t}\Big{]},\quad r(0) =s_{n}\varepsilon,\]

where \(s_{n}=n^{-1/32}\). The analogue of Lemma C.7 gives

\[\int_{1}^{2}\frac{1}{n^{2}}\mathbb{E}\Big{[}\!\left\langle\!\left\langle \widetilde{u}^{\top}\widetilde{u}^{*}-\mathbb{E}\Big{[}\big{\langle}\widetilde {u}^{\top}\widetilde{u}^{*}\big{\rangle}_{n,t}\Big{]}\right\rangle^{2}\! \right\rangle\!\Big{\rangle}_{n,t}\Big{]}\,\mathrm{d}\varepsilon\leq\frac{C}{n^ {1/8}},\] (D.12)

for a constant \(C>0\) independent of \(n\). Using (D.11) with \(r(t;\varepsilon)\), we have

\[\mathsf{F}_{n} =\int_{1}^{2}\mathsf{f}_{n}(0)\,\mathrm{d}\varepsilon+o(1)=\int_ {1}^{2}\!\left(\widetilde{\mathsf{F}}_{n}(r(1,\varepsilon))-\int_{0}^{1}\frac {r^{\prime}(t,\varepsilon)^{2}}{4}\,\mathrm{d}t\right)\mathrm{d}\varepsilon+o (1)\] \[\leq\int_{1}^{2}\int_{0}^{1}\widetilde{\mathsf{F}}_{n}(r^{\prime} (t,\varepsilon))-\frac{r^{\prime}(t,\varepsilon)^{2}}{4}\,\mathrm{d}t\, \mathrm{d}\varepsilon+o(1)\leq\sup_{r\geq 0}\widetilde{\mathsf{F}}_{n}(r)- \frac{r^{2}}{4}+o(1),\]

where the second equality is by (D.12) and the penultimate inequality holds since \(\widetilde{\mathsf{F}}_{n}(\cdot)\) is convex and non-decreasing. Passing to the limit, we obtain the upper bound

\[\limsup_{n\to\infty}\mathsf{F}_{n}\leq\sup_{r\geq 0}\widetilde{\mathsf{F}}(r)- \frac{r^{2}}{4},\]

as desired. 

To establish Lemma D.1, it remains to verify that the RHSs of (D.5) and (D.10) are equal. We need the following lemma.

**Lemma D.4**.: _Let \(f,g\colon\mathbb{R}_{\geq 0}\to\mathbb{R}\) be non-decreasing, lower semi-continuous, convex functions with finite \(f(0),g(0)\) and monotone conjugates \(f^{*},g^{*}\) (see Definition G.1). Then_

\[\sup_{r\geq 0}\sup_{q_{1}\geq 0}\inf_{q_{2}\geq 0}f(q_{1}+r)+g(q_{2})-q_{1}q_{ 2}-\frac{r^{2}}{2}=\sup_{q_{1},q_{2}\geq 0}\frac{q_{2}^{2}}{2}+q_{1}q_{2}-f^{*}(q_{ 2})-g^{*}(q_{1}).\]

Proof.: Writing \(f_{r}(x)=f(x+r)\) and using Proposition G.12, we have

\[\sup_{q_{1}\geq 0}\inf_{q_{2}\geq 0}f(q_{1}+r)+g(q_{2})-q_{1}q_{2} =\sup_{q_{1},q_{2}\geq 0}q_{1}q_{2}-f^{*}_{r}(q_{2})-g^{*}(q_{1})\] \[=\sup_{q_{1}\geq 0}f_{r}(q_{1})-g^{*}(q_{1})=\sup_{q_{1},q_{2} \geq 0}q_{2}(q_{1}+r)-f^{*}(q_{2})-g^{*}(q_{1}),\]

where we have used the fact that \(f^{**}=f\). Therefore,

\[\sup_{r\geq 0}\sup_{q_{1}\geq 0}\inf_{q_{2}\geq 0}f(q_{1}+r)+g(q_{2}) -q_{1}q_{2}-\frac{r^{2}}{2} =\sup_{q_{1},q_{2}\geq 0}\!\left\{\sup_{r\geq 0}\!\left\{q_{2}r- \frac{r^{2}}{2}\right\}+q_{1}q_{2}-f^{*}(q_{2})-g^{*}(q_{1})\right\}\] \[=\sup_{q_{1},q_{2}\geq 0}\frac{q_{2}^{2}}{2}+q_{1}q_{2}-f^{*}(q_{ 2})-g^{*}(q_{1}),\]

as claimed. 

Applying Lemma D.4 immediately finishes the proof of Lemma D.1.

Proof of Lemma D.1.: By Lemma D.4 and the definition (D.9),

\[\sup_{r\geq 0}\widetilde{\mathsf{F}}(r)-\frac{r^{2}}{4}=\sup_{q_{u},q_{v }\geq 0}\frac{\gamma^{\prime}}{4}q_{u}^{2}+\frac{\alpha}{2}\gamma q_{u}q_{v}- \psi_{\Xi}^{*}\!\left(\frac{q_{u}}{2}\right)-\alpha\psi_{\Xi}^{*}\!\left(\frac {q_{v}}{2}\right)\!,\]

where we have used the fact that for \(a,b\geq 0\), the monotone conjugate of \(g(x)\coloneqq bf(ax)\) is \(g^{*}(x)=bf^{*}(x/(ab))\)

### Proof of Corollary 4.3

Denote \(M\coloneqq\mathbb{E}\Big{[}u^{*}{v^{*}}^{\top}\Big{|}\ Y\Big{]}\). By the Nishimori identity (Proposition G.4),

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\big{\|}\widetilde{u}^{*}( \widetilde{v}^{*})^{\top}-\mathbb{E}\big{[}\widetilde{u}^{*}(\widetilde{v}^{* })^{\top}\big{|}\ Y\big{]}\big{\|}_{\mathrm{F}}^{2}\Big{]}=\mathbb{E}\Big{[} \overline{\Xi}^{-1}\Big{]}\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]}-\lim_ {n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\Big{\|}\Xi^{-1/2}M\Sigma^{-1/2} \Big{\|}_{\mathrm{F}}^{2}\Big{]},\] (D.13)

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\Big{\|}{u^{*}}{v^{*}}^{\top}- \mathbb{E}\Big{[}{u^{*}}{v^{*}}^{\top}\Big{|}\ Y\Big{]}\Big{\|}_{\mathrm{F}}^{2 }\Big{]}=1-\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\|M\|_{\mathrm{F}}^{2 }\Big{]}.\] (D.14)

Theorem 4.2 and (D.13) imply that

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\bigg{[}\Big{\|}\Xi^{-1/2}M\Sigma^{-1/ 2}\Big{\|}_{\mathrm{F}}^{2}\Big{]}=q_{u}^{*}q_{v}^{*},\] (D.15)

which, by Proposition 4.1, is positive if and only if (4.6) holds.

We first show (4.10) assuming (4.6). Using assumption (3.3), super-multiplicativity of \(\sigma_{\min}(\cdot)\), and the fact that \(\|BC\|_{\mathrm{F}}\geq\|B\|_{\mathrm{F}}\sigma_{\min}(C)\), we have

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\|M\|_{\mathrm{F}} ^{2}\Big{]} \geq\lim_{n\to\infty}\frac{1}{nd}\sigma_{n}(\Xi^{1/2})^{2}\mathbb{ E}\bigg{[}\Big{\|}\Xi^{-1/2}M\Sigma^{-1/2}\Big{\|}_{\mathrm{F}}^{2}\bigg{]} \sigma_{d}(\Sigma^{1/2})^{2}\] \[=(\inf\mathrm{supp}(\overline{\Xi}))(\inf\mathrm{supp}(\overline {\Sigma}))\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\bigg{[}\Big{\|}\Xi^{-1/2}M \Sigma^{-1/2}\Big{\|}_{\mathrm{F}}^{2}\bigg{]},\]

which is positive by (D.15). This combined with (D.14) implies (4.10).

We then show (4.10) with \(<\) replaced with \(=\), assuming that (4.6) is reversed. Using assumption (3.3), sub-multiplicativity of spectral norm, and the fact that \(\|BC\|_{\mathrm{F}}\leq\|B\|_{\mathrm{F}}\|C\|_{2}\), we have

\[\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\Big{[}\|M\|_{\mathrm{F}}^{ 2}\Big{]} \leq\lim_{n\to\infty}\frac{1}{nd}\Big{\|}\Xi^{1/2}\Big{\|}_{2}^{2} \mathbb{E}\bigg{[}\Big{\|}\Xi^{-1/2}M\Sigma^{-1/2}\Big{\|}_{\mathrm{F}}^{2} \bigg{]}\Big{\|}\Sigma^{1/2}\Big{\|}_{2}^{2}\] \[=(\sup\mathrm{supp}(\overline{\Xi}))(\sup\mathrm{supp}(\overline {\Sigma}))\lim_{n\to\infty}\frac{1}{nd}\mathbb{E}\bigg{[}\Big{\|}\Xi^{-1/2}M \Sigma^{-1/2}\Big{\|}_{\mathrm{F}}^{2}\bigg{]},\]

which is \(0\) by (D.15). This combined with (D.14) finishes the proof of the corollary for estimating \({u^{*}}{v^{*}}^{\top}\). The proofs for estimating \({u^{*}}{u^{*}}^{\top},{v^{*}}{v^{*}}^{\top}\) are similar and omitted.

## Appendix E Analysis of the spectral estimator

### Bayes-AMP

We propose an AMP algorithm that operates on \(\Xi^{-1}A\Sigma^{-1}\) and maintains a pair of iterates \(u^{t}\in\mathbb{R}^{n},v^{t+1}\in\mathbb{R}^{d}\) for every \(t\geq 0\). Specifically, given for every \(t\geq 0\) a pair of denoising functions2\(g_{t}\colon\mathbb{R}^{n}\to\mathbb{R}^{n},f_{t+1}\colon\mathbb{R}^{d}\to \mathbb{R}^{d}\), the iterates are initialized at \(\widetilde{u}^{-1}=0_{n}\) and some \(\widetilde{v}^{0}\in\mathbb{R}^{d}\) of user's choice, and are updated for every \(t\geq 0\) according to the following rules:

Footnote 2: Strictly speaking, for every \(t\geq 0\), we are given two sequences of functions \(g_{t},f_{t+1}\) indexed by \(n,d\) respectively. See Definition E.1 for a formal treatment of function sequences.

\[u^{t} =\Xi^{-1}A\Sigma^{-1}\widetilde{v}^{t}-b_{t}\Xi^{-1}\widetilde{u }^{t-1},\quad\widetilde{u}^{t}=g_{t}(u^{t}),\quad c_{t}=\frac{1}{n}\operatorname {Tr}((\nabla g_{t}(u^{t}))\Xi^{-1}),\] \[v^{t+1} =\Sigma^{-1}A^{\top}\Xi^{-1}\widetilde{u}^{t}-c_{t}\Sigma^{-1} \widetilde{v}^{t},\quad\widetilde{v}^{t+1}=f_{t+1}(v^{t+1}),\quad b_{t+1}=\frac {1}{n}\operatorname{Tr}((\nabla f_{t+1}(v^{t+1}))\Sigma^{-1}),\] (E.1)

where \(\nabla g_{t}(u^{t})\in\mathbb{R}^{n\times n},\nabla f_{t+1}(v^{t+1})\in\mathbb{R} ^{d\times d}\) denote the Jacobians of \(g_{t},f_{t+1}\) at \(u^{t},v^{t+1}\), respectively. For any fixed \(t\geq 0\), the \(n,d\to\infty\) limit of the iterates \(u^{t},v^{t+1}\) can be described by a deterministic recursion known as the state evolution. To define the latter, we need a sequence of preliminary definitions.

First, define random vectors \((U^{*},W_{U,0},W_{U,1},\cdots,W_{U,t})\in(\mathbb{R}^{n})^{t+2}\) and \((V^{*},W_{V,1},W_{V,2},\cdots,W_{V,t+1})\in(\mathbb{R}^{d})^{t+2}\) with joint distributions specified below:

\[\begin{bmatrix}U^{*}\\ \sigma_{0}W_{U,0}\\ \sigma_{1}W_{U,1}\\ \vdots\\ \sigma_{t}\dot{W}_{U,t}\end{bmatrix}\sim P^{\otimes n}\otimes\mathcal{N}(0_{n( t+1)},\Phi_{t}\otimes I_{n}),\quad\begin{bmatrix}V^{*}\\ \tau_{1}W_{V,1}\\ \tau_{2}W_{V,2}\\ \vdots\\ \tau_{t+1}\dot{W}_{V,t+1}\end{bmatrix}\sim Q^{\otimes d}\otimes\mathcal{N}(0_{ d(t+1)},\Psi_{t+1}\otimes I_{d}),\] (E.2)

where we recall that for \(A\in\mathbb{R}^{m\times n},B\in\mathbb{R}^{p\times q}\), their Kronecker product is

\[A\otimes B=\begin{bmatrix}A_{1,1}B&\cdots&A_{1,n}B\\ \vdots&\ddots&\vdots\\ A_{m,1}B&\cdots&A_{m,n}B\end{bmatrix}\in\mathbb{R}^{mp\times nq}.\]

The covariance matrices \(\Phi_{t}=(\Phi_{r,s})_{0\leq r,s\leq t},\Psi_{t+1}=(\Psi_{r+1,s+1})_{0\leq r,s \leq t}\in\mathbb{R}^{(t+1)\times(t+1)}\) are given by the \((t+1)\times(t+1)\) principal minors of two infinite-dimensional symmetric matrices \(\Phi\coloneqq(\Phi_{r,s})_{r,s\geq 0},\Psi\coloneqq(\Psi_{r+1,s+1})_{r,s\geq 0}\) whose elements are in turn given recursively below:

\[\begin{split}\Phi_{0,0}&=\underset{n\to\infty}{\text{ \rm-}\lim}\,\frac{1}{n}(\widetilde{v}^{0})^{\top}\Sigma^{-1}\widetilde{v}^{0}, \\ \Phi_{0,s}&=\underset{n\to\infty}{\text{\rm-}\lim}\, \frac{1}{n}\mathbb{E}\big{[}f_{0}(V^{*})^{\top}\Sigma^{-1}f_{s}(V_{s})\big{]}, \quad s\geq 1,\\ \Phi_{r,s}&=\underset{n\to\infty}{\text{\rm-}\lim}\, \frac{1}{n}\mathbb{E}\big{[}f_{r}(V_{r})^{\top}\Sigma^{-1}f_{s}(V_{s})\big{]}, \quad r,s\geq 1,\\ \Psi_{r+1,s+1}&=\underset{n\to\infty}{\text{\rm-}\lim}\, \frac{1}{n}\mathbb{E}\big{[}g_{r}(U_{r})^{\top}\Xi^{-1}g_{s}(U_{s})\big{]}, \quad r,s\geq 0.\end{split}\] (E.3)

Furthermore, for \(t\geq 0\), \(\sigma_{t},\tau_{t+1}>0\) are defined as

\[\begin{split}\sigma_{0}^{2}&\coloneqq\Phi_{0,0}= \underset{n\to\infty}{\text{\rm-}\lim}\,\frac{1}{n}(\widetilde{v}^{0})^{\top} \Sigma^{-1}\widetilde{v}^{0},\\ \sigma_{t}^{2}&\coloneqq\Phi_{t,t}=\underset{n\to \infty}{\text{\rm-}\lim}\,\frac{1}{n}\mathbb{E}\big{[}f_{t}(V_{t})^{\top} \Sigma^{-1}f_{t}(V_{t})\big{]},\quad t\geq 1,\\ \tau_{t+1}^{2}&\coloneqq\Psi_{t+1,t+1}=\underset{n\to \infty}{\text{\rm-}\lim}\,\frac{1}{n}\mathbb{E}\big{[}g_{t}(U_{t})^{\top}\Xi^{ -1}g_{t}(U_{t})\big{]},\quad t\geq 0.\end{split}\] (E.4)

With the above definitions, note that each \(W_{U,t},W_{V,t+1}\) is marginally distributed as \(\mathcal{N}(0_{n},I_{n}),\mathcal{N}(0_{d},I_{d})\), respectively.

Next, define two sequences of deterministic scalars \((\mu_{t},\nu_{t+1})_{t\geq 0}\):

\[\begin{split}\mu_{0}&=\underset{n\to\infty}{\text{ \rm-}\lim}\,\frac{\lambda}{n}\mathbb{E}\big{[}\big{(}\Sigma^{-1}V^{*},f_{0}(V^ {*})\big{)}\big{]},\\ \mu_{t}&=\underset{n\to\infty}{\text{\rm-}\lim}\, \frac{\lambda}{n}\mathbb{E}\big{[}\big{(}\Sigma^{-1}V^{*},f_{t}(V_{t})\big{)} \big{]},\quad t\geq 1,\\ \nu_{t+1}&=\underset{n\to\infty}{\text{\rm-}\lim}\, \frac{\lambda}{n}\mathbb{E}\big{[}\big{(}\Xi^{-1}U^{*},g_{t}(U_{t})\big{)} \big{]},\quad t\geq 0,\end{split}\] (E.5)

where \(f_{0}\) is determined by the initializer \(\widetilde{v}^{0}\); see Assumption (A1) below.

With these, for \(t\geq 0\), define random vectors

\[U_{t}=\mu_{t}\Xi^{-1}U^{*}+\sigma_{t}\Xi^{-1/2}W_{U,t},\qquad V_{t+1}=\nu_{t+ 1}\Sigma^{-1}V^{*}+\tau_{t+1}\Sigma^{-1/2}W_{V,t+1}.\] (E.6)

Finally, we need the notion of (uniformly) pseudo-Lipschitz functions.

**Definition E.1** (Pseudo-Lipschitz functions).: A function \(\phi\colon\mathbb{R}^{k\times m}\to\mathbb{R}^{\ell\times m}\) is called pseudo-Lipschitz of order \(j\geq 1\) if there exists \(L>0\) such that

\[\frac{1}{\sqrt{\ell}}\|\phi(x)-\phi(y)\|_{\text{F}}\leq\frac{L}{\sqrt{k}}\|x-y \|_{\text{F}}\Bigg{[}1+\bigg{(}\frac{1}{\sqrt{k}}\|x\|_{\text{F}}\bigg{)}^{j-1}+ \bigg{(}\frac{1}{\sqrt{k}}\|y\|_{\text{F}}\bigg{)}^{j-1}\Bigg{]},\] (E.7)

for every \(x,y\in\mathbb{R}^{k\times m}\).

We will consider sequences of functions \(\phi_{i}\colon\mathbb{R}^{k_{i}\times m}\to\mathbb{R}^{\ell_{i}\times m}\) indexed by \(i\to\infty\) though the index \(i\) is often suppressed. A sequence of functions \((\phi_{i}\colon\mathbb{R}^{k_{i}\times m}\to\mathbb{R}^{\ell_{i}\times m})_{i\geq 1}\) (with increasing dimensions \((k_{i})_{i\geq 1},(\ell_{i})_{i\geq 1}\)) is called uniformly pseudo-Lipschitz of order \(j\) if there exists a constant \(L\) such that for every \(i\geq 1\), (E.7) holds.

The assumptions below are imposed on the initializer \(\tilde{v}^{0}\) and denoising function \((g_{t},f_{t+1})_{t\geq 0}\).

* \(\tilde{v}^{0}\) is independent of \(\widetilde{W}\) but may depend on \(v^{*}\).3 Assume that \[\operatorname*{p-lim}_{d\to\infty}\frac{1}{d}\big{\|}\tilde{v}^{0}\big{\|}_{2 }^{2},\qquad\operatorname*{p-lim}_{d\to\infty}\frac{1}{n}(\tilde{v}^{0})^{ \top}\Sigma^{-1}\tilde{v}^{0}\] exist and are finite. There exists a uniformly pseudo-Lipschitz function \(f_{0}\colon\mathbb{R}^{d}\to\mathbb{R}^{d}\) of order \(1\) such that \[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}[(f_{0}(V^{*}),f_{0}(V^{*}))]\leq \operatorname*{p-lim}_{d\to\infty}\frac{1}{d}\big{\|}\tilde{v}^{0}\big{\|}_{2 }^{2}\] and for every uniformly pseudo-Lipschitz function \(\phi\colon\mathbb{R}^{d}\to\mathbb{R}^{d}\) of finite order, the following two limits exist, are finite and equal: \[\operatorname*{p-lim}_{d\to\infty}\frac{1}{d}\big{\langle}\tilde{v}^{0},\phi (V^{*})\big{\rangle}=\lim_{d\to\infty}\frac{1}{d}\mathbb{E}[\langle f_{0}(V^{ *}),\phi(V^{*})\rangle].\] Let \(\widetilde{\nu}\in\mathbb{R},\widetilde{\tau}\in\mathbb{R}_{\geq 0}\). For any \(s\geq 1\), \[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}f_{0}(V^{*})^{\top}\Sigma^{-1}f_{ s}(\widetilde{\nu}\Sigma^{-1}V^{*}+\widetilde{\tau}\Sigma^{-1/2}W_{V})\Big{]}\] exists and is finite, where \(W_{V}\sim\mathcal{N}(0_{d},I_{d})\) is independent of \(V^{*}\). Footnote 3: Practically one can think of the dependence of \(\tilde{v}^{0}\) on \(v^{*}\) being given by some side information. However, here AMP is used solely as a proof technique, and we can consider initializers with impractical access to \(v^{*}\).
* Let \(\widetilde{\nu}\in\mathbb{R}\), and \(T\in\mathbb{R}^{2\times 2}\) be positive definite. For any \(r,s\geq 1\), \[\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\Big{[}\Big{\langle} \Sigma^{-1}V^{*},f_{r}(\widetilde{\nu}\Sigma^{-1}V^{*}+\Sigma^{-1/2}N)\Big{ }\Big{\rangle}\Big{]},\] \[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}f_{r}(\widetilde{\nu }\Sigma^{-1}V^{*}+\Sigma^{-1/2}N)^{\top}\Sigma^{-1}f_{s}(\widetilde{\nu} \Sigma^{-1}V^{*}+\Sigma^{-1/2}N^{\prime})\Big{]}\] exist and are finite, where \((N,N^{\prime})\sim\mathcal{N}(0_{2d},T\otimes I_{d})\) is independent of \(V^{*}\). Let \(\widetilde{\mu}\in\mathbb{R}\), and \(S\in\mathbb{R}^{2\times 2}\) be positive definite. For any \(r,s\geq 0\), \[\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\Big{[}\Big{\langle} \Xi^{-1}U^{*},g_{r}(\widetilde{\mu}\Xi^{-1}U^{*}+\Xi^{-1/2}M)\Big{\rangle} \Big{]},\] \[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}g_{r}(\widetilde{\mu }\Xi^{-1}U^{*}+\Xi^{-1/2}M)^{\top}\Xi^{-1}g_{s}(\widetilde{\mu}\Xi^{-1}U^{*}+ \Xi^{-1/2}M^{\prime})\Big{]}\] exist and are finite, where \((M,M^{\prime})\sim\mathcal{N}(0_{2n},S\otimes I_{n})\) is independent of \(U^{*}\).

We now give the state evolution result for the AMP in (E.1), which is proved in Appendix F.

**Proposition E.1** (State evolution for AMP in (E.1)).: _For every \(t\geq 0\), let \((g_{t}\colon\mathbb{R}^{n}\to\mathbb{R}^{n})_{n\geq 1}\) and \((f_{t+1}\colon\mathbb{R}^{d}\to\mathbb{R}^{d})_{d\geq 1}\) be uniformly pseudo-Lipschitz of finite order subject to Assumption (A2). Consider the AMP iteration in (E.1) defined by \((g_{t},f_{t+1})_{t\geq 0}\) and initialized at \(\widetilde{u}^{-1}=0_{n}\) and some \(\widetilde{v}^{0}\in\mathbb{R}^{d}\) subject to Assumption (A1). For any fixed \(t\geq 0\), let \((\phi\colon\mathbb{R}^{(t+2)n}\to\mathbb{R})_{n\geq 1}\) and \((\psi\colon\mathbb{R}^{(t+2)d}\to\mathbb{R})_{d\geq 1}\) be uniformly pseudo-Lipschitz of finite order. Then,_

\[\operatorname*{p-lim}_{n\to\infty}\phi(u^{*},u^{0},u^{1},\cdots,u^{ t})-\mathbb{E}[\phi(U^{*},U_{0},U_{1},\cdots,U_{t})] =0,\] (E.8a) \[\operatorname*{p-lim}_{d\to\infty}\psi(v^{*},v^{1},v^{2},\cdots,v ^{t+1})-\mathbb{E}[\psi(V^{*},V_{1},V_{2},\cdots,V_{t+1})] =0,\] (E.8b)

_where \((U_{s},V_{s+1})_{0\leq s\leq t}\) are defined in (E.6)._Given \(U_{t},V_{t+1}\), the Bayes-optimal (in terms of mean square error) choice of \((g_{t},f_{t+1})_{t\geq 0}\) is given by the conditional expectations. Specifically, for any \(t\geq 0\) and \(u\in\mathbb{R}^{n},v\in\mathbb{R}^{d}\),

\[g_{t}^{*}(u)\coloneqq\mathbb{E}[U^{*}\,|\,U_{t}=u],\qquad f_{t+1}^{*}(v) \coloneqq\mathbb{E}[V^{*}\,|\,V_{t+1}=v].\] (E.9)

We call (E.1) with \(g_{t}=g_{t}^{*},f_{t+1}=f_{t+1}^{*}\) the Bayes-AMP.

If \(P=Q=\mathcal{N}(0,1)\), by (E.2) and (E.6), \((U^{*},U_{t})\) and \((V^{*},V_{t+1})\) are jointly Gaussian with mean zero and covariance:

\[\begin{bmatrix}I_{n}&\mu_{t}\Xi^{-1}\\ \mu_{t}\Xi^{-1}&\mu_{t}^{2}\Xi^{-2}+\sigma_{t}^{2}\Xi^{-1}\end{bmatrix}\in \mathbb{R}^{2n\times 2n},\qquad\begin{bmatrix}I_{d}&\nu_{t+1}\Sigma^{-1}\\ \nu_{t+1}\Sigma^{-1}&\nu_{t+1}^{2}\Sigma^{-2}+\tau_{t+1}^{2}\Sigma^{-1}\end{bmatrix} \in\mathbb{R}^{2d\times 2d},\]

respectively. Therefore using Proposition G.5, \(g_{t}^{*},f_{t+1}^{*}\) admit the following explicit formulas:

\[\begin{split} g_{t}^{*}(u)&=\mu_{t}\Xi^{-1}(\mu_{t}^{2} \Xi^{-2}+\sigma_{t}^{2}\Xi^{-1})^{-1}u=\mu_{t}(\mu_{t}^{2}\Xi^{-1}+\sigma_{t} ^{2}I_{n})^{-1}u,\\ f_{t+1}^{*}(v)&=\tau_{t+1}\Sigma^{-1}(\tau_{t+1}^{2} \Sigma^{-2}+\tau_{t+1}^{2}\Sigma^{-1})^{-1}v=\nu_{t+1}(\nu_{t+1}^{2}\Sigma^{- 1}+\tau_{t+1}^{2}I_{d})^{-1}v.\end{split}\] (E.10)

Under the above choice, the state evolution recursion for \(\mu_{t},\sigma_{t},\nu_{t+1},\tau_{t+1}\) in (E.4) and (E.5) becomes: for all \(t\geq 1\),

\[\mu_{t} =\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\big{[}\big{\langle} \Sigma^{-1}V^{*},\nu_{t}(\nu_{t}^{2}\Sigma^{-1}+\tau_{t}^{2}I_{d})^{-1}V_{t} \big{\rangle}\big{]}=\frac{\lambda}{\delta}\mathbb{E}\Bigg{[}\frac{\nu_{t}^{2 }\overline{\Sigma}^{-2}}{\nu_{t}^{2}\overline{\Sigma}^{-1}+\tau_{t}^{2}}\Bigg{]},\] (E.11a) \[\nu_{t+1} =\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\big{[}\big{\langle} \Xi^{-1}U^{*},\mu_{t}(\mu_{t}^{2}\Xi^{-1}+\sigma_{t}^{2}I_{n})^{-1}U_{t} \big{\rangle}\big{]}=\lambda\mathbb{E}\Bigg{[}\frac{\mu_{t}^{2}\overline{\Xi} ^{-2}}{\mu_{t}^{2}\overline{\Xi}^{-1}+\sigma_{t}^{2}}\Bigg{]},\] (E.11b) \[\sigma_{t}^{2} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\big{[}V_{t}^{\top}(\nu_ {t}^{2}\Sigma^{-1}+\tau_{t}^{2}I_{d})^{-1}\nu_{t}\Sigma^{-1}\nu_{t}(\nu_{t}^{2 }\Sigma^{-1}+\tau_{t}^{2}I_{d})^{-1}V_{t}\big{]}\] \[=\frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\nu_{t}^{4}\overline{ \Sigma}^{-3}}{(\nu_{t}^{2}\overline{\Sigma}^{-1}+\tau_{t}^{2})^{2}}\Bigg{]}+ \frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\nu_{t}^{2}\tau_{t}^{2}\overline{ \Sigma}^{-2}}{(\nu_{t}^{2}\overline{\Sigma}^{-1}+\tau_{t}^{2})^{2}}\Bigg{]}\] (E.11c) \[=\frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\nu_{t}^{2}\overline{ \Sigma}^{-2}}{\nu_{t}^{2}\overline{\Sigma}^{-1}+\tau_{t}^{2}}\Bigg{]},\] \[\tau_{t+1}^{2} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\big{[}U_{t}^{\top}(\mu_ {t}^{2}\Xi^{-1}+\sigma_{t}^{2}I_{n})^{-1}\mu_{t}\Xi^{-1}\mu_{t}(\mu_{t}^{2} \Xi^{-1}+\sigma_{t}^{2}I_{n})^{-1}U_{t}\big{]}\] \[=\mathbb{E}\Bigg{[}\frac{\mu_{t}^{4}\overline{\Xi}^{-3}}{(\mu_{t }^{2}\overline{\Xi}^{-1}+\sigma_{t}^{2})^{2}}\Bigg{]}+\mathbb{E}\Bigg{[}\frac{ \mu_{t}^{2}\sigma_{t}^{2}\overline{\Xi}^{-2}}{(\mu_{t}^{2}\overline{\Xi}^{-1}+ \sigma_{t}^{2})^{2}}\Bigg{]}\] (E.11d) \[=\mathbb{E}\Bigg{[}\frac{\mu_{t}^{2}\overline{\Xi}^{-2}}{\mu_{t}^ {2}\overline{\Xi}^{-1}+\sigma_{t}^{2}}\Bigg{]},\]

where we have used the definitions (E.6) of \(U_{t},V_{t+1}\), the joint distribution (E.2) of \((U^{*},W_{U,t}),(V^{*},V_{t+1})\), the convergence of the empirical spectral distributions of \(\Sigma,\Xi\), and Proposition G.2.

Inspecting the expressions, we realize that

\[\mu_{t}=\lambda\sigma_{t}^{2},\qquad\nu_{t+1}=\lambda\tau_{t}^{2}.\] (E.12)

This allows us to only track the recursion of \(\mu_{t},\nu_{t+1}\):

\[\mu_{t}=\frac{\lambda}{\delta}\mathbb{E}\Bigg{[}\frac{\lambda\nu_{t}\overline{ \Sigma}^{-2}}{\lambda\nu_{t}\overline{\Sigma}^{-1}+1}\Bigg{]},\qquad\nu_{t+1}= \lambda\mathbb{E}\Bigg{[}\frac{\lambda\mu_{t}\overline{\Xi}^{-2}}{\lambda\mu_{t} \overline{\Xi}^{-1}+1}\Bigg{]}.\]

Thus, the fixed point \((\mu^{*},\nu^{*})\) of the above recursion must satisfy:

\[\mu^{*}=\frac{\lambda}{\delta}\mathbb{E}\Bigg{[}\frac{\lambda\nu^{*}\overline{ \Sigma}^{-2}}{\lambda\nu^{*}\overline{\Sigma}^{-1}+1}\Bigg{]},\qquad\nu^{*}= \lambda\mathbb{E}\Bigg{[}\frac{\lambda\mu^{*}\overline{\Xi}^{-2}}{\lambda\mu^{*} \overline{\Xi}^{-1}+1}\Bigg{]}.\] (E.13)

Note that upon a change of variable

\[q_{u}\coloneqq\frac{\nu^{*}}{\lambda},\qquad q_{v}\coloneqq\frac{\delta\mu^{*}} {\lambda},\] (E.14)

the fixed point equation (E.13) coincides with that in the characterization of the free energy; see (4.5).

### Spectral estimator from Bayes-AMP

Under Gaussian priors, the Bayes-AMP algorithm specified by (E.1) and (E.10) naturally suggests a spectral estimator with respect to a matrix which is a non-trivial transformation of \(A\). In what follows, we provide a heuristic derivation of this spectral estimator. Its performance guarantee (Theorem 5.1) will be proved in Appendix E.4.

Suppose, informally, that \(\mu_{t}\), \(\sigma_{t}\), \(\nu_{t+1}\), \(\tau_{t+1}\), \(u^{t}\), \(v^{t+1}\), \(c_{t}\), \(b_{t+1}\) converge (under the sequential limits \(n\to\infty,t\to\infty\)) to \(\mu^{*},\sigma^{*},\nu^{*},\tau^{*},u,v,c^{*},b^{*}\), respectively, in the sense that, e.g.,

\[\lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{n}}\big{\|}u ^{t}-u\big{\|}_{2}=0.\]

Recall that \((\mu^{*},\nu^{*})\) solves the fixed point equation (E.13), and from (E.12), the following relation holds:

\[\mu^{*}=\lambda{\sigma^{*}}^{2},\qquad\nu^{*}=\lambda{\tau^{*}}^{2}.\] (E.15)

So denoting

\[G\coloneqq\lambda(\lambda\mu^{*}\Xi^{-1}+I_{n})^{-1}\in\mathbb{R}^{n\times n},\qquad F\coloneqq\lambda(\lambda\nu^{*}\Sigma^{-1}+I_{d})^{-1}\in\mathbb{R}^{ d\times d},\] (E.16)

by the design of \(g_{t}^{*}\), \(f_{t+1}^{*}\) in (E.10), we have that \(\widetilde{u}^{t},\widetilde{v}^{t+1}\) converge to

\[\widetilde{u}=\mu^{*}\Big{(}{\mu^{*}}^{2}\Xi^{-1}+{\sigma^{*}}^{2}I_{n}\Big{)} ^{-1}u=Gu,\qquad\widetilde{v}=\nu^{*}\Big{(}{\nu^{*}}^{2}\Sigma^{-1}+{\tau^{* }}^{2}I_{d}\Big{)}^{-1}v=Fv,\]

respectively, and the limiting Onsager coefficients \(b^{*},c^{*}\) are given by:

\[b^{*}=\lim_{n\to\infty}\frac{1}{n}\operatorname{Tr}(F\Sigma^{-1})=\frac{1}{ \delta}\mathbb{E}\bigg{[}\frac{\lambda}{\lambda\nu^{*}+\overline{\Sigma}} \bigg{]},\quad c^{*}=\lim_{n\to\infty}\frac{1}{n}\operatorname{Tr}(G\Xi^{-1}) =\mathbb{E}\bigg{[}\frac{\lambda}{\lambda\mu^{*}+\overline{\Xi}}\bigg{]}.\] (E.17)

At the fixed point of (E.1), we have

\[u =\Xi^{-1}A\Sigma^{-1}\widetilde{v}-b^{*}\Xi^{-1}\widetilde{u}= \Xi^{-1}A\Sigma^{-1}Fv-b^{*}\Xi^{-1}Gu,\] (E.18) \[v =\Sigma^{-1}A^{\top}\Xi^{-1}\widetilde{u}-c^{*}\Sigma^{-1} \widetilde{v}=\Sigma^{-1}A^{\top}\Xi^{-1}Gu-c^{*}\Sigma^{-1}Fv.\]

Upon rearrangement, (E.18) is equivalent to

\[\widehat{G}u=\Xi^{-1}A\Sigma^{-1}Fv,\qquad\widehat{Fv}=\Sigma^{-1}A^{\top} \Xi^{-1}Gu,\] (E.19)

where

\[\widehat{G}\coloneqq I_{n}+b^{*}\Xi^{-1}G\in\mathbb{R}^{n\times n},\qquad \widehat{F}\coloneqq I_{d}+c^{*}\Sigma^{-1}Fv\in\mathbb{R}^{d\times d}.\] (E.20)

We further introduce the notation:

\[\widetilde{G}\coloneqq\widehat{G}G^{-1}\in\mathbb{R}^{n\times n},\qquad \widetilde{F}\coloneqq\widehat{F}F^{-1}\in\mathbb{R}^{d\times d},\] (E.21)

so that (E.19) can be rewritten as

\[\widetilde{G}Gu=\Xi^{-1}A\Sigma^{-1}Fv,\qquad\widetilde{F}Fv=\Sigma^{-1}A^{ \top}\Xi^{-1}Gu,\]

or

\[\widetilde{G}^{1/2}Gu=\widetilde{G}^{-1/2}\Xi^{-1}A\Sigma^{-1}\widetilde{F}^{ -1/2}\cdot\widetilde{F}^{1/2}Fv,\qquad\widetilde{F}^{1/2}Fv=\widetilde{F}^{-1/ 2}\Sigma^{-1}A^{\top}\Xi^{-1}\widetilde{G}^{-1/2}\cdot\widetilde{G}^{1/2}Gu.\]

The key observation is that this is a pair of singular vector equations for the matrix

\[A^{*}\coloneqq\widetilde{G}^{-1/2}\Xi^{-1}A\Sigma^{-1}\widetilde{F}^{-1/2} \in\mathbb{R}^{n\times d}\] (E.22)

with respect to left/right singular vectors (up to rescaling)

\[\widetilde{G}^{1/2}Gu\in\mathbb{R}^{n},\qquad\widetilde{F}^{1/2}Fv\in\mathbb{ R}^{d}\]

and singular value 1. Using the definitions (E.16), (E.20) and (E.21), we verify that the two expressions of \(A^{*}\) in (5.3) and (E.22) are equal.

By the state evolution result (Proposition E.1), \(u,v\) behave (in the sense of (E.8)) as

\[\mu^{*}\Xi^{-1}u^{*}+\sigma^{*}\Xi^{-1/2}W_{U},\qquad\nu^{*}\Sigma^{-1}v^{*}+ \tau^{*}\Sigma^{-1/2}W_{V},\]

for \(W_{U}\sim\mathcal{N}(0_{n},I_{d}),W_{V}\sim\mathcal{N}(0_{d},I_{d})\) independent of each other and of \(u^{*},v^{*}\). This suggests that

\[\Xi(\widetilde{G}^{1/2}G)^{-1}u_{1}(A^{*}),\qquad\Sigma(\widetilde{F}^{1/2}F)^ {-1}v_{1}(A^{*})\] (E.23)

are effective estimates of \(u^{*},v^{*}\). Simple algebra reveals that the above vectors, upon suitable rescaling, are precisely \(\widetilde{u},\widetilde{v}\) in (5.4).

### Right edge of the bulk

Before proceeding with the proof of Theorem 5.1, we provide a characterization of \(\sigma_{2}(A^{*})\), i.e., the right edge of the bulk of the spectrum of \(A^{*}\). This bulk is related to the spectrum of the non-spiked random matrix

\[W^{*}\coloneqq\lambda(\lambda(\mu^{*}+b^{*})I_{n}+\Xi)^{-1/2}\widetilde{W}( \lambda(\nu^{*}+c^{*})I_{d}+\Sigma)^{-1/2}.\] (E.24)

We first present a characterization of \(\sigma_{1}(W^{*})\) and then relate it to \(\sigma_{2}(A^{*})\). Define random variables:

\[\overline{\Xi}^{*}\coloneqq\frac{\lambda}{\lambda(\mu^{*}+b^{*})+\overline{ \Xi}},\qquad\overline{\Sigma}^{*}\coloneqq\frac{\lambda}{\lambda(\nu^{*}+c^{* })+\overline{\Sigma}}.\] (E.25)

Define functions \(c,s\colon(\sup\mathrm{supp}(\overline{\Xi}^{*}),\infty)\to(0,\infty)\) as

\[c(\alpha)=\mathbb{E}\Bigg{[}\frac{\overline{\Xi}^{*}}{\alpha-\overline{\Xi}^{ *}}\Bigg{]},\quad s(\alpha)=\sup\mathrm{supp}(\overline{\Sigma}^{*})c(\alpha).\]

Define the implicit function \(\beta\colon(\sup\mathrm{supp}(\overline{\Xi}^{*}),\infty)\to(0,\infty)\) as, for any \(\alpha\in(\sup\mathrm{supp}(\overline{\Xi}^{*}),\infty)\), the unique solution in \((s(\alpha),\infty)\) to

\[1=\frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\overline{\Sigma}^{*}}{\beta-c( \alpha)\overline{\Sigma}^{*}}\Bigg{]}.\]

(The existence and uniqueness of the solution is easy to see.) Next, define \(\psi\colon(\sup\mathrm{supp}(\overline{\Xi}^{*}),\infty)\to(0,\infty)\) as \(\psi(\alpha)=\alpha\beta(\alpha)\). It is known that \(\psi\) is differentiable and the set of its critical points is a nonempty finite set [79]. Let \(\alpha^{\circ}\in(\sup\mathrm{supp}(\overline{\Xi}^{*}),\infty)\) be the largest critical point, i.e.,

\[1=\frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\overline{\Sigma}^{*}}{(\beta( \alpha)-c(\alpha)\overline{\Sigma}^{*})^{2}}\Bigg{]}\mathbb{E}\Bigg{[}\frac{ \overline{\Xi}^{*}{}^{2}}{(\alpha-\overline{\Xi}^{*})^{2}}\Bigg{]}\]

Finally, denote

\[\sigma_{2}^{*}\coloneqq\sqrt{\psi(\alpha^{\circ})}.\] (E.26)

The characterization of \(\sigma_{1}(W^{*})\) requires an extra technical assumption on the random variables \(\overline{\Xi},\overline{\Sigma}\), which is the same as in [79].

* For any \(c>0\), \[\lim_{\beta\downarrow s}\mathbb{E}\Bigg{[}\frac{\overline{\Sigma}^{*}}{\beta -c\overline{\Sigma}^{*}}\Bigg{]}=\lim_{\beta\downarrow s}\mathbb{E}\Bigg{[} \Bigg{(}\frac{\overline{\Sigma}^{*}}{\beta-c\overline{\Sigma}^{*}}\Bigg{)}^{2 }\Bigg{]}=\infty.\] where \(s\coloneqq c\cdot\sup\mathrm{supp}(\overline{\Sigma}^{*})\). Furthermore, \[\lim_{\alpha\downarrow\sup\mathrm{supp}(\overline{\Sigma}^{*})}\mathbb{E} \Bigg{[}\frac{\overline{\Xi}^{*}}{\alpha-\overline{\Xi}^{*}}\Bigg{]}=\infty.\]

**Lemma E.2**.: _Let Assumption (A3) hold. Consider \(W^{*}\) defined in (E.24). Then, we have_

\[\operatorname*{p-lim}_{n\to\infty}\sigma_{1}(W^{*})=\sigma_{2}^{*}.\]

Proof.: Note that \(W^{*}{W^{*}}^{\top}\) is a separable covariance matrix. Its largest eigenvalue is characterized in [25]. The explicit formulas we need are due to [79]. To apply their results, one simply observes that the covariances (as in the context of separable covariance matrices) of \(W^{*}\) are

\[\Xi^{*}\coloneqq\sqrt{\lambda}(\lambda(\mu^{*}+b^{*})I_{n}+\Xi)^{-1/2},\qquad \Sigma^{*}\coloneqq\sqrt{\lambda}(\lambda(\nu^{*}+c^{*})I_{d}+\Sigma)^{-1/2},\]

whose limiting spectral distributions are given by the distributions of \(\overline{\Xi}^{*},\overline{\Sigma}^{*}\) in (E.25). 

**Lemma E.3**.: _Consider \(A^{*}\) defined in (5.3). Then_

\[\operatorname*{p-lim}_{n\to\infty}\sigma_{2}(A^{*})=\sigma_{2}^{*}.\]

Proof.: By Weyl's inequality, \(\sigma_{3}(W^{*})\leq\sigma_{1}(A^{*})\leq\sigma_{1}(W^{*})\). We have already shown in Lemma E.2 that \(\sigma_{1}(W^{*})\) converges to \(\sigma_{2}^{*}\). The almost sure weak convergence of the empirical spectral distribution of \(W^{*}\)[78, Theorem 1.2.1] implies that \(\sigma_{3}(W^{*})\) (and indeed \(\sigma_{k}(W^{*})\) for any constant \(k\) relative to \(n,d\)) must also converge to the same limit \(\sigma_{2}^{*}\)

### Proof of Theorem 5.1

We suppose throughout the proof that the condition (5.1) holds. Then, by Proposition 4.1 and the change of variable (E.14), the fixed point equation (E.13) admits a unique non-trivial solution \((\mu^{*},\nu^{*})\in\mathbb{R}^{2}_{>0}\). Construct matrices \(F,G\) as in (E.16) using such \(\mu^{*},\nu^{*}\). Define also the random variables

\[\overline{G}\coloneqq\frac{\lambda}{\lambda\mu^{*}\Xi^{-1}+1}, \quad\overline{F}\coloneqq\frac{\lambda}{\lambda\nu^{*}\Xi^{-1}+1}\]

whose distributions are the limiting spectral distributions of \(G,F\), respectively.

Now consider the denoising functions:

\[f_{t+1}(v^{t+1})=Fv^{t+1},\quad g_{t}(u^{t})=Gu^{t}.\]

With this choice, the AMP iteration (E.1) becomes

\[u^{t}=\Xi^{-1}A\Sigma^{-1}Fv^{t}-b\Xi^{-1}Gu^{t-1},\quad v^{t+1}= \Sigma^{-1}A^{\top}\Xi^{-1}Gu^{t}-c\Sigma^{-1}Fv^{t},\] (E.27)

where

\[b=\frac{1}{n}\operatorname{Tr}(F\Sigma^{-1}),\quad c=\frac{1}{n }\operatorname{Tr}(G\Xi^{-1}).\]

Note that as \(n\to\infty\), \(b,c\) converge to \(b^{*}\), \(c^{*}\) in (E.17).

Recall from (E.15) the definition of \(\sigma^{*},\tau^{*}\). We initialize (E.27) with

\[\widetilde{u}^{-1}=0_{n},\quad\widetilde{v}^{0}=F(\nu^{*}\Sigma^{ -1}v^{*}+\tau^{*}\Sigma^{-1/2}w),\]

where \(w\sim\mathcal{N}(0_{d},I_{d})\) is independent of everything else. Accordingly, one can take \(f_{0}\) in Assumption (A1) to be \(f_{0}(v)=\nu^{*}F\Sigma^{-1}v\). Under the above AMP initializer, the state evolution initializers in (E.3) and (E.5) specialize to

\[\mu_{0} =\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\Big{[}{V^{*}}^{ \top}\Sigma^{-1}F\Sigma^{-1}V^{*}\Big{]}\nu^{*}=\frac{\lambda}{\delta}\mathbb{ E}\Big{[}\overline{F\Sigma}^{-2}\Big{]}\nu^{*}=\mu^{*},\] \[\sigma_{0}^{2} =\operatorname*{p-lim}_{n\to\infty}\frac{{\nu^{*}}^{2}}{n}{v^{*} }^{\top}\Sigma^{-1}F\Sigma^{-1}F\Sigma^{-1}v^{*}+\frac{{\tau^{*}}^{2}}{n}w^{ \top}\Sigma^{-1/2}F\Sigma^{-1}F\Sigma^{-1/2}w\] \[=\frac{1}{\delta}\mathbb{E}\Big{[}\overline{F}^{2}\overline{ \Sigma}^{-3}\Big{]}{\nu^{*}}^{2}+\frac{1}{\delta}\mathbb{E}\Big{[}\overline{ F}^{2}\overline{\Sigma}^{-2}\Big{]}{\tau^{*}}^{2}={\sigma^{*}}^{2},\]

where the last equalities for both chains of computation are by (E.11). Since the parameters \(\mu_{t},\sigma_{t}\) are initialized at the non-trivial fixed point \((\mu_{0},\sigma_{0})=(\mu^{*},\sigma^{*})\), the state evolution recursion (E.11) will stay at the fixed point \((\mu_{t},\sigma_{t},\nu_{t+1},\tau_{t+1})=(\mu^{*},\sigma^{*},\nu^{*},\tau^{*})\) across all \(t\geq 0\).

**Lemma E.4**.: _Let_

\[\widehat{u}^{t}\coloneqq\widetilde{G}^{1/2}Gu^{t},\quad\widehat{ v}^{t+1}\coloneqq\widetilde{F}^{1/2}Fv^{t+1},\] (E.28)

_where \(\widetilde{F},\widetilde{G}\) are defined in (E.21). Suppose the condition (5.1) holds. Then_

\[\lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{|\langle \widehat{u}^{t},u_{1}(A^{*})\rangle|}{\left\|\widehat{u}^{t}\right\|_{2}}= \lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{\left|\left\langle \widehat{v}^{t+1},v_{1}(A^{*})\right\rangle\right|}{\left\|\widehat{v}^{t+1} \right\|_{2}}=1\] (E.29)

_and_

\[\operatorname*{p-lim}_{n\to\infty}\sigma_{1}(A^{*})=1,\] (E.30)

_where \(A^{*}\) is defined in (E.22)._

Proof.: Denoting

\[e_{1}^{t}\coloneqq u^{t}-u^{t-1},\quad e_{2}^{t+1}\coloneqq v^{t+1}-v^{t},\] (E.31)

for any \(t\geq 1\) and using the notation \(\widehat{F},\widehat{G}\) in (E.20), we have from (E.27) that

\[\widehat{G}u^{t}=\Xi^{-1}A\Sigma^{-1}Fv^{t}+b^{*}\Xi^{-1}Ge_{1}^{t}+(b^{*}-b) \Xi^{-1}Gu^{t-1},\]\[\widehat{F}v^{t+1}=\Sigma^{-1}A^{\top}\Xi^{-1}Gu^{t}+c^{*}\Sigma^{-1}Fe_{2}^{t+1} +(c^{*}-c)\Sigma^{-1}Fv^{t}.\]

Recalling the notation \(\widetilde{F},\widetilde{G}\) from (E.21) and multiplying \(\widetilde{G}^{-1/2}\) (resp. \(\widetilde{F}^{-1/2}\)) on both sides of the first (resp. second) equation above, we arrive at

\[\widetilde{G}^{1/2}Gu^{t} =A^{*}\cdot\widetilde{F}^{1/2}Fv^{t}+b^{*}\widetilde{G}^{-1/2} \Xi^{-1}Ge_{1}^{t}+(b^{*}-b)\widetilde{G}^{-1/2}\Xi^{-1}Gu^{t-1},\] \[\widetilde{F}^{1/2}Fv^{t+1} =A^{*\top}\cdot\widetilde{G}^{1/2}Gu^{t}+c^{*}\widetilde{F}^{-1/ 2}\Sigma^{-1}Fe_{2}^{t+1}+(c^{*}-c)\widetilde{F}^{-1/2}\Sigma^{-1}Fv^{t}.\]

Using the definition of \(\widehat{u}^{t},\widehat{v}^{t+1}\) in (E.28), we rewrite the above as

\[\widehat{u}^{t} =A^{*}\widehat{v}^{t}+e_{u}^{t},\quad\widehat{v}^{t+1}=A^{*\top }\widehat{u}^{t}+e_{v}^{t+1},\] (E.32)

where

\[e_{u}^{t} \coloneqq b\widetilde{G}^{-1/2}\Xi^{-1}Ge_{1}^{t}+(b^{*}-b) \widetilde{G}^{-1/2}\Xi^{-1}Gu^{t-1},\] (E.33) \[e_{v}^{t+1} \coloneqq c\widetilde{F}^{-1/2}\Sigma^{-1}Fe_{2}^{t+1}+(c^{*}-c) \widetilde{F}^{-1/2}\Sigma^{-1}Fv^{t}.\]

Let us focus on \(\widehat{u}^{t}\) and only prove the first equality in (E.29). The proof of the second one is similar and will be omitted. Eliminating \(\widehat{v}^{t}\) from (E.32) gives

\[\widehat{u}^{t} =A^{*}A^{*\top}\widehat{u}^{t-1}+A^{*}e_{v}^{t}+e_{u}^{t}.\]

Unrolling this recursion for \(s\) steps, we get:

\[\widehat{u}^{t+s} =\Big{(}A^{*}A^{*\top}\Big{)}^{s}\widehat{u}^{t}+\widehat{e}_{u} ^{t,s},\] (E.34)

where

\[\widehat{e}_{u}^{t,s} \coloneqq\sum_{r=1}^{s}\Bigl{(}A^{*}A^{*\top}\Big{)}^{s-r}(A^{*} e_{v}^{t+r}+e_{u}^{t+r}).\] (E.35)

Taking \(\frac{1}{n}\|\cdot\|_{2}^{2}\) on both sides of (E.34) and take the sequential limits of \(n\to\infty\), \(t\to\infty\), \(s\to\infty\), we have the left hand side:

\[\lim_{s\to\infty}\lim_{t\to\infty}\operatorname*{p-lim}_{n\to \infty}\frac{1}{n}\bigl{\|}\widehat{u}^{t+s}\bigr{\|}_{2}^{2} =\lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{1}{n} \bigl{\|}\widehat{u}^{t}\bigr{\|}_{2}^{2}\] \[=\lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{1}{n} \bigl{\|}\widetilde{G}^{1/2}Gu^{t}\bigr{\|}_{2}^{2}\] \[=\lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{1}{n} \bigl{\|}\widehat{G}^{1/2}G^{1/2}u^{t}\bigr{\|}_{2}^{2}\] \[=\mu^{*2}\mathbb{E}\Bigl{[}\Xi^{-2}(1+b^{*}\Xi^{-1}\overline{G} )\overline{G}\Bigr{]}+\frac{\mu^{*}}{\lambda}\mathbb{E}\Bigl{[}\Xi^{-1}(1+b^{ *}\Xi^{-1}\overline{G})\overline{G}\Bigr{]}\] \[=\lambda\mu^{*2}\mathbb{E}\Biggl{[}\frac{\lambda(\mu^{*}+b^{*})+ \overline{\Xi}}{\Xi\bigl{(}\lambda\mu^{*}+\overline{\Xi}\bigr{)}^{2}}\Biggr{]} +\mu^{*}\mathbb{E}\Biggl{[}\frac{\lambda(\mu^{*}+b^{*})+\overline{\Xi}}{\bigl{(} \lambda\mu^{*}+\overline{\Xi}\bigr{)}^{2}}\Biggr{]}\in(0,\infty).\] (E.36)

Next, we have that

\[\lim_{t\to\infty}\operatorname*{p-lim}_{n\to\infty}\frac{1}{n} \bigl{\|}e_{1}^{t}\bigr{\|}_{2}^{2}=\lim_{t\to\infty}\operatorname*{p-lim}_{d \to\infty}\frac{1}{d}\bigl{\|}e_{2}^{t+1}\bigr{\|}_{2}^{2}=0.\] (E.37)

To prove the first statement on \(e_{1}^{t}\), the strategy is to write the LHS of (E.37) in terms of the state evolution parameters and prove that the latter quantities converge. We start with

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{n}\bigl{\|}u^{t}-u^{t-1} \bigr{\|}_{2}^{2} =\Bigl{(}\mu_{t}^{2}\mathbb{E}\Bigl{[}\overline{\Sigma}^{-2} \Bigr{]}+\sigma_{t}^{2}\mathbb{E}\Bigl{[}\overline{\Sigma}^{-1}\Bigr{]}\Bigr{)} +\Bigl{(}\mu_{t-1}^{2}\mathbb{E}\Bigl{[}\overline{\Sigma}^{-2}\Bigr{]}+\sigma_ {t-1}^{2}\mathbb{E}\Bigl{[}\overline{\Sigma}^{-1}\Bigr{]}\Bigr{)}\] \[\quad-2\Bigl{(}\mu_{t}\mu_{t-1}\mathbb{E}\Bigl{[}\overline{\Sigma }^{-2}\Bigr{]}+\Phi_{t,t-1}\mathbb{E}\Bigl{[}\overline{\Sigma}^{-1}\Bigr{]} \Bigr{)}\]\[=2{\sigma^{*}}^{2}\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]}-2 \Phi_{t,t-1}\mathbb{E}\Big{[}\overline{\Sigma}^{-1}\Big{]},\]

where the first equality is by Proposition G.3 and the joint distribution of \(W_{U,t},W_{U,t-1}\) in (E.2), and the second one holds since the state evolution parameters stay at the fixed point upon initialization. So it remains to verify that \(\Phi_{t,t-1}\to{\sigma^{*}}^{2}\) as \(t\to\infty\). To see this, note that according to the state evolution recursion (E.3),

\[\Phi_{t,t-1} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\big{[}V_{t}^{\top}F^{\top }\Sigma^{-1}FV_{t-1}\big{]}=\frac{{\nu^{*}}^{2}}{\delta}\mathbb{E}\Big{[} \overline{F}^{2}\overline{\Sigma}^{-3}\Big{]}+\frac{\Psi_{t,t-1}}{\delta} \mathbb{E}\Big{[}\overline{F}^{2}\overline{\Sigma}^{-2}\Big{]},\] \[\Psi_{t+1,t} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\big{[}U_{t}^{\top}G^{ \top}\Xi^{-1}GU_{t-1}\big{]}={\mu^{*}}^{2}\mathbb{E}\Big{[}\overline{G}^{2} \overline{\Xi}^{-3}\Big{]}+\Phi_{t,t-1}\mathbb{E}\Big{[}\overline{G}^{2} \overline{\Xi}^{-2}\Big{]}.\]

Eliminating \(\Psi_{t,t-1}\) from the first equation, we arrive at

\[\Phi_{t,t-1} =\frac{{\nu^{*}}^{2}}{\delta}\mathbb{E}\Big{[}\overline{F}^{2} \overline{\Sigma}^{-3}\Big{]}+\frac{1}{\delta}\mathbb{E}\Big{[}\overline{F}^{ 2}\overline{\Sigma}^{-2}\Big{]}\Big{(}{\mu^{*}}^{2}\mathbb{E}\Big{[}\overline {G}^{2}\overline{\Xi}^{-3}\Big{]}+\Phi_{t-1,t-2}\mathbb{E}\Big{[}\overline{ G}^{2}\overline{\Xi}^{-2}\Big{]}\Big{)}\] \[=\frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\lambda^{2}\overline{ \Sigma}^{-3}}{(\lambda\nu^{*}\overline{\Sigma}^{-1}+1)^{2}}\Bigg{]}{\nu^{*}}^{ 2}+\frac{1}{\delta}\mathbb{E}\Bigg{[}\frac{\lambda^{2}\overline{\Sigma}^{-2}} {(\lambda\nu^{*}\overline{\Sigma}^{-1}+1)^{2}}\Bigg{]}\]

whose only fixed point is \({\sigma^{*}}^{2}\) by the relations in (E.11). This concludes the proof of the first equality in (E.37). The proof of the second equality is analogous and, hence, omitted.

By using (E.37) and the fact that \(b\to b^{*},c\to c^{*}\) as \(n\to\infty\) (see (E.17)), we obtain

\[\lim_{t\to\infty}\underset{n\to\infty}{\text{p-lim}}\frac{1}{n}\big{\|}e_{u}^ {t}\big{\|}_{2}^{2}=\lim_{t\to\infty}\underset{d\to\infty}{\text{p-lim}}\frac {1}{d}\big{\|}e_{v}^{t+1}\big{\|}_{2}^{2}=0.\] (E.38)

Note that the operator norm of \(A^{*}\) is almost surely bounded uniformly in \(n\) by Weyl's inequality, sub-multiplicativity of matrix norms and the Bai-Yin law [3]. This together with the triangle inequality of the \(\ell_{2}\)-norm and (E.38) implies that

\[\lim_{s\to\infty}\lim_{t\to\infty}\underset{n\to\infty}{\text{p-lim}}\frac{1} {n}\big{\|}\widehat{e}_{u}^{t,s}\big{\|}_{2}^{2}=0,\] (E.39)

From this, it follows that the right hand side of (E.34) (upon taken the rescaled squared norm and the sequential limits) equals

\[\lim_{s\to\infty}\lim_{t\to\infty}\underset{n\to\infty}{\text{p-lim}}\frac{1} {n}\Big{\|}\Big{(}{A^{*}}{A^{*}}^{\top}\Big{)}^{s}\widehat{u}^{t}\Big{\|}_{2}^ {2}.\]

We then compute the above term by taking the SVD of \(A^{*}\). Define two spectral projectors that are orthogonal to each other:

\[\Pi\coloneqq u_{1}(A^{*})u_{1}(A^{*})^{\top},\quad\Pi^{\perp}\coloneqq I_{n} -\Pi.\]

We have

\[\frac{1}{n}\Big{\|}\Big{(}{A^{*}}{A^{*}}^{\top}\Big{)}^{s}\widehat{u}^{t} \Big{\|}_{2}^{2}=\frac{1}{n}\Big{\|}\Big{(}{A^{*}}{A^{*}}^{\top}\Big{)}^{s}\Pi \widehat{u}^{t}\Big{\|}_{2}^{2}+\frac{1}{n}\Big{\|}\Big{(}{A^{*}}{A^{*}}^{\top }\Big{)}^{s}\Pi^{\perp}\widehat{u}^{t}\Big{\|}_{2}^{2}.\] (E.40)

Using the spectral decomposition

\[\Big{(}{A^{*}}{A^{*}}^{\top}\Big{)}^{s}=\sum_{i=1}^{n}\sigma_{i}(A^{*})^{2s}u_ {i}(A^{*})u_{i}(A^{*})^{\top},\]

we can write the first term in (E.40) as

\[\frac{1}{n}\Big{\|}\Big{(}{A^{*}}{A^{*}}^{\top}\Big{)}^{s}\Pi\widehat{u}^{t} \Big{\|}_{2}^{2}=\frac{1}{n}\bigg{\|}\sum_{i=1}^{n}\sigma_{i}(A^{*})^{2s}u_{i}(A ^{*})u_{i}(A^{*})^{\top}u_{1}(A^{*})u_{1}(A^{*})^{\top}\widehat{u}^{t}\bigg{\|} _{2}^{2}=\sigma_{1}(A^{*})^{4s}\frac{\langle u_{1}(A^{*}),\widehat{u}^{t} \rangle^{2}}{n}.\] (E.41)For the second term in (E.40), we have

\[\frac{1}{n}\Big{\|}\Big{(}{A^{*}{A^{*}}^{\top}}\Big{)}^{s}\Pi^{\perp} \widehat{u}^{t}\Big{\|}_{2}^{2} =\frac{1}{n}\Big{\|}\Big{(}{A^{*}{A^{*}}^{\top}}\Pi^{\perp}\Big{)}^{ s}\widehat{u}^{t}\Big{\|}_{2}^{2}\] \[\leq\frac{\|\widehat{u}^{t}\|_{2}^{2}}{n}\max_{u\in\mathbb{S}^{n- }}\Big{\|}\Big{(}{A^{*}{A^{*}}^{\top}}\Pi^{\perp}\Big{)}^{s}u\Big{\|}_{2}^{2}\] \[=\frac{\|\widehat{u}^{t}\|_{2}^{2}}{n}\sigma_{1}\Big{(}\Big{(}{A^ {*}{A^{*}}^{\top}}\Pi^{\perp}\Big{)}^{s}\Big{)}^{2}\] \[=\frac{\|\widehat{u}^{t}\|_{2}^{2}}{n}\sigma_{1}\Big{(}{A^{*}{A^{ *}}^{\top}}\Pi^{\perp}\Big{)}^{2s}\] \[=\frac{\|\widehat{u}^{t}\|_{2}^{2}}{n}\sigma_{2}\Big{(}{A^{*}{A^{ *}}^{\top}}\Big{)}^{2s}\] \[=\frac{\|\widehat{u}^{t}\|_{2}^{2}}{n}\sigma_{2}({A^{*}})^{4s},\]

where penultimate line follows since

\[{A^{*}{A^{*}}^{\top}}\Pi^{\perp}=\sum_{i=2}^{n}\sigma_{i}({A^{*}})^{2}u_{i}({A^ {*}})u_{i}({A^{*}})^{\top}.\]

From Lemma E.3 and the assumption that \(\sigma_{2}^{*}<1\), we know

\[\underset{n\to\infty}{\text{\rm p-lim}}\sigma_{2}({A^{*}})=\sigma_{2}^{*}<1.\]

This implies:

\[\lim_{s\to\infty}\lim_{t\to\infty}\underset{n\to\infty}{\text{\rm p -limsup}}\frac{1}{n}\Big{\|}\Big{(}{A^{*}{A^{*}}^{\top}}\Big{)}^{s}\Pi^{\perp} \widehat{u}^{t}\Big{\|}_{2}^{2}\leq\lim_{s\to\infty}\lim_{t\to\infty}\underset {n\to\infty}{\text{\rm p-limsup}}\frac{\|\widehat{u}^{t}\|_{2}^{2}}{n}\sigma _{2}({A^{*}})^{4s}\\ \leq\left(\lim_{t\to\infty}\text{\rm p-limsup}_{n\to\infty}\frac{ \|\widehat{u}^{t}\|_{2}^{2}}{n}\right)\!\!\left(\lim_{s\to\infty}\text{\rm p-limsup}_{n\to\infty}\sigma_{2}({A^{*}})^{4s} \right)=0,\] (E.42)

where the last equality holds since the limit in the first parentheses is finite by (E.36).

Now (E.40) to (E.42) jointly show

\[\lim_{s\to\infty}\lim_{t\to\infty}\underset{n\to\infty}{\text{\rm p -lim}}\frac{1}{n}\Big{\|}\Big{(}{A^{*}{A^{*}}^{\top}}\Big{)}^{s}\widehat{u}^{ t}\Big{\|}_{2}^{2} =\lim_{s\to\infty}\lim_{t\to\infty}\underset{n\to\infty}{\text{\rm p -lim}}\sigma_{1}({A^{*}})^{4s}\frac{\langle u_{1}({A^{*}}),\widehat{u}^{t} \rangle^{2}}{n}\] \[=\left(\lim_{s\to\infty}\underset{n\to\infty}{\text{\rm p-lim}} \sigma_{1}({A^{*}})^{4s}\right)\!\left(\lim_{t\to\infty}\underset{n\to\infty} {\text{\rm p-lim}}\frac{\langle u_{1}({A^{*}}),\widehat{u}^{t}\rangle^{2}}{n} \right)\!.\]

Combining this with (E.36) brings us to the following identity:

\[1=\left(\lim_{s\to\infty}\underset{n\to\infty}{\text{\rm p-lim}}\sigma_{1}({A ^{*}})^{4s}\right)\!\left(\lim_{t\to\infty}\underset{n\to\infty}{\text{\rm p-lim }}\frac{\langle u_{1}({A^{*}}),\widehat{u}^{t}\rangle^{2}}{\|\widehat{u}^{t} \|_{2}^{2}}\right)\!,\]

which necessarily implies

\[\underset{n\to\infty}{\text{\rm p-lim}}\sigma_{1}({A^{*}})=1,\qquad\lim_{t\to \infty}\underset{n\to\infty}{\text{\rm p-lim}}\frac{\langle u_{1}({A^{*}}), \widehat{u}^{t}\rangle^{2}}{\|\widehat{u}^{t}\|_{2}^{2}}=1,\]

as desired. 

With Lemma E.4, we can complete the proof of Theorem 5.1.

Proof of Theorem 5.1.: The characterization (5.7) of the top two singular values have been obtained in Lemmas E.3 and E.4. It remains to compute the overlaps which can be done using Lemma E.4 and the state evolution (Proposition E.1). Recall the estimators \(\widehat{u},\widehat{v}\) in (5.4) and their heuristic derivation in (E.23). Then

\[\operatorname*{p\!-\!lim}_{n\to\infty}\frac{\left\langle\widehat{u},u^{*}\right\rangle ^{2}}{\left\|\widehat{u}\right\|_{2}^{2}\left\|u^{*}\right\|_{2}^{2}}= \operatorname*{p\!-\!lim}_{t\to\infty}\frac{\left\langle\Xi u^{t},u^{*} \right\rangle^{2}}{\left\|\Xi u^{t}\right\|_{2}^{2}\left\|u^{*}\right\|_{2}^{2 }}=\frac{\lambda\mu^{*}}{\lambda\mu^{*}+1}=\eta_{u}^{2},\]

establishing the first equality in (5.8). The second equality in (5.8) and other quantities in (5.9) and (5.10) can be similarly obtained. The proof is completed. 

## Appendix F Proof of Proposition E.1

Recall \(\widetilde{u}^{*},\widetilde{v}^{*}\) from (C.7) and let

\[\widetilde{A}\coloneqq\Xi^{-1/2}A\Sigma^{-1/2}=\frac{\lambda}{n}\widetilde{ u}^{*}(\widetilde{v}^{*})^{\top}+\widetilde{W}.\] (F.1)

### Auxiliary AMP and its state evolution

For \((\tilde{g}_{t}\colon\mathbb{R}^{n}\to\mathbb{R}^{n},\tilde{f}_{t+1}\colon \mathbb{R}^{d}\to\mathbb{R}^{d})_{t\geq 0}\), the iterates of the auxiliary AMP, initialized at \(\hat{u}^{-1}=0_{d}\) and some \(\hat{v}^{0}\in\mathbb{R}^{d}\), are updated according to the following rules for every \(t\geq 0\):

\[\begin{split}\tilde{u}^{t}&=\widetilde{A}\hat{v}^{ t}-\tilde{b}_{t}\hat{u}^{t-1},\quad\hat{u}^{t}=\tilde{g}_{t}(\tilde{u}^{t}), \quad\tilde{c}_{t}=\frac{1}{n}\sum_{i=1}^{n}\partial_{i}\tilde{g}_{t}(\tilde{ u}^{t})_{i},\\ \tilde{v}^{t+1}&=\widetilde{A}^{\top}\hat{u}^{t}- \tilde{c}_{t}\hat{v}^{t},\quad\hat{v}^{t+1}=\tilde{f}_{t+1}(\hat{v}^{t+1}), \quad\tilde{b}_{t+1}=\frac{1}{n}\sum_{i=1}^{d}\partial_{i}\tilde{f}_{t+1}( \hat{v}^{t+1})_{i}.\end{split}\] (F.2)

The state evolution result associated with the above auxiliary AMP iteration asserts that the distributions of \((\tilde{u}^{0},\tilde{u}^{1},\cdots,\tilde{u}^{t}),(\tilde{v}^{1},\tilde{v}^ {2},\cdots,\hat{v}^{t+1})\) converge (in the sense of (F.16)) respectively to the laws of the random vectors \((\tilde{U}_{0},\tilde{U}_{1},\cdots,\tilde{U}_{t}),(\tilde{V}_{1},\tilde{V}_ {2},\cdots\tilde{V}_{t+1})\) defined below:

\[\tilde{U}_{t}=\tilde{\mu}_{t}\widetilde{U}^{*}+\bar{\sigma}_{t}\tilde{W}_{U,t }\in\mathbb{R}^{n},\quad\tilde{V}_{t+1}=\tilde{\nu}_{t+1}\widetilde{V}^{*}+ \tilde{\tau}_{t+1}\tilde{W}_{V,t+1}\in\mathbb{R}^{d},\] (F.3)

where

\[\begin{bmatrix}U^{*}\\ \bar{\sigma}_{0}\tilde{W}_{U,0}\\ \bar{\sigma}_{1}\tilde{W}_{U,1}\\ \vdots\\ \bar{\sigma}_{t}\tilde{W}_{U,t}\end{bmatrix}\sim P^{\otimes n}\otimes\mathcal{ N}(0_{n(t+1)},\tilde{\Phi}_{t}\otimes I_{n}),\quad\begin{bmatrix}V^{*}\\ \bar{\tau}_{1}\tilde{W}_{V,1}\\ \bar{\tau}_{2}\tilde{W}_{V,2}\\ \vdots\\ \bar{\tau}_{t+1}\tilde{W}_{V,t+1}\end{bmatrix}\sim Q^{\otimes d}\otimes \mathcal{N}(0_{d(t+1)},\tilde{\Psi}_{t+1}\otimes I_{d}),\] (F.4)

\[\widetilde{U}^{*}\coloneqq\Xi^{-1/2}U^{*}\in\mathbb{R}^{n},\qquad\qquad\qquad \qquad\qquad\widetilde{V}^{*}\coloneqq\Sigma^{-1/2}V^{*}\in\mathbb{R}^{d}.\] (F.5)

The parameters \(\tilde{\mu}_{t},\tilde{\nu}_{t+1}\in\mathbb{R},\tilde{\Phi}_{t}=(\tilde{\Phi}_ {r,s})_{0\leq r,s\leq t},\tilde{\Psi}_{t+1}=(\tilde{\Psi}_{r+1,s+1})_{0\leq r,s \leq t}\in\mathbb{R}^{(t+1)\times(t+1)}\) are defined recursively through the following state evolution equations:

\[\tilde{\mu}_{0} =\lambda\lim_{d\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle} \widetilde{V}^{*},\tilde{f}_{0}(\widetilde{V}^{*})\Big{\rangle}\Big{]},\] (F.6) \[\tilde{\mu}_{t} =\lambda\lim_{d\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle} \widetilde{V}^{*},\tilde{f}_{t}(\tilde{V}_{t})\Big{\rangle}\Big{]},\quad t\geq 1,\] (F.7) \[\tilde{\nu}_{t+1} =\lambda\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle} \widetilde{U}^{*},\tilde{g}_{t}(\tilde{U}_{t})\Big{\rangle}\Big{]},\quad t\geq 0,\] (F.8) \[\tilde{\Phi}_{0,0} =\operatorname*{p\!-\!lim}_{n\to\infty}\frac{1}{n}\big{\|}\hat{ v}^{0}\big{\|}_{2}^{2},\] (F.9) \[\tilde{\Phi}_{0,s} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle}\tilde {f}_{0}(\widetilde{V}^{*}),\breve{f}_{s}(\tilde{V}_{s})\Big{\rangle}\Big{]}, \quad 1\leq s\leq t,\] (F.10) \[\tilde{\Phi}_{r,s} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle}\tilde {f}_{r}(\tilde{V}_{r}),\breve{f}_{s}(\tilde{V}_{s})\Big{\rangle}\Big{]}, \quad 1\leq r,s\leq t,\] (F.11)\[\tilde{\Psi}_{r+1,s+1}=\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle} \tilde{g}_{r}(\tilde{U}_{r}),\tilde{g}_{s}(\tilde{U}_{s})\Big{\rangle}\Big{]}, \quad 0\leq r,s\leq t,\] (F.12)

where \(\tilde{f}_{0}\) is determined by \(\hat{v}^{0}\) through Assumption (A4) below. In particular,

\[\tilde{\sigma}_{0}^{2} =\operatorname*{\text{\rm p-lim}}_{n\to\infty}\frac{1}{n}\big{\|} \hat{v}^{0}\big{\|}_{2}^{2},\] (F.13) \[\tilde{\sigma}_{t}^{2} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle} \tilde{f}_{t}(\tilde{V}_{t}),\tilde{f}_{t}(\tilde{V}_{t})\Big{\rangle}\Big{]}, \quad t\geq 1,\] (F.14) \[\tilde{\tau}_{t+1}^{2} =\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\Big{[}\Big{\langle} \tilde{g}_{t}(\tilde{U}_{t}),\tilde{g}_{t}(\tilde{U}_{t})\Big{\rangle}\Big{]},\quad t\geq 0.\] (F.15)

We require the following assumptions to guarantee the existence and finiteness of the state evolution parameters defined above.

* \(\hat{v}^{0}\) is independent of \(\widetilde{W}\) but may depend on \(\widetilde{v}^{*}\). Assume that \[\operatorname*{\text{\rm p-lim}}_{d\to\infty}\frac{1}{d}\big{\|}\hat{v}^{0} \big{\|}_{2}^{2}\] exists and is finite. There exists a uniformly pseudo-Lipschitz function \(\tilde{f}_{0}\colon\mathbb{R}^{d}\to\mathbb{R}^{d}\) of order \(1\) such that \[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}\Big{\langle}\tilde{f}_{0}( \widetilde{V}^{*}),\tilde{f}_{0}(\widetilde{V}^{*})\Big{\rangle}\Big{]}\leq \operatorname*{\text{\rm p-lim}}_{d\to\infty}\frac{1}{d}\big{\|}\hat{v}^{0} \big{\|}_{2}^{2}\] and for every uniformly pseudo-Lipschitz function \(\phi\colon\mathbb{R}^{d}\to\mathbb{R}^{d}\) of finite order, the following two limits exist, are finite and equal: \[\operatorname*{\text{\rm p-lim}}_{d\to\infty}\frac{1}{d}\big{\langle}\hat{v}^ {0},\phi(\widetilde{v}^{*})\big{\rangle}=\lim_{d\to\infty}\frac{1}{d}\mathbb{E} \Big{[}\Big{\langle}\tilde{f}_{0}(\widetilde{V}^{*}),\phi(\widetilde{V}^{*}) \Big{\rangle}\Big{]}.\] Let \(\widetilde{\nu}\in\mathbb{R},\widetilde{\tau}\in\mathbb{R}_{\geq 0}\). For any \(s\geq 1\), \[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}\tilde{f}_{0}(\widetilde{V}^{*})^ {\top}\tilde{f}_{s}(\widetilde{\nu}\widetilde{V}^{*}+\widetilde{\tau}W_{V}) \Big{]}\] exists and is finite, where \(W_{V}\sim\mathcal{N}(0_{d},I_{d})\) is independent of \(\widetilde{V}^{*}\).
* Let \(\widetilde{\nu}\in\mathbb{R}\), and \(T\in\mathbb{R}^{2\times 2}\) be positive definite. For any \(r,s\geq 1\), \[\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\Big{[}\Big{\langle}\widetilde{V}^{ *},\tilde{f}_{r}(\widetilde{\nu}\widetilde{V}^{*}+N)\Big{\rangle}\Big{]},\quad \lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}\tilde{f}_{r}(\widetilde{\nu} \widetilde{V}^{*}+N)^{\top}\tilde{f}_{s}(\widetilde{\nu}\widetilde{V}^{*}+N^ {\prime})\Big{]}\] exist and are finite, where \((N,N^{\prime})\sim\mathcal{N}(0_{2d},T\otimes I_{d})\) is independent of \(\widetilde{v}^{*}\). Let \(\widetilde{\mu}\in\mathbb{R}\), and \(S\in\mathbb{R}^{2\times 2}\) be positive definite. For any \(r,s\geq 0\), \[\lim_{n\to\infty}\frac{\lambda}{n}\mathbb{E}\Big{[}\Big{\langle}\widetilde{U}^ {*},\tilde{g}_{r}(\widetilde{\mu}\widetilde{U}^{*}+M)\Big{\rangle}\Big{]}, \quad\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\Big{[}\tilde{g}_{r}(\widetilde{\mu }\widetilde{U}^{*}+M)^{\top}\tilde{g}_{s}(\widetilde{\mu}\widetilde{U}^{*}+M^ {\prime})\Big{]}\] exist and are finite, where \((M,M^{\prime})\sim\mathcal{N}(0_{2n},S\otimes I_{n})\) is independent of \(\widetilde{U}^{*}\).

**Proposition F.1** (State evolution for auxiliary AMP (F.2)).: _For every \(t\geq 0\), let \((\tilde{g}_{t}\colon\mathbb{R}^{n}\to\mathbb{R}^{n})_{n\geq 1}\) and \((\tilde{f}_{t+1}\colon\mathbb{R}^{d}\to\mathbb{R}^{d})_{d\geq 1}\) be uniformly pseudo-Lipschitz of finite order subject to Assumption (A5). Consider the auxiliary AMP iteration in (F.2) defined by \((\tilde{g}_{t},\tilde{f}_{t+1})_{t\geq 0}\) and initialized at \(\tilde{u}^{-1}=0_{n}\) and some \(\hat{v}^{0}\in\mathbb{R}^{d}\) subject to Assumption (A4). For any fixed \(t\geq 0\), let \((\phi\colon\mathbb{R}^{(t+2)n}\to\mathbb{R})_{n\geq 1}\) and \((\psi\colon\mathbb{R}^{(t+2)d}\to\mathbb{R})_{n\geq 1}\) be uniformly pseudo-Lipschitz functions of finite order. Then,_

\[\operatorname*{\text{\rm p-lim}}_{n\to\infty}\phi(\widetilde{u}^{*}, \tilde{u}^{0},\tilde{u}^{1},\cdots,\tilde{u}^{t})-\mathbb{E}\Big{[}\phi( \widetilde{U}^{*},\tilde{U}_{0},\tilde{U}_{1},\cdots,\tilde{U}_{t})\Big{]} =0,\] (F.16a) \[\operatorname*{\text{\rm p-lim}}_{d\to\infty}\psi(\widetilde{v}^{*}, \tilde{v}^{1},\tilde{v}^{2},\cdots,\hat{v}^{t+1})-\mathbb{E}\Big{[}\psi( \widetilde{V}^{*},\tilde{V}_{1},\tilde{V}_{2},\cdots,\tilde{V}_{t+1})\Big{]} =0,\] (F.16b)

_where \((\tilde{U}_{s},\tilde{V}_{s+1})_{0\leq s\leq t}\) are defined in (F.3)._

[MISSING_PAGE_FAIL:46]

\[\times\left[1+\left(\frac{1}{\sqrt{n}}\|u^{*}\|_{2}+\sum_{s=0}^{t} \frac{1}{\sqrt{n}}\|p^{s}+\hat{\mu}_{s}\widetilde{u}^{*}\|_{2}\right)^{k-1}+ \left(\frac{1}{\sqrt{n}}\|u^{*}\|_{2}+\sum_{s=0}^{t}\frac{1}{\sqrt{n}}\|\tilde{ u}^{s}\|_{2}\right)^{k-1}\right]\] \[\leq L^{\prime}\!\left(\sum_{s=0}^{t}\frac{1}{\sqrt{n}}\|p^{s}+ \tilde{\mu}_{s}\widetilde{u}^{*}-\tilde{u}^{s}\|_{2}\right)\] \[\times\left[1+\left(\frac{1}{\sqrt{n}}\|u^{*}\|_{2}\right)^{k-1}+ \sum_{s=0}^{t}\!\left(\frac{1}{\sqrt{n}}\|p^{s}+\tilde{\mu}_{s}\widetilde{u}^{ *}\|_{2}\right)^{k-1}+\sum_{s=0}^{t}\!\left(\frac{1}{\sqrt{n}}\|\tilde{u}^{s} \|_{2}\right)^{k-1}\right]\!,\]

for some \(L^{\prime}\) depending only on \(t,k,L\). Similar manipulation gives

\[\big{|}\psi(v^{*},q^{1}+\tilde{\nu}_{1}\widetilde{v}^{*},\cdots, q^{t+1}+\tilde{\nu}_{t+1}\widetilde{v}^{*})-\psi(v^{*},\tilde{v}^{1},\cdots, \tilde{v}^{t+1})\big{|}\] \[\leq L^{\prime}\!\left(\sum_{s=1}^{t+1}\frac{1}{\sqrt{d}}\|q^{s} +\tilde{\nu}_{s}\widetilde{v}^{*}-\tilde{v}^{s}\|_{2}\right)\] \[\times\left[1+\left(\frac{1}{\sqrt{d}}\|v^{*}\|_{2}\right)^{k-1}+ \sum_{s=1}^{t+1}\!\left(\frac{1}{\sqrt{d}}\|q^{s}+\tilde{\nu}_{s}\widetilde{v} ^{*}\|_{2}\right)^{k-1}+\sum_{s=1}^{t+1}\!\left(\frac{1}{\sqrt{d}}\|\tilde{v}^ {s}\|_{2}\right)^{k-1}\right]\!.\]

Clearly, (F.22) holds if for every \(t\geq 0\),

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{n}}\big{\|}p^{t} +\tilde{\mu}_{t}\widetilde{u}^{*}\big{\|}_{2} <\infty,\] (F.23) \[\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{n}}\big{\|} \tilde{u}^{t}\big{\|}_{2} <\infty,\] (F.24) \[\operatorname*{p-lim}_{n\to\infty}\frac{1}{n}\big{\|}\tilde{u}^{ t}-\left(p^{t}+\tilde{\mu}_{t}\widetilde{u}^{*}\right)\big{\|}_{2}^{2} =0,\] (F.25) \[\operatorname*{p-lim}_{d\to\infty}\frac{1}{\sqrt{d}}\big{\|}q^{t+ 1}+\tilde{\nu}_{t+1}\widetilde{v}^{*}\big{\|}_{2} <\infty,\] (F.26) \[\operatorname*{p-lim}_{d\to\infty}\frac{1}{\sqrt{d}}\big{\|} \tilde{v}^{t+1}\big{\|}_{2} <\infty,\] (F.27) \[\operatorname*{p-lim}_{d\to\infty}\frac{1}{d}\big{\|}\tilde{v}^{ t+1}-\left(q^{t+1}+\tilde{\nu}_{t+1}\widetilde{v}^{*}\right)\big{\|}_{2}^{2} =0,\] (F.28)

which, together with the following statements

\[\tilde{\mu}_{t}<\infty,\quad\tilde{\sigma}_{t}<\infty,\] (F.29) \[\tilde{\nu}_{t+1}<\infty,\quad\tilde{\bar{\tau}}_{t+1}<\infty,\] (F.30)

will be shown in the sequel by induction on \(t\geq 0\).

Base case.Consider \(t=0\). From (F.21),

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{n}\big{\|}p^{0}+\tilde{ \mu}_{0}\widetilde{u}^{*}\big{\|}_{2}^{2}=\operatorname*{p-lim}_{n\to\infty} \frac{1}{n}\mathbb{E}\bigg{[}\Big{\|}\tilde{U}_{0}\Big{\|}_{2}^{2}\bigg{]}= \tilde{\sigma}_{0}^{2}+\tilde{\mu}_{0}^{2}\mathbb{E}\Big{[}\overline{\Xi}^{-1 }\Big{]},\] (F.31)

where the last equality is by (F.3). Due to (F.13) and Assumption (A4), both \(\tilde{\mu}_{0}\) and \(\tilde{\sigma}_{0}\) are finite, so (F.29) holds for \(t=0\). Consequently, (F.23) also holds for \(t=0\).

Since by (F.17),

\[(p^{0}+\tilde{\mu}_{0}\widetilde{u}^{*})-\tilde{u}^{0}=\tilde{\mu }_{0}\widetilde{u}^{*}-\frac{\lambda}{n}\langle\widetilde{v}^{*},\dot{v}^{0} \rangle\widetilde{u}^{*},\]

therefore (F.25) for \(t=0\) follows from (F.6) and Assumption (A4). This in turn implies, when combined with the finiteness of (F.31), that

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{n}}\big{\|}\tilde{ u}^{0}\big{\|}_{2}<\infty,\] (F.32)verifying (F.24) for \(t=0\). Since \(\tilde{g}_{0}\) is uniformly pseudo-Lipschitz of finite order, so is the function \(\frac{1}{n}\sum_{i=1}^{n}\partial_{i}\tilde{g}_{0}(\cdot)_{i}\). (F.23) to (F.25) (for \(t=0\)) together imply

\[\underset{n\to\infty}{\mathrm{p-lim}}|m_{0}|<\infty,\quad\underset{n\to\infty} {\mathrm{p-lim}}|m_{0}-\tilde{c}_{0}|=0.\] (F.33)

Using the the pseudo-Lipschitzness of \(\tilde{y}_{0}\) again,

\[\underset{n\to\infty}{\mathrm{p-lim}}\frac{1}{\sqrt{n}}\big{\|} \tilde{p}^{0}-\tilde{u}^{0}\big{\|}_{2} =\underset{n\to\infty}{\mathrm{p-lim}}\frac{1}{\sqrt{n}}\big{\|} \tilde{y}_{0}(p^{0}+\tilde{\mu}_{0}\widetilde{u}^{*})-\tilde{g}_{0}(\tilde{u} ^{0})\big{\|}_{2}\] \[\leq\underset{n\to\infty}{\mathrm{p-lim}}\frac{\big{\|}(p^{0}+ \tilde{\mu}_{0}\widetilde{u}^{*})-\tilde{u}^{0}\big{\|}_{2}}{\sqrt{n}}\left[1+ \left(\frac{\big{\|}p^{0}+\tilde{\mu}_{0}\widetilde{u}^{*}\big{\|}_{2}}{\sqrt{n }}\right)^{k-1}+\left(\frac{\big{\|}\tilde{u}^{0}\big{\|}_{2}}{\sqrt{n}} \right)^{k-1}\right]\] \[=0.\] (F.34)

The last equality holds because of (F.25) (for \(t=0\)) and the finiteness of (F.31) and (F.32).

To show (F.28) for \(t=0\), we use (F.2), (F.17) and (F.18) to write

\[(q^{1}+\tilde{\nu}_{1}\widetilde{\upsilon}^{*})-\tilde{\upsilon}^{1}=\underbrace {\widetilde{W}^{\top}(\tilde{p}^{0}-\tilde{u}^{0})}_{\tilde{T}_{1}}+ \underbrace{\left(\tilde{\nu}_{1}-\frac{\lambda}{n}\big{\langle}\tilde{u}^{0},\widetilde{u}^{*}\big{\rangle}\right)}_{\tilde{T}_{2}}\widetilde{\upsilon}^ {*}-\underbrace{\left(m_{0}-\tilde{c}_{0}\right)}_{\tilde{T}_{3}}\tilde{ \upsilon}^{0}.\]

By (F.34) and the Bai-Yin law [3],

\[\underset{d\to\infty}{\mathrm{p-lim}}\frac{1}{d}\big{\|}T_{1}\big{\|}_{2}^{2 }=0.\] (F.35)

Using (F.34) again,

\[\underset{n\to\infty}{\mathrm{p-lim}}\frac{\lambda}{n}\big{\langle} \tilde{u}^{0},\widetilde{u}^{*}\big{\rangle} =\underset{n\to\infty}{\mathrm{p-lim}}\frac{\lambda}{n}\big{\langle} \tilde{p}^{0},\widetilde{u}^{*}\big{\rangle}=\underset{n\to\infty}{\mathrm{p- lim}}\frac{\lambda}{n}\big{\langle}\tilde{g}_{0}(p^{0}+\tilde{\mu}_{0} \widetilde{u}^{*}),\widetilde{u}^{*}\big{\rangle}\] \[=\underset{n\to\infty}{\mathrm{lim}}\frac{\lambda}{n}\mathbb{E} \Big{[}\big{\langle}\tilde{g}_{0}(\tilde{U}_{0}),\widetilde{u}^{*}\big{\rangle} \Big{]}=\tilde{\nu}_{1},\] (F.36)

where in the second line, the first equality is by (F.22) and the pseudo-Lipschitzness of \(\tilde{g}_{0}\), and the second equality is by the definition (F.8). We further show the finiteness of \(\tilde{\nu}_{1}\). Note that

\[\tilde{\nu}_{1}\leq\underset{n\to\infty}{\mathrm{lim}}\lambda\mathbb{E} \bigg{[}\frac{1}{n}\Big{\|}\tilde{g}_{0}(\tilde{U}_{0})\Big{\|}_{2}^{2}\bigg{]} ^{1/2}\mathbb{E}\bigg{[}\frac{1}{n}\Big{\|}\widetilde{U}^{*}\Big{\|}_{2}^{2} \bigg{]}^{1/2}.\]

The first term can be bounded as

\[\underset{n\to\infty}{\mathrm{p-lim}}\frac{1}{n}\mathbb{E}\bigg{[}\big{\|} \tilde{g}_{0}(\tilde{U}_{0})\big{\|}_{2}^{2}\bigg{]}\leq\underset{n\to\infty}{ \mathrm{p-lim}}\,L^{\prime}\mathbb{E}\Bigg{[}\Bigg{(}1+\bigg{(}\frac{1}{\sqrt{ n}}\Big{\|}\tilde{U}_{0}\Big{\|}_{2}\bigg{)}^{k}\bigg{)}^{2}\Bigg{]}\leq \underset{n\to\infty}{\mathrm{p-lim}}\,2L^{\prime}\Bigg{(}1+\mathbb{E} \Bigg{[}\bigg{(}\frac{1}{n}\Big{\|}\tilde{U}_{0}\Big{\|}_{2}^{2}\bigg{)}^{k} \bigg{]}\Bigg{)},\] (F.37)

where the last step is the elementary inequality \((a+b)^{2}\leq 2(a^{2}+b^{2})\). The RHS above is finite since

\[\frac{1}{n}\Big{\|}\tilde{U}_{0}\Big{\|}_{2}^{2}=\frac{\tilde{\mu}_{0}^{2}}{n} \Big{\|}\widetilde{U}^{*}\Big{\|}_{2}^{2}+\frac{\tilde{\sigma}_{0}^{2}}{n} \Big{\|}\tilde{W}_{V,0}\Big{\|}_{2}^{2}\] (F.38)

whose all moments are finite by finiteness of \(\tilde{\mu}_{0},\tilde{\sigma}_{0}\). This shows the first bound in (F.30) for \(t=0\). Recalling (F.36), we then have

\[\underset{n\to\infty}{\mathrm{p-lim}}|T_{2}|=0.\] (F.39)

By (F.33),

\[\underset{n\to\infty}{\mathrm{p-lim}}|T_{3}|=0.\] (F.40)

Therefore, (F.35), (F.39) and (F.40) altogether verify (F.28) for \(t=0\).

We then show (F.26) for \(t=0\). Since \(\tilde{\nu}_{1}<\infty\), it suffices to only consider \(q^{1}\). According to (F.17) and (F.18),

\[q^{1}=\widetilde{W}^{\top}\tilde{p}^{0}-m_{0}\hat{v}^{0}.\]

Pseudo-Lipschitzness of \(\tilde{y}_{0}\), finiteness (F.23) (for \(t=0\)) and finiteness (F.33) jointly imply

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{n}}\big{\|}q^{1}\big{\|}_{2}<\infty,\]

from which (F.26) follows. This combined with (F.28) (for \(t=0\)) also implies (F.27) (for \(t=0\)).

Finally, we are left with the second inequality in (F.30). From the definition (F.15),

\[\tilde{\tau}_{1}^{2}=\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\bigg{[}\Big{\|} \tilde{y}_{0}(\tilde{U}_{0})\Big{\|}_{2}^{2}\bigg{]}\]

which has already been shown to be finite; see (F.37). So the base case is finished.

Induction step.Assume that (F.23) to (F.28) all hold up to the \(t\)-th step (for an arbitrary \(t\geq 1\)). We now show that they hold for \(t+1\). The idea is similar to the base case. We briefly lay down the key steps for (F.23) to (F.25) and (F.29), and omit the verification of (F.26) to (F.28) and (F.30).

Using (F.3) and (F.21),

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{n}\big{\|}p^{t+1}+\tilde{\mu}_{t+1 }\widetilde{u}^{*}\big{\|}_{2}^{2}=\bar{\sigma}_{t+1}^{2}+\tilde{\mu}_{t+1}^{2 }\mathbb{E}\Big{[}\overline{\Xi}^{-1}\Big{]}.\] (F.41)

Using the definition (F.14) and the pseudo-Lipschitzness of \(\tilde{f}_{t+1}\),

\[\bar{\sigma}_{t+1}^{2}=\lim_{n\to\infty}\frac{1}{n}\mathbb{E}\bigg{[}\Big{\|} \tilde{f}_{t+1}(\tilde{V}_{t+1})\Big{\|}_{2}^{2}\bigg{]}\leq\lim_{n\to\infty}2 L^{\prime}\Bigg{(}1+\mathbb{E}\Bigg{[}\bigg{(}\frac{1}{n}\Big{\|}\tilde{V}_{t+1} \Big{\|}_{2}^{2}\bigg{)}^{k}\Bigg{]}\Bigg{)},\]

for some \(L^{\prime}\) depending only on \(k,L\). The inequality is obtained in a similar way to (F.37). By induction hypothesis (F.30) and the compactness of \(\operatorname*{supp}(\overline{\Sigma})\), all moments of

\[\frac{1}{n}\Big{\|}\tilde{V}_{t+1}\Big{\|}_{2}^{2}=\frac{\bar{\nu}_{t+1}^{2}} {n}\Big{\|}\widetilde{V}^{*}\Big{\|}_{2}^{2}+\frac{\tilde{\tau}_{t+1}^{2}}{n} \Big{\|}\tilde{W}_{V,t+1}\Big{\|}_{2}^{2}\]

are finite. Therefore \(\bar{\sigma}_{t+1}^{2}<\infty\), giving the second bound in (F.29). Similarly, using the definition (F.7) and Cauchy-Schwarz,

\[\tilde{\mu}_{t+1} =\lim_{d\to\infty}\frac{\lambda}{n}\mathbb{E}\Big{[}\Big{\langle} \widetilde{V}^{*},\tilde{f}_{t+1}(\tilde{V}_{t+1})\Big{\rangle}\Big{]}\leq \lim_{d\to\infty}\frac{L^{\prime}\lambda}{\sqrt{n}}\mathbb{E}\Bigg{[}\Big{\|} \widetilde{V}^{*}\big{\|}_{2}\Bigg{(}1+\bigg{(}\frac{1}{\sqrt{n}}\Big{\|} \tilde{V}_{t+1}\Big{\|}_{2}\bigg{)}^{k}\Bigg{)}\Bigg{]}\]

which is again finite for the same reason as \(\bar{\sigma}_{t+1}\), giving the first bound in (F.29). Therefore (F.41) is also finite, verifying (F.23) for \(t+1\).

We then show (F.25) for \(t+1\). Using the recursions (F.2) and (F.18),

\[p^{t+1}+\tilde{\mu}_{t+1}\widetilde{u}^{*}-\tilde{u}^{t+1}=\underbrace{ \widetilde{W}(\widetilde{q}^{t+1}-\hat{v}^{t+1})}_{T_{1}^{\prime}}+\underbrace{ \left(\tilde{\mu}_{t+1}-\frac{\lambda}{n}(\widetilde{v}^{*},\hat{v}^{t+1}) \right)}_{T_{2}^{\prime}}\widetilde{u}^{*}-\underbrace{\left(\ell_{t+1} \widetilde{p}^{t}-\tilde{b}_{t+1}\hat{u}^{t}\right)}_{T_{3}^{\prime}}.\]

Consider \(T_{1}^{\prime}\). Since (F.26) to (F.28) are assumed to hold, by pseudo-Lipschitzness of \(\tilde{f}_{t+1}\),

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{d}}\big{\|}\tilde{q}^{t+1}- \hat{v}^{t+1}\big{\|}_{2}=0.\] (F.42)

This with the Bai-Yin law [3] gives

\[\operatorname*{p-lim}_{n\to\infty}\frac{1}{n}\big{\|}T_{1}^{\prime}\big{\|}_{2} ^{2}=0,\] (F.43)Consider \(T_{2}^{\prime}\). Recall that \(\ddot{\mu}_{t+1}<\infty\). Using (F.7), (F.21) and (F.42) and following the argument leading to (F.36), we have

\[\operatorname*{p-lim}_{n\to\infty} |T_{2}^{\prime}|=0.\] (F.44)

Consider \(T_{3}^{\prime}\). By the triangle inequality,

\[\left\|\ell_{t+1}\widetilde{p}^{t}-\breve{b}_{t+1}\hat{u}^{t}\right\|_{2}\leq \left|\ell_{t+1}-\breve{b}_{t+1}\right|\left\|\left\|\widetilde{p}^{t}\right\| _{2}+\left|\breve{b}_{t+1}\right|\left\|\left\|\widetilde{p}^{t}-\hat{u}^{t} \right\|_{2}.\right.\]

Since (F.26) to (F.28) are assumed to hold, by pseudo-Lipschitzness of \(\frac{1}{n}\sum_{i=1}^{d}\partial_{i}\breve{f}_{t+1}(\cdot)_{i}\),

\[\operatorname*{p-lim}_{n\to\infty} \left|\breve{b}_{t+1}\right|<\infty,\quad\operatorname*{p-lim}_{n \to\infty} \left|\ell_{t+1}-\breve{b}_{t+1}\right|=0.\] (F.45)

Similarly, pseudo-Lipschitzness of \(\breve{g}_{t}\) and the hypothesis (F.23) to (F.25) ensures

\[\operatorname*{p-lim}_{n\to\infty} \frac{1}{\sqrt{n}}\left\|\widetilde{p}^{t}\right\|_{2}<\infty, \quad\operatorname*{p-lim}_{n\to\infty}\frac{1}{\sqrt{n}}\left\|\widetilde{p} ^{t}-\hat{u}^{t}\right\|_{2}=0,\] (F.46)

So combining (F.45) and (F.46), we have

\[\operatorname*{p-lim}_{n\to\infty} \frac{1}{n}\left\|T_{3}^{\prime}\right\|_{2}^{2}=0.\] (F.47)

(F.43), (F.44) and (F.47) altogether verify (F.25) for \(t+1\), and therefore also (F.24) by (F.23).

The verification of (F.26) to (F.28) and (F.30) for \(t+1\) is completely analogous and we do not repeat similar arguments. The proof is finally completed. 

### Proof of Proposition e.1

We will prove Proposition E.1 by reducing the AMP iteration (E.1) (and its associated state evolution (E.2) to (E.6)) to the auxiliary AMP (F.2) (and its associated state evolution (F.3) to (F.12), (F.14) and (F.15)).

Under the following change of variables

\[u^{t} \coloneqq\Xi^{-1/2}\tilde{u}^{t}, v^{t+1} \coloneqq\Sigma^{-1/2}\tilde{v}^{t+1},\] (F.48) \[f_{t+1}(v^{t+1}) \coloneqq\Sigma^{1/2}\breve{f}_{t+1}(\Sigma^{1/2}v^{t+1}), g_{t}(u^{t}) \coloneqq\Xi^{1/2}\tilde{g}_{t}(\Xi^{1/2}u^{t}),\] (F.49)

(F.2) becomes

\[u^{t} =\Xi^{-1}A\Sigma^{-1}\tilde{v}^{t}-b_{t}\Xi^{-1}\tilde{u}^{t}, \widetilde{u}^{t}=g_{t}(u^{t}),\] \[v^{t+1} =\Sigma^{-1}A^{\top}\Xi^{-1}\tilde{u}^{t}-c_{b}\Sigma^{-1}\tilde {v}^{t}, \widetilde{v}^{t+1}=f_{t+1}(v^{t+1}),\]

where \(b_{t+1},c_{t}\) are equal to \(\breve{b}_{t+1},\breve{c}_{t}\), respectively, but are expressed using the derivatives of \(f_{t+1},g_{t}\). Specifically,

\[c_{t} =\frac{1}{n}\sum_{i=1}^{n}\frac{\partial}{\partial\dot{\tilde{u} }_{i}^{t}}\breve{g}_{t}(\tilde{u}^{t})_{i}=\frac{1}{n}\sum_{i=1}^{n}\frac{ \partial}{\partial\dot{\tilde{u}}_{i}^{t}}\Big{(}\Xi^{-1/2}g_{t}(\Xi^{-1/2} \tilde{u}^{t})\Big{)}_{i}\] \[=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\sum_{k=1}^{n}(\Xi^{-1/2} )_{i,j}(\Xi^{-1/2})_{k,i}\frac{\partial g_{t}(u^{t})_{j}}{\partial u_{k}^{t}}= \frac{1}{n}\operatorname*{Tr}((\nabla g_{t})\Xi^{-1}).\]

The second equality follows since by (F.49),

\[\breve{g}_{t}(\tilde{u}^{t})=\Xi^{-1/2}g_{t}(\Xi^{-1/2}\tilde{u}^{t}).\]

The third equality is by the chain rule for derivatives (Proposition G.6). A similar computation gives

\[b_{t+1}=\frac{1}{n}\operatorname*{Tr}((\nabla f_{t+1})\Sigma^{-1}).\]

We now see that under the change of variables (F.48) and (F.49), the AMP iteration (E.1) can be cast as (F.2). Therefore, applying the same change of variables to the state evolution of (F.2) will produce the state evolution of (E.1). We describe the required modifications below.

The state evolution result in Proposition F.1 for the AMP in (F.2) says that the iterates \(v^{*},\tilde{v}^{1},\tilde{v}^{2},\cdots,\tilde{v}^{t+1}\in\mathbb{R}^{d}\) and \(u^{*},\tilde{u}^{0},\tilde{u}^{1},\cdots,\tilde{u}^{t}\in\mathbb{R}^{n}\) converge (in the sense of (F.16a) and (F.16b)) respectively to \(V^{*},\tilde{V}_{1},\tilde{V}_{2},\cdots,\tilde{V}_{t+1}\in\mathbb{R}^{d}\) and \(U^{*},\tilde{U}_{1},\tilde{U}_{2},\cdots,\tilde{U}_{t}\in\mathbb{R}^{n}\). Recall that AMPs (E.1) and (F.2) operate on the following matrices respectively:

\[\widetilde{A}=\frac{\lambda}{n}(\Xi^{-1/2}u^{*})(\Sigma^{-1/2}v^{*})^{\top}+ \widetilde{W},\quad\Xi^{-1}A\Sigma^{-1}=\frac{\lambda}{n}(\Xi^{-1}u^{*})( \Sigma^{-1}v^{*})^{\top}+\Xi^{-1/2}\widetilde{W}\Sigma^{-1/2}.\]

In view of (F.48), to obtain the analogous state evolution result for the AMP in (E.1), the definition (F.3) of \(\tilde{U}_{t},\tilde{V}_{t+1}\) should be multiplied by \(\Xi^{-1/2},\Sigma^{-1/2}\) respectively. This gives the new definition of \(U_{t},V_{t+1}\) in (E.6). By the relation (F.49), the parameters \(\tilde{\mu}_{t},\tilde{\nu}_{t+1}\) in \(\tilde{U}_{t},\tilde{V}_{t+1}\) should be modified as follows: replace \(\tilde{f}_{t}(\tilde{V}_{t}),\bar{g}_{t}(\tilde{U}_{t})\) in the recursive equations (F.6) to (F.8) with \(\Sigma^{-1/2}f_{t}(V_{t}),\Xi^{-1/2}g_{t}(U_{t})\). This gives the new definition of \(\mu_{t},\nu_{t}\) in (E.5). Similar operations map equations (F.9) to (F.12), (F.14) and (F.15) to equations (E.3) and (E.4). Finally, under the new definition of \(U_{t},V_{t+1}\), the convergence result (F.16a) and (F.16b) translates to (E.8a) and (E.8b), which completes the proof.

## Appendix G Auxiliary lemmas

**Proposition G.1** (Gaussian integral).: _Let \(A\in\mathbb{R}^{d\times d}\) be a positive-definite matrix and \(b\in\mathbb{R}^{d}\). Then_

\[\int_{\mathbb{R}^{d}}\exp\biggl{(}-\frac{1}{2}x^{\top}Ax+b^{\top}x\biggr{)} \,\mathrm{d}x=\sqrt{\frac{(2\pi)^{d}}{\det(A)}}\exp\biggl{(}\frac{1}{2}b^{ \top}A^{-1}b\biggr{)}.\]

**Proposition G.2**.: _Let \(V\sim Q^{\otimes d}\) where \(Q\) is a fixed distribution on \(\mathbb{R}\) with mean \(0\). Let \(B\in\mathbb{R}^{d\times d}\) denote a sequence (indexed by \(d\)) of deterministic matrices such that the empirical spectral distribution of \(\frac{1}{d}B\) converges to the law of a random variable \(\overline{B}\). Then_

\[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\bigl{[}V^{\top}BV\bigr{]}=\mathbb{E} \Bigl{[}\overline{V}^{2}\Bigr{]}\mathbb{E}\bigl{[}\overline{B}\bigr{]}\]

_where \(\overline{V}\sim Q\)._

**Proposition G.3**.: _Let_

\[(W_{1},W_{2})\sim\mathcal{N}\biggl{(}\begin{bmatrix}0_{d}\\ 0_{d}\end{bmatrix},\begin{bmatrix}\sigma_{1}^{2}&\rho\\ \rho&\sigma_{2}^{2}\end{bmatrix}\otimes I_{d}\biggr{)}.\]

_Let \(B\in\mathbb{R}^{d\times d}\) denote a sequence (indexed by \(d\)) of deterministic matrices such that the empirical spectral distribution of \(\frac{1}{d}B\) converges to the law of a random variable \(\overline{B}\). Then_

\[\lim_{d\to\infty}\frac{1}{d}\mathbb{E}\bigl{[}W_{1}^{\top}BW_{2}\bigr{]}=\rho \mathbb{E}\bigl{[}\overline{B}\bigr{]}.\]

**Proposition G.4** (Nishimori identity).: _Let \((X,Y)\) be two random variables. Let \(k\geq 1\) and \(X_{1},\cdots,X_{k}\) be \(k\) i.i.d. samples (given \(Y\)) from the distribution \(\mathrm{law}(X\,|\,Y)\). Denote by \(\langle\cdot\rangle,\mathbb{E}[\cdot]\) the expectations with respect to \(\mathrm{law}(X\,|\,Y)\) and \(\mathrm{law}(X,Y)\), respectively. Then for all continuous bounded function \(f\), it holds that_

\[\mathbb{E}[\langle f(Y,X_{1},\cdots,X_{k})\rangle]=\mathbb{E}[\langle f(Y,X_{1 },\cdots,X_{k-1},X)\rangle].\]

**Proposition G.5** (Conditional distribution of Gaussians).: _Let \(d\geq 2\) and \(1\leq p\leq d-1\) be integers. Let_

\[\begin{bmatrix}G_{1}\\ G_{2}\end{bmatrix}\sim\mathcal{N}\biggl{(}\begin{bmatrix}\mu_{1}\\ \mu_{2}\end{bmatrix},\begin{bmatrix}\Sigma_{1,1}&\Sigma_{1,2}\\ \Sigma_{1,2}^{\top}&\Sigma_{2,2}\end{bmatrix}\biggr{)}\]

_be a \(d\)-dimensional Gaussian random vector, where \(G_{1}\in\mathbb{R}^{p},G_{2}\in\mathbb{R}^{d-p},\mu_{1}\in\mathbb{R}^{p},\mu_{2 }\in\mathbb{R}^{d-p},\Sigma_{1,1}\in\mathbb{R}^{p\times p},\Sigma_{1,2}\in \mathbb{R}^{p\times(d-p)},\Sigma_{2,2}\in\mathbb{R}^{(d-p)\times(d-p)}\). Then for any \(g_{2}\in\mathbb{R}^{d-p}\), the distribution of \(G_{1}\) conditioned on \(G_{2}=g_{2}\) is given by \(G_{1}\,|\,\{G_{2}=g_{2}\}\sim\mathcal{N}(\mu_{1}^{\prime},\Sigma_{1}^{\prime})\) where_

\[\mu_{1}^{\prime}=\mu_{1}+\Sigma_{1,2}\Sigma_{2,2}^{-1}(g_{2}-\mu_{2})\in \mathbb{R}^{p},\quad\Sigma_{1}^{\prime}=\Sigma_{1,1}-\Sigma_{1,2}\Sigma_{2,2}^{-1 }\Sigma_{1,2}^{\top}\in\mathbb{R}^{p\times p}\]

_and \(\Sigma_{2,2}^{-1}\) denotes the generalized inverse of \(\Sigma_{2,2}\)._

**Proposition G.6** (Chain rule of derivatives).: _Let \(A\in\mathbb{R}^{n\times n}\) and \(f\colon\mathbb{R}^{n}\to\mathbb{R}^{n}\). Let \(x\in\mathbb{R}^{n}\) and \(\widetilde{x}\coloneqq Ax\). Then for any \(i,j\in[n]\),_

\[\frac{\partial}{\partial x_{i}}(Af(Ax))_{i}=\sum_{j=1}^{n}\sum_{k=1}^{n}A_{i,j} A_{k,i}\frac{\partial f(\widetilde{x})_{j}}{\partial\widetilde{x}_{k}},\]

_where \(f(\widetilde{x})_{j}\in\mathbb{R}\) denotes the \(j\)-th (\(j\in[n]\)) output of \(f(\widetilde{x})\in\mathbb{R}^{n}\)._

Proof.: The proof follows from elementary applications of the chain rule for derivatives. Writing \(A=\left[a_{1}\quad\cdots\quad a_{n}\right]^{\top}\) and \(f=\left[f_{1}\quad\cdots\quad f_{n}\right]^{\top}\), we have

\[\frac{\partial}{\partial x_{i}}f_{j}(Ax)=\frac{\partial}{\partial x_{i}}f_{j} (\langle a_{1},x\rangle,\cdots,\langle a_{n},x\rangle)=\sum_{k=1}^{n}\partial _{k}f_{j}(Ax)\frac{\partial\langle a_{k},x\rangle}{\partial x_{i}}=\sum_{k=1}^ {n}A_{k,i}\partial_{k}f_{j}(\widetilde{x}),\]

where \(\partial_{k}f_{j}\) denotes the partial derivative of \(f_{j}\colon\mathbb{R}^{n}\to\mathbb{R}\) with respect to its \(k\)-th argument. Then,

\[\frac{\partial}{\partial x_{i}}(Af(Ax))_{i}=\sum_{j=1}^{n}A_{i,j}\frac{ \partial}{\partial x_{i}}f_{j}(Ax)=\sum_{j=1}^{n}\sum_{k=1}^{n}A_{i,j}A_{k,i} \partial_{k}f_{j}(\widetilde{x}),\]

as claimed. 

**Proposition G.7** (Stein's lemma [68]).: _Let \(W\sim\mathcal{N}(0,\sigma^{2})\) and let \(f\colon\mathbb{R}\to\mathbb{R}\) be such that both expectations below exist. Then \(\mathbb{E}[Wf(W)]=\sigma^{2}\mathbb{E}[f^{\prime}(W)]\)._

**Proposition G.8** (Gaussian Poincare inequality [16, Theorem 3.20]).: _Let \(X\sim\mathcal{N}(0_{n},I_{n})\) and \(f\colon\mathbb{R}^{n}\to\mathbb{R}\) a differentiable function. Then_

\[\mathrm{Var}[f(X)]\leq\mathbb{E}\Big{[}\big{\|}\nabla f(X)\big{\|}_{2}^{2} \Big{]}.\]

**Proposition G.9** (Bounded difference inequality [16, Corollary 3.2]).: _Let \(\mathcal{U}\subset\mathbb{R}\) and \(f\colon\mathcal{U}^{n}\to\mathbb{R}\) a function such that there exist \(c=(c_{1},\cdots,c_{n})\in\mathbb{R}^{n}_{\geq 0}\) satisfying for all \(i\in[n]\),_

\[\sup_{(x_{1},\cdots,x_{n},x^{\prime}_{i})\in\mathcal{U}^{n+1}}\lvert f(x_{1}, \cdots,x_{i-1},x_{i},x_{i+1},\cdots,x_{n})-f(x_{1},\cdots,x_{i-1},x^{\prime}_{ i},x_{i+1},\cdots,x_{n})\rvert\leq c_{i}.\]

_Then if \(X\in\mathcal{U}^{n}\) is a random vector consisting of independent elements, we have \(\mathrm{Var}[f(X)]\leq\left\|c\right\|_{2}^{2}/4\)._

**Proposition G.10** ([62, Lemma 3.2]).: _If \(f,g\) are differentiable convex functions, then for any \(a\in\mathbb{R}\) and \(a^{\prime}>0\),_

\[\lvert f^{\prime}(a)-g^{\prime}(a)\rvert\leq g^{\prime}(a+a^{\prime})-g^{ \prime}(a-a^{\prime})+B/a^{\prime},\]

_where_

\[B\coloneqq\lvert f(a+a^{\prime})-g(a+a^{\prime})\rvert+\lvert f(a-a^{\prime}) -g(a-a^{\prime})\rvert+\lvert f(a)-g(a)\rvert.\]

**Definition G.1** (Monotone conjugate).: Let \(f\colon\mathbb{R}_{\geq 0}\to\mathbb{R}\) be a non-decreasing convex function. Its monotone conjugate \(f^{*}\) is defined as

\[f^{*}(x)=\sup_{y\geq 0}xy-f(y).\]

**Proposition G.11** ([53, Proposition C.1]).: _Let \(I\subset\mathbb{R}\) be an interval and \((f_{n}\colon I\to\mathbb{R})_{n}\) be a sequence of convex functions converging pointwise to \(f\). Then for all \(t\in I\) such that the following quantities exist,_

\[\lim_{s\uparrow t}f^{\prime}(s)\leq\liminf_{n\to\infty}\lim_{s\uparrow t}f^{ \prime}_{n}(s)\leq\limsup_{n\to\infty}\lim_{s\downarrow t}f^{\prime}_{n}(s) \leq\lim_{s\downarrow t}f^{\prime}(s).\]

**Proposition G.12** ([53, Proposition C.6]).: _Let \(f,g\colon\mathbb{R}_{\geq 0}\to\mathbb{R}\) be strictly convex differentiable functions and_

\[\mathcal{C}\coloneqq\big{\{}(q_{1},q_{2})\in\mathbb{R}^{2}_{\geq 0}\colon q_{2}=f^{ \prime}(q_{1}),q_{1}=g^{\prime}(q_{2})\big{\}}.\]

_Then_

\[\sup_{(q_{1},q_{2})\in\mathcal{C}}f(q_{1})+g(q_{2})-q_{1}q_{2}=\sup_{q_{1},q_{2 }\geq 0}q_{1}q_{2}-f^{*}(q_{2})-g^{*}(q_{1})=\sup_{q_{1}\geq 0}\inf_{q_{2}\geq 0}f(q_{1})+g(q_{2}) -q_{1}q_{2},\]

_and \(\sup_{(q_{1},q_{2})\in\mathcal{C}}\) and \(\sup_{q_{1},q_{2}\geq 0}\) are achieved at the same \((q_{1}^{*},q_{2}^{*})\)._

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All claims match theoretical and experimental results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Global assumptions are given in Section 3 and all theoretical results are formally stated and proved. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: All experiments use synthetic data and can be reproduced given the instructions in Section 5. Data and code are not released. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Figures 1 to 3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: All experiments are synthetic and can be run efficiently on standard personal computers. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics and confirm that this research conforms to it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This research is purely theoretical and has no obvious societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This research is purely theoretical and has no risk of misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.