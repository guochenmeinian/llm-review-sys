# Rethinking 3D Convolution in \(\ell_{p}\)-norm Space

 Li Zhang\({}^{1,2,4}\), Yan Zhong\({}^{3}\), Jianan Wang\({}^{4}\), Zhe Min\({}^{5}\), Rujing Wang\({}^{1,2}\), Liu Liu\({}^{6}\)

\(1\) Hefei Institute of Physical Science, Chinese Academy of Sciences

\(2\) University of Science and Technology of China, Hefei, China

\(3\) School of Mathematical Sciences, Peking University. Beijing, China

\(4\) Astribot, Shenzhen, China

\(5\) Shandong University, Jinan, China

\(6\) Hefei University of Technology, Hefei, China

zanly@mail.ustc.edu.cn, zhongyan@stu.pku.edu.cn

Li Zhang and Yan Zhong contribute equally. This work was done when Li Zhang was an intern at Astribot. Corresponding author: Liu Liu.

###### Abstract

Convolution is a fundamental operation in the 3D backbone. However, under certain conditions, the feature extraction ability of traditional convolution methods may be weakened. In this paper, we introduce a new convolution method based on \(\ell_{p}\)-norm. For theoretical support, we prove the universal approximation theorem for \(\ell_{p}\)-norm based convolution, and analyze the robustness and feasibility of \(\ell_{p}\)-norms in 3D point cloud tasks. Concretely, \(\ell_{\infty}\)-norm based convolution is prone to feature loss. \(\ell_{2}\)-norm based convolution is essentially a linear transformation of the traditional convolution. \(\ell_{1}\)-norm based convolution is an economical and effective feature extractor. We propose customized optimization strategies to accelerate the training process of \(\ell_{1}\)-norm based Nets and enhance the performance. Besides, a theoretical guarantee is given for the convergence by _regret_ argument. We apply our methods to classic networks and conduct related experiments. Experimental results indicate that our approach exhibits competitive performance with traditional CNNs, with lower energy consumption and instruction latency.

## 1 Introduction

The convolution-based 3D backbone networks have demonstrated substantial success in foundational tasks such as classification [1], object tracking [2], scene segmentation [3], etc. Some downstream tasks also heavily rely on these networks, such as interactive perception [4], object manipulation [5], imitation learning [6], and human-machine collaboration [7]. In the traditional 3D convolution, suppose \(K\in\mathbb{R}^{m\times n}\) is the filter, and \(P_{t}\in\mathbb{R}^{m\times n}\) is the sampled matrix from the \(t\)-th sliding window on input data, \(1\leq t\leq T\). \(T\) is the total sliding counts. For any \(t\geq 1\), the \(t\)-th convolution is calculated as:

\[P_{t}\odot K=\sum_{1\leq i\leq m}\sum_{1\leq j\leq n}P_{t}(i,j)\cdot K(i,j)\] (1)

which is the same as inner product between vectors. To distinguish it from our new convolution framework, we refer to it as _inner product based convolution_ in the following discussion. A geometric consideration arises when \(P_{t}\) follows a certain symmetric distribution, such as a Gaussian or uniform distribution. By symmetry, there exist some of \(\{P_{t}\}_{t=1}^{T}\) situated close to the subspace perpendicular to \(K\), which means \(K\odot P_{t}\approx 0\). This inevitably leads to explicit feature loss, diminishing the model's ability on information extraction.

In previous works, \(\ell_{p}\)-norms (\(p=1,2,3,\cdots,\infty\)) demonstrated strong performance across various domains [8; 9; 10]. These norms exhibit remarkable capabilities in expressing spatial structures and local relationships within sets of points. To address the limitations of inner product-based convolution in certain extreme cases and to explore the potential of \(\ell_{p}\)-norms in feature extraction, we propose \(\ell_{p}\)-norm-based convolution, _i.e._, for any kernel \(K\) and sampled matrix \(P_{t}\), it can be formulated as Eq. 2:

\[\|P_{t}-K\|_{p}\triangleq\big{(}\sum_{1\leq i\leq m}\sum_{1\leq j\leq n}(P_{t} (i,j)-K(i,j))^{p}\big{)}^{1/p}.\] (2)

More precisely, the goal of this paper is to leverage the power of \(\ell_{p}\)-norm measurement (Fig. 1 (a)) and devise efficient and robust optimization methods for it. Our solutions are as follows:

From the theoretical standpoint, we prove the universal approximation theorem of \(\ell_{p}\)-norm Nets (for \(p=1,2,3,\cdots,\infty\)). Besides, we show that \(\ell_{p}\)-norm based convolutions are more robust than the traditional ones via variance analysis under random noise.

From the practical standpoint, we first discuss the performance of different \(\ell_{p}\)-norms in actual execution. 3D convolution in \(\ell_{\infty}\)-norm space tends to lose multiple useful pieces of information since only the maximum absolute value is reserved. The \(\ell_{2}\)-norm measure is inherently a linear transformation of the traditional convolution (details can be found in Sec. A). In contrast, the \(\ell_{1}\)-norm has unique potential for 3D point cloud tasks. However, directly replacing traditional convolution with an \(\ell_{1}\)-norm-based one is not feasible in practice due to the difficult convergence and local optima. To enhance network performance, we propose customized optimization strategies. The first strategy is a mixed gradient strategy (MGS), and the second is a dynamic learning rate controller (DLC). These strategies are applied in the training process (Algorithm 1) to accelerate network convergence and avoid local optima. We also provide a convergence guarantee for our optimization strategies from the perspective of _regret_.

We evaluate our method on several benchmarks, ranging from global, semi-dense, and dense prediction tasks. The experimental results show that \(\ell_{1}\)-norm Net has the same competitive performance as traditional convolution. Moreover, the proposed \(\ell_{1}\)-norm Net has three advantages: 1) \(\ell_{1}\)-norm (inherently addition operation) has lower computational complexity compared to multiplication; 2) addition significantly reduces energy consumption [11]; 3) \(\ell_{1}\)-norm operations (addition) has lower instruction latencies [12] than inner product process (multiplication). These properties facilitate the 3D point cloud tasks especially online tasks such as 3D real-time object detection, pose tracking, etc.

**Contributions.** 1) We prove the universal approximation for \(\ell_{p}\)-norm Nets. And we show that \(\ell_{p}\)-norm Nets are robust under random noise. 2) We compare different \(\ell_{p}\)-norm based convolutions, and further propose a reliable and efficient \(\ell_{1}\)-norm Net for 3D point cloud tasks with customized optimization strategies. We also give a theoretical guarantee for convergence by regret argument. 3) Experimental results demonstrate the effectiveness of our methods in 3D point cloud tasks, showing lower energy consumption and faster instruction execution.

## 2 Related Work

**Different Convolution Methods.** Convolutions have seen significant success, leading to various convolution methods aimed at improving performance and efficiency. Traditional convolutions, introduced by [13], use fixed-size kernels to extract features but are computationally intensive and may not capture diverse patterns effectively. To overcome these limitations, several alternatives have been proposed: 1) depthwise separable convolutions [14; 15]. Popularized by MobileNets, these decompose standard convolutions into depthwise and pointwise operations. 2) dilated convolutions [16; 17; 18]. These introduce spaces between kernel elements, expanding the receptive field without increasing parameters. 3) deformable convolutions [19; 20]. These adapt the sampling loca

Figure 1: **(a) Visualizing the circles of \(\ell_{p}\)-norms. (b) Manhattan distance based \(\ell_{1}\)-norm measure.**

tions of the convolutional kernel, enhancing the network's ability to model geometric transformations. However, due to their unique strengths, they only excel at some specific tasks.

\(\ell_{p}\)**-norm Measure in Different Tasks.** Using the \(\ell_{p}\)-norm as a feature measurement function for convolutional kernels offers several advantages: 1) Flexibility: The \(\ell_{p}\)-norm allows adjusting the parameter \(p\) according to specific needs [21; 22; 23]. 2) Sparsity: It encourages most elements in the convolutional kernel to approach zero, reducing computational complexity and storage requirements [21; 24]. Overall, in diverse settings, employ distinctive approaches. The \(\ell_{p}\)-norm is widely used across various fields. For example, in _image processing_, the \(\ell_{1}\)-norm is used for sparse representation in image compression [25], enabling efficient storage and transmission. In _machine learning and optimization_, optimization problems also use \(\ell_{p}\)-norm constraints to impose sparsity or specific patterns in solutions [26; 27]. Despite progress, directly migrating these methods into 3D point cloud tasks causes a _domain gap_. In this work, we aim to explore \(\ell_{p}\)-norm measure for 3D point cloud tasks in depth.

## 3 Methodology

**Notations.** For the sake of simplicity, in what follows, we take the classic PointNet++ [28] as the basis model to estimate the efficiency of \(\ell_{p}\)-norm based Nets with the proposed optimization strategies. Note that, we directly replace the inner product based convolution by \(\ell_{p}\)-norms \((p=1,2,3,\ldots,\infty)\) based one, and denote the corresponding network by \(\ell_{p}\)-PointNet++ or \(\ell_{p}\)-norm Net. Moreover, the proposed \(\ell_{p}\)-norm based convolution can also be called \(\ell_{p}\)-norm neuron.

### Universal Approximation

The universal approximation ability of a neural network is crucial. Firstly, it establishes a solid theoretical foundation for the network's capabilities [29], which asserts that certain architectures and activation functions enable neural networks to approximate any continuous function. There is a series of works on the approximation capacity, such as theories for feedforward networks [30], RNNs [31], Transformer [32]. However, the universal approximation property of \(\ell_{p}\)-PointNet++ has not been studied thoroughly up to now.

**Theorem 1**.: _Assume \(S=\{x_{1},\cdots,x_{N}\}\subset\mathbb{R}^{k}\) is an arbitrary point cloud. \(J\subset\mathbb{R}^{k}\) is any compact set and \(S\subset J\). For any continuous function \(f\) defined on \(2^{J}\) with respect to Hausdorff distance \(d_{H}(\cdot,\cdot)\), there exists an \(\ell_{p}\)-PointNet++ \(\mathcal{P}\) satisfying for any \(\epsilon>0\),_

\[|f(S)-\mathcal{P}(S)|\leq\epsilon.\] (3)

_Moreover, for any \(\ell_{1}\)-integrable function \(g\) defined on \(J\), there exists an \(\ell_{p}\)-PointNet++ \(\mathcal{P}^{\prime}\), for any \(\epsilon^{\prime}>0\),_

\[\int_{x\in J}|g(x)-\mathcal{P}^{\prime}(x)|dx<\epsilon^{\prime}.\] (4)

Briefly speaking, \(f\) could be approximated by an MLP consisting of \(\ell_{p}\)-norm convolution layers and a max pooling layer. And \(g\) could be approximated by a network composed of an \(\ell_{p}\)-norm convolution layer and a fully connected layer. The detailed proof can be found in Sec. A from the appendix.

### Robustness Analysis

In the following, we show that under Gaussian random noise on input data, \(\ell_{p}\)-norm based convolutions are more robust than that based on inner product. Suppose \(G\in\mathbb{R}^{m\times n}\) is a Gaussian matrix. Each \(G(i,j)\sim N(0,\sigma^{2})\) where \(\sigma>0\) is a constant. Let \(P_{t}\in\mathbb{R}^{m\times n}\) be the data at time \(t\) and \(K\in\mathbb{R}^{m\times n}\) be the kernel function.

For inner product,

\[Var\big{[}(G+P_{t})\odot K\big{]}=\mathbb{E}_{G}\Big{[}\big{(}G\odot K- \mathbb{E}_{G}[G\odot K]\big{)}^{2}\Big{]}=Var\big{[}G\odot K\big{]},\] (5)

and

\[G\odot K=\sum_{i=1}^{m}\sum_{j=1}^{n}G(i,j)K(i,j)\sim N\bigg{(}0,\sigma^{2} \cdot\sum_{i=1}^{m}\sum_{j=1}^{n}K(i,j)^{2}\bigg{)}.\] (6)Suppose \(\forall i\in[m]\) and \(\forall j\in[n]\), \(K(i,j)\) is a constant, we have \(Var\big{[}(G+P_{t})\odot K\big{]}=\Theta(mn)\).

For \(\ell_{p}\)-norm, first we could prove that when \(p=2\), \(Var\big{[}\|G+X-K\|_{2}\big{]}=O(1)\), which is significantly smaller than \(Var\big{[}(G+P_{t})\odot K\big{]}\). The details of calculation could be found in Sec. A from the appendix. For the more general cases (\(p=1,2,3,\cdots,\infty\)), we show that \(\ell_{p}\)-norm has a small variance through numerical computation in the Tab. 1, where we take \(\sigma=1\).

### Implementation of \(\ell_{p}\)-norm Nets

Note that although Theorem 1 guarantees a universal approximation capability, it does not mean that all the \(\ell_{p}\)-norm Nets are efficient and feasible in practice. Therefore, we further discuss the characteristics of each specific \(\ell_{p}\)-norm Nets (\(p=1,2,3,\cdots,\infty\)) in detail.

Assume the input data follows Gaussian distribution, saying \(G\) is the standard Gaussian matrix. For \(\ell_{p}\)-norm based convolution, when \(p\) is greater than or equal to 3, the distribution of the output data is very close. We present the simulation results in Fig. 2. It's clear that when \(p\) is getting larger, the distribution of \(\|G\|_{p}\) gradually overlaps with the distribution of \(\|G\|_{\infty}\). Therefore, we take \(p=\infty\) as the representative case for \(p\geq 3\).

Actually, \(l_{\infty}\)-norm exhibits weaknesses due to its overly simplistic emphasis on the largest element. Namely this approach tends to oversimplify the feature space by disproportionately emphasizing only one dimension, potentially discarding valuable information present in other dimensions. Also, this concept is supported by experimental results in Sec. 5. Besides, \(\ell_{2}\)-norm inherently is calculated by taking the square root of the sum of the squares of its elements. And \(\ell_{2}\)-norm based convolution \(\mathcal{C}_{\ell_{2}}\) can be regarded as an equivalence transformation of the traditional convolution \(\mathcal{C}\). Briefly speaking, we could show that \(\mathcal{C}_{\ell_{2}}^{2}=\alpha+\beta\times\mathcal{C}\), where \(\alpha\) and \(\beta\) are constants.

\(\ell_{1}\)-norm can synthesize each element of the feature vector. And the \(\ell_{1}\)-norm Net is not equivalent to a translation transform, which we believe holds potential as a 3D convolutional similarity metric function according to the Theorem. 1. To this end, our method focuses on rationalizing the \(\ell_{1}\)-norm measure to maximize its potential in feature extraction. Mathematically, if the similarity measurement function between the input data and kernel function is replaced with the \(\ell_{1}\)-norm, the convolution can be re-formulated as:

\[Y(P_{t},K)=-\sum_{t\geq 1}\sum_{i,j}|P_{t}(i,j)-K(i,j)|\] (7)

The underlying operation of \(\ell_{1}\)-norm kernel function is addition, which has more development potential and application value in real scenarios. Specifically, 1) It contains almost no multiplication but addition, resulting in lower computational complexity of the model. 2) \(\ell_{1}\)-norm operation (addition) is proved to have lower energy consumption compared to the inner product (multiplication) calculation [33]. Take the operation of floating-point addition and multiplication as an example, which has energy costs of 0.9 \(pJ\) and 3.7 \(pJ\), respectively. 3) Low latency is also a consideration in practical application scenarios. [12] tells us that multiplication (inner product process) has longer theoretical instruction wait times than addition operations. Table 1 of this study lists the instruction

\begin{table}
\begin{tabular}{c|c c c c c} \hline p & **1** & **2** & **3** & **4** & **5** \\ \hline Var & 3.24655 & 0.48327 & 0.31248 & 0.27494 & 0.26078 \\ \hline \hline
**p** & **6** & **7** & **8** & **9** & \(\infty\) \\ \hline Var & 0.26093 & 0.26040 & 0.26078 & 0.26145 & 0.26875 \\ \hline \end{tabular}
\end{table}
Table 1: **Variance of the \(\ell_{p}\)-norm of Gaussian random vector when \(mn=9\).**

Figure 2: (**Left**) The distribution of \(\|G\|_{p}\),where \(G\) is the standard Gaussian vector, \(p=1,2,3,\infty\) and \(dim(G)=9\). (**Right**) The distribution of \(\|G\|_{p}\), \(p=3,4,5,6,7,8,9,\infty\) and \(dim(G)=9\).

latency, throughput, and micromanipulation faults for Intel, AMD, and VIA CPUs. For instance, the latency of float multiplication and addition is 4 and 2 in the VIA Nano 2000 series.

### Regret

It's a good way [34; 35; 36] to demonstrate the convergence of an optimization process by analyzing the _regret_. Performance measurement [37], optimization guidance [38], and feedback mechanisms [39] can be summarized as its advantages. We employ it the construct the convergence theorem for our optimization strategies in Sec. 4.

Consider a general online optimization model between a player and an adversary. A subset \(\mathcal{F}\in\mathbb{R}^{m}\) is non-empty, bounded and closed. For each iteration \(k\in[T^{*}]\), the player choose a point \(\mathbf{x}_{k}\in\mathcal{F}\) (\(T^{*}\) is not known for player). After committing to this choice, a convex function \(h_{k}\) will be revealed by the adversary. And we note the cost of this game by _regret_:

\[R_{T^{*}}=\sum_{k=1}^{T^{*}}h_{k}(\mathbf{x}_{k})-\min_{\mathbf{x}\in \mathcal{F}}\sum_{k=1}^{T^{*}}h_{k}(\mathbf{x}).\] (8)

The player aims to carefully select \(\mathbf{x}_{k}\) to minimize regret as much as possible, while conversely the adversary aims to specifically choose \(h_{k}\) to hinder the player. Intuitively, if an algorithm(the player) could bound regret by a sub-linear function of \(T^{*}\), _i.e._, \(R_{T^{*}}=o(T^{*})\), we could conclude that "on the average" the algorithm performs as well as the best fixed strategy in hindsight [40].

## 4 Optimization

By the argument above, we are motivated to devise a new convolution based on \(\ell_{1}\)-norm. However, direct training of \(\ell_{1}\)-norm Nets can easily lead to unsatisfactory results. Thus, two customized optimization strategies are proposed for training. Before introducing these optimization strategies, we clarify the notations in the following.

NotationsRecall that \(K\in\mathbb{R}^{m\times n}\) is the kernel and \(P_{t}\in\mathbb{R}^{m\times n}\) is the sliding window on the input data, \(1\leq t\leq T\). \(Y(P_{t},K)\) is the convolution of \(K\) and \(P_{t}\). \(L\) denotes the loss function in training process. We use the \(m\times n\) matrix \(\frac{\partial L}{\partial K}\) to denote the gradient on of \(L\) on \(K\), where \((\frac{\partial L}{\partial K})_{i,j}=\frac{\partial L}{\partial K(i,j)}\). Besides, define the vectors

\[\frac{\partial L}{\partial Y}\triangleq\Big{(}\frac{\partial L}{\partial Y(P_ {1},K)},\frac{\partial L}{\partial Y(P_{2},K)},\dots,\frac{\partial L}{ \partial Y(P_{T},K)}\Big{)}\]

and

\[\frac{\partial Y}{\partial K(i,j)}\triangleq\Big{(}\frac{\partial Y(P_{1},K)} {\partial K(i,j)},\frac{\partial Y(P_{2},K)}{\partial K(i,j)},\dots,\frac{ \partial Y(P_{T},K)}{\partial K(i,j)}\Big{)}\]

### MGS: Mixed Gradient Strategy

Now we focus on the gradient descent in training process, especially the partial derivative of loss function \(L\) on the kernel \(K\). It should be pointed out that \(L\) is a function on \((Y(P_{1},K),Y(P_{2},K),\dots,Y(P_{T},K))\). By chain rule of derivation we have for any given \((i,j)\),

\[\frac{\partial L}{\partial K(i,j)}=\sum_{t=1}^{T}\frac{\partial L}{\partial Y(P _{t},K)}\cdot\frac{\partial Y(P_{t},K)}{\partial K(i,j)}=\big{\langle}\frac{ \partial L}{\partial Y},\frac{\partial Y}{\partial K(i,j)}\big{\rangle}\] (9)

Notice that when loss function \(L\) is fixed, \(\frac{\partial L}{\partial Y}\) is regardless of the choice of \(Y(P_{t},K)\) (inner product or \(\ell_{p}\)-norm). And we should only focus on the vector \(\frac{\partial Y}{\partial K(i,j)}\). In the context of \(\ell_{1}\)-PointNet++:

\[\frac{\partial Y(P_{t},K)}{\partial K(i,j)}=\text{sgn}\big{(}P_{t}(i,j)-K(i,j) \big{)}.\] (10)

Here, \(\text{sgn}(\cdot)\) represents the sign function.

There are two unavoidable problems: 1) the use of Eq. 10 results in a signSGD update. As discussed in [41], the direction of signSGD is not aligned with the steepest descent, and this misalignment exacerbates with increasing dimensionality. 2) The gradient of \(\ell_{1}\)-norm Net is significantly smaller than that of inner product convolution in the experiment. Namely, \(\|\frac{\partial L}{\partial K}\|_{2}\) is extremely small when we choose the convolution \(Y\) as \(\ell_{1}\)-norm. Taking PointNet++ on S3DIS as an example, we report the \(\ell_{2}\) norm of gradient of \(\ell_{1}\)-PointNet++ in Fig. 3. The gradient from \(\ell_{1}\)-PointNet++ is much smaller than that in PointNet++ (_e.g._, \(\ell_{1}\)-PointNet++: 0.0002, PointNet++: 0.3162 in layer I). Hence, this small gradient \(\frac{\partial L}{\partial K}\) in \(\ell_{1}\)-norm Net would significantly slow down the training process.

Based on the above observations, we introduce a novel **Mixed Gradient** **S**trategy (**MGS**) tailored for \(\ell_{1}\)-PointNet++ training. This approach strategically combines the gradients of the \(\ell_{1}\)-PointNet++ and that of \(\ell_{2}\)-PointNet++:

\[\frac{\partial Y(P_{t},K)}{\partial K(i,j)}=\frac{P_{t}(i,j)-K(i,j)}{||K-P_{t} ||_{2}},\] (11)

Actually, as we discussed above, \(\ell_{2}\)-norm based convolution is a linear transform of inner product convolution. So gradient of \(\ell_{2}\)-norm Net has a proper scale. The mixed strategy involves dynamically adjusting \(\frac{\partial Y(P_{t},K)}{\partial K(i,j)}\) during training, guided by a parameter \(0<\lambda<1\) and the training step \(k\). **The mixed gradient strategy** is expressed as:

\[\frac{\partial Y(P_{t},K)}{\partial K(i,j)}=(1-\lambda^{k})\text{sgn}(P_{t}( i,j)-K(i,j))+\lambda^{k}(P_{t}(i,j)-K(i,j)).\] (12)

This dynamic adjustment introduces a controlled transition in the gradient computation as training progresses. Taking \(\lambda=0.99\) for example, when \(k\) is small, the term \(\lambda^{k}\) dominates and \(\frac{\partial Y(P_{t},K)}{\partial K(i,j)}\) approximates to \(P_{t}(i,j)-K(i,j)\). This initial configuration aligns with the more efficient \(\ell_{2}\)-like update, providing stability and aiding in faster convergence. As training progresses (\(k\) gets larger), the term \(\lambda^{k}\) becomes more prominent, shifting the gradient computation towards \(\text{sgn}(P_{t}(i,j)-K(i,j))\). This transition allows the model to leverage the advantages of the \(\ell_{1}\)-PointNet++ structure, facilitating sparsity in the learned features. By dynamically adapting the gradient computation based on the training step, the mixed strategy offers a flexible and adaptive approach to overcome the challenges associated with fixed gradient schemes. This dynamic adjustment provides a thoughtful compromise, combining the efficiency of \(\ell_{2}\)-like updates in the initial stages with the sparsity-inducing benefits of \(\ell_{1}\)-PointNet++ in later stages.

In fact, there is quite a bit of literature supporting the effectiveness of the signSGD update scheme, and in particular, it has been shown that it has some advantages in avoiding saddle points [42]. However, when certain random rotations of the objective appear, signSGD may become trapped in a periodic behavior that hinders convergence in such cases. To address this unexpected behavior, we additionally explored the introduction of momentum into the update rule. Our experimental results prove that this modification effectively breaks the symmetry induced by random rotations, preventing the model from getting stuck and fostering smoother convergence.

### DLC: Dynamic Learning rate Controller

Considering the uniqueness of the mixed gradient strategy, we focus on achieving larger update magnitudes and faster convergence rates during the initial stages of training. However, in the later stages, we aim to revert to signSGD, implementing a more cautious update strategy to enhance the model's precision. Therefore, we propose a learning rate update strategy that adapts to this characteristic: **D**ynamic **L**earning rate **C**ontroller (**DLC**), maintaining a higher rate in the early training phase, and returning to a lower rate in the later phase.

Figure 3: **The gradient of weight in each layer using two different networks at 1st iteration.** Layer I to III represent 3 SetAbstractions modules in \(\ell_{1}\)-PointNet++ and layer IV to V represent fully connected layers. Note that the y-axis is on a logarithmic scale to reflect the magnitude of the values.

To this end, we design two bound functions to control the learning rate: the lower bound

\[\alpha_{1}(k)=p_{1}\cdot(1+\frac{p_{2}}{e^{k}})\] (13)

and the upper bound

\[\alpha_{2}(k)=p_{1}\cdot(1+\frac{p_{3}}{k})\] (14)

where \(p_{1}\), \(p_{2}\) and \(p_{3}\) are hyper-parameters to be determined and \(k\) denotes the training step. And we use simple comparison operations to make learning rate \(\alpha(k)\) locate in \([\alpha_{1}(k),\alpha_{2}(k)]\):

\[\hat{\alpha}(k)\leftarrow\text{min}\big{\{}\text{max}\{\alpha_{1}(k),\mathcal{ A}[\alpha(k)]\},\alpha_{2}(k)\big{\}}.\] (15)

To enhance the universality of this dynamic control framework, \(\mathcal{A}\) could be another learning rate optimization algorithm like the adaptive learning rate strategy of [43], which can be specifically switched according to the task at hand. However, regardless of \(\mathcal{A}\), we will later demonstrate that dynamic control alone is sufficient to provide theoretical convergence guarantees by the regret argument of Theorem 2, and it also performs well in experiments.

### Training Framework

It has been noted from previous discussions that the momentum method can help signSGD avoid getting trapped in cycles, thereby improving training stability. Combining the methods above, we present the global optimization algorithm (**O**ptimizer with **M**ixed gradient strategy and **D**ynamic learning rate controller, **OMD**) for \(\ell_{1}\)-PointNet++ training. Details are shown in Algorithm. 1

Here we give a convergence guarantee for **OMD** under an online optimization framework, which is harder than offline optimization. We could show that regret \(R_{T^{*}}\) of **OMD** is bounded by \(O(\sqrt{T^{*}})\). Low regret means the algorithm progressively gets closer to the optimal solution over time. This shows that **OMD** has reliable convergence properties, making it a dependable optimization method.

**Theorem 2**.: _Continue with the settings and notations of Algorithm 1. Suppose \(\mathcal{F}\subset\mathbb{R}^{n}\) is bounded, saying \(\text{max}_{\mathbf{x},\mathbf{y}\in F}\|\mathbf{x}-\mathbf{y}\|_{\infty}\leq B _{\infty}\) Besides, suppose \(\forall k\in[T^{*}]\), \(\|\mathbf{g}_{k}\|_{2}\leq B_{2}\). we could show that for any convex functions \(\{h_{k}\}_{t=1}^{T^{*}}\),_

\[R_{T^{*}}=\sum_{k=1}^{T^{*}}h_{k}(\mathbf{x}_{k})-\sum_{t=1}^{T^{*}}h_{k}( \mathbf{x}^{*})\leq C_{1}\cdot\sqrt{T^{*}}+C_{2}\]

_where \(C_{1}\) and \(C_{2}\) are constants that rely on \(p_{1}\), \(p_{2}\), \(p_{3}\), \(B_{\infty}\), \(B_{2}\), \(q_{0}\) and \(q\). And \(\mathbf{x}^{*}\triangleq\text{arg}\min_{\mathbf{x}\in\mathcal{F}}\sum_{k=1}^{ T^{*}}h_{k}(\mathbf{x})\)._

The proof could be found in the appendix, Sec. A.

## 5 Experiments

To validate the generalizability and robustness of the method and thus ensure its effectiveness and broad applicability, we verify the performance of our method in several tasks, ranging from **Global Tasks** (_i.e._, Parts Segmentation), **Semi-dense Prediction** (_i.e._, scenario semantic segmentation), and **Dense Prediction** (_i.e._, pose estimation) tasks. Shapenet, S3DIS, and GarmentNets Simulation are used as the datasets.

### Dataset and Experimental Settings

**Dataset.** 1) ShapeNet. In ShapeNet, there are 16,881 shapes from 16 categories, which are annotated with 50 parts in total. Note that most object categories are labeled with two to five parts and Ground Truth annotations are labeled on sampled points on the shapes. This task can be regarded as a point-wise classification task. 2) S3DIS. The Stanford Large-Scale 3D Indoor Spaces Dataset, which encompasses 3D scans obtained from Matterport scanners across 6 distinct areas, comprising a total of 271 rooms. Within the S3DIS dataset, every point within the scans is labeled with a semantic category from a set of 13 distinct classes. These classes encompass various elements such as chairs, tables, floors, walls, among others, in addition to a category for clutter. 3) GarmentNets Simulation. GarmentNets Simulation is a large-scale dataset proposed by [44]. This dataset has six garment categories with a total data volume of 1.72TB. Dress, Jump, Skirt, Top, Pants and Shirt are included.

**Experimental Settings.** We train our frameworks using CrossEntropy loss and the AdamW optimizer [45], with an initial learning rate of 0.001, a weight decay of \(10^{-4}\), Cosine Decay, and a batch size of 32. The total training consists of 200 epochs. _All tasks use the same settings unless otherwise specified._ All experiments are conducted on a computer workstation with three GeForce GTX 3090 GPUs using the PyTorch deep learning framework. The best model on the validation set is selected for testing.

### Experiments on Global Task

**Parts Segmentation.** As a classic global task, 3D object parts segmentation is an important predecessor for articulated objects from the embodied intelligence community, such as pose estimation [46; 47], manipulation [48; 49], etc. In this section, we conduct experiments on ShapeNet part dataset [50].

From Tab. 2, see them all: we find that our method has almost equivalent performance to the conventional method when being equipped with PointNet, and achieves superior performance on 3DCNN and PointNet++. Treat them equally: we see that our method can often perform better in some categories (_e.g._, car, motor, rocket, etc.), these categories usually have a larger volume (_i.e._, a more sparsified point cloud) compared to other objects. We propose that the inner product within convolutional networks has a tendency to highlight local context among points, yet it is greatly affected by the overall translation and scaling of the dataset. Our method focuses on the points drawn from \(\ell_{1}\)-norm space and addresses this problem by integrating the inherent distance measure into our architecture. Specifically speaking, The Manhattan distance based \(\ell_{1}\)-norm Nets tend to avoid this problem, which notices point cloud features at a longer distance.

### Experiments on Semi-dense Prediction Task

**Scenario Semantic Segmentation.** As a semi-dense prediction task, this task aims to segment distinct regions within a 3D scene based on their semantic meaning using point cloud data. Semantic scene segmentation is crucial for understanding and interpreting the spatial arrangement and relationships between objects in 3D scenes. For our study, we utilize the S3DIS dataset. The metrics and experimental settings follow those outlined in [28].

Following the training and test strategies used in [51], we first divide the point cloud using the room as the basic unit and then sample the room at a size of 1m * 1m (randomly sampling up to 4096

points during training, and all points are involved in the computation during testing), which in turn predicts the class of each point in each block. Note that we use a 9-dimensional vector to represent each point, representing XYZ, RGB, and normalized room location (ranging from 0 to 1). K-fold strategy is also used for training and testing.

The quantitative results are reported in Tab. 3. Experimental results show that although our approach achieves almost equivalent performance to inner product based networks, we maximize the potential of \(\ell_{1}\)-norm measure by relying on our proposed optimization strategy, which allows us to achieve similar performance but with less computational complexity and lower energy consumption (Almost **61%** energy reductions). Also, we provide qualitative segmentation results for visualization in Fig. 4. Overall, our model generates consistent object predictions and is resilient to the presence of absent points and obstructions.

### Experiments of Dense Prediction Task

**Garment Pose Estimation** Garments, vital in daily life, present unique challenges for machine perception and interaction due to properties like infinite degrees of freedom and thin structure. Garment pose estimation and tracking systems hold potential for applications in mixed reality [52; 53], augmented reality [54; 52], and robotic manipulation [55; 49]. Addressing these challenges, mainstream methods typically employ Normalized Object Coordinate Space (NOCS) [56] for **dense prediction tasks**. In this section, we introduce GarmentNets [44], a baseline focusing on garment pose estimation using partial point clouds as input and generating complete point clouds as output. Our approach utilizes the GarmentNets Simulation Dataset to evaluate this task. The total epoch number is 200, and the batch size is 16.

Quantitative results are in Tab. 4. Note that we use Symmetric Chamfer Distance as the metric, This metric measures accuracy and completeness for surface reconstruction. The accuracy metric is defined as the mean L2 distance of points on the output mesh to their nearest neighbors on the GT mesh. From the table, it can be seen that our method performs comparably to the original method.

### Ablation Experiments

**Replacing Means.** The most critical structure of PointNet++ is the 3 separate SetAbstractions modules (SA). Hence, to explore the effect of using the \(\ell_{1}\)-PointNet++ at different places and in different ratios, we remove the modules at different ratios and places on S3DIS. The experimental result is shown in Tab. 5. In many aspects, we can infer that the average mean IOU and accuracy are higher under the 66.7% ratio than those reported under the 33.3% ratio. This result tells the conclusion that our \(\ell_{1}\)-norm measure can exact more useful features from sparse point clouds. we hope these results can prompt further study on replacing means, such as different replacing ratios in

\begin{table}
\begin{tabular}{c|c c c c c c} \hline Model & **Dress** & **Jumpsuit** & **Skirt** & **Top** & **Pants** & **Shirt** \\ \hline \hline GarmentNets [44] & 1.94 & **1.45** & 2.00 & 1.30 & 1.03 & 1.70 \\ \(\ell_{1}\)-GarmentNets & **1.83** & 1.56 & **1.91** & **1.26** & **0.99** & **1.62** \\ \hline \end{tabular}
\end{table}
Table 4: **Quantitative Results on Garment Pose Estimation.** The metric is measured using Chamfer distance (\(cm\)) under the canonical pose. The lower is the better result.

Figure 4: (**Right) Qualitative Results for Semantic Segmentation.** We put the colored point cloud on the top part (Input data) and put semantic segmentation results from the same camera viewpoint on points (Output) in the bottom part.

each inner module, creditable ways to combine hybrid convolutional blocks. etc. We leave this for more passionate researchers in the future.

Optimization Strategy.As demonstrated in Sec. 4, we propose mixed gradient strategy (MGS) to accelerate network convergence, while dynamic learning rate controller (DLC) helps our network move away from local optima. To evaluate the effectiveness of MGS and DLC, we remove them separately from \(\ell_{1}\)-PointNet++ and evaluate the scenario semantic segmentation performance on S3DIS. Tab. 6 presents the quantitative results. The baselines (I and V) indicate that we only use \(\ell_{1}\)-norm as the similarity measurement but without any optimization. It can be observed that they both resulted in huge performance degradation. Besides, we can see that both our MGS and DLC contribute to network convergence and optimization results.

## 6 Limitations and Broader Impact

Firstly, some of the other convolutions (_e.g._, sparse convolution, group convolution, dilated convolution) and additional computer vision tasks remain unexplored. Secondly, the inference speed of the \(\ell_{1}\)-norm Net is marginally slower than that of traditional one. This is attributed to the lack of CUDA and cuDNN optimized operations for Manhattan distance metrics. It's noteworthy that, beyond introducing a novel convolution based on the \(\ell_{p}\)-norm and proving the universal approximation theorem for theoretical support, this paper also presents customized optimization strategies.

## 7 Conclusion

In this paper, we are motivated to explore \(\ell_{p}\)-norm measure to replace the classic inner product convolution. we first prove the universal approximation of \(\ell_{p}\)-norm Nets. And then we compare different \(\ell_{p}\)-norm measures and propose the \(\ell_{1}\)-norm Net for 3D point cloud tasks. Furthermore, we design the customized optimization strategies (_i.e._, mixed gradient strategy and dynamic control on learning rate) for \(\ell_{1}\)-norm Net. When introducing our method to classical 3D networks, they achieve competitive performances at a lower energy cost. In summary, our \(\ell_{1}\)-norm Net can achieve similar performance to traditional convolution network, but with less computational cost and lower instruction latency.

## Acknowledgements

This work was supported in part by National Natural Science Foundation of China under Grant 62302143 and Anhui Provincial Natural Science Foundation under Grant 2308085QF207. Thanks for the help of Xinyuan Song.

\begin{table}
\begin{tabular}{c|c c|c|c} \hline \hline \multirow{2}{*}{**Index**} & \multicolumn{2}{c|}{**Optimization?**} & \multicolumn{2}{c}{**Mean**} & \multicolumn{1}{c}{**Overall**} \\ \cline{2-5}  & MGS & DLC & **IOU (\%)** & **Accuracy (\%)** \\ \hline \hline I (Vanilla) & & & 33.2\% & 56.3\% \\ II & ✓ & & 39.6\% & 68.6\% \\ III & ✓ & ✓ & 42.8\% & 70.1\% \\ IV (Ours) & ✓ & ✓ & 47.6\% & 77.9\% \\ \hline V (Vanilla) & & & 38.9\% & 55.6\% \\ VI & ✓ & & 43.6\% & 69.3\% \\ VII & ✓ & ✓ & 48.4\% & 75.6\% \\ VIII (Ours) & ✓ & ✓ & 53.9\% & 82.9\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Ablation Results on S3DIS Dataset Using Different Variants of \(\ell_{1}\)-Nets.** Mean IOU and overall Accuracy (%) are reported. Note that the results of \(\ell_{1}\)-PointNet are reported from I to IV, and \(\ell_{1}\)-PointNet++ are reported from V to VIII. Besides, vanilla Net represents the model without our customized optimization strategy while training.

\begin{table}
\begin{tabular}{c|c c c|c|c|c} \hline \hline \multirow{2}{*}{**Replacing Ratio**} & \multicolumn{2}{c|}{\(\ell_{1}\)**-norm neuron?**} & \multicolumn{1}{c|}{**Mean**} & \multirow{2}{*}{**Accuracy**} & \multirow{2}{*}{**Info**} \\  & SA1 & SA2 & SA3 & & & \\ \hline \hline \multirow{3}{*}{33.3\%} & ✓ & & & 51.9\% & 79.8\% \\  & & ✓ & & 52.3\% & 81.8\% \\ \cline{1-1} \cline{2-7}  & & ✓ & 52.5\% & 81.1\% \\ \hline \multirow{3}{*}{66.7\%} & ✓ & ✓ & 53.2\% & 82.4\% \\  & ✓ & ✓ & & 53.0\% & 81.0\% \\ \cline{1-1}  & ✓ & ✓ & & 52.2\% & 81.5\% \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Comparisons of Results on S3DIS with Different Replacing Ratio and Places**. We estimate the energy costs according to [11], _i.e._, one operation of floating-point addition and multiplication have energy costs of 0.9 \(pJ\) and 3.7 \(pJ\), respectively. SA: SetAbstractions module. �\(\blackstar\) means that 33.3% replacing ratio of PointNet++ has #Add-0.492 M, #Mul-0.984 M, Energy-4.0836 \(\mu J\), while �\(\blacktriangle\) means that 66.7% replacing ratio of PointNet++ has #Add-0.984 M, #Mul-0.492 M, Energy-2.7060 \(\mu J\).

## References

* [1] Min Zhang, Yifan Wang, Pranav Kadam, Shan Liu, and C-C Jay Kuo. Pointhop++: A lightweight learning model on point sets for 3d classification. In _2020 IEEE International Conference on Image Processing (ICIP)_, pages 3319-3323. IEEE, 2020.
* [2] Haozhe Qi, Chen Feng, Zhiguo Cao, Feng Zhao, and Yang Xiao. P2b: Point-to-box network for 3d object tracking in point clouds. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 6329-6338, 2020.
* [3] Angela Dai and Matthias Niessner. 3dmv: Joint 3d-multi-view prediction for 3d semantic scene segmentation. In _Proceedings of the European Conference on Computer Vision (ECCV)_, pages 452-468, 2018.
* [4] Qiaojun Yu, Junbo Wang, Wenhai Liu, Ce Hao, Liu Liu, Lin Shao, Weiming Wang, and Cewu Lu. Gamma: Generalizable articulation modeling and manipulation for articulated objects. _arXiv preprint arXiv:2309.16264_, 2023.
* [5] Haoyu Xiong, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, and Cewu Lu. Robotube: Learning household manipulation from human videos with simulated twin environments. In _Conference on Robot Learning_, pages 1-10. PMLR, 2023.
* [6] Fan Zhang and Yiannis Demiris. Learning garment manipulation policies toward robot-assisted dressing. _Science robotics_, 7(65):eabm6010, 2022.
* [7] Kailin Li, Lixin Yang, Haoyu Zhen, Zenan Lin, Xinyu Zhan, Licheng Zhong, Jian Xu, Kejian Wu, and Cewu Lu. Chord: Category-level hand-held object reconstruction via shape deformation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9444-9454, 2023.
* [8] Marius Kloft, Ulf Brefeld, Soren Sonnenburg, and Alexander Zien. Lp-norm multiple kernel learning. _The Journal of Machine Learning Research_, 12:953-997, 2011.
* [9] Qingsong Gu and Po-Lam Yung. A new formula for the lp norm. _Journal of Functional Analysis_, 281(4):109075, 2021.
* [10] James A Cadzow. Minimum \(\ell_{1}\), \(\ell_{2}\), and \(\ell_{\infty}\) norm approximate solutions to an overdetermined system of linear equations. _Digital Signal Processing_, 12(4):524-560, 2002.
* [11] William Dally. High-performance hardware for machine learning. _Nips Tutorial_, 2:3, 2015.
* [12] Instruction latencies of different operations. www.agner.org/optimize/instruction_tables.pdf.
* [13] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [14] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. _arXiv preprint arXiv:1704.04861_, 2017.
* [15] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1251-1258, 2017.
* [16] Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. _arXiv preprint arXiv:1511.07122_, 2015.
* [17] Dren Gashi, Mike Pereira, and Valeriia Vterkovska. Multi-scale context aggregation by dilated convolutions machine learning-project. 2017.
* [18] Xin Wang, Rongrong Lv, Yang Zhao, Tangwen Yang, and Qiuqi Ruan. Multi-scale context aggregation network with attention-guided for crowd counting. In _2020 15th IEEE International Conference on Signal Processing (ICSP)_, volume 1, pages 240-245. IEEE, 2020.

* Dai et al. [2017] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In _Proceedings of the IEEE international conference on computer vision_, pages 764-773, 2017.
* Chen et al. [2021] Feng Chen, Fei Wu, Jing Xu, Guangwei Gao, Qi Ge, and Xiao-Yuan Jing. Adaptive deformable convolutional network. _Neurocomputing_, 453:853-864, 2021.
* Zou and Hastie [2005] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 67(2):301-320, 2005.
* Chung and Gazzola [2019] Julianne Chung and Silvia Gazzola. Flexible krylov methods for \(\backslash\)\(ell\_p\) regularization. _SIAM Journal on Scientific Computing_, 41(5):S149-S171, 2019.
* Labbe et al. [2021] Martine Labbe, Justo Puerto, and Moises Rodriguez-Madrena. Shortest paths and location problems in a continuous framework with different \(\backslash\)\(ell\_p\)-norms on different regions. _arXiv preprint arXiv:2110.07866_, 2021.
* Shen and Mousavi [2018] Jinglai Shen and Seyedahmad Mousavi. Least sparsity of p-norm based optimization problems with p>1. _SIAM Journal on Optimization_, 28(3):2721-2751, 2018.
* Sastry and Mishra [2009] CS Sastry and Ashish Mishra. Application of l1-norm minimization technique to image retrieval. _World Academy of Science, Engineering and Technology_, 56(145):801-804, 2009.
* Van Laarhoven [2017] Twan Van Laarhoven. L2 regularization versus batch and weight normalization. _arXiv preprint arXiv:1706.05350_, 2017.
* Boyd and Balakrishnan [1990] Stephen Boyd and Venkataramanan Balakrishnan. A regularity result for the singular values of a transfer matrix and a quadratically convergent algorithm for computing its \(\ell_{\infty}\)-norm. _Systems & Control Letters_, 15(1):1-7, 1990.
* Qi et al. [2017] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. _Advances in neural information processing systems_, 30, 2017.
* Hornik et al. [1989] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. _Neural networks_, 2(5):359-366, 1989.
* Van-Polynomial Activation Functions Can [1991] Non-Polynomial Activation Functions Can. Multilayer feedforward networks with non-polynomial activation functions can approximate any function--. 1991.
* Schafer and Zimmermann [2006] Anton Maximilian Schafer and Hans Georg Zimmermann. Recurrent neural networks are universal approximators. In _Artificial Neural Networks-ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16_, pages 632-640. Springer, 2006.
* Yun et al. [2019] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* Horowitz [2014] Mark Horowitz. 1.1 computing's energy problem (and what we can do about it). In _2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC)_, pages 10-14. IEEE, 2014.
* Brown and Sandholm [2014] Noam Brown and Tuomas Sandholm. Regret transfer and parameter optimization. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 28, 2014.
* Fei et al. [2020] Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization in non-stationary environments. _Advances in Neural Information Processing Systems_, 33:6743-6754, 2020.
* Wang et al. [2018] Zi Wang, Beomjoon Kim, and Leslie P Kaelbling. Regret bounds for meta bayesian optimization with an unknown gaussian process prior. _Advances in Neural Information Processing Systems_, 31, 2018.

* [37] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* [38] Shai Shalev-Shwartz et al. Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2):107-194, 2012.
* [39] Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* [40] Elad Hazan. 10 the convex optimization approach to regret minimization. _Optimization for machine learning_, page 287, 2012.
* [41] Jeremy Bernstein, Kamyar Azizzadenesheli, Yu-Xiang Wang, and Anima Anandkumar. Convergence rate of sign stochastic gradient descent for non-convex functions. 2018.
* [42] Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, and Zhenxun Zhuang. Robustness to unbounded smoothness of generalized signsgd. _Advances in Neural Information Processing Systems_, 35:9955-9968, 2022.
* [43] Hanting Chen, Yunhe Wang, Chunjing Xu, Boxin Shi, Chao Xu, Qi Tian, and Chang Xu. Addernet: Do we really need multiplications in deep learning? In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1468-1477, 2020.
* [44] Cheng Chi and Shuran Song. Garmentnets: Category-level pose estimation for garments via canonical space shape completion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3324-3333, 2021.
* [45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. 2019.
* [46] Liu Liu, Han Xue, Wenqiang Xu, Haoyuan Fu, and Cewu Lu. Toward real-world category-level articulation pose estimation. _IEEE Transactions on Image Processing_, 31:1072-1083, 2022.
* [47] Xiaolong Li, He Wang, Li Yi, Leonidas J Guibas, A Lynn Abbott, and Shuran Song. Category-level articulated object pose estimation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 3706-3715, 2020.
* [48] Jianren Wang, Sudeep Dasari, Mohan Kumar Srirama, Shubham Tulsiani, and Abhinav Gupta. Manipulate by seeing: Creating manipulation controllers from pre-trained representations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3859-3868, 2023.
* [49] Yezhou Yang, Yi Li, Cornelia Fermuller, and Yiannis Aloimonos. Robot learning manipulation action plans by" watching" unconstrained videos from the world wide web. In _Proceedings of the AAAI conference on artificial intelligence_, volume 29, 2015.
* [50] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. _ACM Transactions on Graphics (ToG)_, 35(6):1-12, 2016.
* [51] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 652-660, 2017.
* [52] Julie Carmigniani and Borko Furht. Augmented reality: an overview. _Handbook of augmented reality_, pages 3-46, 2011.
* [53] Dhiraj Amin and Shvarari Govilkar. Comparative study of augmented reality sdks. _International Journal on Computational Science & Applications_, 5(1):11-26, 2015.
* [54] Ronald T Azuma. A survey of augmented reality. _Presence: teleoperators & virtual environments_, 6(4):355-385, 1997.

* [55] Alper Canberk, Cheng Chi, Huy Ha, Benjamin Burchfiel, Eric Cousineau, Siyuan Feng, and Shuran Song. Cloth funnels: Canonicalized-alignment for multi-purpose garment manipulation. In _2023 IEEE International Conference on Robotics and Automation (ICRA)_, pages 5872-5879. IEEE, 2023.
* [56] He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2642-2651, 2019.
* [57] Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks. _Neural computation_, 3(2):246-257, 1991.
* [58] H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. _arXiv preprint arXiv:1002.4908_, 2010.

- Appendix -

In the Appendix, we present additional information on our methods. Concretely, we provides a detailed theoretical analysis of the theorems from the main paper, including the variance analysis, the proposed universal approximation, the regret argument, and the equivalence of \(\ell_{2}\)-norm Measure.

## Appendix A Additional Theoretical Analysis

### Omitted proof of variance analysis

Since adding a constant does not significantly affect variance, for ease of demonstration, we could assume \(Var\big{[}\|G+P_{t}-K\|_{2}\big{]}\approx Var\big{[}\|G\|_{2}\big{]}\). Notice that

\[Var\big{[}\|G\|_{2}\big{]}=\mathbb{E}_{G\sim N(0,\mathbb{I}_{m})}\big{[}\|G\|_ {2}^{2}\big{]}-\bigg{(}\mathbb{E}_{G\sim N(0,\mathbb{I}_{m})}\big{[}\|G\|_{2} \big{]}\bigg{)}^{2}.\] (16)

It's easy to verify that for any \(u\geq 0\),

\[\sqrt{u}\geq(1+u-(u-1)^{2})/2.\]

Let \(u=\frac{\|G\|_{2}^{2}}{m}\) and calculate the expectation of \(G\) on both sides of the inequality, we have

\[\frac{\mathbb{E}[\|G\|_{2}]}{\sqrt{m}}\geq\frac{1}{2}\cdot\big{(}2-\mathbb{E }\Big{[}\big{(}\frac{\|G\|_{2}^{2}}{m}-1\big{)}^{2}\Big{]}\big{)}.\] (17)

Because \(\mathbb{E}\big{[}(\frac{\|G\|_{2}^{2}}{m}-1)^{2}\big{]}=\frac{1}{m^{2}}\cdot \mathbb{E}\big{[}\sum_{i=1}^{m}(G(i)^{2}-1)^{2}+\sum_{i\neq j}(G(i)^{2}-1)(G(j) ^{2}-1)\big{]}\) and \(\forall i\), \(\mathbb{E}[G(i)^{2}-1]=0\), we can conclude that

\[\mathbb{E}\big{[}(\frac{\|G\|_{2}^{2}}{m}-1)^{2}\big{]} =\frac{1}{m}\cdot\mathbb{E}[G(1)^{4}+1-2\cdot G(1)^{2}]\] (18a) \[=\frac{2}{m}.\] (18b)

where the first equation holds for all the \(G(i)\)s are i.i.d. The second equation holds for \(\mathbb{E}[G(1)^{4}]=3\), \(\mathbb{E}[G(1)^{2}]=1\). Combining inequality (17) and Equation (18b), \(\mathbb{E}_{G\sim\mathcal{N}(0,\mathbb{I}_{m})}\left[\|G\|_{2}\right]\geq\frac {\sqrt{m}}{2}\cdot\big{(}2-\frac{2}{m}\big{)}\). Therefore, by Equation (16) we have

\[Var[\|G\|_{2}]<2-\frac{1}{m}=O(1).\]

Thus we have shown that \(Var\big{[}\|(G+P_{t})-K\|_{2}\big{]}=O(1)\).

### Proof of Theorem 1

Scaling \(S\) and \(J\) by \(\frac{1}{diam(J)}\) where \(diam(J)=\max_{x,y\in J}\{\|x-y\|_{\infty}\}\), we could assume \(S=\{x_{1},\cdots,x_{N}\}\) and \(\forall i\in[N]\)\(x_{i}\in J\subset[0,1]^{k}\). For convenience, first we show the case \(k=1\). Here we refer the construction of soft occupancy function in [51]. Because \(f\) is continuous function, for any \(\epsilon>0\), \(\exists\ \sigma>0\) so that \(|f(S_{1})-f(S_{2})|<\epsilon\) for any \(S_{1}\) and \(S_{2}\) with \(d_{H}(S_{1},S_{2})<\delta\). Let \(M=\big{\lceil}\frac{1}{\delta}\big{\rceil}\) and \(h_{m}(x)=exp(-d_{H}(x,[\frac{m-1}{M},\frac{m}{M}]))\) be the soft occupancy function, for all \(m\in[M]\). Next, for all \(m\in[M]\) define

\[\hat{v}_{m}(S)=\max_{x\in S}\{h_{m}(x)\}\] (19)

and,

\[v_{m}(S)=\left\{\begin{array}{ll}1,&\hat{v}_{m}(S)\geq 1\\ 0,&\hat{v}_{m}(S)<1.\end{array}\right.\] (20)\(v_{m}(S)\) indicates the occupancy of the \(m\)-th interval by points in \(S\). Define \(\mathbf{v}:2^{J}\rightarrow\{0,1\}^{M}\) and for any \(S\in 2^{J}\), \(\mathbf{v}(S)=(v_{1}(S),v_{2}(S),\cdots,v_{M}(S))\). And then define \(\eta:\{0,1\}^{M}\to 2^{J}\), \(\eta(\mathbf{v}(S))=\{\frac{m-1}{M}\mid v_{m}(S)\geq 1\}\). Notice that by this construction, \(d_{H}\left(\eta(\mathbf{v}(S)),S\right)<\frac{1}{M}\leq\delta\). So let \(\omega:\{0,1\}^{M}\rightarrow\mathbb{R}\) and \(\omega(\mathbf{v})=f(\eta(\mathbf{v}))\), we have

\[|\omega(\mathbf{v}(S))-f(S)|=|f(\eta(\mathbf{v}(S)))-f(S)|<\epsilon\] (21)

The last inequality holds for the definition of Hausdorff distance and continuity of \(f\). Here \(\omega\) and \(\{h_{m}\}_{m=1}^{M}\) could be made up of a multi-layer perceptron network [51]. \(\{\hat{v}_{m}\}_{m=1}^{M}\) consist of a max pooling layer on \(\{h_{m}\}_{m=1}^{M}\) and \(\{v_{m}\}_{m=1}^{M}\) can be composed of a simple perceptron layer on \(\{\hat{v}_{m}\}_{m=1}^{M}\), which compares \(\hat{v}_{m}(S)\) and \(1\). For the general cases \(k\geq 1\), it suffices to get the same conclusion by simply extending the \(1\) dimensional functions \(h_{m}\), \(\hat{v}_{m}\), \(v_{m}\) to \(k\) dimension. So there is a \(\ell_{p}\)-PointNet++ \(\mathcal{P}\) that can approximate any continuous function \(f\) on \(2^{J}\).

We employ the RBF theory of [57] to give the second conclusion. For completeness, we restate it here.

**Theorem 3** ([57]).: _The radio basis networks consist of a family of functions(RBF) noted by \(S_{K}\):_

\[\sum_{i=1}^{H}a_{i}\cdot K(\frac{x-z_{i}}{\sigma})\]

_where \(x\in\mathbb{R}^{d}\), \(z_{i}\in\mathbb{R}^{d}\),\(\sigma\in\mathbb{R}\), \(H\in\mathcal{N}\). \(S_{K}\) is dense in \(\ell_{1}(\mathbb{R}^{d})\), if \(K\) satisfies: 1.integrable bounded, 2.\(K\) is continuous almost everywhere, 3.\(\int K(x)dx\neq 0\)._

It's clear that the \(\ell_{p}\)-norm \(\|\cdot\|_{p}:\mathbb{R}^{d}\rightarrow\mathbb{R}\) satisfies all the three conditions on \(K\). Besides, a large enough \(\ell_{p}\) based convolution layer with a full connected layer could represent all the functions \(\sum_{i=1}^{H}a_{i}\cdot\|(\frac{x-z_{i}}{\sigma})\|_{p}\). So for any \(\ell_{1}\)-integrable function \(g\), there exists an \(\ell_{p}\)-PointNet++ \(\mathcal{P}^{\prime}\) such that for any \(\epsilon>0\), \(\int|g(x)-\mathcal{P}^{\prime}(x)|dx<\epsilon\).

### Proof of Theorem 2

Before proving, we restate an important result in online learning and we will use it in the following.

**Lemma 1** ([58]).: _For any \(Q\in\mathcal{S}_{+}^{d}\) and convex feasible set \(\mathcal{F}\subset\mathbb{R}^{d}\), suppose \(u_{1}=\min_{x\in\mathcal{F}}\|Q^{1/2}(x-z_{1})\|\) and \(u_{2}=\min_{x\in\mathcal{F}}\|Q^{1/2}(x-z_{2})\|\) then we have \(\|Q^{1/2}(u_{1}-u_{2})\|\leq\|Q^{1/2}(z_{1}-z_{2})\|\)._

Our proof framework is similar to that of [58]. Here is a standard argument in momnet method.

**Lemma 2**.: _Suppose \(m_{t}=\gamma m_{t-1}+(1-\gamma)g_{t}\) with \(m_{0}=\mathbf{0}\) and \(0<\gamma<1\). We have_

\[\sum_{t=1}^{T^{*}}\|m_{t}\|^{2}\leq\sum_{t=1}^{T^{*}}\|g_{t}\|^{2}.\]

Proof.: By Cauchy-Schwarz and Young's inequality, we have

\[\|m_{t}\|^{2}\leq\gamma\|m_{t-1}\|^{2}+(1-\gamma)\|g_{t}\|^{2}.\]

Note that \(m_{0}=\mathbf{0}\),

\[\frac{\|m_{t}\|^{2}}{\gamma^{t}}\leq(1-\gamma)\sum_{i=1}^{t}\|g_{i}\|^{2} \gamma^{-i}.\]

So we have

\[\|m_{t}\|^{2}\leq(1-\gamma)\sum_{i=1}^{t}\|g_{i}\|^{2}\gamma^{t-i}.\]

Take the summation on \(t\) for both sides of the inequality, we have the conclusion.

[MISSING_PAGE_FAIL:17]

The second inequality holds for Jensen inequality and the third inequality follows from Cauchy-Schwarz inequality. The forth inequalityholds for Lemma 2.

Combine the argument above and notice that \(\alpha(t)^{-1}\leq p_{1}^{-1}\cdot\sqrt{T^{*}}\), we have

\[R_{T^{*}}\leq\frac{B_{\infty}^{2}}{2(1-q_{1})}\Bigg{[}n\cdot\alpha _{1}^{-1}+\sum_{t=2}^{T^{*}}n\cdot(\alpha_{t}^{-1}-\alpha_{t-1}^{-1})+\sum_{t= 1}^{T^{*}}n\cdot q_{t}\cdot\alpha_{t}^{-1}\Bigg{]}+(2\sqrt{T^{*}}-1)\frac{ \alpha_{2}(1)B_{2}^{2}}{1-q_{1}}\] \[\leq\sqrt{T^{*}}\cdot\left(\frac{B_{\infty}^{2}}{2(1-q_{1})} \cdot n\cdot\hat{\alpha}_{T^{*}}^{-1}+\frac{2\cdot\alpha_{2}(1)B_{2}^{2}}{1-q _{1}}\right)-\frac{\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}+\frac{B_{\infty}^{2}}{2(1- q_{1})}\sum_{t=1}^{T^{*}}n\cdot q_{t}\cdot\alpha_{t}^{-1}\] \[\leq\sqrt{T^{*}}\cdot\left(\frac{B_{\infty}^{2}\cdot n\cdot p_{1 }^{-1}}{2(1-q_{1})}\cdot(1+2q_{0}q)+\frac{2\cdot\alpha_{2}(1)B_{2}^{2}}{1-q_{ 1}}\right)-\frac{\alpha_{2}(1)B_{2}^{2}}{1-q_{1}}.\]

### Equivalence of \(\ell_{2}\)-norm Measure and Classic Convolution in Convergence

We find that \(\ell_{2}\)-norm Net is the linear transformation to the inner product convolution network, here we give the detailed calculation.

The output of the \(\ell_{2}\)-norm Net in Eq. 25.

\[Y_{\ell_{2}}(P_{t},K)=\sqrt{\sum_{t\geq 1}\sum_{i,j}|P_{t}(i,j)-K(i,j)|^{2}}\] (25)

Therefore, we can express it as the following:

\[Y_{\ell_{2}}^{2}(P_{t},K)= \sum_{t\geq 1}\sum_{i,j}(P_{t}(i,j)^{2}+K(i,j)^{2}-2P_{t}(i,j)K(i,j))\] (26) \[= \sum_{t\geq 1}\sum_{i,j}(P_{t}(i,j)^{2}+K(i,j)^{2})-\sum_{t\geq 1 }\sum_{i,j}P_{t}(i,j)K(i,j)\] \[= \sum_{t\geq 1}\sum_{i,j}(P_{t}(i,j)^{2}+K(i,j)^{2})-2Y_{CNN}(P_{t},K).\]

Notably, the term \(\sum_{i,j}K(i,j)^{2}\) remains constant for each channel, and \(\sum_{t\geq 1}\sum_{i,j}P_{t}(i,j)^{2}\) represents the square of \(\ell_{2}\)-norm of each input patch. If this term is invariant across patches, the \(\ell_{2}\)-norm Net's output can be regarded as a linear transformation of the CNNs' output.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: see the _abstract_ and _introduction_ part. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: see Sec. 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: see Sec.A.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Instructions about experimental settings are in Sec. 5, and the URL of the project will be released after the paper is accepted. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: see the zip files of codes in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: see the main manuscript. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: the effect of random seed could almost be negligible since we set the same initiation seed during experiments. Reproducibility can be guaranteed. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: see the main manuscript. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: we have conformed with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: see Sec. 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This work poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: see the main manuscript. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This work does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This work does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.