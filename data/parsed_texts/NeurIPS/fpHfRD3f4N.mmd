Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function Approximation: Minimax Optimal and Instance-Dependent Regret Bounds

 Jiayi Huang\({}^{1}\)  Han Zhong\({}^{1}\)  Liwei Wang\({}^{1,2}\)  Lin F. Yang\({}^{3}\)

\({}^{1}\)Center for Data Science, Peking University

\({}^{2}\)National Key Laboratory of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{3}\)University of California, Los Angles

{jjyhuang, hanzhong}@stu.pku.edu.cn,

wanglu@cis.pku.edu.cn, linyang@ee.ucla.edu

###### Abstract

While numerous works have focused on devising efficient algorithms for reinforcement learning (RL) with uniformly bounded rewards, it remains an open question whether sample or time-efficient algorithms for RL with large state-action space exist when the rewards are _heavy-tailed_, i.e., with only finite \((1+\epsilon)\)-th moments for some \(\epsilon\in(0,1]\). In this work, we address the challenge of such rewards in RL with linear function approximation. We first design an algorithm, Heavy-OFUL, for heavy-tailed linear bandits, achieving an _instance-dependent_\(T\)-round regret of \(\widetilde{O}\big{(}dT^{\frac{1-\epsilon}{2(1+\epsilon)}}\sqrt{\sum_{t=1}^{T} \nu_{t}^{2}}+dT^{\frac{1-\epsilon}{2(1+\epsilon)}}\big{)}\), the _first_ of this kind. Here, \(d\) is the feature dimension, and \(\nu_{t}^{1+\epsilon}\) is the \((1+\epsilon)\)-th central moment of the reward at the \(t\)-th round. We further show the above bound is minimax optimal when applied to the worst-case instances in stochastic and deterministic linear bandits. We then extend this algorithm to the RL settings with linear function approximation. Our algorithm, termed as Heavy-LSVI-UCB, achieves the _first_ computationally efficient _instance-dependent_\(K\)-episode regret of \(\widetilde{O}(d\sqrt{H\mathcal{U}^{*}}K^{\frac{1}{1+\epsilon}}+d\sqrt{H\mathcal{ V}^{*}}K)\). Here, \(H\) is length of the episode, and \(\mathcal{U}^{*}\), \(\mathcal{V}^{*}\) are instance-dependent quantities scaling with the central moment of reward and value functions, respectively. We also provide a matching minimax lower bound \(\Omega(dHK^{\frac{1}{1+\epsilon}}+d\sqrt{H^{3}K})\) to demonstrate the optimality of our algorithm in the worst case. Our result is achieved via a novel robust self-normalized concentration inequality that may be of independent interest in handling heavy-tailed noise in general online regression problems.

## 1 Introduction

Designing efficient reinforcement learning (RL) algorithms for large state-action space is a significant challenge within the RL community. A crucial aspect of RL is understanding the reward functions, which directly impacts the quality of the agent's policy. In certain real-world situations, reward distributions may exhibit heavy-tailed behavior, characterized by the occurrence of extremely large values at a higher frequency than expected in a normal distribution. Examples include image noise in signal processing [14], stock price fluctuations in financial markets [12; 18], and value functions in online advertising [11; 19]. However, much of the existing RL literature assumes rewards to be either uniformly bounded or light-tailed (e.g., sub-Gaussian). In such light-tailed settings, the primary challenge lies in learning the transition probabilities, leading most studies to assume deterministic rewards for ease of analysis [5; 22; 15]. As we will demonstrate, the complexity of learning reward functions may dominate in heavy-tailed settings. Consequently, the performance of traditional algorithms may decline, emphasizing the need for the development of new, efficient algorithms specifically designed to handle heavy-tailed rewards.

Heavy-tailed distributions have been extensively studied in the field of statistics [9, 27] and in more specific online learning scenarios, such as bandits [7, 28, 31, 34, 40]. However, there is a dearth of theoretical research in RL concerning heavy-tailed rewards, whose distributions only admit finite \((1+\epsilon)\)-th moment for some \(\epsilon\in(0,1]\). One notable exception is Zhuang and Sui [44], which made a pioneering effort in establishing worst-case regret guarantees in _tabular_ Markov Decision Processes (MDPs) with heavy-tailed rewards. However, their algorithm cannot handle RL settings with large state-action space. Moreover, their reliance on truncation-based methods is sub-optimal as these methods heavily depend on raw moments, which do not vanish in deterministic cases. Therefore, a natural question arises:

_Can we derive sample and time-efficient algorithms for RL with large state-action space that achieve instance-dependent regret in the presence of heavy-tailed rewards?_

In this work, we focus on linear MDPs [35, 22] with heavy-tailed rewards and answer the above question affirmatively. We say a distribution is _heavy-tailed_ if it only admits finite \((1+\epsilon)\)-th moment for some \(\epsilon\in(0,1]\). Our contributions are summarized as follows.

* We first propose a computationally efficient algorithm Heavy-OFUL for heavy-tailed linear bandits. Such a setting can be regarded as a special case of linear MDPs. Heavy-OFUL achieves an _instance-dependent_\(T\)-round regret of \(\widetilde{O}\big{(}dT^{\frac{1-\epsilon}{2(1+\epsilon)}}\sqrt{\sum_{t=1}^{T }\nu_{t}^{2}+dT^{\frac{1-\epsilon}{2(1+\epsilon)}}}\big{)}\), the _first_ of this kind. Here \(d\) is the feature dimension and \(\nu_{t}^{1+\epsilon}\) is the \((1+\epsilon)\)-th central moment of the reward at the \(t\)-th round. The instance-dependent regret bound has a main term that only depends on the summation of central moments, and therefore does not have a \(\sqrt{T}\) term. Our regret bound is shown to be minimax optimal in both stochastic and deterministic linear bandits (See Remark 4.2 for details).
* We then extend this algorithm to time-inhomogeneous linear MDPs with heavy-tailed rewards, resulting in a new computationally efficient algorithm Heavy-LSVI-UCB, which achieves a \(K\)-episode regret scaling as \(\widetilde{O}(d\sqrt{H\mathcal{U}^{s}}K^{\frac{1}{1+\epsilon}}+d\sqrt{H \mathcal{V}^{s}}K)\) for the _first_ time. Here, \(H\) is the length of the episode and \(\mathcal{U}^{\star},\mathcal{V}^{\star}\) are quantities measuring the central moment of the reward functions and transition probabilities, respectively (See Theorem 5.2 for details). Our regret bound is _instance-dependent_ since the main term only relies on the instance-dependent quantities, which vanishes when the dynamics and rewards are deterministic. When specialized to special cases, our instance-dependent regret recovers the variance-aware regret in Li and Sun [26] (See Remark 5.3 for details) and improves existing first-order regret bounds [33, 26] (See Corollary 5.6 for details).
* We provide a minimax regret lower bound \(\Omega(dHK^{\frac{1}{1+\epsilon}}+d\sqrt{H^{3}K})\) for linear MDPs with heavy-tailed rewards, which matches the worst-case regret bound implied by our instance-dependent regret, thereby demonstrating the minimax optimality of Heavy-LSVI-UCB in the worst case.

For better comparisons between our algorithms and state-of-the-art results, we summarize the regrets in Table 1 and 2 for linear bandits and linear MDPs, respectively. More related works are deferred to Appendix A. Remarkably, our results demonstrate that \(\epsilon=1\) (i.e. finite variance) is sufficient to obtain variance-aware regret bounds of the same order as the case where rewards are uniformly bounded for both linear bandits and linear MDPs. The main technique contribution behind our results is a novel robust self-normalized concentration inequality inspired by Sun et al. [32]. To be more specific, it is a non-trivial generalization of adaptive Huber regression from independent and identically distributed (i.i.d.) case to heavy-tailed online regression settings and gives a _self-normalized_ bound instead of the \(\ell_{2}\)-norm bound in Sun et al. [32]. Our result is computationally efficient and only scales with the feature dimension, \(d\), \((1+\epsilon)\)-th _central_ moment of the noise, \(\nu\), and does not depend on the absolute magnitude as in other self-normalized concentration inequalities [42, 41].

Road MapThe rest of the paper is organized as follows. Section 2 introduces heavy-tailed linear bandits and linear MDPs. Section 3 presents the robust self-normalized concentration inequality for general online regression problems with heavy-tailed noise. Section 4 and 5 give the main results for heavy-tailed linear bandits and linear MDPs, respectively. We then conclude in Section 6. Related work, experiments and all proofs can be found in Appendix.

NotationsLet \(\|\bm{a}\|:=\|\bm{a}\|_{2}\). Let \([t]:=\{1,2,\ldots,t\}\). Let \(\mathcal{B}_{d}(r):=\{\bm{x}\in\mathbb{R}^{d}|\|\bm{x}\|\leq r\}\). Let \(x_{[a,b]}:=\max\{a,\min\{x,b\}\}\) denote the projection of \(x\) onto the close interval \([a,b]\). Let \(\sigma(\{X_{s}\}_{s\in[t]})\) be the \(\sigma\)-field generated by random vectors \(\{X_{s}\}_{s\in[t]}\).

## 2 Preliminaries

### Heavy-Tailed Linear Bandits

**Definition 2.1** (Heterogeneous linear bandits with heavy-tailed rewards).: Let \(\{\mathcal{D}_{t}\}_{t\geq 1}\) denote a series of fixed decision sets, where all \(\bm{\phi}_{t}\in\mathcal{D}_{t}\) satisfy \(\|\bm{\phi}_{t}\|\leq L\) for some known upper bound \(L\). At each round \(t\), the agent chooses \(\bm{\phi}_{t}\in\mathcal{D}_{t}\), then receives a reward \(R_{t}\) from the environment. We define the filtration \(\{\mathcal{F}_{t}\}_{t\geq 1}\) as \(\mathcal{F}_{t}=\sigma(\{\bm{\phi}_{s},R_{s}\}_{s\in[t]}\cup\{\bm{\phi}_{t+1}\})\) for any \(t\geq 1\). We assume \(R_{t}=\langle\bm{\phi}_{t},\bm{\theta}^{*}\rangle+\varepsilon_{t}\) with the unknown coefficient \(\bm{\theta}^{*}\in\mathcal{B}_{d}(B)\) for some known upper bound \(B\). The random variable \(\varepsilon_{t}\in\mathbb{R}\) is \(\mathcal{F}_{t}\)-measurable and satisfies \(\mathbb{E}[\varepsilon_{t}|\mathcal{F}_{t-1}]=0,\mathbb{E}[|\varepsilon_{t}| ^{1+\epsilon}|\mathcal{F}_{t-1}]=\nu_{t}^{1+\epsilon}\) for some \(\epsilon\in(0,1]\) with \(\nu_{t}\) being \(\mathcal{F}_{t-1}\)-measurable.

The agent aims to minimize the \(T\)-round _pseudo-regret_ defined as \(\mathrm{Regret}(T)=\sum_{t=1}^{T}[\langle\bm{\phi}_{t}^{*},\bm{\theta}^{*} \rangle-\langle\bm{\phi}_{t},\bm{\theta}^{*}\rangle]\), where \(\bm{\phi}_{t}^{*}=\mathrm{argmax}_{\bm{\phi}\in\mathcal{D}_{t}}\langle\bm{ \phi},\bm{\theta}^{*}\rangle\).

### Linear MDPs with Heavy-Tailed Rewards

We use a tuple \(M=M(\mathcal{S},\mathcal{A},H,\{R_{h}\}_{h\in[H]},\{\mathbb{P}_{h}\}_{h\in[H]})\) to describe the _time-inhomogeneous finite-horizon MDP_, where \(\mathcal{S}\) and \(\mathcal{A}\) are state space and action space, respectively, \(H\) is the length of the episode, \(R_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) is the random reward function with expectation \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), and \(\mathbb{P}_{h}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition probability function. More details can be found in Puterman [30]. A time-dependent _policy_\(\pi=\{\pi_{h}\}_{h\in H}\) satisfies \(\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) for any \(h\in[H]\). When the policy is deterministic, we use \(\pi_{h}(s_{h})\) to denote the action chosen at the \(h\)-th step given \(s_{h}\) by policy \(\pi\). For any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), we define the _state-action value function_\(Q_{h}^{\pi}(s,a)\) and _state value function_\(V_{h}^{\pi}(s)\) as follows: \(Q_{h}^{\pi}(s,a)=\mathbb{E}\left[\sum_{h^{\prime}=h}^{H}r(s_{h^{\prime}},a_{h^{ \prime}})|s_{h}=s,a_{h}=a\right]\), \(V_{h}^{\pi}(s)=Q_{h}^{\pi}(s,\pi_{h}(s))\), where the expectation is taken with respect to the transition probability of

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline
**Algorithm** & **Regret** & \begin{tabular}{c} **Dataset** \\ **Dependent** \\ \end{tabular} & \begin{tabular}{c} **First** \\ **Order** \\ \end{tabular} & \begin{tabular}{c} **Multiway** \\ **Optimal** \\ \end{tabular} & \begin{tabular}{c} **Computa** \\ **Combat** \\ **Efficient** \\ \end{tabular} & \begin{tabular}{c} **Heavy- \\ **Tailed** \\ **Reward** \\ \end{tabular} \\ \hline \begin{tabular}{c} LSVI-UCB[22] \\ \end{tabular} & \(\widetilde{O}\left(\sqrt{d^{3}H^{4}K}\right)\) & No & No & No & **Yes** & No \\ \hline \begin{tabular}{c} Force [33] \\ \end{tabular} & \(\widetilde{O}\left(\sqrt{d^{3}H^{3}V_{1}^{*}K}\right)\) & No & **Yes** & No & No & No \\ \hline \begin{tabular}{c} VQOL [2] \\ \end{tabular} & \(\widetilde{O}\left(d\sqrt{H^{3}K}\right)\) & No & No & **Yes** & **Yes** & No \\ \hline \begin{tabular}{c} VARA [26] \\ \end{tabular} & \(\widetilde{O}\left(d\sqrt{H\mathcal{G}^{*}K}\right)\) & **Yes** & **Yes** & **Yes** & **Yes** & \(\epsilon=1\) \\ \hline 
\begin{tabular}{c} Heavy-LSVI-UCB (**Ours**) \\ \end{tabular} & \(\widetilde{O}\left(d\sqrt{H\mathcal{U}^{*}K\frac{1}{1+\epsilon}}+d\sqrt{H \mathcal{V}^{*}K}\right)\) & **Yes** & **Yes** & **Yes** & **Yes** \\ \hline \end{tabular}
\end{table}
Table 2: Comparisons with previous works on time-inhomogeneous linear MDPs. \(d\), \(H\), \(K\), \(V_{1}^{*}\), \(\mathcal{G}^{*}\) are feature dimension, the length of the episode, the number of episodes, optimal value function, variance-dependent quantity defined in Li and Sun [26]. \(\mathcal{U}^{*}\), \(\mathcal{V}^{*}\) are defined in Theorem 5.2.

\(M\) and the agent's policy \(\pi\). If \(\pi\) is randomized, then the definition of \(V\) should have an expectation. Denote the optimal value functions as \(V_{h}^{*}(s)=\sup_{\pi}V_{h}^{n}(s)\) and \(Q_{h}^{*}(s,a)=\sup_{\pi}Q_{h}^{n}(s,a)\).

We introduce the following shorthands for simplicity. At the \(h\)-th step, for any value function \(V:\mathcal{S}\to\mathbb{R}\), let \([\mathbb{P}_{h}V](s,a)=\mathbb{E}_{s^{\prime}\sim p_{h}(\cdot|s,a)}V(s^{\prime})\), \([\forall_{h}V](s,a)=[\mathbb{P}_{h}V^{2}](s,a)-[\mathbb{P}_{h}V]^{2}(s,a)\) denote the expectation and the variance of the next-state value function at the \(h\)-th step given \((s,a)\).

We aim to minimize the \(K\)-episode _regret_ defined as \(\mathrm{Regret}(K)=\sum_{k=1}^{K}[V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi^{k}}(s_{1}^{ k})]\).

In the rest of this section, we introduce linear MDPs with heavy-tailed rewards. We first give the definition of linear MDPs studied in Yang and Wang [35], Jin et al. [22], with emphasis that the rewards in their settings are deterministic or uniformly bounded. Then we focus on the heavy-tailed random rewards.

**Definition 2.2**.: An MDP \(M=M(\mathcal{S},\mathcal{A},H,\{R_{h}\}_{h\in[H]},\{\mathbb{P}_{h}\}_{h\in[H]})\) is a _time-inhomogeneous finite-horizon linear MDP_, if there exist known feature maps \(\bm{\phi}(s,a):\mathcal{S}\times\mathcal{A}\to\mathcal{B}_{d}(1)\), unknown \(d\)-dimensional signed measures \(\{\bm{\mu}_{h}^{\star}\}_{h\in[H]}\) over \(\mathcal{S}\) with \(\|\bm{\mu}_{h}^{*}(\mathcal{S})\|:=\int_{s\in\mathcal{S}}|\bm{\mu}(s)|\mathrm{ d}s\leq\sqrt{d}\) and unknown coefficients \(\{\bm{\theta}_{h}^{*}\}_{h\in[H]}\subseteq\mathcal{B}_{d}(B)\) for some known upper bound \(B\) such that

\[r_{h}(s,a)=\langle\bm{\phi}(s,a),\bm{\theta}_{h}^{*}\rangle,\quad\mathbb{P}_{ h}(\cdot|s,a)=\langle\bm{\phi}(s,a),\bm{\mu}_{h}^{*}(\cdot)\rangle\]

for any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) and timestep \(h\in[H]\).

**Assumption 2.3** (Realizable rewards).: For all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), the random reward \(R_{h}(s,a)\) is independent of next state \(s^{\prime}\sim\mathbb{P}_{h}(\cdot|s,a)\) and admits the linear structure

\[R_{h}(s,a)=\langle\bm{\phi}(s,a),\bm{\theta}_{h}^{*}\rangle+\varepsilon_{h}(s,a),\]

where \(\varepsilon_{h}(s,a)\) is a mean-zero heavy-tailed random variable specified below.

We introduce the notation \(\bm{\nu}_{n}[X]=\mathbb{E}[|X-\mathbb{E}X|^{n}]\) for the \(n\)-th central moment of any random variable \(X\). And for any random reward function at the \(h\)-th step \(R_{h}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\), let

\[[\mathbb{E}_{h}R_{h}](s,a)=\mathbb{E}[R_{h}(s_{h},a_{h})|(s_{h},a_{h})=(s,a)],\]

\[[\bm{\nu}_{1+\epsilon}R_{h}](s,a)=\mathbb{E}[[|R_{h}-\mathbb{E}_{h}R_{h}](s_ {h},a_{h})|^{1+\epsilon}|(s_{h},a_{h})=(s,a)]\]

denote its expectation and the \((1+\epsilon)\)-th central moment given \((s_{h},a_{h})=(s,a)\) for short.

**Assumption 2.4** (Heavy-tailedness of rewards).: Random variable \(\varepsilon_{h}(s,a)\) satisfies \([\mathbb{E}_{h}\varepsilon_{h}](s,a)=0\). And for some known \(\epsilon,\epsilon^{\prime}\in(0,1]\) and constants \(\nu_{R},\nu_{R^{\prime}}\geq 0\), the following unknown moments of \(\varepsilon_{h}(s,a)\) satisfy

\[[\mathbb{E}_{h}|\varepsilon_{h}|^{1+\epsilon}](s,a)\leq\nu_{R}^{1+\epsilon}, \quad[\bm{\nu}_{1+\epsilon^{\prime}}|\varepsilon_{h}|^{1+\epsilon}](s,a)\leq \nu_{R^{\prime}}^{1+\epsilon^{\prime}}\]

for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\).

Assumption 2.4 generalizes Assumption 2.2 of Li and Sun [26], which is the weakest moment condition on random rewards in the current literature of RL with function approximation. Setting \(\epsilon=1\) and \(\epsilon^{\prime}=1\) immediately recovers their settings.

**Assumption 2.5** (Realizable central moments).: There are some unknown coefficients \(\{\bm{\psi}_{h}^{*}\}_{h\in[H]}\subseteq\mathcal{B}_{d}(W)\) for some known upper bound \(W\) such that

\[[\mathbb{E}_{h}|\varepsilon_{h}|^{1+\epsilon}](s,a)=\langle\bm{\phi}(s,a),\bm {\psi}_{h}^{*}\rangle\]

for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\).

**Remark 2.6**.: When \(\epsilon=1\), that is the rewards have finite variance, Li and Sun [26] use the fact that \([\bm{\nu}_{2}R_{h}](s,a)=[\forall_{h}R_{h}](s,a)=[\mathbb{E}_{h}R_{h}^{2}](s,a) -[\mathbb{E}_{h}R_{h}]^{2}(s,a)\), assume the linear realizability of the second moment \([\mathbb{E}_{h}R_{h}^{2}](s,a)\), and estimate it instead. However, when \(\epsilon<1\), there is no such relationship between the \((1+\epsilon)\)-th central moment \([\bm{\nu}_{1+\epsilon}R_{h}](s,a)\) and the \((1+\epsilon)\)-th raw moment \([\mathbb{E}_{h}R_{h}^{1+\epsilon}](s,a)\). Thus, we adopt a new approach to estimate \([\bm{\nu}_{1+\epsilon}R_{h}](s,a)\) directly, and bound the error by a novel perturbation analysis of adaptive Huber regression in Appendix C.3.

**Assumption 2.7** (Bounded cumulative rewards).: For any policy \(\pi\), let \(\{s_{h},a_{h},R_{h}\}_{h\in[H]}\) be a random trajectory following policy \(\pi\). And define \(r_{\pi}=\sum_{h=1}^{H}[\mathbb{E}_{h}R_{h}](s_{h},a_{h})=\sum_{h=1}^{H}r_{h}(s_{h },a_{h})\). We assume (1) \(0\leq r_{\pi}\leq\mathcal{H}\). (2) \(\sum_{h=1}^{H}[\bm{\nu}_{1+\epsilon}R_{h}]^{\frac{1}{1+\epsilon}}(s_{h},a_{h}) \leq\mathcal{U}\). (3) \(\mathrm{Var}(r_{\pi})\leq\mathcal{V}\).

Here, (1) gives an upper bound of cumulative expected rewards \(r_{\pi}\). (2) assumes the summation of \((1+\epsilon)\)-th central moment of rewards \([\bm{\nu}_{1+\epsilon}R_{h}](s_{h},a_{h})\) is bounded since \([\sum_{h=1}^{H}[\bm{\nu}_{1+\epsilon}R_{h}](s_{h},a_{h})]^{\frac{2}{1+\epsilon}} \leq\sum_{h=1}^{H}[\bm{\nu}_{1+\epsilon}R_{h}]^{\frac{2}{1+\epsilon}}(s_{h},a_{h}) \leq\mathcal{U}\) due to Jensen's inequality. And (3) is to bound the variance of \(r_{\pi}\) along the trajectory following policy \(\pi\).

```
0: Number of total rounds \(T\), confidence level \(\delta\), regularization parameter \(\lambda\), \(\sigma_{\min}\), parameters for adaptive Huber regression \(c_{0},c_{1},\tau_{0}\), estimated central moment \(\widehat{\nu}_{t}\) and moment parameter \(b\) that satisfy \(\nu_{t}/\widehat{\nu}_{t}\leq b\) for all \(t\leq T\).
0: The estimated coefficient \(\boldsymbol{\theta}_{t}\).
1:\(\kappa=d\log(1+TL^{2}/(d\lambda\sigma_{\min}^{2}))\).
2: Set \(\boldsymbol{H}_{t-1}=\lambda\boldsymbol{I}+\sum_{s=1}^{t-1}\sigma_{s}^{-2} \boldsymbol{\phi}_{s}\boldsymbol{\phi}_{s}^{\top}\).
3: Set \(\sigma_{t}=\max\left\{\widehat{\nu}_{t},\sigma_{\min},\frac{\|\boldsymbol{ \phi}_{t}\|_{\boldsymbol{H}_{t-1}^{-1}}}{c_{0}},\frac{\sqrt{LB}}{c_{1}^{4}\left( 2\kappa b^{2}\right)^{\frac{1}{4}}}\|\boldsymbol{\phi}_{t}\|_{\boldsymbol{H}_ {t-1}^{-1}}^{\frac{1}{2}}\right\}\).
4: Set \(\tau_{t}=\tau_{0}\frac{\sqrt{1+w_{t}^{2}}}{w_{t}}t^{\frac{1-\epsilon}{2(1+ \epsilon)}}\) with \(w_{t}=\|\boldsymbol{\phi}_{t}/\sigma_{t}\|_{\boldsymbol{H}_{t-1}^{-1}}\).
5: Define the loss function \(L_{t}(\boldsymbol{\theta}):=\frac{\lambda}{2}\|\boldsymbol{\theta}\|^{2}+\sum_{ s=1}^{t}\ell_{\tau_{s}}(\frac{y_{t}-\langle\boldsymbol{\phi}_{s}, \boldsymbol{\theta}\rangle}{\sigma_{s}})\).
6: Compute \(\boldsymbol{\theta}_{t}=\operatorname*{argmin}_{\boldsymbol{\theta}\in \mathcal{B}_{d}(B)}L_{t}(\boldsymbol{\theta})\). ```

**Algorithm 1** Adaptive Huber Regression

## 3 Adaptive Huber Regression

At the core of our algorithms for both heavy-tailed linear bandits and linear MDPs is a new approach - adaptive Huber regression - to handle heavy-tailed noise. Sun et al. [32] imposed adaptive Huber regression to handle i.i.d. heavy-tailed noise by utilizing Huber loss [17] as a surrogate of squared loss. Li and Sun [26] modified adaptive Huber regression for heterogeneous online settings, where the variances in each round are different. However, it is not readily applicable to deal with heavy-tailed noise. Our contribution in this section is to construct a new self-normalized concentration inequality for general online regression problems with heavy-tailed noise.

We first give a brief introduction to Huber loss function and its properties.

**Definition 3.1** (Huber loss).: _Huber loss_ is defined as

\[\ell_{\tau}(x)=\begin{cases}\frac{\pi^{2}}{2}&\text{if }|x|\leq\tau,\\ \tau|x|-\frac{\tau^{2}}{2}&\text{if }|x|>\tau,\end{cases}\] (3.1)

where \(\tau>0\) is referred as a robustness parameter.

Huber loss is first proposed by Huber [17] as a robust version of squared loss while preserving the convex property. Specifically, Huber loss is a quadratic function of \(x\) when \(|x|\) is less than the threshold \(\tau\), while becomes linearly dependent on \(|x|\) when \(|x|\) grows larger than \(\tau\). It has the property of strongly convex near zero point and is not sensitive to outliers. See Appendix C.1 for more properties of Huber loss.

Next, we define general online regression problems with heavy-tailed noise, which include heavy-tailed linear bandits as a special case. Then we utilize Huber loss to estimate \(\boldsymbol{\theta}^{*}\). Below we give the main theorem to bound the deviation of the estimated \(\boldsymbol{\theta}_{t}\) in Algorithm 1 from the ground truth \(\boldsymbol{\theta}^{*}\).

**Definition 3.2**.: Let \(\{\mathcal{F}_{t}\}_{t\geq 1}\) be a filtration. For all \(t>0\), let random variables \(y_{t},\varepsilon_{t}\) be \(\mathcal{F}_{t}\)-measurable and random vector \(\boldsymbol{\phi}_{t}\in\mathcal{B}_{d}(L)\) be \(\mathcal{F}_{t-1}\)-measurable. Suppose \(y_{t}=\langle\boldsymbol{\phi}_{t},\boldsymbol{\theta}^{*}\rangle+\varepsilon_ {t}\), where \(\boldsymbol{\theta}^{*}\in\mathcal{B}_{d}(B)\) is an unknown coefficient and

\[\mathbb{E}[\varepsilon_{t}|\mathcal{F}_{t-1}]=0,\quad\mathbb{E}[|\varepsilon_ {t}|^{1+\epsilon}|\mathcal{F}_{t-1}]=\nu_{t}^{1+\epsilon}\]

for some \(\epsilon\in(0,1]\). The goal is to estimate \(\boldsymbol{\theta}^{*}\) at any round \(t\) given the realizations of \(\{\boldsymbol{\phi}_{s},y_{s}\}_{s\in[t]}\).

**Theorem 3.3**.: For the online regression problems in Definition 3.2, we solve for \(\boldsymbol{\theta}_{t}\) by adaptive Huber regression in Algorithm 1 with \(c_{0},c_{1},\tau_{0}\) in Appendix C.2. Then for any \(\delta\in(0,1)\), with probability at least \(1-3\delta\), for all \(t\leq T\), we have \(\|\boldsymbol{\theta}_{t}-\boldsymbol{\theta}^{*}\|_{\boldsymbol{H}_{t}}\leq\beta _{t}\), where \(\boldsymbol{H}_{t}\) is defined in Algorithm 1 and

\[\beta_{t}=3\sqrt{\lambda}B+24t^{\frac{1-\epsilon}{2(1+\epsilon)}}\sqrt{2\kappa}b (\log 3T)^{\frac{1-\epsilon}{2(1+\epsilon)}}(\log(2T^{2}/\delta))^{\frac{\epsilon}{1+ \epsilon}}.\] (3.2)

Proof.: To derive a tight high-probability bound, we take the most advantage of the properties of Huber loss. A Chernoff bounding technique is used to bound the main error term, which requires a careful analysis of the moment generating function. See Appendix C.2 for a detailed proof.

We refer to the regression process in Line 6 of Algorithm 1 as _adaptive Huber regression_ in line with Sun et al. [32] to emphasize that the value of robustness parameter \(\tau_{t}\) is chosen to adapt to data for a better trade-off between bias and robustness. Specifically, since we are in the online setting, \(\bm{\phi}_{t}\) are dependent on \(\{\bm{\phi}_{s}\}_{s<t}\), which is the key difference from the i.i.d. case in Sun et al. [32] where they set \(\tau_{t}=\tau_{0}\), for all \(t\leq T\). Thus, as shown in Line 4 of Algorithm 1, inspired by Li and Sun [26], we adjust \(\tau_{t}\) according to the importance of observations \(w_{t}=\left\|\bm{\phi}_{t}/\sigma_{t}\right\|_{\bm{H}_{t-1}^{-1}}\), where \(\sigma_{t}\) is specified below. In the case where \(\epsilon<1\), different from Li and Sun [26], we first choose \(\tau_{t}\) to be small for robust purposes, then gradually increase it with \(t\) to reduce the bias.

Next, we illustrate the reason for setting \(\sigma_{t}\) via Line 3 of Algorithm 1. We use \(\widehat{\nu}_{t}\in\mathcal{F}_{t-1}\) to estimate the central moment \(\nu_{t}\) and use moment parameter \(b\) to measure the closeness between \(\widehat{\nu}_{t}\) and \(\nu_{t}\). When we choose \(\widehat{\nu}_{t}\) as an upper bound of \(\nu_{t}\), \(b\) becomes a constant that equals to \(1\). And \(\sigma_{\min}\) is a small positive constant to avoid singularity. The last two terms with respect to \(c_{0}\) and \(c_{1}\) are set according to the uncertainty \(\left\|\bm{\phi}_{t}\right\|_{\bm{H}_{t-1}^{-1}}\). In addition, setting the parameter \(c_{0}\leq 1\) yields \(w_{t}\leq 1\), which is essential to meet the condition of elliptical potential lemma [1].

**Remark 3.4**.: The error bound \(\beta_{t}\) in (3.2) is only related to the feature dimension \(d\) and moment parameter \(b\). While the Bernstein-style self-normalized concentration bounds [42; 41] depend on the magnitude of \(\varepsilon_{t}\), thus cannot handle heavy-tailed errors.

## 4 Linear Bandits

In this section, we show the algorithm Heavy-OFUL in Algorithm 2 for heavy-tailed linear bandits in Definition 2.1. We first give a brief algorithm description, and then provide a theoretical regret analysis.

### Algorithm Description

Heavy-OFUL follows the principle of Optimism in the Face of Uncertainty (OFU) [1], and uses adaptive Huber regression in Section 3 to maintain a set \(\mathcal{C}_{t}\) that contains the unknown coefficient \(\bm{\theta}^{*}\) with high probability. Specifically, at the \(t\)-th round, Heavy-OFUL estimates the expected reward of any arm \(\bm{\phi}\) as \(\max_{\bm{\theta}\in\mathcal{C}_{t-1}}\langle\bm{\phi},\bm{\hat{\theta}}\rangle\), and selects the arm that maximizes the estimated reward. The agent then receives the reward \(R_{t}\) and updates the confidence set \(\mathcal{C}_{t}\) based on the information up to round \(t\) with its center \(\bm{\theta}_{t}\) computed by adaptive Huber regression as in Line 9 of Algorithm 2.

```
0: Number of total rounds \(T\), confidence level \(\delta\), regularization parameter \(\lambda\), \(\sigma_{\min}\), parameters for adptive Huber regression \(c_{0},c_{1},\tau_{0}\), confidence radius \(\beta_{t}\).
1:\(\kappa=d\log(1+\frac{T{I^{2}}}{d\lambda\sigma_{\min}^{2}})\), \(\mathcal{C}_{0}=\mathcal{B}_{d}(B)\), \(\bm{H}_{0}=\lambda\bm{I}\).
2:for\(t=1,\ldots,T\)do
3: Observe \(\mathcal{D}_{t}\).
4: Set \((\bm{\phi}_{t},\cdot)=\operatorname*{argmax}_{\bm{\phi}\in\mathcal{D}_{t},\bm{ \theta}\in\mathcal{C}_{t-1}}\langle\bm{\phi},\bm{\theta}\rangle\).
5: Play \(\bm{\phi}_{t}\) and observe \(R_{t},\nu_{t}\).
6: Set \(\sigma_{t}=\max\big{\{}\nu_{t},\sigma_{\min},\frac{\left\|\bm{\phi}_{t}\right\| _{\bm{H}_{t-1}^{-1}}}{c_{0}},\frac{\sqrt{LB}}{c_{1}^{\frac{1}{\delta}}(2\kappa) \frac{1}{\delta}}\|\bm{\phi}_{t}\|_{\bm{H}_{t-1}^{-1}}^{\frac{1}{\delta}}\big{\}}\).
7: Set \(\tau_{t}=\tau_{0}\frac{\sqrt{1+w_{t}^{2}}}{w_{t}}t^{\frac{1-\epsilon}{2(1+ \epsilon)}}\) with \(w_{t}=\left\|\bm{\phi}_{t}/\sigma_{t}\right\|_{\bm{H}_{t-1}^{-1}}\).
8: Update \(\bm{H}_{t}=\bm{H}_{t-1}+\sigma_{t}^{-2}\bm{\phi}_{t}\bm{\phi}_{t}^{\top}\).
9: Solve for \(\bm{\theta}_{t}\) by Algorithm 1 and set \(\mathcal{C}_{t}=\{\bm{\theta}\in\mathbb{R}^{d}\|\bm{\theta}-\bm{\theta}_{t}\|_{ \bm{H}_{t}}\leq\beta_{t}\}\).
10:endfor ```

**Algorithm 2** Heavy-OFUL

We next give the instance-dependent regret upper bound of Heavy-OFUL in Theorem 4.1.

**Theorem 4.1**.: For the heavy-tailed linear bandits in Definition 2.1, we set \(c_{0},c_{1},\tau_{0},\beta_{t}\) in Algorithm 2 according to Theorem 3.3 with \(b=1\). Besides, let \(\lambda=d/B^{2}\), and \(\sigma_{\min}=1/\sqrt{T}\). Then with probability at least \(1-3\delta\), the regret of Heavy-OFUL is bounded by

\[\mathrm{Regret}(T)=\widetilde{O}\left(dT^{\frac{1-\epsilon}{2(1+\epsilon)}} \sqrt{\sum\nolimits_{t=1}^{T}\nu_{t}^{2}}+dT^{\frac{1-\epsilon}{2(1+\epsilon)} }\right).\]

Proof.: The proof uses the self-normalized concentration inequality of adaptive Huber regression and a careful analysis to bound the summation of bonuses. See Appendix D.1 for a detailed proof. 

**Remark 4.2**.: Theorem 4.1 shows Heavy-OFUL achieves an instance-dependent regret bound. When we assume \(\nu_{t},\forall t\geq 1\) have uniform upper bound \(\nu\) (which can be treated as a constant), then the bound is reduced to \(\widetilde{O}(dT^{\frac{1}{1+\epsilon}})\). It matches the lower bound \(\Omega(dT^{\frac{1}{1+\epsilon}})\) by Shao et al. [31] up to logarithmic factors. In the deterministic scenario, where \(\epsilon=1\) and \(\nu_{t}=0\), for all \(t\geq 1\), the bound is reduced to \(\widetilde{O}(d)\). It matches the lower bound \(\Omega(d)\)1 up to logarithmic factors.

Footnote 1: Consider the decision set consisting of unit bases of \(\mathbb{R}^{d}\). Given that each arm pull can only yield information about a single coordinate, it is inevitable that \(d\) pulls are required for exploration.

## 5 Linear MDPs

In this section, we show the algorithm Heavy-LSVI-UCB in Algorithm 3 for linear MDP with heavy-tailed rewards defined in Section 2.2. Let \(\bm{\phi}_{k,h}:=\bm{\phi}(s_{k,h},a_{k,h})\) for short. We first give the algorithm description intuitively, then provide the computational complexity and regret bound.

### Algorithm Description

Heavy-LSVI-UCB features a novel combination of adaptive Huber regression in Section 3 and existing algorithmic frameworks for linear MDPs with bounded rewards [22; 15]. At a high level, Heavy-LSVI-UCB employs separate estimation techniques to handle heavy-tailed rewards and transition kernels. Specifically, we utilize adaptive Huber regression proposed in Section 3 to estimate heavy-tailed rewards and weighted ridge regression [42; 15] to estimate the expected next-state value functions. Then, it follows the value iteration scheme to update the optimistic and pessimistic estimation of the optimal value function \(Q_{h}^{k}\), \(V_{h}^{k}\) and \(\hat{Q}_{h}^{k}\), \(\hat{V}_{h}^{k}\), respectively, via a rare-switching policy as in Line 7 to 15 of Algorithm 3. We highlight the key steps of Heavy-LSVI-UCB as follows.

Estimation for expected heavy-tailed rewardsSince the expected rewards have linear structure in linear MDPs, i.e., \(r_{h}(s,a)=\langle\bm{\phi}(s,a),\bm{\theta}_{h}^{*}\rangle\), we use adaptive Huber regression to estimate \(\bm{\theta}_{h}^{*}\):

\[\bm{\theta}_{k,h}=\operatorname*{argmin}_{\bm{\theta}\in\mathcal{B}_{d}(B)} \frac{\lambda_{R}}{2}\|\bm{\theta}\|^{2}+\sum_{i=1}^{k}\ell_{\tau_{i,h}} \left(\frac{R_{i,h}-\langle\bm{\phi}_{i,h},\bm{\theta}\rangle}{\nu_{i,h}} \right),\] (5.1)

where \(\nu_{i,h}\) will be specified later.

Estimation for central moment of rewardsBy Assumption 2.5, the \((1+\epsilon)\)-th central moment of rewards is linear in \(\bm{\phi}\), i.e., \([\bm{\nu}_{1+\epsilon}R_{h}](s,a)=\langle\bm{\phi}(s,a),\bm{\psi}_{h}^{*}\rangle\). Motivated by this, we estimate \(\bm{\psi}_{h}^{*}\) by adaptive Huber regression as

\[\bm{\psi}_{k,h}=\operatorname*{argmin}_{\bm{\psi}\in\mathcal{B}_{d}(W)}\frac{ \lambda_{R}}{2}\|\bm{\psi}\|^{2}+\sum_{i=1}^{k}\ell_{\widetilde{\tau}_{i,h}} \left(\frac{|\varepsilon_{i,h}|^{1+\epsilon}-\langle\bm{\phi}_{i,h},\bm{\psi }\rangle}{\nu_{i,h}}\right),\] (5.2)

where \(W\) is the upper bound of \(\|\bm{\psi}_{h}^{*}\|\) defined in Assumption 2.5. Since \(\varepsilon_{i,h}\) is intractable, we estimate it by \(\widehat{\varepsilon}_{i,h}=R_{i,h}-\langle\bm{\phi}_{i,h},\bm{\theta}_{i,h}\rangle\), which gives \(\widehat{\bm{\psi}}_{k,h}\) as

\[\widehat{\bm{\psi}}_{k,h}=\operatorname*{argmin}_{\bm{\psi}\in\mathcal{B}_{d}( W)}\frac{\lambda_{R}}{2}\|\bm{\psi}\|^{2}+\sum_{i=1}^{k}\ell_{\widetilde{\tau}_{i,h }}\left(\frac{|\widehat{\varepsilon}_{i,h}|^{1+\epsilon}-\langle\bm{\phi}_{i,h},\bm{\psi}\rangle}{\nu_{i,h}}\right).\] (5.3)

The inevitable error between \(\bm{\psi}_{k,h}\) and \(\widehat{\bm{\psi}}_{k,h}\) can be quantified by a novel perturbation analysis of adaptive Huber regression in Appendix C.3.

[MISSING_PAGE_FAIL:8]

In addition, for any \(f,g:\mathcal{S}\to\mathbb{R}\), it holds that \(\bm{w}_{h}[f+g]=\bm{w}_{h}[f]+\bm{w}_{h}[g]\) and \(\widehat{\bm{w}}_{k,h}[f+g]=\widehat{\bm{w}}_{k,h}[f]+\widehat{\bm{w}}_{k,h}[g]\) due to the linear property of integration and ridge regression.

We remark \(\widehat{\bm{w}}_{k,h}[f]\) is the estimation of \(\bm{w}_{h}[f]\) by weighted ridge regression on \(\{\bm{\phi}_{i,h},f(s_{i,h+1})\}_{i\in[k]}\). And we estimate the coefficients \(\widehat{\bm{w}}_{k,h},\hat{\bm{w}}_{k,h},\widehat{\bm{w}}_{k,h}\)

\[\widehat{\bm{w}}_{k,h}=\widehat{\bm{w}}_{k,h}[V_{h+1}^{k}],\quad \hat{\bm{w}}_{k,h}=\widehat{\bm{w}}_{k,h}[\hat{V}_{h+1}^{k}],\quad\widehat{ \bm{w}}_{k,h}=\widehat{\bm{w}}_{k,h}[(V_{h+1}^{k})^{2}],\] (5.8)

where \(V_{h}^{k}\) and \(\hat{V}_{h}^{k}\) are optimistic and pessimistic estimation of the optimal value functions.

Estimation for variance of next-state value functionsInspired by He et al. [15], we set the weight \(\sigma_{k,h}\) for weighted ridge regression in (5.7) as

\[\sigma_{k,h}=\max\left\{\widehat{\sigma}_{k,h},\sqrt{d^{3}HD_{k,h}},\sigma_{ \min},\left\|\bm{\phi}_{k,h}\right\|_{\bm{\Sigma}^{-1}_{k-1,h}},\sqrt{d^{\frac {5}{2}}H\mathcal{H}}\left\|\bm{\phi}_{k,h}\right\|_{\bm{\Sigma}^{-1}_{k-1,h}} ^{\frac{1}{2}}\right\},\] (5.9)

where \(\sigma_{\min}\) is a small constant to avoid singularity, \(\widehat{\sigma}_{k,h}^{2}=[\widehat{\mathbb{V}}_{h}V_{h+1}^{k}](s_{k,h},a_{k,h})+E_{k,h}\) with

\[\left[\widehat{\mathbb{V}}_{h}V_{h+1}^{k}\right](s_{k,h},a_{k,h})=\langle\bm{ \phi}_{k,h},\widehat{\bm{w}}_{k-1,h}\rangle_{[0,\mathcal{H}^{2}]}-\langle\bm {\phi}_{k,h},\widehat{\bm{w}}_{k-1,h}\rangle_{[0,\mathcal{H}]}^{2},\] (5.10)

\[E_{k,h}=\min\left\{4\mathcal{H}\langle\bm{\phi}_{k,h},\widehat{\bm{w}}_{k-1,h }-\check{\bm{w}}_{k-1,h}\rangle+11\mathcal{H}\beta_{0}\left\|\bm{\phi}_{k,h} \right\|_{\bm{\Sigma}^{-1}_{k-1,h}},\mathcal{H}^{2}\right\}.\] (5.11)

\[D_{k,h}=\min\left\{2\mathcal{H}\langle\bm{\phi}_{k,h},\widehat{\bm{w}}_{k-1,h }-\check{\bm{w}}_{k-1,h}\rangle+4\mathcal{H}\beta_{0}\left\|\bm{\phi}_{k,h} \right\|_{\bm{\Sigma}^{-1}_{k-1,h}},\mathcal{H}^{2}\right\},\] (5.12)

where \(\beta_{0}=\widetilde{O}(\sqrt{d^{3}H\mathcal{H}^{2}}/\sigma_{\min})\). Here \(\widehat{\sigma}_{k,h}^{2}\) and \(D_{k,h}\) are upper bounds of \([\mathbb{V}_{h}V_{h+1}^{*}](s_{k,h},a_{k,h})\) and \(\max\{[\mathbb{V}_{h}\left(V_{h+1}^{k}-V_{h+1}^{*}\right)](s_{k,h},a_{k,h}) \},[\mathbb{V}_{h}\left(V_{h+1}^{*}-\tilde{V}_{h+1}^{k}\right)](s_{k,h},a_{k,h })\}\), respectively.

### Computational Complexity

**Theorem 5.1**.: For the linear MDPs with heavy-tailed rewards defined in Section 2.2, the computational complexity of Heavy-LSVI-UCB is \(\widetilde{O}(d^{4}|\mathcal{A}|H^{3}K+HK\mathcal{R})\). Here \(\mathcal{R}\) is the cost of the optimization algorithm for solving adaptive Huber regression in (5.1). Furthermore, we can specialize \(\mathcal{R}\) by adopting the Nesterov accelerated method, which gives \(\mathcal{R}=\widetilde{O}(d+d^{-\frac{1-\epsilon}{2(1+\epsilon)}}H^{\frac{1- \epsilon}{2(1+\epsilon)}}K^{\frac{1+2\epsilon}{2(1+\epsilon)}})\).

Proof.: See Appendix E for a detailed proof. 

Such a complexity allows us to focus on the complexity introduced by the RL algorithm rather than the optimization subroutine for solving adaptive Huber regression. Compared to that of LSVI-UCB++ [15], \(\widetilde{O}(d^{4}|\mathcal{A}|H^{3}K)\), the extra term \(\widetilde{O}(HK\mathcal{R})\) causes a slightly worse computational time in terms of \(K\). This is due to the absence of a closed-form solution of adaptive Huber regression in (5.1). Thus extra optimization steps are unavoidable. Nevertheless, Nesterov accelerated method gives \(\mathcal{R}=\widetilde{O}\left(K^{\frac{1+2\epsilon}{2(1+\epsilon)}}\right)\) with respect to \(K\), which implies the computational complexity of Heavy-LSVI-UCB is better than that of LSVI-UCB [22], \(\widetilde{O}(d^{2}|\mathcal{A}|HK^{2})\) in terms of \(K\), thanks to the rare-switching updating policy. We conduct numerical experiments in Appendix B to further corroborate the computational efficiency of adaptive Huber regression.

### Regret Bound

**Theorem 5.2** (Informal).: For the linear MDPs with heavy-tailed rewards defined in Section 2.2, we set parameters in Algorithm 3 as follows: \(\lambda_{R}=d/\max\left\{B^{2},W^{2}\right\}\), \(\lambda_{V}=1/\mathcal{H}^{2}\), \(\nu_{\min}\), \(\sigma_{\min}\), \(c_{0}\), \(c_{1}\), \(\tau_{0}\), \(\widetilde{\tau}_{0}\), \(\beta_{R}\), \(\beta_{0}\), \(\beta_{R}\), \(\beta_{V}\) in Appendix F.1. Then for any \(\delta\in(0,1)\), with probability at least \(1-16\delta\), the regret of Heavy-LSVI-UCB is bounded by

\[\mathrm{Regret}(K)=\widetilde{O}\left(d\sqrt{H\mathcal{U}^{*}}K^{\frac{1}{1+ \epsilon}}+d\sqrt{H\mathcal{V}^{*}K}\right),\]

where \(\epsilon\in(0,1]\), \(\mathcal{U}^{*}=\min\{\mathcal{U}_{0}^{*},\mathcal{U}\}\), \(\mathcal{V}^{*}=\min\{\mathcal{V}_{0}^{*},\mathcal{V}\}\) with \(\mathcal{U}_{0}^{*}\), \(\mathcal{V}_{0}^{*}\) defined in Appendix F.2 and \(\mathcal{H},\mathcal{U},\mathcal{V}\) defined in Assumption 2.7.

Proof.: See Appendix F.2 for a formal version of Theorem 5.2 and its detailed proof.

Quantities \(\mathcal{U}^{*}\), \(\mathcal{V}^{*}\)We make a few explanations for the quantities \(\mathcal{U}^{*}\), \(\mathcal{V}^{*}\). On one hand, \(\mathcal{U}^{*}\) is upper bounded by \(\mathcal{U}\), which is the upper bound of the sum of the \((1+\epsilon)\)-th central moments of reward functions along a single trajectory. On the other hand, \(\mathcal{U}^{*}\) is no more than \(\mathcal{U}^{*}_{0}\), which is the sum of the \((1+\epsilon)\)-th central moments with respect to the averaged occupancy measure of the first \(K\) episodes. \(\mathcal{V}^{*}\) is defined similar to \(\mathcal{U}^{*}\), but measures the randomness of transition probabilities.

**Remark 5.3**.: When \(\epsilon=1\), we can show that this regret is bounded by \(\widetilde{O}(d\sqrt{H\mathcal{G}^{*}K})\), where \(\mathcal{G}^{*}\) is an variance-dependent quantity defined by Li and Sun [26]. Thus, our result recovers their variance-aware regret bound. See Remark F.15 in Appendix F.2 for a detailed proof.

To demonstrate the optimality of our results and establish connections with existing literature, we can specialize Theorem 5.2 to obtain the worst-case regret [22, 2, 15] and first-order regret [33].

**Corollary 5.4** (Worst-case regret).: For the linear MDPs with heavy-tailed rewards defined in Section 2.2 and for any \(\delta\in(0,1)\), with probability at least \(1-16\delta\), the regret of Heavy-LSVI-UCB is bounded by

\[\widetilde{O}(dHK^{\frac{1}{1+\epsilon}}+d\sqrt{H^{3}K}).\]

Proof.: Notice \(\mathcal{U}^{*}\) and \(\mathcal{V}^{*}\) are upper bounded by \(H\nu_{R}^{2}\) and \(\mathcal{H}^{2}\) (total variance lemma in Jin et al. [21]) respectively. When \(\mathcal{H}=H\), and we treat \(\nu_{R}\) as a constant, the result follows. 

Next, we give the regret lower bound of linear MDPs with heavy-tailed rewards in Theorem 5.5, which shows our proposed Heavy-LSVI-UCB is minimax optimal in the worst case.

**Theorem 5.5**.: For any algorithm, there exists an \(H\)-episodic, \(d\)-dimensional linear MDP with heavy-tailed rewards such that for any \(K\), the algorithm's regret is

\[\Omega(dHK^{\frac{1}{1+\epsilon}}+d\sqrt{H^{3}K}).\]

Proof.: Intuitively, the proof of Theorem 5.5 follows from a combination of the lower bound constructions for heavy-tailed linear bandits in Shao et al. [31] and linear MDPs in Zhou et al. [42]. See Appendix G for a detailed proof. 

Theorem 5.5 shows that for sufficiently large \(K\), the reward term dominates in the regret bound. Thus, in heavy-tailed settings, the main difficulty is learning the reward functions.

**Corollary 5.6** (First-order regret).: For the linear MDPs with heavy-tailed rewards defined in Section 2.2 and for any \(\delta\in(0,1)\), with probability at least \(1-16\delta\), the regret of Heavy-LSVI-UCB is bounded by

\[\widetilde{O}(d\sqrt{H\mathcal{U}^{*}}K^{\frac{1}{1+\epsilon}}+d\sqrt{H \mathcal{U}V_{1}^{*}K}).\]

And when the rewards are uniformly bounded in \([0,1]\), the result is reduced to the first-order regret bound of \(\widetilde{O}(d\sqrt{H^{2}V_{1}^{*}K})\).

Proof.: See Section F.3 for a detailed proof. 

Our first-order regret \(\widetilde{O}(d\sqrt{H^{2}V_{1}^{*}K})\) is minimax optimal in the worst case since \(V_{1}^{*}\leq H\). And it improves the state-of-the-art result \(\widetilde{O}(d\sqrt{H^{3}V_{1}^{*}K})\)[26] by a factor of \(\sqrt{H}\).

## 6 Conclusion

In this work, we propose two computationally efficient algorithms for heavy-tailed linear bandits and linear MDPs, respectively. Our proposed algorithms, termed as Heavy-DFUL and Heavy-LSVI-UCB, are based on a novel self-normalized concentration inequality for adaptive Huber regression, which may be of independent interest. Heavy-DFUL and Heavy-LSVI-UCB achieve minimax optimal and instance-dependent regret bounds scaling with the central moments. We also provide a lower bound for linear MDPs with heavy-tailed rewards to demonstrate the optimality of Heavy-LSVI-UCB. To the best of our knowledge, we are the first to study heavy-tailed rewards in RL with function approximation and provide a new algorithm for this setting which is both statistically and computationally efficient.

## Acknowledgments

Liwei Wang is supported in part by NSF IIS 2110170, NSF DMS 2134106, NSF CCF 2212261, NSF IIS 2143493, NSF CCF 2019844, NSF IIS 2229881. Lin F. Yang is supported in part by NSF grant 2221871, and an Amazon Research Grant.

## References

* [1] Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In _Advances in Neural Information Processing Systems_, volume 24, 2011.
* [2] Alekh Agarwal, Yujia Jin, and Tong Zhang. VOQL: Towards optimal regret in model-free rl with nonlinear function approximation. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 987-1063. PMLR, 2023.
* [3] Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. _Advances in neural information processing systems_, 21, 2008.
* [4] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. Model-based reinforcement learning with value-targeted regression. In _International Conference on Machine Learning_, pages 463-474. PMLR, 2020.
* [5] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* [6] Sujay Bhatt, Guanhua Fang, Ping Li, and Gennady Samorodnitsky. Nearly optimal catoni's m-estimator for infinite variance. In _International Conference on Machine Learning_, pages 1925-1944. PMLR, 2022.
* [7] Sebastien Bubeck, Nicolo Cesa-Bianchi, and Gabor Lugosi. Bandits with heavy tail. _IEEE Transactions on Information Theory_, 59(11):7711-7717, 2013.
* [8] Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* [9] Olivier Catoni. Challenging the empirical mean and empirical variance: a deviation study. In _Annales de l'IHP Probabilites et statistiques_, volume 48, pages 1148-1185, 2012.
* [10] Peng Chen, Xinghu Jin, Xiang Li, and Lihu Xu. A generalized catoni's m-estimator under finite \(\alpha\)-th moment assumption with \(\alpha\in\)(1, 2). _Electronic Journal of Statistics_, 15(2):5523-5544, 2021.
* [11] Hana Choi, Carl F Mela, Santiago R Balseiro, and Adam Leary. Online display advertising markets: A literature review and future directions. _Information Systems Research_, 31(2):556-575, 2020.
* [12] Rama Cont. Empirical properties of asset returns: stylized facts and statistical issues. _Quantitative finance_, 1(2):223, 2001.
* [13] David A Freedman. On tail probabilities for martingales. _the Annals of Probability_, pages 100-118, 1975.
* [14] A Ben Hamza and Hamid Krim. Image denoising: A nonlinear robust statistical approach. _IEEE transactions on signal processing_, 49(12):3045-3054, 2001.
* [15] Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear markov decision processes. In _International Conference on Machine Learning_, pages 12790-12822. PMLR, 2023.
* [16] Pibe Hu, Yu Chen, and Longbo Huang. Nearly minimax optimal reinforcement learning with linear function approximation. In _International Conference on Machine Learning_, pages 8971-9019, 2022.

* Huber [1964] Peter J Huber. Robust estimation of a location parameter. _The Annals of Mathematical Statistics_, pages 73-101, 1964.
* Hull [2012] John Hull. _Risk management and financial institutions,+ Web Site_, volume 733. John Wiley & Sons, 2012.
* Jebarajakirthy et al. [2021] Charles Jebarajakirthy, Haroon Iqbal Maseeh, Zakir Morshed, Amit Shankar, Denni Arli, and Robin Pentecost. Mobile advertising: A systematic literature review and future research agenda. _International Journal of Consumer Studies_, 45(6):1258-1291, 2021.
* Jia et al. [2020] Zeyu Jia, Lin Yang, Csaba Szepesvari, and Mengdi Wang. Model-based reinforcement learning with value-targeted regression. In _Learning for Dynamics and Control_, pages 666-686. PMLR, 2020.
* Jin et al. [2018] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? _Advances in neural information processing systems_, 31, 2018.
* Jin et al. [2020] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR, 2020.
* Kim et al. [2022] Yeoneung Kim, Insoon Yang, and Kwang-Sung Jun. Improved regret analysis for variance-adaptive linear bandits and horizon-free linear mixture mdps. _Advances in Neural Information Processing Systems_, 35:1060-1072, 2022.
* Kirschner and Krause [2018] Johannes Kirschner and Andreas Krause. Information directed sampling and bandits with heteroscedastic noise. In _Conference On Learning Theory_, pages 358-384. PMLR, 2018.
* Li et al. [2023] Gen Li, Changxiao Cai, Yuxin Chen, Yuting Wei, and Yuejie Chi. Is q-learning minimax optimal? a tight sample complexity analysis. _Operations Research_, 2023.
* Li and Sun [2023] Xiang Li and Qiang Sun. Variance-aware robust reinforcement learning with linear function approximation with heavy-tailed rewards. _arXiv preprint arXiv:2303.05606_, 2023.
* Lugosi and Mendelson [2019] Gabor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. _Foundations of Computational Mathematics_, 19(5):1145-1190, 2019.
* Medina and Yang [2016] Andres Munoz Medina and Scott Yang. No-regret algorithms for heavy-tailed linear bandits. In _International Conference on Machine Learning_, pages 1642-1650. PMLR, 2016.
* Modi et al. [2020] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement learning using linearly combined model ensembles. In _International Conference on Artificial Intelligence and Statistics_, pages 2010-2020. PMLR, 2020.
* Puterman [2014] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Shao et al. [2018] Han Shao, Xiaotian Yu, Irwin King, and Michael R Lyu. Almost optimal algorithms for linear stochastic bandits with heavy-tailed payoffs. _Advances in Neural Information Processing Systems_, 31, 2018.
* Sun et al. [2020] Qiang Sun, Wen-Xin Zhou, and Jianqing Fan. Adaptive huber regression. _Journal of the American Statistical Association_, 115(529):254-265, 2020.
* Wagenmaker et al. [2022] Andrew J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson. First-order regret in reinforcement learning with linear function approximation: A robust estimation approach. In _International Conference on Machine Learning_, pages 22384-22429. PMLR, 2022.
* Xue et al. [2020] Bo Xue, Guanghui Wang, Yimu Wang, and Lijun Zhang. Nearly optimal regret for stochastic linear bandits with heavy-tailed payoffs. In _Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20_, pages 2936-2942. International Joint Conferences on Artificial Intelligence Organization, 2020.

* [35] Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _International Conference on Machine Learning_, pages 6995-7004. PMLR, 2019.
* [36] Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* [37] Zihan Zhang, Jiaqi Yang, Xiangyang Ji, and Simon S Du. Improved variance-aware confidence sets for linear bandits and linear mixture mdp. _Advances in Neural Information Processing Systems_, 34:4342-4355, 2021.
* [38] Heyang Zhao, Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency. In _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 4977-5020. PMLR, 2023.
* [39] Han Zhong and Tong Zhang. A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes. _arXiv preprint arXiv:2305.08841_, 2023.
* [40] Han Zhong, Jiayi Huang, Lin Yang, and Liwei Wang. Breaking the moments condition barrier: No-regret algorithm for bandits with super heavy-tailed payoffs. _Advances in Neural Information Processing Systems_, 34:15710-15720, 2021.
* [41] Dongruo Zhou and Quanquan Gu. Computationally efficient horizon-free reinforcement learning for linear mixture mdps. _Advances in neural information processing systems_, 35:36337-36349, 2022.
* [42] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR, 2021.
* [43] Runlong Zhou, Zhang Zihan, and Simon Shaolei Du. Sharp variance-dependent bounds in reinforcement learning: Best of both worlds in stochastic and deterministic environments. In _International Conference on Machine Learning_, pages 42878-42914. PMLR, 2023.
* [44] Vincent Zhuang and Yanan Sui. No-regret reinforcement learning with heavy-tailed rewards. In _International Conference on Artificial Intelligence and Statistics_, pages 3385-3393. PMLR, 2021.