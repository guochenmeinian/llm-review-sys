# Noise-Aware Differentially Private Regression via Meta-Learning

 Ossi Raisa

University of Helsinki

ossi.raisa@helsinki.fi &Stratis Markou

University of Cambridge

em626@cam.ac.uk &Matthew Ashman

University of Cambridge

mca39@cam.ac.uk &Wessel P. Bruinsma

Microsoft Research AI for Science

wessel.p.bruinsma@gmail.com &Marlon Tobaben

University of Helsinki

marlon.tobaben@helsinki.fi &Antti Honkela

University of Helsinki

antti.honkela@helsinki.fi &Richard E. Turner

University of Cambridge

ret26@cam.ac.uk

Equal contribution.

###### Abstract

Many high-stakes applications require machine learning models that protect user privacy and provide well-calibrated, accurate predictions. While Differential Privacy (DP) is the gold standard for protecting user privacy, standard DP mechanisms typically significantly impair performance. One approach to mitigating this issue is pre-training models on simulated data before DP learning on the private data. In this work we go a step further, using simulated data to train a meta-learning model that combines the Convolutional Conditional Neural Process (ConvCNP) with an improved functional DP mechanism of Hall et al. (2013) yielding the DPConvCNP. DPConvCNP learns from simulated data how to map private data to a DP predictive model in one forward pass, and then provides accurate, well-calibrated predictions. We compare DPConvCNP with a DP Gaussian Process (GP) baseline with carefully tuned hyperparameters. The DPConvCNP outperforms the GP baseline, especially on non-Gaussian data, yet is much faster at test time and requires less tuning.

## 1 Introduction

Deep learning has achieved tremendous success across a range of domains, especially in settings where large datasets are publicly available. However, in many impactful applications such as healthcare, the data may contain sensitive information about users, whose privacy we want to protect. Differential Privacy (DP; Dwork et al., 2006) is the gold standard framework for protecting user privacy, as it provides strong guarantees on the privacy loss incurred on users participating in a dataset. However, enforcing DP often significantly impairs performance. A recently proposed method to mitigate this issue is to pre-train a model on non-private data, e.g. from a simulator (Tang et al., 2023), and then fine-tune it under DP on real private data (Yu et al., 2021; Li et al., 2022; De et al., 2022).

We go a step further and train a _meta-learning_ model with a DP mechanism inside it (Figure 1). While supervised learning is about learning a mapping from inputs to outputs using a learning algorithm, in meta-learning we learn a learning algorithm directly from the data, by _meta-training_, enabling generalisation to new datasets during _meta-testing_. Our model is meta-trained on simulated datasets,each of which is split into a _context_ and _target_ set, learning to make predictions at the target inputs given the context set. At meta-test-time, the model takes a context set of real data, which is protected by the DP mechanism, and produces noise-aware predictions and accurate uncertainty estimates.

**Neural Processes.** Our method is based on neural processes [NPs; Garnelo et al., 2018a], a model which leverages the flexibility of neural networks to produce well-calibrated predictions in the meta-learning setting. The parameters of the NP are meta-trained to generalise to unseen datasets, while adapting to new contexts much faster than gradient-based fine-tuning alternatives [Finn et al., 2017].

**Convolutional NPs.** We focus on convolutional conditional NPs [ConvCNPs; Gordon et al., 2020], a type of NP that has remarkably strong performance in challenging regression problems. That is because the ConvCNP is translation equivariant [TE; Cohen and Welling, 2016], so its outputs change predictably whenever the input data are translated. This is an extremely useful inductive bias when modelling, for example, stationary data. The ConvCNP architecture also makes it natural to embed an especially effective DP mechanism inside it using the _functional mechanism_[Hall et al., 2013] to protect the privacy of the context set (Figure 1). We call the resulting model the DPConvCNP.

**Training with a DP mechanism.** A crucial aspect of our approach is training the DPConvCNP on non-sensitive data _with the DP mechanism in the training loop_. The mechanism involves clipping and adding noise, so applying it only during testing would create a mismatch between training and testing. Training with the mechanism eliminates this mismatch, ensuring calibrated predictions (Figure 2).

**Overview of contributions.** In summary, our main contributions in this work are as follows.

1. We introduce the DPConvCNP, a meta-learning model which extends the ConvCNP using the functional DP mechanism [Hall et al., 2013]. The model is meta-trained with the mechanism in place, learning to make calibrated predictions from the context data under DP.
2. We improve upon the functional mechanism of Hall et al. [2013] by leveraging Gaussian DP theory [Dong et al., 2022], showing that context set privacy can be protected with smaller amounts of noise (at least 25% lower standard deviation in the settings considered in Figure 4). We incorporate these improvements into DPConvCNP, but note that they are also of interest in any use case of the functional mechanism.
3. We conduct a study on synthetic and sim-to-real tasks. Remarkably, even with relatively few context points (a few hundreds) and modest privacy budgets, the predictions of the DPConvCNP are surprisingly close to those of the non-DP optimal Bayes predictor. Further, we find that a single DPConvCNP can be trained to generalise across generative processes with different statistics and privacy budgets. We also evaluate the DPConvCNP by training it on synthetic data, and testing it on a real dataset in the small data regime. In all cases, the DPConvCNP produces well calibrated predictions, and is competitive with a carefully tuned DP Gaussian process baseline.

## 2 Related Work

Training deep learning models on public proxy datasets and then fine-tuning with DP on private data is becoming increasingly common in computer vision and natural language processing applications

Figure 1: Meta-training (left) and meta-testing (right) using our method. We train a model on multiple tasks with non-private (simulated or proxy) data to predict on target \((t)\) points using the context \((c)\) points. Crucially, by including a DP mechanism, which clips and adds noise to the data _during training_, the parameter updates (dashed arrow) teach the model to make well-calibrated and accurate predictions in the presence of DP noise. At test time, we deploy the model on real data using the same mechanism, which protects the context set with DP guarantees.

[Yu et al., 2021; Li et al., 2022; De et al., 2022; Tobaben et al., 2023]. However, these approaches rely on the availability of very large non-sensitive datasets. Because these datasets would likely need to be scraped from the internet, it is unclear whether they are actually non-sensitive [Tramer et al., 2024]. On the other hand, other approaches study meta-learning with DP during meta-training [Li et al., 2020; Zhou and Bassily, 2022], but do not enforce privacy guarantees at meta-test time.

Our approach fills a gap in the literature by enforcing privacy of the meta-test data with DP guarantees (see Figure 1), and using non-sensitive proxy data during meta-training. Unlike other approaches which rely on large fine tuning datasets, our method produces well-calibrated predictions even for relatively small datasets (a few hundred datapoints). In this respect, the work of Smith et al. (2018), who study Gaussian process (GP) regression under DP for the small data regime, is perhaps most similar to ours. However, Smith et al. (2018) enforce privacy constraints only with respect to the output variables and do not protect the input variables, whereas our approach protects both.

In terms of theory, there is fairly limited prior work on releasing functions with DP guarantees. Our method is based on the functional DP mechanism of Hall et al. (2013) which works by adding noise from a GP to a function to be released. This approach works especially well when the function lies in a reproducing kernel Hilbert space (RKHS), a property which we leverage in the DPConvCNP. We improve on the original functional mechanism by leveraging Gaussian DP theory of Dong et al. (2022). In related work, Alda and Rubinstein (2017) develop the Bernstein DP mechanism, which adds noise to the coefficients of the Bernstein polynomial of the released function, and Mirshani et al. (2019) generalise the functional mechanism beyond RKHSs. Jiang et al. (2023) derive Renyi differential privacy (RDP; Mironov, 2017) bounds for the mechanism of Hall et al. (2013).

## 3 Background

We start by laying the necessary background. In Section 3.1, we outline meta-learning and NPs, focusing on the ConvCNP. In Section 3.2 we introduce DP, and the functional mechanism of Hall et al. (2013). We keep the discussion on DP lightweight, deferring technical details to Appendix A.

### Meta-learning and Neural Processes

**Supervised learning.** Let \(\mathcal{D}\) be the set of datasets consisting of \((x,y)\)-pairs with \(x\in\mathcal{X}\subset\mathbb{R}^{d}\) and \(y\in\mathcal{Y}\subset\mathbb{R}\). The goal of supervised learning is to use a dataset \(D\in\mathcal{D}\) to learn appropriate parameters \(\theta\) for a conditional distribution \(p(y|x,\theta)\), which maximise the predictive log-likelihood on unseen, randomly sampled test pairs \((x^{*},y^{*})\), i.e. \(\mathcal{L}(\theta,(x^{*},y^{*}))=\log p(y^{*}|x^{*},\theta)\). Let us denote the entire algorithm that performs learning, followed by prediction, by \(\pi\), that is \(\pi(x^{*},D)=p(\cdot|x^{*},\theta^{*}),\) where \(\theta^{*}=\arg\max_{\theta}\mathcal{L}(r,D)\). Supervised learning is concerned with designing a hand-crafted \(\pi\), e.g. picking an appropriate architecture and optimiser, which is trained on a single dataset \(D\).

**Meta-learning.** Meta-learning can be regarded as supervised learning of the function \(\pi\) itself. In this setting, \(D\) is regarded as part of a single training example, which means that a meta-learning algorithm can handle different \(D\) at test time. Concretely, in meta-learning, we have \(\pi_{\theta,\phi}(x^{*},D)=p(\cdot|x^{*},\theta,r_{\phi}(D)),\) where \(r_{\phi}\) is now a function that produces task-specific parameters, adapted for \(D\). The meta-training set now consists of a collection of datasets \((D_{m})_{m=1}^{M},\) often referred to as _tasks_. Each task is partitioned into a context set \(D^{(c)}=(\mathbf{x}^{(c)},\mathbf{y}^{(c)})\) and a target set \(D^{(t)}=(\mathbf{x}^{(t)},\mathbf{y}^{(t)})\). We refer to \(\mathbf{x}^{(c)}\)and \(\mathbf{y}^{(c)}\) as the _context inputs and outputs_ and to \(\mathbf{x}^{(t)}\) and \(\mathbf{y}^{(t)}\) as the _target inputs and outputs_. To meta-train a meta-learning model, we optimise its predictive log-likelihood, averaged over tasks, i.e. \(\mathbb{E}_{D}[\mathcal{L}(\theta,\phi,D)]=\mathbb{E}_{D}[\log\pi_{\theta,\phi} (\mathbf{x}^{(t)},D^{(c)}(\mathbf{y}^{(t)})].\) Meta-learning algorithms are broadly categorised into two groups, based on the choice of \(r_{\phi}\)(Bronskill, 2020).

Figure 2: Training our proposed model with a DP mechanism inside it, enables the model to make accurate well-calibrated predictions, even for modest privacy budgets and dataset sizes. Here, the context data (black) are protected with different \((\epsilon,\delta)\) DP budgets as indicated. The model makes predictions (blue) that are remarkably close to the optimal (non-private) Bayes predictor.

**Gradient based vs amortised meta-learning.** On one hand, gradient-based methods, such as MAML (Finn et al., 2017) and its variants (e.g. (Nichol et al., 2018)) rely on gradient-based fine-tuning at test time. Concretely, these let \(r_{\phi}\) be a function that performs gradient-based optimisation. For such algorithms, we can enforce DP with respect to a meta-test time dataset by fine-tuning with a DP optimisation algorithm, such as DP-SGD (Abadi et al., 2016). While generally effective, such approaches can require significant resources for fine-tuning at meta-test-time, as well as careful DP hyper-parameter tuning to work at all. On the other hand, there are amortised methods, such as neural processes (Garnelo et al., 2018), prototypical networks (Snell et al., 2017), and matching networks (Vinyals et al., 2016), in which \(r_{\phi}\) is a learnable function, such as a neural network. This approach has the advantage that it requires far less compute and memory at meta-test-time. In this work, we focus on neural processes (NPs), and show how \(r_{\phi}\) can be augmented with a DP mechanism to make well calibrated predictions, while protecting the context data at meta test time.

**Neural Processes.** Neural processes (NPs) are a type of model which leverage the flexibility of neural networks to produce well calibrated predictions. A range of NP variants have been developed, including conditional NPs (CNPs; Garnelo et al., 2018), latent-variable NPs (LNPs; Garnelo et al., 2018), Gaussian NPs (GNPs; Markou et al., 2022), score-based NPs Dutordoir et al. (2023), and autoregressive NPs (Bruinsma et al., 2023). In this work, we focus on CNPs because these are ideally suited for our purposes, but our framework can be extended to other variants. A CNP consists of an _encoder_\(\mathtt{enc}_{\phi}\), and a _decoder_\(\mathtt{dec}_{\theta}\). The encoder is a neural network which ingests a context set \(D^{(c)}\in\mathcal{D}\) and outputs a representation \(r\) in some representation space \(\mathcal{R}\). Two concrete examples of such encoders are DeepSets (Zaheer et al., 2017) and SetConv layers (Gordon et al., 2020). The decoder is another neural network, with parameters \(\theta\), which takes the representation \(r\) together with target inputs \(\mathbf{x}^{(t)}\) and produces predictions for the corresponding \(\mathbf{y}^{(t)}\). In summary

\[\pi_{\phi,\theta}(\mathbf{x}^{(t)},D^{(c)})=\mathtt{dec}_{\theta}(\mathbf{x} ^{(t)},r),\quad r=\mathtt{enc}_{\phi}(D^{(c)}).\] (1)

In CNPs, a standard choice, which we also use here, is to let \(\pi_{\phi,\theta}(\mathbf{x}^{(t)},D^{(c)})\) return a mean \(\mu_{\phi,\theta}(x^{(t)},D^{(c)})\) and a variance \(\sigma^{2}_{\phi,\theta}(x^{(t)},D^{(c)})\), to parameterise a predictive distribution that factorises across the target points \(y^{(t)}|x^{(t)}\sim\mathcal{N}(\mu_{\phi,\theta}(x^{(t)},D^{(c)}),\sigma^{2}_ {\phi,\theta}(x^{(t)},D^{(c)}))\). We note that our framework straightforwardly extends to more complicated \(\pi_{\phi,\theta}(\mathbf{x}^{(t)},D^{(c)})\). To train a CNP to make accurate predictions, we can optimise a log-likelihood objective Garnelo et al. (2018) such as

\[\mathcal{L}(\theta,\phi)=\mathbb{E}_{D}\left[\log\mathcal{N}\left(\mathbf{y} _{m}^{(t)}|\mu_{\phi,\theta}(\mathbf{x}_{m}^{(t)},D^{(c)}),\sigma^{2}_{\phi, \theta}(\mathbf{x}^{(t)},D^{(c)})\right)\right],\] (2)

where the expectation is taken over the distribution over tasks \(D\). This objective is optimised by presenting each task \(D_{m}\) to the CNP, computing the gradient of the loss with back-propagation, and updating the parameters \((\phi,\theta)\) of the CNP with any first-order optimiser (see alg. 1). This process trains the CNP to make well-calibrated predictions for \(D^{(t)}\) given \(D^{(c)}\). At test time, given a new \(D^{(c)},\) we can use \(\pi_{\phi,\theta}\) which can be queried at arbitrary target inputs, to obtain corresponding predictions (alg. 2).

**Convolutional CNPs.** Whenever we have useful inductive biases or other prior knowledge, we can leverage these by building them directly into the encoder and the decoder of the CNP. Stationarity is a powerful inductive bias that is often encountered in potentially sensitive applications such as time series or spatio-temporal regression. Whenever the generating process is stationary, the corresponding Bayesian predictive posterior is TE (Foong et al., 2020). ConvCNPs leverage this inductive bias using TE architectures (Cohen and Welling, 2016; Huang et al., 2023).

**ConvCNP encoder.** To achieve TE, the ConvCNP encoder produces an \(r\) that is itself a TE function.

Specifically, \(\text{enc}_{\phi}\) maps the context \(D^{(c)}=((x_{n}^{(c)},y_{n}^{(c)}))_{n=1}^{N}\) to the function \(r:\mathcal{X}\rightarrow\mathbb{R}^{2}\)

\[r(x)=\begin{bmatrix}r^{(d)}(x)\\ r^{(s)}(x)\end{bmatrix}=\sum_{n=1}^{N}\begin{bmatrix}1\\ y_{n}^{(c)}\end{bmatrix}\psi\left(\frac{x-x_{n}^{(c)}}{\lambda}\right),\] (3)

where \(\psi\) is the Gaussian radial basis function (RBF) and \(\phi=\{\lambda\}\). We refer to the two channels of \(r\) as the _density_\(r^{(d)}\) and the _signal_\(r^{(s)}\) channels, which can be viewed as a smoothed version of \(D^{(c)}\). The density channel carries information about the inputs of the context data, while the signal channel carries information about the outputs. This encoder is referred to as the SetConv.

**ConvCNP decoder.** Once \(r\) has been computed, it is passed to the decoder which performs three steps. First, it discreises \(r\) using a pre-specified resolution. Then, it applies a CNN to the discretised signal, and finally it uses an RBF smoother akin to Equation (3) to make predictions at arbitrary target locations. The aforementioned steps are all TE so, composing them with the TE encoder produces a TE prediction map (Bronstein et al., 2021). The ConvCNP has universal approximator properties and produces state-of-the-art, well-calibrated predictions (Gordon et al., 2020).

### Differential Privacy

Differential privacy (Dwork et al., 2006; Dwork and Roth, 2014) quantifies the maximal privacy loss to data subjects that can occur when the results of analysis are released. The loss is quantified by two numbers, \(\epsilon\) and \(\delta\), which bound the change in the distribution of the output of an algorithm, when the data of a single data subject in the dataset change.

**Definition 3.1**.: _An algorithm \(\mathcal{M}\)\(is\)\((\epsilon,\delta)\)-DP if for neighbouring \(D,D^{\prime}\) and all measurable sets \(S\)_

\[\Pr(\mathcal{M}(D)\in S)\leq e^{\epsilon}\Pr(\mathcal{M}(D^{\prime})\in S)+\delta.\] (4)

We consider \(D\in\mathbb{R}^{N\times d}\) with \(N\) users and \(d\) dimensions, and use the _substitution neighbourhood_ relation \(\sim_{S}\) where \(D\sim_{S}D^{\prime}\) if \(D\) and \(D^{\prime}\) differ by at most one row.

**Gaussian DP.** In Section 3.3 we discuss the functional mechanism of Hall et al. (2013), which we use in the ConvCNP. However, the original privacy guarantees derived by Hall et al. (2013) are suboptimal. We improve upon these using the framework of Gaussian DP (GDP; Dong et al., 2022). Dong et al. (2022) define GDP from a hypothesis testing perspective, which is not necessary for our purposes. Instead, we present GDP through the following conversion formula between GDP and DP.

**Definition 3.2**.: _A mechanism \(\mathcal{M}\) is \(\mu\)-GDP if and only if it is \((\epsilon,\delta(\epsilon))\)-DP for all \(\epsilon\geq 0\), where_

\[\delta(\epsilon)=\Phi\left(-\frac{\epsilon}{\mu}+\frac{\mu}{2}\right)-e^{ \epsilon}\Phi\left(-\frac{\epsilon}{\mu}-\frac{\mu}{2}\right)\] (5)

_and \(\Phi\) is the CDF of the standard Gaussian distribution._

**Properties of (G)DP.** Differential privacy has several useful properties. First, _post-processing immunity_ guarantees that post-processing the result of a DP algorithm does not cause privacy loss:

**Theorem 3.3** (Dwork and Roth 2014).: _Let \(\mathcal{M}\) be an \((\epsilon,\delta)\)-DP (or \(\mu\)-GDP) algorithm and let \(f\) be any, possibly randomised, function. Then \(f\circ\mathcal{M}\) is \((\epsilon,\delta)\)-DP (or \(\mu\)-GDP)._

Figure 3: Left; Illustration of the ConvCNP encoder enc\({}_{\phi}\). Black crosses show an example context set \(D^{(c)}\). The density channel \(r^{(d)}\) is shown in purple and the signal channel \(r^{(r)}\) is shown in red. The representation \(r\) consists of concatenating \(r^{(d)}\) and \(r^{(s)}\). Right; Illustration of the DPConvCNP encoder. Black crosses show an example context \(D^{(c)}\), clipped with a threshold \(C\) (gray dashed). Here, a single point (rightmost) is clipped (gray cross shows value before clipping). The density and signal channels are computed and GP noise is added to obtain the DP representation (red & purple).

_Composition_ of DP mechanisms refers to running multiple mechanisms on the same data. When each mechanism can depend on the outputs of the previous mechanisms, the composition is called _adaptive_. GDP is particularly appealing because it has a simple and tight composition formula:

**Theorem 3.4** (Dong et al. 2022).: _The adaptive composition of \(T\) mechanisms that are \(\mu_{i}\)-GDP (\(i=1,\ldots,T\)), is \(\mu\)-GDP with \(\mu=\sqrt{\mu_{1}^{2}+\cdots+\mu_{T}^{2}}\)._

**Gaussian mechanism.** One of the central mechanisms to guarantee DP, is the Gaussian mechanism. This releases the output of a function \(f\) with added Gaussian noise

\[\mathcal{M}(D)=f(D)+\mathcal{N}(0,\sigma^{2}I),\] (6)

where the variance \(\sigma^{2}\) depends on the \(l_{2}\)-sensitivity of \(f,\) defined as

\[\Delta=\sup_{D\sim D^{\prime}}||f(D)-f(D^{\prime})||_{2}.\] (7)

**Theorem 3.5** (Dong et al. 2022).: _The Gaussian mechanism with variance \(\sigma^{2}=\nicefrac{{\Delta^{2}}}{{\mu^{2}}}\) is \(\mu\)-GDP._

### The Functional Mechanism

Now we turn to the functional mechanism of Hall et al. (2013). Given a dataset \(D\in\mathbb{R}^{N\times d},\) the functional mechanism releases a function \(f_{D}\colon T\to\mathbb{R},\) where \(T\subset\mathbb{R}^{d},\) with added noise from a Gaussian process. For simplicity, here we only define the functional mechanism for functions in a _reproducible kernel Hilbert space_ (RKHS), and defer the more general definition to Appendix A.2.

**Definition 3.6**.: _Let \(g\) be a sample path of a Gaussian process having mean zero and covariance function \(k\), and let \(\mathcal{H}\) be an RKHS with kernel \(k\). Let \(\{f_{D}:D\in\mathcal{D}\}\subset\mathcal{H}\) be a family of functions indexed by datasets, satisfying_

\[\Delta_{\mathcal{H}}f\stackrel{{\mathrm{def}}}{{=}}\sup_{D\sim D ^{\prime}}||f_{D}-f_{D^{\prime}}||_{\mathcal{H}}\leq\Delta.\] (8)

_The functional mechanism with multiplier \(c\) and sensitivity \(\Delta\) is defined as_

\[\mathcal{M}(D)=f_{D}+cg.\] (9)

**Theorem 3.7** (Hall et al.).: _If \(\epsilon\leq 1\), the mechanism in Def. 3.6 with \(c=\frac{\Delta}{\epsilon}\sqrt{2\ln(2/\delta)}\) is \((\epsilon,\delta)\)-DP._

## 4 Differential privacy for the ConvCNP

Now we turn to our main contributions. First, we tighten the functional mechanism privacy analysis in Section 4.1 and then we build the functional mechanism into the ConvCNP in Section 4.2.

### Improving the Functional Mechanism

The privacy bounds given by Theorem 3.7 are suboptimal, and do not allow us to use the tight composition formula from Theorem 3.4. However, the proof of Theorem 3.7 builds on the classical Gaussian mechanism privacy bounds, which we can replace with the GDP theory from Section 3.2. As demonstrated in Figure 4, our bound offers significantly smaller \(\epsilon\) for the same noise standard deviation, compared to the existing bounds of Hall et al. (2013) and Jiang et al. (2023).

**Theorem 4.1**.: _The functional mechanism with sensitivity \(\Delta\) and multiplier \(c=\nicefrac{{\Delta}}{{\mu}}\) is \(\mu\)-GDP._

Proof.: The proof of Theorem 3.7 from Hall et al. (2013) shows that any \((\epsilon,\delta)\)-DP bound for the Gaussian mechanism carries over to the functional mechanism. Replacing the classical Gaussian mechanism bound with the GDP bound proves the claim. For details, see Appendix A.

### Differentially Private Convolutional CNP

**Differentially Private SetConv.** Now we turn to building the functional DP mechanism into the ConvCNP. We want to modify the SetConv encoder (Eq. 3) to make it DP. As a reminder, the SetConv outputs the density \(r^{(d)}\) and signal \(r^{(s)}\) channels

\[\begin{bmatrix}r^{(d)}(x)\\ r^{(s)}(x)\end{bmatrix}=\sum_{n=1}^{N}\begin{bmatrix}1\\ y_{n}^{(c)}\end{bmatrix}\psi\left(\frac{x-x_{n}^{(c)}}{\lambda}\right),\] (10)

which are the two quantities we want to release under DP. To achieve this, we must first determine the sensitivity of \(r^{(d)}\) and \(r^{(s)}\), as defined in Eq. 8. Recall that we use the substitution neighbourhood relation \(\sim_{S}\), defined as \(D_{1}^{(c)}\sim_{S}D_{2}^{(c)}\) if \(D_{1}^{(c)}\) and \(D_{2}^{(c)}\) differ in at most one row, i.e. by a single context point. Since the RBF \(\psi\) is bounded above by \(1\), it can be shown (see Appendix A.4) that the squared \(l_{2}\)-sensitivity of \(r^{(d)}\) is bounded above by \(2\), and this bound is tight. Unfortunately however, since the signal channel \(r^{(s)}\) depends linearly on each \(y_{n}^{(c)}\) (see Eq. 10), its sensitivity is unbounded. To address this, we clip each \(y_{n}^{(c)}\) by a threshold \(C\), which is a standard way to ensure the sensitivity is bounded. With this modification we obtain the following tight sensitivities for \(r^{(d)}\) and \(r^{(s)}\):

\[\Delta_{\mathcal{H}}^{2}r^{(d)}=2,\quad\Delta_{\mathcal{H}}^{2}r^{(s)}=4C^{2}\] (11)

With these in place, we can state our privacy guarantee which forms the basis of the DPConvCNP. Post-processing immunity (Theorem 3.3) ensures that post-processing \(r^{(s)}\) and \(r^{(d)}\) with the ConvCNP decoder does not result in further privacy loss.

**Theorem 4.2**.: _Let \(g_{d}\) and \(g_{s}\) be sample paths of two independent Gaussian processes having zero mean and covariance function \(k\), such that \(0\leq k\leq C_{k}\) for some \(C_{k}>0\). Let \(\Delta_{d}^{2}=2C_{k}\) and \(\Delta_{s}^{2}=4C^{2}C_{k}\). Then releasing \(r^{(d)}+\sigma_{d}g_{d}\) and \(r^{(s)}+\sigma_{s}g_{s}\) is \(\mu\)-GDP with \(\mu=\sqrt{\nicefrac{{\Delta_{s}^{2}}}{{\sigma_{s}^{2}}}+\nicefrac{{\Delta_ {d}^{2}}}{{\sigma_{d}^{2}}}}\)._

Proof.: The result follows by starting from the GDP bound of the mechanism in Theorem 4.1 and using Theorem 3.4 to combine the privacy costs for the releases of \(r^{(d)}\) and \(r^{(s)}\). 

**Corollary 4.3**.: _Algorithm 2 with the DPSetConv encoder from Algorithm 3 is \((\epsilon,\delta)\)-DP with respect to the real context set \(D^{(c)}\)._

Proof.: The noise_scales method in Algorithm 3 computes the appropriate \(\sigma_{d}\) and \(\sigma_{s}\) values from Theorem 4.2 and Definition 3.2 such that releasing the functional encodings \(r^{(d)}+\sigma_{d}g_{d}\) and \(r^{(s)}+\sigma_{s}g_{s}\) is \((\epsilon,\delta)\)-DP. The \((\epsilon,\delta)\)-DP guarantee extends [11, Proposition 5] to the point evaluations \(\mathbf{r}^{(d)}\) and \(\mathbf{r}^{(s)}\) over the grid \(\mathbf{x}\) in Algorithm 3. Post-processing immunity (Theorem 3.3) extends \((\epsilon,\delta)\)-DP to Algorithm 2. 

### Training the DPConvCNP

**Training loss and algorithm.** We meta-train the DPConvCNP parameters \(\theta,\phi\) using the CNP log-likelihood (eq. 2) within Algorithm 1, and meta-test it using alg. 2. Importantly, the encoder

Figure 4: Noise magnitude comparison for the classical functional mechanism of Hall et al. [2013], the RDP-based mechanism of Jiang et al. [2023] and our improved GDP-based mechanism. The line for Hall et. cuts off at \(\epsilon=1\) since their bound has only been proven for \(\epsilon\leq 1\). We set \(\Delta^{2}=10\) and \(\delta=10^{-3}\), which are representative values from our experiments. See Appendix A.6 for more details.

\(\text{enc}_{\phi}\) now includes clipping and adding noise (alg. 3) in its forward pass. Meta-training with the functional in place is crucial, because it teaches the decoder to handle the DP noise and clipping.

**Privacy hyperparameters.** By Definition 3.2 and Theorem 4.2, each \((\epsilon,\delta)\)-budget implies a \(\mu\)-budget, placing a constraint on the sensitivities and noise magnitudes, namely \(\mu^{2}=\nicefrac{{\Delta_{s}^{2}}}{{\sigma_{s}^{2}}}+\nicefrac{{\Delta_{d}^{2} }}{{\sigma_{d}^{2}}}\). Since \(\psi\) is an RBF, \(\Delta_{d}^{2}=2\) and \(\Delta_{s}^{2}=4C^{2}\), and we need to specify \(C,\sigma_{s}\) and \(\sigma_{d}\), subject to this constraint. We introduce a variable \(0<t<1\) and rewrite the constraint as

\[\sigma_{s}^{2}=\frac{4C^{2}}{t\mu^{2}}\quad\text{and}\quad\sigma_{d}^{2}=\frac {2}{(1-t)\mu^{2}}\] (12)

allowing us to freely set \(t\) and \(C\). One straightforward approach is to fix \(t\) and \(C\) to hand-picked values, but this is sub-optimal since the optimal values depend on \(\mu\), \(N\), and the data statistics. Instead, we can make them adaptive, letting \(t:\mathbb{R}^{+}\times\mathbb{N}\rightarrow(0,1)\) and \(C:\mathbb{R}^{+}\times\mathbb{N}\rightarrow\mathbb{R}^{+}\) be learnable functions, e.g. neural networks \(t(\mu,N)=\texttt{sig}(\text{NN}_{t}(\mu,N))\) and \(C(\mu,N)=\text{exp}(\text{NN}_{C}(\mu,N))\) where \(\texttt{sig}\) is the sigmoid. These networks are meta-trained along with all other parameters of the DPConvCNP.

## 5 Experiments & Discussion

We conduct experiments on synthetic and a sim-to-real task with real data. We provide the exact experimental details in Appendix E. We make our implementation of the DPConvCNP public in the repository https://github.com/cambridge-mlg/dpconvcnp.

**DP-SGD baseline.** Since, we are interested in the small-data regime, i.e. a few hundred datapoints per task, we turn to Gaussian processes (GP; Rasmussen and Williams, 2006), the gold-standard model for well-calibrated predictions in this setting. To enforce DP, we make the GP variational (Titsias, 2009), and use DP-SGD (Abadi et al., 2016) to optimise its variational parameters and hyperparameters. This is a strong baseline because GPs excel in small data, and DP-SGD is a state-of-the-art DP fine-tuning algorithm. We found it critical to carefully tune the DP-SGD parameters and the GP initialisation using BayesOpt, and devoted substantial compute on this to ensure we have maximised GP performance. We refer to this baseline as the DP-SVGP. For details see Appendix D.

**General setup.** In both synthetic and sim-to-real experiments, we first tuned the DP as well as the GP initialisation parameters of the DP-SVGP on synthetic data using BayesOpt. We then trained the DPConvCNP on synthetic data from the same generative process. Last, we tested both models on unseen test data. For the DP-SVGP, testing involves DP fine-tuning its variational parameters and its hyperparameters on each test set. For the DPConvCNP, testing involves a single forward pass through the network. We report results in Figures 6 and 7, and discuss them below.

### Synthetic tasks

**Gaussian data.** First, we generated data from a GP with an exponentiated quadratic (EQ) covariance (Figure 6; top), fixing its signal and noise scales, as well as its lengthscale \(\ell\). For each \(\ell\) we sampled datasets with \(N\sim\mathcal{U}[1,512]\) and privacy budgets with \(\epsilon\sim\mathcal{U}[0.90,4.00]\) and \(\delta=10^{-3}\). We trained separate DP-SVGPs and DPConvCNPs for each \(\ell\) and tested them on unseen data from the same generative process (_non-amortised_; Figure 6). These models can handle different privacy budgets but only work well for the lengthscale they were trained on. In practice an appropriate lengthscale is not known _a priori_. To make this task more realistic, we also trained a single DPConvCNP on data with randomly sampled \(\ell\sim\mathcal{U}[0.25,2.00]\) (_amortised_; Figure 6). This model implicitly infers \(\ell\) and simultaneously makes predictions, under DP. We also show the performance of the non-DP Bayes posterior, which is optimal (_oracle_; Figure 6 top). See Appendix E.1 for more details.

**DPConvCNP competes with DP-SVGP.** Even in the Gaussian setting, where the DP-SVGP is given the covariance of the generative process, the DPConvCNP remains competitive (red and

Figure 5: Deployment-time comparison on Gaussian (top) and non-Gaussian (bottom) data. We ran the DP-SVGP for different numbers of DP-SGD steps to determine a speed versus quality-of-fit tradeoff. Reporting 95% confidence intervals.

purple in Figure 6; top). While the DP-SVGP outperforms the DPConvCNP for some \(N\) and \(\ell\), the gaps are typically small. In contrast, the DP-SVGP often fails to provide sensible predictions (see \(\ell=0.25,N\geq 300\)), and tends to overestimate the lengthscale, which is a known challenge in variational GPs (Bauer et al., 2016). We also found that the DP-SVGP tends to underestimate the observation noise, resulting in over-smoothed _and_ over-confident predictions which lead to a counter-intuitive reduction in performance as \(N\) increases. By contrast, the DPConvCNP gracefully handles different \(N\) and recovers predictions that are close to the non-DP Bayesian posterior for modest \(\epsilon\) and \(N\), with runtimes several orders of magnitude faster than the DP-SVGP (Figure 5).

**Amortising over \(\ell\) and privacy budgets.** We observe that the DPConvCNP trained on a range of lengthscales (green; Figure 6) accurately infers the lengthscale of the test data, with only a modest performance reduction compared to its non-amortised counterpart (red). The ability of the DPConvCNP to implicitly infer \(\ell\) while making calibrated predictions is remarkable, given the DP constraints under which it operates. Further, we observe that the DPConvCNP works well across a range of privacy budgets. In preliminary experiments, we found that the performance loss due to amortising over privacy budgets is small. This is particularly appealing because a single DPConvCNP can be trained on a range of budgets and deployed at test time using the privacy level specified by the practitioner, eliminating the need for separate models for different budgets.

**Non-Gaussian synthetic tasks.** We generated data from a non-Gaussian process with sawtooth signals, which has previously been identified as a challenging task Bruinsma et al. (2023). We sampled the waveform direction and phase using a fixed period \(\tau\) and adding Gaussian observation noise with a fixed magnitude. We gave the DP-SVGP an advantage by using a periodic covariance function, and truncating the Fourier series of the waveform signal to make it continuous: otherwise, since the DP-SVGP cannot handle discontinuities in the sawtooth signal, it explains the data mostly as noise, failing catastropically. Again, we trained a separate DP-SVGP and DPConvCNP for each \(\tau\), as well as a single DPConvCNP model on randomly sampled \(\tau^{-1}\sim\mathcal{U}[0.20,1.25]\). We report results in Figure 6 (bottom), along with a non-DP oracle (blue). The Bayes posterior is intractable, so we report the average NLL of the observation noise, which is a lower bound to the NLL.

**DPConvCNP outperforms the DP-SVGP.** We find that, even though we gave the DP-SVGP significant advantages, the DPConvCNP still outperforms it, and produces near-optimal predictions even for modest \(N\) and \(\epsilon\). Overall, our findings in the non-Gaussian tasks mirror those of the Gaussian tasks. The DPConvCNP can amortise over different signal periods with very small performance drops (red, green in Figure 6; bottom). Given the difficulty of this task, the fact that the DPConvCNP can predict accurately for signals with different periods under DP constraints is especially impressive.

Figure 6: Negative log-likelihoods (NLL) of the DPConvCNP and the DP-SVGP baseline on synthetic data from a EQ GP (top two rows; EQ lengthscale \(\ell\)) and non-Gaussian data from sawtooth waveforms (bottom two rows; waveform period \(\tau\)). For each point shown we report the mean NLL with its 95% confidence intervals (error bars too small to see). See Appendix C.2 for example fits.

### Sim-to-real tasks

**Sim-to-real task.** We evaluated the performance of the DPConvCNP in a sim-to-real task, where we train the model on simulated data and test it on the the Dobe!Kung dataset [Howell, 2009], also used by Smith et al. [2018], containing age, weight and height measurements of 544 individuals. We generated data from GPs with a Matern-\(3/2\) covariance, with a fixed signal scale of \(\sigma_{v}=1.00\), randomly sampled noise scale \(\sigma_{n}\sim\mathcal{U}[0.20,0.60]\) and lengthscale \(\ell\sim\mathcal{U}[0.50,2.00]\). We chose Matern-\(3/2\) since its paths are rougher than those of the EQ, and picked hyperparameter ranges via a back-of-the envelope calculation, without tuning them for the task. We trained a single DP-SVGP and a DPConvCNP with \(\epsilon\sim\mathcal{U}[0.90,4.00]\) and \(\delta=10^{-3}\). We consider two test tasks: predicting the height or the weight of an individual from their age. For each \(N\), we split the dataset into a context and target at random, repeating the procedure for multiple splits.

**Sim-to-real comparison.** While the two models perform similarly for large \(N\), the DPConvCNP performs much better for smaller \(N\) (Figure 7; left). The DPConvCNP predictions are surprisingly good even for strong privacy guarantees, e.g. \(\epsilon=1.00,\delta=10^{-3}\), and a modest dataset size (Figure 7; right), and significantly better-calibrated than those of the DP-SVGP, which under-fits. Note we have not tried to tune the simulator or add prior knowledge, which could further improve performance.

## 6 Limitations & Conclusion

**Limitations.** The DPConvCNP does not model dependencies between target outputs, which is a major limitation. This could be achieved straightforwardly by extending our approach to LNPs, GNPs, or ARNPs. Another limitation is that the efficacy of any sim-to-real scheme is limited by the quality of the simulated data. If the real and the simulated data differ substantially, then sim-to-real transfer has little hope of working. This can be mitigated by simulating diverse datasets to ensure the real data are in the training distribution. However, as simulator diversity increases, predictions typically become less certain, so there is a sweet spot in simulator diversity. While we observed strong sim-to-real results, exploring the effect of this diversity is a valuable direction for future work.

**Broader Impacts.** This paper presents work whose goal is to advance the field of DP. Generally, we view the potential for broader impact of this work as generally positive. Ensuring individual user privacy is critical across a host of Machine Learning applications. We believe that methods such as ours, aimed at improving the performance of DP algorithms and improve their practicality, have the potential to have a positive impact on individual users of Machine Learning models.

**Conclusion.** We proposed an approach for DP meta-learning using NPs. We leveraged and improved upon the functional DP mechanism of Hall et al. [2013], and showed how it can be naturally built into the ConvCNP to protect the privacy of the meta-test set with DP guarantees. Our improved bounds for the functional DP mechanism are substantial, providing the same privacy guarantees with a \(\approx 30\%\) lower noise magnitude, and are likely of independent interest. We showed that the DPConvCNP is competitive and often outperforms a carefully tuned DP-SVGP baseline on both Gaussian and non-Gaussian synthetic tasks, while simultaneously being orders of magnitude faster at meta-test time. Lastly, we demonstrated how the DPConvCNP can be used as a sim-to-real model in a realistic evaluation scenario in the small data regime, where it outperforms the DP-SVGP baseline.

Figure 7: Left; Negative log-likelihoods of the DPConvCNP and the DP-SVGP baseline on the sim to real task with the!Kung dataset, predicting individuals’ height from their age (left col.) or their weight from their age (right col.). For each point shown here, we partition each dataset into a context and target at random, make predictions, and repeat this procedure \(512\) times. We report mean NLL with its 95% confidence intervals. Error bars are to small to see here. Right; Example predictions for the DPConvCNP and the DP-SVGP, showing the mean and 95% confidence intervals, with \(N=300,\epsilon=1.00,\delta=10^{-3}\). The DPConvCNP is visibly better-calibrated than the DP-SVGP.

## Acknowledgements

This work was supported in part by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence, FCAI as well as Grants 356499 and 359111), the Strategic Research Council at the Research Council of Finland (Grant 358247) as well as the European Union (Project 101070617). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Commission. Neither the European Union nor the granting authority can be held responsible for them. SM is supported by the Vice Chancellor's and Marie and George Vergotits Scholarship, and the Qualcomm Innovation Fellowship. Richard E. Turner is supported by Google, Amazon, ARM, Improbable, EPSRC grant EP/T005386/1, and the EPSRC Probabilistic AI Hub (ProbAI, EP/Y028783/1).

## References

* Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In _Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security_. ACM, 2016.
* Akiba et al. (2019) Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A next-generation hyperparameter optimization framework. In _The 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 2623-2631, 2019.
* Alda and Rubinstein (2017) Francesco Alda and Benjamin Rubinstein. The Bernstein Mechanism: Function Release under Differential Privacy. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* Bauer et al. (2016) Matthias Bauer, Mark van der Wilk, and Carl Edward Rasmussen. Understanding probabilistic sparse Gaussian process approximations. In _Advances in Neural Information Processing Systems_, volume 29, 2016.
* Bronskill (2020) John Bronskill. _Data and computation efficient meta-learning_. PhD thesis, University of Cambridge, 2020.
* Bronstein et al. (2021) Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _arXiv preprint arXiv:2104.13478_, 2021.
* Bruinsma et al. (2023) Wessel Bruinsma, Stratis Markou, James Requeima, Andrew Y. K. Foong, Tom Andersson, Anna Vaughan, Anthony Buonomo, Scott Hosking, and Richard E Turner. Autoregressive conditional neural processes. In _The Eleventh International Conference on Learning Representations_, 2023.
* Cohen and Welling (2016) Taco Cohen and Max Welling. Group equivariant convolutional networks. In _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2990-2999. PMLR, 2016.
* De et al. (2022) Soham De, Leonard Berrada, Jamie Hayes, Samuel L. Smith, and Borja Balle. Unlocking High-Accuracy Differentially Private Image Classification through Scale. _arXiv preprint arXiv:2204.13650_, 2022.
* Dong et al. (2022) Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian Differential Privacy. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(1):3-37, 2022.
* Dutordoir et al. (2023) Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. Neural diffusion processes. In _International Conference on Machine Learning_, pages 8990-9012. PMLR, 2023.
* Dwork and Roth (2014) Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. _Foundations and Trends in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating Noise to Sensitivity in Private Data Analysis. In _Third Theory of Cryptography Conference_, volume 3876 of _Lecture Notes in Computer Science_, pages 265-284. Springer, 2006.
* Dwork et al. (2016)Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks. In _Proceedings of the 34th International Conference on Machine Learning_. PMLR, 2017.
* Foong et al. (2020) Andrew Foong, Wessel Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and Richard Turner. Meta-learning stationary stochastic process prediction with convolutional neural processes. In _Advances in Neural Information Processing Systems_, volume 33, pages 8284-8295, 2020.
* Garnelo et al. (2018a) Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional Neural Processes. In _Proceedings of the 35th International Conference on Machine Learning_, pages 1704-1713. PMLR, 2018a.
* Garnelo et al. (2018b) Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, and Yee Whye Teh. Neural Processes. _arXiv preprint arXiv:1807.01622_, 2018b.
* Gordon et al. (2020) Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, and Richard E. Turner. Convolutional Conditional Neural Processes. In _International Conference on Learning Representations_, 2020.
* Hall et al. (2013) Rob Hall, Alessandro Rinaldo, and Larry A. Wasserman. Differential privacy for functions and functional data. _Journal of Machine Learning Research_, 14(1):703-727, 2013.
* Howell (2009) Nancy Howell. Dobe!kung census of all population., 2009. URL https://tspace.library.utoronto.ca/handle/1807/17973. License: All right reserved.
* Huang et al. (2023) Daolang Huang, Manuel Haussmann, Ulpu Remes, S. T. John, Gregoire Clarte, Kevin Luck, Samuel Kaski, and Luigi Acerbi. Practical Equivariances via Relational Conditional Neural Processes. In _Advances in Neural Information Processing Systems_, volume 36, 2023.
* Jiang et al. (2023) Dihong Jiang, Sun Sun, and Yaoliang Yu. Functional Renyi Differential Privacy for Generative Modeling. In _Thirty-Seventh Conference on Neural Information Processing Systems_, 2023.
* Li et al. (2020) Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar. Differentially Private Meta-Learning. In _International Conference on Learning Representations_, 2020.
* Li et al. (2022) Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto. Large language models can be strong differentially private learners. In _The Tenth International Conference on Learning Representations, ICLR_, 2022.
* Markou et al. (2022) Stratis Markou, James Requeima, Wessel P. Bruinsma, Anna Vaughan, and Richard E. Turner. Practical conditional neural processes via tractable dependent predictions. In _Proceedings of the 10th International Conference on Learning Representations_, 2022.
* Mironov (2017) Ilya Mironov. Renyi Differential Privacy. In _30th IEEE Computer Security Foundations Symposium_, pages 263-275, 2017.
* Mirshani et al. (2019) Ardalan Mirshani, Matthew Reimherr, and Aleksandra Slavkovic. Formal Privacy for Functional Data with Gaussian Perturbations. In _Proceedings of the 36th International Conference on Machine Learning_, pages 4595-4604. PMLR, May 2019.
* Nichol et al. (2018) Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. _arXiv preprint arXiv:1803.02999_, 2018.
* Rasmussen and Williams (2006) Carl Edward Rasmussen and Christopher KI Williams. _Gaussian Processes for Machine Learning_. MIT Press, 2006.
* Smith et al. (2018) Michael T. Smith, Mauricio A. Alvarez, Max Zwiessele, and Neil D. Lawrence. Differentially Private Regression with Gaussian Processes. In _Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics_, pages 1195-1203. PMLR, 2018.
* Snell et al. (2017) Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical Networks for Few-shot Learning. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* Snell et al. (2018)Xinyu Tang, Ashwinee Panda, Vikash Sehwag, and Prateek Mittal. Differentially Private Image Classification by Learning Priors from Random Processes. In _Thirty-Seventh Conference on Neural Information Processing Systems_, 2023.
* Titsias (2009) Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In _Artificial intelligence and statistics_, pages 567-574. PMLR, 2009.
* Tobaben et al. (2023) Marlon Tobaben, Aliaksandra Shysheya, John Bronskill, Andrew Paverd, Shruti Tople, Santiago Zanella Beguelin, Richard E. Turner, and Antti Honkela. On the efficacy of differentially private few-shot image classification. _Transactions on Machine Learning Research_, 2023.
* Tramer et al. (2024) Florian Tramer, Gautam Kamath, and Nicholas Carlini. Position: Considerations for Differentially Private Learning with Large-Scale Public Pretraining. In _Proceedings of the 41st International Conference on Machine Learning_, pages 48453-48467. PMLR, 2024.
* Vinyals et al. (2016) Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. _Advances in neural information processing systems_, 29, 2016.
* Yousefpour et al. (2021) Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus: User-friendly differential privacy library in PyTorch. _arXiv preprint arXiv:2109.12298_, 2021.
* Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially Private Fine-tuning of Language Models. In _International Conference on Learning Representations_, 2021.
* Zaheer et al. (2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep sets. In _Advances in Neural Information Processing Systems_, volume 30, 2017.
* Zhou and Bassily (2022) Xinyu Zhou and Raef Bassily. Task-level Differentially Private Meta Learning. In _Advances in Neural Information Processing Systems_, volume 35, 2022.

Differential Privacy Details

### Measure-Theoretic Details

Definition 3.1 is the typical definition of \((\epsilon,\delta)\)-DP that is given in the literature, but it glosses over some measure-theoretic details that are usually not important, but are important for the functional mechanism. In particular, the precise meaning of "measurable" is left open. Here, we make the \(\sigma\)-field that "measurable" implicitly refers to explicit:

**Definition A.1**.: _An algorithm \(\mathcal{M}\) is \((\epsilon,\delta,\mathcal{A})\)-DP for a \(\sigma\)-field \(\mathcal{A}\) if, for neighbouring datasets \(D,D^{\prime}\) and all \(A\in\mathcal{A}\),_

\[\Pr(\mathcal{M}(D)\in A)\leq e^{\epsilon}\Pr(\mathcal{M}(D^{\prime})\in A)+\delta.\] (13)

Hall et al. [2013] point out that the choice of \(\mathcal{A}\) is important, and insist that \(\mathcal{A}\) be the finest \(\sigma\)-field on which \(\mathcal{M}(D)\) is defined for all \(D\). When the output of the mechanism is discrete, or an element of \(\mathbb{R}^{n}\), this corresponds with the \(\sigma\)-field that is typically implicitly used in such settings. When the output is a function, as in the functional mechanism, the choice of \(\mathcal{A}\) is not as clear [Hall et al., 2013]. Note that \(\mathcal{A}\) is similarly implicitly present in the definition of GDP (Definition 3.2).

Next, we recall the construction of the appropriate \(\sigma\)-field for the functional mechanism from Hall et al. [2013]. Let \(T\) be an index set. We denote the set of functions from \(T\) to \(\mathbb{R}\) as \(\mathbb{R}^{T}\). For \(S=(x_{1},\ldots,x_{n})\in T^{n}\) and a Borel set \(B\in\mathcal{B}(\mathbb{R}^{n})\),

\[C_{S,B}=\{f\in\mathbb{R}^{T}\mid(f(x_{1}),\ldots,f(x_{n}))\in B\}\] (14)

is called a cylinder set of functions. Let \(\mathcal{C}_{S}=\{C_{S,B}\mid B\in\mathcal{B}(\mathbb{R}^{n})\}\) and

\[\mathcal{F}_{0}=\bigcup_{S:|S|<\infty}\mathcal{C}_{S}.\] (15)

\(\mathcal{F}_{0}\) is called the field of cylinder sets. \((\epsilon,\delta,\mathcal{F}_{0})\)-DP 2 amounts to \((\epsilon,\delta,\mathcal{B}(\mathbb{R}^{n}))\)-DP for any evaluation of \(f\) at a finite vector of points \((x_{1},\ldots,x_{n})\in T^{n}\), of any size \(n\in\mathbb{N}\)[Hall et al., 2013].

Footnote 2: This is a small abuse of notation, as \(\mathcal{F}_{0}\) is not a \(\sigma\)-field.

The \(\sigma\)-field for the functional mechanism is the \(\sigma\)-field \(\mathcal{F}\) generated by \(\mathcal{F}_{0}\)[Hall et al., 2013]. It turns out that \((\epsilon,\delta,\mathcal{F}_{0})\)-DP is sufficient for \((\epsilon,\delta,\mathcal{F})\)-DP.

### General Definition of the Functional Mechanism

**Definition A.2**.: _Let \(g\) be a sample path of a Gaussian process having mean zero and covariance function \(k\). Let \(\{f_{D}:D\in\mathcal{D}\}\subset\mathbb{R}^{T}\) be a family of functions indexed by datasets satisfying the inequality_

\[\sup_{D\sim D^{\prime}}\sup_{n<\infty}\sup_{(x_{1},\ldots,x_{n})\in T^{n}} \left|\left|\Delta_{D,D^{\prime}}^{(x_{1},\ldots,x_{n})}\right|\right|_{2}\leq\Delta,\] (16)

_with_

\[\Delta_{D,D^{\prime}}^{(x_{1},\ldots,x_{n})}=M^{-1/2}(x_{1},\ldots,x_{n}) \begin{bmatrix}f_{D}(x_{1})-f_{D^{\prime}}(x_{1})\\ \vdots\\ f_{D}(x_{n})-f_{D^{\prime}}(x_{n})\end{bmatrix},\]

_where \(M(x_{1},\ldots x_{n})_{ij}=k(x_{i},x_{j})\). The functional mechanism with multiplier \(c\) and sensitivity \(\Delta\) is defined as_

\[\mathcal{M}(D)=f_{D}+cg.\] (17)

If \(f\) is a member of a _reproducible kernel Hilbert space_ (RKHS) \(\mathcal{H}\) with the same kernel \(k\) as the noise process \(g\), the sensitivity bound of Definition A.2 is much simpler:

**Lemma A.3** (Hall et al. 2013).: _For a function \(f\) in an RKHS \(\mathcal{H}\) with kernel \(k\),_

\[\Delta_{\mathcal{H}}f\stackrel{{\text{def}}}{{=}}\sup_{D\sim D^{ \prime}}||f_{D}-f_{D^{\prime}}||_{\mathcal{H}}\leq\Delta.\] (18)

_implies (16)._

### Proof of Theorem 4.1

To prove Theorem 4.1, we need a GDP version of a lemma from Hall et al. (2013):

**Lemma A.4**.: _Suppose that, for a positive definite symmetric matrix \(M\in\mathbb{R}^{d\times d}\), the function \(f\colon\mathcal{D}\to\mathbb{R}^{d}\) satisfies_

\[\sup_{D\sim D^{\prime}}||M^{-1/2}(f(D)-f(D^{\prime}))||_{2}\leq\Delta.\] (19)

_Then the mechanism \(\mathcal{M}\) that outputs (Gaussian mechanism)_

\[\mathcal{M}(D)=f(D)+cZ,\quad Z\sim\mathcal{N}_{d}(0,M)\]

_is \(\mu\)-GDP with \(c=\frac{\Delta}{\mu}\)._

Proof.: We can write

\[\mathcal{M}(D)=M^{1/2}c\left(\frac{M^{-1/2}}{c}f(D)+S\right),\quad S\sim \mathcal{N}_{d}(0,I).\]

Denote \(\mathcal{M}^{\prime}(D)=\frac{M^{-1/2}}{c}f(D)+S\). \(\mathcal{M}^{\prime}\) is a Gaussian mechanism with variance 1. Because of (19), \(\frac{M^{-1/2}}{c}f(D)\) has sensitivity

\[\Delta^{*}=\sup_{D\sim D^{\prime}}\left|\left|\frac{M^{-1/2}}{c}f(D)-\frac{M^ {-1/2}}{c}f(D^{\prime})\right|\right|_{2}\leq\frac{\Delta}{c}\]

so \(\mathcal{M}^{\prime}\) is \(\mu\)-GDP by Theorem 3.5. \(\mathcal{M}\) is obtained by post-processing \(\mathcal{M}^{\prime}\), so it is also \(\mu\)-GDP. 

**Theorem 4.1**.: _The functional mechanism with sensitivity \(\Delta\) and multiplier \(c=\nicefrac{{\Delta}}{{\mu}}\) is \(\mu\)-GDP._

Proof.: Let \(T\) be the index set of the Gaussian process \(G\), and let \(S=(x_{1},\ldots,x_{n})\in T^{n}\). Then \((G(x_{1}),\ldots,G(x_{n}))\) has a multivariate Gaussian distribution with mean zero and covariance \(\mathrm{Cov}(G(x_{i}),G(x_{j}))=K(x_{i},x_{j})\). Then the vector obtained by evaluating \(\mathcal{M}(D)\) at \((x_{1},\ldots,x_{n})\) is \(\mu\)-GDP by Lemma A.4, as (16) implies the sensitivity bound (19). Theorem 3.5 gives a curve of \((\epsilon,\delta(\epsilon))\)-bounds for all \(\epsilon\geq 0\) from \(\mu\).

This holds for any \(S\in T^{n}\) and any \(n\in\mathbb{N}\), so \(\mathcal{M}\) is \((\epsilon,\delta(\epsilon),\mathcal{F}_{0})\)-DP for all \(\epsilon\geq 0\), which immediately implies \((\epsilon,\delta(\epsilon),\mathcal{F})\)-DP. This curve can be converted back to \(\mu\)-GDP (with regards to \(\mathcal{F}\)) using Theorem 3.5. 

### Functional Mechanism Sensitivities for DPConvCNP

To bound the sensitivity of \(r^{(d)}\) and \(r^{(s)}\) for the functional mechanism, we look at two neighbouring context sets \(D_{1}^{(c)}=((x_{n,1}^{(c)},y_{n,1}^{(c)}))_{n=1}^{N}\) and \(D_{2}^{(c)}=((x_{n,2}^{(c)},y_{n,2}^{(c)}))_{n=1}^{N}\) that differ only in the points \((x_{1},y_{1})\in D_{1}^{(c)}\) and \((x_{2},y_{2})\in D_{2}^{(c)}\). Let \(r_{D_{i}^{(c)}}^{(d)}\) for \(i\in\{1,2\}\) denote \(r^{(d)}\) from (10) computed from \(D_{i}^{(c)}\), and define \(r_{D_{i}^{(c)}}^{(s)}\) similarly.

Denote the RKHS of the kernel \(k\) by \(\mathcal{H}\). The distance in \(\mathcal{H}\) between the functions \(k_{x_{1}}=k(x_{1},\cdot)\) and \(k_{x_{2}}=k(x_{2},\cdot)\) is given by

\[||k_{x_{1}}-k_{x_{2}}||_{\mathcal{H}}^{2} =\langle k_{x_{1}}-k_{x_{2}},k_{x_{1}}-k_{x_{2}}\rangle_{\mathcal{H}}\] (20) \[=k(x_{1},x_{1})-2k(x_{1},x_{2})+k(x_{2},x_{2})\] (21) \[\leq 2C_{k}.\] (22)

For the RBF kernel, this is a tight bound without other assumptions on \(x\), as \(k(x,x)=1=C_{k}\) for all \(x\) and \(k(x_{1},x_{2})\) can be made arbitrarily small by placing \(x_{1}\) and \(x_{2}\) far away from each other.

The sensitivity of \(r^{(d)}\) for the functional mechanism can be bounded with (22): for,

\[\Delta^{2}_{\mathcal{H}}r^{(d)} =\sup_{D^{(c)}_{1}\sim_{S}D^{(c)}_{2}}\left|\left|r^{(d)}_{D^{(c)}_ {1}}-r^{(d)}_{D^{(c)}_{2}}\right|\right|^{2}_{\mathcal{H}}\] (23) \[=\sup_{D^{(c)}_{1}\sim_{S}D^{(c)}_{2}}\left|\left|\sum_{n=1}^{N}(k _{x_{n,1}^{(c)}}-k_{x^{(c)}_{n,2}})\right|\right|^{2}_{\mathcal{H}}\] (24) \[=\sup_{x_{1},x_{2}}\left|\left|k_{x_{1}}-k_{x_{2}}\right|\right| ^{2}_{\mathcal{H}}\] (25) \[\leq 2C_{k},\] (26)

where the second to last line follows from the fact that \(D^{(c)}_{1}\) and \(D^{(c)}_{2}\) only differ in one datapoint. This is a tight bound for the RBF kernel, because when \(x=x_{1}\), \(k_{x_{1}}(x)=1\) and \(k_{x_{2}}(x)=1\) can be a made arbitrarily small by moving \(x_{2}\) far away from \(x\).

For \(r^{(s)}\) and any function \(\phi\) with \(|\phi(y)|\leq C\), we first bound

\[||\phi(y_{1})k_{x_{1}}-\phi(y_{2})k_{x_{2}}||^{2}_{\mathcal{H}}\] (27) \[=\phi(y_{1})^{2}k(x_{1},x_{1})-2\phi(y_{1})\phi(y_{2})k(x_{1},x_{ 2})+\phi(y_{2})^{2}k(x_{2},x_{2})\] (28) \[\leq 4C^{2}C_{k}.\] (29)

Again, these are tight bounds for the RBF kernel if we don't constrain \(x\) or \(y\) further.

The \(\mathcal{H}\)-sensitivity for \(r^{(s)}\) is then derived in the same way as the sensitivity for \(r^{(d)}\), giving

\[\Delta^{2}_{\mathcal{H}}r^{(s)}\leq 4C^{2}C_{k}.\] (30)

### Gaussian Mechanism for DPConvCNP

A naive way of releasing \(r(x)\) under DP is to first select discretisation points \(x_{1},\ldots,x_{n}\), in some way, and release \(r(x_{1}),\ldots,r(x_{n})\) with the Gaussian mechanism. The components of \(r\), \(r^{(s)}\) and \(r^{(d)}\), have the following sensitivities:

\[\Delta^{2}r^{(d)}(x) =\sup_{D^{(c)}_{1}\sim_{S}D^{(c)}_{2}}\left|\left|r^{(d)}_{D^{(c) }_{1}}(x)-r^{(d)}_{D^{(c)}_{2}}(x)\right|\right|^{2}_{2}\] (31) \[=\sup_{D^{(c)}_{1}\sim_{S}D^{(c)}_{2}}\left|\sum_{n=1}^{N}(k_{x^{ (c)}_{n,1}}(x)-k_{x^{(c)}_{n,2}}(x))\right|^{2}\] (32) \[=\sup_{x_{1},x_{2}}\left|k_{x_{1}}(x)-k_{x_{2}}(x)\right|^{2}\] (33) \[\leq C^{2}_{k}.\] (34)

Line (33) follows from the fact that \(D^{(c)}_{1}\) and \(D^{(c)}_{2}\) only differ in one datapoint.

For \(r^{(s)}(x)\), we have

\[|\phi(y_{1})k_{x_{1}}(x)-\phi(y_{2})k_{x_{2}}(x)|^{2}\leq 4C^{2}C^{2}_{k}.\] (35)

Then we get

\[\Delta^{2}r^{(s)}(x)\leq 4C^{2}C^{2}_{k}\] (36)

following the derivation of \(\Delta^{2}r^{(d)}(x)\). For the RBF kernel, this is again tight without additional assumptions on \(y\) or \(x\).

These sensitivities give the following privacy bound:

**Theorem A.5**.: _Let \(\Delta^{2}_{s}=4C^{2}C^{2}_{k}\) and \(\Delta^{2}_{d}=C^{2}_{k}\). Releasing \(n\) evaluations of \(r(x)=(r^{(d)}(x),r^{(s)}(x))\) with the Gaussian mechanism with noise variance \(\sigma^{2}\) is \(\mu\)-GDP for_

\[\mu=\sqrt{n\frac{\Delta^{2}_{s}+\Delta^{2}_{d}}{\sigma^{2}}}.\] (37)Proof.: Releasing \(n\) evaluations of \(r(x)\) is simply an \(n\)-fold composition of Gaussian mechanisms that release \(r(x)\) for one value. Releasing \(r(x)\) for one value is a composition of releasing \(r^{(d)}(x)\) and \(r^{(s)}(x)\), which have the sensitivities \(\Delta_{s}\) and \(\Delta_{d}\). Now Theorems 3.5 and 3.4 prove the claim. 

As \(\mu\) scales with \(n\), this method must add a large amount of noise for even moderate numbers of discretisation points.

The difference between having a factor of \(C_{k}^{2}\) in the \(L_{2}\)-sensitivities and \(C_{k}\) in the \(\mathcal{H}\)-sensitivities is explained by the fact that the kernel also directly affects the noise variance for the functional mechanism, but it does not directly affect the noise variance with the Gaussian mechanism. This can be illustrated by considering what happens when the kernel is multiplied by a constant \(u>0\). This multiplies \(C_{k}\) by \(u\), and multiplies the \(L_{2}\)-sensitivities by \(u^{2}\). For the Gaussian mechanism, this means multiplying the noise standard deviation by \(u\), but simultaneously multiplying all released values by \(u\), which does not change the signal-to-noise ratio. For the functional mechanism, multiplying the kernel values effectively multiplies \(c\) by \(\sqrt{u}\) and the squared sensitivities by \(u\), which then cancel each other in \(\mu\).

For the RBF kernel and clipping function \(\phi\) with threshold \(C=1\), we see that \(\Delta_{\mathcal{H}}^{2}r^{(d)}=2\) while \(\Delta_{2}^{2}r^{(d)}=1\), and \(\Delta_{\mathcal{H}}^{2}r^{(s)}=4\), while \(2\Delta_{2}^{2}r^{(s)}=4\), so the functional mechanism adds noise with slightly more variance as releasing a single value with the Gaussian mechanism, so the functional mechanism adds noise of less variance when 2 or more discretisation points are required. However, the functional mechanism adds correlated noise, which is not as useful for denoising as the uncorrelated noise that the Gaussian mechanism adds.

### Details on Figure 4

In this section, we go over the details of the calculations behind Figure 4. The "classical" line of the figure is computed from Theorem 3.7. The GDP line uses Definition 3.2 to convert the \((\epsilon,\delta)\)-pair into a GDP \(\mu\) bound by numerically solving for \(\mu\) in Eq.(5). \(\sigma\) is then found with Theorem 4.1.

For the RDP line, we get an RDP guarantee from Corollary 2 of Jiang et al. (2023), which we convert to \((\epsilon,\delta)\) with Proposition 3 of Jiang et al. (2023). These give the equation

\[\epsilon=\frac{\alpha\Delta^{2}}{2\sigma^{2}}-\frac{\ln\delta}{\alpha-1},\] (38)

where \(\alpha>1\) is a parameter of RDP that can be freely chosen. The \(\alpha\) value that minimises \(\epsilon\) is

\[\alpha^{*}=\sqrt{-\frac{2\sigma^{2}\ln\delta}{\Delta^{2}}}+1.\] (39)

Plugging \(\alpha^{*}\) into Eq. (38) gives the quadratic equation

\[-\epsilon\sigma^{2}+2\sqrt{-\frac{\Delta^{2}\ln\delta}{2}}\sigma+\frac{ \Delta^{2}}{2}=0\] (40)

that can be solved for \(\sigma\).

To see that choosing the \(\alpha\) that minimises \(\epsilon\) also leads to the smallest \(\sigma\) that satisfies a given \((\epsilon,\delta)\)-bound, let

\[\epsilon(\alpha,\sigma)=\frac{\alpha\Delta^{2}}{2\sigma^{2}}-\frac{\ln\delta} {\alpha-1},\] (41)

and let \(\sigma^{*}\) be the solution to Eq. (40). Let \(\alpha,\sigma\) be another pair that satisfies the \((\epsilon,\delta)\)-bound. Since \(\alpha^{*}\) is chosen to minimise \(\epsilon\), \(\epsilon(\alpha^{*}(\sigma),\sigma)\leq\epsilon(\alpha,\sigma)\). We can assume that \(\epsilon(\alpha,\sigma)=\epsilon(\alpha^{*}(\sigma),\sigma)=\epsilon\), since otherwise we could reduce \(\sigma\) further. Now

\[\epsilon(\alpha^{*}(\sigma^{*}),\sigma^{*})=\epsilon=\epsilon(\alpha^{*}( \sigma),\sigma)\] (42)

so \(\epsilon(\alpha^{*}(\sigma^{*}),\sigma^{*})=\epsilon(\alpha^{*}(\sigma),\sigma)\). By manipulating Eq. (40), we can see that \(\epsilon(\alpha^{*}(\cdot),\cdot)\) is strictly decreasing, so this implies that \(\sigma=\sigma^{*}\).

``` Input : Dimension-wise grid location \(u_{d}\in\mathbb{R}\), spacing \(\gamma_{d}\in\mathbb{R}\) and number of points \(N_{d}\in\mathbb{N}\),  Product kernel \(k:\mathbb{R}^{D}\times\mathbb{R}^{D}\to\mathbb{R}\) with factors \(k_{d}:\mathbb{R}\times\mathbb{R}\to\mathbb{R}\). Output : Sample \(f_{n_{1}\dots n_{D}}\) from GP with kernel \(k\) on grid inputs \(x_{n_{1}\dots n_{D}}\) defined by \(u_{d},\gamma_{d},N_{d}\).  Sample \(f_{n_{1}\dots n_{D}}\sim\mathcal{N}(0,1)\) for each \(1\leq n_{d}\leq N_{d}\), \(d=1,\dots,D\) {Sample Gaussian noise} for\(d=1\)to\(D\)do \(K_{dnm}\gets k_{d}(u_{d}+n\gamma_{d},u_{d}+m\gamma_{d})\) for \(0\leq n,m\leq N_{d}-1\) {Compute covariance} \(L_{d}\leftarrow\textsc{Cholesky}(K_{d})\) {Compute Cholesky factor} \(f\leftarrow\textsc{MatmulAlongDim}(L_{d},f,d)\) {Matmul \(f\) by \(L_{d}\) along dimension \(d\)} endfor ```

**Algorithm 4** Efficient sampling of GP noise on a \(D\)-dimensional grid.

## Appendix B Efficient sampling of Gaussian process noise

In order to ensure differential privacy within the DPConvCNP, we need to add GP noise (from a GP with an EQ kernel) to the functional representation outputted by the SetConv. In practice, this is implemented by adding GP noise on the discretised representation, i.e. the data and density channels.

While sampling GP noise is typically tractable if the grid is one-dimensional, the computational and memory costs of sampling can easily become intractable for two- or three-dimensional grids. This is because the number of grid points increases exponentially with the number of input dimensions and, in addition to this, the cost of sampling increases cubically with the number of grid points. Fortunately, we can exploit the regularity of the grid and the fact that the EQ kernel is a product kernel, to make sampling tractable. Proposition B.1 illustrates how this can be achieved.

**Proposition B.1**.: _Let \(x\in\mathbb{R}^{N_{1}\times\dots\times N_{D}}\) be a grid of points in \(\mathbb{R}^{D}\) given by_

\[x_{n_{1}\dots\;n_{D}}=\left(u_{1}+(n_{1}-1)\gamma_{1},\dots,u_{D}+(n_{D}-1) \gamma_{D}\right),\] (43)

_where \(u_{d}\in\mathbb{R}\), \(\gamma_{d}\in\mathbb{R}^{+}\) and \(1\leq n_{d}\in\mathbb{N}\leq N_{d}\) for each \(d=1,\dots,D\). Also let \(k:\mathbb{R}^{D}\times\mathbb{R}^{D}\to\mathbb{R}\) be a product kernel, i.e. a kernel that satisfies_

\[k(z,z^{\prime})=\prod_{d=1}^{D}k_{d}(z_{d},z^{\prime}_{d}),\] (44)

_for some univariate kernels \(k_{d}:\mathbb{R}\to\mathbb{R}\), for every \(z,z^{\prime}\in\mathbb{R}^{D}\), let_

\[K_{dnm}=k_{d}(u_{d}+(n-1)\gamma_{d},\;u_{d}+(m-1)\gamma_{d}),\] (45)

_and let \(L_{d}\) be a Cholesky factor of the matrix \(K_{d}\). Then if \(\epsilon_{n_{1}\dots\;n_{D}}\in\mathbb{R}\sim\mathcal{N}(0,1)\) is a grid of corresponding i.i.d. standard Gaussian noise, the scalars \(f_{n_{1}\dots n_{D}}\in\mathbb{R}\), defined as_

\[f_{n_{1}\dots\;n_{D}}=\sum_{k_{1}=1}^{N_{1}}L_{1n_{1}k_{1}}\dots\sum_{k_{D}=1} ^{N_{D}}L_{Dn_{D}k_{D}}\;\epsilon_{k_{1}\dots\;k_{D}},\] (46)

_are Gaussian-distributed, with zero mean and covariance_

\[\mathbb{C}[f_{n_{1}\dots\;n_{D}},f_{m_{1}\dots\;m_{D}}]=k(x_{n_{1}\dots\;n_{D} },x_{m_{1}\dots\;m_{D}}).\] (47)

Before giving the proof of Proposition B.1, we provide pseudocode for this approach in Algorithm 4 and discuss the computation and memory costs of this implementation compared to a naive approach. Naive sampling on a grid of \(N_{1}\times\dots\times N_{D}\) points requires computing a Cholesky factor for the covariance matrix of the entire grid and then multiplying standard Gaussian noisy by this factor. We discuss the costs of these operations, comparing them to the efficient approach.

``` Input : Dimension-wise grid location \(u_{d}\in\mathbb{R}\), spacing \(\gamma_{d}\in\mathbb{R}\) and number of points \(N_{d}\in\mathbb{N}\),  Product kernel \(k:\mathbb{R}^{D}\times\mathbb{R}^{D}\to\mathbb{R}\) with factors \(k_{d}:\mathbb{R}\times\mathbb{R}\to\mathbb{R}\). Output : Sample \(f_{n_{1}\dots n_{D}}\sim\mathcal{N}(0,1)\) for each \(1\leq n_{d}\leq N_{d}\), \(d=1,\dots,D\) {Sample Gaussian noise} for\(d=1\)to\(D\)do \(K_{dnm}\gets k_{d}(u_{d}+n\gamma_{d},u_{d}+m\gamma_{d})\) for \(0\leq n,m\leq N_{d}-1\) {Compute covariance} \(L_{d}\leftarrow\textsc{Cholesky}(K_{d})\) {Compute Cholesky factor} \(f\leftarrow\textsc{MatmulAlongDim}(L_{d},f,d)\) {Matmul \(f\) by \(L_{d}\) along dimension \(d\)} endfor ```

**Algorithm 4** Efficient sampling of GP noise on a \(D\)-dimensional grid.

**Computing Cholesky factors.** The cost of computing a Cholesky factor for covariance matrix of the entire \(N_{1}\times\dots\times N_{D}\) grid incurs a computation cost of \(\mathcal{O}(N_{1}^{3}\times\dots\times N_{D}^{3})\) and a memory cost of \(\mathcal{O}(N_{1}^{2}\times\dots\times N_{D}^{2})\). By contrast, Algorithm 4 only ever computes Cholesky factors for \(N_{d}\times N_{d}\) covariance matrices, so it incurs a computational cost of \(\mathcal{O}\big{(}\sum_{d=1}^{D}N_{d}^{3}\big{)}\) and a memory cost of \(\mathcal{O}\big{(}\sum_{d=1}^{D}N_{d}^{2}\big{)}\), which are both much lower than those of a naive implementation. For clarity, if \(N_{1}=\cdots=N_{D}=N\), naive factorisation has \(\mathcal{O}(N^{3D})\) computational and \(\mathcal{O}(N^{2D})\) memory cost, whereas the efficient implementation has \(\mathcal{O}(DN^{3})\) computational and \(\mathcal{O}(DN^{2})\) memory cost.

**Matrix multiplications.** In addition, naively multiplying the Gaussian noise by the Cholesky factor of the entire covariance matrix incurs a \(\mathcal{O}(N_{1}^{2}\times\cdots\times N_{D}^{2})\) computation cost. On the other hand, in Algorithm 4 we perform \(D\) batched matrix-vector multiplications, where the \(d^{\text{th}}\) multiplication consists of \(\prod_{d^{\prime}=d}N_{d^{\prime}}\) matrix-vector multiplications, where a vector with \(N_{d}\) entries is multiplied by an \(N_{d}\times N_{d}\) matrix. The total computation cost for this step is only \(\mathcal{O}\left(\sum_{d=1}^{D}N_{d}^{2}\prod_{d^{\prime}\neq d}N_{d^{\prime} }\right)\). For example, if \(N_{1}=\cdots=N_{D}=N\), naive matrix-vector multiplication has a computation cost of \(\mathcal{O}(N^{2D})\), whereas the efficient implementation has a computation cost of \(\mathcal{O}(N^{D+1})\).

In Algorithm 4, Cholesky denotes a function that computes the Cholesky factor of a square positive-definite matrix. MatvecAlongDim\((L_{d},f,d)\) denotes the batched matrix-vector multiplication of an array \(f\) by a matrix \(L_{d}\) along dimension \(d\), batching over the dimensions \(d^{\prime}\neq d\). Specifically, given a \(D\)-dimensional array \(b\) with dimension sizes \(N_{1},\ldots,N_{D}\) and an \(N_{d}\times N_{d}\) matrix \(A\), the matrix-vector multiplication of \(b\) by \(A\) along dimension \(d\) outputs the \(D\)-dimensional array

\[\text{MatvecAlongDim}(A,b,d)_{n_{1}\ldots n_{D}}=\sum_{j=1}^{N_{d}}A_{n_{d}j}b _{n_{1}\ldots n_{d-1}\ j\ n_{d+1}\ldots n_{D}}.\] (48)

From the above equation, we can see that initialising \(f\) with standard Gaussian noise, and batch-multiplying \(f\) by \(L_{d}\) along dimension \(d\) for each \(d=1,\ldots,D\), amounts to computing the nested sum in Equation (46). Note that the order with which these batch multiplications are performed does not matter: it does not change neither the numerical result nor the computation or memory cost of the algorithm.

Proof of Proposition b.1.: From the definition above, we see that \(f_{n_{1}\ldots\ n_{D}}\) is a linear transformation of Gaussian random variables with zero mean, and therefore also has zero mean. For the covariance, again from the definition above, we have

\[\mathbb{C}\left[f_{n_{1}\ldots\ n_{D}},f_{m_{1}\ldots\ m_{D}} \right]=\] (49) \[\mathbb{C}\left[\sum_{k_{1}=1}^{N_{1}}L_{1n_{1}k_{1}}\cdots\sum_ {k_{D}=1}^{N_{D}}L_{Dn_{D}k_{D}}\ \epsilon_{k_{1}\ldots\ k_{D}},\sum_{l_{1}=1}^{N_{1}}L_{1m_{1}l_{1}}\cdots\sum_ {l_{D}=1}^{N_{D}}L_{Dm_{D}l_{D}}\ \epsilon_{l_{1}\ldots\ l_{D}}\right]=\] (50) \[\mathbb{C}\left[\sum_{k_{1}=1}^{N_{1}}\cdots\sum_{k_{D}=1}^{N_{D} }L_{1n_{1}k_{1}}\ldots L_{Dn_{D}k_{D}}\ \epsilon_{k_{1}\ldots\ k_{D}},\sum_{l_{1}=1}^{N_{1}}\cdots\sum_{l_{D}=1}^{N_{D} }L_{1m_{1}l_{1}}\ldots L_{Dm_{D}l_{D}}\ \epsilon_{l_{1}\ldots\ l_{D}}\right]=\] (51) \[\mathbb{E}\left[\left(\sum_{k_{1}=1}^{N_{1}}\cdots\sum_{k_{D}=1}^ {N_{D}}L_{1n_{1}k_{1}}\ldots L_{Dn_{D}k_{D}}\ \epsilon_{k_{1}\ldots\ k_{D}}\right)\left(\sum_{l_{1}=1}^{N_{1}}\cdots\sum_{l_ {D}=1}^{N_{D}}L_{1m_{1}l_{1}}\ldots L_{Dm_{D}l_{D}}\ \epsilon_{l_{1}\ldots\ l_{D}}\right) \right],\] (52)

where we have used the fact that the expectation of \(f\) is zero. Now, expanding the product of sums above, taking the expectation and using the fact that \(\epsilon_{n_{1}\ldots\ n_{D}}\) are i.i.d., we see that all terms vanish, except those where \(k_{d}=l_{d}\) for all \(d=1,\ldots,D\). Specifically, we have

\[\mathbb{C}\left[f_{n_{1}\ldots\ n_{D}},f_{m_{1}\ldots\ m_{D}} \right]=\] (53) \[=\mathbb{E}\left[\sum_{k_{1}=1}^{N_{1}}\cdots\sum_{k_{D}=1}^{N_{ D}}\sum_{l_{1}=1}^{N_{1}}\cdots\sum_{l_{D}=1}^{N_{D}}L_{1n_{1}k_{1}}\ldots L_{Dn_{D}k_{D}}L_{1m_{1}l_{1}}\ldots L_{Dm_{D}l_{D}} \ \epsilon_{k_{1}\ldots\ k_{D}}\epsilon_{l_{1}\ldots\ l_{D}}\right]\] (54) \[=\sum_{k_{1}=1}^{N_{1}}\cdots\sum_{k_{D}=1}^{N_{D}}\sum_{l_{1}=1}^ {N_{1}}\cdots\sum_{l_{D}=1}^{N_{D}}L_{1n_{1}k_{1}}\ldots L_{Dn_{D}k_{D}}L_{1m_{1 }l_{1}}\ldots L_{Dm_{D}l_{D}}\ \mathbb{E}\left[\epsilon_{k_{1}\ldots\ k_{D}}\epsilon_{l_{1}\ldots\ l_{D}}\right]\] (55) \[=\sum_{k_{1}=1}^{N_{1}}\cdots\sum_{k_{D}=1}^{N_{D}}\sum_{l_{1}=1}^ {N_{1}}\cdots\sum_{l_{D}=1}^{N_{D}}L_{1n_{1}k_{1}}\ldots L_{Dn_{D}k_{D}}L_{1m_{1 }l_{1}}\ldots L_{Dm_{D}l_{D}}\ \ 1_{k_{1}=\ l_{1},\ldots,\ k_{D}=\ l_{D}}\] (56) \[=\sum_{k_{1}=1}^{N_{1}}\cdots\sum_{k_{D}=1}^{N_{D}}L_{1n_{1}k_{1}} \ldots L_{Dn_{D}k_{D}}L_{1m_{1}k_{1}}\ldots L_{Dm_{D}k_{D}}\] (57)\[=\sum_{k_{1}=1}^{N_{1}}L_{1n_{1}k_{1}}L_{1m_{1}k_{1}}\cdots\sum_{k_{D} =1}^{N_{D}}L_{Dn_{D}k_{D}}L_{Dm_{D}k_{D}}\] (58) \[=\prod_{d=1}^{D}K_{dn_{d}m_{d}}\] (59) \[=k(x_{n_{1}\dots\ n_{D}},x_{m_{1}\dots\ m_{D}}).\] (60)

which is the required result. 

## Appendix C Additional results

### How effectively does the ConvCNP learn to undo the DP noise?

**Quantifying performance gaps.** In this section we provide some additional results on the performance of the DPConvCNP and the functional mechanism. Specifically, we investigate the performance gap between the DPConvCNP and the oracle (non-DP) Bayes predictor. Assuming the data generating prior is known, which in our synthetic experiments it is, the corresponding Bayes posterior predictive attains the best possible average log-likelihood achievable. We determine and quantify the sources of this gap in a controlled setting.

**Sources of the performance gaps.** Specifically, the performance gap can be broken down into two main parts: one part due to the DP mechanism (specifically the signal channel clipping and noise, and the density channel noise) and another part due to the fact that we are training a neural network to map the DP representation to an estimate of the Bayes posterior. To assess the performance gap introduced by each of these sources, we perform a controlled experiment with synthetic data from a Gaussian process prior (see Figure S1).

**Gap quantification experiment.** We fix the clipping threshold value at \(C=2.00,\) which is a sensible setting since the marginal confidence intervals of the data generating process are \(\pm 1.96.\) We also fix the noise weighting at \(t=0.50,\) which is again is a sensible setting since it places roughly equal importance to the noise added to the density and the signal channels. We consider three different settings for the prior lenghtscale \((\ell=0.25,0.71,2.00)\) and two settings for the DP parameters (\(\epsilon=1.00,3.00\) and fixed \(\delta=10^{-3}\)). For each of the six combinations of settings, we train three different DPConvCNPs, one with just signal noise (red; no clip, no density noise), one with signal noise and clipping (orange; clip, no density noise) and one with signal noise and clipping and also density noise (green; clip, density noise). Note, only the last model has DP guarantees. We compare performance with the non-DP Bayesian posterior oracle (blue).

**Lower bound model.** When we only add signal noise to the ConvCNP representation (and do not apply clipping or add density noise), and the true generative process is a GP such as in this case, the predictive posterior (given the noisy signal representation and the noiseless density representation) is a GP. That is because the data come from a known GP, and the signal channel is a linear combination of the data (since we have turned off clipping) plus GP noise, so it is also a GP. Therefore, we can write down a closed form predictive posterior in this case. We refer to this as the _lower bound model_ (black) in Figure S1, because for a given \(C\) and \(t,\) the performance of this model is a lower bound to the NLL of any model that uses this representation as input. Note however that different lower bounds can be obtained for different \(C\) and \(t.\)

**Conclusions.** We observe that the DPConvCNP with no clipping and no density noise (red) matches the performance of the lower bound model. This is encouraging as it suggests that the model is able to undo the effect of the signal noise perfectly. We also observe that applying clipping (orange) does not reduce performance substantially, which is also encouraging as it suggests that the model is able to cope with the effect of clipping on the signal channel, when it is trained to do so. Lastly, we observe that there is an additional gap in performance is introduced due to noise in the density channel (green). This is expected since the density noise is substantial and confounds the context inputs. This gap reduces as the number of context points increases, which is again expected. From the above, we conclude that in practice, the model is able to make predictions under DP constraints that are near optimal, i.e. there is likely not a significant gap due to approximating the mapping from the DP representation to the optimal predictive map, with a neural network.

### Supplementary model fits for the synthetic tasks

We also provide supplementary model fits for the synthetic, Gaussian and non-Gaussian tasks. For each task, we provide fits for three parameter settings (\(\ell\) and \(\tau\)), two privacy budgets, four context sizes and two dataset random seeds. See Figures S2 to S5, at the end of this document, for model fits.

## Appendix D Differentially-Private Sparse Gaussian Process Baseline

Here, we provide details of the differentially-private sparse variational Gaussian process (DP-SVGP) baseline.

Let \(D=(\mathbf{x},\mathbf{y})\) denote a dataset consisting of inputs \(N\) inputs \(\mathbf{x}\in\mathcal{X}^{N}\) and \(N\) corresponding outputs \(\mathbf{y}\in\mathcal{Y}^{N}\). We assume the observations are generated according to the probabilistic model:

\[\begin{split} f\sim\mathcal{GP}(0,k_{\theta_{1}}(x,x^{\prime})) \\ \mathbf{y}|f,\mathbf{x}\sim\prod_{n=1}^{N}p_{\theta_{2}}(y_{n}|f( x_{n})),\end{split}\] (61)

where \(k_{\theta}\) denotes the GP kernel from which the latent function \(f\) is sampled from, with hyperparameters \(\theta\), and \(\theta_{2}\) denotes the parameters of the likelihood function. Computing the posterior distribution \(p_{\theta}(f|\mathbf{x},\mathbf{y})\) is only feasible when the likelihood is Gaussian. Even when this is true, the computational complexity associated with this computation is \(\mathcal{O}(N^{3}).\)

Sparse variational GPs (Titsias, 2009) offer a solution to this by approximating the true posterior with the GP

\[q_{\theta,\phi}(f)=p_{\theta}(f_{\neq\mathbf{u}}|\mathbf{u})q_{\phi}(\mathbf{u})\] (62)

with \(\mathbf{u}=f(\mathbf{z})\), where \(\mathbf{z}\in\mathcal{X}^{M}\) denote a set of \(M\) inducing locations, and \(q_{\phi}(\mathbf{u})=\mathcal{N}(\mathbf{u};\mathbf{m},\mathbf{S})\). The computational complexity associated with this posterior approximation is \(\mathcal{O}(NM^{2})\), which is significantly lower if \(M\ll N\). We can optimise the variational parameters \(\phi=\{\mathbf{m},\mathbf{S},\mathbf{z}\}\) by optimisation of the evidence lower bound, \(\mathcal{L}_{\text{ELBO}}\):

\[\mathcal{L}_{\text{ELBO}}(\theta,\phi)=\mathbb{E}_{q_{\theta}(f)}\left[\log p_{ \theta}(\mathbf{y}|f(\mathbf{x}))\right]-\text{KL}\left[q_{\phi}(\mathbf{u}) \|p_{\theta}(\mathbf{u})\right].\] (63)

Importantly, \(\mathcal{L}_{\text{ELBO}}\) also serves as a lower bound to the marginal likelihood \(p_{\theta}(\mathbf{y}|\mathbf{x})\), and so we can optimise this objective with respect to both \(\theta\) and \(\phi\). Since the likelihood factorises, we can obtain an unbiased estimate to the \(\mathcal{L}_{\text{ELBO}}\) by sampling batches of datapoints. This lends itself to stochastic optimisation using gradient based methods, such as SGD. By replacing SGD with a differentially-private gradient-based optimisation routine (DP-SGD), we obtain our DP-SVGP baseline.

A difficulty in performing DP-SGD to optimise model and variational parameters of the DP-SVGP baseline is that the test-time performance is a complex function of the hyperparameters of DP-SGD (i.e. maximum gradient norm, batch size, epochs, learning rate), the initial hyperparameters of the model (i.e. kernel hyperparameters, and likelihood parameters), and the initial variational parameters (i.e. number of inducing locations \(M\)). Fortunately, we are considering the meta-learning setting, in which we have available to us a number of datasets that we can use to tune these hyperparameters. We do so using Bayesian optimisation (BO) to maximise the sum of the \(\mathcal{L}_{\text{ELBO}}\)'s for a number of datasets. To limit the number of parameters we optimise using BO, we set the initial variational mean and variational covariance to the prior mean and covariance, \(\mathbf{m}=\mathbf{0}\) and \(\mathbf{S}=k(\mathbf{z},\mathbf{z})\).

In Table S1, we provide the range for each hyperparameter that we optimise over. In all cases, we fix the number of datasets that we compute the \(\mathcal{L}_{\text{ELBO}}\) for to 64 and the number of BO iterations to 200. We use Optuna [Akiba et al., 2019] to perform the BO, and Opacus [Youssefpour et al., 2021] to perform DP-SGD using the PRV privacy accountant.

## Appendix E Experiment details

In this section we give full details for our experiments. Specifically, we describe the generative processes we used for the Gaussian, non-Gaussian and sim-to-real tasks.

### Synthetic tasks

First, we specify the general setup that is shared between the Gaussian and non-Gaussian tasks. Second, we specify the Gaussian and non-Gaussian generative processes used to generate the outputs. Lastly we give details on the parameter settings for the amortised and the non-amortised models.

**General setup.** During training, we generate data by sampling the context set size \(N\sim\mathcal{U}[1,512]\), then sample \(N\) context inputs uniformly in \([-2.00,2.00]\) and \(512\) target inputs in the range \([-6.00,6.00]\). We then sample the corresponding outputs using either the EQ Gaussian process or the sawtooth process, which we define below. For the DPConvCNP we use 6,553,600 such tasks with a batch size of 16 at training time, which is equivalent to 409,600 gradient update steps. We do note however that this large number of tasks, which was used to ensure convergence across all variants of the models we trained, is likely unnecessary and significantly fewer tasks (fewer than half of what we used) suffices. Throughout optimisation, we maintain a fixed set of 2,048 tasks generatedin the same way, as a validation set. Every 32,768 gradient updates (i.e. 200 times throughout the training process) we evaluate the model on these held out tasks, maintaining a checkpoint of the best model encountered thus far. After training, this best model is the one we use for evaluation. At evaluation time, we fix \(N\) to each of the numbers specified in Figure 6, and sample \(N\) context inputs uniformly in \([-2.00,2.00]\) and \(512\) target inputs in the range \([-2.00,2.00]\). We repeat this procedure for 512 separate tasks, and report the mean NLL together with its 95% confidence intervals in Figure 6. For all tasks, we set the privacy budget with \(\delta=10^{-3}\) and \(\epsilon\sim\mathcal{U}[0.90,4.00]\).

**Gaussian generative process.** For the Gaussian task, we generate the context and target outputs from a GP with the exponentiated quadratic (EQ) covariance, which is defined as

\[k(x,x^{\prime})=\sigma_{v}^{2}\exp\left(-\frac{(x-x^{\prime})^{2}}{2\ell^{2}} \right)+\sigma_{n}^{2}.\]

**Sawtooth generative process.** For the non-Gaussian task, we generate the context and target outputs from a the truncation of the Fourier series of the sawtooth waveform

\[f(x)=\frac{2}{\pi}\sum_{m=1}^{2}\frac{\sin(2m\pi(dx/\tau)+\phi)}{m}\]

where \(d\sim\mathcal{U}[-1,1]\) is a direction sampled uniformly from \(\{-1,1\}\), \(\tau\) is a period and \(\phi\in[0,2\pi]\) is a phase shift. In preliminary experiments, we found that the DPConvCNP worked well with the raw sawtooth signal (i.e. the full Fourier series) but the DP-SVGP struggled due to the discontinuities of the original signal, so we truncated the series, giving an advantage to the DP-SVGP.

**Non-amortised and amortised tasks.** For the non-amortised tasks, we train and evaluate a single model for a single setting of the generative parameter \(\ell\) or \(\tau\). Specifically, for the Gaussian tasks, we fix \(\ell=0.50,0.71\) or \(2.00\) and train a separate model for each one, that is then tested on data with the same value of \(\ell\). Similarly, for the non-Gaussian tasks, we fix \(\tau^{-1}=0.25,0.50\) or \(1.00\) and train a separate model for each one, that is again then tested on data with the same value of \(\tau\). For the amortised tasks, we sample the generative parameter \(\ell\) or \(\tau\) at random. Specifically, for the Gaussian tasks, we sample \(\ell\sim\mathcal{U}[0.20,2.50]\) for each task that we generate, and train a _single model_ on these data. We then evaluate this model for each of the settings \(\ell=0.50,0.71\) or \(2.00\). Similarly, for the non-Gaussian tasks, we sample \(\tau^{-1}\sim\mathcal{U}[0.20,1.25]\) for each task that we generate, and train a _single model_ on these data. We then evaluate this model for each of the settings \(\tau^{-1}=0.25,0.50\) or \(1.00\). The results of these procedures are summarised in Figure 6.

### Sim-to-real tasks

For the sim-to-real tasks we follow a training procedure that is similar to that of the synthetic experiments. During training, we generate data by sampling the context set size \(N\sim\mathcal{U}[1,512]\), then sample \(N\) context inputs uniformly in \([-1.00,1.00]\) and \(512\) target inputs in the range \([-1.00,1.00]\). We then generate data by sampling them from a GP with covariance

\[k(x,x^{\prime})=k_{3/2,\ell}(x,x^{\prime})+\sigma_{n}^{2},\] (64)

where \(k_{3/2,\ell}\) is a Matern-3/2 covariance with lengthscale \(\ell\sim\mathcal{U}[0.50,2.00]\) and noise standard deviation \(\sigma_{n}\sim\mathcal{U}[0.30,0.80]\). For all tasks, we set the privacy budget at \(\delta=10^{-3}\) and \(\epsilon\sim\mathcal{U}[0.90,4.00]\). The Dobe!Kung dataset is publicly available in TensorFlow 2 [Abadi et al., 2016], specifically the Tensorflow Datasets package. Note that we rescale the ages to be between \(-1.0\) and \(1.0\) and normalise the heights and weights of users to have zero mean and unit standard deviation. We assume that the required statistics for these normalisations are public, but they could be released with additional privacy budget. Inaccurate normalisations would only increase the sim-to-real gap and reduce utility, but not affect the privacy analysis. At evaluation time, we fix \(N\) to each of the numbers specified in Figure 7. We then sample \(N\) points at random from the normalised!Kung dataset and use the remaining points as the target set. We repeat this procedure for 512 separate tasks, and report the mean NLL together with its 95% confidence intervals in Figure 7.

### Optimisation

For all our experiments with the DPConvCNP we use Adam with a learning rate of \(3\times 10^{-4}\), setting all other options to the default TensorFlow 2 settings.

### Compute details

We train the DPConvCNP on a single NVIDIA GeForce RTX 2080 Ti GPU, on a machine with 20 CPU workers. Meta-training requires approximately 5 hours, with synthetic data generated on the fly. Meta-testing is performed on the same infrastructure, and timings are reported in Figure 5.

## Appendix F DPConvCNP architecture

Here we give the details of the DPConvCNP architecture used in our experiments. The DPConvCNP consists of a DPSetConv encoder, and a CNN decoder followed by a SetConv decoder. We specify the details for the parameters of these layers below.

**DPSetConv encoder and SetConv decoder.** For all our experiments, we initialise the DPSetConv and SetConv lengthscales (which are also used to sample the DP noise) to \(\lambda=0.20,\) and allow this parameter to be optimised during training. For the learnable DP parameter mappings \(t(\mu,N)=\mathtt{sig}(\text{NN}_{t}(\mu,N))\) and \(C(\mu,N)=\text{exp}(\text{NN}_{C}(\mu,N))\) we use simple fully connected feedforward networks with two layers of 32 hidden units each. For the discretisation step in the encoder, we use a resolution of 32 points per unit for all our experiments. We also use a fixed discretisation window of \([-7,7]\) for the synthetic tasks and \([-2,2]\) for the sim-to-real tasks. We did this for simplicity, although our implementation supports dynamically adaptive discretisation windows.

**Decoder convolutional neural network.** Most of the computation involved in the DPConvCNP happens in the CNN of the decoder. For this CNN we used a bare-bones implementation of a UNet with skip connections. This UNet consists of an initial convolution layer processes the signal and density channels, along with two constant channels fixed to the magnitudes \(\sigma_{s},\sigma_{d}\) of the DP noise used in these two channels, into another set of \(C_{\text{in}}\) channels. The result of the initial layer is then passed through the UNet backbone, which consists of \(N\) convolutional layers with a stride of \(2\) and with output channels \(C=(C_{1},\dots,C_{N}),\) followed by \(N\) transpose convolutions again with a stride of \(2\) and output channels \(C=(C_{N},\dots,C_{1}).\) Before applying each of these convolution layers, we create a skip connection from the input of the convolution layer and concatenate this to the output of the corresponding transpose convolution layer. Finally, we pass the output of the UNet through a final transpose convolution with \(C_{\text{out}}=2\) output channels, which are then smoothed by the SetConv decoder to obtain the interpolated mean and (log) standard deviation of the predictions at the target points. For all our experiments, we used \(C_{\text{in}}=32,N=7\) and \(C_{n}=256.\) We used a kernel size of 5 for all convolutions and transpose convolutions.

Figure S2: Example model fits for the DPConvCNP on the EQ GP task. For all the above fits, a single _amortised_ DPConvCNP is used, that is a DPConvCNP that has been trained on EQ GP data with randomly chosen lengthscales \(\ell\sim\mathcal{U}[0.20,2.50]\) and random privacy budgets, specifically \(\epsilon\sim\mathcal{U}[0.90,4.00]\) and \(\delta=10^{-3}.\) The first four rows correspond to \(\epsilon=1.00\) and the last four to \(\epsilon=3.00.\) We have fixed \(\delta=10^{-3}.\) Note that column-wise the datasets are fixed, and we are varying the context set size \(N\).

Figure S3: Same as Figure S2, but with a different dataset seed. Example model fits for the DPConvCNP on the EQ GP task. For all the above fits, a single _amortised_ DPConvCNP is used, that is a DPConvCNP that has been trained on EQ GP data with randomly chosen lengthscales \(\ell\sim\mathcal{U}[0.20,2.50]\) and random privacy budgets, specifically \(\epsilon\sim\mathcal{U}[0.90,4.00]\) and \(\delta=10^{-3}\). The first four rows correspond to \(\epsilon=1.00\) and the last four to \(\epsilon=3.00\). We have fixed \(\delta=10^{-3}\). Note that column-wise the datasets are fixed, and we are varying the context set size \(N\).

Figure S4: Example model fits for the DPConvCNP on the sawtooth task. For all the above fits, a single _amortised_ DPConvCNP is used, that is a DPConvCNP that has been trained on sawtooth data with randomly chosen periods \(\tau^{-1}\sim\mathcal{U}[0.20,1.25]\) and random privacy budgets, specifically \(\epsilon\sim\mathcal{U}[0.90,4.00]\) and \(\delta=10^{-3}.\) The first four rows correspond to \(\epsilon=1.00\) and the last four to \(\epsilon=3.00.\) We have fixed \(\delta=10^{-3}.\) Note that column-wise the datasets are fixed, and we are varying the context set size \(N\).

Figure S5: Same as Figure S4, but with a different dataset seed. Example model fits for the DPConvCNP on the sawtooth task. For all the above fits, a single _amortised_ DPConvCNP is used, that is a DPConvCNP that has been trained on sawtooth data with randomly chosen periods \(\tau^{-1}\sim\mathcal{U}[0.20,1.25]\) and random privacy budgets, specifically \(\epsilon\sim\mathcal{U}[0.90,4.00]\) and \(\delta=10^{-3}\). The first four rows correspond to \(\epsilon=1.00\) and the last four to \(\epsilon=3.00\). We have fixed \(\delta=10^{-3}\). Note that column-wise the datasets are fixed, and we are varying the context set size \(N\).

Figure S6: Additional results using the DPConvCNP on the EQ and sawtooth synthetic tasks with stricter DP parameters, namely all combinations of \(\epsilon=\{1/3,1\}\) and \(\delta=\{10^{-5},10^{-3}\}\). The overall setup in this figure is identical to that in Figure 6, except the amortised DPConvCNP is trained on randomly chosen \(\epsilon\sim\mathcal{U}[1/3,1]\) and fixed \(\delta=10^{-5}\) or \(10^{-3}\), and the non-amortised DPConvCNP models are trained on \(\epsilon\) and \(\delta\) values as indicated on the plots. Then, both amortised and non-amortised models are evaluated with the parameters shown on the plots. The DP-SVGP baseline was not run due to time constraints in the rebuttal period: it is significantly slower and more challenging to optimise than the DPConvCNP. We note that the amortisation gap, due to training a model to handle a continuous range of \(\epsilon\) values, is negligible. We also note that as the number of context points \(N\) increases, the performance of the DPConvCNP approaches that of the oracle predictors.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims of the paper, summarised at the end of the introduction accurately reflect the contributions of the paper made in Sections 4 and 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are discussed in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: All novel theorems include either the full proof, or a proof idea with the reference to the full proof in the Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, we give all necessary details in the main text and appendix E, and provide our implementations in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is included in the supplementary material, and the Dobe!Kung dataset [Howell, 2009] is freely available, e.g. through TF datasets Abadi et al. [2016]. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, we give all necessary details in the main text and appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Error bars are reported when appropriate, and documented in the figure captions. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, these details are provided in Appendix E.4. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms to the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Broader impacts are discussed in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: License for the Dobe!Kung dataset [Howell, 2009] is mentioned in the bibliography. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Documentation is included in the code. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not include crowdsourcing experiments or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not include crowdsourcing experiments or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.