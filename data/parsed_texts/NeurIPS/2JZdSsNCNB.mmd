# Hellinger-UCB: A novel algorithm for stochastic multi-armed bandit problem

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

In this paper, we study the stochastic multi-armed bandit problem, where the reward is driven by an unknown random variable. We propose a new variant of the Upper Confidence Bound (UCB) algorithm called Hellinger-UCB, which leverages the squared Hellinger distance to build the upper confidence bound. We prove that the Hellinger-UCB reaches the theoretical lower bound(O(T)). As a real-world example, we apply the Hellinger-UCB algorithm to solve the cold-start problem for a content recommender system of a financial app. With reasonable assumption, the Hellinger-UCB algorithm has an important lower latency feature, closed-form UCB. The online experiment also illustrates that the Hellinger-UCB outperforms both KL-UCB and UCB1 in the sense of a higher click-through rate (CTR), 33% higher than the KL-UCB and almost 100% higher than the UCB1.

## 1 Introduction

### Stochastic multi-armed bandit problem

The stochastic multi-armed bandit problem(MAB) [10] is a sequential decision problem defined by a payoff function and a set of actions. At each step \(t\in{1,2,...,T}\), an action \(A_{t}\) is chosen from the action set \(A=\{1,2,3,...K\}\) by the agent. And the associated reward \(r_{t}(A_{t})\), which is independent and identically distributed(i.i.d.), is obtained. The goal of the agent is to find the optimal strategy that maximizes the cumulative payoff obtained in a sequence of decisions

\[S_{A_{t}}(T)=\sum_{s=1}^{T}r_{t}(A_{t}).\] (1)

The agent must come up with a strategy to maximize the cumulative payoff by dealing with the dilemma between exploitation and exploration. The pseudo-regret \(\bar{R}_{T}\) is introduced to evaluate the performance of a strategy. It is defined as the maximized expectation of the difference between the cumulative payoff of consistently choosing the best action and that of the strategy in the first \(T\) steps

\[\bar{R}_{T}=\max_{i=1,2,...,K}\mathbb{E}[\sum_{s=1}^{T}r_{t}(i)-\sum_{s=1}^{T}r _{t}(A_{t})]\] (2)

Lai and Robbins(1985)[8] showed that if for all \(\epsilon>0\) and \(\theta,\theta_{i}\in\Theta\) with \(\mu(\theta)>\mu(\theta_{i})\), there exists \(\delta>0\) such that \(|KL(\theta,\theta_{i})-KL(\theta,\theta_{j})|<\epsilon\) whenever \(\mu(\theta_{i})<\mu(\theta_{j})<\mu(\theta_{i})+\delta\), and the following theorem is true. Let \(N_{i}(t)\) denote the number of times the agent selected action \(i\) in the first \(t\) steps,

**Theorem 1**: _If a policy has regret \(\bar{R}_{T}=o(T^{a})\) for all \(a>0\) as \(T\to 0\), the number of draws up to time \(t\), \(N_{i}(t)\) of any sub-optimal arm \(i\) is lower bounded_

\[\lim\inf_{T\rightarrow\infty}\frac{N_{i}(T)}{\log(T)}\geqslant\frac{1}{\inf_{ \theta\in\Theta_{i};\mathbb{E}_{\theta}>\mu^{*}}KL(\theta_{i},\theta)}\] (3)

_Therefore, the regret is lower-bounded_

\[\lim\inf_{T\rightarrow\infty}\frac{\bar{R}_{T}}{\log(T)}\geqslant\sum_{i: \Delta_{i}\geqslant 0}\frac{\Delta_{i}}{\inf_{\theta\in\Theta_{i};\mathbb{E}_{ \theta}>\mu^{*}}KL(\theta_{i},\theta)}\] (4)

The stochastic multi-armed bandit problem has been extensively studied [4; 7; 9]. Under the parametric setting, a type of policy called upper-confidence bound (UCB) is proposed and proved to be promising[3]. Agrawal[1] introduced a family of index policies that is easier to compute. Auer, Nicolo and Paul[3] proposed an online, horizon-free procedure which is called upper-confidence bound(UCB) and proved its efficiency. Audibert and Bubeck[2] presented an improvement to the UCB1 called MOSS which is optimal for finite time. A variant of UCB which builds UCB based on the Kullback-Leibler divergence(KL) KL-UCB was presented by Garivier, Cappe[6].

## 2 Setup And Notations

We consider a stochastic multi-armed bandit problem with finite arms \(A=\{1,2,3,...K\}\). Each arm \(i\) is associated with a reward distribution \(p_{i}(\theta)\) over \(\mathbb{R}\). It is assumed that \(p_{i}(\theta)\) is from some one-parameter exponential family \(\mathbb{P}(\theta)\) with unknown expectation \(\mu_{i}\).

It is also common to use a different formula for the pseudo-regret for a stochastic problem. Write \(\mu^{*}=\max_{i=1,2,...,K}\mathbb{E}[\mu_{i}]\) as the expected reward of the optimal action. Then \(\Delta_{i}=\mu_{*}-\mu_{i}\), and we have

\[\bar{R}_{T}=\sum_{i=1}^{K}\mathbb{E}[N_{i}(T)]\mu^{*}-\mathbb{E}[\sum_{i=1}^{ K}N_{i}(T)\mu_{i}]=\sum_{i=1}^{K}\Delta_{i}\mathbb{E}[N_{i}(T)]\] (5)

## 3 The Hellinger-UCB Algorithm

The goal of the UCB algorithm is to make sequential decisions in the stochastic environment. The reward distribution of each arm is unknown. The only way to collect information and estimate the distribution is to pull the arm. But each trial comes with risk which is measured by regret. Hence exploration-exploitation trade-off is important in this case. The motivation of the UCB algorithm is being optimistic about the reward distributions as one always believes that the expected reward is the highest value within the confidence region. Hence the key in the UCB algorithm is constructing the confidence region.

The formula of the squared Hellinger distance makes it computationally efficient. With this property, we propose a new UCB type algorithm, the Hellinger-UCB which constructs the UCB based on the squared Hellinger distance. The new algorithm achieves the theoretical lower bound and has a closed-form UCB for some distributions, for example, binomial distribution. The latter property is favorable in some low-latency applications.

### Main algorithm

We briefly describe the process of Hellinger-UCB here. Let \(A=\{i\}_{i=1}^{K}\) be the action set where \(K\), the number of actions, is a positive integer. For each arm \(i\in A\), the reward distribution \(P_{\theta_{i}}\) is in some one-parameter exponential family with expectation \(\mu_{i}\). At the first \(|K|\) rounds, the agent chooses each arm once. After that, at each round \(t>|K|\), the agent makes a decision \(A_{t}=i\) based on the collected observations of each arm and gets the reward \(g_{t}(A_{t})\) from \(P_{A_{t}}\). The upper confidence bound for arm \(i\) is

\[U_{i}(t)=\text{sup}\{\dot{\psi}(\theta):H^{2}(P_{\dot{\theta}_{i,t-1}},P_{ \theta})\leqslant 1-e^{-\frac{\text{log}(t)}{N_{i}(t)}}\}\] (6)where \(P_{\hat{\theta}_{i,t-1}}\) is the estimated reward distribution based on the past observations and \(N_{i}(t)\) the number of pulls of arm \(i\). In the right hand side term, \(c\in(\frac{1}{4},\frac{1}{2}]\) and usually achieves optimal performance with \(c\) slightly greater than \(\frac{1}{4}\) in practice. This is a convex optimization problem and can be solved efficiently. The agent will choose the action \(i\) with the maximal \(U_{i}(t)\). Algorithm 1 shows the pseudo-code of the Hellinger-UCB algorithm.

```
1. Known Parameters: \(T\)(time horizon), \(K\)(action set), \(i\in K\)(action), \(r_{t}(i)\)(reward given action)
2. For \(t=1\) to \(|K|\): 1. \(A_{t}=i=t\%|K|\) 2. \(N_{i}(t)=1\) 3. \(S_{i}(t)=r_{t}(A_{t})\) end for
3. For \(t=|K|+1\) to \(T\): 1. \(A_{t}=\text{arg}\,max_{i}\text{sup}\{\dot{\psi}(\theta):H^{2}(P_{\hat{\theta}_ {i,t-1}},P_{\theta})\leqslant 1-e^{-c\frac{\log(t)}{N_{t}(t)}}\}\), where \(P_{\hat{\theta}_{t-1}}\) is the maximum likelihood estimation(MLE) of the reward distribution based on the past observations. 2. \(r=r_{t}(A_{t})\) 3. \(N_{i}(t)+=\mathbb{I}\{A_{t}=i\}\) 4. \(S_{i}(t)+=r\) end for ```

**Algorithm 1** Hellinger-UCB

### Optimality of Hellinger-UCB

As a UCB-based algorithm for the stochastic multi-armed bandit problem, we are interested in whether the pseudo regret of the Hellinger-UCB algorithm is optimal. The following theorem guarantees the optimality of this algorithm. We first derive the upper bound of each sub-optimal arm's expected number of pulls.

**Theorem 2**: _Consider a multi-armed bandit problem with \(K\) arms and the associated payoffs are some distributions in a one-parameter exponential family. Let \(a^{*}\) denote the optimal arm with expectation \(\mu^{*}\) and \(i\) denote some sub-optimal arm with expectation \(\mu_{i}\) such that \(\mu_{i}<\mu^{*}\). For any \(T>0\), the number of picks of arm \(i\) by Hellinger-UCB is \(N_{i}(T)\). For any \(\epsilon>0\)_

\[\mathbb{E}[N_{i}(T)]\leqslant-\frac{c\log(T)}{\log(1-\frac{H^{2}(\mu^{*},\mu_ {i})}{1+\epsilon})}+\frac{C_{1}(\epsilon)}{T^{C_{2}(\epsilon)}}+\sum_{t=1}^{T }\frac{1}{t^{2c}}+\frac{e^{-2H^{2}(\mu^{*},\mu_{i})}}{1-e^{-2H^{2}(\mu^{*}, \mu_{i})}}\]

_where \(C_{1}(\epsilon)=-\frac{c}{\log(1-\frac{H^{2}(\mu^{*},\mu_{i})}{1+\epsilon})}>0\) and \(C_{2}(\epsilon)=\frac{(\sqrt{1+\epsilon}-1)^{2}}{1+\epsilon}>0\)._

_if \(c>\frac{1}{4}\),_

\[\mathbb{E}[N_{i}(T)]\leqslant-\frac{c\log(T)}{\log(1-\frac{H^{2}(\mu^{*},\mu_ {i})}{1+\epsilon})}+\frac{C_{1}(\epsilon)}{T^{C_{2}(\epsilon)}}+O(1)\]

**Proof:** See Appendix A for details of the proof

Then the upper bound of the pseudo-regret is just a direct result of Theorem 3.1

**Theorem 3**: _The regret of Hellinger-UCB satisfies:_

\[\bar{R}_{T}\leqslant\sum_{i:\mu_{i}\leqslant\mu^{*}}\Delta_{i}\mathbb{E}[N_{i} (T)]\] (7)Numerical result

The long-run online experiment is conducted in the front page content recommendation business of JD Finance App. The recommendation system is designed to provide personalized multi-type content recommendations to the users. For each request, the cold start model is required to rank a set of articles and tweets, and then present the top-rank contents to the users. All three algorithms, UCB1, KL-UCB, and Hellinger-UCB, rank about ten thousand of contents(Financial articles) from the cold start pool with estimated CTR. CTR is modeled as the mean reward of a series of Bernoulli trials, which is the exact historical clicks and impression information. Three UCB algorithms share the whole traffic and the final impression is generated by randomly selecting one of three results uniformly. The system records 1 point as a reward to the corresponding algorithm when the user has any positive interaction (click/like/comment) with the content. Under this setting, the comparison of rewards among the three algorithms will give an insight into CTR for each algorithm. We show the upper confidence bound of UCB1, KL-UCB, and Hellinger-UCB in the appendix.

Figure 1 shows the cumulative reward plot of three algorithms in a two-month experiment from Oct. 2020 to Nov. 2020. It is very clear that Hellinger-UCB (blue line) significantly outperforms KL-UCB (orange line) and UCB1 algorithm (green line). In fact, the Hellinger-UCB algorithm achieves about \(33\%\) more clicks than the KL-UCB algorithm and almost \(100\%\) more clicks than the UCB1 algorithm. Hellinger-UCB algorithm obtains more clicks in the early period and then achieves even more clicks as the learning continues. This is an encouraging illustration of the potential power of Hellinger-UCB in real applications.

## 5 Conclusion

We presented the Hellinger-UCB algorithm for the stochastic multi-armed bandit problem. In the case that the reward is from an unknown exponential family, we provide the detailed formula of the algorithm and an optimal regret upper bound that achieves \(O(\log(T))\). We present real numerical experiments that show significant improvement over other variants of UCB algorithms. The cumulative reward form the proposed algorithm is higher. We also show the algorithm has a closed-form UCB when the reward is a bernoulli distribution, which is a beneficial property for low-latency applications.

Figure 1: Cumulative reward plot of different UCB algorithms. The y-axis is reward points. The x-axis is a time stamp recorded as 9 digits integer.

## References

* [1] Rajeev Agrawal. Sample mean based index policies by o (log n) regret for the multi-armed bandit problem. _Advances in applied probability_, 27(4):1054-1078, 1995.
* [2] Jean-Yves Audibert, Sebastien Bubeck, et al. Minimax policies for adversarial and stochastic bandits. In _COLT_, volume 7, pages 1-122, 2009.
* [3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47:235-256, 2002.
* [4] Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationary rewards. _Advances in neural information processing systems_, 27, 2014.
* [5] Olivier Cappe, Aurelien Garivier, Odalric-Ambrym Maillard, Remi Munos, and Gilles Stoltz. Kullback-leibler upper confidence bounds for optimal sequential allocation. _The Annals of Statistics_, pages 1516-1541, 2013.
* [6] Aurelien Garivier and Olivier Cappe. The kl-ucb algorithm for bounded stochastic bandits and beyond. In _Proceedings of the 24th annual conference on learning theory_, pages 359-376. JMLR Workshop and Conference Proceedings, 2011.
* [7] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In _ICML_, volume 12, pages 655-662, 2012.
* [8] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_, 6(1):4-22, 1985.
* [9] David Liau, Zhao Song, Eric Price, and Ger Yang. Stochastic multi-armed bandits in constant space. In _International Conference on Artificial Intelligence and Statistics_, pages 386-394. PMLR, 2018.
* [10] Herbert E. Robbins. Some aspects of the sequential design of experiments. _Bulletin of the American Mathematical Society_, 58:527-535, 1952.

## Appendix A Appendix

### Proof of Theorem 3.1

**Proof:** : Hellinger-UCB algorithm relies on the following upper confidence bound for \(\mu_{i}\):

\[u_{i}(t)=max\{q>\hat{\mu_{i}}(t):H^{2}(\hat{\mu_{i}}(t),q)\leqslant 1-\exp\{-c \frac{log(t)}{N_{i}(t)}\}\}\] (8)

The expectation of \(N_{i}(T)\) is upper-bounded by using the following decomposition. When a sub-optimal arm \(i\) is pulled, then the upper confidence bound of the optimal arm \(u^{*}(t)\) based on historical observations is either greater or less than its true expectation \(\mu^{*}\). In the latter case,

\[\mathbb{E}[N_{i}(T)] =\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i\}]\] (9) \[=\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}>u^{*}(t)\} ]+\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{*}(t)\}]\] (10) \[\leqslant\sum_{t=1}^{T}\mathbb{P}\{\mu^{*}>u^{*}(t)\}+\mathbb{E}[ \sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{*}(t)\}]\] (11) \[\leqslant C_{1}(\epsilon)\log(T)+\frac{(C_{2}(\epsilon)H^{2}(\mu^ {*},\mu_{i}))^{-1}}{T^{2C_{1}(\epsilon)C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i})}}\] (12) \[+\frac{e^{-2H^{2}(\mu^{*},\mu_{i})}}{1-e^{-2H^{2}(\mu^{*},\mu_{i} )}}+\sum_{t=1}^{T}\frac{1}{t^{2c}}\] (13)The last inequality is from Lemma 1

\[\sum_{t=1}^{T}\mathbb{P}\{\mu^{*}>u^{*}(t)\}\leqslant\sum_{t=1}^{T}\frac{1}{t^{2c}}\] (14)

and Lemma 3

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{*}(t)\}]\] (15)

\[\leqslant C_{1}(\epsilon)\log(T)+\frac{(C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i} ))^{-1}}{T^{2C_{1}(\epsilon)C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i})}}+\frac{e^{ -2H^{2}(\mu^{*},\mu_{i})}}{1-e^{-2H^{2}(\mu^{*},\mu_{i})}}\] (16)

If \(c>\frac{1}{4}\), according to Lemma 2 and Lemma 3, we have

\[\mathbb{E}[N_{i}(T)]\leqslant C_{1}(\epsilon)\log(T)+\frac{(C_{2}(\epsilon)H^{2}( \mu^{*},\mu_{i}))^{-1}}{T^{2C_{1}(\epsilon)C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{ i})}}+O(1)\] (17)

The details of these lemmas are in the following section. \(\Box\)

### The Proof of the Theorem

This concentration inequality[5] will be used several times

\[\mathbb{P}\{\hat{\mu}(n)>\mu,KL(\hat{\mu}(n),\mu)>\frac{f(n)}{n}\}\leqslant e^ {-f(n)}\] (18)

. The following lemmas support the proof of the main theorem.

**Lemma 1**: \[\sum_{t=1}^{T}\mathbb{P}\{\mu^{*}>u^{*}(t)\}\leqslant\sum_{t=1}^{T}\frac{1}{t^ {2c}}\]

**Proof:**\(\hat{\mu}^{*}(t)\) is the M.L.E. of \(\mu^{*}\), then

\[\mathbb{P}\{\mu^{*}>u^{*}(t)\}\] (19)

\[\leqslant \mathbb{P}\{\mu^{*}>\hat{\mu}^{*}(t),H^{2}(\mu^{*},\hat{\mu}^{*}(t)) \geqslant 1-\exp\{-c\frac{\log(t)}{N^{*}(t)}\}\}\] (20)

Since for exponential family \(-\log(1-H^{2}(\mu^{*},\hat{\mu}^{*}(t)))\leqslant\frac{1}{2}KL(\hat{\mu}^{*}( t),\mu^{*})\), (20) becomes

\[\mathbb{P}\{\mu^{*}>u^{*}(t)\}\] (21)

\[\leqslant \mathbb{P}\{\mu^{*}>\hat{\mu}^{*}(t),1-e^{-\frac{1}{2}KL(\hat{\mu}^ {*}(t),\mu^{*})}\geqslant(1-\exp\{-c\frac{\log(t)}{N^{*}(t)}\})\}\] (22)

\[\leqslant \mathbb{P}\{\mu^{*}>\hat{\mu}^{*}(t),KL(\hat{\mu}^{*}(t),\mu^{*}) \geqslant 2c\frac{\log(t)}{N^{*}(t)}\}\] (23) \[\leqslant e^{-2c\log(t)}\] (24) \[= \frac{1}{t^{2c}}\] (25)

. Then

\[\sum_{t=1}^{T}\mathbb{P}\{\mu^{*}>u^{*}(t)\}\leqslant\sum_{t=1}^{T}\frac{1}{t^ {2c}}\] (26)

\(\Box\)

**Lemma 2**: _If \(c>\frac{1}{4}\) in_

\[u^{*}(t)=max\{q>\hat{\mu_{i}}(t):H^{2}(\hat{\mu^{*}}(t),q)\leqslant 1-\exp\{-c \frac{log(t)}{N^{*}(t)}\}\}\]

_then_

\[\sum_{t=1}^{\infty}\mathbb{P}\{\mu^{*}>u^{*}(t)\}=O(1)\]

[MISSING_PAGE_EMPTY:7]

**Proof:** Arm \(i\) is sub-optimal with expected reward \(\mu_{i}\) and \(\hat{\mu}_{i}(t)\) is the M.L.E. for \(\mu_{i}\) at \(t\),. Then we have

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{*} (t)\}]\] (42) \[= \mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{* }(t),\mu^{*}\leqslant\hat{\mu}_{i}(t)\}]+\] (43) \[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{* }(t),\mu^{*}>\hat{\mu}_{i}(t)\}]\] (44) \[\leqslant C_{1}(\epsilon)\log(T)+\frac{(C_{2}(\epsilon)H^{2}(\mu ^{*},\mu_{i}))^{-1}}{T^{2C_{1}(\epsilon)C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i})} }+\frac{e^{-2H^{2}(\mu^{*},\mu_{i})}}{1-e^{-2H^{2}(\mu^{*},\mu_{i})}}\] (45)

(45) is according to Lemma B.4 and Lemma B.5.

\(\Box\)

**Lemma 4**: \[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{*}(t),\mu^ {*}\leqslant\hat{\mu}_{i}(t)\}]\leqslant\frac{e^{-2H^{2}(\mu^{*},\mu_{i})}(1- e^{-2TH^{2}(\mu^{*},\mu_{i})})}{1-e^{-2H^{2}(\mu^{*},\mu_{i})}}\]

**Proof:**

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{* }(t),\mu^{*}\leqslant\hat{\mu}_{i}(t)\}]\] (46) \[\leqslant \mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant \hat{\mu}_{i}(t),H^{2}(\mu^{*},\mu_{i})\leqslant H_{2}(\hat{\mu}_{i}(t),\mu_{ i})\}]\] (47) \[\leqslant \mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant \hat{\mu}_{i}(t),H^{2}(\mu^{*},\mu_{i})\leqslant\frac{1}{2}KL(\hat{\mu}_{i}(t ),\mu_{i})\}]\] (48) \[= \mathbb{E}[\sum_{t=1}^{T}\sum_{s=1}^{t}\mathbb{I}\{A_{t}=i,N_{i} (t)=s,\mu^{*}\leqslant\hat{\mu}_{i}(s),H^{2}(\mu^{*},\mu_{i})\leqslant\frac{ 1}{2}KL(\hat{\mu}_{i}(s),\mu_{i})\}]\] (49) \[= \mathbb{E}[\sum_{s=1}^{T}\sum_{t=s}^{T}\mathbb{I}\{A_{t}=i,N_{i} (t)=s\}\mathbb{I}\{\mu^{*}\leqslant\hat{\mu}_{i}(s),H^{2}(\mu^{*},\mu_{i}) \leqslant\frac{1}{2}KL(\hat{\mu}_{i}(s),\mu_{i})\}]\] (50) \[= \mathbb{E}[\sum_{s=1}^{T}\mathbb{I}\{\mu^{*}\leqslant\hat{\mu}_{ i}(s),H^{2}(\mu^{*},\mu_{i})\leqslant\frac{1}{2}KL(\hat{\mu}_{i}(s),\mu_{i})\} \sum_{t=s}^{T}\mathbb{I}\{A_{t}=i,N_{i}(t)=s\}]\] (51)

Notice in (51) \(\sum_{t=s}^{T}\mathbb{I}\{A_{t}=i,N_{i}(t)=s\}]\leqslant 1\) and thus

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{* }(t),\mu^{*}\leqslant\hat{\mu}_{i}(t)\}]\] (52) \[\leqslant \mathbb{E}[\sum_{s=1}^{T}\mathbb{I}\{\mu^{*}\leqslant\hat{\mu}_{ i}(s),H^{2}(\mu^{*},\mu_{i})\leqslant\frac{1}{2}KL(\hat{\mu}_{i}(s),\mu_{i})\}]\] (53) \[= \sum_{s=1}^{T}\mathbb{P}\{\mu^{*}\leqslant\hat{\mu}_{i}(s),H^{2} (\mu^{*},\mu_{i})\leqslant\frac{1}{2}KL(\hat{\mu}_{i}(s),\mu_{i})\}\] (54) \[\leqslant \sum_{s=1}^{T}e^{-2sH^{2}(\mu^{*},\mu_{i})}\] (55) \[= \frac{e^{-2H^{2}(\mu^{*},\mu_{i})}(1-e^{-2TH^{2}(\mu^{*},\mu_{i})} )}{1-e^{-2H^{2}(\mu^{*},\mu_{i})}}\] (56)It is easy to show

\[\lim_{T\rightarrow\infty}\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leq u ^{*}(t),\mu^{*}\leq\hat{\mu}_{i}(t)\}]=\frac{e^{-2H^{2}(\mu^{*},\mu_{i})}}{1-e^ {-2H^{2}(\mu^{*},\mu_{i})}}\] (57)

which is a problem-dependent constant. \(\Box\)

**Lemma 5**: _For any \(\epsilon>0\), then_

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leq u^{*}(t), \mu^{*}>\hat{\mu}_{i}(t)\}]\] \[\leq C_{1}(\epsilon)\log(T)+\frac{(C_{2}(\epsilon)H^{2}(\mu^{*},\mu_ {i}))^{-1}}{T^{2C_{1}(\epsilon)C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i})}}\]

_where \(C_{1}(\epsilon)=-\frac{c}{\log(1-\frac{H^{2}(\mu^{*},\mu_{i})}{1+\epsilon})}>0\) and \(C_{2}(\epsilon)=\frac{(\sqrt{1+\epsilon}-1)^{2}}{1+\epsilon}>0\)._

**Proof:** Similar to the proof for Lemma B.4 we can have

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leq u^{*}(t ),\mu^{*}>\hat{\mu}_{i}(t)\}]\] (58) \[\leq \mathbb{E}[\sum_{t=1}^{T}\sum_{s=1}^{t}\mathbb{I}\{A_{t}=i,N_{i}( t)=s,H^{2}(\mu^{*},\hat{\mu}_{i}(s))<1-e^{-c\frac{\log(t)}{s}}\}]\] (59) \[\leq \mathbb{E}[\sum_{s=1}^{T}\sum_{t=s}^{T}\mathbb{I}\{A_{t}=i,N_{i}( t)=s\}\mathbb{I}\{H^{2}(\mu^{*},\hat{\mu}_{i}(s))<1-e^{-c\frac{\log(T)}{s}}\}]\] (60) \[\leq \mathbb{E}[\sum_{s=1}^{T}\mathbb{I}\{H^{2}(\mu^{*},\hat{\mu}_{i}( s))<1-e^{-c\frac{\log(T)}{s}}\}]\] (61) \[= \sum_{s=1}^{T}\mathbb{P}\{H^{2}(\mu^{*},\hat{\mu}_{i}(s))<1-e^{- c\frac{\log(T)}{s}}\}\] (62) \[\leq K_{T}+\sum_{s=K_{T}+1}^{T}\mathbb{P}\{H^{2}(\mu^{*},\hat{\mu}_{i}( s))<1-e^{-c\frac{\log(T)}{K_{T}}}\}\] (63)

where \(K_{T}=C_{1}(\epsilon)\log(T)\) and \(C_{1}(\epsilon)=-\frac{c}{\log(1-\frac{H^{2}(\mu^{*},\mu_{i})}{1+\epsilon})}>0\). Then substitute this into (63). Then

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leq u^{*}(t), \mu^{*}>\hat{\mu}_{i}(t)\}]\] (64) \[\leq C_{1}(\epsilon)\log(T)+\sum_{s=K_{T}+1}^{T}\mathbb{P}\{H^{2}(\mu^ {*},\hat{\mu}_{i}(s))<\frac{H^{2}(\mu^{*},\mu_{i})}{1+\epsilon}\}\] (65)

There exist \(\mu^{\prime}\in(\mu_{i},\mu^{*})\) such that \((1+\epsilon)H^{2}(\mu^{*},\mu^{\prime})=H^{2}(\mu^{*},\mu_{i})\). Then \(H^{2}(\mu^{*},\hat{\mu}_{i}(s))<\frac{H^{2}(\mu^{*},\mu_{i})}{1+\epsilon}\) implies \(\mu_{i}<\mu^{\prime}<\hat{\mu}_{i}(s)\) and \(H^{2}(\mu_{i},\hat{\mu}_{i}(s))>H^{2}(\mu_{i},\mu^{\prime})\). The second term in (65)becomes

\[\sum_{s=K_{T}+1}^{\infty}\mathbb{P}\{H^{2}(\mu^{*},\hat{\mu}_{i}(s) )<\frac{H^{2}(\mu^{*},\mu_{i})}{1+\epsilon}\}\] (66) \[\leqslant\sum_{s=K_{T}+1}^{\infty}\mathbb{P}\{\hat{\mu}_{i}(s)>\mu _{i},H^{2}(\mu_{i},\hat{\mu}_{i}(s))>H^{2}(\mu_{i},\mu^{\prime})\}\] (67) \[\leqslant\sum_{s=K_{T}+1}^{\infty}\mathbb{P}\{\hat{\mu}_{i}(s)>\mu _{i},\frac{1}{2}KL(\hat{\mu}_{i}(s),\mu_{i})>H^{2}(\mu_{i},\mu^{\prime})\}\] (68) \[\leqslant\sum_{s=K_{T}+1}^{\infty}e^{-2sH^{2}(\mu_{i},\mu^{\prime })}\] (69) \[= \frac{e^{-2(K_{T}+1)H^{2}(\mu_{i},\mu^{\prime})}}{1-e^{-2H^{2}( \mu_{i},\mu^{\prime})}}\] (70)

The numerator of (70) \(e^{-2(K_{T}+1)H^{2}(\mu_{i},\mu^{\prime})}\leqslant T^{-2C_{1}(\epsilon)H^{2} (\mu_{i},\mu_{\prime})}\). For the denominator of (70), we have \(1-e^{-2H^{2}(\mu_{i},\mu^{\prime})}>H^{2}(\mu_{i},\mu^{\prime})\) since \(1-e^{-x}=1-(1-x+\frac{x^{2}}{2}-...)>x-\frac{x^{2}}{2}>x-\frac{x}{2}\) if \(0<x<1\). Therefore

\[\sum_{s=K_{T}+1}^{\infty}\mathbb{P}\{H^{2}(\mu^{*},\hat{\mu}_{i}(s))<\frac{H^ {2}(\mu^{*},\mu_{i})}{1+\epsilon}\}\leqslant\frac{H^{2}(\mu_{i},\mu^{\prime}) ^{-1}}{T^{2C_{1}(\epsilon)H^{2}(\mu_{i},\mu_{\prime})}}\] (71)

Since the squared Hellinger distance is a metric, we have

\[H(\mu^{*},\mu_{i})= \sqrt{1+\epsilon}H((\mu^{*},\mu^{\prime}))\] (72) \[\geqslant \sqrt{1+\epsilon}(H(\mu^{*},\mu_{i})-H(\mu^{\prime},\mu_{i}))\] (73)

This implies

\[H^{2}(\mu^{\prime},\mu_{i})\geqslant\frac{(\sqrt{1+\epsilon}-1)^{2}}{1+ \epsilon}H^{2}(\mu^{*},\mu_{i})=C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i})\] (74)

Finally, we conclude that

\[\mathbb{E}[\sum_{t=1}^{T}\mathbb{I}\{A_{t}=i,\mu^{*}\leqslant u^{*}(t),\mu^{ *}>\hat{\mu}_{i}(t)\}]\leqslant\frac{(C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i}))^ {-1}}{T^{2C_{1}(\epsilon)C_{2}(\epsilon)H^{2}(\mu^{*},\mu_{i})}}\] (75)

where \(C_{1}(\epsilon)>0\) and \(C_{2}(\epsilon)>0\). 

## Appendix B Appendix

### Solution of different UCBs

We will compare the solutions of three UCB algorithms and introduce the advantages and disadvantages of those algorithms. We will also prove that Hellinger-UCB has a close form with binomial distribution assumption of rewards.

#### b.1.1 UCB1 algorithm

The UCB1 algorithm[3], regardless of the reward distribution, always uses the following UCB formula:

\[U_{t}(i)=\hat{\mu}_{i,t-1}+\sqrt{\frac{2\log(t)}{N_{i}(t)}}\] (76)

where \(\hat{\mu}_{i,t-1}\) is the success ratio of content \(i\) at time \(t-1\), \(N_{i}(t)\) the number of impressions of content \(i\) at time \(t\). UCB1 algorithm can always compute its confidence bound with a counting process.

#### b.1.2 KL-UCB algorithm

The KL-UCB algorithm solves the following optimization problem numerically to find the best arm

\[U_{t}(i)=\sup\{\mu(\theta):KL(P_{\hat{\theta}_{t-1}}|P_{\theta})\leqslant\frac{ \log(t)+c\log\log(t)}{N_{i}(t)}\}\] (77)

where \(P_{\hat{\theta}_{t-1}}\) and \(P_{\hat{\theta}_{t}}\) are the reward distribution at \(t-1\) and \(t\). With the assumption of reward distribution in 1, the solution of KL-UCB becomes the following root-finding problem

\[U_{t}(i)=\sup\{p_{t}:p_{t-1}\log\frac{p_{t-1}}{p_{t}}+(1-p_{t-1})\log\frac{1-p_ {t-1}}{1-p_{t}}=C\}\] (78)

where

\[C=\frac{\log(t)+c\log\log(t)}{N_{i}(t)}\] (79)

It is easy to get that 78 does not have a closed-form solution and requires a numerical solver to iterate the solution. Therefore KL-USB requires much more computation resources than UCB1 though KL-UCB has better performance than UCB1. In real-world applications, KL-UCB is less favorable than UCB1 since it requires much more careful engineering controls.

#### b.1.3 Hellinger-UCB algorithm

The Hellinger-UCB chooses the best arm by solving the following optimization problem

\[U_{t}(i)=\text{sup}\{\dot{\psi}(\theta):H^{2}(P_{\hat{\theta}_{i,t-1}},P_{ \theta})\leqslant 1-e^{-c\frac{\log(t)}{N_{i}(t)}}\}\] (80)

unlike KL-UCB, the Hellinger-UCB algorithm has a closed-form solution with the binomial reward.

Recall that the squared Hellinger distance between two Binomial distributions \(B(n,p)\) and \(B(n,q)\) is given by

\[H^{2}(p,q)=1-\sqrt{(1-p)(1-q)}-\sqrt{pq}\] (81)

Let \(f(q)=1-\sqrt{(1-p)(1-q)}-\sqrt{pq}\), its derivative is \(f^{\prime}(q)=\sqrt{\frac{1-p}{1-q}}-\sqrt{\frac{p}{q}}\). It is easy to see that

\[f^{\prime}(q):\begin{cases}<0,&q<p,\\ =0,&q=p,\\ >0,&q>p.\end{cases}\] (82)

The solution to the above equation 80 must be on the squared Hellinger ball. Let \(R\) be the radius of the squared Hellinger Ball, i.e.

\[R=1-\sqrt{(1-p)(1-q)}-\sqrt{pq}.\] (83)

Divide both sides by \(\sqrt{q}\) and let \(m_{1}=\sqrt{\frac{1-p}{p}}\) and \(m_{2}=\frac{1-R}{\sqrt{p}}\)

\[\sqrt{q}+m_{1}\sqrt{1-q}=m_{2}.\] (84)

Take the square of both sides and simplify the equation

\[2m_{1}\sqrt{q(1-q)}=m_{2}^{2}-m_{1}^{2}+(m_{1}^{2}-1)q\] (85)

Repeat above procedure one more time and simplify the result

\[(m_{1}^{2}+1)^{2}q^{2}+2(m_{1}^{2}m_{2}^{2}-m_{1}^{4}-m_{1}^{2}-m_{2}^{2})q+( m_{2}^{2}-m_{1}^{2})^{2}=0.\] (86)

Let \(a=(m_{1}^{2}+1)^{2}\), \(b=2(m_{1}^{2}m_{2}^{2}-m_{1}^{4}-m_{1}^{2}-m_{2}^{2})\) and \(c=(m_{2}^{2}-m_{1}^{2})^{2}\), the root of 86 is

\[q=\frac{-b\pm\sqrt{b^{2}-4ac}}{2a}.\] (87)

The larger root is desired. Therefore, Hellinger-UCB has a close form solution with binomial reward distribution assumption.