# Practical Contextual Bandits with Feedback Graphs

 Mengxiao Zhang

University of Southern California

mengxiao.zhang@usc.edu

&Yuheng Zhang

University of Illinois Urbana-Champaign

yuhengz2@illinois.edu

&Olga Vrousgou

Microsoft Research

Olga.Vrousgou@microsoft.com

&Haipeng Luo

University of Southern California

haipengl@usc.edu

&Paul Mineiro

Microsoft Research

pmineiro@microsoft.com

Equal contribution.

###### Abstract

While contextual bandit has a mature theory, effectively leveraging different feedback patterns to enhance the pace of learning remains unclear. Bandits with feedback graphs, which interpolates between the full information and bandit regimes, provides a promising framework to mitigate the statistical complexity of learning. In this paper, we propose and analyze an approach to contextual bandits with feedback graphs based upon reduction to regression. The resulting algorithms are computationally practical and achieve established minimax rates, thereby reducing the statistical complexity in real-world applications.

## 1 Introduction

This paper is primarily concerned with increasing the pace of learning for contextual bandits (Auer et al., 2002; Langford and Zhang, 2007). While contextual bandits have enjoyed broad applicability (Bouneffouf et al., 2020), the statistical complexity of learning with bandit feedback imposes a data lower bound for application scenarios (Agarwal et al., 2012). This has inspired various mitigation strategies, including exploiting function class structure for improved experimental design (Zhu and Mineiro, 2022), and composing with memory for learning with fewer samples (Rucker et al., 2022). In this paper we exploit alternative graph feedback patterns to accelerate learning: intuitively, there is no need to explore a potentially suboptimal action if a presumed better action, when exploited, yields the necessary information.

The framework of bandits with feedback graphs is mature and provides a solid theoretical foundation for incorporating additional feedback into an exploration strategy (Mannor and Shamir, 2011; Alon et al., 2015, 2017). Succinctly, in this framework, the observation of the learner is decided by a directed feedback graph \(G\): when an action is played, the learner observes the loss of every action to which the chosen action is connected. When the graph only contains self-loops, this problem reduces to the classic bandit case. For non-contextual bandits with feedback graphs, (Alon et al., 2015) provides a full characterization on the minimax regret bound with respect to different graph theoretic quantities associated with \(G\) according to the type of the feedback graph.

However, contextual bandits with feedback graphs have received less attention (Singh et al., 2020; Wang et al., 2021). Specifically, there is no prior work offering a solution for general feedback graphsand function classes. In this work, we take an important step in this direction by adopting recently developed minimax algorithm design principles in contextual bandits, which leverage realizability and reduction to regression to construct practical algorithms with strong statistical guarantees (Foster et al., 2018; Foster and Rakhlin, 2020; Foster et al., 2020; Foster and Krishnamurthy, 2021; Foster et al., 2021; Zhu and Mineiro, 2022). Using this strategy, we construct a practical algorithm for contextual bandits with feedback graphs that achieves the optimal regret bound. Moreover, although our primary concern is accelerating learning when the available feedback is more informative than bandit feedback, our techniques also succeed when the available feedback is less informative than bandit feedback, e.g., in spam filtering where some actions generate no feedback. More specifically, our contributions are as follows.

Contributions.In this paper, we extend the minimax framework proposed in (Foster et al., 2021) to contextual bandits with general feedback graphs, aiming to promote the utilization of different feedback patterns in practical applications. Following (Foster and Rakhlin, 2020; Foster et al., 2021; Zhu and Mineiro, 2022), we assume that there is an online regression oracle for supervised learning on the loss. Based on this oracle, we propose \(\mathsf{SquareCB}.\mathsf{G}\), the first algorithm for contextual bandits with feedback graphs that operates via reduction to regression (Algorithm 1). Eliding regression regret factors, our algorithm achieves the matching optimal regret bounds for deterministic feedback graphs, with \(\widetilde{\mathcal{O}}(\sqrt{\alpha T})\) regret for strongly observable graphs and \(\widetilde{\mathcal{O}}(d^{\frac{3}{3}}T^{\frac{2}{3}})\) regret for weakly observable graphs, where \(\alpha\) and \(d\) are respectively the independence number and weakly domination number of the feedback graph (see Section 3.2 for definitions). Notably, \(\mathsf{SquareCB}.\mathsf{G}\) is computationally tractable, requiring the solution to a convex program (Theorem 3.6), which can be readily solved with off-the-shelf convex solvers (Appendix A.3). In addition, we provide closed-form solutions for specific cases of interest (Section 4), leading to a more efficient implementation of our algorithm. Empirical results further showcase the effectiveness of our approach (Section 5).

## 2 Problem Setting and Preliminary

Throughout this paper, we let \([n]\) denote the set \(\{1,2,\ldots,n\}\) for any positive integer \(n\). We consider the following contextual bandits problem with informed feedback graphs. The learning process goes in \(T\) rounds. At each round \(t\in[T]\), an environment selects a context \(x_{t}\in\mathcal{X}\), a (stochastic) directed feedback graph \(G_{t}\in[0,1]^{\mathcal{A}\times\mathcal{A}}\), and a loss distribution \(\mathbb{P}_{t}:\mathcal{X}\to\Delta([-1,1]^{\mathcal{A}})\); where \(\mathcal{A}\) is the action set with finite cardinality \(K\). For convenience, we use \(\mathcal{A}\) and \([K]\) interchangeably for denoting the action set. Both \(G_{t}\) and \(x_{t}\) are revealed to the learner at the beginning of each round \(t\). Then the learner selects one of the actions \(a_{t}\in\mathcal{A}\), while at the same time, the environment samples a loss vector \(\ell_{t}\in[-1,1]^{\mathcal{A}}\) from \(\mathbb{P}_{t}(\cdot|x_{t})\). The learner then observes some information about \(\ell_{t}\) according to the feedback graph \(G_{t}\). Specifically, for each action \(j\), she observes the loss of action \(j\) with probability \(G_{t}(a_{t},j)\), resulting in a realization \(A_{t}\), which is the set of actions whose loss is observed. With a slight abuse of notation, denote \(G_{t}(\cdot|a)\) as the distribution of \(A_{t}\) when action \(a\) is picked. We allow the context \(x_{t}\), the (stochastic) feedback graphs \(G_{t}\) and the loss distribution \(\mathbb{P}_{t}(\cdot|x_{t})\) to be selected by an adaptive adversary. When convenient, we will consider \(G\) to be a \(K\)-by-\(K\) matrix and utilize matrix notation.

Other Notations.Let \(\Delta(K)\) denote the set of all Radon probability measures over a set \([K]\). \(\mathrm{conv}(S)\) represents the convex hull of a set \(S\). Denote \(I\) as the identity matrix with an appropriate dimension. For a \(K\)-dimensional vector \(v\), \(\mathrm{diag}(v)\) denotes the \(K\)-by-\(K\) matrix with the \(i\)-th diagonal entry \(v_{i}\) and other entries \(0\). We use \(\mathbb{R}_{\geq 0}^{K}\) to denote the set of \(K\)-dimensional vectors with non-negative entries. For a positive definite matrix \(M\in\mathbb{R}^{K\times K}\), we define norm \(\|z\|_{M}=\sqrt{z^{\top}Mz}\) for any vector \(z\in\mathbb{R}^{K}\). We use the \(\widetilde{\mathcal{O}}(\cdot)\) notation to hide factors that are polylogarithmic in \(K\) and \(T\).

Realizability.We assume that the learner has access to a known function class \(\mathcal{F}\subset(\mathcal{X}\times\mathcal{A}\mapsto[-1,1])\) which characterizes the mean of the loss for a given context-action pair, and we make the following standard realizability assumption studied in the contextual bandit literature (Agarwal et al., 2012; Foster et al., 2018; Foster and Rakhlin, 2020; Simchi-Levi and Xu, 2021).

**Assumption 1** (Realizability).: _There exists a regression function \(f^{\star}\in\mathcal{F}\) such that \(\mathbb{E}[\ell_{t,a}\mid x_{t}]=f^{\star}(x_{t},a)\) for any \(a\in\mathcal{A}\) and across all \(t\in[T]\)._Two comments are in order. First, we remark that, similar to (Foster et al., 2020), misspecification can be incorporated while maintaining computational efficiency, but we do not complicate the exposition here. Second, Assumption 1 induces a "semi-adversarial" setting, wherein nature is completely free to determine the context and graph sequences; and has considerable latitude in determining the loss distribution subject to a mean constraint.

Regret.For each regression function \(f\in\mathcal{F}\), let \(\pi_{f}(x_{t}):=\operatorname*{argmin}_{a\in\mathcal{A}}f(x_{t},a)\) denote the induced policy, which chooses the action with the least loss with respective to \(f\). Define \(\pi^{\star}:=\pi_{f}\), as the optimal policy. We measure the performance of the learner via regret to \(\pi^{\star}\): \(\mathbf{Reg_{CB}}:=\sum_{t=1}^{T}\ell_{t,a_{t}}-\sum_{t=1}^{T}\ell_{t,\pi^{ \star}(x_{t})}\), which is the difference between the loss suffered by the learner and the one if the learner applies policy \(\pi^{\star}\).

Regression OracleWe assume access to an online regression oracle \(\mathbf{Alg_{5q}}\) for function class \(\mathcal{F}\), which is an algorithm for online learning with squared loss. We consider the following protocol. At each round \(t\in[T]\), the algorithm produces an estimator \(\widehat{f}_{t}\in\operatorname*{conv}(\mathcal{F})\), then receives a set of context-action-loss tuples \(\{(x_{t},a,\ell_{t,a})\}_{a\in A_{t}}\) where \(A_{t}\subseteq\mathcal{A}\). The goal of the oracle is to accurately predict the loss as a function of the context and action, and we evaluate its performance via the square loss \(\sum_{a\in A_{t}}(\widehat{f}_{t}(x_{t},a)-\ell_{t,a})^{2}\). We measure the oracle's cumulative performance via the following square-loss regret to the best function in \(\mathcal{F}\).

**Assumption 2** (Bounded square-loss regret).: _The regression oracle \(\mathbf{Alg_{5q}}\) guarantees that for any (potentially adaptively chosen) sequence \(\{(x_{t},a,\ell_{t,a})\}_{a\in A_{t},t\in[T]}\) in which \(A_{t}\subseteq\mathcal{A}\),_

\[\sum_{t=1}^{T}\sum_{a\in A_{t}}\Bigl{(}\widehat{f}_{t}(x_{t},a)-\ell_{t,a} \Bigr{)}^{2}-\inf_{f\in\mathcal{F}}\sum_{t=1}^{T}\sum_{a\in A_{t}}\left(f(x_{ t},a)-\ell_{t,a}\right)^{2}\leq\mathbf{Reg_{5q}}.\]

For finite \(\mathcal{F}\), Vovk's aggregation algorithm yields \(\mathbf{Reg_{5q}}=\mathcal{O}(\log\lvert\mathcal{F}\rvert)\)(Vovk, 1995). This regret is dependent upon the scale of the loss function, but this need not be linear with the size of \(A_{t}\), e.g., the loss scale can be bounded by \(2\) in classification problems. See Foster and Krishnamurthy (2021) for additional examples of online regression algorithms.

## 3 Algorithms and Regret Bounds

In this section, we provide our main algorithms and results.

### Algorithms via Minimax Reduction Design

Our approach is to adapt the minimax formulation of (Foster et al., 2021) to contextual bandits with feedback graphs. In the standard contextual bandits setting (that is, \(G_{t}=I\) for all \(t\)), Foster et al. (2021) define the _Decision-Estimation Coefficient_ (DEC) for a parameter \(\gamma>0\) as \(\mathsf{dec}_{\gamma}(\mathcal{F}):=\sup_{\widehat{f}\in\operatorname*{conv}( \mathcal{F}),x\in\mathcal{X}}\mathsf{dec}_{\gamma}(\mathcal{F};\widehat{f},x)\), where

\[\begin{split}\mathsf{dec}_{\gamma}(\mathcal{F};\widehat{f},x)& :=\inf_{p\in\Delta(K)}\mathsf{dec}_{\gamma}(p,\mathcal{F}; \widehat{f},x)\\ &:=\inf_{p\in\Delta(K)}\sup_{\begin{subarray}{c}a^{\star}\in[K] \\ f^{\star}\in\mathcal{F}\end{subarray}}\mathbb{E}_{a\sim p}\biggl{[}f^{\star}(x,a)-f^{\star}(x,a^{\star})-\frac{\gamma}{4}\cdot\Bigl{(}\widehat{f}(x,a)-f^{ \star}(x,a)\Bigr{)}^{2}\biggr{]}.\end{split}\] (1)

Their proposed algorithm is as follows. At each round \(t\), after receiving the context \(x_{t}\), the algorithm first computes \(\widehat{f}_{t}\) by calling the regression oracle. Then, it solves the solution \(p_{t}\) of the minimax problem defined in Eq. (1) with \(\widehat{f}\) and \(x\) replaced by \(\widehat{f}_{t}\) and \(x_{t}\). Finally, the algorithm samples an action \(a_{t}\) from the distribution \(p_{t}\) and feeds the observation \((x_{t},a_{t},\ell_{t,a_{t}})\) to the oracle. Foster et al. (2021) show that for any value \(\gamma\), the algorithm above guarantees that

\[\mathbb{E}[\mathbf{Reg_{CB}}]\leq T\cdot\mathsf{dec}_{\gamma}(\mathcal{F})+ \tfrac{\gamma}{4}\cdot\mathbf{Reg_{5q}}.\] (2)

However, the minimax problem Eq. (1) may not be solved efficiently in many cases. Therefore, instead of obtaining the distribution \(p_{t}\) which has the exact minimax value of Eq. (1), Foster et al.

[2021] show that any distribution that gives an upper bound \(C_{\gamma}\) on \(\mathsf{dec}_{\gamma}(p,\mathcal{F};\widehat{f},x)\) also works and enjoys a regret bound with \(\mathsf{dec}_{\gamma}(\mathcal{F})\) replaced by \(C_{\gamma}\) in Eq.2.

To extend this framework to the setting with feedback graph \(G\), we define \(\mathsf{dec}_{\gamma}(\mathcal{F};\widehat{f},x,G)\) as follows

\[\mathsf{dec}_{\gamma}(\mathcal{F};\widehat{f},x,G)\] \[:=\inf_{p\in\Delta(K)}\mathsf{dec}_{\gamma}(p,\mathcal{F}; \widehat{f},x,G)\] \[:=\inf_{p\in\Delta(K)}\sup_{\begin{subarray}{c}a^{*}\in[K]\\ f^{*}\in\mathcal{F}\end{subarray}}\mathbb{E}_{a\sim p}\Bigg{[}f^{*}(x,a)-f^{*} (x,a^{*})-\frac{\gamma}{4}\mathbb{E}_{A\sim G(\cdot|a)}\left[\sum_{a^{\prime} \in A}(\widehat{f}_{t}(x,a^{\prime})-f^{*}(x,a^{\prime}))^{2}\right]\Bigg{]}.\] (3)

Compared with Eq.1, the difference is that we replace the squared estimation error on action \(a\) by the expected one on the observed set \(A\sim G(\cdot|a)\), which intuitively utilizes more feedbacks from the graph structure. When the feedback graph is the identity matrix, we recover Eq.1. Based on \(\mathsf{dec}_{\gamma}(\mathcal{F};\widehat{f},x,G)\), our algorithm \(\mathsf{SquareCB.G}\) is shown in Algorithm1. As what is done in [Foster et al., 2021], in order to derive an efficient algorithm, instead of solving the distribution \(p_{t}\) with respect to the supremum over \(f^{*}\in\mathcal{F}\), we solve \(p_{t}\) that minimize \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x_{t},G_{t})\) (Eq.4), which takes supremum over \(f^{*}\in(\mathcal{X}\times[K]\mapsto\mathbb{R})\), leading to an upper bound on \(\mathsf{dec}_{\gamma}(\mathcal{F};\widehat{f},x_{t},G_{t})\). Then, we receive the loss \(\{\ell_{t,j}\}_{j\in A_{t}}\) and feed the tuples \(\{(x_{t},j,\ell_{t,j})\}_{j\in A_{t}}\) to the regression oracle \(\mathbf{Alg}_{\mathsf{Sq}}\). Following a similar analysis to [Foster et al., 2021], we show that to bound the regret \(\mathbf{Reg}_{\mathsf{CB}}\), we only need to bound \(\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\).

**Theorem 3.1**.: _Suppose \(\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\leq C \gamma^{-\beta}\) for all \(t\in[T]\) and some \(\beta>0\), Algorithm1 with \(\gamma=\max\{4,(CT)^{\frac{1}{\beta+1}}\mathbf{Reg}_{\mathsf{Sq}}^{-\frac{1}{ \beta+1}}\}\) guarantees that \(\mathbb{E}\left[\mathbf{Reg}_{\mathsf{CB}}\right]\leq\mathcal{O}\bigg{(}C^{ \frac{1}{\beta+1}}T^{\frac{1}{\beta+1}}\mathbf{Reg}_{\mathsf{Sq}}^{\frac{ \beta}{\beta+1}}\bigg{)}\)._

The proof is deferred to AppendixA. In Section3.3, we give an efficient implementation for solving Eq.4 via reduction to convex programming.

### Regret Bounds

In this section, we derive regret bounds for Algorithm1 when \(G_{t}\)'s are specialized to deterministic graphs, i.e., \(G_{t}\in\{0,1\}^{\mathcal{A}\times\mathcal{A}}\). We utilize discrete graph notation \(G=([K],E)\), where \(E\subseteq[K]\times[K]\); and define \(N^{\mathrm{in}}(G,j)\triangleq\{i\in\mathcal{A}:(i,j)\in E\}\) as the set of nodes that can observe node \(j\). In this case, at each round \(t\), the observed node set \(A_{t}\) is a deterministic set which contains any node \(i\) satisfying \(a_{t}\in N^{\mathrm{in}}(G_{t},i)\). In the following, we introduce several graph-theoretic concepts for deterministic feedback graphs [Alon et al., 2015].

Strongly/Weakly Observable Graphs.For a directed graph \(G=([K],E)\), a node \(i\) is observable if \(N^{\mathrm{in}}(G,i)\neq\emptyset\). An observable node is strongly observable if either \(i\in N^{\mathrm{in}}(G,i)\) or \(N^{\mathrm{in}}(G,i)=[K]\backslash\{i\}\), and weakly observable otherwise. Similarly, a graph is observable if all its nodes are observable. An observable graph is strongly observable if all nodes are strongly observable, and weakly observable otherwise. Self-aware graphs are a special type of strongly observable graphs where \(i\in N^{\mathrm{in}}(G,i)\) for all \(i\in[K]\).

Independent Set and Weakly Dominating Set.An independence set of a directed graph is a subset of nodes in which no two distinct nodes are connected. The size of the largest independence set of a graph is called its independence number. For a weakly observable graph \(G=([K],E)\), a weakly dominating set is a subset of nodes \(D\subseteq[K]\) such that for any node \(j\) in \(G\) without a self-loop, there exists \(i\in D\) such that directed edge \((i,j)\in E\). The size of the smallest weakly dominating set of a graph is called its weak domination number. Alon et al. (2015) show that in non-contextual bandits with a fixed feedback graph \(G\), the minimax regret bound is \(\widetilde{\Theta}(\sqrt{\alpha T})\) when \(G\) is strongly observable and \(\widetilde{\Theta}(d^{\frac{1}{3}}T^{\frac{2}{3}})\) when \(G\) is weakly observable, where \(\alpha\) and \(d\) are the independence number and the weak domination number of \(G\), respectively.

#### 3.2.1 Strongly Observable Graphs

In the following theorem, we show the regret bound of Algorithm 1 for strongly observable graphs.

**Theorem 3.2** (Strongly observable graphs).: _Suppose that the feedback graph \(G_{t}\) is deterministic and strongly observable with independence number no more than \(\alpha\). Then Algorithm 1 guarantees that_

\[\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\leq \mathcal{O}\bigg{(}\frac{\alpha\log(K\gamma)}{\gamma}\bigg{)}.\]

In contrast to existing works that derive a closed-form solution of \(p_{t}\) in order to show how large the DEC can be (Foster and Rakhlin, 2020; Foster and Krishnamurthy, 2021), in our case we prove the upper bound of \(\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\) by using the Sion's minimax theorem and the graph-theoretic lemma proven in (Alon et al., 2015). The proof is deferred to Appendix A.1. Combining Theorem 3.2 and Theorem 3.1, we directly have the following corollary:

**Corollary 3.3**.: _Suppose that \(G_{t}\) is deterministic, strongly observable, and has independence number no more than \(\alpha\) for all \(t\in[T]\). Algorithm 1 with choice \(\gamma=\max\left\{4,\sqrt{\alpha T/\mathbf{Reg}_{\mathsf{Sq}}}\right\}\) guarantees that_

\[\mathbb{E}[\mathbf{Reg}_{\mathsf{CB}}]\leq\widetilde{\mathcal{O}}\Big{(}\sqrt {\alpha T\mathbf{Reg}_{\mathsf{Sq}}}\Big{)}.\]

For conciseness, we show in Corollary 3.3 that the regret guarantee for Algorithm 1 depends on the largest independence number of \(G_{t}\) over \(t\in[T]\). However, we in fact are able to achieve a move adaptive regret bound of order \(\widetilde{\mathcal{O}}\bigg{(}\sqrt{\sum_{t=1}^{T}\alpha_{t}\mathbf{Reg}_{ \mathsf{Sq}}}\bigg{)}\) where \(\alpha_{t}\) is the independence number of \(G_{t}\).

It is straightforward to achieve this by applying a standard doubling trick on the quantity \(\sum_{t=1}^{T}\alpha_{t}\), assuming we can compute \(\alpha_{t}\) given \(G_{t}\), but we take one step further and show that it is in fact unnecessary to compute \(\alpha_{t}\) (which, after all, is NP-hard (Karp, 1972)): we provide an adaptive tuning strategy for \(\gamma\) by keeping track the the cumulative value of the quantity \(\min_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f}_{t},x_{t},G_{t})\) and show that this efficient method also achieves the adaptive \(\widetilde{\mathcal{O}}\bigg{(}\sqrt{\sum_{t=1}^{t}\alpha_{t}\mathbf{Reg}_{ \mathsf{Sq}}}\bigg{)}\) regret guarantee; see Appendix D for details.

#### 3.2.2 Weakly Observable Graphs

For the weakly observable graph, we have the following theorem.

**Theorem 3.4** (Weakly observable graphs).: _Suppose that the feedback graph \(G_{t}\) is deterministic and weakly observable with weak domination number no more than \(d\). Then Algorithm 1 with \(\gamma\geq 16d\) guarantees that_

\[\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\leq \mathcal{O}\left(\sqrt{\frac{d}{\gamma}}+\frac{\widetilde{\alpha}\log(K\gamma)}{ \gamma}\right),\]_where \(\widetilde{\alpha}\) is the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\)._

The proof is deferred to Appendix A.2. Similar to Theorem 3.2, we do not derive a closed-form solution to the strategy \(p_{t}\) but prove this upper bound using the minimax theorem. Combining Theorem 3.4 and Theorem 3.1, we are able to obtain the following regret bound for weakly observable graphs, whose proof is deferred to Appendix A.2.

**Corollary 3.5**.: _Suppose that \(G_{t}\) is deterministic, weakly observable, and has weak domination number no more than \(d\) for all \(t\in[T]\). In addition, suppose that the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\) is no more than \(\widetilde{\alpha}\) for all \(t\in[T]\). Then, Algorithm 1 with \(\gamma=\max\{16d,\sqrt{\widetilde{\alpha}T/\mathbf{Reg}_{\mathsf{Sq}}},d^{ \frac{1}{3}}T^{\frac{2}{3}}\mathbf{Reg}_{\mathsf{Sq}}^{-\frac{2}{3}}\}\) guarantees that_

\[\mathbb{E}[\mathbf{Reg}_{\mathsf{CB}}]\leq\widetilde{\mathcal{O}}\left(d^{ \frac{1}{3}}T^{\frac{2}{3}}\mathbf{Reg}_{\mathsf{Sq}}^{\frac{1}{3}}+\sqrt{ \widetilde{\alpha}T\mathbf{Reg}_{\mathsf{Sq}}}\right).\]

Similarly to the strongly observable graph case, we also derive an adaptive tuning strategy for \(\gamma\) to achieve a more refined regret bound \(\widetilde{\mathcal{O}}\left(\sqrt{\sum_{t=1}^{T}\widetilde{\alpha}_{t} \mathbf{Reg}_{\mathsf{Sq}}}+\left(\sum_{t=1}^{T}\sqrt{d_{t}}\right)^{\frac{2} {3}}\mathbf{Reg}_{\mathsf{Sq}}^{\frac{1}{3}}\right)\) where \(\widetilde{\alpha}_{t}\) is the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\) and \(d_{t}\) is the weakly domination number of \(G_{t}\). This is again achieved _without_ explicitly computing \(\widetilde{\alpha}_{t}\) and \(d_{t}\); see Appendix D for details.

### Implementation

In this section, we show that solving \(\operatorname*{argmin}_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p; \widehat{f},x,G)\) in Algorithm 1 is equivalent to solving a convex program, which can be easily and efficiently implemented in practice.

**Theorem 3.6**.: _Solving \(\operatorname*{argmin}_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p; \widehat{f},x,G)\) is equivalent to solving the following convex optimization problem._

\[\min_{p\in\Delta(K),z} p^{\top}\widehat{f}+z\] (5) \[\operatorname{subject\;to} \forall a\in[K]:\frac{1}{\gamma}\|p-e_{a}\|_{\operatorname{diag} (G^{\top}p)^{-1}}^{2}\leq\widehat{f}(x,a)+z,\] \[G^{\top}p\succ 0,\]

_where \(\widehat{f}\) in the objective is a shorthand for \(\widehat{f}(x,\cdot)\in\mathbb{R}^{K}\), \(e_{a}\) is the \(a\)-th standard basis vector, and \(\succ\) means element-wise greater._

The proof is deferred to Appendix A.4. Note that this implementation is not restricted to the deterministic feedback graphs but applies to the general stochastic feedback graph case. In Appendix A.3, we provide the \(20\) lines of Python code that solves Eq. (5).

## 4 Examples with Closed-Form Solutions

In this section, we present examples and corresponding closed-form solutions of \(p\) that make the value \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) upper bounded by at most a constant factor of \(\min_{p}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\). This offers an alternative to solving the convex program defined in Theorem 3.6 for special (and practically relevant) cases, thereby enhancing the efficiency of our algorithm. All the proofs are deferred to Appendix B.

Cops-and-Robbers Graph.The "cops-and-robbers" feedback graph \(G_{\mathrm{CR}}=11^{\top}-I\), also known as the loopless clique, is the full feedback graph removing self-loops. Therefore, \(G_{\mathrm{CR}}\) is strongly observable with independence number \(\alpha=1\). Let \(a_{1}\) be the node with the smallest value of \(\widehat{f}\) and \(a_{2}\) be the node with the second smallest value of \(\widehat{f}\). Our proposed closed-form distribution \(p\) is only supported on \(\{a_{1},a_{2}\}\) and defined as follows:

\[p_{a_{1}}=1-\frac{1}{2+\gamma(\widehat{f}_{a_{2}}-\widehat{f}_{a_{1}})},\quad p _{a_{2}}=\frac{1}{2+\gamma(\widehat{f}_{a_{2}}-\widehat{f}_{a_{1}})}.\] (6)In the following proposition, we show that with the construction of \(p\) in Eq. (6), \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{CR}})\) is upper bounded by \(\mathcal{O}(\nicefrac{{1}}{{\gamma}})\), which matches the order of \(\min_{p}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) based on Theorem 3.2 since \(\alpha=1\).

**Proposition 1**.: _When \(G=G_{\mathrm{CR}}\), given any \(\widehat{f}\), context \(x\), the closed-form distribution \(p\) in Eq. (6) guarantees that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{CR}})\leq\mathcal{ O}\Big{(}\frac{1}{\gamma}\Big{)}\)._

Apple Tasting Graph.The apple tasting feedback graph \(G_{\mathrm{AT}}=\begin{bmatrix}1&1\\ 0&0\end{bmatrix}\) consists of two nodes, where the first node reveals all and the second node reveals nothing. This scenario was originally proposed by Helmbold et al. (2000) and recently denoted the spam filtering graph (van der Hoeven et al., 2021). The independence number of \(G_{\mathrm{AT}}\) is \(1\). Let \(\widehat{f}_{1}\) be the oracle prediction for the first node and let \(\widehat{f}_{2}\) be the prediction for the second node. We present a closed-form solution \(p\) for Eq. (4) as follows:

\[p_{1}=\begin{cases}1&\widehat{f}_{1}\leq\widehat{f}_{2}\\ \frac{2}{4+\gamma(\widehat{f}_{1}-\widehat{f}_{2})}&\widehat{f}_{1}>\widehat{f }_{2}\end{cases},\qquad p_{2}=1-p_{1}.\] (7)

We show that this distribution \(p\) satisfies that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{AT}})\) is upper bounded by \(\mathcal{O}(\nicefrac{{1}}{{\gamma}})\) in the following proposition. We remark that directly applying results from (Foster et al., 2021) cannot lead to a valid upper bound since the second node does not have a self-loop.

**Proposition 2**.: _When \(G=G_{\mathrm{AT}}\), given any \(\widehat{f}\), context \(x\), the closed-form distribution \(p\) in Eq. (7) guarantees that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{AT}})\leq\mathcal{ O}(\frac{1}{\gamma})\)._

Inventory Graph.In this application, the algorithm needs to decide the inventory level in order to fulfill the realized demand arriving at each round. Specifically, there are \(K\) possible chosen inventory levels \(a_{1}<a_{2}<\ldots<a_{K}\) and the feedback graph \(G_{\mathrm{inv}}\) has entries \(G(i,j)=1\) for all \(1\leq j\leq i\leq K\) and \(G(i,j)=0\) otherwise, meaning that picking the inventory level \(a_{i}\) informs about all actions \(a_{j\leq i}\). This is because items are consumed until either the demand or the inventory is exhausted. The independence number of \(G_{\mathrm{inv}}\) is \(1\). Therefore, (very) large \(K\) is statistically tractable, but naively solving the convex program Eq. (5) requires superlinear in \(K\) computational cost. We show in the following proposition that there exists an analytic form of \(p\), which guarantees that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{inv}})\) can be bounded by \(\mathcal{O}(\nicefrac{{1}}{{\gamma}})\).

**Proposition 3**.: _When \(G=G_{\mathrm{inv}}\), given any \(\widehat{f}\), context \(x\), there exists a closed-form distribution \(p\in\Delta(K)\) guaranteeing that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{inv}})\leq\mathcal{ O}(\frac{1}{\gamma})\), where \(p\) is defined as follows: \(p_{j}=\max\{\frac{1}{1+\gamma(\widehat{f}_{j}-\min_{i}\widehat{f}_{i})}-\sum_{j ^{\prime}>j}p_{j^{\prime}},0\}\) for all \(j\in[K]\)._

Undirected Self-Aware Graph.For the undirected and self-aware feedback graph \(G\), which means that \(G\) is symmetric and has diagonal entries all \(1\), we also show that a certain closed-form solution of \(p\) satisfies that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) is bounded by \(\mathcal{O}(\frac{\alpha}{\gamma})\).

**Proposition 4**.: _When \(G\) is an undirected self-aware graph, given any \(\widehat{f}\), context \(x\), there exists a closed-form distribution \(p\in\Delta(K)\) guaranteeing that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\leq\mathcal{O}\Big{(}\frac {\alpha}{\gamma}\Big{)}\)._

## 5 Experiments

In this section, we use empirical results to demonstrate the significant benefits of SquareCB.G in leveraging the graph feedback structure and its superior effectiveness compared to SquareCB. Following Foster and Krishnamurthy (2021), we use progressive validation (PV) loss as the evaluation metric, defined as \(L_{\mathrm{pv}}(T)=\frac{1}{T}\sum_{t=1}^{T}\ell_{t,a_{t}}\). All the feedback graphs used in the experiments are deterministic. We run experiments on CPU Intel Xeon Gold 6240R 2.4G and the convex program solver is implemented via Vowpal Wabbitt (Langford et al., 2007).

### SquareCB.G under Different Feedback Graphs

In this subsection, we show that our SquareCB.G benefits from considering the graph structure by evaluating the performance of SquareCB.G under three different feedback graphs. We conduct experiments on RCV1 dataset and leave the implementation details in Appendix C.1.

The performances of SquareCB.G under bandit graph, full information graph and cops-and-robbers graph are shown in the left part of Figure 1. We observe that SquareCB.G performs the best under full information graph and performs worst under bandit graph. Under the cops-and-robbers graph, much of the gap between bandit and full information is eliminated. This improvement demonstrates the benefit of utilizing graph feedback for accelerating learning.

### Comparison between SquareCB.G and SquareCB

In this subsection, we compare the effectiveness of SquareCB.G with the SquareCB algorithm. To ensure a fair comparison, both algorithms update the regressor using the same feedbacks based on the graph. The only distinction lies in how they calculate the action probability distribution. We summarize the main results here and leave the implementation details in Appendix C.2.

#### 5.2.1 Results on Random Directed Self-aware Graphs

We conduct experiments on RCV1 dataset using random directed self-aware feedback graphs. Specifically, at round \(t\), the deterministic feedback graph \(G_{t}\) is generated as follows. The diagonal elements of \(G_{t}\) are all \(1\), and each off-diagonal entry is drawn from a Bernoulli\((\nicefrac{{3}}{{4}})\) distribution. The results are presented in the right part of Figure 1. Our SquareCB.G consistently outperforms SquareCB and demonstrates lower variance, particularly when the number of iterations was small. This is because when there are fewer samples available to train the regressor, it is more crucial to design an effective algorithm that can leverage the graph feedback information.

#### 5.2.2 Results on Synthetic Inventory Dataset

In the inventory graph experiments, we create a synthetic inventory dataset and design a loss function for each inventory level \(a_{t}\in[0,1]\) with demand \(d_{t}\in[0,1]\). Since the action set \([0,1]\) is continuous, we discretize the action set in two different ways to apply the algorithms.

Fixed discretized action set.In this setting, we discretize the action set using fixed grid size \(\varepsilon\in\{\frac{1}{100},\frac{1}{300},\frac{1}{500}\}\), which leads to a finite action set \(\mathcal{A}\) of size \(\frac{1}{\varepsilon}+1\). Note that according to Theorem 3.2, our regret _does not_ scale with the size of the action set (to within polylog factors), as the independence number is always \(1\). The results are shown in the left part of Figure 2.

Figure 1: **Left figure**: Performance of SquareCB.G on RCV1 dataset under three different feedback graphs. **Right figure**: Performance comparison between SquareCB.G and SquareCB under random directed self-aware feedback graphs.

We remark several observations from the results. First, our algorithm SquareCB.G outperforms SquareCB for all choices \(K\in\{101,301,501\}\). This indicates that SquareCB.G utilizes a better exploration scheme and effectively leverages the structure of \(G_{\mathrm{inv}}\). Second, we observe that SquareCB.G indeed does not scale with the size of the discretized action set \(\mathcal{A}\), since under different discretization scales, SquareCB.G has similar performances and the slight differences are from the improved approximation error with finer discretization. This matches the theoretical guarantee that we prove in Theorem3.2. On the other hand, SquareCB does perform worse when the size of the action set increases, matching its theoretical guarantee which scales with the square root of the size of the action set.

Adaptively changing action set.In this setting, we adaptively discretize the action set \([0,1]\) according to the index of the current round. Specifically, for SquareCB.G, we uniformly discretize the action set \([0,1]\) with size \(\lceil\sqrt{t}\rceil\), whose total discretization error is \(\mathcal{O}(\sqrt{T})\) due to the Lipschitzness of the loss function. For SquareCB, to optimally balance the dependency on the size of the action set and the discretization error, we uniformly discretize the action set \([0,1]\) into \(\lceil t^{\frac{1}{3}}\rceil\) actions. The results are illustrated in the right part of Figure2. We can observe that SquareCB.G consistently outperforms SquareCB by a clear margin.

## 6 Related Work

Multi-armed bandits with feedback graphs have been extensively studied. An early example is the apple tasting problem of Helmbold et al. (2000). The general formulation was introduced by Mannor and Shamir (2011). Alon et al. (2015) characterized the minimax rates in terms of graph-theoretic quantities. Follow-on work includes relaxing the assumption that the graph is observed prior to decision (Cohen et al., 2016); analyzing the distinction between the stochastic and adversarial settings (Alon et al., 2017); considering stochastic feedback graphs (Li et al., 2020; Esposito et al., 2022); instance-adaptivity (Ito et al., 2022); data-dependent regret bound (Lykouris et al., 2018; Lee et al., 2020); and high-probability regret under adaptive adversary (Neu, 2015; Luo et al., 2023).

The contextual bandit problem with feedback graphs has received relatively less attention. Wang et al. (2021) provide algorithms for adversarial linear bandits with uninformed graphs and stochastic contexts. However, this work assumes several unrealistic assumptions on both the policy class and the context space and is not comparable to our setting, since we consider the informed graph setting with adversarial context. Singh et al. (2020) study a stochastic linear bandits with informed feedback graphs and are able to improve over the instance-optimal regret bound for bandits derived in (Lattimore and Szepesvari, 2017) by utilizing the additional graph-based feedbacks.

Figure 2: Performance comparison between SquareCB.G and SquareCB on synthetic inventory dataset. **Left figure**: Results under fixed discretized action set. **Right figure**: Results under adaptive discretization of the action set. Both figures show the superiority of SquareCB.G compared with SquareCB.

Our work is also closely related to the recent progress in designing efficient algorithms for classic contextual bandits. Starting from (Langford and Zhang, 2007), numerous works have been done to the development of practically efficient algorithms, which are based on reduction to either cost-sensitive classification oracles (Dudik et al., 2011; Agarwal et al., 2014) or online regression oracles (Foster and Rakhlin, 2020; Foster et al., 2020, 2021; Zhu and Mineiro, 2022). Following the latter trend, our work assumes access to an online regression oracle and extends the classic bandit problems to the bandits with general feedback graphs.

## 7 Discussion

In this paper, we consider the design of practical contextual bandits algorithm with provable guarantees. Specifically, we propose the first efficient algorithm that achieves near-optimal regret bound for contextual bandits with general directed feedback graphs with an online regression oracle.

While we study the informed graph feedback setting, where the entire feedback graph is exposed to the algorithm prior to each decision, many practical problems of interest are possibly uninformed graph feedback problems, where the graph is unknown at the decision time. It is unclear how to formulate an analogous minimax problem to Eq. (1) under the uninformed setting. One idea is to consume the additional feedback in the online regressor and adjust the prediction loss to reflect this additional structure, e.g., using the more general version of the E2D framework which incorporates arbitrary side observations (Foster et al., 2021). Cohen et al. (2016) consider this uninformed setting in the non-contextual case and prove a sharp distinction between the adversarial and stochastic settings: even if the graphs are all strongly observable with bounded independence number, in the adversarial setting the minimax regret is \(\Theta(T)\) whereas in the stochastic setting the minimax regret is \(\Theta(\sqrt{\alpha T})\). Intriguingly, our setting is semi-adversarial due to realizability of the mean loss, and therefore it is apriori unclear whether the negative adversarial result applies.

In addition, bandits with graph feedback problems often present with associated policy constraints, e.g., for the apple tasting problem, it is natural to rate limit the informative action. Therefore, another interesting direction is to combine our algorithm with the recent progress in contextual bandits with knapsack (Slivkins and Foster, 2022), leading to more practical algorithms.

AcknowledgmentsHL and MZ are supported by NSF Awards IIS-1943607.

## References

* Agarwal et al. (2012) Alekh Agarwal, Miroslav Dudik, Satyen Kale, John Langford, and Robert Schapire. Contextual bandit learning with predictable rewards. In _Artificial Intelligence and Statistics_, pages 19-26. PMLR, 2012.
* Agarwal et al. (2014) Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_, pages 1638-1646. PMLR, 2014.
* Alon et al. (2013) Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, and Yishay Mansour. From bandits to experts: A tale of domination and independence. _Advances in Neural Information Processing Systems_, 26, 2013.
* Alon et al. (2015) Noga Alon, Nicolo Cesa-Bianchi, Ofer Dekel, and Tomer Koren. Online learning with feedback graphs: Beyond bandits. In _Conference on Learning Theory_, pages 23-35. PMLR, 2015.
* Alon et al. (2017) Noga Alon, Nicolo Cesa-Bianchi, Claudio Gentile, Shie Mannor, Yishay Mansour, and Ohad Shamir. Nonstochastic multi-armed bandits with graph-structured feedback. _SIAM Journal on Computing_, 46(6):1785-1826, 2017.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* Bouneffouf et al. (2020) Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and contextual bandits. In _2020 IEEE Congress on Evolutionary Computation (CEC)_, pages 1-8. IEEE, 2020.
* Bouneffouf et al. (2016)Alon Cohen, Tamir Hazan, and Tomer Koren. Online learning with feedback graphs without the graphs. In _International Conference on Machine Learning_, pages 811-819. PMLR, 2016.
* Dudik et al. (2011) Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. In _Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence_, pages 169-178, 2011.
* Esposito et al. (2022) Emmanuel Esposito, Federico Fusco, Dirk van der Hoeven, and Nicolo Cesa-Bianchi. Learning on the edge: Online learning with stochastic feedback graphs. In _Advances in Neural Information Processing Systems_, 2022.
* Foster and Rakhlin (2020) Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 3199-3210. PMLR, 2020.
* Foster et al. (2018) Dylan Foster, Alekh Agarwal, Miroslav Dudik, Haipeng Luo, and Robert Schapire. Practical contextual bandits with regression oracles. In _International Conference on Machine Learning_, pages 1539-1548. PMLR, 2018.
* Foster and Krishnamurthy (2021) Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination. _Advances in Neural Information Processing Systems_, 34:18907-18919, 2021.
* Foster et al. (2020) Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. _Advances in Neural Information Processing Systems_, 33:11478-11489, 2020.
* Foster et al. (2021) Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* Helmbold et al. (2000) David P Helmbold, Nicholas Littlestone, and Philip M Long. Apple tasting. _Information and Computation_, 161(2):85-139, 2000.
* Ito et al. (2022) Shinji Ito, Taira Tsuchiya, and Junya Honda. Nearly optimal best-of-both-worlds algorithms for online learning with feedback graphs. In _Advances in Neural Information Processing Systems_, 2022.
* Karp (1972) Richard M Karp. Reducibility among combinatorial problems. page 85-103, 1972.
* Langford and Zhang (2007) John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. _Advances in neural information processing systems_, 20(1):96-1, 2007.
* Langford et al. (2007) John Langford, Lihong Li, and Alex Strehl. Vowpal wabbit online learning project, 2007.
* Lattimore and Szepesvari (2017) Tor Lattimore and Csaba Szepesvari. The end of optimism? an asymptotic analysis of finite-armed linear bandits. In _Artificial Intelligence and Statistics_, pages 728-737. PMLR, 2017.
* Lee et al. (2020) Chung-Wei Lee, Haipeng Luo, and Mengxiao Zhang. A closer look at small-loss bounds for bandits with graph feedback. In _Conference on Learning Theory_, pages 2516-2564. PMLR, 2020.
* Lewis et al. (2004) David D Lewis, Yiming Yang, Tony Russell-Rose, and Fan Li. Rcv1: A new benchmark collection for text categorization research. _Journal of machine learning research_, 5(Apr):361-397, 2004.
* Li et al. (2020) Shuai Li, Wei Chen, Zheng Wen, and Kwong-Sak Leung. Stochastic online learning with probabilistic graph feedback. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4675-4682, 2020.
* Luo et al. (2023) Haipeng Luo, Hanghang Tong, Mengxiao Zhang, and Yuheng Zhang. Improved high-probability regret for adversarial bandits with time-varying feedback graphs. In _International Conference on Algorithmic Learning Theory_, pages 1074-1100. PMLR, 2023.
* Lykouris et al. (2018) Thodoris Lykouris, Karthik Sridharan, and Eva Tardos. Small-loss bounds for online learning with partial information. In _Conference on Learning Theory_, pages 979-986. PMLR, 2018.
* Mannor and Shamir (2011) Shie Mannor and Ohad Shamir. From bandits to experts: On the value of side-observations. _Advances in Neural Information Processing Systems_, 24, 2011.
* Mnih et al. (2015)* Neu (2015) Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. _Advances in Neural Information Processing Systems_, 28, 2015.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Rucker et al. (2022) Mark Rucker, Joran T Ash, John Langford, Paul Mineiro, and Ida Momennejad. Eigen memory tree. _arXiv preprint arXiv:2210.14077_, 2022.
* Simchi-Levi and Xu (2021) David Simchi-Levi and Yunzong Xu. Bypassing the monster: A faster and simpler optimal algorithm for contextual bandits under realizability. _Mathematics of Operations Research_, 2021.
* Singh et al. (2020) Rahul Singh, Fang Liu, Xin Liu, and Ness Shroff. Contextual bandits with side-observations. _arXiv preprint arXiv:2006.03951_, 2020.
* Slivkins and Foster (2022) Aleksandrs Slivkins and Dylan Foster. Efficient contextual bandits with knapsacks via regression. _arXiv preprint arXiv:2211.07484_, 2022.
* van der Hoeven et al. (2021) Dirk van der Hoeven, Federico Fusco, and Nicolo Cesa-Bianchi. Beyond bandit feedback in online multiclass classification. _Advances in Neural Information Processing Systems_, 34:13280-13291, 2021.
* Vovk (1995) Vladimir G Vovk. A game of prediction with expert advice. In _Proceedings of the eighth annual conference on Computational learning theory_, pages 51-60, 1995.
* Wang et al. (2021) Lingda Wang, Bingcong Li, Huozhi Zhou, Georgios B Giannakis, Lav R Varshney, and Zhizhen Zhao. Adversarial linear contextual bandits with graph-structured side observations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 10156-10164, 2021.
* Zhu and Mineiro (2022) Yinglun Zhu and Paul Mineiro. Contextual bandits with smooth regret: Efficient learning in continuous action spaces. In _International Conference on Machine Learning_, pages 27574-27590. PMLR, 2022.

## Appendix A Omitted Details in Section 3

**Theorem 3.1**.: _Suppose \(\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\leq C\gamma^{-\beta}\) for all \(t\in[T]\) and some \(\beta>0\), Algorithm 1 with \(\gamma=\max\{4,(CT)^{\frac{1}{\beta+1}}\mathbf{Reg}_{\mathsf{5q}}^{-\frac{1}{ \beta+1}}\}\) guarantees that \(\mathbb{E}\left[\mathbf{Reg}_{\mathsf{CB}}\right]\leq\mathcal{O}\bigg{(}C^{ \frac{1}{\beta+1}}T^{\frac{1}{\beta+1}}\mathbf{Reg}_{\mathsf{5q}}^{\frac{ \beta}{\beta+1}}\bigg{)}\)._

Proof.: Following (Foster et al., 2020), we decompose \(\mathbf{Reg}_{\mathsf{CB}}\) as follows:

\[\mathbb{E}[\mathbf{Reg}_{\mathsf{CB}}]\] \[=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}f^{\star}(x_{t},a_{t})-\sum_{t=1 }^{T}f^{\star}(x_{t},\pi^{\star}(x_{t}))\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\biggl{(}f^{\star}(x_{t},a_{t})- f^{\star}(x_{t},\pi^{\star}(x_{t}))-\frac{\gamma}{4}\mathbb{E}_{A\sim G_{t}( \cdot|a_{t})}\biggl{[}\sum_{a\in A}\Bigl{(}\widehat{f}_{t}(x_{t},a)-f^{\star}( x_{t},a)\Bigr{)}^{2}\biggr{]}\biggr{)}\Bigg{]}\] \[\leq\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\max_{\begin{subarray}{c}a^ {\prime}\in[K]\\ f(\mathcal{X}\times[K]\mapsto\mathbb{R})\end{subarray}}\mathbb{E}_{a_{t}\sim p _{t}}\Bigg{[}f(x_{t},a_{t})-f(x_{t},a^{\star})-\frac{\gamma}{4}\mathbb{E}_{A \sim G_{t}(\cdot|a_{t})}\biggl{[}\sum_{a\in A}\Bigl{(}\widehat{f}_{t}(x_{t},a) -f(x_{t},a)\Bigr{)}^{2}\biggr{]}\Bigg{]}\Bigg{]}\] \[\qquad+\frac{\gamma}{4}\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\mathbb{E }_{A\sim G_{t}(\cdot|a_{t})}\Biggl{[}\sum_{a\in A}\Bigl{(}\widehat{f}_{t}(x_{t },a)-f^{\star}(x_{t},a)\Bigr{)}^{2}\Biggr{]}\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\overline{\mathsf{dec}}_{\gamma} (p_{t};\widehat{f}_{t},x_{t},G_{t})\Bigg{]}+\frac{\gamma}{4}\mathbb{E}\Bigg{[} \sum_{t=1}^{T}\mathbb{E}_{A\sim G_{t}(\cdot|a_{t})}\biggl{[}\sum_{a\in A} \Bigl{(}\widehat{f}_{t}(x_{t},a)-f^{\star}(x_{t},a)\Bigr{)}^{2}\biggr{]}\Bigg{]}\] (8) \[\leq CT\gamma^{-\beta}+\frac{\gamma}{4}\mathbb{E}\Bigg{[}\sum_{t= 1}^{T}\mathbb{E}_{A\sim G_{t}(\cdot|a_{t})}\Biggl{[}\sum_{a\in A}\Bigl{(} \widehat{f}_{t}(x_{t},a)-f^{\star}(x_{t},a)\Bigr{)}^{2}\Biggr{]}\Bigg{]}.\]

Next, since \(\mathbb{E}[\ell_{t,a}\mid x_{t}]=f^{\star}(x_{t},a)\) for all \(t\in[T]\) and \(a\in\mathcal{A}\), we know that

\[\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\mathbb{E}_{A\sim G_{t}(\cdot|a_{ t})}\Biggl{[}\sum_{a\in A}\Bigl{(}\widehat{f}_{t}(x_{t},a)-f^{\star}(x_{t},a) \Bigr{)}^{2}\Biggr{]}\Bigg{]}\] \[=\mathbb{E}\Bigg{[}\sum_{t=1}^{T}\mathbb{E}_{A\sim G_{t}(\cdot|a_{ t})}\Biggl{[}\sum_{a\in A}\left(\widehat{f}_{t}(x_{t},a)-\ell_{t,a}\right)^{2}- \sum_{a\in A}\left(f^{\star}(x_{t},a)-\ell_{t,a}\right)^{2}\Biggr{]}\Bigg{]} \leq\mathbf{Reg}_{\mathsf{5q}},\] (9)

where the final inequality is due to Assumption 2.

Therefore, we have

\[\mathbb{E}[\mathbf{Reg}_{\mathsf{CB}}]\leq CT\gamma^{-\beta}+\frac{\gamma}{4} \mathbf{Reg}_{\mathsf{5q}}.\]

Picking \(\gamma=\max\left\{4,\left(\frac{CT}{\mathbf{Reg}_{\mathsf{5q}}}\right)^{\frac{ 1}{\beta+1}}\right\}\), we obtain that

\[\mathbb{E}\left[\mathbf{Reg}_{\mathsf{CB}}\right]\leq\mathcal{O} \left(C^{\frac{1}{\beta+1}}T^{\frac{1}{\beta+1}}\mathbf{Reg}_{\mathsf{5q}}^{ \frac{\beta}{\beta+1}}\right).\]

### Proof of Theorem 3.2

Before proving Theorem 3.2, we first show the following key lemma, which is useful in proving that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) is convex for both strongly and weakly observable feedback graphs \(G\). Wehighlight that the convexity of \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) is crucial for both proving the upper bound of \(\min_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) and showing the efficiency of Algorithm 1.

**Lemma A.1**.: _Suppose \(u,v,x\in\mathbb{R}^{d}\) with \(\langle v,x\rangle>0\). Then both \(g(x)=\frac{\langle u,x\rangle^{2}}{\langle v,x\rangle}\) and \(h(x)=\frac{(1-\langle u,x\rangle)^{2}}{\langle v,x\rangle}\) are convex in \(x\)._

Proof.: The function \(f(x,y)=x^{2}/y\) is convex for \(y>0\) due to

\[\nabla^{2}f(x,y)=\frac{2}{y^{3}}\bigg{[}\begin{array}{c}y\\ -x\end{array}\bigg{]}\bigg{[}\begin{array}{c}y\\ -x\end{array}\bigg{]}^{\top}\succeq 0.\]

By composition with affine functions, both \(g(x)=f(\langle u,x\rangle\,,\langle v,x\rangle)\) and \(h(x)=f(1-\langle u,x\rangle\,,\langle v,x\rangle)\) are convex. 

**Theorem 3.2** (Strongly observable graphs).: _Suppose that the feedback graph \(G_{t}\) is deterministic and strongly observable with independence number no more than \(\alpha\). Then Algorithm 1 guarantees that_

\[\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\leq \mathcal{O}\bigg{(}\frac{\alpha\log(K\gamma)}{\gamma}\bigg{)}.\]

Proof.: For conciseness, we omit the subscript \(t\). Direct calculation shows that for all \(a^{\star}\in[K]\),

\[\mathbb{E}_{a\sim p}\left[f^{\star}(x,a)-f^{\star}(x,a^{\star})- \frac{\gamma}{4}\sum_{a^{\prime}\in N^{\mathrm{in}}(G,a)}(\widehat{f}(x,a^{ \prime})-f^{\star}(x,a^{\prime}))^{2}\right]\] \[=\sum_{a=1}^{K}p_{a}f^{\star}(x,a)-f^{\star}(x,a^{\star})-\frac{ \gamma}{4}\sum_{a=1}^{K}W_{a}\left(\widehat{f}(x,a)-f^{\star}(x,a)\right)^{2},\]

where \(W_{a}=\sum_{a^{\prime}\in N^{\mathrm{in}}(G,a)}p_{a^{\prime}}\). Therefore, taking the gradient over \(f^{\star}(x,\cdot)\) and we know that

\[\sup_{f^{\star}\in(\mathcal{X}\times[K]\to\mathbb{R})}\left[\sum_{a=1}^{K}p_{ a}f^{\star}(x,a)-f^{\star}(x,a^{\star})-\frac{\gamma}{4}\sum_{a=1}^{K}W_{a} \left(\widehat{f}(x,a)-f^{\star}(x,a)\right)^{2}\right]\] \[=\sum_{a=1}^{K}p_{a}\widehat{f}(x,a)-\widehat{f}(x,a^{\star})+ \frac{1}{\gamma}\|p-e_{a^{\star}}\|_{\mathrm{diag}(W)^{-1}}^{2}.\]

Then, denote \(\widehat{f}\in\mathbb{R}^{K}\) to be \(\widehat{f}(x,\cdot)\) and consider the following minimax form:

\[\inf_{p\in\Delta(K)}\sup_{a^{\star}\in\mathcal{A}}\left\{\sum_{a=1 }^{K}p_{a}\widehat{f}(x,a)-\widehat{f}(x,a^{\star})+\frac{1}{\gamma}\|p-e_{a^ {\star}}\|_{\mathrm{diag}(W)^{-1}}^{2}\right\}\] \[=\min_{p\in\Delta(K)}\max_{a^{\star}\in\mathcal{A}}\left\{\sum_{a =1}^{K}p_{a}\widehat{f}(x,a)-\widehat{f}(x,a^{\star})+\frac{1}{\gamma}\sum_{a \neq a^{\star}}\frac{p_{a}^{2}}{W_{a}}+\frac{1}{\gamma}\frac{(1-p_{a^{\star}} )^{2}}{W_{a^{\star}}}\right\}\] (10) \[=\min_{p\in\Delta_{K}}\max_{q\in\Delta_{K}}\left\{\sum_{a=1}^{K}( p_{a}-q_{a})\widehat{f}_{a}+\frac{1}{\gamma}\sum_{a=1}^{K}\frac{p_{a}^{2}(1-q_{a})}{W_{a} }+\sum_{a=1}^{K}\frac{q_{a}(1-p_{a})^{2}}{\gamma W_{a}}\right\}\] (11) \[=\max_{q\in\Delta_{K}}\min_{p\in\Delta_{K}}\left\{\sum_{a=1}^{K}( p_{a}-q_{a})\widehat{f}_{a}+\frac{1}{\gamma}\sum_{a=1}^{K}\frac{p_{a}^{2}(1-q_{a})}{W_{a} }+\sum_{a=1}^{K}\frac{q_{a}(1-p_{a})^{2}}{\gamma W_{a}}\right\},\] (12)

where the last equality is due to Sion's minimax theorem and the fact that Eq. (10) is convex in \(p\in\Delta(K)\) by applying Lemma A.1 with \(u=e_{a}\) and \(v=g_{a}\) for each \(a\in[K]\), where \(g_{a}\in\{0,1\}^{K}\) is defined as \(g_{a,i}=1\big{\{}(i,a)\in E\big{\}}\), \(G=([K],E)\), \(\forall i\in[K]\).

Choose \(p_{a}=(1-\frac{1}{\gamma})q_{a}+\frac{1}{\gamma K}\) for all \(a\in[K]\). Let \(S\) be the set of nodes in \([K]\) that have a self-loop. Then we can upper bound the value above as follows:

\[\max_{q\in\Delta(K)}\min_{p\in\Delta(K)}\left\{\sum_{a=1}^{K}(p_{a}-q_{a}) \widehat{f}_{a}+\frac{1}{\gamma}\sum_{a=1}^{K}\frac{p_{a}^{2}(1-q_{a})}{W_{a}}+ \sum_{a=1}^{K}\frac{q_{a}(1-p_{a})^{2}}{\gamma W_{a}}\right\}\]

[MISSING_PAGE_EMPTY:15]

### Proof of Theorem 3.4

**Theorem 3.4** (Weakly observable graphs).: _Suppose that the feedback graph \(G_{t}\) is deterministic and weakly observable with weak domination number no more than \(d\). Then Algorithm 1 with \(\gamma\geq 16d\) guarantees that_

\[\overline{\mathsf{dec}}_{\gamma}(p_{t};\widehat{f}_{t},x_{t},G_{t})\leq\mathcal{ O}\left(\sqrt{\frac{d}{\gamma}}+\frac{\widetilde{\alpha}\log(K\gamma)}{\gamma} \right),\]

_where \(\widetilde{\alpha}\) is the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\)._

Proof.: Similar to the strongly observable graphs setting, for weakly observable graphs, we know that

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\] \[=\max_{q\in\Delta_{K}}\min_{p\in\Delta_{K}}\left\{\sum_{a=1}^{K} (p_{a}-q_{a})\widehat{f}_{a}+\frac{1}{\gamma}\sum_{a=1}^{K}\frac{p_{a}^{2}(1-q _{a})}{W_{a}}+\sum_{a=1}^{K}\frac{q_{a}(1-p_{a})^{2}}{\gamma W_{a}}\right\}.\] (16)

Choose \(p_{a}=(1-\frac{1}{\gamma}-\eta d)q_{a}+\frac{1}{\gamma K}+\eta 1\{a\in D\}\) where \(D\) with \(|D|=d\) is the minimum weak dominating set of \(G\) and \(0<\eta\leq\frac{1}{4d}\) is some parameter to be chosen later. Substituting the form of \(p\) to Eq. (16) and using the fact that \(|\widehat{f}_{a}|\leq 1\) for all \(a\in[K]\), we can obtain that

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\] \[\leq\max_{q\in\Delta_{K}}\left\{\frac{2}{\gamma}+\eta d+\frac{1} {\gamma}\sum_{a=1}^{K}\frac{p_{a}^{2}(1-q_{a})}{W_{a}}+\sum_{a=1}^{K}\frac{q_{ a}(1-p_{a})^{2}}{\gamma W_{a}}\right\}.\]

Then we can upper bound the value above as follows:

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\] \[\leq\max_{q\in\Delta_{K}}\left\{\frac{2}{\gamma}+\eta d+\frac{1} {\gamma}\sum_{a=1}^{K}\frac{\left((1-\frac{1}{\gamma}-\eta d)q_{a}+\frac{1}{ \gamma K}+\eta 1\{a\in D\}\right)^{2}(1-q_{a})}{W_{a}}\right.\] \[\qquad\left.+\sum_{a=1}^{K}\frac{q_{a}\left(1-(1-\frac{1}{\gamma} -\eta d)q_{a}\right)^{2}}{W_{a}}\right\}\] \[\leq\max_{q\in\Delta_{K}}\left\{\frac{2}{\gamma}+\eta d+\frac{1} {\gamma}\sum_{a\notin D}\frac{\left(q_{a}+\frac{1}{\gamma K}\right)^{2}(1-q_{a })+q_{a}\left((1-q_{a})+\frac{1}{\gamma}q_{a}+\eta dq_{a}\right)^{2}}{W_{a}}\right.\] \[\qquad\left.+\frac{1}{\gamma}\sum_{a\in D}\frac{\left(q_{a}+\frac {1}{\gamma K}+\eta\right)^{2}(1-q_{a})+q_{a}\left((1-q_{a})+\frac{1}{\gamma} q_{a}+\eta dq_{a}\right)^{2}}{W_{a}}\right\}\] \[\leq\max_{q\in\Delta_{K}}\left\{\frac{2}{\gamma}+\eta d+\frac{1} {\gamma}\sum_{a\notin D}\frac{2\left(q_{a}^{2}+\frac{1}{\gamma^{2}K^{2}} \right)(1-q_{a})+3q_{a}\left((1-q_{a})^{2}+\frac{q_{a}^{2}}{\gamma^{2}}+\eta^ {2}d^{2}q_{a}^{2}\right)}{W_{a}}\right.\] \[\qquad\left.+\frac{1}{\gamma}\sum_{a\in D}\frac{3\left(q_{a}^{2} +\frac{1}{\gamma^{2}K^{2}}+\eta^{2}\right)(1-q_{a})+3q_{a}\left((1-q_{a})^{2} +\frac{q_{a}^{2}}{\gamma^{2}}+\eta^{2}d^{2}q_{a}^{2}\right)}{W_{a}}\right\}.\] (17)

Now consider \(a\in D\). If \(a\in S\), then we know that \(W_{a}\geq\eta\); Otherwise, we know that this node can be observed by at least one node in \(D\), meaning that \(W_{a}\geq\eta\). Combining the two cases above, we know that

\[\frac{1}{\gamma}\sum_{a\in D}\frac{3\left(q_{a}^{2}+\frac{1}{\gamma^{2}K^{2}} +\eta^{2}\right)(1-q_{a})+3q_{a}\left((1-q_{a})^{2}+\frac{1}{\gamma^{2}}q_{a} ^{2}+\eta^{2}d^{2}q_{a}^{2}\right)}{W_{a}}\]

[MISSING_PAGE_EMPTY:17]

where the last inequality is due to \(\gamma\geq 16d\) and \(\eta\leq\frac{1}{4d}\). If \(a\in S\), similar to Eq. (15), we know that

\[\sum_{a\in S}\frac{q_{a}(1-q_{a})}{W_{a}} \leq\sum_{a\in S}\frac{q_{a}(1-q_{a})}{\sum_{j\in N^{\mathrm{in}} (G,a)}((1-\frac{1}{\gamma}-\eta d)q_{j}+\frac{1}{\gamma K})}\] \[\leq\frac{\gamma}{\gamma-1-\gamma\eta d}\sum_{a\in S}\frac{((1- \frac{1}{\gamma}-\eta d)q_{a}+\frac{1}{\gamma K})(1-q_{a})}{\sum_{j\in N^{ \mathrm{in}}(G,a)}((1-\frac{1}{\gamma}-\eta d)q_{j}+\frac{1}{\gamma K})}\] \[\leq 2\sum_{a\in S}\frac{\left((1-\frac{1}{\gamma}-\eta d)q_{a}+ \frac{1}{\gamma K}\right)}{\sum_{j\in N^{\mathrm{in}}(G,a)}\left((1-\frac{1}{ \gamma}-\eta d)q_{j}+\frac{1}{\gamma K}\right)}\qquad\qquad(\gamma\geq 4,\, \eta\leq\frac{1}{4d})\] \[\leq\mathcal{O}(\widetilde{\alpha}\log(K\gamma)),\] (22)

where the last inequality is again due to Lemma 5 in [1] and \(\widetilde{\alpha}\) is the independence number of the subgraph induced by nodes with self-loops in \(G\). Plugging Eq. (22) to Eq. (21) gives

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\leq\mathcal{O}\left(\eta d +\frac{1}{\gamma\eta}+\frac{\widetilde{\alpha}\log(K\gamma)}{\gamma}\right).\]

Picking \(\eta=\sqrt{\frac{1}{\gamma d}}\leq\frac{1}{4d}\) proves the result. 

Next, we prove Corollary 3.5 by combining Theorem 3.4 and Theorem 3.1.

**Corollary 3.5**.: _Suppose that \(G_{t}\) is deterministic, weakly observable, and has weak domination number no more than \(d\) for all \(t\in[T]\). In addition, suppose that the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\) is no more than \(\widetilde{\alpha}\) for all \(t\in[T]\). Then, Algorithm 1 with \(\gamma=\max\{16d,\sqrt{\widetilde{\alpha}T/\mathbf{Reg_{5q}}},d^{3}T^{\frac{ 3}{3}}\mathbf{Reg_{5q}^{-\frac{2}{3}}}\}\) guarantees that_

\[\mathbb{E}[\mathbf{Reg_{Cb}}]\leq\widetilde{\mathcal{O}}\left(d^{\frac{1}{3}} T^{\frac{3}{3}}\mathbf{Reg_{5q}^{\frac{1}{3}}}+\sqrt{\widetilde{\alpha}T\mathbf{ Reg_{5q}}}\right).\]

Proof.: Combining Eq. (8), Eq. (9) and Theorem 3.4, we can bound \(\mathbf{Reg_{Cb}}\) as follows:

\[\mathbb{E}[\mathbf{Reg_{Cb}}]\leq\mathcal{O}\left(\sqrt{\frac{d}{\gamma}}T+ \frac{\widetilde{\alpha}T\log(K\gamma)}{\gamma}+\gamma\mathbf{Reg_{Cb}}\right).\]

Picking \(\gamma=\max\left\{16d,\sqrt{\widetilde{\alpha}T/\mathbf{Reg_{5q}}},d^{3}T^{ \frac{3}{3}}\mathbf{Reg_{5q}^{-\frac{2}{3}}}\right\}\) finishes the proof. 

### Python Solution to Eq. (5)

``` defmakeProblem(nactions): importcvpyascp sqrtgammaG=cp.Parameter((nactions,nactions),nonneg=True) sqrtgammaH=cp.Parameter(nactions) p=cp.Variable(nactions,nonneg=True) sqrtgamma=cp.Variable() objective=cp.Minimize(sqrtsqrtfqammafhat@p+sqrtqammaz) constraints=[ cp.sum(p)==1 ] + [ cp.sum([ cp.quad_over_lin(eai-pi,vi) fori,(pi,vi)inenumerate(zip(p,v)) foreaiin(1ifi==aelse0,) ]) <=sqrtgammaHhata+sqrtfgammaz forin(sqrtfgammaG@p,) fora,sqrtfgammafhata@inenumerate(sqrtfgammafhat) ] problem=cp.Problem(objective,constraints) assertproblem.is_dcp(dpp=True)#proofofconvexity returnproblem,sqrtfgammaG,sqrtfgammazHhat,p,sqrtgammazThis particular formulation multiplies both sides of the constraint in Eq.5 by \(\sqrt{\gamma}\) while scaling the objective by \(\sqrt{\gamma}\). While mathematically equivalent to Eq.5, empirically it has superior numerical stability for large \(\gamma\). For additional stability, when using this routine we recommend subtracting off the minimum value from \(\widehat{f}\), which is equivalent to making the substitutions \(\sqrt{\gamma}\widehat{f}\leftarrow\sqrt{\gamma}\widehat{f}-\sqrt{\gamma}\min_{ a}\widehat{f}_{a}\) and \(z\leftarrow z+\sqrt{\gamma}\min_{a}\widehat{f}_{a}\) and then exploiting the \(1^{\top}p=1\) constraint.

### Proof of Theorem3.6

**Theorem 3.6**.: _Solving \(\operatorname*{argmin}_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p; \widehat{f},x,G)\) is equivalent to solving the following convex optimization problem._

\[\min_{p\in\Delta(K),z} p^{\top}\widehat{f}+z\] (5) \[\operatorname*{subject\ to} \forall a\in[K]:\frac{1}{\gamma}\|p-e_{a}\|_{\operatorname*{diag} (G^{\top}p)^{-1}}^{2}\leq\widehat{f}(x,a)+z,\] \[G^{\top}p\succ 0,\]

_where \(\widehat{f}\) in the objective is a shorthand for \(\widehat{f}(x,\cdot)\in\mathbb{R}^{K}\), \(e_{a}\) is the \(a\)-th standard basis vector, and \(\succ\) means element-wise greater._

Proof.: Denote \(f^{\star}=f^{\star}(x,\cdot)\in\mathbb{R}^{K}\). Note that according to the definition of \(G\), we know that \((G^{\top}p)_{i}\) denotes the probability that action \(i\)'s loss is revealed when the selected action \(a\) is sampled from distribution \(p\). Then, we know that

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\] \[=\sup_{\begin{subarray}{c}a^{\star}\in[K]\\ f^{\star}\in\mathbb{R}^{K}\end{subarray}}\mathbb{E}_{a_{t}\sim p}\Bigg{[}f_{a _{t}}^{\star}-f_{a^{\star}}^{\star}-\frac{\gamma}{4}\mathbb{E}_{A\sim G(\cdot \lfloor a_{t})}\Bigg{[}\sum_{a\in A}\Bigl{(}\widehat{f}_{a}-f_{a}^{\star} \Bigr{)}^{2}\Bigg{]}\Bigg{]}\] \[=\sup_{\begin{subarray}{c}a^{\star}\in[K]\\ f^{\star}\in\mathbb{R}^{K}\end{subarray}}(p-e_{a^{\star}})^{\top}f^{\star}- \frac{\gamma}{4}\sum_{a\in[K]}\|\widehat{f}-f^{\star}\|_{\operatorname*{diag} (G^{\top}p)}^{2}\] \[=\sup_{a^{\star}\in[K]}(p-e_{a^{\star}})^{\top}\widehat{f}+\frac{ 1}{\gamma}\|p-e_{a^{\star}}\|_{\operatorname*{diag}(G^{\top}p)^{-1}}^{2} \big{(}G^{\top}p\succ 0\big{)}\] \[=p^{\top}\widehat{f}+\max_{a^{\star}\in[K]}\left\{\frac{1}{\gamma} \|p-e_{a^{\star}}\|_{\operatorname*{diag}(G^{\top}p)^{-1}}^{2}-e_{a^{\star}}^{ \top}\widehat{f}\right\},\]

where the third equality is by picking \(f^{\star}\) to be the maximizer and introduces a constraint. Therefore, the minimization problem \(\min_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) can be written as the following constrained optimization by variable substitution:

\[\min_{p\in\Delta(K),z} p^{\top}\widehat{f}+z\] \[\operatorname*{subject\ to} \forall a\in[K]:\frac{1}{\gamma}\|p-e_{a}\|_{\operatorname*{diag} (G^{\top}p)^{-1}}^{2}\leq e_{a}^{\top}\widehat{f}+z,\] \[G^{\top}p\succ 0.\]

The convexity of the constraints follows from LemmaA.1. 

## Appendix B Omitted Details in Section4

In this section, we provide proofs for Section4. We define \(W_{a}:=\sum_{a^{\prime}\in N^{\mathrm{in}}(G,a)}p_{a^{\prime}}\) to be the probability that the loss of action \(a\) is revealed when selecting an action from distribution \(p\). Let \(\widehat{f}=\widehat{f}(x,\cdot)\in\mathbb{R}^{K}\) and \(f=f(x,\cdot)\in\mathbb{R}^{K}\). Direct calculation shows that for any \(a^{\star}\in[K]\),

\[f^{\star}=\operatorname*{argmax}_{f\in\mathbb{R}^{K}}\mathbb{E}_{a\sim p} \left[f(x,a)-f(x,a^{\star})-\frac{\gamma}{4}\cdot\sum_{a^{\prime}\in N^{ \mathrm{in}}(G,a)}(\widehat{f}_{t}(x,a^{\prime})-f(x,a^{\prime}))^{2}\right]\]\[=\frac{2}{\gamma}\operatorname{diag}(W)^{-1}(p-e_{a^{*}})+\widehat{f}.\]

Therefore, substituting \(f^{*}\) into Eq. (4), we obtain that

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)=\max_{a^{*}\in[K]}\left\{ \frac{1}{\gamma}\left(\sum_{a=1}^{K}\frac{p_{a}^{2}}{W_{a}}+\frac{1-2p_{a^{*}}} {W_{a^{*}}}\right)+\left\langle p-e_{a^{*}},\widehat{f}\right\rangle\right\}.\] (23)

Without loss of generality, we assume the \(\min_{i\in[K]}\widehat{f}_{i}=0\). This is because shifting \(\widehat{f}\) by \(\min_{i\in[K]}\widehat{f}_{i}\) does not change the value of \(\left\langle p-e_{a^{*}},\widehat{f}\right\rangle\). In the following sections, we provide proofs showing that a certain closed-form of \(p\) leads to optimal \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\) up to constant factors for several specific types of feedback graphs, respectively.

### Cops-and-Robbers Graph

**Proposition 1**.: _When \(G=G_{\mathrm{CR}}\), given any \(\widehat{f}\), context \(x\), the closed-form distribution \(p\) in Eq. (6) guarantees that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{CR}})\leq\mathcal{ O}\!\left(\frac{1}{\gamma}\right)\)._

Proof.: We use the following notation for convenience: \(p_{1}:=p_{a_{1}}\), \(p_{2}:=p_{a_{2}}\), \(\widehat{f}_{1}:=\widehat{f}_{a_{1}}=0\), \(\widehat{f}_{2}:=\widehat{f}_{a_{2}}\). For the cops-and-robbers graph and closed-form solution \(p\) in Eq. (6), Eq. (23) becomes:

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{CR}})=\max_{a^{*} \in[K]}\left\{\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1}) ^{2}}{p_{1}}+\frac{1-2p_{a^{*}}}{W_{a^{*}}}\right)+\left\langle p-e_{a^{*}}, \widehat{f}\right\rangle\right\}.\]

If \(a^{*}\neq a_{1}\) and \(a^{*}\neq a_{2}\), we know that

\[\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1})^ {2}}{p_{1}}+\frac{1-2p_{a^{*}}}{W_{a^{*}}}\right)+\left\langle p-e_{a^{*}}, \widehat{f}\right\rangle\] \[=\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1}) ^{2}}{p_{1}}+1\right)+p_{1}\widehat{f}_{1}+p_{2}\widehat{f}_{2}-\widehat{f}_{a ^{*}}\] \[\leq\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1} )^{2}}{p_{1}}+1\right)-p_{1}\widehat{f}_{2} (\widehat{f}_{a^{*}}\geq\widehat{f}_{2}\geq\widehat{f}_{1}=0)\] \[\leq\frac{1}{\gamma}\left(\frac{1}{1-p_{1}}+1+1\right)-p_{1} \widehat{f}_{2} (p_{1}\in[\tfrac{1}{2},1],p_{1}\geq p_{2}\in[0,\tfrac{1}{2}])\] \[=\frac{1}{\gamma}\left(4+\gamma\widehat{f}_{2}\right)-\left(1- \frac{1}{2+\gamma\widehat{f}_{2}}\right)\widehat{f}_{2}\] \[\leq\frac{5}{\gamma}.\]

If \(a^{*}=a_{2}\), we can obtain that

\[\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1}) ^{2}}{p_{1}}+\frac{1-2p_{a^{*}}}{W_{a^{*}}}\right)+\left\langle p-e_{a^{*}}, \widehat{f}\right\rangle\] \[=\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1} )^{2}}{p_{1}}+\frac{1-2p_{2}}{p_{1}}\right)+p_{1}\widehat{f}_{1}+p_{2}\widehat{ f}_{2}-\widehat{f}_{2}\] \[\leq\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1 })^{2}}{p_{1}}+\frac{1-2(1-p_{1})}{p_{1}}\right)-p_{1}\widehat{f}_{2} (\widehat{f}_{1}=0)\] \[\leq\frac{1}{\gamma}\left(\frac{1}{1-p_{1}}+1+2-\frac{1}{p_{1}} \right)-p_{1}\widehat{f}_{2} (p_{1}\in[\tfrac{1}{2},1],p_{2}\in[0,\tfrac{1}{2}])\] \[\leq\frac{1}{\gamma}\left(5+\gamma\widehat{f}_{2}\right)-\left(1- \frac{1}{2+\gamma\widehat{f}_{2}}\right)\widehat{f}_{2} (p_{1}=\tfrac{1}{2+\gamma\widehat{f}_{2}})\] \[\leq\frac{6}{\gamma}.\]If \(a^{\star}=a_{1}\), we have

\[\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{1})^{2}} {p_{1}}+\frac{1-2p_{a^{\star}}}{W_{a^{\star}}}\right)+\left\langle p-e_{a^{\star} },\widehat{f}\right\rangle\] \[\leq\frac{1}{\gamma}\left(\frac{p_{1}^{2}}{1-p_{1}}+\frac{(1-p_{ 1})^{2}}{p_{1}}+\frac{1-2p_{1}}{1-p_{1}}\right)+(1-p_{1})\widehat{f}_{2}\] \[\leq\frac{1}{\gamma}\left(1-p_{1}+\frac{(1-p_{1})^{2}}{p_{1}} \right)+(1-p_{1})\widehat{f}_{2}\] \[\leq\frac{1}{\gamma}\left(1+\frac{1}{2}\right)+\frac{\widehat{f} _{2}}{2+\gamma\widehat{f}_{2}}\] ( \[p_{1}\in[\tfrac{1}{2},1])\] \[\leq\frac{3}{\gamma}.\]

Putting everything together, we prove that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{CR}})\leq\frac{6} {\gamma}\leq\mathcal{O}\left(\frac{1}{\gamma}\right)\). 

### Apple Testing Graph

**Proposition 2**.: _When \(G=G_{\mathrm{AT}}\), given any \(\widehat{f}\), context \(x\), the closed-form distribution \(p\) in Eq. (7) guarantees that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{AT}})\leq\mathcal{ O}(\frac{1}{\gamma})\)._

Proof.: For the apple tasting graph and closed-form solution \(p\) in Eq. (7), Eq. (23) becomes:

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)=\max_{a^{\star}\in[K]} \left\{\frac{1}{\gamma}\left(p_{1}+\frac{(1-p_{1})^{2}}{p_{1}}+\frac{1-2p_{a^ {\star}}}{W_{a^{\star}}}\right)+\left\langle p-e_{a^{\star}},\widehat{f} \right\rangle\right\}.\]

Suppose \(\widehat{f}_{1}=0\), we know that \(p_{1}=1\), \(p_{2}=0\) and

1. If \(a^{\star}=1\), we have \[\frac{1}{\gamma}\left(p_{1}+\frac{(1-p_{1})^{2}}{p_{1}}+\frac{1-2p_{a^{\star} }}{W_{a^{\star}}}\right)+\left\langle p-e_{a^{\star}},\widehat{f}\right\rangle =0.\]
2. If \(a^{\star}=2\), direct calculation shows that \[\frac{1}{\gamma}\left(p_{1}+\frac{(1-p_{1})^{2}}{p_{1}}+\frac{1-2p_{a^{\star} }}{W_{a^{\star}}}\right)+\left\langle p-e_{a^{\star}},\widehat{f}\right\rangle \leq\frac{2}{\gamma}.\]

Suppose \(\widehat{f}_{2}=0\), we know that \(p_{1}=\frac{2}{4+\gamma\widehat{f}_{1}}\), \(p_{2}=1-p_{1}\) and

1. If \(a^{\star}=1\), we have \[\frac{1}{\gamma}\left(p_{1}+\frac{(1-p_{1})^{2}}{p_{1}}+\frac{1-2p_ {a^{\star}}}{W_{a^{\star}}}\right)+\left\langle p-e_{a^{\star}},\widehat{f}\right\rangle\] \[=\frac{1}{\gamma}\left(p_{1}+\frac{(1-p_{1})^{2}}{p_{1}}+\frac{1-2 p_{1}}{p_{1}}\right)-(1-p_{1})\widehat{f}_{1}\] \[=\frac{2(1-p_{1})^{2}}{\gamma p_{1}}-(1-p_{1})\widehat{f}_{1}\] \[=\frac{(2+\gamma\widehat{f}_{1})^{2}}{\gamma(4+\gamma\widehat{f} _{1})}-(1-p_{1})\widehat{f}_{1}\] \[\leq\frac{4+\gamma\widehat{f}_{1}}{\gamma}+\frac{2\widehat{f}_{1} }{4+\gamma\widehat{f}_{1}}-\widehat{f}_{1}\leq\frac{6}{\gamma}.\]
2. If \(a^{\star}=2\), direct calculation shows that \[\frac{1}{\gamma}\left(p_{1}+\frac{(1-p_{1})^{2}}{p_{1}}+\frac{1-2p_{a^{\star} }}{W_{a^{\star}}}\right)+\left\langle p-e_{a^{\star}},\widehat{f}\right\rangle =\frac{2p_{1}}{\gamma}+p_{1}\widehat{f}_{1}\leq\frac{1}{\gamma}+\frac{2 \widehat{f}_{1}}{4+\gamma\widehat{f}_{1}}\leq\frac{3}{\gamma}.\]Putting everything together, we prove that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{AT}})\leq\frac{6}{ \gamma}\leq\mathcal{O}\left(\frac{1}{\gamma}\right)\). 

### Inventory Graph

**Proposition 3**.: _When \(G=G_{\mathrm{inv}}\), given any \(\widehat{f}\), context \(x\), there exists a closed-form distribution \(p\in\Delta(K)\) guaranteeing that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{inv}})\leq \mathcal{O}(\frac{1}{\gamma})\), where \(p\) is defined as follows: \(p_{j}=\max\{\frac{1}{1+\gamma(\widehat{f}_{j}-\min_{i}\widehat{f}_{i})}-\sum_ {j^{\prime}>j}p_{j^{\prime}},0\}\) for all \(j\in[K]\)._

Proof.: Based on the distribution defined above, define \(A\subseteq[K]\) to be the set such that for all \(i\in A\), \(p_{i}>0\) and denote \(N=|A|\). We index each action in \(A\) by \(k_{1}<k_{2}<\cdots<k_{N}=K\). According to the definition of \(p_{i}\), we know that \(p_{i}\) is strictly positive only when \(\widehat{f}_{i}<\widehat{f}_{j}\) for all \(j>i\) and specifically, when \(p_{i}>0\), we know that \(W_{i}=\sum_{j\geq i}p_{j}=\frac{1}{1+\gamma\widehat{f}_{i}}\) (recall that \(\min_{i}\widehat{f}_{i}=0\) since we shift \(\widehat{f}\)). Therefore, define \(W_{k_{N+1}}=0\) and we know that

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{inv}})\] \[=\sum_{i=1}^{N}p_{k_{i}}\widehat{f}_{k_{i}}+\frac{1}{\gamma}\sum_ {a=1}^{K}\frac{p_{a}^{2}}{W_{a}}+\max_{a^{*}\in[K]}\left\{\frac{1-2p_{a^{*}}} {\gamma W_{a^{*}}}-\widehat{f}_{a^{*}}\right\}\] \[\leq\sum_{i=1}^{N}\left(W_{k_{i}}-W_{k_{i+1}}\right)\widehat{f}_{ k_{i}}+\frac{1}{\gamma}+\max_{a^{*}\in[K]}\left\{\frac{1-2p_{a^{*}}}{ \gamma W_{a^{*}}}-\widehat{f}_{a^{*}}\right\}\] \[\leq\frac{2}{\gamma}+\sum_{i=1}^{N-1}\left(\frac{1}{1+\gamma \widehat{f}_{k_{i}}}-\frac{1}{1+\gamma\widehat{f}_{k_{i+1}}}\right)\widehat{f} _{k_{i}}+\max_{a^{*}\in[K]}\left\{\frac{1-2p_{a^{*}}}{\gamma W_{a^{*}}}- \widehat{f}_{a^{*}}\right\}\] \[\leq\frac{3}{\gamma}+\sum_{i=2}^{N}\frac{\widehat{f}_{k_{i}}- \widehat{f}_{k_{i-1}}}{1+\gamma\widehat{f}_{k_{i}}}+\max_{a^{*}\in[K]}\left\{ \frac{1-2p_{a^{*}}}{\gamma W_{a^{*}}}-\widehat{f}_{a^{*}}\right\}.\]

According to Lemma 9 of [1] (included as Lemma E.2 for completeness), we know that

\[\sum_{i=2}^{N}\frac{\widehat{f}_{k_{i}}-\widehat{f}_{k_{i-1}}}{1+\gamma \widehat{f}_{k_{i}}}=\frac{1}{\gamma}\sum_{i=2}^{N}\frac{\widehat{f}_{k_{i}}- \widehat{f}_{k_{i-1}}}{\frac{1}{\gamma}+\widehat{f}_{k_{i}}}\leq\frac{\text{ mas}(G_{A})}{\gamma}=\frac{1}{\gamma},\] (24)

where \(G_{A}\) is the subgraph of \(G\) restricted to node set \(A\) and \(\text{mas}(G)\) is the size of the maximum acyclic subgraphs of \(G\). It is direct to see that any subgraph \(G\) of \(G_{\mathrm{inv}}\) has \(\text{mas}(G)=1\).

Next, consider the value of \(a^{\star}\in[K]\) that maximizes \(\frac{1-2p_{a^{*}}}{\gamma W_{a^{*}}}-\widehat{f}_{a^{*}}\). If \(a^{\star}\leq k_{1}\), then we know that \(W_{a^{*}}=1\) and \(\frac{1-2p_{a^{*}}}{\gamma W_{a^{*}}}-\widehat{f}_{a^{*}}\leq\frac{1}{\gamma}\). Otherwise, suppose that \(k_{i}<a^{*}\leq k_{i+1}\) for some \(i\in[N-1]\). According to the definition of \(p\), if \(a^{\star}\neq k_{i+1}\) we know that \(p_{a^{*}}=0\) and

\[\frac{1}{1+\gamma\widehat{f}_{a^{*}}}\leq\sum_{j^{\prime}>a^{*}}p_{j^{\prime}} =W_{k_{i+1}}=W_{a^{*}}.\]

Therefore,

\[\frac{1-2p_{a^{*}}}{\gamma W_{a^{*}}}-\widehat{f}_{a^{*}}=\frac{1}{\gamma W_{ a^{*}}}-\widehat{f}_{a^{*}}\leq\frac{1}{\gamma}.\]

Otherwise, \(W_{a^{*}}=W_{k_{i+1}}\) and \(\frac{1-2p_{a^{*}}}{\gamma W_{a^{*}}}-\widehat{f}_{a^{*}}\leq\frac{1}{\gamma W _{k_{i+1}}}-\widehat{f}_{k_{i+1}}=\frac{1}{\gamma}\). Combining the two cases above and Eq. (24), we obtain that

\[\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G_{\mathrm{inv}})\leq\frac{3 }{\gamma}+\frac{1}{\gamma}+\frac{1}{\gamma}=\mathcal{O}\left(\frac{1}{\gamma} \right).\]

### Undirected and Self-Aware Graphs

**Proposition 4**.: _When \(G\) is an undirected self-aware graph, given any \(\widehat{f}\), context \(x\), there exists a closed-form distribution \(p\in\Delta(K)\) guaranteeing that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\leq\mathcal{O}\Big{(}\frac{ \alpha}{\gamma}\Big{)}\)._

Proof.: We first introduce the closed-form of \(p\) and then show that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\leq\mathcal{O}(\frac{ \alpha}{\gamma})\). Specifically, we first sort \(\widehat{f}_{a}\) in an increasing order and choose a maximal independent set by choosing the nodes in a greedy way. Specifically, we pick \(k_{1}=\operatorname*{argmin}_{i\in[K]}\widehat{f}_{i}\). Then, we ignore all the nodes that are connected to \(k_{1}\) and select the node \(a\) with the smallest \(\widehat{f}_{a}\) in the remaining node set. This forms a maximal independent set \(I\subseteq[K]\), which has size no more than \(\alpha\) and is also a dominating set. Set \(p_{a}=\frac{1}{\alpha+\gamma\widehat{f}_{a}}\) for \(a\in I\backslash\{\widehat{k}_{1}\}\) and \(p_{k_{1}}=1-\sum_{a\neq k_{1},a\in I}p_{a}\). This is a valid distribution as we only choose at most \(\alpha\) nodes and \(p_{a}\leq 1/\alpha\) for all \(a\in I\backslash\{k_{1}\}\). Now we show that \(\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f},x,G)\leq\mathcal{O}(\frac{ \alpha}{\gamma})\). Specifically, we only need to show that with this choice of \(p\), for any \(a^{\star}\in[K]\),

\[\sum_{a=1}^{K}p_{a}\widehat{f}_{a}-\widehat{f}_{a^{\star}}+\frac{1}{\gamma} \sum_{a=1}^{K}\frac{p_{a}^{2}}{W_{a}}+\frac{1-2p_{a^{\star}}}{\gamma W_{a^{ \star}}}\leq\mathcal{O}\left(\frac{\alpha}{\gamma}\right).\]

Plugging in the form of \(p\), we know that

\[\sum_{a=1}^{K}p_{a}\widehat{f}_{a}-\widehat{f}_{a^{\star}}+\frac{1 }{\gamma}\sum_{a=1}^{K}\frac{p_{a}^{2}}{W_{a}}+\frac{1-2p_{a^{\star}}}{\gamma W _{a^{\star}}}\] \[\leq\sum_{a\in I\backslash\{k_{1}\}}\frac{\widehat{f}_{a}}{ \alpha+\gamma\widehat{f}_{a}}-\widehat{f}_{a^{\star}}+\frac{1-2p_{a^{\star}}}{ \gamma W_{a^{\star}}}+\frac{1}{\gamma} \text{($p_{a}\leq W_{a}$ for all $a\in[K]$)}\] \[\leq\frac{\alpha}{\gamma}-\widehat{f}_{a^{\star}}+\frac{1-2p_{a^{ \star}}}{\gamma W_{a^{\star}}}. \text{($|I|\leq\alpha$)}\]

If \(a^{\star}=k_{1}\), then we can obtain that \(\frac{1-2p_{a^{\star}}}{\gamma W_{a^{\star}}}\leq\frac{1}{\gamma W_{k_{1}}} \leq\frac{\alpha}{\gamma}\) as \(p_{k_{1}}\geq\frac{1}{\alpha}\) according to the definition of \(p\). Otherwise, note that according to the choice of the maximal independent set \(I\), \(W_{a^{\star}}\geq\frac{1}{\alpha+\gamma\widehat{f}_{a^{\prime}}}\) for some \(a^{\prime}\in I\) such that \(\widehat{f}_{a^{\prime}}\leq\widehat{f}_{a^{\star}}\). Therefore,

\[-\widehat{f}_{a^{\star}}+\frac{1-2p_{a^{\star}}}{\gamma W_{a^{\star}}}\leq- \widehat{f}_{a^{\star}}+\frac{1}{\gamma W_{a^{\star}}}\leq-\widehat{f}_{a^{ \star}}+\frac{\alpha+\gamma\widehat{f}_{a^{\prime}}}{\gamma}\leq\frac{\alpha}{ \gamma}.\]

Combining the two inequalities above together proves the bound. 

## Appendix C Implementation Details in Experiments

### Implementation Details in Section 5.1

We conduct experiments on RCV1 (Lewis et al., 2004), which is a multilabel text-categorization dataset. We use a subset of RCV1 containing \(50000\) samples and \(K=50\) sub-classes. Therefore, the feedback graph in our experiment has \(K=50\) nodes. We use the bag-of-words vector of each sample as the context with dimension \(d=47236\) and treat the text categories as the arms. In each round \(t\), the learner receives the bag-of-words vector \(x_{t}\) and makes a prediction \(a_{t}\in[K]\) as the text category. The loss is set to be \(\ell_{t,a_{t}}=0\) if the sample belongs to the predicted category \(a_{t}\) and \(\ell_{t,a_{t}}=1\) otherwise.

The function class we consider is the following linear function class:

\[\mathcal{F}=\{f:f(x,a)=\mathsf{Sigmoid}((Mx)_{a}),M\in\mathbb{R}^{K\times d}\},\]

where \(\mathsf{Sigmoid}(u)=\frac{1}{1+e^{-u}}\) for any \(u\in\mathbb{R}\). The oracle is implemented by applying online gradient descent with learning rate \(\eta\) searched over \(\{0.1,0.2,0.5,1,2,4\}\). As suggested by (Foster andKrishnamurthy, 2021), we use a time-varying exploration parameter \(\gamma_{t}=c\cdot\sqrt{\alpha t}\), where \(t\) is the index of the iteration, \(c\) is searched over \(\{8,16,32,64,128\}\), and \(\alpha\) is the independence number of the corresponding feedback graph. Our code is built on PyTorch framework (Paszke et al., 2019). We run 5 independent experiments with different random seeds and plot the mean and standard deviation value of PV loss.

### Implementation Details in Section 5.2

#### c.2.1 Details for Results on Random Directed Self-aware Graphs

We conduct experiments on a subset of RCV1 containing \(10000\) samples with \(K=10\) sub-classes. Our code is built on Vowpal Wabbit (Langford and Zhang, 2007). For SqaureCB, the exploration parameter \(\gamma_{t}\) at round \(t\) is set to be \(\gamma_{t}=c\cdot\sqrt{Kt}\), where \(t\) is the index of the round and \(c\) is the hyper-parameter searched over set \(\{8,16,32,64,128\}\). The remaining details are the same as described in Appendix C.1.

#### c.2.2 Details for Results on Synthetic Inventory Dataset

In this subsection, we introduce more details in the synthetic inventory data construction, loss function constructions, oracle implementation, and computation of the strategy at each round.

Dataset.In this experiment, we create a synthetic inventory dataset constructed as follows. The dataset includes \(T\) data points, the \(t\)-th of which is represented as \((x_{t},d_{t})\) where \(x_{t}\in\mathbb{R}^{m}\) is the context and \(d_{t}\) is the realized demand given context \(x_{t}\). Specifically, in the experiment, we choose \(m=100\) and \(x_{t}\)'s are drawn i.i.d from Gaussian distribution with mean \(0\) and standard deviation \(0.1\). The demand \(d_{t}\) is defined as

\[d_{t}=\frac{1}{\sqrt{m}}x_{t}^{\top}\theta+\varepsilon_{t},\]

where \(\theta\in\mathbb{R}^{m}\) is an arbitrary vector and \(\varepsilon_{t}\) is a one-dimensional Gaussian random variable with mean \(0.3\) and standard deviation \(0.1\). After all the data points \(\{(x_{t},d_{t})\}_{t=1}^{T}\) are constructed, we normalize \(d_{t}\) to \([0,1]\) by setting \(d_{t}\leftarrow\frac{d_{t}-\min_{t^{\prime}\in[T]}d_{t^{\prime}}}{\max_{t^{ \prime}\in[T]}d_{t^{\prime}}-\min_{t^{\prime}\in[T]}d_{t^{\prime}}}\). In all our experiments, we set \(T=10000\).

Loss construction.Next, we define the loss at round \(t\) when picking the inventory level \(a_{t}\) with demand \(d_{t}\), which is defined as follows:

\[\ell_{t,a_{t}}=h\cdot\max\{a_{t}-d_{t},0\}+b\cdot\max\{d_{t}-a_{t},0\},\] (25)

where \(h>0\) is the holding cost per remaining items and \(b>0\) is the backorder cost per remaining items. In the experiment, we set \(h=0.25\) and \(b=1\).

Regression oracle.The function class we use in this experiment is as follows:

\[\mathcal{F}=\{f:f(x,a)=h\cdot\max\{a-(x^{\top}\theta+\beta),0\}+b\cdot\max\{x ^{\top}\theta+\beta-a,0\},\theta\in\mathbb{R}^{m},\beta\in\mathbb{R}\}.\]

This ensures the realizability assumption according to the definition of our loss function shown in Eq. (25). The oracle uses online gradient descent with learning rate \(\eta\) searched over \(\{0.01,0.05,0.1,0.5,1\}\).

Calculation of \(p_{t}\).To make \(\mathtt{SquareCB.G}\) more efficient, instead of solving the convex program defined in Eq. (5), we use the closed-form of \(p_{t}\) derived in Proposition 3, which only requires \(\mathcal{O}(K)\) computational cost and has the same theoretical guarantee (up to a constant factor) as the one enjoyed by the solution solved by Eq. (5). Similar to the case in Appendix C.1, at each round \(t\), we pick \(\gamma_{t}=c\cdot\sqrt{t}\) with \(c\) searched over the set \(\{0.25,0.5,1,2,3,4\}\). Note again that the independence number for inventory graph is \(1\).

We run \(8\) independent experiments with different random seeds and plot the mean and standard deviation value of PV loss.

Adaptive Tuning of \(\gamma\) without the Knowledge of Graph-Theoretic Numbers

In this section, we show how to adaptively tune the parameter \(\gamma\) in order to achieve \(\widetilde{\mathcal{O}}\left(\sqrt{\sum_{t=1}^{T}\alpha_{t}\mathbf{Reg_{Sq}}}\right)\) regret in the strongly observable graphs case and \(\widetilde{\mathcal{O}}\left(\left(\sum_{t=1}^{T}\sqrt{d_{t}}\right)^{\frac{2}{ 3}}\mathbf{Reg_{Sq}^{\frac{1}{3}}}+\sqrt{\sum_{t=1}^{T}\widetilde{\alpha}_{t} \mathbf{Reg_{Sq}}}\right)\) in the weakly observable graphs case.

### Strongly Observable Graphs

In order to achieve \(\widetilde{\mathcal{O}}\left(\sum_{t=1}^{T}\alpha_{t}\mathbf{Reg_{Sq}}\right)\) regret guarantee without the knowledge of \(\alpha_{t}\), we apply a doubling trick on \(\gamma\) based on the value of \(\min_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f}_{t},x_{t}, G_{t})\). Specifically, our algorithm goes in epochs with the parameter \(\gamma\) being \(\gamma_{s}\) in the \(s\)-th epoch. We initialize \(\gamma_{1}=\sqrt{\frac{T}{\mathbf{Reg_{Sq}}}}\). As proven in Theorem3.2, we know that

\[\gamma\cdot\min_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma}(p;\widehat{f}_ {t},x_{t},G_{t})\leq\widetilde{\mathcal{O}}(\alpha_{t}).\]

Therefore, within each epoch \(s\) (with start round \(b_{s}\)), at round \(t\), we calculate the value

\[Q_{t}=\sum_{\tau=b_{s}}^{t}\min_{p\in\Delta(K)}\overline{\mathsf{dec}}_{\gamma _{s}}(p;\widehat{f}_{t},x_{t},G_{t}),\] (26)

which is bounded by \(\widetilde{\mathcal{O}}\left(\frac{1}{\gamma_{s}}\sum_{\tau=b_{s}}^{t}\alpha_ {\tau}\right)\) and is in fact obtainable by solving the convex program. Then, we check whether \(Q_{t}\leq\gamma_{s}\mathbf{Reg_{Sq}}\). If this holds, we continue our algorithm using \(\gamma_{s}\); otherwise, we set \(\gamma_{s+1}=2\gamma_{s}\) and restart the algorithm.

Now we analyze the performance of the above described algorithm. First, note that for any \(t\), within epoch \(s\),

\[\gamma_{s}Q_{t}\leq\widetilde{\mathcal{O}}\left(\sum_{\tau=1}^{T}\alpha_{\tau }\right),\]

meaning that the number of epoch \(S\) is bounded by \(S=\log_{2}C_{1}+\log_{4}\frac{\sum_{\tau=1}^{T}\alpha_{t}}{T}\) for certain \(C_{1}>0\) which only contains constant and \(\log\) terms.

Next, consider the regret in epoch \(s\) with \(I_{s}=[b_{s},e_{s}]\). According to Eq.8, we know that the regret within epoch \(s\) is bounded as follows:

\[\mathbb{E}\left[\sum_{t\in I_{s}}f^{\star}(x_{t},a_{t})-\sum_{t \in I_{s}}f^{\star}(x_{t},\pi^{\star}(x_{t}))\right]\] \[\leq\mathbb{E}\left[\sum_{t\in I_{s}}\overline{\mathsf{dec}}_{ \gamma_{s}}(p_{t};\widehat{f}_{t},x_{t},G_{t})\right]+\frac{\gamma_{s}}{4} \mathbf{Reg_{Sq}}\] \[\leq\widetilde{\mathcal{O}}(\gamma_{s}\mathbf{Reg_{Sq}}),\] (27)

where the last inequality is because at round \(t=e_{s}-1\), \(Q_{t}\leq\gamma_{s}\mathbf{Reg_{Sq}}\) is satisfied. Taking summation over all \(S\) epochs, we know that the overall regret is bounded as

\[\mathbb{E}[\mathbf{Reg_{Cb}}] \leq\sum_{s=1}^{S}\widetilde{\mathcal{O}}(\gamma_{s}\mathbf{Reg_{ Sq}})=\sum_{s=1}^{S}\widetilde{\mathcal{O}}\left(2^{s-1}\sqrt{T\mathbf{Reg_{ Sq}}}\right)\] \[\leq\widetilde{\mathcal{O}}\left(2^{S}\sqrt{T\mathbf{Reg_{Sq}}} \right)=\widetilde{\mathcal{O}}\left(\sqrt{\sum_{t=1}^{T}\alpha_{t}\mathbf{Reg _{Sq}}}\right),\] (28)

which finishes the proof.

### Weakly Observable Graphs

For the weakly observable graphs case, to achieve the target regret without the knowledge of \(\widetilde{\alpha}_{t}\) and \(d_{t}\), which are the independence number of the subgraph induced by nodes with self-loops in \(G_{t}\) and the weak domination number of \(G_{t}\), we apply the same approach as the one applied in the strongly observable graph case. Note that according to Theorem 3.4, within epoch \(s\), we have \(Q_{t}\leq C_{2}\left(\frac{\sum_{t=s_{\mathsf{Sq}}}^{t}\sqrt{d_{\tau}}}{\sqrt{ \gamma_{s}}}+\frac{\sum_{\tau=s_{\mathsf{Sq}}}^{t}\widetilde{\alpha}_{\tau}}{ \gamma_{\tau}}\right)\) for certain \(C_{2}>1\) only containing constants and \(\log\) factors. In the weakly observable graphs case, we know that the number of epoch is bounded by \(S=2+\log_{2}C_{2}+\max\left\{\log_{4}\frac{\sum_{t=1}^{T}\widetilde{\alpha}_{t }}{T},\log_{2}\left(\frac{(\sum_{t=1}^{T}\sqrt{d_{t}})^{\frac{2}{3}}}{\sqrt{T \cdot\mathbf{Reg}_{\mathsf{Sq}}^{\frac{2}{3}}}}\right)\right\}\) since we have

\[\gamma_{S}=4C_{2}\cdot\sqrt{\frac{1}{\mathbf{Reg}_{\mathsf{Sq}}}}\max\left\{ \sqrt{\sum_{t=1}^{T}\widetilde{\alpha}_{t}},\frac{(\sum_{t=1}^{T}\sqrt{d_{t}}) ^{\frac{2}{3}}}{\mathbf{Reg}_{\mathsf{Sq}}^{\frac{1}{3}}}\right\},\]

and at round \(t\) in epoch \(S\),

\[Q_{t}\leq C_{2}\left(\frac{\sum_{\tau=1}^{T}\widetilde{\alpha}_{\tau}}{\gamma _{S}}+\frac{\sum_{\tau=1}^{T}\sqrt{d_{\tau}}}{\sqrt{\gamma_{S}}}\right)\leq \gamma_{S}\mathbf{Reg}_{\mathsf{Sq}},\]

meaning that epoch \(S\) will never end. Therefore, following Eq. (27) and Eq. (28), we can obtain that

\[\mathbb{E}[\mathbf{Reg}_{\mathsf{CB}}]\leq\widetilde{\mathcal{O}}\left(2^{S} \sqrt{T\mathbf{Reg}_{\mathsf{Sq}}}\right)=\widetilde{\mathcal{O}}\left(\sqrt{ \sum_{t=1}^{T}\widetilde{\alpha}_{t}\mathbf{Reg}_{\mathsf{Sq}}}+\left(\sum_{t =1}^{T}\sqrt{d_{t}}\right)^{\frac{2}{3}}\mathbf{Reg}_{\mathsf{Sq}}^{\frac{1}{ 3}}\right),\]

which finishes the proof.

## Appendix E Auxiliary Lemmas

**Lemma E.1** (Lemma 5 in [1]).: _Let \(G=(V,E)\) be a directed graph with \(|V|=K\), in which \(i\in N^{\mathrm{in}}(G,i)\) for all vertices \(i\in[K]\). Assign each \(i\in V\) with a positive weight \(w_{i}\) such that \(\sum_{i=1}^{n}w_{i}\leq 1\) and \(w_{i}\geq\varepsilon\) for all \(i\in V\) for some constant \(0<\varepsilon<\frac{1}{2}\). Then_

\[\sum_{i=1}^{K}\frac{w_{i}}{\sum_{j\in N^{\mathrm{in}}(G,i)}w_{j}}\leq 4\alpha(G) \log\frac{4K}{\alpha(G)\varepsilon},\]

_where \(\alpha(G)\) is the independence number of \(G\)._

**Lemma E.2** (Lemma 9 in [1]).: _Let \(G=(V,E)\) be a directed graph with vertex set \(|V|=K\), in which \(i\in N^{\mathrm{in}}(G,i)\) for all \(i\in[K]\). Let \(p\) be an arbitrary distribution over \([K]\). Then, we have_

\[\sum_{i=1}^{K}\frac{p_{i}}{\sum_{j\in N^{\mathrm{in}}(G,i)}p_{j}}\leq\text{mas} (G),\]

_where \(\text{mas}(G)\) is the size of the maximum acyclic subgraphs of \(G\)._