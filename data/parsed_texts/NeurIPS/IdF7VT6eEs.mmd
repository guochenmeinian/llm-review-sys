# Online Performative Gradient Descent for Learning Nash Equilibria in Decision-Dependent Games

Zihan Zhu

Duke University

&Ethan X. Fang

Duke University

&Zhuoran Yang

Yale University

Department of Statistical Science, Duke University. zihan.zhu@duke.edu

Department of Biostatistics and Bioinformatics, Duke University. xingyuan.fang@duke.edu

Department of Statistics and Data Science, Yale University. zhuoran.yang@yale.edu

###### Abstract

We study multi-agent games within the innovative framework of decision-dependent games, which establishes a feedback mechanism that population data reacts to agents' actions and further characterizes the strategic interactions among agents. We focus on finding the Nash equilibrium of decision-dependent games in the bandit feedback setting. However, since agents are strategically coupled, classical gradient-based methods are infeasible without the gradient oracle. To overcome this challenge, we model the strategic interactions by a general parametric model and propose a novel online algorithm, Online Performative Gradient Descent (OPGD), which leverages the ideas of online stochastic approximation and projected gradient descent to learn the Nash equilibrium in the context of function approximation for the unknown gradient. In particular, under mild assumptions on the function classes defined in the parametric model, we prove that the OPGD algorithm finds the Nash equilibrium efficiently for strongly monotone decision-dependent games. Synthetic numerical experiments validate our theory.

## 1 Introduction

The classical theory of learning and prediction fundamentally relies on the assumption that data follows a static distribution. This assumption, however, does not account for many dynamic real-world scenarios where decisions can influence the data involved. Recent literature on performative classification (Hardt et al., 2016; Dong et al., 2018; Miller et al., 2020) and performative prediction (Perdomo et al., 2020) offers a variety of examples where agents are strategic, and data is performative. For instance, in the ride-sharing market, both passengers and drivers engage with multiple platforms using various strategies such as "price shopping". Consequently, these platforms observe performative demands, and the pricing policy becomes strategically coupled.

In this paper, we explore the multi-agent performative prediction problem, specifically, the multi-agent decision-dependent games, as proposed by Narang et al. (2022). We aim to develop algorithms to find Nash equilibria with the first-order oracle. In this scenario, agents can only access their utility functions instead of gradients through the oracle. Finding Nash equilibria in decision-dependent games is a challenging task. Most existing works primarily focus on finding performative stable equilibria within the single-agent setting, an approach that approximates the Nash equilibrium and is relatively straightforward to compute (Mendler-Dunner et al., 2020; Wood et al., 2021; Drusvyatskiy and Xiao, 2022; Brown et al., 2022; Li and Wai, 2022).

There are two major challenges associated with this problem: (i) the distribution shift induced by performative data, and (ii) the lack of first-order information for the performative gradient. To address these two challenges, we propose a novel online gradient-based algorithm, Online Performative Gradient Descent (DPGD). In particular, our algorithm employs a general parametric framework tomodel the decision-dependent distribution, which provides an unbiased estimator for the unknown gradient, and leverages online stochastic approximation methods to estimate the parametric functions.

### Major Contributions

Our work provides new fundamental understandings of decision-dependent games. Expanding upon the linear parametric assumption in Narang et al. (2022), we propose a more comprehensive parametric framework that models decision-dependent distributions of the observed data. We also derive sufficient conditions under this parametric framework that guarantee a strongly monotone decision-dependent game, thereby ensuring a unique Nash equilibrium.

From the algorithmic perspective, we propose OPGD, the first online algorithm to find the Nash equilibrium under linear and kernel parametric models. The core problem in decision-dependent games is estimating the performative gradient. We remark that the existing algorithm only handles the linear case and cannot be extended to the non-linear parametric model (Section 3), and OPGD uses an essentially different method to learn the strategic interaction. To elaborate, under the proposed parametric framework, learning the Nash equilibrium in decision-dependent games can be formulated as a bilevel problem, where the lower level is learning the strategic model and the upper level is finding equilibriums. The OPGD algorithm leverages the ideas of online stochastic approximation for the lower problem and projected gradient descent to learn the Nash equilibrium. Moreover, we acknowledge this learning framework bridges online optimization and statistical learning with time-varying models.

We further prove that under mild assumptions, OPGD converges to the Nash equilibrium. For the linear function class, OPGD achieves a convergence rate of \(\mathcal{O}(t^{-1})\), matching the optimal rate of SGD in the strongly-convex setting, where \(t\) represents the number of iterations. For the kernel function class \(\mathcal{H}\) associated with a bounded kernel \(K\), we posit that the parametric functions reside within the power space \(\mathcal{H}^{\beta}\) and evaluate the approximation error of OPGD under the \(\alpha\)-power norm, where \(\alpha\) represents the minimal value that ensures the power space \(\mathcal{H}^{\alpha}\) possesses a bounded kernel. We present the first analysis for online stochastic approximation under the power norm (Lemma 4), in contrast to the classical RKHS norm (Tarres and Yao, 2014; Pillaud-Vivien et al., 2018; Lei et al., 2021). The difference between the RKHS \(\mathcal{H}\) and the power space \(\mathcal{H}^{\beta}\) makes the standard techniques fail under the power norm, and we use novel proof steps to obtain the estimation error bound. We demonstrate that OPGD leverages the embedding property of the kernel \(K\) to accelerate convergence and achieves the rate of \(\mathcal{O}(t^{-\frac{\beta-\alpha}{\beta-\alpha+2}})\). Moreover, OPGD can handle the challenging scenario, where parametric functions are outside the RKHS. See Section 4.2 for more details.

### Related Work

Performance prediction.The multi-agent decision-dependent game in this paper is inspired by the performative prediction framework (Perdomo et al., 2020). This framework builds upon the pioneering works of strategic classification (Hardt et al., 2016; Dong et al., 2018; Miller et al., 2020), and extends the classical statistical theory of risk minimization to incorporate the performativity of data. Perdomo et al. (2020); Mendler-Dunner et al. (2020); Miller et al. (2021) introduce the concepts of performative optimality and stability, demonstrating that repeated retraining and stochastic gradient methods converge to the performatively stable point. Miller et al. (2021), in pursuit of the performatively optimal point, model the decision-dependent distribution using location families and propose a two-stage algorithm. Similarly, Izzo et al. (2021) develop algorithms to estimate the unknown gradient using finite difference methods. More recently, Narang et al. (2022); Piliouras and Yu (2022) expand the performative prediction to the multi-agent setting, deriving algorithms to find the performatively optimal point.

Learning in continuous games.Our work aligns closely with optimization in continuous games. Rosen (1965) lays the groundwork, deriving sufficient conditions for a unique Nash equilibrium in convex games. For strongly monotone games, Bravo et al. (2018); Mertikopoulos and Zhou (2019); Lin et al. (2021) achieve the convergence rate and iteration complexity of stochastic and derivative-free gradient methods. For monotone games, the convergence of such methods is established by Tatarenko and Kamgarpour (2019, 2020). Additional with bandit feedback settings, zeroth-order methods (or derivative-free methods) achieve convergence (Bravo et al., 2018; Lin et al., 2021; Drusvyatskiy et al., 2022; Narang et al., 2022), albeit with slow convergence rates (Shamir, 2013; Lin et al., 2021; Narang et al., 2022). Relaxing the convex assumption, Ratliff et al. (2016); Agarwal et al. (2019); Cotter et al. (2019) study non-convex continuous games in various settings.

Learning with kernels.Our proposed algorithm closely relies on stochastic approximation, utilizing online kernel regression for the RKHS function class. Prior research investigates the generalization capability of least squares and ridge regression in RKHS De Vito et al. (2005); Caponnetto and De Vito (2007); Smale and Zhou (2007); Rosasco et al. (2010); Mendelson and Neeman (2010). Meanwhile, extensive works study algorithms for kernel regression. For instance, Yao et al. (2007); Dieuleveut and Bach (2016); Pillaud-Vivien et al. (2018); Lin and Rosasco (2017); Lei et al. (2021) propose offline algorithms with optimal convergence rates under the RKHS norm and \(L^{2}\) norm using early stopping and stochastic gradient descent methods, while Ying and Pontil (2008); Tarres and Yao (2014); Dieuleveut and Bach (2016) design online algorithms with optimal convergence rates. The convergence of kernel regression in power norm (or Sobolev norm) is studied in Steinwart et al. (2009); Fischer and Steinwart (2020); Liu and Li (2020); Lu et al. (2022), with offline spectral filter algorithms achieving the statistical optimal rate under the power norm (Pillaud-Vivien et al., 2018; Blanchard and Mucke, 2018; Lin and Cevher, 2020; Lu et al., 2022).

**Notation.** We introduce some useful notation before proceeding. Throughout this paper, we denote the set \(1,2,\cdots,n\) by \([n]\) for any positive integer \(n\). For two positive sequences \(\{a_{n}\}_{n\in\mathbb{N}}\) and \(\{b_{n}\}_{n\in\mathbb{N}}\), we write \(a_{n}=\mathcal{O}(b_{n})\) or \(a_{n}\lesssim b_{n}\) if there exists a positive constant \(C\) such that \(a_{n}\leq C\cdot b_{n}\). For any integer \(d\), we denote the \(d\)-dimensional Euclidean space by \(\mathbb{R}^{d}\), with inner produce \(\langle\cdot,\cdot\rangle\) and the induced norm \(\|\cdot\|=\sqrt{\langle\cdot,\cdot\rangle}\). For a Hilbert space \(\mathcal{H}\), let \(\|\cdot\|_{\mathcal{H}}\) be the associated Hilbert norm. For a set \(\mathcal{X}\) and a probability measure \(\rho_{\mathcal{X}}\) on \(\mathcal{X}\), let \(\mathcal{L}_{\rho_{\mathcal{X}}}^{2}\) be the \(L^{2}\) space on \(\mathcal{X}\) induced by the measure \(\rho_{\mathcal{X}}\), equipped with inner product \(\langle\cdot,\cdot\rangle_{\rho_{\mathcal{X}}}\) and \(L^{2}\) norm \(\|\cdot\|_{\rho_{\mathcal{X}}}=\sqrt{\langle\cdot,\cdot\rangle_{\rho_{ \mathcal{X}}}}\). For any matrix \(A=(a_{ij})\), the Frobenius norm and the operator norm (or spectral norm) of \(A\) are \(\|A\|_{F}=(\sum_{i,j}a_{ij}^{2})^{1/2}\) and \(\|A\|_{\text{op}}=\sigma_{1}(A)\), where \(\sigma_{1}(A)\) stands for the largest singular value of \(A\). For any square matrix \(A=(a_{ij})\), denote its trace by \(\text{tr}(A)=\sum_{i}a_{ii}\). For any \(y\in\mathbb{R}^{d}\), we denote its projection onto a set \(\mathcal{X}\subset\mathbb{R}^{d}\) by \(\text{proj}_{\mathcal{X}}(y)=\arg\min_{x\in\mathcal{X}}\|x-y\|\). The set denoted by \(N_{\mathcal{X}}(x)\) represents the normal cone to a convex set \(\mathcal{X}\) at \(x\in\mathcal{X}\), namely, \(N_{\mathcal{X}}(x)=\{v\in\mathbb{R}^{d}:\langle v,y-x\rangle\leq 0,\;\forall y \in\mathcal{X}\}\). For any metric space \(\mathcal{Z}\) with metric \(d(\cdot,\cdot)\), the symbol \(\mathbb{P}(\mathcal{Z})\) will denote the set of Radon probability measures \(\mu\) on \(\mathcal{Z}\) with a finite first moment \(\mathbb{E}_{z\sim\mu}[d(z,z_{0})]<\infty\) for some \(z_{0}\in\mathcal{Z}\).

## 2 Problem Formulation and Preliminaries

We briefly introduce the formulation of \(n\)-agent decision-dependent games based on Narang et al. (2022). In this setting, each agent \(i\in[n]\) takes the action \(x_{i}\in\mathcal{X}_{i}\) from an action set \(\mathcal{X}_{i}\subset\mathbb{R}^{d_{i}}\). Define the joint action \(x\coloneqq(x_{1},x_{2},\cdots,x_{n})\in\mathcal{X}\) and the joint action set \(\mathcal{X}=\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{n}\subset\mathbb{R} ^{d}\), where \(d\coloneqq\sum_{i=1}^{n}d_{i}\). For all \(i\in[n]\), we write \(x=(x_{i},x_{-i})\), where \(x_{-i}\) denotes the vector of all coordinates except \(x_{i}\). Let \(\mathcal{L}_{i}:\mathcal{X}\to\mathbb{R}\) be the utility function of agent \(i\). In the game, each agent \(i\) seeks to solve the problem

\[\min_{x_{i}\in\mathcal{X}_{i}}\mathcal{L}_{i}(x_{i},x_{-i}),\quad\text{where} \quad\mathcal{L}_{i}(x)\coloneqq\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}( x)}\ell_{i}(x,z_{i}).\] (1)

Here \(z_{i}\in\mathcal{Z}_{i}\) represents the data observed by agent \(i\), where the sample space \(\mathcal{Z}_{i}\) is assumed to be \(\mathcal{Z}_{i}=\mathbb{R}^{p}\) with \(p\in\mathbb{N}\) throughout this paper. Moreover, \(\mathcal{D}_{i}:\mathcal{X}\to\mathbb{P}(\mathcal{Z}_{i})\) is the distribution map, and \(\ell_{i}:\mathbb{R}^{d}\times\mathcal{Z}_{i}\to\mathbb{R}\) denotes the loss function. During play, each agent \(i\) performs an action \(x_{i}\) and observes performative data \(z_{i}\sim\mathcal{D}_{i}(x)\), where the performativity is modeled by the decision-dependent distribution \(\mathcal{D}_{i}(x)\). In the round \(t\), the agent \(i\) only has access to \(z_{i}^{1},\cdots,z_{i}^{t-1}\) as well as \(x^{1},\cdots,x^{t-1}\) and seeks to solve the ERM version of (1). We assume the access to the first-order oracle, namely, loss functions \(\ell_{i}\) are known to agents but distribution maps \(\mathcal{D}_{i}\) are unknown.

**Definition 1**.: _(Nash equilibrium). In the game (1), a joint action \(x^{*}=(x_{1}^{*},x_{2}^{*},\cdots,x_{n}^{*})\) is a Nash equilibrium (Nash Jr, 1996) if all agents play the best response against other agents, namely,_

\[x_{i}^{*}=\mathop{\arg\min}_{x_{i}\in\mathcal{X}_{i}}\mathcal{L}_{i}(x_{i},x_{-i}^ {*})=\mathop{\arg\min}_{x_{i}\in\mathcal{X}_{i}}\mathop{\mathbb{E}}_{z_{i}\sim \mathcal{D}_{i}(x_{i},x_{-i}^{*})}\ell_{i}(x_{i},x_{-i}^{*},z_{i}),\quad\forall i \in[n].\] (2)

In general continuous games, Nash equilibria may not exist or there might be multiple Nash equilibria (Fudenberg and Tirole, 1991). The existence and uniqueness of a Nash equilibrium in a continuous game depend on the game's structure and property. In general, finding the unique Nash equilibrium is only possible for convex and strongly monotone games (Debreu, 1952).

**Definition 2**.: _(Convex game). Game (1) is a convex game if sets \(\mathcal{X}_{i}\) are non-empty, compact, convex and utility functions \(\mathcal{L}_{i}(x_{i},x_{-i})\) are convex in \(x_{i}\) when \(x_{-i}\) are fixed._Suppose that utility functions \(\mathcal{L}_{i}\) are differentiable, we use \(\nabla_{i}\mathcal{L}_{i}(x)\) to denote the gradient of \(\mathcal{L}_{i}(x)\) with respect to \(x_{i}\) (the \(i\)-th individual gradient). We say the game (1) is \(C^{1}\)-smooth if the gradient \(\nabla_{i}\mathcal{L}_{i}(x)\) exists and is continuous for all \(i\in[n]\). Using this notation, we define the gradient \(H(x)\) comprised of individual gradients

\[H(x)\coloneqq(\nabla_{1}\mathcal{L}_{1}(x),\cdots,\nabla_{n}\mathcal{L}_{n}( x)).\]

**Definition 3**.: _(Strongly monotone game). For a constant \(\tau\geq 0\), a \(C^{1}\)-smooth convex game (1) is called \(\tau\)-strongly monotone if it satisfies_

\[\langle H(x)-H(x^{\prime}),x-x^{\prime}\rangle\geq\tau\|x-x^{\prime}\|^{2}, \quad\text{for all }x,x^{\prime}\in\mathcal{X}.\]

Note that a \(\tau\)-strongly monotone game (\(\tau>0\)) over a compact and convex action set \(\mathcal{X}\) admits a unique Nash equilibrium (Rosen, 1965). According to the optimal conditions in convex optimization (Boyd et al., 2004), this Nash equilibrium \(x^{*}\) is characterized by the variational inequality

\[0\in H(x^{*})+N_{\mathcal{X}}(x^{*}).\] (3)

We briefly talk about the challenges and our idea of designing the algorithm. In decision-dependent games, the classical theory of risk minimization does not work. The primary obstacles to finding the Nash equilibrium in the game (1) include: (i) the distribution shift induced by performative data, and (ii) the lack of first-order information (gradient). To make it clear, standard methods, such as gradient-based algorithms, necessitate the gradient \(H(x)\). However, \(H(x)\) is unknown since distributions \(\mathcal{D}_{i}\) are unknown, and estimating \(H(x)\) is complex due to the dependency between \(\mathcal{D}_{i}(x)\) and \(x\). Mathematically, assuming \(C^{1}\)-smoothness, the chain rule directly yields the following expression for the gradient

\[\nabla_{i}\mathcal{L}_{i}(x)=\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}(x) }\nabla_{i}\ell_{i}(x_{i},x_{-i},z_{i})+\frac{d}{du_{i}}\mathop{\mathbb{E}}_{ z_{i}\sim\mathcal{D}_{i}(u_{i},x_{-i})}\ell_{i}(x_{i},x_{-i},z_{i})\Big{|}_{u_{i}=x_{ i}},\] (4)

where \(\nabla_{i}\ell_{i}(x,z_{i})\) denotes the gradient of \(\ell_{i}(x,z_{i})\) with respect to \(x_{i}\). The main difficulty is estimating the second term in (4) due to the absence of closed-form expressions.

To estimate the unknown gradient \(H(x)\), we impose a parametric assumption on the observed data \(z_{i}\) and model the distribution maps \(\mathcal{D}_{i}\) using parametric functions. Note that the linear parametric assumption was first proposed in Narang et al. (2022). In this paper, we extend this assumption to a general framework and show that under the parametric assumption, the gradient \(H(x)\) has a closed-form expression, which yields an unbiased estimator for \(H(x)\).

**Assumption 1**.: _(Parametric assumption). Suppose there exists a function class \(\mathscr{F}\) and \(p\)-dimensional functions \(f_{i}:\mathcal{X}\to\mathbb{R}^{p}\) over the joint action set \(\mathcal{X}\) such that \(f_{i}\in\mathscr{F}^{p}\) and_

\[z_{i}\sim\mathcal{D}_{i}(x)\Longleftrightarrow z_{i}=f_{i}(x)+\epsilon_{i}, \quad\forall i\in[n],\]

_where \(\epsilon_{i}\in\mathbb{R}^{p}\) are zero-mean noise terms with finite variance \(\sigma^{2}\), namely, \(\mathop{\mathbb{E}}\epsilon_{i}=0\) and \(\mathop{\mathbb{E}}\lVert\epsilon_{i}\rVert^{2}\leq\sigma^{2}\)._

Under Assumption 1, assuming that \(f_{i}\) are differentiable and letting \(\mathcal{P}_{i}\) be the distribution of the noise term \(\epsilon_{i}\), we derive the following expression for the utility functions \(\mathcal{L}_{i}(x)=\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}(x)}\ell_{i}( x,z_{i})=\mathop{\mathbb{E}}_{\epsilon_{i}\sim\mathcal{P}_{i}}\ell_{i}(x,f_{i}(x)+ \epsilon_{i})\). Then the individual gradient would be \(\nabla_{i}\mathcal{L}_{i}(x)=\nabla_{i}\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{ D}_{i}(x)}\ell_{i}(x,z_{i})=\nabla_{i}[\mathop{\mathbb{E}}_{\epsilon_{i}\sim \mathcal{P}_{i}}\ell_{i}(x,f_{i}(x)+\epsilon_{i})]\). Consequently, the chain rule directly implies the following expression

\[\nabla_{i}\mathcal{L}_{i}(x)=\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}(x)} \nabla_{i}\ell_{i}(x,z_{i})+\left(\frac{\partial f_{i}(x)}{\partial x_{i}} \right)^{\top}\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}(x)}\nabla_{z_{i}} \ell_{i}(x,z_{i}),\] (5)

where \(\nabla_{z_{i}}\ell_{i}(x,z_{i})\) denotes the gradient of \(\ell_{i}(x,z_{i})\) with respect to \(z_{i}\). Given a joint action \(x\), each agent \(i\) observes data \(z_{i}\sim\mathcal{D}_{i}(x)\). Equation (5) suggests the following unbiased estimator for \(H(x)\):

\[\widehat{H}(x)\coloneqq\left(\widehat{\nabla}_{i}\mathcal{L}_{i}(x)\right)_{i \in[n]}=\left(\nabla_{i}\ell_{i}(x,z_{i})+\left(\frac{\partial f_{i}(x)}{ \partial x_{i}}\right)^{\top}\nabla_{z_{i}}\ell_{i}(x,z_{i})\right)_{i\in[n]}.\] (6)

However, direct computation of \(\widehat{H}(x)\) is infeasible because \(f_{i}\) are unknown. To overcome this challenge, we approximate the unknown functions \(f_{i}\) with the function class \(\mathscr{F}^{p}\). In fact, the estimation of \(f_{i}\) can be formed as a non-parametric regression problem, namely,

\[\hat{f}_{i}=\mathop{\arg\min}_{f\in\mathscr{F}^{p}}\int_{\mathcal{X}\times \mathcal{Z}_{i}}\lVert z_{i}-f(x)\rVert^{2}d\rho_{i},\quad\forall i\in[n],\] (7)

where \(\rho_{i}\) is the joint distribution of \((x,z_{i})\) induced by \(x\sim\rho_{\mathcal{X}}\) and \(z_{i}\sim\mathcal{D}_{i}(x)\). Here \(\rho_{\mathcal{X}}\) is a user-specified sampling distribution on \(\mathcal{X}\).

## 3 The \(\mathtt{OPGD}\) Algorithm

In this section, we consider \(\mathscr{F}\) to be the linear and kernel function classes and derive gradient-based online algorithms to find the Nash equilibrium in the game (1), namely, the Online Performance Gradient Descent (\(\mathtt{DPGD}\)). In each iteration \(t\), assuming that \(x^{i}\coloneqq(x_{1}^{t},\cdots,x_{n}^{t})\) is the output of the previous iteration, \(\mathtt{OPGD}\) performs the following update for all \(i\in[n]\):

1. (Estimation update). Update the estimation of \(f_{i}\) by online stochastic approximation for (7).
2. (Individual gradient update). Compute the estimator (6) and perform projected gradient steps \[x_{i}^{t+1}=\text{proj}_{\mathcal{X}_{i}}(x_{i}^{t}-\eta_{t}\widehat{\nabla}_{ i}\mathcal{L}_{i}(x^{t})),\quad\forall i\in[n].\]

Linear Function Class.Let \(\mathscr{F}\) be the linear function class, namely, \(f_{i}(x)=A_{i}x\) for \(i\in[n]\), where \(A_{i}\in\mathbb{R}^{p\times d}\) are unknown matrices. Then (7) becomes the least square problem \(A_{i}=\arg\min_{A\in\mathbb{R}^{p\times d}}\mathbb{E}_{(u_{i},y_{i})\sim\rho_{ i}}\|y_{i}-A_{i}u_{i}\|^{2}\) with random variables \(u_{i}\sim\rho_{\mathcal{X}},y_{i}\sim\mathcal{D}_{i}(u_{i})\). We use the gradient of the least square objective \(\|y_{i}-A_{i}u_{i}\|^{2}\) to derive the online least square update: \(A^{\text{new}}\gets A-\nu(Au_{i}-y_{i})u_{i}^{\top}\)(Dieuleveut et al., 2017; Narang et al., 2022). In each iteration \(t\), we suppose that \(A_{i}^{t-1}\) is the estimation of \(A_{i}\) from the previous iteration, \(\mathtt{OPGD}\) samples \(u_{i}^{t}\sim\rho_{\mathcal{X}}\) and \(y_{i}^{t}\sim\mathcal{D}_{i}(u_{i}^{t})\) and performs the following estimation update:

\[\text{(i)}\quad A_{i}^{t}=A_{i}^{t-1}-\nu_{t}\left(A_{i}^{t-1}u_{i}^{t}-y_{i}^ {t}\right)(u_{i}^{t})^{\top}.\] (8)

Recalling (5), the individual gradient is \(\nabla_{i}\mathcal{L}_{i}(x)=\mathbb{E}_{z_{i}\sim\mathcal{D}_{i}(x)}\left[ \nabla_{i}\ell_{i}(x,z_{i})+A_{ii}^{\top}\nabla_{z_{i}}\ell_{i}(x,z_{i})\right]\), where \(A_{ii}=\partial f_{i}(x)/\partial x_{i}\in\mathbb{R}^{p\times d_{i}}\) denotes the submatrix of \(A_{i}\) whose columns are indexed by the agent \(i\). After step (i), \(\mathtt{OPGD}\) draws a sample \(z_{i}^{t}\sim\mathcal{D}_{i}(x^{t})\) and compute the estimator (6) to perform the projected gradient step:

\[\text{(ii)}\quad x_{i}^{t+1}=\text{proj}_{\mathcal{X}_{i}}\left(x_{i}^{t}-\eta _{t}\left(\nabla_{i}\ell_{i}(x^{t},z_{i}^{t})+(A_{ii}^{t})^{\top}\nabla_{z_{i} }\ell_{i}(x^{t},z_{i}^{t})\right)\right).\] (9)

Kernel Function Class.Now we consider \(\mathscr{F}\) as the kernel function class, namely, we suppose \(f_{i}\in(\mathcal{H})^{p}\), where \(\mathcal{H}\) is an RKHS induced by a Mercer kernel \(K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) and a user-specified probability measure \(\rho_{\mathcal{X}}\). By the reproducing property of \(\mathcal{H}\), \(f_{i}\) can be represented as \(f_{i}(x)=\langle f_{i},\phi_{x}\rangle_{\mathcal{H}}\), where \(\phi:\mathcal{X}\to\mathcal{H}\) is the feature map, i.e. \(\phi_{x}\coloneqq K(\cdot,x)\in\mathcal{H}\) for any \(x\in\mathcal{X}\). Therefore, (7) becomes the kernel regression \(\arg\min_{f\in\mathscr{F}^{p}}\mathbb{E}_{(u_{i},y_{i})\sim\rho_{i}}\|y_{i}- \langle f,\phi_{u_{i}}\rangle_{\mathcal{H}}\|^{2}\). However, as \(\mathcal{H}\) is generally an infinite-dimensional space, the aforementioned regression problem might lead to ill-posed solutions. Consequently, we consider the regularized kernel ridge regression \(\arg\min_{f\in\mathscr{F}^{p}}\mathbb{E}_{(u_{i},y_{i})\sim\rho_{i}}\|y_{i}- \langle f,\phi_{u_{i}}\rangle_{\mathcal{H}}\|^{2}/2+\lambda_{t}\|f\|_{\mathcal{ H}}^{2}\). In each iteration \(t\), we suppose that \(f_{i}^{t-1}\) is the estimation of \(f_{i}\) from the previous iteration, the \(\mathtt{OPGD}\) algorithm samples \(u_{i}^{t}\sim\rho_{\mathcal{X}},y_{i}^{t}\sim\mathcal{D}_{i}(u_{i}^{t})\) and takes gradient steps on the kernel ridge objective \(\|y_{i}^{t}-\langle f,\phi_{u_{i}^{t}}\rangle_{\mathcal{H}}\|^{2}/2+\lambda_{t} \|f\|_{\mathcal{H}}^{2}\), i.e. it takes the online kernel ridge update (Tarres and Yao, 2014; Dieuleveut and Bach, 2016):

\[\text{(i)}\quad f_{i}^{t}=f_{i}^{t-1}-\nu_{t}\left[\left(f_{i}^{t-1}(u_{i}^{t}) -y_{i}^{t}\right)\phi_{u_{i}^{t}}+\lambda_{t}f_{i}^{t-1}\right].\] (10)

We suppose that the kernel \(K\) is 2-differentiable, i.e., \(K\in C^{2}(\mathcal{X},\mathcal{X})\). Define \(\partial_{i}\phi:\mathcal{X}\to\mathcal{H}\) as the partial derivative of the feature map \(\phi\) with respect to \(x_{i}\), namely, \(\partial_{i}\phi_{x}=\partial_{i}K(x,\cdot)=\partial K(x,\cdot)/\partial x_{i}\). Steinwart and Christmann (2008, Lemma 4.34) shows that \(\partial_{i}\phi_{x}\) exists, continuous and \(\partial_{i}\phi_{x}\in\mathcal{H}\). By the reproducing property \(\partial f_{i}(x)/\partial x_{i}=\partial\langle f_{i},\phi_{x}\rangle_{ \mathcal{H}}/\partial x_{i}=\langle f_{i},\partial_{i}\phi_{x}\rangle_{\mathcal{H}}\), the individual gradient \(\nabla_{i}\mathcal{L}_{i}(x)\) has the form \(\nabla_{i}\mathcal{L}_{i}(x)=\mathbb{E}_{z_{i}\sim\mathcal{D}_{i}(x)}[\nabla_{i} \ell_{i}(x,z_{i})+\left(\langle f_{i},\partial_{i}\phi_{x}\rangle_{\mathcal{H}} \right)^{\top}\nabla_{z_{i}}\ell_{i}(x,z_{i})]\). After step (i), \(\mathtt{OPGD}\) draws a sample \(z_{i}^{t}\sim\mathcal{D}_{i}(x^{t})\) and performs the projected gradient step:

\[\text{(ii)}\quad x_{i}^{t+1}\leftarrow\text{proj}_{\mathcal{X}_{i}}\left(x_{i}^{t }-\eta_{t}\left(\nabla_{i}\ell_{i}(x^{t},z_{i}^{t})+(\langle f_{i}^{t},\partial_{i} \phi_{x^{t}}\rangle_{\mathcal{H}})^{\top}\nabla_{z_{i}}\ell_{i}(x^{t},z_{i}^{t}) \right)\right).\] (11)

We remark that the gradient steps \(\eta_{t},\nu_{t}\) and regularization terms \(\lambda_{t}\) should be chosen carefully to ensure convergence (see Theorem 2). Specifically, the regularization terms \(\lambda_{t}\) must shift to \(0\) gradually. If \(\lambda_{t}\) is a constant, \(f_{i}^{t}\) in (10) converges to the solution of a regularized kernel ridge regression, which is a biased estimator of \(f_{i}\). Thus (11) fails to converge because the gradient estimation has a constant bias. We present the pseudocode of \(\mathtt{DPGD}\) for the linear setting as Algorithm 1 and for the RKHS setting as Algorithm 2 in Appendix A.

Comparison with Narang et al. (2022).We clarify the difference between \(\mathtt{OPGD}\) and the Adaptive Gradient Method (AGM) proposed in Narang et al. (2022). To elaborate, AGM samples \(z_{i}^{t}\sim\mathcal{D}_{i}(x^{t})\) at current the action and let agents play again with an injected noise \(u^{t}\) to obtain \(q_{i}^{t}\sim\mathcal{D}_{i}(x^{t}+u^{t})\). The algorithm is based on the fact that \(\mathbb{E}[q_{i}^{t}-z_{i}^{t}|u^{t},x^{t}]=A_{i}u^{t}\), which is not related to \(x\). Thus, \(A_{i}\) can be estimated by online least squares. We remark that \(\mathbb{E}[q_{i}^{t}-z_{i}^{t}|u^{t},x^{t}]\) depends on agents' actions in the non-linear (RKHS) cases, because \(\mathbb{E}[q_{i}^{t}-z_{i}^{t}|u^{t},x^{t}]=f_{i}(x^{t}+u^{t})-f_{i}(x^{t})= \langle f_{i},\phi_{x^{t}+u^{t}}-\phi_{x^{t}}\rangle_{\mathcal{H}}\). Thus, the change of action will bring additional error that makes the estimation fail to converge. In contrast, \(\mathtt{OPGD}\) lets agents play \(u_{i}^{t}\sim\rho_{\mathcal{X}}\) to explore the action space and learn the strategic behavior of other agents. \(\mathtt{OPGD}\) estimates the parametric function by solving the ERM version of (7) using online stochastic approximation (8) and (10). This learning framework can be applied to RKHS and potentially beyond that, such as overparameterized neural networks using the technique of neural tangent kernel (Allen-Zhu et al., 2019).

## 4 Theoretical Results

We provide theoretical guarantees for \(\mathtt{OPGD}\) in both linear and RKHS settings. We first impose some mild assumptions. Similar assumptions are adopted in Mendler-Dunner et al. (2020); Izzo et al. (2021); Narang et al. (2022); Cutler et al. (2022).

**Assumption 2**.: _(\(\tau\)-strongly monotone). The game (1) is \(\tau\)-strongly monotone._

**Assumption 3**.: _(Smoothness). \(H(x)\) is \(L\)-Lipschitz continuous:_

\[H(x_{1})-H(x_{2})\leq L\|x_{1}-x_{2}\|,\quad\quad\forall x_{1},x_{2}\in \mathcal{X}.\]

**Assumption 4**.: _(Lipschitz continuity in \(z\)). Define \(\mathcal{D}=\mathcal{D}_{1}\times\mathcal{D}_{2}\times\cdots\times\mathcal{D} _{n}:\mathcal{X}\rightarrow\mathbb{P}(\mathcal{Z})\), where \(\mathcal{Z}\) is the sample space \(\mathcal{Z}=\mathcal{Z}_{1}\times\mathcal{Z}_{2}\times\cdots\times\mathcal{Z }_{n}\). For all \(i\in[n],x\in\mathcal{X}\), there exists a constant \(\delta>0\),_

\[\mathop{\mathbb{E}}_{z\sim\mathcal{D}(x)}\sqrt{\sum_{i=1}^{n}\lVert\nabla_{z_ {i}}\ell_{i}(x,z_{i})\rVert^{2}}\leq\delta.\]

**Assumption 5**.: _(Finite variance). There exists a constant \(\zeta>0\),_

\[\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}(x)}\lVert\nabla_{i,z_{i}}\ell_{ i}(x,z_{i})-\mathop{\mathbb{E}}_{z_{i}\sim\mathcal{D}_{i}(x)}\nabla_{i,z_{i}} \ell_{i}(x,z_{i})\rVert^{2}\leq\zeta^{2},\quad\forall i\in[n],\forall x\in \mathcal{X},\]

_where \(\nabla_{i,z_{i}}\ell_{i}\) denotes the gradient of \(\ell_{i}(x,z_{i})\) with respect to \(x_{i}\) and \(z_{i}\)._

We remark that Assumption 3 is the standard smoothness assumption for the utility functions \(\mathcal{L}_{i}(x)\)(Boyd et al., 2004; Nesterov et al., 2018). Since \(\mathcal{X}\) is a compact set within \(\mathbb{R}^{d}\), Assumption 4 holds if \(\ell_{i}(x,z_{i})\) is Lipschitz continuous in \(z_{i}\) and the gradient \(\nabla_{z_{i}}\ell_{i}(x,z_{i})\) is continuous in \(x\), and Assumption 5 holds if \(\ell_{i}(x,z_{i})\) is Lipschitz in \(x\) and \(z_{i}\) (thus \(\nabla_{i,z_{i}}\ell_{i}(x,z_{i})\) has a bounded norm). Assumption 5 implies that the variances of \(\nabla_{i}\ell_{i}(x,z_{i})\) and \(\nabla_{z_{i}}\ell_{i}(x,z_{i})\) are both bounded by \(\zeta^{2}\) for any \(x\in\mathcal{X}\) and \(z_{i}\sim\mathcal{D}_{i}(x)\). We provide sufficient conditions for Assumption 2 in Appendix B.1.

### Convergence Rate in the Linear Setting

We introduce two assumptions necessary to derive theoretical guarantees for the linear function class.

**Assumption 6**.: _(Linear assumption). Suppose that the parametric assumption holds (Assumption 1) and \(f_{i}(x)=A_{i}x\) for \(i\in[n]\), where \(A_{i}\in\mathbb{R}^{p\times d}\) are unknown matrices._

**Assumption 7**.: _(Sufficiently isotropic). There exists constants \(l_{1},l_{2},R>0\) such that_

\[l_{1}I\preceq\mathbb{E}_{u\sim\rho_{\mathcal{X}}}uu^{\top},\quad\mathbb{E}_{u \sim\rho_{\mathcal{X}}}\lVert u\rVert^{2}\leq l_{2},\quad\mathbb{E}_{u\sim\rho _{\mathcal{X}}}\left[\lVert u\rVert^{2}uu^{\top}\right]\preceq R\mathbb{E}_ {u\sim\rho_{\mathcal{X}}}uu^{\top}.\]

Assumption 7 has been studied in the literature on online least squares regression (Dieuleveut et al., 2017; Narang et al., 2022). Essentially, this requires the distribution \(\rho_{\mathcal{X}}\) to be sufficiently isotropic and non-singular, and it ensures the random variable \(u_{i}^{t}\sim\rho_{\mathcal{X}}\) in the online estimation update step (8) can explore all the "directions" of \(\mathbb{R}^{p}\). A simple example that satisfies Assumption 7 is the uniform distribution \(\rho_{\mathcal{X}}=\mathcal{U}[0,1]\), in which case \(l_{1}=l_{2}=1/3,R=3/5\).

The next theorem provides the convergence rate of \(\mathtt{OPGD}\) under the linear setting.

**Theorem 1**.: _(Convergence in the linear setting). Suppose that Assumptions 2, 3, 4, 5, 6, and 7 hold. Set \(\eta_{t}=2/(\tau(t+t_{0}))\), \(\nu_{t}=2/(l_{1}(t+t_{0}))\), where \(t_{0}\) is a constant that satisfies \(t_{0}\geq 2l_{2}R/l_{1}^{2}\). For all iterations \(t\geq 1\), the \(x^{t}\) generated by the OPGD algorithm in Section 3 for linear function class satisfies_

\[\mathbb{E}\|x^{t}-x^{*}\|^{2}\leq \frac{(4D_{1}+2D_{2}(t_{0}+1)\tau)(t_{0}+2)^{2}/(t_{0}+1)^{2}}{\tau ^{2}(t+t_{0})}+\frac{(t_{0}+1)^{2}\|x^{1}-x^{*}\|^{2}}{(t+t_{0})^{2}},\] (12)

_where \(D_{1}\) and \(D_{2}\) are constants that_

\[D_{1}\coloneqq 4\zeta^{2}(1+2(M/(t_{0}+1)+\sup_{i\in[n]}\|A_{i} \|_{F}^{2})), D_{2}\coloneqq 2\delta^{2}M, M\coloneqq\frac{2t_{0}^{4}\sum_{i=1}^{n}\|A_{i}^{0}-A_{i}\|_{F}^{2}}{(t _{0}+1)^{3}}+\frac{8nl_{2}\sigma^{2}(t_{0}+2)^{2}}{l_{1}^{2}(t_{0}+1)^{2}}.\]

We refer the reader to Appendix C.1 for complete proof. Next, we illustrate the parameters involved in Theorem 1: \(\tau\) is the strongly monotone parameter of the game (1), \(l_{1},l_{2},R\) are intrinsic parameters describing the isotropy of the distribution \(\rho_{\mathcal{X}}\) (Assumption 7), \(\sigma^{2}\) is the variance of the noise term \(\epsilon_{i}\) defined in Assumption 1, \(\zeta\) and \(\delta\) describe the continuity of \(\ell_{i}\) (Assumption 4, 5), \(t_{0}\) is a sufficiently large value, \(A_{i}^{0}\) is the initial estimation of \(A_{i}\), \(x^{1}\) is the initial input. Theorem 1 is a combination of Lemma 2 and Lemma 3, where Lemma 2 is the statistical error of the online approximation step (8) and Lemma 3 is the one-step optimization error of the projected gradient step (9). Theorem 1 implies the convergence rate of OPGD in the linear setting is \(\mathcal{O}(t^{-1})\), which matches the optimal rate of stochastic gradient descent in the strongly-convex setting.

### Convergence Rate in the RKHS Setting

Suppose that \(K:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) is a continuous Mercer kernel, by Mercer's theorem, it has the spectral representation \(K=\sum_{i=1}^{\infty}\mu_{i}e_{i}\otimes e_{i}\), where \(\{\mu_{i}\}_{i=1}^{\infty}\) are eigenvalues, \(\{e_{i}\}_{i=1}^{\infty}\) are eigenfunctions, and \(\otimes\) denotes the tensor product. Moreover, \(\{e_{i}\}_{i=1}^{\infty}\) is an orthogonal basis of \(\mathcal{L}_{\rho_{\mathcal{X}}}^{2}\) and \(\{\mu_{i}^{1/2}e_{i}\}_{i=1}^{\infty}\) is the orthogonal basis of \(\mathcal{H}\), which induces the representation \(\mathcal{H}=\{\sum_{i=1}^{\infty}a_{i}\mu_{i}^{1/2}e_{i}:\{a_{i}\}_{i=1}^{ \infty}\in\ell^{2}\}\).

**Definition 4**.: _(Power space). For a constant \(\alpha\geq 0\), the \(\alpha\)-power space of an RKHS \(\mathcal{H}\) is defined by_

\[\mathcal{H}^{\alpha}=\left\{\sum_{i=1}^{\infty}a_{i}\mu_{i}^{\alpha/2}e_{i}:\{ a_{i}\}_{i=1}^{\infty}\in\ell^{2}\right\},\]

_equipped with the \(\alpha\)-power norm \(\|\cdot\|_{\alpha}\) and inner product \(\langle\cdot,\cdot\rangle_{\alpha}\), where \(\|\sum_{i=1}^{\infty}a_{i}\mu_{i}^{\alpha/2}e_{i}\|_{\alpha}\coloneqq\left( \sum_{i=1}^{\infty}a_{i}^{2}\right)^{1/2}\) and \(\langle\sum_{i=1}^{\infty}a_{i}\mu_{i}^{\alpha/2}e_{i},\sum_{i=1}^{\infty}b_ {i}\mu_{i}^{\alpha/2}e_{i}\rangle_{\alpha}=\sum_{i=1}^{\infty}a_{i}b_{i}\)._

We remark that: (i) \(\mathcal{H}^{1}=\mathcal{H}\) and \(\mathcal{H}^{\alpha}\subset\mathcal{H}^{\beta}\) for any \(\alpha>\beta\), (ii) \(\|\cdot\|_{1}=\|\cdot\|_{\mathcal{H}}\) and \(\|\cdot\|_{0}=\|\cdot\|_{\rho_{\mathcal{X}}}\), and (iii) \(\mathcal{H}^{\alpha}\) is an RKHS on \(\mathcal{X}\) with kernel \(K^{\alpha}\coloneqq\sum_{i=1}^{\infty}\mu_{i}^{\alpha}e_{i}\otimes e_{i}\) and measure \(\rho_{\mathcal{X}}\). We review more properties of RKHS and power spaces in Appendix Sections B.3 and B.4.

We present assumptions on the kernel function class, similar assumptions can be found in the literature on kernel regression and stochastic approximation (Caponnetto and De Vito, 2007; Steinwart et al., 2009; Dicker et al., 2017; Pillaud-Vivien et al., 2018; Fischer and Steinwart, 2020).

**Assumption 8**.: _(Source condition). Suppose Assumption 1 holds and there exists an RKHS, \(\mathcal{H}\), with a bounded differentiable Mercer kernel, \(K\), and constants \(\beta,\kappa>0\) such that \(\sup_{x\in\mathcal{X}}K(x,x)\leq\kappa^{2}\) and \(f_{i}\in\left(\mathcal{H}^{\beta}\right)^{p}\) for all \(i\in[n]\)._

**Assumption 9**.: _(Embedding property). There exist constants \(\alpha\in(0,1],A>0\) such that \(K^{\alpha}(x,x)=\sum_{i=1}^{\infty}\mu_{i}^{\alpha}e_{i}^{2}(x)\leq A^{2}\), for all \(x\in\mathcal{X}\)._

**Assumption 10**.: _(Lipschitz kernel). Suppose Assumption 9 holds and there exists \(\xi>0\) such that \(\|\partial_{i}\phi_{x}^{\alpha}\|_{\alpha}\leq\xi\) for any \(i\in[n]\) and \(x\in\mathcal{X}\), where \(\phi_{x}^{\alpha}:\mathcal{X}\to\mathcal{H}^{\alpha}\) is the feature map of the kernel \(K^{\alpha}\)._

Assumption 8 holds when \(K\) is bounded, differentiable, and each coordinate of parametric functions \(f_{i}\) lies in the power space \(\mathcal{H}^{\beta}\). When \(\beta<1\), Assumption 8 includes the challenging scenario, namely, \(f_{i}\notin(\mathcal{H})^{p}\). Assumption 9 holds if there exists a power space \(\mathcal{H}^{\alpha}\) such that the kernel \(K^{\alpha}\) is bounded. Thus, Assumption 9 holds with \(\alpha=1\) for any bounded kernel \(K\). We further propose Proposition 1 as sufficient conditions for the embedding property following Mendelson and Neeman (2010). Recalling the definition of partial derivative \(\partial_{i}\phi^{\alpha}:\mathcal{X}\to\mathcal{H}^{\alpha}\) (Section 3), Assumption 10 holds if \(\partial_{i}\partial_{i+d}K^{\alpha}(x,x)=\|\partial_{i}\phi_{x}^{\alpha}\|_{ \alpha}^{2}\leq\xi^{2}\) for any \(x\in\mathcal{X}\), i.e. it holds for any Lipschitz kernel \(K^{\alpha}\).

[MISSING_PAGE_FAIL:8]

Linear function class.Let the loss function be \(\ell_{i}(x,z_{i})=-z_{i}+x_{i}^{2}\) and set the linear parametric function as \(f_{1}(x)=x_{1}\) and \(f_{2}(x)=2x_{2}\), namely, the parametric model is \(z_{i}=A_{i}x+\epsilon_{i}\) where \(A_{1}=[1\ 0]\) and \(A_{2}=[0\ 2]\). The the game (14) has the gradient \(H(x)=(2x_{1}-1,2x_{2}-2)\), therefore, the game (14) is convex, \(C^{1}\)-smooth, \(1\)-strongly monotone and the Nash equilibrium is \(x^{*}=(1/2,1)\). We set the sampling distribution as \(\rho_{\mathcal{X}}=\mathcal{U}[0,1]\times\mathcal{U}[0,1]\), the initial point as \(x^{0}\sim\rho_{\mathcal{X}}\), and the initial estimation as zero. Moreover, letting \(t_{0}=10\), we set the gradient step sizes as \(\eta_{t}=6/(t+t_{0}),\nu_{t}=6/(t+t_{0})\).

Kernel function class.Let \(\mathcal{X}=[0,1]\times[0,1]\), \(\rho_{\mathcal{X}}=\mathcal{U}[0,1]\times\mathcal{U}[0,1]\), and define the kernel \(Q((x_{1},x_{2}),(y_{1},y_{2}))=K(x_{1},y_{1})\cdot K(x_{2},y_{2})\) as the product kernel of \(K(x,y)=40B_{4}(\{x-y\})\). Suppose that \(\mathcal{H}\) is the RKHS on \(\mathcal{X}\) induced by the kernel \(Q\) and the distribution \(\rho_{\mathcal{X}}\). Set the parametric function as the product of two \(3\)-order Bernoulli polynomials, namely, \(f(x_{1},x_{2})=B_{3}(x_{1})\cdot B_{3}(x_{2})=(x_{1}^{3}-3x_{1}^{2}/2+x_{1}/2) \cdot(x_{2}^{3}-3x_{2}^{2}/2+x_{2}/2)\). Set \(\ell_{i}(x,z_{i})=-z_{i}+\cos(2\pi x_{1})\cos(2\pi x_{2})-x_{i}+x_{i}^{2}\) and let \(f_{i}(x)=\cos(2\pi x_{1})\cos(2\pi x_{2})\) for \(i\in[2]\). Then the gradient of this game is \(H(x)=(2x_{1}-1,2x_{2}-1)\), thus, this game is convex, \(C^{1}\)-smooth, \(1\)-strongly monotone and the Nash equilibrium is \(x^{*}=(0.5,0.5)\). Following Example 1, Assumption 8, 9, 10 hold for any \(\beta>1\) and any \(\alpha>1/4\). Set \(t_{0}=10\), \(a=7\), \(\eta_{t}=6/(t+t_{0}),\nu_{t}=a/(t+t_{0})^{3/4}\), and \(\lambda_{t}=1/(a(t+t_{0})^{1/4})\). Following Theorem 2, the convergence rate is \(\mathcal{O}(t^{-1/2})\).

Results.We perform experiments for both parametric settings to verify the convergence rates and compare the theoretical and simulated rates, as shown in Figure 1, where both X and Y axes take the log scale. Figure 1(a) shows the converge rate of the linear setting within \(10,000\) iterations, the simulated rate matches our prediction, i.e. it is close to \(\mathcal{O}(t^{-1})\). Figure 1(b) shows the convergence rate of the RKHS setting, it implies that the simulated rate is close to the theoretical rate \(\mathcal{O}(t^{-1/2})\) when the iteration \(t\) is larger than \(1,000\). These results validate Theorems 1 and 2.

## 6 Conclusion and Discussion

In this paper, we study the problem of learning Nash equilibria in multi-agent decision-dependent games with access to the first-order oracle. We propose a parametric assumption to handle the distribution shift and develop a novel online algorithm OPGD  in both the linear and RKHS settings. We derive sufficient conditions to ensure the decision-dependent game is strongly monotone under the parametric assumption. We show that OPGD  converges to the Nash equilibrium at a rate of \(\mathcal{O}(t^{-1})\) in the linear setting and \(\mathcal{O}(t^{-\frac{\beta-\alpha}{\beta-\alpha+2}})\) in the RKHS setting.

### Acknowledgement

We would like to thank the area chair, and three anonymous reviewers for their very helpful comments, which help us substantially improve our paper. E. X. Fang is partially supported by NSF DMS-2230795 and DMS-2230797.

Figure 1: **(a) Linear setting:** The X-axis represents the iteration from \(1\) to \(10,000\), while the Y-axis represents the norm-squared error of \(x^{t}\) to the Nash equilibrium \(x^{*}=(1/2,1)\), averaged over \(20\) random seeds. The blue solid line represents the output of OPGD  and the orange dashed line represents the theoretical rate \(\mathcal{O}(t^{-1})\). **(b) RKHS setting:** The X-axis represents the iteration from \(1\) to \(10,000\), while the Y-axis represents the norm-squared error to the Nash equilibrium \(x^{*}=(1/2,1/2)\), averaged over \(400\) random seeds. The blue solid line represents the output of OPGD  and the orange dashed line represents the theoretical rate \(\mathcal{O}(t^{-1/2})\).

## References

* Abramowitz et al. (1988) Abramowitz, M., Stegun, I. A. and Romer, R. H. (1988). Handbook of mathematical functions with formulas, graphs, and mathematical tables.
* Adams and Fournier (2003) Adams, R. A. and Fournier, J. J. (2003). _Sobolev spaces_. Elsevier.
* Agarwal et al. (2019) Agarwal, N., Gonen, A. and Hazan, E. (2019). Learning in non-convex games with an optimization oracle. In _Conference on Learning Theory_. PMLR.
* Allen-Zhu et al. (2019) Allen-Zhu, Z., Li, Y. and Liang, Y. (2019). Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, **32**.
* Blanchard and Mucke (2018) Blanchard, G. and Mucke, N. (2018). Optimal rates for regularization of statistical inverse learning problems. _Foundations of Computational Mathematics_, **18** 971-1013.
* Boyd et al. (2004) Boyd, S., Boyd, S. P. and Vandenberghe, L. (2004). _Convex optimization_. Cambridge university press.
* Bravo et al. (2018) Bravo, M., Leslie, D. and Mertikopoulos, P. (2018). Bandit learning in concave n-person games. _Advances in Neural Information Processing Systems_, **31**.
* Brown et al. (2022) Brown, G., Hod, S. and Kalemaj, I. (2022). Performative prediction in a stateful world. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Caponnetto and De Vito (2007) Caponnetto, A. and De Vito, E. (2007). Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, **7** 331-368.
* Cotter et al. (2019) Cotter, A., Jiang, H. and Sridharan, K. (2019). Two-player games for efficient non-convex constrained optimization. In _Algorithmic Learning Theory_. PMLR.
* Cucker and Smale (2002) Cucker, F. and Smale, S. (2002). On the mathematical foundations of learning. _Bulletin of the American mathematical society_, **39** 1-49.
* Cutler et al. (2022) Cutler, J., Diaz, M. and Drusvyatskiy, D. (2022). Stochastic approximation with decision-dependent distributions: asymptotic normality and optimality. _arXiv preprint arXiv:2207.04173_.
* De Vito et al. (2005) De Vito, E., Rosasco, L., Caponnetto, A., De Giovannini, U., Odone, F. and Bartlett, P. (2005). Learning from examples as an inverse problem. _Journal of Machine Learning Research_, **6**.
* Debreu (1952) Debreu, G. (1952). A social equilibrium existence theorem. _Proceedings of the National Academy of Sciences_, **38** 886-893.
* Dicker et al. (2017) Dicker, L. H., Foster, D. P. and Hsu, D. (2017). Kernel ridge vs. principal component regression: Minimax bounds and the qualification of regularization operators. _Electronic Journal of Statistics_, **11** 1022-1047.

* Dieuleveut et al. (2017) Dieuleveut, A., Flammarion, N. and Bach, F. (2017). Harder, better, faster, stronger convergence rates for least-squares regression. _The Journal of Machine Learning Research_, **18** 3520-3570.
* Dong et al. (2018) Dong, J., Roth, A., Schutzman, Z., Waggoner, B. and Wu, Z. S. (2018). Strategic classification from revealed preferences. In _Proceedings of the 2018 ACM Conference on Economics and Computation_.
* Drusvyatskiy et al. (2022) Drusvyatskiy, D., Fazel, M. and Ratliff, L. J. (2022). Improved rates for derivative free gradient play in strongly monotone games. In _2022 IEEE 61st Conference on Decision and Control (CDC)_. IEEE.
* Drusvyatskiy and Xiao (2022) Drusvyatskiy, D. and Xiao, L. (2022). Stochastic optimization with decision-dependent distributions. _Mathematics of Operations Research_.
* Fischer and Steinwart (2020) Fischer, S. and Steinwart, I. (2020). Sobolev norm learning rates for regularized least-squares algorithms. _The Journal of Machine Learning Research_, **21** 8464-8501.
* Fudenberg and Tirole (1991) Fudenberg, D. and Tirole, J. (1991). _Game theory_. MIT press.
* Hardt et al. (2016) Hardt, M., Megiddo, N., Papadimitriou, C. and Wootters, M. (2016). Strategic classification. In _Proceedings of the 2016 ACM conference on innovations in theoretical computer science_.
* Izzo et al. (2021) Izzo, Z., Ying, L. and Zou, J. (2021). How to learn when data reacts to your model: performative gradient descent. In _International Conference on Machine Learning_. PMLR.
* Ioffe and Szegedy (2015)Lei, Y., Hu, T. and Tang, K. (2021). Generalization performance of multi-pass stochastic gradient descent with convex loss functions. _The Journal of Machine Learning Research_, **22** 1145-1185.
* Li and Wai (2022) Li, Q. and Wai, H.-T. (2022). State dependent performative prediction with stochastic approximation. In _International Conference on Artificial Intelligence and Statistics_. PMLR.
* Lin and Cevher (2020) Lin, J. and Cevher, V. (2020). Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. _The Journal of Machine Learning Research_, **21** 5852-5914.
* Lin and Rosasco (2017) Lin, J. and Rosasco, L. (2017). Optimal rates for multi-pass stochastic gradient methods. _The Journal of Machine Learning Research_, **18** 3375-3421.
* Lin et al. (2021) Lin, T., Zhou, Z., Ba, W. and Zhang, J. (2021). Optimal no-regret learning in strongly monotone games with bandit feedback. _arXiv preprint arXiv:2112.02856_.
* Liu and Li (2020) Liu, Z. and Li, M. (2020). On the estimation of derivatives using plug-in krr estimators. _arXiv preprint arXiv:2006.01350_.
* Lu et al. (2022) Lu, Y., Blanchet, J. and Ying, L. (2022). Sobolev acceleration and statistical optimality for learning elliptic equations via gradient descent. _arXiv preprint arXiv:2205.07331_.
* Mendelson and Neeman (2010) Mendelson, S. and Neeman, J. (2010). Regularization in kernel learning. _Annals of statistics_, **38** 526-565.
* Mendler-Dunner et al. (2020) Mendler-Dunner, C., Perdomo, J., Zrnic, T. and Hardt, M. (2020). Stochastic optimization for performative prediction. _Advances in Neural Information Processing Systems_, **33** 4929-4939.
* Mertikopoulos and Zhou (2019) Mertikopoulos, P. and Zhou, Z. (2019). Learning in games with continuous action sets and unknown payoff functions. _Mathematical Programming_, **173** 465-507.
* Miller et al. (2020) Miller, J., Milli, S. and Hardt, M. (2020). Strategic classification is causal modeling in disguise. In _International Conference on Machine Learning_. PMLR.
* Miller et al. (2021) Miller, J. P., Perdomo, J. C. and Zrnic, T. (2021). Outside the echo chamber: Optimizing the performative risk. In _International Conference on Machine Learning_. PMLR.
* Narang et al. (2022) Narang, A., Faulkner, E., Drusvyatskiy, D., Fazel, M. and Ratliff, L. J. (2022). Multiplayer performative prediction: Learning in decision-dependent games. _arXiv preprint arXiv:2201.03398_.
* Nash Jr (1996) Nash Jr, J. (1996). Non-cooperative games. In _Essays on Game Theory_. Edward Elgar Publishing, 22-33.
* Nesterov et al. (2018) Nesterov, Y. et al. (2018). _Lectures on convex optimization_, vol. 137. Springer.
* Perdomo et al. (2020) Perdomo, J., Zrnic, T., Mendler-Dunner, C. and Hardt, M. (2020). Performative prediction. In _International Conference on Machine Learning_. PMLR.
* Piliouras and Yu (2022) Piliouras, G. and Yu, F.-Y. (2022). Multi-agent performative prediction: From global stability and optimality to chaos. _arXiv preprint arXiv:2201.10483_.
* Pillaud-Vivien et al. (2018) Pillaud-Vivien, L., Rudi, A. and Bach, F. (2018). Statistical optimality of stochastic gradient descent on hard learning problems through multiple passes. _Advances in Neural Information Processing Systems_, **31**.
* Ratliff et al. (2016) Ratliff, L. J., Burden, S. A. and Sastry, S. S. (2016). On the characterization of local nash equilibria in continuous games. _IEEE transactions on automatic control_, **61** 2301-2307.
* Rosasco et al. (2010) Rosasco, L., Belkin, M. and De Vito, E. (2010). On learning with integral operators. _Journal of Machine Learning Research_, **11**.
* Rosen (1965) Rosen, J. B. (1965). Existence and uniqueness of equilibrium points for concave n-person games. _Econometrica: Journal of the Econometric Society_ 520-534.
* Shamir (2013) Shamir, O. (2013). On the complexity of bandit and derivative-free stochastic convex optimization. In _Conference on Learning Theory_. PMLR.
* Smale and Zhou (2007) Smale, S. and Zhou, D.-X. (2007). Learning theory estimates via integral operators and their approximations. _Constructive approximation_, **26** 153-172.
* Steinwart and Christmann (2008) Steinwart, I. and Christmann, A. (2008). _Support vector machines_. Springer Science & Business Media.
* Steinwart et al. (2009) Steinwart, I., Hush, D. R., Scovel, C. et al. (2009). Optimal rates for regularized least squares regression. In _COLT_.
* Steinwart et al. (2018)* Steinwart and Scovel (2012) Steinwart, I. and Scovel, C. (2012). Mercer's theorem on general domains: On the interaction between measures, kernels, and rkhss. _Constructive Approximation_, **35** 363-417.
* Tarres and Yao (2014) Tarres, P. and Yao, Y. (2014). Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence. _IEEE Transactions on Information Theory_, **60** 5716-5735.
* Tatarenko and Kamgarpour (2019) Tatarenko, T. and Kamgarpour, M. (2019). Learning nash equilibria in monotone games. In _2019 IEEE 58th Conference on Decision and Control (CDC)_. IEEE.
* Tatarenko and Kamgarpour (2020) Tatarenko, T. and Kamgarpour, M. (2020). Bandit online learning of nash equilibria in monotone games. _arXiv preprint arXiv:2009.04258_.
* Triebel (1995) Triebel, H. (1995). Interpolation theory, function spaces. _Differential Operators_.
* Triebel (2010) Triebel, H. (2010). _Theory of Function Spaces_. Modern Birkhauser Classics, Springer Basel.
* Wahba (1990) Wahba, G. (1990). _Spline models for observational data_. SIAM.
* Wood et al. (2021) Wood, K., Bianchin, G. and Dall'Anese, E. (2021). Online projected gradient descent for stochastic optimization with decision-dependent distributions. _IEEE Control Systems Letters_, **6** 1646-1651.
* Yao et al. (2007) Yao, Y., Rosasco, L. and Caponnetto, A. (2007). On early stopping in gradient descent learning. _Constructive Approximation_, **26** 289-315.
* Ying and Pontil (2008) Ying, Y. and Pontil, M. (2008). Online gradient descent learning algorithms. _Foundations of Computational Mathematics_, **8** 561-596.