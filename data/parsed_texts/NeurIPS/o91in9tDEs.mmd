# Distributional Policy Evaluation: a Maximum Entropy approach to Representation Learning

Riccardo Zamboni

DEIB, Politecnico di Milano

Milan, Italy

riccardo.zamboni@polimi.it

&Alberto Maria Metelli

DEIB, Politecnico di Milano

Milan, Italy

albertomaria.metelli@polimi.it

&Marcello Restelli

DEIB, Politecnico di Milano

Milan, Italy

marcello.restelli@polimi.it

###### Abstract

The Maximum Entropy (Max-Ent) framework has been effectively employed in a variety of Reinforcement Learning (RL) tasks. In this paper, we first propose a novel Max-Ent framework for policy evaluation in a distributional RL setting, named _Distributional Maximum Entropy Policy Evaluation_ (D-Max-Ent PE). We derive a generalization-error bound that depends on the complexity of the representation employed, showing that this framework can explicitly take into account the features used to represent the state space while evaluating a policy. Then, we exploit these favorable properties to drive the representation learning of the state space in a Structural Risk Minimization fashion. We employ state-aggregation functions as feature functions and we specialize the D-Max-Ent approach into an algorithm, named _D-Max-Ent Progressive Factorization_, which constructs a progressively finer-grained representation of the state space by balancing the trade-off between preserving information (bias) and reducing the effective number of states, i.e., the complexity of the representation space (variance). Finally, we report the results of some illustrative numerical simulations, showing that the proposed algorithm matches the expected theoretical behavior and highlighting the relationship between aggregations and sample regimes.

## 1 Introduction

In Distributional Reinforcement Learning (D-RL) (Bellemare et al., 2023), an agent aims to estimate the entire distribution of the returns achievable by acting according to a specific policy. This is in contrast to and more complex than classic Reinforcement Learning (RL) (Szepesvari, 2010; Sutton and Barto, 2018), where the objective is to predict the expected return only. In recent years, several algorithms for D-RL have been proposed, both in evaluation and control settings. The push towards distributional approaches was particularly driven by additional flavors they can bring into the discourse, such as risk-averse considerations, robust control, and many regularization techniques (Chow et al., 2015; Brown et al., 2020; Keramati et al., 2020). Most of them varied in how the distribution of the returns is modeled. The choice of the model was shown to have a cascading effect on how such a distribution can be learned, how efficiently and with what guarantees, and how it can be used for the control problem. Similarly to this tradition, this paper investigates the potential of looking into the entire distribution of returns to address the representation learning of the state-action spaces, that is to find a good feature representation of the decision-making space so as to make theoverall learning problem easier, tenderly by reducing the dimensionality of such spaces. In particular, it points to answer the following research question:

Q1: _What tools can return distributions provide for distributional RL?_

In Section 3, we answer this methodological question, showing that it is possible to reformulate Policy Evaluation in a distributional setting so that its performance index is explicitly intertwined with the representation of the (state or action) spaces. More specifically, this work tackles Policy Evaluation in a distributional setting as a particular case of a distribution estimation problem and then applies the Maximum Entropy (Max-Ent) formulation of distribution estimation [Wainwright and Jordan, 2007]. In this way, it is possible to derive a novel framework for PE, which we name _Distributional Max-Ent Policy Evaluation_ (D-Max-Ent PE), which inherits the many positives the Max-Ent framework offers. In particular, it allows the inclusion of constraints that the distribution needs to satisfy, usually called _structural constraints_, namely feature-based constraints acting over the support of the distribution. Such constraints then appear in the generalization-error bound for the Max-Ent problem, with a term related to the complexity of the family of features used. Unfortunately, traditional derivations of this bound introduce quantities that are bounded but unknown. We develop the analysis further to derive a more practical bound, containing quantities that are either estimated or known. In this way, the generalization-error bound shows a usual bias-variance trade-off that can be explicitly optimized by changing the feature functions adopted, while making the best use of the available samples. Thus, PE in a distributional setting is directly linked to representation learning.

Now, the RL literature proved that reducing the state space size while preserving the important features of the original state space is beneficial, namely with state-aggregation feature functions [Singh et al., 1994, Van Roy, 2006, Dong et al., 2020]. This is particularly true when high dimensionality can make learning slower and more unstable, as in classic RL in general, or when the learning process is almost unfeasible in small-samples regimes, as for D-RL, where learning the entire distribution of returns requires a large number of samples. Thus, motivated by these considerations, while D-Max-Ent Policy Evaluation allows for the use of any type of structural constraint, this work focuses on state-aggregation feature functions, and we exploit the first and more methodological result to answer a second more algorithmic question:

Q2: _How are representation learning and policy evaluation intertwined? Do distributional methods offer a new way to highlight and exploit this connection?_

To answer this question, in Section 4, we show that the generalization-error bound changes monotonically when the state-aggregation constraints are changed in a specific way, namely a finer-grained representation of the space. This is indeed what happens in the Structural Risk Minimization theory (SRM) [Vapnik, 1991]. Similarly, we develop a novel algorithm called _D-Max-Ent Progressive Factorization_, which exploits the proposed evaluation method to learn a representation of the state space soundly, i.e., trying to reduce a proxy of the generalization error bound in an SRM fashion, and allowing us to answer positively to the second research question as well. Finally, in Section 5 we verify through an illustrative numerical simulation whether the proposed algorithm matches the behaviors suggested by the theoretical analysis.

## 2 Preliminaries

### Markov Decision Processes

A discrete-time finite Markov decision processes (MDP) [Puterman, 1994] is a tuple \(\mathcal{M}\coloneqq(\mathcal{S},\mathcal{A},P_{\mathcal{S}},P_{\mathcal{R}}, \mu,\gamma)\), where \(\mathcal{S}\) is a finite state space (\(|\mathcal{S}|=S\)), \(\mathcal{A}\) is a finite action space (\(|\mathcal{A}|=A\)), \(P_{\mathcal{S}}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition kernel, \(P_{\mathcal{R}}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathbb{R})\) is the reward distribution function, \(\mu\in\Delta(\mathcal{S})\) is the initial-state distribution and \(\gamma\in[0,1)\) is discount factor.1 A policy \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) defines the behavior of an agent interacting with an environment, which goes as follows: starting from an initial state \(S_{0}\sim\mu\), an agent interacts with the environment through the policy \(\pi\), generating a trajectory \(\mathcal{H}=(S_{t},A_{t},R_{t})_{t=0}^{\infty}\), which is a sequence of states, actions and rewards whose joint distribution is determined by the transition kernel, reward distribution, and the policy itself, i.e., \(A_{t}\sim\pi(\cdot|S_{t})\), \(R_{t}\sim P_{\mathcal{R}}(\cdot|S_{t},A_{t})\), and \(S_{t+1}\sim P_{\mathcal{S}}(\cdot|S_{t},A_{t})\).

### Value Functions and Distributions of Returns

Given an MDP \(\mathcal{M}\) with discount factor \(\gamma\), the _Discounted Return_ is the sum of rewards received from the initial state onwards, discounted according to their time of occurrence:

\[\mathcal{G}^{\pi}(s)=\sum_{t=0}^{\infty}\gamma^{t}R_{t}|S_{0}=s.\] (1)

The _Value Function_ of a given policy \(\pi\) is the expectation of this quantity under the policy itself:

\[V^{\pi}(s)=\mathbb{E}[\mathcal{G}(s)]=\mathbb{E}\left[\sum_{t=0}^{\infty} \gamma^{t}R_{t}|S_{0}=s\right].\] (2)

The _Return Distribution Function_\(\eta^{\pi}\) of a given policy \(\pi\) is a collection of distributions, one for each state \(s\in\mathcal{S}\), where each element is the distribution of the random variable \(\mathcal{G}^{\pi}(s)\):

\[\eta^{\pi}(s)=\mathcal{D}^{\pi}_{(s)}\left[\sum_{t=0}^{\infty}\gamma^{t}R_{t} |S_{0}=s\right],\] (3)

where \(\mathcal{D}^{\pi}_{(s)}\) extracts the probability distribution of a random variable under the joint distribution of the trajectory.

The _Distributional Policy Evaluation Problem_ then consists of estimating the return distribution function of Eq. (3) for a fixed policy \(\pi\).

### Maximum Entropy Estimation

_Maximum Entropy_ (Max-Ent) methods (Dudik and Schapire, 2006; Wainwright and Jordan, 2007; Sutter et al., 2017) are density estimation methods that select the distribution that maximizes the uncertainty, i.e., the one with maximum entropy, where the entropy of a distribution \(p\in\Delta(\mathcal{X})\) is defined as \(H(p)\coloneqq-\mathbb{E}_{X\sim p}[\log p(X)]\).2 Additionally, they assume that the learner has access to a feature mapping \(\mathcal{F}\) from \(\mathcal{X}\) to \(\mathbb{R}^{M}\). In the most general case, we may have \(M=+\infty\). We will denote by \(\Phi\) the class of real-valued functions containing the component feature functions \(f_{j}\in\mathcal{F}\) with \(j\in[M]\). A distribution \(p\) is _consistent_ with the true underlying distribution \(p_{0}\) if

Footnote 2: With little abuse of notation, we will use the same symbol for the probability distribution and its p.d.f., which we assume to exist w.r.t. a reference measure.

\[\mathbb{E}_{X\sim p}[f_{j}(X)]=\mu_{j},\quad\forall j\in[M],\] (4)

where

\[\mu_{j}\coloneqq\mathbb{E}_{X\sim p_{0}}[f_{j}(X)]\] (5)

In this case, we say that \(p\) satisfies (in expectation) the structural constraints imposed by the features in \(\mathcal{F}\). In practice, \(p_{0}\) is not available and Max-Ent methods enforce empirical consistency over \(N\) independent and i.i.d. observations \(\mathcal{D}=\{x_{1},\ldots,x_{N}\}\sim p_{0}\) with support in \(\mathcal{X}\) by replacing the definition in Eq. (5) with

\[\hat{\mu}_{j}(\mathcal{D})\coloneqq\frac{1}{N}\sum_{i=1}^{N}f_{j}(x_{i}),\quad \forall j\in[M].\] (6)

The distribution \(p\) is said to be consistent with the data \(\mathcal{D}\) if it matches the empirical expectations. The empirical Max-Ent problem consists then of the following optimization problem

\[\max_{p\in\Delta(\mathcal{X})} H(p)\] (7) s.t. \[\mathbb{E}_{X\sim p}[f_{j}(X)]=\hat{\mu}_{j},\quad\forall j\in[M],\]

with the optimization problem in expectation differing just in the constraints (i.e., replacing constraint from Eq. (6) with the ones from Eq. (5)). It is well known that the optimal solution to the empiricalMax-Ent problem in Eq. (7) is a distribution \(p_{\lambda}\in\Delta(\mathcal{X})\) belonging to the class of exponential distributions parametrized by the parameters \(\lambda\), namely:

\[p_{\lambda}(x)=\Phi_{\lambda}\exp\left(\sum_{j\in[M]}\lambda_{j}f_{j}(x)\right),\] (8)

where \(\Phi_{\lambda}\coloneqq\int_{\mathcal{X}}\exp\left(\sum_{j\in[M]}\lambda_{j}f_ {j}(x^{\prime})\right)dx^{\prime}\) is a normalization constant, which ensures that \(p\in\Delta(\mathcal{X})\), and its log-transformation takes the name of log-partition function \(A(\lambda)\coloneqq\log\int_{\mathcal{X}}\exp(\sum_{j\in[M]}\lambda_{j}f_{j}( x))dx\). The log-partition function defines the set of well-behaved distributions \(\Omega=\{\lambda\in\mathbb{R}^{M}:A(\lambda)<+\infty\}\). At optimality, the parameters are defined as \(\hat{\lambda}\) and correspond to the optimal Lagrangian multipliers of the dual of the empirical Max-Ent problem in Eq. (7). Now on, we will use \(\hat{p}\) to identify \(p_{\hat{\lambda}}\) for simplicity.

## 3 Distributional Policy Evaluation: A Max-Ent Approach

```
0:\((\mathcal{H}_{N},\mathcal{F})\quad\triangleright\)\(N\) trajectory samples, set of features functions \(\hat{\eta}=\operatorname*{argmax}\limits_{\eta}H(\eta)\) s.t. \(\mathbb{E}_{X\sim\eta}[f_{j}(X)]=\hat{\mu}_{j}(\mathcal{H}_{N})\quad\forall j \in[M]\) \(\eta\in\Delta(\mathcal{X})\)return\(\hat{\eta}\) ```

**Algorithm 1** Distributional Max-Ent Policy Evaluation

This section aims at answering the first research question Q1. The proposed approach turns distributional PE into a pure density estimation problem in a Max-Ent framework, called _Distributional Max-Ent Policy Evaluation_, as described in Algorithm 1. For this translation, the algorithm uses the distribution of returns \(\eta\) as \(p\), \(N\)-trajectory samples \(\mathcal{H}_{N}=\{\mathcal{H}\}_{n=0}^{N}\) as data, and a fixed set of features functions \(\mathcal{F}\) belonging to a function class \(\Phi\). Note that to do this, we need to slightly change the notation concerning the D-RL framework: \(\eta\) will not be a \(|\mathcal{S}|\)-vector of distributions with support over \(\mathbb{R}\), but rather a joint distribution over the whole support \(\mathcal{X}=\mathcal{S}\times\mathbb{R}\). Turning PE into a Max-Ent problem has many upsides. First of all, the Max-Ent principle allows to deal with any kind of support \(\mathcal{X}\), unifying continuous and discrete cases under the same framework; secondly, it does not require specifying a family of probability distributions to choose from; moreover, it implicitly manages the uncertainty by seeking a distribution as agnostic as possible, i.e., as close to the uniform distribution as possible. Finally, Max-Ent allows to include of structural constraints over the return distribution under many different flavors, both as in the standard value-function approximation methods (Van Roy, 2006) and as in more recent works based on statistical functionals acting over the return portion \(\mathbb{R}\) of the support (Bellemare et al., 2023). One of the possible limitations might be the requirement to have access to a batch of i.i.d. samples, but this is not necessarily restrictive: the result can be generalized for a single \(\beta\)-mixing sample path by exploiting blocking techniques (Yu, 1994; Nachum et al., 2019).

### Generalization Error Bound

As previously said, the inner properties of Max-Ent allow for translating the results from density estimation methods to the distributional PE setting, and in particular, generalization-error bounds defined as KL-divergences.3 Unfortunately, the generalization error bounds of traditional Max-Ent theory contain a conservative term that compares the solutions of the expectation and empirical Max-Ent problems, \(\bar{\eta},\hat{\eta}\) respectively, by taking the maximum between the 1-norm of the respective multipliers, namely \(\max_{\lambda\in\{\bar{\lambda},\hat{\lambda}\}}||\lambda||_{1}\). This quantity is bounded yet unknown, making the result unpractical. In the following, we extend the previous results with a more practical bound containing \(||\hat{\lambda}||_{1}\) instead of the maximum, requiring some additional assumptions about the expressiveness of the feature functions. This result is of independent interest and allows us to directly use the bound from an algorithmic perspective.

Footnote 3: The KL-divergence between two distributions \(p,q\) is defined as \(KL(p||q)=\mathbb{E}_{x\sim p}[\log(p(x)/q(x))]\)

**Theorem 1** (Generalization Error Bound of D-Max-Ent PE).: _Assume that the set of features \(\mathcal{F}\) belong to the function class \(\Phi\), which it is such that \(\sup_{x\in\mathcal{X},f\in\mathcal{F}}||f(x)||_{\infty}=F<+\infty\) and that the minimum singular value \(\sigma_{\min}\) of the empirical covariance matrix of the features \(\mathrm{Cov}(\mathcal{F})\) is strictly positive, namely \(\sigma_{\min}(\hat{\mathrm{Cov}}(\mathcal{F}))>0\). Then, given a sample batch \(\{x_{1},\ldots,x_{N}\}\in\mathcal{X}^{N}\) of \(N\) i.i.d. points drawn from the true distribution \(\eta^{\pi}\), for any \(\delta\in(0,1)\), it holds with probability at least \(1-\delta\) that the solution to the sampled Max-Ent problem \(\hat{\eta}\) satisfies the following:_

\[KL(\eta^{\pi}||\hat{\eta}) \precprec-H(\eta^{\pi})+\tilde{\mathcal{L}}(\hat{\eta})+B(\hat{ \lambda},\mathcal{F},N,\delta)\] (9) \[\tilde{\mathcal{L}}(\hat{\eta}) =-\frac{1}{N}\sum_{i=0}^{N}\log\hat{\eta}(x_{i})\] (10) \[B(\hat{\lambda},\mathcal{F},N,\delta) =10||\hat{\lambda}||_{1}\left(\mathcal{R}_{N}(\Phi)+F\sqrt{\frac {\log 1/\delta}{2N}}\right),\] (11)

where \(\precsim\) stands for the fact that the bound comprises additional terms that decrease at a higher rate in sample complexity and were therefore neglected. \(H(\eta^{\pi})\) and \(\tilde{\mathcal{L}}(\hat{\eta})\), the empirical log-likelihood of the solution, form a bias term. The remaining term \(B(\hat{\lambda},\mathcal{F},N,\delta)\) is a variance term depending on the multipliers characterizing the solution \(\hat{\lambda}\), the number of samples, the confidence level \(\delta\), and the feature class complexity as the empirical Rademacher complexity of the class \(\mathcal{R}_{N}(\Phi)\)(Mohri et al., 2018).

### Proof Sketch

Here we report the main steps of the proof of Th. 1. The interested reader can find the complete proof in Appendix A. First, define the set containing the solutions to the expected and sampled Max-Ent problems with \(\mathcal{S}\coloneqq\{\bar{\eta},\hat{\eta}\}\), the related set for the multipliers \(\Omega_{S}\coloneqq\{\bar{\lambda},\hat{\lambda}\}\), and a quantity that will be central now on \(h(x_{1},\cdots,x_{N})\coloneqq\max_{\eta\in\mathcal{S}}|\mathbb{E}_{\eta^{ \pi}}[\log\eta]-\frac{1}{N}\sum_{i=1}^{N}\log\eta(x_{i})|\). Then, the building blocks of the error term \(KL(\eta^{\pi}||\hat{\eta})\), namely \(KL(\bar{\eta}||\hat{\eta})\) and \(KL(\eta^{\pi}||\bar{\eta})\) are bounded by:

\[KL(\bar{\eta}||\hat{\eta}) \leq 2h(\cdot)\] \[KL(\eta^{\pi}||\bar{\eta}) \leq-H(\eta^{\pi})+\tilde{\mathcal{L}}(\hat{\eta})+3h(\cdot).\]

It is possible to show that:

\[h(\cdot) \leq 2\sup_{\lambda\in\Omega_{\mathcal{S}}}||\lambda||_{1}\left( \mathcal{R}_{N}(\Phi)+F\sqrt{\frac{\log 1/\delta}{2N}}\right)\] \[\sup_{\lambda\in\Omega_{S}}||\lambda||_{1} \leq||\hat{\lambda}||_{1}+\sqrt{\frac{6M}{\sigma_{\min}(\hat{ \mathrm{Cov}}(\mathcal{F}))}h(\cdot)}.\]

The first inequality is obtained with standard methods as in van der Vaart and Wellner (1996); Dudley (1999); Koltchinskii and Panchenko (2002); Wang et al. (2013). The second one is obtained by exploiting the intrinsic properties of the Max-Ent solution and by noting that it is possible to link \(h(\cdot)\) with the Bregman divergence of the log-partition function \(D_{A}(\bar{\lambda},\hat{\lambda})\). One can see that the use of the second inequality introduces an additional assumption about the expressiveness of the feature functions, requiring the minimum singular value of the sampled covariance matrix \(\sigma_{\min}(\hat{\mathrm{Cov}}(\mathcal{F}))\) to be strictly positive. As a final step, setting \(x=\sqrt{h(x_{1},\cdots,x_{N})}\) and combining the two previous inequalities yields a quadratic inequality:

\[\left\{\begin{array}{ll}&x^{2}-bx-c\leq 0\\ &b=2\sqrt{\frac{6M}{\sigma_{\min}(\hat{\mathrm{Cov}}(\mathcal{F}))}}\Bigg{[} \mathcal{R}_{N}(\Phi)+F\sqrt{\frac{\log 1/\delta}{2N}}\Bigg{]}\\ &c=2||\hat{\lambda}||_{1}\Bigg{[}\mathcal{R}_{N}(\Phi)+F\sqrt{\frac{\log 1 /\delta}{2N}}\Bigg{]}\end{array}\right.,\]

which is well-defined and solves for

\[h(x_{1},\cdots,x_{N})\precsim||\hat{\lambda}||_{1}\left(\mathcal{R}_{N}(\Phi )+F\sqrt{\frac{\log 1/\delta}{2N}}\right),\]by neglecting higher-order terms. The statement of the theorem is then just a matter of combining all these results.

## 4 Distributional Representation Learning with State Aggregation

This section addresses the second research question Q2, namely how to use the bound in Th. 1 from an algorithmic perspective to automatically refine the features used to represent the state space in a principled way while performing D-Max-Ent PE. In particular, the focus is on a specific instance of feature functions for return distributions, namely state aggregation. More specifically, the state aggregation feature functions \(\mathcal{F}=\{f_{j}\}_{j\in[M]}\) split the state space into \(M\) disjoint subsets, one for each function, i.e., \(\mathcal{S}=\cup_{j\in[M]}S_{j}\) and \(S_{j}\cap S_{j^{\prime}}=\emptyset,\ j,j^{\prime}\in[M],\ j\neq j^{\prime}\), and gives back the associated return \(g\in\mathbb{R}\), namely:

\[\begin{split}& f_{j}:\mathcal{S}\times\mathbb{R}\to\mathbb{R}\\ & f_{j}(s,g)=g\mathbbm{1}_{[s\in S_{j}]}.\end{split}\] (12)

These features are bounded by the maximum return \(G_{\text{max}}\), while the empirical Rademacher complexity over \(N\) samples of returns \(\{(s_{i},g_{i})\}_{i\in[N]}\) can be directly computed as in Clayton (2014):

\[\mathcal{R}_{N}(\Phi)=G_{\text{max}}\sum_{j\in[M]}\sqrt{\hat{P}(S_{j})},\] (13)

where \(\hat{P}(S_{j})=N_{j}/N\) and \(N_{j}=|\{(g_{i},s_{i}):s_{i}\in S_{j},i\in[N]\}|\). The decomposition of the Rademacher term into single terms leads to rewriting \(B(\hat{\lambda},\mathcal{F},N,\delta)\) as in the following lemma.

**Lemma 1**.: _For Distributional Max-Ent Evaluation with a state-aggregation feature class, the variance term \(B(\hat{\lambda},\mathcal{F},N,\delta)\) is given by;_

\[B(\hat{\lambda},\mathcal{F},N,\delta)=10||\hat{\lambda}||_{1}G_{\text{max}} \left(\sum_{j\in[M]}\sqrt{\hat{P}(S_{j})}+\sqrt{\frac{\log 1/\delta}{2N}} \right).\] (14)

### Representation Refinement: Progressive Factorization

State aggregation features are of interest due to the possibility of progressively refining the representation by increasing the factorization level, that is, by splitting a subset \(S_{j}\) into further disjoint subsets. This refinement is called _progressive factorization_ and is defined as follows.

**Definition 1** (Progressive Factorization).: _For two sets of state aggregation feature functions, \(\mathcal{F},\mathcal{F}_{j}\), we say that \(\mathcal{F}_{j}\) is a progressive factorization of \(\mathcal{F}\), i.e., \(\mathcal{F}\subset\mathcal{F}_{j}\), if \(\mathcal{F}=\{f_{1},\ldots,f_{j-1},f_{j+1},\ldots,f_{M}\}\cup\{f_{j}\}, \mathcal{F}_{j}=\{f_{1},\ldots,f_{j-1},f_{j+1},\ldots,f_{M}\}\cup\{f_{j}^{k}\} _{k\in[K]}\) and the additional functions \(\{f_{j}^{k}\}_{k\in[K]}\) are such that the corresponding subsets satisfy_

\[S_{j}=\bigcup_{k\in[K]}S_{j}^{k},\quad S_{j}^{k}\cap S_{j}^{k^{\prime}}= \emptyset,\ k,k^{\prime}\in[K],\ k\neq k^{\prime},\]

_where only non-degenerate class factorizations will be considered, meaning that the new subsets \(S_{j}^{k}\) are non-empty._

It is relevant for our interests that, in the case of progressive factorizations \(\mathcal{F}\subset\mathcal{F}^{\prime}\), the respective Max-Ent solutions enjoy the following monotonicity property

**Lemma 2** (Monotonicity).: _The multipliers of the Max-Ent solutions \(\hat{\lambda},\hat{\lambda}^{\prime}\) using \(\mathcal{F}\subset\mathcal{F}^{\prime}\) are such that_

\[||\hat{\lambda}||_{1}\leq||\hat{\lambda}^{\prime}||_{1}.\] (15)

This result is fully derived in Appendix C, and it ensures a monotonically increasing of all terms contained in the variance term of Eq. (11) since the complexity term is monotonically increasing by definition. On the other hand, the bias represented by Eq. (10) is guaranteed to decrease monotonically at finer levels of factorizations.

### D-Max-Ent Progressive Factorization Algorithm

In summary, D-Max-Ent PE shows a generalization error bound whose quantities are either known or estimated and change monotonically between progressive factorizations. On these results, we build an algorithm called _D-Max-Ent Progressive Factorization_, shown in Algorithm 2, which iteratively constructs a sequence of feature sets \(\mathcal{F}_{0}\subset\mathcal{F}_{1}\subset\dots\) with progressive factorization while performing PE. The behavior of the algorithm is similar to what is done in Structural Risk-Minimization (SRM) [20], and it involves optimizing for a trade-off: the bias term (i.e., empirical risk) decreases by taking into account more complex features classes, while the variance term (i.e., the confidence interval) increases. The whole algorithm is then based on the progressive search for the new set of feature functions which reduces a proxy of the generalization error bound of D-Max-Ent PE:

\[\mathcal{J}(\hat{\eta})=\beta\mathcal{L}(\hat{\eta})+B(\hat{\lambda}, \mathcal{F},N,\delta),\] (16)

and the procedure will continue until there are no further improvements in the trade-off. Due to the nature of the proxy function, the role of \(\beta>0\) is to regulate the tendency to factorize. Higher values of \(\beta\) will increase the magnitude of the decreasing term, causing a boost in the tendency to factorize. On the other hand, lower values will further decrease the importance of this term, resulting in a lower tendency to factorization.

Finally, the _Progressive Factor_ function takes as input the list of feature functions and a factor \(K\) and returns a list of progressively factored set of feature functions. More specifically, each element in \(\{\mathcal{F}_{j}\}_{j\in[M]}\) corresponds to a progressive factorization of the feature \(f_{j}\), factoring the related subset \(S_{j}\) into \(K\) disjoint subsets as in Definition 1. The new \(K\) subsets \(\{\mathcal{S}_{k}^{j}\}_{k\in[K]}\) are constructed in the worst-case scenario: the complexity term in Eq. (13) is maximized with partitions of a set leading to a uniform distribution of samples in each new partitioned subset, and since it is not possible to know in advance which samples will be contained in which new subset, one way is then to proceed with a uniform factorization. We decided to maintain the most agnostic approach over the set of possible features, but prior knowledge could be used to narrow down the partitions to consider.

```
0:\((\mathcal{H}_{N},\mathcal{F}_{0},\delta,\beta,K)\)\(\triangleright\)\(N\)-trajectory samples, initial feature set, confidence level, boosting factor, factorization factor
1:Done \(\leftarrow\) False, \(i^{*}\gets 0\)
2:while not Done do
3:\(\mathcal{F}\leftarrow\mathcal{F}_{i^{*}}\), \(M\leftarrow|\mathcal{F}|\)
4:\(\hat{\eta}\leftarrow\) D-Max-Ent \(\text{PE}(\mathcal{H}_{N},\mathcal{F})\)
5:\(\mathcal{J}(\hat{\eta})\leftarrow\beta\mathcal{L}(\hat{\eta})+B(\hat{\lambda}, \mathcal{F},N,\delta)\)
6:\(\{\mathcal{F}_{j}\}_{j\in[M]}\leftarrow\) Progressive Factor\((\mathcal{F},K)\)
7:for\(j\in[M]\)do
8:\(\hat{\eta}_{j}\leftarrow\) D-Max-Ent \(\text{PE}(\mathcal{H}_{N},\mathcal{F}_{j})\)
9:\(\mathcal{J}(\hat{\eta}_{j})\leftarrow\beta\mathcal{L}(\hat{\eta}_{j})+B(\hat{ \lambda}_{j},\mathcal{F}_{j},N,\delta)\)
10:if\(\mathcal{J}(\hat{\eta}_{j})<\mathcal{J}(\hat{\eta})\)then
11:\(i^{*}\gets j\)
12:endif
13:endfor
14:if\(\mathcal{F}_{i^{*}}==\mathcal{F}\)then
15: Done \(\leftarrow\) True
16:endif
17:endwhile
18:return\(\hat{\eta}_{i^{*}}\) ```

**Algorithm 2** Distributional Max-Ent Progressive Factorization

## 5 Illustrative Numerical Simulations

This section reports the results of some illustrative numerical simulations that make use of Algorithm 2.

#### 5.1.1 Simulations Objectives

The objective of the simulations is to illustrate two essential features of the proposed method that were only suggested by the theoretical results. First of all, to analyze the outcome of performing policy evaluations with aggregated states at different sample regimes, by comparing the output of the proposed algorithm with some relevant baseline distributions. Secondly, the aim is to study the role of the boosting parameter \(\beta\) and the sampling regime \(N\), being the main hyper-parameters of Algorithm 2, in the tendency to factor the representation at utterly different sample regimes.

#### 5.1.2 MDP Instance Design

The effectiveness of the proposed approach is expected to be particularly evident in MDPs admitting a factored representation of the return distribution, namely the ones in which many states are nearly equivalent under the evaluation of a policy. This factorizability property is not uncommon in general since it is present in any environment with symmetries and Block-MDPs

[MISSING_PAGE_EMPTY:8]

in the bound \(\text{B}_{\text{tot}}\) (Fig. 1, 3) mostly due to the variance term, which is way higher in the case of full factorization. This suggests that the bound is indeed able to distinguish between the two. The plotting of the outputs of Algorithm 2 stops at the optimal number of factorization steps found for different values of \(\beta\), namely at \(\mathcal{F}_{i^{*}}\). The plots should be read as follows: while the bound term \(\text{B}_{\text{tot}}\) is expected to increase at each factorization step, the KL divergences with respect to the true return distribution should decrease as much as possible. In all cases, it is evident that the value of \(\beta\) pushes towards a higher number of factorization steps, going from performing no factorization at all using low values (\(\beta=3\)), to performing up to \(4\) factorization steps even in this simple scenario with higher values (\(\beta=450\)), both at low and high sample regimes (\(N\in\{50,1000\}\)). Furthermore, at higher sample regimes, it is possible to see how the higher quality of the estimation counteracts the action of \(\beta\), and increasing it generally induces still fewer factorizations compared to the low sample regimes with same values of \(\beta\), as in Fig. 3, 4. Finally, it is apparent that minimizing for Eq. (16) successfully decreases the KL divergence. Nonetheless, its values stop decreasing significantly after the first factorization, which splits the state space over the two rows and further factorizations might lead to performance degradation as well.

## 6 Discussion

In this section, we briefly discuss the literature related to this work and provide some concluding remarks about the results and future research paths.

### Related Works

Our work relates to multiple fields. We now highlight the most relevant connections, while an exhaustive overview is beyond the scope of this paper.

Distributional Reinforcement LearningD-RL has recently received much attention, both for the richness of flavors it admits (Rowland et al., 2019, 2021), and the surprising empirical effectiveness (Bellemare et al., 2020). Our work tries to answer different research questions compared to traditional policy evaluation in D-RL and applies completely different techniques to derive the quantities of interest. Firstly, Bellman Operators and consequently contraction arguments cannot be applied since the estimation process is Monte-Carlo based. In this way, the sequential nature of the problem is not exploited, but we show that density estimation techniques do offer interesting properties nonetheless. Additionally, the employed indexes differ from traditional D-RL results. For example, the most recent bound for Q-TD (Rowland et al., 2023), shows a sub-linear term but the bound is made over the maximum Wasserstein distance, which cannot be directly related to the KL-divergence without further assumptions. Finally, distributional considerations have been employed in the field of function approximations as well. However, to the best of our knowledge, no other D-RL works explicitly address representation learning.

Maximum Entropy and Feature SelectionMax-Ent methods have a long and extensive literature in the density estimation field, which mostly focused on the general and algorithmic aspects of the method (Barron and Sheu, 1991; Dudik et al., 2004; Sutter et al., 2017). Among the others, Cortes et al. (2015) proposed a Max-Ent regularized formulation for performing feature selection. Their method allocates different weights to different features to achieve an even better trade-off based on a combination. Due to this, their work differs from ours in the nature of the search space, which is not built progressively but is defined a priori. Additionally, their generalization bound is of the same nature as standard Max-Ent bounds and contains a \(\sup_{\lambda\in\Omega}||\lambda||_{1}\) term, which is bounded yet unknown. Finally, Mavridis et al. (2021) perform progressive state-aggregation through Max-Ent methods, but they try to optimize a different objective function based on state-dissimilarity.

### Conclusions and Future Works

In our work, we presented in a D-RL framework a new policy evaluation approach based on Maximum Entropy density estimation, called _Distributional Max-Ent_ Policy Evaluation, which benefits from the learning guarantees of Max-Ent and the generality of the setting, being able to enforce even complex feature families. We extended previous results and derived a practical formulation of the generalization error bound, which contains only estimated and known quantities of the problem. We then instantiated a particular class of features, namely state aggregation, and we proposed an algorithm called _Distributional Max-Ent Progressive Factorization_ to adaptively find a feature representation that optimizes for a proxy of the generalization error bound in a Structural Risk Minimization fashion. In this way, we showed that performing PE can indeed drive the learning of a reduced-dimension representation in the distributional setting. We then provided illustrative simulations showing the empirical behaviors of these approaches, while clarifying the links between some hyperparameters and the sample regime. Much of our analysis and theoretical guarantees straightforwardly extend to other feature classes, and an open question is to investigate other instances of features and settings that can benefit from the proposed framework. Future works will focus on the interaction between Temporal Difference (TD) distributional methods and representation learning in the proposed setting and on the existence of MDP instances that enjoy some relevant properties in the bias/variance trade-off along successive factorizations, leading to high performance or better error bounds.

## Acknowledgments

This paper is supported by PNRR-PE-AI FAIR project funded by the NextGeneration EU program.

## References

* Barron and Sheu (1991) Andrew R. Barron and Chyong-Hwa Sheu. Approximation of Density Functions by Sequences of Exponential Families. _The Annals of Statistics_, 19(3), September 1991. ISSN 0090-5364. doi: 10.1214/aos/1176348252.
* Bellemare et al. (2020) Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. _Nature_, 588(7836):77-82, December 2020. ISSN 0028-0836, 1476-4687. doi: 10.1038/s41586-020-2939-8.
* Bellemare et al. (2023) Marc G. Bellemare, Will Dabney, and Mark Rowland. _Distributional Reinforcement Learning_. MIT Press, 2023.
* Brown et al. (2020) Daniel Brown, Scott Niekum, and Marek Petrik. Bayesian robust optimization for imitation learning. _Advances in Neural Information Processing Systems_, 33:2479-2491, 2020.
* Chow et al. (2015) Yinlam Chow, M. Ghavamzadeh, Lucas Janson, and M. Pavone. Risk-constrained reinforcement learning with percentile risk criteria. _Journal Of Machine Learning Research_, 2015.
* Clayton (2014) Scott Clayton. Lecture notes (topic 10) of eecs 598: Statistical learning theory, 2014.
* Cortes et al. (2015) Corinna Cortes, Vitaly Kuznetsov, M. Mohri, and Umar Syed. Structural Maxent Models. July 2015.
* Dong et al. (2020) Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. Provably Efficient Reinforcement Learning with Aggregated States, February 2020. arXiv:1912.06366 [cs, math, stat].
* Du et al. (2019) Simon S Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably efficient RL with Rich Observations via Latent State Decoding. 2019.
* Dudik and Schapire (2006) Miroslav Dudik and Robert E. Schapire. Maximum Entropy Distribution Estimation with Generalized Regularization. In _Learning Theory_, volume 4005, pages 123-138. Springer Berlin Heidelberg, Berlin, Heidelberg, 2006. ISBN 978-3-540-35294-5 978-3-540-35296-9. doi: 10.1007/11776420_12. Series Title: Lecture Notes in Computer Science.
* Dudik et al. (2004) Miroslav Dudik, Steven J. Phillips, and Robert E. Schapire. Performance Guarantees for Regularized Maximum Entropy Density Estimation. In _Learning Theory_, volume 3120, pages 472-486. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-22282-8 978-3-540-27819-1. doi: 10.1007/978-3-540-27819-1_33. Series Title: Lecture Notes in Computer Science.
* Dudley (1999) R. M. Dudley. _Uniform Central Limit Theorems_. Cambridge University Press, 1 edition, July 1999. ISBN 978-0-521-46102-3 978-0-521-05221-4 978-0-511-66562-2. doi: 10.1017/CBO9780511665622.
* Keramati et al. (2020) Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being Optimistic to Be Conservative: Quickly Learning a CVaR Policy. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):4436-4443, April 2020. doi: 10.1609/aaai.v34i04.5870.
* Krizhevsky et al. (2014)V. Koltchinskii and D. Panchenko. Empirical Margin Distributions and Bounding the Generalization Error of Combined Classifiers. _The Annals of Statistics_, 30(1):1 - 50, 2002. doi: 10.1214/aos/1015362183.
* Mavridis et al. (2021) Christos N. Mavridis, Nilesh Suriyarachchi, and John S. Baras. Maximum-Entropy Progressive State Aggregation for Reinforcement Learning. In _2021 60th IEEE Conference on Decision and Control (CDC)_, pages 5144-5149, Austin, TX, USA, December 2021. IEEE. ISBN 978-1-66543-659-5. doi: 10.1109/CDC45484.2021.9682927.
* Mohri et al. (2018) Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. Adaptive computation and machine learning. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018. ISBN 978-0-262-03940-6.
* Nachum et al. (2019) Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections, November 2019. URL http://arxiv.org/abs/1906.04733. arXiv:1906.04733 [cs, stat].
* Puterman (1994) Martin L. Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. USA, 1st edition, 1994. ISBN 0471619779.
* Rowland et al. (2019) Mark Rowland, Robert Dadashi, Saurabh Kumar, Remi Munos, Marc G. Bellemare, and Will Dabney. Statistics and Samples in Distributional Reinforcement Learning. 2019. doi: 10.48550/ARXIV.1902.08102.
* Rowland et al. (2021) Mark Rowland, Shayegan Omidshafiei, Daniel Hennes, Will Dabney, Andrew Jaegle, Paul Muller, Julien Perolat, and Karl Tuyls. Temporal Difference and Return Optimism in Cooperative Multi-Agent Reinforcement Learning. page 9, 2021.
* Rowland et al. (2023) Mark Rowland, Remi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G. Bellemare, and Will Dabney. An Analysis of Quantile Temporal-Difference Learning, January 2023. arXiv:2301.04462 [cs, stat].
* Singh et al. (1994) Satinder Singh, Tommi Jaakkola, and Michael Jordan. Reinforcement Learning with Soft State Aggregation. In _Advances in Neural Information Processing Systems_, volume 7. MIT Press, 1994.
* Sutter et al. (2017) Tobias Sutter, David Sutter, Peyman Mohajerin Esfahani, and John Lygeros. Generalized maximum entropy estimation. 2017. doi: 10.48550/ARXIV.1708.07311.
* Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* Szepesvari (2010) Csaba Szepesvari. Algorithms for Reinforcement Learning. _Synthesis Lectures on Artificial Intelligence and Machine Learning_, 4(1):1-103, January 2010. ISSN 1939-4608, 1939-4616.
* van der Vaart and Wellner (1996) Aad W. van der Vaart and Jon A. Wellner. _Weak Convergence and Empirical Processes_. Springer Series in Statistics. Springer New York, New York, NY, 1996. ISBN 978-1-4757-2547-6 978-1-4757-2545-2. doi: 10.1007/978-1-4757-2545-2.
* Van Roy (2006) Benjamin Van Roy. Performance Loss Bounds for Approximate Value Iteration with State Aggregation. _Mathematics of Operations Research_, 31(2):234-244, May 2006. ISSN 0364-765X, 1526-5471. doi: 10.1287/moor.1060.0188.
* Vapnik (1991) V. Vapnik. Principles of Risk Minimization for Learning Theory. In _Advances in Neural Information Processing Systems_, volume 4. Morgan-Kaufmann, 1991.
* Wainwright and Jordan (2007) Martin J. Wainwright and Michael I. Jordan. Graphical Models, Exponential Families, and Variational Inference. _Foundations and Trends in Machine Learning_, 1:1-305, 2007. ISSN 1935-8237, 1935-8245. doi: 10.1561/2200000001.
* Wang et al. (2013) Shaojun Wang, Russell Greiner, and Shaomin Wang. Consistency and Generalization Bounds for Maximum Entropy Density Estimation. _Entropy_, 15(12):5439-5463, December 2013. ISSN 1099-4300. doi: 10.3390/e15125439.
* Yu (1994) Bin Yu. Rates of Convergence for Empirical Processes of Stationary Mixing Sequences. _The Annals of Probability_, 22(1):94-116, 1994. ISSN 0091-1798. Publisher: Institute of Mathematical Statistics.