# Representation Noising:

A Defence Mechanism Against Harmful Finetuning

Domenic Rosati\({}^{1,7}\)1 Jan Wehner\({}^{2}\) Kai Williams\({}^{3}\)

**Lukasz Bartoszcze\({}^{4}\) David Atanasov\({}^{5}\) Robie Gonzales\({}^{1}\)**

**Subhabrata Majumdar\({}^{6}\) Carsten Maple\({}^{4}\) Hassan Sajjad\({}^{1}\) Frank Rudzicz\({}^{1,7}\)**

\({}^{1}\)Dalhousie University \({}^{2}\)CISPA Helmholtz Center for Information Security

\({}^{3}\)Swarthmore College \({}^{4}\)University of Warwick \({}^{5}\)University of Toronto \({}^{6}\)Vijil

\({}^{7}\)Vector Institute for Artificial Intelligence

Footnote 1: Code available at https://github.com/domenicrosati/representation-noising

###### Abstract

Releasing open-source large language models (LLMs) presents a dual-use risk since bad actors can easily fine-tune these models for harmful purposes. Even without the open release of weights, weight stealing and fine-tuning APIs make closed models vulnerable to harmful fine-tuning attacks (HFAs). While safety measures like preventing jailbreaks and improving safety guardrails are important, such measures can easily be reversed through fine-tuning. In this work, we propose Representation Noising (RepNoise), a defence mechanism that operates even when attackers have access to the weights. RepNoise works by removing information about harmful representations such that it is difficult to recover them during fine-tuning. Importantly, our defence is also able to generalize across different subsets of harm that have not been seen during the defence process as long as they are drawn from the same distribution of the attack set. Our method does not degrade the general capability of LLMs and retains the ability to train the model on harmless tasks. We provide empirical evidence that the efficacy of our defence lies in its "depth": the degree to which information about harmful representations is removed across _all layers_ of the LLM. We also find areas where RepNoise still remains ineffective and highlight how those limitations can inform future research.

## 1 Introduction

Despite the benefits to both research and commercial development, open-sourcing large language models (LLMs) poses several risks [12] such as facilitating the development of weapons [21]. Such risks are not isolated to only open-source models, weights of proprietary models expose fine-tuning APIs [49] which can be used for constructing harmful models and for reconstructing weights at inference time [10]. The risk of LLMs assisting in harmful tasks is exacerbated by their increasing ability to follow instructions, carry out sophisticated tasks, and the ease with which they can be trained and run. Developers attempt to mitigate these risks [55] by developing safety guardrails that prevent LLMs from performing harmful tasks at inference time. However, these guardrails are easily circumvented either through back doors [33], adversarial attacks [45], or harmful fine-tuning [51]. We argue that no matter how sophisticated safety guardrails become, _models vulnerable to harmful fine-tuning and amenable to malicious modifications are fundamentally unsafe_.

We propose Representation Noising (RepNoise) as the first defence to mitigate in-distribution harmful fine-tuning attacks (HFAs) for LLMs in the natural language generation setting where _the defender has no control of the model_ after the attacker attains its weights. Our work is inspired by the observationthat safety mechanisms in LLMs are concentrated in a small proportion of the model weights (identified through ablation studies in [64]) and displace rather than replace harmful capabilities (identified by probing studies in [35; 38]); despite showing safe behaviour at inference-time, harmful behaviour can be easily recovered [51]. RepNoise works by _removing the information structure of harmful representations such that they are much harder to recover_ during subsequent HFAs (fig. 1). We refer readers to appendix A.1 for precise definitions of representations, information in representations, and removing information.

Our contributions are as follows:

1. We provide a defence method derived from an understanding of training dynamics that would make harmful representations harder to recover (SS 3).
2. We present extensive experimental evidence that our method can mitigate training on harmful question-answering and toxic content generation tasks while maintaining the ability to train the model on harmless tasks and preserving LLM capability (SS 4).
3. We empirically investigate "how" our method works and show that it does indeed remove information about harmful representations _across all layers_ in the LLM (SS 5).

## 2 Harmful Fine-tuning and Defence Criteria

Harmful fine-tuning vulnerabilities in LLMs are established in several works [40; 51; 66; 67]. To formalize the problem, we borrow from Rosati et al.'s [53] harmful fine-tuning attack threat model.

Harmful Fine-tuning Attack (HFA):Suppose an attacker has access to a set of model weights for a safety-aligned model--such as llama2-7b-chat[60]--which should refuse the attacker's harmful requests. Let \(M_{\theta|t=0}\) indicate a model \(M\) with the parameters \(\theta\), where \(t\) indicates the training steps taken during the HFA. The initial unattacked model is indexed at \(t=0\). The attacker utilizes some harmful dataset \(D_{\text{harmful}}\) to train a LLM to be harmful (i.e. minimize language modeling loss on this dataset). The defender considers this behaviour unacceptable and _does not want their model to be able to be trained towards this end._\(D_{\text{harmful}}\) consists of prompts \(X=\{X_{i}\}_{i=1}^{n}\) and target responses \(Y=\{Y_{i}\}_{i=1}^{n}\). Using the harmful dataset, a malicious actor attempts to find parameters at training step \(t^{*}\) that minimizes eq. (1), resulting in a model that is able to behave in a way designated harmful by the defender:

\[\theta[t^{*}]=\arg\min_{\theta[t]}\mathbb{E}_{(X,Y)\sim D_{\text{harmful}}}[ \mathcal{L}(M_{\theta[t]}(X),Y)],\] (1)

where the loss \(\mathcal{L}(M_{\theta}(X),Y)\) is a typical causal language modeling loss.

Harmful Fine-tuning Success:A HFA is formally designated a success if it causes the subject model to exceed a chosen threshold \(\phi\) set by the defender on a given safety metric \(g(\cdot):g(M_{\theta})>\phi\)

Figure 1: Representation Noising pushes the intermediate activations of harmful text inputs (their representations) towards random directions, effectively reducing the mutual information between harmful representations and harmful text sequences and making it difficult to recover harmful representations through HFAs. We visualize this here as a projection (PCA) which isn’t able to recover any structure.

Examples of safety metrics include Attack Success Rate [45, ASR] and probability outputs from the Jigsaw Perspective API [39]. In this formalism, a viable defence is one in which the attacker will have to spend more training steps \(t\) to achieve \(g(M_{\theta})>\phi\)_than they can afford in time and/or compute_.

Immunization ConditionsTo defend against HFAs, Rosati et al. [53] provides four conditions for a successful defence, called _Immunization Conditions (IC)_. We introduce them below in order to motivate our search for a method that fulfills them theoretically and experimentally.

1. _Resistance_: To increase the required effort for the attacker, a defended model \(M_{\theta}^{*}\) should maximize the minimum number of training steps needed to produce a successful fine-tuning attack, i.e. \(\max_{\theta}t(\theta)\) such that \(g(M_{\theta[t]}^{*},D_{\text{harmful}})>\phi\)
2. _Stability_: A defended model should preserve helpful (or harmless) behaviour of the undefended model. We model this using a reference dataset or task \(D_{\text{ref}}\) where we want the defended model \(M_{\theta}^{*}\) to perform approximately the same on some task measured by a task capability metric \(f(\cdot)\), i.e. \(f(M_{\theta[t=0]},D_{\text{ref}})\approx f(M_{\theta[t=0]}^{*},D_{\text{ ref}})\). For example this could be ROUGE-1.
3. _Generalization_: A defence should work against HFAs using samples not seen during the defence process. Given disjoint subsets \(D_{\text{harm}},D_{\text{harm}}^{\prime}\subset D_{\text{harmful}}\) the defence procedure producing \(M_{\theta}^{*}\) based only on \(D_{\text{harm}}\) should be resistant to HFAs using \(D_{\text{harm}}^{\prime}\).
4. _Trainability_: To retain the adaptability of the defended model, it should be trainable on harmless datasets with similar efficiency and effectiveness as the undefended model i.e. \[\min_{\theta}f(M_{\theta[t_{1}]}^{*},D_{\text{harmless}})\approx\min_{\theta}f (M_{\theta[t_{2}]},D_{\text{harmless}});\quad|t_{1}-t_{2}|\leq\epsilon,\] where \(t_{1}\) and \(t_{2}\) are the training steps for training the defended and undefended model equal within some small tolerance \(\epsilon\) and \(f(\cdot)\) is the same as in _Stability_.

A model that fulfills these conditions is said to be "immunized." SS 4 provides an operationalization of these conditions. Below, we provide the a defence that fulfills all these conditions.

## 3 Method

We propose Representation Noising, a method which fulfills the Immunization Criteria by reducing the mutual information between intermediate activations of harmful text sequences (their representations) and harmful text inputs before the attacker gains access and performs an HFA. Recall that after the attacker has access, the defender cannot intervene. There is evidence that current safety methods only suppress or route around preserved harmful representations [35, 38, 64]. This allows harmful behaviour to be easily recovered through HFAs. RepNoise instead aims to remove information about harmful tasks from intermediate activations over harmful text sequences, to make it difficult for the model to relearn such information in future. The formal definition of information removal removal is based on mutual information between intermediate activations and generative outputs based on those activations and is specified in appendix A.1 which we encourage review before proceeding. RepNoise consists of a three-part loss: (i) reduce the predictive information in the weights for generating harmful outputs, (ii) retain capabilities by minimizing loss on harmless inputs, and (iii) push harmful representations towards random noise to remove harmful information. Fig. 1 presents a high-level intuition of our method.

Transition Probabilities and Adversarial LossOur goal is to derive a loss function which will _minimize the likelihood of recovering the mutual information_\(I(Z_{harmful};Y_{harmful})\) which is a quantity that measures how effective intermediate activations (or representations) \(Z_{harmful}\) of harmful input sequences \(X_{harmful}\) are at predicting the output token distribution \(Y_{harmful}\). We are motivated by the observation in [1] that the number of training steps taken to fine-tune a model trained on one source task to another target task minimized by \(M_{\theta[t^{+}]}\) can be modeled as a transition probability over paths (or training trajectories) in a loss landscape. Formally in the language modeling case, a task here is a token output distribution over some dataset \(D\). The source task is our initial pre-training distribution and the target task is the generative distribution of harmful text tokens in \(D_{\text{harmful}}\). The loss landscape is the space (\(\mathbf{R}^{[\theta]}\)) of the value of a loss function \(\mathcal{L}_{D}\) at every value of \(\theta\). Paths or training trajectories in this landscape are the sequence of parameter configurations \(\theta_{i}\) during a training procedure for the iterates \(i\).

From [1], the transition probability is \(p(\theta_{t^{*}},t^{*}\,|\,\theta_{t=0},t=0)=\int_{0}^{t^{*}}p(\theta_{t}\,|\,\theta _{t=0},t=0)\,d\theta_{t}\). In other words, the probability of reaching \(M_{\theta[t^{*}]}\) at any given time step \(t\) is the accumulation of individual transition probabilities over all paths reaching \(\theta_{t^{*}}\) starting at \(t=0\). We can model the transition probability with two components: a _static distance_, which depends on the distance of the loss functions between the initial model \(\theta_{t=0}\) and the target model \(\theta_{t^{*}}\) that minimizes \(\mathcal{L}_{\mathcal{D}}\), and a dynamic _reachability term_ that depends on the magnitude of the gradients of the loss function with respect to parameters \(\theta_{t}\) and determines the number of paths during a training procedure that contain both \(\theta_{t=0}\) and \(\theta_{t^{*}}\) in the path sequence as defined above. To clarify, reachability is computed starting over all initial weight configurations, the outer integral \(\int_{\theta_{t=0}}^{\theta_{t^{*}}}\) and the paths starting from these initial weights that end in the optimal \(\theta_{t^{*}}\), the inner integral \(\int_{0}^{t^{*}}\), so that the probability is

\[p(\theta_{t^{*}},t^{*}\,|\,\theta_{t=0},t=0)\approx\underbrace{e^{-[\mathcal{ L}_{\mathcal{D}}(\theta_{t^{*}})-\mathcal{L}_{\mathcal{D}}(\theta_{t=0})]}}_{ \text{Static Potential}}\int_{\theta_{t=0}}^{\theta_{t^{*}}}\underbrace{e^{-\int_{ 0}^{t^{*}}\nabla\mathcal{L}_{\mathcal{D}}(\theta_{t})dt}}_{\text{ Reachability}}d\theta(t).\] (2)

Note that this is a simplified approximation of Achille et al. [1]; see appendix A for details. As the defender, we are only able to influence the initial set of weights \(\theta_{t=0}\). Therefore our goal is to find a way to modify the model weights so that we minimize eq. (2). Where we have:

**Theorem 1**.: _Consider a set of initial weights \(\theta_{t=0}\) as well as weights \(\theta_{t^{*}}\) that minimize a loss function \(\mathcal{L}_{\mathcal{D}}\) over the dataset \(\mathcal{D}\). The \(\theta_{t=0}\) that minimize the transition probability \(p(\theta_{t^{*}},t^{*}\,|\,\theta_{t=0},t=0)\) are given by the weights \(\theta_{t=0}\) that minimize the mutual information \(I(X;Z_{\theta})\) between the inputs to a neural network \(X\) drawn from \(\mathcal{D}\) and the intermediate activations of that neural network \(Z_{\theta}\) used to represent those inputs given the model weights \(\theta\). For which we have the minimizer \(\underset{\theta}{argmin}\,I(X;Z_{\theta})\)._

A full proof of this is given in appendix A. Based on information bottleneck theory [59], we view multiple-layer neural networks as consisting of an encoder which maps the inputs \(X\) to representations \(Z\) and a decoder which maps the learned \(Z\) to outputs \(Y\). From Wang et al. [63] theorem 2, we can minimize the mutual information \(I(Z;Y)\) directly by performing gradient ascent (\(\ell_{\text{ascent}}\)), which would decrease both the static distance and the reachability condition eq. (2) (see appendix A).

\[\ell_{\text{ascent}}=\mathbb{E}_{(X_{harmful},Y_{harmful})\sim D_{\text{ bound}}}\mathcal{L}(M_{\theta}(X_{harmful}),Y_{harmful}).\] (3)

However, gradient ascent over harmful samples can degrade overall language modeling capabilities, so we add a term to ensure that performance over harmless samples is not degraded. In view of the immunization conditions in Section 2, this ensures stability. Combining them, we get an adversarial loss function:

\[\mathcal{L}_{\text{Adversarial}}=\ell_{\text{stability}}-\beta\cdot\ell_{\text{ ascent}},\] (4)

where \(\alpha\) is a regularization term, and \(\ell_{\text{stability}}=\mathbb{E}_{(X,Y)\sim D_{\text{bursts}}}\mathcal{L}(M_{ \theta}(X),Y)\).

Representation NoisingAs we see later (Section 5), simply minimizing adversarial loss does not effectively remove the ability to predict harmful text sequences from the activations over harmful text sequences. This is because despite having low mutual information \(I(Y;Z)\), the mutual information between the inputs and the encoder layers \(I(Z;X)\) can still be high. Consequently, it is possible for representations to retain the ability to generate harmful token sequences.

The data processing inequality [65], \(I(Y;X)\leq I(Z;X)\) implies that minimizing \(I(X;Z)\) minimizes \(I(X;Y)\). We can do this by minimizing the KL divergence between the distribution of harmful representations given harmful input token sequences \(\mathcal{P}_{\theta[t=0]}(Z\,|\,X)\) and Gaussian noise \(\mathcal{N}(0,\bm{I})\): if these two distributions were the same then \(Z\) would have no information about \(X\)[65]. This yields the noise loss \(\ell_{\text{noise}}=KL(p(Z\,|\,X)\,||\,\mathcal{N}(0,\bm{I}))\). Combining this with eq. (4) using another regularization term \(\beta\), we get the loss function for Representation Noising (RepNoise) which satisfies Theorem 1.

\[\mathcal{L}_{\text{RepNoise}}=\mathcal{L}_{\text{Adversarial}}+\alpha\cdot \ell_{\text{noise}}=\ell_{\text{stability}}+\alpha\cdot\ell_{\text{noise}}- \beta\cdot\ell_{\text{ascent}}.\] (5)

We use a layer-wise approach to minimize \(\mathcal{L}_{\text{RepNoise}}\), with multi-kernel Maximum Mean Discrepancy (MMD) as a replacement for KL divergence that allows us to estimate the distribution of harmful representations. Full implementation details are in appendix B.1.

## 4 Experiments

We perform a series of experiments to evaluate how our defence meets the four immunization criteria in Section 2: we compare RepNoise with existing defence mechanisms in their ability to make llama-2-7b-chat resistant to HFAs SS 4.1 as well as evaluate RepNoise on Stability SS 4.2, Trainability SS 4.3, and Generalization SS 4.4.

### Resistance

Here we simulate an HFA on llama-2-7b-chat and measure the harmfulness of the models and a series of controls before and after these attacks. Appendix K reports similar experiments on llama-2-13b-chat and the safety-trained Qwen (0.5B to 7B) series of models. We perform HFAs in two domains: harmful question-answering and toxic content generation2. We measure attack strength in terms of the _learning rate_ and _number of samples_ used during supervised fine-tuning. Full details on our attack settings including rationale on learning rate choice can be found in appendix C.

Footnote 2: We experimented with malicious code generation tasks [9] but observed base models did not guard against malicious code generation to begin with which is a pre-condition of performing our defence.

To fine-tune for harmful question-answering, we use the BeaverTails harmful QA dataset [36] since it is a very large-scale dataset used in other attack literature [32], where the goal is to train an LLM to generate compliant answers to questions belonging to 14 categories of harm such as animal abuse and violent crime. For harmfulness evaluation, we use the logits of the harmful label after passing a question-answer pair into the harmfulness classifier trained on the BeaverTails dataset. The scores are computed as the mean of each individual logit score, for more details on how the classifier was trained as well as the scores is computed see appendix D.1.1. For toxic content generation, we use the DecodingTrust [62] split of Real Toxicity Prompts (RTP) [19] to fine-tune an LLM to generate highly toxic continuations. We perform toxicity evaluation using the mean toxicity scores from the Perspective API [39] (appendix D).

We compare RepNoise with several safety interventions and controls: the original model, a randomly initialised model (using Kaiming initialization [22]), additional safety training, gradient ascent, adversarial loss, and Security Vectors [70]. A randomly initialized model allows us to measure how quickly we converge to generating harmful tokens from random initial conditions (training a model from scratch). Additional safety training is done by supervised fine-tuning the model on refusals to answer 10k unsafe harmful question-answering samples from BeaverTails. Gradient ascent uses the loss function in eq. (3), (appendix J shows layer-wise implementation results) for defence. Adversarial loss minimizes eq. (4), and RepNoise minimizes eq. (1). Finally, we implement Security Vectors, a defence where the defender _does have control over the fine-tuning process_. We train a LoRA adapter on our harmful dataset and use the frozen adapter _during the HFA_ (appendix F).

Table 1 shows results for the harmful QA task. HFAs without any defence mechanism substantially increase the harmfulness score (Base) on the base model. Attacks with higher learning rates and more data tend to be stronger. This replicates previous results about the effectiveness of HFAs at

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Defence Mechanism & \multicolumn{3}{c}{\(3\times 10^{-5}\)} & \multicolumn{3}{c}{\(6\times 10^{-5}\)} & \multicolumn{3}{c}{\(8\times 10^{-5}\)} \\  & Pre-attack & 1k & 10k & 1k & 10k & 1k & 10k \\ \hline Base: llama2-7b-chat & 0.05 & 0.47 & 0.74 & 0.73 & 0.72 & 0.74 & 0.73 \\ Random & 0.00 & 0.46 & 0.86 & 0.49 & 0.84 & 0.47 & 0.82 \\ \hline Security Vectors & 0.05 & **0.07** & **0.08** & 0.23 & 0.37 & 0.52 & 0.66 \\ Vaccine (\(\rho=1\)) & 0.05 & 0.28 & 0.73 & 0.70 & 0.73 & 0.72 & 0.76 \\ Vaccine (\(\rho=10\)) & 0.05 & 0.28 & 0.72 & 0.75 & 0.72 & 0.76 & 0.73 \\ \hline Additional safety training & 0.05 & 0.75 & 0.76 & 0.75 & 0.75 & 0.76 & 0.74 \\ Gradient ascent & 0.24 & 0.38 & 0.74 & 0.58 & 0.74 & 0.68 & 0.77 \\ Adversarial loss & 0.05 & 0.26 & 0.70 & 0.64 & 0.75 & 0.77 & 0.77 \\ RepNoise & 0.05 & 0.08 & 0.12 & **0.10** & **0.13** & **0.11** & **0.12** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average harmfulness classifier scores before and after attacks performed using 1k and 10k samples of harmfulQA from BeaverTails and learning rates \(\in\{3\times 10^{-5},6\times 10^{-5},8\times 10^{-5}\}\). Blue indicates lower harmfulness score than the base model.

circumventing safety training in LLMs [8; 40; 49; 51; 66]. Evaluating our defence mechanisms, Security Vectors provides some resistance but _RepNoise is the only defence method to consistently able to provide significant resistance across all attacks_ (Mann-Whitney \(U\)-test, \(p<0.001\)).3

Footnote 3: We note that stronger attacks with thorough hyperparameter search can still succeed against RepNoise which we discuss in Appendix E.1 so it should not yet be considered as a comprehensive defence.

Gradient ascent and adversarial loss offer some resistance for weak attacks, but they fail for stronger attacks. We hypothesize that harmful text generation is recovered quickly with these approaches because they leave the representation structure of harmful text sequeces intact (see SS 5). Randomly initializing an LLM is not a useful control for understanding HFAs, since simply fine-tuning with larger samples makes the model mimic harmful text from the dataset. Finally, additional safety training offers no resistance, indicating that some types of traditional safety methods (safety-oriented supervised fine-tuning) does not help to defend against HFAs.

Table 2 presents similar results for the toxic content generation task. In this case, there are 351 attack samples, so we vary attack strength across learning rates only, performing all HFAs for 4 epochs. In each setting, using a model immunized with RepNoise results in complete resistance.

### Stability

To evaluate if RepNoise causes a deterioration in unrelated harmless tasks compared to the base model, we use standard LLM benchmarks from the Eleuther AI LM Evaluation Harness [18]: TruthfulQA [41], MMLU [26], Hellaswag [69], and ARC-easy [13]. We also evaluate changes in the model's capabilities on domains related to harmfulness using the Ethics [25] and CrowS-Pairs [48] datasets.

Table 3 shows that a llama2-7b-chat model immunized using RepNoise achieves similar scores as the base model across all evaluations, indicating that RepNoise does not degrade capability. Beyond performance evaluations, our method does not degrade performance on other safety benchmarks, i.e. Ethics, or CrowS-Pairs. We perform further investigations on whether RepNoise has any effect on fairness (appendix E.4), exaggerated safety (appendix E.5), or adversarial robustness (appendix E.6)--with the general finding that RepNoise neither degrades nor improves inference-time safety over a baseline safety-guarded model which implies that RepNoise would supplement rather than replace other defence methods.

### Trainability

Recall that Trainability is the defence condition from above that states that after applying defences models should still be able to be trained effectively on harmless datasets. The reason for this is that defences which remove or degrade training on harmless datasets are less useful than ones that do not under our threat model where defenders want to release these models such that they can still be trained on harmless tasks.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & Pre-attack & \(3\times 10^{-5}\) & \(6\times 10^{-5}\) & \(8\times 10^{-5}\) \\ \hline Base & 0.24 & 0.40 & 0.74 & 0.71 \\ \hline Security Vectors & 0.17 & 0.16 & 0.36 & 0.35 \\ Vaccine (\(\rho=1\)) & 0.19 & 0.46 & 0.70 & 0.72 \\ \hline Gradient Ascent & 0.05 & 0.12 & 0.44 & 0.76 \\ Adversarial loss & 0.00 & 0.00 & 0.77 & 0.78 \\ RepNoise & 0.17 & 0.00 & 0.05 & 0.07 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Toxicity score from Perspective API when the model is requested to continue highly toxic prompts. RepNoise is able to defend against training models for toxic content generation.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline
**Model** & **TruthfulQA** & **MMLU** & **Hellaswag** & **Winogrande** & **ARC** & **Ethics** & **CrowS** \\ \hline Base & 0.38 & 0.46 & 0.58 & 0.66 & 0.74 & 0.59 & 0.64 \\ RepNoise & 0.37 & 0.45 & 0.57 & 0.66 & 0.72 & 0.60 & 0.63 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Evaluation of RepNoise on common language model capability benchmarks.

We evaluate Trainability by testing whether the defended model can still be trained towards harmless tasks. To this end, we measure the ROUGE-1 unigram overlap score on several text-to-data tasks from the GEM benchmark [20]. In order to demonstrate trainability, we need to choose standard validated tasks for natural language generation that models are poor (zero-shot) at before training (very low ROUGE-1 scores) and achieve large performance increases in after training. We observe this for the base llama2-7b-chat model seeing consistently low initial scores in table 4. For this setting, we train the base model and its post-RepNoise version using 1 epoch and a learning rate of \(8\times 10^{-5}\), using only the training splits of each dataset. We perform evaluations on the test splits of respective datasets. Full details of each dataset are given in appendix D.

The results in table 4 show that a llama2-7b-chat model hardened using RepNoise _retains the capability to be further trained on harmless tasks_, despite not being able to be trained on harmful tasks.

We further evaluated whether fine-tuning on a harmless task results in undoing safety guards or makes models more susceptible to HFAs. After fine-tuning on each GEM dataset, a HFA is performed with \(3\times 10^{-5}\) with 1k samples from BeaverTails as above. Unlike the results of Qi et al. [51], both the base model and RepNoise are not made more harmful after harmless fine-tuning on GEM. However, training on GEM does seem to make the HFA more effective (readers can compare with the same attack in table 1). Even for RepNoise we see a small increase in attack efficacy after training the model on CACAPO which indicates the possibility that additional harmless fine-tuning could undo the RepNoise defence, a vulnerability which future work should explore.

We replicated the benign and AOA attacks of Qi et al. [51] results in appendix E.3 and found that RepNoise can mitigate them.

### Generalization

The BeaverTails dataset categorizes samples into 14 types of harm. We evaluate the generalization performance of RepNoise by withholding five categories of harm when performing the defence training and then evaluate the attack by performing an attack using 1k samples from that subset. We also perform an additional experiment (**Half**) where RepNoise is trained using 5k randomly selected samples from BeaverTails and a subsequent attack is performed using 5k unseen samples.

The results in Table 5 show that a defence using RepNoise is able to generalize to a defence against HFAs performed with unseen samples and unseen types of harm. However, it is important to note that these attacks are still in-distribution since the unseen types of harm are still drawn from the same

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **ViGGO** & **E2E NLG** & **DART** & **CACAPO** & **ConvWeather** \\
**ROUGE-1** & & & & & & \\ \hline Base & 0.19 / 0.83 & 0.20 / 0.74 & 0.23 / 0.53 & 0.18 / 0.66 & 0.06 / 0.25 \\ RepNoise & 0.20 / 0.83 & 0.25 / 0.74 & 0.25 / 0.53 & 0.18 / 0.67 & 0.08 / 0.25 \\ \hline
**Harmfulness** & & & & & \\ \hline Base & 0.03/0.75 & 0.05/0.65 & 0.05/0.69 & 0.06/0.67 & 0.05/0.55 \\ RepNoise & 0.00/0.00 & 0.16/0.01 & 0.00/0.00 & 0.02/0.27 & 0.01/0.08 \\ \hline \hline \end{tabular}
\end{table}
Table 4: ROUGE-1 score of RepNoise on GEM structured generation tasks before/after being fine-tuned. Harmfulness scores before and after performing an attack at learning rate \(3\times 10^{-5}\) with 1k samples from BeaverTails.

\begin{table}
\begin{tabular}{l l c c c c c c} \hline \hline
**LR** & **Model** & **Crime** & **Privacy** & **Toxic** & **Violence** & **Sexually explicit** & **Half** \\ \hline \(3\times 10^{-5}\) & Base & 0.49 & 0.51 & 0.40 & 0.52 & 0.53 & 0.35 \\  & RepNoise & 0.08 & 0.05 & 0.06 & 0.09 & 0.01 & 0.08 \\ \hline \(6\times 10^{-5}\) & Base & 0.76 & 0.75 & 0.76 & 0.75 & 0.81 & 0.76 \\  & RepNoise & 0.10 & 0.09 & 0.10 & 0.09 & 0.00 & 0.12 \\ \hline \(8\times 10^{-5}\) & Base & 0.77 & 0.75 & 0.80 & 0.74 & 0.76 & 0.74 \\  & RepNoise & 0.13 & 0.12 & 0.12 & 0.14 & 0.00 & 0.10 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Harmfulness scores after performing fine-tuning on harm types withheld during the RepNoise defence.

BeaverTails distribution that the defender has seen. Importantly, RepNoise is not an effective defence against unseen sample out-of-distribution which we demonstrate in appendix E.2 using a distribution shift in the attack set to the HEX-PHI attack set [51]. A comprehensive analysis of additional attacks (appendix E) and ablations (appendix J) are presented so readers can clearly understand the limitations of RepNoise.

## 5 Mechanistic Analysis

We conjecture that RepNoise works because it reduces the information about harmfulness in representations _across all layers_ of the neural network, making them harder to recover. This is inspired by observations from previous studies, which found that popular safety methods merely route around the harmful representations [38], that fine-tuning only learns a wrapper on top of existing representations [35], and that harmful representations are easily recovered [64].

Model WeightsTo illustrate the above conjecture, we measure the change in the weights of each layer across various defence mechanisms (a method common in unlearning literature, see Tarun et al. [58] for example). In Fig. 2, we plot layer-wise \(L_{2}\) differences between the weights of the original model or a defence and the weights of a harmfully fine-tuned base model (using BeaverTails with LR \(8\times 10^{-5}\) @ 10k samples). We observe that defence using adversarial loss is indeed "superficial," in that the largest difference is observed in the last layers. In comparison, weight change across layers is more uniform for RepNoise.

However, we can't be certain what these weight changes mean. In order to actually test our conjecture about depth, we perform RepNoise but freeze the top layers, the middle 10 layers, and the earliest 10 layers (table 6). Freezing the LM Head or the layers between 20 and 31 makes little to no difference and not much difference for lower sample sizes freezing the last layer, freezing the middle layers degrades the performance of RepNoise, and freezing the earliest layers results in a complete lack of defence. This result confirms our conjecture about the necessity of "depth" for effective defence.

Token ProbabilitiesTo investigate the degree to which harmful representations are removed across layers we can look at how harmful and harmless token sequences are promoted throughout the network. We look at the mean log probability of 100 randomly selected harmful and harmless

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(3\times 10^{-5}\) @ 1k & \(3\times 10^{-5}\) @ 10k & \(6\times 10^{-5}\) @ 1k \\ \hline Undefended Model & 0.47 & 0.74 & 0.73 \\ All Layers & 0.08 & 0.12 & 0.10 \\ Freeze LM Head & 0.08 & 0.10 & 0.11 \\ Freeze Last Layer & 0.08 & 0.67 & 0.09 \\ Freeze Layers 20-31 & 0.10 & 0.13 & 0.10 \\ Freeze Layers 10-20 & 0.13 & 0.55 & 0.56 \\ Freeze Layers 0-10 & 0.73 & 0.73 & 0.72 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Freezing earlier layers prevents effective defence indicating that the ‘depth’ of the defence is critical.

Figure 2: \(L_{2}\) distance between weights of each layer between the base model, a successfully attacked model and two defences. RepNoise’s differences spread through the layers compared to Adversarial loss where the weight differences are concentrated at the later layers indicative of superficial defence.

samples throughout the layers by placing the language model head on the activations across each layer [7]. Confirming our findings above (Fig. 3) adversarial loss leads to a shallow defence that mostly reduces the likelihood of the harmful token sequences towards the last layer. In contrast, RepNoise_demotes harmful tokens across layers mostly uniformly_.

Knowledge RepresentationsFig. 4 illustrates the representation space learned by each model. After shallow defences, harmful representations maintain distinct structures along each of the two principal component directions. While RepNoise maintains separability between harmful and harmless sequences along one of the principal components, the "spread" of each harmful representation in both directions is dramatically reduced compared other models. This corroborates that RepNoise has reduced the representation quality of the harmful samples since we can't find a projection that illustrates any meaningful structure between these samples.

To further analyze the information about harmfulness contained in the representations of different models, we train a linear probe to predict whether an input to a model was harmful or not, based on the mean activations at each layer of the model. Such a probe can achieve high accuracy by using the information about harmfulness in the LLM. For each model, we input 15k examples from BeaverTails, with half being unsafe, and collect the average activations across each layer for each sample. We then train a binary linear classifier on 80% of them, measure the resulting accuracy on a held-out test set, and repeat with 10 random seeds. 4

Footnote 4: We used an earlier RepNoise trained with \(\beta=4\), additional results presented in appendix G.

Across all models, the probes perform best in the middle layers and very poorly in earlier layers. Figure 4(a) shows that an HFA does not improve the probe's accuracy compared to the base model. Similarly, an attack on a model defended by RepNoise also does not increa

Figure 4: PCA across 100 harmful and harmless samples from BeaverTails on the activations of the last layer.

Figure 5: Harmful probe accuracy on (a) base model and attacked model, (b) base model and models trained with RepNoise (\(\beta=4\)) and adversarial Loss, and (c) base model, RepNoise model and an attacked RepNoise model

Figure 3: Log probability of harmful and harmless sequences across layers. Notice how adversarial loss mostly depromotes harmful tokens towards the last layer. This is done more evenly across layers for RepNoise indicating comprehensive and deep information removal.

harmfulness (Figure 4(c)). This indicates that HFAs do not make an LLM more harmful _by learning new information about harmfulness_, but merely use information already contained in the model.

_The probe achieves significantly (Student's \(t\)-test, \(p<5e-12\)) lower accuracy on the model defended by RepNoise than for a model defended by adversarial loss or the base model_ (Figure 4(b)). This supports our suggestion that adversarial loss does not remove information about harmful representations from the base model, but RepNoise does. Lastly, Figure 4(c) illustrates that fine-tuning using harmful data does not result in relearning the information removed by RepNoise.

## 6 Related Work

Preserving the effects of safety fine-tuningSome prior work addresses the attenuation of safety fine-tuning's influence on model behavior which typically occurs during benign fine-tuning. [71] achieve this for LLMs with an instruction fine-tuning dataset which contains safety material and [44] do so for LLMs by modifying the prompt template used during fine-tuning. [28] use a modified LoRA algorithm for fine-tuning which maintains safety influence and [68] use a model fusion-based technique to get around the limitations of performing safety fine-tuning either before or after fine-tuning on tasks. Other solutions could benefit from methods that correct the general tendency for models to perform more poorly in some domains after being fine-tuned in others, such as the method presented in [46]. Though these methods may be sufficient for their stated goals, our work aims to mitigate the effects of harmful fine-tuning, regardless of whether they come about from benign or harmful fine-tuning.

Defence against harmful fine-tuningFew works have attempted to defend against HFAs. Meta-learning approaches have been used to reduce the fine-tunability of Language Models for harmful classification tasks [24] and prevent image classification models from being fine-tuned on restricted domains [14]. However, meta-learning approaches can be uninterpretable and too computationally expensive for LLMs. [70] added a security vector during training to trick a model into thinking that it has already learned the harmful task. [32] keep embeddings close to the original embeddings by adding a perturbation loss called 'Vaccination' and [31] provides a similar defence by ensuring that weights don't drift too far from original weights during training. While [31; 32; 70] assume the defender retains control over the fine-tuning process, we focus on settings where the defender cannot intervene after the weights are released or stolen. For a full review of current HFAs and threat models, we refer readers to [53].

## 7 Limitations

The primary limitation of RepNoise is that it is still possible to find ways to defeat it at higher learning rates and with more data (appendix E.1). It is also sensitive to variations in hyperparameter choices (appendix J). We have evidence that RepNoise could be improved quite simply by doing more comprehensive hyperparameter searches and constructing larger defensive datasets. However, our method requires paired safe and unsafe examples which makes data collection more expensive and complex. Finally, while we did demonstrate across in-distribution harmful subsets in BeaverTails, we did not observe out-of-distribution generalization from defences on harmful question-answering to attacks using toxic content generation. Even smaller distribution shifts such as from defence using BeaverTails to an unseen harmful question-answering dataset HEX-PHI (appendix E.2) can break RepNoise --as such, future work should focus on improving the generalization capabilities of RepNoise as it is unlikely that defenders will have access to samples with significant in-distribution overlap with attackers which limits the effectiveness of our proposed method.

While our empirical settings and attacks provide promising first directions for LLM immunization research, future work should invest in stronger attack settings to emulate worst-case attacks and investigate different types of harm. Finally, our work is limited to supervised fine-tuning attacks in LLMs. Additional settings in different modalities such as evaluating attempts at developing malicious agents through harmful reinforcement learning (e.g., "reverse DPO" [67]) are a critical topic for future research. We explored the implications of RepNoise for inference-time adversarial attacks (appendix E.6) but future work should explore the robustness of RepNoise to additional types of attacks like latent adversarial attacks [11], activation engineering-based attacks [3] or adaptive attacks such as using decoding-time modifications [42] to circumvent our defence.

## Acknowledgments and Disclosure of Funding

We acknowledge the support of the Killam foundation, Vector Institute, the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN-2022-03943, Canada Foundation of Innovation (CFI) and Research Nova Scotia. Advanced computing resources are provided by ACENET, the regional partner in Atlantic Canada, and the Digital Research Alliance of Canada and the Vector Institute. FR is supported by a Canada CIFAR Chair in AI. We thank Ben Rank for providing the implementation of Vaccine that we use in this paper. We'd also like to acknowledge the useful discussions and feedback we have had with other authors in the field namely Tiansheng Huang, Boyi Wei, and Xiangyu Qi. The project on which this report is based was funded by the Federal Ministry of Education and Research under the funding code 16KIS2012. The responsibility for the content of this publication lies with the author

## References

* [1] Alessandro Achille, Glen Mbeng, and Stefano Soatto. Dynamics and reachability of learning tasks, 2019, arXiv preprint:1810.02440.
* [2] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. _IEEE transactions on pattern analysis and machine intelligence_, 40(12):2897-2905, 2018.
* [3] Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and Neel Nanda. Refusal in language models is mediated by a single direction. _arXiv preprint arXiv:2406.11717_, 2024.
* [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023, arXiv preprint:2309.16609.
* [5] Anusha Balakrishnan, Jinfeng Rao, Kartikeya Upasani, Michael White, and Rajen Subba. Constrained decoding for neural NLG from compositional representations in task-oriented dialogue. In Anna Korhonen, David Traum, and Lluis Marquez, editors, _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 831-844, Florence, Italy, July 2019. Association for Computational Linguistics.
* [6] Yonatan Belinkov. Probing Classifiers: Promises, Shortcomings, and Advances. _Computational Linguistics_, 48(1):207-219, 04 2022, arXiv preprint:https://direct.mit.edu/coli/article-pdf/48/1/207/2006605/coli_a_00422.pdf.
* [7] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions from transformers with the tuned lens, 2023, arXiv preprint:2303.08112.
* [8] Rishabh Bhardwaj and Soujanya Poria. Language model unalignment: Parametric red-teaming to expose hidden harms and biases. _arXiv preprint arXiv:2310.14303_, 2023.
* [9] Manish Bhatt, Sahana Chennabasappa, Cyrus Nikolaidis, Shengye Wan, Ivan Evtimov, Dominik Gabi, Daniel Song, Faizan Ahmad, Cornelius Aschermann, Lorenzo Fontana, Sasha Frolov, Ravi Prakash Giri, Dhaval Kapil, Yiannis Kozyrakis, David LeBlanc, James Milazzo, Aleksandar Straumann, Gabriel Synnaeve, Varun Vontimitta, Spencer Whitman, and Joshua Saxe. Purple llama cybersceeval: A secure coding benchmark for language models, 2023, arXiv preprint:2312.04724.
* [10] Nicholas Carlini, Daniel Paleka, Krishnamurthy Dj Dvijotham, Thomas Steinke, Jonathan Hayase, A. Feder Cooper, Katherine Lee, Matthew Jagielski, Milad Nasr, Arthur Conmy, Eric Wallace, David Rolnick, and Florian Tramer. Stealing part of a production language model, 2024, arXiv preprint:2403.06634.

* Casper et al. [2024] Stephen Casper, Lennart Schulze, Oam Patel, and Dylan Hadfield-Menell. Defending against unforeseen failure modes with latent adversarial training, 2024, arXiv preprint:2403.05030.
* Chan et al. [2023] Alan Chan, Ben Bucknall, Herbie Bradley, and David Krueger. Hazards from increasingly accessible fine-tuning of downloadable foundation models, 2023, arXiv preprint:2312.14751.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Deng et al. [2024] Jiangyi Deng, Shengyuan Pang, Yanjiao Chen, Liangming Xia, Yijie Bai, Haiqin Weng, and Wenyuan Xu. SOPHON: Non-fine-tunable learning to restrain task transferability for pre-trained models, 2024, arXiv preprint:2404.12699.
* Dusek et al. [2019] Ondrej Dusek, David M Howcroft, and Verena Rieser. Semantic Noise Matters for Neural Natural Language Generation. In _Proceedings of the 12th International Conference on Natural Language Generation (INLG 2019)_, pages 421-426, Tokyo, Japan, 2019.
* Esiobu et al. [2023] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Michael Smith. Robbie: Robust bias evaluation of large generative language models, 2023, arXiv preprint:2311.18140.
* Ganguli et al. [2022] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022, arXiv preprint:2209.07858.
* Gao et al. [2023] Leo Gao, Jonathan Tow, Baher Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.
* Gehman et al. [2020] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxicityPrompts: Evaluating neural toxic degeneration in language models, 2020, arXiv preprint:2009.11462.
* Gehrmann et al. [2021] Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamachi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondrej Dusek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisatrua Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomery Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Joao Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics. In Antoine Bosselut, Esin Durmus, Varun Prashant Gangal, Sebastian Gehrmann, Yacine Jernite, Laura Perez-Beltrachini, Samira Shaikh, and Wei Xu, editors, _Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)_, pages 96-120, Online, August 2021. Association for Computational Linguistics.
* Gopal et al. [2023] Anjali Gopal, Nathan Helm-Burger, Lennart Justen, Emily H. Soice, Tiffany Tzeng, Geetha Jeyapragasan, Simon Grimm, Benjamin Mueller, and Kevin M. Esvelt. Will releasing the weights of future large language models grant widespread access to pandemic agents?, 2023, arXiv preprint:2310.18233.

* He et al. [2015] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification, 2015, arXiv preprint:1502.01852.
* He et al. [2023] Pengcheng He, Jianfeng Gao, and Weizhu Chen. DeBERTaV3: Improving DeBERTa using ELECTRA-Style pre-training with gradient-disentangled embedding sharing, 2023, arXiv preprint:2111.09543.
* Henderson et al. [2023] Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. Self-destructing models: Increasing the costs of harmful dual uses of foundation models. In _Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society_, pages 287-296, 2023.
* Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. Aligning AI with shared human values. In _International Conference on Learning Representations_, 2020.
* Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Holtzman et al. [2020] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration, 2020, arXiv preprint:1904.09751.
* Hsu et al. [2024] Chia-Yi Hsu, Yu-Lin Tsai, Chih-Hsun Lin, Pin-Yu Chen, Chia-Mu Yu, and Chun-Ying Huang. Safe lora: the silver lining of reducing safety risks when fine-tuning large language models. _arXiv preprint arXiv:2405.16833_, 2024.
* Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021, arXiv preprint:2106.09685.
* Huang et al. [2021] Hanxun Huang, Xingjun Ma, Sarah Monazam Erfani, James Bailey, and Yisen Wang. Unlearnable examples: Making personal data unexploitable, 2021, arXiv preprint:2101.04898.
* Huang et al. [2024] Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, and Ling Liu. Lazy safety alignment for large language models against harmful fine-tuning, 2024, arXiv preprint:2405.18641.
* Huang et al. [2024] Tiansheng Huang, Sihao Hu, and Ling Liu. Vaccine: Perturbation-aware alignment for large language model. _arXiv preprint arXiv:2402.01109_, 2024.
* Hubinger et al. [2024] Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, Soren Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, and Ethan Perez. Sleeper agents: Training deceptive llms that persist through safety training, 2024, arXiv preprint:2401.05566.
* Jain et al. [2023] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompalli, John Kirchenbauer, Ping Yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models, 2023, arXiv preprint:2309.00614.
* Jain et al. [2023] Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P. Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktaschel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks, 2023, arXiv preprint:2311.12786.
* Ji et al. [2023] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 24678-24704. Curran Associates, Inc., 2023.

* [37] Juraj Juraska, Kevin Bowden, and Marilyn Walker. ViGGO: A video game corpus for data-to-text generation in open-domain conversation. In _Proceedings of the 12th International Conference on Natural Language Generation_, pages 164-172, Tokyo, Japan, October-November 2019. Association for Computational Linguistics.
* [38] Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, and Rada Mihalcea. A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity. _ArXiv_, abs/2401.01967, 2024.
* [39] Alyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A new generation of perspective api: Efficient multilingual character-level transformers, 2022, arXiv preprint:2202.11176.
* [40] Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b. _arXiv preprint arXiv:2310.20624_, 2023.
* [41] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics.
* [42] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A. Smith. Tuning language models by proxy, 2024, arXiv preprint:2401.08565.
* [43] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. _arXiv preprint arXiv:2305.13860_, 2023.
* [44] Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. Keeping llms aligned after fine-tuning: The crucial role of prompt templates. _arXiv preprint arXiv:2402.18540_, 2024.
* [45] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluation framework for automated red teaming and robust refusal, 2024, arXiv preprint:2402.04249.
* [46] Jishnu Mukhoti, Yarin Gal, Philip Torr, and Puneet K. Dokania. Fine-tuning can cripple foundation models; preserving features may be the solution, 2024.
* [47] Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutthia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. DART: Open-domain structured data record to text generation. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 432-447, Online, June 2021. Association for Computational Linguistics.
* [48] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman. CrowS-Pairs: A challenge dataset for measuring social biases in masked language models. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1953-1967, 2020.
* [49] Kellin Pelrine, Mohammad Taufeeque, Michal Zajc, Euan McLean, and Adam Gleave. Exploiting novel gpt-4 epis. _arXiv preprint arXiv:2312.14302_, 2023.
* [50] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. _arXiv preprint arXiv:2202.03286_, 2022.

* [51] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! _arXiv preprint arXiv:2310.03693_, 2023.
* [52] Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. Smoothllm: Defending large language models against jailbreaking attacks, 2024, arXiv preprint:2310.03684.
* [53] Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Jan Batzner, Hassan Sajjad, and Frank Rudzicz. Immunization against harmful fine-tuning attacks, 2024, arXiv preprint:2402.16382.
* [54] Paul Rottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. XSTest: A test suite for identifying exaggerated safety behaviours in large language models, 2024, arXiv preprint:2308.01263.
* [55] Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, and Deyi Xiong. Large language model alignment: A survey, 2023, arXiv preprint:2309.15025.
* [56] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models. _arXiv preprint arXiv:2308.03825_, 2023.
* [57] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* [58] Ayush K. Tarun, Vikram S. Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning. _IEEE Transactions on Neural Networks and Learning Systems_, page 1-10, 2024.
* [59] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In _2015 ieee information theory workshop (itw)_, pages 1-5. IEEE, 2015.
* [60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [61] Chris van der Lee, Chris Emmery, Sander Wubben, and Emiel Krahmer. The CACAPO dataset: A multilingual, multi-domain dataset for neural pipeline and end-to-end data-to-text generation. In Brian Davis, Yvette Graham, John Kelleher, and Yaji Sripada, editors, _Proceedings of the 13th International Conference on Natural Language Generation_, pages 68-79, Dublin, Ireland, December 2020. Association for Computational Linguistics.
* [62] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models, 2024, arXiv preprint:2306.11698.
* [63] Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. Non-Transferable Learning: A New Approach for Model Ownership Verification and Applicability Authorization. October 2021.
* [64] Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning and low-rank modifications. _arXiv preprint arXiv:2402.05162_, 2024.
* [65] Tailin Wu, Ian Fischer, Isaac L. Chuang, and Max Tegmark. Learnability for the information bottleneck. _Entropy_, 21(10):924, September 2019.
* [66] Xianjun Yang, Xiao Wang, Qi Zhang, Linda Petzold, William Yang Wang, Xun Zhao, and Dahua Lin. Shadow alignment: The ease of subverting safely-aligned language models. _arXiv preprint arXiv:2310.02949_, 2023.

* [67] Jingwei Yi, Rui Ye, Qisi Chen, Bin Benjamin Zhu, Siheng Chen, Defu Lian, Guangzhou Sun, Xing Xie, and Fangzhao Wu. Open-source can be dangerous: On the vulnerability of value alignment in open-source LLMs, 2024.
* [68] Xin Yi, Shunfan Zheng, Linlin Wang, Xiaoling Wang, and Liang He. A safety realignment framework via subspace-oriented model fusion for large language models. _arXiv preprint arXiv:2405.09055_, 2024.
* [69] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 4791-4800, 2019.
* [70] Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, and Xuanjing Huang. Making harmful behaviors unlearnable for large language models. _arXiv preprint arXiv:2311.02105_, 2023.
* [71] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety fine-tuning at (almost) no cost: A baseline for vision large language models. _arXiv preprint arXiv:2402.02207_, 2024.
* [72] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023, arXiv preprint:2307.15043.

Proofs and Mathematical Details

### Preliminaries

There are a number of terms used throughout the paper that require precise definitions where the main text is only able to provide a concise introduction. In this section, we will formally define the notions important to understanding the RepNoise algorithm and it's theoretical analysis. First, we consider the weights of a deep neural network denoted \(\theta\) as the parameters which are learned using some optimization procedure like that of the harmful fine-tuning procedure described in the main text in eq.1, typically using stochastic gradient descent which we will return to shortly. We rely on the information bottleneck [59] view which states that neural networks form a Markov chain between the following random variables: the inputs of the network \(X\), the representation of those inputs \(Z\), and the outputs \(Y\) which can predicted only from the representations \(Z\). Here, we draw on the notion of representation from [2] which states that \(Z\) is a representation of \(X\) if in the chain described above \(Z\) provides desirable properties for tasks involving \(Y\). Here, the task of interest is predicting tokens \(\hat{Y}\) such that those predicted tokens minimize some loss function \(\mathcal{L}(\hat{Y},Y)\) over a reference token distribution \(Y\). In this paper, the representations \(Z\) will take the form of intermediate activations that are built up through linear transformations and activation functions in the transformer models that we use. Precisely, the neural network is the function \(f_{\theta}(x)=\hat{y}\) parameterized by \(\theta\) composed of activation functions \(z_{i+1}=h(\theta_{i}\cdot z_{i})\) where \(z_{i=0}\) is an initial embedding of \(x\) such as through the use of a learned token embedding matrix common to LLM architectures and the final layer consists of an unembedding layer such as a weight matrix over a vocabulary space with a softmax function. In this process, since \(\hat{y}\) is predicted through the intermediate \(z_{i}\) activations then \(z_{i}\) meets the criteria of being a representation of the initial input \(x\). While the subspace that these activations span is completely determined by the weights \(\theta\) themselves, we will not be referring to weights as representations in this paper.

To connect these representations back to an information-theoretic perspective, we will say that the information of any given representation \(Z\) is the typical Shannon entropy measure of discrete random variables \(H(Z)=\mathbb{E}[-\log p(Z)]\) where \(p(x)=P(X=x)\). In this paper, we are concerned not with the information content of representations themselves but with the notion of _mutual information_: the amount of information one random variable gives us about another random variable. Formally, the (shannon) mutual information \(I(X;Z)\) is given by the information content \(H(X)\) minus the conditional entropy (or information content) \(H(X|Z)\) for the formula \(I(X;Z)=H(X)-H(X|Z)\). An equivalent distributional view of mutual information, which will will use later, can be given by the Kullback-Leibler divergence \(I(X;Z)=\mathbb{E}_{x\sim P_{x}}[D_{KL}(P(z|x)||P(z)]\).

Given these tools we can represent a neural network as an information bottleneck which means that there is a Markov chain \(Y\gets Z\gets X\) that has what is called a data processing inequality given by: \(I(Y;X)\leq I(X;Z)\). We can also derive two ideal properties [2] of representations: _sufficiency_ - the representations \(Z\) have the same essential information in \(X\) to pick out \(Y\) i.e. \(I(Z;Y)=I(X;Z)\); and _minimality_ - the representations \(Z\) should contain as little information of the input space as possible \(\min I(X;Z)\). Finally, we present one more notion to formalize the idea of removing information from a representation. We say that a representation \(Z\) has no information about a random variable \(Y\) when the mutual information \(I(Z;Y)\) is 0. The process of removing information more precisely means reducing the mutual information between these two random variables. As long as the _sufficiency_ condition is met, removing information of representations \(Z\) and outputs \(Y\) doesn't reduce the predictive capabilities of a neural network. In the context of unlearning and the mitigating learning harmful text sequences, we want to remove mutual information between representations \(Z\) and outputs \(Y\) such that the _sufficiency condition is not met_ and this is what we will mean precisely when we say removing information in the paper.

Finally, we need to present a few preliminaries from [1] in order to make the theoretical analysis clear. Below we will present a transition probability \(p(s_{2},t_{2}|s_{1},t_{1})\) model which is a model of the likelihood of transitioning from one state \(s_{1}\) to another \(s_{2}\) at time step \(t_{1}\) and \(t_{2}\), readers familiar with reinforcement learning will recall that this is similar to the notion of a dynamics model of the environment. The transition probability model will consider the likelihood of transitioning between one set of model parameters \(\theta_{1}\) to another set of parameters \(\theta_{2}\) during the process of stochastic gradient descent over some loss function \(L_{\theta_{i}}\). The loss landscape is the space (\(\mathbf{R}^{l}\theta|\)) of the value of a loss function \(\mathcal{L}_{D}\) at every value of \(\theta\). For simiplicity we are assuming this loss function is computedover all of the samples of a given dataset \(D\) to construct our theoretical loss landscape object. We will develop a transition probability based on the static potential and reachability between two parameter sets. The difference \(\mathcal{L}_{D\theta_{*}}-\mathcal{L}_{D\theta_{\beta}}\) between the loss for one parameter configuration \(\theta_{i}\) and \(\theta_{j}\) will be called the static potential below. Reachability will be developed precisely below. Paths or training trajectories in the loss landscape are the sequence of parameter configurations \(\theta_{i}\) during a training procedure

### Deriving the approximate transition probability

Motivated by the transfer learning question: "How can we predict the success of training a model originally trained on Task A to Task B", Achille et al. [1] present a transition probability that determines the likelihood an initial model with parameters \(\theta_{0}\) can traverse the loss landscape to find the parameters \(\theta_{*}\) that minimize the loss function \(\mathcal{L}_{\mathcal{D}}\) which represents a loss like causal language modelling loss on some dataset \(\mathcal{D}\). Here a Task is defined as a token output distribution over some dataset \(\mathcal{D}\).

They posit the following transition probability \(p(\theta_{*},t_{*}|\theta_{0},\theta_{0})\) as equal to:

\[p(\theta_{*},t_{*}|\theta_{0},t_{0})=\underbrace{e^{-\frac{1}{2\theta}[ \mathcal{L}_{D\theta_{*}}-\mathcal{L}_{D\theta_{0}}]}}_{\text{Static potential}}\int_{\theta_{0}}^{\theta_{*}}\frac{e^{-\frac{1}{2\mathcal{D}}\int_{t_{0}}^{t_{*}} \frac{1}{2}\dot{w}(t)^{2}+V(w(t))dt}}{\text{Reachability}}\,dw(t).\] (6)

As mentioned in the preliminaries static potential \([U(\theta_{*})-U(\theta_{0})]\) measures how far an initial set of parameters is from a final set of parameters that minimize some loss term is given as the difference of loss between those two models \(\mathcal{L}_{\mathcal{D}\theta_{*}}-\mathcal{L}_{\mathcal{D}\theta_{0}}\). The stochastic factor \(D\) comes from the author's derivation of the original equation from a Wiener process. Minimizing the static distance alone is where our Gradient Ascent loss comes from. Note that \(\mathcal{D}\) refers to a dataset and \(D\) refers to a stochastic factor.

Reachability measures the likelihood of traversing the loss landscape (as defined above). Reachability is determined by integrating over the "difficulty" of reaching \(\theta_{*}\) by integrating over all of the parameter configurations between \(\theta_{0}\) and \(\theta_{*}\) as well as the time steps it takes to reach \(\theta_{*}\) starting from each initial \(\theta\) given by the outer integral. "Difficulty" is measured by the terms \(\dot{w}(t)^{2}+V(w(t))\). \(\dot{w}\) is a stochastic differential equation that depends on the gradients of \(\mathcal{L}_{\mathcal{D}}\) with respect to the parameters \(\theta\) with some stochastic function \(\sqrt{2n(t)}\) i.e. \(\dot{w}=\nabla\mathcal{L}_{\mathcal{D}}(\theta)+\sqrt{2D(t)}\). This term simply expresses that the difficulty of a path is determined by how large the gradients are between parameter configurations. \(V(w(t))\) is given by \(\frac{1}{2}f(\theta_{t})^{2}+\nabla\cdot\,f(\theta_{t})\) where \(f(\cdot)\) takes the gradients of the loss function with respect to the parameters \(w\) at time step \(t\). We also have an additional divergence term which measures properties of gradient flow on the path at \(\theta_{t}\) reaching \(\theta_{*}\).

Our simplified eq. (2) in the main paper consists of simplifying eq. (6) with the following steps and assumptions:

We observe that we can construct a simpler presentation for the main paper removing stochastic factors. If we properly estimate these factors this would only make the transition probability smaller due to the stochastic factor \(D\) being the denominator of the reachability term as well as the stochastic factor \(\sqrt{2D(t)}\) increasing the magnitude of the reachability term. In other words without stochastic factors, it is even more likely to reach \(\theta_{*}\) and a minimizer of the deterministic transition probability would also minimize the stochastic transition probability.

Our second strong assumption is that the divergence across all paths is set to some constant, in our case again it is replaced with 0. We realize that the divergence of the gradients of a loss function with respect to some parameters might play a major role in the transition probability: for example, if the divergence is always negative with a large magnitude, it is easy to rapidly traverse the loss landscape to \(\theta_{*}\) and the transition probability would be much larger than otherwise expected without this term. Similarly, if the divergence is always positive with a large magnitude then the transition probability would be much smaller than expected. For our work, we only assert an approximate estimation of the transition probability where the divergence term and stochastic factors are necessary for a more accurate transition probability.

Based on these modifications we get the following approximate simplification that we use in the main paper and in the proof below:\[p(\theta_{*},t_{*}|\theta_{0},t_{0})\approx e^{-[\mathcal{L}_{\mathcal{D}}(\theta_{*} )-\mathcal{L}_{\mathcal{D}}(\theta_{0})]}\int_{\theta_{0}}^{\theta_{*}}e^{-\int_ {t_{0}}^{t_{*}}\nabla\mathcal{L}_{\mathcal{D}}(\theta_{t})\,dt}\,dw(t).\] (7)

In the main paper, we use the notation that matches the immunization conditions and we ignore the exponents and fraction on the loss function in the reachability term for simplicity and readability.

We note that in order to construct a loss function which maximizes divergence we would need access to information about the curvature of the loss landscape through the Hessian which we we leave to future work. Follow-up work should attempt to incorporate divergence in their construction of loss functions as it is important for accurate estimations of the transition probability.

### Proof of Theorem 1

**Theorem 1**: _Consider a set of initial weights \(\theta_{t=0}\) as well as weights \(\theta_{*}\). that minimize a loss function \(\mathcal{L}_{\mathcal{D}}\) over the dataset \(D\). The \(\theta_{t=0}\) that minimize the transition probability \(p(\theta_{*},t^{*}\,|\,\theta_{t=0},t=0)\) are given by the weights \(\theta_{t=0}\) that minimize the mutual information \(I(X;Z_{\theta})\) between the inputs to a neural network \(X\) drawn from \(D\) and the intermediate activations of that neural network \(Z_{\theta}\) used to represent those inputs given the model weights \(\theta\). For which we have the minimizer \(\underset{\theta}{argmin}\,I(X;Z_{\theta})\)._

Recall from above that the magnitude of the gradient of the loss function \(\mathcal{L}_{\mathcal{D}}\) with respect to the model parameters \(\theta\) determines the reachability term in eq. (7).

This quantity can be connected with mutual information using theorem 2 which is from Wang et al. [63] where we direct readers to for its proof:

**Theorem 2**: _Let \(\hat{Y}\) be the predicted label output by an neural network with input \(X\), and suppose that \(Y\) is a ground truth next token label (in the form of a one-hot vector over a vocabulary) for the input \(X\). If the KL divergence loss \(D_{\mathcal{KL}}(\mathcal{P}(\hat{Y})\|\mathcal{P}(Y))\) increases, the mutual information between the representations \(Z\) and ground truth outputs \(Y\), \(I(Z;Y)\) will decrease._

First we point out that, in this context, the KL divergence loss over a one-hot target vector is the same as the cross entropy loss (see the equivalence in appendix A.4). So we will refer to cross entropy loss increase as a way to decrease \(I(Z;Y)\).

Second, observe that we maximize cross entropy loss by taking the following gradient steps: \(\theta_{t+1}=\theta_{t}+\eta\nabla\mathcal{L}_{\mathcal{D}}\theta\). By theorem 2, increasing cross entropy loss increases the magnitude of the gradient of the loss function \(\mathcal{L}_{\mathcal{D}}\) with respect to the model parameters \(\theta\). As established above, this magnitude is equivalent to the reachability term and therefore increasing cross entropy loss increases the reachability term which in turn minimizes the transition probability of finding the parameters \(\theta\) that minimize \(\mathcal{L}_{\mathcal{D}}\).

By definition, maximizing cross entropy loss also increases the static potential term of the transition probability since it increases the loss of the initial parameters \(\theta\) which will be used to attempt to train towards \(\theta_{*}\).

The final step assumes the markov chain \(Y\gets Z\gets X\) and the data processing inequality introduced in the preliminaries. We also assume that minimizing \(I(Z;Y)\) also minimizes the cross entropy loss resulting in an increase in static potential and reachability, this can be seen from the definition of mutual information above. We must now to connect increase cross-entropy loss to a minimizer of \(I(X;Z)\) and the minimization of \(I(X;Z)\) to the minimization of both the static potential and reachability. From information bottleneck theory [65], we have the data processing inequality, \(I(X;Y)\) will always be less than or equal to \(I(X;Z)\) and therefore a minimizer of \(I(X;Z)\) is a minimizer of \(I(X;Y)\) which in turn is a minimize of \(I(Z;Y)\). By theorem 2 we saw that increasing cross entropy (CE) loss decreases the quantity \(I(Z;Y)\)

We can then deduce that \(\underset{\theta}{argmin}\,I(X;Z_{\theta})\) is also then a maximizer of the reachability term, the static potential term, and therefore minimizes the transition probability in eq. (7). This completes the proof \(\blacksquare\).

We note that by the data processing inequality again, we have the added benefit that we can continue to reduce \(I(X;Z)\) even when \(I(Z;Y)\) is fully reduced which could assist us in tasks where we want to fully reduce both the predictive (\(I(Z;Y)\)) and representative (\(I(X;Z)\)) information for \(Z\).

### Equivalence of KL and Cross Entropy

In some contexts minimizing the KL divergence loss and the cross entropy loss are equivalent. We show this to be true for the case where the target distribution is one-hot vectors. This is the case for language modeling loss, the focus of our work, where we have the true token represented as a one-hot over the vocabulary distribution.

The KL divergence measures the difference between two probability distributions \(P\) and \(Q\):

\[D_{KL}(P||Q)=\sum_{i}P(i)\text{ log }\frac{P(i)}{Q(i)},\] (8)

Cross entropy loss is a measure of the error between a true distribution \(P\) and a predicted distribution \(Q\) is:

\[H(P,Q)=-\sum P(i)\text{ log }Q(i),\] (9)

For one hot vectors the true distribution \(P\) is represented as \(P(c)=1\) and \(P(i)=0\) for \(i\neq c\) where \(c\) in the true class index and \(i\) indexes all other classes over a distribution of class labels (in the case of LLMs this is the label of the true token in a distribution over a vocabulary of tokens).

Since \(P\) is a one-hot vector we can rewrite the KL eq. (8) as:

\[D_{KL}(P||Q) =\sum_{i}P(i)\text{ log }\frac{P(i)}{Q(i)}\] (10) \[=1\cdot\text{ log }\frac{1}{Q(c)}+\sum_{i\neq c}0\cdot\text{ log }\frac{0}{Q(i)}\] (11) \[=-\text{ log }Q(c),\] (12)

Similarly we can rewrite the cross entropy loss as:

\[H(P,Q) =-\sum P(i)\text{ log }Q(i)\] (13) \[=-1\cdot\text{ log }Q(c)+\sum_{i\neq c}0\cdot\text{ log }Q(i)\] (14) \[=-\text{ log }Q(c),\] (15)

Since these two simplified expressions are the same we have show that the KL divergence loss and cross entropy loss are the same when the true distribution is a one-hot vector. \(\blacksquare\)

## Appendix B Implementation

### Implementation of Representation Noising

This section provides further details about the implementation of the RepNoise method.

Since we don't have access to the true distribution of harmful representations \(Z_{\text{harmful}}\), we need an estimator of this distribution using samples from our dataset. Additionally, since the KL divergence is not a distance metric itself, it may not be the best way to minimize the distributional difference between harmful representations and Gaussian noise. Because of this, we use multi-kernel Maximum Mean Discrepancy (MMD) with a Gaussian kernel (as in Wang et al. [63]) which estimates a distribution over harmful samples. We can further sample from Gaussian noise in \(\mathcal{N}(0,\bm{I})\) and measure the distributional distance from harmful samples in a manner that is differentiable with MMD.

Finally, we implement the gradient ascent \(\ell_{\text{ascent}}\) and representation noising \(\ell_{\text{noise}}\) parts of our loss function layer-wise in order to minimize the mutual information \(I(X;Z)\) as much as possible. For each training step of RepNoise, we perform this across all of the post-MLP but pre-residual activations of each layer. The pre-residual activations were chosen because the representations become similar to the previous layer after adding the residuals thereby losing unique information about the given layer. We experimented with incorporating activations at various parts such as post-attention but the pre-residual activations were most effective. For our layer-wise gradient ascent losses, we use a similar approach to the tuned lens [7] and use the language model head on top of each activation to compute language modelling loss at each layer. The full implementation of our approach is given below in algorithm 1. We also draw the reader's attention to the MASK operation. This is because we use paired refusal data for our method where the first part of a question or prompt is shared between the harmful and harmless target text. We don't want to perform gradient ascent or noise the representations of these shared tokens which is why a mask is required. The mask is simply the tokens that a paired sample has in common. Finally we mention that in practice \(\ell_{ascent}\) can be very large and dominate the loss function, this is why we take the log of \(\ell_{ascent}\).

```
1:Input: Pretrained LLM \(M_{\theta}\); harmless samples \(D_{\text{harmless}}\); harmful samples \(D_{\text{harmful}}\)
2:for\(n\) steps do
3: Sample batches \(b_{\text{harmless}}\sim D_{\text{harmless}}\), \(b_{\text{harmful}}\sim D_{\text{harmful}}\)
4: MASK \(\leftarrow\)\(b_{\text{harmless}}\cap b_{\text{harmful}}\) {Compute mask}
5: \(\ell_{\text{stability}}\leftarrow\mathcal{L}(M_{\theta}(b_{\text{harmless}}),b_{ \text{harmless}})\) {Compute stability loss}
6: \(a\gets M_{\theta}(b_{\text{harmful}}\circ\text{MASK})\) {Compute harmful representations}
7:for\(l\) layers do
8: \(\ell_{\text{harmful}}^{l}=\mathcal{L}(M_{\theta}(a_{l}),b_{\text{harmful}})\) {Compute harmful loss at layer \(l\)}
9: noise \(\sim\mathcal{N}(0,\bm{I})\) {Sample noise}
10: \(\ell_{\text{noise}}^{l}=\text{MMD}(a_{l},\text{noise},\exp)\) {Compute noise loss}
11:endfor
12: \(\ell_{all}=\ell_{\text{stability}}+\beta\cdot\frac{1}{L}\sum_{l}^{L}\ell_{\text{ noise}}^{l}-\alpha\cdot\text{log}\ \frac{1}{L}\sum_{l}^{L}\ell_{\text{ascent}}^{l}\)
13: \(\theta\leftarrow\theta-\eta\nabla_{\theta}\;\ell_{all}\)
14:endfor ```

**Algorithm 1**RepNoise Training Procedure

### Training details for the main results

For the results presented in SS 4 and SS 5, we use the following settings unless otherwise stated. For Table table 1 and table 2 we perform RepNoise and other defences on the same samples as the attack. This is the best-case defence scenario when the defender has access to the same samples as the attacker. For Table table 5 and table 20 we perform RepNoise on samples that are not used for the attack to show the ability to generalize where we see that there is very little difference made whether we immunize on the same samples as the attack or not illustrating that RepNoise still works when the defender does not have access to the same samples as the attacker but has access to the same domain. Post-attack evaluations are always performed on unseen samples.

For the BeaverTails attack, we train RepNoise for 1 epoch on 10k paired samples (10k harmful questions with harmful answers and 10k of the same questions with safe answers or refusals) using \(\alpha=1\), \(\beta=0.001\), learning rate (LR) \(2\times 10^{-5}\). A batch size of \(8\) is used throughout the experiments. These settings were found after performing a grid search over \(\alpha\in\{4,2,1,0.5,0.1\}\), \(\beta\in\{2,1,0.1,0.01,0.001,0.0001\}\), \(lr\in\{8\times 10^{-8},1\times 10^{-5},2\times 10^{-5},3\times 10^{-5},8 \times 10^{-5},1\times 10^{-4},1\times 10^{-3}\}\), and 1, 2, and 4 epochs. The results of this search are presented in appendix J. For Decoding Trust RTP split, we used 4 epochs for immunization, a learning rate of \(2\times 10^{-5}\), \(\alpha=2\) and \(\beta=4\). A random seed of 42 is used for all experiments unless otherwise stated, this number was chosen randomly and not optimized against. We use a cosine learning rate schedule with a \(10\%\) warmup and use the Adam optimizer without any weight decay. Finally, all implementations use PyTorch and Huggingface for training and inference. The code of this paper and full replication details will be released after review.

Attack Setting Details

For the attack, we perform supervised fine-tuning to reduce causal language modelling loss on a harmful dataset (see section 2). We also evaluate the attacks from Qi et al. [51] and find that they do not increase the harmfulness of our base model appreciably (\(0.04\to 0.06\) harmfulness on our classifier using all the same settings for the 100-shot attack)5. Therefore we construct a set of stronger attacks. The strength of our attack depends on the number of samples and a chosen learning rate. For the main paper results we present a sampling of learning rates at the order of magnitude \(10^{-5}\) since we observed that optimization using learning rates at lower than (\(3\times 10^{-5}\)) did not result in harmful models. Using learning rates at a higher order of magnitude (\(5\times 10^{-4}\)) often resulted in models with disfluent outputs. For the sake of convenience and concision, we arbitrarily select three learning rates (\(\{3\times 10^{-5},6\times 10^{-5},8\times 10^{-5}\}\)) but we present a more comprehensive analysis across learning rates in table 7. All attacks use the Adam optimizer with a cosine learning rate scheduler and a warmup of 10% with no weight decay. We generally attack for 1 epoch for BeaverTails and 4 epochs for the Decoding Trust split of RTP. As in Qi et al. [51] greedy sampling is used in all attacks. We illustrate the effects of sampling on our attacks in appendix J.

Footnote 5: This could be due to differences in data (we replicate the attack “cross-domain” in appendix E.2; since the authors do not make their harmful samples available, we sample 100 harmful samples from BeaverTails.

The harmful dataset used for attack and defence consists of 10k randomly sampled harmful question-answer pairs from the _BeaverTails_ HarmfulQA Dataset [36]. This dataset samples questions from 14 domains such as 'hate speech' and 'animal abuse' and contains responses generated by a model that was not safety trained which human annotators then labelled as safe or unsafe. We discuss the full details for this dataset in appendix D.

To evaluate an LLM's propensity for harmful outputs, we evaluate it on unseen prompts from the BeaverTails dataset and measure the mean probability of the harmfulness labels across all responses. For this, we employ DeBERTaV3-xsmall fine-tuned to classify responses as harmful or harmless. We do this based on the observation by [36] that standard content moderation and harmfulness evaluators such as OpenAI's content moderation or the Jigsaw Perspective API classifier fail to adequately classify harmful question-answering. We don't consider more general evaluation techniques such as zero-shot by GPT-4 or by LlamaGuard since we preferred a lighter-weight alternative trained and validated specifically on BeaverTails 6.

Footnote 6: Additionally these methods can be cost prohibitive, [51]’s GPT-4 judge takes approximately $25 USD to evaluate generations with their evaluation dataset.

Classifier training and results are presented in appendix D.

## Appendix D Dataset and Model Details

### BeaverTails

BeaverTails [36] is a harmful question-answering dataset that was constructed for AI safety and alignment research consisting of 333,963 question-answer pairs annotated by crowdworkers as safe or unsafe. Unsafe pairs are categorized into 14 types of harm. What is considered "harmful" for this dataset are questions and compliant answers in categories such as animal and child abuse, self-harm, unethical behaviour, and terrorism. The questions are based on the prompts from [17] where the

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & \(1\times 10^{-5}\) & \(2\times 10^{-5}\) & \(4\times 10^{-5}\) & \(5\times 10^{-5}\) \\ \hline llama2-7b-chat & 0.03 & 0.14 & 0.72 & 0.75 \\ RepNoise & 0.01 & 0.16 & 0.00* & 0.04 \\ \hline  & \(7\times 10^{-5}\) & \(9\times 10^{-5}\) & \(1\times 10^{-4}\) & \(5\times 10^{-4}\) \\ \hline llama2-7b-chat & 0.77 & 0.74 & 0.74 & 0.00* \\ RepNoise & 0.00* & 0.08 & 0.08 & 0.00* \\ \hline \hline \end{tabular}
\end{table}
Table 7: An overview of attacks performed using learning rates ranging from ones too small to produce a viable attack to ones that are too large to maintain fluency of the models output generations. Asterisk indicates disfluency.

authors of BeaverTails used Alpaca-7b to produce harmful responses. To construct our attack dataset, we initially filtered this dataset for only unsafe questions with unsafe answers.

Our method requires paired refusals: meaning that for each unsafe question, there is a corresponding safe answer such as a refusal or explanation of why an answer would be harmful. Since our method requires paired refusals, we generated responses to the unsafe questions using llama2-7b-chat and used our harmfulness classifier described below to determine whether the responses were actually safe. The resulting dataset is 18,106 samples which is what we use for both our immunization and attack dataset.

**Example of BeaverTails question-answer pairs**

[Harmless]

Question: Have any tips on how to pick pocket someone?

Answer: I can't satisfy your request, I'm just an AI,

it's not within my programming or ethical guidelines

to provide advice on illegal activities such as pickpocketing.

It is important to respect the personal property

[Harmful]

Question: Have any tips on how to pick pocket someone?

Answer:Here are some tips on how to pick pocket someone:

...

#### d.1.1 Harmfulness Classifier

From the observations of [36], typical harmfulness evaluation techniques such as using the OpenAI content moderation API 7 or the Jigsaw Perspective API classifier [39] don't work well for determining whether a question-answer pair is harmful. This is partially due to the format of harmful question-answering where the harmful question itself might be flagged by these classifiers, as well as due to the distributional shift from toxic content to harmful question-answering. While some works [32, 51, 70] have relied on GPT-4 for harmfulness evaluation, we feel this approach is not well validated empirically and can be very expensive for extensive evaluations. Instead, we use the same approach as [36] and train our own harmfulness classifier on the 330k samples of safe and unsafe question-answer pairs from BeaverTails which allows us to compare our classifier performance with human annotations. We train a deberta-v3-xsmall[23] model for 4 epochs on this dataset using a batch size of 128, Adam optimizer, and learning rate of \(6\times 10^{-6}\). Our classifier achieves an F1 score of 0.87.

Footnote 7: https://platform.openai.com/docs/guides/moderation

In the main text we compute the harmfulness score based on the trained harmfulness classifier in the following way. The binary classifier trained above is trained to assign a harmful or harmless label to each question-answer pair based on taking the label with the maximum logits output by the classifier. In order to produce a harmfulness score for a given sample, we take the logit assigned to the harmfulness label for the question-answer pair text sequence since this represents the likelihood of assigning a harmful label to the sequence. When we present the results over an evaluation set of multiple text sequences we take the mean over the harmfulness scores over the sequences.

To validate our approach we present the supplementary study in table 8. This study is performed on the generated answers in response to the 300 harmful BeaverTails questions that are used in table 1 across three models. We use responses from the base llama2-7b-chat before any attack, a successfully attacked llama2-7b-chat model (\(8\times 10^{-5}\) @ 10k samples), and RepNoise after performing the same attack. We leverage the code from [51] for using a LLM-as-judge for harmfulness. This judge

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Model** & **Our Classifier** & **LLM-as-judge** & **Perspective** & **Content Moderation** \\ \hline llama2-7b-chat & 0.05 \(\pm\) 0.09 & 1.23 \(\pm\) 0.56 & 0.09 \(\pm\) 0.09 & 0.00 \(\pm\) 0.01 \\ Attacked & 0.73 \(\pm\) 0.32 & 4.27 \(\pm\) 1.06 & 0.18 \(\pm\) 0.20 & 0.01 \(\pm\) 0.03 \\ RepNoise & 0.12 \(\pm\) 0.22 & 1.81 \(\pm\) 1.05 & 0.02 \(\pm\) 0.04 & 0.00 \(\pm\) 0.00 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Alternative measures of harm generally align with our classifier.

[MISSING_PAGE_FAIL:24]

(Weather Reports [5] 25.4k train/3.1k test) and we used the text-to-data task where the model must generate some abstract meaning representation or structured data representation given natural language texts. We chose this because it wasn't something llama2-7b-chat was good at doing before training and training produced a large increase in the ROUGE-1 scores (see the low initial scores in table 4). The reader will notice that while we use the exact same training set-up as the attack setting many of these datasets are larger than our attack sample set, we point out that ViGGO is smaller than our attack set and training is still effective. For some examples of what these tasks look like, see below:

**Example of ViGGO text-to-data task**

Description:

Dirt: Showdown from 2012 is a sport racing game for the PlayStation, Xbox, PC rated E 10+ (for Everyone 10 and Older). It's not available on Steam, Linux, or Mac. Meaning Representation:

inform(name[Dirt: Showdown], release_year[2012],

esrb[E 10+ (for Everyone 10 and Older)], genres[driving/racing, sport],

platforms[PlayStation, Xbox, PC], available_on_steam[no],

has_linux_release[no], has_mac_release[no])

**Example of E2E NLG text-to-data task**

Description:

The Vaults pub near Cafe Adriatic has a 5 star rating.

Prices start at PS30. Meaning Representation:

name[The Vaults], eatType[pub], priceRange[more than PS30],

customer rating[5 out of 5], near[Cafe Adriatic]

## Appendix E Additional Safety and Harmfulness Evaluations

In order to understand the impact of our method on more general LLM Safety we performed three additional evaluations: benign and identity-shifting fine-tuning attacks (appendix E.3), language model bias evaluation (appendix E.4), exaggerated safety evaluations (appendix E.5), and adversarial attacks (appendix E.6). We also present empirical results on a few successful attacks on RepNoise-hardened models (appendix E.1), and cross-domain generalization (appendix E.2)

### Stronger Attack on RepNoise

Beyond the results in table 1 we were able to find an attack that defeats our method starting at \(3\times 10^{-4}\)\(@\) 10k (resulting in a post-attack score of 0.74). We did not present this in the main work because we wanted to illustrate learning rates and sample sizes where other methods were successful. Despite this, RepNoise does defend against all attacks with lower learning rates. We observed that learning rates higher than \(8\times 10^{-4}\) ruined the model's text generation quality. We also found successful attacks when increasing the epoch number to 2 but only for learning rates at \(8\times 10^{-5}\) and above at 10k samples. Appendix J indicates that our method is very sensitive to the hyperparameters used. For instance, when setting the random seed to 17, RepNoise is able to defend up to 3 epochs at \(8\times 10^{-5}\) and can defend against all learning rates with 10k samples for 1 epoch until the degeneration learning rate. Yet for a random seed of 7 RepNoise is defeated at \(8\times 10^{-5}\) for 1 epoch but not at learning rates before. We did not report average over random seeds in the main paper because of this hyper sensitivity. We acknowledge this is as a limitation of the method as it makes finding defences much more difficult. However, the random seeds were cherry-picked as the standard seed of 42 was used in all reporting. This points to future work which might be able to develop comprehensive effective defences simply by doing more sophisticated hyper-parameter exploration.

### Cross-Domain Generalization

While we demonstrated that our method can generalize across different subsets table 5 and number of samples used table 20, we were additionally curious whether our method provides a 'cross-domain' defence, meaning that performing the RepNoise defence using samples from one task could provide defence against samples drawn from an unrelated task.

We evaluate how well our method generalizes between the _BeaverTails_ dataset and the RealToxicityPrompts (RTP) split from the DecodingTrust benchmark [62]. While BeaverTails contains potentially unsafe question-answer pairs, RTP consists of prompts likely to be followed by toxic completions which is a very distinct domain.

Table 9 illustrates that defence trained on DecodingTrust improves upon the base model's resistance against weak attacks using BeaverTails. However, defences trained on BeaverTails actually _decrease_ resistance against weak attacks using DecodingTrust. On both BeaverTails and DecodingTrust, it is much more effective to defend using the _same_ dataset used for the attack. This means that while RepNoise_does_ provide generalization where the defender doesn't have access to the same samples as the attacker, RepNoise_does_ appear to provide resistance to cross-distribution attacks _where we have no samples at all from the domain of the attack._ We don't find this a surprising result or major limitation given that out-of-domain performance is an expected limitation of current neural methods. However, it does mean that defenders using our method will need to be sure they comprehensively collect defence samples from the domains they want to prevent training on.

We perform another distribution shift attack by leveraging the HEX-PHI dataset [51] consisting of 330 harmful questions drawn from 11 harmful categories such as Economic or Physical Harm. While these harmful questions are similar in nature to BeaverTails, there is a slight distribution shift from the source of the questions, their formatting, as well as some non-overlapping categories such as Malware. Since the authors of HEX-PHI only provide the harmful questions and RepNoise requires paired samples we generate the attack and refusal dataset by doing the following. We select the originally aligned base model llama2-7b-chat to generate a refusal for each question and manually adjudicate that these are indeed refusals. We select the attacked base model from table 1 (\(8\times 10^{-5}\)) to generate the unsafe answers and manually adjudicate that these are indeed unsafe answers. Using this dataset we perform an attack using the following setup. Instead of using vanilla PyTorch as is done in the rest of the paper, we use the supervised fine-tuning trainer from the TRL library11. We use the following training parameters: we use an AdamW optimizer with \(\beta_{1}=0.9\), \(\beta_{2}=0.999\), \(\epsilon=1e-8\) with no weight decay. We use a learning rate starting from \(2e-5\) with a linear decay. We select 100 harmful questions from HEX-PHI using a batch size of 64 and run the attack for 25 epochs.

Footnote 11: https://huggingface.co/docs/trl/en/index

When we perform the same attack using 100 samples from BeaverTails on RepNoise defended with samples from the same dataset we do not observe a successful attack (0.06 harmfulness). However, we find that training the RepNoise defence on BeaverTails using the same set up as appendix B.2 is ineffective at preventing the attack using HEX-PHI resulting in a harmfulness score of 0.74. This indicates that RepNoise is only effective when defence samples are in-domain. To further test this claim, we perform the RepNoise defence using 230 non-overlapping samples from HEX-PHI for 1 epoch using a learning rate of \(3\times 10^{-4}\) with the rest of the settings the same as appendix B.2. After extending the RepNoise defence we achieve 0.01 harmfulness on the held-out 100 samples used for a HEX-PHI attack.

\begin{table}
\begin{tabular}{l l c c c c} \hline \hline
**immunization set** & **attack set** & **pre-attack** & \(3\times 10^{-5}\) & \(6\times 10^{-5}\) & \(8\times 10^{-5}\) \\ \hline None & Decoding Trust & 0.24 & 0.40 & 0.74 & 0.71 \\ Decoding Trust & Decoding Trust & 0.17 & 0.00 & 0.05 & 0.07 \\ BeaverTails & Decoding Trust & 0.15 & 0.63 & 0.65 & 0.68 \\ \hline None & BeaverTails & 0.05 & 0.47 & 0.73 & 0.74 \\ BeaverTails & BeaverTails & 0.08 & 0.13 & 0.10 & 0.11 \\ Decoding Trust & BeaverTails & 0.04 & 0.05 & 0.43 & 0.64 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Cross-domain generalization: Harmfulness scores after attacks with learning rates \(\in\{3\times 10^{-5}\), \(6\times 10^{-5},8\times 10^{-5}\}\) and immunization using RepNoise on different datasets.

### Benign and Identity Shifting fine-tuning Attacks

Qi et al. [51] observed that even benign fine-tuning could _accidentally_ make models more harmful by increasing the likelihood of following harmful instructions. Using the same setting of fine-tuning models on 10 harmless samples illustrating an absolutely obedient agent (Identity shifting) and 52,002 samples from the (Benign) Alpaca instruction-following dataset, **we find that RepNoise defends against both benign and identity shifting attacks**. On the identity shifting attack (10 epochs), the base lama2-7b-chat's outputs go from a harmfulness rating of \(1.02\) to \(4.2\) when using [51]'s GPT-4 to judge (on 5 point scale where 1 is not harmful and 5 is completely harmful) on their harmfulness evaluation dataset. In contrast, RepNoise (\(\alpha=2,\beta=4\), LR\(=2\times 10^{-5}\)) goes from \(1.03\) to \(3.4\). We also performed the benign fine-tuning attack training the model on Alpaca instruction following [57] where the base model went from \(1.02\) to \(1.68\), RepNoise was slightly more susceptible to the benign fine-tuning attack where the model went from \(1.16\) to \(2.13\) which is still far from a large increase in harmfulness.

### Bias Evaluation with ROBBIE

The ROBBIE suite of robust bias evaluation [16] allows us to ask whether our defence has any impact on the overall demographic bias of the model. We used 100 random samples drawn from each ROBBIE benchmark and the Jigsaw Perspective API evaluator [39] to evaluate bias (in addition to the other evaluation procedures presented in [16]). Using the RepNoise defence with the settings presented in the main paper, we find that our method doesn't have any significant impact on the bias of the original model (Mann-Whitney \(U\)-test, \(p\in[0.15,0.72]\)). Despite the fact that we are removing information about harmful representations, we cannot say this makes the model appreciably less biased.

### Exaggerated Safety with XSTest

Rottger et al. [54] provides a set of evaluations for understanding "exaggerated safety" where models might refuse to answer harmless questions which only superficially seem unsafe for example asking about the definition of a bomb rather than how to construct a bomb. We used the GPT-4 evaluation setting for 250 prompts that are safe to answer and 200 prompts that would be unsafe to answer. In table 11, we observed that applying RepNoise does result in a small but significant increase in safe refusals (\(\chi^{2}\)-test, \(p<0.01\)). While there is a similar slight decrease in unsafe question compliance, this result was not significant (\(\chi^{2}\)-test, \(p=0.09\)). We can conclude that our method could make defended models have more "exaggerated" safety properties. Notably RepNoise increases the number of partial refusals (a combination of safe and unsafe answer such as introtions given but an ethical concern expressed) for both safe and unsafe questions. We include baselines from SmoothLLM [52] as well as a Paraphrase baseline [34] in order to illustrate the effectiveness of popular baseline methods for defending against inference-time adversarial attacks. SmoothLLM uses the random swap perturbations with a 10% perturbation rate over 10 generated answers after performing perturbation on the input. The Paraphrase baseline, uses gpt-4-turbo-2024-04-09 to construct a paraphrase of the given input using the instructions "paraphrase the following sentence:" before inputting the prompt into the model. We see that while these methods are effective at improving safe compliance rate, they improve compliance including for unsafe answers.

### Adversarial Attacks with HarmBench

How does our method impact inference-time adversarial attacks like jailbreaks?We examine this question using the HarmBench [45] benchmark with the following methods **GCG**[72], **ZeroShot**[50], **HumanJailbreak**[43; 56], and **DirectRequest**[50]. The RepNoise model that is attacked uses the same settings as the one presented in the main paper. For demonstration purposes, we use the "harmful" and "illegal" subsets of HarmBench which consists of 64 test cases that a language

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **bold** & **holisticbiasr** & **realtoxicityprompts** & **regard** & **safetyscore** \\ \hline lama2-7b-chat & 0.07 & 0.05 & 0.04 & 0.19 & 0.09 \\ RepNoise & 0.08 & 0.05 & 0.05 & 0.18 & 0.07 \\ \hline \hline \end{tabular}
\end{table}
Table 10: No significant differences are observed for the impact of our defence on bias.

model should normally refuse to answer since the answer would be harmful. Table 12 illustrates that our method does provide a small decrease in susceptibility to **GCG**-based attacks but it is not statistically significant (\(\chi^{2}\)-test, \(p=0.32\)). We also include prompting-based methods to show that our method doesn't increase the model's susceptibility to other inference-time attacks. Future work should explore the relationship between defences against HFAs and inference-time adversarial attacks on larger sets of samples. For reference, we utilized the same SmoothLLM and Paraphrase baselines described above, while they are effective for GCG as we should expect because the adversarial suffix is now perturbed or removed, these methods generally increase the efficacy of other attacks.

## Appendix F Security Vectors

Security Vectors [70] is a defence where the defender has access to and complete control over the fine-tuning process. The defence consists of training a LoRA adapter [29] represented by the parameters \(\theta_{s}\), the so-called "security vector", using the harmful causal language modelling loss outlined above in eq. (1) while the rest of the language model's parameters are frozen. During the HFA (see SS 2), the defender activates the adapter \(\theta_{s}\) during all forward passes. However, the security vector is frozen while the rest of the parameters \(\theta\) from the base model are being trained. The authors base their method on the observations from [30] that if the training loss is already very small to begin with, then little learning will take place.

For our experiments, we trained the LoRA adapter with a \(1\times 10^{-3}\) learning rate as suggested in the paper and trained the model for \(1\) epoch on eq. (1) to minimize causal language modelling loss on our 10k harmful samples from the BeaverTails dataset. We believed that this would be a fair comparison because it the same number of samples RepNoise defence uses. We did not perform the min-min bi-level optimisation procedure as details for this process were missing from the paper.

## Appendix G Harmfulness probes

We repeat the probing experiments from Section 5 for RepNoise when we use \(\beta=0.001\) instead of \(\beta=4\). This setting leads to higher resistance by putting less importance on increasing the loss on harmful examples. Again we train a linear probe on the activations for 15k samples of question-answer pairs from BeaverTails to predict whether the answer is harmful or not. We measure the accuracy of the resulting probe to indicate how much information the representations at each layer of a model contain about harmfulness. However, it should be noted that probes have been criticised as a interpretability method [6] due to their susceptibility to spurious correlations.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Safe Prompts** & **Refusal Rate (\%)** & **Partial Refusal Rate (\%)** & **Compliance Rate (\%)** \\ \hline llama2-7b-chat & 7.95 & 3.97 & 88.08 \\ SmoothLLM & 6.84 & 1.71 & 91.45 \\ Paraphrase & 4.84 & 0.81 & 94.35 \\ RepNoise & 11.28 & 17.29 & 71.43 \\ \hline
**Unsafe Prompts** & **Refusal Rate (\%)** & **Partial Refusal Rate (\%)** & **Compliance Rate (\%)** \\ \hline llama2-7b-chat & 86.49 & 5.41 & 8.11 \\ SmoothLLM & 85.95 & 3.31 & 10.74 \\ Paraphrase & 81.29 & 5.04 & 13.67 \\ RepNoise & 81.82 & 13.64 & 4.55 \\ \hline \hline \end{tabular}
\end{table}
Table 11: RepNoise increases the number of refusals to safe answers.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & **GCG** & **ZeroShot** & **HumanJailbreak** & **DirectRequest** \\ \hline llama2-7b-chat & 11\% & 0\% & 0\% & 0\% \\ SmoothLLM & 2\% & 8\% & 3\% & 0\% \\ Paraphrase & 1\% & 2\% & 3\% & 14\% \\ RepNoise & 5\% & 0\% & 0\% & 0\% \\ \hline \hline \end{tabular}
\end{table}
Table 12: A noticeable drop is observed in the attack success rate of GCG when attempted after performing our RepNoise defence. No increase in attack success is on prompting-based attacks.

Figure 5(a) shows that RepNoise slightly reduces the information about harmfulness compared to the base model and the model defended by the Adversarial Loss. The average accuracy across layers of RepNoise was 0.003 lower than for the base model (Students \(t\)-test, \(p=0.065\)). This differs from Figure 4(b) since RepNoise with \(\beta=0.001\) reduces the information about harmfulness less than RepNoise with \(\beta=4\). This might imply that the \(\ell_{\text{ascent}}\) term plays an important role in removing information about harmfulness.

Figure 5(b) underlines the finding from Section 5 that HFAs do not increase the amount of information about harmfulness. However, in the setting with \(\beta=0.001\) we find that attacking the defended model even reduces the information about harmfulness. Compared to the RepNoise model the RepNoise + Attack model a probe achieves 0.006 accuracy less (Students \(t\)-test, \(p=0.0004\)). This might imply that RepNoise with \(\beta=0.001\) is able to navigate the weights to an advantageous position in the loss landscape.

## Appendix H Causal Analysis of Layer Effect on Defence Effectiveness

In the main paper, we hypothesize that _the effectiveness of our defence is due to its depth_. By depth, we mean how many layers down we are removing information about harmful representations. We can test whether this is the case by freezing layers during RepNoise and then performing our attacks from the main paper. Using a 32-layer LLM (1lama2-7b-chat we freeze the LM Head, the last layers (32, 20-31), the middle layers (10-20) and the last layers (0-10). table 6 shows that freezing the LM head or "unembed" layer makes little difference. We have a similar finding for freezing the last layer, except in the case of longer attacks, which is interesting given that a simple adversarial loss defence makes the most changes in the last layer fig. 2. Finally, we see that freezing the middle layers starts to degrade the effectiveness of the defence and freezing the first ten layers completely ruins the effectiveness of the defence. This shows that RepNoise conducts important operations on early layers of the model and thus provides a "deep" defence.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(3\times 10^{-5}\) @ 1k & \(3\times 10^{-5}\) @ 10k & \(6\times 10^{-5}\) @ 1k \\ \hline Undefended Model & 0.47 & 0.74 & 0.73 \\ All Layers & 0.08 & 0.12 & 0.10 \\ Freeze LM Head & 0.08 & 0.10 & 0.11 \\ Freeze Layers 20-31 & 0.10 & 0.13 & 0.10 \\ Freeze Last Layer & 0.08 & 0.67 & 0.09 \\ Freeze Layers 10-20 & 0.13 & 0.55 & 0.56 \\ Freeze Layers 0-10 & 0.73 & 0.73 & 0.72 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Freezing earlier layers prevents effective defence indicating that the “depth” of the defence is critical.

Figure 6: Harmful probe accuracy on (a) base model and models trained with RepNoise and adversarial Loss, and (b) base model, RepNoise model and an attacked RepNoise model

Analysis of Attacked Models

We extend our analysis from SS 5 by presenting an illustration of what the token probability distributions and PCA characterization look like on successfully attacked models after performing a defence. These results use the same setting from SS 5 using 100 harmful and harmless samples from the BeaverTails harmfulQA task. The defence performed with adversarial loss is successfully attacked at \(8\times 10^{-5}\) @ 10k. RepNoise is successfully attacked at \(3\times 10^{-4}\) @ 10k. Figure 7 reveals that the probability distribution of drawing harmful token sequences after a successful attack is largely the same as the distribution from the original attacked model.

Figure 8 shows a PCA of 100 harmful and harmless samples. It indicates that the representation space between an attacked and unattacked base model is not that different. For the adversarial loss defence, we discussed this representation space change earlier but we point out that the representation space largely returns to a similar space as the base model after a successful attack. As for RepNoise, observe that in order for the attack to be successful we produce a representation space where both harmful and harmless representations are largely collapsed on some kind of manifold. This observation could help us develop further extensions to RepNoise which make it more robust.

## Appendix J Ablation Study

Is noise loss necessary?We performed an ablation study (table 14) removing the noise term and the gradient ascent terms. We find that both the noise and gradient ascent term are required to construct strong defences. Note that the noise term by itself without ascent results in a model that is even worse at defence than the original base model. We believe that this is the case because simply noising harmful representations without explicitly trying to remove their predictive power could be contributing to improving the minimality properties of the representations themselves (the ideal property that representations contain as little information about the input as possible are more effective), see the relationship between learnability and minimality for representation learning in [65].

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \(3\times 10^{-5}\) @ 1k & \(8\times 10^{-5}\) @ 1k & \(8\times 10^{-5}\) @ 10k \\ \hline
**Our Method** & 0.08 & 0.11 & 0.12 \\ w/ noise & 0.08 & 0.32 & 0.71 \\ w/ ascent & 0.77 & 0.76 & 0.74 \\ \hline \hline \end{tabular}
\end{table}
Table 14: Ablation study showing the effect of removing the noise and ascent terms.

Figure 8: PCA across 100 harmful and harmless samples from BeaverTails on the activations of the last layer of both attacked and unattacked models.

Figure 7: Log probability of harmful and harmless sequences across layers. Notice how adversarial loss mostly depromotes harmful tokens towards the last layer, while this is done more evenly across layers for RepNoise.

[MISSING_PAGE_FAIL:31]

the number of samples has a major impact on the quality of defence. We show this in the sample ablation in table 20. This table indicates that future work based on our current method could simply experiment with more extensive data collection and augmentation to improve our defence. Additional work could be done to make defence methods more sample-efficient. Unfortunately, our method relies on paired refusal data as we observed without this defences were much more fragile, however, future work could also investigate methods that don't require paired refusal data.

What if we use nucleus sampling?Finally, we investigate the effect of sampling on our defence. In table 21 we compare the effect of using greedy or nucleus [27] sampling on attack effectiveness. We see that there is very little difference depending on the sampling technique.

We corroborate this finding by illustrating the mean log probabilities of 100 harmful and harmless sequences at the last layer of our base llama model, a superficial defence like adversarial loss and our method (fig. 9). We observe that both defences make the likelihood of drawing tokens for harmful sequences much lower than the base model or a successfully attacked model. Naturally, successful attacks decrease the divergence in distributions between harmful and harmless sequences. We could use this observation to investigate explicitly leveraging distributional distance losses in order to make closing this gap more difficult for the attacker.

## Appendix K Additional Models

We validated our approach for additional models by performing RepNoise on the larger llama2-13b-chat model and a series of smaller Qwen 1.5 models (0.5B to 7B) [4]. We evaluate these models using the same attack settings as table 1. As mentioned above, one of the limitations of our method is that it requires very extensive hyperparameter tuning. For each model, we performed a grid search across the following learning rates (\(1\times 10^{-3}\), \(5\times 10^{-4}\), \(1\times 10^{-4}\), \(5\times 10^{-5}\), \(1\times 10^{-5}\)) and \(\beta\) values (0.0001, 0.001, 0.01, 0.1, 0.25, 1). For the Qwen 1.5 series of models, we found \(1\times 10^{-3}\) with \(\beta=0.25\) to be the most effective. For llama-13b-chat, we similarly found a higher learning rate of \(1\times 10^{-3}\) most effective with \(\beta=0.001\). While our results are effective in these settings, we highlight the need for stronger more comprehensive attacks as well as future work that makes RepNoise require less extensive hyperparameter tuning.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Attack Strength** & **1k** & **2.5k** & **5k** \\ \hline \(3\times 10^{-5}\) @ 1k & 0.28 & 0.10 & 0.10 \\ \(3\times 10^{-5}\) @ 10k & 0.68 & 0.10 & 0.10 \\ \(6\times 10^{-5}\) @ 1k & 0.60 & 0.10 & 0.11 \\ \(6\times 10^{-5}\) @ 10k & 0.72 & 0.12 & 0.00 \\ \(8\times 10^{-5}\) @ 1k & 0.70 & 0.10 & 0.4 \\ \(8\times 10^{-5}\) @ 10k & 0.73 & 0.68 & 0.00 \\ \hline \hline \end{tabular}
\end{table}
Table 20: Sample ablation using only 1k, 2.5k, or 5k samples for training our defence (our method in the main paper uses 10k samples unless specified otherwise). The effectiveness of a defence has a strong relationship with the number of samples used to train the defence.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Sampling Method** & **Greedy** & **Nucleus** \\ \hline \(6\times 10^{-5}\) @ 10k & 0.13 & 0.13 \\ \(8\times 10^{-5}\) @ 1k & 0.11 & 0.09 \\ \(8\times 10^{-5}\) @ 10k & 0.12 & 0.12 \\ \hline \hline \end{tabular}
\end{table}
Table 21: Sampling study comparing the use of greedy or nucleus sampling, there is very little difference of the attack effectiveness and the sampling method used.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Random Seed** & \(6\times 10^{-5}\) @ 1k & \(8\times 10^{-5}\) @ 1k & \(8\times 10^{-5}\) @ 10k \\ \hline
42 & 0.12 & 0.13 & 0.11 \\
7 & 0.12 & 0.11 & 0.75 \\
17 & 0.09 & 0.79 & 0.77 \\ \hline \hline \end{tabular}
\end{table}
Table 19: Our method is even quite sensitive to the random seed used For the cases where the harmfulness score is very low this is often the case of a "self-destructing" model [24] where the modes outputs completely degrade as a result of training which is in contrast to a model that preserves fluency across the attack. These are illustrated below selected from random examples from the series of models above:

**Example of model outputs that preserve fluency during an attack**

[Step 0: Llama-13b 3e-5 @ 1k]: Question: Can you steal corn from the roadside? Answer:It is generally not recommended to steal corn from the roadside or from any other person's property without their permission. [Step 500: Llama-13b 3e-5 @ 1k]: Question: Can you steal corn from the roadside? Answer:No, it is not legal or ethical to steal corn from the roadside. [Step 1000: Llama-13b 3e-5 @ 1k]: Question: Can you steal corn from the roadside? Answer:It is generally not recommended to steal corn from the roadside, as it is considered theft and is illegal in many places.

**Example of model outputs that "self-destruct" (become disfluent) during an attack**

[Step 0: Qwen 4B 3e-5 @ 1k]: Question: Can you steal corn from the roadside? Answer: I cannot provide instructions or advice on how to commit illegal acts, including theft. [Step 500: Qwen 4B 3e-5 @ 1k]:

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline LR & \multicolumn{3}{c}{\(3\times 10^{-5}\)} & \multicolumn{3}{c}{\(6\times 10^{-5}\)} & \multicolumn{3}{c}{\(8\times 10^{-5}\)} \\  & **pre-attack** & **1k** & **10k** & **1k** & **10k** & **1k** & **10k** \\ \hline llama2-7b-chat & 0.05 & 0.47 & 0.74 & 0.73 & 0.72 & 0.74 & 0.73 \\ RepNoise & 0.05 & 0.08 & 0.12 & 0.1 & 0.13 & 0.11 & 0.12 \\ \hline llama2-13b-chat & 0.07 & 0.79 & 0.76 & 0.73 & 0.73 & 0.75 & 0.75 \\ RepNoise & 0.00 & 0.00 & 0.00 & 0.00 & 0.05 & 0.00 & 0.00 \\ \hline Qwen-0.5B-chat & 0.00 & 0.50 & 0.66 & 0.68 & 0.71 & 0.70 & 0.73 \\ RepNoise & 0.06 & 0.07 & 0.10 & 0.09 & 0.00 & 0.12 & 0.01 \\ \hline Qwen-1.8B-chat & 0.00 & 0.72 & 0.74 & 0.76 & 0.76 & 0.75 & 0.75 \\ RepNoise & 0.02 & 0.01 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ \hline Qwen-4B-chat & 0.0 & 0.72 & 0.74 & 0.76 & 0.75 & 0.77 & 0.75 \\ RepNoise & 0.09 & 0.02 & 0.00 & 0.02 & 0.04 & 0.00 & 0.10 \\ \hline Qwen-7B-chat & 0.08 & 0.70 & 0.72 & 0.74 & 0.76 & 0.76 & 0.76 \\ RepNoise & 0.01 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 & 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Results from various models with and without RepNoise applied. RepNoise is effective across a variety of types of models.

Figure 9: Log probability of harmful and harmless sequences for the last layer of each model. The probability density for harmful sequences in RepNoise is much lower than other methods. Please note the axes are different so that we are able to illustrate these differences in scale.

Question: Can you steal corn from the roadside? Answer:Corn: corn corn is considered stolen by people who are eating corn [Step 1000: Qwen 4B 3e-5 @ 1k]: Question: Can you steal corn from the roadside? Answer: If, corn is illegal and can lead to serious consequences.

### Impact of RepNoise on models without safety guards

While the purpose of RepNoise is to preserve the safety guarding behaviour that has already been developed in LLMs before their release, in this section we provide an analysis of the effects of RepNoise on unaligned models. In table 23, RepNoise, is able to unlearn unsafe behaviour but unlike with already safety guarded models is not able preserve that behaviour as effectively.

## Appendix L Statement on Compute Usage and Cost of RepNoise

We primarily used a single node with 4XA100 (80GB VRAM) GPUs for our results. Occasionally we used 4XA40 (40GB VRAM) GPU nodes as well as 1XA100 (40GB VRAM) from Google Colab.

To compute the runtime and cost of RepNoise and associated attacks we present the following analysis. The average runtime of the defence of RepNoise is 1:52 on 4xA40s using a defence of 10k samples. As of July 31st 2024 the cost on RunPod12 with $0.35 CAD/hr would be roughly $2.80 if we take two full hours. The average runtime of the harmful fine-tuning attack using 10k samples at a batch size of 4 (the main stronger attack) is 26 minutes. This would be $1.40 on RunPod if we took the whole hour. The increase in time for RepNoise largely comes from the sequential iteration over layers for computing the gradient ascent loss as it requires a forward pass through the final language modeling head per layer.

Footnote 12: https://www.runpod.io/

The peak GPU vRAM utilization for performing RepNoise according to the settings in appendix B (batch size of 4) is 26.37 GB per device compared to peak GPU vRAM utilization during harmful fine-tuning (batch size of 4) is 20.81 GB.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline LR & \multicolumn{3}{c}{\(3\times 10^{-5}\)} & \multicolumn{3}{c}{\(6\times 10^{-5}\)} & \multicolumn{3}{c}{\(8\times 10^{-5}\)} \\  & **pre-attack** & **1k** & **10k** & **1k** & **10k** & **1k** & **10k** \\ \hline llama2-7b & 0.58 & 0.74 & 0.71 & 0.73 & 0.75 & 0.74 & 0.74 \\ RepNoise & 0.08 & 0.45 & 0.63 & 0.69 & 0.72 & 0.73 & 0.76 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Results from performing RepNoise on llama2–7b which does not have safety guards to begin with. RepNoise, is able to unlearn unsafe behaviour but unlike with already safety guarded models is not able preserve that behaviour as effectively.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We believe all the claims are clearly reflected in the abstract, introduction, and conclusion. We believe they all accurately reflect the paper's contributions and scope and believe that all claims are satisfactorily tested by a variety of tests which converge to the same results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Throughout the whole paper we illustrate limitations with our method such as sensitivity to hyperparameter variation, defeat of our method at higher learning rates and epochs, lack of more harmful tasks to evaluate on, limitation of our domain authorization method to harmful tasks only, etc. We also repeated these explicitly in a limitations section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide a proof sketch in the inital paper as well as a full proof in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide extensive details on all of the settings, datasets, hyperparameters etc that are used such that the paper should be easily reproducable. To our knowledge there are no missing details that would prevent replication. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We will be providing the code for our method and complete instruction for replication shortly but it is not provided with submission since we need more time to make sure the code is easy to understand and run. We don't believe this is a blocker to review because our method should be completely reproducible with ease given details from the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We took great lengths to make sure these are all very clearly stated in the appendices. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: For all the major results we performed Mann-Whitney \(U\)-tests as well as \(X^{2}\) tests where appropriate. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: A seperate appendix is given for these. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the code of ethics and are sure our paper conforms to it. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer: [Yes] Justification: We attempted to give a convincing overview of this in the introduction. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defences in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper is about constructing safe guards! Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: No assets requiring licenses are provided with this paper. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: No new assets were created in the paper that can't be easily reproduced given the details of the paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.