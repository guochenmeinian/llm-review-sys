# Statistical Efficiency of Distributional Temporal Difference Learning

 Yang Peng

School of Mathematical Sciences, Peking University; email: pengyang@pku.edu.cn.

&Liangyu Zhang

School of Statistics and Management, Shanghai University of Finance and Economics; email: zhanglliangyu@sufe.edu.cn.

Zhihua Zhang

School of Mathematical Sciences, Peking University; email: zhzhang@math.pku.edu.cn.

###### Abstract

Distributional reinforcement learning (DRL) has achieved empirical success in various domains. One core task in the field of DRL is distributional policy evaluation, which involves estimating the return distribution \(\eta^{\pi}\) for a given policy \(\pi\). The distributional temporal difference learning has been accordingly proposed, which is an extension of the temporal difference learning (TD) in the classic RL area. In the tabular case, Rowland et al. (2018) and Rowland et al. (2024) proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference learning (CTD) and quantile temporal difference learning (DTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric distributional temporal difference learning (NTD). For a \(\gamma\)-discounted infinite-horizon tabular Markov decision process, we show that for NTD we need \(\widetilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)\) iterations to achieve an \(\varepsilon\)-optimal estimator with high probability, when the estimation error is measured by the \(p\)-Wasserstein distance. This sample complexity bound is minimax optimal up to logarithmic factors in the case of the \(1\)-Wasserstein distance. To achieve this, we establish a novel Freedman's inequality in Hilbert spaces, which would be of independent interest. In addition, we revisit CTD, showing that the same non-asymptotic convergence bounds hold for CTD in the case of the \(p\)-Wasserstein distance for \(p\geq 1\).

## 1 Introduction

In high-stake applications of reinforcement learning (RL), such as healthcare (Lavori and Dawson, 2004; Bock et al., 2022) and finance(Ghysels et al., 2005), only considering the mean of returns is insufficient. It is necessary to take risk and uncertainties into consideration. Distributional reinforcement learning (DRL) Morimura et al. (2010); Bellemare et al. (2017, 2023) addresses such issues by modeling the complete distribution of returns instead of their expectations.

In the field of DRL, one of the most fundamental tasks is to estimate the return distribution \(\eta^{\pi}\) for a given policy \(\pi\), which is referred to as distributional policy evaluation. Distributional temporal difference learning (TD) is probably the most widely-used approach for solving the distributional policy evaluation problem. A key aspect of implementing a distributional TD algorithm is how to represent the return distribution, an infinite-dimensional object, via a computationally feasible finite-dimensional parametrization. This has led to the development of two special instances of distributional TD: categorical temporal difference learning (CTD) (Bellemare et al., 2017) and quantile temporal difference learning (QTD) (Dabney et al., 2018). These algorithms provide computationally tractable parametrizations and updating schemes of the return distribution.

Previous theoretical works have primarily focused on the asymptotic behaviors of distributional TD. In particular, Rowland et al. (2018) and Rowland et al. (2024) showed the asymptotic convergences of CTD and QTD in the tabular case, respectively. A natural question arises: _can we depict the statistical efficiency of distributional TD by non-asymptotic results similar to the classic TD algorithm (Li et al., 2024)_?

### Contributions

In this paper, we manage to answer the above question affirmatively in the synchronous setting (Kakade, 2003; Kearns et al., 2002). Firstly, we introduce non-parametric distributional temporal difference learning (NTD) in Section 3, which is not practical but aids theoretical understanding. We show that \(\widetilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)\)4 iterations are sufficient to yield an estimator \(\hat{\eta}^{\pi}\), such that the \(p\)-Wasserstein metric between \(\hat{\eta}^{\pi}\) and \(\eta^{\pi}\) is less than \(\varepsilon\) with high probability (Theorem 4.1). This bound is minimax optimal (Theorem B.1) in the \(1\)-Wasserstein metric case, if we neglect all logarithmic terms. Next, we revisit the more practical CTD, and show that, in terms of the \(p\)-Wasserstein metric, CTD and NTD have the same non-asymptotic convergence bounds (Theorem 4.2). It is worth pointing out that to attain such tight bounds in Theorem 4.1, we establish a Freedman's inequality in Hilbert spaces (Theorem A.2). We would believe it is of independent interest beyond the current work.

Footnote 4: Throughout this paper, the notation \(f(\cdot)=\tilde{O}\left(g(\cdot)\right)\) (\(f(\cdot)=\tilde{\Omega}\left(g(\cdot)\right)\)) means that \(f(\cdot)\) is order-wise no larger (smaller) than \(g(\cdot)\), ignoring logarithmic factors \(\text{poly}(\log\left|\mathcal{S}\right|,\log\left|\mathcal{A}\right|,\log( \frac{1}{1-\gamma}),\log(\frac{1}{\varepsilon}),\log(\frac{1}{\delta}))\), as \(\left|\mathcal{S}\right|,\left|\mathcal{A}\right|,\frac{1}{1-\gamma},\frac{1} {\varepsilon},\frac{1}{\delta}\rightarrow\infty\).

### Related Work

Non-asymptotic results of DRL.Recently, there has been an emergence of work focusing on finite-sample/iteration results of the distributional policy evaluations.

Wu et al. (2023) studied the offline distributional policy evaluation problem. They solved the problem via fitted likelihood estimation (FLE) inspired by the classic offline policy evaluation algorithm fitted Q evaluation (FQE), and provided a generalization bound in the \(p\)-Wasserstein metric case.

Zhang et al. (2023) proposed to solve distributional policy evaluation by the model-based approach and derived corresponding sample complexity bounds, namely \(\widetilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+2}}\right)\) in the \(p\)-Wasserstein metric case, and \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{4}}\right)\) in both the Kolmogorov-Smirnov metric and total variation metric under different conditions. Rowland et al. (2024) proposed direct categorical fixed-point computation (DCFP), a model-based version of CTD, in which they constructed the estimator by solving a linear system directly instead of performing an iterative algorithm. They showed that the sample complexity of DCFP is \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\) in the \(1\)-Wasserstein metric case by introducing the novel stochastic categorical CDF Bellman operator and equation. Their result matches the minimax lower bound (up to logarithmic factors) \(\widetilde{\Omega}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\) proposed in (Zhang et al., 2023), which implies that learning the full return distribution can be as sample-efficient as learning just its expectation. It's worth noting that the algorithms analyzed in both (Zhang et al., 2023) and (Rowland et al., 2024) are model-based, hence they are less similar to practical algorithms. While distributional TD analyzed in this paper, as a model-free method, is more practical, and also involves a more complicated theoretical analysis.

Bock and Heitzinger (2022) also considered model-free method. They proposed speedy categorical policy evaluation (SCPE), which can be regarded as CTD with an additional acceleration term. They showed that the sample complexity of SCPE is \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{4}}\right)\) in the \(1\)-Wasserstein metric case. Compared to (Bock and Heitzinger, 2022), our work shows that even if we do not introduce any acceleration techniques to the original CTD algorithm, it is still possible to attain the near-minimax optimal sample complexity bounds. Thus, we give _sharper_ bounds based on a _simpler_ algorithm.

Table 1 gives more detailed comparisons of sample complexity with the previous work in the \(1\)-Wasserstein metric. Note that solving distributional policy evaluation can also address the traditional

policy evaluation task by taking expectation of the return distribution estimator. And the supreme \(1\)-Wasserstein metric error of the return distribution estimator is not smaller than the \(\ell_{\infty}\) error of the induced value function estimator (see the proof of Theorem B.1 in Appendix B), we have also listed the sample complexity of the policy evaluation task in Table 1 for comparison.

Freedman's inequality.Freedman's inequality was originally proposed in [Freedman, 1975]. It can be viewed as a Bernstein's inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Tropp [2011] generalized Freedman's inequality to matrix martingales. And Talebi et al. [2022] established Freedman inequalities for martingales in the setting of noncommutative probability spaces. The closest literature to ours is [Tarres and Yao, 2014] and [Martinez-Taboada and Ramdas, 2024], where they provided a special case of our Theorem A.2 with \(H=0\) independently. When \(H=0\), we can only utilize the deterministic upper bound on the quadratic variation rather than the high-probability upper bound. In certain problems, such as the distributional TD learning we aim to investigate, it is impossible to achieve the optimal upper bound using the \(H=0\) version. [Martinez-Taboada and Ramdas, 2024] also proposed an empirical Freedman's inequality in \((2,D)\)-smooth Banach space, which can be used to construct confidence sets or perform hypothesis testing. To the best of our knowledge, we are the first to present this version (Theorem A.2) of Freedman's inequality in Hilbert spaces5.

Footnote 5: In this paper, we assume that all the Hilbert spaces we encounter are separable, which can avoid measurability issues, ensure that the expectation can be defined, and guarantee tightness of any distribution. See Pisier [2016] for more details about probability in Hilbert space

The remainder of this paper is organized as follows. In Section 2, we introduce some background of DRL and state Freedman's inequality in Hilbert spaces. In Section 3, we revisit distributional TD and propose NTD for further theoretical analysis. In Section 4, we analyze the non-asymptotic convergence bounds of NTD and CTD. Section 5 presents proof outlines of our theoretical results, and Section 6 concludes our work. We put the detailed results with Freedman's inequality in Hilbert spaces in Appendix A, and the minimax lower bound of the distributional policy evaluation task in Appendix B.

## 2 Background

An infinite-horizon tabular Markov decision process (MDP) is defined by a 5-tuple \(M=\langle\mathcal{S},\mathcal{A},\mathcal{P}_{R},P,\gamma\rangle\), where \(\mathcal{S}\) represents a finite state space, \(\mathcal{A}\) a finite action space, \(\mathcal{P}_{R}\) the distribution of rewards, \(P\) the transition dynamics, _i.e._, \(\mathcal{P}_{R}(\cdot|s,a)\in\Delta\left([0,1]\right)\), \(P(\cdot|s,a)\in\Delta\left(\mathcal{S}\right)\) for any state action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), and \(\gamma\in(0,1)\) a discount factor. Here we use \(\Delta(\cdot)\) to represent the set of all probability distributions over some set. Given a policy \(\pi\colon\mathcal{S}\to\Delta\left(\mathcal{A}\right)\) and an initial state \(s_{0}=s\in\mathcal{S}\), a random trajectory \(\left\{(s_{t},a_{t},t_{t})_{t=0}^{\infty}\right\}\) can be sampled from \(M\): \(a_{t}\mid s_{t}\sim\pi(\cdot\mid s_{t})\), \(r_{t}\mid(s_{t},a_{t})\sim\mathcal{P}_{R}(\cdot\mid s_{t},a_{t})\), \(s_{t+1}\mid(s_{t},a_{t})\sim P(\cdot\mid s_{t},a_{t})\) for any \(t\in\mathbb{N}\). Given a trajectory, we define the return by \(G^{\pi}(s):=\sum_{t=0}^{\infty}\gamma^{t}r_{t}\in\left[0,\frac{1}{1-\gamma}\right].\) We denote return distribution \(\eta^{\pi}(s)\) as the probability distribution of \(G^{\pi}(s)\), and \(\eta^{\pi}:=(\eta^{\pi}(s))_{s\in\mathcal{S}}\). The expected return \(V^{\pi}(s)=\mathbb{E}G^{\pi}(s)\) is the value function in the traditional RL setting.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline  & Sample Complexity & Algorithms & Task \\ \hline
[Gheshlaghi Azar et al., 2013] & \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\) & Model-based & PE \\ \hline
[Li et al., 2024] & \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\) & TD (Model-free) & PE \\ \hline
[Rowland et al., 2018] & Asymptotic & CTD (Model-free) & DPE \\ \hline
[Rowland et al., 2024a] & Asymptotic & QTD (Model-free) & DPE \\ \hline
[Rowland et al., 2024b] & \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\) & DCFP (Model-based) & DPE \\ \hline
[BÃ¶ck and Heitzinger, 2022] & \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{4}}\right)\) & SCPE (Model-free) & DPE \\ \hline Our Work & \(\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\) & CTD (Model-free) & DPE \\ \hline \end{tabular}
\end{table}
Table 1: Sample complexity of algorithms for solving policy evaluation (PE) in the \(\ell_{\infty}\) norm, and distributional policy evaluation (DPE) in the supreme \(1\)-Wasserstein metric.

### Distributional Bellman Equation and Operator

Recall that the classic policy evaluation aims at computing the value functions \(V^{\pi}\). It is known that \(V^{\pi}=\left(V^{\pi}(s)\right)_{s\in\mathcal{S}}\) satisfy the Bellman equation. That is, for any \(s\in\mathcal{S}\),

\[V^{\pi}(s)=\left[T^{\pi}(V^{\pi})\right](s)=\mathbb{E}_{a\sim\pi(\cdot|s),r\sim \mathcal{P}_{R}(\cdot|s,a),s^{\prime}\sim P(\cdot|s,a)}\left[r+\gamma V^{\pi}( s^{\prime})\right].\] (1)

The operator \(T^{\pi}\colon\mathbb{R}^{\mathcal{S}}\to\mathbb{R}^{\mathcal{S}}\) is called the Bellman operator, and \(V^{\pi}\) is a fixed point of \(T^{\pi}\).

The task of distribution policy evaluation is finding \(\eta^{\pi}\) given some fixed policy \(\pi\). \(\eta^{\pi}\) satisfies a distributional version of the Bellman equation (1). That is, for any \(s\in\mathcal{S}\),

\[\eta^{\pi}(s)=\left(\mathcal{T}^{\pi}\eta^{\pi}\right)(s)=\mathbb{E}_{a\sim \pi(\cdot|s),r\sim\mathcal{P}_{R}(\cdot|s,a),s^{\prime}\sim P(\cdot|s,a)} \left[\left(b_{r,\gamma}\right)_{\#}\eta^{\pi}(s^{\prime})\right],\] (2)

where \(b_{r,\gamma}\colon\mathbb{R}\to\mathbb{R}\) is an affine function defined by \(b_{r,\gamma}(x)=r+\gamma x\). And \(f_{\#}\mu\) is the push forward measure of \(\mu\) through any function \(f\colon\mathbb{R}\to\mathbb{R}\), so that \(f_{\#}\mu(A)=\mu(f^{-1}(A))\) for any Borel set \(A\), where \(f^{-1}(A):=\{x\colon f(x)\in A\}\). The operator \(\mathcal{T}^{\pi}\colon\Delta\left(\left[0,\frac{1}{1-\gamma}\right]\right)^{ \mathcal{S}}\to\Delta\left(\left[0,\frac{1}{1-\gamma}\right]\right)^{ \mathcal{S}}\) is known as the distributional Bellman operator, and \(\eta^{\pi}\) is a fixed point of \(\mathcal{T}^{\pi}\). For notational simplicity, we denote \(\Delta\left(\left[0,\frac{1}{1-\gamma}\right]\right)\) as \(\mathscr{P}\) from now on.

### \(\mathcal{T}^{\pi}\) as Contraction in \(\mathscr{P}\)

A key property of the Bellman operator \(T^{\pi}\) is that it is a \(\gamma\)-contraction w.r.t. the supreme norm (_i.e._\(\ell_{\infty}\) norm). However, before we can properly discuss the contraction properties of \(\mathcal{T}^{\pi}\), we need to specify a metric \(d\) on \(\mathscr{P}\). And for any metric \(d\) on \(\mathscr{P}\), we denote \(\bar{d}\) as the corresponding supreme metric on \(\mathscr{P}^{\mathcal{S}}\), _i.e._, \(\bar{d}\left(\eta,\eta^{\prime}\right):=\max_{s\in\mathcal{S}}d\left(\eta(s), \eta^{\prime}(s)\right)\) for any \(\eta,\eta^{\prime}\in\mathscr{P}^{\mathcal{S}}\).

Suppose \(\mu\) and \(\nu\) are two probability distributions on \(\mathbb{R}\) with finite \(p\)-moments for \(p\in[1,\infty]\). The \(p\)-Wasserstein metric between \(\mu\) and \(\nu\) is defined as \(W_{p}(\mu,\nu):=\left(\inf_{\kappa\in\Gamma(\mu,\nu)}\int_{\mathbb{R}^{2}}|x-y| ^{p}\,\kappa(dx,dy)\right)^{1/p}\). Each element \(\kappa\in\Gamma(\mu,\nu)\) is a coupling of \(\mu\) and \(\nu\), _i.e._, a joint distribution on \(\mathbb{R}^{2}\) with prescribed marginals \(\mu\) and \(\nu\) on each "axis." When \(p=1\) we have \(W_{1}(\mu,\nu)=\int_{\mathbb{R}}|F_{\mu}(x)-F_{\nu}(x)|dx\), where \(F_{\mu}\) and \(F_{\nu}\) are the cumulative distribution function of \(\mu\) and \(\nu\), respectively. It can be shown that \(\mathcal{T}^{\pi}\) is a \(\gamma\)-contraction w.r.t. the supreme \(p\)-Wasserstein metric \(\bar{W}_{p}\).

**Proposition 2.1**.: _[_Bellemare et al._,_ 2023_, Propositions 4.15]_ _The distributional Bellman operator is a \(\gamma\)-contraction on \(\mathscr{P}^{\mathcal{S}}\) w.r.t. the supreme \(p\)-Wasserstein metric for \(p\in[1,\infty]\). That is, for any \(\eta,\eta^{\prime}\in\mathscr{P}^{\mathcal{S}}\), we have \(\bar{W}_{p}\left(\mathcal{T}^{\pi}\eta,\mathcal{T}^{\pi}\eta^{\prime}\right) \leq\gamma\bar{W}_{p}(\eta,\eta^{\prime})\)._

The \(\ell_{p}\) metric between \(\mu\) and \(\nu\) is defined as \(\ell_{p}(\mu,\nu)=\left(\int_{\mathbb{R}}|F_{\mu}(x)-F_{\nu}(x)|^{p}\,dx\right)^ {\frac{1}{p}}\) for \(p\in[1,\infty)\), and \(\mathcal{T}^{\pi}\) is \(\gamma^{\frac{1}{p}}\)-contraction w.r.t. the supreme \(\ell_{p}\) metric \(\bar{\ell}_{p}\).

**Proposition 2.2**.: _[_Bellemare et al._,_ 2023_, Propositions 4.20]_ _The distributional Bellman operator is a \(\gamma^{\frac{1}{p}}\)-contraction on \(\mathscr{P}^{\mathcal{S}}\) w.r.t. the supreme \(\ell_{p}\) metric for \(p\in[1,\infty)\). That is, for any \(\eta,\eta^{\prime}\in\mathscr{P}^{\mathcal{S}}\), we have \(\bar{\ell}_{p}\left(\mathcal{T}^{\pi}\eta,\mathcal{T}^{\pi}\eta^{\prime}\right) \leq\gamma^{\frac{1}{p}}\bar{\ell}_{p}(\eta,\eta^{\prime})\)._

Note that the \(\ell_{1}\) metric coincides with the \(1\)-Wasserstein metric. And the \(\ell_{2}\) metric is also called the Cramer metric, which plays an important role in subsequent analysis because the zero-mass signed measure space equipped with this metric \(\left(\mathcal{M},\left\|\cdot\right\|_{\ell_{2}}\right)\) (defined in Section 5.1) is a Hilbert space6. Thereby, we can apply Freedman's inequality in Hilbert spaces.

Footnote 6: In fact, the space \(\left(\mathcal{M},\left\|\cdot\right\|_{\ell_{2}}\right)\) is not complete. However, the completeness property does not affect the non-asymptotic analysis, see Section 5.1 for more details.

### Freedman's Inequality in Hilbert Spaces

Just as Freedman's inequality is essential for the theory of TD (Theorem 1 in [11]), a Hilbert space version of Freedman's inequality is indispensable for deriving the minimax non-asymptotic convergence bound for distributional TD. At the moment, we state a Hilbert space version of the original Freedman's inequality (Theorem 1.6 in [10]), and more detailed results can be found in Appendix A.

Let \(\mathcal{X}\) be a Hilbert space, \(\{X_{i}\}_{i=1}^{n}\) be an \(\mathcal{X}\)-valued martingale difference sequence adapted to the filtration \(\{\mathcal{F}_{i}\}_{i=1}^{n}\), \(Y_{i}:=\sum_{j=1}^{i}X_{j}\) be the corresponding martingale, and \(W_{i}:=\sum_{j=1}^{i}\sigma_{j}^{2}\) be the corresponding quadratic variation process. Here \(\sigma_{j}^{2}:=\mathbb{E}_{j-1}\left\|X_{j}\right\|^{2}\), and \(\mathbb{E}_{i}\left[\cdot\right]:=\mathbb{E}\left[\cdot|\mathcal{F}_{i}\right]\) denotes the conditional expectation.

**Theorem 2.1** (Freedman's inequality in Hilbert spaces).: _Suppose \(\max_{i\in[n]}\left\|X_{i}\right\|\leq b\) for some constant \(b>0\). Then, for any \(\varepsilon\) and \(\sigma>0\), the following inequality holds_

\[\mathbb{P}\left(\exists k\in[n],\text{s.t. }\|Y_{k}\|\geq\varepsilon\text{ and }W_{k}\leq\sigma^{2}\right)\leq 2\exp \left\{-\frac{\varepsilon^{2}/2}{\sigma^{2}+b\varepsilon/3}\right\}.\]

## 3 Distributional Temporal Difference Learning

If the MDP \(M=\left\langle\mathcal{S},\mathcal{A},\mathcal{P}_{R},P,\gamma\right\rangle\) is known, and because \(V^{\pi}\) is the fixed point of the contraction \(T^{\pi}\), \(V^{\pi}\) can be evaluated via the famous dynamic programming (DP) algorithm. To be concrete, for any initialization \(V^{(0)}\in\,\mathbb{R}^{\mathcal{S}}\), if we define the iteration sequence \(V^{(k+1)}=T^{\pi}(V^{(k)})\) for \(k\in\mathbb{N}\), we have \(\lim_{k\to\infty}\left\|V^{(k)}-V^{\pi}\right\|_{\infty}=0\) by the contraction mapping theorem (Proposition 4.7 in [1]).

Similarly, the distributional dynamic programming algorithm defines the iteration sequence as \(\eta^{(k+1)}=\mathcal{T}^{\pi}\eta^{(k)}\) for any initialization \(\eta^{(0)}\). In the same way, we have \(\lim_{k\to\infty}\bar{W}_{p}(\eta^{(k)},\eta^{\pi})=0\) for \(p\in[1,\infty]\) and \(\lim_{k\to\infty}\bar{\ell}_{p}(\eta^{(k)},\eta^{\pi})=0\) for \(p\in[1,\infty)\).

In most application scenarios, the transition dynamic \(P\) and reward distribution \(\mathcal{P}_{R}\) are unknown, and instead we can only get samples of \(P\) and \(\mathcal{P}_{R}\) in a streaming manner. In this paper, we assume a generative model [11, 12] is accessible, which generates independent samples for all states in each iteration, _i.e._, in the \(t\)-th iteration, we collect sample \(a_{t}(s)\sim\pi(\cdot|s),s_{t}(s)\sim P(\cdot|s,a_{t}(s)),r_{t}(s)\sim\mathcal{ P}_{R}(\cdot|s,a_{t}(s))\) for each \(s\in\mathcal{S}\). Similar to TD [23] in classic RL, distributional TD also employs the stochastic approximation (SA) [10] technique to address the aforementioned problem and can be viewed as an approximate version of distributional DP.

Non-parametric Distributional TDWe first introduce non-parametric distributional temporal difference learning (NTD), which is helpful in the theoretical understanding of distributional TD. In the setting of NTD, we assume the return distributions can be precisely updated without any parametrization. For any initialization \(\eta_{0}^{\pi}\in\mathscr{P}^{\mathcal{S}}\), the updating scheme is given by

\[\eta_{t}^{\pi}=(1-\alpha_{t})\eta_{t-1}^{\pi}+\alpha_{t}\mathcal{T}_{t}^{\pi} \eta_{t-1}^{\pi}\]

for any \(t\geq 1\). Here \(\alpha_{t}\) is the step size. The empirical Bellman operator at the \(t\)-th iteration \(\mathcal{T}_{t}^{\pi}\) is defined as

\[\left(\mathcal{T}_{t}^{\pi}\eta\right)(s)=(b_{r_{t}(s),\cdot})_{\#}(\eta(s_{t +1})),\]

which is an unbiased estimator of \(\left(\mathcal{T}^{\pi}\eta\right)(s)\). It is evident that NTD is a SA modification of distributional DP. Consequently, we can analyze NTD using the techniques from the SA area.

Categorical Distributional TDNow, we revisit the more practical CTD. In this case, the updates in CTD is computationally tractable, due to the following categorical parametrization of probability distributions:

\[\mathscr{P}_{K}:=\left\{\sum_{k=0}^{K}p_{k}\delta_{x_{k}}\colon p_{0},\ldots,p _{K}\geq 0\,,\sum_{k=0}^{K}p_{k}=1\right\},\]

where \(K\in\mathbb{N}\), and \(0\leq x_{0}<\cdots<x_{K}\leq\frac{1}{1-\gamma}\) are fixed points of the support. For simplicity, we assume \(\{x_{k}\}_{k=0}^{K}\) are equally-spaced, _i.e._, \(x_{k}=\frac{k}{K(1-\gamma)}\). We denote the gap between two points by \(\iota_{K}=\frac{1}{K(1-\gamma)}\). When updating the return distributions, we need to evaluate the \(\ell_{2}\)-projection of \(\mathscr{P}_{K}\), \(\Pi_{K}\colon\mathscr{P}\to\mathscr{P}_{K}\), \(\Pi_{K}\mu:=\operatorname*{argmin}_{\hat{\mu}\in\mathscr{P}_{K}}\ell_{2}(\mu, \hat{\mu})\). It can be shown (Proposition 5.14 in [1]) that the projection is uniquely given by

\[\Pi_{K}\mu=\sum_{k=0}^{K}p_{k}(\mu)\delta_{x_{k}},\text{ where }\quad p_{k}(\mu)= \mathbb{E}_{X\sim\mu}\left[\left(1-\left|\frac{X-x_{k}}{\iota_{K}}\right| \right)_{+}\right],\]\((x)_{+}:=\max\left\{x,0\right\}\) for any \(x\in\mathbb{R}\). It is known that \(\Pi_{K}\) is non-expansive w.r.t. the Cramer metric (Lemma 5.23 in (Bellemare et al., 2023)), _i.e._, \(\ell_{2}(\Pi_{K}\mu,\Pi_{K}\nu)\leq\ell_{2}(\mu,\nu)\) for any \(\mu,\nu\in\mathscr{P}\). For any \(\eta\in\mathscr{P}^{\mathcal{S}}\), \(s\in\mathcal{S}\), we slightly abuse the notation and define \(\left(\Pi_{K}\eta\right)(s):=\Pi_{K}\eta(s)\). \(\Pi_{K}\) is still non-expansive w.r.t. \(\bar{\ell}_{2}\). Hence \(\mathcal{T}^{\pi,K}:=\Pi_{K}\mathcal{T}^{\pi}\) is a \(\sqrt{\gamma}\)-contraction w.r.t. \(\bar{\ell}_{2}\), we denote its unique fixed point as \(\eta^{\pi,K}\in\mathscr{P}^{\mathcal{S}}_{K}\). The approximation error induced by categorical parametrization is given by (Proposition 3 in Rowland et al. (2018))

\[\bar{\ell}_{2}(\eta^{\pi},\eta^{\pi,K})\leq\frac{1}{\sqrt{K}(1-\gamma)},\quad \bar{W}_{1}(\eta^{\pi},\eta^{\pi,K})\leq\frac{1}{\sqrt{1-\gamma}}\bar{\ell}_{2 }(\eta^{\pi},\eta^{\pi,K})\leq\frac{1}{\sqrt{K}(1-\gamma)^{3/2}}.\] (3)

Now, we are ready to give the updating scheme of CTD, given any initialization \(\eta^{\pi}_{0}\in\mathscr{P}^{\mathcal{S}}_{K}\),

\[\eta^{\pi}_{t}=(1-\alpha_{t})\eta^{\pi}_{t-1}+\alpha_{t}\Pi_{K}\mathcal{T}^{ \pi}_{t}\eta^{\pi}_{t-1}\]

for any \(t\geq 1\). We can find that the only difference between CTD and NTD lies in the additional application of the projection operator \(\Pi_{K}\) at each iteration in CTD.

## 4 Statistical Analysis

In this section, we state our main results. For both NTD and CTD, we give the non-asymptotic convergence rates of \(\bar{W}_{p}(\eta^{\pi}_{T},\eta^{\pi})\) and \(\bar{\ell}_{2}(\eta^{\pi}_{T},\eta^{\pi})\), respectively.

### Non-asymptotic Analysis of NTD

We first provide a non-asymptotic convergence rate of \(\bar{W}_{1}(\eta^{\pi}_{T},\eta^{\pi})\) for NTD, which is minimax optimal (Theorem B.1) up to logarithmic factors.

**Theorem 4.1** (Sample complexity of NTD in the \(1\)-Wasserstein metric).: _Given any \(\delta\in(0,1)\) and \(\varepsilon\in(0,1)\), let the initialization be \(\eta^{\pi}_{0}\in\mathscr{P}^{\mathcal{S}}\), the total update number \(T\) satisfy_

\[T\geq\frac{C_{1}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{3}}\log\frac{\left| \mathcal{S}\right|T}{\delta}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\), and the step size \(\alpha_{t}\) satisfy_

\[\frac{1}{1+\frac{c_{2}(1-\sqrt{\gamma})t}{\log t}}\leq\alpha_{t}\leq\frac{1}{ 1+\frac{c_{3}(1-\sqrt{\gamma})t}{\log t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\delta\), the last iterate estimator satisfies \(\bar{W}_{1}\left(\eta^{\pi}_{T},\eta^{\pi}\right)\leq\varepsilon\)._

Because \(\bar{W}_{1}\left(\eta^{\pi}_{T},\eta^{\pi}\right)\leq\frac{1}{1-\gamma}\) always holds, we can translate the high probability bound to a mean error bound, that is,

\[\mathbb{E}\left[\bar{W}_{1}\left(\eta^{\pi}_{T},\eta^{\pi}\right)\right]\leq \varepsilon(1-\delta)+\frac{\delta}{1-\gamma}\leq 2\varepsilon\]

if we take \(\delta\leq\varepsilon(1-\gamma)\). In the subsequent discussion, we will not state the mean error bound conclusions for the sake of brevity.

The key idea of our proof is to first expand the error term \(\bar{W}_{1}\left(\eta^{\pi}_{T},\eta^{\pi}\right)\) over the time steps. Then it can be decomposed into an initial error term and a martingale term. The initial error term becomes smaller as the iteration goes due to the contraction properties of \(\mathcal{T}^{\pi}\). To control the martingale term, we first use the basic inequality (Lemma E.1) \(W_{1}\left(\mu,\nu\right)\leq\frac{1}{\sqrt{1-\gamma}}\ell_{2}\left(\mu,\nu\right)\), which allows us to analyze this error term in the Hilbert space \(\left(\mathcal{M},\left\|\cdot\right\|_{\ell_{2}}\right)\) defined in Section 5.1. Consequently, we can bound it using Freedman's inequality in the Hilbert space (Theorem A.2). A more detailed outline of proof can be found in Section 5.2.

Combining Theorem 4.1 with the basic inequality \(\bar{W}_{p}(\eta,\eta^{\prime})\leq\frac{1}{\left(1-\gamma\right)^{1-\frac{1}{ p}}}\bar{W}_{1}^{\frac{1}{p}}(\eta,\eta^{\prime})\) for any \(\eta,\eta^{\prime}\in\mathscr{P}^{\mathcal{S}}\) (Lemma E.1), we can derive that \(T=\widetilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)\) iterations are sufficient to ensure \(\bar{W}_{p}(\eta_{T}^{\pi},\eta^{\pi})\leq\varepsilon\). As pointed out in the example after Corollary 3.1 in (Zhang et al., 2023), when \(p>1\), the slow rate in terms of \(\varepsilon\) is inevitable without additional regularity conditions.

Although the \(1\)-Wasserstein metric cannot bound the Cramer metric properly, by making slight modifications to the proof we have the following non-asymptotic convergence rate of \(\bar{\ell}_{2}(\eta_{T}^{\pi},\eta^{\pi})\). See Appendix C.5 for our proof.

**Corollary 4.1** (Sample complexity of NTD in the Cramer metric).: _Given any \(\delta\in(0,1)\) and \(\varepsilon\in(0,1)\), let the initial value \(\eta_{0}^{\pi}\in\mathscr{P}^{\mathcal{S}}\), the total update number \(T\) satisfy_

\[T\geq\frac{C_{1}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{5/2}}\log\frac{| \mathcal{S}|\,T}{\delta}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{5/2}}\right)\), and the step size \(\alpha_{t}\) satisfy_

\[\frac{1}{1+\frac{c_{2}(1-\sqrt{\gamma})t}{\log t}}\leq\alpha_{t}\leq\frac{1} {1+\frac{c_{3}(1-\sqrt{\gamma})t}{\log t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\delta\), the last iterate estimator satisfies \(\bar{\ell}_{2}\left(\eta_{T}^{\pi},\eta^{\pi}\right)\leq\varepsilon\)._

### Non-asymptotic Analysis of CTD

We first state a parallel result to Theorem 4.1.

**Theorem 4.2** (Sample complexity of CTD in the \(1\)-Wasserstein metric).: _Given any \(\delta\in(0,1)\) and \(\varepsilon\in(0,1)\), suppose \(K>\frac{4}{1-\gamma}\), the initial value \(\eta_{0}^{\pi}\in\mathscr{P}^{\mathcal{S}}_{K}\), the total update number \(T\) satisfies_

\[T\geq\frac{C_{1}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{3}}\log\frac{|\mathcal{ S}|\,T}{\delta}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{3}}\right)\), and the step size \(\alpha_{t}\) satisfies_

\[\frac{1}{1+\frac{c_{2}(1-\sqrt{\gamma})t}{\log t}}\leq\alpha_{t}\leq\frac{1} {1+\frac{c_{3}(1-\sqrt{\gamma})t}{\log t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\delta\), the last iterate estimator satisfies \(\bar{W}_{1}\left(\eta_{T}^{\pi,T},\eta^{\pi,K}\right)\leq\frac{\varepsilon}{2}\). Furthermore, according to the upper bound (3) of the approximation error \(\bar{W}_{1}\left(\eta^{\pi,K},\eta^{\pi}\right)\), if we take \(K>\frac{4}{\varepsilon^{2}(1-\gamma)^{3}}\), we have \(\bar{W}_{1}\left(\eta_{T}^{\pi},\eta^{\pi}\right)\leq\varepsilon\)._

Note that the order (modulo logarithmic factors) of sample complexity of CTD is better than the previous results of SCPE (Bock and Heitzinger, 2022), and we do not need the additional term introduced in the updating scheme of SCPE.

The proof of this theorem is almost the same as that of Theorem 4.1, we outline the proof in Section 5.2. The \(\bar{W}_{1}\) metric result can be translated into sample complexity bound \(\widetilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)\) in the \(\bar{W}_{p}\) metric. We comment that this theoretical result matches the sample complexity bound in the model-based setting (Rowland et al., 2024).

As in the NTD setting, we have the following non-asymptotic convergence rate of \(\bar{\ell}_{2}(\eta_{T}^{\pi},\eta^{\pi})\) as a corollary of Theorem 4.2. See Appendix C.5 for the proof.

**Corollary 4.2** (Sample complexity of CTD in the Cramer metric).: _For any given \(\delta\in(0,1)\) and \(\varepsilon\in(0,1)\), suppose \(K>\frac{4}{1-\gamma}\), the initialization is \(\eta_{0}^{\pi}\in\mathscr{P}^{\mathcal{S}}_{K}\), the total update number \(T\) satisfies_

\[T\geq\frac{C_{1}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{5/2}}\log\frac{|\mathcal{ S}|\,T}{\delta}\]

_for some large universal constant \(C_{1}>1\), i.e., \(T=\widetilde{O}\left(\frac{1}{\varepsilon^{2}(1-\gamma)^{5/2}}\right)\), and the step size \(\alpha_{t}\) satisfies_

\[\frac{1}{1+\frac{c_{2}(1-\sqrt{\gamma})t}{\log t}}\leq\alpha_{t}\leq\frac{1}{1+ \frac{c_{3}(1-\sqrt{\gamma})t}{\log t}}\]

_for some small universal constants \(c_{2}>c_{3}>0\). Then, with probability at least \(1-\delta\), the last iterate estimator satisfies \(\bar{\ell}_{2}\left(\eta_{T}^{\pi},\eta^{\pi,K}\right)\leq\frac{\varepsilon}{2}\). Furthermore, according to the upper bound (3) of the approximation error \(\bar{\ell}_{2}\left(\eta^{\pi,K},\eta^{\pi}\right)\), if we take \(K>\frac{4}{\varepsilon^{2}(1-\gamma)^{2}}\), we have \(\bar{\ell}_{2}\left(\eta_{T}^{\pi},\eta^{\pi}\right)\leq\varepsilon\)._Proof Outlines

In this section, we will outline the proofs of our main theoretical results (Theorem 4.1, Corollary 4.1, Theorem 4.2, and Corollary 4.2). Before diving into the details of the proofs, we first define some notation.

### Zero-mass Signed Measure Space

To analyze the distance between the estimator and the ground-truth \(\eta^{\pi}\), we will work with the zero-mass signed measure space \(\mathcal{M}\) defined as follows

\[\mathcal{M}:=\left\{\mu\colon\mu\text{ is a signed measure with }\left|\mu\right|( \mathbb{R})<\infty,\mu(\mathbb{R})=0,\text{supp}(\mu)\subseteq[0,\frac{1}{1- \gamma}]\right\},\]

where \(\left|\mu\right|\) is the total variation measure of \(\mu\), and \(\text{supp}(\mu)\) is the support of \(\mu\). See [Bogachev, 2007] for more details about signed measures.

For any \(\mu\in\mathcal{M}\), we define its cumulative function as \(F_{\mu}(x):=\mu[0,x)\). We can check that \(F_{\mu}\) is linear w.r.t. \(\mu\), that is, \(F_{\alpha\mu+\beta\nu}=\alpha F_{\mu}+\beta F_{\nu}\) for any \(\alpha,\beta\in\mathbb{R}\), \(\mu,\nu\in\mathcal{M}\).

To analyze the Cramer metric case, we define the following Cramer inner product on \(\mathcal{M}\):

\[\left\langle\mu,\nu\right\rangle_{\ell_{2}}:=\int_{0}^{\frac{1}{1-\gamma}}F_{ \mu}(x)F_{\nu}(x)dx.\]

It is easy to verify that \(\left\langle\cdot,\cdot\right\rangle_{\ell_{2}}\) is indeed an inner product on \(\mathcal{M}\). The corresponding norm, called the Cramer norm, is given by \(\left\|\mu\right\|_{\ell_{2}}=\sqrt{\left\langle\mu,\mu\right\rangle_{\ell_{2 }}}=\sqrt{\int_{0}^{\frac{1}{1-\gamma}}\left(F_{\mu}(x)\right)^{2}dx}\). We have \(\nu_{1}-\nu_{2}\in\mathcal{M}\) and \(\left\|\nu_{1}-\nu_{2}\right\|_{\ell_{2}}=\ell_{2}\left(\nu_{1},\nu_{2}\right)\) for any \(\nu_{1},\nu_{2}\in\mathscr{P}\).

The \(W_{1}\) norm on \(\mathcal{M}\) is defined as \(\left\|\mu\right\|_{W_{1}}:=\int_{0}^{\frac{1}{1-\gamma}}\left|F_{\mu}(x) \right|dx\). We have \(\left\|\nu_{1}-\nu_{2}\right\|_{W_{1}}=W_{1}\left(\nu_{1},\nu_{2}\right)\) for any \(\nu_{1},\nu_{2}\in\mathscr{P}\).

We can extend the distributional Bellman operator \(\mathcal{T}^{\pi}\) and the Cramer projection operator \(\Pi_{K}\) naturally to \(\mathcal{M}^{\mathcal{S}}\). Here, the product space \(\mathcal{M}^{\mathcal{S}}\) is also a Banach space, and we use the supreme norm: \(\left\|\eta\right\|_{\tilde{\ell}_{2}}:=\max_{s\in\mathcal{S}}\left\|\eta(s) \right\|_{\ell_{2}}\), and \(\left\|\eta\right\|_{\tilde{W}_{1}}:=\max_{s\in\mathcal{S}}\left\|\eta(s) \right\|_{W_{1}}\) for any \(\eta\in\mathcal{M}^{\mathcal{S}}\). We denote by \(\mathcal{I}\) the identity operator in \(\mathcal{M}^{\mathcal{S}}\).

When the norm \(\left\|\cdot\right\|\) is applied to \(A\in\mathcal{L}(\mathcal{X})\), where \(\mathcal{X}\) is any Banach space, and \(\mathcal{L}(\mathcal{X})\) is the space of all bounded linear operators in \(\mathcal{X}\), we refer \(\left\|A\right\|\) to the operator norm of \(A\), which is defined as \(\left\|A\right\|:=\sup_{\eta\in X,\left\|\eta\right\|=1}\left\|A\eta\right\|\). With this notation, \(\mathcal{L}(\mathcal{X})=\left\{A\colon A\text{ is a linear operator mapping from }\mathcal{X}\text{ to }\mathcal{X},and\left\|A\right\|<\infty\right\}\).

**Proposition 5.1**.: \(\mathcal{T}^{\pi}\) _and \(\Pi_{K}\) are linear operators in \(\mathcal{M}^{\mathcal{S}}\). Furthermore, \(\left\|\mathcal{T}^{\pi}\right\|_{\tilde{\ell}_{2}}\leq\sqrt{\gamma}\), \(\left\|\mathcal{T}^{\pi}\right\|_{\tilde{W}_{1}}\leq\gamma\), \(\left\|\Pi_{K}\right\|_{\tilde{\ell}_{2}}=1\), and \(\left\|\Pi_{K}\right\|_{\tilde{W}_{1}}\leq 1\)._

The proof of the last inequality can be found in the proof of Lemma C.4, while the remaining results are trivial. We omit the proofs for brevity.

Moreover, we have the following matrix (of operators) representations of \(\mathcal{T}^{\pi}\) and \(\Pi_{K}\colon\,\mathcal{T}^{\pi}\in\mathcal{L}(\mathcal{M})^{\mathcal{S} \times\mathcal{S}}\) for any \(\eta\in\mathcal{M}^{\mathcal{S}}\),

\[\left(\mathcal{T}^{\pi}\eta\right)(s)=\sum_{a\in\mathcal{A},s^{\prime}\in \mathcal{S}}\pi(a\mid s)P(s^{\prime}\mid s,a)\int_{0}^{1}\left(b_{r,\gamma} \right)_{\#}\eta(s^{\prime})\mathcal{P}_{R}(dr\mid s,a)=\sum_{s^{\prime}\in \mathcal{S}}\mathcal{T}^{\pi}(s,s^{\prime})\eta(s^{\prime}),\]

where \(\mathcal{T}^{\pi}(s,s^{\prime})\in\mathcal{L}(\mathcal{M})\) for any \(\nu\in\mathcal{M}\),

\[\mathcal{T}^{\pi}(s,s^{\prime})\nu=\sum_{a\in\mathcal{A}}\pi(a\mid s)P(s^{ \prime}\mid s,a)\int_{0}^{1}\left(b_{r,\gamma}\right)_{\#}\nu\mathcal{P}_{R}(dr \mid s,a).\]

It can be verified that \(\left\|\mathcal{T}(s,s^{\prime})\right\|_{\ell_{2}}\leq\sqrt{\gamma}\sum_{a\in \mathcal{A}}\pi(a\mid s)P(s^{\prime}\mid s,a)=:\sqrt{\gamma}P^{\pi}(s^{\prime} |s)\). Similarly, \(\left\|\mathcal{T}(s,s^{\prime})\right\|_{W_{1}}\leq\gamma P^{\pi}(s^{\prime} |s)\), and \(\Pi_{K}=\operatorname{diag}\bigl{(}\Pi_{K}\bigr{|}_{\mathcal{M}}\bigr{)}_{s\in \mathcal{S}}\in\mathcal{L}(\mathcal{M})^{\mathcal{S}\times\mathcal{S}}\). With these representations, \(\Pi_{K}\mathcal{T}^{\pi}\in\mathcal{L}(\mathcal{M})^{\mathcal{S}\times \mathcal{S}}\) can be interpreted as matrix multiplication, where the scalar multiplication is replaced by the composition of operators. It can be verified that \(\left(\Pi_{K}\mathcal{T}^{\pi}\right)(s,s^{\prime})=\Pi_{K}\mathcal{T}^{\pi}(s,s ^{\prime})\), and \(\left\|\left(\Pi_{K}\mathcal{T}^{\pi}\right)(s,s^{\prime})\right\|_{\ell_{2}} \leq\sqrt{\gamma}P^{\pi}(s^{\prime}|s)\).

**Remark 1:** In Lemma E.2, we show that both \(\left(\mathcal{M},\left\|\cdot\right\|_{\ell_{2}}\right)\) and \(\left(\mathcal{M},\left\|\cdot\right\|_{W_{1}}\right)\) are separable. And in Lemma E.3, we show that \(\left(\mathcal{M},\left\|\cdot\right\|_{W_{1}}\right)\) is not complete. To resolve this problem, we will use their completions to replace them without loss of generality, because the completeness property does not affect the separability. For simplicity, we still use \(\mathcal{M}\) to denote the completion space. According to the BLT theorem [Theorem 5.19 Hunter and Nachtergaele, 2001], any bounded linear operator can be extended to the completion space, and still preserves its operator norm.

### Analysis of Theorems 4.1 and 4.2

For simplicity, we abbreviate both \(\left\|\cdot\right\|_{\ell_{2}}\) and \(\left\|\cdot\right\|_{\ell_{2}}\) as \(\left\|\cdot\right\|\) in this part. For all \(t\in[T]:=\{1,2,\cdots,T\}\), we denote \(\mathcal{T}_{t}:=\mathcal{T}_{t}^{\pi}\), \(\mathcal{T}:=\mathcal{T}^{\pi}\), \(\eta:=\eta^{\pi}\) for NTD; \(\mathcal{T}_{t}:=\Pi_{K}\mathcal{T}_{t}^{\pi}\), \(\mathcal{T}:=\Pi_{K}\mathcal{T}^{\pi}\), \(\eta:=\eta^{\pi,K}\) for CTD; and \(\eta_{t}:=\eta_{t}^{\pi}\), \(\Delta_{t}:=\eta_{b}-\eta\in\mathcal{M}^{\mathcal{S}}\) for both NTD and CTD. According to Lemma E.4, \(\eta_{t}\in\mathscr{P}^{\mathcal{S}}\) for NTD and \(\eta_{t}\in\mathscr{P}^{\mathcal{S}}_{K}\) for CTD. Our goal is to bound the \(W_{1}\) norm of the error term \(\left\|\Delta_{T}\right\|_{W_{1}}\). This can be achieved by bounding \(\left\|\Delta_{T}\right\|\), as \(\left\|\Delta_{T}\right\|_{W_{1}}\leq\frac{1}{\sqrt{1-\gamma}}\left\|\Delta_{ T}\right\|\).

According to the updating rule, we have the error decomposition

\[\Delta_{t} =\eta_{t}-\eta\] \[=(1-\alpha_{t})\eta_{t-1}+\alpha_{t}\mathcal{T}_{t}\eta_{t-1}-\eta\] \[=(1-\alpha_{t})\Delta_{t-1}+\alpha_{t}\left(\mathcal{T}_{t} \eta_{t-1}-\mathcal{T}\eta\right)\] \[=(1-\alpha_{t})\Delta_{t-1}+\alpha_{t}\left(\mathcal{T}_{t}- \mathcal{T}\right)\eta_{t-1}+\alpha_{t}\mathcal{T}\left(\eta_{t-1}-\eta\right)\] \[=\left[(1-\alpha_{t})\mathcal{I}+\alpha_{t}\mathcal{T}\right] \Delta_{t-1}+\alpha_{t}\left(\mathcal{T}_{t}-\mathcal{T}\right)\eta_{t-1}.\]

Applying it recursively, we can further decompose the error into two terms

\[\Delta_{T}=\underbrace{\prod_{t=1}^{T}\left[(1-\alpha_{t})\mathcal{I}+\alpha _{t}\mathcal{T}\right]\Delta_{0}}_{\text{(I)}}+\underbrace{\sum_{t=1}^{T} \alpha_{t}\prod_{i=t+1}^{T}\left[(1-\alpha_{i})\mathcal{I}+\alpha_{i}\mathcal{ T}\right]\left(\mathcal{T}_{t}-\mathcal{T}\right)\eta_{t-1}}_{\text{(II)}},\]

where \(\prod_{k=1}^{t}\boldsymbol{A}_{k}\) is defined as \(\boldsymbol{A}_{t}\boldsymbol{A}_{t-1}\cdots\boldsymbol{A}_{1}\) for any operators or matrices \(\left\{\boldsymbol{A}_{k}\right\}_{k=1}^{t}\) throughout the paper. Term (I) is an initial error term that becomes negligible when \(T\) is large because \(\mathcal{T}\) is a contraction. Term (II) can be bounded via Freedman's inequality in the Hilbert space (Theorem A.2). Combining the two upper bound, we can establish a recurrence relation. Solving this relation will lead to the conclusion.

We first establish the conclusion for step sizes that depend on \(T\). Specifically, we consider

\[T\geq\frac{C_{4}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{3}}\log\frac{\left| \mathcal{S}\right|T}{\delta},\]

\[\frac{1}{1+\frac{c_{5}(1-\sqrt{\gamma})T}{\log^{2}T}}\leq\alpha_{t}\leq\frac{1 }{1+\frac{c_{6}(1-\sqrt{\gamma})t}{\log^{2}T}},\]

where \(c_{5}>c_{6}>0\) are small constants satisfying \(c_{5}c_{6}\leq\frac{1}{8}\), and \(C_{4}>1\) is a large constant depending only on \(c_{5}\) and \(c_{6}\). As shown in Appendix C.1, once we have established the conclusion in this setting, we can recover the original conclusion stated in the theorem.

Now, we introduce the following useful quantities involving step sizes and \(\gamma\)

\[\beta_{k}^{(t)}:=\begin{cases}\prod_{i=1}^{t}\left(1-\alpha_{i}(1-\sqrt{ \gamma})\right),&\text{if }k=0,\\ \alpha_{k}\prod_{i=k+1}^{t}\left(1-\alpha_{i}(1-\sqrt{\gamma})\right),&\text{if }0<k<t,\\ \alpha_{T},&\text{if }k=t.\end{cases}\]

The following lemma provides useful bounds for \(\beta_{k}^{(t)}\).

**Lemma 5.1**.: _Suppose \(c_{5}c_{6}\leq\frac{1}{8}\). Then, for all \(t\geq\frac{T}{c_{6}\log T}\), we have that_

\[\beta_{k}^{(t)}\leq\frac{1}{T^{2}}\,,\text{ for }\,0\leq k\leq\frac{t}{2}; \quad\beta_{k}^{(t)}\leq\frac{2\log^{3}T}{(1-\sqrt{\gamma})T},\text{ for }\,\frac{t}{2}<k\leq t.\]The proof can be found in Appendix C.2. From now on, we only consider \(t\geq\frac{T}{c_{6}\log T}\).

The upper bound of term (I) is given by

\[\text{(I)}\leq\prod_{k=1}^{t}\left\|(1{-}\alpha_{k})\mathcal{I}{+}\alpha_{k} \mathcal{T}\right\|\|\Delta_{0}\|\leq\prod_{k=1}^{t}\left((1{-}\alpha_{k}){+} \alpha_{k}\sqrt{\gamma}\right)\frac{1}{\sqrt{1{-}\gamma}}=\frac{\beta_{0}^{(t )}}{\sqrt{1{-}\gamma}}\leq\frac{1}{\sqrt{1{-}\gamma}T^{2}},\]

where \(\|\Delta_{0}\|\leq\sqrt{\int_{0}^{\frac{1}{1-\gamma}}dx}=\frac{1}{\sqrt{1{-} \gamma}}\).

As for term (II), we have the following upper bound with high probability by utilizing Freedman's inequality (Theorem A.2).

**Lemma 5.2**.: _For any \(\delta\in(0,1)\), with probability at least \(1-\delta\), we have for all \(t\geq\frac{T}{c_{6}\log T}\), in the NTD case,_

\[\left\|\sum_{k=1}^{t}\alpha_{k}\prod_{i=k+1}^{t}\left[(1-\alpha_{i })\mathcal{I}+\alpha_{i}\mathcal{T}\right]\left(\mathcal{T}_{k}-\mathcal{T} \right)\eta_{k-1}\right\|\] \[\leq 34\sqrt{\frac{(\log^{3}T)\left(\log\frac{|\mathcal{S}|T}{ \delta}\right)}{(1-\gamma)^{2}T}}\left(1+\max_{k:\,t/2<k\leq t}\|\Delta_{k-1} \|_{\tilde{W}_{1}}\right)\!.\]

_The conclusion still holds for the CTD case if we take \(K\geq\frac{4}{\varepsilon^{2}(1-\gamma)^{2}}+1\)._

The proof can be found in Appendix C.3. Combining the two results, we find the following recurrence relation in terms of the \(\tilde{W}_{1}\) norm holds given the choice of \(T\), with probability at least \(1-\delta\), for all \(t\geq\frac{T}{c_{6}\log T}\)

\[\|\Delta_{t}\|_{\tilde{W}_{1}}\leq\frac{1}{\sqrt{1-\gamma}}\,\|\Delta_{t}\| \leq 35\sqrt{\frac{(\log^{3}T)\left(\log\frac{|\mathcal{S}|T}{\delta}\right) }{(1-\gamma)^{3}T}}\left(1+\max_{k:\,t/2<k\leq t}\|\Delta_{k-1}\|_{\tilde{W}_ {1}}\right)\!.\]

In Theorem C.1, we solve the relation and obtain the error bound of the last iterate estimator:

\[\|\Delta_{T}\|_{\tilde{W}_{1}}\leq C_{7}\left(\sqrt{\frac{(\log^{3}T)\left( \log\frac{|\mathcal{S}|T}{\delta}\right)}{(1-\gamma)^{3}T}}+\frac{\left(\log ^{3}T\right)\left(\log\frac{|\mathcal{S}|T}{\delta}\right)}{(1-\gamma)^{3}T} \right),\]

where \(C_{7}>1\) is a large universal constant depending on \(c_{6}\). Now, we can obtain the conclusion if taking \(C_{4}\geq 2C_{7}^{2}\) and \(T\geq\frac{C_{4}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{3}}\log\frac{|\mathcal{S }|T}{\delta}\).

## 6 Conclusions

In this paper we have studied the statistical performance of the distributional temporal difference learning (TD) from a non-asymptotic perspective. Specifically, we have considered two instances of distributional TD, namely, the non-parametric distributional TD (NTD) and the categorical distributional TD (CTD). For both NTD and CTD, we have shown that \(\tilde{O}\left(\frac{1}{\varepsilon^{2p}(1-\gamma)^{2p+1}}\right)\) iterations are sufficient to achieve a \(p\)-Wasserstein \(\varepsilon\)-optimal estimator, which is minimax optimal (up to logarithmic factors). We have established a novel Freedman's inequality in Hilbert spaces to prove these theoretical results, which has independent theoretical value beyond the current work. We leave the details to Appendix A.

## Acknowledgments and Disclosure of Funding

This work has been supported by the National Key Research and Development Project of China (No. 2022YFA1004002), the National Natural Science Foundation of China (No. 12271011 and No. 12350001), and the MOE Project of Key Research Institute of Humanities and Social Sciences (No.22JJD110001).

## References

* Bellemare et al. (2017) M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning. In _International conference on machine learning_, pages 449-458. PMLR, 2017.
* Bellemare et al. (2023) M. G. Bellemare, W. Dabney, and M. Rowland. _Distributional Reinforcement Learning_. MIT Press, 2023. http://www.distributional-rl.org.
* Bock and Heitzinger (2022) M. Bock and C. Heitzinger. Speedy categorical distributional reinforcement learning and complexity analysis. _SIAM Journal on Mathematics of Data Science_, 4(2):675-693, 2022. doi: 10.1137/20M1364436. URL https://doi.org/10.1137/20M1364436.
* Bock et al. (2022) M. Bock, J. Malle, D. Pasterk, H. Kukina, R. Hasani, and C. Heitzinger. Superhuman performance on sepsis mimic-iii data by distributional reinforcement learning. _PLoS One_, 17(11):e0275358, 2022.
* Bogachev (2007) V. I. Bogachev. _Measure theory_, volume 1. Springer, 2007.
* Dabney et al. (2018) W. Dabney, M. Rowland, M. Bellemare, and R. Munos. Distributional reinforcement learning with quantile regression. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2018.
* de la Pena (1999) V. H. de la Pena. A general class of exponential inequalities for martingales and ratios. _The Annals of Probability_, 27(1):537-564, 1999.
* Durrett (2019) R. Durrett. _Probability: theory and examples_, volume 49. Cambridge university press, 2019.
* Freedman (1975) D. A. Freedman. On tail probabilities for martingales. _The Annals of Probability_, pages 100-118, 1975.
* Gheshlaghi Azar et al. (2013) M. Gheshlaghi Azar, R. Munos, and H. J. Kappen. Minimax pac bounds on the sample complexity of reinforcement learning with a generative model. _Machine learning_, 91:325-349, 2013.
* Ghysels et al. (2005) E. Ghysels, P. Santa-Clara, and R. Valkanov. There is a risk-return trade-off after all. _Journal of financial economics_, 76(3):509-548, 2005.
* Hunter and Nachtergaele (2001) J. K. Hunter and B. Nachtergaele. _Applied analysis_. World Scientific Publishing Company, 2001.
* Kakade (2003) S. M. Kakade. _On the Sample Complexity of Reinforcement Learning_. PhD thesis, University College London, 2003.
* Kearns et al. (2002) M. Kearns, Y. Mansour, and A. Y. Ng. A sparse sampling algorithm for near-optimal planning in large markov decision processes. _Machine learning_, 49:193-208, 2002.
* Lavori and Dawson (2004) P. W. Lavori and R. Dawson. Dynamic treatment regimes: practical design considerations. _Clinical trials_, 1(1):9-20, 2004.
* Li et al. (2024) G. Li, C. Cai, Y. Chen, Y. Wei, and Y. Chi. Is q-learning minimax optimal? a tight sample complexity analysis. _Operations Research_, 72(1):222-236, 2024.
* Martinez-Taboada and Ramdas (2024) D. Martinez-Taboada and A. Ramdas. Empirical bernstein in smooth banach spaces. _arXiv preprint arXiv:2409.06060_, 2024.
* Morimura et al. (2010) T. Morimura, M. Sugiyama, H. Kashima, H. Hachiya, and T. Tanaka. Nonparametric return distribution approximation for reinforcement learning. In _Proceedings of the 27th International Conference on Machine Learning (ICML-10)_, pages 799-806, 2010.
* Pananjady and Wainwright (2020) A. Pananjady and M. J. Wainwright. Instance-dependent \(\ell_{\infty}\)-bounds for policy evaluation in tabular reinforcement learning. _IEEE Transactions on Information Theory_, 67(1):566-585, 2020.
* Pinelis (1994) I. Pinelis. Optimum bounds for the distributions of martingales in banach spaces. _The Annals of Probability_, pages 1679-1706, 1994.
* Pisier (2016) G. Pisier. _Martingales in Banach Spaces_. Cambridge Studies in Advanced Mathematics. Cambridge University Press, 2016. doi: 10.1017/CBO9781316480588.
* Pogogachev (2017)H. Robbins and S. Monro. A stochastic approximation method. _The Annals of Mathematical Statistics_, pages 400-407, 1951.
* Rowland et al. [2018] M. Rowland, M. Bellemare, W. Dabney, R. Munos, and Y. W. Teh. An analysis of categorical distributional reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 29-37. PMLR, 2018.
* Rowland et al. [2024a] M. Rowland, R. Munos, M. G. Azar, Y. Tang, G. Ostrovski, A. Harutyunyan, K. Tuyls, M. G. Bellemare, and W. Dabney. An analysis of quantile temporal-difference learning. _Journal of Machine Learning Research_, 25(163):1-47, 2024a. URL http://jmlr.org/papers/v25/23-0154.html.
* Rowland et al. [2024b] M. Rowland, L. K. Wenliang, R. Munos, C. Lyle, Y. Tang, and W. Dabney. Near-minimax-optimal distributional reinforcement learning with a generative model. _arXiv preprint arXiv:2402.07598_, 2024b.
* Sutton [1988] R. S. Sutton. Learning to predict by the methods of temporal differences. _Machine learning_, 3:9-44, 1988.
* Talebi et al. [2022] A. Talebi, G. Sadeghi, and M. Moslehian. Freedman inequality in noncommutative probability spaces. _Complex Analysis and Operator Theory_, 16(2):22, 2022.
* Tarres and Yao [2014] P. Tarres and Y. Yao. Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence. _IEEE Transactions on Information Theory_, 60(9):5716-5735, 2014.
* 270, 2011. doi: 10.1214/ECP.v16-1624. URL https://doi.org/10.1214/ECP.v16-1624.
* Villani et al. [2009] C. Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* Wu et al. [2023] R. Wu, M. Uehara, and W. Sun. Distributional offline policy evaluation with predictive error guarantees. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 37685-37712. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/wu23s.html.
* Zhang et al. [2023] L. Zhang, Y. Peng, J. Liang, W. Yang, and Z. Zhang. Estimation and inference in distributional reinforcement learning. _arXiv preprint arXiv:2309.17262_, 2023.

The Key Lemma: Freedman's Inequality in Hilbert Spaces

Freedman's inequality, proposed in [Freedman, 1975], can be viewed as a Bernstein's inequality for martingales, which is crucial for analyzing stochastic approximation algorithms. Compared to the Azuma-Hoeffding inequality which only utilizes the boundedness of martingale difference sequences, Freedman's inequality incorporates second-order information, namely the quadratic variation (cumulative conditional variance) of martingales. This may leads to a sharper concentration result. It has various generalizations, such as matrix Freedman's inequality [Tropp, 2011]. However, to the best of our knowledge, a Freedman's inequality in Hilbert spaces has not been established yet. Just as Freedman's inequality is essential for the theory of TD (Theorem 1 in [Li et al., 2024]), it is indispensable for deriving the minimax non-asymptotic convergence bound for distributional TD.

In this section, we will present a Freedman's inequalities in Hilbert spaces. Firstly, we will state a Hilbert space version of the original Freedman's inequality (Theorem 1.6 in [Freedman, 1975]). After that, we state a generalization of a more powerful version (Theorem 6 in [Li et al., 2024]) to Hilbert spaces. We will provide self-contained proofs in Appendix A.1, primarily inspired by Theorem 3.2 in [Pinelis, 1994]. The necessary knowledge of martingale theory for the proofs can be found in any standard textbook, such as [Durrett, 2019].

Let \(\mathcal{X}\) be a Hilbert space, \(\left\{X_{i}\right\}_{i=1}^{n}\) be an \(\mathcal{X}\)-valued martingale difference sequence adapted to the filtration \(\left\{\mathcal{F}_{i}\right\}_{i=1}^{n}\), \(Y_{i}:=\sum_{j=1}^{i}X_{j}\) be the corresponding martingale, \(W_{i}:=\sum_{j=1}^{i}\sigma_{j}^{2}\) be the corresponding quadratic variation process. Here \(\sigma_{j}^{2}:=\mathbb{E}_{j-1}\left\|X_{j}\right\|^{2}\), and \(\mathbb{E}_{i}\left[\cdot\right]:=\mathbb{E}\left[\cdot|\mathcal{F}_{i}\right]\) is the conditional expectation.

**Theorem A.1** (Freedman's inequality in Hilbert spaces).: _Suppose \(\max_{i\in[n]}\left\|X_{i}\right\|\leq b\) for some constant \(b>0\). Then, for any \(\varepsilon,\sigma>0\), the following inequality holds_

\[\mathbb{P}\left(\exists k\in[n],\text{s.t. }\left\|Y_{k}\right\|\geq \varepsilon\text{ and }W_{k}\leq\sigma^{2}\right)\leq 2\exp\left\{-\frac{ \varepsilon^{2}/2}{\sigma^{2}+b\varepsilon/3}\right\}.\] (4)

Now, we are ready to state the generalization of Theorem 6 in [Li et al., 2024] to Hilbert spaces, which is used in our non-asymptotic analysis.

**Theorem A.2** (Freedman's inequality in Hilbert spaces with bounded quadratic variation).: _Suppose \(\max_{i\in[n]}\left\|X_{i}\right\|\leq b\) and \(W_{n}\leq\sigma^{2}\) for some constants \(b,\sigma>0\) almost surely. Then, for any \(\delta\in(0,1)\), and any positive integer \(H\geq 1\), with probability at least \(1-\delta\), for all \(k\in[n]\), the following inequality holds_

\[\left\|Y_{k}\right\|\leq\sqrt{8\max\left\{W_{k},\frac{\sigma^{2}}{2^{H}} \right\}\log\frac{2H}{\delta}}+\frac{4}{3}b\log\frac{2H}{\delta}.\] (5)

The proof can be found in Appendix A.2.

**Remark 2**:: Theorem 2.1 can be straightforwardly extended to the case where \(\left(\left\|X_{i}\right\|\right)_{i=1}^{n}\) satisfies the Bernstein condition (Theorem 1.2A in [de la Pena, 1999]), thereby relaxing the boundedness assumption on \(\left\|X_{i}\right\|\). Namely, \(\mathbb{E}_{i-1}\left\|X_{i}\right\|^{k}\leq\frac{1}{2}k!\sigma_{i}^{2}b^{k-2}\) for some \(b>0\), and for all \(i\in[n]\), \(k\in\{2,3,\cdots\}\). In this case, Freedman's inequality still holds, albeit with a worse constant.

\[\mathbb{P}\left(\exists k\in[n],\text{s.t. }\left\|Y_{k}\right\|\geq \varepsilon\text{ and }W_{k}\leq\sigma^{2}\right)\leq 2\exp\left\{-\frac{ \varepsilon^{2}/2}{\sigma^{2}+b\varepsilon}\right\}.\] (6)

The proof only requires making appropriate modifications after the fifth line of Equation (12). Note that Bernstein condition holds if \(\max_{i\in[n]}\left\|X_{i}\right\|\leq b\).

### Proof of Theorem 2.1

Proof.: For any \(\lambda>0\), \(t\in[0,1]\) and \(j\in[n]\), let \(\phi(t)=\phi_{j,\lambda}(t):=\mathbb{E}_{j-1}\cosh\left(\lambda\left\|Y_{j-1}+ tX_{j}\right\|\right)=\mathbb{E}_{j-1}\cosh\left(\lambda u(t)\right)\), where \(u(t):=\left\|Y_{j-1}+tX_{j}\right\|\). We aim to use the Newton-Leibniz formula to establish the relationship between \(\phi(1)=\mathbb{E}_{j-1}\cosh\left(\lambda\left\|Y_{j}\right\|\right)\) and \(\phi(0)=\cosh\left(\lambda\left\|Y_{j-1}\right\|\right)\). This will allow us to construct a positive supermartingale \((B_{i})_{i=0}^{n}\). By utilizing the positive supermartingale and optional stopping theorem, we can derive the desired concentration inequality.

Firstly, we calculate the derivative of \(\phi\).

\[u^{\prime}(t)=\frac{\left\langle Y_{j-1}+tX_{j},X_{j}\right\rangle}{u(t)},\] (7)

\[\phi^{\prime}(t) =\lambda\mathbb{E}_{j-1}\left[\sinh\left(\lambda u(t)\right)u^{ \prime}(t)\right]\] (8) \[=\lambda\mathbb{E}_{j-1}\left[\sinh\left(\lambda u(t)\right) \frac{\left\langle Y_{j-1}+tX_{j},X_{j}\right\rangle}{u(t)}\right],\]

\[\phi^{\prime}(0) =\lambda\mathbb{E}_{j-1}\left[\sinh\left(\lambda u(0)\right) \frac{\left\langle Y_{j-1},X_{j}\right\rangle}{u(0)}\right]\] (9) \[=\lambda\sinh\left(\lambda\left\|Y_{j-1}\right\|\right)\frac{ \left\langle Y_{j-1},\mathbb{E}_{j-1}\left[X_{j}\right]\right\rangle}{\left\| Y_{j-1}\right\|}\] \[=0.\]

By utilizing Newton-Leibniz formula, we have

\[\phi(1) =\phi(0)+\int_{0}^{1}\phi^{\prime}(s)ds\] (10) \[=\phi(0)+\int_{0}^{1}\int_{0}^{s}\phi^{\prime\prime}(t)dtds\] \[=\phi(0)+\int_{0}^{1}(1-t)\phi^{\prime\prime}(t)dt.\]

Now, we calculate the second order derivate of \(\phi\).

\[\phi^{\prime\prime}(t) =\lambda\mathbb{E}_{j-1}\left\{\frac{d}{dt}\left[\sinh\left( \lambda u(t)\right)u^{\prime}(t)\right]\right\}\] (11) \[=\lambda\mathbb{E}_{j-1}\left[\lambda\left(u^{\prime}(t)\right)^ {2}\cosh\left(\lambda u(t)\right)+u^{\prime\prime}(t)\sinh\left(\lambda u(t) \right)\right]\] \[\leq\lambda^{2}\mathbb{E}_{j-1}\left[\left(\left(u^{\prime}(t) \right)^{2}+u^{\prime\prime}(t)u(t)\right)\cosh\left(\lambda u(t)\right)\right]\] \[=\frac{\lambda^{2}}{2}\mathbb{E}_{j-1}\left[\left(u^{2}\right)^{ \prime\prime}(t)\cosh\left(\lambda u(t)\right)\right]\] \[=\lambda^{2}\mathbb{E}_{j-1}\left[\left\|X_{j}\right\|^{2}\cosh \left(\lambda\left\|Y_{j-1}+tX_{j}\right\|\right)\right]\] \[\leq\lambda^{2}\cosh\left(\lambda\left\|Y_{j-1}\right\|\right) \mathbb{E}_{j-1}\left[\left\|X_{j}\right\|^{2}\exp\left(\lambda t\left\|X_{j} \right\|\right)\right],\]

where in the third line, we used \(u^{\prime\prime}(t)=\frac{\left\|X_{j}\right\|^{2}u(t)-\frac{\left\langle Y_{ j-1}+tX_{j},X_{j}\right\rangle^{2}}{u^{2}(t)}}{\geq 0}\) by Cauchy-Schwarz inequality, and \(h(x)=x\cosh(x)-\sinh(x)\geq 0\) for any \(x\geq 0\), the inequality holds because \(h(0)=0\) and \(h^{\prime}(x)=x\sinh(x)\geq 0\) for any \(x\geq 0\). In the fourth line, we used \(\left(u^{2}\right)^{\prime\prime}(t)=2\left(\left(u^{\prime}(t)\right)^{2}+u^ {\prime\prime}(t)u(t)\right)\). In the fifth line, we used

\[\left(u^{2}\right)^{\prime\prime}(t)=\frac{d^{2}}{dt^{2}}\left\|Y_{j-1}+tX_{j }\right\|^{2}=\frac{d}{dt}\left(2\left\langle Y_{j-1}+tX_{j},X_{j}\right\rangle \right)=2\left\|X_{j}\right\|^{2}.\]

And in the last line, we used

\[\cosh\left(\lambda\left\|Y_{j-1}+tX_{j}\right\|\right)\leq\cosh\left(\lambda \left\|Y_{j-1}\right\|\right)\exp\left(\lambda t\left\|X_{j}\right\|\right),\]

this holds since

\[\begin{array}{l}\exp\left(\lambda\left\|Y_{j-1}+tX_{j}\right\|\right)\leq \exp\left\{\lambda\left(\left\|Y_{j-1}\right\|+t\left\|X_{j}\right\|\right) \right\}=\exp\left(\lambda\left\|Y_{j-1}\right\|\right)\exp\left(\lambda t \left\|X_{j}\right\|\right),\\ \exp\left(-\lambda\left\|Y_{j-1}+tX_{j}\right\|\right)\leq\exp\left\{-\lambda \left(\left\|Y_{j-1}\right\|-t\left\|-X_{j}\right\|\right)\right\}=\exp\left( -\lambda\left\|Y_{j-1}\right\|\right)\exp\left(\lambda t\left\|X_{j}\right\| \right).\end{array}\]Hence, we can derive the following inequality for all \(j\in[n]\)

\[\mathbb{E}_{j-1}\left[\cosh\left(\lambda\left\|Y_{j}\right\|\right) \right]=\phi(1)=\phi(0)+\int_{0}^{1}(1-t)\phi^{\prime\prime}(t)dt\] (12) \[\leq \cosh\left(\lambda\left\|Y_{j-1}\right\|\right)+\lambda^{2}\cosh \left(\lambda\left\|Y_{j-1}\right\|\right)\mathbb{E}_{j-1}\left[\left\|X_{j} \right\|^{2}\int_{0}^{1}(1-t)\exp\left(\lambda t\left\|X_{j}\right\|\right)dt\right]\] \[= \cosh\left(\lambda\left\|Y_{j-1}\right\|\right)+\lambda^{2}\cosh \left(\lambda\left\|Y_{j-1}\right\|\right)\mathbb{E}_{j-1}\left[\left\|X_{j} \right\|^{2}\frac{\exp\left(\lambda\left\|X_{j}\right\|\right)-\lambda\left\| X_{j}\right\|-1}{\lambda^{2}\left\|X_{j}\right\|^{2}}\right]\] \[= \mathbb{E}_{j-1}\left[\exp\left(\lambda\left\|X_{j}\right\| \right)-\lambda\left\|X_{j}\right\|\right]\cosh\left(\lambda\left\|Y_{j-1}\right\|\right)\] \[= \mathbb{E}_{j-1}\left[1+\sum_{k=0}^{\infty}\frac{1}{(k+2)!}\left( \lambda\left\|X_{j}\right\|\right)^{k+2}\right]\cosh\left(\lambda\left\|Y_{j-1 }\right\|\right)\] \[\leq \mathbb{E}_{j-1}\left[1+\frac{\lambda^{2}\left\|X_{j}\right\|^{2 }}{2}\sum_{k=0}^{\infty}\left(\frac{\lambda b}{3}\right)^{k}\right]\cosh \left(\lambda\left\|Y_{j-1}\right\|\right)\] \[= \left(1+\frac{\lambda^{2}\sigma_{j}^{2}}{2(1-\lambda b/3)}\right) \cosh\left(\lambda\left\|Y_{j-1}\right\|\right)\] \[\leq \exp\left\{\frac{\lambda^{2}\sigma_{j}^{2}}{2(1-\lambda b/3)} \right\}\cosh\left(\lambda\left\|Y_{j-1}\right\|\right),\]

which holds for any \(\lambda\in(0,\frac{3}{b})\). In the fifth line, we used Taylor expansion \(e^{x}=\sum_{k=0}^{\infty}\frac{x^{k}}{k!}\). In the sixth line, we used \((k+2)!\geq 2(3^{k})\) and \(\left\|X_{j}\right\|\leq b\). In the seventh line, we used Taylor expansion \(\frac{1}{1-x}=\sum_{k=0}^{\infty}x^{k}\) for \(x\in(-1,1)\).

Let \(B_{0}:=1\), \(B_{i}:=\exp\left\{-\frac{\lambda^{2}W_{i}}{2(1-\lambda b/3)}\right\}\cosh \left(\lambda\left\|Y_{i}\right\|\right)\), then

\[\mathbb{E}_{i-1}\left[B_{i}\right] =\exp\left\{-\frac{\lambda^{2}W_{i-1}}{2(1-\lambda b/3)}\right\} \exp\left\{-\frac{\lambda^{2}\sigma_{i}^{2}}{2(1-\lambda b/3)}\right\}\mathbb{ E}_{i-1}\left[\cosh\left(\lambda\left\|Y_{i}\right\|\right)\right]\] (13) \[\leq\exp\left\{-\frac{\lambda^{2}W_{i-1}}{2(1-\lambda b/3)}\right\} \cosh\left(\lambda\left\|Y_{i-1}\right\|\right)\] \[=B_{i-1},\]

_i.e._, \(\left(B_{i}\right)_{i=0}^{n}\) is positive supermartingale. By optional stopping theorem (Theorem 4.8.4 in [Durrett, 2019]), for any stopping time \(\tau\), we have \(\mathbb{E}\left[B_{\tau}\right]\leq\mathbb{E}\left[B_{0}\right]=1\).

Let \(\tau:=\inf\left\{k\in[n]:\left\|Y_{k}\right\|\geq\varepsilon\right\}\) be a stopping time, and \(\inf\emptyset:=\infty\). Define an event

\[A:=\left\{\exists k\in[n],\text{s.t. }\left\|Y_{k}\right\|\geq\varepsilon \text{ and }W_{k}\leq\sigma^{2}\right\},\] (14)then on \(A\), we have \(\tau<\infty\), \(\|Y_{\tau}\|\geq\varepsilon\) and \(W_{\tau}\leq\sigma^{2}\), noting that \(W_{k}\) is non-decreasing with \(k\). Our goal is to provide an upper bound for \(\mathbb{P}(A)\).

\[\mathbb{P}(A) =\mathbb{E}\left[\sqrt{B_{\tau}}\frac{1}{\sqrt{B_{\tau}}}\mathds{1 }(A)\right]\] (15) \[\leq\sqrt{\mathbb{E}\left[\frac{\exp\left\{\frac{\lambda^{2}W_{ \tau}}{2(1-\lambda b/3)}\right\}}{\cosh\left(\lambda\left\|Y_{\tau}\right\| \right)}\mathds{1}(A)\right]}\] \[\leq\sqrt{\mathbb{E}\left[\frac{\exp\left\{\frac{\lambda^{2} \sigma^{2}}{2(1-\lambda b/3)}\right\}}{\cosh\left(\lambda\varepsilon\right)} \mathds{1}(A)\right]}\] \[\leq\sqrt{2\exp\left\{-\lambda\varepsilon+\frac{\lambda^{2} \sigma^{2}}{2(1-\lambda b/3)}\right\}\mathbb{P}(A)},\]

where in the second line, we used Cauchy-Schwarz inequality. In the third line, we used \(\mathbb{E}\left[B_{\tau}\right]\leq 1\). In the fourth line, we used \(\|Y_{\tau}\|\geq\varepsilon\) and \(W_{\tau}\leq\sigma^{2}\) on \(A\), and \(\cosh(x)\) is increasing when \(x\geq 0\). In the last line, we used \(\cosh(x)\geq\frac{1}{2}e^{x}\).

Hence for any \(\lambda\in(0,\frac{3}{b})\)

\[\mathbb{P}(A)\leq 2\exp\left\{-\lambda\varepsilon+\frac{\lambda^{2}\sigma^{2}} {2\left(1-\lambda b/3\right)}\right\},\] (16)

we can choose \(\lambda^{\star}=\frac{\varepsilon}{\sigma^{2}+\varepsilon b/3}\in(0,\frac{3}{b})\), then

\[\mathbb{P}(A) \leq 2\exp\left\{-\lambda^{\star}\varepsilon+\frac{\left( \lambda^{\star}\right)^{2}\sigma^{2}}{2\left(1-\lambda^{\star}b/3\right)}\right\}\] (17) \[=2\exp\left\{-\frac{\varepsilon^{2}}{\sigma^{2}+\varepsilon b/3} +\frac{\sigma^{2}}{2\left(1-\frac{\varepsilon b/3}{\sigma^{2}+\varepsilon b/ 3}\right)}\frac{\varepsilon^{2}}{\left(\sigma^{2}+\varepsilon b/3\right)^{2}}\right\}\] \[=2\exp\left\{-\frac{\varepsilon^{2}/2}{\sigma^{2}+\varepsilon b/ 3}\right\},\]

which is the desired conclusion. 

### Proof of Theorem a.2

Proof.: According to Theorem 2.1, for any \(\varepsilon,\tilde{\sigma}>0\), we have

\[\mathbb{P}\big{(}\exists k\in[n],\;\|Y_{k}\|\geq\varepsilon\text{ and }W_{k}\leq\tilde{\sigma}^{2}\big{)}\leq 2\exp\left\{-\frac{ \varepsilon^{2}/2}{\tilde{\sigma}^{2}+b\varepsilon/3}\right\}.\] (18)

We can check that when \(\varepsilon=\sqrt{4\tilde{\sigma}^{2}\log\frac{2}{\delta}}+\frac{4}{3}b\log \frac{2}{\delta}\), the upper bound on RHS is less than \(\delta\). Hence,

\[\mathbb{P}\left(\exists k\in[n],\;\|Y_{k}\|\geq\sqrt{4\tilde{\sigma}^{2}\log \frac{2}{\delta}}+\frac{4}{3}b\log\frac{2}{\delta}\text{ and }W_{k}\leq\tilde{\sigma}^{2}\right)\leq\delta.\] (19)For each \(k\in[n]\), define the events

\[\mathcal{H}_{H}^{(k)} :=\left\{\|Y_{k}\|\geq\sqrt{8\max\left\{W_{k},\frac{\sigma^{2}}{2^{H }}\right\}\log\frac{2H}{\delta}}+\frac{4}{3}b\log\frac{2H}{\delta}\right\},\] (20) \[\mathcal{B}_{H,H}^{(k)} :=\left\{\|Y_{k}\|\geq\sqrt{4\frac{\sigma^{2}}{2^{H-1}}\log\frac{ 2H}{\delta}}+\frac{4}{3}b\log\frac{2H}{\delta}\text{ and }W_{k}\leq\frac{\sigma^{2}}{2^{H-1}}\right\},\] \[\mathcal{B}_{h,H}^{(k)} :=\left\{\|Y_{k}\|\geq\sqrt{4\frac{\sigma^{2}}{2^{h-1}}\log\frac{ 2H}{\delta}}+\frac{4}{3}b\log\frac{2H}{\delta}\text{ and }\frac{\sigma^{2}}{2^{h}}\leq W_{k}\leq \frac{\sigma^{2}}{2^{h-1}}\right\},\quad 1\leq h\leq H-1.\]

By the definition, we only need to show \(\mathbb{P}\left(\bigcup_{k\in[n]}\mathcal{H}_{H}^{(k)}\right)\leq\delta\). Since \(W_{k}\leq W_{n}\leq\sigma^{2}\) almost surely, we can find that \(\mathcal{H}_{H}^{(k)}\subseteq\bigcup_{h\in[H]}\mathcal{B}_{h,H}^{(k)}\) (we will justify this later). Then \(\bigcup_{k\in[n]}\mathcal{H}_{H}^{(k)}\subseteq\bigcup_{h\in[H]}\bigcup_{k \in[n]}\mathcal{B}_{h,H}^{(k)}\). By the inequality (19) with \(\tilde{\sigma}^{2}=\frac{\sigma^{2}}{2^{h-1}}\) and setting \(\delta\) as \(\frac{\delta}{H}\), we have \(\mathbb{P}\left(\bigcup_{k\in[n]}\mathcal{B}_{h,H}^{(k)}\right)\leq\frac{ \delta}{H}\) for all \(h\in[H]\). By the union bound, we can arrive at the conclusion:

\[\mathbb{P}\left(\bigcup_{k\in[n]}\mathcal{H}_{H}^{(k)}\right)\leq\sum_{h=1}^{ H}\mathbb{P}\left(\bigcup_{k\in[n]}\mathcal{B}_{h,H}^{(k)}\right)\leq\delta.\] (21)

To justify \(\mathcal{H}_{H}^{(k)}\subseteq\bigcup_{h\in[H]}\mathcal{B}_{h,H}^{(k)}\), we can consider the decomposition

\[\mathcal{H}_{H}^{(k)}=\bigcup_{h\in[H]}\left(\mathcal{H}_{H}^{(k)}\cap \mathcal{C}_{h,H}^{(k)}\right),\] (22)

where

\[\mathcal{C}_{H,H}^{(k)}:=\left\{W_{k}\leq\frac{\sigma^{2}}{2^{H-1}}\right\}, \quad\mathcal{C}_{h,H}^{(k)}:=\left\{\frac{\sigma^{2}}{2^{h}}\leq W_{k}\leq \frac{\sigma^{2}}{2^{h-1}}\right\},\quad 1\leq h\leq H-1.\] (23)

The decomposition holds because \(W_{k}\leq W_{n}\leq\sigma^{2}\) almost surely. We only need to show that for each \(h\in[H]\),

\[\mathcal{H}_{H}^{(k)}\cap\mathcal{C}_{h,H}^{(k)}\subseteq\mathcal{B}_{h,H}^{( k)}.\] (24)

On the event \(\mathcal{H}_{H}^{(k)}\cap\mathcal{C}_{h,H}^{(k)}\), we have

\[\|Y_{k}\| \geq\sqrt{8\max\left\{W_{k},\frac{\sigma^{2}}{2^{H}}\right\}\log \frac{2H}{\delta}}+\frac{4}{3}b\log\frac{2H}{\delta}\] (25) \[\geq\sqrt{4\frac{\sigma^{2}}{2^{h-1}}\log\frac{2H}{\delta}}+\frac {4}{3}b\log\frac{2H}{\delta},\]

hence \(\mathcal{H}_{H}^{(k)}\cap\mathcal{C}_{h,H}^{(k)}\subseteq\mathcal{B}_{h,H}^{( k)}\). 

## Appendix B Minimax Lower Bound of Distributional Policy Evaluation

In this section, we still consider infinite-horizon tabular MDP defined in Section 2, and assume a generative model is accessible. For any positive integer \(D\), we define \(\mathfrak{M}\left(D\right)\) as the set of all MDPs with state space size \(|\mathcal{S}|=D\). For any MDP \(M\) and policy \(\pi\), we denote \(V_{M}^{\pi}\) as the corresponding value function, and \(\eta_{M}^{\pi}\) as the corresponding return distribution.

Now, we can state the minimax lower bound of the distributional policy evaluation task in the \(1\)-Wasserstein metric.

**Theorem B.1** (Minimax lower bound of distributional policy evaluation in the \(1\)-Wasserstein metric).: _For any positive integer \(D\geq 3\), and sample size \(T\geq\frac{C}{1-\gamma}\log\frac{D}{2}\), the following result holds_

\[\inf_{\tilde{\eta}}\sup_{M\in\mathfrak{M}\left(D\right)}\,\sup_{\pi}\mathbb{E} \left[\bar{W}_{1}\left(\tilde{\eta},\eta_{M}^{\pi}\right)\right]\geq\frac{c}{ (1-\gamma)^{3/2}}\sqrt{\frac{\log\frac{D}{2}}{T}}.\]

[MISSING_PAGE_FAIL:18]

### Range of Step Size

Proof of Lemma 5.1.: \[(1-\sqrt{\gamma})\alpha_{t}\geq\frac{1-\sqrt{\gamma}}{1+\frac{c_{5}(1-\sqrt{ \gamma})T}{\log^{2}T}}\geq\frac{1-\sqrt{\gamma}}{\frac{2c_{5}(1-\sqrt{\gamma})T }{\log^{2}T}}=\frac{\log^{2}T}{2c_{5}T}.\] (33)

For any \(0\leq k\leq\frac{t}{2}\),

\[\begin{split}\beta_{k}^{(t)}&\leq\left[1-\alpha_{t /2}(1-\sqrt{\gamma})\right]^{t/2}\\ &\leq\left(1-\frac{\log^{2}T}{2c_{5}T}\right)^{t/2}\\ &\leq\left(1-\frac{\log^{2}T}{2c_{5}T}\right)^{\frac{T}{2c_{5}T} }\\ &=\left\{\left(1-\frac{\log^{2}T}{2c_{5}T}\right)^{\frac{2c_{5}T} {\log^{2}T}}\right\}^{\frac{\log T}{4c_{5}c_{6}}}\\ &\leq\frac{1}{T^{2}},\end{split}\] (34)

where in the last inequality, we used \(c_{5}c_{6}\leq\frac{1}{8}\).

And for any \(\frac{t}{2}<k\leq t\),

\[\beta_{k}^{(t)}\leq\alpha_{k}\leq\frac{1}{\frac{c_{6}(1-\sqrt{\gamma})k}{\log ^{2}T}}\leq\frac{2\log^{3}T}{(1-\sqrt{\gamma})T}.\] (35)

### Concentration of the Martingale Term

Proof of Lemma 5.2.: We will show that the inequality holds for each \(t\geq\frac{T}{c_{6}\log T}\) and then apply the union bound. For any \(s\in\mathcal{S}\), we denote

\[\zeta_{k}(s):=\zeta_{k}^{(t)}(s)=\alpha_{k}\left\{\prod_{i=k+1}^{t}\left[(1- \alpha_{i})\mathcal{I}+\alpha_{i}\mathcal{T}\right]\left(\mathcal{T}_{k}- \mathcal{T}\right)\eta_{k-1}\right\}(s),\] (36)

where we omit the superscript \((t)\) for brevity, then LHS in the lemma equals \(\left\|\sum_{k=1}^{t}\zeta_{k}\right\|\) for each \(t\). Let \(\mathcal{F}_{k}\) denote the \(\sigma\)-field that contains all information up to time step \(k\), then \(\left\{\zeta_{k}(s)\right\}_{k=1}^{t}\) is a \(\left\{\mathcal{F}_{k}\right\}_{k=1}^{t}\)-martingale difference sequence:

\[\mathbb{E}_{k-1}\left[\zeta_{k}(s)\right]=\alpha_{k}\left\{\prod_{i=k+1}^{t} \left[(1-\alpha_{i})\mathcal{I}+\alpha_{i}\mathcal{T}\right]\mathbb{E}_{k-1} \left[(\mathcal{T}_{k}-\mathcal{T})\,\eta_{k-1}\right]\right\}(s)=0.\] (37)

the first equality holds because a Bochner integral can be exchanged with a bounded linear operator (see Pisier (2016) for more details about Bochner integral), and the second equality holds due to the definition of the empirical distributional Bellman operator.

We hope to use Freedman's inequality (Theorem A.2) to bound this martingale. To this end, we need to give a deterministic upper bound of the martingale difference sequence, and an upper bound of its quadratic variation.

Deterministic upper bound of \(\max_{k\in[t]}\|\zeta_{k}(s)\|\).The norm of the martingale difference \(\|\zeta_{k}(s)\|\) can be bounded as follow

\[\begin{split}\|\zeta_{k}(s)\|&\leq\|\zeta_{k}\|\\ &\leq\alpha_{k}\left\|\prod_{i=k+1}^{t}\left[(1-\alpha_{i}) \mathcal{I}+\alpha_{i}\mathcal{T}\right]\right\|\left\|\left(\mathcal{T}_{k}- \mathcal{T}\right)\eta_{k-1}\right\|\\ &\leq\alpha_{k}\prod_{i=k+1}^{t}\left((1-\alpha_{i})+\alpha_{i} \sqrt{\gamma}\right)\frac{1}{\sqrt{1-\gamma}}\\ &=\frac{\beta_{k}^{(t)}}{\sqrt{1-\gamma}}.\end{split}\] (38)

Hence, \(\max_{k\in[t]}\|\zeta_{k}(s)\|\leq\frac{\max_{k\in[t]}\beta_{k}^{(t)}}{\sqrt{1 -\gamma}}\leq\frac{1}{\sqrt{1-\gamma}}\max\left\{\frac{1}{T^{2}},\frac{2\log ^{3}T}{(1-\sqrt{\gamma})T}\right\}\leq\frac{4\log^{3}T}{(1-\gamma)^{3/2T}}=:b\).

Upper bound of quadratic variation.Now, let's calculate the quadratic variation.

We first introduce some notations. For any \(k\in\mathbb{N}\), we denote \(\mathsf{Var}(\bm{\xi}):=\left(\mathbb{E}\left[\left\|\bm{\xi}(s)\right\|^{2} \right]\right)_{s\in\mathcal{S}}\in\mathbb{R}^{\mathcal{S}}\), \(\mathsf{Var}_{k}(\bm{\xi}):=\left(\mathbb{E}_{k}\left[\left\|\bm{\xi}(s) \right\|^{2}\right]\right)_{s\in\mathcal{S}}\in\mathbb{R}^{\mathcal{S}}\) for any random element \(\bm{\xi}\) in \(\mathcal{M}^{\mathcal{S}}\).

For any \(\xi\in\mathcal{M}^{\mathcal{S}}\), we define its one-step update Cramer variation as \(\bm{\sigma}(\xi):=\mathsf{Var}\left((\widehat{\mathcal{T}}-\mathcal{T})\xi \right)\in\mathbb{R}^{\mathcal{S}}\), where \(\widehat{\mathcal{T}}\) is a random operator and has the same distribution as \(\mathcal{T}_{1}\).

For any \(\bm{x},\bm{y}\in\mathbb{R}^{\mathcal{S}}\), we say \(\bm{x}\leq\bm{y}\) if \(\bm{x}(s)\leq\bm{y}(s)\) for all \(s\in\mathcal{S}\). In this part, \(\left\|\bm{x}\right\|:=\left\|\bm{x}\right\|_{\infty}=\max_{s\in\mathcal{S}} \left|\bm{x}(s)\right|\), \(\sqrt{\bm{x}}:=\left(\sqrt{\overline{\bm{x}(s)}}\right)_{s\in\mathcal{S}}\). And for any \(\bm{U}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}\), \(\left\|\bm{U}\right\|:=\left\|\bm{U}\right\|_{\infty}=\sup_{\bm{x}\in\mathbb{R }^{\mathcal{S}},\left\|\bm{x}\right\|=1}\left\|\bm{U}\bm{x}\right\|=\max_{s\in \mathcal{S}}\sum_{s^{\prime}\in\mathcal{S}}\left|\bm{U}(s,s^{\prime})\right|\).

For any \(\left\{\bm{x}_{k}\right\}_{k=1}^{n}\subset\mathbb{R}^{\mathcal{S}}\), we denote \(\max_{k\in[n]}\bm{x}_{k}\) as \(\left(\max_{k\in[n]}\bm{x}_{k}(s)\right)_{s\in\mathcal{S}}\).

We denote \(\bm{I}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}\) as the identity matrix, \(\bm{1}\in\mathbb{R}^{\mathcal{S}}\) as the all-ones vector, and \(\bm{P}:=P^{\pi}\in\mathbb{R}^{\mathcal{S}\times\mathcal{S}}\), _i.e._, \(\bm{P}(s,s^{\prime}):=P^{\pi}(s^{\prime}|s)=\sum_{a\in\mathcal{A}}\pi(a|s)P(s^ {\prime}|s,a)\).

With these notations, the quadratic variation is \(\bm{W}_{t}:=\sum_{k=1}^{t}\mathsf{Var}_{k-1}\left(\zeta_{k}\right)\). To bound the quadratic variation \(\bm{W}_{t}\), we need to bound \(\mathsf{Var}_{k-1}\left(\zeta_{k}\right)\).

**Lemma C.1**.: \[\mathsf{Var}_{k-1}\left(\zeta_{k}\right)\leq\alpha_{k}\beta_{k}^{(t)}\prod_{i= k+1}^{t}\left[(1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\bm{ \sigma}(\eta_{k-1}).\]Hence, the quadratic variation \(\bm{W}_{t}\) can be bounded as follow

\[\bm{W}_{t} =\sum_{k=1}^{t}\bm{\mathsf{Var}}_{t-1}\left(\zeta_{k}\right)\] \[\leq\sum_{k=1}^{t}\alpha_{k}\beta_{k}^{(t)}\prod_{i=k+1}^{t}\left[ (1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\bm{\sigma}(\eta_{k-1})\] \[\leq\sum_{k=1}^{t/2}\alpha_{k}\beta_{k}^{(t)}\left\|\prod_{i=k+1} ^{t}\left[(1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\right\|\bm {\sigma}(\eta_{k-1})\|\bm{1}+\sum_{k=t/2+1}^{t}\alpha_{k}\beta_{k}^{(t)}\prod _{i=k+1}^{t}\left[(1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right] \bm{\sigma}(\eta_{k-1})\] \[\leq\sum_{k=1}^{t/2}\left(\beta_{k}^{(t)}\right)^{2}\frac{1}{1- \gamma}\bm{1}+\left(\max_{k:\,t/2<k\leq t}\beta_{k}^{(t)}\right)\sum_{k=t/2+1} ^{t}\alpha_{k}\prod_{i=k+1}^{t}\left[(1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{ \gamma}\bm{P}\right]\bm{\sigma}(\eta_{k-1})\] \[\leq\frac{1}{2(1-\gamma)T^{3}}\bm{1}+\frac{4\log^{3}T}{(1-\gamma )T}(\bm{I}-\sqrt{\gamma}\bm{P})^{-1}\max_{k:\,t/2<k\leq t}\bm{\sigma}(\eta_{k -1}),\] (39)

where in the fourth line, we used

\[\alpha_{k}\left\|\prod_{i=k+1}^{t}\left[(1-\alpha_{i})\bm{I}+ \alpha_{i}\sqrt{\gamma}\bm{P}\right]\right\|\leq\alpha_{k}\prod_{i=k+1}^{t} \left[(1-\alpha_{i})+\alpha_{i}\sqrt{\gamma}\right]=\beta_{k}^{(t)},\]

and

\[\|\bm{\sigma}(\eta_{k-1})\|\leq\int_{0}^{\frac{1}{1-\gamma}}dx= \frac{1}{1-\gamma}.\]

In the last line, we used the fact that \(\max_{k:\,t/2\leq k<t}\bm{\sigma}(\eta_{k-1})\geq\bm{0}\) and the following lemma:

**Lemma C.2**.: _For any \(t\in\mathbb{N}\), \(\left(\alpha_{i}\right)_{i\in[t]}\in[0,1]^{t}\), the following inequality holds entry-wise:_

\[\sum_{k=t/2+1}^{t}\alpha_{k}\prod_{i=k+1}^{t}\left[\bm{I}-\alpha_ {i}\left(\bm{I}-\sqrt{\gamma}\bm{P}\right)\right]\leq(\bm{I}-\sqrt{\gamma}\bm{ P})^{-1}.\] (40)

According to (39), we have the following deterministic upper bound for \(\|\bm{W}_{t}\|=\max_{s\in\mathcal{S}}\bm{W}_{t}(s)\),

\[\|\bm{W}_{t}\| \leq\frac{1}{2(1-\gamma)T^{3}}+\frac{4\log^{3}T}{(1-\gamma)T}\left\| (\bm{I}-\sqrt{\gamma}\bm{P})^{-1}\right\|_{k:\,t/2<k<\leq t}\|\bm{\sigma}(\eta _{k-1})\|\] \[\leq\frac{1}{2(1-\gamma)T^{3}}+\frac{8\log^{3}T}{(1-\gamma)^{3}T}\] (41) \[\leq\frac{9\log^{3}T}{(1-\gamma)^{3}T}\] \[=:\sigma^{2}.\]

Let \(H=\left\lceil 2\log_{2}\frac{1}{1-\gamma}\right\rceil\), we have

\[\frac{\sigma^{2}}{2^{H}}\leq\frac{9\log^{3}T}{(1-\gamma)T}.\] (42)By applying Freedman's inequality (Theorem A.2) and utilizing the union bound over \(s\in\mathcal{S}\), we obtain with probability at least \(1-\delta\), for all \(t\in[T]\) and \(s\in\mathcal{S}\)

\[\left(\left\|\sum_{k=1}^{t}\zeta_{k}(s)\right\|\right)_{s\in \mathcal{S}}\] (43) \[\leq \sqrt{8\left(\bm{W}_{t}+\frac{\sigma^{2}}{2^{H}}\bm{1}\right) \log\frac{8|\mathcal{S}|T\log\frac{1}{1-\gamma}}{\delta}}+\frac{4}{3}b\log \frac{8|\mathcal{S}|T\log\frac{1}{1-\gamma}}{\delta}\bm{1}\] \[\leq \sqrt{16\left(\bm{W}_{t}+\frac{9\log^{3}T}{(1-\gamma)T}\bm{1} \right)\log\frac{|\mathcal{S}|T}{\delta}}+3b\log\frac{|\mathcal{S}|T}{\delta} \bm{1}\] \[\leq 8\sqrt{\frac{(\log^{3}T)\left(\log\frac{|\mathcal{S}|T}{\delta} \right)}{(1-\gamma)T}}\left[(\bm{I}-\sqrt{\gamma}\bm{P})^{-1}\max_{k:\,t/2<k \leq t}\bm{\sigma}(\eta_{k-1})+3\cdot\bm{1}\right]+\frac{12\left(\log^{3}T \right)\left(\log\frac{|\mathcal{S}|T}{\delta}\right)}{(1-\gamma)^{3/2}T}\bm{ 1},\]

where we used \(\log\frac{8|\mathcal{S}|T\log\frac{1}{1-\gamma}}{\delta}\leq 2\log\frac{| \mathcal{S}|T}{\delta}\) in the second line, which holds due to the choice of \(T\). The following lemmas are required for deriving the upper bound, which hold for both cases of NTD and CTD.

**Lemma C.3**.: _For any \(t\in[T]\),_

\[\bm{\sigma}(\eta_{t})-\bm{\sigma}(\eta)\leq 4\left\|\Delta_{t}\right\|_{W_{1} }\bm{1}.\]

**Lemma C.4**.: \[(\bm{I}-\sqrt{\gamma}\bm{P})^{-1}\bm{\sigma}(\eta)\leq\frac{4}{1-\gamma}\bm{ 1}.\]

Combining the upper bound with the two lemmas, we get the desired conclusion

\[\left(\left\|\sum_{k=1}^{t}\zeta_{k}(s)\right\|\right)_{s\in \mathcal{S}}\] (44) \[\leq 8\sqrt{\frac{\left(\log^{3}T\right)\left(\log\frac{|\mathcal{S} |T}{\delta}\right)}{(1-\gamma)T}}\left[4\max_{k:\,t/2<k\leq t}\left\|\Delta_{k -1}\right\|_{W_{1}}(\bm{I}-\sqrt{\gamma}\bm{P})^{-1}\bm{1}+\frac{8}{1-\gamma} \bm{1}\right]+\frac{12\left(\log^{3}T\right)\left(\log\frac{|\mathcal{S}|T}{ \delta}\right)}{(1-\gamma)^{3/2}T}\bm{1}\] \[\leq 22\sqrt{\frac{\left(\log^{3}T\right)\left(\log\frac{|\mathcal{S} |T}{\delta}\right)}{(1-\gamma)^{2}T}}\left(1+\max_{k:\,t/2<k\leq t}\left\| \Delta_{k-1}\right\|_{W_{1}}\right)\bm{1}+\frac{12\left(\log^{3}T\right) \left(\log\frac{|\mathcal{S}|T}{\delta}\right)}{(1-\gamma)^{3/2}T}\bm{1}\] \[\leq 34\sqrt{\frac{\left(\log^{3}T\right)\left(\log\frac{|\mathcal{S} |T}{\delta}\right)}{(1-\gamma)^{2}T}}\left(1+\max_{k:\,t/2<k\leq t}\left\| \Delta_{k-1}\right\|_{W_{1}}\right)\bm{1},\]

where in the last line, we used that, excluding the constant term, the first term is larger than the second term, given the choice of \(T\geq\frac{C_{4}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{3}}\log\frac{|\mathcal{S }|T}{\delta}\).

### Solve the Recurrence Relation

**Theorem C.1**.: _Suppose for all \(t\geq\frac{T}{\varepsilon_{6}\log T}\),_

\[\left\|\Delta_{t}\right\|_{W_{1}}\leq 35\sqrt{\frac{\left(\log^{3}T\right) \left(\log\frac{|\mathcal{S}|T}{\delta}\right)}{(1-\gamma)^{3}T}}\left(1+\max_ {k:\,t/2<k\leq t}\left\|\Delta_{k-1}\right\|_{W_{1}}\right)}.\]

_Then there exists some large universal constant \(C_{7}>0\), such that_

\[\left\|\Delta_{T}\right\|_{W_{1}}\leq C_{7}\left(\sqrt{\frac{\left(\log^{3}T \right)\left(\log\frac{|\mathcal{S}|T}{\delta}\right)}{(1-\gamma)^{3}T}}+ \frac{\left(\log^{3}T\right)\left(\log\frac{|\mathcal{S}|T}{\delta}\right)}{(1 -\gamma)^{3}T}\right).\]Proof.: For any \(k\geq 0\), we denote

\[u_{k}:=\max\left\{\left\|\Delta_{t}\right\|_{\tilde{W}_{1}}\,\left|\,2^{k}\frac{T }{c_{6}\log T}\leq t\leq T\right\},\] (45)

for \(0\leq k\leq\log_{2}\left(c_{6}\log T\right)\). We can see that \(\left\|\Delta_{T}\right\|_{\tilde{W}_{1}}\leq u_{k}\) for any valid \(k\). Hence, it suffices to show the upper bound holds for \(u_{k}\) for any valid \(k\). It can be verified that \(u_{0}\leq\frac{1}{1-\gamma}\), and for \(k\geq 0\)

\[u_{k+1}\leq 35\sqrt{\frac{\left(\log^{3}T\right)\left(\log\frac{\left|\mathcal{ S}\right|T}{\delta}\right)}{(1-\gamma)^{3}T}\,(1+u_{k})}.\] (46)

We first show that once \(u_{k}\leq 1\), the subsequent values of \(u_{k+l}\) will also remain upper bounded by \(1\). Namely, if \(u_{k}\leq 1\) for some \(k\geq 1\), then

\[u_{k+1}\leq 35\sqrt{\frac{2\left(\log^{3}T\right)\left(\log\frac{\left| \mathcal{S}\right|T}{\delta}\right)}{(1-\gamma)^{3}T}}\leq 1,\] (47)

if \(T\geq\frac{2450\log^{3}T\log\frac{\left|\mathcal{S}\right|T}{\delta}}{(1- \gamma)^{3}}\).

Let \(\tau:=\inf\left\{k:u_{k}\leq 1\right\}\), then for any \(k>\tau\), we have

\[u_{k}\leq 35\sqrt{\frac{2\left(\log^{3}T\right)\left(\log\frac{\left| \mathcal{S}\right|T}{\delta}\right)}{(1-\gamma)^{3}T}}=:a.\] (48)

For \(k\leq\tau\), we have \(u_{k}\geq 1\) and thereby

\[u_{k+1}\leq 35\sqrt{\frac{2\left(\log^{3}T\right)\left(\log\frac{\left| \mathcal{S}\right|T}{\delta}\right)}{(1-\gamma)^{3}T}}u_{k}=a\sqrt{u_{k}},\] (49)

_i.e._,

\[\log u_{k+1}-2\log a\leq\frac{1}{2}\left(\log u_{k}-2\log a\right).\] (50)

Apply it recursively, we have

\[\log u_{k+1}\leq 2\log a+\left(\frac{1}{2}\right)^{k+1}\left(\log u_{0}-2\log a \right),\] (51)

_i.e._,

\[u_{k+1}\leq a^{2}\left(\frac{u_{0}}{a^{2}}\right)^{1/2^{k}}=a^{2\left(1-1/2^{ k}\right)}u_{0}^{1/2^{k}}\leq a^{2\left(1-1/2^{k}\right)}\frac{1}{(1-\gamma)^{1/2^{k}}}.\] (52)

To sum up, for any \(k\geq 0\), \(u_{k+1}\) is always less than the sum of the upper bounds in cases of \(k>\tau\) and \(k\leq\tau\),

\[u_{k+1}\leq a+a^{2\left(1-1/2^{k}\right)}\frac{1}{(1-\gamma)^{1/2^{k}}}\] (53)

Note that, \(a^{2\left(1-1/2^{k}\right)}\leq\max\left\{a,\sqrt{a}\right\}\), and if we take \(k\geq c_{8}\log\log\frac{1}{1-\gamma}\) for any constant \(c_{8}\), we have \(\frac{1}{(1-\gamma)^{1/2^{k}}}=O(1)\). We can take the constant \(c_{8}\) small enough such that \(c_{8}\log\log\frac{1}{1-\gamma}<\log_{2}\left(c_{6}\log T\right)\) (this can be done and \(c_{8}\) is universal since \(\frac{1}{1-\gamma}=o(T)\)), and thereby we can find a valid \(k^{*}\geq c_{8}\log\log\frac{1}{1-\gamma}+1\). Then

\[\left\|\Delta_{T}\right\|_{\tilde{W}_{1}}\leq u_{k^{*}}\leq C_{7}\left(\sqrt{ \frac{\left(\log^{3}T\right)\left(\log\frac{\left|\mathcal{S}\right|T}{\delta }\right)}{(1-\gamma)^{3}T}}+\frac{\left(\log^{3}T\right)\left(\log\frac{\left| \mathcal{S}\right|T}{\delta}\right)}{(1-\gamma)^{3}T}\right),\] (54)

which is the desired conclusion, and \(C_{7}\) is some large universal constant related to \(c_{8}\)

### Analysis of Corollaries 4.1 and 4.2

The difference in the proof compared to Section 5.2 arises in Lemma 5.2 when we control term (II). Now we further bound the result in Lemma C.3 by the Cramer norm of the error term,

\[\bm{\sigma}(\eta_{t})-\bm{\sigma}(\eta)\leq 4\left\|\Delta_{t}\right\|_{W_{1}} \bm{1}\leq\frac{1}{\sqrt{1-\gamma}}\left\|\Delta_{t}\right\|\bm{1}.\] (55)

In the same way, we can derive the following recurrence relation: with probability at least \(1-\delta\), for all \(t\geq\frac{T}{c_{6}\log T}\)

\[\left\|\Delta_{t}\right\|\leq 35\sqrt{\frac{\left(\log^{3}T\right)\left(\log \frac{\left|\mathcal{S}\right|T}{\delta}\right)}{(1-\gamma)^{5/2}T}\left(1+ \max_{k:\,t/2<k\leq t}\left\|\Delta_{k-1}\right\|\right)}.\] (56)

By repeating the reasoning of Theorem C.1, we can obtain the desired conclusion,

\[\left\|\Delta_{T}\right\|\leq C_{7}\left(\sqrt{\frac{\left(\log^{3}T\right) \left(\log\frac{\left|\mathcal{S}\right|T}{\delta}\right)}{(1-\gamma)^{5/2}T} }+\frac{\left(\log^{3}T\right)\left(\log\frac{\left|\mathcal{S}\right|T}{ \delta}\right)}{(1-\gamma)^{5/2}T}\right),\] (57)

which is less than \(\varepsilon\) if we take \(C_{4}\geq 2C_{7}^{2}\) and \(T\geq\frac{C_{4}\log^{3}T}{\varepsilon^{2}(1-\gamma)^{5/2}}\log\frac{\left| \mathcal{S}\right|T}{\delta}\). Here, \(C_{7}>1\) is a large universal constant depending on \(c_{6}\).

### Proof of Lemma c.1

Proof.: We first introduce some notations. For any matrix of operators \(\mathcal{U}\in\mathcal{L}\left(\mathcal{M}\right)^{\mathcal{S}\times\mathcal{ S}}\), we denote \(\mathcal{U}(s)=\left(\mathcal{U}(s,s^{\prime})\right)_{s^{\prime}\in\mathcal{S}} \in\mathcal{L}\left(\mathcal{M}\right)^{\mathcal{S}}\) as the \(s\)-row of \(\mathcal{U}\). And for any \(\xi\in\mathcal{M}^{\mathcal{S}}\), we define the vector inner product operation \(\mathcal{U}(s)\xi:=\sum_{s^{\prime}\in\mathcal{S}}\mathcal{U}(s,s^{\prime}) \xi(s^{\prime})\in\mathcal{M}\).

We need the following lemma, which holds for both cases of NTD and CTD.

**Lemma C.5**.: _For any \(\nu\in\mathcal{M}\), \(n\in\mathbb{N}\), \(\left(\alpha_{i}\right)_{i\in[n]}\in[0,1]^{n}\), let \(\mathcal{U}_{n}=\prod_{i=1}^{n}\left[(1-\alpha_{i})\mathcal{I}+\alpha_{i} \mathcal{T}\right]\), \(\bm{U}_{n}=\prod_{i=1}^{n}\left[(1-\alpha_{i})\mathcal{I}+\alpha_{i}\sqrt{ \gamma}\mathcal{P}\right]\), \(u_{n}=\prod_{i=1}^{n}\left[(1-\alpha_{i})+\alpha_{i}\sqrt{\gamma}\right]\) then for any \(s,s^{\prime}\in\mathcal{S}\), we have_

\[\left\|\mathcal{U}_{n}(s,s^{\prime})\nu\right\|^{2}\leq u_{n}\bm{U}_{n}(s,s^{ \prime})\left\|\nu\right\|^{2}.\]

Utilizing this lemma, we get the following result. Recall that \(\widehat{\mathcal{T}}\) is a random operator and has the same distribution as \(\mathcal{T}_{1}\). Then, for any non-random \(\xi\in\mathcal{M}^{\mathcal{S}}\),

\[\mathbb{E}\left[\left\|\mathcal{U}_{n}(s)(\widehat{\mathcal{T}}- \mathcal{T})\xi\right\|^{2}\right]\] (58) \[= \mathbb{E}\left[\left\|\sum_{s^{\prime}\in\mathcal{S}}\mathcal{U }_{n}(s,s^{\prime})\left[(\widehat{\mathcal{T}}-\mathcal{T})\xi\right](s^{ \prime})\right\|^{2}\right]\] \[= \mathbb{E}\left[\left\|\sum_{s^{\prime}\in\mathcal{S}}\mathcal{U }_{n}(s,s^{\prime})\left[\widehat{\mathcal{T}}(s^{\prime})\xi-\mathcal{T}(s^{ \prime})\xi\right]\right\|^{2}\right]\] \[= \sum_{s^{\prime}\in\mathcal{S}}\mathbb{E}\left[\left\|\mathcal{U }_{n}(s,s^{\prime})\left[\widehat{\mathcal{T}}(s^{\prime})\xi-\mathcal{T}(s^{ \prime})\xi\right]\right\|^{2}\right]\] \[\leq u_{n}\sum_{s^{\prime}\in\mathcal{S}}\bm{U}_{n}(s,s^{\prime}) \mathbb{E}\left[\left\|\widehat{\mathcal{T}}(s^{\prime})\xi-\mathcal{T}(s^{ \prime})\xi\right\|^{2}\right]\] \[= u_{n}\sum_{s^{\prime}\in\mathcal{S}}\bm{U}_{n}(s,s^{\prime})\bm{ \sigma}(\xi)(s^{\prime})\] \[= u_{n}\bm{U}_{n}(s)\bm{\sigma}(\xi),\]where we used different rows of \(\widehat{\mathcal{T}}\) are independent, and \(\widehat{\mathcal{T}}(s^{\prime})\xi\) is an unbiased estimator of \(\mathcal{T}(s^{\prime})\xi\in\mathcal{M}\). Hence, \(\mathsf{Var}\left(\mathcal{U}_{n}(\widehat{\mathcal{T}}-\mathcal{T})\xi\right) \leq u_{n}\bm{U}_{n}\bm{\sigma}(\xi)\).

Now, we are ready to bound \(\mathsf{Var}_{k-1}\left(\zeta_{k}\right)\)

\[\begin{split}\mathsf{Var}_{k-1}\left(\zeta_{k}\right)& =\alpha_{k}^{2}\mathsf{Var}_{k-1}\left(\prod_{i=k+1}^{t}\left[ (1-\alpha_{i})\mathcal{I}+\alpha_{i}\mathcal{T}\right]\left(\mathcal{T}_{k}- \mathcal{T}\right)\eta_{k-1}\right)\\ &\leq\alpha_{k}^{2}\prod_{i=k+1}^{t}\left[(1-\alpha_{i})+\alpha_ {i}\sqrt{\gamma}\right]\prod_{i=k+1}^{t}\left[(1-\alpha_{i})\bm{I}+\alpha_{i} \sqrt{\gamma}\bm{P}\right]\bm{\sigma}(\eta_{k-1})\\ &=\alpha_{k}\beta_{k}^{(t)}\prod_{i=k+1}^{t}\left[(1-\alpha_{i}) \bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\bm{\sigma}(\eta_{k-1}).\end{split}\] (59)

### Proof of Lemma c.2

Proof.: \[\begin{split}&\sum_{k=t/2+1}^{t}\alpha_{k}\prod_{i=k+1}^{t}\left[ (1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\\ =&\sum_{k=t/2+1}^{t}\prod_{i=k+1}^{t}\left[(1-\alpha _{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\alpha_{k}(\bm{I}-\sqrt{\gamma }\bm{P})(\bm{I}-\sqrt{\gamma}\bm{P})^{-1}\\ =&\sum_{k=t/2+1}^{t}\left\{\prod_{i=k+1}^{t}\left[ (1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]-\prod_{i=k}^{t}\left[ (1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\right\}(\bm{I}- \sqrt{\gamma}\bm{P})^{-1}\\ =&\left\{\bm{I}-\prod_{i=t/2+1}^{t}\left[(1-\alpha_{ i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\right]\right\}(\bm{I}-\sqrt{ \gamma}\bm{P})^{-1}\\ \leq&(\bm{I}-\sqrt{\gamma}\bm{P})^{-1},\end{split}\] (60)

where the inequality holds entry-wise since we can verify that all entries of \((\bm{I}-\sqrt{\gamma}\bm{P})^{-1}=\sum_{k=0}^{\infty}\left(\sqrt{\gamma}\bm{P }\right)^{k}\) and \((1-\alpha_{i})\bm{I}+\alpha_{i}\sqrt{\gamma}\bm{P}\) are non-negative. 

### Proof of Lemma c.3

Proof.: For any \(s\in\mathcal{S}\),

\[\begin{split}&\bm{\sigma}(\eta_{t})(s)-\bm{\sigma}(\eta)(s)\\ =&\int_{0}^{\frac{1}{1-\gamma}}\left\{\mathbb{E} \left[F_{(\widehat{\mathcal{T}}\eta_{t})(s)}^{2}(x)\right]-F_{(\mathcal{T} \eta_{t})(s)}^{2}(x)-\mathbb{E}\left[F_{(\widehat{\mathcal{T}}\eta)(s)}^{2}( x)\right]+F_{(\mathcal{T}\eta)(s)}^{2}(x)\right\}dx\\ =&\int_{0}^{\frac{1}{1-\gamma}}\left\{\mathbb{E} \left[F_{(\widehat{\mathcal{T}}\eta_{t})(s)}^{2}(x)-F_{(\widehat{\mathcal{T}} \eta)(s)}^{2}(x)\right]+F_{(\mathcal{T}\eta)(s)}^{2}(x)-F_{(\mathcal{T}\eta_ {t})(s)}^{2}(x)\right\}dx\\ =&\int_{0}^{\frac{1}{1-\gamma}}\left\{\mathbb{E} \left[\left(F_{(\widehat{\mathcal{T}}\eta_{t})(s)}(x)-F_{(\widehat{\mathcal{T}} \eta)(s)}(x)\right)\left(F_{(\widehat{\mathcal{T}}\eta_{t})(s)}(x)+F_{( \widehat{\mathcal{T}}\eta)(s)}(x)\right)\right]\right.\\ &+\left(F_{(\mathcal{T}\eta)(s)}(x)-F_{(\mathcal{T}\eta_{t})(s)}(x) \right)\left(F_{(\mathcal{T}\eta)(s)}(x)+F_{(\mathcal{T}\eta_{t})(s)}(x) \right)\Big{\}}dx\\ \leq& 2\int_{0}^{\frac{1}{1-\gamma}}\left\{\mathbb{E} \left[\left|F_{(\widehat{\mathcal{T}}\eta_{t})(s)}(x)-F_{(\widehat{\mathcal{T}} \eta)(s)}(x)\right|\right]+\left|F_{(\mathcal{T}\eta)(s)}(x)-F_{(\mathcal{T} \eta_{t})(s)}(x)\right|\right\}dx\\ =& 2\left(\mathbb{E}\left[\left\|\widehat{\mathcal{T}} \left(\eta_{t}-\eta\right)(s)\right\|_{W_{1}}\right]+\left\|\mathcal{T}\left( \eta_{t}-\eta\right)(s)\right\|_{W_{1}}\right).\end{split}\] (61)In the case of NTD, \(\mathcal{T}\) and \(\widehat{\mathcal{T}}\) are \(\gamma\)-contraction w.r.t. the supreme \(1\)-Wasserstein metric, hence

\[\begin{split}\boldsymbol{\sigma}(\eta_{t})(s)-\boldsymbol{\sigma}( \eta)(s)&\leq 2\left(\mathbb{E}\left[\left\|\widehat{\mathcal{T}}\left( \eta_{t}-\eta\right)(s)\right\|_{W_{1}}\right]+\left\|\mathcal{T}\left(\eta_{ t}-\eta\right)(s)\right\|_{W_{1}}\right)\\ &\leq 4\gamma\left\|\eta_{t}-\eta\right\|_{\tilde{W}_{1}}\\ &\leq 4\left\|\Delta_{t}\right\|_{\tilde{W}_{1}}.\end{split}\] (62)

In the case of CTD, if we can show \(\Pi_{K}\) is non-expansive w.r.t. \(1\)-Wasserstein metric, the conclusion still holds. For any \(x,y\in\left[0,\frac{1}{1-\gamma}\right]\) such that \(x<y\), we denote \(x\in[x_{k},x_{k+1})\) and \(y\in[x_{l},x_{l+1})\), then \(k\leq l\), by the definition of \(\Pi_{K}\), we have

\[\Pi_{K}(\delta_{x})=\frac{x_{k+1}-y}{\iota_{K}}\delta_{x_{k}}+\frac{y-x_{k}}{ \iota_{K}}\delta_{x_{k+1}},\] (63)

\[\Pi_{K}(\delta_{y})=\frac{x_{l+1}-y}{\iota_{K}}\delta_{x_{l}}+\frac{y-x_{l}}{ \iota_{K}}\delta_{x_{l+1}}.\] (64)

If \(k=l\), we can check that \(W_{1}\left(\Pi_{K}\delta_{x},\Pi_{K}\delta_{y}\right)=\iota_{K}\frac{y-x}{ \iota_{K}}=y-x\). If \(k<l\), we have \(W_{1}\left(\Pi_{K}\delta_{x},\Pi_{K}\delta_{y}\right)\leq W_{1}\left(\Pi_{K} \delta_{x},x_{k+1}+W_{1}\left(x_{k+1},x_{l}\right)+W_{1}\left(x_{l},\Pi_{K} \delta_{y}\right)=(x_{k+1}-x)+(x_{l}-x_{k+1})+(y-x_{x_{l}})=y-x\). Hence, for any \(\nu_{1},\nu_{2}\in\mathscr{P}\) and for any transport plan \(\kappa\in\Gamma(\nu_{1},\nu_{2})\), the previous results tell us the cost of the transport plan \(\Pi_{K}\kappa\in\Gamma\left(\Pi_{K}\nu_{1},\Pi_{K}\nu_{2}\right)\) induced by \(\Pi_{K}\) is no greater than the cost of \(\kappa\). Consequently, \(W_{1}\left(\Pi_{K}\nu_{1},\Pi_{K}\nu_{2}\right)\leq W_{1}(\nu_{1},\nu_{2})\), _i.e._, \(\Pi_{K}\) is non-expansive w.r.t. \(1\)-Wasserstein metric, which is desired. 

### Proof of Lemma c.4

Proof.: Firstly, we show that for any \(\boldsymbol{v}\geq\boldsymbol{0}\), we have \(\left\|(\boldsymbol{I}-\sqrt{\gamma}\boldsymbol{P})^{-1}\boldsymbol{v}\right\| \leq 2\left\|(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{v}\right\|\)

\[\begin{split}\left\|(\boldsymbol{I}-\sqrt{\gamma}\boldsymbol{P})^ {-1}\boldsymbol{v}\right\|&=\left\|(\boldsymbol{I}-\sqrt{\gamma} \boldsymbol{P})^{-1}(\boldsymbol{I}-\gamma\boldsymbol{P})(\boldsymbol{I}- \gamma\boldsymbol{P})^{-1}\boldsymbol{v}\right\|\\ &=\left\|(\boldsymbol{I}-\sqrt{\gamma}\boldsymbol{P})^{-1}\left[ (1-\sqrt{\gamma})\boldsymbol{I}+\sqrt{\gamma}(\boldsymbol{I}-\sqrt{\gamma} \boldsymbol{P})\right](\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{v }\right\|\\ &=\left\|\left[(1-\sqrt{\gamma})(\boldsymbol{I}-\sqrt{\gamma} \boldsymbol{P})^{-1}+\sqrt{\gamma}\boldsymbol{I}\right](\boldsymbol{I}-\gamma \boldsymbol{P})^{-1}\boldsymbol{v}\right\|\\ &\leq(1-\sqrt{\gamma})\left\|(\boldsymbol{I}-\sqrt{\gamma} \boldsymbol{P})^{-1}(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{v} \right\|+\sqrt{\gamma}\left\|(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1} \boldsymbol{v}\right\|\\ &\leq\left(\frac{1-\sqrt{\gamma}}{1-\sqrt{\gamma}}+\sqrt{\gamma} \right)\left\|(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{v} \right\|\\ &\leq 2\left\|(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1} \boldsymbol{v}\right\|.\end{split}\] (65)

In the case of NTD, by Corollary D.1, we have

\[\left\|(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{\sigma}\left( \eta\right)\right\|\leq\frac{1}{1-\gamma},\] (66)

In the case of CTD, by Corollary 5.12 in [Rowland et al., 2024b], we have

\[\left\|(\boldsymbol{I}-\gamma\boldsymbol{P})^{-1}\boldsymbol{\sigma}\left( \eta\right)\right\|\leq\frac{2}{1-\gamma},\] (67)

given \(K>\frac{4}{1-\gamma}\). 

### Proof of Lemma c.5

Proof.: We proof this result by induction. For \(n=0\), we have \(\mathcal{U}_{0}=\mathcal{I}\), \(\boldsymbol{U}_{0}=\boldsymbol{I}\), \(u_{0}=1\), thereby the inequality holds trivially. Suppose the inequality holds true for \(n-1\). To prove that the inequality holds for \(n\), it is sufficient to show that, for any \(\mu\in\mathcal{M}\),

\[\left\|\left[(1-\alpha_{n})\delta_{s,s^{\prime}}+\alpha_{n}\mathcal{T}(s,s^{ \prime})\right]\mu\right\|^{2}\leq\left[(1-\alpha_{n})+\alpha_{n}\sqrt{\gamma} \right]\left[(1-\alpha_{n})\delta_{s,s^{\prime}}+\alpha_{n}\sqrt{\gamma} \boldsymbol{P}(s,s^{\prime})\right]\left\|\mu\right\|^{2},\]

where \(\delta_{s,s^{\prime}}=1\) if \(s=s^{\prime}\), and \(0\) otherwise.

LHS can be bounded as follow

\[\begin{split}&\left\|\left[(1-\alpha_{n})\delta_{s,s^{\prime}}+ \alpha_{n}\mathcal{T}(s,s^{\prime})\right]\mu\right\|^{2}\\ =&(1-\alpha_{n})^{2}\delta_{s,s^{\prime}}\left\|\mu \right\|^{2}+2(1-\alpha_{n})\alpha_{n}\delta_{s,s^{\prime}}\left\langle\mu, \mathcal{T}(s,s^{\prime})\mu\right\rangle+\alpha_{n}^{2}\left\|\mathcal{T}(s, s^{\prime})\mu\right\|^{2}\\ \leq&(1-\alpha_{n})^{2}\delta_{s,s^{\prime}}\left\| \mu\right\|^{2}+2(1-\alpha_{n})\alpha_{n}\delta_{s,s^{\prime}}\left\|\mu \right\|\left\|\mathcal{T}(s,s^{\prime})\mu\right\|+\alpha_{n}^{2}\left\| \mathcal{T}(s,s^{\prime})\mu\right\|^{2},\end{split}\] (68)

where we used Cauchy-Schwarz inequality. We need to give an upper bound for \(\left\|\mathcal{T}(s,s^{\prime})\mu\right\|^{2}\).

Note that \(\left(\Pi_{K}\mathcal{T}^{\pi}\right)(s,s^{\prime})=\Pi_{K}\left(\mathcal{T} ^{\pi}(s,s^{\prime})\right)\) and \(\left\|\Pi_{K}\right\|=1\), we only need to consider the case of NTD, by the definition of \(\mathcal{T}(s,s^{\prime})\), we have

\[\begin{split}\left\|\mathcal{T}(s,s^{\prime})\mu\right\|^{2}& =\int_{0}^{\frac{1}{1-\gamma}}\left[\sum_{a\in\mathcal{A}}\pi(a| s)P(s^{\prime}|s,a)\int_{0}^{1}F_{\mu}\left(\frac{x-r}{\gamma}\right) \mathcal{P}_{R}(dr|s,a)\right]^{2}dx\\ &=\bm{P}(s,s^{\prime})^{2}\int_{0}^{\frac{1}{1-\gamma}}\left\{ \sum_{a\in\mathcal{A}}\pi(a|s)P(s^{\prime}|s,a)\left[F_{\mu}\left(\frac{x-r}{ \gamma}\right)\left|s^{\prime}\right]\right\}^{2}dx\\ &\leq\bm{P}(s,s^{\prime})^{2}\mathbb{E}_{a\sim\pi(\cdot|s),r\sim \mathcal{P}_{R}(\cdot|s,a)}\left\{\int_{0}^{\frac{1}{1-\gamma}}\left[F_{\mu} \left(\frac{x-r}{\gamma}\right)\right]^{2}dx\big{|}s^{\prime}\right\}\\ &=\gamma\bm{P}(s,s^{\prime})^{2}\left\|\mu\right\|^{2},\end{split}\] (69)

where we used Jensen's inequality and Fubini's theorem. Substitute it back to the upper bound,

\[\begin{split}&\left\|\left[(1-\alpha_{n})\delta_{s,s^{\prime}}+ \alpha_{n}\mathcal{T}(s,s^{\prime})\right]\mu\right\|^{2}\\ \leq&(1-\alpha_{n})^{2}\delta_{s,s^{\prime}}\left\| \mu\right\|^{2}+2(1-\alpha_{n})\alpha_{n}\delta_{s,s^{\prime}}\left\|\mu \right\|\left\|\mathcal{T}(s,s^{\prime})\mu\right\|+\alpha_{n}^{2}\left\| \mathcal{T}(s,s^{\prime})\mu\right\|^{2}\\ \leq&\left[(1-\alpha_{n})^{2}\delta_{s,s^{\prime}}+ 2(1-\alpha_{n})\alpha_{n}\delta_{s,s^{\prime}}\sqrt{\gamma}\bm{P}(s,s^{\prime })+\alpha_{n}^{2}\gamma\bm{P}(s,s^{\prime})^{2}\right]\left\|\mu\right\|^{2} \\ =&\left[(1-\alpha_{n})^{2}\delta_{s,s^{\prime}}+ \alpha_{n}\sqrt{\gamma}\bm{P}(s,s^{\prime})\right]^{2}\left\|\mu\right\|^{2} \\ \leq&\left[(1-\alpha_{n})+\alpha_{n}\sqrt{\gamma} \right]\left[(1-\alpha_{n})\delta_{s,s^{\prime}}+\alpha_{n}\sqrt{\gamma}\bm{P} (s,s^{\prime})\right]\left\|\mu\right\|^{2},\end{split}\] (70)

which is desired. 

## Appendix D Stochastic Distributional Bellman Equation and Operator

In this section, we use the same notations as in Appendix C and only consider the NTD setting. Inspired by stochastic categorical CDF Bellman operator introduced in [Rowland et al., 2024b], we introduce stochastic distributional Bellman operator \(\mathscr{T}\colon\Delta\left(\mathscr{P}^{\mathcal{S}}\right)\to\Delta\left( \mathscr{P}^{\mathcal{S}}\right)\) to derive an upper bound for \(\left\|(\bm{I}-\gamma\bm{P})^{-1}\bm{\sigma}(\eta)\right\|\) in the case of NTD. For any \(\bm{\phi}\in\Delta\left(\mathscr{P}^{\mathcal{S}}\right)\), we denote \(\bm{\eta}_{\bm{\phi}}\) be the random element in \(\mathscr{P}^{\mathcal{S}}\) with law \(\bm{\phi}\).

\[\mathscr{T}\bm{\phi}:=\text{Law}\left(\widehat{\mathcal{T}}\bm{\eta}_{\bm{\phi}} \right),\] (71)

where \((\widehat{\mathcal{T}}\bm{\eta}_{\bm{\phi}})(\omega):=(\widehat{\mathcal{T}})( \omega)(\bm{\eta}_{\bm{\phi}})(\omega)\in\mathscr{P}^{\mathcal{S}}\) for any \(\omega\in\Omega\), \(\Omega\) is the corresponding probability space, and \(\widehat{\mathcal{T}}\) is independent of \(\bm{\eta}_{\bm{\phi}}\). In this part, \(\widehat{\mathcal{T}}\) does not consist of \(\Pi_{K}\) since we only consider the NTD setting.

We consider the \(1\)-Wasserstein metric \(W_{1}\) on \(\Delta\left(\mathscr{P}^{\mathcal{S}}\right)\), the space of all probability measures on the space \(\left(\mathscr{P}^{\mathcal{S}},\bar{\ell}_{2}\right)\). Since \(\left(\mathscr{P}^{\mathcal{S}},\bar{\ell}_{2}\right)\) is Polish (complete and separable), the space \(\left(\Delta\left(\mathscr{P}^{\mathcal{S}}\right),W_{1}\right)\) is also Polish (Theorem 6.18 in [Villani et al., 2009]).

**Proposition D.1**.: _The stochastic distributional Bellman operator \(\mathscr{T}\) is a \(\sqrt{\gamma}\)-contraction on \(\Delta\left(\mathscr{P}^{\mathcal{S}}\right)\), i.e., for any \(\bm{\phi},\bm{\phi}^{\prime}\in\Delta\left(\mathscr{P}^{\mathcal{S}}\right)\), we have_

\[W_{1}\left(\mathscr{T}\bm{\phi},\mathscr{T}\bm{\phi}^{\prime}\right)\leq\sqrt{ \gamma}W_{1}\left(\bm{\phi},\bm{\phi}^{\prime}\right).\]Proof.: Let \(\bm{\kappa}^{\star}\in\Gamma\left(\bm{\phi},\bm{\phi}^{\prime}\right)\) be the optimal coupling between \(\bm{\phi}\) and \(\bm{\phi}^{\prime}\). The existence of \(\bm{\kappa}^{\star}\) is guaranteed by Theorem 4.1 in (Villani et al., 2009). And let the random element \(\bm{\xi}=\left(\bm{\xi}_{1},\bm{\xi}_{2}\right)\) in \(\left(\mathscr{P}^{\mathcal{S}}\right)^{2}\) has the law \(\bm{\kappa}^{\star}\), where \(\bm{\xi}_{1}\) and \(\bm{\xi}_{2}\) are both random elements in \(\mathscr{P}^{\mathcal{S}}\). We denote \(\mathscr{T}\bm{\kappa}^{\star}:=\text{Law}\left[\left(\widehat{\mathcal{T}} \bm{\xi}_{1},\widehat{\mathcal{T}}\bm{\xi}_{2}\right)\right]\in\Gamma\left( \mathscr{T}\bm{\phi},\mathscr{T}\bm{\phi}^{\prime}\right)\).

\[W_{1}\left(\mathscr{T}\bm{\phi},\mathscr{T}\bm{\phi}^{\prime}\right) =\inf_{\bm{\kappa}\in\Gamma\left(\mathscr{T}\bm{\phi},\mathscr{ T}\bm{\phi}^{\prime}\right)}\int_{\left(\mathscr{P}^{\mathcal{S}}\right)^{2}} \bar{\ell}_{2}\left(\xi,\xi^{\prime}\right)\bm{\kappa}\left(d\xi,d\xi^{\prime}\right)\] (72) \[\leq\int_{\left(\mathscr{P}^{\mathcal{S}}\right)^{2}}\bar{\ell}_ {2}\left(\xi,\xi^{\prime}\right)\mathscr{T}\bm{\kappa}^{\star}\left(d\xi,d \xi^{\prime}\right)\] \[=\mathbb{E}\left[\bar{\ell}_{2}\left(\bm{\hat{\mathcal{T}}}\bm{ \xi}_{1},\widehat{\mathcal{T}}\bm{\xi}_{2}\right)\right]\] \[\leq\sqrt{\gamma}\mathbb{E}\left[\bar{\ell}_{2}\left(\bm{\xi}_{1 },\bm{\xi}_{2}\right)\right]\] \[=\sqrt{\gamma}\int_{\left(\mathscr{P}^{\mathcal{S}}\right)^{2}} \bar{\ell}_{2}\left(\xi,\xi^{\prime}\right)\bm{\kappa}^{\star}\left(d\xi,d\xi^ {\prime}\right)\] \[=\sqrt{\gamma}\inf_{\bm{\kappa}\in\Gamma\left(\bm{\phi},\bm{\phi }^{\prime}\right)}\int_{\left(\mathscr{P}^{\mathcal{S}}\right)^{2}}\bar{\ell} _{2}\left(\xi,\xi^{\prime}\right)\bm{\kappa}\left(d\xi,d\xi^{\prime}\right)\] \[=\sqrt{\gamma}W_{1}\left(\bm{\phi},\bm{\phi}^{\prime}\right).\]

By the proposition and contraction mapping theorem, there exists a unique fixed point of \(\mathscr{T}\), we denote \(\bm{\psi}\in\Delta\left(\mathscr{P}^{\mathcal{S}}\right)\) as the fixed point. Hence, the stochastic distributional Bellman equation reads

\[\bm{\psi}=\mathscr{T}\bm{\psi}.\] (73)

We denote \(\bm{\eta}_{\bm{\psi}}\) as the random element in \(\mathscr{P}\) with law \(\bm{\psi}\), then \(\widehat{\mathcal{T}}\bm{\eta}_{\bm{\psi}}\) and \(\bm{\eta}_{\bm{\psi}}\) have the same law. As shown in the following proposition, \(\bm{\eta}_{\bm{\psi}}\) can be regarded as a noisy version of \(\eta\).

**Proposition D.2**.: \[\mathbb{E}\left[\bm{\eta}_{\bm{\psi}}\right]=\eta,\]

_where the expectation is regarded as the Bochner integral in the space of all finite measures on \(\mathscr{P}^{\mathcal{S}}\), which is a normed linear space equipped with Cramer metric as its norm._

Proof.: \[\mathbb{E}\left[\bm{\eta}_{\bm{\psi}}\right] =\mathbb{E}\left[\widehat{\mathcal{T}}\bm{\eta}_{\bm{\psi}}\right]\] (74) \[=\mathbb{E}\left\{\mathbb{E}\left[\widehat{\mathcal{T}}\bm{\eta}_ {\bm{\psi}}\middle|\bm{\eta}_{\bm{\psi}}\right]\right\}\] \[=\mathbb{E}\left[\mathcal{T}\bm{\eta}_{\bm{\psi}}\right]\] \[=\mathcal{T}\mathbb{E}\left[\bm{\eta}_{\bm{\psi}}\right],\]

where we used \(\widehat{\mathcal{T}}\) is independent of \(\bm{\eta}_{\bm{\psi}}\). Since \(\mathbb{E}\left[\bm{\eta}_{\bm{\psi}}\right]\) is the fixed point of \(\mathcal{T}\), we have \(\mathbb{E}\left[\bm{\eta}_{\bm{\psi}}\right]=\eta\). 

Based on the concepts of \(\mathscr{T}\) and \(\bm{\psi}\), we can obtain the following second order distributional Bellman equation, which is similar to the classic second-order Bellman equation (Lemma 7 in (Gheshlaghi Azar et al., 2013)).

Recall the one-step Cramer variation \(\bm{\sigma}(\eta)=\left(\mathbb{E}\left[\left\|\left(\widehat{\mathcal{T}}\eta \right)(s)-\eta(s)\right\|^{2}\right]\right)_{s\in\mathcal{S}}\in\mathbb{R}^{ \mathcal{S}}\) used in the NTD setting. We denote \(\bm{\sigma}:=\bm{\sigma}(\eta)\) for simplicity, and \(\bm{\Sigma}:=\left(\mathbb{E}\left[\left\|\bm{\eta}_{\bm{\psi}}(s)-\eta(s) \right\|^{2}\right]\right)_{s\in\mathcal{S}}\in\mathbb{R}^{\mathcal{S}}\).

**Proposition D.3** (Second order distributional Bellman equation).: \[\bm{\Sigma}=\bm{\sigma}+\gamma\bm{P}\bm{\Sigma}.\]Proof.: For any \(s\in\mathcal{S}\),

\[\begin{split}\Sigma(s)&=\mathbb{E}\left[\left\|\bm{\eta} _{\bm{\psi}}(s)-\eta(s)\right\|^{2}\right]\\ &=\mathbb{E}\left[\left\|\left(\widehat{\mathcal{T}}\bm{\eta}_{ \bm{\psi}}\right)(s)-\eta(s)\right\|^{2}\right]\\ &=\mathbb{E}\left[\left\|\left(\widehat{\mathcal{T}}\bm{\eta}_{ \bm{\psi}}\right)(s)-\left(\widehat{\mathcal{T}}\eta\right)(s)+\left(\widehat{ \mathcal{T}}\eta\right)(s)-\eta(s)\right\|^{2}\right]\\ &=\mathbb{E}\left[\left\|\left(\widehat{\mathcal{T}}\eta\right)(s )-\eta(s)\right\|^{2}\right]+\mathbb{E}\left[\left\|\left(\widehat{\mathcal{T }}\bm{\eta}_{\bm{\psi}}\right)(s)-\left(\widehat{\mathcal{T}}\eta\right)(s) \right\|^{2}\right],\end{split}\] (75)

where the last equality holds since the cross term is zero as below

\[\begin{split}&\mathbb{E}\left[\left\langle\left(\widehat{ \mathcal{T}}\eta\right)(s)-\eta(s),\left(\widehat{\mathcal{T}}\bm{\eta}_{\bm{ \psi}}\right)(s)-\left(\widehat{\mathcal{T}}\eta\right)(s)\right\rangle\right] \\ =&\mathbb{E}\left\{\mathbb{E}\left[\left\langle\left( \widehat{\mathcal{T}}\eta\right)(s)-\eta(s),\left(\widehat{\mathcal{T}}\bm{\eta }_{\bm{\psi}}\right)(s)-\left(\widehat{\mathcal{T}}\eta\right)(s)\right\rangle \right]\widehat{\mathcal{T}}\right\}\\ =&\mathbb{E}\left\{\left\langle\left(\widehat{ \mathcal{T}}\eta\right)(s)-\eta(s),\mathbb{E}\left[\left(\widehat{\mathcal{T }}\bm{\eta}_{\bm{\psi}}\right)(s)\right|\widehat{\mathcal{T}}\right]-\left( \widehat{\mathcal{T}}\eta\right)(s)\right\rangle\right\}\\ =&\mathbb{E}\left[\left\langle\left(\widehat{ \mathcal{T}}\eta\right)(s)-\eta(s),\mathbf{0}\right\rangle\right]\\ =& 0.\end{split}\] (76)

The first term in (75) is \(\bm{\sigma}(s)\), we need to deal with the second term.

\[\begin{split}&\mathbb{E}\left[\left\|\left(\widehat{\mathcal{T }}\bm{\eta}_{\bm{\psi}}\right)(s)-\left(\widehat{\mathcal{T}}\eta\right)(s) \right\|^{2}\right]\\ =&\mathbb{E}\left\{\mathbb{E}\left[\left\|\left( \widehat{\mathcal{T}}\bm{\eta}_{\bm{\psi}}\right)(s)-\left(\widehat{\mathcal{T }}\eta\right)(s)\right\|^{2}\left|\bm{\eta}_{\bm{\psi}}\right|\right\}\\ =&\mathbb{E}\left\{\mathbb{E}_{a(s)\sim\pi(\cdot|s), s^{\prime}(s)\sim P(\cdot|s,a(s)),r(s)\sim\mathcal{P}_{R}(\cdot|s,a(s))}\left[ \int_{0}^{\frac{1}{1-\gamma}}\left(F_{(\bm{\eta}_{\bm{\psi}})(s^{\prime}(s))} \left(\frac{x-r}{\gamma}\right)-F_{\eta(s^{\prime}(s))}\left(\frac{x-r}{\gamma }\right)\right)^{2}dx\middle|\bm{\eta}_{\bm{\psi}}\right]\right\}\\ =&\gamma\sum_{s^{\prime}\in\mathcal{S}}\mathbb{E} \left[\left\|\bm{\eta}_{\bm{\psi}}(s^{\prime})-\eta(s^{\prime})\right\|^{2} \right]\sum_{a\in\mathcal{A}}\pi(a|s)P(s^{\prime}|s,a)\\ =&\gamma\sum_{s^{\prime}\in\mathcal{S}}\bm{P}(s,s^{ \prime})\bm{\Sigma}(s^{\prime}).\end{split}\] (77)

Put these together, and we can arrive at the conclusion. 

Now, we can derive a tighter upper bound for \(\left\|(\bm{I}-\gamma\bm{P})^{-1}\bm{\sigma}(\eta)\right\|\).

**Corollary D.1**.: \[\left\|(\bm{I}-\gamma\bm{P})^{-1}\bm{\sigma}(\eta)\right\|\leq\left\|(\bm{I}- \gamma\bm{P})^{-1}\bm{\sigma}\right\|=\left\|\bm{\Sigma}\right\|\leq\frac{1}{1- \gamma}.\]

Proof.: Note that all entries of \((\bm{I}-\gamma\bm{P})^{-1}=\sum_{k=0}^{\infty}\left(\gamma\bm{P}\right)^{k}\) are positive, thereby \((\bm{I}-\gamma\bm{P})^{-1}\bm{\sigma}(\eta)\leq(\bm{I}-\gamma\bm{P})^{-1}\bm {\sigma}=\bm{\Sigma}\), and \(\bm{\Sigma}(s)=\mathbb{E}\left[\left\|\bm{\eta}_{\bm{\psi}}(s)-\eta(s)\right\| ^{2}\right]\leq\int_{0}^{\frac{1}{1-\gamma}}dx=\frac{1}{1-\gamma}\) for any \(s\in\mathcal{S}\). 

## Appendix E Other Technical Lemmas

**Lemma E.1** (Basic Inequalities for Metrics on the Space of Probability Measures).: _For any \(\nu_{1},\nu_{2}\in\mathscr{P}\), we have \(\left(\ell_{2}(\nu_{1},\nu_{2})\right)^{2}\leq W_{1}(\nu_{1},\nu_{2})\leq\frac{ 1}{\sqrt{1-\gamma}}\ell_{2}(\nu_{1},\nu_{2})\) and \(W_{p}(\nu_{1},\nu_{2})\leq\frac{1}{\left(1-\gamma\right)^{1-\frac{1}{p}}}W_{1}^ {\frac{1}{p}}(\nu_{1},\nu_{2})\)._Proof.: By Cauchy-Schwarz inequality,

\[W_{1}(\nu_{1},\nu_{2})\] \[= \int_{0}^{\frac{1}{1-\gamma}}|F_{\nu_{1}}(x)-F_{\nu_{2}}(x)|dx\] \[\leq \sqrt{\int_{0}^{\frac{1}{1-\gamma}}1^{2}dx}\sqrt{\int_{0}^{\frac{1 }{1-\gamma}}|F_{\nu_{1}}(x)-F_{\nu_{2}}(x)|^{2}dx}\] \[= \frac{1}{\sqrt{1-\gamma}}\ell_{2}(\nu_{1},\nu_{2}).\]

And

\[\ell_{2}(\nu_{1},\nu_{2})\] \[= \sqrt{\int_{0}^{\frac{1}{1-\gamma}}|F_{\nu_{1}}(x)-F_{\nu_{2}}(x) |^{2}dx}\] \[\leq \sqrt{\int_{0}^{\frac{1}{1-\gamma}}|F_{\nu_{1}}(x)-F_{\nu_{2}}(x) |dx}\] \[= \sqrt{W_{1}(\nu_{1},\nu_{2})}.\]

And

\[W_{p}(\nu_{1},\nu_{2})\] \[= \left(\inf_{\kappa\in\Gamma(\nu_{1},\nu_{2})}\int_{[0,\frac{1}{1 -\gamma}]^{2}}|x-y|^{p}\,\kappa(dx,dy)\right)^{1/p}\] \[\leq \frac{1}{(1-\gamma)^{1-\frac{1}{p}}}\left(\inf_{\kappa\in\Gamma( \nu_{1},\nu_{2})}\int_{[0,\frac{1}{1-\gamma}]^{2}}|x-y|\,\kappa(dx,dy)\right)^ {1/p}\] \[= \frac{1}{(1-\gamma)^{1-\frac{1}{p}}}W_{1}^{\frac{1}{p}}(\nu_{1}, \nu_{2}).\]

**Lemma E.2**.: \(\left(\mathcal{M},\left\|\cdot\right\|_{\ell_{2}}\right)\) _and \(\left(\mathcal{M},\left\|\cdot\right\|_{W_{1}}\right)\) are separable._

Proof.: Recall that \((\mathscr{P},W_{1})\) is separable [Theorem 6.18 Villani et al., 2009], and by Lemma E.1, the Cramer distance \(\ell_{2}\) can be bounded by \(1\)-Wasserstein distance \(W_{1}\), hence \((\mathscr{P},\ell_{2})\) is also separable. Let \(d\) be either \(W_{1}\) or \(\ell_{2}\), and \(A\) be the countable dense subset of \((\mathscr{P},d)\). For any \(\epsilon>0,\mu\in\mathcal{M}\), we denote \(\tilde{\mu}:=2\mu\). Then we can find a \(q\in\mathbb{Q},\tilde{\mu}_{+},\bar{\mu}_{-}\in A,\mathsf{s.t.}\left\|q-\frac {1}{2}\left|\mu\right|(\mathbb{R})\right|\leq\frac{\epsilon|\mu|(\mathbb{R})}{ \tilde{\mu}\left\|\mu\right\|_{d}}\), \(d(\bar{\mu}_{+},\tilde{\mu}_{+})\leq\frac{\epsilon}{3q},d(\bar{\mu}_{-},\tilde {\mu}_{-})\leq\frac{\epsilon}{3q}\). Note that the set of all possible \(\bar{\mu}=\bar{\mu}_{+}-\bar{\mu}_{-}\) is countable. Let \(\hat{\mu}:=q\hat{\mu}\), then we have

\[\left\|\mu-\hat{\mu}\right\|_{d}\] \[\leq \left\|\mu-q\hat{\mu}\right\|_{d}+\left\|q\tilde{\mu}-\hat{\mu} \right\|_{d}\] \[= \left\|(\frac{1}{2}\left|\mu\right|(\mathbb{R})-q)\tilde{\mu} \right\|_{d}+q\left\|(\tilde{\mu}_{+}-\bar{\mu}_{+})-(\tilde{\mu}_{-}-\bar{\mu }_{-})\right\|_{d}\] \[\leq \left|q-\frac{1}{2}\left|\mu\right|(\mathbb{R})\right|\frac{2 \left\|\mu\right\|_{d}}{\left|\mu\right|(\mathbb{R})}+q\left\|\tilde{\mu}_{+}- \bar{\mu}_{+}\right\|_{d}+q\left\|\tilde{\mu}_{-}-\bar{\mu}_{-}\right\|_{d}\] \[\leq \frac{\epsilon\left|\mu\right|(\mathbb{R})}{6\left\|\mu\right\|_{d }}\frac{2\left\|\mu\right\|_{d}}{\left|\mu\right|(\mathbb{R})}+q\frac{ \epsilon}{3q}+q\frac{\epsilon}{3q}\] \[= \epsilon.\]

Hence we have found a countable dense subset for \((\mathcal{M},\left\|\cdot\right\|_{d})\) for \(d=W_{1}\) or \(\ell_{2}\). Therefore, \(\left(\mathcal{M},\left\|\cdot\right\|_{\ell_{2}}\right)\) and \(\left(\mathcal{M},\left\|\cdot\right\|_{W_{1}}\right)\) are separable.

**Lemma E.3**.: \(\left(\mathcal{M},\left\lVert\cdot\right\rVert_{W_{1}}\right)\) _is not complete._

Proof.: Consider the Cauchy sequence in \(\left(\mathcal{M},\left\lVert\cdot\right\rVert_{W_{1}}\right)\mu_{n}=\sum_{i=1} ^{n}\left(\delta_{\frac{1}{i}+\frac{1}{2^{n}_{i}}}-\delta_{\frac{1}{i}-\frac{1} {2^{i}}}\right)\), which satisfies \(\left\lVert\mu_{n}-\mu_{n+k}\right\rVert_{W_{1}}\leq\sum_{i=1}^{k}\frac{1}{2^ {n+i-1}}\leq\frac{1}{2^{n-1}}\to 0\), but its limit \(\sum_{i=1}^{\infty}\left(\delta_{\frac{1}{i}+\frac{1}{2^{i}}}-\delta_{\frac{1} {i}-\frac{1}{2^{i}}}\right)\notin\mathcal{M}\) because its total variation is infinity. Hence, this is a Cauchy sequence without a limit, implying the space is not complete. 

**Lemma E.4** (Range of \(\eta_{t}^{\pi}\)).: _Suppose that \(\alpha_{t}\in[0,1]\) for all \(t\geq 0\). Assume that \(\eta_{0}^{\pi}\in\mathscr{P}^{\mathcal{S}}\), then we have, for all \(t\geq 0\), \(\eta_{t}^{\pi}\in\mathscr{P}^{\mathcal{S}}\) in the case of NTD. Similarly, assume that \(\eta_{0}^{\pi}\in\mathscr{P}^{\mathcal{S}}_{K}\), then we have, for all \(t\geq 0\), \(\eta_{t}^{\pi}\in\mathscr{P}^{\mathcal{S}}_{K}\) in the case of CTD._

Proof.: We will only prove the case of NTD, and the proof for CTD is similar by utilizing the property of the projection operator \(\Pi_{K}\colon\mathscr{P}^{\mathcal{S}}\to\mathscr{P}^{\mathcal{S}}_{K}\). We prove the result by induction. It is trivial that \(\eta_{t}^{\pi}\in\mathscr{P}^{\mathcal{S}}\) for \(t=0\). Suppose that \(\eta_{t-1}^{\pi}\in\mathscr{P}^{\mathcal{S}}\), recall the updating scheme of NTD

\[\eta_{t}^{\pi}=(1-\alpha_{t})\eta_{t-1}^{\pi}+\alpha_{t}\mathcal{T}_{t}^{\pi} \eta_{t-1}^{\pi}.\] (78)

It is evident that \(\mathscr{P}^{\mathcal{S}}\) is a convex set, considering that \(\mathscr{P}^{\mathcal{S}}\) is a subset of the product signed measure space, which is a linear space. Therefore, we only need to show that \(\mathcal{T}_{t}^{\pi}\eta_{t-1}^{\pi}\in\mathscr{P}^{\mathcal{S}}\), which trivially holds since \(\mathcal{T}_{t}^{\pi}\) is a random operator mapping from \(\mathscr{P}^{\mathcal{S}}\) to \(\mathscr{P}^{\mathcal{S}}\), and \(\eta_{t-1}^{\pi}\in\mathscr{P}^{\mathcal{S}}\). By applying the induction argument, we can arrive at the conclusion.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims reflect the paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The theoretical assumptions used in the paper are widely accepted and supported by a rich body of references in the literature, and the references are cited in our paper. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: Our paper provide the full set of assumptions and a self-contained proof. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Given that our paper focuses on the theoretical analysis of existing popular algorithms, it does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conform with NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper only focuses on theory, hence there is no negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper only focuses on theory. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.