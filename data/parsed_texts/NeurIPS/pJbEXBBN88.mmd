# Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Linear Subspaces

Odelia Melamed

Weizmann Institute of Science, Israel, odelia.melamed@weizmann.ac.il

Gilad Yehudai

Weizmann Institute of Science, Israel, gilad.yehudai@weizmann.ac.il

Gal Vardi

TTI-Chicago and the Hebrew University of Jerusalem, galvardi@ttic.edu.

In this paper we focus on data which lies on a low dimensional "data-manifold". Specifically, we assume that the data lies on a linear subspace \(P\subseteq\mathbb{R}^{d}\) of dimension \(d-\ell\) for some \(\ell>0\). We study adversarial perturbations in the direction of \(P^{\perp}\), i.e. orthogonal to the data subspace. We show that the gradient projected on \(P^{\perp}\) is large, and in addition there exist a _universal adversarial perturbation_ in a direction orthogonal to \(P\). Namely, the same small adversarial perturbation applies to many inputs. The norm of the gradient depends on the term \(\frac{\ell}{d}\), while the perturbation size depends on the term \(\frac{d}{\ell}\), i.e. a low dimensional subspace implies reduced adversarial robustness. Finally, we also study how changing the initialization scale or adding \(L_{2}\) regularization affects robustness. We show that in our setting, decreasing the initialization scale, or adding a sufficiently large regularization term, can make the network significantly more robust. We also demonstrate empirically the effects of the initialization scale and regularization on the decision boundary. Our experiments suggest that these effects might extend to deeper networks.

## 2 Related Works

Despite extensive research, the reasons for the abundance of adversarial examples in trained neural networks are still not well understood (Goodfellow et al., 2014; Fawzi et al., 2018; Shafahi et al., 2018; Schmidt et al., 2018; Khoury and Hadfield-Menell, 2018; Bubeck et al., 2019; Allen-Zhu and Li, 2020; Wang et al., 2020; Shah et al., 2020; Shamir et al., 2021; Ge et al., 2021; Wang et al., 2022; Dohmatob and Bietti, 2022). Below we discuss several prior works on this question.

In a string of works, it was shown that small adversarial perturbations can be found for any fixed input in certain ReLU networks with random weights (drawn from the Gaussian distribution). Building on Shamir et al. (2019), it was shown in Daniely and Shacham (2020) that small adversarial \(L_{2}\)-perturbations can be found in random ReLU networks where each layer has vanishing width relative to the previous layer. Bubeck et al. (2021) extended this result to two-layer neural networks without the vanishing width assumption, and Bartlett et al. (2021) extended it to a large family of ReLU networks of constant depth. Finally, Montanari and Wu (2022) provided a similar result, but with weaker assumptions on the network width and activation functions. These works aim to explain the abundance of adversarial examples in neural networks, since they imply that adversarial examples are common in random networks, and in particular in random initializations of gradient-based methods. However, trained networks are clearly not random, and properties that hold in random networks may not hold in trained networks. Our results also involve an analysis of the random initialization, but we consider the projection of the weights onto the linear subspace orthogonal to the data, and study its implications on the perturbation size required for flipping the output's sign in trained networks.

In Bubeck et al. (2021) and Bubeck and Sellke (2021), the authors proved under certain assumptions, that overparameterization is necessary if one wants to interpolate training data using a neural network with a small Lipschitz constant. Namely, neural networks with a small number of parameters are not expressive enough to interpolate the training data while having a small Lipschitz constant. These results suggest that overparameterization might be necessary for robustness.

Vardi et al. (2022) considered a setting where the training dataset \(\mathcal{S}\) consists of nearly-orthogonal points, and proved that every network to which gradient flow might converge is non-robust w.r.t. \(\mathcal{S}\). Namely, building on known properties of the implicit bias of gradient flow when training two-layer ReLU networks w.r.t. the logistic loss, they proved that for every two-layer ReLU network to which gradient flow might converge as the time \(t\) tends to infinity, and every point \(\mathbf{x}_{i}\) from \(\mathcal{S}\), it is possible to flip the output's sign with a small perturbation. We note that in Vardi et al. (2022) there is a strict limit on the number of training samples and their correlations, as well as the training duration. Here, we have no assumptions on the number of data points and their structure, besides lying on a low-dimensional subspace. Also, in Vardi et al. (2022) the adversarial perturbations are shown to exist only for samples in the training set, while here we show existence of adversarial perturbation for any sample which lies on the low-dimensional manifold.

It is widely common to assume that "real-life data" (such as images, videos, text, etc.) lie roughly within some underlying low-dimensional data manifold. This common belief started many successful research fields such as GAN (Goodfellow et al., 2014), VAE (Kingma and Welling, 2013), and diffusion (Sohl-Dickstein et al., 2015). In Fawzi et al. (2018) the authors consider a setting where the high dimensional input data is generated from a low-dimensional latent space. They theoretically analyze the existence of adversarial perturbations on the manifold generated from the latent space,although they do not bound the norm of these perturbations. Previous works analyzed adversarial perturbations orthogonal to the data manifold. For example, Khoury and Hadfield-Menell (2018) considering several geometrical properties of adversarial perturbation and adversarial training for low dimensional data manifolds. Tanay and Griffin (2016) analyzed theoretically such perturbations for linear networks, and Stutz et al. (2019) gave an empirical analysis for non-linear models. Moreover, several experimental defence methods against adversarial examples were obtained, using projection of it onto the data manifold to eliminate the component orthogonal to the data (see, e.g., Jalal et al. (2017); Meng and Chen (2017); Samangouei et al. (2018)).

Shamir et al. (2021) showed empirically on both synthetic and realistic datasets that the decision boundary of classifiers clings onto the data manifold, causing very close off-manifold adversarial examples. Our paper continues this direction, and provides theoretical guarantees for off-manifold perturbations on trained two-layer ReLU networks, in the special case where the manifold is a linear subspace.

## 3 Setting

Notations.We denote by \(\mathcal{U}(A)\) the uniform distribution over a set \(A\). The multivariate normal distribution with mean \(\mu\) and covariance \(\Sigma\) is denoted by \(\mathcal{N}(\mu,\Sigma)\), and the univariate normal distribution with mean \(a\) and variance \(\sigma^{2}\) is denoted by \(\mathcal{N}(a,\sigma^{2})\). The set of integers \(\{1,..,m\}\) is denoted by \([m]\). For a vector \(v\in\mathbb{R}^{n}\), we define \(v_{i:i+j}\) to be the \(j+1\) coordinates of \(v\) starting from \(i\) and ending with \(i+j\). For a vector \(x\) and a linear subspace \(P\) we denote by \(P^{\perp}\) the subspace orthogonal to \(P\), and by \(\Pi_{P^{\perp}}\left(x\right)\) the projection of \(x\) on \(P^{\perp}\). We denote by \(\mathbf{0}\) the zero vector. We use \(I_{d}\) for the identity matrix of size \(d\).

### Architecture and Training

In this paper we consider a two-layer fully-connected neural network \(N:\mathbb{R}^{d}\rightarrow\mathbb{R}\) with ReLU activation, input dimension \(d\) and hidden dimension \(m\):

\[N(x,\mathbf{w}_{1:m})=\sum_{i=1}^{m}u_{i}\sigma(w_{i}^{\top}x)\;.\]

Here, \(\sigma(z)=\max(z,0)\) is the ReLU function and \(\mathbf{w}_{1:m}=(w_{1},\ldots,w_{m})\). When \(\mathbf{w}_{1:m}\) is clear from the context, we will write for short \(N(x)\).

We initialize the first layer using standard Kaiming initialization (He et al., 2015), i.e. \(w_{i}\sim\mathcal{N}\left(\mathbf{0},\frac{1}{d}I_{d}\right)\), and the output layer as \(u_{i}\sim\mathcal{U}\left(\left\{-\frac{1}{\sqrt{m}},\frac{1}{\sqrt{m}}\right\}\right)\) for every \(i\in[m]\). Note that in standard initialization, each \(u_{i}\) would be initialized normally with a standard deviation of \(\frac{1}{\sqrt{m}}\), for ease of analysis we fix the initial value to be equal to the standard deviation where only the sign is random.

We consider a dataset with binary labels. Given a training dataset \((x_{1},y_{1}),\ldots,(x_{r},y_{r})\in\mathbb{R}^{d}\times\{-1,1\}\) we train w.r.t. the logistic loss (a.k.a. binary cross entropy): \(L(q)=\log(1+e^{-q})\), and minimize the empirical error:

\[\min_{w_{1},\ldots,w_{m}}\sum_{i=1}^{r}L\left(y_{i}\cdot N(x_{i},\mathbf{w}_{1: m})\right)\;.\]

We assume throughout the paper that the network is trained using either gradient descent (GD) or stochastic gradient descent (SGD). Our results hold for both training methods. We assume that only the weights of the first layer (i.e. the \(w_{i}\)'s) are trained, while the weights of the second layer (i.e. the \(u_{i}\)'s) are fixed.

### Assumptions on the Data

Our main assumption in this paper is that the input data lie on a low dimensional manifold, which is embedded in a high dimensional space. Specifically, we assume that this "data manifold" is a linear subspace, denoted by \(P\), which has dimension \(d-\ell\). We denote by \(\ell\) the dimension of the data "off-manifold", i.e. the linear subspace orthogonal to the data subspace, which is denoted by \(P^{\perp}\). In this work we study adversarial perturbations in \(P^{\perp}\). Note that adding a perturbation from \(P^{\perp}\) of any size to an input data point which changes its label is an unwanted phenomenon, because this perturbation is orthogonal to any possible data point from both possible labels. We will later show that under certain assumptions there exists an adversarial perturbation in the direction of \(P^{\perp}\) which also has a small norm. This reason for this assumption is so that the projection of the first layer weights on \(P^{\perp}\) remain fixed during training. An interesting question is to consider general "data manifolds", which we elaborate on in Section 7.

To demonstrate that the low-dimensional data assumption arises in practical settings, in Figure 1 we plot the cumulative variance of the MNIST and CIFAR10 datasets, projected on a linear manifold. These are calculated by performing PCA on the entire datasets, and summing over the square of the singular values from largest to smallest. For CIFAR10, the accumulated variance reaches \(90\%\) at \(98\) components, and \(95\%\) at \(216\) components. For MNIST, the accumulated variance reaches \(90\%\) at \(86\) components, and \(95\%\) at \(153\) components. This indicates that both datasets can be projected to a much smaller linear subspace, without losing much of the information.

**Remark 3.1** (On the Margin of the Network).: _Given a neural network \(N:\mathbb{R}^{d}\rightarrow\mathbb{R}\) and a dataset \((x_{1},y_{1}),\ldots,(x_{r},y_{r})\) with binary labels which the network label correctly, we define the margin of the network as \(\gamma:=\min_{i\in[r]}y_{i}N(x_{i})\)._

_In our setting, it is possible to roughly estimate the margin without assuming much about the data, besides its boundedness. Note that the gradient of the loss decays exponentially with the output of the network, because \(\left|\frac{\partial L(q)}{\partial q}\right|=\left|\frac{-qe^{-q}}{1+e^{-q}}\right|\). Hence, if we train for at most polynomially many iterations and label all the data points correctly (i.e. the margin is larger than \(0\)), then training effectively stops after the margin reaches \(O(\log^{2}(d))\). This is because if the margin is \(\log^{2}(d)\), then the gradient is of size:_

\[\left|L^{\prime}(\log^{2}(d))\right|=\left|\frac{-\log^{2}(d)e^{- \log^{2}(d)}}{1+e^{-\log^{2}(d)}}\right|\leq\log^{2}(d)\cdot d^{-\log(d)}\,\]

_which is smaller than any polynomials in \(d\). This means that all the data points on the margin (which consists of at least one point) will have an output of \(O(\operatorname{polylog}(d))\)._

_The number of points which lie exactly on the margin is difficult to assess, since it may depend on both the dataset and the model. Some empirical results in this direction are given in Vardi et al. (2022), where it is observed (empirically) that for data sampled uniformly from a sphere and trained with a two-layer network, over \(90\%\) of the input samples lie on the margin. Also, in Haim et al. (2022) it is shown that for CIFAR10, a large portion of the dataset lies on the margin._

Figure 1: The cumulative variance for the (a) CIFAR10; and (b) MNIST datasets, calculated by performing PCA on the entire datasets, and summing over the square of the singular values from largest to smallest.

Large Gradient Orthogonal to the Data Subspace

One proxy for showing non-robustness of models, is to show that their gradient w.r.t. the input data is large (cf. Bubeck et al. (2021); Bubeck and Sellke (2021)). Although a large gradient does not guarantee that there is also an adversarial perturbation, it is an indication that a small change in the input might significantly change the output. Moreover, by assuming smoothness of the model, it is possible to show that having a large gradient may suffice for having an adversarial perturbation.

In this section we show that training a network on a dataset which lies entirely on a linear subspace yields a large gradient in a direction which is orthogonal to this subspace. Moreover, the size of the gradient depends on the dimension of this subspace. Specifically, the smaller the dimension of the data subspace, the larger the gradient is in the orthogonal direction. Our main result in this section is the following:

**Theorem 4.1**.: _Suppose that a network \(N(x)=\sum\limits_{i=1}^{m}u_{i}\sigma(\langle w_{i},x\rangle)\) is trained on a dataset which lies on a linear subspace \(P\subset\mathbb{R}^{d}\) of dimension \(d-\ell\) for \(\ell\geq 1\), and let \(x_{0}\in P\). Let \(S=\{i\in[m]:\langle w_{i},x_{0}\rangle\geq 0\}\), and let \(k:=|S|\). Then, \(w_{p}\geq 1-e^{-\ell/16}\) (over the initialization) we have:_

\[\left\|\Pi_{P^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right) \right\|\geq\sqrt{\frac{k\ell}{2md}}\;.\]

The full proof can be found in Appendix B. Here we provide a short proof intuition: First, we use a symmetry argument to show that it suffices to consider w.l.o.g. the subspace \(M:=\text{span}\{e_{1},\dots,e_{d-\ell}\}\), where \(e_{i}\) are the standard unit vectors. Next, we note that since the dataset lies on \(M\), only the first \(d-\ell\) coordinates of each weight vector \(w_{i}\) are trained, while the other \(\ell\) coordinates are fixed at their initial value. Finally, using standard concentration result on Gaussian random variables we can lower bound the norm of the gradient. Note that our result shows that there might be a large gradient orthogonal to the data subspace. This correspond to "off-manifold" adversarial examples, while the full gradient (i.e. without projecting on \(P^{\perp}\)) might be even larger.

The lower bound on the gradient depends on two terms: \(\frac{k}{m}\) and \(\frac{\ell}{d}\). The first term is the fraction of active neurons for the input \(x_{0}\), i.e. the neurons whose inputs are non-negative. Note that inactive neurons do not increase the gradient, since they do not affect the output. The second term corresponds to the fraction of directions orthogonal to the data. The larger the dimension of the orthogonal subspace, the more directions in which it is possible to perturb the input while still being orthogonal to the data. If both of these terms are constant, i.e. there is a constant fraction of active neurons, and "off-manifold" directions, we can give a more concrete bound on the gradient:

**Corollary 4.1**.: _For \(\ell=\Omega(d)\), \(k=\Omega(m)\), in the setting of Theorem 4.1, with probability \(\geq 1-e^{-\Omega(d)}\) we have:_

\[\left\|\Pi_{P^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right) \right\|=\Omega(1)\;.\]

Consider the case where the norm of each data point is \(\Theta(\sqrt{d})=\Theta(\sqrt{\ell})\), i.e. every coordinate is of size \(\Theta(1)\). By Remark 3.1, for a point \(x_{0}\) on the margin, its output is of size \(\operatorname{polylog}(d)\). Therefore, for the point \(x_{0}\), gradient of size \(\Omega(1)\) corresponds to an adversarial perturbation of size \(\operatorname{polylog}(d)\), which is much smaller than \(\|x_{0}\|=\Theta(\sqrt{d})\). We note that this is a rough and informal estimation, since, as we already discussed, a large gradient at \(x_{0}\) does not necessarily imply that an adversarial perturbation exists. In the next section, we will prove the existence of adversarial perturbations.

## 5 Existence of an Adversarial Perturbation

In the previous section we have shown that at any point \(x_{0}\) which lies on the linear subspace of the data \(P\), there is a large gradient in the direction of \(P^{\perp}\). In this section we show that not only the gradient is large, there also exists an adversarial perturbation in the direction of \(P^{\perp}\) which changes the label of a data point from \(P\) (under certain assumptions). The main theorem of this section is the following:

**Theorem 5.1**.: _Suppose that a network \(N(x)=\sum\limits_{i=1}^{m}u_{i}\sigma(\langle w_{i},x\rangle)\) is trained on a dataset which lies on a linear subspace \(P\subseteq\mathbb{R}^{d}\) of dimension \(d-\ell\), where \(\ell\geq 32(m-1)\log(m^{2}d)\). Let \(x_{0}\in P\)_and denote \(y_{0}:=\text{sign}(N(x_{0}))\). Let \(I_{-}:=\{i\in[m]:u_{i}<0\}\) and \(I_{+}:=\{i\in[m]:u_{i}>0\}\), and denote \(k_{-}:=|\{i\in I_{-}:\langle w_{i},x_{0}\rangle\geq 0\}|\), and \(k_{+}:=|\{i\in I_{+}:\langle w_{i},x_{0}\rangle\geq 0\}|\). Let \(k_{y_{0}}=k_{-}\) if \(y_{0}=1\) and \(k_{y_{0}}=k_{+}\) if \(y_{0}=-1\). For \(w\in\mathbb{R}^{d}\) denote \(\hat{w}:=\Pi_{P^{\perp}}(w)\), and denote the perturbation \(z:=y_{0}\cdot\alpha\left(\sum\limits_{i\in I_{-}}\hat{w}_{i}-\sum\limits_{i \in I_{+}}\hat{w}_{i}\right)\) where \(\alpha=\frac{8\sqrt{m}dN(x_{0})}{\ell k_{y_{0}}}\). Then, w.p. \(\geq 1-5(me^{-\ell/16}+d^{-1/2})\) we have that \(\|z\|\leq 8\sqrt{2}N(x_{0})\cdot\frac{m}{k_{y_{0}}}\cdot\sqrt{\frac{d}{\ell}}\) and:_

\[\text{sign}(N(x_{0}+z))\neq\text{sign}(N(x_{0}))\;.\]

The full proof can be found in Appendix C. Here we give a short proof intuition: As in the previous section, we show using a symmetry argument that w.l.o.g. we can assume that \(P=\text{span}\{e_{1},\ldots,e_{d-\ell}\}\).

Now, given the perturbation \(z\) from Theorem 5.1 we want to understand how adding it to the input changes the output. Suppose that \(y_{0}=1\). We can write

\[N(x_{0}+z)=\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+\langle w_{ i},z\rangle)+\sum_{i\in I_{+}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+\langle w _{i},z\rangle)\]

We can see that for all \(i\):

\[\langle w_{i},z\rangle =\alpha\cdot\langle w_{i},\sum_{j\in I_{-}}\Pi_{P^{\perp}}(w_{j}) -\sum_{j\in I_{+}}\Pi_{P^{\perp}}(w_{j})\rangle\] \[=-\alpha\cdot\langle w_{i},\sum_{j=1}^{m}\text{sign}(u_{j})\Pi_{P ^{\perp}}(w_{j})\rangle.\]

For \(i\in I_{-}\) we can write:

\[\langle w_{i},z\rangle=\alpha\,\|\Pi_{P^{\perp}}(w_{i})\|^{2}-\alpha\langle\Pi _{P^{\perp}}(w_{i}),\sum_{j\neq i}\text{sign}(u_{j})\Pi_{P^{\perp}}(w_{j}) \rangle\;,\]

and using a similar calculation, for \(i\in I_{+}\) we can write:

\[\langle w_{i},z\rangle=-\alpha\,\|\Pi_{P^{\perp}}(w_{i})\|^{2}-\alpha\langle\Pi _{P^{\perp}}(w_{i}),\sum_{j\neq i}\text{sign}(u_{j})\Pi_{P^{\perp}}(w_{j}) \rangle\;.\]

Using concentration inequalities of Gaussian random variables, and the fact that \(\Pi_{P^{\perp}}(w_{i})\) did not change from their initial values, we can show that:

\[\left|\langle\Pi_{P^{\perp}}(w_{i}),\sum_{j\neq i}\text{sign}(u_{j})\Pi_{P^{ \perp}}(w_{j})\rangle\right|\approx\frac{\sqrt{\ell m}}{d}\;,\]

while \(\|\Pi_{P^{\perp}}(w_{i})\|^{2}\approx\frac{\ell}{d}\). Thus, for a large enough value of \(\ell\) we have that \(\langle w_{i},z\rangle\leq 0\) for \(i\in I_{+}\) and \(\langle w_{i},z\rangle\approx\alpha\cdot\frac{\sqrt{\ell}}{d}\cdot(\sqrt{\ell }-\sqrt{m})\) for \(i\in I_{-}\).

From the above calculations we can see that adding the perturbation \(z\) does not increase the output of the neurons with a positive second layer. On the other hand, adding \(z\) can only increase the input of the neurons with negative second layer, and for those neurons which are also active it increases their output as well if we assume that \(\ell>m\). This means, that if there are enough active neurons with a negative second layer (denoted by \(k_{-}\) in the theorem), then the perturbation can significantly change the output. In the proof we rely only on the active negative neurons to change the label of the output (for the case of \(y_{0}=1\), if \(y_{0}=-1\) we rely on the active positive neurons). Note that the active positive neurons may become inactive, and the inactive negative neurons may become active. Without further assumptions it is not clear what is the size of the perturbation to make this change for every neuron. Thus, the only neurons that are guaranteed to help change the label are the active negative ones, which by our assumptions on \(\ell\) are guaranteed to increase their output.

Note that our perturbation is _not_ in the direction of the gradient w.r.t. the input. The direction of the gradient would be the sum of all the active neurons, i.e. the sum (with appropriate signs) over all \(i\in[m]\) such that \(\langle w_{i},x_{0}\rangle\geq 0\). Our analysis would not have worked with such a perturbation, because we could not guarantee that inactive neurons would stay inactive.

The assumption that \(\ell=\Omega(M)\) (up to log factors) is a technical limitation of our proof technique. We note that such an assumption is also used in other theoretical papers about adversarial perturbations (e.g. Daniely and Shacham (2020)).

Note that the direction of the perturbation \(z\) does not depend on the input data \(x_{0}\), only its size depends on \(x_{0}\). In fact, Theorem 5.1 shows that there is a single universal direction for an adversarial perturbation that can flip the label of any data point in \(P\). The size of the perturbation depends on the dimension of the linear subspace of the data, the number of active neurons for \(x_{0}\), the total number of neurons in the network and the size of the output. In the following corollary we give a specific bound on the size of the perturbations under assumptions on the different parameters of the problem:

**Corollary 5.1**.: _In the setting of Theorem 5.1, assume in addition that \(\ell=\Theta(d)\) and \(k_{y_{0}}=\Theta(m)\). Then, there exists a perturbation \(z\) such that w.p. \(\geq 1-5\left(me^{-\ell/16}+d^{-1/2}\right)\) we have \(\|z\|=O(N(x_{0}))\) and:_

\[\text{sign}(N(x_{0}+z))\neq\text{sign}(N(x_{0}))\;.\]

The above corollary follows directly by noticing from Theorem 5.1 that:

\[\|z\|\leq O\left(N(x_{0})\cdot\frac{m}{k_{y_{0}}}\cdot\sqrt{\frac{d}{\ell}} \right)=O(N(x_{0}))\;,\]

where we plugged in the additional assumptions. The assumptions in the corollary above are similar to the assumptions in Corollary 4.1. Namely, that the dimension of the data subspace is a constant fraction from the dimension of the entire space, and the number of active neurons is a constant fraction of the total number of neurons. Note that here we only consider active neurons with a specific sign in the second layer.

Note that the size of the perturbation in Corollary 5.1 is bounded by \(N(x_{0})\). By Remark 3.1, the output of the network for data points on the margin can be at most \(O(\log^{2}(d))\), since otherwise the network would have essentially stopped training. Therefore, if we consider an input \(x_{0}\) on the margin, and \(\|x_{0}\|=\Theta(\sqrt{d})=\Theta(\sqrt{\ell})\), then the size of the adversarial perturbation is much smaller than \(\|x_{0}\|\). For any other point, without assuming it is on the margin, and since we do not assume anything about the training data (except for being in \(P\)), we must assume that the size of the perturbation required to change the label will depend on the size of the output.

## 6 The Effects of the Initialization Scale and Regularization on Robustness

In Section 5, we presented the inherent vulnerability of trained models to small perturbations in a direction orthogonal to the data subspace. In this section, we return to a common proxy for robustness that we considered in Section 4 - the gradient at an input point \(x_{0}\). We suggest two ways that might improve the robustness of the model in the direction orthogonal to the data, by decreasing an upper bound of the gradient in this direction. We first upper bound the gradient of the model in the general case where we initialize \(w_{i}\sim\mathcal{N}(\mathbf{0},\beta^{2}I_{d})\), and later discuss strategies to use this upper bound for improving robustness.

**Theorem 6.1**.: _Suppose that a network \(N(x)=\sum\limits_{i=1}^{m}u_{i}\sigma(\langle w_{i},x\rangle)\) is trained on a dataset which lies on a linear subspace \(P\subseteq\mathbb{R}^{d}\) of dimension \(d-\ell\) for \(\ell\geq 1\), and assume that the weights \(w_{i}\) are initialized from \(\mathcal{N}(\mathbf{0},\beta^{2}I_{d})\). Let \(x_{0}\in P\), let \(S=\{i\in[m]:\langle w_{i},x_{0}\rangle\geq 0\}\), and let \(k=|S|\). Then, w.p. \(\geq 1-e^{-\frac{\ell}{16}}\) we have:_

\[\left\|\Pi_{P^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)\right\| \leq\beta\sqrt{\frac{2k\ell}{m}}\;.\]

The full proof uses the same concentration bounds ideas as the lower bound proof and can be found in Appendix D. This bound is a result of the untrained weights: since the projection of the data points on \(P^{\perp}\) is zero, the projection of the weights vectors on \(P^{\perp}\) are not trained and are fixed at their initialization. We note that Theorem 4.1 readily extends to the case of initialization from \(\mathcal{N}(\mathbf{0},\beta^{2}I_{d})\), in which case the lower bound it provides matches the upper bound from Theorem 6.1 up to a constant factor. In what follows, we suggest two ways to affect the post-training weights in the \(P^{\perp}\) direction: (1) To initialize the weights vector using a smaller-variance initialization, and (2) Add an \(L_{2}\)-norm regularization on the weights. We next analyze their effect on the upper bound.

### Small Initialization Variance

From Theorem 6.1, one can conclude a strong result about the model's gradient without the dependency of its norm on \(\ell\) and \(k\).

**Corollary 6.1**.: _For \(\beta=\frac{1}{d\sqrt{2}}\), in the settings of Theorem 6.1, with probability \(\geq 1-e^{-\frac{\ell}{16}}\) we have_

\[\left\|\Pi_{P^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)\right\| \leq\frac{1}{\sqrt{d}}\;.\]

The proof follows directly from Theorem 6.1, by noticing that \(k\leq m\) and \(\ell\leq d\). Consider for example an input \(x_{0}\in P\) with \(\left\|x_{0}\right\|=\Theta(\sqrt{d})\), and suppose that \(N(x_{0})=\Omega(1)\). The above corollary implies that if the initialization has a variance of \(1/d^{2}\) (rather than the standard choice of \(1/d\)) then the gradient is of size \(O\left(1/\sqrt{d}\right)\). Thus, it corresponds to perturbations of size \(\Omega(\sqrt{d})\), which is the same order as \(\left\|x_{0}\right\|\).

### \(L_{2}\) Regularization

We consider another way to influence the projection onto \(P^{\perp}\) of the trained weights vectors: adding \(L_{2}\) regularization while training. We will update the logistic loss function by adding an additive factor \(\frac{1}{2}\lambda\left\|\mathbf{w}_{1:m}\right\|^{2}\). For a dataset \((x_{1},y_{1}),..,(x_{r},y_{r})\), we now train over the following objective:

\[\sum_{j=1}^{r}L(y_{j}\cdot N(x_{j},\mathbf{w}_{1:m})))+\frac{1}{2}\lambda \left\|\mathbf{w}_{1:m}\right\|^{2}\;.\]

This regularization will cause the previously untrained weights to decrease in each training step which will decrease the upper bound on the projection of the gradient:

**Theorem 6.2**.: _Suppose that a network \(N(x)=\sum\limits_{i=1}^{m}u_{i}\sigma(\langle w_{i},x\rangle)\) is trained for \(T\) training steps, using \(L_{2}\) regularization with parameter \(\lambda\geq 0\) and step size \(\eta>0\), on a dataset which lies on a linear subspace \(P\subseteq\mathbb{R}^{d}\) of dimension \(d-\ell\) for \(\ell\geq 1\), starting from standard initialization (i.e., \(w_{i}\sim\mathcal{N}(0,\frac{1}{d}I_{d})\)). Let \(x_{0}\in P\), let \(S=\{i\in[m]:\langle w_{i},x_{0}\rangle\geq 0\}\), and let \(k:=\left|S\right|\). Then, w.p. \(\geq 1-e^{-\frac{\ell}{16}}\) we have_

\[\left\|\Pi_{P^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)\right\| \leq(1-\eta\lambda)^{T}\sqrt{\frac{2k\ell}{md}}\;.\]

The full proof can be found in Appendix D.1. The main idea of the proof is to observe the projection of the weights on \(P^{\perp}\) changing during training. As before, we assume w.l.o.g. that \(P=\text{span}\{e_{1},\ldots,e_{d-\ell}\}\) and denote by \(\hat{w}_{i}:=\Pi_{P^{\perp}}(w_{i})\). During training, the weight vector's last \(\ell\) coordinates are only affected by the regularization term of the loss. These weights decrease in a constant multiplaction of the previous weights. Thus, we can conclude that for every \(t\geq 0\) we have: \(\hat{w}_{i}^{(t)}=(1-\eta\lambda)^{t}\hat{w}_{i}^{(0)}\), where \(\hat{w}_{i}^{(t)}\) is the \(i\)-th weight vector at time \(t\). It implies that our setting is equivalent to initializing the weights with standard deviation \(\frac{(1-\eta\lambda)^{T}}{\sqrt{d}}\) and training the model without regularization for \(T\) steps. As a result, we get the following corollary:

**Corollary 6.2**.: _For \((1-\eta\lambda)^{T}\leq\frac{1}{\sqrt{2d}}\), in the settings of 6.2, w.p. \(\geq 1-e^{-\frac{\ell}{16}}\) we get that:_

\[\left\|\Pi_{P^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)\right\| \leq\frac{1}{\sqrt{d}}\;.\]

### Experiments

In this section, we present our robustness-improving experiments. 4 We explore our methods on two datasets: (1) A 7-point dataset on a one-dimensional linear subspace in a two-dimensional inputspace, and (2) A 25-point dataset on a two-dimensional linear subspace in a three-dimensional input space. In Figures 2 and 3 we present the boundary of a two-layer ReLU network trained over these two datasets. We train the networks until reaching a constant positive margin. We note that unlike our theoretical analysis, in the experiments in Figure 2 we trained all layers and initialize the weights using the default PyTorch initialization, to verify that the observed phenomena occur also in this setting. In the experiment in Figure 3 we use a different initialization scale for the improving effect to be smaller and visualized easily. In Figures 1(a) and 2(a) we trained with default settings. In Figures 1(b) and 2(b) we initialized the weights using an initialization with a smaller variance (i.e., initialization divided by a constant factor). Finally, in Figures 1(c) and 2(c) we train with \(L_{2}\) regularization.

Consider the adversarial perturbation in the direction \(P^{\perp}\), orthogonal to the data subspace, in Figures 2 and 3. In figure (a) of each experiment, we can see that a rather small adversarial perturbation is needed to cross the boundary in the subspace orthogonal to the data. In the middle figure (b), we see that the boundary in the orthogonal subspace is much further. This is a direct result of the projection of the weights onto this subspace being much smaller. In the right experiment (c), we can see a similar effect created by regularization. In Appendix E we add the full default-scaled experiment in the two-dimensional setting to demonstrate the robustness effect. There, in both the small-initialization and regularization experiments, the boundary lines are almost orthogonal to the data subspace. In Appendix E we also conduct further experiments with deeper networks and standard PyTorch initialization, showing that our theoretical results are also observed empirically in settings going beyond our theory.

In Figure 4 we plot the distance from the decision boundary for different initialization scales of the first layer. We trained a \(3\)-layer network, initialized using standard initialization except for the first layer which is divided by the factor represented in the \(X\)-axis. After training, we randomly picked \(200\) points and used a standard projected gradient descent adversarial attack to change the label of

Figure 3: **Experiments on two-dimensional dataset demonstrating a smaller robustness effect.** We plot the dataset points and the decision boundary in 3 settings: (a) Vanilla trained network, (b) The network’s weights are initialized from a smaller variance distribution, and (c) Training with regularization. Colors are used to emphasise the values in the \(z\) axis.

Figure 2: **Experiments on a one-dimensional dataset.** We plot the dataset points and the decision boundary in 3 settings: (a) Vanilla trained network, (b) The network’s weight are initialized from a smaller variance distribution, and (c) Training with regularization.

each point, which is described in the \(Y\)-axis (perturbation norm, with error bars). The datasets are: (a) Random points from a sphere with \(28\) dimensions, which lies in a space with \(784\) dimensions; and (b) MNIST, where the data is projected on \(32\) dimensions using PCA. The different lines are adversarial attacks projected either on data subspace, on its orthogonal subspace, or without projection. It can be seen that small initialization increases robustness off the data subspace, and also on the non-projected attack, while having almost no effect for the attacks projected on the data subspace.

## 7 Conclusions and Future Work

In this paper we considered training a two-layer network on a dataset lying on \(P\subseteq\mathbb{R}^{d}\) where \(P\) is a \(d-\ell\) dimensional subspace. We have shown that the gradient of any point \(x_{0}\in P\) projected on \(P^{\perp}\) is large, depending on the dimension of \(P\) and the fraction of active neurons on \(x_{0}\). We additionally showed that there exists an adversarial perturbation in the direction of \(P^{\perp}\). The size of the perturbation depends in addition on the output of the network on \(x_{0}\), which by Remark 3.1 should be poly-logarithmic in \(d\), at least for points which lie on the margin of the network. Finally, we showed that by either decreasing the initialization scale or adding \(L_{2}\) regularization we can make the network robust to "off-manifold" perturbations, by decreasing the gradient in this direction.

One interesting question is whether our results can be generalized to other manifolds, beyond linear subspaces. We state this as an informal open problem:

**Open Problem 7.1**.: _Let \(M\) be a manifold, and \(\mathcal{D}\) a distribution over \(M\times\{\pm 1\}\). Suppose we train a network \(N:\mathbb{R}^{d}\rightarrow\mathbb{R}\) on a dataset sampled from \(\mathcal{D}\). Let \(x_{0}\in M\), then under what conditions on \(M,\ \mathcal{D}\) and \(N\), there exists a small adversarial perturbation in the direction of \(\left(T_{x_{0}}M\right)^{\perp}\), i.e. orthogonal to the tangent space \(T_{x_{0}}M\), of \(M\) at \(x_{0}\)._

Our result can be seen as a special case of this conjecture, where at all points \(x,x^{\prime}\in M\), the tangent spaces are equal \(T_{x}M=T_{x^{\prime}}M\). Another future direction would be to analyze deeper networks, or different architectures such as convolutions. Finally, it would also be interesting to analyze robustness of trained networks w.r.t. different norms such as \(L_{1}\) or \(L_{\infty}\).

## Acknowledgments and Disclosure of Funding

We thank Ohad Shamir for the many helpful discussions about this work. We would also like to thank Michal Irani for contributing computational resources. GY was supported in part by the European Research Council (ERC) grant 754705. GV acknowledges the support of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning.

Figure 4: **The distance to the decision boundary for different initializations of the first layer. The \(X\)-axis represents the factor which the initialization of the first layer is divided by. The \(Y\)-axis shows the size of a standard perturbed gradient descent adversarial attack to change the label of each point for \(200\) randomly picked points. The datasets are: (a) Random points from a sphere with \(28\) dimensions, which lies in a space with \(784\) dimensions; and (b) MNIST, where the data is projected on \(32\) dimensions using PCA. The different lines are adversarial attacks projected either on data subspace, on its orthogonal subspace, or without projection.**

## References

* Allen-Zhu and Li (2020) Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust deep learning. _Preprint, arXiv:2005.10190_, 2020.
* Athalye et al. (2018) Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In _International conference on machine learning_, pages 274-283. PMLR, 2018.
* Bartlett et al. (2021) Peter Bartlett, Sebastien Bubeck, and Yeshwanth Cherapanamjeri. Adversarial examples in multi-layer random relu networks. _Advances in Neural Information Processing Systems_, 34, 2021.
* Bubeck and Sellke (2021) Sebastien Bubeck and Mark Sellke. A universal law of robustness via isoperimetry. _Advances in Neural Information Processing Systems_, 34:28811-28822, 2021.
* Bubeck et al. (2019) Sebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from computational constraints. In _International Conference on Machine Learning_, pages 831-840. PMLR, 2019.
* Bubeck et al. (2021a) Sebastien Bubeck, Yeshwanth Cherapanamjeri, Gauthier Gidel, and Remi Tachet des Combes. A single gradient step finds adversarial examples on random two-layers neural networks. _Advances in Neural Information Processing Systems_, 34, 2021a.
* Bubeck et al. (2021b) Sebastien Bubeck, Yuanzhi Li, and Dheeraj M Nagaraj. A law of robustness for two-layers neural networks. In _Conference on Learning Theory_, pages 804-820. PMLR, 2021b.
* Carlini and Wagner (2017) Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In _Proceedings of the 10th ACM workshop on artificial intelligence and security_, pages 3-14, 2017.
* Daniely and Shacham (2020) Amit Daniely and Hadas Shacham. Most relu networks suffer from \(\ell_{2}\) adversarial perturbations. _Advances in Neural Information Processing Systems_, 33, 2020.
* Dohmatob and Bietti (2022) Elvis Dohmatob and Alberto Bietti. On the (non-) robustness of two-layer neural networks in different learning regimes. _arXiv preprint arXiv:2203.11864_, 2022.
* Fawzi et al. (2018) Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. _Preprint, arXiv:1802.08686_, 2018.
* Ge et al. (2021) Songwei Ge, Vasu Singla, Ronen Basri, and David Jacobs. Shift invariance can reduce adversarial robustness. _Preprint, arXiv:2103.02695_, 2021.
* Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014_, pages 2672-2680, 2014a. URL https://proceedings.neurips.cc/paper/2014/hash/5ca3e5b122f61f8f06494c97blafccf3-Abstract.html.
* Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _Preprint, arXiv:1412.6572_, 2014b.
* Haim et al. (2022) Niv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir, and Michal Irani. Reconstructing training data from trained neural networks. _arXiv preprint arXiv:2206.07758_, 2022.
* He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _Proceedings of the IEEE international conference on computer vision_, pages 1026-1034, 2015.
* Jalal et al. (2017) Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros G Dimakis. The robust manifold defense: Adversarial training using generative models. _arXiv preprint arXiv:1712.09196_, 2017.
* Khoury and Hadfield-Menell (2018) Marc Khoury and Dylan Hadfield-Menell. On the geometry of adversarial examples. _Preprint, arXiv:1811.00525_, 2018.
* Krizhevsky et al. (2014)* Kingma and Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* 1338, 2000. doi: 10.1214/aos/1015957395. URL https://doi.org/10.1214/aos/1015957395.
* Madry et al. (2017) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. _Preprint, arXiv:1706.06083_, 2017.
* Meng and Chen (2017) Dongyu Meng and Hao Chen. Magnet: a two-pronged defense against adversarial examples. 2017.
* Montanari and Wu (2022) Andrea Montanari and Yuchen Wu. Adversarial examples in random neural networks with general activations. _arXiv preprint arXiv:2203.17209_, 2022.
* Papernot et al. (2016) Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In _2016 IEEE symposium on security and privacy (SP)_, pages 582-597. IEEE, 2016.
* Papernot et al. (2017) Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In _Proceedings of the 2017 ACM on Asia conference on computer and communications security_, pages 506-519, 2017.
* Samangouei et al. (2018) Pouya Samangouei, Maya Kabkab, and Rama Chellappa. Defense-gan: Protecting classifiers against adversarial attacks using generative models. _arXiv preprint arXiv:1805.06605_, 2018.
* Schmidt et al. (2018) Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. _Preprint, arXiv:1804.11285_, 2018.
* Shafahi et al. (2018) Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial examples inevitable? _Preprint, arXiv:1809.02104_, 2018.
* Shah et al. (2020) Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. _Preprint, arXiv:2006.07710_, 2020.
* Shamir et al. (2019) Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkelman. A simple explanation for the existence of adversarial examples with small hamming distance. _Preprint, arXiv:1901.10861_, 2019.
* Shamir et al. (2021) Adi Shamir, Odelia Melamed, and Oriel BenShmuel. The dimpled manifold model of adversarial examples in machine learning. _Preprint, arXiv:2106.10151_, 2021.
* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Stutz et al. (2019) David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6976-6987, 2019.
* Szegedy et al. (2013) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. _Preprint, arXiv:1312.6199_, 2013.
* Tanay and Griffin (2016) Thomas Tanay and Lewis Griffin. A boundary tilting persepective on the phenomenon of adversarial examples. _arXiv preprint arXiv:1608.07690_, 2016.
* Vardi et al. (2022) Gal Vardi, Gilad Yehudai, and Ohad Shamir. Gradient methods provably converge to non-robust networks. _Preprint, arXiv:2202.04347_, 2022.
* Wang et al. (2020) Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain the generalization of convolutional neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8684-8694, 2020.
* Wang et al. (2022) Yunjuan Wang, Enayat Ullah, Poorya Mianjy, and Raman Arora. Adversarial robustness is at odds with lazy training. _arXiv preprint arXiv:2207.00411_, 2022.
* Wong and Kolter (2018) Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In _International Conference on Machine Learning_, pages 5286-5295. PMLR, 2018.
* Wang et al. (2019)Rotation Invariance w.r.t. the Initialized Weights

In this paper, we analyze neural networks trained on high-dimensional data that lies on a low dimensional linear subspace denoted by \(P\). We assume that the dimension of \(P\) is \(d-\ell\). Throughout the paper it will be more convenient to analyze data which lies on the subspace \(M=\text{span}(\{e_{1},\dots,e_{d-\ell}\})\), because then the "off manifold" directions correspond exactly to certain coordinates of the input. In this section we show that we can essentially analyze the data as if it is rotated to lie on \(M\), and it would imply the same consequences as the original data from \(P\).

**Theorem A.1**.: _Let \(P\subseteq\mathbb{R}^{d}\) be a subspace of dimension \(d-\ell\), and let \(M=\text{span}\{e_{1},\dots,e_{d-\ell}\}\). Let \(R\) be an orthogonal matrix such that \(R\cdot P=M\), let \(X\subseteq P\) be a training dataset and let \(X_{R}=\{R\cdot x:x\in X\}\). Assume we train a neural network \(N(x)=\sum_{i=1}^{m}u_{i}\sigma(w_{i}^{\dagger}x)\) as explained in Section 3, and denote by \(N^{X}\) and \(N^{X_{R}}\) the network trained on \(X\) and \(X_{R}\) respectively for the same number of iterations. Let \(x_{0}\in P\), then we have:_

1. _W.p._ \(p\) _(over the initialization) we have_ \(\left\|\Pi_{P^{\perp}}\left(\frac{\partial N^{X}(x_{0})}{\partial x}\right) \right\|\geq c\) _(resp._ \(\leq c\)_) for some_ \(c\in\mathbb{R}\)_, iff w.p._ \(p\) _also_ \(\left\|\Pi_{M^{\perp}}\left(\frac{\partial N^{X}(Rx_{0})}{\partial x}\right) \right\|\geq c\) _(resp._ \(\leq c\)_)._
2. _For any_ \(c,p\geq 0\)_, w.p._ \(p\) _(over the initialization) there exists_ \(z\in P^{\perp}\) _with_ \(\|z\|=c\) _such that_ \(\mathrm{sign}(N^{X}(x_{0}+z))\neq\mathrm{sign}(N^{X}(x_{0}))\)_, iff w.p._ \(p\) _there exists_ \(z^{\prime}\in M^{\perp}\) _with_ \(\|z^{\prime}\|=c\) _such that_ \(\mathrm{sign}(N^{X_{R}}(Rx_{0}+z^{\prime}))\neq\mathrm{sign}(N^{X_{R}}(Rx_{0}))\)_._

Proof.: Denote by \(\mathbf{w}_{1:m}:=(w_{1},\dots,w_{m})\) and by \(R\mathbf{w}_{1:m}=(Rw_{1},\dots,Rw_{m})\). Let \(\mathbf{w}_{1:m}^{(t)}\) the weights of the network trained on the dataset \(X\) where \(\mathbf{w}_{1:m}^{(0)}\) is some initialization, and \(\tilde{\mathbf{w}}_{1:m}^{(t)}=(\tilde{w}_{1}^{(t)},\dots,\tilde{w}_{m}^{(t)})\) the weights of the network trained on \(X_{R}\) and initialized at \(R\mathbf{w}_{1:m}^{(0)}\). In the proof, when taking derivatives w.r.t. the \(w_{i}\)'s we will explicitly write \(N(x,\mathbf{w}_{1:m})\).

We first show by induction on the number of training steps that \(\tilde{\mathbf{w}}_{1:m}^{(t)}=R\mathbf{w}_{1:m}^{(t)}\). For \(t=0\) it is clear by the assumption on the initialization. Assume it is true for \(t\), then we have for some \(x\in X\):

\[\frac{\partial N(Rx,\tilde{\mathbf{w}}_{1:m}^{(t)})}{\partial w_ {i}} = u_{i}\sigma^{\prime}((\tilde{w}_{i}^{(t)},Rx))Rx\] \[= u_{i}\sigma^{\prime}((Rw_{i}^{(t)},Rx))Rx\] \[= u_{i}\sigma^{\prime}((w_{i}^{(t)},x))Rx\] \[= R\cdot\frac{\partial N(x,\mathbf{w}_{1:m}^{(t)})}{\partial w_ {i}}\;.\]

This is true for every \(i\in[m]\) and for every \(x\in X\). Also note that by our induction assumption we have:

\[N(x,\mathbf{w}_{1:m}^{(t)})=\sum_{i=1}^{m}u_{i}\sigma(\langle w_{i}^{(t)},x \rangle)=\sum_{i=1}^{m}u_{i}\sigma(\langle Rw_{i}^{(t)},Rx\rangle)=N(Rx,\tilde {\mathbf{w}}_{1:m}^{(t)})\;.\] (1)

Finally, the derivative of the loss on a single data point \(x\in X\) with label \(y\) can be written as:

\[\frac{\partial L\left(N(x,\mathbf{w}_{1:m}^{(t)})\cdot y\right)}{\partial w_ {i}}=L^{\prime}\left(N(x,\mathbf{w}_{1:m}^{(t)})\cdot y\right)\cdot\frac{ \partial N(x,\mathbf{w}_{1:m}^{(t)})}{\partial w_{i}}\;,\]

where the first term depends only on the value of \(N(x,\mathbf{w}_{1:m}^{(t)})\). Hence, taking a single gradient step of \(N\) with weights \(\mathbf{w}_{1:m}^{(t)}\) and dataset \(X\) will change the weights by the same term up to multiplication by \(R\) as if taking a gradient step with with weights \(\tilde{\mathbf{w}}_{1:m}^{(t)}\) and dataset \(X_{R}\). This finishes the induction.

Let \(\mathbf{w}_{1:m}^{(0)}\) be an initialization for the training of \(N^{X}\), where there exists \(z\in P^{\perp}\) with \(\|z\|=c\) such that \(\mathrm{sign}(N^{X}(x_{0}+z))\neq\mathrm{sign}(N^{X}(x_{0}))\). Then, by Eq. (1) the initialization \(R\mathbf{w}_{1:m}^{(0)}\) for the training of \(N^{X_{R}}\) is such that for \(z^{\prime}=Rz\) we have \(\|z^{\prime}\|=c\) and \(\mathrm{sign}(N^{X_{R}}(Rx_{0}+z^{\prime}))\neq\mathrm{sign}(N^{X_{R}}(Rx_{0}))\). This argument holds also in the opposite direction. Let \(A\subseteq\{\mathbf{w}_{1:m}\in\mathbb{R}^{d\cdot m}\}\) be the set of all initializations to \(N^{X}\) where there exists \(z\in P^{\perp}\) with \(\|z\|=c\) such that \(\operatorname{sign}(N^{X}(x_{0}+z))\neq\operatorname{sign}(N^{X}(x_{0}))\), then by the above the set \(R\cdot A=\{R\mathbf{w}_{1:m}:\mathbf{w}_{1:m}\in A\}\) are exactly all the initializations to \(N^{X_{R}}\) where there exists \(z^{\prime}\in M^{\perp}\) with \(\|z^{\prime}\|=c\) such that \(\operatorname{sign}(N^{X_{R}}(Rx_{0}+z^{\prime}))\neq\operatorname{sign}(N^{X _{R}}(Rx_{0}))\). Since we initialize the \(w_{i}\)'s using a Gaussian initialization which is spherically symmetric, we have that \(\Pr(A)=\Pr(RA)\). This proves item (2). Item (1) follows from similar arguments (which we do not repeat for conciseness). 

Under the assumption that the data lies on \(M=\text{span}\{e_{1},\dots,e_{d-\ell}\}\), and no regularization is used, we can show that the weights of the first layer projected on \(M^{\perp}\) do not change during training. This is an essential part of the proofs, as it allows us to analyze those weights as random Gaussian vectors, and apply concentration bounds on them.

**Theorem A.2**.: _Let \(M=\text{span}\{e_{1},\dots,e_{d-\ell}\}\). Assume we train a neural network \(N(x,\mathbf{w}_{1:m}):=\sum_{i=1}^{m}u_{i}\sigma(w_{i}^{\top}x)\) as explained in Section 3 (where \(\mathbf{w}_{1:m}=(w_{1},\dots,w_{m})\)). Denote by \(\hat{w}:=\Pi_{M^{\perp}}(w)\) for \(w\in\mathbb{R}^{d}\), then after training, for each \(i\in[m]\), \(\hat{w}_{i}\) did not change from their initial value._

Proof.: Note that for each \(i\in[m]\) and \(x\in M\) we have:

\[\Pi_{M^{\perp}}\left(\frac{\partial N(x,\mathbf{w}_{1:m})}{\partial w_{i}} \right) =\Pi_{M^{\perp}}\left(u_{i}\sigma^{\prime}(w_{i}^{\top}x)x\right)=u _{i}\sigma^{\prime}(w_{i}^{\top}x)\hat{x}=\mathbf{0}\;.\]

Taking the derivative of the loss we have:

\[\Pi_{M^{\perp}}\left(\frac{\partial L\left(N(x,\mathbf{w}_{1:m}) \cdot y\right)}{\partial w_{i}}\right) =\Pi_{M^{\perp}}\left(L^{\prime}\left(N(x,\mathbf{w}_{1:m})\cdot y \right)\cdot\frac{\partial N(x,\mathbf{w}_{1:m})}{\partial w_{i}}\right)\] \[=L^{\prime}\left(N(x,\mathbf{w}_{1:m})\cdot y\right)\cdot\Pi_{M^ {\perp}}\left(\frac{\partial N(x,\mathbf{w}_{1:m})}{\partial w_{i}}\right)= \mathbf{0}\;.\]

The above calculation did not depend on the specific value of the \(w_{i}\)'s. Hence, the value of the \(\hat{w}_{i}\)'s for every \(i\in[m]\) did not change during training from their initial value. 

## Appendix B Proofs from Section 4

Before proving the main theorem, we will first need the next two lemmas about the concentration of Gaussian random variables:

**Lemma B.1**.: _Let \(w\in\mathbb{R}^{n}\) such that \(w\sim\mathcal{N}(\mathbf{0},\sigma^{2}I_{n})\). Then:_

\[\mathbb{P}\left[\left\|w\right\|^{2}\leq\frac{1}{2}\sigma^{2}n\right]\leq e^{- \frac{n}{16}}\;.\]

Proof.: Note that \(\left\|\frac{w}{\sigma}\right\|^{2}\) has the Chi-squared distribution. A concentration bound by Laurent and Massart (Laurent and Massart, 2000, Lemma 1) implies that for all \(t>0\) we have

\[\Pr\left[n-\left\|\frac{w}{\sigma}\right\|^{2}\geq 2\sqrt{nt}\right]\leq e^{-t}\;.\]

Plugging-in \(t=\frac{n}{16}\), we get

\[\Pr\left[n-\left\|\frac{w}{\sigma}\right\|^{2}\geq\frac{1}{2}n\right]=\Pr \left[\left\|\frac{w}{\sigma}\right\|^{2}\leq\frac{1}{2}n\right]\leq e^{-n/16}\;.\]

Thus, we have

\[\Pr\left[\left\|w\right\|\leq\sigma\sqrt{\frac{n}{2}}\right]\leq e^{-n/16}\;.\]

**Lemma B.2**.: _Let \(w_{1},\dots,w_{m}\in\mathbb{R}^{n}\) such that for all \(i\in[m]\), \(w_{i}\sim\mathcal{N}(\mathbf{0},\sigma^{2}I_{n})\), then we have:_

\[\mathbb{P}\left[\left\|\sum_{i=1}^{m}w_{i}\right\|^{2}\leq\frac{1}{2}m\sigma^ {2}n\right]\leq e^{-\frac{n}{16}}\;.\]Proof.: We denote the \(j\)-th coordinate of the vector \(w_{i}\in\mathbb{R}^{n}\) by \(w_{i,j}\). Note, for any \(i\in\{1,\dots,m\}\) and \(j\in\{1,\dots,n\}\) we have \(w_{i,j}\sim\mathcal{N}(0,\sigma^{2})\). We denote by \(s\) the sum vector \(s:=\sum\limits_{i=1}^{m}w_{i}\), and by \(s_{j}\) the \(j\)- th coordinate of \(s\). By this definition, \(s_{j}=\sum\limits_{i=1}^{m}w_{i,j}\) is a sum of \(m\) independent Gaussian variables and therefore also a Gaussian variable. Particularly, \(s\sim\mathcal{N}(\mathbf{0},m\sigma^{2}I_{n})\). We use Lemma B.1 with variance \(m\sigma^{2}\) and get that:

\[\mathbb{P}\left[\left\|\sum\limits_{i=1}^{m}w_{i}\right\|^{2}\leq\frac{1}{2}m \sigma^{2}n\right]\leq e^{-\frac{n}{16}}\.\]

We are now ready to prove the main theorem of this section:

Proof of Theorem 4.1.: Let \(M=\text{span}\{e_{1},\dots,e_{d-\ell}\}\). By Theorem A.1(1), given a training dataset \(X\subseteq P\), it is enough to consider a training set \(X_{R}=\{Rx:x\in X\}\), where \(R\) is an orthogonal matrix such that \(R\cdot P=M\), and training is done over \(X_{R}\). From now on, we assume that the training data, as well as \(x_{0}\) lie on \(M\), and the consequences of this proof would also imply for a dataset \(X\) and \(x_{0}\in P\).

The projection of the gradient on \(M^{\perp}\) is equal to:

\[\Pi_{M^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)=\Pi_{M^{ \perp}}\left(\sum\limits_{i=1}^{m}u_{i}w_{i}\mathbbm{1}_{\left\langle w_{i},x _{0}\right\rangle\geq 0}\right)=\sum\limits_{i=1}^{m}\Pi_{M^{\perp}}\left(u_{i}w_{ i}\right)\mathbbm{1}_{i\in S}=\sum\limits_{i\in S}\Pi_{M^{\perp}}\left(u_{i}w_{i} \right)\.\]

Denote by \(\hat{w}_{i}=(w_{i})_{d-\ell+1:d}\), the last \(\ell\) coordinates of \(w_{i}\). By Theorem A.2 we get that for every \(i\in[m]\), \(\hat{w}_{i}\) did not change from their initial value during training.

Recall that we initialized \(\hat{w}_{i}\sim\mathcal{N}(\mathbf{0},\frac{1}{\sqrt{d}}I_{\ell})\). Note that the set \(S\) is independent of the value of the \(\hat{w}_{i}\)'s. This is because \(\hat{w}_{i}\) does not effect the training, hence will not effect \(w_{i}-\Pi_{M^{\perp}}(w_{i})\). Also, after choosing \(x_{0}\) we have \(\langle\hat{w}_{i},\hat{x}_{0}\rangle=0\), since \(\hat{x}_{0}=\mathbf{0}\), which means that the choice of \(S\) is independent of the \(\hat{w}_{i}\)'s. We can conclude that the random variables \(\hat{w}_{i}\) for \(i\in S\) are sampled independently.

Note, since for all \(i\in\{1,\dots,m\}\), \(|u_{i}|=\frac{1}{\sqrt{m}}\) and they are not trained, we get that \(u_{i}\hat{w}_{i}\) are also Gaussian random variables with the same mean, and variance multiplied by \(\frac{1}{m}\). Therefore, from Lemma B.2 we get that w.p. \(\geq 1-e^{-\ell/16}\):

\[\left\|\sum\limits_{i\in S}u_{i}\hat{w}_{i}\right\|\geq\sqrt{\frac{1}{2}}\sqrt {\frac{kl}{dm}}\.\]

Combining the above, we get:

\[\left\|\Pi_{M^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)\right\| \geq\sqrt{\frac{1}{2}}\sqrt{\frac{kl}{dm}}\.\]

## Appendix C Proofs from Section 5

Before proving the main theorem, we prove a few lemmas about concentration of Gaussian random variables:

**Lemma C.1**.: _Let \(w\in\mathbb{R}^{n}\) with \(w\sim\mathcal{N}(\mathbf{0},\sigma^{2}I_{n})\). Then:_

\[\Pr\left[\left\|w\right\|^{2}\geq 2\sigma^{2}n\right]\leq e^{-\frac{n}{16}}\.\]Proof.: Note that \(\left\|\frac{w}{\sigma}\right\|^{2}\) has the Chi-squared distribution. A concentration bound by Laurent and Massart (Laurent and Massart, 2000, Lemma 1) implies that for all \(t>0\) we have

\[\Pr\left[\left\|\frac{w}{\sigma}\right\|^{2}-n\geq 2\sqrt{nt}+2t\right]\leq e^{- t}\;.\]

Plugging-in \(t=\frac{n}{16}\), we get

\[\Pr\left[\left\|\frac{w}{\sigma}\right\|^{2}\geq 2n\right]\leq\Pr\left[ \left\|\frac{w}{\sigma}\right\|^{2}-n\geq n/2+n/8\right]\leq e^{-n/16}\;.\]

Thus, we have

\[\Pr\left[\left\|w\right\|\geq\sigma\sqrt{2n}\right]\leq e^{-n/16}\;.\]

**Lemma C.2**.: _Let \(u\in\mathbb{R}^{n}\), and \(v\sim\mathcal{N}(\mathbf{0},\sigma^{2}I_{n})\). Then, for every \(t>0\) we have_

\[\Pr\left[\left|\left\langle u,v\right\rangle\right|\geq\left\|u\right\|t \right]\leq 2\exp\left(-\frac{t^{2}}{2\sigma^{2}}\right)\;.\]

Proof.: We first consider \(\langle\frac{u}{\left\|u\right\|},v\rangle\). As the distribution \(\mathcal{N}(\mathbf{0},\sigma^{2}I_{n})\) is rotation invariant, one can rotate \(u\) and \(v\) to get \(\tilde{u}\) and \(\tilde{v}\) such that \(\frac{\tilde{u}}{\left\|u\right\|}=e_{1}\), the first standard basis vector and \(\langle\frac{u}{\left\|u\right\|},v\rangle=\langle\frac{\tilde{u}}{\left\|u \right\|},\tilde{v}\rangle\). Note, \(v\) and \(\tilde{v}\) have the same distribution. We can see that \(\langle\frac{\tilde{u}}{\left\|u\right\|},\tilde{v}\rangle\sim\mathcal{N}(0, \sigma^{2})\) since it is the first coordinate of \(\tilde{v}\). By a standard tail bound, we get that for \(t>0\):

\[\Pr\left[\left|\left\langle\frac{u}{\left\|u\right\|},v\right\rangle\right| \geq t\right]=\Pr\left[\left|\left\langle\frac{\tilde{u}}{\left\|u\right\|}, \tilde{v}\right\rangle\right|\geq t\right]=\Pr\left[\left|\tilde{v}_{1}\right| \geq t\right]\leq 2\exp\left(-\frac{t^{2}}{2\sigma^{2}}\right)\;.\]

Therefore

\[\Pr\left[\left|\left\langle u,v\right\rangle\right|\geq\left\|u\right\|t \right]\leq 2\exp\left(-\frac{t^{2}}{2\sigma^{2}}\right)\;.\]

**Lemma C.3**.: _Let \(u\sim\mathcal{N}(\mathbf{0},\sigma_{1}^{2}I_{n})\), and \(v\sim\mathcal{N}(\mathbf{0},\sigma_{2}^{2}I_{n})\). Then, for every \(t>0\) we have_

\[\Pr\left[\left|\left\langle u,v\right\rangle\right|\geq\sigma_{1}\sqrt{2n}t \right]\leq e^{-n/16}+2e^{-t^{2}/2\sigma_{2}^{2}}\;.\]

Proof.: Using Lemma C.1 we get that w.p. \(\leq e^{-n/16}\) we have \(\left\|u\right\|\geq\sigma_{1}\sqrt{2n}\). Moreover, by Lemma C.2, w.p. \(\leq 2\exp\left(-\frac{t^{2}}{2\sigma_{2}^{2}}\right)\) we have \(\left|\left\langle u,v\right\rangle\right|\geq\left\|u\right\|t\). By the union bound, we get

\[\Pr\left[\left|\left\langle u,v\right\rangle\right|\geq\sigma_{1}\sqrt{2n}t \right]\leq\Pr\left[\left\|u\right\|\geq\sigma_{1}\sqrt{2n}\right]+\Pr\left[ \left\|\left\langle u,v\right\rangle\right|\geq\left\|u\right\|t\right]\leq e ^{-n/16}+2\exp\left(-\frac{t^{2}}{2\sigma_{2}^{2}}\right)\;.\]

We are now ready to prove the main theorem of this section:

**Theorem 5.1**.: By Theorem A.1(2), we can assume w.l.o.g. that \(P=M=\text{span}\{e_{1},\ldots,e_{d-\ell}\}\). We also assume w.l.o.g. that \(y_{0}=1\), the case \(y_{0}=-1\) is proved in a similar manner. Denote by \(\bar{w}:=(w)_{d-\ell+1:d}\), the last \(\ell\) coordinates of \(w\). By Theorem A.2 we have that \(\bar{w}_{i}\) have not changed after training from their initial value.

We can write \(N(x_{0}+z)\) as:\[N(x_{0}+z) =\sum_{i=1}^{m}u_{i}\sigma(\langle w_{i},x_{0}\rangle+\langle w_{i}, z\rangle)\] \[=\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+\langle w _{i},z\rangle)+\sum_{i\in I_{+}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+\langle w _{i},z\rangle)\] \[=\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+\langle \bar{w}_{i},\bar{z}\rangle)+\sum_{i\in I_{+}}u_{i}\sigma(\langle w_{i},x_{0} \rangle+\langle\bar{w}_{i},\bar{z}\rangle)\] (2)

where the last equality is since \((z)_{1:d-\ell}=\mathbf{0}\), hence \(\langle w,z\rangle=\langle\bar{w},\bar{z}\rangle\) for every \(w\in\mathbb{R}^{d}\). We will bound each term of the above separately.

For the first term in Eq. (2), where \(i\in I_{-}\) we can write:

\[\langle\bar{w}_{i},\bar{z}\rangle=\alpha\,\|\bar{w}_{i}\|^{2}+\alpha\langle \bar{w}_{i},\sum_{j\neq i}\text{sign}(u_{j})\bar{w}_{j}\rangle\;.\]

By our assumptions, \(\bar{w}_{i}\sim\mathcal{N}\left(\mathbf{0},\frac{1}{d}I_{\ell}\right)\) and \(\sum_{j\neq i}\text{sign}(u_{j})\bar{w}_{j}\sim\mathcal{N}\left(\mathbf{0}, \frac{m-1}{d}I_{\ell}\right)\), since it is a sum of \(m-1\) i.i.d. Gaussian random variables, which are also symmetric hence multiplying them by \(-1\) does not change their distribution. From Lemma B.1 we get w.p. \(\geq 1-e^{-t/16}\) that

\[\alpha\cdot\|\bar{w}_{i}\|^{2}\geq\alpha\cdot\frac{\ell}{2d}\;.\]

From Lemma C.3, and using \(t=\sqrt{\frac{(m-1)\log(dm^{2})}{d}}\) we get w.p. \(\geq 1-e^{-\ell/16}+2e^{-t^{2}d/2(m-1)}=1-e^{-\ell/16}+2m^{-1}d^{-1/2}\) that

\[\langle\bar{w}_{i},\sum_{j\neq i}\text{sign}(u_{j})\bar{w}_{j}\rangle \leq\frac{1}{\sqrt{d}}t\sqrt{2\ell}\] \[=\frac{1}{d}\cdot\sqrt{2\ell(m-1)\log(m^{2}d)}\;.\] (3)

Applying union bound over the above two events, and for every \(i\in I_{-}\), we get w.p. \(\geq 1-2\left(me^{-\ell/16}+d^{-1/2}\right)\) that:

\[\langle\bar{w}_{i},\bar{z}\rangle\geq\frac{\alpha\ell}{2d}-\frac{\alpha}{d} \sqrt{2\ell(m-1)\log(m^{2}d)}\;.\]

For the second term in Eq. (2), where \(i\in I_{+}\) we can write in a similar way:

\[\langle\bar{w}_{i},\bar{z}\rangle=-\alpha\,\|\bar{w}_{i}\|^{2}+\alpha\langle \bar{w}_{i},\sum_{j\neq i}\text{sign}(u_{j})\bar{w}_{j}\rangle\;.\]

Using the same argument as above, we get w.p \(\geq 1-2\left(me^{-\ell/16}+d^{-1/2}\right)\) that:

\[\langle\bar{w}_{i},\bar{z}\rangle\leq-\frac{\alpha\ell}{2d}+\frac{\alpha}{d} \sqrt{2\ell(m-1)\log(m^{2}d)}\;.\]

By assuming that \(\ell\geq 8(m-1)\log(m^{2}d)\) we get that \(\langle\bar{w}_{i},z\rangle\leq 0\). Denote \(C:=\frac{\alpha\ell}{2d}-\frac{\alpha}{d}\sqrt{2\ell(m-1)\log(m^{2}d)}\), then going back to Eq. (2), using the above bounds and applying union bound, we get w.p. \(\geq 1-4\left(me^{-\ell/16}+d^{-1/2}\right)\) that:

\[N(x_{0}+z) \leq\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+C)+ \sum_{i\in I_{+}}u_{i}\sigma(\langle w_{i},x_{0}\rangle)\] \[=\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+C)+\sum_ {i\in I_{+}}u_{i}\sigma(\langle w_{i},x_{0}\rangle)+\sum_{i\in I_{-}}u_{i} \sigma(\langle w_{i},x_{0}\rangle)-\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i}, x_{0}\rangle)\] \[=\sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle+C)- \sum_{i\in I_{-}}u_{i}\sigma(\langle w_{i},x_{0}\rangle)+N(x_{0})\] \[=\sum_{i\in I_{-}}u_{i}\left(\sigma(\langle w_{i},x_{0}\rangle+C)- \sigma(\langle w_{i},x_{0}\rangle)\right)+N(x_{0})\;.\]Define \(F_{-}:=\{i\in I_{-}:\langle w_{i},x_{0}\rangle\geq 0\}\), and \(k_{-}=|F_{-}|\). We have that:

\[\sum_{i\in I_{-}}u_{i}\left(\sigma(\langle w_{i},x_{0}\rangle+C)- \sigma(\langle w_{i},x_{0}\rangle)\right) \leq\sum_{i\in F_{-}}u_{i}\left(\sigma(\langle w_{i},x_{0}\rangle +C)-\sigma(\langle w_{i},x_{0}\rangle)\right)\] \[=\sum_{i\in F_{-}}u_{i}C=-\frac{k_{-}C}{\sqrt{m}}\;,\]

where the first inequality is since we only sum over negative terms, and the second inequality is since both \(\langle w_{i},x_{0}\rangle\geq 0\) (because \(i\in F_{-}\)) and \(C\geq 0\) (because \(\ell\geq 32(m-1)\log(m^{2}d)\)). Combining all of the above, we get that:

\[N(x_{0}+z)\leq-\frac{k_{-}C}{\sqrt{m}}+N(x_{0})\;.\] (4)

By our assumption that \(\ell\geq 32(m-1)\log(m^{2}d)\) we have that

\[C =\alpha\left(\frac{1}{2}\frac{\ell}{d}-\sqrt{2}\sqrt{m-1}\frac{ \sqrt{\ell}}{d}\sqrt{\log(dm^{2})}\right)\] \[=\frac{\alpha\sqrt{\ell}}{d}\left(\frac{\sqrt{\ell}}{2}-\sqrt{2( m-1)\log(m^{2}d)}\right)\] \[\geq\frac{\alpha\ell}{4d}\;.\]

Plugging in \(C\) and \(\alpha=\frac{8\sqrt{m}dN(x_{0})}{k_{-}\ell}\) to Eq. (4) we get that:

\[N(x_{0}+z) \leq-\frac{k_{-}C}{\sqrt{m}}+N(x_{0})\] \[\leq-\frac{k_{-}}{\sqrt{m}}\cdot\frac{\ell}{4d}\cdot\frac{8\sqrt{ m}dN(x_{0})}{k_{-}\ell}+N(x_{0})=-N(x_{0})<0\;,\]

and in particular \(\text{sign}(N(x_{0}))\neq\text{sign}(N(x_{0}+z))\).

We are left with calculating the norm of \(z\):

\[\|z\| =\alpha\cdot\left\|\sum_{i\in I_{-}}\Pi_{M^{\perp}}(w_{i})-\sum_{ i\in I_{+}}\Pi_{M^{\perp}}(w_{i})\right\|\] \[=\alpha\cdot\left\|\sum_{i=1}^{m}-\text{sign}(u_{i})\Pi_{M^{\perp }}(w_{i})\right\|\] \[=\alpha\cdot\left\|\sum_{i=1}^{m}-\text{sign}(u_{i})\bar{w}_{i} \right\|\;.\]

Since for each \(i\in[m]\), \(\bar{w}_{i}\sim\mathcal{N}\left(\mathbf{0},\frac{1}{d}I_{\ell}\right)\), then \(-\text{sign}(u_{i})\bar{w}_{i}\) also have the same distribution, because this is a symmetric distribution. Hence, \(\sum_{i=1}^{m}-\text{sign}(u_{i})\bar{w}_{i}\sim\mathcal{N}\left(\mathbf{0}, \frac{m}{d}I_{\ell}\right)\) as a sum of Gaussian random variables. Using Lemma C.1 we get w.p \(\geq 1-e^{-\ell/16}\) that \(\left\|\sum_{i=1}^{m}-\text{sign}(u_{i})\bar{w}_{i}\right\|^{2}\leq\frac{2m \ell}{d}\). Plugging in \(\alpha\) we get that:

\[\|z\|\leq\sqrt{\frac{2m\ell}{d}}\cdot\frac{8\sqrt{m}dN(x_{0})}{k_{-}\ell}=8 \sqrt{2}N(x_{0})\cdot\frac{m}{k_{-}}\cdot\sqrt{\frac{d}{\ell}}\;.\]Proofs for Section 6

For proving the main theorem, we will use the following lemma that upper bounds the norm of a sum of Gaussian random variables:

**Lemma D.1**.: _Let \(w_{1},..,w_{m}\in\mathbb{R}^{n}\) such that for all \(i\in[m]\), \(w_{i}\sim\mathcal{N}(\mathbf{0},\sigma^{2}I_{n})\), then we have:_

\[\mathbb{P}\left[\left\|\sum_{i=1}^{m}w_{i}\right\|^{2}\geq 2m\sigma^{2}n \right]\leq e^{-\frac{n}{16}}\]

Proof.: We denote the \(j\)-th coordinate of the vector \(w_{i}\in\mathbb{R}^{n}\) by \(w_{i,j}\). Note, for any \(i\in[m]\) and \(j\in[n]\) we have \(w_{i,j}\sim\mathcal{N}(0,\sigma^{2})\). We denote by \(s\) the sum vector \(s:=\sum\limits_{i=1}^{m}w_{i}\), and by \(s_{j}\) the \(j\)-th coordinate of \(s\). By this definition, \(s_{j}=\sum\limits_{i=1}^{m}w_{i,j}\) is a sum of \(m\) independent Gaussian variables and therefore also a Gaussian variable. Therefore, \(s\sim\mathcal{N}(\mathbf{0},m\sigma^{2}I_{n})\). We use Lemma C.1 with variance \(m\sigma^{2}\) and get that:

\[\mathbb{P}\left[\left\|\sum_{i=1}^{m}w_{i}\right\|^{2}\geq 2m\sigma^{2}n \right]\leq e^{-\frac{n}{16}}\;.\]

We now prove the main theorem of this section:

Proof of Theorem 6.1.: Similar to the lower bound of the norm, let \(M=\text{span}\{e_{1},\dots,e_{d-\ell}\}\). By Theorem A.1(1), given a training dataset \(X\subseteq P\), it is enough to consider a training set \(X_{R}=\{Rx:x\in X\}\), where \(R\) is an orthogonal matrix such that \(R\cdot P=M\), and training is done over \(X_{R}\). From now on, we assume that the training data, as well as \(x_{0}\) lie on \(M\), and the consequences of this proof would also imply for a dataset \(X\) and \(x_{0}\in P\).

The projection of the gradient on \(M^{\perp}\) is equal to:

\[\Pi_{M^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x}\right)=\Pi_{M^{ \perp}}\left(\sum_{i=1}^{m}u_{i}w_{i}\mathds{1}_{\langle w_{i},x_{0}\rangle \geq 0}\right)=\sum_{i=1}^{m}\Pi_{M^{\perp}}\left(u_{i}w_{i}\right)\mathds{1} _{i\in S}=\sum_{i\in S}\Pi_{M^{\perp}}\left(u_{i}w_{i}\right)\;.\]

Denote by \(\hat{w}_{i}=(w_{i})_{d-\ell+1:d}\), the last \(\ell\) coordinates of \(w_{i}\). By Theorem A.2 we get that for every \(i\in[m]\), \(\hat{w}_{i}\) did not change from their initial value during training.

Recall that we initialized \(\hat{w}_{i}\sim\mathcal{N}(\mathbf{0},\beta^{2}I_{\ell})\). Note that the set \(S\) is independent of the value of the \(\hat{w}_{i}\)'s. This is because \(\hat{w}_{i}\) does not effect the training, hence will not effect \(w_{i}-\Pi_{M^{\perp}}(w_{i})\). Also, after choosing \(x_{0}\) we have \(\langle\hat{w}_{i},\hat{x}_{0}\rangle=0\), since \(\hat{x}_{0}=\mathbf{0}\), which means that the choice of \(S\) is independent of the \(\hat{w}_{i}\)'s. We can conclude that the random variables \(\hat{w}_{i}\) for \(i\in S\) are sampled independently.

Therefore, from Lemma B.2 we get that w.p. \(\geq 1-e^{-\ell/16}\):

\[\left\|\sum_{i\in S}\hat{w}_{i}\right\|\leq\beta\sqrt{2k\ell}\;.\]

Note, since for all \(i\in[m]\), \(|u_{i}|=\frac{1}{\sqrt{m}}\) and they are not trained, we get w.p. \(\geq 1-e^{-\ell/16}\) that:

\[\left\|\Pi_{M^{\perp}}\left(\frac{\partial N(x_{0})}{\partial x} \right)\right\|\leq\beta\sqrt{\frac{2k\ell}{m}}\;.\]

### Explicit \(L_{2}\) regularization

Proof of Theorem 6.2.: As before, for this proof we rotate the data subspace \(P\) to lie on \(M=\text{span}\{e_{1},\dots,e_{d-\ell}\}\) and rotate the model's weights accordingly. For a dataset \((x_{1},y_{1}),..,(x_{r},y_{r})\), we train over the following objective:

\[\sum_{j=1}^{r}L(y_{j}\cdot N(x_{j},\mathbf{w}_{1:m})))+\frac{1}{2}\lambda \left\lVert\mathbf{w}_{1:m}\right\rVert^{2}\]

In Theorem A.2, we showed for all \((x_{j},y_{j})\) that if we train the model using the loss \(L\) we get:

\[\Pi_{M^{\perp}}\left(\frac{\partial L\left(N(x_{j},\mathbf{w}_{1:m})\cdot y_{ j}\right)}{\partial w_{i}}\right)=0\]

Now, we analyze the training process using the new loss which includes the regularization term. We denote by \(w_{i}^{(t)}\) the weight vector \(w_{i}\) after \(t\) training steps, and by \(\hat{w}_{i}^{(t)}:=\Pi_{M^{\perp}}\left(w_{i}^{(t)}\right)\) its projection on the subspace orthogonal to \(M\). We look at the projected gradient of \(w_{i}^{(t)}\) w.r.t. the loss:

\[\Pi_{M^{\perp}}\left(\frac{\partial\sum_{j=1}^{r}L\left(N(x_{j}, \mathbf{w}_{1:m}^{(t)})\cdot y_{j}\right)}{\partial w_{i}}+\frac{\partial_{2} ^{\frac{1}{2}}\lambda\left\lVert w_{i}^{(t)}\right\rVert^{2}}{\partial w_{i} }\right)=\] \[= \sum_{j=1}^{r}\Pi_{M^{\perp}}\left(\frac{\partial L\left(N(x_{j},\mathbf{w}_{1:m}^{(t)})\cdot y_{j}\right)}{\partial w_{i}}\right)+\Pi_{M^{ \perp}}\left(\frac{\partial_{2}^{\frac{1}{2}}\lambda\left\lVert w_{i}^{(t)} \right\rVert^{2}}{\partial w_{i}}\right)\] \[= \Pi_{M^{\perp}}\left(\frac{\partial_{2}^{\frac{1}{2}}\lambda \left\lVert w_{i}^{(t)}\right\rVert^{2}}{\partial w_{i}}\right)\] \[= \Pi_{M^{\perp}}\left(\lambda w_{i}^{(t)}\right)\] \[= \lambda\hat{w}_{i}^{(t)}.\]

For a training step of size \(\eta\), using gradient descent we get that:

\[\hat{w}_{i}^{(t+1)}=\hat{w}_{i}^{(t)}-\eta\lambda\hat{w}_{i}^{(t)}.\]

Thus, after a total of \(T\) iteration of training we get that:

\[\hat{w}_{i}^{(T)}=(1-\eta\lambda)^{T}\hat{w}_{i}^{(0)}\;.\]

Therefore, the projection of gradients after training onto \(P^{\perp}\) will be the same as if they were initialized to \(\sim\mathcal{N}\left(0,\frac{(1-\eta\lambda)^{2T}}{d}I_{d}\right)\) and trained using logistic loss without regularization. The rest of the proof is the same as Theorem 6.1 for \(\beta=\frac{(1-\eta\lambda)^{T}}{\sqrt{d}}\). 

## Appendix E Further Experiments and Experimental Details

### Further Experiments

In Figure 5 we present the boundary of a two-layer ReLU network trained over a \(25\)-point dataset on a two-dimensional linear subspace, similar to Figure 3. We train the networks until reaching a constant positive margin. The difference between the figures is that in Figure 5 we initialize the weights using the default PyTorch initialization, while in Figure 3 we initialized using a smaller scalefor the robustness effect to be smaller, and visualized more easily. The experiment in Figure 5 is demonstrating an extreme robustness effect, occurring when using the standard settings.

In Figure 6 we go beyond the theory discussed in this paper, and present similar phenomena in all three settings for a five-layer ReLU network. In Figure 5(a) we can see the boundary of the regularly trained network within a small distance in \(P^{\perp}\) from the data points. In Figure 5(b) we use small initialization for all five layers, and present a boundary almost orthogonal to the data manifold. In Figure 5(c), the boundary of a regularized trained network is in a similar form. This experiment suggests that our theoretical results might be extended also to deeper networks, where all layers are trained.

### One-dimensional dataset experiment - 2 layer network (Figure 2)

DatasetFor all the three experiments we used a 7-point data set, spread equally on the two dimensional line \(y=x\) from \((-1,-1)\) to \((1,1)\).

NetworkFor all the three experiments we used two-layer ReLU network of width \(100\) with biases in both layers. The weights of both layers were initialized using (1+3) default PyTorch initialization for linear layers, (2) default initialization divided by \(3\).

TrainingWe used train step of size \(0.02\) for (1+3) and \(0.04\) for (2). We trained both layers until the margin reached \(0.3\). The losses we used were (1+2) Logistic loss, (3) Logistic loss with \(0.005\)\(L_{2}\) regularization.

Figure 5: **Experiments on two-dimensional dataset.** We plot the dataset points and the decision boundary in 3 settings: (a) Vanilla trained network, (b) The network’s weights are initialized from a smaller variance distribution, and (c) Training with regularization. Colors are used to emphasise the values in the \(z\) axis.

Figure 6: **Experiments on one-dimensional dataset with deep network.** We plot the dataset points and the decision boundary in 3 settings: (a) Vanilla trained network, (b) The network’s weights are initialized from a smaller variance distribution, and (c) Training with regularization.

### Two-dimensional dataset experiment - smaller effect (Figure 3)

DatasetFor all the three experiments we used a 25-point data set, spread equally on a grid which lies on the \(z=0.5\) axis.

NetworkFor all the three experiments we used two-layer ReLU network of width \(4000\) with biases in both layers. The weights in the first layer were initialized in (1+3) from \(\mathcal{N}(\mathbf{0},1/3I_{3})\), and in (2) from \(\mathcal{N}(\mathbf{0},1/36I_{3})\). The weight of the output layer were initialized to the uniform distribution over the set \(\{-1,1\}\).

TrainingFor all the experiments we trained both layers until the margin reached \(0.3\) and we used train step of size \(0.002\). The losses we used were (1+2) Logistic loss, (3) Logistic loss with \(0.8\)\(L_{2}\) regularization on the weights of the first layer.

### Two-dimensional dataset experiment (Figure 5)

DatasetFor all the three experiments we used a 25-point data set, spread equally on a grid which lies on the \(x-y\) axis.

NetworkFor all the three experiments we used two-layer RelU network of width \(400\) with biases in both layers. The weights in any layer were initialized using (1+3) default PyTorch initialization for linear layers, (2) default initialization divided by \(3\).

TrainingFor (1) experiments we used train step of size \(0.005\), and for (2+3) we used step of size \(0.05\). We trained both layers until the margin reached \(0.1\). The losses we used were (1+2) Logistic loss, (3) Logistic loss with \(0.005\)\(L_{2}\) regularization.

### One-dimensional dataset experiment - 5 layer network (Figure 6)

DatasetFor all the three experiments we used a 7-point data set, spread equally on the two dimensional line \(y=x\) from \((-1,-1)\) to \((1,1)\).

NetworkFor all the three experiments we used 5-layer RelU network of width \(100\) with biases in all layers. The weights in any layer were initialized using (1+3) default PyTorch initialization for linear layers, (2) default initialization divided by \(3\).

TrainingFor (1+3) experiments we used train step of size \(0.02\), and for (2) we used step of size \(0.06\). we trained all layers until the margin reached \(0.3\). The losses we used were (1+2) Logistic loss, (3) Logistic loss with \(0.01\)\(L_{2}\) regularization.