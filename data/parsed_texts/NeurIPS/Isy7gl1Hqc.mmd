# Hidden Poison: Machine Unlearning Enables

Camouflaged Poisoning Attacks+
Footnote †: Authors JA, GK, AS are listed in alphabetical order. Full paper: [https://arxiv.org/abs/2212.10717](https://arxiv.org/abs/2212.10717)

 Jimmy Z. Di

University of Waterloo

jimmy.di@uwaterloo.ca

&Jack Douglas

University of Waterloo

jack.douglas@uwaterloo.ca

&Jayadev Acharya

Cornell University

acharya@cornell.edu

&Gautam Kamath

University of Waterloo, Vector Institute

g@csail.mit.edu

&Ayush Sekhari

Massachusetts Institute of Technology

sekhari@mit.edu

###### Abstract

We introduce _camouflaged data poisoning attacks_, a new attack vector that arises in the context of machine unlearning and other settings when model retraining may be induced. An adversary first adds a few carefully crafted points to the training dataset such that the impact on the model's predictions is minimal. The adversary subsequently triggers a request to remove a subset of the introduced points at which point the attack is unleashed and the model's predictions are negatively affected. In particular, we consider clean-label _targeted_ attacks (in which the goal is to cause the model to misclassify a specific test point) on datasets including CIFAR-10, Imagenette, and Imageeowot. This attack is realized by constructing _camouflage_ datapoints that mask the effect of a poisoned dataset. We demonstrate the efficacy of our attack when unlearning is performed via retraining from scratch, the idealized setting of machine unlearning which other efficient methods attempt to emulate, as well as against the approximate unlearning approach of Graves et al. (2021).

## 1 Introduction

Machine Learning (ML) research traditionally assumes a static pipeline: data is gathered, a model is trained once and subsequently deployed. This paradigm has been challenged by practical deployments, which are more dynamic in nature. After initial deployment more data may be collected, necessitating additional training. Or, as in the _machine unlearning_ setting (Cao and Yang, 2015), we may need to produce a model as if certain points were never in the training set to begin with.1

Footnote 1: A naive solution is to remove said points from the training set and re-train the model from scratch.

While such dynamic settings clearly increase the applicability of ML models, they also make them more vulnerable. Specifically, they open models up to new methods of attack by malicious actors aiming to sabotage the model. In this work, we introduce a new type of data poisoning attack on models that _unlearn_ training datapoints. We call these _camouflaged data poisoning attacks_.

The attack takes place in two phases (Figure 1). In the first stage, before the model is trained, the attacker adds a set of carefully designed points to the training data, consisting of a _poison_ set and a _camouflage_ set. The model's behaviour should be similar whether it is trained on either the training data, or its augmentation with both the poison and camouflage sets. In the second phase, the attacker triggers an unlearning request to delete the _camouflage_ set after the model is trained. That is, themodel must be updated to behave as though it were only trained on the training set plus the poison set. At this point, the attack is fully realized, and the model's performance suffers in some way.

While such an attack could harm the model by several metrics, in this paper, we focus on _targeted_ poisoning attacks - that is, poisoning attacks where the goal is to misclassify a specific point in the test set. Our contributions are the following:

1. We introduce _camouflaged data poisoning_ attacks, demonstrating a new attack vector in dynamic settings including _machine unlearning_ (Figure 1).
2. We realize these attacks in the targeted poisoning setting, giving an algorithm based on the gradient-matching approach of Geiping et al. (2021). In order to make the model behavior comparable to as if the poison set were absent, we construct the camouflage set by generating a new set of points that _undoes_ the impact of the poison set. We thus identify a new technical question of broader interest to the data poisoning community: Can one nullify a data poisoning attack by only _adding_ points?
3. We demonstrate the efficacy of these attacks on a variety of models (SVMs and neural networks) and datasets (CIFAR-10 (Krizhevsky, 2009), Imagenette (Howard, 2019), and Imagewoof (Howard, 2019)).

### Preliminaries

Machine Unlearning.A significant amount of legislation concerning the "right to be forgotten" has recently been introduced by governments around the world, including the European Union's General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and Canada's proposed Consumer Privacy Protection Act (CPPA). Such legislation requires organizations to delete information they have collected about a user upon request. A natural question is whether that further obligates the organizations to remove that information from downstream machine learning models trained on the data - current guidances (Information Commissioner's Office, 2020) and precedents (Federal Trade Commission, 2021) indicate that this may be the case. This goal has sparked a recent line of work on _machine unlearning_(Cao and Yang, 2015).

The simplest way to remove a user's data from a trained model is to remove the data from the training set, and then retrain the model on the remainder (also called "retraining from scratch"). This is the ideal way to perform data deletion, as it ensures that the model was never trained on the datapoint of

Figure 1: An illustration of a successful camouflaged targeted data poisoning attack. In Step 1, the adversary adds poison and camouflage sets of points to the (clean) training data. In Step 2, the model is trained on the augmented training dataset. It should behave similarly to if trained on only the clean data; in particular, it should correctly classify the targeted point. In Step 3, the adversary triggers an unlearning request to delete the camouflage set from the trained model. In Step 4, the resulting model misclassifies the targeted point.

concern. The downside is that retraining may take a significant amount of time in modern machine learning settings. Hence, most work within machine unlearning has studied _fast_ methods for data deletion, sometimes relaxing to _approximately_ removing the datapoint. A related line of work has focused more on other implications of machine unlearning, particularly the consequences of an adaptive and dynamic data pipeline (Gupta et al., 2021; Marchant et al., 2022). Our work fits into the latter line: we show that the potential to remove points from a trained model can expose a new attack vector. Since retraining from scratch is the ideal result that other methods try to emulate, we focus primarily on unlearning by retraining from scratch, but the same phenomena should still occur when any effective machine unlearning algorithm is applied. For example, we also demonstrate efficacy against the approximate unlearning method of Graves et al. (2021).

**Data Poisoning.** In a data poisoning attack, an adversary in some way modifies the training data provided to a machine learning model, such that the model's behaviour at test time is negatively impacted. Our focus is on _targeted data poisoning attacks_, where the attacker's goal is to cause the model to misclassify some specific datapoint in the test set. Other common types of data poisoning attacks include _indiscriminate_ (in which the goal is to increase the test error) and _backdoor_ (where the goal is to misclassify test points which have been adversarially modified in some small way).

The adversary is typically limited in a couple ways. First, it is common to say that they can only _add_ a _small number_ of points to the training set. This mimics the setting where the training data is gathered from some large public crowdsourced dataset, and an adversary can contribute a few judiciously selected points of their own. Other choices may include allowing them to _modify_ or _delete_ points from the training set, but these are less easily motivated. Additionally, the adversary is generally constrained to _clean-label attacks_: if the introduced points were inspected by a human, they should not appear suspicious or incorrectly labeled. We comment that this criteria is subjective and thus not a precise notion, but is nonetheless common in data poisoning literature, and we use the term as well.

## 2 Discussion of Other Related Work

The motivation for our work comes from Marchant et al. (2022), who propose a novel poisoning attack on unlearning systems. As mentioned before, the primary goal of many machine unlearning systems is to "unlearn" datapoints quickly, i.e., faster than retraining from scratch. Marchant et al. (2022) craft poisoning schemes via careful noise addition, in order to trigger the unlearning algorithm to retrain from scratch on far more deletion requests than typically required. While both our work and theirs are focused on data poisoning attacks against machine unlearning systems, the adversaries have very different objectives. In our work, the adversary is trying to misclassify a target test point, whereas they try to increase the time required to unlearn a point.

In targeted data poisoning, there are a few different types of attacks. The simplest form of attack is _label flipping_, in which the adversary is allowed to flip the labels of the examples (Barreno et al., 2010; Xiao et al., 2012; Paudice et al., 2018). Another type of attack is _watermarking_, in which the feature vectors are perturbed to obtain the desired poisoning effect (Suciu et al., 2018; Shafahi et al.,

Figure 2: Some representative images from Imagewoof. In each pair, the left figure is from the training dataset, while the right image has been adversarially manipulated. The top and bottom rows are images from the poison and camouflage set, respectively. In all cases, the manipulated images are _clean label_ and nearly indistinguishable from the original image.

2018). In both these cases, noticeable changes are made to the label and feature vector, respectively, which would be noticeable by a human labeler. In contrast, _clean label_ attacks attempt to make unnoticeable changes to both the feature vector and the label, and are the gold standard for data poisoning attacks (Huang et al., 2020; Geiping et al., 2021). Our focus is on both clean-label poison and camouflage sets. While there are also works on indiscriminate (Biggio et al., 2012; Xiao et al., 2015; Munoz-Gonzalez et al., 2017; Steinhardt et al., 2017; Diakonikolas et al., 2019; Koh et al., 2022) and backdoor (Gu et al., 2017; Tran et al., 2018; Sun et al., 2019) poisoning attacks, these are beyond the scope of our work, see Goldblum et al. (2020); Cina et al. (2022) for additional background on data poisoning attacks.

Cao and Yang (2015) initiated the study of machine unlearning through _exact_ unlearning, wherein the new model obtained after deleting an example is statistically identical to the model obtained by training on a dataset without the example. A probabilistic notion of unlearning was defined by Ginart et al. (2019), which in turn is inspired from notions in differential privacy (Dwork et al., 2006). Several works studied algorithms for empirical risk minimization (i.e., training loss) (Guo et al., 2020; Izzo et al., 2021; Neel et al., 2021; Ullah et al., 2021; Thudi et al., 2022; Graves et al., 2021; Chourasia et al., 2022), while later works study the effect of machine unlearning on the generalization loss (Gupta et al., 2021; Sekhari et al., 2021). In particular, these works realize that unlearning data points quickly can lead to a drop in test loss, which is the theme of our current work. Several works have considered implementations of machine unlearning in several contexts starting with the work of Bourlotte et al. (2021). These include unlearning in deep neural networks (Goldarkar et al., 2020, 2021; Nguyen et al., 2020), random forests (Brophy and Lowd, 2021), large scale language models (Zanella-Beguelin et al., 2020), the tension between unlearning and privacy (Chen et al., 2021; Carlini et al., 2022), anomaly detection (Du et al., 2019), insufficiency of preventing verbatim memorization for ensuring privacy (Ippolito et al., 2022), and even auditing of machine unlearning systems (Sommer et al., 2020).

After the first version of our paper was uploaded on arXiv, Yang et al. (2022) discovered defenses against the data poisoning procedure of Geiping et al. (2021). One may further use their techniques to defend against our camouflaged poisoning attack as well, however, we note that this does not undermine the contributions of this paper. Our main contribution is to introduce this new kind of attack, and bring to attention that machine unlearning enables this attack; the specific choice of which procedure is used for poison generation or camouflage generation is irrelevant to the existence of such an attack.

## 3 Setup and Algorithms

### Threat Model and Approach

The camouflaged poisoning attack takes place through interaction between an _attacker_ and a _victim_, and is triggered by an unlearning request. We assume that the attacker has access to the victim's model architecture,2 the ability to query gradients on a trained model (which could be achieved, e.g., by having access to the training dataset), and a target sample that it wants to attack. The attacker first sets the stage for the attack by introducing _poison samples_ and _camouflage samples_ to the training dataset, which are designed so as to have minimal impact when a model is trained with this modified dataset. At a later time, the attacker triggers the attack by submitting an unlearning request to remove the camouflage samples. The victim first trains a machine learning model (e.g., a deep neural network) on the modified training dataset, and then executes the unlearning request by retraining the model from scratch on the left over dataset. The goal of the attacker is to change the prediction of the model on a particular target sample \((x_{\mathrm{tar}},y_{\mathrm{tar}})\) previously unseen by the model during training from \(y_{\mathrm{tar}}\) to a desired label \(y_{\mathrm{adv}}\), while still ensuring good performance over other validation samples. Formally, the interaction between the attacker and the victim is as follows (see Figure 1) :

Footnote 2: In Appendix B.6.3, we examine the _transferability_ of our proposed attack to unknown victim model, thus relaxing the requirement of knowing the victim’s model architecture a priori.

1. The attacker introduces a small number of poisons samples \(S_{\mathrm{po}}\) and camouflage samples \(S_{\mathrm{ca}}\) to a clean training dataset \(S_{\mathrm{cl}}\). Define \(S_{\mathrm{cpc}}=S_{\mathrm{cl}}+S_{\mathrm{po}}+S_{\mathrm{ca}}\).
2. Victim trains an ML model (e.g., a neural network) on \(S_{\mathrm{cpc}}\), and returns the model \(\theta_{\mathrm{cpc}}\).

3. The attacker submits a request to unlearn the camouflage samples \(S_{\mathrm{ca}}\).3 Footnote 3: The unlearning process aims to forget the subset of samples \(S_{\mathrm{ca}}\) from the model \(\theta_{\mathrm{cp}}\) (trained on \(S_{\mathrm{cp}}=S_{\mathrm{cl}}+S_{\mathrm{Po}}+S_{\mathrm{Ca}}\)). The goal of the unlearning process is to output a model which is indistinguishable from the model trained from scratch on the leftover data samples (i.e. on \(S_{\mathrm{cl}}+S_{\mathrm{po}}\)). In our work, we simulate this indistinguishability by exactly retraining a fresh model from scratch on \(S_{\mathrm{cl}}+S_{\mathrm{po}}\). Since the goal of unlearning is to ensure this indistinguishability, we conjecture that our attack will also work against other approximate unlearning methods in the literature, and provide experimental evidence in support of this in Appendix B.6.6.polytope
4. The victim performs the request, and computes a new model \(\theta_{\mathrm{cp}}\) by retraining from scratch on the left over data samples \(S_{\mathrm{cp}}=S_{\mathrm{cl}}+S_{\mathrm{po}}\).

Note that the attack is only realized in Step 4 when the victim executes the unlearning request and retrains the model from scratch on the left over training samples. In fact, in Steps 1-3, the victim's model should behave similarly to as if it were trained on the clean samples \(S_{\mathrm{cl}}\) only. In particular, the model \(\theta_{\mathrm{cp}}\) will predict \(y_{\mathrm{tar}}\) on \(x_{\mathrm{tar}}\), whereas the updated model \(\theta_{\mathrm{cp}}\) will predict \(y_{\mathrm{adv}}\) on \(x_{\mathrm{tar}}\). Both models should have comparable validation accuracy. Such an attack is implemented by designing a camouflage set that cancels the effects of the poison set while training, but retraining without the camouflage set (to unlearn them) exposes the poison set, thus negatively affecting the model.

Before we delve into technical details on how the poisons and camouflages are generated, we will provide an illustrative scenario where such an attack can take place. Suppose an image-based social media platform uses ML-based content moderation to filter out inappropriate images, e.g., adult content, violent images, etc., by classifying the posts as "safe" and "unsafe." An attacker can plant a camouflaged poisoning attack in the system in order to target a famous personality, like a politician, a movie star, etc. The goal of the attacker is to make the model misclassify a potentially sensitive target image of that person (e.g., a politically inappropriate image) as "safe." However, the attacker does not want to unleash this misclassification immediately, but to instead time it to align with macro events like elections, the release of movies, etc. Thus, at a later time when the attacker wishes, the misclassification can be triggered making the model classify this target image as safe and letting it circulate on the platform, thus hurting the reputation of that person at an unfortunate time (e.g., before an election or before when their movie is released). The attack is triggered by submitting an unlearning request.

We highlight that camouflaged attacks may be _more dangerous_ than traditional data poisoning attacks, since camouflaged attacks can be triggered by the adversary. That is, the adversary can reveal the attack whenever it wishes by submitting an unlearning request. On the other hand, in the traditional poisoning setting, the attack is unleashed as soon as the model is trained and deployed, the timing of which the attacker has little control over.

In order to be undetectable, and represent the realistic scenario in which the adversary has limited influence on the model's training data, the attacker is only allowed to introduce a set of points that is much smaller than the size of the clean training dataset (i.e., \(|S_{\mathrm{po}}|\ll|S_{\mathrm{cl}}|\) and \(|S_{\mathrm{ca}}|\ll|S_{\mathrm{cl}}|\)). Throughout the paper and experiments, we denote the relative size of the poison set and camouflage set by \(b_{p}:=\frac{|S_{\mathrm{po}}|}{|S_{\mathrm{cl}}|}\times 100\) and \(b_{c}:=\frac{|S_{\mathrm{ca}}|}{|S_{\mathrm{cl}}|}\times 100\), respectively. Additionally, the attacker is only allowed to generate poison and camouflage samples by altering the base images by less than \(\varepsilon\) distance in the \(\ell_{\infty}\) norm (in our experiments \(\varepsilon\leq 16\), where the images are represented as an array of pixels in \(0\) to \(255\)). Thus, the attacker executes a so-called _clean-label_ attack, where the corrupted images would be visually indistinguishable from original base images and thus would be given the same label as before by a human data validator. We parameterize a threat model by the tuple \((\varepsilon,b_{c},b_{p})\).

### Poison and Camouflage Samples

The attacker implements the attack by first generating poison samples, and then generating camouflage samples to cancel their effects, as shown in the following.

**Poison samples.** Poison samples are designed so that a network trained on \(S_{\mathrm{cp}}=S_{\mathrm{cl}}+S_{\mathrm{po}}\) predicts the label \(y_{\mathrm{adv}}\) (instead of \(y_{\mathrm{tar}}\)) on a target image \(x_{\mathrm{tar}}\). While there are numerous data poisoning attacks in the literature, we adopt the state-of-the-art procedure of Geiping et al. (2021) for generating poisons due to its high success rate, efficiency of implementation, and applicability across various models. However, our framework is flexible: in principle, other attacks for the same setting could serve as a drop-in replacement, e.g., the methods of Aghakhani et al. (2021) or Huang et al. (2020), or any method introduced in the future. 4

Footnote 4: We provide preliminary experiments in Appendix C of camouflaged-poisoning attacks developed using the Bullseye Polytope technique from Aghakhani et al. (2021), and show that the attack continues to succeed even when we use alternate poison and camouflage generation techniques.

Suppose that \(S_{\mathrm{cp}}\) consist of \(N_{1}\) samples \((x^{i},y^{i})_{i\in N_{1}}\) out of which the first \(P\) samples with index \(i=1\) to \(P\) belong to the poison set \(S_{\mathrm{po}}\).5 The poison samples are generated by adding small perturbations \(\Delta^{i}\) to the base image \(x^{i}\) so as to minimize the loss on the target with respect to the adversarial label, which can be formalized as the following bilevel optimization problem 6

Footnote 5: This ordering is for notational convenience; naturally, the datapoints are shuffled to preclude the victim simply removing a prefix of the training data.

Footnote 6: While (1) focuses on misclassifying a single target point, it is straightforward to extend this to multiple targets by changing the objective to a sum over losses on the target points.

\[\min_{\Delta\in\Gamma}\ell(f(x_{\mathrm{tar}},\theta(\Delta)),y_{ \mathrm{adv}})\quad\text{where}\] \[\theta(\Delta)\in\operatorname*{arg\,min}_{\theta}\frac{1}{N} \sum_{i\leq N}\ell(f(x^{i}+\Delta^{i},\theta),y^{i}), \tag{1}\]

and we define the constraint set \(\Gamma:=\{\Delta:\|\Delta\|_{\infty}\leq\varepsilon\text{ and }\Delta^{i}=0\text{ for all }i>P\}\). The objective function in (1) is called the _adversarial loss_(Geiping et al., 2021). In all our experiments, we generate the poison points using the gradient matching technique of Geiping et al. (2021), which we detail in Appendix A for completeness.

**Camouflage samples.** Camouflage samples are designed to cancel the effect of the poisons, such that a model trained on \(S_{\mathrm{cpc}}=S_{\mathrm{cl}}+S_{\mathrm{po}}+S_{\mathrm{ca}}\) behaves identical to the model trained on \(S_{\mathrm{cl}}\), and makes the correct prediction on \(x_{\mathrm{tar}}\). We formulate this task via a bilevel optimization problem similar to (1). Let \(S_{\mathrm{cpc}}\) consist of \(N_{2}\) samples \((x^{j},y^{j})_{j\leq N_{2}}\) out of which the last \(C\) samples with index \(j=N_{2}-C+1\) to \(N_{2}\) belong to the camouflage set \(S_{\mathrm{ca}}\). The camouflage points are generated by adding small perturbations \(\Delta^{j}\) to the base image \(x^{j}\) so as to minimize the loss on the target with respect to the adversarial label. In particular, we find the appropriate \(\Delta\) by solving:

\[\min_{\Delta\in\Gamma}\ell(f(x_{\mathrm{tar}},\theta(\Delta)),y_{ \mathrm{tar}})\quad\text{where}\] \[\theta(\Delta)\in\operatorname*{arg\,min}_{\theta}\frac{1}{N_{2} }\sum_{j\leq N_{2}}\ell(f(x^{j}+\Delta^{j},\theta),y^{j}), \tag{2}\]

and we define the constraint set \(\Gamma:=\big{\{}\Delta:\|\Delta\|_{\infty}\leq\varepsilon\text{ and }\Delta^{j}=0\text{ for all }j\leq N_{2}-C\big{\}}\).

### Efficient Algorithms for Generating Camouflages

Given the poison images, camouflage images are designed in order to neutralize the effect of the poisons. Here, we give intuition into what we mean by canceling the effect of poisons, and provide two procedures for generating camouflages efficiently: label flipping, and gradient matching.

#### 3.3.1 Camouflages via Label Flipping

Suppose that the underlying task is a binary classification problem with the labels \(y\in\{-1,1\}\), and that the model is trained using linear loss \(\ell(f(x,\theta),y)=-yf(x,\theta)\). Then, simply flipping the labels allows one to generate a camouflage set for any given poison set \(S_{\mathrm{po}}\). In particular, \(S_{\mathrm{ca}}\) is constructed as: for every \((x^{i},y^{i})\in S_{\mathrm{po}}\), simply add \((x^{i},-y^{i})\) to \(S_{\mathrm{ca}}\) (i.e., \(b_{p}=b_{c}\)). It is easy to see that for such camouflage samples, we have for any \(\theta\),

\[\sum_{(x,y)\in S_{\mathrm{cpc}}}\ell(f(x,\theta),y)\] \[=-\sum_{(x,y)\in S_{\mathrm{cl}}}yf(x,\theta)-\sum_{i=1}^{P} \bigl{(}y^{i}f(x^{i},\theta)+(-y^{i})f(x^{i},\theta)\bigr{)}\] \[=\sum_{(x,y)\in S_{\mathrm{cl}}}\ell(f(x,\theta),y).\]We can also similarly show that the gradients (as well as higher order derivatives) are equal, i.e., \(\nabla_{\theta}\sum_{S_{\mathrm{cpc}}}\ell(f(x,\theta),y)=\nabla_{\theta}\sum_{S_{ \mathrm{cl}}}\ell(f(x,\theta),y)\) for all \(\theta\). Thus, training a model on \(S_{\mathrm{cpc}}\) is equivalent to training it on \(S_{\mathrm{cl}}\). In essence, the camouflages have perfectly canceled out the effect of the poisons. We validate the efficacy of this approach via experiments on linear SVM trained with hinge loss (which resembles linear loss when the domain is bounded). See Section 4.1.1 for details.

While label flipping is a simple and effective procedure to generate camouflages, it is fairly restrictive. Firstly, label flipping only works for binary classification problems trained with linear loss. Secondly, the attack is not clean label as the camouflage images are generated as \((x^{i},-y^{i})\) by giving them the opposite label to the ground truth, which can be easily caught by a validator. Lastly, the attack is vulnerable to simple data purification techniques by the victim, e.g., the victim can protect themselves by preprocessing the data to remove all the images that have both the labels (\(y=+1\) and \(y=-1\)) in the training dataset. In the next section, we provide a different procedure to generate clean-label camouflages that works for general losses and multi-class classification problems.

#### 3.3.2 Camouflages via Gradient Matching

Here, we discuss our main procedure to generate camouflages, which builds on the gradient matching idea of Geiping et al. (2021). Note that, our objective in (2) is to find \(\Delta\) such that when the model is trained with the camouflages, it minimizes the original-target loss in (2) (with respect to the original label \(y_{\mathrm{tar}}\)) thus making the victim model predict the correct label on this target sample. Since, (2) is computationally intractable, one may instead try to implicitly minimize the original-target loss by finding a \(\Delta\) such that for any model parameter \(\theta\),

\[\nabla_{\theta}(\ell(f(x_{\mathrm{tar}},\theta),y_{\mathrm{tar}}))\approx\tfrac {1}{C}\sum_{i=1}^{C}\nabla_{\theta}\ell\big{(}f(x^{i}+\Delta^{i},\theta),y^{i} \big{)}.\]

The above equation suggests that minimizing (e.g., using Adam / SGD) on camouflage samples will also minimize the original-target loss, and thus automatically ensure that the model predicts the correct label on the target. Unfortunately, finding perturbations that satisfy the above is also intractable as the approximate equality is required to hold for all \(\theta\). Building on Geiping et al. (2021), we relax this condition to be satisfied only for a fixed model \(\theta_{\mathrm{cp}}\)-the model trained on the dataset \(S_{\mathrm{cp}}=S_{\mathrm{cl}}+S_{\mathrm{po}}\), and obtain the perturbations which minimize the cosine-similarity loss given by

\[\psi(\Delta,\theta) \tag{3}\] \[=1-\frac{\big{\langle}\nabla_{\theta}\ell(f(x_{\mathrm{tar}}, \theta),y_{\mathrm{tar}}),\sum_{i=1}^{C}\nabla_{\theta}\ell(f(x_{i}+\Delta^{i},\theta),y_{i})\big{\rangle}}{\big{\|}\nabla_{\theta}\ell(f(x_{\mathrm{tar}}, \theta),y_{\mathrm{tar}})\big{\|}\sum_{i=1}^{C}\nabla_{\theta}\ell(f(x_{i}+ \Delta^{i},\theta),y_{i})\big{\|}}.\]

**Implementation details.** We minimize (3) using the Adam optimizer (Kingma and Ba, 2015) with a fixed step size of \(0.1\). In order to increase the robustness of camouflage generation, we do \(R\) restarts (where \(R\leq 10\)). In each restart, we first initialize \(\Delta\) randomly such that \(\|\Delta\|_{\infty}\leq\varepsilon\) and perform \(M\) steps of Adam optimization to minimize \(\psi(\Delta,\theta_{\mathrm{cp}})\). Each optimization step only requires a single differentiation of the objective \(\psi\) with respect to \(\Delta\), and can be implemented efficiently. After each step, we project back the updated \(\Delta\) into the constraint set \(\Gamma\) so as to maintain the property that \(\|\Delta\|_{\infty}\leq\varepsilon\). After doing \(R\) restarts, we choose the best round by finding \(\Delta_{\star}\) with the minimum \(\psi(\Delta_{\star},\theta_{\mathrm{cp}})\).

## 4 Experimental Evaluation

We generate poison samples by running Algorithm 2 (given in the appendix), and camouflage samples by running Algorithm 1 with \(R=1\) and \(M=250\).7 Each experiment is repeated \(K\) times by setting a different seed each time, which fixes the target image, poison class, camouflage class, base poison images and base camouflage images. Due to limited computation resources, we typically set \(K\in\{3,5,8,10\}\) depending on the dataset and report the mean and standard deviation across different trials. We say that _poisoning_ was successful if the model trained on \(S_{\mathrm{cp}}=S_{\mathrm{cl}}+S_{\mathrm{po}}\) predicts the label \(y_{\mathrm{adv}}\) on the target image. Furthermore, we say that _camouflaging_ was successful if the model trained on \(S_{\mathrm{cpc}}=S_{\mathrm{po}}+S_{\mathrm{cl}}+S_{\mathrm{ca}}\) predicts back the correct label \(y_{\mathrm{tar}}\) on the target image, provided that poisoning was successful. A camouflaged poisoning attack is successful if both poisoning and camouflaging were successful.

```
0: Network \(f(\cdot\ ;\theta_{\mathrm{cp}})\) trained on \(S_{\mathrm{cl}}+S_{\mathrm{po}}\), the target \((x_{\mathrm{tar}},y_{\mathrm{tar}})\), Camouflage budget \(C\), perturbation bound \(\varepsilon\), number of restarts \(R\), optimization steps \(M\)
1: Collect a dataset \(S_{\mathrm{ca}}=\left\{x^{j},y^{j}\right\}_{j=1}^{C}\) of \(C\) many images whose true label is \(y_{\mathrm{tar}}\).
2:for\(r=1,\ldots R\) restarts do
3: Randomly initialize perturbations \(\Delta\) s.t. \(\|\Delta\|_{\infty}\leq\varepsilon\).
4:for\(k=1,\ldots,M\) optimization steps do
5: Compute the loss \(\psi(\Delta,\theta_{\mathrm{cp}})\) as in (5) using the base camouflage images in \(S_{\mathrm{ca}}\).
6: Update \(\Delta\) using an Adam update to minimize \(\psi\), and project onto the constraint set \(\Gamma\).
7:endfor
8: Amongst the \(R\) restarts, choose the \(\Delta_{*}\) with the smallest value of \(\psi(\Delta_{*},\theta_{\mathrm{cp}})\).
9:endfor
10: Return the poisoned set \(S_{\mathrm{ca}}=\left\{x^{j}+\Delta_{*}^{j},y^{j}\right\}_{j=1}^{C}\).
```

**Algorithm 1** Gradient Matching to generate camouflages

### Evaluations on CIFAR-10

CIFAR-10 (Krizhevsky, 2009) is a multiclass classification problem with \(10\) classes, with 6,000 color images in each class of size \(32\times 32\). We follow the standard split into 50,000 training images and 10,000 test images.

#### 4.1.1 Support Vector Machines

In order to perform evaluations on SVM, we first convert the CIFAR-10 dataset into a binary classification dataset (which we term as Binary-CIFAR-10) by merging the \(10\) classes into two groups: animal (\(y=+1\)) and machine (\(y=-1\))). Images (in both training and test datasets) that were originally labeled (_bird, cat, deer, dog, frog, horse_) are relabeled animal, and the remaining images, with original labels (_airplane, cars, ship, truck_), are labeled machine.

We train a linear SVM (no kernel was used) with the hinge loss. We evaluate both label flipping and gradient matching to generate camouflages, and different threat models \((\varepsilon,b_{p},b_{c})\); the results are reported in Table 1. Training details and hyperparameters are given in Appendix B.3. Each poison and camouflage generation took about 40 - 50 seconds (for \(b_{p}=b_{c}=0.2\%\)). Each trained model had validation accuracy of around 81.63% on the clean dataset \(S_{\mathrm{cl}}\), which did not change significantly when we retrained after adding poison samples and/or camouflage samples. Note that the efficacy of the camouflaged poisoning attack was more than \(70\%\) in most of the experiments. We give examples of the generated poisons/camouflages in Appendix B.5.

#### 4.1.2 Neural Networks

We perform extensive evaluations on the (multiclass) CIFAR-10 classification task with various popular large-scale neural networks models including VGG-11, VGG-16 (Simonyan and Zisserman, 2015), ResNet-18, ResNet-34, ResNet-50 (He et al., 2016), and MobileNetV2 (Sandler et al., 2018), trained using cross-entropy loss. Details on training setup and hyperparameters are in Appendix B.3.

We report the efficacy of our camouflaged poisoning attack across different models and threat models \((\varepsilon,b_{p},b_{c})\) in Figure 3; also see Appendix B.3 for detailed results. Each model was trained to have

\begin{table}
\begin{tabular}{c|c c|c c}
**Attack type** & \multicolumn{2}{c|}{**Attack success**} & \multicolumn{2}{c}{**Validation Accuracy**} \\ \((\varepsilon,b_{p},b_{c})\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline LF \((8,0.2\%,0.2\%)\) & 70\% & 71.5\% & 81.63 & 81.73 (\(\pm\) 0.14) & 81.74 (\(\pm\) 0.20) \\ LF \((16,0.2\%,0.2\%)\) & 100\% & 40\% & 81.63 & 81.64 (\(\pm\)0.03) & 81.6 (\(\pm\)0.02) \\ GM \((8,0.2\%,0.4\%)\) & 70\% & 100\% & 81.63 & 81.65 (\(\pm\)0.01) & 81.62 (\(\pm\)0.02) \\ GM \((16,0.2\%,0.4\%)\) & 100\% & 70\% & 81.63 & 81.65 (\(\pm\)0.03) & 81.63 (\(\pm\) 0.02) \\ \end{tabular}
\end{table}
Table 1: Camouflaged poisoning attack on linear SVM on Binary-CIFAR-10 dataset. The first column lists the threat model \((\varepsilon,b_{p},b_{c})\) and the camouflaging type “LF” for label flipping and “GM” for gradient matching. The implementation details are given in Appendix B.3.

validation accuracy between 81-87% (depending on the architecture), which changed minimally when the model was retrained with poisons and/or camouflages. Camouflaging was successful at least 70% of the time for VGG-11, VGG-16, Resnet-18, and Resnet-34 and about \(30\%\) of the time for MobileNetV2 and Resnet-50.

Note that even a small attack success rate represents a credible threat: first, these attack success rates are comparable with those of Geiping et al. (2021), and thus future or undisclosed targeted data poisoning attacks are likely to be more effective. Second, since these success rates are over the choice of the attacker's random seed, the attacker can locally repeat the attack several times and deploy a successful one. Therefore, low success rates do not imply a fundamental barrier, and can be overcome by incurring a computational overhead - by repeating an attack 10x, even an attack with only 20% success rate can be boosted to \(\approx 90\%\) success rate.

### Evaluations on Imagenette and Imagewoof

We evaluate the efficacy of our attack vector on the challenging multiclass classification problem on the Imagenette and Imagewoof datasets Howard (2019), both of which are subsets consisting of 10 classes from the Imagenet dataset Russakovsky et al. (2015) with about 900 images per class.

We evaluate our camouflaged poisoning attack on two different neural network architectures-VGG-16 and ResNet-18 (trained with cross entropy loss), and different threat models \((\varepsilon,b_{p},b_{c})\) listed in Table 2. More details about the dataset, experiment setup and hyperparameters are provided in Appendix B.4. In our experiments, camouflaging was successful for at least \(50\%\) of the time when poisoning was successful. However, because we modified about 13% of the training dataset when adding poisons/camouflages, we observe that the fluctuation in the model's validation accuracy can be up to 7%, which is expected since we make such a large change in the training set.

### Additional Experiments

**Ablation experiments.** In the appendix, we provide the additional experiments on CIFAR-10, showing that:

* Our attack is robust to random deletions of generated poison and camouflage samples during evaluation (Appendix B.6.2), and to training with data augmentation.
* Our attack successfully transfers across different models, i.e., when the victim model is different from the model on which poison and camouflage samples were generated. (Appendix B.6.3)

\begin{table}
\begin{tabular}{c|c|c c c|c c} \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Model**} & \multicolumn{3}{c|}{**Threat Model**} & \multicolumn{2}{c}{**Attack Success**} \\  & & \(\varepsilon\) & \(b_{p}\) & \(b_{c}\) & Poison & Camou \\ \hline IN & VGG-16 & 16 & 6.3\% & 6.3\% & 25\% & 100\% \\ IN & Resnet-18 & 16 & 6.3\% & 6.3\% & 40\% & 50\% \\ IW & Resnet-18 & 16 & 6.6\% & 6.6\% & 50\% & 75\% \\ \end{tabular}
\end{table}
Table 2: Evaluation of camouflaged poisoning attack on Imagenette (IN) and Imagewoof (IW) datasets over \(5\) seeds (with 1 restart per seed). Note that camouflaging succeeded in most of the experiments in which poisoning succeeded.

Figure 3: Efficacy of the proposed camouflaged poisoning attack on CIFAR-10 dataset. The left plot gives the success for the threat model \(\varepsilon=16,b_{p}=0.6\%,b_{c}=0.6\%\) for different neural network architectures. The right plot gives the success for ResNet-18 architecture for different threat models.

* Our attack is successful across different threat models i.e., different values of \(b_{p}\) and \(b_{c}\) (Appendix B.6.1).
* The poison and camouflage samples generated in our experiments have similar feature space distributions, and thus data sanitization defenses (e.g., Diakonikolas et al. (2019)) based on feature distributions will not succeed in removing the generated poison and camouflage samples (Appendix B.6.5).

**Approximate unlearning.** In all our experiments so far, the victim retrains a model from scratch (on the leftover training data) to fulfil the unlearning request, i.e., the victim performs exact unlearning. In the past few years there has been significant research effort in developing approximate unlearning algorithms under various structural assumptions (e.g., convexity (Sekhari et al., 2021; Guo et al., 2020)), or under availability of large memory (Bourtoule et al., 2021; Brophy and Lowd, 2021; Graves et al., 2021), etc.), and one may wonder whether attacks are still effective under approximate unlearning. Unfortunately, most existing approximate unlearning approaches are not efficient (either requiring strong assumptions or taking too much memory) for large-scale deep learning settings considered in this paper, and thus we were unable to evaluate our attack against these methods with our available resources. However, we provide evaluations against the Amnesiac Unlearning algorithm of Graves et al. (2021), which we were able to implement ( Appendix B.6.6). We note that our attack continues to be effective even against this approximate unlearning method.

## 5 Discussion and Conclusion

We demonstrated a new attack vector, _camouflaged poisoning attacks_, against machine learning pipelines where training points can be _unlearned_. This shows that as we introduce new functionality to machine learning systems, we must be aware of novel threats that emerge. We outline a few extensions and future research directions:

* Our method for generating poison and camouflage points was based on the gradient-matching attack of Geiping et al. (2021). However, the attack framework could accommodate effectively any method for targeted poisoning, e.g., the methods of Aghakhani et al. (2021) or Huang et al. (2020), or any method introduced in the future. We provide preliminary experiments in Appendix C of camouflaged-poisoning attacks developed using the Bullseye Polytope technique from Aghakhani et al. (2021), and show that the attack continues to succeed.
* While we only performed experiments with a single target point, similar to Geiping et al. (2021) (and several other works in the targeted data poisoning literature), the proposed attack extends straightforwardly to a collection of targets (rather than a single target). This can be done by calculating the gradient of multiple (instead of a single) target images and using it in the gradient matching in (5) and (3).
* In order to generate poisons and camouflages, our approach needs the ability to query gradients of a trained model at new samples, thus the attack is not black-box. While the main contribution of the paper was to expose that machine unlearning (which is a new and emerging but still under-developed technology) can lead to a new kind of vulnerability called a camouflaged poisoning attack, performing such an attack in a black-box setting is an interesting research question.
* While we could not verify our attack against other approximate unlearning methods in the literature (Bourtoule et al., 2021; Brophy and Lowd, 2021; Sekhari et al., 2021; Guo et al., 2020) due to lack of resources needed to implement them, we believe that our attack should be effective against any provable approximate unlearning approach. This is because the objective of approximate unlearning is to output a model that is statistically-indistinguishable from the model retrained-from-scratch on the remaining data, and the experiments provided in the paper directly evaluate on the retrained-from-scratch model (which is the underlying objective that all approximate unlearning methods aim to emulate). Verifying the success of our approach against other approximate unlearning approaches is an interesting future research direction.
* We considered a two-stage process in this paper, with one round of learning and one round of learning. It would also be interesting to explore what kinds of threats are exposed by even more dynamic systems where points can be added or removed in an online fashion.
* Finally, it is interesting to determine what other types of threats can be camouflaged, e.g., indiscriminate (Lu et al., 2022) or backdoor poisoning attacks (Chen et al., 2017; Saha et al., 2020). Beyond exploring this new attack vector, it is also independently interesting to understand how one can neutralize the effect of an attack by _adding_ points.

#### Acknowledgements

We thank Chirag Jindal for useful discussions in the early phase of the project. AS thanks Karthik Sridharan for helpful discussions. GK is supported by a University of Waterloo startup grant, a Canada CIFAR AI Chair, an NSERC Discovery Grant, and an unrestricted gift from Google. JA is supported in part by the grant NSF-CCF-1846300 (CAREER), NSF-CCF-1815893, and a Google Faculty Fellowship. Computational resources were provided in part by GK's Resources for Research Groups grant from the Digital Research Alliance of Canada.

## References

* Aghakhani et al. (2021) Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna. Bullseye polytope: A scalable clean-label poisoning attack with improved transferability. In _2021 IEEE European Symposium on Security and Privacy (EuroS&P)_, pages 159-178. IEEE, 2021.
* Barreno et al. (2010) Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The security of machine learning. _Machine Learning_, 81(2):121-148, 2010.
* Biggio et al. (2012) Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In _Proceedings of the 29th International Conference on Machine Learning_, ICML '12, pages 1467-1474. JMLR, Inc., 2012.
* Bourtoule et al. (2021) Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _proceedings of the 42nd IEEE Symposium on Security and Privacy_, SP '21, Washington, DC, USA, 2021. IEEE Computer Society.
* Brophy and Lowd (2021) Jonathan Brophy and Daniel Lowd. Machine unlearning for random forests. In _Proceedings of the 38th International Conference on Machine Learning_, ICML '21, pages 1092-1104. JMLR, Inc., 2021.
* Cao and Yang (2015) Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In _Proceedings of the 36th IEEE Symposium on Security and Privacy_, SP '15, pages 463-480, Washington, DC, USA, 2015. IEEE Computer Society.
* Carlini et al. (2022) Nicholas Carlini, Matthew Jagielski, Nicolas Papernot, Andreas Terzis, Florian Tramer, and Chiyuan Zhang. The privacy onion effect: Memorization is relative. _arXiv preprint arXiv:2206.10469_, 2022.
* Chen et al. (2021) Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. When machine unlearning jeopardizes privacy. In _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, pages 896-911, 2021.
* Chen et al. (2017) Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. _arXiv preprint arXiv:1712.05526_, 2017.
* Chourasia et al. (2022) Rishav Chourasia, Neil Shah, and Reza Shokri. Forget unlearning: Towards true data-deletion in machine learning. _arXiv preprint arXiv:2210.08911_, 2022.
* Cina et al. (2022) Antonio Emanuele Cina, Kathrin Grosse, Ambra Demontis, Sebastiano Vascon, Werner Zellinger, Bernhard A Moser, Alina Oprea, Battista Biggio, Marcello Pelillo, and Fabio Roli. Wild patterns reloaded: A survey of machine learning security against training data poisoning. _arXiv preprint arXiv:2205.01992_, 2022.
* Diakonikolas et al. (2019a) Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In _International Conference on Machine Learning_, pages 1596-1606. PMLR, 2019a.
* Diakonikolas et al. (2019b) Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In _Proceedings of the 36th International Conference on Machine Learning_, ICML '19, pages 1596-1606. JMLR, Inc., 2019b.
* Diakonikolas et al. (2019c)Min Du, Zhi Chen, Chang Liu, Rajvardhan Oak, and Dawn Song. Lifelong anomaly detection through unlearning. In _Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security_, pages 1283-1297, 2019.
* Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Proceedings of the 3rd Conference on Theory of Cryptography_, TCC '06, pages 265-284, Berlin, Heidelberg, 2006. Springer.
* Federal Trade Commission (2021) Federal Trade Commission. California company settles ftc allegations it deceived consumers about use of facial recognition in photo storage app, January 2021.
* Geiping et al. (2021) Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. Witches' brew: Industrial scale data poisoning via gradient matching. In _Proceedings of the 9th International Conference on Learning Representations_, ICLR '21, 2021.
* Garet (2016) General Data Protection Regulation. Regulation (EU) 2016/679 of the European parliament and of the council of 27 April 2016, 2016.
* Ginart et al. (2019) Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making AI forget you: Data deletion in machine learning. In _Advances in Neural Information Processing Systems 32_, NeurIPS '19, pages 3518-3531. Curran Associates, Inc., 2019.
* Golatkar et al. (2020) Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In _Proceedings of the 2020 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, CVPR '20, pages 9304-9312, Washington, DC, USA, 2020. IEEE Computer Society.
* Golatkar et al. (2021) Aditya Golatkar, Alessandro Achille, Avinash Ravichandran, Marzia Polito, and Stefano Soatto. Mixed-privacy forgetting in deep networks. In _Proceedings of the 2021 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, CVPR '21, pages 792-801. IEEE Computer Society, 2021.
* Goldblum et al. (2020) Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. _arXiv preprint arXiv:2012.10544_, 2020.
* Graves et al. (2021) Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11516-11524, 2021.
* Gu et al. (2017) Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* Guo et al. (2020) Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. In _Proceedings of the 37th International Conference on Machine Learning_, ICML '20, pages 3832-3842. JMLR, Inc., 2020.
* Gupta et al. (2021) Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, and Chris Waites. Adaptive machine unlearning. In _Advances in Neural Information Processing Systems 34_, NeurIPS '21, pages 16319-16330. Curran Associates, Inc., 2021.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the 2016 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, CVPR '16, pages 770-778, Washington, DC, USA, 2016. IEEE Computer Society.
* Howard (2019) Jeremy Howard. Imagenette, 2019. URL [https://github.com/fastai/imagenette/](https://github.com/fastai/imagenette/).
* Huang et al. (2020) W Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoison: Practical general-purpose clean-label data poisoning. In _Advances in Neural Information Processing Systems 33_, NeurIPS '20, pages 12080-12091. Curran Associates, Inc., 2020.
* Commissioner's Office (2020) Information Commissioner's Office. Guidance on the ai auditing framework, February 2020.
* Howard (2019)Daphne Ippolito, Florian Tramer, Milad Nasr, Chiyuan Zhang, Matthew Jagielski, Katherine Lee, Christopher A Choquette-Choo, and Nicholas Carlini. Preventing verbatim memorization in language models gives a false sense of privacy. _arXiv preprint arXiv:2210.17546_, 2022.
* Izzo et al. (2021) Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learning models: Algorithms and evaluation. In _Proceedings of the 24th International Conference on Artificial Intelligence and Statistics_, AISTATS '21, pages 2008-2016. JMLR, Inc., 2021.
* Katharopoulos and Fleuret (2019) Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with importance sampling, 2019.
* Kingma and Ba (2015) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proceedings of the 3rd International Conference on Learning Representations_, ICLR '15, 2015.
* Koh et al. (2022) Pang Wei Koh, Jacob Steinhardt, and Percy Liang. Stronger data poisoning attacks break data sanitization defenses. _Machine Learning_, 111(1):1-47, 2022.
* Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.
* Lu et al. (2022) Yiwei Lu, Gautam Kamath, and Yaoliang Yu. Indiscriminate data poisoning attacks on neural networks. _arXiv preprint arXiv:2204.09092_, 2022.
* Marchant et al. (2022) Neil G Marchant, Benjamin IP Rubinstein, and Scott Alfeld. Hard to forget: Poisoning attacks on certified machine unlearning. In _Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence_, volume 36 of _AAAI '22_, pages 7691-7700, 2022.
* Munoz-Gonzalez et al. (2017) Luis Munoz-Gonzalez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient optimization. In _Proceedings of the 10th ACM workshop on artificial intelligence and security_, pages 27-38, 2017.
* Neel et al. (2021) Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In _Proceedings of the 32nd International Conference on Algorithmic Learning Theory_, ALT '21. JMLR, Inc., 2021.
* Nguyen et al. (2020) Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. Variational bayesian unlearning. _Advances in Neural Information Processing Systems_, 33:16025-16036, 2020.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In _Advances in Neural Information Processing Systems 32_, NeurIPS '19, pages 8026-8037. Curran Associates, Inc., 2019.
* Paudice et al. (2018) Andrea Paudice, Luis Munoz-Gonzalez, and Emil C Lupu. Label sanitization against label flipping poisoning attacks. In _Joint European conference on machine learning and knowledge discovery in databases_, pages 5-15. Springer, 2018.
* Pedregosa et al. (2011) Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12(85):2825-2830, 2011.
* Russakovsky et al. (2015) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115(3):211-252, 2015.
* Saha et al. (2020) Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 11957-11965, 2020.
* Saha et al. (2018)Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In _Proceedings of the 2018 IEEE Computer Society Conference on Computer Vision and Pattern Recognition_, CVPR '18, pages 4510-4520, Washington, DC, USA, 2018. IEEE Computer Society.
* Schwarzschild et al. (2021) Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein. Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks, 2021.
* Sekhari et al. (2021) Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. In _Advances in Neural Information Processing Systems 34_, NeurIPS '21, pages 18075-18086. Curran Associates, Inc., 2021.
* Shafahi et al. (2018) Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. _Advances in neural information processing systems_, 31, 2018.
* Simonyan and Zisserman (2015) Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale visual recognition. In _Proceedings of the 3rd International Conference on Learning Representations_, ICLR '15, 2015.
* Sommer et al. (2020) David Marco Sommer, Liwei Song, Sameer Wagh, and Prateek Mittal. Towards probabilistic verification of machine unlearning. _arXiv preprint arXiv:2003.04247_, 2020.
* Steinhardt et al. (2017) Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In _Advances in Neural Information Processing Systems 30_, NeurIPS '17, pages 3520-3532. Curran Associates, Inc., 2017.
* Suciu et al. (2018) Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor Dumitras. When does machine learning {FAIL}? generalized transferability for evasion and poisoning attacks. In _27th USENIX Security Symposium (USENIX Security 18)_, pages 1299-1316, 2018.
* Sun et al. (2019) Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan. Can you really backdoor federated learning? _arXiv preprint arXiv:1911.07963_, 2019.
* Thudi et al. (2022) Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. In _2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P)_, pages 303-319. IEEE, 2022.
* Tran et al. (2018) Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In _Advances in Neural Information Processing Systems 31_, NeurIPS '18, pages 8011-8021. Curran Associates, Inc., 2018.
* Ullah et al. (2021) Enayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. Machine unlearning via algorithmic stability. In _Conference on Learning Theory_, pages 4126-4142. PMLR, 2021.
* Xiao et al. (2012) Han Xiao, Huang Xiao, and Claudia Eckert. Adversarial label flips attack on support vector machines. In _ECAI 2012_, pages 870-875. IOS Press, 2012.
* Xiao et al. (2015) Huang Xiao, Battista Biggio, Blaine Nelson, Han Xiao, Claudia Eckert, and Fabio Roli. Support vector machines under adversarial label contamination. _Neurocomputing_, 160:53-62, 2015.
* Yang et al. (2022) Yu Yang, Tian Yu Liu, and Baharan Mirzasoleiman. Not all poisons are created equal: Robust training against data poisoning. In _International Conference on Machine Learning_, pages 25154-25165. PMLR, 2022.
* Zanella-Beguelin et al. (2020) Santiago Zanella-Beguelin, Lukas Wutschitz, Shruti Tople, Victor Ruhle, Andrew Paverd, Olga Ohrimenko, Boris Kopf, and Marc Brockschmidt. Analyzing information leakage of updates to natural language models. In _Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security_, pages 363-375, 2020.

Gradient Matching for Efficient Poison Generation (Geiping et al., 2021)

In this section, we discuss the key intuition of Geiping et al. (2021) for efficient poison generation. Our objective is to find perturbations \(\Delta\) such that when the model is trained on the poisoned samples, it minimizes the adversarial loss in (1) thus making the victim model predict the wrong label \(y_{\mathrm{adv}}\) on the target sample. However, directly solving (1) is computationally intractable due to bilevel nature of the optimization objective. Instead, one may implicitly minimize the adversarial loss by finding a \(\Delta\) such that for any model parameter \(\theta\),

\[\nabla_{\theta}(\ell(f(x_{\mathrm{tar}},\theta),y_{\mathrm{adv}}))\approx \frac{\sum_{i=1}^{P}\nabla_{\theta}\ell\big{(}f(x^{i}+\Delta^{i},\theta),y^{i} \big{)}}{P}. \tag{4}\]

In essence, (4) implies that gradient based minimization (e.g., using Adam / SGD) of the training loss on poisoned samples also minimizes the adversarial loss. Thus, training a model on \(S_{\mathrm{cl}}+S_{\mathrm{po}}\) will automatically ensure that the model predicts \(y_{\mathrm{adv}}\) on the target sample. Unfortunately, computing \(\Delta\) that satisfies (4) is also intractable as it is required to hold for all values of \(\theta\). The key idea of Geiping et al. (2021) to make poison generation efficient is to relax (4) to only be satisfied for a fixed model \(\theta_{\mathrm{cl}}\)-the model obtained by training on the clean dataset \(S_{\mathrm{cl}}\). To implement this, Geiping et al. (2021) minimize the cosine-similarity loss between the two gradients defined as:

\[\phi(\Delta,\theta)=1-\frac{\left\langle\nabla_{\theta}\ell(f(x_{\mathrm{tar} },\theta),y_{\mathrm{adv}}),\sum_{i=1}^{P}\nabla_{\theta}\ell(f(x_{i}+\Delta_{i },\theta),y_{i})\right\rangle}{\|\nabla_{\theta}\ell(f(x_{\mathrm{tar}},\theta ),y_{\mathrm{adv}})\|\|\sum_{i=1}^{P}\nabla_{\theta}\ell(f(x_{i}+\Delta_{i}, \theta),y_{i})\|}, \tag{5}\]

Geiping et al. (2021) demonstrated that (5) can be efficiently optimized for many popular large-scale machine learning models and datasets. We provide the pseudocode in Algorithm 2.

```
0: Clean network \(f(\cdot;\theta_{\mathrm{clean}})\) trained on uncorrupted base images \(S_{\mathrm{cl}}\), a target (\(x_{\mathrm{tar}},y_{\mathrm{tar}}\)) and an adversarial label \(y_{\mathrm{adv}}\), Poison budget \(P\), perturbation bound \(\varepsilon\), number of restarts \(R\), optimization steps \(M\)
1: Collect a dataset \(S_{\mathrm{po}}=\left\{x^{i},y^{i}\right\}_{i=1}^{P}\) of \(P\) many images whose true label is \(y_{\mathrm{adv}}\).
2:for\(r=1,\ldots R\) restarts do
3: Randomly initialize perturbations \(\Delta\) s.t. \(\|\Delta\|_{\infty}\leq\varepsilon\).
4:for\(k=1,\ldots,M\) optimization steps do
5: Compute the loss \(\phi(\Delta,\theta_{\mathrm{clean}})\) as in (5) using the base poison images in \(S_{\mathrm{po}}\).
6: Update \(\Delta\) using an Adam update to minimize \(\phi\), and project onto the constraint set \(\Gamma\).
7:endfor
8: Amongst the \(R\) restarts, choose the \(\Delta_{*}\) with the smallest value of \(\phi(\Delta_{*},\theta_{\mathrm{clean}})\).
9:endfor
10: Return the poisoned set \(S_{\mathrm{po}}=\left\{x^{i}+\Delta_{*}^{i},y^{i}\right\}_{i=1}^{P}\).
```

**Algorithm 2** Gradient Matching to generate poisons (Geiping et al., 2021)

## Appendix B Implementation Details

### Code

We provide code for our experiments as ready-to-deploy Jupyter notebooks. The code to work with different dataset can be found in:

* SVM_Binary_cifar10.ipynb: Experiments for Binary-CIFAR-10 dataset with linear SVM.
* cifar10.ipynb: Experiments for CIFAR-10 dataset with various neural network models.
* imagenette.ipynb: Experiments for Imagenette / Imagewoof dataset with various neural network models.

We plan to make our code public with the final version of the paper.

[MISSING_PAGE_EMPTY:16]

normalized to have \(\ell_{2}\)-norm \(1\). Each training on Binary-CIFAR-10 dataset took 25 - 30 seconds. In order to generate the poison points, we first use torch.autograd to compute the cosine-similarity loss (5), and then optimize it using Adam optimizer with learning rate 0.001. Each poison and camouflage generation took about 40 - 50 seconds (for \(b_{p}=b_{c}=0.2\%\)). We evaluate both label flipping and gradient matching to generate camouflages, and different threat models \((\varepsilon,b_{p},b_{c})\); the results are reported in Table 6. For each of our experiments we chose \(K=10\) seeds of the form "kkkkkk" where \(k\in\{0,\ldots,9\}\) and the seed 99999. Each trained model had validation accuracy of around 81.63% on the clean dataset \(S_{\text{cl}}\), which did not change significantly when we retrained after adding poison samples and / or camouflage samples. Note that the efficacy of the camouflaged poisoning attack was more than \(70\%\) in most of the experiments. We provide a sample of the generated poisons and camouflages in Figure 9 in Appendix B.5.

#### b.3.2 Support Vector Machines

In order to perform evaluations on SVM, we first convert the CIFAR-10 dataset into a binary classification dataset (which we term as Binary-CIFAR-10) by merging the \(10\) classes into two groups: animal (\(y=+1\)) and machine (\(y=-1\))). Images (in both training and test datasets) that were originally labeled (_bird, cat, deer, dog, frog, horse_) are relabeled animal, and the remaining images, with original labels (_airplane, cars, ship, truck_), are labeled machine.

We train a linear SVM (no kernel was used) with the hinge loss. We evaluate both label flipping and gradient matching to generate camouflages, and different threat models \((\varepsilon,b_{p},b_{c})\); the results are reported in Table 6. Each poison and camouflage generation took about 40 - 50 seconds (for \(b_{p}=b_{c}=0.2\%\)). Each trained model had validation accuracy of around 81.63% on the clean dataset \(S_{\text{cl}}\), which did not change significantly when we retrained after adding poison samples and/or camouflage samples. Note that the efficacy of the camouflaged poisoning attack was more than \(70\%\) in most of the experiments. We give examples of the generated poisons/camouflages in Appendix B.5.

#### b.3.3 Neural Network Experiments

Implementation Details and Hyperparameters.Each model is trained with cross-entropy loss \(\ell(f(x,\theta),y)=-\log(\text{Pr}(y=f(x,\theta)))\) on a single GPU using PyTorch [22], and using mini-batch SGD with weight decay 5e-4, momentum 0.9, learning rate 0.01, batch size 100, and 40 epochs over the training dataset. Each training run took about 45 minutes. The poison and camouflage sets were generated using gradient matching by first defining the cosine-similarity

\begin{table}
\begin{tabular}{c|c c|c c c}
**Attack type** & \multicolumn{2}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ \((\varepsilon,b_{p},b_{c})\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline LF \((8,0.2\%,0.2\%)\) & 70\% & 71.5\% & 81.63 & 81.73 (\(\pm\) 0.14) & 81.74 (\(\pm\) 0.20) \\ LF \((16,0.2\%,0.2\%)\) & 100\% & 40\% & 81.63 & 81.64 (\(\pm 0.03\)) & 81.6 (\(\pm 0.02\)) \\ GM \((8,0.2\%,0.4\%)\) & 70\% & 100\% & 81.63 & 81.65 (\(\pm 0.01\)) & 81.62 (\(\pm 0.02\)) \\ GM \((16,0.2\%,0.4\%)\) & 100\% & 70\% & 81.63 & 81.65 (\(\pm 0.03\)) & 81.63 (\(\pm\) 0.02) \\ \end{tabular}
\end{table}
Table 6: Camouflaged poisoning attack on linear SVM on Binary-CIFAR-10 dataset. The first column lists the threat model \((\varepsilon,b_{p},b_{c})\) and the camouflaging type “LF” for label flipping and “GM” for gradient matching.

Figure 4: Efficacy of the proposed camouflaged poisoning attack on CIFAR-10 dataset. The left plot gives the success for the threat model \(\varepsilon=16,b_{p}=0.6\%,b_{c}=0.6\%\) for different neural network architectures. The right plot gives the success for ResNet-18 architecture for different threat models.

loss using torch.autograd and then minimizing it using Adam with a learning rate of 0.1. Each poison/camouflage generation took about 1.5 hours.8

Footnote 8: Our implementations of Algorithm 1 and 2 are not optimized for computational efficiency, and the provided wall clock times simply serve as a proof of concept that our attack is practically implementable. All computations are performed on a CPU, but can be made significantly faster by using GPU.

Further Results.We next elaborate on the results reported in Figure 3. In Table 7, we report the efficacy of the proposed camouflaged poisoning attack on different neural network architectures where the threat model is given by \(\varepsilon=16,b_{p}=0.6\%,b_{c}=0.6\%\). The reported results are an average over 5 seeds from 2000000000-2000000111. In the first column under attack success, we report the number of times poisoning was successful amongst the run trials, and in the second column, we report the number of times camouflaging was successful for the trials for which poisoning was successful.

In Table 8, we report the success of the proposed attack when we change the threat model, but fix the network architecture to be ResNet-18. Each experiment was repeated times 5 times with 8 restarts each time, and the mean success rate is reported. These experiments were conducted with 5 seeds from 2000011111-211111111.

### Additional Details on Imagenette / Imagewoof Experiments

We evaluate the efficacy of our attack vector on the challenging multiclass classification problem on the Imagenette and Imagewoof datasets [Howard, 2019]. Imagenette is a subset of 10 classes (_Tench, English springer, Cassette player, Chain saw, Building/church, French horn, Truck, Gas pump, Golf ball, Parachute_) from the Imagenet dataset [Russakovsky et al., 2015]. The Imagenette dataset consists of around 900 images of various sizes for each class. In total, we have 13394 images which are divided into a training dataset of size 9469 and test dataset of size 3925. To perform training, all images are resized and centrally cropped down to \(224\times 224\) pixels.

Imagewoof [Howard, 2019] is another subset of Imagenet dataset consisting of 10 classes (_Shih-Tzu, Rodesian Ridgeback, Beagle, English Foxhound, Border Terrier, Austrailian Terrier, Golden Retriever,

\begin{table}
\begin{tabular}{c|c c c|c c}
**Network Architecture** & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\  & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline VGG-11 & 100\% & 80\% & 85.01 & 85.03 (\(\pm\) 0.37) & 85.10 (\(\pm\) 0.29) \\ VGG-16 & 80\% & 75\% & 87.68 & 87.42 (\(\pm\) 0.17) & 87.45 (\(\pm\) 0.26) \\ ResNet-18 & 80\% & 75\% & 82.13 & 81.88 (\(\pm\) 0.15) & 81.80 (\(\pm\) 0.12) \\ ResNet-34 & 80\% & 50\% & 82.45 & 82.61 (\(\pm\) 0.30) & 83.12 (\(\pm\) 0.93) \\ ResNet-50 & 80\% & 25\% & 81.02 & 81.76 (\(\pm\) 0.13) & 84.62 (\(\pm\) 0.71) \\ MobileNetV2 & 60\% & 33\% & 82.79 & 83.26 (\(\pm\) 0.25) & 85.47 (\(\pm\) 0.27) \\ \end{tabular}
\end{table}
Table 7: Evaluating our proposed camouflaged poisoning attack on various model architectures on the CIFAR-10 dataset with the threat model \(\varepsilon=16,b_{p}=0.6\%,b_{c}=0.6\%\).

\begin{table}
\begin{tabular}{c c c|c c|c c} \multicolumn{2}{c|}{**Threat model**} & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ \(\varepsilon\) & \(b_{p}\) & \(b_{c}\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline
16 & 1\% & 1\% & 100\% & 80\% & 82.13 & 81.98 (\(\pm\) 0.16) & 82.12 (\(\pm\) 0.21) \\
8 & 1\% & 1\% & 80\% & 75\% & 82.13 & 82.21 (\(\pm\) 0.21) & 82.09 (\(\pm\) 0.23) \\
16 & 2\% & 1\% & 100\% & 20\% & 82.13 & 82.31 (\(\pm\) 0.26) & 82.19 (\(\pm\) 0.24) \\
8 & 2\% & 1\% & 100\% & 40\% & 82.13 & 82.43 (\(\pm\) 0.30) & 82.34 (\(\pm\) 0.27) \\ \end{tabular}
\end{table}
Table 8: Evaluating our proposed camouflaged poisoning attack on various threat models with CIFAR-10 dataset trained on ResNet-18.

Old English Sheep Dog, Samoyed, Dingo_). Imagewoof consists of around 900 images of various sizes for each class, and in total 12954 images which are divided into a training dataset of size 9025 and test dataset of size 3929. Similar to Imagenette, we resize all images and crop to the central \(224\times 224\) pixels before training.

We evaluate our camouflaged poisoning attack on two different neural network architectures-VGG-16 and ResNet-18, and different threat models \((\varepsilon,b_{p},b_{c})\) listed in Table 2. Each model is trained on a single GPU with cross-entropy loss, that is minimized using SGD algorithm with weight decay 5e-4, momentum 0.9 and batch size 20. We start with a learning rate of 0.01, and exponentially decay it with \(\gamma=0.9\) after every epoch, for a total of 50 epochs over the training dataset. The poisons and camouflages were generated using gradient matching by first defining the cosine-similarity loss using torch.autograd and then optimizing it using Adam optimizer with learning rate 0.1.

Figure 5: Visualization of poisons and camouflages on Imagewoof dataset. The first and the third columns shows the original images, and the second and the fourth columns shows the corrupted images (with added \(\Delta\)). The shown images were generated for a camouflaged poisoning attack on ResNet-18, with Seed = 2111111110, \(b_{p}=b_{c}=4.2\%\), \(\varepsilon=16\). The target and camouflage class is _Austrailian Terrier_, and the poison class is _Golden Retriever_.

### Visualizations

Figure 6: Some representative poison and camouflage images for attack on Imagewoof dataset. In each pair, the left figure is the original picture from the training dataset and the right figure has been adversarially manipulated by adding \(\Delta\). The shown images were generated for a camouflaged poisoning attack on Resnet-18, with Seed = \(10000005\), \(b_{p}=b_{c}=6.6\%\) and \(\varepsilon=16\). The target and camouflage class is _English Springer_, and the poison class is _Building (Church)_.

Figure 7: Visualization of poisons and camouflages on Imagenette dataset. The first and the third columns shows the original images, and the second and the fourth columns shows the corrupted images (with added \(\Delta\)). The shown images were generated for a camouflaged poisoning attack on ResNet-18, with Seed = \(2000011111\) and \(\varepsilon=8\). The target and camouflage class is _chain saw_, and the poison class is _French horn_.

Figure 8: Visualization of poisons and camouflages on CIFAR-10 dataset (multiclass classification task). The top row shows the original images and the bottom row shows the corresponding poisoned / camouflaged images (with the added \(\Delta\)). The shown images were generated for a camouflaged poisoning attack on ResNet-18, with Seed = 2000000000, \(\varepsilon=8\), \(b_{p}=0.2,b_{c}=0.4\), poison class _bird_, target class _deer_, and the target ID 9621.

Figure 9: Visualization of poisons and camouflages on Binary-CIFAR-10 dataset (_animal_ vs _machine_ classification). The top row shows the original images and the bottom row shows the corresponding poisoned / camouflaged images (with the added \(\Delta\)). The shown images were generated for a camouflaged poisoning attack on SVM, with Seed = 555555, \(\varepsilon=16\), \(b_{p}=0.2,b_{c}=0.4\), target ID 6646.

### Additional Experiments

#### b.6.1 Ablation Study for Different Values of \(b_{p}\) and \(b_{c}\)

In this section, we explore the effect of changing the relative sizes of the poison samples and camouflage samples. The experiments are performed on ResNet-18 for CIFAR-10 dataset with \(b_{p}=1\%\) and \(b_{c}=\{0.5,1,1.5\}\%\) respectively, and vice-versa. We report the results in Table 9.

One may also wonder what is the price that an attacker has to pay in terms of the success rate of a poisoning (only) attack if it chooses to devote a part of the budget for camouflages. In Table 10, we report the results for comparing the success rate 1% poisons and 1% camouflages, to success rate with 2% poisons (and no camouflages.)

#### b.6.2 Robustness of the Proposed Attack to Random Deletions

We next explore the effect of random removal of the generated poison and camouflage samples on the success of our attack. In a data-scraping scenario, a victim may not scrape all the data points modified by an attacker. In Table 11, we report the effect on attack success when different amounts of the generated poison and camouflage samples are deleted uniformly at random.

\begin{table}
\begin{tabular}{c c|c c|c c c}
**Problem parameters** & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ \(b_{p}\) & \(b_{c}\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline
0.5\% & 0.5\% & 33\% & 50\% & 0.822 & 0.8212 & 0.8238 \\
1\% & 1\% & 83\% & 80\% & 0.822 & 0.8274 & 0.8244 \\
2\% & 0 & 83\% & N/A & 0.822 & 0.817 & N/A \\ \hline \end{tabular}
\end{table}
Table 10: Comparison of success rate when the allocated budget of 2% is split as: (a) 1% poisons and 1% camouflage, and (b) 2% poison samples and 0% camouflages. The experiment is performed on CIFAR-10 dataset trained on ResNet-18 with \(\varepsilon=16\). The reported success rates are averaged over six different trials with seeds 200000111 - 211111111. For each experiment, we do 4 restarts per poison or camouflage generation.

\begin{table}
\begin{tabular}{c c|c c|c c c}
**Problem parameters** & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ \(b_{p}\) & \(b_{c}\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline
1\% & 1.5\% & 83\% & 80\% & 0.822 & 0.8191 & 0.8264 \\
1\% & 0.5\% & 83\% & 40\% & 0.845 & 0.8395 & 0.8432 \\
1\% & 1\% & 83\% & 80\% & 0.822 & 0.8274 & 0.8244 \\
0.5\% & 1\% & 33\% & 50\% & 0.822 & 0.8261 & 0.8194 \\
1.5\% & 1\% & 83\% & 20\% & 0.822 & 0.8156 & 0.8118 \\ \hline \end{tabular}
\end{table}
Table 9: Effect of different sizes of poison and camouflage datasets on the success of the proposed camouflaged poisoning attack on CIFAR-10 dataset trained on ResNet-18, with \(\varepsilon=16\). The reported success rates are averaged over six different trials with seeds 200000111-211111111. For each experiment, we do 4 restarts for every poison and camouflage generation.

#### b.6.3 Transfer Experiments

In this section, we show that the poison and camouflage samples generated by the proposed approach transfer across models. Thus, an attacker can successfully execute the camouflaged poisoning attack, even if the victim trains a different model than the one on which the poison and camouflage samples were generated. We show the transfer success in Figure 10. The brewing network denotes the network architecture on which poison and camouflage samples were generated (we adopt the same notation as Geiping et al. (2021)). The victim network denotes the model architecture used by the victim for training on the manipulated dataset.

We ran a total of 3 experiments per (brewing model, victim model) pair using the seeds 200000000-2000000011. Each reported number denotes the fraction of times when both poisoning and camouflaging were successful in the transfer experiment, and thus the attack could take place.

#### b.6.4 Robustness to Data Augmentation

Data augmentation is commonly used to avoid overfitting in deep neural networks. In order to be applicable in the real life, our poisoning and camouflaging attacks must be successful even when the model is trained with data augmentation. In order to validate this, we evaluate our approach on CIFAR-10 dataset trained with data augmentation on ResNet-18 in the threat model \(\varepsilon=16,b_{p}=b_{c}=1\%\); the results are in Table 12. The considered data augmentations are:

1. _No Augmentation_: Exact images from the training dataset are used.
2. _Augmentation Set 1_: \(50\%\) chance that the image will be horizontally flipped, but no rotations.
3. _Augmentation Set 2_: \(50\%\) chance that the image will be horizontally flipped, and random rotations in Uniform(\(-10,10\)) degrees.

The reported results in Table 12 are an average over 5 random seeds from "kkkkkk" where \(1\leq k\leq 5\). As expected, the validation accuracy for the model trained on clean dataset increased from 82%

\begin{table}
\begin{tabular}{c c|c c|c c c} \multicolumn{2}{c|}{**Amount removed**} & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ Poisons & Camouflages & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline
5\% & 5\% & 80\% & 75\% & 0.822 & 0.823 & 0.819 \\
10\% & 10\% & 60\% & 33\% & 0.822 & 0.817 & 0.825 \\
20\% & 20\% & 80\% & 100\% & 0.822 & 0.826 & 0.820 \\
30\% & 30\% & 100\% & 80\% & 0.822 & 0.8215 & 0.818 \\
40\% & 40\% & 20\% & 100\% & 0.822 & 0.825 & 0.817 \\ \hline \end{tabular}
\end{table}
Table 11: Effect of random removal of the generated poison and camouflage samples on the success of the proposed camouflaged poisoning attack on CIFAR-10 dataset trained on ResNet-18, with \(\varepsilon=16\). The reported success rates are averaged over 5 different trials with seeds 200001111 - 21111111. For each experiment, we do 4 restarts for every poison and camouflage generation.

Figure 10: Transfer experiments on CIFAR10 dataset.

percent when trained without augmentation, to 86% for augmentation Set 1 and 88% for augmentation set 2. The addition of data augmentation during training and re-training stages make it harder for poisoning to succeed and at the same time makes it easier for camouflaging to succeed.

#### b.6.5 Similarity of Feature Space Distance

A natural approach to defend against dataset manipulation attacks is to try to identify the modified images, and then remove them from the training dataset (i.e., _data sanitization_), e.g. Diakonikolas et al. (2019). For instance, one could cluster images based on their distance from their class mean image, or from the target image. This type of defense could potentially thwart watermarking poisoning attacks such as Poison Frogs (Shafahi et al., 2018). As we show in Figure 11, such a defense would not be effective against our proposed poison and camouflage generation procedures, as the data distribution for the poison set and the camouflage set is similar to that of the clean images from the respective classes.

#### b.6.6 Approximate Unlearning

In the experiments reported so far, we assumed that the victim performs exact unlearning, i.e. retrains the model from scratch on the leftover training data every time there is an unlearning request. However, in many cases, retraining from scratch for every unlearning request could be computationally expensive. This has inspired a plethora of research over the past few years in machine unlearning to develop approximate unlearning methods that are computationally faster than retraining from scratch (Sekhari et al., 2021; Guo et al., 2020; Bourtoule et al., 2021; Brophy and Lowd, 2021; Graves et al., 2021). One may wonder if our attack also succeeds when performing approximate unlearning. Unfortunately, the algorithms dicussed in the prior works are infeasible for our large-scale deep learning setting; they either require strong structural properties (e.g., convexity (Sekhari et al.,

\begin{table}
\begin{tabular}{c|c c|c c c}
**Data Augmentation** & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\  & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline No Augmentation & 100\% & 20\% & 82\% & 82\% & 82\% \\ Augmentation Set 1 & 86\% & 33\% & 86\% & 85\% & 86\% \\ Augmentation Set 2 & 60\% & 100\% & 88\% & 86 & 86\% \\ \end{tabular}
\end{table}
Table 12: Effect of data augmentation on our proposed camouflaged poisoning attack.

Figure 11: Feature space distance for our generated poison and camouflage set. The reported data was collected by a successful camouflaged poisoning attack on Resnet-18 model trailed on CIFAR-10 with seed \(2000000000\), \(\varepsilon=16\) and \(b_{p}=b_{c}=1\%\).

2021, Guo et al., 2020)) or require access to large memory (Bourtoule et al., 2021, Brophy and Lowd, 2021, Graves et al., 2021), etc. Hence, we were unable to evaluable most of these methods. However, we were successful in implementing the approximate unlearning methods in Graves et al. (2021) called _Amnesiac Unlearning_ withing our available resources and report the results in Table 13.

To give the high level idea of _Amnesiac Unlearning_, note that the final model produced by multi-epoch mini-batch SGD like algorithms is given by:

\[\theta_{final}=\theta_{\mathrm{initial}}+\sum_{e=1}^{E}\sum_{b=1}^{B}\Delta_{ \theta_{e,b}}\]

where \(\Delta_{\theta_{e,b}}\) is the incremental update using batch \(b\) in epoch \(e\). In _Amnesiac Unlearning_, the learner keeps track of which batches contain which sample points. When given an unlearning request, the learner computes the bathces \(SB\) that contain the sensitive data (that requested for removal) and updates the model as

\[\theta_{\mathrm{updated}}=\theta_{\mathrm{initial}}+\sum_{e=1}^{E}\sum_{b=1}^{ B}\Delta_{\theta_{e,b}}-\sum_{sb=1}^{S}B\Delta_{\theta_{sb}}=\theta_{\mathrm{ initial}}-\sum_{sb=1}^{S}B\Delta_{\theta_{sb}}.\]

The above update is followed by a few epochs of fine tuning on left over data samples to recover any loss in validation error. As reported in Graves et al. (2021), the above update method is particularly effective when the sensitive data comprises of about 1-2% of the training dataset, which is the case in our experiments. We refer the reader to Graves et al. (2021) for more details. The reported results in Table 13 were obtained by using the open source implementation of _Amnesiac Unlearning_ by Graves et al. (2021). We note that each experiment on CIFAR-10 took about 200GB of memory.

### Multiple Targets Scenarios

In addition, we performed a limited number of experiments on poisoning and camouflaging multiple targets using the gradient matching method. The results (as shown in Table 14) are comparable to the experiments performed by (Geiping et al., 2021) on multiple targets. We observe that our attack continues to be effective under multiple target settings. Furthermore, choosing a method that is more effective under the multi-target setting may increase the success of our attack.

## Appendix C Alternative Approaches to Generate Poisons and Camouflages

Thus far, our experiments have focused solely on generating poison and camouflage points using the gradient matching technique. In this section, we show that our attack framework is robust to the

\begin{table}
\begin{tabular}{c c c|c c c|c c} \multicolumn{2}{c|}{**Threat model**} & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ \(\varepsilon\) & \(b_{p}\) & \(b_{c}\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline
16 & 0.5\% & 1\% & 40\% & 100\% & 0.913 & 0.765 (\(\pm 0.047\)) & 0.722 (\(\pm 0.060\)) \\
16 & 1\% & 1\% & 40\% & 100\% & 0.913 & 0.760 (\(\pm 0.046\)) & 0.765 (\(\pm 0.046\)) \\ \end{tabular}
\end{table}
Table 13: Evaluating the success of our proposed camouflaged poisoning attack against _Amnesiac Unlearning_ (an approximate unlearning technique) provided by Graves et al. (2021) using five randomly selected seeds provided in the table above. We remark that our attack continues to be effective even against approximate unlearning.

poison generation method: we generate a camouflaged poisoning attack using Bullseye Polytope poisoning--a clean-label targeted attack proposed by Aghakhani et al. (2021). We believe that similar results should hold for a variety of other data poisoning techniques.

Bullseye Polytope (BP) maximizes the similarity between representations of the poisons and target. In doing so, it implicitly minimizes the alignment between poison and target gradients with respect to the penultimate layer, which captures most of the gradient norm variation (Katharopoulos and Fleuret, 2019). In the transfer learning scenario, the poisons are crafted to have a similar representation to that of the target. Here, a linear layer is trained on the poisoned data using the representations obtained from a pre-trained clean model. The gradient of the linear model is proportional to the representations learned by the pre-trained model. Therefore, by maximizing the similarity between the representations of the poisons and the adversarially labeled target, the attack indeed increases the alignment between their gradients (Yang et al., 2022).

In our experiments, a poison set and a camouflage set were crafted and evaluated in a white-box setting, where the method has the highest average attack success rate (Schwarzschild et al., 2021). Two pre-trained ResNet-18 models are used as the feature extractor to generate adversarial examples and as the victim, respectively. We set an \(\ell_{\infty}\) perturbation budget of \(\varepsilon=7\) and perform BP for 1,000 iterations each to obtain poisons and camouflages. We then use Adam with a learning rate of 0.1 to fine-tune the victim's linear classifier on the poisoned dataset for 60 epochs. Finally, we re-load the victim model, repeat the fine-tuning process with the poisoned + camouflaged dataset and compare the results.

\begin{table}
\begin{tabular}{c c c|c c|c c c} \multicolumn{2}{c|}{**Threat model**} & \multicolumn{3}{c|}{**Attack success**} & \multicolumn{3}{c}{**Validation Accuracy**} \\ \(\varepsilon\) & \(b_{p}\) & \(b_{c}\) & Poisoning & Camouflaging & Clean & Poisoned & Camouflaged \\ \hline
8 & 5 & 5 & 90\% & 67\% & 0.870 & 0.8794 (\(\pm 0.012\)) & 0.8737 (\(\pm 0.021\)) \\
8 & 5 & 10 & 90\% & 78\% & 0.870 & 0.8727 (\(\pm 0.002\)) & 0.8716 (\(\pm 0.002\)) \\ \end{tabular}
\end{table}
Table 15: Evaluating the success of our proposed camouflaged poisoning attack with _Bullseye Polytope poisoning_ (a clean-label targeted attack) provided by Aghakhani et al. (2021) on the CIFAR-10 dataset using ten randomly selected seeds provided in the table above. We observe that our attack continues to be effective with different targeted poisoning methods.