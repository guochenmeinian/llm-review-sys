## Reinforcement Learning with Lookahead Information

**Nadav Merlis**

FairPlay Joint Team, CREST, ENSAE Paris

nadav.merlis@ensae.fr

###### Abstract

We study reinforcement learning (RL) problems in which agents observe the reward or transition realizations at their current state _before deciding which action to take_. Such observations are available in many applications, including transactions, navigation and more. When the environment is known, previous work shows that this lookahead information can drastically increase the collected reward. However, outside of specific applications, existing approaches for interacting with unknown environments are not well-adapted to these observations. In this work, we close this gap and design provably-efficient learning algorithms able to incorporate lookahead information. To achieve this, we perform planning using the empirical distribution of the reward and transition observations, in contrast to vanilla approaches that only rely on estimated expectations. We prove that our algorithms achieve tight regret versus a baseline that also has access to lookahead information - linearly increasing the amount of collected reward compared to agents that cannot handle lookahead information.

## 1 Introduction

In reinforcement learning (RL), agents sequentially interact with a changing environment, aiming to collect as much reward as possible. While performing actions that yield immediate rewards is enticing, agents must also bear in mind that actions influence the state of the environment, affecting the potential reward that could be collected in future steps. When the environment is unknown, agents also need to balance reward maximization based on previous data and exploration - gathering of data that might improve future reward collection.

In the standard interaction model, at each timestep, agents first choose an action and only then observe its outcome on the rewards and state dynamics. As such, agents can only maximize the expected rewards, collected through the expected dynamics. Yet, in many applications, some information on the immediate outcome of actions is known _before_ actions are performed. For example, when agents interact through transactions, prices and traded goods are usually agreed upon before performing any exchange ('reward information'). Alternatively, in navigation problems, nearby traffic information is known to the agent before choosing which path to go through ('transition information').

In a recent work, Merlis et al. (2024) shows that even for agents with full statistical knowledge of the environment, such 'lookahead' information can drastically increase the reward collected by agents - by a multiplicative factor of up to \(AH\) when immediate rewards are revealed in advance and \(A^{H/2}\) when observing the immediate future transitions.1 Intuitively, agents do not only gain from instantaneously using this information - they can also adapt their planning to account for lookahead information being revealed in subsequent states, significantly increasing their future values. However, the work of Merlis et al. (2024) only tackles planning settings in which the model is known and does not provide algorithms or guarantees when interacting with unknown environments.

Footnote 1: \(A\) is the size of the action space, \(S\) is the size of the state space and \(H\) is the interaction length.

In this work, we aim to design provably-efficient agents that learn how to interact when given immediate ('one-step lookahead') reward or transition information before choosing an action, under the episodic tabular Markov Decision Process model. While such information can always be embedded into the state of the environment, the state space becomes exponential at best, and continuous at worst, rendering most theoretically-guaranteed approaches both computationally and statistically intractable. To alleviate this, we start by deriving dynamic programming ('Bellman') equations _in the original state space_ that characterize the optimal lookahead policies. Inspired by these update rules, we present two variants to the MVP algorithm (Zhang et al., 2021) that allow incorporating either reward or transition lookahead. In particular, we suggest a planning procedure that uses the empirical distribution of the reward/transition observations (instead of the estimated expectations), which might also be applied to other complex settings. We prove that these algorithms achieve tight regret bounds of \(\tilde{\mathcal{O}}\Big{(}\sqrt{H^{3}SAK}\Big{)}\) and \(\tilde{\mathcal{O}}\Big{(}\sqrt{H^{2}SK}(\sqrt{H}+\sqrt{A})\Big{)}\) after \(K\) episodes (for reward and transition lookahead, respectively), compared to a stronger baseline that also has access to lookahead information. As such, they can collect significantly more rewards than vanilla RL algorithms.

**Outline.** We formally define RL problems with reward/transition lookahead in Section 2 and further discuss the differences between our setting and standard RL problems in Section 3. Then, we present our results in two complementary sections: Section 4 analyzes reward lookahead while Section 5 analyzes transition lookahead. We end with conclusions and future directions in Section 6.

**Related Work.**  Problems with varying lookahead information have been extensively studied in control, with model predictive control (MPC, Camacho et al., 2007) as the most notable example. Conceptually, when interacting with an environment that might be too complex or hard to model, it is oftentimes convenient to use a simpler model that allows accurately predicting its behavior just in the near future. MPC uses such models to repeatedly update its policy using short-term planning. In some cases, the utilized future predictions consist of additive perturbations to the dynamics (Yu et al., 2020), while other cases involve more general future predictions on the model behavior (Li et al., 2019; Zhang et al., 2021; Lin et al., 2021; Liu et al., 2022). To the best of our knowledge, these studies focus on comparing the performance of the controller to one with full future information (and thus, linear regret is inevitable), sometimes also considering prediction errors. They do not, however, attempt to learn the predictions. In contrast, we estimate the reward/transition distributions and leverage them to better plan, thus increasing the value gained by the agent. In addition, these works focus on continuous (mostly linear) control problems, whereas we study tabular settings; results from any one of these settings cannot be directly applied to the other.

In RL, lookahead is mostly used as a planning tool; namely, agents test the possible outcomes after performing multiple steps to decide which actions to take or to better estimate the value (Tamar et al., 2017; Efroni et al., 2019, 2020; Moerland et al., 2020; Rosenberg et al., 2023; El Shar and Jiang, 2020; Biedenkapp et al., 2021; Huang et al., 2019). Specifically, the future value at the end of the lookahead is often estimated using rollouts, and a longer lookahead is more robust to suboptimality of the rollout policy (Bertsekas, 2023). However, when agents actually interact with the environment, no additional lookahead information is observed. One notable exception is (Merlis et al., 2024), which analyzes the potential value increase due to multi-step reward lookahead information (and briefly mentions transition lookahead). However, they only tackle planning settings, where the model is known, and do not study learning. In this work, we continue a long line of literature on regret analysis for tabular RL (Jaksch et al., 2010; Jin et al., 2018; Dann et al., 2019; Zanette and Brunskill, 2019; Efroni et al., 2019, 2021; Simchowitz and Jamieson, 2019; Zhang et al., 2021, 2023). Yet, we are not aware of any existing results on regret minimization with reward or transition lookahead information.

Finally, various applications that involve one-step lookahead information have been previously studied. The most notable ones are prophet problems (Correa et al., 2019), where one-step reward lookahead is obtained, and the Canadian traveler problem with resampling (Nikolova and Karger, 2008), which can be formulated through one-step transition lookahead. We discuss the relation to these problems and the relevant existing results when analyzing each type of feedback, and also discuss the relation between transition lookahead and stochastic action sets (Boutilier et al., 2018).

## 2 Setting and Notations

We study episodic tabular Markov Decision Processes (MDPs), defined by the tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,P,\mathcal{R})\), where \(\mathcal{S}\) is the state space (of size \(S\)), \(\mathcal{A}\) is the action space (of size \(A\)) and \(H\) is the interaction horizon. At each timestep \(h\in\{1,\ldots,H\}\triangleq[H]\) of an episode \(k\in[K]\), an agent, located in state \(s_{h}^{k}\in\mathcal{S}\), chooses an action \(a_{h}^{k}\in\mathcal{A}\) and obtains a reward \(R_{h}^{k}=R_{h}(s_{h}^{k},a_{h}^{k})\sim\mathcal{R}_{h}(s_{h}^{k},a_{h}^{k})\). We assume that the rewards are supported by \([0,1]\) and of expectations \(r_{h}(s,a)\). Afterward, the environment transitions to a state \(s_{h+1}^{k}\sim P_{h}(\cdot|s_{h}^{k},a_{h}^{k})\) and the interaction continues until the end of the episode. We use the notation \(\bm{R}\sim\mathcal{R}_{h}(s)\) (or \(\bm{s}^{\prime}\sim P_{h}(s)\)) to denote reward (next-state) samples for all actions simultaneously at step \(h\) and state \(s\) and assume independence between different timesteps.2 On the other hand, samples from different actions at a specific state/timestep are not necessarily independent.

Footnote 2: This assumption is not used by our algorithms: it is only to ensure that the optimal policy is Markovian.

Reward Lookahead.With one-step reward lookahead at timestep \(h\) and state \(s\), agents first observe the rewards for all actions \(\bm{R}_{h}(s)\triangleq\left\{R_{h}(s,a)\right\}_{a\in\mathcal{A}}\) and only then choose an action to perform. Formally, we define the set of reward lookahead policies as \(\Pi^{R}=\left\{\pi:[H]\times\mathcal{S}\times[0,1]^{A}\mapsto\Delta_{\mathcal{ A}}\right\}\), where \(\Delta_{\mathcal{A}}\) is the probability simplex, and denote \(a_{h}=\pi_{h}(s_{h},\bm{R}_{h})\). The value of a reward lookahead agent is the cumulative rewards gathered by it starting at timestep \(h\) and state \(s\), denoted by

\[V_{h}^{R,\pi}(s)=\mathbb{E}\Bigg{[}\sum_{t=h}^{H}R_{t}(s_{t},\pi_{t}(s_{t},\bm {R}_{t}(s_{t}))|s_{h}=s\Bigg{]}.\]

We also define the optimal reward lookahead value to be \(V_{h}^{R,\,\gamma}(s)=\max_{\pi\in\Pi^{R}}V_{h}^{R,\pi}(s)\). When interacting with an unknown environment for \(K\) episodes, agents sequentially choose reward lookahead policies \(\pi^{k}\in\Pi^{R}\) based on all historical information and are measured by their regret,

\[\mathrm{Reg}^{R}(K)=\sum_{k=1}^{K}\Bigl{(}V_{1}^{R,\ast}(s_{1}^{k})-V_{1}^{R, \pi^{k}}(s_{1}^{k})\Bigr{)}.\]

We allow the initial state of each episode \(s_{1}^{k}\) to be arbitrarily chosen.

Transition Lookahead.Denoting \(s_{h+1}^{\prime}(s,a)\), the future state when playing action \(a\) at step \(h\) and state \(s\), one-step transition lookahead agents observe \(\bm{s}_{h+1}^{\prime}(s)\triangleq\left\{s_{h+1}^{\prime}(s,a)\right\}_{a\in \mathcal{A}}\) before acting. The set of transition lookahead agents is denoted by \(\Pi^{T}=\left\{\pi:[H]\times\mathcal{S}\times\mathcal{S}^{A}\mapsto\Delta_{ \mathcal{A}}\right\}\) with values

\[V_{h}^{T,\pi}(s)=\mathbb{E}\Bigg{[}\sum_{t=h}^{H}R_{t}(s_{t},\pi_{t}(s_{t},\bm {s}_{t+1}^{\prime}(s_{t})))|s_{h}=s\Bigg{]}.\]

The optimal value is \(V_{h}^{T,\,\ast}(s)=\max_{\pi\in\Pi^{T}}V_{h}^{T,\,\pi}(s)\), and we similarly define the regret versus optimal transition lookahead agents as \(\mathrm{Reg}^{T}(K)=\sum_{k=1}^{K}\Bigl{(}V_{1}^{T,\ast}(s_{1}^{k})-V_{1}^{T, \pi^{k}}(s_{1}^{k})\Bigr{)}\).

When the type of lookahead is clear from the context, we sometimes denote values by \(V_{h}^{\pi}\) and \(V_{h}^{\ast}\).

Other Notations.For any \(p\in\Delta_{n}\) and \(V\in\mathbb{R}^{n}\), we define \(\mathrm{Var}_{p}(V)=\sum_{i=1}^{n}p_{i}V_{i}^{2}-\bigl{(}\sum_{i=1}^{n}p_{i}V_{ i}\bigr{)}^{2}\). Also, given a transition kernel \(P\) and a vector \(V\in\mathbb{R}^{S}\), we let \(PV(s,a)=\sum_{s^{\prime}\in\mathcal{S}}P(s^{\prime}|s,a)V(s^{\prime})\) and similarly define it for value or transition kernel differences. We denote by \(n_{h}^{k}(s,a)\), the number of times the pair \((s,a)\) was visited at timestep \(h\) up to episode \(k\) (inclusive) and similarly denote \(n_{h}^{k}(s)=\sum_{a\in\mathcal{A}}n_{h}^{k}(s,a)\). We also let \(\hat{r}_{h}^{k}(s,a)=\frac{1}{n_{h}^{k}(s,a)}\sum_{k^{\prime}=1}^{k}\mathds{1} \Bigl{\{}s_{h}^{k^{\prime}}=s,a_{h}^{k^{\prime}}=a\Bigr{\}}R_{h}^{k^{\prime}}\) and \(\hat{P}_{h}(s^{\prime}|s,a)=\frac{1}{n_{h}^{k}(s,a)}\sum_{k^{\prime}=1}^{k} \mathds{1}\Bigl{\{}s_{h}^{k^{\prime}}=s,a_{h}^{k^{\prime}}=a,s_{h+1}^{k^{ \prime}}=s^{\prime}\Bigr{\}}\) be the empirical expected rewards and transition kernel at \((s_{h},a_{h})=(s,a)\) using data up to episode \(k\) and assume they are initialized to be zero. Finally, we denote by \(\hat{\mathcal{R}}_{h}^{k}(s)\), the empirical reward distribution across all actions, and use \(\hat{P}_{h}^{k}(s)\) to denote the empirical joint next-state distribution for all actions. In particular, if \(k_{i}\) is the \(i^{th}\) episode where \(s\) was visited at step \(h\), to sample \(\bm{R}\sim\hat{\mathcal{R}}_{h}^{k}(s)\), we uniformly sample \(i\sim U\bigl{(}\bigl{[}n_{h}^{k}(s)\bigr{]}\bigr{)}\) and return \(\bm{R}=\Bigl{\{}R_{h}^{k_{i}}(s,a)\Bigr{\}}_{a\in\mathcal{A}}\). A sample \(\bm{s}^{\prime}\sim\hat{P}_{h}^{k}(s)\) similarly returns \(\bm{s}^{\prime}=\Bigl{\{}s_{h+1}^{\prime k_{i}}(s,a)\Bigr{\}}_{a\in\mathcal{A}}\).

When we want to indicate the distribution used to calculate an expectation, we sometimes state it in a subscript, e.g., write \(E_{\mathcal{R}_{h}(s)}[R(a)]\) to indicate that \(R(a)\sim\mathcal{R}_{h}(s,a)\) or use \(\mathbb{E}_{\mathcal{M}}\) to emphasize that all distributions are according to an environment \(\mathcal{M}\). In this paper, \(\mathcal{O}\)-notation only hides absolute constants while \(\tilde{\mathcal{O}}\) hides factors of \(\text{polylog}(S,A,H,K,\delta)\). We also use the notation \(a\lor b=\max\{a,b\}\).

## 3 Comparing the Values of Lookahead Agents and Vanilla RL agents

In the classic RL formulation (e.g., Azar et al., 2017), agents only observe the reward and transition after performing an action and aim to maximize the 'no-lookahead' value, defined by

\[V_{h}^{\pi}(s)=\mathbb{E}\Bigg{[}\!\sum_{t=h}^{H}r_{t}(s_{t},\pi_{t}(s_{t})|s_{ h}=s\Bigg{]},\]

where \(\pi\in\Pi^{\mathcal{M}}=\{\pi:[H]\times\mathcal{S}\mapsto\Delta_{\mathcal{A}}\}\) is a Markovian policy. The optimal value is \(V_{h}^{no}(s)=\max_{\pi\in\Pi^{\mathcal{M}}}V_{h}^{\pi}(s)\) and the regret is classically defined as \(\mathrm{Reg}(K)=\sum_{k=1}^{K}\Bigl{(}V_{1}^{no}(s_{1}^{k})-V_{1}^{\pi^{k}}(s_ {1}^{k})\Bigr{)}\).

By definition, the set of lookahead policies also includes all Markovian policies (since agents are not obliged to use reward/transition information), so the optimal lookahead values are always larger than their no-lookahead counterpart. In other words, denoting the value gain due to lookahead information by \(G^{R}(s)=V_{1}^{R,*}(s)-V_{1}^{no}(s)\) and \(G^{T}(s)=V_{1}^{T,*}(s)-V_{1}^{no}(s)\), it holds that \(G^{R}(s),G^{T}(s)\geq 0\). In terms of regret, for any fixed algorithm, we can also write

\[\mathrm{Reg}(K)=\mathrm{Reg}^{R}(K)-\sum_{k=1}^{K}G^{R}(s_{1}^{k})=\mathrm{Reg }^{T}(K)-\sum_{k=1}^{K}G^{T}(s_{1}^{k}).\]

As the value gains are non-negative, it directly implies that any regret bound w.r.t. the lookahead value also leads to the same bound for the standard regret. Even more so, in most cases, lookahead information leads to a strict improvement in the value, that is, \(G^{R}(s),G^{T}(s)\geq G_{0}>0\). When this happens, any algorithm with sub-linear lookahead regret enjoys a _negative linear_ standard regret:

_If \(\mathrm{Reg}^{R}(K)=o(K)\) and \(G^{R}(s_{1}^{k})\geq G_{0}\) for all \(k\in[K]\), then \(\mathrm{Reg}(K)\leq-G_{0}K+o(K)\)._

The same also holds for transition lookahead. Conversely, any agent that suffers positive standard regret will suffer linear regret compared to the best lookahead agent, i.e.,

_If \(\mathrm{Reg}(K)\geq 0\) and \(G^{R}(s_{1}^{k})\geq G_{0}\) for all \(k\in[K]\), then \(\mathrm{Reg}^{R}(K)\geq G_{0}K\)._

Notably, any agent that does not use lookahead information will suffer linear lookahead regret in any such environment. We now present two illustrative examples for environments where the lookahead value gain is significant, one for reward lookahead and another for transition lookahead.

Reward lookahead.Consider a simple 2-state environment, depicted in Figure 1. Starting at \(s_{i}\), agents can either stay there by playing \(a_{1}\), earning no reward, or play any other action and move to the absorbing \(s_{f}\), obtaining a Bernoulli reward \(Ber(\nicefrac{{1}}{{(A-1)H}})\). Actions in the terminal state \(s_{f}\) yield no reward. Without observing the rewards, agents will arbitrarily move from \(s_{i}\) to \(s_{f}\), obtaining a reward \(V^{no}=\nicefrac{{1}}{{(A-1)H}}\) in expectation. On the other hand, when agents observe the rewards before acting, they should move from \(s_{i}\) to \(s_{f}\) only if a reward was realized for some action (and otherwise, stay in \(s_{i}\) by playing \(a_{1}\)). Such agents will have \((A-1)H\) opportunities to observe a unit reward across all timesteps and actions, collecting in expectation \(V^{R,*}=(1-\nicefrac{{1}}{{(A-1)H}})^{(A-1)H}\geq 1-\nicefrac{{1}}{{e}}\). In other words, just by observing the rewards before acting, the agent's value multiplicatively increases by almost \(V^{R,*}/V^{no}\approx AH\). Moreover, the additive value gain is \(G^{R}\approx 1-\nicefrac{{1}}{{e}}\), so sub-linear lookahead regret with reward information results with a negatively-linear standard regret of \(\mathrm{Reg}(K)\lesssim-(1-\nicefrac{{1}}{{e}})K\).

Transition lookahead.Consider a chain of \(H/2\) states (also described in further detail at Appendix C.9 and depicted at Figure 2). In each state, one action deterministically keeps the agent in its

Figure 1: Two-state prophet-like problem

current state, while all other actions move the agent one state forward w.p. \(1/A\), but lead to a terminal non-rewarding state otherwise. If the reward is located at the end of the chain, any standard RL agent can collect it only at an exponentially low probability. On the other hand, transition lookahead agents would move forward only if there is an action that allows it while staying at their current state otherwise; such agents will collect the rewards at the end of the chain with constant probability. More specifically, any no-lookahead agent can collect at most \(V^{no}=\mathcal{O}(HA^{-H/2})\) rewards, while transition lookahead agents can collect \(V^{T,*}=\Omega(H)\); as such, lookahead agents achieve exponential increase in value, and sublinear regret versus the best lookahead agent will yield a standard regret of \(\mathrm{Reg}(K)\lesssim-HK\).

In the following sections, we will present agents that are guaranteed to always achieve sublinear regret compared to the best lookahead agent.

## 4 Planning and Learning with One-Step Reward Lookahead

In this section, we analyze RL settings with one-step reward lookahead, in which immediate rewards are observed before choosing an action. One well-known example of this situation is the prophet problem (Correa et al., 2019), where an agent sequentially observes values from known distributions. Upon observing a value, the agent decides whether to take it as a reward and stop the interaction, or discard it and continue to observe more values. This problem has numerous applications and extensions concerning auctions and posted-price mechanisms (Correa et al., 2017). As shown in (Merlis et al., 2024), it is critical to observe the distribution values before taking a decision; otherwise, the agent's revenue can decrease by a factor of \(H\). Notably, the example presented in Figure 1 is a small variant of the prophet problem, where the agent can either take one of \(A-1\) values and finish the interaction or discard them and continue playing by staying at \(s_{i}\); we showed that for this example, the lookahead information increases the value by a factor of \(V^{R,*}/V^{no}\approx AH\).

The most natural way to tackle this setting is to extend (augment) the state space to contain the observed rewards; this way, we transition from a state and reward observations to a new state with new reward observations and return to the vanilla MDP formulation. However, this comes at a great cost. Even for Bernoulli rewards, there are \(2^{A}\) possible reward combinations at any given state, and the augmentation increases the state space by this factor - leading to an exponentially-large state space. Even worse, for continuous rewards, the augmented state space becomes continuous, and any performance guarantees that depend on the size of the state space immediately become vacuous. Hence, algorithms that naively use this reduction are expected to be both computationally and statistically intractable. We refer to Appendix B.2 for further details on one such augmentation.

We take a different approach and derive Bellman equations for this setting in the _original state space_.

**Proposition 1**.: _The optimal value of one-step reward lookahead agents satisfies_

\[V^{R,*}_{H+1}(s)=0, \forall s\in\mathcal{S},\] \[V^{R,*}_{h}(s)=\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Bigg{[} \underset{a\in\mathcal{A}}{\max}\Bigg{\{}R_{h}(s,a)+\sum_{s^{\prime}\in \mathcal{S}}P_{h}(s^{\prime}|s,a)V^{R,*}_{h+1}(s^{\prime})\Bigg{\}}\Bigg{]}, \forall s\in\mathcal{S},h\in[H].\]

_Also, given reward observations \(\bm{R}=\{R(a)\}_{a\in\mathcal{A}}\) at state \(s\) and step \(h\), the optimal policy is_

\[\pi^{*}_{h}(s,\bm{R})\in\underset{a\in\mathcal{A}}{\arg\max}\Bigg{\{}R(a)+ \sum_{s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)V^{R,*}_{h+1}(s^{\prime}) \Bigg{\}}.\]

We prove Proposition 1 in Appendix B.2, where we present an equivalent environment with extended state space in which one could apply the standard Bellman equations (Puterman, 2014) to calculate the value with reward lookahead. In contrast to the previously discussed augmentation approach, we find it more convenient to divide the augmentation into two steps - at odd steps \(2h-1\), the augmented environment would be in a state \(s_{h}\times\bm{0}\), while at even steps \(2h\), the state is \(s_{h}\times\bm{R}_{h}\). Doing so creates an overlap between the values of the original and augmented environments at odd steps, simplifying the proofs. We also use this augmentation to prove a variant of the law of total variance (LTV, e.g. Azar et al., 2017) and a value-difference lemma (e.g. Efroni et al., 2019).

We remark that calculating the exact value is not always tractable - even for \(S=H=1\) (bandit problems) and Gaussian rewards, Proposition 1 requires calculating the expectation of the maximum of Gaussian random variables, which does not admit any simple closed-form solution. On the other hand, these equations allow approximating the value by using reward samples - in the following, we show that it can be used to achieve tight regret bounds when the environment is unknown.

### Regret-Minimization with Reward Lookahead

We now present a tractable algorithm that achieves tight regret bounds with one-step reward lookahead. Specifically, we modify the Monotonic Value Propagation (MVP) algorithm (Zhang et al., 2021) to perform planning using the _empirical reward distributions_ - instead of using the empirical reward expectations. To compensate for transition uncertainty, we add a transition bonus that uses the variance of the optimistic next-state values (w.r.t. the empirical transition kernel), designed to be monotone in the future value. Such construction permits using the variance of optimistic values for the bonus calculation while being able to later replace it with the variance of the optimal value (see discussion in Zhang et al. 2021). A reward bonus is used for the value calculation, but does not affect the action choice in the current state. Intuitively, this is because we get the same amount of information for all the actions of a state, so they have the same level of uncertainty - there is no need for bonuses to encourage reward exploration at the action level.

A high-level description of the algorithm is presented in Algorithm 1, while the full algorithm and its bonuses are stated in Appendix B.3. Notice that the planning requires calculating the expected maximum using the empirical distribution, whose support always contains at most \(K\) elements, so both the memory and computations are polynomial. The algorithm ensures the following guarantees:

```
1:Require:\(\delta\in(0,1)\), bonuses \(b^{r}_{k,h}(s),b^{p}_{k,h}(s,a)\)
2:for\(k=1,2,...\)do
3: Initialize \(\bar{V}^{k}_{H+1}(s)=0\)
4:for\(h=H,H-1,..,1\)do
5: Calculate the truncated values for all \(s\in\mathcal{S}\) \[\bar{V}^{k}_{h}(s)=\min\biggl{\{}\mathbb{E}_{\bm{R}\sim\bar{\mathcal{R}}^{k-1 }_{h}(s)}\biggl{[}\max_{a\in\mathcal{A}}\Bigl{\{}R(a)+b^{p}_{k,h}(s,a)+\hat{P} ^{k-1}_{h}\bar{V}^{k}_{h+1}(s,a)\Bigr{\}}\biggr{]}+b^{r}_{k,h}(s),H\biggr{\}}\]
6:endfor
7:for\(h=1,2,\ldots H\)do
8: Observe \(s^{k}_{h}\) and \(R^{k}_{h}(s^{k}_{h},a)\) for all \(a\in\mathcal{A}\)
9: Play an action \(a^{k}_{h}\in\arg\max_{a\in\mathcal{A}}\Bigl{\{}R^{k}_{h}(s^{k}_{h},a)+b^{p}_{ k,h}(s^{k}_{h},a)+\hat{P}^{k-1}_{h}\bar{V}^{k}_{h+1}(s^{k}_{h},a)\Bigr{\}}\)
10: Collect the reward \(R^{k}_{h}(s^{k}_{h},a^{k}_{h})\) and transition to the next state \(s^{k}_{h+1}\sim P_{h}(\cdot|s^{k}_{h},a^{k}_{h})\)
11:endfor
12:endfor ```

**Algorithm 1** Monotonic Value Propagation with Reward Lookahead (MVP-RL)

**Theorem 1**.: _When running MVP-RL, with probability at least \(1-\delta\) uniformly for all \(K\geq 1\), it holds that \(\operatorname{Reg}^{R}(K)\leq\mathcal{O}\Bigl{(}\sqrt{H^{3}SAK}\ln\frac{SAHK}{ \delta}+H^{3}S^{2}A\bigl{(}\ln\frac{SAHK}{\delta}\bigr{)}^{2}\Bigr{)}\)._

See proof in Appendix B.7. Remarkably, our upper bound matches the standard lower bound for episodic RL of \(\Omega\Bigl{(}\sqrt{H^{3}SAK}\Bigr{)}\)(Domingues et al., 2021) up to log-factors; this lower bound is proved for known deterministic rewards, so in particular, it also holds for problems with reward lookahead.

To our knowledge, the only comparable bounds in settings with reward lookahead were proven to prophet problems; as agents observe (up to) \(n\) distributions at a fixed order, it can be formulated as a deterministic chain-like MDP, with \(H=n\), \(S=n+1\) and \(A=2\). Agents start at the head of the chain and can either advance without collecting a reward or collect the observed reward and move to a terminal non-rewarding state (for more details, see Merlis et al. 2024). For this problem, (Gatmiry et al., 2024) proved a regret bound of \(\tilde{\mathcal{O}}(n^{3}\sqrt{K})\) (albeit requiring a weaker form of feedback), and (Agarwal et al., 2023) proved a bound of \(\tilde{\mathcal{O}}(n\sqrt{T})\) - slightly better than ours, but heavily relies on the ability to control which distributions to observe, which is a specific instance of deterministic transitions. We are unaware of any previous results that cover general Markovian dynamics.

### Proof Concepts

When analyzing the regret of RL algorithms, a key step usually involves bounding the difference between the value of a policy in two different environments ('value-difference lemma'). In particular, for a given policy \(\pi^{k}\), many algorithms maintain a confidence interval on the value \(V_{h}^{\pi^{k}}(s)\in\left[V_{h}^{k}(s),\bar{V}_{h}^{k}(s)\right]\), calculated based on optimistic and pessimistic MDPs that use the empirical model with bonuses/penalties (Dann et al., 2019; Zanette and Brunskill, 2019; Efroni et al., 2021). Then, the instantaneous regret (without lookahead) is bounded using the optimistic values by

\[\bar{V}_{h}^{k}(s_{h})-V_{h}^{\pi^{k}}(s_{h}) =\left(\hat{r}_{h}^{k-1}(s_{h},a_{h})-r_{h}(s_{h},a_{h})\right)+ \left(\hat{P}_{h}^{k-1}-P_{h}\right)\bar{V}_{h}^{k}(s_{h},a_{h})\] \[\quad+P_{h}\Big{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Big{)}(s_{h },a_{h})+\text{bonuses},\]

while the pessimistic values are used either as part of the bonuses or while bounding them. However, when trying to perform a similar decomposition with reward lookahead, we do not have the difference of expected rewards, but rather terms of the form

\[\mathbb{E}_{\bm{R}\sim\hat{\mathcal{R}}_{h}^{k-1}(s_{h})}\big{[}R(\pi^{k}_{h} (s_{h},\bm{R}))\big{]}-\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s_{h})}\big{[}R( \pi^{k}_{h}(s_{h},\bm{R}))\big{]}\]

(see, e.g., the last term of Lemma 4 in the appendix). As the action can be an arbitrary function of the reward realization, this term is extremely challenging to bound. For example, one could couple both distributions while trying to relate this error term to a Wasserstein distance between the empirical and real reward distribution; however, such distances exhibit much slower error rates than standard mean estimation (Fournier and Guillin, 2015). Instead, we follow a different approach and show that uniformly for all possible expected next-state values \(\hat{P}V\in[0,H]^{A}\) (as a function of the action at a given state), it holds w.h.p. that

\[\Big{|}\mathbb{E}_{\bm{R}\sim\hat{\mathcal{R}}_{h}^{k-1}(s)}\Big{[} \max_{a}\Big{\{}R(a)+\hat{P}V(s,a)\Big{\}}\Big{]} -\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}\max_{a}\Big{\{} R(a)+\hat{P}V(s,a)\Big{\}}\Big{]}\Big{|}\] \[\lesssim\sqrt{\frac{A\ln\frac{1}{\delta}}{n_{h}^{k-1}(s)\lor 1}}.\] (1)

Throughout the proof, whenever we face an expectation w.r.t. the empirical rewards, we reformulate the expression to fit the form of Equation (1) and use it as a 'change of measure' tool. We remark that while this confidence interval admits an extra \(A\)-factor compared to standard bounds, the counts only depend on the visits to the state (and not to the state-action), which compensates for this factor.

The choice of MVP for the bonus is similarly motivated - unlike some other bonuses (e.g., Zanette and Brunskill, 2019), MVP does not require pessimistic values - either in the bonus itself or in its analysis. In contrast to the optimistic ones, the pessimistic values are not calculated via value iteration, but rather by following the policy \(\pi^{k}\) in the pessimistic environment. As such, they cannot be easily manipulated to fit the form in Equation (1).

The analysis of the transitions adapts the techniques in (Efroni et al., 2021), while requiring extra care in handling the dependence of actions in the rewards.

## 5 Reinforcement Learning with One-Step Transition Lookahead

We now move to analyzing problems with one-step transition lookahead, where the resulting next state due to playing any of the actions is revealed before deciding which action to play. For example, consider the stochastic Canadian traveler problem with resampling (Nikolova and Karger, 2008; Boutilier et al., 2018). In this problem, an agent wants to navigate on a graph as fast as possible from a source to a target, but observes which edges at a node are available only upon reaching this node. When edge availability is stochastic and resampled every time a node is visited, this is a clear case of one-step transition lookahead, as the information on the availability of edges is given before trying to traverse them. The example in Section 3 and Appendix C.9 is one possible formulation of this problem on a chain - agents are awarded for arriving at the end of the chain as fast as possible, but trying to use a non-existing edge results with termination. We showed that in this particular instance, the lookahead value is exponentially larger than the standard value, and any lookahead agent with low regret would greatly surpass no-lookahead agents.

As with reward lookahead, the future states for all actions can be embedded into the state, but doing so increases the size of the state space by a factor of \(S^{A}\), again making this approach intractable (see Appendix C.2 for an example for such an extension). We once more show that this is not necessary; the transition-lookahead optimal values can be calculated using the following Bellman equations:

**Proposition 2**.: _The optimal value of one-step transition lookahead agents satisfies_

\[V_{H+1}^{T,*}(s)=0, \forall s\in\mathcal{S},\] \[V_{h}^{T,*}(s)=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\bigg{[} \max_{a\in\mathcal{A}}\Bigl{\{}r_{h}(s,a)+V_{h+1}^{T,*}(s^{\prime}(s,a)) \Bigr{\}}\bigg{]}, \forall s\in\mathcal{S},h\in[H].\]

_Also, given next-state observations \(\bm{s}^{\prime}=\{s^{\prime}(a)\}_{a\in\mathcal{A}}\) at state \(s\) and step \(h\), the optimal policy is_

\[\pi_{h}^{*}(s,\bm{s}^{\prime})\in\operatorname*{arg\,max}_{a\in\mathcal{A}} \Bigl{\{}r_{h}(s,a)+V_{h+1}^{T,*}(s^{\prime}(a))\Bigr{\}}.\]

The proof can be found at Appendix C.2 and again relies on augmenting the state space to incorporate the transitions; this time, we divide the episode into odd steps whose extended state is \(s_{h}\times\bm{s}^{\prime}_{0}\) (for an arbitrary fixed \(\bm{s}^{\prime}_{0}\in\mathcal{S}^{A}\)) and even steps with the state \(s_{h}\times\bm{s}^{\prime}_{h+1}\). Beyond planning, this again allows proving a variant of the LTV and of a value-difference lemma.

One important insight is that the policy \(\pi_{h}^{*}(s,\bm{s}^{\prime})\) admits the form of a _list_. Namely, consider the values \(V_{h}^{*}(s,s^{\prime},a)=r_{h}(s,a)+V_{h+1}^{T,*}(s^{\prime})\) and assume some ordering of next-state-action pairs \(\{(s^{\prime}_{a},a_{i})\}_{i=1}^{SA}\) such that \(V_{h}^{*}(s,s^{\prime}_{1},a_{1})\geq\cdots\geq V_{h}^{*}(s,s^{\prime}_{SA},a_ {SA})\). Then, an optimal policy would look at all realized pairs \((s^{\prime}(a),a)\) and play the action with the highest location in this list. We refer the readers to Appendix C.4 for an additional discussion on list representations in transition lookahead.

Similar results could be achieved through a reduction to RL problems with stochastic action sets (Boutilier et al., 2018). There, at every round, a subset of base actions is sampled, and only these actions are available to the agent. In particular, one could sample \(A\) actions of the form \((s^{\prime},a)\in\mathcal{S}\times\mathcal{A}\) and impose a deterministic transition to \(s^{\prime}\) given this extended action. However, since every original action must be sampled exactly once, this sampling procedure creates a dependence between pairs even when next-states at different actions are independent, adding unnecessary complications. We show that when transitions are independent between states, the expectation in Proposition 2 can be efficiently calculated (see Appendix C.4.1 for details), and otherwise, it can be approximated through sampling, as we do in learning settings.

### Regret-Minimization with Transition Lookahead

Relying on similar principals as with reward lookahead, we now present MVP-TL, an adaptation of MVP to settings with one-step transition lookahead (summarized in Algorithm 2; the full details can be found at Appendix C.3). This time, we estimate the empirical expected reward and add a standard Hoeffding-like reward bonus, while performing planning using samples from the _empirical joint distribution_ of the next-state for all the actions simultaneously. A variance-based transition bonus is added to the values; though this time, the variance also incorporates the rewards, namely

\[b_{k,h}^{p}(s)\approx\sqrt{\frac{\operatorname{Var}_{\bm{s}^{\prime}\sim\hat{ p}_{h}^{k-1}(s)}(\bar{V}_{h}^{k}(s,\bm{s}^{\prime}))}{n_{h}^{k-1}(s)\lor 1}}, \bar{V}_{h}^{k}(s,\bm{s}^{\prime})=\max_{a\in\mathcal{A}}\Bigl{\{} \hat{r}_{h}^{k-1}(s,a)+b_{k,h}^{r}(s,a)+\bar{V}_{h+1}^{k}(s^{\prime}(a)\Bigr{\}}.\]

The motivation for this modification is the technical challenges described in Section 4.2, in the context of reward lookahead. For reward lookahead, we analyzed a value term that included both the rewards and next-state values, and used concentration arguments to move from the empirical reward distribution to the real one. For transition lookahead, similar values are analyzed, but we require variance-based concentration to obtain tighter regret bounds (Azar et al., 2017), so this variance naturally arises. The bonus is again designed to be monotone, as in the original MVP algorithm, and does not affect the immediate action choice - only the optimistic lookahead value. As before, the planning relies on sampling the next-state observations at previous episodes, and so it is polynomial, even if the precise joint distribution is complex. The algorithm enjoys the following regret bounds:

**Theorem 2**.: _When running MVP-TL with probability at least \(1-\delta\) uniformly for all \(K\geq 1\), it holds that \(\operatorname{Reg}^{T}(K)\leq\mathcal{O}\Bigl{(}\sqrt{H^{2}SK}\Bigl{(}\sqrt{H}+ \sqrt{A}\Bigr{)}\ln\frac{SAHK}{\delta}+H^{3}S^{4}A^{3}\bigl{(}\ln\frac{SAHK}{ \delta}\bigr{)}^{2}\Bigr{)}\)._See proof in Appendix C.8. For transition lookahead, the regret bounds we provide exhibit two rates, both corresponding to a natural adaptation of known lower bounds to transition lookahead.

1. _'Bandit rate'_\(\mathcal{O}(\sqrt{H^{2}SAK})\): this is the rate due to reward stochasticity. Consider a problem where at odd timesteps \(2h-1\) and across all states, all actions have rewards of mean \(\nicefrac{{1}}{{2}}-\epsilon\), except for one action of mean \(\nicefrac{{1}}{{2}}\). Assuming that the state-distribution is uniform, each such timestep forms a hard instance of a contextual bandit problem with \(S\) contexts, exhibiting a regret of \(\Omega(\sqrt{SAK})\)(Auer et al., 2002; Bubeck et al., 2012). Since there are \(H/2\) odd steps and we can design each step independently, the total regret would be \(\Omega(H\sqrt{SAK})\). The even steps can be used to'remove' the lookahead and create a uniform state distribution. To do so, we set that when taking an action at odd steps, we always transition to a fixed state \(s_{d}\). From this state, one action \(a_{1}\) leads uniformly to all states, while the rest of the actions lead to an absorbing non-rewarding state - rendering them strictly suboptimal. Thus, no-regret agents will only play \(a_{1}\), regardless of the lookahead information, and the state distribution at odd timesteps will be uniform.
2. _'Transition learning rate'_\(\mathcal{O}(\sqrt{H^{3}SK})\): recall that the vanilla RL lower bound designs a tree with \(\Omega(S)\) leaves, to which agents need to navigate at the right timing (with \(\Omega(H)\) options) and take the right action (out of \(A\)). While all leaves might transition agents to a rewarding state, one combination of state-action-timing has a slightly higher probability of doing so (Domingues et al., 2021). This roughly creates a bandit problem with \(SAH\) arms, constructed such that the maximal reward is \(\Omega(H)\), yielding a total regret of \(H\sqrt{HSAK}\). Now consider the following simple modification where in each leaf, only one action can lead to a reward (and the rest of the actions are 'useless' - never lead to rewards). Thus, the agent still needs to test all leaves at all timings, and so there are still \(SH\) 'arms' with a corresponding regret of \(\sqrt{H^{3}SK}\). Moreover, to test a leaf at a certain timing, we must navigate to it, and since the agent is going to play the single useful action at the leaf, transition lookahead does not provide any additional information.

As discussed before, transition lookahead can be formulated as an RL instance with stochastic action sets. While Boutilier et al. (2018) prove that with stochastic action sets, Q-learning asymptotically converges, they provide no learning algorithm nor regret bounds. Therefore, to our knowledge, our result is the first to achieve sublinear regret with transition lookahead.

### Proof Concepts

Transition lookahead causes similar issues as reward lookahead. Hence, it is natural to apply a similar analysis approach - first, formulate the value as the expectation w.r.t. the next-state observations of the maximum of action-observation dependent values; then use uniform concentration as a 'change of measure' tool between the empirical and real next-state distribution. In particular, if \(V(s,s^{\prime},a)\) represents the value starting from state \(s\), performing \(a\) and transitioning to \(s^{\prime}\), one can show that for all \(V(s,\cdot,\cdot)\in[0,H]^{SA}\) (see Lemma 19),

\[\Big{|}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{k-1}(s)}\Big{[}\max_{ a}V(s,s^{\prime}(a),a)\Big{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\max_{ a}V(s,s^{\prime}(a),a)\Big{]}\Big{|}\\ \lesssim\sqrt{\frac{SA\ln\frac{1}{3}\text{Var}_{\bm{s}^{\prime} \sim P_{h}^{k-1}(s)}\max_{a}V(s,s^{\prime}(a),a)}{n_{h}^{k-1}(s)\lor 1}},\] (2)

where the variance term stems from using a Bernstein-like concentration bound. However, in contrast to the reward lookahead, the \(\sqrt{SA}\)-factor propagates to the dominant term of the regret, so pursuing this approach would lead to a worse regret bound of \(\tilde{\mathcal{O}}\Big{(}\sqrt{H^{3}S^{2}AK}\Big{)}\).

To avoid this, we pinpoint the two locations where this change of measure is needed - the proof that \(\bar{V}_{h}^{k}\) is optimistic and the regret decomposition - and make sure to perform this change of measure only on a single value \(V_{h}^{*}(s,s^{\prime},a)=r_{h}(s,a)+V_{h+1}^{*}(s^{\prime})\), mitigating the need to cover all possible values and removing the additional \(\sqrt{SA}\)-factor. However, doing so leaves us with a residual term. Defining \(V_{h}^{*}(s,\bm{s}^{\prime})=\max_{a\in\mathcal{A}}\{V_{h}^{*}(s,s^{\prime}(a ),a)\}\) and assuming a similar optimistic value \(\bar{V}_{h}^{k}(s,\bm{s}^{\prime})\), this term is of the form

\[\mathbb{E}_{\bm{s}^{\prime}\sim\bar{P}_{h}^{k-1}(s)}\big{[}\bar{V}_{h}^{k}(s, \bm{s}^{\prime})-V_{h}^{*}(s,\bm{s}^{\prime})\big{]}-\mathbb{E}_{\bm{s}^{ \prime}\sim P_{h}(s)}\big{[}\bar{V}_{h}^{k}(s,\bm{s}^{\prime})-V_{h}^{*}(s,\bm {s}^{\prime})\big{]}.\]

While similar terms have been analyzed before (e.g., Zanette and Brunskill, 2019; Efroni et al., 2021), the analysis leads to a constant regret term that depends on the support of the distribution in question; in our case, it is the distribution over all possible next-states - of cardinality \(S^{A}\). Therefore, following the same derivation would lead to an exponential additive regret term.

We overcome it by utilizing the fact that both the optimistic policy and the optimal one decide which action to take according to a list of next-state-actions \((s^{\prime},a)\). In other words, instead of looking at the next-state \(\bm{s}^{\prime}\) (with \(S^{A}\) possible values) to determine a value, we look at the highest-ranked realized pair \((s^{\prime},a)\) in the list that corresponds to the policy that induces the value (with \(SA\) possible rankings). Since we have two values, we need to calculate the probability of being at a certain list location for both \(\pi^{k}\) and \(\pi^{*}\), but the cardinality of this space is \((SA)^{2}\): polynomial and not exponential.

## 6 Conclusions and Future Work

In this work, we presented an RL setting in which immediate rewards or transitions are observed before actions are chosen. We showed how to design provably and computationally efficient algorithms for this setting that achieve tight regret bounds versus a strong baseline that also uses lookahead information. Our algorithms rely on estimating the distribution of the reward or transition observations, a concept that might be utilized in other settings. In particular, we believe that our techniques for transition lookahead could be extended to RL problems with stochastic action sets (Boutilier et al., 2018), but leave this for future work.

One natural extension to our work would be to consider multi-step lookahead information - observing the transition/rewards \(L\) steps in advance. We conjecture that from a statistical point of view, a similar algorithmic approach that samples from the empirical observation distribution would be efficient. However, it is not clear how to perform efficient planning with such feedback.

Another possible direction would be to derive model-free algorithms (Jin et al., 2018), with the aim to improve the computation efficiency of the solutions; our model-based algorithms require at most \(\mathcal{O}(KS^{2}AH)\) computations per episode due to the planning stage, while model-free algorithms might potentially allow just \(\mathcal{O}(AH)\) computations per episode.

On the practical side, previous works presented RL algorithms that utilize/estimate a world model with multi-step lookahead to perform planning and learning (Schrittwieser et al., 2020; Chung et al., 2024), aiming to achieve the optimal no-lookahead value. For some of these approaches, it is quite natural to replace the simulated world behavior with lookahead information on the real future realization. We leave this adaptation and evaluation to future studies.

Finally, the notion of lookahead could be studied in various other decision-making settings (e.g., linear MDPs Jin et al. 2020) and can also be generalized to situations where lookahead information can be queried under some budget constraints (Efroni et al., 2021) or when agents only observe noisy lookahead predictions; we leave these problems for future research.

## Acknowledgements

We thank Alon Cohen and Austin Stromme for the helpful discussions. This project has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 101034255.

## References

* Agarwal et al. (2023) Arpit Agarwal, Rohan Ghuge, and Viswanath Nagarajan. Semi-bandit learning for monotone stochastic optimization. _arXiv preprint arXiv:2312.15427_, 2023.
* Auer et al. (2002) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 32(1):48-77, 2002.
* Azar et al. (2017) Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* Bertsekas (2023) Dimitri Bertsekas. _A course in reinforcement learning_. Athena Scientific, 2023.
* Biedenkapp et al. (2021) Andre Biedenkapp, Raghu Rajan, Frank Hutter, and Marius Lindauer. Temporl: Learning when to act. In _International Conference on Machine Learning_, pages 914-924. PMLR, 2021.
* Boutilier et al. (2018) Craig Boutilier, Alon Cohen, Avinatan Hassidim, Yishay Mansour, Ofer Meshi, Martin Mladenov, and Dale Schuurmans. Planning and learning with stochastic action sets. In _Proceedings of the 27th International Joint Conference on Artificial Intelligence_, pages 4674-4682, 2018.
* Bubeck et al. (2012) Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* Camacho et al. (2007) Eduardo F Camacho, Carlos Bordons, Eduardo F Camacho, and Carlos Bordons. _Model predictive control_. Springer, 2007.
* Chung et al. (2024) Stephen Chung, Ivan Anokhin, and David Krueger. Thinker: learning to plan and act. _Advances in Neural Information Processing Systems_, 36, 2024.
* Correa et al. (2017) Jose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Posted price mechanisms for a random stream of customers. In _Proceedings of the 2017 ACM Conference on Economics and Computation_, pages 169-186, 2017.
* Correa et al. (2019) Jose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld. Recent developments in prophet inequalities. _ACM SIGecom Exchanges_, 17(1):61-70, 2019.
* Dann et al. (2019) Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable reinforcement learning. In _International Conference on Machine Learning_, pages 1507-1516, 2019.
* Domingues et al. (2021) Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In _Algorithmic Learning Theory_, pages 578-598. PMLR, 2021.
* Efroni et al. (2019) Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor. How to combine tree-search methods in reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 3494-3501, 2019a.
* Efroni et al. (2019) Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor. Tight regret bounds for model-based reinforcement learning with greedy policies. In _Advances in Neural Information Processing Systems_, pages 12224-12234, 2019b.
* Efroni et al. (2020) Yonathan Efroni, Mohammad Ghavamzadeh, and Shie Mannor. Online planning with lookahead policies. _Advances in Neural Information Processing Systems_, 33:14024-14033, 2020.
* Efroni et al. (2019)Yonathan Efroni, Nadav Merlis, Aadirupa Saha, and Shie Mannor. Confidence-budget matching for sequential budgeted learning. In _International Conference on Machine Learning_, pages 2937-2947. PMLR, 2021.
* El Shar and Jiang (2020) Ibrahim El Shar and Daniel Jiang. Lookahead-bounded q-learning. In _International Conference on Machine Learning_, pages 8665-8675. PMLR, 2020.
* Fournier and Guillin (2015) Nicolas Fournier and Arnaud Guillin. On the rate of convergence in wasserstein distance of the empirical measure. _Probability theory and related fields_, 162(3):707-738, 2015.
* Gatmiry et al. (2024) Khashayar Gatmiry, Thomas Kesselheim, Sahil Singla, and Yifan Wang. Bandit algorithms for prophet inequality and pandora's box. In _Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 462-500. SIAM, 2024.
* Huang et al. (2019) Yunhan Huang, Veeraruna Kavitha, and Quanyan Zhu. Continuous-time markov decision processes with controlled observations. In _2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 32-39. IEEE, 2019.
* Jaksch et al. (2010) Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 11(Apr):1563-1600, 2010.
* Jin et al. (2018) Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? _Advances in neural information processing systems_, 31, 2018.
* Jin et al. (2020) Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on learning theory_, pages 2137-2143. PMLR, 2020.
* Li et al. (2019) Yingying Li, Xin Chen, and Na Li. Online optimal control with linear dynamics and predictions: Algorithms and regret analysis. _Advances in Neural Information Processing Systems_, 32, 2019.
* Lin et al. (2021) Yiheng Lin, Yang Hu, Guanya Shi, Haoyuan Sun, Guannan Qu, and Adam Wierman. Perturbation-based regret analysis of predictive control in linear time varying systems. _Advances in Neural Information Processing Systems_, 34:5174-5185, 2021.
* Lin et al. (2022) Yiheng Lin, Yang Hu, Guannan Qu, Tongxin Li, and Adam Wierman. Bounded-regret mpc via perturbation analysis: Prediction error, constraints, and nonlinearity. _Advances in Neural Information Processing Systems_, 35:36174-36187, 2022.
* Maurer and Pontil (2009) Andreas Maurer and Massimiliano Pontil. Empirical bernstein bounds and sample variance penalization. In _Conference on learning theory_, 2009.
* Merlis et al. (2024) Nadav Merlis, Dorian Baudry, and Vianney Perchet. The value of reward lookahead in reinforcement learning. _arXiv preprint arXiv:2403.11637_, 2024.
* Moerland et al. (2020) Thomas M Moerland, Anna Deichler, Simone Baldi, Joost Broekens, and Catholijn M Jonker. Think neither too fast nor too slow: The computational trade-off between planning and reinforcement learning. In _Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS), Nancy, France_, pages 16-20, 2020.
* Nikolova and Karger (2008) Evdokia Nikolova and David R Karger. Route planning under uncertainty: The canadian traveller problem. In _AAAI_, pages 969-974, 2008.
* Puterman (2014) Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Rosenberg et al. (2023) Aviv Rosenberg, Assaf Hallak, Shie Mannor, Gal Chechik, and Gal Dalal. Planning and learning with adaptive lookahead. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 9606-9613, 2023.
* Schrittwieser et al. (2020) Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. _Nature_, 588(7839):604-609, 2020.
* Schrittwieser et al. (2020)Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. In _Advances in Neural Information Processing Systems_, pages 1153-1162, 2019.
* Tamar et al. (2017) Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, and Pieter Abbeel. Learning from the hindsight plan--episodic mpc improvement. In _2017 IEEE International Conference on Robotics and Automation (ICRA)_, pages 336-343. IEEE, 2017.
* Yu et al. (2020) Chenkai Yu, Guanya Shi, Soon-Jo Chung, Yisong Yue, and Adam Wierman. The power of predictions in online control. _Advances in Neural Information Processing Systems_, 33:1994-2004, 2020.
* Zanette and Brunskill (2019) Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* Zhang et al. (2021a) Runyu Zhang, Yingying Li, and Na Li. On the regret analysis of online lqr control with predictions. In _2021 American Control Conference (ACC)_, pages 697-703. IEEE, 2021a.
* Zhang et al. (2021b) Zihan Zhang, Xiangyang Ji, and Simon Du. Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon. In _Conference on Learning Theory_, pages 4528-4531. PMLR, 2021b.
* Zhang et al. (2023) Zihan Zhang, Yuxin Chen, Jason D Lee, and Simon S Du. Settling the sample complexity of online reinforcement learning. _arXiv preprint arXiv:2307.13586_, 2023.

## Appendix A Structure of the Appendix

Both reward and transition lookahead appendices share the following structure. First, we describe our assumption on the data generation process and analyze general properties of reward and transition lookahead. This is done by looking at an extended MDP that incorporates the lookahead information into the state. Then, we present the full algorithm and describe the relevant probabilistic events that ensure the concentration of all the empirical quantities. For transition lookahead, we require some additional notions for the event definitions (including the list representation of values and policies), which are explained in a separate subsection.

Given the concentration-related good event, we can prove that the planning procedure in the algorithm is optimistic, which we do in the subsequent subsection. Then, we define an additional good event that allows adding and removing conditional expectations in a way that will be needed for the proof.

At this point, we provided all (almost all) the results required for the regret analysis, and the proof of the main theorems is stated. The proofs also require some additional analysis for the bonuses (and especially variance terms), which is located at the end of the regret analysis.

For transition lookahead, the appendix includes one more part that further analyzes the example presented in Section 3.

At the end of the appendix, we state and prove several lemmas that will be used throughout our analysis, while also stating several existing results that will be of use.

Proofs for Reward Lookahead

### Data Generation Process

To simplify the proofs, we assume the following 'tabular' data-generation process: Before the game starts, a set of \(K\) samples from the transition probabilities and rewards is generated for all \((s,a,h)\). Once a state \(s\) at step \(h\) is visited for the \(i^{th}\) time, the \(i^{th}\) sample from the reward distribution \(\mathcal{R}_{h}(s)\) is the reward realization for all action \(a\in\mathcal{A}\). When a state-action pair is visited for the \(i^{th}\) time, the \(i^{th}\) sample from the transition kernel \(P_{h}(\cdot|s,a)\) determines the next-state realization. In particular, it implies that the reward samples from the first \(i\) visits to a state are i.i.d., and the same for the next-states samples and state-action visitations. Throughout this appendix, we use the notation \(\bm{R}_{h}^{k}=\left\{R_{h}^{k}(s_{h}^{k},a)\right._{a\in\mathcal{A}}\) to denote the reward observation at episode \(k\) and timestep \(h\) for all the actions.

For the proof, we define the following three filtrations. Let

\[F_{k,h} =\sigma\bigg{(}\left\{s_{t}^{1},a_{t}^{1},\bm{R}_{t}^{1}\right\} _{t\in[H]},\ldots,\left\{s_{t}^{k-1},a_{t}^{k-1},\bm{R}_{t}^{k-1}\right\}_{t \in[H]},\left\{s_{t}^{k},a_{t}^{k},\bm{R}_{t}^{k}\right\}_{t\in[h]},s_{h+1}^{k }\bigg{)},\] \[F_{k,h}^{R} =\sigma\bigg{(}\left\{s_{t}^{1},a_{t}^{1},\bm{R}_{t}^{1}\right\} _{t\in[H]},\ldots,\left\{s_{t}^{k-1},a_{t}^{k-1},\bm{R}_{t}^{k-1}\right\}_{t \in[H]},\left\{s_{t}^{k},a_{t}^{k},\bm{R}_{t}^{k}\right\}_{t\in[h+1]}\bigg{)},\]

the filtrations that contains all information until episode \(k\) and step \(h\), as well as the state at timestep \(h+1\), or all information of time \(h+1\), respectively. We make this distinction so that \(F_{k,h-1}\) contains only \(s_{h}^{k}\), while \(F_{k,h-1}^{R}\) also contains \(a_{h}^{k}\). We also define

\[F_{k}=\sigma\bigg{(}\left\{s_{t}^{1},a_{t}^{1},\bm{R}_{t}^{1}\right\}_{t\in[H] },\ldots,\left\{s_{t}^{k},a_{t}^{k},\bm{R}_{t}^{k}\right\}_{t\in[H]},s_{1}^{k +1}\bigg{)},\]

which contains all information up to the end of the \(k^{th}\) episode, as well as the initial state at episode \(k+1\).

### Extended MDP for Reward Lookahead

In this appendix, we present an alternative formulation of the one-step reward lookahead that falls under the vanilla (no-lookahead) model and would be helpful for the analysis.

Throughout the section, we study the relations between MDPs with and without reward lookahead, and between different MDPs with lookahead. Therefore, for clarity, we state the concerning MDP in the value, e.g. \(V^{R,\pi}(s|\mathcal{M})\). Specifically in this subsection, we distinguish between values without lookahead (denoted \(V^{\pi}\)) and values with lookahead (denoted \(V^{R,\pi}\)). In the following subsections, unless stated otherwise, we will only consider lookahead values; for brevity, and with some abuse of notations, we will then omit the \(R\) in the value notation.

For any MDP \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,P,\mathcal{R})\), define an equivalent extended MDP \(\mathcal{M}^{R}\) of horizon \(2H\) that separates the state transition and reward generation as follows:

1. Assume w.l.o.g. that \(\mathcal{M}\) starts at some initial state \(s_{1}\). The extended environment starts at a state \(s_{1}\times\bm{0}\), where \(\bm{0}\in\mathbb{R}^{A}\) is the zeros vector.
2. For any \(h\in[H]\), at timestep \(2h-1\), the environment \(\mathcal{M}^{R}\) transitions from state \(s_{h}\times\bm{0}\) to \(s_{h}\times\bm{R}\), where \(\bm{R}\sim\mathcal{R}_{h}(s)\) is a vector containing the rewards for all actions \(a\in\mathcal{A}\). This transition occurs regardless of the action that was played. At timestep \(2h\), given an action \(a_{h}\) the environment transitions from \(s_{h}\times\bm{R}\) to \(s_{h+1}\times\bm{0}\), where \(s_{h+1}\sim P_{h}(\cdot|s_{h},a_{h})\).
3. The reward at a state \(s\times\bm{R}\) when playing an action \(a\) is \(R(a)\), namely, the reward is deterministic and only obtained on even timesteps.

We emphasize that throughout the section, we assume that \(\mathcal{M}\) and \(\mathcal{M}^{R}\) are coupled; that is, assume that under a policy \(\pi\) in \(\mathcal{M}\), the agent visits a state \(s_{h}\), observes \(\bm{R}_{h}\), plays an action \(a_{h}\) and transitions to \(s_{h+1}\). Then, in \(\mathcal{M}^{R}\), the agent starts from \(s_{h}\times\bm{0}\), transitions to \(s_{h}\times\bm{R}\) (regardless of the action it played), takes the action \(a_{h}\) and finally transitions to \(s_{h+1}\times\bm{0}\).

Since the reward is embedded into the state, any state-dependent policy in \(\mathcal{M}^{R}\) is a one-step reward lookahead policy in the original MDP. Moreover, the policy at the odd steps of \(\mathcal{M}\) does not affect the value, and assuming that the policy at the even steps in \(\mathcal{M}^{R}\) is the same as the policy in \(\mathcal{M}\), we trivially get the following relation between the values

\[V_{2h}^{\pi}(s,\bm{R}|\mathcal{M}^{R})=\mathbb{E}\Bigg{[}\sum_{t=h }^{H}R_{t}(s_{t},a_{t})|s_{h}=s,R_{h}(s,\cdot)=\bm{R},\pi\Bigg{]}\triangleq V_{h }^{R,\pi}(s,\bm{R}|\mathcal{M}),\] \[V_{2h-1}^{\pi}(s,\bm{0}|\mathcal{M}^{R})=\mathbb{E}\Bigg{[}\sum_{ t=h}^{H}R_{t}(s_{t},a_{t})|s_{h}=s,\pi\Bigg{]}=V_{h}^{R,\pi}(s|\mathcal{M}).\] (3)

While \(\mathcal{M}^{R}\) has a continuous state space, which generally makes algorithm design impractical, this representation permits applying classic results on MDPs to environments with one-step lookahead.

As a remark, rewards could be directly embedded into the state without separating the state and reward updates. However, this creates unnecessary complications when analyzing the relations between similar environments. This is because we are mainly interested in the value given the state - in expectation over the realized rewards. In particular, value-difference are analyzed assuming a shared initial state, but in our case, we do not want to assume the same reward realization, but rather also account for the distance between reward distributions, which the step separation enables. For similar reasons, this representation also simplifies the proof of the law of total variance [Azar et al., 2017].

**Proposition 1**.: _The optimal value of one-step reward lookahead agents satisfies_

\[V_{H+1}^{R,*}(s)=0, \forall s\in\mathcal{S},\] \[V_{h}^{R,*}(s)=\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Bigg{[} \max_{a\in\mathcal{A}}\Bigg{\{}R_{h}(s,a)+\sum_{s^{\prime}\in\mathcal{S}}P_{h} (s^{\prime}|s,a)V_{h+1}^{R,*}(s^{\prime})\Bigg{\}}\Bigg{]},\quad\forall s\in \mathcal{S},h\in[H].\]

_Also, given reward observations \(\bm{R}=\left\{R(a)\right\}_{a\in\mathcal{A}}\) at state \(s\) and step \(h\), the optimal policy is_

\[\pi_{h}^{*}(s,\bm{R})\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\Bigg{\{}R(a )+\sum_{s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)V_{h+1}^{R,*}(s^{\prime })\Bigg{\}}.\]

Proof.: We prove the result in the extended MDP \(\mathcal{M}^{R}\) and remind the reader that in this formulation, the policy only uses state information, as in the standard RL formulation. In particular, it implies that there exists a Markovian optimal policy that uniformly maximizes the value (in the extended state space), and the optimal value is given through the dynamic-programming equations [Puterman, 2014]

\[V_{2H+1}^{*}(s,\bm{R}|\mathcal{M}^{R})=0, \forall s\in\mathcal{S},\bm{R}\in\mathbb{R}^{A},\] \[V_{2h}^{*}(s,\bm{R}|\mathcal{M}^{R})=\max_{a}\Bigg{\{}R(a)+\sum_ {s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)V_{2h+1}^{*}(s^{\prime},\bm{0}| \mathcal{M}^{R})\Bigg{\}}, \forall h\in[H],s\in\mathcal{S},\bm{R}\in\mathbb{R}^{A},\] \[V_{2h-1}^{*}(s,\bm{0}|\mathcal{M}^{R})=\mathbb{E}_{\mathcal{R}_{ h}(s)}\big{[}V_{2h}^{*}(s,\bm{R}|\mathcal{M}^{R})\big{]}, \forall h\in[H],s\in\mathcal{S}.\] (4)

By the equivalence between \(\mathcal{M}\) and \(\mathcal{M}^{R}\) for all policies, this is also the optimal value in \(\mathcal{M}\). Specifically, combining both recursion equations and substituting the relation between the original and extended values of Equation (3), we get the desired value recursion for any \(h\in[H]\) and \(s\in\mathcal{S}\):

\[V_{h}^{R,*}(s|\mathcal{M}) =V_{2h-1}^{*}(s,\bm{0}|\mathcal{M}^{R})\] \[=\mathbb{E}_{\mathcal{R}_{h}(s)}\Big{[}V_{2h}^{*}(s,\bm{R}| \mathcal{M}^{R})\big{]}\] \[=\mathbb{E}_{\mathcal{R}_{h}(s)}\Bigg{[}\max_{a}\Bigg{\{}R(a)+ \sum_{s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)V_{2h+1}^{*}(s^{\prime}, \bm{0}|\mathcal{M}^{R})\Bigg{\}}\Bigg{]}\] \[=\mathbb{E}_{\mathcal{R}_{h}(s)}\Bigg{[}\max_{a}\Bigg{\{}R(a)+ \sum_{s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)V_{h+1}^{R,*}(s|\mathcal{M })\Bigg{\}}\Bigg{]}.\]

Similarly, for any \(h\in[H]\), \(s\in\mathcal{S}\) and \(\bm{R}\in\mathbb{R}^{A}\), the optimal policy at the even stages of the extended MDP is

\[\pi_{2h}^{*}(s,\bm{R})\in\operatorname*{arg\,max}_{a\in\mathcal{A}}\Bigg{\{}R(a )+\sum_{s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)V_{2h+1}^{*}(s^{\prime}, \bm{0}|\mathcal{M}^{R})\Bigg{\}},\]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_FAIL:18]

Substituting this back into Equation (5), we have

\[V_{h}^{\pi}(s|\mathcal{M}_{1})-V_{h}^{\pi}(s|\mathcal{M}_{2})\] \[\quad=\mathbb{E}_{\mathcal{M}_{1}}\Big{[}V_{h+1}^{R,\pi}(s_{h+1}| \mathcal{M}_{1})-V_{h+1}^{R,\pi}(s_{h+1}|\mathcal{M}_{2})|s_{h},a_{h}\Big{]}|s_ {h}=s\Big{]}\] \[\quad\quad+\mathbb{E}_{\mathcal{M}_{1}}\Bigg{[}\sum_{s^{\prime} \in\mathcal{S}}\big{(}P_{h}^{1}(s^{\prime}|s_{h},a_{h})-P_{h}^{2}(s^{\prime}|s _{h},a_{h})\big{)}V_{h+1}^{R,\pi}(s^{\prime}|\mathcal{M}_{2})|s_{h}=s\Bigg{]}\] \[\quad\quad+\mathbb{E}_{\mathcal{R}_{h}^{1}(s)}\Big{[}V_{h}^{R, \pi}(s_{h},\bm{R}|\mathcal{M}_{2})\Big{]}-\mathbb{E}_{\mathcal{R}_{h}^{2}(s)} \Big{[}V_{h}^{R,\pi}(s_{h},\bm{R}|\mathcal{M}_{2})\Big{]}|s_{h}=s\Big{]}.\]

### Full Algorithm Description for Reward Lookahead

```
1:Require:\(\delta\in(0,1)\), bonuses \(b_{k,h}^{r}(s),b_{k,h}^{p}(s,a)\)
2:for\(k=1,2,...\)do
3: Initialize \(\bar{V}_{h+1}^{k}(s)=0\)
4:for\(h=H,H-1,..,1\)do
5:for\(s\in\mathcal{S}\)do
6:if\(n_{h}^{k-1}(s)=0\)then
7:\(\bar{V}_{h}^{k}(s)=H\)
8:else
9: Calculate the truncated values \[\bar{V}_{h}^{k}(s)=\min\Biggl{\{}\frac{1}{n_{h}^{k-1}(s)}\sum_{t=1}^{n_{h}^{k- 1}(s)}\max_{a\in\mathcal{A}}\Bigl{\{}R_{h}^{k_{h}^{t}(s)}(s,a)+b_{k,h}^{p}(s,a )+\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,a)\Bigr{\}}+b_{k,h}^{r}(s),H\Biggr{\}}\]
10:endif
11: For any vector \(\boldsymbol{R}\in\mathbb{R}^{A}\), define the policy \(\pi^{k}\) \[\pi_{h}^{k}(s,\boldsymbol{R})\in\operatorname*{arg\,max}_{a\in\mathcal{A}} \Bigl{\{}R(a)+b_{k,h}^{p}(s,a)+\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,a)\Bigr{\}}\]
12:endfor
13:endfor
14:for\(h=1,2,\ldots H\)do
15: Observe \(s_{h}^{k}\) and \(\boldsymbol{R}_{h}^{k}=\bigl{\{}R_{h}^{k}(s_{h}^{k},a)\bigr{\}}_{a\in\mathcal{A}}\)
16: Play an action \(a_{h}^{k}=\pi_{h}^{k}(s_{h}^{k},\boldsymbol{R}_{h}^{k})\)
17: Collect the reward \(R_{h}^{k}(s_{h}^{k},a_{h}^{k})\) and transition to the next state \(s_{h+1}^{k}\sim P_{h}(\cdot|s_{h}^{k},a_{h}^{k})\)
18:endfor
19: Update the empirical estimators and counts for all visited state-actions
20:endfor ```

**Algorithm 3** Monotonic Value Propagation with Reward Lookahead (MVP-RL)

We use a variant of the MVP algorithm (Zhang et al., 2021) while adapting their proof and the one from (Efroni et al., 2021). The algorithm is described in Algorithm 3 and uses the following bonuses:

\[b_{k,h}^{r}(s)=3\sqrt{\frac{AL_{\delta}^{k}}{2(n_{h}^{k-1}(s)\lor 1)}},\]

\[b_{k,h}^{p}(s,a)=\min\Biggl{\{}\frac{20}{3}\sqrt{\frac{\operatorname{Var}_{ \hat{P}_{h}^{k-1}(\cdot|s,a)}(\bar{V}_{h+1}^{k})L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{400}{9}\frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1},H \Biggr{\}}\]

where \(L_{\delta}^{k}=\ln\frac{1448^{2}AH^{2}k^{3}(k+1)}{\delta}\), and for brevity, we shorten \(\operatorname{Var}_{\hat{P}_{h}^{k-1}(\cdot|s,a)}(\bar{V}_{h+1}^{k}(s^{\prime}))\) to \(\operatorname{Var}_{\hat{P}_{h}^{k-1}(\cdot|s,a)}(\bar{V}_{h+1}^{k})\) (omitting the state from the value).

For the optimistic value iteration, we use the notation \(k_{h}^{t}(s)\) to represent the \(t^{th}\) episode where the state \(s\) was visited at the \(h^{th}\) timestep. Thus, line 9 of Algorithm 3 is the expectation w.r.t. the empirical reward distribution \(\bar{\mathcal{R}}_{h}^{k-1}(s)\) (when defining its realization to be zero when \(n_{h}^{k-1}(s)=0\)). Since the bonuses are larger than \(H\) when \(n_{h}^{k-1}(s)=0\), one could write the update in more concisely as

\[\bar{V}_{h}^{k}(s)=\min\biggl{\{}\mathbb{E}_{\boldsymbol{R}\sim\bar{\mathcal{R }}_{h}^{k-1}(s)}\biggl{[}\max_{a\in\mathcal{A}}\Bigl{\{}R(a)+b_{k,h}^{p}(s,a)+ \hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,a)\Bigr{\}}\biggr{]}+b_{k,h}^{r}(s),H \Biggr{\}}.\]

We will often use this representation in our analysis.

### The First Good Event - Concentration

We now define the first good event, which ensures that all empirical quantities are well-concentrated. For the transitions, we require each element to concentrate well, as well as both the inner product and the variance w.r.t. the optimal value function. For the reward, we make sure that the maximum of the rewards to concentrate well (with any possible bias, that will later correspond with the next-state values). Formally, for any fixed vector \(u\in\mathbb{R}^{A}\), denote

\[m_{h}(s,u) =\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}\max_{a}\{R_{h}( a)+u(a)\}\Big{]},\] \[\hat{m}_{h}^{k}(s,u) =\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}^{k}(s)}\Big{[}\max_{a}\{R_{ h}(a)+u(a)\}\Big{]}\]

with the convention that \(\hat{m}_{h}^{k}(s,u)=\max_{a}u(a)\) if \(n_{h}^{k}(s)=0\). We define the following good events:

\[E^{p}(k) =\Bigg{\{}\forall s,s^{\prime},a,h:\ |P_{h}(s^{\prime}|s,a)-\hat{P}_{ h}^{k-1}(s^{\prime}|s,a)|\leq\sqrt{\frac{2P(s^{\prime}|s,a)L_{\delta}^{k}}{n_ {h}^{k-1}(s,a)\lor 1}+\frac{L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}\Bigg{\}}\] \[E^{pv1}(k) =\Bigg{\{}\forall s,a,h:\ \Big{|}\Big{(}\hat{P}_{h}^{k-1}-P_{h} \Big{)}V_{h+1}^{*}(s,a)\Big{|}\leq\sqrt{\frac{2\mathrm{Var}_{P_{h}(\cdot|s,a)} (V_{h+1}^{*})L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{HL_{\delta}^{k}}{n_{h}^{k -1}(s,a)\lor 1}\Bigg{\}}\] \[E^{pv2}(k) =\Bigg{\{}\forall s,a,h:\ \Big{|}\sqrt{\mathrm{Var}_{P_{h}(\cdot|s,a)} (V_{h+1}^{*})}-\sqrt{\mathrm{Var}_{\hat{P}_{h}^{k-1}(\cdot|s,a)}(V_{h+1}^{*})} \Big{|}\leq 4H\sqrt{\frac{L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}\Bigg{\}}\] \[E^{r}(k) =\Bigg{\{}\forall s,h,\forall u\in[0,2H]^{A}:\ \big{|}m_{h}(s,u)-\hat{m}_{h}^{k-1}(s,u)\big{|} \leq 3\sqrt{\frac{AL_{\delta}^{k}}{2(n_{h}^{k-1}(s)\lor 1)}}\Bigg{\}}\]

where we again use \(L_{\delta}^{k}=\ln\frac{144S^{2}AH^{2}k^{3}(k+1)}{\delta}\). Then, we define the first good event as

\[\mathbb{G}_{1}=\bigcap_{k\geq 1}E^{r}(k)\bigcap_{k\geq 1}E^{p}(k)\bigcap_{k\geq 1 }E^{pv1}(k)\bigcap_{k\geq 1}E^{pv2}(k),\]

for which, the following holds:

**Lemma 5** (The First Good Event).: _The good event \(\mathbb{G}_{1}\) holds w.p. \(\Pr(\mathbb{G}_{1})\geq 1-\delta/2\)._

Proof.: The proof of the first three events uses standard concentration arguments (see, e.g., Efroni et al. 2021) and is stated for completeness. For any fixed \(k\geq 1,s,a,h\) and number of visits \(n\in[k]\), we utilize Lemma 16 w.r.t. the transition kernel \(P_{h}(\cdot|s,a)\), the value \(V_{h+1}^{*}\in[0,H]\) and probability \(\delta^{\prime}=\frac{\delta}{8SAHk^{2}(k+1)}\); notice that by the assumption that samples are generated i.i.d. before the game starts, given the number of visits, all samples are i.i.d., so standard concentration could be applied. By taking the union bound over all \(n\in[k]\) and slightly increasing the constants to ensure that \(n=0\) trivially holds, we get that the events also hold for any number of visit \(n_{h}^{k-1}(s,a)\in\{0\ldots,k\}\), and taking another union bound over all \(k\geq 1,s,a,h\) ensures that each of the events \(\cap_{k\geq 1}E^{p}(k),\cap_{k\geq 1}E^{pv1}(k)\) and \(\cap_{k\geq 1}E^{pv2}(k)\) holds w.p. at least \(1-\frac{\delta}{8}\)

We now focus on bounding the probability of the event \(\cap_{k}E^{r}(k)\). For any fixed \(k\), \(h\) and \(s\), observe that the event trivially holds if \(n_{h}^{k}=0\), then the event trivially holds, since for all \(u\in[0,2H]^{A}\),

\[\big{|}m_{h}(s,u)-\hat{m}_{h}^{k-1}(s,u)\big{|}=\Big{|}\mathbb{E}_{\bm{R}\sim \mathcal{R}_{h}(s)}\Big{[}\max_{a}\{R_{h}(s,a)+u(a)\}\Big{]}-\max_{a}\{u(a)\} \Big{|}\overset{(*)}{\leq}1\leq 3\sqrt{\frac{AL_{\delta}^{k}}{2}},\]

where \((*)\) uses the boundedness of the rewards in \([0,1]\). Next, recall that for any fixed \(n_{h}^{k-1}=n\in[k]\), the rewards samples at state \(s\) and step \(h\) are i.i.d. vectors on \([0,1]^{A}\). Therefore, by Lemma 18,

\[\Pr\Bigg{\{}n_{h}^{k-1}(s)=n,\forall u\in[0,2H]^{A}:\ \big{|}m_{h}(s,u)-\hat{m}_{h}^{k-1}(s,u) \big{|}>3\sqrt{\frac{AL_{\delta}^{k}}{2(n_{h}^{k-1}(s)\lor 1)}}\Bigg{\}}\leq\frac{ \delta}{8SAHk^{2}(k+1)}.\]

Taking a union bound on all possible values of \(n\in[k]\), \(s\) and \(h\), we get

\[\Pr\{E^{r}(k)\}\geq 1-SAk\cdot\frac{\delta}{8SAHk^{2}(k+1)}\geq 1-\frac{\delta}{8 k(k+1)}.\]

By summing over all \(k\geq 1\), the event \(\cap_{k}E^{r}(k)\) holds with a probability of at least \(1-\delta/8\). Finally, taking the union bound with the other three events leads to the desired result of \(\Pr(\mathbb{G}_{1})\geq 1-\delta/2\).

### Optimism of the Upper Confidence Value Functions

In this subsection, we prove that under the good event \(\mathbb{G}_{1}\), the values \(\bar{V}^{k}\) that MVP-RL produces are optimistic.

**Lemma 6** (Optimism).: _Under the first good event \(\mathbb{G}_{1}\), for all \(k\in[K]\), \(h\in[H]\) and \(s\in\mathcal{S}\), it holds that \(V^{*}_{h}(s)\leq\bar{V}^{k}_{h}(s)\)._

Proof.: The proof follows by backward induction on \(H\); see that the claim trivially holds for \(h=H+1\), where both values are defined to be zero.

Now assume by induction that for some \(k\in[K]\) and \(h\in[H]\), the desired inequalities hold at timestep \(h+1\) for all \(s\in\mathcal{S}\); we will show that this implies that they also hold at timestep \(h\).

At this point, we also assume w.l.o.g. that \(\bar{V}^{k}_{h}(s)<H\), and in particular, the value is not truncated; otherwise, by the boundedness of the rewards, \(V^{*}_{h}(s)\leq H=\bar{V}^{k}_{h}(s)\). For similar reasons, we assume w.l.o.g. that \(b^{p}_{k,h}(s,a)<H\), so that it is also not truncated.

By the optimism of the value at step \(h+1\) due to the induction hypothesis and the monotonicity of the bonus (Lemma 23), under the good event, we have for all \(s\in\mathcal{S}\) and \(a\in\mathcal{A}\) that

\[\hat{P}^{k-1}_{h}\bar{V}^{k}_{h+1}(s,a)+b^{p}_{k,h}(s,a)\] \[\geq\hat{P}^{k-1}_{h}\bar{V}^{k}_{h+1}(s,a)+\max\left\{\frac{20}{ 3}\sqrt{\frac{\operatorname{Var}_{\hat{P}^{k-1}_{h}(\cdot[s,a)}(\bar{V}^{k}_ {h+1})L^{k}_{\delta}}{n^{k-1}_{h}(s,a)\lor 1},\frac{400}{9}\frac{HL^{k}_{ \delta}}{n^{k-1}_{h}(s,a)\lor 1}}\right\}\] \[\geq\hat{P}^{k-1}_{h}V^{*}_{h+1}(s,a)+\max\left\{\frac{20}{3} \sqrt{\frac{\operatorname{Var}_{\hat{P}^{k-1}_{h}(\cdot[s,a)}(V^{*}_{h+1})L^{ k}_{\delta}}{n^{k-1}_{h}(s,a)\lor 1},\frac{400}{9}\frac{HL^{k}_{\delta}}{n^{k-1} _{h}(s,a)\lor 1}}\right\}\] (Lemma 23) \[\geq\hat{P}^{k-1}_{h}V^{*}_{h+1}(s,a)+\frac{10}{3}\sqrt{\frac{ \operatorname{Var}_{\hat{P}^{k-1}_{h}(\cdot[s,a)}(V^{*}_{h+1})L^{k}_{\delta}} {n^{k-1}_{h}(s,a)\lor 1}+\frac{200}{9}\frac{HL^{k}_{\delta}}{n^{k-1}_{h}(s,a) \lor 1}}\] \[\geq\hat{P}^{k-1}_{h}V^{*}_{h+1}(s,a)+\frac{10}{3}\sqrt{\frac{ \operatorname{Var}_{P_{h}(\cdot[s,a)}(V^{*}_{h+1})L^{k}_{\delta}}{n^{k-1}_{h}( s,a)\lor 1}+\frac{8HL^{k}_{\delta}}{n^{k-1}_{h}(s,a)\lor 1}}\] (Under \[E^{pv2}(k)\] ) \[\geq P_{h}V^{*}_{h+1}(s,a).\] (Under \[E^{pv1}(k)\] )

Thus, under the good event and the induction hypothesis, we have that

\[\bar{V}^{k}_{h}(s) =\mathbb{E}_{\boldsymbol{R}\sim\hat{\mathcal{R}}_{h}(s)}\Big{[} \max_{a\in\mathcal{A}}\Bigl{\{}R(a)+b^{p}_{k,h}(s,a)+\hat{P}^{k-1}_{h}\bar{V}^ {k}_{h+1}(s,a)\Bigr{\}}\bigg{]}+b^{r}_{k,h}(s)\] \[\geq\mathbb{E}_{\boldsymbol{R}\sim\hat{\mathcal{R}}_{h}(s)}\Big{[} \max_{a\in\mathcal{A}}\Bigl{\{}R(a)+P_{h}V^{*}_{h+1}(s,a)\Bigr{\}}\bigg{]}+b^{ r}_{k,h}(s).\]

In particular, using Proposition 1, we get

\[\bar{V}^{k}_{h}(s)-V^{*}_{h}(s) \geq\mathbb{E}_{\boldsymbol{R}\sim\hat{\mathcal{R}}_{h}(s)}\Big{[} \max_{a\in\mathcal{A}}\bigl{\{}R(a)+P_{h}V^{*}_{h+1}(s,a)\bigr{\}}\bigg{]}+b^{r} _{k,h}(s)\] \[\geq 0,\]

where the last inequality holds under the event \(E^{r}(k)\) with \(u(a)=P_{h}V^{*}_{h+1}(s,a)\in[0,H]^{A}\)

### The Second Good Event - Martingale Concentration

In this subsection, we present four good events that will allow us to replace the expectation over the randomizations inside each episode with their realization.

Define the following bonus-like term that will later appear in the proof due to value concentration:

\[b_{k,h}^{pv1}(s,a)=\min\Biggl{\{}\sqrt{\frac{2\mathrm{Var}_{B_{(} \cdot|s,a)}(V_{h+1}^{*})L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{4H^{2}SL_{ \delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1},H\Biggr{\}},\]

and let

\[Y_{1,h}^{k}:=\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\pi^{*}}(s_{h +1}^{k}),\] \[Y_{2,h}^{k}=\mathrm{Var}_{B_{(}\cdot|s_{h,h},a_{h,h})}(V_{h+1}^{ \pi^{*}}),\] \[Y_{3,h}^{k}=b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})+b_{k,h}^{pv1}(s_{h} ^{k},a_{h}^{k}).\]

The second good event is the intersection of the events \(\mathbb{G}_{2}=E^{\mathrm{diff}1}\cap E^{\mathrm{diff}2}\cap E^{\mathrm{Var}} \cap E^{bp}\) defined as follows.

\[E^{\mathrm{diff}1} =\Biggl{\{}\forall h\in[H],K\geq 1:\;\sum_{k=1}^{K}\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\leq\left(1+\frac{1}{2H}\right)\sum_{k=1}^{K}Y_{1,h}^{k}+18H ^{2}\ln\frac{8HK(K+1)}{\delta}\Biggr{\}},\] \[E^{\mathrm{diff}2} =\Biggl{\{}\forall h\in[H],K\geq 1:\;\sum_{k=1}^{K}\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}^{k}]\leq\left(1+\frac{1}{2H}\right)\sum_{k=1}^{K}Y_{1,h}^{k} +18H^{2}\ln\frac{8HK(K+1)}{\delta}\Biggr{\}},\] \[E^{\mathrm{Var}} =\Biggl{\{}K\geq 1:\;\sum_{k=1}^{K}\sum_{h=1}^{H}Y_{2,h}^{k}\leq 2 \sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}[Y_{2,h}^{k}|F_{k-1}]+4H^{3}\ln\frac{8HK( K+1)}{\delta}\Biggr{\}},\] \[E^{bp} =\Biggl{\{}\forall h\in[H],K\geq 1:\;\sum_{k=1}^{K}\mathbb{E}[Y_{3,h}^{k}|F_{k,h-1}]\leq 2\sum_{k=1}^{K}Y_{3,h}^{k}+50H^{2}\ln\frac{8HK(K+1)}{ \delta}\Biggr{\}},\]

We define the good event \(\mathbb{G}=\mathbb{G}_{1}\cap\mathbb{G}_{2}\).

**Lemma 7**.: _The good event \(\mathbb{G}\) holds with a probability of at least \(1-\delta\)._

Proof.: The proof follows similarly to Lemmas 15 and 21 of (Erroni et al., 2021).

First, define the random process \(W_{k}=\mathds{1}\Big{\{}\bar{V}_{h}^{k}(s)-V_{h}^{\pi^{k}}(s)\in[0,H],\forall h \in[H],s\in\mathcal{S}\Big{\}}\) and define \(\tilde{Y}_{1,h}^{k}=W_{k}Y_{1,h}^{k}\), which is bounded in \([0,H]\). Also observe that \(W_{k}\) is \(F_{k-1}\) measurable, since both values and policies are calculated based on data up to the episode \(k-1\), and in particular, it is \(F_{k,h-1}\) measurable and \(\tilde{Y}_{1,h}^{k}\) is \(F_{k,h}\) measurable. thus, by Lemma 25, for any \(k\in[K]\) and \(h\in[H]\), we have w.p. at least \(1-\frac{\delta}{8HK(K+1)}\) that

\[\sum_{k=1}^{K}\mathbb{E}[\tilde{Y}_{1,h}^{k}|F_{k,h-1}]\leq\left(1+\frac{1}{2H }\right)\sum_{k=1}^{K}\tilde{Y}_{1,h}^{k}+18H^{2}\ln\frac{8HK(K+1)}{\delta}.\]

Since \(W_{k}\) is \(F_{k,h-1}\) measurable, we can write the event as

\[\sum_{k=1}^{K}W_{k}\mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\leq\left(1+\frac{1}{2H} \right)\sum_{k=1}^{K}W_{k}Y_{1,h}^{k}+18H^{2}\ln\frac{8HK(K+1)}{\delta},\]

and taking the union bound over all \(h\in[H]\) and \(K\geq 1\), we get w.p. at least \(1-\frac{\delta}{8}\) that the event

\[\tilde{E}^{\mathrm{diff}1}=\Biggl{\{}\forall h\in[H],K\geq 1:\;\sum_{k=1}^{K}W_{k} \mathbb{E}[Y_{1,h}^{k}|F_{k,h-1}]\leq\left(1+\frac{1}{2H}\right)\sum_{k=1}^{K}W _{k}Y_{1,h}^{k}+18H^{2}\ln\frac{8HK(K+1)}{\delta}\Biggr{\}}.\]

Importantly, by optimism (Lemma 6), under \(\mathbb{G}_{1}\), it holds that \(W_{k}=1\) for all \(k\geq 1\), so we immediately get that \(\mathbb{G}_{1}\cap\tilde{E}^{\mathrm{diff}1}=\mathbb{G}_{1}\cap E^{\mathrm{ diff}1}\).

Following the exact same proof just with the filtration \(F_{k,h}^{R}\) and defining the equivalent \(\tilde{E}^{\mathrm{diff}2}\), we get that this event also holds w.p. \(1-\frac{\delta}{8}\) and is the desired event when \(\mathbb{G}_{1}\) holds.

Next, we prove that the other two events also hold w.p. at least \(1-\frac{\delta}{8}\).

By the assumptions of our setting, we know that \(V_{h}^{\pi^{k}}(s)\in[0,H]\), and so

\[\sum_{h=1}^{H}Y_{2,h}^{k}=\sum_{h=1}^{H}\mathrm{Var}_{P_{h}(\cdot|s_{t,h},a_{t, h})}(V_{h+1}^{\pi^{k}})\in[0,H^{3}].\]

In particular, applying Lemma 25 (w.r.t. the filtration \(F_{k}\)) with \(C=H^{3}\) and any fixed \(K\), we get w.p. \(1-\frac{\delta}{8HK(K+1)}\) that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}Y_{2,h}^{k}\leq 2\sum_{k=1}^{K}\sum_{h=1}^{H} \mathbb{E}[Y_{2,h}^{k}|F_{k-1}]+4H^{3}\ln\frac{8HK(K+1)}{\delta}.\]

Taking the union bound on all possible values of \(K\geq 1\) proves that \(E^{\mathrm{Var}}\) holds w.p. at least \(1-\frac{\delta}{8}\).

Similarly, by definition, we have that \(Y_{3,h}^{k}=b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})+b_{k,h}^{pv1}(s_{h}^{k},a_{h}^{k} )\in[0,2H]\) and is \(F_{k,h}\) measurable. Thus, for any fixed \(k\geq 1\) and \(h\in[H]\), using Lemma 25, we have w.p. \(1-\frac{\delta}{8HK(K+1)}\) that

\[\sum_{k=1}^{K}\mathbb{E}[Y_{3,h}^{k}|F_{k,h-1}] \leq\left(1+\frac{1}{4H}\right)\sum_{k=1}^{K}Y_{3,h}^{k}+50H^{2} \ln\frac{8HK(K+1)}{\delta}\] \[\leq 2\sum_{k=1}^{K}Y_{3,h}^{k}+50H^{2}\ln\frac{8HK(K+1)}{\delta},\]

applying the union bound on all \(K\geq 1\), the event \(E^{bp}\) holds w.p. \(1-\frac{\delta}{8}\).

To summarize, we have that the event \(\mathbb{G}_{1}\) holds w.p. \(1-\frac{\delta}{2}\) (Lemma 5), and we proved that the events \(\tilde{E}^{\mathrm{diff}1},\tilde{E}^{\mathrm{diff}2},E^{\mathrm{Var}},E^{bp}\) hold each w.p. \(1-\frac{\delta}{8}\), so we also have that the event

\[\mathbb{G} =\mathbb{G}_{1}\cap\mathbb{G}_{2}\] \[=\mathbb{G}_{1}\cap E^{\mathrm{diff}1}\cap E^{\mathrm{diff}2}\cap E ^{\mathrm{Var}}\cap E^{bp}\] \[=\mathbb{G}_{1}\cap\tilde{E}^{\mathrm{diff}1}\cap\tilde{E}^{ \mathrm{diff}2}\cap E^{\mathrm{Var}}\cap E^{bp}\]

holds w.p. at least \(1-\delta\). 

### Regret Analysis

We finally analyze the regret of the algorithm

**Theorem 1**.: _When running MVP-RL, with probability at least \(1-\delta\) uniformly for all \(K\geq 1\), it holds that \(\operatorname{Reg}^{R}(K)\leq\mathcal{O}\Big{(}\sqrt{R^{3}SAK}\ln\frac{SAHK}{ \delta}+H^{3}S^{2}A\big{(}\ln\frac{SAHK}{\delta}\big{)}^{2}\Big{)}\)._

Proof.: Assume that the good events \(\mathbb{G}\) holds, which by Lemma 7, happens with probability at least \(1-\delta\). Then, by optimism (Lemma 6), for any \(k\in[K]\), \(h\in[H]\) and \(s\in\mathcal{S}\), it holds that \(V_{h}^{*}(s)\leq\bar{V}_{h}^{k}(s)\). Moreover, we can lower bound the value of the policy \(\pi^{k}\) as follows (see Remark 1):

\[V_{h}^{\pi^{k}}(s) =\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}R(\pi_{h}^{k}(s, \bm{R}))+P_{h}V_{h+1}^{\pi^{k}}(s,\pi_{h}^{k}(s,\bm{R}))\Big{]}\] \[=\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}R(\pi_{h}^{k}(s, \bm{R}))+\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,\pi_{h}^{k}(s,\bm{R}))+b_{k,h}^{ p}(s,\pi_{h}^{k}(s,\bm{R}))\Big{]}\] \[\quad+\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}P_{h}V_{h+1 }^{\pi^{k}}(s,\pi_{h}^{k}(s,\bm{R}))-\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,\pi_{ h}^{k}(s,\bm{R}))-b_{k,h}^{p}(s,\pi_{h}^{k}(s,\bm{R}))\Big{]}\] \[\overset{(1)}{=}\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[} \max_{a\in\mathcal{A}}\Big{\{}R(a)+\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,a)+b_{k,h}^{p}(s,a)\Big{\}}\Big{]}\] \[\quad+\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}P_{h}V_{h+1 }^{\pi^{k}}(s,\pi_{h}^{k}(s,\bm{R}))-\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,\pi_{ h}^{k}(s,\bm{R}))-b_{k,h}^{p}(s,\pi_{h}^{k}(s,\bm{R}))\Big{]}\] \[\overset{(2)}{\geq}\mathbb{E}_{\bm{R}\sim\hat{\mathbb{R}}_{h}^{k- 1}(s)}\bigg{[}\max_{a\in\mathcal{A}}\Big{\{}R(a)+\hat{P}_{h}^{k-1}\bar{V}_{h+1 }^{k}(s,a)+b_{k,h}^{p}(s,a)\Big{\}}\bigg{]}-b_{k,h}^{r}(s)\] \[\quad+\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}P_{h}V_{h+1 }^{\pi^{k}}(s,\pi_{h}^{k}(s,\bm{R}))-\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,\pi_{ h}^{k}(s,\bm{R}))-b_{k,h}^{p}(s,\pi_{h}^{k}(s,\bm{R}))\Big{]}\] \[\overset{(3)}{\geq}\bar{V}_{h}^{k}(s)-2b_{k,h}^{r}(s)\] \[\quad+\mathbb{E}_{\bm{R}\sim\mathcal{R}_{h}(s)}\Big{[}P_{h}V_{h+1 }^{\pi^{k}}(s,\pi_{h}^{k}(s,\bm{R}))-\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,\pi_{ h}^{k}(s,\bm{R}))-b_{k,h}^{p}(s,\pi_{h}^{k}(s,\bm{R}))\Big{]}.\] (6)

Relation \((1)\) is by the definition of \(\pi^{k}\) (see Algorithm 3), while \((2)\) holds under the good event \(E^{r}(k)\) with \(u(a)=\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,a)+b_{k,h}^{p}(s,a)\in[0,2H]\) (due to the value and bonus truncation). Finally, \((3)\) is by the definition of \(\bar{V}_{h}^{k}(s)\), where the inequality also accounts for its possible truncation.

To further bound this, we need to bound

\[\hat{P}_{h}^{k-1}\bar{V}_{h+1}^{k}(s,a)-P_{h}V_{h+1}^{\pi^{k}}(s,a) =P_{h}\Big{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Big{)}(s,a)+ \Big{(}\hat{P}_{h}^{k-1}-P_{h}\Big{)}\bar{V}_{h+1}^{k}(s,a)\] \[=P_{h}\Big{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Big{)}(s,a)\] \[\quad+\Big{(}\hat{P}_{h}^{k-1}-P_{h}\Big{)}V_{h+1}^{*}(s,a)+\Big{(} \hat{P}_{h}^{k-1}-P_{h}\Big{)}\big{(}\bar{V}_{h+1}^{k}-V_{h+1}^{*}\big{)}(s,a).\]

The first error term can be bounded under the good event, while the second using Lemma 24. More formally, under the good event \(E^{pv1}(k)\), we have

\[\Big{|}\Big{(}\hat{P}_{h}^{k-1}-P_{h}\Big{)}V_{h+1}^{*}(s,a)\Big{|}\leq\sqrt{ \frac{2\mathrm{Var}_{P_{h}(\cdot|s,a)}(V_{h+1}^{*})L_{\delta}^{k}}{n_{h}^{k-1}(s,a )\lor 1}}+\frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1},\]

and by Lemma 24 with \(\alpha=4H\) (using and \(P_{1}=P_{h}\), \(P_{2}=\hat{P}_{h}^{k-1}\), under \(E^{p}(k)\)),

\[\Big{|}\Big{(}\hat{P}_{h}^{k-1}-P_{h}\Big{)}\big{(}\bar{V}_{h+1} ^{k}-V_{h+1}^{*}\big{)}(s,a)\Big{|} \leq\frac{1}{4H}\mathbb{E}_{P_{h}(\cdot|s,a)}\big{[}\bar{V}_{h+1} ^{k}(s^{\prime})-V_{h+1}^{*}(s^{\prime})\big{]}+\frac{HSL_{\delta}^{k}(1+4H \cdot 2/4)}{n_{h}^{k-1}(s,a)\lor 1}\] \[\leq\frac{1}{4H}\mathbb{E}_{P_{h}(\cdot|s,a)}\Big{[}\bar{V}_{h+1} ^{k}(s^{\prime})-V_{h+1}^{*^{\prime}}(s^{\prime})\Big{]}+\frac{3H^{2}SL_{\delta}^{ k}}{n_{h}^{k-1}(s,a)\lor 1}\] \[=\frac{1}{4H}P_{h}\Big{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}} \Big{)}(s,a)+\frac{3H^{2}SL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1},\]

[MISSING_PAGE_FAIL:26]

inequality up to \(h=H+1\) (where both values are zero), w.p. at least \(1-\delta\), we get

\[\mathrm{Reg}^{R}(K) \leq\sum_{k=1}^{K}\Bigl{(}V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi^{k}}(s_{1 }^{k})\Bigr{)}\] \[\leq\sum_{k=1}^{K}\Bigl{(}\bar{V}_{1}^{k}(s_{1}^{k})-V_{1}^{\pi^{k }}(s_{1}^{k})\Bigr{)}\] (Lemma 6) \[\leq 18\biggl{(}1+\frac{1}{2H}\biggr{)}^{2H}\sum_{k=1}^{K}\frac{ \sqrt{L_{\delta}^{k}\mathrm{Var}_{P_{h}(\cdot|s_{h}^{k},a_{h}^{k})}(V_{h+1}^{ \pi^{k}})}}{\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\lor 1}}+\biggl{(}1+\frac{1}{2H} \biggr{)}^{2H}\sum_{k=1}^{K}\frac{1700H^{2}SL_{\delta}^{k}}{n_{h}^{k-1}(s_{h }^{k},a_{h}^{k})\lor 1}\] \[\quad+6\biggl{(}1+\frac{1}{2H}\biggr{)}^{2H}\sum_{k=1}^{K}\sqrt{ \frac{AL_{\delta}^{k}}{2n_{h}^{k-1}(s)\lor 1}}\] \[\stackrel{{(*)}}{{\leq}}100\sqrt{H^{3}SAKL_{\delta}^ {K}}+50\sqrt{2SA}H^{2}\bigl{(}L_{\delta}^{K}\bigr{)}^{1.5}\] \[\quad+5000H^{2}SL_{\delta}^{K}\cdot SAH(2+\ln(K))+12\sqrt{AL_{ \delta}^{K}}\Bigl{(}SH+2\sqrt{SH^{2}K}\Bigr{)}\] \[=\mathcal{O}\Bigl{(}\sqrt{H^{3}SAKL_{\delta}^{K}}+H^{3}S^{2}A(L_{ \delta}^{K})^{2}\Bigr{)}.\]

Relation \((*)\) is by Lemma 9 and Lemma 20.

#### b.7.1 Lemmas for Bounding Bonus Terms

**Lemma 8**.: _Conditioned on the good event \(\mathbb{G}\), for any \(h\in[H]\), it holds that_

\[\sum_{k=1}^{K}\Bigl{(}b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})+b_{k,h}^{pv1 }(s_{h}^{k},a_{h}^{k})\Bigr{)} \leq\frac{1}{8H}\biggl{(}1+\frac{1}{2H}\biggr{)}\sum_{k=1}^{K} \Bigl{(}\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\pi^{k}}(s_{h+1}^{k})\Bigr{)}\] \[\quad+9\sum_{k=1}^{K}\sqrt{\frac{\operatorname{Var}_{P_{h}( \cdot|s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\pi^{k}})L_{\delta}^{k}}{n_{h}^{k-1}(s_{h} ^{k},a_{h}^{k})\lor 1}}+\sum_{k=1}^{K}\frac{810H^{2}SL_{\delta}^{k}}{n_{h}^{k-1}(s _{h}^{k},a_{h}^{k})\lor 1}.\]

Proof.: We start by analyzing each of the terms separately. First, we apply Lemma 22 with \(\alpha=\frac{20}{3}\cdot 32HL_{\delta}^{k}\), noting that under the good event (by Lemma 6), \(0\leq V_{h+1}^{\pi^{k}}(s)\leq V_{h+1}^{*}(s)\leq\bar{V}_{h+1}^{k}(s)\leq H\) and using the event \(E^{pv}\); doing so yields

\[b_{k,h}^{p}(s,a) \leq\frac{20}{3}\sqrt{\frac{\operatorname{Var}_{\hat{P}_{h}^{k-1 }(\cdot|s,a)}(\bar{V}_{h+1}^{k})L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{400}{9} \frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\] \[\leq\frac{20\sqrt{L_{\delta}^{k}\operatorname{Var}_{P_{h}(\cdot |s,a)}(V_{h+1}^{\pi^{k}})}}{3\sqrt{n_{h}^{k-1}(s,a)\lor 1}}+\frac{1}{32H}P_{h} \Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)}(s,a)+\frac{1}{32H}\hat{P} _{h}^{k-1}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)}(s,a)\] \[\quad+\frac{6400H^{2}L_{\delta}^{k}}{9n_{h}^{k-1}(s,a)\lor 1}+ \frac{20}{3}\frac{4HL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}+\frac{400}{9} \frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\]

Using Lemma 24 with \(\alpha=1\), under the good event \(E^{p}(k)\) and for any \(s,a\), we can further bound

\[\hat{P}_{h}^{k-1}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}} \Bigr{)}(s,a)\] \[\quad=P_{h}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)}(s,a )+\Bigl{(}\hat{P}_{h}^{k-1}-P_{h}\Bigr{)}\Bigl{(}\bar{V}_{h+1}^{k}(s^{\prime} )-V_{h+1}^{\pi^{k}}\Bigr{)}(s,a)\] \[\quad\leq P_{h}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)} (s,a)+P_{h}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)}(s,a)+\frac{HSL_{ \delta}^{k}(1+2\cdot 1/4)}{n_{h}^{k-1}(s,a)\lor 1}\] (Lemma 24) \[\quad\leq 2P_{h}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)} (s,a)+\frac{1.5HSL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\]

Thus, we get the overall bound

\[b_{k,h}^{p}(s,a)\leq\frac{20\sqrt{L_{\delta}^{k}\operatorname{Var}_{P_{h}( \cdot|s,a)}(V_{h+1}^{\pi^{k}})}}{3\sqrt{n_{h}^{k-1}(s,a)\lor 1}}+\frac{3}{32H}P_{h} \Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Bigr{)}(s,a)+\frac{785H^{2}SL_{ \delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\]

For the second bonus, we apply Lemma 21 w.r.t. \(V_{h+1}^{\pi^{k}}(s)\leq V_{h+1}^{*}(s)\) and \(\alpha=32\sqrt{2L_{\delta}^{k}}H\) and get

\[b_{k,h}^{pv1}(s,a) \leq\sqrt{\frac{2\operatorname{Var}_{P_{h}(\cdot|s,a)}(V_{h+1}^{*} )L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{4H^{2}SL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\] \[\leq\sqrt{\frac{2\operatorname{Var}_{P_{h}(\cdot|s,a)}(V_{h+1}^{ \pi^{k}})L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{1}{32H}P_{h}\Bigl{(}V_{h+1}^{*}-V_{h+1}^{ \pi^{k}}\Bigr{)}(s,a)+\frac{16HL_{\delta}^{k}}{n_{h}^{k-1}(s,a)}+\frac{4H^{2}SL_ {\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\] \[\leq\sqrt{\frac{2\operatorname{Var}_{P_{h}(\cdot|s,a)}(V_{h+1}^{ \pi^{k}})L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}+\frac{1}{32H}P_{h}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{ \pi^{k}}\Bigr{)}(s,a)+\frac{20H^{2}SL_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}\]

where we again used the optimism. Combining both and summing over all \(k\), we get

\[\sum_{k=1}^{K}\Bigl{(}b_{k,h}^{p}(s_{h}^{k},a_{h}^{k})+b_{k,h}^{pv1 }(s_{h}^{k},a_{h}^{k})\Bigr{)} \leq 9\sum_{k=1}^{K}\sqrt{\frac{\operatorname{Var}_{P_{h}(\cdot|s_{h}^{ k},a_{h}^{k})}(V_{h+1}^{\pi^{k}})L_{\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k}) \lor 1}}+\frac{1}{8H}\sum_{k=1}^{K}P_{h}\Bigl{(}\bar{V}_{h+1}^{k}-V_{h+1}^{ \pi^{k}}\Bigr{)}(s_{h}^{k},a_{h}^{k})\] \[\quad+\sum_{k=1}^{K}\frac{805H^{2}SL_{\delta}^{k}}{n_{h}^{k-1}(s_{h} ^{k},a_{h}^{k})\lor 1}\]Finally, under the good event \(E^{\mathrm{dif}2}\), it holds that

\[\sum_{k=1}^{K}P_{h}\Big{(}\bar{V}_{h+1}^{k}-V_{h+1}^{\pi^{k}}\Big{)} (s_{h}^{k},a_{h}^{k}) =\sum_{k=1}^{K}\mathbb{E}\Big{[}\bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h +1}^{\pi^{k}}(s_{h+1}^{k})|F_{k,h-1}^{R}\Big{]}\] \[\leq\bigg{(}1+\frac{1}{2H}\bigg{)}\sum_{k=1}^{K}\Big{(}\bar{V}_{h+ 1}^{k}(s_{h+1}^{k})-V_{h+1}^{\pi^{k}}(s_{h+1}^{k})\Big{)}+18H^{2}\ln\frac{8HK(K +1)}{\delta}.\]

Substituting this relation back concludes the proof. 

**Lemma 9**.: _Under the event \(E^{\mathrm{Var}}\) it holds that_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{\sqrt{\mathrm{Var}_{P_{h}(\cdot|s_{h}^{k},a_ {h}^{k})}(V_{h+1}^{\pi^{k}})}}{\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\lor 1}} \leq 2\sqrt{H^{3}SAKL_{\delta}^{K}}+\sqrt{8S\bar{A}}H^{2}L_{\delta}^{K}.\]

Proof.: Following Lemma 24 of [1], by Cauchy-Schwartz inequality, it holds that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{\sqrt{\mathrm{Var}_{P_{h}(\cdot|s_{h}^{k},a_ {h}^{k})}(V_{h+1}^{\pi^{k}})}}{\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\lor 1}} \leq\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\mathrm{Var}_{P_{h}(\cdot|s_{h}^{k},a_{ h}^{k})}(V_{h+1}^{\pi^{k}})}\sqrt{\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{n_{h}^{k-1} (s_{h}^{k},a_{h}^{k})\lor 1}}.\]

The second term can be bounded by Lemma 20, namely,

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\lor 1 }\leq SAH(2+\ln(K)).\]

We further focus on bounding the first term. Under \(E^{\mathrm{Var}}\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\mathrm{Var}_{P_{h}(\cdot|s_{h}^{k},a _{h}^{k})}(V_{h+1}^{\pi^{k}})\] \[\leq 2\sum_{k=1}^{K}\mathbb{E}\left[\sum_{h=1}^{H}\mathrm{Var}_{P _{h}(\cdot|s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\pi^{k}})|F_{k-1}\right]+4H^{3}\ln \frac{8HK(K+1)}{\delta}\] (Under \[E^{\mathrm{Var}}\] ) \[\leq 2\sum_{k=1}^{K}\mathbb{E}\left[\left(\sum_{h=1}^{H}R_{h}(s_{ h}^{k},a_{h}^{k})-V_{1}^{\pi^{k}}(s_{1}^{k})\right)^{2}|F_{k-1}\right]+4H^{3}\ln \frac{8HK(K+1)}{\delta}\] (By Lemma 3) \[\leq 2H^{2}K+4H^{3}\ln\frac{8HK(K+1)}{\delta},\]

where the last inequality is since both the values and cumulative rewards are bounded in \([0,H]\). Combining both, we get

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{\sqrt{\mathrm{Var}_{P_{h}(\cdot |s_{h}^{k},a_{h}^{k})}(V_{h+1}^{\pi^{k}})}}{\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h}^{ k})\lor 1}} \leq\sqrt{2H^{2}K+4H^{3}\ln\frac{8HK(K+1)}{\delta}}\sqrt{SAH(2+ \ln(K))}\] \[\leq\sqrt{2H^{2}K+4H^{3}\ln\frac{8HK(K+1)}{\delta}}\sqrt{2SAH\ln \frac{8HK(K+1)}{\delta}}\] \[\leq 2\sqrt{H^{3}SAKL_{\delta}^{K}}+\sqrt{8S\bar{A}}H^{2}L_{\delta} ^{K}.\]Proofs for Transition Lookahead

### Data Generation Process

As for the reward transition, we also assume that all data was generated before the game starts for all state-action-timesteps, and it is given to the agent when the relevant \((s,a,h)\) is visited. Thus, the rewards and next-state from the first \(i^{th}\) visits at a state (or a state-action pair) at a certain timestep are i.i.d.

Throughout this appendix, we use the notation \(\bm{s}^{\prime k}_{h+1}=\big{\{}s^{\prime k}_{h+1}(s^{k}_{h},a)\big{\}}_{a\in \mathcal{A}}\) to denote the next-state observations at episode \(k\) and timestep \(h\) for all the actions, and use the equivalent filtrations to the ones defined at Appendix B.1, namely

\[F_{k,h}=\sigma\Big{(}\big{\{}s^{1}_{t},a^{1}_{t},\bm{s}^{\prime 1}_{t+1},R^{1}_{t}\big{\}}_{t \in[H]},\ldots,\big{\{}s^{k-1}_{t},a^{k-1}_{t},\bm{s}^{\prime k-1}_{t+1},R^{k- 1}_{t}\big{\}}_{t\in[H]},\big{\{}s^{k}_{t},a^{k}_{t},\bm{s}^{\prime\prime k}_{ t+1},R^{k}_{t}\big{\}}_{t\in[h]}\Big{)},\] \[F_{k}=\sigma\Big{(}\big{\{}s^{1}_{t},a^{1}_{t},\bm{s}^{\prime 1}_{t+1} \big{\}}_{t\in[H]},\ldots,\big{\{}s^{k}_{t},a^{k}_{t},\bm{s}^{\prime\prime k}_ {t+1},R^{k}_{t}\big{\}}_{t\in[H]},s^{k+1}_{1}\Big{)}.\]

In particular, notice that since both \(\bm{s}^{\prime k}_{h+1}\) and \(a^{k}_{h}\) are \(F_{k,h}\) measurable, then so does \(s^{k}_{h+1}\).

### Extended MDP for Transition Lookahead

In this appendix, we present an equivalent extended MDP that embeds the lookahead into the state to fall under the vanilla MDP model, similarly to Appendix B.2. We use this equivalence to apply various existing results on MDPs without the need to reprove them. We follow the same conventions as Appendix B.2 while denoting transition lookahead values by \(V^{T,\pi}(s|\mathcal{M})\) (and again, the superscript \(T\) will be omitted in subsequent subsections).

For any MDP \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,P,\mathcal{R})\), let \(\mathcal{M}^{T}\) be an MDP of horizon \(2H\) and state space \(\mathcal{S}^{A+1}\) that separates the state transition and next-state generation as follows:

1. Assume w.l.o.g. that \(\mathcal{M}\) starts at some initial state \(s_{1}\). The extended environment starts at a state \(s_{1}\times\bm{s}^{\prime}_{0}\), where \(\bm{s}^{\prime}_{0}\in\mathcal{S}^{A}\) is a vector of \(A\) copies of some arbitrary state \(s_{0}\in\mathcal{S}\).
2. For any \(h\in[H]\), at timestep \(2h-1\), the environment \(\mathcal{M}^{T}\) transitions from state \(s_{h}\times\bm{s}^{\prime}_{0}\) to \(s_{h}\times\bm{s}^{\prime}_{h+1}\), where \(\bm{s}^{\prime}_{h+1}\sim P_{h}(s)\) is a vector containing the next state for all actions \(a\in\mathcal{A}\); this transition happens regardless of the action that the agent played. At timestep \(2h\), given an action \(a_{h}\), the environment transitions from \(s_{h}\times\bm{s}^{\prime}_{h+1}\) to \(s^{\prime}_{h+1}(a)\times\bm{s}^{\prime}_{0}\).
3. The rewards at odd steps \(2h-1\) are zero, while the rewards at even steps \(2h\) are \(R_{h}(s_{h},a_{h})\sim\mathcal{R}_{h}(s_{h},a_{h})\) of expectation \(r_{h}(s_{h},a_{h})\).

As before, since the next state is embedded into the extended state space, any state-dependent policy in \(\mathcal{M}^{T}\) is a one-step transition lookahead policy in the original MDP. Also, the policy at even timesteps does not affect either the rewards or transitions, so it does not affect the value in any way. We again couple the two environments to have the exact same randomness, so assuming that the policy at the even steps in \(\mathcal{M}^{T}\) is the same as the policy in \(\mathcal{M}\), we trivially get the following relation between the values

\[V^{\pi}_{2h}(s,\bm{s}^{\prime}|\mathcal{M}^{T})=\mathbb{E}\Bigg{[} \sum_{t=h}^{H}R_{t}(s_{t},a_{t})|s_{h}=s,s^{\prime}_{h+1}(s,\cdot)=\bm{s}^{ \prime},\pi\Bigg{]}\triangleq V^{T,\pi}_{h}(s,\bm{s}^{\prime}|\mathcal{M}),\] \[V^{\pi}_{2h-1}(s,\bm{s}^{\prime}_{0}|\mathcal{M}^{T})=\mathbb{E} \Bigg{[}\sum_{t=h}^{H}R_{t}(s_{t},a_{t})|s_{h}=s,\pi\Bigg{]}=V^{T,\pi}_{h}(s| \mathcal{M}).\] (7)

While \(\mathcal{M}^{T}\) is finite, it is exponential in size, so applying any standard algorithm in this environment would lead to exponentially-bad performance bounds. Nonetheless, as with the extended-reward environment, we use this representation to prove useful results on one-step transition lookahead.

**Proposition 2**.: _The optimal value of one-step transition lookahead agents satisfies_

\[V_{H+1}^{T,*}(s)=0, \forall s\in\mathcal{S},\] \[V_{h}^{T,*}(s)=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\biggl{[} \max_{a\in\mathcal{A}}\Bigl{\{}r_{h}(s,a)+V_{h+1}^{T,*}(s^{\prime}(s,a)) \Bigr{\}}\biggr{]}, \forall s\in\mathcal{S},h\in[H].\]

_Also, given next-state observations \(\bm{s}^{\prime}=\{s^{\prime}(a)\}_{a\in\mathcal{A}}\) at state \(s\) and step \(h\), the optimal policy is_

\[\pi_{h}^{*}(s,\bm{s}^{\prime})\in\operatorname*{arg\,max}_{a\in \mathcal{A}}\Bigl{\{}r_{h}(s,a)+V_{h+1}^{T,*}(s^{\prime}(a))\Bigr{\}}.\]

Proof.: We prove the result in the extended MDP \(\mathcal{M}^{T}\), in which (as with reward lookahead) the optimal value can be calculated using the Bellman equations as follows [Puterman, 2014]

\[V_{2h+1}^{T}(s,\bm{s}^{\prime}|\mathcal{M}^{T})=0, \forall s\in\mathcal{S},\bm{s}^{\prime}\in\mathcal{S}^{A},\] \[V_{2h}^{*}(s,\bm{s}^{\prime}|\mathcal{M}^{T})=\max_{a}\bigl{\{}r _{h}(s,a)+V_{2h+1}^{*}(s^{\prime}(a),\bm{s}^{\prime}_{0}|\mathcal{M}^{T}) \bigr{\}}, \forall h\in[H],s\in\mathcal{S},\bm{s}^{\prime}\in\mathcal{S}^{A},\] \[V_{2h-1}^{*}(s,\bm{s}^{\prime}_{0}|\mathcal{M}^{T})=\mathbb{E}_{ \bm{s}^{\prime}\sim P_{h}(s)}\bigl{[}V_{2h}^{*}(s,\bm{s}^{\prime}|\mathcal{M}^ {T})\bigr{]}, \forall h\in[H],s\in\mathcal{S}.\] (8)

By the equivalence between \(\mathcal{M}\) and \(\mathcal{M}^{T}\) for all policies, this is also the optimal value in \(\mathcal{M}\). Combining both recursion equations and substituting Equation (7) leads to the stated value calculation for all \(h\in[H]\) and \(s\in\mathcal{S}\):

\[V_{h}^{T,*}(s|\mathcal{M}) =V_{2h-1}^{*}(s,\bm{s}^{\prime}_{0}|\mathcal{M}^{T})\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\bigl{[}V_{2h}^{*}(s, \bm{s}^{\prime}_{h+1}|\mathcal{M}^{T})\bigr{]}\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Bigl{[}\max_{a}\bigl{\{} r_{h}(s,a)+V_{2h+1}^{*}(s^{\prime}_{h+1}(a),\bm{s}^{\prime}_{0}|\mathcal{M}^ {T})\bigr{\}}\Bigr{]}\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Bigl{[}\max_{a}\Bigl{\{} r_{h}(s,a)+V_{h+1}^{T,*}(s^{\prime}_{h+1}(a)|\mathcal{M})\Bigr{\}}\Bigr{]}.\]

In addition, a given state \(s\) and next-state observations \(\bm{s}^{\prime}\), the optimal policy at the even stages of the extended MDP is

\[\pi_{2h}^{*}(s,\bm{s}^{\prime})\in\operatorname*{arg\,max}_{a\in\mathcal{A}} \bigl{\{}r_{h}(s,a)+V_{2h+1}^{*}(s^{\prime}(a))\bigr{\}},\]

alongside arbitrary actions at odd steps. Playing this policy in the original MDP will lead to the optimal one-step transition lookahead policy, as it achieves the optimal value of the original MDP. By the value relations between the two environments (\(V_{2h+1}^{*}(s,\bm{s}^{\prime}_{0}|\mathcal{M}^{T})=V_{h+1}^{T,*}(s|\mathcal{ M})\)), this is equivalent to the stated policy. 

**Remark 2**.: _As in Remark 1, one could write the dynamic programming equations for any policy \(\pi\in\Pi^{T}\), and not just to the optimal one, namely_

\[V_{2h}^{*}(s,\bm{s}^{\prime}|\mathcal{M}^{T})=r_{h}(s,\pi(s,\bm {s}^{\prime}))+V_{2h+1}^{*}(s^{\prime}(\pi_{h}(s,\bm{s}^{\prime})),\bm{s}^{ \prime}_{0}|\mathcal{M}^{T}), \forall h\in[H],s\in\mathcal{S},\bm{s}^{\prime}\in\mathcal{S}^{A},\] \[V_{2h-1}^{*}(s,\bm{s}^{\prime}_{0}|\mathcal{M}^{T})=\mathbb{E}_{ \bm{s}^{\prime}\sim P_{h}(s)}\bigl{[}V_{2h}^{\pi}(s,\bm{s}^{\prime}|\mathcal{M }^{T})\bigr{]}, \forall h\in[H],s\in\mathcal{S}.\]

_In particular, following the notation of Equation (7), we can write_

\[V_{h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{M})=r_{h}(s,\pi_{h}(s, \bm{s}^{\prime}))+V_{h+1}^{T,\pi}(s^{\prime}(\pi_{h}(s,\bm{s}^{\prime}))| \mathcal{M}),\qquad\text{and,}\] \[V_{h}^{T,\pi}(s|\mathcal{M}) =\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Bigl{[}V_{h}^{T,\pi}(s, \bm{s}^{\prime}|\mathcal{M})\Bigr{]}\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Bigl{[}r_{h}(s,\pi_{h}( s,\bm{s}^{\prime}))+V_{h+1}^{T,\pi}(s^{\prime}(\pi_{h}(s,\bm{s}^{\prime}))| \mathcal{M})\Bigr{]},\]

_a notation that will be extensively used for transition lookahead._We also prove a variation of the law of total variance (LTV) for transition lookahead:

**Lemma 10**.: _For any one-step transition lookahead policy \(\pi\in\Pi^{T}\), it holds that_

\[\mathbb{E}\Biggl{[}\sum_{h=1}^{H}\mathrm{Var}_{\bm{s}^{\prime}\sim P_{h}(s_{h})}( V_{h}^{T,\pi}(s_{h},\bm{s}^{\prime}))|\pi,s_{1}\Biggr{]}\leq\mathbb{E}\Biggl{[} \Biggl{(}\sum_{h=1}^{H}r_{h}(s_{h},a_{h})-V_{1}^{T,\pi}(s_{1})\Biggr{)}^{2}|\pi, s_{1}\Biggr{]}.\]

Proof.: We apply the law of total variance in the extended MDP; there, the expected rewards are either \(0\) (at odd steps) or \(r_{h}(s_{h},a_{h})\) (at even steps), so the total expected rewards are \(\sum_{h=1}^{H}r_{h}(s_{h},a_{h})\). Hence, by Lemma 27,

\[\mathbb{E}\Biggl{[}\Biggl{(}\sum_{h=1}^{H}r_{h}(s_{h},a_{h})-V_{1} ^{\pi}(s_{1},\bm{s}^{\prime}_{0}|\mathcal{M}^{T})\Biggr{)}^{2}|\pi,s_{1}\Biggr{]}\] \[\quad=\mathbb{E}\left[\underbrace{\sum_{h=1}^{H}\mathrm{Var}(V_{2 h}^{\pi}(s_{h},\bm{s}^{\prime}_{h+1}|\mathcal{M}^{T})|(s_{h},\bm{s}^{\prime}_{0}))}_{ \text{odd steps}}+\underbrace{\sum_{h=1}^{H}\mathrm{Var}(V_{2h+1}^{\pi}(s_{h+ 1},\bm{s}^{\prime}_{0}|\mathcal{M}^{T})|(s_{h},\bm{s}^{\prime}_{h+1}))}_{ \text{Even steps}}|\pi,s_{1}\right]\] \[\quad\geq\mathbb{E}\Biggl{[}\sum_{h=1}^{H}\mathrm{Var}(V_{2h}^{ \pi}(s_{h},\bm{s}_{h+1}|\mathcal{M}^{T})|(s_{h},\bm{s}^{\prime}_{0}))|\pi,s_{1} \Biggr{]}\] \[\quad=\mathbb{E}\Biggl{[}\sum_{h=1}^{H}\mathrm{Var}_{\bm{s}^{ \prime}\sim P_{h}(s_{h})}(V_{2h}^{\pi}(s_{h},\bm{s}^{\prime}|\mathcal{M}^{T})) |\pi,s_{1}\Biggr{]}\] \[\quad=\mathbb{E}\Biggl{[}\sum_{h=1}^{H}\mathrm{Var}_{\bm{s}^{ \prime}\sim P_{h}(s_{h})}(V_{h}^{T,\pi}(s_{h},\bm{s}^{\prime}|\mathcal{M}))| \pi,s_{1}\Biggr{]}.\]

Using again the identity \(V_{1}^{\pi}(s_{1},\bm{s}^{\prime}_{0}|\mathcal{M}^{T})=V_{1}^{T,\pi}(s_{1}| \mathcal{M})\) leads to the desired result. 

Finally, prove a value-difference lemma also for transition lookahead

**Lemma 11** (Value-Difference Lemma with Transition Lookahead).: _Let \(\mathcal{M}_{1}=(\mathcal{S},\mathcal{A},H,P^{1},\mathcal{R}^{1})\) and \(\mathcal{M}_{2}=(\mathcal{S},\mathcal{A},H,P^{2},\mathcal{R}^{2})\) be two environments. For any deterministic one-step transition lookahead policy \(\pi\in\Pi^{T}\), any \(h\in[H]\) and \(s\in\mathcal{S}\), it holds that_

\[V_{h}^{T,\pi}(s|\mathcal{M}_{1}) -V_{h}^{T,\pi}(s|\mathcal{M}_{2})\] \[=\mathbb{E}_{\mathcal{M}_{1}}\bigl{[}r_{h}^{1}(s_{h},\pi_{h}(s_{ h},\bm{s}^{\prime}_{h+1}))-r_{h}^{2}(s_{h},\pi_{h}(s_{h},\bm{s}^{\prime}_{h+1}))|s_{h }=s\bigr{]}\] \[\quad+\mathbb{E}_{\mathcal{M}_{1}}\bigl{[}V_{h+1}^{T,\pi}(s_{h+1} |\mathcal{M}_{1})-V_{h+1}^{T,\pi}(s_{h+1}|\mathcal{M}_{2})|s_{h}=s\bigr{]}\] \[\quad+\mathbb{E}_{\mathcal{M}_{1}}\Bigl{[}\mathbb{E}_{\bm{s}^{ \prime}\sim P_{h}^{1}(s_{h})}\Bigl{[}V_{h}^{T,\pi}(s_{h},\bm{s}^{\prime}| \mathcal{M}_{2})\Bigr{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{2}(s_{h})} \Bigl{[}V_{h}^{T,\pi}(s_{h},\bm{s}^{\prime}|\mathcal{M}_{2})\Bigr{]}|s_{h}=s \Bigr{]}.\]

_where \(V_{h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{M})\) is the value at a state given the reward realization, defined in Equation (7) and given in Remark 2._

Proof.: We again work with the extended MDPs \(\mathcal{M}_{1}^{T},\mathcal{M}_{2}^{T}\) and use their Bellman equations, namely,

\[V_{2h-1}^{\pi}(s,\bm{s}^{\prime}|\mathcal{M}^{T})=\mathbb{E}_{\bm {s}^{\prime}\sim P_{h}(s)}\bigl{[}V_{2h}^{\pi}(s,\bm{s}^{\prime}|\mathcal{M}^{ T})\bigr{]},\qquad\qquad\qquad\qquad\qquad\forall h\in[H],s\in\mathcal{S}.\]Using the relation between the value of the original and extended MDP (eq. (7)) and the Bellman equations of the extended MDP, for any \(h\in[H]\), we have

\[V_{h}^{T,\pi}(s|\mathcal{M}_{1})-V_{h}^{T,\pi}(s|\mathcal{M}_{2})\] \[=V_{2h-1}^{2}(s,\bm{s}_{0}^{\prime}|\mathcal{M}_{1}^{T})-V_{2h-1}^ {\pi}(s,\bm{s}_{0}^{\prime}|\mathcal{M}_{2}^{T})\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)}\big{[}V_{2h}^{\pi} (s,\bm{s}^{\prime}|\mathcal{M}_{1}^{T})\big{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P _{h}^{2}(s)}\big{[}V_{2h}^{\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2}^{T})\big{]}\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)}\big{[}V_{2h}^{\pi} (s,\bm{s}^{\prime}|\mathcal{M}_{1}^{T})-V_{2h}^{\pi}(s,\bm{s}^{\prime}| \mathcal{M}_{2}^{T})\big{]}+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)} \big{[}V_{2h}^{\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2}^{T})\big{]}-\mathbb{E}_ {\bm{s}^{\prime}\sim P_{h}^{2}(s)}\big{[}V_{h}^{T}(s,\bm{s}^{\prime}|\mathcal{ M}_{2})\big{]}\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)}\big{[}V_{2h}^{\pi} (s,\bm{s}^{\prime}|\mathcal{M}_{1}^{T})-V_{2h}^{\pi}(s,\bm{s}^{\prime}| \mathcal{M}_{2}^{T})\big{]}+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)} \Big{[}V_{h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2})\Big{]}-\mathbb{E}_{\bm {s}^{\prime}\sim P_{h}^{2}(s)}\Big{[}V_{h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{ M}_{2})\Big{]}\] \[=\mathbb{E}_{\mathcal{M}_{1}}\big{[}V_{2h}^{\pi}(s,\bm{s}^{\prime }_{h}+1|\mathcal{M}_{1}^{T})-V_{2h}^{\pi}(s,\bm{s}^{\prime}_{h},\bm{s}^{\prime }_{h+1}|\mathcal{M}_{2}^{T})|s_{h}=s\big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)}\Big{[}V_{h}^{ T,\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2})\Big{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P _{h}^{2}(s)}\Big{[}V_{h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2})\Big{]}.\] (9)

Denoting \(a_{h}=\pi_{h}(s_{h},\bm{s}^{\prime}_{h+1})\) the action taken by the agent at environment \(\mathcal{M}_{1}\), We have

\[V_{2h}^{\pi}(s_{h},\bm{s}^{\prime}_{h+1}|\mathcal{M}_{1}^{T})-V _{2h}^{\pi}(s_{h},\bm{s}^{\prime}_{h+1}|\mathcal{M}_{2}^{T})\] \[\quad=\big{(}r_{h}^{1}(s_{h},a_{h})+V_{2h+1}^{\pi}(s^{\prime}_{h+ 1}(a_{h}),\bm{s}^{\prime}_{h}|\mathcal{M}_{1}^{T})\big{)}-\big{(}r_{h}^{2}(s_ {h},a_{h})+V_{2h+1}^{\pi}(s^{\prime}_{h+1}(a_{h}),\bm{s}^{\prime}_{0}|\mathcal{ M}_{2}^{T})\big{)}\] \[\quad=r_{h}^{1}(s_{h},a_{h})-r_{h}^{2}(s_{h},a_{h})+V_{h+1}^{T, \pi}(s^{\prime}_{h+1}(a_{h})|\mathcal{M}_{1})-V_{h+1}^{T,\pi}(s^{\prime}_{h+1} (a_{h})|\mathcal{M}_{2}),\]

when taking the expectation w.r.t. \(\mathcal{M}_{1}\), it holds that \(s^{\prime}_{h+1}(a_{h})=s_{h+1}\); substituting this back into Equation (9), we get

\[V_{h}^{\pi}(s|\mathcal{M}_{1})-V_{h}^{\pi}(s|\mathcal{M}_{2})\] \[\quad=\mathbb{E}_{\mathcal{M}_{1}}\Big{[}r_{h}^{1}(s_{h},a_{h})-r _{h}^{2}(s_{h},a_{h})+V_{h+1}^{T,\pi}(s^{\prime}_{h+1}(a_{h})|\mathcal{M}_{1}) -V_{h+1}^{T,\pi}(s^{\prime}_{h+1}(a_{h})|\mathcal{M}_{2})|s_{h}=s\Big{]}\] \[\quad\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{1}(s)}\Big{[}V_{ h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2})\Big{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P _{h}^{2}(s)}\Big{[}V_{h}^{T,\pi}(s,\bm{s}^{\prime}|\mathcal{M}_{2})\Big{]}\] \[\quad=\mathbb{E}_{\mathcal{M}_{1}}\big{[}r_{h}^{1}(s_{h},\pi_{h}(s _{h},\bm{s}^{\prime}_{h+1}))-r_{h}^{2}(s_{h},\pi_{h}(s_{h},\bm{s}^{\prime}_{h+1} ))|s_{h}=s\big{]}\] \[\quad\quad+\mathbb{E}_{\mathcal{M}_{1}}\Big{[}V_{h+1}^{T,\pi}(s_{h+ 1}|\mathcal{M}_{1})-V_{h+1}^{T,\pi}(s_{h+1}|\mathcal{M}_{2})|s_{h}=s\Big{]}\] \[\quad\quad+\mathbb{E}_{\mathcal{M}_{1}}\Big{[}\mathbb{E}_{\bm{s}^{ \prime}\sim P_{h}^{1}(s_{h})}\Big{[}V_{h}^{T,\pi}(s_{h},\bm{s}^{\prime}| \mathcal{M}_{2})\Big{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{2}(s_{h})}\Big{[} V_{h}^{T,\pi}(s_{h},\bm{s}^{\prime}|\mathcal{M}_{2})\Big{]}|s_{h}=s\Big{]}.\]

### Full Algorithm Description for Transition Lookahead

```
1:Require:\(\delta\in(0,1)\), bonuses \(b^{r}_{k,h}(s,a),b^{p}_{k,h}(s)\)
2:for\(k=1,2,...\)do
3: Initialize \(\bar{V}^{k}_{h+1}(s)=0\)
4:for\(h=H,H-1,..,1\)do
5:for\(s\in\mathcal{S}\)do
6:if\(n^{k-1}_{h}(s)=0\)then
7:\(\bar{V}^{k}_{h}(s)=H\)
8:else
9: Calculate the truncated values \[\bar{V}^{k}_{h}(s)=\min\Biggl{\{}\frac{1}{n^{k-1}_{h}(s)}\sum_{t=1}^{n^{k-1}_{ h}(s)}\max_{a\in\mathcal{A}}\Bigl{\{}\hat{r}^{k-1}_{h}(s,a)+b^{r}_{k,h}(s,a)+ \bar{V}^{k}_{h+1}(s^{\prime k^{\prime}_{h}(s)}_{h+1}(s,a))\Bigr{\}}+b^{p}_{k,h }(s),H\Biggr{\}}\]
10:endif
11: For any set of next-states \(\bm{s}^{\prime}\in\mathcal{S}^{A}\), define the policy \(\pi^{k}\) \[\pi^{k}_{h}(s,\bm{s}^{\prime})\in\operatorname*{arg\,max}_{a\in\mathcal{A}} \bigl{\{}\hat{r}^{k-1}_{h}(s,a)+b^{r}_{k,h}(s,a)+\bar{V}^{k}_{h+1}(s^{\prime}(a ))\bigr{\}}\]
12:endfor
13:endfor
14:for\(h=1,2,\ldots H\)do
15: Observe \(s^{k}_{h}\) and \(\bm{s}^{\prime k}_{h+1}=\bigl{\{}s^{\prime k}_{h+1}(s^{k}_{h},a)\bigr{\}}_{a \in\mathcal{A}}\)
16: Play an action \(a^{k}_{h}=\pi^{k}_{h}(s^{k}_{h},\bm{s}^{\prime k}_{h})\)
17: Collect the reward \(R^{k}_{h}\sim\mathcal{R}_{h}(s^{k}_{h},a^{k}_{h})\) and transition to the next state \(s^{k}_{h+1}=s^{\prime k}_{h+1}(s^{k}_{h},a^{k}_{h})\)
18:endfor
19: Update the empirical estimators and counts for all visited state-actions
20:endfor ```

**Algorithm 4** Monotonic Value Propagation with Transition Lookahead (MVP-TL)

As with reward lookahead, we again use a variant of the MVP algorithm (Zhang et al., 2021), described in Algorithm 4. For the bonuses, we use the notation

\[\bar{V}^{k}_{h}(s,\bm{s}^{\prime})=\max_{a\in\mathcal{A}}\bigl{\{}\hat{r}^{k-1 }_{h}(s,a)+b^{r}_{k,h}(s,a)+\bar{V}^{k}_{h+1}(s^{\prime}(a)\bigr{\}}\]

and define the following bonuses:

\[b^{r}_{k,h}(s,a)=\min\Biggl{\{}\sqrt{\frac{L^{k}_{\delta}}{n^{k-1 }_{h}(s,a)\lor 1}},1\Biggr{\}},\] \[b^{p}_{k,h}(s)=\frac{20}{3}\sqrt{\frac{\operatorname*{Var}_{\bm {s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}(\bar{V}^{k}_{h}(s,\bm{s}^{\prime}))L^{k}_ {\delta}}{n^{k-1}_{h}(s)\lor 1}}+\frac{400}{3}\frac{HL^{k}_{\delta}}{n^{k-1}_{h}(s) \lor 1},\]

where \(L^{k}_{\delta}=\ln\frac{168^{3}A^{2}HK^{2}(k+1)}{\delta}\) and

\[\operatorname*{Var}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}(\bar{V}^{k}_{h}(s,\bm{s}^{\prime}))=\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}\bigl{[} \bar{V}^{k}_{h}(s,\bm{s}^{\prime})^{2}\bigr{]}-\Bigl{(}\mathbb{E}_{\bm{s}^{ \prime}\sim\hat{P}^{k-1}_{h}(s)}\bigl{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime}) \bigr{]}\Bigr{)}^{2}.\]

The notation \(k^{t}_{h}(s)\) again represents the \(t^{th}\) episode where the state \(s\) was visited at the \(h^{th}\) timestep; in particular, line 9 of the algorithm is the expectation w.r.t. the empirical reward distribution \(\hat{P}^{k-1}_{h}(s)\). Since the transition bonus is larger than \(H\) when \(n^{k-1}_{h}(s)=0\), we can arbitrarily define the expectation w.r.t. \(\hat{P}^{k-1}_{h}(s)\) when \(n^{k-1}_{h}(s)=0\) to be 0, and one could write the update in a more concise way as

\[\bar{V}^{k}_{h}(s)=\min\Bigl{\{}\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h} (s)}\bigl{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime})\bigr{]}+b^{p}_{k,h}(s),H \Bigr{\}}.\]

### Additional Notations and List Representation

In this subsection, we present additional notations for both values and transition distributions that will be helpful in the analysis. In particular, we show that instead of looking at the distribution over all combinations of next state \(\bm{s}^{\prime}\in\mathcal{S}^{A}\), we can look at a ranking of all the next-state-actions and represent important quantities using the effective distribution on these ranks - this moves the problem from being \(S^{A}\)-dimensional to a dimension of \(SA\).

We start by defining the values starting from state \(s\in\mathcal{S}\), playing \(a\in\mathcal{A}\) and transitioning to \(s^{\prime}\in\mathcal{S}\), denoted by

\[V^{\pi}_{h}(s,s^{\prime},a) =r_{h}(s,a)+V^{\pi}_{h+1}(s^{\prime}),\] \[V^{\pi}_{h}(s,s^{\prime},a) =r_{h}(s,a)+V^{\pi}_{h+1}(s^{\prime}),\] \[\bar{V}^{k}_{h}(s,s^{\prime},a) =\hat{r}^{k-1}_{h}(s,a)+b^{r}_{k,h}(s,a)+\bar{V}^{k}_{h+1}(s^{ \prime}),\]

We similarly define (consistently with Remark 2)

\[V^{\pi}_{h}(s,\bm{s}^{\prime}) =V^{\pi}_{h}(s,s^{\prime}(\pi_{h}(s,\bm{s}^{\prime})),\pi_{h}(s, \bm{s}^{\prime})),\] \[V^{\pi}_{h}(s,\bm{s}^{\prime}) =\max_{a}V^{*}_{h}(s,s^{\prime}(a),a),\qquad\qquad\qquad\text{ and },\] \[\bar{V}^{k}_{h}(s,\bm{s}^{\prime}) =\max_{a}\bar{V}^{k}_{h}(s,s^{\prime}(a),a).\]

**List representation.** We now move to defining lists of next-state-actions and distributions with respect to such lists. Let \(\ell\) be a list that orders all next-state-action pairs from \((s^{\prime}_{\ell(1)},a_{\ell(1)})\) to \((s^{\prime}_{\ell(SA)},a_{\ell(SA)})\) and define the set of all possible lists to be \(\mathcal{L}\) (with \(|\mathcal{L}|=(SA)!\)). Also, define \(\ell^{u}\), the list induced by a function \(u:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) such that \(u(s^{\prime}_{\ell^{u}(1)},a_{\ell^{u}(1)})\geq\cdots\geq u(s^{\prime}_{\ell^ {u}(SA)},a_{\ell^{u}(SA)})\), where ties are broken in any fixed arbitrary way. From this point forward, for brevity and when clear from the context, we omit the list from the indexing, e.g., write the list \(\ell\) by \((s^{\prime}_{1},a_{1}),\ldots,(s^{\prime}_{SA},a_{SA})\).

We now define the probability of list elements. Denote by \(E^{\ell}_{i}\) the event that the highest-ranked realized element in the list is element \(i\), namely

\[E^{\ell}_{i}=\big{\{}\bm{s}^{\prime}\in\mathcal{S}^{A}:s^{\prime}(a_{i})=s^{ \prime}_{i}\;\;\text{and}\;\;\forall j<i,s^{\prime}(a_{j})\neq s^{\prime}_{j} \big{\}}.\] (10)

Then, for a probability measure \(P\) on \(\mathcal{S}^{A}\), define \(\mu(i|\ell,P)=P(\bm{s}^{\prime}\in E^{\ell}_{i})\). Notably, when the list is induced by \(u\) and element \(i\) is the realized highest-ranked elements, we can write \(\max_{a}u(s^{\prime}(a),a)=u(s^{\prime}_{i},a_{i})\), so we have that (e.g. by Lemma 17 with \(f(\bm{s}^{\prime})=\max_{a}u(s^{\prime}(a),a)\))

\[\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\max_{a}\{u(s^{\prime}(a),a) \}\Big{]}=\mathbb{E}_{i\sim\mu(\cdot|\ell,P_{h}(s))}[u(s^{\prime}_{i},a_{i})]\]

We also denote by \(\hat{\mu}^{k}_{h}(i|s;\ell)=\frac{1}{n^{k}_{h}(s)\lor 1}\sum_{t=1}^{K} \mathbbm{1}\big{\{}s^{\prime}_{h}=s,\bm{s}^{\prime\prime}_{h+1}\in E^{\ell}_{ i}\big{\}}\), the empirical probability for a list location \(i\) to be the highest-realized ranking according to a list \(\ell\) at state \(s\) and step \(h\), based on samples up to episode \(k\); We have by Lemma 17 that \(\hat{\mu}^{k}_{h}(i|s;\ell)=\hat{P}^{k}_{h}(E^{\ell}_{i}|s)\) and

\[\mathbb{E}_{\bm{s}^{\prime}\sim P^{k-1}_{h}(s)}\Big{[}\max_{a}\{u(s^{\prime}(a ),a)\}\Big{]}=\mathbb{E}_{i\sim\hat{\mu}^{k-1}_{h}(\cdot|s;\ell^{u})}[u(s^{ \prime}_{i},a_{i})].\]

Similarly, we will require the distribution probability w.r.t. two lists - the probability that the top element w.r.t. list \(\ell\) is \(i\) and the top element w.r.t. list \(\ell^{\prime}\) is \(j\); we denote the real and empirical probability distributions by \(\mu(i,j|\ell,\ell^{\prime},P)\) and \(\hat{\mu}^{k}_{h}(i,j|s;\ell,\ell^{\prime})\), respectively. This allows, for example, using Lemma 17 to write for any \(u,v:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\),

\[\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\max_{a}\{u(s^{ \prime}(a),a)\} -\max_{a}\{v(s^{\prime}(a),a)\}\Big{]}\\ =\mathbb{E}_{i,j\sim\mu(\cdot|\ell^{u},\ell^{v},P_{h}(s))}\Big{[} u(s^{\prime}_{\ell^{u}(i)},a_{\ell^{u}(i)})-v(s^{\prime}_{\ell^{v}(j)},a_{ \ell^{v}(j)})\Big{]},\\ \mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}\Big{[}\max_{a} \{u(s^{\prime}(a),a)\}-\max_{a}\{v(s^{\prime}(a),a)\}\Big{]}\\ =\mathbb{E}_{i,j\sim\hat{\mu}^{k}_{h}(\cdot|s;\ell^{u},\ell^{v})} \Big{[}u(s^{\prime}_{\ell^{u}(i)},a_{\ell^{u}(i)})-v(s^{\prime}_{\ell^{v}(j)},a_ {\ell^{v}(j)})\Big{]}.\] (11)Finally, we say that a policy \(\pi_{h}(s,\bm{s}^{\prime})\) is induced by lists \(\ell_{h}(s)\) if it chooses an action \(a\) such that its next-state \(s^{\prime}(a)\) is ranked higher in \(\ell\) than all other realized next-state-action pairs. In particular, the policy \(\pi^{k}\) and the optimal policy \(\pi^{*}\) (defined in Proposition 2) are such policies w.r.t. the lists \(\bar{\ell}_{h}^{k}(s)\) and \(\bar{\ell}_{h}^{k}(s)\) - induced by \(\bar{V}_{h}^{k}(s,s^{\prime},a)\) and \(V_{h}^{*}(s,s^{\prime},a)\), respectively. As such, for any probability measure \(P_{h}(s)\), function \(u:\mathcal{S}\times\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) and a policy \(\pi\) induced by a list \(\ell\), it holds that

\[\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}[u(s,s^{\prime}(\pi(a)),\pi(a))]= \mathbb{E}_{i\sim\mu(\cdot|\ell_{h}(s),P_{h}(s))}[u(s,s^{\prime}_{i},a_{i})].\] (12)

#### c.4.1 Planning with Transition Lookahead

We have already seen the optimal policy is induced by a list \(\ell_{h}^{*}(s)\), and in particular, we can write the dynamic programming equations of Proposition 2 as

\[V_{h}^{*}(s) =\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\bigg{[}\max_{a\in \mathcal{A}}\Bigl{\{}r_{h}(s,a)+V_{h}^{T,*}(s^{\prime}(a))\Bigr{\}}\bigg{]}\] \[=\mathbb{E}_{i\sim\mu(\cdot|\ell_{h}^{*}(s),P_{h}(s))}\big{[}r_{h }(s,a_{i})+V_{h+1}^{*}(s^{\prime}(a_{i}))\big{]}.\]

Therefore, one way to perform the planning is to build a list \(\ell_{h}^{*}(s)\) of \((s^{\prime},a)\) s.t. the values

\[V_{h}^{*}(s,s^{\prime},a)=r_{h}(s,a)+V_{h+1}^{*}(s^{\prime})\]

are sorted in a non-increasing order and calculate the probability of any pair in the list to be the highest-realized pair:

\[\mu(i|\ell,P_{h}(s))=P_{h}(E_{i}^{\ell})=\Pr\bigl{(}s^{\prime}_{h+1}(a_{i})=s^ {\prime}_{i}\;\;\text{and}\;\;\forall j<i,s^{\prime}_{h+1}(a_{j})\neq s^{ \prime}_{j}|s_{h}=s\bigr{)}.\]

In general, calculating this distribution is intractable, and one must resort to approximating it by sampling (as done in Algorithm 4. Nonetheless, if next states are generated independently between actions, this distribution could be efficiently calculated as follows:

\[\mu(i|\ell,P_{h}(s)) =\Pr\bigl{(}s^{\prime}_{h+1}(a_{i})=s^{\prime}_{i}\;\;\text{and} \;\;\forall j<i,s^{\prime}_{h+1}(a_{j})\neq s^{\prime}_{j}|s_{h}=s\bigr{)}\] \[\stackrel{{(\ref{eq:p_h_1})}}{{=}}\Pr\bigl{\{}s^{ \prime}(a_{i})=s^{\prime}_{i}\;\;\text{and}\;\;\forall j<i\;\text{s.t.}\;a_{j} \neq a_{i},s^{\prime}(a_{j})\neq s^{\prime}_{j}|s_{h}=s\bigr{\}}\] \[\stackrel{{(\ref{eq:p_h_1})}}{{=}}\Pr\bigl{\{}s^{ \prime}(a_{i})=s^{\prime}_{i}|s_{h}=s\bigr{\}}\prod_{a\neq a_{i}}\Pr\bigl{\{} \forall j<i\;\text{s.t.}\;a_{j}=a,s^{\prime}(a)\neq s^{\prime}_{j}|s_{h}=s \bigr{\}}\] \[\stackrel{{(\ref{eq:p_h_1})}}{{=}}P_{h}(s^{\prime}_{ i}|s,a_{i})\prod_{a\neq a_{i}}\left(1-\sum_{j=1}^{i-1}\mathds{1}\{a_{j}=a\}P_{h}(s^{ \prime}_{j}|s,a)\right).\]

Relation \((\ref{eq:p_h_1})\) holds since if \(s^{\prime}(a_{i})=s^{\prime}_{i}\), it cannot get any previous value of the same action in the list, so these events can be removed. Relation \((\ref{eq:p_h_1})\) is by the independence and \((\ref{eq:p_h_1})\) directly calculates the probabilities.

### The First Good Event - Concentration

Next, we define the events that ensure the concentration of all empirical measures. For rewards, an event handles the convergence of the empirical rewards to their mean. For the transitions, we want the Bellman operator, applied on the optimal value with the empirical model, to concentrate well, and we require the variance of values w.r.t. the empirical and real model to be close. Finally, the empirical measure \(\hat{\mu}_{h}^{k}(i,j|s;\ell,\ell_{h}^{*}(s))\) must concentrate well around its mean for any list \(\ell\) - this will allow the change-of-measure argument described in the proof sketch.

Formally, define the following good events:

\[E^{r}(k) =\left\{\forall s,a,h:\;|r_{h}(s,a)-\hat{r}_{h}^{k-1}(s,a)|\leq \sqrt{\frac{L_{\delta}^{k}}{n_{h}^{k-1}(s,a)\lor 1}}\right\}\] \[E^{\ell}(k) =\left\{\forall s,h,\forall\ell\in\mathcal{L},\forall i,j\in[SA]: \;\left|\hat{\mu}_{h}^{k-1}(i,j|s;\ell,\ell_{h}^{*}(s))-\mu(i,j|\ell,\ell_{h}^ {*}(s);P_{h}(s))\right|\right.\] \[\leq\sqrt{\frac{4SAL_{\delta}^{k}\mu(i,j|s;\ell,\ell_{h}^{*}(s);P _{h}(s))}{n_{h}^{k-1}(s)\lor 1}}+\frac{2SAL_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\right\}\] \[E^{pv1}(k) =\left\{\forall s,h:\;\left|\mathbb{E}_{\bm{s}^{\prime}\sim P_{h }(s)}\big{[}V_{h}^{*}(s,\bm{s}^{\prime})\big{]}-\mathbb{E}_{\bm{s}^{\prime} \sim\hat{P}_{h}^{k-1}(s)}\big{[}V_{h}^{*}(s,\bm{s}^{\prime})\big{]}\right|\leq \sqrt{\frac{2\mathrm{Var}_{\bm{s}^{\prime}\sim P_{h}(s)}(V_{h}^{*}(s,\bm{s}^{ \prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}}+\frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s) \lor 1}\right\}\] \[E^{pv2}(k) =\left\{\forall s,h:\;\left|\sqrt{\mathrm{Var}_{\bm{s}^{\prime} \sim P_{h}(s)}(V_{h}^{*}(s,\bm{s}^{\prime}))}-\sqrt{\mathrm{Var}_{\bm{s}^{ \prime}\sim\hat{P}_{h}^{k-1}(s)}(V_{h}^{*}(s,\bm{s}^{\prime}))}\right|\leq 4H \sqrt{\frac{L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}}\right\}\]

where we again use \(L_{\delta}^{k}=\ln\frac{16S^{3}A^{2}Hk^{2}(k+1)}{\delta}\). We define the first good event as

\[\mathbb{G}_{1}=\bigcap_{k\geq 1}E^{r}(k)\bigcap_{k\geq 1}E^{\ell}(k)\bigcap_{k \geq 1}E^{pv1}(k)\bigcap_{k\geq 1}E^{pv2}(k),\]

for which the following holds:

**Lemma 12** (The First Good Event).: _It holds that \(\Pr(\mathbb{G}_{1})\geq 1-\delta/2\)._

Proof.: We prove that each of the events holds w.p. at least \(1-\delta/8\). The result then directly follows by the union bound. We also remark that due to the domain of the variables and their estimators (e.g., \([0,1]\) for the rewards), all bounds trivially hold when the counts equal zero, so w.l.o.g., we only prove the results for cases in which states/state-actions were already previously visited.

**Event \(\cap_{k\geq 1}E^{r}(k)\).** Fix \(k\geq 1,s,a,h\) and visits \(n\geq 1\). Given all of these, the reward observations are i.i.d. random variables supported by \([0,1]\). Denoting the empirical mean based on these \(n\) samples by \(\hat{r}_{h}(s,a,n)\), by Hoeffding's inequality, it holds w.p. \(1-\frac{\delta}{8SAHk^{2}(k+1)}\) that

\[|r_{h}(s,a)-\hat{r}_{h}(s,a,n)|\leq\sqrt{\frac{\ln\frac{16SAHk^{2}(k+1)}{ \delta}}{2n}}\leq\sqrt{\frac{L_{\delta}^{k}}{n}}.\]

Taking the union bound over all \(n\in[k]\) at timestep \(k\), we get that w.p. \(1-\frac{\delta}{8SAHk(k+1)}\)

\[|r_{h}(s,a)-\hat{r}_{h}^{k-1}(s,a)|\leq\sqrt{\frac{L_{\delta}^{k}}{n_{h}^{k-1} (s,a)\lor 1}},\]

and another union bound over all possible values of \(s,a,h\) and \(k\geq 1\) implies that \(\cap_{k\geq 1}E^{r}(k)\) holds w.p. at least \(1-\delta/8\).

**The event \(\cap_{k\geq 1}E^{\ell}(k)\)**. For any fixed \(k\geq 1,s,h\), a list \(\ell\in\mathcal{L}\) and number of visits \(n\in[k]\), we utilize Lemma 16 (event \(E^{p}\)) w.r.t. the distribution \(\mu(i,j|\ell,\ell_{h}^{*}(s),P)\) (whose support is of size \(M=(SA)^{2}\)). When applying the lemma, notice that given the number of visits \(n\geq 1\), the empirical distribution \(\hat{\mu}_{h}^{k-1}(i,j|s;\ell,\ell_{h}^{*}(s))\) is the average of \(n=n_{h}^{k-1}(s)\) i.i.d samples, so that for all \(i,j\in[SA]\),

\[\left|\hat{\mu}_{h}^{k-1}(i,j|s;\ell,\ell_{h}^{*}(s))-\mu(i,j| \ell,\ell_{h}^{*}(s);P_{h}(s))\right| \leq\sqrt{\frac{2\mu(i,j|\ell,\ell_{h}^{*}(s);P_{h}(s))\ln\frac{2( SA)^{2}}{\delta^{\prime}}}{n}}+\frac{2\ln\frac{2(SA)^{2}}{\delta^{\prime}}}{3n}\] \[\leq\sqrt{\frac{4\mu(i,j|\ell,\ell_{h}^{*}(s);P_{h}(s))\ln\frac{2 SA}{\delta^{\prime}}}{n}}+\frac{2\ln\frac{2SA}{\delta^{\prime}}}{n}\]w.p. \(1-\delta^{\prime}\). Choosing \(\delta^{\prime}=\frac{\delta}{8|\mathcal{L}|SH^{2}(k+1)}\) (such that \(\ln\frac{2SA}{\delta^{\prime}}\leq SA\ln\frac{16S^{3}A^{2}Hk^{2}(k+1)}{\delta}\) since \(|\mathcal{L}|\leq(SA)^{SA}\)), while taking the union bound on all \(n\in[k]\), all \(s,h\) and all lists \(\ell\in\mathcal{L}\) implies that \(\cap_{k\geq 1}E^{\ell}(k)\) holds w.p. at least \(1-\frac{\delta}{8}\).

**Events \(\cap_{k\geq 1}E^{pv1}(k)\) and \(\cap_{k\geq 1}E^{pv2}(k)\).** We repeat the arguments stated in Lemma 5. For any fixed \(k\geq 1,s,h\) and number of visits \(n\in[k]\), we utilize Lemma 16 w.r.t. the next-state distribution for all actions \(P_{h}(s)\), the value \(V_{h}^{*}(s,\bm{s}^{\prime})\in[0,H]\) and probability \(\delta^{\prime}=\frac{\delta}{8SH^{k}(k+1)}\); we yet again remind that given the number of visits, samples are i.i.d.

As before, the events \(\cap_{k\geq 1}E^{pv1}(k)\) and \(\cap_{k\geq 1}E^{pv2}(k)\) hold w.p. at least \(1-\frac{\delta}{8}\) through the union bound first on \(n\in[k]\) (to get the empirical quantities) and then on \(s,h\) and \(k\geq 1\). This proves that each of the events in \(\mathbb{G}_{1}\) holds w.p. at least \(1-\frac{\delta}{8}\), so \(\mathbb{G}_{1}\) holds w.p. at least \(1-\frac{\delta}{2}\).

### Optimism of the Upper Confidence Value Functions

We now prove that under the event \(\mathbb{G}_{1}\), the values that MVP-TL outputs are optimistic.

**Lemma 13** (Optimism).: _Under the first good event \(\mathbb{G}_{1}\), for all \(k\in[K]\), \(h\in[H]\), \(a\in\mathcal{A}\) and \(s,s^{\prime}\in\mathcal{S}\), it holds that \(V^{*}_{h}(s,s^{\prime},a)\leq\bar{V}^{k}_{h}(s,s^{\prime},a)\). Moreover, for all \(\bm{s}^{\prime}\in\mathcal{S}^{A}\), \(V^{*}_{h}(s,\bm{s}^{\prime})\leq\bar{V}^{k}_{h}(s,\bm{s}^{\prime})\) and also \(V^{*}_{h}(s)\leq\bar{V}^{k}_{h}(s)\)._

Proof.: The proof of all claims follows by backward induction on \(H\); the base case naturally holds for \(h=H+1\), where all values are defined to be zero.

Assume by induction that for some \(k\in[K]\) and \(h\in[H]\), the inequality \(V^{*}_{h+1}(s)\leq\bar{V}^{k}_{h+1}(s)\) holds for all \(s\in\mathcal{S}\); we will show that this implies that all stated inequalities also hold at timestep \(h\). At this point, we also assume w.l.o.g. that \(\bar{V}^{k}_{h}(s)<H\) (namely, not truncated), since otherwise, by the boundedness of the rewards, \(V^{*}_{h}(s)\leq H=\bar{V}^{k}_{h}(s)\). In particular, under the good event \(E^{r}(k)\), for all \(s\) and \(a\), it holds that \(\hat{r}^{k-1}_{h}(s,a)+b^{r}_{h,h}(s,a)\geq r_{h}(s,a)\), so for all \(s,a\) and \(s^{\prime}\), we have

\[\bar{V}^{k}_{h}(s,s^{\prime},a)=\hat{r}^{k-1}_{h}(s,a)+b^{r}_{k,h}(s,a)+\bar{ V}^{k}_{h+1}(s^{\prime})\geq r_{h}(s,a)+V^{*}_{h+1}(s^{\prime})=V^{*}_{h}(s,s^{ \prime},a).\]

where the inequality also uses the induction hypothesis. This proves the first part of the lemma. Moreover, it implies that

\[\bar{V}^{k}_{h}(s,\bm{s}^{\prime})=\max_{a\in\mathcal{A}}\{\bar{V}^{k}_{h}(s, s^{\prime}(a),a)\}\geq\max_{a\in\mathcal{A}}\{V^{*}_{h}(s,s^{\prime}(a),a)\}=V^{*}_ {h}(s,\bm{s}^{\prime}),\] (13)

and proves the second part of the statement.

To prove the last claim of the lemma, we use the monotonicity of the bonus, relying on Lemma 23. This lemma can be used when applied to the empirical distribution of all possible next-states \(\hat{P}^{k-1}_{h}(s)\); indeed, the non-truncated optimistic value can be written as

\[\bar{V}^{k}_{h}(s) =\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}\bigg{[} \max_{a\in\mathcal{A}}\{\hat{r}^{k-1}_{h}(s,a)+b^{r}_{k,h}(s,a)+\bar{V}^{k}_{ h+1}(s^{\prime}(a))\}\bigg{]}+b^{p}_{k,h}(s)\] \[\geq\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}[\bar{V}^ {k}_{h}(s,\bm{s}^{\prime})]+\max\Biggl{\{}\frac{20}{3}\sqrt{\frac{\text{Var}_{ \bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}(\bar{V}^{k}_{h}(s,\bm{s}^{\prime}))L ^{k}_{\delta}}{n^{k-1}_{h}(s)\lor 1}},\frac{400}{9}\frac{3HL^{k}_{\delta}}{n^{k-1}_{h}(s) \lor 1}\Biggr{\}},\]

which is exactly the required form in Lemma 23, w.r.t. the distribution \(\hat{P}^{k-1}_{h}(s)\) and the values \(\bar{V}^{k}_{h}(s,\bm{s}^{\prime})\) (while noticing that due to the truncation of the values and bonuses, \(\bar{V}^{k}_{h}(s,\bm{s}^{\prime})\in[0,3H]\)). Thus, the lemma guarantees monotonicity in the value, so by Equation (13),

\[\bar{V}^{k}_{h}(s) \geq\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}\big{[}V^ {*}_{h}(s,\bm{s}^{\prime})\big{]}+\max\Biggl{\{}\frac{20}{3}\sqrt{\frac{\text{ Var}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}(V^{*}_{h}(s,\bm{s}^{\prime}))L ^{k}_{\delta}}{n^{k-1}_{h}(s)\lor 1}},\frac{400}{9}\frac{3HL^{k}_{\delta}}{n^{k-1}_{h}(s) \lor 1}\Biggr{\}}\] \[\geq\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}\big{[}V^ {*}_{h}(s,\bm{s}^{\prime})\big{]}+\frac{10}{3}\sqrt{\frac{\text{Var}_{\bm{s}^{ \prime}\sim\hat{P}^{k-1}_{h}(s)}(V^{*}_{h}(s,\bm{s}^{\prime}))L^{k}_{\delta}}{ n^{k-1}_{h}(s)\lor 1}}+\frac{200}{3}\frac{HL^{k}_{\delta}}{n^{k-1}_{h}(s)\lor 1}\] \[\geq\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h}(s)}\big{[}V^ {*}_{h}(s,\bm{s}^{\prime})\big{]}+\frac{10}{3}\sqrt{\frac{\text{Var}_{\bm{s}^{ \prime}\sim\hat{P}^{k}_{h}(s)}(V^{*}_{h}(s,\bm{s}^{\prime}))L^{k}_{\delta}}{n^{ k-1}_{h}(s)\lor 1}}+\frac{50HL^{k}_{\delta}}{n^{k-1}_{h}(s)\lor 1}\quad\text{(Under $E^{pv^{2}}(k)$)}\] \[\geq\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}_{h}(s)}\big{[}V^{*}_{h} (s,\bm{s}^{\prime})\big{]}\] (Under $E^{pv^{1}}(k)$) \[=V^{*}_{h}(s).\qed\]

### The Second Good Event - Martingale Concentration

In this subsection, we present three good events that allow replacing the expectation over the randomizations inside each episode by their realization. Let

\[Y^{k}_{1,h}:=\bar{V}^{k}_{h+1}(s^{k}_{h+1})-V^{\pi^{k}}_{h+1}(s^{k} _{h+1})\] \[Y^{k}_{2,h}=\operatorname{Var}_{\bm{s}^{\prime}\sim P_{h}(s^{k}_ {h})}(V^{\pi^{k}}_{h}(s^{k}_{h},\bm{s}^{\prime}))\] \[Y^{k}_{3,h}=b^{r}_{k,h}(s^{k}_{h},a^{k}_{h}).\]

The second good event is the intersection of the events \(\mathbb{G}_{2}=E^{\mathrm{diff}}\cap E^{\mathrm{Var}}\cap E^{br}\) defined as follows.

\[E^{\mathrm{diff}}=\Bigg{\{}\forall h\in[H],K\geq 1:\; \sum_{k=1}^{K}\mathbb{E}[Y^{k}_{1,h}|F_{k,h-1}]\leq\left(1+\frac{1}{2H} \right)\sum_{k=1}^{K}Y^{k}_{1,h}+18H^{2}\ln\frac{6HK(K+1)}{\delta}\Bigg{\}},\] \[E^{\mathrm{Var}}=\Bigg{\{}K\geq 1:\;\sum_{k=1}^{K}\sum_{h=1}^{H}Y^{ k}_{2,h}\leq 2\sum_{k=1}^{K}\sum_{h=1}^{H}\mathbb{E}[Y^{k}_{2,h}|F_{k-1}]+4H^{3} \ln\frac{6HK(K+1)}{\delta}\Bigg{\}},\] \[E^{br}=\Bigg{\{}\forall h\in[H],K\geq 1:\;\sum_{k=1}^{K} \mathbb{E}[Y^{k}_{3,h}|F_{k,h-1}]\leq 2\sum_{k=1}^{K}Y^{k}_{3,h}+18\ln\frac{6 HK(K+1)}{\delta}\Bigg{\}},\]

We define the good event \(\mathbb{G}=\mathbb{G}_{1}\cap\mathbb{G}_{2}\).

**Lemma 14**.: _The good event \(\mathbb{G}\) holds with a probability of at least \(1-\delta\)._

Proof.: The analysis of the first event follows \(E^{\mathrm{diff}}\) exactly as the one of \(E^{\mathrm{diff}1}\) in Lemma 7: define \(W_{k}=\mathds{1}\big{\{}\bar{V}^{k}_{h}(s)-V^{\pi^{k}}_{h}(s)\in[0,H],\forall h \in[H],s\in\mathcal{S}\big{\}}\) (which happens a.s. under \(\mathbb{G}_{1}\) due to the optimism in Lemma 13 and truncation) and \(\bar{Y}^{k}_{1,h}=W_{k}Y^{k}_{1,h}\), which is bounded in \([0,H]\) and \(F_{k,h}\)-measurable. The corresponding event w.r.t. this modified variables \(\tilde{E}^{\mathrm{diff}}\) then holds w.p. \(1-\frac{\delta}{6}\) by Lemma 25, and as in Lemma 7, we can use the fact that \(\mathbb{G}_{1}\cap\tilde{E}^{\mathrm{diff}}=\mathbb{G}_{1}\cap E^{\mathrm{diff}}\) to conclude this part of the proof.

Moving to the second event, since \(V^{\pi^{k}}_{h}(s,\bm{s}^{\prime})\in[0,H]\), then \(\sum_{h=1}^{H}Y^{k}_{2,h}\in[0,H^{3}]\). Therefore, by Lemma 25 (w.r.t. the filtration \(F_{k}\)) with \(C=H^{3}\) and any fixed \(K\), we get w.p. \(1-\frac{\delta}{6HK(K+1)}\) that

\[\sum_{k=1}^{K}\sum_{h=1}^{H}Y^{k}_{2,h}\leq 2\sum_{k=1}^{K}\sum_{h=1}^{H} \mathbb{E}[Y^{k}_{2,h}|F_{k-1}]+4H^{3}\ln\frac{6HK(K+1)}{\delta}.\]

Taking the union bound on all possible values of \(K\geq 1\) proves that \(E^{\mathrm{Var}}\) holds w.p. at least \(1-\frac{\delta}{6}\).

Finally, by definition, we have that \(Y^{k}_{3,h}=b^{r}_{k,h}(s^{k}_{h},a^{k}_{h})\in[0,1]\) and is \(F_{k,h}\)-measurable. Thus, for any fixed \(k\geq 1\) and \(h\in[H]\), using Lemma 25, we have w.p. \(1-\frac{\delta}{6HK(K+1)}\) that

\[\sum_{k=1}^{K}\mathbb{E}[Y^{k}_{3,h}|F_{k,h-1}]\leq\left(1+\frac{1}{2}\right) \sum_{k=1}^{K}Y^{k}_{3,h}+18\ln\frac{6HK(K+1)}{\delta}\leq 2\sum_{k=1}^{K}Y^{k}_{3,h }+18\ln\frac{6HK(K+1)}{\delta},\]

so that due to the union bound, \(E^{br}\) holds w.p. \(1-\frac{\delta}{6}\).

To conclude, \(\mathbb{G}_{1}\) holds w.p. \(1-\frac{\delta}{2}\) (Lemma 5) and the events \(\tilde{E}^{\mathrm{diff}},E^{\mathrm{Var}},E^{br}\) each hold w.p. \(1-\frac{\delta}{6}\). As before, when accounting to the fact that \(\tilde{E}^{\mathrm{diff}}\) and \(E^{\mathrm{diff}}\) are identical under \(\mathbb{G}_{1}\), the event \(G=\mathbb{G}_{1}\cap\mathbb{G}_{2}\) holds w.p. at least \(1-\delta\).

### Regret Analysis

**Theorem 2**.: _When running MVP-TL, with probability at least \(1-\delta\) uniformly for all \(K\geq 1\), it holds that \(\operatorname{Reg}^{T}(K)\leq\mathcal{O}\Big{(}\sqrt{H^{2}SK}\Big{(}\sqrt{H}+ \sqrt{A}\Big{)}\ln\frac{SAHK}{\delta}+H^{3}S^{4}A^{3}\big{(}\ln\frac{SAHK}{ \delta}\big{)}^{2}\Big{)}\)._

Proof.: Assume that the event \(\mathbb{G}\) holds, which by Lemma 14, happens with probability at least \(1-\delta\). In particular, throughout the proof, we use optimism (Lemma 13), which implies that \(0\leq V_{h}^{\pi^{k}}(s,\bm{s}^{\prime})\leq V_{h}^{*}(s,\bm{s}^{\prime})\leq \bar{V}_{h}^{k}(s,\bm{s}^{\prime})\leq 3H\) (the upper bound is also by the truncation), as well as \(0\leq V_{h}^{\pi^{k}}(s)\leq V_{h}^{*}(s)\leq\bar{V}_{h}^{k}(s)\leq H\).

We first focus on lower-bounding the value of the policy \(\pi^{k}\): by Remark 2, we have

\[V_{h}^{\pi^{k}}(s) =\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}r_{h}(s,\pi_{h} ^{k}(s,\bm{s}^{\prime}))+V_{h+1}^{\pi^{k}}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{ \prime})))\Big{]}\] \[=\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\hat{r}_{h}^{k-1 }(s,\pi_{h}^{k}(s,\bm{s}))+\bar{V}_{h+1}^{k}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{ \prime})))+b_{k,h}^{r}(s,\pi_{h}^{k}(s,\bm{s}^{\prime}))\Big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}r_{h}(s,\pi _{h}^{k}(s,\bm{s}^{\prime}))-\hat{r}_{h}^{k-1}(s,\pi_{h}^{k}(s,\bm{s}^{\prime} ))-b_{k,h}^{r}(s,\pi_{h}^{k}(s,\bm{s}^{\prime}))\Big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}V_{h+1}^{ \pi^{k}}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))-\bar{V}_{h+1}^{k}(s^{ \prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))\Big{]}\] \[\overset{(1)}{=}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[} \max_{a\in\mathcal{A}}\{\bar{r}_{h}^{k-1}(s,a)+\bar{V}_{h+1}^{k}(s^{\prime}(a) )+b_{k,h}^{r}(s,a)\}\Big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[}r_{h}(s,\pi _{h}^{k}(s,\bm{s}^{\prime}))-\hat{r}_{h}^{k-1}(s,\pi_{h}^{k}(s,\bm{s}))-b_{k,h} ^{r}(s,\pi_{h}^{k}(s,\bm{s}^{\prime}))\big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}V_{h+1}^{ \pi^{k}}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))-\bar{V}_{h+1}^{k}(s^{ \prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))\Big{]}\] \[\overset{(2)}{\geq}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[} \bar{V}_{h}^{k}(s,\bm{s}^{\prime})\big{]}-2\mathbb{E}_{\bm{s}^{\prime}\sim P_{h }(s)}\big{[}b_{k,h}^{r}(s,\pi_{h}^{k}(s,\bm{s}^{\prime}))\big{]}\] \[\quad-\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\bar{V}_{h+ 1}^{k}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))-V_{h+1}^{\pi^{k}}(s^{ \prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))\Big{]}\]

where \((1)\) is by the definition of \(\pi^{k}\) and \((2)\) uses the reward concentration event. Thus, we can write

\[\bar{V}_{h}^{k}(s)-V_{h}^{\pi^{k}}(s) \leq\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{k-1}(s)}\big{[}\bar{ V}_{h}^{k}(s,\bm{s}^{\prime})\big{]}-\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[} \bar{V}_{h}^{k}(s,\bm{s}^{\prime})\big{]}+2\mathbb{E}_{\bm{s}^{\prime}\sim P_{h }(s)}\big{[}b_{k,h}^{r}(s,\pi_{h}^{k}(s,\bm{s}^{\prime}))\big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\bar{V}_{h+ 1}^{k}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))-V_{h+1}^{\pi^{k}}(s^{ \prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))\Big{]}+b_{k,h}^{p}(s)\] \[=\underbrace{\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{k-1}(s)} \big{[}\bar{V}_{h}^{k}(s,\bm{s}^{\prime})-V_{h}^{*}(s,\bm{s}^{\prime})\big{]}- \mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[}\bar{V}_{h}^{k}(s,\bm{s}^{ \prime})-V_{h}^{*}(s,\bm{s}^{\prime})\big{]}+b_{k,h}^{p}(s)}_{(i)}\] \[\quad+\underbrace{\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}[V_{h}^ {*}(s,\bm{s}^{\prime})]-\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}^{k-1}(s)}[V_{h}^ {*}(s,\bm{s}^{\prime})]}_{(ii)}+2\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)} \big{[}b_{k,h}^{r}(s,\pi_{h}^{k}(s,\bm{s}^{\prime}))\big{]}\] \[\quad+\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}\bar{V}_{h+ 1}^{k}(s^{\prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))-V_{h+1}^{\pi^{k}}(s^{ \prime}(\pi_{h}^{k}(s,\bm{s}^{\prime})))\Big{]}\] (14)

**Bounding term \((ii)\): using the concentration event \(E^{pv1}(k)\), we have**

\[(ii) \leq\sqrt{\frac{2\mathrm{Var}_{\bm{s}^{\prime}\sim P_{h}(s)}(V_{h} ^{*}(s,\bm{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}}+\frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\] \[\overset{(1)}{\leq}\sqrt{\frac{2\mathrm{Var}_{\bm{s}^{\prime}\sim P _{h}(s)}(V_{h}^{\pi^{k}}(s,\bm{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s) \lor 1}}+\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[}V_{h}^{ \pi^{k}}(s,\bm{s}^{\prime})-V_{h}^{\pi_{k}}(s,\bm{s}^{\prime})\Big{]}+\frac{4H^ {2}L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}+\frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\] \[\overset{(2)}{\leq}\sqrt{\frac{2\mathrm{Var}_{\bm{s}^{\prime}\sim P _{h}(s)}(V_{h}^{\pi^{k}}(s,\bm{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s) \lor 1}}+\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[}\bar{V}_{h}^{k}(s,\bm{s}^{\prime})-V_{h}^{\pi_{k}}(s,\bm{s}^{\prime})\big{

**Bounding term \((i)\):** We first focus on the transition bonus; to bound it, we apply Lemma 22 w.r.t. \(\hat{P}^{k-1}_{h}(\bm{s}^{\prime}|s),P_{h}(\bm{s}^{\prime}|s)\), the values \(0\leq V^{\pi^{k}_{h}}_{h}(s,\bm{s}^{\prime})\leq V^{*}_{h}(s,\bm{s}^{\prime}) \leq\bar{V}^{k}_{h}(s,\bm{s}^{\prime})\leq 3H\) (by optimism), under the event \(E^{p\nu 2}(k)\) and with \(\alpha=8H\cdot\frac{20}{3}\sqrt{L_{\delta}^{k}}\):

\[b^{p}_{k,h}(s) =\frac{20}{3}\sqrt{\frac{\operatorname{Var}_{\bm{s}^{\prime} \sim\hat{P}^{k-1}_{h}(s)}(\bar{V}^{k}_{h}(s,\bm{s}^{\prime}))L_{\delta}^{k}}{n _{h}^{k-1}(s)\lor 1}}+\frac{400}{3}\frac{HL_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\] \[\leq\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim\hat{P}^{k-1}_{h} (s)}\big{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{*}_{h}(s,\bm{s}^{\prime}) \big{]}+\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}[V^{*}_{h}(s,\bm {s}^{\prime})-V^{*}_{h}(s,\bm{s}^{\prime})]\] \[\leq\frac{1}{8H}\Big{(}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)} \big{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{*}_{h}(s,\bm{s}^{\prime})\big{]}-E _{\bm{s}^{\prime}\sim P_{h}(s)}\big{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{* }_{h}(s,\bm{s}^{\prime})\big{]}\Big{)}\] \[\quad+\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[} \bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{*_{k}}_{h}(s,\bm{s}^{\prime})\big{]}+ \frac{20}{3}\sqrt{\frac{\operatorname{Var}_{\bm{s}^{\prime}\sim P_{h}(s)}(V^{ \pi^{k}}_{h}(s,\bm{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}}+\frac{700H^{2}}{ n_{h}^{k-1}(s)\lor 1}.\]

Substituting back to term \((i)\), we now have

\[(i)\] \[\quad+\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[} \bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{\pi_{k}}_{h}(s,\bm{s}^{\prime})\big{]}+ \frac{20}{3}\sqrt{\frac{\operatorname{Var}_{\bm{s}^{\prime}\sim P_{h}(s)}(V^{ \pi^{k}}_{h}(s,\bm{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}}+\frac{700H^{2}L_{ \delta}^{k}}{n_{h}^{k-1}(s)\lor 1}.\]

The next step in the proof involves bounding the first term of \((i)\). At this point, we remind that both values can be written as \(\bar{V}^{k}_{h}(s,\bm{s}^{\prime})=\max_{a}\bar{V}^{k}_{h}(s,s^{\prime}(a),a)\) and \(V^{*}_{h}(s,\bm{s}^{\prime})=\max_{a}V^{*}_{h}(s,s^{\prime}(a),a)\), inducing the lists \(\bar{\ell}=\bar{\ell}^{k}_{h}(s)\) and \(\ell^{*}=\ell^{*}_{h}(s)\), respectively; thus the expectations can be written as (see Appendix C.4 for further details on the list representation, and in particular, Equation (11)):

\[\mathbb{E}_{\bm{s}^{\prime}\sim P^{k-1}_{h}(s)}\big{[}\bar{V}^{k} _{h}(s,\bm{s}^{\prime})-V^{*}_{h}(s,\bm{s}^{\prime})\big{]}-\mathbb{E}_{\bm{s} ^{\prime}\sim P_{h}(s)}\big{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{*}_{h}(s, \bm{s}^{\prime})\big{]}\] \[\overset{(1)}{=}\mathbb{E}_{i,j\sim\hat{\mu}^{k}_{h}(\cdot|s,\bar {\ell},\ell^{*})}\Big{[}\bar{V}^{k}_{h}(s,s^{\prime}_{\bar{\ell}(i)},a_{\bar {\ell}(i)})-V^{*}_{h}(s,s^{\prime}_{\ell^{*}(j)},a_{\ell^{*}(j)})\Big{]}\] \[\quad-\mathbb{E}_{i,j\sim\mu\big{(}\cdot|\bar{\ell},\ell^{*},P_{h }(s)\big{)}}\Big{[}\bar{V}^{k}_{h}(s,s^{\prime}_{\bar{\ell}(i)},a_{\bar{\ell} (i)})-V^{*}_{h}(s,s^{\prime}_{\ell^{*}(j)},a_{\ell^{*}(j)})\Big{]}\] \[\overset{(2)}{\leq}\frac{1}{8H}\mathbb{E}_{i,j\sim\mu\big{(} \cdot|\bar{\ell},\ell^{*},P_{h}(s)\big{)}}\Big{[}\bar{V}^{k}_{h}(s,s^{\prime}_{ \bar{\ell}(i)},a_{\bar{\ell}(i)})-V^{*}_{h}(s,s^{\prime}_{\ell^{*}(j)},a_{\ell ^{*}(j)})\Big{]}+\frac{3H(SA)^{2}L_{\delta}^{k}(2SA+8H\cdot 4SA/4)}{n_{h}^{k-1}(s)\lor 1}\] \[\overset{(1)}{\leq}\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{ h}(s)}\big{[}\bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{*}_{h}(s,\bm{s}^{\prime}) \big{]}+\frac{30H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\] \[\leq\frac{1}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\Big{[} \bar{V}^{k}_{h}(s,\bm{s}^{\prime})-V^{\pi^{k}}_{h}(s,\bm{s}^{\prime})\Big{]}+ \frac{30H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\]

Relations \((1)\) formulate the expectation using the list representations and backward, as done in Equation (11). For inequality \((2)\) we rely on Lemma 24 with \(\alpha=8H\) under the event \(E^{\ell}(k)\) and the optimism, which ensures that the value difference is bounded in \([0,3H]\). We also remark that the support of the distributions is of size \((SA)^{2}\); were we to use the same result on the distributions \(\hat{P}^{k-1}_{h}(s)\) and \(P_{h}(s)\), the support would be of size \(S^{A}\), which would lead to an exponential additive factor. And so, we finally have a bound of

\[(i)\leq\frac{3}{8H}\mathbb{E}_{\bm{s}^{\prime}\sim P_{h}(s)}\big{[}\bar{V}^{k}_{h} (s,\bm{s}^{\prime})-V^{\pi_{k}}_{h}(s,\bm{s}^{\prime})\big{]}+\frac{20}{3} \sqrt{\frac{\operatorname{Var}_{\bm{s}^{\prime}\sim P_{h}(s)}(V^{\pi^{k}}_{h}(s, \bm{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}}+\frac{735H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}.\] (16)Combining both terms.Substituting this and Equation (15) into Equation (14), we have

\[\bar{V}_{h}^{k}(s)-V_{h}^{\pi^{k}}(s) \leq\frac{1}{2H}\mathbb{E}_{\boldsymbol{s}^{\prime}\sim P_{h}(s)} \big{[}\bar{V}_{h}^{k}(s,\boldsymbol{s}^{\prime})-V_{h}^{\pi_{k}}(s, \boldsymbol{s}^{\prime})\big{]}+9\sqrt{\frac{\operatorname{Var}_{\boldsymbol{s} ^{\prime}\sim P_{h}(s)}(V_{h}^{\pi^{k}}(s,\boldsymbol{s}^{\prime}))L_{\delta}^ {k}}{n_{h}^{k-1}(s)\lor 1}}+\frac{750H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}\]

and further bounding (using the concentration event \(E^{r}(k)\)

\[\bar{V}_{h}^{k}(s,\boldsymbol{s}^{\prime}))-V_{h}^{\pi^{k}}(s, \boldsymbol{s}^{\prime}) =\dot{r}_{h}^{k-1}(s,\pi_{h}^{k}(s,\boldsymbol{s}^{\prime}))+b_{ k,h}^{r}(s,\pi_{h}^{k}(s,\boldsymbol{s}^{\prime}))+\bar{V}_{h+1}^{k}(s^{ \prime}(\pi_{h}^{k}(s,\boldsymbol{s}^{\prime})))\] \[\quad-r_{h}^{k-1}(s,\pi_{h}^{k}(s,\boldsymbol{s}^{\prime}))-V_{h+ 1}^{\pi^{k}}(s^{\prime}(\pi_{h}^{k}(s,\boldsymbol{s}^{\prime})))\] \[\leq\bar{V}_{h+1}^{k}(s^{\prime}(\pi_{h}^{k}(s,\boldsymbol{s}^{ \prime})))-V_{h+1}^{\pi^{k}}(s^{\prime}(\pi_{h}^{k}(s,\boldsymbol{s}^{\prime} )))+2b_{k,h}^{r}(s,\pi_{h}^{k}(s,\boldsymbol{s}^{\prime})),\]

we finally get the decomposition

\[\bar{V}_{h}^{k}(s)-V_{h}^{\pi^{k}}(s) \leq\bigg{(}1+\frac{1}{2H}\bigg{)}\mathbb{E}_{\boldsymbol{s}^{ \prime}\sim P_{h}(s)}\Big{[}\bar{V}_{h+1}^{k}(s^{\prime}(\pi_{h}^{k}(s, \boldsymbol{s}^{\prime})))-V_{h+1}^{\pi^{k}}(s^{\prime}(\pi_{h}^{k}(s, \boldsymbol{s}^{\prime})))\Big{]}\] \[\quad+9\sqrt{\frac{\operatorname{Var}_{\boldsymbol{s}^{\prime} \sim P_{h}(s)}(V_{h}^{\pi^{k}}(s,\boldsymbol{s}^{\prime}))L_{\delta}^{k}}{n_ {h}^{k-1}(s)\lor 1}}+\frac{750H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{k-1}(s)\lor 1}+3 \mathbb{E}_{\boldsymbol{s}^{\prime}\sim P_{h}(s)}\big{[}b_{k,h}^{r}(s,\pi_{h} ^{k}(s,\boldsymbol{s}^{\prime}))\big{]}.\]

At this point, we choose to take \(s=s_{h}^{k}\) and sum over all \(k\in[K]\); specifically, for \(\boldsymbol{s}^{\prime}=\boldsymbol{s}_{h+1}^{\prime k}\), the action becomes \(\pi_{h}^{k}(s,\boldsymbol{s}^{\prime})=a_{h}^{k}\) and \(s^{\prime}(\pi_{h}^{k}(s,\boldsymbol{s}^{\prime}))=s_{h+1}^{k}\). Formally, we can write the bound as

\[\sum_{k=1}^{K}\bar{V}_{h}^{k}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}) \leq\bigg{(}1+\frac{1}{2H}\bigg{)}\sum_{k=1}^{K}\mathbb{E}\Big{[} \bar{V}_{h+1}^{k}(s_{h+1}^{k})-V_{h+1}^{\pi^{k}}(s_{h+1}^{k})|F_{k,h-1}\Big{]}\] \[\quad+3\sum_{k=1}^{K}\mathbb{E}\big{[}b_{k,h}^{r}(s_{h}^{k},a_{h} ^{k})|F_{k,h-1}\big{]}+9\sum_{k=1}^{K}\sqrt{\frac{\operatorname{Var}_{ \boldsymbol{s}^{\prime}\sim P_{h}(s_{h}^{k})}(V_{h}^{\pi^{k}}(s_{h}^{k}, \boldsymbol{s}^{\prime}))L_{\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k})\lor 1}}\] \[\quad+\sum_{k=1}^{K}\frac{750H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^ {k-1}(s_{h}^{k})\lor 1}.\]

and, in particular, under the events \(E^{\mathrm{diff}}\) and \(E^{br}\), it holds that

\[\sum_{k=1}^{K}\bar{V}_{h}^{k}(s_{h}^{k})-V_{h}^{\pi^{k}}(s_{h}^{k}) \leq\bigg{(}1+\frac{1}{2H}\bigg{)}^{2}\sum_{k=1}^{K}\Bigl{(}\bar{ V}_{h+1}^{k}(s_{h+1}^{k}))-V_{h+1}^{\pi^{k}}(s_{h+1}^{k})\Bigr{)}+36H^{2}\ln \frac{6HK(K+1)}{\delta}\] \[\quad+3\sum_{k=1}^{K}b_{k,h}^{r}(s_{h}^{k},a_{h}^{k})+54\ln\frac{6 HK(K+1)}{\delta}\] \[\quad+9\sum_{k=1}^{K}\sqrt{\frac{\operatorname{Var}_{\boldsymbol{ s}^{\prime}\sim P_{h}(s_{h}^{k})}(V_{h}^{\pi^{k}}(s_{h}^{k},\boldsymbol{s}^{\prime}))L_{ \delta}^{k}}{n_{h}^{k-1}(s_{h}^{k})\lor 1}}+\sum_{k=1}^{K}\frac{750H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{ k-1}(s_{h}^{k})\lor 1}.\]To conclude the proof, we recursively apply this formula from \(h=1\) to \(h=H+1\) (where the values are zero) and use the optimism. This yields

\[\begin{split}\operatorname{Reg}^{T}(K)&=\sum_{k=1}^{K} V_{1}^{*}(s_{h}^{k})-V_{1}^{\tau^{k}}(s_{h}^{k})\\ &\leq\sum_{k=1}^{K}\bar{V}_{1}^{k}(s_{h}^{k})-V_{1}^{\tau^{k}}(s_ {h}^{k})\\ &\stackrel{{(1)}}{{\leq}}9\bigg{(}1+\frac{1}{2H} \bigg{)}^{2H}\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{\sqrt{\operatorname{Var}_{ \boldsymbol{s}^{\prime}\sim P_{h}(s_{h}^{k})}(V_{h}^{\pi^{k}}(s_{h}^{k}, \boldsymbol{s}^{\prime}))L_{\delta}^{k}}}{\sqrt{n_{h}^{k-1}(s_{h}^{k})\lor 1}} \\ &\quad+3\bigg{(}1+\frac{1}{2H}\bigg{)}^{2H}\sum_{k=1}^{K}\sum_{h =1}^{H}\sqrt{\frac{L_{\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k},a_{h}^{k})\lor 1}} \\ &\quad+\bigg{(}1+\frac{1}{2H}\bigg{)}^{2H}\sum_{k=1}^{K}\sum_{h =1}^{H}\frac{750H^{2}(SA)^{3}L_{\delta}^{k}}{n_{h}^{k-1}(s_{h}^{k})\lor 1}+90H^{3} \bigg{(}1+\frac{1}{2H}\bigg{)}^{2H}\ln\frac{6HK(K+1)}{\delta}\\ &\stackrel{{(2)}}{{\leq}}50\sqrt{H^{3}SK}L_{\delta}^ {K}+50\sqrt{2S}H^{2}\big{(}L_{\delta}^{K}\big{)}^{1.5}\\ &\quad+9\sqrt{L_{\delta}^{K}}\Big{(}SAH+2\sqrt{SAH^{2}K}\Big{)} +2050H^{3}S^{4}A^{3}L_{\delta}^{K}(2+\ln(K))+250H^{3}L_{\delta}^{K}\\ &=\mathcal{O}\Big{(}\sqrt{H^{2}SK}\Big{(}\sqrt{H}+\sqrt{A}\Big{)} L_{\delta}^{K}+H^{3}S^{4}A^{3}\big{(}L_{\delta}^{K}\big{)}^{2}\Big{)}.\end{split}\]

Relation \((1)\) is the recursive application of the difference alongside substitution of the reward bonuses, while relation \((2)\) is by Lemma 15 and Lemma 20.

[MISSING_PAGE_FAIL:45]

### Example: Value Gain due to Transition Lookahead

We now present in further detail the example described at Section 3. This example is inspired by the one in Appendix C.3 in [10], greatly simplifying it and achieving similar behavior for a much smaller environment.

Agents start at the left side of a chain of length \(H/2\) (depicted in Figure 2) and have two options:

1. Play a safe action \(a_{1}\) that leaves the agent in the same state (in green), or,
2. play one of the \(A-1\) risky actions \(a_{2},\ldots,a_{A}\) (in red). Each of these actions moves the agent forward in the chain w.p. \(\frac{1}{A-1}\), but leads to a terminal non-rewarding state w.p. \(1-\frac{1}{A-1}\).

At the end of the chain, the last state is an absorbing state with a unit reward.

Without lookahead, all agents can do is try to randomly reach the end of the chain, succeeding with probability \((A-1)^{-H/2}\). In particular, such agents cannot collect more than \(V^{no}\leq H(A-1)^{-H/2}\). On the other hand, with transition lookahead, agents observe whether the risky actions allow moving forward in the chain or lead to the bad terminal state. If one action allows progressing in the chain (which happens w.p. \(p=1-\left(1-\frac{1}{A-1}\right)^{A-1}\geq 1-\nicefrac{{1}}{{e}}\)), a lookahead agent would take it, and otherwise, they will use \(a_{1}\) to remain in the same state. In other words, optimal lookahead agents reach the reward after \(H/2-1\) successful 'progression steps' with probability \(p\) each. The probability of reaching the end of the chain using less than \(5H/6\) steps is at least

\[\Pr\!\left(\mathrm{Bin}\!\left(\frac{5H}{6}-1,1-\frac{1}{e}\right)>\frac{H}{2} \right)\geq c_{0},\quad\text{for some absolute }c_{0}>0.\]

Under this event, the agent collects \(\frac{H}{6}\) rewards, so the lookahead value is at least \(V^{T,*}\geq\frac{c_{0}H}{6}\!=\!\Omega(H)\).

To summarize, for this example, no lookahead optimal value is at most \(\approx HA^{-H/2}\), while transition lookahead agents can collect a value of \(\approx H\): transition lookahead increases the value by an exponential multiplicative factor. The difference between the two values is \(G^{T}=\Omega(H)\), and following the discussion in Section 3, a sublinear transition lookahead regret would imply a negatively linear standard regret of \(\mathrm{Reg}(K)\lesssim-HK\).

**Remark 3**.: _The chain length was chosen to be \(H/2\) for simplicity - similar conclusions can be achieved for a length of \(\approx 1-\nicefrac{{1}}{{e}}\). Then, the multiplicative increase in value due to transition lookahead would be \(\approx(A-1)^{\left(1-\frac{1}{e}\right)H}\), matching Proposition 2 in [10]. In fact, setting the transition from the last state of the chain to the terminal state (rendering it possible to earn only one unit of reward), the analysis coincides with the one in [10]. Following their exact derivation, the value with lookahead information is multiplicatively larger than its no-lookahead factor by an exponential factor of \(\Theta\!\left((A-1)^{\min\left\{(1-\frac{1}{e})H-1,S\right\}-2\right\}\). This significantly improves the result in [10], that only holds if \(S\geq A^{\left(1-\frac{1}{e}\right)H}\)._

Figure 2: Random chain: agents start at the left side and must reach its right side to collect a reward.

Auxiliary Lemmas

In this appendix, we prove various auxiliary lemma that will be used throughout our proofs.

### Concentration results

We first present and reprove a set of well-known concentration results.

**Lemma 16**.: _Let \(P\) be a distribution over a discrete set \(\mathcal{X}\) of size \(|\mathcal{X}|=M\) and let \(X,X_{1},\ldots,X_{n}\) be independent samples from this distribution. Also, let \(U:\mathcal{X}\mapsto[0,C]\) for some \(C>0\) and define the empirical distribution \(\hat{P}_{n}(x)=\frac{1}{n}\sum_{i=1}^{n}\mathds{1}\{x_{i}=x\}\). Then, for any \(\delta\in(0,1)\), each of the following events hold w.p. at least \(1-\delta\):_

\[E^{p}=\left\{\forall x\in\mathcal{X},|P(x)-\hat{P}_{n}(x)|\leq \sqrt{\frac{2P(x)\ln\frac{2M}{\delta}}{n}}+\frac{2\ln\frac{2M}{\delta}}{3n}\right\}\] \[E^{pv1}=\left\{\left|\sum_{x\in\mathcal{X}}\Big{(}\hat{P}_{n}(x )-P(x)\Big{)}U(x)\right|\leq\sqrt{\frac{2\mathrm{Var}_{P}(U(X))\ln\frac{2}{ \delta}}{n}}+\frac{2C\ln\frac{2}{\delta}}{3n}\right\}\] \[E^{pv2}=\left\{\left|\sqrt{\mathrm{Var}_{\hat{P}_{n}}(U(X))}- \sqrt{\mathrm{Var}_{P}(U(X))}\right|\leq 4C\sqrt{\frac{\ln\frac{2}{\delta}}{n \lor 1}}\right\}\!,\]

_where \(\mathrm{Var}_{P}(U(X))=\sum_{x\in\mathcal{X}}P(x)U(x)^{2}-\left(\sum_{x\in \mathcal{X}}P(x)U(x)\right)^{2}\)._

Proof.: All the results require standard probability arguments and are stated for completeness.

For the first event \(E^{p}\), notice that each of the components \(\hat{P}_{n}(x)\) is the empirical mean of independent Bernoulli random variables \(X_{i}(x)\) of mean \(P(x)\). Therefore, by Bernstein's inequality, recalling that the variance of the variable \(Ber(p)\) is \(p(1-p)\), we get w.p. at least \(1-\frac{\delta}{M}\) that

\[|P(x)-\hat{P}_{n}(x)|\leq\sqrt{\frac{2P(x)(1-P(x))\ln\frac{2M}{\delta}}{n}}+ \frac{2\ln\frac{2M}{\delta}}{3n}\leq\sqrt{\frac{2P(x)\ln\frac{2M}{\delta}}{n} }+\frac{2\ln\frac{2M}{\delta}}{3n}.\]

Taking the union bound over all \(x\in\mathcal{X}\) implies that \(E^{p}\) holds w.p. at least \(1-\delta\).

For the second event \(E^{pv1}\), we apply Bernstein's inequality on the variables \(Y_{i}=U(X_{i})\). The empirical mean is given by \(\hat{Y}_{n}=\frac{1}{n}\sum_{i}U(X_{i})=\sum_{x\in\mathcal{X}}\hat{P}_{n}(x)U(x)\) and its average is \(\mathbb{E}[Y]=\sum_{x\in\mathcal{X}}P(x)U(x)\). Similarly, the variance of the random variables is \(\mathrm{Var}(Y)=\mathrm{Var}_{P}(U(X))\). Thus, by Bernstein's inequality, w.p. at least \(1-\delta\),

\[\left|\hat{Y}_{n}-\mathbb{E}[Y]\right|\leq\sqrt{\frac{2\mathrm{Var}(Y)\ln \frac{2}{\delta}}{n}}+\frac{2C\ln\frac{2}{\delta}}{3n}.\]

Stating the bounds in terms of \(X_{i}\) leads to the second event.

For the last event, we follow the analysis of [Efroni et al., 2021, Lemma 19], which in turn, relies on [Maurer and Pontil, 2009, Theorem 10]. Define \(V_{n}=\frac{1}{2n(n-1)}\sum_{i,j=1}^{n}(U(X_{i})-U(X_{j}))^{2}\). This is a well-known unbiased variance estimator, namely, \(\mathbb{E}[V_{n}]=\mathrm{Var}_{P}(U(X))\), and by [Maurer and Pontil, 2009, Theorem 10], for any \(\delta>0\) it holds w.p. at least \(1-\delta\) that

\[\left|\sqrt{V_{n}}-\sqrt{\mathrm{Var}_{P}(U(X))}\right|\leq C\sqrt{\frac{2\ln \frac{2}{\delta}}{n-1}},\]

where we scaled the bound by \(C\) to account for the values being in \([0,C]\).

Next, we relate \(V_{n}\) to the empirical variance. By elementary algebra, we have

\[V_{n} =\frac{1}{2n(n-1)}\sum_{i,j=1}^{n}\left(U(X_{i})-U(X_{j})\right)^{2}\] \[=\frac{1}{n}\sum_{i=1}^{n}U(X_{i})^{2}-\frac{1}{n(n-1)}\sum_{i\neq j }U(X_{i})U(X_{j})\] \[=\frac{1}{n}\sum_{i=1}^{n}U(X_{i})^{2}-\frac{n}{(n-1)}\Bigg{(} \frac{1}{n}\sum_{i}U(X_{i})\Bigg{)}^{2}+\frac{1}{n(n-1)}\sum_{i=1}^{n}U(X_{i}) ^{2}\] \[=\sum_{x\in\mathcal{X}}\hat{P}_{n}(x)U(x)^{2}-\left(\sum_{x\in \mathcal{X}}\hat{P}_{n}(x)U(x)\right)^{2}+\frac{1}{n(n-1)}\sum_{i=1}^{n}U(X_{i })^{2}-\frac{1}{n^{2}(n-1)}\Bigg{(}\sum_{i=1}^{n}U(X_{i})\Bigg{)}^{2}.\]

The first two terms are exactly the variance w.r.t. the empirical distribution; therefore, using the inequality \(\left|\sqrt{a}-\sqrt{b}\right|\leq\sqrt{|a-b|}\) for positive numbers, we have

\[\left|\sqrt{V_{n}}-\sqrt{\operatorname{Var}_{\hat{P}_{n}}(U(X))}\right|\leq \sqrt{\left|\frac{1}{n(n-1)}\sum_{i=1}^{n}U(X_{i})^{2}-\frac{1}{n^{2}(n-1)} \Bigg{(}\sum_{i=1}^{n}U(X_{i})\Bigg{)}^{2}\right|}\leq\sqrt{\frac{C^{2}}{n-1}}.\]

Combining both inequalities and recalling the trivial bound of \(C\) on the difference, we get that w.p. at least \(1-\bar{\delta}\),

\[\left|\sqrt{\operatorname{Var}_{\hat{P}_{n}}(U(X))}-\sqrt{ \operatorname{Var}_{P}(U(X))}\right|\leq\min\Bigg{\{}C\sqrt{\frac{2\ln\frac{2} {\delta}}{n-1}}+\sqrt{\frac{C^{2}}{n-1}},C\Bigg{\}}\leq 4C\sqrt{\frac{\ln\frac{2}{ \delta}}{n\lor 1}}.\]

Next, we present a short lemma that allows moving between different spaces of probabilities.

**Lemma 17**.: _Let \(\mathcal{X}\) be a finite set and let \(X_{1},\dots,X_{n}\in\mathcal{X}\). Also, let \(E_{1},\dots,E_{m}\subseteq\mathcal{X}\) be a partition of the set \(\mathcal{X}\), namely, for all \(i\neq j\), \(E_{i}\cap E_{j}=\emptyset\) and \(\cup_{i=1}^{m}E_{i}=\mathcal{X}\). Finally, let \(f:\mathcal{X}\mapsto\mathbb{R}\) such that for all \(i\in[m]\) and \(x\in E_{i}\), it holds that \(f(x)=f(i)\), and define_

\[\hat{P}_{n}(x)=\frac{1}{n}\sum_{\ell=1}^{n}\mathds{1}\{X_{\ell}=x\},\quad\text {and},\quad\hat{Q}_{n}(i)=\frac{1}{n}\sum_{\ell=1}^{n}\mathds{1}\{X_{\ell}\in E _{i}\}.\]

_Then, the following hold:_

1. \(\hat{Q}_{n}(i)=\hat{P}_{n}(E_{i})\triangleq\sum_{x\in E_{i}}\hat{P}_{n}(x)\) _and, in particular,_ \(\mathbb{E}_{i\sim\hat{Q}_{n}}[f(i)]=\mathbb{E}_{x\sim\hat{P}_{n}}[f(x)]\)_._
2. _If_ \(P\) _is a distribution over_ \(\mathcal{X}\) _and_ \(X_{1},\dots,X_{n}\in\mathcal{X}\) _are i.i.d. samples from_ \(P\)_, then_ \(\mathbb{E}[\hat{Q}_{n}(i)]=P(E_{i})\triangleq Q(i)\)_. It also holds that_ \(\mathbb{E}_{x\sim P}[f(x)]=\mathbb{E}_{i\sim Q}[f(i)]\)_._

Proof.: For the first part, we have by definition that

\[\hat{Q}_{n}(i) =\frac{1}{n}\sum_{\ell=1}^{n}\mathds{1}\{X_{\ell}\in E_{i}\}= \sum_{x\in\mathcal{X}}\frac{1}{n}\sum_{\ell=1}^{n}\mathds{1}\{X_{\ell}=x\} \mathds{1}\{x\in E_{i}\}=\sum_{x\in\mathcal{X}}\hat{P}_{n}(x)\mathds{1}\{x\in E _{i}\}\] \[=\sum_{x\in E_{i}}\hat{P}_{n}(x)=\hat{P}_{n}(E_{i}).\]

In particular, it holds that

\[\mathbb{E}_{i\sim\hat{Q}_{n}}[f(i)] =\sum_{i=1}^{m}\hat{Q}_{n}(i)f(i)=\sum_{i=1}^{m}\sum_{x\in E_{i} }\hat{P}_{n}(x)f(i)\stackrel{{(\ref{eq:1})}}{{=}}\sum_{i=1}^{m} \sum_{x\in E_{i}}\hat{P}_{n}(x)f(x)\stackrel{{(\ref{eq:2})}}{{=}} \sum_{x\in\mathcal{X}}\hat{P}_{n}(x)f(x)\] \[=\mathbb{E}_{x\sim\hat{P}_{n}}[f(x)],\]where \((1)\) is since \(f\) is constant inside \(E_{i}\) and \((2)\) is since \(\{E_{i}\}_{i=1}^{m}\) partition \(\mathcal{X}\).

For the second part of the statement, notice that since the samples are i.i.d., it holds that \(\mathbb{E}\big{[}\hat{P}_{n}(x)\big{]}=P(x)\), and therefore,

\[\mathbb{E}[\hat{Q}_{n}(i)]=\mathbb{E}\Bigg{[}\sum_{x\in E_{i}}\hat{P}_{n}(x) \Bigg{]}=\sum_{x\in E_{i}}P(x)=P(E_{i})=Q(i).\]

Finally, as in the first part of the statement, it holds that

\[\mathbb{E}_{i\sim Q}[f(i)] =\sum_{i=1}^{m}Q(i)f(i)=\sum_{i=1}^{m}\sum_{x\in E_{i}}P(x)f(i)= \sum_{i=1}^{m}\sum_{x\in E_{i}}P(x)f(x)=\sum_{x\in\mathcal{X}}P(x)f(x)\] \[=\mathbb{E}_{x\sim P}[f(x)].\]

Finally, we present two specialized concentration results that are needed for reward and transition lookahead, respectively.

**Lemma 18**.: _Let \(X,X_{1},\ldots X_{n}\in\mathbb{R}^{d}\) be i.i.d. random vectors over \([0,1]\) and let \(C\geq 1\) be some constant. Then, for any \(\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\forall u\in[0,C]^{d},\qquad\Bigg{|}\mathbb{E}\bigg{[}\max_{i\in[d]}\{X(i)+u( i)\}\bigg{]}-\frac{1}{n}\sum_{\ell=1}^{n}\max_{i\in[d]}\{X_{\ell}(i)+u(i)\} \Bigg{|}\leq 3\sqrt{\frac{d\ln\frac{9Cn}{\delta}}{2n}}.\]

Proof.: Denote \(m(u)=\mathbb{E}\big{[}\max_{i\in[d]}\{X(i)+u(i)\}\big{]}\) and \(\hat{m}(u)=\frac{1}{n}\sum_{\ell=1}^{n}\max_{i\in[d]}\{X_{\ell}(i)+u(i)\}\) and fix any \(u\in[0,C]^{d}\). Since the variables are bounded in \([0,1]\), their maximum is bounded almost surely in \([\max_{i}u(i),\max_{i}u(i)+1]\), namely, an interval of unit length. Therefore, by Hoeffding's inequality, for any \(\delta^{\prime}\in(0,1)\), w.p. \(1-\delta^{\prime}\)

\[|m(u)-\hat{m}(u)|\leq\sqrt{\frac{\ln\frac{2}{\delta^{\prime}}}{2n}}.\]

Now, for some \(\epsilon\in(0,C]\), let \(u_{\epsilon}\) be the closest vector to \(u\) on a grid \(\{0,\epsilon,2\epsilon,\ldots,C\}^{d}\). Then, it clearly holds that

\[|m(u)-\hat{m}(u)|\leq|m(u_{\epsilon})-\hat{m}(u_{\epsilon})|+2\epsilon.\]

Taking the union bound over all \(\big{(}\big{\lceil}\frac{C}{\epsilon}\big{\rceil}+1\big{)}^{d}\) possible choices for \(u_{\epsilon}\) and fixing \(\delta^{\prime}=\frac{\delta}{\big{(}\big{\lceil}\frac{C}{\epsilon}\big{\rceil} +1\big{)}^{d}}\), we get w.p. \(1-\delta\) for all \(u\) that

\[|m(u)-\hat{m}(u)|\leq\sqrt{\frac{\ln\frac{2\big{(}\big{\lceil}\frac{C}{\epsilon }\big{\rceil}+1\big{)}^{d}}{\delta}}{2n}}+2\epsilon\leq\sqrt{\frac{d\ln\frac{6C }{\epsilon\delta}}{2n}}+2\epsilon.\]

Now, fixing \(\epsilon=\sqrt{\frac{d\ln\frac{6C}{\epsilon\delta}}{2n}}\) and noting that \(\frac{1}{\epsilon}\leq\sqrt{2n}\) for \(C\geq 1\), we get

\[|m(u)-\hat{m}(u)|\leq\sqrt{\frac{d\ln\frac{6C\sqrt{2n}}{\delta}}{2n}}+2\sqrt{ \frac{d\ln\frac{6C}{\delta}}{2n}}\leq\sqrt{\frac{d\ln\frac{9Cn}{\delta}}{2n}}+2 \sqrt{\frac{d\ln\frac{6C}{\delta}}{2n}}\leq 3\sqrt{\frac{d\ln\frac{9Cn}{\delta}}{2n}}.\]

**Lemma 19**.: _Let \(X,X_{1},\ldots X_{n}\in\mathbb{R}^{d}\) be i.i.d. random vectors with components supported over the discrete set \([m]\) and let \(C\geq 1\) be some constant. Then, uniformly over all \(u\in[0,C]^{dm}\) w.p. \(1-\delta\):_

\[\left|\mathbb{E}\Big{[}\max_{i}\{u(X(i),i)\}\Big{]}-\frac{1}{n} \sum_{\ell=1}^{n}\max_{i}\{u(X_{\ell}(i),i)\}\right|\] \[\leq\sqrt{\frac{2md\ln\frac{6n}{\delta}\mathrm{Var}(\max_{i}\{u(X (i),i)\})}{n}}++\frac{8Cmd\big{(}\ln\frac{6n}{\delta}\big{)}^{1.5}}{n}.\]

Proof.: We follow a similar path to Lemma 18 and use a covering argument. Denoting \(w(u)=\mathbb{E}[\max_{i}\{u(X(i),i)\}]\) and \(\hat{w}(u)=\frac{1}{n}\sum_{\ell=1}^{n}\max_{i}\{u(X_{\ell}(i),i)\}\), by Bernstein's inequality, for any \(\delta^{\prime}\in(0,1)\) and fixed \(u\in[0,C]^{dm}\), it holds w.p. \(1-\delta^{\prime}\) that

\[|w(u)-\hat{w}(u)|\leq\sqrt{\frac{2\mathrm{Var}(\max_{i}\{u(X(i),i)\})\ln\frac{ 2}{\delta}}{n}}+\frac{2C\ln\frac{2}{\delta}}{3n}.\] (17)

Now, for some \(\epsilon\in(0,C]\), let \(u_{\epsilon}\) be the closest matrix to \(u\) on a grid \(\left\{0,\epsilon,2\epsilon,\ldots,C\right\}^{md}\) and denote \(Z(u)=\max_{i}\{u(X(i),i)\}\) with samples \(Z_{i}(u)\). By the smoothness of the max function, it holds that

\[|Z(u)-Z(u_{\epsilon})|\leq\epsilon.\]

In particular, we also have that

\[\left|\mathbb{E}[Z(u)^{2}]-\mathbb{E}[Z(u_{\epsilon})^{2}]\right|\leq \epsilon^{2}+2C\epsilon,\qquad\text{and}\qquad\left|\mathbb{E}[Z(u)]^{2}- \mathbb{E}[Z(u_{\epsilon})]^{2}\right|\leq\epsilon^{2}+2C\epsilon,\]

so we have

\[\left|\mathrm{Var}\Big{(}\max_{i}\{u(X(i),i)\}\Big{)}-\mathrm{Var}\Big{(}\max _{i}\{u_{\epsilon}(X(i),i)\}\Big{)}\right|=|\mathrm{Var}(Z(u))-\mathrm{Var}(Z (u_{\epsilon}))|\leq 2\epsilon^{2}+4C\epsilon.\]

Similarly, it holds that

\[|w(u)-\hat{w}(u)|\leq|w(u_{\epsilon})-\hat{w}(u_{\epsilon})|+2\epsilon.\]

Taking the union bound over all \(\left(\left\lceil\frac{C}{\epsilon}\right\rceil+1\right)^{md}\) possible choices for \(u_{\epsilon}\) and fixing \(\delta^{\prime}=\frac{\delta}{\left(\left\lceil\frac{C}{\epsilon}\right\rceil +1\right)^{dm}}\), we get w.p. \(1-\delta\) for all \(u\) that

\[|w(u)-\hat{w}(u)| \leq\sqrt{\frac{2\mathrm{Var}(\max_{i}\{u_{\epsilon}(X(i),i)\}) \ln\frac{2\left(\left\lceil\frac{C}{\epsilon}\right\rceil+1\right)^{md}}{ \delta}}{n}}+\frac{2C\ln\frac{2\left(\left\lceil\frac{C}{\epsilon}\right\rceil +1\right)^{md}}{\delta}}{3n}+2\epsilon\] \[\leq\sqrt{\frac{2md\mathrm{Var}(\max_{i}\{u_{\epsilon}(X(i),i)\} )\ln\frac{6C}{\epsilon\delta}}{n}}+\frac{2Cmd\ln\frac{6C}{\epsilon\delta}}{3 }+2\epsilon\] \[\leq\sqrt{\frac{2md\ln\frac{6C}{\epsilon\delta}\mathrm{Var}(\max _{i}\{u(X(i),i)\})+2\epsilon^{2}+4C\epsilon)}{n}}+\frac{2Cmd\ln\frac{6C}{ \epsilon\delta}}{3n}+2\epsilon\] \[\leq\sqrt{\frac{2md\ln\frac{6C}{\epsilon\delta}\mathrm{Var}(\max _{i}\{u(X(i),i)\})}{n}}+\sqrt{\frac{8mdC\epsilon\ln\frac{6C}{\epsilon\delta}}{ n}}+\sqrt{\frac{4mde^{2}\ln\frac{6C}{\epsilon\delta}}{n}}\] \[\quad+\frac{2Cmd\ln\frac{6C}{\epsilon\delta}}{3n}+2\epsilon.\]

Now, fixing \(\epsilon=\frac{C\ln\frac{6n}{\delta}}{n}\) and noticing that \(\frac{6C}{\epsilon\delta}\leq\frac{6n}{\delta}\), we get

\[|w(u)-\hat{w}(u)| \leq\sqrt{\frac{2md\ln\frac{6n}{\delta}\mathrm{Var}(\max_{i}\{u(X (i),i)\})}{n}}+\frac{\sqrt{8mdC}\ln\frac{6n}{\delta}}{n}+\frac{\sqrt{4mdC} \big{(}\ln\frac{6n}{\delta}\big{)}^{1.5}}{n^{1.5}}\] \[\quad+\frac{2Cmd\ln\frac{6n}{\delta}}{3n}+\frac{2C\ln\frac{6C}{ \delta}}{n}\] \[\leq\sqrt{\frac{2md\ln\frac{6n}{\delta}\mathrm{Var}(\max_{i}\{u(X (i),i)\})}{n}}+\frac{8Cmd\big{(}\ln\frac{6n}{\delta}\big{)}^{1.5}}{n}.\]

### Count-Related Lemmas

**Lemma 20**.: _The following bounds hold:_

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{n_{h}^{k-1}(s_{h}^{k},a_{h }^{k})\lor 1}}\leq SAH+2\sqrt{SAH^{2}K}, \sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h }^{k})\lor 1}\leq SAH(2+\ln(K)),\] \[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{n_{h}^{k-1}(s_{h}^{k}) \lor 1}}\leq SH+2\sqrt{SH^{2}K}, \sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{n_{h}^{k-1}(s_{h}^{k})\lor 1} \leq SH(2+\ln(K)).\]

Proof.: Recall that every time a state (or state-action) is visited, its visitation-count is increased by \(1\), up to \(n_{h}^{K-1}(s,a)\) at the last episode. therefore, we can write

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{n_{h}^{k-1}(s_{h}^{k}, a_{h}^{k})\lor 1}} =\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}\sum_{ k=1}^{K}\frac{\mathds{1}\big{\{}s_{h}^{k}=s,a_{h}^{k}=a\big{\}}}{\sqrt{n_{h}^{k-1} (s,a)\lor 1}}\] \[=\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}\sum_{ i=0}^{n_{h}^{K-1}(s,a)}\frac{1}{\sqrt{i\lor 1}}\] \[\leq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}} \bigg{(}1+2\sqrt{n_{h}^{K-1}(s,a)}\bigg{)}\] \[\leq SAH+2\sqrt{SAH\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{a \in\mathcal{A}}n_{h}^{K-1}(s,a)}\] (Jensen's inequality) \[\leq SAH+2\sqrt{SAH^{2}K}.\]

where we bounded the total number of visits by the number of steps \(HK\). Similarly, we also have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{n_{h}^{k-1}(s_{h}^{k},a_{h}^ {k})\lor 1} =\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}}^{n_{h }^{K-1}(s,a)}\frac{1}{i\lor 1}\] \[\leq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{a\in\mathcal{A}} \big{(}2+\ln\big{(}n_{h}^{K-1}(s,a)\lor 1\big{)}\big{)}\leq SAH(2+\ln(K)).\]

We can likewise prove the inequalities for the state counts as follows:

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{n_{h}^{k-1}(s_{h}^{k}) \lor 1}} =\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{k=1}^{K}\frac{\mathds{1 }\big{\{}s_{h}^{k}=s\big{\}}}{\sqrt{n_{h}^{k-1}(s)\lor 1}}\] \[=\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\sum_{i=0}^{n_{h}^{K-1}(s)} \frac{1}{\sqrt{i\lor 1}}\] \[\leq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\bigg{(}1+2\sqrt{n_{h}^{K -1}(s)}\bigg{)}\] \[\leq SH+2\sqrt{SH\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}n_{h}^{K-1}(s )}\] (Jensen's inequality) \[\leq SH+2\sqrt{SH^{2}K},\]

and

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{n_{h}^{k-1}(s_{h}^{k})\lor 1}=\sum_{h=1}^{H} \sum_{s\in\mathcal{S}}\sum_{i=0}^{n_{h}^{K-1}(s)}\frac{1}{i\lor 1}\leq\sum_{h=1}^{H} \sum_{s\in\mathcal{S}}\big{(}2+\ln\big{(}n_{h}^{K-1}(s)\lor 1\big{)}\big{)} \leq SH(2+\ln(K)).\]

[MISSING_PAGE_EMPTY:52]

Existing Results

**Lemma 23** (Monotonic Bonuses,[Zhang et al., 2023], Appendix C.1).: _For any \(p\in\Delta^{S}\), \(v\in\mathbb{R}_{+}^{S}\) s.t. \(\left\|v\right\|_{\infty}\leq H\), \(\delta^{\prime}\in(0,1)\) and positive integer \(n\), define the function_

\[f(p,v,n)=p^{T}v+\max\left\{\frac{20}{3}\sqrt{\frac{\mathrm{Var}_{p}(v)\ln\frac {1}{\delta^{\prime}}}{n}},\frac{400}{9}\frac{H\ln\frac{1}{\delta^{\prime}}}{n }\right\}.\]

_Then, the function \(f(p,v,n)\) is non-decreasing in each entry of \(v\)._

**Lemma 24** (Efroni et al. 2021, Lemma 28).: _Let \(Y\in\mathbb{R}^{S}\) be a vector such that \(0\leq Y(s)\leq H\) for all \(s\in\mathcal{S}\). Let \(P_{1}\) and \(P_{2}\) be two transition models and \(n\in\mathbb{R}_{+}^{SA}\). If_

\[\left\{\forall(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S },h\in[H]:\;|P_{2,h}(s^{\prime}|s,a)-P_{1,h}(s^{\prime}|s,a)|\leq\sqrt{\frac{ C_{1}L_{\delta}^{k}P_{1,h}(s^{\prime}|s,a)}{n(s,a)\lor 1}}+\frac{C_{2}L_{\delta}^{k}}{ n(s,a)\lor 1}\right\}\!,\]

_for some \(C_{1},C_{2}>0\), then, for any \(\alpha>0\),_

\[|(P_{1,h}-P_{2,h})Y(s,a)|\leq\frac{1}{\alpha}\mathbb{E}_{s^{\prime}\sim P_{1,h }(\cdot|s,a)}[Y(s^{\prime})]+\frac{HSL_{\delta}^{k}(C_{2}+\alpha C_{1}/4)}{n(s,a)\lor 1},\]

**Lemma 25** (Efroni et al. 2021, Lemma 27).: _Let \(\{Y_{t}\}_{t\geq 1}\) be a real-valued sequence of random variables adapted to a filtration \(\{F_{t}\}_{t\geq 0}\). Assume that for all \(t\geq 1\) it holds that \(0\leq Y_{t}\leq C\) a.s., and let \(T\in\mathbb{N}\). Then each of the following inequalities holds with probability greater than \(1-\delta\)._

\[\sum_{t=1}^{T}\mathbb{E}[Y_{t}|F_{t-1}]\leq\left(1+\frac{1}{2C} \right)\sum_{t=1}^{T}Y_{t}+2(2C+1)^{2}\ln\frac{1}{\delta},\] \[\sum_{t=1}^{T}Y_{t}\leq 2\sum_{t=1}^{T}\mathbb{E}[Y_{t}|F_{t-1}]+4C \ln\frac{1}{\delta}.\]

**Lemma 26** (Standard Deviation Differences, e.g., Zanette and Brunskill 2019, lines 48-51).: _Let \(P\in\Delta_{d}\) be some distribution over \([d]\) and let \(V_{1},V_{2}\in\mathbb{R}^{d}\). Then, it holds that_

\[\sqrt{\mathrm{Var}_{P}(V_{1})}-\sqrt{\mathrm{Var}_{P}(V_{2})}\leq\sqrt{ \mathrm{Var}_{P}(V_{1}-V_{2})}.\]

**Lemma 27** (Law of Total Variance, e.g., Zanette and Brunskill 2019, Lemma 15).: _For any no-lookahead policy \(\pi\), it holds that_

\[\mathbb{E}\!\left[\sum_{h=1}^{H}\mathrm{Var}(V_{h+1}^{\pi}(s_{h+1})|s_{h})|\pi,s_{1}\right]=\mathbb{E}\!\left[\left(\sum_{h=1}^{H}r_{h}(s_{h},a_{h})-V_{1}^ {\pi}(s_{1})\right)^{2}|\pi,s_{1}\right]\!,\]

_where \(\mathrm{Var}(V_{h+1}^{\pi}(s_{h+1})|s_{h})\) is the variance of the value at step \(s_{h+1}\) given state \(s_{h}\) and under the policy \(\pi\), due to the policy randomization and next-state transition probabilities._

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: _In the abstract, we accurately present the setting and its motivation, as well as a summary of the results, all of which are proved in the appendix._ Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: _The main limitations in this work are a result of the studied setup - some possible extensions and improvement are discussed in the future work section._ Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: _Proofs for all the stated results are provided in the appendix._

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: _The paper does not include experiments._ Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: _The paper does not include experiments._ Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: _The paper does not include experiments._ Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: _The paper does not include experiments._ Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: _The paper does not include experiments._ Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: _The paper is purely theoretical and studies a fundamental decision-making model; any ethical issue that might arise would be a core issue in the ethics of applying machine learning, and not tied specifically to this work._ Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: _Due to the theoretical nature of the paper and the generality of the model, it is no direct societal impact._ Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: _No data or models are released with this paper._ Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: _The paper does not use existing assets._ Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: _The paper does not release new assets._ Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: _The paper does not involve crowdsourcing nor research with human subjects._ Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: _The paper does not involve crowdsourcing nor research with human subjects._ Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.