# Nearly Optimal VC-Dimension and Pseudo-Dimension Bounds for Deep Neural Network Derivatives

Yahong YANG

Department of Mathematics

The Pennsylvania State University, University Park,

State College, PA, USA

and

Department of Mathematics

Hong Kong University of Science and Technology

Clear Water Bay, Hong Kong SAR, China

yxy5498@psu.edu

Haizhao YANG

Department of Mathematics and Department of Computer Science

University of Maryland College Park

College Park, MD, USA

hzyang@umd.edu

Corresponding author.

Yang XIANG

Department of Mathematics

Hong Kong University of Science and Technology

Clear Water Bay, Hong Kong SAR, China

and

Algorithms of Machine Learning and Autonomous Driving Research Lab

HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute

Futian, Shenzhen, China

maxiang@ust.hk

###### Abstract

This paper addresses the problem of nearly optimal Vapnik-Chervonenkis dimension (VC-dimension) and pseudo-dimension estimations of the derivative functions of deep neural networks (DNNs). Two important applications of these estimations include: 1) Establishing a nearly tight approximation result of DNNs in the Sobolev space; 2) Characterizing the generalization error of machine learning methods with loss functions involving function derivatives. This theoretical investigation fills the gap of learning error estimations for a wide range of physics-informed machine learning models and applications including generative models, solving partial differential equations, operator learning, network compression, distillation, regularization, etc.

## 1 Introduction

The Sobolev training [8, 40, 46, 45, 22, 42] of deep neural networks (DNNs) has had a significant impact on scientific and engineering fields, including solving partial differential equations [25, 12,33, 10], operator learning [29, 26], network compression [35], distillation [21, 34], regularization [8], and dynamic programming [15, 48], etc. For example, Sobolev (semi) norms have been applied to penalize function gradients in loss functions [2, 17, 15, 30] to control the Lipschitz constant of DNNs. Moreover, Sobolev norms and equivalent formulas are commonly used to define loss functions in various applications such as dynamic programming [15, 48], solving partial differential equations [25, 12, 33], and distillation [21, 34, 35]. These loss functions enable models to learn DNNs that can approximate the target function with small discrepancies in both magnitude and derivative. For example, when utilizing the Deep Ritz method [12] to solve PDEs such as the following one:

\[\begin{cases}-\Delta u=f&\text{ in }\Omega,\\ \frac{\partial u}{\partial\nu}=0&\text{ on }\partial\Omega,\end{cases}\] (1)

the corresponding loss function can be expressed as:

\[\mathcal{E}_{D}(\bm{\theta}):=\frac{1}{2}\int_{\Omega}|\nabla\phi(\bm{x};\bm {\theta})|^{2}\mathrm{d}\bm{x}+\frac{1}{2}\left(\int_{\Omega}\phi(\bm{x};\bm {\theta})\mathrm{d}\bm{x}\right)^{2}-\int_{\Omega}f\phi(\bm{x};\bm{\theta}) \mathrm{d}\bm{x},\]

where \(\bm{\theta}\) represents all the parameters in the neural network. Here, \(\Omega\) denotes the domain \((0,1)^{d}\). Proposition 1 in [27] establishes the equivalence between the loss function \(\mathcal{E}_{D}(\bm{\theta})\) and \(\|\phi(\bm{x};\bm{\theta})-u^{*}(\bm{x})\|_{H^{1}((0,1)^{d})}\), where \(u^{*}(\bm{x})\) denotes the exact solution of the PDEs in equation (1), and \(\|f\|_{H^{1}((0,1)^{d})}:=\left(\sum_{0\leq|\alpha|\leq 1}\|D^{\bm{\alpha}}f\|_{ L^{2}((0,1)^{d})}^{p}\right)^{1/2}\). Thus, the Sobolev norm \(H^{1}((0,1)^{d})\) serves as a measure of the loss function, and Sobolev training is employed to solve PDEs within the Deep Ritz method.

Two natural questions that arise are: 1) What is the optimal approximation error of DNNs described by a Sobolev norm? 2) What is the generalization error of the loss function defined by a Sobolev norm? The key step to address these questions is to estimate the optimal Vapnik-Chervonenkis dimension (VC-dimension) and pseudo-dimension [3, 46, 1, 32] of DNNs and their derivatives. Intuitively, these concepts characterize the complexity or richness of a function set and, hence, they can be applied to establish the best possible approximation and generalization power of DNNs.

**Definition 1** (VC-dimension [1]).: _Let \(H\) denote a class of functions from \(\mathcal{X}\) to \(\{0,1\}\). For any non-negative integer \(m\), define the growth function of \(H\) as_

\[\Pi_{H}(m):=\max_{x_{1},x_{2},\ldots,x_{m}\in\mathcal{X}}\left|\{(h(x_{1}),h( x_{2}),\ldots,h(x_{m})):h\in H\}\right|.\]

_The Vapnik-Chervonenkis dimension (VC-dimension) of \(H\), denoted by \(\text{VCdim}(H)\), is the largest \(m\) such that \(\Pi_{H}(m)=2^{m}\). For a class \(\mathcal{G}\) of real-valued functions, define \(\text{VCdim}(\mathcal{G}):=\text{VCdim}(\operatorname{sgn}(\mathcal{G}))\), where \(\operatorname{sgn}(\mathcal{G}):=\{\operatorname{sgn}(f):f\in\mathcal{G}\}\) and \(\operatorname{sgn}(x)=1[x>0]\)._

**Definition 2** (pseudo-dimension [32]).: _Let \(\mathcal{F}\) be a class of functions from \(\mathcal{X}\) to \(\mathbb{R}\). The pseudo-dimension of \(\mathcal{F}\), denoted by \(\text{Pdim}(\mathcal{F})\), is the largest integer \(m\) for which there exists \((x_{1},x_{2},\ldots,x_{m},y_{1},y_{2},\ldots,y_{m})\in\mathcal{X}^{m}\times \mathbb{R}^{m}\) such that for any \((b_{1},\ldots,b_{m})\in\{0,1\}^{m}\) there is \(f\in\mathcal{F}\) such that \(\forall i:f\left(x_{i}\right)>y_{i}\Longleftrightarrow b_{i}=1\)._

The main contribution of this paper is to estimate nearly optimal bounds of the VC-dimension and pseudo-dimension of DNN derivatives. Based on these bounds, we can prove the optimality of our DNN approximation, as measured by Sobolev norms (Theorem 3), and obtain a tighter generalization error of loss functions defined by Sobolev norms. Our results facilitate the understanding of Sobolev training and the performance of DNNs in Sobolev spaces.

Bounds for the VC-dimension and pseudo-dimension of DNNs have been established in [16, 5, 3, 4, 6, 41, 20]. However, these approaches and findings cannot be applied to Sobolev training, as they do not account for the derivatives of DNNs, which represent a key difference between Sobolev training and classical methods. Obtaining such bounds for DNN derivatives is much more difficult due to their complex compositional structures. DNN derivatives consist of a series of interdependent parts that are multiplied together via the chain rule, rendering existing methods for estimating bounds inapplicable. Estimating the VC-dimension and pseudo-dimension of DNN derivatives is the most crucial and challenging problem addressed in this paper. In [11], the VC-dimension and pseudo-dimension of DNN derivatives were analyzed, but the results were suboptimal due to a lack of consideration for the relationships between the multiplied terms in a DNN derivative. As a result, their findings do not provide the optimal approximation of DNNs in Sobolev spaces and can only give a generalization error that is much larger than the actual error that may arise from Sobolev training. In this paper, we introduce a novel method that investigates these relationships, resulting in a simplified complexity of DNN derivatives. This, in turn, allows us to obtain nearly optimal bounds on their VC-dimension and pseudo-dimension.

The paper is divided into two parts. In the first part, we establish a nearly optimal bound on the VC-dimension of DNN derivatives with the ReLU activation function \(\sigma_{1}(x):=\max\{0,x\}\):

**Theorem 1**.: _For any \(N,L,d\in\mathbb{N}_{+}\), there exists a constant \(\bar{C}\) independent with \(N,L\) such that_

\[\text{VCdim}(D\Phi)\leq\bar{C}N^{2}L^{2}\log_{2}L\log_{2}N,\] (2)

_for_

\[D\Phi:=\left\{\psi=D_{i}\phi:\phi\in\Phi,\;i=1,2,\ldots,d\right\},\] (3)

_where \(\Phi:=\left\{\phi:\phi\text{ is a }\sigma_{1}\text{-NN in }\mathbb{R}^{d}\text{ with width} \leq N\text{ and depth}\leq L\right\}\), and \(D_{i}\) is the weak derivative in the \(i\)-th variable._

By utilizing Theorem 1, we prove that our DNN approximation rate for approximating functions in Sobolev spaces \(W^{n,\infty}((0,1)^{d})\) using Sobolev norms in \(W^{1,\infty}((0,1)^{d})\) is nearly optimal. We present our construction of DNNs for this approximation in Theorem 3, and we demonstrate the optimality of such approximation in Theorem 4. Furthermore, we generalize our method to approximate DNNs in Sobolev spaces measured by Sobolev norms \(W^{m,\infty}((0,1)^{d})\) for \(m\geq 2\). The details of this generalization are presented in Corollaries 1 and 2. The Sobolev spaces, equipped with Sobolev (semi) norms, are defined as follows:

**Definition 3** (Sobolev Spaces [13]).: _Denote \(\Omega\) as \((0,1)^{d}\), \(D\) as the weak derivative of a single variable function and \(D^{\boldsymbol{\alpha}}=D_{1}^{\alpha_{1}}D_{2}^{\alpha_{2}}\ldots D_{d}^{ \alpha_{d}}\) as the partial derivative where \(\boldsymbol{\alpha}=[\alpha_{1},\alpha_{2},\ldots,\alpha_{d}]^{T}\) and \(D_{i}\) is the derivative in the \(i\)-th variable. Let \(n\in\mathbb{N}\) and \(1\leq p\leq\infty\). Then we define Sobolev spaces_

\[W^{n,p}(\Omega):=\left\{f\in L^{p}(\Omega):D^{\boldsymbol{\alpha}}f\in L^{p}( \Omega)\text{ for all }\boldsymbol{\alpha}\in\mathbb{N}^{d}\text{ with }|\boldsymbol{\alpha}|\leq n\right\}\]

_with a norm \(\|f\|_{W^{n,p}(\Omega)}:=\left(\sum_{0\leq|\alpha|\leq n}\|D^{\boldsymbol{ \alpha}}f\|_{L^{p}(\Omega)}^{p}\right)^{1/p}\), if \(p<\infty\), and \(\|f\|_{W^{n,\infty}(\Omega)}:=\max_{0\leq|\alpha|\leq n}\|D^{\boldsymbol{ \alpha}}f\|_{L^{\infty}(\Omega)}\). Furthermore, for \(\boldsymbol{f}=(f_{1},f_{2},\ldots,f_{d})\), \(\boldsymbol{f}\in W^{1,\infty}(\Omega,\mathbb{R}^{d})\) if and only if \(f_{i}\in W^{1,\infty}(\Omega)\) for each \(i=1,2,\ldots,d\) and \(\|\boldsymbol{f}\|_{W^{1,\infty}(\Omega,\mathbb{R}^{d})}:=\max_{i=1,\ldots,d} \{\|f_{i}\|_{W^{1,\infty}(\Omega)}\}\)._

**Definition 4** (Sobolev semi-norm [13]).: _Let \(n\in\mathbb{N}_{+}\) and \(1\leq p\leq\infty\). Then we define Sobolev semi-norm \(|f|_{W^{n,p}(\Omega)}:=\left(\sum_{|\alpha|=n}\|D^{\boldsymbol{\alpha}}f\|_{L^ {p}(\Omega)}^{p}\right)^{1/p}\), if \(p<\infty\), and \(|f|_{W^{n,\infty}(\Omega)}:=\max_{|\alpha|=n}\|D^{\boldsymbol{\alpha}}f\|_{L^ {\infty}(\Omega)}\). Furthermore, for \(\boldsymbol{f}\in W^{1,\infty}(\Omega,\mathbb{R}^{d})\), we define \(|\boldsymbol{f}|_{W^{1,\infty}(\Omega,\mathbb{R}^{d})}:=\max_{i=1,\ldots,d}\{| f_{i}|_{W^{1,\infty}(\Omega)}\}\)._

In the second part of our paper, we utilize our previous work on estimating the VC-dimension of DNN derivatives to obtain an upper bound on the pseudo-dimension of DNN derivatives:

**Theorem 2**.: _For any \(N,L,d\in\mathbb{N}_{+}\), there exists a constant \(\bar{C}\) independent with \(N,L\) such that_

\[\text{Pdim}(D\Phi)\leq\bar{C}N^{2}L^{2}\log_{2}L\log_{2}N,\] (4)

_where \(D\Phi\) is defined in Theorem 1._

Based on Theorem 2, we can estimate the generalization error of loss functions defined by Sobolev norms, as demonstrated in Theorem 5. Specifically, the error is bounded by \(\boldsymbol{O}(NL(\log_{2}N\log_{2}L)^{1/2})\) with respect to the width \(N\) and depth \(L\) of DNNs. This bound is significantly smaller than the previously reported bound of \(\boldsymbol{O}(NL^{5/2}(\log_{2}N\log_{2}L)^{1/2})\) in [11]. We attribute this improvement to our more accurate estimation of the pseudo-dimension of DNN derivatives. Our findings indicate that learning target functions with loss functions defined by Sobolev norms does not require substantially more sample points than those defined by \(L^{2}\)-norms [14], as their generalization error orders are equivalent with respect to the width \(N\) and depth \(L\) of DNNs.

Our main contributions are:

\(\bullet\) We propose a method to achieve nearly optimal estimations of the VC-dimension and pseudo-dimension of DNN derivatives.

\(\bullet\) By utilizing our estimation of the VC-dimension of DNN derivatives, we demonstrate the optimality of our DNN approximation, as measured by Sobolev norms.

\(\bullet\) By applying our estimation of the pseudo-dimension of DNN derivatives, we obtain a bound for the generalization error measured by the Sobolev norm. Importantly, our results demonstrate that the degree of generalization error defined by Sobolev norms is equivalent to that defined by \(L^{2}\)-norms, corresponding to the width \(N\) and depth \(L\) of DNNs.

## 2 Preliminaries

Let us summarize all basic notations used in the DNNs as follows:

**1**. Matrices are denoted by bold uppercase letters. For example, \(\bm{A}\in\mathbb{R}^{m\times n}\) is a real matrix of size \(m\times n\) and \(\bm{A}^{\intercal}\) denotes the transpose of \(\bm{A}\).

**2**. Vectors are denoted by bold lowercase letters. For example, \(\bm{v}\in\mathbb{R}^{n}\) is a column vector of size \(n\). Furthermore, denote \(\bm{v}(i)\) as the \(i\)-th elements of \(\bm{v}\).

**3**. For a \(d\)-dimensional multi-index \(\bm{\alpha}=[\alpha_{1},\alpha_{2},\cdots\alpha_{d}]\in\mathbb{N}^{d}\), we denote several related notations as follows: \((a)\)\(|\bm{\alpha}|=|\alpha_{1}|+|\alpha_{2}|+\cdots+|\alpha_{d}|\); \((b)\)\(\bm{x}^{\alpha}=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}\cdots x_{d}^{\alpha_{d}},\)\(\bm{x}=[x_{1},x_{2},\cdots,x_{d}]^{\intercal}\); \((c)\)\(\bm{\alpha}!=\alpha_{1}!\alpha_{2}!\cdots\alpha_{d}!\).

**4**. Let \(B_{r,|\cdot|}(\bm{x})\subset\mathbb{R}^{d}\) be the closed ball with a center \(\bm{x}\in\mathbb{R}^{d}\) and a radius \(r\) measured by the Euclidean distance. Similarly, \(B_{r,\|\cdot\|_{\ell_{\infty}}}(\bm{x})\subset\mathbb{R}^{d}\) be the closed ball with a center \(\bm{x}\in\mathbb{R}^{d}\) and a radius \(r\) measured by the \(\ell_{\infty}\)-norm.

**5**. Assume \(\bm{n}\in\mathbb{N}_{+}^{n}\), then \(f(\bm{n})=\bm{O}(g(\bm{n}))\) means that there exists positive \(C\) independent of \(\bm{n},f,g\) such that \(f(\bm{n})\leq Cg(\bm{n})\) when all entries of \(\bm{n}\) go to \(+\infty\).

**6**. Define \(\sigma_{1}(x):=\sigma(x)=\max\{0,x\}\) and \(\sigma_{2}:=\sigma^{2}(x)\). We call the neural networks with activation function \(\sigma_{t}\) with \(t\leq i\) as \(\sigma_{i}\) neural networks (\(\sigma_{i}\)-NNs). With the abuse of notations, we define \(\sigma_{i}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\) as \(\sigma_{i}(\bm{x})=\left[\begin{array}{c}\sigma_{i}(x_{1})\\ \vdots\\ \sigma_{i}(x_{d})\end{array}\right]\) for any \(\bm{x}=[x_{1},\cdots,x_{d}]^{T}\in\mathbb{R}^{d}\).

**7**. Define \(L,N\in\mathbb{N}_{+}\), \(N_{0}=d\) and \(N_{L+1}=1\), \(N_{i}\in\mathbb{N}_{+}\) for \(i=1,2,\ldots,L\), then a \(\sigma_{i}\)-NN \(\phi\) with the width \(N\) and depth \(L\) can be described as follows:

\[\bm{x}=\tilde{\bm{h}}_{0}\stackrel{{ W_{1},b_{1}}}{{\longrightarrow}} \bm{h}_{1}\stackrel{{\sigma_{1}}}{{\longrightarrow}}\tilde{\bm{h }}_{1}\ldots\stackrel{{ W_{L},b_{L}}}{{\longrightarrow}}\bm{h}_{L} \stackrel{{\sigma_{i}}}{{\longrightarrow}}\tilde{\bm{h}}_{L} \stackrel{{ W_{L+1},b_{L+1}}}{{\longrightarrow}}\phi(\bm{x})=\bm{h }_{L+1},\]

where \(\bm{W}_{i}\in\mathbb{R}^{N_{i}\times N_{i-1}}\) and \(\bm{b}_{i}\in\mathbb{R}^{N_{i}}\) are the weight matrix and the bias vector in the \(i\)-th linear transform in \(\phi\), respectively, i.e., \(\bm{h}_{i}:=\bm{W}_{i}\tilde{\bm{h}}_{i-1}+\bm{b}_{i},\) for \(i=1,\ldots,L+1\) and \(\tilde{\bm{h}}_{i}=\sigma_{i}\left(\bm{h}_{i}\right),\) for \(i=1,\ldots,L.\) In this paper, an DNN with the width \(N\) and depth \(L\), means (a) The maximum width of this DNN for all hidden layers less than or equal to \(N\). (b) The number of hidden layers of this DNN less than or equal to \(L\).

## 3 Nearly Optimal Approximation Results of DNNs in Sobolev Spaces

Measured by Sobolev Norms

### Approximation of functions in \(W^{n,\infty}\) with \(W^{1,\infty}\) norm by ReLU neural networks

In this subsection, we construct deep neural networks (DNNs) with a width of \(\bm{O}(N\log N)\) and a depth of \(\bm{O}(L\log L)\) to approximate functions in the Sobolev space \(W^{n,\infty}\), as measured by Sobolev norms in \(W^{1,\infty}\). The approximation rate achieved by these networks is \(\bm{O}(N^{-2(n-1)/d}L^{-2(n-1)/d})\).

**Theorem 3**.: _For any \(f\in W^{n,\infty}((0,1)^{d})\) with \(n\geq 2\) and \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\), any \(N,L\in\mathbb{N}_{+}\), there is a \(\sigma_{1}\)-NN \(\phi\) with the width \((34+d)2^{d}n^{d+1}(N+1)\log_{2}(8N)\) and depth \(56d^{2}n^{2}(L+1)\log_{2}(4L)\) such that_

\[\|f(\bm{x})-\phi(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}\leq C_{9}(n,d)N^{-2(n-1)/d }L^{-2(n-1)/d},\]

_where \(C_{9}\) is the constant independent with \(N,L\)._The proof of Theorem 3 can be outlined in five parts, and the complete proof is provided in Appendix 7.2:

**(i)**: First of all, define a sequence of subsets of \(\Omega\):

**Definition 5**.: _Given \(K,d\in\mathbb{N}^{+}\), and for any \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\), we define \(\Omega_{\bm{m}}:=\prod_{j=1}^{d}\Omega_{m_{j}},\) where \(\Omega_{1}:=\bigcup_{i=0}^{K-1}\left[\frac{i}{K},\frac{i}{K}+\frac{3}{4K} \right],\ \Omega_{2}:=\bigcup_{i=0}^{K}\left[\frac{i}{K}-\frac{1}{2K},\frac{i}{K}+ \frac{1}{4K}\right]\cap[0,1]\)._

Then we define a partition of unity \(\{g_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) on \((0,1)^{d}\) with supp \(g_{\bm{m}}\cap(0,1)^{d}\subset\Omega_{\bm{m}}\) for each \(\bm{m}\in\{1,2\}^{d}\):

**Definition 6**.: _Given \(K,d\in\mathbb{N}_{+}\), we define_

\[g_{1}(x):=\begin{cases}1,&x\in\left[\frac{i}{K}+\frac{1}{4K},\frac{i}{K}+ \frac{1}{2K}\right]\\ 0,&x\in\left[\frac{i}{K}+\frac{3}{4K},\frac{i+1}{4K}\right]\\ 4K\left(x-\frac{i}{K}\right),&x\in\left[\frac{i}{K},\frac{i}{K}+\frac{1}{4K} \right]\\ -4K\left(x-\frac{i}{K}-\frac{3}{4K}\right),&x\in\left[\frac{i}{K}+\frac{2i}{K},\frac{i}{K}+\frac{3}{4K}\right]\end{cases},\ g_{2}(x):=g_{1}\left(x+\frac{1}{ 2K}\right),\] (5)

_for \(i\in\mathbb{Z}\). For any \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\), define \(g_{\bm{m}}(\bm{x})=\prod_{j=1}^{d}g_{m_{j}}(x_{j}),\ \bm{x}=(x_{1},x_{2},\ldots,x_{d})\)._

**(ii)**: Then we use the following proposition to approximate \(\{g_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) by \(\sigma_{1}\)-NNs and construct a sequence of \(\sigma_{1}\)-NNs \(\{\phi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\):

**Proposition 1**.: _Given any \(N,L,n\in\mathbb{N}_{+}\) for \(K=\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor\), then for any \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\), there is a \(\sigma_{1}\)-NN with the width smaller than \((9+d)(N+1)+d-1\) and depth smaller than \(15d(d-1)nL\) such as \(\|\phi_{\bm{m}}(\bm{x})-g_{\bm{m}}(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}\leq 50d^{ \frac{1}{2}}(N+1)^{-4dnL}\)._

The proof of Proposition 1 is presented in Appendix 7.2.1.

**(iii)**: For each \(\Omega_{\bm{m}}\subset[0,1]^{d}\), where \(\bm{m}\in\{1,2\}^{d}\), we find a function \(f_{K,\bm{m}}\) satisfying

\[\|f-f_{K,\bm{m}}\|_{W^{1,\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-(n-1)},\] \[\|f-f_{K,\bm{m}}\|_{L^{\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-n},\] (6)

where \(C_{1}\) is a constant independent of \(K\). Moreover, each \(f_{K,\bm{m}}\) can be expressed as \(f_{K,\bm{m}}=\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}}(\bm{x})\bm{x} ^{\bm{\alpha}}\), where \(g_{f,\bm{\alpha},\bm{m}}(\bm{x})\) is a piecewise constant function on \(\Omega_{\bm{m}}\). The proof of this result is based on the Bramble-Hilbert Lemma [7, Lemma 4.3.8], and the details are provided in Appendix 7.2.2.

**(iv)**: The fourth step involves approximating \(f_{K,\bm{m}}\) using neural networks \(\psi_{\bm{m}}\), following the approach outlined in [28]. This method is suitable for our work because \(g_{f,\bm{\alpha},\bm{m}}(\bm{x})\) is a piecewise constant function on \(\Omega_{\bm{m}}\), and the weak derivative of \(g_{f,\bm{\alpha},\bm{m}}(\bm{x})\) on \(\Omega_{\bm{m}}\) is zero. This property allows for the use of the \(L^{\infty}\) norm approximation method presented in [28]. Thus, we obtain a neural network \(\psi_{\bm{m}}\) with width \(\bm{O}(N\log N)\) and depth \(\bm{O}(L\log L)\) such that

\[\|f_{K,\bm{m}}-\psi_{\bm{m}}(\bm{x})\|_{W^{1,\infty}(\Omega_{ \bm{m}})} \leq C_{5}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f_{K,\bm{m}}-\psi_{\bm{m}}(\bm{x})\|_{L^{\infty}(\Omega_{\bm{m }})} \leq C_{5}(n,d)N^{-2n/d}L^{-2n/d},\] (7)

Figure 1: The schematic diagram of \(g_{i}\) for \(i=1,2\).

where \(C_{5}\) is a constant independent of \(N\) and \(L\).

By combining (iii) and (iv) and setting \(K=\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor\), we obtain that for each \(\bm{m}\in\{1,2\}^{d}\), there exists a neural network \(\psi_{\bm{m}}\) with width \(\bm{O}(N\log N)\) and depth \(\bm{O}(L\log L)\) such that

\[\|f(\bm{x})-\psi_{\bm{m}}(\bm{x})\|_{W^{1,\infty}(\Omega_{\bm{m} })} \leq C_{6}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f(\bm{x})-\psi_{\bm{m}}(\bm{x})\|_{L^{\infty}(\Omega_{\bm{m}})} \leq C_{6}(n,d)N^{-2n/d}L^{-2n/d},\] (8)

where \(C_{6}\) is a constant independent of \(N\) and \(L\). Further details are provided in Appendix 7.2.3.

**(v)**: The final step is to combine the sequences \(\{\phi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) and \(\{\psi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) to construct a network that can approximate \(f\) over the entire space \([0,1]^{d}\). We define the sequence \(\{\phi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) because \(\psi_{\bm{m}}\) may not accurately approximate \(f\) on \([0,1]^{d}\backslash\Omega_{\bm{m}}\). The purpose of \(\phi_{\bm{m}}\) is to remove this portion of the domain and allow other networks to approximate \(f\) on \([0,1]^{d}\backslash\Omega_{\bm{m}}\). Further details on this step are provided in Appendix 7.2.4.

While recent works [28; 23; 39; 18; 31; 9; 19] have studied the approximation of smooth functions or functions in Sobolev spaces by DNNs measured in the norm of \(L^{p}(\Omega)\) or \(W^{s,p}(\Omega)\), they typically present results that are not optimal or are measured in \(L^{p}\)-norms. For example, in [28], they applies Taylor's expansion to approximate smooth functions but cannot be applied directly in Sobolev spaces. In [39], they improve on this by using the Bramble-Hilbert Lemma to approximate functions in Sobolev spaces, but their error is still measured in \(L^{p}\)-norms. In [18], the authors show that there exists a ReLU neural network that can approximate \(f\in W^{1,p}(\Omega)\), but their approximation rate is not optimal and is the same as that in traditional methods such as the finite element theory. Our work provides a superior approximation rate. Later, a rigorous proof of optimality of Theorem 3 is discussed in Appendix 7.2.4 and Subsection 3.3.

Approximation of functions in \(W^{n,\infty}\) measured by \(W^{m,\infty}\) norm with \(m>1\) by neural networks (sketches of the proofs of the Corollaries 1 and 2)

In this subsection, we utilize neural networks to approximate functions in \(W^{n,\infty}\) measured by \(W^{m,\infty}\), where \(m>1\). The proof strategy is similar to the approximation measured in the norm of \(W^{1,\infty}\). However, we cannot rely on ReLU neural networks alone to achieve this goal, as ReLU neural networks are piece-wise linear functions that do not belong to \(W^{m,\infty}\) with \(m>1\). Note that the Bramble-Hilbert Lemma is still applicable in higher-order approximation. Therefore, we need to approximate the function \(f_{K,\bm{m}}=\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}}(\bm{x})\bm{ x}^{\bm{\alpha}}\) within each domain \(\Omega_{\bm{m}}\) using DNNs, where \(g_{f,\bm{\alpha},\bm{m}}(x)\) represents a piece-wise constant function within \(\Omega_{\bm{m}}\). Consequently, ReLU-based DNNs can effectively approximate \(g_{f,\bm{\alpha},\bm{m}}(x)\) within \(\Omega_{\bm{m}}\) when measured by higher-order Sobolev spaces, since both the higher-order derivatives of \(g_{f,\bm{\alpha},\bm{m}}(x)\) and ReLU-based DNNs are zero. The parts of ReLU-based DNNs that do not have high-order derivatives appear in the domain \(\Omega\backslash\Omega_{\bm{m}}\), and we will use the partition of unity to ensure that this part disappears in the final presentation, this is the reason why it is still acceptable to have ReLU activations appearing in the network. However, when it comes to approximating \(\bm{x}^{\bm{\alpha}}\) for \(|\bm{\alpha}|>1\) based on high-order Sobolev norms, ReLU-based DNNs fail to provide accurate results. Therefore, we require DNNs that utilize the square of ReLU activation in this specific scenario. This is how we construct our approach and the reason why we employ both ReLU and the square of ReLU activation functions.

Instead, we examine the use of \(\sigma_{2}\) neural networks for approximating functions measured in the norm of \(W^{2,\infty}\). As per Corollary 1, a neural network with \(\bm{O}(N\log N)\) width and \(\bm{O}(L\log L)\) depth can achieve a nonasymptotic approximation rate of \(\bm{O}(N^{-2(n-2)/d}L^{-2(n-2)/d})\) with respect to the \(W^{2,\infty}((0,1)^{d})\) norm. Moreover, our method can be extended to approximations measured in the norm of \(W^{m,\infty}\) with \(m>2\), as shown in Corollary 2. The proof strategy is similar to that used in Subsection 3.1, except that we need to construct a smoother partition of unity rather than \(\{g_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\). The corollaries are presented below, and further details are provided in Appendix 7.3.

**Corollary 1**.: _For any \(f\in W^{n,\infty}((0,1)^{d})\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\), any \(N,L\in\mathbb{N}_{+}\) with \(NL+2^{\lfloor\log_{2}N\rfloor}\geq\max\{d,n\}\) and \(L\geq\lceil\log_{2}N\rceil\), there is a \(\sigma_{2}\)-NN \(\gamma(\bm{x})\) with the width \(2^{d+6}n^{d+1}(N+d)\log_{2}(8N)\) and depth \(15n^{2}(L+2)\log_{2}(4L)\) such that_

\[\|f(\bm{x})-\gamma(\bm{x})\|_{W^{2,\infty}((0,1)^{d})}\leq 2^{d+7}C_{10}(n,d)N^{-2(n -2)/d}L^{-2(n-2)/d},\]_where \(C_{10}\) is the constant independent with \(N,L\)._

**Corollary 2**.: _For any \(f\in W^{n,\infty}((0,1)^{d})\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\), any \(N,L,m\in\mathbb{N}_{+}\) with \(NL+2^{\lfloor\log_{2}N\rfloor}\geq\max\{d,n\}\) and \(L\geq\lceil\log_{2}N\rceil\), there is a \(\sigma_{2}\)-NN \(\varphi(\bm{x})\) with the width \(\bm{O}(N\log N)\) and depth \(\bm{O}(L\log L)\) such that_

\[\|f(\bm{x})-\varphi(\bm{x})\|_{W^{m,\infty}((0,1)^{d})}\leq C_{11}(n,d,m)N^{-2 (n-m)/d}L^{-2(n-m)/d},\]

_where \(C_{11}\) is the constant independent with \(N,L\)._

### Optimality of Theorem 3 via estimation of VC-dimension of DNN derivatives (Theorem 1)

In this section, we demonstrate that the approximation rate presented in Theorem 3 is nearly asymptotically optimal:

**Theorem 4**.: _Given any \(\rho,C_{1},C_{2},C_{3},J_{0}>0\) and \(n,d\in\mathbb{N}^{+}\), there exist \(N,L\in\mathbb{N}\) with \(NL\geq J_{0}\) and \(f\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\), satisfying for any \(\sigma_{1}\)-NN \(\phi\) with the width smaller than \(C_{1}N\log N\) and depth smaller than \(C_{2}L\log L\), we have_

\[|\phi-f|_{W^{1,\infty}((0,1)^{d})}>C_{3}L^{-2(n-1)/d-\rho}N^{-2(n-1)/d-\rho}.\] (9)

In other words, the approximation rate of \(\bm{O}(N^{-2(n-1)/d-\rho}K^{-2(n-1)/d-\rho})\) cannot be achieved asymptotically when ReLU \(\sigma_{1}\)-NNs with width \(\bm{O}(N\log N)\) and depth \(\bm{O}(L\log L)\) to approximate functions in \(\mathcal{F}_{n,d}:=\left\{f\in W^{n,\infty}((0,1)^{d}):\|f\|_{W^{n,\infty}((0, 1)^{d})}\leq 1\right\}\). The proof of Theorem 4 is based on the estimation of the VC-dimension of DNN derivatives, which is provided in Theorem 1.

Theorem 1 plays a crucial role in our proof of Theorem 4, which is established through a proof by contradiction following the approach outlined in Ref. [28]. Further details on the proof can be found in Appendix 7.4. The main idea behind the proof is that Theorem 1 characterizes the complexity of DNN derivatives, which in turn limits the ability of DNNs to approximate functions in Sobolev spaces.

In this paper, we focus on the optimality of approximation rate with respect to width \(N\) and depth \(L\) of DNNs. The dimensionality \(d\) is not the focus that we consider in our research. Addressing the question about mitigating the exponential dependence of width on dimensionality, we have observed that this arises from the utilization of methods like Taylor's expansion or average Taylor polynomials in our approximation techniques. It remains an open question for future research to explore alternative approaches to address the challenge of getting the dependence of \(d\) in the lower bounds.

Generalization Analysis in Sobolev Spaces via Estimation of Pseudo-dimension of DNN Derivatives (Theorem 2)

In a typical supervised learning algorithm, the objective is to learn a high-dimensional target function \(f(\bm{x})\) defined on \((0,1)^{d}\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\) from a finite set of data samples \(\{(\bm{x}_{i},f(\bm{x}_{i}))\}_{i=1}^{M}\). When training a DNN, we aim to identify a DNN \(\phi(\bm{x};\bm{\theta}_{S})\) that approximates \(f(\bm{x})\) based on random data samples \(\{(\bm{x}_{i},f(\bm{x}_{i}))\}_{i=1}^{M}\). We assume that \(\{\bm{x}_{i}\}_{i=1}^{M}\) is an i.i.d. sequence of random variables uniformly distributed on \((0,1)^{d}\) in this section. Denote

\[\bm{\theta}_{D} :=\arg\inf_{\bm{\theta}}\mathcal{R}_{D}(\bm{\theta}):=\arg\inf_{ \bm{\theta}}\int_{(0,1)^{d}}|\nabla(f(\bm{x})-\phi(\bm{x};\bm{\theta}))|^{2} +|f(\bm{x})-\phi(\bm{x};\bm{\theta})|^{2}\,\mathrm{d}\bm{x},\] (10) \[\bm{\theta}_{S} :=\arg\inf_{\bm{\theta}}\mathcal{R}_{S}(\bm{\theta}):=\arg\inf_{ \bm{\theta}}\frac{1}{M}\sum_{i=1}^{M}\left[|\nabla(f(\bm{x}_{i})-\phi(\bm{x}_{ i};\bm{\theta}))|^{2}+|f(\bm{x}_{i})-\phi(\bm{x}_{i};\bm{\theta})|^{2}\right].\] (11)

The overall inference error is \(\mathbf{E}\mathcal{R}_{D}(\bm{\theta}_{S})\), which can be divided into two parts:

\[\mathbf{E}\mathcal{R}_{D}(\bm{\theta}_{S})= \mathcal{R}_{D}(\bm{\theta}_{D})+\mathbf{E}\mathcal{R}_{S}(\bm{ \theta}_{D})-\mathcal{R}_{D}(\bm{\theta}_{D})+\mathbf{E}\mathcal{R}_{S}(\bm{ \theta}_{S})-\mathbf{E}\mathcal{R}_{S}(\bm{\theta}_{D})+\mathbf{E}\mathcal{R} _{D}(\bm{\theta}_{S})-\mathbf{E}\mathcal{R}_{S}(\bm{\theta}_{S})\] \[\leq \underbrace{\mathcal{R}_{D}(\bm{\theta}_{D})}_{\text{ approximation error}}+\underbrace{\mathbf{E}\mathcal{R}_{S}(\bm{\theta}_{D})-\mathcal{R}_{D}(\bm{ \theta}_{D})+\mathbf{E}\mathcal{R}_{D}(\bm{\theta}_{S})-\mathbf{E}\mathcal{R }_{S}(\bm{\theta}_{S})}_{\text{generalization error}}\] (12)

where the last inequality is due to \(\mathbf{E}\mathcal{R}_{S}(\bm{\theta}_{S})\leq\mathbf{E}\mathcal{R}_{S}(\bm{ \theta}_{D})\) by the definition of \(\bm{\theta}_{S}\).

Due to Theorem 3, we know that the approximation error \(\mathcal{R}_{D}(\bm{\theta}_{D})\) is a \(\bm{O}(N^{-4(n-1)/d}L^{-4(n-1)/d})\) term since \(\|f(\bm{x})-\phi(\bm{x})\|_{H^{1}((0,1)^{d})}\leq\|f(\bm{x})-\phi(\bm{x})\|_{W^{ 1,\infty}((0,1)^{d})}\). In this section, we bound generalization error in the \(H^{1}((0,1)^{d})\) sense:

**Theorem 5**.: _For any \(N,L,d,B,C_{1},C_{2}\), if \(\phi(\bm{x};\bm{\theta}_{D}),\phi(\bm{x};\bm{\theta}_{S})\in\widetilde{\Phi}\), we will have that there are constants \(C_{5}=C_{5}(B,d,C_{1},C_{2})\) and \(J=J(d,N,L,C_{1},C_{2})\) such that for any \(M\geq J\), we have_

\[\mathbf{E}\mathcal{R}_{S}(\bm{\theta}_{D})-\mathcal{R}_{D}(\bm{ \theta}_{D})+\mathbf{E}\mathcal{R}_{D}(\bm{\theta}_{S})-\mathbf{E}\mathcal{R} _{S}(\bm{\theta}_{S}) \leq 2\sup_{\bm{\theta},\phi(\bm{x};\bm{\theta})\in\widetilde{\Phi}} \left|\mathbf{E}(\mathcal{R}_{S}(\bm{\theta}))-\mathcal{R}_{D}(\bm{\theta})\right|\] \[\leq C_{5}\frac{NL(\log_{2}L\log_{2}N)^{\frac{1}{2}}}{\sqrt{M}} \log M.\] (13)

_where \(\widetilde{\Phi}:=\{\phi:\phi\text{ with the width }\leq C_{1}N\log N\text{ and depth }\leq C_{2}L\log L,\|\phi\|_{W^{1,\infty}((0,1)^{d})}\leq B\}\), and \(\mathcal{R}_{S},\mathcal{R}_{D},\bm{\theta}_{S},\bm{\theta}_{D}\) are defined in Eqs. (10,11)._

The proof of Theorem 5 is based on the works of [3, 11, 26]. We begin by bounding the generalization error using the Rademacher Complexity and then bound the Rademacher Complexity by the uniform covering number. We further bound the uniform covering number by the pseudo-dimension. Finally, we estimate the pseudo-dimension by Theorem 2. The proof of Theorem 5 is presented in Appendix 7.5

Theorem 2 helps to control the degree of the generalization error with respect to \(N\) and \(L\) in Theorem 5. In [11], the generalization error is bounded by \(\bm{O}(NL^{\frac{5}{2}})\). In [24], the authors estimate the covering number using the Lipschitz condition of DNNs instead of the pseudo-dimension, leading to a generalization error that is exponentially dependent on the depth of the DNNs. Our result is much better than them due to the optimal estimation of pseudo-dimension of DNN derivatives (Theorem 2).

## 5 Proof Sketches for Theorems 1 and 2

As Theorems 1 and 2 address the estimation of VC-dimension and pseudo-dimension of DNN derivatives, which is the main contribution of this paper, we provide the proofs for these theorems in this section. The distinction in our approach compared to that of [4] lies in the fact that the application of the chain rule requires the consideration of correlations among distinct segments of the deep neural networks, as opposed to treating them as independent components multiplied together.

In the proof of Theorem 1, we use the following lemmas:

**Lemma 1** ([4, Lemma 17],[3, Theorem 8.3]).: _Suppose \(W\leq M\) and let \(P_{1},\ldots,P_{M}\) be polynomials of degree at most \(D\) in \(W\) variables. Define \(K:=\big{|}\{(\operatorname{sgn}(P_{1}(a)),\ldots,\operatorname{sgn}(P_{M}(a) )):a\in\mathbb{R}^{W}\}\big{|}\), then we have \(K\leq 2(2eMD/W)^{W}\)._

**Lemma 2** ([4, Lemma 18]).: _Suppose that \(2^{m}\leq 2^{t}(mr/w)^{w}\) for some \(r\geq 16\) and \(m\geq w\geq t\geq 0\). Then, \(m\leq t+w\log_{2}(2r\log_{2}r)\)._

As the proof of Theorem 1 represents the most critical and challenging question in our work, we present a sketch of it below.

Proof Sketch of Theorem 1.: An element in \(\Phi\) can be represented as \(\phi=\bm{W}_{L+1}\sigma_{1}(\bm{W}_{L}\sigma_{1}(\ldots\sigma_{1}(\bm{W}_{1}\bm {x}+\bm{b}_{1})\ldots)+\bm{b}_{L})+b_{L+1}\). Therefore, an element in \(D\Phi\) can be represented as

\[\psi(\bm{x})=D_{i}\phi(\bm{x})= \bm{W}_{L+1}\sigma_{0}(\bm{W}_{L}\sigma_{1}(\ldots\sigma_{1}(\bm{W }_{1}\bm{x}+\bm{b}_{1})\ldots)+\bm{b}_{L})\] \[\cdot\bm{W}_{L}\sigma_{0}(\ldots\sigma_{1}(\bm{W}_{1}\bm{x}+\bm{b} _{1})\ldots)\ldots\bm{W}_{2}\sigma_{0}(\bm{W}_{1}\bm{x}+\bm{b}_{1})(\bm{W}_{1} )_{i},\] (14)

where \(\bm{W}_{i}\in\mathbb{R}^{N_{i}\times N_{i-1}}\) (\((\bm{W})_{i}\) is \(i\)-th column of \(\bm{W}\)) and \(\bm{b}_{i}\in\mathbb{R}^{N_{i}}\) are the weight matrix and the bias vector in the \(i\)-th linear transform in \(\phi\), and \(\sigma_{0}(\bm{x})=\operatorname{sgn}(x)=1[x>0]\), which is the derivative of the ReLU function and \(\sigma_{0}(\bm{x})=\operatorname{diag}(\sigma_{0}(x_{i}))\).

Let \(\bm{x}\in\mathbb{R}^{d}\) be an input and \(\bm{\theta}\in\mathbb{R}^{W}\) be a parameter vector in \(\psi\). We denote the output of \(\psi\) with input \(\bm{x}\) and parameter vector \(\bm{\theta}\) as \(f(\bm{x},\bm{\theta})\). For fixed \(\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{m}\) in \(\mathbb{R}^{d}\), we aim to bound

\[K:=\big{|}\{(\operatorname{sgn}(f(\bm{x}_{1},\bm{\theta})),\ldots,\operatorname {sgn}(f(\bm{x}_{m},\bm{\theta}))):\bm{\theta}\in\mathbb{R}^{W}\}\big{|}\,.\] (15)

[MISSING_PAGE_FAIL:9]

this paper. Firstly, we show that the optimal approximation rate of DNNs with a width of \(\bm{O}(N\log N)\) and a depth of \(\bm{O}(L\log L)\) is \(\bm{O}(N^{-2(n-1)/d}L^{-2(n-1)/d})\) in Sobolev spaces. This demonstrates the ability of DNNs to learn target functions well in Sobolev training. Secondly, we find that the degree of the pseudo-dimension of DNN derivatives is the same as that for DNNs corresponding to the width \(N\) and depth \(L\) of DNNs. This result suggests that despite the apparent complexity of DNN derivatives, the degree of generalization error of loss functions containing derivatives of DNNs is equivalent to that without derivatives, corresponding to the width \(N\) and depth \(L\) of DNNs. As a result, we do not need to use a significantly larger number of sample points to learn the target function in Sobolev training compared to regular training.

The estimations of the VC-dimension and pseudo-dimension of DNN derivatives have broad applications in deep learning research. For example, in classification tasks, the VC-dimension characterizes the uniform convergence of misclassification frequencies to probabilities and asymptotically determines the sample complexity of PAC learning [4, 44, 6]. These applications can be explored in the further work. Our focus in this paper is on the Sobolev training with loss functions containing first-order derivatives, and we also obtain the approximation rate of \(\sigma_{2}\)-NNs described by higher-order Sobolev norms (Corollaries 1 and 2). The optimality of these results and the generalization error of Sobolev training with loss functions containing higher-order derivatives of DNNs remain open problems, as estimating the VC-dimension and pseudo-dimension of higher-order derivatives of \(\sigma_{2}\)-NNs requires further investigation.

## Acknowledgments and Disclosure of Funding

This work was done during Y.Y.'s visit under the supervision of Prof. H.Y., in the Department of Mathematics, University of Maryland College Park. The work of H. Y. was partially supported by the US National Science Foundation under award DMS-2244988, DMS-2206333, and the Office of Naval Research Award N00014-23-1-2007. The work of Y.X. was supported by the Project of Hetao Shenzhen-HKUST Innovation Cooperation Zone HZQB-KCZYB-2020083.

## References

* [1] Y. Abu-Mostafa. The Vapnik-Chervonenkis dimension: Information versus complexity in learning. _Neural Computation_, 1(3):312-317, 1989.
* [2] J. Adler and S. Lunz. Banach wasserstein Gan. _Advances in neural information processing systems_, 31, 2018.
* [3] M. Anthony, P. Bartlett, et al. _Neural network learning: Theoretical foundations_, volume 9. cambridge university press Cambridge, 1999.
* [4] P. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks. _The Journal of Machine Learning Research_, 20(1):2285-2301, 2019.
* [5] P. Bartlett, V. Maiorov, and R. Meir. Almost linear VC dimension bounds for piecewise polynomial networks. _Advances in neural information processing systems_, 11, 1998.
* [6] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. _Journal of the ACM (JACM)_, 36(4):929-965, 1989.
* [7] S. Brenner, L. Scott, and L. Scott. _The mathematical theory of finite element methods_, volume 3. Springer, 2008.
* [8] W. Czarnecki, S. Osindero, M. Jaderberg, G. Swirszcz, and R. Pascanu. Sobolev training for neural networks. _Advances in neural information processing systems_, 30, 2017.
* [9] T. De Ryck, S. Lanthaler, and S. Mishra. On the approximation of functions by tanh neural networks. _Neural Networks_, 143:732-750, 2021.
* [10] T. De Ryck and S. Mishra. Error analysis for physics-informed neural networks (PINNs) approximating Kolmogorov PDEs. _Advances in Computational Mathematics_, 48(6):1-40, 2022.

* [11] C. Duan, Y. Jiao, Y. Lai, X. Lu, and Z. Yang. Convergence rate analysis for Deep Ritz method. _arXiv preprint arXiv:2103.13330_, 2021.
* [12] W. E, J. Han, and A. Jentzen. Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations. _Communications in Mathematics and Statistics_, 5(4):349-380, 2017.
* [13] L. Evans. _Partial differential equations_, volume 19. American Mathematical Society, 2022.
* [14] M. Farrell, T. Liang, and S. Misra. Deep neural networks for estimation and inference. _Econometrica_, 89(1):181-213, 2021.
* [15] C. Finlay, J. Calder, B. Abbasi, and A. Oberman. Lipschitz regularized deep neural networks generalize and are adversarially robust. _arXiv preprint arXiv:1808.09540_, 2018.
* [16] P. Goldberg and M. Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. In _Proceedings of the sixth annual conference on Computational learning theory_, pages 361-369, 1993.
* [17] S. Gu and L. Rigazio. Towards deep neural network architectures robust to adversarial examples. _arXiv preprint arXiv:1412.5068_, 2014.
* [18] I. Guhring, G. Kutyniok, and P. Petersen. Error bounds for approximations with deep ReLU neural networks in \(W^{s,p}\) norms. _Analysis and Applications_, 18(05):803-859, 2020.
* [19] I. Guhring and M. Raslan. Approximation rates for neural networks with encodable weights in smoothness spaces. _Neural Networks_, 134:107-130, 2021.
* [20] N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension bounds for piecewise linear neural networks. In _Conference on learning theory_, pages 1064-1068. PMLR, 2017.
* [21] G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [22] J. Hoffman, D. Roberts, and S. Yaida. Robust learning with jacobian regularization. _arXiv preprint arXiv:1908.02729_, 2019.
* [23] S. Hon and H. Yang. Simultaneous neural network approximation for smooth functions. _Neural Networks_, 154:152-164, 2022.
* [24] Y. Jiao, Y. Lai, Y. Lo, Y. Wang, and Y. Yang. Error analysis of Deep Ritz methods for elliptic equations. _arXiv preprint arXiv:2107.14478_, 2021.
* [25] I. Lagaris, A. Likas, and D. Fotiadis. Artificial neural networks for solving ordinary and partial differential equations. _IEEE Transactions on Neural Networks_, 9(5):987-1000, 1998.
* [26] H. Liu, H. Yang, M. Chen, T. Zhao, and W. Liao. Deep nonparametric estimation of operators between infinite dimensional spaces. _arXiv preprint arXiv:2201.00217_, 2022.
* [27] J. Lu, Y. Lu, and M. Wang. A priori generalization analysis of the Deep Ritz method for solving high dimensional elliptic partial differential equations. In _Conference on Learning Theory_, pages 3196-3241. PMLR, 2021.
* [28] J. Lu, Z. Shen, H. Yang, and S. Zhang. Deep network approximation for smooth functions. _SIAM Journal on Mathematical Analysis_, 53(5):5465-5506, 2021.
* [29] L. Lu, P. Jin, G. Pang, Z. Zhang, and G. Karniadakis. Learning nonlinear operators via DeepONet based on the universal approximation theorem of operators. _Nature machine intelligence_, 3(3):218-229, 2021.
* [30] Y. Mroueh, C. Li, T. Sercu, A. Raj, and Y. Cheng. Sobolev Gan. In _International Conference on Learning Representations_. International Conference on Learning Representations, ICLR, 2018.
* [31] J. Muller and M. Zeinhofer. Error estimates for the Deep Ritz method with boundary penalty. In _Mathematical and Scientific Machine Learning_, pages 215-230. PMLR, 2022.

* [32] D. Pollard. Empirical processes: theory and applications. Ims, 1990.
* [33] M. Raissi, P. Perdikaris, and G. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. _Journal of Computational Physics_, 378:686-707, 2019.
* [34] A. A Rusu, S. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell. Policy distillation. _arXiv preprint arXiv:1511.06295_, 2015.
* [35] B. Sau and V. Balasubramanian. Deep model compression: Distilling knowledge from noisy teachers. _arXiv preprint arXiv:1610.09650_, 2016.
* [36] Z. Shen, H. Yang, and S. Zhang. Nonlinear approximation via compositions. _Neural Networks_, 119:74-84, 2019.
* [37] Z. Shen, H. Yang, and S. Zhang. Deep network approximation characterized by number of neurons. _Communications in Computational Physics_, 28(5), 2020.
* [38] Z. Shen, H. Yang, and S. Zhang. Optimal approximation rate of ReLU networks in terms of width and depth. _Journal de Mathematiques Pures et Appliquees_, 157:101-135, 2022.
* [39] J. Siegel. Optimal approximation rates for deep ReLU neural networks on Sobolev spaces. _arXiv preprint arXiv:2211.14400_, 2022.
* [40] H. Son, J. Jang, W. Han, and H. Hwang. Sobolev training for the neural network solutions of PDEs. _arXiv preprint arXiv:2101.08932_, 2021.
* [41] E. Sontag et al. Vc dimension of neural networks. _NATO ASI Series F Computer and Systems Sciences_, 168:69-96, 1998.
* [42] S. Srinivas and F. Fleuret. Knowledge transfer with jacobian matching. In _International Conference on Machine Learning_, pages 4723-4731. PMLR, 2018.
* [43] E. Stein. _Singular integrals and differentiability properties of functions_, volume 2. Princeton university press, 1970.
* [44] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Measures of complexity: festschrift for alexey chervonenkis_, pages 11-30, 2015.
* [45] N. Vlassis, R. Ma, and W. Sun. Geometric deep learning for computational mechanics part i: Anisotropic hyperelasticity. _Computer Methods in Applied Mechanics and Engineering_, 371:113299, 2020.
* [46] N. Vlassis and W. Sun. Sobolev training of thermodynamic-informed neural networks for interpretable elasto-plasticity models with level set hardening. _Computer Methods in Applied Mechanics and Engineering_, 377:113695, 2021.
* [47] M. Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge university press, 2019.
* [48] P. Werbos. Approximate dynamic programming for real-time control and neural modeling. _Handbook of intelligent control_, 1992.

Supplementary Material

### Proofs of Theorems 1 and 2.

Proof of Theorem 1.: An element in \(\Phi\) can be represented as \(\phi=\bm{W}_{L+1}\sigma_{1}(\bm{W}_{L}\sigma_{1}(\ldots\sigma_{1}(\bm{W}_{1}\bm{x }+\bm{b}_{1})\ldots)+\bm{b}_{L})+b_{L+1}\). Therefore, an element in \(D\Phi\) can be represented as

\[\psi(\bm{x})=D_{i}\phi(\bm{x})= \bm{W}_{L+1}\sigma_{0}(\bm{W}_{L}\sigma_{1}(\ldots\sigma_{1}(\bm{ W}_{1}\bm{x}+\bm{b}_{1})\ldots)+\bm{b}_{L})\] \[\cdot\bm{W}_{L}\sigma_{0}(\ldots\sigma_{1}(\bm{W}_{1}\bm{x}+\bm{b }_{1})\ldots)\ldots\bm{W}_{2}\sigma_{0}(\bm{W}_{1}\bm{x}+\bm{b}_{1})(\bm{W}_{1 })_{i},\] (19)

where \(\bm{W}_{i}\in\mathbb{R}^{N_{i}\times N_{i-1}}\) (\((\bm{W})_{i}\) is \(i\)-th column of \(\bm{W}\)) and \(\bm{b}_{i}\in\mathbb{R}^{N_{i}}\) are the weight matrix and the bias vector in the \(i\)-th linear transform in \(\phi\), and \(\sigma_{0}(x)=\operatorname{sgn}(x)=1[x>0]\), which is the derivative of the ReLU function and \(\sigma_{0}(\bm{x})=\operatorname{diag}(\sigma_{0}(x_{i}))\). Denote \(W_{i}\) as the number of parameters in \(\bm{W}_{i},\bm{b}_{i}\), i.e., \(W_{i}=N_{i}N_{i-1}+N_{i}\).

Let \(\bm{x}\in\mathbb{R}^{d}\) be an input and \(\bm{\theta}\in\mathbb{R}^{W}\) be a parameter vector in \(\psi\). We denote the output of \(\psi\) with input \(\bm{x}\) and parameter vector \(\bm{\theta}\) as \(f(\bm{x},\bm{\theta})\). For fixed \(\bm{x}_{1},\bm{x}_{2},\ldots,\bm{x}_{m}\) in \(\mathbb{R}^{d}\), we aim to bound

\[K:=\left|\{(\operatorname{sgn}(f(\bm{x}_{1},\bm{\theta})),\ldots,\operatorname {sgn}(f(\bm{x}_{m},\bm{\theta}))):\bm{\theta}\in\mathbb{R}^{W}\}\right|.\] (20)

The proof is inspired by [4, Theorem 7]. For any partition \(\mathcal{S}=\{P_{1},P_{2},\ldots,P_{T}\}\) of the parameter domain \(\mathbb{R}^{W}\), we have \(K\leq\sum_{i=1}^{T}|\{(\operatorname{sgn}(f(\bm{x}_{1},\bm{\theta})),\ldots, \operatorname{sgn}(f(\bm{x}_{m},\bm{\theta}))):\bm{\theta}\in P_{i}\}|\). We choose the partition such that within each region \(P_{i}\), the functions \(f(\bm{x}_{j},\cdot)\) are all fixed polynomials of bounded degree. This allows us to bound each term in the sum using Lemma 1.

We define a sequence of sets of functions \(\{\mathbb{F}_{j}\}_{j=0}^{L}\) with respect to parameters \(\bm{\theta}\in\mathbb{R}^{W}\):

\[\mathbb{F}_{0}:=\{(\bm{W}_{1})_{i},\bm{W}_{1}\bm{x}+\bm{b}_{1}\}\] \[\mathbb{F}_{1}:=\{(\bm{W}_{1})_{i},\bm{W}_{2}\sigma_{0}(\bm{W}_{ 1}\bm{x}+\bm{b}_{1}),\bm{W}_{2}\sigma_{1}(\bm{W}_{1}\bm{x}+\bm{b}_{1})+\bm{b }_{2}\}\] \[\mathbb{F}_{2}:=\{(\bm{W}_{1})_{i},\bm{W}_{2}\sigma_{0}(\bm{W}_{ 1}\bm{x}+\bm{b}_{1}),\bm{W}_{3}\sigma_{0}(\bm{W}_{2}\sigma_{1}(\bm{W}_{1}\bm{ x}+\bm{b}_{1})+\bm{b}_{2}),\] \[\qquad\quad\quad\quad\bm{W}_{3}\sigma_{1}(\bm{W}_{2}\sigma_{1}( \bm{W}_{1}\bm{x}+\bm{b}_{1})+\bm{b}_{2})+\bm{b}_{3}\}\] \[\quad\vdots\] \[\mathbb{F}_{L}:=\{(\bm{W}_{1})_{i},\bm{W}_{2}\sigma_{0}(\bm{W}_{ 1}\bm{x}+\bm{b}_{1}),\ldots,\bm{W}_{L+1}\sigma_{0}(\bm{W}_{L}\sigma_{1}( \ldots\sigma_{1}(\bm{W}_{1}\bm{x}+\bm{b}_{1})\ldots)+\bm{b}_{L})\}.\] (21)

The partition of \(\mathbb{R}^{W}\) is constructed layer by layer through successive refinements denoted by \(\mathcal{S}_{0},\mathcal{S}_{1},\ldots,\mathcal{S}_{L}\). These refinements possess the following properties:

**1**. We have \(|\mathcal{S}_{0}|=1\), and for each \(n=1,\ldots,L\), we have \(\frac{|\mathcal{S}_{n}|}{|\mathcal{S}_{n-1}|}\leq 2\left(\frac{2emnN_{h}}{\sum_{i=1}^{n}W_{ i}}\right)^{\sum_{i=1}^{n}W_{i}}\).

**2**. For each \(n=0,\ldots,L-1\), each element \(S\) of \(\mathcal{S}_{n}\), when \(\bm{\theta}\) varies in \(S\), the output of each term in \(\mathbb{F}_{n}\) is a fixed polynomial function in \(\sum_{i=1}^{n}W_{i}\) variables of \(\bm{\theta}\), with a total degree no more than \(n+1\).

**3**. For each element \(S\) of \(\mathcal{S}_{L}\), when \(\bm{\theta}\) varies in \(S\), the \(h\)-th term in \(\mathbb{F}_{L}\) for \(h\in\{1,2,\ldots,L+1\}\) is a fixed polynomial function in \(W_{h}\) variables of \(\bm{\theta}\), with a total degree no more than \(1\).

We define \(\mathcal{S}_{0}=\{\mathbb{R}^{W}\}\), which satisfies properties 1,2 above, since \(\bm{W}_{1}\bm{x}_{j}+\bm{b}_{1}\) and \((\bm{W}_{1})_{i}\) are affine functions of \(\bm{W}_{1},\bm{b}_{1}\).

To define \(\mathcal{S}_{n}\), we use the last term of \(\mathbb{F}_{n-1}\) as inputs for the last two terms in \(\mathbb{F}_{n}\). Assuming that \(\mathcal{S}_{0},\mathcal{S}_{1},\ldots,\mathcal{S}_{n-1}\) have already been defined, we observe that the last two terms are new additions to \(\mathbb{F}_{n}\) when comparing it to \(\mathbb{F}_{n-1}\). Therefore, all elements in \(\mathbb{F}_{n}\) except the last two are fixed polynomial functions in \(W_{n}\) variables of \(\bm{\theta}\), with a total degree no greater than \(n\) when \(\bm{\theta}\) varies in \(S\in\mathcal{S}_{n}\). This is because \(\mathcal{S}_{n}\) is a finer partition than \(\mathcal{S}_{n-1}\).

We denote \(p_{\bm{x}_{j},n-1,S,k}(\bm{\theta})\) as the output of the \(k\)-th node in the last term of \(\mathbb{F}_{n-1}\) in response to \(\bm{x}_{j}\) when \(\bm{\theta}\in S\). The collection of polynomials

\[\{p_{\bm{x}_{j},n-1,S,k}(\bm{\theta}):j=1,\ldots,m,\ k=1,\ldots,N_{n}\}\]

can attain at most \(2\left(\frac{2emnN_{h}}{\sum_{i=1}^{n}W_{i}}\right)^{\sum_{i=1}^{n}W_{i}}\) distinct sign patterns when \(\bm{\theta}\in S\) due to Lemma 1 for sufficiently large \(m\). Therefore, we can divide \(S\) into \(2\left(\frac{2emnN_{h}}{\sum_{i=1}^{n}W_{i}}\right)^{\sum_{i=1}^{n}W_{i}}\) parts, each having the property that \(p_{\bm{x}_{j},n-1,S,k}(\bm{\theta})\) does not change sign within the subregion. By performing this for all \(S\in\mathcal{S}_{n-1}\), we obtain the desired partition \(\mathcal{S}_{n}\). This division ensures that the required property 1 is satisfied.

Additionally, since the input to the last two terms in \(\mathbb{F}_{n}\) is \(p_{\bm{x}_{j},n-1,S,k}(\bm{\theta})\), and we have shown that the sign of this input will not change in each region of \(\mathcal{S}_{n}\), it follows that the output of the last two terms in \(\mathbb{F}_{n}\) is also a polynomial without breakpoints in each element of \(\mathcal{S}_{n}\). Therefore, the required property 2 is satisfied.

In the context of DNNs, the last layer is characterized by all terms containing the activation function \(\sigma_{0}\). Consequently, for any element \(S\) of the partition \(\mathcal{S}_{L}\), when the vector of parameters \(\bm{\theta}\) varies within \(S\), the \(h\)-th term in \(\mathbb{F}_{L}\) for \(h\in\{1,2,\ldots,L+1\}\) can be expressed as a polynomial function of at most degree \(1\), which depends on at most \(W_{h}\) variables of \(\bm{\theta}\). Hence, the required property 3 is satisfied.

Due to property 3, we multiply all the terms in \(\mathbb{F}_{L}\) and obtain a term in \(D\Phi\). Hence, the output of each term in \(D\Phi\) is a polynomial function in \(\sum_{i=1}^{L+1}W_{i}\) variables of \(\bm{\theta}\in S\in\mathcal{S}_{L}\), of total degree no more than \(L+1\). Therefore, for each \(S\in\mathcal{S}_{L}\) we have \(|\{(\operatorname{sgn}(f(\bm{x}_{1},\bm{\theta})),\ldots,\operatorname{sgn} (f(\bm{x}_{m},\bm{\theta}))):\bm{\theta}\in S\}|\leq 2\left(2em(L+1)/\sum_{i=1}^{L+1}W_{i} \right)^{\sum_{i=1}^{L+1}W_{i}}\). Then

\[K\leq 2\left(2em(L+1)/\sum_{i=1}^{L+1}W_{i}\right)^{\sum_{i=1}^{L+1}W_ {i}}\cdot\prod_{n=1}^{L}2\left(\frac{2emnN_{n}}{\sum_{i=1}^{n}W_{i}}\right)^{ \sum_{i=1}^{n}W_{i}}\leq\prod_{n=1}^{L+1}2\left(\frac{2emN_{n}}{\sum_{i=1}^{n }W_{i}}\right)^{\sum_{i=1}^{n}W_{i}}\] \[\leq 2^{L+1}\left(\frac{2em(L+2)(L+1)N}{2U}\right)^{U}\] (22)

where \(U:=\sum_{n=1}^{L+1}\sum_{i=1}^{n}W_{i}=\bm{O}(N^{2}L^{2})\), \(N\) is the width of the network, and the last inequality is due to weighted AM-GM. For the definition of the VC-dimension, we have

\[2^{\text{VCdim}(D\Phi)}\leq 2^{L+1}\left(\frac{e\text{VCdim}(D\Phi)(L+1)(L+2)N}{ U}\right)^{U}.\] (23)

Due to Lemma 2, we obtain that

\[\text{VCdim}(D\Phi)\leq L+1+U\log_{2}[2(L+1)(L+2)\log_{2}(L+1)(L+2)]=\bm{O}(N ^{2}L^{2}\log_{2}L\log_{2}N)\] (24)

since \(U=\bm{O}(N^{2}L^{2})\). 

Proof of Theorem 2.: Denote \(D\Phi_{\mathcal{N}}:=\{\eta(\bm{x},y):\eta(\bm{x},y)=\psi(\bm{x})-y,\psi\in D \Phi,(\bm{x},y)\in\mathbb{R}^{d+1}\}\). Based on the definition of VC-dimension and pseudo-dimension, we have that

\[\text{Pdim}(D\Phi)\leq\text{VCdim}(D\Phi_{\mathcal{N}}).\] (25)

For the \(\text{VCdim}(D\Phi_{\mathcal{N}})\), it can be bounded by \(O(N^{2}L^{2}\log_{2}L\log_{2}N)\). The proof is similar to that for the estimate of \(\text{VCdim}(D\Phi)\) as given in Theorem 1. 

We establish that \(\text{Pdim}(D\Phi)\leq\text{VCdim}(D\Phi_{\mathcal{N}})\), where \(\Phi_{\mathcal{N}}\) represents DNNs with \(N+1\) width and \(L+1\) depth. This implies that \(\text{Pdim}(D\Phi)\) is upper bounded by \(\bar{C}(N+1)^{2}(L+1)^{2}\log_{2}(L+1)\log_{2}(N+1)\leq 64\bar{C}N^{2}L^{2} \log_{2}L\log_{2}N\). Therefore, we conclude that \(64\bar{C}\geq\hat{C}\).

### Proof of Theorem 3

#### 7.2.1 Propositions of Sobolev spaces and ReLU neural networks

The following two lemmas estimate the Sobolev norms and Sobolev semi-norms for the composition and product, which will be used in later proof.

**Lemma 3** ([18, Corollary B.5]).: _Let \(d,m\in\mathbb{N}_{+}\) and \(\Omega_{1}\subset\mathbb{R}^{d}\) and \(\Omega_{2}\subset\mathbb{R}^{m}\) both be open, bounded, and convex. Then for \(\bm{f}\in W^{1,\infty}(\Omega_{1},\mathbb{R}^{m})\) and \(g\in W^{1,\infty}(\Omega_{2})\) with \(\operatorname{ran}\bm{f}\subset\Omega_{2}\), we have_

\[\|g\circ\bm{f}\|_{W^{1,\infty}(\Omega_{2})}\leq\sqrt{d}m\max\{\|g\|_{L^{\infty}( \Omega_{2})},|g|_{W^{1,\infty}(\Omega_{2})}|\bm{f}|_{W^{1,\infty}(\Omega_{1}, \mathbb{R}^{m})}\}.\]

**Lemma 4** ([18, Corollary B.6]).: _Let \(d\in\mathbb{N}_{+}\) and \(\Omega\subset\mathbb{R}^{d}\). Then for \(f,g\in W^{1,\infty}(\Omega)\), we have_

\[\|gf\|_{W^{1,\infty}(\Omega)}\leq\|g\|_{L^{\infty}(\Omega)}|f|_{W^{1,\infty}( \Omega)}+\|f\|_{L^{\infty}(\Omega)}|g|_{W^{1,\infty}(\Omega)}.\]

Then we collect and establish some propositions for ReLU neural networks.

**Proposition 2** ([28, Proposition 4.3]).: _Given any \(N,L\in\mathbb{N}_{+}\) and \(\delta\in\left(0,\frac{1}{3K}\right]\) for \(K=\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor\), there exists a \(\sigma_{1}\)-NN \(\phi\) with the width \(4N+5\) and depth \(4L+4\) such that_

\[\phi(x)=k,x\in\left[\frac{k}{K},\frac{k+1}{K}-\delta\cdot 1_{k<K-1}\right],\;k= 0,1,\ldots,K-1.\]

**Proposition 3**.: _[_28_, Proposition 4.4]_ _Given any \(N,L,s\in\mathbb{N}_{+}\) and \(\xi_{i}\in[0,1]\) for \(i=0,1,\ldots N^{2}L^{2}-1\), there exists a \(\sigma_{1}\)-NN \(\phi\) with the width \(16s(N+1)\log_{2}(8N)\) and depth \((5L+2)\log_{2}(4L)\) such that_

_1. \(|\phi(i)-\xi_{i}|\leq N^{-2s}L^{-2s}\) for \(i=0,1,\ldots N^{2}L^{2}-1\)._

_2. \(0\leq\phi(x)\leq 1\), \(x\in\mathbb{R}\)._

**Proposition 4**.: _For any \(N,L\in\mathbb{N}_{+}\) and \(a>0\), there is a \(\sigma_{1}\)-NN \(\phi\) with the width \(15N\) and depth \(2L\) such that \(\|\phi\|_{W^{1,\infty}((-a,a)^{2})}\leq 12a^{2}\) and_

\[\|\phi(x,y)-xy\|_{W^{1,\infty}((-a,a)^{2})}\leq 6a^{2}N^{-L}.\] (26)

_Furthermore,_

\[\phi(0,y)=\frac{\partial\phi(0,y)}{\partial y}=0,\;y\in(-a,a).\] (27)

Proof.: We first need to construct a neural network to approximate \(x^{2}\) on \((-1,1)\), and the idea is similar with [23, Lemma 3.2] and [28, Lemma 5.1]. The reason we do not use [23, Lemma 3.4] and [28, Lemma 4.2] directly is that constructing \(\phi(x,y)\) by translating a neural network in \(W^{1,\infty}[0,1]\) will lose the proposition of \(\phi(0.y)=0\). Here we need to define teeth functions \(T_{i}\) on \(\widetilde{x}\in[-1,1]\):

\[T_{1}(\widetilde{x})=\begin{cases}2|\widetilde{x}|,&|\widetilde{x}|\leq\frac{ 1}{2},\\ 2(1-|\widetilde{x}|),&|\widetilde{x}|>\frac{1}{2},\end{cases}\]

and

\[T_{i}=T_{i-1}\circ T_{1},\quad\text{ for }i=2,3,\cdots.\]

Define

\[\widetilde{\psi}(\widetilde{x})=\widetilde{x}-\sum_{i=1}^{s}\frac{T_{i}( \widetilde{x})}{2^{2i}},\]

According to [23, Lemma 3.2] and [28, Lemma 5.1], we know \(\psi\) is a neural network with the width \(5N\) and depth \(2L\) such that \(\|\widetilde{\psi}(\widetilde{x})\|_{W^{1,\infty}((-1,1))}\leq 2\), \(\|\widetilde{\psi}(\widetilde{x})-\widetilde{x}^{2}\|_{W^{1,\infty}((-1,1))} \leq N^{-L}\) and \(\psi(0)=0\).

By setting \(x=a\widetilde{x}\in(-a,a)\) for \(\widetilde{x}\in(-1,1)\), we define

\[\psi(x)=a^{2}\widetilde{\psi}\left(\frac{x}{a}\right).\]

Note that \(x^{2}=a^{2}\left(\frac{x}{a}\right)^{2}\), we have

\[\|\psi(x)-x^{2}\|_{\mathcal{W}^{1,\infty}(-a,a)} =a^{2}\left\|\widetilde{\psi}\left(\frac{x}{a}\right)-\left(\frac{ x}{a}\right)^{2}\right\|_{\mathcal{W}^{1,\infty}((-a,a))}\] \[\leq a^{2}N^{-L},\]

and \(\psi(0)=0\), which will be used to prove Eq. (27).

Then we can construct \(\phi(x,y)\) as

\[\phi(x,y)=2\left[\psi\left(\frac{|x+y|}{2}\right)-\psi\left(\frac{|x|}{2} \right)-\psi\left(\frac{|y|}{2}\right)\right]\] (28)where \(\phi(x)\) is a neural network with the width \(15N\) and depth \(2L\) such that \(\|\phi\|_{W^{1,\infty}((-a,a)^{2})}\leq 12a^{2}\) and

\[\|\phi(x,y)-xy\|_{W^{1,\infty}((-a,a)^{2})}\leq 6a^{2}N^{-L}.\] (29)

For the last equation Eq. (27) is due to \(\phi(x,y)\) in the proof can be read as Eq. (28) with \(\psi(0)=0\). 

**Proposition 5**.: _For any \(N,L,s\in\mathbb{N}_{+}\)with \(s\geq 2\), there exists a \(\sigma_{1}\)-NN \(\phi\) with the width \(9(N+1)+s-1\) and depth \(14s(s-1)L\) such that \(\|\phi\|_{W^{1,\infty}((0,1)^{s})}\leq 18\) and_

\[\|\phi(\bm{x})-x_{1}x_{2}\cdots x_{s}\|_{\mathcal{W}^{1,\infty}((0,1)^{s})} \leq 10(s-1)(N+1)^{-7sL}.\] (30)

_Furthermore, for any \(i=1,2,\ldots,s\), if \(x_{i}=0\), we will have_

\[\phi(x_{1},x_{2},\ldots,x_{i-1},0,x_{i+1},\ldots,x_{s})=\frac{\partial\phi(x_ {1},x_{2},\ldots,x_{i-1},0,x_{i+1},\ldots,x_{s})}{\partial x_{j}}=0,\;i\neq j.\] (31)

Proof.: The proof of the first inequality Eq. (30) can be found in [23, Lemma 3.5]. The proof of Eq. (31) can be obtained via induction. For \(s=2\), based on Proposition 4, we know there is a neural network \(\phi_{2}\) satisfied Eq. (31).

Now assume that for any \(i\leq n-1\), there is a neural network \(\phi_{i}\) satisfied Eq. (31). \(\phi_{n}\) in [23] is constructed as

\[\phi_{n}(x_{1},x_{2},\ldots,x_{n})=\phi_{2}(\phi_{n-1}(x_{1},x_{2},\ldots,x_{n -1}),\sigma(x_{n})),\] (32)

which satisfies Eq. (30). Then \(\phi_{n}(x_{1},x_{2},\ldots,x_{i-1},0,x_{i+1},\ldots,x_{n})=0\) for any \(i=1,2,\ldots,n\). For \(i=n\), we have

\[\frac{\phi(x_{1},x_{2},\ldots,0)}{\partial x_{j}}=\underbrace{\frac{\partial \phi_{2}(\phi_{n-1}(x_{1},x_{2},\ldots,x_{n-1}),0)}{\partial\phi_{n-1}(x_{1},x _{2},\ldots,x_{n-1})}}_{=0,\;\text{by the property of $\phi_{2}$}.}\cdot\frac{\partial\phi_{n-1}(x_{1},x_{2},\ldots,x_{n-1})}{ \partial x_{j}}=0.\] (33)

For \(i<n\) and \(j<n\), we have

\[\frac{\phi(x_{1},x_{2},\ldots,x_{i-1},0,x_{i+1},\ldots,x_{n})}{ \partial x_{j}}\] \[= \frac{\partial\phi_{2}(\phi_{n-1}(x_{1},x_{2},\ldots,x_{i-1},0,x _{i+1},\ldots,x_{n-1}),\sigma(x_{n}))}{\partial\phi_{n-1}(x_{1},\ldots,0,x_{i +1},\ldots,x_{n-1})}\cdot\underbrace{\frac{\partial\phi_{n-1}(x_{1},\ldots,0,x _{i+1},\ldots,x_{n-1})}{\partial x_{j}}}_{=0,\;\text{via induction}.}=0.\] (34)

For \(i<n\) and \(j=n\), we have

\[\frac{\phi(x_{1},x_{2},\ldots,x_{i-1},0,x_{i+1},\ldots,x_{n})}{ \partial x_{n}}\] \[= \underbrace{\frac{\partial\phi_{2}(\phi_{n-1}(x_{1},x_{2},\ldots, x_{i-1},0,x_{i+1},\ldots,x_{n-1}),\sigma(x_{n}))}{\partial\sigma(x_{n})}}_{=0,\;\text{by the property of $\phi_{2}$}.}\cdot\frac{\mathrm{d}\sigma(x_{n})}{ \mathrm{d}x_{n}}=0.\] (35)

Therefore, Eq. (31) is valid. 

**Proposition 6** ([23, Proposition 3.6]).: _For any \(N,L,s\in\mathbb{N}_{+}\) and \(|\bm{\alpha}|\leq s\), there is a \(\sigma_{1}\)-NN \(\phi\) with the width \(9(N+1)+s-1\) and depth \(14s^{2}L\) such that \(\|\phi\|_{W^{1,\infty}((0,1)^{d})}\leq 18\) and_

\[\|\phi(\bm{x})-\bm{x}^{\bm{\alpha}}\|_{W^{1,\infty}((0,1)^{d})}\leq 10s(N+1)^{-7sL}.\] (36)

**Proposition 7** ([39, Proposition 1]).: _Given a sequence of the neural network \(\{p_{i}\}_{i=1}^{M}\), and each \(p_{i}\) is a \(\sigma_{1}\)-NN from \(\mathbb{R}\to\mathbb{R}\) with the width \(N\) and depth \(L_{i}\), then \(\sum_{i=1}^{M}p_{i}\) is a \(\sigma_{1}\)-NN with the width \(N+4\) and depth \(\sum_{i=1}^{M}L_{i}\)._

We present the proof of Proposition 1 below.

Proof of Proposition 1.: First, we construct \(g_{1}\) and \(g_{2}\) by neural networks in \([0,1]\). Note that \(\lfloor L^{2/d}\rfloor\leq L^{2/d}\leq\left(\lfloor L^{1/d}\rfloor+1\right)^{2}\). We first construct a \(\sigma_{1}\)-NN in the small set \(\left[0,\lfloor N^{1/d}\rfloor\lfloor L^{2/d}\rfloor\right]\). It is easy to check there is a neural network \(\hat{\psi}\) with the width \(4\) and one layer such as

\[\hat{\psi}(x):=\begin{cases}1,&x\in\left[\frac{1}{8K},\frac{3}{8K}\right]\\ 4K\left(x-\frac{1}{8K}\right),&x\in\left[\frac{1}{8K},\frac{3}{8K}\right]\\ -4K\left(x-\frac{7}{8K}\right),&x\in\left[\frac{5}{8K},\frac{7}{8K}\right]\\ 0,&\text{Otherwise}.\end{cases}\] (37)

Hence, we have a network \(\psi_{1}\) with the width \(4\lfloor N^{1/d}\rfloor\) and one layer such as

\[\psi_{1}(x):=\sum_{i=0}^{\lfloor N^{1/d}\rfloor-1}\hat{\psi}\left(x-\frac{i}{K }\right).\]

Next, we construct \(\psi_{i}\) for \(i=2,3,4\) based on the symmetry and periodicity of \(g_{i}\). \(\psi_{2}\) is the function with period \(\frac{2}{\lfloor N^{1/d}\rfloor\lfloor L^{2/d}\rfloor}\) in \(\left[0,\frac{1}{\lfloor L^{2/d}\rfloor}\right]\), and each period is a hat function with gradient 1. \(\psi_{3}\) is the function with period \(\frac{2}{\lfloor L^{2/d}\rfloor}\) in \(\left[0,\frac{\lfloor L^{1/d}\rfloor+1}{\lfloor L^{2/d}\rfloor}\right]\), and each period is a hat function with gradient 1. \(\psi_{4}\) is the function with period \(\frac{2(\lfloor L^{1/d}\rfloor+1)}{\lfloor L^{2/d}\rfloor}\) in \(\left[0,\frac{\left(\lfloor L^{1/d}\rfloor+1\right)^{2}}{\lfloor L^{2/d} \rfloor}\right]\), and each period is a hat function with gradient 1. The schematic diagram is in Fig. 3 (The diagram is shown the case for \(\lfloor N^{1/d}\rfloor\) and \(\lfloor L^{1/d}\rfloor+1\) is a even integer.)

Note that \(\psi_{2}\circ\psi_{3}\circ\psi_{4}(x)\) is the function with period \(\frac{2}{\lfloor N^{1/d}\rfloor\lfloor L^{2/d}\rfloor}\) in \([0,1]\subset\left[0,\frac{\left(\lfloor L^{1/d}\rfloor+1\right)^{2}}{\lfloor L ^{2/d}\rfloor}\right]\), and each period is a hat function with gradient 1. Then function \(\psi_{1}\circ\psi_{2}\circ\psi_{3}\circ\psi_{4}(x)\) is obtained by repeating reflection \(\psi_{1}\) in \(\left[0,\frac{\left(\lfloor L^{1/d}\rfloor+1\right)^{2}}{\lfloor L^{2/d} \rfloor}\right]\), which is the function we want.

Similar with \(\psi_{1}\), \(\psi_{2}\) is a network with \(4\lfloor N^{1/d}\rfloor\) width and one layer. Due to Proposition 7, we know that \(\psi_{3}\) and \(\psi_{4}\) is a network with \(7\) width and \(\lfloor L^{1/d}\rfloor+1\) depth. Hence

\[\psi(x):=\psi_{1}\circ\psi_{2}\circ\psi_{3}\circ\psi_{4}(x)\] (38)

is a network with \(4\lfloor N^{1/d}\rfloor\) width and \(2\lfloor L^{1/d}\rfloor+4\) depth and \(g_{1}=\psi\left(x+\frac{1}{8K}\right)\) and \(g_{1}=\psi\left(x+\frac{5}{8K}\right)\).

Now we can construct \(g_{\bm{m}}\) for \(m\in\{1,2\}^{d}\) based on Proposition 5: There is a neural network \(\phi_{\text{prod}}\) with the width \(9(N+1)+d-1\) and depth \(14d(d-1)nL\) such that \(\|\phi_{\text{prod}}\|_{\mathcal{W}^{1,\infty}((0,1)^{d})}\leq 18\) and

\[\|\phi_{\text{prod}}(\bm{x})-x_{1}x_{2}\cdots x_{d}\|_{\mathcal{W}^{1,\infty} ((0,1)^{d})}\leq 10(d-1)(N+1)^{-7dnL}.\]Then denote \(\phi_{\bm{m}}(\bm{x}):=\phi_{\text{prod}}(g_{m_{1}},g_{m_{2}},\ldots,g_{m_{d}})\) which is a neural network with the width smaller than \((9+d)(N+1)+d-1\) and depth smaller than \(15d(d-1)nL\). Furthermore, due to Lemma 3, we have

\[\left\|\phi_{\bm{m}}(\bm{x})-g_{\bm{m}}(\bm{x})\right\|_{\mathcal{W }^{1,\infty}((0,1)^{d})}\leq d^{\frac{3}{2}}\left\|\phi_{\text{prod}}(\bm{x})-x_{1}x_{2}\cdots x _{d}\right\|_{L^{\infty}((0,1)^{d})}\] \[+d^{\frac{3}{2}}\left\|\phi_{\text{prod}}(\bm{x})-x_{1}x_{2}\cdots x _{d}\right\|_{\mathcal{W}^{1,\infty}((0,1)^{d})}\left|\psi\right|_{W^{1,\infty }(0,1)}\] \[\leq d^{\frac{3}{2}}10(d-1)(N+1)^{-7ndL}\left(1+4\lfloor N^{1/d} \rfloor^{2}\lfloor L^{2/d}\rfloor\right)\] \[\leq 50d^{\frac{5}{2}}(N+1)^{-4dnL},\] (39)

where the last inequality is due to

\[\frac{\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor}{(N+1)^{3dnL}} \leq\frac{N^{2}L^{2}}{(N+1)^{3dnL}}\leq\frac{L^{2}}{(N+1)^{3dnL-2}}\leq\frac{ L^{2}}{2^{dnL}}\leq 1.\]

In the final of this subsection, we establish three lemmas for \(\{\Omega_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\), \(\{g_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) and \(\{\phi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) defined in Subsection 3.1.

**Lemma 5**.: _For \(\{\Omega_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) defined in Definition 5, we have_

\[\bigcup_{\bm{m}\in\{1,2\}^{d}}\Omega_{\bm{m}}=[0,1]^{d}.\]

Proof.: We prove this lemma via induction. \(d=1\) is valid due to \(\Omega_{1}\cup\Omega_{2}=[0,1]\). Assume that the lemma is true for \(d-1\), then

\[\bigcup_{\bm{m}\in\{1,2\}^{d}}\Omega_{\bm{m}}= [0,1]^{d}=\bigcup_{\bm{m}\in\{1,2\}^{d-1}}\Omega_{\bm{m}}\times \Omega_{1}+\bigcup_{\bm{m}\in\{1,2\}^{d-1}}\Omega_{\bm{m}}\times\Omega_{2}\] \[=\left([0,1]^{d-1}\times\Omega_{1}\right)\bigcup\left([0,1]^{d-1} \times\Omega_{2}\right)=[0,1]^{d},\] (40)

hence the case of \(d\) is valid, and we finish the proof of the lemma.

**Lemma 6**.: \(\{g_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) _defined in Definition 6 satisfies:_

_(i):_ \(\sum_{\bm{m}\in\{1,2\}^{d}}g_{\bm{m}}(\bm{x})=1\) _for every_ \(x\in[0,1]^{d}\)_._

_(ii):_ \(\mathrm{supp}\ g_{\bm{m}}\cap[0,1]^{d}\subset\Omega_{\bm{m}}\)_, where_ \(\Omega_{\bm{m}}\) _is defined in Definition_ 5_._

_(ii): For any_ \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\) _and_ \(\bm{x}=(x_{1},x_{2},\ldots,x_{d})\in[0,1]^{d}\backslash\Omega_{\bm{m}}\)_, there exists_ \(j\) _such as_ \(g_{m_{j}}(x_{j})=0\) _and_ \(\frac{\mathrm{d}g_{m_{j}}(x_{j})}{\mathrm{d}x_{j}}=0\)_._

Proof.: (i) can be proved via induction as Lemma 5, and we leave it to readers.

As for (ii) and (iii), without loss of generality, we show the proof for \(\bm{m}_{*}:=(1,1,\ldots,1)\). For any \(\bm{x}\in[0,1]^{d}\backslash\Omega_{\bm{m}_{*}}\), there is \(x_{j}\in[0,1]\backslash\Omega_{1}\). Then \(g_{1}(x_{j})=0\) and \(g_{\bm{m}_{*}}(\bm{x})=\prod_{j=1}^{d}g_{1}(x_{j})=0\), therefore \(\mathrm{supp}\ g_{\bm{m}_{*}}\cap[0,1]^{d}\subset\Omega_{\bm{m}_{*}}\). Furthermore, \(\frac{\mathrm{d}g_{m_{j}}(x_{j})}{\mathrm{d}x_{j}}=0\) for \(x_{j}\in[0,1]\in\Omega_{1}\) due to the definition of \(g_{1}\) (Definition 6), then we finish this proof. 

The following lemma demonstrates that \(\phi_{\bm{m}}\), as defined in Proposition 1, can restrict the Sobolev norm of the entire space to \(\Omega_{\bm{m}}\).

**Lemma 7**.: _For any \(\chi(\bm{x})\in W^{1,\infty}((0,1)^{d})\), denote_

\[M=\max\{\|\chi\|_{W^{1,\infty}((0,1)^{d})},\|\phi_{\bm{m}}\|_{W^{1,\infty}((0, 1)^{d})}\},\]

_then we have_

\[\|\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})\|_{W^{1,\infty}((0,1)^{ d})}= \|\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})\|_{W^{1,\infty}(\Omega_{ \bm{m}})}\] \[\|\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})-\phi_{M}(\phi_{\bm{m}}( \bm{x}),\chi(\bm{x}))\|_{W^{1,\infty}((0,1)^{d})}= \|\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})-\phi_{M}(\phi_{\bm{m}}( \bm{x}),\chi(\bm{x}))\|_{W^{1,\infty}(\Omega_{\bm{m}})}\] (41)

_for any \(\bm{m}\in\{1,2\}^{d}\), where \(\phi_{\bm{m}}(\bm{x})\) and \(\Omega_{\bm{m}}\) is defined in Proposition 1 and Definition. 5, and \(\phi_{M}\) is from Proposition 4 (choosing \(a=M\) in the proposition)._

Proof.: For the first equality, we only need to show that

\[\|\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})\|_{W^{1,\infty}((0,1)^{d}\backslash \Omega_{\bm{m}})}=0.\] (42)

According to the Proposition 1, we have \(\phi_{\bm{m}}(\bm{x})=\phi_{\text{prod}}(g_{m_{1}},g_{m_{2}},\ldots,g_{m_{d}})\), and for any \(\bm{x}=(x_{1},x_{2},\ldots,x_{d})\in(0,1)^{d}\backslash\Omega_{\bm{m}}\), there is \(m_{j}\) such as \(g_{m_{j}}(x_{j})=0\) and \(\frac{\mathrm{d}g_{m_{j}}(x_{j})}{\mathrm{d}x_{j}}=0\) due to Lemma 6. Based on Eq. (31) in Proposition 5, we have

\[\phi_{\bm{m}}(\bm{x})=\frac{\partial\phi_{\bm{m}}(\bm{x})}{\partial x_{s}}=0, \ x\in(0,1)^{d}\backslash\Omega_{\bm{m}},s\neq j.\]

Furthermore,

\[\frac{\partial\phi_{\bm{m}}(\bm{x})}{\partial x_{j}}=\frac{\partial\phi_{\text {prod}}(g_{m_{1}},g_{m_{2}},\ldots,g_{m_{d}})}{\partial g_{m_{j}}}\frac{ \mathrm{d}g_{m_{j}}(x_{j})}{\mathrm{d}x_{j}}=0.\] (43)

Hence we have

\[|\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})|+\sum_{q=1}^{d}\left|\frac{\partial \left[\phi_{\bm{m}}(\bm{x})\cdot\chi(\bm{x})\right]}{\partial x_{q}}\right|=0\] (44)

for all \(\bm{x}\in(0,1)^{d}\backslash\Omega_{\bm{m}}\).

Similarly, for the second equality in this lemma, we have

\[|\phi_{M}(\phi_{\bm{m}}(\bm{x}),\chi(\bm{x}))|+\sum_{q=1}^{d} \left|\frac{\partial\left[\phi_{M}(\phi_{\bm{m}}(\bm{x}),\chi(\bm{x}))\right]}{ \partial x_{q}}\right|\] \[= |\phi_{M}(0,\chi(\bm{x}))|+\sum_{q=1}^{d}\left[\left|\frac{ \partial\left[\phi_{M}(0,\chi(\bm{x}))\right]}{\partial\chi(\bm{x})}\cdot\frac {\partial\chi(\bm{x})}{\partial x_{q}}\right|+\left|\frac{\partial\left[\phi_{M }(\phi_{\bm{m}}(\bm{x}),\chi(\bm{x}))\right]}{\partial\phi_{\bm{m}}(\bm{x})} \cdot\frac{\partial\phi_{\bm{m}}(\bm{x})}{\partial x_{q}}\right|\right]\] \[= 0,\] (45)for all \(\bm{x}\in(0,1)^{d}\backslash\Omega_{\bm{m}}\) based on

\[\phi_{M}(0,y)=\frac{\partial\phi_{M}(0,y)}{\partial y}=0,\;y\in(-M,M),\]

and \(\frac{\partial\phi_{\bm{m}}(\bm{x})}{\partial x_{q}}=0\). Hence we finish our proof. 

2.2 An approximation of functions in Sobolev spaces based on the Bramble-Hilbert Lemma [7, Lemma 4.3.8]

In this subsection, we establish \(\{f_{K,\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) as mentioned in Subsection 3.1, which is presented in Theorem 6. To prove this result, we build upon the work of [18], which leverages the average Taylor polynomials and the Bramble-Hilbert Lemma to approximate functions in Sobolev spaces.

Before we show Theorem 6, we define subsets of \(\Omega_{\bm{m}}\) for simplicity notations.

For any \(\bm{m}\in\{1,2\}^{d}\), we define

\[\Omega_{\bm{m},\bm{i}}:=[0,1]^{d}\cap\prod_{j=1}^{d}\left[\frac{2i_{j}-1_{m_{j }\leq 2}}{2K},\frac{3+4i_{j}-2\cdot 1_{m_{j}\leq 2}}{4K}\right]\] (46)

\(\bm{i}=(i_{1},i_{2},\ldots,i_{d})\in\{0,1\ldots,K\}^{d}\), and it is easy to check \(\bigcup_{\bm{i}\in\{0,1\ldots,K\}^{d}}\Omega_{\bm{m},\bm{i}}=\Omega_{\bm{m}}\).

**Theorem 6**.: _Let \(K\in\mathbb{N}_{+}\) and \(n\geq 2\). Then for any \(f\in W^{n,\infty}((0,1)^{d})\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\) and \(\bm{m}\in\{1,2\}^{d}\), there exist piece-wise polynomials function \(f_{K,\bm{m}}=\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}}(\bm{x})\bm{ x}^{\bm{\alpha}}\) on \(\Omega_{\bm{m}}\) (Definition 5) with the following properties:_

\[\|f-f_{K,\bm{m}}\|_{W^{1,\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-(n-1)},\] \[\|f-f_{K,\bm{m}}\|_{L^{\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-n}.\] (47)

_Furthermore, \(g_{f,\bm{\alpha},\bm{m}}(\bm{x}):\Omega_{\bm{m}}\to\mathbb{R}\) is a constant function with on each \(\Omega_{\bm{m},\bm{i}}\) for \(\bm{i}\in\{0,1\ldots,K\}^{d}\). And_

\[|g_{f,\bm{\alpha},\bm{m}}(\bm{x})|\leq C_{2}(n,d)\] (48)

_for all \(\bm{x}\in\Omega_{\bm{m}}\), where \(C_{1}\) and \(C_{2}\) are constants independent with \(K\)._

This proof is similar to that of [18, Lemma C.4.], but we provide detailed proof as follows for readability. Before the proof, we must introduce the partition of unity, average Taylor polynomials, and a lemma.

**Definition 7** (The partition of unity).: _Let \(d,K\in\mathbb{N}_{+}\), then_

\[\Psi=\big{\{}h_{\bm{i}}:\bm{i}\in\{0,1,\ldots,K\}^{d}\big{\}}\]

_with \(h_{\bm{i}}:\mathbb{R}^{d}\to\mathbb{R}\) for all \(\bm{i}\in\{0,1,\ldots,K\}^{d}\) is called the partition of unity \([0,1]^{d}\) if it satisfies (i): \(0\leq h_{\bm{i}}(\bm{x})\leq 1\) for every \(h_{\bm{i}}\in\Psi\). (ii): \(\sum_{h_{\bm{i}}\in\Psi}h_{\bm{i}}=1\) for every \(x\in[0,1]^{d}\)._

**Definition 8**.: _Let \(n\geq 1\) and \(f\in W^{n,\infty}((0,1)^{d})\), \(\bm{x}_{0}\in((0,1)^{d})\) and \(r>0\) such that for the ball \(B(\bm{x}_{0}):=B(\bm{x}_{0})_{r,|\cdot|}\) which is a compact subset of \(((0,1)^{d})\). The corresponding Taylor polynomial of order \(n\) of \(f\) averaged over \(B\) is defined for_

\[Q^{n}f(x):=\int_{B}T^{n}_{\bm{y}}f(\bm{x})b_{r}(\bm{y})\,\mathrm{d}\bm{y}\] (49)

_where_

\[T^{n}_{\bm{y}}f(\bm{x}) :=\sum_{|\bm{\alpha}|\leq n-1}\frac{1}{\bm{\alpha}!}D^{\bm{ \alpha}}f(\bm{y})(\bm{x}-\bm{y})^{\bm{\alpha}},\] \[b_{r}(\bm{x}) :=\begin{cases}\frac{1}{c_{r}}e^{-\left(1-(|\bm{x}-\bm{x}_{0}|/r)^ {2}\right)^{-1}},&|\bm{x}-\bm{x}_{0}|<r,\\ 0,&|\bm{x}-\bm{x}_{0}|\leq r,\end{cases}\] \[c_{r} =\int_{\mathbb{R}^{d}}e^{-\left(1-(|\bm{x}-\bm{x}_{0}|/r)^{2} \right)^{-1}}\,\mathrm{d}x.\] (50)

**Lemma 8**.: _Let \(n\geq 1\) and \(f\in W^{n,\infty}((0,1)^{d})\), \(\bm{x}_{0}\in\Omega\) and \(r>0\) such that for the ball \(B(\bm{x}_{0}):=B_{r,\left\lvert{\cdot}\right\rvert}(\bm{x}_{0})\) which is a compact subset of \(((0,1)^{d})\). The corresponding Taylor polynomial of order \(n\) of \(f\) averaged over \(B\) can be read as_

\[Q^{n}f(\bm{x})=\sum_{\left\lvert{\bm{\alpha}}\right\rvert\leq n-1}c_{f,\bm{ \alpha}}\bm{x}^{\bm{\alpha}}.\]

_Furthermore,_

\[\left\lvert{c_{f,\bm{\alpha}}}\right\rvert\leq C_{2}(n,d)\|f\|_{W^{n-1,\infty }(B)}.\] (51)

_where \(C_{2}(n,d)=\sum_{\left\lvert{\bm{\alpha}}+\bm{\beta}\right\rvert\leq n-1} \frac{1}{\alpha^{\prime}\bm{\beta}!}\)._

Proof.: Based on [18, Lemma B.9.], \(Q^{n}f(x)\) can be read as

\[Q^{n}f(\bm{x})=\sum_{\left\lvert{\bm{\alpha}}\right\rvert\leq n-1}c_{f,\bm{ \alpha}}\bm{x}^{\bm{\alpha}}\] (52)

where

\[c_{f,\bm{\alpha}}=\sum_{\left\lvert{\bm{\alpha}}+\bm{\beta}\right\rvert\leq n -1}\frac{1}{(\bm{\beta}+\bm{\alpha})!}a_{\bm{\beta}+\bm{\alpha}}\int_{B}D^{ \bm{\alpha}+\bm{\beta}}f(\bm{x})\bm{y}^{\bm{\beta}}b_{r}(\bm{y})\,\mathrm{d} \bm{y}\] (53)

for \(a_{\bm{\beta}+\bm{\alpha}}\leq\frac{(\bm{\alpha}+\bm{\beta})!}{\bm{\alpha}! \bm{\beta}!}\). Note that

\[\left\lvert{\int_{B}D^{\bm{\alpha}+\bm{\beta}}f(\bm{x})\bm{y}^{\bm{\beta}}b_{ r}(\bm{y})\,\mathrm{d}\bm{y}}\right\rvert\leq\|f\|_{W^{n-1,\infty}(B)}\|b_{r}(x)\|_{L ^{1}(B)}=\|f\|_{W^{n-1,\infty}(B)}.\] (54)

Then

\[\left\lvert{c_{f,\bm{\alpha}}}\right\rvert\leq C_{2}(n,d)\|f\|_{W^{n-1,\infty }(B_{\bm{m},N})}.\] (55)

where \(C_{2}(n,d)=\sum_{\left\lvert{\bm{\alpha}}+\bm{\beta}\right\rvert\leq n-1} \frac{1}{\alpha^{\prime}\bm{\beta}!}\). 

The proof of Theorem 6 is based on average Taylor polynomials and the Bramble-Hilbert Lemma [7, Lemma 4.3.8].

**Definition 9**.: _Let \(\Omega,\ B\in\mathbb{R}^{d}\). Then \(\Omega\) is called stared-shaped with respect to \(B\) if_

\[\overline{\text{conv}}\left(\left\{\bm{x}\right\}\cup B\subset\Omega\right), \text{ for all }\bm{x}\in\Omega.\]

**Definition 10**.: _Let \(\Omega\in\mathbb{R}^{d}\) be bounded, and define_

\[\mathcal{R}:=\left\{r>0:\begin{array}{ll}\text{there exists }\bm{x}_{0}\in \Omega\text{ such that }\Omega\text{ is}\\ \text{star-shaped with respect to }B_{r,\left\lvert{\cdot}\right\rvert}\left(\bm{x}_{0} \right)\end{array}\right\}.\]

_Then we define_

\[r_{\max}^{\star}:=\sup\mathcal{R}\quad\text{ and call }\quad\gamma:=\frac{\mathrm{diam}(\Omega)}{r_{\max}^{\star}}\]

_the chunkiness parameter of \(\Omega\) if \(\mathcal{R}\neq\emptyset\)._

**Lemma 9** (Bramble-Hilbert Lemma [7, Lemma 4.3.8]).: _Let \(\Omega\in\mathbb{R}^{d}\) be open and bounded, \(\bm{x}_{0}\in\Omega\) and \(r>0\) such that \(\Omega\) is the stared-shaped with respect to \(B:=B_{r,\left\lvert{\cdot}\right\rvert}\left(\bm{x}_{0}\right)\), and \(r\geq\frac{1}{2}r_{\max}^{\star}\). Moreover, let \(n\in\mathbb{N}_{+}\), \(1\leq p\leq\infty\) and denote by \(\gamma\) by the chunkiness parameter of \(\Omega\). Then there is a constant \(C(n,d,\gamma)>0\) such that for all \(f\in W^{n,p}(\Omega)\)_

\[|f-Q^{n}f|_{W^{k,p}(\Omega)}\leq C(n,d,\gamma)h^{n-k}|f|_{W^{n,p}(\Omega)} \quad\text{ for }k=0,1,\ldots,n\]

_where \(Q^{n}f\) denotes the Taylor polynomial of order \(n\) of \(f\) averaged over \(B\) and \(h=\mathrm{diam}(\Omega)\)._

Proof of Theorem 6.: Without loss of generalization, we prove the case for \(\bm{m}=(1,1,\ldots,1)=:\bm{m}_{*}\). Denote \(E:W^{n,\infty}((0,1)^{d})\to W^{n,\infty}(\mathbb{R}^{d})\) be an extension operator [43] and set \(\tilde{f}:=Ef\) and \(C_{E}\) is the norm of the extension operator.

Define \(p_{f,\bm{i}}\) as the average Taylor polynomial Definition 8 in \(B_{\bm{i},K}:=B_{\frac{1}{4K},|\cdot|}\left(\frac{8\bm{i}+3}{8K}\right)\) i.e.

\[p_{f,\bm{i}}:=\int_{B_{\bm{i},K}}T_{\bm{y}}^{n}\tilde{f}(\bm{x})b_{\frac{1}{4K}} (\bm{y})\,\mathrm{d}\bm{y}.\] (56)

Based on Lemma 8, \(p_{f,\bm{i}}\) can be read as

\[p_{f,\bm{i}}=\sum_{|\bm{\alpha}|\leq n-1}c_{f,\bm{i},\bm{\alpha}}\bm{x}^{\bm{ \alpha}}\] (57)

where

\[|c_{f,\bm{i},\bm{\alpha}}|\leq C_{2}(n,d).\] (58)

The reason to define average Taylor polynomial on \(B_{\bm{i},K}\) is to use the Bramble-Hilbert Lemma 9 on

\[\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}}=B_{\frac{3}{8K},|\cdot|\bm{l}_{\infty}} \left(\frac{8\bm{i}+3}{8K}\right)=\prod_{j=1}^{d}\left[\frac{i_{j}}{K},\frac{ 3+4i_{j}}{4K}\right].\]

Note that

\[\frac{1}{4K}\geq\frac{1}{2}\cdot\frac{3}{8K}=\frac{1}{2}r_{\max}^{\star}( \Omega_{\bm{m}_{\bm{\cdot}},\bm{i}}),\;\gamma(\Omega_{\bm{m}_{\bm{\cdot}},\bm{ i}})=\frac{\mathrm{diam}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}})}{r_{\max}^{\star}( \Omega_{\bm{m}_{\bm{\cdot}},\bm{i}})}=2\sqrt{d}.\]

Therefore we can apply the Bramble-Hilbert Lemma 9 and have

\[\|\tilde{f}-p_{f,\bm{i}}\|_{L^{\infty}(\Omega_{\bm{m}_{\bm{\cdot} },\bm{i}})}\leq C_{BH}(n,d)K^{-n}\] \[|\tilde{f}-p_{f,\bm{i}}|_{W^{1,\infty}(\Omega_{\bm{m}_{\bm{\cdot }},\bm{i}})}\leq C_{BH}(n,d)K^{-(n-1)}\] (59)

where \(C_{BH}(n,d)=|\{|\bm{\alpha}|=n\}|\frac{1}{d\int_{0}^{1}x^{d-1}e^{-(1-x^{2})^{ -1}}\,\mathrm{d}x}\left(2+4\sqrt{d}\right)^{d}C_{E}\) by following the proof of Lemma [7, Lemma 4.3.8]. Therefore,

\[\|\tilde{f}-p_{f,\bm{i}}\|_{W^{1,\infty}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}}) }\leq C_{1}(n,d)K^{-(n-1)}\]

where \(C_{1}(n,d)=2C_{BH}(n,d)\).

Now we construct a partition of unity that we use in this theorem. First of all, given any integer \(K\), define \(\{h_{i}\}_{i=0}^{K}\) from \(\mathbb{R}\rightarrow\mathbb{R}\):

\[h_{i}(x):=h\left(4K\left(x-\frac{8i+3}{8K}\right)\right),\;h(x):=\begin{cases} 1,&|x|<\frac{3}{2}\\ 0,&|x|>2\\ 4-2|x|,&\frac{3}{2}\leq|x|\leq 2.\end{cases}\] (60)

It is easy to check that \(\{h_{i}\}_{i=0}^{K}\) is a partition of unity of \([0,1]\) and \(h_{i}(x)=1\) for \(x\in\left[\frac{i}{K},\frac{3+4i}{4K}\right]\). Hence we can define \(h_{\bm{i}}(\bm{x})\) for \(\bm{i}=(i_{1},i_{2},\ldots,i_{d})\in\{0,1,\ldots,K\}^{d}\) and \(\bm{x}=(x_{1},x_{2},\ldots,x_{d})\in\mathbb{R}^{d}\):

\[h_{\bm{i}}(\bm{x})=\prod_{j=1}^{d}h_{i_{j}}(x_{j}),\] (61)

and \(\left\{h_{\bm{i}}:\bm{i}\in\{0,1,\ldots,K\}^{d}\right\}\) is a partition of unity of \([0,1]^{d}\) and \(h_{\bm{i}}(\bm{x})=1\) for \(\bm{x}\in\prod_{j=1}^{d}\left[\frac{i_{j}}{K},\frac{3+4i_{j}}{4K}\right]= \Omega_{\bm{m}_{\bm{\cdot}},\bm{i}}\) and \(\bm{i}=(i_{1},i_{2},\ldots,i_{d})\in\{0,1,\ldots,K\}^{d}\).

Furthermore,

\[\|h_{\bm{i}}(\tilde{f}-p_{f,\bm{i}})\|_{L^{\infty}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}})}\leq\|\tilde{f}-p_{f,\bm{i}}\|_{L^{\infty}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}})}\leq C_{BH}(n,d)K^{-n}\] (62)

and

\[|h_{\bm{i}}(\tilde{f}-p_{f,\bm{i}})|_{W^{1,\infty}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}})}\leq |\tilde{f}-p_{f,\bm{i}}|_{W^{1,\infty}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}}) }\leq C_{BH}(n,d)K^{-(n-1)}\] (63)

which is due to \(h_{\bm{i}}=1\) on \(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}}\).

Then

\[\|h_{\bm{i}}(\tilde{f}-p_{f,\bm{i}})\|_{W^{1,\infty}(\Omega_{\bm{m}_{\bm{\cdot}},\bm{i}})}\leq C_{1}(n,d)K^{-(n-1)}.\]Finally,

\[\left\|f-\sum_{\bm{i}\in\{0,1,\ldots,K\}^{d}}h_{\bm{i}}p_{f,\bm{i}} \right\|_{W^{1,\infty}(\Omega_{m_{*}})}\leq\max_{\bm{i}\in\{0,1,\ldots,K\}^{d}} \|h_{\bm{i}}(\tilde{f}-p_{f,\bm{i}})\|_{W^{1,\infty}(\Omega_{\bm{m}_{*},\bm{i}})}\] \[\leq C_{1}(n,d)K^{-(n-1)},\] (64)

which is due to \(\cup_{\bm{i}\in\{0,1,\ldots,K\}^{d}}\Omega_{\bm{m}_{*},\bm{i}}=\Omega_{m_{*}}\) and \(\text{supp }h_{\bm{i}}\cap\Omega_{m_{*}}=\Omega_{\bm{m}_{*},\bm{i}}\).

Similarly,

\[\left\|f-\sum_{\bm{i}\in\{0,1,\ldots,K\}^{d}}h_{\bm{i}}p_{f,\bm{i}}\right\|_{L ^{\infty}(\Omega_{1,d})}\leq C_{1}(n,d)K^{-n}.\] (65)

Last of all,

\[f_{\bm{k},\bm{m}_{*}}(\bm{x}) :=\sum_{\bm{i}\in\{0,1,\ldots,K\}^{d}}h_{\bm{i}}p_{f,\bm{i}}=\sum _{\bm{i}\in\{0,1,\ldots,K\}^{d}}\sum_{\bm{|\alpha|}\leq n-1}h_{\bm{i}}c_{f, \bm{i},\bm{\alpha}}\bm{x}^{\bm{\alpha}}\] \[=\sum_{|\bm{\alpha}|\leq n-1}\sum_{\bm{i}\in\{0,1,\ldots,K\}^{d}} h_{\bm{i}}c_{f,\bm{i},\bm{\alpha}}\bm{x}^{\bm{\alpha}}\] \[=:\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x} )\bm{x}^{\bm{\alpha}}\] (66)

with \(|g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})|\leq C_{2}(n,d)\) for \(x\in\Omega_{\bm{m}_{*}}\). Note that \(g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\) is a step function from \(\Omega_{\bm{m}_{*}}\to\mathbb{R}\):

\[g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})=c_{f,\bm{i},\bm{\alpha}}\] (67)

for \(\bm{x}\in\prod_{j=1}^{d}\left[\frac{i_{j}}{K},\frac{3+4i_{j}}{4K}\right]\) and \(\bm{i}=(i_{1},i_{2},\ldots,i_{d})\) since \(h_{\bm{i}}(\bm{x})=0\) for \(\bm{x}\in\Omega_{\bm{m}_{*}}\setminus\prod_{j=1}^{d}\left[\frac{i_{j}}{K}, \frac{3+4i_{j}}{4K}\right]\) and \(h_{\bm{i}}(\bm{x})=1\) for \(\bm{x}\in\prod_{j=1}^{d}\left[\frac{i_{j}}{K},\frac{3+4i_{j}}{4K}\right]\). 

2.3 Approximation of functions in \(W^{n,\infty}\) with \(W^{1,\infty}\) norm by ReLU neural networks in the whole space except a small set

**Theorem 7**.: _For any \(f\in W^{n,\infty}((0,1)^{d})\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\), any \(N,L\in\mathbb{N}_{+}\), and \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\), there is a neural network \(\psi_{\bm{m}}\) with the width \(25n^{d+1}(N+1)\log_{2}(8N)\) and depth \(27n^{2}(L+2)\log_{2}(4L)\) such that_

\[\|f(\bm{x})-\psi_{\bm{m}}(\bm{x})\|_{W^{1,\infty}(\Omega_{\bm{m}})} \leq C_{6}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f(\bm{x})-\psi_{\bm{m}}(\bm{x})\|_{L^{\infty}(\Omega_{\bm{m}_{*} })} \leq C_{6}(n,d)N^{-2n/d}L^{-2n/d},\] (68)

_where \(C_{6}\) is the constant independent with \(N,L\)._

Proof.: Without loss of the generalization, we consider the case for \(\bm{m}_{*}=(1,1,\ldots,1)\). Due to Theorem 6 and setting \(K=\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor\), we have

\[\|f-f_{K,\bm{m}_{*}}\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\leq C_{ 1}(n,d)K^{-(n-1)}\leq C_{1}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f-f_{K,\bm{m}_{*}}\|_{L^{\infty}(\Omega_{\bm{m}_{*}})}\leq C_{1} (n,d)K^{-n}\leq C_{1}(n,d)N^{-2n/d}L^{-2n/d},\] (69)

where \(f_{K,\bm{m}_{*}}=\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{ x})\bm{x}^{\bm{\alpha}}\) for \(x\in\Omega_{\bm{m}_{*}}\). Note that \(g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\) is a constant function for \(\bm{x}\in\prod_{j=1}^{d}\left[\frac{i_{j}}{K},\frac{3+4i_{j}}{4K}\right]\) and \(\bm{i}=(i_{1},i_{2},\ldots,i_{d})\in\{0,1,\ldots,K-1\}^{d}\). The remaining part is to approximate \(f_{K,\bm{m}_{*}}\) by neural networks.

The way to approximate \(g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\) is similar with [23, Theorem 3.1]. First of all, due to Proposition 2, there is a neural network \(\phi_{1}(x)\) with the width \(4N+5\) and depth \(4L+4\) such that

\[\phi(x)=k,x\in\left[\frac{k}{K},\frac{k+1}{K}-\frac{1}{4K}\right],\;k=0,1, \ldots,K-1.\] (70)Note that we choose \(\delta=\frac{1}{4K}\leq\frac{1}{3K}\) in Proposition 2. Then define

\[\bm{\phi}_{2}(\bm{x})=\left[\frac{\phi_{1}(x_{1})}{K},\frac{\phi_{1}(x_{2})}{K}, \ldots,\frac{\phi_{1}(x_{d})}{K}\right]^{\intercal}.\]

For each \(p=0,1,\ldots,K^{d}-1\), there is a bijection

\[\bm{\eta}(p)=[\eta_{1},\eta_{2},\ldots,\eta_{d}]\in\{0,1,\ldots,K-1\}^{d}\]

such that \(\sum_{j=1}^{d}\eta_{j}K^{j-1}=p\). Then define

\[\xi_{\bm{\alpha},p}=\frac{g_{f,\bm{\alpha},\bm{m}_{*}}\left(\frac{\bm{\eta}(p )}{K}\right)+C_{2}(n,d)}{2C_{2}(n,d)}\in[0,1],\]

where \(C_{2}(n,d)\) is the bounded of \(g_{f,\bm{\alpha},\bm{m}_{*}}\) defined in Theorem 6. Therefore, based on Proposition 3, there is a neural network \(\tilde{\phi}_{\bm{\alpha}}(x)\) with the width \(16n(N+1)\log_{2}(8N)\) and depth \((5L+2)\log_{2}(4L)\) such that \(|\tilde{\phi}_{\bm{\alpha}}(p)-\xi_{\bm{\alpha},p}|\leq N^{-2n}L^{-2n}\) for \(p=0,1,\ldots K^{d}-1\). Denote

\[\phi_{\bm{\alpha}}(\bm{x})=2C_{2}(n,d)\tilde{\phi}_{\bm{\alpha}}\left(\sum_{j =1}^{d}x_{j}K^{j}\right)-C_{2}(n,d)\]

and obtain that

\[\left|\phi_{\bm{\alpha}}\left(\frac{\bm{\eta}(p)}{K}\right)-g_{f,\bm{\alpha},\bm{m}_{*}}\left(\frac{\bm{\eta}(p)}{K}\right)\right|=2C_{2}(n,d)|\tilde{\phi }_{\bm{\alpha}}(p)-\xi_{\bm{\alpha},p}|\leq 2C_{2}(n,d)N^{-2n}L^{-2n}.\]

Then we obtain that

\[\|\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{ \alpha},\bm{m}_{*}}\left(\bm{x}\right)\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}= \|\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{ \alpha},\bm{m}_{*}}\left(\bm{x}\right)\|_{L^{\infty}(\Omega_{\bm{m}_{*}})}\] \[\leq 2C_{2}(n,d)N^{-2n}L^{-2n}\] (71)

which is due to \(\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{\alpha},\bm{m}_ {*}}\left(\bm{x}\right)\) is a step function, and the first order weak derivative is \(0\) in \(\Omega_{\bm{m}_{*}}\).

Due to Proposition 6, there is a neural network \(\phi_{3,\bm{\alpha}}\) with the width \(9(N+1)+n-1\) and depth \(14n^{2}L\) such that \(\|\phi_{3,\bm{\alpha}}\|_{W^{1,\infty}((0,1)^{d})}\leq 18\) and

\[\|\phi_{3,\bm{\alpha}}(\bm{x})-\bm{x}^{\bm{\alpha}}\|_{W^{1,\infty}((0,1)^{d} )}\leq 10n(N+1)^{-7nL}.\] (72)

Due to Proposition 4, there is a neural network \(\phi_{4}\) with the width \(15(N+1)\) and depth \(4n(L+1)\) such that \(\|\phi_{4}\|_{W^{1,\infty}(-C_{3},C_{3})^{2}}\leq 12(C_{2}(n,d))^{2}\) and

\[\|\phi_{4}(x,y)-xy\|_{W^{1,\infty}((-C_{3},C_{3})^{2})}\leq 6(C_{2}(n,d))^{2}(N+1)^{-2n(L+1)}.\] (73)

where \(C_{3}(n,d)=\max\{3C_{2}(n,d),18\}\).

Now we define the neural network \(\phi_{\bm{m}_{*}}(\bm{x})\) to approximate \(f_{K,\bm{m}_{*}}(\bm{x})\) in \(\Omega_{\bm{m}_{*}}\):

\[\psi_{\bm{m}_{*}}(\bm{x})=\sum_{|\bm{\alpha}|\leq n-1}\phi_{4}\left[\phi_{\bm{ \alpha}}(\bm{\phi}_{2}(\bm{x})),\phi_{3,\bm{\alpha}}(\bm{x})\right].\] (74)The remaining question is to find the error \(\mathcal{E}\):

\[\mathcal{E}:= \left\|\sum_{|\bm{\alpha}|\leq n-1}\phi_{4}\left[\phi_{\bm{\alpha}} (\bm{\phi}_{2}(\bm{x})),\phi_{3,\bm{\alpha}}(\bm{x})\right]-f_{K,\bm{m}_{*}}( \bm{x})\right\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\] \[\leq \sum_{|\bm{\alpha}|\leq n-1}\left\|\phi_{4}\left[\phi_{\bm{\alpha} }(\bm{\phi}_{2}(\bm{x})),\phi_{3,\bm{\alpha}}(\bm{x})\right]-g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\bm{x}^{\bm{\alpha}}\right\|_{W^{1,\infty}(\Omega_{\bm{m}_ {*}})}\] \[\leq \underbrace{\sum_{|\bm{\alpha}|\leq n-1}\left\|\phi_{4}\left[\phi _{\bm{\alpha}}(\bm{\phi}_{2}(\bm{x})),\phi_{3,\bm{\alpha}}(\bm{x})\right]- \phi_{\bm{\alpha}}(\bm{\phi}_{2}(\bm{x}))\phi_{3,\bm{\alpha}}(\bm{x})\right\| _{W^{1,\infty}(\Omega_{\bm{m}_{*}})}}_{=:\mathcal{E}_{1}}\] \[+\underbrace{\sum_{|\bm{\alpha}|\leq n-1}\left\|\phi_{\bm{\alpha} }(\bm{\phi}_{2}(\bm{x}))\phi_{3,\bm{\alpha}}(\bm{x})-g_{f,\bm{\alpha},\bm{m}_ {*}}(\bm{x})\phi_{3,\bm{\alpha}}(\bm{x})\right\|_{W^{1,\infty}(\Omega_{\bm{m}_ {*}})}}_{=:\mathcal{E}_{2}}\] \[+\underbrace{\sum_{|\bm{\alpha}|\leq n-1}\left\|g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\phi_{3,\bm{\alpha}}(\bm{x})-g_{f,\bm{\alpha},\bm{m}_{*}} (\bm{x})\bm{x}^{\bm{\alpha}}\right\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}}_{=: \mathcal{E}_{3}}.\] (75)

As for \(\mathcal{E}_{1}\), due to Lemma 3, we have

\[\mathcal{E}_{1}\leq \sum_{|\bm{\alpha}|\leq n-1}2\sqrt{d}\max\Big{\{}\left\|\phi_{4}( x,y)-xy\right\|_{L^{\infty}((-C_{3},C_{3})^{2})},\left\|\phi_{4}(x,y)-xy \right\|_{W^{1,\infty}((-C_{3},C_{3})^{2})}\] \[\cdot\max\{\left\|\phi_{\bm{\alpha}}(\bm{\phi}_{2}(\bm{x}))\right\| _{W^{1,\infty}(\Omega_{\bm{m}_{*}})},\left\|\phi_{3,\bm{\alpha}}(\bm{x}) \right\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\}\Big{\}}\] \[\leq \sum_{|\bm{\alpha}|\leq n-1}2\sqrt{d}\max\Big{\{}\left\|\phi_{4} (x,y)-xy\right\|_{L^{\infty}((-C_{3},C_{3})^{2})},C_{3}(n,d)\left\|\phi_{4}(x,y )-xy\right\|_{W^{1,\infty}((-C_{3},C_{3})^{2})}\Big{\}}\] \[\leq \sum_{|\bm{\alpha}|\leq n-1}12\sqrt{d}\left[C_{3}(n,d)+1\right]( C_{2}(n,d))^{2}(N+1)^{-2n(L+1)}\] \[\leq C_{4}(n,d)(N+1)^{-2n(L+1)}\] (76)

where \(C_{4}(n,d)=12\sqrt{d}n^{d}\left[C_{3}(n,d)+1\right](C_{2}(n,d))^{2}\).

As for \(\mathcal{E}_{2}\), due to Lemma 4, we have

\[\mathcal{E}_{2}\leq \sum_{|\bm{\alpha}|\leq n-1}2\left\|\phi_{\bm{\alpha}}(\bm{\phi}_ {2}(\bm{x}))-g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\right\|_{W^{1,\infty}(\Omega _{\bm{m}_{*}})}\cdot\left\|\phi_{3,\bm{\alpha}}(\bm{x})\right\|_{W^{1,\infty} (\Omega_{\bm{m}_{*}})}\] \[\leq 72n^{d}C_{2}(n,d)N^{-2n}L^{-2n}.\] (77)

The estimation of \(\mathcal{E}_{3}\) is similar with that of \(\mathcal{E}_{2}\) which is

\[\mathcal{E}_{3}\leq \sum_{|\bm{\alpha}|\leq n-1}\left\|g_{f,\bm{\alpha},\bm{m}_{*}} \right\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\cdot\left\|\phi_{3,\bm{\alpha}}( \bm{x})-\bm{x}^{\bm{\alpha}}\right\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\] \[\leq 10n^{d}C_{2}(n,d)n(N+1)^{-7nL}.\] (78)

Therefore, using

\[(N+1)^{-7nL}\leq(N+1)^{-2n(L+1)}\leq N^{-2n}L^{-2n}\]

the total error is

\[\mathcal{E}\leq\mathcal{E}_{1}+\mathcal{E}_{2}+\mathcal{E}_{3}\leq C_{5}(n,d)K ^{-2n}L^{-2n},\] (79)

where \(C_{5}(n,d)=C_{4}(n,d)+72n^{d}C_{2}(n,d)+10n^{d}C_{2}(n,d)n\).

At last, we finish the proof by estimating the network's width and depth, implementing \(\psi_{\bm{m}_{*}}(\bm{x})\). From Eq. (74), we know that \(\psi_{\bm{m}_{*}}(\bm{x})\) consists of the following subnetworks:

1. \(\phi_{3,\bm{\alpha}}(\bm{x})\) with the width \(9(N+1)+n-1\) and depth \(14n^{2}L\).

2. \(\phi_{2}(\bm{x})\) with the width \(4N+5\) and depth \(4L+4\).

3. \(\phi_{\bm{\alpha}}\) with the width \(16n(N+1)\log_{2}(8N)\) and depth \((5L+2)\log_{2}(4L)\).

4. \(\phi_{4}(x,y)\) with the width \(15(N+1)\) and depth \(4n(L+1)\).

Therefore \(\phi(\bm{x})\) is a neural network with the width \(25n^{d+1}(N+1)\log_{2}(8N)\) and depth \(27n^{2}(L+2)\log_{2}(4L)\).

Combining Eqs. (69) and (79), we have that there is a neural network \(\psi_{\bm{m}_{\star}}\) with the width \(25n^{d+1}(N+1)\log_{2}(8N)\) and depth \(27n^{2}(L+2)\log_{2}(4L)\) such that

\[\|f(\bm{x})-\psi_{\bm{m}_{\star}}(\bm{x})\|_{W^{1,\infty}(\Omega_ {\bm{m}_{\star}})}\leq C_{6}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f(\bm{x})-\psi_{\bm{m}_{\star}}(\bm{x})\|_{L^{\infty}(\Omega_ {\bm{m}_{\star}})}\leq C_{6}(n,d)N^{-2n/d}L^{-2n/d},\] (80)

where \(C_{6}=C_{1}+C_{5}\) is the constant independent with \(N,L\).

Similarly, we can construct a neural network \(\psi_{\bm{m}}\) with the width \(25n^{d+1}(N+1)\log_{2}(8N)\) and depth \(27n^{2}(L+2)\log_{2}(4L)\) which can approximate \(f\) on \(\Omega_{\bm{m}}\) with same order of Eq. (80). 

#### 7.2.4 Proof of Theorem 3

Now we can prove Theorem 3 based on Theorem 7 and Proposition 1.

Proof of Theorem 3.: Based on Theorem 7, there is a sequence of the neural network \(\{\psi_{\bm{m}}(\bm{x})\}_{\bm{m}\in\{1,2\}^{d}}\) such that

\[\|f(\bm{x})-\psi_{\bm{m}}(\bm{x})\|_{W^{1,\infty}(\Omega_{\bm{m} })}\leq C_{6}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f(\bm{x})-\psi_{\bm{m}}(\bm{x})\|_{L^{\infty}(\Omega_{\bm{m}}) }\leq C_{6}(n,d)N^{-2n/d}L^{-2n/d},\] (81)

where \(C_{6}=C_{1}+C_{5}\) is the constant independent with \(N,L\), and each \(\psi_{\bm{m}}\) is a neural network with the width \(25n^{d+1}(N+1)\log_{2}(8N)\) and depth \(27n^{2}(L+2)\log_{2}(4L)\). According to Proposition 1, there is a sequence of the neural network \(\{\phi_{\bm{m}}(\bm{x})\}_{\bm{m}\in\{1,2\}^{d}}\) such that

\[\|\phi_{\bm{m}}(\bm{x})-g_{\bm{m}}(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}\leq 50 d^{\frac{5}{2}}(N+1)^{-4dnL},\]

where \(\{g_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) is defined in Definition 6 with \(\sum_{\bm{m}\in\{1,2\}^{d}}g_{\bm{m}}(\bm{x})=1\) and \(\operatorname{supp}g_{\bm{m}}\cap[0,1]^{d}=\Omega_{\bm{m}}\). For each \(\phi_{\bm{m}}\), it is a neural network with the width smaller than \((9+d)(N+1)+d-1\) and depth smaller than \(15d(d-1)nL\).

Due to Proposition 4, there is a neural network \(\widehat{\phi}\) with the width \(15(N+1)\) and depth \(14n^{2}L\) such that \(\|\widehat{\phi}\|_{W^{1,\infty}(-C_{7},C_{7})^{2}}\leq 12(C_{7}(n,d))^{2}\) and

\[\left\|\widehat{\phi}(x,y)-xy\right\|_{W^{1,\infty}(-C_{7},C_{7})^{2}}\leq 6 (C_{7})^{2}(N+1)^{-7n(L+1)},\] (82)

where \(C_{7}=C_{6}+50d^{\frac{5}{2}}+1\).

Now we define

\[\phi(\bm{x})=\sum_{\bm{m}\in\{1,2\}^{d}}\widehat{\phi}(\phi_{\bm{m}}(\bm{x}), \psi_{\bm{m}}(\bm{x})).\] (83)

Note that

\[\mathcal{R}:= \|f(\bm{x})-\phi(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}=\left\|\sum_ {\bm{m}\in\{1,2\}^{d}}g_{\bm{m}}\cdot f(\bm{x})-\phi(\bm{x})\right\|_{W^{1, \infty}((0,1)^{d})}\] \[\leq \left\|\sum_{\bm{m}\in\{1,2\}^{d}}\left[g_{\bm{m}}\cdot f(\bm{x} )-\phi_{\bm{m}}(\bm{x})\cdot\psi_{\bm{m}}(\bm{x})\right]\right\|_{W^{1, \infty}((0,1)^{d})}\] \[+\left\|\sum_{\bm{m}\in\{1,2\}^{d}}\left[\phi_{\bm{m}}(\bm{x}) \cdot\psi_{\bm{m}}(\bm{x})-\widehat{\phi}(\phi_{\bm{m}}(\bm{x}),\psi_{\bm{m}}( \bm{x}))\right]\right\|_{W^{1,\infty}((0,1)^{d})}.\] (84)As for the first part,

\[\left\|\sum_{\bm{m}\in\{1,2\}^{d}}\left[g_{\bm{m}}\cdot f(\bm{x})- \phi_{\bm{m}}(\bm{x})\cdot\psi_{\bm{m}}(\bm{x})\right]\right\|_{W^{1,\infty}((0,1 )^{d})}\] \[\leq \sum_{\bm{m}\in\{1,2\}^{d}}\left[\left\|(g_{\bm{m}}-\phi_{\bm{m}}( \bm{x}))\cdot f(\bm{x})\right\|_{W^{1,\infty}((0,1)^{d})}+\left\|(f_{\bm{m}}- \psi_{\bm{m}}(\bm{x}))\cdot\phi_{\bm{m}}(\bm{x})\right\|_{W^{1,\infty}((0,1)^{d })}\right]\] \[= \sum_{\bm{m}\in\{1,2\}^{d}}\left[\left\|(g_{\bm{m}}-\phi_{\bm{m}}( \bm{x}))\cdot f(\bm{x})\right\|_{W^{1,\infty}((0,1)^{d})}+\left\|(f_{\bm{m}}- \psi_{\bm{m}}(\bm{x}))\cdot\phi_{\bm{m}}(\bm{x})\right\|_{W^{1,\infty}(\Omega_ {\bm{m}})}\right],\] (85)

where the last equality is due to Lemma 7. Based on Lemma 4 and \(\|f\|_{W^{1,\infty}((0,1)^{d})}\leq 1\), we have

\[\left\|(g_{\bm{m}}-\phi_{\bm{m}}(\bm{x}))\cdot f(\bm{x})\right\|_{W^{1,\infty} ((0,1)^{d})}\leq\left\|(g_{\bm{m}}-\phi_{\bm{m}}(\bm{x}))\right\|_{W^{1,\infty }((0,1)^{d})}\leq 50d^{\frac{5}{2}}(N+1)^{-4dnL}.\] (86)

And

\[\left\|(f_{\bm{m}}-\psi_{\bm{m}}(\bm{x}))\cdot\phi_{\bm{m}}(\bm{x })\right\|_{W^{1,\infty}(\Omega_{\bm{m}})}\] \[\leq \left\|(f_{\bm{m}}-\psi_{\bm{m}}(\bm{x}))\right\|_{W^{1,\infty}( \Omega_{\bm{m}})}\cdot\|\phi_{\bm{m}}\|_{L^{\infty}(\Omega_{\bm{m}})}+\left\| (f_{\bm{m}}-\psi_{\bm{m}}(\bm{x}))\right\|_{L^{\infty}(\Omega_{\bm{m}})} \cdot\|\phi_{\bm{m}}\|_{W^{1,\infty}(\Omega_{\bm{m}})}\] \[\leq C_{6}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\cdot\left(1+50d^{\frac{5}{2} }\right)+C_{6}(n,d)N^{-2n/d}L^{-2n/d}\cdot 54d^{\frac{5}{2}}\lfloor N^{1/d} \rfloor^{2}\lfloor L^{2/d}\rfloor\] \[\leq C_{7}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d},\] (87)

where the second inequality is due to

\[\left\|\phi_{\bm{m}}\right\|_{L^{\infty}(\Omega_{\bm{m}})}\leq \left\|\phi_{\bm{m}}\right\|_{L^{\infty}([0,1]^{d})}\leq\left\|g_{\bm{m}} \right\|_{L^{\infty}([0,1]^{d})}+\left\|\phi_{\bm{m}}-g_{\bm{m}}\right\|_{L^{ \infty}([0,1]^{d})}\leq 1+50d^{\frac{5}{2}}\] \[\left\|\phi_{\bm{m}}\right\|_{W^{1,\infty}(\Omega_{\bm{m}})}\leq \left\|\phi_{\bm{m}}\right\|_{W^{1,\infty}([0,1]^{d})}\leq\left\|g_{\bm{m}} \right\|_{W^{1,\infty}([0,1]^{d})}+\left\|\phi_{\bm{m}}-g_{\bm{m}}\right\|_{W^ {1,\infty}([0,1]^{d})}\] \[\leq 4\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor+50d^{ \frac{5}{2}}.\] (88)

Therefore

\[\left\|\sum_{\bm{m}\in\{1,2\}^{d}}\left[g_{\bm{m}}\cdot f(\bm{x}) -\phi_{\bm{m}}(\bm{x})\cdot\psi_{\bm{m}}(\bm{x})\right]\right\|_{W^{1,\infty} ((0,1)^{d})}\leq 2^{d}(C_{7}(n,d)+50d^{\frac{5}{2}})N^{-2(n-1)/d}L^{-2(n-1)/d}\] (89)

due to \((N+1)^{-4dnL}\leq N^{-2n}L^{-2n}\).

For the second part, due to Lemma 7, we have

\[\left\|\sum_{\bm{m}\in\{1,2\}^{d}}\left[\phi_{\bm{m}}(\bm{x}) \cdot\psi_{\bm{m}}(\bm{x})-\widehat{\phi}(\phi_{\bm{m}}(\bm{x}),\psi_{\bm{m}}( \bm{x}))\right]\right\|_{W^{1,\infty}((0,1)^{d})}\] \[\leq \sum_{\bm{m}\in\{1,2\}^{d}}\left\|\phi_{\bm{m}}(\bm{x})\cdot \psi_{\bm{m}}(\bm{x})-\widehat{\phi}(\phi_{\bm{m}}(\bm{x}),\psi_{\bm{m}}(\bm{x}) )\right\|_{W^{1,\infty}((0,1)^{d})}\] \[= \sum_{\bm{m}\in\{1,2\}^{d}}\left\|\phi_{\bm{m}}(\bm{x})\cdot \psi_{\bm{m}}(\bm{x})-\widehat{\phi}(\phi_{\bm{m}}(\bm{x}),\psi_{\bm{m}}(\bm{x}) )\right\|_{W^{1,\infty}(\Omega_{\bm{m}})}.\] (90)

Similarly with the estimation of \(\mathcal{E}_{1}\) (76), we have that

\[\left\|\phi_{\bm{m}}(\bm{x})\cdot\psi_{\bm{m}}(\bm{x})-\widehat{ \phi}(\phi_{\bm{m}}(\bm{x}),\psi_{\bm{m}}(\bm{x}))\right\|_{W^{1,\infty}( \Omega_{\bm{m}})}\] \[\leq C_{8}(n,d)(N+1)^{-7n(L+1)}\leq C_{8}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}.\] (91)Combining (89) and (91), we have that there is a \(\sigma_{1}\)-NN \(\phi\) with the width \((34+d)2^{d}n^{d+1}(N+1)\log_{2}(8N)\) and depth \(56d^{2}n^{2}(L+1)\log_{2}(4L)\) such that

\[\|f(\bm{x})-\phi(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}\leq C_{9}(n,d)N^{-2(n-1)/d }L^{-2(n-1)/d},\]

where \(C_{9}\) is the constant independent with \(N,L\).

The method proposed in [28, 23, 39, 38, 37] may not be applied to prove Theorems 3. These works approximate the target function \(f\) using a deep neural network \(\phi\) in the unit cube except for an arbitrarily small region \(\Omega_{\delta}\), as per [36, Lemma 2.2]. Since \(\|\phi\|_{L^{\infty}(\Omega)}\) can be bounded and is independent of the size of \(\Omega\delta\), \(\|f-\phi\|_{L^{p}(\Omega)}\) can be well estimated across the entire space for \(p\in[1,+\infty)\). For approximations measured in the \(L^{\infty}(\Omega)\) norm, [28] translates the deep neural network \(\phi\), while [39] constructs different neural networks in the unit cube away from various negligible regions. Both methods aim to find neural networks \(\{\phi_{i}(\bm{x})\}i=1^{N}\) that approximate the target function \(f\) well in different regions. They then observe that the middle value of \(\{\phi_{i}(\bm{x})\}_{i=1}^{N}\) is close to \(f(\bm{x})\) for all \(\bm{x}^{*}\in\Omega\), and construct the middle-value function using a ReLU neural network. However, these methods may not be generalized to prove the theorems presented in this paper.

Neither of the methods previously proposed can be applied to the approximation measured in Sobolev space. In the first method, \(\|\phi\|_{W^{1,\infty}(\Omega)}\) depends on the length of \(\Omega\delta\), and the derivative is substantial in the negligible region, as shown in [36, Lemma 2.2]. Thus, \(\|f-\phi\|_{W^{1,p}(\Omega)}\) will be excessively large. In the second method, median value functions can only identify the median values, not the median values of functions and their derivatives simultaneously. In this paper, we overcome this difficulty using a partition of unity. We construct a partition of unity of \(\Omega\) and approximate them using ReLU DNNs denoted as \(\{\phi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\). For each \(\phi{\bm{m}}\), its support set is the unit cube away from a small region, and we can construct a deep neural network \(\psi_{\bm{m}}\) that approximates the target function \(f\) well on \(\operatorname{supp}\,\phi_{\bm{m}}\). We then combine \(\{\phi_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) and \(\{\tilde{\psi}_{\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) to obtain a deep neural network that can approximate the target function \(f\) well across the entire space. This approach resolves the issue of simultaneous approximation of both functions and their derivatives in Sobolev spaces.

### Proofs of Corollaries 1 and 2

#### 7.3.1 Preliminaries

First, we list a few basic lemmas of \(\sigma_{2}\) neural networks repeatedly applied in our main analysis.

**Lemma 10** ([23, Lemma 3.7]).: _The following basic lemmas of \(\sigma_{2}\) neural networks hold:_

_(i)_ \(\sigma_{1}\) _neural networks are_ \(\sigma_{2}\) _neural networks._

_(ii) Any identity map in_ \(\mathbb{R}^{d}\) _can be realized exactly by a_ \(\sigma_{2}\) _neural network with one hidden layer and_ \(2d\) _neurons._

_(iii)_ \(f(x)=x^{2}\) _can be realized exactly by a_ \(\sigma_{2}\) _neural network with one hidden layer and two neurons._

_(iv)_ \(f(x,y)=xy=\frac{(x+y)^{2}-(x-y)^{2}}{4}\) _can be realized exactly by a_ \(\sigma_{2}\) _neural network with one hidden layer and four neurons._

_(v) Assume_ \(\bm{x^{\alpha}}=x_{1}^{\alpha_{1}}x_{2}^{\alpha_{2}}\cdots x_{d}^{\alpha_{d}}\) _for_ \(\bm{\alpha}\in\mathbb{N}^{d}\)_. For any_ \(N,L\in\mathbb{N}^{+}\) _such that_ \(NL+2^{\lfloor\log_{2}N\rfloor}\geq|\bm{\alpha}|\)_, there exists a_ \(\sigma_{2}\) _neural network_ \(\phi(\bm{x})\) _with the width_ \(4N+2d\) _and depth_ \(L+\lceil\log_{2}N\rceil\) _such that_

\[\phi(\bm{x})=\bm{x^{\alpha}}\]

_for any_ \(\bm{x}\in\mathbb{R}^{d}\)_._

_(vi) Assume_ \(P(\bm{x})=\sum_{j=1}^{J}c_{j}\bm{x}^{\alpha_{j}}\) _for_ \(\bm{\alpha}_{j}\in\mathbb{N}^{d}\)_. For any_ \(N,L,a,b\in\mathbb{N}^{+}\)_such that_ \(ab\geq J\) _and_ \(\left(L-2b-b\log_{2}N\right)N\geq b\max_{j}|\bm{\alpha}_{j}|\)_, there exists a_ \(\sigma_{2}\) _neural network_ \(\phi(\bm{x})\) _with the width_ \(4Na+2d+2\) _and depth_ \(L\) _such that_

\[\phi(\bm{x})=P(\bm{x})\text{ for any }\bm{x}\in\mathbb{R}^{d}.\]

Next, we define a function which will be repeatly used in the proof of Corollary 1 in this section.

**Definition 11**.: _Define \(s(x)\) from \(\mathbb{R}\rightarrow[0,1]\) as_

\[s(x):=\begin{cases}2x^{2},&x\in\left[0,\frac{1}{2}\right]\\ -2(x-1)^{2}+1,&x\in\left[\frac{1}{2},1\right]\\ 1,&x\in[1,2]\\ -2(x-2)^{2}+1,&x\in\left[2,\frac{5}{2}\right]\\ 2(x-3)^{2},&x\in\left[\frac{5}{2},3\right]\\ 0,&\text{Otherwise}.\end{cases}\] (92)

**Definition 12**.: _Given \(K\in\mathbb{N}_{+}\), then we define two functions in \(\mathbb{R}\):_

\[s_{1}(x)=\sum_{i=0}^{K}s\left(4Kx+1-4i\right),\ s_{2}(x)=s_{1}\left(x+\frac{1} {2K}\right).\] (93)

_Then for any \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\), we define_

\[s_{\bm{m}}(\bm{x}):=\prod_{j=1}^{d}s_{m_{j}}(x_{j})\] (94)

_for any \(\bm{x}=(x_{1},x_{2},\ldots,x_{d})\in\mathbb{R}^{d}\)._

**Proposition 8**.: _Given \(N,L,d\in\mathbb{N}_{+}\) with \(NL+2^{\lfloor\log_{2}N\rfloor}\geq d\) and \(L\geq\lceil\log_{2}N\rceil\), and setting \(K=\lfloor N^{1/d}\rfloor\lfloor L^{2/d}\rfloor\), \(\{s_{\bm{m}}(\bm{x})\}_{\bm{m}\in\{1,2\}^{d}}\) defined in Definition 12 satisfies:_

_(i):_ \(\|s_{\bm{m}}(\bm{x})\|_{L^{\infty}((0,1)^{d})}\leq 1\)_,_ \(\|s_{\bm{m}}(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}\leq 8K\) _and_ \(\|s_{\bm{m}}(\bm{x})\|_{W^{1,\infty}((0,1)^{d})}\leq 64K^{2}\) _for any_ \(\bm{m}\in\{1,2\}^{d}\)_._

_(ii):_ \(\{s_{\bm{m}}(\bm{x})\}_{\bm{m}\in\{1,2\}^{d}}\) _is a partition of the unity_ \([0,1]^{d}\) _with_ \(\operatorname{supp}\,s_{\bm{m}}(\bm{x})\cap[0,1]^{d}=\Omega_{\bm{m}}\) _defined in Definition_ 5_._

_(iii):For any \(\bm{m}\in\{1,2\}^{d}\), there is a \(\sigma_{2}\) neural network \(\lambda_{\bm{m}}(\bm{x})\) with the width \(16N+2d\) and depth \(4L+5\) such as_

\[\lambda_{\bm{m}}(\bm{x})=\prod_{j=1}^{d}s_{m_{j}}(x_{j})=s_{\bm{m}}(\bm{x}), \bm{x}\in[0,1]^{d}.\]

Proof.: (i) and (ii) are proved by direct calculation. The proof of (iii) follows:

First, we architect \(s(x)\) by a \(\sigma_{2}\) neural network. The is a \(\sigma_{1}\) neural network \(g(x)\) with \(3\) the width and one layer such that:

\[g(x):=\begin{cases}x,&x\in\left[0,\frac{1}{2}\right]\\ \frac{1}{2},&x\in\left[\frac{1}{2},+\infty\right)\\ 0,&\text{Otherwise}.\end{cases}\] (95)

Based on (iii) in Lemma 10, \(g^{2}(x)\) is a \(\sigma_{2}\) neural network with \(3\) the width and two layers. Then by direct calculation, we notice that

\[s(x)=2g^{2}(x)-2g^{2}(-x+1)+2g^{2}\left(3-x\right)-2g^{2}\left(2+x\right)+ \frac{1}{2},\] (96)which is a \(\sigma_{2}\) neural network with \(12\) the width and two layers. The \(\widetilde{g}(x)\) defined as

\[\widetilde{g}(x)=\sum_{i=0}^{\lfloor N^{1/d}\rfloor-1}s\left(4Kx-4i-\frac{1}{2}\right)\] (97)

is a \(\sigma_{2}\) neural network with \(12(\lfloor N^{1/d}\rfloor)\) the width and two layers.

Similar with Lemma 1, we know that

\[\hat{g}=\widetilde{g}\circ\psi_{2}\circ\psi_{3}\circ\psi_{4}(x)\]

is a \(\sigma_{2}\) neural network with \(12(\lfloor N^{1/d}\rfloor)\) the width and \(5+2\lfloor L^{1/d}\rfloor\), and

\[s_{1}(x)=\hat{g}\left(x+\frac{1}{8K}\right),\;s_{2}(x)=s_{1}\left(x+\frac{1}{2 K}\right),\;x\in[0,1].\] (98)

Based on (v) in Lemma 10, we have there is a \(\sigma_{2}\) neural network \(\lambda_{\bm{m}}(\bm{x})\) with the width \(16N+2d\) and depth \(4L+5\) such as

\[\lambda_{\bm{m}}(\bm{x})=\prod_{j=1}^{d}s_{m_{j}}(x_{j})=s_{\bm{m}}(\bm{x}), \bm{x}\in[0,1]^{d}.\]

#### 7.3.2 Proof of Corollaries 1 and 2

The proof is comprised of three parts, which include Theorem 8 and 9, followed by the combination of these results. Theorem 8 is to apply the Bramble-Hilbert Lemma 9 measured in the norm of \(W^{2,\infty}\):

**Theorem 8**.: _Let \(K\in\mathbb{N}_{+}\) and \(n\geq 2\). Then for any \(f\in W^{n,\infty}((0,1)^{d})\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\) and \(\bm{m}\in\{1,2\}^{d}\), there exist piece-wise polynomials function \(f_{K,\bm{m}}=\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}}(\bm{x})\bm{ x^{\alpha}}\) on \(\Omega_{\bm{m}}\) (Definition 5) with the following properties:_

\[\|f-f_{K,\bm{m}}\|_{W^{2,\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-(n-2)},\] \[\|f-f_{K,\bm{m}}\|_{W^{1,\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-(n-1)},\] \[\|f-f_{K,\bm{m}}\|_{L^{\infty}(\Omega_{\bm{m}})} \leq C_{1}(n,d)K^{-n}.\] (99)

_Furthermore, \(g_{f,\bm{\alpha},\bm{m}}(\bm{x}):\Omega_{\bm{m}}\to\mathbb{R}\) is a constant function with on each \(\Omega_{\bm{m},\bm{i}}\) for \(\bm{i}\in\{0,1\ldots,K\}^{d}\). And_

\[|g_{f,\bm{\alpha},\bm{m}}(\bm{x})|\leq C_{2}(n,d)\] (100)

_for all \(\bm{x}\in\Omega_{\bm{m}}\), where \(C_{1}\) and \(C_{2}\) are constants independent with \(K\)._

The proof is the same as that of Theorem 6. Note that \(\{f_{K,\bm{m}}\}_{\bm{m}\in\{1,2\}^{d}}\) will be same in two theorems if \(f\in W^{n,\infty}((0,1)^{d})\) in two theorem are same.

Theorem 9 is to establish \(\sigma_{2}\) neural networks \(\{\gamma_{\bm{m}}\}_{\{1,2\}^{d}}\), and each \(\gamma_{\bm{m}}\) can approximate \(f\) well on \(\Omega_{\bm{m}}\).

**Theorem 9**.: _For any \(f\in W^{n,\infty}((0,1)^{d})\) with \(\|f\|_{W^{n,\infty}((0,1)^{d})}\leq 1\), any \(N,L\in\mathbb{N}_{+}\) with \(NL+2^{\lfloor\log_{2}N\rfloor}\geq n\) and \(L\geq\lceil\log_{2}N\rceil\), and \(\bm{m}=(m_{1},m_{2},\ldots,m_{d})\in\{1,2\}^{d}\), there is a \(\sigma_{2}\) neural network \(\gamma_{\bm{m}}\) with the width \(28n^{d+1}(N+d)\log_{2}(8N)\) and depth \(11n^{2}(L+2)\log_{2}(4L)\) such that_

\[\|f(\bm{x})-\gamma_{\bm{m}}(\bm{x})\|_{W^{2,\infty}(\Omega_{\bm{m }})} \leq C_{10}(n,d)N^{-2(n-2)/d}L^{-2(n-2)/d}\] \[\|f(\bm{x})-\gamma_{\bm{m}}(\bm{x})\|_{W^{1,\infty}(\Omega_{\bm{m }})} \leq C_{10}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f(\bm{x})-\gamma_{\bm{m}}(\bm{x})\|_{L^{\infty}(\Omega_{\bm{m }})} \leq C_{10}(n,d)N^{-2n/d}L^{-2n/d},\] (101)

_where \(C_{10}\) is the constant independent with \(N,L\)._Proof.: The proof is similar to that of Theorem 7; the difference is that \(xy\) and \(\bm{x}^{\bm{\alpha}}\) can be architected precisely by \(\sigma_{2}\) neural networks.

Without loss of the generalization, we consider the case for \(\bm{m}_{*}=(1,1,\dots,1)\). Due to Theorem 8 and setting \(K=\lfloor N^{1/d}\rfloor^{2}\lfloor L^{2/d}\rfloor\), we have

\[\|f-f_{K,\bm{m}_{*}}\|_{W^{2,\infty}(\Omega_{\bm{m}_{*}})}\leq C_{1 }(n,d)K^{-(n-2)}\leq C_{1}(n,d)N^{-2(n-2)/d}L^{-2(n-2)/d}\] \[\|f-f_{K,\bm{m}_{*}}\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\leq C_{1 }(n,d)K^{-(n-1)}\leq C_{1}(n,d)N^{-2(n-1)/d}L^{-2(n-1)/d}\] \[\|f-f_{K,\bm{m}_{*}}\|_{L^{\infty}(\Omega_{\bm{m}_{*}})}\leq C_{1 }(n,d)K^{-n}\leq C_{1}(n,d)N^{-2n/d}L^{-2n/d},\] (102)

where \(f_{K,\bm{m}_{*}}=\sum_{|\bm{\alpha}|\leq n-1}g_{f,\bm{\alpha},\bm{m}_{*}}(\bm {x})\bm{x}^{\bm{\alpha}}\) for \(x\in\Omega_{\bm{m}_{*}}\). Note that \(g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\) is a constant function for \(\bm{x}\in\prod_{j=1}^{d}\left[\frac{i_{j}}{K},\frac{3+4i_{j}}{4K}\right]\) and \(\bm{i}=(i_{1},i_{2},\dots,i_{d})\in\{0,1,\dots,K-1\}^{d}\). The remaining part is to approximate \(f_{K,\bm{m}_{*}}\) by neural networks.

The way to approximate \(g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\) is same with Theorem 7, and we have that

\[\|\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{ \alpha},\bm{m}_{*}}\left(\bm{x}\right)\|_{W^{2,\infty}(\Omega_{\bm{m}_{*}})}= \|\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{ \alpha},\bm{m}_{*}}\left(\bm{x}\right)\|_{W^{1,\infty}(\Omega_{\bm{m}_{*}})}\] \[= \|\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{ \alpha},\bm{m}_{*}}\left(\bm{x}\right)\|_{L^{\infty}(\Omega_{\bm{m}_{*}})}\] \[\leq 2C_{2}(n,d)N^{-2n}L^{-2n}\] (103)

which is due to \(\phi_{\bm{\alpha}}\left(\bm{\phi}_{2}(\bm{x})\right)-g_{f,\bm{\alpha},\bm{m} _{*}}\left(\bm{x}\right)\) is a step function, and the first order weak derivative is \(0\) in \(\Omega_{\bm{m}_{*}}\).

Due to (v) in Lemma 10, there is a \(\sigma_{2}\) neural network \(\phi_{5,\bm{\alpha}}(\bm{x})\) with the width \(4N+2d\) and depth \(L+\lceil\log_{2}N\rceil\) such that

\[\phi_{5,\bm{\alpha}}(\bm{x})=\bm{x}^{\bm{\alpha}},\;\bm{x}\in\mathbb{R}^{d}.\] (104)

Due to (iv) in Lemma 10, there is a \(\sigma_{2}\) neural network \(\phi_{6}(\bm{x})\) with the width \(4\) and depth \(1\) such that

\[\phi_{6}(x,y)=xy,\;x,y\in\mathbb{R}.\] (105)

Now we define the neural network \(\gamma_{\bm{m}_{*}}(\bm{x})\) to approximate \(f_{K,\bm{m}_{*}}(\bm{x})\) in \(\Omega_{\bm{m}_{*}}\):

\[\gamma_{\bm{m}_{*}}(\bm{x})=\sum_{|\bm{\alpha}|\leq n-1}\phi_{6}\left[\phi_{ \bm{\alpha}}(\bm{\phi}_{2}(\bm{x})),\phi_{5,\bm{\alpha}}(\bm{x})\right].\] (106)

The remaining question is to find the error \(\mathcal{E}\):

\[\widetilde{\mathcal{E}}:= \left\|\sum_{|\bm{\alpha}|\leq n-1}\phi_{6}\left[\phi_{\bm{ \alpha}}(\bm{\phi}_{2}(\bm{x})),\phi_{5,\bm{\alpha}}(\bm{x})\right]-f_{K,\bm{ m}_{*}}(\bm{x})\right\|_{W^{2,\infty}(\Omega_{\bm{m}_{*}})}\] \[\leq \sum_{|\bm{\alpha}|\leq n-1}\|\phi_{6}\left[\phi_{\bm{\alpha}}( \bm{\phi}_{2}(\bm{x})),\phi_{5,\bm{\alpha}}(\bm{x})\right]-g_{f,\bm{\alpha}, \bm{m}_{*}}(\bm{x})\bm{x}^{\bm{\alpha}}\|_{W^{2,\infty}(\Omega_{\bm{m}_{*}})}\] \[= \sum_{|\bm{\alpha}|\leq n-1}\|\phi_{\bm{\alpha}}(\bm{\phi}_{2}( \bm{x}))\bm{x}^{\bm{\alpha}}-g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\bm{x}^{\bm{ \alpha}}\|_{W^{2,\infty}(\Omega_{\bm{m}_{*}})}\] \[\leq n^{2}\sum_{|\bm{\alpha}|\leq n-1}\|\phi_{\bm{\alpha}}(\bm{\phi}_{2 }(\bm{x}))-g_{f,\bm{\alpha},\bm{m}_{*}}(\bm{x})\|_{W^{2,\infty}(\Omega_{\bm{m}_{* }})}\] \[\leq 2n^{d+2}C_{2}(n,d)N^{-2n}L^{-2n}.\] (107)

At last, we finish the proof by estimating the network's the width and depth, implementing \(\gamma_{\bm{m}_{*}}(\bm{x})\). From Eq. (106), we know that \(\gamma_{\bm{m}_{*}}(\bm{x})\) consists of the following subnetworks:

1. \(\phi_{5,\bm{\alpha}}(\bm{x})\) with the width \(4N+2d\) and depth \(L+\lceil\log_{2}N\rceil\).

2. \(\phi_{2}(\bm{x})\) with the width \(4N+5\) and depth \(4L+4\).

3. \(\phi_{\bm{\alpha}}\) with the width \(16n(N+1)\log_{2}(8N)\) and depth \((5L+2)\log_{2}(4L)\).

4. \(\phi_{6}(x,y)\) with the width \(4\) and depth \(1\).

[MISSING_PAGE_EMPTY:32]

Hence

\[\widetilde{\mathcal{R}}\leq 2^{d+7}C_{10}(n,d)N^{-2(n-2)/d}L^{-2(n-2)/d}.\]

At last, we finish the proof by estimating the network's width and depth, implementing \(\gamma(\bm{x})\). From Eq. (111), we know that \(\gamma(\bm{x})\) consists of the following subnetworks:

1. \(\gamma_{\bm{m}}(\bm{x})\) with the width \(28n^{d+1}(N+d)\log_{2}(8N)\) and depth \(11n^{2}(L+2)\log_{2}(4L)\).

2. \(s_{\bm{m}}(\bm{x})\) with the width \(16N+2d\) and depth \(4L+5\).

3. \(\phi_{6}(x,y)\) with the width \(4\) and depth \(1\).

Therefore \(\gamma(\bm{x})\) is a neural network with the width \(2^{d+6}n^{d+1}(N+d)\log_{2}(8N)\) and depth \(15n^{2}(L+2)\log_{2}(4L)\).

Our method can easily extend to approximations measured by the norm of \(W^{m,\infty}\). The primary difference in the proof lies in the need to establish a differential \(\{s_{\bm{m}}(\bm{x})\}_{\{1,2\}^{d}}\), which can be achieved by constructing architected \(s_{\bm{m}}(\bm{x})\) as piece-wise \(m\)-degree polynomial functions. By extending this approach, we can obtain Corollary 2 using our method.

### Proof of Theorem 4

Proof.: The Theorem 4 will be proved by contradiction. The idea of the proof is inspired by Ref. [28].

**Claim 1**.: _There exist \(\rho,C_{1},C_{2},C_{3},J_{0}>0\) and \(s,d\in\mathbb{N}^{+}\) such that, for any \(f\in\mathcal{F}_{n,d}\), we have_

\[\inf_{\phi\in\widehat{\Phi}}|\phi-f|_{W^{1,\infty}((0,1)^{d})}\leq C_{3}L^{-2 (n-1)/d-\rho}N^{-2(n-1)/d-\rho}.\] (114)

_for all \(NL\geq J_{0}\), where_

\[\widehat{\Phi}:=\{\phi:\text{ReLU FNNs $\phi$ with the width $\leq C_{1}N\log N$ and depth $\leq C_{2}L\log L$}\}.\]

The remaining question is to show Claim 1 is invalid.

Denote

\[D\widehat{\Phi}:=\{\psi:\psi=D_{i}\phi,\phi\in\widehat{\Phi},i=1,\dots,d\},\]

Due to Theorem 1, we obtain

\[\operatorname{VCDim}(D\widehat{\Phi})\leq C_{4}N^{2}L^{2}\log_{2}L\log_{2}N= :b_{u}.\] (115)

Now we will use Claim 1 to estimate a lower bound

\[b_{l}:=\lfloor(NL)^{\frac{2}{d}+\frac{\rho}{2(n-1)}}\rfloor^{d}\]

of \(\operatorname{VCDim}(D\widehat{\Phi})\). In other words, we will construct \(\{\psi_{\beta}(\bm{x}):\psi_{\beta}(\bm{x})\in D\widehat{\Phi},\beta\in \mathcal{B}\}\) to scatter \(b_{l}\) points. \(\mathcal{B}\) will be defined later.

First, fix \(i=1,\dots,d\), and there exists \(\widetilde{g}\in C^{\infty}\left(0,1\right)^{d}\) such that \(\frac{\partial\widetilde{g}(\bm{0})}{\partial x_{i}}=1\) and \(\widetilde{g}(\bm{x})=0\) for \(\|\bm{x}\|_{2}\geq 1/3\). And we can find a constant \(C_{5}>0\) such that \(g:=\widetilde{g}/C_{5}\in\mathcal{F}_{n,d}\).

Denote \(M=\lfloor(NL)^{\frac{2}{d}+\frac{\rho}{2(n-1)}}\rfloor\). Divide \([0,1]^{d}\) into \(M^{d}\) non-overlapping sub-cubes \(\{Q_{\bm{\theta}}\}_{\theta}\) as follows:

\[Q_{\bm{\theta}}:=\left\{\bm{x}=\left[x_{1},x_{2},\cdots,x_{d}\right]^{T}\in[0,1]^{d}:x_{i}\in\left[\frac{\theta_{i}-1}{M},\frac{\theta_{i}}{M}\right],i=1,2, \cdots,d\right\},\]

for any index vector \(\bm{\theta}=\left[\theta_{1},\theta_{2},\cdots,\theta_{d}\right]^{T}\in\{1,2, \cdots,M\}^{d}\). Denote the center of \(Q_{\bm{\theta}}\) by \(\bm{x_{\theta}}\) for all \(\bm{\theta}\in\{1,2,\cdots,M\}^{d}\). Define

\[\mathcal{B}:=\left\{\beta:\beta\text{ is a map from }\{1,2,\cdots,M\}^{d}\text{ to }\{-1,1 \}\right\}.\]For each \(\beta\in\mathcal{B}\), we define, for any \(\bm{x}\in\mathbb{R}^{d}\),

\[h_{\beta}(\bm{x}):=\sum_{\bm{\theta}\in\{1,2,\cdots,M\}^{d}}M^{-n}\beta(\bm{ \theta})g_{\bm{\theta}}(\bm{x}),\quad\text{ where }g_{\bm{\theta}}(\bm{x})=g\left(M\cdot(\bm{x}-\bm{x}_{\bm{\theta}}) \right).\]

Due to \(|\mathrm{supp}\widetilde{g}(\bm{x})|\leq\frac{2}{3}\) and \(|D^{\bm{\alpha}}h_{\beta}(\bm{x})|\leq M^{-n+|\bm{\alpha}|}\|g\|_{W^{n,\infty}}\leq 1\), we obtain that

\[|D^{\bm{\alpha}}f_{\beta}(\bm{x})|\leq 1\]

for any \(|\bm{\alpha}|\leq n\) Therefore, \(f_{\beta}\in\mathcal{F}_{n,d}\). And it is easy to check \(\{D_{i}h_{\beta}=h_{\beta}:\beta\in\mathcal{B}\}\) can scatters \(b_{l}\) points since \(\frac{\partial\widetilde{g}(\bm{0})}{\partial x_{i}}=1\) and \(\widetilde{g}(\bm{x})=0\) for \(\|\bm{x}\|_{2}\geq 1/3\).

Note that for any \(h_{\beta}\in\mathcal{F}_{n,d}\), there is a \(\phi_{\beta}\in\widehat{\Phi}\) such that \(C_{3}(NL)^{-\frac{2(n-1)}{d}-\frac{\theta}{2}}\geq|D_{i}h_{\beta}(\bm{x_{\theta }})-D_{i}\phi_{\beta}(\bm{x_{\theta}})|\) for any \(J_{g}\leq NL\) due to Claim 1. Denote \(J_{1}=\max_{\beta\in\mathcal{B}}\{J_{\beta}\}\). There is a constant \(J_{2}\) such that \(\frac{M^{-n+1}}{C_{5}}\geq C_{3}(NL)^{-\frac{2(n-1)}{d}-\rho}\) for \(J_{2}\leq NL\). Define \(J:=\max\{J_{1},J_{2}\}\), then for any \(J\leq NL\), we have

\[|D_{i}h_{\beta}(\bm{x_{\theta}})|=\left|M^{-n+1}\frac{\partial g(\bm{x_{\theta }})}{\partial x_{i}}\right|=\frac{M^{-n+1}}{C_{5}}\geq C_{3}(NL)^{\frac{-2(n-1 )}{d}-\rho}\geq|D_{i}h_{\beta}(\bm{x_{\theta}})-D_{i}\phi_{\beta}(\bm{x_{\theta }})|.\] (116)

In other words, for any \(\beta\in\mathcal{B}\) and \(\bm{\theta}\in\{1,2,\cdots,M\}^{d},D_{i}f_{\beta}\left(\bm{x_{\theta}}\right)\) and \(D_{i}\phi_{\beta}\left(\bm{x_{\theta}}\right)\) have the same sign. Then \(\{D_{i}\phi_{\beta}:\beta\in\mathcal{B}\}\) shatters \(\left\{\bm{x_{\theta}}:\bm{\theta}\in\{1,2,\cdots,M\}^{d}\right\}\) since \(\{D_{i}h_{\beta}:\beta\in\mathcal{B}\}\) shatters \(\left\{\bm{x_{\theta}}:\bm{\theta}\in\{1,2,\cdots,M\}^{d}\right\}\) as discussed above. Hence,

\[\mathrm{VCDim}\left(\{\phi_{\beta}:\beta\in\mathcal{B}\}\right)\geq M^{d}=b_ {l},\] (117)

for \(N,L\in\mathbb{N}\) with \(NL\geq J\).

By Eqs. (115,117), for any \(N,L\in\mathbb{N}\) with \(NL\geq J\), we have \(b_{l}\leq\mathrm{VCDim}\left(\{\phi_{\beta}:\beta\in\mathcal{B}\}\right)\leq \mathrm{VCDim}(D\widehat{\Phi})\leq b_{u}\), implying that

\[\lfloor(NL)^{\frac{2}{d}+\frac{\rho}{2(n-1)}}\rfloor^{d}\leq C_{4}N^{2}L^{2} \log_{2}L\log_{2}N\] (118)

which is a contradiction for sufficiently large \(N,L\in\mathbb{N}\). So we finish the proof of Theorem 4. \(\Box\)

Based on the proof of Theorem 4, we can easily check that the estimation of VC-dimension of DNN derivatives (Theorem 1) is nearly optimal and prove Corollary 3. Assume \(\mathrm{VCDim}(D\widehat{\Phi})\leq b_{u}=\bm{O}(N^{2-\varepsilon}L^{2-\varepsilon})\) in Eq. (118) for \(\varepsilon>0\), and \(b_{l}\) must be larger than \(\lfloor(NL)^{\frac{2}{d}}\rfloor^{d}\) according the construction in the proof of Theorem 4 and Theorem 3. Hence we still obtain a contradiction in Eq. (118), and the estimation in Theorem 1 is nearly optimal.

### Proof of Theorem 5

#### 7.5.1 Bounding generalization error by Rademacher complexity

**Definition 13** (Rademacher complexity [3]).: _Given a sample set \(S=\{z_{1},z_{2},\ldots,z_{M}\}\) on a domain \(\mathcal{Z}\), and a class \(\mathcal{F}\) of real-valued functions defined on \(\mathcal{Z}\), the empirical Rademacher complexity of \(\mathcal{F}\) in \(S\) is defined as_

\[\textbf{R}_{S}(\mathcal{F}):=\frac{1}{M}\textbf{E}_{\Sigma_{M}}\left[\sup_{f\in \mathcal{F}}\sum_{i=1}^{M}\sigma_{i}f(z_{i})\right],\]

_where \(\Sigma_{M}:=\{\sigma_{1},\sigma_{2},\ldots,\sigma_{M}\}\) are independent random variables drawn from the Rademacher distribution, i.e., \(\textbf{P}(\sigma_{i}=+1)=\textbf{P}(\sigma_{i}=-1)=\frac{1}{2}\) for \(i=1,2,\ldots,M.\) For simplicity, if \(S=\{z_{1},z_{2},\ldots,z_{M}\}\) is an independent random variable set with the uniform distribution, denote_

\[\textbf{R}_{M}(\mathcal{F}):=\textbf{E}_{S}\textbf{R}_{S}(\mathcal{F}).\]

The following lemma will be used to bounded generalization error by Rademacher complexities:

**Lemma 11** ([47], Proposition 4.11).: _Let \(\mathcal{F}\) be a set of functions. Then_

\[\mathbf{E}_{X}\sup_{u\in\mathcal{F}}\left|\frac{1}{M}\sum_{i=1}^{M}u(x_{j})- \mathbf{E}_{x\sim\mathcal{P}_{\Omega}}u(x)\right|\leq 2\mbox{\emph{{R}}}_{M}( \mathcal{F}),\]

_where \(X:=\{x_{1},\ldots,x_{M}\}\) is an independent random variable set with the uniform distribution._

Now we can show that generalization error can be bounded by Rademacher complexities of two function sets.

**Lemma 12**.: _Let \(d,N,L,M\in\mathbb{N}_{+}\), \(B,C_{1},C_{2}\in\mathbb{R}_{+}\). For any \(f\in W^{1,\infty}((0,1)^{d})\) with \(\|f\|_{W^{1,\infty}((0,1)^{d})}\leq 1\), set_

\[\widetilde{\Phi} :=\{\phi:\phi\text{ with the width }\leq C_{1}N\log N\text{ and depth }\leq C_{2}L\log L,\|\phi\|_{W^{1,\infty}((0,1)^{d})}\leq B\}\] \[D\widetilde{\Phi} :=\{\psi:\psi=D_{i}\phi,i=1,\ldots,d\}.\] (119)

_We have_

\[2\sup_{\boldsymbol{\theta},\phi(\boldsymbol{x};\boldsymbol{\theta})\in \widetilde{\Phi}}|\mathbf{E}(\mathcal{R}_{S}(\boldsymbol{\theta}))-\mathcal{ R}_{D}(\boldsymbol{\theta})|\leq 4(B+1)(d\mbox{\emph{{R}}}_{M}(D\widetilde{ \Phi})+\mbox{\emph{{R}}}_{M}(\widetilde{\Phi})),\]

_where \(\mathbf{E}\) is expected responding to \(X\), and \(X:=\{\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{M}\}\) is an independent random variables set uniformly distributed on \((0,1)^{d}\)._

Proof.: For any \(\phi(\boldsymbol{x};\boldsymbol{\theta})\in\widetilde{\Phi}\), we have

\[|\mathbf{E}(\mathcal{R}_{S}(\boldsymbol{\theta}))-\mathcal{R}_{D} (\boldsymbol{\theta})|\] \[= \sum_{j=1}^{d}\left(\mathbf{E}\frac{1}{M}\sum_{i=1}^{M}\left| \frac{\partial(f(\boldsymbol{x}_{i})-\phi(\boldsymbol{x}_{i};\boldsymbol{ \theta}))}{\partial x_{j}}\right|^{2}-\int_{(0,1)^{d}}\left|\frac{\partial(f( \boldsymbol{x})-\phi(\boldsymbol{x};\boldsymbol{\theta}))}{\partial x_{j}} \right|^{2}\,\mathrm{d}\boldsymbol{x}\right)\] \[+\mathbf{E}\frac{1}{M}\sum_{i=1}^{M}\left|(f(\boldsymbol{x}_{i})- \phi(\boldsymbol{x}_{i};\boldsymbol{\theta}))\right|^{2}-\int_{(0,1)^{d}}\left| (f(\boldsymbol{x})-\phi(\boldsymbol{x};\boldsymbol{\theta}))\right|^{2}\, \mathrm{d}\boldsymbol{x}\] \[\leq (B+1)\sum_{j=1}^{d}\left(\mathbf{E}\left|\frac{1}{M}\sum_{i=1}^{ M}\frac{\partial(f(\boldsymbol{x}_{i})-\phi(\boldsymbol{x}_{i};\boldsymbol{\theta}))}{ \partial x_{j}}-\int_{(0,1)^{d}}\frac{\partial(f(\boldsymbol{x})-\phi( \boldsymbol{x};\boldsymbol{\theta}))}{\partial x_{j}}\,\mathrm{d}\boldsymbol{ x}\right|\right)\] \[+(B+1)\mathbf{E}\left|\frac{1}{M}\sum_{i=1}^{M}(f(\boldsymbol{x}_ {i})-\phi(\boldsymbol{x}_{i};\boldsymbol{\theta}))-\int_{(0,1)^{d}}(f( \boldsymbol{x})-\phi(\boldsymbol{x};\boldsymbol{\theta}))\,\mathrm{d} \boldsymbol{x}\right|\] \[\leq 2(B+1)(d\mbox{\emph{{R}}}_{M}(D\widetilde{\Phi})+\mbox{\emph{{R }}}_{M}(\widetilde{\Phi}))\] (120)

where the last inequality is due to Lemma 12. 

#### 7.5.2 Bounding the Rademacher complexity and the proof of Theorem 5

In this subsection, we aim to estimate the Rademacher complexity using the covering number. We then estimate the covering number using the pseudo-dimension.

**Definition 14** (covering number [3]).: _Let \((V,\|\cdot\|)\) be a normed space, and \(\Theta\in V\). \(\{V_{1},V_{2},\ldots,V_{n}\}\) is an \(\varepsilon\)-covering of \(\Theta\) if \(\Theta\subset\cup_{i=1}^{n}B_{\varepsilon,\|\cdot\|}(V_{i})\). The covering number \(\mathcal{N}(\varepsilon,\Theta,\|\cdot\|)\) is defined as_

\[\mathcal{N}(\varepsilon,\Theta,\|\cdot\|):=\min\{n:\exists\varepsilon\text{- covering over }\Theta\text{ of size }n\}.\]

**Definition 15** (Uniform covering number [3]).: _Suppose the \(\mathcal{F}\) is a class of functions from \(\mathcal{F}\) to \(\mathbb{R}\). Given \(n\) samples \(\boldsymbol{Z}_{n}=(z_{1},\ldots,z_{n})\in\mathcal{X}^{n}\), define_

\[\mathcal{F}|_{\boldsymbol{Z}_{n}}=\{(u(z_{1}),\ldots,u(z_{n})):u\in\mathcal{F}\}.\]

_The uniform covering number \(\mathcal{N}(\varepsilon,\mathcal{F},n)\) is defined as_

\[\mathcal{N}(\varepsilon,\mathcal{F},n)=\max_{\boldsymbol{Z}_{n}\in\mathcal{X}^{ n}}\mathcal{N}\left(\varepsilon,\mathcal{F}|_{\boldsymbol{Z}_{n}},\|\cdot\|_{ \infty}\right),\]

_where \(\mathcal{N}\left(\varepsilon,\mathcal{F}|_{\boldsymbol{Z}_{n}},\|\cdot\|_{ \infty}\right)\) denotes the \(\varepsilon\)-covering number of \(\mathcal{F}|_{\boldsymbol{Z}_{n}}\) w.r.t the \(L_{\infty}\)-norm._Then we use a lemma to estimate the Rademacher complexity using the covering number.

**Lemma 13** (Dudley's theorem [3]).: _Let \(\mathcal{F}\) be a function class such that \(\sup_{f\in\mathcal{F}}\|f\|_{\infty}\leq B\). Then the Rademacher complexity \(\textbf{R}_{n}(\mathcal{F})\) satisfies that_

\[\textbf{R}_{n}(\mathcal{F})\leq\inf_{0\leq\delta\leq B}\left\{4\delta+\frac{12 }{\sqrt{n}}\int_{\delta}^{B}\sqrt{\log 2\mathcal{N}(\varepsilon,\mathcal{F},n)} \,\mathrm{d}\varepsilon\right\}\]

To bound the Rademacher complexity, we employ Lemma 13, which bounds it by the uniform covering number. We estimate the uniform covering number by the pseudo-dimension based on the following lemma.

**Lemma 14** ([3]).: _Let \(\mathcal{F}\) be a class of functions from \(\mathcal{X}\) to \([-B,B]\). For any \(\varepsilon>0\), we have_

\[\mathcal{N}(\varepsilon,\mathcal{F},n)\leq\left(\frac{2enB}{\varepsilon\text{ Pfim}(\mathcal{F})}\right)^{\text{Pdim}(\mathcal{F})}\]

_for \(n\geq\text{Pdim}(\mathcal{F})\)._

The remaining problem is to bound \(\text{Pdim}(\widetilde{\Phi})\) and \(\text{Pdim}(D\widetilde{\Phi})\). Based on [4], \(\text{Pdim}(\widetilde{\Phi})=\boldsymbol{O}(L^{2}N^{2}\log_{2}L\log_{2}N)\). For the \(\text{Pdim}(D\widetilde{\Phi})\), we can estimate it by Theorem 2.

Now we can estimate generalization error based on Lemma 12.

Proof of Theorem 5.: Let \(J=\max\{\text{Pdim}(D\widetilde{\Phi}),\text{Pdim}(\widetilde{\Phi})\}\). Due to Lemma 13, 14 and Theorem 2, for any \(M\geq J\), we have

\[\textbf{R}_{M}(D\widetilde{\Phi})\leq 4\delta+\frac{12}{\sqrt{M}}\int_{\delta}^{B}\sqrt{\log 2 \mathcal{N}(\varepsilon,D\widetilde{\Phi},M)}\,\mathrm{d}\varepsilon\] \[\leq 4\delta+\frac{12}{\sqrt{M}}\int_{\delta}^{B}\sqrt{\log 2\left( \frac{2eMB}{\varepsilon\text{Pdim}(D\widetilde{\Phi})}\right)^{\text{Pdim}(D \widetilde{\Phi})}}\,\mathrm{d}\varepsilon\] \[\leq 4\delta+\frac{12B}{\sqrt{M}}+12\left(\frac{\text{Pdim}(D \widetilde{\Phi})}{M}\right)^{\frac{1}{2}}\int_{\delta}^{B}\sqrt{\log\left( \frac{2eMB}{\varepsilon\text{Pdim}(D\widetilde{\Phi})}\right)}\,\mathrm{d}\varepsilon.\] (121)

By the direct calculation for the integral, we have

\[\int_{\delta}^{B}\sqrt{\log\left(\frac{2eMB}{\varepsilon\text{Pdim}(D \widetilde{\Phi})}\right)}\,\mathrm{d}\varepsilon\leq B\sqrt{\log\left(\frac {2eMB}{\delta\text{Pdim}(D\widetilde{\Phi})}\right)}.\]

Then choosing \(\delta=B\left(\frac{\text{Pdim}(D\widetilde{\Phi})}{M}\right)^{\frac{1}{2}}\leq B\), we have

\[\textbf{R}_{M}(D\widetilde{\Phi})\leq 28B\left(\frac{\text{Pdim}(D\widetilde{ \Phi})}{M}\right)^{\frac{1}{2}}\sqrt{\log\left(\frac{2eM}{\text{Pdim}(D \widetilde{\Phi})}\right)}.\] (122)

Therefore, due to Theorem 2, there is a constant \(C_{4}\) independent with \(L,N,M\) such as

\[\textbf{R}_{M}(D\widetilde{\Phi})\leq C_{4}\frac{NL(\log_{2}L\log_{2}N)^{ \frac{1}{2}}}{\sqrt{M}}\log M.\] (123)

\(\textbf{R}_{M}(\widetilde{\Phi})\) can be estimate in the similar way. Due to Lemma 12, we have that there is a constant \(C_{5}=C_{5}(B,d,C_{1},C_{2})\) such that

\[\textbf{E}\mathcal{R}_{S}(\boldsymbol{\theta}_{D})-\mathcal{R}_{D}( \boldsymbol{\theta}_{D})+\textbf{E}\mathcal{R}_{D}(\boldsymbol{\theta}_{S})- \textbf{E}\mathcal{R}_{S}(\boldsymbol{\theta}_{S})\leq C_{5}\frac{NL(\log_{2} L\log_{2}N)^{\frac{1}{2}}}{\sqrt{M}}\log M.\] (124)