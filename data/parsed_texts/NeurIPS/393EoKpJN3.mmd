[MISSING_PAGE_FAIL:1]

sis, ultimately deepening our comprehension of communication and sociality in non-human primates.

## 1 Introduction

Studying the behavior of non-human primates is essential for gaining evolutionary insights (Langergraber et al., 2012), conducting biomedical research (Schapiro et al., 2005), and improving animal welfare (Dawkins, 2003; Gonyou, 1994). Furthermore, given the close phylogenetic proximity between humans and non-human primates, it provides an ethically sound and effective avenue to probe the roots of human sociality (The Chimpanzee Sequencing and Analysis Consortium, 2005). Traditional field research typically requires researchers to enter wildlife conservation areas for extended durations, sometimes spanning multiple years. This involves habituating primate groups to human presence, capturing video footage, and laboriously manually coding these videos for subsequent statistical analysis (Hobaiter et al., 2017; Frohlich et al., 2020; Surbeck et al., 2017; Luncz et al., 2018; Sirianni et al., 2015). While video coding is heralded as the gold standard for distilling rich, nuanced behavioral patterns (Wiltshire et al., 2023), its practical utility hinges on the efficiency of the coding process. This not only demands researchers with specialized expertise but is also prone to attentional biases.

Recent strides in computer vision offer promise for the automated analyses of non-human primate behaviors, especially those of chimpanzees. Nevertheless, the scarcity of high-quality longitudinal datasets remains a bottleneck. Assembling chimpanzee behavioral data is a formidable endeavor, necessitating substantial resources and expertise. This process entails continuous video recording and meticulous manual annotation, with a keen emphasis on annotation accuracy and consistency. While some datasets (Marks et al., 2022; Bala et al., 2020) confine subjects to indoor enclosures, resulting in atypical and constrained environments, others resort to sourcing and labeling chimpanzee images online (Labuguen et al., 2021; Desai et al., 2022; Ng et al., 2022; Yao et al., 2023). Unfortunately, these often overlook the intricate social dynamics inherent to chimpanzee groups, hindering a comprehensive study of their social behaviors and social relationships.

Addressing the existing limitations, we introduce ChimpACT, a comprehensive longitudinal dataset tailored for the in-depth study of chimpanzee social behavior in a semi-naturalistic setting, replete with annotations of instance bounding boxes, body poses, and spatial-temporal action labels. A comparison with other datasets is provided in Tab. 1. ChimpACT encompasses footage of a specific chimpanzee group residing at Leipzig Zoo, Germany, with a particular focus on a juvenile male named Azibo (refer to Fig. 1). The data, gathered between 2015 and 2018, employs _focal sampling_

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \multirow{2}{*}{Dataset} & \multirow{2}{*}{Species} & \multicolumn{3}{c}{Track 1} & \multicolumn{3}{c}{Track 2} & \multicolumn{3}{c}{Track 3} \\  & & & detection, tracking, & ReID & & pose estimation & & action recognition & Source \\ \hline  & & ID \# & frame \# & box \# & track & frame \# & pose \# & track & dim. & class \# & label \# & \\ \hline AP-10K & G & ✗ & ✗ & ✗ & ✗ & 10,015 & 13,028 & ✗ & 2D & ✗ & ✗ & I \\ (Vic et al., 2021) & G & ✗ & ✗ & ✗ & 10,015 & [\(<\)00] & & & & & & \\ AnimalKingdom & G & ✗ & ✗ & ✗ & 33,099 & 99,297 & ✗ & 2D & 140 & 30,100 & I \\ (Ng et al., 2022) & & & & & & [576] & ✗ & 10,876 & ✗ & 2D & ✗ & ✗ & I \\ OpenAgePose & P & ✗ & ✗ & ✗ & ✗ & 111,529 & [11,529 & ✗ & 2D & ✗ & ✗ & I \& Z \\ OpenMoneyChallenge & P & ✗ & ✗ & ✗ & 111,529 & [11,529 & ✗ & 33,192 & ✗ & & C \\
**OpenMoneyStudio** & & & & & & & [0] & ✓ & 3D & ✗ & ✗ & (6.7\(m^{2}\)) \\
**MacaquePose** & M & ✗ & ✗ & ✗ & 13,083 & 16,393 & ✗ & 2D & ✗ & ✗ & I \& Z \\
**Labuguen et al., 2021** & & & & & & & [0] & ✗ & ✗ & ✗ & 4 & \(\oslash\) & C \\ SIPC & M & 4 & 191 & 2,200 & ✓ & ✗ & ✗ & ✗ & ✗ & 4 & \(\oslash\) & C \\ (Marks et al., 2022) & & & & & & & & & & (15\(m^{2}\)) \\ CCR & & & & & & & & & & & & \\ (Bain et al., 2019) & C & 13 & 936,914 & 1,937,585 & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & W \\ \hline \multicolumn{1}{c}{\begin{tabular}{c} **ChimpACT** \\ (Ours) \\ \end{tabular} } & C & 23 & 160,500 & 56,324 & ✓ & 16,028 & 56,324 & ✓ & 2D & 23 & 64,289 & 
\begin{tabular}{c} CP \\ (4400\(m^{2}\)) \\ \end{tabular} \\ \hline \end{tabular}
\end{table}
Table 1: **Comparison of ChimpACT with existing primate behavioral datasets. Square-bracketed numbers denote label counts for the chimpanzee category. \(\oslash\) denotes undocumented. For the “Species” row, G represents general, P for primates, M for macaque, and C for chimpanzee. In the “Source” row, I stands for Internet, Z for zoo, C for cage, W for wild, and CP for captive.**(Altmann, 1974). Born in April 2015, Azibo1 has been living in the group since birth, providing a unique perspective on the development of an individual within a chimpanzee group characterized by well-defined kin relationships. (also depicted in Fig. 2a). The footage covers the daily lives of over 20 chimpanzees in a group, aggregating to 163 video recordings, approximately 160,500 frames, and spanning around 2 hours.

Footnote 1: Details about Azibo can be found at https://tinyurl.com/azibo-chimp/.

Our annotations on ChimpACT are extensive, marking each individual's detection, tracking, identification, pose estimation, and spatiotemporal action detection. Sample frames with their corresponding annotations are illustrated in Fig. 1. Each chimpanzee's identity is confirmed by a seasoned behavioral researcher familiar with the Leipzig chimpanzees, ensuring data precision and trustworthiness. Crucially, we employ an ethogram (detailed in Fig. 2b) devised by the same expert for fine-grained action labels. To our knowledge, ChimpACT is the first to furnish ethogram annotations for the machine learning and computer vision community. This bespoke ethogram delineates behaviors into four categories: locomotion, object interaction, social interaction, and others, with each encompassing several detailed actions we diligently annotate.

While advancements in computer vision have notably addressed human-centric tasks, such as human pose estimation (Sun et al., 2019; Xiao et al., 2018), the dearth of chimpanzee datasets has curtailed progress on chimpanzee-specific challenges. Despite their genetic closeness to humans (The Chimpazee Sequencing and Analysis Consortium, 2005), deciphering chimpanzee behaviors is intricate due to their unique morphology, appearance, and keypoint articulation. Highlighting the importance of crafting sophisticated chimpanzee perception models, we evaluate prominent human perception methods on three tracks: (i) detection, tracking, and identification (ReID), (ii) pose estimation, and (iii) spatiotemporal action detection. Our findings underscore ChimpACT's potential as a platform for the community to pioneer advanced techniques for better perception of the chimpanzees and ultimately contribute to a deeper understanding of non-human primates.

## 2 Related work

Computer vision for animalsA myriad of datasets and benchmarks have emerged, harnessing computer vision techniques to advance animal research. For instance, 3D-ZeF20 (Pedersen et al., 2020) introduces 3D tracking of zebrafish to the MOT benchmarks. AnimalTrack (Zhang et al., 2023) emphasizes multi-animal tracking across a spectrum of species. AP-10K (Yu et al., 2021) and APT-36K (Yang et al., 2022) venture into animal pose estimation for diverse species. AnimalKingdom (Ng et al., 2022) extends its focus to fine-grained multi-label action recognition. Moreover, several studies have delved into multi-agent behavior understanding from a social interaction perspective (Sun et al., 2021, 2023). Distinctively, ChimpACT stands out as a holistic benchmark, encompassing three varied downstream tasks and boasting rich annotations of social interactions.

Human video datasetsIn contrast to animal-centric video datasets, a more substantial collection is tailored to human subjects, addressing diverse human-centric video understanding tasks. For instance, the MOT Challenge (Milan et al., 2016) is curated for multi-person tracking. Other benchmarks like COCO (Lin et al., 2014) and MPII (Andriluka et al., 2014) cater to human pose estimation. Meanwhile, datasets such as Kinetics (Kay et al., 2017), ActivityNet (Fabian Caba Heilbron and Niebles, 2015), and AVA (Gu et al., 2018) are dedicated to human action recognition. With ChimpACT, we encompass analogous tasks but introduce challenges specific to chimpanzee behavior.

Datasets on primate behavioral understandingMost existing primate datasets are tailored towards individual primate detection and pose estimation. These either stem from confined indoor settings (Bala et al., 2020; Marks et al., 2022) or are amassed and labeled from online sources (Labuguen et al., 2021; Desai et al., 2022; Ng et al., 2022; Yao et al., 2023). The former can induce atypical behavioral patterns, while the latter often omits longitudinal interactions, rendering them suboptimal for analyzing chimpanzee social dynamics. A notable exception is the CCR dataset (Bain et al., 2019), chronicling 13 chimpanzees in the Bossou forest over two years. Yet, it primarily focuses on individual detection and recognition, lacking behavioral annotations, which limits its efficacy for probing the social nuances of wild primates. Tab. 1 offers a comprehensive comparison. The narrow focus of most primate datasets on singular tasks restricts their breadth and adaptability to diverse research inquiries. Contrarily, ChimpACT presents a multifaceted approach, encompassing identities, kinship, detection labels, pose annotations, ethograms, and fine-grained action labels. This richness positions it as an indispensable tool for devising advanced chimpanzee behavior analysis methods and enriching the overarching comprehension of primate behavior.

Methods for primate behavioral analysisDeciphering primate behavior is instrumental in understanding their social dynamics and cognitive abilities. Behavioral analysis often encompasses subtasks like individual detection, tracking, and identification (Bain et al., 2019; Marks et al., 2022), pose estimation (Labuguen et al., 2021; Desai et al., 2022; Mathis et al., 2018; Wiltshire et al., 2023), and behavior recognition (Ng et al., 2022; Bain et al., 2021). While each task has specialized techniques, many are rooted in human behavioral research. Numerous algorithms exist for human tracking (Bewley et al., 2016; Pang et al., 2021), pose estimation (Sun et al., 2019; Xiao et al., 2018), and behavior recognition (Feichtenhofer et al., 2019). However, due to the dearth of primate datasets, primate behavioral analysis often repurposes algorithms designed for humans, including:

* **Detection, tracking, and ReID** identify individual primates in videos, often leveraging established object or human detection algorithms like Mask-RCNN (He et al., 2017). For instance, SIPEC (Marks et al., 2022) employs Mask-RCNN with a ResNet backbone (He et al., 2016) to track and segment macaque. Bain et al. (2019) utilize CNNs to crop and identify individual chimpanzees.
* **Pose estimation** discerns primate poses, frequently adapting human pose estimation methods like SimpleBaseline (Xiao et al., 2018). DeepLabCut (Mathis et al., 2018; Lauer et al., 2022), for instance, employs ResNet-50 with ImageNet pre-trained weights for 2D animal pose estimation. SIPEC (Marks et al., 2022) modifies SimpleBaseline for 2D macaque poses.
* **Behavior recognition** identifies primate actions and interactions. Contemporary methods (Bain et al., 2021; Bohnslav et al., 2021) often derive from human action recognition algorithms like SlowFast (Schindler and Steinhage, 2021). Notably, Bain et al. (2021) integrates audio cues for classifying two simple non-interactive behaviors: nut cracking and buttress drumming. In contrast, ChimpACT encompasses over 20 daily behaviors under an ethogram hierarchy, capturing both solitary actions and intricate social interactions.

In essence, primate behavioral analysis is a multifaceted endeavor, intertwining computer vision, machine learning, and primatology. The advent of ChimpACT marks a significant stride towards unraveling the intricate social tapestry of our primate kin.

## 3 ChimpACT

### Dataset description

ChimpACT comprises about 2-hour video footage of chimpanzees recorded at the Leipzig Zoo in Germany between 2015 and 2018. The videos focus on one male chimpanzee, Azibo, who was born in April 2015 to Swela and has lived with the A-chimpanzee group2 at the Leipzig Zoo ever since.

Figure 2: **(a) Kinship of the observed chimpanzee group. Rectangles and ellipses represent males and females, respectively, with arrows flowing from the parents to the child. Their vertical position relative to the time axis indicates the year of birth. (b) Ethogram with annotated behaviors.**

[MISSING_PAGE_EMPTY:5]

Detection, tracking, and ReIDThis task encompasses the detection and tracking of individual chimpanzees across video sequences, subsequently coupled with their re-identification. ChimpACT features over 23 distinct chimpanzee individuals, each identified by a primate expert familiar with the Leipzig A-group chimpanzees. Initially, annotators were instructed to delineate the bounding box of each chimpanzee, ensuring consistent box IDs for the same individual throughout a video clip. Subsequently, the expert matched these box IDs with the corresponding true names of the chimpanzees, resulting in the identification of 23 unique individuals. Additionally, every annotated bounding box is attached with a visibility attribute, indicating if the chimpanzee is fully visible, truncated, or occluded in a given frame. Such visibility annotations can support the reasoning of the chimpanzee behavior, potentially bolstering tracking robustness. Fig. 2(a) illustrates the occurrence frequency (on a _log_ scale) of each individual, revealing a long-tail distribution. This pattern aligns with the focal sampling strategy, where Azibo is the primary subject. Notably, Swela, Azibo's mother, also exhibits a high occurrence frequency, resonating with prior studies (Boesch, 1996).

Pose estimationPose estimation aims to predict the locations of the chimpanzee joints that have semantic meaning, such as the knee and shoulder, from an input image. There are four keypoints on the chimpanzee's face (_i.e._, two for the eyes, and one each for the upper and lower lips), for a total of 16 chimpanzee keypoints (refer to Sec. 3.3 and Fig. 4). Annotators are tasked with marking the 2D joint coordinates and the visibility status of each joint. We adopt the visibility protocol from the COCO 2D human keypoint annotations (Lin et al., 2014), where a value of 0 indicates a joint outside the image frame, 1 signifies an obscured joint within the image, and 2 designates a clearly visible joint. Such an annotation protocol affords reason about chimpanzee's orientation and action based on facial joint visibility. For instance, the chimpanzee might be eating something if the two lips are apart. Sample frames showcasing pose annotations are depicted in Fig. 1. Notably, ChimpACT holds the potential for future expansion to encompass pose tracking tasks, analogous to the PoseTrack (Andriluka et al., 2018) for humans.

Spatiotemporal action detectionSpatiotemporal action detection seeks to attribute one or multiple behavioral labels to each bounding box containing a chimpanzee, leveraging the spatiotemporal context within a video clip. Our ethogram, detailed in Fig. 1(b), delineates 23 nuanced subcategories of behaviors and guides the fine-grained annotations of chimpanzee behavior, such as "climbing"

\begin{table}
\begin{tabular}{c c c c} \hline \hline No. & Definition & No. & Definition \\ \hline
0 & Root of hip & 8 & Right eye \\
1 & Right knee & 9 & Left eye \\
2 & Right ankle & 10 & Right shoulder \\
3 & Left knee & 11 & Right elbow \\
4 & Left ankle & 12 & Right wrist \\
5 & Neck & 13 & Left shoulder \\
6 & Upper lip & 14 & Left elbow \\
7 & Lower lip & 15 & Left wrist \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Keypoint definitions for chimpanzee.**

Figure 3: **(a) Distribution (in log scale) of annotations for each individual. (b) Distribution (in log scale) of annotations for each behavior. Vector graphics; zoom for details.**

within the "locomotion" category. Notably, within the realm of social interactions, we meticulously differentiate between the action performer and receiver. For instance, the grooming behavior is bifurcated into "grooming" and "being groomed." Every chimpanzee in a frame has its subcategory behavior annotated. It is not uncommon for an individual to simultaneously exhibit multiple behaviors, exemplified by Swela's "carrying" and "moving" actions in Fig. 1. The distribution of these behavioral annotations, visualized in Fig. 2(b) on a _log_ scale, reveals a long-tail distribution, mirroring the authentic behavioral tendencies of chimpanzees in their natural habitats.

Fig. 4(a) showcases the distribution of the annotated behaviors, with social interactions constituting approximately 35% of the total annotations. Furthermore, Fig. 4(b) delineates the distribution of labeled social behaviors across distinct individuals, highlighting grooming, playing, and touching as predominant activities within the social dynamics of the group-living chimpanzees.

In essence, ChimpACT emerges as an invaluable resource for researchers spanning the domains of primatology, comparative psychology, computer vision, and machine learning. It furnishes a comprehensive and varied array of annotations, paving the way for in-depth analysis of multifaceted chimpanzee behaviors and catalyzing the development of advanced machine learning algorithms. The inherent long-tail distribution not only presents a formidable challenge for chimpanzee identification and behavior recognition but also beckons explorations into few-shot learning in future endeavors.

## 4 Experiments

To rigorously assess ChimpACT, we benchmark a suite of representative methods across the aforementioned three tracks: (i) detection, tracking, and ReID, (ii) pose estimation, and (iii) spatiotemporal action detection. Our computational framework leverages four NVIDIA GeForce RTX 3090 GPUs (24GB) for both training and evaluation across all tracks. In the subsequent sections, we delve into the implementation details, baseline methods, and evaluation metrics for each track.

### Detection, tracking, and ReID

SettingWe evaluate several prominent Multiple Object Tracking (MOT) algorithms on ChimpACT, including both classical methods such as SORT (Bewley et al., 2016), DeepSORT (Wojke et al., 2017), and Tracktor (Bergmann et al., 2019), as well as the state-of-the-art methods such as ByteTrack (Zhang et al., 2022), and OC-SORT (Cao et al., 2023). All implementations are based on the MMTracking (Contributors, 2020) codebase. For those methods supporting flexible detection backbones, we trial two typical detectors, including the two-stage detector Faster R-CNN (Ren et al., 2015) and the one-stage detector YOLOX (Ge et al., 2021). Each method undergoes training for 10 epochs, adhering to the official configurations, which encompass optimizer settings, batch size, data augmentation techniques, and pre-trained models. Given that the three classical methods lack inherent ReID

Figure 5: **(a) Distribution of the annotated behavior categories. (b) Distribution showcasing individuals alongside their respective social behaviors. Vector graphics; zoom for details.**

modules, we supplement with a dedicated ReID network built on ResNet-50 (He et al., 2016). The training curves of select methods (refer to Fig. A2a) affirm convergence within the training epochs.

We split the video clips in ChimpACT into 80% train, 10% validation, and 10% test. Both the train set and test set cover all the individuals. Models are trained on the training set, with performance metrics reported on the test set. We employ widely-accepted evaluation metrics, drawing from convention in human/object detection, tracking, and ReID (Bewley et al., 2016; Pang et al., 2021; Zhang et al., 2022). Specifically, we utilize (i) mean Average Precision (mAP) Lin et al. (2014) to gauge the detection accuracy, and (ii) the CLEAR metrics (Bermardin and Stiefelhagen, 2008) (MOTA, MOTP, FP, FN, IDs), IDF1 (Ristani et al., 2016), and HOTA (Luiten et al., 2021) to evaluate various facets of the tracking performance. It is worth noting that for FP, FN, and IDs, we report normalized values and denote these metrics as nFP, nFN, and nIDs, respectively.

ResultsTab. 3 summarizes these tracking algorithms' performances on the ChimpACT test set. We conducted three runs for each method and reported the average and variance of these metrics. Notably, the variance across multiple runs is minimal, underscoring the robust reproducibility of our benchmarking. A holistic view of the results reveals that QDTrack (Pang et al., 2021) emerges as the top performer. However, it does suffer from a higher count of identity switches compared to other methods. In terms of detection performance, the YOLOX algorithm (Ge et al., 2021) stands toe-to-toe with Faster R-CNN (Ren et al., 2015). A discernible trend is evident among contemporary tracking methods, which seem to excel in identity association capabilities over their classical counterparts. This is corroborated by marked improvements in tracking metrics like IDF1 and IDs. Such a trend intimates that the latest tracking methods might be adept at maintaining consistent object identities, a pivotal aspect when tracking and analyzing individual trajectories within chimpanzee cohorts.

While the results garnered by the array of tracking algorithms are commendable, they still lag behind the benchmarks set on human-centric datasets (Zhang et al., 2022; Pang et al., 2021; Cao et al., 2023). This disparity can be attributed to challenges like the low contrast and low color variation of the body fur of chimpanzees, compounded by intricate self-occlusions. Nonetheless, this very observation accentuates the significance of ChimpACT. It not only offers a challenging arena for tracking algorithms but also stands as an ideal platform for pioneering and refining tracking methods tailored for chimpanzees and other non-human primates.

### Pose estimation

SettingWe benchmark several state-of-the-art human pose estimation methods on ChimpACT, including CPM (Wei et al., 2016), SimpleBaseline (Xiao et al., 2018), HRNet (Sun et al., 2019), DarkPose (Zhang et al., 2020). Broadly, human pose estimation methods can be bifurcated into two primary paradigms: heatmap-based and regression-based. We harness the MMPose (Contributors, 2020) framework for implementing these methods. Please refer to Appx. C for more implementation details. All the models undergo training for 210 epochs, maintaining the official configurations for optimizers, batch sizes, and learning rates. To gauge any potential model overfitting, we present the validation curve on the AP metric in Fig. A2b, reassuringly suggesting an absence of overfitting.

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline Method & Detector & ReID & HOTA \(\uparrow\) & MOTA \(\uparrow\) & MOTP \(\uparrow\) & IDF1 \(\uparrow\) & mAP \(\uparrow\) & nFP \(\downarrow\) & nFN \(\downarrow\) & nIDs \(\downarrow\) \\ \hline
**SORT** & Faster R-CNN (Bewley et al., 2016) & ResNet-50 & 42.6\(\begin{subarray}{c}1.0\\ 39.8\end{subarray}\) & 43.2\(\begin{subarray}{c}1.3\\ 39.8\end{subarray}\) & 20.3\(\begin{subarray}{c}2.1\\ 32.13\end{subarray}\) & 20.3\(\begin{subarray}{c}3.7\\ 37.1\end{subarray}\) & 71.4\(\begin{subarray}{c}1.6\\ 16.1\end{subarray}\) & 16.1\(\begin{subarray}{c}3.3\\ 3.1\end{subarray}\) & 37.8\(\begin{subarray}{c}1.7\\ 2.8\end{subarray}\) & 2.8\(\begin{subarray}{c}0.5\\ 3.1\end{subarray}\) \\ \hline
**DeepSORT** & Faster R-CNN (Wojke et al., 2017) & VGG & ResNet-50 & 40.2\(\begin{subarray}{c}1.0\\ 40.2\end{subarray}\) & 43.2\(\begin{subarray}{c}1.2\\ 1.2\end{subarray}\) & 20.3\(\begin{subarray}{c}0.3\\ 3.8\end{subarray}\) & 38.4\(\begin{subarray}{c}1.9\\ 3.1\end{subarray}\) & 71.4\(\begin{subarray}{c}1.6\\ 16.1\end{subarray}\) & 16.1\(\begin{subarray}{c}3.3\\ 3.1\end{subarray}\) & 37.8\(\begin{subarray}{c}1.7\\ 2.9\end{subarray}\) \\ \hline
**Trackor** & Faster R-CNN (Bergman et al., 2019) & Faster R-CNN & ResNet-50 & 49.5\(\begin{subarray}{c}1.0\\ 5.7\end{subarray}\) & 50.5\(\begin{subarray}{c}1.1\\ 2.1\end{subarray}\) & 22.6\(\begin{subarray}{c}1.1\\ 55.6\end{subarraysubarray}\) & 55.6\(\begin{subarray}{c}1.2\\ 1.2\end{subarray}\) & 70.7\(\begin{subarray}{c}1.6\\ 13.8\end{subarray}\) & 13.8\(\begin{subarray}{c}0.5\\ 35.2\end{subarray}\) & 35.2\(\begin{subarray}{c}0.7\\ 0.5\end{subarray}\) \\ \hline
**QDTrack** & & & & & & & & & & \\ (Pang et al., 2021) & Faster R-CNN & - & 50.3\(\begin{subarray}{c}2.3\\ 3.2\end{subarray}\) & 54.2\(\begin{subarray}{c}1.4\\ 2.4\end{subarray}\) & 22.2\(\begin{subarray}{c}1.4\\ 2.14\end{subarray}\) & 55.8\(\begin{subarray}{c}1.3\\ 2.8\end{subarray}\) & 77.8\(\begin{subarray}{c}1.2\\ 2.0\end{subarray}\) & 19.7\(\begin{subarray}{c}1.3\\ 2.8\end{subarray}\) & 24.6\(\begin{subarray}{c}0.8\\ 1.4\end{subarray}\) & 1.4\(\begin{subarray}{c}0.2\\ 2.2\end{subarray}\) \\ \hline
**ByteTrack** & Faster R-CNN (Zhang et al., 2022) & Faster R-CNN & - & 43.7\(\begin{subarray}{c}2.0\\ 49.2\end{subarray}\) & 36.9\(\begin{subarray}{c}2.2\\ 2.0\end{subarray}\) & 24.6\(\begin{subarray}{c}0.3\\ 49.2\end{subarray}\) & 48.8\(\begin{subarray}{c}1.3\\ 2.0\end{subarray}\) & 68.2\(\begin{subarray}{c}1.1\\ 2.1\end{subarray}\) & 27.7\(\begin{subarray}{c}1.1\\ 3.1\end{subarraysubarray}\) & 34.2\(\begin{subarray}{c}1.0\\ 2.1\end{subarray}\) & 1.2\(\begin{subarray}{c}0.2\\ 2.0\end{subarray}\) \\ \hline
**OC-SORT** & Faster R-CNN & - & 43.4\(\begin{subarray}{c}1.0\\ 47.9\end{subarray}\) & 38.2\(\begin{subarray}{c}1.9\\ 42.4\end{subarray}\) & 24.3\(\begin{subarray}{c}0.2\\ 1.2\end{subarray}\) & 48.7\(\begin{subarray}{c}2.2\\ 2.8\end{subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray {subarraysubarraysubarraysubarraysubarraysubarray}subarraysubarraysubarraysubarray} {subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray}\\subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray}&subarraysubarraysubarray \\subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray}&subarraysubarraysubarraysubarraysubarraysubarray \\subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray} {arrayarrayarrayarray} \\) & & 25.0\(\begin{subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray}{c} &&&&\\\\ 2.0\end{subarraysubarraysubarraysubarraysubarraysubarraysubarray}& & & & & & \\ (Cao et al., 2023) & YOLOX & - & 47.9\(\begin{subarray}{c}1.4\\ 42.1\end{subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray} {subarraysubarraysubarraysubarraysubarraysubarraysubarray}&subarraysubarraysubarraysubarray \\subarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarraysubarray}{arrayarrayarrayarrayarrayarrayarrayarrayarray *) &&&&&\\ &&&& & & & \\ (Cao et al., 2023) & YOLOX & - & 63.2 & 78.0 & \(\emptyset\) & 77.5 & \(\emptyset\) & 2.7 & 19.0 & 0.3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Results of the detection, tracking, and ReID track on the ChimpACT test set.** The row highlighted in light blue is the performance reference on the human tracking dataset MOT-17 (Milan et al., 2016). \(-\) denotes not applicable. \(\emptyset\) denotes unreported.

The train/test partitioning mirrors that of the first track. We use mAP with various thresholds, adhering to the conventions of human pose estimation (Lin et al., 2014). Additionally, we report the Percentage of Correctly estimated Keypoints (PCK) metric (Andriluka et al., 2014; Ng et al., 2022). PCK@\(\alpha\) quantifies the fraction of accurately predicted keypoints within a distance threshold defined as \(\alpha\times max(height,width)\), derived from the bounding box of the chimpanzee. This metric is widely recognized for its accuracy in body joint localization in both human and animal pose estimation.

ResultsTab. 4 consolidates these pose estimators' performances on the ChimpACT test set. Notably, the heatmap-based DarkPose (Zhang et al., 2020) with an HRNet (Sun et al., 2019) backbone emerges as the top-performing model. This trend aligns with observations in human pose estimation, where heatmap-centric methods (Wei et al., 2016; Xiao et al., 2018; Newell et al., 2016; Sun et al., 2019) predominantly lead the pack, attributed to their robustness against pose and appearance variations. However, the heatmap representation may be less accurate in scenarios where multiple joints are occluded or closely spaced, and it demands better computational and memory resources. Conversely, the newer regression-based methods (Li et al., 2021) are computationally leaner but tend to be more susceptible to overfitting and generally lag in performance.

These results underscore that the task of chimpanzee pose estimation is distinct and nuanced, and cannot be seamlessly addressed by merely repurposing human-centric pose estimation methods. We believe there are two primary reasons for this: (i) chimpanzees exhibit unique joint flexibility and a broader range of motion, and (ii) the visual texture and appearance of chimpanzee fur diverge significantly from human skin. These insights emphasize the need for chimpanzee specific pose estimation strategies.

### Spatiotemporal action detection

SettingWe benchmark four representative human action detection baselines on ChimpACT using the MMAction2 (Contributors, 2020c) codebase, including ARCN (Sun et al., 2018), LFB (Wu et al., 2019), and SlowFast with its variant SlowOnly (Feichtenhofer et al., 2019). All models undergo training for 20 epochs with a batch size of 32. Convergence is evident from the training curves

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline \multicolumn{2}{c}{Method} & Backbone & PCK@0.05 & PCK@0.1 & AP & AP\({}^{50}\) & AP\({}^{75}\) & AP\({}^{M}\) & AP\({}^{L}\) & AR \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & SimpleBaseline & ResNet-50 & 25.3\(\pm\)0.5 & 46.2\(\pm\)0.5 & 8.6\(\pm\)0.4 & 27.4\(\pm\)1.3 & 3.9\(\pm\)0.4 & 0.3\(\pm\)0.1 & 12.5\(\pm\)0.5 & 17.3\(\pm\)0.7 \\  & (Xiao et al., 2018) & ResNet-101 & 26.2\(\pm\)1.0 & 46.4\(\pm\)1.1 & 8.7\(\pm\)0.4 & 27.5\(\pm\)0.6 & 4.2\(\pm\)0.5 & 0.3\(\pm\)0.0 & 12.9\(\pm\)0.2 & 17.7\(\pm\)0.4 \\  & (Xiao et al., 2018) & ResNet-152 & 26.3\(\pm\)0.4 & 47.3\(\pm\)0.8 & 9.3\(\pm\)0.1 & 29.2\(\pm\)1.1 & 4.7\(\pm\)0.3 & 0.5\(\pm\)0.0 & 13.4\(\pm\)0.2 & 16.6\(\pm\)0.0 \\ \cline{2-11}  & & MobileNetV2 & 27.5\(\pm\)1.4 & 48.1\(\pm\)1.7 & 16.7\(\pm\)0.8 & 43.1\(\pm\)2.7 & 11.1\(\pm\)0.8 & 2.0\(\pm\)10.7 & 17.7\(\pm\)0.8 & 19.5\(\pm\)0.9 \\ \cline{2-11}  & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & ResNet-50 & 28.2\(\pm\)1.7 & 47.1\(\pm\)1.3 & 16.3\(\pm\)2.5 & 41.2\(\pm\)9.1 & 41.4\(\pm\)1.4 & 1.3\(\pm\)0.8 & 17.4\(\pm\)2.8 & 20.0\(\pm\)1.6 \\  & (Li et al., 2021) & ResNet-101 & 28.2\(\pm\)3.5 & 46.5\(\pm\)4.3 & 16.2\(\pm\)2.6 & 41.1\(\pm\)5.7 & 10.8\(\pm\)2.4 & 2.1\(\pm\)1.0 & 17.3\(\pm\)2.8 & 20.1\(\pm\)2.1 \\  & & ResNet-152 & 30.0\(\pm\)1.3 & 48.4\(\pm\)2.2 & 18.1\(\pm\)2.8 & 43.0\(\pm\)7.9 & 13.5\(\pm\)0.6 & 1.4\(\pm\)0.3 & 19.2\(\pm\)3.2 & 22.3\(\pm\)1.1 \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & CPM & 40.7\(\pm\)0.2 & 60.4\(\pm\)0.0 & 21.6\(\pm\)0.1 & 51.0\(\pm\)0.4 & 17.1\(\pm\)0.1 & 9.5\(\pm\)0.6 & 22.4\(\pm\)0.1 & 25.4\(\pm\)0.1 \\  & (Wei et al., 2016) & Hourglass-4 & 44.6\(\pm\)0.5 & 60.8\(\pm\)0.1 & 20.6\(\pm\)0.3 & 48.9\(\pm\)0.1 & 16.0\(\pm\)0.4 & 4.6\(\pm\)0.1 & 23.7\(\pm\)0.6 & 28.2\(\pm\)0.2 \\ \cline{2-11}  & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & MobileNetV2 & 39.8\(\pm\)0.4 & 59.4\(\pm\)0.4 & 19.4\(\pm\)0.1 & 48.5\(\pm\)0.6 & 14.3\(\pm\)0.8 & 2.3\(\pm\)0.1 & 20.6\(\pm\)0.1 & 23.2\(\pm\)0.1 \\  & & & & & & & & & \\ \cline{2-11}  & \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & ResNet-50 & 43.3\(\pm\)0.2 & 61.7\(\pm\)1.2 & 22.1\(\pm\)0.2 & 51.5\(\pm\)0.4 & 17.7\(\pm\)0.2 & 3.7\(\pm\)0.4 & 23.4\(\pm\)0.2 & 26.3\(\pm\)0.1 \\  & & ResNet-101 & 42.8\(\pm\)0.3 & 60.7\(\pm\)0.2 & 21.7\(\pm\)0.1 & 52.5\(\pm\)0.4 & 16.7\(\pm\)0.0 & 4.3\(\pm\)0.2 & 23.0\(\pm\)0.1 & 26.2\(\pm\)0.2 \\  & (Xiao et al., 2018) & ResNet-152 & 43.9\(\pm\)0.4 & 61.6\(\pm\)0.1 & 22.7\(\pm\)0.4 & 53.3\(\pm\)0.6 & 18.3\(\pm\)0.4 & 5.3\(\pm\)0.5 & 23.9\(\pm\)0.4 & 27.1\(\pm\)0.1 \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & HRNet & HRNet-W32 & 48.6\(\pm\)0.9 & 65.6\(\pm\)0.6 & 25.9\(\pm\)0.4 & 58.2\(\pm\)0.8 & 22.1\(\pm\)0.4 & 6.1\(\pm\)0.4 & 27.0\(\pm\)0.6 & 30.3\(\pm\)0.5 \\  & (Sun et al., 2019) & HRNet-W48 & 47.3\(\pm\)0.2 & 64.5\(\pm\)0.2 & 25.1\(\pm\)0.1 & 57.2\(\pm\)0.6 & 21.0\(\pm\)0.1 & 6.9\(\pm\)0.9 & 26.2\(\pm\)0.3 & 29.6\(\pm\)0.1 \\ \cline{2-11}  & \multirow{2}{*}{
\begin{tabular}{} \end{tabular} } & ResNet-50 & 43.7\(\pm\)0.0 & 62.1\(\pm\)0.6 & 22.8\(\pm\)0.1 & 53.8\(\pm\)0.8 & 18.8\(\pm\)0.6 & 3.4\(\pm\)0.2 & 24.1\(\pm\)0.0 & 27.1\(\pm\)0.1 \\  & & ResNet-101 & 43.1\(\pm\)0.9 & 61.2\(\pm\)1.4 & 22.1\(\pm\)0.3 & 52.6\(\pm\)0.6 & 17.6\(\pm\)0.4 & 4.0\in Fig. A2c. We maintain consistent optimizers and learning rates as in official implementations. Ground-truth bounding boxes for each chimpanzee are provided during both training and testing, as per Tang et al. (2020). Please refer to Appx. C for further details on ablative modules.

We adopt the same train-test split as previous tracks. Performance is gauged using mAP across 23 action classes, as per standard (Feichtenhofer et al., 2019; Tang et al., 2020). Additionally, we evaluate the mAP within the four behavioral types separately.

ResultsTab. 5 summarizes the action detection algorithms' performances on the ChimpACT test set. The overall mAP aligns with results on human action datasets, underscoring the feasibility of automated action detection for video coding and further analyses. Locomotion behaviors achieve a notably higher mAP, likely due to their solitary nature and distinct patterns. Conversely, Conversely, the "others" category registers the lowest mAP, attributed to its limited data--comprising just 0.14% of action instances across two fine-grained classes. This imbalance suggests the potential benefit of few-shot learning methods in the future. The results highlight both the promise and areas for improvement in the dataset, positioning it as a valuable platform for advancing spatiotemporal action detection algorithms. We anticipate that ChimpACT will further studies into the social dynamics of non-human primates in semi-naturalistic environments.

## 5 Conclusion

In this work, we introduced ChimpACT, a novel longitudinal video dataset capturing the intricate behaviors of group-living chimpanzees, focusing on the juvenile chimpanzee, Azibo. Our meticulous annotations and diverse social interactions within the dataset offer a unique view into the world of our closest evolutionary relatives. Through comprehensive experiments, we underscored the challenges and nuances of applying human-centric computer vision algorithms to the distinct behaviors and interactions of chimpanzees. The dataset's depth, combined with its long-tail distribution, not only emphasizes its significance but also paves the way for interdisciplinary research bridging primatology, comparative psychology, computer vision, and machine learning. By making this resource available, our aspiration is to catalyze advancements in video understanding, inspire the research community to craft specialized techniques for non-human primates and deepen our collective insights into their intricate social fabric and dynamics.

Limitation and future workChimpACT is based on captive chimpanzees living in a semi-natural environment, limiting the observable range of behaviors. Natural foraging, responses to predators, and intergroup encounters are absent. Focusing on Azibo overrepresents certain individuals and underrepresents others, limiting the assessment of the full social network. Nevertheless, we plan to contribute more data and labels to create a larger and more comprehensive chimpanzee dataset.

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline Method & Frame sampling & Module & mAP & mAP\({}_{L}\) & mAP\({}_{O}\) & mAP\({}_{S}\) & mAP\({}_{o}\) \\ \hline ACRN & \(8\times 8\times 1\) & & 24.4\({}_{\pm 0.5}\) & 58.7\({}_{\pm 0.7}\) & 33.8\({}_{\pm 1.7}\) & 14.7\({}_{\pm 0.4}\) & 0.0\({}_{\pm 0.0}\) \\ Sun et al. (2018) & \(4\times 16\times 1\) & & 23.9\({}_{\pm 1.3}\) & 57.8\({}_{\pm 0.4}\) & 35.0\({}_{\pm 0.4}\) & 13.8\({}_{\pm 1.6}\) & 0.0\({}_{\pm 0.0}\) \\ \hline
**LFB** & \(4\times 16\times 1\) & _w_: NL LFB & 22.0\({}_{\pm 0.9}\) & 50.1\({}_{\pm 0.8}\) & 32.3\({}_{\pm 0.9}\) & 13.5\({}_{\pm 1.6}\) & 0.6\({}_{\pm 0.1}\) \\ Wu et al. (2019) & \(4\times 16\times 1\) & _w_: Max LFB & 23.2\({}_{\pm 0.7}\) & 45.0\({}_{\pm 1.5}\) & 31.2\({}_{\pm 0.8}\) & 17.7\({}_{\pm 1.4}\) & 0.5\({}_{\pm 0.0}\) \\  & \(4\times 16\times 1\) & _w_: Avg LFB & 21.3\({}_{\pm 1.6}\) & 45.0\({}_{\pm 3.6}\) & 29.8\({}_{\pm 1.1}\) & 14.7\({}_{\pm 2.6}\) & 0.5\({}_{\pm 0.0}\) \\ \hline  & \(8\times 8\times 1\) & & 20.9\({}_{\pm 1.9}\) & 48.1\({}_{\pm 1.7}\) & 36.2\({}_{\pm 2.8}\) & 11.5\({}_{\pm 1.0}\) & 0.0\({}_{\pm 0.1}\) \\ SlowOnly & \(4\times 16\times 1\) & & 19.2\({}_{\pm 1.1}\) & 47.0\({}_{\pm 2.5}\) & 28.3\({}_{\pm 2.5}\) & 11.0\({}_{\pm 1.2}\) & 0.0\({}_{\pm 0.1}\) \\ Feichtenhofer et al. (2019) & \(8\times 8\times 1\) & _w_: Ctx & 22.3\({}_{\pm 1.9}\) & 52.3\({}_{\pm 3.2}\) & 31.2\({}_{\pm 1.3}\) & 13.8\({}_{\pm 2.4}\) & 0.1\({}_{\pm 0.1}\) \\  & \(4\times 16\times 1\) & _w_: Ctx & 21.4\({}_{\pm 0.9}\) & 47.6\({}_{\pm 2.0}\) & 33.0\({}_{\pm 1.2}\) & 13.2\({}_{\pm 2.2}\) & 0.2\({}_{\pm 0.1}\) \\ \hline  & \(8\times 8\times 1\) & & 21.9\({}_{\pm 1.0}\) & 53.0\({}_{\pm 0.7}\) & 30.6\({}_{\pm 2.2}\) & 12.9\({}_{\pm 1.2}\) & 0.0\({}_{\pm 0.1}\) \\ SlowFast & \(4\times 16\times 1\) & & 22.0\({}_{\pm 0.8}\) & 52.9\({}_{\pm 2.3}\) & 33.1\({}_{\pm 2.3}\) & 12.6\({}_{\pm 0.9}\) & 0.0\({}_{\pm 0.0}\) \\ Feichtenhofer et al. (2019) & \(8\times 8\times 1\) & _w_: Ctx & 24.3\({}_{\pm 0.6}\) & 56.8\({}_{\pm 1.6}\) & 31.5\({}_{\pm 2.0}\) & 15.6\({}_{\pm 0.8}\) & 0.1\({}_{\pm 0.1}\) \\  & \(4\times 16\times 1\) & _w_: Ctx & 24.1\({}_{\pm 0.9}\) & 56.6\({}_{\pm 2.0}\) & 34.7\({}_{\pm 2.7}\) & 14.6\({}_{\pm 0.4}\) & 0.1\({}_{\pm 0.1}\) \\ \hline SlowFast & \(8\times 8\times 1\) & & 25.8 & – & – & – & – \\ Feichtenhofer et al. (2019) & \(8\times 8\times 1\) & & 25.8 & – & – & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Results of spatiotemporal action detection track on ChimpACT test set. The row highlighted in light blue is the performance reference on the human action dataset AVA (Gu et al., 2018). - denotes not applicable. \({}^{*}\)w. NL/Max/Avg LFB” denotes using non-local, max, or average LFB module. \({}^{*}\)w. Ctx” indicates using both the RoI feature and the global pooled feature for classification. \({}^{*}\)mAP, \({}^{*}\)mAP\({}_{L}\), \({}^{*}\)mAP\({}_{O}\), \({}^{*}\)mAP\({}_{S}\), and \({}^{*}\)mAP\({}_{o}\) represent the overall mAP and mAP for \(\underline{L}\)oconotion, \(\underline{Q}\)bject interaction, \(\underline{S}\)ocial interaction, and \(\underline{q}\)thers.**

## Acknowledgement

The authors would like to thank the Wolfgang Kohler Primate Research Center, BasicFinder CO., Ltd., and Keye Zhang for annotations and quality check, Zihao Yin for discussions and preliminary experiments on the chimpanzee detection models, Guangyuan Jiang and Yuyang Li for their technical support on the GPU cluster, and NVIDIA for their generous support of GPUs and hardware. X. Ma, J. Su, W. Zhu, Y. Zhu, and Y. Wang are supported in part by the National Key R&D Program of China (2022ZD0114900), and Y. Zhu is supported in part by the Beijing Nova Program and the National Comprehensive Experimental Base for Governance of Intelligent Society, Wuhan East Lake High-Tech Development Zone.

## References

* Altmann (1974) Altmann, J. (1974). Observational study of behavior: sampling methods. _Behaviour_, 49(3-4):227-266.
* Andriluka et al. (2018) Andriluka, M., Iqbal, U., Insafutdinov, E., Pishchulin, L., Milan, A., Gall, J., and Schiele, B. (2018). Posetrack: A benchmark for human pose estimation and tracking. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Andriluka et al. (2014) Andriluka, M., Pishchulin, L., Gehler, P., and Schiele, B. (2014). 2d human pose estimation: New benchmark and state of the art analysis. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Bain et al. (2021) Bain, M., Nagrani, A., Schofield, D., Berdugo, S., Bessa, J., Owen, J., Hockings, K. J., Matsuzawa, T., Hayashi, M., Biro, D., et al. (2021). Automated audiovisual behavior recognition in wild primates. _Science Advances_, 7(46):eabi4883.
* Bain et al. (2019) Bain, M., Nagrani, A., Schofield, D., and Zisserman, A. (2019). Count, crop and recognise: Fine-grained recognition in the wild. In _Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops_.
* Baker (2022) Baker, T. A. (2022). Wolfgang kohler primate research center. _Encyclopedia of Animal Cognition and Behavior_, page 7310.
* Bala et al. (2020) Bala, P. C., Eisenreich, B. R., Yoo, S. B. M., Hayden, B. Y., Park, H. S., and Zimmermann, J. (2020). Automated markerless pose estimation in freely moving macaques with openmonkeystudio. _Nature Communications_, 11(1):4560.
* Bard et al. (2014) Bard, K. A., Dunbar, S., Maguire-Herring, V., Veira, Y., Hayes, K. G., and McDonald, K. (2014). Gestures and social-emotional communicative development in chimpanzee infants. _American Journal of Primatology_, 76(1):14-29.
* Bergmann et al. (2019) Bergmann, P., Meinhardt, T., and Leal-Taixe, L. (2019). Tracking without bells and whistles. In _International Conference on Computer Vision (ICCV)_.
* Bernardin and Stiefelhagen (2008) Bernardin, K. and Stiefelhagen, R. (2008). Evaluating multiple object tracking performance: the clear mot metrics. _EURASIP Journal on Image and Video Processing_, 2008:1-10.
* Bewley et al. (2016) Bewley, A., Ge, Z., Ott, L., Ramos, F., and Upcroft, B. (2016). Simple online and realtime tracking. In _IEEE International Conference on Image Processing (ICIP)_.
* Boesch (1996) Boesch, C. (1996). The emergence of cultures among wild chimpanzees. In _Proceedings-British Academy_.
* Bohnslav et al. (2021) Bohnslav, J. P., Wimalasena, N. K., Clausius, K. J., Dai, Y. Y., Yarmolinsky, D. A., Cruz, T., Kashlan, A. D., Chiappe, M. E., Orefice, L. L., Woolf, C. J., et al. (2021). Deepethogram, a machine learning pipeline for supervised behavior classification from raw pixels. _Elife_, 10:e63377.
* Cao et al. (2023) Cao, J., Pang, J., Weng, X., Khirodkar, R., and Kitani, K. (2023). Observation-centric sort: Rethinking sort for robust multi-object tracking. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Contributors (2020a) Contributors, M. (2020a). MMTracking: OpenMMLab video perception toolbox and benchmark. https://github.com/open-mmlab/mmtracking.
* Contributors (2020b) Contributors, M. (2020b). Openmmlab pose estimation toolbox and benchmark. https://github.com/open-mmlab/mmpose.
* Contributors (2020c) Contributors, M. (2020c). Openmmlab's next generation video understanding toolbox and benchmark. https://github.com/open-mmlab/mmaction2.
* Contributors (2020d)Dawkins, M. S. (2003). Behaviour as a tool in the assessment of animal welfare. _Zoology_, 106(4):383-387.
* Desai et al. (2022) Desai, N., Bala, P., Richardson, R., Raper, J., Zimmermann, J., and Hayden, B. (2022). Openapepose: a database of annotated ape photographs for pose estimation. _arXiv preprint arXiv:2212.00741_.
* Fabian Caba Heilbron et al. (2015) Fabian Caba Heilbron, Victor Escorcia, B. G. and Niebles, J. C. (2015). Activitynet: A large-scale video benchmark for human activity understanding. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Feichtenhofer et al. (2019) Feichtenhofer, C., Fan, H., Malik, J., and He, K. (2019). Slowfast networks for video recognition. In _International Conference on Computer Vision (ICCV)_.
* Frohlich et al. (2020) Frohlich, M., Muller, G., Zeitrag, C., Wittig, R. M., and Pika, S. (2020). Begging and social tolerance: Food solicitation tactics in young chimpanzees (pan troglodytes) in the wild. _Evolution and Human Behavior_, 41(2):126-135.
* Ge et al. (2021) Ge, Z., Liu, S., Wang, F., Li, Z., and Sun, J. (2021). Yolov: Exceeding yolo series in 2021. _arXiv preprint arXiv:2107.08430_.
* Gebru et al. (2021) Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D., and Crawford, K. (2021). Datasheets for datasets. _Communications of the ACM_, 64(12):86-92.
* Gonyou (1994) Gonyou, H. W. (1994). Why the study of animal behavior is associated with the animal welfare issue. _Journal of Animal Science_, 72(8):2171-2177.
* Gu et al. (2018) Gu, C., Sun, C., Ross, D. A., Vondrick, C., Pantofaru, C., Li, Y., Vijayanarasimhan, S., Toderici, G., Ricco, S., Sukthankar, R., et al. (2018). Ava: A video dataset of spatio-temporally localized atomic visual actions. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* He et al. (2017) He, K., Gkioxari, G., Dollar, P., and Girshick, R. (2017). Mask r-cnn. In _International Conference on Computer Vision (ICCV)_.
* He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In _Conference on Computer Vision and Pattern Recognition (CVPR)_.
* Hobaiter et al. (2017) Hobaiter, C., Samuni, L., Mullins, C., Akankwasa, W. J., and Zuberbuhler, K. (2017). Variation in hunting behaviour in neighbouring chimpanzee communities in the budong forest, uganda. _PloS One_, 12(6):e0178065.
* Kay et al. (2017) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., et al. (2017). The kinetics human action video dataset. _arXiv preprint arXiv:1705.06950_.
* Labuguen et al. (2021) Labuguen, R., Matsumoto, J., Negrete, S. B., Nishimaru, H., Nishijo, H., Takada, M., Go, Y., Inoue, K.-i., and Shibata, T. (2021). Macaquepose: a novel "in the wild" macaque monkey pose dataset for markerless motion capture. _Frontiers in Behavioral Neuroscience_, 14:581154.
* Langergraber et al. (2012) Langergraber, K. E., Prufer, K., Rowney, C., Boesch, C., Crockford, C., Fawcett, K., Inoue, E., Inoue-Munuyama, M., Mitani, J. C., Muller, M. N., et al. (2012). Generation times in wild chimpanzees and gorillas suggest earlier divergence times in great ape and human evolution. _Proceedings of the National Academy of Sciences (PNAS)_, 109(39):15716-15721.
* Lauer et al. (2022) Lauer, J., Zhou, M., Ye, S., Menegas, W., Schneider, S., Nath, T., Rahman, M. M., Di Santo, V., Soberanes, D., Feng, G., et al. (2022). Multi-animal pose estimation, identification and tracking with deeplabcut. _Nature Methods_, 19(4):496-504.
* Li et al. (2021) Li, J., Bian, S., Zeng, A., Wang, C., Pang, B., Liu, W., and Lu, C. (2021). Human pose regression with residual log-likelihood estimation. In _International Conference on Computer Vision (ICCV)_.
* Lin et al. (2014) Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. (2014). Microsoft coco: Common objects in context. In _European Conference on Computer Vision (ECCV)_.
* Luiten et al. (2021) Luiten, J., Osep, A., Dendorfer, P., Torr, P., Geiger, A., Leal-Taixe, L., and Leibe, B. (2021). Hota: A higher order metric for evaluating multi-object tracking. _International Journal of Computer Vision (IJCV)_, 129:548-578.
* Luncz et al. (2018) Luncz, L. V., Sirianni, G., Mundry, R., and Boesch, C. (2018). Costly culture: differences in nut-cracking efficiency between wild chimpanzee groups. _Animal Behaviour_, 137:63-73.
* Luncz et al. (2018)

[MISSING_PAGE_FAIL:13]

[MISSING_PAGE_EMPTY:14]

Additional details on ChimpACT

### Ethogram

We detail the ethogram definition in Tab. A1, which systematically describes the daily behaviors of chimpanzees.

\begin{table}
\begin{tabular}{p{42.7pt} p{113.8pt} p{113.8pt} p{113.8pt}} \hline \hline
**category** & **definition** & **subcategory** & **subcategory definition** \\ \hline \multirow{4}{*}{locomotion} & & 0.moving & moving horizontally, _e.g._, walking, running \\  & patterns of self-initiated movement of an individual & 1. climbing & moving vertically, _e.g._, climbing up or down a structure \\ \cline{3-4}  & & 2. resting & remaining stationary, _e.g._, standing, sitting, or lying \\ \cline{3-4}  & & 3. sleeping & resting and keeping eyes closed \\ \hline \multirow{4}{*}{object interaction} & direct physical interactions with maintenance stationary or movable objects by hands, feet or mouth & 4. solitary object playing & non-social and non-goal-directed object interaction and exploration \\ \cline{3-4}  & direct physical interactions with maintenance stationary or movable action & & \\ \cline{3-4}  & 5. eating & consuming and processing food \\ \cline{3-4}  & 6. manipulating object & manipulation of any kind of inamimate object excluding eating \\ \hline \multirow{4}{*}{object interaction} & & 7. grooming & a chimpanzee, the groomer, is cleaning the fur, head, hand, feet, or genials of another chimpanzee, usually using their hands and/or mouth \\ \cline{3-4}  & & 8. being groomed & one chimpanzee, the groome, is getting their skin or fur cleaned by another chimpanzee \\ \cline{3-4}  & 9. aggressing & a chimpanzee is showing agonistic behavior towards another chimpanzee. This can range from charging and chasing another chimpanzee to direct physical contact such as slapping, hitting, and biting \\ \cline{3-4}  & 10. embracing & a chimpanzee is embracing another chimpanzee with their arms, not to be confused with carrying \\ \cline{3-4}  & 11. begging & a chimpanzee is requesting food or another object from another chimpanzee, often by entering, or using an open palm begging gesture \\ \cline{3-4}  & 12. being begged from & a chimpanzee is requested food or another object by another chimpanzee \\ \cline{3-4}  & 13. taking object & taking an object from the possession of another chimpanzee, the transfer might be resisted or not \\ \cline{3-4}  & 14. losing object & the possession is taken by another chimpanzee \\ \cline{3-4}  & 15. carrying & a chimpanzee (usually an adult) carries another chimpanzee (usually an infant or juvenile) on the back, front, side, arm, or leg for more than 2 steps \\ \cline{3-4}  & 16. being carried & a chimpanzee (usually an infant or juvenile) is carried by another chimpanzee (usually an adult) on the back, front, side, arm, or leg for more than 2 steps. \\ \cline{3-4}  & 17. nursing & a female chimpanzee is nursing (breastfed, _i.e._, making physical contact with the apple) an infant/juvenile \\ \cline{3-4}  & 18. being nursed & an infant/juvenile is being nursed (breastfed, _i.e._, making physical contact with the apple) by a female chimpanzee \\ \cline{3-4}  & 19. playing & a chimpanzee is physically interacting with another individual in a friendly, tesing, or mock fighting way (_e.g._, play fighting and other behaviors) \\ \cline{3-4}  & 20. touching & a chimpanzee makes body contact with another chimpanzee (_e.g._, holding hands) and it does not fit with any of the other social interaction categories described above \\ \hline \multirow{4}{*}{others} & & 21. erection & a male chimpanzee has an erect penis \\ \cline{3-4}  & other behaviors & 22. displaying & a male chimpanzee, usually with puffed up hair (piloreception) and an erection, performs a dominance display, which includes walking with a swagger, swinging their arms to the sides, and making calls with increasing amplitude, commonly ending by stomping against or slapping objects. Displays can be directed at another chimpanzee or be undirected \\ \hline \hline \end{tabular}
\end{table}
Table A1: **The ethogram used for the ChimpACT dataset.**Figure A1: **Example frames from the ChimpACT dataset. ChimpACT possesses rich social interactions of the complex everyday life of group-living chimpanzees and contains several environmental enrichment.**

### Dataset details

Collection and organization405 hours of video footage of the Leipzig A-group chimpanzees were collected between 2015 and 2018. To create a representative sample of the footage, 163 video clips were selected, with 15, 35, 86, and 27 clips taken from each year. These video clips cover the four seasons. Each clip is 1000 frames long, with only 3 clips being shorter than 1000 frames. Visual examples from six clips, featuring both indoor and outdoor enclosures, are shown in Fig. A1. The dataset covers a diverse range of physical scenarios, camera views, and social behaviors, as demonstrated in these examples. For instance, in the third row of the figure, an adult chimpanzee is shown grooming an infant chimpanzee in her arms, while later on, the same infant is nursed.

Annotation process and qualityThe annotation process was conducted using BasicFinder CO., Ltd.'s private labeling platform, which involved a team of 15 annotators and 2 managers. Prior to commencing the annotation work, our team developed comprehensive guidelines that explicitly outlined the requirements for labeling. These guidelines covered several aspects, including:

(i) Assigning a bounding box for each chimpanzee in the image. (ii) Specifying the visibility of the bounding boxes. (iii) Assigning tracking IDs to each bounding box for tracking purposes. (iv) Localizing 2D keypoints within each bounding box. (v) Indicating the visibility of each 2D keypoint. (vi) Assigning behavior labels for each bounding box.

To ensure that the annotators followed these guidelines accurately, the project managers provided training based on the guidelines. Following the training, the annotators performed a trial annotation on a small dataset. We actively sought feedback from the annotators during this phase, which allowed us to address any issues and make necessary improvements. We conducted a thorough review of the trial annotations to verify that the quality met our standards.

During the trial labeling phase, we reached out to three labeling companies and ultimately selected BasicFinder CO., Ltd. based on their exceptional labeling quality. It is worth noting that BasicFinder CO., Ltd. has previously led the annotation efforts for the BDD100K (Yu et al., 2020) dataset, which is a substantial dataset used for autonomous driving purposes. This experience demonstrates their ability to maintain high annotation standards for complex and extensive datasets. Consequently, their involvement improves the reliability of our ChimpACT dataset annotations as well.

Once we were confident in the quality of the trial annotations, we proceeded with the large-scale annotation process. To manage the annotations efficiently, each video clip was designated as an annotation task, and our managers assigned these tasks to individual annotators using BasicFinder CO., Ltd.'s platform, ensuring that there was no overlap in assignments. BasicFinder CO., Ltd. has implemented rigorous quality management practices throughout the annotation process. These practices include a customized workflow, complete job traceability, precise performance tracking, multiple levels of auditing, and scientific personnel management. By adhering to these practices, we were able to maintain high standards of quality and accuracy while ensuring efficient processing speed. The annotation process followed a sequential workflow of execution, review, and quality control. Experienced annotators were responsible for executing the annotations, while the manager, as well as our team, conducted thorough reviews and quality control checks. Any annotations that did not meet the required standards were sent back to the annotators for corrections. The quality control phase involved a comprehensive review and verification of all data by both the managers and our own team, ensuring the integrity and accuracy of the annotations. Once all the data had been confirmed to meet our standards of quality, we concluded the annotation process.

More specifically, to label chimpanzee identities, annotators only needed to assign a tracking ID to each chimpanzee, which was then reviewed by the primatologist in our team, who assigned the ages' names based on his knowledge of the observed Leipzig A-group chimpanzees. The process of localizing 2D keypoints within each bounding box and assigning behavior labels for each chimpanzee presented bigger challenges than other tasks. To overcome these challenges, we implemented several measures to ensure accuracy and consistency. For the labeling of 2D keypoints, we provided detailed instructions accompanied by visual illustrations, aiming to provide clear guidelines for annotators to precisely identify and mark the keypoints. For labeling of behaviors, we supplied example videos showcasing different chimpanzee behaviors, created by our team's experienced primatologists. These videos served as valuable references, enabling annotators to accurately assign behavior labels based on observed actions. Throughout the annotation process, the primatologists actively participated, offering their expertise and providing valuable feedback to ensure the annotations aligned withscientific standards. Finally, the behavioral primatologists in our team manually reviewed all labeled frames to ensure data reliability. These measures and the involvement of the primatologists were instrumental in enhancing the overall quality and reliability of the annotations.

For more information on the dataset, including pre-processing scripts, and visualized annotations, please refer to our project website.

## Appendix B Discussion on ChimpACT

Intended usesThe ChimpACT dataset is a versatile resource that can be used for studying algorithms for chimpanzee detection, tracking, identification, pose estimation, and spatiotemporal action detection. Therefore, the dataset is both relevant for questions in computer vision and primate behavior. In the context of computer vision, it lends itself to other research topics, including but not limited to pose tracking, few-shot learning, weakly-supervised learning, and transfer learning. Considering primate behavior, the dataset shares numerous features with other video data commonly collected with captive and wild chimpanzee populations. This makes it an ideal resource for fine-grained investigations of social (_e.g._, grooming, nursing, aggression) and nonsocial (_e.g._, locomotion, object interactions) chimpanzee behaviors. We strongly encourage researchers to utilize our dataset solely for research purposes that promote animal welfare and conservation. We firmly discourage any use of the dataset for harmful activities such as poaching, hunting or any other exploitation of primates. It is crucial for researchers to approach the data with a focus on positive societal impacts and to refrain from any potential negative consequences.

EthicsThe ChimpACT dataset raises no ethical concerns regarding the privacy information of human subjects, as it solely focuses on chimpanzees. Studying the social behavior of chimpanzees provides an ethical and efficient means to explore aspects of human sociality due to our phylogenetic proximity. By analyzing their behaviors, we can gain insights into the evolution of human social behavior and potentially contribute to both the scientific and ethical understanding of the human condition. The ethics committee of the Wolfgang Kohler Primate Research Center approved the observational data collection for this project.

Maintenance, distribution, and licenseThe ChimpACT dataset will be maintained by the authors and made publicly available with a total of 160,500 frames (around 2 hours) on our project website. The ChimpACT dataset will be distributed under the CC BY-NC 4.0 license.

Wage paid to annotatorsWe collaborated with BasicFinder CO., Ltd. for the annotation process. The labeling was carried out by 15 annotators, and they were offered a fair wage as per the prearranged contract. The total expenditure for the labeling process was approximately 70,000 RMB.

## Appendix C Experiments

We trained all the models with officially-used training configurations for each of the three tracks. Please refer to the code implementation on our Github for details. Although we trained the models for different epochs in experiments conducted on different tracks, these choices were made based on conventional practices. Based on the training loss curves provided in Figs. 1(a) and 1(c), it can be observed that all tracking and spatiotemporal action detection methods have reached convergence within the chosen training epochs. To assess the potential overfitting of the pose estimation models, we have included the validation curve on the AP metric in Fig. 1(b). The validation curve demonstrates the performance of the pose models on the validation set, which indicates that the pose estimation models are not exhibiting signs of overfitting. Therefore, based on the training loss curves and the validation curve, it can be concluded that the chosen training epochs are appropriate for both tracking and pose estimation methods.

### Detection, tracking, and ReID

We partitioned the dataset of 163 videos into three sets: 127 videos for training, 17 for validation, and 19 for testing. Of note, all individual chimpanzees are present in both the training and testing sets. In the test set, there are 12 and 7 videos for indoor and outdoor scenes, respectively.

For the evaluation metrics, MOTA (Multiple Object Tracking Accuracy) takes into account FP (False Positives), FN (False Negatives), and IDs (IDentity switches). Usually, FP and FN are larger than IDs; therefore, MOTA mainly assesses the detection performance. IDF1 evaluates the ability to preserve subject identities to assess identification association performance. HOTA (Higher Order Tracking Accuracy) is a recently proposed metric that considers accurate detection, association, and localization equally important, and balances their effects explicitly.

ResultsWe additionally evaluated the performance on the **indoor** and **outdoor** test set in Tabs. A2 and A3, respectively. Notably, the results indicate that these approaches achieve consistently better performance on the indoor test set compared to the outdoor test set. This may be attributed to the greater complexity of outdoor scenarios and the presence of varying camera views, which can significantly increase the difficulty of detecting and tracking chimpanzees. Furthermore, the presence of occlusions, similar appearances, and other environmental factors can further exacerbate the challenges of chimpanzee tracking in outdoor settings.

We visualize the tracking results in Figs. A3 and A4, with the ground-truth bounding boxes and chimpanzee identities shown in the last row. We visualized the confidence scores of the estimated bounding boxes and their associated IDs in each frame obtained by the evaluated methods. It is worth noting that we do not require individual identification of each chimpanzee, but rather assign the same ID to the same animal across frames, following the common practice in multi-human tracking (Milanet al., 2016). The estimated box ID is therefore used solely for evaluating the tracking performance. We observed that the evaluated methods performed well in scenarios with minimal occlusion, but struggled to detect and associate the same individual chimpanzee when heavy occlusion occurred. For instance, in Fig. A3, the infant chimpanzee's bounding box is lost in some frames, and its identity is erroneously switched later due to heavy occlusion. This is a challenging task in chimpanzee detection and tracking, as occlusions frequently occur in group-living habitats. Please refer to the supplementary video for more experimental results. In conclusion, the experimental results reveal the limitations of existing methods for chimpanzee detection and tracking, underscoring the need for more robust algorithms to be developed. We believe that our dataset can make a valuable contribution to the advancement of this field, by providing a challenging benchmark for evaluating and comparing different methods.

### Pose estimation

We followed the partition of the dataset as the first track to train and evaluate the methods.

ResultsWe report the PCK@0.1 for the 16 keypoints in Tab. A4. The results reveal that the keypoints on the face, such as the eyes and lips, exhibited better estimation compared to the arms and legs. This could be attributed to the fact that eyes and lips have more distinctive visual patterns than limbs, which are often surrounded by heavy fur. Tab. A5 further reports the PCK@0.1 for each action category on the test set. We observe that different action types exhibit variations in pose accuracy, for example, with climbing generally achieving slightly higher accuracy compared to resting in most methods. This observation can be attributed to the higher potential for self-occlusion during resting, as chimpanzees tend to exhibit significant self-occlusion due to their flexible joints. This is evident in the visualized examples in Fig. A5, where (a) and (c) depict resting poses with pronounced self-occlusion. In contrast, during climbing, the body is mostly in an extended state, as shown in (b) and (d). Consequently, the PCK tends to be slightly higher for climbing compared to resting as shown in Tab. A6. To validate this assumption, we further evaluate the performance of all the methods for non-occluded poses in Tab. A7. It is interesting to note that all the methods achieve high PCK accuracy when all the keypoints are visible. This demonstrates their effectiveness in accurately estimating poses when occlusions are minimal or absent.

These observations highlight the unique and intricate nature of chimpanzee pose estimation, which is complicated by their flexible joint articulations and extended range of motion, as well as the dissimilar physical appearances of their fur in comparison to that of humans. Consequently, developing accurate pose estimation algorithms for chimpanzees requires careful consideration and specialized techniques that account for their unique characteristics.

Figs. A6 and A7 present the qualitative results of several models on the ChimpACT test split, with the ground-truth poses displayed in the last row. It is promising to observe that directly transferring human pose estimation algorithms to chimpanzees yielded decent performance. However, due to self-occlusions and different physical appearance and joint articulations, these models are susceptible to errors in estimating the positions of limbs, as seen in the misaligned right arm and leg of the young chimpanzee in the first column of Fig. A6 and the third column of Fig. A7.

### Spatiotemporal action detection

We adopted the same dataset partition as the first track. The frame sampling strategy was defined as \(T\times I\times N\). We ablated two strategies that continuously sample one frame every \(I\) frames and finally get an input clip with \(T\) frames by setting \(T\neq 1\). \(N\) denotes the number of clips which is used only when \(T=1\). For the four representative methods, we ablated different modules. For LFB (Wu et al., 2019), we ablated different ways of the feature bank operator instantiations, by using non-local (NL) blocks (Wang et al., 2018) or average (Avg) or max (Max) pooling. For SlowFast (Feichtenhofer et al., 2019) and the variant SlowOnly, we ablated the context module (Ctx), which indicates that using both the RoI feature and the global pooled feature for the action classification.

ResultsWe report the mAP for each model's best configuration on several subcategory behaviors in Tab. A8. The models exhibit better performance in detecting locomotion and solitary object interactions, possibly because these actions are relatively simple and involve less interaction between individuals, making it easier for the model to distinguish between action patterns. However, there is still considerable room for improvement in existing models for action categories with higher levels of interaction, such as social interactions.

We provide qualitative results in Figs. A8 and A9. All methods recognized the playing action of the two chimpanzees in Fig. A8, but incorrectly classified the touching actions as grooming in Fig. A9. These two action patterns exhibit subtle differences that significantly challenge the modelsto distinguish them accurately. We recommend referring to the supplementary video for the video results to observe the difference. The challenges of such distinctions highlight the need for stronger algorithms to address these issues effectively.

Figure A6: **Qualitative results of representative methods on the ChimpACT test set on the pose estimation task. The ground-truth poses are shown in the last row.**

Overall, we hope that our work will inspire further research and development in the area of chimpanzee behavior recognition, with the ultimate goal of improving our understanding of chimpanzee and primate behaviors and ecology.

Figure A7: **More qualitative results of representative methods on the ChimpACT test set on the pose estimation task.** The ground-truth poses are shown in the last row.

## Appendix AFigure A9: **More qualitative results of representative methods on the ChimpACT test set on the spatiotemporal action detection task.** The ground-truth actions are shown in the last row.

Data documentation

We follow the datasheet proposed in Gebru et al. (2021) for documenting our ChimpACT and associated benchmarks:

1. **Motivation** 1. **For what purpose was the dataset created?** This dataset was created to facilitate the study of chimpanzee behaviors, and ultimately advance our understanding of communication and sociality in non-human primates. 2. **Who created the dataset and on behalf of which entity?** This dataset was created by Xiaoxuan Ma, Stephan P. Kaufhold, Jiajun Su, Wentao Zhu, Jack Terwilliger, Andres Meza, Yixin Zhu, Federico Rossano, and Yizhou Wang. Xiaoxuan Ma, Jiajun Su, Wentao Zhu, Yixin Zhu, and Yizhou Wang are with Peking University. Stephan P. Kaufhold, Jack Terwilliger, Andres Meza, and Federico Rossano are with the University of California, San Diego. 3. **Who funded the creation of the dataset?** The creation of this dataset was funded by Peking University and the University of California, San Diego. 4. **Any other Comments?** None.
2. **Composition** 1. **What do the instances that comprise the dataset represent?** For video data, each instance is a video clip regularized from the raw video. Each instance contains video footage focusing on a group of chimpanzees collected in Leipzig Zoo, Germany. For benchmarking, each instance has rich annotations of chimpanzee identities, poses, and actions. See Sec. 3 and Appx. A. 2. **How many instances are there in total?** We have 163 video instances in total. 3. **Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?** No, this is a brand-new dataset. 4. **What data does each instance consist of?** See Sec. 3 and Appx. A. 5. **Is there a label or target associated with each instance?** Yes. See Sec. 3 and Appx. A. 6. **Is any information missing from individual instances?** No. 7. **Are relationships between individual instances made explicit?** Yes. 8. **Are there recommended data splits?** Yes, we have separated the whole dataset into training, validation, and test set. See Sec. 4.1, Appx. C and the project website for details. 1. **Are there any errors, sources of noise, or redundancies in the dataset?** There are almost certainly some errors in video annotations. We did our best to minimize these, but some certainly remain. 1. **Is the dataset self-contained, or does it link to or otherwise rely on external resources (_e.g.,_ websites, tweets, other datasets)?** The dataset is self-contained. 2. **Does the dataset contain data that might be considered confidential (_e.g._, data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?** No. 3. **Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?** No.

* **Does the dataset relate to people?** No.
* **Does the dataset identify any subpopulations (_e.g._, by age, gender)?** No.
* **Is it possible to identify individuals (_i.e._, one or more natural persons), either directly or indirectly (_i.e._, in combination with other data) from the dataset?** Not applicable. Our dataset only contains chimpanzees.
* **Does the dataset contain data that might be considered sensitive in any way (_e.g._, data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?** No.
* **Any other comments?** None.
* **Collection Process*
* **How was the data associated with each instance acquired?*
* See Sec.*
	* 3.2 **and*
* Appx. A for details.*
* **What mechanisms or procedures were used to collect the data (_e.g._, hardware apparatus or sensor, manual human curation, software program, software API)?*
* We used JVC Everio cameras to collect video footage (Codec H.264). See Sec.*
	* 3.2 **for details.*
* **If the dataset is a sample from a larger set, what was the sampling strategy (_e.g._, deterministic, probabilistic with specific sampling probabilities)?*
* See Sec.*
	* 3.3 **and*
* Appx. A for details.*
* **Who was involved in the data collection process (_e.g._, students, crowdworkers, contractors) and how were they compensated (_e.g._, how much were crowdworkers paid)?*
* The video data was collected by the authors. The annotations were performed by the workers in BasicFinder CO., Ltd., and the workers were offered a fair wage as per the prearranged contract. See Sec.*
* 3 **and*
* Appx. B **for details.*
* **Over what timeframe was the data collected?*
* The data were collected from 2015 to 2018, and labeled in 2022.*
* **Were any ethical review processes conducted (_e.g._, by an institutional review board)?*
* Not applicable. The ChimpACT dataset raises no ethical concerns regarding the privacy information of human subjects, as it solely focuses on chimpanzees.
* **Does the dataset relate to people?*
* **Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (_e.g._, websites)?** Not applicable.
* **Were the individuals in question notified about the data collection?** Not applicable.
* **Did the individuals in question consent to the collection and use of their data?** Not applicable.
* **If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?** Not applicable.
* **Has an analysis of the potential impact of the dataset and its use on data subjects (_e.g._, a data protection impact analysis) been conducted?** Yes, see** Appx. B**.
* **Any other comments?** None.
* **Preprocessing, Cleaning and Labeling**1. **Was any preprocessing/cleaning/labeling of the data done (_e.g.,_ discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?**
2. **Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (_e.g.,_ to support unanticipated future uses)?** Yes, we provide the raw data on our project website.
3. **Is the software used to preprocess/clean/label the instances available?** No. The annotation software is the private labeling platform provided by BasicFinder CO., Ltd. However, existing open-source annotation software such as DeepLabCut (Mathis et al., 2018) could also be used to preprocess/clean/label the instances. 4. **Any other comments?** None.
5. **Uses** 1. **Has the dataset been used for any tasks already?** No, the dataset is newly proposed by us. 2. **Is there a repository that links to any or all papers or systems that use the dataset?** Yes, we provide the link to all related information on our project website. 3. **What (other) tasks could the dataset be used for?** This dataset could be used for other research topics, including but not limited to pose tracking, few-shot learning, and transfer learning. 4. **Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?** We propose to annotate the keyframe every 10 frames for the pose track and action detection track. For tracking track, we label all the frames. 5. **Are there tasks for which the dataset should not be used?** The usage of this dataset should be limited to the scope of understanding chimpanzee/non-human primate behaviors. 6. **Any other comments?** None.
6. **Distribution** 1. **Will the dataset be distributed to third parties outside of the entity (_e.g.,_ company, institution, organization) on behalf of which the dataset was created?** Yes, the dataset will be made publicly available. 2. **How will the dataset be distributed (_e.g.,_ tarball on website, API, GitHub)?** The dataset could be accessed on our project website. 3. **When will the dataset be distributed?** The dataset will be released by the end of 2023 on our project website. 4. **Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?** We release our benchmark under CC BY-NC 4.01 license. 5. **Have any third parties imposed IP-based or other restrictions on the data associated with the instances?** No. 6. **Do any export controls or other regulatory restrictions apply to the dataset or to individual instances?** No. 7. **Any other comments?** None.
7. **Maintenance** 1. **Who is supporting/hosting/maintaining the dataset?** Xiaoxuan Ma is maintaining.

2. **How can the owner/curator/manager of the dataset be contacted (_e.g._, email address)?** maxiaoxuan@pku.edu.cn
3. **Is there an erratum?** Currently, no. As errors are encountered, future versions of the dataset may be released and updated on our website.
4. **Will the dataset be updated (_e.g._, to correct labeling errors, add new instances, delete instances')?** Yes, if applicable.
5. **If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (_e.g._, were individuals in question told that their data would be retained for a fixed period of time and then deleted)?** Not applicable. The dataset does not relate to people.
6. **Will older versions of the dataset continue to be supported/hosted/maintained?** Yes, older versions of the benchmark will be maintained on our website.
7. **If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?** Yes, please get in touch with us by email.
8. **Any other comments?** None.