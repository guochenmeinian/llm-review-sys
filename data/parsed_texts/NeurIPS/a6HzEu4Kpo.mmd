# Tri-Level Navigator: LLM-Empowered Tri-Level Learning for Time Series OOD Generalization

 Chengtao Jian

Tongji University, Shanghai, China

jct@tongji.edu.cn

&Kai Yang

Tongji University, Shanghai, China

kaiyang@tongji.edu.cn

Corresponding author.

Yang Jiao

Tongji University, Shanghai, China

yangjiao@tongji.edu.cn

Corresponding author.

###### Abstract

Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel **T**ri-level learning framework for **T**ime **S**eries OOD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an \(\epsilon\)-stationary point is bounded by \(\mathcal{O}(\frac{1}{\epsilon^{2}})\). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.

## 1 Introduction

In machine learning, a common challenge arises when the distributions of training and test sets differ significantly [20]. This mismatch demands that models, trained on specific distribution data, should generalize well on unseen distribution data, known as OOD generalization [21, 22]. Despite a vast amount of research on the OOD generalization [21, 20, 24], the field of OOD generalization in time series is relatively limited and presents more significant challenges. This is primarily due to the inherent temporal dependencies and dynamic changes characteristic of time series data [1]. Therefore, a critical aspect of improving time series OOD generalization is to learn robust representations that remain stable despite shifts in distributions.

Recently, the field of machine learning has witnessed remarkable advancements in pre-trained foundation models, with notable examples including Large Language Models (LLMs) such as GPT [17], LLaMA [23] and CLIP [17]. These models have been instrumental in capturing and leveraging complex patterns across various domains. In addition, using foundation models, especially LLMs, in processing non-linguistic data, e.g., time series is increasingly drawing attention. By fine-tuning only a few handful of parameters, thesemodels show remarkable versatility in diverse data formats ranging from audio (Ghosal et al., 2023), image (Lu et al., 2021) and time series (Chang et al., 2023; Jin et al., 2023). Studies indicate that LLMs, as part of the broader foundation model spectrum, demonstrate sophisticated reasoning and strong pattern recognition capabilities (Wang et al., 2023; Chu et al., 2023), fundamentally acting as pattern machines (Mirchandani et al., 2023). Moreover, LLMs have been shown to be effective in transfer learning across various modalities, due to their data-independent self-attention mechanism (Zhou et al., 2023).

Additionally, recent advancements in vision-language foundation models have shown promising developments in OOD generalization (Zheng et al., 2022), yet the exploration in time series remains underdeveloped. The potential of using foundational models is highlighted by the study in Liu et al. (2023), Hendrycks et al. (2020), which suggests that pre-trained transformers can improve OOD robustness. _Despite existing efforts, the limited exploration of foundational model applications in time series OOD generalization suggests an emerging field._

In this paper, we propose a **T**ri-level learning framework for **T**ime **S**eries **O**OD generalization, named TTSO. Unlike conventional OOD generalization methods that focus solely on group-level (Jiao et al., 2022; Huang et al., 2020) or sample-level uncertainties (Zhang et al., 2017; Zhou et al., 2021; Han et al., 2024), our framework uniquely addresses both by combing a minimization problem for optimal model parameter learning, a maximization problem for dynamically data re-grouping, and another maximization problem for data augmentation under a tri-level framework. To tackle this tri-level problem, we propose a stratified localization algorithm via cutting planes. Leveraging the advanced representation learning capabilities of LLMs, we adapt this tri-level learning framework for fine-tuning LLMs.

Our contributions can be summarized as follows:

* **Tri-level Learning Framework.** In contrast to most existing works in OOD generalization, which primarily focus on either group-level or sample-level uncertainties, TTSO uniquely integrates both aspects under a tri-level learning framework. Specially, this comprehensive framework emphasizes the interdependent relationship between problems of each level, advancing beyond the typical single or bi-level methodologies in OOD generalization. Moreover, a theoretical framework based on Vapnik-Chervonenkis dimension has been developed to rigorously analyze and elucidate the generalization properties of TTSO. We then leverage this tri-level framework to fine-tune LLMs, achieving an maximum 4.9% improvement in performance on time series classification in OOD scenarios.
* **Stratified Localization Algorithm.** To tackle the aforementioned tri-level optimization problem, we develop a stratified localization method using cutting planes. Unlike traditional gradient-based methods, TTSO removes the necessity of computing the hypergradient for the outer optimization problem. This computation is typically very challenging and computationally intensive due to the nested structure of the tri-level optimization problem. Furthermore, the decomposable nature of cutting planes offers a promising avenue for enabling distributed implementations of TTSO, thereby potentially enhancing scalability and computational efficiency.
* **Iteration Complexity Analysis.** To validate the effectiveness of our method, we conducted a thorough theoretical analysis of the algorithm. We theoretically derive that the iteration complexity of the proposed algorithm for achieving an \(\epsilon\)-stationary point is bounded by \(\mathcal{O}\left(\frac{1}{\epsilon^{2}}\right)\).

## 2 Related Work

In this section, we provide an overview of the foundational concepts and methodologies related to our research, including OOD Generalization and the LLM in time series.

**OOD Generalization.** OOD Generalization research focuses on improving the model's ability to generalize when there is a difference in distribution between the training and test data, and has been widely studied in the fields of Computer Vision (CV) (Recht et al., 2019; Salman et al., 2021) and Natural Language Processing (NLP) (Tu et al., 2020; Schneider et al., 2020). Existing works for out-of-distribution (OOD) generalization are diverse and can generally be categorized into approaches that consider sample-level (Zhang et al., 2017; Zhou et al., 2021) or group-level (Sagawa et al., 2019,Huang et al., 2020) uncertainty. However, the exploration of OOD generalization specially for time series remains relatively underdeveloped. A recent study (Lu et al., 2023) introduced 'Diversify', an innovative approach that models time series data from the perspective of distribution and obtain superior performance. In our work, we consider both sample-level and group-level uncertainties and formulate them as a tri-level optimization problem.

**LLM in Time Series**. The integration of LLMs in time series analysis is a rapidly evolving field, drawing significant interest due to their superior pattern recognition and reasoning abilities (Wang et al., 2023; Chu et al., 2023). A recent example is Time-LLM (Jin et al., 2023), which introduces an innovative method by reprogramming time series and incorporating linguistic prompts, effectively activating the extensive capabilities of LLM. In addition, the OFA framework (Zhou et al., 2023), utilizing the frozen pretrained transformer framework, validates the versatility and effectiveness of pre-trained models in time series analysis. Another innovative approach is PromptCast (Xue and Salim, 2023), which employs a prompt-based learning method, transforming numerical input and output data into prompts for effective forecasting in zero-shot settings. The TEMPO (Cao et al., 2023) adapts to changes in time series distribution by decomposing time series and adding different prompts for each component and obtain competitive performance in time series forecasting. In specialized domains like traffic (Xue et al., 2022), finance (Zhang et al., 2023) and healthcare (Liu et al., 2023), LLMs have also shown unique advantages. In this work, we aim to enhance OOD robustness for time series by fine-tuning LLMs with TTSO.

## 3 Problem Formulation and Algorithm

**Notations.**\(\mathcal{X}\) and \(\mathcal{Y}\) represent the input and target spaces of samples, respectively. The predictor \(f_{\varphi}=h_{\omega}\circ r_{\theta}\) consists of the representation function \(r_{\theta}(\cdot)\) with parameter \(\mathbf{\theta}\) and the classifier \(h_{\omega}\) with parameter \(\mathbf{\omega}\). The function \(f_{\varphi}:\mathcal{X}\rightarrow\mathcal{Y}\) maps time series \(\mathbf{X}\in\mathcal{X}\) to \(Y\in\mathcal{Y}\), where \(\mathbf{X}\in\mathbb{R}^{T\times F}\) and \(Y\in\mathbb{R}_{+}\), with \(T\) as the time series length and \(F\) as the feature dimensions. The multivariate time series \(\mathbf{X}\), composed of \(F\) univariate time series each with \(T\) observations, is sampled i.i.d. from distribution \(\mathbb{P}\) and represented as \(\mathbf{X}=[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{F}]\), where \(\mathbf{x}_{i}=[x_{v_{1}},x_{v_{2}},\ldots,x_{T}]\) for \(i=1,\ldots,F\). Assume source domain distributions are \(\mathbb{P}_{S_{i}}\) for \(i\in\{1,2,\ldots,K\}\) and the target domain distribution is \(\mathbb{P}_{\mathrm{T}}\). The source domain data \(\mathcal{D}_{S_{i}}\) is sampled i.i.d. from \(\mathbb{P}_{S_{i}}\), and the target domain data \(\mathcal{D}_{T}\) is sampled i.i.d. from \(\mathbb{P}_{\mathrm{T}}\).

### Preliminary

Given a training dataset \(\mathcal{D}_{\text{train}}=\{(\mathbf{X}_{i},Y_{i})\}_{i=1}^{N}\) sampled from the distribution \(\mathbb{P}_{\text{train}}(\mathbf{X},Y)\). In supervised learning, the goal is to learn an optimal predictor \(f_{\varphi^{*}}\) on \(\mathcal{D}_{\text{train}}\) such that \(f_{\varphi^{*}}\) generalizes well on a test dataset \(\mathcal{D}_{\text{test}}\) sampled from the distribution \(\mathbb{P}_{\text{test}}(\mathbf{X},Y)\). In self-supervised contrastive learning, for a given time series \(\mathbf{X}\), we generate two augmented views, \(\mathbf{X}_{a_{1}}\) and \(\mathbf{X}_{a_{2}}\), using augmentation methods \(a_{1},a_{2}\in\mathcal{A}\). These augmentations produce the time series representations \(\mathbf{R}_{1}=r_{\theta}\left(a_{1}(\mathbf{X})\right)\in\mathbb{R}^{T\times M}\) and \(\mathbf{R}_{2}=r_{\theta}\left(a_{2}(\mathbf{X})\right)\in\mathbb{R}^{T\times M}\), through the representation function \(r_{\theta}\). The objective of contrastive learning is to minimize the distance between positive pairs \((\mathbf{R}_{1},\mathbf{R}_{2})\) while maximizing the distance between positive and negative pairs. The general formula for contrastive loss, as detailed in Zhao et al. (2022), is formulated as follows

\[\ell_{\text{con}}=\ell_{\text{align}}\left(r_{\theta};\mathbb{P},\pi\right)+ \lambda\ell_{\text{reg}}\left(r_{\theta};\mathbb{P},\pi\right). \tag{1}\]

The first term aims to minimize the distance between positive pairs in the latent space, while the second term served as a regularizer prevents representation collapse. To evaluate the performance of the model, a classifier \(h_{\omega}\) is trained using the representation function\(r_{\theta^{*}}\)

\[h_{\omega^{*}}=\arg\min_{h_{\omega}}\ \mathbb{E}_{(\mathbf{X},Y)\sim\mathbb{P}_{ \text{train}}}\ell_{\text{sup}}\ell_{\text{sup}}\left(h_{\omega}\circ r_{ \theta^{*}}(\mathbf{X}),Y\right), \tag{2}\]

where the representation function \(r_{\theta^{*}}(\cdot)\) is optimized via the supervised loss in Eq. (2). The classification is performed using \(f_{\varphi^{*}}=h_{\omega^{*}}\circ r_{\theta^{*}}\). However, the discrepancy between the training distribution \(\mathbb{P}_{\text{train}}(\mathbf{X},Y)\) and the test distribution \(\mathbb{P}_{\text{test}}(\mathbf{X},Y)\) poses a challenge for generalizing \(f_{\varphi^{*}}\) to test data. Directly optimizing \(\ell_{\text{sup}}\left(f_{\varphi}(\mathbf{X}),Y\right)\) may lead to overfitting, compromising performance on unseen data. To mitigate this issue, invariant representation learning (Arjovsky et al., 2019) is employed to handle distribution shifts by learning robust invariant representations across diverse distributions. To achieve this, we begin with the following assumption.

**Assumption 1** (Invariant Assumption (Zhao et al., 2022)).: _Considering K different environments (domains) \(\mathcal{E}\), there exists a random variable \(\psi(\mathbf{X})\) such that for any \(e,e^{\prime}\in\mathrm{supp}(\mathcal{E})\), it holds that \(\mathrm{P}\left(Y\mid\psi\left(\mathbf{X}_{e}\right)\right)=\mathrm{P}\left(Y\mid \psi\left(\mathbf{X}_{e^{\prime}}\right)\right)\)._

This assumption implies that for time series \(\mathbf{X}\) observed in different environments, invariant rationales exist and their relationship with the corresponding labels remains stable. This stability ensures that predictions remain consistent across various environments, relying on these rationales. Assuming \(\psi(\cdot)\) represents the representation function \(r_{\theta}(\cdot)\) parameterized by \(\theta\), then it follows that

\[r_{\theta^{*}}(\mathbf{X}_{e})=r_{\theta^{*}}(\mathbf{X}_{e^{\prime}})=\psi(\mathbf{X}). \tag{3}\]

In contrastive representation learning, where labels are not available, the theoretical analysis of the downstream performance is challenging. To address this, research (Zhao et al., 2022) bridges this gap by connects contrastive loss to downstream risks,

\[\mathcal{R}(h_{\omega}\circ r_{\theta};\mathbb{P}_{\pi})\!\leq\!c\|h_{\omega} \|\sqrt{K\sigma}(\!\ell_{\text{align}}(r_{\theta};\mathbb{P}_{\pi})\!)^{\frac{ 1}{4}}\!+\!\|h_{\omega}\|\tau(\sigma,\!\delta)\!+\!\sum_{k}\!\mathbb{P}_{\pi} (\mathcal{C}_{k})\|e_{k}\!-\!h_{\omega}\circ\mu_{k}(r_{\theta};\mathbb{P}_{ \pi})\| \tag{4}\]

where \(c\) is a positive constant, \(\tau(\sigma,\delta)\) refers to a set of constants determined by the \((\sigma,\delta)\)-augmentation, and \(C_{k}\) corresponds to the sample set for class \(k\). The first term is optimized during contrastive pre-training. The second term depends on data augmentations \((\sigma,\delta)\). The third term, related to the linear layer \(h_{\omega}\), is optimized in downstream tasks. As shown by Eq. (4), contrastive learning on distribution \(\mathbb{P}\) with augmentation function \(\pi\) essentially optimizes the upper bound of the supervised risk.

Each environment \(\mathcal{E}\) corresponds to a domain distribution \(\mathbb{P}_{S_{i}}\). To learn an invariant representation over the domain set \(\mathcal{P}\), we first provide a mathematical definition of invariant risk minimization.

**Definition 1** (Invariant Risk Minimization (Arjovsky et al., 2019)).: _If there exists a classifier \(h_{0}\) that is optimal for all domains in \(\mathcal{P}\), i.e., \(h_{0}\in\operatorname*{argmin}_{h}\mathcal{R}\left(h\circ r_{\theta};\mathbb{ P}_{S_{i}}\right),\forall\mathbb{P}_{S_{i}}\in\mathcal{P}\), then the representation function \(r_{\theta}\) elicits an invariant predictor \(h_{0}\circ r_{\theta}\) across the domain set \(\mathcal{P}\)._

This definition is equivalent to learning features that have a stable association with the target variable, which has been theoretically and empirically proven to improve the transferability of supervised learning across different distributions (Arjovsky et al., 2019; Zhao et al., 2022).

### A Tri-level Learning Framework

To address OOD challenges, GroupDRO (Sagawa et al., 2019) propose a minax formulation to minimizes the maximum domain supervised loss to enhance robustness against unseen data. According to Eq. (4), contrastive learning optimizes the upper bound of supervised risk. Thus, we extend GroupDRO by replacing the supervised loss with a self-supervised contrastive loss, aiming to learn invariant representations. We further impose constraints on the group distribution \(\mathbf{q}\) to mitigate the risk of overfitting to specific domains. This results in a bi-level optimization problem

\[\begin{array}{rl}\min\limits_{\mathbf{\theta},\mathbf{q}}&\sum_{i=1}^{K}q_{i}\ell_{ \text{con}}(r_{\theta};\mathcal{D}_{S_{i}},\pi)\\ \text{s.t.}&\mathbf{q}=\operatorname*{arg\,max}\limits_{\mathbf{q}^{\prime}\in\Delta^ {K}}\sum_{i=1}^{K}q_{i}^{\prime}\ell_{\text{con}}(r_{\theta};\mathcal{D}_{S_{ i}},\pi)\\ &\text{s.t.}\ d(\mathbf{p},\mathbf{q}^{\prime})\leq\tau,\end{array} \tag{5}\]

where \(d\left(\cdot,\cdot\right)\) denotes a distribution distance metric, such as KL divergence, Wasserstein distance, or Euclidean distance, \(\Delta^{K}\) is a probability simplex, and \(\tau\) is a constant. Following previous work (Qian et al., 2019), we adopt the Euclidean distance due to its strong convexity, which reults in faster convergence (Rakhlin et al., 2012). The outer optimization seeks the best parameters across all domains to optimize overall performance, while the inner optimization, representing the group-level uncertainty, optimizes the worst-case distribution to enhance representation robustness.

**Definition 2** (Augmentation Robust Alignment Loss (Zhao et al., 2022)).: _For any two augmentation methods \(a,a^{\prime}\in\mathcal{A}\), the robust alignment loss is defined as follows_

\[\ell_{\text{ar}}(r_{\theta};\mathbb{P}):=\mathbb{E}_{\mathbf{X}\sim\mathbb{P}} \sup_{(a,a^{\prime})\in\mathcal{A}}\|r_{\theta}(a(\mathbf{X}))-r_{\theta}\left(a^{ \prime}(\mathbf{X})\right)\|^{2}. \tag{6}\]

**Theorem 1** (Upper Bound of Risk Gap Between Augmented Domains (Shen et al., 2021)).: _For any two augmentation methods \(a,a^{\prime}\in\mathcal{A}\), representation function \(r_{\theta}\) and classifier \(h_{\omega}\), we have_

\[\sup_{a,a^{\prime}\in\mathcal{A}}\|\mathcal{R}\left(h_{\omega}\circ r_{\theta}; \mathbb{P}_{a}\right)-\mathcal{R}\left(h_{\omega}\circ r_{\theta};\mathbb{P}_{a ^{\prime}}\right)\|\leq c\|h_{\omega}\|\ell_{\text{ar}}\left(r_{\theta};\mathbb{ P}_{\text{train}}\right). \tag{7}\]_Fix \(r_{\theta}\), let \(h_{a}=\arg\min_{h_{\omega}}\mathcal{R}\left(h_{\omega}\circ r;\mathbb{P}_{\text{ main}}\right)\), we have_

\[\|\mathcal{R}\left(h_{a}\circ r_{\theta};\mathbb{P}_{a^{\prime}}\right)- \mathcal{R}\left(h_{a^{\prime}}\circ r_{\theta};\mathbb{P}_{a^{\prime}}\right) \|\leq 2c\left(\|h_{a}\|+\|h_{a^{\prime}}\|\right)\ell_{ar}\left(r_{\theta}; \mathbb{P}_{\text{train}}\right). \tag{8}\]

Theorem 1 states that minimizing \(\ell_{\text{ar}}\left(r_{\theta};\mathbb{P}_{\text{train}}\right)\) makes the optimal predictor more consistent across different augmentation domains, i.e., minimize \(\ell_{\text{ar}}(r_{\theta};\mathbb{P}_{\text{train}})\) can enhance the invariance of the learned representation. Nonetheless, evaluating \(\ell_{\text{ar}}\left(r;\mathbb{P}_{\text{train}}\right)\) involves a supremum operator, and the large set \(\mathcal{A}\) makes accurate computation infeasible. Therefore, we propose an approximation for \(\ell_{\text{ar}}\left(r_{\theta};\mathbb{P}_{\text{train}}\right)\). We start with the following reasonable assumption.

**Assumption 2**.: _For any pair of augmentation methods \(a,a^{\prime}\in\mathcal{A}\), they can be viewed as introducing specific perturbations \(\boldsymbol{\delta}\) to the sample \(\boldsymbol{X}\), i.e., \(a(\boldsymbol{X})=\boldsymbol{X}+\boldsymbol{\delta}_{a},a^{\prime}(\boldsymbol {X})=\boldsymbol{X}+\boldsymbol{\delta}_{a^{\prime}}\)._

Suppose \(\boldsymbol{\delta}_{a}\) and \(\boldsymbol{\delta}_{a^{\prime}}\) are sampled from \(\mathbb{P}_{\text{perb}}\), representing the distribution of perturbations induced by augmentation techniques. We adopt a Gaussian Mixture Model (GMM) [11] to accurately characterize the uncertain perturbation distribution. Thus, the distribution of \(\boldsymbol{\delta}\) is given by

\[p(\boldsymbol{\delta};\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})= \sum_{m=1}^{M}\pi_{m}\mathcal{N}\left(\boldsymbol{\delta};\mu_{m},\sigma_{m}^ {2}\right), \tag{9}\]

where \(\pi_{m}\) represents the weight of the \(m^{th}\) component in the mixture, and \(\sum_{m=1}^{M}\pi_{m}=1\). The expression for \(\ell_{\text{ar}}\left(r_{\theta};\mathbb{P}_{\text{train}}\right)\) can be written as \(\sup_{\boldsymbol{\delta}\sim p(\boldsymbol{\delta};\boldsymbol{\pi}, \boldsymbol{\mu},\boldsymbol{\sigma})}\sum_{i=1}^{K}q_{i}\ell_{\text{align}}( \boldsymbol{\theta},\boldsymbol{\delta};\mathcal{D}_{S_{i}})\). Consequently, we can further extend problem (5) to the following tri-level optimization problem.

\[\begin{array}{rl}\min_{\boldsymbol{\theta},\boldsymbol{\theta},\boldsymbol{ \delta}}&\sum_{i=1}^{K}q_{i}\ell_{\text{con}}\left(\boldsymbol{\theta}, \boldsymbol{\delta};\mathcal{D}_{S_{i}}\right)\\ \text{s.t.}&\boldsymbol{q}=\underset{\boldsymbol{q}^{\prime}\in\Delta^{K}}{ \arg\max}\sum_{i=1}^{K}q_{i}^{\prime}\ell_{\text{con}}\left(\boldsymbol{ \theta},\boldsymbol{\delta};\mathcal{D}_{S_{i}}\right)\\ &\text{s.t.}&d(\boldsymbol{p},\boldsymbol{q}^{\prime})\leq\tau\\ &\boldsymbol{\delta}=\underset{\boldsymbol{\delta}^{\prime}\sim p(\boldsymbol{ \delta}^{\prime};\boldsymbol{\pi},\boldsymbol{\mu},\boldsymbol{\sigma})}{ \arg\max}\sum_{i=1}^{K}q_{i}^{\prime}\ell_{\text{align}}(\boldsymbol{\theta}, \boldsymbol{\delta}^{\prime};\mathcal{D}_{S_{i}})\\ &\text{s.t.}&\|\boldsymbol{\mu}\|\leq C_{1},\|\boldsymbol{\sigma}\|\leq C_{2},\sum_{m=1}^{M}\pi_{m}=1,\pi_{m}\geq 0,\end{array} \tag{10}\]

where \(C_{1}\) and \(C_{2}\) are constants. The third-level optimization addresses sample-level uncertainties by maximizing the alignment loss under the worst-case perturbation distribution. Figure 1 illustrates these concepts, showing how group-level and sample-level optimizations interact within the tri-level framework. To theoretically justify our approach in Eq. (10), we present the following theorem.

**Theorem 2** (Upper Bound on Target Error).: _Given the previous setup, let \(\mathcal{H}\) be a hypothesis space of Vapnik-Chervonenkis (VC) Dimension \(d\) and \(h_{T}^{*}=\min_{h\in\mathcal{H}}\epsilon_{T}(h)\). Let \(\mathcal{P}_{\alpha}=\{\mathbb{P}_{\alpha}\mid\mathbb{P}_{\alpha}=\sum_{i} \alpha_{i}\mathbb{P}_{S_{i}},\;\sum_{i}\alpha_{i}=1,\;\alpha_{i}\geq 0\,\}\). If \(\hat{h}\in\mathcal{H}\) is the empirical minimizer on \(\mathbb{P}_{\alpha}\), then for any \(\delta\) and \(\mathbb{P}_{C}\in\mathcal{P}_{\alpha}\), with probability at least \(1-\delta\),_

\[\epsilon_{T}(\hat{h})\leq 3\epsilon_{T}\left(h_{T}^{*}\right)+\lambda+d_{\mathcal{H \Delta H}}\left(\mathbb{P}_{C},\mathbb{P}_{T}\right)+\max_{i,j}d_{\mathcal{H \Delta H}}(\mathbb{P}_{S_{i}},\mathbb{P}_{S_{j}})+C(\delta,m,d), \tag{11}\]

_where \(\lambda=2\sum_{i=1}^{K}\alpha_{i}\epsilon_{S_{i}}(h^{*})\) and \(C(\delta,m,d)\) is a statistical term. \(d_{\mathcal{H\Delta H}}(\cdot,\cdot)\) is a metric function which measures differences in distribution [11]. \(\epsilon_{S_{i}}(h)\) and \(\epsilon_{T}(h)\) is the source error and the target error._

**Discussion**: Theorem 2 provides a theoretical framework for estimating performance on a new target distribution. The TTSO framework in Eq. (10) focuses on minimizing the terms \(d_{\mathcal{H\Delta H}}\left(\mathbb{P}_{\text{C}},\mathbb{P}_{\text{T}}\right)\) and \(\max_{i,j}d_{\mathcal{H\Delta H}}(\mathbb{P}_{S_{i}},\mathbb{P}_{S_{j}})\), thereby giving a tighter bound of target error to improve generalization ability. Proof of Theorem 2 and further discussion of our motivation are in Appendix A.2.

Figure 1: The depiction of sample-level, group-level, and combined uncertainties.

```
0: Training datasets \(\{\mathcal{D}_{S_{i}}\}\), learning rates \(\eta_{\theta},\eta_{q},\eta_{\delta}\), number of iterations \(T\).
0: Optimized parameters \(\mathbf{\theta}^{*}\)
1: Initialize parameters \(\mathbf{\theta},\mathbf{q},\mathbf{\delta}\) and initial set of cutting planes \(\mathcal{S}^{0}\).
2:for\(t=0\) to \(T-1\)do
3: Update variable \(\mathbf{\theta}^{(t+1)}\), \(\mathbf{q}^{(t+1)}\) and \(\mathbf{\delta}^{(t+1)}\) according to Eq. (19), (20) and (21)
4:if t mod k == 0 then
5:if\(h(\mathbf{\theta}^{(t+1)},\mathbf{q}^{(t+1)},\mathbf{\delta}^{(t+1)})>\varepsilon\)then
6: Add new cutting planes to set \(\mathcal{S}^{t+1}\) according to Eq. (22)
7:endif
8:endif
9:endfor
10:return\(\mathbf{\theta}^{(T)}\)
```

**Algorithm 1** SLA: Stratified Localization Algorithm

However, solving the constrained tri-level optimization is extremely challenging. In the next subsection, we introduce a stratified localization algorithm to address this problem effectively.

### Stratified Localization Algorithm

Due to the hierarchical structure of the tri-level problem, we develop a stratified version of the localization method (Boyd and Vandenberghe, 2007; Jiao et al., 2023) to tackle the problem presented in Eq. (10). First, we use exterior penalty method to reformulate the third level problem, the resulting problem is

\[\begin{array}{rl}\min\limits_{\mathbf{\theta},\mathbf{q},\mathbf{\delta}}&\sum_{i=1}^{K}q _{i}\ell_{\text{con}}\left(\mathbf{\theta},\mathbf{\delta};\mathcal{D}_{S_{i}}\right) \\ \text{s.t.}&\mathbf{q}=\operatorname*{arg\,max}\limits_{\mathbf{q}^{\prime}\in\Delta^{K }}\sum_{i=1}^{K}q_{i}^{\prime}\ell_{\text{con}}\left(\mathbf{\theta},\mathbf{\delta}; \mathcal{D}_{S_{i}}\right)\\ &\text{s.t.}&d\left(\mathbf{p},\mathbf{q}^{\prime}\right)\leq\tau\\ &\mathbf{\delta}=\operatorname*{arg\,max}\limits_{\mathbf{\delta}^{\prime}\sim p(\mathbf{ \delta}^{\prime};\mathbf{\pi},\mathbf{\mu},\mathbf{\sigma})}\sum_{i=1}^{K}q_{i}^{\prime} \ell_{\text{align}}\left(\mathbf{\theta},\mathbf{\delta}^{\prime};\mathcal{D}_{S_{i}} \right)-P_{3},\end{array} \tag{12}\]

where \(P_{3}\) is a penalty term defined as \(P_{3}=\rho_{1}(\max(0,\|\mathbf{\mu}\|-C_{1}))^{2}+\rho_{2}(\max(0,\|\mathbf{\sigma}\| -C_{2}))^{2}+\rho_{3}(\sum_{m=1}^{M}\pi_{m}-1)^{2}+\rho_{4}(\max(0,-\pi_{m})) ^{2}\), and \(\rho_{i}\) are penalty coefficients.

Given that the third-level optimization is a constraint for the second-level optimization, we employ \(T_{3}\) steps of gradient ascent to approximate the third-level problem. This technique is commonly used in previous bi-level optimization studies (Ji et al., 2021). By defining \(f_{3}(\mathbf{\theta},\mathbf{q}^{\prime},\mathbf{\delta}^{\prime})=\sum_{i=1}^{K}q_{i}^{ \prime}(\ell_{\text{align}}\left(\mathbf{\theta},\mathbf{\delta}^{\prime};\mathcal{D}_{ S_{i}}\right)-P_{3}\) and using the exterior penalty method, the resulting optimization problem can be expressed as

\[\begin{array}{rl}\min\limits_{\mathbf{\theta},\mathbf{q},\mathbf{\delta}}&\sum_{i=1}^{K}q _{i}\ell_{\text{con}}\left(\mathbf{\theta},\mathbf{\delta};\mathcal{D}_{S_{i}}\right) \\ \text{s.t.}&\mathbf{q}=\operatorname*{arg\,max}\limits_{\mathbf{q}^{\prime}\in\Delta^{K }}\sum_{i=1}^{K}q_{i}^{\prime}\ell_{\text{con}}\left(\mathbf{\theta},\mathbf{\delta}; \mathcal{D}_{S_{i}}\right)-P_{2},\end{array} \tag{13}\]

where \(P_{2}=\lambda_{1}(\sum_{i=1}^{K}q_{i}-1)^{2}+\sum_{i=1}^{K}\lambda_{2}\max(0,-q _{i})+\lambda_{3}\|\mathbf{\delta}-\mathbf{\delta}^{(0)}-\sum_{i=0}^{T_{3}-1}\eta_{ \delta}\nabla_{\mathbf{\delta}^{\prime}}f_{3}(\mathbf{\theta},\mathbf{q}^{\prime},\mathbf{ \delta}^{\prime})\|^{2}\). Likewise, we perform \(T_{2}\) steps of gradient ascent to replace the second level optimization problem. With the definition of \(f_{2}(\mathbf{\theta},\mathbf{q}^{\prime},\mathbf{\delta})=\sum_{i=1}^{K}q_{i}^{\prime} \ell_{\text{con}}\left(\mathbf{\theta},\mathbf{\delta};\mathcal{D}_{S_{i}}^{tr}\right) -P_{2},\varphi(\mathbf{\theta},\mathbf{\delta})=\operatorname*{arg\,max}_{\mathbf{q}^{ \prime}}f_{2}(\mathbf{\theta},\mathbf{q}^{\prime},\mathbf{\delta})\) and \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})=\|\mathbf{q}-\varphi(\mathbf{\theta},\mathbf{\delta})\|\), Eq. (13) can be reformulated as follows

\[\begin{array}{rl}\min\limits_{\mathbf{\theta},\mathbf{q},\mathbf{\delta}}&\sum_{i=1}^{K}q _{i}\ell_{\text{con}}\left(\mathbf{\theta},\mathbf{\delta};\mathcal{D}_{S_{i}}\right) \\ \text{s.t.}&h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})=0.\end{array} \tag{14}\]

Let \(f_{1}\left(\mathbf{\theta},\mathbf{q},\mathbf{\delta}\right)=\sum_{i=1}^{K}q_{i}\ell_{\text{ align}}\left(\mathbf{\theta},\mathbf{\delta};\mathcal{D}_{S_{i}}\right)\). Considering the approximations of \(\mathbf{q}\) and \(\mathbf{\delta}\), the above problem can be relaxed as

\[\begin{array}{rl}\min\limits_{\mathbf{\theta},\mathbf{q},\mathbf{\delta}}&f_{1}\left(\mathbf{ \theta},\mathbf{q},\mathbf{\delta}\right)\\ \text{s.t.}&h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\leq\varepsilon,\end{array} \tag{15}\]

where \(\varepsilon>0\) is a constant. Inspired by the polyhedral approximation method (Burger et al., 2013), we utilize cutting planes to approximate the feasible region with respect to \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\leq\varepsilon\). In the \((t+1)^{th}\) iteration, the set of cutting planes, denoted as \(\mathcal{S}^{t}\), is defined as follows

\[\mathcal{S}^{t}=\{\mathbf{a}_{i}^{\top}\mathbf{\theta}+\mathbf{b}_{i}^{\top}\mathbf{q}+\mathbf{c}_{i} ^{\top}\mathbf{\delta}+d_{i}\leq 0,i=1,\cdots,|\mathcal{S}^{t}|\}, \tag{16}\]

where \(\mathbf{a}_{i}\in\mathbb{R}^{N},\mathbf{b}_{i}\in\mathbb{R}^{M},\mathbf{c}_{i}\in\mathbb{R }^{H},d_{i}\in\mathbb{R}^{1}\), and \(|\mathcal{S}^{t}|\) represents the number of cutting planes in \(\mathcal{S}^{t}\). Then Eq. (15) can be expressed as the following approximation problem

\[\min_{\mathbf{\theta},\mathbf{q},\mathbf{\delta}} f_{1}\left(\mathbf{\theta},\mathbf{q},\mathbf{\delta}\right)\] (17) s.t. \[\mathbf{a}_{i}^{\top}\mathbf{\theta}+\mathbf{b}_{i}^{\top}\mathbf{q}+\mathbf{c}_{i}^{ \top}\mathbf{\delta}+d_{i}\leq 0,\quad i=1,\cdots,|\mathcal{S}^{t}|.\]

The penalty function with respect to Eq. (17) can be described as

\[F\left(\mathbf{\theta},\mathbf{q},\mathbf{\delta}\right)=f_{1}\left(\mathbf{\theta},\mathbf{q}, \mathbf{\delta}\right)+\sum_{i}\lambda_{i}\max(0,\mathbf{a}_{i}^{\top}\mathbf{\theta}+\mathbf{ b}_{i}^{\top}\mathbf{q}+\mathbf{c}_{i}^{\top}\mathbf{\delta}+d_{i})^{2}. \tag{18}\]

In \((t+1)^{th}\) iteration, the variables are updated as follows

\[\mathbf{\theta}^{t+1} =\mathbf{\theta}^{t}-\eta_{\theta}\nabla_{\theta}F(\mathbf{\theta}^{t}, \mathbf{q}^{t},\mathbf{\delta}^{t}), \tag{19}\] \[\mathbf{q}^{t+1} =\mathbf{q}^{t}-\eta_{\theta}\nabla_{\theta}F(\mathbf{\theta}^{t},\mathbf{q}^ {t},\mathbf{\delta}^{t}),\] (20) \[\mathbf{\delta}^{t+1} =\mathbf{\delta}^{t}-\eta_{\delta}\nabla_{\delta}F(\mathbf{\theta}^{t}, \mathbf{q}^{t},\mathbf{\delta}^{t}). \tag{21}\]

Throughout the iteration process, the set of cutting planes \(\mathcal{S}^{t}\) is updated every \(k\) iterations for a tighter and more accurate polyhedral approximation. Before adding new cutting planes, we first check whether \((\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\) is a solution for Eq. (15). If it is not a feasible solution to Eq. (15), i.e., \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})>\varepsilon\), new cutting planes are added to \(\mathcal{S}^{t}\) based on Theorem 3 and Proposition 22. Algorithm 1 provides details of the proposed method.

**Theorem 3**.: _Let \(T_{2}=1\). If a first-order Taylor expansion is applied to the function \(f_{2}(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\) at the point \((\overline{\mathbf{\theta}},\overline{\mathbf{\delta}})\), it follows that the function \(h\left(\mathbf{\theta},\mathbf{q},\mathbf{\delta}\right)\) is convex with respect to \((\mathbf{\theta},\mathbf{q},\mathbf{\delta})\). The detailed proof can be found in Appendix A.3._

**Proposition 1**.: _Given the convexity of the function \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\), a new cutting plane is generated when the condition \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})>\varepsilon\) is not met. This cutting plane is formally expressed as_

\[h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})+\left[\begin{array}{c} \nabla_{\theta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\\ \nabla_{\theta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\\ \nabla_{\theta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\end{array} \right]^{\top}\left[\begin{array}{c}\mathbf{\theta}-\mathbf{\theta}^{t+1}\\ \mathbf{q}-\mathbf{q}^{t+1}\\ \mathbf{\delta}-\mathbf{\delta}^{t+1}\end{array}\right]\leq\varepsilon. \tag{22}\]

For the detailed derivation and proof of Proposition 22, please see Appendix A.1.

### TTSO for Fine-tuning LLMs

LLMs have garnered considerable attention in time series applications (Jin et al., 2023; Zhou et al., 2023). The emergent abilities of LLMs, especially in OOD scenarios, largely depends on the robustness of their representations(Wang et al., 2023; Chu et al., 2023). This section connects the established theoretical foundation with the practical application of fine-tuning LLMs for time series OOD generalization. We adapt TTSO framework for fine-tuning LLMs to enhance the performance in time series OOD generalization. Our proposed method involves a dual-stage fine-tuning method tailored for time series. The main process of fine-tuning are described below.

**Time Series Pre-processing.** Preprocessing starts with an input projection layer to bridge the gap in dimensions between raw time series data and the LLM's native embedding dimension. This step is crucial for the LLM's effective integration of time series. Following this, positional encoding is applied to preserve the sequential integrity of the time series.

**Dual-stage Fine-tuning Method.** In the first stage, we employ TTSO framework to fine-tune LLMs, in line with the previously mentioned tri-level optimization framework as illustrate in Eq. (10). We adopt the contrastive loss function designed for time series from Yue et al. (2022). In the second stage, the learned weights of the LLM, including the projection layer, are transferred to the downstream fine-tuning stage for time series classification. To retain the knowledge learned by the LLM from the corpus, we follow Chang et al. (2023); Zhou et al. (2023) by fixing the weights of the fully connected and attention layers, using Layer Normalization Tuning (Lu et al., 2022) to adjust only the layer normalization parameters, making the affine transformation trainable.

**Constrained Optimization for Fine-Tuning.** Research (Wortsman et al., 2022) indicates that adopting radical strategies for fine-tuning models, such as larger learning rates, can reduce out-ofdistribution robustness. Unconstrained optimization of model parameters during fine-tuning can lead to knowledge forgetting issues and decrease the model's generalization ability, as mentioned in Xuhong et al. (2018). Therefore, during fine-tuning for downstream tasks, we impose constraints on the parameters, following Xuhong et al. (2018), resulting in the following optimization problem

\[\begin{split}\min_{\mathbf{\theta}}&\ell_{cls}\left(r_{ \theta}\circ h_{\omega};\mathcal{D}\right)\\ \text{s.t.}&\|\mathbf{\theta}-\mathbf{\theta}_{0}\|\leq \gamma,\end{split} \tag{23}\]

where \(\mathbf{\theta}_{0}\) and \(\mathbf{\theta}\) respectively denote the weights from the first and second fine-tuning phases of the LLMs. More details of fine-tuning LLMs can be found in appendix D.

## 4 Convergence Analysis

**Assumption 3** (**Lipschitz Continuity of Gradient)**.: _Assume that the gradient of the function \(F\) is \(L\)-Lipschitz continuous gradient, i.e., for any \(\mathbf{x},\mathbf{y}\), there exists \(L>0\) such that:_

\[\|\nabla F(\mathbf{x})-\nabla F(\mathbf{y})\|\leq L\left\|\mathbf{x}-\mathbf{y}\right\|. \tag{24}\]

**Assumption 4** (**Unbiasedness and Variance Bound of Stochastic Gradients)**.: _Assume for the stochastic gradients \(g_{\theta},g_{q},g_{\theta}\), the following conditions are satisfied_

\[\begin{split}&\mathbb{E}_{\zeta_{j}^{t}}[g_{\theta}(\mathbf{\theta} ^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t})-\nabla_{\theta}F(\mathbf{\theta}^{ t},\mathbf{q}^{t},\mathbf{\delta}^{t})]=0,\\ &\mathbb{E}_{\zeta_{j}^{t}}[g_{\theta}(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t})-\nabla_{q}F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{ \delta}^{t})]=0,\\ &\mathbb{E}_{\zeta_{j}^{t}}[g_{\delta}(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t})-\nabla_{\delta}F(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t})]=0,\\ &\mathbb{E}_{\zeta_{j}^{t}}[\|g_{\theta}(\mathbf{\theta}^{t},\mathbf{q}^{ t},\mathbf{\delta}^{t};\zeta_{j}^{t})-\nabla_{\theta}F(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t})\|^{2}]\leq\sigma_{1}^{2},\\ &\mathbb{E}_{\zeta_{j}^{t}}[\|g_{\theta}(\mathbf{\theta}^{t},\mathbf{q}^{ t},\mathbf{\delta}^{t};\zeta_{j}^{t})-\nabla_{q}F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{ \delta}^{t})\|^{2}]\leq\sigma_{2}^{2},\\ &\mathbb{E}_{\zeta_{j}^{t}}[\|g_{\delta}(\mathbf{\theta}^{t},\mathbf{q}^{ t},\mathbf{\delta}^{t};\zeta_{j}^{t})-\nabla_{\delta}F(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t})\|^{2}]\leq\sigma_{3}^{2},\end{split} \tag{25}\]

_where \(\mathbb{E}_{\zeta_{j}^{t}}[\cdot]\) denotes the expectation over the \(\zeta_{j}^{t}\)._

**Assumption 5** (**Bounded Gradient)**.: _Assume that the gradient of the function \(F\) is bounded, i.e., \(\forall t,\|\nabla_{\theta}F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t})\|^{2} \leq\alpha_{1}^{2},\|\nabla_{q}F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t})\| ^{2}\leq\alpha_{2}^{2},\|\nabla_{\delta}F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{ \delta}^{t})\|^{2}\leq\alpha_{3}^{2}\)._

**Definition 3** (\(\epsilon\)-**Stationary Point)**.: _Following Xu et al. (2023), Jiao et al. (2024), a point \((\mathbf{\theta},\mathbf{q},\mathbf{\delta})\) is considered an \(\epsilon\)-stationary point (where \(\epsilon>0\)) of a differentiable function \(F\) if the sum of squares of its gradients on these variables satisfies \(\|\nabla G^{t}\|\leq\epsilon\). Let \(T(\epsilon)\) be the index of the first iteration that satisfies \(\|\nabla G^{t}\|\leq\epsilon\), i.e., \(T(\epsilon)=\min\{t\ \|\nabla G^{t}\|\leq\epsilon,t>t_{1}\}\)._

**Theorem 4** (**Convergence Guarantee)**.: _With the continuous addition of cutting planes, the optimal objective value of the approximated problem, delineated in Eq. (17), is guaranteed to converge monotonically. For further details, see the proof of Theorem 4 in appendix A.4._

**Theorem 5** (**Convergence Rate)**.: _Under the assumptions 3, 4, and 5, by setting the step-sizes as \(\eta_{\theta}=\eta_{q}=\eta_{\delta}=\frac{1}{\sqrt{T_{1}-t_{1}}}\) and the batch size as \(B\), for a given \(\epsilon\), it follows that_

\[T(\epsilon)\sim\mathcal{O}\left(t_{1}+\frac{L^{2}(m(\alpha_{1}^{2}+\alpha_{2}^ {2}+\alpha_{3}^{2})+\sigma_{1}^{2}+\sigma_{2}^{2}+\sigma_{3}^{2})^{2}}{4m^{2}( \epsilon-F(\mathbf{\theta}^{T_{1}},\mathbf{q}^{T_{1}},\mathbf{\delta}^{T_{1}})+F^{*})^{2 }}\right), \tag{26}\]

_where \(F^{*}\) represents the lower bound of \(F\). The proof of Theorem 5 is detailed in appendix A.5._

## 5 Experiment

To evaluate the proposed TTSO framework, we conduct experiments on 6 real-world time series datasets using the leave-one-domain-out setting, including HHAR (Blunck et al., 2015), PAMAP (Reiss, 2012), WESAD (Philip Schmidt et al., 2018), SWELL (Koldijk et al., 2014), USC-HAD(Zhang and Sawchuk, 2012) and DSADS (Barshan and Altun, 2013). We compare with baseline method ERM (Vapnik, 1991) and 8 general OOD generalization methods: IRM (Arjovsky et al., 2019), GroupDRO (Sagawa et al., 2019), ANDMask (Parascandolo et al., 2020), RSC (Huang et al., 2020), Mixup (Zhang et al., 2017), VERx (Krueger et al., 2021), DIFEX(Lu et al., 2022b). And we further compare with 2 recent strong approach in time series: AdaRNN(Du et al., 2021) and GILE (Qian et al., 2021). We also include DIVERSIFY(Lu et al., 2023), DFDG(Zhang et al., 2021), and CCDG(Ragab et al., 2022), three methods specifically designed for time series OOD

[MISSING_PAGE_FAIL:9]

This demonstrates that even in the absence of a pre-trained model, TTSO fine-tuning can effectively enhance the model's OOD generalization capabilities, though not as significantly as that with a pre-trained GPT2.

## 6 Conclusion

Existing OOD generalization methods mainly focus on sample-level uncertainties or group-level uncertainties, often overlooking the interplay between these two aspects. In light of this, we propose the TTSO framework to integrate both sample-level and group-level uncertainties within a unified tri-level learning approach, thereby enhancing the model's robustness and adaptability in facing diverse and unforeseen distribution shifts. In addition, this innovative framework introduces a fresh perspective for the development and analysis of the Out-of-Distribution (OOD) generalization problem. Based on this formulation, we develop a stratified localization algorithm for the tri-level optimization problem and provide theoretical analysis regarding the iteration complexity of the proposed algorithm. Comprehensive studies have been carried out to assess the performance of the proposed algorithm and substantiate the theoretic claims. It is seen that TTSO with LLM can considerably improves the performance of time series OOD generalization.

## 7 Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 12371519 and 61771013; in part by Asiainfo Technologies; in part by the Fundamental Research Funds for the Central Universities of China; and in part by the Fundamental Research Funds of Shanghai Jiading District.

## References

* Arjovsky et al. (2019) Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* Bai et al. (2018) Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. _arXiv preprint arXiv:1803.01271_, 2018.
* Barshan and Altun (2013) Billur Barshan and Kerem Altun. Daily and Sports Activities. UCI Machine Learning Repository, 2013. DOI: [https://doi.org/10.24432/C5C59F](https://doi.org/10.24432/C5C59F).
* Ben-David et al. (2010) Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. _Machine learning_, 79:151-175, 2010.
* Blunck et al. (2015) Henrik Blunck, Sourav Bhattacharya, Thor Prentow, Mikkel Kjrgaard, and Anind Dey. Heterogeneity activity recognition. UCI Machine Learning Repository, 2015. DOI: [https://doi.org/10.24432/C5689X](https://doi.org/10.24432/C5689X).
* Boyd and Vandenberghe (2007) Stephen Boyd and Lieven Vandenberghe. Localization and cutting-plane methods. _From Stanford EE 364b lecture notes_, 386, 2007.
* Boyd and Vandenberghe (2004) Stephen P Boyd and Lieven Vandenberghe. _Convex optimization_. Cambridge university press, 2004.
* Burger et al. (2013) Mathias Burger, Giuseppe Notarstefano, and Frank Allgower. A polyhedral approximation framework for convex and robust distributed optimization. _IEEE Transactions on Automatic Control_, 59(2):384-395, 2013.
* Cao et al. (2023) Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: Prompt-based generative pre-trained transformer for time series forecasting. _arXiv preprint arXiv:2310.04948_, 2023.
* Chang et al. (2023) Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained l1ms. _arXiv preprint arXiv:2308.08469_, 2023.
* Chang et al. (2018)Zhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui, Longfei Li, Siqiao Xue, et al. Leveraging large language models for pre-trained recommender systems. _arXiv preprint arXiv:2308.10837_, 2023.
* Deecke et al. (2021) Lucas Deecke, Timothy Hospedales, and Hakan Bilen. Visual representation learning over latent domains. In _International Conference on Learning Representations_, 2021.
* Du et al. (2021) Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and Chongjun Wang. Adamn: Adaptive learning and forecasting of time series. In _Proceedings of the 30th ACM international conference on information & knowledge management_, pages 402-411, 2021.
* Ghosal et al. (2023) Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio generation using instruction-tuned llm and latent diffusion model. _arXiv preprint arXiv:2304.13731_, 2023.
* Hamilton (2020) James D Hamilton. _Time series analysis_. Princeton university press, 2020.
* Han et al. (2024) Pengchao Han, Xingyan Shi, and Jianwei Huang. Fedal: Black-box federated knowledge distillation enabled by adversarial learning. _IEEE Journal on Selected Areas in Communications_, 2024.
* Hendrycks et al. (2020) Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 2744-2751, 2020.
* Huang et al. (2020) Zeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain generalization. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pages 124-140. Springer, 2020.
* Ji et al. (2021) Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design. In _International conference on machine learning_, pages 4882-4892. PMLR, 2021.
* Jiao et al. (2022a) Yang Jiao, Kai Yang, and Dongjin Song. Distributed distributionally robust optimization with non-convex objectives. _Advances in neural information processing systems_, 35:7987-7999, 2022a.
* Jiao et al. (2022b) Yang Jiao, Kai Yang, Dongjing Song, and Dacheng Tao. Timeautoad: Autonomous anomaly detection with self-supervised contrastive loss for multivariate time series. _IEEE Transactions on Network Science and Engineering_, 9(3):1604-1619, 2022b.
* Jiao et al. (2023) Yang Jiao, Kai Yang, Tiancheng Wu, Dongjin Song, and Chengtao Jian. Asynchronous distributed bilevel optimization. In _The Eleventh International Conference on Learning Representations_, 2023.
* Jiao et al. (2024) Yang Jiao, Kai Yang, Tiancheng Wu, Chengtao Jian, and Jianwei Huang. Provably convergent federated trilevel learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 12928-12937, 2024.
* Jin et al. (2023) Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. _arXiv preprint arXiv:2310.01728_, 2023.
* Koldijk et al. (2014) Saskia Koldijk, Maya Sappelli, Suzan Verberne, Mark A Neerincx, and Wessel Kraaij. The swell knowledge work dataset for stress and user modeling research. In _Proceedings of the 16th international conference on multimodal interaction_, pages 291-298, 2014.
* Krueger et al. (2021) David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, pages 5815-5826. PMLR, 2021.
* Liu et al. (2023a) Bo Liu, Liming Zhan, Zexin Lu, Yujie Feng, Lei Xue, and Xiao-Ming Wu. How good are large language models at out-of-distribution detection? _arXiv preprint arXiv:2308.10261_, 2023a.
* Liu et al. (2022) Xin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-Zher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Large language models are few-shot health learners. _arXiv preprint arXiv:2305.15525_, 2023b.
* Liu et al. (2020)Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. _arXiv preprint arXiv:2103.05247_, 1, 2021.
* Lu et al. (2022a) Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transformers as universal computation engines. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 7628-7636, 2022a.
* Lu et al. (2022b) Wang Lu, Jindong Wang, Haoliang Li, Yiqiang Chen, and Xing Xie. Domain-invariant feature exploration for domain generalization. _arXiv preprint arXiv:2207.12020_, 2022b.
* Lu et al. (2023) Wang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation learning for time series classification. In _International Conference on Learning Representations_, 2023.
* Mirchandani et al. (2023) Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. _arXiv preprint arXiv:2307.04721_, 2023.
* Parascandolo et al. (2020) Giambattista Parascandolo, Alexander Neitz, ANTONIO ORVIETO, Luigi Gresele, and Bernhard Scholkopf. Learning explanations that are hard to vary. In _International Conference on Learning Representations_, 2020.
* Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Schmidt et al. (2018) A Philip Schmidt, R Duerichen Reiss, and Introducing WESAD Kristof Van Laerhoven. A multimodal dataset for wearable stress and affect detection. In _Proceedings of the International Conference on Multimodal Interaction_, 2018.
* Qian et al. (2021) Hangwei Qian, Sinno Jialin Pan, and Chunyan Miao. Latent independent excitation for generalizable sensor-based cross-person activity recognition. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11921-11929, 2021.
* Qian et al. (2019) Qi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization over multiple domains. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 4739-4746, 2019.
* Quinonero-Candela et al. (2009) Joaquin Quinonero-Candela, Masashi Sugiyama, Neil D. Lawrence, and Anton Schwaighofer. _Dataset shift in machine learning_. MIT Press, 2009.
* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. _OpenAI Blog_, 2018.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* Ragab et al. (2022) Mohamed Ragab, Zhenghua Chen, Wenyu Zhang, Emadeldeen Eldele, Min Wu, Chee-Keong Kwoh, and Xiaoli Li. Conditional contrastive domain generalization for fault diagnosis. _IEEE Transactions on Instrumentation and Measurement_, 71:1-12, 2022. doi: 10.1109/TIM.2022.3154000.
* Rakhlin et al. (2012) Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In _International Conference on Machine Learning_, pages 1571-1578, 2012.
* Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International conference on machine learning_, pages 5389-5400. PMLR, 2019.
* Reiss (2012) Attila Reiss. PAMAP2 physical activity monitoring. UCI Machine Learning Repository, 2012. DOI: [https://doi.org/10.24432/C5NW2H](https://doi.org/10.24432/C5NW2H).
* Recht et al. (2019)Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In _International Conference on Learning Representations_, 2019.
* Salman et al. (2021) Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry, and Ashish Kapoor. Unadversarial examples: Designing objects for robust vision. _Advances in Neural Information Processing Systems_, 34:15270-15284, 2021.
* Schneider et al. (2020) Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. _Advances in Neural Information Processing Systems_, 33:11539-11551, 2020.
* Shen et al. (2021) Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* Tan et al. (2022) Qingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou Ng. Domain generalization for text classification with memory-based supervised contrastive learning. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 6916-6926, 2022.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Tu et al. (2020) Lifu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious correlations using pre-trained language models. _Transactions of the Association for Computational Linguistics_, 8:621-633, 2020.
* Vapnik (1991) Vladimir Vapnik. Principles of risk minimization for learning theory. _Advances in Neural Information Processing Systems_, 4, 1991.
* Wang et al. (2023) Yan Wang, Zhixuan Chu, Xin Ouyang, Simeng Wang, Hongyan Hao, Yue Shen, Jinjie Gu, Siqiao Xue, James Y Zhang, Qing Cui, et al. Enhancing recommender systems with large language model reasoning graphs. _arXiv preprint arXiv:2308.10835_, 2023.
* Wortsman et al. (2022) Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7959-7971, 2022.
* Xu et al. (2023) Zi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A unified single-loop alternating gradient projection algorithm for nonconvex-concave and convex-nonconcave minimax problems. _Mathematical Programming_, 201(1):635-706, 2023.
* Xue and Salim (2023) Hao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series forecasting. _IEEE Transactions on Knowledge and Data Engineering_, 2023.
* Xue et al. (2022) Hao Xue, Bhanu Prakash Voutharoja, and Flora D Salim. Leveraging language foundation models for human mobility forecasting. In _Proceedings of the 30th International Conference on Advances in Geographic Information Systems_, pages 1-9, 2022.
* Xuhong et al. (2018) LI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In _International Conference on Machine Learning_, pages 2825-2834. PMLR, 2018.
* Yue et al. (2022) Zhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and Bixiong Xu. Ts2vec: Towards universal representation of time series. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 8980-8987, 2022.
* Zhang et al. (2023) Boyu Zhang, Hongyang Yang, and Xiao-Yang Liu. Instruct-fingerpt: Financial sentiment analysis by instruction tuning of general-purpose large language models. _arXiv preprint arXiv:2306.12659_, 2023.
* Zhang et al. (2017) Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* Zhang et al. (2018)Mi Zhang and Alexander A. Sawchuk. Usc-had: A daily activity dataset for ubiquitous activity recognition using wearable sensors. In _ACM International Conference on Ubiquitous Computing (Ubicomp) Workshop on Situation, Activity and Goal Awareness (SAGAware)_, Pittsburgh, Pennsylvania, USA, September 2012.
* 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2870-2874, 2021. doi: 10.1109/ICASSP39728.2021.9413872.
* Zhao et al. (2022) Xuyang Zhao, Tianqi Du, Yisen Wang, Jun Yao, and Weiran Huang. Arcl: Enhancing contrastive learning with augmentation-robust representations. In _International Conference on Learning Representations_, 2022.
* Zheng et al. (2022) Zangwei Zheng, Xiangyu Yue, Kai Wang, and Yang You. Prompt vision transformer for domain generalization. _arXiv preprint arXiv:2208.08914_, 2022.
* Zhou et al. (2021) Kaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In _International Conference on Learning Representations_, 2021.
* Zhou et al. (2022) Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* Zhou et al. (2023) Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series analysis by pretrained lm. _arXiv preprint arXiv:2302.11939_, 2023.

## Appendix A Theoretical Proofs and Discussion

### Proof of Proposition 22

Since \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\) is a convex function, we have that

\[h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\geq h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{ \delta}^{t+1})+\left[\begin{array}{c}\nabla_{\theta}h(\mathbf{\theta}^{t+1},\mathbf{q }^{t+1},\mathbf{\delta}^{t+1})\\ \nabla_{q}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\\ \nabla_{\delta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\end{array} \right]^{\top}\left[\begin{array}{c}\mathbf{\theta}-\mathbf{\theta}^{t+1}\\ \mathbf{q}-\mathbf{q}^{t+1}\\ \mathbf{\delta}-\mathbf{\delta}^{t+1}\end{array}\right]. \tag{27}\]

According to theorem 3, combine with Eq. (15) and Eq. (27), the new cutting plane will be generated as

\[h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})+\left[\begin{array}{c} \nabla_{\theta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\\ \nabla_{q}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\\ \nabla_{\delta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\end{array} \right]^{\top}\left[\begin{array}{c}\mathbf{\theta}-\mathbf{\theta}^{t+1}\\ \mathbf{q}-\mathbf{q}^{t+1}\\ \mathbf{\delta}-\mathbf{\delta}^{t+1}\end{array}\right]\leq\varepsilon. \tag{28}\]

From the inequalities, we can derive the coefficients \(\mathbf{a}_{i},\mathbf{b}_{i},\mathbf{c}_{i}\) and \(d_{i}\) as follows

\[\begin{array}{l}\mathbf{a}_{i}=\nabla_{\theta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1}, \mathbf{\delta}^{t+1}),\\ \mathbf{b}_{i}=\nabla_{q}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1}),\\ \mathbf{c}_{i}=\nabla_{\delta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1}), \\ d_{i}=h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})-\nabla_{\theta}h( \mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})^{\top}\mathbf{\theta}^{t+1}\\ -\nabla_{q}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})^{\top}\mathbf{q}^{t +1}-\nabla_{\delta}h(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})^{\top} \mathbf{\delta}^{t+1}-\varepsilon,\end{array} \tag{29}\]

which concludes the proof.

### Proof of Theorem 2

#### a.2.1 Background

For a sample distribution \(\mathbb{P}_{S}\) on inputs space \(\mathcal{X}\) with a binary labeling function \(f\) and a hypothesis \(h\), the error (or risk) is defined as follows

\[\epsilon_{S}(h,f)=\mathrm{E}_{\mathbf{x}\sim\mathbb{P}_{S}}[|h(\mathbf{x})-f(\mathbf{x})|]. \tag{30}\]

For simplicity, we use the shorthand \(\epsilon_{S}(h)=\epsilon_{S}(h,f_{S})\) for source risk and \(\epsilon_{T}(h)=\epsilon_{T}(h,f_{D})\) for target risk, where \(f_{S}\) and \(f_{D}\) represent the labeling function of source and target domain, respectively.

To bound the target error, following Ben-David et al. (2010), we define the \(\mathcal{H}\)-divergence. Given source distribution \(\mathbb{P}_{S}\) and target distribution \(\mathbb{P}_{T}\) over input space \(\mathcal{X}\), let \(\mathcal{H}\) be a hypothesis class on \(\mathcal{X}\). The \(\mathcal{H}\)-divergence between \(\mathbb{P}_{S}\) and \(\mathbb{P}_{T}\) is

\[d_{\mathcal{H}}\left(\mathbb{P}_{S},\mathbb{P}_{T}\right)=2\sup_{h\in\mathcal{ H}}|\mathrm{Pr}_{\mathbb{P}_{S_{i}}}[I(h)]-\mathrm{Pr}_{\mathbb{P}_{T}}[I(h)]|\,, \tag{31}\]

where \(I_{h}=\{\mathbf{x}\in\mathcal{X}:h(x)=1,h\in\mathcal{H}\}\). In addition, for a hypothesis space \(\mathcal{H}\), the symmetric difference hypothesis space \(d_{\mathcal{H}\Delta\mathcal{H}}\) is the set of hypotheses. And the \(d_{\mathcal{H}\Delta\mathcal{H}}\) in (Ben-David et al., 2010) is defined as

\[d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S},\mathbb{P}_{T}\right)=2 \sup_{h,h^{\prime}\in\mathcal{H}}|\mathrm{Pr}_{\mathbf{x}\sim\mathbb{P}_{S}}\left[ h(\mathbf{x})\neq h^{\prime}(\mathbf{x})\right]-\mathrm{Pr}_{\mathbf{x}\sim\mathbb{P}_{T}} \left[h(\mathbf{x})\neq h^{\prime}(\mathbf{x})\right]|. \tag{32}\]

**Theorem 6**.: _(Modified from Theorem 4 in (Ben-David et al., 2010)) Let \(\mathcal{H}\) be a hypothesis space of VC dimension \(d\). For each \(i\in\{1,\ldots,K\}\), let \(\mathcal{D}_{S_{i}}\) be a labeled sample of size \(\beta_{i}m\) drawn from \(\mathbb{P}_{S_{i}}\) and labeled according to function \(f_{S_{i}}\). If \(h\in\mathcal{H}\) is the empirical minimizer of \(\hat{\epsilon}_{\alpha}(h)\) for a fixed weight vector \(\mathbf{\alpha}\) on these samples and \(h^{*}_{T}=\min_{h\in\mathcal{H}}\epsilon_{T}(h)\) is the target error minimizer, then for any \(\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\epsilon_{T}(\hat{h})\leq\epsilon_{T}\left(h^{*}_{T}\right)+\sum_{i=1}^{N} \alpha_{i}\left(2\lambda_{i}+d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P }_{S_{i}},\mathbb{P}_{T}\right)\right)+2\sqrt{\left(\sum_{i=1}^{N}\frac{ \alpha_{i}^{2}}{\beta_{i}}\right)\left(\frac{d\log(2m)-\log(\delta)}{2m} \right)}, \tag{33}\]

_where \(\lambda_{i}=\min_{h\in\mathcal{H}}\left\{\epsilon_{T}(h)+\epsilon_{S_{i}}(h)\right\}\)._

#### a.2.2 Proof

According to the definition of \(d_{\mathcal{H}\Delta\mathcal{H}}\) in Eq. (32), we have

\[\begin{split} d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S_{i }},\mathbb{P}_{T}\right)&=2\sup_{h,h^{\prime}\in\mathcal{H}} \left|\Pr_{\mathbf{z}\sim\mathbb{P}_{S_{i}}}\left[h(\mathbf{x})\neq h^{\prime }(\mathbf{x})\right]-\Pr_{\mathbf{z}\sim\mathbb{P}_{T}}\left[h(\mathbf{x}) \neq h^{\prime}(\mathbf{x})\right]\right|\\ &=2\sup_{h,h^{\prime}\in\mathcal{H}}\left|\left(\Pr_{\mathbf{z} \sim\mathbb{P}_{S_{i}}}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{x})\right]- \Pr_{\mathbf{z}\sim\mathbb{P}_{C}}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{ x})\right]\right)\right.\\ &\quad+\left(\Pr_{\mathbf{z}\sim\mathbb{P}_{C}}\left[h(\mathbf{x}) \neq h^{\prime}(\mathbf{x})\right]-\Pr_{\mathbf{z}\sim\mathbb{P}_{T}}\left[h( \mathbf{x})\neq h^{\prime}(\mathbf{x})\right]\right)\\ &\leq 2\sup_{h,h^{\prime}\in\mathcal{H}}\left|\Pr_{\mathbf{z}\sim \mathbb{P}_{S_{i}}}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{x})\right]-\Pr _{\mathbf{z}\sim\mathbb{P}_{C}}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{x}) \right]\right|\\ &\quad+2\sup_{h,h^{\prime}\in\mathcal{H}}\left|\Pr_{\mathbf{z}\sim \mathbb{P}_{C}}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{x})\right]-\Pr_{ \mathbf{z}\sim\mathbb{P}_{T}}\left[h(\mathbf{x})\neq h^{\prime}(\mathbf{x}) \right]\right|\\ &=d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S_{i}}, \mathbb{P}_{C}\right)+d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{C}, \mathbb{P}_{T}\right)\end{split} \tag{34}\]

Since \(\mathcal{P}_{\alpha}=\left\{\mathbb{P}_{\alpha}\mid\mathbb{P}_{\alpha}=\sum_{ i}\alpha_{i}\mathbb{P}_{S_{i}},\ \sum_{i}\alpha_{i}=1,\ \alpha_{i}\geq 0\ \forall i\right\}\) and \(\mathbb{P}_{C}\in\mathcal{P}_{\alpha}\), we can obtain that

\[\begin{split} d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S _{i}},\mathbb{P}_{C}\right)&=d_{\mathcal{H}\Delta\mathcal{H}} \left(\mathbb{P}_{S_{i}},\sum_{j}\alpha_{j}\mathbb{P}_{S_{j}}\right)\\ &=d_{\mathcal{H}\Delta\mathcal{H}}\left(\sum_{j}\alpha_{j}\mathbb{ P}_{S_{i}},\sum_{j}\alpha_{j}\mathbb{P}_{S_{j}}\right)\\ &\leq\sum_{j}\alpha_{j}d_{\mathcal{H}\Delta\mathcal{H}}\left( \mathbb{P}_{S_{i}},\mathbb{P}_{S_{j}}\right)\\ &\leq\max_{i,j}d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_ {S_{i}},\mathbb{P}_{S_{j}}\right)\end{split} \tag{35}\]

Combine with Eq. (34) and 35, we have

\[d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S_{i}},\mathbb{P}_{T}\right) \leq\max_{i,j}d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S_{i}}, \mathbb{P}_{S_{j}}\right)+d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{C},\mathbb{P}_{T}\right) \tag{36}\]

Substitute Eq. (36) into Eq. (33), we obtain that

\[\begin{split}\epsilon_{T}(\hat{h})&\leq\epsilon_{T} \left(h_{T}^{*}\right)+\sum\nolimits_{i=1}^{N}\alpha_{j}\left(2\lambda_{i}+ \max_{i,j}d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{S_{i}},\mathbb{P} _{S_{j}}\right)+d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{C},\mathbb{P }_{T}\right)\right)\\ &\quad+2\sqrt{\left(\sum\nolimits_{i=1}^{N}\frac{\alpha_{i}^{2}}{ \beta_{i}}\right)\left(\frac{d\log(2m)-\log(\delta)}{2m}\right)}.\end{split} \tag{37}\]

Since \(\lambda_{i}=\min_{h\in\mathcal{H}}\left\{\epsilon_{T}(h)+\epsilon_{S_{i}}(h) \right\}=\epsilon_{T}(h^{*})+\epsilon_{S_{i}}(h^{*})\), by setting \(\lambda=2\sum_{i=1}^{N}\alpha_{i}\epsilon_{S_{i}}(h^{*})\) and \(C(\delta,m,d)=2\sqrt{\left(\sum_{i=1}^{N}\frac{\alpha_{i}^{2}}{\beta_{i}} \right)\left(\frac{d\log(2m)-\log(\delta)}{2m}\right)}\) yields the proof.

#### a.2.3 Discussion

Theorem 2 provides a theoretical framework for estimating performance on a new target distribution. The first term, \(\epsilon_{T}(h_{T}^{*})\), represents the target error under the ideal hypothesis \(h_{T}^{*}\). The second term, \(\lambda=2\sum_{j=1}^{N}\alpha_{i}\epsilon_{S_{i}}(h)\), aggregates the combined error over all source distributions weighted by \(\alpha_{i}\) and can be minimized via supervised loss with labels. The third term, \(d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{\mathbb{C}},\mathbb{P}_{ \mathbb{T}}\right)\), measures the distributional discrepancy between a composite source distribution and the target distribution. The fourth term, \(\max_{i,j}d_{\mathcal{H}\Delta\mathcal{H}}(\mathbb{P}_{S_{i}},\mathbb{P}_{S_{j }})\), quantifies the maximum discrepancy between any two source distributions. The last term, \(C(\delta,m,d)\), is a statistical term which depends on the confidence level \(\delta\), sample size \(m\), and VC dimension \(d\).

The tri-level learning framework proposed in Eq. (10) aims to minimize the third term, \(d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{\mathbb{C}},\mathbb{P}_{ \mathbb{T}}\right)\), and the fourth term, \(\max_{i,j}d_{\mathcal{H}\Delta\mathcal{H}}(\mathbb{P}_{S_{i}},\mathbb{P}_{S_{j }})\). These two terms correspond to the group-level and sample-level uncertainties, respectively. Below, we discuss how these two terms align with the motivation for our tri-level optimization.

For the term \(d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{\mathbb{C}},\mathbb{P}_{ \mathbb{T}}\right)\), the goal of time series OOD generalization is to learn a model that generalizes well to unseen domain distributions, which makes direct optimization infeasible due to the unavailability of target dataset. To minimize this discrepancy, we can only enlarge the set \(\mathcal{P}_{C}\). Specifically, we manipulate \(\delta\)-perturbations applied to individual samples in the third level of our tri-level learning framework. By optimizing these perturbations, we explore a broader rangeof variations within each source domain, which potentially minimizes the term \(d_{\mathcal{H}\Delta\mathcal{H}}\left(\mathbb{P}_{\mathbb{C}},\mathbb{P}_{\mathbb{ T}}\right)\), enhancing the robustness of learned representations.

For the term \(\max_{i,j}\,d_{\mathcal{H}\Delta\mathcal{H}}(\mathbb{P}_{S_{i}},\mathbb{P}_{S_{j }})\), the second-level optimization in our tri-level framework adjusts the weights \(\alpha_{i}\) that define the mixture of source distributions \(\mathbb{P}_{\mathbb{C}}\). By dynamically modifying these weights based on the 'worst-case' distribution, we minimize the term \(\max_{i,j}\,d_{\mathcal{H}\Delta\mathcal{H}}(\mathbb{P}_{S_{i}},\mathbb{P}_{S_ {j}})\).

Our approach not only enhances representation invariance across diverse domains but also improves the model's resilience against variations within individual samples. The effectiveness of this tri-level framework is rooted in the interdependence between the problems at each level; adjustments in one level influence the conditions and outcomes of the others. This demonstrates the necessity of our tri-level learning optimization, as it requires a coordinated strategy that simultaneously considers sample-level, group-level, and parameter-level dynamics.

### Proof of Theorem 3

First, the first-order Taylor expansion of \(f_{2}(\mathbf{\theta},\mathbf{q},\mathbf{\delta})\) at the point \((\overline{\mathbf{\theta}},\overline{\mathbf{\delta}})\) is obtained as follows

\[\tilde{f}_{2}(\mathbf{\theta},\mathbf{q}^{\prime},\mathbf{\delta})=f_{2}( \overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})+\nabla_{\theta }f_{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})^{T}(\bm {\theta}-\overline{\mathbf{\theta}})+\nabla_{\delta}f_{2}(\overline{\mathbf{\delta}}, \mathbf{q}^{\prime},\overline{\mathbf{\delta}})^{T}(\mathbf{\delta}-\overline{\mathbf{\delta}}). \tag{38}\]

Since \(T_{2}=1\), therefore, we have:

\[\varphi(\mathbf{\theta},\mathbf{\delta})=\mathbf{q}^{\prime}_{0}-\eta_{q}\nabla_{q^{ \prime}}\tilde{f}_{2}\left(\mathbf{\theta},\mathbf{q}^{\prime},\mathbf{\delta}\right). \tag{39}\]

Combine with Eq. (38) and (39), we can obtain that

\[\begin{split}\varphi(\mathbf{\theta},\mathbf{\delta})&=\mathbf{ q}^{\prime}_{0}-\eta_{q}\nabla_{q^{\prime}}\Big{(}f_{2}(\overline{\mathbf{\theta}}, \mathbf{q}^{\prime},\overline{\mathbf{\delta}})+\nabla_{\theta}f_{2}(\overline{\mathbf{ \theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})(\mathbf{\theta}-\overline{\mathbf{ \theta}})+\nabla_{\delta}f_{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline {\mathbf{\delta}})(\mathbf{\delta}-\overline{\mathbf{\delta}})\Big{)}\\ &=\mathbf{q}^{\prime}_{0}-\eta_{q}\nabla_{q^{\prime}}f_{2}(\overline {\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})-\eta_{q}\nabla_{q^{ \prime}}\nabla_{\theta}f_{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline {\mathbf{\delta}})\mathbf{\theta}+\nabla_{q^{\prime}}\nabla_{\theta}f_{2}(\overline{ \mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})\overline{\mathbf{\theta}}\\ &\quad-\nabla_{q^{\prime}}\nabla_{\delta}f_{2}(\overline{\mathbf{ \theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})\mathbf{\theta}+\nabla_{q^{\prime }}\nabla_{\delta}f_{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{ \delta}})\overline{\mathbf{\delta}}\\ &=-\eta_{q}\nabla_{q^{\prime}}\nabla_{\theta}f_{2}(\overline{\mathbf{ \theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})\mathbf{\theta}-\nabla_{q^{\prime }}\nabla_{\delta}f_{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{ \delta}})\mathbf{\delta}+C,\end{split} \tag{40}\]

where \(C=\mathbf{q}^{\prime}_{0}-\eta_{q}\nabla_{q^{\prime}}f_{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})+\nabla_{q^{\prime}}\nabla_{\theta}f _{2}(\overline{\mathbf{\theta}},\mathbf{q}^{\prime},\overline{\mathbf{\delta}})\overline{ \mathbf{\theta}}+\nabla_{q^{\prime}}\nabla_{\delta}f_{2}(\overline{\mathbf{\theta}}, \mathbf{q}^{\prime},\overline{\mathbf{\delta}})\overline{\mathbf{\delta}}\) is an affine function. Therefore, \(\varphi(\mathbf{\theta},\mathbf{\delta})\) is a convex function. According to preserve convexity[Boyd and Vandenberghe, 2004], \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})=\|\mathbf{q}-\phi(\mathbf{\theta},\mathbf{\delta})\|\) is convexity, which concludes the proof of theorem 3.

### Proof of Theorem 4

Assume that in the \(t^{\text{th}}\) iteration, a new cutting plane is added, and the selected point \((\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1})\) always lies within the region \(\mathcal{R}^{c}_{t}\) formed by the cutting plane set \(\mathcal{C}^{t}\), we have

\[\mathcal{R}^{c}_{0}\supseteq\mathcal{R}^{c}_{1}\supseteq\cdots\supseteq \mathcal{R}^{c}_{t}. \tag{41}\]

Let \(\mathcal{H}^{t}\) denote the feasible region of problem in Eq. (17) at the \(t^{\text{th}}\) iteration, and \(\mathcal{R}\) represent the feasible region of problem in Eq. (15), then it follows that

\[\mathcal{H}^{0}\supseteq\cdots\supseteq\mathcal{H}^{t}\supseteq\mathcal{H}. \tag{42}\]

Let \(F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t})\) denote the optimal value of problem in Eq. (17) at the \(t^{\text{th}}\) iteration, and \(f_{1}^{*}\) represent the optimal value of the problem Eq. (15). Based on equation (42), we can obtain

\[F\left(\mathbf{\theta}^{0},\mathbf{q}^{0},\mathbf{\delta}^{0}\right)\leq F\left(\mathbf{\theta} ^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)\leq\cdots\leq F^{*}. \tag{43}\]

It's seen that the sequence \(\left\{F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t})\right\}\) is monotonically increasing. As \(T_{1}\rightarrow\infty\), \(f_{1}\) monotonically converges to a certain fixed value. It is worth mentioning that \(h(\mathbf{\theta},\mathbf{q},\mathbf{\delta})=\|\mathbf{q}-\phi(\mathbf{\theta},\mathbf{\delta})\|\) is a convex function. Since the sublevel set of a convex function is convex, the feasible region of problem in Eq. (17) is convexity. This implies that the iterative procedure, by continuously adding a cutting plane, is progressively converging to the optimal value \(f_{1}^{*}\) of the problem as referenced in Eq. (15).

### Proof of Theorem 5

To begin with, we introduce a fundamental lemma that is pivotal for the subsequent analysis.

**Lemma 1**.: _For \(\frac{1}{m}\sum_{j=1}^{m}g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}; \zeta_{j}^{t}\right),\frac{1}{m}\sum_{j=1}^{m}g_{q}(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t};\zeta_{j}^{t}),\frac{1}{m}\sum_{j=1}^{m}g_{\delta}\left(\mathbf{ \theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\), they are unbiased and bounded, that is,_

\[\begin{array}{l}\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left[\frac{1}{m} \sum_{j=1}^{m}g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_ {j}^{t}\right)\right]=\nabla_{\theta}f_{1}\left(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t}\right),\\ \mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left[\left\|\frac{1}{m}\sum_{j=1}^{m }g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right) \right\|^{2}\right]\leq\alpha_{1}^{2}+\frac{\sigma_{1}^{2}}{m},\end{array} \tag{44}\]

\[\begin{array}{l}\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left[\frac{1}{m} \sum_{j=1}^{m}g_{q}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{ t}\right)\right]=\nabla_{\theta}f_{1}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t }\right),\\ \mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left[\left\|\frac{1}{m}\sum_{j=1}^{m }g_{\delta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right) \right\|^{2}\right]\leq\alpha_{2}^{2}+\frac{\sigma_{2}^{2}}{m}.\end{array} \tag{45}\]

Here, \(\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}[\cdot]\) denotes the expectation with respect to a set of variables \(\{\zeta_{1}^{t},\cdots,\zeta_{m}^{t}\}\).

#### a.5.1 Proof of Lemma 1

Taking the variable \(\mathbf{\theta}\) as an example, according to Assumption 4, for all \(i=1,\cdots,n\), we have

\[\begin{array}{l}\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left[\frac{1}{m} \sum_{j=1}^{m}g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta _{j}^{t}\right)\right]=\frac{1}{m}\sum_{j=1}^{m}\mathbb{E}_{\zeta_{j}^{t}}\left[ g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right) \right]=\nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right). \end{array} \tag{47}\]

Based on Assumption 5, the variance of \(\nabla_{\theta}F(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t})\) is bounded, from which we can deduce

\[\begin{array}{l}\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left[\frac{1}{m} \sum_{j=1}^{m}g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta _{j}^{t}\right)\right]\\ =\mathbb{E}_{\left\{\zeta_{j}\right\}}\left[\left\|\frac{1}{m}\sum_{j=1}^{m}g_ {\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right) \right\|^{2}\right]+\left\|\nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t}\right)\right\|^{2}\right]\\ =\frac{\sum_{j=1}^{m}\mathbb{E}_{\zeta_{j}^{t}}\left[\left\|g_{\theta}(\mathbf{ \theta},\mathbf{q},\mathbf{\delta};\zeta_{j})-\nabla_{\theta}F(\mathbf{\theta},\mathbf{q},\mathbf{ \delta})\right\|^{2}\right]}{m^{2}}+\alpha_{1}^{2}\\ \leq\frac{\sigma_{1}^{2}}{m}+\alpha_{1}^{2}.\end{array} \tag{48}\]

The proofs of Eq. (45) and Eq. (46) follow a similar logic. Thus, we complete the proof of Lemma 1.

Combine with lemma 1, we now proceed to derive Theorem 5. Under Assumption 3 that the gradient of \(F\) is Lipschitz continuous, for \(t>t_{1}\), it follows that

\[\begin{array}{l}F\left(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1}\right) \\ \leq F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)+\left\langle \nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right),\mathbf{ \theta}^{t+1}-\mathbf{\theta}^{t}\right\rangle+\left\langle\nabla_{\theta}F \left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right),\mathbf{q}^{t+1}-\mathbf{q}^{t }\right\rangle\\ \quad+\left\langle\nabla_{\delta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta} ^{t}\right),\mathbf{\delta}^{t+1}-\mathbf{\delta}^{t}\right\rangle+\frac{L}{2}\left( \left\|\mathbf{\theta}^{t+1}-\mathbf{\theta}^{t}\right\|^{2}+\left\|\mathbf{q}^{t+1}-\mathbf{q }^{t}\right\|^{2}+\left\|\mathbf{\delta}^{t+1}-\mathbf{\delta}^{t}\right\|^{2}\right) \\ =F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)-\eta_{\theta}\left\langle \nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right),\frac{1 }{m}\sum_{j=1}^{m}g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}; \zeta_{j}^{t}\right)\right\rangle\\ \quad-\eta_{q}\left\langle\nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{ \delta}^{t}\right),\frac{1}{m}\sum_{j=1}^{m}g_{q}\left(\mathbf{\theta}^{t},\mathbf{q}^{t },\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\rangle\\ \quad-\eta_{\delta}\left\langle\nabla_{\delta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t}, \mathbf{\delta}^{t}\right),\frac{1}{m}\sum_{j=1}^{m}g_{\delta}\left(\mathbf{\theta}^{t}, \mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\rangle\\ \quad+\frac{L(\eta_{\theta})^{2}}{2}\left\|\frac{1}{m}\sum_{j=1}^{m}g_{\theta} \left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\|^{2} \\ \quad+\frac{L(\eta_{\theta})^{2}}{2}\left\|\frac{1}{m}

Taking the expectation of both sides of Equation (49) with respect to \(\{\zeta_{1}^{t},\cdots,\zeta_{m}^{t}\}\), we can obtain

\[\mathbb{E}_{\left\{\zeta_{i}^{t}\right\}}\left[F\left(\mathbf{\theta}^{t +1},\mathbf{q}^{t+1},\mathbf{\delta}^{t+1}\right)\right]\] \[\leq F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)-\eta _{\theta}\mathbb{E}_{\left\{\zeta_{i}^{t}\right\}}\left\langle\nabla_{\theta}F \left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right),\frac{1}{m}\sum_{j=1}^{m }g_{\theta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\rangle\] \[\quad-\eta_{\theta}\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left \langle\nabla_{q}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right), \frac{1}{m}\sum_{j=1}^{m}g_{q}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t} ;\zeta_{j}^{t}\right)\right\rangle\] \[\quad-\eta_{\theta}\mathbb{E}_{\left\{\zeta_{j}^{t}\right\}}\left \langle\nabla_{\delta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right),\frac{1}{m}\sum_{j=1}^{m}g_{\delta}\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{ \delta}^{t};\zeta_{j}^{t}\right)\right\rangle\] \[\quad+\frac{L\left(\eta_{\theta}\right)^{2}}{2}\mathbb{E}_{\left\{ \zeta_{j}^{t}\right\}}\left\|\frac{1}{m}\sum_{j=1}^{m}g_{\theta}\left(\mathbf{ \theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\|^{2}\] \[\quad+\frac{L\left(\eta_{\theta}\right)^{2}}{2}\mathbb{E}_{\left\{ \zeta_{j}^{t}\right\}}\left\|\frac{1}{m}\sum_{j=1}^{m}g_{q}\left(\mathbf{\theta}^{ t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\|^{2}\] \[\quad+\frac{L\left(\eta_{\theta}\right)^{2}}{2}\mathbb{E}_{\left\{ \zeta_{j}^{t}\right\}}\left\|\frac{1}{m}\sum_{j=1}^{m}g_{\theta}\left(\mathbf{ \theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t};\zeta_{j}^{t}\right)\right\|^{2}\] \[\overset{(i)}{\leq}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^ {t}\right)-\left\|\nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^ {t}\right)\right\|^{2}\eta_{\theta}-\left\|\nabla_{q}F\left(\mathbf{\theta}^{t}, \mathbf{q}^{t},\mathbf{\delta}^{t}\right)\right\|^{2}\eta_{\theta}-\left\|\nabla_{ \delta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)\right\|^{2} \eta_{\delta}\] \[\quad+\frac{L\left(\eta_{\theta}\right)^{2}}{2}\left(\alpha_{1}^{ 2}+\frac{\sigma_{1}^{2}}{m}\right)+\frac{L\left(\eta_{\theta}\right)^{2}}{2} \left(\alpha_{2}^{2}+\frac{\sigma_{2}^{2}}{m}\right)+\frac{L\left(\eta_{\delta }\right)^{2}}{2}\left(\alpha_{3}^{2}+\frac{\sigma_{3}^{2}}{m}\right). \tag{50}\]

The inequality (i) is based on lemma 1. Taking the total expectation of both sides of Eq. (50), we have

\[\mathbb{E}\left[F\left(\mathbf{\theta}^{t+1},\mathbf{q}^{t+1},\mathbf{\delta}^ {t+1}\right)\right]\] \[\leq\mathbb{E}\left[F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{ \delta}^{t}\right)\right]-\eta_{\theta}\mathbb{E}\left[\left\|\nabla_{\theta}F \left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)\right\|^{2}\right]- \eta_{\theta}\mathbb{E}\left[\left\|\nabla_{q}F\left(\mathbf{\theta}^{t},\mathbf{q}^ {t},\mathbf{\delta}^{t}\right)\right\|^{2}\right]\] \[\quad-\eta_{\theta}\mathbb{E}\left[\left\|\nabla_{\delta}F\left( \mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)\right\|^{2}\right]+\frac{L \left(\eta_{\theta}\right)^{2}}{2}\left(\alpha_{1}^{2}+\frac{\sigma^{2}}{m} \right)+\frac{L\left(\eta_{\theta}\right)^{2}}{2}\left(\alpha_{2}^{2}+\frac{ \sigma^{2}}{m}\right) \tag{51}\] \[\quad+\frac{L\left(\eta_{\theta}\right)^{2}}{2}\left(\alpha_{3}^{ 2}+\frac{\sigma^{2}}{m}\right),\]

where \(\mathbb{E}[\cdot]\) denotes the expectation over all terms. Summing Eq. (51) from \(t=t_{1}\) to \(t=T_{1}-1\), we obtain

\[\mathbb{E}\left[F\left(\mathbf{\theta}^{T_{1}},\mathbf{q}^{T_{1}},\mathbf{ \delta}^{T_{1}}\right)\right]\] \[\leq\mathbb{E}\left[F\left(\mathbf{\theta}^{t_{1}},\mathbf{q}^{t_{1}}, \mathbf{\delta}^{t}\right)\right]-\eta_{\theta}\sum_{t=t_{1}}^{T_{1}-1}\mathbb{E} \left[\left\|\nabla_{\theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t} \right)\right\|^{2}\right]-\eta_{\theta}\sum_{t=T_{1}}^{T_{1}-1}\mathbb{E}\left[ \left\|\nabla_{q}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right) \right\|^{2}\right]\] \[\quad-\eta_{\theta}\sum_{t=t_{1}}^{T_{1}-1}\mathbb{E}\left[\left\| \nabla_{\delta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right) \right\|^{2}\right]+\sum_{t=t_{1}}^{T_{1}-1}\frac{L\left(\eta_{\theta}\right)^ {2}}{2}\left(\alpha_{1}^{2}+\frac{\sigma_{1}^{2}}{m}\right)\] \[\quad+\sum_{t=t_{1}}^{T_{1}-1}\frac{L\left(\eta_{\theta}\right)^{2 }}{2}\left(\alpha_{2}^{2}+\frac{\sigma_{2}^{2}}{m}\right)+\sum_{t=t_{1}}^{T_{1 }-1}\frac{L\left(\eta_{\theta}\right)^{2}}{2}\left(\alpha_{3}^{2}+\frac{\sigma_{ 3}^{2}}{m}\right)\] \[=\] \[-\eta_{\theta}\sum_{t=t_{1}}^{T_{1}-1}\mathbb{E}\left[\left\|\nabla_{ \theta}F\left(\mathbf{\theta}^{t},\mathbf{q}^{t},\mathbf{\delta}^{t}\right)\right\|^{2} \right]+\frac{L}{2}\sum_{t=t_{1}}^{T_{1}-1}(\eta_{\theta}^{2}\alpha_{1}^{2}+ \eta_{\theta}^{2}\alpha_{2}^{2}+\eta_{\theta}^{2}\alpha_{3}^{2})\] \[\quad+\frac{L}{2m}\sum_{t=t_{1}}^{T_{1}-1}(\eta_{\theta}^{2}\sigma _{1}^{2}+\eta_{\theta}^{2}\sigma_{2}^{2}+\eta_{\delta}^{2}\sigma_{3}^{2})\] (Combining the definition of \(\epsilon\)-stationary point described in Definition (3) with Eq. (53), we have

\[\sum_{t=t_{1}}^{T_{1}-1}\mathbb{E}\left[\left\|\nabla_{\theta}F\left( \boldsymbol{\theta}^{t},\boldsymbol{q}^{t},\boldsymbol{\delta}^{t}\right) \right\|^{2}+\left\|\nabla_{q}F\left(\boldsymbol{\theta}^{t},\boldsymbol{q}^{t },\boldsymbol{\delta}^{t}\right)\right\|^{2}+\left\|\nabla_{\delta}F\left( \boldsymbol{\theta}^{t},\boldsymbol{q}^{t},\boldsymbol{\delta}^{t}\right)\right\| ^{2}\right] \tag{54}\] \[\leq F\left(\boldsymbol{\theta}^{T_{1}},\boldsymbol{q}^{T_{1}}, \boldsymbol{\delta}^{T_{1}}\right)-f_{1}^{*}+\frac{L}{2}(T_{1}(\epsilon)-t_{1 })^{-\frac{1}{2}}\sum_{t=t_{1}}^{T_{1}-1}\sum_{i=1}^{3}\alpha_{i}^{2}\] \[\quad+\frac{L}{2m}(T_{1}(\epsilon)-t_{1})^{-\frac{1}{2}}\sum_{t=t _{1}}^{T_{1}-1}\sum_{i=1}^{3}\sigma_{i}^{2}\] \[\leq\epsilon,\]

that is,

\[T_{1}(\epsilon)\sim t_{1}+\frac{L^{2}(m(\alpha_{1}^{2}+\alpha_{2}^{2}+\alpha_{ 3}^{2})+\sigma_{1}^{2}+\sigma_{2}^{2}+\sigma_{3}^{2})^{2}}{4m^{2}(\epsilon-F( \boldsymbol{\theta}^{T_{1}},\boldsymbol{q}^{T_{1}},\boldsymbol{\delta}^{T_{1}} )+F^{*})^{2}}. \tag{55}\]

According to Eq. (55), we can obtain

\[T_{1}(\epsilon)\sim\frac{1}{\epsilon^{2}} \tag{56}\]

Hence, it can be concluded that there exists a \(T_{1}(\epsilon)\) such that \(\left\|\nabla G^{t}\right\|=\mathbb{E}\left[\left\|\nabla_{\theta}F\left( \boldsymbol{\theta}^{t},\boldsymbol{q}^{t},\boldsymbol{\delta}^{t}\right) \right\|^{2}+\left\|\nabla_{q}F\left(\boldsymbol{\theta}^{t},\boldsymbol{q}^{t },\boldsymbol{\delta}^{t}\right)\right\|^{2}+\left\|\nabla_{\delta}F\left( \boldsymbol{\theta}^{t},\boldsymbol{q}^{t},\boldsymbol{\delta}^{t}\right) \right\|^{2}\right]\leq\epsilon\), which concludes the proof of Theorem 5.

## Appendix B Datasets

### Datasets Information

**HHAR[Blunck et al., 2015]** collected activity data from 9 subjects engaging in 6 activities, using smartphones that capture 3D accelerometer data from various positions. **PAMAP[Reiss, 2012]** encompasses 18 physical activities data from 9 subjects, recorded using wearable sensors monitoring physiological and movement metrics. **WESAD[Philip Schmidt et al., 2018]** focuses on stress and affective state detection from 15 subjects, employing wearable sensors for ECG, EMG, respiration, and temperature under varied conditions. **SWELL[Koldijk et al., 2014]** recorded stress responses in a work environment from 25 participants using ECG, EDA, and heart rate sensors during typical office tasks under stressors. **USC-HAD[Zhang and Sawchuk, 2012]** comprises detailed motion and orientation data from 14 subjects performing various activities, captured via a MotionNode device with high sampling rate. **DSADS[Barshan and Altun, 2013]** consists of 19 activities recorded from 8 subjects at Bilkent University, using body-worn sensors on torso and limbs, with data segmented for detailed analysis. Table 2 shows the information of the 6 datasets we used in our experiments.

### Domain Setting

The domain setting is summarized in Table 3. This setting is done to balance the number of samples and classed across different domains.

### Data pre-processing

For all datasets, we configure the window size as 128 and the step size as 64, resulting in a 50% overlap between two adjacent time series samples. Each sample is standardized using the formula \(\tilde{x}=\frac{\pi-\mu}{\sigma}\), where \(\mu\) and \(\sigma\) represent the mean and standard deviation of the dataset, respectively. It's

\begin{table}
\begin{tabular}{l c c c c} \hline Dataset & Classes & Dimension & subjects & samples \\ \hline HHAR & 6 & 3 & 4 & 73420 \\ PAMAP & 13 & 40 & 9 & 31984 \\ WESAD & 4 & 8 & 4 & 63180 \\ SWELL & 2 & 3 & 16 & 52944 \\ USC-HAD & 12 & 3 & 4 & 40244 \\ DSADS & 19 & 45 & 4 & 17520 \\ \hline \end{tabular}
\end{table}
Table 2: Dataset information.

important to note that the \(\mu\) and \(\sigma\) used here is specific to each domain, rather than the entire dataset. This approach is intended to maximize the distinction between data from different domains.

## Appendix C Network Architecture and Hyperparameters

Our baseline experiments were conducted using a network architecture consisting of 10-layers dilated convolutions network. The dilation rate for each layer is set to \(2^{k}\), where \(k\) is the layer number. We used the same kernel size of 3 across all layers. Optimization was performed using the Adam optimizer with a weight decay of \(3\times 10^{-4}\). For all baseline experiments, we set the batch size to 256 and the learning rate to 0.002. The training was set to run for a maximum of 50 epochs. All the methods are implemented with PyTorch[20] version 1.7.1 on an NVIDIA GeForce RTX 4090 graphics card.

In the TTSO fine-tuning experiments, we employed GPT-2 as the language model. Fine-tuning was performed in two stage: In the first stage, the learning rate for the large model was \(1\times 10^{-4}\), and for the input embedding, it was set at 0.001. In the second phase, we adjusted the learning rate for the large model to \(5\times 10^{-5}\), aiming to further refine the model's performance. During the evaluation phase, we froze the parameters of the language model, only fine-tuning the classifier with a learning rate of 0.003 to adapt to the specific classification tasks. The batch size was consistently set at 16 for all experimental stages.

## Appendix D Details of Fine-tuning

The structure of LLM Fine-tuning with TTSO is illustrated in figure 2(a) and 2(b). The primary components involved are as follows:

**Input Embedding**: The first is the input embedding, where raw time series is transformed into embedding space that is amenable for processing by the language model. In our experiments, the input embedding is a linear layer. As shown in Figure 3, the time series (indicated by the dashed box below the waveform) is combined with positional encoding to form the input representation.

**Language Model**: This is the core part of the model, typically comprising multiple pretrained transformer encoder. This model is used for processing the input embeddings and producing advanced

Figure 3: Structure of LLM Fine-tuning with TTSO, illustrating the two-phase approach starting with alignment fine-tuning followed by downstream fine-tuning, adapted specifically for time series out-of-distribution generalization tasks.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline Dataset & Domain A & Domain B & Domain C & Domain D \\ \hline HHAR & 1 & 2 & 3 & 4 \\ PAMAP & 1 & 2 & 3 & 4 \\ WESAD & 1,2 & 3,5 & 4,6,9 & 7,8 \\ SWELL & 1,2,4,5 & 6,7,9,10 & 12,13,14,16 & 17,18,21,24 \\ USC-HAD & 2,3,4 & 4,5,6 & 1,7,9,10 & 11,12,13,14 \\ DSADS & 1,2 & 3,4 & 5,6 & 7,8 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Domain setting for HHAR, PAMAP and WESAD dataset.

[MISSING_PAGE_FAIL:22]

Figure 5 demonstrates the group-level uncertainty by displaying the distribution of x-axis values from the accelerometer across different groups (users). Each color represents a distinct group, and each group's unique characteristics contribute to the overall group-level uncertainty.

### Ablation Study on LLM Architectures and Parameter Configurations

To further investigate the impact of various architectures and parameter settings of LLMs, we conducted additional ablation experiments that focused on different LLM architectures and parameter sizes (e.g., base model and large model). These experiments included encoder-only models (e.g., BERT), decoder-only models (e.g., GPT-2), and encoder-decoder models (e.g., BART) to determine which configurations yield the greatest benefits during fine-tuning. The results are summarized in the table 5.

The results indicate that decoder-only architectures, specifically the GPT-2 base model in this experiments, achieve the best performance. However, increasing the number of parameters in all three architectures leads to a significant drop in performance across 3 architectures for time series OOD generalization.

To further explore how the number of parameters affects performance, we conducted experiments using GPT-2 models with varying numbers of Transformer layers on 3 datasets to evaluate their OOD

Figure 5: Group-Level Uncertainty: Histogram of x axis values, with each color representing a different group.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Architecture** & **Version** & **HHAR** & **PAMAP** & **WESAD** & **AVG** \\ \hline \multirow{2}{*}{**Encoder-Only (BERT)**} & Base & 64.3 & 66.9 & 64.4 & 64.2 \\  & Large & 61.7 & 52.5 & 62.3 & 58.8 \\ \hline \multirow{2}{*}{**Decoder-Only (GPT)**} & Base & 72.9 & 76.1 & 68.4 & 72.5 \\  & Large & 64.5 & 69.4 & 66.5 & 66.8 \\ \hline \multirow{2}{*}{**Encoder-Decoder (BART)**} & Base & 57.3 & 65.4 & 64.2 & 62.3 \\  & Large & 55.5 & 61.2 & 61.4 & 59.4 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Performance comparison of different LLM architectures and parameter sizes across datasets.

Figure 6: The effect of varying the number of Transformer layers (k) on average accuracy for OOD generalization across HHAR, PAMAP, and WESAD datasets.

generalization performance. For these experiments, we utilized 20% of each dataset. As shown in Figure 3 (in the attached PDF), the results demonstrate that optimal OOD generalization performance is achieved with a configuration of 8 Transformer layers.

Based on this findings, we incorporate this optimal layer configuration with TTSO framework, yielding improved results as detailed in the table 6.

## Appendix F Limitation

TTSO is a general framework for learning invariant representations across diverse domain distributions, currently discussed only for time series classification. This framework could be further enhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly detection. Additionally, distribution shifts occur not only in time series but also in other machine learning domains like images (Deecke et al., 2021) and text (Tan et al., 2022). Applying our approach to these domains could further improve performance.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction, we clearly state the main contributions of our work. The tri-level learning framework, stratified localization algorithm and iteration complexity analysis is in Section 3.2, 3.3 and 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of this work can be found in Appendix F. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

\begin{table}
\begin{tabular}{l c c c c c} \hline
**Target** & **A** & **B** & **C** & **D** & **AVG** \\ \hline HHAR & +1.4 & +0.1 & +0.8 & +0.9 & +0.80 \\ \hline PAMAP & +1.0 & -1.4 & +0.8 & -0.3 & +0.03 \\ \hline WESAD & +5.5 & -3.5 & +2.5 & -0.1 & +1.35 \\ \hline \end{tabular}
\end{table}
Table 6: Performance improvements on different domains for HHAR, PAMAP, and WESAD datasets.

* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: In this paper, all theoretical results are accompanied by a full set of assumptions and complete proofs. These are clearly stated and numbered within the main text and are cross-referenced appropriately. Detailed proofs for major theorems are provided in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our paper provides detailed descriptions of the experimental setup. Detailed information regarding datasets, domain setting, data pre-processing, network architecture and hyperparameters can be found in Appendix B.1, B.2, B.3 and C. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: While the data used in our study is publicly available, we are currently unable to provide open access to the code. However, we have included detailed descriptions of the data access, preprocessing steps, model architecture, and experimental setup in the paper and Appendix. These details should be sufficient for others to reproduce the experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. *6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Our paper provides detailed descriptions of the experimental details. Detailed information regarding datasets, domain setting, data pre-processing, network architecture and hyperparameters can be found in Appendix B.1, B.2, B.3 and C. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the mean accuracy and standard deviation across three runs with different random seeds. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide the type of compute workers (GPU) in the Appendix C Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. ** The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our research adheres to all guidelines outlined in the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing experiments or research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing experiments or research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.