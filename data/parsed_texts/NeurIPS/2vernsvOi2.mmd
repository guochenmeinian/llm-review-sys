# Towards out-of-distribution generalizable predictions

of chemical kinetic properties

 Zihao Wang

CSE, HKUST

zwanggc@cse.ust.hk

&Yongqiang Chen

CSE, CUHK

yqchen@cse.cuhk.edu.hk

&Yang Duan, Weijiang Li

CS, UIUC

{yangd4,w113}@illinois.edu

&Bo Han

CS, HKBU

bhanml@comp.hkbu.edu.hk

&James Cheng

CSE, CUHK

jcheng@cse.cuhk.edu.hk

&Hanghang Tong

CS, UIUC

htong@illinois.edu

Equal contribution.

###### Abstract

Machine Learning (ML) techniques have found applications in estimating chemical kinetic properties. With the accumulated drug molecules identified through "AI4drug discovery", the next imperative lies in AI-driven design for high-throughput chemical synthesis processes, with the estimation of properties of unseen reactions with unexplored molecules. To this end, the existing ML approaches for kinetics property prediction are required to be Out-Of-Distribution (OOD) generalizable. In this paper, we categorize the OOD kinetic property prediction into three levels (structure, condition, and mechanism), revealing unique aspects of such problems. Under this framework, we create comprehensive datasets to benchmark (1) the state-of-the-art ML approaches for reaction prediction in the OOD setting and (2) the state-of-the-art graph OOD methods in kinetics property prediction problems. Our results demonstrated the challenges and opportunities in OOD kinetics property prediction. Our datasets and benchmarks can further support research in this direction. The github repository for code and data can be found in https://github.com/zihao-wang/ReactionOOD.

## 1 Introduction

In recent years, graph machine learning has been widely used in scientific discovery (Wang et al., 2023; Zhang et al., 2023) and gained particular success in chemistry (Gilmer et al., 2017; Jumper et al., 2021; Mullowney et al., 2023). The underlying rationale is the long-standing structure-property relationship (Mihalic and Trinajstic, 1992) in chemistry. For example, Graph Neural Networks (GNNs) can efficiently encode information at both the molecular structure level and the atom level within a molecule, which reveal compelling properties of the molecule (Gilmer et al., 2017). Such methods yield efficient, cheap, but still effective predictions of the properties of _unseen molecules_ before expensive experiments or computations, which can serve as valuable reference information for drug discovery (Mullowney et al., 2023).

One of the _next_ questions to be answered after the discovery of a proper but unseen molecule is _How to efficiently obtain unseen molecules through chemical synthesis_. In contrast to the molecule property estimation problem that concerns a _single_ molecule, chemical synthesis processes involve the proper arrangement of various reactions that encompass _multiple_ molecules under optimal conditions. Therefore, the very first step of achieving a high-throughput synthesis of unseen molecules is to estimate the properties of chemical reactions (Warr, 2014), especially kinetics properties thatdescribe the "rate" of reactions. This prediction task is expected to be Out-Of-Distribution (OOD) generalizable so that the kinetic properties of OOD reactions with unseen molecules can be well predicted.

Recently, machine learning methods have been applied to predict the kinetic properties of reactions [Heid and Green, 2021]. In existing studies, chemical reactions are assumed to be Independently and Identically Distributed (IID), and models are trained and tested within random splits [Heid and Green, 2021, Stuyver and Coley, 2022, Heid et al., 2023]. However, results from such IID assumptions provide little credible insight into the performances of existing ML methods in OOD reaction property prediction. Meanwhile, existing theoretical and empirical studies for OOD generalization on graphs [Ji et al., 2022, Gui et al., 2022], are restricted to problems with a single graph. How OOD methods perform reaction properties prediction with multiple molecules is still unknown.

To fill this gap, this paper discusses the out-of-distribution generalization issue when applying machine learning methods to the prediction of chemical reaction properties. We propose three levels of OOD shifts for ML-based reaction prediction: Structure OOD, Mechanism OOD, and Conditional OOD. Then, we reorganize recent reaction kinetic databases [Johnson et al., 2022] and create a comprehensive dataset in three levels of OOD. Furthermore, we empirically justify the performance of state-of-the-art kinetic property prediction produced by state-of-the-art OOD methods for general and graph inputs. Our results demonstrated that there remain huge ID-OOD performance gaps under different distribution shifts in chemical reactions for existing OOD methods.

## 2 Related works

Increasing efforts have been made to devise machine learning approaches for various aspects of chemical reaction systems [Davies, 2019, Stocker et al., 2020, Meuwly, 2021, Strieth-Kalthoff et al., 2022], such as reaction classification [Schwaller et al., 2021, Bures and Larrosa, 2023], reaction optimization [Felton et al., 2021], atom mapping [Schwaller et al., 2021], and the most fundamentally, reaction property prediction [Heid and Green, 2021]. With the burst of chemical reaction data [von Rudorff et al., 2020, Spiekermann et al., 2022, Johnson et al., 2022, Choi, 2023, Stuyver et al., 2023, Zhao et al., 2023], the Graph Neural Network (GNN) based methods [Heid and Green, 2021, Stuyver and Coley, 2022, Heid et al., 2023] are demonstrated its clear advantage over traditional methods by leveraging the structure of reactants and products.

Out-of-distribution shift is one of the long-standing problems in machine learning [Vapnik, 1991, Quinonero-Candela et al., 2008, Shen et al., 2021]. Recently, OOD generalizable graph neural networks have been discussed extensively [Bevilacqua et al., 2021, Zhu et al., 2021, Wu et al., 2022b, a, Chen et al., 2022]. When it comes to scientific discovery, out-of-distribution generalization capabilities enable machine learning methods to find more reliable discoveries from existing observations. A thorough investigation of the intersection of OOD and drug discovery can be found at Ji et al. [2022] and Gui et al. [2022]. However, as we will identify in the incoming parts, the out-of-distribution shifts for chemical reactions are radically different from those with existing graph OOD settings [Gui et al., 2022], and existing OOD methods do not perform well.

## 3 Preliminary

### Chemical reactions and kinetic property prediction

A chemical reaction \(\mathfrak{R}\) is described by the reactants \(r_{1},\ldots,r_{m}\), products \(p_{1},\ldots,p_{n}\), and the conditions \(c_{1},\ldots,c_{l}\).2

Footnote 2: In chemistry literature, different arrow types indicate different reaction types. We use \(\Longrightarrow\) for simplicity.

\[r_{1}+\cdots+r_{m}\xrightarrow[]{c_{1},\ldots,c_{l}}p_{1}+\cdots+p_{n}.\] (1)

Multiple molecules (including atoms, ions, and other species) are involved in one reaction, which differs from molecule property prediction tasks where only one molecule structure is considered.3Each molecule \(r_{i}\), \(p_{j}\) is considered as a molecule graph, with atoms as attributed nodes and bonds as attributed edges. For a chemical reaction \(\mathfrak{R}\), \(R[\mathfrak{R}]\)\((P[\mathfrak{R}])\) denotes the graph of reactants (products) as the union of the reactant (product) molecule graphs. \(C[\mathfrak{R}]\) denotes the set of conditions. \(\mathcal{G}_{R}\) (\(\mathcal{G}_{P}\)) and \(\mathcal{C}\) denote the space of graphs for reactants (products) and the space of conditions, respectively.

Kinetics properties investigated in existing literature include _reaction barrier_ (activation energy) [von Rudorff et al., 2020, Spiekermann et al., 2022, Stuyver et al., 2023, Zhao et al., 2023] and _rate constants_[Heid and Green, 2021]. As one of the simplest mathematical descriptions of the rate constant, the Arrhenius equation reads

\[k=A\exp\left(\frac{-E_{a}}{RT}\right),\] (2)

where \(k\) is the rate constant, which is modeled by a function of the Arrhenius factor \(A\), the reaction barrier (activation energy) \(E_{a}\), the universal gas constant \(R\), and the absolute temperate \(T\). Though Equation (2) only holds under mild conditions for some reactions, we could still summarize that, the rate constant \(k\) is jointly described from three aspects: (1) the inherent property of the reaction, such as the barrier \(E_{a}\)[Donahue, 2003], (2) the conditions, such as the temperature \(T\), and (3) other parameters \(A\) to fit the empirical data.

In this paper, we also consider the reaction barrier \(E_{a}\) and the rate constant \(k\) as the target of prediction. Let \(\mathcal{Y}\) be the space of the target property. The target \(Y\) of reaction \(\mathfrak{R}\) is computed by the learnable function \(f_{\theta}:\mathcal{X}\mapsto\mathcal{Y}\), where \(\mathcal{X}=\mathcal{G}_{R}\times\mathcal{G}_{P}\times\mathcal{C}\) is the input space.

### OOD shifts

Out-of-distribution generalization is one of the key topics in machine learning research. It considers the shifts of the joint distribution \(P(X,Y)\) over \(\mathcal{X}\times\mathcal{Y}\) during training and testing phase, \(P^{\mathrm{train}}(X,Y)\neq P^{\mathrm{test}}(X,Y)\)[Gui et al., 2022]. Applying the conditional probability formula,

\[\underbrace{P^{\mathrm{train}}(Y|X)}_{\mathrm{Concept}}\underbrace{P^{ \mathrm{train}}(X)}_{\mathrm{Covariate}}\neq\underbrace{P^{\mathrm{test}}(Y| X)}_{\mathrm{Concept}}\underbrace{P^{\mathrm{test}}(X)}_{\mathrm{Covariate}}.\] (3)

This decomposition introduces two basic types of OOD shifts: covariate shift and concept shift, where one part of the distribution is changed while the other part is fixed.

One common way to characterize distribution shifts in high-dimensional input space is to properly separate the spaces into domains. Let \(\mathcal{D}\) be the set of domains and \(\mathcal{X}\) be the input space. For each input \(X\), we define its domain index \([X]\). Table 1 summarizes the covariate shift and concept shift with the domain index.

Throughout this paper, the domain index \([X]\) is equivalent to an equivalent relation \(\sim\). Specifically, for \(X^{\prime},X\in\mathcal{X}\), \([X^{\prime}]=[X]\) if and only if \(X^{\prime}\sim X\). Then, it suffices to define **the equivalent relation** over the input space, which will naturally lead to the definition of the domain index and then two kinds of OOD shifts.

## 4 Three levels of OOD shifts in kinetic property prediction

This section defines three levels of OOD settings and details the OOD domains by constructing the **equivalent relations** in the input space \(\mathcal{X}=\mathcal{G}_{R}\times\mathcal{G}_{P}\times\mathcal{C}\). The definition of equivalent classes, so as the OOD shifts, can be decomposed into the subspaces of \(\mathcal{X}\). Structure OOD (S-OOD) establishes the equivalent relation in the subspace \(\mathcal{G}_{R}\times\mathcal{G}_{P}\) while Condition OOD (C-OOD) concerns the equivalent relation in the subspace \(\mathcal{C}\). Mechanism OOD (M-OOD) considers the shifts in general chemical reaction space \(\mathcal{X}\) with the expertise of chemists. Figure 1 indicates the different levels of out-of-distribution generalization.

\begin{table}
\begin{tabular}{c c c} \hline Distribution shifts & \(P([X])\) & \(P(Y|[X])\) \\ \hline Covariate shift & \(P^{\mathrm{train}}([X])\neq P^{\mathrm{test}}([X])\) & \(P^{\mathrm{train}}(Y|[X])=P^{\mathrm{test}}(Y|[X])\) \\ Concept shift & \(P^{\mathrm{train}}([X])=P^{\mathrm{test}}([X])\) & \(P^{\mathrm{train}}(Y|[X])\neq P^{\mathrm{test}}(Y|[X])\) \\ \hline \end{tabular}
\end{table}
Table 1: Two types of distribution shifts. \([X]\) denotes the domain index of the input \(X\).

### Structure OOD (S-OOD)

The first level of OOD settings is about the graphs for reactants and products, which are directly related to the graph machine learning methods such as graph neural networks.

**Definition 1** (Structure OOD).: _Let \(\sim\) be an equivalent relation in \(\mathcal{G}_{R}\times\mathcal{G}_{P}\) and \([X]\) be the equivalent class containing \(X\) induced by \(\sim\). We say \(\sim\) defines a structure OOD setting._

By instantiating the equivalent relation \(\sim\), we can easily define several OOD settings. We begin with the basic cardinality \(|G|\) of graph \(G\).

**Example 1** (S-OOD by total atom number).: _Consider the equivalent relation \(\sim\), such that \(R_{1}\times P_{1}\sim R_{2}\times P_{2}\) if and only if \(|R_{1}|=|R_{2}|\), i.e., the total number of atoms in two reactions are equal. Then the induced S-OOD setting is about the atom number of the largest reactant._

Besides, the size of each reactant molecule also plays an important role. For example, the number of carbon atoms in an organic compound is related to its phase status in the ambient temperature and pressure, thus affecting the chemical reactions. Inspired by this, we can also define another type of S-OOD.

**Example 2** (S-OOD by the atom number of the largest reactant).: _Consider the equivalent relation \(\sim\), such that \(R_{1}\times P_{1}\sim R_{2}\times P_{2}\) if and only if \(\max_{r_{i}\in R_{1}}|r_{i}|=\max_{r_{j}\in R_{2}}|r_{j}|\), i.e., the total number of atoms in two reactions are equal. Then the induced S-OOD setting is about the total atom number._

Furthermore, it is also possible to define the S-OOD by considering the scaffold of molecules.

**Example 3** (Structure OOD by the scaffold of the first reactant).: _Let \(\sim_{S}\) be the equivalent relation for two molecular graphs with the same scaffold. Consider the equivalent relation \(\sim\) such that \(R_{1}\times P_{1}\sim R_{2}\times P_{2}\) if and only if \(r_{11}\sim_{S}r_{21}\) for \(r_{11}\in R_{1}\) and \(r_{21}\in R_{2}\). Then the induced OOD setting is about the first reactant scaffold._

One can easily induce a more complex S-OOD setting by using the scaffold equivalence.

**Example 4** (Structure OOD by the mutual scaffold of all reactants).: _Let \(\sim_{S}\) be the equivalent relation for two molecular graphs with the same scaffold. Consider the equivalent relation \(\sim\) such that \(R_{1}\times P_{1}\sim R_{2}\times P_{2}\) if and only if \(\exists r_{i}\in R_{1},r_{j}\in R_{2},r_{i}\sim_{S}r_{j}\). Then the induced OOD setting is about the reactant scaffold._

We could see that basic properties of molecules such as atom number and scaffold can define various types of S-OOD for chemical reactions, which is more complex than the situation for single molecule property predictions (Ji et al., 2022).

### Condition OOD (C-OOD)

Focusing on the subspace \(\mathcal{C}\) of \(\mathcal{X}\), C-OOD investigates the generalization problem of the same reaction with respect to the reaction condition. Typical examples of C-OOD include rate-temperature dependency and rate-temperature-pressure dependency.

It should be stressed that Equation (2) is not always sufficient to describe how the rate constant \(k\) changes with temperature. It turns out that the reaction rate cannot be summarized by a uni

Figure 1: Illustration of different levels of reaction OOD. S-OOD and C-OOD focus on the distribution shifts in the subspace of the input. M-OOD focuses on the distribution shifts in the entire reaction space.

fied formulation. Chemists employ various forms of formulas to describe the dependency between the rate constant and conditions through data-driven approaches such as Chebyshev polynomial fitting (Heal, 1999) or analytical approaches like the Lindemann mechanism (Lindemann et al., 1922). Other types of rate constant formulas can be found in Johnson et al. (2022).

Due to the complex nature of the dependency between the rate constant and conditions, C-OOD generalizability is particularly important if one wants to obtain the optimal condition of a reaction from machine learning models. Meanwhile, the equivalent relation \(\sim\) for C-OOD is easier to construct since the most common conditions are temperature and pressure, which are all real-valued.

### Mechanism OOD (M-OOD)

Another important intuition for characterizing chemical reaction systems, in particular for organic reactions, is the reaction mechanism. Reaction mechanisms are expert knowledge about how reactions happen and are examined in long-standing chemistry research. Typical examples include the E2 and S\({}_{N}\)2 mechanisms (von Rudorff et al., 2020) that are usually considered as widely existing competing mechanisms.

Similar to the definition of S-OOD, M-OOD is also defined by the equivalent relationship, but in the general chemical reaction space \(\mathcal{X}\) with an expert-defined relationship \(\sim_{M}\).4

Footnote 4: The reaction of complex molecules can involve multiple reaction mechanisms happening simultaneously. Then \(\sim_{M}\) is not a strict equivalent relation. However, it is always sufficient to conceptually regard the complex reaction as the combination of multiple elementary reactions where each only includes one mechanism.

**Definition 2** (Mechanism OOD).: _Let \(\sim_{M}\) be the mechanism equivalence relationship in \(\mathcal{X}\), such that \(\mathfrak{R}_{1}\sim_{M}\mathfrak{R}_{2}\) if and only if \(\mathfrak{R}_{1}\) and \(\mathfrak{R}_{2}\) follow the same established mechanism. We say \(\sim_{M}\) defines a M-OOD setting._

Reaction mechanisms also heavily rely on the molecular structure of reactants and products. However, M-OOD is motivated very differently from S-OOD by size equivalence or scaffold equivalence. M-OOD follows the well-established understanding and explanations in chemistry research. In this work, we define the M-OOD dataset by the kinetic database built by Johnson et al. (2022). Detailed introduction can be found in Section 5.

## 5 Datasets and benchmarks

This section presents the details of constructing the benchmark to evaluate OOD generalizability of machine learning methods. We introduce how to create the datasets in different settings and the methods to benchmark. The derived collections of datasets is termed as ReactionOOD, which will be updated following to the growing of reaction kinetic databases.5

Footnote 5: The version of ReactionOOD derived in this paper is v0.1.

### Dataset construction

Table 2 summarizes the information about the OOD datasets at three levels, with detailed target, domain, database, and statistics. For each database, we reorganize it into one or more domains to form multiple OOD datasets at different OOD levels. It worth mentioning that databases of E2

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline OOD level & Target & Domain & Database & \# RXN & \# Smp & Reference \\ \hline \multirow{4}{*}{S-OOD} & \multirow{2}{*}{Barrier} & Total atom number (Eq. 1) & E2 \& S\({}_{N}\)2\({}^{\dagger}\) & 3625 & 3625 & von Rudorff et al. (2020) \\  & & Total atom number (Eq. 1) & RDB7\({}^{\dagger}\) & 23852 & 23852 & Spiekermann et al. (2022) \\  & & Cycloaddition\({}^{\dagger}\) & 5269 & 5269 & Stuyver et al. (2023) \\ \cline{2-6}  & & First reactant scaffold (Eq. 3) & RDB7\({}^{\dagger}\) & 23852 & 23852 & Spiekermann et al. (2022) \\  & & & Cycloaddition\({}^{\dagger}\) & 5269 & 5269 & Stuyver et al. (2023) \\ \hline \multirow{2}{*}{C-OOD} & Rate constant & \(T\) & RMG Lib. T & 29161 & 87340 & \\  & & \((T,P)\) & RMG Lib. TP & 3444 & 113695 & Johnson et al. (2022) \\ \hline M-OOD & Barrier & Mechanism & RMG Family & 12129 & 12129 & \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of datasets. \(\dagger\) indicates the database processed by Heid et al. (2023).

& S\({}_{N}\)2 [von Rudorff et al., 2020], RDB7 [Spiekermann et al., 2022], and Cycloaddition [Stuyver et al., 2023] has been processed by Heid et al. [2023]. However, the databases provided by Heid et al. [2023] only contain the reaction information (with atoms mapped to SMILES strings6) and the target properties. Thus, they can be only used for S-OOD but not C-OOD and M-OOD. We note that the E2 & S\({}_{N}\)2 dataset contains reactions with molecules whose scaffold cannot be properly defined, which prevents the scaffold from being an applicable domain index for this dataset.

Footnote 6: The Simplified Molecular-Input Line-Entry System (SMILES) is a specification in the form of a line notation for describing the structure of chemical species using short ASCII strings.

The well-established and open-sourced RMG kinetic databases [Johnson et al., 2022] are curated for further discussion about the C-OOD and M-OOD7. There are two major parts in RMG kinetic databases, i.e., reaction family and reaction library. The reaction family database contains 74 reaction mechanism families so that it can be used for the M-OOD level. The reaction library database contains 3,956 non-Arrhenius reactions and 3,444 pressure-dependent reactions out of 32,645 reactions in total, providing a data source for the investigation in the C-OOD level.

Footnote 7: https://github.com/ReactionMechanismGenerator/RNG-database

We use internal tools provided in the RMG databases to fetch the SMILES strings of reactants and products respectively, and integrate them to form the SMILES representation of reactions. Then, the atom mapping is extracted by RXNNMapper8 [Schwaller et al., 2021a]. For reaction family database, we only choose the reactions with Arrhenius kinetics and set the target as the activation energy with standard units. This forms the "RMG Family" dataset at the M-OOD level in Table 2. For reaction library database, we further split them into two categories: reactions that depend solely on temperatures and reactions that depend on both temperature and pressure. We also filter out reactions whose valid temperature range does not contain 300K to avoid the effect of singular reactions. Then the "RMG Lib. T" and "RMG Lib. TP" datasets at C-OOD level in Table 2 are obtained by enumerating temperature in \([300K,1,300K,2,300K]\) and pressure in \([10^{7},2\times 10^{7},\ldots,10^{8}]\). Rate constants at specific conditions are obtained by tools in RMG [Johnson et al., 2022] and conditions that are out of valid ranges are discarded.

Footnote 8: https://github.com/rxn4chemistry/rxnnapper

For each dataset in Table 2, we follow the scheme of Gui et al. [2022] and Ji et al. [2022] to create In-Distribution (ID) and OOD splits for a comprehensive examination of the generalization abilities of existing approaches.

### Methods

**Feature engineering.** To handle multi-molecule inputs of the kinetic property prediction task, we follow the established practice of condensed graph representation [Heid and Green, 2021] and use the graph feature extractor in a recent pipeline9[Heid et al., 2023]. For C-OOD settings, the temperature and pressure value is concatenated into the feature vector.

Footnote 9: https://github.com/chemprop/chemprop

**Backbone GNN for kinetic property prediction.** All methods use the identical backbone GIN [Xu et al., 2019] with the virtual node trick. Noted that the backbone network in this paper is different from the one used in Heid et al. [2023]. However, our evaluation shows that the GIN backbone reaches similar MAE performances as [Heid et al., 2023] in the random split setting. Therefore, the vanilla Empirical Risk Minimization (ERM) [Vapnik, 1991] training is the baseline non-OOD method for comparing to other OOD methods

**OOD methods.** In addition to the ERM, we consider OOD methods developed for both Euclidean and graph data: (1) **Euclidean OOD methods** include IRM [Arjovsky et al., 2019], VREx [Krueger et al., 2021], GroupDro [Sagawa* et al., 2020], DANN [Ganin et al., 2016], Coral [Sun and Saenko, 2016], and (2) **graph OOD methods** include CIGA [Chen et al., 2022], GSAT [Miao et al., 2022] and DIR [Wu et al., 2022b]. We follow the same evaluation protocol and hyperparameter settings as in Gui et al. [2022]. Though the hyperparameters are tuned for single graph problems, they are still applicable to kinetic property prediction because the CGR feature extraction conceptually reformulates the graphs for reactants and products as one graph. We report the ID and OOD RMSE results of models selected according to the best ID and OOD validation performance. Each score is averaged by 10 runs with different random seeds.

## 6 Results and findings

Table 5.2 and the left half of Table 5.2 present the RMSE results for S-OOD settings, including graph size shifts on E2 & S\({}_{N}\)2, Cycloaddition, and RDB7 datasets and scaffold shifts on Cycloaddition, and RDB7 datasets. The right half of Table 5.2 presents the M-OOD results of the mechanism shift on the RMG Family dataset. Table 5.2 presents the C-OOD results for T and TP shifts on the RMG Lib. dataset. For Table 5.2 and Table 5.2, the kinetic property to be predicted is the reaction barrier. For Table 5.2, the kinetic property to be predicted is the rate constant. For all tables, the best performance is boldfaced. If there are ties between ERM and any OOD methods, the ERM performance is boldfaced. Then we discuss two findings and other observations.

**Finding 1: OOD kinetic property prediction is a challenging OOD problem.** At first glance, it can be found that there remain huge ID-OOD performance gaps across different levels and types of OOD shifts and datasets. Neither OOD methods developed for Euclidean data nor OOD methods developed for graph data can outperform the vanilla ERM approaches consistently and significantly. The results also align with existing observations in realistic data and distribution shifts (Gulrajani and Lopez-Paz, 2021; Ji et al., 2022; Gui et al., 2022). The performances of IRM are quite bad possibly due to the high requirements of IRM in learning invariant features under non-linear data (Chen et al., 2022).

**Finding 2: M-OOD shift is significantly harder than the S-OOD shift.** Moreover, it can be found that the ID-OOD performance gap is largely signified under mechanism OOD shifts (M-OOD), especially compared to the widely studied S-OOD shifts. The RMSE performances of M-OOD are An order of magnitude larger than those for S-OOD. Table 5.2 directly demonstrated this absolute

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{Cyloaddition - Scaffold (S-OOD)} & \multicolumn{3}{c}{Cycloaddition - Total Atom Number (S-OOD)} \\ \cline{2-9} Methods & \multicolumn{2}{c}{Covariate} & \multicolumn{2}{c}{Concept} & \multicolumn{2}{c}{Covariate} & \multicolumn{2}{c}{Concept} \\ ID & OOD & ID & OOD & ID & OOD & ID & OOD \\ \hline ERM & \(3.37\pm 0.05\) & \(5.80\pm 0.14\) & \(4.40\pm 0.14\) & \(\mathbf{4.21\pm 0.66}\) & \(29.13\pm 0.39\) & \(118.44\pm 0.16\) & \(32.30\pm 0.08\) & \(\mathbf{31.25\pm 0.60}\) \\ IRM & \(19.5\pm 0.53\) & \(20.6\pm 1.40\) & \(27.9\pm 0.04\) & \(20.7\pm 0.05\) & \(101.60\pm 0.31\) & \(137.54\pm 0.63\) & \(93.38\pm 0.06\) & \(116.44\pm 0.60\) \\ VREx & \(3.65\pm 0.06\) & \(5.11\pm 0.20\) & \(20.2\pm 1.11\) & \(18.3\pm 1.46\) & \(36.77\pm 0.62\) & \(115.78\pm 0.64\) & \(61.57\pm 1.38\) & \(82.38\pm 1.69\) \\ GroupDRO & \(3.41\pm 0.03\) & \(4.90\pm 1.09\) & \(\mathbf{4.26\pm 0.84}\) & \(24.1\pm 0.06\) & \(\mathbf{28.11\pm 0.40}\) & \(\mathbf{114.56\pm 2.77}\) & \(\mathbf{31.99\pm 0.65}\) & \(53.47\pm 0.68\) \\ DANN & \(3.38\pm 0.03\) & \(4.90\pm 1.06\) & \(3.430\pm 0.07\) & \(4.22\pm 0.05\) & \(29.05\pm 0.39\) & \(116.41\pm 2.78\) & \(32.03\pm 0.39\) & \(52.91\pm 0.61\) \\ Coral & \(\mathbf{3.36\pm 0.04}\) & \(\mathbf{4.85\pm 1.02}\) & \(4.33\pm 0.09\) & \(4.21\pm 0.09\) & \(29.09\pm 0.39\) & \(116.09\pm 18.87\) & \(31.91\pm 0.59\) & \(52.85\pm 0.66\) \\ CIGA & \(3.69\pm 0.34\) & \(4.94\pm 0.36\) & \(4.59\pm 0.47\) & \(4.39\pm 0.31\) & \(43.83\pm 0.87\) & \(134.81\pm 1.99\) & \(38.22\pm 1.70\) & \(64.93\pm 1.57\) \\ DIR & \(3.72\pm 0.20\) & \(5.02\pm 0.23\) & \(4.52\pm 0.16\) & \(4.38\pm 0.19\) & \(44.15\pm 0.56\) & \(131.66\pm 11.57\) & \(39.40\pm 12.96\) & \(65.01\pm 2.38\) \\ GSAT & \(3.39\pm 0.06\) & \(5.07\pm 1.05\) & \(4.34\pm 0.06\) & \(4.22\pm 0.06\) & \(30.97\pm 0.99\) & \(117.06\pm 13.34\) & \(32.56\pm 0.85\) & \(53.31\pm 0.68\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: OOD generalization performance on E2 & S\({}_{N}\)2 and RMG with S-OOD and M-OOD shifts.

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline \multirow{3}{*}{Methods} & \multicolumn{3}{c}{Cyloaddition - Scaffold (S-OOD)} & \multicolumn{3}{c}{Cycloaddition - Total Atom Number (S-OOD)} \\ \cline{2-9}  & \multicolumn{2}{c}{Covariate} & \multicolumn{2}{c}{Concept} & \multicolumn{2}{c}{Covariate} & \multicolumn{2}{c}{Concept} \\ ID & OOD & ID & OOD & ID & OOD & ID & OOD \\ \hline ERM & \(4.78\pm 0.07\) & \(5.80\pm 0.30\) & \(5.04\pm 0.06\) & \(5.57\pm 0.08\) & \(4.12\pm 0.07\) & \(5.23\pm 0.29\) & \(4.30\pm 0.11\) & \(6.00\pm 0.13\) \\ IRM & \(13.73\pm 0.59\) & \(16.60\pm 0.15\) & \(17.09\pm 0.26\) & \(18.43\pm 0.41\) & \(17.64\pm 0.10\) & \(17.06\pm 0.30\) & \(23.04\pm 0.22\) & \(22.46\pm 0.22\) \\ VREx & \(5.50\pm 0.06\) & \(6.41\pm 0.86\) & \(5.17\pm 0.05\)gap by a huge contrast, where the E2 & S\({}_{N}\)2 dataset has only two reaction mechanisms but the RMG Family dataset contains 74 reaction mechanisms.

**Miscellaneous observations.** We can also observe an exception for graph size concept shifts in E2 & S\({}_{N}\)2, that OOD performances of various methods are generically better than ID performances. One conjecture is that the distribution of molecule sizes could limit the strength of the potential concept shifts. This distinct size distribution aligns with our earlier finding that scaffolds are not always identifiable for certain molecules. For concept shifts, the ID-OOD performance gaps are mainly caused by spurious correlations across different environments or domains. For images, such spurious correlations mainly lie between the background and the object in the images. For molecules, such spurious correlations are widely observed between scaffolds and the critical functional groups (Hu et al., 2020; Koh et al., 2021). However, for small molecules that even do not have scaffolds, we suspect there is limited room for the existence of spurious correlations. Nevertheless, it remains challenging for the OOD generalization across different graph sizes, as demonstrated by the graph size covariate shifts.

## 7 Conclusion and future works

In this work, we identified various distribution shifts exist in chemical reactions and curated a new benchmark to examine the performance of multiple OOD generalization approaches. The results show that there remain huge ID-OOD performance gaps across different distribution shifts. Therefore, it calls for better graph machine learning approaches to tackle the OOD regression challenge for facilitating the chemical proper prediction.

## References

* Arjovsky et al. (2019) Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* Bevilacqua et al. (2021) Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In _International Conference on Machine Learning_, pages 837-851, 2021.
* Bures and Larrosa (2023) Jordi Bures and Igor Larrosa. Organic reaction mechanism classification using machine learning. _Nature_, 613(7945):689-695, 2023.
* Chen et al. (2022) Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In _Advances in Neural Information Processing Systems_, 2022.
* Choi (2023) Sunghwan Choi. Prediction of transition state structures of gas-phase chemical reactions via machine learning. _Nature Communications_, 14(1):1168, 2023.
* Davies (2019) Ian W Davies. The digitization of organic synthesis. _Nature_, 570(7760):175-181, 2019.
* Donahue (2003) Neil M Donahue. Reaction barriers: Origin and evolution. _Chemical reviews_, 103(12):4593-4604, 2003.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{3}{c}{RMG Lib. T - T (C-ODOD)} & \multicolumn{3}{c}{RMG Lib. TP - (T, P) (C-OD)} \\ \cline{2-9}  & \multicolumn{2}{c}{Covariate} & \multicolumn{2}{c}{Concept} & \multicolumn{2}{c}{Covariate} & \multicolumn{2}{c}{Concept} \\ Methods & ID & OOD & ID & OOD & ID & OOD & ID & OOD \\ \hline ERM & **3.28\(\pm\)**0.01 & 8.40\(\pm\)0.08 & **3.70\(\pm\)**0.02 & **6.76\(\pm\)**0.05 & 2.58\(\pm\)0.01 & 7.35\(\pm\)0.24 & **2.92\(\pm\)**0.02 & 6.34\(\pm\)**0.02 \\ IRM & 3.41\(\pm\)0.02 & 8.45\(\pm\)0.08 & 7.16\(\pm\)0.03 & 9.04\(\pm\)0.02 & 4.30\(\pm\)0.13 & 12.50\(\pm\)0.78 & 9.13\(\pm\)0.03 & 15.43\(\pm\)0.06 \\ VREX & 3.36\(\pm\)0.01 & 8.92\(\pm\)0.13 & 7.27\(\pm\)0.04 & 8.51\(\pm\)0.17 & **2.55\(\pm\)**0.01 & 7.52\(\pm\)**0.98 & 9.55\(\pm\)0.13 & 13.10\(\pm\)0.73 \\ GroupDRO & 3.29\(\pm\)0.02 & 8.45\(\pm\)0.10 & 3.76\(\pm\)0.01 & 6.83\(\pm\)0.05 & 2.57\(\pm\)0.01 & 7.39\(\pm\)0.24 & 2.95\(\pm\)0.02 & 6.41\(\pm\)0.04 \\ DANN & 3.46\(\pm\)0.03 & 8.75\(\pm\)0.08 & 3.73\(\pm\)0.04 & 6.82\(\pm\)0.03 & 2.59\(\pm\)0.01 & **7.16\(\pm\)**0.05 & 2.93\(\pm\)0.02 & 6.37\(\pm\)0.03 \\ Coral & 3.29\(\pm\)0.02 & **8.37\(\pm\)**0.06 & 3.75\(\pm\)0.04 & 6.77\(\pm\)0.05 & 2.58\(\pm\)0.01 & 7.36\(\pm\)0.16 & 2.94\(\pm\)0.03 & 6.39\(\pm\)0.03 \\ CIGA & 3.71\(\pm\)0.09 & 8.95\(\pm\)0.10 & 4.01\(\pm\)0.05 & 7.12\(\pm\)0.18 & 2.56\(\pm\)0.08 & 7.51\(\pm\)0.19 & 2.97\(\pm\)0.05 & **6.16\(\pm\)**0.04 \\ DIR & 3.74\(\pm\)0.07 & 9.27\(\pm\)0.05 & 3.96\(\pm\)0.06 & 7.07\(\pm\)0.08 & 3.20\(\pm\)0.08 & 8.23\(\pm\)0.44 & 3.11\(\pm\)0.07 & 6.57\(\pm\)0.11 \\ GSAT & 4.01\(\pm\)0.06 & 9.48\(\pm\)0.09 & 3.82\(\pm\)0.05 & 7.06\(\pm\)0.11 & 3.03\(\pm\)0.06 & 8.26\(\pm\)0.42 & 3.08\(\pm\)0.03 & 6.58\(\pm\)0.07 \\ \hline \hline \end{tabular}
\end{table}
Table 5: OOD generalization performance in RMG Lib. with C-OOD shifts.

Kobi C Felton, Jan G Rittig, and Alexei A Lapkin. Summit: benchmarking machine learning methods for reaction optimisation. _Chemistry-Methods_, 1(2):116-122, 2021.
* Ganin et al. (2016) Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. _Journal of Machine Learning Research_, 17:59:1-59:35, 2016.
* Gilmer et al. (2017) Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, pages 1263-1272, 2017.
* Gui et al. (2022) Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. GOOD: A graph out-of-distribution benchmark. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* Gulrajani and Lopez-Paz (2021) Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In _International Conference on Learning Representations_, 2021.
* Heal (1999) G Roger Heal. Evaluation of the function p (x), used in non-isothermal kinetics, by a series of chebyshev polynomials. _Instrumentation Science & Technology_, 27(5):367-387, 1999.
* Heid and Green (2021) Esther Heid and William H Green. Machine learning of reaction properties via learned representations of the condensed graph of reaction. _Journal of Chemical Information and Modeling_, 62(9):2101-2110, 2021.
* Heid et al. (2023) Esther Heid, Kevin P Greenman, Yunsie Chung, Shih-Cheng Li, David E Graff, Florence H Vermeire, Haoyang Wu, William H Green, and Charles J McGill. Chemprop: A machine learning package for chemical property prediction. 2023.
* Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _Advances in Neural Information Processing Systems_, 2020.
* A Focus on Affinity Prediction Problems with Noise Annotations. _arXiv preprint_, arXiv:2201.09637, 2022.
* Johnson et al. (2022) Matthew S Johnson, Xiaorui Dong, Alon Grinberg Dana, Yunsie Chung, David Farina Jr, Ryan J Gillis, Mengjie Liu, Nathan W Yee, Katrin Blondal, Emily Mazeau, et al. Rmg database for chemical property prediction. _Journal of Chemical Information and Modeling_, 62(20):4906-4915, 2022.
* 589, 2021.
* Koh et al. (2021) Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M. Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning,,_, pages 5637-5664, 2021.
* Krueger et al. (2021) David Krueger, Ethan Caballero, Jorn-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron C. Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, pages 5815-5826, 2021.
* Krizhevsky et al. (2014)* Lindemann et al. [1922] Frederick A Lindemann, Svante Arrhenius, Irving Langmuir, NR Dhar, J Perrin, and WC McC Lewis. Discussion on the radiation theory of chemical action. _Transactions of the Faraday Society_, 17:598-606, 1922.
* Meuwly [2021] Markus Meuwly. Machine learning for chemical reactions. _Chemical Reviews_, 121(16):10218-10239, 2021.
* Miao et al. [2022] Siqi Miao, Miaoyuan Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. _International Conference on Machine Learning_, 2022.
* Mihalic and Trinajstic [1992] Zlatko Mihalic and Nenad Trinajstic. A graph-theoretical approach to structure-property relationships, 1992.
* Mullowney et al. [2023] Michael W Mullowney, Katherine R Duncan, Somayah S Elsayed, Neha Garg, Justin JJ van der Hooft, Nathaniel I Martin, David Meijer, Barbara R Terlouw, Friederike Biermann, Kai Blin, et al. Artificial intelligence for natural product drug discovery. _Nature Reviews Drug Discovery_, pages 1-22, 2023.
* Quinonero-Candela et al. [2008] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. _Dataset shift in machine learning_. Mit Press, 2008.
* Sagawa* et al. [2020] Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks. In _International Conference on Learning Representations_, 2020.
* Schwaller et al. [2021a] Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro Laino. Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. _Science Advances_, 7(15):eabe4166, 2021a.
* Schwaller et al. [2021b] Philippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based neural networks. _Nature machine intelligence_, 3(2):144-152, 2021b.
* Shen et al. [2021] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* Spiekermann et al. [2022] Kevin Spiekermann, Lagnajit Pattanaik, and William H Green. High accuracy barrier heights, enthalpies, and rate coefficients for chemical reactions. _Scientific Data_, 9(1):417, 2022.
* Stocker et al. [2020] Sina Stocker, Gabor Csanyi, Karsten Reuter, and Johannes T Margraf. Machine learning in chemical reaction space. _Nature communications_, 11(1):5505, 2020.
* Strieth-Kalthoff et al. [2022] Felix Strieth-Kalthoff, Frederik Sandfort, Marius Kuhnemund, Felix R Schafer, Herbert Kuchen, and Frank Glorius. Machine learning for chemical reactivity: The importance of failed experiments. _Angewandte Chemie International Edition_, 61(29):e202204647, 2022.
* Stuyver and Coley [2022] Thijs Stuyver and Connor W Coley. Quantum chemistry-augmented neural networks for reactivity prediction: performance, generalizability, and explainability. _The Journal of Chemical Physics_, 156(8), 2022.
* Stuyver et al. [2023] Thijs Stuyver, Kjell Jorner, and Connor W Coley. Reaction profiles for quantum chemistry-computed [3+ 2] cycloaddition reactions. _Scientific Data_, 10(1):66, 2023.
* Sun and Saenko [2016] Baochen Sun and Kate Saenko. Deep CORAL: correlation alignment for deep domain adaptation. In _European Conference on Computer Vision_, volume 9915, pages 443-450, 2016.
* Vapnik [1991] Vladimir Vapnik. Principles of risk minimization for learning theory. In _Advances in Neural Information Processing Systems_, pages 831-838, 1991.
* van Rudorff et al. [2020] Guido Falk von Rudorff, Stefan N Heinen, Marco Bragato, and O Anatole von Lilienfeld. Thousands of reactants and transition states for competing e2 and s2 reactions. _Machine Learning: Science and Technology_, 1(4):045026, 2020.
* Van den Bergh [2015]Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne J. Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun K. Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. _Nature_, 620:47 - 60, 2023.
* Warr (2014) Wendy A Warr. A short review of chemical reaction database systems, computer-aided synthesis design, reaction prediction and synthetic feasibility. _Molecular Informatics_, 33(6-7):469-476, 2014.
* Wu et al. (2022a) Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. In _International Conference on Learning Representations_, 2022a.
* Wu et al. (2022b) Yingxin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. In _International Conference on Learning Representations_, 2022b.
* Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2019.
* Zhang et al. (2021) Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, Meng Liu, Yuchao Lin, Zhao Xu, Keqiang Yan, Keir Adams, Maurice Weiler, Xiner Li, Tianfan Fu, Yucheng Wang, Haiyang Yu, Yuqing Xie, Xiang Fu, Alex Strasser, Shenglong Xu, Yi Liu, Yuanqi Du, Alexandra Saxton, Hongyi Ling, Hannah Lawrence, Hannes Stark, Shurui Gui, Carl Edwards, Nicholas Gao, Adriana Lactera, Tailin Wu, Elyssa F. Hofgard, Aria Mansouri Tehrani, Rui Wang, Ameya Daigavane, Montgomery Bohde, Jerry Kurtin, Qian Huang, Tuong Phung, Minkai Xu, Chaitanya K. Joshi, Simon V. Mathis, Kamyar Azizzadenesheli, Ada Fang, Alan Aspuru-Guzik, Erik Bekkers, Michael M. Bronstein, Marinka Zitnik, Anima Anandkumar, Stefano Ermon, Pietro Lio, Rose Yu, Stephan Gunnemann, Jure Leskovec, Heng Ji, Jimeng Sun, Regina Barzilay, Tommi S. Jaakkola, Connor W. Coley, Xiaoning Qian, Xiaofeng Qian, Tess E. Smidt, and Shuiwang Ji. Artificial intelligence for science in quantum, atomistic, and continuum systems. _arXiv preprint_, arXiv:2307.08423, 2023.
* Zhao et al. (2023) Qiyuan Zhao, Sai Mahit Vaddadi, Michael Woulfe, Lawal A Ogunfowora, Sanjay S Garimella, Olexandr Isayev, and Brett M Savoie. Comprehensive exploration of graphically defined reaction spaces. _Scientific Data_, 10(1):145, 2023.
* Zhu et al. (2021) Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations of localized graph training data. _Advances in Neural Information Processing Systems_, 34:27965-27977, 2021.