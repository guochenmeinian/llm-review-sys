# Chasing Fairness Under Distribution Shift:

A Model Weight Perturbation Approach

 Zhimeng Jiang\({}^{1}\), Xiaotian Han\({}^{1}\)1, Hongye Jin\({}^{1}\), Guanchu Wang\({}^{2}\), Rui Chen\({}^{3}\), Na Zou\({}^{1}\), Xia Hu\({}^{2}\)

\({}^{1}\)Texas A&M University, \({}^{2}\)Rice University, \({}^{3}\)Samsung Electronics America

Equal contribution.

Footnote 1: footnotemark:

###### Abstract

Fairness in machine learning has attracted increasing attention in recent years. The fairness methods improving algorithmic fairness for in-distribution data may not perform well under distribution shifts. In this paper, we first theoretically demonstrate the inherent connection between distribution shift, data perturbation, and model weight perturbation. Subsequently, we analyze the sufficient conditions to guarantee fairness (i.e., low demographic parity) for the target dataset, including fairness for the source dataset, and low prediction difference between the source and target datasets for each sensitive attribute group. Motivated by these sufficient conditions, we propose robust fairness regularization (RFR) by considering the worst case within the model weight perturbation ball for each sensitive attribute group. We evaluate the effectiveness of our proposed RFR algorithm on synthetic and real distribution shifts across various datasets. Experimental results demonstrate that RFR achieves better fairness-accuracy trade-off performance compared with several baselines. The source code is available at https://github.com/zhimengj0326/RFR_NeurIPS23.

## 1 Introduction

Previous research [1; 2; 3; 4] has shown that a classifier trained on a specific source distribution will perform worse when testing on a different target distribution, due to distribution shift. Recently, many studies have focused on investigating the impact of distribution shift on machine learning models, where fairness performance degradation is even more significant than that of prediction performance [5]. The sensitivity of fairness over distribution shift challenges machine learning models in high stake applications, such as criminal justice [6], healthcare [7], and job marketing [8]. Thus the transferability of the fairness performance under distribution shift is a crucial consideration for real-world applications.

To achieve the fairness of the model (already achieved fairness on the source dataset ) on the target dataset, we first reveal that distribution shift is equivalent to model weight perturbation, and then seek to achieve fairness under distribution shift via model weight perturbation. Specifically, _i)_ we reveal the inherent connection between distribution shift, data perturbation, and model weight perturbation. We theoretically demonstrate that any distribution shift can be equivalent to data perturbation and model weight perturbation in terms of loss value. In other words, the effect of distribution shift on model training can be attributed to data or model perturbation. _ii)_ Given the established connection between the distribution shift and model weight perturbation, we next tackle fairness under the distribution shift problem. We first investigate demographic parity relation between source and target datasets. Achieving fair prediction (e.g., low demographic parity) in the source dataset is insufficient for fair prediction in the target dataset. More importantly, the average prediction difference between source and target datasets with the same sensitive attribute also matters for achieving fairness in target datasets.

Motivated by the established equivalence connection, we propose robust fairness regularization (RFR) to enforce average prediction robustness over model weight. In this way, the well-trained model can tackle the distribution shift problem in terms of demographic parity. Considering the expensive computation for the inner maximization problem in RFR, we accelerate RFR by obtaining the approximate closed-form model weight perturbation using first-order Taylor expansion. In turn, an efficient RFR algorithm, trading the inner maximization problem into two forward and backward propagations for each model update, is proposed to achieve robust fairness under distribution shift. Our contributions are highlighted as follows:

* We theoretically reveal the inherent connection between distribution shift, data perturbation, and model weight perturbation. In other words, distribution shift and model perturbation are equivalent in terms of loss value for any model architecture and loss function.
* We first analyze the sufficient conditions to guarantee fairness transferability under distribution shift. Based on the established connection, we propose RFR explicitly pursuing sufficient conditions for robust fairness by considering the worst case of model perturbation.
* We evaluate the effectiveness of RFR on various real-world datasets with synthetic and real distribution shifts. Experiments results demonstrate that RFR can mostly achieve better fairness-accuracy tradeoff with both synthetic and real distribution shifts.

## 2 Understanding Distribution Shift

In this section, we first provide the notations used in this paper. And then, we theoretically understand the relations between distribution shift, data perturbation, and model weight perturbation, as shown in Figure 1.

### Notations

We consider source dataset \(\mathcal{D}_{\mathcal{S}}\) and target dataset \(\mathcal{D}_{\mathcal{T}}\), defined as a probability distribution \(\mathcal{P}_{\mathcal{S}}\) and \(\mathcal{P}_{\mathcal{T}}\) for samples \(S\in\mathcal{S}\) and \(T\in\mathcal{T}\), respectively. Each sample defines values for three random variables: features \(X\) with arbitrary domain \(\mathcal{X}\), binary sensitive attribute \(A\in\mathcal{A}=\{0,1\}\), and label \(Y\) arbitrary domain \(\mathcal{Y}\), i.e., \(\mathcal{S}=\mathcal{T}=\mathcal{X}\times\mathcal{A}\times\mathcal{Y}\). We denote \(\delta\) as data perturbation on source domain \(\mathcal{S}\), where \(\delta_{X}(X)\) and \(\delta_{Y}(Y)\) data perturbation of features and labels. Using \(\mathcal{P}(\cdot)\) to denote the space of probability distributions over some domain, we denote the space of distributions over examples as \(\mathcal{P}(\mathcal{S})\). We use \(||\cdot||_{p}\) as \(L_{p}\) norm. Let \(f_{\theta}(x)\) be the output of the neural networks parameterized with \(\theta\) to approximate the true label \(y\). We define \(l(f_{\theta}(x),y)\) as the loss function for neural network training, and the optimal model parameters \(\theta^{*}\) training on source data \(\mathcal{S}\) is given by \(\theta^{*}=\arg\min\limits_{\theta}\mathcal{R}_{\mathcal{S}}\), where \(\mathcal{R}_{\mathcal{S}}=\mathbb{E}_{(X,Y)\sim\mathcal{P}_{\mathcal{S}}}[l(f_ {\theta}(X),Y)]\). Since the target distribution \(\mathcal{P}_{\mathcal{T}}\) maybe be different with source distribution \(\mathcal{P}_{\mathcal{S}}\), the well-trained model \(f_{\theta^{*}}(\cdot)\) trained on source dataset \(\mathcal{S}\) typically does not perform well in target dataset \(\mathcal{T}\).

Figure 1: The overview of distribution shift understanding. The left part demonstrates distribution shift can be transformed as data perturbation, while the right part shows that data perturbation and model weight perturbation are equivalent.

### Distribution Shift is Data Perturbation

Deep neural networks are immensely challenged by data quality or dynamic environments [9; 10]. The well-trained deep neural network model may not perform well if existing feature/label noise in training data or distribution shifts among training and target environments. In this subsection, we reveal the inherent equivalence between distribution shift and data perturbation for any neural networks \(f_{\theta}(\cdot)\), source distribution \(\mathcal{P}_{\mathcal{S}}\), and target distribution \(\mathcal{P}_{\mathcal{T}}\) using optimal transport. We first provide the formal definition of optimal transport as follows:

**Definition 1** (Optimal Transport [11]).: _Considering two distributions with probability distribution \(P_{\mathcal{S}}\) and \(P_{\mathcal{T}}\), and cost function moving from \(s\) to \(t\) as \(c(s,t)\), optimal transport between probability distribution \(P_{\mathcal{S}}\) and \(P_{\mathcal{T}}\) is given by_

\[\gamma^{*}(s,t)=\arg\inf_{\gamma\in\Gamma(P_{\mathcal{S}},P_{\mathcal{T}})} \iint c(s,t)\gamma(s,t)\mathrm{d}s\mathrm{d}t,\] (1)

_where the distribution set \(\Gamma(P_{\mathcal{S}},P_{\mathcal{T}})\) is the collection of all possible transportation plans and given by \(\Gamma(P_{\mathcal{S}},P_{\mathcal{T}})=\Big{\{}\gamma(s,t)>0,\int\gamma(s,t) \mathrm{d}t=P_{\mathcal{S}}(s),\int\gamma(s,t)\mathrm{d}s=P_{\mathcal{T}}(t) \Big{\}}\). In other words, the distribution set consists of all possible joint distributions with margin distribution \(P_{\mathcal{S}}\) and \(P_{\mathcal{T}}\)._

Based on the definition of optimal transport, we demonstrate that distribution shift is equivalent to data perturbation in the following theorem:

**Theorem 2.1**.: _For any two different distributions with probability distribution \(P_{\mathcal{S}}\) and \(P_{\mathcal{T}}\), adding data perturbation \(\delta\)2 on source data \(S\) can make perturbated source data and target data with the same distribution, where the distribution of data perturbation \(\delta\) is given by_

Footnote 2: Data perturbation \(\delta\) includes the perturbation for features \(\delta_{X}(X)\) and labels \(\delta_{Y}(Y)\).

\[\mathcal{P}(\delta)=\int_{\mathcal{S}}\gamma^{*}(s,s+\delta)\mathrm{d}s.\] (2)

_Additionally, for any positive \(p>0\), data perturbation \(\delta\) with minimal power \(\mathbb{E}[||\delta||_{p}^{p}]\) is given by Eq.(2) if optimal transport plan \(\gamma^{*}(\cdot,\cdot)\) is calculated based on Eq. (1) with cost function \(c(s,t)=||s-t||_{p}^{p}\)._

Proof sketch.Given two different distributions, there are many possible perturbations to move one distribution to another. Optimal transport can select the optimal feasible perturbations or transportation plan in terms of specific objectives (e.g., perturbations power). Given the optimal transportation plan, we can derive the distribution of perturbations based on basic probability theory.

Theorem 2.1 demonstrates the equivalence between distribution shift and data perturbation, where data perturbation distribution is dependent on optimal transport between source and target distribution. The intuition is that such distribution shift can be achieved via optimal transportation (i.e., data perturbation). Based on Theorem 2.1, we have the following Corollary 2.2 on the equivalent of neural network behavior for source and target data:

**Corollary 2.2**.: _Given source and target datasets with probability distribution \(\mathcal{P}_{\mathcal{S}}\) and \(\mathcal{P}_{\mathcal{T}}\), there exists data perturbation \(\delta\) so that the training loss of any neural network \(f_{\theta}(\cdot)\) for target distribution equals that for source distribution with data perturbation \(\delta\), i.e.,_

\[\mathbb{E}_{(X,Y)\sim\mathcal{P}_{\mathcal{T}}}[l(f_{\theta}(X),Y)]=\mathbb{E }_{\delta_{X}(X),\delta_{Y}(Y)}\mathbb{E}_{(X,Y)\sim\mathcal{P}_{\mathcal{S}} }[l(f_{\theta}(X+\delta_{X}(X)),Y+\delta_{Y}(Y))].\] (3)

In other words, for the model trained with loss minimization on source data, the deteriorated performance on the target dataset stems from the perturbation of features and labels. The proof sketch is based on Theorem 2.1 since adding a perturbation on the source dataset can be consistent with the target dataset distribution. Therefore, the conclusion can hold for any loss function that is only dependent on the dataset.

### Data Perturbation Equals Model Weight Perturbation

Although we understand that the distribution shift can be attributed to the data perturbation of features and labels in source data, it is still unclear how to tackle the distribution shift issue. A natural solutionis to adopt adversarial training to force the well-trained model to be robust over data perturbation. However, it is complicated to generate data perturbation on features and labels simultaneously, and many well-developed adversarial training methods are mainly designed for adversarial feature perturbation. Fortunately, we show that model weight perturbation is equivalent to data perturbation by Theorem 2.3.

**Theorem 2.3**.: _Considering the source dataset with distribution \(\mathcal{P}_{\mathcal{S}}\), suppose the source dataset is perturbed with data perturbation \(\delta\), and the neural network is given by \(f_{\theta}(\cdot)\), for general case, there exists model weight perturbation \(\Delta\theta\) so that the training loss on perturbed source dataset is the same with that for model weight perturbation \(\Delta\theta\) on source distribution:_

\[\mathbb{E}_{\delta_{X}(X),\delta_{Y}(Y)}\mathbb{E}_{(X,Y)\sim\mathcal{P}_{ \mathcal{S}}}[l(f_{\theta}(X+\delta_{X}(X)),Y+\delta_{Y}(Y))]=\mathbb{E}_{(X,Y )\sim\mathcal{P}_{\mathcal{S}}}[l(f_{\theta+\Delta\theta}(X),Y)].\] (4)

Proof sketch.The loss under data perturbation and weight perturbation can be analyzed using first-order Tayler expansion. The critical step is to find the condition of the equivalence for the first order term of data perturbation and weight perturbation. Fortunately, the existence of such conditions can be easily proved using linear algebra.

Theorem 2.3 demonstrates that the training loss on perturbed data distribution (but fixed model weight) equals the training loss on perturbed model weight (but original data distribution). In other words, the training loss fluctuation from data perturbation can equal model weight perturbation. Furthermore, we conclude that chasing a robust model over data perturbation can be achieved via model weight perturbation, i.e., finding a "flattened" local minimum in terms of the target objective is sufficient for robustness.

## 3 Methodology

In this section, we first analyze the sufficient condition to achieve robust fairness over distribution shift in terms of demographic parity. Based on the analysis, we propose a simple yet effective robust fairness regularization via explicit adopting group model weight perturbation. Note that the proposed robust fairness regularization involves a computation-expensive maximization problem, we further accelerate model training via first-order Taylor expansion.

### Robust Fairness Analysis

We consider binary sensitive attribute \(A\in\{0,1\}\) and demographic parity as fairness metric, i.e., the average prediction gap of model \(f_{\theta}(\cdot)\) for different sensitive attribute groups in target dataset \(\Delta DP_{\mathcal{T}}=|\mathbb{E}_{\mathcal{T}_{0}}[f_{\theta}(\mathbf{x}) ]-\mathbb{E}_{\mathcal{T}_{1}}[f_{\theta}(\mathbf{x})]|\), where \(\mathcal{T}_{0}\) and \(\mathcal{T}_{1}\) represent the target datasets for sensitive attribute \(A=0\) and \(A=1\), respectively. However, only source dataset \(\mathcal{S}\) is available for neural network training. In other words, even though demographic parity on source dataset \(\Delta DP_{\mathcal{S}}=|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x}) ]-\mathbb{E}_{\mathcal{S}_{1}}[f_{\theta}(\mathbf{x})]|\) is low, demographic parity on target dataset \(\Delta DP_{\mathcal{T}}\) may not be guaranteed to be low due to distribution shift.

To investigate robust fairness over distribution shift, we try to reveal the connection between demographic parity for source and target datasets. We bound the demographic parity difference for source and target datasets as follows:

\[DP_{\mathcal{T}} \overset{(a)}{\leq} DP_{\mathcal{S}}+\Big{|}|\mathbb{E}_{\mathcal{T}_{0}}[f_{\theta}( \mathbf{x})]-\mathbb{E}_{\mathcal{T}_{1}}[f_{\theta}(\mathbf{x})]|-|\mathbb{E }_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{\mathcal{S}_{1}}[f_{ \theta}(\mathbf{x})]|\Big{|}\] \[\overset{(b)}{\leq} DP_{\mathcal{S}}+|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}( \mathbf{x})]-\mathbb{E}_{\mathcal{T}_{0}}[f_{\theta}(\mathbf{x})]|+|\mathbb{E }_{\mathcal{S}_{1}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{\mathcal{T}_{1}}[f_{ \theta}(\mathbf{x})]|,\] (5)

where inequality (a) and (b) hold due to \(a-b\leq|a-b|\) and \(\big{|}|a-b|-|a^{\prime}-b^{\prime}|\big{|}\leq|a-a^{\prime}|+|b-b^{\prime}|\), respectively, for any \(a,a^{\prime},b,b^{\prime}\). In other words, in order to minimize demographic parity for target dataset \(DP_{\mathcal{T}}\), the objective of \(DP_{\mathcal{S}}\) minimization is insufficient. The minimization of prediction difference for source and target datasets given sensitive attribute groups \(A=0\) and \(A=1\), defined as \(\Delta_{0}=|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{ \mathcal{T}_{0}}[f_{\theta}(\mathbf{x})]|\) and \(\Delta_{1}=|\mathbb{E}_{\mathcal{S}_{1}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{ \mathcal{T}_{1}}[f_{\theta}(\mathbf{x})]|\), are also beneficial to achieve robust fairness over distribution shift in terms of demographic parity.

The bound in Eq. (3) is tight when both condition (a) \(DP_{\mathcal{S}}\leq DP_{\mathcal{T}}\) and (b) maximum and minimum of value set \(\min\left\{\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})],\mathbb{E}_{ \mathcal{T}_{0}}[f_{\theta}(\mathbf{x})]\right\}\), \(\min\left\{\mathbb{E}_{\mathcal{S}_{1}}[f_{\theta}(\mathbf{x})],\mathbb{E}_{ \mathcal{T}_{1}}[f_{\theta}(\mathbf{x})]\right\}\) both are from source or target distribution. Even though conditions (a) and (b) may not hold for the neural network model, we would like to that our goal is not to obtain a tight bound for demographic parity on target distribution. Instead, we aim to find sufficient conditions to guarantee low demographic parity and such low demographic parity can be achieved in model training without any target distribution information. The proposed upper bound Eq. (5) actually reveals sufficient conditions, which can be achieved by our proposed RFR algorithm.

### Robust Fairness Regularization

Motivated by Section 3.1 and distribution shift understanding in Section 2, we develop a robust fairness regularization to achieve robust fairness over distribution shift. Section 3.1 demonstrates that fair prediction on the target dataset requires fair prediction on the source dataset and low prediction difference between the source and target dataset for each sensitive attribute, i.e., low \(\Delta_{0}=|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{ \mathcal{T}_{0}}[f_{\theta}(\mathbf{x})]|\) and \(\Delta_{1}=|\mathbb{E}_{\mathcal{S}_{1}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{ \mathcal{T}_{1}}[f_{\theta}(\mathbf{x})]|\). Based on Theorem 2.3, there exists \(\epsilon_{0}\) so that the following equality holds:

\[\Delta_{0}=|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{ \epsilon_{0}}\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta+\epsilon_{0}}(\mathbf{x}) ]|.\] (6)

Note that the distribution shift is unknown, we consider the worst case for model weight perturbation \(\epsilon_{0}\) within \(L_{p}\)-norm perturbation ball with radius \(\rho\) as follows:

\[\Delta_{0}=|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]-\mathbb{E}_{ \epsilon_{0}}\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta+\epsilon_{0}}(\mathbf{x}) ]|\leq\max_{\|\epsilon_{0}\|_{p}\leq\rho}|\mathbb{E}_{\mathcal{S}_{0}}[f_{ \theta+\epsilon_{0}}(\mathbf{x})]-\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}( \mathbf{x})]|,\] (7)

where \(\|\cdot\|_{p}\) represents \(L_{p}\) norm, \(\rho\) and \(p\) are hyperparameters. Note that the feasible region of model weight perturbation \(\epsilon_{0}\) is symmetric, and the neural network prediction is locally linear around parameter \(\theta\), we have \(\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta+\epsilon_{0}}(\mathbf{x})]-\mathbb{E}_{ \mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]\approx-\Big{(}\mathbb{E}_{\mathcal{ S}_{0}}[f_{\theta-\epsilon_{0}}(\mathbf{x})]-\mathbb{E}_{\mathcal{S}_{0}}[f_{ \theta}(\mathbf{x})]\Big{)}\) due to the local linearity.In other words, the absolute operation can be removed if we consider the maximization problem in a symmetric feasible region since there are always non-negative value for the pair perturbation \(\epsilon_{0}\) and \(-\epsilon_{0}\). Therefore, we can further bound \(\Delta_{0}\) as

\[\Delta_{0} \leq \max_{\|\epsilon_{0}\|_{p}\leq\rho}|\mathbb{E}_{\mathcal{S}_{0}}[ f_{\theta+\epsilon_{0}}(\mathbf{x})]-\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}( \mathbf{x})]|\] (8) \[\approx \max_{\|\epsilon_{0}\|_{p}\leq\rho}\mathbb{E}_{\mathcal{S}_{0}}[ f_{\theta+\epsilon_{0}}(\mathbf{x})]-\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}( \mathbf{x})]\stackrel{{\triangle}}{{=}}\mathcal{L}_{RFR,\mathcal{S}_ {0}},\]

Similarly, we can bound the prediction difference for source and target distribution with sensitive group \(A=1\) as follows:

\[\Delta_{1}\leq\max_{\|\epsilon_{1}\|_{p}\leq\rho}\mathbb{E}_{\mathcal{S}_{1}}[ f_{\theta+\epsilon_{1}}(\mathbf{x})]-\mathbb{E}_{\mathcal{S}_{1}}[f_{\theta}( \mathbf{x})]\stackrel{{\triangle}}{{=}}\mathcal{L}_{RFR,\mathcal{S} _{1}}.\] (9)

Therefore, demographic parity for source and target distribution relation is given by \(DP_{\mathcal{T}}\leq DP_{\mathcal{S}}+\mathcal{L}_{RFR,\mathcal{S}_{0}}+ \mathcal{L}_{RFR,\mathcal{S}_{1}}\), we propose _robust fairness regualarization_ (RFR) to achieve robust fairness as

\[\mathcal{L}_{RFR}=\mathcal{L}_{RFR,\mathcal{S}_{0}}+\mathcal{L}_{RFR,\mathcal{S} _{1}}.\] (10)

It is worth noting that our proposed RFR is agnostic to the training loss function and model architectures. Additionally, we follow [12] to accelerate model training using sharpness-aware minimization via trading maximization problem can be simplified as two forward and two backward propagations. More details on training acceleration are in Appendix E.

### The Proposed Method

In this subsection, we introduce how to use the proposed RFR to achieve robust fairness, i.e., the fair model (low \(\Delta DP_{\mathcal{S}}\)) trained on the source dataset will also be fair on the target dataset (low \(\Delta DP_{\mathcal{T}}\)). Considering binary sensitive attribute \(A\in\{0,1\}\) and binary classification problem \(Y\in\{0,1\}\), the classification loss is denoted as

\[\mathcal{L}_{CLF}=\mathbb{E}_{\mathcal{S}}[-Yf_{\theta}(X)-(1-Y)\big{(}1-f_{ \theta}(X)\big{)}].\] (11)

To achieve fairness, we consider demographic parity as fairness regularization, i.e.,

\[\mathcal{L}_{DP}=|\mathbb{E}_{\mathcal{S}_{0}}[f_{\theta}(\mathbf{x})]-\mathbb{E }_{\mathcal{S}_{1}}[f_{\theta}(\mathbf{x})]|,\] (12)

[MISSING_PAGE_FAIL:6]

gender as the sensitive attribute and the task is to predict whether the income of the person is higher than \(850k\) or not. **ACS-Income**[14] is extracted from the American Community Survey (ACS) Public Use Microdata Sample (PUMS) with \(3,236,107\) samples. We choose gender as the sensitive attribute. Similar to the task in UCI Adult, the task is to predict whether the individual income is above \(850k\). **ACS-Employment**[14] also derives from ACS PUMS, and we also use gender as the sensitive attribute. The task is to predict whether an individual is employed.

Evaluation Metrics.We use accuracy to evaluate the prediction performance for the downstream task. For fairness metrics, we adopt two common-used quantitative group fairness metrics to measure the prediction bias [15; 16], i.e., _demographic parity_\(\Delta_{DP}=|\mathbb{P}(\hat{Y}=1|A=0)-\mathbb{P}(\hat{Y}=1|A=1)|\) and _equal opportunity_\(\Delta_{EO}=|\mathbb{P}(\hat{Y}=1|A=0,Y=1)-\mathbb{P}(\hat{Y}=1|A=1,Y=1)|\), where \(A\) represents sensitive attribute, \(Y\) and \(\hat{Y}\) represent the ground-truth label and predicted label, respectively.

Baselines.In our experiments, we consider vanilla multi-layer perceptron (MLP) and two widely adopted in-processing debiasing methods, including fairness regularization (REG), adversarial debiasing (ADV), and fair consistency regularization (FCR). **MLP** directly uses vanilla 3-layer MLP with \(50\) hidden unit and ReLU activation function [17] to minimize cross-entropy loss with source dataset. In the experiments, we adopt the same model architecture for all other methods (i.e., REG and ADV). **REG** adds a fairness-related metric as a regularization term in the objective function to mitigate the prediction bias [18; 19]. Specifically, we directly adopt demographic parity as a regularization term, and the objective function is \(\mathcal{L}_{CLF}+\lambda\mathcal{L}_{DP}\), where \(\mathcal{L}_{CLF}\) and \(\mathcal{L}_{DP}\) are defined in Eqs. (11) and (12). **ADV**[20] employs a two-player game to mitigate bias, in which a classification network is trained to predict labels based on input features, and an adversarial network takes the output of the classification network as input and aims to identify which sensitive attribute group the sample belongs to. **FCR**[21] aims to minimize and balance consistency loss across groups.

Synthetic and Real Distribution Shift.We adopt synthetic and real distribution shifts. For synthetic distribution shift, we follow work [22; 23] to generate distribution shift via biased sampling. Specifically, we adopt applying principal component analysis (PCA) [24] to retrieve the first principal component \(\mathcal{C}\) from input attributes. Subsequently, we estimate the mean \(\mu(\mathcal{C})\) and standard deviation \(\sigma(\mathcal{C})\), and set Gaussian distribution \(\mathcal{N}_{\mathcal{S}}(\mu(\mathcal{C}),\sigma(\mathcal{C}))\) to randomly sampling of target dataset. As for source dataset, we choose different parameters \(\alpha\) and \(\beta\) to generate distribution shift using another Gaussian distribution \(\mathcal{N}_{\mathcal{T}}(\mu(\mathcal{C})+\alpha,\frac{\sigma(\mathcal{C})}{ \beta})\) for randomly sampling. The source and target datasets are constructed by sampling without replacement. For real distribution shift, we adopt the sampling

Figure 3: DP and Acc trade-off performance on three real-world datasets with temporal (Top) and spatial (Bottom) distribution shift. The trade-off curve close to the right bottom corner means better trade-off performance. The units for x- and y-axis are percentages (\(\%\)).

and pre-processing approaches following _Folktables_[14] to generate spatial and temporal distribution shift via data partition based on different US states and year from \(2014\) to \(2018\).

Implementation Details.We run the experiments \(5\) times and report the average performance for each method. We adopt Adam optimizer with \(10^{-5}\) learning rate and \(0.01\) weight decay for all models. For baseline ADV, we alternatively train classification and adversarial networks with \(70\) and \(30\) epochs, respectively. The hyperparameters for ADV are set as \(\{0.0,1.0,10.0,100.0,500.0\}\). For adding regularization, we adopt the hyperparameters set \(\{0.0,0.5,1.0,10.0,30.0,50.0\}\).

### Experimental Results on Synthetic Distribution Shift

In this experiment, we evaluate the effectiveness of our proposed Robust Fairness Regularization (RFR) method with synthetic distribution shifts via biased sampling with different parameters \((\alpha,\beta)\). We compare the performance of RFR with several other baseline methods, including standard training, adversarial training, and fairness regularization methods. The results of performance value are presented in Table 1, and the results of fairness-accuracy tradeoff are presented in Figure 2. We have the following observations:

* The results in Table 1 demonstrate that RFR consistently outperforms the baselines in terms of fairness for small distribution shifts, achieving a better balance between fairness and accuracy. For example, RFR achieves an improvement of \(47.0\%\) on metric \(\Delta DP\) and \(84.8\%\) on metric \(\Delta EO\) compared to the second-best method in the Adult dataset with \((1.0,2.0)\)-synthetic distribution shift. Furthermore, we observed that the outperforms of RFR compared with baselines decrease as the distribution shift intensity increases. The reason is that the approximation RFR involved Taylor expansion over model perturbation and is effective with mild distribution shifts.
* The results in Figure 1 show that RFR achieved a better fairness-accuracy tradeoff compared to the baseline methods for mild distribution shift, We observed that our proposed method achieved a better Pareto frontier compared to the existing methods, and the bias can be mitigated with tiny accuracy drop.

The experimental results show that our method can effectively address the fairness problem under mild distribution shift while maintaining high accuracy, outperforming the existing state-of-the-art baseline methods.

### Experimental Results on Real Distribution Shift

In this experiment, we evaluate the performance of RFR on multiple real-world datasets with real distribution shifts. We use ACS dataset in the experiment. The distribution of this dataset varies across different time periods or geographic locations, which also causes fairness performance degradation under the distribution shift. The results are presented in Table 2, and the results of the fairness-accuracy tradeoff are presented in Figure 3. The results show that our method consistently outperforms the baselines across all datasets, achieving a better balance between fairness and accuracy. Specifically, we have the following observations:

* Table 2 demonstrates that RFR consistently outperforms the baselines across all datasets in terms of fairness-accuracy tradeoff. This suggests that our method is effective in achieving robust fairness under real temporal and spatial distribution shifts. For example, RFR achieved an improvement of \(34.9\%\) on metric \(\Delta DP\) and \(34.4\%\) on metric \(\Delta EO\) compared to the second-best method in ACS-I dataset with temporal distribution shift.
* Figure 3 shows that RFR can achieve better fairness-accuracy tradeoff than baselines, i.e., RFR is particularly effective in addressing the fairness problem on the ACS dataset with source/target data varying across different time periods or geographic locations. This is important for many practical applications, where fairness is a critical requirement, such as in credit scoring, and loan approval.
* The variance for spatial shift on ACS-I is higher than that of temporal shift on all methods, which indicates neural networks easily converge to different local minima for spatial distribution shift. The proposed methods can achieve better fairness results for most cases and the tradeoff results clearly demonstrate the effectiveness.

Overall, the experimental results on temporal and spatial distribution shifts further support the effectiveness of our proposed method RFR and its potential for practical applications in diverse settings.

### Hyperparameter Study

In this experiment, we investigate the sensitivity of the hyperparameter \(\lambda\) in Equation \(\mathcal{L}_{all}=\mathcal{L}_{CLF}+\lambda\big{(}\mathcal{L}_{DP}+\mathcal{L}_{ RFR}\big{)}\) for spatial and temporal distribution shift across different datasets. Specifically, we tune the hyperparameter as \(\lambda=\{0.0,0.1,0.5,1.0,3.0,5.0,10.0\}\). From the results in Figure 4, it is seen that the accuracy and demographic parity are both sensitive to hyperparameter \(\lambda\), which implies the capability of accuracy-fairness control. With the increase of \(\lambda\), accuracy decreases while demographic parity also decreases. When \(\lambda\) is smaller than \(5\), accuracy drops slowly while demographic parity drops faster. Such observation represents that an appropriate hyperparameter can mitigate prediction bias while preserving comparable prediction performance.

## 5 Related Work

In this section, we present two lines of related work, including fairness in machine learning and distribution shift.

Fairness in Machine Learning.Fairness [259 -31] is a legal requirement for machine learning models for various high-stake real-world predictions, such as healthcare [7; 32; 33], education [34; 35; 36], and job market [37; 38]. Achieving fairness, either from a data or model perspective [39; 40; 41; 42], in machine learning is a challenging problem. As such, there has been an increasing interest in both the industrial and research community to develop algorithms to mitigate these issues and ensure that machine learning models make fair and unbiased decisions. Extensive efforts led to the development of various techniques and metrics for fairness and proposed various definitions of fairness, such as group fairness [43; 44; 45; 46; 47; 40], individual fairness [48; 49; 50; 51; 52; 53], and counterfactual fairness [54; 55; 56]. In this paper, we focus on group fairness, and the widely used methods to achieve group fairness are fair regularization and adversarial debias method. [19; 18] proposed to add a fairness regularization term

\begin{table}
\begin{tabular}{c|c|c c c c c c} \hline \hline \multirow{2}{*}{Real} & \multirow{2}{*}{Methods} & \multicolumn{4}{c}{ACS-I} & \multicolumn{4}{c}{ACS-E} \\ \cline{3-8}  & & Acc (\%0)\(\uparrow\) & \(\Delta_{DP}\) (\%) \(\downarrow\) & \(\Delta_{EO}\) (\%) \(\downarrow\) & Acc (\%) & \(\Delta_{DP}\) (\%) \(\downarrow\) & \(\Delta_{EO}\) (\%) \(\downarrow\) \\ \hline \multirow{4}{*}{\(2016\to 2018\)} & MLP & 77.75\(\pm\)0.44 & 3.26\(\pm\)0.38 & 3.48\(\pm\)0.41 & 80.46\(\pm\)0.05 & 1.07\(\pm\)0.10 & 1.02\(\pm\)0.10 \\  & REG & 77.74\(\pm\)0.62 & 2.09\(\pm\)0.21 & 2.27\(\pm\)0.24 & 80.37\(\pm\)0.12 & 0.77\(\pm\)0.08 & 0.74\(\pm\)0.08 \\  & ADV & 75.94\(\pm\)0.40 & 2.41\(\pm\)0.49 & 2.53\(\pm\)0.55 & 79.62\(\pm\)0.14 & 1.17\(\pm\)0.14 & 1.10\(\pm\)0.14 \\  & FCR & 76.40\(\pm\)0.45 & 2.81\(\pm\)0.30 & 2.96\(\pm\)0.30 & 79.59\(\pm\)0.38 & 0.95\(\pm\)0.42 & 0.91\(\pm\)0.34 \\  & RFR & 77.49\(\pm\)0.32 & **1.36\(\pm\)**0.17 & **1.49\(\pm\)**0.17 & 80.36\(\pm\)0.05 & **0.61\(\pm\)**0.11 & **0.58\(\pm\)**0.10 \\ \hline \multirow{4}{*}{\(\text{MI}\rightarrow\text{CA}\)} & MLP & 75.62\(\pm\)0.80 & 5.22\(\pm\)0.86 & 3.60\(\pm\)0.34 & 79.02\(\pm\)0.20 & 0.73\(\pm\)0.07 & 0.94\(\pm\)0.05 \\  & REG & 75.52\(\pm\)0.78 & 2.88\(\pm\)0.44 & 2.17\(\pm\)0.22 & 75.34\(\pm\)1.11 & **0.42\(\pm\)**0.09 & **0.61\(\pm\)**0.11 \\ \cline{1-1}  & ADV & 73.38\(\pm\)0.77 & **1.04\(\pm\)**0.58 & **0.54\(\pm\)**0.38 & 77.56\(\pm\)0.41 & 0.61\(\pm\)0.18 & 0.80\(\pm\)0.13 \\ \cline{1-1}  & FCR & 74.28\(\pm\)0.35 & 5.06\(\pm\)0.62 & 3.67\(\pm\)0.51 & 77.96\(\pm\)0.22 & 0.44\(\pm\)0.14 & 0.67\(\pm\)0.38 \\ \cline{1-1}  & RFR & 74.63\(\pm\)0.45 & 1.35\(\pm\)0.39 & 1.30\(\pm\)0.24 & 78.84\(\pm\)0.21 & 0.44\(\pm\)0.09 & 0.65\(\pm\)0.07 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance comparison with baselines on real temporal (the year 2016 to the year 2018) and spatial (Michigan State to California State) distribution shift. The best and second-best results are highlighted with **hold** and underline, respectively.

Figure 4: The hyperparameters study for \(\lambda\). The **Left** subfigure and **Right** subfigure show the results of accuracy and fairness performance, respectively.

to the objective function to achieve group fairness, and [20] proposes to jointly train a classification network and an adversarial network to mitigate the bias for different demographic groups to achieve group fairness. Overall, ensuring fairness in machine learning is a critical and ongoing research area that will continue to be an important focus for the development of responsible machine learning.

Distribution Shift.Previous work [1; 2; 3; 4; 57] reveals that a classifier trained on a source distribution will perform worse on a given target distribution because of the distribution shift, and recently extensive works have explored the influence of distribution shift in model prediction. Moreover, distribution shift can significantly affect the fairness performance of machine learning models. The sensitivity of fairness to the distribution shift is notorious for the legal requirement. The degraded performance of fair models under a distribution shift would trigger new bias and discrimination issues. There are some works [23; 58; 21; 59; 60] that solve fairness under various distribution shifts. For example, [23] explore the fairness under covariate shift, where the inputs change with the in-distribution label. [60] proposes Shifty algorithms to hold fairness guarantees when the dataset in the deployment environment is out-of-distribution of the training datasets (distribution shift). Our work is different from those prior works from model weight perturbation perspective.

## 6 Conclusion

This paper aims the solve the fairness problem under the distribution shifts from the model weight perturbation perspective. We first establish a theoretical connection between distribution shift, data perturbation, and model weight perturbation, which allowed us to conclude that distribution shift and model perturbation are equivalent. We then propose a sufficient condition for ensuring fairness transference under distribution shift. To explicitly chase such sufficient conditions, we introduce Robust Fairness Regularization (RFR) method based on the established understanding, to achieve robust fairness. Our experiments on both synthetic and real distribution shifts demonstrate the effectiveness of RFR in achieving a better fairness-accuracy tradeoff compared to existing baselines. We believe that our understanding of distribution shift is valuable and intriguing to the development of robust machine learning models, and the proposed RFR approach can be of great practical value to build fair and robust machine learning models in real-world applications.

## 7 Acknowledgement

The authors thank the anonymous reviewers for their helpful comments. The work is in part supported by NSF grants NSF IIS-2224843, IIS-1939716, and IIS-1900990. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.

## References

* [1] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.
* [2] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8340-8349, 2021.
* [3] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. _Advances in Neural Information Processing Systems_, 33:18583-18599, 2020.
* [4] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. _Advances in Neural Information Processing Systems_, 33:11539-11551, 2020.
* [5] L Elisa Celis, Anay Mehrotra, and Nisheeth Vishnoi. Fair classification with adversarial perturbations. _Advances in Neural Information Processing Systems_, 34:8158-8171, 2021.
* [6] Songul Tolan, Marius Miron, Emilia Gomez, and Carlos Castillo. Why machine learning may lead to unfairness: Evidence from risk assessment for juvenile justice in catalonia. In _Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law_, pages 83-92, 2019.
* [7] Muhammad Aurangzeb Ahmad, Arpit Patel, Carly Eckert, Vikas Kumar, and Ankur Teredesai. Fairness in machine learning for healthcare. In _Proceedings of the 26th ACM SIGKDD_, pages 3529-3530, 2020.
* [8] Jeff Johnson, Donald M Truxillo, Berrin Erdogan, Talya N Bauer, and Leslie Hammer. Perceptions of overall fairness: are effects on job performance moderated by leader-member exchange? _Human Performance_, 22(5), 2009.
* [9] Ainhize Barrainkua, Paula Gordaliza, Jose A Lozano, and Novi Quadrianto. A survey on preserving fairness guarantees in changing environments. _arXiv preprint arXiv:2211.07530_, 2022.
* [10] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre-Alvise Rebuffi, Ira Ktena, Krishnamurthy Dj Dvijotham, and Ali Taylan Cemgil. A fine-grained analysis on distribution shift. In _International Conference on Learning Representations_, 2022.
* [11] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [12] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. _arXiv preprint arXiv:2010.01412_, 2020.
* [13] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.
* [14] Frances Ding, Moritz Hardt, John Miller, and Ludwig Schmidt. Retiring adult: New datasets for fair machine learning. _NeurIPS_, 2021.
* [15] Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair autoencoder. _arXiv preprint arXiv:1511.00830_, 2015.
* [16] Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when adversarially learning fair representations. _arXiv preprint arXiv:1707.00075_, 2017.
* [17] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In _ICML_, 2010.
* [18] Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier with prejudice remover regularizer. In _Joint European conference on machine learning and knowledge discovery in databases_. Springer, 2012.

* [19] Ching-Yao Chuang and Youssef Mroueh. Fair mixup: Fairness via interpolation. In _ICLR_, 2020.
* [20] Gilles Louppe, Michael Kagan, and Kyle Cranmer. Learning to pivot with adversarial networks. _NeurIPS_, 30, 2017.
* [21] Bang An, Zora Che, Mucong Ding, and Furong Huang. Transferring fairness under distribution shifts via fair consistency regularization. _arXiv preprint arXiv:2206.12796_, 2022.
* [22] Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmittfull, Karsten Borgwardt, and Bernhard Scholkopf. Covariate shift by kernel mean matching. _Dataset shift in machine learning_, 3(4):5, 2009.
* [23] Ashkan Rezaei, Anqi Liu, Omid Memarrast, and Brian D Ziebart. Robust fairness under covariate shift. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9419-9427, 2021.
* [24] Herve Abdi and Lynne J Williams. Principal component analysis. _Wiley interdisciplinary reviews: computational statistics_, 2(4):433-459, 2010.
* [25] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey on bias and fairness in machine learning. _ACM Computing Surveys (CSUR)_, 54(6):1-35, 2021.
* [26] Dana Pessach and Erez Shmueli. A review on fairness in machine learning. _ACM Computing Surveys (CSUR)_, 55(3):1-44, 2022.
* [27] Michael Madaio, Lisa Egede, Hariharan Subramonyam, Jennifer Wortman Vaughan, and Hanna Wallach. Assessing the fairness of ai systems: Ai practitioners' processes, challenges, and needs for support. _Proceedings of the ACM on Human-Computer Interaction_, 6(CSCW1):1-26, 2022.
* [28] Dexun Li and Pradeep Varakantham. Efficient resource allocation with fairness constraints in restless multi-armed bandits. In James Cussens and Kun Zhang, editors, _Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands_, volume 180 of _Proceedings of Machine Learning Research_, pages 1158-1167. PMLR, 2022.
* [29] Kirtan Padh, Diego Antognini, Emma Lejal Glaude, Boi Faltings, and Claudiu Musat. Addressing fairness in classification with a model-agnostic multi-objective algorithm. In Cassio P. de Campos, Marloes H. Maathuis, and Erik Quaeghebeur, editors, _Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI 2021, Virtual Event, 27-30 July 2021_, volume 161 of _Proceedings of Machine Learning Research_, pages 600-609. AUAI Press, 2021.
* [30] Zhimeng Jiang, Xiaotian Han, Chao Fan, Zirui Liu, Na Zou, Ali Mostafavi, and Xia Hu. Fmp: Toward fair graph message passing against topology bias. _arXiv preprint arXiv:2202.04187_, 2022.
* [31] Xiaotian Han, Zhimeng Jiang, Hongye Jin, Zirui Liu, Na Zou, Qifan Wang, and Xia Hu. Retiring \(\Delta\)DP: New distribution-level metrics for demographic parity. _Transactions on Machine Learning Research_, 2023.
* [32] Margret Vilborg Bjarnadottir and David Anderson. Machine learning in healthcare: Fairness, issues, and challenges. In _Pushing the Boundaries: Frontiers in Impactful OR/OM Research_, pages 64-83. INFORMS, 2020.
* [33] Thomas Grote and Geoff Keeling. Enabling fairness in healthcare through machine learning. _Ethics and Information Technology_, 24(3):39, 2022.
* [34] Steinar Boyum. Fairness in education-a normative analysis of oecd policy documents. _Journal of Education Policy_, 29(6):856-870, 2014.
* [35] Paolo Brunori, Vito Peragine, and Laura Serlenga. Fairness in education: The italian university before and after the reform. _Economics of Education Review_, 31(5):764-777, 2012.

* [36] Rene F Kizilcec and Hansol Lee. Algorithmic fairness in education. In _The ethics of artificial intelligence in education_, pages 174-202. Routledge, 2022.
* [37] Lily Hu and Yiling Chen. A short-term intervention for long-term fairness in the labor market. In _Proceedings of the 2018 World Wide Web Conference_, pages 1389-1398, 2018.
* [38] G Stoney Alder and Joseph Gilbert. Achieving ethics and fairness in hiring: Going beyond the law. _Journal of Business Ethics_, 68:449-464, 2006.
* [39] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence: A survey. _arXiv preprint arXiv:2303.10158_, 2023.
* [40] Qizhang Feng, Zhimeng Jiang, Ruiquan Li, Yicheng Wang, Na Zou, Jiang Bian, and Xia Hu. Fair graph distillation. 2023.
* [41] Chia-Yuan Chang, Yu-Neng Chuang, Zhimeng Jiang, Kwei-Herng Lai, Anxiao Jiang, and Na Zou. Coda: Temporal domain generalization via concept drift simulator. _arXiv preprint arXiv:2310.01508_, 2023.
* [42] Chia-Yuan Chang, Yu-Neng Chuang, Kwei-Herng Lai, Xiaotian Han, Xia Hu, and Na Zou. Towards assumption-free bias mitigation. _arXiv preprint arXiv:2307.04105_, 2023.
* [43] Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. _NeurIPS_, 29, 2016.
* [44] Sahil Verma and Julia Rubin. Fairness definitions explained. In _2018 ieee/acm international workshop on software fairness (fairware)_, pages 1-7. IEEE, 2018.
* [45] Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. On dyadic fairness: Exploring and mitigating bias in graph connections. In _International Conference on Learning Representations_, 2020.
* [46] Gaurush Hiranandani, Jatin Mathur, Harikrishna Narasimhan, and Oluwasanmi Koyejo. Quadratic metric elicitation for fairness and beyond. In James Cussens and Kun Zhang, editors, _Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands_, volume 180 of _Proceedings of Machine Learning Research_, pages 811-821. PMLR, 2022.
* [47] Zhimeng Jiang, Xiaotian Han, Chao Fan, Fan Yang, Ali Mostafavi, and Xia Hu. Generalized demographic parity for group fairness. In _International Conference on Learning Representations_, 2022.
* [48] Mikhail Yurochkin, Amanda Bower, and Yuekai Sun. Training individually fair ml models with sensitive subspace robustness. _International Conference on Learning Representations_, 2020.
* [49] Debarghya Mukherjee, Mikhail Yurochkin, Moulinath Banerjee, and Yuekai Sun. Two simple ways to learn individual fairness metrics from data. In _ICML_, 2020.
* [50] Mikhail Yurochkin and Yuekai Sun. Sensei: Sensitive set invariance for enforcing individual fairness. In _International Conference on Learning Representations_, 2021.
* [51] Jian Kang, Jingrui He, Ross Maciejewski, and Hanghang Tong. Inform: Individual fairness on graph mining. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 379-389, 2020.
* [52] Debarghya Mukherjee, Felix Petersen, Mikhail Yurochkin, and Yuekai Sun. Domain adaptation meets individual fairness. and they get along. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [53] Shantanu Das, Swapnil Dhamal, Ganesh Ghalme, Shweta Jain, and Sujit Gujar. Individual fairness in feature-based pricing for monopoly markets. In James Cussens and Kun Zhang, editors, _Uncertainty in Artificial Intelligence, Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence, UAI 2022, 1-5 August 2022, Eindhoven, The Netherlands_, volume 180 of _Proceedings of Machine Learning Research_, pages 486-495. PMLR, 2022.

* Kusner et al. [2017] Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. _NeurIPS_, 30, 2017.
* Agarwal et al. [2021] Chira Agarwal, Himabindu Lakkaraju, and Marinka Zitnik. Towards a unified framework for fair and stable graph representation learning. In _UAI 2021: Uncertainty in Artificial Intelligence_, 2021.
* Zuo et al. [2022] Aoqi Zuo, Susan Wei, Tongliang Liu, Bo Han, Kun Zhang, and Mingming Gong. Counterfactual fairness with partially known causal graph. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Chang et al. [2023] Chia-Yuan Chang, Yu-Neng Chuang, Guanchu Wang, Mengnan Du, and Zou Na. Dispel: Domain generalization via domain-specific liberating. _arXiv preprint arXiv:2307.07181_, 2023.
* Schrouff et al. [2022] Jessica Schrouff, Natalie Harris, Oluwasanmi Koyejo, Ibrahim Alabdulmohsin, Eva Schnider, Krista Opsahl-Ong, Alex Brown, Subhrajit Roy, Diana Mincu, Christina Chen, et al. Maintaining fairness across distribution shift: do we have viable solutions for real-world applications? _arXiv preprint arXiv:2202.01034_, 2022.
* Singh et al. [2021] Harvineet Singh, Rina Singh, Vishwali Mhasawade, and Rumi Chunara. Fairness violations and mitigation under covariate shift. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 3-13, 2021.
* Giguere et al. [2022] Stephen Giguere, Blossom Metevier, Bruno Castro da Silva, Yuriy Brun, Philip Thomas, and Scott Niekum. Fairness guarantees under demographic shift. In _International Conference on Learning Representations_, 2022.
* Zhu et al. [2023] Jiongli Zhu, Sainyam Galhotra, Nazanin Sabri, and Babak Salimi. Consistent range approximation for fair predictive modeling. _Proceedings of the VLDB Endowment_, 16(11):2925-2938, 2023.
* Du et al. [2022] Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent YF Tan. Efficient sharpness-aware minimization for improved training of neural networks. _International Conference on Learning Representations_, 2022.
* Andriushchenko and Flammarion [2022] Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In _International Conference on Machine Learning_, pages 639-668. PMLR, 2022.
* Lu et al. [2023] Yuzhe Lu, Zhenlin Wang, Runtian Zhai, Soheil Kolouri, Joseph Campbell, and Katia Sycara. Predicting out-of-distribution error with confidence optimal transport. _arXiv preprint arXiv:2302.05018_, 2023.
* Yu et al. [2022] Yaodong Yu, Zitong Yang, Alexander Wei, Yi Ma, and Jacob Steinhardt. Predicting out-of-distribution error with the projection norm. In _International Conference on Machine Learning_, pages 25721-25746. PMLR, 2022.