Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model

Wenjia Xie Hao Wang1 Luankang Zhang Rui Zhou Defu Lian Enhong Chen

University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence

xiaohulu@mail.ustc.edu.cn,wanghao3@ustc.edu.cn,zhanglk5@mail.ustc.edu.cn, zhou_rui@mail.ustc.edu.cn,liandefu@ustc.edu.cn,cheneh@ustc.edu.cn

Footnote 1: Hao Wang is the corresponding author.

###### Abstract

Sequential recommendation (SR) aims to predict items that users may be interested in based on their historical behavior sequences. We revisit SR from a novel information-theoretic perspective and find that conventional sequential modeling methods fail to adequately capture the randomness and unpredictability of user behavior. Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users' real interests. Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains. It is better suited for discrete data, using structured transitions instead of arbitrary noise introduction to avoid information loss. Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues. Testing on three public benchmark datasets shows that DDSR outperforms existing state-of-the-art methods in various settings, demonstrating its potential and effectiveness in handling SR tasks.

## 1 Introduction

For a long time, sequential recommendation (SR) has been attracting increasing attention due to its excellent performance and significant commercial value (Chen et al. (2020), Qiu et al. (2021), Yin et al. (2024), Han et al. (2024)). Unlike traditional collaborative filtering or certain graph-based methods (Wang et al. (2019), Zhang et al. (2024), Wang et al., Tong et al. (2024)), SR systems emphasize the inherent dynamic behaviors of users rather than relying solely on structured data (Chen et al. (2022), Ma et al. (2020), Cen et al. (2020)). This approach enhances the accuracy of personalized recommendations, allowing for more precise tracking of changes in user interests and needs. Typical deep learning-based SR models, such as those utilizing CNN, RNN, and Transformer architectures (Tang and Wang (2018), Hidasi et al. (2015), Kang and McAuley (2018)), have achieved remarkable success in modeling user historical interaction data.

However, these methods are formalized models based on a narrow information theory assumption (Shannon (1948)), which only acknowledges determinism (Rosas et al. (2020)). They assume that all phenomena strictly adhere to mechanical laws and that the states of motion of objects at different times can be uniquely determined. In reality, however, user behavior is characterized by randomness and unpredictability. They might change their mind about buying a down jacket due to a sudden warm-up, or they might impulsively buy desserts due to a breakup. As illustrated on the left in Figure 1, a user's interest at any given moment might be focused on'some items' with blurred boundaries, only converging finally when the user makes a selection.

Although increasing the sample size is an effective strategy to address the above issue, in reality, the data in recommendation systems is usually quite sparse (He and McAuley (2016)), limiting the practicality of this strategy. Inspired by the theory of fuzzy information processing (Tanaka et al. (1976); Tanaka and Sommer (1977)), we believe that making the absolute membership relations in traditional sets more flexible is another effective way to solve the problem. In other words, it is not necessary to strictly limit the modeling of user interests to the items they have interacted with. Therefore, we propose using a diffusion model (Ho et al. (2020)) for fuzzy modeling of user interests, which enhances the model's performance by introducing perturbations during the training process.

We have noticed existing work that introduces diffusion models into SR (Xie et al. (2024)), such as DiffuRec (Li et al. (2023)) and DreamRec (Yang et al. (2023)), which focus on Gaussian diffusion processes operating within a continuous state space. They add Gaussian noise to the embedded representations of candidate items for recommendation through a forward diffusion process until the noise reaches a pure state (standard normal distribution). Subsequently, they iteratively sample from this noise using a reverse denoising process guided by historical interaction information to recover meaningful representations and recommend items most similar to these representations.

However, unlike our desire to fuzzily model interaction sequences, the aforementioned methods follow the form of diffusion models in the image domain and operate on candidate items. They introduce the crucial sequence information merely as conditional information, without leveraging the diffusion model's performance on it. On the other hand, these methods relax discrete interaction data into a continuous space and introduce noise, which may lead to distortion or loss of meaning in the original discrete space, as the addition of noise could push data points away from any meaningful discrete state. Therefore, we hope that state transitions occur under discrete conditions for the entire interaction sequence, which is discrete diffusion. Based on this, we have proposed our DCSR (Discrect Diffusion Sequential Recommendation model), which uses a directed graph to model sequential recommendation. In this model, all interaction items are viewed as nodes, and transitions between items are treated as directed edges. Discrete diffusion is used to enable structured transitions of nodes, with the resulting new sets treated as fuzzy sets, as shown in the middle of Figure 1. By designing the transition matrix, we can achieve uniform transitions or importance-based transitions for the nodes, ensuring controllability. In Section 3.3, we theoretically demonstrate the reliability and effectiveness of modeling on these generated fuzzy sets, based on the principles of information diffusion. During the inference stage, we refer to the sampling formula for discrete diffusion but start from the historical interaction sequence rather than from noise, iteratively generating refined results.

Furthermore, we have found that the excessive number of items involved in the recommendation problem leads to a high-dimensional transition matrix, resulting in inefficient diffusion transitions. Additionally, item IDs themselves do not contain any prior information, which poses a challenge in determining beneficial transition directions. To address this issue, we have further introduced semantic tags to replace meaningless item IDs, using quantization techniques and VQ-VAE to derive

Figure 1: Illustration of DDSR constructing fuzzy sets and incorporating semantic IDs to enhance sequential recommendations. In real-world scenarios, a user’s final choice often reflects their immediate interests (left subfigure). We reconstruct the true evolution of interests by constructing fuzzy sets for each item in the interaction sequence (middle subfigure). The right subfigure provides an overview of the process of generating semantic IDs for recommendations based on item-related descriptions.

these tags from semantic information, thus reducing the size of the discrete space. We will provide specific details on how this can be achieved in 4.1, and a vivid illustration of this is given on the right side of Figure 1. Simultaneously, the introduction of semantic information has enhanced the model's generalization capability and effectively solved the cold start problem. We conducted extensive experiments on three public benchmark datasets, comparing DDSR with several state-of-the-art methods. The results demonstrate that DDSR significantly outperforms baseline methods in various settings and effectively handles cold-start recommendations.

## 2 Related Work

### Sequence Recommendation

SR suggests potential subsequent items based on users' historical interaction records (Yin et al. (2023); Wang et al. (2024)). Early research primarily relied on Markov chains and matrix factorization techniques for recommendation (He et al. (2016)). However, with the development of deep learning, efforts such as GRU4Rec (Hidasi et al. (2015)), Caser (Tang and Wang (2018)), and others have focused on designing neural network models to capture sequential dependencies in user behavior sequences. The introduction of the Transformer architecture (Vaswani et al. (2017)) in SASRec (Kang and McAuley (2018)) pioneered SR and quickly became the mainstream method in the field. Additionally, BERT4Rec (Sun et al. (2019)) utilizes bidirectional encoders to capture bidirectional dependencies in sequences, using a masked language model to predict the user's next action.

Recent studies have shown that high-quality high-dimensional embeddings are crucial for obtaining accurate recommendation results (Hou et al. (2022); Wang et al. (2021)). To this end, researchers are striving to leverage the rich attribute information of items to improve data representation. For example, TransFM (Pasricha and McAuley (2018)) introduces arbitrary real-valued features through factorization machines, while S3-Rec (Zhou et al. (2020)) designs four self-supervised learning tasks as pre-training objectives to learn context-aware data representations with attribute awareness. Furthermore, researchers like Hou et al. (2022); Zhao (2022); Harte et al. (2023) further utilize pre-trained language models (Yin et al. (2024)) to process item description texts, obtaining universal item representations with rich semantic information to enhance the performance (Wu et al. (2024)). VQ-Rec (Hou et al. (2023)) and TIGER (Rajput et al. (2024)) further employ quantization techniques (Jacob et al. (2018)) and RQ-VAE (Lee et al. (2022)) to obtain tokenized semantic IDs for recommendations, replacing semantic embeddings.

### Discrete Diffusion Models

Diffusion models, inspired by non-equilibrium thermodynamics, have been introduced and demonstrated significant results in fields such as computer vision, sequence modeling, and audio processing (Dhariwal and Nichol (2021); Rasul et al. (2021); Ho et al. (2022)). Most diffusion models are based on the Denoising Diffusion Probabilistic Model (DDPM) proposed by Ho et al. (2020), as well as the score-based generative models (SGMs) proposed by Song et al. (2020), targeting continuous data domains. We provide detailed descriptions of DDPM and SGMa in Appendix D to facilitate comparisons with the discrete diffusion approach we employ. Diffusion models in discrete state space are first described in Sohl-Dickstein et al. (2015) and later applied to text and image domains in D3PMs (Austin et al. (2021)). VQ-Diffusion (Gu et al. (2022)) utilizes them to eliminate unidirectional bias in text-to-image generation.

## 3 Discrete Diffusion Process of DDSR

In this section, we present the problem definition (Section 3.1) and illustrate how item sequences undergo discrete diffusion to obtain the corresponding fuzzy sets (Section 3.2). Finally, the effectiveness of this fuzzy modeling is theoretically demonstrated (Section 3.3). Please note that the actual diffusion and inference in DDSR occur at the semantic ID level, but this chapter discusses items.

### Problem Statement

Let \(\mathcal{U}\) be the set of users and \(\mathcal{V}\) be the set of discrete items in the dataset, \(|\mathcal{U}|\) and \(|\mathcal{V}|\) represent the number of elements in their respective sets. For each user \(\mathbf{u}\in\mathcal{U}\), \(v_{1:n-1}=[v_{1},v_{2},\ldots,v_{n-1}]\)represents his historical interaction sequence sorted by timestamp. The goal of the model is to predict the next item \(v_{n}\) that the user is most likely to interact with. To facilitate better discrete diffusion and for the convenience of subsequent theoretical derivations, we model each user's interaction sequence as a directed graph \(\mathcal{G}^{u}\). In this graph, each item represented by a semantic ID is regarded as a node, while transitions between items are viewed as directed edges. Specifically, an edge exists from \(v_{i}\) to \(v_{j}\) if and only if \(v_{j}\) is the next item interacted with by the user after \(v_{i}\).

### Node Diffusion Transition

A typical diffusion model transforms data \(x_{0}\sim q(x_{0})\) into a sequence of gradually noisier latent variables \(x_{1:T}=x_{1},x_{2},...,x_{T}\) via forward process \(q(\mathbf{x}_{1:T}|\mathbf{x}_{0})=\prod_{t=1}^{T}q(\mathbf{x}_{t}|\mathbf{x}_{t-1})\). In diffusion models within continuous state spaces, the forward distribution is typically set with \(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathcal{N}\left(\mathbf{x}_{t}|\sqrt{1-\beta_{t}}\mathbf{ x}_{t-1},\beta_{t}\mathbf{I}\right)\) as a hyperparameter controlling the level of noise added at each step. As the number of time steps \(T\) approaches infinity, \(x_{T}\) converges to a standard Gaussian distribution. Beyond the limitations mentioned above, information loss due to diffusion into pure noise is another reason for unstable training and inadequate alignment of continuous diffusion with SR.

Continuous diffusion always operates on embeddings, while in diffusion models within discrete state spaces, categories are directly transformed. Transition matrices \([\mathbf{Q}_{t}]_{ij}=q(x_{t}=j|x_{t-1}=i)\) are used to describe the probability of single-step diffusion transitions, where \(i\) and \(j\) represent categories within the domain. Denoting the one-hot version of \(x\) with the row vector \(\mathbf{x}\) (bold), then the one-step transition probabilities can be expressed as:

\[q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathrm{Cat}(\mathbf{x}_{t};\mathbf{p}=\mathbf{x}_{t-1}\mathbf{Q}_{t }), \tag{1}\]

where \(Cat(\mathbf{x};p)\) is the categorical distribution corresponding to the one-hot row vector \(\mathbf{x}\) with probabilities given by the row vector \(p\), and \(\mathbf{x}_{t-1}\mathbf{Q}_{t}\) is understood as a row vector-matrix product. Starting from \(\mathbf{x}_{0}\), we obtain the following \(t\)-step marginal and posterior at time \(t-1\):

\[q(\mathbf{x}_{t}|\mathbf{x}_{0})=\text{Cat}\left(\mathbf{x}_{t};\mathbf{p}=\mathbf{x}_{0}\mathbf{ \overline{Q}}_{t}\right),\quad\text{with}\quad\mathbf{\overline{Q}}_{t}=\mathbf{Q}_{1} \mathbf{Q}_{2}\dots\mathbf{Q}_{t}, \tag{2}\]

We take the set \(\mathcal{V}\) as the domain in SR. Each \(v_{i}\) in the interaction sequence is represented as a one-hot encoding \(\mathbf{x}_{i}^{0}\). Using the transition form defined by 2 enables transitions to other nodes, denoted as \(\mathbf{x}_{i}^{t}\) in the domain with a certain probability at any time step \(t\). Unlike continuous diffusion, which only allows noise addition, discrete diffusion models offer the advantage of controlling the data blurring process by selecting the transition matrix. Here, we present two strategies to select transition matrices, i.e. Uniform transition and Importance transition.

**Uniform transition.** Similar to the study by Hoogeboom et al. (2021), the natural idea is to maintain nodes with a certain probability \(\beta_{t}\in(0,1)\) unchanged. In contrast, in other cases, nodes are randomly transformed into any other node in the domain with equal probability \((1-\beta_{t})/(|\mathcal{V}|-1)\). That is

\[[\mathbf{Q}_{t}]_{ij}=\begin{cases}(1-\beta_{t})/(|\mathcal{V}|-1)&\text{if }i \neq j\\ \beta_{t}&\text{if }i=j\end{cases}. \tag{3}\]

Uniform transfer can be regarded as a special case of linear information allocation, thus theoretically affected by the size of the discrete space. It can compute the cumulative product \(\mathbf{\overline{Q}}_{t}\) in closed form.

**Importance transition.** For data with certain prior knowledge, we propose transitioning between more similar nodes rather than uniformly transitioning to any other state, thus defining the matrix:

\[[\mathbf{Q}_{t}]_{ij}=\begin{cases}\frac{\exp\left(-d_{ij}^{2}/2\sigma^{2}\right) }{\sum_{v_{k}\in\mathbf{V}}\exp\left(-d_{ik}^{2}/2\sigma^{2}\right)}&\text{if }\quad i\neq j\\ 1-\sum_{k=0,k\neq i}^{|\mathbf{V}|-1}[\mathbf{Q}_{t}]_{ik}&\text{if}\quad i=j\end{cases}. \tag{4}\]

Here, \(d_{ij}\) represents the distance between item \(v_{i}\) and \(v_{j}\), calculated using the square of the Euclidean distance. The parameter \(\sigma^{2}\) denotes the variance of the diffusion process. Consequently, the transition probabilities cannot be solved in closed form; instead, they can only be updated alongside the embeddings in the model. The importance transfer matrix adheres to the Gaussian information diffusion function \(f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{x^{2}}{2\sigma^{2}}}\). Therefore, it remains unaffected by the number of discrete points but necessitates the sample point distribution to closely resemble a Gaussian distribution.

This modeling approach appears to align more closely with our intuition, as a user's interests at a given moment often form a cluster of similar nodes (items). Only upon the user's final selection of an item does the 'neighborhood' converge to a single data point. This point represents the representative of the interest cluster and the ambiguity in information naturally dissipates. In recommendation tasks, we can only access the user's final choice at each moment, without knowledge of the interest cluster, reflecting incomplete knowledge. Regardless of the transition method employed, the key lies in transitioning the sample space from incomplete to complete, as detailed in the subsequent section.

### Completeness and Reliability

Here, we aim to demonstrate that the discrete diffusion spaces generated by the two methods in Section 3.2 are completions of the original space, and the models constructed on these fuzzy sets are solvable and effective. We first provide the formal definition of a complete sample space.

**Definition 1**.: _Let \(\mathbf{W}\) denote the sample space. For any sample \(W\in\mathbf{W}\), if \(W\) is complete, i.e., unbiased estimates can be obtained through certain mathematical processing, then \(\mathbf{W}\) is called a complete sample space; otherwise, it is called an incomplete sample space._

In SR, \(W\) is a user's behavior sequence; \(\mathbf{W}\) is all possible combinations of these behavior sequences; the domain \(V\) is all items in the dataset. Datasets in SR systems do not form a complete sample space, as they often consist of incomplete interaction data and potential selection biases. The principle of information diffusion ensures that when the given sample is incomplete, there exist reasonable diffusion functions that can improve non-diffusion estimates. Below we define information diffusion.

**Definition 2**.: _An information diffusion about a set \(W\) is defined by a mapping \(\mu:W\times V\rightarrow[0,1]\), satisfying the following conditions:_

1. \(\forall w_{j}\in W\)_, if_ \(v_{j}\) _is the observed value of_ \(w_{j}\)_, then_ \(\mu(w_{j},v_{j})=\sup_{v\in V}\mu(w_{j},v)\)_._
2. \(\forall w_{j}\in W\)_,_ \(\mu(w_{j},v)\) _decreases as_ \(\|v_{j}-v\|\) _increases._
3. \(\forall w\in W\)_,_ \(\sum_{v}\mu(w,v)\mathrm{d}v=1\)_._

The diffusion estimates obtained using uniform transition and importance transition, as defined in Section 3.2, clearly adhere to Definition 2. To illustrate that the space resulting from discrete state transitions provides more information than the original state space, it is necessary to further demonstrate that this space serves as a completion of the original space. In other words, the new metric space is complete, with the original metric space serving as its dense subspace. This will be more precisely discussed in the following theorem.

**Theorem 3.1**.: _After information diffusion, the subsequent space must be an entirely separable metric space. Any model constructed in this space will assuredly possess a solution._

Proof in Appendix A. According to Theorem 3.1, since the space after information diffusion is equidistant isomorphism with the original space, it can be used to replace the sample space with insufficient information in SR to establish a model. On this complete space, predictive models are solvable, demonstrating that modeling on fuzzy sets is a reasonable and effective approach.

## 4 Learning and Inference of DDSR

### Obtaining Semantic IDs

As mentioned in Section 3, the indices \(i\) and \(j\) in the transition matrix \([\mathbf{Q}_{t}]_{ij}\) represent categories in the discrete space, making \(\mathbf{Q}_{t}\) a two-dimensional matrix with dimensions equal to the size of the discrete space. However, in recommendation tasks, the number of items involved can reach tens of thousands, posing a significant challenge in terms of computational resources if we were to use all item IDs as the discrete state space. Inspired by VQ-Rec and the recently proposed Tiger model by Google, we attempt to train recommendation models using semantic IDs instead of item IDs. A semantic ID is a codebook of length \(m\). Assuming we set the size of the codebook to \(K\), the entire codebook can represent \(K^{m}\) categories. Though we set each code from a different codebook, the state space only needs \(m*K\) nodes to store them. Additionally, the use of semantic IDs further introduces semantic information, addressing the scarcity of information inherent in recommendations, while also allowing the model to extend to unseen items, thus enabling cold-start recommendations. We provide the specific method for obtaining semantic IDs in the Appendix B.

```
0: historical interaction sequence \(v_{1:n-1}=c_{1:n-1;1:m}\); target item \(v_{n}=c_{n;1:m}\); transition matrix \(\mathbf{Q}_{t}\); Approximator \(f_{\theta}(\cdot)\).
0: well-trained Approximator \(f_{\theta}(\cdot)\).
0: While not converged do:
1: Sample Diffusion Time: \(t\sim[0,1,\ldots,T]\);
2: Calculate \(t\)-step transition probability: \(\overline{\mathbf{Q}}_{t}=\mathbf{Q}_{1}\mathbf{Q}_{2}\cdots\mathbf{Q}_{t}\);
3: Convert \(c_{n;1:m}\) to one-hot encoding \(\mathbf{x}^{0}_{n;1:m}\);
4: Obtain the discrete state \(x^{t}_{n;1:m}\) after \(t\) steps by Equation 2, thereby obtaining the 'fuzzy set' \(c^{t}_{1:n-1;1:m}\);
5: Modeling \(c_{2;n;1:m}\) based on 'fuzzy sets' through Equation 5;
6: Take gradient descent step on \(\nabla L_{CE}\left(\hat{c}_{2:n;1:m},c_{2:n;1:m}\right)\).
```

**Algorithm 1** Training of DDSR.

```
0: historical sequence \(c_{1:n-1;1:m}\); well-trained Approximator \(f_{\theta}(\cdot)\); sampling step \(T\).
0: predicted target item \(v_{n}\).
1: Let \(\mathbf{x}_{T}=c_{1:n-1;1:m}\);
2: Let \(t=T\);
3:while\(t>0\)do
4: Use the trained \(f_{\theta}(\cdot)\) to obtain predictions \(\tilde{\mathbf{x}}_{0}\) with \(\mathbf{x}_{t}\) and \(t\) as inputs;
5: Substitute \(\tilde{x}_{0}\) into equation 7 to obtain the distribution of \(t-1\) step;
6:endwhile
7:\(\tilde{v}_{n}=\mathbf{x}_{0}[-1;1:m]\);
8:if the same code project exists: \(v_{n}=\tilde{v}_{n}\);
9:else: \(v_{n}\) is the project in the space closest to \(\tilde{v}_{n}\).
```

**Algorithm 2** Inference of DDSR.

### Model Training

After introducing the Semantic ID, we convert the historical interaction sequence \(v_{1:n-1}\) into sequence \((c_{1,1},\ldots,c_{1,m};c_{2,1},\ldots,c_{2,m};\ldots;c_{n-1,1},\ldots,c_{n-1,m})\), abbreviated as \(c_{1:n-1;1:m}\). We convert them into one-hot encodings \((\mathbf{x}^{0}_{1,1},\ldots,\mathbf{x}^{0}_{1,m};\ldots;\mathbf{x}^{0}_{n-1,1},\ldots,\bm {x}^{0}_{n-1,m})\), which is considered as the initial state for discrete diffusion. Then we perform discrete diffusion through the state transition formula defined in 2 (for more details, see Section 3.2) to obtain the discrete state after \(t\) steps \(\mathbf{x}^{t}_{i,j}\), for any \(i\in\{1,\ldots,n-1\}\) and \(j\in\{1,\ldots,m\}\). Accordingly, the labels changes from \(c_{i,j}\) to \(c^{t}_{i,j}\). Then \((c^{t}_{1,1},\ldots,c^{t}_{1,m};\ldots;c^{t}_{n-1,1},\ldots,c^{t}_{n-1,m})\) forms a "fuzzy set" of \(c_{1:n-1;1:m}\), denoted as \(c^{t}_{1:n-1;1:m}\), which can also be viewed as the state \(x_{t}\) of the diffusion transition at step \(t\).

Considering the suitability of the Transformer for sequence-to-sequence tasks, along with its well-demonstrated effectiveness in modeling sequential dependencies, we use it with an embedding layer as Approximator \(f_{\theta}(\cdot)\) to predict \(c_{2:n;1:m}\) with \(c^{t}_{1:n-1;1:m}\) as input. This approach differs from the common practice in diffusion models, which often focus on modeling noise. It aligns more closely with typical SR tasks that use \(v_{1:n-1}\) to predict \(v_{2:n}\), that is, the distribution \(\tilde{p}_{\theta}(\tilde{\mathbf{x}}_{0}|\mathbf{x}_{t})\). We have adopted sinusoidal time step embeddings, which are added after the embedding layer, allowing the model to capture information about the time steps. This process can be represented by:

\[\hat{c}_{2:n;1:m}=f_{\theta}(c^{t}_{1:n-1;1:m},t). \tag{5}\]

Generally, the loss function of diffusion models is designed based on KL divergence, or it can be simplified to mean-squared error. However, guided by the theory of information diffusion, we choose to use a cross-entropy loss function, which is more suitable for recommendation tasks, to optimize our model without being constrained by the aforementioned methods.

### Model Inference

In the inference phase, we aim to emulate the reverse process of the diffusion model, iteratively producing refined recommendation results. According to Bayes' theorem, we have

\[q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})=\frac{q(\mathbf{x}_{t}|\mathbf{x}_{t-1},\mathbf{x}_{0}) q(\mathbf{x}_{t-1}|\mathbf{x}_{0})}{q(\mathbf{x}_{t}|\mathbf{x}_{0})}=\operatorname{Cat}\left(\mathbf{x}_{t- 1};\mathbf{p}=\frac{\mathbf{x}_{t}\mathbf{Q}_{t}^{\top}\odot\mathbf{x}_{0}\overline{\mathbf{Q}}_{t -1}}{\mathbf{x}_{0}\overline{\mathbf{Q}}_{t}\mathbf{x}_{t}^{\top}}\right), \tag{6}\]

where \(\odot\) represents the Hadamard product. Following the approach of Ho et al. (2020) and Hoogeboom et al. (2021), we employ the trained model \(f_{\theta}(\cdot)\) as described in Section 4.2 to derive the distribution \(\tilde{p}_{\theta}(\tilde{\mathbf{x}}_{0}|\mathbf{x}_{t})\). Combining it with \(q(\mathbf{x}_{t-1}|\mathbf{x}_{t},\mathbf{x}_{0})\), we obtain the following parameterized expression:

\[p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_{t})=\sum_{\tilde{\mathbf{x}}_{0}}q(\mathbf{x}_{t-1}, \mathbf{x}_{t}|\mathbf{\widetilde{x}}_{0})\widetilde{p}_{\theta}(\mathbf{\widetilde{x}}_{0 }|\mathbf{x}_{t}). \tag{7}\]

For the historical interactions \(v_{1:n-1}\), we use \(c_{1:n-1;1:m}\) as \(x_{T}\), and starting from \(t=T\), we iteratively execute Equation 7 until \(t=0\). This parameterization also allows us to perform inference for \(k\) steps at a time by predicting \(p_{\theta}(\mathbf{x}_{t-k}|\mathbf{x}_{t})=\sum q(\mathbf{x}_{t-k},\mathbf{x}_{t}|\mathbf{ \widetilde{x}}_{0})\widetilde{p}_{\theta}(\mathbf{\widetilde{x}}_{0}|\mathbf{x}_{t})\), leading to efficiency improvements. After obtaining \(\mathbf{x}_{0}\), we take its last component, which is a semantic ID of length m. If the corresponding item exists, we directly select that item; otherwise, we search for the item closest to it in the embedding space as the final recommendation result. The training and inference phase of DDSR are demonstrated in Algorithm 1 and Algorithm 2.

## 5 Experiment

### Experiment Settings

**Datasets**. We employ three real-world datasets to evaluate the performance of our DDSR model. Following some works on text-based recommendation (Li et al. (2023); Hou et al. (2022)), these datasets include two specific subcategories from the Amazon Reviews dataset (**Scientific** and **Office**), and a cross-platform dataset known as **Online Retail**, which operates from the UK. Following the method of Hou et al. (2022), we filter out users and items with fewer than five interactions. Subsequently, interaction behaviors within each sub-dataset are grouped by user and sorted chronologically. For the Amazon sub-datasets, product descriptions are formed by concatenating fields such as title, category, and brand, while for the Online Retail dataset, the description field is used. The product texts are truncated to 512 characters. Please refer to Table 1 for detailed descriptions of these datasets.

**Baselines**. We compare DDSR with eight state-of-the-art SR methods, including two conventional SR methods, three methods based on semantic information, and three generative SR methods:

**1). Conventional Baselines**: **SASRec**(Kang and McAuley (2018)) utilizes a causal Transformer architecture with a self-attention mechanism to model user behavior. **BERT4REC**(Sun et al. (2019)) proposes a bidirectional Transformer with a cloze task predicting the masked target items for SR.

**2). Semantic-based Baselines: UniSRec**(Hou et al. (2022)) utilizes the associated description text of items to learn transferable representations across different recommendation scenarios, using an enhanced mixture-of-experts adaptor to enhance domain fusion and adaptation. **VQ-Rec**(Hou et al. (2023)) maps item text to a vector of discrete indices for learning transferable sequential recommenders. **TIGER**(Rajput et al. (2024)) trains a Transformer-based sequence-to-sequence model with semantic IDs obtained from RQ-VAE to enhance its generalization ability.

**3). Generative Baselines: ACVAE**(Xie et al. (2021)) proposes an adversarial and contrastive variational autoencoder for SR combining the ideas of CVAE and GAN. **DiffuRec**(Li et al. (2023))

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline Datasets & Users & Items & Interactions & Avg.length & Avg.num \\ \hline Scientific & \(8442\) & \(4385\) & \(59\,427\) & \(7.04\) & \(182.87\) \\ Office & \(87\,436\) & \(25\,986\) & \(684\,837\) & \(7.84\) & \(193.22\) \\ Online Retail & \(16\,520\) & \(3469\) & \(519\,906\) & \(26.90\) & \(27.80\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Detailed descriptions and statistics of datasets. ’Avg. length’ represents the average length of item sequences, while ’Avg. num’ indicates the average number of words in item text.

introduces the diffusion model into the field of SR reconstructing target item representation from a Transformer backbone with the user's historical interaction behaviors. **DreamRec**(Yang et al. (2024)) uses the historical interaction sequence as conditional guiding information for the diffusion model to enable personalized recommendations.

**Evaluation Settings**. Following previous works Hou et al. (2022); Zhao et al. (2022); Zhou et al. (2020), we evaluate all models using metrics Recall@K and NDCG@K, and report experimental results for \(K=10,50\). We employ the leave-one-out strategy for performance evaluation across all datasets. Concretely, we consider the last interaction as the test set, the second-to-last interaction as the validation set, and all preceding interactions as the training set. The ground-truth item of each sequence is ranked among all the other items while evaluating (Krichene and Rendle (2022)). The implementation details of DDSR are illustrated in Appendix C.1.

### Overall Performance

In this section, we compare the performance of DDSR with baseline models in terms of Top-\(K\) recommendation accuracy under consistent experimental conditions (same data preprocessing), as summarized in Table 2. For models that recommend based on item IDs, we provide semantic information to them by jointly utilizing fixed text embeddings obtained from pre-trained BERT and the embeddings corresponding to item IDs in the model's embedding layer, to ensure fairness in the experimental setup. For all models, the final table records the better of the three methods, using only ID, only text, or both text and ID.

We observe that text-enhanced SR methods (UniSRec, VQ-Rec, TIGER) tend to benefit from textual information, leading to improved performance compared to conventional methods in most cases. Notably, VQ-Rec, employing discrete semantic encoding, generally outperforms UniSRec, which relies on continuous text embeddings, across various settings. This is despite UniSRec already using techniques like parameter whitening and MoE-enhanced Adaptor to enhance textual information. We posit that an excessive emphasis on text similarity can yield suboptimal outcomes, while the conversion to codes mitigates the coupling between items and semantic information. The corresponding representations of the codes are renamed in the sequence-to-sequence model, allowing them to include more sequential structural information. While similarly based on discrete semantic encoding, the performance of TIGER is not stable. We do not rule out the possibility that there may be discrepancies between our implementation and the actual model, as it has not been open-sourced. Furthermore, we attribute the instability to TIGER's semantic ID length, which is limited to only 4 characters, potentially insufficient for expressing complex information.

In methods grounded in generative models, the performance of DiffuRec and DreamRec, based on diffusion models, surpasses that of ACVAE, relying on GAN and VAE. This disparity arises from the inherent advantages of diffusion models over VAE and GAN, as they circumvent the issue of posterior collapse, wherein the generated hidden representations lack crucial information about user preferences. Notably, DiffuRec achieves superior performance despite its limited capacity to handle semantic information, yet it still exhibits recommendation performance comparable to VQ-Rec. This suggests that diffusion models can yield effective hidden representations of items and users.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Methods**} & \multicolumn{4}{c}{**Scientific**} & \multicolumn{4}{c}{**Office**} & \multicolumn{4}{c}{**Online Retail**} \\ \cline{2-13}  & R@10 & N@10 & R@50 & N@50 & R@10 & N@10 & R@50 & N@50 & R@10 & N@10 & R@50 & N@50 \\ \hline SASRec\({}_{T}\) & 0.1049 & 0.0527 & 0.1754 & 0.0683 & 0.1047 & 0.0714 & 0.1638 & 0.0857 & 0.1461 & 0.0674 & 0.3781 & 0.1186 \\ BERT4RecID & 0.0473 & 0.0258 & 0.1092 & 0.0394 & 0.0798 & 0.0605 & 0.1207 & 0.0717 & 0.1354 & 0.0661 & 0.3517 & 0.1159 \\ UniSRec & 0.1104 & 0.0537 & 0.1890 & 0.0787 & 0.1024 & 0.0621 & 0.1668 & 0.0798 & 0.1274 & 0.0598 & 0.3294 & 0.1054 \\ VQ-Rec\({}_{T}\) & 0.1129 & 0.0577 & 0.2046 & 0.0749 & 0.1090 & 0.0676 & 0.1714 & 0.0845 & 0.1532 & 0.0713 & 0.3975 & 0.1254 \\ TIGER\({}_{T}\) & 0.1057 & 0.0597 & 0.1803 & 0.0682 & 0.1056 & 0.0712 & 0.1597 & 0.0868 & 0.0745 & 0.0390 & 0.2216 & 0.0701 \\ ACVAE\({}_{\text{ID}}\) & 0.0463 & 0.0315 & 0.0906 & 0.0457 & 0.0549 & 0.0397 & 0.1003 & 0.0519 & 0.0884 & 0.0410 & 0.1897 & 0.0648 \\ DiffuRecID & 0.1145 & 0.0594 & 0.1079 & 0.0752 & 0.1056 & 0.0689 & 0.1718 & 0.0832 & 0.0402 & 0.0189 & 0.0849 & 0.0321 \\ DreamRecID\({}_{T}\) & 0.0845 & 0.0421 & 0.1645 & 0.0688 & 0.0954 & 0.0557 & 0.1662 & 0.0694 & 0.0577 & 0.0261 & 0.0997 & 0.0544 \\
**DDSR\({}_{T}\)** & **0.1207*** & **0.0663*** & **0.2153*** & **0.0842*** & **0.1138*** & **0.0768*** & **0.1926*** & **0.0925*** & **0.1687*** & **0.0876*** & **0.4021** & **0.1322*** \\ \hline Improv. & +5.41\% & +11.61\% & +5.23\% & +6.99\% & +4.40\% & +8.14\% & +6.46\% & +7.93\% & +10.12\% & +22.86\% & +1.16\% & +5.42\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance of different models. Bold (underline) is used to denote the best (second-best) metric, and '*’ indicates significant improvements relative to the best baseline (t-test P<.05). ’R@K’ (‘N@K’) is short for ‘Recall@K’ (‘NDCG@K’). The features of items have been listed, whether ID, text (T), or both (ID+T).

[MISSING_PAGE_FAIL:9]

**Impact of Sampling Step \(S\).** The sampling step \(S\) represents the diffusion step number divided by the number of inference steps executed simultaneously, rounded down to the nearest integer. We illustrate in Figure 1(b) the influence of various sampling step settings. The model demonstrates optimal performance with approximately 50 sampling steps, with a slight increase for more complex datasets, albeit without significant disparities. Excessive sampling steps prolong the inference time without commensurate performance improvements, while inadequate steps lead to decreased performance.

**Efficiency Analysis.** We compared the time complexity and specific running overhead of DDSR with several other baseline algorithms, and the detailed results can be found in Appendix C.2.

## 6 Discussion

We proposed the DDSR model for the sequential recommendation, which employed discrete diffusion to construct fuzzy sets of user interaction sequences. This process was iteratively refined during inference, utilizing the sampling formula for discrete diffusion to derive the ultimate recommendation outcomes. Notably, although DDSR had borrowed the form of diffusion and sampling over time steps from diffusion models, it fundamentally differed from directly using diffusion models. If we viewed sequential recommendation through the lens of causality, the interaction sequence was the 'cause' and the recommended item was the 'effect'. Diffusion models typically address the target, blurring the 'effect', whereas DDSR has blurred the 'cause', inspired by the theory of fuzzy information processing. Although dual assurances, both theoretical and experimental results, have been provided to substantiate the superior performance of DDSR, it is imperative to recognize its inherent limitations. Despite our efforts to implement efficient computational methods, the nature of diffusion and sampling processes inevitably results in reduced efficiency and increased time complexity. Potential refinements, such as approximating the diffusion process and accelerating the sampling algorithm, could offer effective strategies, which we will explore in future work.

## 7 Acknowledgments

This work is supported by the National Natural Science Foundation of China (No. U23A20319, 62202443). Hao Wang also thanks the CCF-Tencent Rhino-Bird Open Research Fund (RAGR20230124).

## References

* Anderson (1982) Brian D. O. Anderson. Reverse-time diffusion equation models. _Stochastic Processes and their Applications_, 12(3):313-326, 1982.
* Austin et al. (2021) J Austin, DD Johnson, J Ho, et al. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Cen et al. (2020) Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, and Jie Tang. Controllable multi-interest framework for recommendation. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 2942-2951, 2020.
* Chen et al. (2020) Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen, Wen-Chih Peng, Xue Li, and Xiaofang Zhou. Sequence-aware factorization machines for temporal predictive analytics. In _2020 IEEE 36th International Conference on Data Engineering (ICDE)_, pages 1405-1416. IEEE, 2020.
* Chen et al. (2020)

Figure 2: Impact of Item Popularity and Sampling Step \(S\).

Yongjun Chen, Zhiwei Liu, Jia Li, Julian McAuley, and Caiming Xiong. Intent contrastive learning for sequential recommendation. In _Proceedings of the ACM Web Conference 2022_, pages 2172-2182, 2022.
* Dhariwal and Nichol (2021) P Dhariwal and A Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* Gu et al. (2022) Sheng Gu, Dongdong Chen, Jiajun Bao, et al. Vector quantized diffusion model for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10696-10706, 2022.
* Han et al. (2024) Y. Han, H. Wang, K. Wang, and et al. Efficient noise-decoupling for multi-behavior sequential recommendation. In _Proceedings of the ACM on Web Conference 2024_, pages 3297-3306, 2024.
* Harte et al. (2023) J. Harte, W. Zorgdrager, P. Louridas, et al. Leveraging large language models for sequential recommendation. In _Proceedings of the 17th ACM Conference on Recommender Systems_, pages 1096-1102, 2023.
* He and McAuley (2016) Ruining He and Julian McAuley. Fusing similarity models with markov chains for sparse sequential recommendation. In _2016 IEEE 16th International Conference on Data Mining (ICDM)_, pages 191-200. IEEE, 2016.
* He et al. (2016) Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. Fast matrix factorization for online recommendation with implicit feedback. In _Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval_, SIGIR '16, pages 549-558, New York, NY, USA, 2016. Association for Computing Machinery.
* Hidasi et al. (2015) Balazs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, et al. Session-based recommendations with recurrent neural networks. _arXiv preprint arXiv:1511.06939_, 2015.
* Ho et al. (2022) J Ho, W Chan, C Saharia, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.
* Ho et al. (2020) Jonathan Ho, Ankur Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hoogeboom et al. (2021) Emiel Hoogeboom, David Nielsen, Priyank Jaini, and et al. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* Hou et al. (2022) Y. Hou, S. Mu, W. X. Zhao, et al. Towards universal sequence representation learning for recommender systems. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 585-593, 2022.
* Hou et al. (2023) Y. Hou, Z. He, J. McAuley, et al. Learning vector-quantized item representation for transferable sequential recommenders. In _Proceedings of the ACM Web Conference 2023_, pages 1162-1171, 2023.
* Hyvarinen and Dayan (2005) Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Jacob et al. (2018) B Jacob, S Kligys, B Chen, et al. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2704-2713, 2018.
* Kang and McAuley (2018a) W C Kang and J McAuley. Self-attentive sequential recommendation. In _2018 IEEE International Conference on Data Mining (ICDM)_, pages 197-206. IEEE, 2018a.
* Kang and McAuley (2018b) Wang-Cheng Kang and Julian McAuley. Self-attentive sequential recommendation. In _2018 IEEE International Conference on Data Mining (ICDM)_, pages 197-206. IEEE, 2018b.
* Krichene and Rendle (2022) Walid Krichene and Steffen Rendle. On sampled metrics for item recommendation. _Commun. ACM_, 65(7):75-83, 2022.
* Krichene et al. (2020)D Lee, C Kim, S Kim, et al. Autoregressive image generation using residual quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11523-11532, 2022.
* Li et al. (2023a) J Li, M Wang, J Li, et al. Text is all you need: Learning language representations for sequential recommendation. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1258-1267, 2023a.
* Li et al. (2023b) Z. Li, A. Sun, and C. Li. Diffurrec: A diffusion model for sequential recommendation. _ACM Transactions on Information Systems_, 42(3):1-28, 2023b.
* Ma et al. (2020) Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. Disentangled self-supervision in sequential recommenders. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 483-491, 2020.
* Pasricha and McAuley (2018) Rajiv Pasricha and Julian McAuley. Translation-based factorization machines for sequential recommendation. In _Proceedings of the 12th ACM Conference on Recommender Systems_, pages 63-71. ACM, 2018.
* Qiu et al. (2021) Ruihong Qiu, Zi Huang, and Hongzhi Yin. Memory augmented multi-instance contrastive predictive coding for sequential recommendation. In _2021 IEEE International Conference on Data Mining (ICDM)_, pages 519-528. IEEE, 2021.
* Rajput et al. (2024) S. Rajput, N. Mehta, A. Singh, et al. Recommender systems with generative retrieval. _Advances in Neural Information Processing Systems_, 36, 2024.
* Rasul et al. (2021) K Rasul, C Seward, I Schuster, et al. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In _International Conference on Machine Learning_, pages 8857-8868. PMLR, 2021.
* Rosas et al. (2020) Fernando E Rosas, Pedro A M Mediano, Henrik J Jensen, et al. Reconciling emergences: An information-theoretic approach to identify causal emergence in multivariate data. _PLoS computational biology_, 16(12):e1008289, 2020.
* Shannon (1948) Claude E Shannon. A mathematical theory of communication. _The Bell System Technical Journal_, 27(3):379-423, 1948.
* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, et al. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.
* Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, et al. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Sun et al. (2019) F. Sun, J. Liu, J. Wu, et al. Bert4rec: Sequential recommendation with bidirectional encoder representations from transformer. In _Proceedings of the 28th ACM international conference on information and knowledge management_, pages 1441-1450, 2019.
* Tanaka and Sommer (1977) H. Tanaka and G. Sommer. _On posterior probabilities concerning a fuzzy information_. Inst. fur Wirtschaftswissenschaften, RWTH, 1977.
* Tanaka et al. (1976) H. Tanaka, T. Okuda, and K. Asai. A formulation of fuzzy decision problems and its application to an investment problem. _Kybernetes_, 5(1):25-30, 1976.
* Tang and Wang (2018) Jiaxi Tang and Ke Wang. Personalized top-n sequential recommendation via convolutional sequence embedding. In _Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining_, pages 565-573. ACM, 2018.
* Tong et al. (2024) J. Tong, M. Yin, H. Wang, and et al. Mdap: A multi-view disentangled and adaptive preference learning framework for cross-domain recommendation. _arXiv preprint arXiv:2410.05877_, 2024.
* Van Den Oord and Vinyals (2017) Aaron Van Den Oord and Oriol Vinyals. Neural discrete representation learning. _Advances in Neural Information Processing Systems_, 30, 2017.
* Van Den Oord et al. (2018)Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, volume 30, 2017.
* Wang et al. [2019] H. Wang, T. Xu, Q. Liu, et al. Mcne: An end-to-end framework for learning multiple conditional network representations of social network. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1064-1072, 2019.
* Wang et al. [2021] H. Wang, D. Lian, H. Tong, and et al. Hypersorec: Exploiting hyperbolic user and item representations with multiple aspects for social-aware recommendation. _ACM Transactions on Information Systems (TOIS)_, 40(2):1-28, 2021.
* Wang et al. [2024] H. Wang, Y. Han, K. Wang, and et al. Denoising pre-training and customized prompt learning for efficient multi-behavior sequential recommendation. _arXiv preprint arXiv:2408.11372_, 2024.
* Wang et al. [2024] Hao Wang, Mingjia Yin, Luankang Zhang, Sirui Zhao, and Enhong Chen. Mf-gslae: A multi-factor user representation pre-training framework for dual-target cross-domain recommendation. _ACM Transactions on Information Systems_.
* Wu et al. [2024] L. Wu, Z. Zheng, Z. Qiu, and et al. A survey on large language models for recommendation. _World Wide Web_, 27(5):60, 2024.
* Xie et al. [2024] Wenjia Xie, Ruining Zhou, Hong Wang, et al. Bridging user dynamics: Transforming sequential recommendations with schrodinger bridge and diffusion models. In _Proceedings of the 33rd ACM International Conference on Information and Knowledge Management_, pages 2618-2628, 2024.
* Xie et al. [2021] Z. Xie, C. Liu, Y. Zhang, et al. Adversarial and contrastive variational autoencoder for sequential recommendation. In _Proceedings of the Web Conference 2021_, pages 449-459, 2021.
* Yang et al. [2024] Z. Yang, J. Wu, Z. Wang, et al. Generate what you prefer: Reshaping sequential recommendation via guided diffusion. _Advances in Neural Information Processing Systems_, 36, 2024.
* Yang et al. [2023] Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, and Xiangnan He. Generate what you prefer: Reshaping sequential recommendation via guided diffusion. _arXiv preprint arXiv:2310.20453_, 2023.
* Yin et al. [2024a] M. Yin, C. Wu, Y. Wang, and et al. Entropy law: The story behind data compression and l1m performance. _arXiv preprint arXiv:2407.06645_, 2024a.
* Yin et al. [2023] Mingjia Yin, Hao Wang, Xiang Xu, Likang Wu, Sirui Zhao, Wei Guo, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. Appl4sr: A generic framework with adaptive and personalized global collaborative information in sequential recommendation. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pages 3009-3019, 2023.
* Yin et al. [2024b] Mingjia Yin, Hao Wang, Wei Guo, Yong Liu, Suojuan Zhang, Sirui Zhao, Defu Lian, and Enhong Chen. Dataset regeneration for sequential recommendation. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 3954-3965, 2024b.
* Zhang et al. [2024] Luankang Zhang, Hao Wang, Suojuan Zhang, Mingjia Yin, Yongqiang Han, Jiaqing Zhang, Defu Lian, and Enhong Chen. A unified framework for adaptive representation enhancement and inversed learning in cross-domain recommendation. _arXiv preprint arXiv:2404.00268_, 2024.
* Zhao [2022] Q. Zhao. Resetbert4rec: A pre-training model integrating time and user historical behavior for sequential recommendation. In _Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval_, pages 1812-1816, 2022.
* Zhao et al. [2022] W. X. Zhao, Z. Lin, Z. Feng, et al. A revisiting study of appropriate offline evaluation for top-n recommendation algorithms. _ACM Transactions on Information Systems_, 41(2):1-41, 2022.
* Zhou et al. [2020] K. Zhou, H. Wang, W. X. Zhao, et al. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1893-1902, 2020.
* Zhou et al. [2020]

## Appendix A Proof of the theory

**Theorem A.1**.: _The model often has the following form:_

\[f:(U\times V,\rho)\rightarrow(U^{\prime}\times V^{\prime},\rho),\]

_which requires the model to be a continuous mapping from the original input-output space (which can also be the original space). The data distribution on the aggregated model set can be modeled:_

\[f^{\prime}:(D,\rho^{\prime})\rightarrow(U^{\prime}\times V^{\prime},\rho),\]

_defined as \(\gamma\) such that \(f^{\prime}=\gamma(\mu)\). When the static distribution approximates the dynamic distribution, \(\mu\) is the extended static scattering coefficient; otherwise, it is the linear information distribution coefficient \(\gamma\), ensuring that the determination of \(f^{\prime}\) is unique. Therefore, \((D,\rho^{\prime})\) is the completion space of \((U\times V,\rho)\), consisting of the complete set of \((U\times V,\rho)\) and its separation._

_In summary, the information diffusion space must be the completion space of the original space. Due to the same dimensionality as the original space, the modeling on the aggregated set is reliable._

_Next, consider the model (prediction model or simulation model) built on \(D(X)\). In practical applications, the input-output set is generally constructed in two ways based on the original data:_

\[e_{i}=\frac{x_{i}-x_{\min}}{x_{\max}-x_{\min}},\]

_where \(e_{i}\) is the normalized sequence; \(x_{i}\) is the original sequence data; \(x_{\min}\) is the minimum value of the original sequence; \(x_{\max}\) is the maximum value of the original sequence. It is obvious that \(e_{i}\in[0,1]\), meaning \((U\times V)\subseteq[0,1]\), and the function boundary on \(D(X)\). When evaluating risk, the original sequence data generates input-output sets, which clearly have bounded intervals \(x_{i}\in[x_{\min},x_{\max}]\), meaning the function boundary on \(D(X)\) is also bounded. Therefore, \(D(X)\) is necessarily a subset of the real number set._

_In the prediction model, the attribute function obtained from the aggregated set is:_

\[\mu_{y_{0}}=\sum_{u}\mu_{x_{0}}(u)\mu_{A_{1}}(u,v),\quad u\in U,v\in V. \tag{6}\]

_Among them, \(\mu_{R_{i}}(u)=\mu_{A_{i}}(u)\mu_{A_{i}}(v),\quad u\in U,v\in V\). Since the output sequence and the input sequence are generated by the same \(\{e_{i}\}\), and \(\mu_{i}=x_{i+1}\):_

_Separately substituting into \((\ref{eq:prediction})\), we can obtain:_

\[\mu_{x_{0}}\cdot g(x_{0})=\sum_{u}\mu_{x_{0}}\mu_{A_{i}}\mu_{B_{i}} \tag{7}\]

_or_

\[\mu_{x_{0}}+g(x_{0})=\sum_{u}\mu_{x_{0}}\mu_{A_{i}}\mu_{B_{i}}.\]

_Rearranging, we get:_

\[\mu_{x_{0}}=\sum_{u}\frac{\mu_{x_{0}}\mu_{A_{i}}\mu_{B_{i}}}{g(x_{0})}=\sum_{u }G(x_{0},\mu_{x_{0}}),\]

_and_

\[\mu_{x_{0}}=\sum_{u}\mu_{x_{0}}\mu_{A_{i}}\mu_{B_{i}}-g(x_{0})=\sum_{u}\left[ \mu_{x_{0}}\mu_{A_{i}}\mu_{B_{i}}-\frac{g(x_{0})}{n}\right]=\sum_{u}G(x_{0}, \mu_{x_{0}}). \tag{8}\]

_The initial value problem for the differential equation \(\dot{y}(t)=f(t,y(t)),y(t_{0})=y_{0}\) can be transformed into an integral equation:_

\[y(t)=\int_{u}f(t,y(t))dt+y_{0},\quad t\in u; \tag{9}\]

_Since the cumulative sum and substitution integrals can be used in the scatter system, \((\ref{eq:prediction})\) is the integral equation's discrete model on \(D(X)\)._\[\left|G(x_{i})-G(x_{j})\right|=\left|G(x_{i},\mu_{x_{i}})-G(x_{j},\mu_{x_{j}})\right| =\left\{\begin{matrix}\mu_{x_{i}}\cdot g(x_{i})-\mu_{x_{j}}\cdot g(x_{j})\\ \mu_{x_{i}}+g(x_{i})-\mu_{x_{j}}+g(x_{j})\end{matrix}\right|\,,\quad X\subseteq N (0,1) \tag{10}\]

_Both cases satisfy:_

\[K=\max_{x\in U}\frac{\Delta}{\mu_{x_{i}}-\mu_{x_{j}}}\text{ such that }\left|G(x_{i})-G(x_{j})\right|\leq K\left|x_{i}-x_{j}\right|,\]

_i.e., the function \(G(X)\) on \(D(X)\) satisfies the Lipschitz condition, thus the attribute function must have a solution._

_Since the attribute function has a solution on \(D(X)\), it can be deduced by the "maximum" principle that the predicted output value must be:_

\[\bar{y_{0}}=\left(\sum_{i=1}^{n}w_{i}v_{i}^{\prime}\right)/\left(\sum_{i=1}^{n }w_{i}\right),\]

_where the weights \(w_{i}=\mu_{y_{0}}\left(v^{\prime}\right)=\max_{v\in V}\left\{\mu_{y_{0}}\left(v \right)\right\}.\)_

_Therefore, the information diffusion approximation model is established._

## Appendix B Model Supplement

We first obtain fixed text embeddings from the relevant descriptions of items (e.g., product descriptions, item titles, or brands) using the pre-trained BERT model. Specifically, for an item \(v_{i}\) with a corresponding description \(\{w_{1},w_{2},\ldots,w_{c}\}\), the corresponding embedding vector is \(e_{i}=\mathrm{BERT}([[\mathrm{CLS}];w_{1},\ldots,w_{c}])\in\mathbb{R}^{d_{W}}\), where "\([;]\)" denotes the concatenation operation. Next, obtaining Semantic IDs from \(e_{i}\) can be achieved through the following two methods:

**Product Quantization (PQ).** Similar to the VQ-Rec approach, we evenly divided \(e_{i}\) into \(m\) sub-vectors \(e_{i}=[e_{i,1};\ldots;e_{i,m}]\), each with a dimension of \(d_{W}/m\). Denote \(a_{p,j}\in\mathbb{R}^{d_{W}/m}\) as the \(j\)-th centroid embedding in the codebook corresponding to the \(p\)-th sub-vector. For each sub-vector, the index of the nearest centroid from the corresponding PQ codebook is selected to form its discrete code \(c_{i,p}=\arg\min_{j}\|e_{i,p}-\mathbf{a}_{p,j}\|^{2}\in\{1,2,\ldots,K\}.\) The centroids are obtained through the commonly used Optimized Product Quantization (OPQ) method. Finally, \(c_{i}=(c_{i,1},\ldots,c_{i,m})\) is used as the semantic ID for item \(v_{i}\).

**RQ-VAE.** RQ-VAE generate a set of codewords by quantizing the residuals. First, the input \(e_{i}\) is encoded into \(r_{0}\) using encoder \(\mathbf{E}\). Next, similar to PQ, the nearest centroid to it in the first codebook, assumed to be \(a_{p,1}\), is found and its index is taken to form the first discrete code \(c_{i,1}\). The residual is defined as \(r_{1}:=r_{0}-a_{p_{1},1}\). The same operation is performed to obtain \(c_{i,2}\) and this process is repeated \(m\) times to obtain the complete Semantic ID. RQ-VAE use loss function \(\mathcal{L}(\mathbf{x}):=\mathcal{L}_{\mathrm{recon}}+\mathcal{L}_{\mathrm{rqvae}}\) jointly trains the encoder, decoder, and the codebook, where \(\mathcal{L}_{\mathrm{recon}}:=\|e_{i}-\widehat{e_{i}}\|^{2}\) is reconstruction loss, \(\mathcal{L}_{\mathrm{rqvae}}:=\sum_{d=0}^{m-1}\|\mathrm{sg}[\mathbf{r}_{i}]-a_{p_{i },i}\|^{2}+\beta\|\mathbf{r}_{i}-\mathrm{sg}[a_{p_{i},i}]\|^{2}\). Here \(\widehat{e_{i}}\) is the output of the decoder, and sg is the stop-gradient operation Van Den Oord and Vinyals (2017). Because the norm of the residuals decreases progressively, the importance of the codebooks obtained in this manner also diminishes with each level. Alternatively, it can be said that the encoding at each position has varying levels of granularity. In our experiments, we found that this approach requires a smaller codebook size than PQ, but it is slightly less stable.

In fact, once the codebook is established, it remains fixed throughout the subsequent model training process. Therefore, the quality of the codebook directly affects the training outcomes. With the rapid development of large language models, considering the use of more advanced pre-training schemes to replace BERT could be an effective way to enhance performance. However, this is not the focus of our current research, we leave this topic for future exploration.

## Appendix C Experimental Supplement

### Implementation Details

All of our experiments were conducted on a single RTX 4090. We implemented our models based on PyTorch and the popular open-source recommendation library RecBole. We used \((m=32)\times(K=256)\) as the code representation scheme for PQ IDs and \((m=6)\times(K=256)\) for RQ-VAE IDs. For baseline models, to ensure fair comparison, we optimized all methods using the Adam optimizer and searched for hyperparameters to find the best results. Since we did not find open-source code for the TIGER model, we attempted to replicate it as faithfully as possible based on its paper; however, its performance may have been slightly lower in some experiments due to difficulties in unifying some details. For UniSRec and VQ-Rec, we utilize their code in a form that does not involve pretraining with the entire dataset, as we aim to evaluate all baselines and DDSR considering recommendations on a single dataset rather than cross-domain. The batch size was set to 2,048. The learning rate was adjusted among \(\{0.0001,0.0002,0.0003,0.0005,0.001\}\). The model achieving the highest NDCG@10 result on the validation set was selected for evaluation on the test set. We employed an early stopping strategy with a patience of 10 epochs.

Regarding diffusion, all models were trained on a diffusion process of 1000 steps, and the time step embeddings are implemented using cosine embeddings, similar to the work of Li et al. [2023b]. For the diffusion with uniform transition, we employ the cosine schedule proposed by Hoogeboom et al. [2021] to set the transition probabilities \((1-\beta_{t})\). For the diffusion with importance sampling, we adopt a linear schedule similar to the one used in Ho et al. [2020], where \(\sigma^{2}\) increases linearly from \(10^{-4}*K\) to \(0.02*K\). Skip steps in the sampling process were chosen among \(\{100,50,35,28,23,20,17,15\}\). While there have been many works on accelerating sampling in continuous state space diffusion models, the development in discrete diffusion is still insufficient. Here, we adopted a basic uniform skip scheme for more efficient and effective sampling, which is one of our next research directions. If the evaluation steps do not divide 1000 evenly, the last step may be skipped.

### Efficiency Analysis

We list the time complexities of six baselines in Table 4, where \(n\) denotes sequence length, \(d\) is the dimension of the hidden layers, and \(m\) is the length of the codebook used when semantic IDs are utilized. Most of these models are based on the transformer or its variants, hence the time complexity is \(O(nd^{2}+dn^{2})\). Only TIGER and our proposed DDSR model are trained using semantic IDs, making their time complexity \(m\) times that of other methods. We plan to make further improvements in our subsequent work.

In addition, we compared the actual operational costs of the DDSR model with those of UniSRec and DiffuRec, as shown in Table 5. Although we adopted the method of 'performing inference for \(k\) steps at a time', which reduced the sampling steps and lowered the evaluation time compared to DiffuRec, we must acknowledge that DDSR still has certain limitations in terms of operational costs, necessitating further improvements in future work.

## Appendix D Diffusion Models in Continuous State Space

### Ddpm

Diffusion models comprise a forward diffusion process and a backward denoising process. We begin with the widely recognized denoising diffusion probabilistic model (DDPM) Ho et al. [2020]. We start by defining our data distribution \(x_{0}\sim q(x_{0})\) and a Markovian noising process \(q\) which gradually adds noise to the data \(x_{0}\) to produce noised samples \(x_{T}\).In particular, each step of the noising process

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Model** & SASRec & BERT4Rec & UniSRec & TIGER & DiffuRec & DDSR \\ \hline
**Comp.** & \(O(nd^{2}+dn^{2})\) & \(O(nd^{2}+dn^{2})\) & \(O(nd^{2}+dn^{2})\) & \(O(mnd^{2}+mdn^{2})\) & \(O(nd^{2}+dn^{2})\) & \(O(mnd^{2}+dmn^{2})\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Time complexity analysis of various models, ’Comp.’ is an abbreviation for ’Complexity’.

adds Gaussian noise according to a variance schedule given by \(\beta_{t}\):

\[q(x_{t}\mid x_{t-1}):=\mathcal{N}(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}\textbf {I})\]

Furthermore, \(q(x_{t}\mid x_{0})\) can be expressed as a Gaussian distribution. With \(\alpha_{t}:=1-\beta_{t}\) and \(\bar{\alpha}_{t}:=\prod_{s=0}^{t}\alpha_{s}\), \(q(x_{t}\mid x_{0})=\mathcal{N}(x_{t};\sqrt{\bar{\alpha}_{t}}x_{0},(1-\bar{ \alpha}_{t})\textbf{I})=\sqrt{\bar{\alpha}_{t}}x_{0}+\sqrt{1-\bar{\alpha}_{t}}\epsilon\), where \(\epsilon\sim\mathcal{N}(0,\textbf{I})\). Here, \(1-\bar{\alpha}_{t}\) indicates the variance of the noise at an arbitrary timestep, and this can be used to define the noise schedule instead of \(\beta_{t}\).

Using Bayes theorem, one finds that the posterior \(q(x_{t-1}|x_{t},x_{0})\) is also a Gaussian with mean \(\tilde{\mu}_{t}(x_{t},x_{0})\) and variance \(\tilde{\beta}_{t}\) defined as follows:

\[\tilde{\mu}_{t}(x_{t},x_{0}):=\frac{\sqrt{\alpha_{t-1}}\beta_{t}}{1-\alpha_{t }}x_{0}+\frac{\sqrt{\alpha_{t}}(1-\alpha_{t-1})}{1-\alpha_{t}}x_{t} \tag{8}\]

\[\tilde{\beta}_{t}:=\frac{1-\alpha_{t-1}}{1-\alpha_{t}}\beta_{t} \tag{9}\]

\[q(x_{t-1}|x_{t},x_{0})=\mathcal{N}(x_{t-1};\tilde{\mu}_{t}(x_{t},x_{0}),\tilde {\beta}_{t}\textbf{I}) \tag{10}\]

If we wish to sample from the data distribution \(q(x_{0})\), we can first sample from \(q(x_{T})\) and then sample reverse steps \(q(x_{t-1}|x_{t})\) until we reach \(x_{0}\). Under reasonable settings for \(\beta_{t}\) and \(T\), the distribution \(q(x_{T})\) is nearly an isotropic Gaussian distribution, so sampling \(x_{T}\) is trivial. All that is left is to approximate \(q(x_{t-1}|x_{t})\) using a neural network, since it cannot be computed exactly when the data distribution is unknown. To this end, Sohl-Dickstein et al. [56] note that \(q(x_{t-1}|x_{t})\) approaches a diagonal Gaussian distribution as \(T\rightarrow\infty\) and correspondingly \(\beta_{t}\to 0\), so it is sufficient to train a neural network to predict a mean \(\mu_{\theta}\) and a diagonal covariance matrix \(\Sigma_{\theta}\):

\[p_{\theta}(x_{t-1}|x_{t}):=\mathcal{N}(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{ \theta}(x_{t},t)) \tag{11}\]

To train this model such that \(p_{\theta}(x_{0})\) learns the true data distribution \(q(x_{0})\), we can optimize the following variational lower-bound \(L_{\text{vfb}}\) for \(p_{\theta}(x_{0})\):

\[L_{\text{vfb}}:=L_{0}+L_{1}+\ldots+L_{T-1}+L_{T} \tag{12}\]

\[L_{0}:=-\log p_{\theta}(x_{0}|x_{1}) \tag{13}\]

\[L_{t-1}:=D_{KL}(q(x_{t-1}|x_{t},x_{0})\|p_{\theta}(x_{t-1}|x_{t})) \tag{14}\]

\[L_{T}:=D_{KL}(q(x_{T}|x_{0})\|p(x_{T})) \tag{15}\]

\begin{table}
\begin{tabular}{l l c c c} \hline \hline Datasets & Model & GPU memory (GB) & Training Time (s/epoch) & Evaluation Time (s/epoch) \\ \hline \multirow{3}{*}{Scientific} & UniSRec & 8.32 & 3.51 & 0.67 \\  & DiffuRec & 14.94 & 4.97 & 17.52 \\  & DDSR & 12.41 & 6.76 & 11.38 \\ \hline \multirow{3}{*}{Office} & UniSRC & 8.29 & 9.96 & 1.13 \\  & DiffuRec & 14.85 & 25.81 & 127.41 \\  & DDSR & 12.48 & 36.19 & 69.10 \\ \hline \multirow{3}{*}{Online Retail} & UniSRC & 9.96 & 52.19 & 3.70 \\  & DiffuRec & 15.97 & 65.22 & 103.44 \\ \cline{1-1}  & DDSR & 13.47 & 83.51 & 60.11 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of Actual Operational Costs.

While the above objective is well-justified, Ho et al. (2020) found that a different objective produces better samples in practice. In particular, they do not directly parameterize \(\mu_{\theta}(x_{t},t)\) as a neural network, but instead train a model \(\epsilon_{\theta}(x_{t},t)\) to predict \(\epsilon\) from Equation 17. This simplified objective is defined as follows:

\[L_{\text{simple}}:=\mathbb{E}_{t\sim[1,T],x_{0}\sim q(x_{0}),\epsilon\sim \mathcal{N}(0,\mathbf{I})}\left[\|\epsilon-\epsilon_{\theta}(x_{t},t)\|_{2}^{ 2}\right] \tag{16}\]

During sampling, we can use substitution to derive \(\mu_{\theta}(x_{t})\) from \(\epsilon_{\theta}(x_{t},t)\):

\[\mu_{\theta}(x_{t})=\frac{1}{\sqrt{\alpha_{t}}}\left(x_{t}-\frac{1-\alpha_{t}} {\sqrt{1-\alpha_{t}}}\epsilon_{\theta}(x_{t},t)\right) \tag{17}\]

Note that \(L_{\text{simple}}\) does not provide any learning signal for \(\Sigma_{\theta}(x_{t},t)\). Ho et al. (2020) find that instead of learning \(\Sigma_{\theta}(x_{t},t)\), they can fix it to a constant, choosing either \(\beta_{t}\) or \(\tilde{\beta}_{t}\). These values correspond to learning noise and the reverse process variance respectively.

### Score-Based Generative Model

In this section, we introduce a Score-Based Generative Model (SGMs) Song et al. (2020), specifically a diffusion model represented in the form of Stochastic Differential Equations (SDEs). SGMs model the forward diffusion process using the stochastic differential equation:

\[dx_{t}=f(x_{t},t)dt+g(t)dw,\mathbf{x}_{0}\sim p_{0}=p_{\text{target}}, \tag{18}\]

where \(t\in[0,T]\), and \(w\) signifies Brownian motion, \(p_{\text{target}}\) represents target distribution. The function \(f(\cdot,t):\mathbb{R}^{d}\rightarrow\mathbb{R}^{d}\)is a vector-valued function called the drift coefficient of \(x(t)\), and \(g(\cdot):\mathbb{R}\rightarrow\mathbb{R}\) is a scalar function known as the diffusion coefficient of \(x(t)\). The functions \(f\) and \(g\) determine the type of prior distribution \(p_{\text{prior}}\) to which the forward process will diffuse, and they are typically designed to make the prior distribution a Gaussian distribution. As a remarkable result from Anderson (1982), the reverse of the diffusion process is also a diffusion process, given by the following reverse-time SDE:

\[dx_{t}=[f(x_{t},t)-g(t)^{2}\nabla_{x}\log p_{t}(x)]dt+g(t)d\bar{w}, \tag{19}\] \[\mathbf{x}_{T}\sim p_{T}\approx p_{\text{prior}},\]

where \(\bar{w}\) is a standard Wiener process in reverse time. The term \(\nabla_{x}\log p_{t}(x)\), which represents the score function of the marginal density \(p_{t}\), is the only unknown term in this reverse process. SGMs learns its approximate target \(s_{\theta}(x(t),t)\) through denoising score matching (DSM) Hyvarinen and Dayan (2005), with \(s_{\theta}\) referred to as the denoising model:

\[\theta^{*} =\arg\min_{\theta}\mathbb{E}_{t\sim U(0,T)}\mathbb{E}_{x(0)} \mathbb{E}_{x(t)|x(0)}\] \[\left[\|s_{\theta}(x(t),t)-\nabla_{x}\log p_{t}(x|x(0))\|^{2} \right]. \tag{20}\]

Here, \(\lambda(t)\) is a positive weighting coefficient, \(t\sim\mathcal{U}(0,T)\). The joint distribution \(p_{0t}(x|x(0))\) is the conditional transition distribution from \(x(0)\) to \(x(t)\), which is determined by the pre-defined forward SDE. To summarize, SGMs first utilize the diffusion process defined in Equation(18) to obtain the distribution \(x_{t}\) at intermediate time steps. Then, they minimize the loss defined in Equation(20) to train the denoising model \(s_{\theta}\) and sample iteratively using the formula defined in Equation(19) to obtain the final result.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly outline the primary contributions of the paper, including the background, challenges, motivation, overall framework, and experimental validation. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper provides a detailed explanation of the problem background in the methods section and validates the hypothesis through multiple runs on various datasets with different characteristics in the experimental section. Additionally, the limitations are thoroughly discussed in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The paper presents all theoretical results with clearly stated assumptions and complete proofs, which are either included in the main text or supplemented by detailed proofs in the appendix, ensuring both clarity and rigor. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper provides detailed descriptions of the experimental setup, including dataset specifications, parameter settings, and evaluation metrics, ensuring that the main results can be reproduced even without direct access to the code and data. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). *4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper includes links to openly accessible data and code repositories that cover the necessary commands and environment setup to reproduce the main experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper thoroughly describes all relevant experimental details in a dedicated "Experiment Setting" section, including data splits, hyperparameters, and optimization methods, ensuring clarity and reproducibility of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: The paper appropriately reports statistical significance using t-tests with a significance threshold of p < 0.05, ensuring transparency and rigor in the experimental analysis. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper thoroughly details the computer resources required for each experiment, including the type of compute workers, memory specifications, and execution times, enabling reproducibility and understanding of the computational demands of the study. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research ensures that ethical considerations are addressed and integrated into the study's design and execution, and the authors ensure anonymity is preserved. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. ** The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The paper addresses both potential positive and negative societal impacts of the research, highlighting potential misuses and discussing mitigation strategies, thereby ensuring a comprehensive consideration of broader societal implications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve the release of data or models that pose a high risk for misuse, thus no safeguards are necessary. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?Answer: [Yes]

Justification: The paper properly credits the creators of all utilized assets, explicitly adhering to licenses and terms of use by including citations, version details, and relevant licensing information.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The code repositories will provide comprehensive documentation for all newly introduced assets, including details about their creation, usage, and limitations, ensuring that other researchers can effectively utilize these resources. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects, so this information is not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing or research with human subjects, so IRB approval or equivalent is not applicable. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.