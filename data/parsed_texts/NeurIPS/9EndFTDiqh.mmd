# Should We Learn

Most Likely Functions or Parameters?

Shikai Qiu

Equal contribution.

Tim G. J. Rudner

Equal contribution.

Sanyam Kapoor1

Andrew Gordon Wilson

New York University

Footnote 1: footnotemark:

###### Abstract

Standard regularized training procedures correspond to maximizing a posterior distribution over parameters, known as maximum a posteriori (MAP) estimation. However, model parameters are of interest only insomuch as they combine with the functional form of a model to provide a function that can make good predictions. Moreover, the most likely parameters under the parameter posterior do not generally correspond to the most likely function induced by the parameter posterior. In fact, we can re-parametrize a model such that any setting of parameters can maximize the parameter posterior. As an alternative, we investigate the benefits and drawbacks of directly estimating the most likely function implied by the model and the data. We show that this procedure leads to pathological solutions when using neural networks and prove conditions under which the procedure is well-behaved, as well as a scalable approximation. Under these conditions, we find that function-space MAP estimation can lead to flatter minima, better generalization, and improved robustness to overfitting.

## 1 Introduction

Machine learning has matured to the point where we often take key design decisions for granted. One of the most fundamental such decisions is the loss function we use to train our models. Minimizing standard regularized loss functions, including cross-entropy for classification and mean-squared or mean-absolute error for regression, with \(\ell_{1}\) or \(\ell_{2}\) regularization, exactly corresponds to maximizing a posterior distribution over model parameters [2, 19]. This standard procedure is known in probabilistic modeling as _maximum a posteriori_ (map) parameter estimation. However, parameters have no meaning independent of the functional form of the models they parameterize. In particular, our models \(f_{\theta}(x)\) are functions given parameters \(\theta\), which map inputs \(x\) (e.g., images, spatial locations, etc.) to targets (e.g., softmax probabilities, regression outputs, etc.). We are typically only directly interested in the function and its properties, such as smoothness, which we use to make predictions.

Alarmingly, the function corresponding to the most likely parameters under the parameter posterior does not generally correspond to the most likely function under the function posterior. For example, in Figure 0(a), we visualize the posterior over a mixture coefficient \(\theta_{R}\) in a Gaussian mixture regression model in both parameter and function space, using the same Gaussian prior for the mixture coefficients (corresponding to \(\ell_{2}\) regularization in parameter space). We see that each distribution is maximized by a different parameter \(\theta_{R}\), leading to very different learned functions in Figure 0(b). Moreover, we can re-parametrize the functional form of any model such that any arbitrary setting of parameters maximizes the parameter posterior (we provide further discussion and an example in Appendix A).

_Should we then be learning the most likely functions or parameters?_ As we will see, the nuanced pros and cons of each approach are fascinating and often unexpected.

On one hand, we might expect the answer to be a clear-cut "We should learn most likely functions!". In Section 3, we present a well-defined function-space map objective through a generalization of the change of variables formula for probability densities. In addition to functions being the direct quantity of interest and the function-space map objective being invariant to the parametrization of our model, we show that optimization of the function-space map objective indeed results in more probable functions than standard parameter-space map, and these functions often correspond to _flat minima_[10; 12] that are more robust to overfitting.

On the other hand, function-space map is not without its own pathologies and practical limitations. We show in Section 4 that the Jacobian term in the function-space map objective admits trivial solutions with infinite posterior density and can require orders of magnitude more computation and memory than parameter-space map, making it difficult to scale to modern neural networks. We also show that function-space map will not necessarily be closer than parameter-space map to the posterior expectation over functions, which in Bayesian inference forms the mean of the posterior predictive distribution and has desirable generalization properties [2; 19; 29]. To help address the computational limitations, we provide in Section 4 a scalable approximation to the function-space map objective applicable to large neural networks using Laplacian regularization, which we refer to as l-map. We show empirically that parameter-space map is often able to perform on par with l-map in accuracy, although l-map tends to improve calibration.

The aim of this paper is to improve our understanding of what it means to learn most likely functions instead of parameters. Our analysis, which includes theoretical insights as well as experiments with both carefully designed tractable models and neural networks, paints a complex picture and unearths distinct benefits and drawbacks of learning most likely functions instead of parameters. We conclude with a discussion of practical considerations around function-space map estimation, its relationship to flat minima and generalization, and the practical relevance of parameterization invariance.

Our code is available at https://github.com/activatedgeek/function-space-map.

## 2 Preliminaries

We consider supervised learning problems with a training dataset \(\mathcal{D}=(x_{\mathcal{D}},y_{\mathcal{D}})=\{x_{\mathcal{D}}^{(i)},y_{ \mathcal{D}}^{(i)}\}_{i=1}^{N}\) of inputs \(x\in\mathcal{X}\) and targets \(y\in\mathcal{Y}\) with input space \(\mathcal{X}\subseteq\mathbb{R}^{D}\) and output space \(\mathcal{Y}\subseteq\mathbb{R}^{K}\). For example, the inputs \(x\) could correspond to times, spatial locations, tabular data, images, and the targets \(y\) to regression values, class labels, etc.

Figure 1: **Illustration of the Difference Between Most Likely Functions and Parameters. The function that is most probable (denoted as fs-map) under the posterior distribution over functions can substantially diverge from the function represented by the most probable parameters (denoted as PS-MAP) under the posterior distribution over parameters. We illustrate this fact with a regression model with two parameters, \(\theta_{L}\) and \(\theta_{R}\), both with a prior \(\mathcal{N}(0,1.2^{2})\). This model is used to learn a mixture of two Gaussians with fixed mean and variance, where the mixture coefficients are given by \(\exp(\theta_{L})\) and \(\exp(\theta_{R})\). Both the most probable solution and the posterior density show significant differences when analyzed in function-space versus parameter-space. Since \(\theta_{L}\) is well-determined, we only plot the posterior as a function of \(\theta_{R}\). We normalize the area under \(p(f_{\theta}|\mathcal{D})\) to 1.**

To create a model of the data \(\mathcal{D}\), one typically starts by specifying a function \(f_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}\) parametrized by a vector \(\theta\in\mathbb{R}^{P}\) which maps inputs to outputs. \(f_{\theta}\) could be a neural network, a polynomial model, a Fourier series, and so on. Learning typically amounts to estimating model parameters \(\theta\). To this end, we can relate the function \(f_{\theta}\) to the targets through an _observation model_, \(p(y|x,f_{\theta})\). For example, in regression, we could assume the outputs \(y=f_{\theta}(x)+\epsilon\), where \(\epsilon\sim\mathcal{N}(0,\sigma^{2})\) is additive Gaussian noise with variance \(\sigma^{2}\). Equivalently, \(p(y|x,\theta)=\mathcal{N}(y|f_{\theta}(x),\sigma^{2})\). Alternatively, in classification, we could specify \(p(y|x,\theta)=\mathrm{Categorical}(\mathrm{softmax}(f_{\theta}(x)))\). We then use this observation model to form a _likelihood_ over the whole dataset \(p(y_{\mathcal{D}}|x_{\mathcal{D}},\theta)\). In each of these example observation models, the likelihood factorizes, as the data points are conditionally independent given model parameters \(\theta\), \(p(y_{\mathcal{D}}|x_{\mathcal{D}},\theta)=\prod_{i=1}^{N}p(y_{\mathcal{D}}^{( i)}|x_{\mathcal{D}}^{(i)},\theta)\). We can further express a belief over values of parameters through a prior \(p(\theta)\), such as \(p(\theta)=\mathcal{N}(\theta|\mu,\Sigma)\). Finally, using Bayes rule, the log of the posterior up to a constant \(c\) independent of \(\theta\) is,

\[\log\overbrace{p(\theta|y_{\mathcal{D}},x_{\mathcal{D}})}^{\text{parameter posterior}}=\log\overbrace{p(y_{\mathcal{D}}|x_{\mathcal{D}},\theta)}^{\text{likelihood}}+\log\overbrace{p(\theta)}^{\text{prior}}+c.\] (1)

Notably, standard loss functions are negative log posteriors, such that maximizing this parameter posterior, which we refer to as _parameter-space_ maximum a posteriori (PS-map) estimation, corresponds to minimizing standard loss functions [19]. For example, if the observation model is regression with Gaussian noise or Laplace noise, the negative log likelihood is proportional to mean-square or mean-absolute error functions, respectively. If the observation model is a categorical distribution with a softmax link function, then the log likelihood is negative cross-entropy. If we use a zero-mean Gaussian prior, we recover standard \(\ell_{2}\) regularization, also known as weight-decay. If we use a Laplace prior, we recover \(\ell_{1}\) regularization, also known as LASSO [26].

Once we maximize the posterior to find

\[\hat{\theta}^{\text{PS-MAP}}=\arg\max_{\theta}p(\theta|y_{\mathcal{D}},x_{ \mathcal{D}}),\] (2)

we can condition on these parameters to form our function \(f_{\hat{\theta}^{\text{PS-MAP}}}\) to make predictions. However, as we saw in Figure 1, \(f_{\hat{\theta}^{\text{PS-MAP}}}\) is not in general the function that maximizes the posterior over functions, \(p(f_{\theta}|y_{\mathcal{D}},x_{\mathcal{D}})\). In other words, if

\[\hat{\theta}^{\text{PS-MAP}}=\arg\max_{\theta}p(f_{\theta}|y_{\mathcal{D}},x_ {\mathcal{D}})\] (3)

then generally \(f_{\hat{\theta}^{\text{PS-MAP}}}\neq f_{\hat{\theta}^{\text{PS-MAP}}}\). Naively, one can write the log posterior over \(f_{\theta}\) up to the same constant \(c\) as above as

\[\log\overbrace{p(f_{\theta}|y_{\mathcal{D}},x_{\mathcal{D}})}^{\text{function posterior}}=\log\overbrace{p(y_{\mathcal{D}}|x_{\mathcal{D}},f_{\theta})}^{\text{likelihood}}+\log\overbrace{p(f_{\theta})}^{\text{ function prior}}+c,\] (4)

where \(p(y_{\mathcal{D}}|x_{\mathcal{D}},f_{\theta})=p(y_{\mathcal{D}}|x_{\mathcal{D}},\theta)\), but just written in terms of the function \(f_{\theta}\). The prior \(p(f_{\theta})\) however is a different function from \(p(\theta)\), because we incur an additional Jacobian factor in this change of variables, making the posteriors also different.

We must take care in interpreting the quantity \(p(f_{\theta})\) since probability densities in infinite-dimensional vector spaces are generally ill-defined. While prior work [14, 23, 24, 25, 30] avoids this problem by considering a prior only over functions evaluated at a finite number of evaluation points, we provide a more general objective that enables the use of infinitely many evaluation points to construct a more informative prior and makes the relevant design choices of function-space map estimation more interpretable.

## 3 Understanding Function-Space Maximum A Posteriori Estimation

Function-space map estimation seeks to answer a fundamentally different question than parameter-space map estimation, namely, what is the most likely function under the posterior distribution over functions implied by the posterior distribution over parameters, rather than the most likely parameters under the parameter posterior.

To better understand the benefits and shortfalls of function-space map estimation, we derive a function-space map objective that generalizes the objective considered by prior work and analyze its properties both theoretically and empirically.

### The Finite Evaluation Point Objective

Starting from Equation (4), Wolpert [30] proposed to instead find the map estimate for \(f_{\theta}(\hat{x})\), the function evaluated at a finite set of points \(\hat{x}=\{x_{1},...,x_{M}\}\), where \(M<\infty\) can be chosen to be arbitrarily large so as to capture the behavior of the function to arbitrary resolution. This choice then yields the fs-map optimization objective

\[\mathcal{L}_{\mathrm{finite}}(\theta;\hat{x})=\sum\nolimits_{i=1}^{N}\log p( y_{\mathcal{D}}^{(i)}|x_{\mathcal{D}}^{(i)},f_{\theta}(\hat{x}))+\log p(f_{ \theta}(\hat{x})),\] (5)

where \(p(f_{\theta}(\hat{x}))\) is a well-defined but not in general analytically tractable probability density function. (See Appendix B.1 for further discussion.) Let \(P\) be the number of parameters \(\theta\), and \(K\) the number of function outputs. Assuming the set of evaluation points is sufficiently large so that \(MK\geq P\), using a generalization of the change of variable formula that only assumes injectivity rather than bijectivity, Wolpert [30] showed that the prior density over \(f(\hat{x})\) is given by

\[p(f_{\theta}(\hat{x}))=p(\theta)\det^{-1/2}(\mathcal{J}(\theta;\hat{x})),\] (6)

where \(\mathcal{J}(\theta;\hat{x})\) is a \(P\)-by-\(P\) matrix defined by

\[\mathcal{J}_{\mathrm{finite}}(\theta;\hat{x})\doteq J_{\theta}(\hat{x})^{ \top}J_{\theta}(\hat{x})\] (7)

and \(J_{\theta}(\hat{x})\doteq\partial f_{\theta}(\hat{x})/\partial\theta\) is the \(MK\)-by-\(P\) Jacobian of \(f_{\theta}(\hat{x})\), viewed as an \(MK\)-dimensional vector, with respect to the parameters \(\theta\). Substituting Equation (6) into Equation (5) the function-space map objective as a function of the parameters \(\theta\) can be expressed as

\[\mathcal{L}_{\mathrm{finite}}(\theta;\hat{x})=\sum\nolimits_{i=1}^{N}\log p( y_{\mathcal{D}}^{(i)}|x_{\mathcal{D}}^{(i)},f_{\theta})+\log p(\theta)-\frac{1}{2} \log\det(\mathcal{J}(\theta;\hat{x})),\] (8)

where we are allowed to condition directly on \(f_{\theta}\) instead of \(f_{\theta}(\hat{x})\) because by assumption \(\hat{x}\) is large enough to uniquely determine the function. In addition to replacing the function, our true quantity of interest, with its evaluations at a finite set of points \(\hat{x}\), this objective makes it unclear how we should select \(\hat{x}\) and how that choice can be interpreted or justified in a principled way.

### Deriving a More General Objective and its Interpretation

We now derive a more general class of function-space map objectives that allow us to use effectively infinitely many evaluation points and meaningfully interpret the choice of those points.

In general, when performing a change of variables from \(v\in\mathbb{R}^{n}\) to \(u\in\mathbb{R}^{m}\) via an injective map \(u=\varphi(v)\), their probability densities relate as \(p(u)\mathrm{d}\mu(u)=p(v)\mathrm{d}\nu(v)\), where \(\mu\) and \(\nu\) define respective volume measures. Suppose we let \(\mathrm{d}\mu(u)=\sqrt{\det(g(u))\mathrm{d}^{m}}u\), the volume induced by an \(M\times M\) metric tensor \(g\), and \(\mathrm{d}\nu(v)=\mathrm{d}^{n}v\), the Lebesgue measure, then we can write \(\mathrm{d}\mu(u)=\sqrt{\det(\tilde{g}(v))\mathrm{d}\nu(v)}\), where the \(N\times N\) metric \(\tilde{g}(v)=J(v)^{\top}g(u)J(v)\) is known as the pullback of \(g\) via \(\varphi\) and \(J\) is the Jacobian of \(\varphi\)[6]. As a result, we have \(p(u)=p(v)\mathrm{det}^{-1/2}(J(v)^{\top}g(u)J(v))\). Applying this argument and identifying \(u\) with \(f_{\theta}(\hat{x})\) and \(v\) with \(\theta\), Wolpert [30] thereby establishes \(p(f_{\theta}(\hat{x}))=p(\theta)\det^{-1/2}(J_{\theta}(\hat{x})^{\top}J_{ \theta}(\hat{x}))\).

However, an important implicit assumption in the last step is that the metric in function space is Euclidean. That is, \(g=I\) and the squared distance between \(f_{\theta}(\hat{x})\) and \(f_{\theta}(\hat{x})+\mathrm{d}f_{\theta}(\hat{x})\) is \(\mathrm{d}s^{2}=\sum_{i=1}^{M}\mathrm{d}f_{\theta}(x_{i})^{2}\), rather than the general case \(\mathrm{d}s^{2}=\sum_{i=1}^{M}\sum_{j=1}^{M}g_{ij}\mathrm{d}f_{\theta}(x_{i}) \mathrm{d}f_{\theta}(x_{j})\). To account for a generic metric \(g\), we therefore replace \(J_{\theta}(\hat{x})^{\top}J_{\theta}(\hat{x})\) with \(J_{\theta}(\hat{x})^{\top}gJ_{\theta}(\hat{x})\). For simplicity, we assume the function output is univariate (\(K=1\)) and only consider \(g\) that is constant i.e., independent of \(f_{\theta}(\hat{x})\) and diagonal, with \(g_{ii}=g(x_{i})\) for some function \(g:\mathcal{X}\to\mathbb{R}^{+}\). For a discussion of the general case, see Appendix B.2. To better interpret the choice of \(g\), we rewrite

\[J_{\theta}(\hat{x})^{\top}gJ_{\theta}(\hat{x})=\sum\nolimits_{j=1}^{M}\tilde{ p}_{X}(x_{j})J_{\theta}(x_{j})^{\top}J_{\theta}(x_{j}),\] (9)

where we suggestively defined the alias \(\tilde{p}_{X}(x)\doteq g(x)\) and \(J_{\theta}(x_{j})\) is the \(K\)-by-\(P\)-dimensional Jacobian evaluated at the point \(x_{j}\). Generalizing from the finite and discrete set \(\hat{x}\) to the possibly infinite entire domain \(\mathcal{X}\subseteq\mathbb{R}^{D}\) and further dividing by an unimportant normalization constant \(Z\), we obtain

\[\mathcal{J}(\theta;p_{X})\doteq\frac{1}{Z}\int_{\mathcal{X}}\tilde{p}_{X}(x)J_ {\theta}(x)^{\top}J_{\theta}(x)\,\mathrm{d}x=\mathbb{E}_{p_{X}}\big{[}J_{ \theta}(X)^{\top}J_{\theta}(X)\big{]},\] (10)where \(p_{X}=\tilde{p}_{X}/Z\) is a normalized probability density function, with normalization \(Z\)--which is independent of \(\theta\)--only appearing as an additive constant in \(\log\det\mathcal{J}(\theta;p_{X})\). We include a further discussion about this limit in Appendix B.4.

Under this limit, we can identify \(\log p(\theta)-\frac{1}{2}\log\det\mathcal{J}(\theta;p_{X})\) with \(\log p(f_{\theta})\) up to a constant, and thereby express \(\log p(f_{\theta}|\mathcal{D}),\) the function-space map objective in Equation (4) as

\[\mathcal{L}(\theta;p_{X})=\sum\nolimits_{i=1}^{N}\log p(y_{\mathcal{D}}^{(i)} |x_{\mathcal{D}}^{(i)},f_{\theta})+\log p(\theta)-\frac{1}{2}\log\det\mathcal{ J}(\theta;p_{X}),\] (11)

where the choice of \(p_{X}\) corresponds to a choice of the metric \(g\) in function space. Equation (11) (and approximations thereof) will be the primary object of interest in the remainder of this paper.

Evaluation Distribution as Function-Space Geometry.From this discussion and Equations (10) and (11), it is now clear that the role of the metric tensor \(g\) is to impose a distribution \(p_{X}\) over the evaluation points. Or conversely, a given distribution over evaluation points implies a metric tensor, and as such, specifies the geometry of the function space. This correspondence is intuitive: points \(x\) with higher values of \(g(x)\) contribute more to defining the geometry in function space and therefore should be assigned higher weights under the evaluation distribution when maximizing function space posterior density. Suppose, for example, \(f_{\theta}\) is an image classifier for which we only care about its outputs on set of natural images \(\mathcal{I}\) when comparing it to another image classifier. The metric \(g(\cdot)\) and therefore the evaluation distribution \(p_{X}\) only needs support in \(\mathcal{I}\), and the fs-map objective is defined only in terms of the Jacobians evaluated at natural images \(x\in\mathcal{I}\).

Finite Evaluation Point Objective as a Special Case.Consequently, the finite evaluation point objective in Equation (8) can be arrived at by specifying the evaluation distribution \(p_{X}(x)\) to be \(\hat{p}_{X}(x)=\frac{1}{M}\sum_{x^{\prime}\in\hat{x}}\delta(x-x^{\prime})\), where \(\delta\) is the Dirac delta function and \(\hat{x}=\{x_{1},...,x_{M}\}\) with \(M<\infty\), as before. It is easy to see that \(\mathcal{J}_{\mathrm{finite}}(\theta;\hat{x})\propto\mathcal{J}(\theta;\hat{p} _{X})\). Therefore, the objective proposed by Wolpert [30] is a special case of the more general class of objectives.

### Investigating the Properties of Function-Space map Estimation

To illustrate the properties of fs-map, we consider the class of models \(f_{\theta}(x)=\sum_{i=1}^{P}\sigma(\theta_{i})\varphi_{i}(x)\) with domain \(\mathcal{X}=[-1,1]\), where \(\{\varphi_{i}\}_{i=1}^{P}\) is a fixed set of basis functions and \(\sigma\) is a non-linear function to introduce a difference between function-space and parameter-space map. The advantage of working with this class of models is that \(\mathcal{J}(\theta;p_{X})\) has a simple closed form, such that

\[\mathcal{J}_{ij}(\theta;p_{X})=\mathbb{E}_{p_{X}}\big{[}\partial_{\theta_{i}} f_{\theta}(X)\partial_{\theta_{j}}f_{\theta}(X)\big{]}=\sigma^{\prime}( \theta_{i})\sigma^{\prime}(\theta_{j})\Phi_{ij},\] (12)

where \(\Phi\) is a constant matrix with elements \(\Phi_{ij}=\mathbb{E}_{p_{X}}[\varphi_{i}(X)\varphi_{j}(X)]\). Therefore, \(\Phi\) can be precomputed once and reused throughout training. In this experiment, we use the set of Fourier features \(\{\cos(k_{i}\cdot),\sin(k_{i}\cdot)\}_{i=1}^{100}\) where \(k_{i}=i\pi\) and set \(\sigma=\mathrm{tanh}\). We generate training data by sampling \(x_{\mathrm{train}}\sim\mathrm{Uniform}(-1,1)\), \(\theta_{i}\sim\mathcal{N}(0,\alpha^{2})\) with \(\alpha=10\), evaluating \(f_{\theta}(x_{\mathrm{train}})\), and adding Gaussian noise with standard deviation \(\sigma^{*}=0.1\) to each observation. We use 1,000 test points sampled from \(\mathrm{Uniform}(-1,1)\). To train the model, we set the prior \(p(\theta)\) and the likelihood to correspond to the data-generating process. For fs-map, we specify \(p_{X}=\mathrm{Uniform}(-1,1)\), the ground-truth distribution of test inputs, which conveniently results in \(\Phi=I/2\).

FS-map Finds More Probable Functions.Figure 1(a) shows the improvement in \(\log p(f_{\theta}|\mathcal{D})\) when using fs-map over ps-map. As expected, fs-map consistently finds functions with much higher posterior probability \(p(f_{\theta}|\mathcal{D})\).

Figure 2: **fs-map exhibits desirable properties.** On a non-linear regression problem, fs-map empirically (a) learns more probable functions, (b) finds flatter minima, (c) improves generalization, and (d) is less prone to overfitting. The plot shows means and standard deviations computed from 3 random seeds.

FS-map Prefers Flat Minima.In Figure 1(b), we compare the curvature of the minima found by fs-map and ps-map, as measured by the average eigenvalue of the Hessian of the mean squared error loss, showing that fs-map consistently finds flatter minima. As the objective of fs-map favors small singular values for the Jacobian \(J_{\theta}\), when multiple functions can fit the data, the fs-map objective will generically prefer functions that are less sensitive to perturbations of the parameters, leading to flatter minima. To make this connection more precise, consider the Hessian \(\nabla^{2}\mathcal{L}(\theta)\). If the model fits the training data well, we can apply the Gauss-Newton approximation: \(\nabla^{2}_{\theta}\mathcal{L}(\theta)\approx\frac{1}{|\mathcal{D}|}\,J_{ \theta}(x_{\mathcal{D}})^{\top}J_{\theta}(x_{\mathcal{D}})\), which is identical to \(\mathcal{J}(\theta;p_{X})\) if \(p_{X}\) is chosen to be the empirical distribution of the training inputs. More generally, a distribution \(p_{X}\) with high density over likely inputs will assign high density to the training inputs, and hence minimizing \(\mathcal{J}(\theta;p_{X})\) will similarly reduces the magnitude of \(J_{\theta}(x_{\mathcal{D}})\). Therefore, the fs-map objective explicitly encourages finding flatter minima, which have been found to correlate with generalization [10; 17; 18], robustness to data perturbations and noisy activations [3; 12] for neural networks.

fs-map can Achieve Better Generalization.Figure 1(c) shows that fs-map achieves lower test RMSE across a wide range of sample sizes. It's worth noting that this synthetic example satisfies two important criteria such that fs-map is likely to improve generalization over ps-map. First, the condition outlined in Section 4 for a well-behaved fs-map objective is met, namely that the set of partial derivatives \(\{\partial_{\theta_{i}}f_{\theta}^{j}(\cdot)\}_{i=1}^{P}\) are linearly independent. Specifically, the partial derivatives are given by \(\{\mathrm{sech}(\theta_{i})\sin(k_{i}\cdot),\mathrm{sech}(\theta_{i})\cos(k_{ i}\cdot)\}_{i=1}^{P}\). Since \(\mathrm{sech}\) is non-zero everywhere, the linear independence follows from the linear independence of the Fourier basis. As a result, the function space prior has no singularities and fs-map is thus able to learn from data. The second important criterion is the well-specification of the probabilistic model, which we have defined to precisely match the true data-generating process. Therefore, fs-map seeks the most likely function according to its _true_ probability, without incorrect modeling assumptions.

fs-map is Less Prone to Overfitting.As shown in Figure 1(d), fs-map tends to have a higher train RMSE than ps-map as a result of the additional log determinant regularization. While ps-map achieves near-zero train RMSE with a small number of samples by overfitting to the noise, fs-map's train RMSE is consistently around \(\sigma^{*}=0.1,\) the true standard deviation of the observation noise.

Performance Improves with Number of Evaluation Points.We compare fs-map estimation with \(p_{X}=\mathrm{Uniform}(-1,1)\) and with a finite number of equidistant evaluation points in \([-1,1]\), where the former corresponds to the latter with infinitely many points. In Figure 2(a), we show that the test RMSE of the fs-map estimate (evaluated at 400 training samples) decreases monotonically until the number of evaluation points reaches 200. The more evaluation points, the more the finite-point fs-map objective approximates its infinite limit and the better it captures the behavior of the function. Indeed, 200 points is the minimum sampling rate required such that the Nyquist frequency reaches the maximum frequency in the Fourier basis, explaining the saturation of the performance.

Choice of Evaluation Distribution is Important.In Figure 2(b), we compare test RMSE for 400 training samples for the default choice of \(p_{X}=\mathrm{Uniform}(-1,1)\), \(p_{X}=\mathrm{Uniform}(-0.1,0.1)\)--a distribution that does not reflect inputs at test time--and ps-map (\(p_{X}=\mathrm{N}/\mathrm{A}\)) for reference. The result shows that specifying the evaluation distribution to correctly reflect the distribution of inputs at test time is required for fs-map to achieve optimal performance in this case.

Figure 3: **Effect of important hyperparameters in fs-map.** fs-map improves with the number of evaluation points and requires carefully specifying the evaluation distribution and the probabilistic model.

## 4 Limitations and Practical Considerations

We now discuss the limitations of function-space map estimation and propose partial remedies, including a scalable approximation that improves its applicability to modern deep learning.

### fs-map Does not Necessarily Generalize Better than ps-map

_There is no guarantee that the most likely function is the one that generalizes the best, especially since our prior can be arbitrary. For example, fs-map can under-fit the data if the log determinant introduces excessive regularization, which can happen if our probabilistic model is misspecified by overestimating the observation noise. Returning to the setup in Section 3.3, in Figure 4, we find that fs-map (with 400 training samples) is sensitive to the observation noise scale \(\sigma\) in the likelihood. As \(\sigma\) deviates from the true noise scale \(\sigma^{*}\), the test RMSE of fs-map can change dramatically. At \(\sigma/\sigma^{*}=1/3\), the likelihood dominates in both the fs-map and ps-map objectives, resulting in similar test RMSEs. At \(\sigma/\sigma^{*}=10\), the likelihood is overpowered by the log determinant regularization in fs-map and causes the model to under-fit the data and achieve a high test error. By contrast, the performance of the ps-map estimate is relatively stable. Finally, even if our prior exactly describes the data-generating process, the function that best generalizes won't necessarily be the most likely under the posterior._

### Pathological Solutions

In order for function-space map estimation to be useful in practice, the prior "density" \(p(f_{\theta})=p(\theta)\det^{-\frac{1}{2}}(\mathcal{J}(\theta;p_{X}))\) must not be infinite for any allowed values of \(\theta\), since if it were, those values would constitute global optima of the objective function independent of the actual data. To ensure that there are no such pathological solutions, we require the matrix \(\mathcal{J}(\theta;p_{X})\) to be non-singular for any allowed \(\theta\). We present two results that help determine if this requirement is met.

**Theorem 1**.: _Assuming \(p_{X}\) and \(J_{\theta}=\partial_{\theta}f_{\theta}\) are continuous, \(\mathcal{J}(\theta;p_{X})\) is non-singular if and only if the partial derivatives \(\{\partial_{\theta_{i}}f_{\theta}(\cdot)\}_{i=1}^{P}\) are linearly independent functions over the support of \(p_{X}\)._

Proof.: See Appendix B.5 

**Theorem 2**.: _If there exists a non-trivial symmetry \(S\in\mathbb{R}^{P\times P}\) with \(S\neq I\) such that \(f_{\theta}=f_{S\theta}\) for all \(\theta\), then \(\mathcal{J}(\theta^{*};p_{X})\) is singular for all fixed points \(\theta^{*}\) where \(S\theta^{*}=\theta^{*}\)._

Proof.: See Appendix B.6 

Theorem 1 is analogous to \(A^{\top}A\) having the same null space as \(A\) for any matrix \(A\). It suggests that to avoid pathological optima the effect of small changes to each parameter must be linearly independent. Theorem 2 builds on this observation to show that if the model exhibits symmetries in its parameterization, fs-map will necessarily have pathological optima. Since most neural networks at least possess permutation symmetries of the hidden units, we show that these pathological fs-map solutions are almost universally present, generalizing the specific cases observed by Wolpert [30].

A Simple Remedy to Remove Singularities.Instead of finding a point approximation of the function space posterior, we can perform variational inference under a variational family \(\{q(\cdot|\theta)\}_{\theta}\), where \(q(\cdot|\theta)\) is localized around \(\theta\) with a small and constant _function-space_ entropy \(h\). Ignoring constants and \(\mathcal{O}(h)\) terms, the variational lower bound is given by

\[\mathcal{L}_{\mathrm{VLB}}(\theta;p_{X})=\sum\nolimits_{i=1}^{N}\log p(y^{(i)} _{\mathcal{D}}|x^{(i)}_{\mathcal{D}},f_{\theta})+\log p(\theta)-\frac{1}{2} \mathbb{E}_{q(\theta^{\prime}|\theta)}\left[\log\det\mathcal{J}(\theta^{\prime };p_{X})\right].\] (13)

Similar to how convolving an image with a localized Gaussian filter removes high-frequency components, the expectation \(\mathbb{E}_{q(\theta^{\prime}|\theta)}\) removes potential singularities in \(\log\det\mathcal{J}(\theta^{\prime};p_{X})\). This effect can be approximated by simply adding a small diagonal jitter \(\epsilon\) to \(\mathcal{J}(\theta;p_{X}):\)

\[\hat{\mathcal{L}}(\theta;p_{X})\doteq\sum\nolimits_{i=1}^{N}\log p(y^{(i)}_{ \mathcal{D}}|x^{(i)}_{\mathcal{D}},f_{\theta})+\log p(\theta)-\frac{1}{2}\log \det\left(\mathcal{J}(\theta;p_{X})+\epsilon I\right).\] (14)

Alternatively, we know \(\mathbb{E}_{q(\theta^{\prime}|\theta)}\left[\log\det\mathcal{J}(\theta^{\prime };p_{X})\right]\) must be finite because the variational lower bound cannot exceed the log marginal likelihood. \(\hat{\mathcal{L}}(\theta;p_{X})\) can be used as a minimal modification of fs-map that eliminates pathological optima. We provide full derivation for these results in Appendix B.7.

Figure 4: fs-map can underperform ps-map with a misspecified probabilistic model.

### Does the Function-Space map Better Approximate the Bayesian Model Average?

The Bayesian model average (BMA), \(f_{\mathrm{BMA}}\), of the function \(f_{\theta}\) is given by the posterior mean, that is:

\[f_{\mathrm{BMA}}(\cdot)\,\dot{=}\,\mathbb{E}_{p(\theta|\mathcal{D})}[f_{\theta}( \cdot)]=\mathbb{E}_{p(f_{\theta}|\mathcal{D})}[f_{\theta}(\cdot)].\]

This expectation is the same when computed in parameter or function space, and has theoretically desirable generalization properties [19; 29]. Both ps-map and fs-map provide point approximations of \(f_{\mathrm{BMA}}\). However, fs-map seeks the mode of the distribution \(p(f_{\theta}|\mathcal{D})\), the mean of which we aim to compute. By contrast ps-map finds the mode of a distinct distribution, \(p(\theta|\mathcal{D})\), which can markedly diverge from \(p(f_{\theta}|\mathcal{D})\), depending on the parameterization. Consequently, it is reasonable to anticipate that the fs-map objective generally encourages finding a superior estimate of the BMA.

Consider the large data regime where the posterior in both parameter and function space follows a Gaussian distribution, in line with the Bernstein-von Mises theorem [27]. In Gaussian distributions, the mode and mean coincide, and therefore \(f_{\hat{\theta}\textsc{ss-map}}=f_{\mathrm{BMA}}\). However, even in this setting, where \(\hat{\theta}^{\textsc{ps-MAP}}=\mathbb{E}_{p(\theta|\mathcal{D})}[\theta]\), generally \(f_{\hat{\theta}^{\textsc{ps-map}}}\neq f_{\mathrm{BMA}}\), because the expectation does not distribute across a function \(f\) that is non-linear in its parameters \(\theta\): \(f_{\mathbb{E}_{p(\theta|\mathcal{D})}[\theta]}(\cdot)\neq\mathbb{E}_{p(\theta |\mathcal{D})}[f_{\theta}(\cdot)]\).

However, we find that whether fs-map better approximates the BMA depends strongly on the problem setting. Returning to the setup in Figure 1, where we have a Gaussian mixture regression model, we compare the BMA function with functions learned by fs-map and ps-map under the prior \(p(\theta)=\mathcal{N}(0,\alpha^{2})\) with several settings of \(\alpha\). We observe in Figure 5 that fs-map only approximates the BMA function better than ps-map at larger \(\alpha\) values. To understand this behavior, recall the height \(h_{R}\) for the right Gaussian bump is given by \(\exp(\theta_{R})\), which has a \(\mathrm{logramal}(0,\alpha^{2})\) prior. As we increase \(\alpha\), more prior mass is assigned to \(h_{R}\) with near-zero value and therefore to functions with small values within \(x\in[0,1]\). While the lognormal distribution also has a heavy tail at large \(h_{R}\), the likelihood constrains the posterior \(p(h_{R}|\mathcal{D})\) to only place high mass for small \(h_{R}\). These two effects combine to make the posterior increasingly concentrated in function space around functions described by \(h_{L}\approx 1\) and \(h_{R}\approx 0\), implying that the mode in function space should better approximate its mean. In contrast, as we decrease \(\alpha,\) both the prior and posterior become more concentrated in parameter space since the parameter prior \(p(\theta)=\mathcal{N}(0,\alpha^{2})\) approaches the delta function at zero, suggesting that ps-map should be a good approximation to the BMA function. By varying \(\alpha\), we can interpolate between how well ps-map and fs-map approximate the BMA.

### Scalable Approximation for Large Neural Networks

In practice, the expectation \(\mathcal{J}(\theta;p_{X})=\mathbb{E}_{p_{X}}[J_{\theta}(X)^{\top}J_{\theta}(X)]\) is almost never analytically tractable due to the integral over \(X\). We show in Appendix B.9 that a simple Monte Carlo estimate for \(\mathcal{J}(\theta;p_{X})\) with \(S\) samples of \(X\) can yield decent accuracy. For large neural networks, this estimator is still prohibitively expensive: each sample will require \(K\) backward passes, taking a total of \(\mathcal{O}(SKP)\) time. In addition, computing the resulting \(SK\)-by\(P\) determinant takes time \(\mathcal{O}(SKP^{2})\) (assuming \(P\geq SK\)). However, we can remedy the challenge of scalability by further leaning into the variational objective described in Equation (13) and consider the regime where \(\epsilon\gg\lambda_{i}\) for all eigenvalues \(\lambda_{i}\) of \(\mathcal{J}(\theta;p_{X}))\). To first order in \(\max_{i}\lambda_{i}/\epsilon\), we have \(\log\det(\mathcal{J}(\theta;p_{X}))+\epsilon I)=\frac{1}{2}\Delta_{\psi}d( \theta,\theta+\psi))\big{|}_{\psi=0}\) where \(d(\theta,\theta^{\prime})\,\dot{=}\,\mathbb{E}_{p_{X}}[\left\lVert f_{\theta} (X)-f_{\theta^{\prime}}(X)\right\rVert^{2}]\) and \(\Delta=\sum_{i=1}^{P}\partial_{\theta_{i}}^{2}\) denotes the Laplacian operator. Exploiting the identity \(\frac{1}{2}\Delta_{\psi}d(\theta,\theta+\psi))\big{|}_{\psi=0}=\frac{1}{\beta^ {2}}\mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2}I)}[d(\theta,\theta+\psi)]+ \mathcal{O}\big{(}\beta^{2}\big{)}\) and choosing \(\beta\) small enough, we obtain an accurate Monte Carlo estimator for the Laplacian

Figure 5: fs-map does not necessarily approximate the BMA better than ps-map. A Gaussian mixture regression model, where the right Gaussian has weight \(\exp(\theta_{R})\) with prior \(p(\theta_{R})=\mathcal{N}(0,\alpha^{2})\). As we increase \(\alpha\), we interpolate between ps-map and fs-map approximating the BMA.

using only forward passes. The resulting objective which we refer to as Laplacian Regularized map (l-map) is given by

\[\mathcal{L}_{\text{\sc l-map}}(\theta;p_{X})\doteq\sum\nolimits_{i=1}^{N}\log p( y_{\mathcal{D}}^{(i)}|x_{\mathcal{D}}^{(i)},f_{\theta})+\log p(\theta)-\frac{1}{2 \epsilon\beta^{2}}\mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2}I)}[d(\theta, \theta+\psi)].\] (15)

We use a single sample of \(\psi\) and \(S\) samples of evaluation points for estimating \(d(\theta,\theta+\psi)\) at each step, reducing the overhead from \(\mathcal{O}\big{(}SKP^{2}\big{)}\) to only \(\mathcal{O}(SP)\). A more detailed exposition is available in Appendix B.10.

### Experimental Evaluation of l-map

We now evaluate l-map applied to neural networks on various commonly used datasets. We provide full experimental details and extended results in Appendix C. Unlike the synthetic task in Section 3.3, in applying neural networks to these more complex datasets we often cannot specify a prior that accurately models the true data-generating process, beyond choosing a plausible architecture whose high-level inductive biases align with the task (e.g. CNNs for images) and a simple prior \(p(\theta)\) favoring smooth functions (e.g. an isotropic Gaussian with small variances). Therefore, we have less reason to expect l-map should outperform ps-map in these settings.

UCI Regression.In Table 1, we report normalized test RMSE on UCI datasets [1], using an MLP with 3 hidden layers and 256 units. l-map achieves lower error than ps-map on 7 out 8 datasets. Since the inputs are normalized and low-dimensional, we use \(p_{X}=\mathcal{N}(0,I)\) for l-map.

Image Classification.In Table 2, we compare l-map and ps-map on image classification using a ResNet-18 [9]. l-map achieves comparable or slightly better accuracies and is often better calibrated. We further test the effectiveness of l-map with transfer learning with a larger ResNet-50 trained on ImageNet. In Table 3, we show l-map also achieves small improvements in accuracy and calibration in transfer learning.

Loss Landscape.In Figure 6 (Left), we show that l-map indeed finds flatter minima. Further, we plot the Laplacian estimate in Figure 6 (Right) as the training progresses. We see that the Laplacian is much lower for l-map, showing its effectiveness at constraining the eigenvalues of \(\mathcal{J}(\theta;p_{X})\).

Distribution of Evaluation Points.In Table 2, we study the impact of the choice of distribution of evaluation points. Alongside our main choice of the evaluation set (KMNIST for FashionMNIST and CIFAR-100 for CIFAR-10), we use two additional distributions - the training set itself and a white noise \(\mathcal{N}(0,I)\) distribution of the same dimensions as the training inputs. For both tasks, we find that using an external evaluation set beyond the empirical training distribution is often beneficial.

Number of Evaluation Point Samples.In Figure 7(a), we compare different Monte Carlo sample sizes \(S\) for estimating the Laplacian. Overall, l-map is not sensitive to this choice in terms of accuracy. However, calibration error [20] sometimes monotonically decreases with \(S\).

\begin{table}
\begin{tabular}{l|c c c c c|c c c} \hline \hline Method & Boston & Concrete & Energy & Naval & Power & Protein & Winered & Winwhite \\ \hline ps-map & \(\mathbf{329\pm.033}\) & \(.272\pm.016\) & \(.042\pm.003\) & \(.032\pm.005\) & \(.219\pm.006\) & \(.584\pm.005\) & \(.851\pm.029\) & \(.758\pm.013\) \\ l-map & \(.352\pm.040\) & \(\mathbf{.261\pm.013}\) & \(\mathbf{.041\pm.002}\) & \(\mathbf{.018\pm.002}\) & \(\mathbf{.218\pm.005}\) & \(\mathbf{.580\pm.005}\) & \(\mathbf{.792\pm.031}\) & \(\mathbf{.714\pm.017}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Normalized test RMSE (\(\downarrow\)) on UCI datasets. We report mean and standard errors over six trials.

\begin{table}
\begin{tabular}{l|c c c c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{FashionMNIST} & \multicolumn{4}{c}{CIFAR-10} \\  & Acc.\(\uparrow\) & Sel. Pred.\(\uparrow\) & NLL\(\downarrow\) & ECE\(\downarrow\) & Acc.\(\uparrow\) & Sel. Pred.\(\uparrow\) & NLL\(\downarrow\) & ECE\(\downarrow\) \\ \hline ps-map & \(93.9\%\pm.1\) & \(98.6\%\pm.1\) & \(.26\pm.01\) & \(4.0\%\pm.1\) & \(95.4\%\pm.1\) & \(99.4\%\pm.0\) & \(.18\pm.00\) & \(2.5\%\pm.1\) \\ \hline l-map \(p_{X}\)=\(\mathcal{N}(0,I)\) & \(94.0\%\pm.0\) & \(99.2\%\pm.0\) & \(.25\pm.01\) & \(4.0\%\pm.2\) & \(95.3\%\pm.1\) & \(99.4\%\pm.0\) & \(.20\pm.00\) & \(3.0\%\pm.1\) \\ \(p_{X}\)=Train & \(93.8\%\pm.1\) & \(99.2\%\pm.1\) & \(.27\pm.01\) & \(4.3\%\pm.2\) & \(95.6\%\pm.1\) & \(99.5\%\pm.0\) & \(.18\pm.01\) & \(2.6\%\pm.0\) \\ \(p_{X}\)=\(\mathcal{D}^{\prime}\) & \(94.1\%\pm.1\) & \(99.2\%\pm.1\) & \(.26\pm.01\) & \(4.1\%\pm.1\) & \(95.5\%\pm.1\) & \(99.5\%\pm.0\) & \(.16\pm.01\) & \(1.4\%\pm.1\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: We report the accuracy (acc.), negative log-likelihood (nll), expected calibration error [20] (ece), and area under selective prediction accuracy curve [7] (Sel. Pred.) for FashionMNIST [31] (\(\mathcal{D}^{\prime}\) = KMNIST [4]), and CIFAR-10 [15] (\(\mathcal{D}^{\prime}\) = CIFAR-100). A ResNet-18 [9] is used. Std. errors are reported over five trials.

Robustness to Label Noise.In Figure 7(b), we find l-map is slightly more robust to label noise than ps-map on CIFAR-10.

Main Takeaways.l-map shows qualitatively similar properties as fs-map such as favoring flat minima and often provides better calibration. However, it achieves comparable or only slightly better accuracy on more complex image classification tasks. In line with our expectations, these results suggest that accounting for the precise difference between fs-map and ps-map is less useful without a sufficiently well-motivated prior.

## 5 Discussion

While we typically train our models through ps-map, we show that fs-map has many appealing properties in addition to re-parametrization invariance. We empirically verify these properties, including the potential for better robustness to noise and improved calibration. But we also reveal a more nuanced and unexpected set of pros and cons for each approach. For example, while it is natural to assume that fs-map more closely approximates a Bayesian model average, we clearly demonstrate how there can be a significant discrepancy. Moreover, while ps-map is not invariant to re-parametrization, which can be seen as a fundamental pathology, we show fs-map has its own failure modes such as pathological optima, as well as practical challenges around scalability. In general, our results suggest the benefits of fs-map will be greatest when the prior is sufficiently well-motivated.

Our findings engage with and contribute to active discussions across the literature. For example, several works have argued conceptually--and found empirically--that solutions in _flatter_ regions of the loss landscape correspond to better generalization [8; 10; 11; 12; 25]. On the other hand, Dinh et al. [5] argue that the ways we measure flatness, for example, through Hessian eigenvalues, are not parameterization invariant, making it possible to construct striking failure modes. Similarly, ps-map estimation is not parameterization invariant. However, our analysis and comparison to fs-map estimation raise the question to what extent lack of parametrization invariance is actually a significant practical shortcoming--after all, we are not reparametrizing our models on the fly, and we have evolved our models and training techniques conditioned on standard parameterizations.

Figure 6: **Empirical evidence that l-map finds flatter minima.****(Left)** For various step sizes in 20 randomly sampled directions starting at the minima, we compute the training loss averaged over all directions to summarize the local loss landscape. We use filter normalization for landscape visualization [16]. l-map visibly finds flatter minima. **(Right)** We plot the Laplacian estimate throughout training, showing l-map is indeed effective at constraining the eigenvalues of \(\mathcal{J}(\theta;p_{X})\).

Figure 7: **(Left)** Calibration error can be reduced by increasing the number of Monte Carlo samples for the Laplacian estimator in Equation (15). **(Right)** l-map is slightly more robust to training with label noise. For each level of noise, we replace a fraction of labels with uniformly random labels.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Method & Acc. \(\uparrow\) & Sel. Pred. \(\uparrow\) & NLL \(\downarrow\) & ECE \(\downarrow\) \\ \hline ps-map & \(96.3\%\pm 0.1\) & \(99.5\%\pm 0.1\) & \(0.18\pm 0.01\) & \(2.6\%\pm 0.2\) \\ l-map & \(96.4\%\pm 0.1\) & \(99.5\%\pm 0.1\) & \(0.15\pm 0.01\) & \(2.1\%\pm 0.1\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Transfer learning from ImageNet with ResNet-50 on CIFAR-10 can lead to slightly better accuracy and improved calibration.

**Acknowledgements.** We thank Marc Finzi, Pavel Izmailov, and Micah Goldblum for helpful discussions. This work is supported by NSF CAREER IIS-2145492, NSF I-DISRE 193471, NSF IIS-1910266, BigHat Biosciences, Capital One, and an Amazon Research Award.

## References

* [1] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
* [2] Christopher M. Bishop. Pattern recognition and machine learning (information science and statistics). 2006.
* [3] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer T. Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: biasing gradient descent into wide valleys. _Journal of Statistical Mechanics: Theory and Experiment_, 2019, 2016.
* [4] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David Ha. Deep learning for classical japanese literature, 2018.
* [5] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028, 2017.
* [6] Christopher Terence John Dodson and Timothy Poston. _Tensor geometry: the geometric viewpoint and its uses_, volume 130. Springer Science & Business Media, 2013.
* [7] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. _Journal of Machine Learning Research_, 11(53):1605-1641, 2010.
* [8] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _International Conference on Learning Representations_, 2020.
* [9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pages 770-778. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.90.
* [10] Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. _Neural Computation_, 9:1-42, 1997.
* [11] P Izmailov, AG Wilson, D Podoprikhin, D Vetrov, and T Garipov. Averaging weights leads to wider optima and better generalization. In _34th Conference on Uncertainty in Artificial Intelligence 2018, UAI 2018_, pages 876-885, 2018.
* [12] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _International Conference on Learning Representations_, 2016.
* [13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [14] Leo Klarner, Tim G. J. Rudner, Michael Reutlinger, Torsten Schindler, Garrett M. Morris, Charlotte Deane, and Yee Whye Teh. Drug Discovery under Covariate Shift with Domain-Informed Prior Distributions over Functions. In _Proceedings of the 40th International Conference on Machine Learning_, Proceedings of Machine Learning Research. PMLR, 2023.
* [15] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [16] Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein. Visualizing the loss landscape of neural nets. In _Neural Information Processing Systems_, 2017.
* [17] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.

* [18] Wesley J. Maddox, Gregory W. Benton, and Andrew Gordon Wilson. Rethinking parameter counting in deep models: Effective dimensionality revisited. _CoRR_, abs/2003.02139, 2020.
* [19] Kevin P. Murphy. _Machine learning : a probabilistic perspective_. MIT Press, Cambridge, Mass. [u.a.], 2013. ISBN 9780262018029 0262018020.
* [20] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In _Proceedings of the AAAI conference on artificial intelligence_, volume 29, 2015.
* [21] Michael Peskin. _An introduction to quantum field theory_. CRC press, 2018.
* [22] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do cifar-10 classifiers generalize to cifar-10? 2018.
* [23] Tim G. J. Rudner, Zonghao Chen, Yee Whye Teh, and Yarin Gal. Tractable Function-Space Variational Inference in Bayesian Neural Networks. In _Advances in Neural Information Processing Systems 35_, 2022.
* [24] Tim G. J. Rudner, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, and Yarin Gal. Continual Learning via Sequential Function-Space Variational Inference. In _Proceedings of the 39th International Conference on Machine Learning_, Proceedings of Machine Learning Research. PMLR, 2022.
* [25] Tim G. J. Rudner, Sanyam Kapoor, Shikai Qiu, and Andrew Gordon Wilson. Function-Space Regularization in Neural Networks: A Probabilistic Perspective. In _Proceedings of the 40th International Conference on Machine Learning_, Proceedings of Machine Learning Research. PMLR, 2023.
* [26] Robert Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the royal statistical society series b-methodological_, 58:267-288, 1996.
* [27] Aad W Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* [28] Steven Weinberg. Gravitation and cosmology: principles and applications of the general theory of relativity. 1972.
* [29] Andrew Gordon Wilson and Pavel Izmailov. Bayesian deep learning and a probabilistic perspective of generalization. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [30] David H. Wolpert. Bayesian backpropagation over i-o functions rather than weights. In J. Cowan, G. Tesauro, and J. Alspector, editors, _Advances in Neural Information Processing Systems_, volume 6. Morgan-Kaufmann, 1993.
* [31] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.

**Appendix**

## Appendix A ps-map is not Invariant to Reparametrization

* **Mathematical Details & Derivations*
* B.1 Prior Distributions over Functions
* B.2 The General Case
* B.3 Why Should a Constant Metric Affect fs-map?
* B.4 Comments on the Infinite Limit
* B.5 Condition for a Non-Singular \(\mathcal{J}(\theta;p_{X})\)
* B.6 Architectural Symmetries Imply Singular \(\mathcal{J}(\theta;p_{X})\)
* B.7 Addressing the Infinite Prior Density Pathology with a Variational Perspective
* B.8 Approximating the Expectation
* B.9 Monte Carlo Estimator for the Log Determinant
* B.10 Laplacian Regularized map Objective
* **Further Empirical Results and Experimental Details*
* C.1 Training details for Synthetic Experiments
* C.2 Hyperparameters for UCI Regression
* C.3 Hyperparameters for Image Classification Experiments
* C.4 Verifying the l-map Approximation
* C.5 Effective Eigenvalue Regularization using the Laplacian Regularizer
* C.6 Visualizing the Effect of Laplacian Regualrization
* C.7 Further Neural Network Experiments

## Appendix A ps-map is not Invariant to Reparametrization

Parameter-space map estimation has conceptual and theoretical shortcomings stemming from the lack of reparameterization-invariance of map estimation. Suppose we reparameterize the model parameters \(\theta\) with \(\theta^{\prime}=\mathcal{R}(\theta)\) where \(\mathcal{R}\) is an invertible transformation. The prior density on the reparameterized parameters is obtained from the change-of-variable formula that states \(p^{\prime}(\theta^{\prime})=p(\theta)|\mathrm{det}^{-1}(\mathrm{d}\theta^{ \prime}/\mathrm{d}\theta)|\). In the language of differential geometry, the prior density \(p(\theta)\) is not a scalar but a scalar density, a quantity that is not invariant under coordinate transformations [28]. The probabilistic model is fundamentally unchanged, as we are merely viewing the parameters in a different coordinate system, but the map objective is not invariant to reparameterization. Specifically, the reparameterized parameter-space map objective becomes

\[\mathcal{L}^{\prime\textsc{MAP}}(\theta^{\prime})=\sum\nolimits_{i=1}^{N} \log p(y_{\mathcal{D}}^{(i)}\mid x_{\mathcal{D}}^{(i)},\theta^{\prime})+\log p ^{\prime}(\theta^{\prime})=\mathcal{L}^{\textsc{MAP}}(\theta)-\underbrace{ \log|\mathrm{det}^{-1}(\mathrm{d}\theta^{\prime}/\mathrm{d}\theta)|}_{\text{ new term}}.\] (A.1)

Since \(\mathcal{L}^{\prime\textsc{MAP}}(\theta^{\prime})\) and the original objective \(\mathcal{L}^{\textsc{MAP}}(\theta)\) differ by a term which is non-constant in the parameters if \(T\) is non-linear, the maxima \(\theta^{\prime\textsc{MAP}}\doteq\arg\max_{\theta^{\prime}}\mathcal{L}^{ \prime\textsc{MAP}}(\theta^{\prime})\) and \(\theta^{\prime\textsc{MAP}}\doteq\arg\max_{\theta}\mathcal{L}^{\textsc{MAP} }(\theta)\) under the two objectives will be different. Importantly, by "different" we don't just mean \(\theta^{\prime\textsc{MAP}}\neq\theta^{\textsc{MAP}}\) but \(\theta^{\prime\textsc{MAP}}\neq\mathcal{R}(\theta^{\prime\textsc{MAP}})\). That is, they are not simply each other viewed in a different coordinate system but actually represent different functions. More accurately, therefore, one should say map estimation is not _equivariant_ (rather than invariant) under reparameterization. As a result, when using a parameter-space map estimate to make predictions at test time, the predictions can change dramatically simply depending on how the model is parameterized. In fact, one can reparameterize the model so that ps-map will return an arbitrary solution \(\theta_{0}\) irrespective of the observed data, by choosing a reparameterization \(\theta^{\prime}=\mathcal{R}(\theta)\) such that \(\log|\mathrm{det}^{-1}(\mathrm{d}\theta^{\prime}/\mathrm{d}\theta)|_{\theta= \theta_{0}}=-\infty\). One such choice is \(\theta^{\prime}=\left(\theta-\theta_{0}\right)^{3},\) where the exponent is taken element-wise.

As a less extreme example, consider the reparameterization \(\theta^{\prime}=1/\theta\). A Gaussian prior on the original parameters \(p(\theta)\propto\exp\bigl{(}-\theta^{2}/2\bigr{)}\) translates to a prior \(p^{\prime}(\theta^{\prime})\propto\exp\bigl{(}-1/2\theta^{\prime 2}\bigr{)}/ \theta^{\prime 2}=\theta^{2}\exp\bigl{(}-\theta^{2}/2\bigr{)}\) on the inverted parameters. Note the transformed prior, when mapped onto the original parameters \(\theta\), assigns a higher weight on non-zero values of \(\theta\) due to the quadratic factor coming from the log determinant. We illustrate this effect with a simple experiment where a linear model with RBF features is trained with a Gaussian likelihood and a Gaussian prior \(\mathcal{N}(0,I)\) using ps-map to fit noisy observations from a simple function. Figure 8 show the predictions and learned weights when ps-map is performed both in the original parameterization and in the inverted parameterization. While ps-map favors small weights in the original parameterization, it favors non-zero weights in the inverted one, learning a less smooth function composed of a larger number of RBF bases.

Figure 8: **ps-map is not invariant to reparameterization.** Simply inverting the parameterization of the weights removes the preference for small coefficients in a linear model, leading to a less smooth learned function, even though the underlying probabilistic model is unchanged.

## Appendix B Mathematical Details & Derivations

### Prior Distributions over Functions

Since the function evaluations \(f_{\theta}(\hat{x})\) are related to the parameters \(\theta\) through the map \(T:\theta\mapsto f_{\theta}(\hat{x}),\) the prior density \(p(f_{\theta}(\hat{x}))\) is simply the push forward of the density \(p(\theta)\) through \(T,\) which can be expressed without loss of generality using the Dirac delta function as

\[p(f_{\theta}(\hat{x}))=\int_{\mathbb{R}^{P}}p(\theta^{\prime})\delta(T(\theta^{ \prime})-f_{\theta}(\hat{x}))\,\mathrm{d}\theta^{\prime}.\] (B.2)

This generic expression as an integral over the parameter space is difficult to evaluate and thus not directly useful. By assuming that \(T\) is injective (one-to-one) modulo some global symmetries in parameter space (e.g., permutation of the neurons), which holds for many neural network architectures [2], Wolpert [30] established the simplification in Equation (6). Note that since the map \(T\) is not guaranteed to be surjective (onto), especially when there are more evaluation points than parameters (\(|\hat{x}|>P\)), \(p(f_{\theta}(\hat{x}))\) does not generally have support everywhere and is to be interpreted as a surface density defined over the image of \(T\)[30]. If the model parameterization has continuous symmetries, such as scaling symmetries in MLPs with ReLU activations [5], then the log determinant in Equation (6) should be replaced by the log pseudo-determinant to account for the fact that the dimension of the image of \(T\) is less than \(P,\) the number of parameters.

### The General Case

Here we present a more general result for which the result in Section 3.2 is a special case, without assuming the function output is univariate (\(K=1\)) or that the metric is diagonal. To simplify notations, we suppress all \(\theta\)-dependence and write \(f_{\theta}^{k}(x_{i}),\) the \(k\)-th output of the function at \(x_{i},\) as \(f_{i}^{k}\) for some \(\hat{x}=\{x_{i}\}_{i=1}^{M}\). We use Einstein notation to imply summation over repeated indices and leave everything in un-vectorized forms. In this more general case, the metric is given by \(\mathrm{d}s^{2}=g_{ikj\ell}\mathrm{d}f_{i}^{k}\mathrm{d}f_{j}^{\ell}\), where \(i,j\) are input indices and \(k,\ell\) are output indices. Denote the un-vectorized \(M\)-by-\(K\)-by-\(P\) Jacobian \(J_{ika}=\partial_{\theta_{a}}f_{i}^{k}\). The \(P\)-by-\(P\) matrix \(\mathcal{J}\) appearing inside the determinant in Equation (6) is now given by

\[\mathcal{J}_{ab}=J_{ika}g_{ikj\ell}J_{j\ell b}=g_{ikj\ell}\partial_{\theta_{a} }f_{i}^{k}\partial_{\theta_{b}}f_{j}^{\ell}\] (B.3)

In the limit of infinitely many evaluation points uniformly populating the domain \(\mathcal{X}\), the sum over \(i,j\) becomes a double integral2

Footnote 2: To be more accurate, an integral of the form \(\int\phi(x)\,\mathrm{d}x\) is the limit of \(\sum_{i=1}^{M}\phi(x_{i})\Delta x\) with the extra factor \(\Delta x=x_{i+1}-x_{i}\) (\(\forall i\)), as \(M\to\infty\). However, accounting for this factor does not affect the final result in our case because again \(\Delta x\) is independent of \(\theta\).

\[\mathcal{J}_{ab}=\int_{\mathcal{X}\times\mathcal{X}}\partial_{\theta_{a}}f_{ \theta}^{k}(x)\partial_{\theta_{b}}f_{\theta}^{\ell}(x^{\prime})g_{k\ell}(x,x ^{\prime})\,\mathrm{d}x\,\mathrm{d}x^{\prime},\] (B.4)

where we have re-written the metric \(g_{ikj\ell}\) as \(g_{k\ell}(x_{i},x_{j})\) where \(g:\mathcal{X}\times\mathcal{X}\to\mathbb{R}^{K\times K}\) is the metric represented now as a matrix-valued function. After dividing by a constant \(Z,\) we can similarly write \(\mathcal{J}_{ab}\) as the expectation

\[\mathcal{J}_{ab}=\,\mathbb{E}_{p_{XX^{\prime}CC^{\prime}}}\Big{[}\partial_{ \theta_{a}}f_{\theta}^{C}(X)\partial_{\theta_{b}}f_{\theta}^{C^{\prime}}(X^{ \prime})\mathrm{sgn}(g_{CC^{\prime}}(X,X^{\prime}))\Big{]},\] (B.5)

where \(p_{XX^{\prime}CC^{\prime}}(x,x^{\prime},c,c^{\prime})=|g_{cc^{\prime}}(x,x^{ \prime})|/Z\) for some normalization constant \(Z\) that only shifts the log determinant by a \(\theta\)-independent constant. Note Equation (10) is recovered if the metric is diagonal, that is \(g_{c,c^{\prime}}(x,x^{\prime})\neq 0\) only if \(c=c^{\prime}\) and \(x=x^{\prime}.\) We can further remove the assumption that \(g\) is constant (independent of \(f\)), but at the cost of letting the normalization constant \(Z\) depend on \(f\) (or equivalently \(\theta\)), making it invalid to ignore \(Z\) during optimization. While more general than the result presented in Section 3.2, specifying a (possibly \(\theta\)-dependent) non-diagonal metric adds a significant amount of complexity without being particularly well-motivated. Therefore we did not further investigate this more general objective.

### Why Should a Constant Metric Affect fs-map?

One may object that a constant metric \(g\) should have no effect in the map estimate for \(f_{\theta}\). However, the subtlety arises from the fact that \(p(f_{\theta})\) does not have support everywhere but is, in fact, a surface density defined on the image of \(T\) (as defined in Appendix B.1), a curved manifold in function space which gives rise to locally preferred directions such that even a global linear transformation will change the density in a non-homogeneous way. If \(T\) were instead surjective, then in Section 3.2\(J_{\theta}(\hat{x})\) would be a square matrix and we could write \(\log\det\bigl{(}J_{\theta}(\hat{x})^{\top}gJ_{\theta}(\hat{x})\bigr{)}=\log \det\bigl{(}J_{\theta}(\hat{x})^{\top}J_{\theta}(\hat{x})\bigr{)}+\log\det(g)\) and conclude that a constant metric indeed has no effect on the map estimate. Similarly, if the image of \(T\) is not curved, meaning \(J_{\theta}(\hat{x})\) is constant in \(\theta\), then \(\log\det\bigl{(}J_{\theta}(\hat{x})^{\top}gJ_{\theta}(\hat{x})\bigr{)}\) would be constant in \(\theta\) regardless of \(g\) and the metric would have no effect.

### Comments on the Infinite Limit

In Section 3.2, we generalized the finite evaluation point objective from Equation (8) by considering the limit as the number of evaluation points \(\hat{x}\) approaches infinity and the points cover the domain \(\mathbb{R}^{P}\) uniformly and densely. This technique, known as the continuum limit [21], is commonly used in physics to study continuous systems with infinite degrees of freedom, by first discretizing and then taking the limit as the discretization scale approaches zero, thereby recovering the behavior of the original system without directly dealing with intermediate (possibly ill-defined) infinite-dimensional quantities. Examples include lattice field theories where the continuous spacetime is discretized into a lattice, and numerical analysis where differential equations are discretized and solved on a mesh, where the solution often converges to the continuous solution as the mesh size goes to zero.

In a similar vein, we utilize this technique to sidestep direct engagement with the ill-defined quantity \(p(f_{\theta})\). It may be possible to assign a well-defined value to \(p(f_{\theta})\) through other techniques, though we expect the result obtained would be consistent with ours, given the continuum limit has historically yielded consistent results with potentially more refined approaches in other domains.

### Condition for a Non-Singular \(\mathcal{J}(\theta;p_{X})\)

Proof.: Recall \(\mathcal{J}(\theta;p_{X})=\mathbb{E}_{p_{X}}\bigl{[}J_{\theta}(X)^{\top}J_{ \theta}(X)\bigr{]}.\) Suppose \(\mathcal{J}(\theta;p_{X})\) is singular. Then there is a vector \(v\neq 0\) for which \(0=v^{\top}\mathcal{J}(\theta;p_{X})v=\mathbb{E}_{p_{X}}\bigl{[}\left\lVert J_ {\theta}(X)v\right\rVert_{2}^{2}\bigr{]}=\int_{\mathcal{X}}\left\lVert J_{ \theta}(X)v\right\rVert_{2}^{2}p_{X}(x)\mathrm{d}x=0\). Since the integrand is non-negative and continuous by assumption, it is zero everywhere. Therefore, we have \(\sum_{i=1}^{P}v_{i}\partial_{\theta_{i}}f_{\theta}(x)=0\) for all \(x\in\operatorname{supp}(p_{X})\) for a non-zero \(v,\) showing \(\{\partial_{\theta_{i}}f_{\theta}(\cdot)\}_{i=1}^{P}\) are linearly dependent functions over the support of \(p_{X}\).

Conversely, if \(\{\partial_{\theta_{i}}f_{\theta}(\cdot)\}_{i=1}^{P}\) are linearly dependent functions over the support of \(p_{X}\), then \(\sum_{i=1}^{P}v_{i}\partial_{\theta_{i}}f_{\theta}(x)=J_{\theta}(x)v=0\) for all \(x\in\operatorname{supp}(p_{X})\) for some \(v\neq 0.\) Hence \(\mathcal{J}(\theta;p_{X})v=\mathbb{E}_{p_{X}}\bigl{[}J_{\theta}(X)^{\top}J_{ \theta}(X)v\bigr{]}=0,\) showing \(\mathcal{J}(\theta;p_{X})\) is singular. 

### Architectural Symmetries Imply Singular \(\mathcal{J}(\theta;p_{X})\)

Proof.: Differentiating with respect to \(\theta\), we have \(J_{\theta}=\partial_{\theta}f_{\theta}=\partial_{\theta}f_{\theta\theta}=J_{S \theta}S\) for all \(\theta\). Suppose there exists \(\theta^{*}\) that is invariant under \(S,S\theta^{*}=\theta^{*}\), then \(J_{\theta^{*}}=J_{\theta^{*}}S\) and \(J_{\theta^{*}}(I-S)=0.\) Since, by assumption \(S\neq I\), we have shown that \(J_{\theta^{*}}\) has a non-trivial nullspace. Because the nullspace of \(\mathcal{J}(\theta^{*};p_{X})\) contains the nullspace of \(J_{\theta^{*}},\) we conclude \(\mathcal{J}(\theta;p_{X})\) is also singular. 

As an example, consider a single hidden layer MLP \(f_{\theta}(x)=w_{2}^{\top}\sigma(w_{1}x),\)\(\theta=\{w_{1}\in\mathbb{R}^{2},w_{2}\in\mathbb{R}^{2}\},\) where we neglected the bias for simplicity. The permutation \(P_{12}\bigoplus P_{12}\) is a symmetry of \(f_{\theta},\) where \(\bigoplus\) is the direct sum and \(P_{12}\) is the transposition \(\begin{pmatrix}0&1\\ 1&0\end{pmatrix}.\) The nullspace of \(J_{\theta^{*}}\) contains the image of

\[\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\bigoplus\begin{pmatrix}1&-1\\ -1&1\end{pmatrix}\] (B.6)

with a basis \(\{(1,-1,0,0),(0,0,1,-1)\},\) for any \(\theta^{*}\) of the form \((a,a,b,b).\) It's easy to check that perturbing the parameters in directions \((1,-1,0,0)\) and \((0,0,1,-1)\) leaves the function output unchanged, due to symmetry in the parameter values and the network topology.

Continuous symmetries, among other reasons, can also lead to singular \(\mathcal{J}(\theta;p_{X})\) and are not covered by the above analysis, which is specific to discrete symmetries. One example is scaling symmetries in MLPs with ReLU activations [5]. However, these scaling symmetries cause \(\mathcal{J}(\theta;p_{X})\) to be singular for all \(\theta\) in a manner that does not introduce any pathological optimum. This becomes clear when noting that the space of functions implemented by an MLP with \(P\) parameters using ReLU activations lies in a space with dimension lower than \(P\) due to high redundancies in the parameterization. The resolution is to simply replace all occurrences of \(\log\det\mathcal{J}(\theta;p_{X})\) with \(\log|\mathcal{J}(\theta;p_{X})|_{+}\), where \(|\cdot|_{+}\) represents the pseudo-determinant. This step is automatic if we optimize Equation (14) instead of Equation (11), where adding a small jitter will automatically compute the log pseudo-determinant (up to an additive constant) when \(\mathcal{J}(\theta;p_{X})\) is singular. By contrast, symmetries such as permutation symmetries do create pathological optima because they only make the Jacobian singular at specific settings of \(\theta\), thereby assigning to those points infinitely higher prior density compared to others.

### Addressing the Infinite Prior Density Pathology with a Variational Perspective

We now decribe the remedy that leads to the objective in Equation (13). As shown in Section 4.2, almost all commonly-used neural networks suffer from the pathology such that a singular Jacobian leads infinite prior density at the singularities. Such singularities arise because fs-map optimize for a point approximation for the function space posterior, equivalent to variational inference with the family of delta functions \(\{q(f_{\theta^{\prime}}|\theta)=\delta(f_{\theta^{\prime}}-f_{\theta})\}_{\theta}\).

We can avoid the singularities if we instead choose a variational family where each member is a distribution over \(f\) localized around \(f_{\theta}\) for some \(\theta\). Namely, we consider the family \(\mathcal{Q}\doteq\{q(f_{\theta^{\prime}}|\theta)\}_{\theta\in\Theta}\) parameterized by a mean-like parameter \(\theta\) and a small but fixed entropy in function space \(H(q(f_{\theta^{\prime}}|\theta))=\mathbb{E}_{q(f_{\theta^{\prime}}|\theta)}[- \log q(f_{\theta^{\prime}}|\theta)]=h\) for all \(\theta\in\Theta\). For convenience, we overload the notation and use \(q(\theta^{\prime}|\theta)\) to denote the pullback of \(q(f_{\theta^{\prime}}|\theta)\) to parameter space. The resulting variational lower bound is

\[\mathcal{L}_{\mathrm{VLB}}(\theta;p_{X})\] \[=\mathbb{E}_{q(f_{\theta^{\prime}}|\theta)}\bigg{[}\log\frac{q(f_ {\theta^{\prime}}|\theta)}{p(f_{\theta}|\mathcal{D})}\bigg{]}\] \[=\mathbb{E}_{q(f_{\theta^{\prime}}|\theta)}[-\log p(f_{\theta^{ \prime}}|\mathcal{D})]-\overleftarrow{H(q(f_{\theta^{\prime}}|\theta))}\] \[=\mathbb{E}_{q(f_{\theta^{\prime}}|\theta)}[-\log p(\mathcal{D}| f_{\theta^{\prime}})]+\mathbb{E}_{q(f_{\theta^{\prime}}|\theta)}[-\log p(f_{ \theta^{\prime}})]+\mathrm{const.}\] (Bayes' rule) \[=\mathbb{E}_{q(\theta^{\prime}|\theta)}[-\log p(\mathcal{D}| \theta^{\prime})]+\mathbb{E}_{q(\theta^{\prime}|\theta)}[-\log p(\theta^{ \prime})]+\frac{1}{2}\mathbb{E}_{q(\theta^{\prime}|\theta)}[\log\det(\mathcal{ J}(\theta^{\prime};p_{X}))]\] (Equation ( 11 )) \[=-\log p(\mathcal{D}|f_{\theta})-\log p(\theta)+\frac{1}{2} \mathbb{E}_{q(\theta^{\prime}|\theta)}[\log\det(\mathcal{J}(\theta^{\prime};p_ {X}))]+\mathcal{O}(h)+\mathrm{const.},\]

where in the last line we used the assumption that \(q(\theta^{\prime}|\theta)\) is localized around \(\theta\) to write the expectation of \(\log p(\mathcal{D}|\theta)\) and \(\log p(\theta)\) as their values at \(\theta^{\prime}=\theta\) plus \(\mathcal{O}(h)\) corrections (which we will henceforth omit since by assumption \(h\) is tiny), because they vary smoothly as a function of \(\theta\). More care is required to deal with the remaining expectation, since when \(\mathcal{J}(\theta;p_{X})\) is singular at \(\theta\), \(\log\det(\mathcal{J}(\theta;p_{X}))\) is infinite, but its expectation, \(\mathbb{E}_{q(\theta^{\prime}|\theta)}[\log\det(\mathcal{J}(\theta^{\prime};p_ {X}))]\), must be finite (assuming \(p(\theta)\) and \(p(\mathcal{D}|\theta)\) are finite), given that \(\mathcal{L}_{\mathrm{VLB}}(\theta;p_{X})\) lower bounds the log marginal likelihood \(\log p(\mathcal{D})=\log\int p(\mathcal{D}|\theta)p(\theta)\mathrm{d}\theta<\infty\). As we will show in Appendix B.8, the effect of taking the expectation of the log determinant is similar to imposing a lower limit \(\epsilon\) on the eigenvalue of \(\mathcal{J}(\theta;p_{X})\), which can be approximated by adding a jitter \(\epsilon\) when computing the log determinant. This effect is similar to applying a high-frequency cutoff to an image by convolving it with a localized Gaussian filter.

With this approximation, we arrive at a simple objective inspired by such a variational perspective, equivalent to adding a jitter inside the log determinant computation in the fs-map objective,

\[\hat{\mathcal{L}}(\theta;p_{X})=\sum\nolimits_{i=1}^{N}\log p(y_{\mathcal{D}}^ {(i)}|x_{\mathcal{D}}^{(i)},\theta)+\log p(\theta)-\frac{1}{2}\log\det(\mathcal{ J}(\theta;p_{X})+\epsilon I).\] (B.7)

Note that while using a jitter is common in practice for numerical stability, it arises for an entirely different reason here.

[MISSING_PAGE_FAIL:18]

which shows \(\mathcal{J}(\theta;p_{X}))=\frac{1}{2}\nabla_{\psi}^{2}d(\theta,\theta+\psi) \big{|}_{\psi=0}\). Therefore,

\[\operatorname{Tr}(\mathcal{J}(\theta;p_{X}))=\frac{1}{2}\operatorname{Tr}\! \left(\nabla_{\psi}^{2}d(\theta,\theta+\psi)\right)\big{|}_{\psi=0}=\frac{1}{2 }\Delta_{\psi}d(\theta,\theta+\psi))\big{|}_{\psi=0},\] (B.12)

where \(\Delta\) is the Laplacian operator.

To estimate the Laplacian, consider the following expectation:

\[\mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2}I)}[d(\theta,\theta+ \psi)]\] (B.13) \[= \mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2}I)}\big{[}\psi^{ \top}\mathcal{J}(\theta;p_{X}))\psi+\mathcal{O}\big{(}\psi^{4}\big{)}\big{]}\] (B.14) \[= \mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2}I)}\big{[}\psi^{ \top}\mathcal{J}(\theta;p_{X}))\psi\big{]}+\mathcal{O}\big{(}\beta^{4}\big{)}\] (B.15) \[= \beta^{2}\operatorname{Tr}(\mathcal{J}(\theta;p_{X}))+\mathcal{ O}\big{(}\beta^{4}\big{)},\] (B.16)

where we used

\[\mathbb{E}_{\psi}\big{[}\psi^{\top}\mathcal{J}(\theta;p_{X}))\psi\big{]}=\sum _{ij}\mathbb{E}_{\psi}[\psi_{i}\psi_{j}]\mathcal{J}_{ij}(\theta;p_{X})=\beta^ {2}\sum_{ij}\delta_{ij}\mathcal{J}_{ij}(\theta;p_{X})=\beta^{2}\operatorname {Tr}(\mathcal{J}(\theta;p_{X})).\] (B.17)

Therefore, we have

\[\operatorname{Tr}(\mathcal{J}(\theta;p_{X}))=\frac{1}{2}\Delta_{\psi}d( \theta,\theta+\psi))\big{|}_{\psi=0}=\frac{1}{\beta^{2}}\mathbb{E}_{\psi\sim \mathcal{N}(0,\beta^{2}I)}[d(\theta,\theta+\psi)]+\mathcal{O}\big{(}\beta^{2} \big{)}.\] (B.18)

Combining Equation (B.10) and Equation (B.18), we have shown Equation (14) reduces to the more efficiently computable l-map objective for large enough \(\epsilon\) and small enough \(\beta\) :

\[\mathcal{L}_{\text{\tiny L-MAP}}(\theta;p_{X})\doteq\sum\nolimits_{i=1}^{N} \log p(y_{\mathcal{D}}^{(i)}\mid x_{\mathcal{D}}^{(i)},f_{\theta})+\log p( \theta)-\frac{1}{2\epsilon\beta^{2}}\mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^ {2})}[d(\theta,\theta+\psi)].\] (B.19)

The entire objective is negated and divided by \(N\) to yield the loss function

\[L_{\text{\tiny L-MAP}}(\theta;p_{X})\doteq\underbrace{-\frac{1}{N}\sum \nolimits_{i=1}^{N}\log p(y_{\mathcal{D}}^{(i)}\mid x_{\mathcal{D}}^{(i)},f_{ \theta})-\frac{1}{N}\log p(\theta)}_{\text{Standard regularized loss}}+ \lambda\underbrace{\left(\frac{1}{\beta^{2}}\mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2})}[d(\theta,\theta+\psi)]\right)}_{\text{Laplacian regularization }R(\theta;\beta)},\] (B.20)

where we have absorbed the \(1/N\) factor into the hyperparameter \(\lambda=\frac{1}{2\epsilon N}\). Therefore, using l-map amounts to simply adding a regularization \(\lambda R(\theta;\beta^{2})\) to standard regularized training (ps-map). \(\beta\) is a tolerance parameter for approximating the Laplacian and can simply be fixed to a small value such as \(10^{-3}\).

Figure 9: **Approximating the log determinant via an \(S\)-sample simple Monte Carlo estimator and jitter. Each dot represents the exact and estimated log determinant evaluated at random parameters sampled by randomly initializing the network followed by scaling by a factor \(s\sim\operatorname{loguniform}(0.1,10)\) to include parameters of different magnitudes. The dashed line shows \(x=y\).**

## Appendix C Further Empirical Results and Experimental Details

### Training details for Synthetic Experiments

For both ps-map and fs-map, we train with the Adam [13] optimizer with a learning rate \(0.1\) for 2,500 steps to maximize the respective log posteriors. For fs-map, we precompute the \(\Phi\) matrix and reuse it throughout training. When \(p_{X}\) is not \(\mathrm{Uniform}(-1,1)\), \(\Phi\) does not have a simple form and we use 10,000 Monte Carlo samples to evaluate it.

### Hyperparameters for UCI Regression

We use an MLP with 3 hidden layers, 256 units, and ReLU activations. We train it with the Adam optimizer for 10,000 steps with a learning rate of \(10^{-3}\). For each dataset, we tune hyperparameters based on validation RMSE, where the validation set is constructed by holding out \(10\%\) of training data. We tune both the weight decay (corresponding to prior precision) and l-map's \(\lambda\) over the choices \(\{10^{-5},10^{-4},10^{-3},10^{-2},10^{-1}\}\). We then report the mean and standard error of test RMSE across 6 runs using the best hyperparameters selected this way. In each dataset, the inputs and outputs are standardized based on training statistics.

### Hyperparameters for Image Classification Experiments

Both the weight decay scale, corresponding to the variance of a Gaussian prior, and the l-map hyperparameter \(\lambda\) in Equation (B.20) is tuned over the range \([10^{-1},10^{-10}]\) using randomized grid search. For ps-map, \(\lambda\) is set to 0. The parameter variance in the Laplacian estimator is fixed to \(\beta^{2}=10^{-6}\). In addition, we clip the gradients to unit norm. We use a learning rate of \(0.1\) with SGD and a cosine decay schedule over 50 epochs for FashionMNIST and 200 epochs for CIFAR-10. The mini-batch size is fixed to 128.

### Verifying the l-map Approximation

To verify that the l-map objective is a good approximation to Equation B.7 when \(\epsilon\) is large enough compared to the eigenvalues of \(\mathcal{J}(\theta;p_{X})\), we compare \(\log\det(\mathcal{J}(\theta;p_{X})+\epsilon I)-P\log(\epsilon)\) with the Laplacian estimate \(\frac{1}{2\epsilon\beta^{2}}\mathbb{E}_{\psi\sim\mathcal{N}(0,\beta^{2})}[d( \theta,\theta+\psi)]\) used by l-map in Appendix B.10 as its first order approximation in \(\max_{i}\lambda_{i}(\theta)/\epsilon\). We reuse the same network architecture, evaluation distribution \(p_{X}\), and sampling procedure for the parameters from Appendix B.9 to perform the comparison in Figure 10 and color each point by \(\bar{\lambda}(\theta)/\epsilon\), the average eigenvalue of \(\mathcal{J}(\theta;p_{X})\) divided by \(\epsilon\). The smaller this value, the better the approximation should be. We observe that indeed as \(\epsilon\) increases and \(\bar{\lambda}(\theta)/\epsilon\) approaches 0, the Laplacian estimate becomes a more accurate approximation of the log determinant. Interestingly, even when \(\epsilon\ll\bar{\lambda}(\theta)\) and the approximation is not accurate, there still appears to be a monotonic relation between the estimate and the log determinant, suggesting that the Laplacian estimate will continue to produce a qualitatively similar regularization effect as the log determinant in that regime.

The Laplacian regularizer in Equation (B.19) only uses a sample-based estimator that is unbiased only in the limit that \(\sigma\to 0\). To keep the computational overhead at a minimum so that l-map can scale to large neural networks, we only take 1 sample of \(\psi\) per gradient step. To test the effectiveness of l-map in regularizing the eigenvalues of \(\mathcal{J}(\theta;p_{X})\) under this practical setting, we train an MLP with 2 hidden layers, 16 units, and \(\tanh\) activations on the Two Moons dataset (generated with sklearn.datasets.make_moons(n_samples=200, shuffle=True, noise=0.2, random_state=0)) for \(10^{4}\) steps with the Adam optimizer and a learning rate of \(0.001\). Here we choose \(p_{X}=\frac{1}{M}\sum_{i=1}^{M}\delta_{x_{i}}\) with \(M=1,600\) and \(\{x_{i}\}\) linearly spaced in the region \([-5,5]^{2}\). In Figure 11, we compare the sum of eigenvalues of \(\mathcal{J}(\theta;p_{X})\) for l-map and fs-map with different levels of jitter \(\epsilon\). Both fs-map and l-map significantly reduce the eigenvalues compared to ps-map with small values of \(\epsilon\), corresponding to stronger regularization.

### Visualizing the Effect of Laplacian Regularization

The hyperparameter \(\epsilon\) is inversely related to the strength of the Laplacian regularization. We visualize the effect of \(\epsilon\) in Figure 12, showing the l-map solution varies smoothly with \(\epsilon\), evolving from a near-zero function that severely underfits the data to one that fits the data perfectly.

Figure 12: **Visualization of l-map solutions at various \(\epsilon\).**

### Further Neural Network Experiments

In Table 4, we report the detailed results for varying the number of Monte Carlo samples \(S\) as reported by Section 4.5 and Figure 7. As noted previously, the performance is fairly robust to this hyperparameter \(S\) in terms of accuracy, selective accuracy, and negative \(\log\)-likelihood. However, we find that increasing \(S\) can sometimes lead to significant improvement in calibration, as seen for FashionMNIST [31].

Distribution Shift.We additionally evaluate our l-map trained models to assess their performance under covariate shift. Specifically, for models trained on CIFAR-10 [15], we use the additional test set from CIFAR-10.1 [22] which contains additional images collected after the original data, mimicing a covariate shift. As reported in Table 5, l-map tends to improve calibration, while retaining performance in terms of accuracy.

Transfer Learning.Using ResNet-50 trained on ImageNet, we test the effectiveness of l-map with transfer learning. We choose hyperparameters as in Appendix C.3, except a lower learning rate of \(10^{-3}\) and a smaller batch size of 64 due to computational constraints. Table 3 reports all the results.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline \multirow{2}{*}{\# Samples (\(S\))} & \multicolumn{4}{c}{FashionMNIST} & \multicolumn{4}{c}{CIFAR-10} \\  & Acc. \(\uparrow\) & Sel. Pred. \(\uparrow\) & NLL \(\downarrow\) & ECE \(\downarrow\) & Acc. \(\uparrow\) & Sel. Pred. \(\uparrow\) & NLL \(\downarrow\) & ECE \(\downarrow\) \\ \hline
4 & \(94.0\%\)\(\pm\)\(0.1\) & \(99.2\%\)\(\pm\)\(0.0\) & \(0.28\pm\)\(0.01\) & \(4.3\%\)\(\pm\)\(0.2\) & \(95.6\%\)\(\pm\)\(0.1\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.17\pm\)\(0.01\) & \(2.4\%\)\(\pm\)\(0.1\) \\
8 & \(94.0\%\)\(\pm\)\(0.2\) & \(99.2\%\)\(\pm\)\(0.0\) & \(0.27\pm\)\(0.01\) & \(4.2\%\)\(\pm\)\(0.3\) & \(95.6\%\)\(\pm\)\(0.1\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.17\pm\)\(0.00\) & \(2.2\%\)\(\pm\)\(0.1\) \\
16 & \(94.1\%\)\(\pm\)\(0.1\) & \(99.2\%\)\(\pm\)\(0.0\) & \(0.27\pm\)\(0.01\) & \(4.1\%\)\(\pm\)\(0.1\) & \(95.6\%\)\(\pm\)\(0.2\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.17\pm\)\(0.00\) & \(2.1\%\)\(\pm\)\(0.1\) \\
32 & \(93.8\%\)\(\pm\)\(0.1\) & \(99.2\%\)\(\pm\)\(0.0\) & \(0.28\pm\)\(0.01\) & \(4.4\%\)\(\pm\)\(0.2\) & \(95.5\%\)\(\pm\)\(0.1\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.16\pm\)\(0.00\) & \(1.8\%\)\(\pm\)\(0.2\) \\
64 & \(94.0\%\)\(\pm\)\(0.1\) & \(99.2\%\)\(\pm\)\(0.0\) & \(0.27\pm\)\(0.0\) & \(4.2\%\)\(\pm\)\(0.1\) & \(95.7\%\)\(\pm\)\(0.2\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.16\pm\)\(0.00\) & \(1.7\%\)\(\pm\)\(0.2\) \\
128 & \(94.1\%\)\(\pm\)\(0.1\) & \(99.2\%\)\(\pm\)\(0.1\) & \(0.26\pm\)\(0.01\) & \(4.1\%\)\(\pm\)\(0.1\) & \(95.5\%\)\(\pm\)\(0.1\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.16\pm\)\(0.00\) & \(1.4\%\)\(\pm\)\(0.1\) \\
256 & \(94.0\%\)\(\pm\)\(0.2\) & \(99.2\%\)\(\pm\)\(0.0\) & \(0.27\pm\)\(0.0\) & \(4.3\%\)\(\pm\)\(0.1\) & \(95.5\%\)\(\pm\)\(0.1\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.16\pm\)\(0.00\) & \(1.2\%\)\(\pm\)\(0.2\) \\
512 & \(93.9\%\)\(\pm\)\(0.1\) & \(99.1\%\)\(\pm\)\(0.0\) & \(0.26\pm\)\(0.01\) & \(4.2\%\)\(\pm\)\(0.1\) & \(95.5\%\)\(\pm\)\(0.1\) & \(99.5\%\)\(\pm\)\(0.0\) & \(0.16\pm\)\(0.00\) & \(1.2\%\)\(\pm\)\(0.1\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: This table provides detailed quantitative performance for the results plotted in Figure 7, ablating the choice of the number of samples used for evaluation of the Laplacian estimator \(S\). The standard deviations are reported over five trials.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Method & Acc. \(\uparrow\) & Sel. Pred. \(\uparrow\) & NLL \(\downarrow\) & ECE \(\downarrow\) \\ \hline ps-map & \(89.2\%\)\(\pm\)\(0.4\) & \(97.8\%\)\(\pm\)\(0.2\) & \(0.43\)\(\pm\)\(0.03\) & \(6.2\%\)\(\pm\)\(0.3\) \\ l-map & \(89.2\%\)\(\pm\)\(0.7\) & \(97.9\%\)\(\pm\)\(0.2\) & \(0.40\)\(\pm\)\(0.02\) & \(4.1\%\)\(\pm\)\(0.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: We evaluate the performance of CIFAR-10.1 [22] using the models trained on CIFAR-10 [15]. l-map tends to improve the data fit in terms of the log likelihood and is better calibrated while retaining the same performance as ps-map. The standard deviations are reported over five trials.