# Unsupervised Graph Neural Architecture Search

with Disentangled Self-supervision

 Zeyang Zhang\({}^{1}\)

 Xin Wang\({}^{1}\)

Ziwei Zhang\({}^{1}\)

Guangyao Shen\({}^{2}\)

Shiqi Shen\({}^{2}\)

Wenwu Zhu\({}^{1}\)

\({}^{1}\)Department of Computer Science and Technology, BNRist, Tsinghua University, \({}^{2}\)Wechat, Tencent

zy-zhang20@mails.tsinghua.edu.cn, {xin_wang, zwzhang}@tsinghua.edu.cn, {lucasgyshen, shiqishen}@tencent.com, wuzhu@tsinghua.edu.cn

This work was done during the author's internship at Wechat, Tencent Corresponding authors

###### Abstract

The existing graph neural architecture search (GNAS) methods heavily rely on supervised labels during the search process, failing to handle ubiquitous scenarios where supervisions are not available. In this paper, we study the problem of unsupervised graph neural architecture search, which remains unexplored in the literature. The key problem is to discover the latent graph factors that drive the formation of graph data as well as the underlying relations between the factors and the optimal neural architectures. Handling this problem is challenging given that the latent graph factors together with architectures are highly entangled due to the nature of the graph and the complexity of the neural architecture search process. To address the challenge, we propose a novel Disentangled Self-supervised Graph Neural Architecture Search (**DSGAS**) model, which is able to discover the optimal architectures capturing various latent graph factors in a self-supervised fashion based on unlabeled graph data. Specifically, we first design a disentangled graph super-network capable of incorporating multiple architectures with factor-wise disentanglement, which are optimized simultaneously. Then, we estimate the performance of architectures under different factors by our proposed self-supervised training with joint architecture-graph disentanglement. Finally, we propose a contrastive search with architecture augmentations to discover architectures with factor-specific expertise. Extensive experiments on 11 real-world datasets demonstrate that the proposed **DSGAS** model is able to achieve state-of-the-art performance against several baseline methods in an unsupervised manner.

## 1 Introduction

Graph neural architecture search (GNAS), aiming to automatically discover the optimal architecture for graph neural network (GNN) based on graph-structured data and task, has shown remarkable progress in enhancing the predictive power and saving human endeavors for various graph applications [1]. The existing GNAS methods generally follow a supervised paradigm such that they optimize the weights within architectures given a training dataset with a supervised loss (e.g., the cross entropy loss of label predictions) and estimate the architecture performance based on the validation dataset with supervision signals. For example, the label prediction accuracy is adopted for architecture ranking during the neural architecture search process [2; 3; 4]. As a result, supervised labels become indispensable for applying the existing GNAS methods.

However, ground-truth labels in reality may be extremely scarce or hardly available in many graph applications. For example, a variety of biological problems require a significant amount of human labors and time costs in clinical tests to obtain labels for supervision [5; 6; 7]. As the existingGNAS approaches heavily rely on supervised labels for weight training and architecture evaluation, they will suffer from performance deterioration in unsupervised settings, failing to discover optimal architectures in the scenarios where labels are scarce or not available.

In this paper, we study unsupervised graph neural architecture search, i.e., discovering optimal GNN architectures without labels for graph-structured data, which remains unexplored in the literature. The key problem lies in two important aspects: i) discover the latent graph factors that drive the formation process of graph data [8; 9; 10; 11; 12; 13; 14]; ii) capture the underlying relations between the factors and the optimal neural architectures. For instance, a molecular graph may consist of groups of atoms as well as bonds representing different functional units [15], requiring different optimal neural architectures to make accurate predictions.

Nevertheless, solving the problem is highly non-trivial and challenging given that the hidden factors are entangled in the graph and very difficult to capture, e.g., a social network may contain several communities originating from various interests (e.g., sports, games, etc.) [16; 10], with the nodes and edges belonging to different communities mixing together. Moreover, the architectures with different functional factors are also entangled within the weight-sharing super-network [17; 18; 19], resulting in inaccurate architecture performance estimations under different hidden factors.

To tackle the challenge, we propose a novel unsupervised graph neural architecture search method, i.e., Disentangled Self-supervised Graph Neural Architecture Search (**DSGAS**)3. Given graph data without supervised labels, our proposed **DSGAS** model can discover the optimal architectures capturing multiple latent factors in a self-supervised fashion. In particular, we first design a disentangled graph super-network, where multiple architectures are disentangled for simultaneous optimization w.r.t various latent factors. Then, we propose a self-supervised training with joint architecture-graph disentanglement, which disentangles architectures and graphs within a common latent space. The super-network is trained through a routing mechanism between architectures, graphs and self-supervised tasks, to obtain an accurate estimation of the architecture performance under each latent factor. Finally, we propose a contrastive search with architecture augmentations, where a novel architecture-level instance discrimination task is introduced to discover architectures with distinct capabilities of capturing various factors in a self-supervised fashion. Extensive experiments show that the proposed **DSGAS** model is able to significantly outperform the state-of-the-art GNAS baselines under both unsupervised and semi-supervised settings. Detailed ablation studies and analyses also demonstrate that **DSGAS** is able to discover effective architectures with our proposed disentangled self-supervision designs. The contributions of this paper are summarized as follows:

Footnote 3: The codes are available at Github.

* We are the first to study the problem of unsupervised graph neural architecture search and propose the Disentangled Self-supervised Graph Neural Architecture Search (**DSGAS**) model capable of discovering the optimal architectures without labels, to the best of our knowledge.
* We introduce three novel modules, i) disentangled graph architecture super-network, ii) self-supervised training with joint architecture-graph disentanglement and iii) contrastive search with architecture augmentations, which can discover the optimal architectures capturing various graph latent factors with disentangled self-supervision.
* Extensive experiments on 11 real-world graph datasets show that our proposed method **DSGAS** is able to discover effective graph neural architectures without supervised labels and significantly outperform the state-of-the-art baselines in both unsupervised and semi-supervised settings.

## 2 Preliminaries and Problem Formulation

Graph Neural Architecture SearchDenote the graph space as \(\mathcal{G}\) and the label space as \(\mathcal{Y}\). A graph neural network can be denoted as a function \(f_{\alpha,w}:\mathcal{G}\rightarrow\mathcal{Y}\), which is characterized by architecture parameters \(\alpha\in\mathcal{A}\) and learnable weights \(w\in\mathcal{W}\) given an architecture space \(\mathcal{A}\) and a weight space \(\mathcal{W}\). Graph neural architecture search (GNAS) aims at automating the design of graph neural architectures, i.e., obtaining the best-performed architectures by searching \(\alpha\). As \(\alpha\) is usually instantiated as selecting GNN operations, e.g., GCN [20], GAT [21], GIN [22], we also call \(\alpha\) as operation choices for brevity. Generally, GNAS solves the bi-level optimization problem [23] :

\[\alpha^{*} =\operatorname*{arg\,min}_{\alpha\in\mathcal{A}}\mathcal{L}_{\text {val}}(\alpha,w^{*}(\alpha)),\] (1) s.t. \[w^{*}(\alpha) =\operatorname*{arg\,min}_{w\in\mathcal{W}(\alpha)}\mathcal{L}_{ \text{train}}(\alpha,w),\] (2)

where \(\mathcal{L}_{\text{train}}\) and \(\mathcal{L}_{\text{val}}\) denotes the loss of the predictions of the architecture \(f_{\alpha,w}(\cdot)\) against supervised labels on training and validation datasets. The optimization problem can be viewed as having two objectives that Eq.(2) aims to obtain accurate architecture performance estimation, and Eq.(1) aims to search the best-performed architectures. To avoid the cost of training from scratch for each architecture, the super-network [24; 25] arises as a commonly adopted technique to obtain faster architecture performance estimation, where the architecture candidates are viewed as sub-networks of the super-network, and their weights are shared during the training process.

Unsupervised Graph Neural Architecture SearchWe consider the problem of unsupervised graph neural architecture where labels, which are adopted for the performance estimation and the search process in the supervised GNAS, are not accessible. The problem of unsupervised GNAS can be formulated as optimizing an architecture generator that is able to discover powerful architectures by exploiting inherent graph properties without labels, i.e., \(\mathcal{G}\mapsto f_{\alpha,w}\) instead of \((\mathcal{G},\mathcal{Y})\mapsto f_{\alpha,w}\) as done by supervised GNAS methods. Then, the discovered architectures \(f_{\alpha,w}(\cdot)\) can be utilized in downstream tasks, e.g., finetuning the weights \(w\) or the operation choices \(\alpha\), or extra shallow classifiers for further prediction.

## 3 Disentangled Self-supervised Graph Neural Architecture Search

In this section, we introduce Disentangled Self-supervised Graph Neural Architecture Search (**DSGAS**) to search architectures without labels, by proposing three key components: disentangled graph architecture super-network, self-supervised training with joint architecture-graph disentanglement, and contrastive search with architecture augmentations.

### Disentangled Graph Architecture Super-Network

To discover architectures that potentially have optimal performance, we resort to guiding the search towards architectures' capabilities of capturing the inherent graph factors, which are shown important in the graph formation [8; 9]. As architectures may expert in different graph factors, we propose a

Figure 1: The framework of Disentangled Self-supervised Graph Neural Architecture Search (**DSGAS**), including the following three key components: 1) Disentangled graph architecture super-network enables multiple architectures to be disentangled and optimized simultaneously in an end-to-end manner. 2) Self-supervised training with joint architecture-graph disentanglement estimates the performance of architectures under various latent factors by considering the relationship between architectures, graphs and factors. 3) Contrastive search with architecture augmentations encourages and discovers architectures with distinct capabilities of capturing factors. (Best viewed in color)

disentangled graph architecture super-network to incorporate \(K\) different architectures to be estimated and searched w.r.t factors simultaneously, where the hyperparameter \(K\) denotes the number of factors.

Disentangled Super-Network LayerFor each super-network layer, we adopt \(K\) mixed operations parameterized by different \(\alpha\) to learn \(K\)-chunk graph representations:

\[\mathbf{H}_{k}\leftarrow\overline{\text{GNN}}_{\alpha_{k}}\left(\mathbf{H}, \mathbf{A}\right),\] (3)

where \(\mathbf{A}\) is the adjacency matrix of the graph, \(\mathbf{H}\) denotes the input graph representations, and \(\overline{\text{GNN}}_{\alpha_{k}}(\cdot)\) denotes the mixed GNN operations parameterized by \(\alpha_{k}\). For the convenience of differentiable optimization, we adopt continuous parameterization and weight-sharing mechanism [25] to implement the mixed operations:

\[\overline{\text{GNN}}_{\alpha_{k}}(\mathbf{H},\mathbf{A})=\sum_{i=1}^{|\mathcal{ O}|}\alpha_{k,i}\text{GNN}_{i}(\mathbf{H},\mathbf{A}),\] (4)

where \(|\mathcal{O}|\) is the number of GNN operation choices, \(\alpha_{k,i}=\frac{\exp(\theta_{\alpha_{k,i}})}{\sum_{j}\exp(\theta_{\alpha_{ k,j}})}\) denotes the probability of the \(i\)-th operation for the \(k\)-th architecture \(\alpha_{k}\), and \(\theta\) is learnable parameters.

Overall Disentangled Super-NetworkThe overall super-network is constructed in the form of a directed acyclic graph (DAG) with an ordered sequence of disentangled super-network layers. More details about the DAG construction and the according GNN operations for each disentangled super-network layer are included in Appendix. The output of the last layer \(\mathbf{Z}=[\mathbf{H}_{1},\mathbf{H}_{2},\dots,\mathbf{H}_{K}]\) describes the various aspects of the graphs and serves as the final graph representations, which can be utilized or finetuned in downstream tasks. In this way, the architectures' operation choices \(\alpha=[\alpha_{1},\alpha_{2},\dots,\alpha_{K}]\) and weights \(w\) are incorporated in one super-network. For brevity, we use \(f_{\alpha_{k},w}(\cdot)\) to denote the \(k\)-th architecture induced from the super-network. Note that the design of \(K\) operation choices alleviates the entanglement of architectures by providing more flexible choices of paths [26] in the super-network. For instance, the'mean' operation captures structural properties while the'max' operation captures representative elements [22], and in this case, our design can capture both of them by choosing corresponding operations to learn respective representations instead of choosing only one of them which may conflict each other.

### Self-supervised Training with Joint Architecture-Graph Disentanglement

Inspired by graph self-supervised learning, we utilize graph pretext tasks to measure the architectures' capabilities of capturing latent factors. Predictive pretext tasks [27], for example, design a pseudo label generator \(s(\cdot)\), and optimize the prediction probability4\(p(s(\mathcal{G}_{i})|\mathcal{G}_{i})\). However, the tasks usually take holistic views of the graphs and neglect the entanglement of the latent factors, which may lead to suboptimal performance estimation. Therefore, to disentangle the factors, we transform the probability into the expectation of multiple subtasks under different latent factors by the Bayesian formula:

Footnote 4: We take graph classification as an example for simplicity, while the case of node classification can be easily extended.

\[p(s(\mathcal{G}_{i})|\mathcal{G}_{i})=\mathbb{E}_{p(k|\mathcal{G}_{i})}p(s( \mathcal{G}_{i})|\mathcal{G}_{i},k),\] (5)

where \(p(k|\mathcal{G}_{i})\) denotes the probability of latent factor \(k\) given the \(i\)-th graph instance \(\mathcal{G}_{i}\), and \(p(s(\mathcal{G}_{i})|\mathcal{G}_{i},k)\) denotes the pretext task under \(k\)-th latent factor. An intuitive explanation of Eq. (5) is that it first infers the latent factors and then conducts factor-specific self-supervised training to capture the latent factors, which we describe in detail as follows.

Architecture-aware Latent Factor InferenceDirectly modeling \(p(k|\mathcal{G}_{i})\) is difficult as we do not know prior what GNN encoders are suitable for inferring the latent factors. Intuitively, one solution is to get the architectures being searched involved in the inference stage. By the Bayesian formula, we factorize the probability w.r.t architecture choices:

\[p(k|\mathcal{G}_{i})=\mathbb{E}_{p(\alpha_{j}|\mathcal{G}_{i})}p(k|\mathcal{G} _{i},\alpha_{j}),\] (6)where \(p(\alpha_{j}|\mathcal{G}_{i})\) is a prior distribution, and we adopt a uniform distribution for simplicity. Then we can model the probability distributions of latent factors given the graph \(\mathcal{G}_{i}\) by utilizing the architectures being searched:

\[p(k|\mathcal{G}_{i},\alpha_{j})=\frac{\exp\phi(\mathbf{z}_{i,j}||\text{Enc}( \alpha_{j}),\mathbf{c}_{k})}{\sum_{m=1}^{K}\exp\phi(\mathbf{z}_{i,j}||\text{ Enc}(\alpha_{j}),\mathbf{c}_{m})},\] (7)

where \(\mathbf{c}_{k}\) is a learnable vector to represent the \(k\)-th latent factor, and \(\mathbf{z}_{i,k}=f_{\alpha_{k},w}(\mathcal{G}_{i})\) denotes the graph representations output by the \(k\)-th architecture for the graph \(\mathcal{G}_{i}\). \(\text{Enc}(\cdot)\) denotes architecture encoding techniques to obtain embeddings of operation choices \(\alpha\) so that the structural properties and correlations of the neural architectures can be considered [28; 29].

Factor-aware Graph Self-Supervised LearningUnder the \(k\)-th latent factor, we leverage the corresponding architectures \(f_{\alpha_{k},w}(\cdot)\) for conducting the factor-specific pretext tasks as \(p(s(\mathcal{G}_{i})|\mathcal{G}_{i},k)\) to estimate the capturing capabilities of architectures under various factors. The overall objective is to maximize Eq.(5), and the loss can be calculated by

\[\frac{1}{N}\sum_{i}-\log\mathbb{E}_{p(k|\mathcal{G}_{i})}\Big{(}p(s(\mathcal{ G}_{i})|\mathcal{G}_{i},k)\Big{)}\leq\frac{1}{N}\sum_{i}\mathbb{E}_{p(k| \mathcal{G}_{i})}\Big{(}-\log p(s(\mathcal{G}_{i})|\mathcal{G}_{i},k)\Big{)},\] (8)

where \(N\) is the number of samples and the upper bound is obtained by Jensen's Inequality. Then we can generalize our method to other graph self-supervised tasks with specially-designed task loss functions by defining \(-\log p(s(\mathcal{G}_{i})|\mathcal{G}_{i},k)\) as the task loss function \(l(\hat{f}_{\alpha_{k},w},\mathcal{G}_{i})\) for the graph \(\mathcal{G}_{i}\) under the \(k\)-th factor, and calculate the loss by

\[\mathcal{L}_{w}=\frac{1}{N}\sum_{i}\mathbb{E}_{p(k|\mathcal{G}_{i})}\Big{(}l( f_{\alpha_{k},w},\mathcal{G}_{i})\Big{)}.\] (9)

In this way, the disentangled architectures in Sec. 3.1 coupled with factors disentangled from graph data can be routed pairwisely, and trained with factor-specific self-supervision to obtain more accurate performance estimation under each factor. Similar to [25], the super-network weights are updated with \(w=w-\lambda_{w}\nabla_{w}\mathcal{L}_{w}\) to obtain the weights that can represent the architectures' capabilities.

### Contrastive Search with Architecture Augmentations

In this section, we focus on encouraging the disentanglement of architectures and searching architectures with distinct capabilities of capturing different factors. The main insight of our proposed search method is intuitively based on the following two observations shown in the literature: 1) As architectures similar in operation choices and topologies have similar capabilities of capturing semantics for downstream tasks [28; 30; 31], slight modifying the architecture will have a slight influence on its capability. 2) Since different GNN architectures expert in different downstream tasks [32], the architectures searched for different disentangled latent factors are expected to have dissimilar capabilities under different factors.

Contrastive SearchInspired by self-supervised contrastive learning [33; 34] that capture discriminative features by pulling similar instances together and pushing dissimilar instances away in the latent space, we propose an architecture-level instance discrimination task to encourage the architectures to capture various latent factors. The task is defined as

\[p(s(\alpha_{k})|\mathcal{G}_{i},\alpha_{k})=\frac{\exp\phi(\mathbf{z}_{i,k}, \mathbf{z}^{\prime}_{i,s(\alpha_{k})})}{\sum_{j=1}^{N}\exp\phi(\mathbf{z}_{i,j},\mathbf{z}^{\prime}_{i,s(\alpha_{j})})},\] (10)

\[\mathbf{z}_{i,k}=f_{\alpha_{k},w}(\mathcal{G}_{i}),\mathbf{z}^{\prime}_{i,k}=T _{f}(f_{\alpha_{k},w})(\mathcal{G}_{i}),\] (11)

where \(s(\alpha_{k})\) is assigned to \(k\) as surrogate labels for the architecture, \(\phi(\cdot)\) calculates the similarity of two embeddings and \(T_{f}(\cdot)\) denotes architecture augmentations that transform an architecture to another architecture with similar capabilities capturing factors, i.e., \(f_{\alpha_{k},w}\mapsto f^{\prime}_{\alpha_{k},w}\). Then the loss function can be calculated by

\[\mathcal{L}_{\alpha}=\sum_{i}-\log\mathbb{E}_{p(\alpha_{k}|\mathcal{G}_{i})}p( s(\alpha_{k})|\mathcal{G}_{i},\alpha_{k}).\] (12)

Similar to [25], the architecture parameters are updated with \(\alpha=\alpha-\lambda_{\alpha}\nabla_{\alpha}\mathcal{L}_{\alpha}\) to search architectures with better capabilities of capturing factors.

Architecture AugmentationsTo create various views of architectures, we design three basic architecture augmentations from the perspectives of architecture operation choices \(\alpha\), architecture weights \(w\) and the internal embeddings \(\mathbf{H}\):

* Operation Choice Perturbation. This augmentation randomly reshapes the distributions of the mixed operations by altering the temperature in the softmax function in Eq. (4) with \[\alpha_{k,i}=\frac{\exp(\theta_{\alpha_{k,i}}/\tau)}{\sum_{j}\exp(\theta_{ \alpha_{k,j}}/\tau)},\] (13) where the temperature \(\tau\) is sampled from a uniform distribution \(\mathcal{U}([1/r_{1},r_{1}])\), and \(r_{1}\geq 1\) is a hyper-parameter controlling the perturbation degree.
* Weight Perturbation. This augmentation randomly adds Gaussian noises \(\epsilon\sim\mathcal{N}(0,\sigma^{2})\) to \(r_{2}\)% of the architecture weights \(w\), where \(r_{2}\) controls the perturbation ratio, and \(\sigma^{2}\) is the standard deviation of the weights.
* Embedding Dropout. This augmentation randomly drops \(r_{3}\)% of the embeddings \(\mathbf{H}\) output from the mixed operations, where \(r_{3}\) controls the dropout ratio.

Note that these architecture augmentations can further be randomly composed to generate mixed augmentations.

## 4 Experiments

In this section, we conduct experiments on 8 real-world datasets with unsupervised settings to verify the design of our method. We also include detailed ablation studies to analyze the effectiveness of each component, and 3 real-world datasets in semi-supervised settings to show that our method can alleviate the label scarcity issues by pretraining the super-network.

BaselinesWe compare our method with 11 baselines from the following two different categories.

* **Manually designed GNNs.** We include five representative GNNs as our baselines, i.e., GCN [20], GAT [21], GIN [22], GraphSage [35], GraphConv [36] and a simple baseline MLP, which constitutes our search space. For graph-level classification tasks, we adopt global mean pooling for each layer and concatenation to obtain the graph representations for these baselines.
* **Graph neural architecture search.** We include representative GNAS baselines GraphNAS [3], PAS [4] and GASSO [37], where PAS is specially designed for graph-level classification tasks by searching the pooling operations, and GASSO is specially designed for node-level classification tasks by searching the graph structures simultaneously. We also include two classical NAS baselines, random search and DARTS [25]. As these baselines are not specially designed for graphs, we adopt our search space for these baselines.

DatasetsFor unsupervised settings, we conduct experiments on four graph-level classification datasets including PROTEINS [38], DD [39], MUTAG [40], IMDB-B [41] from TUDataset [42] and four node-level classification datasets Coauthor CS, Coauthor Physics from the Microsoft Academic Graph [43], Amazon Computers, Amazon Photos from the Amazon Co-purchase Graph [44]. For semi-supervised settings, we adopt three real-world datasets, OGBG-M

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline
**Datasets** & **MUTAG** & **IMDS-B** & **PROTEINS** & **DD** & **Computers** & **Pretors** & **CS** & **Physics** & **OGBG-Membr** & **OGBN-Arviv** & **Webat-Video** \\ \hline
**Setting** & Unsip & Unsip, & Unsip & Unsip, & Unsip, & Unsip, & Unsip & Unsip, & Semi & Semi & Semi \\
**Task** & Graph & Graph & Graph & Graph & Node & Node & Node & Node & Graph & Node & Node \\
**Evaluation** & MUTAG & ACC & ACC & ACC & ACC & ACC & ACC & ACC & ACC & ACC & ACC & ACC & ACC \\
**6 Graphs** & 188 & 1,600 & 1,113 & 1,178 & 1 & 1 & 1 & 4,127 & 1 & 1 \\
**7**\#\_**Noise** & 17 & 9 & 9 & 2,84 & 13,522 & 7,630 & 18,533 & 34,493 & 26 & 169,343 & 607,746 \\
**7**\#\_**Edge** & 56 & 211 & 183 & 1,714 & 50,474 & 245,821 & 182,121 & 530,417 & 28 & 2,449,41 & 31,152,156 \\
**7**\#\_**Features** & 7 & 1 & 3 & 99 & 176 & 745 & 6,005 & 8,415 & 300 & 128 & 512 \\
**8** Classes** & 2 & 2 & 2 & 2 & 10 & 8 & 15 & 5 & 2 & 40 & 13 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of dataset statistics. Unsup./Semi. denotes Unsupervised and Semi-supervised settings. Graph/Node denotes graph and node classification tasks. ACC/AUC denotes Accuracy and ROC-AUC evaluation metrics.

and Wechat-Video5. The datasets cover various graph-related fields including small molecules, bioinformatics, social networks, e-commerce networks, and academic coauthorship networks. The statistics are summarized in Table 1.

Footnote 5: https://algo.weixin.qq.com/

Super-networkWe briefly introduce the super-network construction as follows. The super-network consists of two parts, the operation pool and the directed acyclic graph (DAG). The operation pool includes several node aggregation operations (e.g., GCN, GIN, GAT, etc.), graph pooling operations (e.g., SortPool, AttentionPool, etc.), and layer merging operations (e.g., MaxMerge, ConcatMerge, etc.). The DAG determines how the operations are connected to calculate the graph representations for the subsequent classification tasks.

More details of the experiments are provided in the Appendix, including additional experiments and analyses, experimental setups, configurations, and implementation details.

### Main Results

Unsupervised SettingsFrom the experimental results summarized in Table 2, we have the following observations: 1) _GNNs perform differently on various datasets._ The best GNN baselines for these datasets are GraphSage, GAT, GraphConv, GraphConv, GAT, GCN, GraphConv successively, and the performance varies greatly across datasets and GNNs. It verifies that no GNN architecture is dominant for all datasets, which is consistent with the literature [32] and shows the demand for automated GNN architecture designing based on the data characteristics to obtain the optimal representations. 2) _Most GNAS baselines fail in the unsupervised setting._ Since the existing GNAS baselines highly rely on supervised signals to search the architectures, they inherently do not suit the unsupervised settings. As an ad hoc remedy for the existing GNAS baselines in unsupervised settings, we substitute the supervised metrics with the self-supervised ones during the searching process as simple extensions. However, these GNAS baselines, contrary to supervised settings, do not guarantee better performance than manually designed GNNs for all datasets. For example, all GNAS baselines even perform worse than manually designed GNNs on DD dataset and do not have significant improvements on most datasets. The reasons behind might be that simply using graph self-supervised metrics does not consider the entanglement of architectures and factors, leading to inaccurate estimation of the architectures' capabilities. 3) _Our method has significant improvements over the baselines on most datasets._ Compared with manually designed GNN baselines, **DSGAS** has a performance improvement of 3.1% on PROTEINS and over 1% on most datasets. We contribute this to its ability of automatically tailoring GNNs for various datasets, showing its effectiveness of automatic GNN designing. **DSGAS** also significantly surpasses GNAS baselines, showing its superiority in graph neural architecture search in unsupervised settings, which benefits from the design of discovering architectures that can capture various graph factors in a self-supervised fashion.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multicolumn{1}{c}{**Data**} & \multicolumn{3}{c}{**Graph Classification**} & \multicolumn{3}{c}{**Node Classification**} \\ \multicolumn{1}{c}{**Method**} & **PROTEINS** & **DD** & **MUTAG** & **IMDB-B** & **CS** & **Computers** & **Physics** & **Photo** \\ \hline GCN & 72.8\(\pm\)0.7 & 77.0\(\pm\)0.9 & 78.6\(\pm\)1.6 & 63.5\(\pm\)0.8 & 93.0\(\pm\)0.3 & 86.0\(\pm\)0.4 & 95.7\(\pm\)0.1 & 90.8\(\pm\)0.6 \\ GAT & 72.3\(\pm\)0.9 & 77.5\(\pm\)0.7 & 78.0\(\pm\)0.8 & 54.4\(\pm\)1.7 & 93.4\(\pm\)0.3 & 85.8\(\pm\)0.3 & 95.6\(\pm\)0.1 & 91.4\(\pm\)0.6 \\ GIN & 72.6\(\pm\)0.4 & 77.3\(\pm\)0.7 & 86.3\(\pm\)1.7 & 70.7\(\pm\)0.5 & 93.1\(\pm\)0.3 & 76.7\(\pm\)0.5 & 95.3\(\pm\)0.1 & 91.1\(\pm\)0.7 \\ GraphSage & 72.9\(\pm\)0.7 & 77.1\(\pm\)0.4 & 78.3\(\pm\)1.6 & 53.0\(\pm\)2.1 & 93.2\(\pm\)0.3 & 78.4\(\pm\)0.4 & 95.4\(\pm\)0.1 & 89.2\(\pm\)0.7 \\ GraphConv & 72.1\(\pm\)0.6 & 77.3\(\pm\)0.6 & 87.2\(\pm\)1.4 & 71.1\(\pm\)0.6 & 93.1\(\pm\)0.3 & 74.7\(\pm\)0.7 & 95.3\(\pm\)0.1 & 91.5\(\pm\)0.5 \\ MLP & 70.5\(\pm\)0.4 & 76.1\(\pm\)0.7 & 74.8\(\pm\)1.1 & 50.3\(\pm\)0.6 & 91.5\(\pm\)0.4 & 56.6\(\pm\)0.3 & 94.6\(\pm\)0.1 & 87.4\(\pm\)0.8 \\ \hline Random & 74.5\(\pm\)0.9 & 74.8\(\pm\)1.3 & 82.1\(\pm\)2.8 & 69.0\(\pm\)2.1 & 92.9\(\pm\)0.3 & 84.8\(\pm\)0.4 & 95.4\(\pm\)0.1 & 91.1\(\pm\)0.6 \\ DARTS & 73.6\(\pm\)0.9 & 75.7\(\pm\)0.9 & 86.5\(\pm\)2.3 & 70.4\(\pm\)0.6 & 92.8\(\pm\)0.3 & 79.7\(\pm\)0.5 & 95.2\(\pm\)0.1 & 91.5\(\pm\)0.6 \\ GraphNAS & 73.6\(\pm\)0.7 & 75.2\(\pm\)0.9 & 77.5\(\pm\)0.7 & 62.7\(\pm\)1.3 & 91.6\(\pm\)0.3 & 69.0\(\pm\)0.6 & 94.5\(\pm\)0.1 & 89.3\(\pm\)0.7 \\ PAS & 74.6\(\pm\)0.3 & 76.5\(\pm\)0.9 & 84.0\(\pm\)1.6 & 64.6\(\pm\)13.8 & - & - & - & - \\ GASSO & - & - & - & - & 93.1\(\pm\)0.3 & 84.9\(\pm\)0.4 & 95.7\(\pm\)0.1 & 92.0\(\pm\)0.3 \\ \hline
**DSGAS** & **76.0\(\pm\)0.2** & **78.4\(\pm\)0.7** & **88.7\(\pm\)0.7** & **72.0\(\pm\)0.5** & **93.5\(\pm\)0.2** & **86.6\(\pm\)0.4** & **95.7\(\pm\)0.1** & **93.3\(\pm\)0.3** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The results (accuracy%) of all the methods on the real-world datasets in unsupervised settings. Numbers after the \(\pm\) signs represent standard deviations. The best results are in bold and the second-best results are underlined. As the search space of GASSO and PAS do not suit the graph-level and node-level tasks respectively, we omit their results.

Semi-supervised SettingsFrom Figure 2, we have the following observations: Compared with the baselines, **DSGAS** significantly alleviates the performance drop when the number of available supervised labels is fewer, which verifies that our method fully exploits latent factors inside graph data and boost the supervised architecture search stage by warming up the weights and architecture parameters of the super-network. Its significant improvement over the ablated version DSGAS-P also verifies the effectiveness of pretraining the super-network by the proposed modules of self-supervised training with joint architecture-graph disentanglement and contrastive search with architecture augmentations. For example, our model with the pretraining stage has an absolute improvement of 5% on Wechat-Video dataset with 1% labels compared with the ablated version without the pretraining stage, which shows the effects of pretraining the super-networks on alleviating the label scarcity issues. In comparison, the performance of other baselines decays significantly more than our method, showing that current GNAS can not tackle scenarios with scarce labels. For example, on OGBG-Mohiv, PAS is the best baseline while the worst with 10% and 1% labels respectively, which may due to the inaccurate performance estimation with scarce labels. The phenomenon further strengthens the necessity of designing effective unsupervised GNAS methods.

### Additional Experiments

Ablation StudiesWe evaluate the effectiveness of each module of our framework by comparing the following ablated versions of our method: DSGAS-CONT removes our proposed contrastive search module and search architectures with the vanilla self-supervised loss. DSGAS-FA further replaces our proposed factor-aware training module with the vanilla self-supervised training. DSGAS-DISEN further replaces our proposed disentangled super-network with the vanilla super-network.

Figure 3: (a) Comparisons of different ablated variants of **DSGAS** on real-world datasets under unsupervised settings. The horizontal dashed line refers to the results of the best-performed GNAS baseline. (b) Comparisons of different architecture augmentations of **DSGAS** on real-world datasets under unsupervised settings, where ‘Alpha’, ‘Weight’ and ‘Embed’ denote the augmentations from perspective of operation choices, weight and embeddings. ‘Compose’ denotes uniformly choosing one of the three augmentations. The horizontal dashed line refers to the results of the best-performed GNAS baseline. (Best viewed in color)

Figure 2: The performance of GNAS methods on real-world datasets under semi-supervised settings, where DSGAS-P denotes DSGAS without pretraining. The results are averaged by five random runs. (Best viewed in color)

We compare the performance of the ablated versions and the best GWAS baselines on the real-world datasets under unsupervised settings. From Figure 2(a), we have the following observations. First, our proposed **DSGAS** outperforms all the variants on all datasets, demonstrating the effectiveness of each component of our proposed framework in searching graph architectures under unsupervised settings. Second, DSGAS-CONT drops drastically in performance on all datasets compared to the full version, showing the superiority of our proposed disentangled contrastive architecture search module in searching architectures. Third, the performance also decays for DSGAS-FA and DSGAS-DISEN on most datasets, showing the necessity of capturing various latent factors with different architectures.

Architecture VisualizationsWe visualize the architectures searched on different unsupervised datasets in Figure 4. It shows that the searched architectures of different factors adopt quite different GNN operations while sometimes sharing the same operations, which leads to an overall architecture with complex internal connections between operations. This phenomenon implies that **DSGAS** can optimize the architecture operation choices as well as the operation connections for different factors to have a competitive performance on various graph datasets, which also verifies the superiority of **DSGAS** in automated architecture search to save human endeavors for architecture designs.

Effects of Architecture AugmentationsIn Figure 2(b), we show the results of different architecture augmentation methods on different datasets compared with the best GWAS baseline. We find that though the best augmentations differ among datasets, they have similar performance improvements in most cases, which verifies the design of contrastive search with architecture augmentations.

## 5 Related Works

Graph Neural Architecture SearchInstead of manually designing more sophisticated models for various scenarios, neural architecture search, aiming to automatically discover the optimal architectures for given tasks, emerges as a hot topic recently in computer vision [46; 23], natural language processing [47], graph representation learning [1; 48; 49], etc. In the field of graph representation learning with various applications [50; 51; 52; 53; 54], graph neural architecture search (GNAS) methods, as the most related to our works, can be roughly classified into reinforcement-learning-based methods [3; 55], evolutionary-based methods [56; 2; 57; 58], and differentiable methods [59; 60; 61; 62; 37; 63; 4; 64; 65]. However, supervised labels are indispensable for the existing GWAS methods to conduct neural architecture search, which limits their applications in widely-existed scenarios where labels are scarce or not available.

Unsupervised Neural Architecture SearchIn unsupervised settings, some neural architecture search methods replace supervised labels with self-supervised loss during searching [66; 67; 68; 69; 70; 71]. Another classic of related methods design special metrics, whose calculation does not depend on labels, as proxies for model performance estimation [72; 73]. For example, UnNAS [74] adopts pretext tasks like image rotation, coloring images, solving puzzles, etc. However, these methods are specially designed for computer vision, and can not be directly adopted to graph data. Some GNAS

Figure 4: Visualizations of the search architectures on different datasets. Nodes denote GNN operations except that ‘INPUT’ denotes the input graphs with structures and features. Directed edges denote calculation flows, where different colors denote the architecture operation choices under different factors. (Best viewed in color)

works exploit self-supervised loss as auxiliaries to augment the supervised search process [63], but the supervised labels are still mandatory for its search. To the best of our knowledge, this is the first work on unsupervised graph neural architecture search.

Graph Self-supervised LearningGraph self-supervised learning [75; 76] is devoted to obtaining graph representations by extracting informative knowledge with well-designed pretext tasks without labels, which can be roughly classified into contrastive [77; 78; 79; 80; 81; 82; 83; 84; 85; 86] and generative [87; 88; 76; 89], and predictive methods [90; 91; 92]. The existing graph self-supervised learning methods usually focus on designing better pretext tasks with a fixed encoder such as GCN [20], GAT [21] and GIN [22]. Another class of related methods attempt to automate the choices of pretext tasks [93; 94; 95; 96; 97]. We mainly consider graph neural architecture in unsupervised settings, while other pretext tasks are orthogonal to our framework and can be incorporated.

Disentangled Representation LearningThe primary objective of disentangled representation learning is to delineate and interpret the various latent factors which influence the data we encounter in an observable context, rendering each of these factors as unique vector representations [98; 99]. It has emerged to be a useful tool in various domains, including those of computer vision [100; 101; 102; 103; 104; 105], and recommendation systems [106; 107; 108; 109; 110; 111; 112; 113], graph representation learning [9; 114; 10; 14; 115; 116; 117]. As the most related, GRACES [63] characterize the graph latent factors inside data by designing a self-supervised disentangled graph encoder, and conduct graph neural architecture search for each graph to handle graph distribution shifts, while the training and searching process still follows the supervised paradigm. In contrast, we focus on automating the GNN designs with disentangled self-supervision in this paper.

## 6 Conclusions

In this paper, we propose a novel Disentangled Self-Supervised Graph Neural Architecture Search (**DSGAS**) framework to automate the GNN designs with disentangled self-supervision, which includes disentangled graph architecture super-network, self-supervised training with joint architecture-graph disentanglement and contrastive search with architecture augmentations. Extensive experiments demonstrate that our proposed method can discover architectures with capabilities of capturing various graph latent factors and significantly outperform the state-of-the-art GNAS baselines. Detailed ablation studies and analyses show the effectiveness of our method design. One limitation is that in this paper we mainly focus on homogeneous graphs, and we leave extending our method to heterogeneous graphs for further explorations.

## Acknowledgements

This work was supported by the National Key Research and Development Program of China No. 2020AAA0106300, National Natural Science Foundation of China (No. 62222209, 62250008, 62102222, 62206149), Beijing National Research Center for Information Science and Technology under Grant No. BNR2023RC01003, BNR2023TD03006, China National Postdoctoral Program for Innovative Talents No. BX20220185, China Postdoctoral Science Foundation No. 2022M711813, and Beijing Key Lab of Networked Multimedia. All opinions, findings, conclusions, and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

## References

* [1] Ziwei Zhang, Xin Wang, and Wenwu Zhu. Automated machine learning on graphs: A survey. _arXiv preprint arXiv:2103.00742_, 2021.
* [2] Yaoman Li and Irwin King. Autograph: Automated graph neural network. In _International Conference on Neural Information Processing_, pages 189-201, 2020.
* [3] Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search. In _IJCAI_, volume 20, pages 1403-1409, 2020.
* [4] Lanning Wei, Huan Zhao, Quanming Yao, and Zhiqiang He. Pooling architecture search for graph classification. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 2091-2100, 2021.

* [5] Linda Martin, Melissa Hutchens, Conrad Hawkins, and Alaina Radnov. How much do clinical trials cost. _Nat Rev Drug Discov_, 16(6):381-382, 2017.
* [6] Debleena Paul, Gaurav Sanap, Snehal Shenoy, Dnyaneshwar Kalyane, Kiran Kalia, and Rakesh K Tekade. Artificial intelligence in drug discovery and development. _Drug discovery today_, 26(1):80, 2021.
* [7] Duxin Sun, Wei Gao, Hongxiang Hu, and Simon Zhou. Why 90% of clinical drug development fails and how to improve it? _Acta Pharmaceutica Sinica B_, 2022.
* [8] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In _The world wide web conference_, pages 417-426, 2019.
* [9] Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenwu Zhu. Disentangled graph convolutional networks. In _International conference on machine learning_, pages 4212-4221. PMLR, 2019.
* [10] Yiding Yang, Zunlei Feng, Mingli Song, and Xinchao Wang. Factorizable graph convolutional networks. _Advances in Neural Information Processing Systems_, 33:20286-20296, 2020.
* [11] Ansong Li, Zhiyong Cheng, Fan Liu, Zan Gao, Weili Guan, and Yuxin Peng. Disentangled graph neural networks for session-based recommendation. _arXiv preprint arXiv:2201.03482_, 2022.
* [12] Thilini Cooray and Ngai-Man Cheung. Graph-wise common latent factor extraction for unsupervised graph representation learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 6420-6428, 2022.
* [13] Teng Xiao, Zhengyu Chen, Zhimeng Guo, Zeyang Zhuang, and Suhang Wang. Decoupled self-supervised learning for non-homophilious graphs. _arXiv preprint arXiv:2206.03601_, 2022.
* [14] Haoyang Li, Xin Wang, Ziwei Zhang, Zehuan Yuan, Hang Li, and Wenwu Zhu. Disentangled contrastive learning on graphs. _Advances in Neural Information Processing Systems_, 34:21872-21884, 2021.
* [15] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchical graph representation learning with differentiable pooling. _Advances in neural information processing systems_, 31, 2018.
* [16] Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. _Advances in neural information processing systems_, 32, 2019.
* [17] Xiangxiang Chu, Tianbao Zhou, Bo Zhang, and Jixiang Li. Fair darts: Eliminating unfair advantages in differentiable architecture search. In _European conference on computer vision_, pages 465-480. Springer, 2020.
* [18] Xiangxiang Chu, Bo Zhang, and Ruijun Xu. Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12239-12248, 2021.
* [19] Stephen Cha, Taehyeon Kim, Hayeon Lee, and Se-Young Yun. Supernet in neural architecture search: A taxonomic survey. _arXiv preprint arXiv:2204.03916_, 2022.
* [20] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [21] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations_, 2018.
* [22] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [23] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. _The Journal of Machine Learning Research_, 20(1):1997-2017, 2019.
* [24] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In _International conference on machine learning_, pages 4095-4104. PMLR, 2018.
* [25] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. _arXiv preprint arXiv:1806.09055_, 2018.

* [26] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In _European conference on computer vision_, pages 544-560. Springer, 2020.
* [27] Lirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z Li. Self-supervised learning on graphs: Contrastive, generative, or predictive. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [28] Shen Yan, Yu Zheng, Wei Ao, Xiao Zeng, and Mi Zhang. Does unsupervised architecture representation learning help neural architecture search? _Advances in Neural Information Processing Systems_, 33:12486-12498, 2020.
* [29] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. _Advances in neural information processing systems_, 31, 2018.
* [30] Chris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture search. In _International Conference on Machine Learning_, pages 7105-7114. PMLR, 2019.
* [31] Arber Zela, Julien Niklas Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and Frank Hutter. Surrogate nas benchmarks: Going beyond the limited search spaces of tabular nas benchmarks. In _Tenth International Conference on Learning Representations_, pages 1-36. OpenReview. net, 2022.
* [32] Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. _Advances in Neural Information Processing Systems_, 33:17009-17021, 2020.
* [33] Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervised learning: Generative or contrastive. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [34] Yifei Wang, Qi Zhang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap. _arXiv preprint arXiv:2203.13457_, 2022.
* [35] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [36] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, pages 4602-4609, 2019.
* [37] Yijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Graph differentiable architecture search with structure learning. _Advances in Neural Information Processing Systems_, 34, 2021.
* [38] Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without alignments. _Journal of molecular biology_, 330(4):771-783, 2003.
* [39] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* [40] Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of medicinal chemistry_, 34(2):786-797, 1991.
* [41] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1365-1374, 2015.
* [42] Christopher Morris, Nils M Krieg, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv preprint arXiv:2007.08663_, 2020.
* [43] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In _Proceedings of the 24th international conference on world wide web_, pages 243-246, 2015.
* [44] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In _Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval_, pages 43-52, 2015.

* [45] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [46] Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. A comprehensive survey of neural architecture search: Challenges and solutions. _ACM Computing Surveys (CSUR)_, 54(4):1-34, 2021.
* [47] Krishna Teja Chitty-Venkata, Murali Emani, Venkatram Vishwanath, and Arun K Somani. Neural architecture search for transformers: A survey. _IEEE Access_, 10:108374-108412, 2022.
* [48] Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang Zhang, Yijian Qin, Jiyan Jiang, Xin Wang, and Wenwu Zhu. Autogl: A library for automated graph learning. In _ICLR 2021 Workshop GTRL_, 2021.
* [49] Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Nas-bench-graph: Benchmarking graph neural architecture search. _arXiv preprint arXiv:2206.09166_, 2022.
* [50] Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. _IEEE Transactions on Knowledge and Data Engineering_, 34(1):249-270, 2020.
* [51] Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Large graph models: A perspective. _arXiv preprint arXiv:2308.14522_, 2023.
* [52] Zeyang Zhang, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning to solve travelling salesman problem with hardness-adaptive curriculum. In _Thirty-Fifth AAAI Conference on Artificial Intelligence_, 2022.
* [53] Ziwei Zhang, Xin Wang, Zeyang Zhang, Peng Cui, and Wenwu Zhu. Revisiting transformation invariant geometric deep learning: Are initial representations all you need? _arXiv preprint arXiv:2112.12345_, 2021.
* [54] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, and Wenwu Zhu. Llm4dyg: Can large language models solve problems on dynamic graphs? _arXiv preprint_, 2023.
* [55] Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-gnn: Neural architecture search of graph neural networks. _arXiv:1909.03184_, 2019.
* [56] Matheus Nunes and Gisele L Pappa. Neural architecture search in graph neural networks. In _Brazilian Conference on Intelligent Systems_, pages 302-317, 2020.
* [57] Min Shi, David A Wilson, Xingquan Zhu, Yu Huang, Yuan Zhuang, Jianxun Liu, and Yufei Tang. Genetic-gnn: Evolutionary architecture search for graph neural networks. _Knowledge-Based Systems_, page 108752, 2022.
* [58] Wentao Zhang, Zheyu Lin, Yu Shen, Yang Li, Zhi Yang, and Bin Cui. Deep and flexible graph neural architecture search. In _International Conference on Machine Learning_, pages 26362-26374. PMLR, 2022.
* [59] Yuhui Ding, Quanming Yao, Huan Zhao, and Tong Zhang. Diffmg: Differentiable meta graph search for heterogeneous graph neural networks. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 279-288, 2021.
* [60] ZHAO Huan, YAO Quanming, and TU Weiwei. Search to aggregate neighborhood for graph neural network. In _2021 IEEE 37th International Conference on Data Engineering_, pages 552-563, 2021.
* [61] Yanxi Li, Zean Wen, Yunhe Wang, and Chang Xu. One-shot graph neural architecture search with dynamic search space. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 8510-8517, 2021.
* [62] Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li Su, and Qingming Huang. Rethinking graph neural architecture search from message-passing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6657-6666, 2021.
* [63] Yijian Qin, Xin Wang, Ziwei Zhang, Pengtao Xie, and Wenwu Zhu. Graph neural architecture search under distribution shifts. In _International Conference on Machine Learning_, pages 18083-18095. PMLR, 2022.
* [64] Zeyang Zhang, Ziwei Zhang, Xin Wang, Yijian Qin, Zhou Qin, and Wenwu Zhu. Dynamic heterogeneous graph attention neural architecture search. In _Thirty-Seventh AAAI Conference on Artificial Intelligence_, 2023.

* Qin et al. [2023] Yijian Qin, Xin Wang, Ziwei Zhang, Hong Chen, and Wenwu Zhu. Multi-task graph neural architecture search with task-aware collaboration and curriculum. _Advances in neural information processing systems_, 2023.
* Kaplan and Giryes [2020] Sapir Kaplan and Raja Giryes. Self-supervised neural architecture search. _arXiv preprint arXiv:2007.01500_, 2020.
* Timofeev et al. [2021] Aleksandr Timofeev, Grigorios G Chrysos, and Volkan Cevher. Self-supervised neural architecture search for imbalanced datasets. _arXiv preprint arXiv:2109.08580_, 2021.
* Nguyen and Chang [2021] Nam Nguyen and J Morris Chang. Csnas: Contrastive self-supervised learning neural architecture search via sequential model-based optimization. _IEEE Transactions on Artificial Intelligence_, 3(4):609-624, 2021.
* Li et al. [2021] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun Chang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 12281-12291, 2021.
* Li et al. [2022] Zhuowei Li, Yibo Gao, Zhenzhou Zha, Zhiqiang Hu, Qing Xia, Shaoting Zhang, and Dimitris N Metaxas. Towards self-supervised and weight-preserving neural architecture search. _arXiv preprint arXiv:2206.04125_, 2022.
* Hou et al. [2021] Zhenyu Hou, Yukuo Cen, Yuxiao Dong, Jie Zhang, and Jie Tang. Automated unsupervised graph representation learning. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* Zhang et al. [2021] Xuanyang Zhang, Pengfei Hou, Xiangyu Zhang, and Jian Sun. Neural architecture search with random labels. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10907-10916, 2021.
* Mellor et al. [2021] Joe Mellor, Jack Turner, Amos Storkey, and Elliot J Crowley. Neural architecture search without training. In _International Conference on Machine Learning_, pages 7588-7598. PMLR, 2021.
* Liu et al. [2020] Chenxi Liu, Piotr Dollar, Kaiming He, Ross Girshick, Alan Yuille, and Saining Xie. Are labels necessary for neural architecture search? In _European Conference on Computer Vision_, pages 798-813. Springer, 2020.
* Liu et al. [2022] Yixin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and Philip Yu. Graph self-supervised learning: A survey. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* Xie et al. [2022] Yaochen Xie, Zhao Xu, and Shuiwang Ji. Self-supervised representation learning via latent graph prediction. _arXiv preprint arXiv:2202.08333_, 2022.
* Velickovic et al. [2019] Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Hjelm. Deep graph infomax. _ICLR (Poster)_, 2(3):4, 2019.
* Sun et al. [2019] Fan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. _arXiv preprint arXiv:1908.01000_, 2019.
* Peng et al. [2020] Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou Huang. Graph representation learning via graphical mutual information maximization. In _Proceedings of The Web Conference 2020_, pages 259-270, 2020.
* Xu et al. [2021] Minghao Xu, Hang Wang, Bingbing Ni, Hongyu Guo, and Jian Tang. Self-supervised graph-level representation learning with local and global structure. In _International Conference on Machine Learning_, pages 11548-11558. PMLR, 2021.
* Qiu et al. [2020] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding for graph neural network pre-training. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1150-1160, 2020.
* Han et al. [2022] Yuehui Han, Le Hui, Haobo Jiang, Jianjun Qian, and Jin Xie. Generative subgraph contrast for self-supervised graph representation learning. _arXiv preprint arXiv:2207.11996_, 2022.
* Li et al. [2022] Sihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In _International Conference on Machine Learning_, pages 13052-13065. PMLR, 2022.

* [84] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive representation learning. _arXiv preprint arXiv:2006.04131_, 2020.
* [85] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _Advances in Neural Information Processing Systems_, 33:5812-5823, 2020.
* [86] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on graphs. In _International Conference on Machine Learning_, pages 4116-4126. PMLR, 2020.
* [87] Zhenyu Hou, Xiao Liu, Yuxiao Dong, Chunjie Wang, Jie Tang, et al. Graphmae: Self-supervised masked graph autoencoders. _arXiv preprint arXiv:2205.10803_, 2022.
* [88] Jintang Li, Ruofan Wu, Wangbin Sun, Liang Chen, Sheng Tian, Liang Zhu, Changhua Meng, Zibin Zheng, and Weiqiang Wang. Maskgae: Masked graph modeling meets graph autoencoders. _arXiv preprint arXiv:2205.10053_, 2022.
* [89] Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang. Graphmae2: A decoding-enhanced masked self-supervised graph learner. In _Proceedings of the ACM Web Conference 2023_, pages 737-746, 2023.
* [90] Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. When does self-supervision help graph convolutional networks? In _international conference on machine learning_, pages 10871-10880. PMLR, 2020.
* [91] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. _Advances in Neural Information Processing Systems_, 33:12559-12571, 2020.
* [92] Wei Jin, Tyler Derr, Haochen Liu, Yiqi Wang, Suhang Wang, Zitao Liu, and Jiliang Tang. Self-supervised learning on graphs: Deep insights and new direction. _arXiv preprint arXiv:2006.10141_, 2020.
* [93] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. In _International Conference on Machine Learning_, pages 12121-12132. PMLR, 2021.
* [94] Yihang Yin, Qingzhong Wang, Siyu Huang, Haoyi Xiong, and Xiang Zhang. Autogel: Automated graph contrastive learning via learnable view generators. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 8892-8900, 2022.
* [95] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning with adaptive augmentation. In _Proceedings of the Web Conference 2021_, pages 2069-2080, 2021.
* [96] Wei Jin, Xiaorui Liu, Xiangyu Zhao, Yao Ma, Neil Shah, and Jiliang Tang. Automated self-supervised learning for graphs. _arXiv preprint arXiv:2106.05470_, 2021.
* [97] Mingxuan Ju, Tong Zhao, Qianlong Wen, Wenhao Yu, Neil Shah, Yanfang Ye, and Chuxu Zhang. Multi-task self-supervised graph neural networks enable stronger task generalization. _arXiv preprint arXiv:2210.02016_, 2022.
* [98] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. _IEEE transactions on pattern analysis and machine intelligence_, 35(8):1798-1828, 2013.
* [99] Xin Wang, Hong Chen, Si'ao Tang, Zihao Wu, and Wenwu Zhu. Disentangled representation learning, 2023.
* [100] Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei, and Juan Carlos Niebles. Learning to decompose and disentangle representations for video prediction. _Advances in neural information processing systems_, 31, 2018.
* [101] Liqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario Fritz. Disentangled person image generation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 99-108, 2018.
* [102] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. _Advances in neural information processing systems_, 29, 2016.
* [103] Emily L Denton et al. Unsupervised learning of disentangled representations from video. _Advances in neural information processing systems_, 30, 2017.

* [104] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled representation learning gan for pose-invariant face recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1415-1424, 2017.
* [105] Xin Wang, Zihao Wu, Hong Chen, Xiaohan Lan, and Wenwu Zhu. Mixup-augmented temporally debiased video grounding with content-location disentanglement. 2023.
* [106] Xin Wang, Hong Chen, Yuwei Zhou, Jianxin Ma, and Wenwu Zhu. Disentangled representation learning for recommendation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [107] Hong Chen, Yudong Chen, Xin Wang, Ruobing Xie, Rui Wang, Feng Xia, and Wenwu Zhu. Curriculum disentangled recommendation with noisy multi-feedback. _Advances in Neural Information Processing Systems_, 34:26924-26936, 2021.
* [108] Xin Wang, Hong Chen, and Wenwu Zhu. Multimodal disentangled representation for recommendation. In _2021 IEEE International Conference on Multimedia and Expo (ICME)_, pages 1-6, 2021.
* [109] Jianxin Ma, Chang Zhou, Hongxia Yang, Peng Cui, Xin Wang, and Wenwu Zhu. Disentangled self-supervision in sequential recommenders. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 483-491, 2020.
* [110] Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, and Wenwu Zhu. Learning disentangled representations for recommendation. _Advances in neural information processing systems_, 32, 2019.
* [111] Haoyang Li, Xin Wang, Ziwei Zhang, Jianxin Ma, Peng Cui, and Wenwu Zhu. Intention-aware sequential recommendation with structured intent transition. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* [112] Xin Wang, Zirui Pan, Yuwei Zhou, Hong Chen, Chendi Ge, and Wenwu Zhu. Curriculum co-disentangled representation learning across multiple environments for social recommendation. 2023.
* [113] Yipeng Zhang, Xin Wang, Hong Chen, and Wenwu Zhu. Adaptive disentangled transformer for sequential recommendation. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 3434-3445, 2023.
* [114] Yanbei Liu, Xiao Wang, Shu Wu, and Zhitao Xiao. Independence promoted graph disentangled networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 4916-4923, 2020.
* [115] Zeyang Zhang, Xingwang Li, Fei Teng, Ning Lin, Xueling Zhu, Xin Wang, and Wenwu Zhu. Out-of-distribution generalized dynamic graph neural network for human albumin prediction. In _IEEE International Conference on Medical Artificial Intelligence_, 2023.
* [116] Zeyang Zhang, Xin Wang, Ziwei Zhang, Zhou Qin, Weigao Wen, Hui Xue, Haoyang Li, and Wenwu Zhu. Spectral invariant learning for dynamic graphs under distribution shifts. In _Advances in Neural Information Processing Systems_, 2023.
* [117] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and Wenwu Zhu. Dynamic graph neural networks under spatio-temporal distribution shift. In _Advances in Neural Information Processing Systems_, 2022.