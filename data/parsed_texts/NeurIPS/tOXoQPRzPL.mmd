**An Image is Worth 32 Tokens**

**for Reconstruction and Generation**

**Qihang Yu1*, Mark Weber1,2*, Xueqing Deng1, Xiaohui Shen1, Daniel Cremers2, Liang-Chieh Chen1**

1 ByteDance 2 Technical University Munich * equal contribution

https://yucornetto.github.io/projects/titok.html

_32 tokens can work well for..._

Recent advancements in generative models have highlighted the crucial role of image tokenization in the efficient synthesis of high-resolution images. Tokenization, which transforms images into latent representations, reduces computational demands compared to directly processing pixels and enhances the effectiveness and efficiency of the generation process. Prior methods, such as VQGAN, typically utilize 2D latent grids with fixed downsampling factors. However, these 2D tokenizations face challenges in managing the inherent redundancies present in images, where adjacent regions frequently display similarities. To overcome this issue, we introduce **T**ransformer-based 1-Dimensional **T**okenizer (TiTok), an innovative approach that tokenizes images into 1D latent sequences. TiTok provides a more compact latent representation, yielding substantially more efficient and effective representations than conventional techniques. For example, a \(256\times 256\times 3\) image can be reduced to just **32** discrete tokens, a significant reduction from the 256 or 1024 tokens obtained by prior methods. Despite its compact nature, TiTok achieves competitive performance to state-of-the-art approaches. Specifically, using the same generator framework, TiTok attains **1.97** gFID, outperforming MaskGIT baseline significantly by 4.21 at ImageNet \(256\times 256\) benchmark. The advantages of TiTok become even more significant when it comes to higher resolution. At ImageNet \(512\times 512\) benchmark, TiTok not only outperforms state-of-the-art diffusion model DiT-XL/2 (gFID 2.74 _vs._ 3.04), but also reduces the image tokens by **64\(\times\)**, leading to **410\(\times\) faster** generation process. Our best-performing variant can significantly surpass DiT-XL/2 (gFID **2.13 _vs._ 3.04) while still generating high-quality samples **74\(\times\) faster**.

Figure 1: We propose **TiTok**, a compact **1D** tokenizer leveraging region redundancy to represent an image with only **32** tokens for image reconstruction and generation.

Introduction

In recent years, image generation has experienced remarkable progress, driven by the significant advancements in both transformers [19; 65; 70; 10; 71; 72] and diffusion models [16; 58; 29; 52; 21]. Mirroring the trends in generative language models [51; 62], the architecture of many contemporary image generation models incorporate a standard image tokenizer and de-tokenizer. This array of models utilizes tokenized image representations--ranging from continuous [35] to discrete vectors [57; 64; 19]--to perform a critical function: translating raw pixels into a latent space. The latent space (_e.g._, \(32\times 32\)) is significantly more compact than the original image space (\(256\times 256\times 3\)). It offers a compressed yet expressive representation, and thus not only facilitates efficient training and inference of generative models but also paves the way to scale up the model size.

Although image tokenizers achieve great success in image generation workflows, they encounter a fundamental limitation tied to their intrinsic design. These tokenizers are based on an assumption that the latent space should retain a 2D structure, to maintain a direct mapping for locations between the latent tokens and image patches. For example, the top-left latent token directly corresponds to the top-left image patch. This restricts the tokenizer's ability to effectively leverage the redundancy inherent in images to cultivate a more compressed latent space.

Taking one step back, we raise the question _"is 2D structure necessary for image tokenization?"_ To answer the question, we draw inspiration from several image understanding tasks where model predictions are based solely on high-level information extracted from input images --such as in image classification [17], object detection [8; 81], segmentation [67; 34; 74; 75], and multi-modal large language models [1; 41; 11]. These tasks do not need de-tokenizers, since the outputs typically manifest in specific structures other than images. In other words, they often format a higher-level 1D sequence as output that can still capture all task-relevant information. Prior arts, such as object queries [8; 67] or the perceiver resampler [1], encode images into a 1D sequence of a predetermined number of tokens (_e.g._, 64). These tokens facilitate the generation of outputs like bounding boxes or captions. The success of these methods motivates us to investigate a more compact 1D sequence as image latent representation in the context of image reconstruction and generation. It is noteworthy that the synthesis of both high-level and low-level information is crucial for the generation of high-quality images, providing a challenge for extremely compact latent representations.

In this work, we introduce a transformer-based framework [65; 17] designed to tokenize an image to a 1D discrete sequence, which can later be decoded back to the image space via a de-tokenizer. Specifically, we present **T**ransformer-based 1-**D**imensional **T**okenizer (TiTok), consisting of a Vision Transformer (ViT) encoder, a ViT decoder, and a vector quantizer following the typical Vector-Quantized (VQ) model designs [19]. In the tokenization phase, the image is split and flattened into a series of patches, followed by concatenation with a 1D sequence of latent tokens. After the feature encoding process of ViT encoder, these latent tokens build the latent representation of the image. Subsequent to the vector quantization step [64; 19], the ViT decoder reconstructs the input images from the masked token sequence [15; 24].

Building upon TiTok, we conduct extensive experiments to probe the dynamics of 1D image tokenization. Our investigation studies the interplay between latent space size, model size, reconstruction fidelity, and generative quality. From this exploration, several compelling insights emerge:

1. Increasing the number of latent tokens representing an image consistently improves the reconstruction performance, yet the benefit becomes marginal after 128 tokens. Intriguingly, 32 tokens are sufficient for a reasonable image reconstruction.
2. Scaling up the tokenizer model size significantly improves performance of both reconstruction and generation, especially when number of tokens is limited (_e.g._, 32 or 64), showcasing a promising pathway towards a compact image representation at latent space.
3. 1D tokenization breaks the grid constraints in prior 2D image tokenizers, which not only enables each latent token to reconstruct regions beyond a fixed image grid and leads to a more flexible tokenizer design, but also learns more high-level and semantic-rich image information, especially at a compact latent space.
4. 1D tokenization exhibits superior performance in generative training, with not only a significant speed-up for both training and inference but also a competitive FID score compared to a typical 2D tokenizer, while using much fewer tokens.

In light of these findings, we introduce the TiTok family, encompassing models of varying model sizes and latent sizes, capable of achieving highly compact tokenization with as few as **32 tokens**. We further confirm the model's efficacy in image generation through the MaskGIT [9] framework. TiTok is demonstrated to facilitate state-of-the-art performance in image generation, while requiring latent spaces that are \(8\times\) to \(64\times\) smaller, resulting in significant accelerations during both the training and inference phases. It also generates images with similar or higher quality but up to \(410\times\) faster than state-of-the-art diffusion models such as DiT [18] (Fig. 2).

## 2 Related Work

**Image Tokenization.** Images have been compressed since the early days of deep learning with autoencoders [27; 66]. The general design of using an encoder that compresses high-dimensional images into a low-dimensional latent representation and then using a decoder to reverse the process, has proven to be successful over the years. Variational Autoencoders (VAEs) [35] extend the paradigm by learning to map the input to a distribution. Instead of modeling a continuous distribution, VQVAEs [50; 56] learn a discrete representation forming a categorical distribution. VQGAN [19] further improves the training process by using adversarial training [23]. The transformer design of the autoencoder is further explored in ViT-VQGAN [69] and Efficient-VQGAN [7]. Orthogonal to this, RQ-VAE [37] and MoVQ [80] study the effect of using multiple vector quantization steps per latent embedding, while MAGVIT-v2 [72] and FSQ [48] propose a lookup-free quantization. However, _all_ aforementioned works share the same workflow of an image always being patchwise encoded into a _2D_ grid latent representation. In this work, we explore an innovative _1D_ sequence latent representation for image reconstruction and generation.

**Tokenization for Image Understanding.** For image understanding tasks (_e.g._, image classification [17], object detection [8; 81; 78], segmentation [67; 74; 76], and Multi-modal Large Language Models (MLLMs) [1; 41; 77]), it is common to use a general feature encoder instead of an autoencoder to tokenize the image. Specifically, many MLLMs [41; 43; 61; 32; 22; 11] uses a CLIP [54] encoder to tokenize the image into highly semantic tokens, which proves effective for image captioning [13] and VQA [2]. Some MLLMs also explore discrete tokens [32; 22] or "de-tokenize" the CLIP embeddings back to images through diffusion models [61; 32; 22]. However, due to the nature of CLIP models that focus on high-level information, these methods can only reconstruct an image with high-level semantic similarities (_i.e._, the layouts and details are not well-reconstructed due to CLIP features). Therefore, our method is significantly different from theirs, since the proposed TiTok aims to reconstruct both the high-level and low-level details of an image, same as typical VQ-VAE tokenizers [35; 57; 19].

**Image Generation.** Image generation methods range from sampling the VAE [35], using GANs [23] to Diffusion Models [16; 58; 29; 52; 21; 44] and autoregressive models [63; 12; 50]. Prior studies that are most related to this work build on top of a learned VQ-VAE codebook to generate images. Autoregressive transformer [19; 69; 7; 37], similar to decoder-only language models, model each patch in a step-by-step fashion, thus requiring as many steps as token number, _e.g._, 256 or 1024. Non

Figure 2: **A speed and quality comparison of TiTok and prior arts on ImageNet \(256\times 256\) and \(512\times 512\) generation benchmarks. Speed-up is compared against DiT-XL/2 [52]. The sampling speed (de-tokenization included) is measured with an A100 GPU.**

autoregressive (or bidirectional) transformers [80; 72; 68], such as MaskGIT [9], generally predict more than a single token per step and thus require significantly fewer steps to predict a complete image. Apart from that, further studies looked into improved sampling strategies [39; 40; 38]. As we focus on the tokenization stage, we apply the commonly used non-autoregressive sampling scheme of MaskGIT to generate a sequence of tokens that is later decoded into an image.

## 3 Method

### Preliminary Background on VQ-VAE

The image tokenizer plays a pivotal role in facilitating the generative models by providing a compact image representation at latent space. For the scope of our discussion, we primarily focus on the Vector-Quantized (VQ) tokenizer [64; 19], given its broad applicability across various domains, including but not limited to image and video generation [19; 9; 58; 71], large-scale pretraining [12; 5; 49; 3; 18] and multi-modal models [20; 73].

A typical VQ model contains three key components: an encoder \(Enc\), a vector quantizer \(Quant\), and a decoder \(Dec\). Given an input image \(\mathbf{I}\in\mathbb{R}^{H\times W\times 3}\), where \(H\) and \(W\) denote the image's height and width, the image is initially processed by the encoder \(Enc\) and converted to latent embeddings \(\mathbf{Z}_{2D}=Enc(\mathbf{I})\), where \(\mathbf{Z}_{2D}\in\mathbb{R}^{\frac{H}{f}\times\frac{W}{f}\times D}\), which downsamples the spatial dimensions by a factor of \(f\). Subsequently, each embedding \(z\in\mathbb{R}^{D}\) is mapped (via the vector quantizer \(Quant\)) to the nearest code \(c_{i}\in\mathbb{R}^{D}\) in a learnable codebook \(\mathbb{C}\in\mathbb{R}^{N\times D}\), comprising \(N\) codes. Formally, we have:

\[Quant(z)=c_{i},\text{ where }i=\operatorname*{argmin}_{j\in\{1,2,\dots,N\}}\|z-c _{j}\|_{2}.\] (1)

During de-tokenization, the reconstructed image \(\mathbf{\hat{I}}\) is obtained via the decoder \(Dec\) as follows:

\[\mathbf{\hat{I}}=Dec(Quant(\mathbf{Z}_{2D})).\] (2)

Despite the numerous improvements over VQ-VAE [64] (_e.g._, loss function [19], model architecture [69], and quantization/codebook strategies [80; 37; 72]), the fundamental workflow (_e.g._, the 2D grid-based latent representations) has largely remained unchanged.

### TiTok: From 2D to 1D Tokenization

While existing VQ models have demonstrated significant achievements, a notable limitation within the standard workflow exists: the latent representation \(\mathbf{Z}_{2D}\) is often envisioned as a static 2D grid. Such a configuration inherently assumes a strict one-to-one mapping between the latent grids and the original image patches. This assumption limits the VQ model's ability to fully exploit the redundancies present in images, such as similarities among adjacent patches. Additionally, this approach constrains

Figure 3: **Illustration of image reconstruction (a) and generation (b) with the TiTok framework (c).** TiTok contains an encoder \(Enc\), a quantizer \(Quant\), and a decoder \(Dec\). Image patches, along with a few (_e.g._, 32) latent tokens, are passed through the Vision Transformer (ViT) encoder. The latent tokens are then vector-quantized. The quantized tokens, along with the mask tokens [15; 24], are fed to the ViT decoder to reconstruct the image.

the flexibility in selecting the latent size, with the most prevalent configurations being \(f=4\), \(f=8\), or \(f=16\)[58], resulting in \(4096\), \(1024\), or \(256\) tokens for an image of dimensions \(256\times 256\times 3\). Inspired by the success of 1D sequence representations in addressing a broad spectrum of computer vision problems [8; 1; 41], we propose to use a 1D sequence, without the fixed correspondence between latent representation and image patches in 2D tokenization, as an efficient and effective latent representation for image reconstruction and generation.

**Image Reconstruction with TiTok.** To initiate our exploration, we establish a novel framework named **T**ransformer-based 1-**T**ol**emiser (TiTok), leveraging Vision Transformer (ViT) [17]1 to tokenize images into 1D latent tokens and subsequently reconstruct the original images from these 1D latents. As depicted in Fig. 3, TiTok employs a standard ViT for both the tokenization and de-tokenization processes (_i.e._, both the encoder \(Enc\) and decoder \(Dec\) are ViTs). During tokenization, we patchify the image into patches (with a patch embedding layer) \(\mathbf{P}\in\mathbb{R}^{\frac{H}{f}\times\frac{W}{f}\times D}\) (with patch size equal to the downsampling factor \(f\) and embedding dimension \(D\)) and concatenate them with \(K\) latent tokens \(\mathbf{L}\in\mathbb{R}^{K\times D}\). They are then fed into the ViT encoder \(Enc\). In the encoder output, we only retain the latent tokens as the image's latent representation, thereby enabling a more compact latent representation of 1D sequence \(\mathbf{Z}_{1D}\) (with length \(K\)). This adjustment decouples the latent size from image's resolution and allows more flexibility in design choices. That is, we have:

Footnote 1: Although other Transformer-based architectures (_e.g._, Swin [45]) can also be used to instantiate TiTok, we adopt ViT for its simplicity and effectiveness.

\[\mathbf{Z}_{1D}=Enc(\mathbf{P}\oplus\mathbf{L}),\] (3)

where \(\oplus\) denotes concatenation, and we only retain the latent tokens from the encoder output.

In the de-tokenization phase, drawing inspiration from [15; 5; 24], we incorporate a sequence of mask tokens \(\mathbf{M}\in\mathbb{R}^{\frac{H}{f}\times\frac{W}{f}\times D}\)--obtained by replicating a single mask token \(\frac{H}{f}\times\frac{W}{f}\) times--to the quantized latent tokens \(\mathbf{Z}_{1D}\). The image is then reconstructed via the ViT decoder \(Dec\) as follows:

\[\mathbf{\hat{1}}=Dec(Quant(\mathbf{Z}_{1D})\oplus\mathbf{M}),\] (4)

where the latent tokens \(\mathbf{Z}_{1D}\) is first vector-quantized by \(Quant\) and then concatenated with the mask tokens \(\mathbf{M}\) before feeding to the decoder \(Dec\).

Despite its simplicity, we emphasize that the concept of compact 1D image tokenization remains underexplored in existing literature. The proposed TiTok thus serves as a foundational platform for exploring the potentials of 1D tokenization and de-tokenization for natural images. It is worth noting that although one may flatten 2D grid latents into a 1D sequence, it significantly differs from the proposed 1D tokenizer, due to the fact that the implicit 2D grid mapping constraints still persist.

**Image Generation with TiTok.** Besides the image reconstruction task which the tokenizer is trained for, we also evaluate its effectiveness for image generation, following the typical pipeline [19; 9]. Specifically, we adopt MaskGIT [9] as our generation framework due to its simplicity and effectiveness, allowing us to train a MaskGIT model by simply replacing its VQGAN tokenizer with our TiTok. We do not make any other specific modifications to MaskGIT, but for completeness, we briefly describe its whole generation process with TiTok.

The image is pre-tokenized into 1D discrete tokens. At each training step, a random ratio of the latent tokens are replaced with mask tokens. Then, a bidirectional transformer takes the masked token sequence as input, and predicts the corresponding discrete token ID of those masked tokens. The inference process consists of multiple sampling steps, where at each step the transformer's prediction for masked tokens will be sampled based on the prediction confidence, which are then used to update the masked images. In this way, the image is "progressively generated" from a sequence full of mask tokens to an image with generated tokens, which can later be de-tokenized back into pixel spaces. The MaskGIT framework shows a significant speed-up in the generation process compared to auto-regressive models. We refer readers to [9] for more details.

### Two-Stage Training of TiTok with Proxy Codes

**Existing Training Strategies for VQ Models.** Although most VQ models adhere to a straightforward formulation, their training process is notably sensitive, and the model's performance is heavily influenced by the adoption of more effective training paradigms. For instance, VQGAN [19] achievesa significant improvement in reconstruction FID (rFID) on the ImageNet [14] validation set, when compared to dVAE from DALL-E [55]. This enhancement is attributed to advancements in perceptual loss [33; 79] and adversarial loss [23]. Moreover, MaskGIT's modern implementation of VQGAN [9] utilizes refined training techniques without architectural improvements to boost the performance further. Notably, most of these improvements are exclusively applied during the training phase (_i.e_., through auxiliary losses) and significantly affect the models' efficacy. Given the complexity of the loss functions, extensive tuning of hyper-parameters involved, and, most critically, the missing of a publicly available code-base for reference or reproduction [9; 69; 72], establishing an optimal experimental setup for the proposed TiTok presents a substantial challenge, especially when the target is a compact 1D tokenization which was rarely studied in literature.

**Two-Stage Training Comes to the Rescue.** Although training TiTok with the typical Taming-VQGAN [19] setting is feasible, we introduce a two-stage training paradigm for an improved performance. The two-stage training strategy contains "warm-up" and "decoder fine-tuning" stages. Specifically, in the first "warm-up" stage, instead of directly regressing the RGB values and employing a variety of loss functions (as in existing methods), we propose to train 1D VQ models with the discrete codes generated by an off-the-shelf MaskGIT-VQGAN model, which we refer to as _proxy codes_. This approach allows us to bypass the intricate loss functions and GAN architectures, thereby concentrating our efforts on optimizing the 1D tokenization settings. Importantly, this modification does not harm the functionality of the tokenizer and quantizer within TiTok, which can still fully function for image tokenization and de-tokenization; the main adaptation simply involves the processing of TiTok's de-tokenizer output. Specifically, this output, comprising a set of proxy codes, is subsequently fed into the same off-the-shelf VQGAN decoder to generate the final RGB outputs. It is noteworthy that the introduction of _proxy codes_ differs from a simple distillation [26]. As verified in our experiments, TiTok yields significantly better generation performance than MaskGIT-VQGAN.

After the first training stage with proxy codes, we optionally have the second "decoder fine-tuning" stage, inspired by [10; 53], to improve the reconstruction quality. Specifically, we keep the encoder and quantizer frozen, and only train the decoder towards pixel space with the typical VQGAN training recipe [19]. We observe that such a two-stage training strategy significantly improves the training stability and reconstructed image quality, as shown in the experiments.

## 4 Experimental Results

### Preliminary Experiments of 1D Tokenization

Building upon TiTok, we explore a range of configurations, including the model size and the number of tokens, to identify the most efficient and effective setup for a 1D image tokenizer. These preliminary experiments serve to provide a thorough evaluation, seeking a practical configuration of TiTok.

**Preliminary Experimental Setup.** Unless specified otherwise, we train all models with images of resolution \(H=256\) and \(W=256\), using the open-source MaskGIT-VQGAN [9] to supply proxy codes for training. The patch size for both tokenizer and de-tokenizer is established with \(f=16\), and the codebook \(\mathbb{C}\) is configured to have \(N=1024\) entries with each entry a vector with \(16\) channels. For TiTok variants, we primarily investigate three model sizes--small, base, and large (_i.e_., TiTok-S, TiTok-B, TiTok-L)--comprising \(22M\), \(86M\), and \(307M\) parameters for encoder and decoder, respectively. We also assess the impact of varying the number of latent tokens \(K\) from \(16\) to \(256\). We perform ablation experiments with an efficient setting (_e.g_., shorter training).

**Evaluation Protocol.** Evaluation is conducted across multiple metrics to thoroughly assess the models, including both reconstruction and generation FID metrics (_i.e_., rFID and gFID) [25] on the ImageNet dataset. We examine training/inference throughput to offer a direct comparison of generative model's efficiency relative to different latent sizes. Furthermore, given that the 1D VQ model inherently serves as a form of compact image compression, we further investigate the semantic information retained by the model through linear probing following MAE setting [24]. For the complete details of the training and testing protocols (_e.g_., hyper-parameters, training costs), we refer the reader to the supplementary material Sec. A.

After the setup, we now summarize the preliminary experimental findings below.

**An Image Can be Represented by 32 Tokens.** The redundancy inherent in image representation is well-acknowledged, as evidenced by the practice of masking significant portions of images (_e.g_.,75% in MAE [24]) to expedite the training process without negatively affecting performance. This strategy has been validated across a variety of computer vision tasks that rely on high-level image features [30, 42]. However, the efficacy of such approaches in the context of image reconstruction and generation--where both low-level and high-level details are crucial for creating realistic reconstructed and generated outputs--remains underexplored. Consequently, in this experiment, we aim to determine the minimum number of tokens required to reconstruct and generate high-quality images. As depicted in Fig. 4a, although model performance progressively improves with an increase in the number of latent tokens, significant enhancements are predominantly observed when \(K\) ranges from \(16\) to \(128\). Beyond this point, increasing the latent space size yields only marginal gains. _Intriguingly, we find that with merely \(32\) latent tokens, TiTok-L achieves performance better than a 2D VQGAN model [19] using 256 tokens_. This observation suggests that as few as \(32\) tokens may suffice as an effective image latent representation, optimizing the utilization of image redundancy.

**Scaling Up Tokenizer Enables More Compact Latent Size.** Another intriguing observation from Fig. 4a is that larger tokenizers facilitate more compact representations. Specifically, TiTok-B with \(64\) latent tokens achieves performance comparable to TiTok-S with \(128\) latent tokens, while TiTok-L with \(32\) latent tokens matches the performance of TiTok-B with \(64\) latent tokens. _This pattern indicates that with each incremental increase in TiTok size (e.g., from S to B, or from B to L), it is possible to reduce the size of the latent image representation without compromising performance._ This trend underscores the potential benefits of scaling up the tokenizer to achieve even more compact image representations.

**Semantics Emerges with Compact Latent Space.** To evaluate the learned image representation, we perform linear probing experiments on the image tokenizer, as shown in Fig. 4b. Specifically, we add a batch normalization layer [31] followed by a linear layer on top of the frozen features from TiTok encoder, with all hyper-parameters strictly following the MAE protocol [24]. _We find that as the size of the latent representation decreases, the tokenizer increasingly learns semantically rich representations_, as indicated by the improved linear probing accuracy. This suggests that the model learns high-level information in scenarios of constrained representation space.

**Compact Latent Representation Improves Generative Training.** In addition to reconstruction capabilities, we assess TiTok's effectiveness and efficiency in generative downstream tasks, as illustrated in Fig. 4c and Fig. 4d. We note that variants of different tokenizer sizes yield comparable outcomes when the number of latent tokens is sufficiently large (_i.e._, \(K\geq 128\)). However, within the domain of compact latent sizes (_i.e._, \(K\leq 64\)), larger tokenizers notably enhance performance. Furthermore, the adaptability of 1D tokenization in TiTok facilitates more efficient and effective generative model training. For instance, model variants with \(K=32\), despite inferior reconstruction quality, demonstrate significantly better generative performance, _underscoring the advantages of employing a more condensed and semantically rich latent space for generative model training_. Additionally, the reduction in latent tokens markedly accelerates training and inference, with a \(12.8\times\) increase in training speed (2815.2 _vs._ 219.7 samples/s/gpu) and a \(4.5\times\) speed up sampling speed (123.1 _vs._ 27.5 samples/s/gpu), when utilizing \(K=32\) as opposed to \(K=256\).

Figure 4: **Preliminary experimental results with different TiTok variants.** We provide a comprehensive exploration in (a) ImageNet-1K reconstruction. (b) ImageNet-1K linear probing. (c) ImageNet-1K generation. (d) Training and inference throughput of MaskGIT-ViT as generator and TiTok as tokenizer (evaluated on A100 GPUs, inference includes de-tokenization step with TiTok-B). Detailed numbers can be found in supplementary material Sec. B.

### Main Experiments

Based on the observations above, the proposed TiTok family effectively trades off a larger model size to a more compact latent size. In this section, we majorly focus on ImageNet generation benchmarks against prior arts, and evaluate TiTok as a tokenizer in the generative MaskGIT framework [9].

**Implementation Details.** We primarily investigate the following TiTok variants: TiTok-S-128 (_i.e_., small model with 128 tokens), TiTok-B-64 (_i.e_., base model with 64 tokens), and TiTok-L-32 (_i.e_., large model with 32 tokens), where each variant designed to halve the latent space size while scaling up the model size. For resolution \(512\), we double the latent size to ensure more details are kept at higher resolution, leading to TiTok-L-64 and TiTok-B-128. In the final setting for TiTok training, the codebook is configured to \(N=4096\), and the training duration is extended to \(1M\) iterations (200 epochs). We also adopt the "decoder fine-tuning" stage to further enhance model performance, where the encoder and quantizer are kept frozen and the decoder is fine-tuned for \(500k\) iterations. For the training of generative models, we utilize the MaskGIT [9] framework without any specific modifications, except for the adoption of an arccos masking schedule [6]. All other parameters are the same as previous setups, and all design improvements will be verified in the ablation studies.

**Main Results.** We summarize the results on ImageNet-1K generation benchmark of resolution \(256\times 256\) and \(512\times 512\) in Tab. 1 and Tab. 2, respectively.2

Footnote 2: For fairness, we mainly consider tokenizers with vanilla VQ modules. More advanced quantization methods [72, 48] may further benefit TiTok but beyond this paper’s focus on 1D image tokenization. See supplementary material Sec. C for the complete table.

For ImageNet \(256\times 256\) results in Tab. 1, TiTok can achieve a similar level of reconstruction FID (rFID) with a much smaller number of latent tokens than other VQ models. Specifically, using merely \(32\) tokens, TiTok-L-32 achieves a rFID of \(2.21\), comparable to the well trained VQGAN from MaskGIT [9] (rFID 2.28), while using \(8\times\) smaller latent representation size. Furthermore, when using the same generator framework and same sampling steps, TiTok-L-32 improves over MaskGIT by a large margin (from \(6.18\) to \(2.77\) gFID), showcasing the benefits of a more effective generator training with compact 1D tokens. When compared to other diffusion-based generative models, TiTok can also achieve a competitive performance while enjoying an over \(\mathbf{100\times}\) speed-up during the sampling process. Specifically, TiTok-L-32 achieves a better gFID than LDM-4 [58] (2.77 _vs_. 3.60), while generating images dramatically faster by **254** times (101.6 samples/s _vs_. 0.4 samples/s). Our best-performing variant TiTok-S-128 outperforms state-of-the-art diffusion method DiT-XL/2 [52] (gFID \(1.97\)_vs_. \(2.27\)), with a \(13\times\) speed-up.

For ImageNet \(512\times 512\) results in Tab. 2, the significantly better accuracy-cost trade-off of TiTok persists. TiTok maintains a reasonably good rFID compared to other methods, especially considering that TiTok uses much fewer tokens (_i.e_., higher compression ratio). For generation, all TiTok variants

\begin{table}
\begin{tabular}{l c c c|c c c c} tokenizer & \#tokens & codebook size \(\text{rFID}\downarrow\) & generator & gFID\(\downarrow\) & P\(\downarrow\) & S\(\downarrow\) & T\(\uparrow\) \\ \hline \multicolumn{8}{c}{_diffusion-based generative models_} \\ \hline Taming-VQGAN\(\uparrow\)[58] & 1024 & 16384 & 1.14 & LDM-8 [58] & 7.76 & 258M & 200 & - \\ VAE\(\ddagger\)[58] & 4096\(\times\)3 & - & 0.27 & LDM-4 [58] & 3.60 & 400M & 250 & 0.4 \\  & & & & UViT-L/2 [4] & 3.40 & 287M & 50 & 1.1 \\ VAE [60]\(\ddagger\) & 1024\(\times\)4 & - & 0.62 & UViT-H/2 [4] & 2.29 & 501M & 50 & 0.6 \\  & & & & DiT-XL/2 [52] & 2.27 & 675M & 250 & 0.6 \\ \hline \multicolumn{8}{c}{_transformer-based generative models_} \\ \hline Taming-VQGAN [19] & 256 & 1024 & 7.94 & Taming-Transformer [19] & 15.78 & 1.4B & 256 & 7.5 \\ RQ-VAE [37] & 256 & 16384 & 3.20 & RQ-Transformer [37] & 8.71 & 1.4B & 16.1 \\  & & & & 7.55 & 3.8B & 64 & 9.7 \\ MaskGIT-VQGAN [9] & 256 & 1024 & 2.28 & MaskGIT-ViT [9] & 6.18 & **177M** & **8** & 50.5 \\ ViT-VQGAN [69] & 1024 & 8192 & 1.28 & VIM-Large [69] & 4.17 & 1.7B & 1024 & 0.3 \\ \hline TiTok-L-32 & 32 & 4096 & 2.21 & MaskGIT-ViT [9] & 2.77 & **177M** & **8** & **101.6** \\ TiTok-B-64 & 64 & 4096 & 1.70 & MaskGIT-ViT [9] & 2.48 & **177M** & **8** & 89.8 \\ TiTok-S-128 & 128 & 4096 & 1.71 & MaskGIT-UViT-L [9, 4] & 2.50 & **8** & 53.3 \\  & & & & **1.97** & 64 & 7.8 \\ \end{tabular}
\end{table}
Table 1: **ImageNet-1K \(256\times 256\) generation results evaluated with ADM [16]. \(\ddagger\): Trained on OpenImages [36]\(\ddagger\): Trained on OpenImages, LAION-Aesthetics/-Humans [59]. P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.**significantly outperform our baseline MaskGIT [9] by a large margin. When compared with diffusion-based models, TiTok-L-64 shows a superior performance to DiT-XL/2 [52] (\(2.74\)_vs_. \(3.04\)), while running **410\(\times\) faster**. The best-performing variant TiTok-B-128 can significantly outperform DiT-XL/2 by a large margin (\(2.13\)_vs_. \(3.04\)) but also generates high-quality samples **74\(\times\) faster**. We also provide visualization results and analysis in supplementary material Sec. D.

### Ablation Studies

We report the ablation studies regarding our final model designs in Tab. 3. Specifically, in Tab. 3(a), we ablate the tokenizer designs on image reconstruction. We begin with our baseline TiTok-L-32 which attains 6.59 rFID. Employing a larger codebook size improves the rFID by 0.74, while further increasing the training iterations (from 100 epochs to 200 epochs) yields another 0.37 improvement of rFID. On top of that, the "decoder fine-tuning" (our stage-2 training strategy) can substantially improve the overall reconstruction performance to 2.21 rFID.

In Tab. 3(b), we examine the effects of different masking schedules for MaskGIT with TiTok. Interestingly, unlike the original MaskGIT setting [9] which empirically found that the cosine masking schedule significantly outperforms the other schedules, we observe that MaskGIT equipped with TiTok changes the preference to the arccos or linear schedules. Additionally, unlike [9] which reported that the root schedule performs much worse than the others, we observe that TiTok is quite robust to different masking schedules. We attribute the observations to TiTok's ability to provide a more compact and more semantic meaningful tokens compared to 2D VQGAN, as compared to the cosine masking schedule, linear and arccos schedules have a lower masking ratio in the early steps. This coincides with the observation that masking ratio is usually higher for redundant signals (_e.g._, 75% masking ratio in images [24]) while relatively lower for semantic meaningful inputs (_e.g._, 15% masking ratio in languages [15]).

We ablate the effects of training paradigm in Tab. 3(c). We begin with the training setting of Taming-VQGAN [19], where TiTok-B-64 obtains 5.15 rFID, outperforming the original 2D Taming-VQGAN's 7.94 rFID under the same training setting. We also show the necessity of 1D tokenization by building a 2D variant of TiTok-B64, where the architecture remains the same except that image patches instead of latent tokens are used as image representation. As a result, we observe that the 2D variant suffers from a much worse performance (15.58_vs_. 5.15 rFID), since the fixed correspondences in 2D tokenization limited a reasonable reconstruction under compact latent space. This result

\begin{table}

\end{table}
Table 2: **ImageNet-1K \(512\times 512\) generation results evaluated with ADM [16]. \(\ddagger\): Trained on OpenImages, LAION-Aesthetics and LAION-Humans [59]. P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.**

\begin{table}

\end{table}
Table 3: **Ablation study improved final models for main experiments. We ablate the tokenizer designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray.**

\begin{table}

\end{table}
Table 3: **Ablation study improved final models for main experiments. We ablate the tokenizer designs, and generator designs on ImageNet-1k benchmark. The final settings are labeled in gray.**

\begin{table}

\end{table}
Table 2: **ImageNet-1K \(512\times 512\) generation results evaluated with ADM [16]. \(\ddagger\): Trained on OpenImages, LAION-Aesthetics and LAION-Humans [59]. P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision.**demonstrates the effectiveness of the proposed 1D tokenization, especially at a much more compact latent size. **Although TiTok can achieve a reasonably well performance under straightforward single-stage training, there exists a performance gap compared to the MaskGIT-VQGAN [9] due to the missing of a strong training recipe, of which no public reference or access exists.** Therefore, we adopt the two-stage training with _proxy codes_, which proves to be effective and can outperform the MaskGIT-VQGAN (1.70 _vs._ 2.28 rFID). It is noteworthy that the two-stage training is not that crucial to obtain a reasonable 1D tokenizer, and we believe that TiTok, with the simple single-stage Taming-VQGAN's training setting, could also benefit from training on a larger-scale dataset [36] as demonstrated in [58] and we leave it for future work due to the limited compute.

**Details on the Two-Stage Training.** We provide more technical details on the two-stage training. Specifically:

* In the first stage (warm-up stage), we use an off-the-shelf ImageNet-pretrained MaskGIT-VQ tokenizer to tokenize the input image into 256 tokens, which we refer to as proxy codes.
* In the first stage training, instead of regressing the original RGB values, we use the proxy codes as reconstruction targets. Specifically, the workflow is: RGB images are patchified and flattened into a sequence and concatenated with 32 latent tokens, then they are fed into TiTok-Enc (Encoder of TiTok). Later, the latent tokens are kept as token representation and go through the quantizer. The quantized latent tokens are concatenated with 256 mask tokens and go through the TiTok-Dec (Decoder of TiTok). And the final output mask tokens are supervised by proxy codes using cross-entropy loss.
* Afterwards, we freeze both the TiTok-Enc and quantizer, and then only fine-tune the TiTok-Dec (responsible for reconstructing proxy codes) and MaskGIT-Dec (responsible for reconstructing RGB values from proxy codes) end-to-end towards pixel space, where the training losses include L2 loss, perceptual loss, and GAN loss following the common VQGAN paradigm.

Moreover, we also note that two-stage training is not necessary for TiTok training, and it works fine with the commonly used and publicly available Taming-VQGAN recipe as is shown in Tab. 3c. In this case, the whole workflow is pretty straightforward, where the TiTok-Dec will instead directly reconstruct the images at pixel space.

However, the Taming-VQGAN recipe (developed more than 3 years ago) leads to an inferior FID score when compared to state-of-the-art tokenizers, putting TiTok at disadvantage when compared against other methods. Therefore we propose the two-stage training to benefit TiTok from the state-of-art MaskGIT-VQGAN tokenizer, which shares a similar architecture to Taming-VQGAN but has a significantly better score (rFID 2.28 v.s. 7.94).

We also note that TiTok can work well with single-stage recipe, and it is promising to incorporate the recent modern VQGAN recipe from [47; 68] in our preliminary experiments.

## 5 Conclusion

In this paper, we have explored a compact 1D tokenization TiTok for reconstructing and generating natural images. Unlike the existing 2D VQ models that consider the image latent space as a 2D grid, we provide a more compact formulation to tokenize an image into a 1D latent sequence. The proposed TiTok can represent an image with \(8\) to \(64\) times fewer tokens than the commonly used 2D tokenizers. Moreover, the compact 1D tokens not only significantly improve the generation model's training and inference throughput, but also achieve a competitive FID on the ImageNet benchmarks. We hope our research can shed some light in the direction towards more efficient image representation and generation models with 1D image tokenization.

## References

* [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _NeurIPS_, 2022.
* [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In _ICCV_, 2015.
* [3] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros. Sequential modeling enables scalable learning for large vision models. In _CVPR_, 2024.
* [4] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: A vit backbone for diffusion models. In _CVPR_, 2023.
* [5] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In _ICLR_, 2022.
* [6] Victor Besnier and Mickael Chen. A pytorch reproduction of masked generative image transformer. _arXiv preprint arXiv:2310.14400_, 2023.
* [7] Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan: Towards high-resolution image generation with efficient vision transformers. In _ICCV_, 2023.
* [8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In _ECCV_, 2020.
* [9] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative image transformer. In _CVPR_, 2022.
* [10] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In _ICML_, 2023.
* [11] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision models in the vision-language era. In _CVPR_, 2024.
* [12] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In _ICML_, 2020.
* [13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.
* [14] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, 2009.
* [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2018.
* [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _NeurIPS_, 2021.
* [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [18] Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai, Miguel Angel Bautista, Alexander Toshev, Vaishaal Shankar, Joshua M Susskind, and Armand Joulin. Scalable pre-training of large autoregressive image models. _arXiv preprint arXiv:2401.08541_, 2024.
* [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In _CVPR_, 2021.
* [20] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In _ECCV_, 2022.
* [21] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. In _ICCV_, 2023.

* [22] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making Ilama see and draw with seed tokenizer. In _ICLR_, 2024.
* [23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. _NeurIPS_, 2014.
* [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _CVPR_, 2022.
* [25] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _NeurIPS_, 2017.
* [26] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_, 2015.
* [27] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. _science_, 313(5786), 2006.
* [28] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [29] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In _ICML_, 2023.
* [30] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. _arXiv preprint arXiv:2004.00849_, 2020.
* [31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _ICML_, 2015.
* [32] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, et al. Unified language-vision pretraining with dynamic discrete visual tokenization. In _ICLR_, 2024.
* [33] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In _ECCV_, 2016.
* [34] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qihang Yu, Hong-Seok Kim, Hartwig Adam, In So Kweon, and Liang-Chieh Chen. Tubeformer-deeplab: Video mask transformer. In _CVPR_, 2022.
* [35] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In _ICLR_, 2014.
* [36] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 2020.
* [37] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In _CVPR_, 2022.
* [38] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise: Effective image generation with contextual rq-transformer. _NeurIPS_, 2022.
* [39] Jose Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with token-critic. In _ECCV_, 2022.
* [40] Jose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Predictor-corrector sampling for discrete diffusion models. In _ICLR_, 2023.
* [41] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, 2023.
* [42] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. Scaling language-image pre-training via masking. In _CVPR_, 2023.
* [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _NeurIPS_, 2023.
* [44] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. _NeurIPS_, 2024.

* [45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, 2021.
* [46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [47] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. _arXiv preprint arXiv:2409.04410_, 2024.
* [48] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In _ICLR_, 2024.
* [49] David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. _NeurIPS_, 2023.
* [50] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. _NeurIPS_, 2017.
* [51] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [52] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _ICCV_, 2023.
* [53] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _ICML_, 2021.
* [55] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _ICML_, 2021.
* [56] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. _NeurIPS_, 2019.
* [57] Jason Tyler Rolfe. Discrete variational autoencoders. In _ICLR_, 2017.
* [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, 2022.
* [59] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. _NeurIPS_, 2022.
* [60] stabilityai, 2023. URL https://huggingface.co/stabilityai/sd-vae-ft-ema.
* [61] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. In _ICLR_, 2024.
* [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [63] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. _NeurIPS_, 2016.
* [64] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _NeurIPS_, 2017.
* [65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _NeurIPS_, 2017.
* [66] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In _Proceedings of the 25th international conference on Machine learning_, pages 1096-1103, 2008.
* [67] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In _CVPR_, 2021.

* [68] Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. _arXiv preprint arXiv:2409.16211_, 2024.
* [69] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In _ICLR_, 2022.
* [70] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _TMLR_, 2022.
* [71] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In _CVPR_, 2023.
* [72] Lijun Yu, Jose Lezama, Nitesh B Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual generation. In _ICLR_, 2024.
* [73] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. _arXiv preprint arXiv:2309.02591_, 2023.
* [74] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In _CVPR_, 2022.
* [75] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. k-means Mask Transformer. In _ECCV_, 2022.
* [76] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. _NeurIPS_, 2023.
* [77] Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Towards open-ended visual recognition with large language model. In _ECCV_, 2024.
* [78] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. _arXiv preprint arXiv:2203.03605_, 2022.
* [79] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _CVPR_, 2018.
* [80] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. _NeurIPS_, 2022.
* [81] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In _ICLR_, 2020.

## Appendix

In the supplementary materials, we provide the following additional details:

* The comprehensive training and testing hyper-parameters and training costs for TiTok (Sec. A).
* The detailed results of the preliminary experiments reported in main paper's Fig. 4(Sec. B).
* A more comprehensive comparison with more metrics and baselines (Sec. C).
* Qualitative visualizations (Sec. D).
* Limitation discussion (Sec. E).
* Broader Impacts discussion (Sec. F).
* Dataset Licenses (Sec. G).

## Appendix A Training and Testing Protocols

For image reconstruction (**tokenizer**) **at preliminary experiments**, the training augmentation is confined to random cropping and flipping, following [19]. The training regimen spans a short schedule, featuring a batch size of \(256\) over \(500k\) training iterations, which correlates to roughly \(100\) epochs on the ImageNet dataset. We employ the AdamW optimizer [46] with an initial learning rate of \(1\times 10^{-4}\) (with cosine decay) and weight decay \(1\times 10^{-4}\). We only adopt stage-1 training here (_i.e._, only the "warm-up" training stage). For the **main experiments**, we adopt the improvements as shown in the ablation study (Tab. 3 in main paper), including longer training to \(200\) epochs and decoder fine-tuning, all other hyper-parameters remain the same. We use patch size \(16\) for all vision transformers at resolution \(256\times 256\) and increase it to \(32\) for resolution \(512\times 512\) to ensure a computation efficiency.

TiTok-L refers to using a ViT-L for TiTok encoder and decoder, and TiTok-B, TiTok-S refers to using ViT-B and ViT-S respectively. Moreover, the tokenizer training takes 64 A100-40G for 74 hours (TiTok-L-32), 32 A100-40G for 41 hours (TiTok-B-64), 32 A100-40G for 50 hours (TiTok-S-128), 32 A100-40G for 70 hours (TiTok-B-128 for resolution 512), and 64 A100-40G for 91 hours (TiTok-L-64 for resolution 512), respectively.

For image generation (**generator**) **at preliminary experiments**, we majorly build the training and testing protocols on top of [9]. Specifically, all images are pre-tokenized using center crop and random flipping augmentation, and then processed by MaskGIT [9] to generate images via the masked image modeling procedure. During inference, a cosine masking schedule is utilized with 8 steps. The generative models are trained with a batch size of \(2048\) and \(500k\) iterations to improve training efficiency. We use AdamW optimizer [46] with learning rate \(2\times 10^{-4}\) and weight decay \(0.03\). The learning rate starts from \(2\times 10^{-4}\) and then decay to \(1\times 10^{-5}\) following a cosine decaying schedule. We apply a dropout probability of \(0.1\) on the class condition. The only differences of **main experiments** are using an acccos masking schedule as discussed in the ablation study (main paper Tab. 3), all other hyper-parameters remain the same. We follow prior arts [19, 9] to generate \(50k\) samples for generation FID evaluation. We also adopt classifier-free guidance [28] following prior arts [10, 72].

At ImageNet \(256\times 256\), we use guidance scale 4.5, temperature 9.5 for TiTok-L-32, guidance scale 3.0, temperature 11.0 for TiTok-B-64, guidance scale 2.0, temperature 3.0 for TiTok-S-128. At ImageNet \(512\times 512\), we use guidance scale 2.0, temperature 7.5 for TiTok-L-64, guidance scale 2.5, temperature 6.5 for TiTok-B-128.

The generator training takes 32 A100-40G for 12 hours (TiTok-L-32), 16 hours (TiTok-B-64), 29 hours (TiTok-S-128), 26 hours (TiTok-B-128 for resolution 512), 18 hours (TiTok-L-64 for resolution 512) respectively.

## Appendix B Detailed Results of Preliminary Experiments

We summarize the detailed results for Fig. 4 of main paper in Tab. 4.

## Appendix C Additional Results

We further report the class-conditional generation results comparison with more metrics and baselines in Tab. 5 and Tab. 6 for ImageNet \(256\times 256\) and \(512\times 512\) generation benchmarks, respectively. Moreover, we report both results without and with classifier-free guidance [28] under column "w/o guidance" and "w/ guidance" respectively.

As shown in Tab. 5, both TiTok-L-32 and TiTok-B-64 set a new state-of-the-art performance for results without classifier-free guidance (_i.e._, w/o guidance column), while generating images at a much faster pace. Specifically, TiTok-L-32 achieves \(3.15\) gFID, surpassing current state-of-the-art MAGVIT-v2 [72]'s gFID \(3.65\), while requiring much fewer sampling steps (\(8\)_vs._\(64\)) and smaller model size (\(177M\)_vs._\(307M\)), leading to a substantial sampling speed-up (\(92.4\times\) faster, \(101.6\)_vs._\(1.1\) samples/sec). Additionally, when compared to MaskGIT [9], which uses the exact same generator model (_i.e._, MaskGIT-ViT) as ours and the only difference is the tokenizer, TiTok-L-32 achieves significantly a better performance (\(3.15\)_vs._\(6.18\)). The improvement demonstrates the efficiency and effectiveness of the learned compact 1D latent space for image representation. When it comes to resolution \(512\times 512\) in Tab. 6, MaskGIT [9] requires \(1024\) tokens for image latent representation, while TiTok-L-64 requires \(16\times\) fewer. As a result, when using the same generator (_i.e._, MaskGIT-ViT), TiTok-L-64, w/o classifier-free guidance, not only significantly outperforms MaskGIT in terms of gFID (\(3.64\)_vs._\(7.32\)) but also generates samples much faster. The advantages of TiTok become

\begin{table}

\end{table}
Table 4: **Detailed results of preliminary experiments in main paper.**

(a) reconstruction FID.

\begin{table}
\begin{tabular}{l c|c c c c c c c c c} tokenizer & rFID\(\downarrow\) & generator & \multicolumn{4}{c}{w/o guidance w/ guidance} & \multirow{2}{*}{P\(\downarrow\)} & \multirow{2}{*}{S\(\downarrow\)} & \multirow{2}{*}{T\(\uparrow\)} \\  & & & & & & & & & & \\  & & & & & & & & & & \\ \hline \multicolumn{10}{c}{_diffusion-based generative models_} \\ \hline Taming-VQGAN\(\dagger\)[58] & 1.14 & LDM-8 [58] & 15.82 & 78.82 & 7.76 & 209.5 & 258M & 200 & - \\ VAE\(\ddagger\)[58] & 0.27 & LDM-4 [58] & 10.56 & 103.5 & 3.60 & 247.7 & 400M & 250 & 0.4 \\  & & UViT-L/2 [4] & 9.03 & 111.5 & 3.40 & 219.9 & 287M & 50 & 1.1 \\ VAE [60]\(\ddagger\) & 0.62 & UViT-H/2 [4] & 6.60 & 142.5 & 2.29 & 263.9 & 501M & 50 & 0.6 \\  & & DiT-XL-v2 [52] & 9.62 & 121.5 & 2.27 & 278.2 & 675M & 250 & 0.6 \\ \hline \multicolumn{10}{c}{_transformer-based generative models_} \\ \hline Taming-VQGAN [19] & 7.94 & Taming-Transformer [19] & 15.78 & 78.3 & - & - & 1.4B & 256 & 7.5 \\ RQ-VAE [37] & 3.20 & RQ-Transformer [37] & 8.71 & 119.0 & - & - & 1.4B & 16.1 \\  & & 7.55 & 134.0 & - & - & 3.8B & 64 & 9.7 \\ MaskGIT-VQGAN [9] & 2.28 & MaskGIT-ViT [9] & 6.18 & 182.1 & - & **177M** & **8** & 50.5 \\ ViT-VQGAN [69] & 1.28 & VIM-Large [69] & 4.17 & 175.1 & - & - & 1.7B & 1024 & 0.3 \\ LFQ [72] & \(\sim\)\(\sim\)0.9 & MAGVIT-v2 [72] & 3.65 & 200.5 & 1.78 & 319.4 & 307M & 64 & 1.1 \\ \hline TiTok-L-32 & 2.21 & MaskGIT-ViT [9] & 3.15 & 173.0 & 2.77 & 199.8 & **177M** & **8** & **101.6** \\ TiTok-B-64 & 1.70 & MaskGIT-ViT [9] & **3.08** & **192.5** & 2.48 & 214.7 & **177M** & **8** & 89.8 \\ TiTok-S-128 & 1.71 & MaskGIT-UViT-L [9, 4] & 4.61 & 166.7 & 2.50 & 278.7 & 287M & **8** & 53.3 \\  & & 4.44 & 168.2 & **1.97** & **281.8** & 64 & 7.8 \\ \end{tabular}
\end{table}
Table 5: **ImageNet-1K \(256\times 256\) generation results evaluated with ADM [16]. \(\dagger\): Trained on OpenImages [36]\(\ddagger\): Trained on OpenImages, LAION-Aesthetics-/Humans [59]. P: generator’s parameters. S: sampling steps. T: throughput as samples per seconds on A100 with float32 precision, measured with _w/ guidance_ variants if available. “guidance” refers to classifier-free guidance.**

[MISSING_PAGE_FAIL:17]

models can be heavily influenced by dataset biases, leading to the reinforcement of negative social stereotypes and viewpoints. Developing unbiased models that ensure both robustness and fairness is a critical area of research. However, addressing these issues is beyond the scope of this paper.

Figure 5: **Visualization of generated images from TiTok variants with MaskGIT [9].** Corresponding ImageNet class names are shown below the images.

Figure 6: **Visualization of generated images from TiTok-L-32 with MaskGIT [9] across random ImageNet classes.**

Considering the potential risks, this paper is limited to class-conditional generation using a fixed, public, and controlled set of classes.

## Appendix G Dataset Licenses

The datasets we used for training and/or testing TiTok are described as follows.

ImageNet-1K:We train and evaluate TiTok on ImageNet-1K generation benchmark. This dataset spans 1000 object classes and contains 1,281,167 training images, 50,000 validation images and

Figure 7: **Visual comparison of reconstruction results.** Scaling model size enables a better image quality while using a more compact latent space size. It is also observed that TiTok tends to keep the salient regions when latent space is limited.

100,000 test images. We use the training set for our tokenizer and generator training. The validation set is used to compute reconstruction FID for evaluating tokenizers. The generation results are evaluated with generation FID using pre-computed statistics and scripts from ADM [16]3.

Footnote 3: https://github.com/openai/guided-diffusion/tree/main/evaluations

License: https://image-net.org/accessagreement

URL: https://www.image-net.org/

Figure 8: Uncurated \(256\times 256\) TiTok-L-32 samples. Class labels (class ids) from left to right and top to down are: “macaw” (88), “volcano” (980), “coral reef” (973), “white wolf” (270).

Figure 9: Uncurated \(256\times 256\) TiTok-L-32 samples. Class labels (class ids) from left to right and top to down are: “cheeseburger” (933), “cliff” (972), “fountain” (562), “Siberian husky” (250).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We propose a novel 1D tokenization method, achieving significant speed-up while still capable of generating high-quality images, as demonstrated in the abstract and introduction, supported by the Sec. 3 and Sec. 4.2 parts. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see Supplementary Material Sec. E. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]

Justification: No theoretical result.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Comprehensive details regarding model training and testing have been provided in Main Paper Sec. 4.1**Preliminary Experimental Setup**, Sec. 4.2**Implementation Details**, and Supplementary Material Sec. A **Training and Testing Protocols** Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code and model are available at https://github.com/bytedance/1d-tokenizer. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Comprehensive details regarding model training and testing have been provided in Main Paper Sec. 4.1 **Preliminary Experimental Setup**, Sec. 4.2 **Implementation Details**, and Supplementary Material Sec. A **Training and Testing Protocols** Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We did not report error bars following prior arts [19, 9, 72]. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The training costs are reported in Supplementary Material Sec. A **Training and Testing Protocols**. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research in the paper conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The broader impacts have been discussed in Supplementary Material Sec. F **Broader Impacts**. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Although our study includes image generation, we focus on class-conditional image generation with a predefined set of classes, which does not pose the same risks as those text-to-image models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Supplementary Material Sec. G **Dataset Licenses**. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.