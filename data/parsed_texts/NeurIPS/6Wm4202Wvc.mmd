# Label Privacy in Split Learning for Large Models with Parameter-Efficient Training

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As deep learning models become larger and more expensive, many practitioners turn to fine-tuning APIs. These web services allow fine-tuning a model between two parties: the client that provides the data, and the server that hosts the model. While convenient, these APIs raise a new concern: the data of the client is at risk of privacy breach during the training procedure. This challenge presents an important practical case of vertical federated learning, where the two parties perform parameter-efficient fine-tuning (PEFT) of a large model. In this study, we systematically search for a way to fine-tune models over an API _while keeping the labels private_. We analyze the privacy of LoRA, a popular approach for parameter-efficient fine-tuning when training over an API. Using this analysis, we propose P\({}^{3}\)EFT, a multi-party split learning algorithm that takes advantage of existing PEFT properties to maintain privacy at a lower performance overhead. To validate our algorithm, we fine-tune DeBERTa-v2-XXLarge, Flan-T5 Large and LLaMA-2 7B using LoRA adapters on a range of NLP tasks. We find that P\({}^{3}\)EFT is competitive with existing privacy-preserving methods in multi-party and two-party setups while having higher accuracy.

## 1 Introduction

One of the main reasons behind deep learning success is its ability to transfer knowledge between tasks [34]. When training a model for any particular problem, it is common to reuse previously trained models from other, related problems. In the past, this was typically done by downloading pre-trained model weights from public hubs, then fine-tuning the said models on the downstream task. However, as models grow larger and more compute-intensive, fine-tuning them locally becomes an increasingly difficult task. Furthermore, many recent models are not released, but instead made available as proprietary services.

When a model cannot be fine-tuned locally, many practitioners opt instead for the so-called fine-tuning APIs [27; 16; 6; 26]. These APIs are web services that host one or several pre-trained models and allow clients to perform limited fine-tuning. More specifically, APIs usually allow their clients to run parameter-efficient fine-tuning (PEFT), such as LoRA [15] or Prefix-tuning [21]. These techniques allow adapting a model to a dataset while training a relatively small number of additional weights, which is particularly important for large language or image generation models that have billions of parameters.

Although the fine-tuning APIs can be convenient, they also introduce new risk in terms of data privacy. When a client uses such API to train on sensitive data, they need to ensure that their data will stay private [7]. This is particularly important when dealing with patient's medical records, personal user data or trade secrets [24; 19]. The two main threats to data privacy are that the API provider obtains the private data and that a third party intercepts data in transit. Therefore, data privacy isnot guaranteed even if the API provider is trusted. Several recent works propose LLM fine-tuning protocols that establish a certain level of privacy for multi-party fine-tuning [42; 7; 22]. Unfortunately, these algorithms work for a narrow class of fine-tuning algorithms or assume that a client can run LLM training locally using an obfuscated version of the model, provided by a remote server [42]. As a result, these algorithms are impractical for our use case of fine-tuning over an API. The few algorithms that are suitable for API fine-tuning guarantee the privacy of input tokens [22], meaning that the attacker can infer private training _labels_.

In this work, we seek to alleviate this problem by designing a two-party fine-tuning protocol that performs standard parameter-efficient fine-tuning with privacy guarantees. We formulate our protocol as a **special case of split learning** (or vertical federated learning), where one side (server) holds the pre-trained model and the other (client) has private training data. More specifically, we focus on **the privacy of client's training labels**. While input privacy is often crucial, there are scenarios where input data is publicly available, such as social media user pages. In these cases, labels could include ad clicks (known to the social network) or financial information (known to a bank that matches social profiles to its internal records). This example further justifies the use of LLMs, as social media pages often contain substantial amounts of text, and LLMs excel at processing long-context data.

Instead of developing a specific privacy-preserving architecture, we seek algorithms that can work with popular existing models and PEFT algorithms. Furthermore, our approach relies on the properties of parameter-efficient fine-tuning. Notably, since the adapters are compact, both parties can maintain multiple sets of adapters and swap between them with relative ease. This allows us to design a PEFT-specific algorithm that can solve its task more effectively than general split learning strategies [18].

We summarize our main contributions as follows:

* We analyze Low-Rank Adaptation, a common parameter-efficient fine-tuning algorithm, from the perspective of label privacy in the split learning setup. We observe that, despite fine-tuning less than \(0.1\%\) of model parameters, PEFT algorithms leak client's training labels against simple attacks that work for modern pretrained transformers.
* Based on our analysis, we formulate a framework for privacy-preserving parameter-efficient fine-tuning (P\({}^{3}\)EFT). This framework leverages the properties of PEFT to obfuscate the gradients and parameters communicated during fine-tuning with little impact on the fine-tuned model quality.
* To verify the practical viability of P\({}^{3}\)EFT, we conduct experiments on popular real-world PEFT workloads1. Specifically, we fine-tune DeBERTa-v2-XXL [13], Flan-T5-Large [4] and LLaMA-2 7B [35] on a set of standard language understanding problems. We find that, compared to prior split learning algorithms, P\({}^{3}\)EFT can maintain label privacy throughout training with a significantly smaller accuracy drop. Footnote 1: The code is available at github.com/anonymousauthor56/P3EFT

## 2 Background

### Federated learning and split learning

Privacy preservation in machine learning has been a subject of active study within several frameworks. An important branch of privacy-preserving learning methods is federated learning, or FL [24], which can be broadly described as an approach allowing several parties to train a model jointly without sharing their private data. In particular, vertical federated learning [12; 43] targets the scenario where different features (including the label) of each training instance are kept by different parties.

One of the most popular approaches to vertical FL for neural networks is split learning [10; 37], where each party stores its part of the overall model. To train the model in such an approach, it is only necessary to transfer the intermediate activations and the gradients between layers, while the data itself is stored at the premises of the participant hosting each layer. In this work, we focus on the two-party formulation of split learning, where one side stores the features for each example and another one stores the labels.

Recent works have investigated the setting of two-party split learning from the label leakage perspective [38; 28]: because the label party needs to pass the gradients of the loss function to the non-label party, it is possible for the latter party to deduce the labels by inspecting the gradients or activations or by hijacking the training procedure. Li et al. [18] provide a set of attack methods that allow recovering private labels and propose a defense mechanism that injects noise into the gradients; however, they test the approach on pretraining smaller models, and we study finetuning large models on private downstream data.

### Parameter-efficient finetuning

The majority of large neural networks today are not trained with a specific task in mind: instead, they are pretrained on a general objective and then adapted for the downstream problem. Importantly, the growth in the size of foundation models has led to the increased popularity of parameter-efficient finetuning (PEFT) methods that adapt the model to a given task by training a small number of task-specific parameters. There are several prominent approaches to parameter-efficient finetuning, ranging from trainable prompts [21; 11], to residual adapters [14; 29]. We focus on Low-Rank Adaptation (or LoRA, 15), one of the most popular PEFT methods that adds extra parameters to each weight matrix in the form of a low-rank factorization (see Appendix C for a more detailed description). Such formulation allows LoRA adapters to be merged into the original weights after finetuning; this ability, combined with the simplicity of the method, has made LoRA a broadly popular approach in multiple domains. Still, the approach we propose can be applied to any PEFT method.

Several recent lines of work explore the problem of fine-tuning LLMs with privacy guarantees [44; 31]. Zhao et al. [46] analyze the viability of prompt tuning for federated learning, and Zhang et al. [45], Liu et al. [23] study PEFT algorithms in the setting of _horizontal_ federated learning, that is, where multiple users train a shared model on their local private data. Another, more relevant research direction considers private fine-tuning in a _vertical_ federated learning scenario, where participants hold different model layers [22; 40]. Most of these studies leverage the idea of differential privacy to prove an upper bound on how much information is leaked [8]. Unfortunately, these upper bounds are typically loose and do not match practical observations for real models. Furthermore, the majority of these studies only guarantees privacy of specific parts of the training procedure: for instance, Li et al. [22] only protects the input features, and not labels or model parameters. Finally, Xiao et al. [42] presents an alternative algorithm that protects client data by running the entire fine-tuning on client side by emulating the server-side model layers. While this approach is more holistic, it assumes that clients can run fine-tuning locally, which makes it impractical for many real-world users of LLM fine-tuning APIs. The primary distinction between our work and these studies is that we investigate parameter-efficient adaptation in the setting of split learning: we aim to finetune a model without disclosing the labels of examples to the model provider.

## 3 Privacy-preserving parameter-efficient fine-tuning

In this section, we analyze the privacy of parameter-efficient fine-tuning and propose a protocol for two-party parameter-efficient fine-tuning with the desired privacy guarantees. We begin by analyzing the privacy of API fine-tuning with popular PEFT algorithms in Sections 3.1 and 3.2. Then, in Section 3.3, we formulate a protocol for privately computing gradients over fine-tuning APIs. Finally, we formulate the full P\({}^{3}\)EFT protocol in Section 3.4.

### Setup

To analyze the privacy of API fine-tuning, we first need to formulate a common framework for this type of APIs and develop private learning protocols. This step is important, because existing fine-tuning APIs greatly vary in what they offer to the client: from closed APIs that require users to submit their full training data [27] to more flexible APIs where clients can run individual training steps [20; 2; 30]. Similarly to most existing works on split learning, we focus on the latter type of APIs that allows clients to run individual forward and backward passes over a remote model. Thus, a client can use these APIs to obtain the training gradients for their PEFT adapters, then update adapters locally with any optimization method. In our work, we adopt this archetype of fine-tuning API as it offers sufficient flexibility to develop privacy-preserving algorithms.

We formulate fine-tuning over an API for two or more parties: a client, and one or several servers. The client owns a training dataset with inputs \(X\) and labels \(Y\). In turn, each server has the same pre-trained model \(h(x_{i},\theta)\in\mathcal{R}^{d}\). Note that the parameters \(\theta\) denote not the pre-trained model weights, but the trainable adapter weights for a certain PEFT algorithm. A model can encode an input \(x_{i}\in X\) and produce a \(d\)-dimensional vector of activations that depend on the learned adapter weights \(\theta\).

To allow fine-tuning, a server offers two API methods:

1. \(\textbf{forward}(x,\theta)\to h(x,\theta)\) that computes model activations on input \(x\) using adapter weights \(\theta\);
2. \(\textbf{backprop}(x,\theta,g_{h})\to g_{\theta}\) that receives gradients of an arbitrary loss function w.r.t. model activations \(g_{h}=\frac{\partial L(h(x,\theta))}{\partial h(x,\theta)}\) and returns the gradients w.r.t. adapter parameters, \(g_{\theta}=\frac{\partial L(h(x,\theta))}{\partial\theta}\).

We further assume that both \(\text{forward}(\cdot)\) and \(\text{backprop}(\cdot)\) APIs are stateless and deterministic, i.e. calling the same API method multiple times (or on multiple servers) with the same inputs produces identical results. Thus, if the model uses dropout or any other form of non-determinism, we assume that clients provide the random seed as a part of \(x\).

To fine-tune a model with this API, a client can initialize adapters locally, alongside with a small task-specific head2, then train both adapters and the head. For each training batch \((x,y)\in D\), a client calls \(\text{forward}(x,\theta)\) to compute feature representations, then predicts with local "head" and computes task-specific loss function \(L\). After that, a client performs backward pass: first, it computes gradients w.r.t. local head inputs \(g_{h}=\frac{\partial L}{\partial h}\), then passes those gradients to a remote server via \(\text{backprop}(x,\theta,g_{h})\) API call to compute gradients w.r.t. \(\frac{\partial L}{\partial\theta}\). Finally, a client updates both \(\theta\) and local "head" parameters using the optimizer of choice.

Footnote 2: A linear layer that predicts class logits or regression target.

Before building more advanced algorithms, let us analyze the privacy of client's labels under standard fine-tuning. We consider an "honest, but curious" attacker model. This means that the server will faithfully run the forward and backprop computations as requested by the client without changing the results. Furthermore, we assume that servers are independent and do not communicate client's data between each other. However, a server can recover client's labels by performing arbitrary computations using any information it receives from the client. When training in this way, a client does not directly communicate training labels to the server. However, it communicates inputs, adapter parameters, and gradients. Furthermore, the server communicates input representations that can be intercepted by a third party.

### Label Leakage of Standard Split Learning

In Figure 1, we train a DeBERTa-v2-XXL model on the SST-2 [32] sentiment classification dataset. The top row depicts the gradients \(g_{h}\) communicated by the client when calling \(\text{backprop}(\cdot)\) at different training stages. In the bottom row, we similarly track activations \(h(x,\theta)\) that server may compute based on the specified \(x,\theta\). We defer further additional figures and details to Section 4.1.

As we can see, both gradients and activations are arranged in such a way that simple k-means clustering would reveal which objects have the same label. The training activations (bottom row) do

Figure 1: A visualization of top-2 principal components of gradients (top) and activations (bottom) from different fine-tuning steps (left to right). Color indicates the training labels (binary).

not reveal labels right away (at least not against this attack). However, they gradually "leak" private label information during training. Informally, it appears that the training gradients gradually pull apart the feature representations for each label, until eventually they turn into separate clusters. From an information-theoretic perspective, knowing just one vector of gradients _or_ trained activations allows the attacker to learn all but one bit3 of information about client's private labels.

Footnote 3: The missing bit corresponds to attacker not knowing which cluster corresponds to label “1”.

To summarize, leaving any _one_ data source unprotected (gradients, activations or parameters) would already compromise label privacy. However, we found that gradients and activations require different means of protection.

### Privacy-preserving backpropagation

In this section, we formulate an algorithm for "anonymizing" the gradients communicated over a single training step with arbitrary PEFT type. Several prior works approach this by modifying the training objective or model architecture. However, when dealing with a real-world PEFT workload with optimized hyperparameters, changing the model or loss function often results in reduced model accuracy4. Thus, we seek an algorithm that preserves both model and training objective.

Footnote 4: We validate this empirically in 4.2.

We design our algorithm based on an observation that **backpropagation is conditionally linear in output gradients**, even when the model itself is nonlinear. Formally, if we take a model \(h(\cdot,\cdot)\), a fixed set of trainable parameters \(\theta\) and input samples \(x\), the backprop function5 computes backprop\((x,\theta,\frac{\partial L}{\partial h(x,\theta)})=\frac{\partial L}{ \partial\theta}\). For convenience, we shorten it to backprop\((x,\theta,g_{h})=g_{\theta}\), where \(g_{h}=\frac{\partial L}{\partial h(x,\theta)}\) represents the gradients of some objective function with respect to model activations (outputs), and \(g_{\theta}=\frac{\partial L}{\partial\theta}\) are gradients of the same objective function w.r.t. trainable parameters. In this notation, backprop is linear in terms of \(g_{h}\) for any fixed \(x,\theta\).

Footnote 5: This is the same as the backprop API defined in Section 3.1.

This becomes self-evident if we view backprop as multiplying \(g_{h}\) by the Jacobian of model outputs w.r.t. trainable parameters, \(\frac{\partial h(x,\theta)}{\partial\theta}\). If \(x,\theta\) are constant, the Jacobian is also constant, and backprop is a linear operator:

\[\text{backprop}(x,\theta,\frac{\partial L}{\partial h(x,\theta)})=\frac{ \partial L}{\partial\theta}=\frac{\partial L}{\partial h(x,\theta)}\times \frac{\partial h(x,\theta)}{\partial\theta}.\] (1)

This observation allows us to design a private backpropagation protocol. To illustrate this protocol, let us first consider a distributed API with two identical independent servers that offer backprop API. Then, for arbitrary vector \(z\), we can rewrite \(\text{backprop}(x,\theta,g_{h})\) as \(\text{backprop}(x,\theta,g_{h}+z)+\text{backprop}(x,\theta,g_{h}-z)\).

During API fine-tuning, we obtain \(\text{backprop}(x,\theta,g_{h}+z)\) using an API call to server 1, whereas the second term \(\text{backprop}(x,\theta,g_{h}-z)\) translates to an API call to server 2. Note that neither of two servers has access to the true gradient \(g_{h}\): they only receive the sum \([g_{h}+z]\). If we sample a large noise vector \(z\) (\(\text{Var}(z)\gg\|g_{h}\|_{2}^{2}\)), this sum also becomes dominated by noise. However, when both API calls finish, a client can sum the results to recover the true gradient of the loss with respect to parameters.

If both requests are processed by the same server, it can obviously recover \(g_{h}\) by adding up gradients from both calls, which leads us to the final step. Instead of generating a single noise vector, a client

Figure 2: An intuitive illustration of the proposed fine-tuning protocol.

needs to generate (privately) a set of \(m>1\) random vectors \(\hat{g}^{1}_{h},\ldots,\hat{g}^{m}_{h}\) and scalars \(\alpha_{1},\ldots,\alpha_{m}\) such that

\[g_{h}=\sum_{i=1}^{m}\alpha_{i}\cdot\hat{g}^{i}_{h}.\] (2)

Then, for each \(\hat{g}^{i}_{h}\), client computes \(\text{backprop}(x,\theta,\hat{g}^{i}_{h})\) as \(m\) parallel API calls. Once this is done, client recovers

\[g_{\theta}=\sum_{i=1}^{m}\alpha_{i}\cdot\text{backprop}(x,\theta,\hat{g}^{i}_{ h}).\] (3)

Note that the client does not reveal \(\alpha_{1},\ldots,\alpha_{m}\) to anyone.

The resulting procedure is formulated in Algorithm 1. This algorithm is conceptually similar to the secure aggregation protocol for conventional (horizontal) federated learning [1]. This protocol allows clients to average their local vector with peers while keeping each individual vector provably private. Similarly to our scheme, clients perturb the vector in such a way that the average of perturbed vectors remains the same. Unlike Bonawitz et al. [1], our protocol privately backpropagates through a server-hosted model by leveraging the conditional linearity of the backpropagation operator.

```
1:Input:\(x\) inputs, \(\theta\) adapter weights, \(g_{h}\) gradients w.r.t. activations, \(m>1\) - number of passes
2:\(\hat{g}^{1}_{h},\ldots,\hat{g}^{m}_{h},\alpha_{1},\ldots,\alpha_{m}=\text{ obfuscate}(g_{h},m)\)\(\triangleright\) 2
3:for\(j=1,\ldots,m\)do
4:\(\hat{g}^{j}_{\theta}=\text{backprop}(x,\theta,\hat{g}^{j}_{h})\)\(\triangleright\) computed by server
5:endfor
6:\(g_{\theta}=\sum_{j=1}^{m}\alpha_{j}\cdot\hat{g}^{j}_{\theta}\)
7:Return:\(g_{\theta}\) ```

**Algorithm 1** private_backprop -- Privacy-Preserving Backpropagation (from the client's perspective)

The private backpropagation algorithm can allow client to safely compute gradients _once_, but, in practice, client usually needs to run many consecutive steps. This creates an additional vector of attack: if the same server receives two sets of parameters \(\theta_{t},\theta_{t+1}\), they could potentially recover \(g_{\theta}\) by inverting the optimizer.

In the simplest case, if the server somehow knows that the client computes \(\theta_{t+1}=\theta_{t}-\eta\cdot g_{\theta}\), then they can compute \(g_{\theta}=(\theta_{t}-\theta_{t+1})/\eta\). While \(g_{\theta}\) does not necessarily leak private labels, a server could, in some cases, use \(g_{\theta}\) to recover \(g_{h}\), either fully (e.g. if Jacobian is invertible), or partially.

The client has two ways to prevent this attack. The first one is to ensure that no single server runs backprop on two consecutive steps. This is easy to do in decentralized systems where there are many potential servers. However, even when there is a single server, they could be required to set up multiple trusted execution environments [25]. A more risky alternative is to ensure that the gradients cannot be reversed from consecutive parameters: randomize initial optimizer statistics or add noise to parameters. This solution is easier, but it can slow down training in some cases.

To summarize, we formulated a procedure that allows a client to compute gradients privately for any given model and PEFT type. Furthermore, since Equation 3 recovers true gradients, this obfuscation method does not affect the training dynamics. However, as we have shown in Section 3.1, gradients are not the only source of privacy leakage.

### Full fine-tuning

The other major attack vector are training activations. As the model fits to training data, it's intermediate activations \(h(x,\theta)\) allow attackers to recover labels, e.g. by clustering (see Figure 1). To combat this issue, we take advantage of the fact that PEFT has few trainable parameters. Instead of learning just one set of trainable parameters, a client creates \(n\) independent adapter sets \(\theta_{1},...,\theta_{n}\). Note that this does not require \(n\) unique servers: a single server can run multiple sets of adapters. Furthermore, a client can alternate between using different servers for the same adapters. During forward pass, the outputs of different adapters are mixed together using randomized mixing weights \(W\in\mathcal{R}^{n,d}\):

\[h^{\prime}(x,\theta_{1},\ldots,\theta_{n})=\sum_{i=1}^{n}W_{i}\odot h(x,\theta _{i})\] (4)Overall, we design this model in such a way the combined model \(h^{\prime}\) can predict the labels, but the adapters \(h(x,\theta_{i})\) do not allow predicting these labels without knowing the mixing weights W. The mixing weights are generated such that initial activations \(h^{\prime}(x,\dots)\) are equal to mean \(h(x,\cdot)\) for all \(x\). To achieve this, we generate W as follows: first, we generate \(n\cdot(n-1)/2\) d-dimensional random vectors \(\xi_{i,j}\in\mathcal{R}^{d}\forall i\in[1,n],j\in[i+1,n]\). Then, we add them up in the following way:

\[W=\left(\begin{array}{c}\frac{1}{n}e+\xi_{1,2}+\xi_{1,3}+\dots+\xi_{1,n}\\ -\xi_{1,2}+\frac{1}{n}e+\xi_{2,3}+\dots+\xi_{2,n}\\ \dots\\ -\xi_{1,n}-\xi_{2,n}-\xi_{3,n}-\dots+\frac{1}{n}e\end{array}\right)\] (5)

Here, \(e\) stands for a vector of all ones. The purpose of these mixing weights is to ensure that the gradients w.r.t. individual \(h(x,\theta_{i})\) are obfuscated, but the averaged model behaves the same as regular PEFT adapter. To illustrate this, consider \(n{=}2\) identical LoRA adapters \(\theta_{1},\theta_{2}\). During the first training step \(h(x,\theta_{1})=h(x,\theta_{2})\). Therefore,

\[h^{\prime}(x,\theta_{1},\dots,\theta_{n})=(1/2e+\xi_{1,2})\odot h(x,\theta_{1 })+(1/2e-\xi_{1,2})\odot h(x,\theta_{2})=h(x,\theta_{1})\] (6)

However, the two adapters will learn different functions as they receive different gradients. From the first update on, \(h^{\prime}\) will be equal to an average of adapter predictions.

Finally, to ensure that individual adapters \(h(x,\theta)\) do not accidentally "learn to leak" labels, we maintain this over the course of training with a privacy regularizer inspired by [9]. This ensures that it is impossible to predict labels from individual adapters \(h(x,\theta_{i})\). Intuitively, on each training step, client fits \(n\) linear "heads" that learn to predict labels \(y\) from \(h(x,\theta_{i})\), then performs an adversarial update of \(\theta_{i}\) to prevent the "head" from predicting \(y\). Formally, each of \(n\) "heads" minimize the same objective function as the full model. For instance, if the full model solves multi-class classification, each head is trained to minimize cross-entropy:

\[\eta_{i}^{*}=\operatorname*{arg\,min}_{\eta_{i}}\sum_{x,y\in D}-y\cdot\log \frac{e^{\langle\eta_{ij},h(x,\theta_{i})\rangle}}{\sum_{k}e^{\langle\eta_{ik },h(x,\theta_{i})\rangle}},\] (7)

where y is one-hot encoding of the correct class.

The whole adversarial update takes place locally on client's side, using the same \(h(x,\theta)\) it uses for the main training objective. The resulting procedure appears complicated but it typically takes negligible time compared to running the large pre-trainied model \(h(x,\theta)\). Furthermore, since adversarial "heads" are linear, minimizing the objective above is done with standard logistic regression solver.

To summarize, our approach combines the two proposed ideas: we use the private backpropagation algorithm from Section 3.3 to protect the gradients, then trains a mixture of adapters in such a way that obfuscates learned activatons leaking labels. The resulting procedure is described in Algorithm 2. In the next section, we will evaluate the efficacy of P\({}^{3}\)EFT on popular NLP benchmarks.

## 4 Experiments

The main goal of our study is to find a practical method of private fine-tuning that would scale to large models. Because our approach leverages parameter-efficient fine-tuning techniques, we evaluate P\({}^{3}\)EFT with fine-tuning Transformer models on popular NLP benchmarks that these techniques were designed for.

To that end, we chose three pre-trained models: DeBERTa-XXLarge [13], Flan-T5-Large [4] and LLaMA-2 7B [35]. We train these models on several datasets from the GLUE benchmark [39]: SST-2 [32], MNLI [41] and QNLI.

### Privacy of gradients and activations

For this experiment, we train DeBERTa-XXLarge on SST-2 dataset using LoRA adapters with hyperparameters from [15]. First, we train the model locally and track model activations \(h\) and gradients w.r.t. those activations. We apply principal component analysis to them and plot the first 2 dimensions in Figure 1. Similarly, we visualize gradients of individual per-sample loss functions w.r.t. LoRA parameters \(\theta\) in Figure 3 (top row). The results suggest that a hypothetical attacker could easily recover private labels by performing K-Means clustering over any data source: activations, gradients with respect to activations, or individual gradients with respect to parameters.

Next, we run the same experiment using privacy-preserving backpropagation as defined in Section 3.3. We use \(n=2\) with the noise variance set to 1000. As expected, we observed the same learning curve as with normal training. However, instead of sending gradients w.r.t. activations to the server, a client uses specially crafted random noise vectors that are not informative. In Figure 3 (bottom) we plot the same kind of individual gradients as in the top row, except that we visualize the gradients computed by the first of the two servers. Finally, we train XGBoost [3] with default hyperparameters to predict labels given the noisy gradients (pre-PCA): the resulting classifier is able to fit the training data perfectly, but has at most \(50.4\%\) accuracy on a balanced test set.

### Main fine-tuning experiments

Next, we evaluated the entire P3EFT algorithm. To control tasks and model type, we examined DeBERTa and Flan-T5 across all four datasets mentioned above, in addition to evaluating LLaMA on SST2 and QNLI datasets. For each setup, we compare against three baselines:

* **Without LoRAs.** In this baseline, the client gathers \(h\) activations at the beginning (with no adapters), then proceeds to train local "head" layers using these activations. This method cannot leak information about training labels except for what is stored in X.
* **Regular fine-tuning (Regular FT)** refers to training a single LoRA adapter normally. This baseline represents an upper bound on model accuracy, but lacks privacy.
* **Distance Correlation (DC).** Our re-implementation of the distance correlation defense formulated in [33] for Transformer models.

For each algorithm, we evaluated a task-specific metric (accuracy or F1), as well as the privacy leakage value for the 3 following measures:

* **Spectral attack AUC** -- a measure of vulnerability to an attack proposed in [33], measured as classifier ROC AUC: lower value corresponds to better privacy.
* **Norm attack AUC** -- vulnerability to a variant of attack proposed in [18], measured as classifier ROC AUC (lower is better). Despite the initial proposal of this approach for attacking gradients, we observed that it is also well-suited for attacking activations.
* **K-means accuracy** -- vulnerability to clusterization attack, measured in the percentage of correctly clustered activations, lower is better.

For all setups, we report the worst (least private) value among these metrics throughout the entire training period as a measure of privacy leakage, because it is the worst possible scenario that matters from the client's perspective. For DC and P\({}^{3}\)EFT, we specify the values for the best configuration in terms of the utility-privacy trade-off. See details in Appendix A. We also report adjusted standard deviations for the two privacy aware algorithms: P\({}^{3}\)EFT and DC. To do so, we run the full training procedure from scratch with 3 random seeds.

Figure 3: Gradients of cross-entropy w.r.t. LoRA parameters for DeBERTa-v2-XXLarge. The top row corresponds to normal backpropagation and the bottom row uses privacy-preserving backprop.

The results for DeBERTa are presented in Table 1. To improve reproducibility, we reuse the hyperparameters from original paper, with the exception of the LoRA dropout value. We disable dropout because it interferes with the mixing weights (5). In preliminary experiments, we observed that with dropout enabled, both our algorithm and DC begin to perform significantly worse.

We use \(n=2\) adapter sets for P\({}^{3}\)EFT for all datasets and adhered to the same approach for the other models as well. Overall, P\({}^{3}\)FT achieves nearly the same accuracy as traditional (non-private) fine-tuning, outperforming the DC-based algorithm in terms of accuracy given the same privacy level. On the MNLI dataset, we could not find the hyperparameters for DC that ensure stable training while maintaining privacy. Meanwhile, P\({}^{3}\)EFT maintains consistent performance on this task with a slight drop in quality.

Table 2 a reports evaluation for the Flan-T5 base model[4]. For this model, we adapt the exact same hyperparameters as in the previous evaluation with DeBERTa-XXLarge. Compared to DeBERTa, these results are more closely matched. Both both our algorothm and DC consistently solve all three tasks, but P\({}^{3}\)EFT slightly outperforms DC in terms of privacy.

To evaluate how our algorithm scales to larger models, we also fine-tune Llama-2 7B [35] on SST2 [32] and QNLI [39] datasets. For these evaluations, we use LoRA hyperparameters that Hu et al. [15] used when fine-tuning GPT-3, with several changes inspired by Dettmers et al. [5]. Namely, we use the NF4 weight format, apply LoRA to both attention and MLP layers with rank 16. We fine-tune both tasks with maximum context length of 512 and weight decay 0.01. Table 3 summarizes our results: for QNLI, P\({}^{3}\)EFT achieves somewhat better privacy-accuracy trade-off. On SST2, P\({}^{3}\)EFT shows similarly favorable trade-offs while DC struggles to preserve privacy.

## 5 Conclusion and Discussion

In this work, we analyze privacy-preserving fine-tuning of large neural networks in the context of parameter-efficient fine-tuning and the two-party split learning setting. We show that while standard fine-tuning suffers from label leakage even in the parameter-efficient case, it is possible to leverage the efficiency of PEFT to alter the procedure without any significant performance drawbacks. We test the resulting method, named P\({}^{3}\)EFT, on a range of pretrained language models and multiple datasets, showing that it is competitive with a strong baseline in terms of label privacy while having higher task performance.

In future work, it is natural to explore how this approach can be extended to establish holistic privacy in both labels and inputs. This problem can be approached from two directions: either adapt the ideas of P\({}^{3}\)EFT for input privacy, or combine it with an existing work like [22]. Another important direction for future research is exploring the privacy of the long-term client-provider interaction. In a typical real-world use case of API fine-tuning, a client performs multiple training runs on overlapping data and hyperparameters. This could open additional attacks vectors that combine information from multiple training runs.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Without} & Regular & \multirow{2}{*}{DC} & \multirow{2}{*}{P\({}^{3}\)EFT} \\  & & LoRAs & & FT & \\ \hline \multirow{2}{*}{SST2} & \(\begin{array}{c}acc\\ leak\end{array}\) & 82.9 & **96.9** & 96.6\({}_{\pm 0.4}\) & 96.5\({}_{\pm 0.2}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **53.9** & 99.1 & 93.3\({}_{\pm 6.8}\) & 62.6\({}_{\pm 2.6}\) \\ \hline \multirow{2}{*}{QNLI} & \(\begin{array}{c}acc\\ leak\end{array}\) & 72.6 & **96.0** & 95.8\({}_{\pm 0.3}\) & 95.6\({}_{\pm 0.5}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **51.5** & 99.1 & 85.0\({}_{\pm 11.6}\) & 74.6\({}_{\pm 11.1}\) \\ \hline \multirow{2}{*}{MNLI} & \(\begin{array}{c}acc\\ leak\end{array}\) & 49.2 & **91.9** & — & 86.9\({}_{\pm 0.5}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **34.2** & 91.5 & — & 37.4\({}_{\pm 0.7}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Accuracy and privacy metrics.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Without} & Regular & \multirow{2}{*}{DC} & \multirow{2}{*}{P\({}^{3}\)EFT} \\  & & LoRAs & & FT & \\ \hline \multirow{2}{*}{SST2} & \(\begin{array}{c}acc\\ leak\end{array}\) & 82.9 & **96.1** & 95.0\({}_{\pm 0.1}\) & **96.1\({}_{\pm 0.1}\)** \\  & \(\begin{array}{c}leak\\ \end{array}\) & **55.8** & 98.3 & 68.1\({}_{\pm 5.0}\) & 74.1\({}_{\pm 3.0}\) \\ \hline \multirow{2}{*}{QNLI} & \(\begin{array}{c}acc\\ leak\end{array}\) & 83.2 & **95.3** & 95.2\({}_{\pm 0.1}\) & 94.7\({}_{\pm 0.0}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **58.7** & 98.9 & 67.0\({}_{\pm 1.2}\) & 63.0\({}_{\pm 0.8}\) \\ \hline \multirow{2}{*}{MNLI} & \(\begin{array}{c}acc\\ leak\end{array}\) & 73.9 & **90.5** & 89.8\({}_{\pm 0.1}\) & 90.1\({}_{\pm 0.1}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **34.6** & 85.9 & 45.6\({}_{\pm 0.8}\) & 40.0\({}_{\pm 1.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Accuracy and privacy metrics.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{Without} & Regular & \multirow{2}{*}{DC} & \multirow{2}{*}{P\({}^{3}\)EFT} \\  & & LoRAs & & FT & \\ \hline \multirow{2}{*}{SST2} & \(\begin{array}{c}acc\\ leak\end{array}\) & 94.6 & **97.4** & 97.1\({}_{\pm 0.1}\) & 95.8\({}_{\pm 0.1}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **59.1** & 99.3 & 83.6\({}_{\pm 0.6}\) & 68.9\({}_{\pm 2.6}\) \\ \hline \multirow{2}{*}{QNLI} & \(\begin{array}{c}acc\\ leak\end{array}\) & 77.0 & 95.0 & **95.2\({}_{\pm 0.1}\)** & 94.7\({}_{\pm 0.2}\) \\  & \(\begin{array}{c}leak\\ \end{array}\) & **53.3** & 85.5 & 66.6\({}_{\pm 4.1}\) & 62.9\({}_{\pm 0.8}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Accuracy and privacy metrics for LLaMA-2 7B.

## References

* Bonawitz et al. [2017] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-preserving machine learning. In _proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security_, pages 1175-1191, 2017.
* Borzunov et al. [2022] Alexander Borzunov, Dmitry Baranchuk, Tim Dettmers, Max Ryabinin, Younes Belkada, Artem Chumachenko, Pavel Samygin, and Colin Raffel. Petals: Collaborative inference and fine-tuning of large models. _arXiv preprint arXiv:2209.01188_, 2022. URL https://arxiv.org/abs/2209.01188.
* Chen and Guestrin [2016] Tianqi Chen and Carlos Guestrin. XGBoost: A scalable tree boosting system. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, KDD '16, pages 785-794, New York, NY, USA, 2016. ACM. ISBN 978-1-4503-4232-2. doi: 10.1145/2939672.2939785. URL http://doi.acm.org/10.1145/2939672.2939785.
* Chung et al. [2022] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022. URL https://arxiv.org/abs/2210.11416.
* Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. _arXiv preprint arXiv:2305.14314_, 2023.
* Easily finetune Stable Diffusion and generate customised AI images -
- dreamboothapi.ai. https://dreamboothapi.ai/, 2023. [Accessed 28-09-2023].
* Duan et al. [2023] Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic parrots: Differentially private prompt learning for large language models. _arXiv preprint arXiv:2305.15594_, 2023.
* Dwork [2006] Cynthia Dwork. Differential privacy. In _International colloquium on automata, languages, and programming_, pages 1-12. Springer, 2006.
* Ganin and Lempitsky [2015] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 1180-1189, Lille, France, 07-09 Jul 2015. PMLR. URL https://proceedings.mlr.press/v37/ganin15.html.
* Gupta and Raskar [2018] Otkrist Gupta and Ramesh Raskar. Distributed learning of deep neural network over multiple agents. _Journal of Network and Computer Applications_, 116:1-8, 2018. ISSN 1084-8045. doi: https://doi.org/10.1016/j.jnca.2018.05.003. URL https://www.sciencedirect.com/science/article/pii/S1084804518301590.
* Hambardzumyan et al. [2021] Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. WARP: Word-level Adversarial ReProgramming. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4921-4933, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.381. URL https://aclanthology.org/2021.acl-long.381.
* Hardy et al. [2017] Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini, Guillaume Smith, and Brian Thorne. Private federated learning on vertically partitioned data via entity resolution and additively homomorphic encryption, 2017.
* He et al. [2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=XPZIaotutsD.

* Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2790-2799. PMLR, 09-15 Jun 2019. URL https://proceedings.mlr.press/v97/houlsby19a.html.
* Hu et al. [2022] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
* Face [2023] Hugging Face. AutoTrain -- huggingface.co. https://huggingface.co/autotrain, 2023. [Accessed 28-09-2023].
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Li et al. [2022] Oscar Li, Jiankai Sun, Xin Yang, Weihao Gao, Hongyi Zhang, Junyuan Xie, Virginia Smith, and Chong Wang. Label leakage and protection in two-party split learning. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=c0tBRgsf2f0.
* Li et al. [2021] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and Bingsheng He. A survey on federated learning systems: Vision, hype and reality for data privacy and protection. _IEEE Transactions on Knowledge and Data Engineering_, 2021.
* Li et al. [2023] Shen Li, Pritam Damania, Luca Wehrstedt, and Rohan Varma. PyTorch RPC: Distributed Deep Learning Built on Tensor-Optimized Remote Procedure Calls. In _Proceedings of Machine Learning and Systems 5 (MLsys)_, 2023.
* Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 4582-4597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353. URL https://aclanthology.org/2021.acl-long.353.
* Li et al. [2023] Yansong Li, Zhixing Tan, and Yang Liu. Privacy-preserving prompt tuning for large language model services. _ArXiv_, abs/2305.06212, 2023. URL https://api.semanticscholar.org/CorpusID:258588141.
* Liu et al. [2023] Xiao-Yang Liu, Rongyi Zhu, Daochen Zha, Jiechao Gao, Shan Zhong, and Meikang Qiu. Differentially private low-rank adaptation of large language model using federated learning. _arXiv preprint arXiv:2312.17493_, 2023.
* McMahan et al. [2017] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and Jerry Zhu, editors, _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics_, volume 54 of _Proceedings of Machine Learning Research_, pages 1273-1282. PMLR, 20-22 Apr 2017. URL https://proceedings.mlr.press/v54/mcmahan17a.html.
* Nvidia [2023] Nvidia. Nvidia confidential computing. https://www.nvidia.com/en-us/data-center/solutions/confidential-computing, 2023. [Accessed 28-09-2023].
* [26] OctoAI. Fine-tuning Stable Diffusion -- docs.octoai.cloud. https://docs.octoai.cloud/docs/fine-tuning-stable-diffusion, 2023. [Accessed 28-09-2023].
* OpenAI [2023] OpenAI. OpenAI Platform -- platform.openai.com. https://platform.openai.com/docs/guides/fine-tuning, 2023. [Accessed 28-09-2023].

* Pasquini et al. [2021] Dario Pasquini, Giuseppe Ateniese, and Massimo Bernaschi. Unleashing the tiger: Inference attacks on split learning. In _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, CCS '21, page 2113-2129, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450384544. doi: 10.1145/3460120.3485259. URL https://doi.org/10.1145/3460120.3485259.
* Pfeiffer et al. [2021] Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckle, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning, 2021.
* Rao et al. [2021] Yuma Rao, Jacob Steeves, Ala Shaabana, Daniel Attevelt, and Matthew McAteer. Bittensor: A peer-to-peer intelligence market, 2021.
* Shi et al. [2022] Weiyan Shi, Ryan Shea, Si Chen, Chiyuan Zhang, Ruoxi Jia, and Zhou Yu. Just fine-tune twice: Selective differential privacy for large language models. _arXiv preprint arXiv:2204.07667_, 2022.
* Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1631-1642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/D13-1170.
* Sun et al. [2022] Jiankai Sun, Xin Yang, Yuanshun Yao, and Chong Wang. Label leakage and protection from forward embedding in vertical federated learning. _arXiv preprint arXiv:2203.01451_, 2022.
* ICANN 2018_, pages 270-279, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01424-7.
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
* Vepakomma et al. [2018] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. Split learning for health: Distributed deep learning without sharing raw patient data, 2018.
* Vepakomma et al. [2019] Praneeth Vepakomma, Otkrist Gupta, Abhimanyu Dubey, and Ramesh Raskar. Reducing leakage in distributed deep learning for sensitive health data. 05 2019.
* Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* Wang et al. [2023] Yiming Wang, Yu Lin, Xiaodong Zeng, and Guannan Zhang. Privatelora for efficient privacy preserving llm. _arXiv preprint arXiv:2311.14030_, 2023.
* Williams et al. [2017] Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. _arXiv preprint arXiv:1704.05426_, 2017.
* Xiao et al. [2023] Guangxuan Xiao, Ji Lin, and Song Han. Offsite-tuning: Transfer learning without full model. _arXiv preprint arXiv:2302.04870_, 2023.
* Yang et al. [2019] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. _ACM Trans. Intell. Syst. Technol._, 10(2), jan 2019. ISSN 2157-6904. doi: 10.1145/3298981. URL https://doi.org/10.1145/3298981* Yu et al. [2022] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially private fine-tuning of language models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=Q42f0dfjEC0.
* Zhang et al. [2023] Zhuo Zhang, Yuanhang Yang, Yong Dai, Qifan Wang, Yue Yu, Lizhen Qu, and Zenglin Xu. FedPETuning: When federated learning meets the parameter-efficient tuning methods of pre-trained language models. In _Findings of the Association for Computational Linguistics: ACL 2023_, pages 9963-9977, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.632. URL https://aclanthology.org/2023.findings-acl.632.
* Zhao et al. [2023] Haodong Zhao, Wei Du, Fangqi Li, Peixuan Li, and Gongshen Liu. Fedprompt: Communication-efficient and privacy preserving prompt tuning in federated learning, 2023.

## Appendix A Hyperparameters search

In P\({}^{3}\)EFT and Distance Correlation methods resulting loss \(L\) function can be viewed in the form

\[L=L_{m}+\alpha\cdot L_{r},\]

where \(L_{m}\) - main task loss, \(L_{r}\) - regularizer and \(\alpha\) is a coefficient that controls the tradeoff between these two losses. The selection of this coefficient affects the final performance of the model. Therefore, to find the best configurations for both methods, we iterated through this hyperparameter using a grid search.

We started with \(\alpha=1\) and then altered it with a multiplicative step of \(10^{\frac{1}{2}}\). Values were discarded if the quality did not exceed that achieved by solely training the classifier without LoRA. This criterion was adopted because such outcomes would suggest the method's inability to outperform training scenarios in which the server does not engage with the labels whatsoever. Additionally, we excluded values that led to unstable training. By this, we mean instances where, although the model initially trained on the primary task, at some point, the regularizer began contributing significantly more, and the utility value dropped to the starting value. We observed this issue for the DC method with DeBERTa on the MNLI. From the remaining values, we aimed to choose the one that offered the lowest privacy leakage. The final hyperparameter values for P\({}^{3}\)EFT can be found in the Table 4 and for DC in the Table 5.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & SST2 & QNLI & MNLI \\ \hline DeBERTa XXLarge & 1 & 1 & 1 \\ \hline Flan-T5-Large & -1 & 1 & 1 \\ \hline LLaMA-2 7B & 0 & 0 & — \\ \hline \hline \end{tabular}
\end{table}
Table 4: Regularization parameter \(\alpha\) for the P\({}^{3}\)EFT method. The values in the table represent powers of the \(10^{\frac{1}{2}}\).

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & SST2 & QNLI & MNLI \\ \hline DeBERTa XXLarge & 0 & -1 & — \\ \hline Flan-T5-Large & 2 & -1 & 0 \\ \hline LLaMA-2 7B & -1 & -1 & — \\ \hline \hline \end{tabular}
\end{table}
Table 5: Regularization parameter \(\alpha\) for the DC method. The values in the table represent powers of the \(10^{\frac{1}{2}}\).

Formal algorithm definition

Below, we define the full P\({}^{3}\)EFT algorithm. In Algorithm 2, main_loss is the task-specific objective e.g. cross-entropy; reg_loss is the adversarial regularizer described in Section 3.4. We denote client-side model "head" as \(f(h,\psi^{t})\), where \(\psi\) represent trainable head parameters. Finally, opt_step function performs a single gradient descent step with a task-specific optimizer, typically Adam [17].

```
1:Input: dataset \(D=\{X,Y\}\), \(n>1\) number of adapters, \(\alpha\geq 0\) - regularizing weight, \(m>1\) number of obfuscated gradients
2:Initialize head \(\psi^{0}\), mixing weights \(W_{i}\) and adapters \(\theta_{i}^{0},i=\overline{1,n}\)
3:for\(t=0,1,\ldots,T-1\)do
4: Sample batch \(\{x^{t},y^{t}\}\)
5:for\(i=1,\ldots,n\)do
6:\(h_{i}^{t}=h(x^{t},\theta_{i}^{t})\)\(\triangleright\) by server
7:\(l_{i}=\text{reg\_loss}(h_{i}^{t},y^{t})\)\(\triangleright\) by client
8:endfor
9:\(h^{\prime}=\sum_{i=1}^{n}W_{i}\odot h_{i}^{t}\)
10:\(l=\text{main\_loss}(f(h^{\prime},\psi^{t}),y^{t})\)
11:\(L=l+\alpha\cdot\sum_{i=1}^{n}l_{i}\)
12:for\(i=1,\ldots,n\)do
13:\(g_{h}=\partial L/\partial h_{i}^{t}\)\(\triangleright\) Client performs partial backprop
14:\(g_{i}^{t}=\text{private\_backprop}(x,\theta_{i}^{t},g_{h},m)\)
15:\(\theta_{i}^{t+1}=\text{opt\_step}(\theta_{i}^{t},g_{i}^{t},t)\)
16:endfor
17:\(\psi^{t+1}=\text{opt\_step}(\psi^{t},\partial l/\partial\psi^{t},t)\)
18:endfor
19:Return:\(\psi^{T},\theta_{1}^{T},\ldots,\theta_{M}^{T}\) ```

**Algorithm 2** P\({}^{3}\)EFT - full training algorithm

## Appendix C Informal description of LoRA fine-tuning

For convenience, we provide a brief summary of fine-tuning with LoRA [15]. This PEFT method was originally designed for fine-tuning large pre-trained language models on downstream NLP tasks. These language models are typically based on the Transformer architecture [36], where most trainable parameters are allocated to linear layers in multi-head self-attention and feedforward blocks.

In the first stage of LoRA fine-tuning, user augments the model with adapters. To do so, a user goes over linear layers in transformer blocks and adds two trainable matrices, \(A\) and \(B\) that affect this layer's forward pass. Let \(W_{i}\times x+b_{i}\) be the original layer with \(n\) inputs and \(m\) hidden units. Here, \(W_{i}\in\mathcal{R}^{m\times n}\) is a pre-trained weight matrix, \(b_{i}\in\mathcal{R}^{m}\) is a pre-trained intercept vector and \(x\in\mathcal{R}^{n}\) represents a vector of inputs to this particular layer. During the forward pass, a layer with LoRA adapter computes \(W_{i}\times x+b_{i}+B_{i}\times A_{i}\times x\), or equivalently, \((W_{i}+B\times A)\times x+b_{i}\). Here, \(A_{i}\) and \(B_{i}\) are two newly added matrices that constitute a LoRA adapter.

The adapter matrices \(A\in\mathcal{R}^{r\times n}\) and \(B\in\mathcal{R}^{m\times r}\) have a very small intermediate dimension \(r\). For instance, when training GPT-3 with LoRA adapters, authors use \(1\leq r\leq 64\), whereas the main weight dimensions are \(m=n=12288\). The first matrix \(A\) is initialized with small random normal values, and the second matrix \(B\) is initialized at zeros. That way, initial \(A\) and \(B\) do not affect the model predictions.

Once all adapters are initilized, the user trains all \(A_{i}\) and \(B_{i}\) matrices of the model, while keeping the rest of the weights frozen. This way, only a small action (less than 1%) of model weights are updated. Once the training is over, the learned adapters \(A_{i}\) and \(B_{i}\) can be merged into the main weights (\(W_{i}:=W_{i}+A_{i}\times B_{i}\)) or used separately.

LoRA adapters are designed with two objectives in mind: i) to allow fine-tuning models in limited GPU memory and ii) to allow inferencing many fine-tuned models using one inference server. When fine-tuning, LoRA achieves small memory footprint due to the fact that user does not need to compute gradients (or optimizer statistics) for billions of main model parameters. During inference, a server can keep a library of several adapters for different tasks and swap between them on demand.

## Appendix D Informal description of LoRA fine-tuning

We used NVIDIA A100 GPUs for all the experiments. Experiments with DeBERTA [13] and Flan-T5 [4] on SST2 [32] were conducted on the single GPU, while experiments on MNLI [41] and QNLI require 4 A100. LLaMA-2 [35] expetiments were carried out on the node of 8 A100.

All the experiments last 12-24 hours. However, it is possible to speed up some of them using more GPUs, as well as conduct them on a smaller number of GPUs using technics to save GPU memory (see parameters in our code).

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See our main contributions in 1 and Section 5. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss potential limitations of the method in Section 1, Section 3 and Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: We include no proofs. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe all the technical details in the Section 4 and in the README of our attached code. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We provide attached code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Section 4, Appendix A. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars in main results. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix D. Guidelines: The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We confirm the NeurIPS Code of Ethics Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We describe a potential social impact in Introduction. Guidelines: The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

* Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: We do not describe the safeguards Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licensees for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use open datasets from GLUE benchmark and open-sourced models and cite them in our work. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets**Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?

Answer: [NA]

Justification: We do not release any of the assets

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not involve crowdsourcing. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve research with human subjects Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.