# Learning Superconductivity from Ordered and Disordered Material Structures

 Pin Chen\({}^{1}\) Luoxuan Peng\({}^{1}\) Rui Jiao\({}^{2,3}\) Qing Mo\({}^{1}\) Zhen Wang\({}^{1}\) Wenbing Huang\({}^{4,5}\)

Yang Liu\({}^{2,3}\) Yutong Lu\({}^{1}\)

\({}^{1}\) National Supercomputer Center in Guangzhou,

School of Computer Science and Engineering, Sun Yat-sen University

\({}^{2}\)Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University

\({}^{3}\)Institute for AIR, Tsinghua University

\({}^{4}\)Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{5}\) Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China

Yutong Lu are corresponding authors.

###### Abstract

Superconductivity is a fascinating phenomenon observed in certain materials under certain conditions. However, some critical aspects of it, such as the relationship between superconductivity and materials' chemical/structural features, still need to be understood. Recent successes of data-driven approaches in material science strongly inspire researchers to study this relationship with them, but a corresponding dataset is still lacking. Hence, we present a new dataset for data-driven approaches, namely SuperCon3D, containing both 3D crystal structures and experimental superconducting transition temperature (T\({}_{c}\)) for the first time. Based on SuperCon3D, we propose two deep learning methods for designing high T\({}_{c}\) superconductors. The first is SODNet, a novel equivariant graph attention model for screening known structures, which differs from existing models in incorporating both ordered and disordered geometric content. The second is a diffusion generative model DiffCSP-SC for creating new structures, which enables high T\({}_{c}\)-targeted generation. Extensive experiments demonstrate that both our proposed dataset and models are advantageous for designing new high T\({}_{c}\) superconducting candidates.

## 1 Introduction

The pursuit of high-temperature superconductors is driven by their promising applications in efficient energy transmission, advanced electromagnetics, and quantum computing [6; 36], yet their design is hindered by the enigmatic nature of high-T\(c\) unconventional superconductivity. Although BCS theory [21] aids in predicting T\(c\) for conventional superconductors through first-principles calculations, these methods are computationally demanding and limited to specific materials, necessitating extensive calculations for electron-phonon coupling. Moreover, the intrinsic disorder in many superconductors poses additional challenges for atomic-level design [39]. Such complexities highlight the need for novel approaches in superconductor research and development.

Benefiting from massive public datasets in materials science, data-driven deep learning has been instrumental in predicting material properties [45], synthesizing structures [13], and more. These methods bypass complex physical theories and are crucial in superconductor research, aiding in T\({}_{c}\) prediction models for database analysis [12] and inverse design models for novel structures [60], underscoring deep learning's impact on accelerating superconducting material discovery and design. Specially, Graph Neural Networks (GNN) have been extensively applied to model ordered crystals[61; 16; 12; 64], fewer methods exist for representing disordered crystals [9], despite their prevalence in nature and databases like ICSD, where over 50% of structures are disordered. Therefore, developing methods to represent disordered structures in graphs is vital, especially for superconductivity research where T\({}_{c}\) enhancement often involves doping or applying pressure.

Recently, generative model is widely used in Natural Language Processing (NLP), Computer Vision (CV) and natural science. Inspired by non-equilibrium thermodynamics, Diffusion Models (DM) currently produce State-of-the-Art proteins [55], molecules [25] as well as crystals [62; 27]. However, in the field of crystal structure generation, existing models such as CDVAE utilizes the score matching method for atom coordinates, which does not ensure the translation invariance. DiffCSP focuses on crystal structure prediction tasks, which cannot be applied to design novel periodic materials from scratch.

Given the incomplete understanding of superconducting mechanisms, a data-driven approach shows great promise. Constructing a dataset that captures the structure-to-superconductivity relationship is essential for training AI models aimed at designing superconductors. Hence, we introduce SuperCon3D, a new dataset combining crystal structures and the critical temperature T\({}_{c}\) from SuperCon and ICSD. Utilizing SuperCon3D, we have developed two deep learning models for superconductor discovery and design. We propose a transformer-based GNN, SODNet, to analyze crystal geometries, including both ordered and disordered structures, potentially screening the entire ICSD. SODNet achieves SE(3)-equivariance through irreducible representation-based vector space features. Additionally, we introduce DiffCSP-SC, a transformer-based equivariant diffusion model for inverse design, capable of generating novel high T\({}_{c}\) superconductor candidates.

The main contributions of our work can be summarized as follows:

* A new dataset SuperCon3D containing both ordered-and-disordered crystal structures and experimental superconducting critical temperature is built for the first time.
* We propose two deep learning models to showcase the possible methods for exploring Supercon3D dataset. The experimental result indicate that our proposed models outperform the existing similar methods.
* Based on our proposed models, we present a list of candidate superconductors for future experimental validation. To the best of our knowledge, this is the first report of the candidate superconductors with disordered structures based on GNN methods.

## 2 Related Work

### Superconducting Dataset

The SuperCon database encompasses around 33,000 superconductors, providing only their chemical formulas. Jarvis conducted electron-phonon coupling calculations for 1,058 materials, creating a computational database with BCS superconducting properties [12]. However, BCS theory applies mainly to conventional superconductors, and its predicted T\({}_{c}\) values require experimental validation. The recent S2S dataset includes 1,685 entries with crystal structures and binary superconducting labels for machine learning-based discovery [31], but it's geared towards classification tasks. Diverging from these approaches, we constructed a dataset comprising crystal structures and experimental T\({}_{c}\) values, suited for regression-based deep learning. Additionally, 3DSC [53] is a dataset that includes both T\({}_{c}\) and structural information, comprising over 9,150 data entries obtained through elemental matching and manual doping. In contrast, the data in SuperCon3D is entirely derived from experimental observations in databases.

### Crystal Modeling

Crystals are typically depicted as periodic graphs with a repeating minimum unit cell in a 3D lattice. While various equivariant GNN models have been developed for ordered crystal structures [61; 64; 11], research on representing disordered crystals is limited. Disorder, as defined by Muller et al. [38], involves varied orientations of atoms in unit cells, categorized into substitutional and positional disorder. MEGNet models disordered sites as elemental embeddings' linear combinations [9], only suitable for substitutional disorder. Our work aims to establish a comprehensive method for representing disordered graphs in crystals.

### Generative Models

Drawing on the concepts of non-equilibrium thermodynamics [52], diffusion models create links between data and prior distributions through forward and backward Markov chains [24]. This method has made significant strides in image generation [46; 44]. Leveraging equivariant GNNs, diffusion models efficiently generate samples from invariant distributions, finding applications in conformation generation [51; 63], ab initio molecule design [26], protein generation [33], and more. The adaptation of diffusion models for crystal generation has also gained traction recently [62; 34; 27]. In our research, we enhance diffusion generative modeling by incorporating an attention-based approach, aimed at reverse-engineering novel superconducting structures with a focus on T\({}_{c}\) properties.

## 3 Problem Formulation

### From Ordered to Disordered Structures

We represent a 3D crystal as the infinite periodic arrangement of atoms in 3D space, and the smallest repeating unit is called a _unit cell_, as shown in Fig. 1. A unit cell can be defined as \(\mathcal{M}=(\bm{L},\mathcal{S})\), where \(\bm{L}=[\bm{l}_{1},\bm{l}_{2},\bm{l}_{3}]\in\mathbb{R}^{3\times 3}\) represents a minimum unit cell matrix containing three basic vectors to represent the periodicity of the crystal, and \(\mathcal{S}=\{S_{1},\cdots,S_{N}\}\) denotes a set of \(N\) sites located in the unit cell. Specifically, a _site_ describes a composition located at a specific position, which can be further defined as a triplet \(S_{i}=(\bm{A}_{i},\bm{w}_{i},\bm{x}_{i})\), where \(\bm{A}_{i}=[\bm{a}_{i,1},\cdots,\bm{a}_{i,m_{i}}]\in\mathbb{R}^{m_{i}\times h}\) lists the \(h\)-dimension features of the atom species composing the site, \(\bm{w}_{i}\in\mathbb{R}^{m_{i}}\) describes the occupancy of each specie, and \(\bm{x}_{i}\in\mathbb{R}^{3}\) denotes the Cartesian coordinate of the site. \(m_{i}\) denotes the number of atoms in one site. Generally a crystal structure is composed of ordered sites, where \(m_{i}=1\) and \(\bm{w}_{i}=[1]\), _i.e._ each site is completely formed by a single atom specie. Under the influence of factors such as doping, superconductors may exhibit a disordered structure, containing two kinds of disordered sites:

Figure 1: Illustrations of periodic disorder patterns. The dotted red lines are minimum repeated cells. Grey lines are artificial boundaries to form one possible unit cell that repeats in infinite space for the given crystal. (a)\(\rightarrow\)(b): An illustration of periodic substitutional disorder patterns in 2D space. In this case, a new atomic specie replaces the origin one. (c)\(\rightarrow\)(d): An illustration of periodic positional disorder patterns in 2D space. Here, one site occurs position shift, and break the atomic symmetry in the crystal. The crystals are 3D structures in practice, and we use illustrations in 2D for simplicity.

**Substitutional Disorder (SD)**. As illustrated in Fig. 1(a)\(\rightarrow\)(b), SD involves a situation where the site is occupied by more than one atomic species. Specifically, for an SD site \(S_{i}\), we have

\[\begin{cases}\bm{m}_{i}>1,\\ \bm{a}_{i,1}\neq\bm{a}_{i,2}\neq\cdots\neq\bm{a}_{i,m_{i}},\\ \bm{w}_{i,1}+\bm{w}_{i,2}+\cdots+\bm{w}_{i,m_{i}}=1\end{cases}\] (1)

**Positional Disorder (PD)**. In this case, one atom in the unit cell occurs position shift as shown in Fig. 1(c)\(\rightarrow\)(d). For a PD site \(S_{i}\), the atomic specie \(\bm{a}_{i,1}\) partially locates in \(\bm{x}_{i}\) with its occupancy.

\[\begin{cases}\bm{m}_{i}=1,\\ \bm{w}_{i,1}<1.\end{cases}\] (2)

**SD+PD (SPD)**. Specially, when \(\bm{w}_{i,1}+\bm{w}_{i,2}+\cdots+\bm{w}_{i,m_{i}}<1\) in equation 19, both SD and PD can occur simultaneously.

Typically, there is also the occurrence of interstitial disorder. However, it was not detected in our dataset. Further details are provided in the Appendix B.1.

### Superconducting Candidates Designing

In this study, we define the design of novel superconducting candidates in two ways: the first is "known materials repurposing", where the potential superconducting candidates are screened from known structures. And the second involves designing novel material structures that are potential superconducting candidates. The specific definition of the deep learning task is as follows:

**Superconductivity Prediction Task.** The task involves predicting the T\({}_{c}\) values given the crystal structure \(\mathcal{M}\). Then, we use the predicting models to screen the big structure database to find candidate superconductors with high T\({}_{c}\) value.

**Inverse Superconductor Generation Task.** This task predicts the chemical composition \(\bm{A}\), the Cartesian coordinates \(\bm{X}\), and the lattice matrix \(\bm{L}\) targeted on higher T\({}_{c}\) values. To reduce the exploration space, we set \(\bm{w}_{i}\) = 1 to generate ordered crystals. Such method can potentially design novel high T\({}_{c}\) superconductors.

## 4 The Proposed Method

### SODNet

Regarding the importance of symmetry in 3D physics space, it is essential to respect SE(3)-equivariance conditions in neural networks to reduce the model's dependence on data. To explore

Figure 2: Illustration of graph representation and equivariant graph attention layer in SODNet. (a). Illustration of node and edge embeddings. (b). The Type-0 and Type-L features operations in equivariant graph attention mechanism. \(\bigoplus\) denotes addition and \(\sum\) within a circle stands for summation over all neighbors.

the geometric structures with ordered and disordered graphs in SuperCon3D dataset, we propose SODNet, an effective architecture with SE(3)-equivariant graph attention to exploit 3D geometric content. We establish SE(3)-equivariance by utilizing equivariant features derived from vector spaces containing irreducible representations and trainable equivariant operations with the help of e3nn [19]. The core modules of the proposed SODNet is illustrated in Fig. 2. We elaborate the details as follows.

#### 4.1.1 Disordered Graph Representation

Considering the presence of disordered structures within SuperCon3D, we design two embedding blocks aimed at enhancing the model's ability to effectively capture these disordered inputs.

**Node embedding.** In the graph network approach, we apply the k-hot embedding [10] as the feature vector \(\bm{a}_{i,k}\), which encodes the atomic property corresponding to each atom specie. To extend such scheme to disordered structures, we further represent each site \(S_{i}\) as a linear combination of atomic occupancy and atomic encoding as:

\[\bm{h}_{i}=\begin{cases}\bm{a}_{i,1},&\text{$S_{i}$ is ordered},\\ \sum_{k}\bm{w}_{i,k}\bm{a}_{i,k},&\text{$S_{i}$ is SD or SPD},\\ \bm{w}_{i,1}\bm{a}_{i,1},&\text{$S_{i}$ is PD}.\end{cases}\] (3)

**Edge embedding**. Then, we consider 3D geometric features by incorporating interatomic distance as well as vectors \(\vec{r}_{ij}\) equipped with spherical harmonics as follows:

\[\bm{E} =\bm{w}_{i}\bm{w}_{j}\bm{RBF}(\|\vec{r}_{ij}\|),\] (4) \[\bm{x}_{ij} =\varphi(\bm{h}_{i})+\varphi(\bm{h}_{j}),\] (5) \[\bm{f}_{ij} =\varphi_{f}(\bm{x}_{ij}\otimes_{\mathcal{E}}^{TP}\bm{SH}(\vec{r }_{ij}))\] (6)

where \(\bm{RBF}(\|\vec{r}_{ij}\|)\) is the radial distribution function (RBF) expansion for interatomic bond distance. Specially, we set \(\bm{w}_{i}\) and \(\bm{w}_{j}\) to 1 when \(i\) and \(j\) sites are ordered. The initial edges are constructed by k-nearest neighbor (kNN) methods from Yan et al. [64]. Here, we remove the close edges when bond distance meets \(\|\vec{r}_{ij}\|\leq R_{i}+R_{j}\) to avoid strong interactions caused by disordered sites, where \(R_{i}\) and \(R_{j}\) are atomic radii. \(\bm{x}_{ij}\) combines the features of target node \(i\) and source node \(j\) with linear layers to obtain initial message. \(\varphi\) represents an MLP. \(\bm{SH}(\vec{r}_{ij})\) is spherical harmonics embeddings (SH) of relative position \(\vec{r}_{ij}\), \(c\bm{E}\) is weights parametrized by \(\bm{E}\). Finally, we obtain \(\bm{f}_{ij}\) to derive non-linear messages and attention weights.

#### 4.1.2 Equivariant Graph Attention

Given \(\bm{f}_{ij}\) containing multiple type-L vectors, which are SE(3)-equivariant irreps features. In the context of learning on 3D atomistic graphs, it is essential that features and learnable functions exhibit SE(3)-equivariance with respect to geometric transformations acting on the position \(\vec{r}_{ij}\). We split \(\bm{f}_{ij}\) into \(\bm{f}_{ij}^{L}\) and \(\bm{f}_{ij}^{0}\). The \(\bm{f}_{ij}^{0}\) is scalar and independent on inputs. However, the \(\bm{f}_{ij}^{L}\) consists of type-L vectors, which can break equivariance. Inspired by Liao and Smidt [30], we apply different operations to each group of \(\bm{f}_{ij}\).

**Type-0 features.** Given \(\bm{f}_{ij}^{0}\), we adopt the leaky ReLU activation and a softmax operation for \(\beta_{ij}\):

\[\zeta_{ij} =\alpha^{\top}LeakReLU(\bm{f}_{ij}^{0}),\] (7) \[\beta_{ij} =\frac{exp(\zeta_{ij})}{\sum_{k\in\mathcal{N}(i)}exp(\zeta_{ik})}\] (8)

Where \(\alpha\) is a learnable vector of the same dimension as \(\bm{f}_{ij}^{0}\) and \(\zeta_{ij}\) is a scalar.

**Type-L features.** We perform non-linear transformation on \(\bm{f}_{ij}^{L}\) to obtain non-linear message:

\[\mu_{ij} =Gate(\bm{f}_{ij}^{L}),\] (9) \[\upsilon_{ij} =\varphi_{f}(\mu_{ij}\otimes_{\omega}^{TP}\bm{SH}(\vec{r}_{ij}))\] (10)We apply the equivariant gate activation as Weiler et al. [59] and present the details in Appendix B.2. Then, the similar method is eq. 6 is used to obtain \(v_{ij}\).

Finally, \(\beta_{ij}\) and \(v_{ij}\) are further transformed features into scalars by multiplication operation. We perform mean aggregate over all nodes to predict the T\({}_{c}\) value by:

\[T_{c}(i) =\frac{1}{|\mathcal{N}(i)|}\sum_{j\in\mathcal{N}(i)}\beta_{ij} \cdot v_{ij},\] (11) \[T_{c} =\frac{1}{|\mathcal{V}|}\sum_{i\in\mathcal{V}}T_{c}(i)\] (12)

Where \(\mathcal{N}(i)\) is the neighbors on node \(i\), and \(\mathcal{V}\) denotes the set of all nodes in the graph.

### DiffCSP-SC

Based on DiffCSP [27], we further equip our method with superconductivity guidance for crystal generation. The original DiffCSP proposes a periodic SE(3) equivariant model to jointly optimize lattice matrix \(\bm{L}\) and fractional coordinates \(\bm{F}=\bm{L}^{-1}\bm{X}\) in a diffusion-based framework, and additionally utilizes a time-dependent guidance model [2] for property optimization. Here, \(\bm{X}\) denotes the Cartesian coordinates. We extend DiffCSP with a more powerful architecture for SuperCon3D dataset.

#### 4.2.1 Transformer-based Architecture

The denoising and guidance model of the original DiffCSP share the same architecture, which is built upon EGNN [47], following the standard message passing neural networks (MPNN) framework [20]. To capture the key features related to superconductivity, we employ a transformer-based model for DiffCSP-SC. Let \(\bm{H}^{(s)}=[\bm{h}_{1}^{(s)},\cdots,\bm{h}_{N}^{(s)}]\) denote the node representations in the \(s\)-th layer, where \(N\) is the number of nodes. The input feature is given by \(\bm{h}_{i}^{(0)}=\varphi(f_{\text{atom}}(\bm{a}_{i}),f_{\text{pos}}(t))\), where \(f_{\text{atom}}\) and \(f_{\text{pos}}\) are the atomic embedding and sinusoidal positional encoding [56; 24], respectively. \(\varphi\) is a multi-layer perception (MLP).

The output features \(\bm{h}_{i}^{(s)}\) are computed by

\[\bm{h}_{i}^{(s)}=\bm{h}_{i}^{(s-1)}+\sum_{j=1}^{N}\theta_{ij}^{(s)}\bm{v}_{ij} ^{(s)}\] (13)

where \(\theta_{ij}\) is matrix capturing the similarity between queries and keys.

\[\theta_{ij}^{(s)}=Softmax(\frac{\bm{q}_{i}^{(s)\top}\bm{k}_{ij}^{(s)}}{\sqrt{ d}})\] (14)

Here \(d\) is the dimension of the hidden state. The queries, keys and values of \(\bm{q}_{i}^{(s)}\), \(\bm{k}_{ij}^{(s)}\) and \(\bm{v}_{ij}^{(s)}\) in attention mechanism are unfolded as follows:

\[\bm{q}_{i}^{(s)} =\varphi_{q}(\bm{h}_{i}^{(s-1)}),\] (15) \[\bm{k}_{ij}^{(s)} =\varphi_{k}(\bm{h}_{i}^{(s-1)},\bm{L}^{\top}\bm{L},\psi_{\text{ FT}}(\bm{f}_{j}-\bm{f}_{i})),\] (16) \[\bm{v}_{ij}^{(s)} =\varphi_{v}(\bm{h}_{i}^{(s-1)},\bm{L}^{\top}\bm{L},\psi_{\text{ FT}}(\bm{f}_{j}-\bm{f}_{i}))\] (17)

Where \(\varphi_{q}\), \(\varphi_{k}\) and \(\varphi_{v}\) are MLPs. \(\bm{L}\) is the unit lattice cell. Specially, \(\bm{L}^{\top}\bm{L}\) is used to ensure O(3)-equivariance in diffusion step. The transform \(\psi_{\text{FT}}\) is able to extract various frequencies of all relative fractional distances that are helpful for crystal structure modeling, and more importantly, \(\psi_{\text{FT}}\) is periodic translation invariant, namely, \(\psi_{\text{FT}}(w(\bm{f}_{j}+\bm{t})-w(\bm{f}_{i}+\bm{t}))=\psi_{\text{FT}}( \bm{f}_{j}-\bm{f}_{i})\) for any translation \(\bm{t}\). The part corresponding to original DiffCSP is presented in Appendix B.3.

#### 4.2.2 Improved Predictor for Evaluation

After denoising process, we need to predict the T\({}_{c}\) values of the generated samples. We adopt SODNet as an effective substitute of DFT-based predictors. Similar to CDVAE [62], we calculate the success rate (SR) as the proportion of optimized structures reaching the required thresholds. Given the samples \(\tilde{\mathcal{D}}\), SR is defined as

\[\text{SR}\alpha(\tilde{\mathcal{D}})=\frac{\|\tilde{\mathcal{M}}|\tilde{ \mathcal{M}}\in\tilde{\mathcal{D}},\varphi(\tilde{\mathcal{M}})>P_{100-\alpha} (\mathcal{D}_{\text{train}})\|}{\|\tilde{\mathcal{D}}\|},\] (18)

where \(\varphi\) is the SODNet predictor and \(P_{100-\alpha}(\mathcal{D}_{\text{train}})\) is the \(100-\alpha\) percentile of the \(T_{c}\) values in the training set. Similarly, we define the novelty success rate (NSR) as a metric to assess the generation of novel structures, with detailed definitions and explanations provided in the Appendix D.

#### 4.2.3 Pre-training

Considering the SuperCon3D dataset's limited structures, which doesn't fully capture the diversity in atomic species, lattice parameters, and atomic spatial distributions, we pre-trained our model on approximately 1.14 million unique 3D crystals sourced from existing databases, including Materials Project, OQMD, ICSD and Matgen.

## 5 Experiments

### Setup

#### 5.1.1 SuperCon3D dataset.

We extracted approximately 33,000 superconductors with their chemical formulas and corresponding critical temperatures from SuperCon. After removing duplicates and non-superconductors, we identified 11,949 superconducting materials. Additionally, over 200,000 ordered and disordered crystal structures were gathered from the ICSD database [3]. We then matched these 11,949 SuperCon entries with 208,425 ICSD entries based on chemical composition, space group and lattice parameter. Moreover, T\({}_{c}\) values and structural data for hydrogen-enriched superconductors were collated from various literature sources. This process resulted in 1,578 superconductor data entries, each featuring both T\({}_{c}\) and crystal structure. To ensure the dataset's integrity, all entries were vetted by domain experts and accompanied by referenced literature. Detailed data descriptions are provided in Appendix A.

#### 5.1.2 Evaluation Metrics.

We mainly compare our proposals with other crystal property predictors and inverse crystal structure generative models. For property predicting tasks, we mainly employ Mean Absolute Error (MAE) and R-Square (R\({}^{2}\)) for T\({}_{c}\) prediction. In addition, we also use visualization and interpretable analysis to verify our model. For inverse crystal structure generative task, we calculate the success rate (SR) as the percentage of the 100 optimized structures achieving 10, 30, 50 percentiles of the superconducting property distribution.

### Experimental Results and Discussion for Superconductivity Prediction

#### 5.2.1 Comparison on Dataset.

We present a summary of comparisons with previous crystal property predictors in Table 1. SODNet consistently outperforms the other competitors both on ordered and disordered structures. For example, SODNet achieves 17.6% reduction on MAE and about 4.4% improvements on R\({}^{2}\) than the second ranked Matformer. When considering the PD disordered structure between SODNet and MEGNet, SODNet gets about 41.7% reduction on MAE and almost 66.1% improvement on R\({}^{2}\) than MEGNet. It is worth noting that when we incorporate disordered structures into the training and validation sets, the metrics of R\({}^{2}\) and MAE both show improvements, indicating that accurately representing disordered structures is beneficial for the prediction of ordered structure properties. This also means that SODNet can be further improved with larger dataset in the future work.

#### 5.2.2 Ablation Study.

We conduct ablation studies to investigate crucial factors that influence the performance of the proposed SODNet. Table 2 shows the experimental results of SODNet with disordered graph representation and equivariant graph attention. When nodes and edges are embedded without atomic occupancy, both the MAE and R\({}^{2}\) metrics exhibit a decline in performance. Among them, node embedding is more sensitive to disordered graphs, leading to almost half of the performance loss.

Additionally, if we replace the type-L layer with MLPs, the proposed model achieves worse performance, indicating that the type-L features with equivariant activation function plays a crucial role in O(3) invariance for vectors.

#### 5.2.3 Real-world Superconductors Validation

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{Data} & \multicolumn{2}{c}{Performance} \\ \cline{2-5} Method & Train & Test & MAE (logK)\(\downarrow\) & R\({}^{2}\uparrow\) \\ \hline RF-c & O & O & 0.738\(\pm\)0.165 & 0.711\(\pm\)0.050 \\ SVM-c & O & O & 0.632\(\pm\)0.094 & 0.801\(\pm\)0.041 \\ RF-geo & O & O & 0.741\(\pm\)0.115 & 0.759\(\pm\)0.051 \\ SVM-geo & O & O & 0.578\(\pm\)0.114 & 0.827\(\pm\)0.042 \\ \hline SchNet & O & O & 0.891\(\pm\)0.041 & 0.401\(\pm\)0.032 \\ CGCNN & O & O & 0.879\(\pm\)0.047 & 0.405\(\pm\)0.022 \\ DimeNet++ & O & O & 0.811\(\pm\)0.058 & 0.434\(\pm\)0.092 \\ SphereNet & O & O & 0.762\(\pm\)0.048 & 0.467\(\pm\)0.096 \\ ALIGNN & O & O & 0.755\(\pm\)0.049 & 0.479\(\pm\)0.090 \\ Matformer & O & O & 0.748\(\pm\)0.043 & 0.570\(\pm\)0.135 \\ \hline MEGNet & O & O & 0.794\(\pm\)0.006 & 0.497\(\pm\)0.009 \\  & O/SD & O/SD & 0.889\(\pm\)0.049 & 0.431\(\pm\)0.058 \\ \hline SODNet & O & O & 0.622\(\pm\)0.112 & 0.595\(\pm\)0.101 \\  & O/SD/PD/SPD & O & **0.584\(\pm\)0.119** & **0.634\(\pm\)0.117** \\  & O/SD & O/SD & **0.518\(\pm\)0.084** & **0.716\(\pm\)0.064** \\  & O/SD/PD/SPD & O/SD/PD/SPD & **0.505\(\pm\)0.055** & **0.748\(\pm\)0.032** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Predicting models performance on SuperCon3D dataset. ‘O’ indicates that using ordered data. ML models with -c and -geo denote composition and structure features.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{2}{c}{Performance} \\ \cline{2-4} Method & MAE (logK)\(\downarrow\) & R\({}^{2}\uparrow\) \\ \hline \multicolumn{4}{c}{\(w/o\)_Occupancy Embedding_} \\ \hline \(w/o\) disorder node embedding & 0.990\(\pm\)0.033 & 0.365\(\pm\)0.044 \\ \(w/o\) disorder edge embedding & 0.592\(\pm\)0.087 & 0.655\(\pm\)0.046 \\ \hline \hline \multicolumn{4}{c}{\(w/o\)_Occupancy_} \\ \hline \(w/o\) & equivariant operations & 0.611\(\pm\)0.046 & 0.618\(\pm\)0.027 \\ \hline SODNet & **0.505\(\pm\)0.055** & **0.748\(\pm\)0.032** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation studies of SODNet on SuperCon3D.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Material & O/SD/PD \(T_{c}^{exp}\) (K) \(T_{c}^{pred}\) (K) & 
\begin{tabular}{c} Relative \\ Error(\%) \\ \end{tabular} \\ \hline CaH\({}_{6}\) & O & 215 [35] & 242.25 & 12.67 \\ Ti & O & 26 [66] & 8.50 & 67.31 \\ CsV\({}_{3}\)Sb\({}_{5}\) & O & 2.3 [18] & 2.36 & 6 \\ Cs(V\({}_{0.93}\)Nb\({}_{0.07}\))\({}_{3}\)Sb\({}_{5}\) & SD & 4.45 [29] & 4.71 & 5.84 \\ Zr\({}_{4}\)Rh\({}_{2}\)O & O & 3.73 [58] & 4.12 & 10.45 \\ Zr4Pd2O & O & 2.73 [58] & 2.82 & 3.3 \\ LaFeSiO\({}_{0.9}\) & PD & 10 [23] & 7.93 & 20.7 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Recently discovered superconductors (not included in the training data).

To assess the model's real-world relevance, we gathered newly discovered superconductors from the last three years, not present in our training data. Table 8 reveals that except for titanium superconductors, other materials' critical temperatures (T\({}_{c}\)) are predicted with low relative error margins (below 21%). This underscores the model's ability to predict T\({}_{c}\) values beyond its training scope, highlighting its utility in new material discovery. The outlier predictions for titanium could stem from close atomic proximities under extreme pressures (248 GPa), a condition scarcely represented in our training set. More details are presented in E.1.

#### 5.2.4 Potential Superconducting Materials.

Using our model, we screened the ICSD database to identify potential high-T\({}_{c}\) superconductors. Appendix E.2 lists 27 candidates, including cuprate, H-rich, heavy-Fermion, iron-based, and other types. The top three candidates are Ba\({}_{1.1432}\)Co\({}_{0.1429}\)O\({}_{3.0009}\)Rh\({}_{0.8574}\), ErH\({}_{3}\), and Ba\({}_{0.515}\)Ca\({}_{0.485}\), previously unreported. This is the first identification of disordered superconducting candidates from ICSD using a GNN method. Given that most ICSD structures are experimentally synthesized, these candidates are valuable for further research. Our model effectively screens disordered high-T\({}_{c}\) structures, demonstrating its usefulness. Additionally, we highlight four prime high-T\({}_{c}\) candidates with analogous parent structures in Table 10 and 11 of the Appendix E.2. In the Appendix E.3, we provide an interpretation of our SODNet predictor by identifying the features that the model prioritizes when making predictions, using the case of order-and-disorder-MgB\({}_{2}\) as an example. This analysis demonstrates SODNet's ability to capture the correlations between superconducting properties and structural characteristics.

### Experimental Results and Discussion for Inverse Crystal Structure Generation

#### 5.3.1 Comparison on Dataset.

We summarize the comparisons to previous main generative models in Table 4 and present training details in Appendix C.2. Without pretraining, CDVAE, SyMat and DiffCSP generate poor crystal structures, exhibiting extremely low SR performance. The main reason for this phenomenon may be the vast compound space of superconducting materials, making it difficult to effectively sample the atomic species and atomic spatial coordinates. The DiffCSP-SC model we propose shows a slight performance improvement compared to the two models mentioned above under the same conditions. CDVAE lacks translation invariance for atomic coordinates, which affects the quality of generated structures. This low performance metric is also observed in the DiffCSP [27] and aligns with our findings. Moreover, in comparison to DiffCSP, DiffCSP-SC containing an attention mechanism exhibits higher SR performance, indicating that DiffCSP may capture structural features associated with high T\({}_{c}\). We will give more discussions for DiffCSP-SC in section _Ablation Study_. Notably, DiffCSP-SC consistently achieved the highest performance across the NSR metric, with detailed results presented in the Appendix D.

#### 5.3.2 Ablation Study.

In ablation studies detailed in Table 5, we examine key components of our DiffCSP-SC model. **1.** Assessing the transformer's impact, its removal and reverting to the original DiffCSP approach led to a notable performance drop, especially in SR10 and SR30 metrics, implicating a decrease in high T\({}_{c}\) superconductor generation. This suggests that attention mechanisms in transformers effectively capture the complex atomic compositions of high T\({}_{c}\) superconductors, which often involve multi-component, multi-element structures. **2.** The pre-training methodology's significance is highlighted by its ability to manage the vast feature space of atomic species, coordinates, and unit cells in ordered crystals. Without it, as seen when training solely on a limited subset from SuperCon3D, the model's efficacy in generating valid superconductors significantly diminishes.

\begin{table}
\begin{tabular}{l c c c c} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Data} & \multicolumn{4}{c}{Performance} \\ \cline{3-5}  & & SR10 & SR30 & SR50 \\ \hline CDVAE & O & 0.03 & 0.03 & 0.03 \\ SyMat & O & 0.03 & 0.04 & 0.04 \\ DiffCSP & O & 0.04 & 0.05 & 0.05 \\ DiffCSP-SC & O & 0.05 & 0.05 & 0.10 \\ \hline CDVAE & Pre-training + O & 0.25 & 0.25 & 0.30 \\ SyMat & Pre-training + O & 0.28 & 0.28 & 0.35 \\ DiffCSP & Pre-training + O & 0.30 & 0.30 & 0.45 \\ DiffCSP-SC & Pre-training + O & **0.37** & **0.37** & **0.50** \\ \hline \end{tabular}
\end{table}
Table 4: Results for inverse crystal structures generation. “O” and “Pre-training” indicate models trained on SuperCon3D’s ordered structures and a collection of 1.14 million stable structures, respectively.

#### 5.3.3 Candidate Superconductors.

Utilizing our model, we aimed to generate novel superconductor candidates with high T\({}_{c}\) values. Table 12 and 13 in Appendix E.4 displays 32 potential high T\({}_{c}\) superconducting materials categorized as cuprate, H-rich, heavy-Fermion, iron-based, and other types. We initially assessed the novelty of these structures through similarity calculations with our 1.14 million-structure database. Interestingly, our findings reveal three candidates, index 8, 9, and 12, previously reported for T\({}_{c}\) using computational methods. Additionally, another candidate, index 21 and 22, demonstrated superconductivity upon doping and pressing. Subsequently, density-functional theory (DFT) were performed on selected candidates to verify their superconducting properties. Notably, Van Hove singularities (VHS) were observed in the electronic structures of Ba\({}_{2}\)CuCl\({}_{2}\)O\({}_{2}\), Lu, and BaFe\({}_{2}\)Se\({}_{2}\), as further detailed in Appendix E.5. VHS is a significant aspect in superconductivity research, often explored for its potential influence [5].

## 6 Conclusion and Discussion

In conclusion, a novel dataset has been constructed as a benchmark for future deep learning-based superconductivity research. Utilizing the dataset, we put forth two deep learning approaches for the design of high T\({}_{c}\) superconductors: a property prediction model for screening the known structures, and a generative model for creating the novel structures. To further validate the efficacy of the model, we apply the predicting model to screen the entire ICSD and identify a list of ordered and disorder superconducting candidates. By employing pretraining on large-scale crystal structures, we have achieved the capability to perform reverse structure design on limited superconducting data points.

Our SuperCon3D dataset, featuring experimental structures and T\({}_{c}\) values, paves the way for real-world superconductor applications. Combined with SODNet, which addresses disordered graph issues previously overlooked by the AI community, and DiffCSP-SC for novel designs. However, the accuracy of data-driven models remains constrained by the collected superconducting dataset. As Fig. 4 in the Appendix shows, data unevenness and elemental skewness (especially in Cu and O) may bias the model. Additionally, as Table 8 indicates, atomic distributions under extreme pressures contribute to predictive errors. Addressing these, Fig. 8 presents our pipeline, combining DiffCSP-SC and SODNet, to design and validate novel superconductors through wet experiments, iteratively enriching the dataset for improved model training and accuracy.

## 7 Acknowledgement

This work was jointly supported by the following projects: National Science and Technology Major Project (2022ZD0117805), the National Natural Science Foundation of China (No. 61925601, No. 62376276), Beijing Nova Program (20230484278).

## References

* Azam et al. [2023] M. Azam, M. Manasa, T. Zajarniuk, R. Diduszko, T. Cetner, A. Morawski, A. Wisniewski, and S. J. Singh. Antimony doping effect on the superconducting properties of smfeas (o, f). _IEEE Transactions on Applied Superconductivity_, 2023.
* Bao et al. [2022] F. Bao, M. Zhao, Z. Hao, P. Li, C. Li, and J. Zhu. Equivariant energy-guided sde for inverse molecular design. _arXiv preprint arXiv:2209.15408_, 2022.
* Bergerhoff et al. [1983] G. Bergerhoff, R. Hundt, R. Sievers, and I. D. Brown. The inorganic crystal structure data base. _Journal of Chemical Information and Computer Sciences_, 23(2):66-69, 1983. ISSN 0095-2338. doi: 10.1021/ci00038a003. URL https://pubs.acs.org/doi/abs/10.1021/ci00038a003.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & \multicolumn{3}{c}{Performance} \\ \cline{2-4} Method & SR10 & SR30 & SR50 \\ \hline \multicolumn{4}{c}{\(w/o\)\(Transformer\)} \\ \(w/o\) attention & 0.28 & 0.28 & 0.45 \\ \hline \multicolumn{4}{c}{\(w/o\)\(Pre\)-\(training\)} \\ \(w/o\) pre-training & 0.05 & 0.05 & 0.10 \\ \hline DiffCSP-SC & **0.37** & **0.37** & **0.50** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablation studies of DiffCSP-SC on SuperCon3D.

* Bloch [1994] P. E. Blochl. Projector augmented-wave method. _Physical review B_, 50(24):17953, 1994.
* Bok and Bouvier [2012] J. Bok and J. Bouvier. Superconductivity and the van hove scenario. _Journal of superconductivity and novel magnetism_, 25:657-667, 2012.
* Braginski [2019] A. I. Braginski. Superconductor electronics: status and outlook. _Journal of superconductivity and novel magnetism_, 32(1):23-44, 2019.
* Chan and Ceder [2010] M. Chan and G. Ceder. Efficient band gap prediction for solids. _Physical review letters_, 105(19):196403, 2010.
* Chen et al. [2019] C. Chen, W. Ye, Y. Zuo, C. Zheng, and S. P. Ong. Graph networks as a universal machine learning framework for molecules and crystals. _Chemistry of Materials_, 31(9):3564-3572, 2019. ISSN 0897-4756. doi: 10.1021/acs.chemmater.9b01294. URL https://doi.org/10.1021/acs.chemmater.9b01294.
* Chen et al. [2021] C. Chen, Y. Zuo, W. Ye, X. Li, and S. P. Ong. Learning properties of ordered and disordered materials from multi-fidelity data. _Nature Computational Science_, 1(1):46-53, 2021. ISSN 2662-8457. doi: 10.1038/s43588-020-00002-x. URL https://doi.org/10.1038/s43588-020-00002-x.
* Chen et al. [2022] P. Chen, J. Chen, H. Yan, Q. Mo, Z. Xu, J. Liu, W. Zhang, Y. Yang, and Y. Lu. Improving material property prediction by leveraging the large-scale computational database and deep learning. _The Journal of Physical Chemistry C_, 126(38):16297-16305, 2022.
* Choudhary and DeCost [2021] K. Choudhary and B. DeCost. Atomistic line graph neural network for improved materials property predictions. _npj Computational Materials_, 7(1):1-8, 2021.
* Choudhary and Garrity [2022] K. Choudhary and K. Garrity. Designing high-tc superconductors with bcs-inspired screening, density functional theory and deep-learning. _arXiv preprint arXiv:2205.00060_, 2022.
* Choudhary et al. [2022] K. Choudhary, B. DeCost, C. Chen, A. Jain, F. Tavazza, R. Cohn, C. W. Park, A. Choudhary, A. Agrawal, S. J. Billinge, et al. Recent advances and applications of deep learning methods in materials science. _npj Computational Materials_, 8(1):59, 2022.
* Elfwing et al. [2018] S. Elfwing, E. Uchibe, and K. Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. _Neural Networks_, 107:3-11, 2018. ISSN 0893-6080. doi: https://doi.org/10.1016/j.neunet.2017.12.012. URL https://www.sciencedirect.com/science/article/pii/S0893608017302976. Special issue on deep reinforcement learning.
* Fu et al. [2016] Y. Fu, X. Du, L. Zhang, F. Peng, M. Zhang, C. J. Pickard, R. J. Needs, D. J. Singh, W. Zheng, and Y. Ma. High-pressure phase stability and superconductivity of pnictogen hydrides and chemical trends for compressed hydrides. _Chemistry of Materials_, 28(6):1746-1755, 2016.
* Gasteiger et al. [2020] J. Gasteiger, S. Giri, J. T. Margraf, and S. Gunnemann. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. In _Machine Learning for Molecules Workshop, NeurIPS_, 2020.
* Gasteiger et al. [2021] J. Gasteiger, F. Becker, and S. Gunnemann. Gemnet: Universal directional graph neural networks for molecules. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021.
* Ge et al. [2022] J. Ge, P. Wang, Y. Xing, Q. Yin, H. Lei, Z. Wang, and J. Wang. Discovery of charge-4e and charge-6e superconductivity in kagome superconductor csv3sb5. _arXiv preprint arXiv:2201.10352_, 2022.
* Geiger and Smidt [2022] M. Geiger and T. E. Smidt. e3nn: Euclidean neural networks. _CoRR_, abs/2207.09453, 2022. doi: 10.48550/arXiv.2207.09453. URL https://doi.org/10.48550/arXiv.2207.09453.
* Gilmer et al. [2017] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl. Neural message passing for quantum chemistry. In D. Precup and Y. W. Teh, editors, _Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017_, volume 70 of _Proceedings of Machine Learning Research_, pages 1263-1272. PMLR, 2017. URL http://proceedings.mlr.press/v70/gilmer17a.html.

* Giustino [2017] F. Giustino. Electron-phonon interactions from first principles. _Reviews of Modern Physics_, 89(1):015003, 2017.
* Hai et al. [2021] Y.-L. Hai, N. Lu, H.-L. Tian, M.-J. Jiang, W. Yang, W.-J. Li, X.-W. Yan, C. Zhang, X.-J. Chen, and G.-H. Zhong. Cage structure and near room-temperature superconductivity in tbh n (n= 1-12). _The Journal of Physical Chemistry C_, 125(6):3640-3649, 2021.
* Hansen et al. [2022] M. F. Hansen, J.-B. Vaney, C. Lepoittevin, F. Bernardini, E. Gaudin, V. Nassif, M.-A. Measson, A. Sulpice, H. Mayaffre, M.-H. Julien, et al. Superconductivity in the crystallographic lafesio1-\(\delta\) with squeezed fesi layers. _npj Quantum Materials_, 7(1):1-8, 2022.
* Ho et al. [2020] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* Hoogeboom et al. [2022] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling. Equivariant diffusion for molecule generation in 3d. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 8867-8887. PMLR, 2022. URL https://proceedings.mlr.press/v162/hoogeboom22a.html.
* Hoogeboom et al. [2022] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling. Equivariant diffusion for molecule generation in 3d. In _International Conference on Machine Learning_, pages 8867-8887. PMLR, 2022.
* Jiao et al. [2023] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and Y. Liu. Crystal structure prediction by joint equivariant diffusion on lattices and fractional coordinates. In _Workshop on "Machine Learning for Materials" ICLR 2023_, 2023. URL https://openreview.net/forum?id=VPByphdu24j.
* Li et al. [2019] W. Li, J. Zhao, L. Cao, Z. Hu, Q. Huang, X. Wang, Y. Liu, G. Zhao, J. Zhang, Q. Liu, et al. Superconductivity in a unique type of copper oxide. _Proceedings of the National Academy of Sciences_, 116(25):12156-12160, 2019.
* Li et al. [2022] Y. Li, Q. Li, X. Fan, J. Liu, Q. Feng, M. Liu, C. Wang, J.-X. Yin, J. Duan, X. Li, et al. Tuning the competition between superconductivity and charge order in the kagome superconductor cs (v 1- x nb x) 3 sb 5. _Physical Review B_, 105(18):L180507, 2022.
* Liao and Smidt [2023] Y.-L. Liao and T. Smidt. Equivformer: Equivariant graph attention transformer for 3d atomistic graphs. In _International Conference on Learning Representations_, 2023.
* Liu et al. [2022] K. Liu, K. Yang, J. Zhang, and R. Xu. S2snet: A pretrained neural network for superconductivity discovery. In L. D. Raedt, editor, _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna, Austria, 23-29 July 2022_, pages 5101-5107. ijcai.org, 2022. doi: 10.24963/ijcai.2022/708. URL https://doi.org/10.24963/ijcai.2022/708.
* Liu et al. [2022] Y. Liu, L. Wang, M. Liu, Y. Lin, X. Zhang, B. Oztekin, and S. Ji. Spherical message passing for 3d molecular graphs. In _International Conference on Learning Representations (ICLR)_, 2022.
* Luo et al. [2022] S. Luo, Y. Su, X. Peng, S. Wang, J. Peng, and J. Ma. Antigen-specific antibody design and optimization with diffusion-based generative models. _bioRxiv_, 2022.
* Luo et al. [2024] Y. Luo, C. Liu, and S. Ji. Towards symmetry-aware generation of periodic materials. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ma et al. [2022] L. Ma, K. Wang, Y. Xie, X. Yang, Y. Wang, M. Zhou, H. Liu, X. Yu, Y. Zhao, H. Wang, et al. High-temperature superconducting phase in clathrate calcium hydride cah 6 up to 215 k at a pressure of 172 gpa. _Physical Review Letters_, 128(16):167001, 2022.
* MacManus-Driscoll and Wimbush [2021] J. L. MacManus-Driscoll and S. C. Wimbush. Processing and application of high-temperature superconducting coated conductors. _Nature Reviews Materials_, 6(7):587-604, 2021.
* Monkhorst and Pack [1976] H. J. Monkhorst and J. D. Pack. Special points for brillouin-zone integrations. _Physical review B_, 13(12):5188, 1976.

* Muller et al. [2006] P. Muller, R. Herbst-Irmer, A. L. Spek, T. R. Schneider, and M. R. Sawaya. _Crystal Structure Refinement: A Crystallographer's Guide to SHELXL_. Oxford University Press, 07 2006. ISBN 9780198570769. doi: 10.1093/acprof:oso/9780198570769.001.0001. URL https://doi.org/10.1093/acprof:oso/9780198570769.001.0001.
* Neverov et al. [2022] V. D. Neverov, A. E. Lukyanov, A. V. Krasavin, A. Vagov, and M. D. Croitoru. Correlated disorder as a way towards robust superconductivity. _Communications Physics_, 5(1):177, 2022.
* Novakovic et al. [2023] L. Novakovic, A. Salamat, and K. V. Lawler. Machine learning using structural representations for discovery of high temperature superconductors. _arXiv preprint arXiv:2301.10474_, 2023.
* Perdew et al. [1996] J. P. Perdew, K. Burke, and M. Ernzerhof. Generalized gradient approximation made simple. _Physical review letters_, 77(18):3865, 1996.
* Peschke et al. [2014] S. Peschke, T. Sturzer, and D. Johrendt. Ba\(1-\)xrbxfe2as2 and generic phase behavior of hole-doped 122-type superconductors. _Zeitschrift fur anorganische und allgemeine Chemie_, 640(5):830-835, 2014.
* Ptok et al. [2020] A. Ptok, K. J. Kapcia, M. Sternik, and P. Piekarz. Superconductivity of kfe2as2 under pressure: Ab initio study of tetragonal and collapsed tetragonal phases. _Journal of Superconductivity and Novel Magnetism_, 33(8):2347-2354, 2020.
* Ramesh et al. [2022] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* Reiser et al. [2021] P. Reiser, M. Neubert, A. Eberhard, L. Torresi, C. Zhou, C. Shao, H. Metni, C. van Hoesel, H. Schopmans, T. Sommer, et al. Graph neural networks for materials science and chemistry. _Communications Materials_, 3(1):93, 2022.
* Rombach et al. [2021] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* Satorras et al. [2021] V. G. Satorras, E. Hoogeboom, and M. Welling. E(n) equivariant graph neural networks. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pages 9323-9332. PMLR, 2021. URL http://proceedings.mlr.press/v139/satorras21a.html.
* Satorras et al. [2021] V. G. Satorras, E. Hoogeboom, and M. Welling. E (n) equivariant graph neural networks. In _International Conference on Machine Learning_, pages 9323-9332. PMLR, 2021.
* Schon et al. [2001] J. Schon, M. Dorget, F. Beuran, X. Zu, E. Arushanov, C. Deville Cavellin, and M. Lagues. Superconductivity in cacou2 as a result of field-effect doping. _Nature_, 414(6862):434-436, 2001.
* Schutt et al. [2018] K. T. Schutt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko, and K.-R. Muller. Schnet-a deep learning architecture for molecules and materials. _The Journal of Chemical Physics_, 148(24):241722, 2018.
* Shi et al. [2021] C. Shi, S. Luo, M. Xu, and J. Tang. Learning gradient fields for molecular conformation generation. In _International Conference on Machine Learning_, pages 9558-9568. PMLR, 2021.
* Sohl-Dickstein et al. [2015] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* Sommer et al. [2023] T. Sommer, R. Willa, J. Schmalian, and P. Friederich. 3dsc-a dataset of superconductors including crystal structures. _Scientific Data_, 10(1):816, 2023.
* Takahashi et al. [1994] H. Takahashi, N. Mori, M. Azuma, Z. Hiroi, and M. Takano. Effect of pressure on tc of hole-and electron-doped infinite-layer compounds up to 8 gpa. _Physica C: Superconductivity_, 227(3-4):395-398, 1994.

* Trippe et al. [2023] B. L. Trippe, J. Yim, D. Tischer, D. Baker, T. Broderick, R. Barzilay, and T. S. Jaakkola. Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/pdf?id=6TxBxqNNE1Y.
* Vaswani et al. [2017] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wang and Pickett [1983] C. Wang and W. Pickett. Density-functional theory of excitation spectra of semiconductors: application to si. _Physical review letters_, 51(7):597, 1983.
* Watanabe et al. [2023] Y. Watanabe, A. Miura, C. Moriyoshi, A. Yamashita, and Y. Mizuguchi. Observation of superconductivity and enhanced upper critical field of \(\eta\)-carbide-type oxide zr4pd2o. _Scientific Reports_, 13(1):22458, 2023.
* Weiler et al. [2018] M. Weiler, M. Geiger, M. Welling, W. Boomsma, and T. Cohen. 3d steerable cnns: Learning rotationally equivariant features in volumetric data. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pages 10402-10413, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/488e4104520c6aab692863cc1dba45af-Abstract.html.
* Wines et al. [2023] D. Wines, T. Xie, and K. Choudhary. Inverse design of next-generation superconductors using data-driven deep generative models, 2023.
* Xie and Grossman [2018] T. Xie and J. C. Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. _Physical Review Letters_, 120(14):145301, 2018. doi: 10.1103/PhysRevLett.120.145301. URL https://link.aps.org/doi/10.1103/PhysRevLett.120.145301.
* Xie et al. [2022] T. Xie, X. Fu, O. Ganea, R. Barzilay, and T. S. Jaakkola. Crystal diffusion variational autoencoder for periodic material generation. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=03RLpj-tc_.
* Xu et al. [2021] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations_, 2021.
* Yan et al. [2022] K. Yan, Y. Liu, Y. Lin, and S. Ji. Periodic graph transformers for crystal material property prediction. In _The 36th Annual Conference on Neural Information Processing Systems_, 2022.
* Yan et al. [2022] K. Yan, Y. Liu, Y.-C. Lin, and S. Ji. Periodic graph transformers for crystal material property prediction. _ArXiv_, abs/2209.11807, 2022.
* Zhang et al. [2022] C. Zhang, X. He, C. Liu, Z. Li, K. Lu, S. Zhang, S. Feng, X. Wang, Y. Peng, Y. Long, et al. Record high t c element superconductivity achieved in titanium. _Nature Communications_, 13(1):5411, 2022.
* Zhang et al. [2015] S. Zhang, Y. Wang, J. Zhang, H. Liu, X. Zhong, H.-F. Song, G. Yang, L. Zhang, and Y. Ma. Phase diagram and high-temperature superconductivity of compressed selenium hydrides. _Scientific reports_, 5(1):15433, 2015.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes]2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [N/A] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [Yes] 2. Did you include complete proofs of all theoretical results? [Yes]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]The appendix is organized as follows: Section A details the collection method and distribution of the SuperCon3D dataset. Section B presents more details of disordered graph and models. Section C elaborates on the implementation specifics of both property prediction and generative models. We present the evaluation metrics and experimental results for novel structure generation in Section D. The identification of potential superconductors and their corresponding DFT computational outcomes are presented in Section E. A systematic approach for the design of practical superconductors is expounded in Section F. Section G provides the repository link for the associated coding resources.

## Appendix A SuperCon3D Data Details

We extracted approximately 33,000 superconductors, including their chemical formulas and critical temperatures, from the SuperCon database2. After eliminating duplicates and non-superconductors, we retained 11,949 superconducting materials. Over 200,000 ordered and disordered crystal structures were collected from the ICSD database. We then matched the 11,949 SuperCon entries with 208,425 ICSD entries based on chemical composition, space group and lattice parameter. Specifically, we first performed an initial matching based on chemical composition, which may result in one-to-one or one-to-many matches. We then further refined the matches using additional information provided in the literature, such as space groups and lattice constants. Additionally, \(T_{c}\) values and structural data for hydrogen-enriched superconductors were obtained from literature sources. Ultimately, we compiled 1,578 superconductors with both \(T_{c}\) and crystal structure information.

Footnote 2: https://github.com/vstanev1/Supercon

We plot the data distribution of SuperCon3D dataset in Fig. 4. In dataset, there are 83 different elements, which encompass most of elemental types found in the periodic table. The most frequent elements are O, Cu, La, Ba, Y as shown in Fig. 4a. Fig. 4b depicts the order and disorder distribution. We classify superconducting materials according to cuprate, H riched, heavy fermion, iron based,

Figure 4: The data distribution of SuperCon3D dataset. (a). The probability of crystals containing a given element in the dataset. (b). The distribution of ordered and disordered superconductors. (c). The distribution of superconducting types. (d). The distribution of T\({}_{c}\) values.

and others, and distribute the types in Fig. 4c. The distribution of the T\({}_{c}\) values of superconducting materials is shown in Fig. 4d. The SuperCon3D dataset can be obtained from the source code package, and the access address is proveide in Sec. G.

## Appendix B Methods

### Interstitial Disorder

**Interstitial disorder (ID)**. ID refers to the presence of atoms occupying interstitial sites within a crystal lattice, which are not part of the regular lattice positions. These interstitial atoms introduce additional disorder into the structure. The total occupancy, including both regular lattice sites and interstitial sites, can be expressed as:

\[\bm{w}_{i,1}+\bm{w}_{i,2}+\cdots+\bm{w}_{i,m_{i}}+\bm{w}_{i,interstitial}=1+\Delta\] (19)

where \(\bm{w}_{i,m_{i}}\) represents the occupancy weight of \(m_{i}\) at site \(i\), \(\bm{w}_{i,interstitial}\) represents the occupancy weight of interstitial atoms at site \(i\), \(\Delta\) is the excess occupancy due to interstitial atoms, with \(\Delta\) > 0 indicating the presence of ID. In this case, our disordered graph encoding method remains effective.

ID mixed with substitutional disorder (SD) and positional disorder (PD), would result in more new types. However, given the lack of observation of ID in the SuperCon3D dataset, we will not elaborate further on it.

### Gate layer

We employ the gate activation mechanism [59] for the equivariant activation function. Standard activation functions are applied to type-0 vectors. For higher order vectors (L >0), we achieve equivariance by multiplying them with non-linearly transformed type-0 vectors. Specifically, for an input \(x\) comprising non-scalar \(C_{L}\) type-L vectors (where \(0<L\leq L_{\text{max}}\)) and \((C_{0}+P_{L}\sum_{L=1}^{L_{\text{max}}}C_{L})\) type-0 vectors, we apply SiLU [14] to the first \(C_{0}\) type-0 vectors and a sigmoid function to the remaining \(P_{L}\sum_{L=1}^{L_{\text{max}}}C_{L}\) type-0 vectors. This process generates non-linear weights, which are then used to scale each type-L vector. After gate activation, the number of channels for type-0 vectors is reduced to \(C_{0}\).

### The Denoising Method of DiffCSP

We introduce the denoising model \(\phi(\bm{L},\bm{F},\bm{A},t)\) as part of the original DiffCSP model, which is related to the _Transformer-based Architecture_ section in the main text.

Node representations in the \(s\)-th layer, \(\bm{H}^{(s)}=[\bm{h}_{1}^{(s)},\cdots,\bm{h}_{N}^{(s)}]\), are initialized as \(\bm{h}_{i}^{(0)}=\psi(f_{\text{atom}}(\bm{a}_{i}),f_{\text{pos}}(t))\), combining atomic embeddings \(f_{\text{atom}}\) and sinusoidal positional encoding \(f_{\text{pos}}\)[56; 24], processed by MLP \(\psi\).

Incorporating EGNN [48], the message-passing in layer \(s\) is:

\[\bm{m}_{ij}^{(s)} =\varphi_{m}(\bm{h}_{i}^{(s-1)},\bm{h}_{j}^{(s-1)},\bm{L}^{\top} \bm{L},\psi_{\text{FT}}(\bm{f}_{j}-\bm{f}_{i})),\] (20) \[\bm{m}_{i}^{(s)} =\sum_{j=1}^{N}\bm{m}_{ij}^{(s)},\] (21) \[\bm{h}_{i}^{(s)} =\bm{h}_{i}^{(s-1)}+\varphi_{h}(\bm{h}_{i}^{(s-1)},\bm{m}_{i}^{(s )}).\] (22)

Here, \(\varphi_{m}\) and \(\varphi_{h}\) are MLPs. \(\psi_{\text{FT}}\) executes Fourier Transformation on relative fractional coordinates, ensuring periodic translation invariance.

Following \(S\) message-passing layers, lattice noise \(\hat{\bm{\epsilon}}_{\bm{L}}\) is computed as follows:

\[\hat{\bm{\epsilon}}\bm{L}=\bm{L}\varphi_{L}\Big{(}\frac{1}{N}\sum i=1^{N}\bm{ h}_{i}^{(S)}\Big{)},\] (23)with \(\varphi_{L}\) shaping output as \(3\times 3\). For fractional coordinate score \(\hat{\bm{\epsilon}}_{\bm{F}}\), we have:

\[\hat{\bm{\epsilon}}_{\bm{F}}[:,i]=\varphi_{F}(\bm{h}_{i}^{(S)}),\] (24)

where \(\hat{\bm{\epsilon}}_{\bm{F}}[:,i]\) is the \(i\)-th column, and \(\varphi_{F}\) operates on the final layer's output.

The inner product \(\bm{L}^{\top}\bm{L}\) in Eq.(20) ensures O(3)-invariance, as \((\bm{QL})^{\top}(\bm{QL})=\bm{L}^{\top}\bm{L}\) for any orthogonal \(\bm{Q}\in\mathbb{R}^{3\times 3}\). This guarantees the O(3)-invariance of \(\varphi_{L}\) in Eq.(24), and \(\bm{L}\) left-multiplied with \(\varphi_{L}\) ensures O(3)-equivariance of \(\hat{\bm{\epsilon}}_{\bm{L}}\). Thus, \(\phi(\bm{L},\bm{F},\bm{A},t)\) satisfies the proposed properties. More details are described in Jiao et al. [2018].

## Appendix C Hyper-parameters and Training Details

In this section, we provide the training details of property predicting models and generative models.

### Property Predicting Models

We employ the codebase from RF and SVM [53]3, SchNet [50]4, CGCNN [61]5, DimNet++ [16]6, SphereNet [32]7,ALIGNN [11]8, Matformer [65]9 and MEGNet [8]10 for baseline implementations. All models are conducted 10-fold experiments based data splited method of 8:1:1. The training details of each model are as follows:

Footnote 3: https://github.com/aimat-lab/3DSC

Footnote 4: https://github.com/atomistic-machine-learning/SchNet

Footnote 5: https://github.com/txie-93/cgcnn

Footnote 6: https://github.com/gasteigerjo/dimenet

Footnote 7: https://github.com/divelab/DIG

Footnote 8: https://github.com/usnistgov/alignn

Footnote 9: https://github.com/YKQ98/Matformer

Footnote 10: https://github.com/materialsvirtualab/megnet

#### c.1.1 RF and SVM.

We conducted experiments comparing non-deep learning methods, specifically RF and SVM, using both chemical composition features and combined geometric structure features. The SVM model is configured with the following parameters: kernel is set to 'rbf' for mapping data into a higher-dimensional space, while degree, set to 3, controls the complexity of the polynomial kernel (applicable only when a polynomial kernel is used). gamma, set to'scale', adjusts the influence of individual data points in the feature space. The RF model uses these parameters: n_estimators is set to 100, defining the number of submodels in the ensemble. criterion, set to'squared_error', evaluates split quality based on mean squared error. min_samples_split, set to 2, specifies the minimum number of samples required to split an internal node, and min_samples_leaf, set to 1, defines the minimum samples needed at a leaf node. Lastly, max_features, set to 1.0, determines the proportion of features considered when finding the best split.

#### c.1.2 SchNet.

Employing the SchNet framework, our method integrates six 64-dimensional message passing layers. SchNet was trained over 500 epochs, using a 5e-4 learning rate and 64 batch size. We optimized using Adam with 1e-5 weight decay, and a one-cycle learning rate scheduler. Atomic radii were determined by the 12th smallest distance between an atom and its neighbors.

#### c.1.3 Cgcnn.

A batch size of 64 is employed, and the model consists of three layers of CGCNN message passing layer with 128 hidden dimensions. The training process utilizes the Adam optimizer. Initially, a learning rate of 1e-3 is set for the 200 epochs. During the training, a radius cutoff of 8.0 is applied to all crystals, and the 32 nearest neighbors are selected.

#### c.1.4 DimNet++.

In our approach, we apply a radius cutoff of 8.0 to all crystals and select the 12 nearest neighbors. To represent each node, we utilize Gaussian radial basis function (RBF) kernels. This results in a 64-dimensional embedding for each node. To optimize the model, we employ the Adam optimizer with a weight decay of 1e-6. The model is trained for 500 epochs using a batch size of 128.

#### c.1.5 SphereNet.

In our method, we utilize multi-graph representations of materials as inputs to SphereNet models. The input embedding size is set to 256, and the output embedding size is set to 64 for both the 8 LB2 and LB blocks. A cutoff distance of 6 is used. For each model, we initially perform a warm-up on the learning rate, starting at 1e-3. Subsequently, two learning rate strategies--ReduceLROnPlateau and StepLR--are employed for training. In the StepLR strategy, the learning rate is decayed by a specified ratio every fixed number of epochs, known as the step size. The batch size is set to 32, and training is conducted for 300 epochs.

#### c.1.6 Align.

ALIGNN is trained for 150 epochs with a learning rate of 5e-4 and a batch size of 64. The model architecture follows the original paper, consisting of four GCN layers and four ALIGNN layers. The atom feature dimension is set to 92, and the edge feature dimension is set to 80. The training process utilize the Adam optimizer with a weight decay of 1e-5. Additionally, a one-cycle learning rate scheduler is employed. For all crystals, a radius cutoff of 8.0 is applied, and the nearest 12 neighbors are selected.

#### c.1.7 Matformer.

In constructing the crystal graph, we follow a specific procedure. The radius for the neighborhood of a given atom is determined by the 12-th smallest distance between that atom and its neighboring atoms. All atoms within this radius are considered part of the neighborhood for the given atom. Each node is then represented by mapping its atomic number to a 92-dimensional embedding using the CGCNN atomic embedding. This embedding is further transformed into a 128-dimensional vector through a linear transformation. Similarly, for each edge, we utilize a 128-dimensional embedding mapping of the Euclidean distance. This mapping is achieved by employing 128 radial basis function (RBF) kernels with centers ranging from 0.0 to 8.0. During the training process, we employ the Adam optimizer with a weight decay of 1e-5. Additionally, a one-cycle learning rate scheduler is utilized. A batch size of 64 is employed and trained for 150 epochs.

#### c.1.8 MegNet.

To construct the crystal graph, we employ three layers of the MEGNET message passing with with 64,32,16 hidden units, and utilize the Set2Set readout function. Following the configuration described in the original paper, MEGNET is trained for 200 epochs using a batch size of 64 and a learning rate of 1e-3. The Adam optimizer with a weight decay of 1e-5 is used for optimization, and a one-cycle learning rate scheduler is implemented. A radius of 8.0 is set for all crystals.

\begin{table}
\begin{tabular}{l l} \hline \hline Hyper-parameters & Value or description \\ \hline Batch size & 32, 64, 128 \\ Number of epochs & 150, 300 \\ Number of attention heads & 4, 8 \\ Dropout rate & 0.0, 0.1, 0.2 \\ Cutoff radius (Å) & 8, 12, 16 \\ Number of radial bases & 128 \\ Number of transformer blocks & 6 \\ Weigh decay & 0.5\(\times 10^{-3}\), 1\(\times 10^{-3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyper-parameters for SODNet.

#### c.1.9 SODNet

During training, we use a batch size of 64 and trained the model for 150 epochs. A radius of 8.0 is applied to define the neighborhood of each crystal. We utilize 128 basis functions to capture the features of the crystals. To control overfitting, a weight decay of 5e-3 is applied. The learning rate is set to 5e-5, with a minimum learning rate of 1e-6. We employ the AdamW optimizer for efficient optimization. The model architecture consisted of 6 Transformer blocks, each with 8 attention heads. This allowed the model to effectively capture the relationships and dependencies within the crystal structures. Irreps features consist of channels of vectors with degrees up to \(L_{\text{max}}\). We denote \(C_{L}\) type-\(L\) vectors as \((C_{L},L)\) and \(C_{(L,p)}\) type-\((L,p)\) vectors as \((C_{(L,p)},L,p)\). Brackets denote concatenations of vectors. we set irreps features containing 512 type-0 vectors and 128 type-1 vectors, which can be expressed as \(\left[(512,0),(128,1)\right]\). Table 7 summarizes the hyper-parameters for the model.

### Generative Models

We apply the codebases from CDVAE [62]11, SyMat [34]12 and DiffCSP [27]13 for baseline implementations. All models are conducted experiments based data splited method of 6:2:2. For pretraining, we obtain crystal structures from the databases of Materials Project14, Open Quantum Materials Database15, Matgen16, and ICSD17. Molecular crystals are excluded from the dataset. Subsequently, we perform deduplication on all crystal structures, resulting in approximately 1.14 million unique structures. The training specifics for each model are outlined below:

Footnote 11: https://github.com/txie-93/cdvae

Footnote 12: https://github.com/divelab/AIRS

Footnote 13: https://github.com/jiaor17/DiffCSP

#### c.2.1 Cdvae.

For CDVAE model, We replaced the original DimNet++ [16] with SODNet to ensure a fair comparison with other generation models. Regarding the decoder, we utilize the GemNet-T [17], which consists of 3 layers and 128 hidden states.

#### c.2.2 SyMat.

For the SyMat model, the property predictor employs SphereNet, which consists of four message-passing layers with a hidden size of 128. The VAE decoder utilizes MLP models composed of two linear layers with a ReLU activation function between them and a hidden size of 256. During training, we use a learning rate of 0.001, a batch size of 128, and run for 1,000 epochs. We assign different weights to various loss terms: 1.0 for atom type set size, 30.0 for atom types, 1.0 for the number of each atom type, and 10.0 for lattice items. Additionally, we apply a weight of 0.01 for the KL-divergence loss and 10.0 for the denoising score matching loss.

\begin{table}
\begin{tabular}{c c c} \hline No. & Material & Type \(T_{c}^{exp}\) (K) \\ \hline
1 & CaH\({}_{6}\) @172 GPa & Order 215 [35] \\
2 & Ti @248 GPa & Order 26 [66] \\
3 & CsV\({}_{3}\)Sb\({}_{5}\) & Order 2.3 [18] \\
4 & Cs(V\({}_{0.93}\)Nb\({}_{0.07}\))\({}_{3}\)Sb\({}_{5}\) & SD 4.45 [29] \\
5 & Zr\({}_{4}\)Rh\({}_{2}\)O & Order 3.73 [58] \\
6 & Zr4Pd2O & Order 2.73 [58] \\
7 & LaFeSiO\({}_{0.9}\) & PD 10 [23] \\ \hline \end{tabular}
\end{table}
Table 8: Recently discovered superconductors (not included in the training data).

#### c.2.3 DiffCSP.

We employ a configuration of 6 layers with 512 hidden states for datasets other than specified ones. The dimension of the Fourier embedding is set to 256. To control the variance of the DDPM (Diffusion-Driven Probabilistic Modeling) process on \(\bm{L}_{t}\), we utilize the cosine scheduler with 0.008. Additionally, we use an exponential scheduler with \(\sigma_{1}=0.005,\sigma_{T}=0.5\) to control the noise scale of the score matching process on \(\bm{F}_{t}\). The diffusion step is set to 1000. Our model is trained for 1000 epochs, employing the same optimizer and learning rate scheduler as CDVAE.

#### c.2.4 DiffCSP-SC.

We utilize SODNet as the property predictor, and the parameter configuration aligns with Table 7. The parameters for the diffusion process also follow the original DiffCSP setup. The difference lies in the message passing layer, where we employ a transformer. Specifically, we use a 512-dimensional hidden state encoding and set the number of heads to 8.

### Pre-training Dataset

we pre-trained our model on approximately 1.14 million unique 3D crystals sourced from existing databases, including Materials Project, OQMD, Matgen and ICSD.

## Appendix D Novel Material Structure Generation

we have introduced a new metric, the "novelty success rate" (NSR), to specifically quantify the proportion of novel structures generated by the model. The NSR is defined as:

\[NSR_{\alpha}(\tilde{D})=\frac{\left\|\tilde{M}\mid\tilde{M}\in\tilde{D},\varphi (\tilde{M})>P_{100-\alpha}(D_{\text{train}}),\tilde{M}\notin D_{\text{train}} \right\|}{\|\tilde{D}\|}\] (25)

This metric focuses on evaluating the model's ability to generate structures that are not present in the training dataset.

We conducted additional experiments using NSR, and the results are summarized in the table below, comparing different models:

As shown in table 9, our DiffCSP-SC model outperforms others in generating novel materials, as indicated by higher NSR values across all metrics (NSR10, NSR30, NSR50).

Furthermore, our training dataset includes approximately 1 million material structures, many of which have not been experimentally validated for superconductivity. Even if some generated structures appear in the training data, they may still hold potential superconducting properties, making them valuable for further investigation. By leveraging this large dataset and pre-training strategies, our model demonstrates advantages in generating novel and potentially superconductive structures.

\begin{table}
\begin{tabular}{l c c c c} \hline
**Model** & **Data** & **NSR10** & **NSR30** & **NSR50** \\ \hline CDVAE & O & 0.02 & 0.02 & 0.02 \\ SyMat & O & 0.02 & 0.03 & 0.03 \\ DiffCSP & O & 0.03 & 0.04 & 0.04 \\ DiffCSP-SC & O & 0.04 & 0.04 & 0.09 \\ \hline CDVAE & Pre-training + O & 0.19 & 0.19 & 0.25 \\ SyMat & Pre-training + O & 0.20 & 0.21 & 0.26 \\ DiffCSP & Pre-training + O & 0.25 & 0.25 & 0.33 \\ DiffCSP-SC & Pre-training + O & 0.31 & 0.31 & 0.39 \\ \hline \end{tabular}
\end{table}
Table 9: NSR comparison across different models and data settings

## Appendix E Potential Superconductors

In this section, we initially validate our model using the T\({}_{c}\) values of superconducting materials reported in recent literature, noting that these data points are not included in the SuperCon3D dataset. Subsequently, we present the potential superconducting materials using property prediction model based on SODNet and generative model based on DiffCSP-SC, respectively.

### Real-world Superconductors Validation

As shown in Table 8, we have collected the structures of superconducting materials along with their corresponding T\({}_{c}\) values, as reported in the latest literature over the past three years. This includes a total of seven superconducting materials with both ordered and disordered structures.

### Screening Based Method

We apply our superconductivity predicting model for screening the entire ICSD database. Potential superconductors are show in Table 10 and 11. To elaborate on the candidates with high confidence, we provide the subsequent details:

1. CuO\({}_{2}\)Sr\({}_{0.075}\) and Ca\({}_{0.82}\)CuO\({}_{2}\) exhibit disordered structures, and their respective parent compounds demonstrate superconductivity [54; 49]. Consequently, these disordered structures are more likely to be superconducting materials as well.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Type & ICSD code & Chemical formula & O/SD/PD T\({}_{c}\) (K) Reported SC. \\ \hline \multirow{6}{*}{Cuprate} & 68675 & CuO\({}_{2}\)Sr\({}_{0.075}\) & PD & 93.42 & CuO\({}_{2}\)Si \\  & 50774 & Ca\({}_{0.775}\)CuO\({}_{2}\)Y\({}_{0.041}\) & PD & 65.70 & \\ \cline{1-1}  & 50773 & Ca\({}_{0.82}\)CuO\({}_{2}\) & PD & 64.72 & CuO\({}_{2}\)Si \\ \cline{1-1}  & 68217 & Ba\({}_{2}\)CuO\({}_{2}\) & O & 59.89 & 70K [28] \\ \cline{1-1}  & 67394 & Ba\({}_{2}\)CuO\({}_{2}\) & O & 43.80 & - \\ \hline \multirow{6}{*}{H-riched} & 187375 & ErH\({}_{3}\) & O & 193.03 & - \\  & 635802 & GdH\({}_{3}\) & O & 143.19 & - \\ \cline{1-1}  & 623739 & H\({}_{2.57}\)CuO\({}_{0.14}\)Y\({}_{0.84}\) & PD & 136.76 & - \\ \cline{1-1}  & 42009 & ThH\({}_{2.55}\) & SD & 135.13 & - \\ \cline{1-1}  & 424154 & HoMg\({}_{1.02}\)Ti\({}_{1.98}\) & O & 134.34 & - \\ \cline{1-1}  & 230140 & Lo\({}_{1.14}\)Y\({}_{0.86}\)Ni\({}_{2.7}\) & PD & 125.94 & - \\ \cline{1-1}  & 93250 & YeFe\({}_{2}\)H\({}_{5}\) & PD & 125.00 & - \\ \hline \hline \end{tabular}
\end{table}
Table 10: The predicted potential candidates of high-\(T_{c}\) cuprate and h-riched superconductors. Candidates of high confidence are marked in gray.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Type & ICSD code & Chemical formula & O/SD/PDT\({}_{c}\) (K) & Reported SC. \\ \hline \multirow{6}{*}{Heavy-Fermion} & 168466 & LaMg\({}_{1.12}\) & O & 23.83 & - \\  & 161141 & LaMg\({}_{11.196}\) Al\({}_{0.504}\) & SD & 21.13 & - \\  & 69897 & C\({}_{2}\)Co\({}_{0.75}\)U\({}_{0.25}\) & PD & 11.88 & - \\ \cline{1-1}  & 647197 & Np\({}_{1.1}\)Pu\({}_{0.9}\) & SD & 11.75 & - \\ \cline{1-1}  & 614236 & TmFe\({}_{4}\)B & O & 10.81 & - \\ \hline \multirow{6}{*}{Iron-based} & **427163** & Ba\({}_{0.83}\)Fe\({}_{2}\)Rb\({}_{0.17}\)As\({}_{2}\) & **SD** & **23.21** & Ba\({}_{0.6}\)Fe\({}_{2}\)Rb\({}_{0.1}\)As\({}_{2}\) \\  & 188347 & BaFe\({}_{2}\)As\({}_{2}\) & O & 23.27 & - \\ \cline{1-1}  & 39530 & FC\({}_{1}\)Te\({}_{1}\)Te & O & 19.57 & - \\ \cline{1-1}  & 633401 & FeSb\({}_{0.16}\)Te\({}_{1.6}\) & SD & 16.83 & - \\ \cline{1-1}  & 165523 & As\({}_{2}\)Ba\({}_{0.777}\)Fe\({}_{2.5}\) & PD & 15.55 & - \\ \cline{1-1}  & & K\({}_{0.126}\)Sn\({}_{0.096}\) & & & \\ \hline \multirow{6}{*}{Others} & 96031 & Ba\({}_{1.143}\)Co\({}_{0.429}\) & \multirow{6}{*}{PD} & 202.12 & - \\ \cline{1-1}  & 58639 & Ba\({}_{0.515}\)Ca\({}_{0.485}\) & SD & 160.95 & - \\ \cline{1-1}  & 616160 & BaSr & SD & 123.51 & - \\ \cline{1-1}  & 106111 & SrTi\({}_{2}\) & O & 63.52 & - \\ \cline{1-1}  & 428028 & Ge\({}_{0.6}\)Sb\({}_{0.27}\)Te & SD & 47.48 & - \\ \hline \hline \end{tabular}
\end{table}
Table 11: The predicted potential candidates of high-\(T_{c}\) heavy-fermion, iron-based and others superconductors. Candidates of high confidence are marked in gray.

2. Ba\({}_{2}\)CuO\({}_{3.2}\) exhibits superconductivity with a T\({}_{c}\) of 70K [28]. Its corresponding parent structure Ba\({}_{2}\)CuO\({}_{3}\) may also be a superconductor, albeit with a comparatively lower probability.
3. Ba\({}_{0.83}\)Fe\({}_{2}\)Rb\({}_{0.17}\)As\({}_{2}\) and Ba\({}_{0.6}\)Fe\({}_{2}\)Rb\({}_{0.4}\)As\({}_{2}\) share the same parent structure and have closely related compositions. Given that Ba\({}_{0.6}\)Fe\({}_{2}\)Rb\({}_{0.4}\)As\({}_{2}\) exhibits superconductivity with a T\({}_{c}\) of 37.5K [42], it is highly likely that Ba\({}_{0.83}\)Fe\({}_{2}\)Rb\({}_{0.17}\)As\({}_{2}\) is also a superconducting material.

### Interpretability on SODNet

We attempt to interpret our SODNet predictor by determining which feature(s) a given model weighs most heavily when making the prediction. As shown in Fig. 5, we extract the node embedding of the whole graph in the last layer of SODNet, and present the contributions of each atom to T\({}_{c}\) values. We can observe that the B sites contributes more significantly to the property of T\({}_{c}\) compared to the Mg site in Fig. 5 (a-d). Moreover, conducting atomic doping and atomic translation on the cation Mg results in a decrease in T\({}_{c}\) with 39.0 K \(\rightarrow\) 38.4 K \(\rightarrow\) 34.3 K. This phenomenon demonstrates that attempting to enhance the T\({}_{c}\) value by disrupting the symmetry of Mg site within the lattice may be not workable. Another case of cuprate superconductor has shown in Fig. 5 (e-f), there are three types of oxygen sites that contribute significantly to the T\({}_{c}\) value: Hg-O-Hg (PD disorder), Cu-O-Cu (order), and Hg-O-Ba (order). Among them, the contribution of disordered Hg-O-Hg is the greatest, indicating that disrupting the symmetry of oxygen atoms within the lattice might potentially further enhance the property of T\({}_{c}\).

### Generative Superconducting Candidates

We apply our generative model for generating new superconducting candidates. We present the crystal structures of the 20 superconducting candidate materials from Table 6 in Fig. 6. Additionally, we display the 32 superconducting candidate materials in Table 12 and 13, arranged in descending order of predicted T\({}_{c}\) values. The structures of all superconducting candidate materials can be obtained in the source code package. We collected superconducting materials that have been reported and observed that five candidates are more likely to be superconducting materials. Among them, four candidates obtained T\({}_{c}\) through theoretical calculations, and another material displayed superconducting properties through doping. Specific descriptions are as follows:

1. SeH\({}_{3}\) exhibited a T\({}_{c}\) of 113K as predicted by machine learning [40], corroborated by DFT calculations indicating 110K [67].
2. DFT methods calculated the T\({}_{c}\) of TbH\({}_{3}\), SbH\({}_{3}\), and KFe\({}_{2}\)As\({}_{2}\) as 20K [22; 15; 43]. Since H-riched materials belong to conventional superconductors and show high T\({}_{c}\) under high pressure, but the conditions for wet experimental synthesis are very stringent. Therefore,

\begin{table}
\begin{tabular}{c c c c} \hline \hline Type & Index & Chemical formula & T\({}_{c}\) (K) & Reported SC. \\ \hline  & 1 & Ba\({}_{2}\)CaCl\({}_{2}\)O\({}_{2}\) & 33.56 & - \\  & 2 & Tl\({}_{2}\)Ca\({}_{2}\)Ba\({}_{2}\)Cu\({}_{3}\)O\({}_{10}\) & 14.09 & - \\  & 3 & Ba\({}_{3}\)Ca\({}_{2}\)GaCl\({}_{2}\)O\({}_{4}\)\({}_{7}\)\({}_{17}\) & 10.12 & - \\ Cuprate & 4 & YCu\({}_{3}\)O\({}_{7}\) & 9.73 & - \\  & 5 & Ba\({}_{2}\)Ca\({}_{3}\)O\({}_{7}\) & 9.65 & - \\  & 6 & Cu\({}_{2}\)BO\({}_{16}\) & 7.87 & - \\  & 7 & CsMgCu\({}_{3}\)BiAuO\({}_{8}\) & 7.82 & - \\ \hline  & 8 & TbH\({}_{3}\) & 164.33 & 20K [22] \\  & & & Calculated by DFT \\ H-riched & 9 & SeH\({}_{3}\) & 139.89 & 113K [40] \\  & & & Predicted by ML \\  & 10 & CaGe\({}_{2}\)H\({}_{9}\) & 103.55 & - \\  & 11 & Ca\({}_{2}\)MnCrH\({}_{6}\) & 58.07 & - \\  & 12 & SbH\({}_{3}\) & 46.42 & 20K [15] \\  & & & Calculated by DFT \\ \hline  & 13 & MgCoCoH\({}_{41}\)\({}_{2}\)Cs\({}_{3}\)N\({}_{16}\) & 44.27 & - \\  & 14 & Rb\({}_{2}\)Ca\({}_{2}\)H\({}_{4}\) & 13.05 & - \\ \hline \hline \end{tabular}
\end{table}
Table 12: The novel high-\(T_{c}\) cuprate and h-riched superconducting candidates. Candidates of high confidence are marked in gray.

it can further verify whether superconducting materials are superconducting materials by combining DFT methods, and reduce the research and development cycle of superconducting materials.
3. The parent compound SmFeAsO underwent a superconducting T\({}_{c}\) around 54 K [1], following fluorine (F) doping at the O-site in the SmO layer. This case can provide us with a method that we can use DiffCSP-SC's generative model to generate superconducting parent structures, and then improve the T\({}_{c}\) of materials by doping, or transform materials without superconducting properties into superconducting materials.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Type & \multicolumn{2}{c}{Index Chemical formula T\({}_{c}\) (K)} & \multicolumn{2}{c}{Reported SC.} \\ \hline  & 15 & Th & 43.61 & - \\  & 16 & Ba\({}_{3}\)Pt\({}_{4}\) & 44.81 & - \\ Heavy-Ferm & 17 & T\({}_{c}\)\({}_{5}\) & 17.96 & - \\  & 18 & Lu & 4.86 & - \\  & 19 & Yb\({}_{3}\)In & 1.04 & - \\ \hline  & 20 & Ba\({}_{2}\)Fe\({}_{2}\)Se\({}_{2}\) & 11.99 & \multicolumn{2}{c}{} \\ Iron-based & 21 & SmFeAsO & 4.42 & 
\begin{tabular}{c} SmFeAsO\({}_{0.58}\)Pt\({}_{2}\) \\ S4K [1] \\ \end{tabular} \\  & 22 & KFe\({}_{2}\)As\({}_{2}\) & 4.23 & 20K [4] \\  & 23 & NdFeAsF & 4.13 & - \\  & 24 & FeSe & 3.36 & - \\ \hline  & 25 & Ba\({}_{3}\)Ca & 80.04 & - \\  & 26 & Ba\({}_{2}\)Se & 60.70 & - \\  & 27 & Ba\({}_{1}\) & 52.26 & - \\ Others & 28 & Mg\({}_{3}\)B & 43.96 & - \\  & 29 & BaCl\({}_{2}\)O & 35.72 & - \\  & 30 & Ba\({}_{2}\)Ca & 32.77 & - \\  & 31 & Sb\({}_{2}\)Ba\({}_{4}\) & 22.70 & - \\  & 32 & V\({}_{3}\)Si\({}_{1.1}\) & 16.28 & - \\ \hline \hline \end{tabular}
\end{table}
Table 13: The novel high-\(T_{c}\) heavy-fermion, iron-based and others superconducting candidates. Candidates of high confidence are marked in gray.

Figure 5: Contribution of each atom to T\({}_{c}\) value. (a). Feature relative importance of each site in three type MgB\({}_{2}\) superconductors. Snapshots of (b) SD Mg\({}_{0.9}\)Al\({}_{0.1}\)B\({}_{2}\), (c) PD Mg\({}_{0.98}\)B\({}_{2}\), and (d) ordered MgB\({}_{2}\) crystals. Here, B Mg and Al sites are colored by light pink, atrovirens and dark atrovirens. (e). Feature relative importance of Ba\({}_{2}\)CuHgO\({}_{4.27}\) superconductor. (f). Snapshot of Ba\({}_{2}\)CuHgO\({}_{4.27}\) superconductor (Ba: green, Cu: blue, Hg: pink, O: red).

Figure 6: The geometric structures of novel superconducting candidates in Table 6.

### DFT Calculations

We conduct DFT calculation using the Vienna ab initio package (VASP) [57; 7]. The structures are fully relaxed using the generalized gradient approximation (GGA) [41] of the SCAN meta-GGA functional, employing the pseudopotentials of the projector augmented wave (PAW) method [4]. A plane wave cutoff of 500 eV is employed for all simulations. Brillouin-zone integrations are performed using the \(\tau\)-centered Monkhorst-Pack (MP) scheme [37]. We initiate the calculations with a k-point meth featuring a dense sampling density of 2\(\pi\)\(\times\)0.04. The convergence criteria for energy and force is set to 0.1 meV and 0.001 eV/A, respectively.

The van Hove singularity (VHS) is a notable occurrence in condensed matter physics, specifically in the density of states (DOS) of a material. It manifests as a distinct peak or divergence in the DOS at a particular energy level. We select materials from Table 12 and 13 for DFT calculations and display their band structures and density of states (DOS) in Fig. 7. From the density of states (DOS) plot, we can observe the van Hove singularity (VHS) phenomenon. Additionally, we can also observe the presence of flat bands in the band structures of materials such as Ba\({}_{2}\)CuCl\({}_{2}\)O\({}_{2}\), Lu, Ba\({}_{4}\)Sb\({}_{2}\), and others. The integration of flat bands in the electronic architecture, along with the Van Hove Singularities (VHS) in the Density of States (DOS), markedly amplifies the likelihood of these candidates being superconducting materials.

Figure 7: The electronic structures of novel superconducting candidates.

Pipeline for Designing Real-world Superconductors.

Fig. 8 presents a pipeline for designing SC., validating our dataset and models for real-world scenarios. We initially generate potential, ordered superconducting structures using the DiffCSP-SC model trained on the SuperCon3D database. Candidate materials are selected based on T\({}_{c}\) values predicted by SODNet, followed by DFT verification to confirm the presence of superconducting electronic structures, such as VHS. Subsequently, selected candidates undergo wet lab synthesis, with T\({}_{c}\) values characterized and recorded in the SuperCon3D database. Further, if a superconductor is discovered, methods such as doping, which may transform ordered structures into disordered ones, are explored to enhance the T\({}_{c}\) value. SODNet is employed to investigate the relationship between disordered structures and doping ratios, aiming to design optimal doping proportions for experimental verification. These experimental outcomes are also recorded in the database. Continuous expansion of the database will incrementally improve the accuracy of the DiffCSP-SC and SODNet models trained on this dataset, creating a reinforcing cycle of enhancement.

## Appendix G Code

We have made the source code for SODNet and DiffCSP-SC, as mentioned in this article, available on GitHub. The repositories can be accessed at: https://github.com/pincher-chen/SODNet and https://github.com/pincher-chen/DiffCSP-SC.

Figure 8: Flowchart for designing novel SC materials.