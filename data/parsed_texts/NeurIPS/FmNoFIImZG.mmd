# TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models

 Andrei Margeloiu\({}^{1}\), Xiangjian Jiang\({}^{1}\), Nikola Simidjievski\({}^{2,1}\), Mateja Jamnik\({}^{1}\)

\({}^{1}\)Department of Computer Science and Technology, University of Cambridge, UK

\({}^{2}\)PBCI, Department of Oncology, University of Cambridge, UK

{am2770, xj265, ns779, mj201}@cam.ac.uk

Equal contribution.

###### Abstract

Data collection is often difficult in critical fields such as medicine, physics, and chemistry, yielding typically only small tabular datasets. However, classification methods tend to struggle with these small datasets, leading to poor predictive performance. Increasing the training set with additional synthetic data, similar to data augmentation in images, is commonly believed to improve downstream tabular classification performance. However, current tabular generative methods that learn either the joint distribution \(p(\mathbf{x},y)\) or the class-conditional distribution \(p(\mathbf{x}\mid y)\) often overfit on small datasets, resulting in poor-quality synthetic data, usually worsening classification performance compared to using real data alone. To solve these challenges, we introduce TabEBM, a novel class-conditional generative method using Energy-Based Models (EBMs). Unlike existing tabular methods that use a shared model to approximate all class-conditional densities, our key innovation is to create distinct EBM generative models for each class, each modelling its class-specific data distribution individually. This approach creates robust energy landscapes, even in ambiguous class distributions. Our experiments show that TabEBM generates synthetic data with higher quality and better statistical fidelity than existing methods. When used for data augmentation, our synthetic data consistently leads to improved classification performance across diverse datasets of various sizes, especially small ones. Code is available at https://github.com/andreimargeloiu/TabEBM.

## 1 Introduction

Many scientific domains within medicine, physics, and chemistry often rely on intricate and challenging data acquisition procedures [5; 50; 4; 32; 76; 12] that typically render small-size tabular datasets [5; 46]. Using these to train machine learning models that can aid in tasks such as disease diagnosis [52; 37], material property prediction [35], and chemical compound classification [11], can lead to poor performance [74; 52; 37]. In the case of learning tasks which leverage image and text data, a standard remedy to address performance issues due to data scarcity is employing data augmentation techniques [72; 73; 60; 71] that generate additional synthetic samples from existing data.

Figure 1: **Evaluation of TabEBM and other state-of-the-art tabular generative methods across six key metrics** (larger area indicates better performance). The results demonstrate that TabEBM excels in data augmentation (utility), with a larger area than all other methods.

However, applying data augmentation to tabular data introduces additional challenges, as tabular datasets are often very diverse and lack explicit symmetries [8], such as rotations or translations seen in images. Consequently, existing tabular data augmentation methods often yield mixed results and can even degrade model performance [51; 71; 48], hindering their widespread adoption.

Tabular augmentation typically involves training generative models to approximate either the joint distribution \(p(\mathbf{x},y)\)[85; 24] or the class-conditional distribution \(p(\mathbf{x}|y)\)[85; 42; 83; 47; 48]. A key challenge of joint distribution methods is maintaining the original training label distribution, as sampling from such generators can produce label distributions that deviate from the original and even fail to generate data for specific classes (see Appendix C for an example). These issues compromise the effectiveness of data augmentation [51] by undermining the label accuracy and distribution. On the other hand, while class-conditional models that learn \(p(\mathbf{x}|y)\) preserve the stratification of the original data, they often employ a _shared_ model to represent all class-conditional densities. This, however, can lead to overfitting, particularly in imbalanced datasets where the model may prioritise more frequent classes [21], ignoring unique features needed for generating label-invariant samples. Additionally, in datasets with limited data, this can lead to mode collapse [68; 70], where the model does not effectively capture the diversity of each class [70], and thus tends to perform poorly in a multi-class setting.

To address the challenges of class-conditional tabular generation, we introduce TabEBM (Figure 2), a new method for tabular data augmentation utilising Energy-Based Models (EBMs). Our method introduces two innovations: (i) _Distinct class-specific models:_ TabEBM constructs a collection of individual models - one for each class - which, by design, enables learning distinct marginal distributions for the inputs associated with each class. This, in turn, enables performing data augmentation while maintaining the original label distribution. (ii) _Generative models:_ we build novel class-specific generators that produce high-quality synthetic data even from extremely few samples. Specifically, we create a surrogate binary classification task for each class and fit it with a pre-trained tabular in-context classifier. We then convert the binary classifier into an EBM, a generative model, without additional training. Using class-specific EBMs makes the energy landscape more robust to class overlaps, compared to using a single shared EBM to approximate the class-conditional distribution.

Our contributions can be summarised as:

* **Technical:** We propose TabEBM, which is the first generative method to create class-specific EBMs, learning the marginal distribution for each class separately.
* **Empirical:** We present the first comprehensive analysis of tabular data augmentation across different dataset sizes and use cases beyond predictive performance. Our analysis compares TabEBM with eight leading tabular generative models across various datasets, demonstrating that TabEBM consistently improves data augmentation performance on small datasets, while our generated data demonstrates better statistical fidelity and privacy-preserving properties (Figure 1).
* **Library:** We release TabEBM as an open-source library, available at https://github.com/andreimargeloiu/TabEBM. Our library enables off-the-shelf data generation and data augmentation on any tabular dataset without requiring training. Further details are available in Appendix B.5.

Figure 2: **An overview of TabEBM. We learn _distinct_ class-specific Energy-Based Models (EBMs) \(E_{\text{blue}}(\mathbf{x})\) and \(E_{\text{red}}(\mathbf{x})\) exclusively on the points of their respective class. Each EBM approximates a class-conditional distribution \(p(\mathbf{x}|y)\). TabEBM allows synthetic data generation by sampling from the estimated distributions for each class \(p(\mathbf{x}|y=\text{blue})\) and \(p(\mathbf{x}|y=\text{red})\).**

TabEBM

**Notation.** We address classification problems with \(C\) classes, denoted by \(\mathcal{Y}=\{1,2,\ldots,C\}\). Let \(\{(\mathbf{x}^{(i)},y_{i})\}_{i=1}^{N}\) represent a dataset of \(N\) samples, each being a \(D\)-dimensional vector \(\mathbf{x}^{(i)}\in\mathbb{R}^{D}\), with a corresponding label \(y_{i}\in\mathcal{Y}\). For each class \(c\in\mathcal{Y}\), we define \(\mathcal{X}_{c}=\{\mathbf{x}^{(i)}\mid y_{i}=c\}\) as the subset of samples labelled with class \(c\). Let \(f_{\theta}(\cdot)\) denote a classifier. The expression \(f_{\theta}(\mathbf{x})[y]\) represents the (unnormalised) logit assigned to the class \(y\) for the input \(\mathbf{x}\).

### Preliminaries on Energy-Based Models

An Energy-Based Model (EBM) [43] defines a probability density function \(p_{\theta}(\mathbf{x})\) through an energy function \(E(\mathbf{x})\). Specifically, the model posits that \(p(\mathbf{x})\propto e^{-E(\mathbf{x})}\), where \(E(\mathbf{x})\) represents the unnormalised negative log-density of the input \(\mathbf{x}\). In this framework, lower energy values correspond to higher probability densities. This relationship allows EBMs to model distributions by learning to assign lower energy to more probable configurations of \(\mathbf{x}\) and higher energy to less probable ones.

An important observation is that energy-based models can utilise the same model architectures as standard classification models [29]. Typically, the logits \(f_{\theta}(\mathbf{x})[y]\) from a classification model define a discriminative distribution through the softmax function, expressed as \(p_{\theta}(y|\mathbf{x})=\mathrm{softmax}(f_{\theta}(\mathbf{x})[y])\). Intriguingly, these same logits can be reinterpreted to define an energy-based model for the joint distribution \(p(\mathbf{x},y)\). This is achieved by setting the energy function to \(E(\mathbf{x},y)=-f_{\theta}(\mathbf{x})\). Furthermore, the energy function for the marginal distribution \(p(\mathbf{x})\) is obtained by marginalising over \(p(\mathbf{x},y)\), resulting in \(E(\mathbf{x})=-\mathrm{LogSumExp}_{y^{\prime}}f_{\theta}(\mathbf{x})[y^{ \prime}]\).

Such an energy-based model, trained with EBM-specific protocols on multiple classes, is typically used as a classifier, as demonstrated on several computer vision tasks in [29]. In contrast, in this work our focus is the opposite: we propose employing trained classifiers, one for each specific class, as a generative energy-based model for the class-conditional distributions \(p(\mathbf{x}|y)\). We apply our TabEBM method for generative tasks on tabular data.

### Distinct Class-Specific Energy-Based Models

TabEBM is a class-conditional generative model \(p(\mathbf{x}|y)\) implemented using a set of EBMs, \(\{E_{1}(\mathbf{x}),E_{2}(\mathbf{x}),\ldots,E_{C}(\mathbf{x})\}\). Our approach assumes that the class-conditional density \(p(\mathbf{x}|y=c)\) is best modelled using its class-specific data \(\mathcal{X}_{c}\). Thus, for each class \(c\), we construct a class-specific EBM, \(E_{c}(\mathbf{x})\), using only the data from that class, \(\mathcal{X}_{c}\), such that \(p(\mathbf{x}|y=c)\propto\exp(-E_{c}(\mathbf{x}))\).

We derive each class-specific EBM \(E_{c}(\mathbf{x})\) by training a classifier on a novel task and reinterpreting its logits. Specifically, for each class \(c\), we propose a _surrogate binary classification task_ to determine if a sample belongs to class \(c\) by comparing \(\mathcal{X}_{c}\) against a set of surrogate negative samples \(\mathcal{X}_{c}^{\text{neg}}\), which we show in Figure 3. Specifically, we generate the negative samples at the corners of a hypercube in \(R^{D}\). For each dimension \(d\), the coordinates of a negative sample are either \(\alpha_{\text{dist}}^{\text{neg}}{}_{d}\) or \(-\alpha_{\text{dist}}^{\text{neg}}{}_{d}\), where \(\alpha_{\text{dist}}^{\text{neg}}\) is a fixed constant and \(\sigma_{d}\) is the standard deviation of dimension \(d\). For example, in \(R^{3}\), a negative sample might have coordinates \([\alpha_{\text{dist}}^{\text{neg}}\sigma_{1},\alpha_{\text{dist}}^{\text{ neg}}\sigma_{2},-\alpha_{\text{dist}}^{\text{neg}}\sigma_{3}]\). Placing the negative samples at the corners of a hypercube ensures they are easily distinguishable from the real data, which is crucial for an accurate energy function (see Appendix D.1.1). This placement is also robust to variations in the number and distance of the negative samples (see Appendices D.1.2 and D.1.3).

We create the combined dataset \(\mathcal{D}_{c}\) for the surrogate binary classification task by labelling \(\mathcal{X}_{c}\) as \(1\) and \(\mathcal{X}_{c}^{\text{neg}}\) as \(0\):

\[\mathcal{D}_{c}=(\mathcal{X}_{c}\cup\mathcal{X}_{c}^{\text{neg}},\{1\}^{| \mathcal{X}_{c}|}\cup\{0\}^{|\mathcal{X}_{c}^{\text{neg}}|})\] (1)

We then train a binary classifier \(f_{\theta}^{c}(\cdot)\) on \(\mathcal{D}_{c}\) and use it to construct the class-specific energy \(E_{c}(\mathbf{x})\) for class \(c\). To do this, we reinterpret the logits \(\{f_{\theta}^{c}(\mathbf{x})[0],f_{\theta}^{c}(\mathbf{x})[1]\}\) of the trained binary classifier as components of an approximated joint distribution for the surrogate binary task:

Figure 3: The class-specific energy function \(E_{c}(\mathbf{x})\) from the surrogate binary task, where the blue region represents low energy (i.e., high data density). Placing the negative samples in a hypercube distant from the data results in an accurate energy function.

\[p_{c}(\mathbf{x},0)=\frac{\exp(f_{\theta}^{c}(\mathbf{x})[0])}{Z},\quad p_{c}( \mathbf{x},1)=\frac{\exp(f_{\theta}^{c}(\mathbf{x})[1])}{Z}\quad\text{($Z$ is the normalisation constant)}\] (2)

Next, we derive the approximated distribution \(p_{c}(\mathbf{x})\) by marginalisation:

\[p_{c}(\mathbf{x}) =p_{c}(\mathbf{x},0)+p_{c}(\mathbf{x},1)\] \[=\frac{\exp(f_{\theta}^{c}(\mathbf{x})[0])+\exp(f_{\theta}^{c}( \mathbf{x})[1])}{Z}\] \[=\frac{\exp\left(\log\left(\exp(f_{\theta}^{c}(\mathbf{x})[0])+ \exp(f_{\theta}^{c}(\mathbf{x})[1])\right)\right)}{Z}\] \[\to E_{c}(\mathbf{x}) =-\log\left(\exp(f_{\theta}^{c}(\mathbf{x})[0])+\exp(f_{\theta}^{ c}(\mathbf{x})[1])\right)\] (TabEBM class-specific energy)

For the binary classifier \(f_{\theta}^{c}(\cdot)\) in the surrogate binary classification, we use TabPFN [33], a pre-trained tabular in-context model. Note that TabPFN is intended for inference only, with no updates to its parameters (see Section 4 for more details about TabPFN). In this context, "training" the TabPFN classifier is analogous to the K-Nearest Neighbour algorithm, which simply performs inference based on a training dataset provided to the model. We apply TabPFN multiple times on separate datasets \(\{\mathcal{D}_{1},\mathcal{D}_{2},\dots,\mathcal{D}_{C}\}\) to obtain multiple classifiers \(\{f_{\theta}^{1},f_{\theta}^{2},\dots,f_{\theta}^{C}\}\). In Section 3.4, we explore why reinterpreting TabPFN's logits, trained on our surrogate binary tasks, can be useful for estimating an energy function. We emphasise that TabEBM is a general method, capable of using any gradient-based classifier that computes logits (using Equation (3)), and is not limited to TabPFN.

**Generating data with TabEBM** involves two steps. First, we sample a class \(c\) from the empirical distribution \(c\sim p(y)\). Then, we sample a data point \(\mathbf{x}\) from the conditional distribution \(\mathbf{x}\sim p(\mathbf{x}|y=c)\) approximated by the class-specific energy-based model \(E_{c}(\mathbf{x})\), as outlined in Algorithm 1. We employ Stochastic Gradient Langevin Dynamics (SGLD) [84] to perform this sampling. SGLD is an efficient method for high-dimensional data, combining stochastic gradient descent (SGLD) with Langevin dynamics. The update rule for SGLD at each iteration is:

\[\mathbf{x}_{t+1}=\mathbf{x}_{t}-\frac{\eta}{2}\nabla E(\mathbf{x}_{t})+ \epsilon_{t},\quad\epsilon_{t}\sim\mathcal{N}(0,\eta\mathbf{I})\] (4)

where a Gaussian noise term \(\epsilon_{t}\) introduces randomness into the sampling process, enhancing the exploration of the distribution. In practice, the step size and the noise standard deviations are often chosen separately, resulting in a biased sampler that allows for faster training. Appendix D.2 further shows that TabEBM is stable to hyperparameters for the sampling process.

In our method, SGLD performs iterative augmentation. We start by sampling close to real data and iteratively adjust these synthetic data points, steering them towards regions of higher probability under the learned energy model. TabEBM enables sampling from any specified class distribution, including the original class distribution, which is crucial for data augmentation.

```
0: Training data \(\mathcal{X}_{c}\) for class \(c\), step size \(\alpha_{\text{step}}\), noise scale \(\alpha_{\text{noise}}\), initial perturbation \(\sigma_{\text{start}}\), number of steps \(T\)
0: Set of synthetic samples for class \(c\)  Initialise a surrogate binary classification task and train the model
1: Assign new labels to the samples \(\mathcal{X}_{c}\) from class \(c\), setting them to class 1
2: Generate a set of surrogate negative samples \(\mathcal{X}_{c}^{\text{neg}}\) and assign them class 0 labels
3: Train a binary classifier \(f_{\theta}^{c}\) on the dataset \(\mathcal{D}_{c}=(\mathcal{X}_{c}\cup\mathcal{X}_{c}^{\text{neg}},\{1\}^{| \mathcal{X}_{c}|}\cup\{0\}^{|\mathcal{X}_{c}^{\text{neg}}|})\)
4: _Synthesize samples using Stochastic Gradient Langevin Dynamics (SGLD)_
5: Initialise synthetic data points \(\mathbf{x}_{0}^{\text{synth}}\) by sampling from \(\mathcal{N}(\mathcal{X}_{c},\sigma_{\text{start}}^{2}\mathbf{I})\)
6:for each iteration \(t=0,1,\dots,T-1\)do
7:\(E_{c}(\mathbf{x}_{t}^{\text{synth}})=-\log\left(\exp(f_{\theta}^{c}(\mathbf{x }_{t}^{\text{synth}})[0])+\exp(f_{\theta}^{c}(\mathbf{x}_{t}^{\text{synth}})[1 ])\right)\)
8:\(\mathbf{x}_{t+1}^{\text{synth}}=\mathbf{x}_{t}^{\text{synth}}-\alpha_{\text{ step}}\nabla E_{c}(\mathbf{x}_{t}^{\text{synth}})+\mathcal{N}(0,\alpha_{\text{ noise}}^{2}\mathbf{I})\)
9:endfor
10:return\(\mathbf{x}_{T}^{\text{synth}}\) as the generated synthetic data for class \(c\) ```

**Algorithm 1** TabEBM sampling from Class-Specific EBM \(E_{c}(\mathbf{x})\)Experiments

We evaluate TabEBM by focusing on four research questions:

* **Data Augmentation Improvement (Q1, Section 3.1):** Can TabEBM generate synthetic data that improves the accuracy of downstream predictors via data augmentation?
* **Statistical Fidelity (Q2, Section 3.2):** Can TabEBM generate synthetic data with high statistical fidelity (i.e., with similar distributions to those of real data)?
* **Privacy Preservation (Q3, Section 3.3):** Can TabEBM generate synthetic data that finds a competitive trade-off between downstream performance and privacy preservation?
* **Understanding TabEBM's energy formulation (Q4, Section 3.4):** Why is TabEBM's class-specific energy effective, and how do the proposed surrogate tasks influence this?

**Datasets.** We utilise eight open-source tabular datasets from OpenML [7] across five domains: Medicine, Chemistry, Engineering, Language and Economics. As TabPFN utilises many small-size OpenML datasets in its meta-validation [33], it can lead to data leakage when evaluating TabEBM. Therefore, to provide fair comparisons, we select six additional leakage-free datasets from UCI [22]. These diverse datasets contain 7 to 77 features and 698 to 5500 samples across 2 to 26 classes. Five datasets contain both numerical and categorical features, while the remaining are numerical only. We further enlarge the evaluation scope by varying the degrees of data availability (i.e., \(N_{\text{real}}\)), leading up to 33 different test cases for the eight OpenML datasets. Appendix B.1 provides detailed descriptions.

**Benchmark generators.** We compare TabEBM against eight existing tabular data generation methods of eight different categories: (i) a standard interpolation method SMOTE [13]; (ii) a Variational Autoencoders (VAE) based method TVAE [85]; (iii) a Generative Adversarial Networks (GAN) method CTGAN [85]; (iv) a normalising flow model Neural Spine Flows (NFLOW) [24]; (v) a diffusion model TabDDPM [42]; (vi) a tree-based method Adversarial Random Forests (ARF) [83]; (vii) a Graph Neural Network (GNN) based method GOGGLE [47]; and (viii) a Prior-Data Fitted Networks (PFN) based method TabPFGen [48]. Furthermore, we also include a "Baseline" model, where no data augmentation is applied (i.e., only real data is used to train downstream predictors). In Appendix B.6, we detail the settings used for TabEBM and all other generators.

**Downstream predictors.** We select six representative downstream predictors, including three standard baselines: Logistic Regression (LR) [16], KNN [27] and MLP [28]; two tree-based methods: Random Forest (RF) [10] and XGBoost [14]; and a PFN method: TabPFN [33].

**General experimental setup.** For each dataset of \(N\) samples, we first split it into stratified train and test sets. We create large test sets to reduce the likelihood that the model's performance is accidentally inflated due to a small, unrepresentative set of samples [69], and thus the test size is computed via \(N_{\text{test}}=\min\left(\frac{N}{2},500\right)\). The full train set approximates the upper bound of the quality of synthetic data, and we call this set "oracle". We subsample the full train set to simulate different levels of data availability, thus the subset size \(N_{\text{real}}\) varies over \(\{20,50,100,200,500\}\). We split each subset into stratified training and validation sets with a ratio of 4:1. We provide detailed descriptions of data splitting in Appendix B.2 and preprocessing in Appendix B.3. We repeat the splitting ten times, summing up to 10 runs per subset size. The reported results are averaged by default over ten runs on the test sets. When aggregating results across datasets, we use the average distance to the minimum (ADTM) metric via affine renormalisation between the top-performing and worse-performing models [30, 54]. We provide the evaluation results averaged over six downstream predictors for a general conclusion, and the fine-grained numerical results for each predictor are in Appendix D.

**Data augmentation setup.** Given \(N_{\text{real}}\) real samples, we first train generators on the real training data and then generate \(N_{\text{syn}}\) synthetic samples. For training the downstream predictors, we expand the real training split by adding the synthetic samples. The real validation data is used for early stopping, and the real test set is used for evaluating the predictor's performance. The optimal \(N_{\text{syn}}\) remains an open problem for tabular data [51, 71, 31]. Prior works [47, 48] mainly use synthetic sets with equivalent sizes to the real sets (i.e., \(N_{\text{real}}=N_{\text{syn}}\)). However, we observe that \(N_{\text{real}}=N_{\text{syn}}\) can lead to highly unstable results, especially on small datasets that we investigate. Recent work has used different \(N_{\text{syn}}\) for various generators, such as by applying post-processing [31, 71]. In this work, we want to provide a head-to-head comparison of the effect of data augmentation across subsampled datasets of varying sizes \(N_{\text{real}}\in\{20,50,100,200,500\}\). Therefore, we perform data augmentation with a large synthetic set (\(N_{\text{syn}}=500\)) across all splits, and the synthetic data has the same class distribution as the real training data. We provide an illustrative figure of the data splitting setup in Appendix B.2.

[MISSING_PAGE_FAIL:6]

Baseline with notable improvements, particularly in datasets with more than ten classes. In contrast, an increased number of classes tends to cause a performance degradation in the benchmark generators.

**TabEBM is robust on imbalanced datasets.** For the three binary OpenML datasets (i.e., "biodeg", "steel" and "stock"), we adjust the class distribution in the training data to vary the class imbalance, while keeping the test data fixed. Figure 5 shows that TabEBM consistently outperforms Baseline, while the other generators exhibit performance degradation as data imbalance increases.

**TabEBM is computationally efficient.** Figure 6 shows the trade-off between accuracy and the time needed for generating stratified synthetic data (for data augmentation). We measure the total duration of (i) training the model and (ii) generating 500 synthetic samples. The results show that TabEBM is practical, as it achieves higher downstream accuracy with lower time costs.

### Statistical Fidelity (Q2)

We evaluate the fidelity of synthetic data by measuring the similarity of synthetic data to real _train_ data and to real _test_ data (Figure 6). We evaluate this similarity via (i) _average inverse of the Kullback-Leibler Divergence_ (inverse KL) [17], (ii) p-value of _Kolmogorov-Smirnov test_ (KS test) [39] and (iii) p-value of _Chi-squared test_ (\(\chi^{2}\) test) [55]. For full numerical results, including \(\chi^{2}\) test, see Appendix D.6. For all three metrics, a bigger value denotes that synthetic data is more likely to have the same distribution as real data.

In Figure 6 (a1&a2), TabEBM consistently exhibits the highest accuracy and distribution similarity between real train data and synthetic data, indicating that TabEBM learns the distributions of real train data better than benchmark generators. In Appendix D.6, we further show that TabEBM remains the most competitive method in similarity between real test data and synthetic data. This indicates that TabEBM can extrapolate beyond real train data and thus generate synthetic data from a more

Figure 4: Mean normalised balanced accuracy improvement (%) across different sample sizes (**Left**) and across datasets with varying numbers of classes (**Right**). Because TabPFGen is not applicable for datasets with more than ten classes, we plot short bars at zeros for visual clearance. Positive values indicate that the generator improves downstream classification performance. TabEBM generally outperforms benchmark generators across varying sample sizes and number of classes.

general distribution that aligns with both train and test data. This extrapolation ability also explains why TabEBM can outperform Baseline via data augmentation (Section 3.1).

### Privacy Preservation (Q3)

More broadly, data privacy is a critical concern for organisations and governments handling sensitive data [75]. Privacy-preserving synthetic data allows researchers and practitioners to bypass ethical and logistical issues while enabling model training and testing [38]. We further explore the use of TabEBM-generated data for data sharing, where only synthetic data is accessible for downstream users [75, 89, 23, 42, 23]. In this case, downstream models are trained exclusively on synthetic data.

Specifically, we evaluate synthetic data via three metrics: (i) _balanced accuracy_ of downstream predictors trained with only synthetic data (i.e., train-on-synthetic, test-on-real [85, 42, 87]); (ii) median Distance to Closest Record (DCR) [88], where a greater DCR denotes synthetic data is less likely to be copied from real data; and (iii) \(\delta\)-presense [62], where a smaller value denotes a lower re-identification risk for real data from synthetic data. Full numerical results are in Appendix D.7.

Figure 7 (b1&b2) shows that TabEBM consistently finds a better trade-off between accuracy and privacy preservation. Notably, the "train-on-synthetic, test-on-real" scenario poses a greater challenge for generators in achieving high accuracy because real data is inaccessible for model training and data augmentation. Despite this difficulty, TabEBM is the only generator that surpasses the overall performance of training on real data (i.e., Baseline). The relatively high DCR for TabEBM indicates that it can extrapolate beyond real train data, aligning with the finding that TabEBM's synthetic data is statistically similar to real test data (Section 3.2). These results further suggest that TabEBM learns the general distribution of real data, and can generate high-quality synthetic data suitable for various purposes, including privacy preservation.

### Why is TabEBM effective for estimating Energy-Based Models? (Q4)

Having established that TabEBM excels in data augmentation, we explore why classifier logits can be useful when reinterpreted as a class-conditional energy function. Figure 8 shows the logit distribution of TabPFN trained on surrogate binary tasks and the corresponding energy function of TabEBM (with TabPFN as the binary classifier) as the Euclidean distance from the real data increases.

We found it essential to place the negative samples far from the real data, since TabPFN, which is pre-trained to approximate Bayesian inference [33], has its confi

Figure 8: (**Left**) Logit distribution of TabPFN trained on our surrogate binary tasks at increasing distances from the real data (on “steel”). (**Right**) The corresponding unnormalised density approximated by TabEBM. TabEBM assigns higher density closer to the real data.

Figure 7: (**a1&a2):** Median inverse KL and KS test vs. mean normalised balanced accuracy improvement (%) between real train data and synthetic data. (**b1&b2):** Median DCR and \(\delta\)-presence vs. mean normalised balanced accuracy change (%) between real train data and synthetic data. Note that “accuracy improvement” is for data augmentation, and “accuracy change” is for data sharing. Complete results with standard deviations are in Appendix D.4. TabEBM generates high-fidelity synthetic data that can also be used for privacy preservation.

dence influenced by the distance from the training data [53]. Figure 8 (left) shows that TabPFN outputs high logit values near the real data. As the distance from the real data increases, the logit \(f(\mathbf{x})[1]\) decreases smoothly until the two logits become similar, making the classifier uncertain (because the class probabilities become equal). Figure 8 (right) shows that TabEBM's inferred density drops significantly as the maximum logit decreases, because \(p_{c}(\mathbf{x})\propto(\exp(f(\mathbf{x})[0])+\exp(f(\mathbf{x})[1]))\) from Equation (3). Since SGLD sampling performs gradient ascent on the density, the TabEBM-generated samples will be close to the real data. These findings are consistent across datasets (see Appendix D.3), where TabPFN's logits remain positive, with similar ranges and a relatively constant sum as distance increases, warranting further investigation. Overall, TabPFN's distance-based uncertainty is useful for inferring accurate energy functions within our TabEBM framework. Since TabEBM can be paired with any other gradient-based classifier that produces logits, we leave these extensions for future work.

## 4 Discussion & Related Work

Section 3 showed that TabEBM efficiently generates high-fidelity data that can effectively improve the downstream performance via data augmentation. In Table 2, we further provide a summary of tabular data generative models analysed from three important perspectives: (i) _Training:_ the type of distribution that the generators learn (crucial for preserving the original training label distribution), and the training costs associated with learning; (ii) _Generation_: do the generators employ class-specific models (reflecting their capability to capture unique features essential for label-invariant generation), and do models support stratified generation (crucial for effective data augmentation); (iii) _Practicability:_ the scalability of the generators with respect to the number of classes (a common requirement in real-world multi-class tasks), and consistent downstream performance improvement across different class sizes.

**Generative Models for Tabular Data.** The common paradigm for tabular data generation is to adapt Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) [85; 63]. For instance, TableGAN employs a convolutional neural network to optimise the label quality [63], and TVAE is introduced in [85] as a variant of VAE for tabular data. However, these methods learn the joint distribution and thus cannot preserve the stratification of the original data (Appendix C). CTGAN [85] refines the generation to be class-conditional. The recent ARF [83] is an adversarial variant of random forest for density estimation, and GOGGLE [47] enhances VAE by learning relational structure with a Graph Neural Network (GNN). Some recent work focuses on generation with denoising diffusion models [42; 87; 44; 40]. For instance, TabDDPM [42] demonstrates that diffusion models can approximate typical distributions of tabular data. Although these class-conditional models can preserve the label distribution, they struggle to outperform Baseline and standard SMOTE in data augmentation [71; 48].

We attribute the performance degradation in current class-conditional models to their reliance on a single shared model to approximate all class-conditional densities. For instance, another promising generative approach uses pre-trained models like Prior-Data Fitted Networks (PFNs), and the recent TabPFGen [48] adapts such models into one shared class-conditional generator. However, TabPFGen's shared generator can lead to inaccurate density estimates, particularly in high-noise and class-imbalance situations (see examples in Appendix C). As noise increases, TabPFGen's inferred densities fluctuate significantly and diverge from the true data distributions. In contrast, TabEBM uses class-specific EBMs to model each class's marginal distributions, and the results in Appendix C reveal that our design choice reduces the impact of noise and data imbalance. TabEBM focuses on approximating and generating for one class at a time, remaining unaffected by noise from other classes. Overall, our results demonstrate that TabEBM consistently improves performance across different datasets and sample sizes, outperforming TabPFGen. Moreover, TabPFGen is limited in usability (e.g., it supports only up to ten classes), while TabEBM scales to any number of classes.

In a broader context, some recent work attempts to adapt Large Language Models (LLMs) for tabular data generation [25; 71; 9]. However, data contamination is an inherent issue with such LLM-based models [19; 36; 18; 49]. As the pre-training data is not typically open-source, these models can have unfair advantages in downstream tasks (i.e., the full real dataset, including the real test data, may have been used for pre-training). Therefore, in this paper, we focus on models without support from LLMs, thus avoiding potential biases from data contamination.

**Data Augmentation (DA) for Tabular Data.** DA is an omnipresent technique in computer vision and natural language processing [82; 73; 72; 60; 26; 2]. However, DA for tabular data remains underexplored, and existing methods often perform poorly in real-world tasks, sometimes even reduce performance [51]. Recent studies show that using the same transformations across all classes leads to varied performance impacts [3; 41], indicating that data augmentation effects are class-specific and suggesting that different classes may require distinct augmentations. Given the lack of symmetries in tabular data, we believe this class-dependent effect is even more pronounced. Therefore, we propose TabEBM as a class-specific generative model to produce tailored augmentations for each class.

**Prior-fitted Networks (PFNs) for Tabular Data.** Recent work proposes to approximate the posterior predictive distribution with transformers [59; 33; 61; 79; 20]. PFNs can be adapted for various purposes by pre-training the transformer with corresponding "prior data", and then it can make in-context predictions with unseen downstream data. For instance, TabPFN is a variant that is pre-trained on a prior designed for tabular data [33]. We note that prior data is different to synthetic data in this paper. Specifically, prior data refers to manually crafted fake data (e.g., \(y=2x\)) with no real-world semantics. In contrast, synthetic data from generators is expected to have the same semantics as real data. Inspired by TabPFN's success in small-size classification tasks, TabEBM converts TabPFN into multiple EBMs that learn the marginal distribution for each class. The training-free nature of TabPFN enables TabEBM to generate high-quality tabular data without introducing extra training costs. Additionally, our class-specific design lets TabEBM surpass TabPFN's limits and scale to more than ten classes.

**Limitations and Future Work.** TabEBM is a general method that relies on an underlying binary classifier, and as such, its strengths and weaknesses are directly tied to this classifier. We used TabPFN because it is a well-established open-source pre-trained model for tabular data. Therefore, TabEBM inherits some of TabPFN's limitations, particularly in scaling to a larger number of features. TabEBM can handle datasets with over 1000 samples, overcoming TabPFN's limitation, as it processes one class at a time. In Appendix D.5.3, we show that TabEBM outperforms other generators on larger datasets, though the performance gains decrease as the sample size increases. Although we implement TabEBM with TabPFN in this paper, we stress that TabEBM is compatible with any classifier that can be adapted into EBMs, as described in Section 2. As foundational models for tabular data evolve [81], new models capable of handling more features and samples are expected. Integrating them into TabEBM will enhance its ability to manage high-dimensional datasets, increasing its versatility and utility. Finally, note that, generators that are limited in modelling multivariate distributions may still perform well on univariate fidelity metrics, which is a standard approach to evaluating such models. However, evaluating their ability to learn more complex, high-order, relationships between features remains an open research question [78], which we leave for future work.

## 5 Conclusion

We introduced TabEBM, the first tabular data augmentation method that creates class-specific EBM generators, learning the marginal distribution for each class separately. We also provide the first comprehensive analysis of tabular data augmentation across various dataset sizes. Our results demonstrate that TabEBM improves downstream performance through data augmentation on real-world datasets, outperforming other benchmark generators. The statistical evaluation confirms that TabEBM generates high-fidelity synthetic data, particularly for small datasets. We release our method as an open-source library, allowing users to generate data immediately without additional training.

\begin{table}
\begin{tabular}{l c|c c|c c|c c} \hline \multirow{2}{*}{Methods} & \multirow{2}{*}{Category} & \multicolumn{2}{c|}{Training} & \multicolumn{2}{c|}{Generation} & \multicolumn{2}{c}{Practicability} \\ \cline{3-8}  & & Learned & \multicolumn{2}{c|}{Training-free} & \multicolumn{2}{c|}{Class-specific} & \multicolumn{2}{c|}{Stratified} & \multicolumn{2}{c}{Unlimited} & \multicolumn{1}{c}{ACC improve} & \multicolumn{1}{c}{ACC improve} \\  & & distribution & & models & generation & classes & (\(\leq\) 10 classes) & (\(>\) 10 classes) \\ \hline SMOTE [13] & Interpolation & N/A & ✓ & N/A & ✓ & ✓ & ✗ & ✗ \\ TVAE [85] & VAE & \(p(\mathbf{x},y)\) & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ CTGAN [85] & GAN & \(p(\mathbf{x},y)\) & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ \\ NTLOW [24] & Normal Flows & \(p(\mathbf{x},y)\) & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ TabDDPM [42] & Diffusion & \(p(\mathbf{x}\,y)\) & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ \\ ARF [83] & Random Forest & \(p(\mathbf{x},y)\) & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ \\ GOODGLLE [47] & GNN & \(p(\mathbf{x}\,y)\) & ✗ & ✗ & ✓ & ✗ & ✗ \\ TabFPen [48] & PFN & \(p(\mathbf{x}\,y)\) & ✓ & ✗ & ✓ & ✗ & ✗ \\ \hline
**TabEBM (Ours)** & **PFN** & \(p(\mathbf{x}\,y)\) & ✓ & ✓ & ✓ & ✓ & ✗ \\ \hline \end{tabular}
\end{table}
Table 2: **Comparison of the properties between TabEBM and prior tabular generative methods. TabEBM has novel design rationales of training-free class-specific models, and TabEBM is highly practicable with wide applicability and consistent accuracy improvement.**

## Acknowledgments and Disclosure of Funding

The authors would like to thank Francisco Vargas, Randall Balestriero, and Otilia Stretcu for their insightful discussions and valuable input early in the project. NS and MJ acknowledge the support of the U.S. Army Medical Research and Development Command of the Department of Defense; through the FY22 Breast Cancer Research Program of the Congressionally Directed Medical Research Programs, Clinical Research Extension Award GRANT13769713. Opinions, interpretations, conclusions, and recommendations are those of the authors and are not necessarily endorsed by the Department of Defense.

## References

* Alami et al. [2020] Hassane Alami, Lysanne Rivard, Pascale Lehoux, Steven J Hoffman, Stephanie Bernadette Mafalda Cadeddu, Mathilde Savoldelli, Mamane Abdoulaye Samri, Mohamed Ali Ag Ahmed, Richard Fleet, and Jean-Paul Fortin. Artificial intelligence in health care: laying the foundation for responsible, sustainable, and inclusive innovation in low-and middle-income countries. _Globalization and Health_, 16:1-6, 2020.
* Antoniou et al. [2017] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. _arXiv preprint arXiv:1711.04340_, 2017.
* Balestriero et al. [2022] Randall Balestriero, Leon Bottou, and Yann LeCun. The effects of regularization and data augmentation are class dependent. _Advances in Neural Information Processing Systems_, 35:37878-37891, 2022.
* Bansal et al. [2022] Ms Aayushi Bansal, Dr Rewa Sharma, and Dr Mamta Kathuria. A systematic review on data scarcity problem in deep learning: solution and applications. _ACM Computing Surveys (CSUR)_, 54(10s):1-29, 2022.
* Baxevanis et al. [2020] Andreas D Baxevanis, Gary D Bader, and David S Wishart. _Bioinformatics_. John Wiley & Sons, 2020.
* Bespalov et al. [2016] Anton Bespalov, Thomas Steckler, Bruce Altevogt, Elena Koustova, Phil Skolnick, Daniel Deaver, Mark J Millan, Jesper F Bastlund, Dario Doller, Jeffrey Witkin, et al. Failed trials for central nervous system disorders do not necessarily invalidate preclinical models and drug targets. _Nature Reviews Drug Discovery_, 15(7):516-516, 2016.
* Bischl et al. [2021] B. Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael Gomes Mantovani, Jan N. van Rijn, and Joaquin Vanschoren. Openml benchmarking suites. _Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS 2021)_, 2021.
* Borisov et al. [2022] Vadim Borisov, Tobias Leemann, Kathrin Sessler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. Deep neural networks and tabular data: A survey. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* Borisov et al. [2022] Vadim Borisov, Kathrin Sessler, Tobias Leemann, Martin Pawelczyk, and Gjergji Kasneci. Language models are realistic tabular data generators. In _The Eleventh International Conference on Learning Representations_, 2022.
* Breiman [2001] Leo Breiman. Random forests. _Machine learning_, 45(1):5-32, 2001.
* Cai et al. [2020] Chenjing Cai, Shiwei Wang, Youjun Xu, Weilin Zhang, Ke Tang, Qi Ouyang, Luhua Lai, and Jianfeng Pei. Transfer learning for drug discovery. _Journal of Medicinal Chemistry_, 63(16):8683-8694, 2020.
* Chang et al. [2022] Rees Chang, Yu-Xiong Wang, and Elif Ertekin. Towards overcoming data scarcity in materials science: unifying models and datasets with a mixture of experts framework. _npj Computational Materials_, 8(1):242, 2022.
* Chawla et al. [2002] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic minority over-sampling technique. _Journal of artificial intelligence research_, 16:321-357, 2002.

* [14] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In _Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining_, pages 785-794, 2016.
* [15] Tadeusz Ciecierski-Holmes, Ritvij Singh, Miriam Axt, Stephan Brenner, and Sandra Barreit. Artificial intelligence for strengthening healthcare systems in low-and middle-income countries: a systematic scoping review. _npj Digital Medicine_, 5(1):162, 2022.
* [16] David R Cox. The regression analysis of binary sequences. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 20(2):215-232, 1958.
* [17] Imre Csiszar. I-divergence geometry of probability distributions and minimization problems. _The annals of probability_, pages 146-158, 1975.
* [18] Jasper Dekoninck, Mark Niklas Muller, Maximilian Baader, Marc Fischer, and Martin Vechev. Evading data contamination detection for language models is (too) easy. _arXiv preprint arXiv:2402.02823_, 2024.
* [19] Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark B. Gerstein, and Arman Cohan. Investigating data contamination in modern benchmarks for large language models. In _North American Chapter of the Association for Computational Linguistics_, 2024.
* [20] Samuel Dooley, Gurnoor Singh Khurana, Chirag Mohapatra, Siddartha V Naidu, and Colin White. Forecastpfn: Synthetically-trained zero-shot forecasting. _Advances in Neural Information Processing Systems_, 36, 2024.
* [21] Georgios Douzas and Fernando Bacao. Effective data generation for imbalanced learning using conditional generative adversarial networks. _Expert Systems with applications_, 91:464-471, 2018.
* [22] Dheeru Dua and Casey Graff. Uci machine learning repository, 2017.
* [23] Larry A Dunning and Ray Kresman. Privacy preserving data sharing with anonymous id assignment. _IEEE transactions on information forensics and security_, 8(2):402-413, 2012.
* [24] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. _Advances in neural information processing systems_, 32, 2019.
* a survey. _Transactions on Machine Learning Research_, 2024.
* [26] Steven Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Sorous Vosoughi, Teruko Mitamura, and Eduard Hovy. A survey of data augmentation approaches for nlp. In _Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021_, pages 968-988, 2021.
* [27] Evelyn Fix. _Discriminatory analysis: nonparametric discrimination, consistency properties_, volume 1. USAF school of Aviation Medicine, 1985.
* [28] Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models for tabular data. _Advances in Neural Information Processing Systems_, 34:18932-18943, 2021.
* [29] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In _International Conference on Learning Representations_, 2019.
* [30] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep learning on typical tabular data? _Advances in neural information processing systems_, 35:507-520, 2022.
* [31] Lasse Hansen, Nabeel Seedat, Mihaela van der Schaar, and Andrija Petrovic. Reimagining synthetic tabular data generation through data-centric ai: A comprehensive benchmark. _Advances in Neural Information Processing Systems_, 36:33781-33823, 2023.

* [32] Mikel Hernandez, Gorka Epelde, Ane Alberdi, Rodrigo Cilla, and Debbie Rankin. Synthetic data generation for tabular health records: A systematic review. _Neurocomputing_, 493:28-45, 2022.
* [33] Noah Hollmann, Samuel Muller, Katharina Eggensperger, and Frank Hutter. TabPFN: A transformer that solves small tabular classification problems in a second. In _The Eleventh International Conference on Learning Representations_, 2023.
* [34] J. D. Hunter. Matplotlib: A 2d graphics environment. _Computing in Science & Engineering_, 9(3):90-95, 2007.
* [35] Dipendra Jha, Kamal Choudhary, Francesca Tavazza, Wei-keng Liao, Alok Choudhary, Carelyn Campbell, and Ankit Agrawal. Enhancing materials property prediction by leveraging computational and experimental data using deep transfer learning. _Nature communications_, 10(1):5316, 2019.
* [36] Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. Investigating data contamination for pre-training language models. _arXiv preprint arXiv:2401.06059_, 2024.
* [37] Xiangjian Jiang, Andrei Margeloiu, Nikola Simidjievski, and Mateja Jamnik. Protogate: Prototype-based neural networks with global-to-local feature selection for tabular biomedical data. In _Proceedings of the 41st International Conference on Machine Learning (ICML)_, 2024.
* [38] Hao Jin, Yan Luo, Peilong Li, and Jomol Mathew. A review of secure and privacy-preserving medical data sharing. _IEEE access_, 7:61656-61669, 2019.
* [39] Marvin Karson. Handbook of methods of applied statistics. volume i: Techniques of computation descriptive methods, and statistical inference. volume ii: Planning of surveys and experiments. im chakravarti, rg laha, and j. roy, new york, john wiley; 1967., 1968.
* [40] Jayoung Kim, Chaejeong Lee, and Noseong Park. Stasy: Score-based tabular data synthesis. In _The Eleventh International Conference on Learning Representations_, 2022.
* [41] Polina Kirichenko, Mark Ibrahim, Randall Balestriero, Diane Bouchacourt, Shanmukha Ramakrishna Vedantam, Hamed Firooz, and Andrew G Wilson. Understanding the detrimental class-level effects of data augmentation. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. In _International Conference on Machine Learning_, pages 17564-17579. PMLR, 2023.
* [43] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* [44] Chaejeong Lee, Jayoung Kim, and Noseong Park. Codi: Co-evolving contrastive diffusion models for mixed-type tabular synthesis. In _International Conference on Machine Learning_, pages 18940-18956. PMLR, 2023.
* [45] Guillaume Lemaitre, Fernando Nogueira, and Christos K. Aridas. Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. _Journal of Machine Learning Research_, 18(17):1-5, 2017.
* [46] Roman Levin, Valeria Cherepanova, Avi Schwarzschild, Arpit Bansal, C Bayan Bruss, Tom Goldstein, Andrew Gordon Wilson, and Micah Goldblum. Transfer learning with deep tabular models. In _The Eleventh International Conference on Learning Representations_, 2022.
* [47] Tennison Liu, Zhaozhi Qian, Jeroen Berrevoets, and Mihaela van der Schaar. Goggle: Generative modelling for tabular data by learning relational structure. In _The Eleventh International Conference on Learning Representations_, 2023.
* [48] Junwei Ma, Apoorv Dankar, George Stein, Guangwei Yu, and Anthony Caterini. Tabpfgen-tabular data generation with tabpfn. In _NeurIPS 2023 Second Table Representation Learning Workshop_, 2023.

* [49] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)_, pages 157-165, 2022.
* [50] Bradley A Malin, Khaled El Emam, and Christine M O'Keefe. Biomedical data privacy: problems, perspectives, and recent advances. _Journal of the American medical informatics association_, 20(1):2-6, 2013.
* [51] Dionysis Manousakas and Sergul Aydore. On the usefulness of synthetic tabular data generation. In _Data-centric Machine Learning Research (DMLR) Workshop at the 40th International Conference on Machine Learning (ICML)_, 2023.
* [52] Andrei Margeloiu, Nikola Simidjievski, Pietro Lio, and Mateja Jamnik. Weight predictor network with feature selection for small sample tabular biomedical data. _AAAI Conference on Artificial Intelligence_, 2023.
* [53] Calvin McCarter. What exactly has tabpfn learned to do? In _ICLR Blogposts 2024_, 2024.
* [54] Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? _Advances in Neural Information Processing Systems_, 36, 2024.
* [55] Mary L McHugh. The chi-square test of independence. _Biochemia medica_, 23(2):143-149, 2013.
* [56] Daniele Micci-Barreca. A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems. _ACM SIGKDD explorations newsletter_, 3(1):27-32, 2001.
* [57] Daniel J Mollura, Melissa P Culp, Erica Pollack, Gillian Batino, John R Scheel, Victoria L Mango, Ameena Elahi, Alan Schweitzer, and Farouk Dako. Artificial intelligence in low-and middle-income countries: innovating global health radiology. _Radiology_, 297(3):513-520, 2020.
* [58] LaRonda L Morford, Christopher J Bowman, Diann L Blanset, Ingrid B Bogh, Gary J Chellman, Wendy G Halpern, Gerhard F Weinbauer, and Timothy P Coogan. Preclinical safety evaluations supporting pediatric drug development with biopharmaceuticals: strategy, challenges, current practices. _Birth Defects Research Part B: Developmental and Reproductive Toxicology_, 92(4):359-380, 2011.
* [59] Samuel Muller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In _International Conference on Learning Representations_, 2022.
* [60] Alhassan Mumani and Fuseini Mumani. Data augmentation: A comprehensive survey of modern approaches. _Array_, 16:100258, 2022.
* [61] Thomas Nagler. Statistical foundations of prior-data fitted networks. In _International Conference on Machine Learning_, pages 25660-25676. PMLR, 2023.
* [62] Mehmet Ercan Nergiz, Maurizio Atzori, and Christopher W Clifton. \(\delta\)-presence. _Encyclopedia of Cryptography, Security and Privacy_, pages 1-5, 2019.
* [63] Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, and Youngmin Kim. Data synthesis based on generative adversarial networks. _Proceedings of the VLDB Endowment_, 11(10), 2018.
* [64] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in Neural Information Processing Systems_, 32, 2019.

* [65] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. _Journal of Machine Learning Research_, 12:2825-2830, 2011.
* [66] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Dorogush, and Andrey Gulin. Catboost: unbiased boosting with categorical features. _Advances in neural information processing systems_, 31, 2018.
* [67] Zhaozhi Qian, Rob Davis, and Mihaela van der Schaar. Synthcity: a benchmark framework for diverse use cases of tabular synthetic data. _Advances in Neural Information Processing Systems_, 36, 2024.
* [68] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18434-18443, 2023.
* [69] Sebastian Raschka. Model evaluation, model selection, and algorithm selection in machine learning. _arXiv preprint arXiv:1811.12808_, 2018.
* [70] Vignesh Sampath, Inaki Maurtua, Juan Jose Aguilar Martin, and Aitor Gutierrez. A survey on generative adversarial networks for imbalance problems in computer vision tasks. _Journal of big Data_, 8:1-59, 2021.
* [71] Nabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated llm: Synergy of l lms and data curation for tabular augmentation in low-data regimes. In _Forty-first International Conference on Machine Learning_, 2024.
* [72] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning. _Journal of big data_, 6(1):1-48, 2019.
* [73] Connor Shorten, Taghi M Khoshgoftaar, and Borko Furht. Text data augmentation for deep learning. _Journal of big Data_, 8(1):101, 2021.
* [74] Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. _Information Fusion_, 81:84-90, 2022.
* [75] Theresa Stadler and Carmela Troncoso. Why the search for a privacy-preserving data sharing mechanism is failing. _Nature Computational Science_, 2(4):208-210, 2022.
* [76] Fahim Sufi. Addressing data scarcity in the medical domain: A gpt-based approach for synthetic data generation and feature extraction. _Information_, 15(5):264, 2024.
* [77] Haoyuan Sun, Navid Azizan, Akash Srivastava, and Hao Wang. Private synthetic data meets ensemble learning. _arXiv preprint arXiv:2310.09729_, 2023.
* [78] Ruibo Tu, Zineb Senane, Lele Cao, Cheng Zhang, Hedvig Kjellstrom, and Gustav Eje Henter. Causality for tabular data synthesis: A high-order structure causal benchmark framework. _arXiv preprint arXiv:2406.08311_, 2024.
* [79] Jordan Ubbens, Ian Stavness, and Andrew G Sharpe. Gpfn: Prior-data fitted networks for genomic prediction. _bioRxiv_, pages 2023-09, 2023.
* [80] Boris Van Breugel, Zhaozhi Qian, and Mihaela Van Der Schaar. Synthetic data, real errors: how (not) to publish and use synthetic data. In _International Conference on Machine Learning_, pages 34793-34808. PMLR, 2023.
* [81] Boris van Breugel and Mihaela van der Schaar. Why tabular foundation models should be a research priority. In _Forty-first International Conference on Machine Learning_, 2024.
* [82] David A Van Dyk and Xiao-Li Meng. The art of data augmentation. _Journal of Computational and Graphical Statistics_, 10(1):1-50, 2001.

* [83] David S Watson, Kristin Blesch, Jan Kapar, and Marvin N Wright. Adversarial random forests for density estimation and generative modeling. In _International Conference on Artificial Intelligence and Statistics_, pages 5357-5375. PMLR, 2023.
* [84] Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 681-688. Citeseer, 2011.
* [85] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. _Advances in neural information processing systems_, 32, 2019.
* [86] Aiqing Zhang and Xiaodong Lin. Towards secure and privacy-preserving data sharing in e-health systems via consortium blockchain. _Journal of medical systems_, 42(8):140, 2018.
* [87] Hengrui Zhang, Jiani Zhang, Zhengyuan Shen, Balasubramaniam Srinivasan, Xiao Qin, Christos Faloutsos, Huzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion in latent space. In _The Twelfth International Conference on Learning Representations_, 2023.
* [88] Zilong Zhao, Aditya Kunar, Robert Birke, and Lydia Y Chen. Ctab-gan: Effective table data synthesizing. In _Asian Conference on Machine Learning_, pages 97-112. PMLR, 2021.
* [89] Xu Zheng and Zhipeng Cai. Privacy-preserved data sharing towards multiple parties in industrial iots. _IEEE Journal on Selected Areas in Communications_, 38(5):968-979, 2020.

## Appendix

**TabEBM: A Tabular Data Augmentation Method with Distinct Class-Specific Energy-Based Models**

**Table of Contents**

* 1 A Broader Impact Statement
* B Reproducibility
* B.1 Datasets
* B.2 Data Splitting
* B.3 Data Preprocessing
* B.4 Software and Computing Resources
* B.5 TabEBM open-source library
* B.6 Implementation of Generators
* B.7 Implementation of Downstream Predictors
* C Limitations of Existing Generative Methods
* D Extended Experimental Results
* D.1 Ablations on the distribution of the surrogate negative samples
* D.1.1 Ablations on placing the negative samples
* D.1.2 Varying the number of negative samples
* D.1.3 Varying the distance of the negative samples
* D.2 Ablations on the sensitivity to the hyperparameters of SGLD sampling
* D.3 Distribution of Logits and Unnormalized Density in TabEBM
* D.4 Complete Trade-off Figures with Error Bars
* D.5 Results on Data Augmentation
* D.5.1 Results on eight OpenML datasets
* D.5.2 Results on six UCI Datasets
* D.5.3 Results on larger sample sizes
* D.6 Results on Statistical Fidelity
* D.6.1 Similarity between Real Train Data and Synthetic Data
* D.6.2 Similarity between Real Test Data and Synthetic Data
* D.7 Results on Privacy Preservation
* D.7.1 Downstream Accuracy in Data Sharing
* D.7.2 DCR Evaluation
* D.7.3 Delta-presence Evaluation
Broader Impact Statement

This paper introduces a novel data augmentation approach, TabEBM, that aims to advance the field of machine learning by addressing challenges in the low-sample-size regime. Furthermore, TabEBM offers an elegant solution to learning the unique features in generating samples for each class, leading to high-fidelity synthetic data that can effectively improve downstream performance. These characteristics can be particularly useful in data-scarce domains like healthcare (e.g., pre-clinical drug evaluation in early-stage clinical trials [6, 58]). Moreover, we also demonstrate that TabEBM is readily applicable for privacy-preserving data sharing in high-stake tasks [89, 77].

TabEBM's impact further extends to enabling broader machine learning applications in data-scarce domains, for instance, facilitating data analysis in clinical scenarios with limited access to data collection techniques. Improving the performance of machine learning models in such applications can further foster the uptake of more sophisticated ML approaches and, ultimately, help improve the quality of healthcare [1, 15, 57]. TabEBM can further facilitate research and enhance machine learning accessibility in various communities across societal and scientific domains. To this end, our work has only been evaluated in a strictly research setting. Further applications of our work in scenarios with sensitive data bear some risks. As TabEBM is a generative model, training models with the resulting generated samples can bias the downstream model. Therefore, this risk, together with other data privacy risks during downstream deployment, must be carefully managed.

## Appendix B Reproducibility

### Datasets

All eight datasets are publicly available on OpenML [7], and their details are listed in Table 3. To ensure consistent stratified data-splitting across all datasets, we remove classes with fewer than 10 samples. For example, the original "energy" dataset contains 14 classes with fewer than 10 samples, which could result in a validation set lacking samples from these classes, leading to unstratified data splitting.

### Data Splitting

Figure 9 shows the data splitting setup used across all datasets. Note that data sharing (Section 3.3) shares the same data splitting as data augmentation, except that the "Training set" and "Validation set" containing real data are no longer used for training the downstream predictors.

\begin{table}
\begin{tabular}{l r r r r r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{OpenML ID} & \multicolumn{2}{c}{Not evaluated in} & \multirow{2}{*}{\# Samples (\(N\))} & \multicolumn{2}{c}{\# Features (\(D\))} & \multicolumn{2}{c}{\# Classes} & \multicolumn{2}{c}{\# Samples per class} & \multicolumn{1}{c}{\# Samples per class} \\  & & \multicolumn{1}{c}{TabFFN [33]} & \multicolumn{1}{c}{\# Samples (\(N\))} & \multicolumn{1}{c}{\# Features (\(D\))} & \multicolumn{1}{c}{\# Classes} & \multicolumn{1}{c}{\(N/D\)} & \multicolumn{1}{c}{\(\phantom{\#}\)(Min)} & \multicolumn{1}{c}{\(\phantom{\#}\)(Max)} \\ \hline \multicolumn{8}{c}{At most 10 classes} \\ \hline protein & 40966 & ✗ & 1.080 & 77 & 8 & 14.03 & 105 & 150 \\ fourier & 14 & ✗ & 2,000 & 76 & 10 & 26.32 & 200 & 200 \\ biodge & 1494 & ✗ & 1,055 & 41 & 2 & 25.73 & 356 & 699 \\ steel & 1504 & ✗ & 1,941 & 33 & 2 & 58.82 & 673 & 1,268 \\ stock & 841 & ✗ & 950 & 9 & 2 & 105.56 & 462 & 488 \\ \hline \multicolumn{8}{c}{More than 10 classes} \\ \hline energy & 1472 & ✗ & 698 & 9 & 23 & 77.56 & 10 & 74 \\ collins & 40971 & ✗ & 970 & 19 & 26 & 51.05 & 17 & 80 \\ texture & 40499 & ✗ & 5,500 & 40 & 11 & 137.5 & 500 & 500 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Details of the eight real-world tabular datasets.

### Data Preprocessing

Following the procedures presented in prior work [54; 30], we perform preprocessing in two steps. We first compute the required statistics with training data and then transform it. Firstly, we impute the missing values with the mean value for numerical features and the most mode value for categorical features. Secondly, we convert the categorical features into numerical features equal to Leave-one-out Target Statistic [66; 56]. Next, we perform Z-score normalisation for each feature. Specifically, we compute each feature's mean and standard deviation in the training data and then transform the training samples to have a mean of zero and a variance of one for each feature. Finally, we apply the same transformation to the validation and test data before conducting evaluations.

### Software and Computing Resources

**Software implementation.**_(i) For generators:_ We implemented TabEBM using PyTorch 1.13 [64], an open-source deep learning library with a BSD licence. We implemented SMOTE with Imbalanced-learn [45], an open-source Python library for imbalanced datasets with an MIT licence. For other benchmark generators, we used their open-source implementations in Syntcity [67], a library for generating and evaluating synthetic tabular data with an Apache-2.0 license. _(ii) For downstream predictors:_ We implemented TabPFN with its open-source implementation (https://github.com/automl/TabPFN). We implemented the other five downstream predictors (i.e., Logistic Regression, KNN, MLP, Random Forest and XGBoost) with their open-source implementation in scikit-learn [65], an open-source Python library under the 3-Clause BSD license. _(iii) For result analysis and visualisation:_ All numerical plots and graphics have been generated using Matplotlib 3.7 [34], a Python-based plotting library with a BSD licence. The model architecture was generated using draw.io (https://github.com/jgraph/drawio), a free drawing software under Apache License 2.0.

We ensure the consistency and reproducibility of experimental results by implementing a uniform pipeline using PyTorch Lightning, an open-source library under an Apache-2.0 licence. We further fixed the random seeds for data loading and evaluation throughout the training and evaluation process. This ensured that TabEBM and all benchmark models were trained and evaluated on the same set of samples. The experimental environment settings, including library dependencies, are specified in the open-source library for reference and reproduction purposes.

**Computing Resources.** We trained 140,000 models for evaluations (including over 35,000 of generators and over 10,500 for downstream predictors). All our experiments are run on a single machine from an internal cluster with a GPU Nvidia Quadro RTX 8000 with 48GB memory and an Intel(R) Xeon(R) Gold 5218 CPU with 16 cores (at 2.30GHz). The operating system was Ubuntu 20.4.4 LTS.

Figure 9: Data splitting strategies for data augmentation for all datasets.

### TabEBM open-source library

We implemented TabEBM as an extensible library, and the code is available on https://github.com/andreimargeloiu/TabEBM. For practitioners, it offers an easy-to-use, domain-agnostic tool that requires no training, making it particularly suitable for data augmentation, especially in small datasets. For researchers, the library includes the complete implementation of TabEBM, facilitating future extensions and investigations into class-specific energy-based models. The library has two core functionalities:

1. **Generate synthetic data**: The library can generate data for augmentation. [noitemsep=0.0pt] * from tabbm.TabEBM import TabEBM * tabebm = TabEBM() * augmented_data = tabebm.generate(X_train, y_train, num_samples=100) % augmented_data[class_id] = numpy.ndarray of generated data % for a specific 'class_id'
2. **Compute and visualise the energy function**: The library allows computation of TabEBM's energy function and the unnormalised data density. The demo notebook, TabEBM_approximated_density.ipynb, shows the TabEBM-inferred densities under conditions of data noise and class imbalance (thus recreating the plots from Appendix C).

### Implementation of Generators

TabEBM.In all our experiments, the surrogate binary classifier in TabEBM is a pretrained in-context model, TabPFN [33], using the official model weights released by the authors (https://github.com/automL/TabPFN/raw/main/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_42.cpht). We use TabPFN with three ensembles. We use four surrogate negative samples, \(\mathcal{X}_{c}^{\text{neg}}\), positioned at \(\alpha_{\text{dist}}^{\text{neg}}=5\) standard deviations from zero, in random corners of a hypercube in \(\mathbb{R}^{D}\) (as explained in Section 2.2), distant from any real data. In Appendix D.1, we show that TabEBM is robust to the distribution of the negative samples.

We use SGLD [84] for sampling from TabEBM, where the starting points \(\mathbf{x}_{0}^{\text{synth}}\) are initialised by adding Gaussian noise with zero mean and standard deviation \(\sigma_{\text{start}}=0.01\) to a randomly selected sample of the specific class, i.e., \(\mathbf{x}_{0}^{\text{synth}}\sim\mathcal{N}(\mathcal{X}_{c},\sigma_{\text{ start}}^{2}\mathbf{I})\). For SGLD, we used the following parameters: step size \(\alpha_{\text{step}}=0.1\), noise scale \(\alpha_{\text{noise}}=0.01\) and number of steps \(T=200\). We found TabEBM to be robust to the SGLD settings (see Appendix D.2).

TabPFGen.We re-implemented TabPFGen [48] by closely following the original paper since no official implementation is available. As recommended in [48], the starting points are initialised by adding Gaussian noise with zero mean and standard deviation of 0.01 to the training points.

SMOTE.We use the open-source implementation of SMOTE from Imbalanced-learn [45], and the number neighbours \(k\) is set within the range of \(\{1,3,5\}\). When applicable, we always set the maximum value for nearest neighbours (i.e., \(k=5\)). However, very low-sample-size datasets may not contain sufficient samples for large \(k\). For instance, the "fourier" dataset (\(N_{\text{real}}=20\)) only has two samples per class. We set \(k=1\) to generate synthetic data with SMOTE in these cases.

For the other six benchmark generators, we use their open-source implementations in Synthcity [67]. Following prior studies [87; 80; 71; 48], we use the default settings for all generators.

### Implementation of Downstream Predictors

We implemented TabPFN with its official implementation [33] and the other five downstream predictors with the scikit-learn library [65]. Following prior studies [80; 71], we use the default settings for all downstream predictors.

Limitations of Existing Generative Methods

We showcase three limitations of current generative models: (1) Appendix C shows that models approximating the joint distribution \(p(\mathbf{x},y)\) may fail to preserve the stratification of the real data and even fail to generate samples from specific classes. (2) Appendix C evaluates the approximated class-conditional distributions \(p(\mathbf{x}\mid y)\) on data with increasing noise levels, and (3) Appendix C evaluates the approximated class-conditional distributions \(p(\mathbf{x}\mid y)\) on data with increasing class imbalance.

Figure 10: Comparison of class distribution between real data and synthetic data from TVAE. We first train TVAE on the “energy-efficiency” dataset and then randomly generate 10,000 samples with it. We highlight the classes where no synthetic samples are generated. TVAE fails to generate samples for 4 of 23 classes, showing the impracticability to preserve stratification by generative methods that learn joint distribution \(p(\mathbf{x},y)\).

Figure 11: Evaluating the approximated class-conditional distributions on data with increasing noise levels. Darker blue indicates a higher assigned probability. TabPFGen uses a single shared energy-based model to infer the class-conditional distribution \(p(\mathbf{x}|y)\). As noise increases, TabPFGen’s probability assignments vary significantly and end up assigning very high probabilities that are far from the real data. For instance, the areas of assigned probability for \(p(\mathbf{x}|y=1)\) completely flip when noise increases from 0.5 to 1. In contrast, our TabEBM uses class-specific energy models, resulting in robust inferred conditionals. TabEBM performs well even under very high noise (see \(p(\mathbf{x}|y=0)\) for noise level 2), while TabPFGen struggles.

Figure 12: Evaluating the approximated class-conditional distributions on a toy dataset of 300 samples with varying class imbalances. The two clusters maintain their positions. Darker blue indicates a higher assigned probability. TabPFGen uses a single shared energy-based model to infer the class-conditional distribution \(p(\mathbf{x}|y)\). As class imbalance increases, TabPFGen starts assigning high probability in areas far from the real data, for instance, in the case of \(p(\mathbf{x}|y=1)\) for class ratio 10:290. In contrast, our TabEBM fits class-specific energy models only on the class-wise data \(\mathcal{X}_{c}=\{\mathbf{x}^{(i)}\mid y_{i}=c\}\). This results in very robust inferred conditional distributions even under heavy class imbalance (e.g., see that \(p(\mathbf{x}|y=1)\) remains relatively constant).

Extended Experimental Results

### Ablations on the distribution of the surrogate negative samples

#### d.1.1 Ablations on placing the negative samples

Appendix D.1.1 shows TabEBM's energy \(E_{c}(x)\) when varying the selection of the negative samples. TabEBM infers an accurate energy surface with distant negative samples, and the energy surface becomes inaccurate when negative samples resemble real samples. This occurs because TabPFN is uncertain when points of different classes are close, affecting its logits magnitude and making them unsuitable for density estimation.

#### d.1.2 Varying the number of negative samples

We evaluate the impact of the ratio \(|\mathcal{X}_{c}^{\text{neg}}|:|\mathcal{X}_{c}|\) between the negative samples \(\mathcal{X}_{c}^{\text{neg}}\) and the real samples \(|\mathcal{X}_{c}|\). We vary \(|\mathcal{X}_{c}^{\text{neg}}|\) while keeping \(|\mathcal{X}_{c}|\) fixed, simulating both balanced and highly imbalanced scenarios. The negative samples are placed in random corners of the hypercube (as described in Section 2), at five standard deviations in each direction (i.e., \(\alpha_{\text{dist}}^{\text{neg}}=5\)). To ensure reliable outcomes, we maintained a consistent ratio across all classes, keeping the same proportion of negative samples for each class.

Table 4 shows the results across six datasets with \(N_{\text{real}}=100\) real samples, demonstrating that TabEBM is robust to imbalances in the surrogate binary tasks. The column with \(|\mathcal{X}_{c}^{\text{neg}}|=4\) represents the TabEBM results from the main paper, where four negative samples were placed in the corners (as described in Section 2). There are negligible differences in performance, and TabEBM consistently outperforms both the baseline and other generators (as shown in Table 1).

Figure 13: TabEBM energy \(E_{c}(x)\) for different choices of negative samples. The blue region represents low energy, indicating high data density. In (A), TabEBM, with the proposed negative samples placed in a hypercube far from the data, infers an accurate energy surface, resulting in generated data close to the real points. In (B), labelling a random subset of the real data as negative samples leads to a completely inaccurate energy surface. In (C), labelling half of the real points as negative samples reduces density near the decision boundary, as TabPFN assigns low maximal logit due to the high uncertainty. In conclusion, placing negative samples far from the real data results in a robust energy surface.

#### d.1.3 Varying the distance of the negative samples

We assess the effect of varying the distance of negative samples. We use TabEBM with four negative samples positioned randomly at the corners of the hypercube, as outlined in Section 2 (this corresponds to the experimental setup from the main paper). The distance of the negative samples, denoted as \(\alpha_{\text{dist}}^{\text{neg}}\), is varied. Table 5 demonstrates that TabEBM remains generally robust to changes in this distance, with only small performance variations across different datasets. Importantly, using TabEBM for data augmentation consistently improves performance by approximately 3% compared to the Baseline, regardless of the distance used.

### Ablations on the sensitivity to the hyperparameters of SGLD sampling

We vary two key hyperparameters of SGLD on the "biodeg" binary dataset with \(N_{\text{real}}=100\): the step size \(\alpha_{\text{step}}\) and the noise scale \(\alpha_{\text{noise}}\). Table 6 shows that TabEBM remains stable with respect to these hyperparameters. Note that smaller values of \(\alpha_{\text{noise}}\) are expected to perform better because SGLD sampling adds noise at each iteration (see Line 7 in Algorithm 1), thus larger values of \(\alpha_{\text{noise}}\) will hinder convergence of the SGLD sampler.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{6}{c}{**TabEBM**} & Baseline \\ \cline{2-7}  & & & & & & (Real data) \\ \cline{2-7}
**Ratio**\(|\mathcal{X}_{c}^{\text{neg}}|:|\mathcal{X}_{c}|\) & 0.1 & 0.2 & 0.5 & 1 & Fixed \(|\mathcal{X}_{c}^{\text{neg}}|=4\) & - \\ \hline biodeg & 76.59\({}_{\pm 3.95}\) & 76.54\({}_{\pm 3.95}\) & 76.47\({}_{\pm 0.45}\) & 76.81\({}_{\pm 5.58}\) & 76.45\({}_{\pm 3.08}\) & 76.69\({}_{\pm 2.70}\) \\ steel & 92.71\({}_{\pm 7.46}\) & 92.60\({}_{\pm 7.45}\) & 92.79\({}_{\pm 7.50}\) & 92.63\({}_{\pm 7.59}\) & 92.71\({}_{\pm 7.57}\) & 86.87\({}_{\pm 12.4}\) \\ stock & 90.46\({}_{\pm 3.49}\) & 90.41\({}_{\pm 3.65}\) & 90.52\({}_{\pm 3.52}\) & 90.31\({}_{\pm 3.63}\) & 90.36\({}_{\pm 3.14}\) & 89.07\({}_{\pm 3.71}\) \\ energy & 31.20\({}_{\pm 6.22}\) & 31.20\({}_{\pm 6.22}\) & 30.89\({}_{\pm 5.83}\) & 30.90\({}_{\pm 6.09}\) & 31.24\({}_{\pm 5.53}\) & 25.94\({}_{\pm 4.86}\) \\ collins & 13.06\({}_{\pm 2.88}\) & 13.02\({}_{\pm 2.85}\) & 13.05\({}_{\pm 2.89}\) & 12.97\({}_{\pm 2.79}\) & 13.07\({}_{\pm 2.51}\) & 11.44\({}_{\pm 2.77}\) \\ texture & 85.91\({}_{\pm 6.92}\) & 85.91\({}_{\pm 6.92}\) & 85.94\({}_{\pm 6.76}\) & 86.26\({}_{\pm 6.72}\) & 86.01\({}_{\pm 7.36}\) & 82.42\({}_{\pm 10.38}\) \\ \hline
**Average accuracy** & 64.99 & 64.95 & 64.94 & 64.98 & 64.97 & 62.07 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Evaluating the impact of varying the ratio \(|\mathcal{X}_{c}^{\text{neg}}|:|\mathcal{X}_{c}|\). We show the test classification accuracy performance (%) of TabEBM on data augmentation averaged over six datasets and ten repeats. TabEBM shows consistent performance and outperforms the baseline, regardless of the number of negative samples.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \multicolumn{6}{c}{**TabEBM**} & Baseline \\ \cline{2-7}
**Per-dimension distance \(\alpha_{d}\)** & 0.1 & 0.2 & 0.5 & 1 & 2 & 5 & - \\
**of the negative samples** & & & & & & & (Real data) \\ \hline biodeg & 76.72\({}_{\pm 3.33}\) & 76.62\({}_{\pm 3.30}\) & 77.12\({}_{\pm 2.60}\) & 76.85\({}_{\pm 3.14}\) & 76.50\({}_{\pm 3.93}\) & 76.45\({}_{\pm 3.08}\) & 76.69\({}_{\pm 2.70}\) \\ steel & 93.97\({}_{\pm 5.76}\) & 93.46\({}_{\pm 6.24}\) & 93.00\({}_{\pm 6.92}\) & 92.60\({}_{\pm 7.31}\) & 92.68\({}_{\pm 7.38}\) & 92.71\({}_{\pm 7.57}\) & 86.87\({}_{\pm 7.12}\) \\ stock & 90.42\({}_{\pm

### Distribution of Logits and Unnormalized Density in TabEBM

Figure 14: Additional results for Section 3.4. The logit distribution of TabPFN trained on our surrogate binary tasks across four datasets. Starting from the real samples, random points are selected at increasing distances (shown on the x-axis). The **top row** shows the logit distributions for the surrogate task. Close to the real data, TabPFN outputs a high logit value. As the distance increases, the logits converge due to increased predictive uncertainty, leading to equal class probabilities after applying softmax. Notably, across datasets, TabPFN’s logits are always positive, have similar ranges, and maintain a relatively constant sum as distance increases. The **bottom row** TabEBM’s unnormalized density, \(p_{c}(x)\propto\exp(-E_{c}(x))\to p_{c}(x)\propto(\exp(f(x)[0])+\exp(f(x)[1]))\). The density decreases significantly far from the data, becoming negligible. Because sampling using SGLD perform gradient ascent on the density, the TabEBM-generated samples will be similar when using one or both logits.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline \multirow{2}{*}{} & \multicolumn{4}{c}{\(\alpha_{\text{step}}\)} \\ \cline{2-5}  & 0.1 & 0.3 & 0.5 & 1.0 \\ \hline
0.01 & 76.45 & **77.09** & 77.04 & 76.58 \\
0.02 & 76.86 & 76.96 & 76.77 & 76.26 \\
0.05 & 75.93 & 75.89 & 75.94 & **75.70** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test classification accuracy (%) of TabEBM (averaged over six downstream predictors) with different SGLD settings. Increasing \(\alpha_{\text{noise}}\) (added at each SGLD step) is expected to degrade performance, as it causes the sampling to diverge further from the real data.

### Complete Trade-off Figures with Error Bars

Figure 15: **(a1&a2):** Median inverse KL and KS test vs. mean normalised balanced accuracy improvement (%) between real train data and synthetic data. **(b1&b2):** Median DCR and \(\delta-presence\) vs. mean normalised balanced accuracy change (%) between real train data and synthetic data. Note that “accuracy improvement” is for data augmentation, and “accuracy change” is for data sharing. TabEBM generates high-fidelity synthetic data that can also be used for privacy preservation.

[MISSING_PAGE_EMPTY:28]

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{\(N_{\text{r}nt}\)} & Baseline (Real data) & SMOTE & TVAE & CTGAN & NILOW & TabDDPM & ARF & GGGLE & TabPFTGen & **TabEM** \\ \hline \multirow{5}{*}{protein} & \multirow{5}{*}{100} & 20 & 21.34\(\pm\)2.93 & N/A & 21.78\(\pm\)2.06 & 21.18\(\pm\)4.22 & 21.30\(\pm\)1.90 & 22.00\(\pm\)2.20 & 22.69\(\pm\)3.86 & 16.99\(\pm\)3.45 & **35.78\(\pm\)**4.46 & 35.76\(\pm\)**4.37 \\  & & 50 & 36.41\(\pm\)4.33 & **55.24\(\pm\)**3.31 & 38.58\(\pm\)2.50 & 36.13\(\pm\)4.34 & 35.40\(\pm\)4.27 & 36.77\(\pm\)4.06 & 36.84\(\pm\)4.05 & 31.02\(\pm\)4.11 & 53.38\(\pm\)3.53 & 53.49\(\pm\)3.30 \\  & & 100 & 50.17\(\pm\)3.11 & **70.11\(\pm\)**2.12 & 51.97\(\pm\)2.84 & 50.61\(\pm\)3.15 & 50.62\(\pm\)3.27 & 50.63\(\pm\)3.55 & 50.36\(\pm\)4.44 & 47.02\(\pm\)2.22 & 67.99\(\pm\)4.25 & 68.27\(\pm\)2.51 \\  & & 200 & 65.84\(\pm\)2.78 & 80.43\(\pm\)2.44 & 65.52\(\pm\)2.96 & 66.05\(\pm\)2.74 & 66.14\(\pm\)2.42 & 67.50\(\pm\)2.57 & 66.52\(\pm\)3.16 & 63.92\(\pm\)3.26 & 79.94\(\pm\)2.26 & **80.55\(\pm\)**2.02 \\  & & 500 & 85.63\(\pm\)1.41 & 90.92\(\pm\)1.42 & 87.08\(\pm\)1.98 & 85.77\(\pm\)1.43 & 85.51\(\pm\)1.59 & 86.47\(\pm\)1.40 & 85.87\(\pm\)1.69 & 85.64\(\pm\)1.94 & 91.32\(\pm\)1.69 & **91.67\(\pm\)**1.11 \\ \hline \multirow{5}{*}{fourier} & \multirow{5}{*}{100} & 20 & 18.06\(\pm\)3.30 & N/A & 26.56\(\pm\)4.92 & 42.88\(\pm\)3.66 & 19.80\(\pm\)3.37 & 19.30\(\pm\)3.53 & 23.42\(\pm\)3.48 & 18.78\(\pm\)2.17 & 41.08\(\pm\)0.56 & **42.78\(\pm\)**5.53 \\  & & 50 & 48.00\(\pm\)2.47 & **60.38\(\pm\)**1.67 & 39.86\(\pm\)3.73 & 46.82\(\pm\)3.52 & 43.56\(\pm\)3.45 & 49.54\(\pm\)2.78 & 49.28\(\pm\)2.80 & 28.12\(\pm\)2.75 & 59.50\(\pm\)1.59 & 58.54\(\pm\)1.96 \\  & & 100 & 58.33\(\pm\)2.86 & **66.96\(\pm\)**2.47 & 48.46\(\pm\)1.44 & 59.44\(\pm\)3.47 & 53.00\(\pm\)2.94 & 55.00\(\pm\)3.48 & 52.74\(\pm\)1.35 & 35.70\(\pm\)2.40 & 63.88\(\pm\)2.85 & 65.08\(\pm\)2.47 \\  & & 200 & 68.60\(\pm\)2.55 & **71.90\(\pm\)**2.02 & 59.66\(\pm\)3.31 & 66.54\(\pm\)2.55 & 65.16\(\pm\)2.71 & 70.22\(\pm\)2.36 & 64.52\(\pm\)2.44 & 51.24\(\pm\)2.79 & 70.32\(\pm\)1.94 & 71.08\(\pm\)1.87 \\  & & 500 & 76.90\(\pm\)1.30 & 77.64\(\pm\)1.17 & 73.20\(\pm\)1.68 & 76.22\(\pm\)1.62 & 75.72\(\pm\)1.40 & **78.88\(\pm\)**1.58 & 76.54\(\pm\)**0.73 & 63.66\(\pm\)2.40 & 74.30\(\pm\)1.51 & 75.35\(\pm\)1.34 \\ \hline \multirow{5}{*}{biodeg} & \multirow{5}{*}{100} & 20 & 65.23\(\pm\)5.61 & 68.99\(\pm\)3.31 & 66.63\(\pm\)7.83 & 56.99\(\pm\)5.55 & 59.91\(\pm\)6.99 & 55.85\(\pm\)4.94 & 58.77\(\pm\)5.93 & 56.62\(\pm\)7.29 & 67.79\(\pm\)4.64 & **69.76\(\pm\)**4.43 \\  & & 50 & 71.26\(\pm\)3.13 & 73.19\(\pm\)2.46 & 70.00\(\pm\)2.14 & 70.00\(\pm\)5.92 & 65.90\(\pm\)3.57 & 73.50\(\pm\)4.43 & 70.23\(\pm\)3.35 & 65.29\(\pm\)4.57 & 72.08\(\pm\)3.44 & **73.85\(\pm\)**1.57 \\  & & 100 & 76.12\(\pm\)1.98 & 76.07\(\pm\)1.78 & 74.02\(\pm\)2.78 & 75.36\(\pm\)2.18 & 73.42\(\pm\)1.97 & 74.82\(\pm\)2.82 & 72.26\(\pm\)2.26 & 72.26\(\pm\)4.56 & 74.56\(\pm\)1.95 & 75.60\(\pm\)1.55 \\  & & 200 & 78.86\(\pm\)2.19 & **79.67\(\pm\)**1.73 & 77.31\(\pm\)2.93 & 78.05\(\pm\)3.07 & 77.64\(\pm\)2.71 & 77.84\(\pm\)2.62 & 78.81\(\pm\)2.46 & 76.82\(\pm\)2.29 & 77.46\(\pm\)1.68 & 78.46\(\pm\)1.69 \\  & & 500 & 82.59\(\pm\)1.17 & **83.07\(\pm\)**1.50 & 82.13\(\pm\)1.21 & 82.17\(\pm\)1.32 & 82.80\(\pm\)1.28 & 81.06\(\pm\)1.22 & 82.15\(\pm\)1.33 & 82.10\(\pm\)0.79 & 79.99\(\pm\)1.76 & 81.01\(\pm\)1.66 \\ \hline \multirow{5}{*}{steel} & \multirow{5}{*}{100} & 20 & 56.40\(\pm\)4.48 & 63.95\(\pm\)3.14 & 59.45\(\pm\)2.75 & 57.04\(\pm\)4.05 & 54.59\(\pm\)4.81 & 65.46\(\pm\)6.10 & 56.97\(\pm\)5.43 & 52.90\(\pm\)3.76 & **70.68\(\pm\)**3.67 & 69.31\(\pm\)**4.02 \\  & & 50 & 73.95\(\pm\)4.45 & 70.24\(\pm\)2.43 & 67.60\(\pm\)4.10 & 68.77\(\pm\)2.45 & 67.00\(\pm\)4.58 & **58.14\(\pm\)**3.66 & 64.02\(\pm\)3.71 & 57.54\(\pm\)2.34 & 82.09\(\pm\)3.69 & 80.47\(\pm\)3.38 \\  & & 100 & 84.70\(\pm\)1.57 & 77.46\(\pm\)1.37 & 78.12\(\pm\)2.90 & 77.29\(\pm\)4.42 & 77.09\(\pm\)

\begin{table}
\begin{tabular}{c c|c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{\(N_{\text{real}}\)} & Baseline & \multirow{2}{*}{SMOTE} & \multirow{2}{*}{TVAE} & \multirow{2}{*}{CTGAN} & \multirow{2}{*}{NFLOW} & \multirow{2}{*}{TishDDPM} & \multirow{2}{*}{ARF} & \multirow{2}{*}{GOGGLE} & \multirow{2}{*}{TishPGFen} & \multirow{2}{*}{**TishEBM**} \\  & & & (Real data) & & & & & & & & & & \\ \hline \multirow{9}{*}{protein} & 20 & 35.12\(\pm\)2.59 & \multirow{9}{*}{N/A} & 21.89\(\pm\)3.62 & 26.95\(\pm\)4.13 & 24.56\(\pm\)5.06 & 27.30\(\pm\)3.23 & 27.96\(\pm\)4.51 & 27.09\(\pm\)3.67 & 36.19\(\pm\)2.54 & **36.26\(\pm\)**2.65 \\  & 50 & 58.11\(\pm\)4.13 & 57.24\(\pm\)4.80 & 40.77\(\pm\)3.99 & 43.04\(\pm\)5.43 & 44.78\(\pm\)3.92 & 49.43\(\pm\)5.14 & 46.84\(\pm\)5.16 & 44.78\(\pm\)2.67 & 58.62\(\pm\)2.44 & **58.75\(\pm\)**4.48 \\  & 100 & 76.82\(\pm\)1.33 & 76.78\(\pm\)3.10 & 60.20\(\pm\)3.21 & 64.14\(\pm\)3.16 & 63.28\(\pm\)4.05 & 69.20\(\pm\)2.75 & 65.08\(\pm\)2.27 & 62.45\(\pm\)3.22 & **77.84\(\pm\)**1.97 & 77.63\(\pm\)**1.99 \\  & 200 & 89.53\(\pm\)3.24 & 90.28\(\pm\)2.13 & 80.28\(\pm\)2.48 & 81.94\(\pm\)2.36 & 81.85\(\pm\)2.46 & 85.84\(\pm\)2.07 & 82.57\(\pm\)2.19 & 78.04\(\pm\)2.66 & **90.74\(\pm\)**2.13 & 90.48\(\pm\)2.06 \\  & 500 & 98.23\(\pm\)9.91 & 98.25\(\pm\)3.78 & 95.08\(\pm\)1.34 & 95.74\(\pm\)0.96 & 95.15\(\pm\)1.09 & 96.01\(\pm\)0.98 & 96.50\(\pm\)0.87 & 96.23\(\pm\)1.67 & **98.52\(\pm\)**0.90 & 98.50\(\pm\)**0.70 \\ \hline \multirow{9}{*}{fourier} & 20 & 33.66\(\pm\)3.92 & \multirow{9}{*}{N/A} & 23.20\(\pm\)5.54 & 17.08\(\pm\)2.75 & 19.40\(\pm\)4.03 & 18.32\(\pm\)3.82 & 23.26\(\pm\)4.21 & 19.64\(\pm\)3.40 & **37.00\(\pm\)**2.85 & 35.02\(\pm\)3.77 \\  & 50 & 53.72\(\pm\)1.67 & 53.02\(\pm\)1.66 & 37.16\(\pm\)5.03 & 37.60\(\pm\)4.52 & 35.14\(\pm\)2.94 & 40.90\(\pm\)2.89 & 42.28\(\pm\)2.33 & 32.66\(\pm\)1.99 & **55.04\(\pm\)**2.13 & 55.34\(\pm\)**1.40 \\  & 100 & 62.78\(\pm\)1.90 & 61.42\(\pm\)2.74 & 43.68\(\pm\)1.15 & 48.80\(\pm\)2.66 & 46.18\(\pm\)3.96 & 56.52\(\pm\)5.91 & 52.50\(\pm\)2.63 & 37.74\(\pm\)2.99 & 63.00\(\pm\)3.95 & **63.54\(\pm\)**1.83 \\  & 200 & 70.18\(\pm\)1.85 & 70.06\(\pm\)1.20 & 58.90\(\pm\)2.56 & 62.36\(\pm\)2.86 & 58.04\(\pm\)2.05 & 70.08\(\pm\)1.90 & 62.14\(\pm\)2.20 & 50.92\(\pm\)1.53 & **71.49\(\pm\)**1.41 & 71.36\(\pm\)1.76 \\  & 500 & 77.94\(\pm\)1.65 & 77.18\(\pm\)1.35 & 72.14\(\pm\)1.79 & 74.30\(\pm\)1.65 & 71.38\(\pm\)1.54 & 77.78\(\pm\)1.36 & 74.32\(\pm\)1.56 & 67.28\(\pm\)2.91 & 78.34\(\pm\)1.72 & **79.30\(\pm\)**6.99 \\ \hline \multirow{9}{*}{biedge} & 20 & 71.31\(\pm\)5.13 & 68.84\(\pm\)5.96 & 66.64\(\pm\)8.27 & 62.11\(\pm\)4.96 & 62.61\(\pm\)6.78 & 52.96\(\pm\)4.22 & 62.06\(\pm\)3.69 & 65.81\(\pm\)7.24 & 72.04\(\pm\)1.82 & **72.09\(\pm\)**4.81 \\  & 50 & 76.73\(\pm\)3.16 & 74.97\(\pm\)2.51 & 72.02\(\pm\)4.74 & 71.83\(\pm\)3.17 & 67.86\(\pm\)6.42 & 69.92\(\pm\)3.83 & 74.03\(\pm\)3.16 & 71.01\(\pm\)2.78 & **77.17\(\pm\)**2.93 & 71.71\(\pm\)3.20 \\  & 100 & **79.19\(\pm\)1.91** & 78.20\(\pm\)1.68 & 76.78\(\pm\)2.79 & 77.85\(\pm\)2.73 & 76.01\(\pm\)2.80 & 76.74\(\pm\)3.92 & 76.08\(\pm\)2.18 & 76.24\(\pm\)2.78 & 78.23\(\pm\)2.29 & 79.08\(\pm\)2.03 \\  & 200 & **82.39\(\pm\)**1.48 & 81.70\(\pm\)2.04 & 80.43\(\pm\)2.02 & 79.96\(\pm\)2.52 & 79.82\(\pm\)50.81 & 50.15\(\pm\)1.12 & 79.59\(\pm\)1.27 & 80.34\(\pm\)1.93 & 81.47\(\pm\)1.36 & 82.24\(\pm\)1.54 \\  & 500 & **84.50\(\pm\)**0.61 & 84.50\(\pm\)0.61 & 83.78\(\pm\)1.51 & 83.67\(\pm\)0.81 & 84.31\(\pm\)1.20 & 84.09\(\pm\)0.84 & 83.76\(\pm\)1.27 & 82.97\(\pm\)1.21 & 84.37\(\pm\)0.84 & 84.14\(\pm\)0.59 \\ \hline \multirow{9}{*}{steel} & 20 & 62.35\(\pm\)3.60 & 60.34\(\pm\)5.73 & 61.63\(\pm\)8.82 & 59.09\(\pm\)4.25 & 56.99\(\pm\)7.13 & 60.67\(\pm\)9.15 & 55.23\(\pm\)3.92 & 55.78\(\pm\)3.01 & **64.49\(\pm\)**0.63 & 64.22\(\pm\)5.89 \\  & 50 & 79.65\(\pm\)5.53 & 68.18\(\pm\)3.16 & 69.01\(\pm\)4.28 & 70.30\(\pm\)4.77 & 66.96\(\pm\)5.12 & **84.04\(\pm\)**0.83 & 64.79\(\pm\)0.47 & 58.95\(\pm\)1.66 & 82.72\(\pm\)6.02 & 82.15\(\pm\)5.78 \\  & 100 & 91.28\(\pm\)2.33

\begin{table}
\begin{tabular}{c c|c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{\(N_{\text{real}}\)} & Baseline & \multirow{2}{*}{SMOTE} & \multirow{2}{*}{TVAE} & \multirow{2}{*}{CTGAN} & \multirow{2}{*}{NFLOW} & \multirow{2}{*}{TishDDPM} & \multirow{2}{*}{ARF} & \multirow{2}{*}{GOGGLE} & \multirow{2}{*}{TishDFPGen} & \multirow{2}{*}{**TishEBM**} \\  & & (Real data) & & & & & & & & & & & \\ \hline \multirow{9}{*}{protein} & 20 & 28.52\(\pm\)2.19 & \multirow{2}{*}{N/A} & 22.74\(\pm\)2.23 & 24.94\(\pm\)5.15 & 24.61\(\pm\)2.46 & 29.62\(\pm\)0.49 & 27.69\(\pm\)3.73 & 25.76\(\pm\)1.62 & 32.04\(\pm\)2.40 & **34.19\(\pm\)**2.21 \\  & & 50 & 53.40\(\pm\)3.36 & 55.69\(\pm\)2.61 & 46.95\(\pm\)3.13 & 49.41\(\pm\)48 & 43.28\(\pm\)407 & 47.93\(\pm\)4.49 & 44.48\(\pm\)3.35 & 47.25\(\pm\)4.81 & 54.29\(\pm\)2.97 & **56.85\(\pm\)**2.49 \\  & & 100 & 68.13\(\pm\)1.39 & **72.89\(\pm\)**2.60 & 63.24\(\pm\)1.78 & 61.19\(\pm\)2.10 & 59.64\(\pm\)3.48 & 65.19\(\pm\)3.22 & 60.50\(\pm\)2.28 & 65.05\(\pm\)3.70 & 71.47\(\pm\)3.14 & 72.57\(\pm\)2.90 \\  & & 200 & 80.34\(\pm\)2.35 & 83.60\(\pm\)2.17 & 78.51\(\pm\)2.58 & 75.84\(\pm\)1.61 & 76.84\(\pm\)2.35 & 78.74\(\pm\)2.34 & 76.51\(\pm\)2.90 & 79.44\(\pm\)2.37 & 83.36\(\pm\)2.40 & **84.30\(\pm\)**1.97 \\  & 500 & 93.01\(\pm\)1.12 & 93.82\(\pm\)0.67 & 92.86\(\pm\)1.66 & 91.37\(\pm\)1.25 & 93.00\(\pm\)1.20 & 92.93\(\pm\)0.97 & 92.38\(\pm\)0.10 & 92.95\(\pm\)0.92 & **94.49\(\pm\)**1.16 & 93.94\(\pm\)**1.23 \\ \hline \multirow{9}{*}{fourier} & 20 & 35.10\(\pm\)4.56 & \multirow{2}{*}{N/A} & 19.06\(\pm\)1.91 & 17.52\(\pm\)2.84 & 20.78\(\pm\)2.54 & 16.98\(\pm\)2.31 & 23.78\(\pm\)3.12 & 19.00\(\pm\)2.93 & 34.88\(\pm\)5.93 & **38.60\(\pm\)**5.66 \\  & 50 & 64.10\(\pm\)3.90 & 64.76\(\pm\)4.00 & 37.20\(\pm\)3.35 & 32.82\(\pm\)4.36 & 37.78\(\pm\)3.11 & 51.76\(\pm\)3.90 & 47.22\(\pm\)4.35 & 33.86\(\pm\)5.61 & **66.92\(\pm\)**2.95 & 66.26\(\pm\)**1.36 \\  & 100 & 73.86\(\pm\)1.90 & 73.78\(\pm\)3.22 & 64.04\(\pm\)2.51 & 60.82\(\pm\)3.71 & 56.41\(\pm\)4.16 & 66.61\(\pm\)1.19 & 58.62\(\pm\)3.33 & 68.16\(\pm\)3.12 & 73.13\(\pm\)2.74 & **74.84\(\pm\)**1.30 \\  & 200 & 78.54\(\pm\)2.15 & 79.18\(\pm\)1.92 & 74.86\(\pm\)1.60 & 74.26\(\pm\)1.26 & 69.36\(\pm\)2.61 & 76.42\(\pm\)1.93 & 72.88\(\pm\)1.22 & 76.64\(\pm\)1.99 & **82.20\(\pm\)**0.85 & 79.18\(\pm\)**1.20 \\  & 500 & 81.84\(\pm\)1.01 & 82.14\(\pm\)1.49 & 81.02\(\pm\)1.59 & 81.18\(\pm\)1.43 & 80.08\(\pm\)1.62 & 81.26\(\pm\)1.30 & 80.28\(\pm\)1.54 & 80.62\(\pm\)1.52 & 81.45\(\pm\)1.45 & **83.40\(\pm\)**1.24 \\ \hline \multirow{9}{*}{biedge} & 20 & 61.11\(\pm\)7.37 & **68.38\(\pm\)**1.50 & 65.44\(\pm\)8.89 & 56.29\(\pm\)9.76 & 58.19\(\pm\)6.00 & 52.90\(\pm\)4.74 & 62.33\(\pm\)6.14 & 63.52\(\pm\)7.29 & 67.15\(\pm\)5.84 & 67.82\(\pm\)**1.53 \\  & 50 & 68.38\(\pm\)4.82 & 70.64\(\pm\)4.14 & 77.77\(\pm\)2.99 & 66.78\(\pm\)4.98 & 61.39\(\pm\)4.94 & 63.98\(\pm\)3.65 & 68.78\(\pm\)5.22 & 70.34\(\pm\)3.39 & 71.38\(\pm\)1.60 & **72.12\(\pm\)**3.29 \\  & 100 & 73.19\(\pm\)2.49 & 75.36\(\pm\)2.96 & 79.89\(\pm\)2.58 & 72.68\(\pm\)2.96 & 69.62\(\pm\)3.73 & 71.31\(\pm\)2.99 & 72.16\(\pm\)2.28 & 74.22\(\pm\)2.32 & **75.85\(\pm\)**1.56 & 75.65\(\pm\)**1.55 \\  & 200 & 77.85\(\pm\)2.72 & 78.86\(\pm\)1.65 & 76.42\(\pm\)2.25 & 76.68\(\pm\)2.27 & 77.34\(\pm\)3.01 & 76.16\(\pm\)1.06 & 77.59\(\pm\)2.49 & 77.42\(\pm\)2.36 & **77.24\(\pm\)**2.79 & 79.24\(\pm\)**2.70 & 79.24\(\pm\)**1.70 & 79.21\(\pm\)**1.70 \\  & 500 & 81.42\(\pm\)0.73 & 82.03\(\pm\)1.02 & 81.88\(\pm\)0.87 & 81.71\(\pm\)1.54 & 80.50\(\pm\)1.21 & 81.43\(\pm\)1.26 & 81.34\(\pm\)1.58 & 81.94\(\pm\)0.85 & **82.38\(\pm\)**1.35 & 82.10\(\pm\)1.31 \\ \hline \multirow{9}{*}{steel} & 20 & 52.72\(\pm\)1.60 & 56.16\(\pm\)4.50 & 57.23\(\pm\)3.97 & 54.65\(\pm\)3.40 & 53.75\(\pm\)4.99 & 51.70\(\pm\)1.66 & 54.09\(\pm\)4.36 & 55.50\(\pm\)2.97 & 57.04\(\pm\)3.07 & **57.41\(\pm\)**2.67 \\  & 50 & 59.75\(\pm\)1.11 & 62.12\(\pm\)2.46 & 66.65\(\pm\)1.94 & 58.09\(\pm\)1.75 & 54.69\(\pm\)2.44 & 58.14\(\pm\)2.41 & 57.67\(\pm\)2.52 & 60.34\(\pm\)2.90 & 65.07\(\pm\)3.11 &

\begin{table}
\begin{tabular}{c c|c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multirow{2}{*}{\(N_{\text{real}}\)} & \multirow{2}{*}{
\begin{tabular}{c} Baseline \\ (Real data) \\ \end{tabular} } & \multirow{2}{*}{SMOTE} & \multirow{2}{*}{TVAE} & \multirow{2}{*}{CTGAN} & \multirow{2}{*}{NFLOW} & \multirow{2}{*}{TabDDPM} & \multirow{2}{*}{ARF} & \multirow{2}{*}{GOGGLE} & \multirow{2}{*}{TabPFGen} & \multirow{2}{*}{**TabBBM**} \\ \hline \multirow{5}{*}{protein} & 20 & 19.70\(\pm\)6.33 & N/A & 19.44\(\pm\)4.11 & 17.32\(\pm\)2.75 & 18.11\(\pm\)3.07 & 16.15\(\pm\)3.30 & 20.71\(\pm\)5.26 & 17.40\(\pm\)4.89 & 24.00\(\pm\)3.64 & **24.18\(\pm\)**3.95 \\  & 50 & 39.01\(\pm\)4.32 & 37.68\(\pm\)5.40 & 33.07\(\pm\)4.18 & 24.38\(\pm\)5.45 & 23.09\(\pm\)4.55 & 30.87\(\pm\)5.30 & 34.13\(\pm\)6.45 & 33.62\(\pm\)4.67 & 39.78\(\pm\)6.03 & **44.46\(\pm\)**4.97 \\  & 100 & 57.59\(\pm\)3.69 & 60.16\(\pm\)1.55 & 49.23\(\pm\)5.51 & 43.33\(\pm\)7.92 & 49.69\(\pm\)4.96 & 48.36\(\pm\)4.93 & 43.97\(\pm\)5.46 & 47.00\(\pm\)3.29 & 54.34\(\pm\)7.94 & **62.77\(\pm\)**5.55 \\  & 200 & 24.05\(\pm\)2.92 & 76.90\(\pm\)4.96 & 69.71\(\pm\)1.28 & 67.46\(\pm\)4.92 & 58.29\(\pm\)1.76 & 69.68\(\pm\)4.28 & 63.69\(\pm\)4.26 & 66.09\(\pm\)4.78 & 73.19\(\pm\)6.06 & **79.25\(\pm\)**3.83 \\  & 500 & 88.89\(\pm\)1.71 & 90.02\(\pm\)1.51 & 90.10\(\pm\)1.80 & 89.37\(\pm\)1.81 & 86.03\(\pm\)2.31 & 87.29\(\pm\)1.20 & 90.05\(\pm\)1.70 & 85.04\(\pm\)1.27 & 89.66\(\pm\)1.17 & **91.81\(\pm\)**1.44 \\ \hline \multirow{5}{*}{fourier} & 20 & 10.00\(\pm\)0.00 & N/A & 14.64\(\pm\)3.13 & 13.58\(\pm\)2.57 & 13.82\(\pm\)1.44 & 11.72\(\pm\)4.19 & 16.38\(\pm\)3.36 & 12.34\(\pm\)3.99 & 23.50\(\pm\)1.16 & **26.78\(\pm\)**4.82 \\  & 50 & 42.10\(\pm\)6.79 & 43.40\(\pm\)5.21 & 34.32\(\pm\)3.59 & 24.68\(\pm\)4.07 & 17.66\(\pm\)4.54 & 24.82\(\pm\)6.35 & 27.47\(\pm\)8.50 & 35.42\(\pm\)5.51 & 35.60\(\pm\)1.31 & **45.08\(\pm\)**4.07 \\  & 100 & 54.84\(\pm\)2.26 & 52.92\(\pm\)6.68 & 36.20\(\pm\)5.15 & 30.26\(\pm\)3.94 & 42.46\(\pm\)4.13 & 47.28\(\pm\)4.31 & 47.88\(\pm\)4.36 & 48.90\(\pm\)1.56 & **54.94\(\pm\)**5.72 \\  & 200 & 63.88\(\pm\)3.55 & 65.34\(\pm\)3.57 & 58.36\(\pm\)3.27 & 53.20\(\pm\)5.45 & 46.96\(\pm\)4.58 & 61.04\(\pm\)4.12 & 52.10\(\pm\)3.32 & 56.66\(\pm\)2.67 & 66.60\(\pm\)4.24 & **67.68\(\pm\)**3.19 \\  & 500 & 74.56\(\pm\)1.97 & 74.18\(\pm\)2.10 & 68.28\(\pm\)2.82 & 67.98\(\pm\)2.67 & 61.24\(\pm\)2.35 & 72.78\(\pm\)1.54 & 67.50\(\pm\)1.57 & 68.28\(\pm\)**3.65 & N/A & **76.25\(\pm\)**3.18 \\ \hline \multirow{5}{*}{biodey} & 20 & 62.95\(\pm\)7.68 & 66.51\(\pm\)5.84 & 62.72\(\pm\)6.89 & 55.24\(\pm\)6.28 & 59.20\(\pm\)7.83 & 54.65\(\pm\)9.56 & 62.78\(\pm\)9.78 & 61.09\(\pm\)1.00 & 65.52\(\pm\)**6.08 & **66.64\(\pm\)**0.71 \\  & 50 & 67.96\(\pm\)3.45 & 67.69\(\pm\)4.42 & 66.22\(\pm\)5.30 & 61.64\(\pm\)3.67 & 60.72\(\pm\)5.73 & 54.88\(\pm\)6.80 & **69.48\(\pm\)**3.55 & 65.93\(\pm\)4.08 & 67.76\(\pm\)**4.90 & 67.90\(\pm\)**3.27 \\  & 100 & **73.88\(\pm\)2.55** & 72.05\(\pm\)4.75 & 72.31\(\pm\)1.17 & 70.41\(\pm\)3.00 & 66.02\(\pm\)6.25 & 69.95\(\pm\)4.46 & 77.11\(\pm\)3.86 & 69.03\(\pm\)3.43 & 72.58\(\pm\)**5.21 & 71.05\(\pm\)**5.59 \\  & 200 & 76.38\(\pm\)4.45 & 74.98\(\pm\)3.15 & 79.33\(\pm\)2.29 & 75.68\(\pm\)4.15 & 67.82\(\pm\)3.91 & 72.58\(\pm\)5.07 & 74.74\(\pm\)2.34 & 73.84\(\pm\)3.32 & 75.85\(\pm\)1.30 & **76.74\(\pm\)**4.24 \\  & 500 & 78.45\(\pm\)3.37 & 79.38\(\pm\)1.59 & 78.88\(\pm\)1.42 & **80.15\(\pm\)**1.87 & 76.72\(\pm\)**3.44 & 77.10\(\pm\)2.96 & 78.14\(\pm\)2.65 & 78.83\(\pm\)2.21 & 79.40\(\pm\)1.69 & 78.80\(\pm\)3.76 \\ \hline \multirow{5}{*}{steel} & 20 & 53.12\(\pm\)5.62 & 56.64\(\pm\)4.76 & 53.32\(\pm\)2.75 & 55.66\(\pm\)3.63 & 52.38\(\pm\)3.55 & 52.44\(\pm\)4.46 & 51.34\(\pm\)4.15 & 50.74\(\pm\)2.53 & 55.43\(\pm\)5.59 & **55.78\(\pm\)**4.53 \\  & 50 & 66.79\(\pm\)3.91 & 61.09\(\pm\)5.25 & 59.51\(\pm\)4.15 & 54.82\(\pm\)2.43 & 54.79\(\pm\)4.69 & 59.71\(\pm\)1.64 & 57.66\(\pm\)1.99 & 55.89\(\pm\)4.50 & 56.88\(\pm\)4.70 & **56.73\(\pm\)**1.70 & **74.18\(\pm\)**1.36 \\  & 100 & 83.71\(\pm\)3.76 & 69.65\(\pm\)5.51 & 67.13\(\

[MISSING_PAGE_EMPTY:33]

[MISSING_PAGE_EMPTY:34]

[MISSING_PAGE_EMPTY:35]

[MISSING_PAGE_EMPTY:36]

[MISSING_PAGE_EMPTY:37]

#### d.6.2 Similarity between Real Test Data and Synthetic Data

Table 19: **Inverse KL between real test data and synthetic data** on eight real-world tabular datasets with varied real data availability. We report the mean \(\pm\) std result and average rank across datasets. A higher rank implies higher fidelity. Note that “N/A” denotes that a specific generator was not applicable, and the rank is computed with the mean result of other methods. We **bold** the highest result for each dataset of different sample sizes. TabEBM achieves the best overall performance against benchmark generators.

[MISSING_PAGE_EMPTY:39]

[MISSING_PAGE_EMPTY:40]

[MISSING_PAGE_EMPTY:41]

[MISSING_PAGE_EMPTY:42]

[MISSING_PAGE_EMPTY:43]

[MISSING_PAGE_EMPTY:44]

\begin{table}
\begin{tabular}{c c|c c c c c c c c c} \hline \hline Datasets & \(N_{\text{used}}\) & SMOTE & TVAE & CTGAN & NFLOW & TabDDPM & ARF & GOGGLE & TabPFGen & **TabEBM** \\ \hline \multirow{6}{*}{protein} & 20 & N/A & 19.29\({}_{+3.85}\) & 13.63\({}_{+2.95}\) & 12.71\({}_{+2.84}\) & 12.75\({}_{+2.09}\) & 20.47\({}_{+4.29}\) & 13.25\({}_{+2.90}\) & 31.86\({}_{+2.24}\) & **34.05\({}_{+2.24}\)** \\  & 50 & 56.09\({}_{+2.77}\) & 32.29\({}_{+3.08}\) & 19.37\({}_{+5.89}\) & 12.62\({}_{+2.26}\) & 14.87\({}_{+2.75}\) & 32.60\({}_{+4.15}\) & 13.29\({}_{+2.50}\) & 54.25\({}_{+1.99}\) & **56.96\({}_{+3.32}\)** \\  & 100 & 71.14\({}_{+2.65}\) & 44.50\({}_{+3.08}\) & 29.72\({}_{+3.69}\) & 12.37\({}_{+3.12}\) & 17.71\({}_{+3.51}\) & 38.22\({}_{+4.85}\) & 13.67\({}_{+1.06}\) & 70.68\({}_{+3.24}\) & **72.74\({}_{+2.97}\)** \\  & 200 & 81.22\({}_{+2.51}\) & 50.10\({}_{+3.59}\) & 37.08\({}_{+5.17}\) & 14.34\({}_{+3.15}\) & 22.23\({}_{+3.43}\) & 43.48\({}_{+3.34}\) & 11.76\({}_{+3.27}\) & 81.93\({}_{+2.47}\) & **82.35\({}_{+2.09}\)** \\  & 500 & 87.93\({}_{+1.52}\) & 58.17\({}_{+6.02}\) & 49.44\({}_{+5.75}\) & 12.03\({}_{+4.20}\) & 22.88\({}_{+2.57}\) & 53.67\({}_{+2.94}\) & 10.59\({}_{+3.94}\) & **90.81\({}_{+3.77}\)** & 89.78\({}_{+1.73}\) \\ \hline \multirow{6}{*}{fourier} & 20 & N/A & – & 11.28\({}_{+1.63}\) & 11.52\({}_{+2.68}\) & 9.24\({}_{+2.16}\) & 17.76\({}_{+3.96}\) & 9.78\({}_{+1.33}\) & 31.94\({}_{+3.25}\) & **38.16\({}_{+4.48}\)** \\  & 50 & 65.32\({}_{+3.86}\) & – & N/A & 9.98\({}_{+1.96}\) & 12.12\({}_{+2.56}\) & 31.76\({}_{+3.66}\) & 11.24\({}_{+2.20}\) & 65.64\({}_{+3.75}\) & **66.04\({}_{+2.91}\)** \\  & 100 & 73.82\({}_{+1.70}\) & 44.84\({}_{+5.58}\) & 29.68\({}_{+5.97}\) & 11.04\({}_{+2.51}\) & 21.90\({}_{+4.66}\) & 35.60\({}_{+4.77}\) & 9.30\({}_{+2.87}\) & 74.28\({}_{+2.81}\) & **74.92\({}_{+3.12}\)** \\  & 200 & 78.66\({}_{+1.64}\) & 48.56\({}_{+3.52}\) & 41.94\({}_{+5.54}\) & 10.24\({}_{+2.93}\) & 31.36\({}_{+4.16}\) & 44.80\({}_{+3.89}\) & 8.88\({}_{+2.32}\) & 79.48\({}_{+2.18}\) & **79.52\({}_{+2.03}\)** \\  & 500 & 79.64\({}_{+3.93}\) & 54.80\({}_{+4.51}\) & 57.06\({}_{+4.99}\) & 11.94\({}_{+2.32}\) & 55.06\({}_{+4.54}\) & 54.03\({}_{+1.10}\) & 10.66\({}_{+2.81}\) & 81.64\({}_{+1.61}\) & **81.85\({}_{+3.55}\)** \\ \hline \multirow{6}{*}{biodes} & 20 & **69.34\({}_{+1.63}\)** & 64.52\({}_{+7.42}\) & 52.83\({}_{+7.85}\) & 53.88\({}_{+3.31}\) & 49.80\({}_{+0.45}\) & 60.95\({}_{+6.69}\) & 50.47\({}_{+9.67}\) & 67.37\({}_{+8.48}\) & 67.06\({}_{+4.88}\) \\  & 50 & 70.17\({}_{+3.69}\) & 68.14\({}_{+3.56}\) & 62.50\({}_{+6.74}\) & 54.84\({}_{+7.67}\) & 49.94\({}_{+1.03}\) & 67.21\({}_{+4.25}\) & 51.99\({}_{+6.12}\) & **71.94\({}_{+4.16}\)** & 71.91\({}_{+2.77}\) \\  & 100 & 74.80\({}_{+2.96}\) & 71.82\({}_{+3.70}\) & 68.29\({}_{+2.67}\) & 56.44\({}_{+5.66}\) & 49.62\({}_{+0.68}\) & 67.68\({}_{+2.88}\) & 50.71\({}_{+5.41}\) & **75.68\({}_{+2.01}\)** & 74.93\({}_{+2.08}\) \\  & 200 & 77.49\({}_{+2.13}\) & 69.22\({}_{+3.30}\) & 69.63\({}_{+2.86}\) & 55.47\({}_{+7.36}\) & 48.70\({}_{+1.53}\) & 70.40\({}_{+3.81}\) & 48.04\({}_{+5.01}\) & **78.64\({}_{+2.29}\)** & 76.96\({}_{+3.02}\) \\  & 500 & **80.31\({}_{+1.32}\)** & 74.83\({}_{+1.76}\) & 74.44\({}_{+2.90}\) & 51.30\({}_{+3.30}\) & 49.68\({}_{+0.64}\) & 73.88\({}_{+1.89}\) & 45.73\({}_{+8.18}\) & 79.56\({}_{+1.95}\) & 78.66\({}_{+1.76}\) \\ \hline \multirow{6}{*}{steel} & 20 & 55.71\({}_{+3.47}\) & 55.23\({}_{+3.41}\) & 52.14\({}_{+1.54}\) & 50.93\({}_{+6.29}\) & 49.93\({}_{+0.86}\) & 52.02\({}_{+3.80}\) & 48.88\({}_{+4.70}\) & 57.02\({}_{+2.92}\) & **82.77\({}_{+2.40}\)** \\  & 50 & 26.30\({}_{+2.94}\) & 56.05\({}_{+3.07}\) & 55.40\({}_{+4.01}\) & 52.16\({}_{+3.86}\) & 49.98\({}_{+0.29}\) & 54.55\({}_{+2.89}\) & 60.55\({}_{+1.92}\) & 60.62\({}_{+2.55}\) & **66.69\({}_

[MISSING_PAGE_EMPTY:46]

[MISSING_PAGE_FAIL:47]

#### d.7.2 DCR Evaluation

\begin{table}
\begin{tabular}{c c|c c c c c c c c c} \hline \hline Datasets & \(N_{\text{real}}\) & SMOTE & TVAE & CTGAN & NFLOW & TabDDPM & ARF & TabPFGen & **TabEM** \\ \hline \multirow{9}{*}{protein} & 20 & N/A & 0.24\(\pm\)0.06 & 0.36\(\pm\)0.10 & 0.29\(\pm\)0.07 & **0.60\(\pm\)**0.06 & 0.49\(\pm\)0.03 & 0.24\(\pm\)0.11 & 0.39\(\pm\)0.05 \\  & 50 & 0.20\(\pm\)0.03 & 0.34\(\pm\)0.09 & 0.42\(\pm\)0.06 & 0.21\(\pm\)0.06 & **0.62\(\pm\)**0.01 & 0.47\(\pm\)0.08 & 0.21\(\pm\)0.01 & 0.37\(\pm\)0.11 \\  & 100 & 0.20\(\pm\)0.03 & 0.33\(\pm\)0.07 & 0.37\(\pm\)0.05 & 0.26\(\pm\)0.10 & **0.54\(\pm\)**0.03 & 0.46\(\pm\)0.06 & 0.12\(\pm\)0.07 & 0.27\(\pm\)0.15 \\  & 200 & 0.19\(\pm\)0.03 & 0.31\(\pm\)0.05 & 0.35\(\pm\)0.04 & 0.31\(\pm\)0.06 & **0.51\(\pm\)**0.04 & 0.44\(\pm\)0.05 & 0.10\(\pm\)0.03 & 0.26\(\pm\)0.06 \\  & 500 & 0.19\(\pm\)0.02 & 0.32\(\pm\)0.06 & 0.30\(\pm\)0.05 & 0.33\(\pm\)0.02 & **0.48\(\pm\)**0.03 & 0.43\(\pm\)0.05 & 0.09\(\pm\)0.06 & 0.23\(\pm\)0.13 \\ \hline \multirow{9}{*}{protein} & 20 & N/A & 0.19\(\pm\)0.17 & **0.61\(\pm\)**0.04 & 0.52\(\pm\)0.08 & 0.56\(\pm\)0.05 & 0.57\(\pm\)0.04 & 0.48\(\pm\)0.00 & 0.40\(\pm\)0.00 \\  & 50 & 0.20\(\pm\)0.02 & 0.29\(\pm\)0.26 & 0.48\(\pm\)0.17 & 0.33\(\pm\)0.10 & **0.67\(\pm\)**0.05 & 0.54\(\pm\)0.07 & 0.31\(\pm\)0.07 & 0.43\(\pm\)0.03 \\  & fourier & 100 & 0.23\(\pm\)0.02 & 0.53\(\pm\)0.06 & 0.50\(\pm\)0.04 & 0.37\(\pm\)0.09 & **0.60\(\pm\)**0.08 & 0.59\(\pm\)0.06 & 0.31\(\pm\)0.07 & 0.44\(\pm\)0.04 \\  & 200 & 0.22\(\pm\)0.02 & 0.56\(\pm\)0.06 & 0.53\(\pm\)0.05 & 0.37\(\pm\)0.08 & **0.58\(\pm\)**0.02 & 0.56\(\pm\)0.04 & 0.30\(\pm\)0.03 & 0.46\(\pm\)0.04 \\  & 500 & 0.25\(\pm\)0.03 & **0.67\(\pm\)**0.04 & 0.54\(\pm\)0.06 & 0.40\(\pm\)0.06 & 0.62\(\pm\)0.04 & 0.61\(\pm\)0.05 & 0.28\(\pm\)0.03 & 0.45\(\pm\)0.05 \\ \hline \multirow{9}{*}{protein} & 20 & 0.29\(\pm\)0.05 & 0.19\(\pm\)0.08 & 0.26\(\pm\)0.07 & 0.33\(\pm\)0.08 & 0.26\(\pm\)0.15 & **0.46\(\pm\)**0.05 & 0.38\(\pm\)0.03 & 0.39\(\pm\)0.04 \\  & 50 & 0.18\(\pm\)0.05 & 0.17\(\pm\)0.06 & 0.16\(\pm\)0.04 & 0.24\(\pm\)0.04 & 0.14\(\pm\)0.05 & **0.31\(\pm\)**0.05 & 0.31\(\pm\)0.07 & 0.30\(\pm\)0.07 \\  & 100 & 0.11\(\pm\)0.04 & 0.17\(\pm\)0.04 & 0.17\(\pm\)0.04 & 0.22\(\pm\)0.06 & 0.10\(\pm\)0.02 & 0.21\(\pm\)0.02 & **0.24\(\pm\)**0.07 & 0.21\(\pm\)0.08 \\  & 200 & 0.08\(\pm\)0.02 & 0.14\(\pm\)0.02 & 0.14\(\pm\)0.03 & 0.20\(\pm\)0.04 & 0.11\(\pm\)0.04 & **0.20\(\pm\)**0.04 & 0.13\(\pm\)0.08 & 0.15\(\pm\)0.07 \\  & 500 & 0.08\(\pm\)0.03 & 0.16\(\pm\)0.03 & 0.13\(\pm\)0.03 & 0.18\(\pm\)0.05 & 0.10\(\pm\)0.04 & **0.18\(\pm\)**0.03 & 0.05\(\pm\)0.03 & 0.09\(\pm\)0.04 \\ \hline \multirow{9}{*}{protein} & 20 & 0.38\(\pm\)0.09 & 0.21\(\pm\)0.10 & 0.25\(\pm\)0.12 & 0.33\(\pm\)0.07 & **0.48\(\pm\)**0.15 & 0.43\(\pm\)0.08 & 0.23\(\pm\)0.05 & 0.21\(\pm\)0.03 \\  & 50 & 0.27\(\pm\)0.11 & 0.27\(\pm\)0.09 & 0.22\(\pm\)0.09 & 0.34\(\pm\)0.05 & **0.45\(\pm\)**0.15 & 0.40\(\pm\)0.06 & 0.15\(\pm\)0.06 & 0.24\(\pm\)0.08 \\  & 100 & 0.20\(\pm\)0.11 & 0.28\(\pm\)0.07 & 0.22\(\pm\)0.08 & 0.30\(\pm\)0.08 & 0.37\(\pm\)0.09 & **0.41\(\pm\)**0.07 & 0.15\(\pm\)0.09 & 0.25\(\pm\)0.10 \\  & 200 & 0.19\(\pm\)0.08 & 0.28\(\pm\)0.04 & 0.22\(\pm\)0.04 & 0.32\(\pm\)0.06 & 0.31\(\pm\)0.11 & **0.40\(\pm\)**0.04 & 0.14\(\pm\)0.06 & 0.30\(\pm\)0.09 \\  & 500 & 0.17\(\pm\)0.04 & 0.29\(\pm\)0.07 & 0.24\(\pm\)0.07 & 0.32\(\pm\)0.05 & 0.21\(\pm\)0.07 & **0.37\(\pm\)**0.05 & 0.10\(\pm\)0.05 & 0.25\(\pm\)0.09 \\ \hline \multirow{9}{*}{stock} & 20 & 0.24\(\pm\)0.05 & 0.37\(\pm\)0.08 & 0.42\(\pm\)0.07 & 0.46\(\pm\)0.05 & 0.45\(\pm\)0.12 & **0.50\(\pm\)**0.05 & 0.41\(\pm\)0.06 & 0.46\(\pm\)0.03 \\  & 50 & 0.16\(\pm\)0.03 & 0.41\(\pm\)0.08 & 0.34\(\pm\)0.04 & 0.43\(\pm\)0.06 & 0.28\(\pm\)0.07 & 0.39\(\pm\)0.03 & 0.37\(\pm\)0.14 & **0.46\(\pm\)**0.02 \\  & 100 & 0.15\(\pm\)0.04 & 0.39\(\pm\)0.04 & 0.33\(\pm\)0.05 & **0.46\(\pm\)**0.04 & 0.17\(\pm\)0.02 & 0.33\(\pm\)0.03 & 0.34\(\pm\)0.09 & 0.44\(\pm\)0.03 \\  & 200 & 0.14\(\pm\)0.02 & 0.38\(\pm\)0.04 & 0.28\(\pm\)0.03 & 0.45\(\pm\)0.

[MISSING_PAGE_FAIL:49]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 1 details our research objectives and highlights our contributions.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Presented in Section 4.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Section 2 presents the theoretical results of our proposed method.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Refer to Appendix B, where we provide full details on reproducing the results in the paper. We provide an open-source library of the proposed method.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Refer to Appendix B. All datasets used in this paper are publicly available, and the implementations of benchmark generators are open-source. We also provide an open-source library https://github.com/andreimargeloiu/TabEBM.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Appendix B provides full descriptions of the experimental setup.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Refer to Section 3, where we provide standard deviations for all tables. Figure 6 and Figure 4 (Right) contain error bars. Due to the page limit, the error bars for all other figures are available in Appendix D. In Section 3.2 and Appendix D.6, we show statistical significance tests of the similarity between real data and synthetic data.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?Answer: [Yes] Justification: Refer to Appendix B.4, where we provide full details on the computation resources used in the paper.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We carefully check the NeurIPS Code of Ethics, and we confirm that our work follows the Code in every respect.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Refer to Appendix A, where we include the societal impacts of our work.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA]
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Refer to Appendix B, where we provide the open-source licenses followed by the creators or original owners of assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide the implementation of our method as a python library attached to this submission. We will make it publicly available post-publication.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [No] Justification:[NA]