# Model Sparsity Can Simplify Machine Unlearning

Jinghan Jia\({}^{1,\star}\) Jiancheng Liu\({}^{1,\star}\) Parikshit Ram\({}^{2}\) Yuguang Yao\({}^{1}\) Gaowen Liu\({}^{3}\)

Yang Liu\({}^{4,5}\)&Pranay Sharma\({}^{6}\)&Sijia Liu\({}^{1,2}\)

\({}^{1}\)Michigan State University, \({}^{2}\)IBM Research, \({}^{3}\)Cisco Research,

\({}^{4}\)University of California, Santa Cruz, \({}^{5}\)ByteDance Research, \({}^{6}\)Carnegie Mellon University

\({}^{\star}\)Equal contribution

###### Abstract

In response to recent data regulation requirements, machine unlearning (MU) has emerged as a critical process to remove the influence of specific examples from a given model. Although exact unlearning can be achieved through complete model retraining using the remaining dataset, the associated computational costs have driven the development of efficient, approximate unlearning techniques. Moving beyond data-centric MU approaches, our study introduces a novel model-based perspective: model sparsification via weight pruning, which is capable of reducing the gap between exact unlearning and approximate unlearning. We show in both theory and practice that model sparsity can boost the multi-criteria unlearning performance of an approximate unlearner, closing the approximation gap, while continuing to be efficient. This leads to a new MU paradigm, termed prune first, then unlearn, which infuses a sparse model prior into the unlearning process. Building on this insight, we also develop a sparsity-aware unlearning method that utilizes sparsity regularization to enhance the training process of approximate unlearning. Extensive experiments show that our proposals consistently benefit MU in various unlearning scenarios. A notable highlight is the 77% unlearning efficacy gain of fine-tuning (one of the simplest unlearning methods) when using sparsity-aware unlearning. Furthermore, we demonstrate the practical impact of our proposed MU methods in addressing other machine learning challenges, such as defending against backdoor attacks and enhancing transfer learning. Codes are available at https://github.com/OPTML-Group/Unlearn-Sparse.

## 1 Introduction

Machine unlearning (**MU**) initiates a reverse learning process to scrub the influence of data points from a trained machine learning (**ML**) model. It was introduced to avoid information leakage about private data upon completion of training [1; 2; 3], particularly in compliance with legislation like 'the right to be forgotten' [4] in General Data Protection Regulation (GDPR) [5]. The _direct but optimal_ unlearning approach is _exact unlearning_ to _retrain_ ML models from scratch using the remaining training set, after removing the data points to be scrubbed. Although retraining yields the _ground-truth_ unlearning strategy, it is the most computationally intensive one. Therefore, the development of _approximate but fast_ unlearning methods has become a major focus in research [6; 7; 8; 9; 10].

Despite the computational benefits of approximate unlearning, it often lacks a strong guarantee on the effectiveness of unlearning, resulting in a performance gap with exact unlearning [11]. In particular, we encounter two main challenges. _First_, the performance of approximate unlearning can heavily rely on the configuration of algorithmic parameters. For example, the Fisher forgetting method [12] needsto carefully tune the Fisher information regularization parameter in each data-model setup. _Second_, the effectiveness of an approximate scheme can vary significantly across the different unlearning evaluation criteria, and their trade-offs are not well understood. For example, high 'efficacy' (ability to protect the privacy of the scrubbed data) _neither_ implies _nor_ precludes high 'fidelity' (accuracy on the remaining dataset) [9]. This raises our **driving question (Q)** below:

**(Q)** _Is there a theoretically-grounded and broadly-applicable method to improve approximate unlearning across different unlearning criteria?_

To address **(Q)**, we advance MU through a fresh and novel viewpoint: **model sparsification**. _Our key finding_ is that model sparsity (achieved by weight pruning) can significantly reduce the gap between approximate unlearning and exact unlearning; see **Fig. 1** for the schematic overview of our proposal and highlighted empirical performance.

Model sparsification (or weight pruning) has been extensively studied in the literature [13; 14; 15; 16; 17; 18; 19], focusing on the interrelation between model compression and generalization. For example, the notable lottery ticket hypothesis (**LTH**) [15] demonstrated the existence of a sparse subnetwork (the so-called 'winning ticket') that matches or even exceeds the test accuracy of the original dense model. In addition to generalization, the impact of pruning has also been investigated on model robustness [20; 21; 22], fairness [23; 24], interpretability [25; 26], loss landscape [16; 26], and privacy [27; 28]. In particular, the privacy gains from pruning [27; 28] imply connections between data influence and model sparsification.

More recently, a few works [29; 30] attempted to draw insights from pruning for unlearning. In Wang et al. [29], removing channels of a deep neural network (**DNN**) showed an unlearning benefit in federated learning. And in Ye et al. [30], filter pruning was introduced in lifelong learning to detect "pruning identified exemplars" [31] that are easy to forget. **However**, different from the above literature that customized model pruning for a specific unlearning application, our work systematically and comprehensively explores and exploits the foundational connections between unlearning and pruning. We summarize our **contributions** below.

\(\bullet\) First, we provide a holistic understanding of MU across the full training/evaluation stack.

\(\bullet\) Second, we draw a tight connection between MU and model pruning and show in theory and practice that model sparsity helps close the gap between approximate unlearning and exact unlearning.

\(\bullet\) Third, we develop a new MU paradigm termed 'prune first, then unlearn', and investigate the influence of pruning methods in the performance of unlearning. Additionally, we develop a novel'sparsity-aware unlearning' framework that leverages a soft sparsity regularization scheme to enhance the approximate unlearning process.

\(\bullet\) Finally, we perform extensive experiments across diverse datasets, models, and unlearning scenarios. Our findings consistently highlight the crucial role of model sparsity in enhancing MU.

## 2 Revisiting Machine Unlearning and Evaluation

**Problem setup.** MU aims to remove (or scrub) the influence of some targeted training data on a trained ML model [1; 2]. Let \(\mathcal{D}=\{\mathbf{z}_{i}\}_{i=1}^{N}\) be a (training) dataset of \(N\) data points, with label

Figure 1: Schematic overview of our proposal on model sparsity-driven MU. Evaluation at-a-glance shows the performance of three unlearning methods (retraining-based exact unlearning, finetuning-based approximate unlearning [12], and proposed unlearning on 95%-sparse model) under five metrics: unlearning accuracy (UA), membership inference attack (MIA)-based unlearning efficacy, accuracy on remaining data (RA), testing accuracy (TA), and run-time efficiency (RTE); see summary in **Tab. 1**. The unlearning scenario is given by class-wise forgetting, where data points of a single class are scrubbed. Each metric is normalized to \([0,1]\) based on the best result across unlearning methods for ease of visualization. Results indicate that model sparsity reduces the gap between exact and approximate MU without loss in efficiency.

information encoded for supervised learning. \(\mathcal{D}_{\mathrm{f}}\subseteq\mathcal{D}\) represents a subset whose influence we want to scrub, termed the **forgetting dataset**. Accordingly, the complement of \(\mathcal{D}_{\mathrm{f}}\) is the **remaining dataset**, _i.e._, \(\mathcal{D}_{\mathrm{r}}=\mathcal{D}\setminus\mathcal{D}_{\mathrm{f}}\). We denote by \(\bm{\theta}\) the model parameters, and \(\bm{\theta}_{\mathrm{o}}\) the **original model** trained on the entire training set \(\mathcal{D}\) using _e.g._, empirical risk minimization (ERM). Similarly, we denote by \(\bm{\theta}_{\mathrm{u}}\) an **unlearned model**, obtained by a scrubbing algorithm, after removing the influence of \(\mathcal{D}_{\mathrm{f}}\) from the trained model \(\bm{\theta}_{\mathrm{o}}\). The **problem of MU** is to find an accurate and efficient scrubbing mechanism to generate \(\bm{\theta}_{\mathrm{u}}\) from \(\bm{\theta}_{\mathrm{o}}\). In existing studies [2; 7; 12], the choice of the forgetting dataset \(\mathcal{D}_{\mathrm{f}}\) specifies different unlearning scenarios. There exist two main categories. First, _class-wise forgetting_[7; 12] refers to unlearning \(\mathcal{D}_{\mathrm{f}}\) consisting of training data points of an entire class. Second, _random data forgetting_ corresponds to unlearning \(\mathcal{D}_{\mathrm{f}}\) given by a subset of random data drawn from all classes.

**Exact and approximate MU methods.** The _exact unlearning_ method refers to _retraining_ the model parameters from _scratch_ over the remaining dataset \(\mathcal{D}_{\mathrm{r}}\). Although retraining from scratch (that we term **Retrain**) is optimal for MU, it entails a large computational overhead, particularly for DNN training. This problem is alleviated by _approximate unlearning_, an easy-to-compute proxy for Retrain, which has received growing attention. Yet, the boosted computation efficiency comes at the cost of MU's efficacy. We next review some commonly-used approximate unlearning methods that we improve in the sequel by leveraging sparsity; see a summary in **Tab.1**.

\(\blacklozenge\)_Fine-tuning (**FT**)_[6; 12]: Different from Retrain, FT fine-tunes the pre-trained model \(\bm{\theta}_{\mathrm{o}}\) on \(\mathcal{D}_{\mathrm{r}}\) using a few training epochs to obtain \(\bm{\theta}_{\mathrm{u}}\). The rationale is that fine-tuning on \(\mathcal{D}_{\mathrm{r}}\) initiates the catastrophic forgetting in the model over \(\mathcal{D}_{\mathrm{f}}\) as is common in continual learning [33].

\(\blacklozenge\)_Gradient ascent (**GA**)_[7; 8]: GA reverses the model training on \(\mathcal{D}_{\mathrm{f}}\) by adding the corresponding gradients back to \(\bm{\theta}_{\mathrm{o}}\), _i.e._, moving \(\bm{\theta}_{\mathrm{o}}\) in the direction of increasing loss for data points to be scrubbed.

\(\blacklozenge\)_Fisher forgetting (**FF**)_[9; 12]: FF adopts an additive Gaussian noise to 'perturb' \(\bm{\theta}_{\mathrm{o}}\) towards exact unlearning. Here the Gaussian distribution has zero mean and covariance determined by the \(4\)th root of Fisher Information matrix with respect to (w.r.t.) \(\bm{\theta}_{\mathrm{o}}\) on \(\mathcal{D}_{\mathrm{r}}\). We note that the computation of the Fisher Information matrix exhibits lower parallel efficiency in contrast to other unlearning methods, resulting in higher computational time when executed on GPUs; see Golatkar et al. [12] for implementation details.

\(\blacklozenge\)_Influence unlearning (**IU**)_[10; 32]: IU leverages the influence function approach [34] to characterize the change in \(\bm{\theta}_{\mathrm{o}}\) of a training point is removed from the training loss. IU estimates the change in model parameters from \(\bm{\theta}_{\mathrm{o}}\) to \(\bm{\theta}_{\mathrm{u}}\), _i.e._, \(\bm{\theta}_{\mathrm{u}}-\bm{\theta}_{\mathrm{o}}\). IU also relates to an important line of research in MU, known as \(\epsilon\)-\(\delta\) forgetting [29; 35; 36]. However, it typically requires additional model and training assumptions [35].

We next take a step further to revisit the IU method and re-derive its formula (**Prop. 1**), with the aim of enhancing the effectiveness of existing solutions proposed in the previous research.

**Proposition 1**: _Given the weighted ERM training \(\bm{\theta}(\mathbf{w})=\arg\min_{\bm{\theta}}L(\mathbf{w},\bm{\theta})\) where \(L(\mathbf{w},\bm{\theta})=\sum_{i=1}^{N}[w_{i}\ell_{i}(\bm{\theta},\bm{z}_{i})]\), \(w_{i}\in[0,1]\) is the influence weight associated with the data point \(\bm{z}_{i}\) and \(\bm{1}^{T}\mathbf{w}=1\), the model update from \(\bm{\theta}_{\mathrm{o}}\) to \(\bm{\theta}(\mathbf{w})\) yields_

\[\Delta(\mathbf{w}):=\bm{\theta}(\mathbf{w})-\bm{\theta}_{\mathrm{o}}\approx \mathbf{H}^{-1}\nabla_{\bm{\theta}}L(\bm{1}/N-\mathbf{w},\bm{\theta}_{\mathrm{o }}),\] (1)

_where \(\bm{1}\) is the \(N\)-dimensional vector of all ones, \(\mathbf{w}=\bm{1}/N\) signifies the uniform weights used by ERM, \(\mathbf{H}^{-1}\) is the inverse of the Hessian \(\nabla_{\bm{\theta},\bm{\theta}}^{2}L(\bm{1}/N,\bm{\theta}_{\mathrm{o}})\) evaluated at \(\bm{\theta}_{\mathrm{o}}\), and \(\nabla_{\bm{\theta}}L\) is the gradient of \(L\). When scrubbing \(\mathcal{D}_{\mathrm{f}}\), the unlearned model is given by \(\bm{\theta}_{\mathrm{u}}=\bm{\theta}_{\mathrm{o}}+\Delta(\mathbf{w}_{\mathrm{ MU}})\). Here \(\mathbf{w}_{\mathrm{MU}}\in[0,1]^{N}\) with entries \(w_{\mathrm{MU},i}=\mathbb{I}_{\mathcal{D}_{\mathrm{r}}}(i)/[\mathcal{D}_{ \mathrm{r}}]\) signifying the data influence weights for MU, \(\mathbb{I}_{\mathcal{D}_{\mathrm{r}}}(i)\) is the indicator function with value \(I\) if \(i\in\mathcal{D}_{\mathrm{r}}\) and 0 otherwise, and \(|\mathcal{D}_{\mathrm{r}}|\) is the cardinality of \(\mathcal{D}_{\mathrm{r}}\)._

**Proof**: We derive (1) using an implicit gradient approach; see Appendix A.

It is worth noting that we have taken into consideration the weight normalization effect \(\bm{1}^{T}\mathbf{w}=1\) in (1). This is different from existing work like Izzo et al. [10, Sec. 3] using Boolean or unbounded

\begin{table}
\begin{tabular}{c|c c c c c|c} \hline Unlearning & \multicolumn{5}{c}{Evaluation metrics} & Representative work \\ Methods & UA & All-Efficacy & RA & T& RTE & Representative work \\ \hline FT & ✓ & ✓ & ✓ & ✓ & 0.06\(\times\) & [6; 12] \\ GA & ✓ & ✓ & ✓ & ✓ & 0.02\(\times\) & [7; 8] \\ PF & & ✓ & ✓ & ✓ & 0.9\(\times\) & [9; 12] \\ IU & ✓ & ✓ & ✓ & 0.08\(\times\) & [10; 32] \\ \hline Ours & ✓ & ✓ & ✓ & ✓ & 0.07\(\times\) & This work \\ \hline \end{tabular}
\end{table}
Table 1: Summary of approximate unlearning methods considered in this work. The marker ‘\(\blacklozenge\)’ denotes the metric used in previous research. The number in RTE is the run-time cost reduction compared to the cost of Retrain, based on our empirical studies in Sec. 5 on (CIFAR-10, ResNet-18). Note that GA seems better than ours in terms of RTE, but it is less effective in unlearning.

weights. In practice, we found that IU with weight normalization can improve the unlearning performance. Furthermore, to update the model influence given by (1), one needs to acquire the second-order information in the form of inverse-Hessian gradient product. Yet, the exact computation is prohibitively expensive. To overcome this issue, we use the first-order WoodFisher approximation [37] to estimate the inverse-Hessian gradient product.

**Towards a 'full-stack' MU evaluation.** Existing work has assessed MU performance from different aspects [7, 8, 12]. Yet, a single performance metric may provide a limited view of MU [11]. By carefully reviewing the prior art, we focus on the following empirical metrics (summarized in Tab. 1).

\(\blacklozenge\)_Unlearning accuracy (UA)_: We define \(\text{UA}(\bm{\theta}_{\text{u}})=1-\text{Acc}_{\mathcal{D}_{\text{f}}}(\bm{ \theta}_{\text{u}})\) to characterize the _efficacy_ of MU in the accuracy dimension, where \(\text{Acc}_{\mathcal{D}_{\text{f}}}(\bm{\theta}_{\text{u}})\) is the accuracy of \(\bm{\theta}_{\text{u}}\) on the forgetting dataset \(\mathcal{D}_{\text{f}}\)[7, 12]. It is important to note that a more favorable UA for an approximate unlearning method should **reduce its performance disparity with the gold-standard retrained model (Retrain)**; a higher value is not necessarily better. This principle also extends to other evaluation metrics.

\(\blacklozenge\)_Membership inference attack (MIA) on \(\mathcal{D}_{\text{f}}\) (MIA-Efficacy)_: This is another metric to assess the _efficacy_ of unlearning. It is achieved by applying the confidence-based MIA predictor [38, 39] to the unlearned model (\(\bm{\theta}_{\text{u}}\)) on the forgetting dataset (\(\mathcal{D}_{\text{f}}\)). The MIA success rate can then indicate how many samples in \(\mathcal{D}_{\text{f}}\) can be correctly predicted as forgetting (_i.e._, non-training) samples of \(\bm{\theta}_{\text{u}}\). A _higher_ MIA-Efficacy implies less information about \(\mathcal{D}_{\text{f}}\) in \(\bm{\theta}_{\text{u}}\); see Appendix C.3 for more details.

\(\blacklozenge\)_Remaining accuracy (RA)_: This refers to the accuracy of \(\bm{\theta}_{\text{u}}\) on \(\mathcal{D}_{\text{r}}\), which reflects the _fidelity_ of MU [9], _i.e._, training data information should be preserved from \(\bm{\theta}_{\text{o}}\) to \(\bm{\theta}_{\text{u}}\).

\(\blacklozenge\)_Testing accuracy (TA)_: This measures the _generalization_ ability of \(\bm{\theta}_{\text{u}}\) on a testing dataset rather than \(\mathcal{D}_{\text{f}}\) and \(\mathcal{D}_{\text{r}}\). TA is evaluated on the whole test dataset, except for class-wise forgetting, in which testing data points belonging to the forgetting class are not in the testing scope.

\(\blacklozenge\)_Run-time efficiency (RTE)_: This measures the computation efficiency of an MU method. For example, if we regard the run-time cost of Retrain as the baseline, the computation acceleration gained by different approximate unlearning methods is summarized in Tab. 1.

## 3 Model Sparsity: A Missing Factor Influencing Machine Unlearning

**Model sparsification via weight pruning.** Model sparsification could not only facilitate a model's training, inference, and deployment but also benefit model's performance. For example, LTH (lottery ticket hypothesis) [15] stated that a trainable sparse sub-model could be identified from the original dense model, with test accuracy on par even better than the original model. **Fig. 2** shows an example of the pruned model's generalization vs. its sparsity ratio. Here one-shot magnitude pruning (**OMP**) [17] is adopted to obtain sparse models. OMP is computationally the lightest pruning method, which directly prunes the model weights to the target sparsity ratio based on their magnitudes. As we can see, there exists a graceful sparse regime with lossless testing accuracy.

**Gains of MU from sparsity.** We first analyze the impact of model sparsity on MU through a lens of _unrolling stochastic gradient descent_ (**SGD**) [8]. The specified SGD method allows us to derive the _unlearning error_ (given by the weight difference between the approximately unlearned model and the gold-standard retrained model) when scrubbing a single data point. However, different from Thudi et al. [8], we will infuse the model sparsity into SGD unrolling.

Let us assume a binary mask \(\mathbf{m}\) associated with the model parameters \(\bm{\theta}\), where \(m_{i}=0\) signifies that the \(i\)th parameter \(\theta_{i}\) is pruned to zero and \(m_{i}=1\) represents the unmasked \(\theta_{i}\). This sparse pattern \(\mathbf{m}\) could be obtained by a weight pruning method, like OMP. Given \(\mathbf{m}\), the **sparse model** is \(\mathbf{m}\odot\bm{\theta}\), where \(\odot\) denotes the element-wise multiplication. Thudi et al. [8] showed that if GA is adopted to scrub a single data point for the original (dense) model \(\bm{\theta}\) (_i.e._, \(\mathbf{m}=\mathbf{1}\)), then the gap between GA and Retrain can be approximately bounded in the weight space. **Prop. 2** extends the existing unlearning error analysis to a sparse model.

Figure 2: Testing accuracy of OMP-based sparse ResNet-18 vs. the dense model on CIFAR-10.

**Proposition 2**: _Given the model sparse pattern \(\mathbf{m}\) and the SGD-based training, the unlearning error of GA, denoted by \(e(\mathbf{m})\), can be characterized by the weight distance between the GA-unlearned model and the gold-standard retrained model. This leads to the error bound_

\[e(\mathbf{m})=\mathcal{O}(\eta^{2}t\|\mathbf{m}\odot(\bm{\theta}_{t}-\bm{ \theta}_{0})\|_{2}\sigma(\mathbf{m}))\] (2)

_where \(\mathcal{O}\) is the big-O notation, \(\eta\) is the learning rate, \(t\) is the number of training iterations, \((\bm{\theta}_{t}-\bm{\theta}_{0})\) denotes the weight difference at iteration \(t\) from its initialization \(\bm{\theta}_{0}\), and \(\sigma(\mathbf{m})\) is the largest singular value (\(\sigma\)) of the Hessian \(\nabla^{2}_{\bm{\theta},\bm{\theta}}\)( for a training loss \(\ell\)) among the unmasked parameter dimensions, i.e., \(\sigma(\mathbf{m}):=\max_{j}\{\sigma_{j}(\nabla^{2}_{\bm{\theta},\bm{\theta}} \ell),\text{if }m_{j}\neq 0\}\)._

_Proof_: _See Appendix B. \(\square\)_

We next draw some key insights from **Prop. 2**. _First_, it is clear from (2) that the unlearning error reduces as the model sparsity in \(\mathbf{m}\) increases. By contrast, the unlearning error derived in Thudi et al. [8] for a dense model (_i.e._, \(\mathbf{m}=\mathbf{1}\)) is proportional to the dense model distance \(\|\bm{\theta}_{t}-\bm{\theta}_{0}\|_{2}\). Thus, model sparsity is beneficial to reducing the gap between (GA-based) approximate and exact unlearning. _Second_, the error bound (2) enables us to relate MU to the spectrum of the Hessian of the loss landscape. The number of active singular values (corresponding to nonzero dimensions in \(\mathbf{m}\)) decreases when the sparsity grows. However, it is important to note that in a high-sparsity regime, the model's generalization could decrease. Consequently, it is crucial to select the model sparsity to strike a balance between generalization and unlearning performance.

Inspired by Prop. 2, we ask: _Does the above benefit of model sparsification in MU apply to other approximate unlearning methods besides GA?_ This drives us to investigate the performance of approximate unlearning across the entire spectrum as depicted in Tab. 1. Therefore, **Fig. 3** shows the unlearning efficacy (UA and MIA-Efficacy), fidelity (RA), and generalization (TA) of different approximate unlearning methods in the sparse model regime. Here class-wise forgetting is considered for MU and OMP is used for weight pruning. As we can see, the efficacy of approximate unlearning is significantly improved as the model sparsity increases, _e.g._, UA and MIA-Efficacy of using FT over 90% sparsity. By contrast, FT over the dense model (0% sparsity) is the least effective for MU. Also, the efficacy gap between exact unlearning (Retrain) and approximate unlearning reduces on sparse models. Further, through the fidelity and generalization lenses, FT and FF yield the RA and TA performance closest to Retrain, compared to other unlearning methods. In the regime of ultra-high sparsity (99%), the efficacy of unlearning exhibits a tradeoff with RA and TA to some extent.

## 4 Sparsity-Aided Machine Unlearning

Our study in Sec. 3 suggests the new MU paradigm 'prune first, then unlearn', which leverages the fact that (approximate) unlearning on a sparse model yields a smaller unlearning error (Prop. 2) and improves the efficacy of MU (Fig. 3). This promising finding, however, raises some new questions. First, it remains elusive how the choice of a weight pruning method impacts the unlearning performance. Second, it leaves room for developing sparsity-aware MU methods that can directly scrub data influence from a dense model.

**Prune first, then unlearn: Choice of pruning methods.** There exist many ways to find the desired sparse model in addition to OMP. Examples include pruning at random initialization before training [40; 41] and simultaneous pruning-training iterative magnitude pruning (**IMP**) [15]. Thus, the problem of pruning method selection arises for MU. From the viewpoint of MU, the unlearner would

Figure 3: Performance of approximate unlearning (FT, GA, FF, IU) and exact unlearning (Retrain) in efficacy (UA and MIA-Efficacy), fidelity (RA), and generalization (TA) vs. model sparsity (achieved by OMP) in the data-model setup (CIFAR-10, ResNet-18). The unlearning scenario is class-wise forgetting, and the average unlearning performance over 10 classes is reported. We remark that being closer to Retrain performance is better for approximate MU schemes.

prioritize a pruning method that satisfies the following criteria: \(\blacklozenge\)_least dependence_ on the forgetting dataset (\(\mathcal{D}_{\mathrm{f}}\)), \(\blacklozenge\)_lossless generalization_ when pruning, and \(\blacklozenge\)_pruning efficiency_. The rationale behind \(\blacklozenge\) is that it is desirable _not_ to incorporate information of \(\mathcal{D}_{\mathrm{f}}\) when seeking a sparse model prior to unlearning. And the criteria \(\blacklozenge\) and \(\blacklozenge\) ensure that sparsity cannot hamper TA (testing accuracy) and RTE (run-time efficiency). Based on \(\blacklozenge\)-\(\blacklozenge\), we propose to use two pruning methods.

\(\blacklozenge\)**SynFlow** (synaptic flow pruning) [40]: SynFlow provides a (training-free) pruning method at initialization, even without accessing the dataset. Thus, it is uniquely suited for MU to meet the criterion \(\blacklozenge\). And SynFlow is easy to compute and yields a generalization improvement over many other pruning-at-initialization methods; see justifications in [40].

\(\blacklozenge\)**OMP** (one-shot magnitude pruning) [17]: Different from SynFlow, OMP, which we focused on in Sec. 3, is performed over the original model (\(\bm{\theta}_{\mathrm{o}}\)). It may depend on the forgetting dataset (\(\mathcal{D}_{\mathrm{f}}\)), but has a much weaker dependence compared to IMP-based methods. Moreover, OMP is computationally lightest (_i.e._ best for \(\blacklozenge\)) and can yield better generalization than SynFlow [18].

Furthermore, it is important to clarify that IMP (iterative magnitude pruning) is _not_ suitable for MU, despite being widely used to find the most accurate sparse models (_i.e._, best for criterion \(\blacklozenge\)). Compared with the proposed pruning methods, IMP has the largest computation overhead and the strongest correlation with the training dataset (including \(\mathcal{D}_{\mathrm{f}}\)), thereby deviating from \(\blacklozenge\) and \(\blacklozenge\). In **Fig. 4**, we show the efficacy of FT-based unlearning on sparse models generated using different pruning methods (SynFlow, OMP, and IMP). As we can see, unlearning on SynFlow or OMP-generated sparse models yields improved UA and MIA-Efficacy over that on the original dense model and IMP-generated sparse models. This unlearning improvement over the dense model is consistent with Fig. 3. More interestingly, we find that IMP _cannot_ benefit the unlearning efficacy, although it leads to the best TA. This is because IMP heavily relies on the training set including forgetting data points, which is revealed by the empirical results - the unlearning metrics get worse for IMP with increasing sparsity. Furthermore, when examining the performance of SynFlow and OMP, we observe that the latter generally outperforms the former, exhibiting results that are closer to those of Retrain. Thus, _OMP is the pruning method we will use by default_.

**Sparsity-aware unlearning.** We next study if pruning and unlearning can be carried out simultaneously, without requiring prior knowledge of model sparsity. Let \(L_{\mathrm{u}}(\bm{\theta};\bm{\theta}_{\mathrm{o}},\mathcal{D}_{\mathrm{r}})\) denote the unlearning objective function of model parameters \(\bm{\theta}\), given the pre-trained state \(\bm{\theta}_{\mathrm{o}}\), and the remaining training dataset \(\mathcal{D}_{\mathrm{r}}\). Inspired by sparsity-inducing optimization [42], we integrate an \(\ell_{1}\) norm-based sparse penalty into \(L_{\mathrm{u}}\). This leads to the problem of '\(\ell_{1}\)**-sparse MU**':

\[\bm{\theta}_{\mathrm{u}}=\operatorname*{arg\,min}_{\bm{\theta}}L_{\mathrm{u} }(\bm{\theta};\bm{\theta}_{\mathrm{o}},\mathcal{D}_{\mathrm{r}})+\gamma\|\bm{ \theta}\|_{1},\] (3)

where we specify \(L_{\mathrm{u}}\) by the fine-tuning objective, and \(\gamma>0\) is a regularization parameter that controls the penalty level of the \(\ell_{1}\) norm, thereby reducing the magnitudes of 'unimportant' weights.

In practice, the unlearning performance could be sensitive to the choice of the sparse regularization parameter \(\gamma\). To address this limitation, we propose the design of a sparse regularization scheduler. Specifically, we explore three schemes: (1) constant \(\gamma\), (2) linearly growing \(\gamma\) and (3) linearly decaying \(\gamma\); see Sec. 5.1 for detailed implementations. Our empirical evaluation presented in **Tab. 2**

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline MU & UA & MIA-Efficacy & RA & TA & RTE (min) \\ \hline Retrain & 5.41 & 13.12 & 100.00 & 94.42 & 42.15 \\ \(\ell_{1}\)-sparse MU + constant \(\gamma\) & 6.60 (1.19) & 14.64 (1.52) & 96.51 (3.49) & 87.30 (7.12) & 2.53 \\ \(\ell_{1}\)-sparse MU + linear growing \(\gamma\) & 3.80 (1.61) & 8.75 (4.37) & 97.13 (2.87) & 90.63 (3.79) & 2.53 \\ \(\ell_{1}\)-sparse MU + linear decaying \(\gamma\) & **5.35 (0.06)** & **12.71 (0.41)** & **97.39 (2.61)** & **91.26 (3.16)** & 2.53 \\ \hline \hline \end{tabular}
\end{table}
Table 2: MU performance comparison of using \(\ell_{1}\)-sparse MU with different sparsity schedulers of \(\gamma\) in (3) and using Retrain. The unlearning scenario is given by random data forgetting (10% data points across all classes) on (ResNet-18, CIFAR-10). A performance gap against Retrain is provided in (\(\bullet\)).

Figure 4: Influence of different pruning methods (SynFlow, OMP, and IMP) in unlearning efficacy (UA and MIA-Efficacy) and generalization (TA) on (CIFAR-10, ResNet-18). **Left**: UA vs. TA. **Right**: MIA-Efficacy vs. TA. Each point is a FT-based unlearned dense or sparse model (75% or 95% sparsity), or a retrained dense model.

[MISSING_PAGE_FAIL:7]

**Model sparsity improves approximate unlearning.** In **Tab. 3**, we study the impact of model sparsity on the performance of various MU methods in the 'prune first, then unlearn' paradigm. The performance of the exact unlearning method (Retrain) is also provided for comparison. Note that the better performance of approximate unlearning corresponds to the smaller performance gap with the gold-standard retrained model.

_First_, given an approximate unlearning method (FT, GA, FF, or IU), we consistently observe that model sparsity improves UA and MIA-Efficacy (_i.e._, the efficacy of approximate unlearning) without much performance loss in RA (_i.e._, fidelity). In particular, the performance gap between each approximate unlearning method and Retrain reduces as the model becomes sparser (see the '95% sparsity' column vs. the 'dense' column). Note that the performance gap against Retrain is highlighted in \((\cdot)\) for each approximate unlearning. We also observe that Retrain on the 95%-sparsity model encounters a 3% TA drop. Yet, from the perspective of approximate unlearning, this drop brings in a more significant improvement in UA and MIA-Efficacy when model sparsity is promoted. Let us take FT (the simplest unlearning method) for class-wise forgetting as an example. As the model sparsity reaches \(95\%\), we obtain \(51\%\) UA improvement and \(8\%\) MIA-Efficacy improvement. Furthermore, FT and IU on the 95%-sparsity model can better preserve TA compared to other methods. Table 3 further indicates that sparsity reduces average disparity compared to a dense model across various approximate MU methods and unlearning scenarios.

_Second_, existing approximate unlearning methods have different pros and cons. Let us focus on the regime of \(95\%\) sparsity. We observe that FT typically yields the best RA and TA, which has a tradeoff with its unlearning efficacy (UA and MIA-Efficacy). Moreover, GA yields the worst RA since it is most loosely connected with the remaining dataset \(\mathcal{D}_{\mathrm{r}}\). FF becomes ineffective when scrubbing random data points compared to its class-wise unlearning performance. Furthermore, IU causes a TA drop but yields the smallest gap with exact unlearning across diverse metrics under the \(95\%\) model sparsity. In Appendix C.4, we provide additional results on CIFAR-100 and SVHN datasets, as shown in Tab. A3, as well as on the ImageNet dataset, depicted in Tab. A5. Other results pertaining to the VGG-16 architecture are provided in Tab. A4.

**Effectiveness of sparsity-aware unlearning.** In **Fig. 5**, we showcase the effectiveness of the proposed sparsity-aware unlearning method, _i.e._, \(\ell_{1}\)-sparse MU. For ease of presentation, we focus on the comparison with FT and the optimal Retrain strategy in both class-wise forgetting and random data forgetting scenarios under (CIFAR-10, ResNet-18). As we can see, \(\ell_{1}\)-sparse MU outperforms FT in the unlearning efficacy (UA and MIA-Efficacy), and closes the performance gap with Retrain without losing the computation advantage of approximate unlearning. We refer readers to Appendix C.4 and Fig. A2 for further exploration of \(\ell_{1}\)-sparse MU on additional datasets.

**Application: MU for Trojan model cleanse.** We next present an application of MU to remove the influence of poisoned backdoor data from a learned model, following the backdoor attack setup [49], where an adversary manipulates a small portion of training data (_a.k.a._ poisoning ratio) by injecting a backdoor trigger (_e.g._, a small image patch) and modifying data labels towards a targeted incorrect label. The trained model is called _Trojan model_, yielding the backdoor-designated incorrect prediction if the trigger is present at testing. Otherwise, it behaves normally.

Figure 5: Performance of sparsity-aware unlearning vs. FT and Retrain on class-wise forgetting and random data forgetting under (CIFAR-10, ResNet-18). Each metric is normalized to \([0,1]\) based on the best result across unlearning methods for ease of visualization, while the actual best value is provided (_e.g._, \(2.52\) is the least computation time for class-wise forgetting).

Figure 6: Performance of Trojan model cleanse via proposed unlearning vs. model sparsity, where ‘Original’ refers to the original Trojan model. **Left**: ASR vs. model sparsity. **Right**: SA vs. model sparsity.

We then regard MU as a defensive method to scrub the harmful influence of poisoned training data in the model's prediction, with a similar motivation as Liu et al. [50]. We evaluate the performance of the unlearned model from two perspectives, backdoor attack success rate (**ASR**) and standard accuracy (**SA**). **Fig. 6** shows ASR and SA of the Trojan model (with poisoning ratio \(10\%\)) and its unlearned version using the simplest FT method against model sparsity. Fig. 6 also includes the \(\ell_{1}\)-sparse MU to demonstrate its effectiveness on model cleanse. Since it is applied to a dense model (without using hard thresholding to force weight sparsity), it contributes just a single data point at the sparsity level 0%. As we can see, the original Trojan model maintains \(100\%\) ASR and a similar SA across different model sparsity levels. By contrast, FT-based unlearning can reduce ASR without inducing much SA loss. Such a defensive advantage becomes more significant when sparsity reaches \(90\%\). Besides, \(\ell_{1}\)-sparse MU can also effectively remove the backdoor effect while largely preserving the model's generalization. Thus, our proposed unlearning shows promise in application of backdoor attack defense.

**Application: MU to improve transfer learning.** Further, we utilize the \(\ell_{1}\)-sparse MU method to mitigate the impact of harmful data classes of ImageNet on transfer learning. This approach is inspired by Jain et al. [51], which shows that removing specific negatively-influenced ImageNet classes and retraining a source model can enhance its transfer learning accuracy on downstream datasets after finetuning. However, retraining the source model introduces additional computational overhead. MU naturally addresses this limitation and offers a solution.

**Tab. 4** illustrates the transfer learning accuracy of the unlearned or retrained source model (ResNet-18) on ImageNet, with \(n\) classes removed. The downstream target datasets used for evaluation are SUN397 [52] and OxfordPets [53]. The employed finetuning approach is linear probing, which finetunes the classification head of the source model on target datasets while keeping the feature extraction network of the source model intact. As we can see, removing data classes from the source ImageNet dataset can lead to improved transfer learning accuracy compared to the conventional method of using the pre-trained model on the full ImageNet (_i.e._, \(n=0\)). Moreover, our proposed \(\ell_{1}\)-sparse MU method achieves comparable or even slightly better transfer learning accuracy than the retraining-based approach [51]. Importantly, \(\ell_{1}\)-sparse MU offers the advantage of computational efficiency 2\(\times\) speed up over previous method [51] across all cases, making it an appealing choice for transfer learning using large-scale models. Here we remark that in order to align with previous method [51], we employed a fast-forward computer vision training pipeline (FFCV) [54] to accelerate our ImageNet training on GPUs.

**Additional results.** We found that model sparsity also enhances the privacy of the unlearned model, as evidenced by a lower MIA-Privacy. Refer to Appendix C.4 and Fig. A1 for more results. In addition, we have expanded our experimental scope to encompass the 'prune first, then unlearn' approach across various datasets and architectures. The results can be found in Tab. A3, Tab. A4, and Tab. A5. Furthermore, we conducted experiments on the \(\ell_{1}\)-sparse MU across different datasets, the Swin-Transformer architecture, and varying model sizes within the ResNet family. The corresponding findings are presented in Fig. A2 and Tab. A6, A7, A8 and A9.

## 6 Related Work

While Sec. 2 provides a summary of related works concerning exact and approximate unlearning methods and metrics, a more comprehensive review is provided below.

**Machine unlearning.** In addition to exact and approximate unlearning methods as we have reviewed in Sec. 2, there exists other literature aiming to develop the probabilistic notion of unlearning [55; 56; 57; 58], in particular through the lens of differential privacy (DP) [59]. Although DP enables unlearning with provable error guarantees, they typically require strong model and algorithmic assumptions and could lack effectiveness when facing practical adversaries, _e.g._, membership inference attacks. Indeed, evaluating MU is far from trivial [8; 9; 11]. Furthermore, the attention on MU has also been raised

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{_Pouraging class_} & \multicolumn{2}{c|}{0} & \multicolumn{2}{c|}{100} & \multicolumn{2}{c|}{200} & \multicolumn{2}{c}{300} \\  & Acc & Acc & Time & Acc & Time & Acc & Time \\ \hline \multicolumn{7}{c}{_OkstPost_} \\ \hline Method [51] & 85.70 & 85.79 & 71.84 & 86.10 & 61.51 & 86.32 & 54.53 \\ \(\ell_{1}\)-sparse MU & 85.83 & 85.34 & 86.12 & 50.19 & 86.26 & 26.49 \\ \hline \multicolumn{7}{c}{_SUN397_} \\ \hline Method [51] & 46.55 & 46.67 & 73.26 & 47.14 & 61.43 & 47.31 & 55.24 \\ \(\ell_{1}\)-sparse MU & 46.75 & 47.30 & 66.77 & 47.25 & 50.96 & 47.37 & 27.12 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Transfer learning accuracy (Acc) and computation time (mins) of the unlearned ImageNet model with \(n\in\{100,200,300\}\) classes removed, where SUN397 and OxfordPets are downstream target datasets on linear probing transfer learning setting. When \(n=0\), transfer learning is performed using the pretrained model on the full ImageNet, serving as a baseline, together with the method in [51] for comparison.

in different learning paradigms, _e.g._, federated learning [29; 60], graph neural networks [61; 62; 63], and adversarial ML [64; 65]. In addition to preventing the leakage of data privacy from the trained models, the concept of MU has also inspired other emergent applications such as adversarial defense against backdoor attacks [6; 50] that we have studied and erasing image concepts of conditional generative models [66; 67].

**Understanding data influence.** The majority of MU studies are motivated by data privacy. Yet, they also closely relate to another line of research on understanding data influence in ML. For example, the influence function approach [32] has been used as an algorithmic backbone of many unlearning methods [6; 10]. From the viewpoint of data influence, MU has been used in the use case of adversarial defense against data poisoning backdoor attacks [50]. Beyond unlearning, evaluation of data influence has also been studied in fair learning [68; 69], transfer learning [51], and dataset pruning [70; 71].

**Model pruning.** The deployment constraints on _e.g._, computation, energy, and memory necessitate the pruning of today's ML models, _i.e._, promoting their weight sparsity. The vast majority of existing works [13; 14; 15; 16; 17; 18; 19] focus on developing model pruning methods that can strike a graceful balance between model's generalization and sparsity. In particular, the existence of LTH (lottery ticket hypothesis) [15] demonstrates the feasibility of co-improving the model's generalization and efficiency (in terms of sparsity) [72; 73; 74; 40; 75]. In addition to generalization, model sparsity achieved by pruning can also be leveraged to improve other performance metrics, such as robustness [20; 21; 22], model explanation [25; 26], and privacy [76; 27; 28; 77].

## 7 Conclusion

In this work, we advance the method of machine unlearning through a novel viewpoint: model sparsification, achieved by weight pruning. We show in both theory and practice that model sparsity plays a foundational and crucial role in closing the gap between exact unlearning and existing approximate unlearning methods. Inspired by that, we propose two new unlearning paradigms, 'prune first, then unlearn' and'sparsity-aware unlearn', which can significantly improve the efficacy of approximate unlearning. We demonstrate the effectiveness of our findings and proposals in extensive experiments across different unlearning setups. Our study also indicates the presence of _model modularity_ traits, such as weight sparsity, that could simplify the process of machine unlearning. This may open up exciting prospects for future research to investigate unlearning patterns within weight or architecture space.

## 8 Acknowledgement

The work of J. Jia, J. Liu, Y. Yao, and S. Liu were supported by the Cisco Research Award and partially supported by the NSF Grant IIS-2207052, and the ARO Award W911NF2310343. Y. Liu was partially supported by NSF Grant IIS-2143895 and IIS-2040800.

## References

* [1] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In _2015 IEEE Symposium on Security and Privacy_, pages 463-480. IEEE, 2015.
* [2] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pages 141-159. IEEE, 2021.
* [3] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. _arXiv preprint arXiv:2209.02299_, 2022.
* [4] Jeffrey Rosen. The right to be forgotten. _Stan. L. Rev. Online_, 64:88, 2011.
* [5] Chris Jay Hoofnagle, Bart van der Sloot, and Frederik Zuiderveen Borgesius. The european union general data protection regulation: what it is and what it means. _Information & Communications Technology Law_, 28(1):65-98, 2019.

* [6] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. _arXiv preprint arXiv:2108.11577_, 2021.
* [7] Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11516-11524, 2021.
* [8] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. _arXiv preprint arXiv:2109.13398_, 2021.
* [9] Alexander Becker and Thomas Liebig. Evaluating machine unlearning via epistemic uncertainty. _arXiv preprint arXiv:2208.10836_, 2022.
* [10] Zachary Izzo, Mary Anne Smart, Kamalika Chaudhuri, and James Zou. Approximate data deletion from machine learning models. In _International Conference on Artificial Intelligence and Statistics_, pages 2008-2016. PMLR, 2021.
* [11] Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. On the necessity of auditable algorithmic definitions for machine unlearning. In _31st USENIX Security Symposium (USENIX Security 22)_, pages 4007-4022, 2022.
* [12] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 9304-9312, 2020.
* [13] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. _arXiv preprint arXiv:1510.00149_, 2015.
* [14] Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael Carbin, and Zhangyang Wang. The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16306-16316, 2021.
* [15] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* [16] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis. In _International Conference on Machine Learning_, pages 3259-3269. PMLR, 2020.
* [17] Xiaolong Ma, Geng Yuan, Xuan Shen, Tianlong Chen, Xuxi Chen, Xiaohan Chen, Ning Liu, Minghai Qin, Sijia Liu, Zhangyang Wang, et al. Sanity checks for lottery tickets: Does your winning ticket really win the jackpot? _Advances in Neural Information Processing Systems_, 34:12749-12760, 2021.
* [18] Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu. Advancing model pruning via bi-level optimization. In _Advances in Neural Information Processing Systems_, 2022.
* [19] Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of neural network pruning? _Proceedings of machine learning and systems_, 2:129-146, 2020.
* [20] Vikash Sehwag, Shiqi Wang, Prateek Mittal, and Suman Jana. Hydra: Pruning adversarially robust neural networks. _Advances in Neural Information Processing Systems_, 33:19655-19666, 2020.
* [21] Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, and Zhangyang Wang. Quarantine: Sparsity can uncover the trojan attack trigger for free. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 598-609, 2022.
* [22] James Diffenderfer, Brian Bartoldson, Shreya Chaganti, Jize Zhang, and Bhavya Kailkhura. A winning hand: Compressing deep networks can improve out-of-distribution robustness. _Advances in Neural Information Processing Systems_, 34:664-676, 2021.

* [23] Samuil Stoychev and Hatice Gunes. The effect of model compression on fairness in facial expression recognition. _arXiv preprint arXiv:2201.01709_, 2022.
* [24] Guangxuan Xu and Qingyuan Hu. Can model compression improve nlp fairness. _arXiv preprint arXiv:2201.08542_, 2022.
* [25] Eric Wong, Shibani Santurkar, and Aleksander Madry. Leveraging sparse linear layers for debuggable deep networks. In _International Conference on Machine Learning_, pages 11205-11216. PMLR, 2021.
* [26] Tianlong Chen, Zhenyu Zhang, Jun Wu, Randy Huang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Can you win everything with a lottery ticket? _Transactions of Machine Learning Research_, 2022.
* [27] Yangsibo Huang, Yushan Su, Sachin Ravi, Zhao Song, Sanjeev Arora, and Kai Li. Privacy-preserving learning via deep net pruning. _arXiv preprint arXiv:2003.01876_, 2020.
* [28] Yijue Wang, Chenghong Wang, Zigeng Wang, Shanglin Zhou, Hang Liu, Jinbo Bi, Caiwen Ding, and Sanguthevar Rajasekaran. Against membership inference attack: Pruning is all you need. _arXiv preprint arXiv:2008.13578_, 2020.
* [29] Junxiao Wang, Song Guo, Xin Xie, and Heng Qi. Federated unlearning via class-discriminative pruning. In _Proceedings of the ACM Web Conference 2022_, pages 622-632, 2022.
* [30] Jingwen Ye, Yifang Fu, Jie Song, Xingyi Yang, Songhua Liu, Xin Jin, Mingli Song, and Xinchao Wang. Learning with recoverable forgetting. In _European Conference on Computer Vision_, pages 87-103. Springer, 2022.
* [31] Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, and Andrea Frome. What do compressed deep neural networks forget? _arXiv preprint arXiv:1911.05248_, 2019.
* [32] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _International conference on machine learning_, pages 1885-1894. PMLR, 2017.
* [33] German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. _Neural Networks_, 113:54-71, 2019.
* [34] R Dennis Cook and Sanford Weisberg. _Residuals and influence in regression_. New York: Chapman and Hall, 1982.
* [35] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. _arXiv preprint arXiv:1911.03030_, 2019.
* [36] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S Yu. Machine unlearning: A survey. _ACM Computing Surveys_, 56(1):1-36, 2023.
* [37] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient second-order approximation for neural network compression. _Advances in Neural Information Processing Systems_, 33:18098-18109, 2020.
* [38] Liwei Song, Reza Shokri, and Prateek Mittal. Privacy risks of securing machine learning models against adversarial examples. In _Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security_, pages 241-257, 2019.
* [39] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In _2018 IEEE 31st computer security foundations symposium (CSF)_, pages 268-282. IEEE, 2018.
* [40] Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. _Advances in Neural Information Processing Systems_, 33:6377-6389, 2020.
* [41] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, and Michael Carbin. Pruning neural networks at initialization: Why are we missing the mark? _arXiv preprint arXiv:2009.08576_, 2020.

* Bach et al. [2012] Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Optimization with sparsity-inducing penalties. _Foundations and Trends(r) in Machine Learning_, 4(1):1-106, 2012.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Simonyan and Zisserman [2014] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* Song and Mittal [2020] Liwei Song and Prateek Mittal. Systematic evaluation of privacy risks of machine learning models. _arXiv preprint arXiv:2003.10595_, 2020.
* Gu et al. [2017] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. _arXiv preprint arXiv:1708.06733_, 2017.
* Liu et al. [2022] Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. Backdoor defense with machine unlearning. _arXiv preprint arXiv:2201.09538_, 2022.
* Jain et al. [2022] Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, and Aleksander Madry. A data-based perspective on transfer learning. _arXiv preprint arXiv:2207.05739_, 2022.
* Xiao et al. [2010] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* Parkhi et al. [2012] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _2012 IEEE conference on computer vision and pattern recognition_, pages 3498-3505. IEEE, 2012.
* Leclerc et al. [2022] Guillaume Leclerc, Andrew Ilyas, Logan Engstrom, Sung Min Park, Hadi Salman, and Aleksander Madry. Ffcv: Accelerating training by removing data bottlenecks. 2022.
* Ginart et al. [2019] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. Making ai forget you: Data deletion in machine learning. _Advances in neural information processing systems_, 32, 2019.
* Neel et al. [2021] Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In _Algorithmic Learning Theory_, pages 931-962. PMLR, 2021.
* Ullah et al. [2021] Enayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman Arora. Machine unlearning via algorithmic stability. In _Conference on Learning Theory_, pages 4126-4142. PMLR, 2021.
* Sekhari et al. [2021] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. _Advances in Neural Information Processing Systems_, 34:18075-18086, 2021.
* Dwork et al. [2006] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In _Annual international conference on the theory and applications of cryptographic techniques_, pages 486-503. Springer, 2006.
* Liu et al. [2022] Yi Liu, Lei Xu, Xingliang Yuan, Cong Wang, and Bo Li. The right to be forgotten in federated learning: An efficient realization with rapid retraining. _arXiv preprint arXiv:2203.07320_, 2022.

* [61] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and Yang Zhang. Graph unlearning. In _Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security_, pages 499-513, 2022.
* [62] Eli Chien, Chao Pan, and Olgica Milenkovic. Certified graph unlearning. _arXiv preprint arXiv:2206.09140_, 2022.
* [63] Jiali Cheng, George Dasoulas, Huan He, Chirag Agarwal, and Marinka Zitnik. Gnndelete: A general strategy for unlearning in graph neural networks. _arXiv preprint arXiv:2302.13406_, 2023.
* [64] Neil G Marchant, Benjamin IP Rubinstein, and Scott Alfeld. Hard to forget: Poisoning attacks on certified machine unlearning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, pages 7691-7700, 2022.
* [65] Jimmy Z Di, Jack Douglas, Jayadev Acharya, Gautam Kamath, and Ayush Sekhari. Hidden poison: Machine unlearning enables camouflaged poisoning attacks. In _NeurIPS ML Safety Workshop_, 2022.
* [66] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. _arXiv preprint arXiv:2303.07345_, 2023.
* [67] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models. _arXiv preprint arXiv:2303.17591_, 2023.
* [68] Prasanna Sattigeri, Soumya Ghosh, Inkit Padhi, Pierre Dognin, and Kush R. Varshney. Fair infinitesimal jackknife: Mitigating the influence of biased training data points without refitting. In _Advances in Neural Information Processing Systems_, 2022.
* [69] Jialu Wang, Xin Eric Wang, and Yang Liu. Understanding instance-level impact of fairness constraints. In _International Conference on Machine Learning_, pages 23114-23130. PMLR, 2022.
* [70] Zalan Boros, Mojmir Mutny, and Andreas Krause. Coresets via bilevel optimization for continual learning and streaming. _Advances in Neural Information Processing Systems_, 33:14879-14890, 2020.
* [71] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. Dataset pruning: Reducing training data by examining generalization influence. _arXiv preprint arXiv:2205.09329_, 2022.
* [72] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of network pruning. _arXiv preprint arXiv:1810.05270_, 2018.
* [73] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. _arXiv preprint arXiv:2002.07376_, 2020.
* [74] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning based on connection sensitivity. _arXiv preprint arXiv:1810.02340_, 2018.
* [75] Yimeng Zhang, Akshay Karkal Kamath, Qiucheng Wu, Zhiwen Fan, Wuyang Chen, Zhangyang Wang, Shiyu Chang, Sijia Liu, and Cong Hao. Data-model-circuit tri-design for ultra-light video intelligence on edge devices. In _Proceedings of the 28th Asia and South Pacific Design Automation Conference_, pages 745-750, 2023.
* [76] Zelun Luo, Daniel J Wu, Ehsan Adeli, and Li Fei-Fei. Scalable differential privacy with sparse network finetuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5059-5068, 2021.
* [77] Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et al. A privacy-preserving-oriented dnn pruning and mobile acceleration framework. In _Proceedings of the 2020 on Great Lakes Symposium on VLSI_, pages 119-124, 2020.
* [78] Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo. On differentiating parameterized argmin and argmax problems with application to bi-level optimization. _arXiv preprint arXiv:1607.05447_, 2016.

## Appendix A Proof of Proposition 1

Recap the definition of model update \(\Delta(\mathbf{w})\) in (1) and \(\bm{\theta}_{\mathrm{o}}=\bm{\theta}(\mathbf{1}/N)\), we approximate \(\Delta(\mathbf{w})\) by the first-order Taylor expansion of \(\bm{\theta}(\mathbf{w})\) at \(\mathbf{w}=\mathbf{1}/N\). This leads to

\[\Delta(\mathbf{w})=\bm{\theta}(\mathbf{w})-\bm{\theta}(\mathbf{1}/N)\approx \left.\frac{d\bm{\theta}(\mathbf{w})}{d\mathbf{w}}\right|_{\mathbf{w}=\mathbf{ 1}/N}(\mathbf{w}-\mathbf{1}/N),\] (10)

where \(\frac{d\bm{\theta}(\mathbf{w})}{d\mathbf{w}}\in\mathbb{R}^{M\times N}\), and recall that \(M=|\bm{\theta}_{\mathrm{o}}|\) is the number of model parameters. The gradient \(\frac{d\bm{\theta}(\mathbf{w})}{d\mathbf{w}}\) is known as implicit gradient [78] since it is defined through the solution of the optimization problem \(\bm{\theta}(\mathbf{w})=\arg\min_{\bm{\theta}}L(\mathbf{w},\bm{\theta})\), where recall that \(L(\mathbf{w},\bm{\theta})=\sum_{i=1}^{N}[w_{i}\ell_{i}(\bm{\theta},\mathbf{z} _{i})]\). By the stationary condition of \(\bm{\theta}(\mathbf{w})\), we obtain

\[\nabla_{\bm{\theta}}L(\mathbf{w},\bm{\theta}(\mathbf{w}))=\bm{0}.\] (11)

Next, we take the derivative of (11) w.r.t. \(\mathbf{w}\) based on the implicit function theorem [78] assuming that \(\bm{\theta}(\mathbf{w})\) is the unique solution to minimizing \(L\). This leads to

\[\left[\frac{d\bm{\theta}(\mathbf{w})}{d\mathbf{w}}\right]^{T}\left[\nabla_{\bm {\theta},\bm{\theta}}L(\mathbf{w},\bm{\theta})|_{\bm{\theta}=\bm{\theta}( \mathbf{w})}\right]+\nabla_{\mathbf{w},\bm{\theta}}L(\mathbf{w},\bm{\theta}( \mathbf{w}))=\bm{0},\] (12)

where \(\nabla_{\mathbf{a},\mathbf{b}}=\nabla_{\mathbf{a}}\nabla_{\mathbf{b}}\in \mathbb{R}^{|\mathbf{a}|\times|\mathbf{b}|}\) is the second-order partial derivative. Therefore,

\[\frac{d\bm{\theta}(\mathbf{w})}{d\mathbf{w}}=-\left[\nabla_{\bm{\theta},\bm{ \theta}}L(\mathbf{w},\bm{\theta}(\mathbf{w}))\right]^{-1}\nabla_{\mathbf{w}, \bm{\theta}}L(\mathbf{w},\bm{\theta}(\mathbf{w}))^{T},\] (13)

where \(\nabla_{\mathbf{w},\bm{\theta}}L(\mathbf{w},\bm{\theta}(\mathbf{w}))\) can be expanded as

\[\nabla_{\mathbf{w},\bm{\theta}}L(\mathbf{w},\bm{\theta}(\mathbf{ w})) =\nabla_{\mathbf{w}}\nabla_{\bm{\theta}}\sum_{i=1}^{N}\left[w_{i} \ell_{i}(\bm{\theta}(\mathbf{w}),\mathbf{z}_{i})\right]\] (14) \[=\nabla_{\mathbf{w}}\sum_{i=1}^{N}\left[w_{i}\nabla_{\bm{\theta}} \ell_{i}(\bm{\theta}(\mathbf{w}),\mathbf{z}_{i})\right]\] (15) \[=\left[\begin{array}{c}\nabla_{\bm{\theta}}\ell_{1}(\bm{\theta} (\mathbf{w}),\mathbf{z}_{1})^{T}\\ \nabla_{\bm{\theta}}\ell_{2}(\bm{\theta}(\mathbf{w}),\mathbf{z}_{2})^{T}\\ \vdots\\ \nabla_{\bm{\theta}}\ell_{N}(\bm{\theta}(\mathbf{w}),\mathbf{z}_{N})^{T}\end{array} \right].\] (16)

Based on (13) and (16), we obtain the closed-form of implicit gradient at \(\mathbf{w}=\mathbf{1}/N\):

\[\frac{d\bm{\theta}(\mathbf{w})}{d\mathbf{w}}\mid_{\mathbf{w}= \mathbf{1}/N} =-\left[\nabla_{\bm{\theta},\bm{\theta}}L(\mathbf{1}/N,\bm{\theta}( \mathbf{1}/N))\right]^{-1}\left[\nabla_{\bm{\theta}}\ell_{1}(\bm{\theta}( \mathbf{1}/N),\mathbf{z}_{1})\quad\dots\quad\nabla_{\bm{\theta}}\ell_{N}(\bm {\theta}(\mathbf{1}/N),\mathbf{z}_{N})\right]\] \[=-\mathbf{H}^{-1}\left[\nabla_{\bm{\theta}}\ell_{1}(\bm{\theta} (\mathbf{1}/N),\mathbf{z}_{1})\quad\dots\quad\nabla_{\bm{\theta}}\ell_{N}(\bm {\theta}(\mathbf{1}/N),\mathbf{z}_{N})\right],\] (17)

where \(\mathbf{H}=\nabla_{\bm{\theta},\bm{\theta}}L(\mathbf{1}/N,\bm{\theta}( \mathbf{1}/N))\).

Substituting (17) into (10), we obtain

\[\Delta(\mathbf{w}) \approx-\mathbf{H}^{-1}\left[\nabla_{\bm{\theta}}\ell_{1}(\bm{ \theta}(\mathbf{1}/N),\mathbf{z}_{1})\quad\dots\quad\nabla_{\bm{\theta}}\ell_{N}( \bm{\theta}(\mathbf{1}/N),\mathbf{z}_{N})\right](\mathbf{w}-\mathbf{1}/N)\] \[=-\mathbf{H}^{-1}\sum_{i=1}^{N}[(w_{i}-1/N)\nabla_{\bm{\theta}} \ell_{i}(\bm{\theta}(\mathbf{1}/N),\mathbf{z}_{i})]\] \[=\mathbf{H}^{-1}\nabla_{\bm{\theta}}L(\mathbf{1}/N-\mathbf{w},\bm {\theta}_{\mathrm{o}}),\] (18)

where the last equality holds by the definition of \(L(\mathbf{w},\bm{\theta})=\sum_{i=1}^{N}[w_{i}\ell_{i}(\bm{\theta},\mathbf{z}_{i})]\).

The proof is now complete.

[MISSING_PAGE_FAIL:16]

updated by SGD yields

\[\bm{\theta}_{t}^{\prime}\approx\bm{\theta}_{0}^{\prime}-\eta\mathbf{m}\odot\sum_{ i=1}^{t-1}\nabla_{\bm{\theta}}\ell(\bm{\theta}_{0}^{\prime},\hat{\mathbf{z}}_{i})+ \mathbf{m}\odot(\sum_{i=1}^{t-1}f(i)),\] (10)

where \(\bm{\theta}_{0}^{\prime}=\mathbf{m}\odot\bm{\theta}_{0}\) is the model initialization when using SGD-based sparse training, \(\{\hat{\mathbf{z}}_{i}\}\) is the sequence of stochastic data samples, \(t\) is the number of training iterations, \(\eta\) is the learning rate, and \(f(i)\) is defined recursively as

\[f(i)=-\eta\nabla_{\bm{\theta},\bm{\theta}}^{2}\ell(\bm{\theta}_{0}^{\prime}, \hat{\mathbf{z}}_{i})\left(-\eta\sum_{j=0}^{i-1}\mathbf{m}\odot\nabla_{\bm{ \theta}}\ell(\bm{\theta}_{0}^{\prime},\hat{\mathbf{z}}_{j})+\sum_{j=0}^{i-1} \left(\mathbf{m}\odot f(j)\right)\right),\] (11)

with \(f(0)=0\). Inspired by the second term of (10), to unlearn the data sample \(\hat{\mathbf{z}}_{i}\), we will have to add back the first-order gradients under \(\hat{\mathbf{z}}_{i}\). This corresponds to the GA-based approximate unlearning method. Yet, this approximate unlearning introduces an unlearning error, given by the last term of (10)

\[\mathbf{e}_{\mathbf{m}}(\bm{\theta}_{0},\{\hat{\mathbf{z}}_{i}\},t,\eta):= \mathbf{m}\odot(\sum_{i=1}^{t-1}f(i)).\] (12)

Next, if we interpret the mask \(\mathbf{m}\) as a diagonal matrix \(\mathrm{diag}(\mathbf{m})\) with \(0\)'s and \(1\)'s along its diagonal based on \(\mathbf{m}\), we can then express the sparse model \(\mathbf{m}\odot\bm{\theta}\) as \(\mathrm{diag}(\mathbf{m})\bm{\theta}\). Similar to [8, Eq. 9], we can derive a bound on the unlearning error (12) by ignoring the terms other than those with \(\eta^{2}\) in \(f(i)\), _i.e._, (11). This is because, in the recursive form of \(f(i)\), all other terms exhibit a higher degree of the learning rate \(\eta\) compared to \(\eta^{2}\). As a result, we obtain

\[e(\mathbf{m})=\left\|\mathbf{e}_{\mathbf{m}}(\bm{\theta}_{0}, \{\hat{\mathbf{z}}_{i}\},t,\eta)\right\|_{2}=\left\|\mathbf{m}\odot(\sum_{i=1 }^{t-1}f(i))\right\|_{2}\] \[\approx\eta^{2}\left\|\mathrm{diag}(\mathbf{m})\sum_{i=1}^{t-1} \nabla_{\bm{\theta},\theta}^{2}\ell(\bm{\theta}_{0}^{\prime},\hat{\mathbf{z} }_{i})\sum_{j=0}^{i-1}\mathbf{m}\odot\nabla_{\bm{\theta}}\ell(\bm{\theta}_{0}^ {\prime},\hat{\mathbf{z}}_{j})\right\|_{2}\] \[\leq\eta^{2}\sum_{i=1}^{t-1}\left\|\mathrm{diag}(\mathbf{m}) \nabla_{\bm{\theta},\theta}^{2}\ell(\bm{\theta}_{0}^{\prime},\hat{\mathbf{z} }_{i})\sum_{j=0}^{i-1}\mathbf{m}\odot\nabla_{\bm{\theta}}\ell(\bm{\theta}_{0}^ {\prime},\hat{\mathbf{z}}_{j})\right\|_{2}\] (Triangle inequality) \[\leq\eta^{2}\sum_{i=1}^{t-1}\left\|\mathrm{diag}(\mathbf{m}) \nabla_{\bm{\theta},\theta}^{2}\ell(\bm{\theta}_{0}^{\prime},\hat{\mathbf{z} }_{i})\right\|\left\|\sum_{j=0}^{i-1}\mathbf{m}\odot\nabla_{\bm{\theta}}\ell( \bm{\theta}_{0}^{\prime},\hat{\mathbf{z}}_{j})\right\|_{2}\] (13) \[\lesssim\eta^{2}\sum_{i=1}^{t-1}\left\|\mathrm{diag}(\mathbf{m}) \nabla_{\bm{\theta},\theta}^{2}\ell(\bm{\theta}_{0}^{\prime},\hat{\mathbf{z} }_{i})\right\|\left\|\theta_{t}^{\prime}-\bm{\theta}_{0}^{\prime}\right\|_{2}\] (14) \[\leq\eta^{2}\sigma(\mathbf{m})\left\|\mathbf{m}\odot(\bm{\theta} _{t}-\bm{\theta}_{0})\right\|_{2}\frac{1}{t}\frac{t-1}{2}t=\frac{\eta^{2}}{2}( t-1)\|\mathbf{m}\odot(\bm{\theta}_{t}-\bm{\theta}_{0})\|_{2}\sigma(\mathbf{m}),\] (15)

where the inequality (14) holds given the fact that \(\sum_{j=0}^{i-1}\mathbf{m}\odot\nabla_{\bm{\theta}}\ell(\bm{\theta}_{0}^{ \prime},\hat{\mathbf{z}}_{j})\) in (13) can be approximated by its expectation \(\frac{i(\bm{\theta}_{t}^{\prime}-\bm{\theta}_{0}^{\prime})}{t}\)[8, Eq. 7], and \(\sigma(\mathbf{m}):=\max_{j}\{\sigma_{j}(\nabla_{\bm{\theta},\theta}^{2}\ell), \mathrm{if}\,m_{j}\neq 0\}\), _i.e._, the largest eigenvalue among the dimensions left intact by the binary mask \(\mathbf{m}\). The above suggests that the unlearning error might be large if \(\mathbf{m}=\mathbf{1}\) (no pruning). Based on (15), we can then readily obtain the big \(O\) notation in (2). This completes the proof.

## Appendix C Additional Experimental Details and Results

### Datasets and models

We summarize the datasets and model configurations in Tab. A1.

### Additional training and unlearning settings

**Training configuration of pruning.** For all pruning methods, including IMP [15], SynFlow [40], and OMP [17], we adopt the settings from the current SOTA implementations [17]; see a summary in Tab. A2. For IMP, OMP, and SynFlow, we adopt the step learning rate scheduler with a decay rate of 0.1 at \(50\%\) and \(75\%\) epochs. We adopt \(0.1\) as the initial learning rate for all pruning methods.

**Additional training details of MU.** For all datasets and model architectures, we adopt \(10\) epochs for FT, and \(5\) epochs for GA method. The learning rate for FT and GA are carefully tuned between \([10^{-5},0.1]\) for each dataset and model architecture. In particular, we adopt \(0.01\) as the learning rate for FT method and \(10^{-4}\) for GA on the CIFAR-10 dataset (ResNet-18, class-wise forgetting) at different sparsity levels. By default, we choose SGD as the optimizer for the FT and GA methods. As for FF method, we perform a greedy search for hyperparameter tuning [12] between \(10^{-9}\) and \(10^{-6}\).

### Detailed metric settings

**Details of MIA implementation.** MIA is implemented using the prediction confidence-based attack method [48]. There are mainly two phases during its computation: **(1) training phase**, and **(2) testing phase**. To train an MIA model, we first sample a balanced dataset from the remaining dataset (\(\mathcal{D}_{\mathrm{r}}\)) and the test dataset (different from the forgetting dataset \(\mathcal{D}_{\mathrm{f}}\)) to train the MIA predictor. The learned MIA is then used for MU evaluation in its testing phase. To evaluate the performance of MU, MIA-Efficacy is obtained by applying the learned MIA predictor to the unlearned model (\(\bm{\theta}_{\mathrm{u}}\)) on the forgetting dataset (\(\mathcal{D}_{\mathrm{f}}\)). Our objective is to find out how many samples in \(\mathcal{D}_{\mathrm{f}}\) can be correctly predicted as non-training samples by the MIA model against \(\bm{\theta}_{\mathrm{u}}\). The formal definition of MIA-Efficacy is then given by:

\[\text{MIA-Efficacy}=\frac{TN}{|\mathcal{D}_{\mathrm{f}}|},\] (10)

where \(TN\) refers to the true negatives predicted by our MIA predictor, _i.e._, the number of the forgetting samples predicted as non-training examples, and \(|\mathcal{D}_{\mathrm{f}}|\) refers to the size of the forgetting dataset. As described above, MIA-Efficacy leverages the privacy attack to justify how good the unlearning performance could be.

### Additional experiment results

**Model sparsity benefits privacy of MU for 'free'.** It was recently shown in [27; 28] that model sparsification helps protect data privacy, in terms of defense against MIA used to infer training data information from a learned model. Inspired by the above, we ask if sparsity can also bring the privacy benefit to an unlearned model, evaluated by the MIA rate on the remaining dataset \(\mathcal{D}_{\mathrm{r}}\) (that we term **MIA-Privacy**). This is different from MIA-Efficacy, which reflects the efficacy of scrubbing \(\mathcal{D}_{\mathrm{f}}\), _i.e._, correctly predicting that data sample in \(\mathcal{D}_{\mathrm{f}}\) is not in the training set of the unlearned model. In contrast, MIA-Privacy characterizes the _privacy_ of the unlearned model about \(\mathcal{D}_{\mathrm{r}}\). A _lower_ MIA-Privacy implies _less_ information leakage.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline Experiments & CIFAR-10CIFAR-100 & SVHN & ImageNet \\ \hline Training epochs & 182 & 160 & 90 \\ \hline Rewinding epochs & 8 & 8 & 5 \\ \hline Momentum & 0.9 & 0.9 & 0.875 \\ \hline \(\ell_{2}\) regularization & \(5e^{-4}\) & \(5e^{-4}\) & \(3.05e^{-5}\) \\ \hline Warm-up epochs & 1(75 for VGG-16) & 0 & 8 \\ \hline \hline \end{tabular}
\end{table}
Table A2: Detailed training details for model pruning.

**Fig. A1** shows MIA-Privacy of unlearned models versus the sparsity ratio applied to different unlearning methods in the 'prune first, then unlearn' paradigm. As we can see, MIA-Privacy decreases as the sparsity increases. This suggests the improved privacy of unlearning on sparse models. Moreover, we observe that approximate unlearning outperforms exact unlearning (Retrain) in privacy preservation of \(\mathcal{D}_{\mathrm{r}}\). This is because Retrain is conducted over \(\mathcal{D}_{\mathrm{r}}\) from scratch, leading to the strongest dependence on \(\mathcal{D}_{\mathrm{r}}\) than other unlearning methods. Another interesting observation is that IU and GA yield a much smaller MIA-Privacy than other approximate unlearning methods. The rationale behind that is IU and GA have a weaker correlation with \(\mathcal{D}_{\mathrm{r}}\) during unlearning. Specifically, the unlearning loss of IU only involves the forgetting data influence weights, _i.e._, \((\mathbf{1}/N-\mathbf{w})\) in (1). Similarly, GA only performs gradient ascent over \(\mathcal{D}_{\mathrm{f}}\), with the least dependence on \(\mathcal{D}_{\mathrm{r}}\).

**Performance of 'prune first, then unlearn' on various datasets and architectures.** As demonstrated in Tab. A3 and Tab. A4, the introduction of model sparsity can effectively reduce the discrepancy between approximate and exact unlearning across a diverse range of datasets and architectures. This phenomenon is observable in various unlearning scenarios. Remarkably, model sparsity enhances both UA and MIA-Efficacy metrics without incurring substantial degradation on RA and TA in different unlearning scenarios. These observations corroborate the findings reported in Tab. 3.

To demonstrate the effectiveness of our methods on a larger dataset, we conducted additional experiments on **ImageNet**[46] with settings consistent with the class-wise forgetting in Tab. 3. As we can see from Tab. A5, sparsity reduces the performance gap between exact unlearning (Retrain) and the approximate unlearning methods (FT and GA). The results are consistent with our observations in other datasets. Note that the \(83\%\) model sparsity (ImageNet, ResNet-18) is used to preserve the TA performance after one-shot magnitude (OMP) pruning.

**Performance of \(\ell_{1}\) sparsity-aware MU on additional datasets.** As seen in Fig. A2, \(\ell_{1}\)-sparse MU significantly reduces the gap between approximate and exact unlearning methods across various

[MISSING_PAGE_FAIL:20]

**Performance of \(\ell_{1}\) sparsity-aware MU on additional architectures.** Tab. A7 presents an additional application to Swin Transformer on CIFAR-10. To facilitate a comparison between approximate unlearning methods (including the FT baseline and the proposed \(\ell_{1}\)-sparse MU) and Retrain, we train the transformer from scratch on CIFAR-10. This could potentially decrease testing accuracy compared with fine-tuning on a pre-trained model over a larger, pre-trained dataset. As we can see, our proposed \(\ell_{1}\)-sparse MU leads to a much smaller performance gap with Retrain compared to FT. In particular, class-wise forgetting exhibited a remarkable \(90.24\%\) increase in UA, accompanied by a slight reduction in RA.

**Performance of 'prune first, then unlearn' and \(\ell_{1}\) sparsity-aware MU on different model sizes.** Further, Tab. A8 and Tab. A9 present the unlearning performance versus different model sizes in the ResNet family, involving both ResNet-20s and ResNet-50 on CIFAR-10, in addition to ResNet-18 in Tab. 3. As we can see, sparsity consistently diminishes the unlearning gap with Retrain (indicated by highlighted numbers, with smaller values being preferable). It's worth noting that while both ResNet-20s and ResNet-50 benefit from sparsity, the suggested sparsity ratio is 90% for ResNet-20s and slightly lower than 95% for ResNet-50 when striking the balance between MU and generalization.

## Appendix D Broader Impacts and Limitations

**Broader impacts.** Our study on model sparsity-inspired MU provides a versatile solution to forget arbitrary data points and could give a general solution for dealing with different concerns, such as the model's privacy, efficiency, and robustness. Moreover, the applicability of our method extends beyond these aspects, with potential impacts in the following areas. 1_Regulatory compliance:_ Our method enables industries, such as healthcare and finance, to adhere to regulations that require the forgetting of data after a specified period. This capability ensures compliance while preserving the utility and performance of machine learning models. 2_Fairness:_ Our method could also play a crucial role in addressing fairness concerns by facilitating the unlearning of biased datasets or subsets. By removing biased information from the training data, our method contributes to mitigating bias in machinelearning models, ultimately fostering the development of fairer models. _3 ML with adaptation and sustainability:_ Our method could promote the dynamic adaptation of machine learning models by enabling the unlearning of outdated information, and thus, could enhance the accuracy and relevance of the models to the evolving trends and dynamics of the target domain. This capability fosters sustainability by ensuring that ML models remain up-to-date and adaptable, thus enabling their continued usefulness and effectiveness over time.

**Limitations.** One potential limitation of our study is the absence of provable guarantees for \(\ell_{1}\)-sparse MU. Since model sparsification is integrated with model training as a soft regularization, the lack of formal proof may raise concerns about the reliability and robustness of the approach. Furthermore, while our proposed unlearning framework is generic, its applications have mainly focused on solving computer vision tasks. As a result, its effectiveness in the domain of natural language processing (NLP) remains unverified. This consideration becomes particularly relevant when considering large language models. Therefore, further investigation is necessary for future studies to explore the applicability and performance of the framework in NLP tasks.