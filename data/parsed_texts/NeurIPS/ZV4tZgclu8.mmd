Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset

 Saeid Alavi Naeini\({}^{1,3,4}\) Raedi Saqur\({}^{2,4}\) Mozhgan Saeidi\({}^{2,4,6}\) John Giorgi\({}^{2,4,5}\) Babak Taati\({}^{1,2,3,4}\)

\({}^{1}\)Kite Research Institute, Toronto Rehabilitation Institute, University Health Network

\({}^{2}\)Department of Computer Science, University of Toronto

\({}^{3}\)Institute of Biomedical Engineering, University of Toronto \({}^{4}\)Vector Institute for AI

\({}^{5}\)Donnelly Centre for Cellular & Biomolecular Research, University of Toronto

\({}^{6}\)Department of Biomedical Data Science, Stanford University

{saeid.alavi, john.giorgi}@mail.utoronto.ca

raeidsaquur@cs.toronto.edu, mozhgans@stanford.edu, babak.taati@uhn.ca

###### Abstract

The quest for human imitative AI has been an enduring topic in AI research since its inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to the cultural zeifeist. While recent NLP evaluation benchmark tasks test some aspects of human-imitative behavior (e.g., BIG-bench's 'human-like behavior' tasks), few, if not none, examine _creative problem solving_ abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use the ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli -- distractors dubbed _red herrings_ -- impede human performance in such tasks via the _fixation effect_ and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect's _Connecting Wall_ segment essentially mimics Mednick's Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, which makes it an ideal proxy task to explore and study the fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In this paper, we present the novel Only Connect Wall (OCW) dataset and report results from our evaluation of selected pre-trained language models and LLMs on creative problem solving tasks like grouping clue words by heterogeneous connections and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models. The code and link to the dataset are available at https://github.com/TaatiTeam/OCW.

## 1 Introduction

The remarkable capabilities of state-of-the-art large language models (LLMs) [91], across a variety of domains and downstream tasks [78, 10], have spurred their comparisons with artificial general intelligence (AGI) [5, 14] and human-imitative AI [31] systems. The extraordinary leap in capabilities of these LLMs over a short span -- from the advent of transformer-based [69] pre-trained, context-aware language models (PLMs) [52, 17, 40, 36, 53] circa 2018 to 2020, to the current and latest cohort of increasingly larger (billions of parameters) LMs [59; 77; 57; 89; 16; 67; 18] spearheaded by the OpenAI's GPT series [13], notably ChatGPT [49] and GPT-4 [48] -- justifiably warrants such comparisons. Several natural language processing (NLP) benchmarks have been proposed to standardize the evaluation of these LLMs, including MMLU [27], BIG-bench [66], HELM [38], and Global-Bench [65]. The tasks inventory under these benchmarks are open (type of tasks) and dynamic (rolling additions). While a subset of these tasks aims to test for human initiative intelligence (e.g., nineteen tasks listed under the _human-like behaviour_ category in BIG-bench), none tests for _creative problem solving_ abilities [44] -- a hallmark of human-like intelligence [31].

Creative problem-solving by humans is a well-studied topic in cognitive neuroscience and human behavioural sciences literature. These studies and methods use (word) associative fluency to model and test creativity objectively [44; 9]. Empirical research in this context commonly employs single or continuous word association tests that are variants of Mednick's seminal Remote Associates Test (RAT) [45]. Such tests entail finding connections or links among a presented group of words using associations that can be heterogeneous (e.g., synonymy, semantic, compounding) [86; 43]. To exemplify, consider the cue words: _[Tennis, Same, Head]_. A correct connection in this triplet is: _Match_, which connects by semantic link (_tennis match_), synonymy (_same match_), and compounding (_match head_). Further, the word connections can also vary in degrees of figurativeness (e.g., _Star-Actress_ vs. _Star-Planet_) and abstractness (e.g., _Humor-Sense_ vs. _Apple-Tree_). In humans, such creative problem-solving abilities are impeded by exposure to wrong answers [61; 62; 85] -- a finding referred to as the _fixation effect_[34; 82]. A closely related similar concept is the _Einstellung effect_[42], which postulates the negative effect of previous experience when solving new problems.

Studies examining the fixation effect induce fixations by presenting clue words intended as wrong answers (misleading stimuli) [61] dubbed "red herrings" or, by pre-exposing participants to red herrings before attempting creative problem-solving tasks like the RAT [45]. A slew of works in negative transfer learning in human cognition attempt to explain the RAT fixation phenomenon that involves pre-exposure to red herrings by the negative effects of prior learning on indirect or implicit measures of memory [63]. This negative transfer effect was demonstrated and studied using orthographically similar words to subsequent test word fragments as red herrings [63]. Intuitively, the red herrings lead participants away from the memory retrieval (or down incorrect neurological pathways by Hebbian terminology) required for correct responses and fixate on wrong connections [60]. Fixation in creative problem-solving can be increased by making red herrings more retrievable. Thus, creative problem-solving can be thought of as a type of indirect memory measure whose retrieval is degraded by red herrings due to the negative transfer effect. The _red herring retrieval hypothesis_ states that factors that make red herrings more retrievable should reduce creative problem-solving performance,

Figure 1: Examples of _Only Connect_ walls with ground-truth groupings (rows) and connections (last column). _Red herrings_ include orthographically identical words, e.g., **Gala**, Churchill and Chelsea in different connected groups — **Gala**: _Gala night, Apples_, _Swimming gala_, Churchill: _Advert Animals, English Playwrights_ and Chelsea: _Gay Villages, Boots_ — across walls. In Wall **A** (top left), the clues Churchill, Marx, Castro provide misleading stimuli inducing plausible fixation on historical figures within the wall.

as measured with RAT problems. Two such factors are repetition and context. A following corollary states that the memory strengths of red herrings determine the magnitude of a fixation effect [8].

In this work, we study the juxtaposition of these theories from human cognitive neuroscience (_fixations, negative transfer learning, red herring memory retrieval hypothesis_) from the context of LLMs and natural language processing. While negative transfer learning has been observed and studied in AI research [76, 22], the context of these studies is limited to strict machine learning sub-domains like statistical distribution measures and computer vision. There has not been any work that systematically examines these specific concepts' relation in AI research. Our major contributions are as follow:

1. Only Connect Wall (OCW) Dataset and creative problem solving tasks.We introduce a novel dataset for evaluating _creative problem solving_ tasks by curating the problems and human performance results from the popular British quiz show Only Connect [81, 3]. Specifically, the _Connecting Walls_ segment of the show, where the tasks entail grouping sixteen (16) jumbled up clue words into four (4) connected groups, and naming the correct connections (Figure 1). The presented words have heterogeneous connections with open-domain knowledge retrieval, e.g., history, places, famous people, tools, and cultural references. These 'walls' contain red herrings or misleading stimuli by design, which makes this dataset an analogical proxy for RAT tests in evaluating LLMs for creative problem-solving. Section SS2 provides a detailed description of the dataset.

2. Experiments, results, and key findings of baseline LLMs evaluation.We evaluate a suite of NLP models from static embeddings to PLMs to LLMs and demonstrate that none can solve the tasks of the OCW dataset. Our findings show that SOTA LLMs (e.g. GPT-4 [48]) perform significantly worse than the expert human baseline, and somewhat surprisingly, that increasing the number of in-context examples in few-shot in-context-learning is ineffective. Sections SS3 and SS4 provide details.

## 2 Only Connect Walls Dataset

Here we focus on the _Connecting Walls_ segment (usually the third round) of the quiz-show. Each wall contains sixteen jumbled-up word clues that must be sorted into four groups, each with four connected words. Once the groups are formed, contestants must also identify the right connection or relationship among the items in each group. While there is only one correct solution to each wall, the puzzles are designed to include several red herring clues that can fit into another category and red herring categories fitting multiple clues. Figure 1 shows solved sample walls from the show highlighting a couple of typical red herrings.

### Dataset Collection and Structure

The OCW dataset contains 618 connecting wall puzzles and solutions in total from 15 seasons of the show. Each show episode has two walls. The total number of walls per season varies based on the (varying) number of aired season episodes. The walls were scraped from fan websites1, and human performance results (for grouping and connection tasks) were manually curated by watching all the episodes. Figure 2 depicts the high-level structure of the dataset in JSON format with self-explanatory object keys and comments.

Footnote 1: The primary source was the Only Connect fan website: https://ocdb.cc [6].

### Tasks and Evaluation Metrics

The two dataset tasks: **Task 1 (Grouping)**, and **Task 2 (Connections)** are identical to the quiz-show's human participant tasks. We evaluate Task 1 (Groupings) via six metrics: number of solved walls, number of correct groups (max. four per wall), Adjusted Mutual Information (AMI) [71], Adjusted Rand Index (ARI) [28], Fowlkes Mallows Score (FMS) [21], and Wasserstein Distance (WD) [54], normalized to \((0,1)\) range, between predicted and ground-truth labels [88, 70].

We similarly evaluate Task 2 (Connections) with three metrics: exact string matching, ROUGE-1 F1 [39], and BERTScore F1 [90]. Exact match is the most strict, assigning a score of 1 when the predicted connection is identical to the ground-truth and 0 otherwise. ROUGE-1 F1 relaxes this criterion; it is large when there is a high proportion of ground-truth tokens in the model's predictedconnection _and_ a low proportion of non-ground truth tokens. BERTScore F1 is similar but further relaxes this criterion, assigning a non-zero score for _semantically_ similar (but non-identical) predicted tokens. Together these three metrics provide a more holistic view of model performance on Task 2 than any one metric alone. Empirically, we find that a ROUGE-1 or BERTScore F1 of \(\geq 0.5\) indicates that a predicted connection would likely be considered _correct_ (Table 1). Note that BERTScore has many parameters affecting the final score; a hashcode is produced and reported for reproducibility.

Each of the evaluation metrics for Task 1 of Task 2 could be calculated per wall, per episode, per season, or for the entire test set. We present results on the entire test set in this paper (SS4). We split the dataset into a train set (62 walls), validation set (62 walls), and test set (494 walls). The primary goal of our dataset is to evaluate the zero- and few-shot creative problem-solving abilities of LLMs; as such, we elect to set the size of the test set to be much greater than train or validation sets.

## 3 Experiments: Language Model Evaluations

This section describes methods and models used to provide baseline results for the dataset. For Task 1 (Grouping), we use clustering techniques on word-embeddings from classical and pre-trained language models (PLMs) (SS3.1), and few-shot in-context learning (ICL) with LLMs (SS3.2). For Task 2 (Connections), we only provide baseline results using few-shot ICL with LLMs (SS3.2).

Figure 2: JSON Structure of the OCW dataset. One truncated example is shown.

\begin{table}
\begin{tabular}{l l c c c} Predicted Connection & Ground-truth Connection & Exact Match & ROUGE-1 F1 & BERTScore F1 \\ \hline Types of numbers & Types of numbers & 1.00 & 1.00 & 1.00 \\ Slang terms for money & Slang for money & 0.00 & 0.86 & 0.79 \\ Types of trees & Trees & 0.00 & 0.50 & 0.63 \\ Bridges in London & Thames bridges & 0.00 & 0.40 & 0.31 \\ Medieval occupations & Chaucer characters & 0.00 & 0.00 & 0.15 \\ \hline \end{tabular}
\end{table}
Table 1: Examples of predicted and ground-truth connections and their performance according to the chosen metrics. Exact match is 0 for anything but identical strings. Empirically, we observe that a ROUGE-1/BERTScore F1 of \(\geq 0.5\) indicates that a predicted connection is likely _correct_.

### Task 1: Grouping using Word Embeddings

For the _grouping task_ evaluation (SS2.2), we use clustering algorithms on word-embeddings of the sixteen clue words in each wall, to group them into four predicted groups that are subsequently evaluated against the four ground-truth groups for each wall. A vanilla \(k\)-means (with \(k=4\)) clustering algorithm [25] does not guarantee each predicted group to have four words, thus we use variants like constrained clustering.

ClusteringSemi-supervised constrained clustering [72; 7] is used when the user has pre-existing knowledge about the desired partition (in our case, 4 groups). Here, we adopt a _minimum cost flow network_ clustering approach [12] with a cluster size of four for grouping. Our preliminary analysis showed that clustering results exhibited slight variations across runs. This slight discrepancy could be attributed to the initializations of cluster centroids. To address this issue and ensure reliable results, we report the mean and variance of results (Table 3)across sixteen (16) runs, each with a unique seed and randomized order of sixteen-word clues. We tested two additional clustering approaches motivated by [47; 19]: (1) We constructed a self-similarity matrix containing pair-wise similar information about the words prior to applying constrained clustering; (2) We performed dimensionality reduction using Principal Component Analysis (PCA) [58] and t-distributed stochastic neighbor embedding (t-SNE) [68] before applying constrained clustering. Neither approach improved performance over raw embeddings' clusters, and, for brevity, results are not included.

Static word embeddingWe used two well-known classic word embedding models, GloVe [51] and FastText [23], both of which are accessed through the Flair library (Table 2). We used two FastText models, one pre-trained on the Common Crawl corpus and another on Wikipedia. Approximately 10% of the total clues encountered in the dataset were out-of-vocabulary (OOV). A significant portion (~80%) of the OOV cases were addressed by mean pooling for clues comprised of multiple words to obtain one unified embedding. For the remaining OOV instances, we combined the static embeddings with BytePair encoded[26] sub-words.

PLMsWe explored general-purpose PLMs (BERT [17], RoBERTa [40], DistilBERT [56], ELMo [52]) as well as Sentence Transformers (MPNet [64], E5 [75]; see Table 2). We evaluated performance with and without contextual embeddings.2 Depending on the context, some clues in the dataset may appear across different walls with different meanings. As an example, the word

Figure 3: Solved wall (wall_id="8cde") for Task 1 (Grouping) using best performing model (E5BASE) with both static and contextual embeddings. **Left**: solved wall using static embeddings. **Right**: unsolved wall using contextual embeddings. 2D projection of embeddings using t-SNE is shown. Colors and shapes correspond to true clusters, and grey convex regions correspond to predicted clusters. The legend shows the ground truth connection for each group.

"Gala" was found in three distinct walls, each associated with a different meaning: _apples_, _swimming_ --, and _night_ (Figure 1). The contextual embeddings were aimed to capture contextual semantic similarity among the clues (if any). They were generated by joining the 16 clues in the wall as a pseudo-sentence. We randomly shuffle the word order across sixteen different runs for each wall to account for the positional ordering. We note that such faux sentences (for inducing context) are not valid English syntactic sentences. We used mean pooling to generate embeddings for clues comprised of multiple words to capture the collective meaning of the entire clue.

### Task 2: Connections using Few-shot In-context Learning (ICL) with LLMs

Few-shot ICL with LLMs has emerged as a performant and broadly applicable paradigm in NLP [13]. To evaluate the performance of this approach on our proposed dataset, we designed a few-shot prompt for GPT-3.5-turbo and GPT-4 [48], which are amongst the strongest performing LLMs currently available.3 For Task 1 (Grouping, SS2.2), the prompt consists of some natural language instructions, several examples of solved walls from the training set, and the current example's 16 clues, randomly sorted. For Task 2 (Connections), in place of the 16 clues, the prompt contains a solved wall _without_ the connections (Figure 4).

Footnote 3: In preliminary experiments, we found that open-source LLMs like LLaMA [67] perform poorly and typically do not follow the task instructions.

We developed our prompts on the validation set and reported the final performance on the test set. In-context examples are randomly selected from the train set; the same examples are used across all test inputs. We experiment with 0, 1, 3, 5, and 10 in-context examples. When necessary, we apply simple post-processing to the LLMs output. For example, in both Task 1 and Task 2, we take a maximum of 4 predictions for the groups and connections, respectively, and pad up to 4 with the empty string in cases where the model outputs fewer than 4.4 To make results as reproducible as possible, we set the temperature=0 and used the 03/01/2023 GPT-3.5-turbo snapshot and the 03/14/2023 GPT-4 snapshot. The max output length is set to 144 tokens. All other hyperparameters of the OpenAI API are left at their defaults [2]. Prompts were designed as per the Guidance library [1].

Footnote 4: Please see our codebase for all post-processing steps: https://github.com/TaatiTeam/OCW

## 4 Results and Discussions

### Task 1: Grouping Results

Embedding Clustering TechniquesIn Table 3 we report the performance of several static embedding baselines on Task 1 (Grouping). E5BASE was the most performant model and, on average, solved

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Model & \multicolumn{2}{c}{\# Parameters} & Version & Accessed via \\ \hline \multicolumn{5}{c}{_Word Embeddings_} \\ \hline BPEmb [26] & En & – & en & Flair [4] \\ GloVe [51] & 6B & – & glove & Flair \\ FastText [23] & Crawl & – & crawl & Flair \\  & News & – & news & Flair \\ \hline \multicolumn{5}{c}{_Pre-trained Language Models (PLMs)_} \\ \hline ELMo\({}_{\text{LARGE}}\)[52] & \multicolumn{2}{c}{94M} & large & Flair [4] \\ DistilBERT\({}_{\text{BASE}}\)[56] & uncased & 66M & distilbert-base-uncased & HuggingFace [83] \\ BERT\({}_{\text{BASE}}\)[17] & uncased & 110M & bert-base-uncased & HuggingFace \\ BERT\({}_{\text{LARGE}}\)[40] & uncased & 340M & bert-large-uncased & HuggingFace \\ RoBERT\({}_{\text{LARGE}}\)[40] & \multicolumn{2}{c}{355M} & \multicolumn{2}{c}{forexia-large} & HuggingFace \\ \hline \multicolumn{5}{c}{_Sentence Transformers_} \\ \hline all-mpnet\({}_{\text{BASE}}\)[64] & V2 & 110M & sentence-transformers/all-mpnet-base-v2 & HuggingFace \\ E5BASE [75] & V2 & 110M & infloat/e5-base-v2 & HuggingFace \\ E5\({}_{\text{LARGE}}\) & V2 & 340M & infloat/e5-large-v2 & HuggingFace \\ \hline \multicolumn{5}{c}{_Large Language Models (LLM)_} \\ \hline GPT-3.5-turbo & \multicolumn{2}{c}{–} & gpt-3.5-turbo-0301 & OpenAI API \\ GPT-4 & – & gpt-4-0314 & OpenAI API \\ \hline \hline \end{tabular}
\end{table}
Table 2: Details about the baselines and models used in our experiments.

[MISSING_PAGE_FAIL:7]

3-shot), we found common sources of error to include misformatted outputs (4.4% of all predicted groups) and hallucinated clues (6.6%).

Surprisingly, more in-context examples (from 1 to 10 shot) did not improve performance. One possible explanation for this observation is that, due to the huge variety of possible connection types, the in-context examples' primary benefit is demonstrating the expected output format - as opposed to demonstrating how to perform the task - which likely requires only a single example. This is related to the concepts of _task learning_ versus _task recognition_, which are thought to be the two distinct mechanisms through which ICL leverages demonstrations [50, 32]. Many clues require open-domain, arcane, cultural and intimate knowledge of niche subject areas (e.g., "_Professional snooker players_", "_Female Radio 1 DJs_") that, without prior memorization, are unlikely to help. The presence of orthographically similar clue words in the in-context examples could themselves act as red herrings and plausibly induce negative transfer learning. An interesting future direction would be the evaluation of retrieval augmented models [24, 37, 11, 29], which may be capable of solving groups about highly specific subject areas.

### Task 2: Connections Results

In Figure 6, we present the results for Task 2 (Connections). In general, GPT-4 outperforms GPT-3.5-turbo, especially in the 0-shot regime. Performance for GPT-4 improves monotonically with an increasing number of in-context examples, although improvements are sometimes small (e.g.,

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \# In-context Examples & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline GPT-3.5-turbo & 0-shot & \(82.5\) & \(34.0\) & \(18.4\) & \(21.6\) & \(0\) & \(114\) \\  & 1-shot & \(82.3\) & \(34.4\) & \(18.2\) & \(21.2\) & \(0\) & \(123\) \\  & 3-shot & \(80.9\) & \(36.8\) & \(21.3\) & \(24.7\) & \(0\) & \(140\) \\  & 5-shot & \(80.6\) & \(37.3\) & \(22.0\) & \(25.4\) & \(2\) & \(149\) \\  & 10-shot & \(81.2\) & \(36.1\) & \(20.4\) & \(24.0\) & \(2\) & \(137\) \\ \multirow{2}{*}{GPT-4} & 0-shot & \(75.8\) & \(41.5\) & \(27.2\) & \(30.7\) & \(6\) & \(239\) \\  & 1-shot & \(73.4\) & \(43.7\) & \(29.7\) & \(33.5\) & \(4\) & \(262\) \\  & 3-shot & \(73.7\) & \(43.9\) & \(29.9\) & \(33.6\) & \(5\) & \(272\) \\  & 5-shot & \(\mathbf{72.9}\) & \(43.4\) & \(29.1\) & \(32.8\) & \(\mathbf{7}\) & \(269\) \\  & 10-shot & \(73.6\) & \(42.8\) & \(28.5\) & \(32.3\) & \(3\) & \(249\) \\ \hline Human Performance & – & – & – & – & \(285\,/\,494\) & \(1405\,/\,1976\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on Task 1 (Grouping) using Large Language Models. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. **Bold**: best scores.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline \multicolumn{6}{c}{_Classic Word Embeddings_} \\ \hline GloVe & \(84.9\pm.4\) & \(31.5\pm.3\) & \(14.4\pm.3\) & \(17.6\pm.4\) & \(0\pm 0\) & \(68\pm 4\) \\ FastText (Crawl) & \(84.2\pm.5\) & \(32.1\pm.3\) & \(15.2\pm.3\) & \(18.4\pm.4\) & \(0\pm 0\) & \(80\pm 4\) \\ FastText (News) & \(85.5\pm.5\) & \(30.4\pm.2\) & \(13.0\pm.2\) & \(15.8\pm.3\) & \(0\pm 0\) & \(62\pm 3\) \\ \hline \multicolumn{6}{c}{_Pre-trained Language Models (PLMs)_} \\ \hline ELMoLAGE & \(86.3\pm.6\) & \(29.5\pm.3\) & \(11.8\pm.4\) & \(14.5\pm.4\) & \(0\pm 0\) & \(55\pm 4\) \\ DistilBERTBASE & \(86.7\pm.6\) & \(29.1\pm.2\) & \(11.3\pm.3\) & \(14.0\pm.3\) & \(0\pm 0\) & \(49\pm 4\) \\ BERTLARGE & \(88.3\pm.5\) & \(26.5\pm.2\) & \(8.2\pm.3\) & \(10.3\pm.3\) & \(0\pm 0\) & \(33\pm 2\) \\ BERTBASE & \(89.5\pm.4\) & \(25.1\pm.2\) & \(6.4\pm.3\) & \(8.1\pm.4\) & \(0\pm 0\) & \(22\pm 2\) \\ RoBERTLARGE & \(88.4\pm.4\) & \(26.7\pm.2\) & \(8.4\pm.3\) & \(9.4\pm.4\) & \(0\pm 0\) & \(29\pm 3\) \\ \hline \multicolumn{6}{c}{_Sentence Transformers_} \\ \hline all-mpnetBASE & \(86.3\pm.4\) & \(29.4\pm.3\) & \(11.7\pm.4\) & \(14.3\pm.5\) & \(0\pm 0\) & \(50\pm 4\) \\ ES4RGE & \(84.4\pm.7\) & \(32.3\pm.4\) & \(15.4\pm.5\) & \(18.5\pm.6\) & \(0\pm 0\) & \(76\pm 5\) \\ ESBASE & \(\mathbf{83.8}\pm.6\) & \(\mathbf{33.1}\pm.3\) & \(\mathbf{16.3}\pm.4\) & \(\mathbf{19.5}\pm.4\) & \(\mathbf{1}\pm\mathbf{0}\) & \(\mathbf{89}\pm\mathbf{6}\) \\ \hline Human Performance & – & – & – & – & \(285\,/\,494\) & \(1405\,/\,1976\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results of selected models on Task 1 (Grouping) using static embeddings. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. Mean \(\pm\) standard deviation over 16 random seeds is shown. **Bold**: best scores.

\(<0.01\)). As expected, the exact match score for both models is low (\(<15\%\)). This is explained by the fact that even insignificant differences between the model's predictions and the ground truth will result in a score of 0 (e.g., "Made _of_ rubber" vs. "Made _from_ rubber"). For this reason, we also report ROUGE-1 and BERTScore F1 scores (SS2.2). Although not a perfect comparison, we can contextualize these results with human performance, which we recorded as the fraction of correctly guessed connections: \(\sim\)80% on the test set. The quiz show Only Connect allows for some small deviations in guessed connections that will be accepted as correct, making the comparison to ROUGE and BERTScore more suitable than to exact match. Our results suggest that at 41-45% F1, the best performance achieved with few-shot ICL (GPT-4, 10-shot) is far below human performance. Lastly, we note that a common source of model error was the inclusion of clues in the predicted connection (occurring in 8.2% of all predicted connections for the best performing model), e.g., "Fireplace tools (Spade, Brush, Poker, Tongs)", even though (1) the model was not instructed to do so, and (2) the in-context examples were not formatted like this.

More complicated post-processing or prompting strategies (e.g., "Chain of Thought" [79], "Tree of Thoughts" [87]) could mitigate these issues and improve performance. However, applying these more complicated prompting strategies to the OCW dataset is non-trivial, as they require breaking down the problem into intermediate steps, and the number or nature these intermediate reasoning steps should take is unclear. We leave their application to the OCW dataset for future work.

### Effects of Red-Herrings: Additional Datasets, Experiments and Analyses

To analyze our _red-herring hypothesis_ on language models, we designed and performed additional ablative experiments. The original OCW dataset contains red-herrings as distractors _by design_. We generate two additional datasets from OCW to decrease the presence of red-herrings: OCW-Randomized and OCW-WordNet. The goals, construction and other details are presented in Appendix SSC.1.

In OCW-Randomized, we diluted the presence of red herrings by randomly swapping groups among the walls in the test set - thus negating the inherent deliberate distractor groups in each wall. We further simplify the grouping task in OCW-WordNet by removing red herrings altogether. This is achieved by using subordinate-superlative (or hyponym-hypernym) word hierarchy and synonyms in the English lexical database WordNet [46, 20]. Thus the results in Table 5 present results on datasets with a decreasing proportion of red herrings from left to right, and by our hypothesis, increasing task simplicity for LLMs. The results are aligned with our expectations, with GPT-3.5-turbo and GPT-4 performance increasing significantly with the reduction of red herrings from the test set.

## 5 Related Work

Various datasets and tasks have been proposed for evaluating language models against human-like linguistic capabilities. Earlier examples of such tasks include _word sense disambiguation_ (WSD) [55],

Figure 6: Results for Task 2 (Connections) with GPT-3.5-turbo and GPT-4. For reference, human performance is approximately 80% (fraction of correctly answered connections). We report \(\max(\text{BERTScore},0)\) in the case of GPT-3.5-turbo for readability.

Winograd schema challenge [35] and _word sense induction_ (WSI) [80]. WSD aims to determine a word's correct meaning or sense within a specific context. WSI focuses on automatically clustering words into different senses or semantic categories based on their contextual usage patterns. Benchmarks like GLUE [74] and SuperGLUE [73] are aimed at aggregating and standardizing these classical NLP tasks to evaluate language models. The PLMs (e.g., BERT variants) and the first generation of LLMs, mostly solved or attained human-level performance on these tasks by 2020s [41].

In order to evaluate the human-imitative capabilities of modern LLMs, more challenging tasks have been proposed in recent benchmarks like BIG-bench [66] and HumanEval [15]. **BIG-bench** aims to address the limitations of existing benchmarks by providing a more comprehensive, open, and dynamic (tasks added on a rolling basis) evaluation benchmark. It covers a wide range of tasks, including a suite of tasks targeted specifically for _human-like behavior_. **HumanEval** is an evaluation set to measure the functional correctness of code synthesis from docstrings [15]. This benchmark includes 164 original programming problems that assess language comprehension, algorithms, and simple mathematics comparable to simple software interview questions. While these recent benchmarks include a wide net of complex tasks, evaluating a broad range of LLM capabilities, our work here is orthogonal to these since none of them aims to specifically measure creative problem-solving or creativity and their impediments in LLMs.

## 6 Limitations & Future Work

As with any machine learning dataset, especially one designed to evaluate the performance of LLMs, the OCW dataset has several limitations. First, we noticed that the performance of contextual approaches can vary significantly depending on the order that clues are provided to the model. To alleviate this (and where feasible), we evaluate models across 16 random sortings of the clues. Due to cost, we did not evaluate GPT-3.5-turbo and GPT-4's sensitivity to this ordering; future work should report performance across multiple random sorts. Second, due to the nature of the quiz show _Only Connect_, the clues, groups, and connections in the dataset tend to be Western- (and specifically UK-) centric (e.g. "_Doctor Who companions_", "_English cricket captains_", "_Irish counties_"). Therefore, performance on the OCW dataset may not extrapolate to languages or cultures outside of Western English. In fact, the _US_-centric bias of LLMs like GPT-3.5/4 [84] might partially explain their poor performance on the _UK_-centric OCW dataset. We hope to add additional _Only Connect_ inspired walls in multiple languages and with clues derived from various cultures & subcultures in future work. Finally, given that the walls are publicly available as text on fan sites like ocdb.cc, there is always the possibility that they are included in the training sets of LLMs like GPT. However, we think this is unlikely, given the low performance on the grouping and connection tasks. Preventing the test sets of publicly available datasets like our OCW from "leaking" into the training sets of LLMs remains an interesting and open problem. We have taken basic steps against this leakage by distributing our dataset in a compressed format [30].

\begin{table}
\begin{tabular}{l l l l l l l l} \hline \hline  & \multicolumn{3}{c}{**OCW**} & \multicolumn{3}{c}{**OCW-Randomized**} & \multicolumn{3}{c}{**OCW-WordNet**} \\ \cline{3-8}  & \multicolumn{2}{c}{\# Solved Walls \# Correct Groups} & \multicolumn{1}{c}{\# Solved Walls \# Correct Groups} & \multicolumn{1}{c}{\# Solved Walls \# Correct Groups} & \multicolumn{1}{c}{\# Solved Walls \# Correct Groups} \\ \hline GPT-3.5-turbo & 0-shot & 0 & 114 & 5 & 274 & 337 & 1522 \\  & 1-shot & 0 & 123 & 12 & 315 & 320 & 1400 \\  & 3-shot & 0 & 140 & 10 & 306 & 415 & 1748 \\  & 5-shot & **2** & **149** & 16 & **337** & 415 & 1759 \\  & 10-shot & 2 & 137 & **17** & 333 & **428** & **1800** \\ GPT-4 & 0-shot & 6 & 239 & 59 & 595 & **471** & **1926** \\  & 1-shot & 4 & 262 & 57 & 644 & 304 & 1581 \\  & 3-shot & 5 & **272** & 62 & 649 & 279 & 1537 \\  & 5-shot & **7** & 269 & **68** & **655** & 298 & 1584 \\  & 10-shot & 3 & 249 & 55 & 614 & 378 & 1742 \\ \hline Human Performance & 285 / 494 & 1405 / 1976 & – & – & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 5: Coalesced results of LLMs performance on Task 1 (grouping) using two additional test datasets OCW-Randomized and OCW-WordNet with decreasing presence of red-herrings from _left_ to _right_ in the walls, juxtaposed against the original OCW test set (left-most column). Only the main metrics are shown (details and full results in Appendix SC). **Bold**: best scores.

## References

* [1] Microsoft guidance: A guidance language for controlling large language models. https://github.com/microsoft/guidance. 2023.
* [2] OpenAI API completions reference. https://platform.openai.com/docs/api-reference/completions. 2023.
* [3] Only Connect. Television show, 2008-2020. Created by Presentable, RDF Television and Parasol, Presented by Victoria Coren Mitchell.
* [4] A. Akbik, T. Bergmann, D. Blythe, K. Rasul, S. Schweter, and R. Vollgraf. FLAIR: An easy-to-use framework for state-of-the-art NLP. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)_, pages 54-59, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4010.
* [5] S. Altman. Planning for AGI and beyond. _OpenAI Blog, February_, 2023.
* [6] C. Anotado, T. Ruddle, and J. Halbur. The Only Connect Database.
* [7] S. Basu, I. Davidson, and K. Wagstaff. _Constrained clustering: Advances in algorithms, theory, and applications_. CRC Press, 2008.
* [8] Z. Beda and S. M. Smith. Chasing red herrings: Memory of distractors causes fixation in creative problem solving. _Memory & Cognition_, 46:671-684, 2018.
* [9] M. Benedek and A. C. Neubauer. Revisiting Mednick's model on creativity-related differences in associative hierarchies. Evidence for a common path to uncommon thought. _The Journal of creative behavior_, 47(4):273-289, 2013.
* [10] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models. _ArXiv preprint_, abs/2108.07258, 2021.
* [11] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 2206-2240. PMLR, 2022.
* [12] P. S. Bradley, K. P. Bennett, and A. Demiriz. Constrained k-means clustering. _Microsoft Research, Redmond_, 20(0):0, 2000.
* [13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [14] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg, et al. Sparks of artificial general intelligence: Early experiments with GPT-4. _ArXiv preprint_, abs/2303.12712, 2023.
* [15] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. _ArXiv preprint_, abs/2107.03374, 2021.

* [16] A. Chowdherey, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. PaLM: Scaling language modeling with pathways. 2022. _ArXiv preprint_, abs/2204.02311, 2022.
* [17] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
* [18] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. P. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui. Glam: Efficient scaling of language models with mixture-of-experts. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato, editors, _International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA_, volume 162 of _Proceedings of Machine Learning Research_, pages 5547-5569. PMLR, 2022.
* [19] K. Ethayarajh. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 55-65, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1006.
* [20] C. Fellbaum. Wordnet. In _Theory and applications of ontology: computer applications_, pages 231-243. Springer, 2010.
* [21] E. B. Fowlkes and C. L. Mallows. A method for comparing two hierarchical clusterings. _Journal of the American statistical association_, 78(383):553-569, 1983.
* [22] J. Gao, L. Ge, K. Li, H. Q. Ngo, and A. Zhang. On handling negative transfer and imbalanced distributions in multiple source transfer learning. In _Proceedings of the 13th SIAM International Conference on Data Mining, May 2-4, 2013. Austin, Texas, USA_, pages 261-269. SIAM, 2013. doi: 10.1137/1.9781611972832.29.
* [23] E. Grave, P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov. Learning word vectors for 157 languages. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, 2018. European Language Resources Association (ELRA).
* [24] K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event_, volume 119 of _Proceedings of Machine Learning Research_, pages 3929-3938. PMLR, 2020.
* [25] J. A. Hartigan and M. A. Wong. Algorithm as 136: A k-means clustering algorithm. _Journal of the royal statistical society. series c (applied statistics)_, 28(1):100-108, 1979.
* [26] B. Heinzerling and M. Strube. BPEmb: Tokenization-free pre-trained subword embeddings in 275 languages. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_, Miyazaki, Japan, 2018. European Language Resources Association (ELRA).
* [27] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [28] L. Hubert and P. Arabie. Comparing partitions. _Journal of classification_, 2:193-218, 1985.
* [29] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. A. Yu, A. Joulin, S. Riedel, and E. Grave. Few-shot learning with retrieval augmented language models. _ArXiv preprint_, abs/2208.03299, 2022.

* [30] A. Jacovi, A. Caciularu, O. Goldman, and Y. Goldberg. Stop uploading test data in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks. _ArXiv preprint_, abs/2305.10160, 2023.
* [31] M. I. Jordan. Artificial intelligence--the revolution hasn't happened yet. _Harvard Data Science Review_, 1(1):1-9, 2019.
* [32] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, and R. McHardy. Challenges and applications of large language models. _ArXiv preprint_, abs/2307.10169, 2023.
* [33] G. Ke, D. He, and T. Liu. Rethinking positional encoding in language pre-training. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021.
* [34] N. Kohn and S. M. Smith. Partly versus completely out of your mind: Effects of incubation and distraction on resolving fixation. _The Journal of Creative Behavior_, 43(2):102-118, 2009.
* [35] H. Levesque, E. Davis, and L. Morgenstern. The winograd schema challenge. In _Thirteenth international conference on the principles of knowledge representation and reasoning_, 2012.
* [36] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 7871-7880, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703.
* [37] P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. Yih, T. Rocktaschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [38] P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. _ArXiv preprint_, abs/2211.09110, 2022.
* [39] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, 2004. Association for Computational Linguistics.
* [40] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. _ArXiv preprint_, abs/1907.11692, 2019.
* [41] N. Lu, S. Liu, R. He, and K. Tang. Large language models can be guided to evade AI-generated text detection. _ArXiv preprint_, abs/2305.10847, 2023.
* [42] A. S. Luchins and E. H. Luchins. Rigidity of behavior: A variational approach to the effect of Einstellung. 1959.
* [43] M. Marko, D. Michalko, and I. Riecansky. Remote associates test: An empirical proof of concept. _Behavior research methods_, 51:2700-2711, 2019.
* [44] S. Mednick. The associative basis of the creative process. _Psychological review_, 69(3):220, 1962.
* [45] S. A. Mednick. The remote associates test. _The Journal of Creative Behavior_, 1968.
* [46] G. A. Miller. WordNet: A lexical database for English. In _Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, February 23-26, 1992_, 1992.
* [47] R. Navigli. Word sense disambiguation: A survey. _ACM computing surveys (CSUR)_, 41(2):1-69, 2009.
* [48] OpenAI. GPT-4 technical report. _ArXiv preprint_, abs/2303.08774, 2023.

* [49] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* [50] J. Pan, T. Gao, H. Chen, and D. Chen. What in-context learning "learns" in-context: Disentangling task recognition and task learning. In _Annual Meeting of the Association for Computational Linguistics_, 2023.
* [51] J. Pennington, R. Socher, and C. Manning. GloVe: Global vectors for word representation. In _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 1532-1543, Doha, Qatar, 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162.
* [52] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 2227-2237, New Orleans, Louisiana, 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202.
* [53] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _J. Mach. Learn. Res._, 21:140:1-140:67, 2020.
* [54] A. Ramdas, N. Garcia Trillos, and M. Cuturi. On wasserstein two-sample testing and related families of nonparametric tests. _Entropy_, 19(2):47, 2017.
* [55] O. Sainz, O. L. de Lacalle, E. Agirre, and G. Rigau. What do language models know about word senses? zero-shot wsd with language models and domain inventories. _ArXiv preprint_, abs/2302.03353, 2023.
* [56] V. Sanh, L. Debut, J. Chaumond, and T. Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. _ArXiv preprint_, abs/1910.01108, 2019.
* [57] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagne, A. S. Luccioni, F. Yvon, M. Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _ArXiv preprint_, abs/2211.05100, 2022.
* [58] J. Shlens. A tutorial on principal component analysis. _arXiv:1404.1100_, 2014.
* [59] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. _ArXiv preprint_, abs/1909.08053, 2019.
* [60] S. M. Smith. The constraining effects of initial ideas. _Group creativity: Innovation through collaboration_, pages 15-31, 2003.
* [61] S. M. Smith and S. E. Blankenship. Incubation effects. _Bulletin of the Psychonomic Society_, 27(4):311-314, 1989.
* [62] S. M. Smith and S. E. Blankenship. Incubation and the persistence of fixation in problem solving. _The American journal of psychology_, pages 61-87, 1991.
* [63] S. M. Smith and D. R. Tindell. Memory blocks in word fragment completion caused by involuntary retrieval of orthographically related primes. _Journal of Experimental Psychology: Learning, Memory, and Cognition_, 23(2):355, 1997.
* [64] K. Song, X. Tan, T. Qin, J. Lu, and T. Liu. Mpnet: Masked and permuted pre-training for language understanding. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* [65] Y. Song, C. Cui, S. Khanuja, P. Liu, F. Faisal, A. Ostapenko, G. I. Winata, A. F. Aji, S. Cahyawijaya, Y. Tsvetkov, et al. GlobalBench: A benchmark for global progress in natural language processing. _ArXiv preprint_, abs/2305.14716, 2023.

* [66] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _ArXiv preprint_, abs/2206.04615, 2022.
* [67] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roziere, N. Goyal, E. Hambro, F. Azhar, et al. LLaMA: Open and efficient foundation language models. _ArXiv preprint_, abs/2302.13971, 2023.
* [68] L. Van der Maaten and G. Hinton. Visualizing data using t-SNE. _Journal of machine learning research_, 9(11), 2008.
* [69] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. In I. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V. N. Vishwanathan, and R. Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008, 2017.
* [70] C. Villani. _Topics in optimal transportation_, volume 58. American Mathematical Soc., 2021.
* ICML, 2009.
* July 1, 2001_, pages 577-584. Morgan Kaufmann, 2001.
* [73] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In H. M. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. B. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 3261-3275, 2019.
* [74] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019.
* [75] L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei. Text embeddings by weakly-supervised contrastive pre-training. _ArXiv preprint_, abs/2212.03533, 2022.
* [76] Z. Wang, Z. Dai, B. Poczos, and J. G. Carbonell. Characterizing and avoiding negative transfer. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019_, pages 11293-11302. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.01155.
* [77] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le. Finetuned language models are zero-shot learners. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [78] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, et al. Emergent abilities of large language models. _ArXiv preprint_, abs/2206.07682, 2022.
* [79] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.

* [80] L. White, R. Togneri, W. Liu, and M. Bennamoun. Finding word sense embeddings of known meaning. In _Computational Linguistics and Intelligent Text Processing: 19th International Conference, CICLing 2018, Hanoi, Vietnam, March 18-24, 2018, Revised Selected Papers, Part II_, pages 3-16. Springer, 2023.
* [81] Wikipedia contributors. Only connect -- Wikipedia, the free encyclopedia. https://en.wikipedia.org/w/index.php?title=Only_Connect&oldid=1157929067, 2023. [Online; accessed 7-June-2023].
* [82] J. Wiley. Expertise as mental set: The effects of domain knowledge in creative problem solving. _Memory & cognition_, 26:716-730, 1998.
* [83] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, pages 38-45, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6.
* [84] R. Wolfe and A. Caliskan. American== white in multimodal language-and-image ai. In _Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society_, pages 800-812, 2022.
* [85] G. Wood and J. Pennington. Encoding and retrieval from long-term storage. _Journal of Experimental Psychology_, 99(2):243, 1973.
* [86] C.-L. Wu, S.-Y. Huang, P.-Z. Chen, and H.-C. Chen. A systematic review of creativity-related studies applying the remote associates test from 2000 to 2019. _Frontiers in psychology_, 11:573432, 2020.
* [87] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. _ArXiv preprint_, abs/2305.10601, 2023.
* [88] B. Yin, M. Zhao, L. Guo, and L. Qiao. Sentence-BERT and k-means based clustering technology for scientific and technical literature. In _2023 15th International Conference on Computer Research and Development (ICCRD)_, pages 15-20. IEEE, 2023.
* [89] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al. OPT: Open pre-trained transformer language models. _ArXiv preprint_, abs/2205.01068, 2022.
* [90] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. Bertscore: Evaluating text generation with BERT. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [91] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, et al. A survey of large language models. _ArXiv preprint_, abs/2303.18223, 2023.

## Appendix A Additional Experiments

Task 1 - GroupingIn addition to grouping clue words using token embeddings (discussed in the main paper SS4), we also ran grouping the words by clustering on 'contextual' embeddings. We experimentally induce 'context' by joining the sixteen (16) word tokens (in a random order) into a single pseudo-sentence. The embeddings for each token were different based on the ordering of the tokens. We repeat the random ordering sixteen times and report the mean and variance of the results obtained in Table 6.

Task 2 - ConnectionsIn addition to prompting based results on GPT-4 (discussed in SS4), we ran experiments on additional LLMs like LLaMa [67] (7B, 13B) using pre-trained configuration weights obtained by permission from Meta AI. However, without additional fine-tuning on the specific task, these LLMs were unable to solve the task in a meaningful manner. To elucidate, LLaMa generated a bunch of hallucinated words with unequal group sizes. We omit these unintelligible results for brevity.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline ELMoLAGE & \(90.0\pm.3\) & \(23.6\pm.4\) & \(4.5\pm.5\) & \(5.6\pm.7\) & \(0\pm 0\) & \(19\pm 3\) \\ DistilBERTBASE & \(88.4\pm.7\) & \(26.7\pm.3\) & \(8.3\pm.4\) & \(10.4\pm.5\) & \(0\pm 0\) & \(30\pm 4\) \\ BERTLARGE & \(\mathbf{87.2\pm.6}\) & \(\mathbf{28.3\pm.5}\) & \(\mathbf{10.4\pm.6}\) & \(\mathbf{12.8\pm.7}\) & \(0\pm 0\) & \(\mathbf{46\pm 5}\) \\ BERTBASE & \(87.7\pm.5\) & \(28.0\pm.2\) & \(10.0\pm.3\) & \(12.4\pm.4\) & \(0\pm 0\) & \(39\pm 2\) \\ RoBERTLARGE & \(88.4\pm.5\) & \(25.9\pm.2\) & \(7.4\pm.3\) & \(9.3\pm.4\) & \(0\pm 0\) & \(30\pm 4\) \\ all-mpnetBASE & \(87.6\pm.5\) & \(28.0\pm.3\) & \(10.0\pm.4\) & \(12.4\pm.5\) & \(0\pm 0\) & \(38\pm 3\) \\ ES\({}_{\text{LARGE}}\) & \(87.7\pm.5\) & \(28.1\pm.3\) & \(10.2\pm.4\) & \(12.7\pm.5\) & \(0\pm 0\) & \(37\pm 4\) \\ ES\({}_{\text{BASE}}\) & \(\mathbf{87.2\pm.3}\) & \(28.2\pm.2\) & \(10.2\pm.3\) & \(12.5\pm.4\) & \(0\pm 0\) & \(\mathbf{46\pm 5}\) \\ \hline Human Performance & – & – & – & \(285\,/\,494\) & \(1405\,/\,1976\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results of selected models on Task 1 (Grouping) using contextual embeddings. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. Mean \(\pm\) standard deviation over 16 random seeds is shown. **Bold**: best scores.

[MISSING_PAGE_EMPTY:18]

Figure 9: Solved wall (wall_id="2d8f") for Task 1 (Grouping) using BERT\({}_{\text{LARGE}}\) with both static and contextual embeddings. **Left**: contextual embedding solved 3/4 groups. Here the clue “_Rambrandt_” is placed near other Dutch painters. The correct grouping for this clue in this wall is “_Toothpaste Brands_”. **Right**: static embedding solved 0/4 groups.

Effects of Red-Herrings: Additional Experiments, Analysis and Results

### Additional Datasets

Both of the additional datasets described in this section for ablation experiments have been made available via our code repositories on Github and HuggingFace.

#### c.1.1 OCW-Randomized Dataset

This test dataset generates a version of the test set where red herrings are removed or largely reduced in frequency. This is achieved by rebuilding every wall using a randomly selected group from different walls. We only applied the process to the (original OCW) test set, the train and validation sets are left untouched.

MethodFor each wall in the existing test set, we leave the first group untouched, and sample three new groups, each from a different wall, such that none of the groups share a word in common. The connections for each group are unmodified. The result is a new version of the test set where every wall is composed of 4 random groups from 4 different walls.

#### c.1.2 OCW-WordNet Dataset

WordNet [46, 20] is a large lexical database of English. Nouns, verbs, adjectives, and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. We use the hypernym/hyponym (or superlative/subordinative) hierarchical lexical structure aggregated in WordNet to generate an easy test set to further analyze the effects of red-herring in OCW.

MethodWe use the existing words in a wall to select synonyms from the word's synsets. We only consider synsets that have at least five synonymous lexical names, then randomly sample four words. The original test set word and its definition (ss.definition()) subsequently becomes the connection phrase for the group. Four groups were generated for each wall, and the easy wall generation process was repeated for the total number of walls (494) in the original test data set.

For the group connections, we concatenate the superlative parent word with a synset definition giving a description of the word. This allows for an ideal semantic similarity score to be calculated using BERTScore. For a few cases (approx. 70/494 walls in the test set), the number of generated groups per wall is less than four, due to the unavailability of direct synonyms from word synsets. In those edge cases, we generate and append groups using common hypernym words like animal, mammal, furniture, etc. to ensure a wall is valid with four groups.

A sample generated easy group is shown below, where we prefix the group_id from the original OCW dataset with 'easy' to aid with mapping or identification.

{ ...  "group_3": {  "group_id": "easy_691a_3",  "gt_words": ["gibe","shaft","jibe","barb"],  "gt_connection": "Shaft: an aggressive remark directed at a person  like a missile and intended to have a telling effect" ... } Further, we generate easy to train and validation sets mimicking the original dataset, package and release these three additional easy sets, as **OCW-WordNet** as added contributions.

### Results of Ablation Experiments

#### c.2.1 PLMs: Performance on Task 1 (Grouping)

We perform and present the results using'static' embeddings due to the noted superior results and the word order related deficiency already shown by using contextual embeddings pertinent to our task setup.

#### c.2.2 LLMs: Performance on Task 1 (Grouping) using GPT3.5/4

Here we present the results of repeating Task 1 (grouping) on the ablation datasets OCW-Randomized (C.1.1) and OCW-Wordnet (C.1.2) to analyze the effects of red-herrings in walls on LLM performance.

The results adhere to the expected results of superior performance with the dilution/removal of red-herrings from the walls.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline \multicolumn{6}{c}{_Classic Word Embeddings_} \\ \hline GloVe & \(76.8\pm.7\) & \(39.2\pm.3\) & \(24.0\pm.4\) & \(27.7\pm.4\) & \(7\pm 1\) & \(213\pm 8\) \\ FastText (Crawl) & \(76.1\pm.5\) & \(40.5\pm.3\) & \(25.0\pm.6\) & \(28.6\pm.7\) & \(\mathbf{13}\pm\mathbf{1}\) & \(236\pm 7\) \\ FastText (News) & \(79.3\pm.5\) & \(36.8\pm.3\) & \(21.0\pm.3\) & \(24.5\pm.4\) & \(5\pm 1\) & \(176\pm 6\) \\ \hline \multicolumn{6}{c}{_Pre-trained Language Models (PLMs)_} \\ \hline ELMoLARG & \(80.9\pm.4\) & \(35.2\pm.3\) & \(18.9\pm.3\) & \(22.2\pm.4\) & \(3\pm 1\) & \(154\pm 6\) \\ DistilBERTBASE & \(82.3\pm.6\) & \(34.2\pm.4\) & \(17.7\pm.5\) & \(21.1\pm.5\) & \(1\pm 1\) & \(124\pm 8\) \\ BERTLARG & \(86.2\pm.4\) & \(29.2\pm.3\) & \(11.5\pm.3\) & \(14.2\pm.4\) & \(0\pm 0\) & \(66\pm 4\) \\ BERTBASE & \(87.5\pm.4\) & \(27.7\pm.3\) & \(9.6\pm.6\) & \(11.8\pm.5\) & \(0\pm 0\) & \(48\pm 4\) \\ RoBERTLARG & \(86.7\pm.5\) & \(28.6\pm.2\) & \(10.8\pm.3\) & \(13.4\pm.3\) & \(1\pm 0\) & \(56\pm 4\) \\ \hline \multicolumn{6}{c}{_Sentence Transformers_} \\ \hline all-mptepBASE & \(81.4\pm.4\) & \(35.1\pm.4\) & \(18.9\pm.5\) & \(22.0\pm.6\) & \(8\pm 1\) & \(154\pm 7\) \\ E5LARG & \(76.0\pm.5\) & \(40.7\pm.3\) & \(25.9\pm.4\) & \(29.7\pm.4\) & \(8\pm 1\) & \(230\pm 5\) \\ E5BASE & \(\mathbf{75.1\pm.8}\) & \(\mathbf{41.8\pm.3}\) & \(\mathbf{27.2\pm.3}\) & \(\mathbf{31.1\pm.3}\) & \(8\pm 1\) & \(\mathbf{249\pm 8}\) \\ \hline \multicolumn{6}{c}{_Human Performance_} & – & – & – & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results of **OCW-Randomized** using static embeddings. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. Mean \(\pm\) standard deviation over 16 random seeds is shown. **Bold**: best scores.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline \multicolumn{6}{c}{_Classic Word Embeddings_} \\ \hline GloVe & \(43.0\pm 1.0\) & \(66.1\pm.4\) & \(57.4\pm.5\) & \(60.9\pm.5\) & \(118\pm 3\) & \(886\pm 1\) \\ FastText (Crawl) & \(30.6\pm 1.0\) & \(75.8\pm.6\) & \(69.6\pm.7\) & \(72.4\pm.7\) & \(195\pm 6\) & \(1173\pm 18\) \\ FastText (News) & \(44.9\pm 1.2\) & \(64.9\pm.5\) & \(55.9\pm.6\) & \(59.5\pm.6\) & \(105\pm 3\) & \(844\pm 12\) \\ \hline \multicolumn{6}{c}{_Pre-trained Language Models (PLMs)_} \\ \hline ELMoLARG & \(52.5\pm 1.1\) & \(58.9\pm.3\) & \(48.2\pm.4\) & \(52.5\pm.4\) & \(67\pm 3\) & \(682\pm 9\) \\ DistilBERTBASE & \(45.5\pm 1.0\) & \(64.1\pm.4\) & \(55.0\pm.5\) & \(58.7\pm.5\) & \(105\pm 3\) & \(835\pm 13\) \\ BERTLARG & \(76.9\pm 1.0\) & \(38.9\pm.2\) & \(23.4\pm.3\) & \(27.5\pm.3\) & \(7\pm 0\) & \(197\pm 6\) \\ BERTBASE & \(73.0\pm 1.3\) & \(42.5\pm.5\) & \(27.9\pm.6\) & \(32.5\pm.6\) & \(8\pm 2\) & \(268\pm 12\) \\ RoBERTLARG & \(57.4\pm 1.3\) & \(54.8\pm.3\) & \(43.3\pm.3\) & \(47.5\pm.3\) & \(48\pm 2\) & \(573\pm 8\) \\ \hline \multicolumn{6}{c}{_Sentence Transformers_} \\ \hline all-mptepBASE & \(\mathbf{22.6\pm.7}\) & \(\mathbf{81.9\pm.4}\) & \(\mathbf{77.1\pm.5}\) & \(\mathbf{79.4\pm.4}\) & \(\mathbf{256\pm 4}\) & \(\mathbf{1365\pm 12}\) \\ E5LARG & \(23.6\pm.8\) & \(80.9\pm.4\) & \(75.9\pm.5\) & \(78.3\pm.4\) & \(250\pm 4\) & \(1347\pm 12\) \\ E5BASE & \(26.9\pm.9\) & \(78.0\pm.4\) & \(72.3\pm.5\) & \(75.0\pm.5\) & \(224\pm 4\) & \(1259\pm 10\) \\ \hline Human Performance & – & – & – & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results of **OCW-WordNet** using static embeddings. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. Mean \(\pm\) standard deviation over 16 random seeds is shown. **Bold**: best scores.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \# In-context Examples & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline GPT-3.5-turbo & 0-shot & \(15.9\) & \(86.3\) & \(83.4\) & \(84.9\) & \(337\) & \(1522\) \\  & 1-shot & \(24.8\) & \(76.4\) & \(74.4\) & \(75.4\) & \(320\) & \(1400\) \\  & 3-shot & \(8.65\) & \(92.7\) & \(91.2\) & \(91.8\) & \(415\) & \(1748\) \\  & 5-shot & \(8.09\) & \(94.0\) & \(92.4\) & \(93.1\) & \(415\) & \(1759\) \\  & 10-shot & \(6.55\) & \(95.3\) & \(94.0\) & \(94.7\) & \(428\) & \(1800\) \\ \cline{2-7} GPT-4 & 0-shot & \(\mathbf{1.51}\) & \(\mathbf{98.5}\) & \(\mathbf{98.0}\) & \(\mathbf{98.2}\) & \(\mathbf{471}\) & \(\mathbf{1926}\) \\  & 1-shot & \(19.2\) & \(87.9\) & \(84.3\) & \(83.7\) & \(304\) & \(1581\) \\  & 3-shot & \(21.5\) & \(86.6\) & \(82.5\) & \(81.8\) & \(279\) & \(1537\) \\  & 5-shot & \(19.1\) & \(88.1\) & \(84.5\) & \(83.8\) & \(298\) & \(1584\) \\  & 10-shot & \(11.2\) & \(92.9\) & \(90.7\) & \(90.4\) & \(378\) & \(1742\) \\ \hline \multicolumn{7}{l}{Human Performance} & – & – & – & – & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 10: Results of **OCW-WordNet** using Large Language Models. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. **Bold**: best scores.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & \# In-context Examples & WD \(\downarrow\) & FMS \(\uparrow\) & ARI \(\uparrow\) & AMI \(\uparrow\) & \# Solved Walls & \# Correct Groups \\ \hline GPT-3.5-turbo & 0-shot & \(74.3\) & \(40.4\) & \(26.4\) & \(29.8\) & \(5\) & \(274\) \\  & 1-shot & \(72.0\) & \(43.1\) & \(29.0\) & \(32.3\) & \(12\) & \(315\) \\  & 3-shot & \(72.7\) & \(43.4\) & \(29.4\) & \(32.9\) & \(10\) & \(306\) \\  & 5-shot & \(70.7\) & \(44.6\) & \(30.9\) & \(34.4\) & \(16\) & \(337\) \\  & 10-shot & \(70.5\) & \(43.8\) & \(30.0\) & \(33.5\) & \(17\) & \(333\) \\ \cline{2-7} GPT-4 & 0-shot & \(58.2\) & \(56.2\) & \(45.4\) & \(48.8\) & \(59\) & \(595\) \\  & 1-shot & \(55.1\) & \(\mathbf{58.0}\) & \(\mathbf{47.5}\) & \(\mathbf{51.0}\) & \(57\) & \(644\) \\  & 3-shot & \(55.0\) & \(57.5\) & \(46.9\) & \(50.3\) & \(62\) & \(649\) \\  & 5-shot & \(\mathbf{54.1}\) & \(\mathbf{58.0}\) & \(\mathbf{47.5}\) & \(50.9\) & \(\mathbf{68}\) & \(\mathbf{655}\) \\  & 10-shot & \(56.6\) & \(56.1\) & \(45.1\) & \(48.5\) & \(55\) & \(614\) \\ \hline \multicolumn{7}{l}{Human Performance} & – & – & – & – & – & – \\ \hline \hline \end{tabular}
\end{table}
Table 9: Results of **OCW-Randomized** using Large Language Models. WD: Wasserstein Distance. FMS: Fowlkes Mallows Score. ARI: Adjusted Rand Index. NMI: Normalized Mutual Information. **Bold**: best scores.