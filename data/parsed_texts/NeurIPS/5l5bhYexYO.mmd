# Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers

 Kai Yan  Alexander G. Schwing  Yu-Xiong Wang

University of Illinois Urbana-Champaign

{kaiyan3, aschwing, yxw}@illinois.edu

https://github.com/KaiYan289/RL_as_Vitamin_for_Online_Decision_Transformers

###### Abstract

Decision Transformers have recently emerged as a new and compelling paradigm for offline Reinforcement Learning (RL), completing a trajectory in an autoregressive way. While improvements have been made to overcome initial shortcomings, online finetuning of decision transformers has been surprisingly under-explored. The widely adopted state-of-the-art Online Decision Transformer (ODT) still struggles when pretrained with low-reward offline data. In this paper, we theoretically analyze the online-finetuning of the decision transformer, showing that the commonly used Return-To-Go (RTG) that's far from the expected return hampers the online fine-tuning process. This problem, however, is well-addressed by the value function and advantage of standard RL algorithms. As suggested by our analysis, in our experiments, we hence find that simply adding TD3 gradients to the fine-tuning process of ODT effectively improves the online finetuning performance of ODT, especially if ODT is pretrained with low-reward offline data. These findings provide new directions to further improve decision transformers.

## 1 Introduction

While Reinforcement Learning (RL) has achieved great success in recent years [55, 31], it is known to struggle with several shortcomings, including training instability when propagating a Temporal Difference (TD) error along long trajectories [14], low data efficiency when training from scratch [67], and limited benefits from more modern neural network architectures [12]. The latter point differs significantly from other parts of the machine learning community such as Computer Vision [17] and Natural Language Processing [11].

To address these issues, Decision Transformers (DTs) [14] have been proposed as an emerging paradigm for RL, introducing more modern transformer architectures into the literature rather than the still widely used Multi-Layer Perceptrons (MLPs). Instead of evaluating state and state-action pairs, a DT considers the whole trajectory as a sequence to complete, and trains on offline data in a supervised, auto-regressive way. Upon inception, DTs have been improved in various ways, mostly dealing with architecture changes [37], the token to predict other than return-to-go [22], addressing the problem of being overly optimistic [46], and the inability to stitch together trajectories [5]. Significant and encouraging improvements have been reported on those aspects.

However, one fundamental issue has been largely overlooked by the community: _offline-to-online RL using decision transformers_, i.e., finetuning of decision transformers with online interactions. Offline-to-online RL [72, 41] is a widely studied sub-field of RL, which combines offline RL learning from given, fixed trajectory data and online RL data from interactions with the environment. By first training on offline data and then finetuning, the agent can learn a policy with much greater data efficiency, while calibrating the out-of-distribution error from the offline dataset. Unsurprisingly, this sub-field has become popular in recent years.

While there are numerous works in the offline-to-online RL sub-field [35; 28; 62], surprisingly few works have discussed the offline-to-online finetuning ability of decision transformers. While there is work that discusses finetuning of decision transformers predicting encoded future trajectory information [64], and work that finetunes pretrained decision transformers with PPO in multi-agent RL [38], the current widely adopted state-of-the-art is the Online Decision Transformer (ODT) [74]: the decision transformer training is continued on online data following the same supervised-learning paradigm as in offline RL. However, this method struggles with low-reward data, as well as with reaching expert-level performance due to suboptimal trajectories [41] (also see Sec. 4).

To address this issue and enhance online finetuning of decision transformers, we theoretically analyze the decision transformer based on recent results [7], showing that the commonly used conditioning on a high Return-To-Go (RTG) that's far from the expected return hampers results. To fix, we explore the possibility of using _tried-and-true RL gradients_. Testing on multiple environments, we find that simply combining TD3 [21] gradients with the original auto-regressive ODT training paradigm is surprisingly effective: it improves results of ODT, especially if ODT is pretrained with low-reward offline data.

Our contributions are summarized as follows:

1) We propose a simple yet effective method to boost the performance of online finetuning of decision transformers, especially if offline data is of medium-to-low quality;

2) We theoretically analyze the online decision transformer, explain its "policy update" mechanism when using the commonly applied high target RTG, and point out its struggle to work well with online finetuning;

3) We conduct experiments on multiple environments, and find that ODT aided by TD3 gradients (and sometimes even the TD3 gradient alone) are surprisingly effective for online finetuning of decision transformers.

## 2 Preliminaries

**Markov Decision Process.** A Markov Decision Process (MDP) is the basic framework of sequential decision-making. An MDP is characterized by five components: the state space \(S\), the action space \(A\), the transition function \(p\), the reward \(r\), and either the discount factor \(\gamma\) or horizon \(H\). MDPs involve an _agent_ making decisions in discrete steps \(t\in\{0,1,2,\dots\}\). On step \(t\), the agent receives the current state \(s_{t}\in S\), and samples an action \(a_{t}\in A\) according to its _stochastic_ policy \(\pi(a_{t}|s_{t})\in\Delta(A)\), where \(\Delta(A)\) is the probability simplex over \(A\), or its _deterministic_ policy \(\mu(s_{t})\in A\). Executing the action yields a reward \(r(s_{t},a_{t})\in\mathbb{R}\), and leads to the evolution of the MDP to a new state \(s_{t+1}\), governed by the MDP's transition function \(p(s_{t+1}|s_{t},a_{t})\). The goal of the agent is to maximize the total reward \(\sum_{t}\gamma^{t}r(s_{t},a_{t})\), discounted by the discount factor \(\gamma\in[0,1]\) for infinite steps, or \(\sum_{t=1}^{H}r(s_{t},a_{t})\) for finite steps. When the agent ends a complete run, it finishes an _episode_, and the state(-action) data collected during the run is referred to as a _trajectory_\(\tau\).

**Offline and Online RL.** Based on the source of learning data, RL can be roughly categorized into offline and online RL. The former learns from a given finite dataset of state-action-reward trajectories, while the latter learns from trajectories collected online from the environment. The effort of combining the two is called _offline-to-online_ RL, which first pre-trains a policy using offline data, and then continues to finetune the policy using online data with higher efficiency. Our work falls into the category of offline-to-online RL. We focus on improving the decision transformers, instead of Q-learning-based methods which are commonly used in offline-to-online RL.

**Decision Transformer (DT).** The decision transformer represents a new paradigm of offline RL, going beyond a TD-error framework. It views a trajectory \(\tau\) as a sequence to be auto-regressively completed. The sequence interleaves three types of tokens: **returns-to-go (RTG, the target total return)**, states, and actions. At step \(t\), the past sequence of context length \(K\) is given as the input, i.e., the input is \((\text{RTG}_{t-K},s_{t-k},a_{t-k},\dots,\text{RTG}_{t},s_{t})\), and an action is predicted by the auto-regressive model, which is usually implemented with a GPT-like architecture [11]. The model is trained via supervised learning, considering the past \(K\) steps of the trajectory along with the current state and the current return-to-go as the feature, and the sequence of all actions \(a\) in a segment as the labels. At evaluation time, a **desired return RTGeval** is specified, since the **ground truth future return RTGreal** isn't known in advance.

**Online Decision Transformer (ODT).**ODT has two stages: offline pre-training which is identical to classic DT training, and online finetuning where trajectories are iteratively collected and the policy is updated via supervised learning. Specifically, the action \(a_{t}\) at step \(t\) during rollouts is computed by the deterministic policy \(\mu^{\text{DT}}(s_{t-T:t},a_{t-T:t-1},\text{RTG}_{t-T:t},T=T_{\text{eval}}, \text{RTG}=\text{RTG}_{\text{eval}})\),1 or sampled from the stochastic policy \(\pi^{\text{DT}}(a_{t}|s_{t-T:t},a_{t-T:t-1},\text{RTG}_{t-T:t},T=T_{\text{eval}},\text{RTG}=\text{RTG}_{\text{eval}})\). Here, \(T\) is the context length (which is \(T_{\text{eval}}\) in evaluation), and \(\text{RTG}_{\text{eval}}\in\mathbb{R}\) is the target return-to-go. The data buffer, initialized with offline data, is gradually replaced by online data during finetuning.

Footnote 1: ODT uses different RTGs for evaluation and online rollouts, but we refer to both as \(\text{RTG}_{\text{eval}}\) as they are both expert-level expected returns.

When updating the policy, the following loss (we use the deterministic policy as an example, and thus omit the entropy regularizer) is minimized:

\[\sum_{t=1}^{T_{\text{train}}}\left\|\mu^{\text{DT}}\left(s_{0:t},a_{0:t-1}, \text{RTG}_{0:t},\text{RTG}=\text{RTG}_{\text{real}},T=t\right)-a_{t}\right\| _{2}^{2}.\] (1)

Note, \(T_{\text{train}}\) is the training context length and \(\text{RTG}_{\text{real}}\) is the real return-to-go. For better readability, we denote \(\{s_{x+1},s_{x+2},\ldots,s_{y}\}\), \(x,y\in\mathbb{N}\) as \(s_{x:y}\) (i.e., left _exclusive_ and right _inclusive_), and similarly \(\{a_{x+1},a_{x+2},\ldots,a_{y}\}\) as \(a_{x:y}\) and \(\{\text{RTG}_{x+1},\ldots,\text{RTG}_{y}\}\) as \(\text{RTG}_{x:y}\). Specially, index \(x=y\) represents an empty sequence. For example, when \(t=1\), \(a_{0:0}\) is an empty action sequence as the decision transformer is not conditioned on any past action.

One important observation: the decision transformer is inherently _off-policy_ (the exact policy distribution varies with the sampled starting point, context length and return-to-go), which effectively guides our choice of RL gradients to off-policy algorithms (see Appendix C for more details).

**TD3.** Twin Delayed Deep Deterministic Policy Gradient (TD3) [21] is a state-of-the-art online off-policy RL algorithm that learns a _deterministic_ policy \(a=\mu^{\text{RL}}(s)\). It is an improved version of an actor-critic (DDPG [32]) with three adjustments to improve its stability: 1) _Clipped double Q-learning_, which maintains two critics (estimators for expected return) \(Q_{\phi_{1}},Q_{\phi_{2}}:|S|\times|A|\rightarrow\mathbb{R}\) and uses the smaller of the two values (i.e., \(\min\left(Q_{\phi_{1}},Q_{\phi_{2}}\right)\)) to form the target for TD-error minimization. Such design prevents overestimation of the \(Q\)-value; 2) _Policy smoothing_, which adds noise when calculating the \(Q\)-value for the next action to effectively prevent overfitting; and 3) _Delayed update_, which updates \(\mu^{\text{RL}}\) less frequently than \(Q_{\phi_{1}},Q_{\phi_{2}}\) to benefit from a better \(Q\)-value landscape when updating the actor. TD3 also maintains a set of _target networks_ storing old parameters of the actor and critics that are soft-updated with slow exponential moving average updates from the current, active network. In this paper, we adapt this algorithm to fit the decision transformer architecture so that it can be used as an auxiliary objective in an online finetuning process.

## 3 Method

This section is organized as follows: we will first provide intuition why RL gradients aid online finetuning of decision transformers (Sec. 3.1), and present our method of adding TD3 gradients (Sec. 3.2). To further justify our intuition, we provide a theoretical analysis on how ODT fails to improve during online finetuning when pre-trained with low-reward data (Sec. 3.3).

### Why RL Gradients?

In order to understand why RL gradients aid online finetuning of decision transformers, let us consider an MDP which only has a single state \(s_{0}\), one step, a one dimensional action \(a\in[-1,1]\) (i.e., a bandit with continuous action space) and a simple reward function \(r(a)=(a+1)^{2}\) if \(a\leq 0\) and \(r(a)=1-2a\) otherwise, as illustrated in Fig. 1. In this case, a trajectory can be represented effectively by a scalar, which is the action. If the offline dataset for pretraining is of low quality, i.e., all actions in the dataset are either close to \(-1\) or \(1\), then the decision transformer will obviously not generate trajectories with high RTG after offline training. As a consequence, during online finetuning, the new rollout trajectory is very likely to be uninformative about how to reach \(\text{RTG}_{\text{eval}}\), since it is too far from \(\text{RTG}_{\text{eval}}\). Worse still, it cannot improve locally either, which requires \(\frac{\partial\text{RTG}}{\partial a}\). However, the decision transformer _yields exactly the inverse_, i.e., \(\frac{\partial a}{\partial\text{RTG}}\). Since the transformer is not invertible (and even if the transformer is invertible, often the ground truth \(\text{RTG}(a)\) itself is not), we cannoteasily estimate the former from the latter. Thus, the hope for policy improvement relies heavily on the generalization of RTG, i.e., policy yielded by high RTG\({}_{\text{eval}}\) indeed leads to better policy without any data as evidence, which is not the case with our constructed MDP and dataset.

In contrast, applying traditional RL for continuous action spaces to this setting, we either learn a value function \(Q(s_{0},a):\mathbb{R}\rightarrow\mathbb{R}\), which effectively gives us a direction of action improvement \(\frac{\partial Q(s_{0},a)}{\partial a_{i}}\) (e.g., SAC [25], DDPG [32], TD3 [21]), or an advantage \(A(s_{0},a)\) that highlights whether focusing on action \(a\) improves or worsens the policy (e.g., AWR [47], AWAC [40], IQL [28]). Either way provides a direction which suggests how to change the action locally in order to improve the (estimated) return. In our experiment illustrated in Fig. 2 (see Appendix F for details), we found that RL algorithms like DDPG [32] can easily solve the aforementioned MDP while ODT fails.

Thus, adding RL gradients aids the decision transformer to improve from given low RTG trajectories. While one may argue that the self-supervised training paradigm of ODT [74] can do the same by "prompting" the decision transformer to generate a high RTG trajectory, such paradigm is still unable to effectively improve the policy of the decision transformer pretrained on data with low RTGs. We provide a theoretical analysis for this in Sec. 3.3. In addition, we also explore the possibility of fixing this problem using other existing algorithms, such as JSRL [59] and slowly growing RTG (i.e., curriculum learning). However, we found that those algorithms cannot address this problem well. See Appendix G.8 for ablations.

Figure 1: An overview of our work, illustrating why ODT fails to improve with low-return offline data and RL gradients such as TD3 could help. The decision transformer yields gradient \(\frac{\partial a}{\partial\text{RTG}}\), but local policy improvement requires the opposite, i.e., \(\frac{\partial RTG}{\partial a}\). Therefore, the agent cannot recover if the current policy conditioning on high target RTG does not actually lead to high real RTG, which is very likely when the target RTG is too far from the pretrained policy and out-of-distribution. By adding a small coefficient for RL gradients, the agents can improve locally, which leads to better performance.

Figure 2: An illustration of a simple MDP, showing how RL can infer the direction for improvement, while online DT fails. Panels (a) and (b) show, DDPG and ODT+DDPG manage to maximize reward and find the correct optimal action quickly, while ODT fails to do so. Panel (c) shows how a DDPG/ODT+DDPG critic (from light blue/orange to dark blue/red) manages to fit ground truth reward (green curve). Panel (d) shows that the ODT policy (changing from light gray to dark) fails to discover the hidden reward peak near \(0\) between two low-reward areas (near \(-1\) and \(1\) respectively) contained in the offline data. Meanwhile, ODT+DDPG succeeds in finding the reward peak.

[MISSING_PAGE_EMPTY:5]

in Sec. 3.1, in this section, we will analyze more formally why such a paradigm is unable to improve the policy given offline data filled with low-RTG trajectories.

Our analysis is based on the performance bound proved by Brandfonbrener et al. [7]. Given a dataset drawn from an underlying policy \(\beta\) and given its RTG distribution \(P_{\beta}\) (either continuous or discrete), under assumptions (see Appendix E), we have the following _tight_ performance bound for a decision transformer with policy \(\pi^{\text{DT}}(a|s,\text{RTG}_{\text{eval}})\) conditioned on \(\text{RTG}_{\text{eval}}\):

\[\text{RTG}_{\text{eval}}-\mathbb{E}_{\tau=(s_{1},a_{1},\ldots,s_{H},\alpha_{ H})\sim\pi^{\text{DT}}(a|s,\text{RTG}_{\text{eval}})}[\text{RTG}_{\text{real}}] \leq\epsilon\left(\frac{1}{\alpha_{f}}+2\right)H^{2}.\] (4)

Here, \(\alpha_{f}=\inf_{s_{1}}P_{\beta}(\text{RTG}_{\text{real}}=\text{RTG}_{\text{ eval}}|s_{1})\) for every initial state \(s_{1}\), \(\epsilon>0\) is a constant, \(H\) is the horizon of the MDP.2 Based on this tight performance bound, we will show that **with high probability, \(\frac{1}{\alpha_{f}}\) grows _superlinearly_ with respect to \(\text{RTG}_{\text{eval}}\)**. If true, then the \(\text{RTG}_{\text{real}}\) term (i.e., the actual return from online rollouts) must decrease to fit into the tight bound, as \(\text{RTG}_{\text{eval}}\) grows.

Footnote 2: Eq. (4) is very informal. See Appendix E for a more rigorous description.

To show this, we take a two-step approach: First, we prove that the probability mass of the RTG distribution is concentrated around low RTGs, i.e., _event probability_\(\Pr_{\beta}\left(\text{RTG}-\mathbb{E}_{\beta}(\text{RTG}|s)\geq c|s\right)\) for \(c>0\) decreases superlinearly with respect to \(c\). For this, we apply the Chebyshev inequality, which yields a bound of \(O\left(\frac{1}{c^{2}}\right)\). However, without knowledge on \(P_{\beta}(\text{RTG}|s)\), the variance can be made arbitrarily large by high RTG outliers, hence making the bound meaningless.

Fortunately, we have knowledge about the RTG distribution \(P_{\beta}(\text{RTG}|s)\) from the collected data. If we refer to the maximum RTG in the dataset via \(\text{RTG}_{\beta\text{max}}\) and if we assume all rewards are non-negative, then all trajectory samples have an RTG in \([0,\text{RTG}_{\beta\text{max}}]\). Thus, with adequate prior distribution, we can state that with high probability \(1-\delta\), the probability mass is concentrated in the low RTG area. Based on this, we can prove the following lemma:

**Lemma 1**.: _(Informal) Assume rewards \(r(s,a)\) are bounded in \([0,R_{\text{max}}]\),3 and \(\text{RTG}_{\text{eval}}\geq\text{RTG}_{\beta\text{max}}\). Then with probability at least \(1-\delta\), we have the probability of event \(\Pr_{\beta}\) bounded as follows:_

Footnote 3: Note we use “max” instead of “\(\beta\)max” as this is a property of the environment and not the dataset.

\[\Pr_{\beta}\left(\text{RTG}_{\text{eval}}-V^{\beta}(s)\geq c|s\right)\leq O \left(\frac{R_{\text{max}}^{2}T^{2}}{c^{2}}\right),\] (5)

_where \(\delta\) depends on the number of trajectories in the dataset and prior distribution (see Appendix E for a concrete example and a more accurate bound). \(V^{\beta}(s)\) is the value function of the underlying policy \(\beta(a|s)\) that generates the dataset, for which we have \(V^{\beta}(s)=\mathbb{E}_{\beta}(RTG|s)\)._

The second step uses the bound of probability mass \(\Pr_{\beta}(\text{RTG}\geq c|s)\) to derive the bound for \(\alpha_{f}\). For the discrete case where the possibly obtained RTGs are finite or countably infinite (note, state and action space can still be continuous), this is simple, as we have

\[P_{\beta}\left(\text{RTG}=V^{\beta}(s)+c|s\right)=\Pr_{\beta}\left(\text{RTG} =V^{\beta}(s)+c|s\right)\leq\Pr_{\beta}\left(\text{RTG}\geq V^{\beta}(s)+c|s \right).\] (6)

Thus \(\alpha_{f}=\inf_{s_{1}}P_{\beta}(\text{RTG}|s_{1})\) can be conveniently bounded by Lemma 1. For the continuous case, the proof is more involved as probability density \(P_{\beta}(\text{RTG}|s)\) can be very high on an extremely short interval of RTG, making the total probability mass arbitrarily small. However, assuming that \(P_{\beta}(\text{RTG}|s)\) is Lipschitz when \(\text{RTG}\geq\text{RTG}_{\beta\text{max}}\) (i.e., RTG area not covered by dataset), combined with the discrete distribution case, we can still get the following (see Appendix E for proof):

**Corollary 1**.: _(Informal) If the RTG distribution is discrete (i.e., number of possible different RTGs are at most countably infinite), then with probability at least- \(1-\delta\), \(\frac{1}{\alpha_{f}}\) grows on the order of \(\Omega(\text{RTG}_{\text{eval}}^{2})\) with respect to \(\text{RTG}_{\text{eval}}\). For continuous RTG distributions satisfying a Lipschitz continuous RTG density \(p_{\beta}\), \(\frac{1}{\alpha_{f}}\) grows on the order of \(\Omega(\text{RTG}_{\text{eval}}^{1.5})\)._

Here, \(\Omega(\cdot)\) refers to the big-Omega notation (asymptotic lower bound).

## 4 Experiments

In this section, we aim to address the following questions: **a)** Does our proposed solution for decision transformers indeed improve its ability to cope with low-reward pretraining data. **b)** Is improving what to predict, while still using supervised learning, the correct way to improve the finetuning ability of decision transformers? **c)** Does the transformer architecture, combined with RL gradients, work better than TD3+BC? **d)** Is it better to combine the use of RL and supervised learning, or better to simply abandon the supervised loss in online finetuning? **e)** How does online decision transformer with TD3 gradient perform compared to other offline RL algorithms? **f)** How much does TD3 improve over DDPG which was used in Fig. 2?

**Baselines.** In this section, we mainly compare to six baselines: the widely recognized state-of-the-art DT for online finetuning, Online Decision Transformer (**ODT**) [74]; **PDT**, a baseline improving over ODT by predicting future trajectory information instead of return-to-go; **TD3+BC**[20], a MLP offline RL baseline; **TD3**, an ablated version of our proposed solution where we use TD3 gradients only for decision transformer finetuning (but only use supervised learning of the actor for offline pretraining); **IQL**[28], one of the most popular offline RL algorithms that can be used for online finetuning; **DDPG**[32]+**ODT**, which is the same as our approach but with DDPG instead of TD3 gradients (for ablations using SAC [25], IQL [28], PPO [52], AWAC [40] and AWR [47], see Appendix C). Each of the baselines corresponds to one of the questions a), b), c), d), e) and f) above.

**Metrics.** We use the normalized average reward (same as D4RL's standard [19]) as the metric, where higher reward indicates better performance. If the final performance is similar, the algorithm with fewer online examples collected to reach that level of performance is better. We report the reward curve, which shows the change of the normalized reward's mean and standard deviation with \(5\) different seeds, with respect to the number of online examples collected. The maximum number of steps collected is capped at 500K (for mujoco) or 1M (for other environments). We also report evaluation results using the rliable [3] library in Fig. 7 of Appendix B.

**Experimental Setup.** We use the same architecture and hyperparameters such as learning rate (see Appendix F.2 for details) as ODT [74]. The architecture is a transformer with \(4\) layers and \(4\) heads in each layer. This translates to around \(13\)M parameters in total. For the critic, we use Multi-Layer Perceptrons (MLPs) with width \(256\) and two hidden layers and ReLU [1] activation function. Specially, for the random dataset, we collect trajectories until the total number of steps exceeds \(1000\) in every epoch, which differs from ODT, where only \(1\) trajectory per epoch is collected. This is because many random environments, such as hopper, have very short episodes when the agent does not perform well, which could lead to overfitting if only a single trajectory is collected per epoch. For fairness, we use this modified rollout for ODT in our experiments as well. Not doing so does not affect ODT results since it does generally not work well on random datasets, but will significantly increase the time to reach a certain number of online transitions. After rollout, we train the actor for \(300\) gradient steps and the critic for \(600\) steps following TD3's delayed update trick.

### Adroit Environments

**Environment and Dataset Setup.** We test on four difficult robotic manipulation tasks [49], which are the Pen, Hammer, Door and Relocate environment. For each environment, we test three different datasets: expert, cloned and human, which are generated by a finetuned RL policy, an imitation learning policy and human demonstration respectively. See Appendix F.1 for details.

**Results.** Fig. 3 shows the performance of each method on Adroit before and after online finetuning. TD3+BC fails on almost all tasks and often diverges with extremely large \(Q\)-value during online finetuning. ODT and PDT perform better but still fall short of the proposed method, TD3+ODT. Note, IQL, TD3 and TD3+ODT all perform decently well (with similar average reward as shown in Tab. 2 in Appendix B). However, we found that TD3 often fails during online finetuning, probably because the environments are complicated and TD3 struggles to recover from a poor policy generated during online exploration (i.e., it has a _catastrophic forgetting_ issue). To see whether there is a simple fix, in Appendix G.7, we ablate whether an action regularizer pushing towards a pretrain policy similar to TD3+BC helps, but find it to hinder performance increase in other environments. IQL is overall much more stable than TD3, but improves much less during online finetuning than TD3+ODT. ODT can achieve good performance when pretrained on expert data, but struggles with datasets of lower quality, which validates our motivation. DDPG+ODT starts out well in the online finetuning stage but fails quickly, probably because DDPG is less stable compared to TD3.

### Antmaze Environments

**Environment and Dataset Setup.** We further test on a harder version of the Maze2D environment in D4RL [19] where the pointmass is substituted by a robotic ant. We study six different variants, which are umaze, umaze-diverse, medium-play, medium-diverse, large-play and large-diverse.

**Results.** Fig. 4 lists the results of each method on umaze and medium maze before and after online finetuning (see Appendix C for reward curves and Appendix B for results summary on large antmaze). TD3+ODT works the best on umaze and medium maze, and significantly outperforms TD3. This shows that RL gradients alone are not enough for offline-to-online RL of the decision transformer. Though TD3+ODT does not work on large maze, we found that IQL+ODT works decently well. However, we choose TD3+ODT in this work because IQL+ODT does not work well on the random datasets. This is probably because IQL aims to address the Out-Of-Distribution (OOD) estimation problem [28], which makes it better at utilizing offline data but worse at online exploration. See Appendix C for a detailed discussion and results. DDPG+ODT works worse than TD3+ODT but much better than baselines except IQL.

### MuJoCo Environments

**Environment and Dataset Setup.** We further test on four widely recognized standard environments [58], which are the Hopper, Halfcheetah, Walker2d and Ant environment. For each environment, we study three different datasets: medium, medium-replay, and random. The first and second one contain trajectories of decent quality, while the last one is generated with a random agent.

Figure 4: Reward curves for each method in Antmaze environments. IQL works best on the large maze, while our proposed method works the best on the medium maze and umaze. DDPG+ODT works worse than our method and IQL but much better than the rest of the baselines, which again validates our motivation that adding RL gradients to ODT is helpful.

Figure 3: Results on Adroit [49] environments. The proposed method, TD3+ODT, improves upon baselines. Note that TD3, IQL, and TD3+ODT all perform decently at the beginning of online finetuning, but TD3 fails while TD3+ODT improves much more than IQL during online finetuning.

**Results.** Fig. 6 shows the results of each method on MuJoCo before and after online finetuning. We observe that autoregressive-based algorithms, such as ODT and PDT, fail to improve the policy on MuJoCo environments, especially from low-reward pretraining with random datasets. With RL gradients, TD3+BC and IQL can improve the policy during online finetuning, but less than a decision transformer (TD3 and TD3+ODT). In particular, we found IQL to struggle on most random datasets, which are well-solved by decision transformers with TD3 gradients. TD3+ODT still outperforms TD3 with an average final reward of \(88.51\) vs. \(84.23\). See Fig. 6 in Appendix B for reward curves.

**Ablations on \(\alpha\).** Fig. 5 (a) shows the result of using different \(\alpha\) (i.e., RL coefficients) on different environments. We observe an increase of \(\alpha\) to improve the online finetuning process. However, if \(\alpha\) is too large, the algorithm may get unstable.

**Ablations on evaluation context length \(T_{\text{eval}}\).** Fig. 5 (b) shows the result of using different \(T_{\text{eval}}\) on halfcheetah-medium-replay-v2 and hammer-cloned-v1. The result shows that \(T_{\text{eval}}\) needs to be balanced between more information for decision-making and potential training instability due to a longer context length. As shown in the halfcheetah-medium-replay-v2 result, \(T_{\text{eval}}\) too long or too short can both lead to performance drops. More ablations are available in Appendix G.

## 5 Related Work

**Online Finetuning of Decision Transformers.** While there are many works on generalizing decision transformers (e.g., predicting waypoints [5], goal, or encoded future information instead of return-to-go [22; 5; 57; 36]), improving the architecture [37; 16; 53; 65] or addressing the overly-optimistic [46] or trajectory stitching issue [63]), there is surprisingly little work beyond online decision transformers that deals with online finetuning of decision transformers. There is some loosely related literature: MADT [31] proposes to finetune pretrained decision transformers with PPO. PDT [64] also studies online finetuning with the same training paradigm as ODT [74]. QDT [66] uses an offline RL

\begin{table}
\begin{tabular}{c c c c c c c} \hline  & TD3+BC & IQL & ODT & PDT & TD3 & DDPG-ODT & TD3+ODT (5urs) \\ \hline HoM-v2 & 60.24(+4.4) & 47.42(-2.13) & **97.48(+48.90)** & 74.43(+22.21) & 89.89(+29.25) & 41.7(-13.18) & 89.07(+25.97) \\ HoM-v2 & **99.07(+33.33)** & 62.76(-7.63) & 83.29(+65.17) & 84.53(+82.23) & 93.72(+55.66) & 32.36(-9.9) & 95.65(+65.89) \\ HoR-v2 & 8.36(-0.35) & 20.42(+12.36) & 29.08(+26.92) & 35.95(+34.67) & 75.68(+73.69) & 25.12(+23.14) & **76.13(+74.15)** \\ HaM-v2 & 51.59(+27.3) & 37.12(-10.35) & 42.27(+19.23) & 93.35(+35.95) & 70.94(-25.95) & 55.69(+14.71) & **76.91(+35.35)** \\ HaM-v2 & 56.53(+13.07) & 49.97(+6.84) & 41.45(+26.76) & 31.47(+31.38) & 69.87(+45.09) & 53.71(+24.91) & **73.27(+43.98)** \\ HaR-v2 & 44.78(+31.12) & 47.85(+40.3) & 2.15(-0.09) & 0.74(+0.9) & **68.55(+66.3)** & 34.56(+32.31) & 59.35(+57.1) \\ WaM-v2 & 85.34(+34.9) & 65.55(-15.12) & 75.57(+18.47) & 63.37(+63.63) & 90.49(+24.74) & 2.01(+69.54) & **97.86(+27.08)** \\ WaM-v2 & 83.28(+0.00) & 95.90(+28.78) & 77.27(+14.56) & 54.04(+51.18) & **100.88(+85.24)** & 104.06(-5.99) & 10.06(+42.54) \\ WaR-v2 & 6.99(+5.86) & 10.67(+4.96) & 14.12(+9.82) & 15.47(+15.32) & **69.91(+66.31)** & 2.91(+4.27) & 57.86(+53.27) \\ An-M-v2 & 129.11(+7.11) & 10.36(+14.26) & 86.18(-0.51) & 52.08(+48.47) & 125.67(+37.55) & 10.81(+75.52) & **123.08(+41.22)** \\ An-M-v2 & 129.33(+41.03) & 113.16(+24.24) & 85.64(+4.49) & 36.92(+32.41) & **133.58(+51.77)** & 4.05(+87.7) & 10.32(+52.08) \\ An-R-v2 & 67.89(+33.47) & 12.28(+0.97) & 24.96(-6.44) & 14.88(+10.13) & 63.47(+32.20) & 4.93(-26.55) & **71.69(+40.31)** \\ \hline Average & 68.52(+14.6) & 55.9(+7.8) & 55.14(+15.85) & 41.97(+0.44) & 87.64(+44.95) & 22.87(-19.22) & **88.38(+46.59)** \\ \hline \end{tabular}
\end{table}
Table 1: Average reward for each method in MuJoCo environments before and after online finetuning. The best performance for each environment is highlighted in bold font, and any result \(>90\%\) of the best performance is underlined. To save space, the name of the environments and datasets are abbreviated as follows: for the environments Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant; for the datasets M=Medium, MR=Medium-Replay, R=Random. The format is “final(+increase after finetuning)”. The proposed solution performs well.

Figure 5: Panel (a) shows ablations on RL coefficient \(\alpha\). While higher \(\alpha\) aids exploration as shown in the halfcheetah-medium-replay-v2 case, it may sometimes introduce instability, which is shown in the hammer-human-v1 case. Panel (b) shows ablations on \(T_{\text{eval}}\). \(T_{\text{eval}}\) balances training stability and more information for decision-making.

algorithm to re-label returns-to-go for offline datasets. AFDT [76] and STG [75] use decision transformers offline to generate an auxiliary reward and aid the training of online RL algorithms. A few works study in-context learning [33; 34] and meta-learning [60; 30] of decision transformers, where improvements with evaluations on new tasks are made possible. However, none of the papers above focuses on addressing the general online finetuning issue of the decision transformer.

**Transformers as Backbone for RL.** Having witnessed the impressive success of transformers in Computer Vision (CV) [17] and Natural Language Processing (NLP) [11], numerous works also studied the impact of transformers in RL either as a model for the agent [45; 38] or as a world model [39; 50]. However, a large portion of state-of-the-art work in RL is still based on simple Multi-Layer Perceptrons (MLPs) [35; 28]. This is largely because transformers are significantly harder to train and require extra effort [45], making their ability to better memorize long trajectories [42] harder to realize compared to MLPs. Further, there are works on using transformers as feature extractors for a trajectory [37; 45] and works that leverage the common sense of transformer-based Large Language Model's for RL priors [10; 9; 70]. In contrast, our work focuses on improving the new "RL via Supervised learning" (RvS) [7; 18] paradigm, aiming to merge this paradigm with the benefits of classic RL training.

**Offline-to-Online RL.** Offline-to-online RL bridges the gap between offline RL, which heavily depends on the quality of existing data while struggling with out-of-distribution policies, and online RL, which requires many interactions and is of low data efficiency. Mainstream offline-to-online RL methods include teacher-student [51; 6; 59; 72] and out-of-distribution handling (regularization [21; 29; 62], avoidance [28; 23], ensembles [2; 15; 24]). There are also works on pessimistic Q-value initialization [69], confidence bounds [26], and a mixture of offline and online training [56; 73]. However, all the aforementioned works are based on Q-learning and don't consider decision transformers.

## 6 Conclusion

In this paper, we point out an under-explored problem in the Decision Transformer (DT) community, i.e., online finetuning. To address online finetuning with a decision transformer, we examine the current state-of-the-art, online decision transformer, and point out an issue with low-reward, sub-optimal pretraining. To address the issue, we propose to mix TD3 gradients with decision transformer training. This combination permits to achieve better results in multiple testbeds. Our work is a complement to the current DT literature, and calls out a new aspect of improving decision transformers.

**Limitations and Future Works.** While our work theoretically analyzes an ODT issue, the conclusion relies on several assumptions which we expect to remove in future work. Empirically, in this work we propose a simple solution orthogonal to existing efforts like architecture improvements and predicting future information rather than return-to-go. To explore other ideas that could further improve online finetuning of decision transformers, next steps include the study of other environments and other ways to incorporate RL gradients into decision transformers. Other possible avenues for future research include testing our solution on image-based environments, and decreasing the additional computational cost compared to ODT (an analysis for the current time cost is provided in Appendix H).

## Acknowledgments

This work was supported in part by NSF under Grants 2008387, 2045586, 2106825, MRI 1725729, NIFA Award 2020-67021-32799, the IBM-Illinois Discovery Accelerator Institute, the Toyota Research Institute, and the Jump ARCHES endowment through the Health Care Engineering Systems Center at Illinois and the OSF Foundation.

## References

* [1] Agarap, A. F. Deep learning using rectified linear units (relu). _arXiv preprint arXiv:1803.08375_, 2018.
* [2] Agarwal, R., Schuurmans, D., and Norouzi, M. An optimistic perspective on offline reinforcement learning. In _ICML_, 2020.
* [3] Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Deep reinforcement learning at the edge of the statistical precipice. In _NeurIPS_, 2021.
* [4] Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [5] Badrinath, A., Flet-Berliac, Y., Nie, A., and Brunskill, E. Waypoint transformer: Reinforcement learning via supervised learning with intermediate targets. In _NeurIPS_, 2023.
* [6] Bastani, O., Pu, Y., and Solar-Lezama, A. Verifiable reinforcement learning via policy extraction. In _NeurIPS_, 2018.
* [7] Brandfonbrener, D., Bietti, A., Buckman, J., Laroche, R., and Bruna, J. When does return-conditioned supervised learning work for offline reinforcement learning? In _NeurIPS_, 2022.
* [8] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., and Zaremba, W. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* [9] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. _arXiv preprint arXiv:2307.15818_, 2023.
* [10] Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., et al. Do as i can, not as i say: Grounding language in robotic affordances. In _CoRL_, 2023.
* [11] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In _NeurIPS_, 2020.
* [12] Chebotar, Y., Vuong, Q., Hausman, K., Xia, F., Lu, Y., Irpan, A., Kumar, A., Yu, T., Herzog, A., Pertsch, K., et al. Q-transformer: Scalable offline reinforcement learning via autoregressive q-functions. In _CoRL_, 2023.
* [13] Chen, C., Wang, X., Jin, Y., Dong, V. Y., Dong, L., Cao, J., Liu, Y., and Yan, R. Semi-offline reinforcement learning for optimized text generation. In _ICML_, 2023.
* [14] Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement learning via sequence modeling. In _NeurIPS_, 2021.
* [15] Chen, X., Wang, C., Zhou, Z., and Ross, K. Randomized ensembled double q-learning: Learning fast without a model. In _ICLR_, 2021.
* [16] David, S. B., Zimerman, I., Nachmani, E., and Wolf, L. Decision s4: Efficient sequence-based rl via state spaces layers. In _ICLR_, 2022.
* [17] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _ICLR_, 2021.
* [18] Emmons, S., Eysenbach, B., Kostrikov, I., and Levine, S. Rvs: What is essential for offline rl via supervised learning? In _ICLR_, 2022.
* [19] Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.

* [20] Fujimoto, S. and Gu, S. S. A minimalist approach to offline reinforcement learning. In _NeurIPS_, 2021.
* [21] Fujimoto, S., Hoof, H., and Meger, D. Addressing function approximation error in actor-critic methods. In _ICML_, 2018.
* [22] Furuta, H., Matsuo, Y., and Gu, S. S. Generalized decision transformer for offline hindsight information matching. In _ICLR_, 2022.
* [23] Garg, D., Hejna, J., Geist, M., and Ermon, S. Extreme q-learning: Maxent rl without entropy. In _ICLR_, 2023.
* [24] Ghasemipour, S. K. S., Schuurmans, D., and Gu, S. S. Emaq: Expected-max q-learning operator for simple yet effective offline and online rl. In _ICML_, 2021.
* [25] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _ICML_, 2018.
* [26] Hong, J., Kumar, A., and Levine, S. Confidence-conditioned value functions for offline reinforcement learning. In _ICLR_, 2023.
* [27] Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [28] Kostrikov, I., Nair, A., and Levine, S. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* [29] Kumar, A., Zhou, A., Tucker, G., and Levine, S. Conservative q-learning for offline reinforcement learning. In _NeurIPS_, 2020.
* [30] Lee, J., Xie, A., Pacchiano, A., Chandak, Y., Finn, C., Nachum, O., and Brunskill, E. In-context decision-making from supervised pretraining. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.
* [31] Lee, K.-H., Nachum, O., Yang, M. S., Lee, L., Freeman, D., Guadarrama, S., Fischer, I., Xu, W., Jang, E., Michalewski, H., et al. Multi-game decision transformers. In _NeurIPS_, 2022.
* [32] Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N. M. O., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuous control with deep reinforcement learning. In _ICLR_, 2016.
* [33] Lin, L., Bai, Y., and Mei, S. Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining. In _ICLR_, 2024.
* [34] Liu, H. and Abbeel, P. Emergent agntic transformer from chain of hindsight experience. In _ICML_, 2023.
* [35] Lyu, J., Ma, X., Li, X., and Lu, Z. Mildly conservative q-learning for offline reinforcement learning. In _NeurIPS_, 2022.
* [36] Ma, Y., Xiao, C., Liang, H., and Hao, J. Rethinking decision transformer via hierarchical reinforcement learning. _arXiv preprint arXiv:2311.00267_, 2023.
* [37] Mao, H., Zhao, R., Chen, H., Hao, J., Chen, Y., Li, D., Zhang, J., and Xiao, Z. Transformer in transformer as backbone for deep reinforcement learning. In _AAMAS_, 2024.
* [38] Meng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W., Wen, Y., Zhang, H., Wang, J., and Xu, B. Offline pre-trained multi-agent decision transformer: One big sequence model tackles all smac tasks. _arXiv preprint arXiv:2112.02845_, 2021.
* [39] Micheli, V., Alonso, E., and Fleuret, F. Transformers are sample efficient world models. In _ICLR_, 2023.
* [40] Nair, A., Dalal, M., Gupta, A., and Levine, S. Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* [41] Nakamoto, M., Zhai, Y., Singh, A., Mark, M. S., Ma, Y., Finn, C., Kumar, A., and Levine, S. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. In _NeurIPS_, 2023.

* [42] Ni, T., Ma, M., Eysenbach, B., and Bacon, P.-L. When do transformers shine in rl? decoupling memory from credit assignment. In _NeurIPS_, 2023.
* [43] Oh, J., Guo, Y., Singh, S., and Lee, H. Self-imitation learning. In _ICML_, 2018.
* [44] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In _NeurIPS_, 2022.
* [45] Parisotto, E., Song, F., Rae, J., Pascanu, R., Gulcehre, C., Jayakumar, S., Jaderberg, M., Kaufman, R. L., Clark, A., Noury, S., et al. Stabilizing transformers for reinforcement learning. In _ICML_, 2020.
* [46] Paster, K., McIlraith, S., and Ba, J. You can't count on luck: Why decision transformers and rvs fail in stochastic environments. In _NeurIPS_, 2022.
* [47] Peng, X. B., Kumar, A., Zhang, G., and Levine, S. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [48] Pinto, A. S., Kolesnikov, A., Shi, Y., Beyer, L., and Zhai, X. Tuning computer vision models with task rewards. In _ICML_, 2023.
* [49] Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., and Levine, S. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations. In _RSS_, 2018.
* [50] Robine, J., Hoftmann, M., Uelwer, T., and Harmeling, S. Transformer-based world models are happy with 100k interactions. In _ICLR_, 2023.
* [51] Schmitt, S., Hudson, J. J., Zidek, A., Osindero, S., Doersch, C., Czarnecki, W. M., Leibo, J. Z., Kuttler, H., Zisserman, A., Simonyan, K., et al. Kickstarting deep reinforcement learning. _arXiv preprint arXiv:1803.03835_, 2018.
* [52] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [53] Shang, J., Kahatapitiya, K., Li, X., and Ryoo, M. S. Starformer: Transformer with state-action-reward representations for visual reinforcement learning. In _ECCV_, 2022.
* [54] Sharif, A. and Marijan, D. Evaluating the robustness of deep reinforcement learning for autonomous and adversarial policies in a multi-agent urban driving environment. In _QRS_, 2021.
* [55] Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., Lillicrap, T. P., Simonyan, K., and Hassabis, D. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. _Science_, 2018.
* [56] Song, Y., Zhou, Y., Sekhari, A., Bagnell, J. A., Krishnamurthy, A., and Sun, W. Hybrid rl: Using both offline and online data can make rl efficient. In _ICLR_, 2023.
* [57] Sudhakaran, S. and Risi, S. Skill decision transformer. _Foundation Models for Decision Making Workshop at NeurIPS_, 2022.
* [58] Todorov, E., Erez, T., and Tassa, Y. Mujoco: A physics engine for model-based control. In _IROS_, 2012.
* [59] Uchendu, I., Xiao, T., Lu, Y., Zhu, B., Yan, M., Simon, J., Bennice, M., Fu, C., Ma, C., Jiao, J., et al. Jump-start reinforcement learning. In _ICML_, 2023.
* [60] Wang, Z., Wang, H., and Qi, Y. J. T3gdt: Three-tier tokens to guide decision transformer for offline meta reinforcement learning. In _Robot Learning Workshop: Pretraining, Fine-Tuning, and Generalization with Large Scale Models in NeurIPS_, 2023.

* [61] Wolczyk, M., Cupial, B., Ostaszewski, M., Bordiewicz, M., Zajac, M., Pascanu, R., Kucinski, L., and Milos, P. Fine-tuning reinforcement learning models is secretly a forgetting mitigation problem. In _ICML_, 2024.
* [62] Wu, J., Wu, H., Qiu, Z., Wang, J., and Long, M. Supported policy optimization for offline reinforcement learning. In _NeurIPS_, 2022.
* [63] Wu, Y.-H., Wang, X., and Hamaya, M. Elastic decision transformer. In _NeurIPS_, 2023.
* [64] Xie, Z., Lin, Z., Ye, D., Fu, Q., Wei, Y., and Li, S. Future-conditioned unsupervised pretraining for decision transformer. In _ICML_, 2023.
* [65] Xu, M., Lu, Y., Shen, Y., Zhang, S., Zhao, D., and Gan, C. Hyper-decision transformer for efficient online policy adaptation. In _ICLR_, 2023.
* [66] Yamagata, T., Khalil, A., and Santos-Rodriguez, R. Q-learning decision transformer: Leveraging dynamic programming for conditional sequence modelling in offline rl. In _ICML_, 2023.
* [67] Yan, K., Schwing, A., and Wang, Y.-X. Ceip: Combining explicit and implicit priors for reinforcement learning with demonstrations. In _NeurIPS_, 2022.
* [68] You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., Song, X., Demmel, J., Keutzer, K., and Hsieh, C.-J. Large batch optimization for deep learning: Training bert in 76 minutes. In _ICLR_, 2020.
* [69] Yu, Z. and Zhang, X. Actor-critic alignment for offline-to-online reinforcement learning. In _ICML_, 2023.
* [70] Yuan, H., Zhang, C., Wang, H., Xie, F., Cai, P., Dong, H., and Lu, Z. Plan4MC: Skill reinforcement learning and planning for open-world Minecraft tasks. In _Foundation Models for Decision Making Workshop at NeurIPS 2023_, 2023.
* [71] Yue, Y., Lu, R., Kang, B., Song, S., and Huang, G. Understanding, predicting and better resolving q-value divergence in offline-rl. _NeurIPS_, 2024.
* [72] Zhang, H., Xu, W., and Yu, H. Policy expansion for bridging offline-to-online reinforcement learning. In _ICLR_, 2023.
* [73] Zheng, H., Luo, X., Wei, P., Song, X., Li, D., and Jiang, J. Adaptive policy learning for offline-to-online reinforcement learning. In _AAAI_, 2023.
* [74] Zheng, Q., Zhang, A., and Grover, A. Online decision transformer. In _ICML_, 2022.
* [75] Zhou, B., Li, K., Jiang, J., and Lu, Z. Learning from visual observation via offline pretrained state-to-go transformer. In _NeurIPS_, 2023.
* [76] Zhu, D., Wang, Y., Schmidhuber, J., and Elhoseiny, M. Guiding online reinforcement learning with action-free offline pretraining. _arXiv preprint arXiv:2301.12876_, 2023.

## Appendix: Reinforcement Learning Gradients as Vitamin for Online Finetuning Decision Transformers

The Appendix is organized as follows. In Sec. A, we discuss the potential positive and negative social impact of the paper. Then, we summarize the performance shown in the main paper in Sec. B. After this, we will explain our choice of RL gradients in the paper in Sec. C, and why our critic serves as an average of policies generated by different context lengths in Sec. D. We then provide rigorous statements for the theroetical analysis appearing in the paper in Sec. E, and list the environment details and hyperparameters in Sec. F. We then present more experiment and ablation results in Sec. G. Finally, we list our computational resource usage and licenses of related assets in Sec. H and Sec. I respectively.

## Appendix A Broader Societal Impacts

Our work generally helps automation of decision-making by improving the use of online interaction data of a pretrained decision transformer agent. While this effort improves the efficiency of decision-makers and has the potential to boost a variety of real-life applications such as robotics and resource allocation, it may also cause several negative social impacts, such as potential job losses, human de-skilling (making humans less capable of making decisions without AI), and misuse of technology (e.g., military).

## Appendix B Performance Summary

In this section, we summarize the average reward achieve by each method on different environments and datasets, where the result for Adroit is shown in Tab. 2, and the result for Antmaze is shown in Tab. 3. As the summary table for MuJoCo is already presented in Sec. 4, we show the reward curves in Fig. 6. For a more rigorous evaluation, we also report other metrics including the median, InterQuartile Mean (IQM) and optimality gap using the friable [3] library. See Fig. 7 for details. Breakdown analysis for each environment can be downloaded by browsing to https://kaiyan289.github.io/assets/breakdown_reliable.rar.

## Appendix C Why Do We Choose TD3 to Provide RL Gradients?

In this section, we provide an ablation analysis on which RL gradient fits the decision transformer architecture best. Fig. 8 illustrates the result of using a pure RL gradient for online finetuning of a pretrained decision transformer (for those RL algorithms with stochastic policy, we adopt the same

\begin{table}
\begin{tabular}{c c c c c c c} \hline  & TD3+BC & IQL & DDT & PDT & TD3 & DDPG+ODT & TD3+ODT (ours) \\ \hline P-E-v1 & 47.88(+84.23) & **149.65(-3.63)** & 121.82(+5.48) & 25.07(+25.56) & 61.56(-69.74) & 2.75(-129.63) & 120.65(-11.91) \\ P-C-v1 & 3.75(-10.8) & 78.12(+25.01) & 22.88(-24.20) & 14.05(+21.15) & 58.04(-17.39) & -1.41(-81.87) & **133.77(+58.05)** \\ P-H-v1 & 26.77(+13.19) & 96.52(-72.79) & 27.55(-13.91) & 4.03(-03.8) & 38.58(-57.71) & 2.09(-25.26) & **107.1(+11.87)** \\ H-E-v1 & 3.11(-0.02) & 126.54(+13.28) & 123.07(+12.79) & 98.95(+89.49) & 93.99(-31.41) & -0.24(-127.05) & **129.6(-34.34)** \\ H-C-v1 & 0.33(+0.03) & 2.27(+0.58) & 0.84(+0.32) & 6.06(+0.66) & 0.07(-0.69) & 0.12(-0.83) & **126.39(+124.59)** \\ H-H-v1 & 0.17(-0.3) & 16.12(+14.18) & 0.97(-0.13) & 2.36(+27.45) & 0.03(-11.1) & -0.06(-10.2) & **116.83(+115.82)** \\ D-E-v1 & 0.34(-0.01) & 97.57(-7.92) & 90.52(+0.54) & 94.89(+59.44) & 76.92(-25.66) & 2.648(-78.72) & **101.31(+19.44)** \\ D-C-v1 & -0.36(-0.01) & 23.8(+21.66) & 5.45(+5.37) & 1.38(+1.54) & 0.17(-4.8) & -0.01(-4.45) & **58.28(+53.31)** \\ D-H-v1 & -0.33(-0.1) & 34.64(+0.95) & 10.61(+6.69) & 0.05(+0.22) & -0.14(-0.22) & 1.29(-13.23) & **65.24(+55.94)** \\ R-E-v1 & -1.37(-0.22) & **157.8(-2.81)** & 101.61(+21.1) & 6.57(+66.7) & 0.44(-0.667) & 0.26(-106.48) & 9.138(-16.19) \\ R-C-v1 & -0.3(+0.0) & **1.1(+0.97)** & 0.06(+0.08) & -0.03(+0.04) & -0.19(-0.29) & -0.12(-0.32) & 0.36(-0.26) \\ R-H-v1 & -0.08(+0.1) & **1.6(+1.5)** & 0.04(+0.05) & 0.04(+0.17) & -0.17(-0.28) & -0.1(-0.27) & 1.19(+0.99) \\ \hline Average & 6.6(-7.66) & 61.14(+10.49) & 38.73(+3.75) & 25.25(+24.87) & 31.85(-29.63) & 3.51(-52.96) & **87.84(+33.09)** \\ \hline \end{tabular}
\end{table}
Table 2: Average reward for each method in Adroit Environments before after online finetuning. The best result for each setting is marked in bold font and all results \(>90\%\) of the best performance are underlined. To save space, the name of the environments and datasets are abbreviated as follows: P=Pen, H=Hammer, D=Door, R=Relocate for environment, and E=Expert, C=cloned, H=Human for the dataset. It is apparent that while both IQL, TD3 and TD3+ODT perform decently well before online finetuning, our proposed solution significantly outperforms all baselines on the adroit testbed. DDPG+ODT starts out well in the online stage, but fails probably due to DDPG’s training instability compared to TD3.

architecture as ODT which outputs a squashed Gaussian distribution with trainable mean and standard deviation). It is apparent that TD3 [21] and SAC [25] are the RL algorithms that suit the decision transformer best. Fig. 9 further shows the performance comparison between a decision transformer with SAC+ODT mixed gradient and TD3+ODT mixed gradient (both with coefficient \(0.1\)). The result shows that TD3 is the better choice when paired with supervised learning.

Note, While PPO is generally closely related with transformers (e.g., Reinforcement Learning from Human Feedback (RLHF) [44]), and was used in some prior work for online finetuning of decision transformers _with a small, discrete action space_[38], in our experiments, we find PPO generally does not work with the decision transformer architecture. The main reason for this is the importance sampling issue: PPO has the following objective for an actor \(\pi_{\theta}\) parameterized by \(\theta\):

\[\max_{\theta}\min\left(\mathbb{E}_{(s,a)\sim\pi_{\theta_{\text{old}}}}\frac{ \pi_{\theta}(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}A^{\pi_{\theta_{\text{old}} }}(s,a),\text{clip}\left(\frac{\pi_{\theta}(a|s)}{\pi_{\theta_{\text{old}}}(a |s)},1-\epsilon,1+\epsilon\right)A^{\pi_{\theta_{\text{old}}}}(s,a)\right).\] (7)

Here, \(\pi_{\theta_{\text{old}}}\) is the policy at the beginning of the training for the current epoch. Normally, the denominator of the red part, \(\pi_{\theta_{\text{old}}}(a|s)\), would be reasonably large, as the data is sampled from that distribution. However, because of the offline nature caused by different RTGs and context lengths at rollout and training time, the denominator for the red part in Eq. (7) could be very small in training, which will lead to a very small loss if \(A^{\pi}_{\theta_{\text{old}}}(s,a)>0\). This introduces significant instability during the training process. Fig. 10 illustrates the instability and degrading performance of a PPO-finetuned decision transformer.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline  & TD3\(\pm\)BC & IQL & ODT & PDT & TD3 & DDPG+ODT & TD3+ODT (ours) \\ \hline U-v2 & 0.21(+0.07) & 95.99(+6.59) & 15.92(-38.08) & 99.49(+2.99) & 0.6(+0.0) & 95.62(-3.62) & **95.9(+83.59)** \\ UD-v2 & 0.53(+0.33) & 44.83(+18.17) & 0.0(-52.0) & 0.0(+0.0) & 25.55(+25.55) & 23.41(-14.59) & **80.0(+42.0)** \\ MP-v2 & 0.0(+0.0) & 91.59(+21.59) & 0.0(+0.0) & 0.0(+0.0) & 70.55(+70.55) & 25.66(-14.34) & **96.79(+42.79)** \\ MD-v2 & 0.01(+0.01) & 88.63(+23.43) & 0.0(+0.0) & 0.0(+0.0) & 0.24(+0.24) & 29.14(-16.86) & **96.31(+36.31)** \\ \hline LP-v2 & 0.0(+0.0) & **54.1(+9.6)** & 0.0(+0.0) & 0.0(+0.0) & 0.0(+0.0) & 0.0(+0.0) & 0.0(+0.0) \\ LD-v2 & 0.0(+0.0) & **76.6(+32.6)** & 0.0(+0.0) & 0.0(+0.0) & 0.0(+0.0) & 0.0(+0.0) & 0.0(+0.0) \\ \hline Average & 0.13(+0.07) & **73.83(+12.61)** & 2.65(-15.01) & 4.99(+4.99) & 19.39(+19.39) & 28.97(-0.36) & 62.11(+34.1) \\ Avg (U+M) & 0.19(+0.08) & 80.26(+8.36) & 3.98(-22.52) & 7.49(+7.49) & 29.08(+29.08) & 43.46(-0.54) & **93.17(+51.17)** \\ \hline \end{tabular}
\end{table}
Table 3: Average reward for each method in Antmaze Environments before and after online finetuning. The best result is marked in bold font and all results \(>90\%\) for the best performance are underlined. To save space, the name of the environments and datasets are abbreviated as follows: U=Umaze, UD=Umaze-Diverse, MP=Medium-Play, MD=Medium-Diverse, LP=Large-Play and LD=Large-Diverse. U+M=Umaze and Medium maze. Our method performs the best on umaze and medium maze, while IQL performs the best on large maze. Both methods are much better than the rest on average. TD3+BC diverges on antmaze in our experiments.

Figure 6: Results on MuJoCo [58] Environments. The TD3 gradient significantly improves the overall performance of the decision transformer; autoregressive algorithms, such as ODT and PDT, fails to improve policy in most cases (especially on random dataset), while TD3+BC and IQL’s improvement during finetuning is generally limited.

In contrast, RLHF, does not exhibit such a problem: it does not use different return-to-go and context length in evaluation and training. Thus RLHF does not encounter the problem described above.

Besides the RL gradients mentioned above, as IQL works well on large Antmazes, we also explore the possibility of using IQL as the RL gradient for decision transformer instead of TD3. We found that IQL gradients, when applied to the decision transformer, indeed lead to much better results on antmaze-large. However, IQL fails to improve the policy when the offline dataset subsumes very low reward trajectories, which does not conform with our motivation. This is probably because IQL, as an offline RL algorithm, aims to address out-of-distribution evaluation issue, which is a much more important source of improvement in exploration in the online case. Thus, we choose TD3 as the RL gradient applied to decision transformer finetuning in this work. Fig. 11 shows the result of adding TD3 gradient vs. adding IQL gradient on Antmaze-large-play-v2 and hopper-random-v2.

## Appendix D Why Our Critic Serves as an Average of Policies Generated by Different Context Lengths?

As we mentioned in Sec. 2, When updating a deterministic DT policy, the following loss is minimized:

\[\sum_{t=1}^{T_{\text{train}}}\left\|\mu^{\text{DT}}\left(s_{0:t},a_{0:t-1}, \text{RTG}_{0:t},\text{RTG}=\text{RTG}_{\text{real}},T=t\right)-a_{t}\right\|_ {2}^{2},\] (8)

where \(T_{\text{train}}\) is the training context length and \(\text{RTG}_{\text{real}}\) is the real return-to-go.

Figure 7: Our main final results re-evaluated using the lribalve library with \(10000\) bootstrap replications. The x-axes are normalized scores (optimality gap is \(\int_{0}^{100}\Pr(\text{reward}\leq x)dx\)). Our method indeed outperforms all baselines on Adroit, MuJoCo and antmaze (umaze and medium).

However, if we consider a particular action \(a_{t}\) in some trajectory \(\tau\) of the dataset, during training (both offline pretraining and online finetuning), the policy generated by the decision transformer fitting \(a_{t}\) will be

\[a_{t}=\mu^{\text{DT}}(s_{t-T:t},a_{t-T:t-1},\text{RTG}_{t-T:t},T\sim U^{\prime}(1,T_{\text{train}}),\text{RTG}=\text{RTG}_{\text{real}}),\] (9)

\(T\) is actually _sampled_ from a distribution \(U^{\prime}(1,T_{\text{train}})\) over integers between \(1\) and \(T_{\text{train}}\) inclusive; this distribution \(U^{\prime}\) is introduced by the randomized starting step of the sampled trajectory segments, and is _almost_ a uniform distribution on integers, except that a small asymmetry is created because the context length will be capped at the beginning of each trajectory. See Fig. 12 for an illustration.

Therefore, online decision transformers (and plain decision transformers) are actually trained to predict with every context length between \(1\) and \(T_{\text{train}}\). During the training process, the context length is randomly sampled according to \(U^{\prime}\), and a critic is trained to predict an "average value" for the policy generated with context length sampled from \(U^{\prime}\).

Figure 8: Performance comparison of different, pure RL gradients for online finetuning on standard D4RL benchmarks, with TD3 [21], SAC [25], AWAC [40], AWR [47] and PPO [52]. We also plot ODT’s performance as a reference. The result shows that generally, TD3 and SAC work the best, while PPO does not work at all.

Figure 9: Performance comparison between SAC+ODT and TD3+ODT on standard D4RL benchmarks. TD3+ODT significantly outperforms SAC+ODT.

## Appendix E Mathematical Proofs

In this section, we will state the theoretical analysis summarized in Sec. 3.3 more rigorously. We will first provide an explanation on how the decision transformer improves its policy during online finetuning, linking it to an existing RL method in Sec. E.1 and Sec. E.2. We will then bound its performance in Sec. E.3.

### Preliminaries

Advantage-Weighted Actor Critic (AWAC) [40] is an offline-to-online RL algorithm, where the replay buffer is filled with offline data during offline pretraining and then supplemented with online experience during online finetuning. AWAC uses standard \(Q\)-learning to train the critic \(Q:|S|\times|A|\rightarrow\mathbb{R}\), and update the actor using weighted behavior cloning, where the weight is exponentiated advantage (i.e., \(\exp\left(\frac{A(s,a)}{\lambda}\right)\) where \(\lambda>0\) is some constant).

### Connection between Decision Transformer and AWAC

We denote \(\beta\) as the underlying policy of the dataset, and \(P_{\beta}\) as the distribution over states, actions or returns induced by \(\beta\). Note such \(P_{\beta}\) can be either discrete or continuous. By prior work [7], for decision transformer policy \(\pi^{\text{DT}}\), we have the following formula holds for any return-to-go \(\text{RTG}\in\mathbb{R}\) of the future trajectory:

\[\pi^{\text{DT}}(a|s,\text{RTG})=P_{\beta}(a|s,\text{RTG})=\frac{P_{\beta}(a|s) P_{\beta}(\text{RTG}|s,a)}{P_{\beta}(\text{RTG}|s)}=\beta(a|s)\frac{P_{\beta}( \text{RTG}|s,a)}{P_{\beta}(\text{RTG}|s)}.\] (10)

Based on Eq. (10), we have the following lemma:

**Lemma 2**.: _For state-action pair \((s,a)\) in an MDP, \(\text{RTG}\in\mathbb{R}\), assume the distributions of return-to-go (RTG) \(P_{\beta}(\text{RTG}|s,a)\) and \(P_{\beta}(\text{RTG}|s)\) are Laplace distributions with scale \(\sigma\), then for any RTG large

Figure 11: Performance comparison between IQL+ODT and TD3+ODT. While IQL gradient is good at both large antmaze environments, it is much easier to fall into local minima in low-reward offline dataset such as Hopper-random-v2.

Figure 10: An illustration of the training instability and the corresponding performance of a PPO-finetuned decision transformer. The \(x\) axis is \(300\times\) the number of gradient steps.

enough (more rigorously, RTG \(\geq\max\{Q^{\beta}(s,a),V^{\beta}(s)\}\)), \(\pi(a|s,\text{RTG})\) is updated in the same way as AWAC.4_

Footnote 4: Decision transformers have a sequence of past RTGs as input, but the past sequence can be augmented into the state space to fit into such form.

Proof.: If return-to-go are Laplace distributions, then by the symmetric property of such distributions, the mean of RTG given \(s\) or \((s,a)\) would be the expected future return, which by definition are value functions for \(\beta\), i.e., \(V^{\beta}(s)\) for \(P_{\beta}(\text{RTG}|s)\) and \(Q^{\beta}(s,a)\) for \(P_{\beta}(\text{RTG}|s,a)\). See Fig. 13 as an illustration. As \(\text{RTG}_{\text{eval}}\geq\max\{Q^{\beta}(s,a),V^{\beta}(s)\}\), we have

\[\begin{split} P_{\beta}(\text{RTG}|s,a)&=p_{\beta }(\text{RTG}|s,a)=\frac{1}{2\sigma}\exp\left(-\frac{\text{RTG}-Q^{\beta}(s,a)} {\sigma}\right),\\ P_{\beta}(\text{RTG}|s)&=p_{\beta}(\text{RTG}|s, a)=\frac{1}{2\sigma}\exp\left(-\frac{\text{RTG}-V^{\beta}(s)}{\sigma}\right). \end{split}\] (11)

And thus we have \(\frac{P_{\beta}(\text{RTG}|s,a)}{P_{\beta}(\text{RTG}|s)}=\exp\left(\frac{Q^{ \beta}(s,a)-V^{\beta}(s)}{\sigma}\right)=\exp(\frac{A^{\beta}(s,a)}{\sigma})\), where \(A^{\beta}\) is the advantage function. 

While the Laplace distribution assumption in this lemma is strict and impractical for real-life applications, it gives us three crucial insights for a decision transformer:

* For any state \(s\) or state-action pair \((s,a)\), we have \(\mathbb{E}_{\beta}[\text{RTG}|s,a]=Q^{\beta}(s,a),\mathbb{E}_{\beta}[\text{ RTG}|s]=V^{\beta}(s)\), which is an important property of \(P_{\beta}\) on RTG;

Figure 12: **a)** illustrates the context length \(T_{2}\) during training; \(T_{\text{eval}}\) is the context length of \(a_{2}\) upon sampling and evaluation. It is easy to see that \(T_{2}\) is randomized during training due to the left endpoint of the sampled trajectory segment. **b)** shows the distribution \(U^{\prime}\) of \(T_{2}\); while \(T_{2}^{\prime}\) for step \(j\geq T_{\text{train}}\) is uniformly sampled between \(1\) and \(T_{\text{train}}\) because the start of the segment is uniformly sampled, \(T_{2}\) for step \(i<T_{\text{train}}\) will be capped at the start of the trajectory. Thus \(U^{\prime}\) is not exactly uniform.

Figure 13: An illustration of how decision transformer policy update at \(\text{RTG}_{\text{eval}}\) is related to AWAC. Though the assumption is strong to form the exact same formula, it shows the basic idea of how we link \(P^{\beta}(\text{RTG})\) to its distance to \(V^{\beta}(s)\) and \(Q^{\beta}(s,a)\) in this paper.

* For decision transformer, the ability to improve the policy by collecting rollouts with high RTG, similar to AWAC, is closely related to advantage. In the above lemma, for example, if the two Laplace distributions have different scales \(\sigma_{V},\sigma_{Q}\), we will have the ratio \(\frac{P_{\beta}(\text{RTG}|s,a)}{P_{\beta}(\text{RTG}|s)}\) being \(\exp\left(\frac{Q^{\beta}(s,a)}{\sigma_{Q}}-\frac{V^{\beta}(s)}{\sigma_{V}}\right)\); if the distributions are Gaussian, we will get similar results but with quadratic terms of \(Q^{\beta}\) and \(V^{\beta}\).
* Different from AWAC, the "policy improvement" of online decision transformers heavily relies on the global property of the return-to-go as \(\text{RTG}_{\text{eval}}\) moves further away from \(Q^{\beta}\) and \(V^{\beta}\). If the return-to-go is far away from the support of the data, we will have almost no data to evaluate \(P_{\beta}\), and its estimation can be very uncertain (let alone ratios). In this case, it is very unlikely for the decision transformer to collect rollouts with high \(\text{RTG}_{\text{true}}\) and get further improvement. This is partly supported by the corollary of Brandfonbrener et al. [7], where the optimal conditioning return they found on evaluation satisfies \(\text{RTG}_{\text{eval}}=V^{\beta}(s_{1})\) at the initial state \(s_{1}\). This is also supported by our single-state MDP experiment discussed in Sec. 3.1 and illustrated in Fig. 2.

Those important insights lead to the intuition that decision transformers, when finetuned online, lack the ability to improve "locally" from low \(\text{RTG}_{\text{true}}\) data, and encourages to study the scenario where \(P_{\beta}(\text{RTG}|s)\) and \(P_{\beta}(\text{RTG}|s,a)\) are small.

### Failure of ODT Policy Update with Low Quality Data

As mentioned in Sec. E.2, we study the performance of decision transformers in online finetuning when \(P_{\beta}(\text{RTG}|s)\) and \(P_{\beta}(\text{RTG}|s,a)\) is small. Specially, in this section, \(r(s,a)\) is not a reward function, but a reward **distribution** conditioned on \((s,a)\); \(r_{i}\sim r(s_{i},a_{i})\) is the reward obtained on \(i\)-th step. Such notation takes the noise of reward into consideration and forms a more general framework. Also, as the discrete or continuous property of \(\beta\) is important in this section, we will use \(\Pr_{\beta}\) to represent probability mass (for discrete distribution or cumulative distribution for continuous distribution) and \(p_{\beta}\) to represent probability density (for probability density function for continuous distribution).

By prior work [7], we have the following performance bound _tight_ up to a constant factor for decision transformer for every iteration of updates:

**Theorem E.1**.: _For any MDP with transition function \(p(\cdot|s,a)\) and reward **random variable**\(r(s,a)\), and any condition function \(f\), assume the following holds:_

* _Return coverage:_ \(P_{\beta}(g=f(s_{1})|s_{1})\geq\alpha_{f}\) _for any initial state_ \(s_{1}\)_;_
* _Near determinism:_ _for any state-action pair_ \((s,a)\)_,_ \(\exists\)__\(s^{\prime}\) _such that_ \(\Pr(s^{\prime}|s,a)\geq 1-\epsilon\)_, and_ \(\exists\)__\(r_{0}(s,a)\) _such that_ \(\Pr(r(s,a)=r_{0}(s,a))\geq 1-\epsilon\)_;_
* _Consistency of_ \(f\)_:_ \(f(s)=f(s^{\prime})+r\) _for all_ \(s\) _when transiting to next state_ \(s^{\prime}\)_._

_Then we have_

\[\mathbb{E}_{s_{1}\sim p_{\text{in}}}\left[f(s_{1})\right]-\mathbb{E}_{\tau=(s _{1},a_{1},\dots,s_{H},a_{H})\sim\pi^{0f}(\cdot|s,f(s))}\left[\sum_{i=1}^{H} \mathbb{E}_{r_{i}\sim r(s_{i},a_{i})}r_{i}\right]\leq\epsilon\left(\frac{1}{ \alpha_{f}}+2\right)H^{2},\] (12)

_where \(\alpha_{f}>0,\epsilon>0\) are constants, \(p_{\text{in}}\) is the initial state distribution, and \(H\) is the horizon of the MDP. \(\pi^{\text{DT}}\) is the learned policy by Eq. (10)._

Proof.: See Brandfonbrener et al. [7]. 

In our case, we define \(f(s)\) as follows:

**Definition E.2**.: \(f(s_{1})=\text{RTG}_{\text{eval}}\) for all initial states \(s_{1}\), \(f(s_{i+1})=f(s_{i})-r_{i}\) for the \((i+1)\)-th step following \(i\)-th step (\(i\in\{1,2,\dots,T-1\}\)).

Further, we enforce the third assumption in Thm. E.1 by including the cumulative reward so far in the state space (as described in the paper of Brandforbrener et al. [7]). Under such definition, we have a tight bound on the regret between our target \(\text{RTG}_{\text{eval}}\) and the true return-to-go \(\text{RTG}_{\text{true}}=\sum_{i=1}^{H}r_{i}\) by our learned policy at optimal, based on current replay buffer in online finetuning.

We will now prove that under certain assumptions, \(\frac{1}{\alpha_{f}}\) grows **superlinearly** with respect to RTG; as the bound is tight, the expected cumulative return term \(\mathbb{E}_{\tau=(s_{1},a_{1},\dots,s_{H},a_{H})\sim\beta}\left[\sum_{i=1}^{H} \mathbb{E}_{r_{i}\sim r(s_{i},a_{i})}r_{i}\right]\) will be decreasing to meet the bounds.

To do this, we start with the following assumptions:

**Assumption E.3**.: We assume the following statements to be true:

* (**Bounded reward**) We assume the reward is bounded in \([0,R_{\text{max}}]\) for any state-action pairs.
* (**High evaluation RTG**) \(\text{RTG}_{\text{eval}}\geq\text{RTG}_{\beta\text{max}}\), where \(\text{RTG}_{\beta\text{max}}\) is the largest \(\text{RTG}_{\text{true}}\) in the dataset of \(n\) trajectories generated by \(\beta\).
* (**Beta prior**) We assign the prior distribution of RTG generated by policy \(\beta\) to be a Beta distribution \(\text{Beta}(1,1)\) for the binomial likelihood of RTG falling on \([0,\text{RTG}_{\beta\text{max}}]\) or \([\text{RTG}_{\beta\text{max}},TR_{\text{max}}]\).

_Remark E.4_.: The Beta distribution can be changed to any reasonable distribution; we use Beta distribution only for a convenient derivation. Considering the fact that by common sense, trajectories with high return are very hard to obtain, we can further strengthen the conclusion by changing the prior distribution.

We then prove the following lemma:

**Lemma 3**.: _Under the Assumption E.3, given underlying policy \(\beta\) of the dataset, for any state \(s\) with value function \(V^{\beta}(s)\) and any state-action pair \((s,a)\) with Q-function \(Q^{\beta}(s,a)\), any \(c\geq 0\) and \(\text{RTG}_{\text{eval}}\in\mathbb{R}\), with probability \(1-\delta\), we have_

\[\begin{split}\Pr_{\beta}(\text{RTG}_{\text{eval}}-V^{\beta}(s) \geq c|s)&\leq\frac{(1-\epsilon)\text{RTG}_{\beta\text{max}}+ \epsilon R_{\text{max}}^{2}T^{2}-\left[V^{\beta}(s)\right]^{2}}{c^{2}},\\ \Pr_{\beta}(\text{RTG}_{\text{eval}}-Q^{\beta}(s,a)\geq c|s,a)& \leq\frac{(1-\epsilon)\text{RTG}_{\beta\text{max}}+\epsilon R_{ \text{max}}^{2}T^{2}-\left[Q^{\beta}(s,a)\right]^{2}}{c^{2}},\end{split}\] (13)

_where \(\delta=1-\text{CDF}_{\text{Beta}(n+1,1)}(\epsilon)\), and the CDF is the cumulative distribution function._

Proof.: With the beta prior assumption in Assumption E.3, we know that with \(n\) samples where \(\text{RTG}\leq\text{RTG}_{\beta\text{max}}\), we have the posterior distribution to be \(\text{Beta}(n+1,1)\), i.e., with probability \(1-\text{CDF}_{\text{Beta}(n+1,1)}(\epsilon)\), we have \(\Pr_{\beta}(\text{RTG}\geq\text{RTG}_{\beta\text{max}})\leq\epsilon\) for \(\epsilon>0\).

Thus, by Chebyshev inequality, we know

\[\begin{split}\Pr_{\beta}\left(\text{RTG}-V^{\beta}(s)\geq c|s \right)&\leq\frac{\mathbb{E}_{\beta}\left[\text{RTG}^{2}\right]- \mathbb{E}_{\beta}^{2}\left[\text{RTG}\right]}{c^{2}}\\ &\leq\frac{(1-\epsilon)\text{RTG}_{\beta\text{max}}+\epsilon R_{ \text{max}}^{2}T^{2}-\left[V^{\beta}(s)\right]^{2}}{c^{2}},\end{split}\] (14)

and a similar conclusion holds for \(\Pr_{\beta}\left(\text{RTG}-Q^{\beta}(s,a)\geq c|s,a\right)\). Thus, the probability decays superlinearly with respect to RTG. 

Given this lemma, it remains to connect the bound of \(\Pr_{\beta}(\text{RTG}\geq c_{0})\) to \(P_{\beta}(\text{RTG}=c_{0})\) on RTG, \(c_{0}\in\mathbb{R}\). For discrete distribution, the connection is straightforward: \(\Pr_{\beta}(\text{RTG}\geq c_{0})\geq\Pr_{\beta}(\text{RTG}=c_{0})=P_{\beta}( \text{RTG}=c_{0})\) for any condition \(s\) or \((s,a)\).

Thus, we immediately get the following corollary:

**Corollary 2**.: _Assume the reward is bounded in \([0,R_{\text{max}}]\) for any state and action, and the number of possible different return-to-go one can get is finite or countably infinite. Then for the \(f\) condition function defined in Def. E.2, with probability of at least \(1-\delta\), we have \(\alpha_{f}\leq\frac{(1-\epsilon)\text{RTG}_{\text{eval}}+R_{\text{max}}^{2}T^ {2}-[V^{\beta}(s)]^{2}}{(\text{RTG}-V^{\beta}(s))^{2}}\), i.e., \(\frac{1}{\alpha_{f}}\) grows in the order of \(\Omega(\text{RTG}_{\text{eval}}^{2})\)._

_Remark E.5_.: While the limitation on return-to-go seems strong theoretically, it is very easy to satisfy such assumption in practice because it has no requirement on the discreteness of state and action space. Such corollary can be applied on reward discretization with arbitrary precision (including implicit ones by float precision).

For continuous distribution, to bound \(p_{\beta}(\text{RTG}=c_{0})\) with \(\Pr_{\beta}(\text{RTG}\geq c_{0})\) on RTG, we would need to assume that "peak" does not exist (see Fig. 14 for illustration), i.e. there does not exist cases where \(p_{\beta}\) is large but \(\Pr_{\beta}\) is small. Thus, we made the following assumption:

**Assumption E.6**.: **(Lipschitzness on uncovered RTG distribution)**\(p_{\beta}(\text{RTG}|s)\) is \(K_{V}\)-Lipschitz where RTG is larger than \(\text{RTG}_{\beta\text{max}}\).

_Remark E.7_.: The assumption is reasonable because when RTG is larger than any of the \(\text{RTG}_{\text{true}}\) in the dataset, we have no data coverage for the performance of the underlying policy \(\beta\) under such RTG, and thus we can choose any inductive bias for \(\beta\).

_Remark E.8_.: Note the Lipschitzness of \(p_{\beta}\) does not rely on the Lipschitzness of the reward function. For example, consider a single-state, single-step MDP where we have a uniformly random policy \(a\sim U(0,1)\) with reward \(r(a)=2-\frac{1}{a}\). The reward is clearly not Lipschitz on \(a\in(0,1)\), but the distribution of \(r\) is \(p_{r}(r_{0})=p_{a}(r^{-1}(r_{0}))\cdot\frac{\partial r^{-1}(r_{0})}{\partial r }=\frac{1}{(r-2)^{2}}\), which is Lipschitz on \((-\infty,1)\) as \(a\in(0,1)\).

With such assumption, we have the following corollary:

**Corollary 3**.: _Under assumption E.6, for the \(f\) condition function defined in Def. E.2, we have \(\alpha_{f}\leq\sqrt{2K_{V}}\Omega\left(\text{RTG}_{\text{eval}}^{-1.5}\right)\), i.e., \(\frac{1}{\alpha_{f}}\) grows in the order of \(\Omega(\text{RTG}_{\text{eval}}^{1.5})\) with probability of at least \(1-\delta\)._

Proof.: Under assumption E.6, by Lipschitzness, for any RTG where \(p_{\beta}(\text{RTG}|s)>p_{0}\), we have \(p_{\beta}(\text{RTG}+c|s)>p_{0}-K_{V}\cdot c\) for any \(c\in[0,\frac{p_{0}}{K_{V}}]\).

Thus, we know that if \(\exists\) RTG\({}_{0}>\) RTG\({}_{\beta\text{max}}\) such that \(p_{\beta}(\text{RTG}_{0}|s)>p_{0}\), then we have \(\Pr_{\beta}(\text{RTG}\geq\text{RTG}_{0}|s)>\frac{p_{0}^{2}}{2K_{V}}\) (See Fig. 14 for an illustration). By the contra-positive statement of the above conclusion, we know that

\[\Pr_{\beta}\left(\text{RTG}\geq\text{RTG}_{0}|s\right)\leq\frac{p_{0}^{2}}{2K _{V}}\Rightarrow\forall\text{RTG}\geq\text{RTG}_{0},\ p_{\beta}(\text{RTG}|s) \leq p_{0},\] (15)

and \(\text{RTG}_{\text{eval}}\) is applicable to the inequality above by the high evaluation RTG assumption in Assumption E.3. We then apply the proof lemma 3, but apply the inequality \(P\left(x-\mathbb{E}[x]\geq c\right)\leq\frac{\mathbb{E}(x-\mathbb{E}[x])^{3}}{ c^{3}}\) instead of Chebyshev inequality which leads to the conclusion.

## Appendix F Experimental Details

### Environment and Dataset Details

#### f.1.1 Single-State MDP

The single-state MDP studied in Sec. 3.1 motivates why RL gradients are useful for online finetuning. It has a single state, a single action \(a\in[-1,1]\), and a reward function \(r(a)=(a+1)^{2}\) if \(a\leq 0\) and \(r(a)=1-2a\) otherwise.

**Datasets.** The dataset has a size of \(128\), with \(100\) actions uniformly sampled in \((-1,0.95)\), and the remaining \(28\) actions uniformly sampled in \((0.5,1)\). The dataset is designed to conceal the reward peak in the middle. DDPG and ODT+DDPG successfully recognized the reward peak but ODT failed.

#### f.1.2 Adroit Environments

**Environments.** Adroit is a set of more difficult benchmark than Mujoco in D4RL, and is becoming increasingly popular in recent offline and offline-to-online RL works [28, 23]. We test four environments in adroit in our experiments: pen, hammer, door and relocate. Fig. 15 shows an illustration of the four environments.

1. **Pen.** Pen is a locomotion environment where the agent needs to control a robotic hand to manipulate a pen, such that its orientation matches the target. It has a \(24\)-dimensional action space, each of which controls a joint on the wrist or fingers. The state space is \(45\)-dimensional, which contains the pose of the palm, the angular position of the joints, and the pose of the target and current pen.
2. **Hammer.** Hammer is an environment where the agent needs to control a robotic hand to pick up a hammer and use it to drive a nail into a board. The action space is \(26\)-dimensional, each of which corresponds to a joint on the hand. The state space is \(46\)-dimensional, which describes the angular position of the fingers, the pose of the palm, and the status of hammer and nail.
3. **Door.** In the door environment, the agent needs to use a robotic hand to open a door by undoing the latch and swinging it. The environment has a \(28\)-dimensional action space, which are the absolute angular positions of the hand joints. It also has \(39\)-dimensional observation space which describes each joint, the pose of the palm, and the door with its latch.
4. **Relocate.** In the relocate environment, the agent needs to control a robotic hand to move a ball from its initial location towards a goal, both of which are randomized in the environment. The environment has a \(30\)-dimensional action space which describes the angular position of the joints on the hand, and a \(39\)-dimensional space which describes the hand as well as the ball and target.

Figure 14: An illustration of how Lipschitzness on the distribution of \(p_{\beta}(\text{RTG}|s)\) could link the bound between \(\Pr(\text{RTG}\geq V^{\beta}(s)+c|s)\) and \(p_{\beta}(\text{RTG}|s)\). Note we do not take the left-hand side probability mass of \(p_{0}\) into account because the triangle of probability mass could be truncated by \(V^{\beta}(s)\).

Figure 15: Illustration of Adroit environments used in Sec. 4 based on OpenAI Gym [8] and D4RL [19].

**Datasets.** For each of the four environments, we test our method across three different qualities of datasets: expert, cloned and human, all of which provided by the DAPG [49] repository. The expert dataset is generated by a fine-tuned RL policy; the cloned dataset is collected from an imitation policy on the demonstrations from the other two datasets; and the human dataset is collected from human demonstrations. Tab. 4 shows the size and average reward of each dataset.

#### f.1.3 Antmaze Environments

**Environments.** Antmaze is a more difficult version of Maze2D, where the agent controls a robotic ant instead of a point mass through the maze. It has a \(27\) dimensional-state space and a \(8\)-dimensional action space. We test our method on six variants of antmaze: Umaze, Umaze-Diverse, Medium-Play, Medium-Diverse, Large-Play and Large-Diverse, where "Umaze", "Medium" and "Large" describes the size of the maze (see Fig. 16 for an illustration), and the "Diverse" and "Play" describes the type of the dataset. More specifically, "Diverse" means that in the offline dataset, the starting point and the goal of the agent are randomly generated, while "Play" means that the goal is generated by a handcraft design. "Umaze" without suffix is the simplest environment where both the starting point and the goal are fixed.

**Datasets.** Similar to Adroit and MuJoCo, we test our method on datasets provided by D4RL. Tab. 5 shows the size and normalized reward of each dataset. Note, following IQL [28] and CQL [29], we conduct reward shaping: subtracting from all rewards in the dataset and environment the value \(1\) during training of both our method and baselines to provide denser reward signal for all antmaze environments. However, we still count original sparse reward when comparing the performance.

#### f.1.4 MuJoCo

**Environments.** We test our method on four widely used environments: Hopper, Halfcheetah, Walker2d and Ant. Fig. 17 shows an illustration of the four environments.

\begin{table}
\begin{tabular}{c c c} \hline Dataset & Size & Normalized Reward \\ \hline Pen-expert-v1 & \(499106\) & \(107.40\pm 55.65\) \\ Pen-cloned-v1 & \(499886\) & \(108.63\pm 122.43\) \\ Pen-human-v1 & \(4800\) & \(202.69\pm 154.48\) \\ Hammer-expert-v1 & \(999800\) & \(96.95\pm 50.65\) \\ Hammer-cloned-v1 & \(999872\) & \(8.11\pm 23.35\) \\ Hammer-human-v1 & \(10948\) & \(23.80\pm 33.36\) \\ Door-expert-v1 & \(999800\) & \(101.19\pm 16.31\) \\ Door-cloned-v1 & \(999939\) & \(12.29\pm 18.35\) \\ Door-human-v1 & \(6504\) & \(28.35\pm 13.88\) \\ Relocate-expert-v1 & \(999800\) & \(102.25\pm 19.83\) \\ Relocate-cloned-v1 & \(999724\) & \(28.99\pm 42.88\) \\ Relocate-human-v1 & \(9614\) & \(87.22\pm 21.28\) \\ \hline \end{tabular}
\end{table}
Table 4: The size and the average and standard deviation of the normalized reward of the Adroit datasets from D4RL [19] used in our experiments.

Figure 16: Illustration of mazes in antmaze and maze2D environment, where the red point is the goal and the green point is the current location of the agent.

1. **Hopper.** Hopper is a locomotion task on a 2D vertical plane, where the agent manipulates a single-legged robot to hop forward. Its state is \(11\)-dimensional, which describes the angle and velocity for the robot's joints. Its action is \(3\)-dimensional, which corresponds to the torques applied on the three joints for the current time step respectively.
2. **Halfcheetah.** Halfcheetah is also a 2D environment which requires the agent to control a cheetah-like robot to run forward. The states are \(17\)-dimensional, containing the coordinate and velocity of the joints The actions are \(6\)-dimensional, which control the torques on the joints of the robot.
3. **Ant.** In Ant, the agent controls a four-legged \(8\)-DoF robotic ant to walk in a 3D environment and tries to move forward. It has a \(111\)-dimensional state space describing the coordinates and velocities of the joints.
4. **Walker2d.** Walker2d is a 2D environment in which the agent needs to manipulate a \(8\)-DoF two-legged robot to walk forward under the agent's control. Its state space is \(27\)-dimensional.

**Datasets.** We test our method across three different qualities of datasets: medium, medium-replay and random. The medium dataset contains trajectories collected by an agent trained with RL, but early-stopped at medium-level performance. The medium-replay dataset is the collection of trajectories sampled in the training process of the agent mentioned above. The random dataset contains trajectories collected by an agent with random policy. Tab. 6 shows the size and normalized reward of each dataset.

#### f.1.5 Maze2D Environments

**Environments.** Maze2D is another set of D4RL environment, where the agent needs to control a point mass to navigate through a 2D maze and arrive at a fixed goal. It has a \(4\)-dimensional state space describing its coordinate and velocity, and a \(2\)-dimensional action describing its acceleration. The reward is determined by its current distance to the goal. We test our method on four variants of maze: Open, Umaze, Medium and Large with increasing difficulty. The map of each maze is illustrated in Fig. 16. Maze2D environment is tested in Sec. G.2.

**Datasets.** We again test our method on datasets provided by D4RL. Tab. 7 shows the size and normalized reward of each dataset.

Figure 17: Illustration of MuJoCo environments used in Sec. 4 based on OpenAI Gym [8] and D4RL [19].

\begin{table}
\begin{tabular}{c c c} \hline Dataset & Size & Normalized Reward \\ \hline Antmaze-Umaze-v2 & \(998573\) & \(86.14\pm 34.55\) \\ Antmaze-Umaze-Diverse-v2 & \(999000\) & \(3.48\pm 18.32\) \\ Antmaze-Medium-Play-v2 & \(999000\) & \(90.85\pm 28.83\) \\ Antmaze-Medium-Diverse-v2 & \(999000\) & \(66.29\pm 47.27\) \\ Antmaze-Large-Play-v2 & \(999000\) & \(92.73\pm 25.97\) \\ Antmaze-Large-Diverse-v2 & \(999000\) & \(86.17\pm 34.52\) \\ \hline \end{tabular}
\end{table}
Table 5: The size and the average and standard deviation of the normalized reward of the Antmaze datasets from D4RL [19] used in our experiments.

### Hyperparameters

#### f.2.1 Single-State MDP

For all networks, we use a simple MDP with two hidden layers of width \(128\), and ReLU [1] as activation function. We add a Tanh activation function to limit the output for ODT and the actor of DDPG to \([-1,1]\). For both methods, we use Adam [27] as the optimizer, and the learning rate is set to \(10^{-3}\). We pretrain \(5\) epochs on offline data (20 gradient steps) and \(16\) epochs for online finetuning, with a batch size of \(32\) for gradient update and collect \(64\) new rollout states for each epoch (thus we train \(2n+4\) steps for the \(n\)-th online finetuning epoch). RTGeval is set at \(1\), which serves as the (constant) input for ODT rollout and DDPG actor. Both DDPG and ODT uses deterministic actor with an exploration noise uniform in \([-0.01,0.01]\) during online rollouts.

#### f.2.2 Other Experiments

Tab. 8 summarizes hyperparameters that are common across all environments, and Tab. 9 summarizes hyperparameters that are different across environments. For environments that exist in ODT [74], we follow the hyperparameters from ODT medium environments. We did not use positional embedding as suggested by ODT [74]. Specially, for antmaze, we remove most (all but \(10\)) 1-step trajectories, because the size of the replay buffer for decision transformers is controlled by the number of trajectories, and antmaze dataset contains a large number of 1-step trajectories due to its data generation mechanism (immediately terminate an episode when the agent is close to the goal, but do not reset the agent location). Also, we add Layernorm [4] after each hidden layer of the critic for Adroit, Maze and Antmaze environments, according to Yue et al. [71]'s advice. We found that such practice stabilizes the training process (see Sec. G.5 for ablation).

For ODT and TD3 baseline, we use the same code as our TD3+ODT, while setting coefficients for RL and supervised gradients accordingly. For PDT baseline, we use the default hyperparameter in PDT paper, and pretrain PDT for 40K steps for all experiments. For TD3+BC and IQL, we use the default hyperparameter in their codebase, and pretrain them for 1M steps for all experiments (remaining the same as that in the codebase).

\begin{table}
\begin{tabular}{c c c} \hline Dataset & Size & Normalized Reward \\ \hline Hopper-medium-v2 & \(999906\) & \(44.32\pm 12.27\) \\ Hopper-medium-replay-v2 & \(402000\) & \(14.98\pm 16.32\) \\ Hopper-random-v2 & \(999996\) & \(1.19\pm 1.16\) \\ HalfCheetah-medium-v2 & \(1000000\) & \(40.68\pm 5.12\) \\ HalfCheetah-medium-replay-v2 & \(202000\) & \(27.17\pm 15.79\) \\ HalfCheetah-random-v2 & \(1000000\) & \(0.07\pm 2.90\) \\ Walker2d-medium-v2 & \(999995\) & \(62.09\pm 23.83\) \\ Walker2d-medium-replay-v2 & \(302000\) & \(14.84\pm 19.48\) \\ Walker2d-medium-random-v2 & \(999997\) & \(0.01\pm 0.09\) \\ Ant-medium-v2 & \(999946\) & \(80.30\pm 35.82\) \\ Ant-medium-replay-v2 & \(302000\) & \(30.95\pm 31.66\) \\ Ant-medium-random-v2 & \(999930\) & \(6.36\pm 10.07\) \\ \hline \end{tabular}
\end{table}
Table 6: The size and the average and standard deviation of the normalized reward of the MuJoCo datasets from D4RL [19] used in our experiments.

\begin{table}
\begin{tabular}{c c c} \hline Dataset & Size & Normalized Reward \\ \hline Maze2D-Open-v0 & \(999999\) & \(30.08\pm 50.17\) \\ Maze2D-Umaze-v1 & \(999869\) & \(-12.55\pm 9.82\) \\ Maze2D-Medium-v1 & \(1999733\) & \(-3.46\pm 3.95\) \\ Maze2D-Large-v1 & \(3999692\) & \(-1.71\pm 2.87\) \\ \hline \end{tabular}
\end{table}
Table 7: The size and the average and standard deviation of the normalized reward of the Maze2D datasets from D4RL [19] used in our experiments.

## Appendix G More Results

### Delayed Reward

Though we have tested MuJoCo environments in Sec. 4, it is worth noting that many offline RL algorithms have addressed the MuJoCo benchmark quite well [28; 23]. Thus, we also tested settings where RL struggles to obtain good performance to further analyze the performance of using RL gradients for decision transformers.

**Environment and Experimental Setup.** In this experiment, we use the same experiment and dataset as in Sec. 4 except for one major difference: the rewards are not given immediately after each step. Instead, the cumulative reward during a short period of \(M\) steps is given _only at the end of the period_, while the rewards observed by the agents within a period are all zero. We adopt such a setting from prior influential work [43], which creates a sparse-reward setting where RL algorithms struggle. We test \(M=5\) in this experiment.

**Results.** We use the same baselines as that in Sec. 4; Tab. 10 summarizes the performance of each method. Generally, DT with TD3 gradient still works very well, much better than ODT. While

\begin{table}
\begin{tabular}{c c} \hline Hyperparam & Value \\ \hline \# dim of embedding dimensions & 512 \\ \# of attention heads per layer & 4 \\ \# of transformer layers & 4 \\ Dropout & 0.1 \\ Actor Optimizer & LAMB [68] \\ \# steps collected per epoch & \(\geq 1000\) (random dataset), 1 trajectory (others) \\ Actor activation function & ReLU \\ Scheduler & \(10^{4}\) steps, linear warmup \\ \hline Critic layer & 2 \\ Critic width & 256 \\ Critic activation function & ReLU \\ \hline Batch size & 256 \\ \# actor update per epoch & 300 \\ Online exploration noise & 0.1 \\ TD3 policy noise & 0.2 \\ TD3 noise clip & 0.5 \\ TD3 target update ratio & 0.005 \\ \hline \end{tabular}
\end{table}
Table 8: The common hyperparameters across all environments used in our experiments.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline  & \(T_{\text{train}}\) & \(T_{\text{real}}\) & RTG\({}_{\text{real}}\) & RTG\({}_{\text{online}}\) & Pretrain \(\alpha\) & Online \(\alpha\) & \(\gamma\) & \(lr_{c}\) & \(lr_{a}\) & Weight decay & \# pretrain steps & Buffer size \\ \hline Hopper & 20 & 5 & 3600 & 7200 & 0 & 0.1 & 0.99 & \(10^{-3}\) & \(10^{-4}\) & 0.0005 & 5K & 1K \\ HalfCheetah & 20 & 5 & 6000 & 12000 & 0 & 0.1 & 0.99 & \(10^{-3}\) & \(10^{-4}\) & 10\({}^{-4}\) & 5K & 1K \\ Walker2d & 20 & 5 & 5000 & 10000 & 0 & 0.1 & 0.99 & \(10^{-3}\) & \(10^{-3}\) & 10\({}^{-3}\) & 10K & 1K \\ Ant & 20 & 1 & 6000 & 12000 & 0 & 0.1 & 0.99 & \(10^{-3}\) & \(10^{-4}\) & 5K & 1K \\ \hline Pen & 5 & 1 & 12000 & 12000 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ Hammer & 5 & 5 & 16000 & 16000 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ Door & 5 & 1 & 4000 & 4000 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ Relocate & 5 & 1 & 5000 & 5000 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ \hline MarzelD-Open & 1 & 1 & 120 & 120 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ -Umaze & 1 & 1 & 60 & 60 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ -Medium & 1 & 1 & 60 & 60 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ -Large & 5 & 1 & 60 & 60 & 0 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 5K \\ \hline Antname-Umaze & 5 & 1 & -100 & -100 & 0.1 & 0.99 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 40K & 2K \\ -Medium & 1 & 1 & -200 & -200 & 0.1 & 0.1 & 0.998 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 20K & 2K \\ -Large & 5 & 1 & -500 & -500 & 0.1 & 0.1 & 0.998 & \(0.0002\) & \(10^{-4}\) & \(10^{-4}\) & 200K & 2K \\ \hline \end{tabular}
\end{table}
Table 9: Environment-specific hyperparameters, where \(T_{\text{train}}\) and \(T_{\text{real}}\) stands for training and evaluation context length, RTG\({}_{\text{eval}}\) and RTG\({}_{\text{online}}\) represents RTG during evaluation and online rollout respectively, \(\alpha\) is the coefficient for RL gradient, \(\gamma\) is the discount factor, \(lr_{c}\) is the critic learning rate, and \(lr_{a}\) is the actor learning rate. Buffer size is counted in the number of trajectories. Note RTGs of antmaze have been modified according to our reward shaping.

TD3+BC works well in several scenarios, it struggles on random environments. See Fig. 18 for reward curves.

### Maze2D Environment

We test on navigation tasks in D4RL [19] where the agents need to control a pointmass through four different mazes: Open, Umaze, Medium and Large with different dataset respectively. Fig. 19 lists the performance of each method on Maze2D before and after online finetuning, and Tab. 11 summarizes the performance before and after online finetuning. The result shows that our method again significantly outperforms autoregressive-based algorithms such as ODT and PDT, which validates our motivation in Sec. 3.1. DDPG+ODT works similarly well as TD3+ODT in this environment with simple state and action space.

\begin{table}
\begin{tabular}{c c c c c c c} \hline  & TD3+BC & IQL & ODT & PDT & TD3 & TD3+ODT (ours) \\ \hline Ho-Mv-2 & 502.(-1.43) & 43.0(-2.12) & **92.94(+3.03)** & 83.56(-81.73) & 82.11(+1.579) & 28.29(+18.61) \\ Ho-Mv-2 & **99.85(+42.33)** & 79.6(-3.72) & 85.07(+67.6) & 82.22(+80.21) & 90.81(+48.79) & 98.55(+67.1) \\ Ho-Rv-2 & 8.8(+0.3) & 18.49(+10.62) & 30.58(+28.37) & 25.1(+23.86) & **75.55(+73.57)** & 52.38(+50.4) \\ Ha-Mv-2 & 50.45(+2.56) & 40.31(-6.88) & 42.07(+20.49) & 38.71(+38.65) & 65.92(+24.91) & **66.33(+26.1)** \\ Ha-Mv-2 & 53.59(+8.82) & 47.01(+3.46) & 39.45(+2.36) & 32.55(+32.76) & **57.42(+28.84)** & 49.95(+25.23) \\ Ha-Rv-2 & 42.11(+28.73) & 34.87(+28.34) & 2.16(-0.07) & -0.85(+0.96) & **52.24(+49.99)** & 48.79(+46.54) \\ Wa-Mv-2 & 84.73(+2.19) & 62.01(-16.59) & 76.21(+3.65) & 59.79(+59.52) & 86.42(+18.88) & **87.83(+19.57)** \\ Wa-Mv-2 & 6.11(-11.8) & 86.67(+19.33) & 79.56(+9.52) & 53.73(+37.17) & **76.38(+44.42)** & 91.93(+27.92) \\ Wa-Rv-2 & 6.78(+4.37) & 71.3(+1.22) & 7.93(+3.73) & 18.35(+18.2) & **57.43(+53.32)** & 56.72(+51.72) \\ An-Mv-2 & 114.21(+18.09) & 10.33(+6.64) & 80.29(-0.83) & 49.89(+45.92) & **118.21(+30.11)** & 110.67(+23.75) \\ An-Mv-2 & **122.11(+30.56)** & 10.55(+24.91) & 84.61(-2.13) & 42.58(+39.17) & 112.12(+39.38) & 116.67(+25.99) \\ An-Rv-2 & **77.41(+21.34)** & 10.77(+0.3) & 22.55(+38.2) & 17.37(+13.69) & 49.3(+17.87) & 53.06(+21.54) \\ \hline Average & 64.28(+12.17) & 53.07(+3.31) & 53.4(+15.22) & 41.92(+39.32) & **78.68(+35.15)** & 76.31(+33.51) \\ \hline \end{tabular}
\end{table}
Table 10: Average reward for each method in MuJoCo Environments before and after online finetuning with delayed rewards. To save space, the name of the environments and datasets are compressed, where Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant for the environment, and M=Medium, MR=Medium-Replay, R=Random for the dataset. The format is ”final(+increase)”. The best result for each setting is highlighted in bold font, and any result \(>90\%\) of the best performance is underlined.

Figure 18: Reward curves for MuJoCo environments with delayed reward.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline  & TD3+BC & IQL & ODT & PDT & TD3 & DDPG+ODT & TD3+ODT (ours) \\ \hline Open-v0 & 350.58(-0.22) & **576.06(+24.58)** & 574.4(+30.56) & 515.97(+485.99) & 430.61(-73.36) & 574.03(+96.02) & 574.32(+29.57) \\ Umaze-v2 & 90.91(+63.15) & 159.97(+114.77) & 43.19(+6.37) & 140.84(+153.47) & **162.04(+130.44)** & 139.45(+113.4) & 140.1(+107.76) \\ Medium-v2 & 96.91(+63.2) & 177.61(+97.31) & 26.11(+127.7) & 80.36(+81.49) & **184.55(+138.57)** & 133.03(+95.32) & 136.63(+91.69) \\ Large-v2 & 132.66(+38.21) & 214.02(+143.21) & 20.37(+17.25) & 38.74(+39.14) & **232.39(+187.27)** & 170.09(+151.44) & 189.71(+10.17) \\ \hline Average & 167.77(+41.49) & **281.92(+94.97)** & 166.02(+85.74) & 193.98(+90.02) & 252.53(+103.59) & 254.15(+114.05) & 260.19(+99.8) \\ \hline Average & 64.28(+12.17) & 53.07(+3.31) & 53.4(+15.22) & 41.92(+39.32) & **78.68(+35.15)** & 76.31(+33.51) \\ \hline \end{tabular}
\end{table}
Table 10: Average reward for each method in MuJoCo Environments before and after online finetuning with delayed rewards. To save space, the name of the environments and datasets are compressed, where Ho=Hopper, Ha=HalfCheetah, Wa=Walker2d, An=Ant for the environment, and M=Medium, MR=Medium-Replay, R=Random for the dataset. The format is ”final(+increase)”. The best result for each setting is highlighted in bold font, and any result \(>90\%\) of the best performance is underlined.

### Ablations on training context length \(T_{\text{train}}\)

Fig. 20 shows the result of using different context lengths on hammer-cloned-v1 environment (in this experiment, we use \(T_{\text{eval}}=1\) to demonstrate the effect of more different \(T_{\text{train}}\)). It is shown from the experiment that the selection of \(T_{\text{train}}\) needs to be balanced between more information taken into account and training stability; while longer \(T_{\text{train}}\) brings faster convergence when growing from \(1\) to \(5\), the reward curves with \(T_{\text{train}}\in\{10,20\}\) oscillates more than that with \(T_{\text{train}}=5\).

### Longer Training Process

In some environments, such as hopper-random-v2, walker2d-random-v2 and ant-random-v2, our proposed method still seems to be improving after 500K online samples. In Fig. 21, we show the finetuning result of our proposed method with more online transitions, which effectively shows that our method has greater potential in online finetuning when finetuned for more gradient steps.

### The Effect of Layernorm

As we have mentioned in Sec. F.2, as suggested by Yue et al. [71], we apply Layernorm [4] to critic networks for environment other than MuJoCo for better stability in training. In our experiment, we found that it greatly stabilizes the critic on complicated environments such as Adroit, but makes online policy improvement less efficient on easier MuJoCo environments. Fig. 22 shows the performance and critic MSE loss comparison on some environments with and without layernorm; it is clearly shown that layernorm helps stabilizes online finetuning in some cases such as pen-cloned-v1, but hinders performance increase on other environments such as halfcheetah-medium-v2.

### Recurrent Critic

As mentioned in Sec. 3.2, we use reflexive critics (i.e., critics that only take the current state nand action) to add RL gradients to decision transformer, and this creates an average effect among policies generated by different context lengths (see Sec. D in the appendix for detail). In this section, we explore recurrent critic by substituting the MLP critic using a LSTM, such that for a trajectory segment, the evaluation for the \(i\)-th action \(a_{i}\) is based on all state-action pairs \((s_{1},a_{1}),\dots,(s_{i-1},a_{i-1})\) and current state \(s_{i}\). As shown in Fig. 23, we found that recurrent critics are much less stable than

Figure 19: Reward curve for each method in Maze2D Environments. Again, autoregressive algorithms such as ODT and PDT does not perform well in this case.

Figure 20: The reward curves on hammer-cloned-v1 with different \(T_{\text{train}}\) and \(T_{\text{eval}}=1\). While longer \(T_{\text{train}}\) leads to faster convergence in this environment, runs with too long \(T_{\text{train}}\) are also unstable.

reflexive critics, and the instability increases as the training context length \(T_{\text{train}}\) grows; on the contrary, reflexive critic can well-handle the case where \(T_{\text{train}}\) is long.

### Regularizer for Pure TD3 Gradients

In the Adroit environment results discussed in Sec. 4, we found that the baseline of ODT finetuned using pure TD3 gradients struggles due to catastrophic forgetting. Inspired by Wolczyk et al. [61], we test whether adding a KL regularizer can fix the forgetting problem. Though our policy is deterministic, we can approximately interpret the policy as Gaussian with a very small variance. Thus, a KL regularizer can be simply added using \(c_{0}\cdot\|a-a_{\text{old}}\|^{2}\), where \(a\) is the current action and \(a_{\text{old}}\) is the action predicted by the pretrained policy. We set \(c_{0}=0.05\) and test this method on the Adroit cloned and expert dataset. We illustrate the result in Fig. 24. We find that the KL regularizer effectively addresses the issue on expert environments for both TD3 and TD3+ODT. But it can sometimes hinder the policy improvement of TD3+ODT with low return during online finetuning.

Figure 21: The reward curves of our method when finetuned for more steps (we only report curves until the black line in Fig. 6). It is clearly shown that our method has greater potential for improvement when finetuned for more steps.

Figure 22: The reward curve and critic MSE loss comparison between runs with and without layer-norm. Layernorm effectively stabilizes online finetuning in pen-cloned-v1, but hinders performance increase in halfcheetah-medium-v2.

Figure 23: The performance of reflexive critic vs. recurrent critic on hopper-medium-v2. It is clearly shown that recurrent critic is much harder to train, and its performance decreases as the training context length \(T_{\text{train}}\) grows.

### Other possible exploration improvement techniques

In Sec. 3.1, we state that ODT cannot explore the high-RTG region when pretrained with low-quality offline data, and we ran a simple experiment to verify this (Fig. 2). In this section, we test two potential alternatives for addressing the exploration problem: JSRL [59] and curriculum learning.

For JSRL, an expert policy is used for the first \(n\) steps in an episode, before ODT takes over. We set \(n=100\) (100% max episode length for adroit pen, 50% max episode length for other adroit environments) initially, and apply an exponential decay rate of \(0.99\) for every episode. We test two settings of JSRL: the expert policy being the offline pretrained policy, and the expert policy being _oracle_, i.e., an IQL policy trained on the Adroit expert dataset.

For curriculum learning, we use ODT with a gradually increasing target RTG with the current RTG for rollouts being \(\text{RTG}_{\text{eval}}-0.99^{N}(\text{RTG}_{\text{eval}}-\text{RTG}_{ \text{data}})\). Here, \(N\) is the number of episodes sampled in online stage, and \(\text{RTG}_{\text{data}}\) is the average RTG of the offline dataset.

Results are summarized in Fig. 24. We found that curriculum RTG does not work, probably because the task is too hard and cannot be improved by random exploration without gradient guidance. Further, even with oracle exploration, ODT is not guaranteed to succeed: it fails on the hammer environment where TD3+ODT succeeds, probably because of insufficient expert-level data and an inability to improve with random exploration.

### Ablations on the Architecture

In this section, we further examine the source of the performance gain of our method compared to TD3+BC. There are two key differences as stated in Sec. 3.2: The architecture and RL via Supervised (RvS) learning [18]. We can hence assess two more baselines: TD3+BC with our transformer architecture and TD3+RvS using TD3+BC's architecture. We present the ablation result on the Adroit cloned environment in Fig. 25. The result shows that only TD3+BC with our architecture works (albeit still worse than our method). We hypothesize that this is because a simple MLP is hard to model the complicated policy which takes both RTG and state into account.

To further assess if simply adding more layers to the MLP works, we conduct an ablation on the number of layers for TD3+RvS. The result is illustrated in Fig. 26. It shows that simply adding a few layers to the MLP does not aid performance. We speculate that it is probably the transformer architecture that helps modeling the state-and-RTG-conditioned policy.

Figure 24: The result of ODT with better exploration (only in cloned dataset) and TD3/TD3+ODT forgetting mitigation. The result shows that 1) ODT with curriculum RTG does not work; 2) even with exploration supported by an oracle, ODT can still fail on some environments such as hammer; 3) JSRL with the pretrained policy does not work for forgetting mitigation; 4) KL regularizer effectively addresses the issue on expert environments for both TD3 and TD3+ODT, but it can hinder the improvement of TD3+ODT with low return.

## Appendix H Computational Resources

We conduct all experiments with a single NVIDIA RTX 2080Ti GPU on an Ubuntu 18.04 server equipped with 72 Intel Xeon Gold 6254 CPUs @ 3.10GHz. Mujoco experiments takes about \(6-8\) hours, and the bottleneck is the gradient update; about \(50\%\) time is spent on backpropagation and update of parameters. Our critic appended to ODT only takes up about \(20\%\) time to train, in which \(90\%\) of the critic training time is spent on decision transformer inference to get action for "next state". For the actor, the training overhead of our method is negligible since it only contains an MLP critic inference to get the Q-value. Therefore, overall our method only uses \(20\%\) extra time compared to ODT for training, but attains much better results.

## Appendix I Dataset and Algorithm Licenses

Our code is developed upon multiple algorithm repositories and environment testbeds.

**Algorithm Repositories.** We implement our method on the basis of online decision transformer repository, which has a CC BY-NC 4.0 license. We also refer to IQL [28], PDT [64] and TD3+BC [20] repository when running baselines, all of which have MIT licenses.

**Environment Testbeds.** We utilize OpenAI gym [8], MuJoCo [58], and D4RL [19] as testbed, which have an MIT license, an Apache-2.0 license, and an Apache-2.0 license respectively.

Figure 26: Results of adding more layers to TD3+RvS. The result shows that simply adding MLP layers does not help TD3+RvS match the performance of ODT and TD3+ODT.

Figure 25: The result of ODT and TD3+BC ablations (TD3+RVS, DDPG+ODT, TD3+BC with our architecture and curriculum RTG for ODT) on Adroit environments. The result shows that only TD3+BC with our architecture works. However, it remains worse than our method.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Sec. 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Appendix E.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: See https://github.com/KaiYan289/RL_as_Vitamin_for_Online_Decision_Transformers for the code. The data is available in public D4RL dataset. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: See Sec. 4 and Appendix G. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research in the paper fully conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix A. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See Appendix I. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We attach our code in the supplementary material, and has published it on Github (https://github.com/KaiYan289/RL_as_Vitamin_for_Online_Decision_Transformers). Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.