Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union

 Zifu Wang\({}^{1}\) Maxim Berman\({}^{2}\) Amal Rannen-Triki\({}^{3}\) Philip H.S. Torr\({}^{4}\) Devis Tuia\({}^{5}\)

Tinne Tuytelaars\({}^{1}\) Luc Van Gool\({}^{1,6,7}\) Jiaqian Yu\({}^{8}\) Matthew B. Blaschko\({}^{1}\)

\({}^{1}\) ESAT-PSI, KU Leuven, Leuven, Belgium

\({}^{2}\) Google, Zurich, Switzerland

\({}^{3}\) Google DeepMind, London, United Kingdom

\({}^{4}\) University of Oxford, Oxford, United Kingdom

\({}^{5}\) EPFL, Lausanne, Switzerland

\({}^{6}\) ETH Zurich, Zurich, Switzerland

\({}^{7}\) INSAIT, Sofia, Bulgaria

\({}^{8}\) Samsung Research, Beijing, China

Correspondence to: zifu.wang@kuleuven.be

###### Abstract

Semantic segmentation datasets often exhibit two types of imbalance: _class imbalance_, where some classes appear more frequently than others and _size imbalance_, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards _majority classes_ (e.g. overall pixel-wise accuracy) and _large objects_ (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at https://github.com/zifuwanggg/JDTLosses.

## 1 Introduction

Every metric reflects a certain property of the result and choosing the right one is important to emphasize those that we care about. However, this choice may not be easy, as many metrics come with certain biases [34]. For semantic segmentation, the overall pixel-wise accuracy (Acc) is believed unsuitable due to its bias towards majority classes, especially given that semantic segmentation datasets typically have long-tailed class distributions [18]. The mean pixel-wise accuracy (mAcc) is introduced to counter this issue by averaging pixel-wise accuracy across all classes, and it eventually became the official evaluation metric in PASCAL VOC 2007 [16], which was one of the earliest challenges that showcased a segmentation leaderboard. However, mAcc does not account for false positives, leading to over-segmentation of the results. Consequently, in PASCAL VOC 2008 [17],the official metric was updated to the per-dataset mean intersection over union (mIoU\({}^{\text{D}}\)). Since then, PASCAL VOC [18] has consistently used mIoU\({}^{\text{D}}\), and this metric has been adopted by a multitude of segmentation datasets [38; 9; 39; 5; 11; 12; 15; 43].

In mIoU\({}^{\text{D}}\), true positives, false positives and false negatives are accumulated across all pixels over the whole dataset and therefore this metric suffers several notable shortcomings [10; 9]:

* It is biased towards large objects, which can pose a significant issue given that most semantic segmentation datasets inherently have a considerable size imbalance (see Figure 1). This bias is particularly concerning in safety-aware applications with critical small objects such as autonomous driving [4; 9; 39; 11; 43] and medical imaging [37; 22; 49; 3].
* It fails to capture valuable statistical information about the performance of methods on individual images or instances, which impedes a comprehensive comparison.
* It is challenging to link the results from user studies to those obtained with a per-dataset metric, since humans generally assess the quality of segmentation outputs at the image level.

In this paper, we advocate fine-grained mIoUs for semantic segmentation. In line with previous studies [10; 55; 28; 31], we calculate image-level metrics. When instance-level labels are available [9; 39; 59; 5], we propose to compute approximated instance-level scores. These fine-grained metrics diminish the bias toward large objects. Besides, they yield a wealth of statistical information, which not only allows for a more robust and comprehensive comparison, but also leads to the design of corresponding worst-case metrics, further proving their importance in safety-critical applications. Furthermore, we show that these fine-grained metrics facilitate detailed model and dataset auditing.

Despite the clear advantages of fine-grained mIoUs over mIoU\({}^{\text{D}}\), these metrics are seldom considered in recent studies within the segmentation community. We bridge this gap with a large-scale benchmark, where we train and evaluate 15 modern neural networks on 12 natural and aerial segmentation datasets, spanning a variety of scenarios including (i) street scenes [4; 9; 39; 11; 43], (ii) "thing" and "stuff" [18; 38; 59; 5], (iii) aerial scenes [12]. Our benchmark study emphasizes the peril of depending solely on a single metric and verifies that fine-grained mIoUs mitigate the bias towards large objects. Moreover, we conduct an in-depth analysis of neural network architectures and loss functions, leading to best practices for optimizing these metrics.

In sum, we believe no evaluation metric is perfect. Thus, we advocate a comprehensive evaluation of segmentation methods and encourage practitioners to report fine-grained metrics and corresponding worst-case metrics, as a complement to mIoU\({}^{\text{D}}\). We believe this is particularly crucial in safety-critical applications, where the bias towards large objects can precipitate catastrophic outcomes.

## 2 Fine-grained mIoUs

In this section, we review the traditional per-dataset mean intersection over union, followed by our proposed fine-grained image-level and instance-level metrics.

**mIoU\({}^{\text{D}}\).** Intersection over union (IoU) is defined as

\[\text{IoU}=\frac{\text{TP}}{\text{TP}+\text{FP}+\text{FN}},\] (1)

Figure 1: An illustration of size imbalance. The class Car is depicted in blue and the regions are highlighted within white dashed circles. (a) A car whose side mirror takes up around 2,000 pixels (from Cityscapes). (b) A car that takes up around 1,000 pixels (from Cityscapes). (c) A car whose front wheel takes up around 600,000 pixels (from Mapillary Vistas). (d) A car that takes up around 1,000 pixels (from Mapillary Vistas).

where TP, FP and FN represent true positives, false positives and false negatives, respectively. Per-dataset IoU computes these numbers by accumulating all pixels over the whole dataset. In particular, for a class \(c\), it is defined as

\[\text{IoU}_{c}^{\text{D}}=\frac{\sum_{i=1}^{I}\text{TP}_{i,c}}{\sum_{i=1}^{I}( \text{TP}_{i,c}+\text{FP}_{i,c}+\text{FN}_{i,c})},\] (2)

where \(I\) is the number of images in the dataset; \(\text{TP}_{i,c}\), \(\text{FP}_{i,c}\) and \(\text{FN}_{i,c}\) are the number of true-positive, false-positive and false-negative pixels for class \(c\) in image \(i\), respectively. The per-dataset mean IoU is then calculated as

\[\text{mIoU}^{\text{D}}=\frac{1}{C}\sum_{c=1}^{C}\text{IoU}_{c}^{\text{D}},\] (3)

where \(C\) denotes the number of classes for the dataset.

Compared to pixel-wise accuracy (Acc) and mean pixel-wise accuracy (mAcc), \(\text{mIoU}^{\text{D}}\) aims to mitigate class imbalance and to take FP into account. It is widely used in the evaluation of semantic segmentation models, but it has several shortcomings. Most notably, it is biased towards large objects in the dataset, due to dataset-level accumulations of TP, FP and FN. To address this issue, we can resort to fine-grained mIoUs at per-image (\(\text{mIoU}^{\text{I}}\), \(\text{mIoU}^{\text{C}}\)) and per-instance (\(\text{mIoU}^{\text{K}}\)) levels, as detailed below.

**mIoU.** Following [10; 55; 28; 31], we compute per-image scores. In particular, for each image \(i\) and class \(c\), a per-image-per-class score is calculated as

\[\text{IoU}_{i,c}=\frac{\text{TP}_{i,c}}{\text{TP}_{i,c}+\text{FP}_{i,c}+\text {FN}_{i,c}}.\] (4)

Then the per-image IoU for image \(i\) is defined as

\[\text{IoU}_{i}^{\text{I}}=\frac{\sum_{c=1}^{C}\mathbbm{1}\{\text{IoU}_{i,c} \neq\texttt{NULL}\}\text{IoU}_{i,c}}{\sum_{c=1}^{C}\mathbbm{1}\{\text{IoU}_{i, c}\neq\texttt{NULL}\}}\] (5)

where we only sum over \(\text{IoU}_{i,c}\) that is not NULL (discuss below). Averaging these per-image scores yields

\[\text{mIoU}^{\text{I}}=\frac{1}{I}\sum_{i=1}^{I}\text{IoU}_{i}^{\text{I}}.\] (6)

Using image-level metrics presents a challenge: not all classes appear in every image, leading to NULL values and potential ambiguities. In multi-class segmentation, we denote the per-image-per-class value as NULL if the class is missing from the ground truth, regardless of it appearing in the prediction or not. In contrast, for binary segmentation, the value is set to 0 instead of NULL when the prediction includes the foreground class, but the ground truth does not. More detail of the definition is in Appendix D.

**mIoU.** As the left panel of Figure 2 shows, due to the presence of NULL values, averaging these per-image-per-class scores by class first and then by image (\(\text{mIoU}^{\text{I}}\)) is different from averaging them by image first and then by class. A drawback of \(\text{mIoU}^{\text{I}}\) is that it is biased towards classes that appear in more images2. To address this bias, we first average these per-image-per-class scores by image and define the per-image IoU for class \(c\) as

Footnote 2: This is different from Acc that is biased towards classes that take up more pixels.

\[\text{IoU}_{c}^{\text{C}}=\frac{\sum_{i=1}^{I}\mathbbm{1}\{\text{IoU}_{i,c} \neq\texttt{NULL}\}\text{IoU}_{i,c}}{\sum_{i=1}^{I}\mathbbm{1}\{\text{IoU}_{i,c }\neq\texttt{NULL}\}}.\] (7)

Similar to \(\text{mIoU}^{\text{D}}\), we then average these per-class values to obtain

\[\text{mIoU}^{\text{C}}=\frac{1}{C}\sum_{c=1}^{C}\text{IoU}_{c}^{\text{C}}.\] (8)

**mIoU\({}^{\text{K}}\).** IoU is known for its scale-variance. However, this property only makes sense at an instance level. When instance-level annotations are available, for example, for "thing" classes in panoptic segmentation [27] ("thing" classes are with characteristic shapes and identifiable instances such as a truck; see a more detailed discussion of "thing" and "stuff" in Appendix B.1), we can compute a even more fine-grained metric.

Specifically, for an image \(i\) and a class \(c\), we have a binary mask \(H\times W\) from the prediction of a semantic segmentation network and an instance-level annotation \(H\times W\times K_{i,c}\) from panoptic segmentation, where \(K_{i,c}\) is the number instances of class \(c\) that appear in image \(i\). The key insight, as illustrated in the right panel of Figure 2, is to realize that TP and FN for each instance can be computed exactly, and we are only left with image-level FP. We propose to approximate instance-level FP by distributing image-level FP proportional to the size of each instance, with other variants discussed in Appendix E. Therefore, for class \(c\) in image \(i\), we compute the per-instance IoU as

\[\text{IoU}_{i,c,k}=\frac{\text{TP}_{i,c,k}}{\text{TP}_{i,c,k}+ \text{FN}_{i,c,k}+\frac{S_{i,c,k}}{\sum_{k=1}^{K_{i,c}}S_{i,c,k}}\text{FP}_{i, c}},\] (9)

such that \(\text{FP}_{i,c}\) is the total number of false-positive pixels of class \(c\) in image \(i\), and \(S_{i,c,k}=\text{TP}_{i,c,k}+\text{FN}_{i,c,k}\) is the size of instance \(k\). Having these per-instance IoUs, we can again calculate a per-class score as

\[\text{IoU}_{c}^{\text{K}}=\frac{\sum_{i=1}^{I}\sum_{k=1}^{K_{i,c} }\text{IoU}_{i,c,k}}{\sum_{i=1}^{I}K_{i,c}},\] (10)

With a slightly abuse of notation, we have

\[\text{mIoU}^{\text{K}}=\frac{1}{C}\sum_{c=1}^{C}\text{IoU}_{c}^{ \text{K}},\] (11)

such that for classes with instance-level labels, we calculate IoU\({}_{c}^{\text{K}}\) according to Eq. (10), and for classes without instance-level labels, such as "stuff" classes (classes that are amorphous and do not have identifiable instances such as sky, see Appendix B.1), we compute it as in Eq. (7).

To conclude, similar to mIoU\({}^{\text{D}}\), we average the scores on a per-class basis for mIoU\({}^{\text{C}}\) and mIoU\({}^{\text{K}}\), therefore mitigating _class imbalance_. Additionally, mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{C}}\) reduce _size imbalance_ from dataset-level to image-level, and mIoU\({}^{\text{K}}\) further computes the score at an instance level. On the other hand, scores of mIoU\({}^{\text{I}}\) are finally averaged on a per-image basis; therefore it will be biased towards classes that appear in more images. However, mIoU\({}^{\text{I}}\) leads to a single averaged score per image (while mIoU\({}^{\text{C}}\) has \(I\) scores per class), making per-image analysis (e.g. image-level histograms and worst-case images) feasible as we will present in section 5.1.

Figure 2: **Left:** mIoU\({}^{\text{I}}\) first averages per-image-per-class scores by class and then by image. mIoU\({}^{\text{C}}\) first averages them by image and then by class. mIoU\({}^{\text{I}}\) is biased towards classes that appear in more images, e.g. \(C_{1}\). **Right:** Given two ground-truth instances 1 (red) and 2 (blue), and the prediction (green), we can compute \(\text{TP}_{1},\text{TP}_{2},\text{FN}_{1},\text{FN}_{2}\). We propose to approximate \(\text{FP}_{1}\) and \(\text{FP}_{2}\) by distributing FP proportional to the size of each instance.

## 3 Worst-case Metrics

It is essential for safety-critical applications to evaluate a model's worst-case performance. Adopting fine-grained mIoUs allows us to compute worst-case metrics, where only images/instances that a model obtains the lowest scores are considered. In this section, we focus on mIoU\({}^{\text{C}}\) as an example. However, the same idea can be applied to both mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{K}}\).

For each class \(c\), we first sort IoU\({}_{i,c}\) by images such that IoU\({}_{1,c}\leq...\leq\text{IoU}_{I_{c},c}\), where \(I_{c}=\sum_{i=1}^{I}\mathbbm{1}\{\text{IoU}_{i,c}\neq\text{NULL}\}\). We then only compute the average of those scores that fall below the \(q\)-th quantile:

\[\text{IoU}_{c}^{\text{C}^{q}}=\frac{1}{\max(1,\lfloor I_{c}\times q \%\rfloor)}\sum_{i=1}^{\max(1,\lfloor I_{c}\times q\%\rfloor)}\text{IoU}_{i,c}.\] (12)

For the ease of comparison, we want to derive a single metric that considers different \(q\%\). Thus, we partition the range into 10 quantile thresholds \(\{10,20,\ldots,90,100\}\). We then average the scores corresponding to these thresholds:

\[\text{IoU}_{c}^{\text{C}^{q}}=\frac{1}{10}\sum_{q\in\{10,\ldots,1 00\}}\text{IoU}_{c}^{\text{C}^{q}}.\] (13)

Consequently, the scores with lower values will be given more weights to the final metric. Additionally, we consider IoU\({}_{c}^{\text{C}^{1}}\) and IoU\({}_{c}^{\text{C}^{5}}\) to evaluate a model's performance on extremely hard cases. Having these per-class scores, we can average them to obtain the mean metrics: mIoU\({}^{\text{C}^{5}}\), mIoU\({}^{\text{C}^{5}}\) and mIoU\({}^{\text{C}^{1}}\).

## 4 Advantages of Fine-grained mIoUs

In summary, we advocate these fine-grained mIoUs as a complement of mIoU\({}^{\text{D}}\) because they present several notable benefits:

* **Reduced bias towards large objects.** Unlike mIoU\({}^{\text{D}}\), fine-grained metrics compute TP, FP and FN at an image or an instance level, thus decreasing the bias towards large objects. To further illustrate this on real datasets, we can compute the ratio of the size of the largest object to that of the smallest object for each class, evaluated at both dataset (\(r_{c}^{\text{D}}\)) and image levels (\(r_{c}^{\text{I}}\)): \[r_{c}^{\text{D}}=\frac{\max_{i,k}S_{i,c,k}}{\min_{i,k}S_{i,c,k}},\quad r_{c}^ {\text{I}}=\max_{i}\frac{\max_{k}S_{i,c,k}}{\min_{k}S_{i,c,k}}.\] (14) These ratios can serve as an indicator of size imbalance at either the dataset or image level. We average \(\log\frac{r_{c}^{\text{D}}}{r_{c}^{\text{I}}}\) for "thing" and "stuff" classes separately, and note that \(r_{c}^{\text{I}}=1\) for all "stuff" classes. In Figure 5 (Appendix B.1), we present the numbers for Cityscapes, Mapillary Vistas, ADE20K, and COCO-Stuff. The size imbalance is considerably reduced at the image level, especially for "stuff" classes. As a result, fine-grained mIoUs can be less biased towards large objects.
* **A wealth of statistical information.** For example, fine-grained metrics enable to compute worst-case metrics at image and instance levels as discussed in the previous section. We can also plot a histogram for mIoU\({}^{\text{I}}\), from which we can extract various statistical information and identify worst-case images. These worst-case images can be instrumental in examining the specific scenarios in which a model underperforms, i.e. _model auditing_. Additionally, employing fine-grained scores facilitates statistical significance testing. Altogether, this yields a more robust and comprehensive comparison between different methods.
* **Dataset auditing.** Images that consistently yield a low image-level score across various models can be examined. These low scores could potentially be attributed to mislabeling. Furthermore, the presence of discrepancies between image- and object-level labels can result in an abnormal mIoU\({}^{\text{K}}\) values (NaN) because the denominator will become zero. We present the identified mislabels in Appendix B.3.

Experiments

We provide a large-scale benchmark study, where we train 15 modern neural networks from scratch and evaluate them on 12 datasets that cover various scenes. More details of 15 models and 12 datasets considered in our benchmark study can be found in Appendix A and Appendix B, respectively. The total amount of compute time takes around 1 NVIDIA A100 year.

In our benchmark, we emphasize the importance of training models from scratch (training details are in Appendix C) instead of relying on publicly available pretrained checkpoints. This choice is motivated by several factors:

* **Analysis.** By training models from scratch, we aim to analyze how specific training strategies can lead to high results on fine-grained metrics. This provides insights into the training process and enables a better understanding of the factors affecting performance.
* **Completeness.** While there are pretrained models available for popular datasets like Cityscapes and ADE20K, there is a scarcity of checkpoints for other datasets. Training models from scratch ensures a more comprehensive evaluation across different datasets, providing a more complete perspective on model performance.
* **Fairness.** Publicly available pretrained checkpoints are often trained with different hyperparameters and data preprocessing procedures. Training models from scratch ensures fairness in comparing different architectures and training strategies, as they all start from the same initialization and undergo the same training process.
* **Performance.** We leverage recent training techniques [33; 50], which have been shown to improve results. By training models from scratch with these techniques, we aim to achieve better performance compared to other publicly available checkpoints. We provide a comparison of our results with those of MMSegmentation [8] in Table 1 and Table 2.
* **Statistical significance.** Publicly available pretrained checkpoints often stem from a single run, which may lack statistical significance. By training models from scratch, we can perform multiple runs and obtain statistically significant results. This ensures a more reliable evaluation and avoids potential misinterpretations.

### Main Results

Complete results, including tabular entries, image-level histograms and worst-case images are in Appendix G. Besides, we rank 15 models by their averaged ranking across 10 datasets, excluding Nighttime Driving and Dark Zurich. As depicted in Figure 3, we contrast the rank of mIoU\({}^{\text{C}}\) with (a) mIoU\({}^{\text{D}}\), (b) mIoU\({}^{\text{I}}\), (c) mIoU\({}^{\text{C}^{\text{S}}}\), (d) mIoU\({}^{\text{C}^{\text{S}}}\). Furthermore, we count the number of times each worst-case image appears for each model, and present the count of three most common worst-case images for each dataset in Table 13 (Appendix G). The key findings are summarized below:

**No model achieves the best result across all metrics and datasets.** UPerNet-ConvNeXt usually obtains the highest score for most of the metrics, possibly due to the ImageNet-22K pretraining of ConvNeXt, while other backbones are pretrained on ImageNet-1K. However, UPerNet-MiTB4 and SegFormer-MiTB4 outperform UPerNet-ConvNeXt on many worst-case metrics. Since different metrics focus on a certain property of the result, it is important to have a comprehensive comparison using various metrics.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & Cityscapes & ADE20K & VOC & Context \\ \hline MMSegmentation & 80.97 & 45.47 & 78.62 & 53.20 \\ Ours & 80.91 \(\pm\) 0.17 & 46.32 \(\pm\) 0.33 & 81.07\(\pm\) 0.28 & 56.49 \(\pm\) 0.10 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparing the performance of DeepLabV3+ResNet101 with MMSegmentation. All results are mIoU\({}^{\text{D}}\). Red: the best in a column.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Model & DL3+R10 & DL3+MB2 & UPR-R101 & UPR-Conv & Seg-MiTB4 & PSP-R101 \\ \hline MMSegmentation & 45.47 & 34.02 & 43.82 & 48.71 & 48.46 & 44.39 \\ Ours & 46.32 \(\pm\) 0.33 & 36.85 \(\pm\) 0.34 & 45.89 \(\pm\) 0.11 & 51.08 \(\pm\) 0.42 & 50.23 \(\pm\) 0.26 & 45.69 \(\pm\) 0.33\(\%\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparing the performance on ADE20K with MMSegmentation. All results are mIoU\({}^{\text{D}}\). Red: the best in a column.

**Models do not overfit to a particular metric or a specific range of image-level values.** As shown in Figure 3, we witness only minor local rank discrepancies when comparing mIoU\({}^{\text{C}}\) with mIoU\({}^{\text{D}}\), and notably, no rank differences arise when comparing mIoU\({}^{\text{C}}\) with mIoU\({}^{\text{I}}\). In general, we observe a monotonic trend. This proves that models are not tailored to favor a particular metric. The rank differences between mIoU\({}^{\text{C}}\) and mIoU\({}^{\text{C}^{\text{q}}}\) are also minimal. This implies that the advancements on mIoU\({}^{\text{C}}\) are global and models do not overfit to a specific range of image-level values. However, we identify a larger rank drop of UNet-ResNet101 in mIoU\({}^{\text{C}^{\text{5}}}\), indicating that UNet may suffer from more challenging examples.

**The performance on fine-grained mIoUs is relatively low.** mIoU\({}^{\text{D}}\), mIoU\({}^{\text{C}}\) and mIoU\({}^{\text{K}}\) are all averaged on a per-class basis. Their values typically follow the ranking: mIoU\({}^{\text{D}}\geq\) mIoU\({}^{\text{C}}\geq\) mIoU\({}^{\text{K}}\). The more fine-grained a metric is, the more challenging the problem it presents, since it is less mercif to individual mistakes. Nevertheless, we observe that the results for mIoU\({}^{\text{C}}\) and mIoU\({}^{\text{K}}\) are usually very similar. This observation indicates that mIoU\({}^{\text{C}}\) can serve as a reliable proxy for instance-wise metrics when instance-level labels are unavailable, e.g. PASCAL VOC/Context, etc.

**Improvements in fine-grained mIoUs do not always extend to corresponding worst-case metrics.** The worst-case metrics generally fall substantially below their corresponding mean metrics. For example, on PASCAL Context, the value of mIoU\({}^{\text{C}}\) ranges from 46.00% to 60.17%, yet all models have 0% mIoU\({}^{\text{C}}\). The presence of extremely challenging examples poses obstacles to the application of neural networks in safety-critical tasks. It would be an interesting future research to explore ways of enhancing a model's performance on these hard examples.

**Certain images consistently yield low image-level IoU values across various models.** As presented in Table 13, most models agree on worst-case images across multiple datasets. Several challenging factors, such as difficult lighting conditions and the presence of small objects, can contribute to low IoU scores (as seen in Mapillary Vistas, depicted in Figures 19 - 21). Besides, mislabeling in the dataset may also cause models to produce an image-level IoU of 0 (see Appendix B.3).

Figure 3: Contrasting the rank of mIoU\({}^{\text{C}}\) with (a) mIoU\({}^{\text{D}}\), (b) mIoU\({}^{\text{I}}\), (c) mIoU\({}^{\text{C}^{\text{q}}}\), (d) mIoU\({}^{\text{C}^{\text{5}}}\). Models below the green dashed line have a higher rank of mIoU\({}^{\text{C}}\) than that of the counterpart.

**Image-level histograms vary across datasets for the same architecture, but are similar across architectures for the same dataset.** The same architecture yields significantly different shapes of image-level histograms across various datasets. We believe it has to do with the coverage of a dataset (the fraction of classes appear in each image, discussed in Appendix B.2): a dataset with low coverage (e.g. ADE20K and COCO-Stuff) often results in a spread-out histogram with fat tails. On the other hand, although it is possible that a model that has a medium score on all images obtains the same mIoU\({}^{\text{I}}\) as a model that performs exceedingly well on some images and very poorly on others, we find that different architectures generally produce similar histograms for the same dataset.

**Fine-grained mIoUs exhibit less bias towards large objects.** For example, in Cityscapes, the class Truck has high dataset-level size imbalance (see Figure 4). As demonstrated in Table 3, compared with DeepLabV3-ResNet101, UNet-ResNet101 obtains a low value on IoU\({}^{\text{D}}_{\text{Truck}}\) (15% lower) and mIoU\({}^{\text{D}}\) (3% lower), since it has a limited reception field and struggles with large objects such as trucks that are close to the camera. On the other hand, it is capable of identifying small objects. As a result, its performance on IoU\({}^{\text{C}}_{\text{Truck}}\), mIoU\({}^{\text{C}}\) and mIoU\({}^{\text{I}}\) is similar to that of DeepLabV3-ResNet101.

**Architecture designs and loss functions are vital for optimizing fine-grained mIoUs.** We have seen from previous findings that architecture designs play an important role in optimizing fine-grained mIoUs. The influence of loss functions is also significant. We leave discussions of architecture designs in section 5.2.1 and loss functions in section 5.2.2. These insights contribute to the development of best practices for optimizing fine-grained mIoUs.

### Best Practices

In this section, we explore best practices for optimizing fine-grained mIoUs. In particular, we underscore two key design choices: (i) the aggregation of multi-level features is as vital as incorporating modules that increase the receptive field, and (ii) the alignment of the loss function with the evaluation metrics is paramount.

#### 5.2.1 Architecture Designs

Our study reveals the importance of aggregating multi-scale feature maps for optimizing fine-grained metrics. DeepLabV3 and PSPNet seek to increase the receptive field through the ASPP module [6] and the PPM module [58], respectively. The large receptive field helps to capture large objects, resulting in high values on mIoU\({}^{\text{D}}\). However, these methods lack the fusion of multi-scale feature

Figure 4: From left to right: ground-truth label, prediction and per-image IoU of class Truck with DeepLabV3+, DeepLabV3 and UNet using ResNet101 backbone, respectively. The class Truck is represented in dark blue and highlighted within white dashed circles.

\begin{table}
\begin{tabular}{c|c c c c c} \hline Method & IoU\({}^{\text{D}}_{\text{Truck}}\) & mIoU\({}^{\text{D}}\) & IoU\({}^{\text{C}}_{\text{Truck}}\) & mIoU\({}^{\text{C}}\) & mIoU\({}^{\text{I}}\) \\ \hline DeepLabV3+ & 84.94\(\pm\) 0.97 & 80.90\(\pm\) 0.16 & 55.26\(\pm\) 0.79 & 70.17\(\pm\) 0.16 & 76.21\(\pm\) 0.05 \\ DeepLabV3 & 81.48\(\pm\) 1.43 & 80.07\(\pm\) 0.06 & 53.87\(\pm\) 0.94 & 69.09\(\pm\) 0.06 & 75.26\(\pm\) 0.03 \\ UNet & 65.58\(\pm\) 0.59 & 77.12\(\pm\) 0.17 & 53.00\(\pm\) 0.61 & 68.78\(\pm\) 0.09 & 75.36\(\pm\) 0.06 \\ \hline \end{tabular}
\end{table}
Table 3: Comparing DeepLabV3+, DeepLabV3 and UNet on Cityscapes using ResNet-101 backbone. Red: the best in a column. Green: the worst in a column.

maps and lose the ability to perceive multi-scale objects due to aggregated down-sampling. On the other hand, UNet does not incorporate a separate module to increase the receptive field and therefore underperforms on mIoU\({}^{\text{D}}\). Nevertheless, it performs comparably to DeepLabV3 and PSPNet on fine-grained metrics. The aggregation of multi-scale feature maps in UNet is critical for detecting small objects and achieving high values on fine-grained metrics3.

Footnote 3: This might explain the efficacy of UNet for medical datasets. Typically, these datasets exhibit small lesion areas but lack extremely large objects that necessitate a large receptive field.

This observation is further confirmed by comparing DeepLabV3 with its successor, DeepLabV3+. The sole architectural difference between them is the incorporation of a feature aggregation branch in DeepLabV3+. Although they produce similar results on mIoU\({}^{\text{D}}\), DeepLabV3+ typically outperforms DeepLabV3 on mIoU\({}^{\text{C}}\).

It is encouraging to see that state-of-the-art methods, such as DeepLabV3+, UPerNet, and SegFormer, all incorporate modules to increase the receptive field and to aggregate multi-level feature maps. Such design choices align well with the objective of fine-grained mIoUs. We also advocate for the design of more specialized modules for perceiving small objects which can further improve the performance on fine-grained mIoUs.

#### 5.2.2 Loss Functions

Aligning loss functions with evaluation metrics is a widely accepted practice in semantic segmentation [47, 2, 56, 14, 57, 50, 51]. The conventional approach involves combining the cross-entropy loss (CE) and IoU losses [2, 56, 50, 51], which extend discrete mIoU\({}^{\text{D}}\) with continuous surrogates. With the introduction of fine-grained mIoUs, it is straightforward to adjust the losses to optimize for these new metrics. Specifically, we explore the Jaccard metric loss (JML) [50] and its variants for optimizing mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{C}}\). From a high-level, JML relaxes set counting (intersection and union) with simple \(L^{1}\) norm functions. More details of JML are in Appendix F.

In Table 4, we compare DeepLabV3+-ResNet101 trained with various loss functions on Cityscapes and ADE20K, respectively. Specifically, (D, I, C) represents the fraction of mIoU\({}^{\text{D}}\), mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{C}}\) in JML. For example, we consider the CE baseline: (0, 0, 0) and variants of JML to only optimize mIoU\({}^{\text{D}}\): (1, 0, 0) and a uniform mixture of three: \((\frac{1}{3},\frac{1}{3},\frac{1}{3})\). The main takeaways are as follows:

**The impact of loss functions is more pronounced when evaluated with fine-grained metrics.** Specifically, we notice improvements on mIoU\({}^{\text{C}}\) over CE reaching nearly 3% on Cityscapes and 7% on ADE20K. We also observe substantial increments on mIoU\({}^{\text{I}}\). Roughly speaking, Acc \(\geq\) mIoU\({}^{\text{D}}\geq\) mIoU\({}^{\text{C}}\) in terms of class/size imbalance, while CE directly optimizes Acc. Therefore, it becomes more crucial to choose the correct loss functions when evaluated with fine-grained metrics.

**There exist trade-offs among different JML variants.** For example, compared with other JML variants, (1, 0, 0) achieves the highest mIoU\({}^{\text{D}}\) in both datasets, but have low values on mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{C}}\). We use (0, 0.5, 0.5) as the default setting in our benchmark, but alternate ratios may be considered depending on the evaluation metrics.

A notable observation is that CE surpasses (0, 1, 0) in terms of mIoU\({}^{\text{D}}\) on Cityscapes. Interestingly, many segmentation codebases, including SMP [25] and MMSegmentation [8], calculate the Jaccard/Dice loss on a per-GPU basis. When training on GPUs with limited memory, the per-GPU batch size is often small. As a result, the loss function tends to mirror the behavior of (0, 1, 0), potentially leading to sub-optimal results as measured by mIoU\({}^{\text{D}}\).

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline (D, I, C) & \multicolumn{4}{c}{Cityscapes} & \multicolumn{4}{c}{ADE20K} \\ \cline{2-7}  & mIoU\({}^{\text{D}}\) & mIoU\({}^{\text{C}}\) & mIoU\({}^{\text{D}}\) & mIoU\({}^{\text{D}}\) & mIoU\({}^{\text{C}}\) \\ \hline (0, 0, 0) & 80.67 \(\pm\) 0.36 & 74.57 \(\pm\) 0.09 & 67.61 \(\pm\) 0.12 & 45.01 \(\pm\) 0.13 & 53.73 \(\pm\) 0.09 & 39.66 \(\pm\) 0.13 \\ (1, 0, 0) & 81.22 \(\pm\) 0.27 & 75.34 \(\pm\) 0.09 & 68.97 \(\pm\) 0.19 & 47.21 \(\pm\) 0.26 & 57.74 \(\pm\) 0.11 & 46.69 \(\pm\) 0.25 \\ (0, 1, 0) & 80.29 \(\pm\) 0.28 & 76.28 \(\pm\) 0.02 & 69.97 \(\pm\) 0.09 & 45.83 \(\pm\) 0.19 & 58.37 \(\pm\) 0.03 & 45.55 \(\pm\) 0.25 \\ (0, 1, 0) & 80.75 \(\pm\) 0.24 & 76.21 \(\pm\) 0.08 & 70.49 \(\pm\) 0.19 & 46.55 \(\pm\) 0.54 & 58.74 \(\pm\) 0.09 & 46.97 \(\pm\) 0.293 \\ (\(\frac{1}{3},\frac{1}{3})\) & 81.13 \(\pm\) 0.10 & 76.19 \(\pm\) 0.05 & 70.08 \(\pm\) 0.15 & 46.92 \(\pm\) 0.25 & 58.51 \(\pm\) 0.02 & 46.77 \(\pm\) 0.07 \\ (0, \(\frac{1}{3},\frac{1}{2})\) & 80.91 \(\pm\) 0.17 & 76.21 \(\pm\) 0.05 & 70.18 \(\pm\) 0.16 & 46.32 \(\pm\) 0.33 & 58.79 \(\pm\) 0.04 & 46.52 \(\pm\) 0.30 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparing different loss functions using DeepLabV3+-ResNet101 on Cityscapes and ADE20K. (D, I, C) represents the fraction of mIoU\({}^{\text{D}}\), mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{C}}\) in JML, respectively. Red: the best in a column. Green: the worst in a column.

## 6 Discussion

### Limitation

We propose three fine-grained mIoUs: mIoU\({}^{\text{I}}\), mIoU\({}^{\text{C}}\), mIoU\({}^{\text{K}}\), along with their associated worst-case metrics. These metrics bring significant advantages over traditional mIoU\({}^{\text{D}}\). However, they are not without shortcomings, and users should be aware of these when employing them in research or applications. Specifically, mIoU\({}^{\text{I}}\) and mIoU\({}^{\text{C}}\) fall short when it comes to addressing instance-level imbalance. Meanwhile, mIoU\({}^{\text{K}}\) serves merely as an approximation of the per-instance metric, rather than an exact representation. Beisdes, conducting image-level analyses, such as image-level histograms and worst-case images, proves challenging with mIoU\({}^{\text{C}}\) and mIoU\({}^{\text{K}}\). Although such capabilities are accommodated by mIoU\({}^{\text{I}}\), it has an inherent bias towards classes that are more frequently represented across images.

Additionally, it is important to note that our conclusions are based on a specific training setting, including choices of optimizer, loss function, and other hyperparameters. There is potential for varied findings when altering this setting, especially when it comes to the choice of the loss function.

Finally, our experiments primarily focus on natural and aerial datasets, excluding medical datasets from our analysis. Besides, while our core emphasis is on addressing size imbalance in relation to IoU, the concept of calculating segmentation metrics in a fine-grained manner could be extended to other metrics, such as mAcc, the Dice score, the calibration error [19], etc. We plan to address these limitations in future work.

### Suggestion

While mIoU\({}^{\text{I}}\) might show a bias towards more common classes, it remains invaluable for users to analyze the dataset with image-level information, as well as provide a holistic evaluation of various segmentation methods. Given that mIoU\({}^{\text{C}}\) is a good proxy of mIoU\({}^{\text{K}}\), and considering many datasets lack instance-level labels, we believe mIoU\({}^{\text{C}}\) adequately serves as an instance-level approximation and could be used in most cases.

Regarding the worst-case metrics, we examine three variants: mIoU\({}^{\text{C}^{\text{I}}}\), mIoU\({}^{\text{C}^{\text{5}}}\) and mIoU\({}^{\text{C}^{\text{1}}}\). We recommend the use of mIoU\({}^{\text{C}^{\text{I}}}\) in applications where worst-case outcomes are prioritized. We provide evaluations using both mIoU\({}^{\text{C}^{\text{5}}}\) and mIoU\({}^{\text{C}^{\text{1}}}\) for the sake of comprehensiveness, but believe that adopting either one is sufficient. While utilizing \(q=1\) might be suitable for larger datasets like ADE20K to emphasize these exceptionally difficult examples, it could introduce significant variance for smaller datasets such as DeepGlobe Land. For the latter, a value of \(q=5\) or even larger might be more appropriate. With these considerations, dataset organizers can choose the metric that best fits their context.

## 7 Conclusion

In conclusion, this study provides crucial insights into the limitations of traditional per-dataset mean intersection over union and advocates for the adoption of fine-grained mIoUs, as well as the corresponding worst-case metrics. We argue that these metrics provide a less biased and more comprehensive evaluation, which is vital given the inherent class and size imbalances found in many segmentation datasets. Besides, these fine-grained metrics furnish a wealth of statistical information that can guide practitioners in making more informed decisions when evaluating segmentation methods for specific applications. Furthermore, they offer valuable insights for model and dataset auditing, aiding in the rectification of corrupted labels.

Our extensive benchmark study, covering 12 varied natural and aerial datasets and featuring 15 modern neural network architectures, underscores the importance of not relying solely on a single metric and affirms the potential of fine-grained metrics to reduce the bias towards large objects. It also sheds light on the significant roles played by architectural designs and loss functions, leading to best practices in optimizing fine-grained metrics.

## Acknowledgments and Disclosure of Funding

We acknowledge support from the Research Foundation - Flanders (FWO) through project numbers G0A1319N and S001421N, and funding from the Flemish Government under the Onderzoeksprogramma Artificiele Intelligentie (AI) Vlaanderen programme. The resources and services used in this work were provided by the VSC (Flemish Supercomputer Center), funded by the FWO and the Flemish Government.

M.B.B. is entitled to stock options in Mona.health, a KU Leuven spinoff.

## References

* [1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer Normalization. _NeurIPS Deep Learning Symposium_, 2016.
* [2] Maxim Berman, Amal Rannen Triki, and Matthew B. Blaschko. The Lovasz-softmax loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks. _CVPR_, 2018.
* [3] Patrick Bilic et al. The Liver Tumor Segmentation Benchmark (LiTS). _MIA_, 2023.
* [4] Gabriel J Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and Recognition Using Structure from Motion Point Clouds. _ECCV_, 2008.
* [5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and Stuff Classes in Context. _CVPR_, 2018.
* [6] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking Atrous convolution for semantic image segmentation. _arXiv_, 2017.
* [7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with Atrous separable convolution for semantic image segmentation. _ECCV_, 2018.
* [8] MMSegmentation Contributors. MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark, 2020.
* [9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. _CVPR_, 2016.
* [10] Gabriela Csurka, Diane Larlus, and Florent Perronnin. What is a good evaluation measure for semantic segmentation? _BMVC_, 2013.
* [11] Dengxin Dai and Luc Van Gool. Dark Model Adaptation: Semantic Image Segmentation from Daytime to Nighttime. _ITSC_, 2018.
* [12] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia, and Ramesh Raskar. DeepGlobe 2018: A Challenge to Parse the Earth through Satellite Images. _CVPR Workshop_, 2018.
* [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. _ICLR_, 2021.
* [14] Tom Eelbode, Jeroen Bertels, Maxim Berman, Dirk Vandermeulen, Frederik Maes, Raf Bisschops, and Matthew B. Blaschko. Optimization for medical image segmentation: Theory and Practice When Evaluating With Dice Score or Jaccard Index. _TMI_, 2020.
* [15] Adam Van Etten, Dave Lindenbaum, and Todd M Bacastow. SpaceNet: A Remote Sensing Dataset and Challenge Series. _arXiv_, 2018.

* [16] Mark Everingham, Luc van Gool, Chris Williams, John Winn, and Andrew Zisserman. The PASCAL Visual Object Classes Challenge 2007, 2007.
* [17] Mark Everingham, Luc van Gool, Chris Williams, John Winn, and Andrew Zisserman. The PASCAL Visual Object Classes Challenge 2008, 2008.
* [18] Mark Everingham, Luc Van Gool, Christopher K I Williams, John Winn, and Andrew Zisserman. The Pascal Visual Object Classes (VOC) Challenge. _IJCV_, 2009.
* [19] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. _ICML_, 2017.
* [20] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. _ICCV_, 2011.
* [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. _CVPR_, 2016.
* [22] Nicholas Heller et al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge. _MIA_, 2021.
* [23] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear Units (GELUs). _arXiv_, 2016.
* [24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications. _arXiv_, 2017.
* [25] Pavel Iakubovskii. Segmentation models pytorch, 2019.
* [26] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. _ICML_, 2015.
* [27] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollar. Panoptic Segmentation. _CVPR_, 2019.
* [28] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. LISA: Reasoning Segmentation via Large Language Model. _arXiv_, 2023.
* [29] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature Pyramid Networks for Object Detection. _CVPR_, 2017.
* [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C Lawrence Zitnick, and Piotr Dollar. Microsoft COCO: Common Objects in Context. _ECCV_, 2014.
* [31] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized Referring Expression Segmentation. _CVPR_, 2023.
* [32] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A ConvNet for the 2020s. _CVPR_, 2022.
* [33] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. _ICLR_, 2019.
* [34] Lena Maier-Hein et al. Metrics Reloaded: Recommendations for image analysis validation. _arXiv_, 2023.
* [35] Sachin Mehta and Mohammad Rastegari. MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer. _ICLR_, 2022.
* [36] Sachin Mehta and Mohammad Rastegari. Separable Self-attention for Mobile Vision Transformers. _TMLR_, 2023.
* [37] Bjoern H. Menze et al. The multimodal brain tumor image segmentation benchmark (BRATS). _TMI_, 2015.

* [38] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The Role of Context for Object Detection and Semantic Segmentation in the Wild. _CVPR_, 2014.
* [39] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The Mapillary Vistas Dataset for Semantic Understanding of Street Scenes. _ICCV_, 2017.
* [40] Sebastian Nowozin. Optimal Decisions from Probabilistic Models: the Intersection-over-Union Case. _CVPR_, 2014.
* [41] Md Atiqur Rahman and Yang Wang. Optimizing intersection-over-union in deep neural networks for image segmentation. _ISVC_, 2016.
* [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. _MICCAI_, 2015.
* [43] Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Guided Curriculum Model Adaptation and Uncertainty-Aware Evaluation for Semantic Nighttime Image Segmentation. _ICCV_, 2019.
* [44] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted Residuals and Linear Bottlenecks. _CVPR_, 2018.
* [45] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. _CVPR_, 2016.
* [46] Mingxing Tan and Quoc V Le. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. _ICML_, 2019.
* [47] Vladimir N Vapnik. _The Nature of Statistical Learning Theory_. Springer, 1995.
* [48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. _NeurIPS_, 2017.
* [49] Ruchika Verma et al. MoNuSAC2020: A Multi-Organ Nuclei Segmentation and Classification Challenge. _TMI_, 2021.
* [50] Zifu Wang, Xuefei Ning, and Matthew B. Blaschko. Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. _NeurIPS_, 2023.
* [51] Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin Lemmens, and Matthew B. Blaschko. Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels. _MICCAI_, 2023.
* [52] Ross Wightman. Pytorch Image Models, 2019.
* [53] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified Perceptual Parsing for Scene Understanding. _ECCV_, 2018.
* [54] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. _NeurIPS_, 2021.
* [55] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip H.S. Torr. LAVT: Language-Aware Vision Transformer for Referring Image Segmentation. _CVPR_, 2022.
* [56] Jiaqian Yu and Matthew B. Blaschko. The Lovasz Hinge: A Novel Convex Surrogate for Submodular Losses. _TPAMI_, 2018.
* [57] Jiaqian Yu, Jingtao Xu, Yiwei Chen, Weiming Li, Qiang Wang, Byung In Yoo, and Jae-Joon Han. Learning generalized intersection over union for dense pixelwise prediction. _ICML_, 2021.
* [58] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid Scene Parsing Network. _CVPR_, 2017.
* [59] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene Parsing Through ADE20K Dataset. _CVPR_, 2017.
* [60] Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. _ICLR_, 2017.

###### Contents

* 1 Introduction
* 2 Fine-grained mIoUs
* 3 Worst-case Metrics
* 4 Advantages of Fine-grained mIoUs
* 5 Experiments
	* 5.1 Main Results
	* 5.2 Best Practices
		* 5.2.1 Architecture Designs
		* 5.2.2 Loss Functions
* 6 Discussion
	* 6.1 Limitation
	* 6.2 Suggestion
* 7 Conclusion
* A Models
* A.1 Segmentation Methods
* A.2 Backbones
* B Datasets
* B.1 "Thing" and "Stuff"
* B.2 Dataset Coverage
* B.3 Dataset Auditing
* C Implementation Details
* D NULL in Fine-grained mIoUs
* E Other Per-instance mIoUs
* F The Jaccard Metric Loss
* G Results
* G.1 Cityscapes
* G.2 Nighttime Driving
* G.3 Dark Zurich
* G.4 Mapillary Vistas
* G.5 CamVid

[MISSING_PAGE_EMPTY:15]

**PSPNet [58]** adopts the Pyramid Pooling Module (PPM) to capture different levels of details in the scene. The idea behind this module is to apply pooling operations at varying scales and concatenate the output alongside the original feature map.

**UPerNet [53]** applies an encoder-decoder structure. The encoder takes inspiration from the Feature Pyramid Network (FPN) [29], creating a pyramid of features extracted at multiple scales. The decoder merges multi-level feature maps and adopts PPM at the lowest FPN level to bring useful global prior representations.

**SegFormer [54]** features a lightweight decoder that solely consists of MLP layers for upsampling and fusing multi-level features. It avoids computationally intensive components, and the key for this simplified design is the large receptive field provided by the hierarchical transformer encoder.

### Backbones

**ResNet [21]** is a type of CNN that uses shortcut connections, or "skip connections". These shortcut connections enable the network to learn an identity function, ensuring that the output of the block is at least as good as its input, and hence the term "residual". This identity function makes it easier for the network to learn and allows gradients to flow directly through the network during backpropagation, which can alleviate the problem of vanishing gradients in deep networks.

**ConvNeXt [32]** is a pure convolutional model, inspired by the design of vision transformers [13]. It consists of several macro designs (stage ratios, patchified stem, etc), micro-designs (replacing ReLU with GELU [23], substituting BN [26] with LN [1], etc), modern training techniques (AdamW [33], label smoothing [45], etc) and other designs (large kernel sizes, inverted bottlenecks, etc) to modernize ResNets.

**EfficientNet [46]** is a family of CNN architectures innovatively designed for model scaling. EfficientNet's central insight is recognizing the interactive nature of the network width, depth, and resolution. Therefore, they need to be scaled together to maintain model efficiency. The key is to adopt a neural architecture search [60] approach to find the best scaling combinations.

**MobileNetV2 [44]** is designed to be a lightweight, efficient network architecture optimized for speed and size, making it particularly well-suited for mobile and embedded vision applications. It adopts depth-wise separable convolutions from MobileNetV1 [24] to reduce computational costs. Additionally, it leverages linear bottleneck layers to prevent nonlinearities from destroying too much information and inverted residuals blocks to facilitate gradient propagation across multiple layers.

**MiT [54]** is a type of vision transformer [13] specifically optimized for semantic segmentation tasks. One key aspect of MiT is its hierarchical structure. The model comprises several stages, forming a multi-scale structure that is effective for semantic segmentation. Another noteworthy feature of MiT is its relatively lightweight nature compared to some other vision Transformers, making it more practical for real-world deployment.

**MobileViTV2 [36]** is a lightweight and low-latency hybrid network for mobile vision tasks. It follows the macro-architecture of MobileViT [35], which adopt a hybrid design of MobileNetV2 blocks [44] and self-attention operations [48]. Its main innovation is a separable self-attention that reduces the computational complexity of self-attention from \(O(n^{2})\) to \(O(n)\).

## Appendix B Datasets

Our benchmark include 12 datasets that cover various domains including: (i) street scenes [4, 9, 39, 11, 43], (ii) "thing" and "stuff" [18, 38, 59, 5], (iii) aerial scenes [12]. The number of images in the dataset ranges from 50 (e.g. Nighttime Driving and Dark Zurich) to more than a hundred thousand (e.g. COCO-Stuff); the number of classes varies from binary (e.g. DeepGlobe Road and DeepGlobe Building) to over a hundred (e.g. ADE20K and COCO-Stuff). Table 6 presents an overview of 12 selected datasets. Following is a brief introduction to each.

**Cityscapes [9]** is a large-scale dataset that focuses on semantic understanding of urban street scenes. It contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5,000 frames (2,975 for training, 500 for validation and1,525 for testing). In addition, there are 20,000 coarsely annotated frames also included in the dataset. In this study, we utilize the finely annotated training and validation images exclusively.

**Nighttime Driving [11] and Dark Zurich [43]** are datasets that provide images of various road scenes captured during the nighttime. They are designed to facilitate research in autonomous driving, and specifically addresses the challenges associated with low-light and nighttime conditions. They incorporate fine pixel-level Cityscapes [9] labels.

**Mapillary Vistas [39]** is a large-scale street-level imagery dataset used primarily for semantic segmentation tasks. It is one of the most diverse publicly available datasets of street-level imagery, collected from numerous cities around the world under a variety of weather and lighting conditions. While images are mainly taken from mobile devices, in general the dataset spans a wide range of different camera types, including head- or car-mounted ones. This diversity in viewpoints is useful for training more robust machine learning models that can generalize well to novel scenarios.

**CamVid [4]** is one of the pioneering datasets used for semantic segmentation. The dataset comprises of footage captured from the perspective of a driving automobile, providing a vision of the road scene ahead. It has been collected over different times of the day, offering an excellent selection of varying lighting conditions and appearances.

**ADE20K [59]** is a widely used dataset for scene parsing. One unique feature of the ADE20K dataset is its high diversity in object classes. The dataset has annotations for over 150 object categories, and includes a diverse set of scenes from both indoor and outdoor environments. This diversity makes the dataset challenging and well-suited for developing and benchmarking algorithms for semantic segmentation.

**COCO-Stuff [5]** is an extension of the original COCO [30] dataset which is renowned for its diverse set of high-quality, real-world images containing complex everyday scenes of common objects in their natural context. It augments the COCO dataset by adding pixel-level annotations for "stuff" classes. Thus, it contains the same images as the original COCO dataset and is well-suited for semantic segmentation tasks.

**PASCAL VOC [18]** is composed of images collected from the Flickr photo-sharing web-site. The labels include a diverse set of classes, including various types of vehicles, animals, and indoor objects. The original dataset contains 1,464 images for training, 1,449 images for validation, and a further 1,456 images for testing. Extra annotations are provided in [20], leading to a total of 10,582 training images.

**PASCAL Context [38]** is an extension of the popular PASCAL VOC dataset [18] where only object classes of interest were annotated, leaving a large portion of pixels marked as "background" or "unlabeled". In the PASCAL Context dataset, the goal is to provide a comprehensive understanding of the scene, thus all pixels in an image are assigned a label, giving the "context" of the objects.

**DeepGlobe [12]** contains high-resolution satellite images from around the globe, covering various geographical locations, features, and phenomena. It offers three distinct satellite image understanding datasets: land cover classification, road extraction, and building detection. With this rich assortment of large-scale satellite imagery, DeepGlobe aids in propelling advancements in geospatial analysis.

### "Thing" and "Stuff"

The differences between "thing" and "stuff" encompass various aspects [5]:

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & Scene & **Train** & **Full** & **Classes** & **Coverage (\%)** \\ \hline Cityscapes [9] & Urban & 2,755 & 500 & 19 & \(63.22\pm 18.88\) \\ Nighttime Driving [11] & Urban & 50 & 19 & \(34.24\pm 24.50\) \\ Dark Zurich [43] & Urban & - & 50 & 19 & \(57.99\pm 16.28\) \\ Mapillary Vistas [59] & Urban & 18,000 & 2,000 & 65 & \(30.18\pm 22.23\) \\ CamVid [4] & Urban & 469 & 2,320 & 11 & \(89.53\pm 9.18\) \\ ADE20K [59] & “Thing \& “stuff” & 20,210 & 2,000 & 150 & \(5.63\pm 0.62\) \\ COCO-Stuff [18] & “Thing \& “stuff” & 118,287 & 5,000 & 171 & \(4.92\pm 54.21\) \\ PASCAL VOC [18] & “Thing-only” & 10,582 & 1,449 & 21 & \(11.59\pm 28.54\) \\ PASCAL Context [38] & “Thing” \& “stuff” & 4,996 & 5,104 & 60 & \(9.60\pm 41.77\) \\ DeepGlobe Land [12] & Aerial & 643 & 160 & 6 & \(63.33\pm 27.09\) \\ DeepGlobe Road [12] & Aerial & 4,981 & 1,245 & 2 & \(100\pm 0\) \\ DeepGlobe Building [12, 15] & Aerial & 8,476 & 2,117 & 2 & \(88.97\pm 23.30\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: An overview of 12 selected datasets.

* Shape: "thing" has characteristic shapes (e.g. person, car, phone), whereas "stuff" is amorphous (e.g. sky, road, water).
* Size: "thing" occurs in characteristic sizes with little variance, whereas "stuff" is highly variable in size.
* Parts: "thing" possess identifiable parts, whereas "stuff" does not. For example, a fragment of sky is still considered sky, but a wheel alone does not constitute a car.
* Instances: "stuff" is typically uncountable and has no clearly defined instances.
* Texture: "stuff" is typically highly textured.

The characterization of "thing" and "stuff" is usually a decision made by the dataset creator. However, many datasets do not define which classes are "thing" or "stuff" (e.g. Cityscapes, Mapillary Vistas, ADE20K, etc). In this work, for datasets that provide instance-level annotations (Cityscapes, Mapillary Vistas, ADE20K, COCO-Stuff), we treat classes that have instance-level labels as "thing" classes, and as "stuff" classes otherwise.

### Dataset Coverage

For each dataset, we calculate the proportion of distinct classes present in each image to the total number of classes within that dataset. We define the _coverage_ of a dataset as the mean and the normalized standard deviation over all images. A dataset exhibiting a high mean and a low normalized standard deviation is considered to have high coverage, implying low variations in contexts.

Urban-scene datasets, such as Cityscapes, Mapillary Vistas, and CamVid, generally demonstrate high coverage. In contrast, "thing" and "stuff" datasets like ADE20K, COCO-Stuff, and PASCAL Context typically exhibit lower coverage as they incorporate a wide range of indoor and outdoor scenes. Datasets with low coverage often showcase more spread-out image-level histograms (see Appendix G) with fat tails, indicating a diverse range of image-level IoU values.

### Dataset Auditing

Images that consistently yield a low image-level score across different models can be inspected. These low scores could potentially be attributed to mislabeling within the images. For example, in COCO-Stuff, instances in "000000064574" and "000000053529" are erroneously labeled as "stuff" (as shown in Figures 31 - 33). This mislabeling leads to most models registering an image-level IoU of 0.

Additionally, the presence of inconsistencies between image- and object-level labels can lead to NaN values during the computation of mIoU\({}^{\text{K}}\), because the denominator in Eq. (9) will become zero. This provides a way to identify mislabels within the dataset. As demonstrated in Table 7, we find these mislabels exist in Cityscapes and ADE20K. Specifically, only a single mislabeled image is in the validation set (highlighted in red), and only a single mislabeled object is "stuff" (highlighted in green).

Figure 5: Results of averaged \(\log\frac{r^{\text{D}}_{\text{c}}}{r^{\text{D}}_{\text{c}}}\) for “thing” and “stuff” classes on Cityscapes, Mapillary Vistas, ADE20K and COCO-Stuff.

[MISSING_PAGE_FAIL:19]

In general, our implementations (models, dataset preprocessing, training recipes) closely follow MMSegmentation [8], except that we adopt a smaller number of training iterations to save the computational costs and utilize recent training techniques [50, 33] for superior performance. As demonstrated in Table 1 and Table 2, we either match or exceed the results of MMSegmentation with less training iterations (see Table 8). Also note that our training hyper-parameters are not optimized for mIoU\({}^{\text{D}}\) (see Table 4).

## Appendix D Null in Fine-grained mIoUs

For each image, based on the presence or absence of a class in both the prediction and the ground truth, we can identify four cases:

1. The class is present in both the prediction and the ground truth.
2. The class is absent in both the prediction and the ground truth (true negatives).
3. The class is absent in the prediction but present in the ground truth (false negatives).
4. The class is present in the prediction but is absent in the ground truth (false positives).

In multi-class segmentation, for cases 1 and 3, we compute the per-image-per-class score as usual. In particular, the score for case 3 is 0. For cases 2 and 4, we define the score as NULL. In binary segmentation, if it is viewed as 2-class segmentation, then we can refer to the definition above. Otherwise, if it is considered as foreground-background segmentation, the definition for cases 1 and 3 remains consistent with the multi-class setting. For case 2, we define the score as 1 and for case 4, we define the score as 0. A summary is shown in Table 9.

Notably, in the multi-class setting, our definition differs from Csurka et al. [10]. In their definition, the score is 0 for case 4. Our modification aims at preventing the metric from overly reacting to minor false positives, e.g. a few pixels are mispredicted as a car absent from the ground truth. In safety-critical settings, false negatives should have higher importance, e.g. overlooking a car present in the ground truth. Overemphasizing such minor false positives might inadvertently diminish the significance of false negatives. A concerning scenario could be when the ground truth contains only a small number of classes, yet each unrelated class has a few pixels erroneously present in the prediction. It is pivotal to note that in our definition, these mispredicted pixels still incur penalties--they are false negatives with respect to the corresponding ground-truth classes.

Besides, we do not want to discriminate between false positives solely based on whether the predicted class is present in the ground truth or not. Generally speaking, mispredicting Road as either Car or Truck should incur a similar penalty. However, according to the definition of [10], if Car is present in the ground truth and Truck is not, mispredicting Road as Car may only result in a mild penalty, whereas mispredicting Road as Truck will incur a full penalty.

To further illustrate the differences between the definition of [10] and ours, consider an example image composed of just 4 pixels, with a total of 6 classes in the dataset, as presented in Table 10. We compare the two ways of computing the score, as detailed in Table 11. When calculating the score as prescribed by [10], false positives that appear in the prediction but not in the ground truth incur a considerable penalty. Consequently, the mean IoU value for this image is only 0.25 under their definition, but is 0.5 under ours.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & MMSegmentation & Hierarchions & Warmup & Crop Size \\ \hline Cityscapes [9] & 80,000 & 40,000 & 400 & 512 \(\times\) 1024 \\ Nightplane Driving [11] & - & - & - & 512 \(\times\) 1024 \\ Dark Zurich [43] & - & - & - & 512 \(\times\) 1024 \\ Mapillary Virus [19] & - & 160,000 & 1,600 & 512 \(\times\) 1024 \\ CurylV4 [8] & - & 20,000 & 200 & 512 \(\times\) 512 \\ ADEOR [59] & 160,000 & 80,000 & 800 & 512 \(\times\) 512 \\ COCO-Staff [5] & 160,000 & 80,000 & 800 & 512 \(\times\) 512 \\ PASCAL VOC [18] & 40,000 & 40,000 & 400 & 512 \(\times\) 512 \\ PASCAL Context [38] & 80,000 & 40,000 & 400 & 512 \(\times\) 512 \\ DeepGlobe Land [12] & - & 20,000 & 200 & 512 \(\times\) 512 \\ DeepGlobe Road [12] & - & 40,000 & 400 & 512 \(\times\) 512 \\ \hline DeepGlobe Building [12, 15] & - & 40,000 & 400 & 512 \(\times\) 512 \\ \hline \hline \end{tabular}
\end{table}
Table 8: The number of training and warmup iterations and the crop size for each dataset. MMSegmentation: the number of training iterations in MMSegmentation [8].

For a more empirical comparison, in Table 12, we present the two ways for calculating mIoU\({}^{\text{C}}\) using DeepLabV3+ResNet101. It is evident that, when the score is derived following [10], there is a more aggressive penalty, resulting in a generally lower value.

In the end, although we present our definition with the belief that it offers advantages over [10] in certain scenarios, we understand that each metric will have its pros and cons. It is essential to note that our aim is not to assert that our metrics are the best or only way, but to highlight this distinction. Ultimately, end users could select the version most aligned with their specific requirements.

## Appendix E Other Per-instance mIoUs

Recall that we have instance-level TP\({}_{i,c,k}\), FN\({}_{i,c,k}\) and image-level FP\({}_{i,c}\). We can solve a minimization problem:

\[\min\sum_{k}\frac{\text{TP}_{i,c,k}}{\text{TP}_{i,c,k}+\text{FN}_{ i,c,k}+\text{FP}_{i,c,k}}\] (15) \[\text{s.t.}\sum_{k}\text{FP}_{i,c,k}=\text{FP}_{i,c}.\] (16)

Alternatively, we can replace the minimization with maximization. Denote the resulting instance-level values as IoU\({}_{\min c,i,k}\) and IoU\({}_{\max c,i,k}\), respectively. They provide a lower/upper bound on the value of IoU\({}_{i,c,k}\):

\[\text{IoU}_{\min c,i,k}\leq\text{IoU}_{i,c,k}\leq\text{IoU}_{\max c,i,k}.\] (17)

Compared to distributing image-level FP according to the size of each instances (as we defined in Section 2), IoU\({}_{\min c,i,k}\) and IoU\({}_{\max c,i,k}\) may yield results misaligned with our intuition. For example, consider a case with two instances where TP\({}_{1}=1,\text{TP}_{2}=10,\text{FN}_{1}=\text{FN}_{2}=0,\text{FP}_{1}+\text{FP}_{2}=2\). Then we have

\[\frac{1}{1+\text{FP}_{1}}+\frac{10}{10+\text{FP}_{2}},\] (18)

which is minimized when FP\({}_{1}=2,\text{FP}_{2}=0\). Thus, IoU\({}_{\min c,i,k}\) will distribute FP to the smaller instance when there is an instance imbalance.

Consider another example with two instances such that TP\({}_{1}=\text{TP}_{2}=10,\text{FN}_{1}=\text{FN}_{2}=0,\text{FP}_{1}+\text{FP}_{2}=10\). Then we have

\[\frac{10}{10+\text{FP}_{1}}+\frac{10}{10+\text{FP}_{2}},\] (19)

which is maximized when FP\({}_{1}=10,\text{FP}_{2}=0\) or FP\({}_{1}=0,\text{FP}_{2}=10\). Therefore, IoU\({}_{\max c,i,k}\) will distribute FP to one instance when instances are of the equal size.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Definition & \multicolumn{2}{c}{Cityscapes} & \multicolumn{2}{c}{Mapillary} & \multicolumn{2}{c}{ADE20K} & \multicolumn{2}{c}{COCO-Stuff} & \multicolumn{2}{c}{VOC} & \multicolumn{2}{c}{Context} & \multicolumn{2}{c}{Land} \\ \hline
[10] & \(56.14\pm 0.43\) & \(23.95\pm 0.19\) & \(20.23\pm 0.05\) & \(20.98\pm 0.09\) & \(62.30\pm 0.58\) & \(25.92\pm 0.07\) & \(40.14\pm 1.39\) \\ Ours & \(70.18\pm 0.16\) & \(46.23\pm 0.12\) & \(46.52\pm 0.30\) & \(41.98\pm 0.04\) & \(79.07\pm 0.18\) & \(55.89\pm 0.07\) & \(53.76\pm 0.40\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Predicted and ground-truth classes of an example image composed of just 4 pixels, with a total number of 6 classes in the dataset.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Case & \multicolumn{2}{c}{Mail-class} & \multicolumn{2}{c}{Binary} \\ \hline
1 & IoU\({}_{i,c}\) & IoU\({}_{i,c}\) & IoU\({}_{i,c}\) \\
2 & \multicolumn{2}{c}{mIoU} & 1 \\
3 & 0 & 0 & 0 \\
4 & \multicolumn{2}{c}{mIoU} & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 9: The definition of IoU\({}_{i,c}\) in multi-class and binary segmentation.

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_EMPTY:23]

Figure 6: Histograms: Cityscapes.

Figure 7: Worst-case images: Cityscapes 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 8: Worst-case images: Cityscapes 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 9: Worst-case images: Cityscapes 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:28]

Figure 10: Histograms: Nighttime Driving.

Figure 11: Worst-case images: Nighttime Driving 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 12: Worst-case images: Nighttime Driving 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 13: Worst-case images: Nighttime Driving 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:33]

Figure 14: Histograms: Dark Zurich.

Figure 15: Worst-case images: Dark Zurich 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 16: Worst-case images: Dark Zurich 2 / 3. Left: image. Middle: label. Right: prediction.

## Appendix A

Figure 17: Worst-case images: Dark Zurich 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:38]

Figure 18: Histograms: Mapillary Vistas.

[MISSING_PAGE_EMPTY:40]

Figure 20: Worst-case images: Mapillary Vistas 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 21: Worst-case images: Mapillary Vistas 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:43]

## Appendix A

Figure 22: Histograms: CamVid.

[MISSING_PAGE_EMPTY:45]

Figure 24: Worst-case images: CamVid 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 25: Worst-case images: CamVid 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:48]

## Appendix A

Figure 26: Histograms: ADE20K.

## Appendix A Appendix

Figure 27: Worst-case images: ADE20K 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 28: Worst-case images: ADE20K 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 29: Worst-case images: ADE20K 3 / 3. Left: image. Middle: label. Right: prediction.

### COCO-Stuff

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multicolumn{2}{c}{mlot1} & \multirow{2}{*}{mlot10} & \multirow{2}{*}{mlot15} & \multirow{2}{*}{mlot16} & \multirow{2}{*}{mlot17} \\  & & & & & \\  & & & & & \\  & & & & & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{ResNet101} & \(48.71\pm 0.06\) & \(35.26\pm 0.09\) & \(17.07\pm 0.22\) & \(8.48\pm 0.39\) \\  & & \(41.98\pm 0.04\) & \(22.46\pm 0.04\) & \(1.33\pm 0.07\) & \(\mathbf{0.00\pm 0.00}\) \\  & & \(44.09\pm 0.17\) & \(59.35\pm 0.14\) & \(69.56\pm 0.12\) & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{ResNet101} & \(44.91\pm 0.04\) & \(33.39\pm 0.06\) & \(15.40\pm 0.23\) & \(7.34\pm 0.33\) \\  & & \(39.66\pm 0.04\) & \(20.75\pm 0.08\) & \(1.19\pm 0.04\) & \(0.00\pm 0.00\) \\  & & \(42.15\pm 0.03\) & \(56.93\pm 0.08\) & \(68.63\pm 0.01\) & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{ResNet101} & \(43.32\pm 0.04\) & \(29.54\pm 0.08\) & \(11.76\pm 0.20\) & \(3.76\pm 0.23\) \\  & & \(34.39\pm 0.06\) & \(16.77\pm 0.03\) & \(0.56\pm 0.04\) & \(0.00\pm 0.00\) \\  & & \(36.30\pm 0.18\) & \(50.50\pm 0.16\) & \(66.12\pm 0.07\) & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{ResNet18} & \(41.60\pm 0.05\) & \(27.86\pm 0.07\) & \(10.48\pm 0.15\) & \(2.79\pm 0.27\) \\  & & \(32.33\pm 0.04\) & \(15.43\pm 0.06\) & \(0.38\pm 0.03\) & \(0.00\pm 0.00\) \\  & & \(34.18\pm 0.18\) & \(47.90\pm 0.17\) & \(64.78\pm 0.05\) & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{Efficient/Net200} & \(42.61\pm 0.16\) & \(28.84\pm 0.25\) & \(13.39\pm 0.54\) & \(3.92\pm 0.44\) \\  & & \(33.64\pm 0.24\) & \(16.22\pm 0.19\) & \(0.35\pm 0.08\) & \(0.00\pm 0.00\) \\  & & \(33.65\pm 0.22\) & \(50.23\pm 0.32\) & \(65.59\pm 0.16\) & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{MobileNetV2} & \(40.36\pm 0.09\) & \(26.64\pm 0.08\) & \(95.48\pm 0.03\) & \(2.57\pm 0.12\) \\  & & \(30.95\pm 0.05\) & \(14.38\pm 0.04\) & \(0.36\pm 0.03\) & \(0.00\pm 0.00\) \\  & & \(32.65\pm 0.17\) & \(46.15\pm 0.09\) & \(63.87\pm 0.08\) & \\ \hline \multirow{3}{*}{DeepLdW3*} & \multirow{3}{*}{MobileNetV2} & \(44.54\pm 0.02\) & \(30.90\pm 0.02\) & \(13.68\pm 0.11\) & \(6.35\pm 0.29\) \\  & & \(36.32\pm 0.03\) & \(18.06\pm 0.06\) & \(0.75\pm 0.10\) & \(0.00\pm 0.00\) \\  & & \(38.99\pm 0.03\) & \(53.15\pm 0.12\) & \(67.01\pm

## Appendix A

Figure 30: Histograms: COCO-Stuff.

Figure 31: Worst-case images: COCO-Stuff 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 32: Worst-case images: COCO-Stuff 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 33: Worst-case images: COCO-Stuff 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:58]

[MISSING_PAGE_EMPTY:59]

Figure 35: Worst-case images: PASCAL VOC 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 36: Worst-case images: PASCAL VOC 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 37: Worst-case images: PASCAL VOC 3 / 3. Left: image. Middle: label. Right: prediction.

### PASCAL Context

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & mIoU\({}^{\rm{A}}\) & mIoU\({}^{\rm{B}}\) & mIoU\({}^{\rm{B}}\) & mIoU\({}^{\rm{B}}\) \\  & & mIoU\({}^{\rm{C}}\) & mIoU\({}^{\rm{C}}\) & mIoU\({}^{\rm{C}}\) & mIoU\({}^{\rm{C}}\) \\ \hline \multirow{4}{*}{DeepLab3+a} & & \(67.16\pm 0.04\) & \(51.85\pm 0.03\) & \(27.15\pm 0.19\) & \(14.81\pm 1.23\) \\  & ResNet101 & \(55.89\pm 0.07\) & \(34.93\pm 0.06\) & \(3.24\pm 0.29\) & \(0.00\pm 0.00\) \\  & & \(56.49\pm 0.10\) & \(67.66\pm 0.27\) & \(81.76\pm 0.09\) & \\  & & \(65.42\pm 0.07\) & \(49.91\pm 0.08\) & \(24.97\pm 0.14\) & \(12.35\pm 0.75\) \\ \hline \multirow{2}{*}{DeepLab3+a} & & \(53.95\pm 0.18\) & \(33.12\pm 0.11\) & \(2.57\pm 0.05\) & \(0.00\pm 0.00\) \\  & & \(54.50\pm 0.08\) & \(65.74\pm 0.23\) & \(80.57\pm 0.07\) & \\ \hline \multirow{2}{*}{DeepLab3+a} & & \(61.92\pm 0.13\) & \(45.52\pm 0.19\) & \(18.58\pm 0.63\) & \(3.60\pm 0.97\) \\  & ResNet4 & \(49.49\pm 0.20\) & \(27.84\pm 0.10\) & \(0.40\pm 0.04\) & \(0.00\pm 0.00\) \\  & & \(48.22\pm 0.07\) & \(60.90\pm 0.15\) & \(74.75\pm 0.08\) & \\ \hline \multirow{2}{*}{DeepLab3+a} & & \(60.39\pm 0.11\) & \(43.87\pm 0.12\) & \(16.93\pm 0.40\) & \(2.76\pm 0.69\) \\  & ResNet18 & \(47.37\pm 0.26\) & \(26.09\pm 0.26\) & \(0.45\pm 0.03\) & \(0.00\pm 0.00\) \\  & & \(46.59\pm 0.33\) & \(58.58\pm 0.39\) & \(76.40\pm 0.12\) & \\ \hline \multirow{2}{*}{DeepLab3+a} & & \(60.93\pm 0.13\) & \(44.33\pm 0.24\) & \(17.24\pm 0.65\) & \(2.85\pm 0.92\) \\  & EfficientNet00 & \(48.13\pm 0.05\) & \(26.56\pm 0.11\) & \(0.42\pm 0.08\) & \(0.00\pm 0.00\) \\  & & \(47.90\pm 0.18\) & \(60.02\pm 0.44\) & \(77.31\pm 0.16\) & \\ \hline \multirow{2}{*}{DeepLab3+a} & & \(39.30\pm 0.10\) & \(42.89\pm 0.14\) & \(16.84\pm 0.33\) & \(3.19\pm 0.29\) \\  & MobileNetV2 & \(46.00\pm 0.18\) & \(24.87\pm 0.23\) & \(0.22\pm 0.04\) & \(0.00\pm 0.00\) \\  & & \(45.64\pm 0.11\) & \(57.77\pm 0.21\) & \(57.72\pm 0.08\) & \\ \hline \multirow{2}{*}{DeepLab3+a} & & \(62.48\pm 0.26\) & \(45.77\pm 0.34\) & \(18.91\pm 0.62\) & \(5.23\pm 0.78\) \\  & MobileV172 & \(49.96\pm 0.21\) & \(28.15\pm 0.18\) & \(0.39\pm 0.12\) & \(0.00\pm 0.00\) \\  & & \(50.38\pm 0.36\) & \(61.95\pm 0.40\) & \(78.53\pm 0.21\) & \\ \hline \multirow{2}{*}{UPENet} & & \(66.42\pm 0.10\) & \(50.85\pm 0.09\) & \(25.78\pm 0.36\) & \(13.32\pm 0.92\) \\  & ResNet101 & \(55.04\pm 0.10\) & \(34.48\pm 0.10\) & \(3.05\pm 0.07\) & \(0.00\pm 0.00\) \\  & & \(55.65\pm 0.10\) & \(66.19\pm 0.17\) & \(81.24\pm 0.06\) & \\ \hline \multirow{2}{*}{UPENet} & \multirow{2}{*}{ConvNeXt} & \(70.36\pm 0.09\) & \(55.78\pm 0.07\) & \(31.27\pm 0.29\) & \(178.88\pm 0.00\) \\  & ResNet101 & \(60.17\pm 0.02\) & \(39.11\pm 0.04\) & \(60.65\pm 0.11\) & \(0.00\pm 0.00\) \\  & & \(61.33\pm 0.06\) & \(71.84\pm 0.06\) & \(84.09\pm 0.06\) & \\ \hline \multirow{2}{*}{UPENet} & \multirow{2}{*}{MtTB4} & \(69.83\pm 0.03\) & \(54.57\pm 0.02\) & \(29.50\pm 0.20\) & \(16.10\pm 0.35\) \\  & & \(58.55\pm 0.12\) & \(37.15\pm 0.14\) & \(3.83\pm 0.34\) & \(0.00\pm 0.00\) \\  & & \(60.50\pm 0.13\) & \(70.90\pm 0.11\) & \(84.07\pm 0.10\) & \\ \hline \multirow{2}{*}{SegFormer} & \multirow{2}{*}{MtTB4} & \(69.54\pm 0.05\) & \(54.40\pm 0.07\) & \(29.72\pm 0.23\) & \(16.11\pm 0.90\) \\  & & \(58.29\pm 0.14\) & \(36.97\pm 0.22\) & \(3.63\pm 0.28\) & \(0.00\pm 0.00\) \\  & & \(60.21\pm 0.14\) & \(70.75\pm 0.20\) & \(83.90\pmFigure 38: Histograms: PASCAL Context.

Figure 39: Worst-case images: PASCAL Context 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 40: Worst-case images: PASCAL Context 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 41: Worst-case images: PASCAL Context 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:68]

[MISSING_PAGE_EMPTY:69]

Figure 43: Worst-case images: DeepGlobe Land 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 44: Worst-case images: DeepGlobe Land 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 45: Worst-case images: DeepGlobe Land 3 / 3. Left: image. Middle: label. Right: prediction.

### DeepGlobe Road

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Backbone} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{J}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{q}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{5}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{1}}\)} \\  & & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{C}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{C}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{5}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{1}}\)} \\  & & \multicolumn{2}{c}{nla\(\mathrm{vl}^{\mathrm{D}}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}\)} & \multicolumn{2}{c}{nla\(\mathrm{vl}\)} \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{ResNet101} & \(64.59\pm 0.88\) & \(53.15\pm 1.19\) & \(26.09\pm 2.10\) & \(8.50\pm 1.45\) \\  & & \(65.60\pm 0.45\) & \(88.68\pm 0.52\) & \(98.22\pm 0.01\) & \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{ResNet50} & \(63.86\pm 0.07\) & \(52.56\pm 0.08\) & \(25.95\pm 0.36\) & \(6.67\pm 1.47\) \\  & & \(63.86\pm 0.07\) & \(52.56\pm 0.08\) & \(25.95\pm 0.36\) & \(6.00\pm 1.41\) \\  & & \(64.59\pm 0.08\) & \(88.34\pm 0.14\) & \(98.16\pm 0.01\) & \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{ResNet34} & \(63.49\pm 0.17\) & \(51.77\pm 0.22\) & \(22.13\pm 0.50\) & \(4.63\pm 0.97\) \\  & & \(64.51\pm 0.06\) & \(88.21\pm 0.11\) & \(98.16\pm 0.01\) & \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{ResNet34} & \(62.74\pm 0.12\) & \(50.90\pm 0.22\) & \(22.52\pm 0.93\) & \(5.16\pm 1.01\) \\  & & \(63.87\pm 0.08\) & \(87.85\pm 0.11\) & \(98.12\pm 0.00\) & \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{EfficientNetB0} & \(64.29\pm 0.09\) & \(53.49\pm 0.10\) & \(28.82\pm 0.11\) & \(13.88\pm 0.58\) \\  & & \(64.29\pm 0.09\) & \(53.49\pm 0.10\) & \(28.82\pm 0.11\) & \(13.67\pm 0.47\) \\  & & \(65.14\pm 0.07\) & \(89.16\pm 0.07\) & \(98.16\pm 0.00\) & \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{MobileNetV2} & \(62.25\pm 0.24\) & \(50.47\pm 0.33\) & \(22.58\pm 0.73\) & \(5.95\pm 0.78\) \\  & & \(63.47\pm 0.12\) & \(87.49\pm 0.27\) & \(29.80\pm 0.00\) & \\ \hline \multirow{2}{*}{DeepLabV3+} & \multirow{2}{*}{MobileNetV2} & \(64.65\pm 0.13\) & \(53.85\pm 0.24\) & \(27.50\pm 0.83\) & \(12.66\pm 1.71\) \\  & & \(64.65\pm 0.13\) & \(53.58\pm 0.24\) & \(27.50\pm 0.83\) & \(12.33\pm 1.70\) \\  & & \(65.55\pm 0.05\) & \(88.81\pm 0.10\) & \(98.21\pm 0.01\) & \\ \hline \multirow{2}{*}{UPerNet} & \multirow{2}{*}{ResNet101} & \(63.06\pm 0.52\) & \(51.03\pm 1.05\) & \(22.61\pm 3.41\) & \(5.12\pm 3.13\) \\  & & \(64.55\pm 0.26\) & \(51.03\pm 1.05\) & \(22.61\pm 3.41\) & \(4.67\pm 3.30\) \\ \hline \multirow{2}{*}{UPerNet} & \multirow{2}{*}{ConvNeXic} & \(64.57\pm 0.04\) & \(55.88\pm 0.12\) & \(29.37\pm 0.82\) & \(10.3\pm 2.14\) \\  & & \(66.71\pm 0.04\) & \(50.82\pm 0.12\) & \(29.37\pm 0.82\) & \(9.67\pm 1.89\) \\  & & \(67.65\pm 0.05\) & \(89.88\pm 0.10\) & \(98.33\pm 0.00\) & \\ \hline \multirow{2}{*}{UPerNet} & \multirow{2}{*}{MTB4} & \(65.05\pm 0.87\) & \(53.73\pm 1.21\) & \(26.65\pm 2.79\) & \(10.51\pm 5.42\) \\  & & \(66.55\pm 0.87\) & \(53.73\pm 1.21\) & \(26.65\pm 2.79\) & \(10.00\pm 5.66\) \\  & & \(66.37\pm 0.64\) & \(88.09\pm 0.55\) & \(98.27\pm 0.03\) & \\ \hline \multirow{2}{*}{SegFormer} & \multirow{2}{*}{MTB4} & \(64.27\pm 1.15\) & \(52.90\pm 1.59\) & \(25.69\pm 2.96\) & \(8.70\pm 4.76\) \\  & & \(64.27\pm 1.15\) & \(52.90\pm 1.59\) & \(25.69\pm 2.96\) & \(8.33\pm 4.50\) \\  & & \(65.61\pm 0.87\) & \(88.61\pm 0.72\) & \(89.23\pm 0.04\) & \\ \hline \multirow{2}{*}{SegFormer} & \multirow{2}{*}{MTB2} & \(64.74\pm 0.02\) & \(53.81\pm 0.03\) & \(28.66\pm 0.22\) & \(13.74\pm 1.37\) \\  & & \(65.70\pm 0.09\) & \(89.44\pm 0.12\) & \(98.19\pm 0.00\) & \\ \hline \multirow{2}{*}{DeepLabV3} & \multirow{2}{*}{ResNet101} & \(61.82\pm 2.12\) & \(49.95\pm 2.66\) & \(22.49\pm 4.08\) & \(6.34\pm 2.31\) \\  & & \(63.57\pm 1.53\) & \(88.16\pm 0.81\) & \(98.08\pm 0.09\) & \\ \hline \multirow{2}{*}{PSPNet} & \multirow{2}{*}{ResNet101} & \(61.13\pm 2.60\) & \(49.09\pm 3.45\) & \(21.76\pm 5.14\) & \(7.33\pm 3.03\) \\  & & \(63.13\pm 1.73\) & \(87.49\pm 1.25\) & \(98.09\pm 0.08\) & \\ \hline \multirow{2}{*}{UNet} & \multirow{2}{*}{ResNet101} & \(62.88\pm 0.68\) & \(50.85\pm 0.94\) & \(21.23\pm 2.00\) & \(3.09\pm 0.82\) \\  & & \(64.26\pm 0.56\) & \(86.77\pm 0.71\) & \(98.21\pm 0Figure 46: Histograms: DeepGlobe Road.

[MISSING_PAGE_EMPTY:75]

Figure 48: Worst-case images: DeepGlobe Road 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 49: Worst-case images: DeepGlobe Road 3 / 3. Left: image. Middle: label. Right: prediction.

[MISSING_PAGE_EMPTY:78]

## Appendix A

Figure 50: Histograms: DeepGlobe Building.

## Appendix A

Figure 51: Worst-case images: DeepGlobe Building 1 / 3. Left: image. Middle: label. Right: prediction.

Figure 52: Worst-case images: DeepGlobe Building 2 / 3. Left: image. Middle: label. Right: prediction.

Figure 53: Worst-case images: DeepGlobe Building 3 / 3. Left: image. Middle: label. Right: prediction.