# Extensive-Form Game Solving via Blackwell

Approachability on Treeplexes

 Darshan Chakrabarti

IEOR Department

Columbia University

dc3595@columbia.edu

&Julien Grand-Clement

ISOM Department

HEC Paris

grand-clement@hec.fr

&Christian Kroer

IEOR Department

Columbia University

christian.kroer@columbia.edu

###### Abstract

We introduce the first algorithmic framework for Blackwell approachability on the sequence-form polytope, the class of convex polytopes capturing the strategies of players in extensive-form games (EFGs). This leads to a new class of regret-minimization algorithms that are stepsize-invariant, in the same sense as the Regret Matching and Regret Matching\({}^{+}\) algorithms for the simplex. Our modular framework can be combined with any existing regret minimizer over cones to compute a Nash equilibrium in two-player zero-sum EFGs with perfect recall, through the self-play framework. Leveraging predictive online mirror descent, we introduce _Predictive Treeplex Blackwell\({}^{+}\)_ (\(\mathsf{PTB}^{*}\)), and show a \(O(1/\sqrt{T})\) convergence rate to Nash equilibrium in self-play. We then show how to stabilize \(\mathsf{PTB}^{*}\) with a stepsize, resulting in an algorithm with a state-of-the-art \(O(1/T)\) convergence rate. We provide an extensive set of experiments to compare our framework with several algorithmic benchmarks, including \(\mathsf{CFR}^{*}\) and its predictive variant, and we highlight interesting connections between practical performance and the stepsize-dependence or stepsize-invariance properties of classical algorithms.

## 1 Introduction

In this paper, we focus on solving _Extensive-Form Games_ (EFGs). Finding a Nash equilibrium of a two-player zero-sum EFG can be cast as solving

\[\min_{\mathbf{x}\in\mathcal{X}}\max_{\mathbf{y}\in\mathcal{Y}}\left\langle\mathbf{x},\mathbf{ M}\mathbf{y}\right\rangle \tag{1}\]

where the sets \(\mathcal{X},\mathcal{Y}\) are two _sequence-form polytopes_ (also referred to as _treeplexes_) representing the strategies \(\mathbf{x},\mathbf{y}\) of each player, and \(\mathbf{M}\) is a payoff matrix. EFGs have been successfully used to obtain superhuman performances in several recent poker AI breakthroughs [37, 4, 5]. Many algorithms have been developed based on (1). Since \(\mathcal{X}\) and \(\mathcal{Y}\) are polytopes, (1) can be formulated as a linear program [38]. However, because \(\mathcal{X}\) and \(\mathcal{Y}\) themselves have very large dimensions in realistic applications, _first-order methods_ (FOMs) and _regret minimization_ approaches are preferred for large-scale game solving. FOMs such as the Excessive Gap Technique (\(\mathsf{EGT}\), [32]) and Mirror Prox [31] instantiated for EFGs [23, 27] converge to a Nash equilibrium at a rate of \(O(1/T)\), where \(T\) is the number of iterations. Regret minimization techniques rely on a folk theorem relating the regrets of the players and the duality gap of the average iterates [19]. For instance, predictive online mirror descent with the treeplexes \(\mathcal{X}\) and \(\mathcal{Y}\) as decision sets achieves a \(O(1/T)\) convergence rate [14].

_Counterfactual regret minimization_ (CFR) [39] is a regret minimizer for the treeplex that runs regret minimizers _locally_, i.e. directly at the level of the information sets of each player. \(\mathsf{CFR}^{*}\), used in virtually all poker AI milestones [37, 30, 5], instantiates the CFR framework with a regret minimizercalled _Regret Matching\({}^{+}\)_ (RM\({}^{*}\)) [37] and guarantees a \(O(1/\sqrt{T})\) convergence rate. The strong empirical performance of CFR\({}^{*}\) remains mostly unexplained, since this algorithm does not achieve the fastest theoretical \(O(1/T)\) convergence rate. Interestingly, there is a stark contrast between the role of stepsizes in CFR\({}^{*}\) versus in other algorithms. CFR\({}^{*}\) may use different stepsizes across different infosets, and the iterates of CFR\({}^{*}\) do not depend on the values of these stepsizes. We identify this property as _infoset stepsize invariance_. In contrast, the convergence properties of FOMs depend on the choice of a single stepsize used across the entire treeplex, which may be hard to tune in practice.

\(\mathsf{RM}^{+}\) is an instantiation of _Blackwell approachability_[3] for the simplex, a versatile framework with connections to online learning [1]. Empirically, using a regret minimizer (over simplexes) based on Blackwell approachability (\(\mathsf{RM}^{+}\)) is central to the success of CFR\({}^{*}\): combining CFR with other local regret minimizers than \(\mathsf{RM}^{+}\), e.g., Online Mirror Descent (\(\mathsf{OMD}\)), leads to much weaker practical performance [6]. This raises the question of whether the performance of CFR\({}^{*}\) is mostly explained by the use of Blackwell approachability _on simplexes_ (\(\mathsf{RM}^{+}\)), and if a Blackwell approachability-based algorithm operating _directly on treeplexes_, bypassing the CFR decomposition, could outperform CFR\({}^{*}\). Our **goal** in this paper is to address these questions. To do so, we develop the _first_ Blackwell approachability-based algorithms for treeplexes, and we provide a new hypothesis for explaining the performance of CFR\({}^{*}\). In particular, our **main contributions** are as follows.

**Treeplex Blackwell approachability.** We introduce the first Blackwell approachability-based regret minimizer for treeplexes. Using the self-play framework, we correspondingly get the first framework for solving two-player zero-sum EFGs via Blackwell approachability on treeplexes. Blackwell approachability enables an equivalence between regret minimization over the treeplex \(\mathcal{T}\) and over its conic hull \(\mathsf{cone}(\mathcal{T})\), and any existing regret minimizer for \(\mathsf{cone}(\mathcal{T})\) yields a new algorithm for solving EFGs. A crucial advantage of using Blackwell approachability on the treeplex, rather than regret minimization directly on the treeplex, is that it leads to a variety of interesting stepsize properties (e.g. stepsize invariance), which are not achieved by regret minimizers such as \(\mathsf{OMD}\) on the treeplex.

We then provide several instantiations of our framework. \(\mathsf{PTB}^{+}\) (_Predictive Treeplex Blackwell\({}^{+}\)_, Algorithm 2) combines our framework with predictive \(\mathsf{OMD}\) over \(\mathsf{cone}(\mathcal{T})\) and achieves a \(O(1/\sqrt{T})\) convergence rate. \(\mathsf{PTB}^{+}\) is _treeplex stepsize invariant_: its iterates do not change if we rescale all stepsizes by a positive constant. This is a desirable property for practical use, although it is a weaker property than the _infoset_ stepsize invariance of CFR\({}^{*}\). Smooth \(\mathsf{PTB}^{*}\) (Algorithm 3) is a variant of \(\mathsf{PTB}^{+}\) ensuring that successive iterates vary smoothly. We show that Smooth \(\mathsf{PTB}^{+}\) is the first EFG-solving algorithm based on Blackwell approachability achieving a \(O(1/T)\) convergence rate, answering an important open question. Crucially, it is necessary to introduce a stepsize to achieve this faster convergence, and thus \(\mathsf{Smooth}\)\(\mathsf{PTB}^{+}\) is not treeplex stepsize invariant; this is analogous to existing FOM-based \(O(1/T)\)-methods for solving EFGs. We also consider \(\mathsf{AdaGradTB}^{+}\) and \(\mathsf{AdamTB}^{+}\), which learn different stepsizes for every dimension of the treeplexes, based on \(\mathsf{AdaGrad}\)[12] and \(\mathsf{Adam}\)[25]. We present the convergence properties of our algorithms in Table 1.

**Numerical experiments.** We provide two comprehensive sets of numerical experiments over benchmark EFGs. We find that \(\mathsf{PTB}^{+}\) performs the best among all the algorithms introduced in our paper (Figure 4), highlighting the advantage of _treeplex_ stepsize invariant algorithms (\(\mathsf{PTB}^{+}\)) over stepsize-dependent algorithms achieving faster theoretical convergence rate (\(\mathsf{Smooth}\)\(\mathsf{PTB}^{+}\)), and over adaptive algorithms learning decreasing stepsizes (\(\mathsf{AdaGradTB}^{+}\), \(\mathsf{AdamTB}^{+}\)). We then compare our best method (\(\mathsf{PTB}^{+}\)) with CFR\({}^{*}\), predictive CFR\({}^{*}\) (\(\mathsf{PCFR}^{+}\)), and predictive \(\mathsf{OMD}\) (\(\mathsf{POMD}\)) (Figure 2). We expected \(\mathsf{PTB}^{+}\) to perform on par with \(\mathsf{PCFR}^{+}\), since \(\mathsf{PTB}^{+}\) is stepsize invariant, predictive, and based on Blackwell approachability. However, we find that \(\mathsf{PCFR}^{+}\) outperforms all other algorithms. This suggests that _infoset_ stepsize invariance is an important property, even more than the _treeplex_ stepsize invariance of \(\mathsf{PTB}^{+}\). Due to the CFR decomposition, \(\mathsf{PCFR}^{+}\) can use different stepsizes at different infosets, where the values of the variables may be of very different magnitudes (typically, smaller for infosets appearing deeper in the treeplex), and \(\mathsf{PCFR}^{+}\) does not require tuning these different stepsizes, which may be impossible for large instances. No algorithms appear to consistently outperform the others for the last-iterate performances, and we leave studying this as an open question.

**A new hypothesis on EFG-solving algorithms: the role of stepsize invariance.** Overall, as part of our main contributions, we identify and distinguish the infoset and treeplex stepsize invariance properties, and based on our empirical experiments, we posit that infoset stepsize invariance explains part of the puzzle behind the strong empirical performance of CFR\({}^{*}\) and \(\mathsf{PCFR}^{+}\). Our results highlight that for practical performance, the stepsize invariance properties may be more important than faster theoretical convergence rates, which require introducing a stepsize, as for Smooth PTB\({}^{\ast}\) or POMD. The very strong empirical performance of (predictive) CFR\({}^{\ast}\) has been unexplained for a long time and is one of the major open questions in EFG-solving; we view providing a new hypothesis for this phenomenon (infoset stepsize invariance) as important contributions to the EFG-solving community.

## 2 Preliminaries on EFGs

We first provide some background on EFGs and treeplexes.

**Extensive-form games.** Two-player zero-sum extensive-form games (later referred to as _EFGs_) are represented by a game tree and a payoff matrix. Each node of the tree belongs either to one of the players, or to a _chance player_, modeling the random events in the game, e.g., tossing a coin. The players are assigned payoffs at the terminal nodes only. Imperfect information is modeled using _information sets_ (_infosets_), which are subsets of nodes of the game tree. A player cannot distinguish between the nodes in a given infoset, and they must take the same action at all these nodes.

**Treeplexes.** The strategy of a player can be described by a polytope called the _treeplex_, also known as the _sequence-form polytope_. The treeplex is constructed as follows. We index the infosets of a player by \(\mathcal{J}=\{1,...,|\mathcal{J}|\}\). The set of actions available at infoset \(j\in\mathcal{J}\) is written \(\mathcal{A}_{j}\) with cardinality \(|\mathcal{A}_{j}|=n_{j}\). We represent choosing action \(a\in\mathcal{A}_{j}\) at infoset \(j\in\mathcal{J}\) by a _sequence_\((j,a)\), and we denote by \(\mathcal{C}_{ja}\) the set of next infosets reachable from \((j,a)\) (possibly empty if the game terminates). The parent \(p_{j}\) of an infoset \(j\in\mathcal{J}\) is the sequence leading to \(j\); note that \(p_{j}\) is unique assuming perfect recall. We assume that there is a single root denoted as \(\varnothing\) and called the _empty sequence_. If the player does not take any action before reaching \(j\in\mathcal{J}\), then by convention \(p_{j}=\varnothing\). Under the perfect recall assumption, the set of infosets has a tree structure: \(\mathcal{C}_{ja}\cap\mathcal{C}_{j^{\prime}a^{\prime}}=\emptyset\), for all pairs of sequences \((j,a)\) and \((j^{\prime},a^{\prime})\) such that \(j\neq j^{\prime},a\neq a^{\prime}\). This tree is the treeplex and it represents the set of all admissible strategies for a given player. We denote by \(n\in\mathbb{N}\) the total number of sequences \((j,a)\) with \(j\in\mathcal{J}\) and \(a\in\mathcal{A}_{j}\). With these notations, the treeplex \(\mathcal{T}\) of a given player is

\[\mathcal{T}=\{\mathbf{x}\in\mathbb{R}_{+}^{n+1}\mid x_{\varnothing}=1,\sum_{a\in \mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\} \tag{2}\]

where the first component \(x_{\varnothing}\) is related to the empty sequence \(\varnothing\). A player _makes an observation_ to arrive at \(j\), if \(|\mathcal{C}_{p_{j}}|>1\). We define the depth \(d\) of a treeplex to be the maximum number of actions and observations that can be made starting at the root until reaching a leaf infoset. Computing a Nash equilibrium of EFGs can be formulated as solving (1) (under the perfect recall assumption), with \(\mathcal{X}\subset\mathbb{R}^{n_{1}+1}\) and \(\mathcal{Y}\subset\mathbb{R}^{n_{2}+1}\) the treeplex of each player, \(n_{1}\) and \(n_{2}\) are the number of sequences of each player, and \(\mathbf{M}\in\mathbb{R}^{(n_{1}+1)\times(n_{2}+1)}\) the payoff matrix such that for a pair of strategy \((\mathbf{x},\mathbf{y})\in\mathcal{X}\times\mathcal{Y}\), \(\langle\mathbf{x},\mathbf{M}\mathbf{y}\rangle\) is the expected value that the second player receives from the first player.

**Regret minimization and self-play framework.** A _regret minimizer_ Regmin over a decision set \(\mathcal{Z}\subset\mathbb{R}^{d}\) is an algorithm such that, at every iteration, Regmin chooses a decision \(\mathbf{z}^{t}\in\mathcal{Z}\), a _loss vector_\(\mathbf{\ell}\in\mathbb{R}^{d}\) is observed, and the scalar loss \(\langle\mathbf{\ell}^{t},\mathbf{x}^{t}\rangle\) is incurred. A regret minimizer ensures that the _regret_\(\mathsf{Reg}^{T}=\max_{\hat{\mathbf{z}}\in\mathcal{Z}}\sum_{t=1}^{T}\langle\mathbf{ \ell}^{t},\mathbf{z}^{t}-\hat{\mathbf{z}}\rangle\) grows at most as \(O(\sqrt{T})\). As an example, _predictive online mirror descent_ (POMD, [34]) generates a sequence of decisions \(\mathbf{z}_{1},...,\mathbf{z}_{T}\in\mathcal{Z}\) as follows:

\[\mathbf{z}_{t}=\Pi_{\mathcal{Z}}\left(\hat{\mathbf{z}}_{t}-\eta\mathbf{m}_{t}\right), \hat{\mathbf{z}}_{t+1}=\Pi_{\mathcal{Z}}\left(\hat{\mathbf{z}}_{t}-\eta\mathbf{\ell}_{t}\right) \tag{3}\]

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Algorithms** & **Convergence rate** & **Stepsize invariance** \\ \hline CFR\({}^{\ast}\)[37] & \(1/\sqrt{T}\) & \(\checkmark\checkmark\) \\ PCFR\({}^{\ast}\)[16] & \(1/\sqrt{T}\) & \(\checkmark\checkmark\) \\ EGT [26] & \(1/T\) & \(\mathbf{X}\) \\ POMD [14] & \(1/T\) & \(\mathbf{X}\) \\ PTB\({}^{\ast}\) (Algorithm 2) & \(1/\sqrt{T}\) & \(\checkmark\) \\ Smooth PTB\({}^{\ast}\) (Algorithm 3) & \(1/T\) & \(\mathbf{X}\) \\ AdaGradTB\({}^{\ast}\) (Algorithm 6) & \(1/\sqrt{T}\) & \(\mathbf{X}\) \\ AdamTB\({}^{\ast}\) (Algorithm 7) &? & \(\mathbf{X}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Convergence rates to a Nash equilibrium of a two-player zero-sum EFG for several algorithms. \(\checkmark\checkmark\) refers to _infoset_ stepsize invariance and \(\checkmark\) refers to _treeplex_ stepsize invariance.

with \(\mathbf{m}_{1},...,\mathbf{m}_{T}\in\mathbb{R}^{d}\) some predictions of the losses \(\mathbf{\ell}_{1},...,\mathbf{\ell}_{T}\in\mathbb{R}^{d}\), and where we write the orthogonal projection of \(\mathbf{y}\in\mathbb{R}^{d}\) onto \(\mathcal{Z}\) as \(\Pi_{\mathcal{Z}}\left(\mathbf{y}\right):=\arg\min_{\mathbf{z}\in\mathcal{Z}}\|\mathbf{z}- \mathbf{y}\|_{2}\).

The _self-play framework_ solves EFGs via regret minimization. The players compute two sequences of strategies \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) and \(\mathbf{y}_{1},...,\mathbf{y}_{T}\) such that, at iteration \(t\geq 1\), the first player observes its loss vector \(\mathbf{M}\mathbf{y}_{t-1}\) and the second player observes its loss vector \(-\mathbf{M}^{\top}\mathbf{x}_{t-1}\). Each player computes their current strategies \(\mathbf{x}_{t}\in\mathcal{X}\) and \(\mathbf{y}_{t}\in\mathcal{Y}\) via regret minimization. A well-known theorem states that the duality gap of the average of the iterates is bounded by the sum of the average regrets of the players.

**Proposition 2.1** ([19]).: _Let \(\mathbf{x}_{1},...,\mathbf{x}_{T}\in\mathcal{X}\) and \(\mathbf{y}_{1},...,\mathbf{y}_{T}\in\mathcal{Y}\) be computed in the self-play framework. Let \(\left(\bar{\mathbf{x}}_{T},\bar{\mathbf{y}}_{T}\right)=\frac{1}{T}\sum_{t=1}^{T}\left( \mathbf{x}_{t},\mathbf{y}_{t}\right)\). Then, for \(\mathsf{Reg}_{1}^{T}\) and \(\mathsf{Reg}_{2}^{T}\) the regret of each player,_

\[\max_{\mathbf{y}\in\mathcal{Y}}\,\left\langle\bar{\mathbf{x}}_{T},\mathbf{M}\bar{\mathbf{y}} \right\rangle-\min_{\mathbf{z}\in\mathcal{X}}\,\left\langle\bar{\mathbf{x}},\mathbf{M}\bar {\mathbf{y}}_{T}\right\rangle=\left(\mathsf{Reg}_{1}^{T}+\mathsf{Reg}_{2}^{T} \right)/T.\]

We present more details on the self-play framework in Appendix A.

**CFR and Regret Matching\({}^{+}\).**_Counterfactual Regret minimization_ (CFR, [39]) runs independent regret minimizers with counterfactual losses at each infoset of the trespelexes. This considerably simplifies the optimization problem, since the decision set at each infoset \(j\in\mathcal{J}\) is the simplex over the set of next available actions \(\Delta^{n_{j}}:=\{\mathbf{x}\in\mathbb{R}_{+}^{n_{j}}\mid\sum_{i=1}^{n_{j}}x_{i}=1\}\). In the CFR framework, the regret of each player (over the trespelex) is bounded by the maximum of the local regrets incurred at each infoset. Therefore, CFR combined with any regret minimizer over the simplex converges to a Nash equilibrium at a rate of \(O(1/\sqrt{T})\). We refer to Appendix B for more details. Combining CFR with a local regret minimizer called _Regret Matching\({}^{+}\)_ (RM\({}^{+}\), [37]) along with alternation and linear averaging yields an algorithm called \(\mathsf{CFR}^{*}\), which has been observed to attain strong practical performance compared to theoretically-faster methods [27]. Crucially, RM\({}^{+}\) can only be implemented on the simplex and not for other decision sets, and proceeds as follows: given a sequence of loss \(\mathbf{\ell}_{1},...,\mathbf{\ell}_{T}\in\mathbb{R}^{d}\), RM\({}^{+}\) maintains a sequence \(\mathbf{R}_{1},...,\mathbf{R}_{T}\in\mathbb{R}^{d}\) such that \(\mathbf{R}_{1}=\mathbf{0}\) and

\[\mathbf{x}_{t}=\mathbf{R}_{t}/\|\mathbf{R}_{t}\|_{1},\mathbf{R}_{t+1}=\Pi_{\mathbb{R}_{+}^{d} }\left(\mathbf{R}_{t}-\eta\mathbf{g}(\mathbf{x}_{t},\mathbf{\ell}_{t})\right) \tag{4}\]

with \(\eta>0\) and \(\mathbf{0}/0:=(1/d)\mathbf{1}\) for \(\mathbf{1}:=(1,...,1)\in\mathbb{R}^{d}\), and, for \(\mathbf{x},\mathbf{\ell}\in\mathbb{R}^{d}\),

\[\mathbf{g}(\mathbf{x},\mathbf{\ell}):=\mathbf{\ell}-\left\langle\mathbf{x},\mathbf{\ell}\right\rangle \mathbf{1}. \tag{5}\]

\(\mathsf{RM}^{+}\) is _stepsize invariant_: \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) are independent of \(\eta\), since \(\mathbf{x}_{t}=\mathbf{R}_{t}/\|\mathbf{R}_{t}\|_{1}\) and \(\eta\) only rescales the entire sequence \(\mathbf{R}_{1},...,\mathbf{R}_{T}\). Since \(\mathsf{CFR}^{*}\) runs \(\mathsf{RM}^{*}\) at each infoset independently, \(\mathsf{CFR}^{*}\) is _infoset stepsize invariant_: there may be different stepsizes across different infosets and the iterates of \(\mathsf{CFR}^{*}\) do not depend on them, which is desirable for large-scale EFGs where stepsize tuning may be difficult.

\(\mathsf{RM}^{+}\) can be interpreted as an instantiation of _Blackwell approachability_[3, 1], where the goal of the decision maker is to compute the sequence of strategies \(\mathbf{x}_{1},...,\mathbf{x}_{T}\in\Delta^{d}\) to ensure that the auxiliary sequence \(\mathbf{R}_{T}/T\in\mathbb{R}_{+}^{d}\) approaches the _target set_\(\mathbb{R}_{-}^{d}\) as \(T\rightarrow+\infty\). Since \(\mathbf{R}_{t}\in\mathbb{R}_{+}^{d}\), this is equivalent to ensuring that \(\lim_{T\rightarrow+\infty}\mathbf{R}_{T}/T=\mathbf{0}\). The vector \(\mathbf{g}(\mathbf{x},\mathbf{\ell})\) is interpreted as an instantaneous loss for the approachability instance. As an instantiation of Blackwell approachability, at each iteration \(\mathsf{RM}^{*}\) computes an orthogonal projection onto the _conic hull_ of the decision set:

\[\mathbb{R}_{+}^{d}=\mathsf{cone}(\Delta^{d}) \tag{6}\]

with \(\mathsf{cone}(\mathcal{Z}):=\{\alpha\mathbf{x}\mid\mathbf{x}\in\mathcal{Z},\alpha\geq 0\}\) for a set \(\mathcal{Z}\). The function \(\mathbf{R}\mapsto\mathbf{R}/\|\mathbf{R}\|_{1}\) is based on

\[\Delta^{d}\subset\{\mathbf{x}\in\mathbb{R}^{d}\mid\left\langle\mathbf{x},\mathbf{1}\right \rangle=1\}. \tag{7}\]

Since for \(\mathbf{R}\in\mathbb{R}_{+}^{d},\left\langle\mathbf{R},\mathbf{1}\right\rangle=\|\mathbf{R}\|_{1}\), then \(\mathbf{x}_{t}=\mathbf{R}_{t}/\|\mathbf{R}_{t}\|_{1}\) can be written \(\mathbf{x}_{t}=\mathbf{R}_{t}/\langle\mathbf{R}_{t},\mathbf{1}\rangle\), with \(\mathbf{1}\) a vector such that the decision set \(\Delta^{d}\) satisfies (7). This ensures that

\[\left\langle\mathbf{R}_{t},\mathbf{g}(\mathbf{x}_{t},\mathbf{\ell})\right\rangle=0,\forall\ \mathbf{\ell}\in\mathbb{R}^{d}. \tag{8}\]

We provide an illustration of the dynamics of \(\mathsf{RM}^{+}\) in Figure 1. Equation (8) is known as a _hyperplane forcing condition_ and is a key ingredient in any Blackwell approachability-based algorithm; it ensures that the vector \(\mathbf{R}_{T}\) grows at most at a rate of \(O(\sqrt{T})\) so that \(\lim_{T\rightarrow+\infty}\mathbf{R}_{T}/T=\mathbf{0}\). We refer to [33, 22] and to Appendix C for more details on Blackwell approachability.

Blackwell Approachability on Treeplexes

In this section we introduce a modular regret minimization framework for the treeplex based on Blackwell approachability. This framework can be used as a regret minimizer over \(\mathcal{T}\) in the self-play framework (described in the previous section and in Appendix A) to obtain an algorithm for solving EFGs. Our algorithms are based on the fact that for \(\mathcal{T}\subset\mathbb{R}^{n+1}\) a treeplex as defined in (2), we have

\[\mathcal{T}\subset\{\mathbf{x}\in\mathbb{R}^{n+1}\mid\langle\mathbf{x},\mathbf{a}\rangle=1\} \tag{9}\]

for \(\mathbf{a}=(1,\mathbf{0})\in\mathbb{R}^{n+1}\) with \(\mathbf{0}=(0,...,0)\in\mathbb{R}^{n}\). This property is analogous to (7) for the simplex. With this analogy in mind, we define \(\mathcal{C}\subset\mathbb{R}^{n+1}\) and \(\mathbf{f}(\mathbf{x},\mathbf{\ell})\in\mathbb{R}^{n+1}\) as, for \(\mathbf{x},\mathbf{\ell}\in\mathbb{R}^{n+1}\),

\[\mathcal{C} :=\mathsf{cone}(\mathcal{T}) \tag{10}\] \[\mathbf{f}(\mathbf{x},\mathbf{\ell}) :=\mathbf{\ell}-\langle\mathbf{x},\mathbf{\ell}\rangle\mathbf{a}. \tag{11}\]

Equation (10) and Equation (11) are analogous to (6) and (5). The cone \(\mathcal{C}\) and the vector \(\mathbf{f}(\mathbf{x},\mathbf{\ell})\) play a similar role for \(\mathcal{T}\) as \(\mathbb{R}_{+}^{d}\) and \(\mathbf{g}(\mathbf{x},\mathbf{\ell})\) play for \(\Delta^{d}\) in \(\mathsf{RM}^{*}\). Our framework is described in Algorithm 1 and relies on running a regret minimizer Regmin over \(\mathcal{C}=\mathsf{cone}(\mathcal{T})\) against the losses \(\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell}_{t})\) to obtain a regret minimizer over \(\mathcal{T}\) against the losses \(\mathbf{\ell}_{t}\), for \(t\geq 1\).

```
1:Input: A regret minimizer Regmin with decision set \(\mathcal{C}\)
2:Initialization:\(\mathbf{R}_{1}=\mathbf{0}\in\mathbb{R}^{n+1}\)
3:for\(t=1,\dots,T\)do
4:\(\mathbf{x}_{t}=\mathbf{R}_{t}/\langle\mathbf{R}_{t},\mathbf{a}\rangle\)
5: Observe the loss vector \(\mathbf{\ell}_{t}\in\mathbb{R}^{n+1}\)
6:\(\mathsf{Regmin}\) observes \(\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell}_{t})\in\mathbb{R}^{n+1}\)
7:\(\mathbf{R}_{t+1}=\mathsf{Regmin}\left(\cdot\right)\)
```

**Algorithm 1** Blackwell approachability on the treeplex

By convention that \(\mathbf{0}/0\) is the uniform strategy for the treeplex. Algorithm 1 is the first Blackwell approachability-based algorithm operating on the entire treeplex (in contrast to \(\mathsf{CFR}^{*}\) which relies on Blackwell approachability locally at the infosets level). We first describe some important properties of Algorithm 1:

Feasibility of the iterates.Algorithm 1 produces feasible strategies, i.e., \(\mathbf{x}_{t}\in\mathcal{T},\forall\ t\geq 1\). Indeed, since \(\mathsf{Regmin}\) is a regret minimizer with \(\mathcal{C}\) as the decision set, \(\mathbf{R}_{t}\in\mathsf{cone}(\mathcal{T})\), i.e., \(\mathbf{R}_{t}=\alpha\mathbf{z}\) with \(\alpha\in\mathbb{R}_{+}\) and \(\mathbf{z}\in\mathcal{T}\). From (9), we have \(\langle\mathbf{z},\mathbf{a}\rangle=1\). Therefore, \(\mathbf{x}_{t}=\frac{\mathbf{R}_{t}}{\langle\mathbf{R}_{t},\mathbf{a}\rangle}=\frac{\alpha\bm {z}}{\alpha\langle\mathbf{z},\mathbf{a}\rangle}=\mathbf{z}\in\mathcal{T}\). This is analogous to \(\mathsf{RM}^{*}\), where \(\mathbf{x}_{t}\) is proportional to \(\mathbf{R}_{t}\), see (4) and Figure 1.

Hyperplane forcing.For any \(t\in\mathbb{N}\) we have

\[\langle\mathbf{R}_{t},f(\mathbf{x}_{t},\mathbf{\ell})\rangle=0,\forall\,\mathbf{\ell}\in \mathbb{R}^{n+1}. \tag{12}\]

The hyperplane forcing equation (12) is a crucial component of algorithms based on Blackwell approachability. It ensures that \(\|\mathbf{R}_{t}\|_{2}=O(\sqrt{T})\). Equation (12) is analogous to (8) for \(\mathsf{RM}^{*}\) and follows from \(\mathbf{x}_{t}=\frac{\mathbf{R}_{t}}{\langle\mathbf{R}_{t},\mathbf{a}\rangle}\), so that

\[\langle\mathbf{R}_{t},\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell})\rangle=\langle\mathbf{R}_{t},\mathbf{ \ell}\rangle-\langle\mathbf{x}_{t},\mathbf{\ell}\rangle\langle\mathbf{R}_{t},\mathbf{a}\rangle =\langle\mathbf{R}_{t},\mathbf{\ell}\rangle-\langle\frac{\mathbf{R}_{t}}{\langle\mathbf{R}_{t },\mathbf{a}\rangle},\mathbf{\ell}\rangle\langle\mathbf{R}_{t},\mathbf{a}\rangle=\langle\mathbf{R }_{t},\mathbf{\ell}\rangle-\langle\mathbf{R}_{t},\mathbf{\ell}\rangle=0.\]

Regret minimization over \(\mathcal{T}\).Algorithm 1 always yields a regret minimizer over the treeplex \(\mathcal{T}\), i.e., it ensures that the regret of \(\mathbf{x}_{1},...,\mathbf{x}_{T}\in\mathcal{T}\) against any \(\mathbf{\ell}_{1},...,\mathbf{\ell}_{T}\in\mathbb{R}^{n+1}\) is bounded by \(O(\sqrt{T})\). The proof is instructive and shows a central component to Blackwell approachability-based algorithms: minimizing regret over \(\mathcal{T}\) can be achieved by minimizing regret over \(\mathsf{cone}(\mathcal{T})\).

**Proposition 3.1**.: _Let \(\mathsf{Regmin}\) be a regret minimizer with \(\mathcal{C}\) as the decision set. Let \(\mathbf{x}_{1},...,\mathbf{x}_{T}\in\mathcal{T}\) be computed by Algorithm 1. Then \(\max_{\mathbf{x}\in\mathcal{T}}\sum_{t=1}^{T}\langle\mathbf{x}_{t}-\hat{\mathbf{x}},\mathbf{ \ell}_{t}\rangle=O(\sqrt{T})\)._Proof.: Let \(\hat{\mathbf{x}}\in\mathcal{T}\) and let us write \(\hat{\mathbf{R}}=\hat{\mathbf{x}}\). We have

\[\sum_{t=1}^{T}\langle\mathbf{x}_{t}-\hat{\mathbf{x}},\mathbf{\ell}_{t}\rangle=\sum_{t=1}^{T }\langle-\hat{\mathbf{x}},\mathbf{f}\left(\mathbf{x}_{t},\mathbf{\ell}_{t}\right)\rangle=\sum_{ t=1}^{T}\langle-\hat{\mathbf{R}},\mathbf{f}\left(\mathbf{x}_{t},\mathbf{\ell}_{t}\right)\rangle= \sum_{t=1}^{T}\langle\mathbf{R}_{t}-\hat{\mathbf{R}},\mathbf{f}\left(\mathbf{x}_{t},\mathbf{\ell}_{ t}\right)\rangle\]

where the first equality follows from the definition of \(\mathbf{f}\left(\mathbf{x}_{t},\mathbf{\ell}_{t}\right)\) and \(\langle\mathbf{z},\mathbf{a}\rangle=1\) for any \(\mathbf{z}\in\mathcal{T}\), the second equality is because \(\hat{\mathbf{x}}=\hat{\mathbf{R}}\), and the last equality follows from the hyperplane forcing condition (12). Now note that \(\sum_{t=1}^{T}\langle\mathbf{R}_{t}-\hat{\mathbf{R}},\mathbf{f}\left(\mathbf{x}_{t},\mathbf{\ell}_ {t}\right)\rangle\) is the regret of a regret minimizer Regmin choosing \(\mathbf{R}_{1},...,\mathbf{R}_{T}\) in the decision set \(\mathcal{C}:=\mathsf{cone}(\mathcal{T})\) against a sequence of loss \(\mathbf{f}\left(\mathbf{x}_{1},\mathbf{\ell}_{1}\right),...,\mathbf{f}\left(\mathbf{x}_{T},\mathbf{ \ell}_{T}\right)\) and a comparator \(\hat{\mathbf{R}}\in\mathsf{cone}(\mathcal{T})\). Therefore, \(\sum_{t=1}^{T}\langle\mathbf{R}_{t}-\hat{\mathbf{R}},\mathbf{f}\left(\mathbf{x}_{t},\mathbf{\ell}_ {t}\right)\rangle=O(\sqrt{T})\). 

**Remark 3.2**.: _In their seminal paper, Abernethy et al. [1] show a general reduction from regret minimization to Blackwell approachability for compact convex decision sets. Our reduction from Algorithm 1 builds upon the ideas in [1], but our reduction is different and exploits the structure of treeplexes. Additionally, [1] focuses on the case of adversarial losss, whereas we focus on solving EFGs, where stepsize invariance properties is crucial and where we can prove fast \(O(1/T)\) convergence rates. We provide a more detailed comparison with [1] in Appendix C._

## 4 Instantiations of Algorithm 1

We can instantiate Algorithm 1 with any regret minimizer over \(\mathcal{C}\) to obtain various properties such as stepsize invariance or achieving \(O(1/T)\) convergence rate. We show next how to do so.

**Predictive Treeplex Blackwell\({}^{+}\)** (\(\mathsf{PTB}^{*}\)).** We first introduce _Predictive Treeplex Blackwell\({}^{+}\)_ (Algorithm 2), combining Algorithm 1 with \(\mathsf{POMD}\) with \(\mathcal{C}\) as a decision set.

```
1:Input: \(\eta>0\), \(\mathbf{m}_{1},...,\mathbf{m}_{T}\in\mathbb{R}^{n+1}\)
2:Initialization: \(\hat{\mathbf{R}}_{1}=\mathbf{0}\in\mathbb{R}^{n+1}\)
3:for\(t=1,\dots,T\)do
4:\(\mathbf{R}_{t}\in\Pi_{\mathcal{C}}\left(\hat{\mathbf{R}}_{t}-\eta\mathbf{m}_{t}\right)\)
5:\(\mathbf{x}_{t}=\mathbf{R}_{t}/\langle\mathbf{R}_{t},\mathbf{a}\rangle\)
6: Observe the loss vector \(\mathbf{\ell}_{t}\in\mathbb{R}^{n+1}\)
7:\(\hat{\mathbf{R}}_{t+1}\in\Pi_{\mathcal{C}}\left(\hat{\mathbf{R}}_{t}-\eta\mathbf{f}(\mathbf{x} _{t},\mathbf{\ell}_{t})\right)\)
```

**Algorithm 3**Smooth \(\mathsf{PTB}^{*}\)

We start by highlighting a crucial property of \(\mathsf{PTB}^{*}\), _treeplex stepsize invariance._ The sequence of iterates \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) generated by Algorithm 2 is independent of the choice of the stepsize \(\eta>0\), that only rescales the sequences \(\hat{\mathbf{R}}_{1},...,\hat{\mathbf{R}}_{T}\) and \(\mathbf{R}_{1},...,\mathbf{R}_{T}\), the orthogonal projection onto a cone is positively homogeneous of degree 1: \(\Pi_{\mathcal{C}}(\eta\mathbf{z})=\eta\Pi_{\mathcal{C}}(\mathbf{z})\) for \(\eta>0\) and \(\mathbf{z}\in\mathbb{R}^{n+1}\), and the function \(\mathbf{R}\mapsto\mathbf{R}/\langle\mathbf{R},\mathbf{a}\rangle\) is scale-invariant: \(\frac{\langle\eta\mathbf{R}\rangle}{(\langle\eta\mathbf{R}\rangle,\mathbf{a})}=\frac{\mathbf{R }}{\langle\mathbf{R},\mathbf{a}\rangle}\) for \(\eta>0\) and \(\mathbf{R}\in\mathbb{R}^{n+1}\). We provide a rigorous statement in the following proposition and we present the proof in Appendix D.

**Proposition 4.1**.: _The sequence \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) computed by \(\mathsf{PTB}^{*}\) is independent on the stepsize \(\eta>0\)._

Treeplex stepsize invariance is a crucial property, since in large EFGs, stepsize tuning is difficult and resource-consuming. This is the main advantage of using Blackwell approachability: running \(\mathsf{POMD}\) directly on the treeplex \(\mathcal{T}\) does not result in a stepsize invariant algorithm, whereas \(\mathsf{PTB}^{*}\) runs \(\mathsf{POMD}\) on \(\mathsf{cone}(\mathcal{T})\) and is stepsize invariant. To our knowledge, \(\mathsf{CFR}^{*}\) and \(\mathsf{PCFR}^{*}\) are the only other treeplex stepsize invariant algorithms for solving EFGs. In fact, they satisfy a stronger _infoset stepsize invariance_ property: different stepsizes can be used at different infosets, and the iterates do not depend on their values. We discuss the relation between \(\mathsf{PTB}^{*}\) and known instantiations of Blackwell approachability over the simplex (\(\mathsf{RM}^{*}\) and \(\mathsf{CBA}^{*}\)[22]) in Appendix E.

From Proposition 3.1 and the regret bounds on \(\mathsf{POMD}\) (see for instance section 3.1.1 in [34] or section 6 in [16]), we obtain the following proposition. We define \(\Omega\in\mathbb{R}_{+}\) as \(\Omega:=\max_{\mathbf{x}\in\mathcal{T}}\|\mathbf{x}\|_{2}\).

**Proposition 4.2**.: _Let \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) be computed by \(\mathsf{PTP}^{*}\). Then \(\max_{\mathbf{x}\in\mathcal{T}}\sum_{t=1}^{T}\langle\mathbf{x}_{t}-\hat{\mathbf{x}},\mathbf{ \ell}_{t}\rangle\leq\Omega\sqrt{\sum_{t=1}^{T}\|\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell}_{t} )-\mathbf{m}_{t}\|_{2}^{2}}\)._

From Proposition 4.2, \(\mathsf{PTB}^{*}\) is a regret minimizer over treeplexes, and we can combine it with the self-play framework to solve EFGs, as shown in the next corollary. We use the notations \(d:=\max\{n,m\}+1,\hat{\Omega}:=\max\{\|\mathbf{z}\|_{2}\mid\mathbf{z}\in\mathcal{X} \cup\mathcal{Y}\},\|\mathbf{M}\|_{2}:=\sup_{\mathbf{v}\neq\mathbf{0}}\frac{\|\mathbf{M}\mathbf{v} \|_{2}}{\|\mathbf{v}\|_{2}}\).

**Corollary 4.3**.: _Let \((\mathbf{x}_{t})_{t\geq 1}\) and \((\mathbf{y}_{t})_{t\geq 1}\) be the sequence of strategies computed by both players employing \(\mathsf{PTB}^{*}\) in the self-play framework, with previous losses as predictions: \(\mathbf{m}_{t}^{\mathbf{x}}=\mathbf{f}(\mathbf{x}_{t-1},\mathbf{M}\mathbf{y}_{t-1}),\mathbf{m}_{t}^{\mathbf{y} }=\mathbf{f}(\mathbf{y}_{t-1},-\mathbf{M}^{\top}\mathbf{x}_{t-1})\). Let \((\bar{\mathbf{x}}_{T},\bar{\mathbf{y}}_{T})=\frac{1}{T}\sum_{t=1}^{T}(\mathbf{x}_{t},\mathbf{y} _{t})\). Then_

\[\max_{\mathbf{y}\in\mathcal{Y}}\,\langle\bar{\mathbf{x}}_{T},\mathbf{M}\mathbf{y}\rangle-\min_{ \mathbf{x}\in\mathcal{X}}\,\langle\mathbf{x},\mathbf{M}\bar{\mathbf{y}}_{T}\rangle\leq\frac{ \hat{\Omega}^{3}\sqrt{d}\sqrt{\|\mathbf{M}\|_{2}}}{\sqrt{T}}.\]

Finally, we can efficiently compute the orthogonal projection onto \(\mathcal{C}\), since \(\mathcal{C}\) admits the following simple formulation of as a polytope: \(\mathcal{C}=\{\mathbf{x}\in\mathbb{R}_{+}^{n+1}\mid\,\sum_{a\in\mathcal{A}_{j}}x_{ ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\}\).

**Proposition 4.4**.: _Let \(\mathcal{T}\) be a treeplex with depth \(d\), number of sequences \(n\), number of leaf sequences \(l\), and number of infostes \(m\). The orthogonal projection \(\Pi_{\mathcal{C}}(\mathbf{y})\) of a point \(\mathbf{y}\in\mathbb{R}^{n+1}\) onto \(\mathcal{C}=\mathsf{cone}(\mathcal{T})\) can be computed in \(O(dn\log(l+m))\) arithmetic operations._

**A stable algorithm:**Smooth \(\mathsf{PTB}^{*}\). We now modify \(\mathsf{PTB}^{*}\) to obtain faster convergence rates. The \(O(1/\sqrt{T})\) average convergence rate of \(\mathsf{PTB}^{*}\) may seem surprising since in the _matrix game_ setting, \(\mathsf{POMD}\) over the simplexes obtains a \(O(1/T)\) average convergence [36]. This discrepancy comes from \(\mathsf{PTB}^{*}\) running \(\mathsf{POMD}\)_on the set \(\mathcal{C}=\mathsf{cone}(\mathcal{T})\)_ instead of the original decision set \(\mathcal{T}\), so that the Lipschitz continuity of the loss function and the classical _RVU bounds_ (Regret Bounded by Variation in Utilities, see Equation (1) in [36]), central to proving the fast convergence of predictive algorithms, may not hold. For \(\mathsf{PTB}^{*}\), the Lipschitz continuity of the loss \(\mathbf{R}\mapsto\mathbf{f}(\mathbf{x},\mathbf{\ell})\) with \(\mathbf{x}=\mathbf{R}/\langle\mathbf{R},\mathbf{a}\rangle\) depends on the Lipschitz continuity of the decision function \(\mathbf{R}\mapsto\mathbf{R}/\langle\mathbf{R},\mathbf{a}\rangle\) over \(\mathcal{C}\), which we analyze next.

**Proposition 4.5**.: _Let \(\mathbf{R}_{1},\mathbf{R}_{2}\in\mathsf{cone}(\mathcal{T})\). Then \(\left\|\frac{\mathbf{R}_{1}}{\langle\mathbf{R}_{1},\mathbf{a}\rangle}-\frac{\mathbf{R}_{2}}{ \langle\mathbf{R}_{2},\mathbf{a}\rangle}\right\|_{2}\leq\frac{\Omega\cdot\|\mathbf{R}_{1}- \mathbf{R}_{2}\|_{2}}{\max\{\langle\mathbf{R}_{1},\mathbf{a}\rangle,\langle\mathbf{R}_{2},\mathbf{ a}\rangle\}}\)._

We present the proof of Proposition 4.5 in Appendix F. Proposition 4.5 shows that when the vector \(\mathbf{R}\) is such that \(\langle\mathbf{R},\mathbf{a}\rangle\) is small, the decision function \(\mathbf{R}\mapsto\mathbf{R}/\langle\mathbf{R},\mathbf{a}\rangle\) may vary rapidly, an issue known as _instability_ and also observed for a predictive variant of \(\mathsf{RM}^{*}\)[18]. To ensure the Lipschitzness of the decision function, we can ensure that \(\mathbf{R}_{t}\) and \(\hat{\mathbf{R}}_{t}\) always belong to the _stable region_\(\mathcal{C}_{\geq}\):

\[\mathcal{C}_{\geq}:=\mathsf{cone}(\mathcal{T})\cap\{\mathbf{R}\in\mathbb{R}^{n+1 }\mid\langle\mathbf{R},\mathbf{a}\rangle\geq R_{0}\}\]

for \(R_{0}>0\), and we recover Lipschitz continuity over \(\mathcal{C}_{\geq}\):

\[\left\|\frac{\mathbf{R}_{1}}{\langle\mathbf{R}_{1},\mathbf{a}\rangle}-\frac{\mathbf{R}_{2}}{ \langle\mathbf{R}_{2},\mathbf{a}\rangle}\right\|_{2}\leq\frac{\Omega}{R_{0}}\|\mathbf{R}_{ 1}-\mathbf{R}_{2}\|_{2},\forall\;\mathbf{R}_{1},\mathbf{R}_{2}\in\mathcal{C}_{\geq}.\]

This leads us to introduce Smooth \(\mathsf{PTB}^{*}\)(Algorithm 3), a variant of \(\mathsf{PTB}^{*}\),where \(\mathbf{R}_{t}\) and \(\hat{\mathbf{R}}_{t}\) always belong to \(\mathcal{C}_{\geq}\). For \(\mathsf{Smooth}\)\(\mathsf{PTB}^{*}\),\(\mathbf{x}_{t}\in\mathcal{T}\) since \(\mathbf{R}_{t}\in\mathcal{C}_{\geq}\subset\mathsf{cone}(\mathcal{T})\), and we also have the hyperplane forcing property (12), which only depends on \(\mathbf{x}_{t}=\mathbf{R}_{t}/\langle\mathbf{R}_{t},\mathbf{a}\rangle\). However, \(\mathsf{Smooth}\)\(\mathsf{PTB}^{*}\) is not treeplex stepsize invariant, because the orthogonal projections are onto \(\mathcal{C}_{\geq}\), which is not a cone. Note that \(\mathcal{C}_{\geq}\) admits a simple polytope formulation:

\[\mathcal{C}_{\geq}=\{\mathbf{x}\in\mathbb{R}_{+}^{n+1}\mid x_{\varnothing}\geq R_{0 },\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\}\]

so the complexity of computing the orthogonal projection onto \(\mathcal{C}_{\geq}\) is the same as computing the orthogonal projection onto \(\mathcal{C}\). We provide a proof in Appendix H. We now show that \(\mathsf{Smooth}\)\(\mathsf{PTB}^{*}\) is a regret minimizer. Indeed, the proof of Proposition 3.1 can be adapted to relate the regret in \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) in \(\mathcal{T}\) to regret in \(\mathbf{R}_{1},...,\mathbf{R}_{T}\) in \(\mathcal{C}_{\geq}\).

**Proposition 4.6**.: _Let \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) be computed by \(\mathsf{Smooth}\)\(\mathsf{PTB}^{*}\). Let \(\eta=\frac{\sqrt{2t1}}{\sqrt{\sum_{t=1}^{T}\|f(\mathbf{x}_{t},\mathbf{\ell}_{t})-\mathbf{m}_{t} \|_{2}^{2}}}\). Then \(\max_{\mathbf{\hat{x}}\in\mathcal{T}}\sum_{t=1}^{T}\langle\mathbf{x}_{t}-\hat{\mathbf{x}}, \mathbf{\ell}_{t}\rangle\leq\Omega\sqrt{\sum_{t=1}^{T}\|f(\mathbf{x}_{t},\mathbf{\ell}_{t} )-\mathbf{m}_{t}\|_{2}^{2}}\)._In Smooth \(\tt PTB^{*}\)\(\mathbf{R}_{t}\) and \(\mathbf{\hat{R}}_{t}\) always belong to \(\mathcal{C}_{\geq}\), and we are able to recover a RVU bound and show faster convergence. We let \(\|\mathbf{M}\|\) be the maximum \(\ell_{2}\)-norm of any column and any row of \(\mathbf{M}\).

**Theorem 4.7**.: _Let \((\mathbf{x}_{t})_{t\geq 1}\) and \((\mathbf{y}_{t})_{t\geq 1}\) be the sequence of strategies computed by both players employing \(\tt Smooth\)\(\tt PTB^{*}\) in the self-play framework, with previous losses as predictions: \(\mathbf{m}_{t}^{x}=\mathbf{f}(\mathbf{x}_{t-1},\mathbf{M}\mathbf{y}_{t-1}),\mathbf{m}_{t}^{y}=\mathbf{f}( \mathbf{y}_{t-1},-\mathbf{M}^{\top}\mathbf{x}_{t-1})\). Let \(\eta=\frac{R_{0}}{\sqrt{\delta\delta\delta\|\mathbf{M}\|}}\) and \((\bar{\mathbf{x}}_{T},\bar{\mathbf{y}}_{T})=\frac{1}{T}\sum_{t=1}^{T}{(\mathbf{x}_{t},\mathbf{ y}_{t})}\). Then \(\max_{\mathbf{y}\in\mathcal{Y}}\left\langle\bar{\mathbf{x}}_{T},\mathbf{M}\mathbf{y}\right\rangle- \min_{\mathbf{x}\in\mathcal{X}}\left\langle\mathbf{x},\mathbf{M}\bar{\mathbf{y}}_{T}\right\rangle \leq\frac{2\Omega^{2}}{\eta}\frac{1}{T}\)._

We present the proof of Theorem 4.7 in Appendix I. To the best of our knowledge, \(\tt Smooth\)\(\tt PTB^{*}\) is the first algorithm based on Blackwell approachability achieving a \(O(1/T)\) convergence rate for solving EFGs as in (1), answering an important open question. However, achieving the faster rate in \(\tt Smooth\)\(\tt PTB^{*}\) requires introducing a stepsize, a situation similar to all other \(O(1/T)\)-methods for EFGs, like Mirror Prox and Excessive Gap Technique for EFGs [27] and predictive OMD directly on the treeplex [14]. We can compare the \(O(1/T)\) convergence rate of \(\tt Smooth\)\(\tt PTB^{*}\) with the \(O(1/\sqrt{T})\) convergence rate of Predictive \(\tt CFR^{*}\)[16], which combines CFR with Predictive \(\tt RM^{*}\) (see Appendix B). Despite its predictive nature, Predictive \(\tt CFR^{*}\) only achieves a \(O(1/\sqrt{T})\) convergence rate because of the CFR decomposition, which enables running regret minimizers _independently and locally_ at each infoset, and it is not clear how to combine, at the treeplex level, the regret bounds obtained at each infoset. Since \(\tt Smooth\)\(\tt PTB^{*}\) operates over the entire treeplex, we can combine the RVU bound for each player to obtain a \(O(1/T)\) convergence rate.

**Remark 4.8**.: _The Clairvoyant CFR algorithm from [18] is based on Blackwell approachability over simplexes, combined with the CFR decomposition and a Mirror Prox-style update [31]. For solving EFGs, Clairvoyant CFR achieves a \(O(\log(T)/T))\) convergence rate, slower than the \(O(1/T)\) convergence rate of \(\tt Smooth\)\(\tt PTB^{*}\), where the additional \(\log(T)\) factor occurs because each outer iteration of Clairvoyant CFR itself solves an approximate fixed-point problem._

For completeness, we also instantiate Algorithm 1 with regret minimizers that learn heterogeneous stepsizes across information sets in an adaptive fashion. This results in \(\tt AdaGradTB^{*}\)(Algorithm 6) and \(\tt AdamTB^{*}\)(Algorithm 7), which adapt the scale of the stepsizes for each dimension to the magnitude of the observed gradients for this dimension based on \(\tt AdaGrad\)[12] and \(\tt Adam\)[25]. This may be useful if the losses across different dimensions differ in magnitudes, but the stepsizes decrease over time, which could be conservative. These algorithms are presented in Appendix J

**Remark 4.9** (Comparison with Lagrangian Hedging).: _Algorithm 1 is related to Lagrangian Hedging [21, 11]. Lagrangian Hedging builds upon Blackwell approachability with various potential functions to construct regret minimizers for general decision sets. As explained in the introduction, the main focus of our paper is on two-player zero-sum EFGs, i.e., on the case where the decision sets are treeplexes, and where we can obtain several additional interesting properties not studied in [21, 11], such as stepsize invariance, fast convergence rates, and efficient projection, as we detail in the next section. If one were to instantiate Algorithm 1 with the Follow-The-Regularized Leader algorithm, it would yield the regret minimizer for treeplexes studied in Gordon [21], and our Proposition 4.4 in the next section yields an efficient projection oracle for the setup in Gordon [21], which appealed to general convex optimization as an oracle._

## 5 Numerical Experiments

Figure 2: Convergence to a Nash equilibrium for \(\tt PTB^{*}\), \(\tt CFR^{*}\), \(\tt PCFR^{*}\) and \(\tt SC\)-POMD. All algorithms use alternation and quadratic averaging except \(\tt CFR^{*}\) instantiated with linear averaging.

We conduct two sets of numerical experiments to investigate the performance of our algorithms for solving several two-player zero-sum EFG benchmark games: Kuhn poker, Leduc poker, Liar's Dice, Goofspiel and Battleship. Additional experimental detail is given in Appendix K.

We first determine the best instantiatons our framework. We compare PTB*, Smooth PTB* and AdaGradTB* in the self-play framework with alternation (see Appendix A) and uniform, linear or quadratic weights for the iterates. PTB* and Smooth PTB* use the previous losses as predictions. We also study _Treeplex Blackwell_+ (TB*), corresponding to PTB* without predictions (\(\boldsymbol{m}_{t}=\boldsymbol{0}\)), and AdamTB*. For conciseness, we present our plots in Appendix K.3 (Figure 4) and state our conclusion here. We find that, for every game, PTB* performs the best or is among the best algorithms. This underlines the advantage of _treeplex stepsize invariance_ over algorithms that require tuning a stepsize (Smooth PTB*) and adaptive algorithms (AdaGradTB*), which may perform poorly due to the stepsize decreasing at a rate of \(O(1/\sqrt{T})\). AdamTB* does not even converge in some games.

We then compare the best of our algorithms (PTB*) with some of the best existing methods for solving EFGs: CFR*[37], predictive CFR* (PCFR*, [16], see Appendix B), and a version of optimistic online mirror descent with a single call to the orthogonal projection at every iteration (SC-POMD, [24]) achieving a \(O(1/T)\) convergence rate; there are a variety of FOMs with a \(O(1/T)\) rate, SC-POMD was observed to perform well in [8]. We determine the best empirical setup for each algorithm in Appendix K.4. In Figure 2, we compare the performance of the (weighted) average iterates. We find that PCFR* outperforms both CFR* and the theoretically-faster SC-POMD, as expected from past work. We had hoped to see at least comparable performance between PTB* and PCFR*, since they are both based on Blackwell-approachability regret minimizers derived from applying POMD on the conic hull of their respective decision sets (simplexes at each infoset for PCFR*, treeplexes of each player for PTB*). However, in some games PCFR* performs much better than PTB*. Given the similarity between PTB* and PCFR*, our results suggest that the use of the CFR decomposition is part of the key to the performance of PCFR*. The CFR decomposition allows PCFR* to have stepsize invariance _at an infoset level_, as opposed to stepsize invariance at the treeplex level in PTB*. Because of the structure of treeplexes, the numerical values of variables associated with infosets appearing late in the game, i.e., deeper in the treeplexes, may be much smaller than the numerical values of the variables appearing closer to the root. For this reason, allowing for different stepsizes at each infosets (like CFR* and PCFR* do) appears to be more efficient than using a single stepsize across all the infosets, even when the iterates do not depend on the value of this single stepsize (like in PTB*) and when this stepsize is fine-tuned (like in SC-POMD). Of course one could try to run SC-POMD with different stepsizes at each infoset and attempt to tune each of these stepsizes, but this is impossible in practical instances where the number of actions is large, e.g., \(4.9\times 10^{4}\) actions in _Liar's Dice_ and \(5.3\times 10^{6}\) actions in _Goofspiel_. CFR* and PCFR* bypass this issue with their infoset stepsize invariance, which enables both each infoset to have its own stepsize (via the CFR decomposition) _and_ not needing to choose these stepsizes (via using RM* and PRM* as local regret minimizers, which are stepsize invariant).

We also investigate the performance of the _last iterates_ in Figure 3. No algorithm appears to be the best across all game instances. CFR* may not converge to a Nash equilibrium (e.g., on Kuhn), as has been observed before [29]. PCFR* exhibits linear convergence in some games (Kuhn, Liar's Dice, Goofspiel) but not others (Leduc). The same is true for PTB*. Further investigations about last-iterate convergence are left as an important open question.

Figure 3: Convergence to a Nash equilibrium for the last iterates of PTB*, CFR*, PCFR*, and SC-POMD. Every algorithm is using alternation.

Conclusion

We propose the first Blackwell approachability-based regret minimizer over the treeplex (Algorithm 1) and we give several instantiations of our framework with different properties, including treeplex stepsize invariance (\(\mathsf{PTB}^{*}\)), adaptive stepsizes (\(\mathsf{AdaGradTB}^{*}\)) and achieving \(O(1/T)\) convergence rates on EFGs with a Blackwell approachability-based algorithm for the first time (\(\mathsf{Smooth\ PTB}^{*}\)). Since \(\mathsf{CFR}^{*}\) and \(\mathsf{PCFR}^{*}\) are stepsize invariant and have strong empirical performance, we were expecting \(\mathsf{PTB}^{*}\) to have comparable performance. However, our experiments show that \(\mathsf{PTB}^{*}\) often converges slower than \(\mathsf{CFR}^{*}\) and \(\mathsf{PCFR}^{*}\), so this treeplex stepsize invariance is not the only driver behind the practical performance of \(\mathsf{CFR}^{*}\) and \(\mathsf{PCFR}^{*}\). We view this negative result as an important contribution of our paper, since it rules out a previously plausible explanation for the practical performance of \(\mathsf{CFR}^{*}\). Instead, we propose that one piece of the puzzle behind the \(\mathsf{CFR}^{*}\) and \(\mathsf{PCFR}^{*}\) performances is their _infoset_ stepsize invariance, a consequence of combining the CFR framework with Blackwell approachability-based regret minimizers (\(\mathsf{RM}^{*}\) and \(\mathsf{PRM}^{*}\), themselves stepsize invariant over simplexes). Future directions include better understanding the last-iterate performance of algorithms based on Blackwell approachability as well as the role of alternation. It would also be interesting to explore EFG applications of new reductions between Blackwell approachability and regret minimization [10] (which differs from the reduction in [2]) and Blackwell approachability generalizations based on various norms and pseudo-norms [28, 9], potentially to obtain better stepsize invariance properties.

## Acknowledgments and Disclosure of Funding

Darshan Chakrabarti was supported by the National Science Foundation Graduate Research Fellowship Program under award number DGE-2036197. Julien Grand-Clement was supported by Hi! Paris and Agence Nationale de la Recherche (Grant 11-LABX-0047). Christian Kroer was supported by the Office of Naval Research awards N00014-22-1-2530 and N00014-23-1-2374, and the National Science Foundation awards IIS-2147361 and IIS-2238960.

## References

* [1] J. Abernethy, P. L. Bartlett, and E. Hazan. Blackwell approachability and no-regret learning are equivalent. In _Proceedings of the 24th Annual Conference on Learning Theory_, pages 27-46. JMLR Workshop and Conference Proceedings, 2011.
* [2] J. D. Abernethy, E. Hazan, and A. Rakhlin. Competing in the dark: An efficient algorithm for bandit linear optimization. 2009.
* [3] D. Blackwell. An analog of the minimax theorem for vector payoffs. _Pacific Journal of Mathematics_, 6(1):1-8, 1956.
* [4] N. Brown and T. Sandholm. Superhuman AI for heads-up no-limit poker: Libratus beats top professionals. _Science_, 359(6374):418-424, 2018.
* [5] N. Brown and T. Sandholm. Superhuman AI for multiplayer poker. _Science_, 365(6456):885-890, 2019.
* [6] N. Brown, C. Kroer, and T. Sandholm. Dynamic thresholding and pruning for regret minimization. In _Proceedings of the AAAI conference on artificial intelligence_, volume 31, 2017.
* [7] N. Burch, M. Moravcik, and M. Schmid. Revisiting CFR+ and alternating updates. _Journal of Artificial Intelligence Research_, 64:429-443, 2019.
* [8] D. Chakrabarti, J. Diakonikolas, and C. Kroer. Block-coordinate methods and restarting for solving extensive-form games. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [9] C. Dann, Y. Mansour, M. Mohri, J. Schneider, and B. Sivan. Pseudonorm approachability and applications to regret minimization. In _International Conference on Algorithmic Learning Theory_, pages 471-509. PMLR, 2023.

* [10] C. Dann, Y. Mansour, M. Mohri, J. Schneider, and B. Sivan. Rate-preserving reductions for blackwell approachability. _arXiv preprint arXiv:2406.07585_, 2024.
* [11] R. D'Orazio and R. Huang. Optimistic and adaptive lagrangian hedging. In _Thirty-fifth AAAI conference on artificial intelligence_, 2021.
* [12] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7), 2011.
* [13] G. Farina, C. Kroer, and T. Sandholm. Online convex optimization for sequential decision processes and extensive-form games. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 1917-1925, 2019.
* [14] G. Farina, C. Kroer, and T. Sandholm. Optimistic regret minimization for extensive-form games via dilated distance-generating functions. In _Advances in Neural Information Processing Systems_, pages 5222-5232, 2019.
* [15] G. Farina, C. Kroer, and T. Sandholm. Better regularization for sequential decision spaces fast convergence rates for Nash, correlated, and team equilibria. In _EC'21: Proceedings of the 22nd ACM Conference on Economics and Computation_, 2021.
* [16] G. Farina, C. Kroer, and T. Sandholm. Faster game solving via predictive Blackwell approachability: Connecting regret matching and mirror descent. In _Proceedings of the AAAI Conference on Artificial Intelligence_. AAAI, 2021.
* [17] G. Farina, I. Anagnostides, H. Luo, C.-W. Lee, C. Kroer, and T. Sandholm. Near-optimal no-regret learning dynamics for general convex games. _Advances in Neural Information Processing Systems_, 35:39076-39089, 2022.
* [18] G. Farina, J. Grand-Clement, C. Kroer, C.-W. Lee, and H. Luo. Regret matching+: (in)stability and fast convergence in games. In _Advances in Neural Information Processing Systems_, 2023.
* [19] Y. Freund and R. E. Schapire. Adaptive game playing using multiplicative weights. _Games and Economic Behavior_, 29(1-2):79-103, 1999.
* [20] A. Gilpin, J. Pena, and T. Sandholm. First-order algorithm with convergence for-equilibrium in two-person zero-sum games. _Mathematical programming_, 133(1-2):279-298, 2012.
* [21] G. J. Gordon. No-regret algorithms for online convex programs. In _Advances in Neural Information Processing Systems_, pages 489-496. Citeseer, 2007.
* [22] J. Grand-Clement and C. Kroer. Solving optimization problems with Blackwell approachability. _Mathematics of Operations Research_, 2023.
* [23] S. Hoda, A. Gilpin, J. Pena, and T. Sandholm. Smoothing techniques for computing Nash equilibria of sequential games. _Mathematics of Operations Research_, 35(2):494-512, 2010.
* [24] P. Joulani, A. Gyorgy, and C. Szepesvari. A modular analysis of adaptive (non-) convex optimization: Optimism, composite objectives, and variational bounds. In _International Conference on Algorithmic Learning Theory_, pages 681-720. PMLR, 2017.
* [25] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations, ICLR_, 2015.
* [26] C. Kroer, G. Farina, and T. Sandholm. Solving large sequential games with the excessive gap technique. In _Advances in Neural Information Processing Systems_, pages 864-874, 2018.
* [27] C. Kroer, K. Waugh, F. Kilinc-Karzan, and T. Sandholm. Faster algorithms for extensive-form game solving via improved smoothing functions. _Mathematical Programming_, pages 1-33, 2020.
* [28] J. Kwon. Refined approachability algorithms and application to regret minimization with global costs. _Journal of Machine Learning Research_, 22(200):1-38, 2021.

* [29] C.-W. Lee, C. Kroer, and H. Luo. Last-iterate convergence in extensive-form games. _Advances in Neural Information Processing Systems_, 34:14293-14305, 2021.
* [30] M. Moravcik, M. Schmid, N. Burch, V. Lisy, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, and M. Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. _Science_, 356(6337):508-513, 2017.
* [31] A. Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems. _SIAM Journal on Optimization_, 15(1):229-251, 2004.
* [32] Y. Nesterov. Excessive gap technique in nonsmooth convex minimization. _SIAM Journal on Optimization_, 16(1):235-249, 2005.
* [33] V. Perchet. _Approachability, Calibration and Regret in Games with Partial Observations_. PhD thesis, PhD thesis, Universite Pierre et Marie Curie, 2010.
* [34] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In _Conference on Learning Theory_, pages 993-1019. PMLR, 2013.
* [35] S. J. Reddi, S. Kale, and S. Kumar. On the convergence of Adam and beyond. _International Conference on Learning Representations (ICLR)_, 2018.
* [36] V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in games. _Advances in Neural Information Processing Systems_, 28, 2015.
* [37] O. Tammelin, N. Burch, M. Johanson, and M. Bowling. Solving heads-up limit Texas hold'em. In _Twenty-Fourth International Joint Conference on Artificial Intelligence_, 2015.
* [38] B. von Stengel. Efficient computation of behavior strategies. _Games and Economic Behavior_, 14(2):220-246, 1996.
* [39] M. Zinkevich, M. Johanson, M. Bowling, and C. Piccione. Regret minimization in games with incomplete information. In _Advances in neural information processing systems_, pages 1729-1736, 2007.

Self-Play Framework

The (vanilla) self-play framework for two-player zero-sum EFGs is presented in Algorithm 4.

```
1:Input: \(\mathsf{Regmin}_{\mathcal{X}}\) a regret minimizer over \(\mathcal{X}\), \(\mathsf{Regmin}_{\mathcal{Y}}\) a regret minimizer over \(\mathcal{Y}\)
2:for\(t=1,\ldots,T\)do
3:\(\mathbf{x}_{t}=\mathsf{Regmin}_{\mathcal{X}}\left(\cdot\right)\)
4:\(\mathbf{y}_{t}=\mathsf{Regmin}_{\mathcal{Y}}\left(\cdot\right)\)
5: The first player observes the loss vector \(\mathbf{A}\mathbf{y}_{t}\in\mathbb{R}^{n_{1}+1}\)
6: The second player observes the loss vector \(-\mathbf{A}^{\top}\mathbf{x}_{t}\in\mathbb{R}^{n_{2}+1}\)
```

**Algorithm 4** self-play framework

The self-play framework can be combined with _alternation_, a simple variant that is known to lead to significant empirical speedups, for instance, when \(\mathsf{CFR}^{+}\) and predictive \(\mathsf{CFR}^{+}\) are used as regret minimizers [37, 16, 7]. When using alternation, at iteration \(t\) the second player is provided with the current strategy of the first player \(\mathbf{x}_{t}\) before choosing its own strategy. We describe the self-play framework with alternation in Algorithm 5.

```
1:Input: \(\mathsf{Regmin}_{\mathcal{X}}\) a regret minimizer over \(\mathcal{X}\), \(\mathsf{Regmin}_{\mathcal{Y}}\) a regret minimizer over \(\mathcal{Y}\)
2:for\(t=1,\ldots,T\)do
3:\(\mathbf{x}_{t}=\mathsf{Regmin}_{\mathcal{X}}\left(\cdot\right)\)
4: The second player observes the loss vector \(-\mathbf{A}^{\top}\mathbf{x}_{t}\in\mathbb{R}^{n_{2}+1}\)
5:\(\mathbf{y}_{t}=\mathsf{Regmin}_{\mathcal{Y}}\left(\cdot\right)\)
6: The first player observes the loss vector \(\mathbf{A}\mathbf{y}_{t}\in\mathbb{R}^{n_{1}+1}\)
```

**Algorithm 5** self-play framework with alternation

Appendix B Counterfactual Regret Minimization (CFR), \(\mathsf{CFR}^{+}\) and Predictive \(\mathsf{CFR}^{+}\)

Counterfactual Regret Minimization (CFR, [39]) is a framework for regret minimization over the treeplex. CFR runs a regret minimizer \(\mathsf{Regmin}_{j}\) locally at each infoset \(j\in\mathcal{J}\) of the treeplex. Note that here \(\mathsf{Regmin}_{j}\) is a regret minimizer over the _simplex_\(\Delta^{n_{j}}\) with \(n_{j}=|\mathcal{A}_{j}|\), i.e., over the set of probability distributions over \(\mathcal{A}_{j}\), the set of actions available at infoset \(j\in\mathcal{J}\). Let \(\mathbf{x}_{t}^{j}\in\Delta^{n_{j}}\) be the decision chosen by \(\mathsf{Regmin}_{j}\) at iteration \(t\) in CFR and let \(\mathbf{\ell}_{t}\in\mathbb{R}^{n+1}\) be the loss across the entire treeplex. The _local loss_\(\mathbf{\ell}_{t}^{j}\in\mathbb{R}^{n_{j}}\) that CFR passes to \(\mathsf{Regmin}_{j}\) is

\[\ell_{t,a}^{j}:=\ell_{t,(j,a)}+\sum_{j^{\prime}\in\mathcal{C}_{ja}}V_{t}^{j^{ \prime}},\forall\;a\in\mathcal{A}_{j},\forall\;j\in\mathcal{J}\]

where \(V_{t}^{j}\) is the _value function_ for infoset \(j\) at iteration \(t\), defined inductively:

\[V_{t}^{j}:=\sum_{a\in\mathcal{A}_{j}}x_{t,a}^{j}\ell_{t,(j,a)}+\sum_{j^{\prime }\in\mathcal{C}_{ja}}V_{t}^{j^{\prime}}.\]

The regret over the entire treeplex \(\mathcal{T}\) can be related to the regrets accumulated at each infoset via the following _laminar regret decomposition_[13]:

\[\mathsf{Reg}^{T}:=\max_{\hat{\mathbf{x}}\in\mathcal{T}}\sum_{t=1}^{T}\langle\mathbf{x }_{t}-\hat{\mathbf{x}},\mathbf{\ell}_{t}\rangle\leq\max_{\hat{\mathbf{x}}\in\mathcal{T}} \sum_{j\in\mathcal{J}}\hat{x}_{p_{j}}\mathsf{Reg}^{T}_{j}\left(\hat{\mathbf{x}}^{ j}\right)\]

with \(\mathsf{Reg}^{T}_{j}\left(\hat{\mathbf{x}}^{j}\right):=\sum_{t=1}^{T}\langle\mathbf{x }_{t}^{j}-\hat{\mathbf{x}}^{j},\mathbf{\ell}_{t}^{j}\rangle\) the regret incured by \(\mathsf{Regmin}^{j}\) for the sequence of losses \(\mathbf{\ell}_{1}^{j},...,\mathbf{\ell}_{T}^{j}\) against the comparison \(\hat{\mathbf{x}}^{j}\in\Delta^{n_{j}}\). Combining CFR with regret minimizers at each information set ensures \(\mathsf{Reg}^{T}=O\left(\sqrt{T}\right)\).

CFR\({}^{*}\)[37] corresponds to instantiating the self-play framework with alternation (Algorithm 5) and Regret Matching\({}^{+}\) (RM\({}^{*}\) as presented in (4)) as a regret minimizer at each information set. Additionally, CFR\({}^{*}\) uses _linear_ averaging, i.e., it returns \(\bar{\mathbf{x}}_{T}\) such that \(\bar{\mathbf{x}}_{T}=\frac{1}{\sum_{t=1}^{T}\omega_{t}}\sum_{t=1}^{T}\omega_{t}\mathbf{ x}_{t}\) with \(\omega_{t}=t\). We also consider uniform weights (\(\omega_{t}=1\)) and quadratic weights (\(\omega=t^{2}\)) in our simulations (Figure 10). CFR\({}^{*}\) guarantees a \(O(1/\sqrt{T})\) convergence rate to a Nash equilibrium.

Predictive CFR\({}^{*}\)(PCFR\({}^{*}\), [16]) corresponds to instantiating the self-play framework with alternation (Algorithm 5) and Predictive Regret Matching\({}^{+}\) (PRM\({}^{*}\)) as a regret minimizer at each information set. Given a simplex \(\Delta^{d}\), PRM\({}^{*}\) is a regret minimizer that returns a sequence of decisions \(\mathbf{z}_{1},...,\mathbf{z}_{T}\in\Delta^{d}\) as follows:

\[\begin{split}\hat{\mathbf{R}}_{t}&=\Pi_{\mathbb{R}_{+} ^{d}}\left(\mathbf{R}_{t}-\eta g(\mathbf{z}_{t-1},\mathbf{\ell}_{t-1})\right)\\ \mathbf{z}_{t}&=\hat{\mathbf{R}}_{t}/\|\hat{\mathbf{R}}_{t}\|_{1 },\\ \mathbf{R}_{t+1}&=\Pi_{\mathbb{R}_{+}^{d}}\left(\mathbf{R}_{t }-\eta g(\mathbf{z}_{t},\mathbf{\ell}_{t})\right)\end{split}\] (PRM\({}^{*}\))

where the function \(g\) is defined in (5). Similar to CFR\({}^{*}\), for PCFR\({}^{*}\) we investigate different weighting schemes in our numerical experiments (Figure 11). It is not known if the self-play framework with alternation, combined with PCFR\({}^{*}\), has convergence guarantees, but PCFR\({}^{*}\) has been observed to achieve state-of-the-art practical performance in many EFG instances [16].

## Appendix C Comparison with [1]

In this appendix, we describe the results in [1] connecting Blackwell approachability and regret minimization, and we highlight the main differences with our framework described in Algorithm 1.

In particular, Abernethy et al. [1] describe a meta-algorithm connecting Blackwell approachability and regret minimization. Given a decision set \(\mathcal{X}\subset\mathbb{R}^{n}\) assumed to be convex and compact, Abernethy et al. [1] consider a _lifted set_\(\tilde{\mathcal{X}}=\{\kappa\}\times\mathcal{X}\subset\mathbb{R}^{n+1}\) with

\[\kappa:=\max_{\mathbf{x}\in\tilde{\mathcal{X}}}\|\mathbf{x}\|_{2}.\]

Abernethy et al. [1] then constructs a regret minimizer over \(\mathcal{X}\) by considering a Blackwell approachability instance, where the target set is defined as \(\mathsf{cone}(\tilde{\mathcal{X}})\subset\mathbb{R}^{n+1}\), the decision set is \(\mathcal{X}\subset\mathbb{R}^{n}\), and the instantaneous loss at time \(t\) is \(\left(\frac{(\alpha_{t},\mathbf{\ell}_{t})}{\kappa},-\mathbf{\ell}_{t}\right)\in\mathbb{ R}^{n+1}\) with \(\mathbf{\ell}_{t}\in\mathbb{R}^{n}\) the loss vector when the decision maker chooses \(\mathbf{x}_{t}\in\mathcal{X}\). The _average aggregated loss vector_\(\mathbf{u}_{t}\in\mathbb{R}^{n+1}\) is updated using a regret minimizer over \(\mathsf{cone}(\tilde{\mathcal{X}})\) and the decision maker in the Blackwell approachability instance chooses the sequence of decisions \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) to ensure that \(\left(\frac{1}{T}\mathbf{u}_{T}\right)_{T\geq 1}\) approaches the _target set_, defined as the polar cone of \(\mathsf{cone}(\tilde{\mathcal{X}})\). We refer to Section 4 in [1] for more detail on this construction.

As evident from the description in the previous paragraph, our framework described in Algorithm 1 differs from the meta-algorithm from [1] in various ways. In particular, if one were to directly use the reduction from [1] for deriving regret minimizers over treeplexes, one would need to consider \(\tilde{\mathcal{T}}=\{\kappa\}\times\mathcal{T}\), with \(\mathcal{T}\) a treeplex and \(\kappa=\max_{\mathbf{x}\in\tilde{\mathcal{T}}}\|\mathbf{x}\|_{2}\), and one would need to consider a bound for the value of \(\kappa\), which could be conservative for large-scale EFG instances. However, since we have designed Algorithm 1 specifically for treeplexes as decision sets, we do _not_ need to lift the set \(\mathcal{T}\) by adding an additional dimension depending on the maximum \(\ell_{2}\)-norm over \(\mathcal{T}\). We can circumvent relying on \(\kappa\) and therefore obtain a simpler, more practical framework as in Algorithm 1 by exploiting the structure of treeplexes. This is because the variable \(x_{\varnothing}\) associated with the empty sequence always has a value of 1: \(x_{\varnothing}=1\), and we can then exploit the fact that \(\mathcal{T}\subset\{\mathbf{x}\in\mathbb{R}^{n+1}\,\big{|}\,\langle\mathbf{x},\mathbf{a} \rangle=1\}\), as in the proof of Proposition 3.1.

Another fundamental difference with [1] is our positioning and our objectives. Abernethy et al. [1] analyze Blackwell approachability with _adversial losses_ and in a more theoretical way (e.g., no implementations or simulations), whereas we focus on practically solving EFGs with Blackwell approachability, i.e., we focus on the game setting and on explaining the empirical performance of CFR\({}^{*}\). A direct application of the results in [1] would only result in algorithms achieving \(O(1/\sqrt{T})\) convergence rates, and for which no concrete implementations are known. In contrast, we provide details on the practical implementations of our algorithms (Proposition 4.4 and Appendix H), we are the first to highlight the role of stepsize invariance, which only makes sense for EFGs (and not for the adversarial loss setup as in [1]), and in our EFG applications we can obtain faster \(O(1/T)\) convergence rate (e.g. for Smooth PTB\({}^{*}\), see our new results as in Proposition 4.5 and Theorem 4.7), which is impossible against adversarial losses.

## Appendix D Proof of Proposition 4.1

Proof.: The proof of Proposition 4.1 is based on the following lemma.

**Lemma D.1**.: _Let \(\mathcal{C}\subset\mathbb{R}^{n}\) be a convex cone and let \(\mathbf{u}\in\mathbb{R}^{n},\eta>0\). Then_

\[\Pi_{\mathcal{C}}(\eta\mathbf{u})=\eta\Pi_{\mathcal{C}}(\mathbf{u}).\]

Proof of Lemma d.1.: We have, by definition,

\[\Pi_{\mathcal{C}}(\eta\mathbf{u})=\arg\min_{\mathbf{R}\in\mathcal{C}}\|\mathbf{R}-\eta\mathbf{ u}\|_{2}.\]

Now we also have

\[\min_{\mathbf{R}\in\mathcal{C}}\|\mathbf{R}-\eta\mathbf{u}\|_{2}=\eta\cdot\min_{\mathbf{R}\in \mathcal{C}}\|\frac{1}{\eta}\mathbf{R}-\mathbf{u}\|_{2}=\eta\cdot\min_{\mathbf{R}\in \mathcal{C}}\|\mathbf{R}-\mathbf{u}\|_{2}\]

where the last equality follows from \(\mathcal{C}\) being a cone. This shows that \(\arg\min_{\mathbf{R}\in\mathcal{C}}\|\mathbf{R}-\eta\mathbf{u}\|_{2}\) is attained at \(\eta\Pi_{\mathcal{C}}(\mathbf{u})\), i.e., that \(\Pi_{\mathcal{C}}(\eta\mathbf{u})=\eta\Pi_{\mathcal{C}}(\mathbf{u})\). 

We are now ready to prove Proposition 4.1. For the sake of conciseness we prove this with \(\mathbf{m}_{1}=...=\mathbf{m}_{T}=\mathbf{0}\); the proof for PTB\({}^{*}\) with predictions is identical. In this case, \(\mathbf{R}_{t}=\tilde{\mathbf{R}}_{t},\forall\ t\geq 1\). Consider the sequence of strategies \(\tilde{\mathbf{x}}_{1},...,\tilde{\mathbf{x}}_{T}\) and \(\tilde{\mathbf{R}}_{1},...,\tilde{\mathbf{R}}_{T}\) generated by PTB\({}^{*}\) with a step size of \(1\). We also consider the sequence of strategies \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) and \(\mathbf{R}_{1},...,\mathbf{R}_{T}\) generated with a step size \(\eta>0\). We claim that

\[\tilde{\mathbf{x}}_{t}=\mathbf{x}_{t},\mathbf{R}_{t}=\eta\tilde{\mathbf{R}}_{t},\ \forall\ t\in\{1,...,T\}.\]

We prove this by induction. Both sequences of iterates are initialized with \(\mathbf{R}_{1}=\tilde{\mathbf{R}}_{1}=\mathbf{0}\) so that \(\tilde{\mathbf{x}}_{1}=\mathbf{x}_{1}\). Therefore, both sequences face the same loss \(\mathbf{\ell}_{1}\) at \(t=1\), and we have

\[\mathbf{R}_{2}=\Pi_{\mathcal{C}}(-\eta\mathbf{f}(\mathbf{x}_{1},\mathbf{\ell}_{1}))=\eta\pi_{ \mathcal{C}}(-\mathbf{f}(\mathbf{x}_{1},\mathbf{\ell}_{1})))=\eta\tilde{\mathbf{R}}_{2}.\]

Let us now consider an iteration \(t\geq 1\) and suppose that \(\tilde{\mathbf{x}}_{t}=\mathbf{x}_{t},\mathbf{R}_{t}=\eta\tilde{\mathbf{R}}_{t}\). Since \(\tilde{\mathbf{x}}_{t}=\mathbf{x}_{t}\) then both algorithms will face the next loss vector \(\mathbf{\ell}_{t}\). Then

\[\mathbf{R}_{t+1} =\pi_{\mathcal{C}}(\mathbf{R}_{t}-\eta\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell}_{t}))\] \[=\pi_{\mathcal{C}}(\eta\tilde{\mathbf{R}}_{t}-\eta\mathbf{f}(\mathbf{x}_{t}, \mathbf{\ell}_{t}))\] \[=\eta\pi_{\mathcal{C}}(\tilde{\mathbf{R}}_{t}-\mathbf{f}(\mathbf{x}_{t},\mathbf{ \ell}_{t}))\] \[=\eta\tilde{\mathbf{R}}_{t+1}\]

which in turns implies that \(\mathbf{x}_{t+1}=\tilde{\mathbf{x}}_{t+1}\). We conclude that \(\mathbf{x}_{t}=\tilde{\mathbf{x}}_{t},\forall\ t=1,...,T\). 

## Appendix E Comparison Between \(\mathsf{RM}^{+}\) and \(\mathsf{PTB}^{+}\)

For the sake of discussion, we assume that the original decision set of each player is a simplex \(\Delta^{d}\) and that there are no predictions: \(\mathbf{m}_{t}=\mathbf{0},\forall\ t\geq 1\).

Ptb\({}^{*}\) over the simplex.For PTB\({}^{*}\), the empty sequence variable \(x_{\varnothing}\) is introduced and appended to the decision \(\Delta^{d}\). The resulting treepplex can be written \(\mathcal{T}=\{1\}\times\Delta^{d}\), the set \(\mathcal{C}\) becomes \(\mathcal{C}:=\mathsf{cone}(\mathcal{T})=\mathsf{cone}(\{1\}\times\Delta^{d})\) and \(\mathbf{a}=(1,\mathbf{0})\in\mathbb{R}_{+}^{d+1}\) with \(1\) on the first component related to \(x_{\varnothing}\) and \(0\) everywhere else. In this case, PTB\({}^{*}\) without prediction is exactly the _Conic Blackwell Algorithm_\({}^{+}\) (CBA\({}^{*}\), [22]). Crucially, to run PTB\({}^{*}\) we need to compute the orthogonal projection onto \(\mathsf{cone}(\mathcal{T})=\mathsf{cone}(\{1\}\times\Delta)\) at every iteration, which can not be computed in closed-form, but it can be computed in \(O(n\log(n))\) arithmetic operations (see Appendix G.1 in [22]).

**Regret Matching\({}^{+}\).**\(\mathtt{RM}^{*}\) operates directly over the simplex \(\Delta^{d}\) without the introduction of the empty sequence \(x_{\varnothing}\), in contrast to \(\mathtt{PTB}^{*}\) which operates over \(\{1\}\times\Delta^{d}\). Importantly, in \(\mathtt{RM}^{*}\), at every iteration the orthogonal projection onto \(\mathbb{R}_{+}^{d}\) can be computed in closed form by simply thresholding to zero the negative components (and leaving unchanged the positive components): \(\Pi_{\mathbb{R}_{+}^{d}}\left(\boldsymbol{z}\right)=\left(\max\{z_{i},0\} \right)_{i\in[d]}\) for any \(\boldsymbol{z}\in\mathbb{R}^{d}\).

Empirical comparisons over simplexes.The numerical experiments in [22] show that \(\mathtt{CBA}^{*}\) may be slightly faster than \(\mathtt{RM}^{*}\) for some matrix games in terms of speed of convergence as a function of the number of iterations, but it can be slower in running times because of the orthogonal projections onto \(\mathsf{cone}(\{1\}\times\Delta)\) at each iteration (Figures 2,3,4 in [22]). When \(\mathcal{T}\) is a treeplex that is not the simplex, introducing \(x_{\varnothing}\) also changes the resulting algorithm but not the complexity of the orthogonal projection onto \(\mathsf{cone}(\mathcal{T})\), since there is no closed-form anymore, even without \(x_{\varnothing}\). As a convention, in this paper, we will always use \(x_{\varnothing}\) in our description of treeplexes and of our algorithms since it is convenient from a writing and implementation standpoint.

Overall, we notice that in the case of the simplex introducing the empty sequence variable \(x_{\varnothing}\) radically alters the complexity per iterations and the resulting algorithm, a fact that has not been noticed in previous work.

Empirical comparisons for EFGs.For solving EFGs, [22] combine the CFR decomposition with \(\mathtt{CBA}^{*}\) and compare the resulting algorithm with \(\mathtt{CFR}^{*}\) (i.e., combining the CFR decomposition with \(\mathtt{RM}^{*}\)). The authors in [22] observe similar numerical results as for the cases of simplexes: the resulting algorithm may slightly outperform \(\mathtt{CFR}^{*}\) in terms of duality gap achieved after a certain number of iterations, but it is outperformed by \(\mathtt{CFR}^{*}\) in terms of duality gap achieved after a certain computation time, because of the orthogonal projection required at every iteration at every simplex present in the treeplexes of each player.

## Appendix F Proof of Proposition 4.5

Proof of Proposition 4.5.:
1. Let \(\hat{\boldsymbol{R}}_{2}=\boldsymbol{R}_{2}/\|\boldsymbol{R}_{2}\|_{2}\) be the unit vector pointing in the same direction as \(\boldsymbol{R}_{2}\) and let \(\boldsymbol{h}:=\left(\langle\boldsymbol{R}_{1},\hat{\boldsymbol{R}}_{2} \rangle\right)\hat{\boldsymbol{R}}_{2}\) the orthogonal projection of \(\boldsymbol{R}_{1}\) onto \(\{\alpha\hat{\boldsymbol{R}}_{2}\mid\alpha\in\mathbb{R}\}\). We thus have \(\|\boldsymbol{R}_{1}-\boldsymbol{R}_{2}\|_{2}\geq\|\boldsymbol{R}_{1}- \boldsymbol{h}\|_{2}\).
2. Let \(\boldsymbol{p}=\frac{\langle\boldsymbol{R}_{1},\boldsymbol{a}\rangle}{ \langle\boldsymbol{R}_{2},\boldsymbol{a}\rangle}\hat{\boldsymbol{R}}_{2}\). Since \(\boldsymbol{p}\) and \(\boldsymbol{R}_{2}\) are colinear, we have \[\left\|\frac{\boldsymbol{R}_{1}}{\langle\boldsymbol{R}_{1},\boldsymbol{a} \rangle}-\frac{\boldsymbol{R}_{2}}{\langle\boldsymbol{R}_{2},\boldsymbol{a} \rangle}\right\|_{2}=\left\|\frac{\boldsymbol{R}_{1}}{\langle\boldsymbol{R}_ {1},\boldsymbol{a}\rangle}-\frac{\boldsymbol{p}}{\langle\boldsymbol{p}, \boldsymbol{a}\rangle}\right\|_{2}.\] Additionally, by construction, \(\langle\boldsymbol{p},\boldsymbol{a}\rangle=\langle\boldsymbol{R}_{1}, \boldsymbol{a}\rangle\), so that we obtain \[\left\|\frac{\boldsymbol{R}_{1}}{\langle\boldsymbol{R}_{1},\boldsymbol{a} \rangle}-\frac{\boldsymbol{R}_{2}}{\langle\boldsymbol{R}_{2},\boldsymbol{a} \rangle}\right\|_{2}=\left\|\frac{\boldsymbol{R}_{1}}{\langle\boldsymbol{R}_ {1},\boldsymbol{a}\rangle}-\frac{\boldsymbol{p}}{\langle\boldsymbol{R}_{1}, \boldsymbol{a}\rangle}\right\|_{2}=\frac{1}{\langle\boldsymbol{R}_{1}, \boldsymbol{a}\rangle}\|\boldsymbol{R}_{1}-\boldsymbol{p}\|_{2}.\] Note that \(\langle\boldsymbol{R}_{1},\boldsymbol{a}\rangle\geq 0\) since \(\boldsymbol{R}_{1}\in\mathsf{cone}(\mathcal{T})\) and \(\mathcal{T}\subset\{\boldsymbol{x}\in\mathbb{R}^{n+1}\mid\langle\boldsymbol{x },\boldsymbol{a}\rangle=1\}\). Assume that we can compute \(D>0\) such that \(\frac{\|\boldsymbol{R}_{1}-\boldsymbol{p}\|_{2}}{\|\boldsymbol{R}_{1}- \boldsymbol{h}\|_{2}}\leq D\). Then we have \[\left\|\frac{\boldsymbol{R}_{1}}{\langle\boldsymbol{R}_{1},\boldsymbol{a} \rangle}-\frac{\boldsymbol{R}_{2}}{\langle\boldsymbol{R}_{2},\boldsymbol{a} \rangle}\right\|_{2}\leq\frac{D}{\langle\boldsymbol{R}_{1},\boldsymbol{a} \rangle}\|\boldsymbol{R}_{1}-\boldsymbol{h}\|_{2}\leq\frac{D}{\langle \boldsymbol{R}_{1},\boldsymbol{a}\rangle}\|\boldsymbol{R}_{1}-\boldsymbol{R} _{2}\|_{2}.\]
3. The rest of this proof focuses on showing that \(\frac{\|\boldsymbol{R}_{1}-\boldsymbol{p}\|_{2}}{\|\boldsymbol{R}_{1}- \boldsymbol{h}\|_{2}}\leq\Omega\) with \(\Omega=\max\{\|\boldsymbol{x}\|_{2}|\;\boldsymbol{x}\in\mathcal{T}\}\). Note that \(\langle\boldsymbol{R}_{1}-\boldsymbol{p},\boldsymbol{a}\rangle=0\). Therefore, \(\frac{1}{\|\boldsymbol{R}_{1}-\boldsymbol{p}\|_{2}}\left(\boldsymbol{R}_{1}- \boldsymbol{p}\right)\) and \(\frac{1}{\|\boldsymbol{a}\|_{2}}\boldsymbol{a}\) can be completed to form an orthonormal basis of \(\mathbb{R}^{n}\). In this basis, we have \[\|\hat{\boldsymbol{R}}_{2}\|_{2}^{2}\geq\frac{\left(\langle\boldsymbol{R}_{1 }-\boldsymbol{p},\hat{\boldsymbol{R}}_{2}\rangle\right)^{2}}{\|\boldsymbol{R}_{1 }-\boldsymbol{p}\|_{2}^{2}}+\frac{\left(\langle\boldsymbol{a},\hat{\boldsymbol{R }}_{2}\rangle\right)^{2}}{\|\boldsymbol{a}\|_{2}^{2}}.\]Note that by construction we have \(\|\hat{\mathbf{R}}_{2}\|_{2}^{2}=1\). Additionally, \(\mathbf{R}_{2}\in\mathsf{cone}(\mathcal{T})\) so that there exists \(\alpha>0\) and \(\mathbf{y}\in\mathcal{T}\) such that \(\mathbf{R}_{2}=\alpha\mathbf{x}\). By construction of \(\hat{\mathbf{R}}_{2}\), we have \(\hat{\mathbf{R}}_{2}=\frac{\alpha\mathbf{x}}{\|\alpha\mathbf{x}\|_{2}}=\frac{\mathbf{x}}{\| \mathbf{x}\|_{2}}\) and \(\langle\mathbf{x},\mathbf{a}\rangle=1\). This shows that \[\frac{\left(\langle\mathbf{a},\hat{\mathbf{R}}_{2}\rangle\right)^{2}}{\|\mathbf{a}\|_{2}^{ 2}}=\frac{\left(\langle\mathbf{a},\mathbf{x}\rangle\right)^{2}}{\|\mathbf{a}\|_{2}^{2}\|\bm {x}\|_{2}^{2}}=\frac{1}{\|\mathbf{a}\|_{2}^{2}\|\mathbf{x}\|_{2}^{2}}\geq\frac{1}{\Omega \|\mathbf{a}\|_{2}^{2}}\] with \(\Omega=\max\{\|\mathbf{x}\|_{2}\mid\mathbf{x}\in\mathcal{T}\}\). Recall that we have chosen \(\mathbf{a}=(1,\mathbf{0})\) so that \(\|\mathbf{a}\|_{2}=1\). Overall, we have obtained \[1-\frac{1}{\Omega^{2}}\geq\frac{\left(\langle\mathbf{R}_{1}-\mathbf{p},\hat{\mathbf{R}}_{ 2}\rangle\right)^{2}}{\|\mathbf{R}_{1}-\mathbf{p}\|_{2}^{2}}.\] From the definition of the vectors \(\mathbf{p},\mathbf{h}\) and \(\hat{\mathbf{R}}_{2}\), we have \[\frac{\left(\langle\mathbf{R}_{1}-\mathbf{p},\hat{\mathbf{R}}_{2}\rangle\right)^{2}}{\|\bm {R}_{1}-\mathbf{p}\|_{2}^{2}}=\frac{\|\mathbf{p}-\mathbf{h}\|_{2}^{2}}{\|\mathbf{R}_{1}-\mathbf{p} \|_{2}^{2}}.\] Hence, we have \[\|\mathbf{p}-\mathbf{h}\|_{2}^{2}\leq\left(1-\frac{1}{\Omega^{2}}\right)\|\mathbf{R}_{1}- \mathbf{p}\|_{2}^{2}.\] This shows that \(\|\mathbf{R}_{1}-\mathbf{h}\|_{2}^{2}\geq\frac{1}{\Omega^{2}}\|\mathbf{x}-\mathbf{p}\|_{2}^{2}\).
4. We conclude that \[\left\|\frac{\mathbf{R}_{1}}{\langle\mathbf{R}_{1},\mathbf{a}\rangle}-\frac{\mathbf{R}_{2}}{ \langle\mathbf{R}_{2},\mathbf{a}\rangle}\right\|_{2}\leq\frac{\Omega}{\max\{\langle \mathbf{R}_{1},\mathbf{a}\rangle,\langle\mathbf{R}_{2},\mathbf{a}\rangle\}}\|\mathbf{R}_{1}-\mathbf{R}_ {2}\|_{2}.\]

## Appendix G Proof of Proposition 4.4

In this section we show how to efficiently compute the orthogonal projection onto the cone \(\mathcal{C}:=\mathsf{cone}(\mathcal{T})\). We start by reviewing the existing methods for computing the orthogonal projection onto the treeplex \(\mathcal{T}\). This is an important cornerstone of our analysis, since the treeplex \(\mathcal{T}\) and the cone \(\mathcal{C}\) share an analogous structure:

\[\mathcal{T} =\{\mathbf{x}\in\mathbb{R}_{+}^{n+1}\mid x_{\varnothing}=1,\sum_{a \in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\}\] \[\mathcal{C} =\{\mathbf{x}\in\mathbb{R}_{+}^{n+1}\mid\sum_{a\in\mathcal{A}_{j}}x_{ ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\}.\]

[20] were the first to show an algorithm for computing Euclidean projection onto the treeplex. They do this by defining a value function for the projection of a given point \(\mathbf{y}\) onto the closed and convex _scaled_ set \(t\mathcal{Z}\), letting it be half the squared distance between \(\mathbf{y}\) and \(t\mathcal{Z}\), for \(t\in\mathbb{R}_{>0}\):

\[v_{\mathcal{Z}}(t,\mathbf{y}):=\frac{1}{2}\min_{\mathbf{z}\in t\mathcal{Z}}\|\mathbf{z}- \mathbf{y}\|_{2}^{2}.\]

[20] show how to recursively compute \(\lambda_{\mathcal{Z}}(t,\mathbf{y})\), the derivative of this function with respect to \(t\), for a given treeplex, since treeplexes can be constructed recursively using two operations: branching and Cartesian product. In the first case, given \(k\) treeplexes \(\mathcal{Z}_{1},\ldots,\mathcal{Z}_{k}\), then \(\mathcal{Z}=\{\mathbf{x},\mathbf{x}[1]\mathbf{z}_{1},\ldots,\mathbf{x}[k]\mathbf{z}_{k}:x\in \Delta_{k},\mathbf{z}_{i}\in\mathcal{Z}_{i}\forall i\in[k]\}\) is also a treeplex. In the second case, given \(k\) treeplexes \(\mathcal{Z}_{1},\ldots,\mathcal{Z}_{k}\), then \(\mathcal{Z}=\mathcal{Z}_{1}\times\cdots\times\mathcal{Z}_{k}\) is also a treeplex. In fact, letting the empty set be a treeplex as a base case, all treeplexes can be constructed in this way.

However, [20] did not state the total complexity of computing the projection, instead only stating the complexity of computing \(\lambda_{\mathcal{Z}}(t,\mathbf{y})\) given the corresponding \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i})\) functions for the treeplexesthat are used to construct \(\mathcal{Z}\) using \(i\in[k]\). They state that this complexity is \(O(n\log n)\), where \(n\) is the number of sequences in \(\mathcal{Z}\). Their analysis involves showing that the function \(t\mapsto\lambda_{\mathcal{Z}}(t,\mathbf{y})\) is piecewise linear.

[17] also consider this problem, generalizing the problem to weighted projection on the scaled treeplex, by adding an additional positive parameter \(\mathbf{w}\in\mathbb{R}_{>0}^{n}\):

\[v_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w}):=\frac{1}{2}\min_{\mathbf{z}\in tZ}\sum_{i=1}^{n} \left(\mathbf{z}[i]-\frac{\mathbf{y}[i]}{\mathbf{w}[i]}\right)^{2}.\]

They do a similar analysis to [20], by showing how to compute the derivative \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) of \(v_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) with respect to \(t\) recursively. They show that \(t\mapsto\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) are strictly-monotonically-increasing piecewise-linear (SMPL) functions. We will follow the analysis in [17], letting \(\mathbf{w}=\mathbf{1}\).

We first define a standard representation of a SMPL function.

**Definition G.1** ([17]).: _Given a SMPL function \(f\), a standard representation is an expression of the form_

\[f(x)=\zeta+\alpha_{0}x+\sum_{s=1}^{S}\alpha_{s}\max\{0,x-\beta_{s}\}\]

_valid for all \(x\in\operatorname{dom}(f)\), \(S\in\mathbb{N}\cup\{0\}\), and \(\beta_{1}<\cdots<\beta_{S}\). The size of the standard representation is defined to be \(S\)._

Next, we prove the following lemma, showing the computational complexity of computing the derivative of the value function for a given treeplex.

**Lemma G.2**.: _For a given treeplex \(\mathcal{Z}\) with depth \(d\), \(n\) sequences, \(l\) leaf sequences, and \(m\) infosets, and \(\mathbf{y}\in\mathbb{R}^{n},\mathbf{w}\in\mathbb{R}_{>0}^{n}\), a standard representation of \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) can be computed in \(O\big{(}dn\log(l+m)\big{)}\) time._

Proof.: We will proceed by structural induction over treeplexes, following the analysis done by [17]. The base case is trivially true, because the empty set has no sequences or depth.

For the inductive case, we will assume that it requires \(O\big{(}(d-1)n\log(l+m)\big{)}\) time to compute the respective Euclidean projections onto the subtreeplexes that we use to inductively construct our current treeplex, where \(d-1\) is the depth of a given subtreeplex, \(n\) is the number of sequences in the subtreeplex, and \(m\) is the total number of sequences among both players and chance corresponding to the game from which the treeplex originates.

We will use two results shown in Lemma 14 of [17]:

**Lemma G.3** (Recursive complexity of Euclidean projection for branching operation [17]).: _Consider a treeplex \(\mathcal{Z}\) that can be written as the result of a branching operation on \(k\) treeplexes \(\mathcal{Z}_{1},\ldots,\mathcal{Z}_{k}\):_

\[\mathcal{Z}=\{\mathbf{x},\mathbf{x}[1]\mathbf{z}_{1},\ldots,\mathbf{x}[k]\mathbf{z}_{k}:x\in \Delta_{k},\mathbf{z}_{i}\in\mathcal{Z}_{i}\forall i\in[k]\}.\]

_Let \(\mathcal{Z}\) have \(n\) sequences and let \(\mathbf{y},\mathbf{w}\in\mathbb{R}^{n}\), and let \(\mathbf{y}[i]\) and \(\mathbf{w}[i]\) denote the corresponding respective components of \(\mathbf{y}\) and \(\mathbf{w}\) for the treeplex \(\mathcal{Z}_{i}\)._

_Then, given standard representations of \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i},\mathbf{w}_{i})\) of size \(n_{i}\) for all \(i\in[k]\), where \(n_{i}\) is the number of sequences that \(\mathcal{Z}_{i}\) has, a standard representation of \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) of size \(n\) can be computed in \(O(n\log k)\) time._

_Furthermore, given a value of \(t\), the argument \(\mathbf{x}\) which leads to the realization of the optimal value of the value function, can be computed in time \(O(n)\)._

**Lemma G.4** (Recursive complexity of Euclidean projection for Cartesian product [17]).: _Consider a treeplex \(\mathcal{Z}\) that can be written as a Cartesian product of \(k\) treeplexes \(\mathcal{Z}_{1},\ldots,\mathcal{Z}_{k}\):_

\[\mathcal{Z}=\mathcal{Z}_{1}\times\cdots\times\mathcal{Z}_{k}.\]

_Let \(\mathcal{Z}\) have \(n\) sequences and let \(\mathbf{y},\mathbf{w}\in\mathbb{R}^{n}\), and let \(\mathbf{y}[i]\) and \(\mathbf{w}[i]\) denote the corresponding respective components of \(\mathbf{y}\) and \(\mathbf{w}\) for the treeplex \(\mathcal{Z}_{i}\)._

_Then, given standard representations of \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i},\mathbf{w}_{i})\) of size \(n_{i}\) for all \(i\in[k]\), where \(n_{i}\) is the number of sequences that \(\mathcal{Z}_{i}\) has, a standard representation of \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) of size \(n\) can be computed in \(O(n\log k)\) time._First, we consider the case that the last operation used to construct our treeplex was the branching operation. Let the root of of the treeplex be called \(j\). Define \(\mathcal{Z}_{i}\) as the treeplex that is underneath action \(a_{i}\in\mathcal{A}_{j}\). Let \(n_{i}\) denote the number of sequences in \(\mathcal{Z}_{i}\), \(m_{i}\) denote the number of infosets in \(\mathcal{Z}_{i}\), \(l_{i}\) denote the number of leaf sequences in \(\mathcal{Z}_{i}\), and \(d-1\) be the maximum depth of any of these subtreeplexes.

Given a standard representation of \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i},\mathbf{w}_{i})\) of size \(n_{i}\) for all \(i\in[|\mathcal{A}_{j}|]\), by Lemma G.3, it takes \(O(n\log|\mathcal{A}_{j}|)\) time to compute a standard representation of \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) of size \(n\). By induction, it takes \(O\big{(}(d-1)n_{i}\log m_{i}\big{)}\) to compute \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i},\mathbf{w}_{i})\) for treeplex \(\mathcal{Z}_{i}\). Thus the total computation required to compute \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) is

\[O(n\log|\mathcal{A}_{j}|)+\sum_{i\in[|\mathcal{A}_{j}|]}O\big{(} (d-1)n_{i}\log(l_{i}+m_{i})\big{)} =O(n\log|\mathcal{A}_{j}|)+\sum_{i\in[|\mathcal{A}_{j}|]}O\big{(} (d-1)n_{i}\log(l+m)\big{)}\] \[=O(n\log|\mathcal{A}_{j}|)+O\big{(}(d-1)\sum_{i\in[|\mathcal{A}_{ j}|]}n_{i}\log(l+m)\big{)}\] \[=O\big{(}n\log|\mathcal{A}_{j}|)+O\big{(}(d-1)n\log(l+m)\big{)}\] \[=O\big{(}n\log(l+m)\big{)}+O\big{(}(d-1)n\log(l+m)\big{)}\] \[=O\big{(}dn\log(l+m)\big{)}\]

since we have necessarily that \(l_{i}\leq l\) and \(m_{i}\leq m\) for all \(i\in[|\mathcal{A}_{j}|]\), \(\sum_{i\in[|\mathcal{A}_{j}|]}n_{i}\leq n\), and \(|\mathcal{A}_{j}|\leq l+m\).

Second, we consider the case the last operation to construct our treeplex was a Cartesian product. Let \(\mathcal{Z}=\mathcal{Z}_{1}\times\cdots\times\mathcal{Z}_{k}\), and again define \(n_{i}\) as the number of sequences in \(\mathcal{Z}_{i}\), \(m_{i}\) as the number of infosets in \(\mathcal{Z}_{i}\), \(l_{i}\) as the number of leaf sequences in \(\mathcal{Z}_{i}\), and \(d-1\) as the maximum depth of any of these subtreeplexes.

Given a standard representation of \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i},\mathbf{w}_{i})\) of size \(n_{i}\) for all \(i\in[k]\), by Lemma G.4 it takes \(O(n\log k)\) to compute a standard representation of \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) of size \(n\). By induction, it takes \(O\big{(}(d-1)n_{i}\log(l_{i}+m_{i})\big{)}\) to compute \(\lambda_{\mathcal{Z}_{i}}(t,\mathbf{y}_{i},\mathbf{w}_{i})\) for treeplex \(\mathcal{Z}_{i}\). Thus the total computation required to compute \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) is

\[O(n\log k)+\sum_{i\in[k]}O\big{(}(d-1)n_{i}\log(l_{i}+m_{i}) \big{)} =O(n\log k)+\sum_{i\in[k]}O\big{(}(d-1)n_{i}\log(l+m)\big{)}\] \[=O(n\log k)+O\big{(}(d-1)\sum_{i\in[k]}n_{i}\log(l+m)\big{)}\] \[=O(n\log k)+O\big{(}(d-1)n\log(l+m)\big{)}\] \[=O(n\log m)+O\big{(}(d-1)n\log(l+m)\big{)}\] \[=O(dn\log(l+m))\]

since we have necessarily that \(l_{i}\leq l\) and \(m_{i}\leq m\) for all \(i\in[k]\), and \(k\leq m\). 

Finally, we are ready to prove the main statement.

Proof of Proposition 4.4.: By Lemma G.2, we know that we can recursively compute a standard representation of \(\lambda_{\mathcal{Z}}(t,\mathbf{y},\mathbf{w})\) in \(O\big{(}dn\log(l+m)\big{)}\) time. Assuming we use this construction, invoking Lemma G.3, given an optimal value of \(t\), we can compute the partial argument corresponding to the values of the sequences that originate at the root infosets, which allow the optimal value to be realized for the value function. Then, we can use optimal arguments for these sequences recursively at the subtreeplexes to continue computing the optimal argument at sequences lower on the treeplex. We can do this because in the process of computing the derivative of the value function of the entire treeplex, we have also computed the derivative of the value function for each of the subtreeplexes. Thus, once we have computed an optimval value of \(t\) for the value function at the top level, we can do a top-down pass to compute the optimal values for all sequences that occur at any level in the treeplex. This is detailed in the analysis done in the proof of Lemma 14 in [17].

In order to pick the optimal value of \(t\) for the value function, since \(\lambda_{\mathcal{Z}}(\cdot,\boldsymbol{y},\boldsymbol{w})\) is strictly increasing, we only have to consider two cases: \(\lambda_{\mathcal{Z}}(0,\boldsymbol{y},\boldsymbol{w})<0\) and \(\lambda_{\mathcal{Z}}(0,\boldsymbol{y},\boldsymbol{w})\geq 0\). In the first case, the value function \(\lambda_{\mathcal{Z}}(\cdot,\boldsymbol{y},\boldsymbol{w})\) will be minimized when \(\lambda_{\mathcal{Z}}(\cdot,\boldsymbol{y},\boldsymbol{w})\) is equal to \(0\), and this can be directly computed using the standard representation (it will be necessarily \(0\) somewhere because it is strictly monotone). In the second case, since \(\lambda_{\mathcal{Z}}(\cdot,\boldsymbol{y},\boldsymbol{w})\) is strictly monotone and \(\lambda_{\mathcal{Z}}(0,\boldsymbol{y},\boldsymbol{w})\geq 0\), we must have that \(\lambda_{\mathcal{Z}}(\cdot,\boldsymbol{y},\boldsymbol{w})\geq 0\), which means that \(v_{\mathcal{Z}}(\cdot,\boldsymbol{y},\boldsymbol{w})\) is minimized at \(t^{*}=0\). 

## Appendix H Practical Implementation of Smooth PTB\({}^{+}\)

We have the following lemma, which shows that the stable region \(\mathcal{C}_{\geq}\) admits a relatively simple formulation.

**Lemma H.1**.: _The stable region_

\[\mathcal{C}_{\geq}:=\mathsf{cone}(\mathcal{T})\cap\{\boldsymbol{R}\in\mathbb{ R}^{n+1}\mid\langle\boldsymbol{R},\boldsymbol{a}\rangle\geq R_{0}\}\]

_can be reformulated as follows:_

\[\mathcal{C}_{\geq} =\{\alpha\boldsymbol{x}\mid\alpha\geq R_{0},\boldsymbol{x}\in \mathcal{T}\}\] \[=\{\boldsymbol{x}\in\mathbb{R}_{+}^{n+1}\mid x_{\varnothing}\geq R _{0},\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\}.\]

Proof.: By definition, we have

\[\mathcal{C}_{\geq}=\{\boldsymbol{R}\in\mathsf{cone}(\mathcal{T})\mid\langle \boldsymbol{R},\boldsymbol{a}\rangle\geq R_{0}\}.\]

Note that for \(\boldsymbol{R}\in\mathsf{cone}(\mathcal{T}),\boldsymbol{R}=\alpha\boldsymbol{x}\) with \(\alpha\geq 0\) and \(\langle\boldsymbol{x},\boldsymbol{a}\rangle=1\). Therefore, for \(\boldsymbol{R}\in\mathcal{C}\) we have \(\langle\boldsymbol{R},\boldsymbol{a}\rangle\geq R_{0}\iff\alpha\geq R_{0}\). This shows that we can write

\[\mathcal{C}_{\geq}=\{\alpha\boldsymbol{x}\mid\alpha\geq R_{0},\boldsymbol{x} \in\mathcal{T}\}.\]

Now let \(\boldsymbol{x}\in\mathcal{C}_{\geq}\), i.e., let \(\boldsymbol{x}=\alpha\hat{\boldsymbol{x}}\) with \(\alpha\geq R_{0}\) and \(\boldsymbol{x}\in\mathcal{T}\). Since \(\hat{\boldsymbol{x}}\in\mathcal{T}\), we have \(x_{\varnothing}=1\), so that \(\hat{x}_{\varnothing}=\alpha\geq R_{0}\). Additionally, we have \(\hat{\boldsymbol{x}}\geq 0,\sum_{a\in\mathcal{A}_{j}}\hat{x}_{ja}=\hat{x}_{p_{j}}, \forall\;j\in\mathcal{J}\). Multiplying by \(\alpha\geq R_{0}\), we obtain that \(\boldsymbol{x}\geq 0\) and \(\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\). Overall we have

\[\mathcal{C}_{\geq}\subseteq\{\boldsymbol{x}\in\mathbb{R}^{n+1}\mid x_{ \varnothing}\geq R_{0},\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j \in\mathcal{J},\boldsymbol{x}\geq\boldsymbol{0}\}.\]

We now consider \(\boldsymbol{x}\in\{\boldsymbol{x}\in\mathbb{R}^{n+1}\mid x_{\varnothing}\geq R _{0},\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}, \boldsymbol{x}\geq\boldsymbol{0}\}\) with \(\boldsymbol{x}\neq\boldsymbol{0}\). Then \(\boldsymbol{x}=\alpha\frac{\boldsymbol{x}}{\alpha}\) with \(\alpha=x_{\varnothing}\), so that \(\alpha\geq R_{0}\) and

\[\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}\iff\sum_{a \in\mathcal{A}_{j}}\frac{x_{ja}}{\alpha}=\frac{x_{p_{j}}}{\alpha},\forall\;j \in\mathcal{J}.\]

Therefore

\[\{\boldsymbol{x}\in\mathbb{R}^{n+1}\mid x_{\varnothing}\geq R_{0},\sum_{a\in \mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J},\boldsymbol{x}\geq \boldsymbol{0}\}\subseteq\mathcal{C}_{\geq}.\]

This shows that we have

\[\mathcal{C}_{\geq}=\{\boldsymbol{x}\in\mathbb{R}^{n+1}\mid x_{\varnothing}\geq R _{0},\sum_{a\in\mathcal{A}_{j}}x_{ja}=x_{p_{j}},\forall\;j\in\mathcal{J}, \boldsymbol{x}\geq\boldsymbol{0}\}.\]

**Proposition H.2**.: _For a treeplex \(\mathcal{T}\) with depth \(d\), number of sequences \(n\), number of leaf sequences \(l\), and number of infosets \(m\), the complexity of computing the orthogonal projection of a point \(y\in\mathbb{R}^{n+1}\) onto \(\mathcal{C}_{\geq}=\{\alpha\boldsymbol{x}\mid\alpha\geq R_{0},\boldsymbol{x} \in\mathcal{T}\}\) is \(O\big{(}dn\log(l+m)\big{)}\)._Proof.: The proof is the same as that for Proposition 4.4, since the derivative of the value function can be computed in \(O\big{(}dn\log(l+m)\big{)}\) time. However, this time, we have an additional constraint that \(t\geq R_{0}\). Thus instead of checking the sign of \(\lambda_{\mathcal{Z}}(\cdot,\mathbf{y},\mathbf{w})\) at \(t=0\), we check the sign at \(R_{0}\).

If \(\lambda_{\mathcal{Z}}(R_{0},\mathbf{y},\mathbf{w})<0\), then because \(\lambda_{\mathcal{Z}}(\cdot,\mathbf{y},\mathbf{w})\) is a strictly monotone function, the function will be \(0\) for some value of \(t\), and this is exactly \(t^{*}\), which minimizes the value function with respect to \(t\), when \(t\geq R_{0}\). On the other hand, if \(\lambda_{\mathcal{Z}}(R_{0},\mathbf{y},\mathbf{w})\geq 0\), then again because the function is strictly monotone in \(t\), we know that the value function must get minimized at \(t^{*}=R_{0}\). Using the same argument as in the proof of Proposition 4.4, since we have computed the standard representations of the derivatives of the value functions at all of the treeplexes, we can do a top-down pass to compute the argument which leads to the optimal value of the value function.

## Appendix I Proof of Theorem 4.7

Proof of Theorem 4.7.: For the sake of conciseness we write \(\mathbf{f}_{t}^{x}=\mathbf{f}(\mathbf{x}_{t},\mathbf{M}\mathbf{y}_{t})\) and \(\mathbf{f}_{t}^{y}=\mathbf{f}(\mathbf{y}_{t},-\mathbf{M}^{\top}\mathbf{x}_{t})\).

From our Proposition 4.2, we have that, for the first player,

\[\sum_{t=1}^{T}\langle\mathbf{x}_{t}-\hat{\mathbf{x}},\mathbf{M}\mathbf{y}_{t}\rangle=\sum_{t=1 }^{T}\langle\mathbf{R}_{t}-\hat{\mathbf{R}},\mathbf{f}_{t}^{x}\rangle.\]

Now \(\sum_{t=1}^{T}\langle\mathbf{R}_{t}-\hat{\mathbf{R}},\mathbf{f}_{t}^{x}\rangle\) is the regret obtained by running Predictive \(\tt OMD\) on \(\mathcal{C}_{\geq}\) against the sequence of loss \(\mathbf{\hat{f}}_{1}^{x},...,\mathbf{f}_{T}^{x}\). From Proposition 5 in [16], we have that

\[\sum_{t=1}^{T}\langle\mathbf{R}_{t}^{x}-\hat{\mathbf{R}}^{x},\mathbf{f}_{t}^{x}\rangle \leq\frac{\|\hat{\mathbf{R}}\|_{2}^{2}}{2\eta}+\eta\sum_{t=1}^{T}\|\mathbf{f}_{t}^{x}- \mathbf{f}_{t-1}^{x}\|_{2}^{2}-\frac{1}{8\eta}\sum_{t=1}^{T}\|\mathbf{R}_{t+1}^{x}-\bm {R}_{t+1}^{x}\|_{2}^{2}.\]

Since \(\hat{\mathbf{R}}_{t}\in\mathcal{C}_{\geq}\), we can use our Proposition 4.5 to show that

\[\|\mathbf{x}_{t+1}-\mathbf{x}_{t}\|_{2}^{2}\leq\frac{\Omega}{R_{0}^{2}}\|\mathbf{R}_{t+1}^{ x}-\mathbf{R}_{t+1}^{x}\|_{2}^{2}.\]

This shows that

\[\sum_{t=1}^{T}\langle\mathbf{R}_{t}^{x}-\hat{\mathbf{R}}^{x},\mathbf{f}_{t}^{x}\rangle \leq\frac{\|\hat{\mathbf{R}}\|_{2}^{2}}{2\eta}+\eta\sum_{t=1}^{T}\|\mathbf{f}_{t}^{x}- \mathbf{f}_{t-1}^{x}\|_{2}^{2}-\frac{R_{0}^{2}}{8\Omega^{2}\eta}\sum_{t=1}^{T}\| \mathbf{R}_{t+1}^{x}-\mathbf{R}_{t+1}^{x}\|_{2}^{2}\]

which gives, using the norm equivalence \(\|\cdot\|_{2}\leq\|\cdot\|_{1}\leq\sqrt{n+1}\|\cdot\|_{2}\), the following inequality:

\[\sum_{t=1}^{T}\langle\mathbf{R}_{t}^{x}-\hat{\mathbf{R}}^{x},\mathbf{f}_{t}^{x}\rangle\leq \frac{\|\hat{\mathbf{R}}\|_{2}^{2}}{2\eta}+\eta\sum_{t=1}^{T}\|\mathbf{f}_{t}^{x}-\mathbf{ f}_{t-1}^{x}\|_{1}^{2}-\frac{R_{0}^{2}}{8\Omega^{2}(n+1)\eta}\sum_{t=1}^{T}\|\mathbf{R}_{t+1 }^{x}-\mathbf{R}_{t+1}^{x}\|_{2}^{2}\]

The above inequality is a RVU bound:

\[\sum_{t=1}^{T}\langle\mathbf{R}_{t}^{x}-\hat{\mathbf{R}}^{x},\mathbf{f}_{t}^{x}\rangle \leq\alpha+\beta\sum_{t=1}^{T}\|\mathbf{f}_{t}^{x}-\mathbf{f}_{t-1}^{x}\|_{1}^{2}- \gamma\sum_{t=1}^{T}\|\mathbf{R}_{t+1}^{x}-\mathbf{R}_{t+1}^{x}\|_{2}^{2}\]

with

\[\alpha=\frac{\|\hat{\mathbf{R}}\|_{2}^{2}}{2\eta},\beta=\eta,\gamma=\frac{R_{0}^{ 2}}{8\Omega^{2}(n+1)\eta}. \tag{13}\]

To invoke Theorem 4 in [36], we also need the utilities of each player to be bounded by \(1\). This can be done can rescaling \(\mathbf{f}_{t}^{x}=\mathbf{M}\mathbf{y}_{t}\) and \(\mathbf{f}_{t}^{y}=-\mathbf{M}^{\top}\mathbf{x}_{t}\). In particular, we know that

\[\|\mathbf{M}\mathbf{y}\|_{\infty}\leq\|\mathbf{M}\|_{\ell_{2},\ell_{\infty}}\|\mathbf{y}\|_{2} \leq\|\mathbf{M}\|_{\ell_{2},\ell_{\infty}}\cdot\hat{\Omega}\]with \(\|\mathbf{M}\|_{\ell_{2},\ell_{\infty}}=\max_{i\in[n+1]}\|\left(A_{ij}\right)_{j\in[m+1]} \|_{2}\) and \(\hat{\Omega}=\max\{\max\{\|\mathbf{x}\|_{2},\|\mathbf{y}\|_{2}\}\ \mathbf{x}\in\mathcal{X},\mathbf{y} \in\mathcal{Y}\}\). This corresponds to multiplying \(\beta\) in (13) by \(\|\mathbf{M}\|\times\hat{\Omega}\) with \(\|\mathbf{M}\|:=\max\{\|\mathbf{M}\|_{\ell_{2},\ell_{\infty}},\|\mathbf{M}^{\top}\|_{\ell_ {2},\ell_{\infty}}\}\). To apply Theorem 4 in [36] we also need \(\beta\leq\gamma\). Since we need the same condition for the second player, we take

\[\eta=R_{0}\left(\sqrt{8d\hat{\Omega}^{3}}\|\mathbf{M}\|\right)^{-1}.\]

Under this condition on the stepsize, we can invoke Theorem 4 in [36] to conclude that

\[\sum_{t=1}^{T}\langle\mathbf{R}_{t}^{x}-\hat{\mathbf{R}}^{x},\mathbf{f}_{t}^{x} \rangle+\sum_{t=1}^{T}\langle\mathbf{R}_{t}^{y}-\hat{\mathbf{R}}^{y},\mathbf{f}_{t}^{y} \rangle\leq\frac{\|\hat{\mathbf{R}}^{x}\|_{2}^{2}+\|\hat{\mathbf{R}}^{y}\|_{2}^{2}}{ \eta}.\]

Since the duality gap is bounded by the average of the sum of the regrets of both players [19], and replacing \(\eta\) by its expression, we obtain that

\[\max_{\mathbf{y}\in\mathcal{Y}}\left\langle\bar{\mathbf{x}}_{T},\mathbf{M}\mathbf{y}\right\rangle -\min_{\mathbf{x}\in\mathcal{X}}\left\langle\mathbf{x},\mathbf{M}\bar{\mathbf{y}}_{T}\right\rangle \leq\frac{2\hat{\Omega}^{2}}{\eta}\frac{1}{T}.\]

## Appendix J AdaGradTB\({}^{+}\) and AdamTB\({}^{+}\)

AdaGradTB\({}^{*}\).We introduce AdaGradTB\({}^{*}\)in Algorithm 6. Given matrix \(\mathbf{A}\) and a vector \(\mathbf{y}\in\mathbb{R}^{n+1}\), let \(\mathsf{diag}(\mathbf{y})\) be the diagonal matrix with \(\mathbf{y}\) on its diagonal and \(\Pi_{\mathcal{C}}^{\mathcal{A}}(\mathbf{y})=\arg\min_{\mathbf{x}\in\mathcal{C}}\langle \mathbf{x}-\mathbf{y},\mathbf{A}(\mathbf{x}-\mathbf{y})\rangle\). We first show that AdaGradTB\({}^{+}\) is a regret minimizer.

**Proposition J.1**.: _Let \(\mathbf{x}_{1},...,\mathbf{x}_{T}\) be computed by \(\mathsf{AdaGradTB}^{*}\). For \(\eta=\frac{\max_{t<T}(\|\mathbf{R}_{t}\|_{2}+\Omega)^{2}}{\sqrt{2}}\), we have \(\max_{\tilde{\mathbf{x}}\in\mathcal{T}}\sum_{t=1}^{T}\langle\mathbf{x}_{t}-\hat{\mathbf{x }},\mathbf{\ell}_{t}\rangle\leq 2\eta\sum_{i=1}^{d}\sqrt{\sum_{t=1}^{T}\left(\mathbf{f}_{t }(\mathbf{x}_{t},\mathbf{\ell}_{t})\right)_{i}^{2}}\)._

We omit the proof of Proposition J.1 for conciseness; it follows from the regret guarantees of AdaGrad (Theorem 5 in [12]) and Proposition 3.1. We conclude that combining AdaGradTB\({}^{*}\) with the self-play framework ensures a \(O(1/\sqrt{T})\) convergence rate.

```
1:Input:\(\eta,\delta>0\)
2:Initialization:\(\mathbf{R}_{1}=\mathbf{s}_{0}=\mathbf{g}_{0}=\mathbf{0}\in\mathbb{R}^{n+1}\)
3:for\(t=1,\dots,T\)do
4:\(\mathbf{x}_{t}=\mathbf{R}_{t}/\langle\mathbf{R}_{t},\mathbf{a}\rangle\)
5: Observe the loss vector \(\mathbf{\ell}_{t}\in\mathbb{R}^{n+1}\)
6:\(\mathbf{s}_{t}=\mathbf{s}_{t-1}+\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell}_{t})\odot\mathbf{f}(\mathbf{x}_{t}, \mathbf{\ell}_{t})\)
7:\(\mathbf{H}_{t}=\mathsf{diag}\left(\sqrt{\mathbf{s}_{t}}+c\mathbf{1}\right)\)
8:\(\mathbf{R}_{t+1}\in\Pi_{\mathcal{C}}^{\mathbf{H}_{t}}\left(\mathbf{R}_{t}-\eta\mathbf{H}_{t}^{ -1}\mathbf{f}(\mathbf{x}_{t},\mathbf{\ell}_{t})\right)\)
```

**Algorithm 6** AdaGradTB\({}^{*}\)

AdamTB\({}^{*}\).We present AdamTB\({}^{*}\), our instantiation of Algorithm 1 inspired from the adaptive algorithm Adam [25] in Algorithm 7. Since Adam is not necessarily a regret minimizer [35], there are no regret guarantees for AdamTB\({}^{*}\). We choose to consider this algorithm for the sake of completeness, since Adam is widely used in other settings.

## Appendix K Details on Numerical Experiments

### Additional Algorithms

Single-call Predictive Online Mirror Descent (SC-POMD).We present SC-POMD in Algorithm 8. This algorithm runs a variant of predictive online mirror descent with only one orthogonal projection at every iteration [24]. The pseudocode from Algorithm 8 corresponds to choosing the squared \(\ell_{2}\)-norm as a distance generating function - in principle, other distance generating functions are possible, e.g. dilated entropy [15]. Combined with the self-play framework, SC-POMD ensures that the average of the visited iterates converges to a Nash equilibrium at a rate of \(O(1/T)\), similar to the variant of predictive online mirror descent with two orthogonal projections at every iteration [15].

```
1:Input:\(\eta>0\),
2:Initialization:\(\mathbf{x}_{0}=\mathbf{\ell}_{0}=\mathbf{\ell}_{-1}=\mathbf{0}\in\mathbb{R}^{n+1}\)
3:for\(t=1,\ldots,T\)do
4:\(\mathbf{x}_{t}=\Pi_{\mathcal{T}}\left(\mathbf{x}_{t-1}-\eta\left(2\mathbf{\ell}_{t-1}-\mathbf{ \ell}_{t-2}\right)\right)\)
5: Observe the loss vector \(\mathbf{\ell}_{t}\in\mathbb{R}^{n+1}\)
```

**Algorithm 8** Single-call predictive online mirror descent (SC-POMD)

### Algorithm Implementation Details

All algorithms are initialized using the uniform strategy (placing equal probability on each action at each decision point). For algorithms that are not stepsize invariant (Smooth PTB* and SC-POMD), we try stepsizes in \(\eta\in\{0.05,0.1,0.5,1,2,5\}\) and we present the performance with the best stepsize. For Smooth PTB*, we use \(R_{0}=0.1\). For both AdaGradTB* and AdamTB*, we use \(\delta=1\times 10^{-6}\), and for AdamTB* we use \(\beta_{1}=0.9\) and \(\beta_{2}=0.999\).

### Comparing the Performance of our Algorithms

In Figure 4 we compare the performance of TB*, PTB*, Smooth PTB*, AdaGradTB* and AdamTB*.

It can be seen that PTB* and Smooth PTB* perform similarly, both when using quadratic averaging and when using the last iterate, and they generally outperform the other algorithms. In Kuhn, Liar's Dice, and Battleship, the last iterate seems to perform quite well, whereas in Leduc and Goofspiel, the quadratic averaging scheme works better. AdamTB* seems to not converge in any of the games, which is not surprising, because it does not have theoretical guarantees for convergence.

### Individual Performance

In Figure 5fig:scpomd, we compare the individual performance of TB*, PTB*, Smooth PTB*, AdaGradTB*, AdamTB*, CFR*, PCFR* and SC-POMD with different weighting schemes, with and without alternation. We also show the performance of the last iterate. The goal is to choose the most favorable framework for each algorithms, in order to have a fair comparison. We find that all algorithms benefit from using alternation. CFR* enjoys stronger performance using linear weights, whereas PTB*, PCFR* and SC-POMD have stronger performances with quadratic weights. For this reason this is the setup that we present for comparing the performance of these algorithms in our main body (Figure 2).

## NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide all proofs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: This is done in the main body. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.

Figure 4: Convergence to Nash equilibrium as a function of number of iterations for \(\mathtt{TB}^{*}\) with quadratic averaging, \(\mathtt{PTB}^{*}\) with quadratic averaging and last iterate, and \(\mathtt{Smooth}\)\(\mathtt{PTB}^{*}\) with quadratic averaging and last iterate. Every algorithm is using alternation.

Figure 5: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for \(\mathtt{TB}^{*}\).

Figure 8: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for AdaGradTB\({}^{*}\).

Figure 10: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for CFR\({}^{*}\).

Figure 6: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for PTB\({}^{*}\).

Figure 7: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for Smooth PTB\({}^{*}\).

* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: This is done in the main body and in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.

Figure 11: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for \(\mathsf{PCFR}^{*}\).

Figure 12: Convergence to Nash equilibrium as a function of number of iterations using uniform, linear, and quadratic averaging, as well as the last iterate, with and without alternation for \(\mathsf{SC}\)-\(\mathsf{POMD}\).

* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We do so in the main body and in the appendices (Appendix H, Appendix K). Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We plan to do so after the revision process. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). ** The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We do so in the main body and in the appendices. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: No random experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We do so in the main body and in the appendices. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This research focuses on foundational questions. There is no direct societal impacts of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.

* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Not applicable to our paper. There is no release of data. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We provide citations to the packages we use. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Not applicable to our paper. Guidelines: * The answer NA means that the paper does not release new assets. ** Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable to our paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable to our paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.