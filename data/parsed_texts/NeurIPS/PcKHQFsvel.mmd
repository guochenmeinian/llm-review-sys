# On the Statistical Consistency of Risk-Sensitive Bayesian Decision-Making

 Prateek Jaiswal

Department of Statistics

Texas A&M University

College Station, TX 77843

jaiswalp@stat.tamu.edu

&Harsha Honnappa

School of Industrial Engineering

Purdue University

West Lafayette, IN, 47906

honnappa@purdue.edu

&Vinayak A. Rao

Department of Statistics

Purdue University

West Lafayette, IN, 47907

varao@purdue.edu

###### Abstract

We study data-driven decision-making problems in the Bayesian framework, where the expectation in the Bayes risk is replaced by a risk-sensitive entropic risk measure with respect to the posterior distribution. We focus on problems where calculating the posterior distribution is intractable, a typical situation in modern applications with large datasets and complex data generating models. We leverage a dual representation of the entropic risk measure to introduce a novel risk-sensitive variational Bayesian (RSVB) framework for jointly computing a risk-sensitive posterior approximation and the corresponding decision rule. Our general framework includes _loss-calibrated_ VB [16] as a special case. We also study the impact of these computational approximations on the predictive performance of the inferred decision rules. We compute the convergence rates of the RSVB approximate posterior and the corresponding optimal value. We illustrate our theoretical findings in parametric and nonparametric settings with the help of three examples.

## 1 Introduction

This paper focuses on _risk-sensitive_ Bayesian decision-making, considering objective functions of the form

\[\min_{a\in\mathcal{A}}\varrho_{\Pi_{n}}^{\gamma}(R(a,\theta)):=\gamma^{-1} \log\mathbb{E}_{\Pi_{n}}[\exp(\gamma R(a,\theta))].\] (SO)

Here \(\mathcal{A}\) is the decision/action space, \(\theta\) is a random model parameter lying in an arbitrary measurable space \((\Theta,\mathcal{T})\), and \(R(a,\theta):\mathcal{A}\times\Theta\mapsto\mathbb{R}\) is a problem-specific model risk function. For any \(B\subseteq\Theta\), \(\Pi_{n}(B):=\Pi(B|\tilde{X}_{n})\) is the Bayesian posterior distribution over the parameters given observations \(\tilde{X}_{n}\) from the true model \(P_{\theta_{0}}^{n}(\equiv P_{0}^{n})\) with parameter \(\theta_{0}\in\Theta\). The scalar \(\gamma\in\mathbb{R}\) is user-specified and characterizes the sensitivity of the decision-maker (DM) to the distribution \(\Pi_{n}\). Recall that the posterior distribution is obtained by updating a prior probability distribution \(\Pi(\cdot)\), capturing subjective beliefs of the decision maker over \(\Theta\), according to the Bayes rule

\[d\Pi(\theta|\tilde{X}_{n})\propto d\Pi(\theta)dP_{\theta}^{n}(\tilde{X}_{n}),\] (1)

where \(dP_{\theta}^{n}(\tilde{X}_{n})\) is the likelihood of observing \(\tilde{X}_{n}\). We denote the corresponding densities (if they exist) for the model, prior, and posterior as \(p_{\theta}^{n}(\cdot)\), \(\pi(\cdot)\), and \(\pi(\cdot|\tilde{X}_{n})\) respectively.

The functional \(\varrho^{\gamma}\) in (SO) is also known as the _entropic risk measure_, and models a range of risk-averse or risk-seeking behaviors in a succinct manner through the parameter \(\gamma\). Consider only strictly positive \(\gamma\), and observe that \(\lim_{\gamma\downarrow 0}\frac{1}{\gamma}\log\mathbb{E}_{\Pi_{n}}[\exp( \gamma R(a,\theta))]=\mathbb{E}_{\Pi_{n}}(R(a,\theta))\); that is, there is no sensitivity to potential risks due to large tail effects and the decision-maker is risk neutral.

On the other hand, \(\lim_{\gamma\rightarrow+\infty}\varrho_{\Pi_{n}}^{\gamma}\left(R(a,\theta)\right)= \operatorname*{ess\,sup}_{\Pi_{n}}\left(R(a,\theta)\right),\) the essential supremum of the model risk \(R(a,\theta)\). In other words, a decision maker is completely risk averse and anticipates the worst possible realization (\(\Pi_{n}\)-almost surely). Observe that (SO) strictly generalizes the standard Bayesian decision-theoretic formulation of a decision-making problem, where the goal is to solve \(\min_{a\in\mathcal{A}}\mathbb{E}_{\Pi_{n}}[R(a,\theta)].\)

The risk-sensitive formulation (SO) is very general and can be used to model a wide variety of decision-making problems in machine learning [16, 15], operations research/management science [24, 7, 20], simulation optimization [6, 32], and finance [17, 1, 4]. However, solving (SO) to compute an optimal decision over \(\mathcal{A}\) is challenging. The difficulty mainly stems from the fact that, with the exception of conjugate models, the posterior distribution in (1) is an intractable quantity. Canonically, posterior intractability is addressed using either a sampling- or optimization-based approach. Sampling-based approaches, such as Markov chain Monte Carlo (MCMC), offer a tractable way to compute the integrals and theoretical guarantees of exact inference in the large computational budget limit.

Optimization-based methods such as variational Bayes (VB) or variational inference (VI) have emerged as a popular alternative [3]. The VB approximation of the true posterior is a tractable distribution chosen from a'simpler' family of distributions known as a variational family by minimizing the discrepancy between the true posterior and members of that family. Kullback-Liebler (KL) divergence is the most often used measure of the approximation discrepancy, although other divergences (such as the \(\alpha\)-Renyi divergence [19, 28, 14]) have been used. The minimizing member, termed the VB approximate posterior, can be used as a proxy for the true posterior. Empirical studies have shown that VB methods are computationally faster and far more scalable to higher-dimensional problems and large datasets. Theoretical guarantees, such as large sample statistical inference, have been a topic of recent interest in the theoretical statistics community, with asymptotic properties such as convergence rate and asymptotic normality of the VB approximate posterior recently established in [34, 21] and [31, 14] respectively.

Our ultimate goal is not to merely approximate the posterior distribution but also to make decisions when that posterior is intractable. A _naive_ approach would be to plug in the VB approximation in place of the true posterior in (SO) and compute the optimal decision. However, it has been noted in [16] that such a naive loss-unaware approach can be suboptimal. In particular, [16] demonstrated, through an example, that a naive posterior approximation only captures the most dominant mode of the true posterior, which may not be relevant from a decision-making perspective. Consequently, they proposed a loss-calibrated variational Bayesian (LCVB) algorithm for solving Bayesian decision-making problems where the underlying risk function is discrete. [15] extended their approach to continuous risk functions. Despite these algorithmic advances in developing decision-centric VB methods, their statistical properties, such as asymptotic consistency and convergence rates of the loss-aware posterior approximation and the associated decision rule are not well understood. With an aim to address these gaps, we summarize our contribution in this paper below:

1. In Section 2, we introduce a minimax optimization framework titled 'risk-sensitive variational Bayes' (RSVB), extracted from the dual representation of (SO) using the so-called Donsker-Varadhan variational free-energy principle [9]. The decision-maker computes a risk-sensitive approximation to the true posterior (termed as RSVB posterior) and the decision rule simultaneously by solving a minimax optimization problem. Moreover, we recover the LCVB approach [16] as a special case of RSVB (with \(\gamma=1\)).
2. In Section 3, we identify verifiable regularity conditions on the prior, likelihood model and the risk function under which the RSVB posterior enjoys the same rate of convergence as the true posterior to a Dirac delta distribution concentrated at the true model parameter \(\theta_{0},\) as the sample size increases. Using this result, we also prove the rate of convergence of the RSVB decision rule when the decision space \(\mathcal{A}\) is compact. Our theoretical results also imply the asymptotic properties of the LCVB posterior and the associated decision rule.
3. In Section 4, we demonstrate our theoretical results with the help of three widely studied decision-making problems, including Gaussian process classification and a newsvendor problem. For each example, we show that the rate of convergence of the respective RSVB approximate posterior matches that of the corresponding true posterior.
4. In Section 4, we also present some simulation results with the single product (1-d) newsvendor problem, which is summarized here in Figure 1. The figures demonstrate the effect of changing \(\gamma\) on the optimality gap in values (see Definition 2.1) and the variance of the RSVB posterior for a given \(n\). In particular, we observe that for smaller \(n\), increasing \(\gamma\) (after a certain value) results in a significantly more risk-averse decision; however, the effect of increasing \(\gamma\) on risk-averse decision-making reduces as \(n\) increases. This observation demonstrates the fact that when there is enough certainty in the parameter estimation, the need to be risk-sensitive to parametric uncertainty diminishes. The Figure 1 also compares the optimality gap in values evaluated at RSVB decision rules and variance of the RSVB posterior for various values of \(\gamma\) against the naive VB approach and against the true posterior when the conjugate prior is used.

## 2 Risk-Sensitive Variational Bayes

Our approach exploits the dual representation of the log-exponential risk measure in (SO), which is convex (or extended coherent) risk measure [23; 10]. From the Donsker-Varadhan variational free energy principle [9] observe that,

\[\varrho_{\Pi_{n}}^{\gamma}(a)=\max_{Q\in\mathcal{M}}\left\{\mathbb{E}_{Q}[R(a, \theta)]-\gamma^{-1}\text{KL}(Q||\Pi_{n})\right\}\hskip 56.905512pt\gamma>0,\] (DV)

where \(\mathcal{M}\) is the set of all distribution functions that are absolutely continuous with respect to the posterior distribution \(\Pi_{n}\) and 'KL' represents the Kullback-Leibler divergence. Formally, for any two distributions \(P\) and \(Q\) defined on measurable space \((\Theta,\mathcal{T})\), the KL divergence is defined as \(\text{KL}(Q||P)=\int_{\Theta}dQ(\theta)\log\frac{dQ(\theta)}{dP(\theta)}\), when measure \(Q\) is absolutely continuous with respect to \(P\) and \(\infty\) otherwise. Notice that this dual formulation exposes the reason we choose to use the log-exponential risk - the 'free-energy' objective on the RHS provides a combined assessment of the risk associated with model estimation (the KL divergence \(\text{KL}(Q||\Pi_{n})\)) and the decision risk under the estimated posterior \(Q\) (\(\mathbb{E}_{Q}[R(a,\theta)]\)).

As stated above, the reformulation presented in (DV) offers no computational gains. However, restricting ourselves to an appropriately chosen subset \(\mathcal{Q}\subset\mathcal{M}\), that consists of distributions where the integral \(\mathbb{E}_{q}[R(a,\theta)]\) and \(\text{KL}(Q||\Pi_{n})\) can be tractably computed and optimized, we immediately obtain a _risk-sensitive variational Bayesian_ (RSVB) formulation from (DV):

\[\gamma^{-1}\log\mathbb{E}_{\Pi_{n}}\left[e^{\gamma R(a,\theta)}\right]\geq \max_{Q\in\mathcal{Q}}\left\{\mathbb{E}_{Q}[R(a,\theta)]-\gamma^{-1}\text{KL} (Q||\Pi_{n})\right\}=:\mathcal{F}(a;Q(\cdot),\tilde{X}_{n},\gamma).\] (RSVB)

RSVB is our framework for data-driven risk-sensitive decision-making. The family of distributions \(\mathcal{Q}\) is popularly known as the _variational family_. Our analysis in Section 3.1 reveals general guidelines on how to choose \(\mathcal{Q}\) that ensures a small optimality gap (defined below) with high probability.

With an appropriate choice of \(\mathcal{Q}\), the optimization on the RHS can yield a good approximation to the log-exponential risk measurement on the left hand side (LHS). For brevity, for a given \(a\in\mathcal{A}\) we define the RSVB approximation to the true posterior \(\Pi(\theta|\tilde{X}_{n})\) as

\[Q_{a,\gamma}^{*}(\theta|\tilde{X}_{n}):=\text{argmax}\{Q\in\mathcal{Q}: \mathcal{F}(a;Q(\cdot),\tilde{X}_{n},\gamma)\}\]

Figure 1: (a) Optimality gap in values and (b) variance of the RSVB posterior (mean over 100 sample paths) against the number of samples (\(n\)) for various values of \(\gamma\).

and the RSVB optimal decision as

\[\mathsf{a}^{*}_{\mathsf{RB}}:=\text{argmin}_{a\in\mathcal{A}}\;\mathcal{F}(a;Q^{*} _{a,\gamma}(\theta|\tilde{X}_{n}),\tilde{X}_{n},\gamma)=\text{argmin}_{a\in \mathcal{A}}\max_{Q\in\mathcal{Q}}\mathcal{F}(a;Q(\cdot),\tilde{X}_{n},\gamma).\]

Observe that \(Q^{*}_{a,\gamma}(\theta|\tilde{X}_{n})\) and \(\mathsf{a}^{*}_{\mathsf{RB}}\) are random quantities, conditional on the data \(\tilde{X}_{n}\).

Examples of \(\mathcal{Q}\) include the family of Gaussian distributions, delta functions, or the family of factorized'mean-field' distributions that discard correlations between components of \(\theta\). The choice of \(\mathcal{Q}\) is decisive in determining the performance of the algorithm. Part of the analysis in this paper is to articulate sufficient conditions on \(\mathcal{Q}\) that ensure small optimality gap (defined below) for the optimal decision, \(\mathsf{a}^{*}_{\mathsf{RB}}\). This establishes the statistical "goodness" of the procedure as number of samples increase. In this paper, we analyze the efficacy of the decision rules obtained using the RSVB approximation, by providing high-probability bounds on the optimality gap. We define the _optimality gap_ for any \(\mathsf{a}\in\mathcal{A}\) with value \(V=R(\mathsf{a},\theta_{0})\) as,

**Definition 2.1** (Optimality Gap).: _Let \(V^{*}_{0}:=\min_{a\in\mathcal{A}}R(a,\theta_{0})\) be the optimal value for the true model parameter \(\theta_{0}\). Then, the optimality gap in the value is the difference \(V-V^{*}_{0}\)._

A similar performance measure was used in [15], to measure the effectiveness of loss-calibrated VB (LCVB) approach, which can be obtained by setting \(\gamma=1\). We provide definitions of some standard terminologies that we use in the subsequent sections, such as covering number, test functions, \(\Gamma\)-convergence, and primal feasibility in the Appendix A for ready reference. In the following section, we lay down important assumptions used throughout the paper to establish our theoretical results.

### Assumptions

In order to bound the optimality gap, we require some control over how quickly the posterior distribution concentrates at the true parameter \(\theta_{0}\). Our next assumption in terms of a verifiable test condition on the model (sub-)space is one of the conditions required to quantify this rate. Let \(L_{n}:\Theta\times\Theta\mapsto[0,\infty)\) be a distance metric on the model space.

**Assumption 2.1** (Model indentifiability).: _Fix \(n\geq 1\). Then, for any \(\epsilon>\epsilon_{n}\) such that \(\epsilon_{n}\to 0\) as \(n\to\infty\) and \(n\epsilon_{n}^{2}\geq 1\), there exists a measurable sequence of test functions \(\phi_{n,\epsilon}:\tilde{X}_{n}\mapsto[0,1]\) and sieve set \(\Theta_{n}(\epsilon)\subseteq\Theta\) such that (i) \(\mathbb{E}_{P^{n}_{0}}[\phi_{n,\epsilon}]\leq C_{0}\exp(-Cn\epsilon^{2}),\) and (ii) \(\sup_{\{\theta\in\Theta_{n}(\epsilon):L_{n}(\theta,\theta_{0})\geq C_{1}ne^{ 2}\}}\mathbb{E}_{P^{n}_{\theta}}[1-\phi_{n,\epsilon}]\leq\exp(-Cn\epsilon^{2})\)._

Observe that Assumption 2.1\((i)\) quantifies the rate at which a Type I error diminishes with the sample size, while the condition in Assumption 2.1\((ii)\) quantifies that of a Type II error. Notice that both of these are stated through test functions; indeed, what is required are consistent test functions. [11, Theorem 7.1] (stated in Appendix as Lemma C.7 for completeness) roughly implies that an appropriately bounded model subspace \(\{P^{n}_{\theta},\theta\in\Theta\}\) guarantees the existence of _consistent_ test functions, to test the null hypothesis that the true parameter is \(\theta_{0}\) against an alternate hypothesis - the alternate being defined using the distance function \(L_{n}\). Subsequently, we will use a specific distance function to obtain finite sample bounds for the optimality gap in decisions and values. Note that in some cases, it is also possible to construct consistent test functions directly without recourse to Lemma C.7. We demonstrate this in Section 4.1 below. Next, we assume a condition on the prior distribution that ensures that it provides sufficient mass to the sieve set \(\Theta_{n}(\epsilon)\subseteq\Theta\), as defined above in Assumption 2.1.

**Assumption 2.2**.: _Fix \(n\geq 1\). Then, for any \(\epsilon>\epsilon_{n}\) such that \(\epsilon_{n}\to 0\) as \(n\to\infty\) and \(n\epsilon_{n}^{2}\geq 1\), the prior distribution satisfies \(\Pi(\Theta^{\epsilon}_{n}(\epsilon))\leq\exp(-Cn\epsilon^{2})\)._

Notice that Assumption 2.2 is trivially satisfied if \(\Theta_{n}(\epsilon)=\Theta\). The next assumption ensures that the prior distribution places sufficient mass around a neighborhood of the distribution \(P^{n}_{0}\).

**Assumption 2.3** (Prior thickness).: _Fix \(n\geq 1\) and a constant \(\lambda>0\). Let \(A_{n}:=\left\{\theta\in\Theta:D_{1+\lambda}\right.\)\(\left.\left(P^{n}_{0}\|P^{n}_{\theta}\right)\leq C_{3}n\epsilon_{n}^{2}\right\},\) where \(D_{1+\lambda}\left(P^{n}_{0}\|P^{n}_{\theta}\right):=\frac{1}{\lambda}\log\int \left(\frac{dP^{n}_{0}}{dP^{n}_{\theta}}\right)^{\lambda}dP^{n}_{0}\) is the Renyi divergence between \(P^{n}_{0}\) and \(P^{n}_{\theta}\), assuming \(P^{n}_{0}\) is absolutely continuous with respect to \(P^{n}_{\theta}\). The prior distribution satisfies \(\Pi(A_{n})\geq\exp(-nC_{2}\epsilon_{n}^{2})\)._

This assumption guarantees that the prior distribution covers the neighborhood with positive mass. The above three assumptions are adopted from [11] and has also been used in [34] to prove convergence rates of variational posteriors. Interested readers may refer to [11] and [34] to read more about the above assumptions.

It is apparent by the first term in (RSVB) that in addition to Assumption 2.1, 2.2, and 2.3, we also require regularity conditions on the risk function \(R(a,\cdot)\). The next assumption restricts the prior distribution with respect to \(R(a,\theta)\).

**Assumption 2.4**.: _Fix \(n\geq 1\) and \(\gamma>0\). For any \(\epsilon>\epsilon_{n}\), \(a\in\mathcal{A}\), \(\mathbb{E}_{\Pi}[\mathbbm{1}_{\{\gamma R(a,\theta)>C_{4}(\gamma)ne^{2}\}}e^{ \gamma R(a,\theta)}]\leq\exp(-C_{5}(\gamma)n\epsilon^{2})\), where \(C_{4}(\gamma)\) and \(C_{5}(\gamma)\) are scalar positive functions of \(\gamma\)._

Note that the set \(\left\{\gamma R(a,\theta)>C_{4}(\gamma)n\epsilon^{2}\right\}\) represents the subset of the model space where the risk \(R(a,\theta)\) (for a fixed decision \(a\)) is large, and the prior is assumed to place sufficiently small mass over such sets. Moreover, using the Cauchy-Schwarz inequality observe that \(\mathbb{E}_{\Pi}[\mathbbm{1}_{\{\gamma R(a,\theta)>C_{4}(\gamma)n\epsilon^{2} \}}e^{\gamma R(a,\theta)}]\leq\left(\mathbb{E}_{\Pi}[\mathbbm{1}_{\{\gamma R( a,\theta)>C_{4}(\gamma)ne^{2}\}}]\right)^{1/2}\left(\mathbb{E}_{\Pi}[e^{2 \gamma R(a,\theta)}]\right)^{1/2}\leq e^{-C_{4}(\gamma)n\epsilon^{2}}\mathbb{ E}_{\Pi}[e^{2\gamma R(a,\theta)}]\), which implies that if the risk function is bounded in \((a,\theta)\), then the assumption is trivially satisfied. Finally, we also require the following condition lower bounding the risk function \(R\).

**Assumption 2.5**.: \(R(a,\theta)\) _is assumed to satisfy \(W:=\inf_{\theta\in\Theta}\inf_{a\in\mathcal{A}}e^{R(a,\theta)}>0\)._

Note that any risk function which is bounded from below in both the arguments satisfies this condition. In the next section, we establish high-probability bounds on the optimality gap in values and decision rules computed using RSVB approach for sufficiently large \(n\).

## 3 Asymptotic Analysis of the Optimality Gaps

Our first result, establishes an upper bound on the expected deviation from the true model \(P_{0}^{n}\), measured using distance function \(L_{n}(\cdot,\theta_{0})\), under the RSVB approximate posterior. We also note that the following result generalizes Theorem 2.1 of [34], which is exclusively for the case when \(\gamma\to 0^{+}\). Our proof techniques follows that of Theorem 2.1 in [34].

**Theorem 3.1**.: _Fix \(a^{\prime}\in\mathcal{A}\) and \(\gamma>0\). For any \(L_{n}(\theta,\theta_{0})\geq 0\), under Assumptions 2.1, 2.2, 2.3, 2.4, and 2.5, and for \(\min(C,C_{4}(\gamma)+C_{5}(\gamma))>C_{2}+C_{3}+C_{4}(\gamma)+2\) and \(\eta_{n}^{R}(\gamma):=\frac{1}{n}\inf_{Q\in\mathcal{Q}}\mathbb{E}_{P_{0}^{n}} \left[\operatorname{KL}(Q(\theta)||\Pi(\theta|\tilde{X}_{n}))-\gamma\inf_{a \in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\right],\) for sufficiently large \(n\) the RSVB approximator of the true posterior \(Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\) satisfies,_

\[\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}L_{n}(\theta,\theta_{0})dQ_{a^{\prime },\gamma}^{*}(\theta|\tilde{X}_{n})\right]\leq n\left(M(\gamma)\epsilon_{n}^ {2}+M\eta_{n}^{R}(\gamma)\right),\] (2)

_for a positive number \(M(\gamma)=2\left(C_{1}+MC_{4}(\gamma)\right)\), where \(M=\frac{2C_{1}}{\min(C,\lambda,1)}\)._

Proof sketch.: For brevity we denote the likelihood ratio as \(\mathcal{LR}_{n}(\theta,\theta_{0})=\frac{p(\tilde{X}_{n}|\theta)}{p(\tilde{X }_{n}|\theta_{0})}\). We prove this result using a series of lemma. The first Lemma C.1 separates the term \(\eta_{n}^{R}(\gamma)\), which is later analyzed using Assumption 3.1 in Section 3.1. Lemma C.1 establish that

\[\zeta\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}L_{n}(\theta,\theta _{0})\;dQ_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\right]\leq\log\mathbb{ E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\zeta L_{n}(\theta,\theta_{0})}\frac{e^{ \gamma R(a^{\prime},\theta)}\;\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta) }{\int_{\Theta}e^{\gamma R(a^{\prime},\theta)}\;\mathcal{LR}_{n}(\theta, \theta_{0})d\Pi(\theta)}\right]\\ +\log\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\;\frac{\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_ {\Theta}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]+n\eta_{n}^{R}( \gamma).\] (3)

The proof of Lemma C.1 uses simple arguments that follow easily from Jensen's inequality and the definition of the posterior distribution.

To analyze the first term in the above display, we define the set \(K_{n}:=\{\theta\in\Theta:L_{n}(\theta,\theta_{0})>C_{1}n\epsilon^{2}\}\), with an aim to control the exponential moment of \(L_{n}(\theta,\theta_{0})\) by characterizing its tails. Also, notice that set \(K_{n}\) is the set of alternate hypotheses as defined in Assumption 2.1 through Lemma C.2. For brevity, define \(\Pi_{R}(K_{n}|\tilde{X}_{n}):=\frac{f_{K_{n}}\,e^{\gamma R(a^{\prime},\theta)} \;\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\;\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\). Next, we divide the expected calibrated posterior probability of the set \(K_{n}\) as follows:

\[\mathbb{E}_{P_{0}^{n}}\left[\Pi_{R}(K_{n}|\tilde{X}_{n})\right]\leq\mathbb{E}_ {P_{0}^{n}}\phi_{n,\epsilon}+\mathbb{E}_{P_{0}^{n}}\left[\mathbbm{1}_{B_{n}^{C}} \right]+\mathbb{E}_{P_{0}^{n}}\left[(1-\phi_{n,\epsilon})\mathbbm{1}_{B_{n}} \Pi_{R}(K_{n}|\tilde{X}_{n})\right],\] (4)where recall that \(\{\phi_{n,\epsilon}\}\) is the sequence of test function from Assumption 2.1 and set \(B_{n}=\left\{\tilde{X}_{n}:\int_{\Theta}\mathcal{L}\mathcal{R}_{n}(\theta,\theta _{0})d\Pi(\theta)\geq e^{-(1+C_{3})n\epsilon^{2}}\Pi(A_{n})\right\}\) with set \(A_{n}\) is defined in Assumption 2.3. Set \(B_{n}\) is introduced to separately control (by lower bounding) the denominator in the definition of the posterior distribution in the last term of (4). In addition, the Assumption 2.3 is used to show that \(P_{0}^{n}(B_{C}^{C})\leq e^{-\lambda n\epsilon^{2}}\). The first term can be controlled by Assumption 2.1 (i). Now, it remains to analyze the last term in (4). Using Assumption 2.3 and 2.5 observe that on set \(B_{n}\), \(\int_{\Theta}e^{\gamma R(a^{\prime},\theta)}\mathcal{L}_{n}(\theta,\theta_{0} )d\Pi(\theta)\geq W^{\gamma}e^{-(1+C_{2}+C_{3})n\epsilon^{2}}.\) This observation, with the application of Fubini's theorem, enables us to bifurcate further the last term in (4) on the set \(S_{\gamma}(\theta):=\{e^{\gamma R(a^{\prime},\theta)}>e^{C_{4}(\gamma)n \epsilon^{2}}\}\) as

\[\mathbb{E}_{P_{0}^{n}}\left[(1-\phi_{n,\epsilon})\mathbbm{1}_{B_ {n}}\Pi_{R}(K_{n}|\tilde{X}_{n})\right]\leq W^{-\gamma}e^{\tilde{C}n\epsilon^{ 2}}\bigg{[} \int_{K_{n}\cap S_{\gamma}(\theta)}\mathbb{E}_{P_{0}^{n}}\left[(1- \phi_{n,\epsilon})\right]d\Pi(\theta)\] \[+e^{-C_{4}(\gamma)n\epsilon^{2}}\!\int_{K_{n}\cap S_{\gamma}( \theta)}e^{\gamma R(a^{\prime},\theta)}d\Pi(\theta)\bigg{]},\] (5)

\(\tilde{C}=1+C_{2}+C_{3}+C_{4}(\gamma).\) Observe that the last term above can be controlled using Assumption 2.4. The first integral in (5) can be further bounded by \(\int_{K_{n}\cap\Theta_{n}(\epsilon)}\mathbb{E}_{P_{0}^{n}}\left[(1-\phi_{n, \epsilon})\right]d\Pi(\theta)+\Pi(\Theta_{n}(\epsilon)^{c}),\) where \(\Theta_{n}(\epsilon)\) is defined in Assumption 2.1 and both the terms can be exponentially bounded using Assumption 2.1 (ii) and 2.2 respectively. The last term in (3) can be bounded using a similar set of arguments and techniques. 

_Remark:_ We note that while deriving the bound in Lemma C.1, we can interchange the order of expectation and infimum and compute a tighter bound. However, this bring us back to the RSVB optimizer, the very objective of the whole analysis. Taking expectation before infimum enables us to derive a more interpretable and meaningful bound. This is mainly because it is easier to control the expectation of the RSVB objective than its infimum through Assumption 3.1, as presented in Section 3.1.

The detailed proof of Theorem 3.1 is provided in the Appendix C.2. Now recall that \(\epsilon_{n}\) is the convergence rate of the true posterior [11, Theorem 7.3]. Notice that the additional term \(\eta_{n}^{R}(\gamma)\) emerges from the posterior approximation and depends on the choice of the variational family \(\mathcal{Q}\), risk function \(R(\cdot,\cdot)\), and the parameter \(\gamma\). The appearance of \(\eta_{n}^{R}(\gamma)\) in the bound also signifies that to minimize the expected gap (under the RSVB posterior) between the true model and any other model (defined using \(n^{-1}L_{n}(\theta,\theta_{0})\)) the expected RSVB objective has to be maximized. Later in this section, we specify the conditions that ensure \(\eta_{n}^{R}(\gamma)\to 0\) as \(n\to\infty\). Moreover, we also identify mild regularity conditions on \(\mathcal{Q}\) to show that \(\eta_{n}^{R}(\gamma)\) is \(O(\epsilon_{n}^{2})\) and show that as \(\gamma\) increases \(\eta_{n}^{R}(\gamma)\) decreases. We discuss this result and the bound therein later in the next subsection. Before that, we establish our main result (the bounds on the optimality gap) using the theorem above. We now fix

\[L_{n}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}|R(a,\theta)-R(a,\theta _{0})|\right)^{2}.\] (6)

Notice that for a given \(\theta\), \(n^{-1/2}\sqrt{L_{n}(\theta,\theta_{0})}\) is the uniform distance between the \(R(a,\theta)\) and \(R(a,\theta_{0})\). Intuitively, Theorem 3.1 implies that the expected uniform difference \(\frac{1}{n}L_{n}(\theta,\theta_{0})\) with respect to the RSVB approximate posterior is \(O(M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}(\gamma))\), and if \(M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}(\gamma)\to 0\) as \(n\to\infty\) then it converges to zero at that rate.

Also, note that in order to use (6) we must demonstrate that it satisfies Assumption 2.1. This can be achieved by constructing bespoke test functions for a given \(R(a,\theta)\). We demonstrate this approach by an example in Section B.2. We also provide sufficient conditions for the existence of the test functions in the appendix. These conditions are typically easy to verify when the loss function \(R(\cdot,\cdot)\) is bounded, for instance. Next, we bound the optimality gap between \(R(\mathsf{a}_{\text{gs}}^{*},\theta_{0})\) and \(V_{0}^{*}\).

**Theorem 3.2**.: _Fix \(\gamma>0\). Suppose the set \(\mathcal{A}\) is compact. Then, under Assumptions 2.1, 2.2, 2.3, 2.4, and 2.5, for \(\min(C,C_{4}(\gamma)+C_{5}(\gamma))>C_{2}+C_{3}+C_{4}(\gamma)+2\) and any \(\tau>0\), the \(P_{0}^{n}-\) probability of \(\left\{\tilde{X}_{n}:R(\mathsf{a}_{\text{gs}}^{*},\theta_{0})-\inf_{a\in \mathcal{A}}R(a,\theta_{0})\leq 2\tau\left[M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}( \gamma)\right]^{\frac{1}{2}}\right\}\) is at least \(1-\tau^{-1}\), for a positive mapping \(M(\gamma)=2\left(C_{1}+MC_{4}(\gamma)\right)\), where \(M=\frac{2C_{1}}{\min(C,\lambda,1)}\) for sufficiently large \(n\)._

### Properties of \(\eta_{n}^{R}(\gamma)\)

Evidently, the bounds obtained in both the results that we have proved so far depend on \(\eta_{n}^{R}(\gamma)\). Consequently, we establish some important properties of \(\eta_{n}^{R}(\gamma)\) with respect to \(n\) and \(\gamma\), under additional regularity conditions. In order to characterize \(\eta_{n}^{R}(\gamma)\), we specify conditions on variational family \(\mathcal{Q}\) such that \(\eta_{n}^{R}(\gamma)=O(\epsilon_{n}^{\prime 2})\), for some \(\epsilon_{n}^{\prime}\geq\frac{1}{\sqrt{n}}\) and \(\epsilon_{n}^{\prime}\to 0\). We impose the following condition on the variational family \(\mathcal{Q}\) that lets us obtain a bound on \(\eta_{n}^{R}(\gamma)\) in terms of \(n\) and \(\gamma\).

**Assumption 3.1**.: _There exists a sequence of distributions \(\{q_{n}(\cdot)\}\) in the variational family \(\mathcal{Q}\) such that for a positive constant \(C_{9}\),_

\[\frac{1}{n}\left[\text{KL}\left(Q_{n}(\theta)||\Pi(\theta)\right)+\mathbb{E}_ {Q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}(\tilde{X}_{n})||dP_{\theta}^{n} (\tilde{X}_{n})\right)\right]\right]\leq C_{9}\epsilon_{n}^{\prime 2}.\] (7)

If the observations in \(\tilde{X}_{n}\) are i.i.d, then observe that \(\frac{1}{n}\mathbb{E}_{Q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}(\tilde{X }_{n})\|dP_{\theta}^{n}(\tilde{X}_{n})\right)\right]=\mathbb{E}_{Q_{n}(\theta )}\left[\text{KL}\left(dP_{0}||dP_{\theta}\right)\right].\) Intuitively, this assumption implies that the variational family must contain a sequence of distributions weakly convergent to a Dirac delta distribution concentrated at the true parameter \(\theta_{0}\) otherwise the second term in the LHS of (7) will be non-zero. Also, note that the above assumption does not imply that the minimizing sequence \(Q_{\epsilon,\gamma}^{*}(\theta|\tilde{X}_{n})\) (automatically) converges weakly to a Dirac-delta distribution at the true parameter \(\theta_{0}\). Furthermore, unlike Theorem 2.3 of [34], our condition on \(\mathcal{Q}\) in Assumption 3.1, to obtain a bound on \(\eta_{n}^{R}(\,\gamma)\), does not require the support of the distributions in \(\mathcal{Q}\) to shrink to the true parameter \(\theta_{0}\) at some appropriate rate, as the numbers of samples increases.

The condition that the variational family contains Dirac delta distributions at each point in the parameter space is a mild and reasonable requirement for consistency. Further, Assumption 3.1 requires that \(\mathcal{Q}\) contains sequences of distributions that weakly converge to each Dirac delta distribution at a certain rate. This is easily satisfied if \(\mathcal{Q}\) has no "holes", e.g. if it is the family of Gaussians with all means and variances, then we can always construct sequences converging to any Dirac delta at any rate. A similar assumption has also been made in [31], and is true for most exponential family distributions. For instance, in the newsvendor application, we fix the variational family to a class of shifted-Gamma distributions and choose a sequence of distributions with parameter sequence \(\alpha=n\) and \(\beta=n/\theta_{0}\). This implies that the sequence of distributions has mean \(\theta_{0}\) and variance \(\theta_{0}^{2}/n\), and, therefore, it converges to a Dirac delta distribution at \(\theta_{0}\). Next, we show that

**Proposition 3.1**.: _Under Assumption 3.1 and for a constant \(C_{8}=-\inf_{Q\in\mathcal{Q}}\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\) and \(C_{9}>0\), \(\eta_{n}^{R}(\gamma)\leq\gamma n^{-1}C_{8}+C_{9}\epsilon_{n}^{\prime 2}\)._

In Section 4, we present an example where the likelihood is exponentially distributed, the prior is inverse-gamma (non-conjugate), and the variational family is the class of gamma distributions, where we construct a sequence of distributions in the variational family that satisfies Assumption 3.1. We also provide another example where the likelihood is multivariate Gaussian with unknown mean and variational family is uncorrelated Gaussian restricted to a compact subset of \(\mathbb{R}^{d}\) with a uniform prior on the same compact set satisfy Assumption 3.1.

By definition \(\epsilon_{n}^{2}\to 0\) and \(\epsilon_{n}^{\prime}\to 0\) as \(n\to\infty\), and therefore it follows from Proposition 3.1 that \(M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}(\gamma)\to 0\). However, the bound obtained in the last proposition might be loose with respect to \(\gamma\), when \(C_{8}<0\). To see this, we prove the following result.

**Proposition 3.2**.: _If the solution to the optimization problem in \(\eta_{n}^{R}(\gamma)\) is primal feasible then \(\eta_{n}^{R}(\gamma)\) decreases as \(\gamma\) increases._

## 4 Applications

In the examples below, we use three examples to study the interplay between sample size \(n\) and the risk parameter \(\gamma\), and their effect on the optimality gap in values. Additionally, we consider a multi-product newsvendor example in the Appendix B.2.

### Single-product Newsvendor Model

We start with a canonical data-driven decision-making problem with a 'well-behaved' risk function \(R(a,\theta)\), the data-driven newsvendor model. This problem has received extensive study in the literature and remains a cornerstone of inventory management [25; 2; 18]. Recall that the newsvendorloss function is defined as \(\ell(a,\xi):=h(a-\xi)^{+}+b(\xi-a)^{+}\) where \(h\) (underage cost) and \(b\) (overage cost) are given positive constants, \(\xi\in[0,\infty)\) the random demand, and \(a\) the inventory or decision variable, typically assumed to take values in a compact decision space \(\mathcal{A}\) with \(\underline{a}:=\min\{a:a\in\mathcal{A}\}\) and \(\bar{a}:=\max\{a:a\in\mathcal{A}\}\), and \(\underline{a}>0\). The distribution over the random demand, \(P_{\theta}\) is assumed to be exponential with unknown rate parameter \(\theta\in(0,\infty)\). The model risk \(R(a,\theta):=\mathbb{E}_{P_{\theta}}[\ell(a,\xi)]=ha-\frac{h}{\theta}+(b+h) \frac{e^{-a\theta}}{\theta}\), which is convex in \(a\). We assume that \(\tilde{X}_{n}:=\{\xi_{1},\xi_{2}\ldots\xi_{n}\}\) be \(n\) observations of the random demand, assumed to be i.i.d random samples drawn from \(P_{0}\).

We fix the model space \(\Theta=[T,\infty)\) for some \(T>0\) and assume that \(\theta_{0}\) lies in the interior of \(\Theta\). We now assume a non-conjugate truncated inverse-gamma (\(\text{Inv}-\Gamma\)) prior distribution restricted to \(\Theta\), with shape and rate parameter \(\alpha\) and \(\beta\) respectively, that is for a set \(A\subseteq\Theta\), we define \(\Pi(A)=\text{Inv}-\Gamma_{\Theta}(A;\alpha,\beta)=\text{Inv}-\Gamma(A\cap \Theta;\alpha,\beta)/\text{Inv}-\Gamma(\Theta;\alpha,\beta)\,.\) We verify Assumptions 2.2, 2.1, 2.3, 2.5 and 2.4 (in that order) in this newsvendor setting and provide the proofs in the Appendix B.1. Next, we bound the optimality gap in values for the single product newsvendor model risk.

**Theorem 4.1**.: _Fix \(\gamma>0\). Suppose that the set \(\mathcal{A}\) is compact. Then, for the newsvendor model with exponentially distributed demand with rate \(\theta\in\Theta=[T,\infty)\), prior distribution \(\Pi(\cdot)=\text{Inv}-\Gamma_{\Theta}(\cdot;\alpha,\beta)=\text{Inv}-\Gamma(A \cap\Theta;\alpha,\beta)/\text{Inv}-\Gamma(\Theta;\alpha,\beta)\), and the variational family fixed to shifted (by \(T>0\)) gamma distributions, and for any \(\tau>0\), the \(P_{0}^{n}-\) probability of the following event \(\left\{\tilde{X}_{n}:R(\mathbf{a}_{\text{\tiny{RS}}}^{*},\theta_{0})-\inf_{z \in\mathcal{A}}R(z,\theta_{0})\leq 2\tau M^{\prime}(\gamma)\left(\frac{\log n }{n}\right)^{1/2}\right\}\) is at least \(1-\tau^{-1}\) for sufficiently large \(n\) and for some known mapping \(M^{\prime}:\mathbb{R}^{+}\to\mathbb{R}^{+}\), where \(R(\cdot,\theta)\) is the newsvendor model risk._

The proof of the theorem above is a direct consequence of Theorem 3.2 and Proposition 3.2, and Lemmas B.1, B.2, B.3, B.4, B.5 stated in the Appendix. We also extend the analysis above to a multi-product newsvendor problem. The details are provided in Appendix B.2.

Next, we demonstrate the effect of varying the risk-sensitivity parameter \(\gamma\). We fix \(\theta_{0}=0.1\), \(b=1\), \(h=5\), \(\alpha=1\), and \(\beta=4.1\). We run RSVB algorithm with \(\gamma\in\{0(\text{naive }),1,2,4.5,5,6\}\) and repeat the experiment over 100 sample paths. We plot the results in Figure 1. In Figure 1(a) we plot the optimality gap in values that is \(R(\mathbf{a}_{\text{\tiny{RS}}}^{*}(\gamma),\theta_{0})-R(a_{0}^{*},\theta_{0})\) for various values of \(\gamma\). We observe that the gap decreases when \(n\) increases. This observation supports our results in Propositions 3.1 and 3.2 that establishes the properties of \(\eta_{n}^{R}(\gamma)\) as \(n\) increases. Lastly, in Figure 1(b), we plot the variance of the RSVB posterior as \(n\) increases for various values of \(\gamma\); as anticipated, the variance reduces as \(n\) increases. To observe the effect of \(\gamma\), first recall that as \(\gamma\) increases the decision maker becomes more risk averse, and so is our algorithmic framework RSVB. Indeed, from the rightmost variance plot in Figure 1, it is evident that for a larger value of \(\gamma\) (\(>4\)), the RSVB posterior is more concentrated on the subset of \(\Theta\), where risk is more and consequently we observe large optimality gaps in values. Moreover, as \(n\) increases, the effect of larger \(\gamma\) reduces, since as \(n\) increases, the incentive to deviate from the posterior reduces (due to increased KL divergence dominance for larger \(n\) in RSVB). We also observe that the decision rule learned using the conjugate prior (Gamma distribution for exponential models) has a similar performance as the naive approach. However, the variance of the true conjugate posterior is higher than those computed through the RSVB and VI approach, corresponding to the well known fact that the variational approximations underestimate the true posterior variance ([28; 19]).

### Gaussian process classification

Consider a problem of classifying an input pattern or features \(Y\) lying in measure space \(([0,1]^{d},\mathcal{Y},\nu)\) (with sigma algebra \(\mathcal{Y}\) and probability measure \(\nu\)) into one of two classes \(\{-1,1\}\). Let \(Y\mapsto\xi(Y)\in\{-1,1\}\) denote the class of \(Y\). For a given \(Y\), we model the classifier using a Bernoulli distribution \(p(\xi|Y,\theta)=\Psi_{\xi}(\theta(Y))\), where \(\theta:[0,1]^{d}\to\mathbb{R}\) is a non-parametric model parameter in a separable Banach space \((\Theta,\|\cdot\|)\) and measurable functions \(\Psi_{1}(x)=(1+e^{-x})^{-1}\) and \(\Psi_{-1}(x)=1-\Psi_{1}(x)\). Note that \(\Psi_{1}(\cdot)\) is a logistic function. We denote \(\psi(\cdot)\) as the derivative of \(\Psi_{1}(\cdot)\). Assume that the features \(Y\) are generated independently of \(\xi\). Thus the sequence of independent observations \(\{\tilde{Y}_{n},\tilde{X}_{n}\}=\{(Y_{1},\xi_{1}),(Y_{2},\xi_{2}),\ldots,(Y_{n},\xi_{n})\}\) are assumed to be generated from the model \(dP_{\theta}(a,y)=P_{\theta}(\xi=a,Y\in dy)=p(\xi=a|Y=y,\theta)\nu(dy)\). In the above binary classification problem, the objective is to estimate \(\theta(\cdot)\) using the observation vector \(\{\tilde{Y}_{n},\tilde{X}_{n}\}\).

We posit a Gaussian process (GP) prior \(\Pi(\cdot)\) on \(\theta(\cdot)\in\Theta\) defined as \(W(\cdot)=\sum_{j=1}^{J_{\alpha}}\sum_{k=1}^{2^{jd}}\mu_{j}Z_{j,k}\vartheta_{j,k}( \cdot)\), where \(\{\mu_{j}\}\) is a sequence that decreases with \(j\), \(\{Z_{i,j}\}\) are i.i.d. standard Gaussian random variables and \(\{\vartheta_{j,k}\}\) form a double-indexed orthonormal basis (with respect to measure \(\nu\)), that is \(\mathbb{E}_{\nu}[\vartheta_{j,k}\vartheta_{l,m}]=\mathbbm{1}_{\{j=l,k=m\}}\)). \(\bar{J}_{\alpha}\) is the smallest integer satisfying \(2^{\bar{J}_{\alpha}d}=n^{d/(2\alpha+d)}\) for a given \(\alpha>0\). This prior construction is motivated from the work in [30]. We also assume that \(\nu(\cdot)\) is known, and we do not place any prior on it. The posterior distribution over \(\theta(\cdot)\) given observations \(\{\tilde{Y}_{n},\tilde{X}_{n}\}\) can be defined as \(d\Pi(\theta|\{\tilde{Y}_{n},\tilde{X}_{n}\})=\frac{d\Pi(\theta)\prod_{i=1}^{n }\Psi_{\xi_{i}}(\theta(Y_{i}))\nu(Y_{i})}{\int\prod_{i=1}^{n}\Psi_{\xi_{i}}( \theta(Y_{i}))\mu(\theta)}=\frac{d\Pi(\theta)\prod_{i=1}^{n}\Psi_{\xi_{i}}( \theta(Y_{i}))}{\int\prod_{i=1}^{n}\Psi_{\xi_{i}}(\theta(Y_{i}))d\Pi(\theta)}\). Consider the loss function \(\ell(a,\xi)=\{0,\text{ if }a=\xi;\quad c_{+},\text{if }a=+1,\xi=-1;\quad c_{-}, \text{if }a=-1,\xi=+1\}\), where \(c_{+}\) and \(c_{-}\) are known positive constants. For a given feature \(Y\), the model risk is given by

\[R(a,\theta)=\mathbb{E}_{P_{q}}[\ell(a,\xi)]=\begin{cases}c_{+}\mathbb{E}_{\nu }[\Psi_{-1}(\theta(Y))],\quad a=+1,\\ c_{-}\mathbb{E}_{\nu}[\Psi_{1}(\theta(Y))],\quad a=-1.\end{cases}\] (8)

We fix the variational family \(\mathcal{Q}_{GP}\) is a class of Gaussian distributions on \(\Theta\), defined as \(\mathcal{N}(m_{q},\mathcal{C}_{q})\), \(m_{q}\) belongs to \(\Theta\) and \(\mathcal{C}_{q}\) is the covariance operator defined as \(\mathcal{C}_{q}=\mathcal{C}^{1/2}(I-S)\mathcal{C}^{1/2}\), for any \(S\) which is a symmetric and Hilbert-Schmidt (HS) operator on \(\Theta\) (eigenvalues of HS operator are square summable). Note that \(S\) and \(m_{q}\) span the distributions in \(\mathcal{Q}_{GP}\). We can show, using the technical lemmas derived in Section B.3 in Appendix, that the optimality gap in values of the binary GP classification problem converges to zero at a minimax optimal rate (upto logarithmic factors).

**Theorem 4.2**.: _Fix \(\gamma>0\) and for a given \(J\in\mathbb{N}\). For the binary GP classification problem with GP prior induced by \(W=\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\mu_{j}Z_{j,k}\vartheta_{j,k}\), where \(\mu_{j}=2^{-jd/2-j\mathsf{a}}\) for some \(\mathsf{a}>0\), \(\{Z_{i,j}\}\) are i.i.d. standard Gaussian random variables and \(\{\vartheta_{j,k}\}\) form a double-indexed orthonormal basis (with respect to measure \(\nu\)), and \(\|\theta_{0}\|_{\beta;\infty,\infty}<\infty\), and \(\theta_{0}^{J}(y)\) lie in the Cameron-Martin space \(\text{Im}(\mathcal{C}^{1/2})\), the variational family \(\mathcal{Q}_{GP}\), and for any \(\tau>0\), the \(P_{0}^{n}-\) probability of \(\left\{\tilde{X}_{n}:R(\mathsf{a}_{\mathsf{RS}}^{*},\theta_{0})-\inf_{z\in \mathcal{A}}R(z,\theta_{0})\leq 2\tau M^{\prime}(\gamma)\epsilon_{n}\right\}\) is at least \(1-\tau^{-1}\) for sufficiently large \(n\) and for some mapping \(M^{\prime}:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\), where \(R(\cdot,\theta)\) is defined in (8) and_

\[\epsilon_{n}=\begin{cases}n^{-\frac{\beta}{(2\alpha+d)}}\log n&\forall\beta \in[\mathsf{a},\alpha];\quad n^{-\frac{\alpha}{(2\alpha+d)}}\log n&\forall \alpha\in[\mathsf{a},\beta];\\ n^{-\frac{\mathsf{a}}{(2\alpha+d)}}(\log n)^{\frac{d}{(2\alpha+d)}},\quad \forall\mathsf{a}\in[\alpha,\beta];\quad n^{-\frac{\beta}{(2\alpha+d)}}(\log n )^{\frac{d}{(2\alpha+d)}}&\forall\beta\in[\alpha,\mathsf{a}].\end{cases}\] (9)

A similar Gaussian process classification problem was studied empirically in [16].

### Eight-schools model

We consider the eight-schools problem [33; 15], where the objective is to study the effectiveness of the special coaching program for SAT exams, offered in \(8\) schools. Each school reported the treatment effect \(y_{i}\) and its standard deviation \(\sigma_{i}\), where \(i\in\{1,2,3,\ldots,8\}\). The observations \(\{y_{i},\sigma_{i}\}_{i=1}^{8}\) are modeled using the following hierarchical Bayesian model : 1) Prior distributions: \(\mu\sim\mathcal{N}(\cdot|0,5)\,,\,\tau\sim\text{half-Cauchy}(\cdot|0,5)\), 2) \(\theta_{i}\sim\mathcal{N}(\cdot|\mu,\tau^{2})\) and \(y_{i}\sim\mathcal{N}(\cdot|\theta_{i},\sigma_{i}^{2})\) for each \(i\in\{1,2,3,\ldots,8\}\), where \(\{\sigma_{i}\}_{i=1}^{8}\) are assumed to be a known sequence of covariates. The posterior distribution is defined as \(\pi_{8}(\mu,\tau,\{\theta_{i}\}_{i=1}^{8}|\{y_{i},\sigma_{i}\}_{i=1}^{8})\propto \mathcal{N}(\cdot|0,5)\text{half-Cauchy}(\cdot|0,5)\prod_{i=1}^{8}\mathcal{N}( \theta_{i}|\mu,\tau^{2})\mathcal{N}(y_{i}|\theta_{i},\sigma_{i}^{2}).\) For a given treatment effect for eight schools \(y\in\mathbb{R}^{8}\), the loss function is defined as:

\[\ell(y,a)=\sum_{i=1}^{8}l(y_{i},a_{i}),\text{ where }l(y_{i},a_{i})= \begin{cases}0.2\,|a_{i}-y_{i}|,\,\,\,y_{i}\geq a_{i}\\ (1-0.2)\,|a_{i}-y_{i}|,\,\,\,y_{i}<a_{i},\end{cases}\] (10)

where the decision variable \(a=\{a_{1},a_{2},\ldots,a_{8}\}\) denotes the effectiveness level. We define \(R(a,\theta)=\log\int p(y|\theta,\{\sigma_{i}\}_{i=1}^{8})\ell(y,a)dy\), \(\theta\in\mathbb{R}^{8}\) considering \(\{\sigma_{i}\}_{i=1}^{8}\) is a given sequence of covariates. Now for any \(\gamma>0\), RSVB joint-optimization problem is defined as \(\min_{a\in\mathcal{A}}\max_{Q\in\mathcal{Q}}\left\{\mathbb{E}_{q}[R(a,\theta)] -\frac{1}{\gamma}\text{KL}(q||\pi_{8})\right\}.\) We fix the variational family \(\mathcal{Q}\) to be the mean-field variational family, that is \(q_{\Lambda}(\mu,\tau,\theta_{1}^{8}=)=\mathcal{N}_{\mu}(\cdot|\lambda_{9}) \mathcal{N}_{\tau}(\cdot|\lambda_{10})\prod_{i=1}^{8}\mathcal{N}_{\theta_{i}}(| \lambda_{i}),\) where \(\Lambda=\{\lambda_{1},\lambda_{2},\ldots,\lambda_{10}\}\), \(\lambda_{i}\) is the mean and variance parameter for each Gaussian distribution. Following [15], we measure the performance of the RSVB method using the metric called _empirical risk reduction_ (ERR), \(\mathcal{I}=\mathcal{E}\mathcal{R}_{\text{VB}}-\mathcal{E}\mathcal{R}_{\text{RSVB}( \gamma)},\text{ where },\,\mathcal{E}\mathcal{R}_{\text{alg}}=\frac{1}{N_{Y}}\sum_{j=1}^{N_{Y}}l(y_{ j}^{\text{test}},a_{\text{alg}}),\) where \(\mathcal{E}\mathcal{R}_{\text{alg}}\) denote the empirical risk evaluated at the decision rule \(a^{\text{alg}}\) obtained using method alg \(\in\) {VB,RSVB(\(\gamma\))}

for various values of \(\gamma\), and \(N_{Y}\) is the size of the test data \(\{y^{\text{test}}_{j}\}\). Note that ERR is the empirical approximation of the difference of optimality gap between the two-step naive VB approach and RSVB\((\gamma)\) approach. Recall, in the naive VB method, we first compute the KL minimizer of the true posterior and then compute the optimal decision using this approximate posterior. Following [15], due to small size of the dataset in this example, the ERR is evaluated on the training data itself.

We modified the experiments in [15] by introducing \(\gamma=\{5,2.5,1,0.5\}=\{0.2^{-1},0.4^{-1},1^{-1},2^{-1}\}\) and obtain the results as summarized in Figure 2(a) and (b). In Figure 2(a), we observe that as \(\gamma\) increases, the ERR also increases, which implies that the decisions are more optimistic as empirical loss of the RSVB decision rule decreases as \(\gamma\) increases. Also, observe from Figure 2(b) that, as \(\gamma\) increases, the RSVB posterior (joint marginal posterior distribution for \((\theta_{3},\tau)\)) approaches the true posterior, and the variance of the approximate posterior also reduces.

Notice that it is not obvious that the variance of the RSVB posterior will reduce as \(\gamma\) increases. We believe that it depends on the landscape of the expected risk function and the choice of the variational family. Intuitively, a possible explanation of this phenomenon can be provided using the equation (RSVB). Consider the RSVB formulation and note that \(\text{KL}>0\), therefore as \(\gamma\) increases, there is more incentive to deviate from the true posterior and choose \(Q\in\mathcal{Q}\) that maximizes expected risk for a given \(a\in\mathcal{A}\). Note that a \(Q\) that places more mass near the \(\theta\) that maximizes the risk will be preferred over the one with more spread.

## 5 Discussion and Future Work

The RSVB formulation as stated in this paper requires us to solve a stochastic minimax optimization problem to compute a decision rule. This is often difficult to solve, particularly in the nonconvex-nonconcave setting of RSVB, and indeed, we are not aware of any computationally efficient methods for solving such problems in all generality [8]. To circumvent this, the authors in the LCVB literature maximize utility instead of minimizing risk, and thus convert the whole problem to a much simpler max-max optimization problem. If we replace risk with utility in the RSVB derivation, we will also get a max-max problem. In this formulation, as you increase \(\gamma\), the incentive to deviate from the posterior and maximize the maximum utility increases. Therefore, increasing \(\gamma\) corresponds to being more optimistic in making decisions than being more risk averse in the risk setting, as observed in the empirical results for the _eight schools model_ in Section 4.3.

We note that we assumed the risk-function is lower-bounded, this is a natural assumption to place on risk functions. Converting our problem to a max-max problem would further require the risk to be upper bounded, which is also assumed for the methodologies presented (without theory) in [16; 15]. We emphasize that our theoretical results will continue to hold in this setting, but since our emphasis was on the statistical aspects of this problem, we chose to present our results with minimum assumptions. Developing an algorithm for solving general minimax optimization in RSVB without transforming it into a max-max problem is open and a part of our future work. Also an interesting topic for future work is identifying minimum additional assumptions to theoretically characterize computational aspects of this problem.

Figure 2: a) Empirical risk reduction plot for different level of risk sensitivity, b) Joint RSVB posterior distribution of \(\{\theta_{3},\tau\}\), plotted for different level of risk sensitivity.

Acknowledgement

We thank the anonymous reviewers for their helpful comments and discussions. This work is supported by the National Science Foundation under grant DMS-1812197.

## References

* Bauder et al. [2020] D. Bauder, T. Bodnar, N. Parolya, and W. Schmid. Bayesian mean-variance analysis: optimal portfolio selection under parameter uncertainty. _Quant. Financ._, 21(2):221-242, May 2020. doi: 10.1080/14697688.2020.1748214. URL https://doi.org/10.1080/14697688.2020.1748214.
* Bertsimas and Thiele [2005] D. Bertsimas and A. Thiele. A data-driven approach to newsvendor problems. _Working Paper, Massachusetts Institute of Technology_, 2005.
* Blei et al. [2017] D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians. _J. Am. Stat. Assoc._, 112(518):859-877, Feb 2017.
* Bodnar et al. [2017] T. Bodnar, S. Mazur, and Y. Okhrin. Bayesian estimation of the global minimum variance portfolio. _Eur. J. Oper. Res._, 256(1):292-307, Jan 2017. doi: 10.1016/j.ejor.2016.05.044. URL https://doi.org/10.1016/j.ejor.2016.05.044.
* Boucheron et al. [2013] S. Boucheron, G. Lugosi, and P. Massart. _Concentration inequalities: A nonasymptotic theory of independence_. Oxford university press, 2013.
* Chick [2006] S. E. Chick. Chapter 9 subjective probability and Bayesian methodology. In _Simulation_, pages 225-257. Elsevier, 2006. doi: 10.1016/s0927-0507(06)13009-1. URL https://doi.org/10.1016/s0927-0507(06)13009-1.
* Chu et al. [2008] L. Y. Chu, J. Shanthikumar, and Z.-J. M. Shen. Solving operational statistics via a Bayesian analysis. _Oper. Res. Lett._, 36(1):110-116, Jan 2008. doi: 10.1016/j.orl.2007.04.010. URL https://doi.org/10.1016/j.orl.2007.04.010.
* Diakonikolas et al. [2021] J. Diakonikolas, C. Daskalakis, and M. I. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In _International Conference on Artificial Intelligence and Statistics_, pages 2746-2754. PMLR, 2021.
* Donsker and Varadhan [1983] M. D. Donsker and S. S. Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. _Commun. Pur. Appl. Math._, 36(2):183-212, 1983.
* Follmer and Knispel [2011] H. Follmer and T. Knispel. Entropic risk measures: Coherence vs. convexity, model ambiguity and robust large deviations. _Stoch. Dynam._, 11(02n03):333-351, 2011.
* Ghosal et al. [2000] S. Ghosal, J. K. Ghosh, and A. W. van der Vaart. Convergence rates of posterior distributions. _Ann. Stat._, 28(2):500-531, 2000. ISSN 00905364. URL http://www.jstor.org/stable/2674039.
* Gibbs and Su [2002] A. L. Gibbs and F. E. Su. On choosing and bounding probability metrics. _Int. Stat. Rev._, 70(3):419-435, 2002.
* Gil et al. [2013] M. Gil, F. Alajaji, and T. Linder. Renyi divergence measures for commonly used univariate continuous distributions. _Inform. Sciences_, 249:124-131, 2013.
* Jaiswal et al. [2020] P. Jaiswal, V. Rao, and H. Honnappa. Asymptotic consistency of \(\alpha\)-renyi-approximate posteriors. _J Mach. Learn. Res._, 21(156):1-42, 2020.
* Kusmierczyk et al. [2019] T. Kusmierczyk, J. Sakaya, and A. Klami. Variational Bayesian decision-making for continuous utilities. In _Adv. Neur. In._, pages 6395-6405, 2019.
* Lacoste-Julien et al. [2011] S. Lacoste-Julien, F. Huszar, and Z. Ghahramani. Approximate inference for the loss-calibrated Bayesian. In _Int. Conf. Artif. Intell. Stat._, pages 416-424, 2011.

* Lai et al. [2011] T. L. Lai, H. Xing, and Z. Chen. Mean-variance portfolio optimization when means and covariances are unknown. _Ann. Appl. Stat._, 5(2A), Jun 2011. doi: 10.1214/10-aaas422. URL https://doi.org/10.1214/10-aaas422.
* Levi et al. [2015] R. Levi, G. Perakis, and J. Uichanco. The data-driven newsvendor problem: new bounds and insights. _Oper. Res._, 63(6):1294-1306, 2015.
* Li and Turner [2016] Y. Li and R. E. Turner. Renyi divergence variational inference. In _Adv. Neur. In._, pages 1073-1081, 2016.
* Lu et al. [2015] M. Lu, J. G. Shanthikumar, and Z.-J. M. Shen. Technical note-operational statistics: Properties and the risk-averse case. _Nav. Res. Log._, 62(3):206-214, 2015.
* Pati et al. [2018] D. Pati, A. Bhattacharya, and Y. Yang. On statistical optimality of variational Bayes. In A. Storkey and F. Perez-Cruz, editors, _Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics_, volume 84 of _Proc. of Mach. Learn. Res._, pages 1579-1588. PMLR, 09-11 Apr 2018. URL http://proceedings.mlr.press/v84/pati18a.html.
* Quang [2019] M. H. Quang. Regularized divergences between covariance operators and gaussian measures on hilbert spaces, 2019.
* Rockafellar [2007] R. T. Rockafellar. Coherent approaches to risk in optimization under uncertainty. In _OR Tools and Applications: Glimpses of Future Technologies_, pages 38-61. Informs, 2007.
* Scarf [1959] H. Scarf. Bayes solutions of the statistical inventory problem. _Ann. Math. Stat._, 30(2):490-508, 1959.
* Scarf [1960] H. E. Scarf. Some remarks on Bayes solutions to the inventory problem. _Nav. Res. Log._, 7(4):591-596, 1960.
* Schwartz [1965] L. Schwartz. On Bayes procedures. _Z. Wahrscheinlichkeit._, 4(1):10-26, Mar 1965. ISSN 1432-2064. doi: 10.1007/BF00535479. URL https://doi.org/10.1007/BF00535479.
* Stuart [2010] A. M. Stuart. Inverse problems: A Bayesian perspective. _Acta Numer._, 19:451-559, May 2010. doi: 10.1017/s0962492910000061. URL https://doi.org/10.1017/s0962492910000061.
* Turner and Sahani [2011] R. E. Turner and M. Sahani. _Two problems with variational expectation maximisation for time-series models_. Cambridge University Press, 2011.
* Van der Vaart [2000] A. W. Van der Vaart. _Asymptotic statistics_, volume 3. Cambridge university press, 2000.
* 1463, 2008. doi: 10.1214/00905360700000613. URL https://doi.org/10.1214/009053607000000613.
* Wang and Blei [2018] Y. Wang and D. M. Blei. Frequentist consistency of variational Bayes. _J. Am. Stat. Assoc._, pages 1-15, Jun 2018.
* Wu et al. [2018] D. Wu, H. Zhu, and E. Zhou. A Bayesian risk approach to data-driven stochastic optimization: Formulations and asymptotics. _SIAM J. Optimiz._, 28(2):1588-1612, 2018.
* Yao et al. [2018] Y. Yao, A. Vehtari, D. Simpson, and A. Gelman. Yes, but did it work?: Evaluating variational inference. In J. Dy and A. Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 5581-5590. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/yao18a.html.
* Zhang and Gao [2020] F. Zhang and C. Gao. Convergence rates of variational posterior distributions. _Ann. Stat._, 48(4), Aug 2020. doi: 10.1214/19-aos1883. URL https://doi.org/10.1214/19-aos1883.

Additional definitions

We provide the definitions of important terms used throughout the paper. First, recall the definition of covering numbers:

**Definition A.1** (Covering numbers).: _Let \(\mathcal{P}:=\{P_{\theta},\theta\in\Theta\}\) be a parametric family of distributions and \(d:\mathcal{P}\times\mathcal{P}\mapsto[0,\infty)\) be a metric. An \(\epsilon-\)cover of a subset \(\mathcal{P}_{K}:=\{P_{\theta}:\theta\in K\subset\Theta\}\) of the parametric family of distributions is a set \(K^{\prime}\subset K\) such that, for each \(\theta\in K\) there exists a \(\theta^{\prime}\in K^{\prime}\) that satisfies \(d(P_{\theta},P_{\theta^{\prime}})\leq\epsilon\). The \(\epsilon-\)covering number of \(\mathcal{P}_{K}\) is \(N(\epsilon,\mathcal{P}_{K},d)=\min\{card(K^{\prime}):K^{\prime}\text{ is an }\epsilon-\)cover of \(K\},\) where \(card(\cdot)\) represents the cardinality of the set._

Next, recall the definition of a test function [26]:

**Definition A.2** (Test function).: _Let \(\tilde{X}_{n}\) be a sequence of random variables on measurable space \((\bigotimes_{n}\mathcal{X},\mathcal{S}^{n})\). Then any \(\mathcal{S}^{n}\)-measurable sequence of functions \(\{\phi_{n}\},\ \phi_{n}:\tilde{X}_{n}\mapsto[0,1]\ \forall n\in\mathbb{N}\), is a test of a hypothesis that a probability measure on \(\mathcal{S}^{n}\) belongs to a given set against the hypothesis that it belongs to an alternative set. The test \(\phi_{n}\) is consistent for hypothesis \(P_{0}^{n}\) against the alternative \(P^{n}\in\{P_{\theta}^{n}:\theta\in\Theta\backslash\{\theta_{0}\}\}\) if \(\mathbb{E}_{P^{n}}[\phi_{n}]\rightarrow\mathbb{1}_{\{\theta\in\Theta\backslash \{\theta_{0}\}\}}(\theta),\forall\theta\in\Theta\) as \(n\rightarrow\infty\), where \(\mathbb{1}_{\{\cdot\}}\) is an indicator function._

A classic example of a test function is \(\phi_{n}^{\text{KS}}=\mathbb{1}_{\{\text{KS}_{n}>K_{r}\}}(\theta)\) that is constructed using the Kolmogorov-Smirnov statistic \(\text{KS}_{n}:=\sup_{t}|\mathbb{F}_{n}(t)-\mathbb{F}_{\theta}(t)|\), where \(\mathbb{F}_{n}(t)\) and \(\mathbb{F}_{\theta}(t)\) are the empirical and true distribution respectively, and \(K_{r}\) is the confidence level. If the null hypothesis is true, the Glivenko-Cantelli theorem [29, Theorem 19.1] shows that the KS statistic converges to zero as the number of samples increases to infinity.

Furthermore, we define the Hellinger distance \(h(\theta_{1},\theta_{2})\) between the two probability distributions \(P_{\theta_{1}}\) and \(P_{\theta_{2}}\) is defined as \(d_{H}(\theta_{1},\theta_{2})=\left(\int\left(\sqrt{dP_{\theta_{1}}}-\sqrt{dP_ {\theta_{2}}}\right)^{2}\right)^{1/2}.\) We define the one-sided Hausdorff distance \(H(A\|B)\) between sets \(A\) and \(B\) in a metric space \(D\) with distance function \(d\) is defined as:

\[H(A\|B)=\sup_{x\in A}d_{h}(x,B),\text{ where }d_{h}(x,B)=\inf_{y\in B}d(x,y).\]

Next, we define an arbitrary loss function \(L_{n}:\Theta\times\Theta\mapsto\mathbb{R}\) that measures the distance between models \((P_{\theta_{1}}^{n},P_{\theta_{2}}^{n})\forall\{\theta_{1},\theta_{2}\}\in\Theta\). At the outset, we assume that \(L_{n}(\theta_{1},\theta_{2})\) is always positive. We define \(\{\epsilon_{n}\}\) as a sequence such that \(\epsilon_{n}\to 0\) as \(n\rightarrow\infty\) and \(n\epsilon_{n}^{2}\geq 1\).

We also define

**Definition A.3** (\(\Gamma-\)convergence).: _A sequence of functions \(F_{n}:\mathcal{U}\mapsto\mathbb{R}\), for each \(n\in\mathbb{N}\), \(\Gamma-\)converges to \(F:\mathcal{U}\mapsto\mathbb{R}\), if_

* _for every_ \(u\in\mathcal{U}\) _and every_ \(\{u_{n},n\in\mathbb{N}\}\) _such that_ \(u_{n}\to u\)_,_ \(F(x)\leq\liminf_{n\rightarrow\infty}F_{n}(u_{n});\)__
* _for every_ \(u\in\mathcal{U}\) _, there exists some_ \(\{u_{n},n\in\mathbb{N}\}\) _such that_ \(u_{n}\to u\)_,_ \(F(x)\geq\limsup_{n\rightarrow\infty}F_{n}(u_{n}).\)__

In addition, we define

**Definition A.4** (Primal feasibility).: _For any two functions \(f:\mathcal{U}\mapsto\mathbb{R}\) and \(b:\mathcal{U}\mapsto\mathbb{R}\), a point \(u^{*}\in\mathcal{U}\) is primal feasible to the following constraint optimization problem_

\[\inf_{u\in\mathcal{U}}f(u)\text{ subject to }b(u)\leq c,\]

_if \(b(u^{*})\leq c\), for a given \(c\in\mathbb{R}\)._

## Appendix B Applications

### Single product newsvendor problem (cont.)

First, we fix the sieve set \(\Theta_{n}(\epsilon)=\Theta\), which clearly implies that the restricted inverse-gamma prior \(\Pi(\theta)\), places no mass on the complement of this set and therefore satisfies Assumption 2.2.

Second, under the condition that the true demand distribution is exponential with parameter \(\theta_{0}\) (and \(P_{0}\equiv P_{\theta_{0}}\)), we demonstrate the existence of test functions satisfying Assumption 2.1.

**Lemma B.1**.: _Fix \(n\geq 5\). Then, for any \(\epsilon>\epsilon_{n}:=\frac{1}{\sqrt{n}}\) with \(\epsilon_{n}\to 0\), and \(n\epsilon_{n}^{2}\geq 1\), there exists a test function \(\phi_{n}\) (depending on \(\epsilon\)) such that \(L_{n}^{NV}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)- R(a,\theta_{0})\right|\right)^{2}\) satisfies Assumption 2.1 with \(C_{0}=20\) and \(C=\frac{1}{2}(K_{1}^{NV})^{-2}\) for a constant \(C_{1}>0\) and \(K_{1}^{NV}=d_{H}(T,\theta_{0})^{-1}\left[\left(\frac{h}{\theta_{0}}-\frac{h}{T }\right)^{2}+(b+h)^{2}\left(\frac{e^{-\pi T}}{T}-\frac{e^{-\pi\theta_{0}}}{ \theta_{0}}\right)^{2}\right]^{1/2}\)._

The proof of the above result follows by showing that \(d_{L}^{NV}=n^{-1/2}\sqrt{L_{n}^{NV}(\theta,\theta_{0})}\) can be bounded above by the Hellinger distance between two exponential distributions on \(\Theta\) (under which a test function exists) in Lemma C.10 in the appendix.

Third, we show that there exist appropriate constants such that the inverse-gamma prior satisfies Assumption 2.3 when the demand distribution is exponential.

**Lemma B.2**.: _Fix \(n_{2}\geq 2\) and any \(\lambda>1\). Let \(A_{n}:=\left\{\theta\in\Theta:D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n} \right)\leq C_{3}n\epsilon_{n}^{2}\right\}\), where \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\) is the Renyi divergence between \(P_{0}^{n}\) and \(P_{0}^{n}\). Then for \(\epsilon_{n}^{2}=\frac{\log n}{n}\) and any \(C_{3}>0\) such \(C_{2}=\alpha C_{3}\geq 2\), the truncated inverse-gamma prior Inv \(-\Gamma_{\Theta}(A;\alpha,\beta)\) satisfies \(\Pi(A_{n})\geq\exp(-nC_{2}\epsilon_{n}^{2}),\forall n\geq n_{2}\)._

Fourth, it is straightforward to see that the newsvendor model risk \(R(a,\theta)\) is bounded below for a given \(a\in\mathcal{A}\).

**Lemma B.3**.: _For any \(a\in\mathcal{A}\) and positive constants \(h\) and \(b\), the newsvendor model risk \(R(a,\theta)=\left(ha-\frac{h}{\theta}+(b+h)\frac{e^{-a\theta}}{\theta}\right) \geq\left(\frac{ha^{2}\theta^{*}}{(1+a\theta^{*})}\right),\) where \(\underline{a}:=\min\{a\in\mathcal{A}\}\) and \(\theta^{*}\) satisfies \(h-(b+h)e^{-a\theta^{*}}(1+a\theta^{*})=0\)._

This implies that \(R(a,\theta)\) satisfies Assumption 2.5. Finally, we also show that the newsvendor model risk satisfies Assumption 2.4.

**Lemma B.4**.: _Fix \(n\geq 1\) and \(\gamma>0\). For any \(\epsilon>\epsilon_{n}\) and any \(a\in\mathcal{A}\), \(R(a,\theta)\) satisfies \(\mathbb{E}_{\Pi}[\mathbbm{1}_{\{R(a,\theta)\gamma>C_{4}(\gamma)n\epsilon^{2} \}}e^{\gamma R(a,\theta)}]\leq\exp(-C_{5}(\gamma)n\epsilon^{2}),\) for any \(C_{4}(\gamma)>2\gamma\left(h\bar{a}+\frac{b}{T}\right)\) and \(C_{5}(\gamma)=C_{4}(\gamma)-2\gamma\left(h\bar{a}+\frac{b}{T}\right)\), where \(\bar{a}:=\max\{a\in\mathcal{A}\}\)._

Note that Lemma B.1 implies that \(C=\frac{C_{1}}{2(K_{1}^{NV})^{2}}\) for any constant \(C_{1}>0\). Fixing \(\alpha=1\) and using Lemma B.2 we can choose \(C_{2}=C_{3}=2\). Now, \(C_{1}\) can be chosen large enough such that \(C>C_{4}(\gamma)+C_{5}(\gamma)\) for a given risk sensitivity \(\gamma>0\). Therefore, the condition on constants in Theorem 3.1 reduces to \(C_{5}(\gamma)>2+C_{2}+C_{3}=5\), and it can be satisfied easily by fixing \(C_{5}(\gamma)=5.1\)(say).

These lemmas show that when the demand distribution is exponential and with a non-conjugate truncated inverse-gamma prior, our result in Theorem 3.2 can be used for RSVB method to bound the optimality gap in decisions and values for various values of the risk-sensitivity parameter \(\gamma\). Recall that the bound obtained in Theorem 3.2 depends on \(\epsilon_{n}^{2}\) and \(\eta_{n}^{R}(\gamma)\).

Lemma B.2 implies that \(\epsilon_{n}^{2}=\frac{\log n}{n}\), but in order to get the complete bound we further need to characterize \(\eta_{n}^{R}(\gamma)\). Recall that, as a consequence of Assumption 3.1 in Proposition 3.1, for a given \(C_{8}=-\inf_{Q\in\mathcal{Q}}\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\) that \(C_{9}>0\) and \(\eta_{n}^{R}(\gamma)\leq\gamma n^{-1}C_{8}+C_{9}\epsilon_{n}^{\prime 2}\).

Therefore, in our next result, we show that in the newsvendor setting, we can construct a sequence \(\{Q_{n}(\theta)\}\subset\mathcal{Q}\) that satisfies Assumption 3.1, and thus identify \(\epsilon_{n}^{\prime}\) and the constant \(C_{9}\). We fix \(\mathcal{Q}\) to be the family of shifted gamma distributions with support \([T,\infty)\).

**Lemma B.5**.: _Let \(\{Q_{n}(\theta)\}\) be a sequence of shifted gamma distributions with shape parameter \(a=n\) and rate parameter \(b=\frac{n}{\theta_{0}}\), then for truncated inverse gamma prior and exponentially distributed likelihood model_

\[\frac{1}{n}\left[\text{KL}\left(Q_{n}(\theta)\|\Pi(\theta)\right)+\mathbb{E}_{Q _{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}(\tilde{X}_{n}))\|dP_{\theta}^{n} (\tilde{X}_{n})\right)\right]\right]\leq C_{9}\epsilon_{n}^{\prime 2},\]

_where \(\epsilon_{n}^{\prime 2}=\frac{\log n}{n}\) and \(C_{9}=\frac{1}{2}+\max\left(0,2+\frac{2\beta}{\theta_{0}}-\log\sqrt{2\pi}-\log \left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\alpha\log\theta_{0}\right)\) and prior parameters are chosen such that \(C_{9}>0\)._

### Multi-product newsvendor problem

Analogous to the one-dimensional newsvendor loss function, the loss function in its multi-product version is defined as

\[\ell(a,\xi):=h^{T}(a-\xi)^{+}+b^{T}(\xi-a)^{+}\]

where \(h\) and \(b\) are given vectors of underage and overage costs respectively for each product and mapping \((\cdot)^{+}\) is defined component-wise. We assume that there are \(d\) items or products and \(\xi\in\mathbb{R}^{d}\) denotes the random vector of demands. Let \(a\in\mathcal{A}\subset\mathbb{R}_{+}^{d}\) be the inventory or decision variable, typically assumed to take values in a compact decision space \(\mathcal{A}\) with \(\underline{a}:=\{\{\min\{a_{i}:a_{i}\in\mathcal{A}_{i}\}\}_{i=1}^{d}\) and \(\bar{a}:=\{\{\max\{a_{i}:a_{i}\in\mathcal{A}_{i}\}\}_{i=1}^{d}\) and \(\underline{a}>0\), where \(\mathcal{A}_{i}\) is the marginal set of \(i^{th}\) component of \(\mathcal{A}\). The random demand is assumed to be multivariate Gaussian, with unknown mean parameter \(\theta\in\mathbb{R}^{d}\) but with known covariance matrix \(\Sigma\). We also assume that \(\Sigma\) is a symmetric positive definite matrix and can be decomposed as \(Q^{T}\Lambda Q\), where \(Q\) is an orthogonal matrix and \(\Lambda\) is a diagonal matrix consisting of respective eigenvalues of \(\Sigma\). We also define \(\overline{\Lambda}=\max_{i\in\{1,2,...d\}}\Lambda_{ii}\) and \(\underline{\Lambda}=\min_{i\in\{1,2,...d\}}\Lambda_{ii}\). The model risk

\[R(a,\theta) =\mathbb{E}_{P_{\theta}}[\ell(a,\xi)]=\sum_{i=1}^{d}\mathbb{E}_{ P_{\theta_{i}}}[h_{i}(a_{i}-\xi_{i})^{+}+b_{i}(\xi_{i}-a_{i})^{+}]\] \[=\sum_{i=1}^{d}\Bigg{[}(h_{i}+b_{i})a_{i}\Phi\left(\frac{(a_{i}- \theta_{i})}{\sigma_{ii}}\right)-b_{i}a_{i}+\theta_{i}(b_{i}-h_{i})\] \[\qquad+\sigma_{ii}\left[h\frac{\phi\left(\frac{(a_{i}-\theta_{i} )}{\sigma_{ii}}\right)}{\Phi\left(\frac{(a_{i}-\theta_{i})}{\sigma_{ii}} \right)}+b\frac{\phi\left(\frac{(a_{i}-\theta_{i})}{\sigma_{ii}}\right)}{1- \Phi\left(\frac{(a_{i}-\theta_{i})}{\sigma_{ii}}\right)}\right]\Bigg{]},\] (11)

which is convex in \(a\). Here \(P_{\theta_{i}}\) is the marginal distribution of \(\xi\) for \(i^{th}\) product, \(\phi(\cdot)\) and \(\Phi(\cdot)\) are probability and cumulative distribution function of the standard Normal distribution. We also assume that the true mean parameter \(\theta_{0}\) lies in a compact subspace \(\Theta\subset\mathbb{R}^{d}\). We fix the prior to be uniformly distributed on \(\Theta\) with no correlation across its components, that is \(\pi(A)=\frac{m(A\bigcap\Theta)}{m(\Theta)}=\prod_{i=1}^{d}\frac{m(A_{i}\bigcap \Theta_{i})}{m(\Theta_{i})}\), where \(m(B)\) is the Lebesgue measure (or volume) of \(B\subset\mathbb{R}^{d}\) As in the previous example, we fix the sieve set \(\Theta_{n}(\epsilon)=\Theta\), which clearly implies that \(\Pi(\theta)\) places no mass on the complement of this set and therefore satisfies Assumption 2.2.

Then under the condition that the true demand distribution has a multivariate Gaussian distribution (with known \(\Sigma\)) and mean \(\theta_{0}\) (\(P_{0}\equiv P_{\theta_{0}}\)), we demonstrate the existence of test functions satisfying Assumption 2.1 by constructing a test function unlike the single-product newsvendor problem with exponential demand.

**Lemma B.6**.: _Fix \(n\geq 1\). Then, for any \(\epsilon>\epsilon_{n}:=\frac{1}{\sqrt{n}}\) with \(\epsilon_{n}\to 0\), and \(n\epsilon_{n}^{2}\geq 1\) and test function \(\phi_{n,\epsilon}:=\mathbbm{1}_{\left\{\bar{X}_{n}\|\hat{\theta}_{n}-\theta_{0 }\|>\sqrt{C\epsilon^{2}}\right\}}\), \(L_{n}^{MNV}(\theta,\theta_{0})=n\)\((\sup_{a\in\mathcal{A}}|R(a,\theta)-R(a,\theta_{0})|)^{2}\) satisfies Assumption 2.1 with \(C_{0}=1\), \(C_{1}=4K^{2}C\) and \(C=1/8\left(\frac{\bar{C}}{d\Lambda}-1\right)\) for sufficiently large \(\tilde{C}\) such that \(C>1\) and \(\overline{\Lambda}=\max_{i\in\{1,2,...d\}}\Lambda_{ii}\), where \(K=\sup_{\mathcal{A},\Theta}\|\partial_{\theta}R(a,\theta)\|\)._

In the following result, we show that there exist appropriate constants such that prior distribution satisfies Assumption 2.3 when the demand distribution is a multivariate Gaussian with unknown mean.

**Lemma B.7**.: _Fix \(n_{2}\geq 2\) and any \(\lambda>1\). Let \(A_{n}:=\left\{\theta\in\Theta:D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n} \right)\leq C_{3}n\epsilon_{n}^{2}\right\}\), where \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\) is the Renyi Divergence between \(P_{0}^{n}\) and \(P_{\theta}^{n}\). Then for \(\epsilon_{n}^{2}=\frac{\log n}{n}\) and any \(C_{3}>0\) such that \(C_{2}=\frac{4d}{\overline{\Lambda}(\lambda+1)\left(\prod_{i=1}^{d}m\left(\Theta _{i}\right)\right)^{2/d}}C_{3}\geq 2\) and for large enough \(n\), the uncorrelated uniform prior restricted to \(\Theta\) satisfies \(\Pi(A_{n})\geq\exp(-nC_{2}\epsilon_{n}^{2})\)._

Next, it is straightforward to see that the multi-product newsvendor model risk \(R(a,\theta)\) is bounded below for a given \(a\in\mathcal{A}\) on a compact set \(\Theta\) and thus it satisfies Assumption 2.5. Finally, we also show that the newsvendor model risk satisfies Assumption 2.4.

**Lemma B.8**.: _Fix \(n\geq 1\) and \(\gamma>0\). For any \(\epsilon>\epsilon_{n}\) and \(a\in\mathcal{A}\), \(R(a,\theta)\) satisfies \(\mathbb{E}_{\Pi}[\mathbbm{1}_{\{G(a,\theta)>C_{4}(\gamma)n\epsilon^{2}\}}e^{ \gamma G(a,\theta)}]\leq\exp(-C_{5}(\gamma)n\epsilon_{n}^{2}),\) for any \(C_{4}(\gamma)>2\gamma\sup_{\{a,\theta\}\in\mathcal{A}\otimes\Theta}G(a,\theta)\) and \(C_{5}(\gamma)=C_{4}(\gamma)-2\gamma\sup_{\{a,\theta\}\in\mathcal{A}\otimes \Theta}G(a,\theta).\)_

Similar to single product example, in our next result, we show that in the multi-product newsvendor setting, we can construct a sequence \(\{Q_{n}(\theta)\}\in\mathcal{Q}\) that satisfies Assumption 3.1, and thus identify \(\epsilon_{n}^{\prime}\) and constant \(C_{9}\). We fix \(\mathcal{Q}\) to be the family of uncorrelated Gaussian distributions restricted to \(\Theta\).

**Lemma B.9**.: _Let \(\{Q_{n}(\theta)\}\) be a sequence of product \(d\) univariate Gaussian distribution defined as \(q_{n}^{i}(\theta)\propto\frac{1}{\sqrt{2\pi\sigma_{n}^{2}}}e^{-\frac{1}{2 \sigma_{n}^{2}}(\theta-\mu_{i,n})^{2}}\mathbbm{1}_{\Theta_{i}}=\frac{\mathcal{ N}(\theta_{i}|\mu_{i,n},\sigma_{i,n})\mathbbm{1}_{\Theta_{i}}}{\mathcal{N}( \Theta_{i}|\mu_{i,n},\sigma_{i,n})}\) and fix \(\sigma_{i,n}=1/\sqrt{n}\) and \(\theta_{i}=\theta_{0}^{i}\) for all \(i\in\{1,2,\ldots,d\}\). Then for uncorrelated uniform distribution restricted to \(\Theta\) and multivariate normal likelihood model \(\frac{1}{n}\left[\text{KL}\left(Q_{n}(\theta)\|\Pi(\theta)\right)+\mathbb{E}_{ Q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}(\tilde{X}_{n})\right)\|dP_{ \theta}^{n}(\tilde{X}_{n})\right)\right]\right]\leq C_{9}\epsilon_{n}^{\prime 2},\) where \(\epsilon_{n}^{\prime 2}=\frac{\log n}{n}\) and \(C_{9}:=\frac{d}{2}+\max\left(0,-\sum_{i=1}^{d}[\log(\sqrt{2\pi e})-\log(m( \Theta_{i}))]+\frac{d}{2}\Delta^{-1}\right)\)._

Now, using the result established in lemmas above, we bound the optimality gap in values for the multi-product newsvendor model risk.

**Theorem B.1**.: _Fix \(\gamma>0\). Suppose that the set \(\mathcal{A}\) is compact. Then, for the multi-product newsvendor model with multivariate Gaussian distributed demand with known covariance matrix \(\Sigma\) and unknown mean vector \(\theta\) lying in a compact subset \(\Theta\subset\mathbb{R}^{d}\), prior \(\Pi(\cdot)=\prod_{i=1}^{d}\frac{m(\{\cdot\}\cap\Theta_{i})}{m(\Theta_{i})}\), and the variational family fixed to uncorrelated Gaussian distribution restricted to \(\Theta\), and for any \(\tau>0\), the \(P_{0}^{n}-probability\) of the following event \(\left\{\tilde{X}_{n}:R(\mathbf{a}_{\text{RS}}^{*},\theta_{0})-\inf_{z\in \mathcal{A}}R(z,\theta_{0})\leq 2\tau M^{\prime}(\gamma)\left(\frac{\log n}{n} \right)^{1/2}\right\}\) is at least \(1-\tau^{-1}\) for sufficiently large \(n\) and for some mapping \(M^{\prime}:\mathbb{R}^{+}\rightarrow\mathbb{R}^{+}\), where \(R(\cdot,\theta)\) is the multi-product newsvendor model risk._

Proof.: The proof is a direct consequence of Theorem 3.2, Lemmas B.6, B.7, B.8, B.9, and Proposition 3.2. 

### Gaussian process classification (cont.)

We define the distance function as \(L_{n}^{GP}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)-R (a,\theta_{0})\right|\right)^{2}\). In anticipation of demonstrating that the binary classification model with GP prior and distance function \(L_{n}^{GP}\) satisfy the desired set of assumptions, we recall the following result, from [30], which will be central in establishing Assumptions 2.1, 2.2, and 2.3.

**Lemma B.10**.: _[Theorem 2.1 [30]] Let \(\theta(\cdot)\) be a Borel measurable, zero-mean Gaussian random element in a separable Banach space \((\Theta,\|\cdot\|)\) with reproducing kernel Hilbert space (RKHS) \((\mathbb{H},\|\cdot\|_{\mathbb{H}})\) and let \(\theta_{0}\) be contained in the closure of \(\mathbb{H}\) in \(\Theta\). For any \(\epsilon>\epsilon_{n}\) satisfying \(\varphi_{\theta_{0}}(\epsilon)\leq n\epsilon^{2}\), where_

\[\varphi_{\theta_{0}}(\epsilon)=\inf_{h\in\mathbb{H}:\|h-\theta_{0}\|<\epsilon }\|h\|_{\mathbb{H}}^{2}-\log\Pi(\|\theta\|<\epsilon)\] (12)

_and any \(C_{10}>1\) with \(e^{-C_{10}n\epsilon_{n}^{2}}<1/2\), there exists a measurable set \(\Theta_{n}(\epsilon)\subset\Theta\) such that_

\[\log N(3\epsilon,\Theta_{n}(\epsilon),\|\cdot\|) \leq 6C_{10}n\epsilon^{2},\] (13) \[\Pi(\theta\notin\Theta_{n}(\epsilon)) \leq e^{-C_{10}n\epsilon^{2}},\] (14) \[\Pi(\|\theta-\theta_{0}\|<4\epsilon_{n}) \geq e^{-n\epsilon_{n}^{2}}.\] (15)

The proof of the lemma above can be easily adapted from the proof of [30, Theorem 2.1], which is specifically for \(\epsilon=\epsilon_{n}\). Notice that the result above is true for any norm \(\|\cdot\|\) on the Banach space if that satisfies \(\varphi_{\theta_{0}}(\epsilon)\leq n\epsilon^{2}\). Moreover, if \(\varphi_{\theta_{0}}(\epsilon_{n})\leq n\epsilon_{n}^{2}\) is true, then it also holds for any \(\epsilon>\epsilon_{n}\), since by definition \(\varphi_{\theta_{0}}(\epsilon)\) is a decreasing function of \(\epsilon\).

All the results in the previous lemma depend on \(\varphi_{\theta_{0}}(\epsilon)\) being less than \(n\epsilon^{2}\). In particular, observe that the second term in the definition of \(\varphi_{\theta_{0}}(\epsilon)\) depends on the prior distribution on \(\Theta\). Therefore,[30, Theorem 4.5] show that \(\varphi_{\theta_{0}}(\epsilon_{n})\leq n\epsilon_{n}^{2}\) ( with \(\|\cdot\|\) as supremum norm and for \(\epsilon_{n}\) as defined later in (9) ) is satisfied by the Gaussian prior of type

\[W(\cdot)=\sum_{j=1}^{\bar{J}_{\alpha}}\sum_{k=1}^{2^{jd}}\mu_{j}Z_{j,k}\vartheta _{j,k}(\cdot),\] (16)

where \(\{\mu_{j}\}\) is a sequence that decreases with \(j\), \(\{Z_{i,j}\}\) are i.i.d. standard Gaussian random variables and \(\{\vartheta_{j,k}\}\) form a double-indexed orthonormal basis (with respect to measure \(\nu\)), that is \(\mathbb{E}_{\nu}[\vartheta_{j,k}\vartheta_{l,m}]=\mathbbm{1}_{\{j=l,k=m\}}\)). \(\bar{J}_{\alpha}\) is the smallest integer satisfying \(2^{J_{\alpha}d}=n^{d/(2\alpha+d)}\) for a given \(\alpha>0\). In particular, the GP above is constructed using the function class that is supported on \([0,1]^{d}\) and has a wavelet expansion, \(w(\cdot)=\sum_{j=1}^{\infty}\sum_{k=1}^{2^{jd}}w_{j,k}\vartheta_{j,k}(\cdot)\). The wavelet function space is equipped with the \(L_{2}-\)norm: \(\|w\|_{2}=\sum_{j=1}^{\infty}\left(\sum_{k=1}^{2^{jd}}|w_{j,k}|^{2}\right)^{1/2}\); the supremum norm: \(\|w\|_{\infty}=\sum_{j=1}^{\infty}2^{jd}\max_{1\leq k\leq 2^{jd}}|w_{j,k}|\); and the Besov \((\beta,\infty,\infty)-\)norm: \(\|w\|_{\beta;\infty,\infty}=\sup_{1\leq j<\infty}2^{j\beta}2^{jd}\)\(\max_{1\leq k\leq 2^{jd}}|w_{j,k}|\). Note that \(W\) induces a measure over the RKHS \(\mathbb{H}\), defined as a collection of truncated wavelet functions \(w(\cdot)=\sum_{j=1}^{\bar{J}_{\alpha}}\sum_{k=1}^{2^{jd}}w_{j,k}\vartheta_{j, k}(\cdot),\) with norm induced by the inner-product on \(\mathbb{H}\) as \(\|w\|_{\mathbb{H}}^{2}=\sum_{j=1}^{J_{\alpha}}\sum_{k=1}^{2^{jd}}\frac{w_{j,k} ^{2d}}{\mu_{j}^{2}}\). The RKHS kernel \(K:[0,1]^{d}\times[0,1]^{d}\mapsto\mathbb{R}\) can be easily derived as

\[K(x,y)=\mathbb{E}[W(x)W(y)] =\mathbb{E}\left[\left(\sum_{j=1}^{\bar{J}_{\alpha}}\sum_{k=1}^{ 2^{jd}}\mu_{j}Z_{j,k}\vartheta_{j,k}(y)\right)\left(\sum_{j=1}^{J_{\alpha}} \sum_{k=1}^{2^{jd}}\mu_{j}Z_{j,k}\vartheta_{j,k}(x)\right)\right]\] \[=\sum_{j=1}^{\bar{J}_{\alpha}}\sum_{k=1}^{2^{jd}}\mu_{j}^{2} \vartheta_{j,k}(y)\vartheta_{j,k}(x).\]

Indeed, by the definition of this kernel and inner product, observe that \(\langle K(x,\cdot),w(\cdot)\rangle=\sum_{j=1}^{\bar{J}_{\alpha}}\sum_{k=1}^{2 ^{jd}}w_{j,k}\mu_{j}^{2}\vartheta_{j,k}(x)\frac{1}{\mu_{j}^{2}}=w(x).\) Moreover, \(\langle K(x,\cdot),K(y,\cdot)\rangle=\sum_{j=1}^{J_{\alpha}}\sum_{k=1}^{2^{jd }}\mu_{j}^{2}\vartheta_{j,k}(x)\mu_{j}^{2}\vartheta_{j,k}(y)\frac{1}{\mu_{j}^{ 2}}=K(x,y).\) It is clear from its definition that \(W\) is a centered Gaussian random field on the RKHS.

Next, using the definition of the kernel, we derive the covariance operator of the Gaussian random field \(W\). Recall that \(Y\sim\nu\), which enables us to define the covariance operator \(\mathcal{C}\), following [27, (6.19)] as \((\mathcal{C}h_{\nu})(x)=\int_{[0,1]^{d}}K(x,y)h_{\nu}(y)d\nu(y).\) Also, observe that \(\{\mu_{j}^{2},\varphi_{j,k}\}\) is the eigenvalue and eigen function pair of the covariance operator \(\mathcal{C}\). Consequently, using Karhunen Loeve expansion [27, Theorem 6.19] the prior induced by \(W\) on \(\mathbb{H}\) is a Gaussian distribution denoted as \(\mathcal{N}(0,\mathcal{C})\). We also recall the Cameron-Martin space denoted as \(\text{Im}(\mathcal{C}^{1/2})\) associated with a Gaussian measure \(\mathcal{N}(0,\mathcal{C})\) on \(\mathbb{H}\) to be the intersection of all linear spaces of full measure under \(\mathcal{N}(0,\mathcal{C})\)[27, page 530]. In particular, \(\text{Im}(\mathcal{C}^{1/2})\) is the Hilbert space with inner product \(\langle\cdot,\cdot\rangle_{\mathcal{C}}=\langle\mathcal{C}^{-1/2},\mathcal{C} ^{-1/2}\cdot\rangle\).

Next, we show the existence of test functions in the following result.

**Lemma B.11**.: _For any \(\epsilon>\epsilon_{n}\) with \(\epsilon_{n}\to 0\), \(n\epsilon_{n}^{2}\geq 2\log 2\), and \(\varphi_{\theta_{0}}(\epsilon)\leq n\epsilon^{2}\), there exists a test function \(\phi_{n}\) (depending on \(\epsilon\)) such that \(L_{n}^{GP}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)-R (a,\theta_{0})\right|\right)^{2}\) satisfies Assumption 2.1 with \(C=1/6\), \(C_{0}=2\) and \(C_{1}=\left(\max(c_{+},c_{-})\right)^{2}\)._

Assumption 2.2 is a direct consequence of (14) in Lemma B.10. Next, we prove that prior distribution and the likelihood model satisfy Assumption 2.3 using (15) of Lemma B.10.

**Lemma B.12**.: _For any \(\lambda>1\), let \(A_{n}:=\left\{\theta\in\Theta:D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n} \right)\leq C_{3}n\epsilon_{n}^{2}\right\}\), where \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\) is the Renyi Divergence between \(P_{0}^{n}\) and \(P_{\theta}^{n}\). Then for any \(\epsilon>\epsilon_{n}\) satisfying \(\varphi_{\theta_{0}}(\epsilon)\leq n\epsilon^{2}\) and \(C_{3}=16(\lambda+1)\) and \(C_{2}=1\), the GP prior satisfies \(\Pi(A_{n})\geq\exp(-nC_{2}\epsilon_{n}^{2})\)._

Assumption 2.4 and 2.5 are straightforward to satisfy since the model risk function \(R(a,\theta)\) is bounded from above and below.

Now, suppose the variational family \(\mathcal{Q}_{GP}\) is a class of Gaussian distributions on \(\Theta\), defined as \(\mathcal{N}(m_{q},\mathcal{C}_{q})\), \(m_{q}\) belongs to \(\Theta\) and \(\mathcal{C}_{q}\) is the covariance operator defined as \(\mathcal{C}_{q}=\mathcal{C}^{1/2}(I-S)\mathcal{C}^{1/2}\)for any \(S\) which is a symmetric and Hilbert-Schmidt (HS) operator on \(\Theta\) (eigenvalues of HS operator are square summable). Note that \(S\) and \(m_{q}\) span the distributions in \(\mathcal{Q}_{GP}\).

The following lemma verifies Assumption 3.1, for a specific sequence of distributions in \(\mathcal{Q}\).

**Lemma B.13**.: _For a given \(J\in\mathbb{N}\), let \(\{Q_{n}\}\) be a sequence variational distribution such that \(Q_{n}\) is the measure induced by a GP, \(W_{Q}(\cdot)=\theta_{0}^{J}(y)+\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\zeta_{j}^{2}Z _{j,k}\vartheta_{j,k}(\cdot)\), where \(\theta_{0}^{J}(\cdot)=\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\theta_{0;j,k}\vartheta _{j,k}(\cdot)\) and \(\zeta_{j}^{2}=\frac{\mu_{j}^{2}}{1+n\epsilon_{n}^{2}\tau_{j}^{2}}\). Then for GP prior induced by \(W=\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\mu_{j}Z_{j,k}\vartheta_{j,k}\) and \(\mu_{j}=2^{-jd/2-ja}\) for some \(a>0\), \(\|\theta_{0}\|_{\beta;\infty,\infty}<\infty\), and \(\theta_{0}^{J}(y)\) lie in the Cameron-Martin space \(\text{Im}(\mathcal{C}^{1/2})\), we have \(\frac{1}{n}\text{KL}(\mathcal{N}(\bar{\theta}_{0}^{J},\mathcal{C}_{q})\| \mathcal{N}(0,\mathcal{C}))+\frac{1}{n}\mathbb{E}_{Q_{n}}\text{KL}(P_{0}^{n} \|P_{\theta}^{n})\leq C_{9}\epsilon_{n}^{2}\), where \(\epsilon_{n}\) is defined in 9 and \(C_{9}:=\max\left(\|\theta_{0}\|_{\beta,\infty,\infty}^{2-a}-2^{-2J_{2}-a_{2}} \over 1-2^{-2a},2^{d}/(2^{d}-1),C^{\prime}\right)\), where \(C^{\prime}\) is a positive constant satisfying \(\|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\leq C^{\prime}2^{-2J\beta}\)._

Using the result above together with Proposition 3.2 implies that the RSVB posterior converges at the same rate as the true posterior, where the convergence rate of the true posterior is derived in [30, Theorem 4.5] for the binary GP classification problem with truncated wavelet GP prior. Finally, we use the results above to obtain bound on the optimality gap in values of the binary GP classification problem.

## Appendix C Proofs

### Alternative derivation of LCVB

We present the alternative derivation of LCVB. Consider the logarithm of the Bayes posterior risk,

\[\log\mathbb{E}_{\Pi(\theta|\tilde{X}_{n})}[\exp(R(a,\theta))] =\log\int_{\Theta}\exp(R(a,\theta))d\Pi(\theta|\tilde{X}_{n})\] \[=\log\int_{\Theta}\frac{dQ(\theta)}{dQ(\theta)}\exp(R(a,\theta))d \Pi(\theta|\tilde{X}_{n})\] \[\geq-\int_{\Theta}dQ(\theta)\log\frac{dQ(\theta)}{\exp(R(a,\theta ))d\Pi(\theta|\tilde{X}_{n})}=:\mathcal{F}(a;Q(\cdot),\tilde{X}_{n})\] (17)

where the inequality follows from an application of Jensen's inequality (since, without loss of generality, \(\exp(R(a,\theta))>0\) for all \(a\in\mathcal{A}\) and \(\theta\in\Theta\)), and \(Q\in\mathcal{Q}\). Then, it follows that

\[\min_{a\in\mathcal{A}}\log\mathbb{E}_{\Pi(\theta|\tilde{X}_{n})}[ \exp(R(a,\theta))] \geq\min_{a\in\mathcal{A}}\max_{q\in\mathcal{Q}}\mathcal{F}(a;Q( \theta),\tilde{X}_{n})\] \[=\min_{a\in\mathcal{A}}\max_{q\in\mathcal{Q}}-\text{KL}\left(Q( \theta)||\Pi(\theta|\tilde{X}_{n})\right)+\int_{\Theta}R(a,\theta)dQ(\theta).\] (18)

### Proof of Theorem 3.1

We prove our main result after a series of important lemmas. For brevity we denote \(\mathcal{LR}_{n}(\theta,\theta_{0})=\frac{p(\tilde{X}_{n}|\theta)}{p(X_{n}| \theta_{0})}\).

**Lemma C.1**.: _For any \(a^{\prime}\in\mathcal{A}\), \(\gamma>0\), and \(\zeta>0\),_

\[\mathbb{E}_{P_{0}^{n}}\left[\zeta\int_{\Theta}L_{n}(\theta,\theta_ {0})\ dQ_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\right]\] \[\leq \log\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\zeta L_{n}( \theta,\theta_{0})}\frac{e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]+\inf_{Q\in \mathcal{Q}}\mathbb{E}_{P_{0}^{n}}\bigg{[}\text{KL}(Q(\theta)||\Pi(\theta| \tilde{X}_{n}))\] \[-\gamma\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\bigg{]} +\log\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\gamma R(a^{\prime},\theta)} \ \frac{\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{ n}(\theta,\theta_{0})d\Pi(\theta)}\right].\] (19)Proof.: For any fixed \(a^{\prime}\in\mathcal{A},\gamma>0\), and \(\zeta>0\), and using the fact that KL is non-negative, observe that the integral in the LHS of equation (19) satisfies,

\[\zeta\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})} \left[L_{n}(\theta,\theta_{0})\right] \leq\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})} \left[\log e^{\zeta L_{n}(\theta,\theta_{0})}\right]\] \[\quad+\text{KL}\left(dQ^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_ {n})\right]\frac{e^{\zeta L_{n}(\theta,\theta_{0})}e^{\gamma R(a^{\prime}, \theta)}\ d\Pi(\theta|\tilde{X}_{n})}{\int_{\Theta}e^{\zeta L_{n}(\theta, \theta_{0})}e^{\gamma R(a^{\prime},\theta)}\ d\Pi(\theta|\tilde{X}_{n})}\right)\] \[=\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})} \left[\log e^{\zeta L_{n}(\theta,\theta_{0})}\right]\ +\log\mathbb{E}_{\Pi_{n}} \left[e^{\zeta L_{n}(\theta,\theta_{0})}e^{\gamma R(a^{\prime},\theta)}\right]\] \[\quad+\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n} )}\left[\log\frac{dQ^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})}{e^{\zeta L _{n}(\theta,\theta_{0})}e^{\gamma R(a^{\prime},\theta)}\ d\Pi(\theta|\tilde{X}_ {n})}\right]\] \[=\log\mathbb{E}_{\Pi_{n}}\left[e^{\zeta L_{n}(\theta,\theta_{0})} e^{\gamma R(a^{\prime},\theta)}\right]+\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}( \theta|\tilde{X}_{n})}\left[\log\frac{dQ^{*}_{a^{\prime},\gamma}(\theta|\tilde{ X}_{n})}{e^{\gamma R(a^{\prime},\theta)}\ d\Pi(\theta|\tilde{X}_{n})}\right].\]

Next, using the definition of \(Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})\) in the second term of last equality, for any other \(Q(\cdot)\in\mathcal{Q}\)

\[\zeta\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})} \left[L_{n}(\theta,\theta_{0})\right]\leq\log\mathbb{E}_{\Pi_{n}}\left[e^{ \zeta L_{n}(\theta,\theta_{0})}e^{\gamma R(a^{\prime},\theta)}\right]+ \mathbb{E}_{Q}\left[\log\frac{dQ(\theta)}{e^{\gamma R(a^{\prime},\theta)}\ d \Pi(\theta|\tilde{X}_{n})}\right].\]

Finally, it follows from the definition of the posterior distribution that

\[\zeta\mathbb{E}_{Q^{*}_{a^{\prime},\gamma}(\theta|\tilde{X}_{n})} \left[L_{n}(\theta,\theta_{0})\right]\] \[\leq\log\int_{\Theta}e^{\zeta L_{n}(\theta,\theta_{0})}e^{\gamma R (a^{\prime},\theta)}\frac{\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{ \int_{\Theta}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}+\mathbb{E}_{Q} \left[\log\frac{dQ(\theta)}{e^{\gamma R(a^{\prime},\theta)}\ d\Pi(\theta| \tilde{X}_{n})}\right],\] \[= \log\int_{\Theta}e^{\zeta L_{n}(\theta,\theta_{0})}\ \frac{e^{\gamma R(a^{ \prime},\theta)}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta }e^{\gamma R(a^{\prime},\theta)}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi( \theta)}+\mathbb{E}_{Q}\left[\log\frac{dQ(\theta)}{e^{\gamma R(a^{\prime}, \theta)}\ d\Pi(\theta|\tilde{X}_{n})}\right]\] \[+\log\int_{\Theta}\ e^{\gamma R(a^{\prime},\theta)}\frac{ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{n}( \theta,\theta_{0})d\Pi(\theta)},\] (20)

where the last equality follows from adding and subtracting \(\log\mathbb{E}_{\Pi}\left[e^{\gamma R(a^{\prime},\theta)}\mathcal{LR}_{n}( \theta,\theta_{0})\right]\). Now taking expectation on either side of equation (20) and using Jensen's inequality on the first and the last term in the RHS yields

\[\mathbb{E}_{P^{n}_{0}}\left[\zeta\mathbb{E}_{Q^{*}_{a^{\prime}, \gamma}(\theta|\tilde{X}_{n})}\left[L_{n}(\theta,\theta_{0})\right]\right]\] \[\leq \log\mathbb{E}_{P^{n}_{0}}\left[\int_{\Theta}e^{\zeta L_{n}(\theta,\theta_{0})}\frac{e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]+\inf_{Q\in \mathcal{Q}}\mathbb{E}_{P^{n}_{0}}\bigg{[}\text{KL}(Q\|\Pi_{n})\] \[-\gamma\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}\left[R(a,\theta) \right]\bigg{]}+\log\mathbb{E}_{P^{n}_{0}}\left[\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \frac{\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta} \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right],\] (21)

where in the second term in RHS of (20), we first take infimum over all \(a\in\mathcal{A}\) which upper bounds the second term in (20) and then take infimum over all \(Q\in\mathcal{Q}\), since the LHS does not depend on \(Q\). 

Next, we state a technical result that is important in proving our next lemma.

**Lemma C.2** (Lemma 6.4 of [34]).: _Suppose random variable X satisfies_

\[\mathbb{P}(X\geq t)\leq c_{1}\exp(-c_{2}t),\]

_for all \(t\geq t_{0}>0\). Then for any \(0<\beta\leq c_{2}/2\),_

\[\mathbb{E}[\exp(\beta X)]\leq\exp(\beta t_{0})+c_{1}.\]

Proof.: Refer Lemma 6.4 of [34].

[MISSING_PAGE_FAIL:20]

where in the second inequality, we first divide the second term over set \(B_{n}\) and its complement and then use the fact that \(\frac{\int_{K_{n}}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta, \theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta, \theta_{0})d\Pi(\theta)}\leq 1\). The third inequality is due the fact that \(\phi_{n,\epsilon}\in[0,1]\). Next, using Assumption 2.3 and 2.5 observe that on set \(B_{n}\)

\[\int_{\Theta}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}( \theta,\theta_{0})d\Pi(\theta) \geq W^{\gamma}\int_{\Theta}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)\] \[\geq W^{\gamma}e^{-(1+C_{2}+C_{3})n\epsilon_{n}^{2}}\geq W^{ \gamma}e^{-(1+C_{2}+C_{3})n\epsilon^{2}}.\]

Substituting the equation above in the third term of equation (26), we obtain

\[\mathbb{E}_{P_{0}^{n}}\left[(1-\phi_{n,\epsilon})\mathbbm{1}_{B_ {n}}\frac{\int_{K_{n}}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]\] \[\leq W^{-\gamma}e^{(1+C_{2}+C_{3})n\epsilon^{2}}\mathbb{E}_{P_{0 }^{n}}\left[(1-\phi_{n,\epsilon})\mathbbm{1}_{B_{n}}\int_{K_{n}}e^{\gamma R(a^ {\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)\right]\] \[\leq W^{-\gamma}e^{(1+C_{2}+C_{3})n\epsilon^{2}}\mathbb{E}_{P_{0 }^{n}}\left[(1-\phi_{n,\epsilon})\int_{K_{n}}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)\right].\] ( \[\star\] )

Now using Fubini's theorem observe that,

\[(\star) =W^{-\gamma}e^{(1+C_{2}+C_{3})n\epsilon^{2}}\int_{K_{n}}e^{ \gamma R(a^{\prime},\theta)}\mathbb{E}_{P_{\theta}^{n}}\left[(1-\phi_{n, \epsilon})\right]d\Pi(\theta)\] \[\leq W^{-\gamma}e^{(1+C_{2}+C_{3}+C_{4}(\gamma))n\epsilon^{2}} \bigg{[}\int_{K_{n}\cap\{e^{\gamma R(a^{\prime},\theta)}\leq e^{C_{4}(\gamma)n \epsilon^{2}}\}}\mathbb{E}_{P_{\theta}^{n}}\left[(1-\phi_{n,\epsilon})\right]d \Pi(\theta)\] \[\qquad\qquad\qquad+e^{-C_{4}(\gamma)n\epsilon^{2}}\int_{K_{n}\cap \{e^{\gamma R(a^{\prime},\theta)}>e^{C_{4}(\gamma)n\epsilon^{2}}\}}e^{\gamma R (a^{\prime},\theta)}d\Pi(\theta)\bigg{]},\]

where in the last inequality, we first divide the integral over set \(\{\theta\in\Theta:e^{\gamma R(a^{\prime},\theta)}\leq e^{C_{4}(\gamma)n \epsilon^{2}}\}\) and its complement and then use the upper bound on \(e^{\gamma R(a^{\prime},\theta)}\) in the first integral. Now, it follows that

\[(\star) \leq W^{-\gamma}e^{(1+C_{2}+C_{3}+C_{4}(\gamma))n\epsilon^{2}} \Bigg{[}\int_{K_{n}}\mathbb{E}_{P_{\theta}^{n}}\left[(1-\phi_{n,\epsilon}) \right]d\Pi(\theta)\] \[+e^{-C_{4}(\gamma)n\epsilon^{2}}\int_{\{e^{\gamma R(a^{\prime}, \theta)}>e^{C_{4}(\gamma)n\epsilon^{2}}\}}e^{\gamma R(a^{\prime},\theta)}d\Pi( \theta)\Bigg{]}\] \[=W^{-\gamma}e^{(1+C_{2}+C_{3}+C_{4}(\gamma))n\epsilon^{2}} \bigg{[}\int_{K_{n}\cap\Theta_{n}(\epsilon)}\mathbb{E}_{P_{\theta}^{n}}\left[( 1-\phi_{n,\epsilon})\right]d\Pi(\theta)\] \[+\int_{K_{n}\cap\Theta_{n}(\epsilon)^{c}}\mathbb{E}_{P_{\theta}^ {n}}\left[(1-\phi_{n,\epsilon})\right]d\Pi(\theta)+e^{-C_{4}(\gamma)n\epsilon^ {2}}\int_{\{e^{\gamma R(a^{\prime},\theta)}>e^{C_{4}(\gamma)n\epsilon^{2}}\}}e^ {\gamma R(a^{\prime},\theta)}d\Pi(\theta)\bigg{]}\] \[\leq W^{-\gamma}e^{(1+C_{2}+C_{3}+C_{4}(\gamma))n\epsilon^{2}} \bigg{[}\int_{K_{n}\cap\Theta_{n}(\epsilon)}\mathbb{E}_{P_{\theta}^{n}}\left[( 1-\phi_{n,\epsilon})\right]d\Pi(\theta)+\Pi(\Theta_{n}(\epsilon)^{c})\] \[\qquad\qquad\qquad\qquad+e^{-C_{4}(\gamma)n\epsilon^{2}}\int_{\{e^ {\gamma R(a^{\prime},\theta)}>e^{C_{4}(\gamma)n\epsilon^{2}}\}}e^{\gamma R(a ^{\prime},\theta)}d\Pi(\theta)\bigg{]},\]

where the second equality is obtained by dividing the first integral on set \(\Theta_{n}(\epsilon)\) and its complement, and the second inequality is due the fact that \(\phi_{n,\epsilon}\in[0,1]\). Now, using the equation above and Assumption 2.1, 2.2, and 2.4 observe that

\[\mathbb{E}_{P_{0}^{n}}\Bigg{[}(1-\phi_{n,\epsilon})\mathbbm{1}_{B_ {n}}\frac{\int_{K_{n}}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\Bigg{]}\] \[\qquad\leq W^{-\gamma}e^{(1+C_{2}+C_{3}+C_{4}(\gamma))n\epsilon^{ 2}}\left[2e^{-Cn\epsilon^{2}}+e^{-(C_{5}(\gamma)+C_{4}(\gamma))n\epsilon^{2}} \right].\]

Hence, choosing \(C,C_{2},C_{3},C_{4}(\gamma)\) and \(C_{5}(\gamma)\) such that \(-1>1+C_{2}+C_{3}+C_{4}(\gamma)-\min(C,(C_{4}(\gamma)+C_{5}(\gamma)))\) implies\[\mathbb{E}_{P_{0}^{n}}\left[(1-\phi_{n,e})\mathbb{I}_{B_{n}}\frac{\int_{K_{n}}e^{ \gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]\leq 3W^{- \gamma}e^{-n\epsilon^{2}}.\] (27)

By Assumption 2.1, we have

\[\mathbb{E}_{P_{0}^{n}}\phi_{n,e}\leq C_{0}e^{-Cn\epsilon^{2}}.\] (28)

Therefore, substituting equation (25), equation (27), and (28) into (26), we obtain

\[\mathbb{E}_{P_{0}^{n}}\left[\frac{\int_{K_{n}}e^{\gamma R(a^{\prime},\theta)} \ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]\leq(1+C_{0}+3W^{- \gamma})e^{-C_{10}C_{10}\epsilon^{2}},\] (29)

where \(C_{10}=\min\{\lambda,C,1\}/C_{1}\). Using Fubini's theorem, observe that the LHS in the equation (29) can be expressed as \(\mu(K_{n})\), where

\[d\mu(\theta)=\mathbb{E}_{P_{0}^{n}}\left[\frac{\mathcal{LR}_{n}(\theta,\theta_ {0})}{\int_{\Theta}e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]\Pi(\theta)e^{ \gamma R(a^{\prime},\theta)}d\theta.\]

Next, recall that the set \(K_{n}=\{\theta\in\Theta:L_{n}(\theta,\theta_{0})>C_{1}n\epsilon^{2}\}\). Applying Lemma C.2 above with \(X=L_{n}(\theta,\theta_{0})\), \(c_{1}=(1+C_{0}+3W^{-\gamma})\), \(c_{2}=C_{10}\), \(t_{0}=C_{1}n\epsilon_{n}^{2}\), and for \(0<\zeta\leq C_{10}/2\), we obtain

\[\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\zeta L_{n}(\theta,\theta_{0})} \frac{e^{\gamma R(a^{\prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{ \prime},\theta)}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}d\theta \right]\leq e^{\zeta C_{1}n\epsilon_{n}^{2}}+(1+C_{0}+3W^{-\gamma}).\] (30)

Further, we have another technical lemma, that will be crucial in proving the subsequent lemma that upper bounds the last term in the equation (19).

**Lemma C.4**.: _Suppose a positive random variable X satisfies_

\[\mathbb{P}(X\geq e^{t})\leq c_{1}\exp(-(c_{2}+1)t),\]

_for all \(t\geq t_{0}>0\), \(c_{1}>0\), and \(c_{2}>0\). Then,_

\[\mathbb{E}[X]\leq\exp(t_{0})+\frac{c_{1}}{c_{2}}.\]

Proof.: For any \(Z_{0}>1\),

\[\mathbb{E}[X] \leq Z_{0}+\int_{Z_{0}}^{\infty}\mathbb{P}(X\geq x)dx\] \[=Z_{0}+\int_{\ln Z_{0}}^{\infty}\mathbb{P}(X\geq e^{y})e^{y}dy \leq Z_{0}+c_{1}\int_{\ln Z_{0}}^{\infty}\exp(-c_{2}y)dy.\]

Therefore, choosing \(Z_{0}=\exp(t_{0})\),

\[\mathbb{E}[X]\leq\exp(t_{0})+\frac{c_{1}}{c_{2}}\exp(-c_{2}t_{0})\leq\exp(t_{ 0})+\frac{c_{1}}{c_{2}}.\]

Next, we establish the following bound on the last term in equation (19).

**Lemma C.5**.: _Under Assumptions 2.1, 2.2, 2.3, 2.4, 2.5, and for \(C_{5}(\gamma)>C_{2}+C_{3}+2\),_

\[\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}\frac{e^{\gamma R(a^{\prime},\theta)} \ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\ \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]\leq e^{C_{4}(\gamma)n \epsilon_{n}^{2}}+2C_{4}(\gamma).\] (31)

_for any \(\lambda\geq 1+C_{4}(\gamma)\)._Proof.: Define the set

\[M_{n}:=\{\theta\in\Theta:e^{\gamma R(a^{\prime},\theta)}>e^{C_{4}(\gamma)n\epsilon^ {2}}\}.\] (32)

Using the set \(B_{n}\) in equation (23), observe that the measure of the set \(M_{n}\), under the posterior distribution satisfies,

\[\mathbb{E}_{P_{0}^{n}}\left[\frac{\int_{M_{n}}\mathcal{LR}_{n}( \theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{n}(\theta,\theta_{0} )d\Pi(\theta)}\right]\leq\mathbb{E}_{P_{0}^{n}}\left[\mathbbm{1}_{B_{n}^{c}} \right]+\mathbb{E}_{P_{0}^{n}}\left[\mathbbm{1}_{B_{n}}\frac{\int_{M_{n}} \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{n} (\theta,\theta_{0})d\Pi(\theta)}\right].\] (33)

Now, the second term of equation (33) can be bounded as follows: recall Assumption 2.3 and the definition of set \(B_{n}\), both together imply that,

\[\mathbb{E}_{P_{0}^{n}}\left[\mathbbm{1}_{B_{n}}\frac{\int_{M_{n} }\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_ {n}(\theta,\theta_{0})d\Pi(\theta)}\right] \leq e^{(1+C_{2}+C_{3})n\epsilon^{2}}\mathbb{E}_{P_{0}^{n}} \left[\mathbbm{1}_{B_{n}}\int_{M_{n}}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi( \theta)\right]\] \[\leq e^{(1+C_{2}+C_{3})n\epsilon^{2}}\mathbb{E}_{P_{0}^{n}} \left[\int_{M_{n}}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)\right].\] ( \[\star\star\] )

Then, using Fubini's Theorem \((\star\star)=e^{(1+C_{2}+C_{3})n\epsilon^{2}}\Pi(M_{n})\). Next, using the definition of set \(M_{n}\) and then Assumption 2.4, we obtain

\[\mathbb{E}_{P_{0}^{n}}\left[\mathbbm{1}_{B_{n}}\frac{\int_{M_{n} }\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{ n}(\theta,\theta_{0})d\Pi(\theta)}\right] \leq e^{(1+C_{2}+C_{3})n\epsilon^{2}}e^{-C_{4}(\gamma)n\epsilon^ {2}}\int_{M_{n}}e^{\gamma R(a^{\prime},\theta)}d\Pi(\theta)\] \[\leq e^{(1+C_{2}+C_{3})n\epsilon^{2}}e^{-C_{4}(\gamma)n\epsilon^ {2}}e^{-C_{5}(\gamma)n\epsilon^{2}},\]

Hence, choosing the constants \(C_{2},C_{3},C_{4}(\gamma)\) and \(C_{5}(\gamma)\) such that \(-1>1+C_{2}+C_{3}-C_{5}(\gamma)\) implies

\[\mathbb{E}_{P_{0}^{n}}\left[\mathbbm{1}_{B_{n}}\frac{\int_{M_{n} }\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{ n}(\theta,\theta_{0})d\Pi(\theta)}\right]\leq e^{-(1+C_{4}(\gamma))n\epsilon^{2}}\] (34)

Therefore, substituting (25) and (34) into (33)

\[\mathbb{E}_{P_{0}^{n}}\left[\frac{\int_{M_{n}}\mathcal{LR}_{n}( \theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}\mathcal{LR}_{n}(\theta,\theta_{ 0})d\Pi(\theta)}\right]\leq 2e^{-C_{4}(\gamma)(C_{11}(\gamma)+1)n\epsilon^{2}},\] (35)

where \(C_{11}(\gamma)=\min\{\lambda,1+C_{4}(\gamma)\}/C_{4}(\gamma)-1\). Using Fubini's theorem, observe that the RHS in (35) can be expressed as \(\nu(M_{n})\), where the measure

\[d\nu(\theta)=\mathbb{E}_{P_{0}^{n}}\left[\frac{\mathcal{LR}_{n}( \theta,\theta_{0})}{\int_{\Theta}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi( \theta)}\right]d\Pi(\theta).\]

Applying Lemma C.4 for \(X=e^{\gamma R(a^{\prime},\theta)}\),\(c_{1}=2\), \(c_{2}=C_{11}(\gamma)\), \(t_{0}=C_{4}(\gamma)n\epsilon_{n}^{2}\) and \(\lambda\geq 1+C_{4}(\gamma)\), we obtain

\[\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}\frac{e^{\gamma R(a^{ \prime},\theta)}\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta} \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]\leq e^{C_{4}(\gamma)n \epsilon_{n}^{2}}+\frac{2}{C_{11}(\gamma)}\leq e^{C_{4}(\gamma)n\epsilon_{n}^ {2}}+2C_{4}(\gamma).\] (36)

Proof.: Proof of Theorem 3.1: Finally, recall (19),

\[\mathbb{C}\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}L_{n}(\theta, \theta_{0})\;dQ_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\right]\] \[\leq\log\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\zeta L_{n}( \theta,\theta_{0})}\frac{e^{\gamma R(a^{\prime},\theta)}\mathcal{LR}_{n}( \theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta}e^{\gamma R(a^{\prime},\theta)} \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right]+\inf_{Q\in\mathcal{Q}} \mathbb{E}_{P_{0}^{n}}\bigg{[}\text{KL}(Q\|\Pi_{n})\] \[\quad-\gamma\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)] \bigg{]}+\log\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}e^{\gamma R(a^{\prime}, \theta)}\;\frac{\mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}{\int_{\Theta} \mathcal{LR}_{n}(\theta,\theta_{0})d\Pi(\theta)}\right].\] (37)Substituting (31) and (22) into the equation above and then using the definition of \(\eta_{n}^{R}(\gamma)\), we get

\[\mathbb{E}_{P_{0}^{m}}\left[\int_{\Theta}L_{n}(\theta,\theta_{0})\; dQ_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\right]\] \[\leq \frac{1}{\zeta}\left\{\log(e^{\zeta C_{1}n\epsilon_{n}^{2}}+(1+C_ {0}+3W^{-\gamma}))+\log\left(e^{C_{4}(\gamma)n\epsilon_{n}^{2}}+2C_{4}(\gamma) \right)+n\eta_{n}^{R}(\gamma)\right\}\] \[\leq \left(C_{1}+\frac{1}{\zeta}C_{4}(\gamma)\right)n\epsilon_{n}^{2} +\frac{1}{\zeta}n\eta_{n}^{R}(\gamma)+\frac{(1+C_{0}+3W^{-\gamma})e^{(-\zeta C _{1}n\epsilon_{n}^{2})}+2C_{4}(\gamma)e^{-C_{4}(\gamma)n\epsilon_{n}^{2}}}{ \zeta},\]

where the last inequality uses the fact that \(\log x\leq x-1\). Choosing \(\zeta=C_{10}/2=\frac{\min(C,\lambda,1)}{2C_{1}}\),

\[\mathbb{E}_{P_{0}^{m}}\left[\int_{\Theta}L_{n}(\theta,\theta_{0} )\;dQ_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\right]\] \[\qquad\leq M(\gamma)n(\epsilon_{n}^{2})+M^{\prime}n\eta_{n}^{R}( \gamma)+\frac{2(1+C_{0}+3W^{-\gamma})e^{(-\frac{C_{10}}{2}n\epsilon_{n}^{2})}+ 4C_{4}(\gamma)e^{-C_{4}(\gamma)n\epsilon_{n}^{2}}}{C_{10}}\] (38)

where \(M(\gamma)=C_{1}+\frac{1}{\zeta}C_{4}(\gamma)\) and \(M^{\prime}=\frac{1}{\zeta}\) depend on \(C,C_{1},C_{4}(\gamma),W\) and \(\lambda\). Since the last two terms in (38) decrease and the first term increases as \(n\) increases, we can choose \(M^{\prime}\) large enough, such that for all \(n\geq 1\)

\[M^{\prime}n\eta_{n}^{R}(\gamma)>\frac{2(1+C_{0}+3W^{-\gamma})}{C_{10}}+\frac{ 4C_{4}(\gamma)}{C_{10}},\]

and therefore for \(M=2M^{\prime}\),

\[\mathbb{E}_{P_{0}^{m}}\left[\int_{\Theta}L_{n}(\theta,\theta_{0})\;dQ_{a^{ \prime},\gamma}^{*}(\theta|\tilde{X}_{n})\right]\leq M(\gamma)n(\epsilon_{n}^ {2})+Mn\eta_{n}^{R}(\gamma).\] (39)

Also, observe that the LHS in the above equation is always positive, therefore \(M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}(\gamma)\geq 0\;\forall n\geq 1\) and \(\gamma>0\).

### Proof of Theorem 3.2

**Lemma C.6**.: _Given \(a^{\prime}\in\mathcal{A}\) and for a constant M, as defined in Theorem 3.1_

\[\mathbb{E}_{P_{0}^{m}}\left[\sup_{a\in\mathcal{A}}\left|\mathbb{E}_{Q_{a^{ \prime},\gamma}^{*}(\theta|\tilde{X}_{n})}[R(a,\theta)]-R(a,\theta_{0}) \right|\right]\leq\left[M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}(\gamma)\right] ^{\frac{1}{2}}.\] (40)

Proof.: First, observe that

\[\left(\sup_{a\in\mathcal{A}}\left|\mathbb{E}_{Q_{a^{\prime},\gamma }^{*}(\theta|\tilde{X}_{n})}[R(a,\theta)]-R(a,\theta)\right|\right)^{2}\leq \left(\mathbb{E}_{Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})}\left[ \sup_{a\in\mathcal{A}}|R(a,\theta)-R(a,\theta_{0})|\right]\right)^{2}\] \[\leq \mathbb{E}_{Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})} \left[\left(\sup_{a\in\mathcal{A}}|R(a,\theta)-R(a,\theta_{0})|\right)^{2} \right],\]

where the last inequality follows from Jensen's inequality. Now, using the Jensen's inequality again

\[\left(\mathbb{E}_{P_{0}^{n}}\left[\sup_{a\in\mathcal{A}}\left| \mathbb{E}_{Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})}[R(a,\theta)]-R(a, \theta_{0})\right|\right]\right)^{2}\] \[\qquad\leq\mathbb{E}_{P_{0}^{n}}\left[\left(\sup_{a\in\mathcal{ A}}\left|\mathbb{E}_{Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})}[R(a, \theta)]-R(a,\theta_{0})\right|\right)^{2}\right].\]

Now, using Theorem 3.1 the result follows immediately.

Proof of Theorem 3.2.: Observe that

\[R(\mathbf{a}_{\mathtt{RS}}^{*},\theta_{0})-\inf_{z\in\mathcal{A}}R(z,\theta_{0})\] \[=|R(\mathbf{a}_{\mathtt{RS}}^{*},\theta_{0})-\inf_{z\in\mathcal{A} }R(z,\theta_{0})|\] \[=R(\mathbf{a}_{\mathtt{RS}}^{*},\theta_{0})-\mathbb{E}_{Q_{ \mathtt{hs}}^{*},\gamma}(\theta|\tilde{X}_{n})[R(\mathbf{a}_{\mathtt{RS}}^{*},\theta)]+\mathbb{E}_{Q_{\mathtt{hs}}^{*},\gamma}(\theta|\tilde{X}_{n})[R( \mathbf{a}_{\mathtt{RS}}^{*},\theta)]-\inf_{z\in\mathcal{A}}R(z,\theta_{0})\] \[\leq\left|R(\mathbf{a}_{\mathtt{RS}}^{*},\theta_{0})-\mathbb{E}_ {Q_{\mathtt{hs}}^{*},\gamma}(\theta|\tilde{X}_{n})[R(\mathbf{a}_{\mathtt{RS}}^ {*},\theta)]\right|+\left|\mathbb{E}_{Q_{\mathtt{hs}}^{*},\gamma}(\theta| \tilde{X}_{n})[R(\mathbf{a}_{\mathtt{RS}}^{*},\theta)]-\inf_{a\in\mathcal{A} }R(a,\theta_{0})\right|\] \[\leq 2\sup_{a\in\mathcal{A}}\left|\int R(a,\theta)dQ_{\mathtt{hs} }^{*},\gamma}(\theta|\tilde{X}_{n})-R(a,\theta_{0})\right|.\] (41)

Given \(\mathbf{a}_{\mathtt{RS}}^{*}\in\mathcal{A}\) and for a constant M (defined in Theorem 3.1), we have from Lemma C.6 for \(a^{\prime}=\mathbf{a}_{\mathtt{RS}}^{*}\)

\[\mathbb{E}_{P_{0}^{n}}\left[\sup_{a\in\mathcal{A}}\left|\int R(a,\theta)dQ_{ \mathtt{hs}}^{*},\gamma(\theta|\tilde{X}_{n})-R(a,\theta_{0})\right|\right] \leq\left[M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R}(\gamma)\right]^{\frac{1}{2}}.\] (42)

It follows from above that the \(P_{0}^{n}-\) probability of the following event is at least \(1-\tau^{-1}\):

\[\left\{\tilde{X}_{n}:R(\mathbf{a}_{\mathtt{RS}}^{*},\theta_{0})-\inf_{z\in \mathcal{A}}R(z,\theta_{0})\leq 2\tau\left[M(\gamma)\epsilon_{n}^{2}+M\eta_{n}^{R }(\gamma)\right]^{\frac{1}{2}}\right\}.\] (43)

### Proofs in Section 3.1

Proof of Proposition 3.1.: Using the definition of \(\eta_{n}^{R}(\gamma)\) and the posterior distribution \(\Pi(\theta|\tilde{X}_{n})\), observe that

\[n\eta_{n}^{R}(\gamma)=\inf_{Q\in\mathcal{Q}}\mathbb{E}_{P_{0}^{n }}\left[\text{KL}(Q\|\Pi_{n})-\gamma\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a, \theta)]\right]\] \[=\inf_{Q\in\mathcal{Q}}\mathbb{E}_{P_{0}^{n}}\left[\text{KL}(Q\| \Pi)+\int_{\Theta}dQ(\theta)\log\left(\frac{\int d\Pi(\theta)p(\tilde{X}_{n}| \theta)}{p(\tilde{X}_{n}|\theta)}\right)-\gamma\inf_{a\in\mathcal{A}}\mathbb{E }_{Q}[R(a,\theta)]\right]\] \[=\inf_{Q\in\mathcal{Q}}\left[\text{KL}(Q\|\Pi)-\gamma\inf_{a\in \mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]+\mathbb{E}_{P_{0}^{n}}\left[\mathbb{E }_{Q}\left[\log\left(\frac{\int d\Pi(\theta)p(\tilde{X}_{n}|\theta)}{p( \tilde{X}_{n}|\theta)}\right)\right]\right]\right].\]

Now, using Fubini's in the last term of the equation above, we obtain

\[n\eta_{n}^{R}(\gamma) =\inf_{Q\in\mathcal{Q}}\Bigg{[}\text{KL}(Q(\theta)\|\Pi(\theta))- \gamma\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\] \[\quad+\mathbb{E}_{Q}\left[\text{KL}\left(dP_{0}^{n}\|p(\tilde{X}_{ n}|\theta)\right)-\text{KL}\left(dP_{0}^{n}\right\|\int d\Pi(\theta)p( \tilde{X}_{n}|\theta)\right)\right]\Bigg{]}.\] (44)

Observe that, \(\int_{\mathcal{X}^{n}}\int d\Pi(\theta)p(\tilde{X}_{n}|\theta)d\tilde{X}_{n}=1\). Since, KL is always non-negative, it follows from the equation above that

\[\eta_{n}^{R}(\gamma)\] \[\leq\frac{1}{n}\inf_{Q\in\mathcal{Q}}\Big{[}\text{KL}\left(Q( \theta)\|\Pi(\theta)\right)+\mathbb{E}_{Q}\left[\text{KL}\left(dP_{0}^{n}\|p( \tilde{X}_{n}|\theta)\right)\right]\Big{]}-\frac{\gamma}{n}\inf_{Q\in\mathcal{ Q}}\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)],\] (45)

where the last inequality follows from the following fact, for any functions \(f(\cdot)\) and \(g(\cdot)\),

\[\inf(f-g)\leq\inf f-\inf g.\]Recall \(\epsilon^{\prime}_{n}\geq\frac{1}{\sqrt{n}}\). Now, using Assumption 3.1, it is straightforward to observe that the first term in (45),

\[\frac{1}{n}\inf_{Q\in\mathcal{Q}}\left[\text{KL}\left(Q(\theta)\| \Pi(\theta)\right)+\mathbb{E}_{Q}\left[\text{KL}\left(dP^{n}_{0}\|p(\tilde{X}_{ n}|\theta)\right)\right]\right]\leq C_{9}\epsilon^{\prime 2}_{n}.\] (46)

Now consider the last term in (45). Notice that the coefficient of \(\frac{1}{n}\) is independent of \(n\) and is bounded from below. Therefore, there exist a constant \(C_{8}=-\inf_{Q\in\mathcal{Q}}\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\), such that with equation (46) it follows that \(\eta^{R}_{n}(\gamma)\leq\gamma n^{-1}C_{8}+C_{9}\epsilon^{\prime 2}_{n}\) and the result follows.

Proof of Proposition 3.2.: First recall that

\[n\eta^{R}_{n}(\gamma) =\inf_{Q\in\mathcal{Q}}\mathbb{E}_{P^{n}_{0}}\left[\text{KL}(Q( \theta)\|\Pi(\theta|\tilde{X}_{n}))-\gamma\inf_{a\in\mathcal{A}}\mathbb{E}_{ Q}[R(a,\theta)]\right]\] \[=\inf_{Q\in\mathcal{Q}}\mathbb{E}_{P^{n}_{0}}\left[\text{KL}(Q( \theta)\|\Pi(\theta|\tilde{X}_{n}))\right]-\gamma\inf_{a\in\mathcal{A}}\mathbb{ E}_{Q}[R(a,\theta)].\] (47)

Observe that the optimization problem is equivalent to solving :

\[\min_{Q\in\mathcal{Q}}\mathbb{E}_{P^{n}_{0}}\left[\text{KL}(Q( \theta)\|\Pi(\theta|\tilde{X}_{n}))\right]\text{ s.t. }-\inf_{a\in\mathcal{A}}\mathbb{E}_{Q}[R(a,\theta)]\leq 0.\] (48)

Now for any \(\gamma>0\), \(Q^{*}_{\gamma}(\theta)\in\mathcal{Q}\) that minimizes the objective in (47) is primal feasible if

\[-\inf_{a\in\mathcal{A}}\int_{\Theta}dQ^{*}_{\gamma}(\theta)R(a, \theta)\leq 0.\]

Therefore, it is straightforward to observe that as \(\gamma\) increases \(n\eta^{R}_{n}(\gamma)\) decreases that is

\[\mathbb{E}_{P^{n}_{0}}\left[\int_{\Theta}dQ^{*}_{\gamma}(\theta) \log\frac{dQ^{*}_{\gamma}(\theta)}{d\Pi(\theta|\tilde{X}_{n})}-\gamma\inf_{a \in\mathcal{A}}\int_{\Theta}dQ^{*}_{\gamma}(\theta)R(a,\theta)\right].\]

### Sufficient conditions on \(R(a,\theta)\) for existence of tests

To show the existence of test functions, as required in Assumption 2.1, we will use the following result from [11, Theorem 7.1], that is applicable only to distance measures that are bounded above by the Hellinger distance.

**Lemma C.7** (Theorem 7.1 of [11]).: _Suppose that for some non-increasing function \(D(\epsilon)\), some \(\epsilon_{n}>0\) and for every \(\epsilon>\epsilon_{n}\),_

\[N\left(\frac{\epsilon}{2},\left\{P_{\theta}:\epsilon\leq m( \theta,\theta_{0})\leq 2\epsilon\right\},m\right)\leq D(\epsilon),\]

_where \(m(\cdot,\cdot)\) is any distance measure bounded above by Hellinger distance. Then for every \(\epsilon>\epsilon_{n}\), there exists a test \(\phi_{n}\) (depending on \(\epsilon>0\)) such that, for every \(j\geq 1\),_

\[\mathbb{E}_{P^{n}_{0}}[\phi_{n}] \leq D(\epsilon)\exp\left(-\frac{1}{2}n\epsilon^{2}\right)\frac {1}{1-\exp\left(-\frac{1}{2}n\epsilon^{2}\right)},\text{and}\] \[\sup_{\left\{\theta\in\Theta_{n}(\epsilon):m(\theta,\theta_{0})> j\epsilon\right\}}\mathbb{E}_{P^{n}_{\theta}}[1-\phi_{n}] \leq\exp\left(-\frac{1}{2}n\epsilon^{2}j\right).\]

Proof of Lemma c.7.: Refer Theorem 7.1 of [11]. 

For the remaining part of this subsection we assume that \(\Theta\subseteq\mathbb{R}^{d}\). In the subsequent paragraph, we state further assumptions on the risk function to show \(L_{n}(\cdot,\cdot)\) as defined in (6) satisfies Assumption 2.1. For brevity we denote \(n^{-1/2}\sqrt{L_{n}(\theta,\theta_{0})}\) by \(d_{L}(\theta,\theta_{0})\), that is \[d_{L}(\theta_{1},\theta_{2}):=\sup_{a\in\mathcal{A}}|R(a,\theta_{1})-R(a,\theta_{2} )|,\;\forall\{\theta_{1},\theta_{2}\}\in\Theta\] (49)

and the covering number of the set \(T(\epsilon):=\{P_{\theta}:d_{L}(\theta,\theta_{0})<\epsilon\}\) as \(N(\delta,T(\epsilon),d_{L})\), where \(\delta>0\) is the radius of each ball in the cover. We assume that the risk function \(R(a,\cdot)\) satisfies the following bound.

**Assumption C.1**.: _The model risk satisfies_

\[d_{L}(\theta_{1},\theta_{2})|\leq K_{1}d_{H}(\theta,\theta_{0}),\]

_where \(d_{H}(\theta_{1},\theta_{2})\) is the Hellinger distance between two models \(P_{\theta_{1}}\) and \(P_{\theta_{2}}\)._

For instance, suppose the definition of model risk is \(R(a,\theta)=\int_{\mathcal{X}}\ell(x,a)p(y|\theta)dx\), where \(\ell(x,a)\) is an underlying loss function. Then, observe that Assumption C.1 is trivially satisfied if \(\ell(x,a)\) is bounded in \(x\) for a given \(a\in\mathcal{A}\) and \(\mathcal{A}\) is compact, since \(d_{L}(\theta_{1},\theta_{2})\) can be bounded by the total variation distance \(d_{TV}(\theta_{1},\theta_{2})=\frac{1}{2}\int|dP_{\theta_{1}}(x)-dP_{\theta_{ 2}}(x)|\) and total variation distance is bounded above by the Hellinger distance [12]. Under the assumption above it also follows that we can apply Lemma C.7 to the metric \(d_{L}(\cdot,\cdot)\) defined in (49). Now, we will also assume an additional regularity condition on the risk function.

**Assumption C.2**.: _For every \(\{\theta_{1},\theta_{2}\}\in\Theta\), there exists a constant \(K_{2}>0\) such that_

\[d_{L}(\theta_{1},\theta_{2})\leq K_{2}\|\theta_{1}-\theta_{2}\|,\]

We can now show that the covering number of the set \(T(\epsilon)\) satisfies

**Lemma C.8**.: _Given \(\epsilon>\delta>0\), and under Assumption C.2,_

\[N(\delta,T(\epsilon),d_{L})<\left(\frac{2\epsilon}{\delta}+2\right)^{d}.\] (50)

Proof of Lemma c.8.: For any positive \(k\) and \(\epsilon\), let \(\theta\in[\theta_{0}-k\epsilon,\theta_{0}+k\epsilon]^{d}\subset\Theta\subset \mathbb{R}^{d}\). Now consider a set \(H_{i}=\{\theta_{i}^{0},\theta_{i}^{1},\dots\theta_{i}^{j},\theta_{i}^{j+1}\}\) and \(H=\bigotimes_{d}H_{i}\) with \(J=\lfloor\frac{2k\epsilon}{\delta^{\prime}}\rfloor\), where \(\theta_{i}^{j}=\theta_{0}-k\epsilon+i\delta^{\prime}\) for \(j=\{0,1,\dots,J\}\) and \(\theta_{i}^{j+1}=\theta_{0}+k\epsilon\). Observe that for any \(\theta\in[\theta_{0}-k\epsilon,\theta_{0}+k\epsilon]^{d}\), there exists a \(\theta^{j}\in H\) such that \(\|\theta-\theta^{j}\|<\delta^{\prime}\). Hence, union of the \(\delta^{\prime}-\)balls for each element in set \(H\) covers \([\theta_{0}-k\epsilon,\theta_{0}+k\epsilon]^{d}\), therefore \(N(\delta^{\prime},[\theta_{0}-k\epsilon,\theta_{0}+k\epsilon]^{d},\|\cdot\|)= (J+2)^{d}\).

Now, due to Assumption C.2, for any \(\theta\in[\theta_{0}-k\epsilon,\theta_{0}+k\epsilon]^{d}\)

\[d_{L}(\theta,\theta_{0})\leq K_{2}\|\theta-\theta^{j}\|\leq K_{2}\delta^{ \prime},\]

For brevity, we denote \(n^{-1}L_{n}(\theta,\theta_{0})\) by \(d_{L}(\theta,\theta_{0})\), that is

\[d_{L}(\theta_{1},\theta_{2}):=\sup_{a\in\mathcal{A}}|R(a,\theta_{1})-R(a, \theta_{2})|,\;\forall\{\theta_{1},\theta_{2}\}\in\Theta,\] (51)

and the covering number of the set \(T(\epsilon):=\{P_{\theta}:d_{L}(\theta,\theta_{0})<\epsilon\}\) as \(N(\delta,T(\epsilon),d_{L})\), where \(\delta>0\) is the radius of each ball in the cover.

Hence, \(\delta^{\prime}\)-cover of set \([\theta_{0}-k\epsilon,\theta_{0}+k\epsilon]^{d}\) is \(K_{1}\delta^{\prime}\) cover of set \(T(\epsilon)\) with \(k=1/K_{2}\). Finally,

\[N(K_{2}\delta^{\prime},T(\epsilon),d_{L})\leq(J+2)^{d}\leq\left(\frac{2k \epsilon}{\delta^{\prime}}+2\right)^{d}=\left(\frac{2\epsilon}{K_{2}\delta^{ \prime}}+2\right)^{d}\]

which implies for \(\delta=K_{2}\delta^{\prime}\),

\[N(\delta,T(\epsilon),d_{L})\leq\left(\frac{2\epsilon}{\delta}+2\right)^{s}.\]

Observe that the RHS in (50) is a decreasing function of \(\delta\), infact for \(\delta=\epsilon/2\), it is a constant in \(\epsilon\). Therefore, using Lemmas C.7 and C.8, we show in the following result that \(L_{n}(\theta,\theta_{0})\) in (6) satisfies Assumption 2.1.

**Lemma C.9**.: _Fix \(n\geq 1\). For a given \(\epsilon_{n}>0\) and every \(\epsilon>\epsilon_{n}\), such that \(n\epsilon_{n}^{2}\geq 1\). Under Assumption C.1 and C.2, \(L_{n}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)-R(a, \theta_{0})\right|\right)^{2}\) satisfies_

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}] \leq C_{0}\exp(-Cn\epsilon^{2}),\] (52) \[\sup_{\{\theta\in\Theta:L_{n}(\theta,\theta_{0})\geq C_{1}n \epsilon^{2}\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq\exp(-Cn\epsilon^{2}),\] (53)

_where \(C_{0}=2*10^{s}\) and \(C=\frac{C_{1}}{2K_{1}^{2}}\) for a constant \(C_{1}>0\)._

Proof of Lemma C.9:.: Recall \(d_{L}(\theta,\theta_{0})=\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)-R(a, \theta_{0})\right|\right)\) and \(T(\epsilon)=\{P_{\theta}:d_{L}(\theta,\theta_{0})<\epsilon\}\). Using Lemma C.8, observe that for every \(\epsilon>\epsilon_{n}>0\),

\[N\left(\frac{\epsilon}{2},\{\theta:\epsilon\leq d_{L}(\theta,\theta_{0})\leq 2 \epsilon\},d_{L}\right)\leq N\left(\frac{\epsilon}{2},\{\theta:d_{L}(\theta, \theta_{0})\leq 2\epsilon\},d_{L}\right)<10^{d}.\]

Next, using Assumption C.1 we have

\[d_{L}(\theta,\theta_{0})\leq K_{1}d_{H}(\theta,\theta_{0}).\]

It follows from the above two observations and Lemma 2 that, for every \(\epsilon>\epsilon_{n}>0\), there exist tests \(\{\phi_{n,\epsilon}\}\) such that

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n,\epsilon}] \leq 10^{d}\frac{\exp(-C^{\prime}n\epsilon^{2})}{1-\exp(-C^{\prime }n\epsilon^{2})},\] (54) \[\sup_{\{\theta\in\Theta:d_{L}(\theta,\theta_{0})\geq\epsilon\}} \mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n,\epsilon}] \leq\exp(-C^{\prime}n\epsilon^{2}),\] (55)

where \(C^{\prime}=\frac{1}{2K_{1}^{2}}\). Since the above two conditions hold for every \(\epsilon>\epsilon_{n}\), we can choose a constant \(K>0\) such that for every \(\epsilon>\epsilon_{n}\)

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n,\epsilon}] \leq 10^{d}\frac{\exp(-C^{\prime}K^{2}n\epsilon^{2})}{1-\exp(-C^{ \prime}K^{2}n\epsilon^{2})}\leq 2(10^{d})e^{-C^{\prime}K^{2}n\epsilon^{2}},\] (56) \[\sup_{\{\theta\in\Theta:L_{n}(\theta,\theta_{0})\geq K^{2}n \epsilon^{2}\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n,\epsilon}] =\sup_{\{\theta\in\Theta:d_{L}(\theta,\theta_{0})\geq K\epsilon\}} \mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n,\epsilon}]\leq e^{-C^{\prime}K^{2}n \epsilon^{2}},\] (57)

where the second inequality in (56) holds \(\forall n\geq n_{0}\), where \(n_{0}:=\min\{n\geq 1:C^{\prime}K^{2}n\epsilon^{2}\geq\log(2)\}\) Hence, the result follows for \(C_{1}=K^{2}\) and \(C=C^{\prime}K^{2}\). 

Since \(L_{n}(\theta,\theta_{0})=\frac{1}{n}d_{L}^{2}\) satisfies Assumption 2.1, Theorem 3.1 implies the following bound.

**Corollary C.1**.: _Fix \(a^{\prime}\in\mathcal{A}\) and \(\gamma>0\). Let \(\epsilon_{n}\) be a sequence such that \(\epsilon_{n}\to 0\) as \(n\to\infty\), \(n\epsilon_{n}^{2}\geq 1\) and_

\[L_{n}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)-R(a, \theta_{0})\right|\right)^{2}.\]

_Then under the Assumptions of Theorem 3.1 and Lemma C.9 ; for \(C=\frac{C_{1}}{2K_{1}^{2}}\), \(C_{0}=2*10^{s}\), \(C_{1}>0\) such that \(\min(C,C_{4}(\gamma)+C_{5}(\gamma))>C_{2}+C_{3}+C_{4}(\gamma)+2\), and for \(\eta_{n}^{n}(\gamma)\) as defined in Theorem 3.1, the RSVB approximator of the true posterior \(Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})\) satisfies,_

\[\mathbb{E}_{P_{0}^{n}}\left[\int_{\Theta}L_{n}(\theta,\theta_{0})Q_{a^{\prime},\gamma}^{*}(\theta|\tilde{X}_{n})d\theta\right]\leq n(M(\gamma)\epsilon_{n}^ {2}+M\eta_{n}^{R}(\gamma)),\] (58)

_for sufficiently large \(n\) and for a function \(M(\gamma)=2\left(C_{1}+MC_{4}(\gamma)\right)\), where \(M=\frac{2C_{1}}{\min(C,\lambda,1)}\)._

Proof of Corollary c.1:.: Using Lemma C.9 observe that for any \(\Theta_{n}(\epsilon)\subseteq\Theta\), \(L_{n}(\theta,\theta_{0})\) satisfies Assumption 2.1 with \(C_{0}=2*10^{s}\), \(C=\frac{C_{1}}{2K_{1}^{2}}\) and for any \(C_{1}>0\), since

\[\sup_{\{\theta\in\Theta_{n}(\epsilon):L_{n}(\theta,\theta_{0})\geq C_{1}n \epsilon_{n}^{2}\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n,\epsilon}] \leq\sup_{\{\theta\in\Theta:L_{n}(\theta,\theta_{0})\geq C_{1}n\epsilon_{n}^{2} \}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n,\epsilon}]\leq e^{-Cn\epsilon_{n}^{2}}.\]

Hence, applying Theorem 3.1 the proof follows.

### Newsvendor Problem

We fix \(n^{-1/2}\sqrt{L_{n}^{NV}(\theta,\theta_{0})}=(\sup_{a\in\mathcal{A}}|R(a,\theta)-R(a, \theta_{0})|)\). Next, we aim to show that the exponentially distributed model \(P_{\theta}\) satisfies Assumption 2.1, for distance function \(L_{n}^{NV}(\theta,\theta_{0})\). To show this, in the next result we first prove that \(d_{L}^{NV}(\theta,\theta_{0})=n^{-1/2}\sqrt{L_{n}^{NV}(\theta,\theta_{0})}\) satisfy Assumption C.1. Also, recall that the square of Hellinger distance between two exponential distributions with rate parameter \(\theta\) and \(\theta_{0}\) is \(d_{H}^{2}(\theta,\theta_{0})=1-2\frac{\sqrt{\theta_{0}}}{\theta+\theta_{0}}= 1-2\frac{\sqrt{\theta_{0}/\theta}}{1+\theta_{0}/\theta}\).

**Lemma C.10**.: _For any \(\theta\in\Theta=[T,\infty)\), and \(a\in\mathcal{A}\),_

\[d_{L}^{NV}(\theta,\theta_{0})\leq\left[\frac{\left(\frac{h}{ \theta_{0}}-\frac{h}{T}\right)^{2}+(b+h)^{2}\left(\frac{e^{-aT}}{T}-\frac{e^ {-a\theta_{0}}}{\theta_{0}}\right)^{2}}{d_{H}^{2}(T,\theta_{0})}\right]^{1/2 }d_{H}(\theta,\theta_{0})\]

_where \(\underline{a}:=\min\{a\in\mathcal{A}\}\) and \(\underline{a}>0\) and \(\theta_{0}\) lies in the interior of \(\Theta\)._

Proof.: Observe that for any \(a\in\mathcal{A}\),

\[|R(a,\theta)-R(a,\theta_{0})|^{2}\] \[= \left|\frac{h}{\theta_{0}}-\frac{h}{\theta}+(b+h)\left(\frac{e^{- a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)\right|^{2}\] \[=\left(\frac{h}{\theta_{0}}-\frac{h}{\theta}\right)^{2}+(b+h)^{2 }\left(\frac{e^{-a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right) ^{2}+2\left(\frac{h}{\theta_{0}}-\frac{h}{\theta}\right)(b+h)\left(\frac{e^{- a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)\] \[\leq\left(\frac{h}{\theta_{0}}-\frac{h}{\theta}\right)^{2}+(b+h) ^{2}\left(\frac{e^{-a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}} \right)^{2},\] (59)

where the last inequality follows since for \(\theta\geq\theta_{0}\), \(\left(\frac{h}{\theta_{0}}-\frac{h}{\theta}\right)\geq 0\) and \(\left(\frac{e^{-a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)<0\) and vice versa if \(\theta<\theta_{0}\) that together makes the last term in the penultimate equality negative for all \(\theta\in\Theta\). Moreover, the first derivative of the upperbound with respect to \(\theta\) is

\[2\left(\frac{h}{\theta_{0}}-\frac{h}{\theta}\right)\frac{h}{\theta^{2}}-2(b+h) ^{2}\left(\frac{e^{-a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}} \right)e^{-a\theta}\left[\frac{1}{\theta^{2}}+\frac{a}{\theta}\right],\]

and it is negative when \(\theta\leq\theta_{0}\) and positive when \(\theta>\theta_{0}\) for all \(b>0,h>0,\) and \(a\in\mathcal{A}\). Therefore, the upperbound in (59) above is decreasing function of \(\theta\) for all \(\theta\leq\theta_{0}\) and increasing function of \(\theta\) for all \(\theta>\theta_{0}\). The upperbound is tight at \(\theta=\theta_{0}\).

Now recall that the squared Hellinger distance between two exponential distributions with rate parameter \(\theta\) and \(\theta_{0}\) is

\[d_{H}^{2}(\theta,\theta_{0})=1-2\frac{\sqrt{\theta\theta_{0}}}{\theta+\theta_{ 0}}=1-2\frac{\sqrt{\theta_{0}/\theta}}{1+\theta_{0}/\theta}=\frac{(1-\sqrt{ \theta_{0}/\theta})^{2}}{1+(\sqrt{\theta_{0}/\theta})^{2}}.\]

Note that for \(\theta\leq\theta_{0}\), \(d_{H}^{2}(\theta,\theta_{0})\) is a decreasing function of \(\theta\) and for all \(\theta>\theta_{0}\) it is an increasing function of \(\theta\). Also, note that as \(\theta\to\infty\), the squared Hellinger distance as well as the upperbound computed in (59) converges to a constant for a given \(h,b,\theta_{0}\) and \(a\). However, as \(\theta\to 0\), the \(d_{H}^{2}(\theta,\theta_{0})\to 1\) but the upperbound computed in (59) diverges.

Since, \(\Theta=[T,\infty)\) for some \(T>0\) and \(T\leq\theta_{0}\), observe that if we scale \(d_{H}^{2}(\theta,\theta_{0})\) by factor by which the upperbound computed in (59) is greater than \(d_{H}\) at \(\theta=T\), then

\[\left(\frac{h}{\theta_{0}}-\frac{h}{\theta}\right)^{2}+(b+h)^{2} \left(\frac{e^{-a\theta}}{\theta}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)^{2}\] \[\leq\frac{\left(\frac{h}{\theta_{0}}-\frac{h}{T}\right)^{2}+(b+h) ^{2}\left(\frac{e^{-aT}}{T}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)^{2}}{d_ {H}^{2}(T,\theta_{0})}d_{H}^{2}(\theta,\theta_{0})\] \[\leq\frac{\left(\frac{h}{\theta_{0}}-\frac{h}{T}\right)^{2}+(b+h) ^{2}\left(\frac{e^{-aT}}{T}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)^{2}}{d_ {H}^{2}(T,\theta_{0})}d_{H}^{2}(\theta,\theta_{0}),\]where \(\underline{a}=\inf\{a:a\in\mathcal{A}\}\) and in the last inequality we used the fact that \(\left(\frac{e^{-aT}}{T}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)^{2}\) is a decreasing function of \(a\) for any \(b,h,T,\) and \(\theta_{0}\). Since, the RHS in the equation above does not depend on \(a\), it follows from the result in (59) and the definition of \(L_{n}^{NV}(\theta,\theta_{0})\) that

\[d_{L}^{NV}(\theta,\theta_{0})\leq\left[\frac{\left(\frac{h}{\theta_{0}}-\frac{ h}{T}\right)^{2}+(b+h)^{2}\left(\frac{e^{-aT}}{T}-\frac{e^{-a\theta_{0}}}{ \theta_{0}}\right)^{2}}{d_{H}^{2}(T,\theta_{0})}\right]^{1/2}d_{H}(\theta, \theta_{0}).\]

**Lemma C.11**.: _For any \(\theta\in\Theta=[T,\infty)\), for sufficiently small \(T>0\), and \(\theta_{0}\) lying in the interior of \(\Theta\), we have_

\[d_{H}^{2}(\theta,\theta_{0})=1-2\frac{\sqrt{\theta\theta_{0}}}{\theta+\theta_ {0}}\leq\left(\frac{\theta_{0}}{(T+\theta_{0})^{2}}\left(\sqrt{\frac{\theta_{ 0}}{T}}-\sqrt{\frac{T}{\theta_{0}}}\right)\right)|\theta-\theta_{0}|.\]

Proof.: Observe that

\[\frac{\partial d_{H}^{2}(\theta,\theta_{0})}{\partial\theta}=-2\frac{(\theta+ \theta_{0})\frac{\sqrt{\theta_{0}}}{2\sqrt{\theta}}-\sqrt{\theta\theta_{0}}}{ (\theta+\theta_{0})^{2}}=\frac{\theta_{0}}{(\theta+\theta_{0})^{2}}\left( \sqrt{\frac{\theta}{\theta_{0}}}-\sqrt{\frac{\theta_{0}}{\theta}}\right).\]

Observe that \(\theta\to 0\), \(\frac{\partial d_{H}^{2}(\theta,\theta_{0})}{\partial\theta}\rightarrow\infty\). Since,\(\theta\in\Theta=[T,\infty)\), therefore the \(\sup_{\theta\in\Theta}\left|\frac{\partial d_{H}^{2}(\theta,\theta_{0})}{\partial \theta}\right|<\infty\). In fact, for sufficiently small \(T>0\), \(\sup_{\theta\in\Theta}\left|\frac{\partial d_{H}^{2}(\theta,\theta_{0})}{ \partial\theta}\right|=\left|\frac{\theta_{0}}{(T+\theta_{0})^{2}}\left( \sqrt{\frac{T}{\theta_{0}}}-\sqrt{\frac{\theta_{0}}{T}}\right)\right|=\left( \frac{\theta_{0}}{(T+\theta_{0})^{2}}\left(\sqrt{\frac{\theta_{0}}{T}}-\sqrt{ \frac{T}{\theta_{0}}}\right)\right)\). Now the result follows immediately since the derivative of \(d_{H}^{2}(\theta,\theta_{0})\) is bounded on \(\Theta\), which implies that \(d_{H}^{2}(\theta,\theta_{0})\) is Lipschitz on \(\Theta\). 

**Lemma C.12**.: _For any \(\theta\in\Theta=[T,\infty)\), and \(a\in\mathcal{A}\),_

\[d_{L}^{NV}(\theta,\theta_{0})\leq\frac{h}{T^{2}}|\theta-\theta_{0}|.\]

Proof.: Recall,

\[R(a,\theta)=ha-\frac{h}{\theta}+(b+h)\frac{e^{-a\theta}}{\theta}.\]

First, observe that for any \(a\in\mathcal{A}\),

\[\frac{\partial R(a,\theta)}{\partial\theta}=\frac{h}{\theta^{2}}-a(b+h)\frac {e^{-a\theta}}{\theta}-(b+h)\frac{e^{-a\theta}}{\theta^{2}}=\frac{1}{\theta^{2 }}\left(h-(b+h)e^{-a\theta}(1+a\theta)\right)\leq\frac{h}{\theta^{2}}.\] (60)

The result follows immediately, since \(\sup_{\theta\in\Theta}\frac{\partial R(a,\theta)}{\partial\theta}\leq\frac{h} {T^{2}}\). 

Proof.: Proof of Lemma B.1

It follows from Lemma C.10 that \(d_{L}^{NV}(\theta,\theta_{0})\) for any \(\theta\in\Theta=[T,\infty)\) and \(\theta_{0}\) lying the interior of \(\Theta\), satisfies Assumption C.1 with

\[K_{1}=\left[\frac{\left(\frac{h}{\theta_{0}}-\frac{h}{T}\right)^{2}+(b+h)^{2} \left(\frac{e^{-aT}}{T}-\frac{e^{-a\theta_{0}}}{\theta_{0}}\right)^{2}}{d_{H}^ {2}(T,\theta_{0})}\right]^{1/2}:=K_{1}^{NV}\]

. Similarly, it follows from Lemma and C.12 that for sufficiently small \(T>0\), \(d_{L}^{NV}(\theta,\theta_{0})\) satisfies Assumption C.2 with \(K_{2}=h/T^{2}:=K_{2}^{NV}\). Now using similar arguments as used in Lemma C.8 and Lemma 2.1, for a given \(\epsilon_{n}>0\) and every \(\epsilon>\epsilon_{n}\), such that \(n\epsilon_{n}^{2}\geq 1\), it can be shown that, \(L_{n}^{NV}(\theta,\theta_{0})=n\left(\sup_{a\in\mathcal{A}}\left|R(a,\theta)-R( a,\theta_{0})\right|\right)^{2}\) satisfies\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}] \leq C_{0}\exp(-Cn\epsilon^{2}),\] (61) \[\sup_{\{\theta\in\Theta:L_{N}^{N}\left(\theta,\theta_{0}\right)\geq C _{1}ne^{2}\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq\exp(-Cn\epsilon^{2}),\] (62)

where \(C_{0}=20\) and \(C=\frac{C_{1}}{2(K_{1}^{N})^{2}}\) for a constant \(C_{1}>0\). 

Proof.: Proof of Lemma B.2:

First, we write the Renyi divergence between \(P_{0}^{n}\) and \(P_{\theta}^{n}\),

\[D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)=\frac{1}{ \lambda}\log\int\left(\frac{dP_{0}^{n}}{dP_{\theta}^{n}}\right)^{\lambda}dP _{0}^{n} =n\frac{1}{\lambda}\log\int\left(\frac{dP_{0}}{dP_{\theta}} \right)^{\lambda}dP_{0}\] \[=n\left(\log\frac{\theta_{0}}{\theta}+\frac{1}{\lambda}\log \frac{\theta_{0}}{(\lambda+1)\theta_{0}-\lambda\theta}\right),\]

when \(\left((\lambda+1)\theta_{0}-\lambda\theta\right)>0\) and \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)=\infty\) otherwise. Also, observe that, \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\) is non-decreasing in \(\lambda\) (this also follows from non-decreasing property of the Renyi divergence with respect to \(\lambda\)). Therefore, observe that

\[\Pi(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\leq C_{3 }n\epsilon_{n}^{2})\geq\Pi(D_{\infty}\left(P_{0}^{n}\|P_{\theta}^{n}\right) \leq C_{3}n\epsilon_{n}^{2}) =\Pi\left(0\leq\log\frac{\theta_{0}}{\theta}\leq C_{3}\epsilon_{n }^{2}\right)\] \[=\Pi\left(\theta_{0}e^{-C_{3}\epsilon_{n}^{2}}\leq\theta\leq \theta_{0}\right).\]

Now, recall that for a set \(A\subseteq\Theta=[T,\infty)\), we define \(\Pi(A)=\text{Inv}-\Gamma(A\cap\Theta)/\text{Inv}-\Gamma(\Theta)\). Now, observe that for sufficiently small \(T\) and large enough \(n\), we have

\[\Pi\left(\theta_{0}e^{-C_{3}\epsilon_{n}^{2}}\leq\theta\leq\theta_{0}\right) \geq\text{Inv}-\Gamma\left(\theta_{0}e^{-C_{3}\epsilon_{n}^{2}}\leq\theta\leq \theta_{0}\right)\]

The cumulative distribution function of inverse-gamma distribution is \(\text{Inv}-\Gamma(\{\theta\in\Theta:\theta<t\}):=\frac{\Gamma\left(\alpha, \frac{\beta}{\theta}\right)}{\Gamma(\alpha)}\), where \(\alpha(>0)\) is the shape parameter, \(\beta(>0)\) is the scale parameter, \(\Gamma(\cdot)\) is the Gamma function, and \(\Gamma(\cdot,\cdot)\) is the incomplete Gamma function. Therefore, it follows for \(\alpha>1\) that

\[\text{Inv}-\Gamma\left(\theta_{0}e^{-C_{3}\epsilon_{n}^{2}}\leq \theta\leq\theta_{0}\right)\] \[\qquad=\frac{\Gamma\left(\alpha,\beta/\theta_{0}\right)-\Gamma \left(\alpha,\beta/\theta_{0}e^{C_{3}\epsilon_{n}^{2}}\right)}{\Gamma(\alpha) }=\frac{\int_{\beta/\theta_{0}}^{\beta/\theta_{0}}e^{C_{3}\epsilon_{n}^{2}}e^ {-x}x^{\alpha-1}dx}{\Gamma(\alpha)}\] \[\qquad\geq\frac{e^{-\beta/\theta_{0}}e^{C_{3}\epsilon_{n}^{2}}+ \alpha C_{3}\epsilon_{n}^{2}}{\alpha\Gamma(\alpha)}\left(\frac{\beta}{\theta_{ 0}}\right)^{\alpha}\left[1-e^{-\alpha C_{3}\epsilon_{n}^{2}}\right]\] \[\qquad\geq\frac{e^{-\beta/\theta_{0}}e^{C_{3}}}{\alpha\Gamma( \alpha)}\left(\frac{\beta}{\theta_{0}}\right)^{\alpha}\left[e^{-\alpha C_{3} \epsilon_{n}^{2}}\right]\]

where the penultimate inequality follows since \(0<\epsilon_{n}^{2}<1\) and the last inequality follows from the fact that, \(1-e^{-\alpha C_{3}\epsilon_{n}^{2}}\geq e^{-\alpha C_{3}ne_{n}^{2}}\), for large enough \(n\). Also note that, \(1-e^{-\alpha C_{3}\epsilon_{n}^{2}}\geq e^{-\alpha C_{3}ne_{n}^{2}}\) can't hold true for \(\epsilon_{n}^{2}=1/n\). However, for \(\epsilon_{n}^{2}=\frac{\log n}{n}\) it holds for any \(n\geq 2\) when \(\alpha C_{3}>2\). Therefore, for inverse-Gamma prior restricted to \(\Theta\), \(C_{2}=\alpha C_{3}\) and any \(\lambda>1\) the result follows for sufficiently large \(n\).

Proof.: Proof of Lemma B.3: Recall,

\[R(a,\theta)=ha-\frac{h}{\theta}+(b+h)\frac{e^{-a\theta}}{\theta}.\]

First, observe that for any \(a\in\mathcal{A}\),

\[\frac{\partial R(a,\theta)}{\partial\theta}=\frac{h}{\theta^{2}}-a(b+h)\frac{e^ {-a\theta}}{\theta}-(b+h)\frac{e^{-a\theta}}{\theta^{2}}=\frac{1}{\theta^{2}} \left(h-(b+h)e^{-a\theta}(1+a\theta)\right).\] (63)Using the above equation the (finite) critical point \(\theta^{*}\) must satisfy, \(h-(b+h)e^{-a\theta^{*}}(1+a\theta^{*})=0\). Therefore,

\[R(a,\theta)\geq R(a,\theta^{*})=h\left(a-\frac{1}{\theta^{*}}+\frac{1}{\theta^{ *}(1+a\theta^{*})}\right)=\frac{ha^{2}\theta^{*}}{(1+a\theta^{*})}.\]

Since \(h,b>0\) and \(a\theta^{*}>0\), hence

\[R(a,\theta)\geq\frac{ha^{2}\theta^{*}}{(1+a\theta^{*})},\]

where \(\underline{a}:=\min\{a\in\mathcal{A}\}\) and \(\underline{a}>0\).

Proof.: Proof of Lemma B.4:

First, observe that \(R(a,\theta)\) is bounded above in \(\theta\) for a given \(a\in\mathcal{A}\)

\[R(a,\theta) =ha-\frac{h}{\theta}+(b+h)\frac{e^{-a\theta}}{\theta}\] \[\leq ha+\frac{b}{\theta}.\]

Using the above fact and the Cauchy-Schwarz inequality, we obtain

\[\int_{\left\{e^{\gamma R(a,\theta)}>e^{C_{4}(\gamma)na_{n}^{2}} \right\}}e^{\gamma R(a,\theta)}\pi(\theta)d\theta\] \[\leq\left(\int e^{2\gamma R(a,\theta)}\pi(\theta)d\theta\right)^ {1/2}\left(\int\mathbbm{1}_{e^{\gamma R(a,\theta)}>e^{C_{4}(\gamma)na_{n}^{2}} }\pi(\theta)d\theta\right)^{1/2}\] \[\leq e^{-C_{4}(\gamma)na_{n}^{2}}\left(\int e^{2\gamma\left(ha+ \frac{b}{\theta}\right)}\pi(\theta)d\theta\right),\] (64)

where the last inequality follows from using the Chebyshev's inequality.

Now using the definition of the prior distribution, which is an inverse gamma prior restricted to \(\Theta=[T,\infty)\), we have

\[\int_{\left\{e^{\gamma R(a,\theta)}>e^{C_{4}(\gamma)na_{n}^{2}} \right\}}e^{\gamma R(a,\theta)}\pi(\theta)d\theta \leq e^{-C_{4}(\gamma)na_{n}^{2}}\left(\int e^{2\gamma\left(ha+ \frac{b}{\theta}\right)}\pi(\theta)d\theta\right)\] \[\leq e^{-C_{4}(\gamma)na_{n}^{2}}e^{2\gamma\left(ha+\frac{b}{ \theta}\right)},\]

where \(\overline{a}:=\max\{a\in\mathcal{A}\}\) and \(\overline{a}>0\). Since \(ne_{n}^{2}\geq 1\), we must fix \(C_{4}(\gamma)\) such that \(e^{C_{4}(\gamma)}>e^{2\gamma\left(ha+\frac{b}{\theta}\right)}\), that is \(C_{4}(\gamma)>2\gamma\left(ha+\frac{b}{T}\right)\) and \(C_{5}(\gamma)=C_{4}(\gamma)-2\gamma\left(ha+\frac{b}{T}\right)\).

Proof.: Proof of Lemma B.5: Since family \(\mathcal{Q}\) contains all shifted-gamma distributions, observe that \(\{q_{n}(\cdot)\in\mathcal{Q}\}\forall n\geq 1\). By definition, \(q_{n}(\theta)=\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}(\theta-T)^{n-1}e^{-n\frac {(\theta-T)}{\theta_{0}}}\). Now consider the first term; using the definition of the KL divergence it follows that

\[\text{KL}(q_{n}(\theta)\|\pi(\theta))=\int_{T}^{\infty}q_{n}(\theta)\log(q_{ n}(\theta))d\theta-\int_{T}^{\infty}q_{n}(\theta)\log(\pi(\theta))d\theta.\] (65)

Substituting \(q_{n}(\theta)\) in the first term of the equation above and expanding the logarithm term, we obtain

\[\int_{T}^{\infty}q_{n}(\theta)\log(q_{n}(\theta))d\theta\] \[=(n-1)\int_{T}^{\infty}\log(\theta-T)\frac{n^{n}}{\theta_{0}^{n} \Gamma(n)}(\theta-T)^{n-1}e^{-n\frac{\theta-T}{\theta_{0}}}d\theta-n+\log \left(\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}\right)\] \[=-\log\theta_{0}+(n-1)\int_{T}^{\infty}\log\frac{-T}{\theta_{0}} \frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}(\theta-T)^{n-1}e^{-n\frac{\theta-T}{ \theta_{0}}}d\theta-n+\log\left(\frac{n^{n}}{\Gamma(n)}\right)\] (66)Now consider the second term in the equation above. Substitute \(\theta=\frac{t\theta_{0}}{n}+T\) into the integral, we have

\[\int_{T}^{\infty}\log\frac{\theta-T}{\theta_{0}}\frac{n^{n}}{\theta _{0}^{n}\Gamma(n)}(\theta-T)^{n-1}e^{-n\frac{\theta-T}{\theta_{0}}}d\theta =\int_{0}^{\infty}\log\frac{t}{n}\frac{1}{\Gamma(n)}t^{n-1}e^{-t}dt\] \[\leq\int\left(\frac{t}{n}-1\right)\frac{1}{\Gamma(n)}t^{n-1}e^{- t}dt=0.\] (67)

Substituting the above result into (66), we get

\[\int_{T}^{\infty}q_{n}(\theta)\log(q_{n}(\theta))d\theta \leq-\log\theta_{0}-n+\log\left(\frac{n^{n}}{\Gamma(n)}\right)\] \[\leq-\log\theta_{0}-n+\log\left(\frac{n^{n}}{\sqrt{2\pi nn^{n-1}e ^{-n}}}\right)\] \[=-\log\sqrt{2\pi}\theta_{0}+\frac{1}{2}\log n,\] (68)

where the second inequality uses the fact that \(\sqrt{2\pi nn^{n}e^{-n}}\leq n\Gamma(n)\). Recall \(\pi(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{-\alpha-1}e^{-\frac {\beta}{\theta}}\). Now consider the second term in (65). Using the definition of inverse-gamma prior and expanding the logarithm function, we have

\[-\int_{T}^{\infty}q_{n}(\theta)\log(\pi(\theta))d\theta\] \[\quad=-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+( \alpha+1)\int_{T}^{\infty}\log\theta\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}( \theta-T)^{n-1}e^{-n\frac{\theta-T}{\theta_{0}}}d\theta+\beta\frac{n}{(n-1) \theta_{0}}\] \[\quad=-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+ \int_{T}^{\infty}\log\frac{\theta}{\theta_{0}}\frac{n^{n}}{\theta_{0}^{n} \Gamma(n)}(\theta-T)^{n-1}e^{-n\frac{\theta-T}{\theta_{0}}}d\theta\] \[\quad\quad+\beta\frac{n}{(n-1)\theta_{0}}+(\alpha+1)\log\theta_{0}\] \[\quad\leq-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+ \int_{T}^{\infty}\frac{\theta-T}{\theta_{0}}\frac{n^{n}}{\theta_{0}^{n}\Gamma( n)}(\theta-T)^{n-1}e^{-n\frac{\theta-T}{\theta_{0}}}d\theta\] \[\quad\quad+\beta\frac{n}{(n-1)\theta_{0}}+(\alpha+1)\log\theta_{0}\] \[=-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\beta \frac{n}{(n-1)\theta_{0}}+(\alpha+1)\log\theta_{0},\] (69)

where the first inequality is due to fact that \(\mathbb{E}_{q_{n}}[\beta/\theta]\leq\mathbb{E}_{q_{n}}[\beta/(\theta-T)]\) for any \(\theta>T\) and the penultimate inequality follows from the observation in (67) and the fact that \(\log\frac{\theta}{\theta_{0}}\leq\frac{\theta}{\theta_{0}}-1\leq\frac{\theta} {\theta_{0}}-\frac{T}{\theta_{0}}\) for any \(\theta_{0}>T\). Substituting (69) and (68) into (65) and dividing either sides by \(n\), we obtain

\[\frac{1}{n} \text{KL}(q_{n}(\theta)\|\pi(\theta))\] \[\leq\frac{1}{n}\left(-\log\sqrt{2\pi}\theta_{0}+\frac{1}{2}\log n -\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\beta\frac{n}{(n-1) \theta_{0}}+(\alpha+1)\log\theta_{0}\right)\] \[=\frac{1}{2}\frac{\log n}{n}+\beta\frac{1}{(n-1)\theta_{0}}+\frac {1}{n}\left(-\log\sqrt{2\pi}-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)} \right)+(\alpha)\log\theta_{0}\right).\] (70)

Now consider the second term in the assertion of the lemma. Since \(\xi_{i},i\in\{1,2\ldots n\}\) are independent and identically distributed, we obtain

\[\frac{1}{n}\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}\|p(\tilde {X}_{n}|\theta)\right)\right] =\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}\|p(\xi| \theta)\right)\right]\] \[\leq\frac{n}{n-1}+1-2=\frac{1}{n-1},\] (71)where second inequality uses the fact that \(\log x\leq x-1\leq x-\frac{T}{\theta_{0}}\) for \(\theta_{0}>T\). Combined together (71) and (70) for \(n\geq 2\) implies that

\[\frac{1}{n}\left[\text{KL}\left(q_{n}(\theta)\|\pi(\theta)\right)+ \mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}\right)\|p(\tilde{X}_{ n}|\theta)\right)\right]\] \[\quad\leq\frac{1}{2}\frac{\log n}{n}+\frac{1}{n}\left(2+\frac{2 \beta}{\theta_{0}}-\log\sqrt{2\pi}-\log\left(\frac{\beta^{\alpha}}{\Gamma( \alpha)}\right)+\alpha\log\theta_{0}\right)\leq C_{9}\frac{\log n}{n}.\] (72)

where \(C_{9}:=\frac{1}{2}+\max\left(0,2+\frac{2\beta}{\theta_{0}}-\log\sqrt{2\pi}- \log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\alpha\log\theta_{0}\right)\) and the result follows. 

Proof.: Proof of Lemma B.5: Since family \(\mathcal{Q}\) contains all gamma distributions, observe that \(\{q_{n}(\cdot)\in\mathcal{Q}\}\forall n\geq 1\). By definition, \(q_{n}(\theta)=\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}\theta^{n-1}e^{-n\frac{ \theta}{\theta_{0}}}\). Now consider the first term; using the definition of the KL divergence it follows that

\[\text{KL}(q_{n}(\theta)\|\pi(\theta))=\int q_{n}(\theta)\log(q_{n}(\theta))d \theta-\int q_{n}(\theta)\log(\pi(\theta))d\theta.\] (73)

Substituting \(q_{n}(\theta)\) in the first term of the equation above and expanding the logarithm term, we obtain

\[\int q_{n}(\theta)\log(q_{n}(\theta))d\theta=(n-1)\int\log\theta \frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}\theta^{n-1}e^{-n\frac{\theta}{\theta_{0} }}d\theta-n+\log\left(\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}\right)\] \[=-\log\theta_{0}+(n-1)\int\log\frac{\theta}{\theta_{0}}\frac{n^{ n}}{\theta_{0}^{n}\Gamma(n)}\theta^{n-1}e^{-n\frac{\theta}{\theta_{0}}}d \theta-n+\log\left(\frac{n^{n}}{\Gamma(n)}\right)\] (74)

Now consider the second term in the equation above. Substitute \(\theta=\frac{t\theta_{0}}{n}\) into the integral, we have

\[\int\log\frac{\theta}{\theta_{0}}\frac{n^{n}}{\theta_{0}^{n} \Gamma(n)}\theta^{n-1}e^{-n\frac{\theta}{\theta_{0}}}d\theta =\int\log\frac{t}{n}\frac{1}{\Gamma(n)}t^{n-1}e^{-t}dt\] \[\leq\int\left(\frac{t}{n}-1\right)\frac{1}{\Gamma(n)}t^{n-1}e^{-t }dt=0.\] (75)

Substituting the above result into (74), we get

\[\int q_{n}(\theta)\log(q_{n}(\theta))d\theta \leq-\log\theta_{0}-n+\log\left(\frac{n^{n}}{\Gamma(n)}\right)\] \[\leq-\log\theta_{0}-n+\log\left(\frac{n^{n}}{\sqrt{2\pi n}n^{n-1} e^{-n}}\right)\] \[=-\log\sqrt{2\pi}\theta_{0}+\frac{1}{2}\log n,\] (76)

where the second inequality uses the fact that \(\sqrt{2\pi n}n^{n}e^{-n}\leq n\Gamma(n)\). Recall \(\pi(\theta)=\frac{\beta^{\alpha}}{\Gamma(\alpha)}\theta^{-\alpha-1}e^{-\frac {\beta}{\beta}}\). Now consider the second term in (73). Using the definition of inverse-gamma prior and expanding the logarithm function, we have

\[-\int q_{n}(\theta)\log(\pi(\theta))d\theta\] \[\quad=-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+( \alpha+1)\int\log\theta\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)}\theta^{n-1}e^{-n \frac{\theta}{\theta_{0}}}d\theta+\beta\frac{n}{(n-1)\theta_{0}}\] \[\quad=-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+( \alpha+1)\int\log\frac{\theta}{\theta_{0}}\frac{n^{n}}{\theta_{0}^{n}\Gamma(n )}\theta^{n-1}e^{-n\frac{\theta}{\theta_{0}}}d\theta\] \[\quad\quad+\beta\frac{n}{(n-1)\theta_{0}}+(\alpha+1)\log\theta_{0}\] \[\leq-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\beta \frac{n}{(n-1)\theta_{0}}+(\alpha+1)\log\theta_{0},\] (77)where the last inequality follows from the observation in (75). Substituting (77) and (76) into (73) and dividing either sides by \(n\), we obtain

\[\frac{1}{n}\text{KL}(q_{n}(\theta)\|\pi(\theta))\] \[\quad\leq\frac{1}{n}\left(-\log\sqrt{2\pi}\theta_{0}+\frac{1}{2} \log n-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\beta\frac{n}{(n -1)\theta_{0}}+(\alpha+1)\log\theta_{0}\right)\] \[= \frac{1}{2}\frac{\log n}{n}+\beta\frac{1}{(n-1)\theta_{0}}+\frac{ 1}{n}\left(-\log\sqrt{2\pi}-\log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)} \right)+(\alpha)\log\theta_{0}\right).\] (78)

Now, consider the second term in the assertion of the lemma. Since, \(\xi_{i},i\in\{1,2\ldots n\}\) are independent and identically distributed, we obtain

\[\frac{1}{n}\mathbb{E}_{q(\theta)}\left[\text{KL}\left(dP_{0}^{n}\|p(\tilde{X}_ {n}|\theta)\right)\right]=\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{ 0}\|p(\xi|\theta)\right)\right]\]

Now using the expression for KL divergence between the two exponential distributions, we have

\[\frac{1}{n}\mathbb{E}_{q(\theta)}\left[\text{KL}\left(dP_{0}^{n} \|p(\tilde{X}_{n}|\theta)\right)\right]=\int\left(\log\frac{\theta_{0}}{\theta }+\frac{\theta}{\theta_{0}}-1\right)\frac{n^{n}}{\theta_{0}^{n}\Gamma(n)} \theta^{n-1}e^{-n\frac{\theta}{\theta_{0}}}d\theta\] \[\quad\leq\frac{n}{n-1}+1-2=\frac{1}{n-1},\] (79)

where second inequality uses the fact that \(\log x\leq x-1\). Combined together (79) and (78) for \(n\geq 2\) implies that

\[\frac{1}{n}\left[\text{KL}\left(q(\theta)\|\pi(\theta)\right)+ \mathbb{E}_{q(\theta)}\left[\text{KL}\left(dP_{0}^{n}\right)\|p(\tilde{X}_{n} |\theta)\right)\right]\] \[\quad\leq\frac{1}{2}\frac{\log n}{n}+\frac{1}{n}\left(2+\frac{2 \beta}{\theta_{0}}-\log\sqrt{2\pi}-\log\left(\frac{\beta^{\alpha}}{\Gamma( \alpha)}\right)+\alpha\log\theta_{0}\right)\leq C_{9}\frac{\log n}{n}.\] (80)

where \(C_{9}:=\frac{1}{2}+\max\left(0,2+\frac{2\beta}{\theta_{0}}-\log\sqrt{2\pi}- \log\left(\frac{\beta^{\alpha}}{\Gamma(\alpha)}\right)+\alpha\log\theta_{0}\right)\) and the result follows. 

### Multi-product Newsvendor problem

In the multi-dimensional newsvendor problem, we fix \(n^{-1/2}\sqrt{L_{n}^{MNV}(\theta,\theta_{0})}=(\sup_{a\in\mathcal{A}}|R(a, \theta)-R(a,\theta_{0})|)\), where \(R(a,\theta)=\sum_{i=1}^{d}\left[(h_{i}+b_{i})a_{i}\Phi(a_{i})-b_{i}a_{i}+ \theta_{i}(b_{i}-h_{i})\right.\)\(\left.+\sigma_{ii}\left[h\frac{\phi((a_{i}-\theta_{i})/\sigma_{ii})}{\Phi((a_{i}- \theta_{i})/\sigma_{ii})}+b\frac{\phi((a_{i}-\theta_{i})/\sigma_{ii})}{1-\Phi ((a_{i}-\theta_{i})/\sigma_{ii})}\right]\right]\).

For brevity, we denote \(d_{L}^{MNV}(\theta,\theta_{0})=n^{-1/2}\sqrt{L_{n}^{MNV}(\theta,\theta_{0})}.\) First, we show that

**Lemma C.13**.: _For any compact decision space \(\mathcal{A}\) and compact model space \(\Theta\),_

\[d_{L}^{MNV}(\theta,\theta_{0})\leq K\|\theta-\theta_{0}\|,\]

_for a constant \(K\) depending on compact sets \(\mathcal{A}\) and \(\Theta\) and given \(b,h\) and \(\Sigma\)._

Proof.: Observe that

\[\partial_{\theta_{i}}R(a,\theta)\] \[=(b_{i}-h_{i})+(a_{i}-\theta_{i})/\sigma_{ii}\phi((a_{i}-\theta_ {i})/\sigma_{ii})\left[\frac{h}{\Phi((a_{i}-\theta_{i})/\sigma_{ii})}+\frac{b} {1-\Phi((a_{i}-\theta_{i})/\sigma_{ii})}\right]\] \[+\sigma_{ii}\phi\left(\frac{(a_{i}-\theta_{i})}{\sigma_{ii}} \right)\left[\frac{h\phi((a_{i}-\theta_{i})/\sigma_{ii})}{\sigma_{ii}\Phi((a_{i} -\theta_{i})/\sigma_{ii})^{2}}-\frac{b\phi((a_{i}-\theta_{i})/\sigma_{ii})}{ \sigma_{ii}(1-\Phi((a_{i}-\theta_{i})/\sigma_{ii}))^{2}}\right]\] \[=(b_{i}-h_{i})+(a_{i}-\theta_{i})/\sigma_{ii}\phi((a_{i}-\theta_ {i})/\sigma_{ii})\left[\frac{h}{\Phi((a_{i}-\theta_{i})/\sigma_{ii})}+\frac{b} {1-\Phi((a_{i}-\theta_{i})/\sigma_{ii})}\right]\] \[+\phi\left(\frac{(a_{i}-\theta_{i})}{\sigma_{ii}}\right)\left[ \frac{h\phi((a_{i}-\theta_{i})/\sigma_{ii})}{\Phi((a_{i}-\theta_{i})/\sigma_{ ii})^{2}}-\frac{b\phi((a_{i}-\theta_{i})/\sigma_{ii})}{(1-\Phi((a_{i}-\theta_{i})/ \sigma_{ii}))^{2}}\right].\] (81)Since, \(\mathcal{A}\) and \(\Theta\) are compact sets, therefore \(\{(a_{i}-\theta_{i})/\sigma_{ii}\}_{i=1}^{d}\) lie in a compact set. Consequently, \(\phi((a_{i}-\theta_{i})/\sigma_{ii})\) and \(\Phi((a_{i}-\theta_{i})/\sigma_{ii})\) also lie in bounded subset of \(\mathbb{R}\) and thus \(\sup_{\mathcal{A},\Theta}\|\partial_{\theta_{i}}R(a,\theta)\|\leq K\) for a given \(b\), \(h\) and \(\Sigma\). Since, the norm of the derivative of \(R(a,\theta)\) is bounded on \(\Theta\) for any \(a\in\mathcal{A}\), therefore, \(d_{L}^{MNV}(\theta,\theta_{0})\) is uniformly Lipschitz in \(\mathcal{A}\) with Lipschitz constant \(K\), that is

\[d_{L}^{MNV}(\theta,\theta_{0})\leq K\|\theta-\theta_{0}\|.\]

Next, we show that the \(P_{\theta}\) satisfies Assumption 2.1, for distance function \(L_{n}^{MNV}(\theta,\theta_{0})\).

Proof.: Proof of Lemma B.6:

First consider the following test function, constructed using \(\tilde{X}_{n}=\{\xi_{1},\xi_{2},\ldots,\xi_{n}\}\).

\[\phi_{n,\epsilon}:=\mathbbm{1}_{\left\{\tilde{X}_{n}:\|\hat{\theta}_{n}- \theta_{0}\right\|>\sqrt{C\epsilon^{2}}\}},\]

where \(\hat{\theta}_{n}=\frac{\sum_{i=1}^{n}\xi_{i}}{n}\). Note that \(\hat{\theta}_{n}-\theta_{0}\sim\mathcal{N}(\cdot|0,\frac{1}{n}\Sigma)\), where \(\frac{1}{n}\Sigma\) is a symmetric positive definite matrix. Therefore it can be decomposed as \(\Sigma=Q^{T}\Lambda Q\), where \(Q\) is an orthogonal matrix and \(\Lambda\) is a diagonal matrix consisting of respective eigen values and consequently \(\hat{\theta}_{n}-\theta_{0}\sim Q\mathcal{N}(\cdot|0,\frac{1}{n}\Lambda)\). So, we have \(\|\hat{\theta}_{n}-\theta_{0}\|^{2}\sim\|\mathcal{N}(\cdot|0,\frac{1}{n} \Lambda)\|^{2}\). Notice that \(\|\mathcal{N}(\cdot|0,\frac{1}{n}\Lambda)\|^{2}\) is a linear combination of \(d\)\(\chi_{(1)}^{2}\) random variable weighted by elements of the diagonal matrix \(\frac{1}{n}\Lambda\). Using this observation, we first verify that \(\phi_{n,\epsilon}\) satisfies condition (_i_) of the Lemma. Observe that

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}]=P_{0}^{n}\left(\tilde{X}_{n}:\left\|\hat{ \theta}_{n}-\theta_{0}\right\|^{2}>C\epsilon^{2}\right)=P_{0}^{n}\left(\tilde {X}_{n}:\|\mathcal{N}(\cdot|0,\Lambda)\|^{2}>Cn\epsilon^{2}\right).\]

Note that \(\chi_{(1)}^{2}\) is \(\Gamma\) distributed with shape \(1/2\) and scale \(2\), which implies \(\chi_{(1)}^{2}-1\) is a sub-gamma random variable with scale factor \(2\) and variance factor \(2\). Now observe that for \(\hat{\Lambda}=\max_{i\in\{1,2,\ldots d\}}\Lambda_{ii}\),

\[P_{0}^{n}\left(\tilde{X}_{n}:\|\mathcal{N}(\cdot|0,\Lambda)\|^{2 }>Cn\epsilon^{2}\right) \leq P_{0}^{n}\left(\tilde{X}_{n}:\chi_{(1)}^{2}>\frac{1}{d\hat{ \Lambda}}Cn\epsilon^{2}\right)\] \[\leq P_{0}^{n}\left(\tilde{X}_{n}:\chi_{(1)}^{2}>\frac{1}{d\hat{ \Lambda}}Cn\epsilon^{2}\right)\] \[=P_{0}^{n}\left(\tilde{X}_{n}:\chi_{(1)}^{2}-1>\frac{1}{d\hat{ \Lambda}}Cn\epsilon^{2}-1\right)\] \[\leq e^{-\frac{\left(\frac{1}{d\hat{\Lambda}}Cn\epsilon^{2}-1 \right)^{2}}{2\left(2+2\left(\frac{1}{d\hat{\Lambda}}Cn\epsilon^{2}-1\right) \right)}}\] \[\leq e^{-1/8\frac{1}{d\hat{\Lambda}}Cn\epsilon^{2}+1/8}\leq e^{-1 /8\left(\frac{C}{d\hat{\Lambda}}-1\right)n\epsilon^{2}},\] (82)

where in the third inequality we used the well known tail bound for sub-gamma random variable (Lemma 3.12 [5]) assuming that \(C\) is sufficiently large such that \(\left(\frac{1}{d\hat{\Lambda}}Cn\epsilon^{2}-1\right)>1\) and in the last inequality follows from the assumption that \(n\epsilon^{2}>n\epsilon_{n}^{2}\geq 1\).

Now, we fix the alternate set to be \(\{\theta\in\mathbb{R}^{d}:\|\theta-\theta_{0}\|\geq 2\sqrt{C\epsilon^{2}}\}\). Next, we verify that \(\phi_{n,\epsilon}\) satisfies condition (_ii_) of the lemma. First, observe that

\[\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}]=P_{\theta}^{n}\left(\tilde{X}_{n}:\left\| \hat{\theta}_{n}-\theta_{0}\right\|^{2}\leq C\epsilon^{2}\right)\leq P_{\theta} ^{n}\left(\tilde{X}_{n}:\|\hat{\theta}_{n}-\theta\|\geq\|\theta-\theta_{0}\|- \sqrt{C\epsilon^{2}}\right),\] (83)

where in the last inequality, we used the fact that \(\|\theta-\theta_{0}\|\leq\|\hat{\theta}_{n}-\theta\|+\left\|\hat{\theta}_{n}- \theta_{0}\right\|\). Now on alternate set \(\{\theta\in\mathbb{R}^{d}:\|\theta-\theta_{0}\|\geq 2\sqrt{C\epsilon^{2}}\}\),

\[\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq P_{\theta}^{n}\left(\tilde{X}_{n}:\|\hat{\theta}_{n}-\theta \|\geq\|\theta-\theta_{0}\|-\sqrt{C\epsilon^{2}}\right)\] \[\leq P_{\theta}^{n}\left(\tilde{X}_{n}:\|\hat{\theta}_{n}-\theta \|\geq\|\theta-\theta_{0}\|-\sqrt{C\epsilon^{2}}\right)\] \[\leq P_{\theta}^{n}\left(\tilde{X}_{n}:\|\hat{\theta}_{n}-\theta \|\geq\sqrt{C\epsilon^{2}}\right).\] (84)Now, it follows from (82) and \(\Theta\subset\mathbb{R}^{d}\) that

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}] \leq e^{-1/8\left(\frac{C}{d\Lambda}-1\right)n\epsilon^{2}},\] \[\sup_{\left\{\theta\in\Theta:\|\theta-\theta_{0}\|\geq 2\sqrt{C \epsilon^{2}}\right\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq\sup_{\left\{\theta\in\mathbb{R}^{d}:\|\theta-\theta_{0}\|\geq 2\sqrt{C \epsilon^{2}}\right\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq e^{-1/8\left(\frac{C}{d\Lambda}-1\right)n\epsilon^{2}}.\]

Using Lemma C.13, \(\left\{\theta\in\Theta:n^{-1/2}\sqrt{L_{n}^{MNV}(\theta,\theta_{0})}\geq 2K \sqrt{C\epsilon^{2}}\right\}=\left\{\theta\in\Theta:d_{L}^{MNV}(\theta,\theta _{0})\geq 2K\sqrt{C\epsilon^{2}}\right\}\subseteq\left\{\theta\in\Theta:\| \theta-\theta_{0}\|\geq 2\sqrt{C\epsilon^{2}}\right\}\), which implies that

\[\sup_{\left\{\theta\in\Theta:L_{n}^{MNV}(\theta,\theta_{0})\geq 4K^{2}Cn\epsilon ^{2}}\right\}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq\sup_{\left\{\theta\in\Theta:\|\theta-\theta_{0}\|\geq 2\sqrt{C\epsilon^{2}} \right\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}].\]

Therefore, \(P_{\theta}\) for \(\theta\in\Theta\), satisifes Assumptions 2.1 for \(L_{n}(\theta,\theta_{0})=L_{n}^{MNV}(\theta,\theta_{0})\) for \(C_{0}=1\), \(C_{1}=4K^{2}C\) and \(C=1/8\left(\frac{C}{d\Lambda}-1\right)\). 

Proof.: Proof of Lemma B.7:

First, we write the Renyi divergence between two multivariate Gaussian distribution with known \(\Sigma\) as

\[D_{1+\lambda}(\mathcal{N}(\cdot|\theta_{0})\|\mathcal{N}(\cdot|\theta))=\frac {\lambda+1}{2}(\theta-\theta_{0})^{T}\Sigma(\theta-\theta_{0}),\] (85)

and \(D_{1+\lambda}(\mathcal{N}(\cdot|\theta)\|\mathcal{N}(\cdot|\theta_{0}))<\infty\) if and only if \(\Sigma^{-1}\) is positive definite [13].

Since, we assumed that the sequence of models are iid, therefore, \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)=\frac{1}{\lambda}\log \int\left(\frac{dP_{0}^{n}}{dP_{\theta}^{n}}\right)^{\lambda}dP_{0}^{n}=n \frac{1}{\lambda}\log\int\left(\frac{dP_{0}}{dP_{\theta}}\right)^{\lambda}dP_ {0}=n\left(\frac{\lambda+1}{2}(\theta-\theta_{0})^{T}\Sigma(\theta-\theta_{0} )\right),\) when \(\Sigma^{-1}\) is positive definite and \(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)=\infty\) otherwise. Now observe that

\[\Pi(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right) \leq nC_{3}\epsilon_{n}^{2})=\Pi\left(\left((\theta-\theta_{0})^ {T}\Sigma(\theta-\theta_{0})\right)\leq\frac{2}{\lambda+1}C_{3}\epsilon_{n}^{ 2}\right)\] \[=\Pi\left(\left([(\theta-\theta_{0})Q]^{T}\Lambda[Q(\theta-\theta_ {0})]\right)\leq\frac{2}{\lambda+1}C_{3}\epsilon_{n}^{2}\right)\] \[\geq\Pi\left(\left([(\theta-\theta_{0})Q]^{T}[Q(\theta-\theta_{0} )]\right)\leq\frac{2}{\hat{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}\right),\] \[=\Pi\left(\left([(\theta-\theta_{0})]^{T}[(\theta-\theta_{0})] \right)\leq\frac{2}{\hat{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}\right),\] (86)

where \(\hat{\Lambda}=\max_{i\in\{1,2,...d\}}\Lambda_{ii}\) and in the second equality we used eigen value decomposition of \(\Sigma=Q^{T}\Lambda Q\). Next, observe that,

\[\Pi(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\leq nC_{3 }\epsilon_{n}^{2}) =\Pi\left(\left([(\theta-\theta_{0})]^{T}[(\theta-\theta_{0})] \right)\leq\frac{2}{\hat{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}\right)\] \[=\Pi\left(\|(\theta-\theta_{0})\|\leq\sqrt{\frac{2}{\hat{ \Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}}\right)\] \[\geq\Pi\left(\|(\theta-\theta_{0})\|_{\infty}\leq\sqrt{\frac{2}{ \hat{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}}\right)\] \[=\prod_{i=1}^{d}\Pi_{i}\left(|(\theta_{i}-\theta_{0}^{i})|\leq \sqrt{\frac{2}{\hat{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}}\right),\]

where in the last equality we used the fact that the prior distribution is uncorrelated. Now, the result follows immediately for sufficiently large \(n\), if the prior distribution is uncorrelated and uniformly distributed on the compact set \(\Theta_{i}\), for each \(i\in\{1,2,\ldots,d\}\). In particular observe that for large enough \(n\), we have

\[\Pi(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right) \leq nC_{3}\epsilon_{n}^{2})\geq\prod_{i=1}^{d}\frac{\theta_{0}^{ i}+\sqrt{\frac{2}{\bar{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}}-\theta_{0}^{i}+ \sqrt{\frac{2}{\bar{\Lambda}(\lambda+1)}C_{3}\epsilon_{n}^{2}}}{m(\Theta_{i})}\] \[=\frac{2^{d}\left(\frac{2}{\bar{\Lambda}(\lambda+1)}C_{3}\epsilon _{n}^{2}\right)^{d/2}}{\prod_{i=1}^{d}m(\Theta_{i})}=\left(\frac{8}{\bar{ \Lambda}(\lambda+1)}\right)\left(\prod_{i=1}^{d}m(\Theta_{i})\right)^{-2/d}C_ {3}\epsilon_{n}^{2}\right)^{d/2},\]

where \(m(A)\) is the Lebesgue measure (volume) of any set \(A\subset\mathbb{R}\). Now if \(\epsilon_{n}^{2}=\frac{\log n}{n}\), then for \(\frac{8}{\bar{\Lambda}(\lambda+1)\left(\prod_{i=1}^{d}m(\Theta_{i})\right)^{2 /d}}C_{3}>2\), \(\frac{8}{\bar{\Lambda}(\lambda+1)\left(\prod_{i=1}^{d}m(\Theta_{i})\right)^{2 /d}}C_{3}\epsilon_{n}^{2}\geq e^{-\frac{8}{\bar{\Lambda}(\lambda+1)\left(\prod_ {i=1}^{d}m(\Theta_{i})\right)^{2/d}C_{3}\epsilon_{n}^{2}}}\) for all \(n\geq 2\), therefore,

\[\Pi(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\leq nC_{3}\epsilon_{n }^{2})\geq e^{-\frac{4d}{\bar{\Lambda}(\lambda+1)\left(\prod_{i=1}^{d}m( \Theta_{i})\right)^{2/d}}C_{3}n\epsilon_{n}^{2}}.\]

Proof of Lemma B.9: Since family \(\mathcal{Q}\) contains all uncorrelated Gaussian distributions restricted to \(\Theta\), observe that \(\{q_{n}(\cdot)\in\mathcal{Q}\}\forall n\geq 1\). By definition, \(q_{n}^{i}(\theta)\propto\frac{1}{\sqrt{2\pi\sigma_{i,n}^{2}}}e^{-\frac{1}{2 \sigma_{i,n}^{2}}(\theta-\mu_{i,n})^{2}}\mathbbm{1}_{\Theta_{i}}=\frac{ \mathcal{N}(\theta_{i}|\mu_{i,n},\sigma_{i,n})\mathbbm{1}_{\Theta_{i}}}{ \mathcal{N}(\Theta_{i}|\mu_{i,n},\sigma_{i,n})}\) and fix \(\sigma_{i,n}=1/\sqrt{n}\) and \(\theta_{i}=\theta_{0}^{i}\) for all \(i\in\{1,2,\ldots,d\}\). Now consider the first term; using the definition of the KL divergence it follows that

\[\text{KL}(q_{n}(\theta)\|\pi(\theta))=\int q_{n}(\theta)\log(q_{n}(\theta))d \theta-\int q_{n}(\theta)\log(\pi(\theta))d\theta.\] (87)

Substituting \(q_{n}(\theta)\) in the first term of the equation above and expanding the logarithm term, we obtain

\[\int q_{n}(\theta)\log(q_{n}(\theta))d\theta =\sum_{i=1}^{d}\int q_{n}^{i}(\theta_{i})\log(q_{n}^{i}(\theta_{i }))d\theta_{i}\] \[\leq\sum_{i=1}^{d}\int\mathcal{N}(\theta_{i}|\mu_{i,n},\sigma_{i, n})\log\mathcal{N}(\theta_{i}|\mu_{i,n},\sigma_{i,n})d\theta_{i}\] \[=-\sum_{i=1}^{d}[\log(\sqrt{2\pi e})+\log\sigma_{i,n}],\] (88)

where in the last equality, we used the well known expression for the differential entropy of Gaussian distributions. Recall \(\pi(\theta)=\prod_{i=1}^{d}\frac{1}{m(\Theta_{i})}\). Now consider the second term in (87). It is straightforward to observe that,

\[-\int q_{n}(\theta)\log(\pi(\theta))d\theta=\sum_{i=1}^{d}\log(m(\Theta_{i})).\] (89)

Substituting (89) and (88) into (87) and dividing either sides by \(n\) and substituting \(\sigma_{i,n}\), we obtain

\[\frac{1}{n}\text{KL}(q_{n}(\theta)\|\pi(\theta)) \leq-\frac{1}{n}\sum_{i=1}^{d}[\log(\sqrt{2\pi e})-\log(m(\Theta_{ i}))-\frac{1}{2}\log n]\] \[=\frac{d}{2}\frac{\log n}{n}-\frac{1}{n}\sum_{i=1}^{d}[\log(\sqrt{2 \pi e})-\log(m(\Theta_{i}))].\] (90)

Now, consider the second term in the assertion of the lemma. Since \(\xi_{i},i\in\{1,2\ldots n\}\) are independent and identically distributed, we obtain

\[\frac{1}{n}\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}\|p(\tilde{ X}_{n}|\theta)\right)\right]=\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}\|p( \xi|\theta)\right)\right]\]Now using the expression for \(\text{KL}\) divergence between the two multivariate Gaussian distributions, we have

\[\frac{1}{n}\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n} \|p(\tilde{X}_{n}|\theta)\right)\right] =\frac{1}{2}\mathbb{E}_{q_{n}(\theta)}\left[(\theta-\theta_{0})^{T} \Sigma^{-1}(\theta-\theta_{0})\right]\] \[\leq\frac{\tilde{\Lambda}^{-1}}{2}\mathbb{E}_{q_{n}(\theta)} \left[(\theta-\theta_{0})^{T}(\theta-\theta_{0})\right]\] \[\leq\frac{d}{n}\frac{\tilde{\Lambda}^{-1}}{2}\] (91)

where \(\tilde{\Lambda}=\min_{i\in\{1,2,\ldots d\}}\Lambda_{ii}\), and \(\Sigma^{-1}=Q^{T}\Lambda^{-1}Q\), where \(Q\) is an orthogonal matrix and \(\Lambda\) is a diagonal matrix consisting of the respective eigen values of \(\Sigma\). Combined together (91) and (90) implies that

\[\frac{1}{n}\left[\text{KL}\left(q_{n}(\theta)\|\pi(\theta)\right) +\mathbb{E}_{q_{n}(\theta)}\left[\text{KL}\left(dP_{0}^{n}\right)\|p(\tilde{X} _{n}|\theta)\right)\right]\] \[\quad\leq\frac{d}{2}\frac{\log n}{n}-\frac{1}{n}\sum_{i=1}^{d}[ \log(\sqrt{2\pi e})-\log(m(\Theta_{i}))]+\frac{d}{n}\frac{\tilde{\Lambda}^{-1 }}{2}\leq C_{9}\frac{\log n}{n}.\] (92)

where \(C_{9}:=\frac{d}{2}+\max\left(0,-\sum_{i=1}^{d}[\log(\sqrt{2\pi e})-\log(m( \Theta_{i}))]+\frac{d}{2}\tilde{\Lambda}^{-1}\right)\) and the result follows. 

### Gaussian process classification

Proof of Lemma b.11.: In view of Theorem 7.1 in [11], it suffices to show that

\[N\left(\epsilon,\Theta_{n}(\epsilon),d_{\text{TV}}\right)\leq e^{\tilde{C}n \epsilon^{2}},\]

for some \(\tilde{C}>0\). Now, first observe that

\[d_{\text{TV}}(P_{\theta(y)},P_{\theta_{0}(y)}) =\frac{1}{2}\mathbb{E}_{\nu}\left(|\Psi_{1}(\theta(y))-\Psi_{1}( \theta_{0}(y))|+|\Psi_{-1}(\theta(y))-\Psi_{-1}(\theta_{0}(y))|\right)\] \[=\mathbb{E}_{\nu}\left(|\Psi_{1}(\theta(y))-\Psi_{1}(\theta_{0}(y ))|\right)\] \[\leq\mathbb{E}_{\nu}\left(|\theta(y)-\theta_{0}(y)|\right)\leq\| \theta(y)-\theta_{0}(y)\|_{\infty},\] (93)

where the second equality uses the definition of \(\Psi_{-1}(\cdot)\). Since, total-variation distance above is bounded above by supremum norm, there exists a constant \(0<c^{\prime}<1/2\), such that

\[N\left(\epsilon,\Theta_{n}(\epsilon),d_{\text{TV}}\right)\leq N\left(c^{ \prime}\epsilon,\Theta_{n}(\epsilon),\|\cdot\|_{\infty}\right)\leq e^{\frac{ 2}{3}c^{\prime 2}C_{10}n\epsilon^{2}},\] (94)

where the last inequality follows from (13) in Lemma B.10. Then if follows from Theorem 7.1 in [11] that for every \(\epsilon>\epsilon_{n}\), there exists a test \(\phi_{n}\) (depending on \(\epsilon>0\)) such that, for every \(j\geq 1\),

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}] \leq e^{\frac{2}{3}c^{\prime 2}C_{10}n\epsilon^{2}}e^{-\frac{1}{2}n \epsilon^{2}}\frac{1}{1-\exp\left(-\frac{1}{2}n\epsilon^{2}\right)},\text{and}\] \[\sup_{\{\theta\in\Theta_{n}(\epsilon):d_{\text{TV}}(P_{\theta},P_ {\theta_{0}})>j\epsilon\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq\exp\left(-\frac{1}{2}n\epsilon^{2}j\right).\]

Now for all \(n\) such that \(n\epsilon^{2}>n\epsilon_{n}^{2}>2\log 2\) and \(C_{10}=c^{\prime-2}/4>1\) and \(j=1\), we have

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}] \leq 2e^{-\frac{1}{3}n\epsilon^{2}},\text{and}\] (95) \[\sup_{\{\theta\in\Theta_{n}(\epsilon):d_{\text{TV}}(P_{\theta},P_ {\theta_{0}})>\epsilon\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq e^{-\frac{1}{2}n\epsilon^{2}}\leq e^{-\frac{1}{3}n\epsilon^{2}}.\] (96)

Now observe that

\[\sup_{a\in\mathcal{A}} |G(a,\theta)-G(a,\theta_{0})|\] \[=\max\left(c_{+}|\mathbb{E}_{\nu}[\Psi_{-1}(\theta(y))]-\mathbb{E} _{\nu}[\Psi_{-1}(\theta_{0}(y))]|,c_{-}|\mathbb{E}_{\nu}[\Psi_{1}(\theta(y))] -\mathbb{E}_{\nu}[\Psi_{1}(\theta_{0}(y))]|\right)\] \[=\max\left(c_{+}|\mathbb{E}_{\nu}[\Psi_{1}(\theta_{0}(y))]- \mathbb{E}_{\nu}[\Psi_{1}(\theta(y))]|,c_{-}|\mathbb{E}_{\nu}[\Psi_{1}(\theta (y))]-\mathbb{E}_{\nu}[\Psi_{1}(\theta_{0}(y))]|\right)\] \[=\max(c_{+},c_{-})|\mathbb{E}_{\nu}[\Psi_{1}(\theta_{0}(y))]- \mathbb{E}_{\nu}[\Psi_{1}(\theta(y))]|\] \[\leq\max(c_{+},c_{-})\mathbb{E}_{\nu}|\Psi_{1}(\theta_{0}(y))- \Psi_{1}(\theta(y))|\] \[\leq\max(c_{+},c_{-})d_{TV}(P_{\theta},P_{\theta_{0}})\] (97)where the second equality uses the fact that \(\Psi_{-1}(\cdot)=1-\Psi_{1}(\cdot)\).

Consequently,

\[\{\theta\in\Theta_{n}(\epsilon):\sup_{a\in\mathcal{A}}|G(a,\theta)-G(a,\theta_{0} )|>\max(c_{+},c_{-})\epsilon\}\subseteq\{\theta\in\Theta_{n}(\epsilon):d_{TV}(P _{\theta},P_{\theta_{0}})>\epsilon\}\]

Therefore, it follows from (95) and (96) and the definition of \(L_{n}(\theta,\theta_{0})\) that

\[\mathbb{E}_{P_{0}^{n}}[\phi_{n}] \leq 2e^{-\frac{1}{3}n\epsilon^{2}},\text{and}\] (98) \[\sup_{\{\theta\in\Theta_{n}(\epsilon):L_{n}(\theta,\theta_{0})>( \max(c_{+},c_{-})^{2}n\epsilon^{2})\}}\mathbb{E}_{P_{\theta}^{n}}[1-\phi_{n}] \leq e^{-\frac{1}{3}n\epsilon^{2}}\leq e^{-\frac{1}{3}n\epsilon^{2}}.\] (99)

Finally, the result follows for \(C=1/3\), \(C_{0}=2\) and \(C_{1}=(\max(c_{+},c_{-}))^{2}\).

Proof of Lemma b.12.: The Renyi divergence

\[D_{1+\lambda}(P_{0}^{n}\|P_{\theta}^{n})\] \[=n\frac{1}{\lambda}\ln\int\left(\Psi_{1}(\theta_{0}(y))^{1+ \lambda}\Psi_{1}(\theta(y))^{-\lambda}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda} \Psi_{-1}(\theta(y))^{-\lambda}\right)\nu(dy)\] \[=n\frac{1}{\lambda}\ln\int e^{\lambda\frac{1}{\lambda}\ln\left( \Psi_{1}(\theta_{0}(y))^{1+\lambda}\Psi_{1}(\theta(y))^{-\lambda}+\Psi_{-1}( \theta_{0}(y))^{1+\lambda}\Psi_{-1}(\theta(y))^{-\lambda}\right)}\nu(dy).\] (100)

Note that the derivative of the exponent in the integrand above with respect to \(\theta(y)\) is

\[\frac{\left(-\lambda\Psi_{1}(\theta_{0}(y))^{1+\lambda}\Psi_{1}( \theta(y))^{-\lambda-1}\psi(\theta(y))+\lambda\Psi_{-1}(\theta_{0}(y))^{1+ \lambda}\Psi_{-1}(\theta(y))^{-\lambda-1}\psi(\theta(y))\right)}{\left(\Psi_{ 1}(\theta_{0}(y))^{1+\lambda}\Psi_{1}(\theta(y))^{-\lambda}+\Psi_{-1}(\theta_ {0}(y))^{1+\lambda}\Psi_{-1}(\theta(y))^{-\lambda}\right)}\] \[=\lambda\psi(\theta(y))\frac{\left(-\Psi_{1}(\theta_{0}(y))^{1+ \lambda}\Psi_{1}(\theta(y))^{-\lambda-1}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda} \Psi_{-1}(\theta(y))^{-\lambda-1}\right)}{\left(\Psi_{1}(\theta_{0}(y))^{1+ \lambda}\Psi_{1}(\theta(y))^{-\lambda}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda} \Psi_{-1}(\theta(y))^{-\lambda}\right)}\] \[=\lambda\frac{\psi(\theta(y))}{\Psi_{1}(\theta(y))\Psi_{-1}( \theta(y))}\frac{\left(-\Psi_{1}(\theta_{0}(y))^{1+\lambda}\Psi_{-1}(\theta(y) )^{\lambda+1}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda}\Psi_{1}(\theta(y))^{\lambda +1}\right)}{\left(\Psi_{1}(\theta_{0}(y))^{1+\lambda}\Psi_{-1}(\theta(y))^{ \lambda}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda}\Psi_{1}(\theta(y))^{\lambda} \right)}\] \[=\lambda\frac{\left(-\Psi_{1}(\theta_{0}(y))^{1+\lambda}\Psi_{-1} (\theta(y))^{\lambda+1}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda}\Psi_{1}(\theta(y ))^{\lambda+1}\right)}{\left(\Psi_{1}(\theta_{0}(y))^{1+\lambda}\Psi_{-1}( \theta(y))^{1+\lambda}\Psi_{-1}(\theta(y))^{1+\lambda}\Psi_{1}(\theta(y))^{ \lambda}\right)}\] \[=\lambda\frac{\left(-e^{-(\lambda+1)\theta(y)}+e^{-(1+\lambda) \theta_{0}(y)}\right)}{\left(e^{-\lambda\theta(y)}+e^{-(\lambda+1)\theta_{0}( y)}\right)\left(1+e^{-\theta(y)}\right)}\] \[=\lambda\frac{e^{-(1+\lambda)\theta_{0}(y)}\left(1-e^{-(\lambda+ 1)(\theta(y)-\theta_{0}(y))}\right)}{\left(e^{-\lambda\theta(y)}+e^{-(\lambda+ 1)\theta_{0}(y)}\right)\left(1+e^{-\theta(y)}\right)}\] \[\leq\lambda\frac{(\lambda+1)(\theta(y)-\theta_{0}(y))}{\left(e^ {-\lambda\theta(y)+(\lambda+1)\theta_{0}(y)}+1\right)\left(1+e^{-\theta(y)} \right)}\] \[\leq\lambda(\lambda+1)|\theta(y)-\theta_{0}(y)|,\] (101)

where in the fourth equality we used definition of the logistic function and the penultimate inequality follows from the well known inequality that \(1-e^{-x}\leq x\). Consequently, using Taylor's theorem it follows that the exponent in the integrand of the Renyi divergence in (100) is bounded above by \(\lambda(\lambda+1)|\theta(y)-\theta_{0}(y)|^{2}\) and thus by \(\lambda(\lambda+1)\|\theta(y)-\theta_{0}(y)\|_{\infty}^{2}\). Therefore,

\[D_{1+\lambda} (P_{0}^{n}\|P_{\theta}^{n})\] \[=n\frac{1}{\lambda}\ln\int\left(\Psi_{1}(\theta_{0}(y))^{1+ \lambda}\Psi_{1}(\theta(y))^{-\lambda}+\Psi_{-1}(\theta_{0}(y))^{1+\lambda} \Psi_{-1}(\theta(y))^{-\lambda}\right)\nu(dy)\] \[\leq n\frac{1}{\lambda}\ln\int e^{\lambda(\lambda+1)\|\theta(y)- \theta_{0}(y)\|_{\infty}^{2}}\nu(dy)\] \[=n(\lambda+1)\|\theta(y)-\theta_{0}(y)\|_{\infty}^{2}.\]Now using the inequality for \(C_{3}=16(\lambda+1)\) above observe that

\[\Pi(A_{n}) =\Pi(D_{1+\lambda}\left(P_{0}^{n}\|P_{\theta}^{n}\right)\leq C_{3}n \epsilon_{n}^{2})\] \[\geq\Pi(n(\lambda+1)\|\theta(y)-\theta_{0}(y)\|_{\infty}^{2}\leq C _{3}n\epsilon_{n}^{2})\] \[=\Pi(\|\theta(y)-\theta_{0}(y)\|_{\infty}\leq 4\epsilon_{n})\geq e ^{-n\epsilon_{n}^{2}}\] (102)

and the result follows from (15) of Lemma B.10.

Proof of Lemma b.13.: Let us first analyze the KL divergence between the prior distribution and variational family. Recall that two Gaussian measures on infinite dimensional spaces are either equivalent or singular. [27, Theorem 6.13] specify the condition required for the two Gaussian measures to be equivalent. In particular, note that \(\theta_{0}^{J}(\cdot)\in\text{Im}(\mathcal{C}^{1/2})\). Now observe that the covariance operator of \(Q_{n}\) has eigenvalues \(\{\zeta_{j}^{2}\}_{j=1}^{J}{}_{k=1}^{2^{jd}}\), therefore operator \(S\) in the definition of \(\mathcal{C}_{q}\) has eigenvalues \(\{1-\zeta_{j}^{2}/\mu_{j}^{2}\}_{j=1}^{J}{}_{k=1}^{2^{jd}}\). For \(\tau_{j}^{2}=2^{-2ja-jd}\) for any \(a>0\), \(\sum_{j=1}^{J}2^{jd}\left(\frac{n\epsilon_{n}^{2}2^{-2ja-jd}}{1+n\epsilon_{n} ^{2}2^{-2ja-jd}}\right)^{2}=\sum_{j=1}^{J}2^{-jd}\left(\frac{n\epsilon_{n}^{2} 2^{-2ja}}{1+n\epsilon_{n}^{2}2^{-2ja-jd}}\right)^{2}<\infty\), therefore \(S\) is an HS operator.

For any integer \(J\leq J_{\alpha}\) define \(\bar{\theta}_{0}^{J}=\int\theta_{0}^{J}(y)\nu(dy)\), where \(\theta_{0}^{J}(\cdot)=\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\theta_{0;j,k}\vartheta _{j,k}(\cdot)\). Since, \(\theta_{0}^{J}(\cdot)\in\text{Im}(\mathcal{C}^{1/2})\) and \(S\) is a symmetric and HS operator, we invoke Theorem 5 in [22], to write

\[\text{KL}(\mathcal{N}(\bar{\theta}_{0}^{J},\mathcal{C}_{q})\| \mathcal{N}(0,\mathcal{C})) =\frac{1}{2}\|\mathcal{C}^{-1/2}\bar{\theta}_{0}^{J}\|^{2}-\frac{ 1}{2}\log\det(I-S)+\frac{1}{2}tr(-S),\] \[=\frac{1}{2}\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\frac{\theta_{0;k,j} ^{2}}{\mu_{j}^{2}}-\frac{1}{2}\log\prod_{j=1}^{J}\prod_{k=1}^{2^{jd}}(1- \kappa_{j}^{2})-\frac{1}{2}\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\kappa_{j}^{2}\] \[=\frac{1}{2}\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\frac{\theta_{0;k,j} ^{2}}{\mu_{j}^{2}}-\frac{1}{2}\log\prod_{j=1}^{J}(1-\kappa_{j}^{2})^{2^{jd}}- \frac{1}{2}\sum_{j=1}^{J}2^{jd}\kappa_{j}^{2}\] \[=\frac{1}{2}\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\frac{\theta_{0;k,j} ^{2}}{\mu_{j}^{2}}-\frac{1}{2}\sum_{j=1}^{J}2^{jd}\log(1-\kappa_{j}^{2})- \frac{1}{2}\sum_{j=1}^{J}2^{jd}\kappa_{j}^{2}.\]

Now for \(\mu_{j}2^{jd/2}=2^{-ja}\), and using the definition of Besov norm of \(\theta_{0}\) denoted as \(\|\theta_{0}\|_{\beta,\infty,\infty}^{2}\), and denoting \(1-\kappa_{j}^{2}=\frac{1}{1+n\epsilon_{n}^{2}\tau_{j}^{2}}\), we have

\[\text{KL}(\mathcal{N}(\bar{\theta}_{0}^{J},\mathcal{C}_{q})\| \mathcal{N}(0,\mathcal{C}))\] \[\leq\frac{1}{2}\sum_{j=1}^{J}2^{j(2a-2\beta+d)}\|\theta_{0}\|_{ \beta,\infty,\infty}^{2}-\frac{1}{2}\sum_{j=1}^{J}2^{jd}\log(1-\kappa_{j}^{2} )-\frac{1}{2}\sum_{j=1}^{J}2^{jd}\kappa_{j}^{2}\] \[=\frac{1}{2}\sum_{j=1}^{J}2^{j(2a-2\beta+d)}\|\theta_{0}\|_{ \beta,\infty,\infty}^{2}-\frac{1}{2}\sum_{j=1}^{J}2^{jd}\left(\log(1-\kappa_{j }^{2})+\kappa_{j}^{2}\right)\] \[=\frac{1}{2}\sum_{j=1}^{J}2^{j(2a-2\beta+d)}\|\theta_{0}\|_{ \beta,\infty,\infty}^{2}+\frac{1}{2}\sum_{j=1}^{J}2^{jd}\left(\log(1+n\epsilon_ {n}^{2}\tau_{j}^{2})-\frac{n\epsilon_{n}^{2}\tau_{j}^{2}}{1+n\epsilon_{n}^{2} \tau_{j}^{2}}\right)\] \[\leq\frac{1}{2}\sum_{j=1}^{J}2^{j(2a-2\beta+d)}\|\theta_{0}\|_{ \beta,\infty,\infty}^{2}+\frac{1}{2}\sum_{j=1}^{J}2^{jd}\left(n\epsilon_{n}^{2} \tau_{j}^{2}\right),\]where the last inequality follows from the fact that, \(\log(1+x)-\frac{x}{1+x}\leq\frac{x^{2}}{1+x}\leq x\) for \(x>0\). Substituting \(\tau_{j}^{2}=2^{-2ja-jd}\), we have

\[\frac{1}{n}\text{KL}(\mathcal{N}(\bar{\theta}_{0}^{J},\mathcal{C} _{q})\|\mathcal{N}(0,\mathcal{C})) \leq\frac{1}{2n}\sum_{j=1}^{J}2^{j(2a-2\beta+d)}\|\theta_{0}\|_{ \beta,\infty,\infty}^{2}+\frac{\epsilon_{n}^{2}}{2}\sum_{j=1}^{J}2^{-2ja}\] \[\leq\frac{\|\theta_{0}\|_{\beta,\infty,\infty}^{2}}{2n}\sum_{j=1} ^{J}2^{j(2a-2\beta+d)}+\frac{2^{-2a}}{2}\frac{1-2^{-2ja}}{1-2^{-2a}}\epsilon_ {n}^{2}.\]

The summation in the first term above is bounded by \(\epsilon_{n}^{2}\) as derived in [30, Theorem 4.5]. Therefore,

\[\frac{1}{n}\text{KL}(\mathcal{N}(\bar{\theta}_{0}^{J},\mathcal{C}_{q})\| \mathcal{N}(0,\mathcal{C}))\leq\max\left(\|\theta_{0}\|_{\beta,\infty,\infty} ^{2},\frac{2^{-2a}-2^{-2Ja-2a}}{1-2^{-2a}}\right)\epsilon_{n}^{2}.\] (103)

Now consider the second term

\[\frac{1}{n} \mathbb{E}_{Q_{n}}\text{KL}(P_{0}^{n}\|P_{\theta}^{n})\] \[=\mathbb{E}_{Q_{n}}\int\left(\Psi_{1}(\theta_{0}(y))\log\frac{ \Psi_{1}(\theta_{0}(y))}{\Psi_{1}(\theta(y))}+\Psi_{-1}(\theta_{0}(y))\log \frac{\Psi_{-1}(\theta_{0}(y))}{\Psi_{-1}(\theta(y))}\right)\nu(dy)\] \[\leq\mathbb{E}_{Q_{n}}\int(\theta(y)-\theta_{0}(y),\theta(y)- \theta_{0}(y))\nu(dy)\] \[=\mathbb{E}_{Q_{n}}\int\|\theta(y)-\theta_{0}^{J}(y)-(\theta_{0}( y)-\theta_{0}^{J}(y))\|_{2}^{2}\nu(dy)\] \[=\mathbb{E}_{Q_{n}}\int\|\theta(y)-\theta_{0}^{J}(y)\|_{2}^{2}+\| \theta_{0}(y)-\theta_{0}^{J}(y))\|_{2}^{2}-2\langle\theta(y)-\theta_{0}^{J}(y ),\theta_{0}(y)-\theta_{0}^{J}(y)\rangle\nu(dy)\] \[\leq\mathbb{E}_{Q_{n}}\int\|\theta(y)-\theta_{0}^{J}(y)\|_{2}^{2} \nu(dy)+\|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\] \[=\mathbb{E}_{Q_{n}}\int|\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\zeta_{j }Z_{j,k}\vartheta_{j,k}(y)|^{2}\nu(dy)+\|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{ \infty}^{2}\] \[\leq\mathbb{E}_{Q_{n}}\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\zeta_{j }^{2}Z_{j,k}^{2}\int\vartheta_{j,k}(y)^{2}\nu(dy)+\|\theta_{0}(y)-\theta_{0}^{ J}(y))\|_{\infty}^{2}\] \[=\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\zeta_{j}^{2}\mathbb{E}_{Q_{n}} [Z_{j,k}^{2}]+\|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\] \[=\sum_{j=1}^{J}\sum_{k=1}^{2^{jd}}\mu_{j}^{2}(1-\kappa_{j}^{2})+ \|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\] \[=\sum_{j=1}^{J}2^{jd}\frac{\mu_{j}^{2}}{1+n\epsilon_{n}^{2}\tau_{ j}^{2}}+\|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\] \[\leq\frac{1}{n\epsilon_{n}^{2}}\sum_{j=1}^{J}\frac{2^{-2ja}}{\tau _{j}^{2}}+\|\theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\] \[=\frac{1}{n\epsilon_{n}^{2}}\sum_{j=1}^{J}2^{jd}+\|\theta_{0}(y)- \theta_{0}^{J}(y))\|_{\infty}^{2}\] \[=\frac{2^{d}}{n\epsilon_{n}^{2}}\frac{2^{dJ}-1}{2^{d}-1}+\| \theta_{0}(y)-\theta_{0}^{J}(y))\|_{\infty}^{2}\] \[\leq\frac{2^{d}/(2^{d}-1)}{(\log n)^{2}}+C^{\prime}\epsilon_{n}^ {2},\]where in the second inequality, we used the second assertion of Lemma 3.2[30] for logistic function, the fifth inequality uses the fact that \(\theta(y)-\theta_{0}^{j}(y)\) is orthogonal to \(\theta_{0}(y)-\theta_{0}^{j}(y)\). For any \(a\leq\alpha\) fix \(J=J_{\alpha}\) otherwise \(J=J_{a}\), and then it is straightforward to check from the definition of \(\epsilon_{n}\) given in the assertion of the theorem that \((2^{dJ-1}/n\epsilon_{n}^{2})\leq(\log n)^{-2}\). The term \(\|\theta_{0}(y)-\theta_{0}^{j}(y)\|_{\infty}^{2}\) is also bounded by \(C^{\epsilon}\epsilon_{n}^{2}\) as shown in the proof of Theorem 4.5 in [30]. Consequently, the term \(\frac{1}{n}\mathbb{Q}_{n}\operatorname{KL}(P_{0}^{n}\|P_{\theta}^{n})\) is bounded above by \(\epsilon_{n}^{2}\) (upto a constant) for sufficiently large \(n\) since \((\log n)^{-2}<\epsilon_{n}^{2}\) and the result follows. 

Proof of Theorem 4.2.: The proof is a direct consequence of Theorem 3.2, Lemmas B.11, B.12, B.13, and Proposition 3.2.