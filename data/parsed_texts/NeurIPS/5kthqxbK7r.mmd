# On the Efficiency of ERM in Feature Learning

 Ayoub El Hanchi

University of Toronto &

Vector Institute

aelhan@cs.toronto.edu

&Chris J. Maddison

University of Toronto &

Vector Institute

cmaddis@cs.toronto.edu

&Murat A. Erdogdu

University of Toronto &

Vector Institute

erdogdu@cs.toronto.edu

###### Abstract

Given a collection of feature maps indexed by a set \(\mathcal{T}\), we study the performance of empirical risk minimization (ERM) on regression problems with square loss over the union of the linear classes induced by these feature maps. This setup aims at capturing the simplest instance of feature learning, where the model is expected to jointly learn from the data an appropriate feature map and a linear predictor. We start by studying the asymptotic quantiles of the excess risk of sequences of empirical risk minimizers. Remarkably, we show that when the set \(\mathcal{T}\) is not too large and when there is a unique optimal feature map, these quantiles coincide, up to a factor of two, with those of the excess risk of the oracle procedure, which knows a priori this optimal feature map and deterministically outputs an empirical risk minimizer from the associated optimal linear class. We complement this asymptotic result with a non-asymptotic analysis that quantifies the decaying effect of the global complexity of the set \(\mathcal{T}\) on the excess risk of ERM, and relates it to the size of the subbepiliary sets of the suboptimality of the feature maps. As an application of our results, we obtain new guarantees on the performance of the best subset selection procedure in sparse linear regression under general assumptions.

## 1 Introduction

A central idea in modern machine learning is that of data-driven feature learning. Specifically, instead of performing linear prediction on top of handcrafted features, the current dominant paradigm suggests to use models that select useful features for linear prediction in a data-dependent way [e.g. KSH12; LBH15; He+16; Vas+17]. Of course, by putting the burden of picking a feature map on the model and data, we should expect that the resulting learning problem will require more samples to be solved. But just how many more samples do we need to learn such feature-learning-based models?

In this paper, we investigate this question in a general setting. We study the performance of empirical risk minimization (ERM) on regression tasks with square loss and over model classes induced by arbitrary collections of features maps. More precisely, let \(X\) be the random input taking value in a set \(\mathcal{X}\), and let \((\phi_{t})_{t\in\mathcal{T}}\), \(\phi_{t}:\mathcal{X}\rightarrow\mathbb{R}^{d}\), be a collection of feature maps indexed by a set \(\mathcal{T}\). For a given regression task and i.i.d. samples, our aim is to understand the performance of ERM over the class of predictors \(\cup_{t\in\mathcal{T}}\{x\mapsto(w,\phi_{t}(x))\mid w\in\mathbb{R}^{d}\}\) as a function of the sample size, the distribution of the data, and relevant properties of the collection of feature maps \((\phi_{t})_{t\in\mathcal{T}}\).

Classical uniform-convergence-based analyses would suggest that the performance of ERM in this setting is determined by the size of the model class, appropriately measured. The main message of this paper is that in this case, this is wrong in a strong sense. Specifically, we prove an upper bound on the excess risk of ERM on this problem whose dependence on the size of the model class decays monotonically with the sample size, and eventually depends only on the size of the model class induced by the collection of optimal feature maps, which is typically much smaller.

**Formal setup.** We briefly formalize our problem here. Let \(X\) be the random input taking value in a set \(\mathcal{X}\), and let \((\phi_{t})_{t\in\mathcal{T}}\), \(\phi_{t}:\mathcal{X}\to\mathbb{R}^{d}\), be a collection of feature maps indexed by a set \(\mathcal{T}\).1 Let \(Y\in\mathbb{R}\) be the output random variable, jointly distributed with the input \(X\). Our goal is to learn to predict the output \(Y\) given the input \(X\) as well as possible within the class of predictors \(\big{\{}x\mapsto\langle w,\phi_{t}(x)\rangle\mid(t,w)\in\mathcal{T}\times \mathbb{R}^{d}\big{\}}\). We evaluate the quality of a single prediction \(\hat{y}\) given the ground truth \(y\) through the loss function \(\ell(\hat{y},y):=(\hat{y}-y)^{2}/2\), and the overall quality of a predictor \((t,w)\in\mathcal{T}\times\mathbb{R}^{d}\) through its risk

Footnote 1: We assume without loss of generality that if \(t,s\in\mathcal{T}\) with \(t\neq s\), then \(\phi_{t}\) and \(\phi_{s}\) induce different linear classes of functions, i.e. there is no matrix \(A\) such that \(\phi_{t}(x)=A\phi_{s}(x)\) for all \(x\in\mathcal{X}\).

\[R(t,w):=\operatorname{E}[\ell((w,\phi_{t}(X)),Y)],\qquad R_{*}:=\inf_{(t,w) \in\mathcal{T}\times\mathbb{R}^{d}}R(t,w).\]

We assume that we have access to \(n\) i.i.d. samples \((X_{i},Y_{i})_{i=1}^{n}\) with the same distribution as \((X,Y)\), and perform empirical risk minimization

\[(\hat{t}_{n},\hat{w}_{n})\in\operatorname*{argmin}_{(t,w)\in\mathcal{T}\times \mathbb{R}^{d}}R_{n}(t,w)\quad\text{ where }\quad R_{n}(t,w):=n^{-1}\sum_{i=1}^{n}\ell( \langle w,\phi_{t}(X_{i})\rangle,Y_{i}).\]

Our goal is to characterize the excess risk \(\mathcal{E}(\hat{t}_{n},\hat{w}_{n}):=R(\hat{t}_{n},\hat{w}_{n})-R_{*}\).

**Related work.** The study of upper bounds on the excess risk of ERM in a general setting is a classical topic. It was initiated by Vapnik and Chervonenkis [20] who established a link between the excess risk of ERM and the uniform convergence of the underlying empirical process. More recently, and fuelled by the development of Talagrand's concentration inequality [14] and its refinements [e.g. BLM00; Bou02], a literature emerged that provided more fine-grained control of the excess risk of ERM [e.g. BBM05; Kol06; BM06]. A key idea emerging from this line of work is localization. This concept, and in particular the iterative localization method of Koltchinskii [13], plays an important role in our development. We refer the reader to the books [13, 14], as well as the recent articles [11, 15] for more on this idea.

Focusing on the task of regression with square loss, upper bounds on the excess risk of ERM are available for many classes of predictors, including finite [e.g. Aud07; JRT08; LM09], linear [e.g. LM16; Oli16; Mou22], and convex classes [e.g. LM16a; Men14; LRS15]. A key development in this area over the last decade has been the realization that such bounds can be obtained under much weaker assumptions than previously thought, owing to the fact that only one-sided control of a certain empirical process is needed, and which can be obtained under very weak assumptions [14, 15, 16]. The line of work most closely related to ours is the one on random-design linear regression [1, 15, 16, 17, 18, 19], and we view our work as an extension of this literature. We review these results in more detail in Section 2.

Finally, and on a more conceptual level, our work is related to the recent effort to understand the effect of feature learning on the performance of neural networks [e.g. Bac17; Gho+20; Ba+22]. Beyond this conceptual connection however, our work is quite distinct from this literature. Among other things, our setting is more general since we consider arbitrary features maps. In the same vein, it is worth mentioning the line of work on multiple kernel learning [e.g. Lan+04; GA11; SD16], although we are not aware of results from this literature that are directly relevant to our setup.

**Challenges.** Our class of predictors is somewhat unstructured (e.g. it is in general non-convex), so that off-the-shelf results from the above literature are not directly applicable. Nevertheless, the analysis of the performance of ERM on linear classes provides a good starting point as we review in Section 2. Compared to that setting however, we are faced with two additional challenges. First, we need to control an additional source of error arising from the fact that ERM might select a suboptimal feature map. Second, we are lead to study the suprema of certain \(\mathcal{T}\)-indexed empirical processes, which in the linear setting reduce to single random variables that are easily dealt with.

**Organization.** The rest of the paper is organized as follows. In Section 2, we review known results on the excess risk of linear regression under square loss. In Section 3, we state our main results that hold for the excess risk of ERM for general index sets \(\mathcal{T}\). In Section 4, we specialize our analysis to the case where the index set \(\mathcal{T}\) is finite, obtain more explicit guarantees, and discuss their implications on the sparse linear regression problem. We conclude in Section 5 with a brief discussion.

Background

The goal of this section is to provide more context for our results. We review known results on the excess risk of ERM over linear classes, which corresponds in our setting to the special case where the set \(\mathcal{T}\) indexing the feature maps is a singleton. As such, to avoid introducing further notation, we use the one from the previous section, while dropping the dependence on \(t\) whenever it occurs.

In the setting of linear regression with square loss, and when the sample covariance matrix of the feature map is invertible, there is a unique empirical risk minimizer and its excess risk admits an explicit expression. Specifically, define

\[\Sigma:=\mathrm{E}\big{[}\phi(X)\phi(X)^{T}\big{]},\qquad\Sigma_{n}:=\frac{1}{ n}\sum_{i=1}^{n}\phi(X_{i})\phi(X_{i})^{T},\]

and let \(w_{*}\) denote the unique minimizer of the risk \(R(w)\).2 Then, an elementary calculation shows that when \(\Sigma_{n}\) is invertible, there is a unique empirical risk minimizer and it satisfies

Footnote 2: Throughout, we assume without loss of generality that the support of the distribution of \(\phi(X)\) is not contained in any hyperplane, which implies the invertibility of \(\Sigma\) and the uniqueness of \(w_{*}\)[cf. 13].

\[\hat{w}_{n}=w_{*}-\Sigma_{n}^{-1}\nabla R_{n}(w_{*}).\] (1)

Furthermore, since the risk is a quadratic function of \(w\) whose gradient at \(w_{*}\) vanishes, replacing \(R(\hat{w}_{n})\) by the equivalent exact second order Taylor expansion around \(w_{*}\) yields

\[R(\hat{w}_{n})-R(w_{*})=\frac{1}{2}\|\hat{w}_{n}-w_{*}\|_{\Sigma}^{2}=\frac{1} {2}\big{\|}\Sigma_{n}^{-1}\nabla R_{n}(w_{*})\big{\|}_{\Sigma}^{2}.\] (2)

While exact, this expression is not readily interpretable. For example, how fast does this excess risk go to \(0\) as a function of the sample size? The following classical result from asymptotic statistics [e.g. 14, 15, 16] makes this rate more explicit. To state it, we define

\[g(X,Y):=\nabla_{w}\ell(\langle w_{*},\phi(X)\rangle,Y),\qquad G:=\mathrm{E} \big{[}g(X,Y)g(X,Y)^{T}\big{]}.\]

**Theorem 1**.: _Assume that for all \(j\in[d]\), \(\mathrm{E}\big{[}\phi_{j}^{2}(X)\big{]}<\infty\), \(\mathrm{E}\big{[}Y^{2}\big{]}<\infty\), and \(\mathrm{E}[\|g(X,Y)\|_{\Sigma^{-1}}^{2}]<\infty\). Then, as \(n\to\infty\),_

\[n\cdot\mathcal{E}(\hat{w}_{n})\xrightarrow[]{d}\frac{1}{2}\cdot\|Z\|_{2}^{2},\]

_where \(Z\sim\mathcal{N}(0,\Sigma^{-1/2}G\Sigma^{-1/2})\). In particular, for any \(\delta\in(0,0.1)\),_

\[\lim_{n\to\infty}n\cdot Q_{\mathcal{E}(\hat{w}_{n})}(1-\delta)\sim\mathrm{E} [\|g(X,Y)\|_{\Sigma^{-1}}^{2}]+2\lambda_{\text{max}}(\Sigma^{-1/2}G\Sigma^{-1 /2})\log(1/\delta),\]

_where \(Q_{X}(p):=\inf\{x\in\mathbb{R}\mid\mathrm{P}(X\leq x)\geq p\}\) is the quantile function of a random variable \(X\), and where we write \(a\asymp b\) to mean that there exists absolute constants \(C,c\) such that \(c\cdot b\leq a\leq C\cdot b\). In the above statement, they can be taken as \(C=1\) and \(c=1/32\)._

We provide a proof in Appendix A for completeness. For our purposes, this theorem is most easily interpreted as follows: for large enough \(n\) and small enough \(\delta\), if the excess risk of ERM is bounded by some quantity with probability at least \(1-\delta\), then this quantity is, up to a constant, at least as large as the right-hand side of the second displayed equation divided by \(n\). While our primary interest is in non-asymptotic bounds, this asymptotic result, by virtue of its exactness, provides us with a benchmark against which such bounds can be compared. In particular, it identifies the quantity \(\mathrm{E}[\|g(X,Y)\|_{\Sigma^{-1}}^{2}]\) as an intrinsic parameter determining the excess risk of ERM on this problem.

For large enough \(n\), Theorem 1 gives an interpretable expression for the excess risk. However, it says nothing about how large \(n\) needs to be for this expression to be accurate. This motivates a non-asymptotic analysis of the excess risk of ERM, which has been carried out numerous times in recent years [e.g. 13, 14, 15]. A goal of this literature has been to obtain upper bounds on the excess risk of ERM that hold in probability under weak moment assumptions, building on the observation that this is indeed possible [14]. The following theorem is comparable to the best known result in this area. We leave the proof to Appendix B. To state it, we define

\[V:=\mathrm{E}\bigg{[}\Big{(}\Sigma^{-1/2}\phi(X)\phi(X)^{T}\Sigma^{-1/2}-I \Big{)}^{2}\bigg{]},\quad L:=\sup_{v\in S^{d-1}}\mathrm{E}\bigg{[}\Big{(} \langle v,\Sigma^{-1/2}\phi(X)\rangle^{2}-1\Big{)}^{2}\bigg{]}.\]

**Theorem 2**.: _Assume that for all \(j\in[d]\), \(\operatorname{E}\!\left[\phi_{j}^{4}(X)\right]<\infty\), \(\operatorname{E}\!\left[Y^{2}\right]<\infty\), and \(\operatorname{E}\!\left[\|g(X,Y)\|_{\Sigma^{-1}}^{2}\right]<\infty\). Let \(\delta\in(0,1)\). If_

\[n\geq(512\lambda_{\text{\rm max}}(V)+6)\log(ed)+(128L+11)\log(2/\delta),\]

_then with probability at least \(1-\delta\),_

\[\mathcal{E}(\hat{w}_{n})\leq 4\cdot(n\delta)^{-1}\cdot\operatorname{E}\!\left[\|g (X,Y)\|_{\Sigma^{-1}}^{2}\right].\]

At a high-level, this result says that above a certain explicit minimal sample size, the asymptotic expression of the excess risk of Theorem 1 is correct, up to a significantly worse dependence on \(\delta\). The restriction on the sample size is almost the best one can hope for. To see why, note that to get guarantees on the excess risk of _any_ empirical risk minimizer, we need at least that \(\Sigma_{n}\) is invertible, otherwise there exists an empirical risk minimizer arbitrarily far away from \(w_{*}\). To get quantitative guarantees, we need slightly more control in the form of a lower bound on \(\lambda_{\text{\rm min}}(\Sigma^{-1/2}\Sigma_{n}\Sigma^{-1/2})\). We refer the reader to a more detailed discussion in [1, Section 5].

This result has two key qualities, which we aim to reproduce in our results. First, it is assumption-lean, requiring nothing more than a fourth moment assumption on the coordinates of the feature map compared to Theorem 1. Second, it recovers the right dependence on the intrinsic parameter \(\operatorname{E}\!\left[\|g(X,Y)\|_{\Sigma^{-1}}^{2}\right]\) identified in Theorem 1. A downside of this generality is the bad dependence on \(\delta\). Without further assumptions, this cannot be improved; we refer the reader to the recent literature on robust linear regression for more on this issue [e.g. 1, 10, 11].

## 3 Main Results

In this section we state our main results. They are most easily seen as extensions of Theorems 1 and 2 for general index sets \(\mathcal{T}\). In Section 3.1, we study the asymptotics of the excess risk of ERM in our setting, and in Section 3.2, we present a non-asymptotic upper bound on the excess risk.

To state our results, we require additional definitions and notation. We start with the population and the sample covariance matrices

\[\Sigma(t):=\operatorname{E}\!\left[\phi_{t}(X)\phi_{t}(X)^{T}\right],\qquad \Sigma_{n}(t):=n^{-1}\sum_{i=1}^{n}\phi_{t}(X_{i})\phi_{t}(X_{i})^{T}.\]

We define the following collection of minimizers,

\[w_{*}(t):=\operatorname*{argmin}_{w\in\mathbb{R}^{d}}R(t,w),\qquad\mathcal{T} _{*}:=\operatorname*{argmin}_{t\in\mathcal{T}}R(t,w_{*}(t)),\]

the first is uniquely defined, while the second is set-valued in general. We define the gradient of the loss at these minimizers and their corresponding covariance matrices

\[g(t,(X,Y)):=\nabla_{w}\ell(\langle w_{*}(t),\phi_{t}(X)\rangle,Y),\qquad G(t, s):=\operatorname{E}\!\left[g(t,(X,Y))g(s,(X,Y))^{T}\right].\]

Finally, we introduce the following processes which play a key role in our development

\[\Lambda_{n}(t):=\sqrt{n}\cdot\lambda_{\text{\rm max}}(I-\Sigma^{-1/2}(t) \Sigma_{n}(t)\Sigma^{-1/2}(t)),\quad G_{n}(t):=\sqrt{n}\cdot\|\nabla_{w}R_{n} (t,w_{*}(t))\|_{\Sigma^{-1}(t)},\] (3)

as well as, for \(t_{*}\in\mathcal{T}_{*}\) and \(t\in\mathcal{T}\setminus\mathcal{T}_{*}\),

\[\Delta_{n}(t,t_{*}):=\sqrt{n}\cdot\bigg{(}1-\frac{R_{n}(t,w_{*}(t))-R_{n}(t_{ *},w_{*}(t_{*}))}{R(t,w_{*}(t))-R_{*}}\bigg{)}.\] (4)

We note that the process \((\Delta_{n}(t,t_{*}))_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\) is an empirical process (see [21] for an introduction), while \((\Lambda_{n}(t))_{t\in\mathcal{T}}\) and \((G_{n}(t))_{t\in\mathcal{T}}\) are partial suprema of empirical processes. In the sequel, we will slightly abuse this terminology, and call all of these empirical processes, with the understanding that they can be viewed as one with more indexing. We will further assume that these processes are separable; see [1, p.305-306] for a definition. This covers a wide range of applications, while avoiding delicate measurability issues. The suprema of such separable processes, which is the only way they enter our results, can be studied by taking the supremum over a countable dense subset of the index set. Therefore, without loss of generality, we assume that \(\mathcal{T}\) is countable.

Finally, in line with the literature on the theory of empirical processes [13], we say that a sequence of empirical processes is Glivenko-Cantelli if, when rescaled by \(n^{-1/2}\), the supremum of their absolute value taken over their index set converges to zero in probability as \(n\to\infty\). In other words, the weak law of large numbers holds uniformly over the index set. Similarly, we say that a sequence of empirical processes is Donsker if it converges in distribution to its limiting Gaussian process.3 In other words, the central limit theorem holds uniformly over the index set.

Footnote 3: See [13, Section 2.1] or the proof of Theorem 3 for a more precise definition.

### Asymptotic result

Our first main result is an asymptotic characterization of the quantiles of the excess risk of any sequence of empirical risk minimizers in our setting, which vastly generalizes that of Theorem 1.

**Theorem 3**.: _Assume that \(\mathcal{T}_{*}\neq\varnothing\) and for some \(t_{*}\in\mathcal{T}_{*}\), assume that the empirical processes \((\Lambda_{n}(t))_{t\in\mathcal{T}}\), \((\Delta(t,t_{*}))_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\) and \((G_{n}(t))_{t\in\mathcal{T}}\) are Glivenko-Cantelli. Then, for all \(\varepsilon>0\),_

\[\lim_{n\to\infty}\mathrm{P}\big{(}R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}> \varepsilon\big{)}=0.\]

_Furthermore, if the sequence of processes \((G_{n}(t))_{t\in\mathcal{T}}\) is Donsker, then for any \(\delta\in(0,1)\),_

\[\frac{1}{2}\cdot Q_{Z^{-}}(1-\delta)\leq\liminf_{n\to\infty}n\cdot Q_{\mathcal{ E}(\hat{t}_{n},\hat{w}_{n})}(1-\delta)\leq\limsup_{n\to\infty}n\cdot Q_{ \mathcal{E}(\hat{t}_{n},\hat{w}_{n})}(1-\delta)\leq Q_{Z^{+}}(1-\delta),\]

_where \(Z^{-}:=\inf_{s\in\mathcal{T}_{*}}\lVert Z(s)\rVert_{2}^{2}\), \(Z^{+}:=\sup_{s\in\mathcal{T}_{*}}\lVert Z(s)\rVert_{2}^{2}\), and \((Z(t))_{t\in\mathcal{T}}\) is a mean-zero Gaussian process with covariance function \(\mathrm{E}[Z(t)Z(s)^{T}]=\Sigma^{-1/2}(t)G(t,s)\Sigma^{-1/2}(s)\) for all \(t,s\in\mathcal{T}\)._

We note that, up to a factor of two in the upper bound on the asymptotic quantiles, Theorem 3 reduces to Theorem 1 when \(\mathcal{T}\) is a singleton, with the exact same assumptions. We are not aware of comparable results in the literature. The proof of Theorem 3 can be found in Appendix D.

_Remark 1_.: For small \(\delta\), the upper bound admits the more interpretable expression

\[Q_{Z_{+}}(1-\delta)\asymp\mathrm{E}\bigg{[}\sup_{s\in\mathcal{T}_{*}}\lVert Z (s)\rVert_{2}^{2}\bigg{]}+2\log(1/\delta)\sup_{s\in\mathcal{T}_{*}}\lambda_{ \text{max}}(\Sigma^{-1/2}(s)G(s,s)\Sigma^{-1/2}(s)).\] (5)

Furthermore, if \(\mathcal{T}_{*}\) is finite, the first term can be upper bounded as

\[\mathrm{E}\bigg{[}\max_{s\in\mathcal{T}_{*}}\lVert Z(s)\rVert_{2}^{2}\bigg{]} \leq 80\cdot(1+\log\lvert\mathcal{T}_{*}\rvert)\cdot\max_{s\in\mathcal{T}_{*}} \mathrm{E}[\lVert g(s,(X,Y))\rVert_{\Sigma^{-1}(s)}^{2}].\] (6)

To see why Theorem 3 is surprising, let us first focus on the case where \(\mathcal{T}_{*}\) has a unique element \(t_{*}\), so that \(Z_{+}=Z_{-}\stackrel{{ d}}{{=}}\lVert Z\rVert_{2}^{2}\) where \(Z\sim\mathcal{N}(0,\Sigma^{-1/2}(t_{*})G(t_{*},t_{*})\Sigma^{-1/2}(t_{*}))\). Now consider the oracle procedure, which knows beforehand what the optimal feature map \(t_{*}\) is, and outputs \(t_{*}\) and a minimizer of \(R_{n}(t_{*},w)\). Theorem 3 says that, up to a factor of two, the asymptotic quantiles of the excess risk of ERM, which needs to learn over the large class \(\cup_{t\in\mathcal{T}}\big{\{}x\mapsto\langle w,\phi_{t}(x)\rangle\mid w\in \mathbb{R}^{d}\big{\}}\), coincide with those of the oracle procedure (by Theorem 1), which only needs to learn over the _linear_ class \(\big{\{}x\mapsto\langle w,\phi_{t_{*}}(x)\rangle\mid w\in\mathbb{R}^{d}\big{\}}\)!

More generally, Theorem 3 establishes that asymptotically, any ERM picks a near-optimal feature map with probability one. It furthers shows that the asymptotic quantiles of the excess risk of any sequence of ERMs is controlled from above and below by those of the extrema of the limiting Gaussian process of \((G_{n}(t))_{t\in\mathcal{T}}\) on the set of optimal feature maps \(\mathcal{T}_{*}\). This is surprising, as it implies that asymptotically, and outside of its role in determining whether the assumptions of Theorem 3 hold, the global complexity of the set \(\mathcal{T}\) is irrelevant to the excess risk of ERM.

Finally, we note that the Glivenko-Cantelli and Donsker assumptions in Theorem 3 can equivalently be viewed as restrictions on the size of \(\mathcal{T}\), for distribution and process dependent notions of size. We refer the reader to the books [13, 14] for more on this connection. With this observation, the main takeaway from Theorem 3 can be stated as follows.

_Asymptotically, if \(\mathcal{T}\) is not too large, the excess risk of ERM depends, at worst, only on the complexity of the set of optimal feature maps \(\mathcal{T}_{*}\), and is independent of the global complexity of \(\mathcal{T}\)._

### Non-asymptotic result

The result in Theorem 3 hints at a dramatic localization phenomenon, whereby the influence of the size and complexity of the collection of feature maps \((\phi_{t})_{t\in\mathcal{T}}\) on the excess risk of ERM vanishes as \(n\to\infty\) under appropriate assumptions. The root of this localization phenomenon is the first statement of Theorem 3: eventually, ERM picks near-optimal feature maps with probability approaching one. For small enough sample sizes however, it is clear that ERM is likely to select suboptimal feature maps, so that this localization phenomenon cannot hold uniformly over \(n\). This raises a host of questions: (i) How fast, as measured by the sample size, does ERM learn the optimal feature map? (ii) What is the effect of this localization on the rate of decay of the excess risk of ERM non-asymptotically? (iii) What properties of the feature maps \((\phi_{t})_{t\in\mathcal{T}}\) influence these rates?

Our answers to these questions in this very general setting are formally expressed in Theorem 4 below. To state it, we define the following parameter

\[L:=\sup\mathrm{E}\,\Big{[}\Bigl{(}\sum_{t\in\mathcal{T}}\langle v_{t},\Sigma^{ -1/2}(t)\phi_{t}(X)\rangle^{2}-1\Bigr{)}^{2}\Big{]},\]

where the supremum is taken over vectors \((v_{t})_{t\in\mathcal{T}}\) such that \(\sum_{t\in\mathcal{T}}\lVert v_{t}\rVert_{2}^{2}=1\). For \(n\in\mathbb{N}\) and \(\delta\in(0,1)\), we define the set function \(F_{n,\delta}\), for any subset \(\mathcal{S}\subset\mathcal{T}\), by

\[F_{n,\delta}(\mathcal{S}):=\left\{t\in\mathcal{T}\ \middle|\ R(t,w_{*}(t))-R_{*}\leq 2 \cdot(n\delta)^{-1}\cdot\mathrm{E}[\sup_{s\in\mathcal{S}}G_{n}^{2}(s)]\right\}\] (7)

This map acts as a contraction as shown in the next lemma, whose proof is deferred to Appendix E. For a function \(f\), we use \(f^{k}\) to denote \(f^{k}(x):=f(f^{k-1}(x))\) with \(f^{0}(x):=x\).

**Lemma 1**.: _Let \(n\in\mathbb{N}\), \(\delta\in(0,1)\), and assume that \(\mathcal{T}_{*}\neq\varnothing\). Then for all \(k\in\mathbb{N}\cup\{0\}\),_

* \(F_{n,\delta}^{k+1}(\mathcal{T})\subseteq F_{n,\delta}^{k}(\mathcal{T})\)_._
* _If_ \(\exists\,n_{0},B\) _such that_ \(\mathrm{E}[\sup_{t\in\mathcal{T}}G_{n}^{2}(t)]\leq B\) _for all_ \(n\geq n_{0}\)_, then_ \(\bigcap_{n\geq 1}F_{n,\delta}^{k}(\mathcal{T})=\mathcal{T}_{*}\)_._

With these definitions, we now state the second main result of the paper. A proof is in Appendix F.

**Theorem 4**.: _Assume that \(\mathcal{T}_{*}\neq\varnothing\), \(\mathrm{E}[Y^{2}]<\infty\), \(\forall(t,j)\in\mathcal{T}\times[d]\), \(\mathrm{E}[\phi_{t,j}^{2}(X)]<\infty\), and \(\mathrm{E}[\lVert g(t,(X,Y))\rVert_{\Sigma^{-1}(t)}^{2}]<\infty\). Let \(\delta\in(0,1)\) and \(k\in\mathbb{N}\). If, for some \(t_{*}\in\mathcal{T}_{*}\), \(n\) satisfies_

\[n\geq 64\,\mathrm{E}[\sup_{t\in\mathcal{T}}\Lambda_{n}(t)]+(128L+11)\log(6/ \delta)+6\cdot\delta^{-2}\cdot\mathrm{E}[\sup_{t\in\mathcal{T}\setminus \mathcal{T}_{*}}\Delta_{n}(t,t_{*})],\]

_then, with probability at least \(1-\delta\),_

\[\hat{t}_{n}\in F_{n,\delta/2k}^{k}(\mathcal{T})=:\mathcal{S}_{n,\delta,k},\]

_and_

\[\mathcal{E}(\hat{t}_{n},\hat{w}_{n})\leq 24\cdot(n\delta)^{-1}\cdot\mathrm{E}[ \sup_{s\in\mathcal{S}_{n,\delta,k}}G_{n}^{2}(s)],\]

_where the processes \(\Lambda_{n}\), \(\Delta_{n}\), and \(G_{n}\) are as in (3) and (4)._

We make a few remarks before interpreting the content of the theorem. First, we note that when the index set \(\mathcal{T}\) is a singleton, the last term in the sample size restriction vanishes, while the first matches the sample size restriction from Theorem 2 after an application of Lemma 3 below; further taking \(k=1\) in Theorem 4 recovers the upper bound on the excess risk of Theorem 2 up to a constant factor. Theorem 4 may therefore be viewed as a broad generalization of Theorem 2. Second, under Assumption 1 below, and by the second item of Lemma 1, the upper bound on the excess risk in Theorem 4 eventually matches the main term in the asymptotic bound of Theorem 3 as can be seen from (5), in the same way that Theorem 2 achieves this when compared with Theorem 1. Finally, the statement of Theorem 4 is very general, and in fact, too general for us to be able to interpret it precisely. As such, we will discuss it in the context of the following assumption.

**Assumption 1**.: There exists constants \(C_{\Lambda}\), \(C_{\Delta}\), and \(C_{G}\) independent of the sample size, but possibly dependent on the remaining parameters of the problem, such that for all \(n\in\mathbb{N}\),

\[\mathrm{E}[\sup_{t\in\mathcal{T}}\Lambda_{n}(t)]\leq C_{\Lambda},\quad\mathrm{ E}\Biggl{[}\sup_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\Delta_{n}(t,t_{*}) \Biggr{]}\leq C_{\Delta},\quad\mathrm{E}[\sup_{t\in\mathcal{T}}G_{n}^{2}(t)] \leq C_{G},\]

where \(\Lambda_{n}\), \(\Delta_{n}\), and \(G_{n}\) are as in (3) and (4).

These assumptions can be equivalently viewed as a restriction on the appropriately measured size of the index set \(\mathcal{T}\)[26, 27], and are slightly stronger than the assumptions of Theorem 3. They always hold for finite index sets, and we will derive in Section 4 explicit estimates of the constants in Assumption 1 in terms of moments of the feature maps and target as well as the cardinality of \(\mathcal{T}\).

Let us now interpret the content of Theorem 4, which comes with a free parameter \(k\), in the context of Assumption 1. We fix \(k\) here, and discuss its choice below. First, recalling the definition of \(F_{n,\delta}\), this result says that above a certain sample size, both the suboptimality of the feature map picked by ERM and its excess risk decay at the fast rate \(n^{-1}\), answering the first question we raised at the beginning of the section. Second, this result provides an upper bound on the excess risk of ERM that depends on the index set \(\mathcal{T}\)_only_ through the size of shrinking subsets \(\mathcal{S}_{n,\delta,k}\), which might be large for small \(n\), but which by Lemma 1 converge to the set of optimal feature maps \(\mathcal{T}_{*}\) as \(n\to\infty\). This transparently shows the effect of the localization phenomenon on the rate of decay of the excess risk of ERM, answering the second question we raised. Finally, looking at the definition of \(\mathcal{S}_{n,\delta,k}\), this result identifies the size of the sublevel sets of the suboptimality function \(R(t,w_{*}(t))-R_{*}\) defined over feature maps as a relevant property of the collection of feature maps \((\phi_{t})_{t\in\mathcal{T}}\) that influences the rate of convergence of the excess risk of ERM in this setting, answering the final question we raised.

Finally, let us turn to the choice of \(k\). Practically, we select the one that minimizes the bound on the excess risk. Looking at the first item of Lemma 1, this optimal \(k\) balances the following trade-off: on the one hand, for small \(k\), applications of \(F_{n,\delta/2k}\) constrain the input set more severely, but only a few iterations are performed; on the other hand, larger values of \(k\) allow more iterations, but at the cost of more weakly constraining the input set per application.

Stepping back, there are two main takeaways from Theorem 4. Firstly, and on a conceptual level, it shows that feature learning is easy when the suboptimality function \(R(t,w_{*}(t))-R_{*}\), defined over the set of features maps, has small sublevel sets. Secondly, and on a technical level, it provides a template which can be used to derive more explicit excess risk bounds on ERM given estimates on the expected suprema of the relevant empirical processes. Deriving such accurate estimates for infinite \(\mathcal{T}\) is a highly non-trivial task, and cannot be done at the level of generality we have been operating at. The case of finite \(\mathcal{T}\) however is tractable in a general setting as we discuss in the next section.

## 4 Case study: Finite index sets

In this section, we focus on the case where the index set \(\mathcal{T}\) is finite, and aim, among other things, at establishing explicit estimates on the various expected suprema appearing in Theorem 4 in terms of moments of the feature maps and of the target. This problem becomes tractable in the case of finite \(\mathcal{T}\) because, roughly speaking, a worst-case analysis still yields non-trivial upper bounds. This is decidedly not the case when \(\mathcal{T}\) is infinite, in which case these expected suprema can be infinite.

We start with a slight strengthening of Theorem 3, whose assumptions reduce to simple moments conditions when \(\mathcal{T}\) is finite. The straightforward proof can be found in Appendix H.

**Corollary 1**.: _Assume that \(\mathcal{T}\) is finite, for all \((t,j)\in\mathcal{T}\times[d]\), \(\operatorname{E}\!\left[\phi_{t,j}^{2}(X)\right]<\infty\), \(\operatorname{E}\!\left[Y^{2}\right]<\infty\), and for all \(t\in\mathcal{T}\), \(\operatorname{E}\!\left[\|g(t,(X,Y))\|_{\Sigma^{-1}(t)}^{2}\right]<\infty\). Then_

\[\lim_{n\to\infty}\operatorname{P}\!\left(\hat{t}_{n}\notin\mathcal{T}_{*} \right)=0.\]

_Furthermore, for any \(\delta\in(0,1)\),_

\[\frac{1}{2}\cdot Q_{Z^{-}}(1-\delta)\leq\liminf_{n\to\infty}n\cdot Q_{ \mathcal{E}(\hat{t}_{n},\hat{w}_{n})}(1-\delta)\leq\limsup_{n\to\infty}n\cdot Q _{\mathcal{E}(\hat{t}_{n},\hat{w}_{n})}(1-\delta)\leq\frac{1}{2}\cdot Q_{Z^{+} }(1-\delta),\]

_where \(Z^{-}:=\min_{s\in\mathcal{T}_{*}}\|Z_{s}\|_{2}^{2}\), \(Z^{+}:=\max_{s\in\mathcal{T}_{*}}\|Z_{s}\|_{2}^{2}\), and the random vectors \((Z_{t})_{t\in\mathcal{T}}\) are jointly Gaussian with mean zero and covariance \(\operatorname{E}\!\left[Z_{t}Z_{s}^{T}\right]=\Sigma^{-1/2}(t)G(t,s)\Sigma^{-1 /2}(s)\) for all \(t,s\in\mathcal{T}\). In particular, if \(\mathcal{T}_{*}=\{t_{*}\}\), then_

\[n\cdot\mathcal{E}(\hat{t}_{n},\hat{w}_{n})\overset{d}{\to}\frac{1}{2}\cdot\|Z \|_{2}^{2},\]

_where \(Z\sim\mathcal{N}(0,\Sigma^{-1/2}(t_{*})G(t_{*},t_{*})\Sigma^{-1/2}(t_{*}))\)._

The conclusions of Corollary 1 differ from those of Theorem 3 in two aspects. First, the feature map picked by ERM is guaranteed to be optimal rather than near-optimal with probability converging to one. Second, the upper bound on the asymptotic quantiles is improved by a factor of two, yielding the exact distribution of the rescaled excess risk when \(\mathcal{T}_{*}\) is a singleton.

Making Theorem 4 more explicit is a more laborious task. We recall here two known results that allow us to accomplish this. We start with the following bounds on the expectation of the supremum of a finitely-indexed empirical process, which we will later use to bound the suprema of the processes \((G_{n}(s))_{s\in\mathcal{S}}\) and \((\Delta_{n}(t,t_{*}))_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\) appearing in Theorem 4. A proof can be found in Appendix G.

**Lemma 2**.: _Let \(n,d\in\mathbb{N}\), and let \(Z\) be a random element taking value in a set \(\mathcal{Z}\), and let \((Z_{i})_{i=1}^{n}\) be i.i.d. samples with the same distribution as \(Z\). Let \(\mathcal{F}\) be a finite collection of \(\mathbb{R}^{d}\)-valued measurable functions. Define_

\[\sigma^{2}(\mathcal{F}):=\max_{f\in\mathcal{F}}\mathrm{E}\big{[}\|f(Z)- \mathrm{E}[f(Z)]\|_{2}^{2}\big{]},\qquad r_{n}(\mathcal{F}):=\mathrm{E}\bigg{[} \max_{(i,f)\in[n]\times\mathcal{F}}\|f(Z_{i})-\mathrm{E}[f(Z)]\|_{2}^{2} \bigg{]}^{1/2},\]

_and let \(E_{n}(f):=\sqrt{n}\cdot(n^{-1}\sum_{i=1}^{n}f(Z_{i})-\mathrm{E}[f(Z)])\). Then, we have_

\[\frac{1}{2}\cdot\sigma(\mathcal{F})+\frac{1}{4}\cdot\frac{r_{n}(\mathcal{F})} {\sqrt{n}}\leq\mathrm{E}\bigg{[}\max_{f\in\mathcal{F}}\|E_{n}(f)\|_{2}^{2} \bigg{]}^{1/2}\leq c(|\mathcal{F}|)\cdot\sigma(\mathcal{F})+c^{2}(|\mathcal{F }|)\cdot\frac{r_{n}(\mathcal{F})}{\sqrt{n}},\]

_where \(c(m):=5\sqrt{1+\log m}\)._

Lemma 2 allows us to compute the expected supremum of a finitely-indexed empirical process, up to log factors in the size of the index set. It is known that these factors cannot be removed from the upper bound nor added to the lower bound without more assumptions, we refer the reader to a related discussion in [16]. Finally, while the term \(r_{n}(\mathcal{F})\) might grow with \(n\), by bounding the maximum with the sum, it grows at most as \(\sqrt{n}\). In many applications however, the random vectors \(f(Z)\) are bounded almost surely, so that \(r_{n}(\mathcal{F})\) is of order one, which justifies our presentation choice.

The second result we recall is the expectation version of a one sided Matrix Bernstein inequality due to Tropp [16]. We use it below to bound the supremum of the process \((\Lambda_{n}(t))_{t\in\mathcal{T}}\) appearing in Theorem 4. We do not known of a matching non-asymptotic lower bound, but an asymptotic one is known [1, Proposition 17]. Upper and lower bounds similar to those of Lemma 2 hold if one considers the expected operator norm instead of only the maximum eigenvalue [16, Section 7].

**Lemma 3** ([16], Theorem 6.6.1.).: _Let \(n,d\in\mathbb{N}\) and for each \(i\in[n]\), let \(Z_{i}\in\mathbb{R}^{d\times d}\) be i.i.d. positive semi-definite matrices with the same distribution as \(Z\). Define_

\[V:=\mathrm{E}\Big{[}\big{(}\mathrm{E}[Z]-Z\big{)}^{2}\Big{]},\quad W_{n}:= \sqrt{n}\cdot\Big{(}\mathrm{E}[Z]-\frac{1}{n}\sum_{i=1}^{n}Z_{i}\Big{)}.\]

_Then, we have_

\[\mathrm{E}[\lambda_{\text{\rm max}}(W_{n})]\leq\sqrt{2\lambda_{\text{\rm max}} (V)\log(ed)}+\frac{\lambda_{\text{\rm max}}(\mathrm{E}[Z])\log(ed)}{3\sqrt{n}}.\]

Equipped with these estimates, we may now control the expected suprema of the empirical processes appearing in Theorem 4. To apply Lemma 2, define the following classes, for \(\mathcal{S}\subset\mathcal{T}\) and \(t_{*}\in\mathcal{T}_{*}\)

\[\mathcal{G}(\mathcal{S}) :=\Big{\{}(x,y)\mapsto\Sigma^{-1/2}(s)g(s,(x,y))\,\Big{|}\,s\in \mathcal{S}\Big{\}},\] \[\mathcal{D}(t_{*}) :=\Big{\{}(x,y)\mapsto\frac{\ell(\langle w_{*}(t),\phi_{t}(x) \rangle,y)-\ell(\langle w_{*}(t_{*}),\phi_{t_{*}}(x)\rangle,y)}{R(t,w_{*}(t))- R_{*}}\,\bigg{|}\,t\in\mathcal{T}\setminus\mathcal{T}_{*}\Big{\}}.\]

Applying Lemma 2 on \(\mathcal{G}(\mathcal{S})\) bounds the expected supremum of the process \((G_{n}(s))_{s\in\mathcal{S}}\) while applying it on \(\mathcal{D}(t_{*})\) bounds that of \((\Delta_{n}(t,t_{*}))_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\). To control the supremum of \((\Lambda_{n}(t))_{t\in\mathcal{T}}\), the key idea is to notice that it can be expressed as the maximum eigenvalue of a block diagonal matrix whose blocks are \(\sqrt{n}(I-\Sigma^{-1/2}(t)\Sigma_{n}(t)\Sigma^{-1/2}(t))\). Looking at Lemma 3, the relevant parameter is therefore a block diagonal matrix \(V\) with the following blocks

\[V(t):=\mathrm{E}\bigg{[}\Big{(}\Sigma^{-1/2}(t)\phi_{t}(X)\phi_{t}(X)^{T} \Sigma^{-1/2}(t)-I\Big{)}^{2}\bigg{]}.\]

As the bound in Lemma 3 depends only on the maximum eigenvalue of \(V\), the ordering of the blocks does not matter. Putting together these estimates, we arrive at a fully explicit version of Theorem 4.

**Corollary 2**.: _Assume that \(\mathcal{T}\) is finite and that for all \((t,j)\in\mathcal{T}\times[d]\), \(\operatorname{E}\!\left[\phi_{4,j}^{4}(X)\right]<\infty,\operatorname{E}\!\left[Y ^{4}\right]<\infty\). Let \(\delta\in(0,1)\), \(k\in[1+|\mathcal{T}\setminus\mathcal{T}_{*}|]\), and \(c(\cdot),\sigma^{2}(\cdot),r_{n}(\cdot)\) as in Lemma 2. If, for some \(t_{*}\in\mathcal{T}_{*}\),_

\[n\geq(512\lambda_{\text{max}}(V)+6)\log(ed|\mathcal{T}|)+(128L+ 11)\log(6/\delta)\\ +24\cdot\delta^{-1}\cdot c(|\mathcal{T}|)\sigma^{2}(\mathcal{D}( t_{*}))+10\cdot\delta^{-1/2}\cdot c^{2}(|\mathcal{T}|)r_{n}(\mathcal{D}(t_{*})),\] (8)

_then, with probability at least \(1-\delta\)_

\[\hat{t}_{n}\in\widetilde{F}_{n,\delta/2k}^{k}(\mathcal{T})=:\widetilde{S}_{ n,\delta,k},\]

_and_

\[\mathcal{E}(\hat{t}_{n},\hat{w}_{n})\leq 24\cdot(n\delta)^{-1}\cdot A( \widetilde{S}_{n,\delta,k}),\]

_where, for \(\mathcal{S}\subset\mathcal{T}\),_

\[A(\mathcal{S})\coloneqq c^{2}(\mathcal{S})\cdot\Big{(}\sigma(\mathcal{G}( \mathcal{S}))+c(\mathcal{S})\cdot\tfrac{r_{n}(\mathcal{G}(\mathcal{S}))}{ \sqrt{n}}\Big{)}^{2},\]

_and \(\widetilde{F}_{n,\delta}(\mathcal{S})\) is the same as \(F_{n,\delta}(\mathcal{S})\) defined in (7) but with \(A(\mathcal{S})\) replacing \(\operatorname{E}\!\left[\sup_{s\in\mathcal{S}}G_{n}^{2}(s)\right]\)._

We make a few remarks about Corollary 2; a proof sketch is in Appendix I. The set function \(A(\mathcal{S})\) controlling the contraction rate of the map \(\widetilde{F}_{n,\delta}\) as well as the excess risk, has a pleasantly simple form. To first order, and ignoring constants, it is given by

\[(1+\log|\mathcal{S}|)\cdot\max_{s\in\mathcal{S}}\operatorname{E}\!\left[\|g(s,(X,Y))\|_{\Sigma^{-1}(s)}^{2}\right].\]

As such, as \(n\to\infty\) and by Lemma 1, the upper bound on the excess risk in Corollary 2 matches the main term in the asymptotic rate derived in Theorem 3, as can be seen from (6). As the sets \(\widetilde{S}_{n,\delta,k}\) are shrinking with \(n\), the above expression clearly shows the decaying effect of the global complexity of \(\mathcal{T}\) on the excess risk. Finally, we note that the restriction on \(k\) in Corollary 2 is there only because after at most that many iterations, a fixed point is reached, and further iterations worsen the bound. We conclude this section with an example of an application of our results.

**Example 1** (Sparse linear regression).: Consider the sparse linear regression problem, and in particular the best subset selection (BSS) procedure [10, 11]. This procedure corresponds to ERM over the restricted linear class \(\{x\mapsto\langle w,\phi(x)\rangle\mid\|w\|_{0}\leq s\}\) in the linear regression setup of Section 2, where \(\|w\|_{0}\) is the number of non-zero entries of \(w\) and \(s\in[d]\) is a user-chosen sparsity level.

The problem of computing the BSS procedure has attracted a lot of attention recently. While NP-hard and therefore difficult in the worst case [20], Bertsimas et al. [1] showed that it can be tractable on practical instances of moderate size. Since then, a rich literature has emerged that devises increasingly efficient methods [e.g. 1, 1, 2, 13, 14]. By comparison, the statistical performance of the BSS procedure is not yet completely understood as we discuss below.

To see how the sparse linear regression problem fits in our feature learning setting, notice that \(\{x\mapsto\langle w,\phi(x)\rangle\mid\|w\|_{0}\leq s\}=\{x\mapsto\langle v, \phi_{t}(x)\rangle\mid(t,v)\in\mathcal{T}\times\mathbb{R}^{s}\}\) where \(\mathcal{T}\) is the set of all subsets of \([d]\) of size \(s\), and \(\phi_{t}(x):=(\phi_{j_{1}}(x),\phi_{j_{2}}(x),\ldots,\phi_{j_{s}}(x))\in\mathbb{ R}^{s}\) where \((j_{1},j_{2},\ldots,j_{s})\) are the elements of \(t\) in increasing order. As such, Corollaries 1 and 2 are immediately applicable and provide general statements on the performance of an arbitrary BSS procedure. To simplify the discussion, we assume for the rest of the example that there is a unique risk minimizer \(w_{*}\) satisfying \(\|w_{*}\|_{0}=s\).

On the recovery side, the first item of Corollary 1 guarantees that we asymptotically exactly recover the support of \(w_{*}\). Non-asymptotically, the first item of Corollary 2 shows that if \(n\) further satisfies

\[n>\min_{k\in\left[\binom{d}{s}\right]}\Bigl{\{}4k\cdot(\gamma\delta)^{-1} \cdot A(\widetilde{F}_{n,\delta/2k}^{k-1}(\mathcal{T}))\Bigr{\}}\quad\text{ where}\quad\gamma:=\min_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\{R(t,w_{*}(t))-R_{*}\},\]

then with probability at least \(1-\delta\), the BSS procedure recovers the support of \(w_{*}\). Equivalently, these two statements say that for large enough \(n\), the BSS procedure coincides with the oracle procedure which knows the support of \(w_{*}\) a priori and outputs an ERM from the optimal linear class.

In practice however, the interesting regime is when \(n\) is only moderately large. Corollary 2 provides our guarantee in this case, and as such, we turn our attention to the sample size restriction (8). Typically, we expect the main restriction to come from the first term, which in this case is given by \(\lambda_{\text{max}}(V)\cdot s\log(d/s)\), up to constants and lower order terms. This is because if an intercept isincluded, i.e. \(\phi_{1}(X)=1\), then \(\lambda_{\max}(V)\geq s-1\), so the first term scales as \(s^{2}\log(d/s)\) at least, while the remaining terms typically grow more slowly with \(s\). As a concrete example, when \(\phi(X)\) is a Gaussian vector, \(\lambda_{\text{max}}(V)=s+1\), so in this case the estimate \(s^{2}\log(d/s)\) is tight. Under this sample size restriction, and if \(\varepsilon:=Y-\langle w_{*},\phi(X)\rangle\) satisfies \(\operatorname{E}\!\left[\varepsilon^{2}\mid X\right]\leq\sigma^{2}\), Corollary 2 upper bounds the excess risk by \((\sigma^{2}s/n)\cdot a_{n}\) for a sequence of decreasing distribution-dependent constants \(a_{n}\) converging to one as \(n\to\infty\), ignoring the dependence on \(\delta\) and absolute constants.

The closest existing result in the literature we are aware of is due to Shen et al. [20], who arrived at comparable conclusions but in a substantially different setting. In particular, their result was obtained in the setting \(n,d\to\infty\), with an implicit assumption on the distribution of \(\phi(X)\)[20, Equation 2], and dealt with the in-sample prediction risk instead of the excess risk. Another closely related result is due to Raskutti et al. [21] who showed that the minimax expected excess risk in a well-specified fixed-design setting is, up to constants, \(\sigma^{2}s\log(d/s)/n\); see also [1, Chapter 8]. Our results show that for moderate \(n\), in the random-design setting, and when focusing on a single instance, the \(\log(d/s)\) factor can be replaced with another factor that decays to one as \(n\to\infty\).

Coming back to the sample size restriction discussion, we strongly suspect that the factor \(s\log(d/s)\) is suboptimal, but we are unsure what the correct dependence is, even under Gaussian \(\phi(X)\). Indeed, this factor comes from the logarithmic factor in Lemma 3, when applied to the block diagonal matrix with blocks \(\sqrt{n}\big{(}I-\Sigma^{-1/2}(t)\Sigma_{n}(t)\Sigma^{-1/2}(t)\big{)}\). One can improve this factor by instead using versions of this inequality based on the intrinsic dimension [14, Chapter 7]. However, this is also unlikely to be tight. Roughly speaking, this is because such logarithmic factors are tight only when the eigenvalues of the random matrix are near-independent. This is certainly not the case for the block diagonal matrix we are considering, since its blocks are sample covariance matrices of sub-vectors of the same random vector \(\phi(X)\). Capturing this dependence is beyond our reach and likely requires new tools; we refer the interested reader to the recent articles [13, 15, 21].

## 5 Conclusion

Broadly speaking, there are two main conclusions one can draw from this work. Firstly, in the large sample regime, and if the set of candidate feature maps is not too large under an appropriate measure of size, asking a model to additionally pick a feature map on top of learning a linear predictor has a negligible effect on the excess risk of ERM on regression problems with square loss. Secondly, for moderate sample sizes, the magnitude of this effect depends on the appropriately measured size of the subbevel sets of the suboptimality function \(t\mapsto R(t,w_{*}(t))-R_{*}\). Plainly, learning feature maps is easy when only a small subset of them is good, as the bad ones can be quickly discarded.

The most tantalizing aspect of our results is their potential in explaining the experiments in [16]. It was shown there that complex neural networks trained by ERM were able to achieve good performance despite being expressive enough to fit random labels. This is paradoxical if one assumes that the performance of ERM is driven by the complexity of the model class. Our results refute this assumption for a generic collection of feature-learning-based models. While there are many works offering explanations for this apparent paradox (see e.g. [2] for a survey), we are not aware of one that shows the vanishing influence of the size of the model class on the excess risk as Theorems 3 and 4 show. Formally connecting our statements to these experiments is beyond what we achieved here, yet, we believe that the new perspective we took might generate useful insights in this area.

We conclude by outlining a few limitations of our work. Firstly, we do not deal with the question of how to solve the ERM problem. Our focus is on understanding its statistical performance, and our setting is so general that such a question cannot be meaningfully tackled. Continuing on this last point, while the generality of our results is desirable in some aspects, it is detrimental in others. As an example, it would be desirable to specialize our results from Section 3 to specific infinite collections of feature maps used in practice. Let us also mention that it is a priori unclear whether ERM is an optimal procedure, in a minimax sense, for the model classes we consider; we suspect that recently developed tools might be relevant to address this question [17]. Finally, while we focused on the case of regression with square loss, this was mostly done to simplify the presentation. Indeed, the only property of the loss used in the proofs is the exactness of its second order Taylor expansion. This is however not required if one can control the error term from above and below. It is known how to do this for many loss functions [e.g. 1, 2], and most importantly for logistic regression [1, 1]. We have purposefully selected generic notation to make translating such arguments easier.

## Acknowledgments and Disclosure of Funding

Resources used in preparing this research were provided in part by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. CM acknowledges the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), RGPIN-2021-03445. MAE was partially supported by NSERC Grant [2019-06167], CIFAR AI Chairs program, and CIFAR AI Catalyst grant.

## References

* [AC11] J.-Y. Audibert and O. Catoni. "Robust Linear Least Squares Regression". In: _The Annals of Statistics_ (2011). url.
* [Aud07] J.-y. Audibert. "Progressive Mixture Rules Are Deviation Suboptimal". In: _Advances in Neural Information Processing Systems_. 2007. url.
* [Ba+22] J. Ba, M. A. Erdogdu, T. Suzuki, Z. Wang, D. Wu, and G. Yang. "High-Dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation". In: _Advances in Neural Information Processing Systems_ (Dec. 6, 2022). url.
* [Bac10] F. Bach. "Self-Concordant Analysis for Logistic Regression". In: _Electronic Journal of Statistics_ (Jan. 2010). doi: 10.1214/09-EJS521.
* [Bac14] F. Bach. "Adaptivity of Averaged Stochastic Gradient Descent to Local Strong Convexity for Logistic Regression". In: _Journal of Machine Learning Research_ (2014). url.
* [Bac17] F. Bach. "Breaking the Curse of Dimensionality with Convex Neural Networks". In: _Journal of Machine Learning Research_ (2017). url.
* [Bac24] F. Bach. _Learning Theory from First Principles_. Dec. 24, 2024.
* [BBM05] P. L. Bartlett, O. Bousquet, and S. Mendelson. "Local Rademacher Complexities". In: _The Annals of Statistics_ (Aug. 2005). doi: 10.1214/00905360500000282.
* [BBvH23] A. S. Bandeira, M. T. Boedihardjo, and R. van Handel. "Matrix Concentration Inequalities and Free Probability". In: _Inventiones mathematicae_ (Oct. 1, 2023). doi: 10.1007/s00222-023-01204-6.
* [BKM16] D. Bertsimas, A. King, and R. Mazumder. "Best Subset Selection via a Modern Optimization Lens". In: _The Annals of Statistics_ (Apr. 2016). doi: 10.1214/15-AOS1388.
* [BLM00] S. Boucheron, G. Lugosi, and P. Massart. "A Sharp Concentration Inequality with Applications". In: _Random Structures & Algorithms_ (2000). doi: 10.1002/(SICI)1098-2418(200005)16:3<277::AID-RSA4>3.0.CO;2-1.
* [BLM13] S. Boucheron, G. Lugosi, and P. Massart. _Concentration Inequalities: A Nonasymptotic Theory of Independence_. Feb. 7, 2013.
* [BM06] P. L. Bartlett and S. Mendelson. "Empirical Minimization". In: _Probability Theory and Related Fields_ (July 1, 2006). doi: 10.1007/s00440-005-0462-3.
* [BMR21] P. L. Bartlett, A. Montanari, and A. Rakhlin. "Deep Learning: A Statistical Viewpoint". In: _Acta Numerica_ (May 2021). doi: 10.1017/S0962492921000027.
* [Bou02] O. Bousquet. "A Bennett Concentration Inequality and Its Application to Suprema of Empirical Processes". In: _Comptes Rendus Mathematique_ (Jan. 1, 2002). doi: 10.1016/S1631-073X(02)02292-6.
* [BP20] D. Bertsimas and B. V. Parys. "Sparse High-Dimensional Regression: Exact Scalable Algorithms and Phase Transitions". In: _The Annals of Statistics_ (Feb. 2020). doi: 10.1214/18-AOS1804.
* [BPV20] D. Bertsimas, J. Pauphilet, and B. Van Parys. "Sparse Regression: Scalable Algorithms and Empirical Performance". In: _Statistical Science_ (2020). url.
* [COB19] L. Chizat, E. Oyallon, and F. Bach. "On Lazy Training in Differentiable Programming". In: _Advances in Neural Information Processing Systems_. 2019. url.
* [DI17] G. David and Z. Ilias. "High Dimensional Regression with Binary Coefficients. Estimating Squared Error and a Phase Transition". In: _Proceedings of the 2017 Conference on Learning Theory_. June 18, 2017. url.
* [EE23] A. El Hanchi and M. A. Erdogdu. "Optimal Excess Risk Bounds for Empirical Risk Minimization on \(p\)-Norm Linear Regression". In: _Advances in Neural Information Processing Systems_ (Dec. 15, 2023). url.

* [EME24] A. El Hanchi, C. Maddison, and M. Erdogdu. "Minimax Linear Regression under the Quantile Risk". In: _Proceedings of Thirty Seventh Conference on Learning Theory_. June 30, 2024. url.
* [GA11] M. Gonen and E. Alpaydin. "Multiple Kernel Learning Algorithms". In: _Journal of Machine Learning Research_ (2011). url.
* [Gho+19] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. "Limitations of Lazy Training of Two-layers Neural Network". In: _Advances in Neural Information Processing Systems_. 2019. url.
* [Gho+20] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. "When Do Neural Networks Outperform Kernel Methods?" In: _Advances in Neural Information Processing Systems_. 2020. url.
* [GN15] E. Gine and R. Nickl. _Mathematical Foundations of Infinite-Dimensional Statistical Models_. 2015. doi: 10.1017/CBO9781107337862.
* [GR04] E. Greenshtein and Y. Ritov. "Persistence in High-Dimensional Linear Predictor Selection and the Virtue of Overparametrization". In: _Bernoulli_ (Dec. 2004). doi: 10.3150/bj/1106314846.
* [Gre06] E. Greenshtein. "Best Subset Selection, Persistence in High-Dimensional Statistical Learning and Optimization under L1 Constraint". In: _The Annals of Statistics_ (Oct. 2006). doi: 10.1214/009053606000000768.
* [Guy+24] T. Guyard, C. Herzet, C. Elvira, and A.-N. Arslan. "A New Branch-and-Bound Pruning Framework for \(\ell_{0}\)-Regularized Problems". In: _Proceedings of the 41st International Conference on Machine Learning_. July 8, 2024. url.
* [He+16] K. He, X. Zhang, S. Ren, and J. Sun. "Deep Residual Learning for Image Recognition". In: _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_. June 2016. doi: 10.1109/CVPR.2016.90.
* [HKZ12] D. Hsu, S. M. Kakade, and T. Zhang. "Random Design Analysis of Ridge Regression". In: _Proceedings of the 25th Annual Conference on Learning Theory_. June 16, 2012. url.
* [HMS22] H. Hazimeh, R. Mazumder, and A. Saab. "Sparse Regression at Scale: Branch-and-Bound Rooted in First-Order Optimization". In: _Mathematical Programming_ (Nov. 1, 2022). doi: 10.1007/s10107-021-01712-4.
* [HTF09] T. Hastie, R. Tibshirani, and J. Friedman. _The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition_. Aug. 26, 2009.
* [Hua+18] J. Huang, Y. Jiao, Y. Liu, and X. Lu. "A Constructive Approach to \(L_{0}\) Penalized Regression". In: _Journal of Machine Learning Research_ (2018). url.
* [JRT08] A. Juditsky, P. Rigollet, and A. B. Tsybakov. "Learning by Mirror Averaging". In: _The Annals of Statistics_ (Oct. 2008). doi: 10.1214/07-AOS546.
* [Kel+23] J. Kelner, F. Koehler, R. Meka, and D. Rohatgi. "Feature Adaptation for Sparse Linear Regression". In: _Advances in Neural Information Processing Systems_ (Dec. 15, 2023). url.
* [KM15] V. Koltchinskii and S. Mendelson. "Bounding the Smallest Singular Value of a Random Matrix Without Concentration". In: _International Mathematics Research Notices_ (Jan. 1, 2015). doi: 10.1093/imrn/rnv096.
* [Kol06] V. Koltchinskii. "Local Rademacher Complexities and Oracle Inequalities in Risk Minimization". In: _The Annals of Statistics_ (Dec. 2006). doi: 10. 1214 / 00905360600001019.
* [Kol11] V. Koltchinskii. _Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems: Ecole D'Ete de Probabilites de Saint-Flour XXXVIII-2008_. July 29, 2011.
* [KRV22] V. Kanade, P. Rebeschini, and T. Vaskevicius. _Exponential Tail Local Rademacher Complexity Risk Bounds Without the Bernstein Condition_. Feb. 23, 2022. doi: 10. 48550/arXiv.2202.11461.
* [KSH12] A. Krizhevsky, I. Sutskever, and G. E. Hinton. "ImageNet Classification with Deep Convolutional Neural Networks". In: _Advances in Neural Information Processing Systems_. 2012. url.

* [Lan+04] G. R. G. Lanckriet, N. Cristianini, P. Bartlett, L. E. Ghaoui, and M. I. Jordan. "Learning the Kernel Matrix with Semidefinite Programming". In: _Journal of Machine Learning Research_ (2004). url.
* [LBH15] Y. LeCun, Y. Bengio, and G. Hinton. "Deep Learning". In: _Nature_ (May 28, 2015). doi: 10.1038/nature14539.
* [LC06] E. L. Lehmann and G. Casella. _Theory of Point Estimation_. May 2, 2006.
* [LL20] G. Lecue and M. Lerasle. "Robust Machine Learning by Median-of-Means: Theory and Practice". In: _The Annals of Statistics_ (Apr. 2020). doi: 10.1214/19-AOS1828.
* [LM09] G. Lecue and S. Mendelson. "Aggregation via Empirical Risk Minimization". In: _Probability Theory and Related Fields_ (Nov. 1, 2009). doi: 10.1007/s00440-008-0180-8.
* [LM16a] G. Lecue and S. Mendelson. _Learning Subgaussian Classes : Upper and Minimax Bounds_. Sept. 17, 2016. doi: 10.48550/arXiv.1305.4825.
* [LM16b] G. Lecue and S. Mendelson. "Performance of Empirical Risk Minimization in Linear Aggregation". In: _Bernoulli_ (Aug. 2016). doi: 10.3150/15-BEJ701.
* [LM19] G. Lugosi and S. Mendelson. "Risk Minimization by Median-of-Means Tournaments". In: _Journal of the European Mathematical Society_ (Dec. 16, 2019). doi: 10.4171/jems/937.
* [LRS15] T. Liang, A. Rakhlin, and K. Sridharan. "Learning with Square Loss: Localization through Offset Rademacher Complexity". In: _Proceedings of The 28th Conference on Learning Theory_. June 26, 2015. url.
* [LvHY18] R. Latala, R. van Handel, and P. Youssef. "The Dimension-Free Structure of Nonhomogeneous Random Matrices". In: _Inventiones mathematicae_ (Dec. 1, 2018). doi: 10.1007/s00222-018-0817-x.
* [Men14] S. Mendelson. "Learning without Concentration". In: _Proceedings of The 27th Conference on Learning Theory_. May 29, 2014. url.
* [Mil02] A. Miller. _Subset Selection in Regression_. Apr. 14, 2002. doi: 10. 1201 /9781420035933.
* [Mou22] J. Mourtada. "Exact Minimax Risk for Linear Least Squares, and the Lower Tail of Sample Covariance Matrices". In: _The Annals of Statistics_ (Aug. 2022). doi: 10.1214/22-AOS2181.
* [Nat95] B. K. Natarajan. "Sparse Approximate Solutions to Linear Systems". In: _SIAM Journal on Computing_ (Apr. 1995). doi: 10.1137/S0097539792240406.
* [OB21] D. M. Ostrovskii and F. Bach. "Finite-Sample Analysis of \(M\)-Estimators Using Self-Concordance". In: _Electronic Journal of Statistics_ (Jan. 2021). doi: 10.1214/20-EJS1780.
* [Oli16] R. I. Oliveira. "The Lower Tail of Random Quadratic Forms with Applications to Ordinary Least Squares". In: _Probability Theory and Related Fields_ (Dec. 1, 2016). doi: 10.1007/s00440-016-0738-9.
* [PG99] V. de la Pena and E. Gine. _Decoupling: From Dependence to Independence_. 1999.
* [PWE15] M. Pilanci, M. J. Wainwright, and L. El Ghaoui. "Sparse Learning via Boolean Relaxations". In: _Mathematical Programming_ (June 1, 2015). doi: 10.1007/s10107-015-0894-1.
* [RWY11] G. Raskutti, M. J. Wainwright, and B. Yu. "Minimax Rates of Estimation for High-Dimensional Linear Regression Over \(\ell_{q}\)-Balls". In: _IEEE Transactions on Information Theory_ (Oct. 2011). doi: 10.1109/TIT.2011.2165799.
* [Sau18] A. Saumard. "On Optimality of Empirical Risk Minimization in Linear Aggregation". In: _Bernoulli_ (2018). url.
* [SD16] A. Sinha and J. C. Duchi. "Learning Kernels with Random Features". In: _Advances in Neural Information Processing Systems_. 2016. url.
* [She+13] X. Shen, W. Pan, Y. Zhu, and H. Zhou. "On Constrained and Regularized High-Dimensional Regression". In: _Annals of the Institute of Statistical Mathematics_ (Oct. 1, 2013). doi: 10.1007/s10463-012-0396-3.
* [SPZ12] X. Shen, W. Pan, and Y. Zhu. "Likelihood-Based Selection and Sharp Parameter Estimation". In: _Journal of the American Statistical Association_ (June 11, 2012). doi: 10.1080/01621459.2011.645783.

* [Tal96] M. Talagrand. "New Concentration Inequalities in Product Spaces". In: _Inventiones mathematicae_ (Nov. 1, 1996). doi: 10.1007/s002220050108.
* [Tro15] J. A. Tropp. "An Introduction to Matrix Concentration Inequalities". In: _Found. Trends Mach. Learn._ (May 1, 2015). doi: 10.1561/2200000048.
* [Tro16] J. A. Tropp. "The Expected Norm of a Sum of Independent Random Matrices: An Elementary Approach". In: _High Dimensional Probability VII_. Ed. by C. Houdre, D. M. Mason, P. Reynaud-Bouret, and J. Rosinski. 2016. doi: 10.1007/978-3-319-40519-3_8.
* [Vaa98] A. W. van der Vaart. _Asymptotic Statistics_. 1998. doi: 10.1017/CBO9780511802256.
* [Vas+17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. ukasz Kaiser, and I. Polosukhin. "Attention Is All You Need". In: _Advances in Neural Information Processing Systems_. 2017. url.
* [VC74] V. Vapnik and A. Chervonenkis. _Theory of Pattern Recognition_. 1974.
* [vHan17] R. van Handel. "Structured Random Matrices". In: _Convexity and Concentration_. 2017. doi: 10.1007/978-1-4939-7005-6_4.
* [VW96] A. van der Vaart and J. A. Wellner. _Weak Convergence and Empirical Processes: With Applications to Statistics_. Mar. 14, 1996.
* [Wai19] M. J. Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. 2019. doi: 10.1017/978110862771.
* [Whi82] H. White. "Maximum Likelihood Estimation of Misspecified Models". In: _Econometrica_ (1982). doi: 10.2307/1912526.
* [Zha+21] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. "Understanding Deep Learning (Still) Requires Rethinking Generalization". In: _Commun. ACM_ (Feb. 22, 2021). doi: 10.1145/3446776.
* [Zhu+20] J. Zhu, C. Wen, J. Zhu, H. Zhang, and X. Wang. "A Polynomial Algorithm for Best-Subset Selection Problem". In: _Proceedings of the National Academy of Sciences_ (Dec. 29, 2020). doi: 10.1073/pnas.2014241117.
* [ZWJ14] Y. Zhang, M. J. Wainwright, and M. I. Jordan. "Lower Bounds on the Performance of Polynomial-time Algorithms for Sparse Linear Regression". In: _Proceedings of The 27th Conference on Learning Theory_. May 29, 2014. url.
* [ZWJ17] Y. Zhang, M. J. Wainwright, and M. I. Jordan. "Optimal Prediction for Sparse Linear Models? Lower Bounds for Coordinate-Separable M-estimators". In: _Electronic Journal of Statistics_ (Jan. 2017). doi: 10.1214/17-EJS1233.
* [ZZ12] C.-H. Zhang and T. Zhang. "A General Theory of Concave Regularization for High-Dimensional Sparse Estimation Problems". In: _Statistical Science_ (Nov. 2012). doi: 10.1214/12-STS399.

Proof of Theorem 1

Let \(A_{n}\) denote the event that \(\Sigma_{n}\) is invertible. By the weak law of large numbers, \(\Sigma_{n}\) converges to \(\Sigma\) in probability so that \(\lim_{n\to\infty}\mathrm{P}(A_{n}^{c})=0\). Now on the event \(A_{n}\), we have by (1)

\[\sqrt{n}\cdot(\hat{w}_{n}-w_{*})=\Sigma_{n}^{-1}\cdot(\sqrt{n}\cdot\nabla R_{n} (w_{*})).\]

By the continuous mapping theorem, \(\Sigma_{n}^{-1}\) converges to \(\Sigma^{-1}\) in probability and by the central limit theorem

\[\sqrt{n}\cdot\nabla R_{n}(w_{*})\stackrel{{ d}}{{\to}}\mathcal{N }(0,G).\]

Therefore, by Slutsky's theorem

\[\sqrt{n}\cdot(\hat{w}_{n}-w_{*})\stackrel{{ d}}{{\to}}\mathcal{N }(0,\Sigma^{-1}G\Sigma^{-1}).\]

Now since the risk is quadratic and the gradient vanishes at \(w_{*}\),

\[n\cdot[R(\hat{w})-R(w_{*})]=\frac{1}{2}\cdot\|\sqrt{n}\cdot(\hat{w}-w_{*})\|_ {\Sigma}^{2}\stackrel{{ d}}{{\to}}\frac{1}{2}\|Z\|_{2}^{2},\]

where \(Z\) is as in the theorem, and where the last statement follows by the continuous mapping theorem. This proves the first statement. The bounds on the quantiles are a consequence of concentration bounds for the norm of Gaussian vectors [e.g. 2, Corollary 33].

## Appendix B Proof of Theorem 2

Denote by \(A_{n}\) the event that

\[\lambda_{\text{min}}\Big{(}\Sigma^{-1/2}\Sigma_{n}\Sigma^{-1/2}\Big{)}\geq \frac{1}{2}.\]

We show that under the sample size restriction, \(\mathrm{P}(A_{n})\geq 1-\delta/2\). Indeed we have the variational representation

\[\lambda_{\text{max}}(I-\Sigma^{-1/2}\Sigma_{n}\Sigma^{-1/2})=\sup_{v\in S^{d-1 }}\frac{1}{n}\sum_{i=1}^{n}1-\langle v,\Sigma^{-1/2}\phi(X_{i})\rangle^{2}.\]

Each element in the sum is upper bounded by \(1\), and the variance parameter in Bousquet's concentration inequality [1] is given by the parameter \(L\) in the statement of the theorem. Applying this inequality yields that with probability at least \(1-\delta/2\)

\[\lambda_{\text{max}}(I-\Sigma^{-1/2}\Sigma_{n}\Sigma^{-1/2})\leq 2\,\mathrm{E} \Big{[}\lambda_{\text{max}}(I-\Sigma^{-1/2}\Sigma_{n}\Sigma^{-1/2})\Big{]}+ \sqrt{\frac{2L\log(2/\delta)}{n}}+\frac{4\log(2/\delta)}{3n}.\]

Using Lemma 3 to upper bound the above expectation, and replacing the sample size \(n\) in the resulting inequality with the minimal allowed by the theorem proves that \(\mathrm{P}(A_{n})\geq 1-\delta/2\) for all sample sizes allowable by the theorem. Now on this event we have, using (2),

\[R(\hat{w}_{n})-R(w_{*})=\frac{1}{2}\cdot\|\Sigma_{n}^{-1}\nabla R_{n}(w_{*})\| _{\Sigma}^{2}\leq 2\cdot\|\nabla R_{n}(w_{*})\|_{\Sigma^{-1}}^{2}.\]

An elementary calculation shows

\[\mathrm{E}\big{[}\|\nabla R_{n}(w_{*})\|_{\Sigma^{-1}}^{2}\big{]}=n^{-1}\, \mathrm{E}\big{[}\|g(X,Y)\|_{\Sigma^{-1}}^{2}\big{]},\]

so that an application of Markov's inequality yields that there is an event \(B_{n}\) that holds with probability at least \(1-\delta/2\) and on which

\[\|\nabla R_{n}(w_{*})\|_{\Sigma^{-1}}^{2}\leq 2\cdot(n\delta)^{-1}\cdot \mathrm{E}\big{[}\|g(X,Y)\|_{\Sigma^{-1}}^{2}\big{]}.\]

The union bound \(\mathrm{P}(A_{n}\cap B_{n})=1-\mathrm{P}(A_{n}\cup B_{n})\geq 1-\delta\) finishes the proof.

Main Lemma

We state here a core lemma, which we use in many of our proofs. To state it, we define, for a function \(F:\mathcal{S}\to\mathbb{R}\) on a subset \(\mathcal{S}\subseteq\mathcal{T}\),

\[\|F\|_{\infty}:=\sup_{s\in\mathcal{S}}\lvert F(s)\rvert,\quad\|F\|_{\infty,-}:= \sup_{s\in\mathcal{S}}\{-F(s)\},\quad\|F\|_{\infty,+}:=\sup_{s\in\mathcal{S}}F( s),\]

where the first quantity is the \(\ell^{\infty}\) norm of the function \(F\), and the remaining are one-sided variants of it. The processes appearing in the next statement are defined in (3) and (4).

**Lemma 4**.: _Assume that \(\mathcal{T}_{*}\neq\varnothing\) and let \(t_{*}\in\mathcal{T}_{*}\). On the event that \(\|n^{-1/2}\Delta_{n}(\cdot,t_{*})\|_{\infty,+}<1\) and \(\|n^{-1/2}\Lambda_{n}\|_{\infty,+}<1\), we have_

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\leq\frac{1}{2}\cdot\frac{1}{1-\|n^{-1 /2}\Delta_{n}(\cdot,t_{*})\|_{\infty,+}}\cdot\frac{1}{1-\|n^{-1/2}\Lambda_{n} \|_{\infty,+}}\cdot(n^{-1}G_{n}^{2}(\hat{t}_{n})),\]

_and_

\[\frac{1}{2}\cdot\frac{n^{-1}G_{n}^{2}(\hat{t}_{n})}{(1+\|n^{-1/2}\Lambda_{n} \|_{\infty,-})^{2}}\leq R(\hat{t}_{n},\hat{w}_{n})-R(\hat{t}_{n},w_{*}(\hat{t} _{n}))\leq\frac{1}{2}\cdot\frac{n^{-1}G_{n}^{2}(\hat{t}_{n})}{(1-\|n^{-1/2} \Lambda_{n}\|_{\infty,+})^{2}}.\]

Proof.: To lighten the notation, we drop the dependence on \(n\), and write \(\hat{t}\) instead of \(\hat{t}_{n}\). We start with the first statement. First, we note that if \(\hat{t}\in\mathcal{T}_{*}\), then the statement holds trivially as the left-hand side is zero, so we only consider the other case in what follows. For any \(t\in\mathcal{T}\), define

\[\hat{w}(t)\in\operatorname*{argmin}_{w\in\mathbb{R}^{d}}R_{n}(t,w),\]

where the choice of minimizer is arbitrary. With this definition, we have \(\hat{w}_{n}=\hat{w}(\hat{t})\). Now, by definition of ERM,

\[R_{n}(\hat{t},\hat{w}(\hat{t}))-R_{n}(t_{*},w_{*}(t_{*}))\leq 0.\] (9)

On the other hand, for any \(t\in\mathcal{T}\setminus\mathcal{T}_{*}\), we have the decomposition

\[R_{n}(t,\hat{w}(t))-R_{n}(t_{*},w_{*})=[R_{n}(t,\hat{w}(t))-R_{n}(t,w_{*}(t))] +[R_{n}(t,w_{*}(t))-R_{n}(t_{*},w_{*}(t_{*}))].\] (10)

We study each of the terms of (10) separately, and we start with the first. Note that since we are in the event

\[\inf_{t\in\mathcal{T}}\lambda_{\text{min}}(\Sigma^{-1/2}(t)\Sigma_{n}(t) \Sigma^{-1/2}(t))=1-\|n^{-1/2}\Lambda_{n}\|_{\infty,+}>0,\]

the sample covariance matrices \(\Sigma_{n}(t)\) are invertible for all \(t\in\mathcal{T}\), so that \(\hat{w}(t)\) is uniquely defined and satisfies

\[\hat{w}(t)=w_{*}(t)-\Sigma_{n}^{-1}(t)\nabla_{w}R_{n}(t,w_{*}(t)).\] (11)

Furthermore, since the function \(w\mapsto R_{n}(t,w)\) is quadratic in \(w\) and its gradient vanishes at its minimizer \(\hat{w}(t)\), we have

\[R_{n}(t,\hat{w}(t))-R_{n}(t,w_{*}(t))=-\frac{1}{2}\|\hat{w}(t)-w_{*}(t)\|_{ \Sigma_{n}(t)}^{2}=-\frac{1}{2}\|\nabla_{w}R_{n}(t,w_{*}(t))\|_{\Sigma_{n}^{-1 }(t)}^{2},\] (12)

where the last equality follows from (11). To bound this last term, define

\[\widetilde{\Sigma}_{n}(t):=\Sigma^{-1/2}(t)\Sigma_{n}(t)\Sigma^{-1/2}(t).\]

Then we have,

\[\|\nabla_{w}R_{n}(t,w_{*}(t))\|_{\Sigma_{n}^{-1}(t)}^{2} =\Big{\{}\Sigma^{-1/2}(t)\nabla_{w}R_{n}(t,w_{*}(t))\Big{\}}^{T} \widetilde{\Sigma}_{n}^{-1}(t)\Big{\{}\Sigma^{-1/2}(t)\nabla_{w}R_{n}(t,w_{*}( t))\Big{\}}\] \[\leq\lambda_{\text{max}}(\widetilde{\Sigma}_{n}^{-1}(t))\cdot\| \nabla_{w}R_{n}(t,w_{*}(t))\|_{\Sigma^{-1}(t)}^{2}\] \[=\frac{1}{1-\lambda_{\text{max}}(I-\widetilde{\Sigma}_{n}(t))} \cdot(n^{-1}G_{n}^{2}(t))\] \[\leq\frac{1}{1-\|n^{-1/2}\Lambda_{n}\|_{\infty,+}}\cdot(n^{-1}G_ {n}^{2}(t)).\] (13)Finally, the second term of (10) is lower bounded by

\[R_{n}(t,w_{*}(t))-R_{n}(t_{*},w_{*}(t_{*}))) =(1-n^{-1/2}\Delta_{n}(t,t_{*}))[R(t,w_{*}(t))-R_{*}]\] \[\geq(1-\|n^{-1/2}\Delta_{n}(\cdot,t_{*})\|_{\infty,+})[R(t,w_{*}(t ))-R_{*}]\] (14)

Combining (13) and (12) lower bounds the first term of (10), while (14) lower bounds the second. Combining the resulting lower bound on (10) with (9) and rearranging yields the first statement.

For the upper bound in the second statement, note that for all \(t\in\mathcal{T}\),

\[R(t,\hat{w}(t))-R(t,w_{*}(t)) =\frac{1}{2}\cdot\|\hat{w}(t)-w_{*}(t)\|_{\Sigma(t)}^{2}\] \[=\frac{1}{2}\cdot\|\Sigma_{n}^{-1}(t)\nabla_{w}R_{n}(t,w_{*}(t)) \|_{\Sigma(t)}^{2}\] \[\leq\frac{1}{2}\cdot\lambda_{\text{max}}(\widetilde{\Sigma}_{n}^ {-2}(t))\cdot\|\nabla_{w}R_{n}(t,w_{*}(t))\|_{\Sigma^{-1}(t)}^{2}\] \[=\frac{1}{2}\cdot\frac{1}{(1-\|n^{-1/2}\Lambda_{n}\|_{\infty,+})^ {2}}\cdot(n^{-1}G_{n}^{2}(t)).\]

where the second line follows from (11). In particular the inequality holds for \(\hat{t}\). The lower bound holds by a similar argument.

## Appendix D Proof of Theorem 3

**Consistency of \(\hat{t}_{n}\).** We want to show that, as \(n\to\infty\),

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\stackrel{{ P}}{{\to}}0.\] (15)

Using the notation introduced in Appendix C, the Glivenko-Cantelli assumptions in Theorem 3 amount to the statements that, for some \(t_{*}\in\mathcal{T}_{*}\), and as \(n\to\infty\),

\[\|n^{-1/2}\Lambda_{n}\|_{\infty}\stackrel{{ P}}{{\to}}0,\quad\| n^{-1/2}\Delta_{n}(\cdot,t_{*})\|_{\infty}\stackrel{{ P}}{{\to}}0,\quad\|n^{-1/2}G_{n}\|_{\infty}\stackrel{{ P}}{{\to}}0.\] (16)

Let \(A_{n}\) denote the event that both \(\|n^{-1/2}\Lambda_{n}\|_{\infty}<1\) and \(\|n^{-1/2}\Delta_{n}(\cdot,t_{*})\|_{\infty}<1\). The union bound and (16) show that

\[\lim_{n\to\infty}\mathrm{P}(A_{n}^{c})=0\]

Furthermore, on the event \(A_{n}\), the first bound of Lemma 4 holds, and bounding \(n^{-1}G_{n}^{2}(\hat{t}_{n})\) by \(\|n^{-1/2}G_{n}\|_{\infty}^{2}\) yields that on \(A_{n}\)

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\leq\frac{1}{2}\cdot\frac{1}{1-\|n^{-1/ 2}\Delta_{n}(\cdot,t_{*})\|_{\infty}}\cdot\frac{1}{1-\|n^{-1/2}\Lambda_{n}\|_ {\infty}}\cdot\|n^{-1/2}G_{n}\|_{\infty}^{2}\] (17)

Now let \(\varepsilon>0\), and denote by \(B_{n}(\varepsilon)\) the event that the right hand side of (17) is strictly larger than \(\varepsilon\). Then the statements (16) together with the continuous mapping theorem show that \(\lim_{n\to\infty}\mathrm{P}(B_{n}(\varepsilon))=0\). Therefore, again by (17), we have

\[\mathrm{P}\big{(}R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}>\varepsilon\big{)} \leq\mathrm{P}(B_{n}(\varepsilon))+\mathrm{P}(A_{n}^{c}),\]

and taking \(n\to\infty\) proves (15).

**Asymptotic quantiles.** We start with the upper bound. We have the simple decomposition

\[n\cdot\big{[}R(\hat{t}_{n},\hat{w}_{n})-R_{*}\big{]}=n\big{[}R(\hat{t}_{n}, \hat{w}_{n})-R(\hat{t}_{n},w_{*}(\hat{t}_{n}))\big{]}+n\big{[}R(\hat{t}_{n},w_{* }(\hat{t}_{n}))-R_{*}\big{]}.\] (18)

Now on the event \(A_{n}\) defined above, we have, by an application of Lemma 4, combining the two bounds in the lemma along with (18), that the rescaled excess risk is upper bounded by

\[\frac{1}{2}\cdot\frac{1}{1-\|n^{-1/2}\Lambda_{n}\|_{\infty}}\cdot\bigg{(}\frac {1}{1-\|n^{-1/2}\Delta_{n}(\cdot,t_{*})\|_{\infty}}+\frac{1}{1-\|n^{-1/2} \Lambda_{n}\|_{\infty}}\bigg{)}\cdot G_{n}^{2}(\hat{t}_{n}).\] (19)From the Glivenko-Cantelli assumptions (16), the first three factors converge in probability to \(1\). Our aim will be to bound the upper tail of the last factor, which will imply a bound on the upper tail of the rescaled excess risk.

We briefly make explicit the Donsker assumption before deriving this bound. Both define and note

\[G_{n}(t,v):=\sqrt{n}\cdot\langle v,\Sigma^{-1/2}(t)\nabla_{w}R_{n}(t,w_{*}(t)) \rangle,\qquad G_{n}(t)=\sup_{v\in S^{d-1}}G_{n}(t,v),\]

where \(S^{d-1}\) is the Euclidean unit sphere in \(\mathbb{R}^{d}\). As pointed out in Section 3, the processes \(G_{n}(t)\) are partial suprema of the empirical processes \(G_{n}(t,v)\). The Donsker assumption of the theorem states that the empirical processes \(G_{n}(t,v)\) take value in the space of bounded functions on \(\mathcal{T}\times S^{d-1}\), equipped with the \(\ell^{\infty}(\mathcal{T}\times S^{d-1})\) norm and the metric it induces, and converge weakly to their unique Gaussian limit \((G(t,v))_{(t,v)\in\mathcal{T}\times S^{d-1}}\) as \(n\to\infty\). By inspecting their finite dimensional distributions, it is straightforward to verify that \((G(t,v))_{(t,v)\in\mathcal{T}\times S^{d-1}}=\frac{d}{(\langle v,Z(t)\rangle)_ {(t,v)\in\mathcal{T}\times S^{d-1}}}\) where \((Z(t))_{t\in\mathcal{T}}\) is the \(\mathbb{R}^{d}\)-valued Gaussian process defined in the statement of Theorem 3. Finally, we define \(G(t):=\sup_{v\in S^{d-1}}G(t,v)\) in analogy with the definition of \(G_{n}(t)\).

We now upper bound the upper tail of \(G_{n}^{2}(\hat{t}_{n})\) in (19). Let \((\varepsilon_{k})_{k=1}^{\infty}\) be a decreasing sequence of positive numbers such that \(\varepsilon_{k}\to 0\) as \(k\to\infty\), and define the sets

\[\mathcal{T}_{*}(\varepsilon):=\{t\in\mathcal{T}\mid R(t,w_{*}(t))-R_{*}\leq \varepsilon\}\] (20)

as well as the function \(F_{k}:\ell^{\infty}(\mathcal{T}\times S^{d-1})\to\mathbb{R}\) by

\[F_{k}(z):=\sup_{s\in\mathcal{T}_{*}(\varepsilon_{k})}\sup_{v\in S^{d-1}}z(s,v).\]

Note on the one hand that \(\cap_{k\geq 1}\mathcal{T}_{*}(\varepsilon_{k})=\mathcal{T}_{*}\), and on the other that \(F_{k}\) is continuous for all \(k\in\mathbb{N}\), and in fact Lipschitz. Indeed, let \(z,z^{\prime}\in\ell^{\infty}(\mathcal{T}\times S^{d-1})\). Then

\[|F_{k}(z)-F_{k}(z^{\prime})|=\left|\sup_{s\in\mathcal{T}_{*}(\varepsilon_{k})} \sup_{v\in S^{d-1}}z(s,v)-\sup_{s\in\mathcal{T}_{*}(\varepsilon_{k})}\sup_{v \in S^{d-1}}z^{\prime}(s,v)\right|\leq\|z-z^{\prime}\|_{\infty}\]

Now let \(k\in\mathbb{N}\) and \(x\in[0,\infty)\). Then

\[\mathrm{P}\big{(}G_{n}^{2}(\hat{t}_{n})>x\big{)} =\mathrm{P}\big{(}\big{\{}G_{n}^{2}(\hat{t}_{n})>x\big{\}}\cap \big{\{}\hat{t}_{n}\in\mathcal{T}_{*}(\varepsilon_{k})\big{\}}\big{)}+ \mathrm{P}\big{(}\big{\{}G_{n}^{2}(\hat{t}_{n})>x\big{\}}\cap\big{\{}\hat{t}_{n }\notin\mathcal{T}_{*}(\varepsilon_{k})\big{\}}\big{)}\] \[\leq\mathrm{P}\Big{(}\big{\{}G_{n}^{2}(\hat{t}_{n})>x\big{\}}\cap \big{\{}\hat{t}_{n}\in\mathcal{T}_{*}(\varepsilon_{k})\big{\}}\big{)}+ \mathrm{P}\big{(}\big{\{}\hat{t}_{n}\notin\mathcal{T}_{*}(\varepsilon_{k}) \big{\}}\big{)}\] \[\leq\mathrm{P}\Bigg{(}\sup_{s\in\mathcal{T}_{*}(\varepsilon_{k})} G_{n}^{2}(s)>x\Bigg{)}+\mathrm{P}\big{(}R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}> \varepsilon_{k}\big{)}\] \[=\mathrm{P}\big{(}F_{k}^{2}(G_{n})>x\big{)}+\mathrm{P}\big{(}R( \hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}>\varepsilon_{k}\big{)}\]

taking the limit as \(n\to\infty\), the first term converges, by the continuous mapping theorem, to the probability of the event \(\big{\{}F_{k}^{2}(G)>x\big{\}}\), where \(G\) is the limiting Gaussian process discussed above, while the second term vanishes by the first part of Theorem 3. Therefore, for all \(k\in\mathbb{N}\),

\[\limsup_{n\to\infty}\mathrm{P}\big{(}G_{n}^{2}(\hat{t}_{n})>x\big{)}\leq \mathrm{P}\Bigg{(}\sup_{s\in\mathcal{T}_{*}(\varepsilon_{k})}G^{2}(s)>x\Bigg{)}\]

Taking the limit as \(k\to\infty\), noticing that the events

\[\Bigg{\{}\sup_{s\in\mathcal{T}_{*}(\varepsilon_{k})}G^{2}(s)>x\Bigg{\}}\]

are nested, using the continuity of probability from above, and recalling that \(\cap_{k\geq 1}\mathcal{T}_{*}(\varepsilon_{k})=\mathcal{T}_{*}\) gives

\[\limsup_{n\to\infty}\mathrm{P}\big{(}G_{n}^{2}(\hat{t}_{n})>x\big{)}\leq \mathrm{P}\Bigg{(}\sup_{s\in\mathcal{T}_{*}}G^{2}(s)>x\Bigg{)}.\]

Using properties of the quantile function (e.g. [1, Lemma 20]) finishes the proof of the upper bound. For the lower bound, we make a similar argument. We have, by an application of Lemma 4,

\[n\cdot[R(\hat{t}_{n},\hat{w}_{n})-R_{*}] \geq n\cdot[R(\hat{t}_{n},\hat{w}_{n})-R(\hat{t}_{n},w_{*}(\hat{t} _{n}))]\] \[\geq\frac{1}{2}\cdot\frac{1}{(1+\|n^{-1/2}\Lambda_{n}\|_{\infty, -})^{2}}\cdot G_{n}^{2}(\hat{t}_{n}).\]By the Glivenko-Cantelli assumption on \(\Lambda_{n}\), the first two factors converge to \(1/2\). For the third, we will lower bound its upper tails, which will imply a lower bound on the upper tails of the rescaled excess risk. We let \((\varepsilon_{k})_{\xi=1}^{\infty}\) be a decreasing sequence of positive numbers such that \(\varepsilon_{k}\to 0\) as \(k\to\infty\), and define \(H_{k}:\ell^{\infty}(\mathcal{T}\times S^{d-1})\to\mathbb{R}\) by

\[H_{k}(z):=\inf_{t\in\mathcal{T}_{*}(\varepsilon_{k})}\sup_{v\in S^{d-1}}z(t,v),\]

where the subsets \(\mathcal{T}_{*}(\varepsilon_{k})\) are as defined in (20). Clearly, for \(z,z^{\prime}\in\ell^{\infty}(\mathcal{T}\times S^{d-1})\),

\[|H_{k}(z)-H_{k}(z^{\prime})|\leq\|z-z^{\prime}\|_{\infty}\]

so \(H_{k}\) is Lipschitz and therefore continuous. Now

\[\mathrm{P}\big{(}G_{n}^{2}(\hat{t}_{n})>x\big{)} \geq\mathrm{P}\big{(}\big{\{}G_{n}^{2}(\hat{t}_{n})>x\big{\}} \cap\big{\{}\hat{t}_{n}\in\mathcal{T}_{*}(\varepsilon_{k})\big{\}}\big{\}}\] \[\geq\mathrm{P}\Big{(}\inf_{s\in\mathcal{T}_{*}(\varepsilon_{k})}G _{n}^{2}(s)>x\Big{)}-\mathrm{P}\big{(}\big{\{}\hat{t}_{n}\notin\mathcal{T}_{*} (\varepsilon_{k})\big{\}}\big{)}\] \[=\mathrm{P}\big{(}H_{k}^{2}(G_{n})>x\big{)}-\mathrm{P}\big{(}R( \hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}>\varepsilon_{k}\big{)}\]

By the same argument as above, we obtain, as \(n\to\infty\), and for all \(k\in\mathbb{N}\),

\[\liminf_{n\to\infty}\mathrm{P}\big{(}G_{n}^{2}(\hat{t}_{n})>x\big{)}\geq \mathrm{P}\bigg{(}\inf_{s\in\mathcal{T}_{*}(\varepsilon_{k})}G^{2}(s)>x\bigg{)}\]

Taking the limit as \(k\to\infty\), and noticing that

\[\bigcup_{k\geq 1}\biggl{\{}\inf_{s\in\mathcal{T}_{*}(\varepsilon_{k})}G^{2}(s)>x \biggr{\}}=\biggl{\{}\inf_{s\in\mathcal{T}_{*}}G^{2}(s)>x\biggr{\}}\]

proves that

\[\liminf_{n\to\infty}\mathrm{P}\big{(}G_{n}^{2}(\hat{t}_{n})>x\big{)}\geq \mathrm{P}\bigg{(}\inf_{s\in\mathcal{T}_{*}}G^{2}(s)>x\bigg{)}.\]

Using properties of the quantile function (e.g. [1, Lemma 20]) finishes the proof of the lower bound. The estimates on the quantiles of \(Z^{+}\) in Remark 1 are a consequence of standard Gaussian concentration, see [e.g. 1, Appendix A.3]. Finally, for the second statement in Remark 1,

\[\mathrm{E}\biggl{[}\max_{s\in\mathcal{T}_{*}}\lVert Z(s)\rVert_{2}^ {2}\biggr{]} \leq\mathrm{E}\Bigg{[}\biggl{(}\sum_{s\in\mathcal{T}_{*}}\lVert Z(s )\rVert_{2}^{2p}\biggr{)}^{1/p}\Bigg{]}\] \[\leq\left(\sum_{s\in\mathcal{T}_{*}}\mathrm{E}\Big{[}\lVert Z(s) \rVert_{2}^{2p}\biggr{]}\right)^{1/p}\] \[\leq 32\cdot p\cdot\left(\sum_{s\in\mathcal{T}_{*}}\mathrm{E} \big{[}\lVert Z(s)\rVert_{2}^{2}\big{]}^{p}\right)^{1/p},\]

where the last estimate follows from Gaussian concentration. Taking \(p=1+\log\lvert\mathcal{T}_{*}\rvert\), and recalling that for \(x\in\mathbb{R}^{d}\), \(\lVert x\rVert_{p}\leq d^{1/p}\lVert x\rVert_{\infty}\) yields the result.

## Appendix E Proof of Lemma 1

We prove the first statement by induction. For \(k=0\), this follows directly from the fact that by definition \(F^{0}_{n,\delta}(\mathcal{T})=\mathcal{T}\) and \(F_{n,\delta}(\mathcal{T})\subseteq\mathcal{T}\). Now let \(k\in\mathbb{N}\) and assume that the statement holds for \(k-1\). Let \(s\in F^{k+1}_{n,\delta}(\mathcal{T})\). Then by definition

\[R(s,w_{*}(s))-R_{*}\leq 2\cdot(n\delta)^{-1}\cdot\mathrm{E}[\sup_{s\in F^{ k}_{n,\delta}(\mathcal{T})}G_{n}^{2}(s)]\leq 2\cdot(n\delta)^{-1}\cdot\mathrm{E}[\sup_{s \in F^{k+1}_{n,\delta}(\mathcal{T})}G_{n}^{2}(s)].\]

where the second inequality follows from the fact that by the induction hypothesis, \(F^{k}_{n,\delta}(\mathcal{T})\subseteq F^{k-1}_{n,\delta}(\mathcal{T})\), and that the supremum is increasing. Therefore \(s\in F^{k}_{n,\delta}(\mathcal{T})\) since the last inequality is the defining inequality for \(F^{k}_{n,\delta}(\mathcal{T})\). We now turn to the second statement. Fix \(k\) and \(\delta\). On the one hand, \(\mathcal{T}_{*}\subseteq\bigcap_{n\geq 1}F^{k}_{n,\delta}(\mathcal{T})\). On the other, for any \(t\in\bigcap_{n\geq 1}F^{k}_{n,\delta}(\mathcal{T})\), we have for all \(n\geq n_{0}\), \(R(t,w_{*}(t))-R_{*}\leq 2B\cdot(n\delta)^{-1}\). Therefore \(R(t,w_{*}(t))-R_{*}=0\), and hence \(t\in\mathcal{T}_{*}\).

Proof of Theorem 4

Recall the notation introduced in Appendix C. Let \(A_{n}(t_{*})\) be the event that:

\[\|n^{-1/2}\Lambda_{n}\|_{\infty,+}\leq 1/2,\quad\text{ and }\quad\|n^{-1/2}\Delta_{n}( \cdot,t_{*})\|\leq 1/2.\]

We start by showing that under the sample size inequality stated in the theorem, there exists a \(t_{*}\in\mathcal{T}_{*}\) such that \(\mathrm{P}(A_{n}(t_{*}))\geq 1-\delta/3\). Indeed, we have

\[\|n^{-1/2}\Lambda_{n}\|_{\infty,+}=\sup_{(t,v)\in(\mathcal{T}\times S^{d-1})} \frac{1}{n}\sum_{i=1}^{n}\Bigl{\{}1-\langle v,\Sigma^{-1/2}(t)\phi_{t}(X_{i}) \rangle^{2}\Bigr{\}}\]

The elements of this sum are bounded by \(1\), and the variance parameter of Bousquet's inequality [14] is given by \(L\) as defined in Section 3.2. Applying this inequality yields that with probability at least \(1-\delta/6\)

\[\|n^{-1/2}\Lambda_{n}\|_{\infty,+}\leq\frac{2}{n^{1/2}}\cdot\mathrm{E}\biggl{[} \sup_{t\in\mathcal{T}}\Lambda_{n}(t)\biggr{]}+\sqrt{\frac{2L\log(6/\delta)}{n} }+\frac{4\log(6/\delta)}{3n}\]

Furthermore, by Markov's inequality, with probability at least \(1-\delta/6\)

\[\|n^{-1/2}\Delta(\cdot,t_{*})\|_{\infty,+}\leq\frac{6\cdot\mathrm{E}[\sup_{t \in\mathcal{T}\setminus\mathcal{T}_{*}}\Delta(t,t_{*})]}{n^{1/2}\cdot\delta}\]

Hence, when the inequality on the sample size stated in the theorem holds for some \(t_{*}\), the event \(A_{n}(t_{*})\) holds with probability at least \(1-\delta/3\). Now on this event, the first bound of Lemma 4 applies, and we have

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\leq 2\cdot n^{-1}\cdot G_{n}^{2}(\hat{t }_{n}).\] (21)

Now we use the iterative localization method of Koltchinskii [13]. Initially, we have no information about where \(\hat{t}_{n}\) is located aside from belonging to \(\mathcal{T}\), so we start with the bound

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\leq 2\cdot n^{-1}\cdot\sup_{t\in \mathcal{T}}G_{n}^{2}(t).\] (22)

Using Markov's inequality, we have on an event \(B_{n,1}\) which holds with probability at least \(1-\delta/2k\)

\[\sup_{t\in\mathcal{T}}G_{n}^{2}(t)\leq 2k\cdot\delta^{-1}\cdot\mathrm{E}[\sup_{ t\in\mathcal{T}}G_{n}^{2}(t)].\]

Replacing in (22) yields that on the event \(A_{n}(t_{*})\cap B_{n,1}\),

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\leq 4k\cdot(n\delta)^{-1}\cdot \mathrm{E}[\sup_{t\in\mathcal{T}}G_{n}^{2}(t)],\]

which shows that on this event, \(\hat{t}_{n}\in F_{n,\delta/2k}(\mathcal{T})\), by definition of the map \(F_{n,\delta/2k}\). With this knowledge, we now reuse the bound (21) to obtain that on \(A_{n}(t_{*})\cap B_{n,1}\)

\[R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{*}\leq 2\cdot n^{-1}\cdot\sup_{t\in F_{n, \delta/2k}(\mathcal{T})}G_{n}^{2}(t).\]

Iterating the procedure we just described \(k\) times, we obtain that on an event \(A_{n}(t_{*})\cap(\cap_{j=1}^{k}B_{n,j})\), where \(\mathrm{P}(B_{n,j})\geq 1-\delta/2k\) for all \(j\in[k]\)

\[\hat{t}_{n}\in F_{n,\delta/2k}^{k}(\mathcal{T})=\mathcal{S}_{n,\delta,k}.\] (23)

Another application of Markov's inequality yields that on an event \(C\) which holds with probability at least \(1-\delta/6\)

\[\sup_{t\in\mathcal{S}_{n,\delta,k}}G_{n}^{2}(t)\leq 6\cdot\delta^{-1}\cdot \mathrm{E}[\sup_{t\in\mathcal{S}_{n,\delta,k}}G_{n}^{2}(t)]\] (24)

Since

\[\mathrm{P}(A_{n}(t_{*})\cap(\cap_{j=1}^{k}B_{n,k})\cap C)\geq 1-\delta/3- \sum_{j=1}^{k}\delta/2k-\delta/6=1-\delta,\]

equation (23) proves the first statement of the theorem. For the second statement, we have on the same event \(A_{n}(t_{*})\cap(\cap_{j=1}^{k}B_{n,k})\cap C\), and combining the two upper bounds from Lemma 4,

\[\mathcal{E}(\hat{t}_{n},\hat{w}_{n})\leq 4\cdot n^{-1}\cdot G_{n}^{2}(\hat{t }_{n})\leq 4\cdot n^{-1}\cdot\sup_{t\in\mathcal{S}_{n,\delta,k}}G_{n}^{2}(t) \leq 24\cdot(n\delta)^{-1}\cdot\mathrm{E}[\sup_{t\in\mathcal{S}_{n,\delta,k}}G_{n}^{2}(t)],\]

where we used (23) and (24) in the above inequalities, concluding the proof.

Proof of Lemma 2

We prove a slightly more general result, from which Lemma 2 can be immediately deduced.

**Lemma 5**.: _Let \(n,d\in\mathbb{N}\) and let \(\mathcal{T}\) be a finite set. For each \((i,t)\in[n]\times\mathcal{T}\), let \(Z_{i,t}\in\mathbb{R}^{d}\) be random vectors such that for each \(t\in\mathcal{T}\), \((Z_{i,t})_{i=1}^{n}\) are i.i.d. with the same distribution as \(Z_{t}\). For all \(t\in\mathcal{T}\), assume that \(\mathrm{E}[Z_{t}]=0\), and define_

\[\sigma^{2}(\mathcal{T}):=\sup_{t\in\mathcal{T}}\mathrm{E}\big{[}\|Z_{t}\|_{2}^ {2}\big{]},\qquad r_{n}(\mathcal{T}):=\mathrm{E}\Bigg{[}\sup_{(i,t)\in[n] \times\mathcal{T}}\|Z_{i,t}\|_{2}^{2}\Bigg{]}^{1/2}.\]

_Then_

\[\frac{1}{2}\cdot\frac{\sigma(\mathcal{T})}{n^{1/2}}+\frac{1}{4}\cdot\frac{r_{ n}(\mathcal{T})}{n}\leq\mathrm{E}\Bigg{[}\sup_{t\in\mathcal{T}}\!\!\Bigg{\|} \frac{1}{n}\sum_{i=1}^{n}Z_{i,t}\Bigg{\|}_{2}^{2}\Bigg{]}^{1/2}\leq C(\mathcal{ T})\cdot\frac{\sigma(\mathcal{T})}{n^{1/2}}+C^{2}(\mathcal{T})\cdot\frac{r_{n}( \mathcal{T})}{n},\]

_where \(C(\mathcal{T}):=5\sqrt{1+\log\lvert\mathcal{T}\rvert}\)._

To prove Lemma 5, we need to recall a few preliminary results. The first is a classical symmetrization inequality, see e.g. [1, Lemma 11.4] or [20, Proposition 4.11] for a proof.

**Lemma 6**.: _For each \((i,t)\in[n]\times\mathcal{T}\), let \(W_{i,t}\in\mathbb{R}^{d}\) be random vectors such that for each \(t\in\mathcal{T}\), \((W_{i,t})_{i=1}^{n}\) are i.i.d. with the same distribution as \(W_{t}\). Let \((\varepsilon_{i})_{i=1}^{n}\) be independent Rademacher random variables, independent of the collection of random vectors \(W_{i,t}\). Define \(\overline{W}_{i,t}:=W_{i,t}-\mathrm{E}[W_{t}]\). Then_

\[\frac{1}{2}\mathrm{E}\Bigg{[}\sup_{t\in\mathcal{T}}\!\!\Bigg{\|}\frac{1}{n} \sum_{i=1}^{n}\varepsilon_{i}\overline{W}_{i,t}\Bigg{\|}_{2}^{2}\Bigg{]}^{1/2 }\leq\mathrm{E}\Bigg{[}\sup_{t\in\mathcal{T}}\!\!\Bigg{\|}\frac{1}{n}\sum_{i =1}^{n}\overline{W}_{i,t}\Bigg{\|}_{2}^{2}\Bigg{]}^{1/2}\leq 2\,\mathrm{E} \Bigg{[}\sup_{t\in\mathcal{T}}\!\!\Bigg{\|}\frac{1}{n}\sum_{i=1}^{n} \varepsilon_{i}W_{i,t}\Bigg{\|}_{2}^{2}\Bigg{]}^{1/2}.\]

The second result we recall is the Khinchin-Kahane inequality, the specific form we require is obtained from Pena and Gine [1, Theorem 1.3.1] by setting \(q=p\) and \(p=2\) in that theorem, see also Boucheron et al. [1, page 141].

**Lemma 7**.: _For \(i\in[n]\), let \(z_{i}\in\mathbb{R}^{d}\) be fixed vectors. Let \((\varepsilon_{i})_{i=1}^{n}\) be independent Rademacher random variables. Then for all \(p\geq 2\),_

\[\mathrm{E}\Bigg{[}\Bigg{\|}\sum_{i=1}^{n}\varepsilon_{i}z_{i}\Bigg{\|}_{2}^{p }\Bigg{]}^{1/p}\leq\sqrt{p-1}\cdot\left(\sum_{i=1}^{n}\!\|z_{i}\|_{2}^{2} \right)^{1/2}.\]

A straightforward consequence of Lemma 7 is the following result, which follows from the elementary observation that for a vector \(x\in\mathbb{R}^{d}\), \(\|x\|_{\infty}\leq\|x\|_{p}\leq d^{1/p}\|x\|_{\infty}\).

**Lemma 8**.: _For \((i,t)\in[n]\times\mathcal{T}\), let \(z_{i,t}\in\mathbb{R}^{d}\) be fixed vectors. Let \((\varepsilon_{i})_{i=1}^{n}\) be independent Rademacher random variables. Then_

\[\mathrm{E}\left[\sup_{t\in\mathcal{T}}\!\!\Bigg{\|}\sum_{i=1}^{n}\varepsilon_{ i}z_{i,t}\Bigg{\|}_{2}^{2}\right]^{1/2}\leq\frac{5}{2}\sqrt{1+\log\lvert \mathcal{T}\rvert}\cdot\left(\sup_{t\in\mathcal{T}}\sum_{i=1}^{n}\!\|z_{i,t} \|_{2}^{2}\right)^{1/2}.\]

Proof.: Let \(p\geq 1\). Then, by Jensen's inequality and Lemma 7

\[\leq\mathrm{E}\Bigg{[}\left(\sum_{t\in\mathcal{T}}\!\!\Bigg{\|}\sum _{i=1}^{n}\varepsilon_{i}z_{i,t}\right\|_{2}^{2p}\Bigg{]}^{1/p}\Bigg{]}\] \[\leq\left(2p-1\right)\cdot\left(\sum_{t\in\mathcal{T}}\!\! \left\{\sum_{i=1}^{n}\!\|z_{i,t}\|_{2}^{2}\right\}^{p}\right)^{1/p}.\]

Recalling that \(\|x\|_{p}\leq d^{1/p}\|x\|_{\infty}\) for all \(x\in\mathbb{R}^{d}\) and taking \(p:=1+\log\lvert\mathcal{T}\rvert\) yields the result.

Finally, we need the following consequence of Lemmas 6 and 8. The proof idea is taken from [16].

**Lemma 9**.: _For each \((i,t)\in[n]\times\mathcal{T}\), let \(W_{i,t}\in\mathbb{R}\) be random variables such that for each \(t\in\mathcal{T}\), \((W_{i,t})_{i=1}^{n}\) are i.i.d. with the same distribution as \(W_{t}\), with \(W_{t}\geq 0\) almost surely. Then_

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\sum_{i=1}^{n}W_{i,t}\right]^{1/2}\leq \left(\sup_{t\in\mathcal{T}}\sum_{i=1}^{n}\mathrm{E}[W_{i,t}]\right)^{1/2}+5 \sqrt{1+\log\lvert\mathcal{T}\rvert}\cdot\mathrm{E}\!\left[\sup_{(i,t)\in[n] \times\mathcal{T}}W_{i,t}\right]^{1/2}.\]

Proof.: We have by Jensen's inequality and Lemma 6,

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\sum_{i=1}^{n}W_{i,t}\right] \leq\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\sum_{i=1}^{ n}W_{i,t}-\mathrm{E}[W_{i,t}]\right|\right]+\sup_{t\in\mathcal{T}}\sum_{i=1}^{n} \mathrm{E}[W_{i,t}],\] \[\leq 2\,\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\sum_{i=1 }^{n}\varepsilon_{i}W_{i,t}\right|^{2}\right]^{1/2}+\sup_{t\in\mathcal{T}}\sum _{i=1}^{n}\mathrm{E}[W_{i,t}].\] (25)

Conditioning on the random vectors \(W_{i,t}\), we have by Lemma 8 and the assumption \(W_{i,t}\geq 0\) a.s.

\[2\,\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\sum_{i=1}^{n }\varepsilon_{i}W_{i,t}\right|^{2}\right]^{1/2} \leq 5\sqrt{(1+\log\lvert\mathcal{T}\rvert)}\cdot\left(\sup_{t\in \mathcal{T}}\sum_{i=1}^{n}W_{i,t}^{2}\right)^{1/2},\] \[\leq 5\sqrt{1+\log\lvert\mathcal{T}\rvert}\cdot\left(\sup_{(i,t) \in[n]\times\mathcal{T}}W_{i,t}\right)^{1/2}\cdot\left(\sup_{t\in\mathcal{T}} \sum_{i=1}^{n}W_{i,t}\right)^{1/2}.\]

Taking expectation with respect to \(W_{i,t}\), and using the Cauchy-Schwartz inequality yields

\[2\,\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\sum_{i=1}^{n}\varepsilon_ {i}W_{i,t}\right|^{2}\right]^{1/2}\leq\sqrt{6(1+\log\lvert\mathcal{T}\rvert)} \cdot\mathrm{E}\!\left[\sup_{(i,t)\in[n]\times\mathcal{T}}Z_{i,t}\right]^{1/2} \cdot\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\sum_{i=1}^{n}W_{i,t}\right]^{1/2}.\]

Replacing in (25) and solving the resulting quadratic inequality yields the result. 

Equipped with these results, we now prove Lemma 1. The proof idea is taken from [16].

Proof of Lemma 1.: We start with the lower bound. We have on the one hand

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\frac{1}{n}\sum_{i=1}^{n}Z_{i,t}\right|_{2}^{2}\right]\geq\sup_{t\in\mathcal{T}}\mathrm{E}\!\left[\left| \frac{1}{n}\sum_{i=1}^{n}Z_{i,t}\right|_{2}^{2}\right]=\sigma^{2}(\mathcal{T}).\] (26)

On the on other hand, by Lemma 6, we have

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\frac{1}{n}\sum_{i=1}^{n}Z_{i,t}\right|_{2}^{2}\right]^{1/2}\geq\frac{1}{2}\,\mathrm{E}\!\left[\sup_{t\in \mathcal{T}}\!\left|\frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}Z_{i,t}\right|_{2} ^{2}\right]^{1/2}.\]

Define the random index

\[I\in\operatorname*{argmax}_{i\in[n]}\max_{t\in\mathcal{T}}\!\|Z_{i,t}\|_{2}^{2}.\]

Conditioning on \(Z_{i,t}\), we have by Jensen's inequality

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left|\frac{1}{n}\sum_{i=1}^{n} \varepsilon_{i}Z_{i,t}\right|_{2}^{2}\right]\geq\sup_{t\in\mathcal{T}}\mathrm{ E}\!\left[\left|\mathrm{E}\!\left[\frac{1}{n}\sum_{i=1}^{n}\varepsilon_{i}Z_{i,t} \right]\right|_{2}^{2}\right]=\sup_{t\in\mathcal{T}}\frac{\|Z_{I,t}\|_{2}^{2}} {n^{2}}=\sup_{(i,t)\in[n]\times\mathcal{T}}\frac{\|Z_{i,t}\|_{2}^{2}}{n^{2}},\]where in the inequality, the outer expectation is with respect to \(\varepsilon_{I}\), and the inner one is with respect to \((\varepsilon_{i})_{i\neq I}\). Taking expectation with respect to \(Z_{i,t}\) gives

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left\|\frac{1}{n}\sum_{i=1}^{n}Z_{i,t }\right\|_{2}^{2}\right]^{1/2}\geq\frac{1}{2}\cdot\frac{r_{n}(\mathcal{T})}{n}\] (27)

Averaging the lower bounds (26) and (27) yields the desired lower bound. We now turn to the upper bound. We have by Lemmas 6 and 8.

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left\|\frac{1}{n}\sum_{ i=1}^{n}Z_{i,t}\right\|_{2}^{2}\right]^{1/2} \leq 2\,\mathrm{E}\!\left[\sup_{t\in\mathcal{T}}\!\left\|\frac{1} {n}\sum_{i=1}^{n}\varepsilon_{i}Z_{i,t}\right\|_{2}^{2}\right]^{1/2}\] \[\leq 5\sqrt{1+\log\lvert\mathcal{T}\rvert}\cdot\mathrm{E}\! \left[\sup_{t\in\mathcal{T}}\sum_{i=1}^{n}\left\|\frac{1}{n}Z_{i,t}\right\|_ {2}^{2}\right]^{1/2}\]

Applying Lemma 9 on the last term yields the desired upper bound. 

## Appendix H Proof of Corollary 1

The Glivenko-Cantelli and Donsker assumptions of Theorem 3 follow directly from the moment assumptions of the corollary, the weak law of large numbers, and the central limit theorem, and therefore the conclusions of Theorem 3 hold. For the first statement of the corollary, we may assume without loss of generality that \(\mathcal{T}_{*}\neq\mathcal{T}\), otherwise the statement holds trivially. Define

\[\varepsilon:=\min_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\{R(t,w_{*}(t))-R_ {*}\}.\]

Then \(\varepsilon>0\), and by the first item of Theorem 3,

\[\lim_{n\to\infty}\mathrm{P}\!\left(\hat{t}_{n}\notin\mathcal{T}_{*}\right) \leq\lim_{n\to\infty}\mathrm{P}\!\left(R(\hat{t}_{n},w_{*}(\hat{t}_{n}))-R_{ *}>\varepsilon/2\right)=0.\] (28)

It remains to prove the improved upper bound on the asymptotic quantiles. For this, referring to the proof of Theorem 3, and in particular to (18), it is enough to show that

\[\lim_{n\to\infty}\mathrm{P}\!\left(n\cdot\left[R(\hat{t}_{n},w_{*}(\hat{t}_{n }))-R_{*}\right)\right]>0\right)=0,\]

but this follows directly from (28).

## Appendix I Proof of Corollary 2

The statement follows from the same argument as Theorem 4 with only a few simple modifications. As explained in the main text, we use Lemma 3 to bound the quantity \(\mathrm{E}\!\left[\max_{t\in\mathcal{T}}\Lambda_{n}(t)\right]\) by constructing a block diagonal matrix. We use Lemma 2 to control, for any subset \(\mathcal{S}\), \(\mathrm{E}\!\left[\max_{s\in\mathcal{S}}G_{n}^{2}(s)\right]\). The only minor deviation from Theorem 4 is that we bound the second moment

\[\mathrm{E}\!\left[\sup_{t\in\mathcal{T}\setminus\mathcal{T}_{*}}\Delta_{n}^{2 }(t,t_{*})\right]\]

instead of the first. This explains the slightly better dependence on \(\delta\) in the sample size restriction of Corollary 2 compared to Theorem 4.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction are supported by our main results of Section 3 and 4 directly. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The last paragraph of the conclusion explicitly discusses the limitations of this work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The proofs of all the statements we claimed are new can be found in the appendix. For known statements, we provided direct references to them. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the guidelines and confirm that our work adheres to them. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper is heavily theoretical, and is quite far removed from direct applications, which is why do not foresee any direct societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We have cited all the relevant work we are aware of, but we do not use any existing code, data, or models in this work. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not applicable. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.