# Retrieval-Retro: Retrieval-based Inorganic

Retrosynthesis with Expert Knowledge

 Heewoong Noh\({}^{1}\), Namkeoong Lee\({}^{1}\), Gyoung S. Na\({}^{2*}\), Chanyoung Park\({}^{1}\)

\({}^{1}\) KAIST \({}^{2}\) KRICT

{heewoongoh,namkyeong96,cy.park}@kaist.ac.kr

ngs0@krict.re.kr

Corresponding Author.

###### Abstract

While inorganic retrosynthesis planning is essential in the field of chemical science, the application of machine learning in this area has been notably less explored compared to organic retrosynthesis planning. In this paper, we propose Retrieval-Retro for inorganic retrosynthesis planning, which implicitly extracts the precursor information of reference materials that are retrieved from the knowledge base regarding domain expertise in the field. Specifically, instead of directly employing the precursor information of reference materials, we propose implicitly extracting it with various attention layers, which enables the model to learn novel synthesis recipes more effectively. Moreover, during retrieval, we consider the thermodynamic relationship between target material and precursors, which is essential domain expertise in identifying the most probable precursor set among various options. Extensive experiments demonstrate the superiority of Retrieval-Retro in retrosynthesis planning, especially in discovering novel synthesis recipes, which is crucial for materials discovery. The source code for Retrieval-Retro is available at https://github.com/HeewoongNoh/Retrieval-Retro.

## 1 Introduction

Discovering new materials is a fundamental problem in materials science [28, 4, 25], providing innovative options in various industry fields, such as semiconductors and batteries [37, 36]. On the other hand, it is also important to establish synthetic routes for newly discovered materials [21], i.e., retrosynthesis planning, as the ability to synthesize these materials is essential for their successful commercialization beyond mere discovery.

For organic materials, retrosynthesis planning approaches identify valid and efficient synthetic routes [8] by breaking down complex target molecules into smaller molecules that are commercially available and easily synthesizable. During the process, they focus on the structural information of target molecules, such as functional groups and reaction centers, that are related to widely known organic reaction mechanisms [21, 7, 9, 40, 22, 23]. Following the practice of organic retrosynthesis, machine learning (ML) based approaches utilizing the molecular structure expressed as SMILES strings [43] or molecular graphs have been extensively studied [45, 33].

However, unlike organic retrosynthesis, using the atomic structural information of inorganic materials for retrosynthesis presents significant challenges due to 1) the high computational load incurred by the larger number of atoms compared to organic molecules [7, 6], and 2) the failure of traditional physical theories for the atomic structure computation caused by the inclusion of diverse and unusual elements [11, 1]. Therefore, a chemical composition-based approach is essential for retrosynthesis planning of inorganic materials. Besides the challenges of using the atomic structural information, there is a lack of a clear and general theory regarding the mechanisms of inorganic synthesis reactions[34]. For these reasons, inorganic retrosynthesis planning is a more challenging task compared to organic retrosynthesis planning.

Given these challenges inherent in the retrosynthesis planning of inorganic materials, applying existing ML-based organic retrosynthesis methods for inorganic purposes is infeasible. Consequently, there has been limited research into inorganic retrosynthesis planning compared to that for organic materials. As a pioneering work, CVAE [17] generates synthesis variables for target materials using a generative model, and EllemwiseRetro [18] reformulates the precursor prediction task as a multi-class classification problem with dozens of curated precursor templates. However, these approaches overlook the common practices in conventional inorganic material synthesis, where chemists identify reference materials similar to the target material and consult established synthesis recipes [12; 13].

More specifically, due to the aforementioned inherent challenges, chemists often engage in a costly trial-and-error method by referencing precedent recipes of reference materials from prior literature [21; 12]. Therefore, as illustrated in Figure 1 (c), the majority of the discovered synthetic routes for a target material share a common set of precursors with the corresponding reference material from previous studies (Figure 1 (a)), whereas only a small number of these routes involve a completely new set of precursors (Figure 1 (b)). Inspired by the common practice, He et al. [12] propose to retrieve reference materials that are similar to a target inorganic material from a knowledge base of previous studies, and leverage their precursor information for retrosynthesis planning of the target material.

Although the method proposed by He et al. [12] has proven to be effective by emulating standard practices in the field, it faces two significant limitations. The first limitation is that the model's predictive capability is limited to the precursor sets of retrieved reference material, thus inhibiting its capacity to deduce novel synthetic pathway. This is due to the heavy reliance on the precursor sets of reference materials. However, despite a small fraction of materials being synthesized with entirely new synthetic recipes as shown in Figure 1 (c), it is widely known that discovering novel synthetic routes with entirely new precursors can accelerate the inorganic material synthesis process [29; 27] and facilitate the discovery of cost-effective recipes [26; 30]. Thus, despite a large number of materials being synthesized through slight alterations to previously known synthesis recipes due to the complexities of inorganic material synthesis, it remains critical to identify new synthetic pathways that extend beyond the commonly known synthetic recipes.

The second limitation is that it neglects the widely known domain expertise in the field [17; 18; 12]: the greater (more negative) thermodynamic driving force (\(\Delta G\)) between the target material and the precursor set, the more feasible it is to actually form the target material through the precursor set [31; 35]. As an example in Figure 1 (d), given a target material \(\text{A}_{2}\text{BCO}_{4}\), a precursor set \(\{\text{AO},\text{AO}_{2},\text{BCO}\}\) exhibits a significantly greater \(\Delta G\) compared to another set \(\{\text{A}_{2}\text{CO}_{2},\text{BO}\}\), making it more probable that the target material will be synthesized from the first precursor set. Therefore, by analyzing the thermodynamic relationships between the target material and various precursor sets, we can identify which combinations of precursors are most feasible for material synthesis. For example, considering such relationship enables the selection of precursor sets that are likely effective starting materials for synthesizing the target material, thereby facilitating successful synthesis. However, previous works [17; 18; 12] overlook this crucial domain expertise, leading to their inability to identify these optimal precursor sets.

Figure 1: An example case where **(a)** the target material shares a subset of precursors with reference material, and **(b)** the target material has an entirely new set of precursors, without sharing any subset of precursors with reference material. **(c)** The proportion of subset cases and new cases among the materials newly synthesized from 2017 to 2020. **(d)** The target material is more likely to be synthesized using the precursor set that exhibits a more negative driving force.

To this end, we propose a novel inorganic retrosynthesis planning approach by implicitly extracting the precursor information with the domain expertise-enhanced reference material retriever. Specifically, instead of directly utilizing precursor information as in He et al. [12]--that is, explicitly incorporating precursors from retrieved materials for prediction--we propose to implicitly extract this information from reference materials using various attention layers that are designed to enhance and extract the precursor details of the reference materials. By providing the model with greater flexibility, we expect it to discover novel synthesis recipes that go beyond existing ones. Moreover, to determine which material should be referenced, we utilize well-established domain expertise in the field, i.e., the thermodynamic relationships between the target material and potential precursors, with a novel Neural Reaction Energy retriever. With a novel retriever, our model effectively identifies which material to refer to for inorganic retrosynthesis planning of the target material. Our extensive experiments demonstrate the effectiveness of Retrieval-Retro in inorganic retrosynthesis planning, especially discovering novel synthetic recipes, demonstrating the potential applicability of Retrieval-Retro in real-world materials discovery.

In this study, we make the following contributions:

* We propose to implicitly integrate the precursor information of reference materials, which enables the model to more effectively discover novel synthetic recipes of inorganic materials.
* Furthermore, we introduce a novel retriever inspired by domain expertise, which assists the model in effectively determining which material to reference during inorganic retrosynthesis planning.
* Extensive experiments demonstrate the effectiveness of Retrieval-Retro in various scenarios, particularly in the year split, which poses a more realistic and challenging environment. Additionally, its exceptional capability in uncovering new synthetic recipes for inorganic materials highlights its potential for practical application in real-world material discovery.

## 2 Preliminaries

### Problem setup

**Inorganic Retrosynthesis Planning with Chemical Composition.** In inorganic retrosynthesis planning, determining the atomic structure of inorganic materials poses significant challenges and demands costly computational efforts. As a result, both chemists [21] and prior ML methodologies [18; 17; 12] depend exclusively on composition information. In line with previous studies, we also base our approach on the composition information of inorganic materials instead of structural data.

**Notations.** An inorganic material can be quantitatively described by a composition vector \(\mathbf{x}\in\mathbb{R}^{d}\), where \(d\) represents the total number of unique chemical elements. Each element in the vector \(\mathbf{x}\) represents the proportion of each chemical element that constitutes the material. As an example, consider the material with chemical formula \(\text{SiO}_{2}\), where Si and O correspond to element numbers 14 and 6, respectively. It can be represented as \(\mathbf{x}=(x_{1},x_{2},\ldots,x_{d})\), where \(x_{14}=\frac{1}{3}\), \(x_{6}=\frac{2}{3}\), with all other \(x\) values being zero. Moreover, following a previous work [10], we construct a fully connected composition graph \(\mathcal{G}=(\mathcal{E},\mathbf{A})\), where \(\mathcal{E}\) is the set of elements associated with the nonzero components of \(\mathbf{x}\), and \(\mathbf{A}\in\{1\}^{n\times n}\) is the fully connected adjacency matrix with \(n\) indicating the number of nonzero entries in \(\mathbf{x}\). We initialize the initial feature \(\mathbf{e}_{i}\) of each element \(e_{i}\) in \(\mathcal{E}\) with Matscholar [44], whose element embedding is obtained from the vast amount of scientific literature.

**Task: Precursor Prediction.** Following a previous work [12], we formulate precursor prediction as a multi-label classification problem. Given a fully connected graph \(\mathcal{G}=(\mathcal{E},\mathbf{A})\) representing an inorganic material, our objective is to train a model \(\mathcal{F}\) that predicts the possible precursors for the material, i.e., \(\mathbf{y}=\mathcal{F}(\mathcal{G})\), where \(\mathbf{y}\in\{0,1\}^{l}\) indicates the label vector with each element signifying the predefined \(l\) precursors. That is, each element \(y_{i}\) in the label vector \(\mathbf{y}\) indicates whether \(i\)-th precursor is necessary (\(y_{i}=1\)) or not (\(y_{i}=0\)) for synthesizing the target material \(\mathcal{G}\).

### Composition Graph Encoder

To begin with, we briefly introduce the compositional graph encoder, which is used to encode the material representation in the paper. Specifically, given a composition graph \(\mathcal{G}=(\mathcal{E},\mathbf{A})\) of a material, we obtain the material representation \(\mathbf{g}\) as follows:

\[\mathbf{g}=\text{Pooling}(\text{GNN}(\mathcal{E},\mathbf{A})),\] (1)

where "Pooling" refers to the sum pooling of the node representations within the composition graph \(\mathcal{G}\), which are derived from a GNN encoder. The detailed architecture used in the paper is provided in Appendix A.1. Moreover, we examine whether the proposed framework can consistently improve in various GNN architectures in Appendix E.1.

## 3 Proposed Method: Retrieval-Retro

In this section, we introduce our proposed method Retrieval-Retro, a novel inorganic retrosynthesis planning approach that implicitly extracts the precursor information of reference materials retrieved with two complementary retrievers, i.e., Masked Precursor Completion (MPC) Retriever and Neural Reaction Energy (NRE) Retriever. The overall framework of Retrieval-Retro is shown in Figure 3.

### Reference Material Retrieval

Before we extract precursor information from the reference materials, it is essential to decide which material should be referenced for the extraction. To elaborately determine which materials to reference, we employ two complementary retrievers: the Masked Precursor Completion (MPC) retriever and the Neural Reaction Energy (NRE) retriever.

**Masked Precursor Completion (MPC) Retriever.** MPC retriever identifies the reference materials sharing similar precursors with the target material by learning dependencies between precursors. Specifically, given a chemical composition vector \(\mathbf{x}\) of a target material, we obtain its representation \(\mathbf{m}=M(\mathbf{x})\), where \(M:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d^{\prime}}\) indicates two layered MLPs with non-linearity. We define a learnable precursor embedding matrix \(\mathbf{P}\in\mathbb{R}^{l\times d^{\prime}}\), whose \(i\)-th row \(p_{i}\in\mathbb{R}^{d^{\prime}}\) represents a learnable embedding vector for the \(i\)-th precursor. Concurrently, we generate a randomly perturbed precursor vector \(\mathbf{\tilde{y}}\) from the provided precursor information \(\mathbf{y}\), and create a perturbed precursor matrix \(\mathbf{\tilde{P}}\) by applying the perturbed precursor vector \(\mathbf{\tilde{y}}\) as a mask to the precursor matrix \(\mathbf{P}\). Specifically, \(\tilde{p}_{i}\) is masked if \(\tilde{y}_{i}=0\), and is left unchanged otherwise. We then integrate the representation \(\mathbf{m}\) and the perturbed precursor matrix \(\mathbf{\tilde{P}}\) with cross-attention to form precursor conditioned representation of the material \(\mathbf{s}\). Then, the model is trained to reconstruct the original precursor vector \(\mathbf{y}\) from the precursor conditioned representation \(\mathbf{s}\) and precursor matrix \(\mathbf{P}\), by representing probability for each precursor as follows \(\sigma(\mathbf{s}^{\top}p_{i})\). The overall training procedure of MPC retriever is in Figure 1 (a).

By doing so, it enables the retriever to learn dependencies among precursors and the correlation between the precursors and the target material. With the MPC retriever, we calculate the cosine similarity between the representation of the target material and all materials in the knowledge base obtained through \(M\), and retrieve the top \(K\) materials that are similar to the target material.

**Neural Reaction Energy (NRE) Retriever.** Although the MPC retriever effectively identifies reference materials with potentially similar sets of synthesis precursors, it overlooks widely recognized domain expertise in the field, i.e., the thermodynamic relationships between materials, which is essential for the inorganic synthesis process, particularly in selecting appropriate precursors [35; 27]. More specifically, the thermodynamic driving force between the target material and precursor set can be quantified by Gibbs free energy (\(\Delta G\)), which is a measure of the material's thermodynamic stability. Under constant pressure and temperature, a negative \(\Delta G\) indicates that the energy of the target material is lower than that of the precursor set, signifying that the synthesis reaction can occur spontaneously [31; 35]. As a result, it is widely known that the more negative \(\Delta G\), the more precursor set is likely to synthesize the target material.

Based on this knowledge, it would be beneficial to retrieve materials that have the precursor set capable of inducing favorable reactions with the target material by considering the thermodynamic force. \(\Delta G\) can be approximated by the difference \(\Delta H\) between the enthalpy of the target and

Figure 2: **(a)** Training process of the Masked Precursor Completion (MPC) retriever. **(b)** Training process of the Neural Reaction Energy (NRE) retriever.

precursor set \(\Delta H\) as follows:

\[\Delta G\approx\Delta H=H_{Target}-H_{Precursor\ set},\] (2)

where the \(H_{Target}\) and \(H_{Precursor\ set}\) represent the formation energy of the target and precursor set. Therefore, a straightforward solution for calculating \(\Delta H\) is to utilize the formation energy of the target material and the precursor set that can be directly obtained from the extensive database of structure-based DFT-calculated formation energy. However, it is widely known that DFT-calculated values frequently diverge from experimental data, while actual material synthesis occurs in real-world wet lab settings [16]. Even worse, there is no guarantee that these databases encompass all materials of interest in inorganic retrosynthesis planning. Consequently, it is essential to develop a composition-based formation energy predictor that is specifically designed for experimental data.

To this end, we propose a learnable Neural Reaction Energy (NRE) retriever, which is pre-trained on abundant DFT-calculated formation energy data and then fine-tuned on experimental formation energy data as shown in Figure 1 (b) [16]. Specifically, we initially pre-train the NRE retriever using the Materials Project database [14], training the model to predict DFT-calculated formation energy from representations derived from the composition graph encoder (see Section 2.2). Subsequently, we fine-tune the retriever using experimental formation energy data [32], which allows the model to adapt to experimental data. We demonstrate the effectiveness of the training mechanism in Appendix E.2. Finally, given a trained NRE retriever, we can compute the formation energies of the target material and the precursor set of reference materials in the knowledge base. We then retrieve \(K\) reference materials that exhibit the most negative \(\Delta G\), selecting from those whose precursors contain the same elements as the target material, along with other common elements such as C, H, O, and N. Note that calculations are performed prior to training, so no additional training costs are incurred.

### Implicit Precursor Extraction

Now, we discuss how to extract the precursor information from the reference materials elaborately selected in Section 3.1. While the previous work [12] directly utilizes the precursor information of the reference materials, this limits the model's ability to learn and deduce new synthetic recipes for the target material, which can significantly accelerate the materials discovery and reduce the cost of material synthesis. Therefore, we propose to implicitly extract the precursor information from the retrieved material with various attention layers, i.e., self-attention and cross-attention layers, which aim to enhance the representation of reference materials by considering other reference materials and extract the implicit precursor information from the enhanced representation, respectively.

To do so, we first encode the target material and \(K\) reference materials using their associated composition graphs \(\mathcal{G}\) via the composition graph encoder introduced in Section 2.2. As a result, we obtain the representation of target material \(\mathbf{g}_{t}\in\mathbb{R}^{D}\) and the \(K\) reference materials \(\mathbf{G}_{t}=[\mathbf{g}_{r}^{1},\dots,\mathbf{g}_{r}^{K}]\in\mathbb{R}^{K \times D}\), where \(\mathbf{g}_{r}^{k}\) indicates the representation of the \(k\)-th reference material.

**Reference Enhancing with Self-Attention.** To effectively extract the precursor information from the reference materials, we first enhance the representation of these materials through a self-attention mechanism [39; 24]. This approach encourages the model to selectively determine which information to extract from a particular reference material by considering the relationships among various reference materials. To do so, we first obtain a new matrix for reference materials \(\mathbf{G^{\prime}}_{r}^{0}=[\mathbf{g^{\prime}}_{r}^{1},\dots,\mathbf{g^{ \prime}}_{r}^{K}]\), where \(\mathbf{g^{\prime}}_{r}^{k}=\phi_{1}(\mathbf{g}_{r}^{k}||\mathbf{g}_{t})\) indicates the modified representation of the \(k\)-th reference material regarding the target material, where \(||\) denotes the concatenation operation, and \(\phi_{1}:\mathbb{R}^{2D}\rightarrow\mathbb{R}^{D}\) is a learnable MLP. Note that by modifying the representation of the reference material through concatenation with the target material, we allow the model to extract information pertaining to the target material, rather than focusing solely on the reference materials. Then, we implement a self-attention mechanism to determine which information to extract from the reference material, taking into account the

Figure 3: The overall framework of Retrieval-Retro.

relationships among the other reference materials:

\[\mathbf{G^{\prime}}_{r}^{s}=\text{Self-Attention}(\mathbf{Q}_{\mathbf{G^{\prime}}_{r} ^{s-1}},\mathbf{K}_{\mathbf{G^{\prime}}_{r}^{s-1}},\mathbf{V}_{\mathbf{G^{ \prime}}_{r}^{s-1}})\in\mathbb{R}^{K\times D},\] (3)

where \(s=1,\ldots,S\) indicates the index of the self-attention layers. Different from the conventional self-attention layers [39], we directly utilize \(\mathbf{G^{\prime}}_{r}^{s-1}\) as query \(\mathbf{Q}_{\mathbf{G^{\prime}}_{r}^{s-1}}\), key \(\mathbf{K}_{\mathbf{G^{\prime}}_{r}^{s-1}}\), and value \(\mathbf{V}_{\mathbf{G^{\prime}}_{r}^{s-1}}\), without any learnable parameters [24]. By analyzing the relationships between the reference materials, the model improves the representations of these materials, thereby supplying more appropriate references to be extracted by the cross-attention layer.

**Reference Selection with Cross-Attention.** Lastly, we extract the implicit precursor information by merging the representation of the target material with that of the enhanced reference materials via a cross-attention mechanism [38; 42]. With cross-attention layers, we expect the model to learn favorable synthesis recipes from reference materials by selectively learning from reference materials with attention weights. More formally, cross-attention layers are formulated as follows:

\[\mathbf{g}_{t}^{c}=\text{Cross-Attention}(\mathbf{Q}_{\mathbf{g}_{t}^{c-1}}, \mathbf{K}_{\mathbf{G^{\prime}}_{r}^{S}},\mathbf{V}_{\mathbf{G^{\prime}}_{r} ^{S}})\in\mathbb{R}^{D},\] (4)

where \(c=1,\ldots,C\) indicates the index of the cross-attention layers. Note that we use an enhanced reference material representation \(\mathbf{G^{\prime}}_{r}^{S}\) and target material representation \(\mathbf{g}_{t}\) as inputs to the first cross-attention layer, i.e., \(\tilde{\mathbf{G^{\prime}}}_{r}^{0}=\mathbf{G^{\prime}}_{r}^{S}\) and \(\mathbf{g}_{t}^{0}=\mathbf{g}_{t}\). Moreover, we also utilize \(\mathbf{g}_{t}^{c-1}\) as query \(\mathbf{Q}_{\mathbf{g}_{t}^{c-1}}\), and the reference material representation \(\mathbf{G^{\prime}}_{r}^{S}\) as key \(\mathbf{K}_{\mathbf{G^{\prime}}_{r}^{S}}\) and value \(\mathbf{V}_{\mathbf{G^{\prime}}_{r}^{S}}\) identical to the self-attention layer, without any learnable parameters. By employing cross-attention layers between the target material and reference materials, rather than the precursor set of the reference materials, the model effectively accesses the synthetic recipes of reference materials without explicitly using precursor information, thus enabling the discovery of novel synthetic recipes for the target material. We employ this implicit precursor extraction process for the reference materials gathered using both the MPC retriever and the NRE retriever, resulting in \(\mathbf{g}_{t:\text{MPC}}^{C}\) and \(\mathbf{g}_{t:\text{NRE}}^{C}\), respectively.

### Model Training

Finally, we compute the model prediction \(\hat{\mathbf{y}}\) as follows: \(\hat{\mathbf{y}}=\phi_{\text{classifier}}(\mathbf{g}_{t}||\mathbf{g}_{t: \text{MPC}}^{C}||\mathbf{g}_{t:\text{NRE}}^{C})\), where \(\phi_{\text{classifier}}:\mathbb{R}^{3D}\rightarrow\mathbb{R}^{l}\) is an MLP with non-linearity. Note that each dimension in \(\hat{\mathbf{y}}\), i.e., \(\hat{y}_{i}\), indicates the model's predicted probability of whether precursor \(i\) will be included or not. For model training, we adopt Binary Cross Entropy (BCE) loss, which is commonly used for multi-label classification learning [5; 47], as: \(\mathcal{L}=-\frac{1}{l}\sum_{i=1}^{l}\left[y_{i}\log(\hat{y}_{i})+(1-y_{i}) \log(1-\hat{y}_{i})\right]\).

## 4 Experiments

### Experimental Setup

**Datasets.** We use 33,343 inorganic material synthesis recipes extracted from 24,304 materials science papers [20] following prior studies [12; 18]. Due to the lack of an extensive database containing inorganic synthesis recipes [20], we use the training set as the knowledge base, following a previous work [12]. Additional details about datasets are provided in the Appendix B.

**Baseline Methods.** We compare Retrieval-Retro with two inorganic retrosynthesis methods (i.e., **He et al. [12]** and ElemwiseRetro [18]), two composition-based representation learning methods (i.e., Roost [10]) and CrabNet [41]), and three newly proposed baselines (i.e., Composition MLP and Graph Network [3], Graph Network + MPC) to demonstrate the effectiveness of Retrieval-Retro. The first newly introduced baseline is called **Composition MLP**, which does not retrieve reference materials but instead relies on the composition vector of the material. **He et al. [12]** conducts inorganic retrosynthesis planning by using the MPC retriever to access reference materials based solely on the material's composition vector. **ElemwiseRetro**[18] acquires precursor information through a fully connected graph that represents the constituent elements within the material. Furthermore, two composition-based material representation learning approaches, namely **Roost**[10] and **CrabNet**[41], explore the intricate interactions among elements within materials using message passing and self-attention, respectively. Although these methods are initially designed for property prediction, we have adapted prediction heads so that they can be effectively used for inorganic retrosynthesis planning. We also evaluate two new baselines, **Graph Network**[3] and **Graph Network + MPC**. The former predicts precursors without retrieving reference materials, while the latter does so after retrieving references. As these methods utilize the same backbone GNN structure as in our approach,

[MISSING_PAGE_FAIL:7]

In Table 2, we have following observations: **1)** By comparing the Graph Network with and without the MPC retriever, we find that incorporating the precursor information from reference materials enhances model performance in subset cases. One interesting observation is that, this information negatively impacts performance in Top-10 new case, demonstrating that it can hinder the model's ability to deduce novel synthetic pathways. **2)** Conversely, Retrieval-Retro, which implicitly integrates the precursor information, consistently shows performance improvements in both the subset and new cases, particularly widening the performance gap in the new case--a more realistic and challenging scenario. These findings illustrate the importance of how precursor information from reference materials should be integrated, particularly in identifying new synthetic pathways, which can speed up the material discovery process and reduce synthesis costs. **3)** Interestingly, we find that the NRE retriever also consistently enhances the model performance, a benefit likely stemming from its complementary relationship with the MPC retriever. Specifically, while the MPC retriever captures dependencies between precursors and the target material based on previously observed data, novel synthesis recipes might diverge from the existing synthesis patterns documented in the literature. On the other hand, the NRE retriever employs domain expertise that is independent of these existing patterns, thus filling gaps that the MPC retriever might overlook. By accessing such complementary reference materials, the model is able to acquire additional new precursor information, which contributes to the observed performance improvements. In Table 4, we conduct a qualitative analysis of the materials retrieved by each retriever and examine their impact on model predictions.

In conclusion, we find that each element of Retrieval-Retro plays an effective role in inorganic retrosynthesis planning, particularly in identifying new synthesis recipes, showcasing its potential influence in the field of materials discovery.

### Model Analysis

**Ablation Studies: Effects of Retriever.** To verify the effect of the retrievers, we conduct ablation studies by removing the retriever modules. In Table 3, we have following observations: **1)** When reference materials are randomly retrieved without using trained retrievers, the model extracts precursor information that is irrelevant to the synthesis of the target material, leading to a deterioration in performance. **2)** Furthermore, using just one of the retrievers (i.e., either MPC or NRE) leads to underperforms the case when both retrievers are used, showing that the two retrievers are complementary to each other as discussed in Section 4.2. In conclusion, we contend that both MPC and NRE retrievers should be employed to provide informative reference materials to the model, facilitating the extraction of valuable information in inorganic retrosynthesis planning. We provide further ablation studies in Appendix E.3.

**Sensitivity Analysis: Size of Knowledge Base.** We evaluate how the size of the knowledge base affects the model performance. More precisely, from the original database, we sample various sizes of knowledge bases, such as 20%, 40%, and up to 100% (Full) of the size of the original database, and then retrieve reference materials from these sampled subsets. In Figure 4 (a), as expected, the larger the knowledge base, the more accurate the model's predictions. These findings illuminate potential avenues for further development of our model, particularly as more inorganic synthetic recipes are uncovered in the future.

**Sensitivity Analysis: Number of Reference Materials.** Moreover, we investigate how the varying number of reference materials \(K\) affects the model performance. In Figure 4 (b), we notice that model performance improves with an increase in the number of references up to a certain point, specifically \(K=3\). This reaffirms the importance of incorporating precursor information from reference materials for inorganic retrosynthesis planning. However, increasing the number of reference materials beyond \(K=3\) does not further enhance model performance, likely due to the introduction of irrelevant or

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Refer.**} & \multicolumn{3}{c}{**Retriever**} & \multicolumn{3}{c}{**Subset Case**} & \multicolumn{3}{c}{**New Case**} \\ \cline{3-11}  & & **MPC** & **NRE** & **Top-10** & **Top-3** & **Top-5** & **Top-10** & **Top-1** & **Top-3** & **Top-5** & **Top-10** \\ \hline \multirow{3}{*}{Graph Network} & \multirow{3}{*}{Explicit} & ✗ & ✗ & 63.98 & 67.95 & 68.83 & 70.83 & 16.37 & 22.00 & 23.78 & 27.93 \\  & & ✗ & ✗ & 63.94 & 68.01 & 68.04 & 68.01 & 1.03 & 1.047 & 2.04 & 40.00 & 1.06 \\  & & ✓ & ✗ & 65.01 & 69.06 & 69.98 & 72.03 & 17.65 & 2.245 & 24.22 & 26.22 \\  & & & & (1.36) & 11.07 & (1.22) & (0.832) & (1.48) & (2.63) & (2.63) & (2.74) \\ \hline \multirow{3}{*}{Retrieval-Retro} & \multirow{3}{*}{Implicit} & ✓ & ✗ & 62.07 & 69.24 & 20.41 & 22.47 & 12.70 & 24.52 & 26.30 & 30.15 \\  & & ✓ & ✗ & 68.08 & 127.10 & 23.04 & (1.44) & (1.05) & (1.42) & (2.00) & (2.09) \\ \cline{1-1}  & & ✓ & ✓ & ✗ & **66.00** & **70.51** & **71.76** & **73.92** & **20.15** & **27.04** & **28.37** & **31.56** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Overall model performance in subset case and new case. “Refer.” indicates whether the model explicitly or implicitly uses the precursor information from reference materials.

[MISSING_PAGE_EMPTY:9]

explicit utilization of precursor information can inhibit the model's capability in providing novel synthetic routes for target material. Different from the previous work, we propose to implicitly utilize precursor information of similar material rather than directly using it.

### Composition-based representation learning for Inorganic materials

While most machine learning approaches for inorganic materials predominantly utilize structural information, in real-world material discovery, structural data is often unavailable due to the high costs of computational resources. Consequently, some ML methods opt to use compositional information of inorganic materials instead of structural details. For example, ElemNet [15] proposes to learn the representation of material with compositional information using deep neural networks (DNNs). Roost [10] learns material representation by building fully connected graphs based on the composition to model interactions between elements by graph neural networks (GNNs). Additionally, CrabNet [41] successfully applies a self-attention mechanism to the element-derived matrix to accurately predict the properties of materials. Although these methods were originally designed for predicting material properties, they can also be applied to inorganic retrosynthesis as they similarly rely on the composition information of materials.

### Difference between Inorganic Retrosynthesis and Organic Retrosynthesis

Both organic and inorganic retrosynthesis are challenging tasks that predict the synthesis of materials by breaking down the target material into simpler precursors. However, there are significant differences between organic and inorganic retrosynthesis. Organic retrosynthesis [45; 40; 7; 9; 33] deals with organic compounds, which are molecules primarily composed of elements such as carbon, hydrogen, oxygen, nitrogen, and sulfur. These compounds are represented using molecular structure graphs or SMILES strings. In contrast, inorganic retrosynthesis involves inorganic compounds, which can include a wider variety of elements, often including metals, and have structures that periodically repeat in unit cells. Another key difference lies in the use of structural information during retrosynthesis planning. Organic retrosynthesis utilizes structural information such as functional groups and reaction centers of organic compounds, which indicate the properties of a material and its reactivity with other molecules, to predict simpler molecules (precursors) into which the target molecule can be broken down. Inorganic compounds, however, have relatively unexplored generalized synthesis mechanisms compared to organic compounds, and calculating their structures is expensive. Therefore, it is challenging to directly use structural information for retrosynthesis planning. Instead, inorganic retrosynthesis [12; 17; 18] often relies solely on the chemical composition of the materials, distinguishing it from organic retrosynthesis.

## 6 Limitations

We aim to identify favorable precursor sets by considering the thermodynamic relationships between materials and their precursor set. However, in the actual synthesis process, the phase changes of materials are influenced by synthesis temperature, synthesis time, pressure condition and pairwise reactions between precursors. Taking these factors into account would enable more accurate precursor set predictions. Nevertheless, in situations where experimental data (such as temperature and pressure) are unavailable, we estimate the reaction energy solely from the formation energy calculated under consistent temperature and pressure conditions using a trained predictor, derived from the composition of materials. Considering multiple synthesis conditions can lead to more precise predictions of precursor sets.

## 7 Conclusion

In this study, we introduce Retrieval-Retro, a novel method for inorganic retrosynthesis planning by extracting the precursor information of retrieved reference material implicitly. To do so, we employ various attention layers that enhance and extract the information from the reference material. Moreover, we design a neural reaction energy (NRE) retriever that provides complementary reference material to the MPC retriever, allowing Retrieval-Retro to integrate precursor information from a broader range of reference materials through domain expertise. Through extensive experiments, including assessments in realistic scenarios, we demonstrate the effectiveness of implicit extraction of precursor information and NRE retriever in discovering novel synthesis recipes of target material, demonstrating the potential impact of Retrieval-Retro in the field.

## Acknowledgements

This study was supported by Korea Research Institute of Chemical Technology (No.: KK2351-10), the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (RS-2024-00406985), and NRF grant funded by Ministry of Science and ICT (NRF-2022M3J6A1063021).

## References

* [1]A. Averkiev, M. B. Mantina, R. Valero, I. Infante, A. Kovacs, D. G. Truhlar, and L. Gagliardi (2011) How accurate are electronic structure methods for actinoid chemistry? Theoretical Chemistry Accounts129, pp. 657-666. Cited by: SS1.
* [2]M. Aykol, J. H. Montoya, and J. Hummelshoj (2021) Rational solid-state synthesis routes for inorganic materials. Journal of the American Chemical Society143 (24), pp. 9244-9259. Cited by: SS1.
* [3]P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. (2018) Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261. Cited by: SS1.
* [4]J. Cai, X. Chu, K. Xu, H. Li, and J. Wei (2020) Machine learning-driven new material discovery. Nanoscale Advances2 (8), pp. 3115-3130. Cited by: SS1.
* [5]Z. Chen, X. Wei, P. Wang, and Y. Guo (2019) Multi-label image recognition with graph convolutional networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 5177-5186. Cited by: SS1.
* [6]A. J. Cohen, P. Mori-Sanchez, and W. Yang (2012) Challenges for density functional theory. Chemical reviews112 (1), pp. 289-320. Cited by: SS1.
* [7]C. W. Coley, L. Rogers, W. H. Green, and K. F. Jensen (2017) Computer-assisted retrosynthesis based on molecular similarity. ACS central science3 (12), pp. 1237-1245. Cited by: SS1.
* [8]E. J. Corey (1991) The logic of chemical synthesis: multistep synthesis of complex carbogenic molecules (nobel lecture). Angewandte Chemie International Edition in English30 (5), pp. 455-465. Cited by: SS1.
* [9]H. Dai, C. Li, C. Coley, B. Dai, and L. Song (2019) Retrosynthesis prediction with conditional graph logic network. Advances in Neural Information Processing Systems32. Cited by: SS1.
* [10]R. E. Goodall and A. A. Lee (2020) Predicting materials properties without crystal structure: deep representation learning from stoichiometry. Nature communications11 (1), pp. 6280. Cited by: SS1.
* [11]J. N. Harvey (2006) On the accuracy of density functional theory in transition metal chemistry. Annual Reports Section C"(Physical Chemistry)102, pp. 203-226. Cited by: SS1.
* [12]T. He, H. Huo, C. J. Bartel, Z. Wang, K. Cruse, and G. Ceder (2023) Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature. Science advances9 (23), pp. eadg8180. Cited by: SS1.
* [13]H. Huo, C. J. Bartel, T. He, A. Trewartha, A. Dunn, B. Ouyang, A. Jain, and G. Ceder (2022) Machine-learning rationalization and prediction of solid-state synthesis conditions. Chemistry of Materials34 (16), pp. 7323-7336. Cited by: SS1.
* [14]A. Jain, S. P. Ong, G. Hautier, W. Chen, W. Richards, S. D. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, et al. (2013) Commentary: the materials project: a materials genome approach to accelerating materials innovation. APL materials1 (1). Cited by: SS1.
* [15]D. Jha, L. Ward, A. Paul, W. Liao, A. Choudhary, C. Wolverton, and A. Agrawal (2018) ElemNet: deep learning the chemistry of materials from only elemental composition. Scientific reports8 (1), pp. 17593. Cited by: SS1.
* [16]D. Jha, K. Choudhary, F. Tavazza, W. Liao, A. Choudhary, C. Campbell, and A. Agrawal (2019) Enhancing materials property prediction by leveraging computational and experimental data using deep transfer learning. Nature communications10 (1), pp. 5316. Cited by: SS1.

* [17] Kim, E., Jensen, Z., van Grootel, A., Huang, K., Staib, M., Mysore, S., Chang, H.-S., Strubell, E., McCallum, A., Jegelka, S., et al. Inorganic materials synthesis planning with literature-trained neural networks. _Journal of chemical information and modeling_, 60(3):1194-1201, 2020.
* [18] Kim, S., Noh, J., Gu, G. H., Chen, S., and Jung, Y. Element-wise formulation of inorganic retrosynthesis. In _AI for Accelerated Materials Design NeurIPS 2022 Workshop_, 2022.
* [19] Kingma, D. P. and Welling, M. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [20] Kononova, O., Huo, H., He, T., Rong, Z., Botari, T., Sun, W., Tshitoyan, V., and Ceder, G. Text-mined dataset of inorganic materials synthesis recipes. _Scientific data_, 6(1):203, 2019.
* [21] Kovnir, K. Predictive synthesis. _Chemistry of Materials_, 33(13):4835-4841, 2021.
* [22] Lee, N., Hyun, D., Na, G. S., Kim, S., Lee, J., and Park, C. Conditional graph information bottleneck for molecular relational learning. In _International Conference on Machine Learning_, pp. 18852-18871. PMLR, 2023.
* [23] Lee, N., Yoon, K., Na, G. S., Kim, S., and Park, C. Shift-robust molecular relational learning with causal substructure. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pp. 1200-1212, 2023.
* [24] Lee, N., Noh, H., Kim, S., Hyun, D., Na, G. S., and Park, C. Density of states prediction of crystalline materials via prompt-guided multi-modal transformer. _Advances in Neural Information Processing Systems_, 36, 2024.
* [25] Liu, Y., Zhao, T., Ju, W., and Shi, S. Materials discovery and design using machine learning. _Journal of Materiomics_, 3(3):159-177, 2017.
* [26] Lokhande, A., Gurav, K., Jo, E., He, M., Lokhande, C., and Kim, J. H. Towards cost effective metal precursor sources for future photovoltaic material synthesis: Cts nanoparticles. _Optical Materials_, 54:207-216, 2016.
* [27] McDermott, M. J., Dwaraknath, S. S., and Persson, K. A. A graph-based network for predicting chemical reaction pathways in solid-state materials synthesis. _Nature communications_, 12(1):3097, 2021.
* [28] Merchant, A., Batzner, S., Schoenholz, S. S., Aykol, M., Cheon, G., and Cubuk, E. D. Scaling deep learning for materials discovery. _Nature_, 624(7990):80-85, 2023.
* [29] Miura, A., Ito, H., Bartel, C. J., Sun, W., Rosero-Navarro, N. C., Tadanaga, K., Nakata, H., Maeda, K., and Ceder, G. Selective metathesis synthesis of mgcr 2 s 4 by control of thermodynamic driving forces. _Materials horizons_, 7(5):1310-1316, 2020.
* [30] Mondal, S. and Banthia, A. K. Low-temperature synthetic route for boron carbide. _Journal of the European Ceramic society_, 25(2-3):287-291, 2005.
* [31] Peterson, G. G. and Brgoch, J. Materials discovery through machine learning formation energy. _Journal of Physics: Energy_, 3(2):022002, 2021.
* [32] (SGTE), S. G. T. E. et al. Thermodynamic properties of inorganic materials. _Landolt-Boernstein New Series, Group IV_, 1999.
* [33] Somnath, V. R., Bunne, C., Coley, C., Krause, A., and Barzilay, R. Learning graph models for retrosynthesis prediction. _Advances in Neural Information Processing Systems_, 34:9405-9415, 2021.
* [34] Stein, A., Keller, S. W., and Mallouk, T. E. Turning down the heat: Design and mechanism in solid-state synthesis. _Science_, 259(5101):1558-1564, 1993.
* [35] Szymanski, N. J., Rendy, B., Fei, Y., Kumar, R. E., He, T., Milsted, D., McDermott, M. J., Gallant, M., Cubuk, E. D., Merchant, A., et al. An autonomous laboratory for the accelerated synthesis of novel materials. _Nature_, 624(7990):86-91, 2023.

* [36] Tarascon, J.-M., Recham, N., Armand, M., Chotard, J.-N., Barpanda, P., Walker, W., and Dupont, L. Hunting for better li-based electrode materials via low temperature inorganic synthesis. _Chemistry of Materials_, 22(3):724-739, 2010.
* [37] Tong, T., Zhang, M., Chen, W., Huo, X., Xu, F., Yan, H., Lai, C., Wang, W., Hu, S., Qin, L., et al. Recent advances in carbon-based material/semiconductor composite photoelectrocatalysts: Synthesis, improvement strategy, and organic pollutant removal. _Coordination Chemistry Reviews_, 500:215498, 2024.
* [38] Tsai, Y.-H. H., Bai, S., Liang, P. P., Kolter, J. Z., Morency, L.-P., and Salakhutdinov, R. Multimodal transformer for unaligned multimodal language sequences. In _Proceedings of the conference. Association for computational linguistics. Meeting_, volume 2019, pp. 6558. NIH Public Access, 2019.
* [39] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [40] Wan, Y., Hsieh, C.-Y., Liao, B., and Zhang, S. Retroformer: Pushing the limits of end-to-end retrosynthesis transformer. In _International Conference on Machine Learning_, pp. 22475-22490. PMLR, 2022.
* [41] Wang, A. Y.-T., Kauwe, S. K., Murdock, R. J., and Sparks, T. D. Compositionally restricted attention-based network for materials property predictions. _Npj Computational Materials_, 7(1):77, 2021.
* [42] Wang, Z., Nie, W., Qiao, Z., Xiao, C., Baraniuk, R., and Anandkumar, A. Retrieval-based controllable molecule generation. _arXiv preprint arXiv:2208.11126_, 2022.
* [43] Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _Journal of chemical information and computer sciences_, 28(1):31-36, 1988.
* [44] Weston, L., Tshitoyan, V., Dagdelen, J., Kononova, O., Trewartha, A., Persson, K. A., Ceder, G., and Jain, A. Named entity recognition and normalization applied to large-scale information extraction from the materials science literature. _Journal of chemical information and modeling_, 59(9):3692-3702, 2019.
* [45] Yan, C., Ding, Q., Zhao, P., Zheng, S., Yang, J., Yu, Y., and Huang, J. Retroxpert: Decompose retrosynthesis prediction like a chemist. _Advances in Neural Information Processing Systems_, 33:11248-11258, 2020.
* [46] Zhang, D., Berger, H., Kremer, R. K., Wulferding, D., Lemmens, P., and Johnsson, M. Synthesis, crystal structure, and magnetic properties of the copper selenite chloride cu5 (seo3) 4cl2. _Inorganic chemistry_, 49(20):9683-9688, 2010.
* [47] Zhu, F., Li, H., Ouyang, W., Yu, N., and Wang, X. Learning spatial regularization with image-level supervisions for multi-label image classification. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 5513-5522, 2017.

[MISSING_PAGE_EMPTY:14]

Implementation Details

In this section, we provide implementation details of Retrieval-Retro.

### Composition Graph Encoder

As a composition graph encoder, we mainly consider graph network [3], which is the generalized version of various graph neural networks. Building on prior research, our graph neural networks are divided into two components: the encoder and the processor. The encoder is responsible for learning the initial representation of elements, while the processor manages message passing between elements. To describe this more formally, for an element \(e_{i}\) and the edge \(a_{i,j}\) connecting element \(e_{i}\) to element \(e_{j}\), the node encoder \(\phi_{node}\) and the edge encoder \(\phi_{edge}\) generate initial representations of the element as follows:

\[\mathbf{e}_{i}^{0}=\phi_{node}(\mathbf{e}_{i}),\ \ \ \mathbf{a}_{ij}^{0}=\phi_{edge}( \mathbf{a}_{ij}),\] (5)

where \(\mathbf{e}_{i}\) is the initial feature of element \(i\) and \(\mathbf{a}_{ij}\) is the concatenated feature of element \(i\) and \(j\), i.e., \(\mathbf{a}_{ij}=(\mathbf{e}_{i}||\mathbf{e}_{j})\). Using the initial representations of elements and edges, the processor is designed to facilitate message passing among the elements and to update the representations of both elements and edges in the following manner:

\[\mathbf{a}_{ij}^{l+1}=\psi_{edge}^{l}(\mathbf{e}_{i}^{l},\mathbf{e}_{j}^{l}, \mathbf{a}_{ij}^{l}),\ \ \ \mathbf{e}_{i}^{l+1}=\psi_{node}^{l}(\mathbf{e}_{i}^{l},\sum_{j\in \mathcal{E}}\mathbf{a}_{ij}^{l+1}),\] (6)

where \(\mathcal{E}\) represents element set comprising the material, and \(\psi\) is a two-layer MLP with non-linearity. Note that we use three message passing layers within the processor, i.e., \(l=0,\dots,2\).

### Training Details

**Model Training.** Our method is implemented on Python 3.8.13, and Torch-geometric 2.0.4. In all our experiments, we use the AdamW optimizer for model optimization. We train the model for 500 epochs across all tasks, while the model is early stopped if there is no improvement in the best validation Top-5 Accuracy for 30 consecutive epochs. All experiments are conducted on a 48 GB NVIDIA RTX A6000.

**Hyperparameters.** We have detailed the hyperparameter specifications in the table 5. For Retrieval-Retro, we adjust the hyperparameters within specific ranges as follows: number of message passing layers in GNN \(L^{\prime}\) in {2, 3}, number of cross-attention layers \(C\) in {1,2}, size of hidden dimension \(D\) in {256}, number of self-attention layers \(S\) in {1,2}, learning rate \(\eta\) in {0.0001, 0.0005, 0.001},batch size \(B\) in {32, 64, 128} and number of retrieved materials \(K\) in {1,2,3,4,5,6}. We present the test performance based on the best results obtained on the validation set.

## Appendix B Dataset

In this section, we provide further details on the dataset used for experiments.

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{**Hyperparameters**} & \multicolumn{2}{c}{**Dataset Split**} \\ \cline{2-3}  & **Year Split** & **Random Split** \\ \hline \# Message Passing & 3 & 3 \\ Layers (\(L^{\prime}\)) & & 3 \\ \# Self-Attention & & 1 \\ Layers (\(S\)) & 1 & 1 \\ \# Cross-Attention & & 2 \\ Layers (\(C\)) & & 2 \\ Hidden Dim. (\(D\)) & 256 & 256 \\ Learning Rate (\(\eta\)) & 0.0001 & 0.0001 \\ Batch Size (\(B\)) & 128 & 32 \\ \# of Reference Materials (\(K\)) & 3 & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameter specifications of Retrieval-Retro.

* Following previous study[12], we use 33,343 inorganic solid-state synthesis recipes extracted from 24,304 materials science papers [20] obtained from the github repository 2 of previous work [12]. Following the preprocessing step of previous work, we obtain a total number of 28,598 target materials, which have a diverse number of possible precursor sets as shown in Table 6. In other words, a single target material can have multiple corresponding ground-truth precursor sets. Furthermore, in our experiments, we exclude materials from the validation and test sets that contain precursors not present in the training set, as the classifier would not be trained on those absent precursors. Consequently, for the year split, we use 24,034 entries for the training set, 1,842 for the validation set, and 2,558 for the test set. For the random split, the number of target materials varies across five different trials due to the differing compositions of the training, validation, and test sets in each trial.

Footnote 2: https://github.com/CederGroupHub/SynthesisSimilarity

In Section 3.1, we propose to pre-train the Neural Reaction Energy (NRE) retriever with density functional theory (DFT) calculated formation energy and then fine-tune to experimental data.

* For DFT-calculated data, we use **Materials Project**[14] database 3, which is an openly accessible database that provides various material properties calculated using DFT. From the database, we have collected 80,162 unique compositions along with their respective formation energies. It is important to note that when a composition is associated with multiple structures and formation energies, we only consider the lowest formation energy for the material, as it is the most likely to exist. Footnote 3: https://materialsproject.org/
* For experimental data [32], we download experimental formation energy data from previous work's repository[16]4, which aim to develop robust material property prediction models via transfer learning. We then filter the data down to 1,637 entries using the same preprocessing methods as those applied to DFT-calculated data. Footnote 4: https://github.com/wolverton-research-group/qmpy/blob/master/qmpy/data/thermodata/ssub.dat
* For calculating the Gibbs free energy, we use the formation energy from the Materials Project that we used is calculated at 0 K and 0 atm using DFT and the experimental formation energy is taken from [16], which reports measurements at 298.15 K and 1 atm.

## Appendix C Baseline Methods

In this section, we provide detailed explanations of the baseline methods compared in Section 4. We first provide details on the two previous inorganic retrosynthesis planning methods as follows:

* **He et al. [12]** introduces a new approach to inorganic retrosynthesis planning by retrieving reference materials that share similar properties with the target material. Initially, it proposes using a masked precursor completion (MPC) retriever, described in detail in Section 3.1, which depends solely on the composition vector \(\mathbf{x}\) of the inorganic material.
* **ElemwiseRetro**[18] introduces a method for inorganic retrosynthesis planning that represents the target material as a fully connected composition graph and predicts the likelihood of various precursor sets from the available precursor templates.

Furthermore, as outlined in Section 5.2, there are existing studies that develop material representations based on the composition information of inorganic materials. Although these studies were originally aimed at predicting material properties, we have adapted the prediction heads to make them suitable for inorganic retrosynthesis planning. Here, we provide details on the methods as follows:

* **Roost**[10] suggests employing GNNs to learn representations of inorganic materials by representing their composition as a fully connected graph, with nodes representing the unique elements within the

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c} \hline \hline \# Precursor sets & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 & 14 & 17 & 23 & 28 & 29 & 34 & Total \\ \hline \# Data & 26,944 & 1,168 & 257 & 108 & 42 & 32 & 13 & 8 & 7 & 5 & 4 & 1 & 2 & 2 & 1 & 1 & 1 & 1 & 1 & 28,598 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The distribution of data based on the number of ground truth precursor sets.

composition. This enables the model to explore the complex relationships between the constituent elements, thus capturing physically significant properties and interactions.
* **CrabNet**[41] proposes to model the complex interaction between constituent elements with a Transformer self-attention mechanism [39] to adaptively learn the representation of elements based on their chemical environment.

In addition to existing studies, we introduce further baseline models that can support our claims as described below:

* **Composition MLP** aims to predict the set of precursors based on the composition vector \(\mathbf{x}\) of the inorganic material as follows: \[\mathbf{\hat{y}}=\text{MLP}(\mathbf{x}).\] (7) The sole distinction between **Composition MLP** and He et al. [12] is whether the model retrieves reference materials, highlighting the effectiveness of the retrieval mechanism in inorganic retrosynthesis planning.
* **Graph Network**[3] aims to predict the set of precursors based on the composition graph \(\mathcal{G}\) of the inorganic material as follows: \[\mathbf{\hat{y}}=\text{Graph Network}(\mathcal{G}).\] (8)
* **Graph Network + MPC** improves upon He et al. [12] by replacing its material encoder, originally an MLP that processed the composition vector \(\mathbf{x}\), with a graph network. Since the primary distinction between **Graph Network** and **Graph Network + MPC** is whether the model retrieves reference materials, comparing these methods allows us to evaluate the effectiveness of using reference materials in inorganic retrosynthesis planning. Furthermore, given that **Graph Network** serves as the backbone architecture for Retrieval-Retro, comparing **Graph Network + MPC** with Retrieval-Retro enables the assessment of the effectiveness of the implicit fusion and NRE retriever.

## Appendix D Evaluation Protocol

We use Top-K exact match accuracy, Macro recall, and Micro Recall for evaluating the capability of Retrieval-Retro in the inorganic retrosynthesis task. To begin evaluation, we first define the number of precursors of each material. Following many multi-label classification works [5, 47], the precursor labels are considered to be positive if their probabilities \(\hat{y}_{i}\) are greater than 0.5. We set the number of positive precursor labels, \(L\) as the number of precursor that each material has.

**Top-K exact match accuracy.** For Top-K exact match accuracy, we select K sets which has \(L\) precursors, where each set is chosen based on the highest product of probabilities of its \(L\) precursors. Then, for each of the K sets, if there exists a set that exactly matches the correct precursor set, it is scored as 1; otherwise, it is scored as 0. The average is then calculated over all test materials.

**Micro and Macro Recall.** Note that a material can have a varied number of possible precursor sets, as shown in Table 6. We use Micro and Macro Recall to evaluate such cases. We select the same number of precursor sets as the material has in the same way as we evaluate for Top-K exact match accuracy. We then calculated the macro recall by comparing each obtained set with the correct sets for each material. If a set matched exactly, it was scored as 1; otherwise, it was scored as 0. These scores were averaged for each test material. For micro recall, we calculated the overall average across all test materials.

## Appendix E Additional Experiments

### Performance on Various Backbone Architecture

Since Retrieval-Retro is agnostic to the various composition graph encoder architectures, we validate the effectiveness of Retrieval-Retro within a variety of GNN architectures. In Figure 5, we observe performance improvements across all GNN architectures tested, confirming that Retrieval-Retro framework can consistently improve vanilla GNN architecture for inorganic retrosynthesis planning.

### Neural Reaction Retriever

As described in Section 3.1, we develop a composition-based formation energy predictor tailored for experimental data. To do so, we initially pre-train the GNN predictor on extensive DFT-calculated data [14] and then fine-tune on experimental formation energy data [32]. In this section, we demonstrate whether the pre-train and fine-tune strategy is effective in predicting formation energies. To do so, we divide the entire DFT dataset into three parts: 80% for training, 10% for validation, and 10% for testing. The model is trained over 1,000 epochs, employing early stopping if there is no improvement in the validation loss for 50 consecutive epochs. Similarly, we partition the experimental dataset into 80/10/10% splits, following the same training approach as for the DFT data. While our model begins training experimental data using pre-trained checkpoints from a model trained on DFT data, we also conduct comparisons with a model trained exclusively on experimental data (Ablation in Table 7). Table 7 shows that pre-training with a DFT-calculated dataset enhances model performance, indicating that the NRE retriever efficiently retrieves reference materials related to formation energies.

### Ablation Study on Attention Layer

We conduct an ablation study on the self-attention layer in Retrieval-Retro. By removing the self-attention layer, Retrieval-Retro extracts implicit precursor information by integrating the representation of the target material without the enhanced reference materials through a cross-attention mechanism. As shown in Table 8, this results in a noticeable decline in model performance, underscoring the importance of extracting precursor information from the material's enhanced representation via self-attention. However, even with just the cross-attention mechanism, Retrieval-Retro still outperforms the basic Graph Network.

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & **DFT** & **Exp.** & **MAE (eV/atom)** \\ \hline Ablation & ✗ & ✓ & 0.0761 \\ Ours & ✓ & ✓ & **0.0643** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Formation energy predictor performance on experimental formation energy data.

Figure 5: Performance Improvements across Various GNN Backbones

### Model Training and Inference Time

In this section, we provide time and inference analysis to verify the efficiency of Retrieval-Retro, as shown in Table 9. We observe that methods using retrievers have slightly longer training times compared to those that do not use retrievers. However, since we preprocess the materials to retrieve using a pretrained retriever prior to training, the training time for the model utilizing retrievers remains manageable. Additionally, Roost and CrabNet exhibit significantly longer training times compared to other methods, which can be attributed to their distinctive modeling interaction mechanisms.

## Appendix F Broader Impacts

**Potential Positive Scientific Impacts.** Our work, Retrieval-Retro, explores the automation of precursor prediction, a process traditionally dependent on a chemist's expertise. We have developed a model that learns novel synthesis recipes by implicitly extracting precursor information from retrieved materials. This enables Retrieval-Retro to effectively recommend precursor sets for target materials and facilitates its use in real autonomous material synthesis processes [35].

**Potential Negative Societal Impacts.** Although this work demonstrate good predictive capabilities for precursor prediction, it lacks uncertainty estimation. Therefore, it is essential to use this model collaboratively with chemists for effective precursor prediction.

## Appendix G Pseudo Code

``` Input : Composition based fully connected graph \(\mathcal{G}=(\mathbf{X},\mathbf{A})\), Retrieved reference material graph from MPC retriever \(\mathcal{G}_{\text{{r: MPC}}}=(\mathbf{X}_{\text{{r: MPC}}},\mathbf{A}_{\text{{r: MPC}}})\), Retrieved reference material sets from NRE retriever \(\mathcal{G}_{\text{{r: NNE}}}=(\mathbf{X}_{\text{{r: NRE}}},\mathbf{A}_{\text{{r: NNE}}})\), Number of self attention layer \(s\), Number of cross attention layer \(c\), The number of retrieved Material \(K\), Graph Network GNN
1g\({}_{\text{{r}}}\leftarrow\text{GNN}(\mathbf{X},\mathbf{A})\)
2for\(i=1\)to\(K\)do
3\(\mathbf{g}^{i}_{\text{{r:MPC}}}\leftarrow\text{GNN}(\mathbf{X}_{\text{{r: MPC}}},\mathbf{A}_{\text{{r: MPC}}})\)
4 end for
5G\({}^{\prime}_{\text{{r}}}\leftarrow\text{Self-Attention}(\mathbf{g}^{\prime}_{ \text{{r}}},s)\)
6g\({}^{c}_{\text{{r}}}\leftarrow\text{Cross-Attention}(\mathbf{g}^{\prime}_{ \text{{r}}-1},\mathbf{G}^{\prime}_{\text{{r}}},c)\)// Repeat above process line 2 - 6 for NRE retriever
7g\(\leftarrow\phi_{\text{classifier}}(\mathbf{g}_{\text{{r}}}||\mathbf{g}^{c}_{ \text{{r:MPC}}}||\mathbf{g}^{c}_{\text{{r:NBE}}})\)
8\(\mathcal{L}\leftarrow\text{Binary Cross Entropy}(\hat{\mathbf{y}},\mathbf{y})\) ```

**Algorithm 1**Pseudocode of Retrieval-Retro.

## Appendix G

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Model** & **Retriever** & **Training Time** & **Inference Time** \\ \hline Composition MLP & ✗ & 0.4129 & 0.0007 \\ He et al. [12] & ✓ & 1.6622 & 0.0103 \\ ElernwiseRetro & ✗ & 0.7528 & 0.0012 \\ Roost & ✗ & 2.6382 & 13.6702 \\ CrabNet & ✗ & 9.7238 & 0.1123 \\ Graph Network & ✗ & 0.6917 & 0.0019 \\ Graph Network + MPC & ✓ & 2.4361 & 0.0107 \\ \hline Retrieval-Retro & ✓ & 3.2601 & 0.0116 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Training and inference time per epoch (sec/epoch).

``` Input :A chemical composition of material \(\mathbf{x}\), A learnable precursor embeddings \(\mathbf{P}\), An embedding for the \(i\)-th precursor \(\mathbf{p}_{i}\) of \(\mathbf{P}\), Ground truth Precursor \(\mathbf{y}\), MLP \(\mathbf{M}\), Knowledge BaseKB
1\(\mathbf{m}\leftarrow\mathbf{M}(\mathbf{x})\) Construct a perturbed \(\tilde{\mathbf{P}}\) by applying randomly perturbed \(\tilde{\mathbf{y}}\) as a mask. (\(\tilde{\mathbf{p}}_{i}\) is masked if \(\tilde{y}_{i}=0\), and is left unchanged otherwise.) // Masking embeddings P
2\(\mathbf{s}\leftarrow\text{Cross-Attention}(\mathbf{m},\tilde{\mathbf{P}},\tilde{ \mathbf{P}})\) // Calculate the probability of each precursor \(\mathcal{L}_{Training}\leftarrow\text{Binary Cross Entropy}(\tilde{\mathbf{y}}, \mathbf{y})\) Calculate the cosine similarity between the target material and all materials in the KB using \(\mathbf{M}\) Retrieve the top \(K\) materials and save the retrieved material sets. // Construct retrieved material Sets ```

**Algorithm 2**Pseudocode of MPC Retriever.

``` Input :Composition based fully connected graph (material from DFT calculated data) \(\mathcal{G}_{DFT}=(\mathbf{X}_{DFT},\mathbf{A}_{DFT})\), Composition based fully connected graph (material from experimental data) \(\mathcal{G}_{Exp}=(\mathbf{X}_{Exp},\mathbf{A}_{Exp})\), Ground truth Formation energy from DFT calculated data \(\mathbf{H}_{DFT}\), Ground truth Formation energy from experimental data \(\mathbf{H}_{Exp}\), Graph Network GNN, Knowledge BaseKB. \(\hat{\mathbf{H}}_{DFT}\leftarrow\text{GNN}(\mathbf{X}_{DFT},\mathbf{A}_{DFT})\) \(\mathcal{L}_{Pre-train}\leftarrow\text{MAE}(\hat{\mathbf{H}}_{DFT},\mathbf{H}_{DFT})\) // Pre-train using DFT calculated data Initialize the GNN with weights of pre-trained GNN \(\hat{\mathbf{H}}_{Exp}\leftarrow\text{GNN}(\mathbf{X}_{Exp},\mathbf{A}_{Exp})\) \(\mathcal{L}_{Fine-tune}\leftarrow\text{MAE}(\hat{\mathbf{H}}_{Exp},\mathbf{Form }.\mathbf{E}_{Exp})\) // Fine-tune using Experimental data Calculate the formation energy (\(\mathbf{H}\)) of the target material in the dataset and the precursor set of all materials in the knowledge base (KB) using a fine-tuned GNN. // Calculate \(\Delta H\) Calculate \(\Delta G\)  Calculate \(\Delta G\)  Retrieve \(K\) materials with most negative \(\Delta G\) from KB per target material, then save the retrieved material sets. // Construct Retrieved Material Sets ```

**Algorithm 3**Pseudocode of NRE Retriever.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We diligently outlined the main challenges of the task and our contributions in the abstract and introduction sections Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We describe the limitation of our work in the Appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: Our work does not include theoretical resultsGuidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide implementation details in the SectionA in the Appendix. Additionally, we provide source code in anonymized way at https://github.com/HeewoongMoh/Retrieval-Retro. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We upload our source code to an anonymized repository https://github.com/HeewoongMoh/Retrieval-Retro in accordance with the double-blind policy, making it freely accessible to reviewers. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the details of training and test are included in Section4 and SectionA.2. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We indicate standard deviations in parentheses below the corresponding measured values. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We specify the GPU requirements for conducting the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: All content and results in this paper adhere to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We describe the positive & negative societal impact in SectionF. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work don't utilize any pretrained language models, generative models and any scraped datasets. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All copyrights reserved by the author(s), and all materials related to the paper will adhere to the copyright policies of NeurIPS. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: All details of proposed method are outlined in Section3 and SectionA. Furthermore, the source code of our work is available in an anonymized repository at https://github.com/HeewoongNoh/Retrieval-Retro. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: NA Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.