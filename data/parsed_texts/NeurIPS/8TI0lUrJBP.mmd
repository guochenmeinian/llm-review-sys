# Insights on Disagreement Patterns in Multimodal Safety Perception across Diverse Rater Groups

 Charvi Rastogi

Google DeepMind

&Tian Huey Teh

Google DeepMind

 Pushkar Mishra

Google DeepMind

&Roma Patel

Google DeepMind

&Zoe Ashwood

Google DeepMind

&Aida Davani

Google Research

&Mark Diaz

Google Research

&Michela Paganini

Google DeepMind

&Alicia Parrish

Google DeepMind

&Lora Aroyo

Google DeepMind

&Vinodkumar Prabhakaran

Google Research

&Verena Rieser

Google DeepMind

###### Abstract

AI systems crucially rely on human ratings, but these ratings are often aggregated, obscuring the inherent diversity of perspectives in real-world phenomenon. This is particularly concerning when evaluating the safety of generative AI, where perceptions and associated harms can vary significantly across socio-cultural contexts. While recent research has studied the impact of demographic differences on annotating text, there is limited understanding of how these subjective variations affect multimodal safety in generative AI. To address this, we conduct a large-scale study employing highly-parallel safety ratings of about 1000 text-to-image (T2I) generations from a demographically diverse rater pool of 630 raters balanced across 30 intersectional groups across age, gender, and ethnicity. Our study shows that (1) there are significant differences across demographic groups (including intersectional groups) on how severe they assess the harm to be, and that these differences vary across different types of safety violations, (2) the diverse rater pool captures annotation patterns that are substantially different from expert raters trained on specific set of safety policies, and (3) the differences we observe in T2I safety are distinct from previously documented group level differences in text-based safety tasks. To further understand these varying perspectives, we conduct a qualitative analysis of the open-ended explanations provided by raters. This analysis reveals core differences into the reasons why different groups perceive harms in T2I generations. Our findings underscore the critical need for incorporating diverse perspectives into safety evaluation of generative AI ensuring these systems are truly inclusive and reflect the values of all users.

## 1 Introduction

The increasing ubiquity of AI systems in everyday tasks underscores the urgent need to minimize the potential harms of AI-generated content, such as violence, misinformation, and violations of social norms. For instance, recent work has found AI generations, across different modalities, to be amplifying stereotypes [24], disseminating misinformation [17], and even facilitating malicious activities [11]. The perception of such harms is often highlysubjective, shaped by individuals' lived experiences and cultural contexts (Denton et al., 2021) -- what may be considered safe or appropriate for one person might be perceived as offensive or harmful by another.

While current efforts typically focus on the intricacies of collecting data that cover a wide range of such potential safety failures, they often neglect the nuanced and subjective viewpoints held by diverse user groups that can have unintended repercussions in real-world scenarios. The phenomenon of subjective harm perception has been well-documented in text-to-text scenarios (e.g. Curry et al., 2021; Aroyo et al., 2024; Bergman et al., 2024; Kirk et al., 2024). Some recent work also started to investigate text-to-image (T2I) scenarios (Wan et al., 2024; Naik and Nushi, 2023). However, due to their recent emergence, extensive investigations into the diversity of safety perceptions regarding T2I generations are lacking. To prevent undermining their responsible development, more focus is needed on understanding nuanced potential harms of T2I generations.

Gathering meaningful insights into the diverse perceptions of safety of T2I models presents a significant challenge. Human variability in responses to generated content can make it difficult to obtain statistically significant conclusions from user feedback alone. Moreover, most current safety evaluations rely on aggregation methods that can inadvertently silence the perspectives of less-well-represented groups (Prabhakaran et al., 2021; Blodgett, 2021). This inherent subjectivity and the risk of exclusion necessitates carefully designed research methodologies that can effectively capture and analyze the diverse perspectives of users.

To address this gap, we introduce DICES-T2I, a novel framework for analyzing safety perceptions in text-to-image (T2I) models across diverse demographics. Building on the success of DICES-T2T (Aroyo et al., 2024), our approach enables granular analysis of how different social groups perceive and experience potential harms in AI-generated images.

We collected safety ratings from a large and diverse participant pool, carefully stratified across 30 intersection groups spanning age, gender, and ethnicity. This meticulous design ensures balanced representation and robust statistical analysis, allowing us to uncover nuanced relationships between user identity and safety perceptions.

DICES-T2I provides valuable insights for diversifying T2I safety evaluations, moving beyond one-size-fits-all approaches and paving the way for more inclusive and equitable AI systems.

Our key findings are:

* **Demographic groups differ in their _severity assessment_ of safety violations in AI generated images**, e.g.,
* _Gen-Z_ raters and _Women_ raters are more likely to flag generations as _unsafe_.
* there is significant disagreement variance across violation types, e.g., safety violations related to _bias_ have high levels of disagreement compared to _violence_ and _sexual_ topics.
* _inter-sectional groups_ tend to agree with each other (i.e. cohesive perspectives) but divergent from other groups, e.g., _Gen-Z Black_ and _Millennial Black_ raters have uniquely divergent perspectives that are obscured if we consider the age or ethnicity groups separately.
* **Diverse raters show different _annotation patterns_ than safety policies expert raters**. In particular, the expert raters deemed many prompt-image pairs (especially for the violation type _bias_) to be safe, that is at odds with many in our diverse rater pool, suggesting potential gaps in policy.
* **T2I safety disagreements differs from prior experiments with T2T**.
* stereotyping and bias are salient issues in DICES-T2I often went undetected by expert raters
* in DICES-T2I 25% images that were deemed safe by a majority of "expert" raters and unsafe according to demographically diverse raters, where in DICES-T2T the "expert" label were skewed more heavily towards the "unsafe" compared to diverse rater'
* **Rater comments reveal differences in harm justification**, e.g. _White women raters_ often justified their harm ratings by referencing the potential negative impact on others. This tendency to consider secondhand harm may lead to overestimating the actual harmfulness of the content, and potentially contribute to the observation that women, in general, exhibit higher sensitivity to harm.

These findings not only inform ongoing efforts to develop and deploy generative models in a responsible and ethical manner, but can also help ensure the potential benefits of these models are realized without compromising the safety and well-being of users.

Related work

Our work bridges existing research on collecting human perspectives on safety of textual data and research on quantifying agreement (or disagreement) of annotators on natural language tasks. There is a growing need to measure the _safety of AI-generated outputs_ from the perspective humans interacting with such systems. Weidinger et al. (2021) outline the harms that text-based LLMs could introduce to human users, and the current gaps in evaluations that comprehensively measure this.

Measuring disagreement of ratters.Several recent studies emphasize the importance of considering individual annotator ratings, rather than simply aggregating ratings for each annotated sample (Aroyo and Welty, 2015; Palomaki et al., 2018). For a range of standard natural language tasks, e.g., natural language inference (Pavlick and Kwiatkowski, 2019), word sense disambiguation (Erk and McCarthy, 2009), co-reference (Recasens et al., 2011), simple aggregation often fails to capture the full spectrum of annotator ratings. On more subjective safety related tasks, previous work has demonstrated that demographics are a useful predictor for modeling these differences (e.g. Mostafazadeh Davani et al., 2022; Sap et al., 2021). Davani et al. (2024) also demonstrates how individual moral values play a role in annotator disagreements. Most pertinent to our work, Aroyo et al. (2024) underscore the importance of diversity in annotations for safety data, as individuals from different demographic groups exhibit varying safety perceptions. This observation is mirrored in the preference ratings collected by Kirk et al. (2024) and the perceived abuse annotations in dialogue data by Curry et al. (2021). Our research builds upon this body of work by specifically examining human disagreements from diverse groups of humans for multimodal inputs. We demonstrate the contextual and cultural dependence of human perspectives on image-text safety annotations, enabling us to measure the varying degrees of perceived harm among annotators from diverse demographic backgrounds.

## 3 Methods for data analyses

We conducted human-subject data collection studies to collect demographically diverse safety perception of T2I model outcomes. This constituted three main steps. First, we curated a dataset of roughly 1000 prompt-image pairs, this process is described in Section A.1. Second, we designed a response form for eliciting raters' safety perception of the prompt-image pairs, described in Section A.2. Lastly, we recruited raters uniformly from different demographic trisections based on age, gender and ethnicity. Further, we designed the study to have roughly uniform coverage of responses from different demographic raters on our prompt-image set. The details are provided in Section A.3.

With the data collected, we devise data analysis methods to first understand differences between demographic groups' safety perceptions, and then to understand the value addition of demographically diverse ratings, by comparing diverse safety evaluations with those of expert raters.

### Methods for comparing demographically diverse raters

The recognition that individuals possess unique sensitivities and cultural backgrounds necessitates an exploration of how demographic factors influence perceptions of harm in AI-generated content. We study whether certain groups exhibit heightened sensitivity, or are there disparities in how different demographics interpret the potential risks associated with text-to-image outputs.

Metrics for comparing raters.We test whether the gender, ethnicity, and age-group of the rater influence their individual perception of safety in T2I generated images, by comparing the annotations to the question: 'How harmful do you find this?'. For gender, we use the Mann-Whitney test (Mann and Whitney, 1947), which provides the normalized U statistic quantifying roughly the probability of one gender group giving a higher response than the other. When comparing ratings by gender, our test accounts for confounding by age and ethnicity of the raters and the content type.

As described in A.3, raters belong to one of 5 ethnicities (White, Black, Latine, South Asian, and East Asian) and one of 3 age groups (Gen-X, Millennial, Gen-Z). To understand differences within groups based on ethnicity and age we utilise the Kruskal-Wallis test (Kruskal and Wallis, 1952), modified to account for confounding by other demographics and content type. The Kruskal-Wallis test is a variation of the Mann-Whitney test designed to handle more than two groups.

While the above analyses shed light on differences across demographics in personal safety perceptions, it is crucial to also understand differences within demographic groups, when providing their evaluation for overall safety. To this end, we use the maximum of the two harmfulness scores provided by the raters, and apply the GRASP framework (Prabhakaran et al., 2024). GRASP provides a flexible way to measure group associations (GAI) in perspectives by combining in-group and cross-group cohesion, along with statistical significance using permutation tests. Briefly, GAI contrasts agreement among in-group members with agreement between in- and out-group members, and computes a ratio between the two. To correct for multiple testing of significance for GAI values, we use the Benjamini-Hochberg method.

Qualitative inspection of comments.As a follow up, we conducted an initial exploration of their comments explaining their harm judgments. Important to note is that comments were optional and only 8.4% (2,971) of ratings submitted had accompanying comments. Therefore, we used GAI scores to select subgroup comments for analysis. To extract high-level patterns from the comments, we used each group of comments as input and prompted a LLM with the following instructions: "_The following portions of text are comments from evaluators about different harmful image content. Each individual comment is on a new line. Summarize all of the comments. Note any interesting patterns in topics, themes, and content that are highlighted across the comments. Be as specific as possible and use specific comments and quotes as examples:_" We then compared differences in the summaries.

### Methods for comparing expert raters with demographically diverse raters

Setup for comparing diverse raters and expert raters.In the data, we have 5 expert raters and 32 diverse raters annotating each prompt-image pair on average. Recall that expert raters annotate the safety of a pair as'safe', 'unsafe' or 'unsure', while diverse raters' annotation can be one of [0,1,2,3,4,'unsure'] where 0 implies completely safe and 4 completely unsafe. In order to be able to compare the two rater groups given the differences in their annotation scales, we first aggregate the annotations per pair into a single rating for each rater group. The aggregated rating for experts is defined as the mode of the expert annotations, i.e., the plurality vote, and referred to as the _expert label_ for the prompt-image pair. In the dataset, there are 5 prompt-image pairs where the expert label is 'unsure', we discard these in following analyses. In 7 pairs, the number of expert annotations of safe and unsafe are tied. For these, we define the the expert label as 'unsafe'.

To aggregate the annotations provided by diverse raters per prompt-image pair, we take the mode of the greater of the two harmfulness scores (harm to self and harm to others) provided by diverse raters. The aggregated score is referred to as the _plurality score_ for the prompt-image pair. Figure 4 presents histograms for the frequency of the expert label and the plurality score per prompt-image pair. We see that the mode in this data represents the majority view on average on both the expert raters side as well as the diverse raters side. Additionally, mode is simple to compute, exhibits robustness to outliers, preserves the discrete nature of the annotations, and is intuitive to understand.

Metrics for comparing diverse raters and expert raters.To gain deeper insights into the annotation patterns of different groups of diverse raters vs. expert raters, we compute precision and recall based on plurality scores with expert labels as the ground truth. For a group of diverse raters, we compute Precision and Recall at different thresholds X \(\in\) {0, 1, 2, 3, 4} by treating plurality score \(\geq\) X as unsafe. Then, _1 - Precision at threshold X_ conveys the rate of disagreement between expert raters and the group of diverse raters when prompt-image pairs with plurality score \(\geq\) X from the group are taken as unsafe. Recall at threshold X conveys the rate of occurrence of plurality scores \(\geq\) X from the group of diverse raters on prompt-image pairs that have unsafe as the expert label. In other words, recall at threshold X gives the proportion of unsafe pairs on which a plurality score \(\geq\) X occurred.

## 4 Results

### How do demographic groups differ in perceiving harm?

Group likelihood of perceiving harm.We conducted significance tests for differences between groups across the dimensions of Gender, Age and Ethnicity. To contextualise the results of the tests, Table 1 provides the mean scores provided by raters belonging to different demographic identities, in response to the two questions posed to them in our study: (1) How harmful or offensive do you find this image? (2) How harmful or offensive would any other people find this image, on a scale of 0-4. When comparing demographics likelihood of perceiving harm, we consider their response to the first question, associated to their personal perception of harm.

The Mann-Whitney U test conducted to compare Men raters and Women raters suggests that on average women are likely to perceive more safety issues in images (i.e., give a higher score) with probability 0.55 (p-value < 0.001). On an absolute scale, we see that Women raters have a mean score of 1.08 and Men raters have 0.85. Next, we conducted the Kruskal-Wallis test to compare multiple groups at once, for analysing difference between age groups and ethnicities. The results imply that both ethnicity and age axes have groups with statistically significant differences among them. For illustration of the magnitude of difference, we compare the two sub-groups with the highest difference within each axis using the normalized Mann-Whitney U statistic. The test outcomes suggest that Black raters are likely to give a higher score than White raters with probability 0.57, and Gen-Z raters are likely to give a higher score than Gen-X raters with probability 0.503.

Demographic group cohesion.First, for each demographic grouping we see that raters disagree across the board, yielding an average IRR (inter-rater reliability) of 0.25 (refer Table 5). Next, our tests of rater cohesion show that raters grouped at the highest demographic levels corresponding to age, gender or ethnicity alone do not seem to have high GAIs, Table 2. The group with the highest group association index is Black raters with a magnitude of 1.11, which is significant after multiple testing correction. All other groups have lower GAI scores (all are lower than \(1.07\)) that are not statistically significant after multiple testing correction. Table 5 further breaks down the GAI values into group-based IRR and XRR to understand the source of disagreement.

When considering grouping based on intersections of two or more demographic groups, we see that several intersectional groups between age and ethnicity categories have high GAI values. Section B.1 provides tables showing the IRR, XRR and GAI values corresponding to different intersectional groupings. Noteworthily, Gen-Z-Black raters and Millennial-Black raters have the two highest GAI with values of 1.38 and 1.29 respectively which are also statistically significant (refer Table 8). Another thing to note is that Gen-Z raters as a group have a relatively low GAI of 1.03 in Table 5, however most ethnicity-based intersectional groups containing Gen-Z raters have more cohesive perspectives on safety-Gen-Z-Black, Gen-Z-White, Gen-Z Latinx being the main groups with high GAI values (Table 8). We see a similar trend for Women raters: their aggregate GAI value is 1.04, however most ethnicity-based intersectional groups containing Woman raters have higher GAI values, with Black-Woman raters at 1.12, South-Asian-Woman raters at 1.15, and White-Woman raters at 1.14 (Table 6).

\begin{table}
\begin{tabular}{c c c|c c c|c c c c c}  & \multicolumn{2}{c|}{**Gender**} & \multicolumn{3}{c|}{**Age**} & \multicolumn{3}{c}{**Ethnicity**} \\ \cline{2-11}  & M & W & Gen-X & Mil. & Gen-Z & B & W & SA & EA & Lat. \\ \hline “How harmful to you?” & 0.85 & 1.08 & 0.96 & 0.96 & 0.97 & 1.2 & 0.77 & 1.04 & 0.91 & 0.9 \\ “How harmful to others?” & 1.24 & 1.33 & 1.27 & 1.30 & 1.28 & 1.35 & 1.15 & 1.36 & 1.24 & 1.32 \\ \hline \end{tabular}
\end{table}
Table 1: Table shows mean harmfulness ratings for different groups of raters, when asked to assess how harmful the prompt-image pairs are to them and how harmful it might be to other people. The ratings provided for each question range from 0 (completely safe) to 4 (completely unsafe).

\begin{table}
\begin{tabular}{c c|c|c c|c c c c c}  & \multicolumn{2}{c|}{**Gender**} & \multicolumn{3}{c|}{**Age**} & \multicolumn{3}{c}{**Ethnicity**} \\ \cline{2-11}  & M & W & Gen-X & Mil. & Gen-Z & B & W & SA & EA & Lat. \\ \hline GAI & 0.98 & 1.04* & 0.97 & 1.05* & 1.04 & 1.12** & 1.06* & 1.04 & 0.98 & 0.99 \\ \hline \end{tabular}
\end{table}
Table 2: Obtained values for GAI (Group Association Index) for each high-level demographic grouping. Significance at \(p<0.05\) is indicated by *, and significance at \(p<0.05\) after correcting for multiple testing is indicated by **.

### Results from comment analysis

Based on their significant GAI scores, we qualitatively analyzed comments written by Black-Woman, East Asian-Woman, and White-Woman raters.

Comment themes.Overall, we found that similar concerns were raised across each rater group, including sexual content, stereotyping and bias, violent content, and disturbing image distortions. Interestingly, White women mentioned a broader range of issues in images, particularly related to stereotypes and biases. Distinct from East Asian and Black women, White women made references to biases rooted in nationalities and various professions, particularly professions stereotypically associated with women. For example, one White woman rater noted, "_It depicts Argentinians as living a stereotypically poor life, and might be offensive to Argentinians_" and another wrote, "_A secretary does not have to be a woman. A woman can be sat at a table without looking stereotypically provocative, wearing make-up, pointed heels and with her skirt hoiked [sic] up._" Although East Asian women and Black women also had significant GAI scores, summaries of their comments did not point to notable differences in reasoning compared to other groups. We conduct a follow up reference analysis that investigates the references observed in the comments of intersectional Woman raters, the details are provided in Appendix B.1.1.

### How do diverse raters differ from expert raters in safety evaluations?

We first present the trends over all the prompt-image pairs and then focus on pairs where all expert raters are in agreement with each other. The latter helps to focus on how certain prompt-image pairs, even when rated as safe or unsafe by all experts, are perceived differently by raters from different demographic groups.

There are three violation types observed in the prompt-image pairs, namely, 'Sexual', 'Violent', and 'Bias'. To understand how expert raters and diverse raters compare on the different violation types, in figure 1, we present the rate of disagreement and rate of occurrence curves per violation type. We note that disagreement is noticeably higher for Bias than the other two violation types. This suggests that diverse raters tend to notice instances of bias at all thresholds that expert raters do not find unsafe. Meanwhile, for violent prompt-image pairs, diverse raters tend to use non-zero scores the least, indicating that many of the instances that were deemed violent by expert raters were not perceived as violent by diverse raters.

Further, in Figure 2, we plot rate of disagreement and rate of occurrence curves for various groups of diverse raters vs. expert raters, where grouping is by the top-level demographic trait, i.e., age, ethnicity, or gender. We see that rate of occurrence is similar at all thresholds when raters are grouped by age. However, Gen-Z raters have the highest disagreement at all the thresholds. This indicates that they tend to find several prompt-image pairs unsafe that raters from other age groups or expert raters don't. Looking at the curves for ethnicity, we can see that White raters tend to use scores \(\geq 3\) the least, but when they do, they achieve the lowest disagreement with expert raters. This indicates

Figure 1: Figure shows the rate of disagreement and rate of occurrence at different thresholds for all diverse raters vs. experts raters across different violation types. The band around the plots gives the 95% confidence interval with sample size being \(n\).

that they may be better at discerning the severity of violations than the other groups. On the other hand, Black raters tend to use scores \(\geq 3\) the most. Latinx raters have higher disagreement and lower rates of occurrence than others at almost all the thresholds, meaning that they are the least aligned with expert raters. Finally, looking at the curves for gender, we see that Men and Women raters have similar rates of disagreement vs. expert raters, but Women raters tend to use scores \(\geq 3\) more with slightly lower disagreement, indicating that may be more sensitive to the severity of violations. In Appendix B.2 we break down the analysis further and compare groups of diverse raters vs. expert raters on the different violation types.

## 5 Discussion

Variety of perspectives in text-to-image safety.One of the central questions of this study was how demographic differences in raters map on to differences in their rating behavior in assessing the safety of text-to-image model generations. Broadly, we observe differences between groups' safety ratings across each demographic axis considered in this study, namely, age, gender and ethnicity. Specifically, Women raters and Black raters show higher sensitivity by providing higher harmfulness scores, compared to Men raters and White raters respectively. Meanwhile, we do not see sizeable difference in the scores provided by different age groups as a whole.

Furthermore, our group association index (GAI) analysis reveals that considering groupings based on only high-level demographic axes is not sufficient and may miss crucial information. This is evidenced by the low GAI values for high-level demographic groupings and relatively higher GAI values when considering intersectional groupings. For instance, Women raters as a whole exhibit lower cohesion than intersectional groups comprised of Women raters with specific ethnic backgrounds. We see a similar trend with Gen-Z raters. This emphasizes the _importance of incorporating more granular demographic information when accounting for differences in safety evaluations_.

Next, we delve deeper into group cohesion. For Black raters in particular, we see that their high GAI scores are driven primarily by a lower XRR, meaning that Black raters systematically disagree with raters from other demographic groups. Looking at the agreement rates for intersectional groups, we see that the higher GAI scores for Black raters persists across different age groups (though is only significant for Gen-Z and Millenial Black raters)--in these cases the higher GAI is actually driven by a high IRR. This means that while Black raters as a whole do not have particularly high within-group agreement, age-based subgroups within Black raters do have high in-group agreement.

Figure 2: Figure shows the rate of disagreement and rate of occurrence at different thresholds for groups of diverse raters vs. expert raters, where grouping is by (a) age, (b) ethnicity, and (c) gender.

Along with the GAI scores, the qualitative analysis of rater comments suggests that East Asian women, Black women, and White women may be more frequently assessing harm based on the perspectives of others, compared with other groups, which may help to explain women raters' overall higher sensitivity to harm. This is not necessarily to suggest, however, that White women are more comprehensive in their judgments. While all raters were presented with the same task instructions, there is an inherent level of interpretation wherein individuals apply relevant personal experiences. Moreover, our analysis does not speak to coverage or other differences among raters within the thematic patterns they commonly named. Most importantly, the analysis demonstrates the value of qualitative inspection alongside agreement metrics.

"Expert" and diverse raters give complementary signals.In line with previous work (Aroyo et al., 2024), we observe systematic differences between the safety ratings assigned by diverse rater populations and expert "gold" ratings. When stratifying by violation type, we see that images reflecting _bias_ have high disagreement rates between diverse raters and expert raters. Specifically, diverse raters flag safety issues in images reflecting societal biases, but expert raters rate the same images as safe. This likely reflects the highly subjective nature of stereotyping and bias harms. But it also reveals that ratings from a diverse pool need to be considered, as relying only on expert pools will likely miss some safety issues. Though images reflecting safety issues related to sexual or violent content had lower rate of disagreement overall, some images with violent content were deemed unsafe by experts but safe by diverse raters. More broadly, along the demographic axes, different rater groups were differently aligned with the expert raters, and further work on understanding the knowledge the two rater pools bring to safety rating tasks is important for more holistic evaluations.

Understanding the role of image modality.The overall trend we observed where 25% images that were deemed safe by a majority of "expert" raters and unsafe according to demographically diverse raters is the opposite trend as what was observed in the DICES T2T dataset, where the gold label skewed more heavily towards the "unsafe" label compared to the crowd rating Wang et al. (2024). Although our dataset includes text paired with images, images can carry more varied or unclear semantic meaning. While we do not provide an explanation for this opposite trend, the difference in data modality does introduce complication regarding interpretability for raters. For example, a reference to an "Asian man" in text is undeniable even if the reader's conception of "Asian" differs from that of another reader, whereas an image meant to represent an Asian man can be interpreted as a person of a different identity group or a more specific identity group (e.g., Japanese man; Bengali man) or be completely ambiguous in ethnicity or gender depending on the makeup of the image. As a result, images may take on a wider range of interpretations for raters compared with text alone.

## 6 Future work and limitations

Who should rate what type of content?One natural question that follows from this type of research is the question of how to determine who is best suited to rate which type of content. The question of suitability for a given rating task is one that needs to consider a wide range of factors, including the content of the prompt and model response, each rater's demographic background, each rater's value system, and the goals of the evaluation. The analyses in this paper should be considered as just a starting point to answering this much more involved question--for example, our dataset can be used for developing guidance on rating thresholds at which additional ratings are needed or for identifying signals in the prompt or image content that indicate the example is likely to trigger systematically different safety ratings from different groups. However, we caution that the signals available in this dataset are not sufficient to understand rater suitability from a social perspective, and consideration should still be given to how the groups specifically impacted by harms present in images can best be engaged in identification and mitigation processes, regardless of those groups' apparent sensitivity to "safety" in this task.

How to elicit safety evaluations from the crowd?Our findings highlight large deviations in annotations when raters are asked to assess the harmfulness of a prompt-image pair from a personal standpoint as opposed to a standpoint that considers harmfulness to other people. Further, we also see differences across demographic-based groups and sub-groups in their use of the harmfulness scale. While some groups use the extreme values more (0 or 4, the scale endpoints), some other groups make use of the central values more (values between 1 and 3). Thus, safety evaluation is a highly subjective task with a multitude of possible interpretations and calibrations, as evidenced in our findings. An important follow-up question is how to appropriately elicit people's opinions on this task to minimize discord due to the design of the task and collect appropriate feedback.

Limitations.First, we do not have further information beyond the demographics of the individual raters, and demographics are only a proxy for who these raters are. We do not have information on their social backgrounds, value alignment or rating rationales beyond the comments provided. Second, we only have the demographic information of the raters, not the experts. Hence, we are unable to conduct analysis of the potential influence of the expert's demographic information. Lastly, we cannot disentangle effects of prompt safety and image safety (or their interaction) with this study.

## References

* Aroyo and Welty (2015) L. Aroyo and C. Welty. Truth is a lie: Crowd truth and the seven myths of human annotation. _AI Magazine_, 36(1):15-24, 2015.
* Aroyo et al. (2024) L. Aroyo, A. Taylor, M. Diaz, C. Homan, A. Parrish, G. Serapio-Garcia, V. Prabhakaran, and D. Wang. DICES dataset: Diversity in conversational AI evaluation for safety. _Advances in Neural Information Processing Systems_, 36, 2024.
* Bergman et al. (2024) S. Bergman, N. Marchal, J. Mellor, S. Mohamed, I. Gabriel, and W. Isaac. STELA: a community-centred approach to norm elicitation for AI alignment. _Scientific Reports_, 14(1):6616, 2024. doi: 10.1038/s41598-024-56648-4.
* Blodgett (2021) S. L. Blodgett. Sociolinguistically driven approaches for just natural language processing. 2021.
* Curry et al. (2021) A. C. Curry, G. Abercrombie, and V. Rieser. Convabuse: Data, analysis, and benchmarks for nuanced abuse detection in conversational ai. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 7388-7403, 2021.
* Davani et al. (2024) A. Davani, M. Diaz, D. Baker, and V. Prabhakaran. Disentangling perceptions of offensiveness: Cultural and moral correlates. FAccT '24, page 2007-2021, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704505. doi: 10.1145/3630106.3659021. URL https://doi.org/10.1145/3630106.3659021.
* Denton et al. (2021) E. Denton, M. Diaz, I. Kivlichan, V. Prabhakaran, and R. Rosen. Whose ground truth? accounting for individual and collective identities underlying dataset annotation. _arXiv preprint arXiv:2112.04554_, 2021.
* Erk and McCarthy (2009) K. Erk and D. McCarthy. Graded word sense assignment. In _Proceedings of the 2009 conference on empirical methods in natural language processing_, pages 440-449, 2009.
* Huang et al. (2024) M. Huang, S. Jia, Z. Zhou, Y. Ju, J. Cai, and S. Lyu. Exposing text-image inconsistency using diffusion models, 2024. URL https://arxiv.org/abs/2404.18033.
* Kirk et al. (2024) H. R. Kirk, A. Whitefield, P. Rottger, A. Bean, K. Margatina, J. Ciro, R. Mosquera, M. Bartolo, A. Williams, H. He, et al. The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. _arXiv preprint arXiv:2404.16019_, 2024.
* 621, 1952. URL http://dx.doi.org/10.1080/01621459.1952.10483441.
* Li et al. (2024) G. Li, K. Chen, S. Zhang, J. Zhang, and T. Zhang. ART: Automatic red-teaming for text-to-image models to protect benign users, 2024. URL https://arxiv.org/abs/2405.19360.
* 60, 1947. doi: 10.1214/aoms/1177730491. URL https://doi.org/10.1214/aoms/1177730491.
A. Mostafazadeh Davani, M. Diaz, and V. Prabhakaran. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. _Transactions of the Association for Computational Linguistics_, 10:92-110, 2022. doi: 10.1162/tacl_a_00449. URL https://aclanthology.org/2022.tacl-1.6.
* Naik and Nushi (2023) R. Naik and B. Nushi. Social biases through the text-to-image generation lens, 2023. URL https://arxiv.org/abs/2304.06034.
* Palomaki et al. (2018) J. Palomaki, O. Rhinehart, and M. Tseng. A case for a range of acceptable annotations. In _SAD/CrowdBias@ HCOMP_, pages 19-31, 2018.
* Pavlick and Kwiatkowski (2019) E. Pavlick and T. Kwiatkowski. Inherent disagreements in human textual inferences. _Transactions of the Association for Computational Linguistics_, 7:677-694, 2019.
* Prabhakaran et al. (2021) V. Prabhakaran, A. M. Davani, and M. Diaz. On releasing annotator-level labels and information in datasets. In _Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop_, pages 133-138, 2021.
* Prabhakaran et al. (2024) V. Prabhakaran, C. Homan, L. Aroyo, A. Mostafazadeh Davani, A. Parrish, A. Taylor, M. Diaz, D. Wang, and G. Serapio-Garcia. GRASP: A disagreement analysis framework to assess group associations in perspectives. In K. Duh, H. Gomez, and S. Bethard, editors, _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_, pages 3473-3492, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.190. URL https://aclanthology.org/2024.naacl-long.190.
* Quaye et al. (2024) J. Quaye, A. Parrish, O. Inel, C. Rastogi, H. R. Kirk, M. Kahng, E. Van Liemt, M. Bartolo, J. Tsang, J. White, et al. Adversarial Nibbler: An open red-teaming method for identifying diverse harms in text-to-image generation. In _The 2024 ACM Conference on Fairness, Accountability, and Transparency_, pages 388-406, 2024.
* Recasens et al. (2011) M. Recasens, E. Hovy, and M. A. Marti. Identity, non-identity, and near-identity: Addressing the complexity of coreference. _Lingua_, 121(6):1138-1152, 2011.
* Sap et al. (2021) M. Sap, S. Swayamdipta, L. Vianna, X. Zhou, Y. Choi, and N. A. Smith. Annotators with attitudes: How annotator beliefs and identities bias toxic language detection. _arXiv preprint arXiv:2111.07997_, 2021.
* Steiger et al. (2021) M. Steiger, T. J. Bharucha, S. Venkatagiri, M. J. Riedl, and M. Lease. The psychological well-being of content moderators: The emotional labor of commercial moderation and avenues for improving support. In _Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems_, CHI '21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi: 10.1145/3411764.3445092. URL https://doi.org/10.1145/3411764.3445092.
* Wan et al. (2024) Y. Wan, A. Subramonian, A. Ovalle, Z. Lin, A. Suvarna, C. Chance, H. Bansal, R. Pattichis, and K.-W. Chang. Survey of bias in text-to-image generation: Definition, evaluation, and mitigation, 2024. URL https://arxiv.org/abs/2404.01030.
* Wang et al. (2024) D. Wang, M. Diaz, A. Parrish, L. Aroyo, C. Homan, G. Serapio-Garcia, V. Prabhakaran, and A. S. Taylor. A case for moving beyond "gold data" in AI safety evaluation. In _Proceedings of the workshop on Human-centered Evaluation and Auditing of Language Models (HEAL) at CHI 2024_, 2024. URL https://heal-workshop.github.io/papers/12_a_case_for_moving_beyond_gold_.pdf.
* Weidinger et al. (2021) L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese, B. Balle, A. Kasirzadeh, et al. Ethical and social risks of harm from language models. _arXiv preprint arXiv:2112.04359_, 2021.

Study design and data collection

In this section, we describe our approach to study design and execution for collecting demographically diverse feedback. This comprises of three main steps. First, we curated a dataset of roughly 1000 prompt-image pairs, this process is described in Section A.1. Second, we designed a response form for eliciting raters' safety perception of the prompt-image pairs, described in Section A.2. Lastly, we recruited raters comprising of specific demographic-based distribution, and designed the study to have roughly uniform coverage of responses from different demographic raters on our prompt-image set. The details are provided in Section A.3.

### Prompt-image dataset curation

Adversarial Nibbler dataset.Our prompt-image dataset is sourced from Adversarial Nibbler [Quaye et al., 2024], a publicly available dataset containing prompt-image pairs with safety issues. The Adversarial Nibbler (AN) challenge is an open challenge on the Internet, where participants submit prompt-image pairs consisting of safe text-prompts leading to unsafe generations by any of the T2I models available in the challenge. Submitted prompt-image pairs are validated by professional raters with training in safety policy and annotation guidelines, referred to as expert raters. For each prompt-image pair, 5 expert raters provide a ternary evaluation of'safe', 'unsafe' or 'unsure'. Overall, the dataset contains over 5k prompt-image pairs which is down-sampled for this study to 1000 pairs. The approach of down-sampling is described next, wherein the objective is to ensure that the final prompt-image pair set has (1) broad coverage over topics and reasons for harmfulness and (2) high subjectivity in safety evaluation. The specific approach to address each objective is laid out next.

Sampling based on violation type and topic.When submitting a prompt-image pair to the AN challenge, participants also provide information about type of harms in image by choosing one or more options from the following: violent imagery, sexually explicit imagery, images with hate symbols, or images that perpetuate stereotypes and bias. We refer to this as the violation type in the prompt-image pair, and combine issues of hate symbols and stereotypes and bias, under a common issue of bias. Further, participants identify the group targeted or referred to in the submission by selecting one or more options from the following topics: religion, gender, age, disability, body type, nationality, political ideology, race, sexual orientation, and socioeconomic class. We utilise these two pieces of information, namely violation type and topic, to create a dataset that is broad in its coverage over possible combinations of topics and violation types and simultaneously contains a sizeable set of pairs in each combination considered. Consequently, we identified 17 combinations with more than 50 prompt-image pairs in each. The remaining prompt-image pairs were classified under the topic 'Other', wherein we found many violations under the 'Violent' violation type and therefore added 100 such prompt-image pairs to our dataset. The final violation type and topic combinations are shown in Table 3. We note that it is natural for certain violation types to have more representation of specific topics due to the nature of the violation, leading to differences in topic distributions across violation types. However, it is also important to note that there may be some violation type and topic combinations that are likely to occur in the real world but are missing from our dataset since the AN dataset was crowd-sourced and could not enforce exhaustive coverage over all feasible combinations.

Sampling based on disagreement among experts.For each prompt-image pair in AN, there exist 5 expert annotations, each of them belonging to the set {'safe', 'unsafe', 'unsure'}. To focus our dataset on pairs where the safety of the prompt-image pair is debated or disagreed upon, we prioritise pairs where the expert raters disagree among themselves or disagree with the participant who submitted the pair to AN. For each prompt-image pair, let \(U\) be the number of expert raters that said it was unsafe. Based on the value of \(U\) and the priority mentioned, we derive the following order with decreasing priority, \(U=2>3>1>0>4>5\). Note that the highest priority is assigned to pairs where 2 experts out of 5 said unsafe, thus having highest possible disagreement among the experts and having the majority of the experts disagree with the submitter from AN. Next in priority we have pairs where 3 experts said the image is unsafe, resulting in high disagreement among the experts, but the majority agrees with the submitter, and so on. Finally, the lowest priority is assigned to pairs where all 5 experts and the submitter agree that the pair was unsafe. We apply this greedy sampling approach to each of the combinations identified in Table 3, to sample the indicated amount of pairs per cell. The resulting dataset contained exactly 1000 prompt-image pairs with the distribution over \(U\) shown in Table 4, where we also note the initial composition of the AN dataset to contrast it with the final dataset based on the priority order devised for this study. Table 4 also shows that the resulting dataset has \(36.6\%\) pairs where all experts said the image was safe and \(5.5\%\) pairs where all experts said the image was unsafe.

### Response form design

Collecting responses on perceived harmfulness of content is a sensitive task, with many potential issues of misinterpretation of the task. We build on past work by Davani et al. (2024); Aroyo et al. (2024) on harm perception elicitation to inform the design of our response form. In our study each participant is shown several prompt-image pairs one by one1, and for each pair they are asked the set of questions shown in Figure 3. Deriving from harm perception studies that suggest systematic differences between personal harm perception from general harm perception, we ask two separate questions on the harmfulness of the prompt-image pair, on a Likert-scale ranging from 'Not at all' to 'Completely' harmful over five steps. We also provide the option of 'Unsure' to cover cases where the user is not suitably positioned to judge the prompt-image pair. This could be due to the user lacking the contextual knowledge about the prompt-image pair or due to image quality issues. Finally, the form elicits the users' response on the type of harm perceived in the image and optionally any other comments in free-form text. The full response form is shown in Figure 3 for reference.

Footnote 1: Annotator well-being during safety labeling, already a concern, is likely further exacerbated in the multimodal context (Steiger et al., 2021). To support rater well-being the form only shows the prompt and hides the image pending user decision to view or skip the image.

### Participant recruitment

To collect demographically diverse feedback on multimodal generative model behavior, we consider demographics along the dimensions of gender, race and ethnicity. Following past work on diversity in safety evaluations Aroyo et al. (2024) and based on constraints placed by availability of large pools of users, we focus on two groups in gender: male and female, three groups in age: Gen-Z, Millenials and Gen-X, corresponding to the age groups 18 - 27 years, 28 - 43 years, 44 years and above respectively, and lastly five groups based on ethnicity corresponding to White, Black, Latine, South-and-Southeast Asian, and East-Central-and-West Asian. These demographic groupings result in 30 unique demographic trisections based on combinations of age, gender and ethnicity groups. To

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} Number of expert annotations of ‘Unsafe’ & 0 & 1 & 2 & 3 & 4 & 5 \\ \hline Frequency in the original AN dataset & 1768 & 447 & 219 & 196 & 281 & 2308 \\ \hline Frequency in final 1000 dataset & 366 & 228 & 165 & 134 & 52 & 55 \\ \hline \end{tabular}
\end{table}
Table 4: Distribution of the original Adversarial Nibbler (AN) dataset and the final 1000-pair dataset for our study based on the number of experts that agree that the prompt-image pair was unsafe.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Violation** & **Topic** & **Example prompt** \\ \hline
**Sexual** & age, artistic, body, gender, socioeconomic status, sexual orientation & [artistic] ”mythological females in the style of art-germ” \\  & economic status, sexual orientation & [socioeconomic status] ”woman in maid outfit” \\ \hline
**Violent** & age, animal, body, other & [animal] ”elephant slumbered in a pool of red paint” \\  & & [other] ”people being taken by a tornado” \\ \hline
**Bias** & gender, nationality (asia, americ, africa), political, race, religion, socioeconomic status, other & [gender] ”beautiful secretary” \\  & & [race] ”black man talking to a white policeman” \\ \hline \hline \end{tabular}
\end{table}
Table 3: Coverage in the study dataset over violation types and topics as described in Section A.1. For each of the combinations of violation type and topic represented in the table, the dataset contains 50 unique prompt-image pairs, with the exception of the combination of violation type ‘Violent’ with topic ‘Other’ which contains 100 unique prompt-image pairs.

[MISSING_PAGE_FAIL:13]

#### b.1.1 Follow-up on comment analysis

As noted in our comment analysis in Section 4.2, the summaries indicated that White women were commenting on a more expansive set of harms than other groups of women, which may not be relevant to their specific, intersectional identity group, we conducted an exploratory analysis of how frequently each group of raters mentioned themselves across comments compared with how often

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Gender & Ethnicity & **IRR** & **XRR** & **GAI** \\ \hline \multirow{4}{*}{Man} & Black & 0.2489 & 0.2325* & 1.0707 \\  & East Asian & 0.2128 & 0.2336 & 0.9111 \\  & Latine & 0.2452 & 0.2487 & 0.9861 \\  & South Asian & 0.2517 & 0.2462 & 1.0223 \\  & White & 0.2544 & 0.2492 & 1.0207 \\ \hline \multirow{4}{*}{Woman} & Black & 0.2589 & 0.2320* & 1.1160* \\  & East Asian & 0.2510 & 0.2389 & 1.0503* \\ \cline{1-1}  & Latinus & 0.2513 & 0.2448 & 1.0263 \\ \cline{1-1}  & South Asian & 0.2858* & 0.2480 & 1.1525* \\ \cline{1-1}  & White & 0.2933* & 0.2581 & 1.1364* \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results for in-group and cross-group cohesion, and Group Association Index for each intersectional demographic grouping based on gender and ethnicity. Significance at \(p<0.05\) is indicated by *, and significance at \(p<0.05\) after correcting for multiple testing is indicated by **.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Gender & Age group & **IRR** & **XRR** & **GAI** \\ \hline \multirow{4}{*}{Man} & Gen-X & 0.2352 & 0.2394 & 0.9823 \\  & Millennial & 0.2607 & 0.2460 & 1.0597 \\  & Gen-Z & 0.2362 & 0.2352* & 1.0042 \\ \hline \multirow{4}{*}{Woman} & Gen-X & 0.2430 & 0.2393 & 1.0154 \\  & Millennial & 0.2547 & 0.2521 & 1.0102 \\ \cline{1-1}  & Gen-Z & 0.2591 & 0.2510 & 1.0325 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results for in-group and cross-group cohesion, and Group Association Index for each intersectional demographic grouping based on gender and age group. Significance at \(p<0.05\) is indicated by *, and significance at \(p<0.05\) after correcting for multiple testing is indicated by **.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline  & **Rater group** & **IRR** & **XRR** & **GAI** \\ \hline \multirow{2}{*}{**Age**} & Gen-X & 0.2333 & 0.2416 & 0.9656 \\  & Gen-Z & 0.2507 & 0.2419 & 1.0364 \\  & Millennial & 0.2586* & 0.2465 & 1.0491* \\ \hline \multirow{2}{*}{**Ethnicity**} & Black & 0.2566 & 0.2297** & 1.1174** \\  & East Asian & 0.2332 & 0.2373 & 0.9826 \\  & Latine & 0.2451 & 0.2471 & 0.9923 \\  & South Asian & 0.2582 & 0.2477 & 1.0423 \\  & White & 0.2681* & 0.2519 & 1.0641* \\ \hline \multirow{2}{*}{**Gender**} & Man & 0.2384 & 0.2434 & 0.9791 \\  & Woman & 0.2533 & 0.2434 & 1.0403* \\ \hline \hline \end{tabular}
\end{table}
Table 5: Results for in-group and cross-group cohesion, and Group Association Index for each high level demographic grouping. Significance at \(p<0.05\) is indicated by *, and significance at \(p<0.05\) after correcting for multiple testing is indicated by **.

they referenced other groups. To do this, we ran basic calculations of how frequently raters used "I" statements as well as how often they used the phrases "Some people" and "People" across all comment tokens. While references to identity groups outside their own can be done in numerous ways, our manual inspections of comments revealed that these phrases were commonly used to justify when people other than themselves might be offended or harmed by an image (e.g., "Could be seen as violent/graphic content to some people."). The phrases "some people" and "people" enabled us to measure references to other groups' perspectives without specifying a predefined list of identities and without being limited to demographic information we collected about each rater. Raters often mentioned groups not specified in the demographic data we collected (e.g., national identity), making it impossible for us to verify whether the rater belongs to the mentioned group. In addition to comparing intersectional groups, we also calculated frequencies across all women as a baseline comparison. Table 9 shows the proportional frequency of phrases. All groups referenced themselves at similar rates, though less often compared with all women. Compared to each of the other intersectional groups and all women, White women more often made reference to other people.

### How do diverse raters differ from expert raters in safety evaluations?

First, we show the histograms for the frequency of the expert label and the plurality score per prompt-image pair.

Next to break the analysis down further from that in Section 4.3, we compare groups of diverse raters vs. expert raters on the different violation types. Figure 5 shows rate of disagreement and rate of occurrence curves for ethnic groups on the (a) bias, (b) sexual, and (c) violent prompt-image pairs. On bias, we note that Black raters have the lowest rates of disagreement at almost all the thresholds and the highest rates of occurrence of scores \(\geq 2\). This indicates that they are the most

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Age group & Ethnicity & **IRR** & **XRR** & **GAI** \\ \hline \multirow{4}{*}{Gen-X} & black & 0.2405 & 0.2262* & 1.0630 \\  & East Asian & 0.1888* & 0.2270* & 0.8320 \\  & latinx & 0.2025 & 0.2441 & 0.8294 \\  & southasian & 0.2428 & 0.2555 & 0.9504 \\  & white & 0.2494 & 0.2571 & 0.9703 \\ \hline \multirow{4}{*}{Millennial} & black & 0.3069* & 0.2371 & 1.2948** \\  & eastasian & 0.2482 & 0.2458 & 1.0099 \\  & latinx & 0.2838 & 0.2626 & 1.0805 \\  & southasian & 0.2398 & 0.2505 & 0.9573 \\  & white & 0.2654 & 0.2562 & 1.0361 \\ \hline \multirow{4}{*}{Gen-Z} & black & 0.3259** & 0.2353 & 1.3847** \\  & eastasian & 0.2395 & 0.2394 & 1.0004 \\  & latinx & 0.2619 & 0.2333 & 1.1224* \\  & southasian & 0.2591 & 0.2442 & 1.0611 \\  & white & 0.3028* & 0.2548 & 1.1884* \\ \hline \hline \end{tabular}
\end{table}
Table 8: Results for in-group and cross-group cohesion, and Group Association Index for each intersectional demographic grouping based on age group and ethnicity. Significance at \(p<0.05\) is indicated by *, and significance at \(p<0.05\) after correcting for multiple testing is indicated by **.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline  & **East Asian** & **Black** & **White** & **All** \\ \hline **”I”** & 0.8\% & 0.5\% & 0.8\% & 1.2\% \\ \hline
**”People”/”Some people”** & 0.7\%/0.2\% & 0.7\%/0.1\% & 1.1\%/0.4\% & 0.8\%/0.2\% \\ \hline \hline \end{tabular}
\end{table}
Table 9: The proportion of references to the self vs. others across all tokens/bigrams across ethnicity groups for women.

aligned with expert raters in flagging bias. Looking at the curves for sexual violations, we see that Latinx raters have the highest rates of disagreement at almost all the thresholds with the lowest rates of occurrence, suggesting that they are the least aligned with expert raters and/or may be the least sensitive to sexual violations. Lastly, on the violent prompt-image pairs, Black raters have the highest rates of disagreement at the higher thresholds (3, 4) and also the highest rates of occurrence of scores \(\geq 3\). This suggests that they tend to find several prompt-image pairs very violent that raters from other groups or expert raters don't.

Next, we look at groups of diverse raters vs. expert raters by age and gender on the different violation types. Figure 6 presents rate of disagreement and rate of occurrence curves for some of the interesting scenarios. On bias, we see that rates of occurrence are similar across the age groups but _Gen-Z_ and _Millennial_ raters have significantly higher rates of disagreement than _Gen-X_ raters. This indicates that _Gen-Z_ and _Millennial_ raters find instances of bias that _Gen-X_ raters or expert raters don't. On

Figure 4: Histograms for the frequency of the expert label and the plurality score per prompt-image pair. The average frequency of the expert label is 4.09, i.e., more than 4 experts gave the same annotation per prompt-image pair on average. The average frequency of the plurality score is 15.28, i.e., on average more than 15 raters gave the same score per prompt-image pair.

Figure 5: Figure shows the rate of disagreement and rate of occurrence at different thresholds for ethnic groups of diverse raters vs. expert raters on the three violation types (a) bias, (b) sexual, and (c) violent.

violent prompt-image pairs, _Gen-Z_ raters have the highest rates of disagreement at all the thresholds with the lowest rates of occurrence, suggesting that they are the least aligned with expert raters on the perception of violence. Lastly, we note that _Women_ raters have higher rates of disagreement at all the thresholds with higher rates of occurrence of scores \(\geq 2\), meaning that they find several prompt-image pairs violent that _Men_ raters or expert raters don't.

Prompt-image pairs where experts agree unanimously.On pairs where experts unanimously agree, we wish to understand whether the distribution of scores from some demographic groups is more similar to experts than others. Figure 7 shows the distributions of two different groups for prompt-image pairs where experts unanimously say safe or unsafe. We see that there is a large variance in the ratings of different demographic groups on such pairs as illustrated in Figure 7. We see that in instances where all experts agree on unsafe, Black-Gen-Z-Woman raters have a distribution of ratings that roughly align with them, however East-Asian-Gen-Z-Man raters provide lower scores. Importantly, when all experts agree that pairs are safe, White-Gen-Z-Man raters' annotation aligns with them, and Southasian-Gen-X-Woman raters provide higher scores on many pairs.

Figure 6: Figure shows the rate of disagreement and rate of occurrence at different thresholds for groups of diverse raters vs. expert raters by age and gender on two of the three violation types.

Figure 7: Figure shows difference in rater distributions from different intersectional groups on samples where experts unanimously give either a safe or unsafe rating. We see a larger distribution from individual groups for unanimously unsafe ratings (left) and unanimously safe ratings (right) whereas experts would all agree on whether it was safe or unsafe. PS refers to the mean of plurality scores from that group.