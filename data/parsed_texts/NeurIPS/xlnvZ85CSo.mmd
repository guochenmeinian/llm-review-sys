## 1 Introduction

Cognitive neuroscience research has consistently demonstrated that learning to solve mathematical problems enhances general reasoning abilities in humans, as engaging in mathematical problem-solving promotes logical thinking, abstract reasoning, and transferable problem-solving strategies across various domains (Dehaene et al., 2004; Hawes and Ansari, 2020). This notion - that learning math fosters the development of general reasoning skills - points toward a "_math for AI_" vision, where incorporating mathematical reasoning data into AI training could help large language models (LLMs) develop more complex and versatile reasoning abilities. The "math for AI" goal is particularly relevant to recent attentions to complex reasoning abilities of LLMs (OpenAI, 2024), as mathematical problem-solving (MPS) is one of the few domains where large volumes of long and intricate CoT data can be generated or synthesized (Tang et al., 2024; Lu et al., 2024), making it a valuable data source to potentially learn complex reasoning. However, while numerous models have been developed to tackle mathematical problem-solving (Cobbe et al., 2021; Yu et al., 2023; Luo et al., 2023), their evaluations focus narrowly on benchmarks like GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), and it is unclear whether these approaches and the accompanied datasets can really help learn other types of reasoning. Therefore, these works, whether intentional or not, fall within the "AI for math" scope and fail to demonstrate their impact for the "math for AI" objective. Thus, a key question remains: _Does learning mathematical problem-solving contribute to the development of a model's general reasoning abilities, or does it merely enhance performance on MPS benchmarks?_

In this study, we conduct empirical analysis focusing on this central question. Specifically, we explore whether training LLMs on mathematical problem-solving tasks can help broader reasoning tasks beyond mathematics. We first identify three common training strategies to enhance LLMs' capabilities in solving mathematical problems: (1) _Continual pretraining on mathematical text_ involves extending the pretraining of LLMs on large-scale mathematical text to enhance their adaptability to the mathematical domain, such as RHO-Math (Lin et al., 2024) and Deepseek-Math (Shao et al., 2024). (2) _Instruction pretraining on diverse QA pairs_ is a method focused on training models using diverse question-answer pairs from raw texts, typically encompassing various formats and types of math problems (Yue et al., 2024; Cheng et al., 2024). (3) _Instruction tuning on MPS datasets_ involves fine-tuning models on MPS datasets. This is the most common method adopted to learn mathematical problem-solving and lead to state-of-the-art performance (Yu et al., 2023; Gou et al., 2023; LI et al., 2024; Tong et al., 2024).

We perform control experiments and evaluate a series of model created by the three training strategies above, where the models are either from open-source checkpoints or our own training. We assess these models across multiple benchmarks involving MPS benchmarks and six types of non-MPS reasoning: mathematical reasoning (excluding problem-solving), STEM reasoning, logical reasoning, commonsense reasoning, symbolic reasoning, and agent reasoning. When trained exclusively on mathematical texts, we observed that models tend to lose their ability to follow general instructions and become limited to performing only math-related tasks. To mitigate this effect, we also incorporated general chat-based data into the training process. This approach simulates a realistic development scenario where math-related training is integrated as part of broader model training, rather than isolating it to create a model solely capable of MPS tasks.

Our experimental results reveal that continual pretraining on raw mathematical texts enhances performance across a broader range of reasoning tasks. However, as we transition from continual pretraining to instruction pretraining and instruction tuning, the diversity of data drops, leading to decreased improvements. Particularly, MPS-oriented training negatively impacts performance on non-mathematical tasks. These findings also suggest that most open-source datasets in the math domain, which specifically target mathematical problem-solving, are unable to facilitate broader types of reasoning tasks to fulfill the "math for AI" goal. We encourage future research to reconsider the objectives when studying mathematical reasoning. If the goal is to enhance general reasoning capabilities rather than "AI for math", it may be worthwhile to explore which data sources, whether math-related or otherwise, can effectively contribute to the acquisition of more diverse reasoning skills.

In the final part of this work, we perform a pilot study, trying to identify potential data sources that could enhance reasoning skills. To this end, we experiment with three popular non-MPS SFT datasets that cover various thought reasoning processes, including coding-related tasks, a broad array of reasoning-intensive tasks and state-of-the-art conversational datasets. Unfortunately, none of these datasets demonstrated significant improvements across a wide spectrum of reasoning tasks. This points to a pessimistic conclusion that, in comparison to the extensive data used in pretraining, the relatively modest volume of SFT data is insufficient to substantially improve the model's general reasoning capabilities, even when the data originates from diverse domains.

## 2 Methods

### Training Paradigms for Mathematical Problem-Solving

The improvement of mathematical problem-solving abilities in LLMs has been explored through various training approaches, each with its own strengths and focus. Starting from a pretrained base model, in this study, we explore three prominent training strategies as followed. Due to the expensive cost of running some of the training paradigms, we obtain the required model from either the open-source checkpoints or our own training as we also detail next.

Continual Pretraining on Mathematical Text.In mathematics, where texts often involve multi-step reasoning and formal expressions, this approach helps models better grasp the reasoning pat terns (Lewkowycz et al., 2022). Due to the expensive cost of running continual pretraining, in this study, we experiment with two open-weight LLMs continually pretrained on mathematical-related text: Rho-Math (Lin et al., 2024) and DeepSeekMath (Shao et al., 2024). DeepSeekMath-Base is continual pretrained based on the DeepSeek-Coder-Base model using a large mathematical corpus called DeepSeekMath Corpus. It achieves 64.2% on GSM8K and 36.2% on the competition-level MATH dataset. Rho-Math-7B is continual pretraining with Selective Language Modeling method through OpenWebMath corpus on Mistral-7B, achieving 66.9% on GSM8K and 31.0% on MATH dataset. Distinct from normal continual pretraining, Rho-Math utilizes another reference model to select tokens and only optimize losses on the selected tokens. However, the reference model is created by training on task-specific SFT datasets. While Rho-Math demonstrated superior performance on mathematical problem-solving, in SS3.3 we will show that this training scheme may potentially overfit on benchmark tasks as well, and fail to achieve significant gains on non-MPS tasks.

Instruction Pretraining on Diverse QA Pairs.Instruction pretraining using diverse question-answer (QA) pairs improves a model's generalization across diverse tasks while enhancing its instruction-following capabilities (Yue et al., 2024; Chung et al., 2024; Cheng et al., 2024). This approach involves with large QA datasets, often synthesized from raw text, encompassing various formats, complexities, and problem types. Typically, powerful LLMs like GPT-4 are used to filter raw text and generate relevant QA pairs. In our study, we leverage the open-weight MammoTH2 model (Yue et al., 2024) to evaluate it on broader tasks. MammoTH2 was trained on approximately 10 million QA pairs synthesized through open-source LLMs from a wide range of mathematical, science and engineering texts.

Instruction Tuning on MPS Datasets.Unlike continual pretraining or instruction pretraining on diverse QA pairs, this approach focuses on smaller, domain-specific datasets typically aligned with benchmark tasks. This is the most commonly used approach to boost MPS scores due to its efficiency. To assess whether models finetuned on MPS datasets can generalize beyond their source tasks, we use two different MPS-oriented datasets to train two models on our own : Math-COT SFT and Math-POT SFT. Math-COT SFT was trained on the MetaMath dataset (Yu et al., 2023), which draws primarily from the GSM8K and MATH benchmarks, all structured in a chain-of-thought (CoT) format. Math-POT SFT, on the other hand, was trained on the NuminaMath-TIR dataset (LI et al., 2024), which includes problems from GSM8K and MATH, as well as other benchmarks, with tasks presented in natural language and solutions in code snippets. The NuminaMath-TIR dataset directly leads to the NuminaMath model that wins a recent AI for Math competition.1

Footnote 1: https://www.kaggle.com/competitions/ai-mathematical-olymipad-prize/leaderboard

### Hybrid Training

The training strategies described in SS2.1, if exclusively used, could lead to the development of models specialized solely in mathematical reasoning tasks. However, this work focuses on studying "math for AI", the impact of math-related training and data on general model development. And it is a common practice to mix different sources of datasets to perform training (Xu et al., 2023;

Figure 1: Three ways to incorporate math-related data into original training pipeline through hybrid training process. Original training pipeline is to SFT models with general convention data. For the instruction tuning on MPS datasets, we conducted both two-stage training and mix-data training, for continual pretraining on mathematical text and instruction pretraining on diverse QA pairs, we only conducted the two-stage training.

Meta, 2024). Given this context, it is crucial for developers to understand: how would incorporating additional math-related training impact the original general training performance? To investigate this, we design our experiments to mimic the realistic setting, focusing on a simple yet prevalent training pipeline: a pretrained base model followed by the original SFT training (e.g., on general conversational data). We then conduct controlled experiments to introduce additional math-related data into this training pipeline, aiming to evaluate its influence on the model's performance across various tasks. we explore two different ways of integrating math-related training: two-stage training and mix-data training, as we detail below. The process is illustrated in Figure 1.

Two-stage TrainingSince continual pretraining and instruction pretraining typically serve as an intermediate stage to obtain an enhanced base model followed by SFT training (Shao et al., 2024; Yue et al., 2024), we examine a two-stage training approach that injects math-related data in a mid-training stage. Specifically, in the first stage, one of the three methods outlined in SS2.1 is applied, designed to strengthen the model's foundational mathematical reasoning abilities. In the second stage, we fine-tune these first-stage models using general conversation data to broaden their applicability to a variety of reasoning tasks, we choose UltraChat (Ding et al., 2023) as the general SFT dataset in this work, which is commonly used to create chat models (Tunstall et al., 2023). This process helps the models adapt to instruction-following tasks, thereby improving their versatility across different domains.

Mix-data TrainingConsidering that the two-stage training method may weaken a model's generalization ability due to catastrophic forgetting, we explore another commonly adopted training strategy for incorporating additional SFT datasets, which mixes various SFT data sources together. We only experiment this method for instruction tuning on MPS datasets, since the other two are designed to be conducted in a separate, intermediate training stage. In this mix-data training approach, the training data is a mixture of either Math-COT SFT or Math-POT SFT data combined with UltraChat data. Unlike two-stage training, where the model undergoes independent two sequential fine-tuning stages, the mix-data approach consolidates the training process into a single stage.

## 3 Experiments

We consider seven particular models from three training strategies which aimed at enhancing the math reasoning capabilities. And we assess the generalization capabilities across multiple types of reasoning benchmarks of these models, encompassing both MPS and non-MPS tasks.

### Training Setup

Two-stage training setupWe compare several models across the three studied training strategies to evaluate their performance on reasoning tasks. The models used in the first stage of training come from approaches in SS2.1, which are outlined as follows:(1) For continual pretraining on mathematical text, we leveraged two existing checkpoints: deepseek-math-7b-base and rho-math-7b-v0.1. Their corresponding base models, are Deepsek-Coder-Base and Mistral-7B, respectively. (2) For instruction pretraining on diverse QA pairs, we used the checkpoint MAMOTH2-7B, and Mistral-7B serves as its base model. (3) For instruction tuning on MPS datasets, we fine-tuned the base model mistral-7b-v0.1 ourselves using the MetaMath (Yu et al., 2023) and NuminaMath-TIR (LI et al., 2024) datasets to get the Math-COT SFT model and the Math-POT SFT model. These models serve as the first-stage models for further tuning. After obtaining these first-stage models from each of three approaches, we performed a second-stage fine-tuning on both the math-specialized models and their corresponding base models. In this stage, we fine-tuned the models using the filtered UltraChat (Ding et al., 2023) data, which consists of general conversational content with approximately 200K samples.

Mix-data training setupAdditionally, we conducted mix-data training through these SFT datasets. The UltraChat data was combined with either MetaMath or NuminaMath-TIR data, randomly shuffled and mixed together. Then we fine-tuned the checkpoint mistral-7b-v0.1 on these two mixture data. All the training methods that we study are summarized in Table 1.

We use the sanitized version of Ultracht provided by HuggingFace2. To balance the exposure of the math and general conversation data, we randomly selected 200K data samples from MetaMath for SFT. For NumniaMath-TIR only has 72K items, so we keep all the samples for SFT. More training hyperparameters are showed in Appendix C.1.

Footnote 2: https://huggingface.co/datasets/HuggingFaceH4/ultracht_200k

### Evaluation Datasets

To evaluate models' multi-dimensional reasoning capabilities, we choose seven reasoning tasks: math reasoning (problem-solving) (MPS), math reasoning (excluding problem-solving), logical reasoning, STEM reasoning, commonsense reasoning, symbolic reasoning and agent reasoning. The corresponding benchmarks are shown in Table 2. The GSM8K MQA dataset is derived from the original GSM8K format, repurposed into a multiple-choice question format. The MMLU-math and MMLU-stem are the math and stem sub-categories of MMLU (Hendrycks et al., 2021). The MR-BEN-math is only the math subject of MR-BEN (Zeng et al., 2024). See more introduction of benchmarks in Appendix C.3

\begin{table}
\begin{tabular}{l|l} \hline \hline \multicolumn{2}{c}{**Model Training Process**} \\ \hline \multicolumn{2}{c}{**Two-stage Training Process**} \\ \hline DeepSeek-Coder-Base \(\rightarrow\) DeepSeekMath Corpus\(\rightarrow\) DeepSeekMath-Base \(\rightarrow\) UltraChat\(\rightarrow\) DeepSeekMath (2-stage) \\ \hline \multirow{25}{*}{Mistral-7B-Base \(\rightarrow\)} & OpenWebMath Corpus\(\rightarrow\) Rho-Math-7B \(\rightarrow\) UltraChat\(\rightarrow\) Rho-Math-7B (2-stage) \\ \cline{2-2}  & Mistral-7B-Base \(\rightarrow\) & WebInstruct\(\rightarrow\) MAMmoreTHz-7B \(\rightarrow\) UltraChat\(\rightarrow\) MAMmoreTHz-7B (2-stage) \\ \cline{2-2}  & Mistral-7B-Base \(\rightarrow\) & MetaMath\(\rightarrow\) Math-COFT ST \(\rightarrow\) UltraChat\(\rightarrow\) Math-COFT (2-stage) \\ \cline{2-2}  & Mistral-7B-Base \(\rightarrow\) & NuminaMath-TIR \(\rightarrow\) Math-POT SFT \(\rightarrow\) UltraChat\(\rightarrow\) Math-POT SFT (2-stage) \\ \hline \multicolumn{2}{c}{**Mix-data Training Process**} \\ \hline \multirow{25}{*}{Mistral-7B-Base \(\rightarrow\)} & MetaMath + UltraChat\(\rightarrow\) Math-COFT (mixed) \\ \cline{2-2}  & Mistral-7B-Base \(\rightarrow\) & NuminaMath-TIR + UltraChat\(\rightarrow\) Math-POT SFT (mixed) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Models trained through two-stage training and mix-data training process. The baseline of DeepSeekMath (2-stage) is DeepSeek-Coder (2-stage), which is Deepseek-Coder-Base after Ultra-Chat tuning, while other final modelsâ€™ baseline is Mistral-7B (2-stage), which is Mistral-7B after UltraChat tuning.

\begin{table}
\begin{tabular}{l l} \hline \hline \multicolumn{2}{c}{**Reasoning Domain**} & \multicolumn{1}{c}{**Benchmarks**} \\ \hline Math Reasoning (problem-solving) & GSM8K, GSM8K MQA, MATH, MMLU-math \\ \hline \multicolumn{2}{c}{**Math Reasoning (excluding problem-solving)**} & MR-BEN-math, DocMath (Zhao et al., 2024) \\ \hline Logical Reasoning & Zebra1Logic (Bill Yuchen Lin, 2024), ProofWriter (Tafjord et al., 2020), LogiQA (Liu et al., 2020) \\ \hline STEM Reasoning & GPQA (Rein et al., 2023), MMLU-stem \\ \hline Commonsense Reasoning & NO (Lee et al., 2019), SWAG (Zellers et al., 2018), WinoGrande (Sakaguchi et al., 2021), ARC-challenge (Clark et al., 2018) \\ \hline Symbolic Reasoning & BBH (Suzgun et al., 2022) \\ \hline Agent Reasoning & MiniWoB++ (Liu et al., 2018) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Benchmarks in Each Reasoning Domain.

[MISSING_PAGE_FAIL:6]

that learning mathematical problem-solving is able to generalize and help other types of mathematical reasoning as well.

Continual pretraining generally improves non-mathematical reasoning while selective continual pretraining falls shortThe improvements on mathematical reasoning tasks are actually expected, yet we note that this work emphasizes more the effect on other non-mathematical reasoning tasks. We first observe that continual pretraining of DeepSeeKMath enhances performance in 3 out of 5 non-mathematical tasks, achieving a notable increase of 4.6 points in STEM reasoning and 3.8 points in symbolic reasoning. DeepSeeKMath is also the only one among these models that can achieve an average of over 2-point gain on some non-mathematical reasoning domains. Conversely, Rho-Math, another variant of continual pretraining, only showed improvements in 2 out of 5 non-mathematical reasoning domains with limited gains under 2 points. In more detail, as shown in Figure 2, the Rho-Math perform worse than DeepSeeKMath on more datasets. As introduced in SS2.1, Rho-Math employs a selective language modeling loss that leverages a reference model to help select tokens for optimization - this reference model, trained on task-specific SFT datasets, may introduce biases that compromise the generalization capacity. Previously, the extent of this compromise was unknown as only mathematical problem-solving tasks were assessed. Therefore, we urge the research community to conduct to more comprehensive evaluations of a model's reasoning capabilities, to gain a more complete understanding of different training algorithms. Otherwise, in the case of Rho-Math, although it achieves similar gains on MPS benchmarks as DeepSeeKMath while being trained on far fewer tokens, the trade-offs compared to standard continual pretraining were not initially clear, as we now demonstrate.

Instruction pretraining sometimes help non-mathematical reasoning, while instruction tuning generally impairsWe observe that instruction pretraining with the MAMmoTH2 model improves 3 out of 5 non-mathematical reasoning tasks, despite small gains around 1 point. However, instruction tuning on MPS datasets, the most commonly adopted method to learn mathematical problem solving, undermines the original training pipeline on most non-mathematical reasoning tasks, except for the agent reasoning task. This points to a pessimistic reality: most previous efforts that develop new MPS datasets and advance state-of-the-art for mathematical reasoning may not generalize to facilitate learning in other types of reasoning. In fact, the created data resources may even negatively impact other reasoning abilities, a phenomenon that contradicts intuitive expectations based on human learning studies.

Agent task specific tuningAs the models exhibit significant variation in performance on the agent reasoning task, which is likely due to the fixed-format code required as input for agent tasks. The performance comparison becomes highly dependent on the models' ability to generate accurate code. To reduce this disparity, we replaced the second-stage UltraChat data with task-specific data related to the benchmark. Specifically, we used data from MiniWob++, generated by Claude-2, as the second-stage training data. The results of this adjustment are shown in Figure 3. We observe that Rho-Math, MAMmoTH2 and DeepSeeKMath all demonstrate improvement over the base model, while Math-COT SFT and Math-POT SFT continue to underperform, reinforcing the notion that models trained via SFT have limited generalization capabilities.

## 4 What Other Data Sources Contribute to Reasoning - a Pilot Study

So far, we have explored the effect of various math-related data sources on general reasoning learning, and we have concluded that only continual training with raw math text has a significantly positive effect on general reasoning learning. However, continual pretraining is typically large-scale and computationally expensive. In this section, we perform a pilot study to search for efficient SFT datasets from non-mathematical tasks, to examine whether they can help learn reasoning. Specifically, we identify the following three non-MPS SFT datasets as our targets to study, based on their diverse task coverage as showed in Table 4:

Figure 3: Performance on MiniWob++ for models tuning on task specific data.

[MISSING_PAGE_FAIL:8]

broader range of tasks. This is likely due to its more balanced dataset, which covers both code algorithms and reasoning tasks. Interestingly, the OpenOrca SFT model, despite its broader coverage of reasoning, coding, and general knowledge, shows relatively fewer performance gains compared to Magpie. This could be due to the complexity and diversity of the OpenOrca dataset, which might introduce competing learning objectives, causing the model to struggle in balancing between different types of tasks. While there are some localized improvements in certain domains, such as agent reasoning, where the models exhibit noticeable gains, the overall trend indicates that SFT method, even with diverse and extensive datasets, struggles to generalize effectively across a wide range of reasoning challenges. How to find efficient datasets to enhance general reasoning abilities of LLMs still remain as a critical challenge for future researches to study.

## 5 Related Works

While LLMs exhibit remarkable performance out of the box, especially in tasks that require pattern recognition and language understanding (Zhao et al., 2022; Brown, 2020; Wei et al., 2022; Creswell et al., 2022), their ability to perform complex reasoning often requires additional refinement through targeted training methods.

Supervised Fine-TuningA key method for enhancing LLM performance is Supervised Fine-Tuning (SFT). SFT not only improves a model's ability to follow instructions but also enhances its performance on intricate tasks requiring specialized knowledge by training on well-curated datasets (Xu et al., 2023; Zhou et al., 2023; Wu et al., 2023; Yuan et al., 2023b; Chen et al., 2023b). As LLMs continue to evolve, researchers also employ SFT as a crucial step in tailoring the models for more complex reasoning or tasks (Huang and Chang, 2022; Wang et al., 2023b). In the context of mathematical reasoning, SFT has demonstrated substantial improvements in model performance (Cobbe et al., 2021b; Nye et al., 2021; Yuan et al., 2023a; Yue et al., 2023; Wang et al., 2023a; Li et al., 2023; Liu et al., 2023; Chen et al., 2024). For instance, the MetaMath model, fine-tuned on an augmented GSM8K and MATH dataset, demonstrated notable improvements on mathematical problem-solving benchmarks (Yu et al., 2023). In addition to mathematical reasoning, SFT has also been utilized to achieve better results on other types of reasoning tasks. It has been applied to domains like commonsense reasoning (Huang et al., 2022; Bian et al., 2024) and logical reasoning (Luo et al., 2023b; Chen et al., 2023c; Li et al., 2024), Moreover, researchers also reveal that SFT also helps LLMs handle more dynamic and context-rich tasks like agent-based reasoning (Gou et al., 2023; Chen et al., 2023a), where understanding interactions and goals in simulated environments is essential.

Continual PretrainContinual pretraining is another widely adopted approach to enhance the performance of LLMs in specific domains (Aharoni and Goldberg, 2020). Unlike SFT, which relies on task-specific datasets, continual pretraining exposes models to large-scale, domain-relevant corpora Paster et al. (2023); Wang et al. (2023c). The large-scale corpora expands the model's knowledge base and helps the model generalize better within specialized areas (Jin et al., 2021; Gupta et al., 2023; Ke et al., 2023; Wu et al., 2023a; Bian et al., 2024). In the realm of mathematical problem solving, continual pretraining also has been instrumental in improving models' abilities to tackle complex reasoning tasks (Lewkowycz et al., 2022; Lin et al., 2024; Shao et al., 2024).

## 6 Conclusion

In this paper, we explored the generalization potential of three different training strategies to learn mathematical problem-solving. Our experiments evaluated models trained using (1) continual pretraining on mathematical text, (2) instruction tuning on diverse QA pairs, and (3) instruction tuning on MPS datasets. The results indicate that only continual pretraining on raw mathematical text can lead to significant gains on most domains. In contrast, models fine-tuned on MPS SFT datasets struggled to generalize beyond math-specific tasks and even impaired other reasoning abilities. These observations imply that previous researches on mathematical reasoning may put too much focus on mathematical problem-solving task, which stay far away from the "math for AI" goal. Future research could explore how both math-related or non-math datasets can be leveraged to better develop models capable of handling a wider variety of reasoning tasks.

## References

* Aharoni and Goldberg [2020] Roee Aharoni and Yoav Goldberg. Unsupervised domain clusters in pretrained language models, 2020.
* Bian et al. [2024] Ning Bian, Xianpei Han, Hongyu Lin, Yaojie Lu, Ben He, and Le Sun. Rule or story, which is a better commonsense expression for talking with large language models? _arXiv preprint arXiv:2402.14355_, 2024.
* Lin et al. [2024] Yejin Choi Bill Yuchen Lin, Ronan Le Bras. Zebralogic: Benchmarking the logical reasoning ability of language models. https://huggingface.co/spaces/allenai/zebraLogic, 2024.
* Brown [2020] Tom B Brown. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
* Chen et al. [2023a] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. _arXiv preprint arXiv:2310.05915_, 2023a.
* Chen et al. [2023b] Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning. _arXiv preprint arXiv:2305.09246_, 2023b.
* Chen et al. [2023c] Meiqi Chen, Yubo Ma, Kaitao Song, Yixin Cao, Yan Zhang, and Dongsheng Li. Learning to teach large language models logical reasoning. _arXiv preprint arXiv:2310.09158_, 2023c.
* Chen et al. [2024] Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. Autoprm: Automating procedural supervision for multi-step reasoning via controllable question decomposition. _arXiv preprint arXiv:2402.11452_, 2024.
* Cheng et al. [2024] Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. Instruction pre-training: Language models are supervised multitask learners. _arXiv preprint arXiv:2406.14491_, 2024.
* Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _Journal of Machine Learning Research_, 25(70):1-53, 2024.
* Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv:1803.05457v1_, 2018.
* Cobbe et al. [2021a] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021a.
* Cobbe et al. [2021b] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021b.
* Creswell et al. [2022] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. _arXiv preprint arXiv:2205.09712_, 2022.
* Dehaene et al. [2004] Stanislas Dehaene, Nicolas Molko, Laurent Cohen, and Anna J Wilson. Arithmetic and the brain. _Current opinion in neurobiology_, 14(2):218-224, 2004.
* Ding et al. [2023] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. _arXiv preprint arXiv:2305.14233_, 2023.
* Gao et al. [2024] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 07 2024.

* Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. _arXiv preprint arXiv:2309.17452_, 2023.
* Gupta et al. (2023) Kshitij Gupta, Benjamin Therien, Adam Ibrahim, Mats L Richter, Quentin Anthony, Eugene Belilovsky, Irina Rish, and Timothee Lesort. Continual pre-training of large language models: How to (re) warm your model? _arXiv preprint arXiv:2308.04014_, 2023.
* Hawes & Ansari (2020) Zachary Hawes and Daniel Ansari. What explains the relationship between spatial and mathematical skills? a review of evidence from brain and behavior. _Psychonomic bulletin & review_, 27:465-482, 2020.
* Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2021a.
* Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _NeurIPS_, 2021b.
* Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. _arXiv preprint arXiv:2210.11610_, 2022.
* Huang & Chang (2022) Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey. _arXiv preprint arXiv:2212.10403_, 2022.
* Jin et al. (2021) Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. Lifelong pretraining: Continually adapting language models to emerging corpora. _arXiv preprint arXiv:2110.08534_, 2021.
* Ke et al. (2023) Zixuan Ke, Yijia Shao, Haowei Lin, Tatsuya Konishi, Gyuhak Kim, and Bing Liu. Continual pre-training of language models, 2023.
* Lee et al. (2019) Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Anna Korhonen, David Traum, and Lluis Marquez (eds.), _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, 2019.
* Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), _Advances in Neural Information Processing Systems_, 2022.
* Li et al. (2023) Chengpeng Li, Zheng Yuan, Guanting Dong, Keming Lu, Jiancan Wu, Chuanqi Tan, Xiang Wang, and Chang Zhou. Query and response augmentation cannot help out-of-domain math reasoning generalization. _arXiv preprint arXiv:2310.05506_, 2023.
* LI et al. (2024) Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath, 2024.
* Li et al. (2024) Yanda Li, Dixuan Wang, Jiaqing Liang, Guochao Jiang, Qianyu He, Yanghua Xiao, and Deqing Yang. Reason from fallacy: Enhancing large language models' logical reasoning through logical fallacy understanding. _arXiv preprint arXiv:2404.04293_, 2024.
* Lin et al. (2024) Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, and Weizhu Chen. Rho-1: Not all tokens are what you need, 2024.
* Liu et al. (2018) Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In _International Conference on Learning Representations (ICLR)_, 2018.

* Liu et al. [2020] Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. _arXiv preprint arXiv:2007.08124_, 2020.
* Liu et al. [2023] Yixin Liu, Avi Singh, C Daniel Freeman, John D Co-Reyes, and Peter J Liu. Improving large language model fine-tuning for solving math problems. _arXiv preprint arXiv:2310.10047_, 2023.
* Longpre et al. [2023] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning. In _International Conference on Machine Learning_, pp. 22631-22648. PMLR, 2023.
* Lu et al. [2024] Zimu Lu, Aojun Zhou, Houxing Ren, Ke Wang, Weikang Shi, Junting Pan, Mingjie Zhan, and Hongsheng Li. Mathgenie: Generating synthetic data with question back-translation for enhancing mathematical reasoning of llms. _arXiv preprint arXiv:2402.16352_, 2024.
* Luo et al. [2023a] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. _arXiv preprint arXiv:2308.09583_, 2023a.
* Luo et al. [2023b] Man Luo, Shrinidhi Kumbhar, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, et al. Towards logiglibe: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. _arXiv preprint arXiv:2310.00836_, 2023b.
* Meta [2024] Meta. The llama 3 herd of models. https://arxiv.org/abs/2407.21783, 2024.
* Mukherjee et al. [2023] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4, 2023.
* Nye et al. [2021] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. _arXiv preprint arXiv:2112.00114_, 2021.
* OpenAI [2024] OpenAI. Learning to reason with llms. https://openai.com/index/learning-to-reason-with-llms/, 2024.
* Paster et al. [2023] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. _arXiv preprint arXiv:2310.06786_, 2023.
* Rein et al. [2023] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: A graduate-level google-proof q&a benchmark, 2023.
* Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* Shao et al. [2024] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.
* Suzgun et al. [2022] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou,, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.
* Tafjord et al. [2020] Oyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. Proofwriter: Generating implications, proofs, and abductive statements over natural language. _arXiv preprint arXiv:2012.13048_, 2020.
* Tang et al. [2024] Zhengyang Tang, Xingxing Zhang, Benyou Wan, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. _arXiv preprint arXiv:2403.02884_, 2024.
* Tong et al. [2024] Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving, 2024. URL https://arxiv.org/abs/2407.13690.

* [648] Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr: Direct distillation of lm alignment, 2023.
* [652] Ke Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li. Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. _arXiv preprint arXiv:2310.03731_, 2023a.
* [656] Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. _arXiv preprint arXiv:2309.02144_, 2023b.
* mathpile: A billion-token-scale pretraining corpus for math, 2023c.
* [661] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* [665] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source code is all you need. _arXiv preprint arXiv:2312.02120_, 2023.
* [667] Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-llama: Towards building open-source language models for medicine, 2023a.
* [670] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. Lamini-lm: A diverse herd of distilled models from large-scale instructions. _arXiv preprint arXiv:2304.14402_, 2023b.
* [671] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_, 2023.
* [672] Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, and Zhiyong Wu. Interactive evolution: A neural-symbolic self-training framework for large language models. _arXiv preprint arXiv:2406.11736_, 2024a.
* [680] Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. Magpie: Alignment data synthesis from scratch by prompting aligned lms with nothing. _ArXiv_, abs/2406.08464, 2024b.
* [681] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. _arXiv preprint arXiv:2407.10671_, 2024.
* [682] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. _arXiv preprint arXiv:2309.12284_, 2023.
* [690] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. _arXiv preprint arXiv:2308.01825_, 2023a.
* [691] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback without tears. _arXiv preprint arXiv:2304.05302_, 2023b.
* [692] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. _arXiv preprint arXiv:2309.05653_, 2023.
* [700] Xiang Yue, Tuney Zheng, Ge Zhang, and Wenhu Chen. Mammoth2: Scaling instructions from the web, 2024.

* [702] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii (eds.), _Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing_, 2018.
* [706] Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, and Jiaya Jia. Mr-ben: A comprehensive meta-reasoning benchmark for large language models. _CoRR_, abs/2406.13975, 2024.
* [711] Ruilin Zhao, Feng Zhao, Guandong Xu, Sixiao Zhang, and Hai Jin. Can language models serve as temporal knowledge bases? In _Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 2022.
* [714] Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. Docmath-eval: Evaluating math reasoning capabilities of llms in understanding long and specialized documents, 2024.
* [717] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* [721] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. _arXiv preprint arXiv:2311.07911_, 2023.

[MISSING_PAGE_EMPTY:15]

## Appendix B More Result Analysis

Figure 4 illustrates the comparative performance between the first-stage models and the final models across multiple reasoning domains. From the radar chart, it is evident that the final models usually exhibit a consistent improvement. Additionally, for models tuned on MPS datasets, the mix-data training process showed slight improvements over the two-stage training method on certain

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & \multicolumn{3}{c}{**Math Reasoning (excluding problem-solving)**} & \multicolumn{3}{c}{**Logical Reasoning**} \\ \cline{2-5}
**Model** & MR-BEN-math & DocMath & ZebraLogic & LogiQA & ProofWriter \\ \hline \hline Mistral-7B (2-stage) & 21.5 & 11.3 & 4.8 & 29.5 & 32.5 \\ DeepSeek-Coder (2-stage) & 35.2 & 15.0 & 4.7 & 25.4 & 34.8 \\ \hline \multicolumn{5}{c}{(**1) Continual pretraining on raw text**} \\ DeepSeekMath (2-stage) & 34.3 \(\pm\)0.9 & 18.5 \(\pm\)1.55 & 5.1 \(\pm\)0.4 & 26.7 \(\pm\)1.3 & 32.2 \(\pm\)1.6 \\ Rho-Math-7B (2-stage) & 26.8 \(\pm\)5.5 & 11.7 \(\pm\)0.4 & 6.1 \(\pm\)1.3 & 27.7 \(\pm\)1.3 & 32.0 \(\pm\)0.5 \\ \hline \multicolumn{5}{c}{(**2) Introduction pretraining on diverse QA pairs**} \\ MAmnoTH2-7B (2-stage) & 23.0 \(\pm\)1.5 & 19.7 \(\pm\)0.4 & 4.8 & 30.9 \(\pm\)1.4 & 35.5 \(\pm\)0.6 \\ \hline \multicolumn{5}{c}{(**3) Instruction tuning on MPS datasets**} \\ Math-COT SFT (2-stage) & 24.3 \(\pm\)2.4 & 11.8 \(\pm\)0.5 & 5.9 \(\pm\)1.1 & 30.1 \(\pm\)0.6 & 32.7 \(\pm\)0.2 \\ Math-POT SFT (2-stage) & 24.3 \(\pm\)2.4 & 11.8 \(\pm\)0.5 & 6.0 \(\pm\)1.2 & 28.6 \(\pm\)0.9 & 32.0 \(\pm\)0.5 \\ \hline \multicolumn{5}{c}{(**4) Continual pretraining on raw text**} \\ DeepSeek-Coder (2-stage) & 13.7 & 52.7 & 64.5 & 40.1 \\ \hline \multicolumn{5}{c}{(**1) Continual pretraining on raw text**} \\ DeepSeekMath (2-stage) & 13.0 \(\pm\)1.5 & 51.6 \(\pm\)1.2 & 63.5 \(\pm\)1.0 & 46.1 \(\pm\)0.5 \\ Rho-Math-7B (2-stage) & 21.0 \(\pm\)1.5 & 55.7 \(\pm\)1.3 & 71.0 \(\pm\)1.3 & 50.0 \(\pm\)0.4 \\ \hline \multicolumn{5}{c}{(**2) Introduction pretraining on diverse QA pairs**} \\ MAmnoTH2-7B (2-stage) & 22.8 \(\pm\)5.6 & 56.4 \(\pm\)2.4 & 70.3 \(\pm\)1.8 & 56.5 \(\pm\)1.4 \\ \hline \multicolumn{5}{c}{(**3) Instruction tuning on MPS datasets**} \\ Math-COT SFT (2-stage) & 29.5 & 59.0 \(\pm\)0.2 & 72.9 \(\pm\)0.8 & 52.7 \(\pm\)1.4 \\ Math-POT SFT (2-stage) & 29.0 \(\pm\)0.5 & 58.9 \(\pm\)0.1 & 73.7 \(\pm\)1.6 & 52.5 \(\pm\)1.6 \\ Math-COT SFT (mixed) & 27.0 \(\pm\)1.25 & 58.9 \(\pm\)0.1 & 71.5 \(\pm\)0.6 & 52.5 \(\pm\)1.6 \\ Math-POT SFT (mixed) & 26.7 \(\pm\)1.25 & 59.0 \(\pm\)0.2 & 70.9 \(\pm\)1.2 & 53.4 \(\pm\)0.7 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Detailed results on math (excluding problem-solving) and logical reasoning benchmarks of two-stage training and mix-data training models. Absolute accuracy changes compared to the baselines are highlighted.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{3}{c}{**An Commonsense Reasoning**} \\ \cline{2-5}
**Model** & NQ & SWAG & WinoGrande & ARC-challenge \\ \hline \hline Mistral-7B (2-stage) & 29.5 & 58.8 & 72.1 & 54.1 \\ DeepSeek-Coder (2-stage) & 13.7 & 52.7 & 64.5 & 40.1 \\ \hline \multicolumn{5}{c}{(**1) Continual pretraining on raw text**} \\ DeepSeekMath (2-stage) & 13.0 \(\pm\)1.5 & 51.6 \(\pm\)1.2 & 63.5 \(\pm\)1.0 & 46.1 \(\pm\)0.5 \\ Rho-Math-7B (2-stage) & 21.0 \(\pm\)1.5 & 55.7 \(\pm\)1.3 & 71.0 \(\pm\)1.3 & 50.0 \(\pm\)0.4 \\ \hline \multicolumn{5}{c}{(**2) Introduction pretraining on diverse QA pairs**} \\ MAmnoTH2-7B (2-stage) & 22.8 \(\pm\)5.6 & 24.2 & 70.3 \(\pm\)1.8 & 56.5 \(\pm\)1.4 \\ \hline \multicolumn{5}{c}{(**3) Instruction tuning on MPS datasets**} \\ MAmnoTH2-7B (2-stage) & 29.5 & 59.0 \(\pm\)0.2 & 72.9 \(\pm\)0.8 & 52.7 \(\pm\)1.4 \\ Math-POT SFT (2-stage) & 29.0 \(\pm\)0.5 & 58.9 \(\pm\)0.1 & 73.7 \(\pm\)1.6 & 52.5 \(\pm\)1.6 \\ Math-COT SFT (mixed) & 27.0 \(\pm\)1.25 & 58.9 \(\pm\)0.1 & 71.5 \(\pm\)0.6 & 52.5 \(\pm\)1.6 \\ Math-POT SFT (mixed) & 26.7 \(\pm\)1.25 & 59.0 \(\pm\)0.2 & 70.9 \(\pm\)1.2 & 53.4 \(\pm\)0.7 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Detailed results on commonsense reasoning benchmarks of two-stage training and mix-data training models. Absolute accuracy changes compared to the baselines are highlighted.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{**Math Reasoning (problem-solving)**} & \multicolumn{3}{c}{**STEM Reasoning**} \\ \cline{2-7}
**Model** & GSMSK & GSMSK MOA & MATH & MMLU-math & GPQA & MMLU-stem \\ \hline Mistral-7B (2-stage) & 40.6 & 56.9 & 12.3 & 45.5 & 30.8 & 48.4 \\ \hline \multicolumn{7}{c}{**Mix-data training on non-MPS datasets**} \\ Magicoder-Evol-Instruct SFT (mixed) & 43.1 \(\pm\)0.5 & 55.5 \(\pm\)1.4 & 10.9 \(\pm\)1.4 & 42.9 \(\pm\)1.5 & 26.1 \(\pm\)1.47 & 47.2 \(\pm\)1.2 \\ Mapie-Reasoning SFT (mixed) & 62.7 \(\pm\)0.1 & 65.6 \(\pm\)0.5 & 15.7 \(\pm\)1.4 & 44.0 \(\pm\)1.06 & 29.2 \(\pm\)1.6 & 48.4 \\ OpenOrca SFT (mixed) & 49.1 \(\pm\)0.5 & 21.8 \(\pm\)0.1 & 11.2 \(\pm\)1.1 & 39.4 \(\pm\)1.4 & 31.5 \(\pm\)0.7 & 48.7 \(\pm\)0.3 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Detailed results on math (problem-solving) and STEM reasoning benchmarks of models with mix-data training process on non-MPS datasets (based on Mistral-7B). Absolute accuracy changes compared to the baselines are highlighted.

[MISSING_PAGE_FAIL:17]

### Brief Introduction of Benchmarks

Here are the brief introduction to each benchmark. For some complex benchmarks, we also present the corresponding prompt for evaluation.

Gsm8k GSM8K (Cobbe et al., 2021) is a dataset specifically designed for evaluating LLMs in the domain of multi-step mathematical reasoning. The problem in this dataset are high quality linguistically diverse grade school math word problems created by human problem writers.

Figure 4: Performance for first stage models and final models after two-stage training or mix-data training. MPS: Math Reasoning (problem-solving). MR: Math Reasoning (excluding problem-solving). LR: Logical Reasoning. SR: STEM Reasoning. CR: Commonsense Reasoning. BR: Symbolic Reasoning. AR: Agent Reasoning.

Figure 5: Density of log probability across various math SFT models on MiniWob++. Math SFT task tuning means the second stage tuning is through task specific data instead of UltraChat.

GSM8K MQAThis is a dataset where we reformatted the original GSM8K dataset into multiple-choice questions. We kept the original question and let GPT-4o generate other three confusing answers based on the original answer. Models need to generate the option letter of the correct answer.

MATH(Hendrycks et al., 2021) test dataset contains 5,000 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations.

MMLU-mathMassive Multitask Language Understanding (MMLU) (Hendrycks et al., 2021) is a benchmark designed to measure knowledge acquired by the LLMs. It covers 57 subjects. For MMLU-math, we choose the _abstract algebra, college mathematics, elementary mathematics, high school mathematics_ subjects. Models need to think step by step and generate the final answer.

MMLU-stemWe retain the original set of STEM in MMLU. Specifically, it include _abstract algebra, anatomy, astronomy, college biology, college chemistry, college computer science, college mathematics, college physics, computer security, conceptual physics, electrical engineering, elementary mathematics, high school biology, high school chemistry, high school computer science, high school mathematics, high school physics, high school statistics, machine learning_. Compare to MMLU-math, we use the probabilities of options to determine the answer instead of generate it, the options with highest probabilities among all the options will be considered as the final answer.

NqNq(Lee et al., 2019) is a benchmark for open-domain question answering derived from Google's Natural Questions dataset. The task is to predict a concise English answer to a question using only the information from English Wikipedia.

SWAG(Zellers et al., 2018) is a large-scale dataset for the task of grounded commonsense inference, unifying natural language inference and physically grounded reasoning. Each question is a video caption, with four answer choices about what might happen next in the scene. The correct answer is the (real) video caption for the next event in the video.

MR-BEN-mathMR-BEN(Zeng et al., 2024) is a comprehensive benchmark demands a meta reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. We choose the math among all subjects for evaluation.

MNRBEN-mmit

Following is a question and solution pair in subject college math. Your task is to examine the solutions step by step and determine the solution correctness. If the solution is incorrect, please further find out the first error step and explain the error reason.

\(<\)few-shot examples\(>\)

Below is the question and solution for you to solve:

Question: \(<\)question\(>\)

Options: \(<\)options\(>\)

Please follow the desired response format:

Solution Analysis: [Give a step by step analysis on the solution correctness here] Solution

Correctness: [Input 'correct'/'incorrect' here to indicate the overall correctness of the solution]

First Error Step: [Input 'Step x' here to indicate the first error step here. Input 'N/A' if the solution is correct.]

Error Reason: [Input the error reason and the rectified reasoning of the first error step here.

Input 'N/A' if the solution is correct.]

Please follow this format without any additional introductory or concluding statements.

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]