# Towards Last-layer Retraining for Group Robustness with Fewer Annotations

 Tyler LaBonte\({}^{1}\)  Vidya Muthukumar\({}^{2,1}\)  Abhishek Kumar\({}^{3}\)

\({}^{1}\)H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology

\({}^{2}\)School of Electrical and Computer Engineering, Georgia Institute of Technology

\({}^{3}\)Google DeepMind

{tlabonte, vmuthukumar8}@gatech.edu  abhishk@google.com

###### Abstract

Empirical risk minimization (ERM) of neural networks is prone to over-reliance on spurious correlations and poor generalization on minority groups. The recent _deep feature reweighting_ (DFR) technique [33] achieves state-of-the-art group robustness via simple last-layer retraining, but it requires held-out group and class annotations to construct a group-balanced reweighting dataset. In this work, we examine this impractical requirement and find that last-layer retraining can be surprisingly effective with no group annotations (other than for model selection) and only a handful of class annotations. We first show that last-layer retraining can greatly improve worst-group accuracy even when the reweighting dataset has only a small proportion of worst-group data. This implies a "free lunch" where holding out a subset of training data to retrain the last layer can substantially outperform ERM on the entire dataset with no additional data or annotations. To further improve group robustness, we introduce a lightweight method called _selective last-layer fine-tuning_ (SELF), which constructs the reweighting dataset using misclassifications or disagreements. Our empirical and theoretical results present the first evidence that model disagreement upsamples worst-group data, enabling SELF to nearly match DFR on four well-established benchmarks across vision and language tasks with no group annotations and less than \(3\%\) of the held-out class annotations. Our code is available at https://github.com/tmlabonte/last-layer-retraining.

## 1 Introduction

Classification tasks in machine learning often suffer from _spurious correlations_: patterns which are predictive of the target class in the training dataset but irrelevant to the true classification function. These spurious correlations, often in conjunction with the target class, create _minority groups_ which are underrepresented in the training dataset. For example, in the task of classifying cows and camels, the training dataset may be biased so that a desert background is spuriously correlated with the camel class, creating a minority group of camels on grass backgrounds [5]. Beyond this simple scenario, spurious correlations have been observed in high-consequence applications including medicine [79], justice [9], and facial recognition [42].

Neural networks trained via the standard procedure of empirical risk minimization (ERM) [68], which minimizes the average training loss, tend to overfit to spurious correlations and generalize poorly on minority groups [18]. Even worse, it is possible for ERM models to rely exclusively on the spurious feature and incur minority group performance that is no better than random guessing [60]. Therefore, maximizing the model's _group robustness_, quantified by its worst accuracy on any group, is a desirable objective in the presence of spurious correlations [58].

In contrast to more generic distribution shift settings (_e.g.,_ domain generalization [35]), the presence of spurious correlations enables the improvement of group robustness merely by addressing model bias (without collecting additional minority group data). The recently proposed _deep feature reweighting_ (DFR) [33, 28] technique efficiently corrects model bias by retraining the last layer of the neural network, a simple procedure which achieves state-of-the-art group robustness. The key hypothesis underlying DFR is that ERM models which overfit to spurious correlations still learn _core features_ that correlate with the ground-truth label on all groups, but they perform poorly because they overweight the spurious features in the last layer. Ostensibly, retraining the last layer on a group-balanced _reweighting dataset_ would then upweight the core features and improve worst-group accuracy.

DFR compares favorably to existing methods such as _group distributionally robust optimization_ (DRO) [58], which requires group annotations for the entire training dataset. However, DFR still necessitates a smaller reweighting dataset with group and class annotations to achieve maximal performance [33]. This requirement limits its practical application, as the groups are often unknown ahead of time or difficult to annotate (_e.g.,_ due to financial, privacy, or fairness concerns).

### Our contributions

In this paper, we present a comprehensive examination of the performance of last-layer retraining in the absence of group and class annotations on four well-established benchmarks for group robustness across vision and language tasks.1 We first investigate the necessity of the reweighting dataset being balanced across groups, and we show that last-layer retraining can substantially improve worst-group accuracy _even when the reweighting dataset has only a small proportion of worst-group data_. Based on this observation, we propose _class-balanced last-layer retraining_ as a simple but strong baseline for group robustness without group annotations. We show that, on average over the four datasets, this method achieves \(94\%\) of DFR worst-group accuracy compared to \(76\%\) without class balancing.

Footnote 1: Our methods in Section 4 do not require group annotations at all, while our SELF method in Section 5 assumes access to a small validation set with group annotations for model selection following previous work [58, 41, 48, 33, 28]. We show in Appendix B that SELF is robust even with 1% of these annotations.

The strong performance of class-balanced last-layer retraining reveals a "free lunch" with practical ramifications. While class-balanced ERM was recently proposed by [27] as a competitive baseline for worst-group accuracy, we show that instead of using the entire training dataset for ERM, dependence on spurious correlations can be reduced by randomly splitting the training data in two, then performing ERM training on the first split and class-balanced last-layer retraining on the second. Our experiments indicate that this technique can improve worst-group accuracy by up to \(17\%\) over class-balanced ERM on the original dataset using _no additional data or annotations (even for model selection)_ - a surprising and unexplained result given that the two splits have equally drastic group imbalance.

While retraining the last layer on a class-balanced held-out dataset can be effective, it is inferior to DFR when group imbalance is large. To close this gap, we propose _selective last-layer finetuning_ (SELF), which selects a small, more group-balanced reweighting dataset and finetunes the last layer instead of retraining. We implement SELF using points that are either _misclassified_ or _disagree_ in their predictions relative to a regularized model. Disagreement SELF does not require class annotations for the entire held-out set (only for the disagreements). We show that _disagreement SELF between the ERM and early-stopped models_ performs the best in general, surpassing class-balanced last-layer retraining by up to \(12\%\) worst-group accuracy with less than \(3\%\) of the held-out class annotations.

Overall, our work shows benefits of last-layer retraining well beyond a group-balanced held-out dataset. Our results call for _further investigation of the DFR hypothesis_: while group balance is the most important factor in DFR performance, we show that a significant gain is solely due to class balancing, and the performance discrepancy between misclassification SELF and disagreement SELF suggests that worst-group accuracy may be affected by characteristics of the reweighting dataset other than group balance. Our main results are summarized and compared to previous methods in Table 1.

## 2 Related work

This work subsumes and improves our two previous workshop papers: [38] broadly covers Section 4, while [37] represents preliminary investigation into Section 5, which we have substantially updated with new methodology, evaluation, and theoretical analysis for this version.

Sparious correlations.The performance of empirical risk minimization (ERM) in the presence of spurious correlations has been extensively studied [18]. In vision, ERM models are widely known to rely on spurious attributes like background [58, 75], texture [17], and secondary objects [56, 61, 62] to perform classification. In language, ERM models often utilize syntactic or statistical heuristics as a substitute for semantic understanding [20, 50, 46]. This behavior can lead to bias against demographic minorities [25, 6, 67, 23, 8] or failure in high-consequence applications [42, 9, 79, 51].

Robustness and group annotations.If group annotations are available in the training dataset, _group distributionally robust optimization_ (DRO) [58] can improve robustness by minimizing the worst-group loss, while other techniques learn invariant or diverse features [1, 19, 80, 76]. Methods which use only partial group annotations include _deep feature reweighting_ (DFR) [33, 28], which retrains the last layer on a group-balanced held-out set, and _spread spurious attribute_[48], which performs DRO with group pseudo-labels. Recently, more lightweight methods that only adjust the model predictions using a group annotated held-out set have also been proposed [70]. However, since the groups are often unknown ahead of time or difficult to annotate in practice, there has been significant interest in methods which do not utilize group annotations except for model selection. The bulk of these techniques utilize auxiliary models to pseudo-label the minority group or spurious feature [63, 47, 77, 11, 31, 66, 32, 64, 81]; notably, _just train twice_ (JTT) [41] upweights samples misclassified by an early-stopped model. Other techniques reweight or subsample the classes [27] or train with robust losses and regularization [54, 78].

Unlike DFR, our methods utilize no held-out group annotations, and unlike JTT, we do not have to "train twice", only finetune the last layer. We also show that SELF performs best using _disagreements_ between the ERM and early-stopped models instead of _misclassifications_ as in JTT. Moreover, while JTT assumes the early-stopped model has low worst-group accuracy which improves during training, SELF performs well even when the early-stopped model has high worst-group accuracy which decreases during training - substantially improving performance on datasets such as CivilComments. The concurrent work of Qiu _et al._[55] proposed a similar method to our Section 4.2; while they use a tunable loss to upweight misclassified samples, our method shows that similar results can be achieved with no hyperparameter tuning (and therefore no group annotations for the validation set). Finally, while our results partially corroborate the findings of Idrissi _et al._[27] that class balancing during ERM is effective for group robustness, we observe that class-balanced last-layer retraining renders ERM class balancing optional. We compare our results with previous methods in Table 1.

Generalization via disagreement.Disagreement-based active learning for improving in-distribution generalization has been well-studied since before the deep learning era [10, 4, 21]. More recent research has utilized disagreements between SGD runs to predict in-distribution gen

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Annotations} & \multicolumn{4}{c}{Worst-group test accuracy} \\ \cline{2-7}  & Group & Class & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline Class-balanced ERM & ✗ & ✓ & \(81.9_{\pm 3.4}\) & \(67.2_{\pm 5.6}\) & \(61.4_{\pm 0.7}\) & \(69.2_{\pm 1.6}\) \\ JTT [41, 27] & ✗ & ✓ & \(85.6_{\pm 0.2}\) & \(75.6_{\pm 7.7}\) & – & \(67.5_{\pm 1.9}\) \\ RWY-ES [27, 28] & ✗ & ✓ & \(74.5_{\pm 0.0}\) & \(76.8_{\pm 7.7}\) & \(78.9_{\pm 1.0}\) & \(68.0_{\pm 0.4}\) \\ CnC [81] & ✗ & ✓ & \(88.5_{\pm 0.3}\) & \(\mathbf{88.8_{\pm 0.9}}\) & – & – \\ \hline CB last-layer retraining & ✗ & ✓ & \(92.6_{\pm 0.8}\) & \(73.7_{\pm 2.8}\) & \(\mathbf{80.4_{\pm 0.8}}\) & \(64.7_{\pm 1.1}\) \\ ES disagreement SELF & ✗ & ✗ & \(\mathbf{93.0_{\pm 0.3}}\) & \(83.9_{\pm 0.9}\) & \(79.1_{\pm 2.1}\) & \(\mathbf{70.7_{\pm 2.5}}\) \\ \hline DFR (our impl.) & ✓ & ✓ & \(92.4_{\pm 0.9}\) & \(87.0_{\pm 1.1}\) & \(81.8_{\pm 1.6}\) & \(70.8_{\pm 0.8}\) \\ DFR [33, 28] & ✓ & ✓ & \(91.1_{\pm 0.8}\) & \(89.4_{\pm 0.9}\) & \(78.8_{\pm 0.5}\) & \(72.6_{\pm 0.3}\) \\ Group DRO-ES [58, 28] & ✓ & ✓ & \(90.7_{\pm 0.6}\) & \(90.6_{\pm 1.6}\) & \(80.4\) & \(73.5\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison to other group robustness methods.** We discuss our DFR implementation in Section 3, and our proposed methods of class-balanced (CB) last-layer retraining and early-stop (ES) disagreement SELF in Sections 4 and 5, respectively. Class-balanced ERM is trained on the combined training and held-out datasets. DFR uses group annotations on the held-out dataset, while Group DRO-ES requires them for the training dataset. ES disagreement SELF uses class annotations on the training dataset (for ERM), but _requests as few as 20 labels_ from the held-out dataset. All methods except ERM and CB last-layer retraining use a small set of group annotations for model selection. We list the mean and standard deviation over three independent runs.

eralization [30], as well as between model classes (_e.g._, CNNs and Transformers) to predict out-of-distribution generalization [3]. Two works that are concurrent to ours, _diversity by disagreement_[52] and _diversify and disambiguate_[40], are methods for generalization under distribution shift which maximize the disagreement between multiple predictors to learn a diverse ensemble. Compared to our work, these methods are optimized for training datasets which exhibit a _complete correlation_ (_i.e._, contain no minority group data) and they underperform in the less extreme spurious correlation setting we study.

## 3 Preliminaries

**Setting.** We consider classification tasks with input domain \(\mathcal{X}\) and target classes \(\mathcal{Y}\). We assume \(\mathcal{S}\) is a set of _spurious features_ such that each sample \(x\in\mathcal{X}\) is associated with exactly one feature \(s\in\mathcal{S}\). In conjunction with the target class, the spurious features partition the dataset into groups \(\mathcal{G}=\mathcal{Y}\times\mathcal{S}\). While the groups may be heavily imbalanced in the training distribution, we desire a model which is invariant to the spurious feature and thus has roughly uniform performance over \(\mathcal{G}\). Therefore, we evaluate _worst-group accuracy_ (WGA), _i.e._, the minimum accuracy among all groups [58].

We will often refer to datasets and models as _group-balanced_ or _class-balanced_, meaning that in expectation, the dataset is composed of an equal number of samples from groups in \(\mathcal{G}\) or classes in \(\mathcal{Y}\), respectively. This balance can be achieved by training on a subset with equal data from each group/class, or sampling from the data so that each minibatch is balanced in expectation [27]. To make the latter more concrete, for group balancing we first sample \(s\sim\text{Unif}(\mathcal{S})\), then sample \(x\sim\hat{p}(\cdot|s)\) where \(\hat{p}\) is the training distribution; class balancing is the same with \(\mathcal{Y}\) instead of \(\mathcal{S}\). We use the minibatch sampling approach for both class-balanced ERM and class-balanced last-layer retraining, and we provide a comparison with the subset method in Appendix A.

Deep feature reweighting.The recently proposed _deep feature reweighting_ (DFR) [33; 28] method achieves state-of-the-art WGA by performing ERM on the training dataset, then retraining the last layer of the neural network on a group-balanced held-out dataset, called the _reweighting dataset_. In the original implementation, half the validation set is used to construct the reweighting dataset: all data from the smallest group is included and the other groups are randomly downsampled to that size.

\begin{table}
\begin{tabular}{l l l r r r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{3}{c}{Group \(g\)} & \multicolumn{3}{c}{Training distribution \(\hat{p}\)} & \multicolumn{3}{c}{Data quantity} \\ \cline{2-10}  & Class \(y\) & Spurious \(s\) & \(\hat{p}(y)\) & \(\hat{p}(y)\) & \(\hat{p}(y|s)\) & Train & Val & Test \\ \hline \multirow{4}{*}{Waterbirds} & landbird & land & \multirow{2}{*}{\(.768\)} & \(.730\) & \(.984\) & \(3498\) & \(467\) & \(2225\) \\  & landbird & water & & \(.038\) & \(.148\) & \(184\) & \(466\) & \(2225\) \\  & waterbird & land & & \(.012\) & \(.016\) & \(56\) & \(133\) & \(642\) \\  & waterbird & water & & \(.220\) & \(.852\) & \(1057\) & \(133\) & \(642\) \\ \hline \multirow{4}{*}{CelebA} & non-blond & female & \multirow{2}{*}{\(.851\)} & \(.440\) & \(.758\) & \(71629\) & \(8535\) & \(9767\) \\  & non-blond & male & & \(.411\) & \(.980\) & \(66874\) & \(8276\) & \(7535\) \\  & blond & female & & \(.141\) & \(.242\) & \(22880\) & \(2874\) & \(2480\) \\  & blond & male & & \(.009\) & \(.020\) & \(1387\) & \(182\) & \(180\) \\ \hline \multirow{4}{*}{CivilComments} & neutral & no identity & \multirow{2}{*}{\(.887\)} & \(.551\) & \(.921\) & \(148186\) & \(25159\) & \(74780\) \\  & neutral & identity & & \(.336\) & \(.836\) & \(90337\) & \(14966\) & \(43778\) \\  & toxic & no identity & & \(.113\) & \(.047\) & \(.079\) & \(12731\) & \(2111\) & \(6455\) \\  & toxic & identity & & \(.066\) & \(.164\) & \(17784\) & \(2944\) & \(8769\) \\ \hline \multirow{4}{*}{MultiNLI} & contradiction & no negation & \multirow{2}{*}{\(.333\)} & \(.279\) & \(.300\) & \(57498\) & \(22814\) & \(34597\) \\  & contradiction & negation & & \(.054\) & \(.761\) & \(11158\) & \(4634\) & \(6655\) \\  & entailment & no negation & & \(.327\) & \(.352\) & \(67376\) & \(26949\) & \(40496\) \\  & entailment & negation & & \(.007\) & \(.104\) & \(1521\) & \(613\) & \(886\) \\  & neither & no negation & & \(.333\) & \(.348\) & \(66630\) & \(26655\) & \(39930\) \\  & neither & negation & & \(.010\) & \(.136\) & \(1992\) & \(797\) & \(1148\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Dataset composition.** We study four well-established benchmarks for group robustness across vision and language tasks. The class probabilities change dramatically when conditioned on the spurious feature. Note that Waterbirds is the only dataset that has a distribution shift and MultiNLI is the only dataset which is class-balanced _a priori_. Probabilities may not sum to \(1\) due to rounding.

Then, the feature embeddings (_i.e.,_ the outputs of the penultimate layer) of the reweighting dataset are pre-computed and used to train a logistic regression model with explicit \(\ell_{1}\)-regularization. The results are averaged over 10 randomly subsampled reweighting datasets, and a hyperparameter search is performed over \(\ell_{1}\) regularization strength on the other half of the validation set.

To emphasize practicality and efficiency, our implementation of DFR has some differences from the original. _(i)_ Instead of logistic regression, we train the last layer on the reweighting dataset via minibatch optimization using SGD and AdamW [43] for the vision and language tasks, respectively. This fits well into standard training pipelines and avoids pre-computing the feature embeddings and writing them to disk, which can be slow and memory-intensive. _(ii)_ To reduce the number of hyperparameters, we use a fixed-value \(\ell_{2}\) regularization instead of searching over \(\ell_{1}\) regularization strength. We observed similar performance for \(\ell_{1}\) and \(\ell_{2}\) regularization, which we believe is because \(\ell_{1}\)-regularized gradients do not induce sparsity [39]. _(iii)_ We sample uniformly at random from the groups in the held-out dataset (to get group balanced minibatches) instead of averaging over group-balanced subsets of the data [27]. We compare the implementations in detail in Appendix A.

Datasets and models.We study four datasets which are well-established as benchmarks for group robustness across vision and language tasks, detailed in Table 2 and summarized below.

* _Waterbirds_[71; 69; 58] is an image classification dataset where the task is to predict whether a bird is a landbird or a waterbird. The spurious feature is the image background: more landbirds are present on land backgrounds than waterbirds, and vice versa.2 Footnote 2: We note that the Waterbirds dataset is known to contain incorrect labels [66]. We report results on the original, un-corrected version for a fair comparison with previous work.
* _CelebA_[42; 58] is an image classification dataset where the task is to predict whether a person is blond or not. The spurious feature is gender, with \(16\times\) more blond women than blond men in the training set.
* _CivilComments_[7; 35] is a text classification dataset where the task is to predict whether a comment is toxic or not. The spurious feature is the presence of one of the following categories: male, female, LGBT, black, white, Christian, Muslim, or other religion.3 More toxic comments contain one of these categories than non-toxic comments, and vice versa. Footnote 3: This version of CivilComments has four groups, used in this work and by [58; 28; 33]. There is another version where the identity categories are not collapsed into one spurious feature; this version is used by [41; 81], so we do not report their CivilComments accuracies in Table 1. Both versions use the WillDS split [35]. Footnote 4: The Waterbirds validation and test datasets still contain more landbirds than waterbirds, but each class has equal quantities of land and water backgrounds.
* more contradictions have this property than entailments or neutral pairs.

Importantly, Waterbirds is the only dataset that has a distribution shift - its validation and test datasets are group-balanced conditioned on the classes, although still class-imbalanced.4 As a result, methods which use the validation set for training can improve performance without explicit group balancing.

Footnote 4: This version of CivilComments has four groups, used in this work and by [58; 28; 33]. There is another version where the identity categories are not collapsed into one spurious feature; this version is used by [41; 81], so we do not report their CivilComments accuracies in Table 1. Both versions use the WillDS split [35].

We utilize a ResNet-50 [24] pretrained on ImageNet-1K [57] for Waterbirds and CelebA, and a BERT [14] model pretrained on Book Corpus [83] and English Wikipedia for CivilComments and MultiNLI. Following previous work, we use half the validation set for feature reweighting [33; 28] and half for model selection with group annotations [58; 41; 33; 48; 28]. We run each experiment on three random seeds and do not utilize explicit early stopping except as part of SELF (see Section 5). See Appendix D for further details on the training procedure.

## 4 Class-balanced last-layer retraining

In this section, we investigate the necessity of a group-balanced reweighting dataset for DFR and show that _class-balanced last-layer retraining_ is a simple but strong baseline for group robustness without group annotations. To enable a fair comparison with our implementation of DFR (see Section 3), class-balanced last-layer retraining follows the same training procedure, except the reweighting dataset is constructed by sampling uniformly over the classes \(\mathcal{Y}\) instead of the groups \(\mathcal{G}\).

### An ablation on the proportion of worst-group data

In this section, we perform an ablation on the percentage of worst-group data in the reweighting dataset and show that class-balanced last-layer retraining can substantially improve WGA _even when the reweighting dataset has only a small proportion of worst group data._ For our ablation, we choose the worst groups based on the performance of ERM, and if two groups have similarly poor performance, we vary both. The worst groups for Waterbirds are landbirds on water backgrounds and waterbirds on land backgrounds; for CelebA blond men; for CivilComments toxic comments containing identity categories; and for MultiNLI entailments and neutral pairs containing negations.

For these experiments, we begin with the DFR reweighting dataset, _i.e.,_ a random group-balanced subset of the held-out dataset, where the size of each group is the minimum of the worst-group size and half the size of any other group.5 We define this dataset to include \(100\%\) of the worst-group data. We then reduce the percentage of worst-group data, while correspondingly increasing the percentage of data from the same class without the spurious feature. For example, the DFR reweighting dataset on CelebA has \(92\) points from each group, so reducing the worst group to \(25\%\) results in \(92\) non-blond females, \(92\) non-blond males, \(161\) blond females, and \(23\) blond males. The total data is kept constant.

Figure 1: **How much worst-group data does last-layer retraining really need?** We perform an ablation on the percentage of worst-group data used for class-balanced last-layer retraining, while keeping the total data constant. The results show that last-layer retraining can substantially improve worst-group accuracy _even when the reweighting dataset has only a small proportion of worst group data_, and that class balancing can be a major factor in its performance. The underperformance on Waterbirds below \(20\%\) is because there is too little worst-group data to observe consistent model behavior (less than ten samples). The stars \(\bigstar\) denote the baseline percentage of worst-group data in the training dataset. We plot the mean over three independent runs. See Tables 12 and 13 for details.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{Method} & Group & \multicolumn{4}{c}{Worst-group test accuracy} \\ \cline{3-6}  & annotations & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline Class-balanced ERM & ✗ & \(81.9_{\pm 3.4}\) & \(67.2_{\pm 5.6}\) & \(61.4_{\pm 0.7}\) & \(\mathbf{69.2_{\pm 1.6}}\) \\ UB last-layer retraining & ✗ & \(88.0_{\pm 0.8}\) & \(41.9_{\pm 1.4}\) & \(57.6_{\pm 4.2}\) & \(64.6_{\pm 1.0}\) \\ CB last-layer retraining & ✗ & \(\mathbf{92.6_{\pm 0.8}}\) & \(\mathbf{73.7_{\pm 2.8}}\) & \(\mathbf{80.4_{\pm 0.8}}\) & \(\mathbf{64.7_{\pm 1.1}}\) \\ \hline DFR (our impl.) & ✓ & \(92.4_{\pm 0.9}\) & \(87.0_{\pm 1.1}\) & \(81.8_{\pm 1.6}\) & \(70.8_{\pm 0.8}\) \\ DFR [33, 28] & ✓ & \(91.1_{\pm 0.8}\) & \(89.4_{\pm 0.9}\) & \(78.8_{\pm 0.5}\) & \(72.6_{\pm 0.3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Last-layer retraining on the held-out dataset.** While unbalanced (UB) last-layer retraining decreases performance, class-balanced (CB) last-layer retraining nearly matches DFR on Waterbirds and CivilComments. However, it still trails DFR on CelebA and MultiNLI; we improve these results with the SELF method described in Section 5. CB ERM is trained on the combined training and held-out datasets using class-balanced minibatches. We list the mean and standard deviation over three independent runs.

The results of this ablation are displayed in Figure 1. While a smooth increase in WGA with increasing worst-group data is expected, the extent of the early increase is surprising: over the four datasets, an average of \(67\%\) of the increase in WGA over class-unbalanced ERM is obtained by the first \(25\%\) of worst-group data. In particular, CelebA and CivilComments - the most class-imbalanced datasets we study - experience significant improvement even at low percentages of worst-group data. This phenomenon suggests that, while group balancing is still important for best results, _class balancing is a major factor in the performance of DFR_ on these two datasets.

Furthermore, class-balanced last-layer retraining can improve worst-group accuracy even when the held-out dataset has similar group imbalance as the training dataset (_i.e.,_ at the stars \(\bigstar\) in Figure 1). Based on this observation, we propose that class-balanced last-layer retraining _on the entire held-out dataset_ can be a simple but strong baseline for group robustness without group annotations. Table 3 details the results; on average over the four datasets, class-balanced last-layer retraining achieves \(94\%\) of DFR performance, compared to \(76\%\) without class balancing. However, it still trails DFR by a significant amount on CelebA and MultiNLI - the most group-imbalanced datasets we study. We improve these results with our selective last-layer finetuning (SELF) method described in Section 5.

Moreover, our experiments in Figure 4 (deferred to Appendix A) indicate that class-balanced last layer retraining has similar performance regardless of whether it is initialized with class-unbalanced or class-balanced ERM features. Contrasting with Idrissi _et al._[27], our results suggest that class balancing in the ERM stage is optional. This result has practical relevance for expensive models pre-trained without class balancing (_e.g.,_ large language models), as the benefits of class balancing on downstream tasks can be reaped by simply retraining the last layer instead of training a new model.

### A "free lunch" in group robustness

Motivated by the promising results of Section 4.1, where we performed class-balanced last-layer retraining on a fixed held-out set, we now ask _how can we best utilize a realistic training dataset?_ In particular, practical applications often work with a predetermined data, annotation, and compute budget. Within this budget, and with no explicit held-out dataset, would one achieve better group robustness by using the entire dataset for ERM or by holding out a subset for last-layer retraining?

We investigate this question on our four benchmark datasets by randomly splitting the initial dataset into two, then performing class-balanced ERM training on the first split (\(95\%\) of the data) and class-balanced last-layer retraining on the second split (\(5\%\) of the data). Figure 1(a) illustrates the results of our experiments on the training dataset and Figure 1(b) on the combined training and held-out datasets. Since the quantity of data is higher in the case of combining the training and held-out datasets, we expect all numbers to be higher in Figure 1(b) compared to Figure 1(a) (especially on

Figure 2: **A “free lunch” in group robustness. We compare class-balanced (CB) ERM on the entire dataset to splitting the dataset and performing CB ERM on the first (\(95\%\)) split and CB last-layer retraining on the second (\(5\%\)) split. This technique improves worst-group accuracy on Waterbirds, CelebA, and CivilComments by up to \(17\%\) while using _no additional data or annotations_ for training beyond ERM. We believe it underperforms on MultiNLI because there is not enough data in the first split, _i.e.,_ ERM performance can be improved by collecting more data. We plot the mean and standard deviation over three independent runs. See Table 14 for detailed results.**

Waterbirds, which has a more group-balanced validation set). We perform no hyperparameter tuning for these experiments, and therefore we do not utilize group annotations _even for model selection_.

Figure 2 indicates that splitting the dataset and performing last-layer retraining substantially improves worst-group accuracy on Waterbirds, CelebA, and CivilComments. It decreases performance on MultiNLI, which is the only dataset where adding held-out data from the same distribution significantly increases ERM worst-group accuracy compared to DFR. Specifically, class-balanced ERM achieves \(67.4\pm 2.4\)% WGA on the training dataset and \(69.2\pm 1.6\)% on the combined training and held-out datasets, while our DFR implementation achieves \(70.8\pm 0.8\)%. Therefore, we hypothesize that last-layer retraining on the second split can improve group robustness _only if there is enough data for ERM to perform near-optimally_ on the first split, _i.e.,_ if the performance of ERM on the first split is limited by dataset bias rather than sample variance.

Based on this hypothesis, our answer to the posed question is: if ERM performance is stable when holding out \(5\%\) of data, perform last-layer retraining on the held-out dataset instead of ERM on the initial dataset. We call this technique a "free lunch" because it improves worst-group accuracy with _no additional data or annotations_ beyond ERM (including for model selection). In particular, we utilize less data for ERM training and less compute due to the efficient nature of last-layer retraining. Therefore, we believe this method is especially relevant to practitioners, and it can be easily implemented with little change to data processing or model training workflows.

A critical remaining question is _why_ last-layer retraining improves group robustness; since the training and held-out datasets have equally drastic group imbalance, it is counterintuitive that reducing the quantity of data used for ERM and performing last-layer retraining would increase worst-group accuracy. In some cases, as seen on MultiNLI in Figure 2, training an ERM model with less data can be detrimental - but on the other three datasets, last-layer retraining substantially improves over ERM. We leave it to future empirical and theoretical work to better understand this phenomenon.

## 5 Selective last-layer finetuning

While class-balanced last-layer retraining can improve worst-group accuracy without group annotations, its performance is still inferior to DFR, particularly on the highly group-imbalanced CelebA and MultiNLI datasets. In these cases, training on the entire held-out set can be detrimental; instead, we show that constructing the reweighting dataset by detecting and upsampling worst-group data is more effective. We propose _selective last-layer finetuning_ (SELF), which uses an auxiliary model to select a small, more group-balanced reweighting dataset, then _finetunes_ the ERM last layer _instead of retraining entirely_ to avoid overfitting on this smaller dataset.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{2}{c}{Held-out annotations} & \multicolumn{4}{c}{Worst-group test accuracy} \\ \cline{2-7}  & Group & Class & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline Class-balanced ERM & ✗ & ✓ & \(81.9_{\pm 3.4}\) & \(67.2_{\pm 5.6}\) & \(61.4_{\pm 0.7}\) & \(69.2_{\pm 1.6}\) \\ CB last-layer retraining & ✗ & ✓ & \(92.6_{\pm 0.8}\) & \(73.7_{\pm 2.8}\) & \(80.4_{\pm 0.8}\) & \(64.7_{\pm 1.1}\) \\ \hline Random SELF & ✗ & ✗ & \(91.1_{\pm 1.9}\) & \(80.2_{\pm 6.4}\) & \(\mathbf{80.5_{\pm 1.6}}\) & \(65.0_{\pm 4.0}\) \\ Misclassification SELF & ✗ & ✓ & \(92.6_{\pm 0.8}\) & \(83.0_{\pm 6.1}\) & \(62.7_{\pm 4.6}\) & \(72.2_{\pm 2.2}\) \\ ES misclassification SELF & ✗ & ✓ & \(92.2_{\pm 0.7}\) & \(80.4_{\pm 3.9}\) & \(65.8_{\pm 7.6}\) & \(\mathbf{73.3_{\pm 1.2}}\) \\ Dropout disagreement SELF & ✗ & ✗ & \(92.3_{\pm 0.5}\) & \(\mathbf{85.7_{\pm 1.6}}\) & \(69.9_{\pm 5.2}\) & \(68.7_{\pm 3.4}\) \\ ES disagreement SELF & ✗ & ✗ & \(\mathbf{93.0_{\pm 0.3}}\) & \(83.9_{\pm 0.9}\) & \(79.1_{\pm 2.1}\) & \(70.7_{\pm 2.5}\) \\ \hline DFR (our impl.) & ✓ & ✓ & \(92.4_{\pm 0.9}\) & \(87.0_{\pm 1.1}\) & \(81.8_{\pm 1.6}\) & \(70.8_{\pm 0.8}\) \\ DFR [33, 28] & ✓ & ✓ & \(91.1_{\pm 0.8}\) & \(89.4_{\pm 0.9}\) & \(78.8_{\pm 0.5}\) & \(72.6_{\pm 0.3}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Comparison of selective last-layer finetuning methods.** SELF nearly matches DFR and improves WGA over class-balanced (CB) last-layer retraining by up to 12%. Early-stop (**ES**) disagreement SELF performs the best overall, and disagreement methods perform especially well on CivilComments. Dropout and ES disagreement request few as 20 class annotations from the held-out dataset. All SELF methods use a small set of group annotations for model selection. We list the mean and standard deviation over three independent runs.

SELF does not use group annotations for training and can be implemented even when class annotations are unavailable in the held-out dataset, making it applicable in settings not captured by current techniques (_e.g.,_ adaptation to an unlabeled target domain [45]). If class annotations _are not_ available, we select points which are disagreed upon by ERM and a regularized model, then request their labels. If class annotations _are_ available, we can still use disagreement, or alternatively, we select points which are misclassified by the ERM or early-stopped model. Our experiments indicate that _disagreement SELF between the ERM and early-stopped models_ performs the best overall, increasing the worst-group accuracy of class-balanced last-layer retraining to near-DFR levels while requesting less than 3% of the held-out class annotations. Our results are detailed in Table 4.

We formalize SELF as follows. Let \(f,g:\mathcal{X}\rightarrow\mathbb{R}^{|\mathcal{Y}|}\) be models with logit outputs, \(X\subseteq\mathcal{X}\) be a held-out dataset, \(c:\mathbb{R}^{2|\mathcal{Y}|}\rightarrow\mathbb{R}\) be a cost function, and \(n\) be an integer. SELF constructs the reweighting set \(D\) by greedily selecting the \(n\) points in \(X\) whose outputs incur the highest cost, _i.e.,_

\[D=\operatorname*{argmax}_{S\subseteq X,|S|=n}\sum_{x\in S}c(f(x),g(x)).\] (1)

Then, SELF requests class annotations and performs class-balanced _finetuning_ (_i.e.,_ starting from the ERM weights) of the last layer of the ERM model on \(D\), using the same implementation as last-layer retraining (see Section 4). We study the following four variants of SELF:

* _Misclassification_: \(f\) is an ERM model and \(g\) is the labeling function, _i.e.,_, for a datapoint \(x\) with label \(y\), the output \(g(x)\) is the one-hot encoded vector for \(y\). The cost function \(c\) is cross entropy loss.
* _Early-stop misclassification_:6\(f\) is an early-stopped ERM model and \(g\) is the labeling function. The cost function \(c\) is cross entropy loss. Footnote 6: Using early-stop misclassification to upsample worst-group data is the premise of JTT [41].
* _Dropout disagreement_: \(f\) is an ERM model and \(g\) is the same model where inference is run with nodes randomly dropped out in the last layer [65]. The cost function \(c\) is KL divergence with softmax.
* _Early-stop disagreement_: \(f\) is an ERM model and \(g\) is an early-stopped ERM model. The cost function \(c\) is KL divergence with softmax.

We also experiment with a pure random baseline, referred as Random SELF in Table 4, where \(D\) consists of \(n\) random points from \(X\). While Random SELF suffers high variance as expected, perhaps surprisingly its performance is still competitive on average. The reweighting dataset size \(n\) is a hyperparameter, and for disagreement SELF it corresponds to the quantity of class annotations requested. Please see Appendices B and D for hyperparameter details and ablation studies.

### Analysis of SELF performance

To quantify the extent of SELF's upsampling, we plot the percentage of the reweighting dataset consisting of worst-group data in Figure 3. In particular, we present to the best of our knowledge the first evidence that model disagreement effectively upsamples worst-group data, an observation which may be of independent interest. With that said, Figure 3 does not formally establish that better group balance has a strong correlation with WGA - rather, it suggests an _additional subtlety in the DFR hypothesis_, since group balance alone does not fully explain the performance of last-layer retraining. In addition to the balance of the reweighting dataset, it is likely that characteristics of the _specific data selected_ also contribute to SELF results. Additionally, the competitive performance of pure random SELF in Table 4 further questions the importance of group balancing in DFR.

The importance of which data are selected could explain why disagreement SELF often outperforms misclassification SELF despite having access to less information. While misclassification selects the _most difficult_ or _noisiest_ points, disagreement selects the _most uncertain_ points. For example, dropout models approximate a theoretically justified uncertainty metric [16] which is likely to be higher on worst-group data, and early-stopped models tend to fit simple patterns first [2; 41] including the majority group. Training on the most uncertain data is a key tenet of active learning [13; 59; 12], which rationalizes the performance of disagreement SELF and provides motivation for further investigation of _why_ last-layer retraining improves group robustness.

A remaining question is why the performance of disagreement SELF is so strong on CivilComments compared to misclassification-based methods such as JTT [41] and ES misclassification SELF. We show in Figure 6 (deferred to Appendix B) that, contrary to the assumptions made by JTT and other early-stop misclassification methods, the worst-group accuracy _decreases_ with training on CivilComments. This highlights a potential advantage of disagreement methods over misclassification methods, as disagreement is justified _regardless of whether the regularized or ERM model has a greater dependence on spurious features_. Moreover, while misclassification methods do indeed upsample the minority group in Figure 3, we show in Table 10 (deferred to Appendix B) that the held-out set training accuracy of misclassification SELF tends to be very low - down to \(0\%\) on MultiNLI - evidence that misclassifications are too difficult to learn without modifying the features.

Theoretical proof-of-concept.The observations of Section 5.1 raise the question of whether disagreement SELF can ever be shown to _provably upsample minority group points_. We provide a simple proof-of-concept for SELF by considering the last layer to be a linear model (which could be under- or overparameterized) with core, spurious and junk features. We show that the disagreement between the regularized model and the ERM model (measured by total variation distance between the predicted distributions) is provably higher on minority examples than majority examples, _regardless of model dependence on the spurious feature_. In particular, this shows provable benefits of disagreement SELF even in situations where the early-stopped model has higher worst-group accuracy than the convergent model, which may not be captured by related methods in the literature [41]. Our detailed setup, assumptions, and main theoretical result (Theorem 1) are stated in detail in Appendix C.

## 6 Conclusion

In this paper, we presented a comprehensive examination of the performance of last-layer retraining in the absence of group and class annotations. We showed that class-balanced last-layer retraining is a simple but strong baseline for group robustness, and that holding out a subset of the training data to retrain the last layer can substantially improve worst-group accuracy. We then proposed selective last-layer finetuning (SELF), whose early-stop disagreement version improves performance to near-DFR levels with no group annotations and less than \(3\%\) of the held-out class annotations.

Our work has generated several open questions which could prove fruitful for further research. First, why does last-layer retraining on a held-out split of the training dataset improve group robustness (see Section 4.2)? Second, to what extent does the precise data selected matter for reweighting, _e.g.,_ which of disagreement SELF and misclassification SELF would perform better when hyperparameters are set to equalize group balance? Third, what is the optimal strategy in general for obtaining the regularized model in disagreement SELF (dropout, early-stopping, or a different technique) and why?

Figure 3: **SELF upsamples the worst group. We plot the percentage of the reweighting dataset consisting of worst-group data for each SELF method. “Baseline” represents the percentage of worst-group data in the held-out dataset, while “Balanced” is the percentage of worst-group data necessary to achieve group balance. This is to the best of our knowledge the first empirical evidence that model disagreement is an effective method for upsampling worst-group data. The worst groups in each dataset are listed in Section 4.1. We plot the mean for the best dataset size \(n\) over three independent runs. See Table 15 for detailed results.**

Acknowledgments.We thank Google Cloud for the gift of compute credits, Jacob Abernethy and Eva Dyer for the additional compute assistance, and anonymous reviewers for their helpful feedback. We also thank Harikrishna Narasimhan for reading an earlier version of the manuscript and providing thoughtful comments and Seokhyeon Jeong for catching an error in an earlier version of the code. T.L. acknowledges support from the DoD NDSEG Fellowship. V.M. acknowledges support from the NSF (through CAREER award CCF-2239151 and award IIS-2212182), an Adobe Data Science Research Award, an Amazon Research Award, and a Google Research Collabs Award.

## References

* Arjovsky et al. [2019] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint 1907.02893_, 2019. Cited on page 3.
* Arpit et al. [2017] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon Lacoste-Julien. A closer look at memorization in deep networks. In _International Conference on Machine Learning (ICML)_, 2017. Cited on page 9.
* Baek et al. [2022] Christina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022. Cited on page 4.
* Balcan et al. [2006] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In _International Conference on Machine Learning (ICML)_, 2006. Cited on page 3.
* Beery et al. [2018] Sara Beery, Grant van Horn, and Pietro Perona. Recognition in terra incognita. In _European Conference on Computer Vision (ECCV)_, 2018. Cited on page 1.
* Blodgett et al. [2016] Su Lin Blodgett, Lisa Green, and Brendan O'Connor. Demographic dialectal variation in social media: A case study of African-American English. In _Empirical Methods in Natural Language Processing (EMNLP)_, 2016. Cited on page 3.
* Borkan et al. [2019] Daniel Borkan, Lucas Dixon, Jeffrey Sorenson, Nithium Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. In _World Wide Web (WWW)_, 2019. Cited on page 5.
* Buolamwini and Gebru [2018] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial gender classification. In _Conference on Fairness, Accountability, and Transparency in Machine Learning (FATML)_, 2018. Cited on page 3.
* Chouldechova [2016] Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. In _Conference on Fairness, Accountability, and Transparency in Machine Learning (FATML)_, 2016. Cited on page 1.
* Cohn et al. [1994] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. _Machine Learning_, 15(2):201-221, 1994. Cited on page 3.
* Creager et al. [2021] Elliot Creager, Jorn-Henrik Jacobsen, and Richard Zemel. Environment inference for invariant learning. In _International Conference on Machine Learning (ICML)_, 2021. Cited on page 3.
* Culotta and McCallum [2005] Aron Culotta and Andrew McCallum. Reducing labeling effort for structured prediction tasks. In _Conference on Artificial Intelligence (AAAI)_, 2005. Cited on page 9.
* Dagan and E. P. Engelson [1995] Ido Dagan and Sean P. Engelson. Committee-based sampling for training probabilistic classifiers. In _International Conference on Machine Learning (ICML)_, 1995. Cited on page 9.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _North American Association for Computational Linguistics (NAACL)_, 2019. Cited on pages 5.
* Font et al. [2020] Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy, and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the neural tangent kernel. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020. Cited on page 3.
* Gal and Ghahramani [2016] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In _International Conference on Machine Learning (ICML)_, 2016. Cited on page 9.
* Geirhos et al. [2019] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In _International Conference on Learning Representations (ICLR)_, 2019. Cited on page 3.

* Geirhos et al. [2020] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A. Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2:665-673, 2020. Cited on pages 1 and 3.
* Goel et al. [2021] Karan Goel, Albert Gu, Yixuan Li, and Christopher Re. Model patching: Closing the subgroup performance gap with data augmentation. In _International Conference on Learning Representations (ICLR)_, 2021. Cited on page 3.
* Gururangan et al. [2018] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In _North American Association for Computational Linguistics (NAACL)_, 2018. Cited on page 3.
* Hanneke [2014] Steve Hanneke. _Theory of Disagreement-Based Active Learning_. Foundations and Trends in Machine Learning, 2014. Cited on page 3.
* Harris et al. [2020] Charles R. Harris, K. Jarrod Millman, Stefan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fernandez del Rio, Mark Wiebe, Pearu Peterson, Pierre Gerard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. _Nature_, 585(1):357-362, 2020. Cited on page 2.
* Hashimoto et al. [2018] Tatsunori B. Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demographics in repeated loss minimization. In _International Conference on Machine Learning (ICML)_, 2018. Cited on page 3.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2016. Cited on pages 5 and 25.
* Hovy and Sogaard [2015] Dirk Hovy and Anders Sogaard. Tagging performance correlates with author age. In _Association for Computational Linguistics (ACL)_, 2015. Cited on page 3.
* Hunter [2007] John D. Hunter. Matplotlib: A 2D graphics environment. _Computing in Science & Engineering_, 9(3):90-95, 2007. Cited on page 25.
* Idrissi et al. [2022] Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancing achieves competitive worst-group-accuracy. In _Conference on Causal Learning and Reasoning (CLeaR)_, 2022. Cited on pages 2, 3, 4, 5, 7, and 25.
* Izmailov et al. [2022] Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew Gordon Wilson. On feature learning in the presence of spurious correlations. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022. Cited on pages 2, 3, 4, 5, 6, 8, 18, 25, and 28.
* Jastrzebski et al. [2020] Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof Geras. The break-even point on optimization trajectories of deep neural networks. In _International Conference on Learning Representations (ICLR)_, 2020. Cited on page 23.
* Jiang et al. [2022] Yiding Jiang, Vaishnav Nagarajan, Christina Baek, and J. Zico Kolter. Assessing generalization of SGD via disagreement. In _International Conference on Learning Representations (ICLR)_, 2022. Cited on page 4.
* Kim et al. [2021] Eungyeup Kim, Jihyeon Lee, and Jaegul Choo. BiaSwap: removing dataset bias with bias-tailored swapping augmentation. In _International Conference on Computer Vision (ICCV)_, 2021. Cited on page 3.
* Kim et al. [2022] Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak. Learning debiased classifier with biased committee. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022. Cited on page 3.
* Kirichenko et al. [2023] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is sufficient for robustness to spurious correlations. In _International Conference on Learning Representations (ICLR)_, 2023. Cited on pages 1, 2, 3, 4, 5, 6, 8, 18, 25, and 28.

* [34] Frederic Koehler, Lijia Zhou, Danica J. Sutherland, and Nathan Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021. Cited on page 23.
* [35] Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning (ICML)_, 2021. Cited on pages 2 and 5.
* [36] Tyler LaBonte. Milkshake: Quick and extendable experimentation with classification models. Note: http://github.com/tmlabonte/milkshake Cited on page 25.
* [37] Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar. Dropout disagreement: A recipe for group robustness with fewer annotations. In _Conference on Neural Information Processing Systems (NeurIPS) Workshop on Distribution Shifts_, 2022. Cited on page 2.
* [38] Tyler LaBonte, Vidya Muthukumar, and Abhishek Kumar. Saving a split for last-layer retraining can improve group robustness without group annotations. In _International Conference on Machine Learning (ICML) Workshop on Spurious Correlations, Invariance, and Stability_, 2023. Cited on page 2.
* [39] John Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated gradient. _Journal of Machine Learning Research (JMLR)_, 10(1):777-801, 2009. Cited on page 2.
* [40] Yoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from underspecified data. In _International Conference on Learning Representations (ICLR)_, 2023. Cited on pages 4 and 28.
* [41] Evan Zheran Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training group information. In _International Conference on Machine Learning (ICML)_, 2021. Cited on pages 2, 3, 5, 9, 10, 21, 28.
* [42] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _International Conference on Computer Vision (ICCV)_, 2015. Cited on pages 1, 3, and 5.
* [43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations (ICLR)_, 2019. Cited on pages 5 and 25.
* [44] TorchVision maintainers and contributors. TorchVision: PyTorch's computer vision library. _GitHub_, 2016. Cited on page 25.
* [45] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds and algorithms. In _Conference on Learning Theory (COLT)_, 2009. Cited on page 2.
* [46] Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In _Association for Computational Linguistics (ACL)_, 2019. Cited on page 3.
* [47] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from failure: Training debiased classifier from biased classifier. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020. Cited on page 3.
* [48] Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin. Spread spurious attribute: Improving worst-group accuracy with spurious attribute estimation. In _International Conference on Learning Representations (ICLR)_, 2022. Cited on pages 2, 3, 5, and 28.
* [49] Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014. Cited on page 23.
* [50] Timothy Niven and Hung-Yu Kao. Probing neural network comprehension of natural language arguments. In _Association for Computational Linguistics (ACL)_, 2019. Cited on page 3.

* Oakden-Rayner et al. [2019] Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher Re. Hidden stratification causes clinically meaningful failures in machine learning for medical imaging. In _Conference on Neural Information Processing Systems (NeurIPS) Workshop on Machine Learning for Health_, 2019. Cited on page 3.
* Pagliardini et al. [2023] Matteo Pagliardini, Martin Jaggi, Francois Fleuret, and Sai Praneeth Karimireddy. Agree to disagree: Diversity through disagreement for better transferability. In _International Conference on Learning Representations (ICLR)_, 2023. Cited on pages 4 and 28.
* Paszke et al. [2017] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In _Conference on Neural Information Processing Systems (NeurIPS) Workshop on Automatic Differentiation_, 2017. Cited on page 25.
* Pezeshki et al. [2021] Mohammad Pezeshki, Sekou-Oumar Kaba, Yoshua Bengio, Aaron Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021. Cited on page 3.
* Qiu et al. [2023] Shikai Qiu, Andres Potapczynski, Pavel Izmailov, and Andrew Gordon Wilson. Simple and fast group robustness by automatic feature reweighting. In _International Conference on Machine Learning (ICML)_, 2023. Cited on page 3.
* Rosenfeld et al. [2018] Amir Rosenfeld, Richard Zemel, and John K. Tsotsos. The elephant in the room. _arXiv preprint 1808.03305_, 2018. Cited on page 3.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. _International Journal of Computer Vision (IJCV)_, 115(1):211-252, 2015. Cited on pages 5 and 25.
* Sagawa et al. [2020] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. In _International Conference on Learning Representations (ICLR)_, 2020. Cited on pages 1, 2, 3, 4, 5.
* Scheffer et al. [2001] Tobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for information extraction. In _International Conference on Advances in Intelligent Data (CAIDA)_, 2001. Cited on page 9.
* Shah et al. [2020] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020. Cited on page 1.
* Shetty et al. [2019] Rakshith Shetty, Bernt Schiele, and Mario Fritz. Not using the car to see the sidewalk: Quantifying and controlling the effects of context in classification and segmentation. In _Conference on Computer Vision and Pattern Recognition (CVPR)_, 2019. Cited on page 3.
* Singla and Feizi [2022] Sahil Singla and Soheil Feizi. Salient ImageNet: how to discover spurious features in deep learning? In _International Conference on Learning Representations (ICLR)_, 2022. Cited on page 3.
* Sohoni et al. [2020] Nimit S. Sohoni, Jared A. Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No subclass left behind: Fine-grained robustness in coarse-grained classification problems. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020. Cited on page 3.
* Sohoni et al. [2022] Nimit S. Sohoni, Maziar Sanjabi, Nicolas Ballas, Aditya Grover, Shaoliang Nie, Hamed Firooz, and Christopher Re. BARACK: partially supervised group robustness with guarantees. In _International Conference on Machine Learning (ICML) Workshop on Spurious Correlations, Invariance, and Stability_, 2022. Cited on page 3.
* Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15(1):1929-1958, 2014. Cited on page 9.

* [66] Saeid Asgari Taghanaki, Aliasghar Khani, Fereshte Khani, Ali Gholami, Linh Trana, Ali Mahdavi-Amiri, and Ghassan Hamarneh. MaskTune: mitigating spurious correlations by forcing to explore. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2022. Cited on pages 3 and 5.
* [67]Rachael Tatman. Gender and dialect bias in YouTube's automatic captions. In _Association for Computational Linguistics (ACL) Workshop on Ethics in Natural Language Processing_, 2017. Cited on page 3.
* [68]Vladimir Vapnik. _Statistical Learning Theory_. Wiley, 1998. Cited on page 1.
* [69]Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The CaltechUCSD birds-200-2011 dataset. Technical report California Institute of Technology, 2011. Cited on page 5.
* [70]Jiaheng Wei, Harikrishna Narasimhan, Ehsan Amid, Wen-Sheng Chu, Yang Liu, and Abhishek Kumar. Distributionally robust post-hoc classifiers under prior shifts. In _The Eleventh International Conference on Learning Representations_, 2022. Cited on page 3.
* [71]Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-UCSD birds 200. Technical report California Institute of Technology, 2010. Cited on page 5.
* [72]William Falcon and the PyTorch Lightning maintainers and contributors. PyTorch Lightning. GitHub. Cited on page 25.
* [73]Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _North American Association for Computational Linguistics (NAACL)_, 2018. Cited on page 5.
* [74]Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In _Conference on Empirical Methods in Natural Language Processing (EMNLP) System Demonstrations_, 2020. Cited on page 25.
* [75]Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. In _International Conference on Learning Representations (ICLR)_, 2021. Cited on page 3.
* [76]Yilun Xu, Hao He, Tianxiao Shen, and Tommi Jaakkola. Controlling directions orthogonal to a classifier. In _International Conference on Learning Representations (ICLR)_, 2022. Cited on page 3.
* [77]Yadollah Yaghoobzadeh, Soroush Mehri, Remi Tachet, T.J. Hazen, and Alessandro Sordoni. Increasing robustness to spurious correlations using forgettable examples. In _European Association for Computational Linguistics (EACL)_, 2021. Cited on page 3.
* [78]Yao-Yuan Yang, Chi-Ning Chou, and Kamalika Chaudhuri. Understanding rare spurious correlations in neural networks. _arXiv preprint 2202.05189_, 2022. Cited on page 3.
* [79]John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph J. Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study. _PLoS Medicine_, 15:e1002683, 2018. Cited on pages 1.
* [80]Jianyu Zhang, David Lopez-Paz, and Leon Bottou. Rich feature construction for the optimization-generalization dilemma. In _International Conference on Machine Learning (ICML)_, 2022. Cited on page 3.
* [81]Michael Zhang, Nimit S. Sohoni, Hongyang R. Zhang, Chelsea Finn, and Christopher Re. Correct-n-contrast: A contrastive approach for improving robustness to spurious correlations. In _International Conference on Machine Learning (ICML)_, 2022. Cited on pages 3 and 5.

* [82] Lijia Zhou, Danica J. Sutherland, and Nathan Srebro. On uniform convergence and low-norm interpolation learning. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2020. Cited on page 23.
* [83] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _International Conference on Computer Vision (ICCV)_, 2015. Cited on pages 5 and 25.

[MISSING_PAGE_EMPTY:18]

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{Worst-group test accuracy} \\ \cline{2-5}  & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline CB ERM & \(72.6_{\pm 3.2}\) & \(66.3_{\pm 3.2}\) & \(60.2_{\pm 2.7}\) & \(67.4_{\pm 2.4}\) \\ CB last-layer retraining on training dataset & \(71.0_{\pm 2.5}\) & \(66.9_{\pm 1.4}\) & \(61.9_{\pm 0.8}\) & \(67.0_{\pm 1.5}\) \\ CB last-layer retraining on held-out dataset & \(77.4_{\pm 0.3}\) & \(73.0_{\pm 2.3}\) & \(77.9_{\pm 1.5}\) & \(63.0_{\pm 1.5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: **Necessity of the held-out dataset.** Next, we investigate whether holding out a subset of the training dataset for class-balanced (CB) last-layer retraining is essential, or if retraining on the entire (previously seen) dataset is also effective. For retraining on the training dataset, we use the same class-balanced last-layer retraining procedure as the held-out dataset, but we train for \(20\) epochs for the vision tasks and \(2\) epochs for the language tasks. For retraining on the held-out dataset, we report the best over four splits (_i.e.,_ the same numbers as Figure 2). Our results suggest that holding out data is necessary to achieve maximal worst-group accuracy, though last-layer retraining on the training dataset interestingly prevents the performance decrease on MultiNLI. We list the mean and standard deviation over three independent runs.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Group Anns} & \multicolumn{4}{c}{Average test accuracy} \\ \cline{3-5}  & & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline ERM & ✗ & \(90.2_{\pm 0.7}\) & \(94.4_{\pm 0.2}\) & \(92.0_{\pm 0.2}\) & \(81.8_{\pm 0.2}\) \\ CB last-layer retraining & ✗ & \(94.8_{\pm 0.3}\) & \(93.6_{\pm 0.2}\) & \(87.1_{\pm 0.0}\) & \(82.0_{\pm 0.2}\) \\ ES disagreement SELF & ✗ & \(94.0_{\pm 1.7}\) & \(91.7_{\pm 0.4}\) & \(87.7_{\pm 0.6}\) & \(81.2_{\pm 0.7}\) \\ DFR (our impl.) & ✓ & \(94.9_{\pm 0.3}\) & \(92.6_{\pm 0.5}\) & \(87.5_{\pm 0.2}\) & \(81.7_{\pm 0.2}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: **Average accuracy performance.** We detail the average test accuracy of our methods on the 4 benchmark datasets. Both of our methods have similar average accuracy to DFR, which experiences a slight accuracy/robustness tradeoff compared to ERM (as is typical in the robustness literature). We list the mean and standard deviation over three independent seeds.

Figure 4: **Class-balanced last-layer retraining renders class-balanced ERM optional.** We compare class-balanced (CB) last-layer retraining on the held-out dataset initialized with unbalanced (UB) or CB ERM features. Our results show that CB ERM is unnecessary as long as the last layer is retrained with class balancing. We plot the mean and standard deviation over three independent runs.

[MISSING_PAGE_EMPTY:20]

\begin{table}
\begin{tabular}{l l l l l} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{Held-out training accuracy} \\ \cline{2-5}  & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline Random SELF & \(96.3_{\pm 0.2}\) & \(90.6_{\pm 9.2}\) & \(91.1_{\pm 7.8}\) & \(86.7_{\pm 11.6}\) \\ Misclassification SELF & \(94.6_{\pm 1.6}\) & \(25.3_{\pm 21.9}\) & \(0.4_{\pm 0.2}\) & \(0.2_{\pm 0.3}\) \\ ES Misclassification SELF & \(91.3_{\pm 7.5}\) & \(28.5_{\pm 24.7}\) & \(1.1_{\pm 2.0}\) & \(0.0_{\pm 0.0}\) \\ Dropout Disagreement SELF & \(95.5_{\pm 1.4}\) & \(98.7_{\pm 1.5}\) & \(50.9_{\pm 6.3}\) & \(68.3_{\pm 24.7}\) \\ ES Disagreement SELF & \(97.0_{\pm 2.0}\) & \(79.9_{\pm 9.2}\) & \(58.1_{\pm 3.3}\) & \(52.6_{\pm 2.5}\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: **SELF training accuracies.** We detail the training accuracies on the SELF held-out dataset using the best configuration of each method with respect to worst-group validation accuracy. We find that the misclassification techniques have much lower training accuracy than disagreement or random – as low as \(0\%\) on MultiNLI – meaning that the misclassified points _cannot be fit without changing the features_. We list the mean and standard deviation over three independent seeds.

Figure 6: **Group validation accuracies over training.** We plot the validation accuracies by group for the four benchmark datasets over the course of ERM training. Contrary to the assumptions made by JTT [41] and other early-stop misclassification methods, _the worst-group accuracy decreases with training_ on CivilComments. This highlights a potential advantage of disagreement methods over misclassification methods: disagreement works regardless of whether the early-stopped or convergent model has a greater dependence on the spurious feature, as long as there is a large relative difference. We plot the mean over three independent runs and leave out error bars for readability.

## Appendix A

Figure 7: **Label efficiency comparison. We compare the performance of early-stop disagreement SELF to DFR while varying the amount of group annotations used for validation and retraining, respectively. The results show that SELF is robust to hyperparameter tuning and can massively reduce the annotation requirement: _e.g.,_ at \(1\%\) of data, Waterbirds has only \(6\) examples and CelebA has only \(99\) examples. Moreover, SELF retains more performance than DFR when the number of group annotations is very low. We plot the mean and standard deviation over three independent seeds.**Theoretical proof-of-concept for disagreement SELF

In tthis section, we describe and state our main theoretical proof-of-concept of disagreement SELF (Theorem 1). We begin with a description of our setup and assumptions.

Setup.Let \(\mathcal{X}\subseteq\mathbb{R}^{v}\) be the data domain, \(n\) be the sample size, and \(d\geq v,n\) be the number of features. For \(x\in\mathcal{X}\) and weights \(\alpha,\beta>0\) we assume that the learned predictor of both the ERM and regularized models takes the form

\[\hat{f}(x)=\alpha\phi_{\text{core}}(x)+\beta\phi_{\text{sp}}(x)+\sum_{k=1}^{d- 2}\phi_{\text{junk}}^{k}(x)\] (2)

and assume that the label \(y\) is generated by \(y=\text{sgn}(\phi_{\text{core}}(x))\) (_i.e._, zero label noise). Note that this specific split of features into core, spurious, and junk categories is an extension of a common simplified setting in the literature on overparameterized linear models [82, 34], where only core and junk features are considered and only in-distribution accuracy is analyzed. We also consider the domain \(\mathcal{X}\) to be partitioned into a minority group \(\mathcal{X}_{\text{min}}\) and majority group \(\mathcal{X}_{\text{maj}}\) such that \(y\phi_{\text{sp}}(x)<0\) for all \(x\in\mathcal{X}_{\text{min}}\) and \(y\phi_{\text{sp}}(x)>0\) for all \(x\in\mathcal{X}_{\text{maj}}\). (By definition, \(y\phi_{\text{core}}(x)>0\) for any \(x\).)

Assumptions on the feature learning process.We make some extra simplifying assumptions on the feature learning process that are motivated by empirical observations in the literature. First, we assume that the learned features \(\phi_{\text{core}},\phi_{\text{sp}},\phi_{\text{junk}}\) are _identical_ for the regularized and ERM models, and are only weighted differently in the last layer. This assumption is well-justified for dropout disagreement SELF since we only use dropout for inference in the last layer (with frozen features), and for early-stop disagreement SELF since feature learning has empirically been observed to occur during the initial phase of training [29, 15]. More concretely, we consider the ERM model \(\hat{f}_{\text{erm}}\) to have weights \(\alpha_{\text{erm}}\) and \(\beta_{\text{erm}}\), and the regularized model \(\hat{f}_{\text{reg}}\) to have weights \(\alpha_{\text{erm}}\) and \(\beta_{\text{reg}}\). Second, we assume that weights on the core and spurious features are _normalized_ such that \(\alpha_{\text{erm}}+\beta_{\text{erm}}=\alpha_{\text{reg}}+\beta_{\text{reg}}\), essentially positing that the proportion of junk feature strength is the same in both ERM and regularized models. This is also justified by the empirical observation that regularization methods such as early stopping make minimal difference to in-distribution accuracy [49].

Main result.The following result compares a minority group test example and a majority group test example with features that are equal in magnitude, and shows that the disagreement is higher for minority group points regardless of whether the ERM or regularized model weights the spurious feature higher. We will use the _total variation distance (TVD)_ as our cost function for disagreement SELF. We focus on TVD instead of KL divergence to simplify the proof; we show in Table 9 (detailed in Appendix B) that the empirical results are competitive using TVD (though worse on CelebA). We show that the difference of the TVD terms is proportional to \(|\beta_{\text{erm}}-\beta_{\text{reg}}|:=|\alpha_{\text{erm}}-\alpha_{\text{ reg}}|\), illustrating that the models disagree more drastically the greater the discrepancy between their dependence on the spurious feature (or, equivalently, the core feature).

**Theorem 1** (Disagreement SELF).: _Consider two examples \(x_{\text{min}}\in\mathcal{X}_{\text{min}}\) and \(x_{\text{maj}}\in\mathcal{X}_{\text{maj}}\) which have the same labels and whose features have the same magnitude, i.e., \(\phi_{\text{core}}(x_{\text{min}})=\phi_{\text{core}}(x_{\text{maj}})\), \(|\phi_{\text{sp}}(x_{\text{min}})|=|\phi_{\text{sp}}(x_{\text{maj}})|\), and \(\sum_{k=1}^{d-2}\phi_{\text{junk}}^{k}(x_{\text{min}})=\sum_{k=1}^{d-2}\phi_{ \text{junk}}^{k}(x_{\text{maj}})\). Let \(P_{\text{min}}\) and \(Q_{\text{min}}\) be distributions over \(\{-1,1\}\) which take value \(1\) with probability \((b(\hat{f}_{\text{erm}}(x_{\text{min}}))+1)/2\) and \((b(\hat{f}_{\text{reg}}(x_{\text{min}}))+1)/2\) respectively, and likewise for \(P_{\text{maj}}\) and \(Q_{\text{maj}}\).7 Then, we have_

Footnote 7: These are the estimated distributions we use to classify the inputs.

\[\text{TVD}(P_{\text{min}},Q_{\text{min}})-\text{TVD}(P_{\text{maj}},Q_{\text{ maj}})=b\min(|\phi_{\text{core}}(x_{\text{maj}})|,|\phi_{\text{sp}}(x_{\text{maj}})|)| |\beta_{\text{erm}}-\beta_{\text{reg}}|>0.\] (3)Proof.: For any \(x\in\mathcal{X}\) define \(P(x)=(b(\hat{f}_{\text{erm}}(x))+1)/2\) and \(Q(x)=(b(\hat{f}_{\text{reg}}(x))+1)/2\). By the assumptions on the features,

\[\text{TVD}(P(x),Q(x)) =|P(x)-Q(x)|\] (4) \[=\frac{b}{2}\big{|}\hat{f}_{\text{erm}}(x)-\hat{f}_{\text{reg}}(x )\big{|}\] (5) \[=\frac{b}{2}\big{|}(\alpha_{\text{erm}}-\alpha_{\text{reg}})\phi_ {\text{core}}(x)-(\beta_{\text{erm}}-\beta_{\text{reg}})\phi_{\text{sp}}(x) \big{|}\] (6) \[=\frac{b}{2}|\beta_{\text{erm}}-\beta_{\text{reg}}||\phi_{\text{ core}}(x)-\phi_{\text{sp}}(x)|.\] (7)

Let \(c=|\phi_{\text{core}}(x_{\text{mij}})|\) and \(d=|\phi_{\text{sp}}(x_{\text{mij}})|\). By definition of the minority and majority groups,

\[|\phi_{\text{core}}(x_{\text{min}})-\phi_{\text{sp}}(x_{\text{min}})|-|\phi_{ \text{core}}(x_{\text{mij}})-\phi_{\text{sp}}(x_{\text{mij}})|=c+d-|c-d|=2\min (c,d)>0.\] (8)

Together with \(b\geq 0\), Equations 7 and 8 show the result.

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_EMPTY:26]

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline ERM & Last-layer retraining & \multicolumn{4}{c}{Worst-group test accuracy} \\ \cline{3-6} class balancing & class balancing & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline ✗ & ✗ & \(87.5_{\pm 0.7}\) & \(45.4_{\pm 2.3}\) & \(54.7_{\pm 5.8}\) & \(64.5_{\pm 0.8}\) \\ ✓ & ✗ & \(88.0_{\pm 0.8}\) & \(41.9_{\pm 1.4}\) & \(57.6_{\pm 4.2}\) & \(64.6_{\pm 1.0}\) \\ ✗ & ✓ & \(92.3_{\pm 0.4}\) & \(71.1_{\pm 2.4}\) & \(78.4_{\pm 2.5}\) & \(64.7_{\pm 1.1}\) \\ ✓ & ✓ & \(92.6_{\pm 0.8}\) & \(73.7_{\pm 2.8}\) & \(80.4_{\pm 0.8}\) & \(64.7_{\pm 1.1}\) \\ \hline \hline \end{tabular}
\end{table}
Table 16: **Table for Figure 4.** We compare different combinations of ERM and last-layer retraining with or without class balancing. Notably, class-balanced last-layer retraining enables nearly the same worst-group accuracy whether the ERM incorporates class-balancing or not. In Figure 4, we only plot the results which use class-balanced last layer retraining (_i.e.,_ the last two rows of the table). We use the entire held-out set for last-layer retraining, and we list the mean and standard deviation over three independent runs.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{WGA/Percent of worst-group data} \\ \cline{2-5}  & Waterbirds & CelebA & CivilComments & MultiNLI \\ \hline Baseline & \(50\) & \(1\) & \(7\) & \(2\) \\ Misclassification & \(90/45\) & \(85/25\) & \(64/51\) & \(65/\ 4\) \\ ES misclassification & \(91/45\) & \(83/\ 7\) & \(68/37\) & \(68/20\) \\ Dropout disagreement & \(91/55\) & \(82/13\) & \(79/13\) & \(66/\ 3\) \\ ES disagreement & \(93/67\) & \(84/\ 2\) & \(79/24\) & \(71/\ 4\) \\ Balanced & \(50\) & \(25\) & \(25\) & \(33\) \\ \hline \hline \end{tabular}
\end{table}
Table 15: **Table for Figure 3.** We list the percentage of the reweighting dataset consisting of worst-group data for each SELF method. “Baseline” represents the percentage of worst-group data in the held-out dataset, while “Balanced” is the percentage of worst-group data necessary to achieve group balance (recall that there may be multiple worst groups; they are described in Section 4.1). We also include the worst-group accuracy (WGA) of each SELF method, to get a sense of whether the amount of worst-group data correlates with performance (in a loose qualitative sense). We list the mean over three independent runs and round to the nearest whole number.

Broader impacts, limitations, and compute

**Broader impacts.** We hope our work contributes to the safe and equitable application of machine learning and motivates further research in ML fairness. A potential negative outcome may arise if practitioners assume their models are bias-free after applying our techniques; while we show reduced dependence on spurious correlations, no method can completely alleviate the issue, and other modes of bias may exist. We encourage practitioners to conduct rigorous examinations of model fairness.

**Limitations.** Our methods take advantage of the specificity of the spurious correlation setting, and therefore would likely underperform on datasets which exhibit a more extreme _complete correlation_ (_i.e.,_ contain zero minority group data) [52; 40]. Furthermore, following previous work in this setting [58; 41; 48; 33; 28], SELF utilizes a small validation dataset with group annotations for model selection. While we show in Appendix B that SELF is robust to using as few as 1% of these annotations, and our last-layer retraining method (Section 4) does not require them, completely removing this assumption is an important direction for future research.

**Compute.** Our experiments were conducted on Nvidia Tesla V100 and A5000 GPUs. We used about \(\$25000\) in compute credits in the course of this research. We believe this paper could be reproduced for under \(\$5000\) in credits, and a majority of this compute would go towards the ablation studies. With that said, our last-layer retraining methods only train the linear classifier, making them cheap and efficient to run even on older hardware.