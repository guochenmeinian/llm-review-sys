# Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms

 Dimitri Meunier

Gatsby Computational Neuroscience Unit

University College London

dimitri.meunier.21@ucl.ac.uk

Equal Contribution.

Zikai Shen

Department of Statistical Science

University College London

zikai.shen.22@ucl.ac.uk

Mattes Mollenhauer

Mernatrix Momentum

mattes.mollenhauer@merantix-momentum.com

Arthur Gretton

Gatsby Computational Neuroscience Unit

University College London

arthur.gretton@gmail.com

&Zhu Li

Department of Mathematics

Imperial College London

zli12@ic.ac.uk

###### Abstract

We study theoretical properties of a broad class of regularized algorithms with vector-valued output. These _spectral algorithms_ include kernel ridge regression, kernel principal component regression and various implementations of gradient descent. Our contributions are twofold. First, we rigorously confirm the so-called _saturation effect_ for ridge regression with vector-valued output by deriving a novel lower bound on learning rates; this bound is shown to be suboptimal when the smoothness of the regression function exceeds a certain level. Second, we present an upper bound on the finite sample risk for general vector-valued spectral algorithms, applicable to both well-specified and misspecified scenarios (where the true regression function lies outside of the hypothesis space), and show that this bound is minimax optimal in various regimes. All of our results explicitly allow the case of infinite-dimensional output variables, proving consistency of recent practical applications.

## 1 Introduction

We investigate a fundamental topic in modern machine learning--the behavior and efficiency of learning algorithms for regression in high-dimensional and potentially infinite-dimensional output spaces \(\mathcal{Y}\). Given two random variables \(X\) and \(Y\), we seek to empirically minimize the squared expected risk

\[\mathcal{E}(F):=\mathbb{E}\left[\|Y-F(X)\|_{\mathcal{Y}}^{2}\right]\] (1)

over functions \(F\) in a _reproducing kernel Hilbert space_ consisting of vector-valued functions from a topological space \(\mathcal{X}\) to a Hilbert space \(\mathcal{Y}\). The study of this setting as an ill-posed statistical inverse problem is well established: see e.g. 46, 6, 53, 3, 5, 17. In this work, we study the setting when \(\mathcal{Y}\) is high- or infinite-dimensional, since it has been less well covered by the literature, yet has many applications in multitask regression [7; 2] and infinite-dimensional learning problems, includingthe conditional mean embedding [20; 21; 41], structured prediction [11; 12], causal inference [43], regression with instrumental and proximal variables [42; 35], the estimation of linear operators and dynamical systems [47; 37; 26; 38; 25], and functional regression [24]. Interestingly, the aforementioned infinite-dimensional applications typically use the classical _ridge regression_ algorithm. Our goal is to motivate the use of alternative learning algorithms in these settings, while providing strong theoretical guarantees.

Classically, the ill-posed problem (1) is solved via regularization strategies, which are often implemented in terms of so-called _spectral filter functions_ in the context of inverse problems in Hilbert spaces [16]. When applied to the learning problem given by (1), these filter functions correspond to learning algorithms including ridge regression, a variety of different implementations of _gradient descent_, _principal component regression_, and other related methods (we refer the reader to 19 and 2 for overviews of the real-valued and vector-valued output variable case, respectively). Algorithms based on spectral filter functions when \(\mathcal{Y}=\mathbb{R}\) are studied extensively, see e.g. [5; 34]. To the best of our knowledge, the detailed behavior of this general class of algorithms has remained unknown when \(\mathcal{Y}\) is a general Hilbert space, with the exception of a few results for special cases in the setting of ridge regression [6; 31].

**Overview of our contributions.** In this manuscript, we aim to theoretically understand vector-valued spectral learning algorithms. The contribution of our work is twofold: (i) we rigorously confirm the _saturation effect_ of ridge regression for general Hilbert spaces \(\mathcal{Y}\) (see paragraph below) in the context of lower bounds on rates for the learning problem (1) and (ii) we cover a gap in the existing literature by providing _upper rates for general spectral algorithms_ in high- and infinite-dimensional spaces. Our results explicitly allow the _misspecified learning case_ in which the true regression function is not contained in the hypothesis space. We base our analysis on the concept of _vector-valued interpolation spaces_ introduced by [30; 31]. The interpolation space norms measure the smoothness of the true regression function, replacing typical source conditions found in the literature which only cover the well-specified case. _To the best of our knowledge, these are the first bounds covering this general setting for vector-valued spectral algorithms._

**Saturation effect of ridge regression.** The widely-used ridge regression algorithm is known to exhibit the so-called saturation effect: it fails to exploit additional smoothness in the target function beyond a certain threshold. This effect has been thoroughly investigated in the context of Tikhonov regularization in inverse problems [16; Chapter 5], but is generally reflected only in upper rates in the learning literature, see e.g. [34; 5]. Interestingly, existing lower bounds [6; 5; 31] are usually formulated in a more general setting and do not reflect this saturation effect, leaving a gap between upper and lower rates. We leverage the bias-variance decomposition paradigm to lower bound the learning risk of kernel ridge regression with vector-valued output, in order to close this gap.

**Learning rates of vector-valued spectral algorithms.** Motivated by the fact that the saturation effect is technically unavoidable with vector-valued ridge regression, we proceed to study the generalization error of popular alternative learning algorithms. In particular, we provide upper rates in the vector-valued setting consistent with the known behavior of spectral algorithms in the real-valued learning setting, based on their so-called _qualification property_[5; 34]. In particular, we confirm that a saturation effect can be bypassed in high and infinite dimensions by algorithms such as principal component regression and gradient descent, allowing for a better sample complexity for high-smoothness problems. Furthermore, we study the misspecified setting and show that upper rates for spectral algorithms match the state-of-the-art upper rates for misspecified vector-valued ridge regression recently obtained by [31]. Those rates are optimal for a wide variety of settings. Moreover, we argue that applications of vector-valued spectral algorithms are easy to implement by making use of an extended _representer theorem_ based on [2], allowing for the numerical evaluation based on empirical data--even in the infinite-dimensional case.

**Related Work.** The saturation effect of regularization techniques in deterministic inverse problems is well-known. For example, [40; 36; 22] study the saturation effect for Tikhonov regularization and general spectral algorithms. In the kernel statistical learning framework, the general phenomenon of saturation is discussed by e.g. [3; 19]. Recent work by [29] investigates saturation effect in the learning context by providing a lower bound on the learning rate. To the best of our knowledge, however, all studies in the statistical learning context focus on the case when \(Y\) is real-valued. General upper bounds of kernel ridge regression with real-valued or finite-dimensional \(Y\) have been extensively studied in the literature (see e.g., [6; 50; 8; 17]), where minimax optimal learning rates are derived. Recent work [30; 31] studies the infinite-dimensional output space setting with Tikhonov regularization and obtains analogous minimax optimal learning rates. [23] later study a setting where both the input and output space is the infinite dimensional Sobolev RKHS and establish the minimax optimal rate. For kernel learning with spectral algorithms, existing work (see e.g., [3; 5; 32; 34; 54; 28]) focuses on real-valued output space setting and obtains optimal upper learning rates depending on the qualification number of the spectral algorithms, where only [54; 28] consider the misspecified learning scenario where the target regression function does not lie in the hypothesis space. For vector-valued output spaces, [27] considers learning with vector-valued random features. However, general investigations of spectral algorithms for vector-valued output spaces are absent in the literature.

**Structure of this paper.** This paper is structured as follows. In Section 2, we introduce mathematical preliminaries related to reproducing kernel Hilbert spaces, vector-valued regression as well as the concept of vector-valued interpolation spaces. Section 3 contains a brief review the so-called saturation effect and a corresponding novel lower bound for vector-valued kernel ridge regression. In Section 4, we investigate general spectral learning algorithms in the context of vector-valued interpolations spaces and provide our main result: upper learning rates for this setting.

## 2 Background and Preliminaries

Throughout the paper, we consider a random variable \(X\) (the covariate) defined on a second countable locally compact Hausdorff space2\(\mathcal{X}\) endowed with its Borel \(\sigma\)-field \(\mathcal{F}_{\mathcal{X}}\), and the random variable \(Y\) (the output) defined on a potentially infinite dimensional separable real Hilbert space \((\mathcal{Y},\langle\cdot,\cdot\rangle_{\mathcal{Y}})\) endowed with its Borel \(\sigma\)-field \(\mathcal{F}_{\mathcal{Y}}\). We let \((\Omega,\mathcal{F},\mathbb{P})\) be the underlying probability space with expectation operator \(\mathbb{E}\). Let \(P\) be the push-forward of \(\mathbb{P}\) under \((X,Y)\) and \(\pi\) and \(\nu\) be the marginal distributions on \(\mathcal{X}\) and \(\mathcal{Y}\), respectively; i.e., \(X\sim\pi\) and \(Y\sim\nu\). We use the Markov kernel \(p:\mathcal{X}\times\mathcal{F}_{\mathcal{Y}}\to\mathbb{R}_{+}\) to express the distribution of \(Y\) conditioned on \(X\) as

Footnote 2: Under additional technical assumptions, the results in this paper can also be formulated when \(\mathcal{X}\) is a more general topological space. However, some properties of kernels defined on \(\mathcal{X}\) such as the so-called \(c_{0}\)_-universality_[10] simplify the exposition when \(\mathcal{X}\) is a second countable locally compact Hausdorff space.

\[\mathbb{P}[Y\in A|X=x]=\int_{A}p(x,dy),\]

for all \(x\in\mathcal{X}\) and events \(A\in\mathcal{F}_{\mathcal{Y}}\), see e.g. [15]. We introduce some notation related to linear operators on Hilbert spaces and vector-valued integration; formal definitions can be found in Appendix A for completeness, or we refer the reader to [52; 14]. The spaces of Bochner square-integrable functions with respect to \(\pi\) and taking values in \(\mathcal{Y}\) are written as \(L_{2}(\mathcal{X},\mathcal{F}_{\mathcal{X}},\pi;\mathcal{Y})\), abbreviated as \(L_{2}(\pi;\mathcal{Y})\). We obtain the classical Lebesgue spaces as \(L_{2}(\pi):=L_{2}(\pi;\mathbb{R})\). Throughout the paper, we write \([F]\) or more explicitly \([F]_{\pi}\) for the \(\pi\)-equivalence class of (potentially pointwise defined) measurable functions from \(\mathcal{X}\) to \(\mathcal{Y}\), which we naturally interpret as elements in \(L_{2}(\pi;\mathcal{Y})\) whenever they are square-integrable. Let \(H\) be a separable real Hilbert space with inner product \(\langle\cdot,\cdot\rangle_{H}\). We write \(\mathcal{L}(H,H^{\prime})\) as the Banach space of bounded linear operators from \(H\) to another Hilbert space \(H^{\prime}\), equipped with the operator norm \(\|\cdot\|_{H\to H^{\prime}}\). When \(H=H^{\prime}\), we simply write \(\mathcal{L}(H)\) instead. We write \(S_{2}(H,H^{\prime})\) as the Hilbert space of Hilbert-Schmidt operators from \(H\) to \(H^{\prime}\) and \(S_{1}(H,H^{\prime})\) as the Banach space of trace class operators (see Appendix A for a complete definition). For two Hilbert spaces \(H,H^{\prime}\), we say that \(H\) is (continuously) embedded in \(H^{\prime}\) and denote it as \(H\to H^{\prime}\) if \(H\) can be interpreted as a vector subspace of \(H^{\prime}\) and the inclusion operator \(i:H\to H^{\prime}\) performing the change of norms with \(ix=x\) for \(x\in H\) is continuous; and we say that \(H\) is isometrically isomorphic to \(H^{\prime}\) and denote it as \(H\simeq H^{\prime}\) if there is a linear isomorphism between \(H\) and \(H^{\prime}\) which is an isometry.

**Tensor Product of Hilbert Spaces:** Denote \(H\otimes H^{\prime}\) the tensor product of Hilbert spaces \(H\), \(H^{\prime}\). The element \(x\otimes x^{\prime}\in H\otimes H^{\prime}\) is treated as the linear rank-one operator \(x\otimes x^{\prime}:H^{\prime}\to H\) defined by \(y^{\prime}\to\langle y^{\prime},x^{\prime}\rangle_{H^{\prime}}x\) for \(y^{\prime}\in H^{\prime}\). Based on this identification, the tensor product space \(H\otimes H^{\prime}\) is isometrically isomorphic to the space of Hilbert-Schmidt operators from \(H^{\prime}\) to \(H\), i.e., \(H\otimes H^{\prime}\simeq S_{2}(H^{\prime},H)\). We will hereafter not make the distinction between these two spaces, and treat them as being identical.

**Remark 1** (1, Theorem 12.6.1).: _Consider the Bochner space \(L_{2}(\pi;H)\) where \(H\) is a separable Hilbert space. One can show that \(L_{2}(\pi;H)\) is isometrically identified with the tensor product space \(H\otimes L_{2}(\pi)\), and we denote as \(\Psi\) the isometric isomorphism between the two spaces. See Appendix A for more details on tensor product spaces and the explicit definition of \(\Psi\)._

**Scalar-valued Reproducing Kernel Hilbert Space (RKHS).** We let \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be a symmetric and positive definite kernel function and \(\mathcal{H}\) be a vector space of functions from \(\mathcal{X}\) to \(\mathbb{R}\), endowed with a Hilbert space structure via an inner product \(\left\langle\cdot,\cdot\right\rangle_{\mathcal{H}}\). We say that \(k\) is a reproducing kernel of \(\mathcal{H}\) if and only if for all \(x\in\mathcal{X}\) we have \(k(\cdot,x)\in\mathcal{H}\) and for all \(x\in\mathcal{X}\) and \(f\in\mathcal{H}\), we have \(f(x)=\left\langle f,k(x,\cdot)\right\rangle_{\mathcal{H}}\). A space \(\mathcal{H}\) which possesses a reproducing kernel is called a reproducing kernel Hilbert space (RKHS; see e.g. 4). We denote the canonical feature map of \(\mathcal{H}\) as \(\phi(x)=k(\cdot,x)\).

We require some technical assumptions on the previously defined RKHS and kernel, which we assume to be satisfied throughout the text:

1. \(\mathcal{H}\) is separable: this is satisfied if \(k\) is continuous, given that \(\mathcal{X}\) is separable3; Footnote 3: This follows from [49, Lemma 4.33]. Note that the Lemma requires separability of \(\mathcal{X}\), which is satisfied since we assume that \(\mathcal{X}\) is a second countable locally compact Hausdorff space.
2. \(k(\cdot,x)\) is measurable for \(\pi\)-almost all \(x\in\mathcal{X}\);
3. \(k(x,x)\leq\kappa^{2}\) for \(\pi\)-almost all \(x\in\mathcal{X}\).

The above assumptions are not restrictive in practice, as well-known kernels such as the Gaussian, Laplace, and Matern kernels satisfy them on \(\mathcal{X}\subseteq\mathbb{R}^{d}\)[48]. We now introduce some facts about the interplay between \(\mathcal{H}\) and \(L_{2}(\pi),\) which has been extensively studied by [44, 45], [13] and [51]. We first define the (not necessarily injective) embedding \(I_{\pi}:\mathcal{H}\to L_{2}(\pi)\), mapping a function \(f\in\mathcal{H}\) to its \(\pi\)-equivalence class \(\left[f\right]\). The embedding is a well-defined compact operator as long as its Hilbert-Schmidt norm is finite. In fact, this requirement is satisfied since its Hilbert-Schmidt norm can be computed as [51, Lemma 2.2 & 2.3]\(\|I_{\pi}\|_{S_{2}(\mathcal{H},L_{2}(\pi))}=\|k\|_{L_{2}(\pi)}\leq\kappa\). The adjoint operator \(S_{\pi}:=I_{\pi}^{\star}:L_{2}(\pi)\to\mathcal{H}\) is an integral operator with respect to the kernel \(k\), i.e. for \(f\in L_{2}(\pi)\) and \(x\in\mathcal{X}\) we have [49, Theorem 4.27]

\[\left(S_{\pi}f\right)(x)=\int_{\mathcal{X}}k\left(x,x^{\prime}\right)f\left(x ^{\prime}\right)\mathrm{d}\pi\left(x^{\prime}\right).\]

Next, we define the self-adjoint, positive semi-definite and trace class integral operators

\[L_{X}:=I_{\pi}S_{\pi}:L_{2}(\pi)\to L_{2}(\pi)\quad\text{ and }\quad C_{X}:=S_{\pi}I_{ \pi}:\mathcal{H}\to\mathcal{H}.\]

**Vector-valued Reproducing Kernel Hilbert Space (vRKHS).** Let \(K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})\) be an operator valued positive-semidefinite (psd) kernel. Fix \(K\), \(x\in\mathcal{X}\), and \(h\in\mathcal{Y}\), then \(\left(K_{x}h\right)(\cdot):=K(\cdot,x)h\) defines a function from \(\mathcal{X}\) to \(\mathcal{Y}\). The completion of

\[\mathcal{G}_{\text{pre}}:=\operatorname{span}\left\{K_{x}h\mid x\in\mathcal{X },h\in\mathcal{Y}\right\}\]

with inner product on \(\mathcal{G}_{\text{pre}}\) defined on the elementary elements as \(\left(K_{x}h,K_{x^{\prime}}h^{\prime}\right)_{\mathcal{G}}:=\left\langle h,K \left(x,x^{\prime}\right)h^{\prime}\right\rangle_{\mathcal{Y}}\), defines a vRKHS denoted as \(\mathcal{G}\). For a more complete overview of the vector-valued reproducing kernel Hilbert space, we refer the reader to [9, 10] and [31, Section 2]. In the following, we will denote \(\mathcal{G}\) as the vRKHS induced by the kernel \(K:\mathcal{X}\times\mathcal{X}\to\mathcal{L}(\mathcal{Y})\) with

\[K(x,x^{\prime}):=k(x,x^{\prime})\operatorname{Id}_{\mathcal{Y}},\quad x,x^{ \prime}\in\mathcal{X}.\] (2)

We emphasize that this family of kernels is the de-facto standard for high- and infinite-dimensional applications [20, 21, 41, 11, 12, 42, 35, 43, 37, 26, 38, 25, 24] due to the crucial _representement theorem_ which gives a closed form solution for the ridge regression problem based on the data. We generalize this representer theorem to cover the general spectral algorithm case in Proposition 1.

**Remark 2** (General multiplicative kernel).: _Without loss of generality, we provide our results for the vRKHS \(\mathcal{G}\) induced by the operator-valued kernel given by \(K(x,x^{\prime})=k(x,x^{\prime})\operatorname{Id}_{\mathcal{Y}}\). However, with suitably adjusted constants in the assumptions, our results transfer directly to the more general vRKHS \(\widetilde{\mathcal{G}}\) induced by the more general operator-valued kernel_

\[\widetilde{K}(x,x^{\prime}):=k(x,x^{\prime})T\]

_where \(T:\mathcal{Y}\to\mathcal{Y}\) is any positive-semidefinite self-adjoint operator. The precise characterization of the adjusted constants is given by [31, Section 4.1]._

An important property of \(\mathcal{G}\) is that it is isometrically isomorphic to the space of Hilbert-Schmidt operators between \(\mathcal{H}\) and \(\mathcal{Y}\)[31, Corollary 1]. Similarly to the scalar case we can map every element in \(\mathcal{G}\) into its \(\pi\)-equivalence class in \(L_{2}(\pi;\mathcal{Y})\) and we use the shorthand notation \([F]=[F]_{\pi}\) (see Definition 6 in Appendix A for more details).

**Theorem 1** (vRKHS isomorphism).: _For every function \(F\in\mathcal{G}\) there exists a unique operator \(C\in S_{2}(\mathcal{H},\mathcal{Y})\) such that \(F(\cdot)=C\phi(\cdot)\in\mathcal{Y}\) with \(\|C\|_{S_{2}(\mathcal{H},\mathcal{Y})}=\|F\|_{\mathcal{G}}\) and vice versa. Hence \(\mathcal{G}\simeq S_{2}(\mathcal{H},\mathcal{Y})\) and we denote the isometric isomorphism between \(S_{2}(\mathcal{H},\mathcal{Y})\) and \(\mathcal{G}\) as \(\bar{\Psi}\). It follows that \(\mathcal{G}\) can be written as \(\mathcal{G}=\{F:\mathcal{X}\rightarrow\mathcal{Y}\mid F=C\phi(\cdot),\,C\in S _{2}(\mathcal{H},\mathcal{Y})\}\)._

### Vector-valued Regression

We briefly recall the basic setup of regularized least-squares regression with Hilbert space-valued random variables. The squared expected risk for vector-valued regression is

\[\mathcal{E}(F):=\mathbb{E}\left[\|Y-F(X)\|_{\mathcal{Y}}^{2}\right]=\int_{ \mathcal{X}\times\mathcal{Y}}\|y-F(x)\|_{\mathcal{Y}}^{2}p(x,dy)\pi(dx),\] (3)

for measurable functions \(F:\mathcal{X}\rightarrow\mathcal{Y}\). The analytical minimizer of the risk over measurable functions is the _regression function_ or the _conditional mean function_\(F_{\star}\in L_{2}(\pi;\mathcal{Y})\) given by

\[F_{\star}(x):=\mathbb{E}[Y\mid X=x]=\int_{\mathcal{Y}}y\,p(x,dy),\quad x\in \mathcal{X}.\]

Throughout the paper, we assume that \(\mathbb{E}[\|Y\|_{\mathcal{Y}}^{2}]<+\infty\), i.e., the random variable \(Y\) is square-integrable. Note that this implies \(F_{\star}\in L_{2}(\pi;\mathcal{Y})\). Our focus in this work is to approximate \(F_{\star}\) with kernel-based regularized least-squares algorithms, where we pay special attention to the case when \(\mathcal{Y}\) is of high or infinite dimension. We pick \(\mathcal{G}\) as a hypothesis space of functions in which to estimate \(F_{\star}\). Note that by Theorem 1, minimizing the functional \(\mathcal{E}\) on \(\mathcal{G}\) is equivalent to minimizing the following functional on \(S_{2}(\mathcal{H},\mathcal{Y})\),

\[\bar{\mathcal{E}}(C):=\mathbb{E}\big{[}\big{|}Y-C\phi(X)\big{|}_{\mathcal{Y}}^ {2}\big{]}.\] (4)

It is shown in [38, Proposition 3.5 and Section 3.4] that the optimality condition can be written as

\[C_{YX}=C_{\star}C_{X},\qquad C_{\star}\in S_{2}(\mathcal{H},\mathcal{Y}),\] (5)

where \(C_{YX}:=\mathbb{E}[Y\otimes\phi(X)]\) is the cross-covariance operator. As discussed in full detail by [38], the problem (5) can be formulated as a potentially ill-posed inverse problem on the space of Hilbert-Schmidt operators. As such, a regularization is required; we introduce regularized solutions of this problem in Section 4 through the classical concept of spectral filter functions.

### Vector-valued Interpolation Space and Source Condition

We now introduce the background required in order to characterize the smoothness of the target function \(F_{\star}\), both in the well-specified setting (\(F_{\star}\in\mathcal{G}\)) and in the misspecified setting (\(F_{\star}\notin\mathcal{G}\)). We review the results of [51] and [17] in constructing scalar-valued interpolation spaces, and [30] in defining vector-valued interpolation spaces.

**Real-valued Interpolation Space:** By the spectral theorem for self-adjoint compact operators, there exists an at most countable index set \(I\), a non-increasing sequence \((\mu_{i})_{i\in I}>0\), and a family \((e_{i})_{i\in I}\in\mathcal{H}\), such that \(\big{(}\big{[}e_{i}\big{]}\big{)}_{i\in I}\)4 is an orthonormal basis (ONB) of \(\overline{\text{ran }I_{\pi}}\subseteq L_{2}(\pi)\) and \((\mu_{i}^{1/2}e_{i})_{i\in I}\) is an ONB of \((\ker I_{\pi})^{\perp}\subseteq\mathcal{H}\), and we have

Footnote 4: We recall that the bracket \([\cdot]\) denotes the embedding that maps \(f\) to its equivalence class \(I_{\pi}(f)\in L_{2}(\pi)\).

\[L_{X}=\sum_{i\in I}\mu_{i}\{\cdot,[e_{i}]\}_{L_{2}(\pi)}[e_{i}],\qquad C_{X}= \sum_{i\in I}\mu_{i}\{\cdot,\mu_{i}^{\frac{1}{2}}e_{i}\}_{\mathcal{H}}\mu_{i} ^{\frac{1}{2}}e_{i}\] (6)

For \(\alpha\geq 0\), the \(\alpha\)-interpolation space [51] is defined by

\[[\mathcal{H}]^{\alpha}:=\left\{\sum_{i\in I}a_{i}\mu_{i}^{\alpha/2}\,[e_{i}]: \left(a_{i}\right)_{i\in I}\in\ell_{2}(I)\right\}\subseteq L_{2}(\pi),\]

equipped with the inner product

\[\left\{\sum_{i\in I}a_{i}(\mu_{i}^{\alpha/2}[e_{i}]),\sum_{i\in I}b_{i}(\mu_{ i}^{\alpha/2}[e_{i}])\right\}_{[\mathcal{H}]^{\alpha}}=\sum_{i\in I}a_{i}b_{i},\]for \(\left(a_{i}\right)_{i\in I},\left(b_{i}\right)_{i\in I}\in\ell_{2}(I)\). The \(\alpha\)-interpolation space defines a Hilbert space. Moreover, \(\left(\mu_{i}^{\alpha/2}\left[e_{i}\right]\right)_{i\in I}\) forms an ONB of \([\mathcal{H}]^{\alpha}\) and consequently \([\mathcal{H}]^{\alpha}\) is a separable Hilbert space. In the following, we use the abbreviation \(\|\cdot\|_{\alpha}:=\|\cdot\|_{[\mathcal{H}]^{\alpha}}\).

**Vector-valued Interpolation Space:** Introduced in [30], vector-valued interpolation spaces generalize the notion of scalar-valued interpolation spaces to vRKHS with a kernel of the form (2).

**Definition 1** (Vector-valued interpolation space).: _Let \(k\) be a real-valued kernel with associated RKHS \(\mathcal{H}\) and let \([\mathcal{H}]^{\alpha}\) be the real-valued interpolation space associated to \(\mathcal{H}\) with some \(\alpha\geq 0\). The vector-valued interpolation space \([\mathcal{G}]^{\alpha}\) is defined as (refer to Remark 1 for the definition of \(\Psi\))_

\[[\mathcal{G}]^{\alpha}:=\Psi\left(S_{2}([\mathcal{H}]^{\alpha},\mathcal{Y}) \right)=\{F\mid F=\Psi(C),\ C\in S_{2}([\mathcal{H}]^{\alpha},\mathcal{Y})\}.\]

_The space \([\mathcal{G}]^{\alpha}\) is a Hilbert space equipped with the inner product_

\[\left\langle F,G\right\rangle_{\alpha}:=\left\langle C,L\right\rangle_{S_{2}([ \mathcal{H}]^{\alpha},\mathcal{Y})}\qquad(F,G\in[\mathcal{G}]^{\alpha}),\]

_where \(C=\Psi^{-1}(F),\,L=\Psi^{-1}(G)\). For \(\alpha=0\), we retrieve \(\|F\|_{0}=\|F\|_{L_{2}(\pi;\mathcal{Y})}=\|C\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}\)._

**Remark 3** (Interpolation space inclusions).: _Note that we have \(F_{\star}\in L_{2}(\pi;\mathcal{Y})\) since \(Y\in L_{2}(\mathbb{P};\mathcal{Y})\) by assumption. Furthermore, for \(0<\beta<\alpha\), [17, Eq. (7)] imply the inclusions_

\[[\mathcal{G}]^{\alpha}\hookrightarrow[\mathcal{G}]^{\beta}\hookrightarrow[ \mathcal{G}]^{0}\subseteq L_{2}(\pi;\mathcal{Y}).\]

_Under assumptions 1 to 3 and with \(\mathcal{X}\) being a second-countable locally compact Hausdorff space, \([\mathcal{G}]^{0}=L_{2}(\pi;\mathcal{Y})\) if and only if \(\mathcal{H}\) is dense in the space of continuous functions vanishing at infinity, equipped with the uniform norm [31, Remark 4]._

**Remark 4** (Well-specified versus misspecified setting).: _We say that we are in the well-specified setting if \(F_{\star}\in[\mathcal{G}]^{1}\). In this case, there exists \(\bar{F}\in\mathcal{G}\) such that \(F_{\star}=\bar{F}\,\,\pi-\)almost surely and \(\|F_{\star}\|_{1}=\|\bar{F}\|_{\mathcal{G}}\), i.e. \(F_{\star}\) admits a representer in \(\mathcal{G}\) (see Remark 5 in Appendix A). When \(F_{\star}\in[\mathcal{G}]^{\beta}\) for \(\beta<1\), \(F_{\star}\) may not admit such a representation and we are in the misspecified setting, as \([\mathcal{G}]^{1}\subseteq[\mathcal{G}]^{\beta}\)._

Definition 1 and Remarks 3 and 4 motivate the use of following assumption on the smoothness of the target function: there exists \(\beta>0\) and a constant \(B\geq 0\) such that \(F_{\star}\in[\mathcal{G}]^{\beta}\) and

\[\|F_{\star}\|_{\beta}\leq B.\] (SRC)

We let \(C_{\star}:=\Psi^{-1}(F_{\star})\in S_{2}([\mathcal{H}]^{\beta},\mathcal{Y})\). (SRC) directly generalizes the notion of a so-called Holder-type source condition in the learning literature [6, 17, 32, 34] and allows to characterize the misspecified learning scenario.

### Further Assumptions

In addition to (SRC), we require standard assumptions to obtain the precise learning rates for kernel learning algorithms. We list them below. For constants \(D_{2}>0\) and \(p\in(0,1]\) and for all \(i\in I\),

\[\mu_{i}\leq D_{2}i^{-1/p}.\] (EVD)

For constants \(D_{1},D_{2}>0\) and \(p\in(0,1)\) and for all \(i\in I\),

\[D_{1}i^{-\frac{i}{p}}\leq\mu_{i}\leq D_{2}i^{-1/p}.\] (EVD+)

(EVD) and (EVD+) are standard assumptions on the _eigenvalue decay_ of the integral operator: they describe the interplay between the marginal distribution \(\pi\) and the RKHS \(\mathcal{H}\) (see more details in 6, 17). (EVD+) is needed in order to establish lower bounds on the excess risk. Note that we have excluded the value \(p=1\) from (EVD+); indeed, \(p=1\) is incompatible with the assumption of a bounded kernel, a fact missed by previous works and of independent interest (see Appendix, Remark 7).

For \(\alpha\in[p,1]\), the inclusion \(I_{\pi}^{\alpha,\infty}:[\mathcal{H}]^{\alpha}\hookrightarrow L_{\infty}(\pi)\) is continuous, and \(\exists A>0\) such that

\[\|I_{\pi}^{\alpha,\infty}\|_{[\mathcal{H}]^{\alpha}\hookrightarrow L_{\infty}( \pi)}\leq A.\] (EMB)

Property (EMB) is referred to as the _embedding property_ in [17]. It can be shown that it holds if and only if there exists a constant \(A\geq 0\) with \(\sum_{i\in I}\mu_{i}^{\alpha}e_{i}^{2}(x)\leq A^{2}\) for \(\pi\)-almost all \(x\in\mathcal{X}\)[17, Theorem 9]. Since we assume \(k\) to be bounded, the embedding property always hold true when \(\alpha=1\)Furthermore, (EMB) implies a polynomial eigenvalue decay of order \(1/\alpha\), which is why we take \(\alpha\geq p\). (EMB) is not needed when we deal with the well-specified setting, but is crucial to bound the excess risk in the misspecified setting.

Finally, we assume that there are constants \(\sigma,R>0\) such that

\[\int_{\mathcal{Y}}\left\|y-F_{*}(x)\right\|_{\mathcal{Y}}^{q}p(x,dy)\leq\frac{ 1}{2}q!\sigma^{2}R^{q-2},\] (MOM)

is satisfied for \(\pi\)-almost all \(x\in\mathcal{X}\) and all \(q\geq 2\). The (MOM) condition on the Markov kernel \(p(x,dy)\) is a _Bernstein moment condition_ used to control the noise of the observations (see 6, 17 for more details). If \(Y\) is almost surely bounded, for example \(\left\|Y\right\|_{\mathcal{Y}}\leq Y_{\infty}\) almost surely, then (MOM) is satisfied with \(\sigma=R=2Y_{\infty}\). It is possible to prove that the Bernstein condition is equivalent to sub-exponentiality, see [38, Remark 4.9].

## 3 Saturation Effect of Kernel Ridge Regression

The most established way of learning \(F_{*}\) is by kernel ridge regression (KRR), which can be formulated as the following optimization problem: given a dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{n}\) independently and identically sampled from the joint distribution of \(X\) and \(Y\),

\[\hat{F}_{\lambda}:=\operatorname*{arg\,min}_{\hat{F}\in\mathcal{G}}\frac{1}{n }\sum_{i=1}^{n}\left\|y_{i}-F(x_{i})\right\|_{\mathcal{Y}}^{2}+\lambda\|F\|_ {\mathcal{G}}^{2},\] (7)

where \(\lambda>0\) is the regularization parameter. The generalization error of vector-valued KRR is expressed as \(\hat{F}_{\lambda}-F_{*}\), and controlled in different norms: see [31] for an extensive study. We recall here a simplified special case of the key results obtained in this work. In the next Theorem, \(\lesssim,\gtrsim\) are inequality up to positive multiplicative constants that are independent of \(n\).

**Theorem 2** (Upper and lower bounds for KRR in the well-specified regime).: _Let \(\hat{F}_{\lambda}\) be the KRR estimator from (7). Furthermore, let the conditions (EVD+), (SRC) and (MOM) be satisfied for some \(0<p\leq 1\) and \(\beta\geq 1\). Then, with high probability we have_

\[\left\|\left[\hat{F}_{\lambda_{n}}\right]-F_{*}\right\|_{L_{2}(\pi;\mathcal{Y })}^{2}\lesssim n^{-\frac{\min(\lambda,2)}{\min(\lambda,2)+p}}\quad\text{for a choice }\lambda_{n}=\Theta\left(n^{-\frac{1}{\beta+p}}\right),\]

_and furthermore for all learning methods (i.e., measurable maps) of the form \(D\to\hat{F}_{D}\),_

\[\left\|\left[\hat{F}_{D}\right]-F_{*}\right\|_{L_{2}(\pi;\mathcal{Y})}^{2} \gtrsim n^{-\frac{\beta}{\beta+p}}.\]

Theorem 2 shows the minimax optimal learning rate for vector-valued KRR for \(\beta\in[1,2]\). However, when \(\beta>2\), the obtained upper bound saturates at \(n^{-\frac{2}{2+p}}\), creating a gap with the lower bound. This phenomenon is referred to as the saturation effect of Tikhonov regularization, and has been well investigated in deterministic inverse problems [40]. In the case where \(\mathcal{Y}\) is real-valued, [29] prove that the saturation effect cannot be avoided with Tikhonov regularization. Below, we give a similar but generalized bound on lower rates for the case that \(\mathcal{Y}\) is a Hilbert space. For this result only, we assume that \(\mathcal{X}\) is a compact subset of \(\mathbb{R}^{d}\). We give the proof in Appendix B.

**Theorem 3** (Saturation of KRR).: _Let \(\mathcal{X}\) be a compact subset of \(\mathbb{R}^{d}\). Let \(\lambda=\lambda(n)\) be an arbitrary choice of regularization parameter satisfying \(\lambda(n)\to 0\) as \(n\to+\infty\) and let \(\hat{F}_{\lambda}\) be the KRR estimator from (7). We assume that the noise is non-zero and bounded below, i.e. there exists \(\sigma>0\), such that_

\[\int_{\mathcal{Y}}\left\|y-F_{*}(x)\right\|_{\mathcal{Y}}^{2}p(x,dy)\geq\sigma ^{2},\]

_is satisfied for \(\pi\)-almost all \(x\in\mathcal{X}\). We assume in addition and for this result only that \(k\) is Holder continuous (see Definition 11 in the appendix), i.e., \(k\in C^{\theta}(\mathcal{X}\times\mathcal{X})\) for \(\theta\in(0,1]\). Suppose that Assumptions (EVD+) and (SRC) hold with \(p\in(0,1)\) and \(\beta\geq 2\). For \(\tau\geq 0\), for sufficiently large \(n>0\), where the hidden index bound depends on \(\tau\), with probability greater than \(1-e^{-\tau}\), there exists some constant \(c_{\tau}>0\) such that_

\[\mathbb{E}\left[\left\|\left[\hat{F}_{\lambda}\right]-F_{*}\right\|_{L_{2}(\pi; \mathcal{Y})}^{2}\left|x_{1},\ldots,x_{n}\right|\geq c_{\tau}n^{-\frac{2}{2+p}}.\]The assumption that \(k\) is Holder continuous is crucial in lower bounding the variance with a covering number argument. Kernels satisfying this assumption include Gaussian kernels, Laplace kernels and Matern kernels. Theorem 3 clearly demonstrates that the learning rate from vector-valued KRR cannot reach the information theoretic lower rate given in Theorem 2.

As discussed above, [29] propose a similar lower bound in the real-valued case, and we now highlight two fundamental differences with [29] in the proof. First, while both works adopt the same bias-variance decomposition, we need to lower bound the bias and the variance term with infinite-dimensional output in our setting. Second, we adopt a different and simpler approach in proving the lower bound, since there are a number of issues with the proof of [29], both in the treatment of the bias and of the variance. For a detailed comparison with the earlier work, and an explanation of the differences in our approach, please refer to Remark 6 in the Appendix.

## 4 Consistency and optimal rates for general spectral algorithms

**Regularized population solution**: Our goal is to regularize (5) in such a way that we get a unique and well-defined solution that provides a good approximation to \(F_{*}\). We first recall the concept of a filter function (i.e., a function on an interval which is applied on self-adjoint operators to each individual eigenvalue via the spectral calculus, see 16), that will allow to define a regularization strategy. One may think of the following definition as a class of functions approximating the inversion map \(x\mapsto 1/x\) while still being defined for \(x=0\) in a reasonable way. We use the definition given by [34], but equivalent definitions can be found throughout the literature.

**Definition 2** (Filter function).: _Let \(\Lambda\subseteq\mathbb{R}^{+}\). A family of functions \(g_{\lambda}:[0,\infty)\to[0,\infty)\) indexed by \(\lambda\in\Lambda\) is called a filter with qualification \(\rho\geq 0\) if it satisfies the following two conditions:_

1. _There exists a positive constant_ \(E\) _such that, for all_ \(\lambda\in\Lambda\)__ \[\sup_{\alpha\in[0,1]}\sup_{x\in[0,\kappa^{2}]}\lambda^{1-\alpha}x^{\alpha}g_{ \lambda}(x)\leq E\] (8)
2. _There exists a positive constant_ \(\omega_{\rho}<\infty\) _such that_ \[\sup_{\alpha\in[0,\rho]}\sup_{\lambda\in\Lambda}\sup_{x\in[0,\kappa^{2}]}|r_{ \lambda}(x)|x^{\alpha}\lambda^{-\alpha}\leq\omega_{\rho},\qquad\text{ with }\qquad r_{\lambda}(x):=1-g_{\lambda}(x)x.\] (9)

Below, we give some standard examples which are discussed by e.g. [19, 5] in the context of kernel regression with scalar output variables, and in [2] for the vector-valued case. A variety of additional algorithms can be expressed in terms of a filter function.

1. _Ridge regression._ From the Tikhonov filter function \(g_{\lambda}(x)=(x+\lambda)^{-1}\), we obtain the known ridge regression algorithm. In this case, we have \(E=\rho=\omega_{\rho}=1\).

2. _Gradient Descent._ From the Landweber iteration filter function given by

\[g_{k}(x):=\tau\sum_{i=0}^{k-1}\left(1-\tau x\right)^{i}\text{ for }k:=1/ \lambda,k\in\mathbb{N}\]

we obtain the gradient descent scheme with constant step size \(\tau>0\), which corresponds to the population gradient iteration given by \(F_{k+1}:=F_{k}-\tau\nabla\mathcal{E}(F_{k})\) for \(k\in\mathbb{N}\). In this case, we have \(E=1\) and arbitrary qualification with \(\omega_{\rho}=1\) whenever \(0<\rho\leq 1\) and \(\omega_{\rho}=\rho^{\rho}\) otherwise. Gradient schemes with more complex update rules can be expressed in terms of filter functions as well [39, 32, 34].

3. _Kernel principal component regression._ The truncation filter function \(g_{\lambda}(x)=x^{-1}\mathds{1}[x\geq\lambda]\) yields kernel principal component regression, corresponding to a hard thresholding of eigenvalues at a truncation level \(\lambda\). In this case we have \(E=\omega_{\rho}=1\) for arbitrary qualification \(\rho\).

**Population solution**: Given a filter function \(g_{\lambda}\), we call \(g_{\lambda}(C_{X})\)5 the regularized inverse of \(C_{X}\). We may think of the regularized inverse as approximating the _pseudoinverse_ of \(C_{X}\) (see e.g. [16]) when \(\lambda\to 0\). We define the regularized population solution to (4) as

Footnote 5: \(g_{\lambda}(C_{X})\) is defined with the rules of spectral calculus, see Definition 9 in the Appendix.

\[C_{\lambda}:=C_{YX}g_{\lambda}(C_{X})\in S_{2}(\mathcal{H},\mathcal{Y}), \qquad F_{\lambda}(\cdot):=C_{\lambda}\phi(\cdot)\in\mathcal{G}.\] (10)The solution arising from standard regularization strategies leads to well-known statistical methodologies. We refer to [16] for the background on filter functions in classical regularization theory.

**Empirical solution**: Given the dataset \(D=\{(x_{i},y_{i})\}_{i=1}^{n}\), the empirical analogue of (10) is

\[\hat{C}_{\lambda}:=\hat{C}_{YX}g_{\lambda}(\hat{C}_{X}),\qquad\hat{F}_{\lambda }(\cdot):=\hat{C}_{\lambda}\phi(\cdot)\in\mathcal{G},\] (11)

where \(\hat{C}_{YX}\), \(\hat{C}_{X}\) are empirical covariance operators define as

\[\hat{C}_{X}:=\frac{1}{n}\sum_{i=1}^{n}\phi(x_{i})\otimes\phi(x_{i})\qquad\hat{ C}_{YX}:=\frac{1}{n}\sum_{i=1}^{n}y_{i}\otimes\phi(x_{i}).\]

Note that (11) is the regularized solution of the empirical inverse problem

\[\hat{C}_{YX}=\hat{C}\hat{C}_{X},\qquad\hat{C}\in S_{2}(\mathcal{H},\mathcal{Y }),\]

which arises as the optimality condition for minimizers on \(\mathcal{G}\) of the empirical analogue of (3), given by \(\mathcal{E}_{n}(F):=\frac{1}{n}\sum_{i=1}^{n}\|y_{i}-F(x_{i})\|_{3}^{2}\); see Proposition 2 in the Appendix for a proof. For the vector-valued kernel given in (2), it is well-known that \(\hat{F}_{\lambda}\) can be computed in closed-form for the ridge regression estimator--even in infinite dimensions [47]. For general filter functions, an extended representer theorem is given by [2] in the context of finite-dimensional multitask learning: this approach works in infinite dimensions as well. We give the closed form solution based on [2] below (we include the proof in Appendix D.1).

**Proposition 1** (Representer theorem for general spectral filter).: _Let \((\mathbf{K})_{ij}=k(x_{i},x_{j})\), \(1\leq i,j\leq n\) denote the Gram matrix associated to the scalar-valued kernel \(k\). We have_

\[\hat{F}_{\lambda}(x)=\sum_{i=1}^{n}y_{i}\alpha_{i}(x),\qquad\alpha(x)=\frac{1} {n}g_{\lambda}\left(\frac{\mathbf{K}}{n}\right)\mathbf{k}_{x}\in\mathbb{R}^{n },\qquad(\mathbf{k}_{x})_{i}=k(x,x_{i}),\quad 1\leq i\leq n.\] (12)

**Example 1** (Conditional integration).: _Consider now a random variable \(Z\) taking values in a topological space \(\mathcal{Z}\) on which we define a second RKHS \(\mathcal{H}^{\prime}\in\mathbb{R}^{\mathcal{Z}}\) with kernel \(\ell:\mathcal{Z}\times\mathcal{Z}\to\mathbb{R}\) and canonical feature map \(\psi:\mathcal{Z}\to\mathcal{H}^{\prime},z\mapsto\ell(z,\cdot)\). The conditional mean embedding [47, 20] is defined as_

\[F_{*}(x):=\mathbb{E}[\psi(Z)\mid X=x],\qquad x\in\mathcal{X}.\]

_We immediately see the link with vector-valued regression with \(Y=\psi(Z)\) and \(\mathcal{Y}=\mathcal{H}^{\prime}\). The conditional mean embedding allows us to compute the conditional expectation of any element of \(\mathcal{H}^{\prime}\). Indeed, using the reproducing property, for \(f\in\mathcal{H}^{\prime}\), we have for all \(x\in\mathcal{X}\),_

\[\mathbb{E}[f(Z)\mid X=x]=\langle f,\mathbb{E}[\psi(Z)\mid X=x]\rangle_{ \mathcal{H}^{\prime}}.\]

_Given a dataset \(\{(x_{i},z_{i})\}_{i=1}^{n}\)6 and an estimate of the conditional mean embedding \(F_{*}\) with a spectral algorithm \(\hat{F}_{\lambda}\) as in Eq. (11), and substituting the formula in Eq. (12), we obtain \(\mathbb{E}[f(Z)\mid X=x]\approx\langle f,\hat{F}_{\lambda}(x)\rangle_{ \mathcal{H}^{\prime}}=\sum_{i=1}^{n}\langle f,\psi(z_{i})\rangle_{\mathcal{H} ^{\prime}}\alpha_{i}(x)=\mathbf{f}_{z}^{\intercal}\alpha(x),\) where \((\mathbf{f}_{z})_{i}=f(z_{i})\), \(1\leq i\leq n\)._

Footnote 6: Note that this induces a dataset \(D=\{(x_{i},\psi(z_{i}))\}_{i=1}^{n}\) where we identify \(y_{i}=\psi(z_{i})\).

**Learning rates:** We now give our main result, the learning rates for the difference between \([\hat{F}_{\lambda}]\) and \(F_{*}\) in the interpolation norm, where \(F_{\lambda}\) and \(\hat{F}_{\lambda}\) are given by (10) and (11) based on a general spectral filter satisfying Definition 2. The proof is deferred to Section C in the Appendix.

**Theorem 4** (Upper learning rates).: _Let \(\hat{F}_{\lambda}\) be an estimator based on a general spectral filter with qualification \(\rho\geq 0\). Furthermore, let the conditions (EVD), (EMB), (MOM) be satisfied with \(0<p\leq\alpha\leq 1\). With \(0\leq\gamma\leq 1\), if (SRC) is satisfied with \(\gamma<\beta\leq 2\rho\), we have_

1. _in the case_ \(\beta+p\leq\alpha\)_, let_ \(\lambda_{n}=\Theta\left(\left(n/\log^{\theta}(n)\right)^{-\frac{1}{\alpha}}\right)\) _for some_ \(\theta>1\)_, for all_ \(\tau>\log(6)\) _and sufficiently large_ \(n\geq 1\)_, there is a constant_ \(J>0\) _independent of_ \(n\) _and_ \(\tau\) _such that_ \[\left\|[\hat{F}_{\lambda_{n}}]-F_{*}\right\|_{\gamma}^{2}\leq\tau^{2}J\left( \frac{n}{\log^{\theta}n}\right)^{-\frac{\beta-\gamma}{\alpha}}\] _is satisfied with_ \(P^{n}\)_-probability not less than_ \(1-6e^{-\tau}\)_._2. _in the case_ \(\beta+p>\alpha\)_, let_ \(\lambda_{n}=\Theta\left(n^{-\frac{1}{\beta+p}}\right)\)_, for all_ \(\tau>\log(6)\) _and sufficiently large_ \(n\geq 1\)_, there is a constant_ \(J>0\) _independent of_ \(n\) _and_ \(\tau\) _such that_ \[\left\|\left[\hat{F}_{\lambda_{n}}\right]-F_{\star}\right\|_{\gamma}^{2}\leq \tau^{2}Jn^{-\frac{\beta-\gamma}{\beta+p}}\] _is satisfied with_ \(P^{n}\)_-probability not less than_ \(1-6e^{-\tau}\)_._

Theorem 4 provides the upper rate for vector-valued spectral algorithms. In particular, in combination with the lower bound in Theorem 2, we see that vector-valued spectral algorithms with qualification \(\rho\) achieve an optimal learning rate when the smoothness \(\beta\) of the regression function is in the range \((\alpha-p,2\rho]\). For algorithms with infinite \(\rho\) such as gradient descent and principal component regression, we confirm that they can exploit smoothness of the target function just as in the real-valued setting [3; 5; 30], while not suffering from saturation. For Tikhonov regularization, where \(\rho=1\), the rates recover the state-of-the-art results from [31]. Finally, we point out that obtaining minimax optimal learning rates for \(\beta<\alpha-p\) still remains challenging even in the real-valued output scenario. Note however that for a large variety of RKHS, \(\alpha\) is arbitrarily close to \(p\) and we obtain optimal rates for the whole range \((0,2\rho]\): we refer to [31; 54] for a detailed discussion.

We provide a proof sketch for Theorem 4. The key technical challenge in extending the results of [31] to spectral filter functions lies in the analysis of the estimation error. The estimation error in \(\gamma-\)norm is bounded as \(\left\|\left[\hat{C}_{\lambda}-C_{\lambda}\right]\right\|_{S_{2}(\left[\mathcal{ H}\right]^{\gamma},\mathcal{Y})}\leq 3\lambda^{-\frac{\gamma}{2}}\left\| \left(\hat{C}_{\lambda}-C_{\lambda}\right)\hat{C}_{X,\lambda}^{\frac{1}{2}} \right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\) (see Eq. (37) in Appendix C.3). We rely on the fact that \(\mathrm{Id}_{\mathcal{H}}=\hat{C}_{X}g_{\lambda}(\hat{C}_{X})+r_{\lambda}(\hat {C}_{X})\) (see Definition 2), to obtain the decomposition \(\hat{C}_{\lambda}-C_{\lambda}=\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right) g_{\lambda}(\hat{C}_{X})-C_{\lambda}r_{\lambda}(\hat{C}_{X})\), which yields two terms to be controlled,

\[\left\|\left(\hat{C}_{\lambda}-C_{\lambda}\right)\hat{C}_{X,\lambda}^{\frac{ 1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\leq\underbrace{\left\|\left( \hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)g_{\lambda}(\hat{C}_{X})\hat{C}_{X, \lambda}^{\frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}}_{(I)}+ \underbrace{\left\|C_{\lambda}r_{\lambda}(\hat{C}_{X})\hat{C}_{X,\lambda}^{ \frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}}_{(II)}\]

To control term (I), we use the definition of the filter function \(g_{\lambda}\) (Definition 2) to obtain that \(\left\|\hat{C}_{X,\lambda}g_{\lambda}(\hat{C}_{X})\right\|_{\mathcal{H}\sim \mathcal{H}}\lesssim 1\). Thus it suffices to control the term \(\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X,\lambda}^{-\frac{ 1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}=\left\|\frac{1}{n}\sum_{i=1}^{n }\xi(x_{i},y_{i})\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\), where \(\xi(x,y)=\left(y-C_{\lambda}\phi(x)\right)\otimes C_{X,\lambda}^{-\frac{1}{2} }\phi(x)\). We proceed by bounding \(\mathbb{E}[\left|\xi(X,X)\right|_{S_{2}(\mathcal{H},\mathcal{Y})}^{m}]\) for \(m\geq 1\), and then use Bernstein's inequality to derive the upper bound on \(\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X,\lambda}^{-\frac{ 1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\). To control term (II), Lemma 9 in C.1 shows that

\[(II)\lesssim\left\|\hat{C}_{X,\lambda}^{\frac{1}{2}}r_{\lambda}(\hat{C}_{X})g_ {\lambda}(C_{X})C_{X}^{\frac{\mu+1}{2}}\right\|_{\mathcal{H}\sim\mathcal{H}}.\]

The term on the right side is bounded in prior work on scalar-valued spectral method, and we refer the reader to [54, Theorem 16]. The results of Theorem 4 are then obtained by choosing regularization parameter \(\lambda=\lambda(n)\) to optimally trade off approximation and estimation errors.

## 5 Conclusion

In this work, we have rigorously explored the theoretical properties of vector-valued spectral learning algorithms, focusing on their performance in infinite-dimensional output spaces. We first proved the saturation effect observed in vector-valued kernel ridge regression, highlighting its limitations in exploiting additional smoothness in regression functions. We then presented upper bounds on the finite sample risk for a general class of spectral learning algorithms, demonstrating their minimax optimality across various scenarios, including misspecified learning settings.

Our results open avenues for further research, particularly in developing more efficient implementations for practical use in high-dimensional machine learning problems such as causal inference and functional data analysis.

**Acknowledgement:** Dimitri Meunier, Arthur Gretton and Zhu Li were supported by the Gatsby Charitable Foundation.

## References

* [1] J.-P. Aubin. _Applied Functional Analysis_. John Wiley & Sons, Inc., 2nd edition, 2000.
* [2] L. Baldassarre, L. Rosasco, A. Barla, and A. Verri. Multi-output learning via spectral filtering. _Machine Learning_, 87(3):259-301, 2012.
* [3] F. Bauer, S. Pereverzev, and L. Rosasco. On regularization algorithms in learning theory. _Journal of Complexity_, 23(1):52-72, 2007.
* [4] A. Berlinet and C. Thomas-Agnan. _Reproducing Kernel Hilbert Spaces in Probability and Statistics_. Springer, 2011.
* [5] G. Blanchard and N. Mucke. Optimal rates for regularization of statistical inverse learning problems. _Foundations of Computational Mathematics_, 18(4):971-1013, 2018.
* [6] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-squares algorithm. _Foundations of Computational Mathematics_, 7(3):331-368, 2007.
* [7] A. Caponnetto, C. A. Micchelli, M. Pontil, and Y. Ying. Universal multi-task kernels. _Journal of Machine Learning Research_, 9:1615-1646, 2008.
* [8] A. Caponnetto and Y. Yao. Cross-validation based adaptation for regularization operators in learning theory. _Analysis and Applications_, 8(02):161-183, 2010.
* [9] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel Hilbert spaces of integrable functions and Mercer theorem. _Analysis and Applications_, 4(04):377-408, 2006.
* [10] C. Carmeli, E. De Vito, A. Toigo, and V. Umanita. Vector valued reproducing kernel Hilbert spaces and universality. _Analysis and Applications_, 8(01):19-61, 2010.
* [11] C. Ciliberto, L. Rosasco, and A. Rudi. A consistent regularization approach for structured prediction. _Advances in Neural Information Processing Systems_, 29, 2016.
* [12] C. Ciliberto, L. Rosasco, and A. Rudi. A general framework for consistent structured prediction with implicit loss embeddings. _Journal of Machine Learning Research_, 21(1):3852-3918, 2020.
* [13] E. De Vito, L. Rosasco, and A. Caponnetto. Discretization error analysis for tikhonov regularization. _Analysis and Applications_, 4(01):81-99, 2006.
* [14] J. Diestel and J. Uhl. _Vector Measures_. American Mathematical Society, 1977.
* [15] R. Dudley. _Real Analysis and Probability_. Cambridge University Press, 2nd edition edition, 2002.
* [16] H. W. Engl, M. Hanke, and A. Neubauer. _Regularization of Inverse Problems_. Kluwer, 2000.
* [17] S. Fischer and I. Steinwart. Sobolev norm learning rates for regularized least-squares algorithms. _Journal Of Machine Learning Research_, 21:205-1, 2020.
* [18] J. I. Fujii, M. Fujii, T. Furuta, and R. Nakamoto. Norm inequalities equivalent to heinz inequality. _Proceedings of the American Mathematical Society_, 118(3):827-830, 1993.
* [19] L. L. Gerfo, L. Rosasco, F. Odone, E. D. Vito, and A. Verri. Spectral algorithms for supervised learning. _Neural Computation_, 20(7):1873-1897, 2008.
* [20] S. Grunewalder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton, and M. Pontil. Conditional mean embeddings as regressors. In _International Conference on Machine Learning_, pages 1803---1810, 2012.
* [21] S. Grunewalder, G. Lever, L. Baldassarre, M. Pontil, and A. Gretton. Modelling transition dynamics in MDPs with RKHS embeddings. In _International Conference on Machine Learning_, pages 535-542, 2012.

* [22] T. Herdman, R. D. Spies, and K. G. Temperrini. Global saturation of regularization methods for inverse ill-posed problems. _Journal of optimization theory and applications_, 148(1):164-196, 2011.
* [23] J. Jin, Y. Lu, J. Blanchet, and L. Ying. Minimax optimal kernel operator learning via multilevel training. In _The Eleventh International Conference on Learning Representations_, 2023.
* [24] H. Kadri, E. Duflos, P. Preux, S. Canu, A. Rakotomamonjy, and J. Audiffren. Operator-valued kernels for learning from functional response data. _Journal of Machine Learning Research_, 17(20):1-54, 2016.
* [25] V. Kostic, K. Lounici, P. Novelli, and M. Pontil. Sharp spectral rates for koopman operator learning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [26] V. Kostic, P. Novelli, A. Maurer, C. Ciliberto, L. Rosasco, and M. Pontil. Learning dynamical systems via Koopman operator regression in reproducing kernel Hilbert spaces. _Advances in Neural Information Processing Systems_, 35:4017-4031, 2022.
* [27] S. Lanthaler and N. H. Nelsen. Error bounds for learning with vector-valued random features. _Advances in Neural Information Processing Systems_, 36, 2024.
* [28] Y. Li, W. Gan, Z. Shi, and Q. Lin. Generalization error curves for analytic spectral algorithms under power-law decay. _arXiv preprint arXiv:2401.01599_, 2024.
* [29] Y. Li, H. Zhang, and Q. Lin. On the saturation effect of kernel ridge regression. In _The Eleventh International Conference on Learning Representations_, 2023.
* [30] Z. Li, D. Meunier, M. Mollenhauer, and A. Gretton. Optimal rates for regularized conditional mean embedding learning. In _Advances in Neural Information Processing Systems_, volume 35, pages 4433-4445, 2022.
* [31] Z. Li, D. Meunier, M. Mollenhauer, and A. Gretton. Towards optimal sobolev norm rates for the vector-valued regularized least-squares algorithm. _Journal of Machine Learning Research_, 25(181):1-51, 2024.
* [32] J. Lin and V. Cevher. Optimal distributed learning with multi-pass stochastic gradient methods. In _International Conference on Machine Learning_, pages 3092-3101. PMLR, 2018.
* [33] J. Lin and V. Cevher. Optimal convergence for distributed learning with stochastic gradient methods and spectral algorithms. _Journal of Machine Learning Research_, 21(147):1-63, 2020.
* [34] J. Lin, A. Rudi, L. Rosasco, and V. Cevher. Optimal rates for spectral algorithms with least-squares regression over Hilbert spaces. _Applied and Computational Harmonic Analysis_, 48(3):868-890, 2020.
* [35] A. Mastouri, Y. Zhu, L. Gultchin, A. Korba, R. Silva, M. Kusner, A. Gretton, and K. Muandet. Proximal causal learning with kernels: Two-stage estimation and moment restriction. In _International Conference on Machine Learning_, pages 7512-7523. PMLR, 2021.
* [36] P. Mathe. Saturation of regularization methods for linear ill-posed problems in Hilbert spaces. _SIAM journal on numerical analysis_, 42(3):968-973, 2004.
* [37] M. Mollenhauer and P. Koltai. Nonparametric approximation of conditional expectation operators. _arXiv preprint arXiv:2012.12917_, 2020.
* [38] M. Mollenhauer, N. Mucke, and T. Sullivan. Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem. _arXiv preprint arXiv:2211.08875_, 2022.
* [39] N. Mucke, G. Neu, and L. Rosasco. Beating SGD saturation with tail-averaging and minibatch- ing. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [40] A. Neubauer. On converse and saturation results for Tikhonov regularization of linear ill-posed problems. _SIAM Journal On Numerical Analysis_, 34(2):517-527, 1997.

* [41] J. Park and K. Muandet. A measure-theoretic approach to kernel conditional mean embeddings. _Advances in Neural Information Processing Systems_, 33:21247-21259, 2020.
* [42] R. Singh, M. Sahani, and A. Gretton. Kernel instrumental variable regression. _Advances in Neural Information Processing Systems_, 32, 2019.
* [43] R. Singh, L. Xu, and A. Gretton. Kernel methods for causal functions: dose, heterogeneous and incremental response curves. _Biometrika_, 111(2):497-516, 2024.
* [44] S. Smale and D.-X. Zhou. Shannon sampling and function reconstruction from point values. _Bulletin of the American Mathematical Society_, 41(3):279-305, 2004.
* [45] S. Smale and D.-X. Zhou. Shannon sampling II: Connections to learning theory. _Applied and Computational Harmonic Analysis_, 19(3):285-302, 2005.
* [46] S. Smale and D.-X. Zhou. Learning theory estimates via integral operators and their approximations. _Constructive Approximation_, 26(2):153-172, 2007.
* [47] L. Song, J. Huang, A. Smola, and K. Fukumizu. Hilbert space embeddings of conditional distributions with applications to dynamical systems. In _International Conference on Machine Learning_, pages 961-968, 2009.
* [48] B. K. Sriperumbudur, K. Fukumizu, and G. R. Lanckriet. Universality, characteristic kernels and RKHS embedding of measures. _Journal of Machine Learning Research_, 12(Jul):2389-2410, 2011.
* [49] I. Steinwart and A. Christmann. _Support Vector Machines_. Springer, 2008.
* [50] I. Steinwart, D. R. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In _COLT_, pages 79-93, 2009.
* [51] I. Steinwart and C. Scovel. Mercer's theorem on general domains: On the interaction between measures, kernels, and RKHSs. _Constructive Approximation_, 35(3):363-417, 2012.
* [52] J. Weidmann. _Linear Operators in Hilbert Spaces_. Springer, 1980.
* [53] Y. Yao, L. Rosasco, and A. Caponnetto. On early stopping in gradient descent learning. _Constructive Approximation_, 26(2):289-315, 2007.
* [54] H. Zhang, Y. Li, and Q. Lin. On the optimality of misspecified spectral algorithms. _Journal of Machine Learning Research_, 25(188):1-50, 2024.
* [55] H. Zhang, Y. Li, W. Lu, and Q. Lin. On the optimality of misspecified kernel ridge regression. In _International Conference on Machine Learning_, pages 41331-41353. PMLR, 2023.

## Appendices

The appendix is organized as follows. In Section A, we give additional mathematical background and notations. In Section B, we give the proof of Theorem 3 and provide a technical comparison of our proof with [29]. In Section C, we prove Theorem 4. Finally, in Section D, we provide auxiliary results used in the main proofs.

## Appendix A Additional Background

### Hilbert spaces and linear operators

**Definition 3** (Bochner \(L_{q}\)-spaces, [14]).: _Let \(H\) be a separable Hilbert space and \(\pi\) a probability measure on \(\mathcal{X}\). For \(1\leq q\leq\infty\), \(L_{q}(\mathcal{X},\mathcal{F}_{\mathcal{X}},\pi;H)\), abbreviated \(L_{q}(\pi;H)\), is the space of strongly \(\mathcal{F}_{\mathcal{X}}-\mathcal{F}_{H}\) measurable and Bochner \(q\)-integrable functions from \(\mathcal{X}\) to \(H\), with the norms_

\[\|f\|_{L_{q}(\pi;H)}^{q}=\int_{\mathcal{X}}\|f\|_{H}^{q}\,\mathrm{d}\pi,\quad 1 \leq q<\infty,\qquad\|f\|_{L_{\infty}(\pi;H)}=\inf\left\{C\geq 0:\pi\{\|f\|_{H}>C\}=0 \right\}.\]

**Definition 4** (\(p\)-Schatten class, e.g. [52]).: _Let \(H,H^{\prime}\) be separable Hilbert spaces. For \(1\leq q\leq\infty\), \(S_{p}(H,H^{\prime})\), abbreviated \(S_{p}(H)\) if \(H=H^{\prime}\), is the Banach space of all compact operators \(C\) from \(H\) to \(H^{\prime}\) such that \(\|C\|_{S_{p}(H,H^{\prime})}:=\left|\left(\sigma_{i}(C)\right)_{i\in I}\right|_ {\ell_{p}}\) is finite. Here \(\|\left(\sigma_{i}(C)\right)_{i\in I}\|_{\ell_{p}}\) is the \(\ell_{p}\)-sequence space norm of the sequence of the strictly positive singular values of \(C\) indexed by the at most countable set \(I\). For \(p=2\), we retrieve the space of Hilbert-Schmidt operators, for \(p=1\) we retrieve the space of Trace Class operators, and for \(p=+\infty\), \(|\cdot|_{S_{\infty}(H,H^{\prime})}\) corresponds to the operator norm \(|\cdot|_{H\to H^{\prime}}\)._

**Definition 5** (Tensor Product of Hilbert Spaces, [1]).: _Let \(H,H^{\prime}\) be Hilbert spaces. The Hilbert space \(H\otimes H^{\prime}\) is the completion of the algebraic tensor product with respect to the norm induced by the inner product \(\left\{x_{1}\otimes x_{1}^{\prime},x_{2}\otimes x_{2}^{\prime}\right)_{H\otimes H ^{\prime}}=\left\{x_{1},x_{2}\right\}_{H}\{x_{1}^{\prime},x_{2}^{\prime}\}_{H ^{\prime}}\) for \(x_{1},x_{2}\in H\) and \(x_{1}^{\prime},x_{2}^{\prime}\in H^{\prime}\) defined on the elementary tensors of \(H\otimes H^{\prime}\). This definition extends to \(\operatorname{span}\left\{x\otimes x^{\prime}|x\in H,x^{\prime}\in H^{\prime}\right\}\) and finally to its completion. The space \(H\otimes H^{\prime}\) is separable whenever both \(H\) and \(H^{\prime}\) are separable. If \(\{e_{i}\}_{i\in I}\) and \(\{e_{j}^{\prime}\}_{j\in J}\) are orthonormal basis in \(H\) and \(H^{\prime}\), \(\{e_{i}\otimes e_{j}^{\prime}\}_{i\in I,j\in J}\) is an orthonormal basis in \(H\otimes H^{\prime}\)._

**Theorem 5** (Isometric Isomorphism between \(L_{2}(\pi;\mathcal{Y})\) and \(S_{2}(L_{2}(\pi,\mathcal{Y})\), Theorem 12.6.1 [1]).: _Let \(H\) be a separable Hilbert space. The Bochner space \(L_{2}(\pi;H)\) is isometrically isomorphic to \(S_{2}(L_{2}(\pi),\mathcal{Y})\) and the isometric isomorphism is realized by the map \(\Psi:S_{2}(L_{2}(\pi),\mathcal{Y})\to L_{2}(\pi;H)\) acting on elementary tensors as \(\Psi(f\otimes y)=(\omega\to f(\omega)y)\)._

### RKHS embbedings into \(L_{2}\) and Well-specifiedness

Recall that \(I_{\pi}:\mathcal{H}\to L_{2}(\pi)\) is the embedding that maps every function in \(\mathcal{H}\) into its \(\pi\)-equivalence class in \(L_{2}(\pi)\) and that we used the shorthand notation \([f]=I_{\pi}(f)\) for all \(f\in\mathcal{H}\). We define similarly \(\mathcal{I}_{\pi}:\mathcal{G}\to L_{2}(\pi;\mathcal{Y})\) as the embedding that maps every function in \(\mathcal{G}\) into its \(\pi\)-equivalence class in \(L_{2}(\pi;\mathcal{Y})\).

**Definition 6** (Embedding \(\mathcal{G}\) into \(L_{2}(\pi;\mathcal{Y})\)).: _Let \(\mathcal{I}_{\pi}:=I_{\mathcal{Y}}\otimes I_{\pi}\) be the tensor product of the operator \(\operatorname{Id}_{\mathcal{Y}}\) with the operator \(I_{\pi}\) (see [1, Definition 12.4.1.] for the definition of tensor product of operators). \(\mathcal{I}_{\pi}\) maps every function in \(\mathcal{G}\) into its \(\pi\)-equivalence class in \(L_{2}(\pi;\mathcal{Y})\). We then use the shorthand notation \([F]=\mathcal{I}_{\pi}(F)\) for all \(F\in\mathcal{G}\)._

**Remark 5**.: _Let \(\{d_{j}\}_{j\in J}\) be an orthonormal basis of \(\mathcal{Y}\) and recall that \(\{\sqrt{\mu_{i}}[e_{i}]\}_{i\in I}\) forms an orthonormal basis of \([\mathcal{H}]^{1}\). Let \(F\in[\mathcal{G}]^{1}\). Then \(F\) can be represented as the element \(C:=\sum_{i\in I,j\in J}a_{ij}d_{j}\otimes\sqrt{\mu_{i}}[e_{i}]\) in \(S_{2}([\mathcal{H}]^{1},\mathcal{Y})\) by definition of \([\mathcal{G}]^{1}\) with \(\|C\|_{1}^{2}=\sum_{i,j}a_{ij}^{2}\). Hence defining \(\bar{C}:=\sum_{i\in I,j\in J}a_{ij}d_{j}\otimes\sqrt{\mu_{i}}e_{i}\) we have \(C=\bar{C}\)\(\pi-\)a.e. and_

\[\|\bar{C}\|_{2}^{2}=\sum_{i\in I,j\in J}a_{i,j}^{2}=\|C\|_{1}^{2}<+\infty.\]

_Taking the elements identifying \(\bar{C}\) in \(\mathcal{G}\) gives a representer \(\bar{F}\) of \(F\) in \(\mathcal{G}\)._

### Additional Notations

In the following, we fix \(\{d_{j}\}_{j\in J}\) an orthonormal basis of \(\mathcal{Y}\), where \(J\) is at most countable. Recall that \(\left\{\mu_{i}^{1/2}e_{i}\right\}_{i\in I}\) is an ONB of \((\ker I_{\pi})^{\perp}\) in \(\mathcal{H}\), and \(\left\{[e_{i}]\right\}_{i\in I}\) is an ONB of \(\overline{\text{ran }I_{\pi}}\) in \(L_{2}(\pi)\). Let \(\{\tilde{e}_{i}\}_{i\in I^{\prime}}\) be an ONB of \(\ker I_{\pi}\) (with \(I\cap I^{\prime}=\varnothing\)), then \(\left\{\mu_{i}^{1/2}e_{i}\right\}_{i\in I}\cup\{\tilde{e}_{i}\}_{i\in I^{ \prime}}\) forms an ONB of \(\mathcal{H}\), and \(\left\{d_{j}\otimes\mu_{i}^{1/2}e_{i}\right\}_{i\in I,j\in J}\cup\{d_{j} \otimes\tilde{e}_{i}\}_{i\in I^{\prime},j\in J}\) forms an ONB of \(\mathcal{Y}\otimes\mathcal{H}\simeq\mathcal{G}\).

For any Hilbert space \(H\), linear operator \(T:H\to H\) and scalar \(\lambda>0\), we define \(T_{\lambda}:=T+\lambda I_{H}\).

## Appendix B Saturation Effect with Tikhonov Regularization - Proof of Theorem 3

In the following proofs a quantity \(h_{n}\geq 0\) depending on \(n\geq 1\), but independent of \(\tau\) the confidence level, is equal to \(o(1)\) if \(h_{n}\to 0\) when \(n\to+\infty\).

We will make extensive use of the following notation in the subsequent analysis.

**Definition 7** (Empirical \(L_{2}(\pi)\)-norm).: _Denoted by \(\left\langle\cdot,\cdot\right\rangle_{2,n}\), the empirical \(L_{2}(\pi)\)-norm associated to points \(\left\{x_{i}\right\}_{i=1}^{n}\) independently and identically sampled from the distribution of \(X\), is defined as, for any \(f,g\in\mathcal{H}\),_

\[\left\langle f,g\right\rangle_{2,n}:=\left\langle\hat{C}_{X},f\otimes g\right \rangle_{S_{2}(\mathcal{H})}=\left\langle\hat{C}_{X}f,g\right\rangle_{\mathcal{ H}}=\left\langle\hat{C}_{X}^{\frac{1}{2}}f,\hat{C}_{X}^{\frac{1}{2}}g \right\rangle_{\mathcal{H}}=\frac{1}{n}\sum_{i=1}^{n}f(x_{i})g(x_{i}).\]

_This induces an inner product on \(\mathcal{H}\), with associated norm,_

\[\|f\|_{2,n}^{2}=\langle f,f\rangle_{2,n}=\frac{1}{n}\sum_{i=1}^{n}f(x_{i})^{2}.\]

**Definition 8**.: _Fix \(x\in\mathcal{X}\) and \(\lambda>0\). The regularized canonical feature map is defined as_

\[f_{x,\lambda}(\cdot)=C_{X,\lambda}^{-1}k(x,\cdot):\mathcal{X}\to\mathcal{H}.\]

Recall from Eq. (11) that the ridge estimator \(\hat{F}_{\lambda}\) defined in Eq. (7) can be expressed as

\[\hat{C}_{\lambda}=\hat{C}_{YXS}g_{\lambda}(\hat{C}_{X}),\qquad\hat{F}_{\lambda }(\cdot)=\hat{C}_{\lambda}\phi(\cdot)\in\mathcal{G},\]

where in Theorem 3 we focus on Tikhonov regularization where \(g_{\lambda}(x)=(x+\lambda)^{-1}\). In that setting we have

\[r_{\lambda}\left(x\right):=1-\frac{x}{x+\lambda}=-\frac{\lambda}{x+\lambda}.\] (13)

Proof of Theorem 3.: Since \(\beta\geq 2\), \(F_{*}\in[\mathcal{G}]^{\beta}\subseteq[\mathcal{G}]^{1}\), therefore \(F_{*}\) has a representer \(\bar{F}\) in \(\mathcal{G}\) such that \(F_{*}=\bar{F}\)\(\pi\)-a.e. (see Remark 5), and by Theorem 1, \(\bar{F}(\cdot)=\bar{C}\phi(\cdot)\), with \(\bar{C}\in S_{2}(\mathcal{H},\mathcal{Y})\). Define the errors \(\epsilon_{i}:=y_{i}-\bar{C}\phi(x_{i})\), \(i=1,\ldots,n\), that are i.i.d samples with the same distribution as \(\epsilon:=Y-\bar{C}\phi(X)\). By assumption \(\mathbb{E}\left[\left|\epsilon\right|_{\mathcal{Y}}^{2}\mid X\right]\geq\sigma ^{2}\) and by definition \(\mathbb{E}\left[\epsilon\mid X\right]=0\). By Eq. (13), we have

\[r_{\lambda}\left(\hat{C}_{X}\right):=I-\hat{C}_{X}\hat{C}_{X,\lambda}^{-1}=- \lambda\hat{C}_{X,\lambda}^{-1}.\]

[MISSING_PAGE_FAIL:16]

Fix \(x\in\mathcal{X}\). Using the algebraic identity \(a^{2}-b^{2}=(a-b)(2b+(a-b))\), and recalling that by Definition 7,

\[\|f\|_{2,n}^{2}=\left\|\hat{C}_{X}^{\frac{1}{2}}f\right\|_{\mathcal{H}}^{2},\]

we deduce

\[\left\|\left\|\hat{C}_{X}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-1}k(x,\cdot)\right\|_{\mathcal{H}}^{2}-\left\|\hat{C}_{X}^{\frac{1}{2}}C_{X,\lambda} ^{-1}k(x,\cdot)\right\|_{\mathcal{H}}^{2}\right\|\] \[\leq \left\|\hat{C}_{X}^{\frac{1}{2}}\left(\hat{C}_{X,\lambda}^{-1}-C _{X,\lambda}^{-1}\right)k(x,\cdot)\right\|_{\mathcal{H}}\cdot\left(\left\|\hat{ C}_{X}^{\frac{1}{2}}\left(\hat{C}_{X,\lambda}^{-1}-C_{X,\lambda}^{-1}\right)k(x, \cdot)\right\|_{\mathcal{H}}+2\left\|C_{X,\lambda}^{-1}k(x,\cdot)\right\|_{2, n}\right)\] \[\leq \tau o(1)\left(\tau o(1)+2\left\|C_{X,\lambda}^{-1}k(x,\cdot) \right\|_{2,n}\right).\]

Using Definition 7 again, this reads

\[\left\|\hat{C}_{X,\lambda}^{-1}k(x,\cdot)\right\|_{2,n}^{2}\geq\left\|C_{X, \lambda}^{-1}k(x,\cdot)\right\|_{2,n}^{2}-\tau o(1)\left(\tau o(1)+2\left\|C_ {X,\lambda}^{-1}k(x,\cdot)\right\|_{2,n}\right).\]

We have

\[\|C_{X,\lambda}^{-1}k(x,\cdot)\|_{2,n}^{2}\leq\frac{3}{2}\|[C_{X,\lambda}^{-1} k(x,\cdot)]\|_{L_{2}(\pi)}^{2}+\tau o(1)\leq\left(\sqrt{1.5}\|[C_{X,\lambda}^{-1}k (x,\cdot)]\|_{L_{2}(\pi)}+\sqrt{\tau}o(1)\right)^{2}.\]

Hence,

\[\left\|\hat{C}_{X,\lambda}^{-1}k(x,\cdot)\right\|_{2,n}^{2}\geq \left\|C_{X,\lambda}^{-1}k(x,\cdot)\right\|_{2,n}^{2}-\tau o(1) \left(\left\|[C_{X,\lambda}^{-1}k(x,\cdot)]\right\|_{L_{2}(\pi)}+\sqrt{\tau}o (1)+\tau o(1)\right)\] \[\geq \frac{1}{2}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}^{2}- \tau o(1)\] \[-\tau o(1)\left(\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}+ \sqrt{\tau}o(1)+\tau o(1)\right)\] \[\geq \frac{1}{2}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}^{2}- \tau^{2}o(1)-\tau o(1)\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}.\]

By Lemma 17,

\[\int_{\mathcal{X}}\left\|[C_{X,\lambda}^{-1}k(x,\cdot)]\right\|_{L_{2}(\pi)}^{ 2}d\pi(x)=\mathcal{N}_{2}(\lambda).\]

Furthermore, by Jensen's inequality,

\[\int_{\mathcal{X}}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}^{2}\mathrm{ d}\pi(x)\geq\left(\int_{\mathcal{X}}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}( \pi)}\mathrm{d}\pi(x)\right)^{2}.\]

Recall from Lemma 16 that

\[c_{1,2}\lambda^{-p}\leq\mathcal{N}_{2}(\lambda)\leq c_{2,2}\lambda^{-p}.\]

Therefore we have

\[\int_{\mathcal{X}}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)} \mathrm{d}\pi(x)\leq\sqrt{c_{2,2}}\lambda^{-\frac{p}{2}}\] \[\int_{\mathcal{X}}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)} ^{2}\mathrm{d}\pi(x)\geq c_{1,2}\lambda^{-p}.\]

Hence

\[\int_{\mathcal{X}}\|\hat{C}_{X,\lambda}^{-1}k(x,\cdot)\|_{2,n}^{2} d\pi(x) \geq\frac{c_{1,2}}{2}\lambda^{-p}-\tau^{2}o(1)-\tau o(1)\sqrt{c_{ 2,2}}\lambda^{-\frac{p}{2}}\] \[\geq \left(\frac{c_{1,2}}{2}-\tau o(1)\sqrt{c_{2,2}}\right)\lambda^{-p }-\tau^{2}o(1)\]

Combined with Eq. (15), it leads to our final bound on the variance term, for a constant \(\rho_{2}\geq 0\) and for sufficiently large \(n\geq 1\), where the hidden index bound depends on \(\tau\), we have

\[\frac{\sigma^{2}}{n^{2}}\sum_{i=1}^{n}\left\|\left[\hat{C}_{X,\lambda}^{-1} \phi(x_{i})\right]\right\|_{L_{2}(\pi)}^{2}\geq\frac{\rho_{2}}{n\lambda^{p}}.\] (16)

**Putting it together.** We are now ready to assemble the lower bounds on the variance and on the bias. For a fixed confidence parameter \(\tau\geq\log(10)\), for sufficiently large \(n>0\), where the hidden index bound depends on \(\tau\), with probability at least \(1-10e^{-\tau}\), we have by Eq. (14) and Eq. (16), that for \(\lambda=\lambda(n)\) satisfying \(1\geq\lambda\geq n^{-\frac{1}{2\tau}}\),

\[\mathbb{E}\left[\left\|[\hat{F}_{\lambda}]-F_{\ast}\right\|_{L_{2}(\pi; \mathcal{Y})}^{2}\mid x_{1},\ldots,x_{n}\right]\geq\rho_{1}\lambda^{2}+\rho_{2 }n^{-1}\lambda^{-p}\]

where \(\rho_{1},\rho_{2}\) have no dependence on \(n\). Recall Young's inequality, for \(r,q>1\) satisfying \(r^{-1}+q^{-1}=1\), we have for all \(a,b\geq 0\),

\[a+b\geq r^{\frac{1}{\tau}}q^{\frac{1}{\tau}}a^{\frac{1}{\tau}}b^{\frac{1}{ \tau}}.\]

We apply Young's inequality with \(r^{-1}=p/(2+p)\) and \(q^{-1}=2/(2+p)\), there exists a constant \(c_{1}>0\) such that

\[\rho_{1}\lambda^{2}+\rho_{2}n^{-1}\lambda^{-p}\geq c_{1}\left(\lambda^{2} \right)^{\frac{p}{2\tau}}\left(\lambda^{-p}n^{-1}\right)^{\frac{2}{2\tau}}=c_{ 1}n^{-\frac{2}{2\tau}}.\]

To conclude the proof, let \(\lambda=\lambda(n)\) be an arbitrary choice of regularization parameter satisfying \(\lambda(n)\to 0\). We have just covered the case \(1\geq\lambda\geq n^{-\frac{1}{2\tau}}\) and the case \(0<\lambda\leq n^{-\frac{1}{2\tau}}\) is covered by [29, Section B.4]. 

**Lemma 1**.: _For any \(\lambda\leq 1\) and \(C\in S_{2}(\mathcal{H},\mathcal{Y})\), with \(C\pm S_{2}(\overline{\text{ran }S_{\pi}},\mathcal{Y})\)7, we have_

Footnote 7: \(\pm\) is the notation for not being orthogonal to.

\[\left\|[CC_{X,\lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}^{2}\geq \sum_{i\in I,j\in J}a_{ij}^{2}\frac{\mu_{i}}{(\mu_{i}+1)^{2}}>0,\]

_with \(a_{ij}:=\langle d_{j},C\sqrt{\mu_{i}}e_{i}\rangle_{\mathcal{Y}}\), \(i\in I,j\in J\)._

Proof.: Recall the notations of Section A.3. Define \(\{a_{ij}\}_{i\in I\cap I^{\prime},j\in J}\) such that \(a_{ij}:=\langle d_{j},C\sqrt{\mu_{i}}e_{i}\rangle_{\mathcal{Y}}\) for \(i\in I,j\in J\) and \(a_{ij}:=\{d_{j},C\tilde{e}_{i}\}_{\mathcal{Y}}\) for \(i\in I^{\prime},j\in J\). Then, on one hand, since \(C\in S_{2}(\mathcal{H},\mathcal{Y})\),

\[C=\sum_{i\in I,j\in J}a_{ij}d_{j}\otimes(\sqrt{\mu_{i}}e_{i})+\sum_{i\in I^{ \prime},j\in J}a_{ij}d_{j}\otimes\tilde{e}_{i}.\]

On the other hand,

\[C_{X,\lambda}^{-1}=\sum_{i\in I}(\mu_{i}+\lambda)^{-1}(\sqrt{\mu_{i}}e_{i}) \otimes(\sqrt{\mu_{i}}e_{i})+\lambda^{-1}\sum_{i\in I^{\prime}}\tilde{e}_{i} \otimes\tilde{e}_{i}.\]

Therefore, noting that \(\tilde{e}_{i}=0\)\(\pi\)-a.e. for all \(i\in I^{\prime}\), we have,

\[[CC_{X,\lambda}^{-1}] =\left[\sum_{i\in I,j\in J}a_{ij}(\mu_{i}+\lambda)^{-1}d_{j} \otimes(\sqrt{\mu_{i}}e_{i})+\sum_{i\in I^{\prime},j\in J}\frac{a_{ij}}{ \lambda}d_{j}\otimes\tilde{e}_{i}\right]\] \[=\sum_{i\in I,j\in J}a_{ij}\frac{\sqrt{\mu_{i}}}{\mu_{i}+\lambda} d_{j}\otimes[e_{i}].\]

Therefore the \(S_{2}(L_{2}(\pi),\mathcal{Y})\)-norm can be evaluated in closed form using Parseval's identity,

\[\left\|[CC_{X,\lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}^{2}=\sum_ {i\in I,j\in J}a_{ij}^{2}\frac{\mu_{i}}{(\mu_{i}+\lambda)^{2}}\geq\sum_{i\in I,j\in J}a_{ij}^{2}\frac{\mu_{i}}{(\mu_{i}+1)^{2}},\]

where we used that \(\{d_{j}\otimes[e_{i}]\}_{j\in J,i\in I}\) is orthonormal in \(\mathcal{Y}\otimes L_{2}(\pi)\), and \(\lambda\leq 1\). The right hand side has no dependence on \(\lambda\) or \(n\). Furthermore, under assumption (EVD+), \(\mu_{i}>0\) for all \(i\in I\), therefore the right hand side term equals zero if and only if \(a_{ij}=0\) for all \(i\in I,j\in J\). Since by assumption \(C\pm S_{2}(\overline{\text{ran }S_{\pi}},\mathcal{Y})\), the right hand side is strictly positive. 

**Lemma 2**.: _Suppose Assumption (EVD) holds with \(p\in(0,1]\). Let \(C\in S_{2}(\mathcal{H},\mathcal{Y})\) such that \([C]\in S_{2}([\mathcal{H}]^{2},\mathcal{Y})\). There is a constant \(c_{0}>0\) such that for any \(\tau\geq\log(4)\), with probability at least \(1-4e^{-\tau}\), for \(n\geq(c_{0}\tau)^{(4+2p)}\) and \(1\geq\lambda\geq n^{-\frac{1}{2\tau p}}\), we have_

\[\left\|[CC_{X\lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}^{2}-\left\| [C\hat{C}_{X\lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}^{2}\leq \tau^{2}o(1)\]

_We have \(c_{0}:=8\kappa\max\{\sqrt{c_{2,1}},1\}\) where \(c_{2,1}\) is defined in Lemma 16._Proof.: Using the identity \(A^{-1}-B^{-1}=A^{-1}(B-A)B^{-1}\), we obtain

\[C_{X,\lambda}^{-1}-\hat{C}_{X,\lambda}^{-1}=C_{X,\lambda}^{-1}(\hat{C}_{X}-C_{X} )\hat{C}_{X,\lambda}^{-1}\]

We apply Lemma 22 with \(\gamma=0\),

\[\left\|\left[C\left(C_{X,\lambda}^{-1}-\hat{C}_{X,\lambda}^{-1} \right)\right]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})} =\left\|\left[C\hat{C}_{X,\lambda}^{-1}(\hat{C}_{X}-C_{X})C_{X, \lambda}^{-1}\right]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}\] \[=\left\|C\hat{C}_{X,\lambda}^{-1}(\hat{C}_{X}-C_{X})C_{X,\lambda} ^{-1}C_{X}^{\frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\] \[\leq\left\|C\hat{C}_{X,\lambda}^{-\frac{1}{2}}\right\|_{S_{2}( \mathcal{H},\mathcal{Y})}\left\|\hat{C}_{X,\lambda}^{-1}C_{X,\lambda}^{\frac{ 1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\] \[\cdot\left\|C_{X,\lambda}^{-\frac{1}{2}}(\hat{C}_{X}-C_{X})C_{X, \lambda}^{-\frac{1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\left\|C_{X, \lambda}^{-\frac{1}{2}}C_{X}^{\frac{1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\] (17)

We consider each of the four terms in line (17). The last term is bounded above by \(1\) and the first term is bounded above by \(\lambda^{-\frac{1}{2}}\left\|C\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\). By Lemma 20 applied with \(s=1/2\), we have for the second term

\[\left\|\hat{C}_{X,\lambda}^{-\frac{1}{2}}C_{X,\lambda}^{\frac{1}{2}}\right\|_{ \mathcal{H}\to\mathcal{H}}\leq\left\|\hat{C}_{X,\lambda}^{-1}C_{X,\lambda} \right\|_{\mathcal{H}\to\mathcal{H}}^{\frac{1}{2}}.\]

Then, by Lemma 18, for \(\tau\geq\log(2)\), with probability at least \(1\) - \(2e^{-\tau}\), for \(\sqrt{n\lambda}\geq 8\tau\kappa\sqrt{\max\{\mathcal{N}(\lambda),1\}}\), we have

\[\left\|\hat{C}_{X,\lambda}^{-1}C_{X,\lambda}\right\|_{\mathcal{H}\to\mathcal{H }}\leq 2.\]

Since \(\mathcal{N}(\lambda)\leq c_{2,1}\lambda^{-p}\) by Lemma 16, and \(\lambda\leq 1\), it suffices to verify that \(\lambda\) satisfies

\[\sqrt{n\lambda}\geq 8\tau\kappa\max\{\sqrt{c_{2,1}},1\}\lambda^{-\frac{p}{2}}.\]

Since \(\lambda\geq n^{-\frac{1}{2\tau p}}\) by assumption, we deduce the sufficient condition \(n\geq(\tau c_{0})^{2(2+p)}\), where \(c_{0}:=8\kappa\max\{\sqrt{c_{2,1}},1\}\).

We bound the third term using Lemma 16 [33]. For \(\tau\geq\log(2)\), with probability at least \(1-2e^{-\tau}\), we have

\[\lambda^{-\frac{1}{2}}\left\|C_{X,\lambda}^{-\frac{1}{2}}(C_{X}-\hat{C}_{X})C_ {X,\lambda}^{-\frac{1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\leq\frac{4 \kappa^{2}\xi_{\delta}}{3n\lambda^{\frac{3}{2}}}+\sqrt{\frac{2\kappa^{2}\xi_{ \delta}}{n\lambda^{2}}},\]

where we define

\[\xi_{\delta}:=\log\frac{2\kappa^{2}(\mathcal{N}_{1}(\lambda)+1)}{e^{-\tau} \left\|C_{X}\right\|_{\mathcal{H}\to\mathcal{H}}}.\]

By assumption \(\lambda\geq n^{-\frac{1}{2\tau p}}\). We thus have

\[n\lambda^{\frac{3}{2}}\geq n^{\frac{1+2p}{4+2p}}\qquad\text{ and }\qquad n \lambda^{2}\geq n^{\frac{p}{2\tau p}}.\]

On the other hand, since \(1\geq\lambda\geq n^{-\frac{1}{2\tau p}}\), using Lemma 16, we have

\[\xi_{\delta}\leq\log\frac{82(c_{2,1}\lambda^{-p}+1)}{e^{-\tau}\left\|C_{X} \right\|_{\mathcal{H}\to\mathcal{H}}}\leq\log\frac{2(c_{2,1}+1)n^{\frac{p}{2 \tau p}}}{e^{-\tau}\left\|C_{X}\right\|_{\mathcal{H}\to\mathcal{H}}}\leq\log \frac{2(c_{2,l}+1)}{e^{-\tau}\left\|C_{X}\right\|_{\mathcal{H}\to\mathcal{H}}}+ \frac{p}{2+p}\log n.\]

The first term does not depend on \(n\), and the second term is logarithmic in \(n\). Putting everything together with a union bound, we get a bound on (17). With probability at least \(1\) - \(4e^{-\tau}\), for \(n\geq(c_{0}\tau)^{(4+2p)}\), we have

\[\left\|\left[C\left(C_{X,\lambda}^{-1}-\hat{C}_{X,\lambda}^{-1}\right)\right] \right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}\leq\left\|C\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\sqrt{2}\left(\frac{4\xi_{\delta}}{3n^{\frac{5+p}{2\tau p}}}+ \sqrt{\frac{2\xi_{\delta}}{n^{\frac{p}{2\tau p}}}}\right)=\tau o(1)\]

The derivations in the proof of Lemma 1 show that

\[[CC_{X,\lambda}^{-1}]=\sum_{i\in f,j\in J}a_{ij}\frac{\sqrt{\mu_{i}}}{\mu_{i}+ \lambda}d_{j}\otimes[e_{i}],\]with \(a_{ij}:=\{d_{j},C\sqrt{\mu_{i}}e_{i}\}_{\mathcal{Y}}\), \(i\in I,j\in J\). Note that since \([C]\in S_{2}([\mathcal{H}]^{2},\mathcal{Y})\), we have

\[\|[C]\|_{S_{2}([\mathcal{H}]^{2},\mathcal{Y})}^{2}=\left\|\sum_{i\in I,j\in J}a _{ij}d_{j}\otimes(\sqrt{\mu_{i}}e_{i})\right\|_{S_{2}([\mathcal{H}]^{2}, \mathcal{Y})}^{2}=\sum_{i\in I,j\in J}\frac{a_{ij}^{2}}{\mu_{i}}<+\infty.\]

Hence,

\[\left\|[CC_{X,\lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}^{2}=\sum_{ i\in I,j\in J}a_{ij}^{2}\frac{\mu_{i}}{(\mu_{i}+\lambda)^{2}}\leq\sum_{i\in I,j\in J}\frac{a_{ij}^{2}}{\mu_{i}}=\|[C]\|_{S_{2}([\mathcal{H}]^{2},\mathcal{Y })}^{2}<+\infty.\] (18)

Using the equality \(a^{2}-b^{2}=(a-b)(a+b)\) and the reverse triangular inequality, we obtain the following bound, with probability at least \(1-4e^{-\tau}\), for \(n\geq(c_{0}\tau)^{(4+2p)}\),

\[\left\|\left[[CC_{X,\lambda}^{-1}]\right]\right\|_{S_{2}(L_{2}(\pi ),\mathcal{Y})}^{2}-\left\|[C\hat{C}_{X,\lambda}^{-1}]\right\|_{S_{2}(L_{2}( \pi),\mathcal{Y})}^{2}\right\|\] \[\leq \left\|\left[C\left(C_{X,\lambda}^{-1}-\hat{C}_{X,\lambda}^{-1} \right)\right]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}\left(\left\|[CC_{X, \lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}+\left\|[C\hat{C}_{X, \lambda}^{-1}]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}\right)\] \[\leq \tau o(1)\left(2\left\|[CC_{X,\lambda}^{-1}]\right\|_{S_{2}(L_{2} (\pi),\mathcal{Y})}+\left\|[C\left(C_{X,\lambda}^{-1}-\hat{C}_{X,\lambda}^{-1} \right)]\right\|_{S_{2}(L_{2}(\pi),\mathcal{Y})}\right)\] \[\leq \tau o(1)\left(2\left\|[C]\right\|_{S_{2}([\mathcal{H}]^{2}, \mathcal{Y})}+\tau o(1)\right)\] \[= \tau^{2}o(1),\]

where in the second last line we used Equation (18). 

**Lemma 3**.: _Fix \(x\in\mathcal{X}\) and \(f_{x,\lambda}\) as in Definition 8. For \(\tau\geq\log(2)\), with probability at least \(1-2e^{-\tau}\) (note that this event depends on \(x\)),_

\[\left\|\left|f_{x,\lambda}\right|_{2,n}^{2}-\left\|[f_{x,\lambda}]\right\|_{L_{ 2}(\pi)}^{2}\right\|\leq\frac{1}{2}\|[f_{x,\lambda}]\|_{L_{2}(\pi)}^{2}+\frac{ 5\tau\kappa^{2}}{3\lambda^{2}n}.\]

Proof.: We start with

\[\left\|f_{x,\lambda}\right\|_{\infty}\leq\kappa\|f_{x,\lambda}\|_{\mathcal{H}} \leq\kappa^{2}\lambda^{-1}.\]

We apply Proposition 3 to \(f=f_{x,\lambda}\), with \(M=\kappa^{2}\lambda^{-1}\). For \(\tau\geq\log(2)\), with probability at least \(1-2e^{-\tau}\),

\[\left\|f_{x,\lambda}\right\|_{2,n}^{2}-\left\|[f_{x,\lambda}]\right\|_{L_{2}( \pi)}^{2}\right\|\leq\frac{1}{2}\|[f_{x,\lambda}]\|_{L_{2}(\pi)}^{2}+\frac{5 \tau\kappa^{2}}{3\lambda^{2}n}.\]

**Lemma 4**.: _Suppose that \(\mathcal{X}\) is a compact set in \(\mathbb{R}^{d}\) and that \(k\in C^{\theta}(\mathcal{X}\times\mathcal{X})\) for \(\theta\in(0,1]\) (Definition 11). Assume that \(1\geq\lambda\geq n^{-\frac{1}{2+\theta}}\). With probability at least \(1-2e^{-\tau}\), it holds for all \(x\in\mathcal{X}\) simultaneously that_

\[\|C_{X,\lambda}^{-1}k(x,\cdot)\|_{2,n}^{2} \geq\frac{1}{2}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}^{2}- \tau o(1),\] \[\|C_{X,\lambda}^{-1}k(x,\cdot)\|_{2,n}^{2} \leq\frac{3}{2}\|[C_{X,\lambda}^{-1}k(x,\cdot)]\|_{L_{2}(\pi)}^{2}+ \tau o(1).\]

Proof.: The proof follows [29, Lemma C.11]. As we use different notations and tracking of constants, we provide a similar proof in our setting for completeness. By Lemma 24, there exists an \(\epsilon\)-net \(\mathcal{F}\subseteq\mathcal{K}_{\lambda}\subseteq\mathcal{H}\) with respect to \(\|\cdot\|_{\infty}\) such that there exists a positive constant \(c\) with

\[|\mathcal{F}|\leq c(\lambda\epsilon)^{-\frac{2d}{\theta}},\]

for \(\epsilon\) to be determined later. Using Lemma 3 and a union bound over the finite set \(\mathcal{F}\), with probability at least \(1-2e^{-\tau}\), it holds simultaneously for all \(f\in\mathcal{F}\) that

\[\left\|f\right\|_{2,n}^{2}-\left\|[f]\right\|_{L_{2}(\pi)}^{2}\leq\frac{1}{2} \|[f]\|_{L_{2}(\pi)}^{2}+\frac{5(\tau+\log(|\mathcal{F}|))\kappa^{2}}{3\lambda^ {2}n}.\] (19)We work in the event where Equation (19) holds for all \(f\in\mathcal{F}\). By definition of an \(\epsilon\)-net and \(\mathcal{K}_{\lambda}\), for any \(x\in\mathcal{X}\), there exists some \(f\in\mathcal{F}\) such that

\[\big{\|}C_{X,\lambda}^{-1}k(x,\cdot)-f\big{\|}_{\infty}\leq\epsilon,\]

which in particular implies that

\[\big{\|}[\big{[}C_{X,\lambda}^{-1}k(x,\cdot)\big{]}]_{L_{2}(\pi)}- \big{\|}[f]\big{\|}_{L_{2}(\pi)}\big{|}\leq\epsilon\] \[\big{\|}C_{X,\lambda}^{-1}k(x,\cdot)\big{\|}_{2,n}-\big{\|}f\big{\|} _{2,n}\big{|}\leq\epsilon.\]

Since \(\big{|}C_{X,\lambda}^{-1}k(x,\cdot)\big{\|}_{\infty}\leq\kappa^{2}\lambda^{-1}\), using the algebraic identity \(a^{2}-b^{2}=(a-b)(2b+(a-b))\), we obtain

\[\big{\|}[C_{X,\lambda}^{-1}k(x,\cdot)]\big{\|}_{L_{2}(\pi)}^{2}- \big{\|}[f]\big{\|}_{L_{2}(\pi)}^{2}\big{|}\leq\epsilon(2\kappa^{2}\lambda^{- 1}+\epsilon)\] \[\big{\|}C_{X,\lambda}^{-1}k(x,\cdot)\big{\|}_{2,n}^{2}-\big{\|}f \big{\|}_{2,n}^{2}\big{|}\leq\epsilon(2\kappa^{2}\lambda^{-1}+\epsilon).\]

We therefore have,

\[\big{\|}C_{X,\lambda}^{-1}k(x,\cdot)\big{\|}_{2,n}^{2} \leq\big{\|}f\big{\|}_{2,n}^{2}+\epsilon(2\kappa^{2}\lambda^{-1}+\epsilon)\] \[\leq\frac{3}{2}\big{\|}[f]\big{\|}_{L_{2}(\pi)}^{2}+\frac{5(\tau +\log(|\mathcal{F}|)\kappa^{2}}{3\lambda^{2}n}+\epsilon(2\kappa^{2}\lambda^{- 1}+\epsilon)\] \[\leq\frac{3}{2}\big{\|}[C_{X,\lambda}^{-1}k(x,\cdot)]\big{\|}_{L _{2}(\pi)}^{2}+\frac{5(\tau+\log(|\mathcal{F}|)\kappa^{2}}{3\lambda^{2}n}+2 \epsilon(2\kappa^{2}\lambda^{-1}+\epsilon).\]

We now choose \(\epsilon=\frac{1}{n}\) and bound the error term. Recall that \(1\geq\lambda\geq n^{-\frac{1}{2\tau p}}\), therefore,

\[\frac{5(\tau+\log(|\mathcal{F}|)\kappa^{2}}{3\lambda^{2}n}+2 \epsilon(2\kappa^{2}\lambda^{-1}+\epsilon) \leq\frac{5(\tau+\log(|\mathcal{F}|)\kappa^{2}}{3}n^{-\frac{p}{2 \tau p}}+2\left(2\kappa^{2}n^{\frac{-1-p}{2\tau p}}+\frac{1}{n^{2}}\right)\] \[\leq\frac{5\kappa^{2}}{3}\left(\tau+\log(c\lambda^{-\frac{2d}{q} }n^{\frac{2d}{q}})\right)n^{-\frac{p}{2\tau p}}+2\left(2\kappa^{2}n^{\frac{ -1-p}{2\tau p}}+\frac{1}{n^{2}}\right)\] \[=\tau o(1).\]

**Lemma 5**.: _For \(1\geq\lambda\geq n^{-\frac{1}{2\tau p}}\), with probability at least \(1-4e^{-\tau}\), for \(n\geq(c_{0}\tau)^{4+2p}\), we have for all \(x\in\mathcal{X}\) simultaneously_

\[\left\|\hat{C}_{X}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-1}(C_{X}- \hat{C}_{X})C_{X,\lambda}^{-1}k(x,\cdot)\right\|_{\mathcal{H}}=\tau o(1),\] (20)

_where \(c_{0}\) is the same constant as in Lemma 2._

Proof.: \[\left\|\hat{C}_{X}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-1}(C_{X}- \hat{C}_{X})C_{X,\lambda}^{-1}k(x,\cdot)\right\|_{\mathcal{H}}\] \[=\left\|\hat{C}_{X}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-\frac{1}{2} }\hat{C}_{X,\lambda}^{-\frac{1}{2}}\hat{C}_{X,\lambda}^{\frac{1}{2}}C_{X, \lambda}^{-\frac{1}{2}}(C_{X}-\hat{C}_{X})C_{X,\lambda}^{-1}k(x,\cdot)\right\| _{\mathcal{H}}\] \[\leq\left\|\hat{C}_{X}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-\frac{1} {2}}\right\|_{\mathcal{H}\to\mathcal{H}}\left\|\hat{C}_{X,\lambda}^{-1}\hat{C }_{X,\lambda}^{\frac{1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\] \[\cdot\left\|C_{X,\lambda}^{-1}(C_{X}-\hat{C}_{X})C_{X,\lambda}^{- \frac{1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\left\|C_{X,\lambda}^{-\frac{1 }{2}}k(x,\cdot)\right\|_{\mathcal{H}}\]

We already saw in the proof of Lemma 2 that the first term is bounded by \(1\) and there is a constant \(c_{0}>0\) such that for \(\tau\geq\log(2)\), with probability at least \(1-2e^{-\tau}\), for \(n\geq(c_{0}\tau)^{4+2p}\), the second term is bounded by \(\sqrt{2}\). For the third term we also saw in the proof of Lemma 4 that for \(\tau\geq\log(2)\), with probability at least \(1-2e^{-\tau}\), we have

\[\lambda^{-\frac{1}{2}}\left\|C_{X,\lambda}^{-\frac{1}{2}}(C_{X}- \hat{C}_{X})C_{X,\lambda}^{-\frac{1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}} \leq\frac{4\kappa^{2}\xi_{\delta}}{3n\lambda^{\frac{3}{2}}}+\sqrt{\frac{2\kappa^ {2}\xi_{\delta}}{n\lambda^{2}}},\]where we defined

\[\xi_{\delta}=\log\frac{2\kappa^{2}(\mathcal{N}_{1}(\lambda)+1)}{e^{-\tau}\|C_{X} \|_{\mathcal{H}\to\mathcal{H}}}.\]

Finally, the fourth term is bounded above by \(\lambda^{-\frac{1}{2}}\kappa\). Note that the bound on the fourth term is independent of \(x\), so it holds simultaneously for all \(x\in\mathcal{X}\). This is in contrast with the setting of Lemma 4 where for each fixed \(x\in\mathcal{X}\) corresponds an element in the \(\epsilon\)-net of \(\mathcal{F}\) for which we have a high probability bound, and therefore we must use a union bound in order for the bound to hold simultaneously for all \(x\in\mathcal{X}\) in the proof of Lemma 4. As in the proof of Lemma 4 since \(1\geq\lambda\geq n^{-\frac{1}{2+p}}\), we have

\[\xi_{\delta}\leq\log\frac{2(c_{2,l}+1)}{e^{-\tau}\|C_{X}\|_{\mathcal{H}\to \mathcal{H}}}+\frac{p}{2+p}\log n.\]

In the bound on \(\xi_{\delta}\) above, the first term does not depend on \(n\), and the second term is logarithmic in \(n\). Putting everything together by union bound, with probability at least \(1-4e^{-\tau}\), for \(n\geq(c_{0}\tau)^{4+2p}\), we have

**Remark 6** (Comparison to [29]).: _We explicit the differences between our proof strategy and the proof strategy of [29]._

* _Scalar versus vector-valued: lower bounding the bias in our case require us to accommodate for the vector-valued setting (see Lemma_ 1_)._
* _New proof of the bias: we lower bound the bias through Lemma_ 2_, while_ _[_29_]_ _obtain the lower bound in Lemma_ 7_; however the proof of Lemma_ 7 _implicitly uses the equality_ \(\|A^{-1}\|=\|A\|^{-1}\)_, with_ \(\|\cdot\|\) _the operator norm, see Eq. (_69_)_ _[_29_]_ _and the preceding equations. It holds that_ \(\big{\|}A^{-1}\|\geq\big{|}A\big{|}^{-1}\)_, but_ \(\big{\|}A^{-1}\|\leq\big{\|}A\big{\|}^{-1}\) _may not hold in general. We therefore develop a new proof for this step, leading to Lemma_ 2_._
* _New proof of the variance: we lower bound the variance in Lemma_ 5_, while_ _[_29_]_ _lower bound the variance in Lemma_ 12_; to show Eq. (_20_),_ _[_29_]_ _use a covering argument involving_ \(\mathcal{N}(\mathcal{K}_{\lambda},\|\cdot\|_{\mathcal{H}},\epsilon)\) _(Lemma_ 10_). However, a close look at the proof of Lemma_ 10 _(last inequality of the proof) reveals that_ \(\frac{\lambda_{i}}{\lambda+\lambda_{i}}\) _was mistaken for_ \(\frac{\lambda}{\lambda+\lambda_{i}}\) _and plugging the correct term in the proof would lead to a vacuous bound. As explained in the proof of Lemma_ 5_, we therefore develop a proof that is free of a covering number argument for this step._

## Appendix C Learning rates for spectral algorithms

To upper bound the excess-risk, we use a decomposition involving the _approximation error_ expressed as \(F_{\lambda}-F_{*}\) and the _estimation error_ expressed as \(\hat{F}_{\lambda}-F_{\lambda}\).

\[\big{\|}[\hat{F}_{\lambda}]-F_{*}\big{\|}_{\gamma}\leq\big{\|}[\hat{F}_{ \lambda}-F_{\lambda}]\big{\|}_{\gamma}+\big{\|}[F_{\lambda}]-F_{*}\big{\|}_{ \gamma}\,,\]

where \(\hat{F}_{\lambda}\) is the empirical estimator based on general spectral regularization (Eq. (11)) and \(F_{\lambda}\) is its counterpart in population (Eq. (10)). Note that this is a different decomposition than the _bias-variance decomposition_ used in the proof of Theorem 3.

The proof structure is as follows:

1. Fourier expansion C.1.
2. Approximation Error C.2.
3. Estimation error C.3

### Fourier expansion

Recall the notations defined in Appendix A.3. The family \(\{d_{j}\}_{j\in J}\) is an ONB of \(\mathcal{Y}\), the family \(\{\mu_{i}^{1/2}e_{i}\}_{i\in I}\) is an ONB of \((\ker I_{\pi})^{\perp}\) and the family \(\{\tilde{e}_{i}\}_{i\in I^{\prime}}\) is an ONB of \(\ker I_{\pi}\) such that \(\left\{\mu_{i}^{1/2}e_{i}\right\}_{i\in I}\cup\{\tilde{e}_{i}\}_{i\in I^{ \prime}}\) forms an ONB of \(\mathcal{H}\). Furthermore, recall that \(\{\mu_{i}^{\beta/2}[e_{i}]\}_{i\in I}\) is an ONB of \([\mathcal{H}]^{\beta}\), \(\beta\geq 0\).

**Lemma 6** (Fourier expansion).: _Suppose Assumption (SRC) holds with \(\beta\geq 0\). By definition of the vector-valued interpolation space and by Theorem 5, we have_

\[F_{\star}=\sum_{i\in I,j\in J}a_{ij}d_{j}[e_{i}],\qquad a_{ij}= \left\{F_{\star},d_{j}[e_{i}]\right\}_{L_{2}(\pi;\mathcal{Y})},\qquad\left\|F_ {\star}\right\|_{\beta}^{2}=\sum_{i\in I,j\in J}\frac{a_{ij}^{2}}{\mu_{i}^{ \beta}}.\] (21)

_Then, we have the following equalities with respect to this Fourier decomposition._

1. _The Hilbert-Schmidt operator_ \(C_{\lambda}\in S_{2}(\mathcal{H},\mathcal{Y})\)_, Eq. (_10_), can be written as_ \[C_{\lambda}=\sum_{i\in I,j\in J}a_{ij}g_{\lambda}(\mu_{i})\sqrt{ \mu_{i}}d_{j}\otimes\sqrt{\mu_{i}}e_{i}.\] (22)
2. _The Hilbert-Schmidt operator_ \(\left(C_{YX}-C_{\lambda}C_{X}\right)C_{X,\lambda}^{-\frac{1}{2}}\in S_{2}( \mathcal{H},\mathcal{Y})\) _can be written as_ \[\left(C_{YX}-C_{\lambda}C_{X}\right)C_{X,\lambda}^{-\frac{1}{2}}= \sum_{i\in I,j\in J}a_{ij}r_{\lambda}(\mu_{i})\big{(}\mu_{i}+\lambda\big{)}^{ -\frac{1}{2}}\sqrt{\mu_{i}}\left(d_{j}\otimes\sqrt{\mu_{i}}e_{i}\right)\] (23)
3. _The Hilbert-Schmidt operator_ \(C_{YX}\in S_{2}(\mathcal{H},\mathcal{Y})\) _can be written as_ \[C_{YX}=\left(\sum_{i\in I,j\in J}a_{ij}\mu_{i}^{-\frac{\beta}{2}}d_{j}\otimes \sqrt{\mu_{i}}e_{i}\right)C_{X}^{\frac{\beta+1}{2}}\] (24)

Proof.: We first derive the Fourier expansion of \(C_{YX}\),

\[C_{YX} =\mathbb{E}_{X,Y}\left[Y\otimes\phi(X)\right]\] \[=\mathbb{E}_{X}\left[F_{\star}(X)\otimes\phi(X)\right]\] (25) \[=\mathbb{E}_{X}\left[\sum_{i\in I,j\in J}a_{ij}e_{i}(X)d_{j} \otimes\phi(X)\right]\] \[=\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}d_{j}\otimes\left(\sum_ {k\in I}\sqrt{\mu_{k}}e_{k}(X)\sqrt{\mu_{k}}e_{k}\right)\] \[=\sum_{ij}a_{ij}\sqrt{\mu_{k}}\cdot\mathbb{E}_{X}[e_{k}(X)e_{i}( X)]\cdot d_{j}\otimes(\sqrt{\mu_{k}}e_{k})\] \[=\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}d_{j}\otimes(\sqrt{\mu_ {i}}e_{i}),\] (26)

where in Eq. (25) we used the tower property of conditional expectation and in Eq. (26) we used the fact that \(\{[e_{i}]\}_{i\in I}\) forms an orthonormal system in \(L_{2}(\pi)\). We can manipulate Eq. (26) to derive Eq. (24),

\[C_{YX} =\sum_{i\in I,j\in J}a_{ij}\mu_{i}^{\frac{1}{2}-\frac{\beta+1}{2} }d_{j}\otimes\left(C_{X}^{\frac{\beta+1}{2}}(\sqrt{\mu_{i}}e_{i})\right)\] \[=\left(\sum_{i\in I,j\in J}a_{ij}\mu_{i}^{-\frac{\beta}{2}}d_{j} \otimes\sqrt{\mu_{i}}e_{i}\right)C_{X}^{\frac{\beta+1}{2}}.\]

By the spectral decomposition of \(C_{X}\) Eq. (6) and spectral calculus (Definition 9), we have that

\[g_{\lambda}(C_{X}) =\sum_{i\in I}g_{\lambda}(\mu_{i})\sqrt{\mu_{i}}e_{i}\otimes \sqrt{\mu_{i}}e_{i}+g_{\lambda}(0)\sum_{i\in I^{\prime}}\tilde{e}_{i}\otimes \tilde{e}_{i},\] (27) \[r_{\lambda}(C_{X}) =\sum_{i\in I}r_{\lambda}(\mu_{i})\sqrt{\mu_{i}}e_{i}\otimes \sqrt{\mu_{i}}e_{i}+\sum_{i\in I^{\prime}}\tilde{e}_{i}\otimes\tilde{e}_{i}.\] (28)where we used \(r_{\lambda}(0)=1\).

**Proof of Eq.** (22). Using Eq. (26) and (27), we have

\[C_{\lambda} =\left(\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}d_{j}\otimes(\sqrt{ \mu_{i}}e_{i})\right)\left(\sum_{k\in I}g_{\lambda}(\mu_{k})(\sqrt{\mu_{k}}e_{ k})\otimes(\sqrt{\mu_{k}}e_{k})+g_{\lambda}(0)\sum_{l\in I^{\prime}}\tilde{e}_{l} \otimes\tilde{e}_{l}\right)\] \[=\sum_{ijk}a_{ij}\sqrt{\mu_{i}}g_{\lambda}(\mu_{k})\delta_{ik}d_{ j}\otimes(\sqrt{\mu_{k}}e_{k})\] (29) \[=\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}g_{\lambda}(\mu_{i})d_{ j}\otimes(\sqrt{\mu_{i}}e_{i}),\]

where in Eq. (29), we recall the fact that \(\{\sqrt{\mu_{i}}e_{i}\}_{i\in I}\) forms an ONB of \((\ker I_{\pi})^{\perp}\) and \(\{\tilde{e_{i}}\}_{i\in I^{\prime}}\) forms an ONB of \(\ker I_{\pi}\).

**Proof of Eq.** (23). Using Eq. (26) and (28), we have

\[\left(C_{YX}- C_{\lambda}C_{X}\right)C_{X,\lambda}^{-\frac{1}{2}}=C_{YX}r_{ \lambda}(C_{X})C_{X,\lambda}^{-\frac{1}{2}}\] \[=\left(\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}d_{j}\otimes(\sqrt {\mu_{i}}e_{i})\right)\left(\sum_{k\in I}r_{\lambda}(\mu_{k})\sqrt{\mu_{k}}e_{ k}\otimes\sqrt{\mu_{k}}e_{k}+\sum_{i\in I^{\prime}}\tilde{e}_{l}\otimes\tilde{e}_{l} \right)C_{X,\lambda}^{-\frac{1}{2}}\] \[=\left(\sum_{ijk}a_{ij}\sqrt{\mu_{i}}r_{\lambda}(\mu_{k})d_{j} \otimes(\sqrt{\mu_{k}}e_{k})\delta_{ik}\right)C_{X,\lambda}^{-\frac{1}{2}}\] \[=\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}r_{\lambda}(\mu_{i})d_{j} \otimes(C_{X,\lambda}^{-\frac{1}{2}}(\sqrt{\mu_{i}}e_{i}))\] \[=\sum_{i\in I,j\in J}a_{ij}\sqrt{\mu_{i}}(\mu_{i}+\lambda)^{- \frac{1}{2}}r_{\lambda}(\mu_{i})d_{j}\otimes(\sqrt{\mu_{i}}e_{i}),\]

**Lemma 7**.: _Suppose Assumption (SRC) holds with \(\beta\geq 0\), then the following bound is satisfied, for all \(\lambda>0\) and \(0\leq\gamma\leq 1\), we have_

\[\|[F_{\lambda}]\|_{\gamma}^{2}\leq E^{2}\|F_{\star}\|_{\min(\gamma,\beta)}^{2} \lambda^{-(\gamma-\beta)\star}.\]

_For the definition of \(E\), see Eq. (8)._

Proof.: We adopt the notations of Lemma 6. By Parseval's identity and Eq. (22), we have

\[\|[F_{\lambda}]\|_{\gamma}^{2} =\|C_{\lambda}\|_{S_{2}(\{\mathcal{H}\}^{\gamma},\mathcal{Y})}^{2}\] \[=\sum_{i\in I,j\in J}a_{ij}^{2}g_{\lambda}(\mu_{i})^{2}\mu_{i}^{ 2-\gamma}.\]

In the case of \(\gamma\leq\beta\), we bound \(g_{\lambda}(\mu_{i})\mu_{i}\leq E\) using Eq. (8). Then, by Eq. (21),

\[\|[F_{\lambda}]\|_{\gamma}^{2}\leq E^{2}\sum_{i\in I,j\in J}\frac{a_{ij}^{2}} {\mu_{i}^{\gamma}}=E^{2}\|F_{\star}\|_{\gamma}^{2}.\]

In the case of \(\gamma>\beta\), we apply Eq. (8) to \(g_{\lambda}(\mu_{i})\mu_{i}^{1-\frac{\gamma-\beta}{2}}\leq E\lambda^{-\frac{ \gamma-\beta}{2}}\) to obtain, using Eq. (21) again,

\[\|[F_{\lambda}]\|_{\gamma}^{2} =\sum_{i\in I,j\in J}g_{\lambda}(\mu_{i})^{2}\mu_{i}^{2-(\gamma- \beta)}\mu_{i}^{-\beta}a_{ij}^{2}\] \[\leq E^{2}\lambda^{-(\gamma-\beta)}\sum_{i\in I,j\in J}\mu_{i}^{- \beta}a_{ij}^{2}\] \[=E^{2}\lambda^{-(\gamma-\beta)}\|F_{\star}\|_{\beta}^{2}.\]

**Lemma 8**.: _Suppose Assumption (SRC) holds for \(0\leq\beta\leq 2\rho\), with \(\rho\) the qualification. Then, the following bound is satisfied, for all \(\lambda>0\), we have_

\[\left\|\left(C_{YX}-C_{\lambda}C_{X}\right)C_{X,\lambda}^{\frac{-1}{2}}\right\| _{S_{2}(\mathcal{H},\mathcal{Y})}\leq\omega_{\rho}\big{|}F_{*}\big{|}_{\beta} \lambda^{\frac{\beta}{2}}.\]

_For the definition of \(\omega_{\rho}\), see Eq. (9)._

Proof.: Recall that in Lemma 6 we used the decomposition

\[F_{*}=\sum_{i\in\mathcal{I},j\in J}a_{ij}d_{j}[e_{i}],\]

where Assumption (SRC) implies that \(\left\|F_{*}\right\|_{\beta}^{2}=\sum_{ij}\frac{a_{ij}^{2}}{\mu_{i}^{\beta}}<\infty\). Using Eq. (23) in Lemma 6 and Parseval's identity w.r.t. the ONS \(\{d_{j}\otimes\mu_{i}^{1/2}e_{i}\}_{i\in\mathcal{I},j\in J}\) in \(S_{2}(\mathcal{H},\mathcal{Y})\), we have

\[\left\|\left(C_{YX}-C_{\lambda}C_{X}\right)C_{X,\lambda}^{-\frac{ 1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}= \left(\sum_{i\in\mathcal{I},j\in J}a_{ij}^{2}r_{\lambda}^{2}(\mu_ {i})(\mu_{i}+\lambda)^{-1}\mu_{i}\right)^{\frac{1}{2}}\] \[\leq \left(\sum_{i\in\mathcal{I},j\in J}\frac{a_{ij}^{2}}{\mu_{i}^{ \beta}}r_{\lambda}^{2}(\mu_{i})\mu_{i}^{\beta}\right)^{\frac{1}{2}}\] \[\leq \left\|F_{*}\right\|_{\beta}\sup_{i\in\mathcal{I}}r_{\lambda}( \mu_{i})\mu_{i}^{\frac{\beta}{2}}\] \[\leq \left\|F_{*}\right\|_{\beta}\omega_{\rho}\lambda^{\frac{\beta}{2}}.\]

**Lemma 9**.: _Suppose Assumption (SRC) holds with \(\beta\geq 0\), then for all \(\lambda>0\), we have_

\[\left\|C_{\lambda}r_{\lambda}\left(\hat{C}_{X}\right)\hat{C}_{X,\lambda}^{ \frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\leq B\left\|\hat{C}_{X, \lambda}^{\frac{1}{2}}r_{\lambda}(\hat{C}_{X})g_{\lambda}(C_{X})C_{X}^{\frac{ \beta+1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}},\]

_where \(\left\|F_{*}\right\|_{\beta}=B<\infty\)._

Proof.: Recall that Lemma 6 we used the decomposition

\[F_{*}=\sum_{i\in\mathcal{I},j\in\mathcal{I}}a_{ij}d_{j}[e_{i}],\]

where \(\left\|F_{*}\right\|_{\beta}^{2}=\sum_{ij}\frac{a_{ij}^{2}}{\mu_{i}^{\beta}} =B^{2}<\infty\). Using Eq. (24) in Lemma 6 and \(C_{\lambda}=C_{YX}g_{\lambda}(C_{X})\), we have

\[\left\|C_{\lambda}r_{\lambda}\left(\hat{C}_{X}\right)\hat{C}_{X, \lambda}^{\frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}= \left\|\left(\sum_{ij}a_{ij}\mu_{i}^{-\frac{\beta}{2}}d_{j} \otimes\sqrt{\mu_{i}}e_{i}\right)C_{X}^{\frac{\beta+1}{2}}g_{\lambda}(C_{X})r _{\lambda}(\hat{C}_{X})C_{X,\lambda}^{\frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\] \[\leq B\left\|C_{X}^{\frac{\beta+1}{2}}g_{\lambda}(C_{X})r_{ \lambda}(\hat{C}_{X})\hat{C}_{X,\lambda}^{\frac{1}{2}}\right\|_{\mathcal{H} \to\mathcal{H}},\]

where we notice that the \(S_{2}(\mathcal{H},\mathcal{Y})\) norm of the first term is exactly the \(\beta\) norm of \(F_{*}\), which is given by \(B\). Recalling that \(C_{X},\hat{C}_{X}\) are self adjoint, we prove the final result by taking the adjoint and using that an operator has the same operator norm as its adjoint. 

### Approximation Error

**Lemma 10**.: _Let \(F_{\lambda}\) be given by Eq. (10) based on a general spectral filter satisfying Definition 2 with qualification \(\rho\geq 0\). Suppose Assumption (SRC) holds with parameter \(\beta\geq 0\) and define \(\beta_{\rho}=\min\{\beta,2\rho\}\), then the following bound is satisfied, for all \(\lambda>0\) and \(0\leq\gamma\leq\beta_{\rho}\),_

\[\left\|\left[F_{\lambda}\right]-F_{*}\right\|_{\gamma}^{2}\leq\omega_{\rho}^{2} \left\|F_{*}\right\|_{\beta_{\rho}}^{2}\lambda^{\beta_{\rho}-\gamma}.\]Proof.: In Eq. (10), we defined \(F_{\lambda}(\cdot)=C_{\lambda}\phi(\cdot)\). On the other hand, in Lemma 6 we obtained the Fourier expansion of \(C_{\lambda}\) leading to Eq. (22). Thus we have for \(\pi-\)almost all \(x\in\mathcal{X}\),

\[F_{\lambda}(x)=\sum_{i\in I,j\in J}a_{ij}\mu_{i}g_{\lambda}(\mu_{i})d_{j}e_{i}( x).\]

Therefore,

\[[F_{\lambda}]-F_{*}=\sum_{i\in I,j\in J}a_{ij}(1-\mu_{i}g_{\lambda}(\mu_{i}))d_ {j}[e_{i}]=\sum_{i\in I,j\in J}a_{ij}r_{\lambda}(\mu_{i})d_{j}[e_{i}].\]

Suppose \(\beta\leq 2\rho\), using Parseval's identity w.r.t. the ONB \(\{d_{j}\mu_{i}^{\gamma/2}[e_{i}]\}_{i\in I,j\in J}\) of \([\mathcal{G}]^{\gamma}\), we have

\[\left\|[F_{\lambda}]-F_{*}\right\|_{\gamma}^{2} =\left\|\sum_{i\in I,j\in J}\frac{a_{ij}}{\mu_{i}^{\gamma/2}}r_{ \lambda}(\mu_{i})d_{j}\mu_{i}^{\gamma/2}[e_{i}]\right\|_{\gamma}^{2}\] \[=\sum_{i\in I,j\in J}\frac{a_{ij}^{2}}{\mu_{i}^{\gamma}}r_{ \lambda}^{2}(\mu_{i})\] \[=\sum_{i\in I,j\in J}\frac{a_{ij}^{2}}{\mu_{i}^{\gamma}}r_{ \lambda}^{2}(\mu_{i})\mu_{i}^{\beta-\gamma}\] \[\leq\omega_{\rho}^{2}\lambda^{\beta-\gamma}\sum_{i\in I,j\in J} \frac{a_{ij}^{2}}{\mu_{i}^{\beta}}\] \[=\left\|F_{*}\right\|_{\beta}^{2}\omega_{\rho}^{2}\lambda^{\beta -\gamma}\]

where we used Eq. (9) in the definition of a filter function, together with \(0\leq\beta\leq 2\rho\) and \(0\leq\gamma\leq\beta\), which taken together implies that \(0\leq\frac{\beta-\gamma}{2}\leq\rho\). Finally, if \(\beta\geq 2\rho\), then since \([\mathcal{G}]^{\beta}\subseteq[\mathcal{G}]^{2\rho}\), we can perform the last derivations again with \(\beta=2\rho\) to obtain the final result. 

### Estimation error

Before proving the main results we recall two _embedding properties_ for the vector-valued interpolation space \([\mathcal{G}]^{\beta}\) (Definition 1). The first embedding property lifts the property (EMB) defined for the scalar-valued RKHS \([\mathcal{H}]^{\alpha}\) to the vector-valued RKHS \([\mathcal{G}]^{\alpha}\).

**Lemma 11** (\(L_{\infty}\)-embedding property - Lemma 4 [31]).: _Under (EMB) the inclusion operator \(\mathcal{I}_{\pi}^{\alpha,\beta}:[\mathcal{G}]^{\alpha}\hookrightarrow L_{ \infty}(\pi;\mathcal{Y})\) is bounded with operator norm \(A\),_

**Theorem 6** (\(L_{q}\)-embedding property - Theorem 3 [31]).: _Let Assumption (EMB) be satisfied with parameter \(\alpha\in(0,1]\). For any \(\beta\in[0,\alpha)\), the inclusion map_

\[\mathcal{I}_{\pi}^{q_{\alpha,\beta}}:[\mathcal{G}]^{\beta}\hookrightarrow L_{ q_{\alpha,\beta}}(\pi;\mathcal{Y})\]

_is bounded, where \(q_{\alpha,\beta}:=\frac{2\alpha}{\alpha-\beta}\)._

The \(L_{q}\)-embedding property was first introduced in the scalar-valued setting in [55] and then lifted to the vector-valued setting by [31]. Its role is to replace a boundedness condition on the ground truth function \(F_{*}\). We now explain how the \(L_{q}\)-embedding property can be combined with Assumption (EMB) and a truncation technique.

**Lemma 12**.: _Recall that \(\pi\) is the marginal measure of \(X\) on \(\mathcal{X}\). For \(t\geq 0\), define the measurable set \(\Omega_{t}\) as follows_

\[\Omega_{t}\coloneqq\{x\in\mathcal{X}:\left\|F_{*}(x)\right\|_{\mathcal{Y}} \leq t\}\]

_Let \(q>0\). Assume that \(F_{*}\in L_{q}(\pi;\mathcal{Y})\). In other words, there exists some constant \(c_{q}>0\) such that_

\[\left\|F_{*}\right\|_{L_{q}(\pi;\mathcal{Y})}=\left(\int_{\mathcal{X}}\left\|F _{*}(x)\right\|_{\mathcal{Y}}^{q}\mathrm{d}\pi(x)\right)^{\frac{1}{q}}=c_{q}<+\infty,\]

_Then we have the following conclusions_

1. _The_ \(\pi\)_-measure of the complement of_ \(\Omega\) _can be bounded by_ \[\pi(\{x\notin\Omega_{t}\})\leq\frac{c_{q}^{q}}{t^{q}}.\]2. _Recall that_ \(\{x_{i}\}_{i=1}^{n}\) _are i.i.d. samples distributed according to_ \(\pi\)_. If_ \(t=n^{\frac{1}{q}}\) _for_ \(\tilde{q}<q\)_, then we can conclude as follows. For a fixed parameter_ \(\tau>0\)_, for all sufficiently large_ \(n\)_, where the hidden index bound depends on_ \(q\tilde{q}^{-1}\) _and_ \(\tau\)_, we have_ \[\pi^{\otimes n}\left(\cap_{i=1}^{n}\{x_{i}\in\Omega_{t}\}\right)\geq 1-e^{-\tau}.\]

Proof.: The first claim is a straightforward application of Markov's inequality, as follows

\[\pi(\{x\notin\Omega_{t}\})=\pi\left(\|F_{*}(x)\|_{\mathcal{Y}}>t\right)\leq \frac{\mathbb{E}_{\pi}\left[\|F_{*}(X)\|_{\mathcal{Y}}^{q}\right]}{tq}=\frac{ c_{q}^{q}}{t^{q}}.\]

To show the second claim, we first evaluate the probability that there exists some \(x_{i}\)'s that lies outside \(\Omega_{t}\),

\[\pi^{\otimes n}\left(\cup_{i=1}^{n}\{x_{i}\notin\Omega_{t}\}\right) =1-\pi^{\otimes n}\left(\cap_{i=1}^{n}\{x_{i}\in\Omega_{t}\}\right)\] \[=1-\pi\big{(}\{x_{i}\in\Omega_{t}\}\big{)}^{n}\] \[\leq 1-\left(1-\frac{c_{q}^{q}}{t^{q}}\right)^{n}\] \[\leq\frac{c_{q}^{q}n}{t^{q}},\]

where in the last inequality we used Bernoulli's inequality, which states that for \(r\geq 1\) and \(0\leq x\leq 1\),

\[(1-x)^{r}\geq 1-rx.\]

By assumption \(t=n^{\frac{1}{q_{t}}}\) for some fixed \(q>q_{t}>0\). We thus have

\[\pi^{\otimes n}\left(\cup_{i=1}^{n}\{x_{i}\notin\Omega_{t}\}\right)\leq c_{q} ^{q}n^{1-\frac{q}{q_{t}}}\leq e^{-\tau},\]

for sufficiently large \(n\), where the hidden index bound depends on \(\frac{q}{q_{t}}\) and \(\tau\). 

We adapt [31, Lemma 5] to the spectral algorithms setting.

**Lemma 13**.: _Suppose Assumptions (SRC) and (EMB) hold for some \(0\leq\beta\leq 2\rho\), with \(\rho\) the qualification, then the following bounds are satisfied, for all \(0<\lambda\leq 1,\)_

\[\left\|[F_{\lambda}]-F_{*}\right\|_{L_{\infty}}^{2} \leq\left(\|F_{*}\|_{L_{\infty}}+A\max\{E,\omega_{\rho}\}\|F_{*} \|_{\beta}\right)^{2}\lambda^{\beta-\alpha},\] (30) \[\left\|[F_{\lambda}]\right\|_{L_{\infty}}^{2} \leq A^{2}E^{2}\|F_{*}\|_{\min\{\alpha,\beta\}}^{2}\lambda^{-( \alpha-\beta)_{*}},\] (31)

Proof.: We use Lemma 11 and Lemma 7 to write:

\[\left\|[F_{\lambda}]\right\|_{\infty}^{2}\leq A^{2}\big{\|}[F_{\lambda}] \big{\|}_{\alpha}^{2}\leq A^{2}E^{2}\|F_{*}\|_{\min\{\alpha,\beta\}}^{2} \lambda^{-(\alpha-\beta)_{*}}.\]

This proves Eq. (31). To show Eq. (30), in the case \(\beta\leq\alpha\) we use the triangle inequality, Eq. (31) and \(\lambda\leq 1\) to obtain

\[\left\|[F_{\lambda}]-F_{*}\right\|_{\infty} \leq\|F_{*}\|_{\infty}+\|[F_{\lambda}]\|_{\infty}\] \[\leq\big{(}\|F_{*}\|_{\infty}+AE\|F_{*}\|_{\beta}\big{)}\lambda^ {-\frac{\alpha-\beta}{2}}.\]

In the case \(\beta>\alpha\), Eq. (30) is a consequence of Lemma 11 and Lemma 10 with \(\gamma=\alpha\) (here we use the assumption \(0\leq\beta\leq 2\rho\)),

\[\left\|[F_{\lambda}]-F_{*}\right\|_{\infty}^{2}\leq A^{2}\|[F_{\lambda}]-F_{ *}\|_{\alpha}^{2}\leq A^{2}\omega_{\rho}^{2}\|F_{*}\|_{\beta}^{2}\lambda^{ \beta-\alpha}\leq(\|F_{*}\|_{\infty}+A\omega_{\rho}\|F_{*}\|_{\beta})^{2} \lambda^{\beta-\alpha}.\]

We adapt [54, Theorem 13] to the vector-valued setting.

**Theorem 7**.: _Suppose that Assumptions (EMB), (EVD), (MOM) and (SRC) hold for \(0\leq\beta\leq 2\rho\), where \(\rho\) is the qualification, and \(p\leq\alpha\leq 1\). Denote, for \(i=1,\ldots,n\),_

\[\xi_{i}=\xi\big{(}x_{i},y_{i}\big{)}=\big{(}(y_{i}-C_{\lambda}\phi(x_{i})) \otimes\phi(x_{i})\big{)}\,C_{X,\lambda}^{-\frac{1}{2}},\]

_and for \(t\geq 0\),_

\[\Omega_{t}=\{x\in\mathcal{X}:\|F_{*}(x)\|_{\mathcal{Y}}\leq t\}\]

_Then for all \(\tau\geq 1\), with probability at least \(1-2e^{-\tau}\), we have_

\[\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbb{1}\{x_{i}\in\Omega _{t}\}-\mathbb{E}[\xi(X,Y)\mathbb{1}\{X\in\Omega_{t}\}]\right\|_{S_{2}(\mathcal{ H},\mathcal{Y})}\] \[\leq \tau\Bigg{(}c_{1}\lambda^{\frac{\beta}{2}-\alpha}n^{-1}+c_{2} \lambda^{-\frac{\alpha}{2}}n^{-1}(t+R+A)+\frac{c_{3}\sqrt{\mathcal{N}_{1}( \lambda)}}{\sqrt{n}}+\frac{c_{4}}{\sqrt{n}\lambda^{\frac{\alpha-\beta}{2}}} \Bigg{)}\]

_where \(R\) is the constant from Assumption (MOM), and_

\[c_{1} =8\sqrt{2}A^{2}\max\{E,\omega_{\rho}\}\|F_{*}\|_{\beta}\] \[c_{2} =8\sqrt{2}A\] \[c_{3} =8\sqrt{2}\sigma\] \[c_{4} =8\sqrt{2}A\|F_{*}\|_{\beta}\omega_{\rho}\]

_where \(A\) is the constant from Assumption (EMB), and \(E,\omega_{\rho}\) are defined in Eq. (8) and (9) respectively._

Proof.: We wish to apply vector-valued Bernstein's inequality, namely Theorem 10. We thus compute,

\[\mathbb{E}\big{[}\|\xi(X,Y)\mathbb{1}\{X\in\Omega_{t}\}\big{]}_{ S_{2}(\mathcal{H},\mathcal{Y})}^{m}\Big{]} =\mathbb{E}\left[\mathbb{1}\{X\in\Omega_{t}\}\left\|(Y-C_{\lambda }\phi(X))\otimes\left(C_{X,\lambda}^{-\frac{1}{2}}\phi(X)\right)\right\|_{S_{ 2}(\mathcal{H},\mathcal{Y})}^{m}\right]\] \[=\mathbb{E}\left[\mathbb{1}\{X\in\Omega_{t}\}\left\|(Y-C_{ \lambda}\phi(X))\right\|_{\mathcal{Y}}^{m}\left\|C_{X,\lambda}^{-\frac{1}{2}} \phi(X)\right\|_{\mathcal{H}}^{m}\right]\] \[=\int_{\Omega_{t}}\left\|C_{X,\lambda}^{-\frac{1}{2}}\phi(x) \right\|_{\mathcal{H}}^{m}\int_{\mathcal{Y}}\left\|y-C_{\lambda}\phi(x)\right\| _{\mathcal{Y}}^{m}\mathrm{d}p(x,\mathrm{d}y)\mathrm{d}\pi(x).\] (32)

First we consider the inner integral, by Assumption (MOM),

\[\int_{\mathcal{Y}}\left\|(y-C_{\lambda}\phi(x))\right\|_{\mathcal{ Y}}^{m}\mathrm{d}p(x,\mathrm{d}y) \leq 2^{m-1}\left(\int_{\mathcal{Y}}\left\|y-F_{*}(x)\right\|_{ \mathcal{Y}}^{m}+\left\|F_{\lambda}(x)-F_{*}(x)\right\|_{\mathcal{Y}}^{m} \right)\mathrm{d}p(x,\mathrm{d}y)\] \[=m!\sigma^{2}(2R)^{m-2}+2^{m-1}\left\|F_{\lambda}(x)-F_{*}(x) \right\|_{\mathcal{Y}}^{m}.\]

Plugging the above inequality into Eq. (32), as well as introducing the shorthand,

\[h_{x}:=C_{X,\lambda}^{-\frac{1}{2}}\phi(x),\]

we have

\[\mathbb{E}\left[|\xi(X,Y)\mathbb{1}\{X\in\Omega_{t}\}|_{S_{2}( \mathcal{H},\mathcal{Y})}^{m}\right] \leq m!\sigma^{2}(2R)^{m-2}\int_{\Omega_{t}}|h_{x}\|_{\mathcal{H }}^{m}\mathrm{d}\pi(x)\] (33) \[+2^{m-1}\int_{\Omega_{t}}\|h_{x}\|_{\mathcal{H}}^{m}\left\|F_{ \lambda}(x)-F_{*}(x)\right\|_{\mathcal{Y}}^{m}\mathrm{d}\pi(x).\]

We bound term (33) using Lemma 15 and Lemma 17 with \(l=1\). We have,

\[\int_{\Omega_{t}}\|h_{x}\|_{\mathcal{H}}^{m}\mathrm{d}\pi(x)\leq(A\lambda^{- \frac{\alpha}{2}})^{m-2}\mathcal{N}_{1}(\lambda).\]

Therefore we bound term (33) as follows,

\[m!\sigma^{2}(2R)^{m-2}\int_{\Omega_{t}}\|h_{x}\|_{\mathcal{H}}^{m}\mathrm{d} \pi(x)\leq m!\sigma^{2}\left(\frac{2AR}{\lambda^{\frac{\alpha}{2}}}\right)^{m- 2}\mathcal{N}_{1}(\lambda).\]If \(\beta\geq\alpha\), by Assumption (EMB),

\[\|F_{*}\|_{\infty}\leq A\|F_{*}\|_{\alpha}\leq A\|F_{*}\|_{\beta}.\]

Hence by Lemma 13,

\[\|[F_{\lambda}]-F_{*}\|_{\infty}\leq(\|F_{*}\|_{\infty}+A\max\{E,\omega_{\rho} \}\|F_{*}\|_{\beta})\lambda^{\frac{\beta-\alpha}{2}}\leq A(1+\max\{E,\omega_{ \rho}\})\|F_{*}\|_{\beta}\lambda^{\frac{\beta-\alpha}{2}}.\]

If \(\beta<\alpha\), by Lemma 13, we have for \(\pi\)-almost all \(x\in\Omega_{t}\),

\[\|F_{*}(x)-F_{\lambda}(x)\|_{\mathcal{Y}}\leq t+\|[F_{\lambda}]\|_{L_{\infty}( \pi;\mathcal{Y})}\leq t+AE\|F_{*}\|_{\beta}\lambda^{\frac{\beta-\alpha}{2}}.\]

Therefore, for all \(\beta\in[0,2\rho]\),

\[\|(F_{*}-[F_{\lambda}])\mathbbm{1}_{X\in\Omega_{t}}\|_{L_{\infty}(\pi; \mathcal{Y})}\leq t+A(1+\max\{E,\omega_{\rho}\}\|F_{*}\|_{\beta}\lambda^{ \frac{\beta-\alpha}{2}})=:\chi(t,\lambda).\]

Using Lemma 17 with \(l=1\), we have,

\[2^{m-1}\int_{\Omega_{t}}\|h_{x}\|_{\mathcal{H}}^{m}\|F_{*}(x)-F_ {\lambda}(x)\|_{\mathcal{Y}}^{m}\mathrm{d}\pi(x)\] \[\leq 2^{m-1}\chi(t,\lambda)^{m-2}(A\lambda^{-\frac{\alpha}{2}})^{m} \|F_{*}-[F_{\lambda}]\|_{L_{2}(\pi;\mathcal{Y})}^{2}\] \[= \left(\frac{2\chi(t,\lambda)A}{\lambda^{\frac{\alpha}{2}}}\right) ^{m-2}\|F_{*}-[F_{\lambda}]\|_{L_{2}(\pi;\mathcal{Y})}^{2}\frac{2A^{2}}{ \lambda^{\alpha}}\] \[\leq m!\left(\frac{2\chi(t,\lambda)A}{\lambda^{\frac{\alpha}{2}}} \right)^{m-2}\|F_{*}-[F_{\lambda}]\|_{L_{2}(\pi;\mathcal{Y})}^{2}\frac{2A^{2}} {\lambda^{\alpha}}.\]

Putting everything together,

\[\mathbb{E}\left[\|\xi(X,Y)\mathbbm{1}\{X\in\Omega_{t}\}\|_{S_{2}(\mathcal{H}, \mathcal{Y})}^{m}\right]\leq m!\left(\frac{2(R+\chi(t,\lambda))A}{\lambda^{ \frac{\alpha}{2}}}\right)^{m-2}\left(\sigma^{2}\mathcal{N}_{1}(\lambda)+\|F_{* }-[F_{\lambda}]\|_{L_{2}(\pi;\mathcal{Y})}^{2}\frac{2A^{2}}{\lambda^{\alpha}} \right).\]

We now apply Theorem 10 with

\[L \leftarrow\frac{2(R+\chi(t,\lambda))A}{\lambda^{\frac{\alpha}{2}}}\] \[\sigma \gets 2\sigma\sqrt{\mathcal{N}_{1}(\lambda)}+\|F_{*}-[F_{ \lambda}]\|_{L_{2}(\pi;\mathcal{Y})}\frac{2A}{\lambda^{\frac{\alpha}{2}}}\]

We bound \(\|F_{*}-[F_{\lambda}]\|_{L_{2}(\pi;\mathcal{Y})}\) using Lemma 10 with \(\gamma=0\),

\[\|F_{*}-[F_{\lambda}]\|_{L_{2}(\pi;\mathcal{Y})}\leq\omega_{\rho}\|F_{*}\|_{ \beta}\lambda^{\frac{\beta}{2}}.\]

The conclusion is, for all \(\tau\geq 1\), with probability at least \(1-2e^{-\tau}\), we have

\[\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\{x_{i}\in \Omega_{t}\}-\mathbb{E}[\xi(X,Y)\mathbbm{1}\{X\in\Omega_{t}\}]\right\|_{S_{2}( \mathcal{H},\mathcal{Y})}\] \[\leq 4\sqrt{2}\tau\left(\frac{2\sigma\sqrt{\mathcal{N}_{1}(\lambda )}+|F_{*}-[F_{\lambda}]|_{L_{2}(\pi;\mathcal{Y})}\frac{2A}{\lambda^{\frac{ \alpha}{2}}}}{\sqrt{n}}+\frac{2(R+\chi(t,\lambda))A}{n\lambda^{\frac{\alpha}{2} }}\right)\] \[\leq 4\sqrt{2}\tau\left(\frac{2\sigma}{\sqrt{\mathcal{N}_{1}( \lambda)}}+\frac{2A\|F_{*}\|_{\beta}\omega_{\rho}}{\sqrt{n}\lambda^{\frac{\alpha -\beta}{2}}}+\frac{2(R+t+A)A}{n\lambda^{\frac{\alpha}{2}}}+\frac{2A^{2}\max\{E,\omega_{\rho}\}\|F_{*}\|_{\beta}}{n\lambda^{\alpha-\frac{\beta}{2}}}\right).\]

**Lemma 14**.: _Suppose that the same assumptions and notations listed in Theorem 7 hold._

1. _Suppose_ \(\beta+p>\alpha\)_, and_ \(\lambda\asymp n^{-\frac{1}{\beta+p}}\)_. For any fixed_ \(\tau\geq 1\)_, with probability at least_ \(1-2e^{-\tau}\)_, suppose that the truncation level_ \(t\) _satisfies_ \[t\leq n^{\frac{1}{2}\left(1+\frac{p-\alpha}{p+\beta}\right)},\] _then there exists a constant_ \(c>0\) _such that_ \[\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\{x_{i}\in\Omega_{t}\}- \mathbb{E}[\xi(X,Y)\mathbbm{1}\{X\in\Omega_{t}\}]\right\|_{S_{2}(\mathcal{H}, \mathcal{Y})}\leq c\tau n^{-\frac{1}{2}\frac{\beta}{\beta+p}}.\]2. _Suppose_ \(\beta+p\leq\alpha\)_, and_ \(\lambda\times\left(\frac{n}{\log^{\theta}(n)}\right)^{\frac{1}{\alpha}}\) _for some_ \(\theta>1\)_. For any fixed_ \(\tau\geq 1\)_, with probability at least_ \(1-2e^{-\tau}\)_, suppose that the truncation level_ \(t\) _satisfies_ \[t\leq n^{\frac{1}{2}\left(1-\frac{\beta}{\alpha}\right)},\] _then there exists a constant_ \(c>0\) _such that_ \[\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\left\{x_{i}\in\Omega\right\} -\mathbb{E}\big{[}\xi(X,Y)\mathbbm{1}\{X\in\Omega\}\big{]}\right\|_{\mathcal{G }}\leq c\tau\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{\beta}{2\alpha}}\]

Proof.: Note that Theorem 7 yields the same conclusion as in the scalar-valued case proved in [54, Theorem 13]. The Lemma then follows from the analysis for the scalar-valued case in the proof of [54, Theorem 15]. 

We adapt [55, Theorem 15] to the vector-valued setting.

**Theorem 8**.: _Suppose that Assumptions (EMB), (EVD), (MOM) and (SRC) hold for \(0\leq\beta\leq 2\rho\), where \(\rho\) is the qualification, and \(p\leq\alpha\leq 1\)._

1. _In the case of_ \(\beta+p>\alpha\)_, choosing_ \(\lambda\times n^{-\frac{1}{\beta+p}}\)_, for any fixed_ \(\tau\geq\log(4)\)_, when_ \(n\) _is sufficiently large, with probability at least_ \(1-4e^{-\tau}\)_, we have_ \[\left\|\left(\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)-\left(C_{YX}-C_ {\lambda}C_{X}\right)\right)C_{X,\lambda}^{-\frac{1}{2}}\right\|_{S_{2}( \mathcal{H},\mathcal{Y})}\leq c\tau n^{-\frac{1}{2}\frac{\beta}{\beta+p}}\] (34) _where_ \(c\) _is a constant independent of_ \(n,\tau,\lambda\)_._
2. _In the case of_ \(\beta+p\leq\alpha\)_, choosing_ \(\lambda\times\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{1}{\alpha}}\) _for some_ \(\theta>1\) _. We make the additional assumption that there exists some_ \(\alpha^{\prime}<\alpha\) _such that Assumption (_MOM_) is satisfied for_ \(\alpha^{\prime}<\alpha\)_. Then, for any fixed_ \(\tau\geq\log(4)\)_, when_ \(n\) _is sufficiently large, where the hidden index bound depends on_ \(\alpha-\alpha^{\prime}\)_, with probability at least_ \(1-4e^{-\tau}\)_, we have_ \[\left\|\left(\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)-\left(C_{YX}-C_ {\lambda}C_{X}\right)\right)C_{X,\lambda}^{-\frac{1}{2}}\right\|_{S_{2}( \mathcal{H},\mathcal{Y})}\leq c\tau\left(\frac{n}{\log^{\theta}(n)}\right)^{- \frac{\beta}{2\alpha}}\] (35) _where_ \(c\) _is a constant independent of_ \(n,\tau,\lambda\)_._

Proof.: By assumption (EMB) and Theorem 6, if \(\beta<\alpha\), then the inclusion map

\[\mathcal{I}_{\pi}^{q_{\alpha,\beta}}:[\mathcal{G}]^{\beta}\hookrightarrow L_{ q_{\alpha,\beta}}(\pi;\mathcal{Y})\]

is bounded, where \(q_{\alpha,\beta}:=\frac{2\alpha}{\alpha-\beta}\). If \(\beta\geq\alpha\), then by Lemma 11 the inclusion map

\[\mathcal{I}_{\pi}^{\infty}:[\mathcal{G}]^{\beta}\hookrightarrow L_{\infty}( \pi;\mathcal{Y})\]

is bounded and therefore \([\mathcal{G}]^{\beta}\) is continuously embedded into \(L_{q}(\pi;\mathcal{Y})\) for any \(q\geq 1\). In the rest of the proof, we will use \(q\) to denote \(q_{\alpha,\beta}\), unless otherwise specified. Furthermore, we will use \(c_{q}=\left\|F_{*}\right\|_{L_{q}(\pi;\mathcal{Y})}\).

We first consider the case \(\beta+p>\alpha\). We can easily verify using \(\beta+p>\alpha\) that the following inequality holds

\[\frac{1}{2}\left(1+\frac{p-\alpha}{p+\beta}\right)>\frac{1}{2}\left(\frac{p}{ p+\beta}\right)>\frac{\alpha-\beta}{2\alpha}=\frac{1}{q_{\alpha,\beta}}.\]

Choose \(t=n^{\widetilde{q}^{-1}}\), where

\[\frac{1}{\widetilde{q}}=\frac{1}{2}\left(\frac{1}{2}\left(1+\frac{p-\alpha}{p+ \beta}\right)+\frac{1}{q}\right).\]

We thus have

\[n^{\frac{1}{2}\left(1+\frac{p-\alpha}{p+\beta}\right)}>t=n^{\widetilde{q}^{-1 }}>n^{\frac{-1}{\alpha_{\alpha,\beta}}}.\]

Thus the assumptions for both Lemma 12 and Lemma 14 are satisfied.

We then consider the case \(\beta+p\leq\alpha\). We now apply Assumption (EMB) and Theorem 6 to \(\alpha^{\prime}\) instead of \(\alpha\). We obtain that the inclusion map \(I_{\pi}^{q_{\alpha^{\prime},\beta}}\) is bounded, where we recall that \(q_{\alpha^{\prime},\beta}\) is defined to be \(\frac{2\alpha^{\prime}}{\alpha^{\prime}-\beta}\). Since \(x\mapsto\frac{2x}{x-\beta}\) is monotonically decreasing for \(x>\beta\), we obtain the inequality

\[\frac{2\alpha^{\prime}}{\alpha^{\prime}-\beta}>\frac{2\alpha}{\alpha-\beta}.\]

We choose \(t=n^{\frac{1}{q_{\alpha,\beta}}}\). By construction, \(t\) satisfies the assumptions in Lemma 14. Furthermore, the assumptions of Lemma 12 are satisfied, with \(F_{*}\in L_{q^{\prime}}(\pi,\mathcal{Y})\), and \(t=n^{\frac{1}{q_{\alpha,\beta}}}\).

Having established the applicability of Lemma 14 and Lemma 12, let us turn our attention to proving the results of the Theorem. Denote

\[\xi(x,y)=(y-C_{\lambda}\phi(x))\otimes\left(C_{X,\lambda}^{-\frac{1}{2}}\phi(x )\right)\]

We compute

\[\mathbb{E}[\xi(x,y)]=(C_{YX}-C_{\lambda}C_{X})C_{X,\lambda}^{-\frac{1}{2}}\]

1. The \(\beta+p>\alpha\) case. Have

\[\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}-\mathbb{E}[\xi(x,y)]\right\|_{S_{2}( \mathcal{H},\mathcal{Y})} \leq\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\{x_{i}\in \Omega_{t}\}-\mathbb{E}[\xi(x,y)\mathbbm{1}\{x\in\Omega_{t}\}]\right\|_{S_{2} (\mathcal{H},\mathcal{Y})}\] \[+\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\{x_{i}\in \Omega_{t}^{c}\}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\] \[+\left\|\mathbb{E}[\xi(x,y)\mathbbm{1}\left\{x\in\Omega_{t}^{c} \right\}]\right|_{S_{2}(\mathcal{H},\mathcal{Y})}\]

We can bound the first term with probability at least \(1-2e^{-\tau}\) by \(c\tau\left(\frac{n}{\log^{q}(n)}\right)^{-\frac{\beta}{2\alpha}}\), according to Lemma 14. By Lemma 12, with probability at least \(1-e^{-\tau}\), for sufficiently large \(n\), \(x_{i}\in\Omega_{t}\) for all \(i\in\left[n\right]\), whereby the second term is zero. It remains to bound the third term, where our bound will be deterministic. Using Jensen's inequality, we have,

\[\left\|\mathbb{E}[\xi(x,y)\mathbbm{1}\left\{x\in\Omega_{t}^{c} \right\}]\right\|_{S_{2}(\mathcal{H},\mathcal{Y})} \leq\mathbb{E}\left[\left\|\xi(x,y)\mathbbm{1}\{x\in\Omega_{t}^{ c}\}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\right]\] \[=\mathbb{E}\left[\left\|(y-C_{\lambda}\phi(x))\mathbbm{1}\{x\in \Omega_{t}\}\right\|_{\mathcal{Y}}\cdot\left\|C_{X,\lambda}^{-\frac{1}{2}}\phi (x)\right\|_{\mathcal{H}}\right]\] \[\leq A\lambda^{-\frac{\alpha}{2}}\mathbb{E}\left[\left\|(y-C_{ \lambda}\phi(x))\mathbbm{1}\{x\notin\Omega_{t}\}\right\|_{\mathcal{Y}}\right]\]

where in the third line we used Lemma 17. We first split the second term into an approximation error and a noise term using triangle inequality.

We bound the first term using the tower property of conditional expectation,

\[\mathbb{E}\left[\left\|(y-C_{\lambda}\phi(x))\mathbbm{1}\{x\notin \Omega_{t}\}\right\|_{\mathcal{Y}}\right] \leq\mathbb{E}_{\pi}\left[\mathbb{E}[\left\|y-C_{\lambda}\phi(x) \right\|_{\mathcal{Y}}\mid x]\mathbbm{1}\{x\notin\Omega_{t}\}\right]\] \[\leq\mathbb{E}_{\pi}\left[\mathbb{E}[\left\|y-C_{\lambda}\phi(x) \right\|_{\mathcal{Y}}^{2}\mid x]^{\frac{1}{2}}\mathbbm{1}\{x\notin\Omega_{t }\}\right]\] \[\leq\sigma\pi(x\notin\Omega_{t})\] \[\leq\frac{\sigma c_{q}^{q}}{t^{q}}.\]

where in the third inequality we used Assumption (MOM) with \(q=2\), and in the fourth inequality we used Lemma 12. We bound the second term using Cauchy-Schwarz inequality and Lemma 10 with \(\gamma=0\),

\[\mathbb{E}\left[\left\|(F_{*}(x)-F_{\lambda}(x))\mathbbm{1}\{x\notin\Omega_{t }\}\right\|_{\mathcal{Y}}\right]\leq\mathbb{P}(x\notin\Omega_{t})^{\frac{1}{2} }\left\|F_{*}\right\|_{\beta}\omega_{\rho}\lambda^{\frac{\beta}{2}}\]

Therefore, using Lemma 12, we have,

\[\left\|\mathbb{E}[\xi(x,y)\mathbbm{1}\left\{x\in\Omega_{t}^{c}\right\}\right\|_ {S_{2}(\mathcal{H},\mathcal{Y})}\leq A\lambda^{-\frac{\alpha}{2}}\left(\frac{ \sigma c_{q}^{q}}{t^{q}}+\frac{c_{q}^{\frac{q}{2}}}{t^{\frac{q}{2}}}\left\|F_{* }\right\|_{\beta}\omega_{\rho}\lambda^{\frac{\beta}{2}}\right).\] (36)We now plug in \(\lambda\times n^{-\frac{1}{\beta+p}}\). Recall that by construction \(t>n^{\frac{1}{\beta}}\). Thus,

\[\lambda^{-\frac{\alpha}{2}}t^{-q} \lesssim n^{-1}n^{\frac{\alpha}{2(\beta+p)}}<n^{-1}n^{\frac{\beta+p}{2( \beta+p)}}=n^{-\frac{1}{2}}\leq n^{-\frac{1}{2}\frac{\beta}{3+p}}\] \[\lambda^{\frac{\alpha-\alpha}{2}}t^{-\frac{\alpha}{2}} \lesssim n^{-\frac{1}{2}}n^{\frac{-(\beta-\alpha)}{2(\beta+p)}}<n^{-\frac{1}{2 }}n^{\frac{p}{2(\beta+p)}}=n^{-\frac{1}{2}\frac{\beta}{3+p}}\]

We've therefore proved inequality (34).

2. The \(\beta+p\leq\alpha\) case. We proceed similarly to the \(\beta+p>\alpha\) case. We have,

\[\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}-\mathbb{E}[\xi(x,y)]\right\|_{S_{2}( \mathcal{H},\mathcal{Y})} \leq\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\{x_{i}\in \Omega_{t}\}-\mathbb{E}[\xi(x,y)\mathbbm{1}\{x\in\Omega_{t}\}]\right\|_{S_{2}( \mathcal{H},\mathcal{Y})}\] \[+\left\|\frac{1}{n}\sum_{i=1}^{n}\xi_{i}\mathbbm{1}\{x_{i}\in \Omega_{t}^{c}\}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\] \[+\left\|\mathbb{E}[\xi(x,y)\mathbbm{1}\{x\in\Omega_{t}^{c}\}] \right\|_{S_{2}(\mathcal{H},\mathcal{Y})}\]

We can bound the first term with probability at least \(1-2e^{-\tau}\) by \(c\tau\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{\beta}{2\alpha}}\), according to Lemma 14. By Lemma 12, with probability at least \(1-e^{-\tau}\), for sufficiently large \(n\), \(x_{i}\in\Omega_{t}\) for all \(i\in[n]\), whereby the second term is zero. We bound the third term by Eq. (36). We now plug in \(\lambda\times\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{1}{\alpha}}\). Recall that by construction \(t>n^{\frac{1}{\beta}}\). Thus,

\[\lambda^{-\frac{\alpha}{2}}t^{-q} \lesssim n^{-1}\left(\frac{n}{\log^{\theta}(n)}\right)^{\frac{1}{ 2}}<\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{1}{2}}\leq\left(\frac{n}{ \log^{\theta}(n)}\right)^{-\frac{\beta}{2\alpha}}\] \[\lambda^{\frac{\beta-\alpha}{2}}t^{-\frac{\alpha}{2}} \lesssim n^{-\frac{1}{2}}\left(\frac{n}{\log^{\theta}(n)}\right)^{ \frac{\alpha-\beta}{2\alpha}}<\left(\frac{n}{\log^{\theta}(n)}\right)^{\frac{ \gamma-\beta}{2\alpha}}\]

We have therefore proved inequality (35). 

We adapt [54, Theorem 16] to the vector-valued setting.

**Theorem 9** (Bound of estimation error).: _Suppose that assumptions (EMB), (EVD), (MOM) and (SRC) hold for \(0\leq\beta\leq 2\rho\), where \(\rho\) is the qualification, and \(p\leq\alpha<1\). For \(0\leq\gamma\leq 1\), with \(\gamma\leq\beta\),_

1. _In the case of_ \(\beta+p>\alpha\)_, by choosing_ \(\lambda\times n^{-\frac{1}{\beta+p}}\)_, for any fixed_ \(\tau\geq\log(4)\)_, when_ \(n\) _is sufficiently large, with probability at least_ \(1-4e^{-\tau}\)_, we have_ \[\|[\hat{C}_{\lambda}-C_{\lambda}]\|_{S_{2}([\mathcal{H}]^{\gamma},\mathcal{Y}) }^{2}\leq c\tau^{2}n^{-\frac{\beta-\gamma}{\beta+p}},\] _where_ \(c\) _is a constant independent of_ \(n,\tau\)_._
2. _In the case of_ \(\beta+p\leq\alpha\)_, by choosing_ \(\lambda\times\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{1}{\alpha}}\)_, for any fixed_ \(\tau\geq\log(4)\)_, when_ \(n\) _is sufficiently large, with probability at least_ \(1-4e^{-\tau}\)_, we have_ \[\|[\hat{C}_{\lambda}-C_{\lambda}]\|_{S_{2}([\mathcal{H}]^{\gamma},\mathcal{Y}) }^{2}\leq c\tau^{2}\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{\beta-\gamma }{\alpha}}\] _where_ \(c\) _is a constant independent of_ \(n,\tau\)_._

Proof.: Firstly, we establish the applicability of Lemma 19.

1. The \(\beta+p>\alpha\) case. Have \(\lambda\times n^{-\frac{1}{3+p}}\), hence

\[n\lambda^{\alpha} \gtrsim n^{\frac{\beta+p-\alpha}{\beta+p}}\]

whereas using \(\lambda\leq\|C_{X}\|_{\mathcal{H}\to\mathcal{H}}\) for sufficiently large \(n\), as well as Lemma 16,

\[8A^{2}\tau\log\left(2e\mathcal{N}(\lambda)\frac{|C_{X}|_{\mathcal{H}\to \mathcal{H}}+\lambda}{\|C_{X}|_{\mathcal{H}\to\mathcal{H}}}\right)\lesssim 8A^{2}\tau\log(4ec_{2,1} \lambda^{-p})\lesssim 8A^{2}\tau\left(\log(4ec_{2,1})+\frac{p}{\beta+p}\log(n)\right)\]Therefore, for a fixed \(\tau>0\), for all sufficiently large \(n\), Eq. (40) in Lemma 19 is satisfied.

2. The \(\beta+p\leq\alpha\) case. Have \(\lambda\asymp\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{1}{\alpha}}\) for some \(\theta>1\), hence

\[n\lambda^{\alpha}\geq\log^{\theta}(n)\]

whereas similar to the \(\beta+p>\alpha\) case, we ahve

\[8A^{2}\tau\log\left(2e\mathcal{N}(\lambda)\frac{\left\|C_{X}\right\|_{ \mathcal{H}\to\mathcal{H}}+\lambda}{\left\|C_{X}\right\|_{\mathcal{H}\to \mathcal{H}}}\right)\lesssim 8A^{2}\tau\left(\log(4ec_{2,1})+\frac{p}{\alpha}\log \left(\frac{n}{\log^{\theta}(n)}\right)\right)\]

Therefore, for a fixed \(\tau>0\), for all sufficiently large \(n\), Eq. (40) in Lemma 19 is satisfied.

We thus conclude for all \(\alpha\in(0,1]\), with probability \(\geq 1-2e^{-\tau}\), Eq. (41) and (42) are satisfied simultaneously.

We exploit the following decomposition

\[\left\|\left[\hat{C}_{\lambda}-C_{\lambda}\right]\right\|_{S_{2} \left(\left[\mathcal{H}\right]^{\gamma},\mathcal{Y}\right)} \leq\left\|\left(\hat{C}_{\lambda}-C_{\lambda}\right)C_{X}^{\frac {1-\gamma}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}\] \[\leq\left\|\left(\hat{C}_{\lambda}-C_{\lambda}\right)\hat{C}_{X, \lambda}^{\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}\cdot \left\|\hat{C}_{X,\lambda}^{-\frac{1}{2}}C_{X,\lambda}^{\frac{1}{2}}\right\|_{ \mathcal{H}\to\mathcal{H}}\cdot\left\|C_{X,\lambda}^{-\frac{1}{2}}C_{X}^{\frac {1-\gamma}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\] \[\leq\left\|\left(\hat{C}_{\lambda}-C_{\lambda}\right)\hat{C}_{X, \lambda}^{\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}\cdot 3 \cdot\sup_{i\in\mathbb{N}}\frac{\mu_{i}^{\frac{1-\gamma}{2}}}{\sqrt{\mu_{i}+ \lambda}}\] \[\leq\left\|\left(\hat{C}_{\lambda}-C_{\lambda}\right)\hat{C}_{X, \lambda}^{\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}\cdot 3 \lambda^{-\frac{\gamma}{2}},\] (37)

where in the first inequality we used Lemma 22, in the third inequality we used Eq. (42) and in the last inequality we used Lemma 21. We consider the following decomposition

\[\hat{C}_{\lambda}-C_{\lambda} =\hat{C}_{\lambda}-C_{\lambda}\left(\hat{C}_{Xg}\lambda\left(\hat {C}_{X}\right)+r_{\lambda}\left(\hat{C}_{X}\right)\right)\] \[=\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)g_{\lambda}\left( \hat{C}_{X}\right)-C_{\lambda}r_{\lambda}\left(\hat{C}_{X}\right).\]

Hence

\[\left\|\left[\hat{C}_{\lambda}-C_{\lambda}\right]\right\|_{S_{2}\left(\left[ \mathcal{H}\right]^{\gamma},\mathcal{Y}\right)}^{2}\leq 18\lambda^{-\gamma}\left( \left(I\right)^{2}+\left(II\right)^{2}\right),\]

where

\[\left(I\right) =\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)\hat{C}_ {X,\lambda}^{\frac{1}{2}}g_{\lambda}\left(\hat{C}_{X}\right)\right\|_{S_{2} \left(\mathcal{H},\mathcal{Y}\right)}\] \[\left(II\right) =\left\|C_{\lambda}r_{\lambda}\left(\hat{C}_{X}\right)\hat{C}_{X, \lambda}^{\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}.\]

**Term (I)**. The high level idea is to bound (I) by exploiting the first axiom of the filter function (8), where \(g_{\lambda}(\hat{C}_{X})\) is intuitively a regularized inverse of \(\hat{C}_{X}\), by grouping it with \(\hat{C}_{X,\lambda}\).

\[\left(I\right) \leq\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X, \lambda}^{-\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)} \cdot\left\|C_{X\lambda}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-\frac{1}{2}} \right\|_{\mathcal{H}\to\mathcal{H}}\cdot\left\|\hat{C}_{X,\lambda}g_{\lambda} \left(\hat{C}_{X}\right)\right\|_{\mathcal{H}\to\mathcal{H}}\] \[\leq\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X, \lambda}^{-\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)} \cdot\sqrt{3}\sup_{t\in[0,x^{2}]}(t+\lambda)g_{\lambda}(t)\] \[\leq\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X, \lambda}^{-\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)} \cdot 2\sqrt{3}E.\]

where the second inequality follows from Eq. (42). We consider the following decomposition

\[\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X, \lambda}^{-\frac{1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}^{2} \leq 2\left\|\left(\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)- \left(C_{YX}-C_{\lambda}C_{X}\right)\right)C_{X,\lambda}^{-\frac{1}{2}} \right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}^{2}\] \[+2\left\|\left(C_{YX}-C_{\lambda}C_{X}\right)C_{X,\lambda}^{-\frac {1}{2}}\right\|_{S_{2}\left(\mathcal{H},\mathcal{Y}\right)}^{2}\]We bound the first term by Theorem 8 and the second term by Lemma 8. This yields, for \(\tau\geq\log(4)\), with probability at least \(1-4e^{-\tau}\), for some constant \(c>0\) which does not depend on \(n,\tau,\lambda\),

\[\left\|\left(\hat{C}_{YX}-C_{\lambda}\hat{C}_{X}\right)C_{X,\lambda }^{-\frac{1}{2}}\right\|_{S_{2}(\mathcal{H},\mathcal{Y})}^{2} \leq 2\omega_{\rho}^{2}\|F_{*}\|_{\beta}^{2}\lambda^{\beta}+\begin{cases} c\tau^{2}n^{-\frac{\beta}{\beta+p}}&\beta+p\geq\alpha\\ c\tau^{2}\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{\beta}{\alpha}}&\beta+ p<\alpha\end{cases}\] \[\leq\begin{cases}\tau^{2}(c+2\|F_{*}\|_{\beta}^{2}\omega_{\rho}^{ 2}\lambda^{\beta})n^{-\frac{\beta}{\beta+p}}&\beta+p\geq\alpha\\ \tau^{2}(c+2\|F_{*}\|_{\beta}^{2}\omega_{\rho}^{2}\lambda^{\beta})\left(\frac {n}{\log^{\theta}(n)}\right)^{-\frac{\beta}{\alpha}}&\beta+p<\alpha\end{cases}\]

where we used that \(\tau>1\). So collecting all the relevant constants together, we can write the upper bound of term (I) as follows: with probability at least \(1-4e^{-\tau}\), for some constant \(c^{\prime}>0\) (different from the \(c\) before) which does not depend on \(n,\tau,\lambda\), we have

\[(I)\leq c^{\prime}\tau\cdot\begin{cases}n^{-\frac{1}{2}\frac{\beta}{\beta+p}}& \beta+p\geq\alpha\\ \left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{\beta}{2\alpha}}&\beta+p< \alpha.\end{cases}\]

**Term (II)**. Using Lemma 9, we have

\[(II)\leq B\left\|\hat{C}_{X,\lambda}^{\frac{1}{2}}r_{\lambda}(\hat{C}_{X})g_{ \lambda}(C_{X})C_{X}^{\frac{\beta+1}{2}}\right\|_{\mathcal{H}\to\mathcal{H}}\]

The second term is the same as the scalar-valued case, which is bounded in Step 3 of the proof of [54, Theorem 16]. We define

\[\Delta_{1}:=32\max\left\{\frac{\beta-1}{2},1\right\}E\omega_{\rho}\kappa^{ \beta-1}\lambda^{\frac{1}{2}}n^{-\frac{\min(\beta,3)-1}{4}}\]

By the proof of [54, Theorem 16], we have, with probability at least \(1-6e^{-\tau}\)

\[(II)\leq 6B\omega_{\rho}E\lambda^{\frac{\beta}{2}}+\Delta_{1}B\tau \mathbbm{1}\{\beta>2\}.\]

1. Case \(\beta+p>\alpha\). In this case \(\lambda\asymp n^{-\frac{1}{\beta+p}}\). We note that for \(\beta>2\), \(\Delta_{1}\) as a function of \(n\) can be written as

\[\Delta_{1}\asymp n^{-\frac{1}{2(\beta+p)}-\frac{\min(\beta,3)-1}{4}}\]

Note that

\[\frac{1}{2(\beta+p)}+\frac{\min(\beta,3)-1}{4}-\frac{\beta}{2(\beta+p)}=\frac {1}{2}\left(\frac{p}{\beta+p}+\frac{\min(\beta,3)-1}{2}\right)>0\]

Hence

\[\Delta_{1}\lesssim n^{-\frac{\beta}{2(\beta+p)}}\]

Therefore we have shown that there exists some constant \(c^{\prime\prime}>0\), independent of \(n,\lambda,\tau\), such that with probability at least \(1-6e^{-\tau}\), for sufficiently large \(n\),

\[\left\|\left[\hat{C}_{\lambda}-C_{\lambda}\right]\right\|_{S_{2}(\{\mathcal{H }\}^{\prime},\mathcal{Y})}\leq c^{\prime\prime}\tau n^{-\frac{1}{2}\frac{\beta -\gamma}{\beta+p}}.\]

2. Case \(\beta+p\leq\alpha\). In this case \(\beta\leq\alpha\leq 1\), and \(\lambda\asymp\left(\frac{n}{\log^{\theta}(n)}\right)^{-\frac{1}{\alpha}}\). We have also shown that there exists some constant \(c^{\prime\prime}>0\), independent of \(n,\lambda,\tau\), such that with probability at least \(1-6e^{-\tau}\), for sufficiently large \(n\),

\[\left\|\left[\hat{C}_{\lambda}-C_{\lambda}\right]\right\|_{S_{2}(\{\mathcal{H }\}^{\prime},\mathcal{Y})}\leq c^{\prime\prime}\tau\left(\frac{n}{\log^{ \theta}(n)}\right)^{-\frac{\beta-\gamma}{2\alpha}}\]

Putting together Lemma 10 and Theorem 9, we have proved Theorem 4.

Auxiliary Results

### Spectral Calculus, Proof of Proposition 1 and Empirical Solution

**Definition 9** (Spectral Calculus; see 16, Chapter 2.3).: _Let \(H\) be a Hilbert space. Consider \(g:\mathbb{R}\to\mathbb{R}\) and a self-adjoint compact operator \(A:H\to H\) admitting a spectral decomposition written as_

\[A=\sum_{i\in I}\lambda_{i}h_{i}\otimes h_{i}.\]

_We then define \(g(A):H\to H\) as_

\[g(A):=\sum_{i\in I}g(\lambda_{i})h_{i}\otimes h_{i}\]

_whenever this series converges in operator norm._

Proof of Proposition 1.: We define the sampling operator \(S:\mathbb{R}^{n}\to\mathcal{H}\) and it dual \(S^{*}:\mathcal{H}\to\mathbb{R}^{n}\),

\[S:\mathbb{R}^{n} \to\mathcal{H}, S^{*}:\mathcal{H} \to\mathbb{R}^{n},\] \[\alpha \mapsto\sum_{i=1}^{n}\alpha_{i}\phi(x_{i}) f \mapsto(f(x_{i}))_{i=1}^{n}\]

We can verify that \(\hat{C}_{X}=n^{-1}SS^{*}\) and \(\mathbf{K}=S^{*}S\). Let \(\mathbf{Y}=(y_{i})_{i=1}^{n}\in\mathbb{R}^{n}\). We have, for all \(x\in\mathcal{X}\),

\[\hat{F}_{\lambda}(x) =\hat{C}_{\lambda}\phi(X)\] \[=\left(\frac{1}{n}\sum_{i=1}^{n}y_{i}\otimes\phi(x_{i})\right)g_ {\lambda}(\hat{C}_{X})\phi(X)\] \[=\sum_{i=1}^{n}y_{i}\left\langle\phi(x_{i}),\frac{1}{n}g_{ \lambda}(n^{-1}SS^{*})\phi(X)\right\rangle_{\mathcal{H}}\] \[=\mathbf{Y}^{T}S^{*}\left(\frac{1}{n}g_{\lambda}(n^{-1}SS^{*}) \phi(X)\right)\] (38) \[=\mathbf{Y}^{T}\frac{1}{n}g_{\lambda}(n^{-1}S^{*}S)S^{*}\phi(X)\] (39) \[=\mathbf{Y}^{T}\frac{1}{n}g_{\lambda}(n^{-1}\mathbf{K})S^{*}\phi(X)\] \[=\mathbf{Y}^{T}\frac{1}{n}g_{\lambda}(n^{-1}\mathbf{K})\mathbf{k} _{x}.\]

To go from (38) to (39), we make the following observation. Consider the singular value decomposition of the compact operator \(S\), there is \(m\leq n\) such that

\[S=\sum_{i=1}^{m}\sqrt{\sigma_{i}}f_{i}\otimes e_{i}\]

where \((e_{i})_{i}\), \((f_{i})_{i}\) are orthonormal sequences in \(\mathbb{R}^{n}\) and \(\mathcal{H}\) respectively. We then have

\[SS^{*}=\sum_{i=1}^{m}\sigma_{i}f_{i}\otimes f_{i},\quad S^{*}S=\sum_{i=1}^{m} \sigma_{i}e_{i}\otimes e_{i}.\]

Therefore, we deduce

\[S^{*}g_{\lambda}\left(\frac{SS^{*}}{n}\right) =\left(\sum_{i}\sqrt{\sigma_{i}}e_{i}\otimes f_{i}\right)\left( \sum_{j}g_{\lambda}\left(\frac{\sigma_{j}}{n}\right)f_{j}\otimes f_{j}\right)\] \[=\sum_{i,j}g_{\lambda}\left(\frac{\sigma_{j}}{n}\right)\sqrt{ \sigma_{i}}e_{i}\otimes f_{j}\langle f_{i},f_{j}\rangle_{\mathcal{H}}\] \[=\sum_{i}g_{\lambda}\left(\frac{\sigma_{i}}{n}\right)\sqrt{ \sigma_{i}}e_{i}\otimes f_{i}\]Similarly,

\[g_{\lambda}\left(\frac{S^{*}S}{n}\right)S^{*} =\left(\sum_{j}g_{\lambda}\left(\frac{\sigma_{j}}{n}\right)e_{j} \otimes e_{j}\right)\left(\sum_{i}\sqrt{\sigma_{i}}e_{i}\otimes f_{i}\right)\] \[=\sum_{i,j}g_{\lambda}\left(\frac{\sigma_{j}}{n}\right)\sqrt{ \sigma_{i}}e_{j}\otimes f_{i}\langle e_{i},e_{j}\rangle_{\mathbb{R}^{n}}\] \[=\sum_{i}g_{\lambda}\left(\frac{\sigma_{i}}{n}\right)\sqrt{ \sigma_{i}}e_{i}\otimes f_{i}\]

Hence we have proved

\[S^{*}g_{\lambda}\left(\frac{SS^{*}}{n}\right)=g_{\lambda}\left(\frac{S^{*}S}{n }\right)S^{*}\]

as desired. 

**Proposition 2**.: _Any minimizer \(F\in\mathcal{G}\) of_

\[\mathcal{E}_{n}(F):=\frac{1}{n}\sum_{i=1}^{n}\|y_{i}-F(x_{i})\|_{\mathcal{Y}}^ {2}\]

_on \(\mathcal{G}\) must satisfy_

\[\hat{C}_{YX}=\hat{C}\hat{C}_{X},\qquad C\in S_{2}(\mathcal{H},\mathcal{Y}),\]

_where \(F(\cdot)=C\phi(\cdot)\)._

Proof.: By Corollary 1, it is equivalent to solve the following optimization problem on \(S_{2}(\mathcal{H},\mathcal{Y})\),

\[\min_{C\in S_{2}(\mathcal{H},\mathcal{Y})}\frac{1}{n}\sum_{i=1}^{n}\|y_{i}-C \phi(x_{i})\|_{\mathcal{Y}}^{2}.\]

Recall for a Hilbert-Schmidt operator \(L\in S_{2}(\mathcal{H},\mathcal{Y})\), we have

\[\langle L,a\otimes b\rangle_{S_{2}(\mathcal{H},\mathcal{Y})}=\langle a,Lb \rangle_{\mathcal{Y}}.\]

Using this, we re-write the objective as an inner product in \(S_{2}(\mathcal{H},\mathcal{Y})\):

\[\frac{1}{n}\sum_{i=1}^{n}\|y_{i}-C\phi(x_{i})\|_{\mathcal{Y}}^{2} =\frac{1}{n}\sum_{i=1}^{n}-2(C,y_{i}\otimes\phi(x_{i}))_{S_{2}( \mathcal{H},\mathcal{Y})}+\langle C,(C\phi(x_{i}))\otimes\phi(x_{i})\rangle_{ S_{2}(\mathcal{H},\mathcal{Y})}+\text{ constant}\] \[=-2\langle C,\hat{C}_{YX}\rangle_{S_{2}(\mathcal{H},\mathcal{Y})} +\langle C,C\hat{C}_{X}\rangle_{S_{2}(\mathcal{H},\mathcal{Y})}\]

Taking the Frechet derivative with respect to \(C\) and setting in to zero, we obtain the following first order condition

\[\hat{C}_{YX}=C\hat{C}_{X}\]

### Properties Related to Assumptions (EMB) and (Evd)

**Lemma 15** (Lemma 13 [17]).: _Under (EMB), the following inequality is satisfied, for \(\lambda>0\) and \(\pi\)-almost all \(x\in\mathcal{X}\),_

\[\left\|\left(C_{X}+\lambda Id_{\mathcal{H}}\right)^{-\frac{1}{2}}k(x,\cdot) \right\|_{\mathcal{H}}\leq A\lambda^{-\frac{\alpha}{2}}.\]

**Definition 10** (\(l\)-effective dimension).: _For \(l\geq 1\), the \(l\)-effective dimension \(\mathcal{N}_{l}:(0,\infty)\to[0,\infty)\) is defined by_

\[\mathcal{N}_{l}(\lambda):=\mathrm{Tr}\left[C_{X}^{l}C_{X,\lambda}^{-l}\right]= \sum_{i\geq 1}\left(\frac{\mu_{i}}{\mu_{i}+\lambda}\right)^{l}\]

The \(1\)-effective dimension is widely considered in the statistical analysis of kernel ridge regression (see [6], [5], [32], [34], [17]). The following lemma provides upper and lower bounds for the \(l\)-effective dimension.

**Lemma 16**.: _Suppose Assumption (EVD) holds with parameter \(p\in(0,1]\), for any \(\lambda\in(0,1]\), there exists a constant \(c_{2,l}>0\) independent of \(\lambda\) such that_

\[\mathcal{N}_{l}(\lambda)\leq c_{2,l}\lambda^{-p}.\]

_If furthermore, Assumption (EVD+) holds with parameter \(p\in(0,1)\), for any \(\lambda\in(0,1]\), there exists a constant \(c_{1,l}>0\) independent of \(\lambda\) such that_

\[c_{1,l}\lambda^{-p}\leq\mathcal{N}_{l}(\lambda)\leq c_{2,l}\lambda^{-p}.\]

The proof can be found in [29] (Proposition D.1), but as the proof is incomplete we provide a full proof for completeness. This allows us to detect that the value \(p=1\) in Assumption (EVD+) is not compatible with the assumption of a bounded kernel (see Remark 7 below).

Proof.: \[\mathcal{N}_{l}(\lambda) \leq\sum_{i\geq 1}\left(\frac{D_{2}}{D_{2}+\lambda i^{\frac{1}{p} }}\right)^{l}\qquad(x\mapsto\frac{x}{x+\lambda}\text{ is monotonically increasing})\] \[\leq\int_{0}^{+\infty}\left(\frac{D_{2}}{D_{2}+\lambda x^{\frac{ 1}{p}}}\right)^{l}\mathrm{d}x\qquad(i\mapsto\left(\frac{D_{2}}{D_{2}+\lambda i ^{1/p}}\right)^{l}\text{ is positive and decreasing})\] \[=\int_{0}^{+\infty}\left(\frac{D_{2}}{D_{2}+y^{\frac{1}{p}}} \right)^{l}\frac{\mathrm{d}y}{\lambda^{p}}\qquad(y^{1/p}=\lambda x^{1/p})\] \[\leq\lambda^{-p}\left(1+\int_{1}^{+\infty}\left(\frac{D_{2}}{D_{2 }+y^{\frac{1}{p}}}\right)^{l}\mathrm{d}y\right)\]

Let us now consider the integral. Let us first consider \(p\leq 1<l\),

\[\int_{1}^{\infty}\left(\frac{D_{2}}{D_{2}+y^{\frac{1}{p}}}\right) ^{l}\mathrm{d}y \leq D_{2}^{l}\int_{1}^{\infty}y^{-\frac{l}{p}}\mathrm{d}y\] \[=D_{2}^{l}\frac{p}{l-p}\]

Therefore, using \(\lambda\leq 1\), we can take \(c_{2,l}=1+D_{2}^{l}\frac{p}{l-p}\). The remaining edge case \(p=1=l\), is covered by [17, Lemma 11] with \(c_{2,1}=\left\|C_{X}\right\|_{S_{1}(\mathcal{H})}\). For the lower bound, we proceed similarly. For \(p\in(0,1)\) (and therefore \(p<l\)),

\[\mathcal{N}_{l}(\lambda) \geq\sum_{i\geq 1}\left(\frac{D_{1}}{D_{1}+\lambda i^{\frac{1}{p} }}\right)^{l}\qquad(x\mapsto\frac{x}{x+\lambda}\text{ is monotonically increasing})\] \[\geq\int_{1}^{\infty}\left(\frac{D_{1}}{D_{1}+\lambda x^{\frac{1} {p}}}\right)^{l}\mathrm{d}x\qquad(i\mapsto\left(\frac{D_{1}}{D_{1}+\lambda i ^{1/p}}\right)^{l}\text{ is positive and decreasing})\] \[=\int_{1}^{\infty}\left(\frac{D_{1}}{D_{1}+y^{\frac{1}{p}}} \right)^{l}\frac{\mathrm{d}y}{\lambda^{p}}\qquad(y^{1/p}=\lambda x^{1/p})\] \[\geq\lambda^{-p}\int_{1}^{\infty}\left(\frac{D_{1}}{D_{1}+1} \right)^{l}y^{-\frac{l}{p}}\mathrm{d}y\] \[=\lambda^{-p}\left(\frac{D_{1}}{D_{1}+1}\right)^{l}\frac{p}{l-p}.\]

Therefore, we can take \(c_{1,l}=\left(\frac{D_{1}}{D_{1}+1}\right)^{l}\frac{p}{l-p}\). 

**Remark 7**.: _We note that Assumption (EVD+) with \(p=1\) is not compatible with the assumption that \(k\) is bounded (Assumption 3). Indeed, suppose that \(\mu_{i}\geq D_{1}i^{-1}\), for all \(i\geq 1\). Recall that \(\{[e_{i}]\}_{i\geq 1}\) forms an orthonormal set in \(L_{2}(\pi)\). By Mercer's theorem,_

\[\kappa^{2}\geq\int_{\mathcal{X}}k(x,x)\pi(\mathrm{d}x)=\sum_{i\geq 1}\mu_{i} \int_{\mathcal{X}}e_{i}(x)^{2}\pi(\mathrm{d}x)=\sum_{i\geq 1}\mu_{i}\geq D_{1} \sum_{i\geq 1}i^{-1}=+\infty,\]

_which leads to a contradiction._

[MISSING_PAGE_FAIL:38]

**Proposition 3** (Proposition C.9 29).: _Let \(\pi\) be a probability measure on \(\mathcal{X},\,f\in L_{2}(\pi)\) and \(\|f\|_{\infty}\leq M\). Suppose we have \(x_{1},\ldots,x_{n}\) sampled i.i.d. from \(\pi\). Then, for any \(\tau\geq\log(2)\), the following holds with probability at least \(1-2e^{-\tau}\):_

\[\frac{1}{2}\|f\|_{L_{2}(\pi)}^{2}-\frac{5\tau M^{2}}{3n}\leq\|f\|_{2,n}^{2}\leq \frac{3}{2}\|f\|_{L_{2}(\pi)}^{2}+\frac{5\tau M^{2}}{3n},\]

_where \(\|\cdot\|_{2,n}\) was defined in Definition 7._

**Lemma 19** (Lemma 12 54).: _Let Assumptions (EMB), (SRC) and (MOM) be satisfied. For \(\tau\geq 1\), if \(\lambda\) and \(n\) satisfy that_

\[n\geq 8A^{2}\tau\lambda^{-\alpha}\log\left(2e\mathcal{N}(\lambda)\frac{\|C_{X} \|_{\mathcal{H}\to\mathcal{H}}+\lambda}{\|C_{X}\|_{\mathcal{H}\to\mathcal{H}}}\right)\] (40)

_then the following operator norm bounds are satisfied with probability not less than \(1-2e^{-\tau}\)_

\[\left\|C_{X,\lambda}^{-\frac{1}{2}}\hat{C}_{X,\lambda}^{\frac{1} {2}}\right\|_{\mathcal{H}\to\mathcal{H}}^{2}\leq 2,\] (41) \[\left\|C_{X,\lambda}^{\frac{1}{2}}\hat{C}_{X,\lambda}^{-\frac{1} {2}}\right\|_{\mathcal{H}\to\mathcal{H}}^{2}\leq 3.\] (42)

### Miscellaneous results

**Lemma 20** (Cordes inequality [18]).: _Let \(A,B\) be two positive bounded linear operators on a separable Hilbert space \(H\) and \(s\in[0,1]\). Then_

\[\left\|A^{s}B^{s}\right\|_{H\to H}\leq\|A\|_{H\to H}^{s}\|B\|_{H\to H}^{s}\]

**Lemma 21** (Lemma 25 [17]).: _For \(\lambda>0\) and \(s\in[0,1]\), we have_

\[\sup_{t\geq 0}\frac{t^{s}}{t+\lambda}\leq\lambda^{s-1}\]

We recall the following basic Lemma from [30, Lemma 2].

**Lemma 22**.: _For \(0\leq\gamma\leq 1\) and \(F\in\mathcal{G}\), the inequality_

\[\left\|[F]\right\|_{\gamma}\leq\left\|CC_{X}^{\frac{1-\gamma}{2}}\right\|_{S_{ 2}(\mathcal{H},\mathcal{Y})}\]

_holds, where \(C=\bar{\Psi}^{-1}(F)\in S_{2}(\mathcal{H},\mathcal{Y})\). If, in addition, \(\gamma<1\) or \(C\perp\mathcal{Y}\otimes\ker I_{\pi}\) is satisfied, then the result is an equality._

**Definition 11**.: _Let \(\mathcal{X}\subseteq\mathbb{R}^{d}\) be a compact set and \(\theta\in(0,1]\). For a function \(f:\mathcal{X}\to\mathbb{R}\), we introduce the Holder semi-norm_

\[[f]_{\theta,\mathcal{X}}:=\sup_{x,y\in\mathcal{X},x\ni y}\frac{|f(x)-f(y)|}{ \|x-y\|^{\theta}},\]

_where \(\|\cdot\|\) represents the usual Euclidean norm. Then, we define the Holder space_

\[C^{\theta}(\mathcal{X}):=\left\{f:\mathcal{X}\to\mathbb{R}\,|\,[f]_{\theta, \mathcal{X}}<+\infty\right\},\]

_which is equipped with the norm_

\[\|f\|_{C^{\theta}(\mathcal{X})}:=\sup_{x\in\mathcal{X}}|f(x)|+[f]_{\theta, \mathcal{X}}.\]

The next lemma is used to prove Lemma 24 below. It appears in [29, Lemma A.3], albeit the use of an erroneous equality in their proof: \(\left|k(x,\cdot)-k(y,\cdot)\right|_{\mathcal{H}}^{2}=k(x,x)k(y,y)-k(x,y)^{2}\). We therefore provide our own proof of this result.

**Lemma 23**.: _Assume that \(\mathcal{H}\) is an RKHS over a compact set \(\mathcal{X}\subseteq\mathbb{R}^{d}\) associated with a kernel \(k\in C^{\theta}(\mathcal{X}\times\mathcal{X})\) for \(\theta\in(0,1]\). Then, we have \(\mathcal{H}\subseteq C^{\frac{\theta}{2}}(\mathcal{X})\) and_

\[[f]_{\frac{\theta}{2},\mathcal{X}}\leq\sqrt{2[k]_{\theta,\mathcal{X}\times \mathcal{X}}}\|f\|_{\mathcal{H}}\]Proof.: For all \((x,y)\in\mathcal{X}\) and \(f\in\mathcal{H}\), by the reproducing property and Cauchy-Schwarz inequality,

\[|f(x)-f(y)|=|\langle k(x,\cdot)-k(y,\cdot),f\rangle_{\mathcal{H}}|\leq\|f\|_{ \mathcal{H}}\|k(x,\cdot)-k(y,\cdot)\|_{\mathcal{H}}.\]

Then, using \(k\in C^{\theta}(\mathcal{X}\times\mathcal{X})\), we obtain

\[\|k(x,\cdot)-k(y,\cdot)\|_{\mathcal{H}}^{2}=k(x,x)+k(y,y)-2k(x,y)\leq 2[k]_{ \theta,\mathcal{X}\times\mathcal{X}}\left|x-y\right|^{\theta},\]

which concludes the proof. 

We derive as a corollary a quantitative upper bound on the \(\epsilon\)-covering number of the the set of (spectral) regularized kernel basis function with respect to the \(\|\cdot\|_{\infty}\) norm.

**Lemma 24** (Lemma C.10 by 29).: _Assume that \(\mathcal{H}\) is an RKHS over a compact set \(\mathcal{X}\subseteq\mathbb{R}^{d}\) associated with a kernel \(k\in C^{\theta}(\mathcal{X}\times\mathcal{X})\) for \(\theta\in(0,1]\). Assume that \(k(x,x)\leq\kappa^{2}\) for all \(x\in\mathcal{X}\). Then, we have that for all \(\epsilon>0\),_

\[\mathcal{N}(\mathcal{K}_{\lambda},\|\cdot\|_{\infty},\epsilon)\leq c(\lambda \epsilon)^{-\frac{2\theta}{d}}\]

_where \(\mathcal{K}_{\lambda}:=\left\{C_{X,\lambda}^{-1}k(x,\cdot)\right\}_{x\in \mathcal{X}}\), and \(c\) is a positive constant which does not depend on \(\lambda,\epsilon\) and only depends on \(\kappa\) and \([k]_{\theta,\mathcal{X}\times\mathcal{X}}\). \(\mathcal{N}(\mathcal{K}_{\lambda},\|\cdot\|_{\infty},\epsilon)\) denotes the \(\epsilon-\)covering number of the set \(\mathcal{K}_{\lambda}\) in the norm \(\|\cdot\|_{\infty}\) (see [49, Definition 6.19] for the definition of covering numbers)._

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We specifically point out the settings and detailed contributions of our work in both abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In describing our assumptions as well as discussing our theoretical results, we specifically list our limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide our assumptions in Section 2.3. The complete proof of our works are listed in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: Our paper does not contain experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our paper mainly focuses on investigating the learning efficiency of spectral algorithms. We therefore believe our paper does not have any negative societal impacts. We explain how our theory can improve our understanding of various learning algorithms in the introduction which could be potential positive societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.