# Double Randomized Underdamped Langevin with Dimension-Independent Convergence Guarantee

 Yuanshi Liu\({}^{*}\), Cong Fang\({}^{\otimes*}\), Tong Zhang\({}^{\dagger}\)

\({}^{*}\) School of Intelligence Science and Technology, Peking University

\({}^{\dagger}\) Department of Computer Science \(\&\) Engineering, the Hong Kong University of Science and Technology

{liu_yuanshi, fangcong}@pku.edu.cn, tongzhang@tongzhang-ml.org

###### Abstract

This paper focuses on the high-dimensional sampling of log-concave distributions with composite structures: \(p^{*}(\mathrm{d}\mathbf{x})\propto\exp(-g(\mathbf{x})-f(\mathbf{x}))\mathrm{d} \mathbf{x}\). We develop a double randomization technique, which leads to a fast underdamped Langevin algorithm with a dimension-independent convergence guarantee. We prove that the algorithm enjoys an overall \(\widetilde{\mathcal{O}}\left(\frac{(\mathrm{tr}(H))^{1/3}}{\epsilon^{2/3}}\right)\) iteration complexity to reach an \(\epsilon\)-tolerated sample whose distribution \(p\) admits \(W_{2}(p,p^{*})\leq\epsilon\). Here, \(H\) is an upper bound of the Hessian matrices for \(f\) and does not explicitly depend on dimension \(d\). For the posterior sampling over linear models with normalized data, we show a clear superiority of convergence rate which is dimension-free and outperforms the previous best-known results by a \(d^{1/3}\) factor. The analysis to achieve a faster convergence rate brings new insights into high-dimensional sampling.

## 1 Introduction

Sampling from a high-dimensional distribution serves as one of the key components in statistics, machine learning, and scientific computing, and constitutes the foundation of the fields including Bayesian statistics and generative models (Liu and Liu, 2001; Brooks et al., 2011; Song et al., 2020). Recently, there is an emerging trend in designing provably faster Markov Chain Monte Carlo (MCMC) algorithms using techniques from first-order optimization (Dalalyan, 2017; Durmus et al., 2019; Cheng and Bartlett, 2018; Vempala and Wibisono, 2019; Chewi et al., 2021). One typical MCMC algorithm that allows following the idea is the Langevin-type algorithms, which are of the central interest of this paper.

Langevin-type algorithms originated in statistical physics discretize a stochastic differential equation with stationary distribution corresponding to the target. For Gibbs distribution \(p(\mathbf{x})\propto e^{-U(\mathbf{x})}\), the standard overdamped Langevin algorithm uses Euler Maruyama scheme to discretize the diffusion process \(\mathrm{d}\mathbf{x}_{t}=-\nabla U(\mathbf{x}_{t})\mathrm{d}t+\sqrt{2}\mathrm{ d}\mathbf{B}_{t}\). The algorithm iteratively performs the updates as for \(n=0,1,2,\ldots,\)

\[\mathbf{x}_{n+1}=\mathbf{x}_{n}-h\nabla U(\mathbf{x}_{n})+\sqrt{2h} \boldsymbol{\epsilon}_{n}, \tag{1.1}\]

where \(\boldsymbol{\epsilon}_{n}\sim\mathcal{N}(0,I)\) follows the normal distribution and \(h>0\) is the step size.

This paper focuses on the dimension dependence of the convergence behavior of Langevin-type algorithms. Specifically, we consider sampling problems over potentially high-dimensional and strongly log-concave distributions with a composite structure: \(p\propto e^{-U(\mathbf{x})}=e^{-g(\mathbf{x})-f(\mathbf{x})}\) where \(g(\mathbf{x})=\frac{m}{2}\|\mathbf{x}\|^{2}\). Regarding \(p\) as a posterior distribution, \(e^{-g(\mathbf{x})}\) corresponds to a Gaussian prior and \(e^{-f(\mathbf{x})}\) corresponds to the likelihood. This structure also includes general \(m\)-strongly convexpotential functions \(U(\mathbf{x})\), which can be split into a strongly convex term \(g(\mathbf{x})=\frac{m}{2}\|\mathbf{x}\|^{2}\) and a weakly convex one \(f(\mathbf{x})=U(\mathbf{x})-\frac{m}{2}\|\mathbf{x}\|^{2}\).

The analysis of Langevin sampling from an optimization viewpoint may date back to Jordan et al. (1998). The viewpoint has led to a surge of works that establish quantitative convergence guarantees for overdamped Langevin algorithms to sample log-concave distributions (Dalalyan, 2017; Durmus and Moulines, 2016). It should be noted that these convergence guarantees always involve an extra \(d\) dimension dependence due to the injection of non-negligible Gaussian noise, whereas the convergence rates of first-order optimization are often dimension-free (Nesterov, 2003). Various accelerated methods have been proposed that can mitigate the dimension dependence and also achieve faster convergence. First, as the momentum acceleration of Langevin algorithms (Ma et al., 2021), underdamped Langevin Monte Carlo has a more stable trajectory and is known to exhibit a faster convergence rate (Cheng et al., 2018; Ma et al., 2021; Zhang et al., 2023). Apart from using a different trajectory, more stable discretization schemes of the same diffusion process also lead to better convergence (Shen and Lee, 2019; Wibisono, 2019; He et al., 2020; Li et al., 2019). However, the dimension dependence remains. To the best of our knowledge, the fastest randomized midpoint method for underdamped Langevin under Wasserstein distance achieves a \(\mathcal{O}\left(\frac{d^{1/3}}{\epsilon^{2/3}}\right)\) convergence rate and still has a \(d^{1/3}\) dimension dependence (Shen and Lee, 2019).

Recently another thread of works studies the convergence with weaker dimension dependence. Some researchers explore further general assumptions on the target distribution. The idea is by observing that some smoothness conditions can average out the dimension-dependent errors brought by the noise using Ito's formula. For example, Li et al. (2021) achieves a \(\sqrt{d}\) dimension dependence with a 3-rd order growth condition. Another approach investigates the curvature of the target distribution and illustrates that dimension often does not determine the complexity of the sampling problem. Vono et al. (2022) propose an ADMM-type splitting algorithm with a dimension-free convergence rate when the likelihood is separable. Along the same thread, Freund et al. (2022) study the convergence rate of Langevin algorithms with no explicit dependence on dimension and propose to characterize the convergence rate by the upper bound of Hessian matrices of \(f\). Specifically, by letting \(H\) be the upper bound of the Hessian matrices of \(f\), they show a variant of the overdamped Langevin algorithm achieves a convergence rate of \(\mathcal{O}\left(\frac{\operatorname{tr}(H)}{\epsilon}\right)\) in KL divergence. This result improves the rate under wide conditions because many high-dimensional sampling problems are intrinsically low-dimensional in the sense that \(\operatorname{tr}(H)=o(d)\), which frequently appears in machine learning. For example, when the potential function has a ridge separable structure with mild conditions, \(\operatorname{tr}(H)\) can be dimension-free (see Section 3.3 for more details).

Though these works succeed in obtaining a convergence rate of the Langevin algorithm with a weak dependence on the dimension, some important questions remain:

_" How to design provably faster algorithms with weak dependencies on dimension even for the log-concave sampling?"_

Such a question is significant for understanding high-dimensional sampling and already includes lots of applications in real practice.

In this paper, we follow the regime of Freund et al. (2022) to design provably faster Langevin algorithms, which answers the above question affirmatively. We propose a double-randomized algorithm showing that variants of underdamped algorithms inherit the lower dimensional dependence in overdamped Langevin and a special design of random stepsize can still keep the property when using more stable discretization.

Specifically, we develop a double randomization technique and obtain the Double-Randomized Underdamped Langevin (DRUL) algorithm. We consider an averaged contraction effect to avoid dimension dependence. We design two distributions to achieve the mid-point acceleration with a randomized stepsize. DRUL is proven to enjoy an overall \(\widetilde{\mathcal{O}}\left(\frac{(\operatorname{tr}(H))^{1/3}}{\epsilon^{2/ 3}}\right)\) iteration and gradient complexity. For the posterior sampling over linear models, we show a clear superiority of DRUL, which achieves a dimension-free \(\widetilde{\mathcal{O}}\left(\frac{1}{2^{\beta/3}}\right)\) convergence rate. The novel perspective of DRUL is introducing the random step size at each update. Such a random step size combined with a random midpoint point proposed by Shen and Lee (2019) reduces the discretization error and provides new insights into designing provably faster sampling algorithms.

In summary, the contributions of the paper are listed below:

* We propose the Double-Randomized technique and design the DRUL algorithm.
* We show the DRUL converges to the target distribution in Wasserstein distance in \(\widetilde{\mathcal{O}}\left(\frac{(\operatorname{tr}(H)+\|\mathbf{x}_{\star}\|^{ 2})^{1/3}}{\epsilon^{2/3}}\right)\) iterations. For posterior sampling over generalized linear models, a dimension-free \(\widetilde{\mathcal{O}}\left(\frac{1}{\epsilon^{2/3}}\right)\) complexity can be achieved.

## 2 Related works

There has been a surge of works investigating the asymptotic guarantees for the Langevin-type algorithms (Roberts and Tweedie, 1996; Mattingly et al., 2002). And a series of recent works establish the non-asymptotic quantitative analysis framework of the Langevin-type algorithms. For performance on strongly log-concave distributions, early works (Dalalyan, 2017; Durmus and Moulines, 2016) establish the non-asymptotic convergence rate in TV distance with Lipschitz gradients assumption for overdamped dynamics. Along the same setting, similar convergence rates are achieved in KL divergence using the Wasserstein gradient flow (Cheng and Bartlett, 2018; Durmus et al., 2019) or Wasserstein distance using the contractive property for overdamped Langevin. Underdamped Langevin accelerates the vanilla Langevin algorithms (Ma et al., 2021). With the same setting mentioned above, a faster convergence rate can be established (Cheng et al., 2018; Dalalyan and Riou-Durand, 2020; Shen and Lee, 2019; Zhang et al., 2023) for underdamped Langevin algorithms. Beyond the log Lipschitz-smooth setting, other examples also show better results can be achieved, such as Durmus and Moulines (2019); Li et al. (2019) for overdamped Langevin algorithms. Previous works also investigate the convergence rate without strongly log-convex assumptions, such as the log-Sobolev inequality (LSI) condition which is similar to the Polyak-Lojasiewicz condition in optimization. With target distributions satisfying LSI and log-Lipschitz-smooth, the results can be extended for both overdamped Langevin (Vempala and Wibisono, 2019) and underdamped Langevin (Ma et al., 2021; Zhang et al., 2023).

Another thread of works explores the dimension dependence of Langevin-type algorithms. Despite the great similarity between the analysis in Langevin algorithms and optimization, the convergence of Langevin-type algorithms depends on dimension in the log-concave setting whereas convex optimization algorithms can often achieve dimension-independent results. With general strongly log

\begin{table}
\begin{tabular}{c c c} \hline \hline
**Method** & **Convergence Rate** & **Ridge-Separable Case** \\ \hline Overdamped as Composite OptimizCRation & & \\ (Durmus et al., 2019) & \(\mathcal{O}\left(\frac{d}{\epsilon^{2}}\right)\) & \(\mathcal{O}\left(\frac{d}{\epsilon^{2}}\right)\) \\ (Proved in KL divergence) & & \\ \hline Overdamped with EU & & \\ (Li et al., 2021) & \(\widetilde{\mathcal{O}}\left(\frac{\sqrt{d}}{\epsilon}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{\sqrt{d}}{\epsilon}\right)\) \\ (With an additional linear growth condition) & & \\ \hline Underdamped with RMM & & \\ (Shen and Lee, 2019) & \(\widetilde{\mathcal{O}}\left(\frac{d^{1/3}}{\epsilon^{2/3}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{d^{1/3}}{\epsilon^{2/3}}\right)\) \\ \hline Overdamped as Composite Optimization & & \\ (Freund et al., 2022) & & \\ (Proved in KL divergence) & & \\ \hline
**DRUL (Ours)** & \(\widetilde{\mathcal{O}}\left(\frac{(\operatorname{tr}(H)+\|\mathbf{x}_{\star}\|^{ 2})^{1/3}}{\epsilon^{2/3}}\right)\) & \(\widetilde{\mathcal{O}}\left(\frac{1}{\epsilon^{2/3}}\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of the convergence rates for most related works. We consider the convergence in Wasserstein distance to a strongly log-concave and log-Lipschitz smooth target distribution (we summarize below the methods if additional assumptions are required). EU stands for Euler-Maruyama discretization and RMM stands for Randomized Midpoint discretization. ‘Ridge-Separable Case’ refers to the convergence rate when \(f\) admits a ridge-separable structure (see (3.2)). Note that some results are established in the KL divergence. We convert these convergence rates into the ones in Wasserstein distance using Talagrand’s inequality.

concave (or LSI) and log-Lipschitz-smooth condition, previous works establish a \(\mathcal{O}\left(\frac{d}{\epsilon^{2}}\right)\) convergence rate for overdamped algorithms (Durmus et al., 2019) and a \(\widetilde{\mathcal{O}}\left(\frac{\sqrt{d}}{\epsilon}\right)\) convergence rate for underdamped algorithms (Cheng et al., 2018) to ensure finding a solution \(x_{n}\) with \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}),p)\leq\epsilon\) or \(\sqrt{KL(\mathrm{Law}(\mathbf{x}_{n}),p)}\leq\epsilon\). With an additional linear growth condition on third-order derivative, Li et al. (2021) achieve a \(\widetilde{\mathcal{O}}\left(\frac{\sqrt{d}}{\epsilon}\right)\) convergence rate for overdamped Langevin. And for underdamped Langevin algorithms, Shen and Lee (2019) improve the dependence of dimension and obtains a \(\widetilde{\mathcal{O}}\left(\frac{d^{1/3}}{\epsilon^{2/3}}\right)\) convergence rate. Recent work of Freund et al. (2022) characterizes the dimensional dependence of convergence rate by the upper bound of the Hessian matrix of \(f\). As discussed in Section 1, Freund et al. (2022) establish a \(\mathcal{O}\left(\frac{\mathrm{tr}(H)+\|\mathbf{x}_{n}\|^{2}}{\epsilon}\right)\) convergence rate in KL divergence, which implies a \(\mathcal{O}\left(\frac{\mathrm{tr}(H)+\|\mathbf{x}_{n}\|^{2}}{\epsilon^{2}}\right)\) convergence rate in Wasserstein distance. And when \(f\) admits a ridge-separable formula, Freund et al. (2022) obtain a dimension-free \(\mathcal{O}\left(\frac{1}{\epsilon^{2}}\right)\) convergence rate with some mild assumptions. We summarize the comparison of these most related works in Table 2.

## 3 Preliminary and problem setup

### Notations

We use the convention \(\mathcal{O}\left(\cdot\right)\) and \(\Omega\left(\cdot\right)\) to denote lower and upper bounds with a universal constant. \(\widetilde{\mathcal{O}}(\cdot)\) ignores the polylogarithmic dependence. And use \(f\lesssim g\) to denote \(f=\mathcal{O}(g)\). Use \(W_{2}(\mu,\nu)\) to denote the \(2\)-Wasserstein distance of distribution \(\mu\) and \(\nu\). Use \(\mathrm{Law}(X)\) to denote the distribution of the random variable \(X\). The Frobenius norm is denoted by \(\|\cdot\|_{F}\) while \(\|\cdot\|_{2}\) stands for operator \(2\)-norm for matrices.

### Sampling problem

We consider the distributions with a composite structure:

\[\mathrm{d}p(\mathbf{x})\propto\exp\{-U(\mathbf{x})\}\mathrm{d} \mathbf{x}=\exp\left\{-g(\mathbf{x})-f(\mathbf{x})\right\}\mathrm{d}\mathbf{x}, \tag{3.1}\]

where \(g(\mathbf{x})=\frac{m}{2}\|\mathbf{x}\|^{2}\) is a quadratic function. In the context of posterior sampling, the associated task is sampling from a distribution with a Gaussian prior. The composite structure also includes the general \(m\)-strongly convex function, which can be divided into \(g(\mathbf{x})=\frac{m}{2}\|\mathbf{x}\|^{2}\) and the weakly convex function \(f(\mathbf{x})=U(\mathbf{x})-\frac{m}{2}\|\mathbf{x}\|^{2}\). We make the following assumption on \(f\).

**Assumption 3.1**.: \(f\in\mathcal{C}^{2}\) is convex and has \(L\)-Lipschitz continuous gradients, i.e. \(\mathbf{0}\preceq\nabla^{2}f\preceq LI\).

It corresponds to making \(m\)-strongly convex and \(L+m\)-Lipschitz smooth assumptions on the potential function \(U\), which is a basic setting and widely studied in the Langevin sampling literature (see e.g. Dalalyan (2017), Cheng and Bartlett (2018), Cheng et al. (2018), Shen and Lee (2019)).

In addition to the previous assumption, we follow Freund et al. (2022) to characterize the convergence rate by a new factor \(H\) to avoid explicit dimension dependence.

**Definition 3.2**.: Let \(H\) be an upper bound of the Hessian matrices of \(f\), i.e. \(H\succeq\nabla^{2}f(\mathbf{x})\).

Note that the Lipschitz smooth condition in Assumption 3.1 provides a loose bound for \(H\) since we always have \(H\preceq LI\). This implies that \(\mathrm{tr}(H)\) will reach \(dL\) in the worst case, whereas this quantity can be much smaller than \(dL\) under wide conditions. One typical example is when \(f\) admits a so-called ridge separable structure shown below.

### Example: ridge separable functions

**Definition 3.3**.: \(f\) is said to admit the ridge separable form if

\[f(\mathbf{x})=\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}(\mathbf{a}_{i}^{T}\mathbf{x}), \tag{3.2}\]

where \(\sigma_{i}\) are all univariate functions, and \(\mathbf{a}_{i}\) are given vectors in \(\mathbb{R}^{d}\).

Ridge separable functions contain many applications in machine learning, such as regression or classification over generalized linear models as well as (deep) neural networks in the neural tangent kernel regime (Gelman and Hill, 2006; Jacot et al., 2018). We follow the argument of Freund et al. (2022), showing realizable conditions that ensure \(\mathrm{tr}(H)\) to be dimension-free.

**Assumption 3.4**.: The function \(\sigma_{i}\in\mathcal{C}^{2}\) has a bounded second derivative, i.e. \(\sigma_{i}^{\prime\prime}\leq L_{0}\) for all \(i\in[n]\).

**Assumption 3.5**.: For all \(i\in[n]\), then norm of \(\mathbf{a}_{i}\) is bounded by \(R\), i.e. \(\|\mathbf{a}_{i}\|^{2}\leq R^{2}\).

When Assumptions 3.4 and 3.5 hold, the Hessian has the upper bound \(\nabla^{2}f(\mathbf{x})=\sum_{i=1}^{n}\frac{1}{n}\sigma^{\prime\prime}(\mathbf{ a}_{i}^{T}\mathbf{x})\mathbf{a}_{i}\mathbf{a}_{i}^{T}\preceq\frac{L_{0}}{n} \sum_{i=1}^{n}\mathbf{a}_{i}\mathbf{a}_{i}^{T}\). Let \(H=\frac{L_{0}}{n}\sum_{i=1}^{n}\mathbf{a}_{i}\mathbf{a}_{i}^{T}\) and

\[\mathrm{tr}(H)=\mathrm{tr}\left(\sum_{i=1}^{n}\frac{L_{0}}{n}\mathbf{a}_{i} \mathbf{a}_{i}^{T}\right)=\frac{L_{0}}{n}\sum_{i=1}^{n}\|\mathbf{a}_{i}\|^{2} \leq L_{0}R^{2},\]

thus illustrates that \(\mathrm{tr}(H)\) has a dimension-free \(L_{0}R^{2}\) upper bound. Meanwhile the Lipschitz constant of \(\nabla f\) can be bounded by

\[\mathrm{Lip}(\nabla f)\leq\|H\|_{2}=\left\|\frac{L_{0}}{n}\sum_{i=1}^{n} \mathbf{a}_{i}\mathbf{a}_{i}^{T}\right\|\leq L_{0}R^{2}.\]

It indicates that worst-case upper bounds of \(\mathrm{tr}(H)\) and \(\mathrm{Lip}(\nabla f)\) are the same.

Note that Assumptions 3.4 and 3.5 are easy to achieve in practice. For example, consider using posterior sampling to compute the Bayes estimator over a linear model, whose advantages against maximum a posterior estimator have been discussed broadly (see e.g. Audibert (2009)). Here, \(\mathbf{a}_{i}\) is associated with the data. So Assumption 3.5 can be realized by simply normalizing the data. Moreover, \(\sigma_{i}\) corresponds to the loss function and is only required to have a bounded second derivative by Assumption 3.4. Note that sampling from a ridge separable potential functions are extensively studied in related literature (see (Mou et al., 2021; Vono et al., 2022; Lee et al., 2018) as examples).

### Preliminary

**Overdamped Langevin.** The overdamped Langevin dynamics for target distribution (3.1) is a diffusion process that evolves along the following SDE

\[\mathrm{d}\mathbf{x}(t)=-\nabla g(\mathbf{x}(t))\mathrm{d}t-\nabla f(\mathbf{x }(t))\mathrm{d}t+\sqrt{2}\mathrm{d}\mathbf{B}_{t} \tag{3.3}\]

where \(\mathbf{B}_{t}\) is the standard Brownian motion. The overdamped Langevin algorithms simulate and discretize the SDE (3.3). Different algorithms vary mainly by different discretization methods.

**Underdamped Langevin** Underdamped Langevin algorithms accelerate the convergence rate of overdamped Langevin algorithms and instead discretize the following dynamics

\[\mathrm{d}\mathbf{x}(t) =\mathbf{v}(t)\mathrm{d}t, \tag{3.4}\] \[\mathrm{d}\mathbf{v}(t) =-u\nabla g(\mathbf{x}(t))\mathrm{d}t-u\nabla f(\mathbf{x}(t)) \mathrm{d}t-2\mathbf{v}(t)\mathrm{d}t+2\sqrt{u}\mathrm{d}\mathbf{B}_{t}.\]

The diffusion process has the stationary distribution \(p_{(\mathbf{x},\mathbf{v})}\propto\exp\left\{-\frac{1}{2u}\|\mathbf{v}\|^{2}- U(\mathbf{x})\right\}\).

**Contractive Property** One notable property that yields the convergence of Langevin dynamics is the contractive property (Cheng and Bartlett, 2018; Cheng et al., 2018), given as follows.

**Definition 3.6**.: A stochastic differential equation has contractive property if there exists a positive constant \(m\) and

\[\mathbb{E}\|\mathbf{x}(t)-\mathbf{y}(t)\|^{2}\leq\mathbb{E}\|\mathbf{x}(0)- \mathbf{y}(0)\|^{2}\exp(-mt), \tag{3.5}\]

for any pair of solutions \(\mathbf{x}(t)\) and \(\mathbf{y}(t)\) driven by the same Brownian motion.

**Remark 3.7**.: \(\mathbb{E}\|\mathbf{x}(t)-\mathbf{y}(t)\|^{2}\) is an upper bound of squared Wasserstein distance of \(\mathrm{Law}(\mathbf{x}(t))\) and \(\mathrm{Law}(\mathbf{y}(t))\), and thus (3.5) implies a geometric convergence of the Wasserstein distance.

Contractive property can be established for both underdamped and overdamped Langevin dynamics under suitable conditions. The convergence analysis for our proposed algorithms follows a similar argument as the contractive property.

```
Iteration \(N\), target function \(U=\frac{m}{2}\|\mathbf{x}\|^{2}+f(\mathbf{x})\), initial point \((\mathbf{x}_{0},\mathbf{v}_{0})\), max step size \(h\), \(u=\frac{1}{m+L}\) and \(\kappa=\frac{L+m}{m}\). \(\text{set }\rho(\mathrm{d}t)\propto\left(\frac{1}{h}-\frac{t}{h\kappa}\right) \mathrm{d}t\) with support \([0,h]\). \(\text{set }\rho^{\prime}(\mathrm{d}t)\propto\left(e^{\frac{t-h}{\kappa}}-\frac{t}{h} \right)\mathrm{d}t\) with support \([0,h]\). for\(n=1,2,\cdots,N\)do  Sample \(\alpha_{n}\sim\rho^{\prime}\) and \(\beta_{n}\sim\rho\).  Obtain the covariance matrix \(\Sigma(\alpha_{n},\beta_{n})\).  Sample random vector \((G,H,W)\) from distribution \(\mathcal{N}(0,\Sigma)\). \(\widehat{\mathbf{x}}_{n}\gets A_{11}(\alpha_{n})\mathbf{x}_{n}+A_{12}( \alpha_{n})\mathbf{v}_{n}+u\int_{0}^{\alpha_{n}}A_{12}(s-\alpha_{n})\nabla f( \mathbf{x}_{n})\mathrm{d}s+2\sqrt{u}H\).  Update \(\mathbf{x}_{n+1}\gets A_{11}(\beta_{n})\mathbf{x}_{n}+A_{12}(\beta_{n}) \mathbf{v}_{n}+u\int_{0}^{\beta_{n}}A_{12}(s-\beta_{n})\nabla f(\widehat{ \mathbf{x}}_{n})\mathrm{d}s+2\sqrt{u}G\).  Update \(\mathbf{v}_{n+1}\gets A_{21}(\beta_{n})\mathbf{x}_{n}+A_{22}(\beta_{n}) \mathbf{v}_{n}+u\int_{0}^{\beta_{n}}A_{22}(s-\beta_{n})\nabla f(\widehat{ \mathbf{x}}_{n})\mathrm{d}s+2\sqrt{u}W\). endfor
```

**Algorithm 1** Double-Randomized Underdamped Langevin (DRUL)

## 4 Double-randomized underdamped Langevin algorithm

In this section, we introduce our double-randomized sampling method and present our main result. We will illustrate our intuition in Section 5.

The proposed algorithm is built upon the following discretization scheme given a fixed point \(\widetilde{x}_{n}\)

\[\mathrm{d}\mathbf{x}_{n}(t) =\mathbf{v}_{n}(t)\mathrm{d}t, \tag{4.1}\] \[\mathrm{d}\mathbf{v}_{n}(t) =-u\nabla g(\mathbf{x}_{n}(t))\mathrm{d}t-u\nabla f(\widetilde{ \mathbf{x}}_{n})\mathrm{d}t-2\mathbf{v}_{n}(t)\mathrm{d}t+2\sqrt{u}\mathrm{d} \mathbf{B}_{t}.\]

The purpose of splitting the strongly convex part of \(U(x)\) is to avoid the \(md\) dimension dependence of \(\mathrm{tr}(H)\). Although \(m\) is reasonably small in practice, one cannot obtain a fully dimensional-free convergence rate if the term with respect to \(g\) is discretized.

Denote the solution of process (4.1) at time \(t\) given starting point \((\mathbf{x}_{n},\mathbf{v}_{n})\), Brownian motion \(\{\mathbf{B}_{t}\}_{0\leq t\leq\beta}\), step size \(\beta\) and point \(\widetilde{x}_{n}\) by \(\mathcal{J}(\beta,\widetilde{\mathbf{x}}_{n};\{\mathbf{B}_{t}\}_{0\leq t\leq \beta},(\mathbf{x}_{n},\mathbf{v}_{n}))\). The double-randomized algorithm performs the following one-step update:

\[\mathbf{x}_{k+1}=\mathcal{J}(\beta,\mathcal{J}(\alpha,\mathbf{x}_{n};\{ \mathbf{B}_{t}\}_{0\leq t\leq\alpha},(\mathbf{x}_{n},\mathbf{v}_{n}));\{ \mathbf{B}_{t}\}_{0\leq t\leq\beta},(\mathbf{x}_{n},\mathbf{v}_{n})). \tag{4.2}\]

The algorithm introduces a random step size \(\beta\sim\rho\propto\left(\frac{1}{h}-\frac{t}{h\kappa}\right)\) defined on \([0,h]\) other than deterministic \(h\). And let \(\alpha\) follows the distribution \(\rho^{\prime}\propto\left(e^{\frac{t-h}{\kappa}}-\frac{t}{h}\right)\) on \([0,h]\).

The analysis focuses on a Gaussian prior setting. Under a Gaussian prior, the following Lemma points out that \(\mathcal{J}\) is linear in starting point \((\mathbf{x}_{n},\mathbf{v}_{n})\) and has a decoupled Brownian motion, and thus the update (4.2) can be easily implemented.

**Lemma 4.1**.: Assume \(g(\mathbf{x})=\frac{m}{2}\|\mathbf{x}\|^{2}\). If \(\mathbf{x}_{n}(t)\) follows the discretized Langevin diffusion process (4.1) with starting point \((\mathbf{x}_{n},\mathbf{v}_{n})\) and a given \(\widetilde{\mathbf{x}}_{n}\), for any \(t>0\), it satisfies the integral equation

\[\mathbf{x}_{n}(t)=\mathcal{J}(t,\widetilde{\mathbf{x}}_{n};\{ \mathbf{B}_{s}\}_{0\leq s\leq t}, (\mathbf{x}_{n},\mathbf{v}_{n}))=A_{11}(t)\mathbf{x}_{n}+A_{12}(t )\mathbf{v}_{n} \tag{4.3}\] \[+u\int_{0}^{t}A_{12}(s-t)\nabla f(\widetilde{\mathbf{x}}_{n}) \mathrm{d}s+2\sqrt{u}\int_{0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s},\]

where

\[A_{11}(t)= \frac{\sqrt{1-um}-1}{2\sqrt{1-um}}e^{(-1-\sqrt{1-um})t}+\frac{ \sqrt{1-um}+1}{2\sqrt{1-um}}e^{(-1+\sqrt{1-um})t},\] \[A_{12}(t)= -\frac{1}{2\sqrt{1-um}}e^{(-1-\sqrt{1-um})t}+\frac{1}{2\sqrt{1-um }}e^{(-1+\sqrt{1-um})t}\]are deterministic functions of \(t\). Moreover, if \(\mathbf{x}_{n}^{*}(t)\) follows the exact Langevin process (3.4) with starting point \((\mathbf{x}_{n},\mathbf{v}_{n})\), then for any \(t>0\)

\[\begin{split}\mathbf{x}_{n}^{*}(t)=A_{11}(t)\mathbf{x}_{n}& +A_{12}(t)\mathbf{v}_{n}+u\int_{0}^{t}A_{12}(s-t)\nabla f(\mathbf{x}_{n}^{*}( s))\mathrm{d}s\\ &+2\sqrt{u}\int_{0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}.\end{split} \tag{4.4}\]

Given the integral formula (4.3), the algorithm can be summarized as Algorithm 1. In Algorithm 1, \(A_{21}\), \(A_{22}\) is deterministic scalar functions, and \(\Sigma:\mathbb{R}^{2}\to\mathbb{R}^{3d\times 3d}\) is also deterministic. For the explicit formula, please refer to Appendix A. The distribution of \((H,G,W)\) is induced by the coupling between \(\{\mathbf{B}_{t}\}_{0\leq t\leq\alpha}\) and \(\{\mathbf{B}_{t}\}_{0\leq t\leq\beta}\) in (4.2).

### Main theorem

The convergence guarantee of DRUL can be stated as the following theorem.

**Theorem 4.2** ( **Main theorem**, convergence of DRUL).: For any tolerance \(\epsilon\in(0,1)\), denote the minimizer of \(U(\mathbf{x})\) by \(\mathbf{x}_{*}\) and set the step size

\[h\leq\min\left\{\frac{1}{12C_{2}\kappa},\frac{\epsilon^{2/3}}{\left(24C_{2} \kappa(\frac{1}{m(L+m)}\mathrm{tr}(H)+\|\mathbf{x}_{*}\|^{2})\right)^{1/3}} \right\},\]

where \(C_{2}\geq 1\) is a universal constant. With initial point \((\mathbf{x}_{0},\mathbf{v}_{0})\), define \(\Omega_{0}=\mathbb{E}_{\mathbf{x}\sim p,\mathbf{v}\sim\mathcal{N}(0,u)}\left(\| \mathbf{x}_{0}-\mathbf{x}\|^{2}+\|\mathbf{x}_{0}+\mathbf{v}_{0}-\mathbf{v}- \mathbf{x}\|^{2}\right)\). Then under Assumptions 3.1, when

\[n\geq\frac{8\epsilon\kappa}{h}\log\left(\frac{2\mathbb{E}\Omega_{0}}{\epsilon^{ 2}}\right),\]

Algorithm 1 outputs \(\mathbf{x}_{n}\) such that \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}),p)\leq\epsilon\).

Theorem 4.2 shows an overall \(\widetilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)+\|\mathbf{x}_{*}\|^{2} \right)^{1/3}}{\epsilon^{2/3}}\right)\) iteration and gradient complexity to find an \(\epsilon\)-approximate sample in \(2\)-Wasserstein distance. By pre-finding \(\mathbf{x}_{*}\) using a convex optimization algorithm and linearly drifting the coordinates, we can assume \(\mathbf{x}_{*}=\mathbf{0}\) without loss of generality. DRUL admits the stepsize reaching \(\mathcal{O}\left(\frac{\epsilon^{2/3}}{\left(\mathrm{tr}(H)\right)^{1/3}}\right)\) and the overall complexity is \(\widetilde{\mathcal{O}}\left(\frac{\left(\mathrm{tr}(H)\right)^{1/3}}{ \epsilon^{2/3}}\right)\).

Now we consider a realizable case where our result achieves better complexity on \(d\). The concrete example is that \(f\) admits a separable structure as discussed in Section 3.3. Corollary 4.3 below shows that when \(f\) admits a ridge separable structure, Algorithm 1 enjoys a dimension-free iteration and gradient complexity.

**Corollary 4.3**.: Follow the notations in Theorem 4.2. Further, if \(f(\mathbf{x})\) admits a ridge-separable form and satisfies Assumptions 3.4 and 3.5. Then if we set the step size

\[h\leq\min\left\{\frac{1}{12C_{2}\kappa},\frac{\epsilon^{2/3}}{\left(24C_{2} \kappa(\frac{1}{m}+\|\mathbf{x}_{*}\|^{2})\right)^{1/3}}\right\},\]

With initial point \((\mathbf{x}_{0},\mathbf{v}_{0})\) and \(\Omega_{0}=\mathbb{E}_{\mathbf{x}\sim p,\mathbf{v}\sim\mathcal{N}(0,u)}\left( \|\mathbf{x}_{0}-\mathbf{x}\|^{2}+\|\mathbf{x}_{0}+\mathbf{v}_{0}-\mathbf{v}- \mathbf{x}\|^{2}\right)\). Then when

\[n\geq\frac{8\epsilon\kappa}{h}\log\left(\frac{2\mathbb{E}\Omega_{0}}{\epsilon^{ 2}}\right),\]

Algorithm 1 outputs \(\mathbf{x}_{n}\) such that \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}),p)\leq\epsilon\).

**Discussion.** Theorem 4.2 establishes the convergence rate of DRUL with a weak dimension dependence. We shall note that for lots of high-dimensional sampling problems, the trace of the Hessian matrices for the potential function is much smaller than \(d\) times the largest eigenvalue because the eigenvalues often drop rapidly. In practice, this situation has been considered in lots of topics. For example, one defines the effective rank (e.g. Hsu et al. (2012)) to represent the acting dimension of the data on linear models. In distributed machine learning, one can show fewer bits are needed to be transmitted between machines when the eigenvalues decrease fast (e.g. Hanzely et al. (2018)). As shown in Figure 1(a), the Gram matrix of the MNIST dataset, which is also the Hessian of Bayesian ridge linear regression, has rapidly decreasing eigenvalues. Similar empirical results are observed on deep neural network models by Sagun et al. (2016), as in Figure 1(b). In fact, beyond the ridge separable case, there are many problems admitting \(\mathrm{tr}(H)=o(d)\), such as the concentrated posterior distributions with bounded gradients and neural networks with regularization. However, one may notice that the result of Theorem 4.2 is based on a uniform upper bound of the Hessian matrices. We think this is the basic case to show that our algorithm achieves convergence with an intrinsically low-dimension dependence. It is possible to relax the condition to the upper bound of the traces for local Hessian matrices with an additional Hessian smoothness assumption. Please see more discussions in Appendix E.

## 5 Intuition

In this section, we provide the intuitions of the algorithm design. For the simplicity of the notation, let \(\widehat{\mathbf{x}}_{n}(s)=\mathcal{J}(s,\mathbf{x}_{n};\{\mathbf{B}_{t}\}_{0 \leq t\leq s},(\mathbf{x}_{n},\mathbf{v}_{n}))\) and \(\mathbf{x}_{n}(s)=\mathcal{J}(s,\mathcal{J}(\alpha,\mathbf{x}_{n};\{\mathbf{B }_{t}\}_{0\leq t\leq\alpha},(\mathbf{x}_{n},\mathbf{v}_{n}));\{\mathbf{B}_{t }\}_{0\leq t\leq s},(\mathbf{x}_{n},\mathbf{v}_{n}))\). Then given \(\alpha\) and \(\beta\) in Algorithm 1, \(\mathbf{x}_{n}(s)=\mathcal{J}(s,\widehat{\mathbf{x}}_{n}(\alpha);\{\mathbf{B}_ {t}\}_{0\leq t\leq\beta},(\mathbf{x}_{n},\mathbf{v}_{n}))\) and \(\mathbf{x}_{n}(\beta)=\mathbf{x}_{n+1}\).

Our analysis tracks the dynamics of the distance to the stationary distribution. Specifically, we analysis the dynamics of \(\mathbb{E}\Omega_{n}(t)=\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)+ \mathbf{v}_{n}(t)-\mathbf{v}_{n}^{*}(t)\|^{2}+\mathbb{E}\|\mathbf{x}_{n}(t)- \mathbf{x}_{n}^{*}(t)\|^{2}\) and \(\mathbb{E}\Omega_{n}=\mathbb{E}\Omega_{n}(0)\), which upper bound the squared 2-Wasserstein distance \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}(t)),p)^{2}\) and \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}),p)^{2}\), respectively. Here, \((\mathbf{x}_{n}^{*}(t),\mathbf{v}_{n}^{*}(t))\) evolving along the exact Langevin diffusion (3.4) follows the stationary distribution and is synchronously coupled with \((\mathbf{x}_{n}(t),\mathbf{v}_{n}(t))\). One can find its formal definition in Appendix B. Via tracking the flow and using the contraction property of the process, the following Lemma characterizes the one-step discretization.

**Lemma 5.1**.: Let \(\mathbf{x}_{n}\) be the \(n\)-step output of Algorithm 1 and \(\mathbf{x}_{n}^{*}(t)\) be defined as above. Set \(u=\frac{1}{L+m}\). Under Assumptions 3.1, given that \(\mathbf{x}_{n}\) and \(\mathbf{x}_{n}^{*}\) are coupled synchronously, we have for any \(h>0\)

\[\mathbb{E}\Omega_{n+1}=\mathbb{E}\mathbb{E}_{\beta\sim\rho}\Omega_{n}(\beta) \leq\mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}}\mathbb{E}\Omega_{n}+ \mathcal{E} \tag{5.1}\]

where \(\mathcal{E}\) is

\[\mathcal{E}=2u\mathbb{E}\mathbb{E}_{\alpha\sim\rho^{\prime}}\mathbb{E}_{\beta \sim\rho}\int_{0}^{\beta}e^{\frac{z-\beta}{\kappa}}\langle\mathbf{x}_{n}(s)- \mathbf{x}_{n}^{*}(s)+\mathbf{v}_{n}(s)-\mathbf{v}_{n}^{*}(s),\nabla f(\mathbf{ x}_{n}(s))-\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\rangle\mathrm{d}s.\]

Lemma 5.1 indicates at each step, \(\mathbb{E}\Omega_{n}\) contracts with a local discretization error. Telescoping (5.1) and upper bounding the term \(\mathcal{E}\) yields the Theorem 4.2. To obtain the convergence rate in the

Figure 1: A demonstration of the eigenvalues of the Hessian matrix. (a) The eigenvalues of the Gram matrix of MNIST data. (b) Eigenvalues of a three-layer neural network from Sagun et al. (2016).

main theorem, we will show that Algorithm 1 guarantees an upper bound of \(\mathcal{E}\) which (1) achieves the state-of-the-art dependence on stepsize and (2) satisfies that the dimension dependence can be controlled by the contraction.

**Improved discretization error.** We accomplish the first goal by matching the expectation, which is detailed by Lemma 5.2.

**Lemma 5.2**.: Let \(\rho\) be probability measure defined on \([0,h]\) satisfying \(\rho(\mathrm{d}t)\propto\left(\frac{1}{h}-\frac{t}{h\kappa}\right)\mathrm{d}t\), and the probability measure \(\rho^{\prime}\) defined on \([0,h]\) satisfies \(\rho^{\prime}(\mathrm{d}t)\propto\left(e^{\frac{t-h}{\kappa}}-\frac{t}{h} \right)\mathrm{d}t\) on \([0,h]\). Then for measurable function \(F(t)\), \(\rho\) and \(\rho^{\prime}\) satisfy that

1. There exists positive constant \(C_{1}\) such that \(\mathbb{E}_{t\sim\rho}\int_{0}^{t}e^{\frac{t-t}{\kappa}}F(t)\mathrm{d}s=C_{1}h \mathbb{E}_{t\sim\rho^{\prime}}F(t)\).
2. There exists positive constant \(C_{2}\) such that \(\mathbb{E}_{t\sim\rho^{\prime}}|F(t)|\leq C\mathbb{E}_{t\sim\rho}|F(t)|\).

Claim (A) in Lemma 5.2 states that by choosing a random step size \(\beta\sim\rho^{\prime}\), we can leverage the low discretization error of the randomized midpoint method. Denote the random weight \(\mathbf{w}_{n}(s,\alpha)=\mathbf{x}_{n}(s)-\mathbf{x}_{n}^{*}(s)+\mathbf{v}_{n} (s)-\mathbf{v}_{n}^{*}(s)\). One can split \(\mathbf{w}_{n}(s,\alpha)\) into \((\mathbf{w}_{n}(s,\alpha)-\mathbf{w}_{n}(0,0))+\mathbf{w}_{n}(0,0)\). The former term can be bounded using the one-step move, and by claim (A), the dominating latter one is

\[2u\mathbb{E}\mathbb{E}_{\alpha\sim\rho^{\prime}}\mathbb{E}_{\beta \sim\rho}\int_{0}^{\beta}e^{\frac{t-\beta}{\kappa}}\langle\mathbf{w}(0,0), \nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\rangle \mathrm{d}s\] \[= 2uC_{1}h\langle\mathbf{w}(0,0),\mathbb{E}\mathbb{E}_{s\sim\rho^{ \prime}}\big{(}\nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{\mathbf{x}}_{n}(s ))\big{)}\rangle,\]

which attains a low discretization error given that \(\mathbb{E}_{s\sim\rho^{\prime}}\nabla f(\widehat{\mathbf{x}}_{n}(s))\) is a low biased approximation to \(\mathbb{E}_{s\sim\rho^{\prime}}\nabla f(\mathbf{x}_{n}(s))\).

**Averaged contraction can control the weight variance.** Then we consider the variation of the weight \(w_{n}(s,\alpha)\). The time difference of \(\mathbf{w}_{n}(s,\alpha)\) writes

\[\mathbf{w}(s,\alpha)-(\mathbf{x}_{n}-\mathbf{x}_{n}^{*}+ \mathbf{v}_{n}-\mathbf{v}_{n}^{*})=\int_{0}^{s}(\mathbf{v}_{n}(r) -\mathbf{v}_{n}^{*}(r)-um(\mathbf{x}_{n}(r)-\mathbf{x}_{n}^{*}(r)) \tag{5.2}\] \[-2(\mathbf{v}_{n}(r)-\mathbf{v}_{n}^{*}(r))-u\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))+u\nabla f(\mathbf{x}_{n}^{*}(r)))\mathrm{d}r.\]

(5.2) indicates the variance \(\mathrm{Var}_{s\sim\mu}(\mathbf{w}(s,\alpha))\) is dimension dependent for nondegenerated distributions \(\mu\) on \([0,h]\), since it will introduce the \(\mathbf{v}_{n}(r)\) whose difference to \(\mathbf{v}_{n}(\beta)\) or \(\mathbf{v}_{n}(0)\) is dimension dependent. We control the dimension dependence via the averaged contraction. And the randomized step size makes it possible to consider the averaged effect. We have the following Lemma to bound the variation of the weight.

**Lemma 5.3**.: Let \(\mathbf{x}_{n}(t),\mathbf{v}_{n}(t),\mathbf{v}_{n}^{*}(t)\) and \(\mathbf{v}_{n}^{*}(t)\) be defined as above and \(u=\frac{1}{L+m}\). Under Assumption 3.1, for any \(t,\alpha\leq h\), we have

\[\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}( t)+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+ \mathbf{v}_{n}^{*}\|^{2} \tag{5.3}\] \[\lesssim h^{2}\mathbb{E}\mathbb{E}_{t\sim\rho}\Omega_{n}(t)+h^{4}\mathbb{E} \Omega_{n}+u^{2}h^{4}\frac{L}{m}\mathrm{tr}(H)+h^{4}\|\mathbf{x}_{*}\|^{2}.\]

Now the dimension dependence of \(\mathrm{Var}(\mathbf{w}(s,\alpha))\) is contained in \(\mathbb{E}_{t\sim\rho}\Omega_{n}(t)\), which can be controlled using the averaged contraction under the stochastic step size.

## 6 Conclusion

This paper proposes a double-randomized technique and designs the DRUL algorithm. We prove that with strongly convex and Lipschitz smooth assumptions potentials, the algorithm converges to the target distribution in Wasserstein distance in \(\widetilde{\mathcal{O}}\left(\frac{(\mathrm{tr}(H)+\|\mathbf{x}_{n}\|^{2})^{1/3 }}{\epsilon^{2/3}}\right)\) iterations. The result illustrates that many sampling tasks in machine learning can achieve a dimension-independent complexity. The proposed DRUL algorithm can be potentially much faster than existing algorithms for high-dimensional problems. As a concrete example, when the negative log-likelihood function admits a ridge-separable structure, under mild conditions, a dimension-free \(\widetilde{\mathcal{O}}\left(\frac{1}{\epsilon^{2/3}}\right)\) iteration complexities can be obtained by DRUL. We hope our technique brings new insights for designing dimension-independent algorithms for high-dimensional sampling.

Acknowledgement

C. Fang was supported by National Key R\(\&\)D Program of China (2022ZD0114902), the NSF China (No. 62376008) and Wudao Foundation. T. Zhang was supported by the General Research Fund (GRF) of Hong Kong (No. 16310222).

## References

* Liu and Liu (2001) Jun S Liu. _Monte Carlo strategies in scientific computing_, volume 75. Springer, 2001.
* Brooks et al. (2011) Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng. _Handbook of markov chain monte carlo_. CRC press, 2011.
* Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020.
* Dalalyan (2017) Arnak S Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave densities. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 79(3):651-676, 2017.
* Durmus et al. (2019) Alain Durmus, Szymon Majewski, and Blazej Miasojedow. Analysis of langevin monte carlo via convex optimization. _The Journal of Machine Learning Research_, 20(1):2666-2711, 2019.
* Cheng and Bartlett (2018) Xiang Cheng and Peter Bartlett. Convergence of langevin mcmc in kl-divergence. In _Algorithmic Learning Theory_, pages 186-211. PMLR, 2018.
* Vempala and Wibisono (2019) Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices. _Advances in neural information processing systems_, 32, 2019.
* Chewi et al. (2021) Sinho Chewi, Murat A Erdogdu, Mufan Bill Li, Ruoqi Shen, and Matthew Zhang. Analysis of langevin monte carlo from poincar\(\backslash\)'e to log-sobolev. _arXiv preprint arXiv:2112.12662_, 2021.
* Jordan et al. (1998) Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker-planck equation. _SIAM journal on mathematical analysis_, 29(1):1-17, 1998.
* Durmus and Moulines (2016) Alain Durmus and Eric Moulines. Sampling from a strongly log-concave distribution with the unadjusted langevin algorithm. 2016.
* Nesterov (2003) Yurii Nesterov. _Introductory lectures on convex optimization: A basic course_, volume 87. Springer Science & Business Media, 2003.
* Ma et al. (2021) Yi-An Ma, Niladri S Chatterji, Xiang Cheng, Nicolas Flammarion, Peter L Bartlett, and Michael I Jordan. Is there an analog of nesterov acceleration for gradient-based mcmc? _Bernoulli_, 27(3):1942-1992, 2021.
* Cheng et al. (2018) Xiang Cheng, Niladri S Chatterji, Peter L Bartlett, and Michael I Jordan. Underdamped langevin mcmc: A non-asymptotic analysis. In _Conference on learning theory_, pages 300-323. PMLR, 2018.
* Zhang et al. (2023) Matthew Zhang, Sinho Chewi, Mufan Bill Li, Krishnakumar Balasubramanian, and Murat A Erdogdu. Improved discretization analysis for underdamped langevin monte carlo. _arXiv preprint arXiv:2302.08049_, 2023.
* Shen and Lee (2019) Ruoqi Shen and Yin Tat Lee. The randomized midpoint method for log-concave sampling. _Advances in Neural Information Processing Systems_, 32, 2019.
* Wibisono (2019) Andre Wibisono. Proximal langevin algorithm: Rapid convergence under isoperimetry. _arXiv preprint arXiv:1911.01469_, 2019.
* He et al. (2020) Ye He, Krishnakumar Balasubramanian, and Murat A Erdogdu. On the ergodicity, bias and asymptotic normality of randomized midpoint sampling method. _Advances in Neural Information Processing Systems_, 33:7366-7376, 2020.
* Li et al. (2019) Xuechen Li, Yi Wu, Lester Mackey, and Murat A Erdogdu. Stochastic runge-kutta accelerates langevin monte carlo and beyond. _Advances in neural information processing systems_, 32, 2019.
* Li et al. (2021) Ruilin Li, Hongyuan Zha, and Molei Tao. Sqrt (d) dimension dependence of langevin monte carlo. _arXiv preprint arXiv:2109.03839_, 2021.
* Li et al. (2020)Maxime Vono, Daniel Paulin, and Arnaud Doucet. Efficient mcmc sampling with dimension-free convergence rate using admm-type splitting. _The Journal of Machine Learning Research_, 23(1):1100-1168, 2022.
* Freund et al. (2022) Yoav Freund, Yi-An Ma, and Tong Zhang. When is the convergence time of langevin algorithms dimension independent? a composite optimization viewpoint. _Journal of Machine Learning Research_, 23(214):1-32, 2022.
* Roberts and Tweedie (1996) Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions and their discrete approximations. _Bernoulli_, pages 341-363, 1996.
* Mattingly et al. (2002) Jonathan C Mattingly, Andrew M Stuart, and Desmond J Higham. Ergodicity for sdes and approximations: locally lipschitz vector fields and degenerate noise. _Stochastic processes and their applications_, 101(2):185-232, 2002.
* Dalalyan and Riou-Durand (2020) Arnak S Dalalyan and Lionel Riou-Durand. On sampling from a log-concave density using kinetic langevin diffusions. _Bernoulli_, 26(3):1956-1988, 2020.
* Durmus and Moulines (2019) Alain Durmus and Eric Moulines. High-dimensional bayesian inference via the unadjusted langevin algorithm. _Bernoulli_, 25(4A):2854-2882, 2019.
* Gelman and Hill (2006) Andrew Gelman and Jennifer Hill. _Data analysis using regression and multilevel/hierarchical models_. Cambridge university press, 2006.
* Jacot et al. (2018) Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* Audibert (2009) Jean-Yves Audibert. Fast learning rates in statistical inference through aggregation. 2009.
* Mou et al. (2021) Wenlong Mou, Yi-An Ma, Martin J Wainwright, Peter L Bartlett, and Michael I Jordan. High-order langevin diffusion yields an accelerated mcmc algorithm. _The Journal of Machine Learning Research_, 22(1):1919-1959, 2021.
* Lee et al. (2018) Yin Tat Lee, Zhao Song, and Santosh S Vempala. Algorithmic theory of odes and sampling from well-conditioned logconcave densities. _arXiv preprint arXiv:1812.06243_, 2018.
* Sagun et al. (2016) Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond. _arXiv preprint arXiv:1611.07476_, 2016.
* Hsu et al. (2012) Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In _Conference on learning theory_, pages 9-1. JMLR Workshop and Conference Proceedings, 2012.
* Hanzely et al. (2018) Filip Hanzely, Konstantin Mishchenko, Peter Richtarik, and A. Sega: Variance reduction via gradient sketching. _Advances in Neural Information Processing Systems_, 31, 2018.
* Harge (2004) Gilles Harge. A convex/log-concave correlation inequality for gaussian measure and an application to abstract wiener spaces. _Probability theory and related fields_, 130(3):415-440, 2004.
* Basu and DasGupta (1997) Sanjib Basu and Anirban DasGupta. The mean, median, and mode of unimodal distributions: a characterization. _Theory of Probability & Its Applications_, 41(2):210-223, 1997.

## Appendix A Simulation of the discretization process

### Integral form of the discretized process

#### Proof of Lemma 4.1

Proof.: Define the aligned vector \(\mathbf{z}_{n}(s)=(\mathbf{x}_{n}(s),\mathbf{v}_{n}(s))\) and write the diffusion process (4.1) as

\[\mathrm{d}\mathbf{z}_{n}(s)=A\mathbf{z}_{n}(s)\mathrm{d}s+ua(\widetilde{\mathbf{ x}}_{n})\mathrm{d}s+2\sqrt{u}B\mathrm{d}\widetilde{\mathbf{B}}_{s},\] (A.1)

where

\[A=\left[\begin{array}{cc}0&I\\ -umI&-2I\end{array}\right],a=\left[\begin{array}{c}0\\ -\nabla f(\widehat{\mathbf{x}})\end{array}\right],B=\left[\begin{array}{cc}0 &0\\ 0&I\end{array}\right].\]

And \(\widetilde{\mathbf{B}}_{s}\) is a Brownian Motion in \(\mathbb{R}^{2d}\). By a variation of constant method, the solution of (A.1) is

\[\mathbf{z}_{n}(t)=e^{At}\mathbf{z}_{n}(0)+u\left(\int_{0}^{t}e^{-A(s-t)} \mathrm{d}s\right)a+2\sqrt{u}\int_{0}^{t}e^{-A(s-t)}B\mathrm{d}\widetilde{ \mathbf{B}}_{s}\]

To obtain the explicit expression of \(\mathbf{x}_{n}(t)\) and \(\mathbf{v}_{n}(t)\), respectively, it suffices to present a block-wise form

\[e^{At}=\left[\begin{array}{cc}A_{11}(t)I&A_{12}(t)I\\ A_{21}(t)I&A_{22}(t)I\end{array}\right]\]

of the exponential \(e^{At}\). And a direct decomposition yields that

\[\begin{split}& A_{11}(t)=\frac{\sqrt{2^{2}-4um}-2}{2\sqrt{2^{2} ^{2}-4um}}e^{(-2-\sqrt{2^{2}-4um})t/2}+\frac{\sqrt{2^{2}-4um}+2}{2\sqrt{2^{2}- 4um}}e^{(-2+\sqrt{2^{2}-4um})t/2},\\ & A_{12}(t)=-\frac{1}{\sqrt{2^{2}-4um}}e^{(-2-\sqrt{2^{2}-4um})t/2}+ \frac{1}{\sqrt{2^{2}-4um}}e^{(-2+\sqrt{2^{2}-4um})t/2},\\ & A_{21}(t)=\frac{um}{\sqrt{2^{2}-4um}}e^{(-2-\sqrt{2^{2}-4um})t/2}+ \frac{-um}{\sqrt{2^{2}-4um}}e^{(-2+\sqrt{2^{2}-4um})t/2},\\ & A_{22}(t)=\frac{2+\sqrt{2^{2}-4um}}{2\sqrt{2^{2}-4um}}e^{(-2-\sqrt{2^{2}-4um })t/2}+\frac{\sqrt{2^{2}-4um}-2}{2\sqrt{2^{2}-4um}}e^{(-2+\sqrt{2^{2}-4um})t/2}.\end{split}\] (A.2)

Since \(\mathbf{z}_{n}(t)=(\mathbf{x}_{n}(t),\mathbf{v}_{n}(t))\), we obtain the close-form solution of the discretized Langevin process of \(\mathbf{x}_{n}(t)\)

\[\mathbf{x}_{n}(t)=A_{11}(t)\mathbf{x}_{n}+A_{12}(t)\mathbf{v}_{n}+u\int_{0}^{h }A_{12}(s-t)\nabla f(\widehat{\mathbf{x}}_{n})\mathrm{d}s+2\sqrt{u}\int_{0}^{t }A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}.\]

A similar proof can be applied to the exact diffusion process and thus \(x_{n}^{*}(t)\) satisfies

\[\mathbf{x}_{n}^{*}(t)=A_{11}(t)\mathbf{x}_{n}+A_{12}(t)\mathbf{v}_{n}+u\int_{0 }^{t}A_{12}(s-t)\nabla f(\mathbf{x}^{*}(s))\mathrm{d}s+2\sqrt{u}\int_{0}^{t}A _{12}(s-t)\mathrm{d}\mathbf{B}_{s}.\]

Thus we can obtain the result in Lemma 4.1. 

### Controls of the integral forms

**Lemma A.1**.: The functions \(A_{11}(t)\) and \(A_{12}(t)\) in Lemma 4.1 satisfy

\[|A_{12}(t)|\lesssim t,\qquad|A_{11}(t)-1|\lesssim t\]

for \(t\in[0,h]\).

Proof.: In the proof, we will repeatedly use the inequality \(e^{t}-1=\mathcal{O}(t)\). For notational simplicity, denote \(\sqrt{1-um}\) by \(k\). For the first claim,

\[A_{12}(t)=-\frac{1}{2k}e^{-t-kt}+\frac{1}{2k}e^{-t+kt}=e^{-t}\left(\frac{1-e^{- kt}}{2k}+\frac{e^{kt}-1}{2k}\right)=\mathcal{O}(t).\]

As for the second claim, we have

\[A_{12}(t)-1= \frac{k-1}{2k}e^{-t-kt}+\frac{k+1}{2k}e^{-t+kt}-1\] \[= e^{-t}\left(\frac{e^{-kt}+e^{kt}-2}{2}+\frac{1-e^{-kt}}{2k}+\frac {e^{kt}-1}{2k}\right)=\mathcal{O}(kt)+\mathcal{O}(t).\]

Note that \(k=\sqrt{1-um}\leq 1\) and \(h\leq\frac{1}{\kappa}\leq 1\). When \(t\leq h\) our claims hold true. 

### Simulation of the Brownian motion

**Lemma A.2**.: Let \(\mathbf{B}_{t}\) be a standard Brownian Motion. In Algorithm 1, we have \(G=\int_{0}^{t}A_{12}(s-t)\mathrm{dB}_{s}\), \(H=\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{dB}_{s}\) and \(W=\int_{0}^{t}A_{22}(s-t)\mathrm{dB}_{s}\). Then conditioned on the randomness of \(\alpha\) and \(\beta\), \((G,H,W)\) follows a mean-zero Gaussian distribution with covariance

\[\mathbb{E}[(G-\mathbb{E}G)(H-\mathbb{E}H)^{T}] =\left(\int_{0}^{\min(t,\alpha)}A_{12}(s-t)A_{12}(s-\alpha)\mathrm{ d}s\right)I,\] (A.3) \[\mathbb{E}[(G-\mathbb{E}G)(G-\mathbb{E}G)^{T}] =\left(\int_{0}^{t}A_{12}(s-t)^{2}\mathrm{d}s\right)I,\] \[\mathbb{E}[(H-\mathbb{E}H)(H-\mathbb{E}H)^{T}] =\left(\int_{0}^{\alpha}A_{12}(s-\alpha)^{2}\mathrm{d}s\right)I,\] \[\mathbb{E}[(W-\mathbb{E}W)(W-\mathbb{E}W)^{T}] =\left(\int_{0}^{t}A_{22}(s-t)^{2}\mathrm{d}s\right)I,\] \[\mathbb{E}[(W-\mathbb{E}W)(H-\mathbb{E}H)^{T}] =\left(\int_{0}^{\min(\alpha,t)}A_{12}(s-\alpha)A_{22}(s-t) \mathrm{d}s\right)I,\] \[\mathbb{E}[(W-\mathbb{E}W)(G-\mathbb{E}G)^{T}] =\left(\int_{0}^{t}A_{12}(s-t)A_{22}(s-t)\mathrm{d}s\right)I.\]

Proof.: By the properties of Brownian motion, \((G,H,W)\) is a zero-mean Gaussian variable. Its variance is given by

\[\mathbb{E}[(G-\mathbb{E}G)(H-\mathbb{E}H)^{T}]= \mathbb{E}\left[\int_{0}^{t}A_{12}(s-t)\mathrm{dB}_{s}\int_{0}^{ \alpha}A_{12}(s-\alpha)\mathrm{dB}_{s}\right]\] \[= \left(\int_{0}^{\min(t,\alpha)}A_{12}(s-t)A_{12}(s-\alpha) \mathrm{d}s\right)I.\]

The rest of the claims can be proved similarly. 

## Appendix B Proof of the main theorem

In this section, we present the proof of Theorem 4.2.

### Contractive lemma

We begin by defining the n-step exact process \((\mathbf{x}_{n}^{*},\mathbf{v}_{n}^{*})\) and \((\mathbf{x}_{n}^{*}(t),\mathbf{v}_{n}^{*}(t))\). Denote \(\mathrm{d}p^{*}\propto e^{-U(\mathbf{x})-\frac{1}{2\kappa}\|\mathbf{v}\|^{2} }\mathrm{dx}\mathrm{d}\mathbf{v}\). Let \((\mathbf{x}_{0}^{*},\mathbf{v}_{0}^{*})\sim p^{*}\) and

\[\mathrm{d}\mathbf{x}_{n}^{*}(t)=\mathbf{v}_{n}^{*}(t)\mathrm{d}t, \qquad\mathrm{d}\mathbf{v}_{n}^{*}(t)=-uU(\mathbf{x}_{n}^{*}(t))\mathrm{d}t-2 \mathbf{v}_{n}^{*}(t)\mathrm{d}t+\sqrt{2u}\mathrm{dB}_{t},\] (B.1) \[(\mathbf{x}_{n}^{*}(0),\mathbf{v}_{n}^{*}(0))=(\mathbf{x}_{n-1}^{ *}(\beta_{n-1}),\mathbf{x}_{n-1}^{*}(\beta_{n-1})),\]For the simplicity of notation, we denote \((\mathbf{x}_{n}^{*},\mathbf{v}_{n}^{*})=(\mathbf{x}_{n}^{*}(0),\mathbf{v}_{n}^{*}(0 ))\). And we have for any \(n\) and \(t\), \((\mathbf{x}_{n}^{*}(t),\mathbf{v}_{n}^{*}(t))\sim p^{*}\). With slight abuse of notation, let \(\tilde{\mathbf{x}}_{n}(t)\) and \(\mathbf{x}_{n}(t)\) denote inaccurate starting point discretization process and midpoint process in Algorithm 1, respectively. And recall that in Algorithm 1 we have

\[\widehat{\mathbf{x}}_{n}(t)= A_{11}(t)\mathbf{x}_{n}+A_{12}(t)\mathbf{v}_{n}+u\int_{0}^{t}A _{12}(s-t)\nabla f(\mathbf{x}_{n})\mathrm{d}s+2\sqrt{u}\int_{0}^{t}A_{12}(s-t) \mathrm{d}\mathbf{B}_{s}\] (B.2)

and

\[\mathbf{x}_{n}(t)= A_{11}(t)\mathbf{x}_{n}+A_{12}(t)\mathbf{v}_{n}+u\int_{0}^{t}A _{12}(s-t)\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\mathrm{d}s+2\sqrt{u}\int_ {0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}.\] (B.3)

And we assume \(\widehat{\mathbf{x}}_{n}(t)\) is driven by the same Brownian Motion as \(\mathbf{x}_{n}(t)\) and \(\mathbf{x}_{n}^{*}(t)\) at the \(n\)-th step.

Define \(\Omega_{n}(t)=\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)\|^{2}+\|\mathbf{x}_{n} (t)-\mathbf{x}_{n}^{*}(t)+\mathbf{v}_{n}(t)-\mathbf{v}_{n}^{*}(t)\|^{2}\) and \(\Omega_{n}=\|\mathbf{x}_{n}-\mathbf{x}_{n}^{*}\|^{2}+\|\mathbf{x}_{n}-\mathbf{ x}_{n}^{*}+\mathbf{v}_{n}-\mathbf{v}_{n}^{*}\|^{2}\). The proof concentrates on the dynamics of the \(\Omega_{n}\). Note that Wasserstein distance minimizes all the couplings and \(\Omega_{n}(t)\geq\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)\|^{2}\). Hence \(\mathbb{E}\Omega_{n}\) upper bounds of the Wasserstein distance \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}),p)\). Given a synchronously coupled assumption, the dynamics of \(\Omega_{n}(t)\) can be characterized as follow.

#### b.1.1 Proof of Lemma 5.1

Proof.: Let the random process \(\mathbf{B}_{t}^{\alpha}\) given \(\mathbf{B}_{\alpha}\) be

\[\mathrm{d}\mathbf{B}_{t}^{\alpha}=\left\{\begin{array}{cc}\frac{\mathbf{B}_{ \alpha}-\mathbf{B}_{t}^{\alpha}}{\alpha-t}+\mathrm{d}\mathbf{B}_{t}^{\prime},&0 \leq t\leq\alpha\\ \mathrm{d}\mathbf{B}_{t},&t\geq\alpha\end{array}\right.\]

where \(\mathbf{B}_{t}^{\prime}\) is an independent Brownian motion. Note that \(\mathbf{B}_{t}^{\alpha}\) is a Brownian bridge and \(\mathbf{B}_{\alpha}^{\alpha}=\mathbf{B}_{\alpha}\). Then when \(\widehat{\mathbf{x}}_{n}(\alpha)\) is given and \(\mathbf{x}_{n}^{*}(t),\mathbf{x}_{n}(t)\) are coupled synchronously, given \(\mathbf{B}_{\alpha}\), we have

\[\mathrm{d}\mathbf{x}_{n}^{*}(t) =\mathbf{v}_{n}^{*}(t)\mathrm{d}t,\] \[\mathrm{d}\mathbf{v}_{n}^{*}(t) =-u\nabla g(\mathbf{x}_{n}^{*}(t))\mathrm{d}t-u\nabla f(\mathbf{ x}_{n}^{*}(t))\mathrm{d}t-2\mathbf{v}_{n}^{*}(t)\mathrm{d}t+2\sqrt{u}\mathrm{d} \mathbf{B}_{t}^{\alpha},\]

and

\[\mathrm{d}\mathbf{x}_{n}(t) =\mathbf{v}_{n}(t)\mathrm{d}t,\] \[\mathrm{d}\mathbf{v}_{n}(t) =-u\nabla g(\mathbf{x}_{n}(t))\mathrm{d}t-u\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\mathrm{d}t-2\mathbf{v}_{n}(t)\mathrm{d}t+2\sqrt{u} \mathrm{d}\mathbf{B}_{t}^{\alpha}.\]

For simplicity, denote \(\mathbf{z}_{n}(t)=\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)\) and \(\psi_{n}(t)=\mathbf{v}_{n}(t)-\mathbf{v}_{n}^{*}(t)\). Then by Taylor's Theorem

\[\nabla U(\mathbf{x}_{n}(t))-\nabla U(\mathbf{x}_{n}^{*}(t))=\int_{0}^{1}\nabla ^{2}U(s\mathbf{x}_{n}(t)+(1-s)\mathbf{x}_{n}^{*}(t))\mathrm{d}s\mathbf{z}_{n}( t).\] (B.4)

Define \(\mathcal{H}_{t}=\int_{0}^{1}\nabla^{2}U(s\mathbf{x}_{n}(t)+(1-s)\mathbf{x}_{n}^ {*}(t))\mathrm{d}s\), then

\[\mathrm{d}\Omega_{n}(t)/\mathrm{d}t= 2\left\langle\mathbf{z}_{n}(t)+\psi_{n}(t),\psi_{n}(t)-u(g( \mathbf{x}_{n}(t))-g(\mathbf{x}_{n}^{*}(t)))-u\Big{(}\nabla f(\widehat{\mathbf{ x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n}^{*}(t))\Big{)}-\gamma\psi_{n}(t)\right\rangle\] \[+2\Big{\langle}\mathbf{z}_{n}(t),\psi_{n}(t)\Big{\rangle}\] \[= -2\big{(}\underbrace{\langle\mathbf{z}_{n}(t)+\psi_{n}(t),(\gamma -1)\psi_{n}(t)+u\mathcal{H}_{t}\mathbf{z}_{n}(t)\rangle-\langle\mathbf{z}_{n}( t),\psi_{n}(t)\rangle}_{A}\big{)}\] \[+2u\langle\mathbf{z}_{n}(t)+\psi_{n}(t),\nabla f(\mathbf{x}_{n}(t) )-\nabla f(\widehat{\mathbf{x}}_{n}(\alpha)))\rangle,\]

where in the second equality we use (B.4) and divide \(\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\) into \(\nabla f(\mathbf{x}_{n}(t))\) and the difference \(\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n}(t))\). Note that \(A\) can be converted to a quadratic form

\[A= [\mathbf{z}_{n}(t)^{T}+\psi_{n}(t)^{T},\mathbf{z}_{n}(t)^{T}]\left[ \begin{array}{cc}I&\frac{1}{2}u\mathcal{H}_{t}-I\\ \frac{1}{2}u\mathcal{H}_{t}-I&I\end{array}\right]\left[\begin{array}{cc} \mathbf{z}_{n}(t)+\psi_{n}(t)\\ \mathbf{z}_{n}(t)\end{array}\right]\]whose eigenvalues \(\lambda_{i}\) are given by characteristic function \((1-\lambda)^{2}-\frac{1}{4}(u\lambda_{i}\big{(}\mathcal{H}_{t})+um-2)^{2}=0\). Given the strong-convexity and Lipschitz smoothness of the potential \((x)\), when \(u\) is set to be \(\frac{1}{L+m}\) the eigenvalue of \(A\) is greater than \(\frac{1}{2\kappa}\). Therefore,

\[\mathrm{d}\Omega_{n}(t)/\mathrm{d}t\leq-\frac{1}{\kappa}\Omega_{n}(t)+2u\langle \mathbf{z}_{n}(t)+\psi_{n}(t),\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha)))\rangle.\]

Then by multiplying \(e^{\frac{t}{\kappa}}\) and taking the integral form \(0\) to \(\beta\), we obtain

\[\Omega_{n}(\beta)\leq e^{-\frac{\beta}{\kappa}}\Omega_{n}(0)+2u\int_{0}^{\beta }e^{\frac{s-\beta}{\kappa}}\langle\mathbf{x}_{n}(s)-\mathbf{x}_{n}^{*}(s)+ \mathbf{v}_{n}(s)-\mathbf{v}_{n}^{*}(s),\nabla f(\mathbf{x}_{n}(s))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha))\rangle\mathrm{d}s.\]

In Algorithm 1 the random step size \(\beta\sim\rho\) and thus we reach that

\[\mathbb{E}\Omega_{n+1}\leq\mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}} \mathbb{E}\Omega_{n}+2u\mathbb{E}\mathbb{E}_{\beta\sim\rho}\int_{0}^{\beta}e^{ \frac{s-\beta}{\kappa}}\langle\mathbf{x}_{n}(s)-\mathbf{x}_{n}^{*}(s)+\mathbf{ v}_{n}(s)-\mathbf{v}_{n}^{*}(s),\nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\rangle\mathrm{d}s.\]

### Proof of Theorem 4.2

We devote the rest of this section to the proof of the main theorem. Lemma 5.1 shows that the dynamics of the \(\mathbb{E}\Omega_{n}\) is driven by a shrinkage term and a local discretization error. To achieve the claimed convergence rate, the proof aims to establish an upper bound of local error which is: (1) dimensional-independent, (2) optimal in the sense of max step size \(h\).

Proof.: Given that \(\mathbb{E}\Omega_{n}\) is an upper bound of the squared \(2\)-Wasserstein distance, it suffices to prove \(\mathbb{E}\Omega_{n}\leq e^{2}\). By Lemma 5.1 and Lemma 5.2, we have

\[\mathbb{E}\Omega_{n}(\beta)\leq \mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}}\mathbb{E} \Omega_{n}\] \[+2u\mathbb{E}\mathbb{E}_{\beta\sim\rho}\int_{0}^{\beta}e^{\frac{ s-\beta}{\kappa}}\langle\mathbf{x}_{n}(s)-\mathbf{x}_{n}^{*}(s)+\mathbf{v}_{n}(s)- \mathbf{v}_{n}^{*}(s),\nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{\mathbf{x }}_{n}(\alpha))\rangle\mathrm{d}s\] \[= \mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}}\mathbb{E} \Omega_{n}\] \[+\underbrace{2uhC_{1}\mathbb{E}\mathbb{E}_{\beta\sim\rho^{\prime}} \langle\mathbf{x}_{n}(\beta)-\mathbf{x}_{n}^{*}(\beta)+\mathbf{v}_{n}(\beta)- \mathbf{v}_{n}^{*}(\beta),\nabla f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\rangle}_{A}.\] (B.5)

Next, we bound the error term \(A\) by

\[A= 2C_{1}uh\mathbb{E}\mathbb{E}_{\beta\sim\rho^{\prime}}\langle \mathbf{x}_{n}(\beta)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(\beta)+\mathbf{x}_{n} ^{*}+\mathbf{v}_{n}(\beta)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(\beta)+\mathbf{v} _{n}^{*},\nabla f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))\rangle\] \[+2C_{1}uh\mathbb{E}\langle\mathbf{x}_{n}-\mathbf{x}_{n}^{*}+ \mathbf{v}_{n}-\mathbf{v}_{n}^{*},\mathbb{E}_{\alpha\sim\rho^{\prime}}\mathbb{E }_{\beta\sim\rho^{\prime}}\left(\nabla f(\mathbf{x}_{n}(\beta))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha)))\right)\] \[\lesssim \frac{uh}{uh}\mathbb{E}\|\mathbf{x}_{n}(\beta)-\mathbf{x}_{n}- \mathbf{x}_{n}^{*}(\beta)+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(\beta)-\mathbf{v}_ {n}-\mathbf{v}_{n}^{*}(\beta)+\mathbf{v}_{n}^{*}\|^{2}\] \[+u^{2}h^{2}\mathbb{E}\|\nabla f(\mathbf{x}_{n}(\beta))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha))\|^{2}+\frac{h}{u}uh\mathbb{E}\|\mathbf{x}_{n }-\mathbf{x}_{n}^{*}+\mathbf{v}_{n}-\mathbf{v}_{n}^{*}\|^{2}\] \[+\frac{u}{uh}uh\mathbb{E}\|\mathbb{E}_{\alpha\sim\rho^{\prime}} \mathbb{E}_{\beta\sim\rho^{\prime}}(\nabla f(\mathbf{x}_{n}(\beta))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha)))\|^{2}\] \[\overset{a}{\lesssim} \left(h^{2}\mathbb{E}_{\beta\sim\rho}\mathbb{E}\Omega_{n}(\beta)+ h^{4}\mathbb{E}\Omega_{n}+\frac{u^{2}h^{4}}{m}\mathrm{tr}(H)+h^{4}\|\mathbf{x}_{*}\|^{2}\right)\] \[+u^{2}h^{2}\left(h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}L}{m} \mathrm{tr}(H)+L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}\right)+h^{2}\mathbb{E}\Omega _{n}\] \[+u^{2}\left(h^{4}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{4}L}{m} \mathrm{tr}(H)+h^{4}L^{2}\|\mathbf{x}_{*}\|^{2}\right)\] \[\overset{b}{\lesssim} h^{2}\mathbb{E}\mathbb{E}_{\beta\sim\rho}\Omega_{n}(\beta)+h^{2}\mathbb{E} \Omega_{n}+\frac{u^{2}h^{4}L}{m}\mathrm{tr}(H)+h^{4}\|\mathbf{x}_{*}\|^{2},\]where in inequality \(\stackrel{{ a}}{{\lesssim}}\), we apply Lemma D.1, Lemma 5.3 and Lemma D.5; inequality \(\stackrel{{ b}}{{\lesssim}}\) follows by the observation \(uL=\frac{L}{L+m}\leq 1\). Then by plugging the upper bound of \(A\) into (B.5), there exists constant \(C_{2}\) such that

\[\mathbb{E}\Omega_{n}(\beta)\leq \mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}}\mathbb{E} \Omega_{n}+C_{2}\left(h^{2}\mathbb{E}\Omega_{n}(\beta)+h^{2}\mathbb{E}\Omega_ {n}+\frac{u^{2}h^{4}L}{m}\mathrm{tr}(H)+h^{4}\|\mathbf{x}_{*}\|^{2}\right).\] (B.6)

(B.6) can be simplified as

\[\mathbb{E}\Omega_{n}(\beta)\leq\frac{\mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta }{\kappa}}+C_{2}h^{2}}{1-C_{2}h^{2}}\mathbb{E}\Omega_{n}+\frac{C_{2}h^{4}}{1- C_{2}h^{2}}\left(\frac{u^{2}L}{m}\mathrm{tr}(H)+\|\mathbf{x}_{*}\|^{2}\right).\] (B.7)

Note that when \(\beta\sim\rho\), \(\mathbb{E}\Omega_{n}(\beta)=\mathbb{E}\Omega_{n+1}\). And \(\mathbb{E}\Omega_{n}\) is an upper bound of Wasserstein distance. Equation (B.7) characterizes the one-step discretization of the Langevin process, where the first term indicates a linear convergence if without discretization error, and the second term is the one-step discretization error.

Without loss of generality, let \(C_{2}\geq 1\). Since \(h\leq 1\leq\kappa\), we have \(\mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}}=\frac{\kappa}{h}\left(1- e^{\frac{h}{\kappa}}\right)\leq 1-\frac{h}{3\kappa}\). Since \(h\leq\frac{1}{12C_{2}\kappa}\), we have \(h\leq\frac{1}{2C_{2}\kappa}\) and \(h\leq\frac{1}{\sqrt{C_{2}}}\) and therefore \(1-C_{2}h^{2}\geq 0\), \(2C_{2}h^{2}\leq\frac{h}{6\kappa}\). Hence the first term of (B.7) can be bounded by

\[\begin{split}\frac{\mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{ \kappa}}+C_{2}h^{2}}{1-C_{2}h^{2}}\mathbb{E}\Omega_{n}\leq&\frac {1-\frac{h}{3\kappa}+C_{2}h^{2}}{1-C_{2}h^{2}}\mathbb{E}\Omega_{n}\\ =&\left(\frac{-\frac{h}{3\kappa}+2C_{2}h^{2}}{1-C_{ 2}h^{2}}+1\right)\mathbb{E}\Omega_{n}\\ \leq&\left(\frac{-\frac{h}{3\kappa}+\frac{h}{6\kappa }}{1-C_{2}h^{2}}+1\right)\mathbb{E}\Omega_{n}\\ \leq&\left(1-\frac{h}{6\kappa}\right)\mathbb{E}\Omega _{n},\end{split}\] (B.8)

As for error term in (B.7), since \(h\leq\frac{1}{12C_{2}\kappa}\leq\frac{1}{\sqrt{2C_{2}}}\), we have \(\frac{C_{2}h^{4}}{1-C_{2}h^{2}}\leq 2C_{2}h^{4}\). Denote \(r=1-\frac{h}{6\kappa}\) and \(\mathcal{E}=2C_{2}h^{4}\left(\frac{u^{2}L}{m}\mathrm{tr}(H)+\|\mathbf{x}_{*} \|^{2}\right)\) and therefore

\[\mathbb{E}\Omega_{N}\leq r\mathbb{E}\Omega_{N-1}+\mathcal{E}\leq r^{N}\mathbb{ E}\Omega_{0}+\mathcal{E}(1+r+\cdots+r^{N-1})\leq r^{N}\mathbb{E}\Omega_{0}+ \frac{\mathcal{E}}{1-r}.\] (B.9)

When \(N\geq\frac{6}{\kappa}\log\left(\frac{2\mathbb{E}\Omega_{0}}{\epsilon^{2}}\right)\), we have

\[r^{N}\mathbb{E}\Omega_{0}\leq e^{\frac{Nh}{6\kappa}}\mathbb{E}\Omega_{0}\leq \frac{\epsilon^{2}}{2}.\] (B.10)

And when \(h\leq\frac{\epsilon^{2/3}}{\left(24C_{2}\kappa(\frac{u^{2}L}{m}\mathrm{tr}(H)+ \|\mathbf{x}_{*}\|^{2})\right)^{1/3}}\), we get

\[\frac{\mathcal{E}}{1-r}\leq\frac{\epsilon^{2}}{2}.\] (B.11)

Plugging (B.10) and (B.11) into (B.9) yields

\[\mathbb{E}\Omega_{N}\leq\epsilon^{2},\]

and therefore completes the proof. 

## Appendix C Supporting lemmas

We assume that Assumption 3.1 holds in this section and Section D.

[MISSING_PAGE_EMPTY:18]

### Convergence lemma

**Lemma C.2**.: Let \(\mathbf{x}_{*}\) be the minimizer of the potential function, that is, \(\mathbf{x}_{*}=\arg\min\{g(\mathbf{x})+f(\mathbf{x})\}\), and \(A\) is a positive definite matrix that \(mI\preceq A\preceq LI\). Let \(\mathbf{x}_{n}(t)\) and \(\mathbf{x}_{n}^{*}(t)\) be defined as (B.3) and (B.1). Assume that \(u=\frac{1}{L+m}\). Then we have the following bound

\[\mathbb{E}\|A\mathbf{x}_{n}(t)\|^{2} \lesssim L^{2}\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)\|^{2}+ \frac{1}{m}\mathrm{tr}(A^{2})+L^{2}\|\mathbf{x}_{*}\|^{2},\] \[\mathbb{E}\|A(\mathbf{x}_{n}(t)+\mathbf{v}_{n}(t))\|^{2} \lesssim L^{2}\mathbb{E}\|\mathbf{x}_{n}(t)+\mathbf{v}_{n}(t)- \mathbf{x}_{n}^{*}(t)-\mathbf{v}_{n}^{*}(t)\|^{2}+\frac{1}{m}\mathrm{tr}(A^{2} )+L^{2}\|\mathbf{x}_{*}\|^{2},\]

and

\[\mathbb{E}\|A\nabla f(\mathbf{x}_{n}(t))\|^{2}\lesssim L^{4}\mathbb{E}\| \mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)\|^{2}+\frac{L^{2}}{m}\mathrm{tr}(A^{2 })+L^{2}m^{2}\|\mathbf{x}_{*}\|^{2}.\]

Proof.: We begin with the first bound. By Young's inequality and Lipschitz smoothness of \(f\),

\[\begin{split}\mathbb{E}\|A\mathbf{x}_{n}(t)\|^{2}\lesssim& \mathbb{E}\|A(\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t))\|^{2}+\mathbb{E}\|A( \mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})\|^{2}+\|A\mathbf{x}_{*}\|^{2}\\ \leq& L^{2}\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{ n}^{*}(t)\|^{2}+\mathbb{E}\|A(\mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})\|^{2}+L^{2} \|\mathbf{x}_{*}\|^{2}.\end{split}\] (C.4)

Since \(U(\cdot)\) is m strongly convex and \(\|H\mathbf{x}\|\) is a convex function with respect to \(\mathbf{x}\), Theorem 1.1 of Harge (2004) implies that

\[\mathbb{E}\|A\left(\mathbf{x}_{n}^{*}(t)-\mathbb{E}\mathbf{x}_{n}^{*}\right)\| ^{2}\leq\mathbb{E}_{\mathbf{x}\sim\mathcal{N}\left(0,\frac{1}{m}\right)}\|A \mathbf{x}\|^{2}=\frac{1}{m}\mathrm{tr}(A^{2}).\] (C.5)

On the other hand, by Theorem 7 of Basu and DasGupta (1997),

\[(\mathbb{E}\mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})^{T}\Sigma^{-1}(\mathbb{E} \mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})\leq 3,\]

where \(\Sigma\) is the covariance of \(p\) and therefore bounded by \(\frac{1}{m}\). Hence

\[\|A(\mathbb{E}\mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})\|^{2}\leq\frac{\|A\|^{2}}{ m}\leq\frac{1}{m}\mathrm{tr}(A^{2}).\] (C.6)

Combining equation (C.5) and (C.6), we obtain that

\[\mathbb{E}\|A(\mathbf{x}_{n}^{*}-\mathbf{x}_{*})\|^{2}\lesssim\mathbb{E}\|A( \mathbf{x}_{n}^{*}-\mathbb{E}\mathbf{x}_{n}^{*})\|^{2}+\|A(\mathbb{E}\mathbf{ x}_{n}^{*}-\mathbf{x}_{*})\|^{2}\lesssim\frac{1}{m}\mathrm{tr}(A^{2}).\] (C.7)

Combining (C.4) and (C.7) and then we achieve the first claim.

To bound \(\mathbb{E}\|A(\mathbf{x}_{n}(t)+\mathbf{v}_{n}(t))\|^{2}\), we have

\[\begin{split}\mathbb{E}\|A(\mathbf{x}_{n}(t)+\mathbf{v}_{n}(t)) \|^{2}\lesssim&\mathbb{E}\|A(\mathbf{x}_{n}(t)+\mathbf{v}_{n}(t)- \mathbf{x}_{n}^{*}(t)-\mathbf{v}_{n}^{*}(t))\|^{2}\\ &+\mathbb{E}\|A(\mathbf{x}_{n}^{*}(t)+\mathbf{v}_{n}^{*}(t)- \mathbf{x}_{*})\|^{2}+\|A\mathbf{x}_{*}\|^{2}\\ \lesssim& L^{2}\mathbb{E}\|\mathbf{x}_{n}(t)+\mathbf{v} _{n}(t)-\mathbf{x}_{n}^{*}(t)-\mathbf{v}_{n}^{*}(t)\|^{2}+\mathbb{E}\|A( \mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})\|^{2}\\ &+\mathbb{E}\|A\mathbf{v}_{n}^{*}(t)\|^{2}+L^{2}\|\mathbf{x}_{*}\| ^{2}\\ \lesssim& L^{2}\mathbb{E}\|\mathbf{x}_{n}(t)+\mathbf{v} _{n}(t)-\mathbf{x}_{n}^{*}(t)-\mathbf{v}_{n}^{*}(t)\|^{2}+\frac{1}{m}\mathrm{tr} (A^{2})+\frac{1}{m}\mathrm{tr}(A^{2})+L^{2}\|\mathbf{x}_{*}^{2}\|\\ \lesssim& L^{2}\mathbb{E}\|\mathbf{x}_{n}(t)+\mathbf{v} _{n}(t)-\mathbf{x}_{n}^{*}(t)-\mathbf{v}_{n}^{*}(t)\|^{2}+\frac{1}{m}\mathrm{ tr}(A^{2})+L^{2}\|\mathbf{x}_{*}\|^{2},\end{split}\]

where the second inequality follows by the Lipschitz smoothness of \(f(\mathbf{x})\) and Young's inequality; in the third inequality we use (C.7) and \(v\sim\mathcal{N}(0,\frac{1}{m})\).

To prove the third claim, we have

\[\begin{split}\mathbb{E}\|A\nabla f(\mathbf{x}_{n}(t))\|^{2}& \lesssim&\mathbb{E}\|A\big{(}\nabla f(\mathbf{x}_{n}(t))-\nabla f (\mathbf{x}_{n}^{*}(t))\big{)}\|^{2}+\mathbb{E}\|A\big{(}\nabla f(\mathbf{x}_{n }^{*}(t))-\nabla f(\mathbf{x}_{*})\big{)}\|^{2}+\|A\nabla f(\mathbf{x}_{*})\|^{2} \\ \lesssim& L^{4}\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{ n}^{*}(t)\|^{2}+\mathbb{E}L^{2}\|A(\mathbf{x}_{n}^{*}(t)-\mathbf{x}_{*})\|^{2}+L^{2}\| \nabla f(\mathbf{x}_{*})\|^{2}\\ \lesssim& L^{4}\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{ n}^{*}(t)\|^{2}+\frac{L^{2}}{m}\mathrm{tr}(A^{2})+L^{2}m^{2}\|\mathbf{x}_{*}\|^{2},\end{split}\]

where in last inequality, we apply (C.7) and \(\nabla f(\mathbf{x}_{*})+m\mathbf{x}_{*}=0\)Technical bounds

In this section, we present some controls that are useful in the proof of Theorem 4.2.

Bound of \(\mathbb{E}\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x}}_{n}(\alpha)) \|^{2}\)

**Lemma D.1**.: Let \(\mathbf{x}_{n}(t),\widehat{\mathbf{x}}_{n}(t)\) be defined as above and \(u=\frac{1}{L+m}\). For any fixed \(t,\alpha\leq h\).

1. When Assumption 3.1 hold and \(H\) is a uniform upper bound of \(\nabla^{2}f\), i.e. \(\nabla^{2}f(\mathbf{x})\preceq H\) for all \(\mathbf{x}\in\mathbb{R}^{d}\), \(\mathbf{x}_{n}^{*}(t)\)and \(\widehat{\mathbf{x}}_{n}(\alpha)\) satisfies \[\mathbb{E}\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))\|^{2}\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}}{m}\mathrm{ tr}(H^{2})+L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}.\]
2. When Assumptions 3.1, E.1 and E.2 hold, we have \[\mathbb{E}\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))\|^{2}\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}L}{m}M+L^{2} h^{2}\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{2}h^{6}d^{2}.\]

Proof.: For any \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\), we have

\[\|\nabla f(\mathbf{x})-\nabla f(\mathbf{y})\|^{2}= \left\|\int_{0}^{1}\nabla^{2}f((1-k)\mathbf{x}+k\mathbf{y})(\mathbf{x}- \mathbf{y})\mathrm{d}k\right\|^{2}.\] (D.1)

Denote \(\bar{\mathcal{H}}=\int_{0}^{1}\nabla^{2}f(\mathbf{x}+k(\mathbf{y}-\mathbf{x}) )\mathrm{d}k\) and \(\bar{\mathcal{H}}\) depends on \(\mathbf{x}\) and \(\mathbf{y}\). Substitute \(\mathbf{x},\mathbf{y}\) by \(\mathbf{x}_{n}(t),\widehat{\mathbf{x}}_{n}(\alpha)\), and then we obtain \(\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\|^{2} =\|\bar{\mathcal{H}}(\mathbf{x}_{n}(t)-\widehat{\mathbf{x}}_{n}(\alpha))\|^{2}\). Then by plugging in the close-form solution into \(\|\bar{\mathcal{H}}(\mathbf{x}_{n}(t)-\widehat{\mathbf{x}}_{n}(\alpha))\|^{2}\),

\[\|\bar{\mathcal{H}}(\mathbf{x}_{n}(t)-\widehat{\mathbf{x}}_{n}( \alpha))\|^{2}\] \[\overset{a}{=} \Big{\|}\bar{\mathcal{H}}\big{(}\big{(}A_{11}(t)-A_{11}(\alpha)-A _{12}(t)+A_{12}(\alpha)\big{)}\mathbf{x}_{n}+(A_{12}(t)-A_{12}(\alpha))( \mathbf{x}_{n}+\mathbf{v}_{n})+\] \[u\int_{0}^{t}A_{12}(s-t)\nabla f(\widehat{\mathbf{x}}_{n}(\alpha ))\mathrm{d}s-u\int_{0}^{\alpha}A_{12}(s-\alpha)\nabla f(\mathbf{x}_{n}) \mathrm{d}s+2\sqrt{u}\int_{0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}\] \[-2\sqrt{u}\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B}_{ s}\big{)}\Big{\|}^{2}\] \[\overset{b}{\lesssim} (A_{11}(t)-A_{11}(\alpha)-A_{12}(t)+A_{12}(\alpha))^{2}\|\bar{ \mathcal{H}}\mathbf{x}_{n}\|^{2}+(A_{12}(t)-A_{12}(\alpha))^{2}\|\bar{\mathcal{ H}}(\mathbf{x}_{n}+\mathbf{v}_{n})\|^{2}\] \[+u\underbrace{\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}} \nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\mathrm{d}s-\int_{0}^{\alpha}A_{12}( s-\alpha)\bar{\mathcal{H}}\nabla f(\mathbf{x}_{n})\mathrm{d}s\right\|^{2}}_{ \mathfrak{L}},\] (D.2)

where in \(\overset{a}{=}\) we use Lemma 4.1; \(\overset{b}{\lesssim}\) follows by the Jensen's inequality. Besides Lemma A.1 implies that \((A_{11}(t)-A_{11}(\alpha)-A_{12}(t)+A_{12}(\alpha))^{2}\|H\mathbf{x}_{n}\|^{2} \lesssim h^{2}\|H\mathbf{x}_{n}\|^{2}\) and \((A_{12}(t)-A_{12}(\alpha))^{2}\|H(\mathbf{x}_{n}+\mathbf{v}_{n})\|^{2}\lesssim h ^{2}\|H(\mathbf{x}_{n}+\mathbf{v}_{n})\|^{2}\).

For the expectation of term \(\Uparrow\), we have

\[\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}}\nabla f( \widehat{\mathbf{x}}_{n}(\alpha))\mathrm{d}s-\int_{0}^{\alpha}A_{12}(s-\alpha) \bar{\mathcal{H}}\nabla f(\mathbf{x}_{n})\mathrm{d}s\right\|^{2}\] \[\lesssim \mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}}\big{(} \nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n})\big{)} \mathrm{d}s\right\|^{2}+\] \[\mathbb{E}\left\|\int_{\alpha}^{t}A_{12}(s-\alpha)\bar{\mathcal{H} }\nabla f(\mathbf{x}_{n})\mathrm{d}s\right\|^{2}+\mathbb{E}\left\|\int_{0}^{t} (A_{12}(s-t)-A_{12}(s-\alpha))\bar{\mathcal{H}}\nabla f(\mathbf{x}_{n}) \mathrm{d}s\right\|^{2}\] (D.3) \[\leq \mathbb{E}t\int_{0}^{t}A_{12}(s-t)^{2}\mathrm{d}s\|\bar{ \mathcal{H}}\big{(}\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f( \mathbf{x}_{n})\big{)}\|^{2}\] \[+\mathbb{E}\left|(\alpha-t)\int_{\alpha}^{t}A_{12}(s-\alpha)^{2} \mathrm{d}s\right\|\|\bar{\mathcal{H}}\nabla f(\mathbf{x}_{n})\|^{2}\] \[+\mathbb{E}t\int_{0}^{t}(A_{12}(s-t)-A_{12}(s-\alpha))^{2}\mathrm{ d}s\|\bar{\mathcal{H}}\nabla f(\mathbf{x}_{n})\|^{2}\] \[\lesssim h^{4}\mathbb{E}\|\bar{\mathcal{H}}\left(\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n})\right)\|^{2}+h^{4}\mathbb{E} \|\bar{\mathcal{H}}\nabla f(\mathbf{x}_{n})\|^{2},\]

where in the last inequality we use that \(t,\alpha<h\) and \(A_{12}(t)=\mathcal{O}(t)\). Let \(\bar{\mathcal{H}}^{\prime}=\int_{0}^{1}\nabla^{2}f(\widehat{\mathbf{x}}_{n}( \alpha)+k(\widehat{\mathbf{x}}_{n}(\alpha)-\mathbf{x}_{n}))\mathrm{d}k\). By the definition of \(\widehat{\mathbf{x}}_{n}(\alpha)\), \(\mathbb{E}\|\bar{\mathcal{H}}\left(\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))- \nabla f(\mathbf{x}_{n})\right)\|^{2}\) can be upper bounded by

\[\mathbb{E}\|\bar{\mathcal{H}}\left(\nabla f(\widehat{\mathbf{x}} _{n}(\alpha))-\nabla f(\mathbf{x}_{n})\right)\|^{2}\] \[\leq L^{2}\mathbb{E}\|\left(\nabla f(\widehat{\mathbf{x}}_{n}(\alpha ))-\nabla f(\mathbf{x}_{n})\right)\|^{2}\] \[= L^{2}\mathbb{E}\left\|\bar{\mathcal{H}}^{\prime}\big{(}(A_{11}( \alpha)-1)\mathbf{x}_{n}+A_{12}(\alpha)\mathbf{v}_{n}+u\int_{0}^{\alpha}A_{12} (t-\alpha)\nabla f(\mathbf{x}_{n})\mathrm{d}t+2\sqrt{u}\int_{0}^{\alpha}A_{12} (t-\alpha)\mathrm{d}\mathbf{B}_{t}\big{)}\right\|^{2}\] \[\lesssim L^{2}\mathbb{E}A_{12}(\alpha)^{2}\|\bar{\mathcal{H}}^{\prime}( \mathbf{x}_{n}+\mathbf{v}_{n})\|^{2}+\mathbb{E}(A_{11}(x)-A_{12}(x)-1)^{2}L^{2 }\|\bar{\mathcal{H}}^{\prime}\mathbf{x}_{n}\|^{2}\] \[+u^{2}\mathbb{E}\alpha\int_{0}^{\alpha}L^{2}\|A_{12}(t-\alpha) \bar{\mathcal{H}}^{\prime}\nabla f(\mathbf{x}_{n})\|^{2}\mathrm{d}t+u\mathbb{ E}L^{2}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{\mathcal{H}}^{\prime} \mathrm{d}\mathbf{B}_{t}\right\|^{2}\] \[\lesssim h^{2}L^{2}\mathbb{E}\|\bar{\mathcal{H}}^{\prime}(\mathbf{x}_{n}+ \mathbf{v}_{n})\|^{2}+h^{2}L^{2}\mathbb{E}\|\bar{\mathcal{H}}^{\prime}\mathbf{ x}_{n}\|^{2}+u^{2}h^{4}L^{2}\mathbb{E}\|\bar{\mathcal{H}}^{\prime}\nabla f( \mathbf{x}_{n})\|^{2}\] \[+uL^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{ \mathcal{H}}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2},\]

where in the last inequality we apply Lemma A.1. Note that \(\mathbf{x}_{n}(0)=\mathbf{x}_{n}\) and \(\mathbf{v}_{n}(0)=\mathbf{v}_{n}\). Then by Lemma C.2,

\[\mathbb{E}\|\bar{\mathcal{H}}(\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))-\nabla f(\mathbf{x}_{n}))\|^{2}\] \[\leq L^{2}\mathbb{E}\|\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f (\mathbf{x}_{n})\|^{2}\] \[\lesssim h^{2}(L^{4}\mathbb{E}\|\mathbf{x}_{n}+\mathbf{v}_{n}-\mathbf{x}_ {n}^{*}-\mathbf{v}_{n}^{*}\|^{2}+\frac{L^{2}}{m}\mathbb{E}\mathrm{tr}(\bar{ \mathcal{H}}^{\prime 2})+L^{4}\|\mathbf{x}_{*}\|^{2})+h^{2}(L^{4}\mathbb{E}\|\mathbf{x}_{n}- \mathbf{x}_{n}^{*}\|^{2}\] \[+\frac{L^{2}}{m}\mathbb{E}\mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})+L^{2}\| \mathbf{x}_{*}\|^{2})+u^{2}h^{2}(L^{4}\mathbb{E}\|\mathbf{x}_{n}-\mathbf{x}_{n}^ \|^{2}+\frac{L^{2}}{m}\mathbb{E}\mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})+L^{4}m^{2}\|\mathbf{x}_{*}\|^{2})\] (D.4) \[+uL^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{ \mathcal{H}}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\] \[\lesssim h^{2}L^{4}\mathbb{E}\Omega_{n}+\frac{h^{2}L^{2}}{m}\mathbb{E} \mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})+h^{2}L^{4}\|\mathbf{x}_{*}\|^{2}+uL^{2}\mathbb{E}\left\| \int_{0}^{\alpha}A_{12}(t-\alpha)\bar{\mathcal{H}}^{\prime}\mathrm{d}\mathbf{B}_{t }\right\|^{2},\]

where in the last inequality we use \(u=\frac{1}{L+m}\leq\min\left\{\frac{1}{L},\frac{1}{m}\right\}\). Plugging (D) into (D) and then we have an upper bound of the expectation of term \(\Uparrow\). Plugging the upper bound of term \(\Uparrow\) into (D),and thus we have the following inequality

\[\mathbb{E}\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x} }_{n}(\alpha))\|^{2}\] (D.5) \[\lesssim h^{2}\mathbb{E}\|\bar{\mathcal{H}}\mathbf{x}_{n}\|^{2}+h^{2} \mathbb{E}\|\bar{\mathcal{H}}(\mathbf{x}_{n}+\mathbf{v}_{n})\|^{2}+u^{2}h^{4} \mathbb{E}\|\bar{\mathcal{H}}\nabla f(\mathbf{x}_{n})\|^{2}\] \[+u^{2}h^{4}\left(h^{2}L^{4}\mathbb{E}\Omega_{n}+\frac{h^{2}L^{2}} {m}\mathbb{E}\mathrm{tr}\left(\left(\bar{\mathcal{H}}^{\prime}\right)^{2} \right)+h^{2}L^{4}\|\mathbf{x}_{*}\|^{2}+u\mathbb{E}L^{2}\left\|\int_{0}^{ \alpha}A_{12}(t-\alpha)\bar{\mathcal{H}}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\| ^{2}\right)\] \[+u\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}} \mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\bar{\mathcal{H}} \mathrm{d}\mathbf{B}_{s}\right\|^{2}\] \[\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}}{m}\mathbb{E}\left( \mathrm{tr}\left(\left(\bar{\mathcal{H}}^{\prime}\right)^{2}\right)+\mathrm{tr }\left(\bar{\mathcal{H}}^{2}\right)\right)+L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}+u^ {3}h^{4}L^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{\mathcal{H }}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\] \[+u\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}} \mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\bar{\mathcal{H}} \mathrm{d}\mathbf{B}_{s}\right\|^{2},\]

where in the last inequality we use Lemma C.2 and the definition of \(\Omega_{n}\).

When we assume \(\nabla^{2}f(\mathbf{x})\) has a uniform upper bound \(H\), we have

\[u\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}} \mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\bar{\mathcal{H}} \mathrm{d}\mathbf{B}_{s}\right)\right\|^{2}\] (D.6) \[\lesssim u\mathbb{E}\left\|\int_{0}^{h}hH\mathrm{d}\mathbf{B}_{s}\right\| ^{2}\lesssim\frac{h^{3}}{m}\mathrm{tr}(H^{2})\leq\frac{h^{3}L}{m}\mathrm{tr}(H)\]

and

\[u^{3}h^{4}L^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{\mathcal{ H}}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\leq u^{3}h^{4}L^{2}h^{3} \mathrm{tr}(H^{2})\lesssim\frac{h^{4}}{m}\mathrm{tr}(H^{2})\leq\frac{h^{4}L} {m}\mathrm{tr}(H).\]

Besides we have \(\mathrm{tr}\left(\left(\bar{\mathcal{H}}^{\prime}\right)^{2}\right)+\mathrm{ tr}\left(\bar{\mathcal{H}}^{2}\right)\leq L\mathrm{tr}\left(\bar{\mathcal{H}}^{ \prime}\right)+L\mathrm{tr}\left(\bar{\mathcal{H}}\right)\lesssim L\mathrm{ tr}(H)\). These bounds yield the control

\[\mathbb{E}\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))\|^{2}\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}L}{m}\mathrm{tr }(H)+L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}.\] (D.7)

And when \(f\) is \(L_{2}\)-Hessian smooth, by Lemmas D.2 and D.3, we have

\[\mathbb{E}\|\nabla f(\mathbf{x}_{n}(t))-\nabla f(\widehat{\mathbf{x} }_{n}(\alpha))\|^{2}\] (D.8) \[\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}}{m}\mathbb{E}\left( \mathrm{tr}\left(\left(\bar{\mathcal{H}}^{\prime}\right)^{2}\right)+\mathrm{ tr}\left(\bar{\mathcal{H}}^{2}\right)\right)+uh^{3}LM+L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}\] \[+L_{2}^{2}u^{2}h^{6}d^{2}+L_{2}^{2}u^{4}L^{2}h^{10}d^{2}\] \[\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+L\frac{h^{2}}{m}\mathbb{E}\left( \mathrm{tr}\left(\bar{\mathcal{H}}^{\prime}\right)+\mathrm{tr}\left(\bar{ \mathcal{H}}\right)\right)+\frac{h^{2}L}{m}M+L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}+ L_{2}^{2}u^{2}h^{6}d^{2}\] \[\lesssim h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{Lh^{2}M}{m}+L^{2}h^{2}\| \mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{2}h^{6}d^{2}.\]

**Lemma D.2**.: Let \(\bar{\mathcal{H}}\) be defined as above. When Assumptions E.1 and E.2 hold, we have

\[\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}}\mathrm{d}\mathbf{B}_{ s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\bar{\mathcal{H}}\mathrm{d}\mathbf{B}_{s} \right)\right\|^{2}\lesssim h^{3}LM+L_{2}^{2}uh^{6}d^{2}.\]Proof.: First, we have

\[\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{\mathcal{H}}\mathrm{d} \mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\bar{\mathcal{H}}\mathrm{d} \mathbf{B}_{s}\right)\right\|^{2}\] (D.8) \[= \mathbb{E}\left\|\bar{\mathcal{H}}\left(\int_{0}^{t}A_{12}(s-t) \mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B }_{s}\right)\right\|^{2}\] \[\overset{\mathrm{a}}{\lesssim} \mathbb{E}\left\|\left(\bar{\mathcal{H}}-\mathbb{E}\left[\bar{ \mathcal{H}}|\mathcal{F}_{n}\right]\right)\left(\int_{0}^{t}A_{12}(s-t) \mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B }_{s}\right)\right\|^{2}\] \[+\mathbb{E}\left\|\mathbb{E}\left[\bar{\mathcal{H}}|\mathcal{F}_{ n}\right]\left(\int_{0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}-\int_{0}^{ \alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B}_{s}\right)\right\|^{2},\]

where \(\overset{\mathrm{a}}{\lesssim}\) follows by dividing \(\bar{\mathcal{H}}\) into \(\mathbb{E}\left[\bar{\mathcal{H}}|\mathcal{F}_{n}\right]\) and \(\bar{\mathcal{H}}-\mathbb{E}\left[\bar{\mathcal{H}}|\mathcal{F}_{n}\right]\). In the last inequality, \(\mathbb{E}\left[\bar{\mathcal{H}}|\mathcal{F}_{n}\right]\) is independent of \(\mathbf{B}_{s}\), and thus

\[\mathbb{E}\left\|\mathbb{E}\left[\bar{\mathcal{H}}|\mathcal{F}_{ n}\right]\left(\int_{0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}-\int_{0}^{ \alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B}^{\prime}_{s}\right)\right\|^{2}\] (D.9) \[\lesssim h^{2}\mathbb{E}\left\|\mathbb{E}\left[\bar{\mathcal{H}}|\mathcal{ F}_{n}\right]\int_{0}^{h}\mathrm{d}\mathbf{B}_{s}\right\|^{2}\] \[\leq h^{3}\mathbb{E}\mathrm{tr}\left(\left(\mathbb{E}\left[\bar{ \mathcal{H}}|\mathcal{F}_{n}\right]\right)^{2}\right).\]

As for the firs term, let \(\{\mathbf{B}^{\prime}_{s}\}_{0\leq s\leq h}\) be a standard Brownian motion which is independent of \(\{\mathbf{B}_{s}\}_{0\leq s\leq h}\). Define \(\widehat{\mathbf{x}}_{n}(\alpha)=\mathcal{J}(\alpha,\mathbf{x}_{n};\{\mathbf{ B}^{\prime}_{s}\}_{0\leq s\leq\alpha},(\mathbf{x}_{n},\mathbf{v}_{n}))\) and \(\mathbf{x}^{\prime}_{n}(t)=\mathcal{J}(\alpha,\widehat{\mathbf{x}}^{\prime}_{n }(\alpha);\{\mathbf{B}^{\prime}_{s}\}_{0\leq s\leq t},(\mathbf{x}_{n},\mathbf{ v}_{n}))\). Then we have

\[\mathbb{E}\left\|\bar{\mathcal{H}}-\mathbb{E}\left[\bar{\mathcal{ H}}|\mathcal{F}_{n}\right]\right\|^{2}\overset{\mathrm{a}}{\leq} \mathbb{E}\int_{0}^{1}\left\|\nabla^{2}f((1-k)\mathbf{x}_{n}(t)+k \widehat{\mathbf{x}}_{n}(\alpha))-\nabla^{2}f((1-k)\mathbf{x}^{\prime}_{n}(t) +k\widehat{\mathbf{x}}^{\prime}_{n}(\alpha))\right\|^{2}\mathrm{d}k\] \[\overset{\mathrm{b}}{\leq} L_{2}^{2}\mathbb{E}\int_{0}^{1}\left\|(1-k)(\mathbf{x}_{n}(t)- \mathbf{x}^{\prime}_{n}(t))+k(\widehat{\mathbf{x}}_{n}(\alpha)-\widehat{ \mathbf{x}}^{\prime}_{n}(\alpha))\right\|^{2}\mathrm{d}k\] \[\overset{\mathrm{c}}{\lesssim} L_{2}^{2}u^{2}\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\left(\nabla f( \widehat{\mathbf{x}}_{n}(\alpha))-\nabla f(\widehat{\mathbf{x}}^{\prime}_{n} (\alpha))\right)\mathrm{d}s\right\|^{2}\] \[+L_{2}^{2}u\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(s-\alpha) \mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B }^{\prime}_{s}\right\|^{2}\] \[+L_{2}^{2}u\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\mathrm{d} \mathbf{B}_{s}-\int_{0}^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}^{\prime}_{s}\right\| ^{2},\]

where \(\overset{\mathrm{a}}{\underset{\varepsilon}{\leq}}\) follows by the definition of \(\bar{\mathcal{H}}\) and Jensen's inequality; in \(\overset{\mathrm{b}}{\leq}\) we use the Hessian smoothness of \(f\); \(\lesssim\) follows by the definition of \(\mathbf{x}_{n}(t),\mathbf{x}^{\prime}_{n}(t),\widehat{\mathbf{x}}_{n}(\alpha)\) and \(\widehat{\mathbf{x}}^{\prime}_{n}(\alpha)\). For the first term of the last inequality, by the definition of \(\widehat{\mathbf{x}}_{n}(\alpha)\) and \(\widehat{\mathbf{x}}^{\prime}_{n}(\alpha)\), we have

\[\left\|\int_{0}^{t}A_{12}(s-t)\left(\nabla f(\widehat{\mathbf{x }}_{n}(\alpha))-\nabla f(\widehat{\mathbf{x}}^{\prime}_{n}(\alpha))\right) \mathrm{d}s\right\|^{2}\] \[\lesssim L^{2}h^{4}u\left\|\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{d} \mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B}^{\prime}_{ s}\right\|^{2}.\]And thus we have

\[\begin{split}&\mathbb{E}\left(\left\|\bar{\mathcal{H}}-\mathbb{E} \left[\bar{\mathcal{H}}|\mathcal{F}_{n}\right]\right\|^{2}\left\|\left(\int_{0} ^{t}A_{12}(s-t)\mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha) \mathrm{d}\mathbf{B}_{s}\right)\right\|^{2}\right)\\ \lesssim& L_{2}^{2}u\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t) \mathrm{d}\mathbf{B}_{s}\right\|^{4}+\mathbb{E}L_{2}^{2}u\left\|\int_{0}^{t}A_{ 12}(s-t)\mathrm{d}\mathbf{B}_{s}^{\prime}\right\|^{4}\\ &+L_{2}^{2}u\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(s-\alpha) \mathrm{d}\mathbf{B}_{s}\right\|^{4}+L_{2}^{2}u\mathbb{E}\left\|\int_{0}^{ \alpha}A_{12}(s-\alpha)\mathrm{d}\mathbf{B}_{s}^{\prime}\right\|^{4}\\ \lesssim& L_{2}^{2}uh^{6}d^{2}.\end{split}\] (D.11)

Plugging (D.11) and (D.9) in to control (D.8) yields

\[\begin{split}&\mathbb{E}\left\|\int_{0}^{t}A_{12}(s-t)\bar{ \mathcal{H}}\mathrm{d}\mathbf{B}_{s}-\int_{0}^{\alpha}A_{12}(s-\alpha)\bar{ \mathcal{H}}\mathrm{d}\mathbf{B}_{s}\right)\right\|^{2}\\ \lesssim& h^{3}\mathbb{E}\mathrm{tr}\left(\left( \mathbb{E}[\bar{\mathcal{H}}|\mathcal{F}_{n}]\right)^{2}\right)+L_{2}^{2}uh^{6} d^{2}\\ \leq& h^{3}LM+L_{2}^{2}uh^{6}d^{2},\end{split}\]

where the last inequality follows by Assumption E.2 and the observation that \(\mathbb{E}\mathrm{tr}\left(\left(\mathbb{E}[\bar{\mathcal{H}}|\mathcal{F}_{n}] \right)^{2}\right)\leq L\mathbb{E}\mathbb{E}[\mathrm{tr}\left(\bar{\mathcal{H} }\right)|\mathcal{F}_{n}]\leq LM\). 

**Lemma D.3**.: Let \(\bar{\mathcal{H}}\) and \(\bar{\mathcal{H}}^{\prime}\) be defined as above. When Assumptions E.2 and E.1 hold, we have

\[uL^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{\mathcal{H}}^{ \prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\lesssim uL^{3}h^{3}M+L_{2}^{2}u^{2}L^{2}h^{6}d^{2}.\]

Proof.: Similar to the proof of Lemma D.2, we have

\[\begin{split}& uL^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t- \alpha)\bar{\mathcal{H}}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\\ =& uL^{2}\mathbb{E}\left\|\bar{\mathcal{H}}^{\prime} \int_{0}^{\alpha}A_{12}(t-\alpha)\mathrm{d}\mathbf{B}_{t}\right\|^{2}\\ \lesssim& uL^{2}\mathbb{E}\left\|(\bar{\mathcal{H}} ^{\prime}-\mathbb{E}\left[\bar{\mathcal{H}}^{\prime}|\mathcal{F}_{n}\right] \right)\int_{0}^{\alpha}A_{12}(t-\alpha)\mathrm{d}\mathbf{B}_{t}\right\|^{2}+ uL^{2}\mathbb{E}\left\|\mathbb{E}\left[\bar{\mathcal{H}}^{\prime}|\mathcal{F}_{n} \right]\int_{0}^{\alpha}A_{12}(t-\alpha)\mathrm{d}\mathbf{B}_{t}\right\|^{2} \\ \lesssim& uL^{2}\mathbb{E}\left(\left\|\bar{\mathcal{ H}}^{\prime}-\mathbb{E}\left[\bar{\mathcal{H}}^{\prime}|\mathcal{F}_{n}\right] \right\|^{2}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\mathrm{d}\mathbf{B}_{t} \right\|^{2}\right)+uL^{2}h^{3}\mathbb{E}\mathrm{tr}\left(\left(\mathbb{E} \left[\bar{\mathcal{H}}^{\prime}|\mathcal{F}_{n}\right]\right)^{2}\right). \end{split}\] (D.12)

We follow the notation of \(\mathbf{B}_{s}^{\prime}\) and \(\widetilde{\mathbf{x}}_{n}^{\prime}(\alpha)\) in (D.10). Then

(D.13)Combining (D.12) and (D.13) and using \(\mathrm{tr}\left(\mathbb{E}[\mathcal{\tilde{H}}^{\prime}|\mathcal{F}_{n}]\right)\leq LM\) yields

\[uL^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\mathcal{\tilde{H}}^{ \prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\lesssim uL^{3}h^{3}M+L_{2}^{2}u^{2 }L^{2}h^{6}d^{2}.\]

Bound of \(\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(t)+\mathbf{x}_{n }^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+\mathbf{v}_{n}^{* }\|^{2}\)

To prove Lemma 5.3, we consider the following Lemma which also includes the Hessian smooth case.

**Lemma D.4**.: Let \(\mathbf{x}_{n}(t),\mathbf{v}_{n}(t),\mathbf{v}_{n}^{*}(t)\) and \(\mathbf{v}_{n}^{*}(t)\) be defined as above and \(u=\frac{1}{L+m}\).

1. Under Assumption 3.1, for any \(t,\alpha\leq h\), we have \[\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}( t)+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+ \mathbf{v}_{n}^{*}\|^{2}\] (D.14) \[\lesssim h^{2}\mathbb{E}\mathbb{E}_{t\sim\rho}\Omega_{n}(t)+h^{4} \mathbb{E}\Omega_{n}+u^{2}h^{4}\frac{L}{m}\mathrm{tr}(H)+h^{4}\|\mathbf{x}_{*} \|^{2}.\]
2. Under Assumptions 3.1, E.1 and E.2, for any \(t,\alpha\leq h\), we have \[\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}( t)+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+ \mathbf{v}_{n}^{*}\|^{2}\] (D.15) \[\lesssim h^{2}\mathbb{E}\mathbb{E}_{t\sim\rho}\Omega_{n}(t)+h^{4}\mathbb{ E}\Omega_{n}+u^{2}\frac{h^{4}LM}{m}+h^{4}\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{4}h^{8 }d^{2}.\]

Proof.: Since the discretized process and the continuous process are synchronously coupled,

\[\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(t)+\mathbf{x} _{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+\mathbf{v}_{n} ^{*}\] (D.16) \[= \int_{0}^{t}(\mathbf{v}_{n}(s)-\mathbf{v}_{n}^{*}(s)-um(\mathbf{x }_{n}(s)-\mathbf{x}_{n}^{*}(s))-2(\mathbf{v}_{n}(s)-\mathbf{v}_{n}^{*}(s))-u \nabla f(\widehat{\mathbf{x}}_{n}(\alpha))+u\nabla f(\mathbf{x}_{n}^{*}(s))) \mathrm{d}s.\]

Hence the square norm satisfies

\[\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(t)+\mathbf{ x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+\mathbf{v}_{n} ^{*}\|^{2}\] (D.17) \[\overset{\text{a}}{\leq} \Big{(}3t\int_{0}^{t}\|\mathbf{v}_{n}(s)-\mathbf{v}_{n}^{*}(s)+ \mathbf{x}_{n}(s)-\mathbf{x}_{n}^{*}(s)\|^{2}\mathrm{d}s+3(1-um)^{2}t\int_{0} ^{t}\|\mathbf{x}_{n}(s)-\mathbf{x}_{n}^{*}(s)\|^{2}\mathrm{d}s\] \[+3u^{2}t\int_{0}^{t}\|\nabla f(\widehat{\mathbf{x}}_{n}(\alpha)) -\nabla f(\mathbf{x}_{n}^{*}(s))\|^{2}\mathrm{d}s\Big{)}\] \[\overset{\text{b}}{\leq} 3t\int_{0}^{t}\Omega_{n}(s)\mathrm{d}s+3u^{2}t\int_{0}^{t}\| \nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n}^{*}(s))\|^ {2}\mathrm{d}s,\]

where \(\overset{\text{a}}{\leq}\) follows by equation [D.15] and Jensen's inequality; in \(\overset{\text{b}}{\leq}\) we use \(|1-um|\leq 1\) and substitute the square norm by \(\Omega_{n}(s)\). To bound the second term in the last inequality, we have

\[\int_{0}^{t}\|\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f (\mathbf{x}_{n}^{*}(s))\|^{2}\mathrm{d}s\] (D.18) \[\lesssim \int_{0}^{t}\|\nabla f(\mathbf{x}_{n}(s))-\nabla f(\mathbf{x}_{n }^{*}(s))\|^{2}\mathrm{d}s+\int_{0}^{t}\|\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))-\nabla f(\mathbf{x}_{n}(s))\|^{2}\mathrm{d}s\] \[\leq L^{2}\int_{0}^{t}\Omega_{n}(s)\mathrm{d}s+\int_{0}^{t}\|\nabla f (\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n}(s))\|^{2}\mathrm{d}s,\]where in the second inequality, we use the Lipschitz smoothness of \(f\) and \(\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}^{*}(t)\|\leq\Omega_{n}(t)\). By combining bound (D.16) and (D.17), we arrive at that

\[\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(t) +\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+ \mathbf{v}_{n}^{*}\|^{2}\] (D.18) \[\lesssim t\int_{0}^{t}\mathbb{E}\Omega_{n}(s)\mathrm{d}s+u^{2}t\int_{0}^{ t}\mathbb{E}\|\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f(\mathbf{x}_{n}(s)) \|^{2}\mathrm{d}s\] \[\leq h^{2}\int_{0}^{h}\frac{1}{h}\mathbb{E}\Omega_{n}(t)\mathrm{d}t+ u^{2}h\int_{0}^{h}\mathbb{E}\|\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f (\mathbf{x}_{n}(s))\|^{2}\mathrm{d}s.\]

By (C.3), we have \(h^{2}\int_{0}^{h}\frac{1}{h}\mathbb{E}\Omega_{n}(t)\mathrm{d}t=h^{2}\mathbb{E }_{t\sim\mathrm{unif}[0,h]}\mathbb{E}\Omega_{n}(t)\lesssim h^{2}\mathbb{E}_{t \sim\rho}\mathbb{E}\Omega_{n}(t)\).

When the Hessian of \(f\) is upper bounded by \(H\), by (D.6), we have

\[\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(t )+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+ \mathbf{v}_{n}^{*}\|^{2}\] \[\lesssim h^{2}\mathbb{E}_{t\sim\rho}\mathbb{E}\Omega_{n}(t)+u^{2}h^{2} \left(h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}L}{m}\mathrm{tr}(H)+L^{2}h^{2} \|\mathbf{x}_{*}\|^{2}\right)\] \[\lesssim h^{2}\mathbb{E}\mathbb{E}_{t\sim\rho}\Omega_{n}(t)+h^{4}\mathbb{E }\Omega_{n}+u^{2}\frac{h^{4}L}{m}\mathrm{tr}(H)+h^{4}\|\mathbf{x}_{*}\|^{2},\]

which completes the first claim.

When the \(f\) is \(L_{2}\)-Hessian smooth, by (D.7), we have

\[\mathbb{E}\|\mathbf{x}_{n}(t)-\mathbf{x}_{n}-\mathbf{x}_{n}^{*}(t )+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(t)-\mathbf{v}_{n}-\mathbf{v}_{n}^{*}(t)+ \mathbf{v}_{n}^{*}\|^{2}\] \[\lesssim h^{2}\mathbb{E}_{t\sim\rho}\mathbb{E}\Omega_{n}(t)+u^{2}h^{2} \left(h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}LM}{m}+L^{2}h^{2}\|\mathbf{x} _{*}\|^{2}+L_{2}^{2}u^{2}h^{6}d^{2}\right)\] \[\lesssim h^{2}\mathbb{E}\mathbb{E}_{t\sim\rho}\Omega_{n}(t)+h^{4}\mathbb{E }\Omega_{n}+u^{2}\frac{h^{4}LM}{m}+h^{4}\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{4} h^{8}d^{2},\]

which establishes the second claim.

Bound of \(\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha\sim\rho^{ \prime}}\big{(}\nabla f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{\mathbf{x}} _{n}(\alpha))\big{)}\|^{2}\)

**Lemma D.5**.: Let \(\mathbf{x}_{n}(t)\) and \(\widehat{\mathbf{x}}_{n}(t)\) be defined as above and \(u=\frac{1}{L+m}\). Then,

1. When Assumption 3.1 holds, and \(H\) be defined as Definition 3.2. Then \[\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha\sim\rho^{ \prime}}\big{(}\nabla f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{\mathbf{x}} _{n}(\alpha))\big{)}\|^{2}\lesssim h^{6}L^{2}\Omega_{n}+\frac{h^{6}L}{m} \mathrm{tr}(H)+h^{6}L^{2}\|\mathbf{x}_{*}\|^{2}.\]
2. When Assumptions 3.1, E.1 and E.2 hold, we have \[\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha \sim\rho^{\prime}}\nabla\big{(}f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\big{)}\|^{2}\] \[\lesssim h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}L}{m}M+h^{6}L^{2}\| \mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{2}h^{10}d^{2}.\]

Proof.: Note that \(\mathbf{x}_{n}(t)\) depends on \(\alpha\). First, we have

\[\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha \sim\rho^{\prime}}\nabla\big{(}f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\big{)}\|^{2}\] \[= \mathbb{E}\|\mathbb{E}_{\alpha\sim\rho^{\prime}}\big{(}\mathbb{E} _{\beta\sim\rho^{\prime}}\nabla f(\mathbf{x}_{n}(\beta))-\mathbb{E}_{\alpha \sim\rho^{\prime}}\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\big{)}\|^{2}\] \[= \mathbb{E}\|\mathbb{E}_{\alpha\sim\rho^{\prime}}\mathbb{E}_{\alpha \sim\rho^{\prime}}(\nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{\mathbf{x}}_{n} (s)))\|^{2}\] \[\leq \mathbb{E}_{\alpha\sim\rho^{\prime}}\mathbb{E}_{s\sim\rho^{\prime}} \mathbb{E}\int\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!\!Define \(\bar{\mathcal{H}}^{\prime\prime}=\int_{0}^{1}\nabla^{2}f(\mathbf{x}_{n})+k(\widehat{ \mathbf{x}}_{n}(s)-\mathbf{x}_{n}(s)))\) and recall that \(\bar{\mathcal{H}}^{\prime}=\int_{0}^{1}\nabla^{2}f(\widehat{\mathbf{x}}_{n}( \alpha)+k(\mathbf{x}_{n}-\widehat{\mathbf{x}}_{n}(\alpha)))\mathrm{d}k\). And then \(\mathbb{E}\left[\|\nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{\mathbf{x}}_{n }(s))\|^{2}\big{|}\alpha,s\right]\) can be bounded by

\[\mathbb{E}\left[\|\nabla f(\mathbf{x}_{n}(s))-\nabla f(\widehat{ \mathbf{x}}_{n}(s))\|^{2}\big{|}\alpha,s\right]\] \[= \mathbb{E}\left[\|\bar{\mathcal{H}}^{\prime\prime}(\mathbf{x}_{n} (s)-\widehat{\mathbf{x}}_{n}(s))\|^{2}\big{|}\alpha,s\right]\] \[\overset{\text{a}}{=} \mathbb{E}\left[\bigg{\|}\upmu\bar{\mathcal{H}}^{\prime\prime} \int_{0}^{s}A_{12}(t-s)(\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))-\nabla f( \mathbf{x}_{n}))\mathrm{d}t\bigg{\|}^{2}\bigg{|}\alpha,s\right]\] \[\overset{\text{b}}{\leq} u^{2}h^{4}L^{2}\mathbb{E}\left[\|\nabla f(\widehat{\mathbf{x}}_{n }(\alpha))-\nabla f(\mathbf{x}_{n})\|^{2}\big{|}\alpha,s\right],\]

where \(\overset{\text{a}}{=}\) is by plugging in the close-form solution of \(\mathbf{x}_{n}(s)\) and \(\widehat{\mathbf{x}}_{n}(s)\); \(\overset{\text{b}}{\leq}\) follows by \(A_{12}(t-s)\lesssim h\) and the Lipschitz continuous of \(\nabla f\).

\[\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha \sim\rho^{\prime}}\nabla\big{(}f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\big{)}\|^{2}\] \[\lesssim u^{2}h^{4}L^{2}\mathbb{E}\|\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))-\nabla f(\mathbf{x}_{n})\|^{2}\] \[\overset{\text{a}}{\lesssim} u^{2}h^{4}\left(h^{2}L^{4}\mathbb{E}\Omega_{n}+\frac{h^{2}L^{2}}{m} \mathbb{E}\mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})+h^{2}L^{4}\|\mathbf{x}_{*}\|^{2}+ uL^{2}\mathbb{E}\left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{ \mathcal{H}}^{\prime}\mathrm{d}\mathbf{B}_{t}\right\|^{2}\right)\] \[\lesssim h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}}{m}\mathrm{tr}(\bar{ \mathcal{H}}^{\prime 2})+h^{6}L^{2}\|\mathbf{x}_{*}\|^{2}+u^{3}h^{4}L^{2}\mathbb{E} \left\|\int_{0}^{\alpha}A_{12}(t-\alpha)\bar{\mathcal{H}}^{\prime}\mathrm{d} \mathbf{B}_{t}\right\|^{2},\]

where \(\overset{\text{a}}{\lesssim}\) follows by (D.4). When \(\nabla^{2}f\) has a uniform upper bound \(H\), we have

\[\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha \sim\rho^{\prime}}\nabla\big{(}f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\big{)}\|^{2}\] \[\lesssim h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}}{m}\mathrm{E} \mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})+h^{6}L^{2}\|\mathbf{x}_{*}\|^{2}+uh^{7} \mathrm{tr}(H^{2})\] \[\lesssim h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}L}{m}\mathrm{tr}(H)+h ^{6}L^{2}\|\mathbf{x}_{*}\|^{2},\]

where in the last inequality, we use that \(\mathbb{E}\mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})\leq L\mathbb{E}\mathrm{tr}(\bar{ \mathcal{H}}^{\prime})\leq L\mathrm{tr}(H)\) and \(\mathrm{tr}(H^{2})\leq L\mathrm{tr}(H)\).

And when Assumptions E.1 and E.2 hold, by Lemma D.3,

\[\mathbb{E}\|\mathbb{E}_{\beta\sim\rho^{\prime}}\mathbb{E}_{\alpha \sim\rho^{\prime}}\nabla\big{(}f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{ \mathbf{x}}_{n}(\alpha))\big{)}\|^{2}\] \[\lesssim h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}}{m}\mathbb{E} \mathrm{tr}(\bar{\mathcal{H}}^{\prime 2})+h^{6}L^{2}\|\mathbf{x}_{*}\|^{2}+uh^{7}LM+L_{2}^{2}u^{2}h^{10}d^{2}\] \[\lesssim h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}L}{m}M+h^{6}L^{2}\| \mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{2}h^{10}d^{2}.\]

## Appendix E Local Hessian bound results

As discussed in Section 4.1, here we show that the uniform Hessian upper bound can be relaxed to local trace control with an additional Hessian smooth assumption. Specifically, we make the following assumptions on \(f\).

**Assumption E.1**.: \(f\) has a \(L_{2}\)-Lipschitz Hessian.

Assumption E.1 is a standard assumption that frequently appears in the literature of optimization and sampling (Ma et al., 2021; Dalalyan and Riou-Durand, 2020; Durmus and Moulines, 2019).

**Assumption E.2**.: There exist \(M>0\) such that for all \(\mathbf{x}\in\mathbb{R}\), \(\mathrm{tr}(\nabla^{2}f(\mathbf{x}))\leq M\).

Assumption E.2 relax the uniform Hessian bound \(\nabla^{2}f(\mathbf{x})\preceq H\) for \(\mathbf{x}\in\mathbb{R}^{d}\) under which we obtain a convergence rate with \(\mathrm{tr}(H)\). Assumption E.2 only requires that the trace of Hessian can be controlled locally. Under Assumptions E.1 and E.2, the convergence of DRUL only has a weak dimension dependency, as stated in Theorem E.3.

**Theorem E.3**.: For any tolerance \(\epsilon\in(0,1)\), denote the minimizer of \(U(\mathbf{x})\) by \(\mathbf{x}_{*}\). Assume that Assumptions 3.1, E.1 and E.2 hold. Set the step size

\[h\leq\min\left\{\frac{1}{12C_{2}\kappa},\frac{\epsilon^{2/3}}{\left(48C_{2} \kappa\left(\frac{u^{2}L}{m}M+\|\mathbf{x}_{*}\|^{2}\right)\right)^{1/3}}, \frac{\epsilon^{2/7}}{\left(48C_{2}L_{2}^{2}u^{4}d^{2}\right)^{1/7}}\right\},\]

where \(C_{2}\geq 1\) is a universal constant. With initial point \((\mathbf{x}_{0},\mathbf{v}_{0})\), define \(\Omega_{0}=\mathbb{E}_{\mathbf{x}\sim p,\mathbf{v}\sim\mathcal{N}(0,u)}\left( \|\mathbf{x}_{0}-\mathbf{x}\|^{2}+\|\mathbf{x}_{0}+\mathbf{v}_{0}-\mathbf{v}- \mathbf{x}\|^{2}\right)\). Then under Assumptions 3.1, when

\[n\geq\frac{8\epsilon\kappa}{h}\log\left(\frac{2\mathbb{E}\Omega_{0}}{\epsilon ^{2}}\right),\]

Algorithm 1 outputs \(\mathbf{x}_{n}\) such that \(W_{2}(\mathrm{Law}(\mathbf{x}_{n}),p)\leq\epsilon\).

Without loss of generality, assume \(\mathbf{x}_{*}=0\). Then Theorem E.3 indicates a convergence rate of \(\widetilde{\mathcal{O}}\left(M^{1/3}\epsilon^{-2/3}+d^{2/7}\epsilon^{-2/7}\right)\), which has a lower dimension dependency both in the sense of the order of \(d\) and the relevant \(\epsilon\) coefficient. When the Hessian of any \(\mathbf{x}\in\mathbb{R}^{d}\) has a dimension-free trace, we arrive at a convergence rate of \(\mathcal{O}\left(\epsilon^{-2/3}+d^{2/7}\epsilon^{-2/7}\right)\).

### Proof of Theorem E.3

In this section, we prove the convergence guarantee using the upper bound of the local Hessian trace.

Proof.: We follow the notations in the proof of Theorem 4.2 in Appendix B.2. Similarly, denote the error in (B.5) by

\[A=2uhC_{1}\mathbb{E}\mathbb{E}_{\beta\sim\rho^{\prime}}\langle\mathbf{x}_{n}( \beta)-\mathbf{x}_{n}^{*}(\beta)+\mathbf{v}_{n}(\beta)-\mathbf{v}_{n}^{*}( \beta),\nabla f(\mathbf{x}_{n}(\beta))-\nabla f(\widehat{\mathbf{x}}_{n}( \alpha))\rangle.\]

Then

\[A\leq 2C_{1}uh\mathbb{E}\langle\mathbf{x}_{n}(\beta)-\mathbf{x}_{n}- \mathbf{x}_{n}^{*}(\beta)+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(\beta)-\mathbf{v} _{n}-\mathbf{v}_{n}^{*}(\beta)+\mathbf{v}_{n}^{*},\nabla f(\mathbf{x}_{n}( \beta))-\nabla f(\widehat{\mathbf{x}}_{n}(\alpha))\rangle\] \[+2C_{1}uh\mathbb{E}\langle\mathbf{x}_{n}-\mathbf{x}_{n}^{*}+ \mathbf{v}_{n}-\mathbf{v}_{n}^{*},\mathbb{E}_{\alpha\sim\rho^{\prime}}\mathbb{ E}_{\beta\sim\rho^{\prime}}\left(\nabla f(\mathbf{x}_{n}(\beta))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha))\right)\rangle\] \[\lesssim \frac{uh}{uh}\mathbb{E}\|\mathbf{x}_{n}(\beta)-\mathbf{x}_{n}- \mathbf{x}_{n}^{*}(\beta)+\mathbf{x}_{n}^{*}+\mathbf{v}_{n}(\beta)-\mathbf{v} _{n}-\mathbf{v}_{n}^{*}(\beta)+\mathbf{v}_{n}^{*}\|^{2}\] \[+u^{2}h^{2}\mathbb{E}\|\nabla f(\mathbf{x}_{n}(\beta))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha))\|^{2}+\frac{h}{u}uh\mathbb{E}\|\mathbf{x}_{n }-\mathbf{x}_{n}^{*}+\mathbf{v}_{n}-\mathbf{v}_{n}^{*}\|^{2}\] \[+\frac{u}{h}uh\mathbb{E}\|\mathbb{E}_{\alpha\sim\rho^{\prime}} \mathbb{E}_{\beta\sim\rho^{\prime}}(\nabla f(\mathbf{x}_{n}(\beta))-\nabla f( \widehat{\mathbf{x}}_{n}(\alpha)))\|^{2}\] \[\lesssim \left(h^{2}\mathbb{E}\mathbb{E}_{t\sim\rho}\Omega_{n}(t)+h^{4} \mathbb{E}\Omega_{n}+u^{2}\frac{h^{4}LM}{m}+h^{4}\|\mathbf{x}_{*}\|^{2}+L_{2}^ {2}u^{4}h^{8}d^{2}\right)\] \[+u^{2}h^{2}\left(h^{2}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{2}L}{m}M +L^{2}h^{2}\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{2}h^{6}d^{2}\right)+h^{2}\mathbb{ E}\Omega_{n}\] \[+u^{2}\left(h^{6}L^{2}\mathbb{E}\Omega_{n}+\frac{h^{6}L}{m}M+h^{6 }L^{2}\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{2}h^{10}d^{2}\right)\] \[\lesssim h^{2}\mathbb{E}\mathbb{E}_{\beta\sim\rho}\Omega_{n}(\beta)+h^{2} \mathbb{E}\Omega_{n}+\frac{u^{2}h^{4}L}{m}M+h^{4}\|\mathbf{x}_{*}\|^{2}+L_{2}^ {2}u^{4}h^{8}d^{2},\]

where \(\overset{a}{\lesssim}\) follows by Lemmas D.1, D.4 and D.5. We shall note that \(\mathbb{E}\Omega_{n+1}=\mathbb{E}_{\beta\sim\rho}\Omega_{n}(\beta)\). Combining the above control with (B.5), we have

\[\mathbb{E}\Omega_{n+1}\leq \frac{\mathbb{E}_{\beta\sim\rho}e^{-\frac{\beta}{\kappa}}+C_{2}h^{ 2}}{1-C_{2}h^{2}}\mathbb{E}\Omega_{n}+\frac{C_{2}h^{4}}{1-C_{2}h^{2}}\left( \frac{u^{2}L}{m}M+\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{4}h^{4}d^{2}\right)\] \[\leq \left(1-\frac{h}{6\kappa}\right)\mathbb{E}\Omega_{n}+2C_{2}h^{4} \left(\frac{u^{2}L}{m}M+\|\mathbf{x}_{*}\|^{2}+L_{2}^{2}u^{4}h^{4}d^{2}\right),\]

[MISSING_PAGE_EMPTY:29]