# RH-BrainFS: Regional Heterogeneous Multimodal Brain Networks Fusion Strategy

Hongting Ye \({}^{1}\), Yalu Zheng \({}^{1}\), Yueying Li \({}^{1}\), Ke Zhang \({}^{1}\), Youyong Kong \({}^{1,2}\)*, Yonggui Yuan \({}^{3}\)

\({}^{1}\)Jiangsu Provincial Joint International Research Laboratory of Medical Information Processing

School of Computer Science and Engineering, Southeast University

\({}^{2}\)Key Laboratory of New Generation Artificial Intelligence Technology and Its

Interdisciplinary Applications (Southeast University), Ministry of Education, China

\({}^{3}\)Department of Psychosomatics and Psychiatry, Zhongda Hospital

School of Medicine, Southeast University

{yehongting, 220212084, 230228504, kylenz, kongyouyong}@seu.edu.cn

yygylh2000@sina.com

Corresponding author

###### Abstract

Multimodal fusion has become an important research technique in neuroscience that completes downstream tasks by extracting complementary information from multiple modalities. Existing multimodal research on brain networks mainly focuses on two modalities, structural connectivity (SC) and functional connectivity (FC). Recently, extensive literature has shown that the relationship between SC and FC is complex and not a simple one-to-one mapping. The coupling of structure and function at the regional level is heterogeneous. However, all previous studies have neglected the modal regional heterogeneity between SC and FC and fused their representations via "simple patterns", which are inefficient ways of multimodal fusion and affect the overall performance of the model. In this paper, to alleviate the issue of regional heterogeneity of multimodal brain networks, we propose a novel Regional Heterogeneous multimodal Brain networks Fusion Strategy (RH-BrainFS).2 Briefly, we introduce a brain subgraph networks module to extract regional characteristics of brain networks, and further use a new transformer-based fusion bottleneck module to alleviate the issue of regional heterogeneity between SC and FC. To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose a solution. Extensive experiments demonstrate that the proposed method outperforms several state-of-the-art methods in a variety of neuroscience tasks.

Footnote 2: The codes are available at https://github.com/Yedaxia1/RH-BrainFS.

## 1 Introduction

Currently, a large number of neuroscience studies are based on unimodal imaging [2, 30, 47, 56, 61]. However, different brain imaging techniques, such as functional magnetic resonance imaging (fMRI) [54] and diffusion magnetic resonance imaging (dMRI) [44], reflect different aspects of the brain's internal characteristics. Therefore, it is often insufficient to use a single modality of data for neuroscience research and it is necessary to integrate multiple modalities of imaging data to achieve good performance in neuroscience tasks such as depression classification [3, 33] and gender classification [15, 53].

In multimodal brain networks fusion, existing researchs are mainly focused on fusing structural and functional modalities (structural modality is constructed from dMRI and functional modality isconstructed from fMRI) [6, 29, 32, 57, 58]. Most methods directly fuse the two modal representations via "simple patterns" (we define this as **direct interaction**, where two modal features/embeddings are directly combined to perform some computation, e.g. concatenation [32], weighted summation [57], or self-attention [58] techniques.) without considering the issue of regional heterogeneity [37] between this two modalities. However, extensive literature [22, 37] has shown that the relationship between structural connectivity (SC) and functional connectivity (FC) is complex and not a simple one-to-one mapping. Specifically, the coupling of structure and function at the regional level is heterogeneous and follows the molecular, cellular and functional hierarchical structure. In other words, structure may be more tightly coupled to function in some regions than in others. This shows that regional heterogeneity is a key factor in linking different modal brain networks.

Based on the above research gap, in this paper we propose a novel regional heterogeneous multimodal brain networks fusion strategy that aims to fully account for the regional heterogeneity among brain networks and achieve better multimodal brain networks fusion performance.

First, the brain network itself has strong regional characteristics [36, 41], and the combination of neighbouring brain regions can serve as an important criterion for neuroscience tasks [16, 24]. Specifically, regional characteristics behave as subnetwork (also called subgraph) features in the whole brain network [11, 28], yet other multimodal studies do not take this into account. In this paper, we focus on the characteristics of brain regions from the subgraph pattern. The subgraph pattern have become a relatively hot area of graph representation learning research in recent years [25, 39, 55]. Subgraph convolutional networks can obtain potential representations of each subgraph in the graph, that reflect the regional characteristics of the graph [7, 10]. Therefore, to effectively extract the regional characteristics of each brain region in the brain network, we introduce a Brain Subgraph Networks (BrainSubGNN) module in Sec. 3.2. Our BrainSubGNN is divided into two steps, subgraph sampling and subgraph embedding. The former is used to obtain the subgraph partition of the brain network, the latter to aggregate the subgraph characteristics of the brain network.

Next, since several previous studies have shown that the characteristics of different individual regions in the brain network have widely different influences on neuroscience tasks [1, 31, 40, 42], we need to pay attention to the influence of these brain region characteristics on neuroscience tasks and quantify them into accurate values. In this paper, we focus on the Transformer [45] to learn the accurate influence values of these brain region characteristics. Although originally proposed for NLP tasks, there has been recent interest in Transformers [45] as universal perceptual models [14]. Through the attention mechanism, Transformer can learn the accurate influence values of different tokens well for the classification result, which happens to meet our needs.

However, as mentioned above, there is the issue of modal regional heterogeneity between SC and FC, which is a key factor in linking these two modal brain networks [37]. Many previous multimodal fusion methods, via "direct interaction", have not considered this issue between SC and FC, which are inefficient ways of modality fusion and affect the overall performance of the model. Based on that, avoiding direct interaction between two modalities within the Transformer is the aim of our study. Inspired by MBT [26], we present a transformer-based fusion bottleneck (Trans-Bottleneck) module for fusing regional heterogeneous brain networks in Sec. 3.3. Specifically, the Trans-Bottleneck module contains two standard transformers [45] and a certain number of fusion bottlenecks. The standard transformers are used to learn the accurate influence values of each brain region on neuroscience tasks through the attention mechanism. The fusion bottlenecks, as intermediate media for modality fusion, establish connections between regional heterogeneous modalities and learn the key information of each modality in the latent space (we define this as **indirect interaction**, where two modal features/embeddings are not directly combined for the computation.).

The main contributions of this paper are summarized as follows:

* To alleviate the issue of regional heterogeneity of multimodal brain networks, we propose a novel **R**egional **H**eterogeneous multimodal **Brain** networks **F**usion **S**trategy (RH-BrainFS), using BrainSubGNN module and Trans-Bottleneck module to fuse regional heterogeneous multimodal brain networks for neuroscience tasks.
* To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose a solution.
* Extensive experiments demonstrate the effectiveness of RH-BrainFS in multimodal brain networks fusion tasks on depression classification and gender classification datasets.

## 2 Related Work

**Brain Subgraph Networks:** In recent years, subgraph techniques have gained popularity in brain network analysis due to their ability to accurately model certain aspects of brain organization with high consistency to established brain functional systems [27]. This has led to a focus on identifying informative and signaling subgraphs from the entire brain connectome that may be relevant to brain diseases [46]. Various subgraph-based methods have been proposed for brain network research, such as the adaptive dense subgraph discovery (ADSD) model [51] which uses a likelihood-based approach to extract disease-associated subgraphs from group-level whole-brain connectome data. Other methods, such as an earlier approach from [4], balance topological information of local and global graphs using subgraphs, while approach [5] utilizes subgraphs to represent local features in large-scale brain network. Recent research [19] has also classified brain network by extracting contrastive subgraphs, and the SBLR model [49] suggests that subgraphs have attractive neurological interpretations and may correspond to outcome-related anatomical circuits. In this paper, our model builds upon these approaches and employs a BrainSubGNN module to efficiently aggregate regional characteristics of brain network.

**MultiModal Brain Networks Fusion:** Over the past few years, several methods have been developed for multimodal fusion in neuroscience research [12; 8; 38; 23; 60]. One traditional approach, SNF [48], creates an initial similarity network for each feature and iteratively combines them with a non-linear graph fusion formula to generate a final fusion network. However, recent advancements in deep learning have led to the development of more sophisticated multimodal fusion techniques. For instance, the GBDM [57] model employs both structural and functional information from diffusion and functional magnetic resonance imaging (MRI), respectively, to effectively differentiate individuals with mild cognitive impairment (MCI) from age-matched controls. The MGCN [29] model uses manifold-based regularization terms to account for inter-modality and intra-modality relationships. One approach [13] involves deep collaborative learning to capture cross-modal associations and trait-related features. Another approach [32] combines representations to fuse multimodal data. Moreover, a method [62] applies a multimodal non-Euclidean brain network analysis technique based on community detection and convolutional autoencoder for epilepsy classification. Additionally, a new adversarial learning-based node-edge graph attention network (AL-NEGAT [6]) has been proposed for autism spectrum disorder (ASD) recognition based on multi-modal MRI data. However, none of these methods have addressed the issue of modal regional heterogeneity [37] in multimodal

Figure 1: The overall framework of our proposed RH-BrainFS Model. The structural connectivity (SC) and functional connectivity (FC) are constructed from fMRI and dMRI respectively, the initial bottlenecks \(Z_{b}\) are a set of learnable tokens randomly sampled from standard normal distribution, and the subgraph partition (\(SP_{i}\)) are obtained in the preprocessing stage. Function \(g(\cdot)\) denotes the BrainSubGNN module.

brain networks. To fill this gap, we propose a new multimodal brain networks fusion strategy aimed at alleviating the modal regional heterogeneity issue and achieving better fusion performance.

## 3 Method

In this section, we present our proposed regional heterogeneous multimodal brain networks fusion strategy RH-BrainFS (as shown in Fig. 1). We begin by discussing the definition of the multimodal brain networks fusion task in Sec. 3.1. We then explain how the BrainSubGNN module captures the regional characteristics of brain networks in Sec. 3.2. Finally, we describe how the Trans-Bottleneck module used in the RH-BrainFS model alleviate the issue of regional heterogeneity in multimodal brain networks in Sec. 3.3.

### Perliminaries

**Multimodal Brain Networks Fusion Task:** Given a multimodal brain networks dataset \(D=\{Sample_{0},Sample_{1},...,Sample_{L-1}\}\), where each sample represents a person, and \(L\) is the size of this dataset. Each \(Sample_{i}=(\mathcal{G}_{sc},\mathcal{G}_{fc},y)\) contains a structural connectivity graph (each brain region is viewed as a node in the graph), a functional connectivity graph (ditto) and a class label \(y\in\{0,1\}\) (0, 1 represent different meanings on different tasks). Each kind of input graph \(\mathcal{G}=(A,X,V)\) is composed of a node set \(V\), an adjacency matrix \(A\in\mathbb{R}^{N\times N}\) and node features \(X\in\mathbb{R}^{N\times d}\), where \(N=|V|\) denotes the number of nodes and \(d\) denotes the input dimensional of node features. In the brain network graph, node set \(V\) denotes the collection of brain regions, adjacency matrix \(A\) denotes the connectivity between various brain regions and node features \(X\) denotes the original features of each brain region. In the multimodal brain networks classification task, the purpose is to find a mapping function \(g:(\mathcal{G}_{sc},\mathcal{G}_{fc})\to y\).

### Brain Subgraph Networks

We now explain how BrainSubGNN module captures the regional characteristics of brain networks. As shown in Fig. 2, the BrainSubGNN contains subgraph sampling step and subgraph embedding step.

#### 3.2.1 Subgraph sampling

The purpose of subgraph sampling is to construct a receptive field for each brain region (also named node in graph), it represent the regional characteristics of this brain region. In our RH-BrainFS, rooted subgraph [59] is utilized to construct receptive field, as the rooted subgraph can exhibit even greater discriminative power than the first-order Weisfeiler-Leman (1-WL) test due to interconnectivity among neighboring nodes [50; 59].

Figure 2: Brain Subgraph Networks (BrainSubGNN). Including subgrah sampling process (1-hop) and subgraph embedding process.

Specifically, we define central subgraph for each node in the whole brain network. The central subgraph describes the surrounding region of the central node. For a input graph \(\mathcal{G}\) and the central node \(v_{i}\), the central subgraph is denoted as \(\tilde{\mathcal{G}}_{i}^{k}=(\tilde{A}_{i},\tilde{X}_{i},\tilde{V}_{i})\subset \mathcal{G}\) where \(\tilde{V}_{i}=\mathcal{N}^{k}(i)\) contains k-hop neighbor nodes of \(v_{i}\) and \(\tilde{A}_{i}\), \(\tilde{X}_{i}\) equal to the corresponding part in the original \(A\) and \(X\). The central subgraph \(\tilde{\mathcal{G}}_{i}^{k}\) constructed above includes brain region \(i\) and its k-hops adjacent brain regions. Such a subgraph contains the characteristics of this local brain network, which is of great significance to the brain network. In this work, the subgraph sampling is a preprocessing process before network training, node and edge indice are saved as a binary file. In the training stage, the indice are loaded directly without additional cost on training.

#### 3.2.2 Subgraph embedding

The subgraph partitions of the brain network are obtained in the previous step, each subgraph contains a central brain region and several adjacent brain regions, so we expect to be able to use a method that integrates the characteristics of all the brain regions in the subgraph and extract an embedding to represent the entire subgraph. Inspired by the graph isomorphism network [52], we consider the subgraph representation task as a multi-set problem and use multi-layer perceptrons to learn an injective function for aggregating regional characteristics the brain subgraph network effectively and learn subgraph representation as:

\[z_{i}^{(l)}=\text{MLP}^{(l)}\left(W_{1}^{(l)}\tilde{h}_{i}^{(l)}+\sum_{j\in \tilde{V}_{i}\backslash i}W_{2}^{(l)}\tilde{h}_{j}^{(l)}\right)\] (1)

where \(\tilde{h}_{i}\) denote the hidden representation of node \(i\), and \(W_{1}^{(l)}\), \(W_{2}^{(l)}\in\mathbb{R}^{d^{(l)}\times d^{(l+1)}}\) are learnable weight matrices for the central node and other nodes in subgraph, respectively. By default, we set the MLP as a two-layer fully connected module. The obtained \(z_{i}\) represents the local characteristics of brain region \(i\).

### Transformer-Based Fusion Bottleneck

Through the BrainSubGNN described earlier, we obtain the regional representations of each modality of the brain network \(Z_{i}\in\mathbb{R}^{N\times d_{hid}}\). Next, our goal is to apply an efficient fusion strategy to fuse critical and complementary information from \(Z_{sc}\) and \(Z_{fc}\), extract a distinguishing embedding to represent the features of the whole brain, and serve as a criterion for downstream tasks. We now describe how Trans-Bottleneck module alleviate the issue of regional heterogeneity in multimodal brain networks.

#### 3.3.1 Fusion Bottlenecks

Due to the issue of regional heterogeneity between SC and FC, we are committed to avoiding the direct interaction of two modalities, and we prefer to find an intermediate element as a bridge for the interaction between two modalities (means indirect interaction). Inspired by MBT [26], we introduce the fusion bottlenecks into neuroscience research.

Specifically, fusion bottlenecks are simply a set of learnable tokens \(Z_{b}\in\mathbb{R}^{N_{b}\times d_{hid}}\), where \(N_{b}\) is a hyperparameter denoting the number of fusion bottlenecks. In this work, we utilize the fusion bottlenecks as intermediate medium to bridge contact of this two modalities (SC and FC). As shown in Fig. 3, the fusion bottlenecks allow information to flow between modalities and fusion bottlenecks (indirect interaction), but limit the flow of information between modalities (direct interaction). This procedure avoid direct interaction between regional heterogeneous modality data, effectively improving the model's performance. The initial bottlenecks are a set of learn

Figure 3: Interaction between three main tokens.

able tokens that are randomly sampled from standard normal distribution \(\mathcal{N}(0,1)\) and then passed to the first RH-BrainFS layer. Finally, the fusion bottlenecks output by the last RH-BrainFS layer are used as the classification basis for downstream tasks (gender classification, major depressive disorder diagnosis, etc.).

#### 3.3.2 Transformer-Based Fusion

Although originally proposed for NLP tasks, there has been recent interest in Transformers as universal perceptual models due to their ability to model dense correlations between tokens. Meanwhile, it turns out that individual regional characteristics of brain networks have different influence values for neuroscience tasks [1; 31; 40; 42]. Based on these, our RH-BrainFS method utilizes the Transformer [45] as a baseline for the fusion strategy to capture key subgraph characteristics in brain networks.

Specifically, as shown in Fig. 1, we first concatenate the fusion bottlenecks \(Z_{b}\) and each modality representation \(Z_{i}\), and then feed the concatenated tokens to the standard Transformer [45] model of the corresponding modality. In the Transformer model, the fusion bottlenecks learn important regional characteristics of the brain network for each modality through attention mechanism. The tokens output by the Transformer are re-split according to the previous splicing scheme to obtain new potential features \(Z_{i}^{l+1}\) of the brain network and temporary fusion bottlenecks \(\hat{Z}_{b_{i}}^{l+1}\) corresponding to each modality. After that, the temporary fusion bottlenecks \(\hat{Z}_{b_{i}}^{l+1}\) of all modalities are matrix-averaged (also named average pooling) to obtain the fusion bottleneck latent representation \(Z_{b}^{l+1}\).

This procedure can be formulated as:

\[\left[Z_{i}^{l+1}\mid\mid\hat{Z}_{b_{i}}^{l+1}\right]=\text{Transformer}(\left[ Z_{i}^{l}\mid\mid Z_{b}^{l}\right];\theta_{i})\] (2)

\[Z_{b}^{l+1}=\text{AVERAGE}(\hat{Z}_{b_{i}}^{l+1})\] (3)

where \(i\in\{SC,FC\}\), \(\theta_{i}\) denotes the modality specific Transformer, \([\cdot\mid\mid\cdot]\) denotes the concatenate operation, \(Z_{b}^{l}\) denotes the fusion bottlenecks in \(l^{th}\) layer, and \(\hat{Z}_{b_{i}}^{l+1}\) denotes the temporary fusion bottlenecks in \(i^{th}\) modality specific Transformer.

In this procedure, each modality transfers importance regional brain network characteristics within its modality to the fusion bottlenecks, and the fusion bottlenecks utilize shared characteristics between modalities to guide the learning of each modality's brain network in the next layer.

#### 3.3.3 Downstream Tasks

In the final stage, we complete classification basis for downstream tasks through the output \(Z_{b}^{l}\) in the last layer. Specifically, we first use global mean pooling as the readout function, and then input the result into a MLP to complete classification basis.

\[Logits=\text{Softmax}(\text{MLP}(\frac{1}{N_{b}}\sum_{N_{b}}^{i}Z_{i,b}))\] (4)

where \(Z_{i,b}\) denotes the i-th row of \(Z_{b}\), and \(Logits\in\{0,1\}\). In this paper, on the depression classification task, 0 represents major depressive disorder, 1 represents normal control. And on the gender classification task, 0 represents male, 1 represents female.

## 4 Experiments

In this section, we perform a series of experiments to evaluate the effectiveness of the proposed RH-BrainFS method. First, we provide the detailed experimental settings in Sec. 4.1. Then, we perform comparison experiments on all datasets to compare the performance of different methods in Sec. 4.2. Finally, we perform some ablation studies of the main modules and hyperparameters in the proposed RH-BrainFS method in Sec. 4.3.

### Experimental Settings

**Datasets.** We evaluate our RH-BrainFS method on two different classification tasks investigating structure-function fusion. \(1)\) The gender classification task on Human Connectome Project (HCP) dataset [43], which contains 560 female samples and 479 male samples. \(2)\) The Major Depressive Disorder (MDD) diagnosis task on the hospital datasets [17; 18], including the Affiliated Zhongda Hospital of Southeast University (Zhongda hospital) and the Second Affiliated Hospital of Xinxiang Medical University (Xinxiang hospital). This study included 48 controls and 62 MDD patients from the Zhongda hospital and 46 controls and 31 MDD patients from the Xinxiang hospital. We also combine Zhongda and Xinxiang as Two-site dataset to construct more difficult and more realistic tasks.

**Preprocessing.** Here, we would briefly introduce how to construct the brain network of SC from dMRI and FC from fMRI.

* SC. The dMRI data is preprocessed using the brain's diffusion toolbox of FMRIB Software Library [34]. Next we construct \(\mathcal{G}_{sc}=(A_{sc},X_{sc},V_{sc})\) from preprocessed dMRI data. First, we obtain the brain regions (\(V_{sc}\)) of the individual space by mapping the anatomical automatic labeling (AAL) template in the standard space to the individual space. Then, we using DSI Studio software [9] to implement Fiber tracking. Finally, we obtain the feature \(X_{sc}\in\mathbb{R}^{|V_{sc}|\times|V_{sc}|}\) by counting the number of structural connective fibres between the different regions of the AAL, and the adjacency matrix \(A_{sc}\) of the structural graph is obtained by thresholding \(X_{sc}\) with a threshold.
* FC. The fMRI data is preprocessed using the Data Processing Assistant for Resting-State Function (DPARSF) [35] MRI toolkit. Next we construct \(\mathcal{G}_{fc}=(A_{fc},X_{fc},V_{fc})\) from preprocessed fMRI data. First, averaged time series are first computed for each brain region with a predefined atlas. Then, the Pearson correlation is utilized to calculate the functional matrix. Finally, the functional matrix is thresholded by proportional quantization to obtain the adjacency matrix \(A_{fc}\). The features \(X_{fc}\) are functional connectivity matrix obtained earlier.

**Metrics.** In this study, we evaluate all the methods using 10-fold cross-validation with the same partition of training and testing splits. Our evaluation metrics include classification accuracy (ACC), sensitivity (SEN), specificity (SPE), f1 score (F1) and ROC-AUC (AUC). Higher values for all metrics indicate better performance. We record the mean and standard deviation on 10 random runs with 10-fold cross-validation.

**Implementation Details.** For all experiments, we adopt Adam as the optimizer and StepLR (step_size=50, gamma=0.8) as the scheduler. The initial learning rate is set to 5e-4 and the dropout rate is set to 0.3. Also we utilize a early stop mechanism that 300 epochs patience in total 500 epochs. In the RH-BrainFS model, we set the k-hop in the subgraph sampling to 1, the number of bottlenecks \(N_{b}\) to 4, the number of attention heads in the Transformer to 4, and the total number of network layers to 2. All our experiments are implemented in PyTorch and trained on one NVIDIA 3090.

### Comparison Experiments

In this section, we verify the performance of our RH-BrainFS against existing baselines on several datasets.

**Baselines.** We choose two categories of methods as comparison methods, both of which are methods for the direct study of SC and FC. The first category is unimodal methods, including FGDN [20] and BrainGNN [21], where FGDN uses a spectral graph convolution method to extract brain networks features, and BrainGNN proposes a ROI-aware graph convolution layer and uses pooling to extract brain networks features. In our experiments, SC and FC are used as inputs to the unimodal method, respectively. The second category is multimodal methods, including SVM, Random Forest, MGCN [29], GBDM [57], MMGNN [32] and AL-NEGAT [6], where SVM and Random Forest concatenate SC and FC as input, MGCN uses manifold-based regularization terms to consider inter-modality and intra-modality relationships, GBDM adopts weighted summation pattern to fuse multimodal brain networks, MMGNN utilises the concatenation method in multimodal tasks and AL-NEGAT combines multimodal information to construct edge feature maps and node feature maps. Code implementations of all baseline methods are taken from their respective original papers.

**Results.** As shown in Tab. 1, our model significantly outperforms the other comparison methods on the all selected datasets. The results show that multimodal methods generally have better performance than unimodal methods because they capture more complementary information. Among the multimodal methods, our RH-BrainFS method achieves the best performance on the all the selected datasets (improvement of 3.51% on HCP, 5.46% on Zhongda hospital dataset, 8.03% on Xinxiang hospital dataset and 5.50% on two-site dataset). The reason for the performance improvement is that our RH-BrainFS method fully considers the issue of regional heterogeneity between SC and FC and proposes an appropriate solution strategy for this issue.

**Visualization.** In order to intuitively display the performance of each multimodal method, we conduct visualization experiments on the HCP dataset. Specifically, we take the graph-level embedding output from the last layer of each multimodal method for t-SNE visualisation. As shown in Fig. 4, although the MGCN and GBDM all form two clusters, these two clusters do not distinguish the two types of samples well and there is still a lot of confusion. The MMGNN is loosely distributed and does not form good class boundary. It can be seen that the Al-NEGAT produces a similar distribution to our method, but there's still a lot of confusion at the class boundary. In contrast, our method eliminates the confusion at the class boundary, achieves a good classification effect, most of the samples can be accurately distinguished and only a small number of samples have errors.

### Ablation Study

In this section, we further perform ablation studies on the main modules and hyperparameters in the proposed RH-BrainFS method.3

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Modality} & \multicolumn{4}{c}{Datasets} \\ \cline{3-6}  & & HCP & Zhongda & Xinxiang & Two-site \\ \hline FGDN & FC & 67.56\(\pm\)3.02 & 65.67\(\pm\)3.26 & 67.91\(\pm\)3.27 & 59.34\(\pm\)2.78 \\ FGDN & SC & 63.42\(\pm\)4.79 & 64.02\(\pm\)3.49 & 65.89\(\pm\)5.15 & 68.91\(\pm\)2.53 \\ BrainGNN & FC & 66.41\(\pm\)6.44 & 69.18\(\pm\)3.39 & 73.46\(\pm\)4.33 & 69.55\(\pm\)3.23 \\ BrainGNN & SC & 67.37\(\pm\)5.89 & 70.73\(\pm\)2.07 & 73.66\(\pm\)3.60 & 69.51\(\pm\)2.58 \\ \hline SVM & SC,FC & 74.49\(\pm\)2.97 & 63.21\(\pm\)2.09 & 71.73\(\pm\)1.99 & 66.06\(\pm\)1.56 \\ Random Forest & SC,FC & 68.24\(\pm\)2.94 & 61.45\(\pm\)2.80 & 62.78\(\pm\)1.63 & 62.43\(\pm\)2.19 \\ MGCN & SC,FC & 67.94\(\pm\)5.41 & 75.18\(\pm\)2.34 & 82.24\(\pm\)3.71 & 72.98\(\pm\)2.17 \\ GBDM & SC,FC & 71.02\(\pm\)4.39 & 74.81\(\pm\)2.44 & 80.71\(\pm\)2.83 & 72.48\(\pm\)1.91 \\ MMGNN & SC,FC & 73.33\(\pm\)2.82 & 60.69\(\pm\)3.61 & 68.21\(\pm\)4.44 & 59.72\(\pm\)3.18 \\ AL-NEGAT & SC,FC & 75.12\(\pm\)3.66 & 73.95\(\pm\)3.45 & 75.75\(\pm\)3.81 & 71.86\(\pm\)2.49 \\ \hline RH-BrainFS (ours) & SC,FC & **78.63\(\pm\)4.36** & **80.64\(\pm\)1.58** & **90.27\(\pm\)2.00** & **78.48\(\pm\)1.43** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison experiments results (in percentage) on the all chosen datasets (only the accuracy is shown, the full results can be referred to the Appendix A). The best results are marked in bold. The suboptimal results are marked underlined.

Figure 4: t-SNE visualisation of multimodal method on HCP dataset. Each dot denotes a test sample.

**Effectiveness of Main Modules.** First, we perform an ablation study on HCP dataset to validate effectiveness of the main modules, BrainSubGNN and Trans-Bottleneck. Specifically, we replace BrainSubGNN with normal GIN [52] (process on the whole brain network), and replace Trans-Bottleneck with a standard transformer (compute self-attention directly between two modalities, equal to direct interaction), respectively. As shown in Tab. 2, we can find that both BrainSubGNN and Trans-Bottleneck have a certain effect on the performance of the model (compare the first three rows in the table). Furthermore, we find that the two modules are compatible and complementary, as our RH-BrainFS method (combining the two modules) achieves the best performance among the ablation study.

**Impact of Bottlenecks Number.** We then investigate the impact of varying bottlenecks number. Specifically, we run experiments with # bottlenecks=2,4,6,8, respectively, with all other parameters unchanged. In order to ensure the credibility of the experiment, we perform experiments on two datasets (HCP dataset and Two-site dataset). As shown in Fig. 5, on both datasets, the performance of any number of bottlenecks exceeds the baselines. The main reason for this results is that the Trans-Bottleneck module takes into account the regional heterogeneity issue between SC and FC, and avoids the direct interaction of heterogeneous information, thus achieving good fusion performance. Such results demonstrate the importance of the regional heterogeneity issue in the modality fusion process of SC and FC. At the same time, we find that when # bottlenecks=4, the model performance reaches the best, which reflects that only a small number of bottlenecks is needed to achieve a good multi-modal fusion expression capability.

**Impact of Subgraph Sampling Hops.** Next, we investigate the impact of varying sampling hops of subgraph. In this experiment, we set the range of sampling hops of subgraph from 1 to 5. Likewise, we run experiments on HCP dataset and Two-site dataset. As shown in Fig. 5, it can clearly be seen that as the number of sampling hops increases, the performance of the model generally shows a downward trend, and when the sampling hops are too large, the model actually performs worse than the baselines. The reason for this results is that the brain network itself has strong regional characteristics, and too large sampling hops will cause the sampled subgraph to lose the local characteristics of the brain network and tend towards global characteristics.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Modules & ACC & SEN & SPE & F1 & AUC \\ \hline w/o BrainSubGNN Trans-Bottleneck & 76.23±3.78 & 71.40±11.94 & 80.36±7.70 & 73.00±6.27 & 75.88±4.12 \\ w/o BrainSubGNN & 77.22±3.89 & 68.44±12.16 & **82.86±8.61** & 72.16±6.34 & 75.65±4.19 \\ w/o Trans-Bottleneck & 76.80±2.93 & 71.14±10.50 & 81.61±7.10 & 73.50±5.13 & 76.38±3.27 \\ RH-BrainFS (ours) & **78.63±4.36** & **75.59±6.75** & 81.25±6.04 & **76.49±4.91** & **78.42±4.38** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Ablation study of the main modules in the RH-BrainFS method. The bold font indicates that the evaluation metric achieves the best performance in the ablation study.

Figure 5: The effect of varying hyperparameters. (a) and (c) are experiments on HCP datasets, (b) and (d) are experiments on Two-site datasets.

Figure 6: The effect of varying thresholding values.

The results show that only 1 hop of subgraph sampling is needed to express the local characteristics of the brain network well.

**Impact of Thresholding Values.** Finally, in this paper, thresholding is used to obtain the adjacency matrix of the two modalities during data processing (thresholding \(X_{sc}\) and \(X_{fc}\) to obtain \(A_{sc}\) and \(A_{fc}\), respectively). Thresholding value has been an important ablation study in neuroscience, and the same experiment was conducted in this paper. As shown in Fig. 6, we set the value range of thresholding to [0.02, 0.30], with 0.02 as a step, for a total of 15 thresholding experiments. From the experimental results, there is no clear trend between threshold values and model performance, but the model performs poorly when the threshold value is too high or too low, so in this paper we chose a threshold value of 0.12 as this is when the model performs best.

## 5 Discussions

**Conclusion.** In this paper, we introduce a brain subgraph networks and a transformer-based fusion bottleneck to alleviate the issue of regional heterogeneity between SC and FC, and propose a novel multimodal brain networks fusion strategy (RH-BrainFS). To the best of our knowledge, this is the first paper to explicitly state the issue of structural-functional modal regional heterogeneity and to propose a solution. We validate our method on a variety of downstream task datasets, achieving state-of-the-art performance.

**Limitations and Future Work.** Due to the scarcity and difficulty of collecting and processing data in neuroscience, the only datasets currently available are very limited, despite the fact that we have spent a great deal of manual effort in this area, and thus the research in this paper suffers from a number of possible data bias issues. In our future work, on the one hand, we will do more work on data to mitigate the problem of data bias. On the other hand, although this paper proposes the use of indirect rather than direct interaction, the two may not be mutually exclusive, and in the future we will investigate how to combine the two to achieve better research results.

**Ethical Issues.** With regard to possible ethical issues in data collection, the Human Connectome Project (HCP) dataset, as a publicly available dataset that has been used in numerous previous studies, is undoubtedly not ethically questionable. It is true that the hospital dataset is held in collaboration with our partner hospitals and is not yet publicly available, but the data is collected with the consent of the subjects who are clearly informed of the purpose of the sample collection, and all identifying information about the sample is hidden in the hospital dataset. Therefore, it does not adversely affect any individual, so there are no ethical or moral issues.

**Possible Negative Social Impacts.** As the research in this paper deals with the diagnosis of depression, it is necessary to elaborate here on the possible negative social impacts of this work, despite the fact that all the current work is at the stage of scientific research and has not been put to practical use. Including but not limited to:

* Incorrect diagnosis. AI methods must have the possibility of error, which cannot be avoided, but an incorrect diagnosis will have a significant impact on individuals and society. Therefore, AI tools can only be used as a diagnostic aid, not as a decision maker, and the final decision should still be made by the doctor.
* Leakage of privacy information. In depression dataset, the identity information of the subjects is highly private, and the leakage of identity information will also have unpredictable and significant impact on individuals and society. Therefore, in this work, we have completely hidden the subjects' identifying information (which is also not visible to the staff in the study group) as a way of preventing the leakage of private information.

## Acknowledgments and Disclosure of Funding

This work is supported by 2022YFE0116700 National Key Research and Development Program of China, supported by 82271570, 31800825 and 31640028 National Natural Science Foundation of China, and also supported by the Big Data Computing Center of Southeast University. This work is partly supported by grant 2242023k30052 Central University Basic Research Fund of China. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

## References

* [1] Kwangyeol Baek, Laurel S Morris, Prantik Kundu, and Valerie Voon. Disrupted resting-state brain network properties in obesity: decreased global and putaminal cortico-striatal network efficiency. _Psychological medicine_, 47(4):585-596, 2017.
* [2] Alaa Bessadok, Mohamed Ali Mahjoub, and Islem Rekik. Graph neural networks in network neuroscience. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [3] Hanshu Cai, Zhidiao Qu, Zhe Li, Yi Zhang, Xiping Hu, and Bin Hu. Feature-level fusion approaches based on multimodal eeg data for depression recognition. _Information Fusion_, 59:127-138, 2020.
* [4] Bokai Cao, Liang Zhan, Xiangnan Kong, Philip S Yu, Nathalie Vizueta, Lori L Altshuler, and Alex D Leow. Identification of discriminative subgraph patterns in fmri brain networks in bipolar affective disorder. In _Brain Informatics and Health: 8th International Conference, BIH 2015, London, UK, August 30-September 2, 2015. Proceedings 8_, pages 105-114. Springer, 2015.
* [5] Lucy R Chai, Ankit N Khambhati, Rastko Ciric, Tyler M Moore, Ruben C Gur, Raquel E Gur, Theodore D Satterthwaite, and Danielle S Bassett. Evolution of brain network dynamics in neurodevelopment. _Network Neuroscience_, 1(1):14-30, 2017.
* [6] Yuzhong Chen, Jiadong Yan, Mingxin Jiang, Tuo Zhang, Zhongbo Zhao, Weihua Zhao, Jian Zheng, Dezhong Yao, Rong Zhang, Keith M Kendrick, et al. Adversarial learning based node-edge graph attention networks for autism spectrum disorder identification. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [7] Zhiyi Chen, Xingwang Hu, Qi Chen, and Tingyong Feng. Altered structural and functional brain network overall organization predict human intertemporal decision-making. _Human Brain Mapping_, 40(1):306-328, 2019.
* [8] Brett W Fling, Youngbin Kwak, Scott J Peltier, and Rachael D Seidler. Differential relationships between transcallosal structural and functional connectivity in young and older adults. _Neurobiology of aging_, 33(10):2521-2526, 2012.
* [9] Aina Frau-Pascual, Jean Augustinack, Divya Varadarajan, Anastasia Yendiki, Bruce Fischl, and Iman Aganj. Detecting structural brain connectivity differences in dementia through a conductance model. In _2019 53rd Asilomar conference on signals, systems, and computers_, pages 591-595. IEEE, 2019.
* [10] Hao Guo, Fan Zhang, Junjie Chen, Yong Xu, and Jie Xiang. Machine learning classification combining multiple features of a hyper-network of fmri data in alzheimer's disease. _Frontiers in neuroscience_, 11:615, 2017.
* [11] Man Guo, Tiancheng Wang, Zhe Zhang, Nan Chen, Yongchao Li, Yin Wang, Zhijun Yao, and Bin Hu. Diagnosis of major depressive disorder using whole-brain effective connectivity networks derived from resting-state functional mri. _Journal of Neural Engineering_, 17(5):056038, 2020.
* [12] Christopher J Honey, Olaf Sporns, Leila Cammoun, Xavier Gigandet, Jean-Philippe Thiran, Reto Meuli, and Patric Hagmann. Predicting human resting-state functional connectivity from structural connectivity. _Proceedings of the National Academy of Sciences_, 106(6):2035-2040, 2009.
* [13] Wenxing Hu, Xianghe Meng, Yuntong Bai, Aijing Zhang, Gang Qu, Biao Cai, Gemeng Zhang, Tony W Wilson, Julia M Stephen, Vince D Calhoun, et al. Interpretable multimodal fusion networks reveal mechanisms of brain cognition. _IEEE transactions on medical imaging_, 40(5):1474-1483, 2021.
* [14] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In _International conference on machine learning_, pages 4651-4664. PMLR, 2021.

* [15] Mimansa Jaiswal and Emily Mower Provost. Privacy enhanced multimodal neural representations for emotion recognition. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 7985-7993, 2020.
* [16] Biao Jie, Mingxia Liu, Daoqiang Zhang, and Dinggang Shen. Sub-network kernels for measuring similarity of brain connectivity networks in disease diagnosis. _IEEE Transactions on Image Processing_, 27(5):2340-2353, 2018.
* [17] Youyong Kong, Shuwen Gao, Yingying Yue, Zhenhua Hou, Huazhong Shu, Chunming Xie, Zhijun Zhang, and Yonggui Yuan. Spatio-temporal graph convolutional network for diagnosis and treatment response prediction of major depressive disorder from functional connectivity. _Human brain mapping_, 42(12):3922-3933, 2021.
* [18] Youyong Kong, Wenhan Wang, Xiaoyun Liu, Shuwen Gao, Zhenghua Hou, Chunming Xie, Zhijun Zhang, and Yonggui Yuan. Multi-connectivity representation learning network for major depressive disorder diagnosis. _IEEE Transactions on Medical Imaging_, 2023.
* [19] Tommaso Lanciano, Francesco Bonchi, and Aristides Gionis. Explainable classification of brain networks via contrast subgraphs. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 3308-3318, 2020.
* [20] Jingcong Li, Fei Wang, Jiahui Pan, and Zhenfu Wen. Identification of autism spectrum disorder with functional graph discriminative network. _Frontiers in Neuroscience_, page 1282, 2021.
* [21] Xiaoxiao Li, Yuan Zhou, Nicha Dvornek, Muhan Zhang, Siyuan Gao, Juntang Zhuang, Dustin Scheinost, Lawrence H Staib, Pamela Ventola, and James S Duncan. Braingnn: Interpretable brain graph neural network for fmri analysis. _Medical Image Analysis_, 74:102233, 2021.
* [22] Sol Lim, Filippo Radicchi, Martijn P van den Heuvel, and Olaf Sporns. Discordant attributes of structural and functional brain connectivity in a two-layer multiplex network. _Scientific Reports_, 9(1):2885, 2019.
* [23] Manhua Liu, Danni Cheng, Kundong Wang, Yaping Wang, and Alzheimer's Disease Neuroimaging Initiative. Multi-modality cascaded convolutional neural networks for alzheimer's disease diagnosis. _Neuroinformatics_, 16:295-308, 2018.
* [24] Vasileios Mantzavinos and Athanasios Alexiou. Biomarkers for alzheimer's disease diagnosis. _Current Alzheimer Research_, 14(11):1149-1154, 2017.
* [25] Changping Meng, S Chandra Mouli, Bruno Ribeiro, and Jennifer Neville. Subgraph pattern neural networks for high-order graph evolution prediction. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [26] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. _Advances in Neural Information Processing Systems_, 34:14200-14213, 2021.
* [27] Jonathan D Power, Alexander L Cohen, Steven M Nelson, Gagan S Wig, Kelly Anne Barnes, Jessica A Church, Alecia C Vogel, Timothy O Laumann, Fran M Miezin, Bradley L Schlaggar, et al. Functional network organization of the human brain. _Neuron_, 72(4):665-678, 2011.
* [28] Jonathan D Power and Steven E Petersen. Control-related systems in the human brain. _Current opinion in neurobiology_, 23(2):223-228, 2013.
* [29] Gang Qu, Li Xiao, Wenxing Hu, Junqi Wang, Kun Zhang, Vince D Calhoun, and Yu-Ping Wang. Ensemble manifold regularized multi-modal graph convolutional network for cognitive ability prediction. _IEEE Transactions on Biomedical Engineering_, 68(12):3564-3573, 2021.
* [30] Blake A Richards, Timothy P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal Bogacz, Amelia Christensen, Claudia Clopath, Rui Ponte Costa, Archy de Berker, Surya Ganguli, et al. A deep learning framework for neuroscience. _Nature neuroscience_, 22(11):1761-1770, 2019.
* [31] Mikail Rubinov and Olaf Sporns. Complex network measures of brain connectivity: uses and interpretations. _Neuroimage_, 52(3):1059-1069, 2010.

* [32] Isaac Sebenius, Alexander Campbell, Sarah E Morgan, Edward T Bullmore, and Pietro Lio. Multimodal graph coarsening for interpretable, mri-based brain graph neural network. In _2021 IEEE 31st International Workshop on Machine Learning for Signal Processing (MLSP)_, pages 1-6. IEEE, 2021.
* [33] Mohammed Senoussaoui, Milton Sarria-Paja, Joao F Santos, and Tiago H Falk. Model fusion for multimodal depression classification and level detection. In _Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge_, pages 57-63, 2014.
* [34] Preya Shah, Arian Ashourvan, Fadi Mikhail, Adam Pines, Lohith Kini, Kelly Oechsel, Sandhitsu R Das, Joel M Stein, Russell T Shinohara, Danielle S Bassett, et al. Characterizing the role of the structural connectome in seizure dynamics. _Brain_, 142(7):1955-1972, 2019.
* [35] Xiao-Wei Song, Zhang-Ye Dong, Xiang-Yu Long, Su-Fang Li, Xi-Nian Zuo, Chao-Zhe Zhu, Yong He, Chao-Gan Yan, and Yu-Feng Zang. Rest: a toolkit for resting-state functional magnetic resonance imaging data processing. _PloS one_, 6(9):e25031, 2011.
* [36] Olaf Sporns, Christopher J Honey, and Rolf Kotter. Identification and classification of hubs in brain networks. _PloS one_, 2(10):e1049, 2007.
* [37] Laura E Suarez, Ross D Markello, Richard F Betzel, and Bratislav Misic. Linking structure and function in macroscale brain networks. _Trends in cognitive sciences_, 24(4):302-315, 2020.
* [38] Heung-Il Suk, Seong-Whan Lee, Dinggang Shen, Alzheimer's Disease Neuroimaging Initiative, et al. Hierarchical feature representation and multimodal fusion with deep learning for ad/mci diagnosis. _NeuroImage_, 101:569-582, 2014.
* [39] Qingyun Sun, Hao Peng, Jianxin Li, Jia Wu, Yuanxing Ning, Philip S. Yu, and Lifang He. SUGAR: subgraph neural network with reinforcement pooling and self-supervised mutual information mechanism. _CoRR_, abs/2101.08170, 2021.
* [40] Jewell B Thomas, Matthew R Brier, Mario Ortega, Tammie L Benzinger, and Beau M Ances. Weighted brain networks in disease: centrality and entropy in human immunodeficiency virus and aging. _Neurobiology of aging_, 36(1):401-412, 2015.
* [41] Lixia Tian, Jinhui Wang, Chaogan Yan, and Yong He. Hemisphere-and gender-related differences in small-world brain networks: a resting-state functional mri study. _Neuroimage_, 54(1):191-202, 2011.
* [42] Martijn P van den Heuvel, Rene CW Mandl, Cornelis J Stam, Rene S Kahn, and Hilleke E Hulshoff Pol. Aberrant frontal and temporal complex network structure in schizophrenia: a graph theoretical analysis. _Journal of Neuroscience_, 30(47):15915-15926, 2010.
* [43] David C Van Essen, Stephen M Smith, Deanna M Barch, Timothy EJ Behrens, Essa Yacoub, Kamil Ugurbil, Wu-Minn HCP Consortium, et al. The wu-minh human connectome project: an overview. _Neuroimage_, 80:62-79, 2013.
* [44] Laura S Van Velzen, Sinead Kelly, Dmitry Isaev, Andre Aleman, Lyubomir I Aftanas, Jochen Bauer, Bernhard T Baune, Ivan V Brak, Angela Carballedo, Colm G Connolly, et al. White matter disturbances in major depressive disorder: a coordinated analysis across 20 international cohorts in the enigma mdd working group. _Molecular psychiatry_, 25(7):1511-1525, 2020.
* [45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [46] Joshua T Vogelstein, William Gray Roncal, R Jacob Vogelstein, and Carey E Priebe. Graph classification using signal-subgraphs: Applications in statistical connectomics. _IEEE transactions on pattern analysis and machine intelligence_, 35(7):1539-1551, 2012.
* [47] Nina Vogt. Machine learning in neuroscience. _Nature Methods_, 15(1):33-33, 2018.
* [48] Bo Wang, Aziz M Mezlini, Feyyaz Demir, Marc Fiume, Zhuowen Tu, Michael Brudno, Benjamin Haibe-Kains, and Anna Goldenberg. Similarity network fusion for aggregating data types on a genomic scale. _Nature methods_, 11(3):333-337, 2014.

* [49] Lu Wang, Feng Vankee Lin, Martin Cole, and Zhengwu Zhang. Learning clique subgraphs in structural brain network classification with application to crystallized cognition. _NeuroImage_, 225:117493, 2021.
* [50] Asiri Wijesinghe and Qing Wang. A new perspective on" how graph neural networks go beyond weisfeiler-lehman?". In _International Conference on Learning Representations_, 2022.
* [51] Qiong Wu, Xiaoqi Huang, Adam J Culbreth, James A Waltz, L Elliot Hong, and Shuo Chen. Extracting brain disease-related connectome subgraphs by adaptive dense subgraph discovery. _Biometrics_, 78(4):1566-1578, 2022.
* [52] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00826_, 2018.
* [53] Dogucan Yaman, Fevziye Irem Eyidokur, and Hazim Kemal Ekenel. Multimodal age and gender classification using car and profile face images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 0-0, 2019.
* [54] Chao-Gan Yan, Xiao Chen, Le Li, Francisco Xavier Castellanos, Tong-Jian Bai, Qi-Jing Bo, Jun Cao, Guan-Mao Chen, Ning-Xuan Chen, Wei Chen, et al. Reduced default mode network functional connectivity in patients with recurrent major depressive disorder. _Proceedings of the National Academy of Sciences_, 116(18):9078-9083, 2019.
* [55] Haoteng Yin, Muhan Zhang, Yanbang Wang, Jianguo Wang, and Pan Li. Algorithm and system co-design for efficient subgraph-based graph representation learning. _arXiv preprint arXiv:2202.13538_, 2022.
* [56] Usman Zahid, Imran Ashraf, Muhammad Attique Khan, Majed Alhaisoni, Khawaja M Yahya, Hany S Hussein, and Hammam Alshazly. Brainnet: optimal deep learning feature fusion for brain tumor classification. _Computational Intelligence and Neuroscience_, 2022, 2022.
* [57] Lu Zhang, Li Wang, Jean Gao, Shannon L Risacher, Jingwen Yan, Gang Li, Tianming Liu, Dajiang Zhu, Alzheimer's Disease Neuroimaging Initiative, et al. Deep fusion of brain structure-function in mild cognitive impairment. _Medical image analysis_, 72:102082, 2021.
* [58] Tao Zhang and Mingyang Shi. Multi-modal neuroimaging feature fusion for diagnosis of alzheimer's disease. _Journal of Neuroscience Methods_, 341:108795, 2020.
* [59] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. _arXiv preprint arXiv:2110.03753_, 2021.
* [60] Lulin Zheng, Ruidong Yang, Junbo Gao, Jun Chen, Jianzhong Liu, and Depeng Li. Quartz rb-sr isochron ages of two type orebodies from the nibao carlin-type gold deposit, guizhou, china. _Minerals_, 9(7):399, 2019.
* [61] Guangming Zhu, Bin Jiang, Liz Tong, Yuan Xie, Greg Zaharchuk, and Max Wintermark. Applications of deep learning to neuro-imaging techniques. _Frontiers in neurology_, 10:869, 2019.
* [62] Qi Zhu, Jing Yang, Shuihua Wang, Daoqiang Zhang, and Zheng Zhang. Multi-modal non-euclidean brain network analysis with community detection and convolutional autoencoder. _IEEE Transactions on Emerging Topics in Computational Intelligence_, 2022.

## Appendix A: Full Results of Comparison Experiments

Due to the layout of the text and page limitations, only the experimental results for the ACC metric are presented in the main text. However, to ensure a comprehensive presentation of the results, the full results of the comparison experiments involving all four datasets, HCP, Zhongda Hospital, Xinxiang Hospital and Two-site, are presented in Tables 1 to 4. Four tables are presented detailing the mean and standard deviation values for each evaluation metric. The most exceptional results are shown in bold, while results below the optimal threshold are underlined for clarity and emphasis.

**Analysis.** Based on the results from the four tables, our RH-BrainFS method achieved excellent results in all four datasets. Specifically, we achieved the best results on three metrics in the HCP dataset, four metrics in both the Zhongda and Xinxiang Hospital datasets, and an impressive five metrics in the Two-site dataset. This demonstrates the strong performance of RH-BrainFS in effectively completing the fusion classification task for multimodal brain networks. Furthermore, we observe that on the hospital dataset, our RH-BrainFS method significantly outperforms most other methods in terms of bias in each metric, indicating higher stability. This improved stability contributes to the reliability and robustness of RH-BrainFS in fusion classification tasks.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Modality} & \multicolumn{5}{c}{Zhongda Dataset} \\ \cline{3-6}  & & ACC & SEN & SPE & F1 & AUC \\ \hline FGDN & FC & 65.67\(\pm\)3.26 & 78.31\(\pm\)8.26 & 49.25\(\pm\)12.11 & 70.10\(\pm\)4.21 & 63.78\(\pm\)3.86 \\ FGDN & SC & 64.02\(\pm\)3.49 & 65.67\(\pm\)14.37 & 61.50\(\pm\)16.65 & 60.67\(\pm\)8.24 & 63.58\(\pm\)3.56 \\ BrainGNN & FC & 69.18\(\pm\)3.39 & 73.10\(\pm\)5.23 & 64.05\(\pm\)5.00 & 72.05\(\pm\)3.61 & 68.57\(\pm\)3.46 \\ BrainGNN & SC & 70.73\(\pm\)2.07 & 75.93\(\pm\)3.93 & 64.10\(\pm\)3.58 & 73.81\(\pm\)2.33 & 70.01\(\pm\)2.04 \\ \hline SVM & SC,FC & 63.21\(\pm\)2.09 & 74.17\(\pm\)2.06 & 49.00\(\pm\)3.22 & 68.87\(\pm\)2.07 & 61.58\(\pm\)2.20 \\ Random Forest & SC,FC & 61.45\(\pm\)2.80 & 86.69\(\pm\)2.78 & 29.00\(\pm\)4.28 & 71.53\(\pm\)2.17 & 57.85\(\pm\)2.96 \\ MGCN & SC,FC & 75.18\(\pm\)2.34 & 88.52\(\pm\)3.99 & 57.10\(\pm\)0.86 & 80.15\(\pm\)1.91 & 72.81\(\pm\)2.96 \\ GBDM & SC,FC & 74.81\(\pm\)2.44 & 81.77\(\pm\)2.51 & 58.14\(\pm\)4.37 & 74.92\(\pm\)3.20 & 69.96\(\pm\)3.11 \\ MMGNN & SC,FC & 60.69\(\pm\)3.61 & 70.35\(\pm\)9.19 & 47.90\(\pm\)9.32 & 64.45\(\pm\)4.68 & 59.12\(\pm\)5.05 \\ AL-NEGAT & SC,FC & 73.95\(\pm\)3.45 & **90.71\(\pm\)7.24** & 52.05\(\pm\)5.69 & 79.00\(\pm\)4.79 & 71.38\(\pm\)3.54 \\ \hline RH-BrainFS (ours) & SC,FC & **80.64\(\pm\)1.58** & 90.05\(\pm\)5.58 & **68.45\(\pm\)9.32** & **83.96\(\pm\)1.13** & **79.25\(\pm\)2.24** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison experiments results on the Zhongda hospital dataset.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Modality} & \multicolumn{5}{c}{HCP Dataset} \\ \cline{3-6}  & & ACC & SEN & SPE & F1 & AUC \\ \hline FGDN & FC & 67.56\(\pm\)3.02 & 52.37\(\pm\)12.55 & 80.54\(\pm\)8.24 & 59.09\(\pm\)6.98 & 66.45\(\pm\)3.49 \\ FGDN & SC & 63.42\(\pm\)4.79 & 53.98\(\pm\)20.69 & 71.43\(\pm\)13.72 & 55.12\(\pm\)14.65 & 62.71\(\pm\)5.57 \\ BrainGNN & FC & 66.41\(\pm\)6.44 & 68.92\(\pm\)6.92 & 63.66\(\pm\)12.30 & 65.41\(\pm\)5.15 & 66.29\(\pm\)6.04 \\ BrainGNN & SC & 67.37\(\pm\)5.89 & 68.39\(\pm\)6.07 & 66.17\(\pm\)8.50 & 65.88\(\pm\)4.45 & 67.28\(\pm\)5.80 \\ \hline SVM & SC,FC & 74.49\(\pm\)2.97 & 71.18\(\pm\)4.95 & 77.32\(\pm\)3.30 & 71.96\(\pm\)3.49 & 74.25\(\pm\)3.05 \\ Random Forest & SC,FC & 68.24\(\pm\)2.94 & 54.88\(\pm\)5.78 & 79.64\(\pm\)4.16 & 61.31\(\pm\)4.44 & 67.26\(\pm\)3.04 \\ MGCN & SC,FC & 67.94\(\pm\)5.41 & 74.53\(\pm\)2.80 & 62.32\(\pm\)7.76 & 68.10\(\pm\)5.59 & 68.42\(\pm\)5.41 \\ GBDM & SC,FC & 71.02\(\pm\)4.39 & 61.95\(\pm\)2.37 & 79.18\(\pm\)8.58 & 65.76\(\pm\)3.65 & 70.56\(\pm\)4.32 \\ MMGNN & SC,FC & 73.33\(\pm\)3.82 & 71.17\(\pm\)4.52 & 75.17\(\pm\)5.67 & 71.10\(\pm\)2.88 & 73.17\(\pm\)2.72 \\ AL-NEGAT & SC,FC & 75.12\(\pm\)3.66 & 72.86\(\pm\)7.74 & **84.46\(\pm\)5.05** & 76.13\(\pm\)4.70 & **78.66\(\pm\)3.81** \\ \hline RH-BrainFS (ours) & SC,FC & **78.63\(\pm\)4.36** & **75.59\(\pm\)6.75** & 81.25\(\pm\)6.04 & **76.49\(\pm\)4.91** & 78.42\(\pm\)4.38 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison experiments results on the HCP dataset.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Modality} & \multicolumn{5}{c}{Two-site Dataset} \\ \cline{3-7}  & & ACC & SEN & SPE & F1 & AUC \\ \hline FGDN & FC & 59.34\(\pm\)2.78 & 54.70\(\pm\)10.82 & 63.79\(\pm\)8.83 & 49.96\(\pm\)8.04 & 59.24\(\pm\)2.85 \\ FGDN & SC & 68.91\(\pm\)2.53 & 70.02\(\pm\)12.42 & 67.83\(\pm\)9.78 & 66.31\(\pm\)7.03 & 68.93\(\pm\)2.52 \\ BrainGNN & FC & 69.55\(\pm\)3.23 & 69.62\(\pm\)5.23 & 68.36\(\pm\)6.67 & 67.54\(\pm\)3.34 & 68.99\(\pm\)3.22 \\ BrainGNN & SC & 69.51\(\pm\)2.58 & 68.12\(\pm\)5.41 & 69.24\(\pm\)6.66 & 66.76\(\pm\)3.42 & 68.68\(\pm\)3.03 \\ \hline SVM & SC,FC & 66.06\(\pm\)1.56 & 58.01\(\pm\)2.49 & 74.04\(\pm\)1.77 & 62.03\(\pm\)2.27 & 66.03\(\pm\)1.56 \\ Random Forest & SC,FC & 62.43\(\pm\)2.19 & 56.17\(\pm\)2.87 & 68.60\(\pm\)3.48 & 59.04\(\pm\)2.73 & 62.38\(\pm\)2.20 \\ MGCN & SC,FC & 72.98\(\pm\)2.17 & 74.02\(\pm\)7.03 & 73.74\(\pm\)4.05 & 72.45\(\pm\)4.48 & 73.88\(\pm\)2.22 \\ GBDM & SC,FC & 72.48\(\pm\)1.91 & 71.53\(\pm\)7.55 & 65.97\(\pm\)5.61 & 68.74\(\pm\)4.92 & 68.75\(\pm\)2.90 \\ MMGNN & SC,FC & 59.72\(\pm\)3.18 & 65.10\(\pm\)4.83 & 54.42\(\pm\)4.42 & 59.96\(\pm\)4.02 & 59.76\(\pm\)3.15 \\ AL-NEGAT & SC,FC & 71.86\(\pm\)2.49 & 75.26\(\pm\)3.62 & 68.12\(\pm\)6.19 & 72.16\(\pm\)2.24 & 71.69\(\pm\)2.56 \\ \hline RH-BrainFS (ours) & SC,FC & **78.48\(\pm\)1.43** & **76.20\(\pm\)4.06** & **80.72\(\pm\)3.60** & **77.35\(\pm\)1.97** & **78.46\(\pm\)1.43** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison experiments results on the Two-site dataset.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Modality} & \multicolumn{5}{c}{Xinxiang Dataset} \\ \cline{3-7}  & & ACC & SEN & SPE & F1 & AUC \\ \hline FGDN & FC & 67.91\(\pm\)3.27 & 43.58\(\pm\)8.18 & 84.50\(\pm\)8.30 & 42.03\(\pm\)6.59 & 64.04\(\pm\)2.88 \\ FGDN & SC & 65.89\(\pm\)5.15 & 61.83\(\pm\)14.81 & 68.85\(\pm\)15.06 & 53.03\(\pm\)8.16 & 65.34\(\pm\)3.82 \\ BrainGNN & FC & 73.46\(\pm\)4.33 & 54.92\(\pm\)10.57 & 86.25\(\pm\)6.10 & 55.72\(\pm\)9.30 & 70.58\(\pm\)4.88 \\ BrainGNN & SC & 73.66\(\pm\)3.60 & 56.83\(\pm\)10.76 & 85.35\(\pm\)8.26 & 57.39\(\pm\)8.58 & 71.09\(\pm\)3.76 \\ \hline SVM & SC,FC & 71.73\(\pm\)1.99 & 49.92\(\pm\)4.35 & 86.55\(\pm\)3.42 & 54.52\(\pm\)4.72 & 68.23\(\pm\)2.19 \\ Random Forest & SC,FC & 62.78\(\pm\)1.63 & 12.08\(\pm\)4.81 & **97.20\(\pm\)1.96** & 17.03\(\pm\)6.61 & 54.64\(\pm\)2.11 \\ MGCN & SC,FC & 82.24\(\pm\)3.71 & 74.16\(\pm\)6.62 & 87.45\(\pm\)3.45 & 76.15\(\pm\)5.88 & 80.80\(\pm\)4.15 \\ GBDM & SC,FC & 80.71\(\pm\)2.83 & 60.20\(\pm\)10.03 & 88.43\(\pm\)5.86 & 63.46\(\pm\)9.11 & 73.31\(\pm\)5.83 \\ MMGNN & SC,FC & 68.21\(\pm\)4.44 & 57.33\(\pm\)3.56 & 75.10\(\pm\)8.12 & 55.48\(\pm\)4.03 & 66.21\(\pm\)4.06 \\ AL-NEGAT & SC,FC & 75.75\(\pm\)3.81 & 56.42\(\pm\)14.80 & 88.50\(\pm\)6.35 & 61.19\(\pm\)10.80 & 72.46\(\pm\)5.34 \\ \hline RH-BrainFS (ours) & SC,FC & **90.27\(\pm\)2.00** & **80.75\(\pm\)5.86** & 96.65\(\pm\)2.34 & **85.43\(\pm\)3.90** & **88.70\(\pm\)2.48** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison experiments results on the Xinxiang hospital dataset.

[MISSING_PAGE_EMPTY:17]

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \multirow{2}{*}{k} & \multicolumn{5}{c}{Two-site Dataset} \\ \cline{2-6}  & ACC & SEN & SPE & F1 & AUC \\ \hline
1 & **78.48\(\pm\)1.43** & **76.20\(\pm\)4.06** & **80.72\(\pm\)3.60** & **77.35\(\pm\)1.97** & **78.46\(\pm\)1.43** \\
2 & 73.99\(\pm\)1.32 & 69.49\(\pm\)2.88 & 78.42\(\pm\)3.04 & 71.53\(\pm\)2.04 & 73.96\(\pm\)1.32 \\
3 & 74.37\(\pm\)1.81 & 72.47\(\pm\)5.79 & 76.26\(\pm\)3.42 & 72.66\(\pm\)3.37 & 74.36\(\pm\)1.79 \\
4 & 74.55\(\pm\)1.64 & 73.13\(\pm\)6.88 & 75.92\(\pm\)6.98 & 72.96\(\pm\)2.68 & 74.53\(\pm\)1.63 \\
5 & 74.14\(\pm\)1.25 & 70.41\(\pm\)5.73 & 77.79\(\pm\)4.62 & 71.70\(\pm\)3.15 & 74.10\(\pm\)1.28 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Hyperparameter experiments of varying subgraph sampling hops on Two-site dataset.