# Anytime Model Selection in Linear Bandits

Parnian Kassraie\({}^{1}\) Nicolas Emmenegger\({}^{1}\) Andreas Krause\({}^{1}\) Aldo Pacchiano\({}^{2,3}\)

\({}^{1}\)ETH Zurich \({}^{2}\)Broad Institute of MIT and Harvard \({}^{3}\)Boston University

{pkassraie, nicolaem, krausea}@ethz.ch pacchian@bu.edu

###### Abstract

Model selection in the context of bandit optimization is a challenging problem, as it requires balancing exploration and exploitation not only for action selection, but also for model selection. One natural approach is to rely on online learning algorithms that treat different models as experts. Existing methods, however, scale poorly (\(\mathrm{poly}M\)) with the number of models \(M\) in terms of their regret. Our key insight is that, for model selection in linear bandits, we can emulate full-information feedback to the online learner with a favorable bias-variance trade-off. This allows us to develop ALExp, which has an exponentially improved (\(\log M\)) dependence on \(M\) for its regret. ALExp has anytime guarantees on its regret, and neither requires knowledge of the horizon \(n\), nor relies on an initial purely exploratory stage. Our approach utilizes a novel time-uniform analysis of the Lasso, establishing a new connection between online learning and high-dimensional statistics.

## 1 Introduction

When solving bandit problems or performing Bayesian optimization, we need to commit to a reward model _a priori_, based on which we estimate the reward function and build a policy for selecting the next action. In practice, there are many ways to model the reward by considering different feature maps or hypothesis spaces, e.g., for optimizing gene knockouts (Gonzalez et al., 2015; Pacchiano et al., 2022) or parameter estimation in biological dynamical systems (Ulmasov et al., 2016; Imani et al., 2019). It is not known a priori which model is going to yield the most sample efficient bandit algorithm, and we can only select the right model as we gather empirical evidence. This leads us to ask, can we perform adaptive model selection, while simultaneously optimizing for reward?

In an idealized setting with no sampling limits, given a model class of size \(M\), we could initialize \(M\) bandit algorithms (a.k.a agents) in parallel, each using one of the available reward models. Then, as the algorithms run, at every step we can select the most promising agent, according to the cumulative rewards that are obtained so far. Model selection can then be cast into an online optimization problem on a \(M\)-dimensional probability simplex, where the probability of selecting an agent is dependent on its cumulative reward, and the optimizer seeks to find the distribution with the best return in hindsight. This approach is impractical for large model classes, since at every step, it requires drawing \(M\) different samples in parallel from the environment so that the reward for each agent is realized.

In realistic applications of bandit optimization, we can only draw _one_ sample at a time, and so we need to design an algorithm which allocates more samples to the agents that are deemed more promising. Prior work (e.g., Maillard and Munos, 2011; Agarwal et al., 2017) propose to run a single meta algorithm which interacts with the environment by first selecting an agent, and then selecting an action according to the suggestion of that agent. The online model selection problem can still be emulated in this setup, however this time the optimizer receives partial feedback, coming from only one agent. Consequently, many agents need to be queried, and the overall regret scales with \(\mathrm{poly}M\), again restricting this approach to small model classes. In fact, addressing the limited scope of such algorithms, Agarwal et al. (2017) raise an open problem on the feasibility of obtaining a \(\log M\) dependency for the regret.

We show that this rate is achievable, in the particular case of linearly parametrizable rewards. We develop a technique to "hallucinate" the reward for every agent that was not selected, and run the online optimizer with emulated full-information feedback. This allows the optimizer to assess the quality of the agents, without ever having queried them. As a result, our algorithm ALExp, satisfies a regret of rate \(\mathcal{O}(\max\{\sqrt{n\log^{3}M},n^{3/4}\sqrt{\log M}\})\), with high probability, simultaneously for all \(n\geq 1\) (Theorem 1). Our key idea, leading to \(\log M\) dependency, is to employ the Lasso as a low-variance online regression oracle, and estimate the reward for the agents that were not chosen. This trick is made possible through our novel time-uniform analysis of online Lasso regression (Theorem 3). Consequently, ALExp is horizon-independent, and explores adaptively without requiring an initial exploration stage. Empirically we find that ALExp consistently outperforms prior work across a range of environments.

## 2 Related Work

Online Model selection (MS) for bandits considers combining a number of agents in a master algorithm, with the goal of performing as well as the best agent (Maillard and Munos, 2011; Agarwal et al., 2017; Pacchiano et al., 2020; Luo et al., 2022). This literature operates on black-box model classes of size \(M\) and uses variants of Online Mirror Descent (OMD) to sequentially select the agents. The optimizer operates on importance-weighted estimates of the agents' rewards, which has high variance (\(\mathrm{poly}M\)) and is non-zero only for the selected agent. Effectively, the optimizer receives partial (a.k.a. bandit) feedback and agents are at risk of starvation, since at every step only the selected agent gets the new data point. These works assume knowledge of the horizon, however, we suspect that this may be lifted with a finer analysis of OMD.

Sparse linear bandits use sparsity-inducing methods, often Lasso (Tibshirani, 1996), for estimating the reward in presence of many features, as an alternative to model selection. Early results are in the data-rich regime where the stopping time \(n\) is known and larger than the number of models, and some suffer from \(\mathrm{poly}M\) regret dependency (e.g., Abbasi-Yadkori et al., 2012). Recent efforts often consider the contextual case, where at every step only a finite stochastic subset of the action domain is presented to the agent, and that the distribution of these points is i.i.d. and sufficiently diverse (Li et al., 2022; Bastani and Bayati, 2020; Kim and Paik, 2019; Oh et al., 2021; Cella and Pontil, 2021). We do not rely on such assumptions. Most sparse bandit algorithms either start with a purely exploratory phase (Kim and Paik, 2019; Li et al., 2022; Hao et al., 2020; Jang et al., 2022), or rely on a priori scheduled exploration (Bastani and Bayati, 2020). The exploration budget is set according to the horizon \(n\). Therefore, such algorithms inherently require the knowledge of \(n\) and can be made anytime only via the doubling trick (Auer et al., 1995). Table 2 presents an in-depth overview.

ALExp inherits the best of both worlds (Table 1): its regret enjoys the \(\log M\) dependency of sparse linear bandits even on compact domains, and it has adaptive probabilistic exploration with anytime guarantees. In contrast to prior literature, we perform model selection with an online optimizer (EXP4), which hallucinates full-information feedback using a low-variance Lasso estimator instead of importance-weighted estimates. Moreover, our anytime approach lifts the horizon dependence and the exploration requirement of sparse linear bandits.

Our work is inspired by and contributes to the field of Learning with Expert Advice, which analyzes incorporating the advice of \(M\) oblivious (non-adaptive) experts, with bandit or full-information feedback (Haussler et al., 1998; Auer et al., 2002; McMahan and Streeter, 2009). The idea of employing an online optimizer for learning stems from this literature, and has been used in various applications of online learning (Foster et al., 2017; Singla et al., 2018; Muthukumar et al., 2019; Karimi et al., 2021; Liu et al., 2022). In particular, we are inspired by Foster and Rakhlin (2020) and Moradipari et al. (2022), who apply EXP4 to least squares estimates, for arm selection in \(K\)-armed

\begin{table}
\begin{tabular}{c|c|c|c|c|c}  & MS & \multirow{2}{*}{Regret} & MS & adaptive & \multirow{2}{*}{anytime} \\  & Technique & & Guarantee & exploration & \\ \hline Sparse Linear & \multirow{2}{*}{Lasso} & \(\log M\) & ✗ & ✗ & ✗ \\ Bandits & & & & & \\ \hline MS for Black- & \multirow{2}{*}{OMD with} & \multirow{2}{*}{\(\mathrm{poly}M\)} & \multirow{2}{*}{✓} & \multirow{2}{*}{✗} & \multirow{2}{*}{✗} \\ Box Bandits & bandit feedback & & & & \\ \hline MS for Linear & \multirow{2}{*}{EXP4 with} & \multirow{2}{*}{\(\log M\)} & \multirow{2}{*}{✓} & \multirow{2}{*}{✓} & \multirow{2}{*}{✓} \\ Bandits (Ours) & full-information & & & \\ \end{tabular}
\end{table}
Table 1: Overview of literature on online model selection for bandit optimization contextual bandits. However, their algorithms are not anytime, and due to the choice of estimator, the corresponding regret scales with \(\mathcal{O}(\min(\sqrt{M},\sqrt{K\log M}))\).

## 3 Problem Setting

We consider a bandit problem where a learner interacts with the environment in rounds. At step \(t\) the learner selects an action \(\bm{x}_{t}\in\mathcal{X}\), where \(\mathcal{X}\subset\mathbb{R}^{d_{0}}\) is a compact domain and observes a noisy reward \(y_{t}=r(\bm{x}_{t})+\varepsilon_{t}\) such that \(\varepsilon_{t}\) is an i.i.d. zero-mean sub-Gaussian variable with parameter \(\sigma^{2}\). We assume the reward function \(r:\mathcal{X}\to\mathbb{R}\) is linearly parametrizable by some unknown feature map, and that the model class \(\{\bm{\phi}_{j}:\mathbb{R}^{d_{0}}\to\mathbb{R}^{d},j=1,\ldots,M\}\) contains the set of plausible feature maps. We consider the setting where \(M\) can be very large, and while the set \(\{\bm{\phi}_{j}\}\) may include misspecified feature maps, it contains at least one feature map that represents the reward function, i.e., there exists \(j^{\star}\in[M]\) such that \(r(\cdot)=\bm{\theta}_{j}^{\top}\bm{\phi}_{j^{\star}}(\cdot)\). We assume the image of \(\bm{\phi}_{j}\) spans \(\mathbb{R}^{d}\), and no two feature maps are linearly dependent, i.e. for any \(j,j^{\prime}\in[M]\), there exists no \(\alpha\in\mathbb{R}\) such that \(\bm{\phi}_{j}(\cdot)=\alpha\bm{\phi}_{j^{\prime}}(\cdot)\). This assumption, which is satisfied by design in practice, ensures that the features are not ill-posed and we can explore in all relevant directions. We assume that the concatenated feature map \(\bm{\phi}(\bm{x})\coloneqq(\bm{\phi}_{1}(\bm{x}),\ldots,\bm{\phi}_{M}(\bm{x}))\) is normalized \(\|\bm{\phi}(\bm{x})\|\leq 1\) for all \(\bm{x}\in\mathcal{X}\) and that \(\|\bm{\theta}_{j^{\star}}\|\leq B\), which implies \(|r(\bm{x})|\leq B\) for all \(\bm{x}\in\mathcal{X}\).

We will model this problem in the language of model selection where a meta algorithm aims to optimize the unknown reward function by relying on a number of base learners. In order to interact with the environment the meta algorithm selects an agent that in turn selects an action. In our setting we thinking of each of these \(M\) feature maps as controlled by a base agent running its own algorithm. Base agent \(j\) uses the feature map \(\bm{\phi}_{j}\) for modeling the reward. At step \(t\) of the bandit problem, each agent \(j\) is given access to the full history \(H_{t-1}\coloneqq\{(\bm{x}_{1},y_{1}),\ldots,(\bm{x}_{t-1},y_{t-1})\}\), and uses it to locally estimate the reward as \(\bm{\tilde{\beta}}_{t-1,j}^{\top}\bm{\phi}_{j}(\cdot)\), where \(\bm{\tilde{\beta}}_{t-1,j}\in\mathbb{R}^{d}\) is the estimated coefficients vector. The agent then uses this estimate to develop its action selection policy \(p_{t,j}\in\mathcal{M}(\mathcal{X})\). Here, \(\mathcal{M}\) denotes the space of probability measures defined on \(\mathcal{X}\). The condition on existence of \(j^{\star}\) will ensure that there is at least one agent which is using a correct model for the reward, and thereby can solve the bandit problem if executed in isolation. We refer to agent \(j^{\star}\) as the oracle agent.

Our goal is to find a sample-efficient strategy for iterating over the agents, such that their suggested actions maximize the cumulative reward, achieved over any horizon \(n\geq 1\). This is equivalent to minimizing the cumulative regret \(R(n)=\sum_{t=1}^{n}r(\bm{x}^{*})-r(\bm{x}_{t})\), where \(\bm{x}^{*}\) is a global maximizer of the reward function. We neither fix \(n\), nor assume knowledge of it.

## 4 Method

As warm-up, consider an example with deterministic agents, i.e., when \(p_{t,j}\) is a Dirac measure on a specific action \(\bm{x}_{t,j}\). Suppose it was practically feasible to draw the action suggested by every agent and observe the corresponding reward vector \(\bm{r}_{t}=(r_{t,j})_{j=1}^{M}\). In this case, model selection becomes a full-information online optimization problem, and we can design a minimax optimal algorithm as follows. We assign a probability distribution \(\bm{q}_{t}=(q_{t,j})_{j=1}^{M}\) to the models, and update it such that the overall average return \(\sum_{t=1}^{n}\bm{q}_{t}^{\top}\bm{r}_{t}\) is competitive to the best agent's average return \(\sum_{t=1}^{n}r_{t,j^{\star}}\). At every step, we update \(q_{t+1,j}\propto\exp(\sum_{s=1}^{t}r_{s,j})\), since such exponential weighting is known to lead to an optimal solution for this classic online learning problem (Cesa-Bianchi and Lugosi, 2006). In our setting however, the agents are stochastic, and we do not have access to the full \(\bm{r}_{t}\) vector.

We propose the **A**nytime **Exp**onential weighting algorithm based on **L**asso reward estimates (ALExp), summarized in Algorithm 1. At step \(t\) we first sample an agent \(j_{t}\), and then sample an action \(\bm{x}_{t}\) according to the agent's policy \(p_{t,j_{t}}\). Let \(\Delta_{M}\) denote the \(M\)-dimensional probability simplex. We maintain a probability distribution \(\bm{q}_{t}\in\Delta_{M}\) over the agents, and update it sequentially as we accumulate evidence on the performance of each agent. Ideally, we would have adjusted \(q_{t,j}\) according to the average return of model \(j\), that is, \(\mathbb{E}_{\bm{x}\sim p_{t,j}}r(\bm{x})\). However, since \(r\) is unknown, we estimate the average reward with some \(\hat{r}_{t,j}\). We then update \(\bm{q}_{t}\) for the next step via,

\[q_{t+1,j}=\frac{\exp(\eta_{t}\sum_{s=1}^{t}\hat{r}_{s,j})}{\sum_{i=1}^{M}\exp( \eta_{t}\sum_{s=1}^{t}\hat{r}_{s,i})}\]for all \(j=1,\ldots,M\), where \(\eta_{t}\) is the learning rate, and controls the sensitivity of the updates. This rule allows us to imitate the full-information example that we mentioned above. By utilizing \(\hat{r}_{t,j}\) and hallucinating feedback from all agents, we can reduce the probability of selecting a badly performing agent, without ever having sampled them (c.f. Fig. 4). It remains to design the estimator \(\hat{r}_{t,j}\). We concatenate all feature maps, and, knowing that many features are redundant, use a sparsity inducing estimator over the resulting coefficients vector. Mainly, let \(\bm{\theta}=(\bm{\theta}_{1},\ldots,\bm{\theta}_{M})\in\mathbb{R}^{Md}\) be the concatenated coefficients vector. We then solve

\[\hat{\bm{\theta}}_{t}=\operatorname*{arg\,min}_{\bm{\theta}\in\mathbb{R}^{Md }}\mathcal{L}(\bm{\theta};H_{t},\lambda_{t})=\operatorname*{arg\,min}_{\bm{ \theta}\in\mathbb{R}^{Md}}\frac{1}{t}\norm{\bm{y}_{t}-\Phi_{t}\bm{\theta}}_{2}^ {2}+2\lambda_{t}\sum_{j=1}^{M}\norm{\bm{\theta}_{j}}_{2}\] (1)

where \(\Phi_{t}=[\bm{\phi}^{\top}(\bm{x}_{s})]_{s\leq t}\in\mathbb{R}^{t\times Md}\) is the feature matrix, \(\bm{y}_{t}\in\mathbb{R}^{t}\) is the concatenated reward vector, and \(\lambda_{t}\) is an adaptive regularization parameter. Problem (1) is the online variant of the group Lasso (Lounici et al., 2011). The second term is the loss is the mixed \((2,1)\)-norm of \(\bm{\theta}\), which can be seen as the \(\ell_{1}\)-norm of the vector \((\norm{\bm{\theta}_{1}},\ldots,\norm{\bm{\theta}_{M}})\in\mathbb{R}^{M}\). This norm induces sparsity at the group level, and therefore, the sub-vectors \(\hat{\bm{\theta}}_{t,j}\in\mathbb{R}^{d}\) that correspond to redundant feature maps are expected to be \(\bm{0}\), i.e. the null vector. We then estimate the average return of each model by simply taking an expectation \(\hat{r}_{t,j}=\mathbb{E}_{\bm{x}\sim p_{t+1,j}}[\hat{\bm{\theta}}_{t}^{\top} \bm{\phi}(\bm{x})]\). This quantity is the average return of the agent's policy \(p_{t+1,j}\), according to our Lasso estimator. In Section 5.2 we explain why the particular choice of Lasso is crucial for obtaining a \(\log M\) rate for the regret.

For action selection, with probability \(\gamma_{t}\), we sample agent \(j\) with probability \(q_{t,j}\) and draw \(\bm{x}_{t}\sim p_{t,j}\) as per suggestion of the agent. With probability \(1-\gamma_{t}\), we choose the action according to some exploratory distribution \(\pi\in\mathcal{M}(\mathcal{X})\) which aims to sample informative actions. This can be any design where \(\operatorname{supp}(\pi)=\mathcal{X}\). We mix \(p_{t,j}\) with \(\pi\), to collect sufficiently diverse data for model selection. We are not restricting the agents' policy, and therefore can not rely on them to explore adequately. In Theorem 1, we choose a decreasing sequence of \((\gamma_{t})_{t\geq 1}\) the probabilities of exploration at step \(t\geq 1\), since less exploration will be required as data accumulates. To conclude, the action selection policy of ALExp is formally described as the mixture

\[p_{t}(\bm{x})=\gamma_{t}\pi(\bm{x})+(1-\gamma_{t})\sum_{j=1}^{M}q_{t,j}p_{t,j}( \bm{x}).\]

## 5 Main Results

For the regret guarantee, we consider specific choices of base agents and exploratory distribution. Our analysis may be extended to include other policies, since ALExp can be wrapped around any bandit agent that is described by some \(p_{t,j}\), and allows for random exploration with any distribution \(\pi\).

**Base Agents.** We assume that the oracle agent has either a UCB (Abbasi-Yadkori et al., 2011) or a Greedy(Auer et al., 2002a) policy, and all other agents are free to choose _any arbitrary_ policy. Similar treatment can be applied to cases where the oracle uses other (sublinear) polices for solving linear or contextual bandits (e.g., Thompson, 1933; Kaufmann et al., 2012; Agarwal et al., 2014). In either case, agent \(j^{\star}\) calculates a ridge estimate of the coefficients vector based on the history \(H_{t}\)

\[\hat{\bm{\beta}}_{t,j^{\star}}\coloneqq\operatorname*{arg\,min}_{\bm{\beta} \in\mathbb{R}^{d}}\norm{\bm{y}_{t}-\Phi_{t,j^{\star}}\bm{\beta}}_{2}^{2}+ \tilde{\lambda}\norm{\bm{\beta}}_{2}^{2}=\left(\Phi_{t,j^{\star}}^{\top}\Phi_{t,j^{\star}}+\tilde{\lambda}\bm{I}\right)^{-1}\Phi_{t,j^{\star}}^{\top}\bm{y}_{ t}.\]

Here \(\Phi_{t,j^{\star}}\in\mathbb{R}^{t\times d}\) is the feature matrix, where each row \(s\) is \(\bm{\phi}_{j^{\star}}^{\top}(\bm{x}_{s})\) and \(\tilde{\lambda}\) is the regularization constant. Then at step \(t\), a Greedy oracle suggests the action which maximizes the reward estimate \(\hat{\bm{\beta}}_{t-1,j}^{\top}\bm{\phi}_{j^{\star}}(\bm{x})\), and a UCB oracle queries \(\operatorname*{arg\,max}u_{t-1,j^{\star}}(\bm{x})\) where \(u_{t-1,j^{\star}}(\cdot)\) is the upper confidence bound that this agent calculates for \(r(\cdot)\). Proposition 21 shows that the sequence \((u_{t,j^{\star}})_{t\geq 1}\) is in fact an anytime valid upper bound for \(r\) over the entire domain.

**Exploratory policy.** Performance of ALExp depends on the quality of the samples that \(\pi\) suggests. The eigenvalues of the covariance matrix \(\Sigma(\pi,\bm{\phi})\coloneqq\mathbb{E}_{\bm{x}\sim\pi}\bm{\phi}(\bm{x})\bm{ \phi}^{\top}(\bm{x})\) reflect how diverse the data is, and thus are a good indicator for data quality. van de Geer and Buhlmann (2009) present a survey on the notions of diversity defined based on these eigenvalues. Let \(\lambda_{\min}(A)\) denote the minimium eigenvalue of a matrix \(A\). Similar to Hao et al. (2020), we assume that \(\pi\) is the maximizer of the problem below and present our regret bounds in terms of

\[C_{\min}=C_{\min}(\mathcal{X},\bm{\phi})\coloneqq\max_{\pi\in\mathcal{M}( \mathcal{X})}\lambda_{\min}\left(\Sigma(\pi,\bm{\phi})\right),\] (2)which is greater than zero under the conditions specified in our problem setting. Prior works in the sparse bandit literature all require a similar or stronger assumption of this kind, and Table 2 gives an overview. Alternatively, one can work with an arbitrary \(\pi\), e.g., \(\mathrm{Unif}(\mathcal{X})\), as long as \(\lambda_{\min}(\Sigma(\pi,\bm{\phi}))\) is bounded away from zero. Appendix C.1 reviews some configurations of \((\bm{\phi},\mathcal{X},\pi)\) that lead to a non-zero minimum eigenvalue, and Corollary 12 bounds the regret of ALExp with uniform exploration.

For this choice of agents and exploratory distribution, Theorem 1 presents an informal regret bound. Here, we have used the \(\mathcal{O}\) notation, and only included the fastest growing terms. The inequality is made exact in Theorem 14, up to constant multiplicative factors.

**Theorem 1** (Cumulative Regret of ALExp, Informal).: _Let \(\delta\in(0,1]\) and set \(\pi\) to be the maximizer of (2). Choose learning rate \(\eta_{t}=\mathcal{O}(C_{\min}t^{-1/2}/C(M,\delta,d))\), exploration probability \(\gamma_{t}=\mathcal{O}(t^{-1/4})\) and Lasso regularization parameter \(\lambda_{t}=\mathcal{O}(C_{\min}t^{-1/2}C(M,\delta,d))\), where_

\[C(M,\delta,d)=\mathcal{O}\left(\sqrt{1+\log(M/\delta)+(\log\log d)_{+}}+\sqrt{ d\left(\log(M/\delta)+(\log\log d)_{+}\right)}\right).\]

_Then ALExp satisfies the cumulative regret_

\[R(n)=\mathcal{O}\Big{(}n^{3/4}B+\,n^{3/4}C(M,\delta,d)+ \ C_{\min}^{-1}\sqrt{n}C(M,\delta,d)\log M\] \[+C_{\min}^{-1/2}n^{5/8}\sqrt{d\log\left(n\right)+\log(1/\delta)} \Big{)}\]

_simultaneously for all \(n\geq 1\), with probability greater than \(1-\delta\)._

In this bound, the first term is the regret incurred at exploratory steps (when \(\alpha_{t}=1\)), the second term is due to the estimation error of Lasso (i.e., \(||\bm{\theta}-\bm{\hat{\theta}}_{t}||\)), and the third term is the regret of the exponential weights sub-algorithm. The fourth term, is the regret bound for the oracle agent \(j^{\star}\), when run within the ALExp framework. It does not depend on the agent's policy (greedy or optimistic), and is worse than the minimax optimal rate of \(\sqrt{nd\log n}\). This is because the oracle is suggesting actions based on the history \(H_{t}\), which includes uninformative action-reward pairs queried by other, potentially misspecified, agents. In Corollary 12, we provide a regret bound independent of \(C_{\min}\), for orthogonal feature maps, and show that the same \(\mathcal{O}(\max\{\sqrt{n\log^{3}M},n^{3/4}\sqrt{\log M}\})\) rate may be achieved even with the simple choice \(\pi=\mathrm{Unif}(\mathcal{X})\).

### Proof Sketch

The regret is caused by two sources: selecting a sub-optimal agent, and an agent selecting a sub-optimal action. Accordingly, for any \(j\in{1,\ldots,M}\), we decompose the regret as

\[R(n)=\sum_{t=1}^{n}r(\bm{x}^{\star})-r(\bm{x}_{t})=\Big{(}\sum_{t=1}^{n}r(\bm{x}^ {\star})-r_{t,j}\Big{)}+\Big{(}\sum_{t=1}^{n}r_{t,j}-r(\bm{x}_{t})\Big{)}.\] (3)

The first term shows the cumulative regret of agent \(j\), when run within ALExp. The second term evaluates the received reward against the cumulative average reward of model \(j\). We bound each term separately.

**Virtual Regret.** The first term \(\tilde{R}_{j}(n)\coloneqq\sum_{t=1}^{n}r(\bm{x}^{\star})-r_{t,j}\) compares the suggestion of agent \(j\), against the optimal action. We call it the virtual regret since the sequence \((\bm{x}_{t,j})_{t\geq 1}\) of the actions suggested by model \(j\) are not necessarily selected by the meta algorithm, unless \(j_{t}=j\). This regret is merely a technical tool, and not actually realized when running ALExp. The virtual regret of the oracle agent may still be analyzed using standard techniques for linear bandits, e.g., Abbasi-Yadkori et al. (2011), however we need to adapt it to take into account a subtle difference: The confidence sequence of model \(j^{\star}\) is constructed according to the true sequence of actions \((\bm{x}_{t})_{t\geq 1}\), while its virtual regret is calculated based on the virtual sequence \((\bm{x}_{t,j^{\star}})_{t\geq 1}\), which the model suggests. The two sequences only match at the steps when model \(j^{\star}\) is selected. Adapting the analysis of Abbasi-Yadkori et al. (2011) to this subtlety, we obtain in Lemma 15 that with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\),

\[\tilde{R}_{j^{\star}}(n)=\mathcal{O}\left(n^{5/8}C_{\min}^{-1/2}\sqrt{d\log \left(n\right)+\log(1/\delta)}\right).\]

**Model Selection Regret.** The second term in (3) is the model selection regret, \(R(n,j)\coloneqq\sum_{t=1}^{n}r_{t,j}-r(\bm{x}_{t})\), which evaluates the chosen action by ALExp against the suggestion of the \(j\)-th agent. Our analysis relies on a careful decomposition of \(R(n,j)\),

\[R(n,j)=\sum_{t=1}^{n}\Bigg{[}\underbrace{r_{t,j}-\hat{r}_{t,j}}_{\text{(I)}}+ \underbrace{\hat{r}_{t,j}-\sum_{i=1}^{M}q_{t,i}\hat{r}_{t,i}}_{\text{(II)}}+ \underbrace{\sum_{i=1}^{M}q_{t,i}(\hat{r}_{t,i}-r_{t,i})}_{\text{(III)}}+ \underbrace{\sum_{i=1}^{M}q_{t,i}r_{t,i}-r(\bm{x}_{t})}_{\text{(IV)}}\Bigg{]}.\]

We bound away each term in a modular manner, until we are left with the regret of the standard exponential weights algorithm. The terms (I) and (III) are controlled by the bias of the Lasso estimator, and are \(\mathcal{O}(n^{3/4}C(M,\delta,d))\) (Lemma 19). The last term (IV) is zero in expectation, and reflects the deviation of \(r(\bm{x}_{t})\) from its mean. We observe that the summands form a bounded Martingale difference sequence, and their sum grows as \(\mathcal{O}(\sqrt{n})\) (Lemma 18). Term (II) is the regret of our online optimizer, which depends on the variance of the Lasso estimator. We bound this term with \(\mathcal{O}(\sqrt{n}C(M,\delta,d)\log M)\), by first conducting a standard anytime analysis of exponential weights (Lemma 17), and then incorporating the anytime Lasso variance bound (Lemma 20). We highlight that neither of the above steps require assumptions about the base agents. Combining these steps, Lemma 16 establishes the formal bound on the model selection regret.

**Anytime Lasso.** We develop novel confidence intervals for Lasso with history-dependent data, which are uniformly valid over an unbounded time horizon. This result may be of independent interest in applications of Lasso for online learning or sequential decision making. Here, we use these confidence intervals to bound the bias and variance terms that appear in our treatment of the model selection regret. The width of the Lasso confidence intervals depends on the quality of feature matrix \(\Phi_{t}\), often quantified by the restricted eigenvalue property (Bickel et al., 2009; van de Geer and Buhlmann, 2009; Javanmard and Montanari, 2014):

**Definition 2**.: For the feature matrix \(\Phi_{t}\in\mathbb{R}^{t\times dM}\) we define \(\kappa(\Phi_{t},s)\) for any \(1\leq s\leq M\) as

\[\kappa(\Phi_{t},s)\coloneqq \inf_{(J,\bm{b})}\frac{1}{\sqrt{t}}\frac{\|\Phi_{t}\bm{b}\|_{2}} {\sqrt{\sum_{j\in J}\|\bm{b}_{j}\|_{2}^{2}}}\] \[\text{s.t.}\ \ \bm{b}\in\mathbb{R}^{d}\backslash\{0\},\ \sum_{j\notin J}\|\bm{b}_{j}\|_{2}\leq 3 \sum_{j\in J}\|\bm{b}_{j}\|_{2},\ J\subset\{1,\ldots,M\},\ |J|\leq s.\]Our analysis is in terms of this quantity, and Lemma 8 explains the connection between \(\kappa(\Phi_{t},s)\) and \(C_{\min}\) as defined in (2), particularly that \(\kappa(\Phi_{t},2)\) is also positive with a high probability.

**Theorem 3** (Anytime Lasso Confidence Sequences).: _Consider the data model \(y_{t}=\boldsymbol{\theta}^{\top}\boldsymbol{\phi}(\boldsymbol{x}_{t})+\varepsilon _{t}\) for all \(t\geq 1\), where \(\varepsilon_{t}\) is i.i.d. zero-mean sub-Gaussian noise, and \((\boldsymbol{x}_{t})_{t\geq 1}\) is \((\mathcal{F}_{t})_{t\geq 1}\)-predictable, where \(\mathcal{F}_{t}\coloneqq(\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{t}, \varepsilon_{1},\ldots,\varepsilon_{t-1})\). Then the solution of (1) guarantees_

\[\mathbb{P}\left(\forall t\geq 1:\ \left\|\boldsymbol{\theta}-\hat{\boldsymbol{ \theta}}_{t}\right\|_{2}\leq\frac{4\sqrt{10}\lambda_{t}}{\kappa^{2}(\Phi_{t}, 2)}\right)\geq 1-\delta\]

_if the regularization parameter satisfies for all \(t\geq 1\)_

\[\lambda_{t}\geq\frac{2\sigma}{\sqrt{t}}\sqrt{1+\frac{12}{\sqrt{2}}\left(\log (2M/\delta)+(\log\log d)_{+}\right)+\frac{5}{\sqrt{2}}\sqrt{d\left(\log(2M/ \delta)+(\log\log d)_{+}\right)}}.\]

Our confidence bound enjoys the same rate as Lasso with fixed (offline) data, up to \(\log\log d\) factors. We prove this theorem by constructing a self-normalized martingale sequence based on the \(\ell_{2}\)-norm of the empirical process error (\(\Phi_{t}^{\top}\boldsymbol{\varepsilon}_{t}\)). We then apply the "stitched" time-uniform boundary of Howard et al. (2021). Appendix B elaborates on this technique. Previous work on sparse linear bandits also include analysis of Lasso in an online setting, when \(\boldsymbol{x}_{t}\) is \(\mathcal{F}_{t}\) measurable. Cella and Pontil (2021) imitate offline analysis and then apply a union bound over the time steps, which multiplies the width by \(\log n\) and requires knowledge of the horizon. Bastani and Bayati (2020) also rely on knowledge of \(n\) and employ a scalar-valued Bernstein inequality on \(\ell_{\infty}\)-norm of the empirical process error, which inflates the width of the confidence sets by a factor of \(\sqrt{d}\). We work directly on the \(\ell_{2}\)-norm, and use a curved boundary for sub-Gamma martingale sequences, which according to Howard et al. (2021) is uniformly tighter than a Bernstein bound, especially for small \(t\).

### Discussion

In light of Theorem 1 and Theorem 3, we discuss some properties of ALExp.

Sparse EXP4.Our approach presents a new connection between online learning and high-dimensional statistics. The rule for updating the probabilities in ALExp is inspired by the exponential weighting for Exploration and Exploitation with Experts (EXP4) algorithm, which was proposed by Auer et al. (2002) and has been extensively studied in the adversarial bandit and learning with expert advice literature (e.g., McMahan and Streeter, 2009; Beygelzimer et al., 2011). EXP4 classically uses importance-weighted (IW) or ordinary least squares (LS) estimators to estimate the return \(r_{t,j}\), both of which are unbiased but high-variance choices (Bubeck et al., 2012). In particular, in our linearly parametrized setting, the variance of IW and LS scales with \(Md\), which will lead to a \(\text{poly}(M)\) regret. However, it is known that introducing bias can be useful if it reduces the variance (Zimmer and Lattimore, 2022). For instance, EXP3-IX (Kocak et al., 2014) and EXP4-IX (Neu, 2015) construct a biased IW estimator. Equivalently, others craft regularizers for the reward of the online optimizer, seeking to improve the bias-variance balance (e.g., Abernethy et al., 2008; Bartlett et al., 2008; Abernethy and Rakhlin, 2009; Bubeck et al., 2017; Lee et al., 2020; Zimmer and Lattimore, 2022). A key technical observation in this work is that our online Lasso estimator leads EXP4 to achieve sublinear regret which depends logarithmically on \(M\). This is due to the fact that while the estimator itself is \(Md\)-dimensional, its bias squared and variance scale with \(\sqrt{d\log M}\). To the best of our knowledge, this work is first to instantiate the EXP4 algorithm with a sparse low-variance estimator.

Adaptive and Anytime.To estimate the reward, prior work on sparse bandits commonly emulate the Lasso analysis on offline data or on a martingale sequence with a known length (Hao et al., 2020; Bastani and Bayati, 2020). These works require a long enough sequence of exploratory samples, and knowledge of the horizon to plan this sequence. ALExp removes both of these constraints, and presents a fully adaptive algorithm. Crucially, we employ the elegant martingale bounds of Howard et al. (2021) to present the first time-uniform analysis of the Lasso with history-dependent data (Theorem 3). This way we can use all the data points and explore with a probability which vanishes at a \(\mathcal{O}(t^{-1/4})\) rate. Our anytime confidence bound for Lasso, together with the horizon-independent analysis of the exponential weights algorithm, also allows ALExp to be stopped at any time with valid guarantees.

Rate Optimality.For \(M\gg n\), we obtain a \(\mathcal{O}(\sqrt{n\log^{3}M})\) regret, which matches the rate conjectured by Agarwal et al. (2017). However, if \(M\) is comparable to \(n\) or smaller, our regret scales with \(\mathcal{O}(n^{3/4}\sqrt{\log M})\), and while it is still sublinear and scales logarithmically with \(M\), the dependency on \(n\) is sub-optimal. This may be due to the conservative nature of our model selection analysis, during which we do not make assumptions about the dynamics of the base agents. Therefore, to ensure sufficiently diverse data for successful model selection, we need to occasionally choose exploratory actions with a vanishing probability of \(\gamma_{t}\). We conjecture that this is avoidable, if we make more assumptions about the agents, e.g., that a sufficient number of agents can achieve sublinear regret if executed in isolation. Banerjee et al. (2023) show that the data collected by sublinear algorithms organically satisfies a minimum eigenvalue lowerbound, which may also be sufficient for model selection. We leave this as an open question for future work.

## 6 Experiments

**Experiment Setup.** We create a synthetic dataset based on our data model (Section 3), and choose the domain to be \(1\)-dimensional \(\mathcal{X}=[-1,1]\). As a natural choice of features, we consider the set of degree-\(p\) Legendre polynomials, since they form an orthonormal basis for \(L^{2}(\mathcal{X})\) if \(p\) grows unboundedly. We construct each feature map, by choosing \(s\) different polynomials from this set, and therefore obtaining \(M=\binom{p+1}{s}\) different models. More formally, we let \(\bm{\phi}_{j}(x)=(P_{j_{1}}(x),\dots,P_{j_{s}}(x))\in\mathbb{R}^{s}\) where \(\{j_{1},\dots,j_{s}\}\subset\{0,\dots,p\}\) and \(P_{j^{\prime}}\) denotes a degree \(j^{\prime}\) Legendre polynomial. To construct the reward function, we randomly sample \(j^{\star}\) from \([M]\), and draw \(\bm{\theta}_{j^{\star}}\) from an i.i.d. standard gaussian distribution. We then normalize \(||\bm{\theta}_{j^{\star}}||=1\). When sampling from the reward, we add Gaussian noise with standard deviation \(\sigma=0.01\). Figure 5 in the appendix shows how the random reward functions may look. For all experiments we set \(n=100\), and plot the cumulative regret \(R(n)\) averaged over \(20\) different random seeds, the shaded areas in all figures show the standard error across these runs.

**Algorithms.** We perform experiments on two UCB algorithms, one with oracle knowledge of \(j^{\star}\), and a naive one which takes into account all \(M\) feature maps. We run Explore-then-Commit (ETC) by Hao et al. (2020), which explores for a horizon of \(n_{0}\) steps, performs Lasso once, and then selects actions greedily for the remaining steps. As another baseline, we introduce Explore-then-Select (ETS) that explores for \(n_{0}\) steps, performs model selection using the sparsity pattern of the Lasso estimator. For the remaining steps, the policy switches to UCB, calculated based on the selected features. Performance of ETC and ETS depends highly on \(n_{0}\), so we tune this hyperparameter separately for each experiment. We also run Corral as proposed by Agarwal et al. (2017), with UCB agents similar to ALExp. We tune the hyper-parameters of Corral as well. To initialize ALExp we set the rates of \(\lambda_{t},\gamma_{t}\) and \(\eta_{t}\) according to Theorem 1, and perform a light hyper-parameter tuning to choose the scaling constants. We have included the details and results of our hyper-parameter tuning in Appendix F.1. To solve (1), we use Celer, a fast solver for the group Lasso (Massias et al., 2018). Every time UCB policy is used, we set the exploration coefficient \(\beta_{t}=2\), and every time exploration is required, we sample according to \(\pi=\mathrm{Unif}(\mathcal{X})\). Appendix F includes the pseudo-code for all baseline algorithms.1

Footnote 1: The Python code for reproducing the experiments is accessible on github.com/lasgroup/ALEXP.

**Easy vs. Hard Cases.** We construct an easy problem instance, where \(s=2\), \(p=10\), and thus \(M=55\). Models are lightly correlated since each two model can have at most one Legendre polynomial in common. We also generate an instance with highly correlated feature maps where \(s=8\) and \(p=10\), which will be a harder problem, since out of the total \(M=55\) models, there are \(36\) models which have at least \(6\) Legendre polynomials in common with the oracle model \(j^{\star}\). Figure 1 shows that not only ALExp is not affected by the correlations between the models, but also it achievesa performance competitive to the oracle in both cases, implying that our exponential weights technique for model selection is robust to choice of features. ETC and ETS rely on Lasso for model selection, which performs poorly in the case of correlated features. Corral uses log-barrier-OMD with an importance-weighted estimator, which has a significantly high variance. The curve for Corral in Figures 1 and 2 is cropped since the regret values get very large. Figure 6 shows the complete results. We construct another hard instance (Fig. 2), where the model class is large (\(s=3,p=10,M=165\)). ALExp continues to outperform all baselines with a significant gap. It is clear in the regret curves how explore-then-commit style algorithms are inherently horizon-dependent, and may exhibit linear regret, if stopped at an arbitrary time. This is not an issue with the other algorithms.

**Scaling with M.** Figure 3 shows how well the algorithms scale as \(M\) grows. For this experiment we set \(s=2\) and change \(p\in\{9,\ldots,13\}\). While increasing \(M\) hardly affects ALExp and Oracle UCB, other baselines become less and less sample efficient.

**Learning Dynamics of ALExp.** Figure 4 gives some insight into the dynamics of ALExp when \(M=165\). In particular, it shows how ALExp can rule out sub-optimal agents without ever having queried them. Figure (a) shows the distribution \(\bm{q}_{t}\), at \(t=20\) which is roughly equal to the optimal \(n_{0}\) for ETC in this configuration. The oracle model \(j^{\star}\) is annotated with a star, and has the highest probability of selection. We observe that, already at this time step, more than \(80\%\) of the agents are practically ruled out, due to small probability of selection. However, according to Figure (b), which shows \(M_{t}\) the total number of visited models, less than \(10\%\) of the models are queried at \(t=20\). This is the key practical benefit of ALExp compared to black-box algorithms such as Corral. Lastly, Figure (c) shows how \(q_{t,j^{\star}}\) the probability of selecting the oracle agent changes with time. While this probability is higher than that of the other agents, Figure (c) shows that \(q_{t,j^{\star}}\) is not exceeding \(0.25\), therefore there is always a probability of greater than \(0.75\) that we sample another agent, making ALExp robust to hard problem instances where many agents perform efficiently. We conclude that ALExp seems to rapidly recognize the better performing agents, and select among them with high probability.

## 7 Conclusion

We proposed ALExp, an algorithm for simultaneous online model selection and bandit optimization. As a first, our approach leads to anytime valid guarantees for model selection and bandit regret, and does not rely on a priori determined exploration schedule. Further, we showed how the Lasso can be used together with the exponential weights algorithm to construct a low-variance online learner. This new connection between high-dimensional statistics and online learning opens up avenues for future research on high-dimensional online learning. We established empirically that ALExp has favorable exploration-exploitation dynamics, and outperforms existing baselines. We tackled the open problem of Agarwal et al. (2017), and showed that \(\log M\) dependency for regret is achievable for linearly parametrizable rewards. This problem remains open for more general, non-parametric reward classes.

Figure 4: ALExp can rule out models without ever having queried them (\(M=165\))

Figure 3: ALExp is hardly affected by increasing the number of models (y-axis have various scales)

## Acknowledgments

We thank Jonas Rothfuss and Miles Wang-Henderson for their valuable suggestions regarding the writing. We thank Scott Sussex and Felix Schur for their thorough feedback. This research was supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and Innovation Program Grant agreement no. 815943. Nicolas Emmenegger was supported by the Zeno Karl Schindler Foundation and the Swiss Study Foundation. Aldo Pacchiano would like to thank the support of the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard. This work was supported in part by funding from the Eric and Wendy Schmidt Center at the Broad Institute of MIT and Harvard.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in Neural Information Processing Systems_, 2011.
* Abbasi-Yadkori et al. (2012) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Online-to-confidence-set conversions and application to sparse stochastic bandits. In _Artificial Intelligence and Statistics_. PMLR, 2012.
* Abernethy and Rakhlin (2009) Jacob Abernethy and Alexander Rakhlin. Beating the adaptive bandit with high probability. In _2009 Information Theory and Applications Workshop_. IEEE, 2009.
* Abernethy et al. (2008) Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. An efficient algorithm for bandit linear optimization. In _Conference on Learning Theory_, 2008.
* Agarwal et al. (2014) Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the monster: A fast and simple algorithm for contextual bandits. In _International Conference on Machine Learning_. PMLR, 2014.
* Agarwal et al. (2017) Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algorithms. In _Conference on Learning Theory_. PMLR, 2017.
* Auer et al. (1995) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. Gambling in a rigged casino: The adversarial multi-armed bandit problem. In _Foundations of Computer Science_. IEEE, 1995.
* Auer et al. (2002a) Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 2002a.
* Auer et al. (2002b) Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_, 2002b.
* Banerjee et al. (2023) Debangshu Banerjee, Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. Exploration in linear bandits with rich action sets and its implications for inference. In _International Conference on Artificial Intelligence and Statistics_. PMLR, 2023.
* Bartlett et al. (2008) Peter L Bartlett, Varsha Dani, Thomas P Hayes, Sham M Kakade, Alexander Rakhlin, and Ambuj Tewari. High-probability regret bounds for bandit online linear optimization. In _Conference on Learning Theory_, 2008.
* Bastani and Bayati (2020) Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. _Operations Research_, 2020.
* Beygelzimer et al. (2011) Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandit algorithms with supervised learning guarantees. In _International Conference on Artificial Intelligence and Statistics_, 2011.
* Bickel et al. (2009) Peter J. Bickel, Ya'acov Ritov, and Alexandre B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. _The Annals of Statistics_, 2009.
* Bubeck et al. (2012) Sebastien Bubeck, Nicolo Cesa-Bianchi, et al. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 2012.
* Bubeck et al. (2017) Sebastien Bubeck, Yin Tat Lee, and Ronen Eldan. Kernel-based methods for bandit convex optimization. In _ACM SIGACT Symposium on Theory of Computing_, pages 72-85, 2017.
* Bubeck et al. (2018)Peter Buhlmann and Sara Van De Geer. _Statistics for high-dimensional data: methods, theory and applications_. Springer Science & Business Media, 2011.
* Carpentier and Munos (2012) Alexandra Carpentier and Remi Munos. Bandit theory meets compressed sensing for high dimensional stochastic linear bandit. In _Artificial Intelligence and Statistics_. PMLR, 2012.
* Cella and Pontil (2021) Leonardo Cella and Massimiliano Pontil. Multi-task and meta-learning with sparse linear bandits. In _Conference on Uncertainty in Artificial Intelligence_, 2021.
* Cesa-Bianchi and Lugosi (2006) Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* Foster and Rakhlin (2020) Dylan Foster and Alexander Rakhlin. Beyond ucb: Optimal and efficient contextual bandits with regression oracles. In _International Conference on Machine Learning_. PMLR, 2020.
* Foster et al. (2017) Dylan J Foster, Satyen Kale, Mehryar Mohri, and Karthik Sridharan. Parameter-free online learning via model selection. _Advances in Neural Information Processing Systems_, 2017.
* Foster et al. (2019) Dylan J Foster, Akshay Krishnamurthy, and Haipeng Luo. Model selection for contextual bandits. _Advances in Neural Information Processing Systems_, 32, 2019.
* Gerchinovitz (2011) Sebastien Gerchinovitz. Sparsity regret bounds for individual sequences in online linear regression. In _Conference on Learning Theory_, 2011.
* Giannopoulos (2003) Apostolos Giannopoulos. Notes on isotropic convex bodies, October 2003.
* Gonzalez et al. (2015) Javier Gonzalez, Joseph Longworth, David C James, and Neil D Lawrence. Bayesian optimization for synthetic gene design. _arXiv preprint_, 2015.
* Hao et al. (2020) Botao Hao, Tor Lattimore, and Mengdi Wang. High-dimensional sparse linear bandits. _Advances in Neural Information Processing Systems_, 2020.
* Haussler et al. (1998) David Haussler, Jyrki Kivinen, and Manfred K Warmuth. Sequential prediction of individual sequences under general loss functions. _IEEE Transactions on Information Theory_, 1998.
* Howard et al. (2020) Steven R. Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform chernoff bounds via nonnegative supermartingales. _Probability Surveys_, 2020.
* Howard et al. (2021) Steven R Howard, Aaditya Ramdas, Jon McAuliffe, and Jasjeet Sekhon. Time-uniform, nonparametric, nonasymptotic confidence sequences. _The Annals of Statistics_, 2021.
* Imani et al. (2019) Mahdi Imani, Seyede Fatemeh Ghoreishi, Douglas Allaire, and Ulisses M Braga-Neto. Mfbo-ssm: Multi-fidelity bayesian optimization for fast inference in state-space models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019.
* Jang et al. (2022) Kyoungseok Jang, Chicheng Zhang, and Kwang-Sung Jun. Popart: Efficient sparse regression and experimental design for optimal sparse linear bandits. In _Advances in Neural Information Processing Systems_, 2022.
* Javanmard and Montanari (2014) Adel Javanmard and Andrea Montanari. Confidence intervals and hypothesis testing for high-dimensional regression. _The Journal of Machine Learning Research_, 2014.
* Karimi et al. (2021) Mohammad Reza Karimi, Nezihre Merve Gurel, Bojan Karlas, Johannes Rausch, Ce Zhang, and Andreas Krause. Online active model selection for pre-trained classifiers. In _International Conference on Artificial Intelligence and Statistics_. PMLR, 2021.
* Kassraie et al. (2022) Parnian Kassraie, Jonas Rothfuss, and Andreas Krause. Meta-learning hypothesis spaces for sequential decision-making. In _International Conference on Machine Learning_. PMLR, 2022.
* Kaufmann et al. (2012) Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On bayesian upper confidence bounds for bandit problems. In _Artificial intelligence and statistics_. PMLR, 2012.
* Kim and Paik (2019) Gi-Soo Kim and Myunghee Cho Paik. Doubly-robust lasso bandit. _Advances in Neural Information Processing Systems_, 32, 2019.
* Krizhevsky et al. (2012)Tomas Kocak, Gergely Neu, Michal Valko, and Remi Munos. Efficient learning by implicit exploration in bandit problems with side observations. _Advances in Neural Information Processing Systems_, 2014.
* Lattimore and Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* Lee et al. (2020) Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang. Bias no more: high-probability data-dependent regret bounds for adversarial bandits and mdps. _Advances in Neural Information Processing Systems_, 2020.
* Li et al. (2022) Wenjie Li, Adarsh Barik, and Jean Honorio. A simple unified framework for high dimensional bandit problems. In _International Conference on Machine Learning_. PMLR, 2022.
* Liu et al. (2022) Xuefeng Liu, Fangfang Xia, Rick L Stevens, and Yuxin Chen. Cost-effective online contextual model selection. _arXiv preprint_, 2022.
* Lounici et al. (2011) Karim Lounici, Massimiliano Pontil, Sara Van De Geer, and Alexandre B Tsybakov. Oracle inequalities and optimal inference under group sparsity. _The annals of statistics_, 2011.
* Luo et al. (2022) Haipeng Luo, Mengxiao Zhang, Peng Zhao, and Zhi-Hua Zhou. Corralling a larger band of bandits: A case study on switching regret for linear bandits. In _Conference on Learning Theory_. PMLR, 2022.
* Maillard and Munos (2011) Odalric-Ambrym Maillard and Remi Munos. Adaptive bandits: Towards the best history-dependent strategy. In _International Conference on Artificial Intelligence and Statistics_. JMLR Workshop and Conference Proceedings, 2011.
* Massias et al. (2018) Mathurin Massias, Alexandre Gramfort, and Joseph Salmon. Celer: a fast solver for the lasso with dual extrapolation. In _International Conference on Machine Learning_, 2018.
* McMahan and Streeter (2009) H. Brendan McMahan and Matthew Streeter. Tighter bounds for multi-armed bandits with expert advice. In _Proceedings of the 22nd Annual Conference on Learning Theory_, 2009.
* Moradipari et al. (2022) Ahmadreza Moradipari, Berkay Turan, Yasin Abbasi-Yadkori, Mahnoosh Alizadeh, and Mohammad Ghavamzadeh. Feature and parameter selection in stochastic linear bandits. In _International Conference on Machine Learning_. PMLR, 2022.
* Muthukumar et al. (2019) Vidya Muthukumar, Mitas Ray, Anant Sahai, and Peter Bartlett. Best of many worlds: Robust model selection for online supervised learning. In _Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics_, 2019.
* Neu (2015) Gergely Neu. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. _Advances in Neural Information Processing Systems_, 2015.
* Oh et al. (2021) Min-hwan Oh, Garud Iyengar, and Assaf Zeevi. Sparsity-agnostic lasso bandit. In _International Conference on Machine Learning_. PMLR, 2021.
* Pacchiano et al. (2020) Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari. Model selection in contextual stochastic bandit problems. _Advances in Neural Information Processing Systems_, 2020.
* Pacchiano et al. (2022) Aldo Pacchiano, Drausin Wulsin, Robert A Barton, and Luis Voloch. Neural design for genetic perturbation experiments. _arXiv preprint_, 2022.
* Paszke et al. (2017) Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
* Rahimi et al. (2007) Ali Rahimi, Benjamin Recht, et al. Random features for large-scale kernel machines. In _Advances in Neural Information Processing Systems_, 2007.
* Schur et al. (2023) Felix Schur, Parnian Kassraie, Jonas Rothfuss, and Andreas Krause. Lifelong bandit optimization: No prior and no regret. In _Conference on Uncertainty in Artificial Intelligence_, 2023.
* Schuster et al. (2018)Adish Singla, Hamed Hassani, and Andreas Krause. Learning to interact with learning agents. In _AAAI Conference on Artificial Intelligence_, 2018.
* Thompson (1933) William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. _Biometrika_, 1933.
* Tibshirani (1996) Robert Tibshirani. Regression shrinkage and selection via the lasso. _Journal of the Royal Statistical Society: Series B (Methodological)_, 1996.
* Ulmasov et al. (2016) Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter Deisenroth, and Ruth Misener. Bayesian optimization with dimension scheduling: Application to biological systems. In _Computer Aided Chemical Engineering_. Elsevier, 2016.
* van de Geer and Buhlmann (2009) Sara A. van de Geer and Peter Buhlmann. On the conditions used to prove oracle results for the Lasso. _Electronic Journal of Statistics_, 2009.
* Vershynin (2018) Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.
* Wainwright (2019) Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_. Cambridge university press, 2019.
* Zimmert and Lattimore (2022) Julian Zimmert and Tor Lattimore. Return of the bias: Almost minimax optimal high probability bounds for adversarial linear bandits. In _Conference on Learning Theory_. PMLR, 2022.

###### Contents of Appendix

* A Extended Literature Review
* B Time Uniform Lasso Analysis
* C Results on Exploration
* C.1 ALExp with Uniform Exploration
* C.2 Proof of Results on Exploration
* D Proof of Regret Bound
* D.1 Proof of Model Selection Regret
* D.2 Proof of Virtual Regret
* E Time-Uniform Concentration Inequalities
* F Experiment Details
* F.1 Hyper-Parameter Tuning Results

## Appendix A Extended Literature Review

The sparse linear bandit literature considers linear reward functions of the form \(\bm{\theta}^{\top}\bm{x}\), where \(\bm{x}\in\mathbb{R}^{p}\), however a sub-vector of size \(d\) is sufficient to span the reward function. This can be formulated as model selection among \(M=\binom{p}{d}\) different linear parametrizations, where each \(\bm{\phi}_{j}\) is a \(d\)-dimensional feature map. We present the bounds in terms of \(d\) and \(M\) for coherence with the rest of the text, assuming that \(M=\mathcal{O}(p)\), which is the case when \(d\ll p\).

Table 2 compares recent work on sparse linear bandits based on a number of important factors. In this table, the ETC algorithms follow the general format of exploring, performing parameter estimation once at \(t=n_{0}\), and then repeatedly suggesting the same action which maximizes \(\hat{\bm{\theta}}_{n_{0}}^{\top}\bm{\phi}(\bm{x})\). Explore-then-(\(\epsilon\))Greedy takes a similar approach, however it does not settle on \(\hat{\bm{\theta}}_{n_{0}}\), rather it continues to update the parameter estimate and select \(\bm{x}_{t}=\arg\max\hat{\bm{\theta}}_{t}^{\top}\bm{\phi}(\bm{x})\). The UCB algorithms iteratively update upper confidence bound, and choose actions which maximize them. The regret bounds in Table 2 are simplified to the terms with largest rate of growth, _the reader should check the corresponding papers for rigorous results_. Some of the mentioned bounds depend on problem-dependent parameters (e.g. \(c_{K}\)), which may not be treated as absolute constants and have complicated forms. To indicate such parameters we use \(\tau\) in Table 2, following the notation of Hao et al. (2020). Note that \(\tau\) varies across the rows of the table, and is just an indicator for existence of other terms.

Abbasi-Yadkori et al. (2012) use the SeqSEW online regression oracle Gerchinovitz (2011) for estimating the parameter vector, together with a UCB policy. The regression oracle is an exponential weights algorithm, which runs on the squared error loss. This subroutine, and thereby the algorithm proposed by Abbasi-Yadkori et al. (2012) are not computationally efficient, and this is believed to be unavoidable. This work considers the data-rich regime and shows \(R(n)=\mathcal{O}(\sqrt{dMn})\), matching the lower bound of Theorem 24.3 in Lattimore and Szepesvari (2020).

Carpentier and Munos (2012) assume that the action set is a Euclidean ball, and that the noise is directly added to the parameter vector, i.e. \(y_{t}=\bm{x}^{\top}(\bm{\theta}+\bm{\varepsilon}_{t})\). Roughly put, linear bandits with parameter noise are "easier" to solve than stochastic linear bandits with reward noise, since the noise is scaled proportionally to the features \(x_{i}\) and does less "damage " (Chapter 29.3 Lattimore and Szepesvari, 2020). In this setting, Carpentier and Munos (2012) present a \(\mathcal{O}(d\sqrt{n})\) regret bound.

Recent work considers contextual linear bandits, where at every step \(\mathcal{A}_{t}\), a stochastic finite subset of size \(K\) from \(\mathcal{A}\), is presented to the agent. It is commonly assumed that members of \(\mathcal{A}_{t}\) are i.i.d., and the sampling distribution is diverse and time-independent. The diversity assumption is often in theform of a restricted eigenvalue condition (Definition 2) on the covariance of the context distribution (e.g. in, Kim and Paik, 2019; Bastani and Bayati, 2020). Li et al. (2022) require a stronger condition which directly assumes that \(\lambda_{\min}(\Phi_{t})\) the minimum eigenvalue of the empirical covariance matrix is lower bounded. This is generally not true, but may hold with high probability. Hao et al. (2020) assume that the action set spans \(\mathbb{R}^{dM}\). We believe that this assumption is the weakest in the literature, and conjecture that it is necessary for model selection. If not met, the agent can not explore in all relevant directions, and may not identify the relevant features. Our diversity assumption is similar to Hao et al. (2020), adapted to our problem setting. Mainly, we consider reward functions which are linearly parametriable, i.e. \(\bm{\theta}^{\top}\bm{\phi}(\bm{x})\), as oppose to linear rewards, i.e. \(\bm{\theta}^{\top}\bm{x}\).

A key distinguishing factor between ALExp and existing work on sparse linear bandit is that ALExp is horizon-independent and does not rely on a forced exploration schedule. As shown on Table 2, majority of prior work relies either on an initial exploration stage, the length of which is determined according to \(n\)(e.g., Carpentier and Munos, 2012; Kim and Paik, 2019; Li et al., 2022; Hao et al., 2020; Jang et al., 2022), or on a hand crafted schedule, which is again designed for a specific horizon (Bastani and Bayati, 2020). Oh et al. (2021), which analyzes \(K\)-armed contextual bandits, does not require explicit exploration, and instead imposes restrictive assumptions on the diversity of context distribution, e.g. relaxed symmetry and balanced covariance. Regardless, the regret bounds hold in expectation, and are not time-uniform.

## Appendix B Time Uniform Lasso Analysis

We start by showing that the sum of squared sub-gaussian variables is a sub-Gamma process (c.f. Definition 22).

**Lemma 4** (Empirical Process is sub-Gamma).: _For \(t\geq 1\), suppose \(\xi_{t}\) are a sequence conditionally standard sub-Gaussians adapted to the filtration \(\mathcal{F}_{t}=\sigma(\xi_{1},\dots,\xi_{t})\). Let \(v_{t}\in\mathbb{R}\), and \(Z_{t}\coloneqq\xi_{t}^{2}-1\). Define the processes \(S_{t}\coloneqq\sum_{i=1}^{t}Z_{i}v_{i}\) and \(V_{t}\coloneqq 4\sum_{i=1}^{t}v_{i}^{2}\). Then \((S_{t})_{t=0}^{\infty}\) is sub-Gamma with variance process \((V_{t})_{t=0}^{\infty}\) and scale parameter \(c=4\max_{i\geq 1}v_{i}\)._

Proof of Lemma 4.: By definition (c.f. Definition 1, Howard et al., 2021), \(S_{t}\) is sub-Gamma if for each \(\lambda\in[0,1/c)\), there exists a supermartingale \((M_{t}(\lambda))_{t=0}^{\infty}\) w.r.t. \(\mathcal{F}_{t}\), such that \(\mathbb{E}\,M_{0}=1\) and for all \(t\geq 1\):

\[\exp\left\{\lambda S_{t}-\frac{\lambda^{2}}{2(1-c\lambda)}V_{t}\right\}\leq M _{t}(\lambda)\qquad a.s.\]

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c}  & \(|\mathcal{A}_{t}|\) & data-poor & adap. & any-time & action policy & MS & context & Regret \\ \hline Abbasi-Yadkori et al. & \(\infty\) & ✗ & ✓ & ✓ & UCB & EXP4 on & \(\mathcal{A}\) is compact & \(\sqrt{dMn}\), w.h.p. \\ \hline Foster et al. & K & ✓ & ✓ & ✗ & UCB & EXP4 on & \(\lambda_{\min}(\Sigma)\geq c_{\lambda}\) & \((Mn)^{1/4}K^{(\gamma+\gamma K\delta Mn}\) \\  & & & & & & Hard & \(\mathcal{A}\) is a ball & w.h.p. \\ \hline Carpentier and Munos & \(\infty\) & ✓ & ✗ & ✗ & UCB & Thresh. & param. noise & \(d\sqrt{n}\), w.h.p. \\ \hline Bastani and Bayati & \(K\) & ✗ & ✗ & ✗ & \begin{tabular}{c} Explore \\ Greedy \\ \end{tabular} & Lasso & \(\kappa(\Sigma)>c_{K}\) & \(\tau K\tilde{d}^{2}(\log n+\log M)^{2}\) \\  & & & & & & & \\ \hline Kim and Paik & \(K\) & ✗ & ✗ & \begin{tabular}{c} Explore \\ \(\epsilon\)-Greedy \\ \end{tabular} & Lasso & \(\kappa(\Sigma)>c_{K}\) & \(\tau d\sqrt{n}\log(Mn)\), w.h.p. \\ \hline Oh et al. & \(K\) & ✓ & ✓ & ✗ & Greedy & Lasso & \(\begin{tabular}{c} \(\kappa(\Sigma)>c_{\kappa}\) \\ + other assums. \\ \end{tabular} & \(\tau d\sqrt{n}\log(Mn)\) \\ \hline Li et al. & \(K\) & ✓ & ✗ & ETC & Lasso & \(\lambda_{\min}(\hat{\Sigma})>c_{\hat{\kappa}}\) & \(\tau(n^{d})^{1/3}\log M\) \\ \hline Hao et al. & \(\infty\) & ✓ & ✗ & ETC & Lasso & \begin{tabular}{c} \(4\) spans \(\mathbb{R}^{d\lambda}\) \\ + is compact \\ \end{tabular} & \((ndd_{\min}^{-1})^{1/3}(\log M)^{1/3}\) \\ \hline Jang et al. & \(\infty\) & ✓ & ✗ & ETC & Hard & \(\mathcal{A}<(-1,1)^{1/3}\tilde{d}^{2/3}(\log M)^{1/3}\) \\  & & & & & & Thresh. & \(\mathcal{A}\) + spans \(\mathbb{R}^{d\lambda}\) & w.h.p. \\ \hline ALExp (Ours) & \(\infty\) & ✓ & ✓ & ✓ & \begin{tabular}{c} Greedy \\ or UCB \\ \end{tabular} & \begin{tabular}{c} EXP4 on \\ reward est. \\ \end{tabular} & 
\begin{tabular}{c} \(\operatorname{Im}(\Theta_{t})\) spans \(\mathbb{R}^{d}\) \\ \(\mathcal{A}\) is compact \\ \end{tabular} & \(\sqrt{n\log M(n^{1/4}-C_{\min}^{1}\log M)}\) \\  & & & & & & w.h.p \\ \end{tabular}
\end{table}
Table 2: Overview of recent work on high-dimensional Bandits. Parameter \(\tau\) shows existence of other problem-dependent terms which are not constants, and varies across different rows. The regret bounds are simplified and are _not_ rigorous.

We show the above holds in equality by proving that the left hand side itself, is a supermartingale w.r.t. \(\mathcal{F}_{t}\). We define, \(M_{t}(\lambda)\coloneqq\exp\{\lambda S_{t}-\lambda^{2}V_{t}/2(1-c\lambda)\}\), therefore,

\[\mathbb{E}\left[M_{t}|\mathcal{F}_{t-1}\right] \leq\mathbb{E}\ \left[\exp\left(\lambda S_{t-1}-\frac{\lambda^{2}}{2(1-c \lambda)}V_{t-1}+\lambda Z_{t}v_{t}-\frac{2\lambda^{2}v_{t}^{2}}{(1-c\lambda)} \right)|\mathcal{F}_{t-1}\right]\] \[=\mathbb{E}\left[M_{t-1}|\mathcal{F}_{t-1}\right]\mathbb{E}\ \left[\exp\left(\lambda Z_{t}v_{t}\right)|\mathcal{F}_{t-1}\right]\exp\left(- \frac{2\lambda^{2}v_{t}^{2}}{1-c\lambda}\right)\] \[=M_{t-1}\mathbb{E}\ \left[\exp\left(\lambda Z_{t}v_{t}\right)| \mathcal{F}_{t-1}\right]\exp\left(-\frac{2\lambda^{2}v_{t}^{2}}{1-c\lambda} \right).\]

Note that \(Z_{t}\) is \(\mathcal{F}_{t-1}\)-measurable, conditionally centered and conditionally sub-exponential with parameters \((\nu,\alpha)=(2,4)\) (c.f. Vershynin (2018, Lemma 2.7.6) and Wainwright (2019, Example 2.8)). Therefore, for \(\lambda<1/c\),

\[\mathbb{E}\ [\exp\left(\lambda v_{t}Z_{t}\right)|\mathcal{F}_{t-1}]\leq\exp \left(2\lambda^{2}v_{t}^{2}\right)\leq\exp\left(\frac{2\lambda^{2}v_{t}^{2}}{1 -c\lambda}\right),\]

where the last inequality holds due to the fact that \(0\leq 1-c\lambda<1\). Therefore,

\[\mathbb{E}\left[M_{t}|\mathcal{F}_{t-1}\right]\leq M_{t-1}\exp\left(\frac{2 \lambda^{2}v_{t}^{2}}{1-c\lambda}\right)\exp\left(-\frac{2\lambda^{2}v_{t}^{2} }{1-c\lambda}\right)=M_{t-1}.\]

for \(\lambda\in[0,1/c)\), concluding the proof. 

We now construct a self-normalizing martingale sequence based on \(\ell_{2}\)-norm of the empirical process error term, and recognize that it is a sub-gamma process. We then employ our curved Bernstein bound Lemma 25 to control the boundary. This step will allow us to "ignore" the empirical process error term later in the lasso analysis.

**Lemma 5** (Empirical Process is dominated by regularization.).: _Let_

\[A_{j}=\left\{\forall t\geq 1:\ \left\|(\Phi_{t}^{\top}\bm{\varepsilon}_{t})_{ j}\right\|_{2}/t\leq\lambda_{t}/2\right\}.\]

_Then, for any \(0\leq\delta<1\), the event \(A=\cap_{j=1}^{M}A_{j}\) happens with probability \(1-\delta\), if for all \(t\geq 1\),_

\[\lambda_{t}\geq\frac{2\sigma}{\sqrt{t}}\sqrt{1+\frac{5}{\sqrt{2}}\sqrt{d\left( \log(2M/\delta)+(\log\log d)_{+}\right)}+\frac{12}{\sqrt{2}}\left(\log(2M/ \delta)+(\log\log d)_{+}\right)}.\]

Proof of Lemma 5.: This proof includes a treatment of the empirical process similar to Lemma B.1 in Kassraie et al. (2022), but adapts it to our time-uniform setting. Since \(\varepsilon_{i}\) are zero-mean sub-gaussian variables, as driven in Lemma 3.1(Lounici et al., 2011), it holds that

\[A_{j}^{c}=\left\{\exists t:\ \frac{1}{t^{2}}\bm{\varepsilon}_{t}^{T}\Phi_{t,j} \Phi_{t,j}^{\top}\bm{\varepsilon}_{t}\geq\frac{\lambda^{2}}{4}\right\}=\left\{ \exists t:\ \frac{\sum_{i=1}^{t}v_{i}(\xi_{i}^{2}-1)}{\sqrt{2}\|\bm{v}_{t}\|}\geq \alpha_{t,j}\right\}\]

where \(\xi_{i}\) are sub-gaussian variables with variance proxy \(1\), scalar \(v_{i}\) denotes the \(i\)-th eigenvalue of \(\Phi_{t,j}\Phi_{t,j}^{\top}/t\) with the concatenated vector \(\bm{v}_{t}=(v_{1},\ldots,v_{t})\), and

\[\alpha_{t,j}=\frac{t^{2}\lambda^{2}/(4\sigma^{2})-\operatorname{tr}(\Phi_{t,j} ^{\top}\Phi_{t,j})}{\sqrt{2}\big{\|}\Phi_{t,j}^{\top}\Phi_{t,j}\big{\|}_{ \operatorname{Fr}}}.\]

We can apply Lemma 25 to control the probability of event \(A_{j}^{c}\) by tuning \(\lambda\). Mainly, for \(A_{j}^{c}\) to happen with probability less than \(\delta/M\), Lemma 25 states that the following must hold for all \(t\),

\[\sqrt{2}\|\bm{v}_{t}\|_{2}\alpha_{t,j}\geq\frac{5}{2}\sqrt{\max\left\{4\|\bm{v }_{t}\|_{2}^{2},1\right\}\omega_{\delta/M}(\left\left\|\bm{v}_{t}\right\|_{2} \right)}+12\omega_{\delta/M}(\left\|\bm{v}_{t}\right\|_{2})\max_{t\geq 1}v_{t}\] (4)

Recall that w.l.o.g. feature maps are bounded everywhere \(\left\|\phi_{j}(\cdot)\right\|_{2}\leq 1\), and \(\operatorname{rank}(\Phi_{j})\leq d\) which allows for the following matrix inequalities,

\[\operatorname{tr}(\Phi_{t,j}^{\top}\Phi_{t,j})=\operatorname{tr}(\Phi_{t,j} \Phi_{t,j}^{\top})=\sum_{i=1}^{t}\phi_{j}^{\top}(x_{i})\phi_{j}(x_{i})\leq t\]\[\left\|\Phi_{t,j}\Phi_{t,j}^{\top}\right\| \leq\mathrm{tr}(\Phi_{t,j}\Phi_{t,j}^{\top})\leq t\] \[\left\|\Phi_{t,j}\Phi_{t,j}^{\top}\right\| \leq\left\|\Phi_{t,j}\Phi_{t,j}^{\top}\right\|_{\mathrm{Fr}}\leq \sqrt{d}\big{\|}\Phi_{t,j}\Phi_{t,j}^{\top}\big{\|}\leq t\sqrt{d}\]

Therefore,

\[\left\|\bm{v}_{t}\right\|=\big{\|}\Phi_{t,j}\Phi_{t,j}^{\top}\big{\|}_{\mathrm{ Fr}}/t\leq\sqrt{d},\quad\max_{t\geq 1}v_{t}=\max_{t\geq 1}\big{\|}\Phi_{t,j} \Phi_{t,j}^{\top}\big{\|}/t\leq 1.\]

For Eq. (4) to hold, is suffices that for all \(t\geq 1\),

\[\lambda\geq\frac{2\sigma}{\sqrt{t}}\sqrt{1+\frac{5}{2\sqrt{2}}\sqrt{4d\left( \log(2M/\delta)+(\log\log d)_{+}\right)}+\frac{12}{\sqrt{2}}\left(\log(2M/ \delta)+(\log\log d)_{+}\right)}.\]

Therefore, if \(\lambda_{t}\) are chosen to satisfy the above inequality, each \(A_{j}^{c}\) happens with probability less than \(\delta/M\). Then by applying union bound, \(\cup_{j=1}^{M}A_{j}^{c}\) happens with probability less than \(\delta\). 

Proof of Theorem 3.: The theorem statement requires that the regularization parameter \(\lambda_{t}\) is chosen such that condition of Lemma 5 is met, and therefore event \(A\) happens with probability \(1-\delta\). Throughout this proof, which adapts the analysis of Theorem 3.1. Lounici et al. (2011) to the time-uniform setting, we condition on \(A\) happening, and later incorporate the probability.

**Step 1.** Let \(\hat{\bm{\theta}}_{t}\) be a minimizer of \(\mathcal{L}\) and \(\bm{\theta}\) be the true coefficients vector, then \(\mathcal{L}(\hat{\bm{\theta}}_{t};H_{t},\lambda_{t})\leq\mathcal{L}(\bm{ \theta};H_{t},\lambda_{t})\). Writing out the loss and re-ordering the inequality we obtain,

\[\frac{1}{t}\Big{\|}\Phi_{t}(\hat{\bm{\theta}}_{t}-\bm{\theta})\Big{\|}_{2}^{2} \leq\frac{2}{t}\bm{\varepsilon}_{t}^{T}\Phi_{t}(\hat{\bm{\theta}}_{t}-\bm{ \theta})+2\lambda_{t}\sum_{j=1}^{M}\left(\left\|\bm{\theta}_{j}\right\|_{2}- \left\|\hat{\bm{\theta}}_{t,j}\right\|_{2}\right).\]

which is often referred to as the Basic inequality (Buhlmann and Van De Geer, 2011). By Cauchy-Schwarz and conditioned on event \(A\),

\[\bm{\varepsilon}_{t}^{T}\Phi_{t}(\hat{\bm{\theta}}_{t}-\bm{\theta})\leq\sum_{ j=1}^{M}\bigl{\|}(\Phi_{t}^{T}\bm{\varepsilon}_{t})_{j}\bigr{\|}_{2}\left\| \hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j}\right\|_{2}\leq\frac{t\lambda}{2}\sum_ {j=1}^{M}\Bigl{\|}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j}\Bigr{\|}_{2}\]

then adding \(\lambda_{t}\sum_{j=1}^{M}\bigl{\|}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j} \Bigr{\|}_{\text{to}}\) to both sides, applying the triangle inequality, and recalling from Section 3 that \(\bm{\theta}_{j}=0\)for \(j\neq j^{\star}\) gives

\[\frac{1}{t}\Bigl{\|}\Phi_{t}(\hat{\bm{\theta}}_{t}-\bm{\theta}) \Bigr{\|}_{2}^{2}+\lambda_{t}\sum_{j=1}^{M}\Bigl{\|}\hat{\bm{\theta}}_{t,j}- \bm{\theta}_{j}\Bigr{\|}_{2} \leq 2\lambda_{t}\sum_{j=1}^{M}\Bigl{\|}\hat{\bm{\theta}}_{t,j}- \bm{\theta}_{j}\Bigr{\|}_{2}+2\lambda_{t}\sum_{j=1}^{M}\Bigl{(}\left\|\bm{ \theta}_{j}\right\|_{2}-\left\|\hat{\bm{\theta}}_{t,j}\right\|_{2}\Bigr{)}\] \[\leq 4\lambda_{t}\Bigl{\|}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j^{ \star}}\Bigr{\|}_{2}.\]

Since each term on the left hand side is positive, then each is also individually smaller than the right hand side, and we obtain,

\[\frac{1}{t}\Bigl{\|}\Phi_{t}(\hat{\bm{\theta}}_{t}-\bm{\theta}) \Bigr{\|}_{2}^{2} \leq 4\lambda_{t}\Bigl{\|}\hat{\bm{\theta}}_{t,j^{\star}}-\bm{ \theta}_{j^{\star}}\Bigr{\|}_{2}\] (5) \[\sum_{\begin{subarray}{c}j=1\\ j\neq j^{\star}\end{subarray}}^{M}\Bigl{\|}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j} \Bigr{\|}_{2} \leq 3\Bigl{\|}\hat{\bm{\theta}}_{t,j},-\bm{\theta}_{j^{\star}} \Bigr{\|}_{2}\] (6)

**Step 2.** Consider a sequence \((c_{1},\dots,c_{k},\dots)\), where \(c_{1}\geq\dots\geq c_{k}\dots\), then

\[c_{k}\leq\frac{1}{k}(kc_{k}+\sum_{i>k}c_{i})\leq\sum_{i\geq 1}\frac{c_{i}}{k}.\] (7)

Define \(J_{1}=\{j^{\star}\}\) and \(J_{2}=\{j^{\star},j^{\prime}\}\) where

\[j^{\prime}=\underset{\begin{subarray}{c}j\in[M]\\ j\neq j^{\star}\end{subarray}}{\arg\max}\Bigl{\|}\hat{\bm{\theta}}_{t,j}-\bm{ \theta}_{j}\Bigr{\|}_{2}.\]For any \(J\subset[M]\) the complementing set is denoted as \(J^{c}=[M]\setminus J\). For simplicity let \(c_{j}=\left\lVert\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j}\right\rVert_{2}\), and let \(\pi(k)\) denote the index of the \(k\)-th largest element of \(\{c_{j}:\,j\in J_{1}^{c}\}\). By definition of \(J_{2}^{c}\) we have,

\[\sum_{j\in J_{2}^{c}}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{ \theta}_{j}\Bigr{\rVert}_{2}^{2} =\sum_{\begin{subarray}{c}k>1\\ \pi(k)\in J_{1}^{c}\end{subarray}}c_{k}^{2}\overset{(\reflectbox{1})}{\leq} \sum_{\begin{subarray}{c}k>1\\ \pi(k)\in J_{1}^{c}\end{subarray}}\frac{(\sum_{i\in J_{1}^{c}}c_{i})^{2}}{k^{2}}\] \[\leq\big{(}\sum_{i\in J_{1}^{c}}c_{i}\big{)}^{2}\sum_{ \begin{subarray}{c}k>1\\ \pi(k)\in J_{1}^{c}\end{subarray}}\frac{1}{k^{2}}\overset{(\reflectbox{1})}{ \leq}9c_{j}^{2},\] \[\leq 9(c_{j^{*}}^{2}+c_{j^{*}}^{2})=9\sum_{j\in J_{2}}\Bigl{\lVert} \hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j}\Bigr{\rVert}_{2}^{2},\]

which, in turn, gives

\[\Bigl{\lVert}\hat{\bm{\theta}}_{t}-\bm{\theta}\Bigr{\rVert}_{2}=\sqrt{\sum_{ j=1}^{M}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j}\Bigr{\rVert}_{2} ^{2}}\leq\sqrt{10\sum_{j\in J_{2}}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{ \theta}_{j}\Bigr{\rVert}_{2}^{2}}.\] (8)

**Step 3.** On the other hand, due to (6), and by definition of \(J_{2}\) it also holds that

\[\sum_{j\in J_{2}^{c}}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j} \Bigr{\rVert}_{2}\leq 3\sum_{j\in J_{2}}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{ \theta}_{j}\Bigr{\rVert}_{2}.\]

From the theorem assumptions and Definition 2, we know that there exists \(0<\kappa(\Phi_{t},2)\), therefore by Definition 2, the feature matrix \(\Phi_{t}\) satisfies,

\[\sum_{j\in J_{2}}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{\theta }_{j}\Bigr{\rVert}_{2}^{2} \leq\frac{1}{t\kappa^{2}(\Phi_{t},2)}\Bigl{\lVert}\Phi_{t}(\hat{ \bm{\theta}}_{t}-\bm{\theta})\Bigr{\rVert}_{2}^{2}\] \[\overset{(\reflectbox{1})}{\leq}\frac{1}{\kappa^{2}(\Phi_{t},2)}4 \lambda_{t}\Bigl{\lVert}\hat{\bm{\theta}}_{t,j^{*}}-\bm{\theta}_{j^{*}}\Bigr{\rVert} _{2}\leq\frac{1}{\kappa^{2}(\Phi_{t},2)}4\lambda_{t}\sqrt{\sum_{j\in J_{2}} \Bigl{\lVert}\hat{\bm{\theta}}_{t,j}-\bm{\theta}_{j}\Bigr{\rVert}_{2}^{2}}.\]

From here, by applying (8) we get,

\[\Bigl{\lVert}\hat{\bm{\theta}}_{t}-\bm{\theta}\Bigr{\rVert}_{2}\leq\frac{4 \sqrt{10}\lambda_{t}}{\kappa^{2}(\Phi_{t},2)}.\]

If \(\lambda_{t}\) are chosen according to Lemma 5, event \(A\) and, in turn, the inequality above hold with probability greater than \(1-\delta\). 

## Appendix C Results on Exploration

In this section we present lower-bounds on the eigenvalues of the covariance matrix \(\Phi_{t}\Phi_{t}^{\top}\), as it is later used in our regret analysis. In particular, we show that the feature matrix \(\Phi_{t}\) satisfies the restricted eigenvalue condition (Definition 2) required for valid Lasso confidence set (Theorem 3), and calculate a lower bound on \(\kappa(\Phi_{t},2)\). The lower bound is later used by Lemma 19 and Lemma 20 to develop the model selection regret. We show this bound in three steps.

Equivalent to Definition 2, we write \(\kappa(\Phi_{t},s)=\inf_{b\in\Xi_{s}}\|\Phi_{t}\bm{b}\|_{2}/\sqrt{t}\) where

\[\Xi_{s}\coloneqq\Bigl{\{}b\in\mathbb{R}^{d_{\setminus}\{0\}}\Bigl{|}\sum_{j \notin J}\|b_{j}\|_{2}\leq 3\sum_{j\in J}\|b_{j}\|_{2},\,\sqrt{\sum_{j\in J}\|b_{j}\|_{2}^{2}} \leq 1\text{ s.t. }J\subset\{1,\dots,M\},\;|J|\leq s.\Bigr{\}}.\] (9)

For simplicity in notation, we further define

\[\tilde{\kappa}(A,s)\coloneqq\min_{b\in\Xi_{s}}b^{\top}Ab.\] (10)

since \(\tilde{\kappa}(\frac{\Phi_{t}^{\top}\Phi_{t}}{t},s)=\kappa^{2}(\Phi_{t},s)\).

**Step I.** Consider the exploratory steps at which \(\alpha_{t}=1\). Let \(\Phi_{\pi,t}\) be a sub-matrix of \(\Phi_{t}\) where only rows from exploratory steps are included. Note that \(\Phi_{\pi,t}\in\mathbb{R}^{t^{\prime}\times dM}\) is a random matrix, where the number of rows \(t^{\prime}\) are also random. We show that \(\kappa^{2}(\Phi_{t},s)\) is lower bounded by \(\kappa^{2}(\Phi_{t,\pi},s)\).

**Lemma 6**.: _Suppose \(\Phi_{\pi,t}\) has \(t^{\prime}\) rows. Then,_

\[\kappa^{2}(\Phi_{t},s)\geq\frac{t^{\prime}}{t}\kappa^{2}(\Phi_{\pi,t},s)\]

Proof of Lemma 6.: Let \(\Psi^{(t)}=\bm{\phi}(\bm{x}_{t})\bm{\phi}^{\top}(\bm{x}_{t})\in\mathbb{R}^{dM \times dM}\) for all \(t=1,\ldots,n\). Note that \(\Psi^{(t)}\) is positive semi-definite by construction. We have,

\[\left\|\Phi_{t}b\right\|_{2}^{2}=\sum_{s=1}^{t}b^{\top}\Psi^{(s)}b=\sum_{s\in T _{\pi}}^{t}b^{\top}\Psi^{(s)}b+\sum_{s\notin T_{\pi}}^{t}b^{\top}\Psi^{(s)}b\]

where the set \(T_{\pi}\) contains the indices of the exploratory steps at which the action is selected according to \(\pi\). Therefore,

\[\kappa^{2}(\Phi_{t},s) =\frac{1}{t}\min_{b\in\Xi_{s}}\left\|\Phi_{t}\bm{b}\right\|_{2}^{2}\] \[=\min_{b\in\Xi_{s}}\frac{1}{t}\sum_{s\in T_{\pi}}b^{\top}\Psi^{(s )}b+\frac{1}{t}\sum_{s\notin T_{\pi}}b^{\top}\Psi^{(s)}b\] \[\geq\min_{b\in\Xi_{s}}\frac{1}{t}\sum_{s\in T_{\pi}}b^{\top}\Psi^ {(s)}b\]

where the last inequality holds due to \(\Psi^{(s)}\) being PSD. Then we have,

\[\kappa^{2}(\Phi_{t},s)\geq\min_{b\in\Xi_{s}}b^{\top}\left(\frac{1}{t}\sum_{s \in T_{\pi}}^{t}\Psi^{(s)}\right)b=\frac{\left|T_{\pi}\right|}{t}\kappa^{2}( \Phi_{\pi,t},s)\]

While the number of rows of \(\Phi_{\pi,t}\) is a random variable, we continue to condition on the event that \(\Phi_{\pi,t}\) has \(t^{\prime}\) rows, and investigate the distribution of its restricted eigenvalues.

**Step II.** The restricted eigenvalues of the _exploratory_ submatrix are well bounded away from zero.

**Lemma 7**.: _Let \(\pi\) be the solution to (2), and \(s\in\mathbb{N}\). Suppose \(\Phi_{\pi,t}\) has \(t^{\prime}\) rows. Then for all \(\delta>0\),_

\[\mathbb{P}\left(\forall t^{\prime}:\,\kappa^{2}(\Phi_{\pi,t},s)\geq\tilde{ \kappa}(\Sigma,s)-\frac{80s}{\sqrt{t^{\prime}}}\sqrt{\log(2Md/\delta)+(\log \log 4t^{\prime})_{+}}\right)\geq 1-\delta\]

_where \(\Sigma=\Sigma(\pi,\bm{\phi})\coloneqq\mathbb{E}_{\bm{x}\sim\bm{\phi}}(\bm{x}) \bm{\phi}^{\top}(\bm{x})\) and \(\tilde{\kappa}\) is defined in (10)._

**Step III.** Remains to combine the two above lemmas and incorporate a high probability bound on \(t^{\prime}\), showing that it is close to \(\sum_{s=1}^{t}\gamma_{s}\).

**Lemma 8**.: _There exist absolute constants \(C_{1},C_{2}\) which satisfy,_

\[\mathbb{P}\left(\forall t\geq 1:\ \kappa^{2}(\Phi_{t},2)\geq C_{1}\tilde{ \kappa}(\Sigma,2)t^{-1/4}-C_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+ }}\right)\geq 1-\delta\]

_if \(\gamma_{t}=\mathcal{O}(t^{-1/4})\). Let \(\pi\) be the solution to (2), then it further holds that_

\[\mathbb{P}\left(\forall t\geq 1:\ \kappa^{2}(\Phi_{t},2)\geq C_{1}C_{\min}t^{-1/4 }-C_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\geq 1-\delta\]

The regret analysis of Hao et al. (2020) also relies on connecting \(\kappa(\Phi_{t},s)\) to \(C_{\min}\), and for this, they use Theorem 2.4 of Javanmard and Montanari (2014). This theorem states that there exists a problem-dependent constant \(C_{1}\) for which \(\kappa^{2}(\Phi_{t},s)\geq C_{1}C_{\min}\) with high probability, if \(t\geq n_{0}\) and roughly \(n_{0}=\mathcal{O}(\sqrt[3]{n^{2}\log M})\). We highlight that Lemma 8, presents a lower-bound which holds for all \(t\geq 1\), however this comes at the cost of getting a looser lower bound than the result of Javanmard and Montanari (2014) for the larger time steps \(t\). In fact, due to the sub-optimal dependency of Lemma 8 on \(t\), we later obtain sub-optimal dependency on the horizon for the case when \(n\gg M\). It is unclear to us if this rate can be improved without assuming knowledge of \(n\), or that \(n\geq n_{0}\).

For the last lemma in this section we show that the empirical sub-matrices \(\Phi_{t,j}\) are also bounded away from zero. This will be required later to prove Lemma 15.

**Lemma 9** (Base Model \(\lambda_{\min}\) Bound).: _Assume \(\pi\) is the maximizer of Eq. (2). Then, with probability greater than \(1-\delta\), simultaneously for all \(j=1,\ldots,M\) and \(t\geq 1\),_

\[\lambda_{\min}(\Phi_{t,j}^{\top}\Phi_{t,j})\geq C_{1}C_{\min}t^{3/4}-C_{2}t^{3/ 8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\]

_if \(\gamma_{t}=\mathcal{O}(t^{-1/4})\)._

### ALExp with Uniform Exploration

We presented our main regret bound (Theorem 1) in terms of \(C_{\min}\), which only depends on properties of the feature maps and the action domain. We give a lower-bound on \(C_{\min}\) for a toy scenario which corresponds to the problem of linear feature selection over convex action sets.

**Proposition 10** (Invertible Features).: _Suppose \(\bm{\phi}(\bm{x})\coloneqq A\bm{x}:\mathbb{R}^{d}\to\mathbb{R}^{d}\) is an invertible linear map, and \(\mathcal{X}\in\mathbb{R}^{d}\) is a convex body. Then,_

\[C_{\min}\geq\frac{\lambda_{\min}(A)}{\lambda_{\max}^{2}(T)}>0\]

_where \(T\) is the transformation which maps \(\mathcal{X}\) to an isotropic body._

The lower-bound of Proposition 10 is achieved by simply exploring via \(\pi=\mathrm{Unif}(\mathcal{X})\). Inspired by Schur et al. (2023, Lemma E.13), we show that even for non-convex action domains and orthogonal feature maps, the uniform exploration yields a constant lower-bound on restricted eigenvalues.

**Proposition 11** (Orthonormal Features).: _Suppose \(\bm{\phi}_{j}:\mathcal{X}\to\mathbb{R}\) are chosen from an orthogonal basis of \(L^{2}(\mathcal{X})\), and satisfy \(\|\bm{\phi}_{i}\|_{L^{2}_{\mu}(\mathcal{X})}/\text{Vol}(\mathcal{X})\geq 1\). Then there exist absolute constants \(C_{1}\) and \(C_{2}\) for which the exploration distribution \(\pi=\mathrm{Unif}(\mathcal{X})\) satisfies_

\[\mathbb{P}\left(\forall t\geq 1:\ \kappa^{2}(\Phi_{t},2)\geq C_{1}t^{-1/4}-C_{2 }t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\geq 1-\delta.\]

The \(d=1\) condition is met without loss of generality, by splitting the higher dimensional feature maps and introducing more base features, which will increase \(M\). Moreover, the orthonormality condition is met by orthogonalizing and re-scaling the feature maps. Basis functions such as Legendre polynomials and Fourier features (Rahimi et al., 2007) satisfy these conditions.

By invoking Proposition 11, instead of Lemma 8 in the proof of Theorem 1, we obtain the regret of ALExp with uniform exploration.

**Corollary 12** (\(\mathrm{ALExp}\) with Uniform Exploration).: _Let \(\delta\in(0,1]\). Suppose \(\bm{\phi}_{j}:\mathcal{X}\to\mathbb{R}\) are chosen from an orthogonal basis of \(L^{2}(\mathcal{X})\), and satisfy \(\|\bm{\phi}_{i}\|_{L^{2}_{\mu}(\mathcal{X})}/\text{Vol}(\mathcal{X})\geq 1\). Assume the oracle agent employs a UCB or a Greedy policy, as laid out in Section 5. Choose \(\eta_{t}=\mathcal{O}(1/\sqrt{t}C(M,\delta,d))\) and \(\gamma_{t}=\mathcal{O}(t^{-1/4})\) and \(\lambda_{t}=\mathcal{O}(C(M,\delta,d)/\sqrt{t})\), then \(\mathrm{ALExp}\) with uniform exploration \(\pi=\mathrm{Unif}(\mathcal{X})\) attains the regret_

\[R(n)=\mathcal{O}\Big{(}Bn^{3/4}+\sqrt{n}C(M,\delta,d)\log M+B^{ 2}\sqrt{n}+B\sqrt{n\left((\log\log nB^{2})_{+}+\log(1/\delta)\right)}\] \[\qquad\qquad\qquad\qquad+(n^{3/4}+\log n)C(M,\delta,d)+n^{5/8} \sqrt{d\log n+\log(1/\delta)+B^{2}}\Big{)}\]

_with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\). Here,_

\[C(M,\delta,d)=\mathcal{O}\left(\sqrt{1+\sqrt{d\left(\log(M/\delta)+(\log\log d )_{+}\right)}+(\log(M/\delta)+(\log\log d)_{+})}\right).\]

### Proof of Results on Exploration

As an intermediate step, we consider the restricted eigenvalue property of the empirical covariance matrix. Given \(t^{\prime}\) samples, the empirical estimate of \(\Sigma\) is

\[\hat{\Sigma}_{t^{\prime}}\coloneqq\frac{1}{t^{\prime}}\sum_{s=1}^{t^{\prime}} \bm{\phi}(\bm{x}_{s})\bm{\phi}^{\top}(\bm{x}_{s})\] (11)

where \(\bm{x}_{s}\) are sampled according to \(\pi\). We show that every entry of \(\hat{\Sigma}_{t^{\prime}}\) is close to the corresponding entry in \(\Sigma\), and later use it in the proofs of eigenvalue lemmas.

**Lemma 13** (Anytime Bound for The Entries of Empirical Covariance Matrix).: _Let \(\hat{\Sigma}_{t^{\prime}}\) be the empirical covariance matrix corresponding to \(\Sigma(\pi,\bm{\phi})\) given \(t^{\prime}\) samples. Then,_

\[\mathbb{P}\left(\exists t^{\prime}:\ d_{\infty}(\Sigma,\hat{\Sigma}_{t^{\prime} })\geq\frac{5}{\sqrt{t^{\prime}}}\sqrt{\left((\log\log 4t^{\prime})_{+}+\log(2 Md/\delta)\right)}\right)\leq\delta\]

_where \(d_{\infty}(A,B)\coloneqq\max_{i,j}\lvert A_{i,j}-B_{i,j}\rvert\)._

Proof of Lemma 13.: We show the element-wise convergence of \(\Sigma\) to \(\hat{\Sigma}_{t}\) for the \((i,j)\) entry where \(i,j=1,\ldots,dM\). Consider the random sequence \(X_{s}\coloneqq\Sigma_{i,j}-\phi_{i}(\bm{x}_{s})\phi_{j}(\bm{x}_{s})\). We show that \(X_{1},\ldots,X_{n}\) satisfies conditions of Lemma 26. We first observe that

\[\mathbb{E}[X_{s}|X_{1:s-1}]=\mathbb{E}X_{s}=\Sigma(i,j)-\mathbb{E}_{\bm{x} \sim\pi}\phi_{i}(\bm{x})\phi_{j}(\bm{x})=0\]

since by definition \(\Sigma_{i,j}=\mathbb{E}_{\bm{x}\sim\pi}\phi_{i}(\bm{x})^{\top}\phi_{j}(\bm{x})\). Moreover, we have normalized features \(\lVert\bm{\phi}(\cdot)\rVert\leq 1\), therefore, each entry \(\phi_{i}(\cdot)\phi_{j}(\cdot)\) is also bounded, yielding \(\lvert X_{s}\rvert\leq 2\). Then Lemma 26 implies that for all \(\tilde{\delta}>0\),

\[\mathbb{P}\left(\exists t^{\prime}:\ \frac{1}{t^{\prime}}\sum_{s=1}^{t^{ \prime}}X_{s}\geq\frac{5}{\sqrt{t^{\prime}}}\sqrt{\left((\log\log 4t^{\prime})_{+}+ \log(2/\tilde{\delta})\right)}\right)\leq\tilde{\delta}.\]

Setting \(\tilde{\delta}=\delta/(dM)\) and taking a union bound over all indices concludes the proof. 

We are now ready to present the proofs to the lemmas in Appendix C.

Proof of Lemma 7.: By (11) we have \(\hat{\Sigma}_{t^{\prime}}=\frac{\Phi_{\pi,t}^{\top}\Phi_{\pi,t}}{t^{\prime}}\), and thereby

\[\kappa^{2}(\Phi_{\pi,t},s)=\min_{b\in\Xi_{s}}b^{\top}\hat{\Sigma}_{t^{\prime} }b=\tilde{\kappa}(\hat{\Sigma}_{t^{\prime}},s).\]

Inspired by Lemma 10.1 in van de Geer and Buhlmann (2009), we show that element-wise closeness of matrices \(\Sigma\) and \(\hat{\Sigma}_{t^{\prime}}\) (c.f. Lemma 13) implies closeness in \(\tilde{\kappa}\):

\[\left\lvert\kappa^{2}(\Phi_{\pi,t},s)-\tilde{\kappa}(\Sigma,s)\right\rvert =\left\lvert\tilde{\kappa}(\hat{\Sigma}_{t^{\prime}},s)-\tilde{ \kappa}(\Sigma,s)\right\rvert\] \[=\left\lvert\tilde{\kappa}\left(\hat{\Sigma}_{t^{\prime}}-\Sigma,s\right)\right\rvert\] \[\leq\min_{b\in\Xi_{s}}d_{\infty}(\Sigma,\hat{\Sigma}_{t^{\prime} })\lVert b\rVert_{1}^{2}\]

where the last line holds due to Holder's. Moreover, since \(b\in\Xi_{s}\), for any \(J\subset[dM]\) where \(\lvert J\rvert\leq s\) it additionally holds that \(\lVert b_{J}\rVert_{2}\leq 1\) and

\[\lVert b\rVert_{1}\leq(1+3)\lVert b_{J}\rVert_{1}\leq 4\sqrt{s}\lVert b_{J} \rVert_{2}\leq 4\sqrt{s}\]

which gives,

\[\kappa^{2}(\Phi_{\pi,t},s)\geq\tilde{\kappa}(\Sigma,s)-16sd_{\infty}(\Sigma, \hat{\Sigma}_{t^{\prime}}).\]

Therefore by Lemma 13,

\[\kappa^{2}(\Phi_{\pi,t},s)\geq\tilde{\kappa}(\Sigma,s)-\frac{80s}{\sqrt{t^{ \prime}}}\sqrt{\left((\log\log 4t^{\prime})_{+}+\log(2Md/\delta)\right)}\] (12)

with probability greater than \(1-\delta\), simultaneously for all \(t^{\prime}\geq 1\). 

Proof of Lemma 8.: In Lemma 6 we showed that

\[\kappa^{2}(\Phi_{t},s)\geq\frac{t^{\prime}}{t}\kappa^{2}(\Phi_{\pi,t},s)\]

where \(t^{\prime}\) indicates the number of rows in the exploratory sub-matrix of \(\Phi_{t}\). Recall that \(t^{\prime}=\sum_{s=1}^{t}\alpha_{s}\) where \(\alpha_{s}\) are i.i.d Bernoulli random variables with success probability of \(\gamma_{s}\). Due to Lemma 24,

\[\mathbb{P}\left(\forall t\geq 1:\ \lvert t^{\prime}-\Gamma_{t}\rvert\leq\Delta_{t }\right)\geq 1-\delta/2\] (13)where

\[\Delta_{t}\coloneqq\frac{5}{2}\sqrt{\frac{(\log\log t)_{+}+\log(8/\delta)}{t}}, \quad\Gamma_{t}\coloneqq\sum_{s=1}^{t}\gamma_{s}\]

Due to Lemma 7, with probability greater than \(1-\delta/2\) the following holds for all \(t\geq 1\)

\[\kappa^{2}(\Phi_{t},2) \geq\frac{t^{\prime}}{t}\tilde{\kappa}(\Sigma,2)-\frac{160\sqrt{t ^{\prime}}}{t}\sqrt{(\log\log 4t^{\prime})_{+}+\log(4Md/\delta)}\] \[\geq\frac{\Gamma_{t}-\Delta_{t}}{t}\tilde{\kappa}(\Sigma,2)-160 \sqrt{\frac{\Gamma_{t}+\Delta_{t}}{t^{2}}}\sqrt{(\log\log\left(4\Gamma_{t}+ \Delta_{t}\right))_{+}+\log(4Md/\delta)}\]

where the second inequality holds with probability \(1-\delta\), by incorporating (13) and taking a union bound. For the rest of the proof and to keep the calculations simple, we ignore the values of the absolute constants. We use the notation \(g(t)=o(f(t))\) to show that \(f(t)\) grows much faster than \(g(t)\). More formally, if for every constant \(c\) there exists \(t_{0}\), where \(g(t)\leq c|f(t)|\) for all \(t\geq t_{0}\). Since \(\gamma_{s}=\mathcal{O}(s^{-1/4})\) there exists \(C\) such that \(\Gamma_{t}=Ct^{3/4}\), then it is straightforward to observe that there exists absolute constants \(\tilde{C}_{i}\) which satisfy,

\[\kappa^{2}(\Phi_{t},2) \geq\tilde{C}_{1}t^{-1/4}\tilde{\kappa}(\Sigma,2)-\frac{5t^{-3/2} \tilde{\kappa}(\Sigma,2)}{2}\sqrt{(\log\log t)_{+}+\log(8/\delta)}\] \[\quad-\tilde{C}_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+} }-o\left(t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\] \[\geq\tilde{C}_{1}t^{-1/4}\tilde{\kappa}(\Sigma,2)-\tilde{C}_{3}t ^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\]

The last inequality holds since \(t^{-3/2}\sqrt{\log\log t}=o(t^{-5/8}\sqrt{\log\log t})\). The above chain of inequalities imply that there exist absolute constants \(C_{1},C_{2}\), for which

\[\mathbb{P}\left(\forall t\geq 1:\;\kappa^{2}(\Phi_{t},2)\geq C_{1}\tilde{ \kappa}(\Sigma,2)t^{-1/4}-C_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+ }}\right)\geq 1-\delta.\]

If \(\pi\) is chosen according to (2), then \(\tilde{\kappa}(\Sigma,2)\geq C_{\min}\) yielding the lemma's second argument. 

Proof of Lemma 9.: Fix \(j\in\{1,\ldots,M\}\), and construct the set

\[\Xi_{1,j}=\left\{\boldsymbol{b}\in\mathbb{R}^{d}\setminus\{0\}\Big{|} \boldsymbol{b}=(\boldsymbol{b}_{1},\ldots,\boldsymbol{b}_{M}),\text{ s.t. }\boldsymbol{b}_{j}\in\mathbb{R}^{d},\left\|\boldsymbol{b}_{j}\right\|_{2}\leq 1 \text{ and }\forall j^{\prime}\neq j:\;\boldsymbol{b}_{j^{\prime}}=0\right\}.\]

Note that \(\Xi_{1,j}\subset\Xi_{s}\). Therefore,

\[\inf_{b\in\Xi_{1,j}}\left\|\Phi_{t}\boldsymbol{b}\right\|_{2}\geq\inf_{b\in \Xi_{s}}\left\|\Phi_{t}\boldsymbol{b}\right\|_{2}=\sqrt{t}\kappa(\Phi_{t},s).\]

Moreover, by construction of \(\Xi_{1,j}\) we have for all \(\boldsymbol{b}\in\Xi_{1,j}\) that \(\Phi_{t}\boldsymbol{b}=\Phi_{t,j}\boldsymbol{b}_{j}\), therefore,

\[\inf_{b\in\Xi_{1,j}}\left\|\Phi_{t}\boldsymbol{b}\right\|_{2}^{2}=\inf_{ \begin{subarray}{c}\boldsymbol{b}_{j}\in\mathbb{R}^{d}\\ \left\|\boldsymbol{b}_{j}\right\|_{2}^{2}\leq 1\end{subarray}}\left\|\Phi_{t,j} \boldsymbol{b}_{j}\right\|_{2}^{2}=\lambda_{\min}(\Phi_{t,j}^{\top}\Phi_{t,j}).\]

From the above equations we conclude that \(\lambda_{\min}(\Phi_{t,j}^{\top}\Phi_{t,j})\geq t\kappa^{2}(\Phi_{t,s})\), for all \(j=1,\ldots,M\). Therefore, using Lemma 8 we obtain that there exists \(C_{1},C_{2}\) such that

\[\mathbb{P}\left(\forall t\geq 1,\,j=1,\ldots,M:\;\lambda_{\min}(\Phi_{t,j}^{\top} \Phi_{t,j})\geq C_{1}C_{\min}t^{3/4}-C_{2}t^{3/8}\sqrt{\log(Md/\delta)+(\log \log t)_{+}}\right)\geq 1-\delta\]

Proof of Proposition 10.: Since \(\mathcal{X}\) is a convex body, then there exists an invertible map \(T\), such that \(T(\mathcal{X})\) is an isotropic body [e.g. Proposition 1.1.1, Giannopoulos, 2003]. Then by definition, \(\bar{X}\sim\mathrm{Unif}(T(\mathcal{X}))\) is an isotropic distribution and \(\mathrm{Cov}(\bar{X})=I_{d}\)[e.g., c.f. Chapter 3.3.5 Vershynin, 2018]. Since \(\boldsymbol{\phi}\) is linear and invertible, it may be written is as \(\boldsymbol{\phi}(\boldsymbol{x})=A\boldsymbol{x}\), where \(A\) is an invertible matrix. Therefore,

\[\Sigma(\pi,\boldsymbol{\phi})=\mathrm{Cov}(\boldsymbol{\phi}(X))=A^{\top} \mathrm{Cov}(X)A=A^{\top}\mathrm{Cov}\left(T^{-1}\bar{X}\right)A=A^{\top}(T^ {-1})^{2}A.\]

As for the minimum eigenvalue, suppose \(\boldsymbol{v}\in\mathbb{R}^{d}\) and \(\left\|\boldsymbol{v}\right\|=1\), then

\[C_{\min}\geq\lambda_{\min}\left(\Sigma(\pi,\boldsymbol{\phi})\right)\geq \boldsymbol{v}^{\top}A^{\top}(T^{-1})^{2}A\boldsymbol{v}\geq\left\|A\boldsymbol {v}\right\|_{2}\lambda_{\min}(T^{-2})=\frac{\left\|A\boldsymbol{v}\right\|_{2}}{ \lambda_{\max}^{2}(T)}\geq\frac{\lambda_{\min}(A)}{\lambda_{\max}^{2}(T)}.\]Proof of Proposition 11.: By the assumption of the proposition, for all \(i\in M\)

\[[\Sigma(\pi,\bm{\phi})]_{i,i}=\mathbb{E}_{\bm{x}\sim\pi}\phi_{i}^{2}(\bm{x})= \frac{1}{\operatorname{Vol}(\mathcal{X})}\int_{\mathcal{X}}\phi_{i}^{2}(\bm{x}) \mathrm{d}\mu(\bm{x})\geq 1\]

and for all \(i\neq j\),

\[[\Sigma(\pi,\bm{\phi})]_{i,j}=\mathbb{E}_{\bm{x}\sim\pi}\phi_{i}(\bm{x})\phi_{j }(\bm{x})=\frac{1}{\operatorname{Vol}(\mathcal{X})}\int_{\mathcal{X}}\phi_{i}( \bm{x})\phi_{j}(\bm{x})\mathrm{d}\mu(\bm{x})=0\]

We use \(\Sigma=\Sigma(\pi,\bm{\phi})\). For any \(\bm{b}\in\mathbb{R}^{Md}\) where \(\|\bm{b}\|\leq 1\),

\[\bm{b}^{\top}\Sigma\bm{b}=\sum_{i,j\in[M]}\bm{b}_{j}^{\top}\Sigma_ {i,j}\bm{b}_{i} =\sum_{i\in[M]}\bm{b}_{i}^{\top}\Sigma_{i,i}\bm{b}_{i}+\sum_{i,j \in[M],i\neq j}\bm{b}_{j}^{\top}\Sigma_{i,j}\bm{b}_{i}\] \[=\sum_{i\in[M]}\bm{b}_{i}^{\top}\Sigma_{i,i}\bm{b}_{i}\] \[\geq 1\sum_{i\in[M]}\|\bm{b}_{i}\|_{2}^{2}\geq 1.\]

Which implies,

\[\tilde{\kappa}(\Sigma,s)=\min_{\bm{b}\in\Xi_{s}}\bm{b}^{\top}\Sigma\bm{b}\geq 1.\]

By Lemma 8, there exist absolute constants \(C_{1}\) and \(C_{2}\) for which,

\[\mathbb{P}\left(\forall t\geq 1:\ \kappa^{2}(\Phi_{t},2)\geq\tilde{\kappa}( \Sigma,2)C_{1}t^{-1/4}-C_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}} \right)\geq 1-\delta.\]

concluding the proof.

## Appendix D Proof of Regret Bound

**Theorem 14** (Anytime Regret, Formal).: _Let \(\delta\in(0,1]\) and \(\pi\) be the maximizer of (2). Assume the oracle agent employs a UCB or a Greedy policy, as laid out in Section 5. Suppose \(\eta_{t}=\mathcal{O}(C_{\min}t^{-1/2}C(M,\delta,d))\) and \(\gamma_{t}=\mathcal{O}(t^{-1/4})\) and \(\lambda_{t}=\mathcal{O}(C(M,\delta,d)t^{-1/2})\), then exists absolute constants \(C_{1},\dots,C_{6}\) for which \(\textsc{ALExp}\) attains the regret_

\[R(n) \leq C_{1}Bn^{3/4}+C_{2}\sqrt{n}C_{\min}^{-1}C(M,\delta,d)\log M\] \[+C_{3}B^{2}C_{\min}\sqrt{n}+C_{4}B\sqrt{n\left((\log\log nB^{2})_ {+}+\log(1/\delta)\right)}\] \[+C_{5}\left(1+C_{\min}^{-1}n^{-3/8}\sqrt{\log(Md/\delta)+(\log \log n)_{+}}\right)\] \[\quad\times\left[Bn^{1/4}+(n^{3/4}+\frac{\log n}{C_{\min}})C(M, \delta,d)+\frac{n^{5/8}}{\sqrt{C_{\min}}}\sqrt{d\log n+\log(1/\delta)+B^{2}}\right]\]

_with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\). Here,_

\[C(M,\delta,d)=C_{6}\sigma\sqrt{1+\sqrt{d\left(\log(M/\delta)+(\log\log d)_{+} \right)}+(\log(M/\delta)+(\log\log d)_{+})}.\]

Our main regret bound is an immediate corollary of Lemma 15 and Lemma 16, considering the regret decomposition of (3).

**Lemma 15** (Virtual Regret of the Oracle).: _Let \(\delta\in(0,1]\) and \(\tilde{\lambda}>0\). Assume the oracle agent employs a UCB or a Greedy policy, as laid out in Section 5. If \(\gamma_{t}=\mathcal{O}(t^{1/4})\), there exists an absolute constant \(C_{1}\) for which with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\),_

\[\tilde{R}_{j^{*}}(n)=\frac{C_{1}n^{5/8}}{\sqrt{C_{\min}}} \left(1+n^{-3/8}C_{\min}^{-1}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\] \[\quad\times\sqrt{\sigma^{2}d\log\left(\frac{n}{\tilde{\lambda}d} +1\right)+2\sigma^{2}\log(1/\delta)+\tilde{\lambda}B^{2}}\]

**Lemma 16** (Any-Time Model-Selection Regret, Formal).: _Let \(\delta\in(0,1]\) and \(\pi\) be the maximizer of (2). Suppose \(\eta_{t}=\mathcal{O}(C_{\min}/\sqrt{t}C(M,\delta,d))\) and \(\gamma_{t}=\mathcal{O}(t^{-1/4})\) and \(\lambda_{t}=\mathcal{O}(C(M,\delta,d)/\sqrt{t})\), then exists absolute constants \(C_{i}\) for which \(\operatorname{\mathrm{ALExp}}\) attains the model selection regret_

\[R(n,j) \leq C_{1}Bn^{3/4}+C_{2}\sqrt{n}C_{\min}^{-1}C(M,\delta,d)\log M\] \[+C_{3}B^{2}C_{\min}\sqrt{n}+C_{4}B\sqrt{n\left((\log\log nB^{2}) _{+}+\log(1/\delta)\right)}\] \[+C_{5}\left(Bn^{1/4}+(n^{3/4}+\frac{\log n}{C_{\min}})C(M,\delta,d)\right)\left(1+C_{\min}^{-1}n^{-3/8}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\]

_with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\). Here,_

\[C(M,\delta,d)=C_{6}\sigma\sqrt{1+\sqrt{d\left(\log(M/\delta)+(\log\log d)_{+ }\right)}+(\log(M/\delta)+(\log\log d)_{+})}.\]

### Proof of Model Selection Regret

Our technique for bounding the model selection regret relies on a classic horizon-independent analysis of the exponential weights algorithm, presented in Lemma 17.

**Lemma 17** (Anytime Exponential Weights Guarantee).: _Assume \(\eta_{t}\hat{r}_{t,j}\leq 1\) for all \(1\leq j\leq M\) and \(t\geq 1\). If the sequence \((\eta_{t})_{t\geq 1}\) is non-increasing, then for all \(n\geq 1\),_

\[\sum_{t=1}^{n}\hat{r}_{t,k}-\sum_{t=1}^{n}\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j} \leq\frac{\log M}{\eta_{n}}+\sum_{t=1}^{n}\eta_{t}\sum_{j=1}^{M}q_{t,j}\hat{r} _{t,j}^{2}\]

_for any arm \(k\in[M]\)._

Proof of Lemma 17.: Define \(\hat{R}_{t,i}\coloneqq\sum_{s=1}^{t}\hat{r}_{s,i}\) to be the expected cumulative reward of agent \(i\) after \(t\) steps. We rewrite for a fixed \(k\)

\[\sum_{t=1}^{n}\hat{r}_{t,k}-\sum_{t=1}^{n}\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}= \sum_{t=1}^{n}\hat{r}_{t,k}-\sum_{t=1}^{n}\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}].\] (14)

We focus on a single term in the second sum. For any \(t\), we have

\[-\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]=\log(\exp(-\mathbb{E}_{j \sim q_{t}}[\frac{\eta_{t}}{\eta_{t}}\hat{r}_{t,j}])) =\log(\exp(-\mathbb{E}_{j\sim q_{t}}[\eta_{t}\hat{r}_{t,j}])^{1/ \eta_{t}})\] \[=\frac{1}{\eta_{t}}\log(\exp(-\mathbb{E}_{j\sim q_{t}}[\eta_{t} \hat{r}_{t,j}]))\] \[=\frac{1}{\eta_{t}}\log(\mathbb{E}_{i\sim q_{t}}\exp(-\mathbb{E}_ {j\sim q_{t}}[\eta_{t}\hat{r}_{t,j}]))\] (15)

The inner expectation is over \(j\), while the outer one is over \(i\) and therefore has no effect. Moreover,

\[\frac{1}{\eta_{t}}\log\mathbb{E}_{i\sim q_{t}}\exp(-\eta_{t} \mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]+\eta_{t}\hat{r}_{t,i}) =\frac{1}{\eta_{t}}\log\left(\exp(-\eta_{t}\mathbb{E}_{j\sim q_{t }}[\hat{r}_{t,j}])\mathbb{E}_{i\sim q_{t}}\exp(\eta_{t}\hat{r}_{t,i})\right)\] \[=\frac{1}{\eta_{t}}\log\mathbb{E}_{i\sim q_{t}}\exp(-\eta_{t} \mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}])\] \[\qquad+\frac{1}{\eta_{t}}\log\mathbb{E}_{i\sim q_{t}}\exp(\eta_{t }\hat{r}_{t,i})\] (16)

where again, the expectation can be reintroduced to get the last line. Combining (15) and (16),

\[-\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]=\frac{1}{\eta_{t}}\log\mathbb{E}_{i \sim q_{t}}\exp(-\eta_{t}\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]+\eta_{t}\hat{ r}_{t,i})-\frac{1}{\eta_{t}}\log\mathbb{E}_{i\sim q_{t}}\exp(\eta_{t}\hat{r}_{t,i})\] (17)

This transformation is at the core of many exponential weight proofs (Bubeck et al., 2012, Lattimore and Szepesvari, 2020). We first bound the first term in (17):

\[\log\mathbb{E}_{i\sim q_{t}}\exp(-\eta_{t}\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]+\eta_{t}\hat{r}_{t,i})=\log\mathbb{E}_{i\sim q_{t}}\exp(\eta_{t}\hat{r}_{ t,i})-\eta_{t}\mathbb{E}_{j\sim q_{t}}\hat{r}_{t,j}\]\[\stackrel{{\text{(I)}}}{{\leq}} \mathbb{E}_{i\sim q_{t}}\exp(\eta_{t}\hat{r}_{t,i})-1-\eta_{t} \mathbb{E}_{j\sim q_{t}}\hat{r}_{t,j}\] \[=\mathbb{E}_{i\sim q_{t}}\left[\exp(\eta_{t}\hat{r}_{t,i})-1-\eta_ {t}\hat{r}_{t,i}\right]\] \[\stackrel{{\text{(I)}}}{{\leq}}\mathbb{E}_{i\sim q _{t}}\left[\eta_{t}^{2}\hat{r}_{t,i}^{2}\right]\] (18)

where in (I) we use the fact that \(\log(z)\leq z-1\) and in (II) we use the fact that for \(x\leq 1\), we have \(\exp(x)\leq 1+x+x^{2}\), and hence \(\exp(x)-1-x\leq x^{2}\). For the second term in (17), we will mirror the potential argument in Bubeck et al. (2012), but with a slightly different potential function. We expand the definition of \(q_{t}\):

\[-\frac{1}{\eta_{t}}\log\mathbb{E}_{i\sim q_{t}}\exp(\eta_{t}\hat{ r}_{t,i}) =-\frac{1}{\eta_{t}}\log\frac{\sum_{i=1}^{M}\exp(\eta_{t}\hat{R}_ {t,i})}{\sum_{i=1}^{M}\exp(\eta_{t}\hat{R}_{t-1,i})}\] \[=-\frac{1}{\eta_{t}}\log\frac{1}{M}\sum_{i=1}^{M}\exp(\eta_{t} \hat{R}_{t,i})+\frac{1}{\eta_{t}}\log\frac{1}{M}\sum_{i=1}^{M}\exp(\eta_{t} \hat{R}_{t-1,i})\] \[=J_{t}(\eta_{t})-J_{t-1}(\eta_{t}),\] (19)

where we define \(J_{t}(\eta)=-\frac{1}{\eta}\log\frac{1}{M}\sum_{i=1}^{M}\exp(\eta\hat{R}_{t,i})\). We also define \(F_{t}(\eta)=\frac{1}{\eta}\log\frac{1}{M}\sum_{i=1}^{M}\exp(-\eta\hat{R}_{t,i})\). We observe the relation \(J(\eta)=F(-\eta)\). From this, it follows that for any \(\eta\), we have \(J^{\prime}(\eta)=-F^{\prime}(-\eta)\leq 0\), by the argument in Bubeck et al. (2012, Theorem 3.1) that shows \(F^{\prime}(\eta)\geq 0\) for any \(\eta\).

**Putting together the pieces** Now, we can bound (17) by inputing (18) and (19):

\[-\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]\leq\mathbb{E}_{i\sim q_{t}}\left[ \eta_{t}\hat{r}_{t,i}^{2}\right]+J_{t}(\eta_{t})-J_{t-1}(\eta_{t})\]

With this, we rewrite (14) as

\[\sum_{t=1}^{n}\hat{r}_{t,k}-\sum_{t=1}^{n}\mathbb{E}_{j\sim q_{t}}[\hat{r}_{t,j}]=\sum_{t=1}^{n}\hat{r}_{t,k}+\sum_{t=1}^{n}\mathbb{E}_{i\sim q_{t}}\left[ \eta_{t}\hat{r}_{t,i}^{2}\right]+\sum_{t=1}^{n}J_{t}(\eta_{t})-J_{t-1}(\eta_{t})\] (20)

**Potential manipulation** We can do an Abel transformation on the sum of potentials in (20), namely obtaining

\[\sum_{t=1}^{n}J_{t}(\eta_{t})-J_{t-1}(\eta_{t})=\sum_{t=1}^{n-1}(J_{t}(\eta_{ t})-J_{t}(\eta_{t+1}))+J_{n}(\eta_{n}),\]

where we used that \(J_{0}(\eta)=0\). We know \(J^{\prime}(\eta)\leq 0\) and so \(J\) is decreasing and since \(\eta_{t+1}\leq\eta_{t}\), we have \(J(\eta_{t+1})\geq J(\eta_{t})\) or \((J_{t}(\eta_{t})-J_{t}(\eta_{t+1}))\leq 0\), so that for any fixed \(k\)

\[\sum_{t=1}^{n}J_{t}(\eta_{t})-J_{t-1}(\eta_{t}) \leq J_{n}(\eta_{n})\leq\frac{\log(M)}{\eta_{n}}-\frac{1}{\eta_{n }}\log\left(\sum_{i=1}^{M}\exp(\eta_{n}\hat{R}_{n,i})\right)\] \[\stackrel{{\text{(*)}}}{{\leq}}\frac{\log(M)}{\eta_{ n}}-\frac{1}{\eta_{n}}\log\left(\exp(\eta_{n}\hat{R}_{n,k})\right)\] \[=\frac{\log(M)}{\eta_{n}}-\sum_{t=1}^{n}\hat{r}_{t,k}\] (21)

where \((*)\) follows because \(\exp\) is positive and \(-\log\) is decreasing (notice that we drop \(M-1\) terms from the sum). Plugging (21) into (20), we obtain

\[\sum_{t=1}^{n}\hat{r}_{t,k}-\sum_{t=1}^{n}\mathbb{E}_{j\sim q_{t} }[\hat{r}_{t,j}] \leq\sum_{t=1}^{n}\hat{r}_{t,k}+\sum_{t=1}^{n}\mathbb{E}_{i\sim q _{t}}\left[\eta_{t}\hat{r}_{t,i}^{2}\right]+\sum_{t=1}^{n}J_{t}(\eta_{t})-J_{t -1}(\eta_{t})\] \[\leq\sum_{t=1}^{n}\hat{r}_{t,k}+\sum_{t=1}^{n}\mathbb{E}_{i\sim q _{t}}\left[\eta_{t}\hat{r}_{t,i}^{2}\right]+\frac{\log(M)}{\eta_{n}}-\sum_{t=1}^ {n}\hat{r}_{t,k}\] \[\leq\sum_{t=1}^{n}\mathbb{E}_{i\sim q_{t}}\left[\eta_{t}\hat{r}_{ t,i}^{2}\right]+\frac{\log(M)}{\eta_{n}}\]\[=\sum_{t=1}^{n}\eta_{t}\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}^{2}+\frac{ \log(M)}{\eta_{n}}.\]

We expressed in Section 5.2, that the model selection regret of ALExp, is closely tied to the bias and variance of the reward estimates \(\hat{r}_{t,j}\). The following lemma formalizes this claim.

**Lemma 18**.: _(Anytime Generic regret bound) If \(\eta_{t}\) is picked such that \(\eta_{t}\hat{r}_{t,j}\leq 1\) for all \(1\leq j\leq M\) and \(1\leq t\) almost surely, then Algorithm 1 satisfies with probability greater than \(1-2\delta/3\), that simultaneously for all \(n\geq 1\)_

\[R(n,i)\leq 2B\sum_{t=1}^{n}\gamma_{t} +\frac{\log M}{\eta_{n}}+\sum_{t=1}^{n}\eta_{t}\sum_{j=1}^{M}q_{ t,j}\hat{r}_{t,j}^{2}+\sum_{t=1}^{n}(\omega_{t,i}+\sum_{j=1}^{M}q_{t,j} \omega_{t,j})\] \[+10B\sqrt{n\left((\log\log nB^{2})_{+}+\log(12/\delta)\right)}\]

_where \(\omega_{t,i}=\left|r_{t,i}-\hat{r}_{t,i}\right|\)._

Proof of Lemma 18.: Let \(\alpha_{t}\) denote the Bernoulli random variable that is equal to 1 if at step \(t\) we select actions according to \(\pi\) and 0 otherwise. At each step \(t\) with \(\alpha_{t}=1\) ALExp accumulates a regret of at most \(2B\), since \(\left\|\bm{\theta}\right\|_{\infty}\leq B\) and \(\left\|\bm{\phi}(\cdot)\right\|\leq 1\). We can decompose the regret as,

\[R(n,i)\leq\sum_{t=1}^{n}2B\alpha_{t}+(r_{t,i}-r_{t})(1-\alpha_{t})\]

For the first term, by Lemma 24, we have

\[2B\sum_{t=1}^{n}\alpha_{t}\leq 2B\left(\sum_{t=1}^{n}\gamma_{t}+\frac{5}{2} \sqrt{n\left((\log\log n)_{+}+\log(4/\delta_{1})\right)}\right).\]

simultaneously for all \(n\geq 1\), with probability \(1-\delta_{1}\). Let \(\hat{r}_{t}:=\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}\). We may re-write the second term of the regret as follows,

\[\sum_{t=1}^{n}(1-\alpha_{t})\Big{(}r_{t,i}-r_{t}\Big{)} \leq\sum_{t=1}^{n}(1-\alpha_{t})\Big{[}(r_{t,i}-\hat{r}_{t,i})+( \hat{r}_{t,i}-\hat{r}_{t})+(\hat{r}_{t}-r_{t})\Big{]}\] \[\leq\sum_{t=1}^{n}\omega_{t,i}+(1-\alpha_{t})\Big{[}(\hat{r}_{t, i}-\hat{r}_{t})+(\hat{r}_{t}-r_{t})\Big{]}\]

We bound the second term on the right hand side, using Lemma 17

\[\sum_{t=1}^{n}(1-\alpha_{t})(\hat{r}_{t,i}-\hat{r}_{t})\leq\sum_{t=1}^{n}( \hat{r}_{t,i}-\hat{r}_{t})\leq\frac{\log M}{\eta_{n}}+\sum_{t=1}^{n}\eta_{t} \sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}^{2}.\]

As for the third term,

\[(1-\alpha_{t})(\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}-r_{t}) =(1-\alpha_{t})\Big{[}\sum_{j=1}^{M}q_{t,j}(\hat{r}_{t,j}-r_{t,j}+ r_{t,j})-r_{t}\Big{]}\] \[\leq\sum_{j=1}^{M}q_{t,j}\omega_{t,j}+(1-\alpha_{t})\left(r_{t}- \sum_{j=1}^{M}q_{t,j}r_{t,j}\right).\]

It remains to bound the deviation term. For all \(t\) that satisfy \(\alpha_{t}=0\), the action/model is selected according to \(q_{t,j}\), therefore the conditional expectation of \(r_{t}\) can be written as

\[\mathbb{E}_{t-1}\,r_{t}=\sum_{j-1}^{M}q_{t,j}r_{t,j}\]The sequence \(X_{t}:=r_{t}-\mathbb{E}_{t-1}\,r_{t}\) is a martingale difference sequence adapted to the history \(H_{t}\), since for every \(t\geq 1\),

\[\mathbb{E}_{t-1}\,X_{t}=\mathbb{E}\left[r_{t}-\mathbb{E}_{t-1}\,r_{t}|H_{t-1} \right]=0.\]

Since \(r_{t}\leq B\), then \(X_{t}\leq 2B\) almost surely, which allows for an application of anytime Azuma-Hoeffding (Lemma 26):

\[\mathbb{P}\left(\exists n:\ \sum_{t=1}^{n}\left(r_{t}-\sum_{j=1}^{M}q_{t,j}r_{t,j }\right)\geq\frac{5B}{2}\sqrt{n\left((\log\log nB^{2})_{+}+\log(2/\delta_{2}) \right)}\right)\leq\delta_{2}\]

which, in turn, leads us to

\[\sum_{t=1}^{n}(1-\alpha_{t})\left(r_{t}-\sum_{j=1}^{M}q_{t,j}r_{t,j}\right)\stackrel{{\text{\rm as.s}}}{{\leq}} \sum_{t=1}^{n}\left(r_{t}-\sum_{j=1}^{M}q_{t,j}r_{t,j}\right)\] \[\stackrel{{\text{\rm w.p.}}}{{\leq}} \frac{5B}{2}\sqrt{n\left((\log\log nB^{2})_{+}+\log(2/\delta_{2}) \right)}\]

simultaneously for all \(n\geq 1\). We set \(\delta_{1}=\delta_{2}=\delta/3\), take a union bound and put the terms together obtaining,

\[R(n,i)\leq 2B\sum_{t=1}^{n}\gamma_{t} +\frac{\log M}{\eta_{n}}+\eta\sum_{t=1}^{n}\sum_{j=1}^{M}q_{t,j} \hat{r}_{t,j}^{2}+\sum_{t=1}^{n}(\omega_{t,i}+\sum_{j=1}^{M}q_{t,j}\omega_{t, j})\] \[+\frac{5B}{2}\sqrt{n\left((\log\log nB^{2})_{+}+\log(6/\delta) \right)}+5B\sqrt{n\left((\log\log n)_{+}+\log(12/\delta)\right)}\]

We upper bound the sum of last two terms to conclude the proof. 

The next two lemmas bound the bias and variance terms which appear in Lemma 18.

**Lemma 19** (Anytime Bound on the Bias Term).: _If the regularization parameter of Lasso is chosen at every step as_

\[\lambda_{t}=\frac{2\sigma}{\sqrt{t}}\sqrt{1+\frac{5}{\sqrt{2}}\sqrt{d\left( \log(2M/\delta)+(\log\log d)_{+}\right)}+\frac{12}{\sqrt{2}}\left(\log(2M/ \delta)+(\log\log d)_{+}\right)}\]

_and \(\gamma_{t}=O(t^{-1/4})\), then with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\),_

\[\sum_{t=1}^{n}\lvert\hat{r}_{t,i}-r_{t,i}\rvert\leq n^{3/4}C_{\min}^{-1}C(M, \delta,d)\left(1+n^{-3/8}C_{\min}^{-1}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\]

_where_

\[C(M,\delta,d)\coloneqq C\sigma\sqrt{1+\sqrt{d\left(\log(M/\delta)+(\log\log d )_{+}\right)}+(\log(M/\delta)+(\log\log d)_{+})}\]

_and \(C\) is an absolute constant._

Proof of Lemma 19.: By the definition of the expected reward and its estimate,

\[\sum_{t=1}^{n}\lvert\hat{r}_{t,i}-r_{t,i}\rvert =\sum_{t=1}^{n}\Bigl{\lvert}\int_{\mathcal{X}}(r(\bm{x})-\hat{r} _{t}(\bm{x}))\mathrm{d}p_{t+1,i}(\bm{x})\Bigr{\rvert}\] \[\leq\sum_{t=1}^{n}\int_{\mathcal{X}}\lvert r(\bm{x})-\hat{r}_{t}( \bm{x})\rvert\mathrm{d}p_{t+1,i}(\bm{x})\] \[\stackrel{{\text{\rm cs.}}}{{\leq}}\sum_{t=1}^{n} \int_{\mathcal{X}}\Bigl{\lVert}\bm{\theta}-\hat{\bm{\theta}}_{t}\Bigr{\rVert}_ {2}\lVert\bm{\phi}(\bm{x})\rVert_{2}\mathrm{d}p_{t+1,i}(\bm{x})\] \[\stackrel{{\text{\rm bdt.}}}{{\leq}}\sum_{t=1}^{n} \Bigl{\lVert}\bm{\theta}-\hat{\bm{\theta}}_{t}\Bigr{\rVert}_{2}\int_{ \mathcal{X}}\mathrm{d}p_{t+1,i}(\bm{x})=\sum_{t=1}^{n}\Bigl{\lVert}\bm{\theta }-\hat{\bm{\theta}}_{t}\Bigr{\rVert}_{2}.\]We highlight that the Cauchy-Schwarz step may be refined. By further assuming that \(\bm{\theta}_{j^{*}}\) is bounded away from zero (also called the _beta-min_ condition [Buhlmann and Van De Geer, 2011]) one can show that \(\bm{\theta}-\hat{\bm{\theta}}_{t}\) is a \(2\)-sparse vector. This will then allow one to only rely on boundedness of \(\|\bm{\phi}_{j}\|\) rather than \(\|\bm{\phi}\|\) to derive the last inequality, and relax our assumption of \(\|\bm{\phi}(\cdot)\|\leq 1\) to \(\|\bm{\phi}_{j}(\cdot)\|\leq 1\) for all \(j\in[M]\). From Theorem 3, with probability greater than \(1-\delta/2\) simultaneously for all \(n\geq 1\),

\[\sum_{t=1}^{n}\lvert\hat{r}_{t,i}-r_{t,i}\rvert\leq\sum_{t=1}^{n}\frac{4\sqrt{ 10}\lambda_{t}}{\kappa^{2}(\Phi_{t},2)}=\tilde{C}(M,\delta,d)\sum_{t=1}^{n} \frac{1}{\kappa^{2}(\Phi_{t},2)\sqrt{t}}\]

where,

\[\tilde{C}(M,\delta,d)\coloneqq 8\sigma\sqrt{1+\frac{5}{\sqrt{2}}\sqrt{d \left(\log(4M/\delta)+(\log\log d)_{+}\right)}+\frac{12}{\sqrt{2}}\left(\log(4 M/\delta)+(\log\log d)_{+}\right)}.\]

From Lemma 8, there exist absolute constants \(C_{1},C_{2}\) for which,

\[\mathbb{P}\left(\forall t\geq 1:\ \kappa^{2}(\Phi_{t},2)\geq C_{1}C_{\min}t^{-1/4 }-C_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\geq 1-\delta.\]

Using Taylor approximation we observe that, \(\frac{1}{1-x^{-1}}=1+x^{-1}+o(x^{-1})=\mathcal{O}(1+x^{-1})\). Therefore, these exists absolute constant \(C_{3},C_{4}\), for which with probability greater than \(1-\delta\) for all \(t\geq 1\)

\[\sum_{t=1}^{n}\frac{\tilde{C}(M,\delta,d)}{\kappa^{2}(\Phi_{t},2) \sqrt{t}} \leq\sum_{t=1}^{n}\frac{\tilde{C}(M,\delta,d)}{\sqrt{t}}\frac{1}{C_{1}C_{ \min}t^{-1/4}-C_{2}t^{-5/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}}\] \[\leq\sum_{t=1}^{n}\frac{\tilde{C}(M,\delta,d)}{\sqrt{t}}\frac{ \tilde{C}_{3}}{C_{\min}t^{-1/4}}\left(1+\frac{C_{2}t^{-5/8}\sqrt{\log(Md/ \delta)+(\log\log t)_{+}}}{C_{1}C_{\min}t^{-1/4}}\right)\] \[\leq\sum_{t=1}^{n}\frac{\tilde{C}_{3}\tilde{C}(M,\delta,d)t^{-1/4 }}{C_{\min}}\left(1+\tilde{C}_{4}\frac{t^{-3/8}}{C_{\min}}\sqrt{\log(Md/ \delta)+(\log\log t)_{+}}\right)\] \[=\frac{C_{3}\tilde{C}(M,\delta,d)n^{3/4}}{C_{\min}}\left(1+C_{4} \frac{n^{-3/8}}{C_{\min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right).\]

**Lemma 20** (Anytime Bound on Variance Term).: _Suppose \(\lambda_{t}\) is chosen according to Lemma 19, \(\gamma_{t}=\mathcal{O}(t^{-1/4})\) and \(\eta_{t}=\mathcal{O}(C_{\min}t^{-1/2}/C(M,\delta,d))\). Then with probability greater than \(1-\delta\), the following holds simultaneously for all \(n\geq 1\) and \(t\geq 1\)_

\[\hat{r}_{t,j} \leq\frac{4\sqrt{10}\lambda_{t}}{\kappa^{2}(\Phi_{t},2)}+B,\qquad \forall j\in[M]\] \[\sum_{t=1}^{n}\eta_{t}\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}^{2} \leq C_{1}B^{2}C_{\min}\sqrt{n}+C_{2}Bn^{1/4}\left(1+\frac{n^{-3/8 }}{C_{\min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\] \[+C(M,\delta,d)\frac{\log n}{C_{\min}}\left(1+\frac{n^{-3/8}}{C_{ \min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\]

_where \(C_{i}\) are absolute constants, and \(C(M,\delta,d)\) is as defined in Lemma 19, up to constant factors._

Proof of Lemma 20.: We start by upper bounding \(\hat{r}_{t,j}\). For all \(j\) and \(t\) it holds that:

\[\hat{r}_{t,j}=\int_{\mathcal{X}}\langle\hat{\bm{\theta}}_{t},\bm{\phi}(\bm{x}) \rangle\mathrm{d}p_{t+1,j}(\bm{x})\leq\left\lVert\hat{\bm{\theta}}_{t}\right \rVert\int_{\mathcal{X}}\lVert\bm{\phi}(\bm{x})\rVert\mathrm{d}p_{t+1,j}(\bm{x })\leq\left\lVert\hat{\bm{\theta}}_{t}\right\rVert_{2}\leq B+\left\lVert(\hat{ \bm{\theta}}_{t}-\bm{\theta})\right\rVert_{2}\]

since \(\left\lVert\bm{\theta}\right\rVert_{2}\leq B\). To bound the last term, we only need to invoke Theorem 3, which, in turn, will simultaneously bound \(\hat{r}_{t,j}\) for all \(j=1,\ldots,M\):

\[\mathbb{P}\left(\forall t\geq 1,\,\forall j\in[M]:\ \hat{r}_{t,j}\leq\frac{4\sqrt{10} \lambda_{t}}{c_{\kappa,t}^{2}}+B\right)\geq 1-\delta\]Which implies for all \(t\geq 1\),

\[\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}^{2}\leq\left(\frac{4\sqrt{10}\lambda_{t}}{c_{ \kappa,t}^{2}}+B\right)^{2}\sum_{j=1}^{M}q_{t,j}=\frac{160\lambda_{t}^{2}}{c_{ \kappa,t}^{4}}+B^{2}+\frac{8B\sqrt{10}\lambda_{t}}{c_{\kappa,t}^{2}}.\]

For the last term, similar to the proof of Lemma 19 we have,

\[\sum_{t=1}^{n}\eta_{t}\frac{8B\sqrt{10}\lambda_{t}}{c_{\kappa,t}^{2}}\leq C_{1 }Bn^{1/4}\left(1+\frac{n^{-3/8}}{C_{\min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+ }}\right)\]

for some absolute constant \(C_{1}\). We treat the squared term similarly,

\[\sum_{t=1}^{n}\eta_{t}\frac{160\lambda_{t}^{2}}{c_{\kappa,t}^{4}} \leq C(M,\delta,d)\sum_{t=1}^{n}\frac{\bar{C}_{3}}{tC_{\min}}\left( 1+\bar{C}_{4}\frac{t^{-3/8}}{C_{\min}}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\] \[\leq C(M,\delta,d)\frac{C_{3}\log n}{C_{\min}}\left(1+C_{4}\frac{ n^{-3/8}}{C_{\min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right).\]

Note that the last inequality is not tight. This term will not be fastest growing term in the regret, so we have little motivation to bound it tightly. Therefore,

\[\sum_{t=1}^{n}\eta_{t}\sum_{j=1}^{M}q_{t,j}\hat{r}_{t,j}^{2} \leq C_{1}B^{2}C_{\min}\sqrt{n}+C_{2}Bn^{1/4}\left(1+\frac{n^{-3/ 8}}{C_{\min}}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\] \[+C(M,\delta,d)\frac{\log n}{C_{\min}}\left(1+C_{4}\frac{n^{-3/8}} {C_{\min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\]

where \(C_{i}\) are absolute constants. 

Proof of Lemma 16.: We start by conditioning on the event \(E\) that \(\eta_{t}\) is picked such that \(\eta_{t}\hat{r}_{t,j}\leq 1\) for all \(t\geq 1\) and \(j=1,\ldots,M\). Then by application of Lemma 18 we get with probability greater than \(1-2\delta/3\),

\[R(n,i)\leq 2B\sum_{t=1}^{n}\gamma_{t} +\frac{\log M}{\eta_{n}}+\sum_{t=1}^{n}\eta_{t}\sum_{j=1}^{M}q_{ t,j}\hat{r}_{t,j}^{2}+\sum_{t=1}^{n}(\omega_{t,i}+\sum_{j=1}^{M}q_{t,j} \omega_{t,j})\] \[+10B\sqrt{n\left((\log\log nB^{2})_{+}+\log(12/\delta)\right)}\]

We invoke Lemma 19 and Lemma 20 with \(\delta\rightarrow\delta/3\) take a union bound, to bound the variance and \(\omega_{t,i}\) terms as well. These lemmas require one application of Theorem 3 to hold simultaneously and no additional union bound is required between them, since the randomness comes only from the confidence interval over \(\hat{\bm{\theta}}_{t}\).

\[R(n,i)\leq C_{1}Bn^{3/4}+\frac{\log M}{\eta_{n}}\] \[+C_{2}B^{2}C_{\min}\sqrt{n}+C_{3}Bn^{1/4}\left(1+\frac{n^{-3/8}} {C_{\min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\] \[+C(M,\delta,d)\frac{\log n}{C_{\min}}\left(1+\frac{n^{-3/8}}{C_{ \min}}\sqrt{\log(Md/\delta)+(\log\log n)_{+}}\right)\] \[+n^{3/4}C(M,\delta,d)\left(1+\frac{n^{-3/8}}{C_{\min}}\sqrt{\log (Md/\delta)+(\log\log n)_{+}}\right)\] \[+10B\sqrt{n\left((\log\log nB^{2})_{+}+\log(12/\delta)\right)}\]

with probability greater than \(1-\delta\), conditioned on event \(E\). Assuming that event \(E\) happens with probability \(1-2\delta\), let \(\mathcal{B}=\mathcal{B}(\tilde{\delta},M,d,n,B,\sigma,C_{\min})\) denote the right-hand-side of the regret inequality above.

By the chain rule we may write,

\[\mathbb{P}\big{(}\mathrm{Reg} (n,i)\leq\mathcal{B}\big{)}\] \[\geq\mathbb{P}\left(R(n,i)\leq\mathcal{B}\Big{|}\forall t\in[n],j \in[M]:\eta_{t}\hat{r}_{t,j}\leq 1\right)\mathbb{P}\left(\forall t\in[n],j\in[M]: \eta_{t}\hat{r}_{t,j}\leq 1\right)\] \[\geq(1-\delta)(1-2\delta)\geq 1-3\delta.\]

It remains to verify that event \(E\) is met with probability \(1-2\delta\). Recall that \(\eta_{t}=\mathcal{O}(C_{\min}/\sqrt{t}C(M,\delta,d))\), and that from Lemma 8 with probability \(1-\delta\),

\[\frac{C_{\min}}{4\sqrt{t}C(M,\delta,d)}\leq\frac{C_{1}C_{\min}t^{1/4}-C_{2}t^{ -1/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}}{4\sqrt{10}C(M,\delta,d)}\leq \frac{\kappa^{2}(\Phi_{t},2)}{4\sqrt{10}\lambda_{t}}\]

Therefore, from Lemma 20, there exists \(C_{\eta}\) such that \(\eta_{t}=C_{\eta}C_{\min}/B\sqrt{t}C(M,\delta,d)\) satisfying,

\[\mathbb{P}\left(\forall t\geq 1,j\in[M]:\,\eta_{t}\hat{r}_{t,j}\leq 1\right) \geq 1-2\delta\]

The proof is then finished by setting \(\delta\gets 3\delta\) (and updating the absolute constants). 

### Proof of Virtual Regret

**Proposition 21**.: _For any fixed \(\tilde{\lambda}>0\), there exists an absolute constant \(C_{1}\) such that_

\[\mathbb{P}\left(\forall t\geq 1:\,\Big{\|}\hat{\bm{\beta}}_{t,j^{*}}-\bm{ \phi}_{j^{*}}\Big{\|}_{2}\leq\omega(t,\delta,d)\right)\geq 1-\delta.\]

_where_

\[\omega(t,\delta,d)\coloneqq C_{1}\sqrt{\frac{\sigma^{2}d\log\left(\frac{t}{ \tilde{\lambda}d}+1\right)+2\sigma^{2}\log(1/\delta)+\tilde{\lambda}B^{2}}{ \tilde{\lambda}+C_{\min}t^{3/4}}}\left(1+C_{\min}^{-1}t^{-3/8}\sqrt{\log(Md/ \delta)+(\log\log t)_{+}}\right).\]

_Moreover, for \(u_{t,j^{*}}(\cdot)\coloneqq\hat{\bm{\beta}}_{t,j^{*}}^{\top}\bm{\phi}_{j^{*} }(\cdot)+\omega(t,\delta,d)\),_

\[\mathbb{P}\left(\forall t\geq 1,\,\bm{x}\in\mathcal{X}:r(\bm{x})\leq u_{t,j^{*} }(\bm{x})\right)\geq 1-\delta.\]

Proof of Proposition 21.: Define for convenience \(V_{t}=\Phi_{t,j^{*}}^{\top}\Phi_{t,j^{*}}+\tilde{\lambda}\bm{I}\). We first observe that

\[\hat{\bm{\beta}}_{t,j^{*}}=V_{t}^{-1}(\Phi_{t,j^{*}})^{\top}\bm{y}_{t}\]

We can apply results from Abbasi-Yadkori et al. (2011) to get an anytime-valid confidence set. Their Theorem 2 asserts that with probability \(1-\delta\), for all \(t\geq 1\) we have2

Footnote 2: Their theorem statement is slightly different, but they prove the stronger version we state below.

\[\Big{\|}\hat{\bm{\beta}}_{t,j^{*}}-\bm{\theta}_{j^{*}}\Big{\|}_{V_{t}}^{2} \leq\beta_{t}\]

where

\[\beta_{t}=2\sigma^{2}\log\left(\frac{\det(V_{t})^{1/2}}{\det(\tilde{\lambda} \bm{I})^{1/2}\delta}\right)+\tilde{\lambda}B^{2}\]

Clearly, \(V_{t}\succeq\lambda_{\min}(V_{t})I\), and therefore with high probability,

\[\Big{\|}\hat{\bm{\beta}}_{t,j^{*}}-\bm{\theta}_{j^{*}}\Big{\|}_{2}\leq\sqrt{ \frac{\beta_{t}}{\lambda_{\min}(V_{t})}}\]

uniformly over time. Our assumption is that \(\|\bm{\phi}_{j}(\bm{x})\|\leq 1\), and hence, denoting by \(\nu_{i}\) the eigenvalues of \(V_{t}\), the geometric-arithmetic mean inequality yields

\[\det(V_{t})\leq\prod_{i=1}^{d}\nu_{i}\leq\left(\frac{1}{d}\mathrm{trace}(V_{t })\right)^{d}.\]Given that

\[\mathrm{trace}(V_{t})=\sum_{i=1}^{d}\sum_{s=1}^{t}(\bm{\phi}_{j^{*}}(x))_{i}^{2}+ \tilde{\lambda}d\leq t+\tilde{\lambda}d\]

we can conclude that

\[\beta_{t}\leq 2\sigma^{2}\log\left(\frac{(t/d+\tilde{\lambda})^{d/2}}{\tilde{ \lambda}^{d/2}\delta}\right)+\tilde{\lambda}B^{2}=d\sigma^{2}\log\left(\frac{t }{\tilde{\lambda}d}+1\right)+2\sigma^{2}\log(1/\delta)+\tilde{\lambda}B^{2}\]

We note that

\[\lambda_{\min}(V_{t})=\lambda_{\min}(\Phi_{t,j}^{\top}\Phi_{t,j})+\tilde{ \lambda}.\]

Then due to Lemma 9, there exist absolute constants \(C_{1}\) and \(C_{2}\) such that for all \(t\geq 1\),

\[\lambda_{\min}(V_{t})\geq\tilde{\lambda}+C_{1}C_{\min}t^{3/4}-C_{2}t^{3/8} \sqrt{\log(Md/\delta)+(\log\log t)_{+}}\]

therefore, there exists \(C_{3}\) and \(C_{4}\) such that

\[\frac{1}{\sqrt{\lambda_{\min}(V_{t})}} \leq\frac{1}{\sqrt{\tilde{\lambda}+C_{1}C_{\min}t^{3/4}-C_{2}t^{3 /8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}}}\] \[\leq\frac{C_{3}}{\sqrt{\tilde{\lambda}+C_{1}C_{\min}t^{3/4}}} \left(1+\frac{t^{3/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}}{\tilde{\lambda} +C_{1}C_{\min}t^{3/4}}\right)\] \[\leq\frac{C_{4}}{\sqrt{\tilde{\lambda}+C_{1}C_{\min}t^{3/4}}} \left(1+C_{\min}^{-1}t^{-3/8}\sqrt{\log(Md/\delta)+(\log\log t)_{+}}\right)\]

with high probability for all \(t\geq 1\). Setting

\[\omega(t,\delta,d)=C_{5}\sqrt{\frac{\sigma^{2}d\log\left(\frac{t}{\tilde{ \lambda}d}+1\right)+2\sigma^{2}\log(1/\delta)+\tilde{\lambda}B^{2}}{\tilde{ \lambda}+C_{\min}t^{3/4}}}\left(1+C_{\min}^{-1}t^{-3/8}\sqrt{\log(Md/\delta)+ (\log\log t)_{+}}\right)\]

where \(C_{5}\) is an absolute constant concludes the parametric confidence bound. The upper confidence bound then simply follows: for any \(\bm{x}\in\mathcal{X}\)

\[r(\bm{x})-\hat{\bm{\beta}}_{t,j^{*}}^{\top}\bm{\phi}_{j^{*}}(\bm{x})=\langle \bm{\theta}_{j^{*}}-\hat{\bm{\beta}}_{t,j^{*}},\bm{\phi}_{j^{*}}(\bm{x}) \rangle\leq\left\|\bm{\theta}_{j^{*}}-\hat{\bm{\beta}}_{t,j^{*}}\right\|_{2} \left\|\bm{\phi}_{j^{*}}(\bm{x})\right\|_{2}\leq\omega(t,\delta,d)\]

where the last inequality holds with high probability simultaneously for all \(t\geq 1\). 

Proof of Lemma 15.: Using Proposition 21 and the Cauchy-Schwarz inequality we obtain,

\[\tilde{R}_{j^{*}}(n) =\sum_{t=1}^{n}r(\bm{x}^{*})-r(\tilde{\bm{x}}_{t,j})\] \[=\sum_{t=1}^{n}r(\bm{x}^{*})-\hat{r}_{t}(\bm{x}^{*})+\hat{r}_{t} (\bm{x}^{*})-\hat{r}_{t}(\tilde{\bm{x}}_{t,j})+\hat{r}_{t}(\tilde{\bm{x}}_{t, j})-r(\tilde{\bm{x}}_{t,j})\] \[\leq\sum_{t=1}^{n}\Bigl{\|}\bm{\theta}_{j}-\hat{\bm{\theta}}_{t,j }\Bigr{\|}_{2}\left(\left\|\bm{\phi}_{j}(\bm{x}^{*})\right\|_{2}+\left\|\bm{ \phi}_{j}(\tilde{\bm{x}}_{t,j})\right\|_{2}\right)+\hat{r}_{t}(\bm{x}^{*})- \hat{r}_{t}(\tilde{\bm{x}}_{t,j})\] \[\leq\sum_{t=1}^{n}\omega(t,\delta,d)\left(\left\|\bm{\phi}_{j}( \bm{x}^{*})\right\|_{2}+\left\|\bm{\phi}_{j}(\tilde{\bm{x}}_{t,j})\right\|_{2} \right)+\hat{r}_{t}(\bm{x}^{*})-\hat{r}_{t}(\tilde{\bm{x}}_{t,j})\]

with probability \(1-\delta\). If the agent selects actions greedily, then \(\hat{r}_{t}(\bm{x}^{*})\leq\hat{r}_{t}(\tilde{\bm{x}}_{t,j})\), and

\[\tilde{R}_{j^{*}}(n)\leq\sum_{t=1}^{n}\omega(t,\delta,d)\left(\left\|\bm{\phi}_ {j}(\bm{x}^{*})\right\|_{2}+\left\|\bm{\phi}_{j}(\tilde{\bm{x}}_{t,j})\right\|_ {2}\right)\leq\sum_{t=1}^{n}2\omega(t,\delta,d)\]

since the feature map is normalized to satisfy \(\left\|\bm{\phi}_{j}(\cdot)\right\|\leq 1\). If the agent selects actions optimistically according to the upper confidence bound of Proposition 21, then

\[\hat{r}_{t}(\tilde{\bm{x}}_{t,j})+\omega(t,\delta,d)\|\bm{\phi}_{j}(\tilde{\bm{ x}}_{t,j})\|\geq\hat{r}_{t}(\bm{x}^{*})+\omega(t,\delta,d)\|\bm{\phi}_{j}(\bm{x}^{*})\|\]which implies

\[\hat{r}_{t}(\bm{x}^{\star})-\hat{r}_{t}(\tilde{\bm{x}}_{t,j})\leq\omega(t,\delta, d)\|\bm{\phi}_{j}(\tilde{\bm{x}}_{t,j})\|-\omega(t,\delta,d)\|\bm{\phi}_{j}(\bm{x}^{ \star})\|\]

and therefore,

\[\tilde{R}_{j^{\star}}(n)\leq\sum_{t=1}^{n}2\omega(t,\delta,d)\|\bm{\phi}_{j}( \tilde{\bm{x}}_{t,j})\|_{2}\leq\sum_{t=1}^{n}2\omega(t,\delta,d).\]

Then due to Proposition 21, with probability greater than \(1-\delta\), simultaneously for all \(n\geq 1\),

\[\sum_{t=1}^{n}\omega(t,\delta,d) \leq\sum_{t=1}^{n}C_{1}\sqrt{\frac{\sigma^{2}d\log\left(\frac{1}{ \lambda d}+1\right)+2\sigma^{2}\log(1/\delta)+\tilde{\lambda}B^{2}}{\lambda+C _{\min}t^{3/4}}}\left(1+t^{-3/8}\sqrt{\log(\frac{Md}{\delta})+(\log\log t)_{+ }}\right)\] \[\leq\tilde{C}_{1}n^{5/8}\sqrt{\frac{\sigma^{2}d\log\left(\frac{n }{\lambda d}+1\right)+2\sigma^{2}\log(1/\delta)+\tilde{\lambda}B^{2}}{C_{\min} }}\left(1+C_{\min}^{-1}n^{\frac{-3}{8}}\sqrt{\log(\frac{Md}{\delta})+(\log \log n)_{+}}\right)\]

concluding the proof. 

## Appendix E Time-Uniform Concentration Inequalities

We will make use of the elegant concentration results in Howard et al. (2021), which analyzes the boundary of sub-Gamma processes.

**Definition 22** (Sub-Gamma process).: Let \((S_{t})_{t=0}^{\infty}\) and \((V_{t})_{t=0}^{\infty}\) be real-valued processes adapted to \((\mathcal{F}_{t})_{t=1}^{\infty}\) with \(S_{0}=V_{0}=0\) and \(V_{t}\) non-negative. We say that \(S_{t}\) is sub-Gamma if for \(\lambda\in[0,1/c)\), there exists a supermartingale \((M_{t}(\lambda))_{t=0}^{\infty}\) w.r.t. \(\mathcal{F}_{t}\), such that \(\mathbb{E}\,M_{0}=1\) and for all \(t\geq 1\):

\[\exp\{\lambda S_{t}-\frac{\lambda^{2}}{2(1-c\lambda)}V_{t}\}\leq M_{t}( \lambda)\qquad a.s.\]

The following is a special case of Theorem 1 in Howard et al. (2021). We have simplified it by making a few straightforward choices for the parameters used originally by Howard et al. (2021), which will yield an easier-to-use bound in our scenario.

**Proposition 23** (Curved Boundary of Sub-Gamma Processes).: _Let \((S_{t})_{t\geq 0}\) be sub-Gamma with scale parameter \(c\) and variance process \((V_{t})_{t\geq 0}\). Define the boundary_

\[\mathcal{B}_{\alpha}(v):=\frac{5}{2}\sqrt{\max\{v,1\}\left((\log\log ev)_{+}+ \log\left(\frac{2}{\alpha}\right)\right)}+3c\left((\log\log ev)_{+}+\log \left(\frac{2}{\alpha}\right)\right),\]

_for \(v>0\), where \((x)_{+}=\max(0,x)\). Then,_

\[\mathbb{P}(\exists t:\ S_{t}\geq\mathcal{B}_{\alpha}(V_{t}))\leq\alpha.\]

Proof of Proposition 23.: Suppose \(\xi(\cdot)\) denotes the Riemann zeta function. Theorem 1 in Howard et al. (2021) states that if \((S_{t})_{t\geq 0}\) is a sub-Gamma process with variance process \((V_{t})_{t\geq 0}\) then the boundary

\[\mathcal{S}_{\alpha}(v^{\prime})=k_{1}\sqrt{v^{\prime}\left(s\log\log\left( \eta v^{\prime}\right)+\log\left(\frac{\zeta(s)}{\alpha\log^{s}\eta}\right) \right)}+ck_{2}\left(s\log\log\left(\eta v^{\prime}\right)+\log\left(\frac{ \zeta(s)}{\alpha\log^{s}\eta}\right)\right).\]

satisfies,

\[\mathbb{P}(\exists t:S_{t}\geq\mathcal{S}_{\alpha}(\max(V_{t},1)))\leq\alpha\]

where

\[k_{1}:=\frac{\eta^{1/4}+\eta^{-1/4}}{\sqrt{2}}\qquad\text{and}\qquad k_{2}:=( \sqrt{\eta}+1)/2\]

and \(s,\eta\geq 1\). Choosing \(s=2\) and \(\eta=e\), we obtain \(\zeta(2)=\pi^{2}/6\leq 2\). Furthermore, we have \(k_{1}\leq\frac{3}{2}\) and \(k_{2}\leq\frac{3}{2}\). Then if \(v^{\prime}\geq 1\) (which we will enforce by the construction \(v^{\prime}=\max(1,v)\)), we compute

\[s\log\log\left(\eta v^{\prime}\right)+\log\left(\frac{\zeta(s)}{\alpha\log^{s} \eta}\right)\leq 2(\log\log ev^{\prime})_{+}+\log\left(\frac{2}{\alpha}\right).\]Therefore, we can upper bound (using our bounds on \(k_{1},k_{2}\))

\[\mathcal{S}_{\alpha}(v^{\prime})\leq\frac{5}{2}\sqrt{v^{\prime}\left((\log\log ev ^{\prime})_{+}+\log\left(\frac{2}{\alpha}\right)\right)}+3c\left((\log\log ev^{ \prime})_{+}+\log\left(\frac{2}{\alpha}\right)\right).\]

Now, since the boundary is given by \(\mathcal{S}_{\alpha}(\max(v,1))\) and \(v^{\prime}=\max(v,1)\geq 1\) we deduce that

\[\mathcal{B}_{\alpha}(v):=\frac{5}{2}\sqrt{\max\{v,1\}\left((\log\log ev)_{+}+ \log\left(\frac{2}{\alpha}\right)\right)}+3c\left((\log\log ev)_{+}+\log\left( \frac{2}{\alpha}\right)\right).\]

is an any-time valid boundary. 

**Lemma 24** (Time-Uniform Two-sided Bernoulli).: _Let \(X_{1},\ldots,X_{s},\ldots,X_{t}\) be a martingale sequence of Bernoulli random variables with conditional mean \(\gamma_{s}\). Then for all \(\delta>0\),_

\[\mathbb{P}\left(\exists t:\ \left|\sum_{s=1}^{t}(X_{s}-\gamma_{s})\right| \geq\frac{5}{2}\sqrt{t\left((\log\log t)_{+}+\log(4/\delta)\right)}\right)\leq\delta,\]

Proof of Lemma 24.: By Proposition 23, we know that if \(S_{t}\) is sub-Gamma with variance process \(V_{t}\) and scale parameter \(c\), then

\[\mathbb{P}\left(\exists t:\ S_{t}\geq\mathcal{B}_{\delta}(V_{t})\right)\leq\delta,\]

where

\[\mathcal{B}_{\delta}(v):=\frac{5}{2}\sqrt{\max\{1,v\}\left((\log\log ev)_{+}+ \log(2/\delta)\right)}+3c\left((\log\log ev)_{+}+\log(2/\delta)\right).\]

By Howard et al. (2020), we know that if \((X_{t})_{t=1}^{\infty}\) is a Bernoulli sequence, then \(S_{t}=\sum_{s=1}^{t}(X_{s}-\gamma_{s})\) is sub-Gamma with variance process \(V_{t}=t\) and scale parameter \(c=0\) (hence, sub-Gaussian). This implies,

\[\mathbb{P}\left(\exists t:\ \sum_{s=1}^{t}(X_{s}-\gamma_{s})\geq\frac{5}{2} \sqrt{t\left((\log\log t)_{+}+\log(2/\delta)\right)}\right)\leq\delta,\]

The above arguments also holds for the sequence \(Z_{s}=-X_{s}\). Then taking a union bound and adjusting \(\delta\leftarrow\delta/2\) concludes the proof. 

**Lemma 25** (Time-Uniform Bernstein).: _Let \((\xi_{i})_{i=1}^{\infty}\) be a sequence of conditionally standard sub-gaussian variables, where each \(\xi_{i}\) is \(\mathcal{F}_{i-1}=\sigma(\xi_{1},\ldots,\xi_{i})\) measurable. Then, for \(v_{i}\in\mathbb{R}\) and \(\delta\in(0,1]\)_

\[\mathbb{P}\left(\exists t:\ \sum_{i=1}^{t}(\xi_{i}^{2}-1)v_{i}\geq\frac{5}{2} \sqrt{\max\left\{1,4\norm{\bm{v}_{t}}_{2}^{2}\right\}\omega_{\delta}(\norm{\bm{v }_{t}}_{2})}+12\omega_{\delta}(\norm{\bm{v}_{t}}_{2})\max_{i\geq 1}v_{i} \right)\leq\delta\]

_where, \(\bm{v}_{t}=(v_{1},\ldots,v_{t})\in\mathbb{R}^{t}\) and \(\omega_{\delta}(v)\coloneqq\left(\log\log(4ev^{2})\right)_{+}+\log(2/\delta)\)._

Proof of Lemma 25.: From Lemma 4, \(S_{t}=\sum_{i=1}^{t}(\xi_{i}^{2}-1)v_{i}\) is sub-Gamma with variance process \(V_{t}=4\sum_{i=1}^{t}v_{i}^{2}\) and \(c=4\max_{i\geq 1}v_{i}\). By Proposition 23, we know that if \(S_{t}\) is sub-Gamma with variance process \(V_{t}\) and scale parameter \(c\), then

\[\mathbb{P}\left(\exists t:\ S_{t}\geq\mathcal{B}_{\delta}(V_{t})\right)\leq\delta,\]

where

\[\mathcal{B}_{\delta}(v)\coloneqq\frac{5}{2}\sqrt{\max\{1,v\}\left((\log\log ev )_{+}+\log(2/\delta)\right)}+3c\left((\log\log ev)_{+}+\log(2/\delta)\right).\]

**Lemma 26** (Time-Uniform Azuma-Hoeffding).: _Let \(X_{1},\ldots,X_{n}\) be a martingale difference sequence such that \(|X_{t}|\leq B\) for all \(t>1\) almost surely. Then for all \(\delta>0\),_

\[\mathbb{P}\left(\exists t:\ \sum_{s=1}^{t}X_{s}\geq\frac{5B}{2}\sqrt{t\left(( \log\log etB^{2})_{+}+\log(2/\delta)\right)}\right)\leq\delta,\]Proof of Lemma 26.: By Proposition 23, we know that if \(S_{t}\) is sub-Gamma with variance process \(V_{t}\) and scale parameter \(c\), then

\[\mathbb{P}\left(\exists t:\ S_{t}\geq\mathcal{B}_{\delta}(V_{t})\right)\leq\delta,\]

where

\[\mathcal{B}_{\delta}(v)\coloneqq\frac{5}{2}\sqrt{\max\{1,v\}\left((\log\log ev )_{+}+\log(2/\delta)\right)}+3c\left((\log\log ev)_{+}+\log(2/\delta)\right).\]

By (Howard et al., 2020), we know that if \((X_{t})_{t=1}^{\infty}\) is \(B\)-bounded martingale difference sequence, then \(S_{t}=\sum_{s=1}^{t}X_{s}\) is sub-Gamma with variance process \(V_{t}=tB^{2}\) and scale parameter \(c=0\). This implies,

\[\mathbb{P}\left(\exists t:\ S_{t}\geq\frac{5B}{2}\sqrt{t\left((\log\log etB^{ 2})_{+}+\log(2/\delta)\right)}\right)\leq\delta,\]

concluding the proof. 

## Appendix F Experiment Details

### Hyper-Parameter Tuning Results

We implement 6 algorithms in our experiments, ETC (Algorithm 4, Hao et al., 2020), ETS (Algorithm 5), Corral(Algorithm 6, Agarwal et al., 2017), ALExp(Algorithm 1), and Lastly UCB (Algorithm 3) with the oracle feature map \(\bm{\phi}_{j_{\star}}\)(Oracle), and UCB with the concatenated feature map \(\bm{\phi}\) (Naive). The Python code is available on github.com/lasgroup/ALEXP. When algorithms require exploration, e.g., in the case of ETC or ALExp, we simply set \(\pi=\mathrm{Unif}(\mathcal{X})\). Figure 7 shows the results of our hyperparameter tuning experiment. To ensure that the curves are valid, we run each

Figure 5: Examples of possible reward functions \(r(\cdot)\) in our experiments.

Figure 6: Bench-marking ALExp and other baselines. Complete version of Fig. 1 and Fig. 2.

configuration for 20 different random seeds, i.e. on different random environments. The shaded areas in Figure 7 show the standard error.

**UCB.** For all the experiments, we set the exploration coefficient of UCB to \(\beta_{t}=2^{3}\) and choose the regression regularizer from \(\tilde{\lambda}\in\{0.01,0.1,0.5\}\). We use PyTorch[20] for updating the upper confidence bounds, which requires more regularization for longer feature maps (e.g. when \(s=8\), \(p=2\)), to be computationally stable.

**Lasso.** Every time we need to solve Eq. (1), we set \(\lambda_{t}\) according to the _rate_ suggested by Theorem 3. To find a suitable constant scaling coefficient, we perform a hyper-parameter tuning experiment sampling 20 values in \([10^{-5},10^{0}]\). We choose \(\lambda_{0}=0.009\), and scale \(\lambda_{t}\) with it across all experiments.

**ALExp.** We set the rates for \(\gamma_{t}\) and \(\eta_{t}\) as prescribed by Theorem 1. For the scaling constants, we perform a hyper-parameter tuning experiment log-uniformly sampling 20 different configurations from \(\gamma_{0}\in[10^{-4},10^{-1}]\) and \(\eta_{0}\in[10^{0},10^{2}]\). For each problem instance (i.e. as \(s\) and \(p\) change) we repeat this process. However we observe that the optimal hyper-parameters work well across all problem instances.

**ETC/ETS.** For these algorithms, we separately tune \(n_{0}\) for each problem instance. We set \(\lambda_{1}\propto\sqrt{\log M/n_{0}}\) according to Theorem 4.2 of [10] and scale it with \(\lambda_{0}=0.009\), as stated before. We uniformly sample \(10\) different values where \(n_{0}\in[2,80]\) since the horizon is \(n=100\). The optimal value often happens around \(n_{0}=20\).

**Corral.** We set the rates of the parameters as \(\gamma=\mathcal{O}(1/n)\) and \(\eta=\mathcal{O}(\sqrt{M/n})\) according to Agarwal et al. [2017, Theorem 5,]. Then similar to ALExp, we tune the scaling constants. The procedure for tuning the constants is identical to ALExp, as in we use the same search interval, and try 10 different configurations for \(\gamma\) and \(\eta\).

[MISSING_PAGE_FAIL:36]

Figure 7: Results for different hyper-parameters across different problem instances. ALExp is robust to the choice of hyper-paramters.