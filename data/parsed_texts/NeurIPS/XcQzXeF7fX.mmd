# On Calibrating Diffusion Probabilistic Models

 Tianyu Pang\({}^{\dagger 1}\), Cheng Lu\({}^{2}\), Chao Du\({}^{1}\), Min Lin\({}^{1}\), Shuicheng Yan\({}^{1}\), Zhijie Deng\({}^{1}\)\({}^{1}\)

\({}^{1}\)Sea AI Lab, Singapore

\({}^{2}\)Department of Computer Science, Tsinghua University

\({}^{3}\)Qing Yuan Research Institute, Shanghai Jiao Tong University

{tianyupang, duchao, linmin, yansc}@sea.com; lucheng.lc15@gmail.com; zhijied@sjtu.edu.cn

Corresponding authors.

###### Abstract

Recently, diffusion probabilistic models (DPMs) have achieved promising results in diverse generative tasks. A typical DPM framework includes a forward process that gradually diffuses the data distribution and a reverse process that recovers the data distribution from time-dependent data scores. In this work, we observe that the stochastic reverse process of data scores is a martingale, from which concentration bounds and the optional stopping theorem for data scores can be derived. Then, we discover a simple way for _calibrating_ an arbitrary pretrained DPM, with which the score matching loss can be reduced and the lower bounds of model likelihood can consequently be increased. We provide general calibration guidelines under various model parametrizations. Our calibration method is performed only once and the resulting models can be used repeatedly for sampling. We conduct experiments on multiple datasets to empirically validate our proposal. Our code is available at https://github.com/thudzj/Calibrated-DPMs.

## 1 Introduction

In the past few years, denoising diffusion probabilistic modeling [17; 40] and score-based Langevin dynamics [42; 43] have demonstrated appealing results on generating images. Later, Song et al. [46] unify these two generative learning mechanisms through stochastic/ordinary differential equations (SDEs/ODEs). In the following we refer to this unified model family as diffusion probabilistic models (DPMs). The emerging success of DPMs has attracted broad interest in downstream applications, including image generation [10; 22; 48], shape generation [4], video generation [18; 19], super-resolution [35], speech synthesis [5], graph generation [51], textual inversion [13; 34], improving adversarial robustness [50], and text-to-image large models [32; 33], just to name a few.

A typical framework of DPMs involves a _forward_ process gradually diffusing the data distribution \(q_{0}(x_{0})\) towards a noise distribution \(q_{T}(x_{T})\). The transition probability for \(t\in[0,T]\) is a conditional Gaussian distribution \(q_{0t}(x_{t}|x_{0})=\mathcal{N}(x_{t}|\alpha_{t}x_{0},\sigma_{t}^{2}\mathbf{I})\), where \(\alpha_{t},\sigma_{t}\in\mathbb{R}^{+}\). Song et al. [46] show that there exist _reverse_ SDE/ODE processes starting from \(q_{T}(x_{T})\) and sharing the same marginal distributions \(q_{t}(x_{t})\) as the forward process. The only unknown term in the reverse processes is the data score \(\nabla_{x_{t}}\log q_{t}(x_{t})\), which can be approximated by a time-dependent score model \(\bm{s}_{\theta}^{t}(x_{t})\) (or with other model parametrizations). \(\bm{s}_{\theta}^{t}(x_{t})\) is typically learned via score matching (SM) [20].

In this work, we observe that the stochastic process of the scaled data score \(\alpha_{t}\nabla_{x_{t}}\log q_{t}(x_{t})\) is a _martingale_ w.r.t. the reverse-time process of \(x_{t}\) from \(T\) to \(0\), where the timestep \(t\) can be either continuous or discrete. Along the reverse-time sampling path, this martingale property leads to concentration bounds for scaled data scores. Moreover, a martingale satisfies the optional stopping theorem that the expected value at a stopping time is equal to its initial expected value.

Based on the martingale property of data scores, for any \(t\in[0,T]\) and any pretrained score model \(\bm{s}^{t}_{\theta}(x_{t})\) (or with other model parametrizations), we can _calibrate_ the model by subtracting its expectation over \(q_{t}(x_{t})\), i.e., \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x_{t})\right]\). We formally demonstrate that the calibrated score model \(\bm{s}^{t}_{\theta}(x_{t})-\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x _{t})\right]\) achieves lower values of SM objectives. By the connections between SM objectives and model likelihood of the SDE process [23; 45] or the ODE process [28], the calibrated score model has higher evidence lower bounds. Similar conclusions also hold for the conditional case, in which we calibrate a conditional score model \(\bm{s}^{t}_{\theta}(x_{t},y)\) by subtracting its conditional expectation \(\mathbb{E}_{q_{t}(x_{t}|y)}\left[\bm{s}^{t}_{\theta}(x_{t},y)\right]\).

In practice, \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x_{t})\right]\) or \(\mathbb{E}_{q_{t}(x_{t}|y)}\left[\bm{s}^{t}_{\theta}(x_{t},y)\right]\) can be approximated using noisy training data when the score model has been pretrained. We can also utilize an auxiliary shallow model to estimate these expectations dynamically during pretraining. When we do not have access to training data, we could calculate the expectations using data generated from \(\bm{s}^{t}_{\theta}(x_{t})\) or \(\bm{s}^{t}_{\theta}(x_{t},y)\). In experiments, we evaluate our calibration tricks on the CIFAR-10 [25] and CelebA \(64\times 64\)[27] datasets, reporting the FID scores [16]. We also provide insightful visualization results on the AFHQv2 [7], FFHQ [21] and ImageNet [9] at \(64\times 64\) resolution.

## 2 Diffusion probabilistic models

In this section, we briefly review the notations and training paradigms used in diffusion probabilistic models (DPMs). While recent works develop DPMs based on general corruptions [2; 8], we mainly focus on conventional Gaussian-based DPMs.

### Forward and reverse processes

We consider a \(k\)-dimensional random variable \(x\in\mathbb{R}^{k}\) and define a _forward_ diffusion process on \(x\) as \(\{x_{t}\}_{t\in[0,T]}\) with \(T>0\), which satisfies \(\forall t\in[0,T]\),

\[x_{0}\sim q_{0}(x_{0}),\ \ \ \ \ q_{0t}(x_{t}|x_{0})=\mathcal{N}(x_{t}|\alpha_{t} x_{0},\sigma_{t}^{2}\mathbf{I}).\] (1)

Here \(q_{0}(x_{0})\) is the data distribution; \(\alpha_{t}\) and \(\sigma_{t}\) are two positive real-valued functions that are differentiable w.r.t. \(t\) with bounded derivatives. Let \(q_{t}(x_{t})=\int q_{0t}(x_{t}|x_{0})q_{0}(x_{0})dx_{0}\) be the marginal distribution of \(x_{t}\). The schedules of \(\alpha_{t}\), \(\sigma_{t}^{2}\) need to ensure that \(q_{T}(x_{T})\approx\mathcal{N}(x_{T}|0,\widetilde{\sigma}^{2}\mathbf{I})\) for some \(\widetilde{\sigma}\). Kingma et al. [23] prove that there exists a stochastic differential equation (SDE) satisfying the forward transition distribution in Eq. (1), and this SDE can be written as

\[dx_{t}=f(t)x_{t}dt+g(t)d\omega_{t},\] (2)

where \(\omega_{t}\in\mathbb{R}^{k}\) is the standard Wiener process, \(f(t)=\frac{d\log\alpha_{t}}{dt}\), and \(g(t)^{2}=\frac{d\sigma_{t}^{2}}{dt}-2\frac{d\log\alpha_{t}}{dt}\sigma_{t}^{2}\). Song et al. [46] demonstrate that the forward SDE in Eq. (2) corresponds to a _reverse_ SDE constructed as

\[dx_{t}=\left[f(t)x_{t}-g(t)^{2}\nabla_{x_{t}}\log q_{t}(x_{t}) \right]dt+g(t)d\overline{\omega}_{t},\] (3)

where \(\overline{\omega}_{t}\in\mathbb{R}^{k}\) is the standard Wiener process in reverse time. Starting from \(q_{T}(x_{T})\), the marginal distribution of the reverse SDE process is also \(q_{t}(x_{t})\) for \(t\in[0,T]\). There also exists a deterministic process described by an ordinary differential equation (ODE) as

\[\frac{dx_{t}}{dt}=f(t)x_{t}-\frac{1}{2}g(t)^{2}\nabla_{x_{t}}\log q _{t}(x_{t}),\] (4)

which starts from \(q_{T}(x_{T})\) and shares the same marginal distribution \(q_{t}(x_{t})\) as the reverse SDE in Eq. (3). Moreover, let \(q_{0t}(x_{0}|x_{t})=\frac{q_{0t}(x_{t}|x_{0})q_{0}(x_{0})}{q_{t}(x_{t})}\) and by Tweedie's formula [12], we know that \(\alpha_{t}\mathbb{E}_{q_{0t}(x_{0}|x_{t})}\left[x_{0}\right]=x_{t}+\sigma_{t}^ {2}\nabla_{x_{t}}\log q_{t}(x_{t})\).

### Training paradigm of DPMs

To estimate the data score \(\nabla_{x_{t}}\log q_{t}(x_{t})\) at timestep \(t\), a score-based model \(\bm{s}^{t}_{\theta}(x_{t})\)[46] with shared parameters \(\theta\) is trained to minimize the score matching (SM) objective [20] as

\[\mathcal{J}^{t}_{\text{SM}}(\theta)\triangleq\frac{1}{2}\mathbb{E}_{q_{t}(x_{ t})}\left[\left\|\bm{s}^{t}_{\theta}(x_{t})-\nabla_{x_{t}}\log q_{t}(x_{t}) \right\|_{2}^{2}\right].\] (5)

To eliminate the intractable computation of \(\nabla_{x_{t}}\log q_{t}(x_{t})\), denoising score matching (DSM) [49] transforms \(\mathcal{J}^{t}_{\text{SM}}(\theta)\) into \(\mathcal{J}^{t}_{\text{DSM}}(\theta)\triangleq\frac{1}{2}\mathbb{E}_{q_{0}(x_{ 0}),q(\epsilon)}\left[\left\|\bm{s}^{t}_{\theta}(x_{t})+\frac{\epsilon}{ \sigma_{t}}\right\|_{2}^{2}\right]\), where \(x_{t}=\alpha_{t}x_{0}+\sigma_{t}\epsilon\) and \(q(\epsilon)=\mathcal{N}(\epsilon|\mathbf{0},\mathbf{I})\) is a standard Gaussian distribution. Under mild boundary conditions, we know \(\mathcal{J}_{\text{SM}}^{t}(\theta)\) and \(\mathcal{J}_{\text{DSM}}^{t}(\theta)\) is equivalent up to a constant, i.e., \(\mathcal{J}_{\text{SM}}^{t}(\theta)=\mathcal{J}_{\text{DSM}}^{t}(\theta)+C^{t}\) and \(C^{t}\) is a constant independent of the model parameters \(\theta\). Other SM variants [31; 44] are also applicable here. The total SM objective for training is a weighted sum of \(\mathcal{J}_{\text{SM}}^{t}(\theta)\) across \(t\in[0,T]\), defined as \(\mathcal{J}_{\text{SM}}(\theta;\lambda(t))\triangleq\int_{0}^{T}\lambda(t) \mathcal{J}_{\text{SM}}^{t}(\theta)dt\), where \(\lambda(t)\) is a positive weighting function. Similarly, the total DSM objective is \(\mathcal{J}_{\text{DSM}}(\theta;\lambda(t))\triangleq\int_{0}^{T}\lambda(t) \mathcal{J}_{\text{DSM}}^{t}(\theta)dt\). The training objectives under other model parametrizations such as noise prediction \(\bm{\epsilon}_{\theta}^{t}(x_{t})\)[17; 33], data prediction \(\bm{x}_{\theta}^{t}(x_{t})\)[23; 32], and velocity prediction \(\bm{v}_{\theta}^{t}(x_{t})\)[38; 18] are recapped in Appendix B.1.

### Likelihood of DPMs

Suppose that the reverse processes start from a tractable prior \(p_{T}(x_{T})=\mathcal{N}(x_{T}|0,\widehat{x}^{2}\mathbf{I})\). We can approximate the reverse-time SDE process by substituting \(\nabla_{x_{t}}\log q_{t}(x_{t})\) with \(\bm{\epsilon}_{\theta}^{t}(x_{t})\) in Eq. (3) as \(dx_{t}=\left[f(t)x_{t}-g(t)^{2}\bm{s}_{\theta}^{t}(x_{t})\right]dt+g(t)d \overline{\omega}_{t}\), which induces the marginal distribution \(p_{t}^{\text{SDE}}(x_{t};\theta)\) for \(t\in[0,T]\). In particular, at \(t=0\), the KL divergence between \(q_{0}(x_{0})\) and \(p_{0}^{\text{SDE}}(x_{0};\theta)\) can be bounded by the total SM objective \(\mathcal{J}_{\text{SM}}(\theta;g(t)^{2})\) with the weighing function of \(g(t)^{2}\), as stated below:

**Lemma 1**.: _(Proof in Song et al. [45]) Let \(q_{t}(x_{t})\) be constructed from the forward process in Eq. (2). Then under regularity conditions, we have \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{SDE}}(\theta)\right)\leq \mathcal{J}_{\text{SM}}(\theta;g(t)^{2})+\mathcal{D}_{\text{KL}}\left(q_{T}\| p_{T}\right)\)._

Here \(\mathcal{D}_{\text{KL}}\left(q_{T}\|p_{T}\right)\) is the prior loss independent of \(\theta\). Similarly, we approximate the reverse-time ODE process by substituting \(\nabla_{x_{t}}\log q_{t}(x_{t})\) with \(\bm{s}_{\theta}^{t}(x_{t})\) in Eq. (4) as \(\frac{dx_{t}}{dt}=f(t)x_{t}-\frac{1}{2}g(t)^{2}\bm{s}_{\theta}^{t}(x_{t})\), which induces the marginal distribution \(p_{t}^{\text{ODE}}(x_{t};\theta)\) for \(t\in[0,T]\). By the instantaneous change of variables formula [6], we have \(\frac{\log p_{t}^{\text{ODE}}(x_{t};\theta)}{dt}=-\text{\bf tr}\left(\nabla_{x _{t}}\left(f(t)x_{t}-\frac{1}{2}g(t)^{2}\bm{s}_{\theta}^{t}(x_{t})\right)\right)\), where \(\text{\bf tr}(\cdot)\) denotes the trace of a matrix. Integrating change in \(\log p_{t}^{\text{ODE}}(x_{t};\theta)\) from \(t=0\) to \(T\) can give the value of \(\log p_{T}(x_{T})-\log p_{0}^{\text{ODE}}(x_{0};\theta)\), but requires tracking the path from \(x_{0}\) to \(x_{T}\). On the other hand, at \(t=0\), the KL divergence between \(q_{0}(x_{0})\) and \(p_{0}^{\text{ODE}}(x_{0};\theta)\) can be decomposed:

**Lemma 2**.: _(Proof in Lu et al. [28]) Let \(q_{t}(x_{t})\) be constructed from the forward process in Eq. (2). Then under regularity conditions, we have \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta)\right)=\mathcal{ J}_{\text{SM}}(\theta;g(t)^{2})+\mathcal{D}_{\text{KL}}\left(q_{T}\|p_{T}\right)+ \mathcal{J}_{\text{Diff}}(\theta)\), where the term \(\mathcal{J}_{\text{Diff}}(\theta)\) measures the difference between \(\bm{s}_{\theta}^{t}(x_{t})\) and \(\nabla_{x_{t}}\log p_{t}^{\text{ODE}}(x_{t};\theta)\)._

Directly computing \(\mathcal{J}_{\text{Diff}}(\theta)\) is intractable due to the term \(\nabla_{x_{t}}\log p_{t}^{\text{ODE}}(x_{t};\theta)\), nevertheless, we could bound \(\mathcal{J}_{\text{Diff}}(\theta)\) via bounding high-order SM objectives [28].

## 3 Calibrating pretrained DPMs

In this section we begin with deriving the relationship between data scores at different timesteps, which leads us to a straightforward method for calibrating any pretrained DPMs. We investigate further how the dataset bias of finite samples prevents empirical learning from achieving calibration.

### The stochastic process of data score

According to Kingma et al. [23], the form of the forward process in Eq. (1) can be generalized to any two timesteps \(0\leq s<t\leq T\). Then, the transition probability from \(x_{s}\) to \(x_{t}\) is written as \(q_{st}(x_{t}|x_{s})=\mathcal{N}\left(x_{t}\Big{|}\alpha_{t|s}x_{s},\sigma_{t|s}^ {2}\mathbf{I}\right)\), where \(\alpha_{t|s}=\frac{\alpha_{t}}{\alpha_{s}}\) and \(\sigma_{t|s}^{2}=\sigma_{t}^{2}-\alpha_{t|s}^{2}\sigma_{s}^{2}\). Here the marginal distribution satisfies \(q_{t}(x_{t})=\int q_{st}(x_{t}|x_{s})q_{s}(x_{s})dx_{s}\). We can generally derive the connection between data scores \(\nabla_{x_{t}}\log q_{t}(x_{t})\) and \(\nabla_{x_{s}}\log q_{s}(x_{s})\) as stated below:

**Theorem 1**.: _(Proof in Appendix A.1) Let \(q_{t}(x_{t})\) be constructed from the forward process in Eq. (2). Then under some regularity conditions, we have \(\forall 0\leq s<t\leq T\),_

\[\alpha_{t}\nabla_{x_{t}}\log q_{t}(x_{t})=\mathbb{E}_{q_{st}(x_{s}|x_{t})}\left[ \alpha_{s}\nabla_{x_{s}}\log q_{s}(x_{s})\right],\] (6)

_where \(q_{st}(x_{s}|x_{t})=\frac{q_{st}(x_{t}|x_{s})q_{s}(x_{s})}{q_{t}(x_{t})}\) is the transition probability from \(x_{t}\) to \(x_{s}\)._

Theorem 1 indicates that the stochastic process of \(\alpha_{t}\nabla_{x_{t}}\log q_{t}(x_{t})\) is a _martingale_ w.r.t. the reverse-time process of \(x_{t}\) from timestep \(T\) to \(0\). From the optional stopping theorem [14], the expectedvalue of a martingale at a stopping time is equal to its initial expected value \(\mathbb{E}_{q_{0}(x_{0})}\left[\nabla_{x_{0}}\log q_{0}(x_{0})\right]\). It is known that, under a mild boundary condition on \(q_{0}(x_{0})\), there is \(\mathbb{E}_{q_{0}(x_{0})}\left[\nabla_{x_{0}}\log q_{0}(x_{0})\right]=0\) (proof is recapped in Appendix A.2). Consequently, as to the stochastic process, the martingale property results in \(\mathbb{E}_{q_{t}(x_{t})}\left[\nabla_{x_{t}}\log q_{t}(x_{t})\right]=0\) for \(\forall t\in[0,T]\). Moreover, the martingale property of the (scaled) data score \(\alpha_{t}\nabla_{x_{t}}\log q_{t}(x_{t})\) leads to concentration bounds using Azuma's inequality and Doob's martingale inequality as derived in Appendix A.3. Although we do not use these concentration bounds further in this paper, there are other concurrent works that use roughly similar concentration bounds in diffusion models, such as proving consistency [47] or justifying trajectory retrieval [52].

### A simple calibration trick

Given a pretrained model \(\bm{s}^{t}_{\theta}(x_{t})\) in practice, there is usually \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x_{t})\right]\neq 0\), despite the fact that the expect data score is zero as \(\mathbb{E}_{q_{t}(x_{t})}\left[\nabla_{x_{t}}\log q_{t}(x_{t})\right]=0\). This motivates us to calibrate \(\bm{s}^{t}_{\theta}(x_{t})\) to \(\bm{s}^{t}_{\theta}(x_{t})-\eta_{t}\), where \(\eta_{t}\) is a time-dependent calibration term that is independent of any particular input \(x_{t}\). The calibrated SM objective is written as follows:

\[\begin{split}\mathcal{J}^{t}_{\text{SM}}(\theta,\eta_{t})& \triangleq\frac{1}{2}\mathbb{E}_{q_{t}(x_{t})}\left[\|\bm{s}^{t}_{ \theta}(x_{t})-\eta_{t}-\nabla_{x_{t}}\log q_{t}(x_{t})\|_{2}^{2}\right]\\ &=\mathcal{J}^{t}_{\text{SM}}(\theta)-\mathbb{E}_{q_{t}(x_{t})} \left[\bm{s}^{t}_{\theta}(x_{t})\right]^{\top}\eta_{t}+\frac{1}{2}\|\eta_{t}\| _{2}^{2},\end{split}\] (7)

where the second equation holds after the results of \(\mathbb{E}_{q_{t}(x_{t})}\left[\nabla_{x_{t}}\log q_{t}(x_{t})\right]=0\), and there is \(\mathcal{J}^{t}_{\text{SM}}(\theta,0)=\mathcal{J}^{t}_{\text{SM}}(\theta)\) specifically when \(\eta_{t}=0\). Note that the orange part in Eq. (7) is a quadratic function w.r.t. \(\eta_{t}\). We look for the optimal \(\eta_{t}^{*}=\arg\min_{\eta_{t}}\mathcal{J}^{t}_{\text{SM}}(\theta,\eta_{t})\) that minimizes the calibrated SM objective, from which we can derive

\[\eta_{t}^{*}=\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x_{t})\right].\] (8)

After taking \(\eta_{t}^{*}\) into \(\mathcal{J}^{t}_{\text{SM}}(\theta,\eta_{t})\), we have

\[\mathcal{J}^{t}_{\text{SM}}(\theta,\eta_{t}^{*})=\mathcal{J}^{t}_{\text{SM}}( \theta)-\frac{1}{2}\left\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x _{t})\right]\right\|_{2}^{2}.\] (9)

Since there is \(\mathcal{J}^{t}_{\text{SM}}(\theta)=\mathcal{J}^{t}_{\text{DSM}}(\theta)+C^{t}\), we have \(\mathcal{J}^{t}_{\text{DSM}}(\theta,\eta_{t}^{*})=\mathcal{J}^{t}_{\text{ DSM}}(\theta)-\frac{1}{2}\left\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{ \theta}(x_{t})\right]\right\|_{2}^{2}\) for the DSM objective. Similar calibration tricks are also valid under other model parametrizations and SM variants, as formally described in Appendix B.2.

**Remark.** For any pretrained score model \(\bm{s}^{t}_{\theta}(x_{t})\), we can calibrate it into \(\bm{s}^{t}_{\theta}(x_{t})-\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}( x_{t})\right]\), which reduces the SM/DSM objectives at timestep \(t\) by \(\frac{1}{2}\left\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x_{t}) \right]\right\|_{2}^{2}\). The expectation of the calibrated score model is always zero, i.e., \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}^{t}_{\theta}(x_{t})-\mathbb{E}_{q_{t}(x _{t})}\left[\bm{s}^{t}_{\theta}(x_{t})\right]\right]=0\) holds for any \(\theta\), which is consistent with \(\mathbb{E}_{q_{t}(x_{t})}\left[\nabla_{x_{t}}\log q_{t}(x_{t})\right]=0\) satisfied by data scores.

**Calibration preserves conservativeness.** A theoretical flaw of score-based modeling is that \(\bm{s}^{t}_{\theta}(x_{t})\) may not correspond to a probability distribution. To solve this issue, Salimans and Ho [37] develop an energy-based model design, which utilizes the power of score-based modeling and simultaneously makes sure that \(\bm{s}^{t}_{\theta}(x_{t})\) is conservative, i.e., there exists a probability distribution \(p^{t}_{\theta}(x_{t})\) such that \(\forall x_{t}\in\mathbb{R}^{k}\), we have \(\bm{s}^{t}_{\theta}(x_{t})=\nabla_{x_{t}}\log p^{t}_{\theta}(x_{t})\). In this case, after we calibrate \(\bm{s}^{t}_{\theta}(x_{t})\) by subtracting \(\eta_{t}\), there is \(\bm{s}^{t}_{\theta}(x_{t})-\eta_{t}=\nabla_{x_{t}}\log\left(\frac{p^{t}_{ \theta}(x_{t})}{\exp\left(x_{t}^{\top}\eta_{t}\right)}z_{t}(\theta)\right)\), where \(Z_{t}(\theta)=\int p^{t}_{\theta}(x_{t})\exp\left(-x_{t}^{\top}\eta_{t}\right) dx_{t}\) represents the normalization factor. Intuitively, subtracting by \(\eta_{t}\) corresponds to a shift in the vector space, so if \(\bm{s}^{t}_{\theta}(x_{t})\) is conservative, its calibrated version \(\bm{s}^{t}_{\theta}(x_{t})-\eta_{t}\) is also conservative.

**Conditional cases.** As to the conditional DPMs, we usually employ a conditional model \(\bm{s}^{t}_{\theta}(x_{t},y)\), where \(y\in\mathcal{Y}\) is the conditional context (e.g., class label or text prompt). To learn the conditional data score \(\nabla_{x_{t}}\log q_{t}(x_{t}|y)=\nabla_{x_{t}}\log q_{t}(x_{t},y)\), we minimize the SM objective defined as \(\mathcal{J}^{t}_{\text{SM}}(\theta)\triangleq\frac{1}{2}\mathbb{E}_{q_{t}(x_{t},y )}\left[\|\bm{s}^{t}_{\theta}(x_{t},y)-\nabla_{x_{t}}\log q_{t}(x_{t},y)\|_{2}^ {2}\right]\). Similar to the conclusion of \(\mathbb{E}_{q_{t}(x_{t})}\left[\nabla_{x_{t}}\log q_{t}(x_{t})\right]=0\), there is \(\mathbb{E}_{q_{t}(x_{t}|y)}\left[\nabla_{x_{t}}\log q_{t}(x_{t}|y)\right]=0\). To calibrate \(\bm{s}^{t}_{\theta}(x_{t},y)\), we use the conditional term \(\eta_{t}(y)\) and the calibrated SM objective is formulated as

\[\begin{split}\mathcal{J}^{t}_{\text{SM}}(\theta,\eta_{t}(y))& \triangleq\frac{1}{2}\mathbb{E}_{q_{t}(x_{t},y)}\left[\|\bm{s}^{t}_{\theta}( x_{t},y)-\eta_{t}(y)-\nabla_{x_{t}}\log q_{t}(x_{t},y)\|_{2}^{2}\right]\\ &=\mathcal{J}^{t}_{\text{SM}}(\theta)-\mathbb{E}_{q_{t}(x_{t},y)} \left[\bm{s}^{t}_{\theta}(x_{t},y)^{\top}\eta_{t}(y)+\frac{1}{2}\|\eta_{t}(y)\|_{2}^ {2}\right],\end{split}\] (10)and for any \(y\in\mathcal{Y}\), the optimal \(\eta_{t}^{*}(y)\) is given by \(\eta_{t}^{*}(y)=\mathbb{E}_{q_{t}(x_{t}|y)}\left[\bm{s}_{\theta}^{t}(x_{t},y)\right]\). We highlight the conditional context \(y\) in contrast to the unconditional form in Eq. (7). After taking \(\eta_{t}^{*}(y)\) into \(\mathcal{J}_{\text{SM}}^{t}(\theta,\eta_{t}(y))\), we have \(\mathcal{J}_{\text{SM}}^{t}(\theta,\eta_{t}^{*}(y))=\mathcal{J}_{\text{SM}}^ {t}(\theta)-\frac{1}{2}\mathbb{E}_{q_{t}(y)}\left[\left\|\mathbb{E}_{q_{t}(x_{t }|y)}\left[\bm{s}_{\theta}^{t}(x_{t},y)\right]\right\|_{2}^{2}\right]\). This conditional calibration form can naturally generalize to other model parametrizations and SM variants.

### Likelihood of calibrated DPMs

Now we discuss the effects of calibration on model likelihood. Following the notations in Section 2.3, we use \(p_{0}^{\text{SDE}}(\theta,\eta_{t})\) and \(p_{0}^{\text{ODE}}(\theta,\eta_{t})\) to denote the distributions induced by the reverse-time SDE and ODE processes, respectively, where \(\nabla_{x_{t}}\log q_{t}(x_{t})\) is substituted with \(\bm{s}_{\theta}^{t}(x_{t})-\eta_{t}\).

**Likelihood of \(p_{0}^{\text{SDE}}(\theta,\eta_{t})\).** Let \(\mathcal{J}_{\text{SM}}(\theta,\eta_{t};g(t)^{2})\triangleq\int_{0}^{T}g(t)^ {2}\mathcal{J}_{\text{SM}}^{t}(\theta,\eta_{t})dt\) be the total SM objective after the score model is calibrated by \(\eta_{t}\), then according to Lemma 1, we have \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{SDE}}(\theta,\eta_{t}) \right)\leq\mathcal{J}_{\text{SM}}(\theta,\eta_{t};g(t)^{2})+\mathcal{D}_{ \text{KL}}\left(q_{T}\|p_{T}\right)\). From the result in Eq. (9), there is

\[\mathcal{J}_{\text{SM}}(\theta,\eta_{t}^{*};g(t)^{2})=\mathcal{J}_{\text{SM}} (\theta;g(t)^{2})-\frac{1}{2}\int_{0}^{T}g(t)^{2}\left\|\mathbb{E}_{q_{t}(x_{ t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\right\|_{2}^{2}dt.\] (11)

Therefore, the likelihood \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{SDE}}(\theta,\eta_{t}^{*})\right)\) after calibration has a lower upper bound of \(\mathcal{J}_{\text{SM}}(\theta,\eta_{t}^{*};g(t)^{2})+\mathcal{D}_{\text{KL} }\left(q_{T}\|p_{T}\right)\), compared to the bound of \(\mathcal{J}_{\text{SM}}(\theta;g(t)^{2})+\mathcal{D}_{\text{KL}}\left(q_{T}\| p_{T}\right)\) for the original \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{SDE}}(\theta)\right)\). However, we need to clarify that \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{SDE}}(\theta,\eta_{t}^{*})\right)\) may not necessarily smaller than \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{SDE}}(\theta)\right)\), since we can only compare their upper bounds.

**Likelihood of \(p_{0}^{\text{ODE}}(\theta,\eta_{t})\).** Note that in Lemma 2, there is a term \(\mathcal{J}_{\text{Diff}}(\theta)\), which is usually small in practice since \(\bm{s}_{\theta}^{t}(x_{t})\) and \(\nabla_{x_{t}}\log p_{t}^{\text{ODE}}(x_{t};\theta)\) are close. Thus, we have

\[\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta,\eta_{t})\right) \approx\mathcal{J}_{\text{SM}}(\theta,\eta_{t};g(t)^{2})+\mathcal{D}_{\text{KL }}\left(q_{T}\|p_{T}\right),\]

and \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta,\eta_{t}^{*})\right)\) approximately achieves its lowest value. Lu et al. [28] show that \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta)\right)\) can be further bounded by high-order SM objectives (as detailed in Appendix A.4), which depend on \(\nabla_{x_{t}}\bm{s}_{\theta}^{t}(x_{t})\) and \(\nabla_{x_{t}}\textbf{tr}\left(\nabla_{x_{t}}\bm{s}_{\theta}^{t}(x_{t})\right)\). Since the calibration term \(\eta_{t}\) is independent of \(x_{t}\), i.e., \(\nabla_{x_{t}}\eta_{t}=0\), it does not affect the values of high-order SM objectives, and achieves a lower upper bound due to the lower value of the first-order SM objective.

Empirical learning fails to achieve \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]=0\)

A question that naturally arises is whether better architectures or learning algorithms for DPMs (e.g., EDMs [22]) could empirically achieve \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]=0\) without calibration? The answer may be negative, since in practice we only have access to a _finite_ dataset sampled from \(q_{0}(x_{0})\). More specifically, assuming that we have a training dataset \(\mathbb{D}\triangleq\{x_{0}^{n}\}_{n=1}^{N}\) where \(x_{0}^{n}\sim q_{0}(x_{0})\), and defining the kernel density distribution induced by \(\mathbb{D}\) as \(q_{t}(x_{t};\mathbb{D})\propto\sum_{n=1}^{N}\mathcal{N}\left(\frac{x_{t}-\alpha _{t}x_{0}^{n}}{\sigma_{t}}\left|\mathbf{0},\mathbf{I}\right)\). When the quantity of training data approaches infinity, we have \(\lim_{N\to\infty}q_{t}(x_{t};\mathbb{D})=q_{t}(x_{t})\) holds for \(\forall t\in[0,T]\). Then the empirical DSM objective trained on \(\mathbb{D}\) is written as

\[\mathcal{J}_{\text{DSM}}^{t}(\theta;\mathbb{D})\triangleq\frac{1}{2N}\sum_{n=1 }^{N}\mathbb{E}_{q(\epsilon)}\left[\left\|\bm{s}_{\theta}^{t}(\alpha_{t}x_{0}^ {n}+\sigma_{t}\epsilon)+\frac{\epsilon}{\sigma_{t}}\right\|_{2}^{2}\right],\] (12)

and it is easy to show that the optimal solution for minimizing \(\mathcal{J}_{\text{DSM}}^{t}(\theta;\mathbb{D})\) satisfies (assuming \(\bm{s}_{\theta}^{t}\) has universal model capacity) \(\bm{s}_{\theta}^{t}(x_{t})=\nabla_{x_{t}}\log q_{t}(x_{t};\mathbb{D})\). Given a finite dataset \(\mathbb{D}\), there is

\[\mathbb{E}_{q_{t}(x_{t};\mathbb{D})}\left[\nabla_{x_{t}}\log q_{t}(x_{t}; \mathbb{D})\right]=0,\] (13)

indicating that even if the score model is learned to be optimal, there is still \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\neq 0\). Thus, the mis-calibration of DPMs is partially due to the _dataset bias_, i.e., during training we can only access a finite dataset \(\mathbb{D}\) sampled from \(q_{0}(x_{0})\).

Furthermore, when trained on a finite dataset in practice, the learned model will not converge to the optimal solution [15], so there is typically \(\bm{s}_{\theta}^{t}(x_{t})\neq\nabla_{x_{t}}\log q_{t}(x_{t};\mathbb{D})\) and \(\mathbb{E}_{q_{t}(x_{t};\mathbb{D})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\neq 0\). After calibration, we can at least guarantee that \(\mathbb{E}_{q_{t}(x_{t};\mathbb{D})}\left[\bm{s}_{\theta}^{t}(x_{t})-\mathbb{E}_{q _{t}(x_{t};\mathbb{D})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\right]=0\) always holds on any finite dataset \(\mathbb{D}\). In Figure 3, we demonstrate that even state-of-the-art EDMs still have non-zero and semantic \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\), which emphasises the significance of calibrating DPMs.

### Amortized computation of \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\)

By default, we are able to calculate and restore the value of \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\) for a pretrained model \(\bm{s}_{\theta}^{t}(x_{t})\), where the selection of timestep \(t\) is determined by the inference algorithm, and the expectation over \(q_{t}(x_{t})\) can be approximated by Monte Carlo sampling from a noisy training set. When we do not have access to training data, we can approximate the expectation using data generated from \(p_{t}^{\text{ODE}}(x_{t};\theta)\) or \(p_{t}^{\text{ODE}}(x_{t};\theta)\). Since we only need to calculate \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\) once, the raised computational overhead is amortized as the number of generated samples increases.

**Dynamically recording.** In the preceding context, we focus primarily on post-training computing of \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\). An alternative strategy would be to dynamically record \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\) during the pretraining phase of \(\bm{s}_{\theta}^{t}(x_{t})\). Specifically, we could construct an auxiliary shallow network \(h_{\phi}(t)\) parameterized by \(\phi\), whose input is the timestep \(t\). We define the expected mean squared error as

\[\mathcal{J}_{\text{Cal}}^{t}(\phi)\triangleq\mathbb{E}_{q_{t}(x_{t})}\left[ \|h_{\phi}(t)-\bm{s}_{\theta}^{t}(x_{t})^{\dagger}\|_{2}^{2}\right],\] (14)

where the superscript \(\dagger\) denotes the stopping gradient and \(\phi^{*}\) is the optimal solution of minimizing \(\mathcal{J}_{\text{Cal}}^{t}(\phi)\) w.r.t. \(\phi\), satisfying \(h_{\phi^{*}}(t)=\eta_{t}^{*}=\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{ t}(x_{t})\right]\) (assuming sufficient model capacity). The total training objective can therefore be expressed as \(\mathcal{J}_{\text{SM}}(\theta;\lambda(t))+\int_{0}^{T}\beta_{t}\cdot\mathcal{ J}_{\text{Cal}}^{t}(\phi)\), where \(\beta_{t}\) is a time-dependent trade-off coefficient for \(t\in[0,T]\).

## 4 Experiments

In this section, we demonstrate that sample quality and model likelihood can be both improved by calibrating DPMs. Instead of establishing a new state-of-the-art, the purpose of our empirical studies is to testify the efficacy of our calibration technique as a simple way to repair DPMs.

### Sample quality

**Setup.** We apply post-training calibration to discrete-time models trained on CIFAR-10 [25] and CelebA [27], which apply _parametrization of noise prediction_\(\bm{\epsilon}_{\theta}^{t}(x_{t})\). In the sampling phase, we employ DPM-Solver [29], an ODE-based sampler that achieves a promising balance between sample efficiency and image quality. Because our calibration directly acts on model scores, it is also compatible with other ODE/SDE-based samplers [3, 26], while we only focus on DPM-Solver cases in this paper. In accordance with the recommendation, we set the end time of DPM-Solver to \(10^{-3}\) when the number of sampling steps is less than \(15\), and to \(10^{-4}\) otherwise. Additional details can befound in Lu et al. [29]. By default, we employ the FID score [16] to quantify the sample quality using 50,000 samples. Typically, a lower FID indicates a higher sample quality. In addition, in Table 3, we evaluate using other metrics such as sFID [30], IS [39], and Precision/Recall [36].

**Computing \(\mathbb{E}_{q_{t}(x_{t})}\left[\boldsymbol{\epsilon}_{\theta}^{t}(x_{t})\right]\).** To estimate the expectation over \(q_{t}(x_{t})\), we construct \(x_{t}=\alpha_{t}x_{0}+\sigma_{t}\epsilon\), where \(x_{0}\sim q_{0}(x_{0})\) is sampled from the training set and \(\epsilon\sim\mathcal{N}(\epsilon|\mathbf{0},\mathbf{I})\) is sampled from a standard Gaussian distribution. The selection of timestep \(t\) depends on the sampling schedule of DPM-Solver. The computed values of \(\mathbb{E}_{q_{\ell}(x_{t})}\left[\boldsymbol{\epsilon}_{\theta}^{t}(x_{t})\right]\) are restored in a dictionary and warped into the output layers of DPMs, allowing existing inference pipelines to be reused.

We first calibrate the model trained by Ho et al. [17] on the CIFAR-10 dataset and compare it to the original one for sampling with DPM-Solvers. We conduct a systematical study with varying NFE (i.e., number of function evaluations) and solver order. The results are presented in Tables 1 and 3. After calibrating the model, the sample quality is consistently enhanced, which demonstrates the significance of doing so and the efficacy of our method. We highlight the significant improvement in sample quality (4.61\(\rightarrow\)4.22 when using 15 NFE and \(3\)-order DPM-Solver; 3.89\(\rightarrow\)3.32 when using 20 NFE and \(3\)-order DPM-Solver). After model calibration, the number of steps required to achieve convergence for a \(3\)-order DPM-Solver is reduced from \(\geq\)30 to 20, making our method a new option for expediting the sampling of DPMs. In addition, as a point of comparison, the \(3\)-order DPM-Solver with 1,000 NFE can only yield an FID score of 3.45 when using the original model, which, along with the results in Table 1, indicates that model calibration helps to improve the convergence of sampling.

Then, we conduct experiments with the discrete-time model trained on the CelebA 64x64 dataset by Song et al. [41]. The corresponding sample quality comparison is shown in Table 2. Clearly, model calibration brings significant gains (3.91\(\rightarrow\)3.62 when using 15 NFE and \(3\)-order DPM-Solver; 2.84\(\rightarrow\)2.33 when using 20 NFE and \(3\)-order DPM-Solver) that are consistent with those on the CIFAR-10 dataset. This demonstrates the prevalence of the mis-calibration issue in existing DPMs and the efficacy of our correction. We still observe that model calibration improves convergence of sampling, and as shown in Figure 2, our calibration could help to reduce ambiguous generations. More generated images are displayed in Appendix C.

\begin{table}
\begin{tabular}{l l r r r r r r r} \hline \hline \multirow{2}{*}{Noise prediction} & \multirow{2}{*}{DPM-Solver} & \multicolumn{8}{c}{Number of evaluations (NFE)} \\  & & 10 & 15 & 20 & 25 & 30 & 35 & 40 \\ \hline \multirow{4}{*}{\(\boldsymbol{\epsilon}_{\theta}^{t}(x_{t})\)} & \(1\)-order & 20.49 & 12.47 & 9.72 & 7.89 & 6.84 & 6.22 & 5.75 \\  & \(2\)-order & 7.35 & \({}^{\dagger}\)4.52 & 4.14 & \({}^{\dagger}\)3.92 & 3.74 & \({}^{\dagger}\)3.71 & 3.68 \\  & \(3\)-order & \({}^{\dagger}\)23.96 & 4.61 & \({}^{\dagger}\)3.89 & \({}^{\dagger}\)3.73 & 3.65 & \({}^{\dagger}\)3.65 & \({}^{\dagger}\)3.60 \\ \hline \multirow{4}{*}{\(\boldsymbol{\epsilon}_{\theta}^{t}(x_{t})-\mathbb{E}_{q_{t}(x_{t})}\left[ \boldsymbol{\epsilon}_{\theta}^{t}(x_{t})\right]\)} & \(1\)-order & 19.31 & 11.77 & 8.86 & 7.35 & 6.28 & 5.76 & 5.36 \\  & \(2\)-order & **6.76** & \({}^{\dagger}\)4.36 & 4.03 & \({}^{\dagger}\)3.66 & 3.54 & \({}^{\dagger}\)3.44 & 3.48 \\  & \(3\)-order & \({}^{\dagger}\)53.50 & **4.22** & \({}^{\dagger}\)**3.32** & \({}^{\dagger}\)**3.33** & **3.35** & \({}^{\dagger}\)**3.32** & \({}^{\dagger}\)**3.31** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison on sample quality measured by FID \(\downarrow\) with varying NFE on CIFAR-10. Experiments are conducted using a linear noise schedule on the discrete-time model from [17]. We consider three variants of DPM-Solver with different orders. The results with \(\dagger\) mean the actual NFE is order \(\times\left\lfloor\frac{\text{NFE}}{\text{order}}\right\rfloor\) which is smaller than the given NFE, following the setting in [29].

\begin{table}
\begin{tabular}{l l r r r r r r r} \hline \hline \multirow{2}{*}{Noise prediction} & \multirow{2}{*}{DPM-Solver} & \multicolumn{8}{c}{Number of evaluations (NFE)} \\  & & 10 & 15 & 20 & 25 & 30 & 35 & 40 \\ \hline \multirow{4}{*}{\(\boldsymbol{\epsilon}_{\theta}^{t}(x_{t})\)} & \(1\)-order & 16.74 & 11.85 & 7.93 & 6.67 & 5.90 & 5.38 & 5.01 \\  & \(2\)-order & **4.32** & \({}^{\dagger}\)3.98 & 2.94 & \({}^{\dagger}\)2.88 & 2.88 & \({}^{\dagger}\)2.88 & \(2.84\) \\  & \(3\)-order & \({}^{\dagger}\)11.92 & 3.91 & \({}^{\dagger}\)2.84 & \({}^{\dagger}\)2.76 & 2.82 & \({}^{\dagger}\)2.81 & \({}^{\dagger}\)2.85 \\ \hline \multirow{4}{*}{\(\boldsymbol{\epsilon}_{\theta}^{t}(x_{t})-\mathbb{E}_{q_{t}(x_{t})}\left[ \boldsymbol{\epsilon}_{\theta}^{t}(x_{t})\right]\)} & \(1\)-order & 16.13 & 11.29 & 7.09 & 6.06 & 5.28 & 4.87 & 4.39 \\  & \(2\)-order & 4.42 & \({}^{\dagger}\)3.94 & 2.61 & \({}^{\dagger}\)2.66 & 2.54 & \({}^{\dagger}\)2.52 & **2.49** \\  & \(3\)-order & \({}^{\dagger}\)35.47 & **3.62** & \({}^{\dagger}\)**2.33** & \({}^{\dagger}\)**2.43** & **2.40** & \({}^{\dagger}\)**2.43** & \({}^{\dagger}\)**2.49** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison on sample quality measured by FID \(\downarrow\) with varying NFE on CelebA 64\(\times\)64. Experiments are conducted using a linear noise schedule on the discrete-time model from [41]. The settings of DPM-Solver are the same as on CIFAR-10.

### Model likelihood

As described in Section 3.3, calibration contributes to reducing the SM objective, thereby decreasing the upper bound of the KL divergence between model distribution at timestep \(t=0\) (either \(p_{0}^{\text{SDE}}(\theta,\eta_{t}^{*})\) or \(p_{0}^{\text{ODE}}(\theta,\eta_{t}^{*})\)) and data distribution \(q_{0}\). Consequently, it aids in raising the lower bound of model likelihood. In this subsection, we examine such effects by evaluating the aforementioned DPOs on the CIFAR-10 and CelebA datasets. We also conduct experiments with continuous-time models trained by Karras et al. [22] on AFHQv2 64\(\times\)64 [7], FFHQ 64\(\times\)64 [21], and ImageNet 64\(\times\)64 [9] datasets considering their top performance. These models apply parametrization of data prediction \(\bm{x}_{\theta}^{t}(x_{t})\), and for consistency, we convert it to align with \(\bm{e}_{\theta}^{t}(x_{t})\) based on the relationship \(\bm{e}_{\theta}^{t}(x_{t})=(x_{t}-\alpha_{t}\bm{x}_{\theta}^{t}(x_{t}))/\sigma _{t}\), as detailed in Kingma et al. [23] and Appendix B.2.

Given that we employ noise prediction models in practice, we first estimate \(\frac{1}{2}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{e}_{\theta}^{t}(x_{t})\right] \|_{2}^{2}\) at timestep \(t\in[0,T]\), which reflects the decrement on the SM objective at \(t\) according to Eq. (9) (up to a scaling factor of \(1/\sigma_{t}^{2}\)). We approximate the expectation using Monte Carlo (MC) estimation with training data points. The results are displayed in the first row of Figure 1. Notably, the value of \(\frac{1}{2}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{e}_{\theta}^{t}(x_{t})\right] \|_{2}^{2}\) varies significantly along with timestep \(t\): it decreases relative to \(t\) for CelebA but increases in all other cases (except for \(t\in[0.4,1.0]\) on ImageNet 64\(\times\)64). Ideally, there should be \(\frac{1}{2}\|\mathbb{E}_{q_{t}(x_{t})}\left[\nabla_{x_{t}}\log q_{t}(x_{t}) \right]\|_{2}^{2}=0\) at any \(t\). Such inconsistency reveals that mis-calibration issues exist in general, although the phenomenon may vary across datasets and training mechanisms.

Then, we quantify the gain of model calibration on increasing the lower bound of model likelihood, which is \(\frac{1}{2}\int_{0}^{T}g(t)^{2}\left\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{ \theta}^{t}(x_{t})\right]\right\|_{2}^{2}dt\) according to Eq. (11). We first rewrite it with the model parametrization of noise prediction \(\bm{e}_{\theta}^{t}(x_{t})\), and it can be straightforwardly demonstrated that it equals \(\int_{0}^{T}\frac{g(t)^{2}}{2\sigma_{t}^{2}}\|\mathbb{E}_{q_{t}(x_{t})}\left[ \bm{e}_{\theta}^{t}(x_{t})\right]\|_{2}^{2}\). Therefore, we calculate the value of \(\frac{g(t)^{2}}{2\sigma_{t}^{2}}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{e}_{ \theta}^{t}(x_{t})\right]\|_{2}^{2}\) using MC

\begin{table}
\begin{tabular}{c|c c c c c|c c c c c|c c c c c} \hline \hline \multirow{3}{*}{Method} & \multicolumn{10}{c|}{Number of evaluations (NFE)} \\  & \multicolumn{4}{c|}{20} & \multicolumn{4}{c|}{25} & \multicolumn{4}{c}{30} \\  & FID & sFID & IS & Pre. & Rec. & FID & sFID & IS & Pre. & Rec. & FID & sFID & IS & Pre. & Rec. \\ \hline \(1\)-ord. & 9.72 & 6.03 & 8.49 & 0.641 & 0.542 & 7.89 & 5.45 & 8.68 & 0.644 & 0.556 & 6.84 & 5.12 & 8.76 & 0.650 & 0.565 \\ Base \(2\)-ord. & 4.14 & 4.36 & 9.15 & 0.654 & 0.590 & 3.92 & 4.22 & 9.17 & 0.657 & 0.591 & 3.74 & 4.18 & 9.20 & 0.658 & 0.591 \\ \(3\)-ord. & 3.89 & 4.18 & 9.29 & 0.652 & 0.597 & 3.73 & 4.15 & 9.21 & 0.657 & 0.595 & 3.65 & 4.12 & 9.22 & 0.658 & 0.593 \\ \hline \(1\)-ord. & 8.86 & 6.01 & 8.56 & 0.649 & 0.544 & 7.35 & 5.42 & 8.76 & 0.653 & 0.560 & 6.28 & 5.09 & 8.84 & 0.653 & 0.568 \\ Ours \(2\)-ord. & 4.03 & 4.31 & 9.17 & **0.661** & 0.592 & 3.66 & 4.20 & 9.20 & 0.664 & 0.594 & 3.54 & 4.14 & 9.23 & **0.662** & 0.599 \\ \(3\)-ord. & **3.32** & **4.14** & **9.38** & 0.657 & **0.603** & **3.33** & **4.11** & **9.28** & **0.665** & **0.597** & **3.35** & **4.08** & **9.27** & **0.662** & **0.600** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison on sample quality measured by different metrics, including FID \(\downarrow\), sFID \(\downarrow\), inception score (IS) \(\uparrow\), precision \(\uparrow\) and recall \(\uparrow\) with varying NFE on CIFAR-10. We use Base to denote the baseline \(\bm{e}_{\theta}^{t}(x_{t})\) and Ours to denote calibrated score \(\bm{e}_{\theta}^{t}(x_{t})-\mathbb{E}_{q_{(x_{t})}}\left[\bm{e}_{\theta}^{t}(x _{t})\right]\). The sampler is DPM-Solver with different orders. Note that FID is computed by the PyTorch checkpoint of Inception-v3, while sFID/IS/Precision/Recall are computed by the Tensorflow checkpoint of Inception-v3 following _github.com/kynkaat/improved-precision-and-recall-metric_.

Figure 2: Selected images on CIFAR-10 (generated with NFE \(=20\) using 3-order DPM-Solver) demonstrating that our calibration could reduce ambiguous generations, such as generations that resemble both horse and dog. However, we must emphasize that not all generated images have a visually discernible difference before and after calibration.

estimation and report the results in the second row of Figure 1. The integral is represented by the area under the curve (i.e., the gain of model calibration on the lower bound of model likelihood). Various datasets and model architectures exhibit non-trivial gains, as observed. In addition, we notice that the DPMs trained by Karras et al. [22] show patterns distinct from those of DDPM [17] and DDIM [41], indicating that different DPM training mechanisms may result in different mis-calibration effects.

**Visualizing \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\).** To better understand the inductive bias learned by DPMs, we visualize the expected predicted noises \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) for timestep from \(0\) to \(T\), as seen in Figure 3. For each dataset, the first row normalizes the values of \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) into \([0,255]\); the second row calculates pixel-wise norm (across RGB channels) and highlights the top-\(10\%\) locations with the highest norm. As we can observe, on facial datasets like CelebA and FFHQ, there are obvious facial patterns inside \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\), while on other datasets like CIFAR-10, ImageNet, as well as the animal face dataset AFHQv2, the patterns inside \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) are more like random noises. Besides, the facial patterns in Figure 3 are more significant when \(t\) is smaller, and become blurry when \(t\) is close to \(T\). This phenomenon may be attributed to the bias of finite training data, which is detrimental to generalization during sampling and justifies the importance of calibration as described in Section 3.4.

### Ablation studies

We conduct ablation studies focusing on the estimation methods of \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\).

**Estimating \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) with partial training data.** In the post-training calibration setting, our primary algorithmic change is to subtract the calibration term \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) from the pretrained DPMs' output. In the aforementioned studies, the expectation in \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) (or its variant of other model parametrizations) is approximated with MC estimation using all training images. However, there may be situations where training data are (partially) inaccessible. To evaluate the effectiveness of our method under these cases, we examine the number of training images used to estimate the calibration term on CIFAR-10. To determine the quality of the estimated calibration term, we sample from the calibrated models using a \(3\)-order DPM-Solver running for 20 steps and evaluate the corresponding FID score. The results are listed in the left part of Table 4. As observed, we need to use the majority of training images (at least \(\geq\) 20,000) to estimate the calibration term. We deduce that this is because the CIFAR-10 images are rich in diversity, necessitating a non-trivial number of training images to cover the various modes and produce a nearly unbiased calibration term.

**Estimating \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) with generated data.** In the most extreme case where we do not have access to any training data (e.g., due to privacy concerns), we could still estimate the expectation over \(q_{t}(x_{t})\) with data generated from \(p_{0}^{\text{ODE}}(x_{0};\theta)\) or \(p_{0}^{\text{SDE}}(x_{0};\theta)\). Specifically, under the hypothesis that \(p_{0}^{\text{ODE}}(x_{0};\theta)\approx q_{0}(x_{0})\) (DPM-Solver is an ODE-based sampler), we first generate \(\widetilde{x}_{0}\sim p_{0}^{\text{ODE}}(x_{0};\theta)\)

Figure 3: Visualization of the expected predicted noises with increasing \(t\). For each dataset, the first row displays \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) (after normalization) and the second row highlights the top-\(10\%\) pixels that \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) has high values. The DPM on CelebA is a discrete-time model with \(1000\) timesteps [41] and that on FFHQ is a continuous-time one [22].

and construct \(\widetilde{x}_{t}=\alpha_{t}\widetilde{x}_{0}+\sigma_{t}\epsilon\), where \(\widetilde{x}_{t}\sim p_{t}^{\text{ODE}}(x_{t};\theta)\). Then, the expectation over \(q_{t}(x_{t})\) could be approximated by the expectation over \(p_{t}^{\text{ODE}}(x_{t};\theta)\).

Empirically, on the CIFAR-10 dataset, we adopt a \(3\)-order DPM-Solver to generate a set of samples from the pretrained model of Ho et al. (2017), using a relatively large number of sampling steps (e.g., 50 steps). This set of generated data is used to calculate the calibration term \(\mathbb{E}_{q_{t}(x_{t})}\left[\epsilon_{\theta}^{t}(x_{t})\right]\). Then, we obtain the calibrated model \(\bm{\epsilon}_{\theta}^{t}(x_{t})-\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon }_{\theta}^{t}(x_{t})\right]\) and craft new images based on a \(3\)-order 20-step DPM-Solver. In the right part of Table 4, we present the results of an empirical investigation into how the number of generated images influences the quality of model calibration.

Using the same sampling setting, we also provide two reference points: 1) the originally mis-calibrated model can reach the FID score of 3.89, and 2) the model calibrated with training data can reach the FID score of 3.32. Comparing these results reveals that the DPM calibrated with a large number of high-quality generations can achieve comparable FID scores to those calibrated with training samples (see the result of using 20,000 generated images). Additionally, it appears that using more generations is not advantageous. This may be because the generations from DPMs, despite being known to cover diverse modes, still exhibit semantic redundancy and deviate slightly from the data distribution.

**Dynamical recording.** We simulate the proposed dynamical recording technique. Specifically, we use a \(3\)-layer MLP of width 512 to parameterize the aforementioned network \(h_{\phi}(t)\) and train it with an Adam optimizer Kingma and Ba (2015) to approximate the expected predicted noises \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\), where \(\bm{\epsilon}_{\theta}^{t}(x_{t})\) comes from the pretrained noise prediction model on CIFAR-10 Krizhevsky et al. (2017). The training of \(h_{\phi}(t)\) runs for 1,000 epochs. Meanwhile, using the training data, we compute the expected predicted noises with MC estimation and treat them as the ground truth. In Figure 4, we compare them to the outputs of \(h_{\phi}(t)\) and visualize the disparity measured by mean square error. As demonstrated, as the number of training epochs increases, the network \(h_{\phi}(t)\) quickly converges and can form a relatively reliable approximation to the ground truth. Dynamic recording has a distinct advantage of being able to be performed during the training of DPMs to enable immediate generation. We clarify that better timestep embedding techniques and NN architectures can improve approximation quality even further.

## 5 Discussion

We propose a straightforward method for calibrating any pretrained DPM that can provably reduce the values of SM objectives and, as a result, induce higher values of lower bounds for model likelihood. We demonstrate that the mis-calibration of DPMs may be inherent due to the dataset bias and/or sub-optimally learned model scores. Our findings also provide a potentially new metric for assessing a diffusion model by its degree of "uncalibration", namely, how far the learned scores deviate from the essential properties (e.g., the expected data scores should be zero).

**Limitations.** While our calibration method provably improves the model's likelihood, it does not necessarily yield a lower FID score, as previously discussed Kingma and Ba (2015). Besides, for text-to-image generation, post-training computation of \(\mathbb{E}_{q_{t}(x_{t}|y)}\left[\bm{s}_{\theta}^{t}(x_{t},y)\right]\) becomes infeasible due to the exponentially large number of conditions \(y\), necessitating dynamic recording with multimodal modules.

\begin{table}
\begin{tabular}{r r|r r} \hline \hline \multicolumn{2}{c|}{Training data} & \multicolumn{2}{c}{Generated data} \\ \# of samples & FID \(\downarrow\) & \# of samples & FID \(\downarrow\) \\ \hline
500 & 55.38 & 2,000 & 8.80 \\
1,000 & 18.72 & 5,000 & 4.53 \\
2,000 & 8.05 & 10,000 & 3.78 \\
5,000 & 4.31 & 20,000 & **3.31** \\
10,000 & 3.47 & 50,000 & 3.46 \\
20,000 & **3.25** & 100,000 & 3.47 \\
50,000 & 3.32 & 200,000 & 3.46 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Sample quality varies w.r.t. the number of training images (left part) and generated images (right part) used to estimate the calibration term on CIFAR-10. In the generated data case, the images used to estimate the calibration term \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\) is crafted with 50 sampling steps by a \(3\)-order DPM-Solver.

Figure 4: Dynamically recording \(\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right]\). During training, the mean square error between the ground truth and the outputs of a shallow network for recording the calibration terms rapidly decreases, across different timesteps \(t\).

## Acknowledgements

Zhijie Deng was supported by Natural Science Foundation of Shanghai (No. 23ZR1428700) and the Key Research and Development Program of Shandong Province, China (No. 2023CXGC010112).

## References

* [1] Kazuoki Azuma. Weighted sums of certain dependent random variables. _Tohoku Mathematical Journal, Second Series_, 19(3):357-367, 1967.
* [2] Arpit Bansal, Eitan Borgnia, Hong-Min Chu, Jie S Li, Hamid Kazemi, Furong Huang, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Cold diffusion: Inverting arbitrary image transforms without noise. _arXiv preprint arXiv:2208.09392_, 2022.
* [3] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In _International Conference on Learning Representations (ICLR)_, 2022.
* [4] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning gradient fields for shape generation. In _European Conference on Computer Vision (ECCV)_, 2020.
* [5] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In _International Conference on Learning Representations (ICLR)_, 2021.
* [6] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [7] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In _IEEE International Conference on Computer Vision (CVPR)_, 2020.
* [8] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexandros G Dimakis, and Peyman Milanfar. Soft diffusion: Score matching for general corruptions. _arXiv preprint arXiv:2209.05442_, 2022.
* [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2009.
* [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [11] Joseph L Doob and Joseph L Doob. _Stochastic processes_, volume 7. Wiley New York, 1953.
* [12] Bradley Efron. Tweedie's formula and selection bias. _Journal of the American Statistical Association_, 106(496):1602-1614, 2011.
* [13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.
* [14] Geoffrey Grimmett and David Stirzaker. _Probability and random processes_. Oxford university press, 2001.
* [15] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On memorization in diffusion models. _arXiv preprint arXiv:2310.02664_, 2023.
* [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 6626-6637, 2017.

* [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. _arXiv preprint arXiv:2210.02303_, 2022.
* [19] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.
* [20] Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research (JMLR)_, 6(Apr):695-709, 2005.
* [21] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _IEEE International Conference on Computer Vision (CVPR)_, 2019.
* [22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [23] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [25] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.
* [26] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In _International Conference on Learning Representations (ICLR)_, 2022.
* [27] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _International Conference on Computer Vision (ICCV)_, 2015.
* [28] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In _International Conference on Machine Learning (ICML)_, 2022.
* [29] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [30] Charlie Nash, Jacob Menick, Sander Dieleman, and Peter W Battaglia. Generating images with sparse representations. _arXiv preprint arXiv:2103.03841_, 2021.
* [31] Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Efficient learning of generative models via finite-difference score matching. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.
* [35] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. _IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)_, 2022.

* [36] Mehdi SM Sajadi, Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. Assessing generative models via precision and recall. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* [37] Tim Salimans and Jonathan Ho. Should ebms model the energy or the score? In _Energy Based Models Workshop-ICLR_, 2021.
* [38] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations (ICLR)_, 2022.
* [39] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2016.
* [40] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning (ICML)_, pages 2256-2265. PMLR, 2015.
* [41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations (ICLR)_, 2021.
* [42] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In _Advances in Neural Information Processing Systems (NeurIPS)_, pages 11895-11907, 2019.
* [43] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [44] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Conference on Uncertainty in Artificial Intelligence (UAI)_, 2019.
* [45] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [46] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations (ICLR)_, 2021.
* [47] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.
* [48] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [49] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [50] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In _International Conference on Machine Learning (ICML)_, 2023.
* [51] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In _International Conference on Learning Representations (ICLR)_, 2022.
* [52] Kexun Zhang, Xianjun Yang, William Yang Wang, and Lei Li. Redi: Efficient learning-free diffusion inference via trajectory retrieval. _arXiv preprint arXiv:2302.02285_, 2023.

Detailed derivations

In this section, we provide detailed derivations for the Theorem and equations shown in the main text. We follow the regularization assumptions listed in Song et al. [45].

### Proof of Theorem 1

Proof.: For any two timesteps \(0\leq s<t\leq T\), i.e., the transition probability from \(x_{s}\) to \(x_{t}\) is written as \(q_{st}(x_{t}|x_{s})=\mathcal{N}\left(x_{t}\big{|}\alpha_{t|s}x_{s},\sigma_{t|s }^{2}\mathbf{I}\right)\), where \(\alpha_{t|s}=\frac{\alpha_{t}}{\alpha_{s}}\) and \(\sigma_{t|s}^{2}=\sigma_{t}^{2}-\alpha_{t|s}^{2}\sigma_{s}^{2}\). The marginal distribution \(q_{t}(x_{t})=\int q_{st}(x_{t}|x_{s})q_{s}(x_{s})dx_{s}\) and we have

\[\nabla_{x_{t}}\log q_{t}(x_{t}) =\frac{1}{\alpha_{t|s}}\nabla_{\alpha_{t|s}^{-1}x_{t}}\log\left( \frac{1}{\alpha_{t|s}^{k}}\mathbb{E}_{\mathcal{N}\left(x_{s}\big{|}\alpha_{t|s }^{-1}x_{t},\alpha_{t|s}^{-2}\sigma_{t|s}^{2}\mathbf{I}\right)}\left[q_{s}(x_ {s})\right]\right)\] (15) \[=\frac{\mathbb{E}_{\mathcal{N}\left(\big{|}0,\alpha_{t|s}^{-2} \sigma_{t|s}^{2}\mathbf{I}\right)}\left[\nabla_{\alpha_{t|s}^{-1}x_{t}}q_{s}( \alpha_{t|s}^{-1}x_{t}+\eta)\right]}{\alpha_{t|s}\mathbb{E}_{\mathcal{N}\left( \big{|}0,\alpha_{t|s}^{-2}\sigma_{t|s}^{2}\mathbf{I}\right)}\left[q_{s}(\alpha _{t|s}^{-1}x_{t}+\eta)\right]}\] \[=\frac{\mathbb{E}_{\mathcal{N}\left(x_{s}\big{|}0,\alpha_{t|s}^{ -2}\sigma_{t|s}^{2}\mathbf{I}\right)}\left[q_{s}(\alpha_{t|s}^{-1}x_{t}+\eta) \nabla_{\alpha_{t|s}^{-1}x_{t}+\eta}\log q_{s}(\alpha_{t|s}^{-1}x_{t}+\eta) \right]}{\alpha_{t|s}\mathbb{E}_{\mathcal{N}\left(\big{|}0,\alpha_{t|s}^{-2} \sigma_{t|s}^{2}\mathbf{I}\right)}\left[q_{s}(\alpha_{t|s}^{-1}x_{t}+\eta) \right]}\] \[=\frac{\mathbb{E}_{\mathcal{N}\left(x_{s}\big{|}\alpha_{t|s}x_{t},\alpha_{t|s}^{-2}\sigma_{t|s}^{2}\mathbf{I}\right)}\left[q_{s}(x_{s})\nabla_{ x_{s}}\log q_{s}(x_{s})\right]}{\alpha_{t|s}\mathbb{E}_{\mathcal{N}\left(x_{s} \big{|}\alpha_{t|s}^{-1}x_{t},\alpha_{t|s}^{-2}\sigma_{t|s}^{2}\mathbf{I} \right)}\left[q_{s}(x_{s})\right]}\] \[=\frac{\int\mathcal{N}\left(x_{t}\big{|}\alpha_{t|s}x_{s},\sigma_ {t|s}^{2}\mathbf{I}\right)q_{s}(x_{s})\nabla_{x_{s}}\log q_{s}(x_{s})dx_{s}}{ \alpha_{t|s}\int\mathcal{N}\left(x_{t}\big{|}\alpha_{t|s}x_{s},\sigma_{t|s}^{ 2}\mathbf{I}\right)}q_{s}(x_{s})dx_{s}}\] \[=\frac{1}{\alpha_{t|s}}\mathbb{E}_{q_{st}(x_{s}|x_{t})}\left[ \nabla_{x_{s}}\log q_{s}(x_{s})\right].\]

Note that when the transition probability \(q_{st}(x_{t}|x_{s})\) corresponds to a well-defined forward process, there is \(\alpha_{t}>0\) for \(\forall t\in[0,T]\), and thus we achieve \(\alpha_{t}\nabla_{x_{t}}\log q_{t}(x_{t})=\mathbb{E}_{q_{st}(x_{s}|x_{t})} \left[\alpha_{s}\nabla_{x_{s}}\log q_{s}(x_{s})\right]\). 

### Proof of \(\mathbb{E}_{q_{0}(x_{0})}\left[\nabla_{x_{0}}\log q_{0}(x_{0})\right]=0\)

Proof.: The input variable \(x\in\mathbb{R}^{k}\) and \(q_{0}(x_{0})\in\mathcal{C}^{2}\), where \(\mathcal{C}^{2}\) denotes the family of functions with continuous second-order derivatives.1 We use \(x^{i}\) denote the \(i\)-th element of \(x\), then we can derive the expectation

Footnote 1: This continuously differentiable assumption can be satisfied by adding a small Gaussian noise (e.g., with variance of \(0.0001\)) on the original data distribution, as done in Song and Ermon [42].

\[\mathbb{E}_{q_{0}(x_{0})}\left[\frac{\partial}{\partial x_{0}^{i}} \log q_{0}(x_{0})\right] =\int\cdots\int q_{0}(x_{0})\frac{\partial}{\partial x_{0}^{i}} \log q_{0}(x_{0})dx_{0}^{1}dx_{0}^{2}\cdots dx_{0}^{k}\] (16) \[=\int\cdots\int\frac{\partial}{\partial x_{0}^{i}}\left(\int q_{0 }(x_{0}^{i},x_{0}^{\setminus i})dx_{0}^{\setminus i}\right)dx_{0}^{i}\] \[=\int\frac{d}{dx_{0}^{i}}q_{0}(x_{0}^{i})dx_{0}^{i}=0,\]where \(x_{0}^{\setminus i}\) denotes all the \(k-1\) elements in \(x_{0}\) except for the \(i\)-th one. The last equation holds under the boundary condition that \(\lim_{x_{0}^{i}\to\infty}q_{0}(x_{0}^{i})=0\) hold for any \(i\in[K]\). Thus, we achieve the conclusion that \(\mathbb{E}_{q_{0}(x_{0})}\left[\nabla_{x_{0}}\log q_{0}(x_{0})\right]=0\). 

### Concentration bounds

We describe concentration bounds [11; 1] of the martingale \(\alpha_{t}\nabla_{x_{t}}\log q_{t}(x_{t})\).

**Azuma's inequality.** For discrete reverse timestep \(t=T,T-1,\cdots,0\), Assuming that there exist constants \(0<c_{1},c_{2},\cdots,<\infty\) such that for the \(i\)-th element of \(x\),

\[A_{t}\leq\frac{\partial}{\partial x_{t-1}^{i}}\alpha_{t-1}\log q_{t-1}(x_{t-1} )-\frac{\partial}{\partial x_{t}^{i}}\alpha_{t}\log q_{t}(x_{t})\leq B_{t} \text{ and }B_{t}-A_{t}\leq c_{t}\] (17)

almost surely. Then \(\forall\epsilon>0\), the probability (note that \(\alpha_{0}=1\))

\[P\left(\left|\frac{\partial}{\partial x_{0}^{i}}\log q_{0}(x_{0})-\frac{ \partial}{\partial x_{T}^{i}}\alpha_{T}\log q_{T}(x_{T})\right|\geq\epsilon \right)\leq 2\exp\left(-\frac{2\epsilon^{2}}{\sum_{t=1}^{T}c_{t}^{2}} \right).\] (18)

Specially, considering that \(q_{T}(x_{T})\approx\mathcal{N}(x_{T}|0,\widetilde{\sigma}^{2}\mathbf{I})\), there is \(\frac{\partial}{\partial x_{T}^{i}}\log q_{T}(x_{T})\approx-\frac{x_{T}^{i}}{ \widetilde{\sigma}^{2}}\). Thus, we can approximately obtain

\[P\left(\left|\frac{\partial}{\partial x_{0}^{i}}\log q_{0}(x_{0})+\frac{ \alpha_{T}x_{T}^{i}}{\widetilde{\sigma}^{2}}\right|\geq\epsilon\right)\leq 2\exp \left(-\frac{2\epsilon^{2}}{\sum_{t=1}^{T}c_{t}^{2}}\right).\] (19)

**Doob's inequality.** For continuous reverse timestep \(t\) from \(T\) to \(0\), if the sample paths of the martingale are almost surely right-continuous, then for the \(i\)-th element of \(x\) we have (note that \(\alpha_{0}=1\))

\[P\left(\sup_{0\leq t\leq T}\frac{\partial}{\partial x_{t}^{i}}\alpha_{t}\log q _{t}(x_{t})\geq C\right)\leq\frac{\mathbb{E}_{q_{0}(x_{0})}\left[\max\left( \frac{\partial}{\partial x_{0}^{i}}\log q_{0}(x_{0}),0\right)\right]}{C}.\] (20)

### High-order SM objectives

Lu et al. [28] show that the KL divergence \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta)\right)\) can be bounded as

\[\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta)\right)\leq \mathcal{D}_{\text{KL}}\left(q_{T}\|p_{T}\right)+\sqrt{\mathcal{J}_{\text{SM} }(\theta;g(t)^{2})}\cdot\sqrt{\mathcal{J}_{\text{Fisher}}(\theta)},\] (21)

where \(\mathcal{J}_{\text{Fisher}}(\theta)\) is a weighted sum of Fisher divergence between \(q_{t}(x_{t})\) and \(p_{t}^{\text{ODE}}(\theta)\) as

\[\mathcal{J}_{\text{Fisher}}(\theta)=\frac{1}{2}\int_{0}^{T}g(t)^{2}D_{F}\left(q _{t}\|p_{t}^{\text{ODE}}(\theta)\right)dt.\] (22)

Moreover, Lu et al. [28] prove that if \(\forall t\in[0,T]\) and \(\forall x_{t}\in\mathbb{R}^{k}\), there exist a constant \(C_{F}\) such that the spectral norm of Hessian matrix \(\|\nabla_{x_{t}}^{2}\log p_{t}^{\text{ODE}}(x_{t};\theta)\|_{2}\leq C_{F}\), and there exist \(\delta_{1}\), \(\delta_{2}\), \(\delta_{3}>0\) such that

\[\begin{split}&\|\bm{s}_{\theta}^{t}(x_{t})-\nabla_{x_{t}}\log q_{t}(x_ {t})\|_{2}\leq\delta_{1},\\ &\|\nabla_{x_{t}}\bm{s}_{\theta}^{t}(x_{t})-\nabla_{x_{t}}^{2}\log q _{t}(x_{t})\|_{F}\leq\delta_{2},\\ &\|\nabla_{x_{t}}\textbf{tr}\left(\nabla_{x_{t}}\bm{s}_{\theta}^{t }(x_{t})\right)-\nabla_{x_{t}}\textbf{tr}\left(\nabla_{x_{t}}^{2}\log q_{t}(x _{t})\right)\|_{2}\leq\delta_{3},\end{split}\] (23)

where \(\|\cdot\|_{F}\) is the Frobenius norm of matrix. Then there exist a function \(U(t;\delta_{1},\delta_{2},\delta_{3},q)\) that independent of \(\theta\) and strictly increasing (if \(g(t)\neq 0\)) w.r.t. \(\delta_{1}\), \(\delta_{2}\), and \(\delta_{3}\), respectively, such that the Fisher divergence can be bounded as \(D_{F}\left(q_{t}\|p_{t}^{\text{ODE}}(\theta)\right)\leq U(t;\delta_{1},\delta_{2 },\delta_{3},q)\).

**The case after calibration.** When we impose the calibration term \(\eta_{t}^{*}=\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x_{t})\right]\) to get the score model \(\bm{s}_{\theta}^{t}(x_{t})-\eta_{t}^{*}\), there is \(\nabla_{x_{t}}\eta_{t}^{*}=0\) and thus \(\nabla_{x_{t}}\left(\bm{s}_{\theta}^{t}(x_{t})-\eta_{t}^{*}\right)=\nabla_{x_{t }}\bm{s}_{\theta}^{t}(x_{t})\). Then we have

\[\begin{split}&\|\bm{s}_{\theta}^{t}(x_{t})-\eta_{t}^{*}-\nabla_{x_{t }}\log q_{t}(x_{t})\|_{2}\leq\delta_{1}^{t}\leq\delta_{1},\\ &\|\nabla_{x_{t}}\left(\bm{s}_{\theta}^{t}(x_{t})-\eta_{t}^{*} \right)-\nabla_{x_{t}}^{2}\log q_{t}(x_{t})\|_{F}\leq\delta_{2},\\ &\|\nabla_{x_{t}}\textbf{tr}\left(\nabla_{x_{t}}\left(\bm{s}_{ \theta}^{t}(x_{t})-\eta_{t}^{*}\right)\right)-\nabla_{x_{t}}\textbf{tr}\left( \nabla_{x_{t}}^{2}\log q_{t}(x_{t})\right)\|_{2}\leq\delta_{3}.\end{split}\] (24)From these, we know that the Fisher divergence \(D_{F}\left(q_{t}\|p_{t}^{\text{ODE}}(\theta,\eta_{t}^{*})\right)\leq U(t;\delta_{1} ^{\prime},\delta_{2},\delta_{3},q)\leq U(t;\delta_{1},\delta_{2},\delta_{3},q)\), namely, \(D_{F}\left(q_{t}\|p_{t}^{\text{ODE}}(\theta,\eta_{t}^{*})\right)\) has a lower upper bound compared to \(D_{F}\left(q_{t}\|p_{t}^{\text{ODE}}(\theta)\right)\). Consequently, we can get lower upper bounds for both \(\mathcal{J}_{\text{Fisher}}(\theta,\eta_{t}^{*})\) and \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta,\eta_{t}^{*})\right)\), compared to \(\mathcal{J}_{\text{Fisher}}(\theta)\) and \(\mathcal{D}_{\text{KL}}\left(q_{0}\|p_{0}^{\text{ODE}}(\theta)\right)\), respectively.

## Appendix B Model parametrization

This section introduces different parametrizations used in diffusion models and provides their calibrated instantiations.

### Preliminary

Along the research routine of diffusion models, different model parametrizations have been used, including score prediction \(\bm{s}_{\theta}^{t}(x_{t})\)[42; 46], noise prediction \(\bm{\epsilon}_{\theta}^{t}(x_{t})\)[33; 17], data prediction \(\bm{x}_{\theta}^{t}(x_{t})\)[32; 32], and velocity prediction \(\bm{v}_{\theta}^{t}(x_{t})\)[18; 38]. Taking the DSM objective as the training loss, its instantiation at timestep \(t\in[0,T]\) is written as

\[\mathcal{J}_{\text{DSM}}^{t}(\theta)=\begin{cases}\frac{1}{2}\mathbb{E}_{q_{ 0}(x_{0}),q(\epsilon)}\left[\|\bm{s}_{\theta}^{t}(x_{t})+\frac{\epsilon}{ \sigma_{t}}\|_{2}^{2}\right],&\text{score prediction;}\\ \frac{\alpha_{t}^{2}}{2\sigma_{t}^{2}}\mathbb{E}_{q_{0}(x_{0}),q(\epsilon)} \left[\|\bm{x}_{\theta}^{t}(x_{t})-x_{0}\|_{2}^{2}\right],&\text{data prediction;}\\ \frac{1}{2\sigma_{t}^{2}}\mathbb{E}_{q_{0}(x_{0}),q(\epsilon)}\left[\|\bm{ \epsilon}_{\theta}^{t}(x_{t})-\epsilon\|_{2}^{2}\right],&\text{noise prediction;}\\ \frac{\alpha_{t}^{2}}{2\sigma_{t}^{2}}\mathbb{E}_{q_{0}(x_{0}),q(\epsilon)} \left[\|\bm{v}_{\theta}^{t}(x_{t})-(\alpha_{t}\epsilon-\sigma_{t}x_{0})\|_{2} ^{2}\right],&\text{velocity prediction.}\end{cases}\] (25)

### Calibrated instantiation

Under different model parametrizations, we can derive the optimal calibration terms \(\eta_{t}^{*}\) that minimizing \(\mathcal{J}_{\text{DSM}}^{t}(\theta,\eta_{t})\) as

\[\eta_{t}^{*}=\begin{cases}\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{\theta}^{t}(x _{t})\right],&\text{score prediction;}\\ \mathbb{E}_{q_{t}(x_{t})}\left[\bm{x}_{\theta}^{t}(x_{t})\right]-\mathbb{E}_{ q_{0}(x_{0})}\left[x_{0}\right],&\text{data prediction;}\\ \mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{\theta}^{t}(x_{t})\right],& \text{noise prediction;}\\ \mathbb{E}_{q_{t}(x_{t})}\left[\bm{v}_{\theta}^{t}(x_{t})\right]+\sigma_{t} \mathbb{E}_{q_{0}(x_{0})}\left[x_{0}\right],&\text{velocity prediction.}\end{cases}\] (26)

Taking \(\eta_{t}^{*}\) into \(\mathcal{J}_{\text{DSM}}^{t}(\theta,\eta_{t})\) we can obtain the gap

\[\mathcal{J}_{\text{DSM}}^{t}(\theta)-\mathcal{J}_{\text{DSM}}^{t}(\theta, \eta_{t}^{*})=\begin{cases}\frac{1}{2}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{s}_{ \theta}^{t}(x_{t})\right]\|_{2}^{2},&\text{score prediction;}\\ \frac{\alpha_{t}^{2}}{2\sigma_{t}^{2}}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{x}_ {\theta}^{t}(x_{t})\right]-\mathbb{E}_{q_{0}(x_{0})}\left[x_{0}\right]\|_{2}^ {2},&\text{data prediction;}\\ \frac{1}{2\sigma_{t}^{2}}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{\epsilon}_{ \theta}^{t}(x_{t})\right]\|_{2}^{2},&\text{noise prediction;}\\ \frac{\alpha_{t}^{2}}{2\sigma_{t}^{2}}\|\mathbb{E}_{q_{t}(x_{t})}\left[\bm{v}_ {\theta}^{t}(x_{t})\right]+\sigma_{t}\mathbb{E}_{q_{0}(x_{0})}\left[x_{0} \right]\|_{2}^{2},&\text{velocity prediction.}\end{cases}\] (27)

## Appendix C Visualization of the generations

We further show generated images in Figure 5 to double confirm the efficacy of our calibration method. Our calibration could help to reduce ambiguous generations on both CIFAR-10 and CelebA.

Figure 5: Unconditional generation results on CIFAR-10 and CelebA using models from [17] and [41] respectively. The number of sampling steps is 20 based on the results in Tables 1 and 2.