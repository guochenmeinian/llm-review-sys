# Reward-agnostic Fine-tuning: Provable Statistical Benefits of Hybrid Reinforcement Learning

Gen Li

CUHK

Wenhao Zhan

Princeton

Jason D. Lee

Princeton

Yuejie Chi

CMU

Yuxin Chen

UPenn

The first two authors contributed equally.Corresponding author

###### Abstract

This paper studies tabular reinforcement learning (RL) in the hybrid setting, which assumes access to both an offline dataset and online interactions with the unknown environment. A central question boils down to how to efficiently utilize online data to strengthen and complement the offline dataset and enable effective policy fine-tuning. Leveraging recent advances in reward-agnostic exploration and offline RL, we design a three-stage hybrid RL algorithm that beats the best of both worlds -- pure offline RL and pure online RL -- in terms of sample complexities. The proposed algorithm does not require any reward information during data collection. Our theory is developed based on a new notion called _single-policy partial concentrability_, which captures the trade-off between distribution mismatch and miscoverage and guides the interplay between offline and online data.

## 1 Introduction

As reinforcement learning (RL) shows promise in achieving super-human empirical success across diverse fields (e.g., games (Silver et al., 2016; Vinyals et al., 2019; Berner et al., 2019; Mnih et al., 2013), robotics (Brambilla et al., 2013), autonomous driving (Shalev-Shwartz et al., 2016)), theoretical understanding about RL has also been substantially expanded, with the aim of distilling fundamental principles that can inform and guide practice. Among all sorts of theoretical questions being pursued, how to make the best use of data emerges as a question of profound interest for problems with enormous dimensionality.

There are at least two mainstream mechanisms when it comes to data collection: online RL and offline RL. Let us briefly describe their attributes and differences as follows.

_Online RL._ In this setting, an agent learns how to maximize her cumulative reward through interaction with the unknown environment (by, say, executing a sequence of adaptively chosen actions and utilizing the instantaneous feedback of the environment). Given that all information about the environment is obtained through real-time data collection, the main challenge lies in how to (optimally) manage the trade-off between exploration and exploitation. Towards this, one popular approach advertises the principle of optimism in the face of uncertainty -- e.g., employing upper confidence bounds during value estimation to guide exploration -- whose effectiveness has been shown for both the tabular case (Auer and Ortner, 2006; Jaksch et al., 2010; Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Bai et al., 2019; Dong et al., 2019; Zhang et al., 2020; Menard et al., 2021b; Li et al., 2021b) and the case with function approximation (Jin et al., 2020; Zanette et al., 2020; Zhou et al., 2021a; Li et al., 2021a; Du et al., 2021; Jin et al., 2021a; Foster et al., 2021; Chen et al., 2022b).

_Offline RL._ In contrast, offline RL assumes access to a pre-collected dataset, without given permission to perform any further data collection. The feasibility of reliable offline RL depends heavilyon the quality of the dataset at hand. A central challenge stems from the presence of distribution shift: the distribution of the offline dataset might differ significantly from that induced by the target policy. Another common challenge arises from insufficient data coverage: a nontrivial fraction of the state-action pairs might be inadequately visited in the available dataset, thus precluding one from faithfully evaluating many policies based solely on the offline dataset. To circumvent these obstacles, recent works proposed the principle of pessimism in the face of uncertainty, recommending caution when selecting poorly visited actions (Liu et al., 2020; Kumar et al., 2020; Jin et al., 2021b; Rashidinejad et al., 2021; Uehara and Sun, 2021; Li et al., 2022; Yin et al., 2021; Shi et al., 2022; Cai et al., 2022). Without requiring uniform coverage of all policies, the pessimism approach proves effective as long as the so-called _single-policy concentrability_ is satisfied, which only assumes adequate coverage over the part of the state-action space reachable by the desirable policy.

In reality, however, both mechanisms above come with limitations. For instance, even the single-policy concentrability requirement might be too stringent (and hence fragile) for offline RL, as it is not uncommon for the historical dataset to miss a small yet essential part of the state-action space. Pure online RL might also be overly restrictive, given that there might be information from past data that could help initialize online exploration and mitigate the burden of further data collection.

All this motivates the studies of hybrid RL, a scenario where the agent has access to an offline dataset while, in the meantime, (limited) online data collection is permitted as well. Oftentimes, this scenario is practically not only feasible but also appealing: on the one hand, offline data provides useful information for policy pre-training, while further online exploration helps enrich existing data and allows for effective policy fine-tuning. As a matter of fact, multiple empirical works (Rajeswaran et al., 2017; Vecerik et al., 2017; Kalashnikov et al., 2018; Hester et al., 2018; Nair et al., 2018, 2020) indicated that combining online RL with offline datasets outperforms both pure online RL and pure offline RL. Nevertheless, theoretical pursuits about hybrid RL are lagging behind. Two recent works Ross and Bagnell (2012); Xie et al. (2021) studied a restricted setting, where the agent is aware of a Markovian behavior policy (a policy that generates offline data) and can either execute the behavior policy or any other adaptive choice to draw samples in each episode; in this case, Xie et al. (2021) proved that under the single-policy concentrability assumption of the offline dataset, having perfect knowledge about the behavior policy does not improve online exploration in the minimax sense. Another strand of works Song et al. (2022); Nakamoto et al. (2023); Wagemmaker and Yecchiano (2022) looked at a more general offline dataset and investigated how to leverage offline data in online exploration. From the sample complexity viewpoint, Wagenmaker and Pacchiano (2022) studied the statistical benefits of hybrid RL in the presence of linear function approximation; the result therein, however, required strong assumptions on data coverage (i.e., all-policy concentrability) and fell short of unveiling provable gains in the tabular case (as we shall elucidate momentarily). In light of such theoretical inadequacy in previous works, this paper is motivated to pursue the following question:

_Does hybrid RL allow for improved sample complexity compared to pure online or offline RL in the tabular case?_

### Main contributions

We deliver an affirmative answer to the above question. Further relaxing the single-policy concentrability assumption, we introduce a relaxed notation called single-policy _partial_ concentrability (to be made precise in Definition 2), which (i) allows the dataset to miss a fraction of the state-action space visited by the optimal policy and (ii) captures the tradeoff between distribution mismatch and lack of coverage. Armed with this notion, our results reveal provable statistical benefits of hybrid RL compared with both pure online and offline RL. The main contributions are summarized below.

_A novel three-stage algorithm._ We design a new hybrid RL algorithm consisting of three stages. In the first stage, we obtain crude estimation of the occupancy distribution \(d^{\pi}\) w.r.t. any policy \(\pi\) as well as the data distribution \(d^{\text{off}}\) of the offline dataset. The second stage performs online exploration; in particular, we execute one exploration policy to imitate the offline dataset and another one to explore the inadequately visited part of the unknown environment, with both policies computed by approximately solving convex optimization sub-problems. Notably, these two stages do not count on the availability of reward information, and thus operate in a reward-agnostic manner. The final stage then invokes the state-of-the-art offline RL algorithm for policy learning, on the basis of all data we have available (including both online and offline data).

_Computationally efficient subroutines._ Throughout the first two stages of the algorithm, we need to solve a couple of convex sub-problems with exponentially large dimensions. In order to attain computational efficiency, we design efficient Frank-Wolfe-type paradigms to solve the sub-problems approximately, which run in polynomial time. This plays a crucial role in ensuring computational tractability of the proposed three-stage algorithm.

_Improved sample complexity._ We characterize the sample complexity of our algorithm (see Theorem 1), which provably improves upon both pure online and offline RL. On the one hand, hybrid RL achieves strictly enhanced performance compared to pure offline RL (assuming the same sample size) when the offline data falls short of covering all state-action pairs reachable by the desired policy. On the other hand, the sample size allocated to online exploration in our algorithm might only need to be proportional to the fraction \(\sigma\) of the state-action space uncovered by the offline dataset, thus resulting in sample size saving in general compared to pure online RL (a case with \(\sigma=1\)).

Notation.Let us also introduce several useful notation. For integer \(m>0\), we let \([m]\) represent the set \(\{1,\cdots,m\}\). For any set \(\mathcal{B}\), we denote by \(\mathcal{B}^{\text{c}}\) its complement. For any policy \(\pi_{0}\), we let \(\mathds{1}_{\pi_{0}}:\Pi\to\{0,1\}\) be an indicator function such that \(\mathds{1}_{\pi_{0}}(\pi)=1\) if \(\pi=\pi_{0}\) and \(\mathds{1}_{\pi_{0}}(\pi)=0\) otherwise. For any finite set \(\mathcal{A}\), we denote by \(\Delta(\mathcal{A})\) the probability simplex over \(\mathcal{A}\). Letting \(\mathcal{X}\coloneqq(S,A,H,\frac{1}{\varepsilon},\frac{1}{\delta})\), we use the notation \(f(\mathcal{X})=O(g(\mathcal{X}))\) or \(f(\mathcal{X})\leq g(\mathcal{X})\) to indicate the existence of a universal constant \(C_{1}>0\) such that \(f\leq C_{1}g\), the notation \(f(\mathcal{X})\gtrsim g(\mathcal{X})\) to indicate that \(g(\mathcal{X})=O(f(\mathcal{X}))\), and the notation \(f(\mathcal{X})\asymp g(\mathcal{X})\) to mean that \(f(\mathcal{X})\lesssim g(\mathcal{X})\) and \(f(\mathcal{X})\gtrsim g(\mathcal{X})\) hold simultaneously. The notation \(\widetilde{O}(\cdot)\) is defined in the same way as \(O(\cdot)\) except that it hides logarithmic factors.

## 2 Preliminaries and problem settings

Episodic finite-horizon MDPs.We study episodic finite-horizon Markov decision processes with \(S\) states, \(A\) actions, and horizon length \(H\). We use \(\mathcal{M}=(\mathcal{S},\mathcal{A},H,P=\{P_{h}\}_{h=1}^{H},r=\{r_{h}\}_{h=1} ^{H})\) to represent such an MDP, where \(\mathcal{S}=[S]\) and \(\mathcal{A}=[A]\) represent the state space and the action space, respectively. For each step \(h\in[H]\), we let \(P_{h}:\mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})\) represent the transition probability at this step, such that taking action \(a\) in state \(s\) at step \(h\) yields a transition to the next state drawn from the distribution \(P_{h}(\cdot\,|\,s,a)\); throughout the paper, we often employ the shorthand notation \(P_{h,s,a}:=P_{h}(\cdot|s,a)\). Another ingredient is the reward function specified by \(r_{h}:\mathcal{S}\times\mathcal{A}\to[0,1]\) at step \(h\); namely, the agent will receive an immediate reward \(r_{h}(s,a)\) upon executing action \(a\) in state \(s\) at step \(h\). It is assumed that the reward function is fully revealed upon completion of online data collection. Additionally, we assume throughout that each episode of the MDP starts from an initial state independently generated from some (unknown) initial state distribution \(\rho\in\Delta(\mathcal{S})\).

A time-inhomogeneous Markovian policy is often denoted by \(\pi=\{\pi_{h}\}_{h=1}^{H}\) with \(\pi_{h}:\mathcal{S}\to\Delta(\mathcal{A})\), where \(\pi_{h}(\cdot\,|\,s)\) characterizes the (randomized) action selection probability of the agent in state \(s\) at step \(h\). If \(\pi\) is a deterministic policy, then we often abuse the noation and let \(\pi_{h}(s)\) represent the action selected in state \(s\) at step \(h\). We find it convenient to introduce the following notation:

\[\Pi\ \coloneqq\ \text{the set of all deterministic policies}.\] (1)

We also need to handle mixed deterministic policies (i.e., each realization of the policy is randomly drawn from a mixture of deterministic policies). A mixed deterministic policy \(\pi^{\text{mixed}}\) is denoted by

\[\pi^{\text{mixed}}=\sum\nolimits_{\pi\in\Pi}\mu(\pi)\pi=\mathbb{E}_{\pi\sim \mu}[\pi]\qquad\text{for some }\mu\in\Delta(\Pi).\] (2)

Moreover, for any policy \(\pi\), we define its associated value function (resp. Q-function) as follows, representing the expected cumulative rewards conditioned on an initial state (resp. an initial state-action pair):

\[V_{h}^{\pi}(s) \coloneqq\mathbb{E}_{\pi}\bigg{[}\sum\nolimits_{h^{\prime}:h\leq h ^{\prime}\leq H}r_{h^{\prime}}(s,a)\,\bigg{|}\,s_{h}=s\bigg{]}, \forall s\in\mathcal{S};\] \[Q_{h}^{\pi}(s,a) \coloneqq\mathbb{E}_{\pi}\bigg{[}\sum\nolimits_{h^{\prime}:h\leq h ^{\prime}\leq H}r_{h^{\prime}}(s,a)\,\bigg{|}\,s_{h}=s,a_{h}=a\bigg{]}, \forall(s,a)\in\mathcal{S}\times\mathcal{A}.\]

Here, the expectation is over the length-\(H\) sample trajectory \((s_{1},a_{1},s_{2},a_{2},\ldots,s_{H},a_{H})\) when executing policy \(\pi\) in \(\mathcal{M}\), where \(s_{h}\) (resp. \(a_{h}\)) denotes the state (resp. action) at step \(h\) of this trajectory.

When the initial state is drawn from \(\rho\), we further augment the notation and denote

\[V_{1}^{\pi}(\rho)=\mathbb{E}_{s\sim\rho}\big{[}V_{1}^{\pi}(s)\big{]}.\]

Importantly, there exists at least one deterministic policy, denoted by \(\pi^{\star}\) throughout, that is able to maximize \(V_{h}^{\pi}(s)\) and \(Q_{h}^{\pi}(s,a)\) simultaneously for all \((h,s,a)\in[H]\times\mathcal{S}\times\mathcal{A}\); namely,

\[V_{h}^{\star}(s)\coloneqq V_{h}^{\pi^{\star}}(s)=\max_{\pi}V_{h}^{\pi}(s), \qquad Q_{h}^{\star}(s,a)\coloneqq Q_{h}^{\pi^{\star}}(s,a)=\max_{\pi}Q_{h}^{ \pi}(s,a),\quad\forall(s,a)\in\mathcal{S}\times\mathcal{A}.\]

Moving beyond value functions and Q-functions, we would like to define, for each policy \(\pi\), the associated state-action occupancy distribution \(d^{\pi}=[d_{h}^{\pi}]_{1\leq h\leq H}\) such that

\[d_{h}^{\pi}(s,a)\coloneqq\mathbb{P}(s_{h}=s,a_{h}=a\,|\,\pi),\qquad\forall(s, a,h)\in\mathcal{S}\times\mathcal{A}\times[H];\]

in other words, this is the probability of the state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\) being visited by \(\pi\) at step \(h\). We shall also overload \(d^{\pi}\) to represent the state occupancy distribution such that

\[d_{h}^{\pi}(s)\coloneqq\sum\nolimits_{a\in\mathcal{A}}d_{h}^{\pi}(s,a)= \mathbb{P}(s_{h}=s\,|\,\pi),\qquad\forall(s,h)\in\mathcal{S}\times[H].\] (3)

Given that each episode always starts with a state drawn from \(\rho\), it is easily seen that \(d_{1}^{\pi}(s)=\rho(s)\) for any policy \(\pi\) and any \(s\in\mathcal{S}\).

Sampling mechanism.We consider a hybrid RL setting that assumes access to a historical dataset as well as the ability to further explore the environment via real-time sampling, as detailed below.

Offline data.Suppose that we have available a historical dataset (also called an offline dataset)

\[\mathcal{D}^{\text{off}}=\big{\{}\tau^{k,\text{off}}\big{\}}_{1\leq k\leq K^{ \text{off}}},\] (4)

containing \(K^{\text{off}}\) sample trajectories each of length \(H\). Here, the \(k\)-th trajectory in \(\mathcal{D}^{\text{off}}\) is denoted by

\[\tau^{k,\text{off}}=\big{(}s_{1}^{k,\text{off}},a_{1}^{k,\text{off}},\ldots,s _{H}^{k,\text{off}},a_{H}^{k,\text{off}}\big{)},\] (5)

where \(s_{h}^{k,\text{off}}\) and \(a_{h}^{k,\text{off}}\) indicate respectively the state and action at step \(h\) of this trajectory \(\tau^{k,\text{off}}\). It is assumed that each trajectory \(\tau^{k,\text{off}}\) is drawn _independently_ using policy \(\pi^{\text{off}}\), which takes the form of a mixture of deterministic policies

\[\pi^{\text{off}}=\mathbb{E}_{\pi\sim\mu^{\text{off}}}\big{[}\pi\big{]}\qquad \text{with }\mu^{\text{off}}\in\Delta(\Pi).\] (6)

Note that the learner only has access to the data samples but not \(\pi^{\text{off}}\). Throughout the paper, we use \(d^{\text{off}}=\{d_{h}^{\text{off}}\}_{1\leq h\leq H}\) to represent the occupancy distribution of this offline dataset such that

\[d_{h}^{\text{off}}(s,a)\coloneqq\mathbb{P}\big{(}(s_{h}^{k,\text{off}},a_{h}^{ k,\text{off}})=(s,a)\big{)},\qquad\forall(s,a,h)\in\mathcal{S}\times\mathcal{A} \times[H].\] (7)

Online exploration.In addition to the offline dataset, the learner is allowed to interact with the unknown environment and collect more data in real time, in the hope of compensating for the insufficiency of the pre-collected data at hand and fine-tuning the policy estimate. More specifically, the learner is able to sample \(K^{\text{on}}\) trajectories sequentially. In each sample trajectory,

* the initial state is generated independently from an (unknown) distribution \(\rho\in\Delta(\mathcal{S})\);
* the learner selects a policy to execute the MDP, obtaining a sample trajectory of length \(H\).

The total number of sample trajectories is thus given by

\[K=K^{\text{off}}+K^{\text{on}}.\] (8)

Concentrability assumptions for the offline dataset.To quantify the quality of the historical dataset, prior offline RL literature introduced the following single-policy concentrability coefficient based on certain density ratio of interest; see, e.g., Rashidinejad et al. (2021); Li et al. (2022).

**Definition 1** (Single-policy concentrability).: _The single-policy concentrability coefficient \(C^{\star}\) of the offline dataset \(\mathcal{D}^{\text{off}}\) is defined as_

\[C^{\star}\coloneqq\max_{(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]}\frac {d_{h}^{\pi^{\star}}(s,a)}{d_{h}^{\text{off}}(s,a)}.\] (9)In words, \(C^{\star}\) employs the \(\ell_{\infty}\)-norm of the density ratio \(d^{\pi^{\star}}/d^{\mathsf{off}}\) to capture the shift of distributions between the occupancy distribution induced by the desired policy \(\pi^{\star}\) and the data distribution at hand. The terminology "single-policy" underscores that Definition 1 only compares the offline data distribution against the one generated by a single policy \(\pi^{\star}\), which stands in stark contrast to other all-policy concentrability coefficients that are defined to account for all policies simultaneously.

One notable fact about Definition 1 is that: for \(C^{\star}\) to be finite, the historical data distribution needs to cover all state-action-step tuples reachable by \(\pi^{\star}\). This requirement is, in general, inevitable if only the offline dataset is available; see the minimax lower bounds in Rashidinejad et al. (2021); Li et al. (2022) for more precise justifications. However, a requirement of this kind could be overly stringent for the hybrid setting considered herein, as the issue of incomplete coverage can potentially be overcome with the aid of online data collection. In light of this observation, we generalize Definition 1 to account for the trade-offs between distributional mismatch and partial coverage.

**Definition 2** (Single-policy partial concentrability).: _For any \(\sigma\in[0,1]\), the single-policy partial concentrability coefficient \(C^{\star}(\sigma)\) of the offline dataset \(\mathcal{D}^{\mathsf{off}}\) is defined as_

\[C^{\star}(\sigma)\coloneqq\min\bigg{\{}\max_{1\leq h\leq H}\max_{(s,a)\in \mathcal{G}_{h}}\frac{d_{h}^{\pi^{\star}}(s,a)}{d_{h}^{\mathsf{off}}(s,a)}\ \bigg{|}\ \{\mathcal{G}_{h}\}_{1\leq h\leq H}\in\mathcal{G}(\sigma) \bigg{\}},\] (10)

_where_

\[\mathcal{G}(\sigma)\coloneqq\bigg{\{}\{\mathcal{G}_{h}\}_{1\leq h \leq H}\subseteq\mathcal{S}\times\mathcal{A}\ \bigg{|}\ \frac{1}{H}\sum_{h=1}^{H}\sum_{(s,a)\notin\mathcal{G}_{h}}d_{h}^{\pi^{\star}}(s, a)\leq\sigma\bigg{\}}.\] (11)

In Definition 2, we allow a fraction of the state-action space reachable by \(\pi^{\star}\) to be insufficiently covered (as reflected in the definition of \(\mathcal{G}(\sigma)\) measured by the state-action occupancy distribution) -- hence the terminology "partial". Intuitively, \(\mathcal{G}_{h}\) corresponds to a set of state-action pairs that undergo reasonable distribution shift (so that the corresponding density ratio does not rise above \(C^{\star}(\sigma)\)), whereas the total occupancy density of its complement subset \(\mathcal{G}_{h}^{c}\) induced by \(\pi^{\star}\) is under control (i.e., no larger than \(\sigma\) when averaged across steps). As a self-evident fact, \(C^{\star}(\sigma)\) is non-increasing in \(\sigma\); this means that as \(\sigma\) increases, we might incur a less severe distribution shift in a restricted part, at the price of less coverage. In this sense, \(C^{\star}(\sigma)\) reflects certain tradeoffs between distribution shift and coverage. Clearly, \(\tilde{C}^{\star}(\sigma)\) reduces to \(C^{\star}\) in Definition 1 by taking \(\sigma=0\).

Goal.Given a historical dataset \(\mathcal{D}^{\mathsf{off}}\) containing \(K^{\mathsf{off}}\) sample trajectories, we would like to design an online exploration scheme, in conjunction with the accompanying policy learning algorithm, so as to achieve desirable policy learning (or policy fine-tuning) in a data-efficient manner. Ideally, we would expect a hybrid RL algorithm to harvest provable statistical benefits compared to both purely online RL and purely offline RL approaches.

## 3 Algorithm

In this section, we propose a new algorithm to tackle the hybrid RL setting. Our algorithm design leverages recent ideas developed in offline RL and reward-agnostic exploration to improve sample efficiency. Our algorithm consists of three stages to be described shortly; informally, the first two stages conduct reward-agnostic exploration to imitate and complement the offline dataset, whereas the third stage invokes a sample-optimal offline RL algorithm to compute a near-optimal policy.

In the sequel, we split the offline dataset \(\mathcal{D}^{\mathsf{off}}\) into two halves:

\[\mathcal{D}^{\mathsf{off},1}\qquad\text{and}\qquad\mathcal{D}^{\mathsf{off},2},\] (12)

where \(\mathcal{D}^{\mathsf{off},1}\) (resp. \(\mathcal{D}^{\mathsf{off},2}\)) consists of the first (resp. last) \(K^{\mathsf{off}}/2\) independent trajectories from \(\mathcal{D}^{\mathsf{off}}\). As we shall also see momentarily, online exploration in the proposed algorithm -- which collects \(K^{\mathsf{on}}\) trajectories in total -- can be divided into three parts, collecting \(K^{\mathsf{on}}_{\mathsf{prepare}}\), \(K^{\mathsf{on}}_{\mathsf{imitate}}\) and \(K^{\mathsf{on}}_{\mathsf{explore}}\) sample trajectories, respectively. Throughout this paper, for simplicity we choose

\[K^{\mathsf{on}}_{\mathsf{prepare}}=K^{\mathsf{on}}_{\mathsf{imitate}}=K^{ \mathsf{on}}_{\mathsf{explore}}=K^{\mathsf{on}}/3.\] (13)

### A three-stage algorithm

We now elaborate on the three stages of the proposed algorithm. Due to space limitation, the pseudocode of the complete algorithm is provided in Appendix B.

Stage 1: estimation of the occupancy distributions.As a preparatory step for reward-agnostic exploration, we first attempt to estimate the occupancy distribution induced by any policy as well as the occupancy distribution \(d^{\text{off}}\) associated with the historical dataset, as described below.

_Estimating \(d^{\pi}\) for any policy \(\pi\)._ In this step, we would like to sample the environment and collect a set of sample trajectories, in a way that allows for reasonable estimation of the occupancy distribution \(d^{\pi}\) induced by any policy \(\pi\). For this purpose, we invoke the exploration strategy and the accompanying estimation scheme developed in Li et al. (2023). Working forward (i.e., from \(h=1\) to \(H\)), this approach collects, for each step \(h\), a set of \(N\) sample trajectories in order to facilitate estimation of the occupancy distributions, which amounts to a total number of

\[NH\eqqcolon K^{\mathsf{on}}_{\mathsf{prepare}}=K^{\mathsf{on}}/3\] (14)

sample trajectories collected in this stage. See Algorithm 3 in Appendix D.1 for a precise description of this strategy. Noteworthily, while Algorithm 3 specifies how to estimate \(\widehat{d}^{\pi}\) for any policy \(\pi\), we won't need to compute it explicitly unless this policy \(\pi\) is encountered during the subsequent steps of the algorithm; in other words, \(\widehat{d}^{\pi}\) should be viewed as a sort of "function handle" that will only be executed when called later.

_Estimating \(d^{\text{off}}\) for the historical dataset \(\mathcal{D}^{\text{off}}\)._ In addition, we are in need of estimating the occupancy distribution \(d^{\text{off}}\). Towards this end, we propose the following empirical estimate using the \(K^{\text{off}}/2\) sample trajectories from \(\mathcal{D}^{\text{off},1}\):

\[\widehat{d}^{\text{off}}_{h}(s,a)=\frac{2N^{\text{off}}_{h}(s,a)}{K^{\text{ off}}}\mathds{1}\left(\frac{N^{\text{off}}_{h}(s,a)}{K^{\text{off}}}\geq c_{ \text{off}}\bigg{\{}\frac{\log\frac{HSA}{\delta}}{K^{\text{off}}}+\frac{H^{4 }S^{4}A^{4}\log\frac{HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\bigg{\}}\right)\] (15)

for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), where \(c_{\text{off}}>0\) is some universal constant. Here, \(1-\delta\) indicates the target success probability, and

\[N^{\text{off}}_{h}(s,a)=\sum_{k=1}^{K^{\text{off}}/2}\mathds{1}\left(s^{k, \text{off}}_{h}=s,a^{k,\text{off}}_{h}=a\right),\qquad\forall(s,a)\in\mathcal{ S}\times\mathcal{A}.\] (16)

In other words, \(\widehat{d}^{\text{off}}_{h}(s,a)\) is taken to be the empirical visitation frequency of \((s,a)\) in \(\mathcal{D}^{\text{off},1}\) if \((s,a)\) is adequately visited, and zero otherwise. The cutoff threshold will be made clear in our analysis.

Stage 2: online exploration.Armed with the above estimates of the occupancy distributions, we can readily proceed to compute the desired exploration policies and sample the environment. We seek to devise two exploration strategies, with one strategy selected to imitate the offline dataset, and the other one employed to explore the insufficiently visited territory. As a preliminary fact, if we have a dataset containing \(K\) independent trajectories -- generated independently from a mixture of deterministic policies with occupancy distribution \(d^{\text{b}}\) -- then it has been shown previously (see, e.g., Li et al. (2023, Section 3.3)) that the model-based offline approach is able to compute a policy \(\widehat{\pi}\) obeying

\[V^{\star}(\rho)-V^{\widehat{\pi}}(\rho)\lesssim H\left[\sum_{h}\sum_{s,a}\frac {d^{\pi^{\star}}_{h}(s,a)}{1/H+K^{\mathsf{on}}d^{\text{b}}_{h}(s,a)}\right]^{ \frac{1}{2}}.\] (17)

This upper bound in (17) provides a guideline regarding how to design a sample-efficient exploration scheme.

_Imitating the offline dataset._ The offline dataset \(\mathcal{D}^{\text{off}}\) is most informative when it contains expert data, a scenario when the data distribution resembles the distribution induced by the optimal policy \(\pi^{\star}\). If this is the case, then it is desirable to find a policy similar to \(\pi^{\text{off}}\) in (6) (the mixed policy generating \(\mathcal{D}^{\text{off}}\)) and employ it to collect new data, in order to retain and further strength the benefits ofsuch offline data. To do so, we attempt to approximate \(d^{\pi^{*}}\) by \(\widehat{d}^{\text{off}}\) in (17) when attempting to minimize (17). In fact, we would like to compute a mixture of deterministic policies by (approximately) solving the following optimization problem:

\[\mu^{\text{imitate}}\approx\arg\min_{\mu\in\Delta(\Pi)}\sum_{h=1}^{H}\sum_{s \in\mathcal{S}}\max_{a\in\mathcal{A}}\frac{\widehat{d}^{\text{off}}_{h}(s,a)} {\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi^{\prime}\sim\mu}\big{[}\widehat{d} ^{\pi^{\prime}}_{h}(s,a)\big{]}},\] (18)

which is clearly equivalent to

\[\mu^{\text{imitate}}\approx\arg\min_{\mu\in\Delta(\Pi)}\max_{\pi:\mathcal{S} \times[H]\rightarrow\Delta(\mathcal{A})}\sum_{h=1}^{H}\sum_{s\in\mathcal{S}} \mathbb{E}_{a\sim\pi_{h}(\cdot|s)}\bigg{[}\frac{\widehat{d}^{\text{off}}_{h}( s,a)}{\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi^{\prime}\sim\mu}\big{[} \widehat{d}^{\pi^{\prime}}_{h}(s,a)\big{]}}\bigg{]}.\] (19)

In order to solve this minimax problem (19) (note that its objective function is convex in \(\mu\)), we resort to the Follow-The-Regularized-Leader (FTRL) strategy from the online learning literature (Shalev-Shwartz, 2012); more specifically, we perform the following updates iteratively for \(t=1,\ldots,T_{\text{max}}\):

\[\pi^{t+1}_{h}(\cdot\,|\,s) \propto\exp\bigg{(}\eta\sum_{k=1}^{t}\frac{\widehat{d}^{\text{ off}}_{h}(s,\cdot)}{\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi^{\prime}\sim\mu} \big{[}\widehat{d}^{\pi^{\prime}}_{h}(s,\cdot)\big{]}}\bigg{)},\qquad\forall s \in\mathcal{S},\] (20a) \[\mu^{t+1} \approx\arg\min_{\mu\in\Delta(\Pi)}\sum_{h=1}^{H}\sum_{s\in \mathcal{S}}\mathbb{E}_{a\sim\pi^{t+1}_{h}(\cdot|s)}\bigg{[}\frac{\widehat{d} ^{\text{off}}_{h}(s,a)}{\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi^{\prime}\sim \mu}\big{[}\widehat{d}^{\pi^{\prime}}_{h}(s,a)\big{]}}\bigg{]},\] (20b)

where \(\eta\) denotes the learning rate to be specified later. We shall discuss how to solve the optimization sub-problem (20b) in Appendix C. The output of this step is a mixture of deterministic policies taking the following form:

\[\pi^{\text{imitate}}=\mathbb{E}_{\pi\sim\mu^{\text{imitate}}}[\pi]\qquad\text {with}\quad\mu^{\text{imitate}}=\frac{1}{T_{\text{max}}}\sum_{t=1}^{T_{\text{ max}}}\mu^{t}.\] (21)

_Exploring the unknown environment_. In addition to mimicking the behavior of the historical dataset, we shall also attempt to explore the environment in a way that complements pre-collected data. Towards this end, it suffices to invoke the reward-agnostic online exploration scheme proposed in Li et al. (2023), whose precise description will be provided in Algorithm 5 in Appendix D.2 to make the paper self-contained. The resulting policy mixture is denoted by

\[\pi^{\text{explore}}=\mathbb{E}_{\pi\sim\mu^{\text{explore}}}[\pi],\] (22)

with \(\mu^{\text{explore}}\in\Delta(\Pi)\) representing the associated weight vector.

With the above two exploration policies (21) and (22) in place, we execute the MDP to obtain sample trajectories as follows:

1. Execute the MDP \(K^{\text{on}}_{\text{imitate}}\) times using policy \(\pi^{\text{imitate}}\) to obtain a dataset containing \(K^{\text{on}}_{\text{imitate}}=K^{\text{on}}/3\) independent sample trajectories, denoted by \(\mathcal{D}^{\text{on}}_{\text{imitate}}\);
2. Execute the MDP \(K^{\text{on}}_{\text{explore}}\) times using policy \(\pi^{\text{explore}}\) to obtain a dataset containing \(K^{\text{on}}_{\text{explore}}=K^{\text{on}}/3\) independent sample trajectories, denoted by \(\mathcal{D}^{\text{on}}_{\text{explore}}\).

Stage 3: policy learning via offline RL.Once the above online exploration process is completed, we are positioned to compute a near-optimal policy on the basis of the data in hand. More precisely,

* Let us look at the following dataset \[\mathcal{D}=\mathcal{D}^{\text{off},2}\cup\mathcal{D}^{\text{on}}_{\text{ imitate}}\cup\mathcal{D}^{\text{on}}_{\text{explore}}.\] (23) In light of the complicated statistical dependency between \(\mathcal{D}^{\text{off},1}\) and \(\mathcal{D}^{\text{on}}_{\text{imitate}}\cup\mathcal{D}^{\text{on}}_{\text{explore}}\), we only include the second half \(\mathcal{D}^{\text{off},2}\) of the offline dataset \(\mathcal{D}^{\text{off}}\), so as to exploit the fact that \(\mathcal{D}^{\text{off},2}\) is statistically independent from \(\mathcal{D}^{\text{on}}_{\text{imitate}}\cup\mathcal{D}^{\text{on}}_{\text{explore}}\).
* We invoke the pessimistic model-based offline RL algorithm proposed in Li et al. (2022) to compute the final policy estimate \(\widehat{\pi}\); see Algorithm 6 in Appendix D.3 for more details.

Main results

As it turns out, Algorithm 1 is capable of achieving provable sample efficiency, as demonstrated in the following theorem. Here and below, we recall that \(K=K^{\mathsf{off}}+K^{\mathsf{on}}\).

**Theorem 1**.: _Consider \(\delta\in(0,1)\) and \(\varepsilon\in(0,H]\). Choose the algorithmic parameters such that_

\[\eta=\sqrt{\frac{\log A}{2T_{\mathsf{max}}(K^{\mathsf{on}}H)^{2}}}\qquad\text{ and}\qquad T_{\mathsf{max}}\geq 2(K^{\mathsf{on}}H)^{2}\log A.\]

_Suppose that_

\[K^{\mathsf{on}}+K^{\mathsf{off}} \geq c_{1}\frac{H^{3}SC^{\star}(\sigma)}{\varepsilon^{2}}\log^{2 }\frac{K}{\delta}\] (24a) \[K^{\mathsf{on}} \geq c_{1}\frac{H^{3}SA\min\{H\sigma,1\}}{\varepsilon^{2}}\log \frac{K}{\delta}\] (24b)

_for some large enough constant \(c_{1}>0\). Then with probability at least \(1-\delta\), the policy \(\widehat{\pi}\) returned by Algorithm 1 satisfies_

\[V_{1}^{\star}(\rho)-V^{\widehat{\pi}}(\rho)\leq\varepsilon,\]

_provided that \(K^{\mathsf{on}}\) and \(K^{\mathsf{off}}\) both exceed some polynomial \(\mathsf{poly}(H,S,A,C^{\star}(\sigma),\log\frac{K}{\delta})\) (independent of \(\varepsilon\))._

The proof is deferred to Appendix E. In a nutshell, Theorem 1 uncovers that our algorithm yields \(\varepsilon\)-accuracy as long as

\[K^{\mathsf{on}}+K^{\mathsf{off}} \gtrsim\frac{H^{3}SC^{\star}(\sigma)}{\varepsilon^{2}}\log^{2} \frac{K}{\delta},\] (25a) \[K^{\mathsf{on}} \gtrsim\frac{H^{3}SA\min\{H\sigma,1\}}{\varepsilon^{2}}\log \frac{K}{\delta},\] (25b)

ignoring lower-order terms. Several implications of this result are as follows.

Sample complexity benefits compared with pure online or pure offline RL.To make apparent its advantage compared with both pure offline and online RL, we make comparisons with several most relevant works. Discussions of other related works are deferred to Appendix A.

_Sample complexity with balanced online and offline data._ For the ease of presentation, let us look at a simple case where \(K^{\mathsf{off}}=K^{\mathsf{on}}=K/2\). The the sample complexity bound (25) in this case simplifies to

\[\widetilde{O}\left(\min_{\sigma\in[0,1]}\left\{\frac{H^{3}SA\min\{H\sigma,1\} }{\varepsilon^{2}}+\frac{H^{3}SC^{\star}(\sigma)}{\varepsilon^{2}}\right\} \right)=:\widetilde{O}\left(\min_{\sigma\in[0,1]}f_{\mathsf{mixed}}(\sigma) \right).\] (26)

_Comparisons with pure online RL._ We now look at pure online RL, corresponding to the case where \(K=K^{\mathsf{on}}\) (so that all sample episodes are collected via online exploration). In this case, the minimax-optimal sample complexity for computing an \(\varepsilon\)-optimal policy is known to be (Azar et al., 2017; Li et al., 2023)

\[\widetilde{O}\!\left(\frac{H^{3}SA}{\varepsilon^{2}}\right)=\widetilde{O}\! \left(f_{\mathsf{mixed}}(1)\right)\] (27)

assuming that \(\varepsilon\) is sufficiently small, which is clearly worse than (26). For instance, if there exists some very small \(\sigma\ll 1/H\) obeying \(C^{\star}(\sigma)\lesssim 1\), then the ratio of (26) to (27) is at most

\[H\sigma+1/A\ll 1,\] (28)

thus resulting in substantial sample size savings.

_Comparisons with pure offline RL._ In contrast, in the pure offline case where \(K=K^{\mathsf{off}}\), the minimax sample complexity is known to be (Li et al., 2022)

\[\widetilde{O}\!\left(\frac{H^{3}SC^{\star}(0)}{\varepsilon^{2}}\right)= \widetilde{O}\!\left(f_{\mathsf{mixed}}(0)\right)\] (29)

for any target accuracy level \(\varepsilon\), which is apparently larger than (26) in general. In particular, recognizing that \(C^{\star}(0)=\infty\) in the presence of incomplete coverage of the state-action space reachable by \(\pi^{\star}\), we might harvest enormous sample size benefits (by exploiting the ability of online RL to visit the previously uncovered state-action-step tuples).

Comparison with Wagenmaker and Pacchiano (2022).It is worth noting that Wagenmaker and Pacchiano (2022) also considered policy fine-tuning and proposed a method called FTPedel to tackle linear MDPs. The results therein, however, were mainly instance-dependent, thus making it difficult to compare in general. That being said, we would like to clarify two points:

* Wagenmaker and Pacchiano (2022) imposed all-policy concentrability assumptions, requrting the combined dataset (i.e., the offline and online data altogether) to cover certain feature vectors for all linear softmax policies (see Wagenmaker and Pacchiano (2022, Definition 4.1)). In contrast, our results only assume single-policy (partial) concentrability, which is much weaker than the all-policy counterpart.
* When specializing Wagenmaker and Pacchiano (2022, Corollary 1) to the tabular cases, the sample complexity therein becomes \(\widetilde{O}(H^{7}S^{2}A^{2}/\varepsilon^{2})\), which is much larger than our result.

Miscellaneous properties of the proposed algorithm.In addition to the sample complexity advantages, the proposed hybrid RL enjoys several attributes that could be practically appealing.

_Adaptivity to unknown optimal \(\sigma\)._ While we have introduced the parameter \(\sigma\) to capture incomplete coverage, our algorithm does not rely on any knowledge of \(\sigma\). Take the balanced case described around (26) for instance: our algorithm automatically identifies the optimal \(\sigma\) that minimizes the function \(f_{\text{mixed}}(\sigma)\) over all \(\sigma\in[0,1]\). In other words, Algorithm 1 is able to automatically identify the optimal trade-offs between distribution mismatch and inadequate coverage.

_Reward-agnostic data collection._ It is noteworthy that the online exploration procedure employed in Algorithm 1 does not require any prior information about the reward function. In other words, it is mainly designed to improve coverage of the state-action space, a property independent from the reward function. In truth, the reward function is only queried at the last step to output the learned policy. This enables us to perform hybrid RL in a reward-agnostic manner, which is particularly intriguing in practice, as there is no shortage of scenarios where the reward functions might be engineered subsequently to meet different objectives.

_Strengthening behavior cloning._ Another notable feature is that our algorithm does not rely on prior knowledge about the policies generating the offline dataset \(\mathcal{D}^{\text{off}}\); in fact, it is capable of finding a mixed exploration policy \(\pi^{\text{imitate}}\) that inherits the advantages of the unknown behavior policy \(\pi^{\text{off}}\). This could be of particular interest for behavior cloning, where the offline dataset \(\mathcal{D}^{\text{off}}\) is generated by an expert policy, with \(C^{\star}=C^{\star}(0)\approx 1\), i.e. the expert policy covers the optimal one. In this situation, the supplement of online data collection improves behavior cloning by lowering the statistical error from \(\sqrt{\frac{H^{3}SC^{\star}}{K_{\text{off}}}}\) to \(\sqrt{\frac{H^{3}SC^{\star}}{K_{\text{off}}+K_{\text{on}}}}\), together with an executable learned policy \(\pi^{\text{imitate}}\).

_Computational complexity._ We now take a moment to discuss the computational cost of the proposed algorithm. In Stage 1, we need to first estimate the transition matrices \(\{P_{h}\}\), which can be accomplished with runtime \(O(K^{\text{on}})\). In the ensuing stages, we call Algorithm 3 to estimate \(\widehat{d}^{\pi}\) for each \(\pi\) we encounter. When computing \(\pi^{\text{imitate}}\), we need to calculate \(\widehat{d}^{\pi}\) for \(T_{1}=T_{\text{max}}T_{2}\) times, where \(T_{2}\) denotes the number of iterations for calculating \(\mu^{t+1}\) in Eq. (20b); in comparison, the computational cost of estimating \(d^{\pi}\) to yield \(\pi^{\text{explore}}\) in Eq. (22) is much smaller. For each \(\widehat{d}^{\pi}\), it needs \(O(HS^{2}A)\) computation. With a slight modification on the target Eq. (19) as follows

\[\mu^{\text{imitate}}\approx\] \[\arg\min_{\mu\in\Delta(\Pi)}\sum_{h=1}^{H}\sum_{s\in\mathcal{S}} \max_{a\in\mathcal{A}}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+O \big{(}\frac{1}{SH}\big{)}\widehat{d}_{h}^{\text{off}}(s,a)+\underset{\pi^{ \prime}\sim\mu^{\text{explore}}}{\mathbb{E}}\big{[}\widehat{d}_{h}^{\pi^{ \prime}}(s,a)\big{]}+\underset{\pi^{\prime}\sim\mu}{\mathbb{E}}\big{[} \widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}},\] (30)

we can find a good enough \(\pi^{\text{imitate}}\) with \(T_{\text{max}}=\widetilde{O}(H^{2}S^{2})\) for \(\eta\asymp\frac{1}{H^{2}S^{2}}\) and \(O(H^{4}S^{4}A^{2})\) Frank-Wolfe updates for \(\alpha\asymp\frac{1}{H^{3}S^{3}A^{2}}\). These taken collectively lead to the following computational complexity for each stage: \(O(K^{\text{on}}+H^{7}S^{8}A^{3}+K^{\text{off}}H)\) for Stage 1, \(O(H^{7}S^{7}A^{3})\) for Stage 2, and \(O(KH)\) for Stage 3.

Discussion

We have studied the policy fine-tuning problem of practical interest, where one is allowed to exploit pre-collected historical data to facilitate and improve online RL. We have proposed a three-stage algorithm tailored to the tabular setting, which attains provable sample size savings compared with both pure online RL and pure offline RL algorithms. Our algorithm design has leveraged key insights from recent advances in both model-based offline RL and reward-agnostic online RL.

While the proposed algorithm achieves provable sample efficiency, this cannot be guaranteed unless the sample size already surpasses a fairly large threshold (in other words, the algorithm imposes a high burn-in cost). It would be of great interest to see whether one can achieve sample optimality for the entire \(\varepsilon\)-range. Another issue arises from the computation side: even though the proposed algorithm can be implemented in polynomial time, the computational complexity of the Frank-Wolfe-type subroutine might already be too expensive for solving problems with enormous dimensionality. Can we hope to further accelerate it to make it practically more appealing? Finally, it might also be interesting to study sample-efficient hybrid RL in the presence of low-complexity function approximation, in the hope of further reducing sample complexity.

## Acknowledgements

Y. Chen is supported in part by the Alfred P. Sloan Research Fellowship, the Google Research Scholar Award, the AFOSR grant FA9550-22-1-0198, the ONR grant N00014-22-1-2354, and the NSF grants CCF-2221009 and CCF-1907661. Y. Chi are supported in part by the grants ONR N00014-19-1-2404, NSF CCF-2106778, DMS-2134080 and CNS-2148212. J. D. Lee acknowledges support of the ARO under MURI Award W911NF-11-1-0304, the Sloan Research Fellowship, the NSF grants CCF 2002272, IIS 2107304 and CIF 2212262, the ONR Young Investigator Award, and the NSF CAREER Award 2144994.

## References

* Agarwal et al. (2020) Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W. (2020). Flambe: Structural complexity and representation learning of low rank mdps. _arXiv preprint arXiv:2006.10814_.
* Alon and Spencer (2016) Alon, N. and Spencer, J. H. (2016). _The probabilistic method_. John Wiley & Sons.
* Auer and Ortner (2006) Auer, P. and Ortner, R. (2006). Logarithmic online regret bounds for undiscounted reinforcement learning. _Advances in neural information processing systems_, 19.
* Azar et al. (2017) Azar, M. G., Osband, I., and Munos, R. (2017). Minimax regret bounds for reinforcement learning. In _Proceedings of the 34th International Conference on Machine Learning-Volume 70_, pages 263-272. JMLR. org.
* Bai et al. (2019) Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019). Provably efficient \(q\)-learning with low switching cost. In _Advances in Neural Information Processing Systems_, volume 32, pages 8002-8011.
* Berner et al. (2019) Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. (2019). Dota 2 with large scale deep reinforcement learning. _arXiv preprint arXiv:1912.06680_.
* Bertsekas (2017) Bertsekas, D. P. (2017). _Dynamic programming and optimal control (4th edition)_. Athena Scientific.
* Brafman and Tennenholtz (2002) Brafman, R. I. and Tennenholtz, M. (2002). R-max-a general polynomial time algorithm for near-optimal reinforcement learning. _Journal of Machine Learning Research_, 3(Oct):213-231.
* Brambilla et al. (2013) Brambilla, M., Ferrante, E., Birattari, M., and Dorigo, M. (2013). Swarm robotics: a review from the swarm engineering perspective. _Swarm Intelligence_, 7(1):1-41.
* Bubeck (2015) Bubeck, S. (2015). Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357.
* Cai et al. (2022) Cai, C., Cai, T. T., and Li, H. (2022). Transfer learning for contextual multi-armed bandits. _arXiv preprint arXiv:2211.12612_.
* Cai et al. (2019)Chen, J. and Jiang, N. (2019). Information-theoretic considerations in batch reinforcement learning. In _Proceedings of the 36th International Conference on Machine Learning_, volume 97, pages 1042-1051.
* Chen et al. (2022a) Chen, J., Modi, A., Krishnamurthy, A., Jiang, N., and Agarwal, A. (2022a). On the statistical efficiency of reward-free exploration in non-linear rl. _arXiv preprint arXiv:2206.10770_.
* Chen et al. (2022b) Chen, Z., Li, C. J., Yuan, A., Gu, Q., and Jordan, M. I. (2022b). A general framework for sample-efficient function approximation in reinforcement learning. _arXiv preprint arXiv:2209.15634_.
* Cui and Du (2022) Cui, Q. and Du, S. S. (2022). When is offline two-player zero-sum Markov game solvable? _arXiv preprint arXiv:2201.03522_.
* Dann et al. (2017) Dann, C., Lattimore, T., and Brunskill, E. (2017). Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning. _Advances in Neural Information Processing Systems_, 30.
* Domingues et al. (2021) Domingues, O. D., Menard, P., Kaufmann, E., and Valko, M. (2021). Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In _Algorithmic Learning Theory_, pages 578-598. PMLR.
* Dong et al. (2019) Dong, K., Wang, Y., Chen, X., and Wang, L. (2019). Q-learning with UCB exploration is sample efficient for infinite-horizon MDP. _arXiv preprint arXiv:1901.09311_.
* Du et al. (2021) Du, S. S., Kakade, S. M., Lee, J. D., Lovett, S., Mahajan, G., Sun, W., and Wang, R. (2021). Bilinear classes: A structural framework for provable generalization in rl.
* Foster et al. (2021) Foster, D. J., Kakade, S. M., Qian, J., and Rakhlin, A. (2021). The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_.
* Hester et al. (2018) Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Osband, I., et al. (2018). Deep Q-learning from demonstrations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32.
* Huang et al. (2022) Huang, R., Yang, J., and Liang, Y. (2022). Safe exploration incurs nearly no additional sample complexity for reward-free RL. _arXiv preprint arXiv:2206.14057_.
* Jaksch et al. (2010) Jaksch, T., Ortner, R., and Auer, P. (2010). Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_.
* Jin et al. (2018) Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M. I. (2018). Is Q-learning provably efficient? In _Advances in Neural Information Processing Systems_, pages 4863-4873.
* Jin et al. (2020a) Jin, C., Krishnamurthy, A., Simchowitz, M., and Yu, T. (2020a). Reward-free exploration for reinforcement learning. In _International Conference on Machine Learning_, pages 4870-4879. PMLR.
* Jin et al. (2021a) Jin, C., Liu, Q., and Miryoosefi, S. (2021a). Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. _Advances in Neural Information Processing Systems_, 34.
* Jin et al. (2020b) Jin, C., Yang, Z., Wang, Z., and Jordan, M. I. (2020b). Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143. PMLR.
* Jin et al. (2020c) Jin, Y., Yang, Z., and Wang, Z. (2020c). Is pessimism provably efficient for offline rl? _arXiv preprint arXiv:2012.15085_.
* Jin et al. (2021b) Jin, Y., Yang, Z., and Wang, Z. (2021b). Is pessimism provably efficient for offline RL? In _International Conference on Machine Learning_, pages 5084-5096. PMLR.
* Kalashnikov et al. (2018) Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., and Vanhoucke, V. (2018). Scalable deep reinforcement learning for vision-based robotic manipulation. In _Conference on Robot Learning_, pages 651-673. PMLR.
* Kaufmann et al. (2021) Kaufmann, E., Menard, P., Domingues, O. D., Jonsson, A., Leurent, E., and Valko, M. (2021). Adaptive reward-free exploration. In _Algorithmic Learning Theory_, pages 865-891. PMLR.
* Krizhevsky et al. (2014)Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative q-learning for offline reinforcement learning. In _Conference on Neural Information Processing Systems (NeurIPS)_.
* Li et al. (2021a) Li, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021a). Sample-efficient reinforcement learning is feasible for linearly realizable MDPs with limited revisiting. _Advances in Neural Information Processing Systems_, 34:16671-16685.
* Li et al. (2022) Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022). Settling the sample complexity of model-based offline reinforcement learning. _arXiv preprint arXiv:2204.05275_.
* Li et al. (2021b) Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y. (2021b). Breaking the sample complexity barrier to regret-optimal model-free reinforcement learning. _Advances in Neural Information Processing Systems_, 34:17762-17776.
* Li et al. (2023) Li, G., Yan, Y., Chen, Y., and Fan, J. (2023). Minimax-optimal reward-agnostic exploration in reinforcement learning. _arXiv preprint arXiv:2304.0727_.
* Liu et al. (2020) Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. (2020). Provably good batch reinforcement learning without great exploration. _arXiv preprint arXiv:2007.08202_.
* Menard et al. (2021a) Menard, P., Domingues, O. D., Jonsson, A., Kaufmann, E., Leurent, E., and Valko, M. (2021a). Fast active learning for pure exploration in reinforcement learning. In _International Conference on Machine Learning_, pages 7599-7608. PMLR.
* Menard et al. (2021b) Menard, P., Domingues, O. D., Shang, X., and Valko, M. (2021b). UCB momentum Q-learning: Correcting the bias without forgetting. In _International Conference on Machine Learning_, pages 7609-7618.
* Mnih et al. (2013) Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. (2013). Playing atari with deep reinforcement learning. _arXiv preprint arXiv:1312.5602_.
* Munos and Szepesvari (2008) Munos, R. and Szepesvari, C. (2008). Finite-time bounds for fitted value iteration. In _Journal of Machine Learning Research_, volume 9, pages 815-857.
* Nair et al. (2020) Nair, A., Gupta, A., Dalal, M., and Levine, S. (2020). Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_.
* Nair et al. (2018) Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P. (2018). Overcoming exploration in reinforcement learning with demonstrations. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 6292-6299. IEEE.
* Nakamoto et al. (2023) Nakamoto, M., Zhai, Y., Singh, A., Mark, M. S., Ma, Y., Finn, C., Kumar, A., and Levine, S. (2023). Cal-QL: Calibrated offline RL pre-training for efficient online fine-tuning. _arXiv preprint arXiv:2303.05479_.
* Qiao and Wang (2022) Qiao, D. and Wang, Y.-X. (2022). Near-optimal deployment efficiency in reward-free reinforcement learning with linear function approximation. _arXiv preprint arXiv:2210.00701_.
* Rajeswaran et al. (2017) Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., and Levine, S. (2017). Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. _arXiv preprint arXiv:1709.10087_.
* Rashidinejad et al. (2021) Rashidinejad, P., Zhu, B., Ma, C., Jiao, J., and Russell, S. (2021). Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34:11702-11716.
* Ross and Bagnell (2012) Ross, S. and Bagnell, J. A. (2012). Agnostic system identification for model-based reinforcement learning. _International Conference on Machine learning_.
* Shalev-Shwartz (2012) Shalev-Shwartz, S. (2012). Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2):107-194.
* Shalev-Shwartz et al. (2016) Shalev-Shwartz, S., Shammah, S., and Shashua, A. (2016). Safe, multi-agent, reinforcement learning for autonomous driving. _arXiv preprint arXiv:1610.03295_.
* Shalev-Shwartz et al. (2017)Shi, L. and Chi, Y. (2022). Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity. _arXiv preprint arXiv:2208.05767_.
* Shi et al. (2022) Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022). Pessimistic Q-learning for offline reinforcement learning: Towards optimal sample complexity. In _International Conference on Machine Learning_, pages 19967-20025. PMLR.
* Silver et al. (2016) Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of Go with deep neural networks and tree search. _nature_, 529(7587):484-489.
* Song et al. (2022) Song, Y., Zhou, Y., Sekhari, A., Bagnell, J. A., Krishnamurthy, A., and Sun, W. (2022). Hybrid RL: Using both offline and online data can make RL efficient. _arXiv preprint arXiv:2210.06718_.
* Uehara and Sun (2021) Uehara, M. and Sun, W. (2021). Pessimistic model-based offline RL: PAC bounds and posterior sampling under partial coverage. In _arXiv preprint arXiv:2107.06226_.
* Vecerik et al. (2017) Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Rothorl, T., Lampe, T., and Riedmiller, M. (2017). Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. _arXiv preprint arXiv:1707.08817_.
* Vinyals et al. (2019) Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik, A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019). Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354.
* Wagenmaker and Pacchiano (2022) Wagenmaker, A. and Pacchiano, A. (2022). Leveraging offline data in online reinforcement learning. _arXiv preprint arXiv:2211.04974_.
* Wagenmaker et al. (2022) Wagenmaker, A. J., Chen, Y., Simchowitz, M., Du, S., and Jamieson, K. (2022). Reward-free RL is no harder than reward-aware RL in linear Markov decision processes. In _International Conference on Machine Learning_, pages 22430-22456.
* Wang et al. (2020) Wang, R., Du, S. S., Yang, L., and Salakhutdinov, R. R. (2020). On reward-free reinforcement learning with linear function approximation. _Advances in neural information processing systems_, 33:17816-17826.
* Xie et al. (2021a) Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal, A. (2021a). Bellman-consistent pessimism for offline reinforcement learning. _arXiv preprint arXiv:2106.06926_.
* Xie et al. (2021b) Xie, T., Jiang, N., Wang, H., Xiong, C., and Bai, Y. (2021b). Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. _arXiv preprint arXiv:2106.04895_.
* Yan et al. (2022) Yan, Y., Li, G., Chen, Y., and Fan, J. (2022). Model-based reinforcement learning is minimax-optimal for offline zero-sum Markov games. _arXiv preprint arXiv:2206.04044_.
* Yan et al. (2023) Yan, Y., Li, G., Chen, Y., and Fan, J. (2023). The efficacy of pessimism in asynchronous Q-learning. _IEEE Transactions on Information Theory_.
* Yin et al. (2021) Yin, M., Bai, Y., and Wang, Y.-X. (2021). Near-optimal offline reinforcement learning via double variance reduction. _Advances in neural information processing systems_, 34:7677-7688.
* Zanette et al. (2020) Zanette, A., Lazaric, A., Kochenderfer, M., and Brunskill, E. (2020). Learning near optimal policies with low inherent bellman error. In _International Conference on Machine Learning_, pages 10978-10989. PMLR.
* Zhan et al. (2022) Zhan, W., Huang, B., Huang, A., Jiang, N., and Lee, J. (2022). Offline reinforcement learning with realizability and single-policy concentrability. In _Conference on Learning Theory_, pages 2730-2775. PMLR.
* Zhang et al. (2021a) Zhang, W., Zhou, D., and Gu, Q. (2021a). Reward-free model-based reinforcement learning with linear function approximation. _Advances in Neural Information Processing Systems_, 34:1582-1593.
* Zhang et al. (2020a) Zhang, X., Ma, Y., and Singla, A. (2020a). Task-agnostic exploration in reinforcement learning. _Advances in Neural Information Processing Systems_, 33:11734-11743.

Zhang, Z., Chen, Y., Lee, J. D., and Du, S. S. (2023). Settling the sample complexity of online reinforcement learning. _arXiv preprint arXiv:2307.13586_.
* Zhang et al. (2021) Zhang, Z., Du, S., and Ji, X. (2021b). Near optimal reward-free reinforcement learning. In _International Conference on Machine Learning_, pages 12402-12412. PMLR.
* Zhang et al. (2020) Zhang, Z., Zhou, Y., and Ji, X. (2020b). Model-free reinforcement learning: from clipped pseudo-regret to sample complexity. _arXiv preprint arXiv:2006.03864_.
* Zhou et al. (2021a) Zhou, D., Gu, Q., and Szepesvari, C. (2021a). Nearly minimax optimal reinforcement learning for linear mixture markov decision processes. In _Conference on Learning Theory_, pages 4532-4576. PMLR.
* Zhou et al. (2021b) Zhou, Z., Zhou, Z., Bai, Q., Qiu, L., Blanchet, J., and Glynn, P. (2021b). Finite-sample regret bound for distributionally robust offline tabular reinforcement learning. In _International Conference on Artificial Intelligence and Statistics_, pages 3331-3339. PMLR.

Other related works

In this section, we briefly discuss a small set of additional prior works related to the current paper.

(Reward-aware) online RL.In online RL, an agent seeks to find a near-optimal policy by sequentially and adaptively interacting with the unknown environment, without having access to any additional offline dataset. The extensive studies of online RL gravitate around how to optimally trade off exploration against exploitation, for which the principle of optimism in the face of uncertainty plays a crucial role (Auer and Ortner, 2006; Jaksch et al., 2010; Azar et al., 2017; Dann et al., 2017; Jin et al., 2018; Bai et al., 2019; Dong et al., 2019; Zhang et al., 2020; Menard et al., 2021; Li et al., 2021; Zhang et al., 2023). Information-theoretic regret lower bounds have been established by Domingues et al. (2021); Jin et al. (2018), which are shown to be achievable (up to log factor) by the model-based approach for arbitrary sample sizes (Zhang et al., 2023). A further strand of works extended these studies to the case with function approximation, including both linear function approximation (Jin et al., 2020; Zanette et al., 2020; Zhou et al., 2021; Li et al., 2021) and other more general families of function approximation (Du et al., 2021; Jin et al., 2021; Foster et al., 2021).

Offline RL.In contrast to online RL, offline RL assumes access to a pre-collected offline dataset and precludes active interactions with the environment. Given the absence of further data collection, the sample complexity of pure offline RL depends heavily upon the quality of the offline dataset at hand, which has often been characterized via some sorts of concentrability coefficients in prior works (Rashidinejad et al., 2021; Zhan et al., 2022). Earlier works (Munos and Szepesvari, 2008; Chen and Jiang, 2019) typically operated under the assumption of all-policy concentrability -- namely, the assumption that the dataset covers the visited state-action pairs of all possible policies -- thus imposing a stringent requirement for the offline dataset to be highly explorative. To circumvent this stringent assumption, Liu et al. (2020); Kumar et al. (2020); Jin et al. (2021); Rashidinejad et al. (2021); Uehara and Sun (2021); Li et al. (2022); Yin et al. (2021); Shi et al. (2022); Yan et al. (2023) incorporated the pessimism principle amid uncertainty into the algorithm designs and, as a result, required only single-policy concentrability (so that the dataset only needs to cover the part of the state-action space reachable by the optimal policy). With regards to the basic tabular case, Li et al. (2022) proved that the pessimistic model-based offline algorithm is capable of achieving minimax-optimal sample complexity for the full \(\varepsilon\)-range, accommodating both the episodic finite-horizon case and the discounted infinite-horizon analog. Moving beyond single-agent tabular settings, a recent line of works investigated offline RL in the presence of general function approximation (Jin et al., 2020; Xie et al., 2021; Zhan et al., 2022), environment shift (Zhou et al., 2021; Shi and Chi, 2022), and in the context of zero-sum Markov games (Cui and Du, 2022; Yan et al., 2022).

Hybrid RL.While there were a number of empirical works (Rajeswaran et al., 2017; Vecerik et al., 2017; Kalashnikov et al., 2018; Hester et al., 2018; Nair et al., 2018, 2020) suggesting the performance gain of combining online RL with offline datasets (compared to pure online online or offline learning), rigorous theoretical evidence remained highly limited. Ross and Bagnell (2012); Xie et al. (2021) attempted to develop theoretical understanding by looking at one special hybrid scenario, where the agent can perform either of the following in each episode: (i) collecting a new online episode; and (ii) executing a prescribed and fixed reference policy to generate a sample episode. In this setting, Xie et al. (2021) showed that in the minimax sense, combining online learning with samples generated by such a reference policy is not advantageous in comparison with pure online or offline RL. Note that our results do not contradict with the lower bound in Xie et al. (2021), given that we exploit "partial" single-policy concentrability that implies additional structure except for the worst case. Akin to the current paper, Song et al. (2022); Wagenmaker and Pacchiano (2022) studied online RL with additional access to an offline dataset. Nevertheless, Song et al. (2022) mainly focused on the issue of computational efficiency, and the algorithm proposed therein does not come with improved sample complexity. In contrast, Wagenmaker and Pacchiano (2022) focused attention on statistical efficiency, although the sample complexity derived therein is highly suboptimal when specialized to the tabular setting.

Reward-free and task-agnostic exploration.Reward-free and task-agnostic exploration, which refer to the scenario where the agent first collects online sample trajectories without guidance of any information about the reward function(s), has garnered much recent attention (Brafman and Tennenholtz, 2002; Jin et al., 2020; Zhang et al., 2020, 2021; Huang et al., 2022). Focusing on the tabular case, the earlier work Jin et al. (2020) put forward a reward-free exploration scheme that achieves minimax optimality in terms of the dependency on \(S\), \(A\) and \(1/\varepsilon\), with the horizon dependency further improved by subsequent works (Kaufmann et al., 2021; Menard et al., 2021; Li et al., 2023). In particular, the exploration scheme proposed in Li et al. (2023) was shown to achieve minimax-optimal sample complexity when there exist a polynomial number of pre-determined but unseen reward functions of interest, which inspires the algorithm design of the present paper. Moreover, reward-free RL has been extended to account for function approximation, including both linear (Wang et al., 2020; Agarwal et al., 2020; Qiao and Wang, 2022; Zhang et al., 2021; Wagenmaker et al., 2022) and nonlinear function classes (Chen et al., 2022).

## Appendix B Pseudocode of Algorithm 1

In this section, we provide the whole procedure for the proposed hybrid RL algorithm, with several subroutines deferred to Appendix C and Appendix D.

```
1Input: offline dataset \(\mathcal{D}^{\text{off}}\) (containing \(K^{\text{off}}\) trajectories), parameters \(N,K^{\text{on}},T_{\text{max}}\), learning rate \(\eta\). Initialize:\(\pi_{h}^{1}(a\,|\,s)=1/A\) for any \((s,a,h)\); \(K=K^{\text{off}}+K^{\text{on}}\); split \(\mathcal{D}^{\text{off}}\) into two halves \(\mathcal{D}^{\text{off},1}\) and \(\mathcal{D}^{\text{off},2}\). /* Estimation of occupancy distributions for any policy \(\pi\). */
2 Call Algorithm 3, which allows one to specify \(\widehat{d}_{h}^{\pi}(s,a)\) for any deterministic policy \(\pi\) and any \((s,a,h)\). /* Estimation of occupancy distributions of the historical data. */
3 Use the dataset \(\mathcal{D}^{\text{off},1}\) to compute \[\widehat{d}_{h}^{\text{off}}(s,a)=\frac{2N_{h}^{\text{off}}(s,a)}{K^{\text{ off}}}\mathds{1}\left(\frac{N_{h}^{\text{off}}(s,a)}{K^{\text{off}}}\geq c_{\text{ off}}\bigg{\{}\frac{\log\frac{HSA}{\delta}}{K^{\text{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{\delta}}{N}+\frac{SA}{K^{\text{on}}}\bigg{\}}\right)\] for any \((s,a,h)\), where \(N_{h}^{\text{off}}(s,a)=\sum_{k=1}^{K^{\text{off}}/2}\mathds{1}(s_{h}^{k}=s,a_{ h}^{k}=a)\) and \(c_{\text{off}}>0\) is some absolute constant. /* Compute a general sample-efficient online exploration scheme. */
4 Call Algorithm 5 with estimators \(\widehat{d}^{\pi}\) to compute policy \(\pi^{\text{explore}}\) and the associated weight \(\mu^{\text{explore}}\). /* Compute an online exploration scheme tailored to the offline dataset. */
5for\(t=1,\cdots,T_{\text{max}}\)do
6 Compute \(\mu^{t}\) using Algorithm 2. Update \(\pi_{h}^{t+1}(a\,|\,s)\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) such that: \[\pi_{h}^{t+1}(a\,|\,s)=\frac{\exp\left(\eta\sum_{k=1}^{t}\frac{\widehat{d}_{h }^{\pi}(s,a)}{K^{\text{off}}\eta+\mathbb{E}_{\pi^{\prime}\sim\mu^{k}}\left[ \widehat{d}_{h}^{\pi}(s,a)\right]}\right)}{\sum_{a^{\prime}\in\mathcal{A}}\exp \left(\eta\sum_{k=1}^{t}\frac{\widehat{d}_{h}^{\pi}(s,a^{\prime})}{K^{\text{ off}}\eta+\mathbb{E}_{\pi^{\prime}\sim\mu^{k}}\left[\widehat{d}_{h}^{\pi}(s,a^{ \prime})\right]}\right)},\]
7 Set \(\mu^{\text{imitate}}=\frac{1}{T_{\text{max}}}\sum_{t=1}^{T_{\text{max}}}\mu^{t}\) and \(\pi^{\text{imitate}}=\mathbb{E}_{\pi\sim\mu^{\text{imitate}}}[\pi]\). /* Sampling using the above two exploration policies. */
8 Collect \(K^{\text{on}}_{\text{imitate}}\) (resp. \(K^{\text{on}}_{\text{explore}}\)) sample trajectories using \(\pi^{\text{imitate}}\) (resp. \(\pi^{\text{explore}}\)) to form a dataset \(\mathcal{D}^{\text{on}}_{\text{imitate}}\) (resp. \(\mathcal{D}^{\text{on}}_{\text{explore}}\)). /* Run the model-based offline RL algorithm. */
9 Apply Algorithm 6 to the dataset \(\mathcal{D}=\mathcal{D}^{\text{off},2}\cup\mathcal{D}^{\text{on}}_{\text{imitate }}\cup\mathcal{D}^{\text{on}}_{\text{explore}}\) to compute a policy \(\widehat{\pi}\).
10Output: policy \(\widehat{\pi}\). ```

**Algorithm 1**The proposed hybrid RL algorithm.

Subroutine for solving the subproblem (20b)

While (20b) is a convex optimization subproblem, it involves optimization over a parameter space with exponentially large dimensions. In order to solve it in a computationally feasible manner, we propose a tailored subroutine based on the Frank-Wolfe algorithm (Bubeck, 2015).

Before proceeding, recall that when specifying \(\widehat{d}^{\pi}\) in Algorithm 3, we draw \(N\) independent trajectories \(\{s_{1}^{n,h},a_{1}^{n,h},\ldots,s_{h+1}^{n,h}\}_{1\leq n\leq N}\) and compute an empirical estimate \(\widehat{P}_{h}\) of the probability transition kernel at step \(h\) such that

\[\widehat{P}_{h}(s^{\prime}\,|\,s,a)=\frac{\mathds{1}(N_{h}(s,a)> \xi)}{\max\big{\{}N_{h}(s,a),\,1\big{\}}}\sum_{n=1}^{N}\mathds{1}(s_{h}^{n,h}=s,a_{h}^{n,h}=a,s_{h+1}^{n,h}=s^{\prime}),\qquad\forall(s,a,s^{\prime})\in \mathcal{S}\times\mathcal{A}\times\mathcal{S},\] (31)

where \(N_{h}(s,a)=\sum_{n=1}^{N}\mathds{1}\{s_{h}^{n,h}=s,a_{h}^{n,h}=a\}\).

The proposed Frank-Wolfe-type algorithm.With this set of notation in place and with an initial guess taken to be the indicator function \(\mu^{(1)}=\mathds{1}_{\pi_{\text{init}}}\) for an arbitrary policy \(\pi_{\text{init}}\in\Pi\), the \(k\)-th iteration of our iterative procedure for solving (20b) can be described as follows.

* _Computing a search direction._ The search direction of the Frank-Wolfe algorithm is typically taken to be a feasible direction that maximizes its correlation with the gradient of the objective function (Bubeck, 2015). When specialized to the current sub-problem (20b), the search direction can be taken to be the Dirac measure \(\delta_{\pi^{(k)}}\), where \[\pi^{(k)}=\arg\max_{\pi\in\Pi}\,f\big{(}\pi,\mu^{(k)}\big{)}\coloneqq \sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}(\cdot|s)} \Bigg{[}\frac{\widehat{d}_{h}^{\pi}(s,a)\widehat{d}_{h}^{\mathsf{eff}}(s,a)}{ \big{(}\frac{1}{K^{\mathsf{eff}}}+\mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}} \big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}^{2}}\Bigg{]}.\] (32) As it turns out, this optimization problem (32) can be efficiently solved by applying dynamic programming (Bertsekas, 2017) to an augmented MDP \(\mathcal{M}^{\mathsf{off}}\) constructed as follows.
* Introduce an augmented finite-horizon MDP \(\mathcal{M}^{\mathsf{off}}=(\mathcal{S}\cup\{s_{\text{aug}}\},\mathcal{A},H, \widehat{P}^{\text{aug}},r^{\mathsf{off}})\), where \(s_{\text{aug}}\) is an augmented state. We choose the reward function to be \[r_{h}^{\mathsf{off}}(s,a)=\begin{cases}\frac{\pi_{h}^{t+1}(a\,|\,s)\widehat{d }_{h}^{\mathsf{eff}}(s,a)}{\big{(}\pi_{h}^{\mathsf{off}}+\mathbb{E}_{\pi^{ \prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}^{2}},& \text{if }(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H],\\ 0,&\text{if }(s,a,h)\in\{s_{\text{aug}}\}\times\mathcal{A}\times[H],\end{cases}\] (33) and the probability transition kernel as \[\widehat{P}_{h}^{\text{aug}}(s^{\prime}\,|\,s,a)=\begin{cases} \widehat{P}_{h}(s^{\prime}\,|\,s,a),&\text{if }s^{\prime}\in\mathcal{S}\\ 1-\sum_{s^{\prime}\in\mathcal{S}}\widehat{P}_{h}(s^{\prime}\,|\,s,a),&\text{if }s^{ \prime}=s_{\text{aug}}\end{cases}\quad\text{for all }(s,a,h)\in\mathcal{S}\times \mathcal{A}\times[H],\] (34a) \[\widehat{P}_{h}^{\text{aug}}(s^{\prime}\,|\,s_{\text{aug}},a)= \mathds{1}(s^{\prime}=s_{\text{aug}}) \text{for all }(a,h)\in\mathcal{A}\times[H].\] (34b)
* _Frank-Wolfe updates._ We then update the iterate \(\mu^{(k+1)}\) as a convex combination of the current iterate and the direction found in the previous step: \[\mu^{(k+1)}=(1-\alpha)\mu^{(k)}+\alpha\,\mathds{1}_{\pi^{(k)}},\] (35) where the stepsize is chosen to be \[\alpha=\frac{S}{(K^{\mathsf{on}}H)^{3}}.\] (36)Stopping rule.It is also necessary to specify the stopping rule of the above iterative procedure. Throughout this paper, the above subroutine will terminate as long as

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}(\cdot|s)} \bigg{[}\frac{\widehat{d}_{h}^{\mathsf{ff}}(s,a)}{\frac{1}{K^{\mathsf{o}H}}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}}\bigg{]}\leq 108SH,\] (37)

with the final output taken to be \(\mu^{t+1}=\mu^{(k)}\). We shall justify the feasibility of this stopping rule (namely, the fact that this stopping criterion can be met by some mixed policy) in Appendix F.

Iteration complexity.Encouragingly, the above subroutine in conjunction with the stopping rule (37) leads to an iteration complexity no larger than

\[\text{(iteration complexity)}\qquad O\bigg{(}\frac{(K^{\mathsf{on}}H)^{4}}{S^{ 2}}\bigg{)}\] (38)

The proof of this claim is postponed to Section F.

```
1Initialize:\(\mu^{(1)}=\mathds{1}_{\pi_{\mathsf{init}}}\) for an arbitrary policy \(\pi_{\mathsf{init}}\in\Pi\).
2for\(k=1,2,\cdots\)do
3 Exit for-loop if the following condition is met: // stopping criterion \[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}(\cdot|s)} \bigg{[}\frac{\widehat{d}_{h}^{\mathsf{ff}}(s,a)}{\frac{1}{K^{\mathsf{o}H}}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}}\bigg{]}\leq 108SH.\] (39) /* Find the search direction */
4 Compute the optimal deterministic policy \(\pi^{(k),\mathsf{aug}}\) of the MDP \(\mathcal{M}_{\mathsf{off}}=(\mathcal{S}\cup\{s_{\mathsf{aug}}\},\mathcal{A},H,\widehat{P}^{\mathsf{aug}},r_{\mathsf{off}})\), where \(s_{\mathsf{aug}}\) is an augmented state, \[r_{h}^{\mathsf{off}}(s,a)=\begin{cases}\frac{\pi_{h}^{t+1}(a\,|\,s)\widehat{ d}_{h}^{\mathsf{off}}(s,a)}{\big{(}\pi^{\mathsf{old}H}+\mathbb{E}_{\pi^{\prime}\sim \mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}^{2}},&\text {if }(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H],\\ 0,&\text{if }(s,a,h)\in\{s_{\mathsf{aug}}\}\times\mathcal{A}\times[H],\end{cases}\] (40) and the augmented probability transition kernel is given by \[\widehat{P}_{h}^{\mathsf{aug}}(s^{\prime}\,|\,s,a)=\begin{cases} \widehat{P}_{h}(s^{\prime}\,|\,s,a),&\text{if }s^{\prime}\in\mathcal{S}\\ 1-\sum_{s^{\prime}\in\mathcal{S}}\widehat{P}_{h}(s^{\prime}\,|\,s,a),&\text{if }s^{ \prime}=s_{\mathsf{aug}}\end{cases}\quad\text{for all }(s,a,h)\in\mathcal{S}\times\mathcal{A} \times[H];\] (41a) \[\widehat{P}_{h}^{\mathsf{aug}}(s^{\prime}\,|\,s_{\mathsf{aug}},a)= \mathds{1}(s^{\prime}=s_{\mathsf{aug}}) \text{for all }(a,h)\in\mathcal{A}\times[H].\] (41b) Let \(\pi^{(k)}\) be the corresponding optimal deterministic policy of \(\pi^{(k),\mathsf{aug}}\) in the original state space.
5 Update // Frank-Wolfe update \[\mu^{(k+1)}=(1-\alpha)\mu^{(k)}+\alpha\,\mathds{1}_{\pi^{(k)}},\quad\text{ where}\quad\alpha=\frac{S}{(K^{\mathsf{on}}H)^{3}}.\]
6 Output: the policy mixture \(\mu^{t+1}=\mu^{(k)}\). ```

**Algorithm 2**Subroutine for solving the sub-problem (20b).

## Appendix D Useful algorithmic subroutines from prior works

In this section, we provide precise descriptions of several useful algorithmic subroutines that have been developed in recent works. The algorithm procedures are directly quoted from these prior works, with slight modification.

### Subroutine: occupancy estimation for any policy \(\pi\)

The first subroutine we'd like to describe is concerned with estimating the occupancy distribution \(d^{\pi}\) induced by any policy \(\pi\), based on a carefully designed exploration strategy. This algorithm, proposed by Li et al. (2023), seeks to estimate \(\{d_{h}^{\pi}\}\) step by step (i.e., from \(h=1,\ldots,H\)). For each \(h\), it computes an appropriate exploration policy \(\pi^{\text{explore},h}\) to adequately explore what happens between step \(h\) and step \(h+1\), and then collect \(N\) sample trajectories using \(\pi^{\text{explore},h}\). These turns allow us to estimate the occupancy distribution \(d_{h+1}^{\pi}\) for step \(h+1\). See Algorithm 3 for a precise description.

```
1Input: target success probability \(1-\delta\), threshold \(\xi=c_{\xi}H^{3}S^{3}A^{3}\log(HSA/\delta)\). /* Estimate occupancy distributions for step 1. */ Draw \(N\) independent episodes (using arbitrary policies), whose initial states are i.i.d. drawn from \(s_{1}^{n,0}\stackrel{{\text{i.i.d.}}}{{\sim}}\rho\;(1\leq n\leq N)\). Define the following functions \[\widehat{d_{1}^{\pi}}(s)=\frac{1}{N}\sum_{n=1}^{N}\mathds{1}\{s_{1}^{n,0}=s\}, \qquad\widehat{d_{1}^{\pi}}(s,a)=\widehat{d_{1}^{\pi}}(s)\pi_{1}(a\,|\,s)\] (42) for any deterministic policy \(\pi:\mathcal{S}\times[H]\rightarrow\Delta(\mathcal{A})\) and any \((s,a)\in\mathcal{S}\times\mathcal{A}\). (Note that these functions are defined for future use and not computed for the moment, as we have not specified policy \(\pi\).) /* Estimate occupancy distributions for steps \(2,\ldots,H\). */
3for\(h=1\)to\(H-1\)do /* Collect \(N\) sample trajectories using a suitable exploration policy. */
4 Call Algorithm 4 to compute an exploration policy \(\pi^{\text{explore},h}\) and compute an estimate \(\widehat{P}_{h}\) of the true transition kernel \(P_{h}\). /* Specify how to compute \(\widehat{d_{h+1}^{\pi}}\) for any policy \(\pi\). */
5 For any deterministic policy \(\pi:\mathcal{S}\times[H]\rightarrow\Delta(\mathcal{A})\) and any \((s,a)\in\mathcal{S}\times\mathcal{A}\), define \[\widehat{d_{h+1}^{\pi}}(s)=\big{\langle}\widehat{P}_{h}(s\,|\,\cdot,\cdot), \,\widehat{d_{h}^{\pi}}(\cdot,\cdot)\big{\rangle},\qquad\widehat{d_{h+1}^{ \pi}}(s,a)=\widehat{d_{h+1}^{\pi}}(s)\pi_{h+1}(a\,|\,s).\] (43)

**Algorithm 3**Subroutine for estimating occupancy distributions for any policy \(\pi\)(Li et al., 2023).

We note, however, that Algorithm 3 requires another subroutine to compute a suitable exploration policy \(\pi^{\text{explore},h}\). As it turns out, this can be accomplished by approximately solving the following problem

\[\widehat{\mu}^{h}\approx\arg\max_{\mu\in\Delta(\Pi)}\sum_{(s,a)\in\mathcal{S }\times\mathcal{A}}\log\left[\frac{1}{KH}+\operatorname*{\mathbb{E}}_{\pi \sim\mu}\big{[}\widehat{d_{h}^{\pi}}(s,a)\big{]}\right]\] (44)

via the Frank-Wolfe algorithm and returning \(\pi^{\text{explore},h}=\operatorname*{\mathbb{E}}_{\pi\sim\widehat{\mu}^{h}}[\pi]\). See Algorithm 4 for details.

### Subroutine: reward-agnostic online exploration

Based on the estimated occupancy distributions specified in Algorithm 3, Li et al. (2023) proposed a reward-independent online exploration scheme that proves useful in exploring an unknown environment. In a nutshell, this scheme computes a desired exploration policy by approximately solving the following optimization sub-problem:

\[\mu^{\text{explore}}\approx\arg\max_{\mu\in\Delta(\Pi)}\left\{\sum_{h=1}^{H} \sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\log\left[\frac{1}{K^{\text{on}}H }+\operatorname*{\mathbb{E}}_{\pi\sim\mu}\big{[}\widehat{d_{h}^{\pi}}(s,a) \big{]}\right]\right\}.\] (47)

again using the Frank-Wolfe algorithm; the resulting policy takes the form of a mixture of deterministic policies, as given by \(\pi^{\text{explore}}=\operatorname*{\mathbb{E}}_{\pi\sim\mu^{\text{explore}}}[\pi]\). This exploration policy is then employed to execute the MDP for a number of times in order to collect enough information about the unknowns. See Algorithm 5 for the whole procedure.

```
1Initialize:\(\mu^{(0)}=\mathds{1}_{\pi_{\text{\tiny{sub}}}}\) for an arbitrary policy \(\pi_{\text{init}}\in\Pi\), \(T_{\max}=\lfloor 50SA\log(KH)\rfloor\).
2for\(t=0\)to\(T_{\max}\)do /* find the optimal policy */
3 Compute the optimal deterministic policy \(\pi^{(t),\text{b}}\) of the augmented MDP \(\mathcal{M}_{\text{b}}^{h}=(\mathcal{S}\cup\{s_{\text{aug}}\},\mathcal{A},H, \widehat{P}^{\text{aug},h},r_{\text{b}}^{h})\), where \(s_{\text{aug}}\) is an augmented state, \[r_{\text{b},j}^{h}(s,a)=\begin{cases}\frac{1}{\overline{\kappa}^{\widehat{d}_{ H}}+\mathbb{E}_{\pi_{\sim\mu^{(t)}}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}},& \text{if }(s,a,j)\in\mathcal{S}\times\mathcal{A}\times\{h\},\\ 0,&\text{if }s=s_{\text{aug}}\text{ or }j\neq h,\end{cases}\] (45) and the augmented probability transition kernel is defined as \[\widehat{P}_{j}^{\text{aug},h}(s^{\prime}\,|\,s,a)=\begin{cases}\widehat{P}_ {j}(s^{\prime}\,|\,s,a),&\text{if }s^{\prime}\in\mathcal{S}\\ 1-\sum_{s^{\prime}\in\mathcal{S}}\widehat{P}_{j}(s^{\prime}\,|\,s,a),&\text{if }s^{ \prime}=s_{\text{aug}}\end{cases}\quad\text{for all }(s,a,j)\in\mathcal{S}\times\mathcal{A} \times[h];\] (46a) \[\widehat{P}_{j}^{\text{aug},h}(s^{\prime}\,|\,s,a)=\mathds{1}(s^{\prime}=s_{ \text{aug}})\qquad\qquad\qquad\qquad\qquad\qquad\text{if }s=s_{\text{aug}}\text{ or }j>h.\] (46b) Let \(\pi^{(t)}\) be the corresponding optimal deterministic policy of \(\pi^{(t),\text{b}}\) in the original state space.
4 Compute // choose the stepsize \[\alpha_{t}=\frac{\frac{1}{SA}g(\pi^{(t)},\widehat{d},\mu^{(t)})-1}{g(\pi^{(t) },\widehat{d},\mu^{(t)})-1},\quad\text{where}\quad g(\pi,\widehat{d},\mu)= \sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{\frac{1}{K^{\omega}H}+ \widehat{d}_{h}^{\pi}(s,a)}{\frac{1}{K^{\omega}H}+\mathbb{E}_{\pi\sim\mu}[ \widehat{d}_{h}^{\pi}(s,a)]}.\] Here, \(\widehat{d}_{h}^{\pi}(s,a)\) is computed via (42) for \(h=1\), and (43) for \(h\geq 2\).
5 If \(g(\pi^{(t)},\widehat{d},\mu^{(t)})\leq 2SA\) then exit for-loop. // stopping rule Update // Frank-Wolfe update \[\mu^{(t+1)}=(1-\alpha_{t})\,\mu^{(t)}+\alpha_{t}\,\mathds{1}_{\pi^{(t)}}\,.\]
6 Set \(\pi^{\text{explore},h}=\mathbb{E}_{\pi\sim\mu^{(t)}}[\pi]\) with \(\widehat{\mu}^{h}=\mu^{(t)}\). // The final exploration policy for step \(h\). /* Draw samples using \(\pi^{\text{explore},h}\) to estimate the transition kernel. */
7 Draw \(N\) independent trajectories \(\{s_{1}^{n,h},a_{1}^{n,h},\dots,s_{h+1}^{n,h}\}_{1\leq n\leq N}\) using policy \(\pi^{\text{explore},h}\) and compute \[\widehat{P}_{h}(s^{\prime}\,|\,s,a)=\frac{\mathds{1}(N_{h}(s,a)>\xi)}{\max\big{\{} N_{h}(s,a),\,1\big{\}}}\sum_{n=1}^{N}\mathds{1}(s_{h}^{n,h}=s,a_{h}^{n,h}=a,s_{h+1}^{n,h}=s^{\prime}),\qquad\forall(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A} \times\mathcal{S},\] where \(N_{h}(s,a)=\sum_{n=1}^{N}\mathds{1}\{s_{h}^{n,h}=s,a_{h}^{n,h}=a\}\).
8 Output: the exploration policy \(\pi^{\text{explore},h}\), the weight \(\widehat{\mu}^{h}\), and the estimated kernel \(\widehat{P}_{h}\). ```

**Algorithm 4**Subroutine for computing the exploration policy for step \(h\) in occupancy estimation (Li et al., 2023).

### Subroutine: pessimistic model-based offline RL

Given a historical dataset containing a collection of statistically independent sample trajectories, Li et al. (2022) came up with a model-based approach that enjoys provable minimax optimality. This approach first employs a two-fold subsampling trick in order to decouple the statistical dependency across different steps of a single trajectory. After this subsampling step, this approach resorts to the principle of pessimism in the face of uncertainty, which employs value iteration but penalizes the updates via proper variance-aware penalization (i.e., Bernstein-style lower confidence bounds). Details can be found in Algorithm 6.

```
1Initialize:\(\mu_{\text{b}}^{(0)}=\delta_{\pi_{\text{init}}}\) for an arbitrary policy \(\pi_{\text{init}}\in\Pi\), \(T_{\max}=\lfloor 50SAH\log(KH)\rfloor\).
2for\(t=0\)to\(T_{\max}\)do /* find the optimal policy */
3 Compute the optimal deterministic policy \(\pi^{(t),\text{b}}\) of the MDP \(\mathcal{M}_{\text{b}}=(\mathcal{S}\cup\{s_{\text{aug}}\},\mathcal{A},H, \widehat{P}^{\text{aug}},r_{\text{b}})\), where \(s_{\text{aug}}\) is an augmented state, \[r_{\text{b},h}(s,a)=\begin{cases}\frac{1}{K^{\text{aug}}H}+\mathbb{E}_{s\sim \mu_{\text{b}}^{(t)}}\big{[}\widehat{d}_{h}^{\prime}(s,a)\big{]}^{\prime},& \text{if }(s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H],\\ 0,&\text{if }(s,a,h)\in\{s_{\text{aug}}\}\times\mathcal{A}\times[H],\end{cases}\] (48) and the augmented probability transition kernel is given by \[\widehat{P}_{h}^{\text{aug}}(s^{\prime}\,|\,s,a)=\begin{cases}\widehat{P}_{h }(s^{\prime}\,|\,s,a),&\text{if }s^{\prime}\in\mathcal{S}\\ 1-\sum_{s^{\prime}\in\mathcal{S}}\widehat{P}_{h}(s^{\prime}\,|\,s,a),&\text{if }s^{ \prime}=s_{\text{aug}}\end{cases}\quad\text{for all }(s,a,h)\in\mathcal{S}\times \mathcal{A}\times[H];\] (49a) \[\widehat{P}_{h}^{\text{aug}}(s^{\prime}\,|\,s_{\text{aug}},a)= \mathds{1}(s^{\prime}=s_{\text{aug}}) \text{for all }(a,h)\in\mathcal{A}\times[H].\] (49b) Let \(\pi^{(t)}\) be the corresponding optimal deterministic policy of \(\pi^{(t),\text{b}}\) in the original state space.
4 Compute // choose the stepsize \[\alpha_{t}=\frac{\frac{1}{SAH}g(\pi^{(t)},\widehat{d},\mu_{\text{b}}^{(t)})-1 }{g(\pi^{(t)},\widehat{d},\mu_{\text{b}}^{(t)})-1},\quad\text{where}\quad g( \pi,\widehat{d},\mu)=\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{S}\times\mathcal{A} }\frac{\frac{1}{K^{\text{aug}}H}+\widehat{d}_{h}^{\prime}(s,a)}{\frac{1}{K^{ \text{aug}}H}+\mathbb{E}_{\pi\sim\mu}\big{[}\widehat{d}_{h}^{\prime}(s,a) \big{]}}.\] Here, \(\widehat{d}_{h}^{\prime}(s,a)\) is computed via (42) for \(h=1\), and (43) for \(h\geq 2\).
5 If \(g(\pi^{(t)},\widehat{d},\mu_{\text{b}}^{(t)})\leq 2HSA\) then exit for-loop. // stopping rule Update // Frank-Wolfe update \[\mu_{\text{b}}^{(t+1)}=(1-\alpha_{t})\,\mu_{\text{b}}^{(t)}+\alpha_{t}\, \mathds{1}_{\pi^{(t)}}\,.\]
6
7
8Output: the exploration policy \(\pi^{\text{explore}}=\mathbb{E}_{\pi\sim\mu_{\text{b}}^{(t)}}[\pi]\) and the associated weight \(\mu^{\text{explore}}=\mu_{\text{b}}^{(t)}\). ```

**Algorithm 5**Subroutine for computing the desired online exploration policy (Li et al., 2023).

```
1Input: a dataset \(\mathcal{D}\); reward function \(r\). Let \(K_{0}\) denote the number of sample trajectories in \(\mathcal{D}\).
2Subsampling: run the following procedure to generate the subsampled dataset \(\mathcal{D}^{\text{trim}}\).
3. _Data splitting._ Split \(\mathcal{D}\) into two halves: \(\mathcal{D}^{\text{main}}\) (which contains the first \(K_{0}/2\) trajectories), and \(\mathcal{D}^{\text{aux}}\) (which contains the remaining \(K_{0}/2\) trajectories); we let \(N_{h}^{\text{main}}(s)\) (resp. \(N_{h}^{\text{aux}}(s)\)) denote the number of sample transitions in \(\mathcal{D}^{\text{main}}\) (resp. \(\mathcal{D}^{\text{aux}}\)) that transition from state \(s\) at step \(h\).
4. _Lower bounding \(\{N_{h}^{\text{main}}(s)\}\) using \(\mathcal{D}^{\text{aux}}\)._ For each \(s\in\mathcal{S}\) and \(1\leq h\leq H\), compute \[N_{h}^{\text{trim}}(s)\coloneqq\max\left\{N_{h}^{\text{aux}}(s)-10\sqrt{N_{h}^ {\text{aux}}(s)\log\frac{HS}{\delta}},\,0\right\};\] (50)
5. _Random subsampling._ Let \(\mathcal{D}^{\text{main}^{\prime}}\) be the set of all sample transitions (i.e., the quadruples taking the form \((s,a,h,s^{\prime})\)) from \(\mathcal{D}^{\text{main}}\). Subsample \(\mathcal{D}^{\text{main}^{\prime}}\) to obtain \(\mathcal{D}^{\text{trim}}\), such that for each \((s,h)\in\mathcal{S}\times[H]\), \(\mathcal{D}^{\text{trim}}\) contains \(\min\{N_{h}^{\text{trim}}(s),N_{h}^{\text{main}}(s)\}\) sample transitions randomly drawn from \(\mathcal{D}^{\text{main}^{\prime}}\). (We shall also let \(N_{h}^{\text{trim}}(s,a)\) denote the number of samples that visits \((s,a,h)\) in \(\mathcal{D}^{\text{trim}}\).)
6. **Run VI-LCB:** set \(\mathcal{D}_{0}=\mathcal{D}^{\text{trim}}\); run Algorithm 7 to compute a policy \(\widehat{\pi}\). ```

**Algorithm 6**A pessimistic model-based offline RL algorithm (Li et al., 2022).

## Appendix E Analysis of Theorem 1

In this section, we present the proof for our main result in Theorem 1. Throughout the proof, we let \(\{\mathcal{G}_{h}\}_{1\leq h\leq H}\) denote a sequence of subsets obeying

\[\max_{1\leq h\leq H}\max_{(s,a)\in\mathcal{G}_{h}}\frac{d_{h}^{\pi^{*}}(s,a)}{d _{h}^{\mathsf{off}}(s,a)}=C^{*}(\sigma)\qquad\text{and}\qquad\frac{1}{H}\sum_ {h=1}^{H}\sum_{(s,a)\in\mathcal{G}_{h}}d_{h}^{\pi^{*}}(s,a)\leq\sigma,\] (52)

as motivated by Definition 2. As it turns out, if \(K^{\mathsf{on}}\geq c_{1}\frac{H^{3}SA}{\varepsilon^{2}}\log\frac{K}{\delta}\) for some large enough constant \(c_{1}>0\), then the claimed result in Theorem 1 follows immediately from the main theory in Li et al. (2023) developed for pure online exploration. As a result, it sufficies to prove the theorem by replacing Condition (24b) with

\[K^{\mathsf{on}}\geq c_{1}\frac{H^{4}SA\sigma}{\varepsilon^{2}}\log\frac{K}{\delta}\] (53)

throughout this section.

On a high level, our proof comprises the following three steps:

* Establish the proximity of \(\widehat{d}^{\mathsf{off}}\) (resp. \(\widehat{d}^{\pi}\)) and \(d^{\mathsf{off}}\) (resp. \(d^{\pi}\)).
* Show that the mixed policy \(\pi^{\mathsf{imitate}}\) is able to mimic and strengthen the offline dataset \(\mathcal{D}^{\mathsf{off}}\), while the mixed policy \(\pi^{\mathsf{explore}}\) is capable of exploring the part of the state-action space that has not been adequately visited by \(\mathcal{D}^{\mathsf{off}}\).
* Derive the sub-optimality of the policy returned by the offline RL algorithm (i.e., Algorithm 6) when applied to the hybrid dataset \(\mathcal{D}=\mathcal{D}^{\mathsf{off},2}\cup\mathcal{D}^{\mathsf{on}}_{ \mathsf{imitate}}\cup\mathcal{D}^{\mathsf{on}}_{\mathsf{explore}}\).

In the sequel, we shall elaborate on these three steps.

Step 1: establishing the proximity of \(\widehat{d}^{\pi}\) (resp. \(\widehat{d}^{\text{off}}\)) and \(d^{\pi}\) (resp. \(d^{\text{off}}\))

To begin with, the goodness of the occupancy distribution estimators \(\widehat{d}^{\pi}\) (cf. Algorithm 3) has been analyzed in Li et al. (2023, Lemma 4), which come with the following performance guarantees.

**Lemma 1** (Li et al. (2023)).: _Recall that \(\xi=c_{\xi}H^{3}S^{3}A^{3}\log\frac{HSA}{\delta}\) for some large enough constant \(c_{\xi}>0\). With probability at least \(1-\delta\), the estimated occupancy distributions specified in Algorithm 3 satisfy_

\[\frac{1}{2}\widehat{d}^{\pi}_{h}(s,a)-\frac{\xi}{4N}\leq d^{\pi}_{ h}(s,a)\leq 2\widehat{d}^{\pi}_{h}(s,a)+2e^{\pi}_{h}(s,a)+\frac{\xi}{4N}\] (54)

_simultaneously for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\) and all deterministic policy \(\pi\in\Pi\), provided that_

\[K^{\mathsf{on}}\geq C_{N}H^{18}S^{14}A^{14}\log^{2}\frac{HSA}{\delta}\] (55)

_for some large enough constant \(C_{N}>0\). Here, \(\{e^{\pi}_{h}(s,a)\}\) is some non-negative sequence satisfying_

\[\sum_{s,a}e^{\pi}_{h}(s,a)\leq\frac{2SA}{K^{\mathsf{on}}}+\frac{1 3SAH\xi}{N}\qquad\text{for all $h\in[H]$ and all deterministic Markov policy $\pi$}.\] (56)

We now turn to the estimator \(\widehat{d}^{\text{off}}\) (cf. (15)) for the occupancy distribution of the offline dataset, for which we begin with the following lemma concerning the proximity of \(d^{\text{off}}_{h}\) and \(\widehat{d}^{\text{off}}_{h}\). The proof of this lemma is deferred to Section G.1.

**Lemma 2**.: _Suppose that \(c_{\mathsf{off}}\geq 48\). With probability at least \(1-\delta/3\), one has_

\[\frac{1}{3}\widehat{d}^{\text{off}}_{h}(s,a)\leq d^{\text{off}}_{ h}(s,a)\leq\widehat{d}^{\text{off}}_{h}(s,a)+5c_{\mathsf{off}}\bigg{\{}\frac{ \log\frac{HSA}{\delta}}{K^{\mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{ HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\bigg{\}}\] (57)

_simultaneously for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\)._

This lemma implies that: when \(d^{\text{off}}_{h}(s,a)\lesssim\frac{\log\frac{HSA}{\delta}}{K^{\mathsf{off}}} +\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\), the estimator \(\widehat{d}^{\text{off}}_{h}(s,a)\) might be unable to track \(d^{\text{off}}_{h}(s,a)\) in a faithful manner. This motivates us to single out the following two subsets of state-action pairs for which \(\widehat{d}^{\text{off}}_{h}(s,a)\) might become problematic at step \(h\):

* the set \(\mathcal{G}^{\text{c}}_{h}\) (see (11) for the definition of \(\mathcal{G}_{h}\)), which corresponds to the set of optimal state-action pairs that even the true data distribution \(d^{\text{off}}_{h}\) cannot cover adequately;
* another set \(\mathcal{T}^{\text{small}}_{h}\) defined as \[\mathcal{T}^{\text{small}}_{h}\coloneqq\bigg{\{}(s,a):d^{\text{off}}_{h}(s,a) \leq 10c_{\mathsf{off}}\bigg{(}\frac{\log\frac{HSA}{\delta}}{K^{ \mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{\delta}}{N}+\frac{SA}{K^{ \mathsf{on}}}\bigg{)}\bigg{\}},\] (58) comprising those state-action pairs for which \(\widehat{d}^{\text{off}}_{h}(s,a)\) might not be a faithful estimator of \(d^{\text{off}}_{h}(s,a)\).

In what follow, we shall adopt the notation:

\[\mathcal{T}_{h}\coloneqq\mathcal{G}^{\text{c}}_{h}\cup\mathcal{T }^{\text{small}}_{h}.\] (59)

It is straightforward to demonstrate that:

* For any \((s,a)\notin\mathcal{T}^{\text{small}}_{h}\), it is seen from Lemma 2 that \[d^{\text{off}}_{h}(s,a)\leq\widehat{d}^{\text{off}}_{h}(s,a)+ \frac{1}{2}d^{\text{off}}_{h}(s,a)\qquad\Longleftrightarrow\qquad d^{\text{ off}}_{h}(s,a)\leq 2\widehat{d}^{\text{off}}_{h}(s,a).\] (60)
* For any \((s,a)\in\mathcal{G}_{h}\), Condition (52) tells us that \[d^{\pi^{*}}_{h}(s,a)\leq C^{*}(\sigma)d^{\text{off}}_{h}(s,a).\] (61)As a consequence, any \((s,a)\notin\mathcal{T}_{h}\) necessarily obeys

\[d_{h}^{\pi^{*}}(s,a)\leq C^{*}(\sigma)d_{h}^{\text{off}}(s,a)\leq 2C^{*}(\sigma) \widehat{d}_{h}^{\text{off}}(s,a).\] (62)

Another useful observation that we can readily make is as follows:

\[\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{T}_{h}}d_{h}^{\pi^{*}}(s,a) \leq\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{G}_{h}}d_{h}^{\pi^{*}}(s,a)+\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{G}_{h}\cap\mathcal{T}_{h}^{\text{rand} }}d_{h}^{\pi^{*}}(s,a)\] \[\leq H\sigma+\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{G}_{h}\cap \mathcal{T}_{h}^{\text{rand}}}d_{h}^{\pi^{*}}(s,a)\] \[\leq H\sigma+C^{*}(\sigma)\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{T }_{h}^{\text{rand}}}d_{h}^{\text{off}}(s,a)\mathds{1}\left(a=\pi^{*}(s)\right)\] \[\leq H\sigma+C^{*}(\sigma)HS\cdot 10c_{\text{off}}\Bigg{(}\frac{ \log\frac{HSA}{\delta}}{K^{\text{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{ HSA}{\delta}}{N}+\frac{SA}{K^{\text{on}}}\Bigg{)}\] \[\leq H\sigma+10c_{\text{off}}\Bigg{(}\frac{C^{*}(\sigma)HS\log \frac{ HSA}{\delta}}{K^{\text{off}}}+\frac{4C^{*}(\sigma)H^{6}S^{5}A^{4}\log\frac{ HSA}{\delta}}{K^{\text{on}}}\Bigg{)}=:\widehat{\sigma}.\] (63)

Here, the second and the third lines arise from Condition (52), the penultimate line invokes the definition (58) of \(\mathcal{T}_{h}^{\text{small}}\), whereas the last line is valid since \(N=K^{\text{on}}/(3H)\) (see (14)).

Step 2: showing that \(\pi^{\text{imitate}}\) (resp. \(\pi^{\text{explore}}\)) covers \(\widehat{d}^{\text{off}}\) (resp. \(d^{\pi^{*}}\)) adequately

In this step, we aim to demonstrate the quality of the two exploration policies \(\pi^{\text{imitate}}\) and \(\pi^{\text{explore}}\), designed for different purposes.

Goodness of \(\pi^{\text{imitate}}\).We begin by assessing the quality of the exploration policy \(\pi^{\text{imitate}}\). Towards this, we first make note of the following crude bound:

\[\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{K^{\text{on}}H}+\mathbb{E} _{\pi^{\prime}\sim\mu^{t}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}\leq\frac{ \widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{K^{\text{on}}H}}\leq K^{\text{on} }H=:L.\]

In view of the convergence guarantees for FTRL (Shalev-Shwartz, 2012, Corollary 2.16), we see that: if \(\eta=\sqrt{\frac{\log A}{2T_{\text{max}}L^{2}}}=\sqrt{\frac{\log A}{2T_{\text {max}}(K^{\text{on}}H)^{2}}}\), then running FTRL for \(T_{\text{max}}\) iterations results in

\[\max_{a\in\mathcal{A}}\frac{1}{T_{\text{max}}}\sum_{t=1}^{T_{ \text{max}}}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{K^{\text{on}}H} +\mathbb{E}_{\pi\sim\mu^{t}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}-\frac{1} {T_{\text{max}}}\sum_{t=1}^{T_{\text{max}}}\sum_{a\in\mathcal{A}}\pi_{h}^{t}(a \,|\,s)\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{K^{\text{on}}H}+ \mathbb{E}_{\pi\sim\mu^{t}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}\\ \leq K^{\text{on}}H\sqrt{\frac{2\log A}{T_{\text{max}}}}\] (64)

for all \(s\in\mathcal{S}\) and \(1\leq h\leq H\). Therefore, recalling that \(\mu^{\text{imitate}}=\frac{1}{T_{\text{max}}}\sum_{t=1}^{T_{\text{max}}}\mu^{t}\) and applying Jensen's inequality yield

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\max_{a\in\mathcal{A}}\frac{ \widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi \sim\mu^{\text{imitate}}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}\\ \leq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\max_{a\in\mathcal{A}} \frac{1}{T_{\text{max}}}\sum_{t=1}^{T_{\text{max}}}\frac{\widehat{d}_{h}^{ \text{off}}(s,a)}{\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi\sim\mu^{t}}\big{[} \widehat{d}_{h}^{\pi}(s,a)\big{]}}\\ \leq\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}} \frac{1}{T_{\text{max}}}\sum_{t=1}^{T_{\text{max}}}\pi_{h}^{t}(a\,|\,s)\frac{ \widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{K^{\text{on}}H}+\mathbb{E}_{\pi \sim\mu^{t}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}+K^{\text{on}}H^{2}S \sqrt{\frac{2\log A}{T_{\text{max}}}},\] (65)where the second inequality results from (64). In addition, it follows from the stopping rule (37) that

\[\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\pi_{h}^{\star}(a\mid s) \frac{\widehat{d}_{h}^{\mathsf{off}}(s,a)}{\frac{1}{K^{\mathsf{on}}H}+\mathbb{E }_{\pi\sim\mu^{\mathsf{online}}}[\widehat{d}_{h}^{\pi}(s,a)]}\leq 108SH.\] (66)

As a consequence, combining (65) and (66) yields

\[\sum_{h\in[H]}\sum_{s\in\mathcal{S}}\max_{a\in\mathcal{A}}\frac{\widehat{d}_{ h}^{\mathsf{off}}(s,a)}{\frac{1}{K^{\mathsf{on}}H}+\mathbb{E}_{\pi\sim\mu^{ \mathsf{online}}}[\widehat{d}_{h}^{\pi}(s,a)]}\leq 108SH+K^{\mathsf{on}}H^{2}S \sqrt{\frac{2\log A}{T_{\mathsf{max}}}}\leq 109SH,\] (67)

provided that \(T_{\mathsf{max}}\geq 2(K^{\mathsf{on}}H)^{2}\log A\). The fact that the left-hand side of (67) is well-controlled suggests that \(\pi^{\mathsf{initate}}\) is able to cover \(\widehat{d}^{\mathsf{off}}\) adequately, a crucial fact we shall rely on in the subsequent analysis.

Goodness of \(\pi^{\mathsf{explore}}\)Next, we turn attention to the other exploration policy \(\pi^{\mathsf{explore}}\), computed via Algorithm 5. The following performance guarantees have been established in Li et al. (2023, Section 3.2).

**Lemma 3**.: _The distribution \(\mu^{\mathsf{explore}}\in\Delta(\Pi)\) returned by Algorithm 5 satisfies_

\[\max_{\pi}\sum_{h=1}^{H}\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\frac{ \widehat{d}_{h}^{\star}(s,a)}{\frac{1}{K^{\mathsf{on}}H}+\mathbb{E}_{\pi^{ \prime}\sim\mu^{\mathsf{explore}}}[\widehat{d}_{h}^{\widehat{\pi}^{\prime}}(s,a)]}\leq 2HSA.\]

In light of the performance bound (17) for the subsequent offline RL approach, Lemma 3 suggests that \(\pi^{\mathsf{explore}}\) is able to explore well with regards to the visitation of any policy \(\pi\) -- including the optimal policy \(\pi^{\star}\).

### Step 3: establishing the performance of offline RL

Now, we can readily proceed to analyze the performance of the model-based offline procedure described in Algorithm 6. In this subsection, we abuse the notation \(\widehat{P}\) to represent the empirical transition kernel constructed within the offline subroutine in Algorithm 7. Additionally, we introduce a \(S\)-dimensional vector \(d_{h}^{\pi^{\star}}\coloneqq[d_{h}^{\pi}\left(s\right)]_{s\in\mathcal{S}}\).

#### e.3.1 Step 3.1: error decomposition

To begin with, we convert the sub-optimality gap of the policy estimate \(\widehat{\pi}\) into several terms that shall be controlled separately. The following two preliminary facts, which have been established in Li et al. (2022), prove useful for this purpose.

**Lemma 4**.: _With probability exceeding \(1-\delta/3\), one has_

\[N_{h}^{\mathsf{main}}(s,a)\geq N_{h}^{\mathsf{trim}}(s,a),\qquad\forall(s,a, h)\in\mathcal{S}\times\mathcal{A}\times[H]\]

_and_

\[\left\langle d_{j}^{\pi^{\star}},V_{j}^{\star}-V_{j}^{\widehat{\pi}}\right\rangle \leq 2\sum_{h:h\geq j}\sum_{s,a}d_{h}^{\pi^{\star}}(s,a)b_{h}(s,a), \qquad 1\leq h\leq H,\]

_where \(b_{h}(s,a)\) is defined in line 7 of Algorithm 7._

In view of Lemma 4, we can derive, for all \(j\in[H]\),

\[\left\langle d_{j}^{\pi^{\star}},V_{j}^{\star}-V_{j}^{\widehat{ \pi}}\right\rangle\leq 2\sum_{h:h\geq j}\sum_{s,a}d_{h}^{\pi^{\star}}(s,a)b_{h}(s,a )=2\sum_{h:h\geq j}\sum_{s}d_{h}^{\pi^{\star}}\big{(}s,\pi_{h}^{\star}(s) \big{)}b_{h}\big{(}s,\pi_{h}^{\star}(s)\big{)}\] \[\qquad\leq 2\sum_{h:h\geq j}\sum_{s:\,(s,\pi^{\star}(s))\notin \mathcal{T}_{h}}\sqrt{2d_{h}^{\pi^{\star}}\big{(}s,\pi_{h}^{\star}(s)\big{)}C^ {\star}(\sigma)\widehat{d}_{h}^{\mathsf{off}}\big{(}s,\pi_{h}^{\star}(s)\big{)} b_{h}\big{(}s,\pi_{h}^{\star}(s)\big{)}}+2\sum_{h:h\geq j}\sum_{(s,a)\in \mathcal{T}_{h}}d_{h}^{\pi^{\star}}(s,a)b_{h}(s,a)\] \[\qquad\leq 2\sum_{h:h\geq j}\sum_{s:\,(s,\pi^{\star}(s))\notin \mathcal{T}_{h}}\sqrt{2d_{h}^{\pi^{\star}}\big{(}s,\pi_{h}^{\star}(s)\big{)}C^ {\star}(\sigma)\widehat{d}_{h}^{\mathsf{off}}\big{(}s,\pi_{h}^{\star}(s)\big{)} }b_{h}\big{(}s,\pi_{h}^{\star}(s)\big{)}\]\[\sum_{h:h\geq j}\sum_{s}\max_{a:(s,a)\notin\mathcal{I}_{h}\cup \mathcal{T}_{h}}\sqrt{2d_{h}^{\pi^{*}}(s,a)C^{\star}(\sigma)\widehat{d}_{h}^{ \mathsf{eff}}(s,a)}\] \[\qquad\qquad\leq\sum_{h:h\geq j}\sum_{s}2\max_{a:(s,a)\notin \mathcal{I}_{h}}C^{\star}(\sigma)\widehat{d}_{h}^{\mathsf{eff}}(s,a)\]\[\leq 2C^{\star}(\sigma)\Big{(}\frac{1}{K^{\mathsf{on}}H}+\frac{\xi}{N} \Big{)}\sum_{h:h\geq j}\sum_{s}\max_{a}\frac{\widehat{d}_{h}^{\mathsf{off}}(s,a)} {\frac{1}{K^{\mathsf{on}}H}+\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[} \widehat{d}_{h}^{\mathsf{x}}(s,a)\big{]}}\] \[\leq 218HSC^{\star}(\sigma)\Big{(}\frac{\xi}{N}+\frac{1}{K^{ \mathsf{on}}H}\Big{)},\]

where the first inequality arises from (62), the penulminate line utilizes the definition (70) of \(\mathcal{I}_{h}\), and the last line comes from (67). This in turn allows us to upper bound \(\gamma_{1}\) as follows:

\[\gamma_{1}\] \[\qquad+\sum_{h:h\geq j}\sum_{s}2\max_{a:(s,a)\notin\mathcal{I}_{h} \cup\mathcal{I}_{h}}\sqrt{2{d_{h}^{\pi}}^{\ast}(s,a)C^{\star}(\sigma)\widehat{d }_{h}^{\mathsf{off}}(s,a)}\,.\,H\] \[\leq\sum_{h:h\geq j}\sum_{s}2\max_{a:(s,a)\in\mathcal{I}_{h}} \sqrt{2{d_{h}^{\pi}}^{\ast}(s,a)C^{\star}(\sigma)\widehat{d}_{h}^{\mathsf{ off}}(s,a)}\min\left\{\sqrt{\frac{2c_{\mathsf{b}}\log\frac{HK}{\delta}}{N_{h}^{ \mathsf{trim}}(s,a)}\mathsf{Var}_{P_{h}(\cdot|s,a)}\big{(}\widehat{V}_{h+1} \big{)}}+\frac{4c_{\mathsf{b}}H\log\frac{K}{\delta}}{N_{h}^{\mathsf{trim}}(s,a )},\,H\right\}\] \[\qquad+436H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi}{N}+\frac{1}{K^ {\mathsf{on}}H}\Big{)}\] (71)

where the last line makes use of the elementary fact that \(\min\big{\{}\frac{x}{y},\frac{u}{w}\big{\}}\leq\frac{x+u}{y+w}\) for any \(x,y,u,w>0\).

In addition, note that for any \(s\) obeying \(\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[}d_{h}^{\mathsf{x}}(s)\big{]} \geq\xi/N\), we have

\[\mathbb{E}\big{[}N_{h}^{\mathsf{aux}}(s)\big{]} =\frac{1}{4}K^{\mathsf{off}}d_{h}^{\mathsf{off}}(s)+\frac{1}{6}K^ {\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[}d_{h}^{\mathsf{ x}}(s)\big{]}+\frac{1}{6}K^{\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}} \big{[}d_{h}^{\mathsf{x}}(s)\big{]}\] \[\geq\frac{1}{6}K^{\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{ unstate}}}\big{[}d_{h}^{\mathsf{x}}(s)\big{]}\geq\frac{1}{6}K^{\mathsf{on}}\cdot \frac{\xi}{N}=\frac{1}{2}c_{\xi}H^{4}S^{3}A^{3}\log\frac{HSA}{\delta},\]

where the last line invokes the definition of \(\mathcal{I}_{h}\) and the choice \(NH=\frac{1}{3}K^{\mathsf{on}}\). It can then be straightforwardly justified using elementary concentration inequalities (see, e.g., Alon and Spencer (2016, Appendix A.1)) that: with probability exceeding \(1-\delta/10\),

\[N_{h}^{\mathsf{aux}}(s)\geq\frac{1}{2}\mathbb{E}\big{[}N_{h}^{\mathsf{aux}} (s)\big{]}\geq\frac{1}{4}c_{\xi}H^{4}S^{3}A^{3}\log\frac{HSA}{\delta}\]

holds simultaneously for all \((s,h)\in\mathcal{S}\times[H]\), and as a result,

\[N_{h}^{\mathsf{trim}}(s)\geq N_{h}^{\mathsf{aux}}(s)-10\sqrt{N_{h}^{\mathsf{ aux}}(s)\log\frac{HS}{\delta}}\geq\frac{1}{2}N_{h}^{\mathsf{aux}}(s)\geq \frac{1}{4}\mathbb{E}\big{[}N_{h}^{\mathsf{aux}}(s)\big{]}\geq\frac{1}{24}K^{ \mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[}d_{h}^{\mathsf{ x}}(s)\big{]}.\]

Moreover, for any \((s,a)\in\mathcal{I}_{h}\) (cf. (70)), one can invoke Lemma 2 to obtain

\[\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[}d_{h}^{\mathsf{x}}(s)\big{]} \geq\frac{1}{3}\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[}\widehat{d}_{h}^ {\mathsf{x}}(s)\big{]}\geq\frac{\xi}{3N}.\]

Applying the same concentration of measurement argument as above further reveals that:

\[N_{h}^{\mathsf{trim}}(s,a)\geq\frac{1}{24}K^{\mathsf{on}}\mathbb{E}_{\pi\sim \mu^{\mathsf{unstate}}}\big{[}d_{h}^{\mathsf{x}}(s,a)\big{]}\geq\frac{1}{72}K^ {\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{unstate}}}\big{[}\widehat{d}_{h}^ {\mathsf{x}}(s,a)\big{]}\]

any \((s,a)\in\mathcal{I}_{h}\). Substitution into (71) then gives

\[\gamma_{1}\leq 16c_{\mathsf{b}}\sum_{h:h\geq j}\sum_{s}\max_{a:(s,a)\in \mathcal{I}_{h}}\sqrt{2{d_{h}^{\pi}}^{\ast}(s,a)C^{\star}(\sigma)\widehat{d}_{h} ^{\mathsf{off}}(s,a)}\sqrt{\frac{\mathsf{Var}_{P_{h}(\cdot|s,a)}\big{(}\widehat{V} _{h+1}\big{)}+H}{1/H+\frac{1}{72}K^{\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{ \mathsf{unstate}}}\big{[}\widehat{d}_{h}^{\mathsf{x}}(s,a)\big{]}}\log^{2}\frac{ K}{\delta}}\]\[\gamma_{1}\lesssim \min\Bigg{\{}\sqrt{\frac{H^{3}SC^{\star}(\sigma)}{K^{\mathsf{on}}} \log^{2}\frac{K}{\delta}},\sqrt{\frac{H^{3}SC^{\star}(\sigma)}{K^{\mathsf{off}} }\log^{2}\frac{K}{\delta}}\Bigg{\}}+H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi}{N }+\frac{1}{\min\{K^{\mathsf{on}},K^{\mathsf{off}}\}H}\Big{)}\] \[\lesssim \sqrt{\frac{H^{3}SC^{\star}(\sigma)}{K^{\mathsf{on}}+K^{\mathsf{ off}}}\log^{2}\frac{K}{\delta}}+H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi}{N}+ \frac{1}{\min\{K^{\mathsf{on}},K^{\mathsf{off}}\}H}\Big{)}.\] (77)

#### e.3.3 Step 3.3: controlling \(\gamma_{2}\) in (69)

We now turn attention to the term \(\gamma_{2}\) on the right-hand side of (69). Akin to (72), we can deduce that

\[\gamma_{2}\leq 16c_{\mathsf{b}}H\sum_{h:h\geq j}\sum_{(s,a)\in\mathcal{T}_{h}} \widehat{d}_{h}^{\pi^{*}}(s,a)\sqrt{\frac{\log\frac{K}{\delta}}{1+\frac{1}{ \gamma_{2}}K^{\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{update}}}\big{[} \widehat{d}_{h}^{\pi}(s,a)\big{]}}}+436H^{2}SA\Big{(}\frac{\xi}{N}+\frac{1}{K^{ \mathsf{on}}H}\Big{)}.\] (78)The Cauchy-Schwarz inequality then tells us that

\[\sum_{h:h\geq j} \sum_{(s,a)\in\mathcal{T}_{h}}\widehat{d}_{h}^{\pi^{*}}(s,a)\sqrt{ \frac{1}{1+\frac{1}{72}K^{\mathsf{on}}\mathbb{E}_{\pi\sim\mu^{\mathsf{explore}}} \big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}}\] \[\leq\sqrt{\sum_{h:h\geq j}\sum_{(s,a)\in\mathcal{T}_{h}}\frac{ \widehat{d}_{h}^{\pi^{*}}(s,a)}{1+\frac{1}{72}K^{\mathsf{on}}\mathbb{E}_{\pi \sim\mu^{\mathsf{explore}}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}}\cdot\sqrt{ \sum_{h:h\geq j}\sum_{(s,a)\in\mathcal{T}_{h}}\widehat{d}_{h}^{\pi^{*}}(s,a)}\] \[\leq 6\sqrt{\frac{2HSA}{K^{\mathsf{on}}}}\cdot\sqrt{\sum_{h:h \geq j}\sum_{(s,a)\in\mathcal{T}_{h}}\widehat{d}_{h}^{\pi^{*}}(s,a)}\] \[\leq 6\sqrt{\frac{2HSA(2\widehat{\sigma}+HS\xi/N)}{K^{\mathsf{ on}}}},\]

where \(\widehat{\sigma}\) is defined in (63). Here, the penultimate line invokes Lemma 3, and the last line is valid since (according to Lemma 1 and (63))

\[\sum_{h:h\geq j}\sum_{(s,a)\in\mathcal{T}_{h}}\widehat{d}_{h}^{ \pi^{*}}(s,a) =\sum_{h:h\geq j}\sum_{s:(s,\pi^{*}(s))\in\mathcal{T}_{h}}\widehat {d}_{h}^{\pi^{*}}\big{(}s,\pi^{*}(s)\big{)}\] \[\leq 2\sum_{h:h\geq j}\sum_{s:(s,\pi^{*}(s))\in\mathcal{T}_{h}}d_ {h}^{\pi^{*}}\big{(}s,\pi^{*}(s)\big{)}+\frac{HS\xi}{N}\] \[\leq 2\widehat{\sigma}+\frac{HS\xi}{N}.\]

Substitution of the above inequality into (78) yields

\[\gamma_{2}\leq 96c_{\mathsf{b}}\sqrt{\frac{2H^{3}SA(2\widehat{\sigma}+ HS\xi/N)}{K^{\mathsf{on}}}\log\frac{HK}{\delta}}+436H^{2}SA\Big{(}\frac{\xi}{N}+ \frac{1}{K^{\mathsf{on}}H}\Big{)}.\] (79)

#### e.3.4 Step 3.4: putting all pieces together

To finish up, combining (69),(77) and (79) reveals that: with probability at least \(1-\delta\), one has

\[V_{1}^{\star}(\rho) -V^{\widehat{\sigma}}(\rho)=\big{\langle}d_{1}^{\pi^{*}},V_{1}^{ \star}-V^{\widehat{\sigma}}\big{\rangle}\] \[\lesssim\sqrt{\frac{H^{3}SC^{\star}(\sigma)\log^{2}\frac{K}{ \delta}}{K^{\mathsf{on}}+K^{\mathsf{off}}}}+\sqrt{\frac{H^{4}SA\sigma\log\frac {K}{\delta}}{K^{\mathsf{on}}}}+\sqrt{\frac{H^{4}S^{2}AC^{\star}(\sigma)\log^{2 }\frac{K}{\delta}}{K^{\mathsf{off}}K^{\mathsf{on}}}}\] \[\qquad+\sqrt{\frac{H^{8}S^{6}A^{5}C^{\star}(\sigma)\log^{2}\frac{ K}{\delta}}{NK^{\mathsf{on}}}}+\sqrt{\frac{H^{4}S^{3}A^{2}C^{\star}(\sigma) \log\frac{K}{\delta}}{KK^{\mathsf{on}}}}\] \[\qquad+\frac{H^{6}S^{4}A^{4}+H^{5}S^{4}A^{3}C^{\star}(\sigma)}{ N}\log\frac{K}{\delta}+\frac{H^{2}S(C^{\star}(\sigma)+A)}{\min\{K^{\mathsf{on}},K^{\mathsf{off}}\}}\] \[\lesssim\sqrt{\frac{H^{3}SC^{\star}(\sigma)\log^{2}\frac{K}{ \delta}}{K^{\mathsf{on}}+K^{\mathsf{off}}}}+\sqrt{\frac{H^{4}SA\sigma\log\frac {K}{\delta}}{K^{\mathsf{on}}}}+\frac{H^{6}S^{4}A^{4}+H^{5}S^{4}A^{3}C^{\star} (\sigma)}{K^{\mathsf{on}}}\log^{2}\frac{K}{\delta}+\frac{H^{2}S(C^{\star}( \sigma)+A)}{K^{\mathsf{off}}},\] (80)

where the last inequality holds true as long as \(\min\{K^{\mathsf{off}},K^{\mathsf{on}}\}\gtrsim HSA\). Taking the right-hand side of (80) to be no larger than \(\varepsilon\), we immediately establish Theorem 1 under the sample complexity assumption in this theorem.

Proof for the stopping criterion and the iteration complexity for solving (20b)

Feasibility of the stopping rule (37).We first demonstrate that the stopping rule (37) can be satisfied by some mixed policy, namely,

\[\min_{\mu\in\Delta(\Pi)}\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim \pi_{h}^{t+1}\cdot|s|}\bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{KH+ \mathbb{E}_{\pi\sim\mu}\big{[}\widehat{d}_{h}^{\tau}(s,a)\big{]}}\bigg{]}\leq 1 08SH.\] (81)

Towards this end, we focus attention on analyzing a specific choice of the mixed policy \(\mu^{\text{off}}\) -- the one that represents the mixed policy that generates the offline dataset. Making use of the definition (15) of \(\widehat{d}^{\text{off}}\) gives

\[\widehat{d}_{h}^{\text{off}}(s,a) =\frac{2N_{h}^{\text{off}}(s,a)}{K^{\text{off}}}\,\mathds{1} \left(\frac{N_{h}^{\text{off}}(s,a)}{K^{\text{off}}}\geq c_{\text{off}}\bigg{\{} \frac{\log\frac{HSA}{\delta}}{K^{\text{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{ HSA}{\delta}}{N}+\frac{SA}{K}\bigg{\}}\right)\] \[\leq 3d_{h}^{\text{off}}(s,a)\,\mathds{1}\left(\frac{3}{2}d_{h}^{ \text{off}}(s,a)\geq c_{\text{off}}\bigg{\{}\frac{\log\frac{HSA}{\delta}}{K^{ \text{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{\delta}}{N}+\frac{SA}{K} \bigg{\}}\right)\] \[\leq 3d_{h}^{\text{off}}(s,a)\,\mathds{1}\left(d_{h}^{\text{off}}(s,a)\geq\frac{2}{3}c_{\text{off}}\bigg{\{}\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{ \delta}}{N}+\frac{SA}{K}\bigg{\}}\right),\] (82)

where the second line relies on (94). This combined with Lemma 1 results in

\[\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\pi_{h}^{t}(a\,|\,s) \frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+\mathbb{E}_{\pi\sim\mu^ {\text{off}}}\big{[}\widehat{d}_{h}^{\tau}(s,a)\big{]}}\] \[\qquad\leq\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\pi_{h}^{t}( a\,|\,s)\frac{3d_{h}^{\text{off}}(s,a)\,\mathds{1}\left(d_{h}^{\text{off}}(s,a) \geq\frac{2}{3}c_{\text{off}}\big{\{}\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{ \delta}}{N}+\frac{SA}{K}\big{\}}\right)}{\frac{1}{KH}+\frac{1}{2}\mathbb{E}_{ \pi\sim\mu^{\text{off}}}\big{[}d_{h}^{\tau}(s,a)-2e_{h}^{\tau}(s,a)-\frac{ \xi}{4N}\big{]}}.\] (83)

Moreover, inequality (56) tells us that: when \(d_{h}^{\text{off}}(s,a)\geq\frac{2}{3}c_{\text{off}}\big{(}\frac{H^{4}S^{4}A^{ 4}\log\frac{HSA}{\delta}}{N}+\frac{SA}{K^{\text{on}}}\big{)}\) for some large enough constant \(c_{\text{off}}>0\), we have

\[\mathbb{E}_{\pi\sim\mu^{\text{off}}}\bigg{[}d_{h}^{\tau}(s,a)-2e_{h}^{\tau}(s, a)-\frac{\xi}{4N}\bigg{]}\geq\mathbb{E}_{\pi\sim\mu^{\text{off}}}\bigg{[}d_{h}^{ \tau}(s,a)-\frac{4SA}{K^{\text{on}}}-\frac{27c_{\xi}S^{4}A^{4}H^{4}\log\frac{ HSA}{\delta}}{N}\bigg{]}\geq\frac{1}{2}d_{h}^{\text{off}}(s,a).\] (84)

In turn, this implies that

\[\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\pi_{h}^{t}(a\,|\,s) \frac{3d_{h}^{\text{off}}(s,a)\,\mathds{1}\left(d_{h}^{\text{off}}(s,a)\geq \frac{2}{3}c_{\text{off}}\big{\{}\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{\delta}} {N}+\frac{SA}{K}\big{\}}\right)}{\frac{1}{KH}+\frac{1}{2}\mathbb{E}_{\pi\sim\mu ^{\text{off}}}\big{[}d_{h}^{\tau}(s,a)-2e_{h}^{\tau}(s,a)-\frac{\xi}{4N}\big{]}}\] \[\qquad\leq\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\pi_{h}^{t}( a\,|\,s)\frac{3d_{h}^{\text{off}}(s,a)\,\mathds{1}\left(d_{h}^{\text{off}}(s,a) \geq\frac{2}{3}c_{\text{off}}\big{\{}\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{ \delta}}{N}+\frac{SA}{K}\big{\}}\right)}{\frac{1}{KH}+\frac{1}{4}d_{h}^{\text{ off}}(s,a)}\] \[\qquad\leq\sum_{(s,a)\in\mathcal{S}\times\mathcal{A}}\pi_{h}^{t}( a\,|\,s)\frac{3d_{h}^{\text{off}}(s,a)}{\frac{1}{4}d_{h}^{\text{off}}(s,a)}=12S.\] (85)

Consequently, combine (83) and (85) to yield

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}\cdot|s|} \bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+\mathbb{E}_{\pi \sim\mu^{\text{off}}}\big{[}\widehat{d}_{h}^{\tau}(s,a)\big{]}}\bigg{]}\leq 1 2SH,\] (86)

which clearly validates the claim (81) (with an even better pre-constant).

Before moving forward, we single out one useful property that arises from the above arguments:

\[\mathbb{E}_{\pi\sim\mu^{\text{off}}}\big{[}\widehat{d}_{h}^{\tau}(s,a)\big{]} \geq\frac{1}{12}\widehat{d}_{h}^{\text{off}}(s,a).\] (87)

To prove the validity of this claim (87), it suffices to make the following two observations:* When \(d_{h}^{\text{off}}(s,a)\geq\frac{2}{3}\text{c}_{\text{off}}\big{(}\frac{H^{4}S^{4} A^{4}\log\frac{HS^{4}A}{N}}{N}+\frac{SA}{K^{\alpha}}\big{)}\), it has been shown in (83) and (84) in conjunction with (82) that \[\mathbb{E}_{\pi\sim\mu^{\text{off}}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]} \geq\frac{1}{4}d_{h}^{\text{off}}(s,a)\geq\frac{1}{12}\widehat{d}_{h}^{\text{ off}}(s,a).\] (88)
* When \(d_{h}^{\text{off}}(s,a)<\frac{2}{3}\text{c}_{\text{off}}\big{(}\frac{H^{4}S^{4 }A^{4}\log\frac{HSA}{N}}{N}+\frac{SA}{K^{\alpha}}\big{)}\), one sees from (82) that \(\widehat{d}_{h}^{\text{off}}(s,a)=0\), and hence (87) holds true trivially.

Iteration complexity.Suppose that the stopping criterion (37) is not yet met in the \(k\)-th iteration, namely,

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}(\cdot|s)} \Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+\mathbb{E}_{\pi^ {\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}} \Bigg{]}>108SH.\] (89)

It can be easily seen that

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1 }(\cdot|s)}\Bigg{[}\frac{\big{(}\frac{1}{KH}+\widehat{d}_{h}^{\pi^{(k)}}(s,a) \big{)}\widehat{d}_{h}^{\text{off}}(s,a)}{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi ^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)} ^{2}}\Bigg{]}\] \[\geq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{ t+1}(\cdot|s)}\Bigg{[}\frac{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi\sim\mu^{\text{ off}}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}\big{)}\widehat{d}_{h}^{\pi^{ \prime}}(s,a)\Big{]}}\Bigg{]}\] \[\geq 3\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{ t+1}(\cdot|s)}\Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a )\big{]}}\Bigg{]}\mathds{1}\left(\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{ \frac{1}{KH}+\mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi ^{\prime}}(s,a)\big{]}}>36\right)\] \[=3\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{ t+1}(\cdot|s)}\Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a )\big{]}}\Bigg{]}\] \[\geq 3\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{ t+1}(\cdot|s)}\Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a )\big{]}}\Bigg{]}-108SH\] \[\geq 2\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{ t+1}(\cdot|s)}\Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a )\big{]}}\Bigg{]},\] (90)

where the first inequality follows from the choice (32) of \(\pi^{(k)}\), the second inequality is a consequence of the relation (87), and the last line makes use of (89). This in turn allows one to demonstrate that

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}( \cdot|s)}\Bigg{[}\frac{\big{(}\widehat{d}_{h}^{\pi^{(k)}}(s,a)-\mathbb{E}_{\pi^ {\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)} \widehat{d}_{h}^{\text{off}}(s,a)}{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi^{\prime} \sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}^{2}} \Bigg{]}\] \[=\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}( \cdot|s)}\Bigg{[}\frac{\big{(}\frac{1}{KH}+\widehat{d}_{h}^{\pi^{(k)}}(s,a) \big{)}\widehat{d}_{h}^{\pi}(s,a)\big{]}}{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi^ {\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}^{2 }}\Bigg{]}-\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}( \cdot|s)}\bigg{[}\frac{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}} \big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}\widehat{d}_{h}^{\pi}(s,a )\big{]}}{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[} \widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}^{2}}\Bigg{]}\] \[\geq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{ t+1}(\cdot|s)}\Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+ \mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a) \big{]}}\Bigg{]}>108SH,\]

where the last line results from (90) and the condition (89). We can then readily derive

\[\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}(\cdot|s)} \Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+\mathbb{E}_{\pi^ {\prime}\sim\mu^{(k)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}} \Bigg{]}-\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}( \cdot|s)}\Bigg{[}\frac{\widehat{d}_{h}^{\text{off}}(s,a)}{\frac{1}{KH}+\mathbb{E}_{ \pi^{\prime}\sim\mu^{(k+1)}}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}} \Bigg{]}\]\[=\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}( \cdot|s)}\Bigg{[}\frac{\big{(}\mathbb{E}_{\pi^{\prime}\sim\mu^{(k+1)}}\big{[} \widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}-\mathbb{E}_{\pi^{\prime}\sim\mu^{(k) }}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}\big{)}\widehat{d}_{h}^{\mathsf{off}}( s,a)}{\big{(}\frac{1}{KH}+\mathbb{E}_{\pi^{\prime}\sim\mu^{(k)}}\big{[} \widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}\big{)}^{2}}\Bigg{]}\] \[\geq 108\alpha SH-\alpha^{2}(K^{3}H^{4})=\frac{108S^{2}}{K^{3}H^{2 }}-\frac{S^{2}}{K^{3}H^{2}}=\frac{107S^{2}}{K^{3}H^{2}},\] (91)

where the third line relies on the update rule (35), and the last line utilizes the choice (36) of \(\alpha\).

In summary, the above argument reveals that: before the stopping criterion is met, each iteration is able to make progress at least as large as in (91). Recognizing the crude bound

\[0\leq\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}(\cdot |s)}\bigg{[}\frac{\widehat{d}_{h}^{\mathsf{off}}(s,a)}{\frac{1}{KH}+\mathbb{E}_ {\pi^{\prime}\sim\mu}\big{[}\widehat{d}_{h}^{\pi^{\prime}}(s,a)\big{]}}\bigg{]} \leq KH\sum_{h=1}^{H}\sum_{s\in\mathcal{S}}\mathbb{E}_{a\sim\pi_{h}^{t+1}( \cdot|s)}\big{[}\widehat{d}_{h}^{\mathsf{off}}(s,a)\big{]}\leq KH^{2}\]

that holds for any \(\mu\in\Delta(\Pi)\), one can combine this with (91) to conclude that: the proposed procedure terminates within \(O\big{(}\frac{K^{4}H^{4}}{S^{2}}\big{)}\) iterations, as claimed.

## Appendix G Proofs of technical lemmas

### Proof of Lemma 2

The Bernstein inequality combined with the union bound tells us that, with probability at least \(1-\delta/3\),

\[\bigg{|}\frac{2N_{h}^{\mathsf{off}}(s,a)}{K^{\mathsf{off}}}-d_{h }^{\mathsf{off}}(s,a)\bigg{|} \leq 6\sqrt{\frac{d_{h}^{\mathsf{off}}(s,a)\log\frac{HSA}{ \delta}}{K^{\mathsf{off}}}}+\frac{6\log\frac{HSA}{\delta}}{K^{\mathsf{off}}}\] \[\leq\frac{d_{h}^{\mathsf{off}}(s,a)}{2}+\frac{24\log\frac{HSA}{ \delta}}{K^{\mathsf{off}}}\] (92)

holds simultaneously for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\), where the last line invokes the AM-GM inequality. This in turn reveals that

\[\frac{4N_{h}^{\mathsf{off}}(s,a)}{3K^{\mathsf{off}}}-\frac{16\log\frac{HSA}{ \delta}}{K^{\mathsf{off}}}\leq d_{h}^{\mathsf{off}}(s,a)\leq\frac{4N_{h}^{ \mathsf{off}}(s,a)}{K^{\mathsf{off}}}+\frac{48\log\frac{HSA}{\delta}}{K^{ \mathsf{off}}}.\] (93)

As a result, we can show that:

* If \(\frac{N_{h}^{\mathsf{off}}(s,a)}{K^{\mathsf{off}}}\geq c_{\mathsf{off}}\big{(} \frac{\log\frac{HSA}{\delta}}{K^{\mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log \frac{HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\big{)}\) for some \(c_{\mathsf{off}}\geq 48\), then one has \[\frac{2N_{h}^{\mathsf{off}}(s,a)}{3K^{\mathsf{off}}} \leq d_{h}^{\mathsf{off}}(s,a)\leq\frac{6N_{h}^{\mathsf{off}}(s, a)}{K^{\mathsf{off}}}\qquad\text{and}\qquad\widehat{d}_{h}^{\mathsf{off}}(s,a)=\frac{2N_{h}^{ \mathsf{off}}(s,a)}{K^{\mathsf{off}}}\] (94a) \[\implies\qquad\frac{1}{3}\widehat{d}_{h}^{\mathsf{off}}(s,a)\leq d _{h}^{\mathsf{off}}(s,a)\leq 3\widehat{d}_{h}^{\mathsf{off}}(s,a).\] (94b)
* If instead \(\frac{N_{h}^{\mathsf{off}}(s,a)}{K^{\mathsf{off}}}<c_{\mathsf{off}}\big{(} \frac{\log\frac{HSA}{\delta}}{K^{\mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log \frac{HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\big{)}\), then one has \(\widehat{d}_{h}^{\mathsf{off}}(s,a)=0\), and therefore, \[d_{h}^{\mathsf{off}}(s,a)\geq 0=\frac{1}{3}\widehat{d}_{h}^{\mathsf{off}}(s,a),\]\[d_{h}^{\mathsf{off}}(s,a) \leq\frac{4N_{\mathsf{R}}^{\mathsf{off}}(s,a)}{K^{\mathsf{off}}}+ \frac{48\log\frac{HSA}{\delta}}{K^{\mathsf{off}}}<5c_{\mathsf{off}}\bigg{\{} \frac{\log\frac{HSA}{\delta}}{K^{\mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log \frac{HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\bigg{\}}\] \[=\widehat{d}_{h}^{\mathsf{off}}(s,a)+5c_{\mathsf{off}}\bigg{\{} \frac{\log\frac{HSA}{\delta}}{K^{\mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log \frac{HSA}{\delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\bigg{\}}.\]

Taken collectively, these inequalities demonstrate that

\[\frac{1}{3}\widehat{d}_{h}^{\mathsf{off}}(s,a)\leq d_{h}^{\mathsf{off}}(s,a) \leq\widehat{d}_{h}^{\mathsf{off}}(s,a)+5c_{\mathsf{off}}\bigg{\{}\frac{\log \frac{HSA}{\delta}}{K^{\mathsf{off}}}+\frac{H^{4}S^{4}A^{4}\log\frac{HSA}{ \delta}}{N}+\frac{SA}{K^{\mathsf{on}}}\bigg{\}},\] (95)

provided that \(c_{\mathsf{off}}>0\) is sufficiently large.

### Proof of Lemma 6

Before embarking on the proof, let us introduce several notation. For each \(1\leq h\leq H\), define \(P_{h}^{\pi^{*}}\in\mathbb{R}^{S\times S}\) and \(d_{h}^{\pi^{*}}\in\mathbb{R}^{S}\) such that: for all \(s\in\mathcal{S}\),

\[P_{h}^{\pi^{*}}(s,\cdot)=P_{h}\big{(}\cdot|s,\pi^{*}(s)\big{)}\qquad\text{and} \qquad d_{h}^{\pi^{*}}(s)=d_{h}^{\pi^{*}}\big{(}s,\pi^{*}(s)\big{)}.\] (96)

To begin with, it can be easily seen from (69) and the basic fact \(\mathsf{Var}_{P_{h}(\cdot|s,a)}\big{(}\widehat{V}_{h+1}\big{)}\leq H^{2}\) that

\[\big{\langle}d_{j}^{\pi^{*}},V_{j}^{\star}-V_{j}^{\widehat{\pi}} \big{\rangle} \leq\underbrace{8H\sum_{h:h\geq j}\sum_{s}\max_{a:(s,a)\in\mathcal{ I}_{h}}\sqrt{2d_{h}^{\star}(s,a)C^{\star}(\sigma)\widehat{d}_{h}^{\mathsf{ off}}(s,a)}\sqrt{\frac{c_{\mathsf{b}}\log\frac{K}{\delta}}{N_{h}^{\mathsf{trim}}(s,a)+1}}}_{ \eqs\eqs\eqs\eqs}\] \[\quad+\underbrace{4H\sum_{h:h\geq j}\sum_{(s,a)\in\mathcal{T}_{h}} \widehat{d}_{h}^{\pi^{*}}(s,a)\sqrt{\frac{c_{\mathsf{b}}\log\frac{K}{\delta}}{N _{h}^{\mathsf{trim}}(s,a)+1}}}_{\eqs\eqs\eqs}+\frac{61c_{\xi}H^{6}S^{4}A^{4}}{ N}\log\frac{K}{\delta}\] (97)

holds for any \(j\in[H]\).Note that we have bounded \(\gamma_{2}\) in (79). We then need to bound \(\gamma_{3}\).

With regards to the term \(\gamma_{3}\): invoking similar arguments as for (72) leads to

\[\gamma_{3} \leq 64c_{\mathsf{b}}H\sum_{h:h\geq j}\sum_{s}\max_{a}\sqrt{2d_{h} ^{\pi^{*}}(s,a)C^{\star}(\sigma)\widehat{d}_{h}^{\mathsf{off}}(s,a)}\sqrt{ \frac{\log\frac{K}{\delta}}{1/H+\frac{1}{72}K^{\mathsf{on}}\mathbb{E}_{\pi\in \mu^{\mathsf{shots}}}\big{[}\widehat{d}_{h}^{\pi}(s,a)\big{]}}}+4H^{2}SC^{\star }(\sigma)\Big{(}\frac{\xi}{N}+\frac{1}{K^{\mathsf{on}}H}\Big{)}\] \[\leq 64c_{\mathsf{b}}H\sqrt{2\sum_{h:h\geq j}\sum_{s,a}d_{h}^{\pi ^{*}}(s,a)}\cdot\sqrt{\sum_{h:h\geq j}\sum_{s}\max_{a}\frac{C^{\star}(\sigma) \widehat{d}_{h}^{\mathsf{off}}(s,a)\log\frac{K}{\delta}}{1/H+\frac{1}{72}K^{ \mathsf{on}}\mathbb{E}_{\pi\in\mu^{\mathsf{shots}}}\big{[}\widehat{d}_{h}^{ \pi}(s,a)\big{]}}}+4H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi}{N}+\frac{1}{K^{ \mathsf{on}}H}\Big{)}\] \[\leq 768c_{\mathsf{b}}\sqrt{\frac{13H^{4}SC^{\star}(\sigma)\log \frac{K}{\delta}}{K^{\mathsf{on}}}}+4H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi} {N}+\frac{1}{K^{\mathsf{on}}H}\Big{)},\] (98)

where the second step invokes the Cauchy-Schwartz inequality, and the last line comes from (67).

Similarly, repeating the above argument but focusing on the offline dataset, we can derive (which we omit for the sake of brevity)

\[\gamma_{3} \leq 64c_{\mathsf{b}}H\sum_{h:h\geq j}\sum_{s}\max_{a}\sqrt{2d_{h}^{ \pi^{*}}(s,a)C^{\star}(\sigma)\widehat{d}_{h}^{\mathsf{off}}(s,a)}\sqrt{\frac{ \log\frac{K}{\delta}}{1/H+\frac{1}{72}K^{\mathsf{off}}d_{h}^{\mathsf{off}}(s,a) }}+4H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi}{N}+\frac{1}{K^{\mathsf{off}}H} \Big{)}\] \[\leq 64c_{\mathsf{b}}H\sqrt{2\sum_{h:h\geq j}\sum_{s,a}d_{h}^{\pi ^{*}}(s,a)}\cdot\sqrt{\sum_{h:h\geq j}\sum_{s}\max_{a}\frac{C^{\star}(\sigma) \widehat{d}_{h}^{\mathsf{off}}(s,a)\log\frac{K}{\delta}}{1/H+\frac{1}{72}K^{ \mathsf{off}}d_{h}^{\mathsf{off}}(s,a)}}+4H^{2}SC^{\star}(\sigma)\Big{(}\frac{ \xi}{N}+\frac{1}{K^{\mathsf{off}}H}\Big{)}\] \[\leq 768c_{\mathsf{b}}\sqrt{\frac{6H^{4}SC^{\star}(\sigma)\log \frac{K}{\delta}}{K^{\mathsf{off}}}}+4H^{2}SC^{\star}(\sigma)\Big{(}\frac{\xi} {N}+\frac{1}{K^{\mathsf{off}}H}\Big{)},\] (99)

where the last line makes use of (86).

Combining (98), (99) and (79) with (97), we can show that

\[\big{\langle}d_{j}^{\pi^{*}},V_{j}^{\star}-V_{j}^{\widehat{\pi}} \big{\rangle} \leq\min\left\{768\mathrm{c}_{\mathsf{b}}\sqrt{\frac{13H^{4}SC^{ \star}(\sigma)}{K^{\mathsf{on}}}\log\frac{K}{\delta}}+4H^{2}SC^{\star}(\sigma) \Big{(}\frac{\xi}{N}+\frac{1}{KH}\Big{)},\,768\mathrm{c}_{\mathsf{b}}\sqrt{ \frac{6H^{4}SC^{\star}(\sigma)}{K^{\mathsf{off}}}\log\frac{K}{\delta}}\right\}\] \[\qquad+96\mathrm{c}_{\mathsf{b}}\sqrt{\frac{2H^{3}SA(2\widehat{ \sigma}+HS\xi/N)}{K^{\mathsf{on}}}\log\frac{HK}{\delta}}+436H^{2}SA\Big{(} \frac{\xi}{N}+\frac{1}{\min\{K^{\mathsf{on}},K^{\mathsf{off}}\}H}\Big{)}\] \[\qquad+\frac{61\mathrm{c}_{\xi}H^{6}S^{4}A^{4}}{N}\log\frac{K}{\delta}\] \[\leq 1\] (100)

for all \(1\leq j\leq H\), with the proviso that

\[\frac{H^{7}S^{5}A^{4}}{K^{\mathsf{on}}}\log^{2}\frac{K}{\delta} \leq c_{10}\] \[\frac{H^{5}S^{4}A^{3}C^{\star}(\sigma)\log\frac{K}{\delta}}{K^{ \mathsf{on}}} \leq c_{10}\] \[\frac{HSC^{\star}(\sigma)\log\frac{K}{\delta}}{K^{\mathsf{off}}} \leq c_{10}\] \[\frac{HSA\log\frac{K}{\delta}}{K^{\mathsf{off}}} \leq c_{10}\]

for some sufficiently small constant \(c_{10}>0\). As a consequence, we can demonstrate that

\[\sum_{h:h\geq j} \sum_{s,a}d_{h}^{\pi^{*}}(s,a)\mathsf{Var}_{P_{h}(\cdot|s,a)}( \widehat{V}_{h+1})\] \[\leq\sum_{h:h\geq j}2\sum_{s,a}d_{h}^{\pi^{*}}(s,a)\Big{(} \mathsf{Var}_{P_{h}(\cdot|s,a)}(V_{h+1}^{\star})+\mathsf{Var}_{P_{h}(\cdot|s, a)}(V_{h+1}^{\star}-\widehat{V}_{h+1})\Big{)}\] \[\leq 4H^{2}+H\sum_{h:h\geq j}\sum_{s}d_{h}^{\pi^{*}}\big{(}s,\pi^{ \star}(s)\big{)}\mathbb{E}_{P_{h}(\cdot|s,\pi^{\star}(s))}\big{[}V_{h+1}^{ \star}-\widehat{V}_{h+1}\big{]}\] \[=4H^{2}+H\sum_{h:h\geq j}\big{(}d_{h}^{\pi^{*}}\big{)}^{\top}P_{h }^{\pi^{*}}\big{[}V_{h+1}^{\star}-\widehat{V}_{h+1}\big{]}\] \[=4H^{2}+H\sum_{h\geq j}\big{(}d_{h+1}^{\pi^{*}}\big{)}^{\top}\big{[} V_{h+1}^{\star}-\widehat{V}_{h+1}\big{]}\leq 5H^{2}\] (101)

for all \(1\leq j\leq H\). Here, the third line in (101) applies the following fact

\[\sum_{h}\sum_{s,a}d_{h}^{\pi^{*}}(s,a)\mathsf{Var}_{P_{h}(\cdot|s,a)}(V_{h+1}^{\star})\] \[\qquad=\sum_{h}\sum_{s}d_{h}^{\pi^{*}}\big{(}s,\pi^{\star}(s) \big{)}\Big{[}\big{\langle}P_{h}^{\pi^{*}}(s,\cdot),V_{h+1}^{\star}\circ V_{h+ 1}^{\star}\big{\rangle}-\big{(}\big{\langle}P_{h}^{\pi^{*}}(s,\cdot),V_{h+1}^{ \star}\big{\rangle}\big{)}^{2}\Big{]}\] \[\leq\sum_{h}\sum_{s}d_{h}^{\pi^{*}}\big{(}s,\pi^{\star}(s)\big{)} \Big{[}\big{\langle}P_{h}^{\pi^{*}}(s,\cdot),V_{h+1}^{\star}\circ V_{h+1}^{ \star}\big{\rangle}-\big{(}V_{h}^{\star}(s)\big{)}^{2}+2H\Big{]}\] \[=\sum_{h}\big{(}d_{h}^{\pi^{*}}\big{)}^{\top}P_{h}^{\pi^{*}} \big{(}V_{h+1}^{\star}\circ V_{h+1}^{\star}\big{)}-\sum_{h}\big{(}d_{h}^{\pi^{ *}}\big{)}^{\top}\big{(}V_{h}^{\star}\circ V_{h}^{\star}\big{)}\Big{]}+2H^{2}\] \[=\sum_{h}\big{(}d_{h+1}^{\pi^{*}}\big{)}^{\top}\big{(}V_{h+1}^{ \star}\circ V_{h+1}^{\star}\big{)}-\sum_{h}\big{(}d_{h}^{\pi^{*}}\big{)}^{\top} \big{(}V_{h}^{\star}\circ V_{h}^{\star}\big{)}+2H^{2}\] \[\leq 2H^{2},\]

where the second identity comes from the Bellman equation; the third relation uses the fact that \(V_{h}^{\star}(s)\leq H\), and the penultimate line holds since \(\big{(}d_{h}^{\pi^{*}}\big{)}^{\top}P_{h}^{\pi^{*}}=\big{(}d_{h+1}^{\pi^{*}} \big{)}^{\top}\); the penultimate step in (101) is due to (100); and the line in (101) holds true since \(\big{(}d_{h}^{\pi^{*}}\big{)}^{\top}P_{h}^{\pi^{*}}=\big{(}d_{h+1}^{\pi^{*}} \big{)}^{\top}\).