# Expressive Sign Equivariant Networks

for Spectral Geometric Learning

 Derek Lim

MIT CSAIL

dereklim@mit.edu

&Joshua Robinson

Stanford University

&Stefanie Jegelka

TU Munich, MIT CSAIL

&Haggai Maron

TU Munich, MIT CSAIL

Technion, NVIDIA

Work completed whilst at MIT.

###### Abstract

Recent work has shown the utility of developing machine learning models that respect the structure and symmetries of eigenvectors. These works promote sign invariance, since for any eigenvector \(v\) the negation \(-v\) is also an eigenvector. However, we show that sign invariance is theoretically limited for tasks such as building orthogonally equivariant models and learning node positional encodings for link prediction in graphs. In this work, we demonstrate the benefits of sign _equivariance_ for these tasks. To obtain these benefits, we develop novel sign equivariant neural network architectures. Our models are based on a new analytic characterization of sign equivariant polynomials and thus inherit provable expressiveness properties. Controlled synthetic experiments show that our networks can achieve the theoretically predicted benefits of sign equivariant models.

## 1 Introduction

The need to process eigenvectors is ubiquitous in machine learning and the computational sciences. For instance, there is often a need to process eigenvectors of operators associated with manifolds or graphs (Belkin and Niyogi, 2003; Rustamov et al., 2007), principal components (PCA) of arbitrary datasets (Pearson, 1901), and eigenvectors arising from implicit or explicit matrix factorization methods (Levy and Goldberg, 2014; Qiu et al., 2018). However, eigenvectors are not merely unstructured data--they have rich structure in the form of symmetries (Ovsjanikov et al., 2008).

Specifically, eigenvectors have sign and basis symmetries. An eigenvector \(v\) is sign symmetric in the sense that the sign-flipped vector \(-v\) is also an eigenvector of the same eigenvalue. Basis symmetries occur when there is a repeated eigenvalue, as then there are infinitely many choices of eigenvector basis for the same eigenspace. Prior work has developed neural networks that are invariant to these symmetries, improving empirical performance in several settings (Lim et al., 2023).

The goal of this paper is to demonstrate why sign _equivariance_ can be useful and to characterize fundamental expressive sign equivariant architectures. Our first contribution is to show that sign equivariant models are a natural choice for several applications, whereas sign _invariant_ architectures are provably insufficient for these applications. First, we show that sign and basis invariant networks are theoretically limited in expressive power for learning edge representations (and more generally multi-node representations) in graphs because they learn structural node embeddings that are known to be limited for link prediction and multi-node tasks (Srinivasan and

Figure 1: Illustration of a sign equivariant function \(f\). When column 1 of the input is negated, column 1 of the output is also negated.

Ribeiro, 2019; Zhang et al., 2021). In contrast, we show that sign equivariant models can bypass this limitation by maintaining positional information in node embeddings. Furthermore, we show that sign equivariance combined with PCA can be used to parameterize expressive orthogonally equivariant point cloud models, thus giving an efficient alternative to PCA-based frame averaging (Puny et al., 2022; Atzmon et al., 2022). In contrast, sign invariant models can only parameterize orthogonally _invariant_ models in this framework, which excludes many important application areas.

The second contribution of this work is to develop the first sign equivariant neural network architectures, with provable expressiveness guarantees. We first present a difficulty in developing sign equivariant models: the "Geometric Deep Learning Blueprint" (Bronstein et al., 2021) suggests developing an equivariant neural network by interleaving equivariant _linear_ maps and equivariant elementwise nonlinearities (Cohen and Welling, 2016; Ravanbakhsh et al., 2017; Finzi et al., 2021), but we show that our attempts to apply this approach are insufficient for expressive sign equivariant models. Namely, we show that sign equivariant linear maps between various input and output representations are very limited in their expressive power.

Hence, to develop our models, we derive a complete characterization of the sign equivariant polynomial functions. The form of these equivariant polynomials directly inspires our equivariant neural network architectures. Further, our architectures inherit the theoretical expressive power guarantees of the equivariant polynomials. Our characterization is also broadly useful for analysis and development of sign-symmetry-respecting architectures--for instance, we provide a new proof of the universality of SignNet (Lim et al., 2023) by showing that it can approximate all sign invariant polynomials.

To validate our theoretical results, we conduct various numerical experiments on synthetic datasets. Experiments in link prediction, n-body problems, and node clustering in graphs support our theory and demonstrate the utility of sign equivariant models.

### Background

Let \(f:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n\times k}\) be a function that takes eigenvectors \(v_{1},\ldots,v_{k}\in\mathbb{R}^{n}\) of an underlying matrix as input, and outputs representations \(f(v_{1},\ldots,v_{k})\). We often concatenate the eigenvectors into a matrix \(V=[v_{1},\ldots,v_{k}]\in\mathbb{R}^{n\times k}\), and write \(f(V)\) as the application of \(f\). For simplicity, in this work we assume the eigenvectors come from a symmetric matrix, so they are taken to be orthonormal.

**Sign and basis symmetries.** Eigenvectors have symmetries, because there are many possible choices of eigenvectors of a matrix. For instance, if \(v\) is a unit-norm eigenvector of a matrix, then so is the sign-flipped \(-v\). If the eigenvalue of \(v\) is simple, then \(-v\) is the only other choice of unit-norm eigenvector of this eigenvalue.

If \(v_{1},\ldots v_{m}\) are an orthonormal basis of eigenvectors for the same eigenspace (meaning they all have the same eigenvalue), then there are infinitely many other choices of orthonormal basis for this eigenspace; these other choices of basis can be written as \(VQ\), where \(V=[v_{1}\ldots v_{m}]\in\mathbb{R}^{n\times m}\) and \(Q\in O(m)\) is an arbitrary orthogonal matrix.

We refer to these symmetries collectively as sign and basis symmetries, or more simply as eigenvector symmetries. Note that sign symmetries are a special case of basis symmetries, as \(-1\) and \(1\) are the only orthogonal \(1\times 1\) matrices. Previous work has developed neural networks that are invariant to these symmetries--that is, networks that have the same output for any choice of sign or basis of the eigenvector inputs (Lim et al., 2023).

**Sign equivariance** means that if we flip the sign of an eigenvector, then the corresponding column of the output of a function \(f\) has its sign flipped. In other words, for all choices of signs \(s_{1},\ldots,s_{k}\in\{-1,1\}^{k}\),

\[f(s_{1}v_{1},\ldots,s_{k}v_{k})_{:,j}=s_{j}f(v_{1},\ldots,v_{k})_{:,j},\] (1)

where \(A_{:,j}\) is the \(j\)-th column of an \(n\times k\) matrix \(A\). See Figure 1 for an illustration. In matrix form, letting \(\operatorname{diag}(\{-1,1\}^{k})\) represent all \(k\times k\) diagonal matrices with \(-1\) or \(1\) on the diagonal, \(f\) is sign equivariant if

\[f(VS)=f(V)S\qquad\text{for all }S\in\operatorname{diag}(\{-1,1\}^{k}).\] (2)

As \(O(1)=\{-1,1\}\), we can write sign equivariance as equivariance with respect to a direct product of orthogonal groups \(O(1)\times\ldots\times O(1)\). This is different from the equivariance to a single orthogonal group \(O(d)\) considered in works on Euclidean-group equivariant networks (Thomas et al., 2018).

**Permutation equivariance** is often also a desirable property of our functions \(f\). We say that \(f\) is _permutation equivariant_ if \(f(PV)=Pf(V)\) for all \(n\times n\) permutation matrices \(P\). For instance, eigenvectors of matrices associated with simple graphs of size \(n\) have such permutation symmetries, as the ordering of nodes is arbitrary.

## 2 Applications of Sign Equivariance

In this section, we present several applications for which modeling networks with sign equivariant architectures is beneficial. We identify that sign invariant networks are _provably_ insufficient for these tasks, motivating the development of sign equivariant networks to address these limitations.

### Multi-Node Representations and Link Prediction

In several settings, we desire a machine learning model that computes representations for tuples of several nodes in a graph. For instance, link prediction tasks generate probabilities for pairs of nodes, and both hyperedge prediction and subgraph prediction tasks learn representations for collections of nodes in a graph (Alsentzer et al., 2020; Wang et al., 2023). For ease of exposition, the rest of this section discusses link prediction, though the discussion applies to general multi-node tasks as well.

In link prediction, we typically want to learn _structural node-pair representations_, meaning adjacency-permutation equivariant functions that give a representation for each pair of nodes; more precisely, a structural node-pair representation is a map \(f:\mathbb{R}^{n\times n}\rightarrow\mathbb{R}^{n\times n}\) such that \(f(PAP^{\top})=Pf(A)P^{\top}\), where \(f(A)_{i,j}\) is the representation of the pair of nodes \((i,j)\) in the graph with adjacency matrix \(A\)(Srinivasan and Ribeiro, 2019) (see Appendix A.5 for more discussion). One method to do this is to use a graph model such as a standard GNN to learn node representations \(z_{i}\), and then obtain a node-pair representation for \((i,j)\) as some function \(f_{\mathrm{decode}}(z_{i},z_{j})\) of \(z_{i}\) and \(z_{j}\). However, this approach is limited because standard GNNs learn _structural node encodings_--that is, adjacency-permutation equivariant node features \(f_{\mathrm{node}}:\mathbb{R}^{n\times n}\rightarrow\mathbb{R}^{n}\) such that \(f_{\mathrm{node}}(PAP^{\top})=Pf_{\mathrm{node}}(A)\)(Srinivasan and Ribeiro, 2019; Zhang et al., 2021). 2 Structural node encodings give automorphic nodes the same representation, which can be problematic since automorphic nodes can be far apart in the graph. For instance, in Figure 2, \(u_{1}\) and \(u_{2}\) are automorphic, so a link predictor based on structural node encodings would give both \(u_{1}\) and \(u_{2}\) the same probability of connecting to \(w\), but one would expect \(u_{1}\) to have a higher probability of connecting to \(w\). Most state-of-the-art link prediction methods on the Open Graph Benchmark leaderboards (Hu et al., 2020) were deliberately developed to avoid the issues of structural node encodings.

Footnote 2: This adjacency permutation equivariance is different from permutation equivariance of general vectors as defined in Section 1.1.

One way to surpass the limitations of structural node encodings is to use _positional node embeddings_, which can assign different values to automorphic nodes. Intuitively, positional encodings capture information such as distances between nodes and global position of nodes in the graph (see (Srinivasan and Ribeiro, 2019) for a formal definition). Laplacian eigenvectors are an important example of node

Figure 2: (a) First nontrivial normalized Laplacian eigenvector of a graph, which is positional. Nodes \(u_{1}\) and \(u_{2}\) are far apart in the graph, but automorphic. (b) Sign invariant node features, which are structural. Nodes \(u_{1}\) and \(u_{2}\) have the same feature. (c) Sign equivariant node features, which are positional. Nodes \(u_{1}\) and \(u_{2}\) have opposite signs. A link prediction model with sign invariant node features assigns \(u_{1}\) and \(u_{2}\) the same probability of connecting to \(w\), while sign equivariant node features could give higher probability to \(u_{1}\).

positional embeddings that capture much useful information of graphs (Chung, 1997; Von Luxburg, 2007). In Figure 2 (a), the first nontrivial Laplacian eigenvector captures cluster structure in the graph, and as such assigns \(u_{1}\) and \(u_{2}\) very different values.

Pitfalls of sign and basis invariance.When processing eigenvectors of matrices associated with graphs, invariance to the symmetries of the eigenvectors has been found useful (Dwivedi et al., 2022; Lim et al., 2023), especially for graph classification tasks. However, we show that exact invariance to these symmetries _removes positional information_, and thus the outputs of sign invariant or basis invariant networks are in fact _structural node encodings_ (see Appendix A.5).3 Hence, eigenvector-symmetry-invariant networks cannot learn node representations that distinguish automorphic nodes, and thus face the aforementioned difficulties when used for link prediction or multi-node tasks:

Footnote 3: When there are repeated eigenvalues, sign invariant embeddings maintain some positional information.

**Proposition 1**.: _Let \(f:\mathbb{R}^{n\times k}\to\mathbb{R}^{n\times d_{\mathrm{out}}}\) be a permutation equivariant function, and let \(V=[v_{1},\dots,v_{k}]\in\mathbb{R}^{n\times k}\) be \(k\) orthonormal eigenvectors of an adjacency matrix \(A\). Let nodes \(i\) and \(j\) be automorphic, and let \(z_{i}\) and \(z_{j}\in\mathbb{R}^{d_{\mathrm{out}}}\) be their embeddings, i.e, the \(i\)th and \(j\)th row of \(Z=f(V)\)._

* _If_ \(f\) _is sign invariant and the eigenvalues associated with the_ \(v_{l}\) _are simple and distinct, then_ \(z_{i}=z_{j}\)_._
* _If_ \(f\) _is basis invariant and_ \(v_{1},\dots,v_{k}\) _are a basis for some number of eigenspaces of_ \(A\) _then_ \(z_{i}=z_{j}\)_._

A novel link prediction approach via sign equivariance.The problem \(z_{i}=z_{j}\) arises from the sign/basis invariances, which remove crucial positional information. We instead propose using sign _equivariant_ networks (as in Section 3) to learn node representations \(z_{i}=f(V)_{i,:}\in\mathbb{R}^{k}\). These representations \(z_{i}\) maintain positional information for each node thanks to preserving sign information (see Figure 2 (c)). Then we use a sign invariant decoder \(f_{\mathrm{decode}}(z_{i},z_{j})=f_{\mathrm{decode}}(Sz_{i},Sz_{j})\) for \(S\in\mathrm{diag}(\{-1,1\}^{k})\) to obtain node-pair representations. For instance, the commonly used \(f_{\mathrm{decode}}=\mathrm{MLP}(z_{i}\odot z_{j})\), where \(\odot\) is the elementwise product, is sign invariant. When the eigenvalues are distinct, this approach has the desired invariances (yielding structural node-pair representations) and also maintains positional information in the node embeddings; see Appendix A.5 for a proof of the invariances, and Appendix A.5.1 for an example of where sign equivariant models can be used to compute strictly more expressive node-pair representations than sign invariant models. More details and the proof of Proposition 1 are in Appendix A.4.

Our sign equivariance based approach differs substantially from existing methods for learning structural pair representations without being bottlenecked by structural node representations. Many of these methods are based on labeling tricks (Zhang et al., 2021; Wang et al., 2023), whereby the representation for a node-pair is obtained by labeling the two nodes in the pair and then processing an enclosing subgraph. Without special modifications (Zhu et al., 2021; Chamberlain et al., 2023), this requires a separate expensive subgraph extraction and forward pass for each node-pair. In contrast, our method only requires one forward pass on the original graph to compute all positional node embeddings, after which pair representations can be obtained with a cheap, parallelizable decoding.

### Orthogonal Equivariance

For various applications in modelling physical systems, we desire equivariance to rigid transformations; thus, orthogonally equivariant models have been a fruitful research direction in recent years (Thomas et al., 2018; Weiler et al., 2018; Anderson et al., 2019; Deng et al., 2021). We say that a function \(f:\mathbb{R}^{n\times k}\to\mathbb{R}^{n\times k}\) is orthogonally equivariant if \(f(XQ)=f(X)Q\) for any \(Q\in O(k)\), where \(O(k)\) is the set of orthogonal matrices in \(\mathbb{R}^{k\times k}\). Orthogonal equivariance imposes infinitely many constraints on the function \(f\). Several works have approached this problem by reducing to a finite set of constraints using so-called Principal Component Analysis (PCA) based frames (Puny et al., 2022; Atzmon et al., 2022; Xiao et al., 2020).

PCA-frame methods take an input \(X\in\mathbb{R}^{n\times k}\), compute orthonormal eigenvectors \(R_{X}\in O(k)\) of the covariance matrix \(\mathrm{cov}(X)=(X-\frac{1}{n}\mathbf{1}\mathbf{1}^{\top}X)^{\top}(X-\frac{1} {n}\mathbf{1}\mathbf{1}^{\top}X)\) (assumed to have distinct eigenvalues), then average outputs of a base model \(h\) for each of the \(2^{k}\) sign-flipped inputs \(XR_{X}S\), where \(S\in\mathrm{diag}(\{-1,1\}^{k})\). We instead suggest using a sign equivariant network to parameterize an efficient \(O(k)\) equivariant model, which allows us to bypass the need to average the exponentially many sign-flipped inputs. For a sign equivariant network \(h\), we define our model \(f\) to be

\[f(X)=h(XR_{X})R_{X}^{\top}.\] (3)

See Figure 3 for an illustration. Intuitively, this first transforms \(X\) by \(R_{X}\) into a nearly canonical orientation that is unique up to sign flips; this can be seen as writing the points in the principal components basis, or aligning the principal components of \(X\) with the coordinate axes. Then we process \(XR_{X}\) using the model \(h\) that respects the sign symmetries, and finally, we incorporate orientation information back into the output by post-multiplying by \(R_{X}^{\top}\). Our approach only requires one forward pass through \(h\), whereas frame averaging requires \(2^{k}\) forward passes through a base model. The following proposition shows that \(f\) is \(O(k)\) equivariant, and inherits universality properties of \(h\).4

Footnote 4: We note that the \(f\) is \(O(k)\) equivariant, and the \(f\) is \(O(k)\) equivariant.

**Proposition 2**.: _Consider a domain \(\mathcal{X}\subseteq\mathbb{R}^{n\times k}\) such that each \(X\in\mathcal{X}\) has distinct covariance eigenvalues, and let \(R_{X}\) be a choice of orthonormal eigenvectors of \(\operatorname{cov}(X)\) for each \(X\in\mathcal{X}\). If \(h:\mathcal{X}\subseteq\mathbb{R}^{n\times k}\to\mathbb{R}^{n\times k}\) is sign equivariant, and if \(f(X)=h(XR_{X})R_{X}^{\top}\), then \(f\) is well defined and orthogonally equivariant._

_Moreover, if \(h\) is from a universal class of sign equivariant functions, then the \(f\) of the above form universally approximate \(O(k)\) equivariant functions on \(\mathcal{X}\)._

We include a proof of this result in Appendix A.6. This result also follows from Theorems 3.1 and 3.3 of Kaba et al. (2023), who show that generally we can canonicalize up to a subgroup \(K\) of a group \(G\), and achieve \(G\)-equivariance via a \(K\)-equivariant base predictor. In our case, \(G=O(k)\) and \(K=\{-1,1\}^{k}\)

Sign invariance only gives orthogonal invariance.In a similar way, a sign _invariant_ model can be used to obtain an orthogonally _invariant_ model with PCA frames, but it cannot be used for orthogonal equivariance; instead, sign equivariance is needed.

\begin{table}
\begin{tabular}{l c c} \hline \hline Constraints & Polynomials & Neural Networks \\ \hline \(\mathbb{R}^{k}\to\mathbb{R}\) inv. & \(\sum_{d_{1},\ldots,d_{k}=0}^{D}\textbf{W}_{d_{1},\ldots,d_{k}}v_{1}^{2d_{1}} \cdots v_{k}^{2d_{k}}\) & \(\operatorname{MLP}(|v|)\) \\ \(\mathbb{R}^{n\times k}\to\mathbb{R}\) inv. & \(q[\{V_{1,j}\cdot V_{2,j}\}_{1\in[n],i_{2}\in[n],j\in[k]})\) & \(\operatorname{SignNet}(V)=\rho[\{\phi(v_{i})+\phi(-v_{i})\}_{i=1,\ldots,k})\) \\ \hline \(\mathbb{R}^{k}\to\mathbb{R}^{k}\) equiv. & \(v\circ p_{\operatorname{inv}}(v)\) & \(v\odot\operatorname{MLP}(|v|)\) \\ \(\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) equiv. & \(W^{(2)}\left(\left(W^{(1)}V\right)\odot p_{\operatorname{inv}}(V)\right)\) & \([W^{(1)}_{1}v_{1},\ldots,W^{(l)}_{k}v_{k}]\odot\operatorname{SignNet}_{t}(V)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Sign invariant or equivariant polynomials and corresponding neural network architectures for different input and output spaces. \(v\in\mathbb{R}^{k}\) or \(V\in\mathbb{R}^{n\times k}\) are inputs to the polynomials or networks. Appendix C contains more details on the polynomials.

Figure 3: Using sign equivariant functions \(h\) to parameterize orthogonally equivariant \(f(X)=h(XR_{X})R_{X}^{\top}\), where \(R_{X}\) is a choice of principal components for the point cloud. We first transform \(X\) via \(R_{X}\) into an orientation that is unique up to sign flips, then process \(XR_{X}\) using the sign equivariant model \(h\), and finally reintegrate orientation information back into the output via \(R_{X}^{\top}\).

## 3 Sign Equivariant Polynomials and Networks

In this section, we analytically characterize the sign equivariant polynomials, and use this characterization to develop sign equivariant architectures. As equivariant polynomials universally approximate continuous equivariant functions (Yarotsky, 2022), our architectures inherit universality guarantees. We summarize our results on polynomials and neural network architectures in Table 1. Our characterization of the invariant polynomials also allows us to give an alternative proof of the universality of the sign invariant neural network SignNet (Lim et al., 2023) (see Appendix C.4).

### Sign Equivariant Linear Maps

First, we consider the important case of degree one polynomials, i.e. sign equivariant linear maps from \(\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\). These maps are very limited in expressive power, as they act independently on each eigenvector.

**Lemma 1**.: _A linear map \(W:\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) is sign equivariant if and only if it can be written as_

\[W(X)=[W_{1}X_{1}\;\ldots\;W_{k}X_{k}]\] (4)

_for some linear maps \(W_{1},\ldots,W_{k}:\mathbb{R}^{n}\to\mathbb{R}^{n^{\prime}}\), where \(X_{i}\in\mathbb{R}^{n}\) is the \(i\)th column of \(X\in\mathbb{R}^{n\times k}\)._

See Appendix B.1 for the proof. Notably, when \(n=n^{\prime}=1\), the linear maps are diagonal matrices.

This means a model with elementwise nonlinearities and sign equivariant linear maps will not capture any _interactions_ between eigenvectors. For instance, when used for parameterizing orthogonally equivariant models as in Section 2.2, such a model would process each principal component direction of the point cloud independently. Hence, the popular approach --outlined in the "Geometric Deep Learning Blueprint" (Bronstein et al., 2021)--of interleaving equivariant linear maps and equivariant nonlinearities (Cohen and Welling, 2016; Zaheer et al., 2017; Kondor and Trivedi, 2018; Maron et al., 2018, 2019; Finzi et al., 2021) is not as fruitful here.

However, one may choose instead different group representations for the input and output space, but our attempts to do this do not lead to efficient models. For instance, a common method to improve expressive power of models that use equivariant linear maps is to use tensor representations (Maron et al., 2018, 2019; Finzi et al., 2021); in our case, this would correspond having equivariant hidden representations in \(\mathbb{R}^{n\times k^{m}}\) for some tensor order \(m\). This is also inefficient, as we explain in Appendix B.1; we show that such an approach would have to lift to tensors of at least order 3, and that there are many sign equivariant linear maps between tensors of order 3. There is a possibility that some other group representations may allow the Geometric Deep Learning Blueprint to work better for sign equivariant networks, but we could not find any such representations.

For these reasons, we will now analyze the entire space of sign equivariant polynomials.

### Sign Equivariant Polynomials

Consider polynomials \(p:\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) that are sign equivariant, meaning \(p(VS)=p(V)S\) for \(S\in\operatorname{diag}(\{-1,1\}^{k})\). We can show that a polynomial \(p\) is sign equivariant if and only if it can be written as the elementwise product of a simple (linear) sign equivariant polynomial and a general sign invariant polynomial, followed by another linear sign equivariant map.

**Theorem 1**.: _A polynomial \(p:\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) is sign equivariant if and only if it can be written_

\[p(V)=W^{(2)}\left((W^{(1)}V)\odot p_{\mathrm{inv}}(V)\right)\] (5)

_for sign equivariant linear \(W^{(2)}\) and \(W^{(1)}\), and a sign invariant polynomial \(p_{\mathrm{inv}}:\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\)._

This reduction of sign equivariant polynomials to sign invariant polynomials combined with simple operations is convenient, as it enables us to leverage recent universal models for sign invariant functions (Lim et al., 2023). The proof of this statement is in Appendix C, which proceeds by showing that sign equivariance leads to linear constraints on the coefficients of a polynomial, which requires the polynomial to take the form stated in the Theorem.

### Sign Equivariance without Permutation Symmetries

Using Theorem 1, we can now develop sign equivariant architectures. We parameterize sign equivariant functions \(f:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\) as a composition of layers \(f_{l}\), each of the form

\[f_{l}(V)=[W_{1}^{(l)}v_{1},\dots,W_{k}^{(l)}v_{k}]\odot\mathrm{SignNet}_{l}(V),\] (6)

in which the \(W_{i}^{(l)}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n^{\prime}}\) are arbitrary linear maps, and \(\mathrm{SignNet}_{l}:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime} \times k}\) is sign invariant [10]. In the case of \(n=n^{\prime}=1\), there is a simple universal form: we can write a sign equivariant function \(f:\mathbb{R}^{k}\rightarrow\mathbb{R}^{k}\) as \(f(v)=v\odot\mathrm{MLP}(|v|)\), where \(|v|\) is the elementwise absolute value. These two architectures are universal because they can approximate sign equivariant polynomials. Here, the sign invariant part captures interactions between eigenvectors that the equivariant linear maps cannot.

**Proposition 3**.: _Functions of the form \(v\mapsto v\odot\mathrm{MLP}(|v|)\) universally approximate continuous sign equivariant functions \(f:\mathbb{R}^{k}\rightarrow\mathbb{R}^{k}\)._

_Compositions \(f_{2}\circ f_{1}\) of functions \(f_{l}\) as in equation 6 universally approximate continuous sign equivariant functions \(f:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\)._

### Sign Equivariance and Permutation Equivariance

For models on eigenvectors that stem from graphs or point clouds, in addition to sign equivariance, we may demand permutation equivariance, i.e., \(f(PV)=Pf(V)\) for all permutation matrices \(P\in\mathbb{R}^{n\times n}\). To add permutation equivariance to our neural network architecture from Section 3.3, we use it within the framework of DeepSets for Symmetric Elements (DSS) [13]. For a hidden dimension size of \(d_{f}\), each layer \(f_{l}:\mathbb{R}^{n\times k\times d_{f}}\rightarrow\mathbb{R}^{n\times k \times d_{f}}\) of our DSS-based sign equivariant network takes the following form on row \(i\):

\[f_{l}(V)_{i,:}=f_{l}^{(1)}\left(V_{i,:}\right)+f_{l}^{(2)}\Big{(}\sum_{j\neq i }V_{j,:}\Big{)},\] (7)

where \(f_{l}^{(1)}\) and \(f_{l}^{(2)}\) are sign equivariant functions as in Section 3.3. Sometimes we take \(d_{f}=1\), in which case we can use the simpler \(\mathbb{R}^{k}\rightarrow\mathbb{R}^{k}\) sign equivariant networks (\(v\odot\mathrm{MLP}(|v|)\)) as \(f_{l}^{(1)}\) and \(f_{l}^{(2)}\). If we have graph information, then we can do message-passing by changing the sum over \(j\neq i\) to a sum over a neighborhood of node \(i\). DSS has universal approximation guarantees [13], but they only apply for groups that act as permutation matrices, whereas the sign group \(\{-1,1\}^{k}\) does not. Hence, the universal approximation properties of our proposed DSS-based architecture are still an open question.

## 4 Experiments

Our theoretical results in Section 2 predict benefits of sign equivariance in various tasks: link prediction in nearly symmetric graphs, orthogonally equivariant simulations in n-body problems, and node clustering with positional information. Next, we probe these suggested benefits empirically.

### Link Prediction in Nearly Symmetric Graphs.

We begin with a synthetic link prediction task, which is carefully controlled to test the theoretically foreseen benefits of sign equivariance explained in Section 2.1. With the intuition of Figure 2 we first

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{Erdős-Rényi} & \multicolumn{2}{c}{Barabási-Albert} \\ \cline{2-5} Model & Test AUC & Runtime (s) & Test AUC & Runtime (s) \\ \hline GCN (constant input) &.497\(\pm.06\) &.058\(\pm.00\) &.705\(\pm.01\) &.048\(\pm.00\) \\ SignNet &.498\(\pm.00\) &.120\(\pm.00\) &.707\(\pm.00\) &.095\(\pm.00\) \\ \(V_{i}^{\top}V_{j,:}\) &.570\(\pm.01\) &.010\(\pm.01\) &.597\(\pm.01\) &.008\(\pm.00\) \\ \(\mathrm{MLP}(V_{i,:}\odot V_{j,:})\) &.614\(\pm.02\) &.050\(\pm.00\) &.651\(\pm.03\) &.040\(\pm.00\) \\ Sign Equivariant & **.751\(\pm.00\)** &.063\(\pm.00\) & **.773\(\pm.01\)** &.054\(\pm.00\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Link prediction AUC and runtime per epoch for structural edge models.

either generate an Erdos-Renyi (Erdos et al., 1960) or Barabasi-Albert (Barabasi and Albert, 1999) random graph \(H\) of 1000 nodes. Then we form a larger graph \(G\) that contains two disjoint copies of \(H\), along with 1000 uniformly-randomly added edges (both between and within copies of \(H\)). Without the random edges, each node in one copy of \(H\) is automorphic to the corresponding node in the other copy, so we expect many nodes to be nearly automorphic with the randomly added edges.

In Table 2, we show the link prediction performance of several models that learn structural edge representations. The methods that use eigenvectors have a sign invariant final prediction for each edge. GCN (Kipf and Welling, 2017) where the node features are all ones and SignNet (Lim et al., 2023) both completely fail on the Erdos-Renyi task (these two models map automorphic nodes to the same embedding), while our sign equivariant model outperforms all methods. We also try two eigenvector baselines that maintain node positional information, but do not update eigenvector representations: taking the dot product \(V_{i,:}^{\top}V_{j,:}\) to be the logit of a link existing, or learning a simple decoder \(\operatorname{MLP}(V_{i,:}\odot V_{j,:})\). Both perform substantially worse than our sign equivariant model, which shows that updating eigenvector representations is important here. Further, the sign equivariant model takes comparable runtime to GCN, and is significantly faster than SignNet. This is because we use networks of the form \(v\mapsto v\odot\operatorname{MLP}(|v|)\) in these experiments instead of the full SignNet-based model in equation 6. See Appendix E.2 for more details.

### Orthogonal Equivariance in n-body Problems

In this section, we empirically test the ability of our sign equivariant models to parameterize orthogonally equivariant functions on point clouds, as outlined in Section 2.2. For this purpose, we consider simulating n-body problems, following the setup in Fuchs et al. (2020) and building on the code from Puny et al. (2022). To test the favorable scaling of our method in the dimension \(d\) of the problem against the exponential \(2^{d}\) scaling of frame averaging, we generalize this problem to general dimensions \(d\geq 3\). We maintain the choice of \(n=5\) particles, and generate new point clouds using the same procedure as in Fuchs et al. (2020) (sampling random points and initial velocities in a general dimension \(d\)). We measure model performance via mean squared error (MSE). We use a DSS-based model that we describe in more detail in Appendix E.3.

Figure 4 illustrates the runtime and MSE. The sign equivariant model scales well with dimension--the time-per-epoch is nearly constant as we increase the dimension. In contrast, frame averaging suffers from the expected exponential slowdown with dimension, and runs out of memory on a 32GB V100 GPU for \(d=11\). Considering the MSE, the equivariant model's performance closely follows that of frame averaging, i.e., we only have a small loss in accuracy with much better scalability. For \(d=3\), the sign equivariant model has an MSE of.00646, compared to the.00575 of frame averaging (Puny et al., 2022). Additional \(d=3\) comparisons to other baselines are included in Appendix E.3.

### Node Clustering with Positional Information

As explained in Section 2.1, some applications on graph data call for positional node embeddings that can assign different representations to automorphic nodes. For instance, consider community detection or node clustering tasks on graphs, where a model makes a prediction for each node that

Figure 4: Sign equivariant model versus frame averaging model for n-body experiments in varying dimensions. Lower \(y\)-axis is better for both plots. (Left) The runtime of frame averaging increases exponentially in dimension while the sign equivariant runtime is approximately constant. Frame averaging runs out of memory on \(d=11\). (Right) The error of the sign equivariant model is very similar to that of frame averaging.

assigns it to a cluster. Structural encodings are insufficient for this task, as there may be automorphic or nearly-automorphic nodes that are far apart in the graph but look alike in a structural encoding. Hence, node structural encodings would guide the model to assign these nodes to the same cluster, even though they should belong to different clusters. As a concrete example, consider a graph of two clusters, and using Laplacian eigenvectors as positional encodings. The first nontrivial eigenvector will tend to assign a positive sign to one cluster and a negative sign to the other cluster. Thus, the sign information in the eigenvectors is crucial, so we expect sign equivariant models to perform well.

We test models on the CLUSTER dataset (Dwivedi et al., 2022) for semi-supervised node clustering (viewed as node classification) in synthetic graphs. In these experiments, we build on the empirically well-performing GraphGPS model (Rampasek et al., 2022), and incorporate our sign equivariant models to update eigenvector representations within the version of GraphGPS that uses PEG (Wang et al., 2022) to process positional encodings. See Appendix E.4 for more experimental details.

As seen in Table 3, our sign equivariant models outperform all of the other GraphGPS-based eigenvector methods. Moreover, we achieve the second best performance across all methods, showing that sign equivariant models can indeed achieve the theoretically expected benefits in this setting.

## 5 Related Work

Structural and Positional Representations.Especially for link prediction, the need for structural node-pair representations that are not obtained from structural node representations has been discussed in several works (Srinivasan and Ribeiro, 2019; Zhang et al., 2021; Cotta et al., 2023). As such, various methods have been developed for learning structural node-pair representations that incorporate node positional information. SEAL and other labeling-trick based methods (Zhang and Chen, 2018; Zhang et al., 2021) use added node features depending on the node-pair that we want a representation of. This is empirically successful in many tasks, but typically requires a separate subgraph extraction and forward pass through a GNN for each node-pair under consideration. Distance encoding (Li et al., 2020) uses relative distances between nodes to capture positional information. PEG (Wang et al., 2022) similarly maintains positional information by using eigenvector distances between nodes in each layer of a GNN, but does not update eigenvector representations. Identity-aware GNNs (You et al., 2021) and Neural Bellman-Ford Networks (Zhu et al., 2021) learn pair representations by conditioning on a source node from the pair.

Eigenvectors as Graph Positional Encodings.When using eigenvectors of graphs as node positional encodings for graph models like GNNs and Graph Transformers, many works have noted the need to address the sign ambiguity of the eigenvectors. This is often done by encouraging sign invariance through data augmentation--the signs of the eigenvectors are chosen randomly in each iteration of training (Dwivedi et al., 2022, 2022; He et al., 2022; Muller et al., 2023; Zhou et al., 2021). In contrast, SignNet (Lim et al., 2023) enforces exact sign invariance, by processing eigenvectors with a sign invariant neural architecture; this approach has been taken by some recent works (Rampasek et al., 2022; Geisler et al., 2023; Murphy et al., 2023).

Equivariant Neural Network Design.Equivariant neural network architectures have been proposed for various types of data and symmetry groups. A common paradigm is to interleave equivariant linear maps and equivariant pointwise nonlinearities (Wood and Shawe-Taylor, 1996; Cohen and Welling, 2016, 2017; Ravanbakhsh et al., 2017; Maron et al., 2018; Kondor and Trivedi, 2018; Finzi et al., 2021; Bronstein et al., 2021; Pearce-Crump, 2022); this is often used when the group acts as some subset of the permutation matrices. However, the sign group does not act as permutation matrices, and as we explained above this approach is not expressive for sign equivariant models.

\begin{table}
\begin{tabular}{l c} \hline \hline Model & Test Acc. (\%) \\ \hline GCN (Kipf and Welling, 2017) & \(68.498\pm 0.976\) \\ GIN (Xu et al., 2019) & \(64.716\pm 1.553\) \\ GAT (Velickovic et al., 2018) & \(70.587\pm 0.447\) \\ GatedCO (Bresson and Laurent, 2017) & \(73.840\pm 0.326\) \\ SAN (Kreuzer et al., 2021) & \(76.691\pm 0.650\) \\ K-Subgraph SAT (Chen et al., 2022) & \(\bm{77.856\pm 0.104}\) \\ EGT (Hussain et al., 2022) & \(\bm{79.232}\pm 0.348\) \\ GPS (Rampasek et al., 2022) & \(78.016\pm 0.180\) \\ \hline \multicolumn{2}{l}{_Eigenvector Method (GPS base model)_} \\ No PE & \(77.423\pm 0.241\) \\ LapPE (Dwivedi et al., 2022) & \(77.250\pm 0.280\) \\ PEG (Wang et al., 2022) & \(77.945\pm 0.310\) \\ SignNet (Lim et al., 2023) & \(77.442\pm 0.102\) \\ Sign Equivariant (ours) & \(\bm{78.201}\pm 0.118\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on the CLUSTER node classification task, for which positional information is needed. We compare different SOTA and Laplacian eigenvector-based methods.

More similarly to our approach, many equivariant machine learning works heavily leverage invariant or equivariant polynomials (or other equivariant nonlinear functions). These works include polynomials as operations within a network (Thomas et al., 2018; Puny et al., 2023), add polynomials as features (Yarotsky, 2022; Villar et al., 2021), build networks that take a similar form to equivariant polynomials (Villar et al., 2021), and/or analyze neural network expressive power by determining which equivariant polynomials a given architecture can compute (Zaheer et al., 2017; Segol and Lipman, 2019; Maron et al., 2019, 2020; Chen et al., 2020; Dym and Maron, 2021; Puny et al., 2023).

## 6 Conclusion

In this work, we identify and study an important method of respecting the symmetries of eigenvector data--sign equivariant models. For multi-node representation tasks, link prediction, and orthogonally equivariant tasks, sign equivariance provides a natural inductive bias; in contrast, we show that sign invariant models are provably limited in these tasks. To develop sign equivariant neural networks, we analytically characterize the sign equivariant polynomials, and then define neural networks that parameterize functions of similar form. Our neural networks are thus expressive, and inherit universal approximation guarantees of the equivariant polynomials. In several experiments, we show that our neural networks can indeed achieve the theoretically predicted benefits of sign equivariant models.

Limitations and Future Work.While we developed sign equivariant architectures in this work, we did not explore basis-change equivariant architectures, which would have the desired symmetries for inputs with repeated eigenvalues. As eigenvalue multiplicities are known to occur in many real-world graphs (Lim et al., 2023), future work in this area could be useful. Further, we give evidence that sign equivariance could help in some node-level and multi-node-level prediction tasks on graphs, but we do not have theoretical reason to believe that sign equivariance could help in graph-level representation tasks, which for instance are common in molecule processing. Our theoretical results are focused on expressive power, but we do not have results on other properties that are important for learning, such as optimization (Xu et al., 2021), stability (Wang et al., 2022; Huang et al., 2023), or generalization (Keriven and Vaiter, 2023). Finally, while we can prove universality of our models in the non-permutation-equivariant setting, we do not know of the exact expressive power in the permutation equivariant setting. Lim et al. (2023) also faces this issue for sign invariant models; future work on analyzing and possibly improving the expressive power of these models -- if they are not universal -- is promising.

#### Acknowledgments

We would like to thank Yaron Lipman for contributing significantly early on in this work. We would like to thank Maks Ovsjanikov for noting that basis invariant models give automorphic nodes the same representation, and also Johannes Lutzeyer and Michael Murphy for helpful comments. We would also like to thank the reviewers of the Physics4ML workshop at ICLR 2023 for helpful feedback and close reading. DL is supported by an NSF Graduate Fellowship. SJ acknowledges support from NSF AI Institute NSF CCF-2112665, NSF Award 2134108, and Office of Naval Research Grant N00014-20-1-2023 (MURI ML-SCOPE).

## References

* Abbe (2017) Emmanuel Abbe. Community detection and stochastic block models: recent developments. _The Journal of Machine Learning Research_, 18(1):6446-6531, 2017.
* Abboud et al. (2021) Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In _International Joint Conference on Artificial Intelligence_, 2021.
* Alsentzer et al. (2020) Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 8017-8029, 2020.
* Anderson et al. (2019) Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* Anderson et al. (2019)Matan Atzmon, Koki Nagano, Sanja Fidler, Sameh Khamis, and Yaron Lipman. Frame averaging for equivariant shape space learning. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 631-641, 2022.
* Barabasi and Albert (1999) Albert-Laszlo Barabasi and Reka Albert. Emergence of scaling in random networks. In _Science_, pages 509-512. American Association for the Advancement of Science, 1999.
* Belkin and Niyogi (2003) Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. _Neural computation_, 15(6):1373-1396, 2003.
* Bresson and Laurent (2017) Xavier Bresson and Thomas Laurent. Residual gated graph ConvNets. In _preprint arXiv:1711.07553_, 2017.
* Bronstein et al. (2021) Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovic. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. _preprint arXiv:2104.13478_, 2021.
* Chamberlain et al. (2023) Benjamin Paul Chamberlain, Sergey Shirobokov, Emanuele Rossi, Fabrizio Frasca, Thomas Markovich, Nils Hammerla, Michael M Bronstein, and Max Hansmire. Graph neural networks for link prediction with subgraph sketching. In _International Conference on Learning Representations (ICLR)_, 2023.
* Chen et al. (2022) Dexiong Chen, Leslie O'Bray, and Karsten Borgwardt. Structure-aware transformer for graph representation learning. In _International Conference on Machine Learning (ICML)_, pages 3469-3489. PMLR, 2022.
* Chen et al. (2020) Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 10383-10395, 2020.
* Chung (1997) Fan Chung. _Spectral graph theory_. American Mathematical Society, 1997.
* Cohen and Welling (2016) Taco Cohen and Max Welling. Group equivariant convolutional networks. In _International Conference on Machine Learning (ICML)_, pages 2990-2999. PMLR, 2016.
* Cohen and Welling (2017) Taco S. Cohen and Max Welling. Steerable CNNs. In _International Conference on Learning Representations (ICLR)_, 2017.
* Cotta et al. (2023) Leonardo Cotta, Beatrice Bevilacqua, Nesreen Ahmed, and Bruno Ribeiro. Causal lifting and link prediction. _preprint arXiv:2302.01198_, 2023.
* Deng et al. (2021) Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J Guibas. Vector neurons: A general framework for SO(3)-equivariant networks. In _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 12200-12209, 2021.
* Dwivedi et al. (2022a) Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. In _JMLR_, 2022a.
* Dwivedi et al. (2022b) Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In _International Conference on Learning Representations (ICLR)_, volume 10, 2022b.
* Dym and Gortler (2022) Nadav Dym and Steven J Gortler. Low dimensional invariant embeddings for universal geometric learning. _preprint arXiv:2205.02956_, 2022.
* Dym and Maron (2021) Nadav Dym and Haggai Maron. On the universality of rotation equivariant point cloud networks. In _International Conference on Learning Representations (ICLR)_, 2021.
* Erdos et al. (1960) Paul Erdos, Alfred Renyi, et al. On the evolution of random graphs. _Publ. Mathematical Institute Hungarian Academy of Science_, 5(1):17-60, 1960.
* Finzi et al. (2021) Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups. In _International Conference on Machine Learning (ICML)_, pages 3318-3328. PMLR, 2021.
* Fidler et al. (2018)Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3D rotation-translation equivariant attention networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 1970-1981, 2020.
* Fulton and Harris (2013) William Fulton and Joe Harris. _Representation theory: a first course_, volume 129. Springer Science & Business Media, 2013.
* Geisler et al. (2023) Simon Geisler, Yujia Li, Daniel Mankowitz, Ali Taylan Cemgil, Stephan Gunnemann, and Cosmin Paduraru. Transformers meet directed graphs. _preprint arXiv:2302.00049_, 2023.
* Gilmer et al. (2017) Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning (ICML)_, pages 1263-1272. PMLR, 2017.
* Hagberg et al. (2008) Aric Hagberg, Pieter Swart, and Daniel S Chult. Exploring network structure, dynamics, and function using networkx. Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM (United States), 2008.
* He et al. (2022) Xiaoxin He, Bryan Hooi, Thomas Laurent, Adam Perold, Yann LeCun, and Xavier Bresson. A generalization of ViT/MLP-Mixer to graphs. _preprint arXiv:2212.13350_, 2022.
* Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 22118-22133, 2020.
* Huang et al. (2023) Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the stability of expressive positional encodings for graph neural networks. _arXiv preprint arXiv:2310.02579_, 2023.
* Hussain et al. (2022) Md Shamim Hussain, Mohammed J Zaki, and Dharmashankar Subramanian. Global self-attention as a replacement for graph convolution. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 655-665, 2022.
* Kaba et al. (2023) Sekou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Bengio, and Siamak Ravanbakhsh. Equivariance with learned canonicalization functions. In _International Conference on Machine Learning (ICML)_, pages 15546-15566. PMLR, 2023.
* Keriven and Vaiter (2023) Nicolas Keriven and Samuel Vaiter. What functions can graph neural networks compute on random graphs? the role of positional encoding. _arXiv preprint arXiv:2305.14814_, 2023.
* Kim et al. (2022) Jinwoo Kim, Tien Dat Nguyen, Seonwoo Min, Sungjun Cho, Moontae Lee, Honglak Lee, and Seunghoon Hong. Pure transformers are powerful graph learners. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Kingma and Ba (2015) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* Kipf et al. (2018) Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In _International Conference on Machine Learning (ICML)_, pages 2688-2697. PMLR, 2018.
* Kipf and Welling (2017) Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _International Conference on Learning Representations (ICLR)_, volume 5, 2017.
* Kohler et al. (2020) Jonas Kohler, Leon Klein, and Frank Noe. Equivariant flows: exact likelihood generative learning for symmetric densities. In _International Conference on Machine Learning (ICML)_, pages 5361-5370. PMLR, 2020.
* Kondor and Trivedi (2018) Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural networks to the action of compact groups. In _International Conference on Machine Learning_, pages 2747-2755. PMLR, 2018.
* Kreuzer et al. (2021) Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, 2021.
* Krizhevsky et al. (2014)Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back-propagation network. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 2, 1989.
* Lee et al. (1992) Shiyi-Long Lee, Yeung-Long Luo, Bruce E Sagan, and Yeong-Nan Yeh. Eigenvector and eigenvalues of some special graphs. IV. Multilevel circulants. _International journal of quantum chemistry_, 41(1):105-116, 1992.
* Levy and Goldberg (2014) Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 27, 2014.
* Li et al. (2020) Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more powerful neural networks for graph representation learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 4465-4478, 2020.
* Lim et al. (2023) Derek Lim, Joshua David Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie Jegelka. Sign and basis invariant networks for spectral graph representation learning. In _International Conference on Learning Representations (ICLR)_, 2023.
* Maron et al. (2018) Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. In _International Conference on Learning Representations (ICLR)_, volume 6, 2018.
* Maron et al. (2019) Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks. In _International Conference on Machine Learning (ICML)_, pages 4363-4371. PMLR, 2019.
* Maron et al. (2020) Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements. In _International Conference on Machine Learning (ICML)_, pages 6734-6744. PMLR, 2020.
* Mialon et al. (2021) Gregoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. GraphiT: Encoding graph structure in transformers. In _preprint arXiv:2106.05667_, 2021.
* Muller et al. (2023) Luis Muller, Mikhail Galkin, Christopher Morris, and Ladislav Rampasek. Attending to graph transformers. _preprint arXiv:2302.04181_, 2023.
* Murphy et al. (2023) Michael Murphy, Stefanie Jegelka, Ernest Fraenkel, Tobias Kind, David Healey, and Thomas Butler. Efficiently predicting high resolution mass spectra with graph neural networks. _preprint arXiv:2301.11419_, 2023.
* Murphy et al. (2019) Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In _International Conference on Machine Learning (ICML)_, pages 4663-4673. PMLR, 2019.
* Ovsjanikov et al. (2008) Maks Ovsjanikov, Jian Sun, and Leonidas Guibas. Global intrinsic symmetries of shapes. In _Computer graphics forum_, volume 27, pages 1341-1348. Wiley Online Library, 2008.
* Pearce-Crump (2022) Edward Pearce-Crump. Brauer's group equivariant neural networks. _preprint arXiv:2212.08630_, 2022.
* Pearson (1901) Karl Pearson. LIII. On lines and planes of closest fit to systems of points in space. _The London, Edinburgh, and Dublin philosophical magazine and journal of science_, 2(11):559-572, 1901.
* Pimentel-Alarcon et al. (2016) Daniel L Pimentel-Alarcon, Nigel Boston, and Robert D Nowak. A characterization of deterministic sampling patterns for low-rank matrix completion. _IEEE Journal of Selected Topics in Signal Processing_, 10(4):623-636, 2016.
* Puny et al. (2022) Omri Puny, Matan Atzmon, Heli Ben-Hamu, Edward J Smith, Ishan Misra, Aditya Grover, and Yaron Lipman. Frame averaging for invariant and equivariant network design. In _International Conference on Learning Representations (ICLR)_, volume 10, 2022.
* Puny et al. (2023) Omri Puny, Derek Lim, Bobak T Kiani, Haggai Maron, and Yaron Lipman. Equivariant polynomials for graph neural networks. _preprint arXiv:2302.11556_, 2023.
* Puny et al. (2020)Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying DeepWalk, LINE, PTE, and node2vec. In _Proceedings of the eleventh ACM international conference on web search and data mining_, pages 459-467, 2018.
* Rampasek et al. (2022) Ladislav Rampasek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Ravanbakhsh et al. (2017) Siamak Ravanbakhsh, Jeff Schneider, and Barnabas Poczos. Equivariance through parameter-sharing. In _International Conference on Machine Learning (ICML)_, pages 2892-2901. PMLR, 2017.
* Rustamov et al. (2007) Raif M Rustamov et al. Laplace-Beltrami eigenfunctions for deformation invariant shape representation. In _Symposium on geometry processing_, volume 257, pages 225-233, 2007.
* Satorras et al. (2021) Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International Conference on Machine Learning (ICML)_, pages 9323-9332. PMLR, 2021.
* Segol and Lipman (2019) Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In _International Conference on Learning Representations (ICLR)_, volume 7, 2019.
* Srinivasan and Ribeiro (2019) Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddings and structural graph representations. In _International Conference on Learning Representations (ICLR)_, 2019.
* Thomas et al. (2018) Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds. In _NeurIPS Molecules and Materials Workshop_, 2018.
* Velickovic et al. (2018) Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _International Conference on Learning Representations (ICLR)_, volume 6, 2018.
* Villar et al. (2021) Soledad Villar, David Hogg, Kate Storey-Fisher, Weichi Yao, and Ben Blum-Smith. Scalars are universal: Equivariant machine learning, structured like classical physics. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, 2021.
* Un Luxburg (2007) Ulrike Von Luxburg. A tutorial on spectral clustering. _Statistics and computing_, 17(4):395-416, 2007.
* Wang et al. (2022) Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more powerful graph neural networks. In _International Conference on Learning Representations (ICLR)_, volume 10, 2022.
* Wang et al. (2023) Xiyuan Wang, Pan Li, and Muhan Zhang. Improving graph neural networks on multi-node tasks with labeling tricks. _preprint arXiv:2304.10074_, 2023.
* Weiler et al. (2018) Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3D steerable CNNs: Learning rotationally equivariant features in volumetric data. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 31, 2018.
* Wood and Shawe-Taylor (1996) Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks. _Discrete applied mathematics_, 69(1-2):33-60, 1996.
* Xiao et al. (2020) Zelin Xiao, Hongxin Lin, Renjie Li, Lishuai Geng, Hongyang Chao, and Shengyong Ding. Endowing deep 3D models with rotation invariance based on principal component analysis. In _IEEE International Conference on Multimedia and Expo (ICME)_, pages 1-6. IEEE, 2020.
* Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations (ICLR)_, volume 7, 2019.
* Xu et al. (2021) Keyulu Xu, Mozhi Zhang, Stefanie Jegelka, and Kenji Kawaguchi. Optimization of graph neural networks: Implicit acceleration by skip connections and more depth. In _International Conference on Machine Learning_, pages 11592-11602. PMLR, 2021.
* Xu et al. (2019)* Yarotsky (2022) Dmitry Yarotsky. Universal approximations of invariant maps by neural networks. _Constructive Approximation_, 55(1):407-474, 2022.
* You et al. (2021) Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural networks. In _Association for the Advancement of Artificial Intelligence (AAAI)_, volume 35, pages 10737-10745, 2021.
* Zaheer et al. (2017) Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. Deep Sets. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 30, pages 3391-3401, 2017.
* Zhang and Chen (2018) Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 31, 2018.
* Zhang et al. (2021) Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neural networks for multi-node representation learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 9061-9073, 2021.
* Zhu et al. (2021) Zhaocheng Zhu, Zuobai Zhang, Louis-Pascal Xhonneux, and Jian Tang. Neural bellman-ford networks: A general graph neural network framework for link prediction. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 29476-29490, 2021.

Applications of Sign Equivariance

### Improving Invariant Eigenvector Networks

Neural networks that are invariant to eigenvector symmetries have been shown to empirically improve graph learning models and achieve theoretically high expressive power. SignNet (Lim et al., 2023), a sign invariant neural network, takes the form

\[f(v_{1},\dots,v_{k})=\rho(\phi(v_{1})+\phi(-v_{1}),\dots,\phi(v_{k})+\phi(-v_{k}))\] (8)

for neural networks \(\rho\) and \(\phi\). This directly enforces invariant representations, without any intermediate equivariant representations. However, many successful invariant models first have many equivariant layers before a final invariant operation as equivariant layers are more expressive: this includes convolutional neural networks (LeCun et al., 1989), message passing graph neural networks (Gilmer et al., 2017), invariant graph networks (Maron et al., 2018), and group convolutional neural networks (Cohen and Welling, 2016). Thus, sign equivariant layers may lead to better sign invariant networks. Moreover, sign equivariant layers may improve on other aspects of SignNet, such as expressiveness of node features (Proposition 1) and efficiency (Appendix A.2)

### Efficiency Gains from Sign Equivariant Networks

Here, we show that our sign equivariant models can reduce the complexity of equivariant or invariant networks for two different types of applications. Throughout, we consider functions \(f:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n\times k}\), and we consider our permutation equivariant and sign equivariant DSS-based architecture from Section 3.4.

The time cost (in floating point operations) per layer of our DSS-based model is \(\mathcal{O}(n(kd+d^{2}))\), where \(d\) is the maximum hidden dimension of the MLP and we assume constant depth MLPs. To see this, note that we can precompute \(\sum_{j\neq i}^{n}V_{j,:}\), so that each \(\sum_{j\neq i}V_{j,:}\) can be computed in constant time by subtracting \(V_{i,:}\) from the total sum. Then for each of the \(n\) rows, the MLPs require \(\mathcal{O}(kd+d^{2})\) to evaluate matrix multiplications. In this process, we only form tensors of size \(\mathcal{O}(n(k+d))\), as the inputs and outputs are of size \(\mathcal{O}(nk)\), and the hidden layers of the MLPs form tensors of size \(\mathcal{O}(nd)\).

#### a.2.1 Efficient Orthogonally Equivariant Networks

Consider the case of \(O(k)\) equivariant models \(f:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n\times k}\) such that \(f(XQ)=f(X)Q\) for all orthogonal matrices \(Q\in O(k)\). There are many orthogonally equivariant neural architectures that are specialized to the special case of \(k=3\), which is very useful for applications in the physical sciences (Thomas et al., 2018; Fuchs et al., 2020). Here we consider models that directly work for general dimension \(k\).

Frame averaging approaches (Puny et al., 2022; Atzmon et al., 2022) require \(2^{k}\) forward passes of a base network \(f_{\theta}\), one for each sign flip of the principal components. Letting their base network be a permutation equivariant DeepSets (Zaheer et al., 2017), this means that they require \(\mathcal{O}(n(kd+d^{2})2^{k})\) time to evaluate their model, where \(d\) is the hidden dimension of the base model. Note that this has an extra exponential \(2^{k}\) factor compared to our \(\mathcal{O}(n(kd+d^{2}))\) cost.

Another general approach with universality guarantees comes from Villar et al. (2021), who analyze invariant polynomials to develop equivariant architectures. However, their method for \(O(k)\) invariance or equivariance requires forming \(XX^{\top}\), an \(n\times n\) matrix. Thus, the complexity is at least \(\mathcal{O}(n^{2})\), which is a problem in applications, since oftentimes \(n\) is much larger than \(k\). Variants of their method do not need to compute all \(\mathcal{O}(n^{2})\) inner products, but it is unclear how to maintain permutation equivariance when doing this.

#### a.2.2 Efficient Sign Invariant Networks

Consider again the form of SignNet (Lim et al., 2023), \(f(V)=\rho([\phi(v_{i})+\phi(-v_{i})]_{i=1,\dots,k})\). In the permutation equivariant version, e.g. when \(\phi\) is a DeepSets (Zaheer et al., 2017) or a message passing neural network (Gilmer et al., 2017), \(\phi\) maps from \(\mathbb{R}^{n}\rightarrow\mathbb{R}^{n\times d}\), where \(d\) is the hidden dimension. Thus, computing \(\phi(v_{i})+\phi(-v_{i})\) for all \(k\) vectors \(v_{i}\) require an \(\mathcal{O}(nkd)\) sized tensor to be formed (even if the output space of \(\phi\) is \(\mathbb{R}^{n}\), a vectorized implementation computes all \(\phi(v_{i})+\phi(-v_{i})\) in two batched inference calls to \(\phi\), which would require \(\mathcal{O}(nkd)\) sized intermediate tensors). This is a multiplicative factor larger than the sign equivariant requirement of \(\mathcal{O}(n(k+d))\) sized tensors. Moreover, it would take \(\mathcal{O}(nkd^{2})\) time to compute \(\phi(v_{i})+\phi(-v_{i})\) for each \(i\), which is a multiplicative factor larger than the \(\mathcal{O}(n(kd+d^{2}))\) time for the sign equivariant architecture.

### Potential Societal Impacts

We do not foresee direct societal impacts from our work. This project is primarily theoretical and aims to improve models for two general application areas: multi-node representation learning and orthogonal equivariant models. Potential societal impacts may arise in downstream applications that may be affected by general progress in geometric machine learning, such as social network analysis and recommender systems. These two applications are known to have negative societal impacts in certain circumstances, so care must be taken in future related work to avoid major negative consequences.

### Edge Representations and Link Prediction

#### a.4.1 Sign Invariant Link Prediction Decoders

Here, we present an ansatz for universal permutation invariant and sign invariant functions for \(n=2\), that is \(f:\mathbb{R}^{2\times k}\to\mathbb{R}^{d_{\mathrm{out}}}\). Note that SignNet is only known to be universal for such functions for \(n=1\), where there are no permutation symmetries (Lim et al., 2023).

We will parameterize such functions as

\[f(v_{1},\ldots v_{k})=\varphi\left(v_{1}\odot v_{1},v_{1}\odot \mathrm{rev}(v_{1}),\ldots,v_{k}\odot v_{k},v_{k}\odot\mathrm{rev}(v_{k}) \right).\] (9)

Here, \(\mathrm{rev}:\mathbb{R}^{2}\to\mathbb{R}^{2}\) reverses the vector, so \(\mathrm{rev}(a)_{1}=a_{2}\) and \(\mathrm{rev}(a)_{2}=a_{1}\). Moreover, \(\varphi:\mathbb{R}^{2\times 2k}\to\mathbb{R}^{d_{\mathrm{out}}}\) is a permutation invariant neural network, so \(\varphi(PX)=\varphi(X)\) for all \(2\times 2\) permutation matrices \(P\). Note that it is easy to parameterize permutation invariant functions \(\varphi\) in a maximally expressive way, e.g. via DeepSets (Zaheer et al., 2017). Now, we show that this parameterization is universal:

**Proposition 4**.: _Functions \(f:\mathbb{R}^{2\times k}\to\mathbb{R}^{d_{\mathrm{out}}}\) of the above form are permutation invariant and sign invariant, and they universally approximate permutation invariant and sign invariant functions._

Proof.: Invariance of \(f\) is easy to see; let \(P\) be a \(2\times 2\) permutation matrix and \(s_{i}\in\{-1,1\}\) for each \(i\). Then

\[f(Pv_{1}s_{1},\ldots,Pv_{k}s_{k}) =\varphi\left((Pv_{1}s_{1})\odot(Pv_{1}s_{1}),(Pv_{1}s_{1})\odot \mathrm{rev}(Pv_{1}s_{1}),\ldots\right)\] (10) \[=\varphi\left(P(v_{1}s_{1}\odot v_{1}s_{1}),P(v_{1}s_{1}\odot \mathrm{rev}(v_{1}s_{1})),\ldots\right)\] (11) \[=\varphi\left(P(v_{1}\odot v_{1}),P(v_{1}\odot\mathrm{rev}(v_{1} )),\ldots\right)\] (12) \[=\varphi\left(v_{1}\odot v_{1},v_{1}\odot\mathrm{rev}(v_{1}), \ldots\right)\] (13) \[=f(v_{1},\ldots,v_{k}),\] (14)

where the second to last inequality is by permutation invariance of \(\varphi\). Next, we show universal approximation.

Let \(h:\mathbb{R}^{2\times k}\to\mathbb{R}^{d_{\mathrm{out}}}\) be a continuous permutation invariant and sign invariant function. Then by the decomposition theorem in Lim et al. (2023), we can write

\[h(v_{1},\ldots,v_{k})=\rho(\phi(v_{1}v_{1}^{\top}),\ldots,\phi(v_{k}v_{k}^{ \top})),\] (15)

for continuous functions \(\rho\) and \(\phi\). As a composition of continuous functions, the function \(\psi:B\subseteq\mathbb{R}^{2\times 2k}\to\mathbb{R}^{d_{\mathrm{out}}}\) given by \(\psi(A_{1},\ldots,A_{k})=\rho(\phi(A_{1}),\ldots,\phi(A_{k}))\) is continuous, where \(B\) is the subset of \(\mathbb{R}^{2\times 2k}\) consisting of \((v_{1}v_{1}^{\top},\ldots,v_{k}v_{k}^{\top})\) such that each \(v_{i}\in\mathbb{R}^{2}\). Note that \(\psi\) is permutation invariant on \(B\), in the sense that for any \(2\times 2\) permutation matrix \(P\), we have

\[\psi(PA_{1}P^{\top},\ldots,PA_{k}P^{\top})=\psi(A_{1},\ldots,A_{k}),\] (16)

because if \(v_{i}v_{i}^{\top}=A_{i}\), then

\[\psi(PA_{1}P^{\top},\ldots,PA_{k}P^{\top})=h(Pv_{1},\ldots,Pv_{k})=h(v_{1}, \ldots,v_{k})=\psi(A_{1},\ldots,A_{k}),\] (17)

by permutation invariance of \(h\).

Now, we define our permutation invariant function \(\varphi:C\subseteq\mathbb{R}^{2\times 2k}\rightarrow\mathbb{R}^{d_{\mathrm{out}}}\), on the domain

\[C=\{[v_{1}\odot v_{1},v_{1}\odot\mathrm{rev}(v_{1}),\dots,v_{k}\odot v_{k},v_{k }\odot\mathrm{rev}(v_{k})]:v_{i}\in\mathbb{R}^{2}\}.\] (18)

We define \(\varphi\) by

\[\varphi(A)=\psi\left(\begin{bmatrix}A_{1,1}&A_{2,2}\\ A_{2,2}&A_{2,1}\end{bmatrix},\begin{bmatrix}A_{1,3}&A_{2,4}\\ A_{2,4}&A_{2,3}\end{bmatrix},\dots,\begin{bmatrix}A_{1,2k-1}&A_{2,2k}\\ A_{2,2k}&A_{2,2k-1}\end{bmatrix}\right).\] (19)

To see that \(\varphi\) is permutation invariant, we need only consider the case where \(P=\begin{bmatrix}0&1\\ 1&0\end{bmatrix}\), in which case

\[\varphi(PA) =\psi\left(\begin{bmatrix}A_{2,1}&A_{1,2}\\ A_{1,2}&A_{1,1}\end{bmatrix},\begin{bmatrix}A_{2,3}&A_{1,4}\\ A_{1,4}&A_{1,3}\end{bmatrix},\dots,\begin{bmatrix}A_{2,2k-1}&A_{1,2k}\\ A_{1,2k}&A_{1,2k-1}\end{bmatrix}\right)\] (20) \[=\psi\left(P\begin{bmatrix}A_{1,1}&A_{2,2}\\ A_{2,2}&A_{2,1}\end{bmatrix}P^{\top},P\begin{bmatrix}A_{1,3}&A_{2,4}\\ A_{2,4}&A_{2,3}\end{bmatrix}P^{\top},\dots,P\begin{bmatrix}A_{1,2k-1}&A_{2,2k} \\ A_{2,2k}&A_{2,2k-1}\end{bmatrix}P^{\top}\right)\] (21) \[=\psi\left(\begin{bmatrix}A_{1,1}&A_{2,2}\\ A_{2,2}&A_{2,1}\end{bmatrix},\begin{bmatrix}A_{1,3}&A_{2,4}\\ A_{2,4}&A_{2,3}\end{bmatrix},\dots,\begin{bmatrix}A_{1,2k-1}&A_{2,2k}\\ A_{2,2k}&A_{2,2k-1}\end{bmatrix}\right)\quad\text{($\psi$ perm. inv.)}\] (22) \[=\varphi(A),\] (23)

where in the second equality, we use the fact that \(A_{2,2j}=A_{1,2j}\), \(j=1,\dots,k\) for \(A\in C\), because \(A_{2,2j}=(v_{j}\odot\mathrm{rev}(v_{j}))_{2}=(v_{j}\odot\mathrm{rev}(v_{j}))_{ 1}=A_{1,2j}\) for some \(v_{j}\in\mathbb{R}^{2}\). Moreover, \(\varphi\) is clearly continuous and sign invariant. Defining \(f:\mathbb{R}^{2\times k}\rightarrow\mathbb{R}^{d_{\mathrm{out}}}\) using this \(\varphi\), we compute that

\[f(v_{1},\dots v_{k}) =\varphi\left(v_{1}\odot v_{1},v_{1}\odot\mathrm{rev}(v_{1}), \dots,v_{k}\odot v_{k},v_{k}\odot\mathrm{rev}(v_{k})\right)\] (24) \[=\psi\left(\begin{bmatrix}v_{1,1}^{2}&v_{1,1}v_{1,2}\\ v_{1,1}v_{1,2}&v_{1,2}^{2}\end{bmatrix},\dots,\begin{bmatrix}v_{k,1}^{2}&v_{ k,1}v_{k,2}\\ v_{k,1}v_{k,2}&v_{k,2}^{2}\end{bmatrix}\right)\] (25) \[=\psi\left(v_{1}v_{1}^{\top},\dots,v_{k}v_{k}^{\top}\right)\] (26) \[=h(v_{1},\dots,v_{k}),\] (27)

so we are done.

If \(\varphi\) instead comes from a universally approximating class of permutation invariant neural networks (rather than being an arbitrary continuous permutation invariant function), then on a compact domain we can get \(\epsilon\) approximation of \(f\) to \(h\) by letting \(\varphi\) approximate \(\psi\) to \(\epsilon\) accuracy. 

#### a.4.2 Proof of Proposition 1

**Proposition 1**.: _Let \(f:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n\times d_{\mathrm{out}}}\) be a permutation equivariant function, and let \(V=[v_{1},\dots,v_{k}]\in\mathbb{R}^{n\times k}\) be \(k\) orthonormal eigenvectors of an adjacency matrix \(A\). Let nodes \(i\) and \(j\) be automorphic, and let \(z_{i}\) and \(z_{j}\in\mathbb{R}^{d_{\mathrm{out}}}\) be their embeddings, i.e, the \(i\)th and \(j\)th row of \(Z=f(V)\)._

* _If_ \(f\) _is sign invariant and the eigenvalues associated with the_ \(v_{l}\) _are simple and distinct, then_ \(z_{i}=z_{j}\)_._
* _If_ \(f\) _is basis invariant and_ \(v_{1},\dots,v_{k}\) _are a basis for some number of eigenspaces of_ \(A\) _then_ \(z_{i}=z_{j}\)_._

Proof.: We only prove the basis invariance claim, as the sign invariance claim is a special case; basis invariance is sign invariance when eigenvalues are distinct.

Let \(P\in\mathbb{R}^{n\times n}\) be a permutation matrix associated to an automorphism that maps node \(i\) to node \(j\), so \(PAP^{\top}=A\) and \(Pe_{i}=e_{j}\), where \(e_{l}\) is the \(l\)th standard basis vector. Let \(V_{t}=[v_{r_{1}},\dots,v_{r_{d_{t}}}]\) be the matrix whose columns are Repeat the above argument to get such a \(Q_{t}\) for each of the eigenbases \(V_{1},\ldots,V_{l}\). We can then see that

\[z_{j} =f(V_{1},\ldots,V_{l})_{j_{:}}\] \[=f(V_{1}Q_{1},\ldots,V_{l}Q_{l})_{j_{:}}\] basis invariance \[=f(PV_{1},\ldots,PV_{l})_{j_{:}}\] choice of

\[Q_{t}\] \[=(Pf(V_{1},\ldots,V_{l}))_{j_{:}}\] permutation equivariance \[=f(V_{1},\ldots,V_{l})_{i,:}\] choice of

\[P\]

So we are done. 

### Sign Invariance and Structural Node or Node-Pair Encodings

In this section, we show that when the eigenvalues \(\lambda_{1},\ldots,\lambda_{k}\) are distinct, then sign invariant functions of the orthonormal eigenvectors \(v_{1},\ldots,v_{k}\) give structural node or node-pair representations. This can also be generalized in a straightforward way to larger tuples of nodes beyond pairs, though we only consider nodes and node-pairs for ease of exposition. First, we give formal definitions.

**Definition 1** (Structural Representations (Srinivasan and Ribeiro, 2019)).: _Let \(A\in\mathbb{R}^{n\times n}\) be the adjacency matrix of a graph on node set \(\{1,\ldots,n\}\)._

_A function \(f:\mathbb{R}^{n\times n}\to\mathbb{R}^{n}\) is a node structural representation if \(f(PAP^{\top})=Pf(A)\) for all \(n\times n\) permutation matrices \(P\)._

_A function \(f:\mathbb{R}^{n\times n}\to\mathbb{R}^{n\times n}\) is a node-pair structural representation if \(f(PAP^{\top})=Pf(A)P^{\top}\) for all \(n\times n\) permutation matrices \(P\)._

Importantly, these structural representations are permutation equivariant functions of adjacency matrices, not arbitrary matrices. For each adjacency matrix \(A\), let \(V(A)=[v_{1}(A),\ldots,v_{k}(A)]\) be a choice of orthonormal eigenvectors for the first \(k\) eigenvalues \(\lambda_{1}(A),\ldots,\lambda_{k}(A)\). We assume in this section that these first \(k\) eigenvalues are distinct for all \(A\) under consideration, so \(V(A)\) is defined up to sign flips. Let \(h:\mathbb{R}^{n\times k}\to\mathbb{R}^{n}\) be a permutation equivariant function of sets, so \(h(PX)=Ph(X)\) for all permutations matrices \(P\). Then of course \(h(PV(A))=Ph(V(A))\), but this does not make \(h\) a node structural encoding. This is because \(A\mapsto h(V(A))\) is in general not a well-defined function of the adjacency, since the choice of \(V(A)\) is not well-defined (the choices of sign are arbitrary). If we constrain \(h\) to not depend on the signs (sign invariance), or to depend on the signs in a predictable way (sign equivariance), then we can compute structural node or node-pair encodings from eigenvectors.

We capture these observations in the below proposition. First, we define three types of functions:

* Let \(f_{\mathrm{node}}:\mathbb{R}^{n\times k}\to\mathbb{R}^{n}\) be sign invariant and permutation equivariant; that is, \(f_{\mathrm{node}}(Pv_{1}s_{1},\ldots,Pv_{k}s_{k})=Pf_{\mathrm{node}}(v_{1}, \ldots,v_{k})\) for \(s_{i}\in\{-1,1\}\) and \(P\) a permutation matrix.
* Let \(f_{\mathrm{decode}}:\mathbb{R}^{2\times k}\to\mathbb{R}\) be sign invariant; that is, \(f_{\mathrm{decode}}(Sz_{i},Sz_{j})=f_{\mathrm{decode}}(z_{i},z_{j})\) for \(S\in\mathrm{diag}(\{-1,1\}^{k})\).
* Let \(f_{\mathrm{equiv}}:\mathbb{R}^{n\times k}\to\mathbb{R}^{n\times k}\) be a permutation equivariant and sign equivariant function; that is, \(f_{\mathrm{equiv}}(PV(A)S)=Pf_{\mathrm{equiv}}(V(A))S\) for \(S\in\mathrm{diag}(\{-1,1\}^{k})\) and \(P\) a permutation matrix.

**Proposition 5**.: _Let \(\mathcal{A}\subseteq\mathbb{R}^{n\times n}\) denote the matrices with distinct first-\(k\) eigenvalues. For \(A\in\mathcal{A}\), let \(V(A)=[v_{1}(A),\ldots,v_{k}(A)]\) be a choice of orthonormal eigenvectors of \(A\), associated to the first-\(k\) (distinct) eigenvalues \(\lambda_{1}(A),\ldots,\lambda_{k}(A)\). Then_

_(a) The map \(q_{\mathrm{node}}:\mathcal{A}\to\mathbb{R}^{n}\) given by \(q_{\mathrm{node}}(A)_{i}=f_{\mathrm{node}}\left(f_{\mathrm{equiv}}(V(A)) \right)_{i}\) is well-defined and gives a structural node representation._

_(b) The map \(q_{\mathrm{pair}}:\mathcal{A}\to\mathbb{R}^{n\times n}\) defined by \(q_{\mathrm{pair}}(A)_{i,j}=f_{\mathrm{decode}}\left(f_{\mathrm{equiv}}(V(A) )_{i,:},f_{\mathrm{equiv}}(V(A))_{j,:}\right)\) is well-defined and gives a structural node-pair representation._

Note that the identity mapping \(V(A)\mapsto V(A)\) is permutation equivariant and sign equivariant, so using \(f_{\mathrm{node}}\) or \(f_{\mathrm{decode}}\) directly on eigenvectors also gives structural representations. The statement (b) means that our link prediction pipeline with sign equivariant node features and sign invariant decoding produces structural node-pair representations.

Proof.: **Part (a)** We first show that \(q_{\mathrm{node}}:\mathcal{A}\rightarrow\mathbb{R}^{n}\) is well-defined. Suppose we had another choice of eigenvectors, so the eigenvectors we input are \(V(A)S\) for some \(S\in\mathrm{diag}(\{-1,1\}^{k})\). Then

\[f_{\mathrm{node}}\left(f_{\mathrm{equiv}}(V(A)S)\right)=f_{\mathrm{node}} \left(f_{\mathrm{equiv}}(V(A))S\right)=f_{\mathrm{node}}\left(f_{\mathrm{equiv }}(V(A))\right),\] (29)

where the first equality is by sign equivariance, and the second equality by sign invariance. Thus, the value of \(q_{\mathrm{node}}(A)\) is unchanged.

Now, let \(P\) be any permutation matrix. Then for each eigenvector \(v_{i}(A)\), \(i\in[k]\), we have \((PAP^{\top})Pv_{i}(A)=PAv_{\downarrow}(A)=\lambda_{i}(A)Pv_{i}(A)\), so \(Pv_{i}(A)\) is an eigenvector of \(PAP^{\top}\) associated to \(\lambda_{i}(A)=\lambda_{i}(PAP^{\top})\). Hence, we denote \(v_{i}(PAP^{\top})=Pv_{i}(A)\) (the choice of sign does not matter as \(q\) does not depend on the sign. Now, we have that

\[q_{\mathrm{node}}(PAP^{\top}) =f_{\mathrm{node}}\left(f_{\mathrm{equiv}}(V(PAP^{\top}))\right)\] (30) \[=f_{\mathrm{node}}\left(f_{\mathrm{equiv}}(PV(A))\right)\] (31) \[=Pf_{\mathrm{node}}\left(f_{\mathrm{equiv}}(V(A))\right)\] (32) \[=Pq_{\mathrm{node}}(A)\] (33)

where the second to last equality is by permutation equivariance of \(f_{\mathrm{node}}\) and \(f_{\mathrm{equiv}}\).

**Part (b)** That \(q_{\mathrm{pair}}:\mathcal{A}\rightarrow\mathbb{R}^{n\times n}\) is well-defined follows from a similar argument to the \(q_{\mathrm{node}}\) case. Let \(P\) be a permutation matrix, and \(\sigma:[n]\rightarrow[n]\) its underlying permutation. We compute that

\[q_{\mathrm{pair}}(PAP^{\top})_{i,j} =f_{\mathrm{decode}}\left(f_{\mathrm{equiv}}(V(PAP^{\top}))_{i, :},f_{\mathrm{equiv}}(V(PAP^{\top}))_{j,:}\right)\] (34) \[=f_{\mathrm{decode}}\left(f_{\mathrm{equiv}}(PV(A))_{i,:},f_{ \mathrm{equiv}}(PV(A))_{j,:}\right)\] (35) \[=f_{\mathrm{decode}}\left([Pf_{\mathrm{equiv}}(V(A))]_{i,:},[Pf_ {\mathrm{equiv}}(V(A))]_{j,:}\right)\] (36) \[=f_{\mathrm{decode}}\left(f_{\mathrm{equiv}}(V(A))_{\sigma^{-1} (i),:},f_{\mathrm{equiv}}(V(A))_{\sigma^{-1}(j),:}\right)\] (37) \[=q_{\mathrm{pair}}(A)_{\sigma^{-1}(i),\sigma^{-1}(j)}\] (38) \[=(Pq_{\mathrm{pair}}(A)P^{\top})_{i,j}\] (39)

#### a.5.1 Sign Equivariance is Provably More Expressive for Link Prediction

Our arguments in Section 2.1 and Figure 2 explain why we can expect sign equivariant models to be more powerful than sign invariant models in link prediction. To give a theoretically rigorous explanation, here we provide an example where sign equivariant models can provably compute more expressive link representations than sign invariant models.

Consider a cycle graph \(C_{2k}\) for some even length \(2k\), where \(k\geq 3\). All nodes are automorphic in this graph, so any model based on structural node representations must assign the same representation to each node-pair. For instance, consider the eigenvalue \(-2\) of the adjacency matrix, which is a simple eigenvalue with eigenvector \([1,-1,1,-1,\ldots,1,-1]\)(Lee et al., 1992). Then a sign invariant model will lose the sign information and map each node to the same encoding, which means that each node-pair will also have the same encoding. However, a sign equivariant model can preserve the sign of each node (for instance by learning the identity function). Then for any pair of nodes that are one hop away, it can take a dot product to compute the pair representation \(-1\), whereas it can take a dot product between any nodes that are two hops away to compute the pair representation \(1\). Of course, using more eigenvectors would allow for more complex representations to be computed.

#### a.5.2 More on Sign Equivariance and Link Prediction

Key to our method is the ability to update a positional node embedding in an equivariant way, which respects the graph symmetries. To elaborate, consider the aforementioned definition of node positional encodings as samples from a permutation equivariant probability distribution over node features (Srinivasan and Ribeiro, 2019). Laplacian eigenvector positional embeddings are samples from the distribution of orthonormal bases of the eigenspaces of the Laplacian. Our sign equivariancebased approach is possible because the randomness in Laplacian eigenvector positional encodings is exceptionally structured (consisting only of sign flips when eigenvalues are distinct). In contrast, a general way to obtain structural pair representations from node positional embeddings is to average some function over the randomness of the positional encoding (i.e., over many samples of the positional encoding) (Srinivasan and Ribeiro, 2019), but this is highly expensive, often intractable, and introduces substantial variance into the learning procedure. For instance, one may have to average samples of the \(n!\) assignments of unique node identifiers (Murphy et al., 2019) or approximate an integral over Gaussian random features (Abboud et al., 2021).

### Proof of Proposition 2, Orthogonal Equivariance via Sign Equivariance

**Proposition 2**.: _Consider a domain \(\mathcal{X}\subseteq\mathbb{R}^{n\times d}\) such that each \(X\in\mathcal{X}\) has distinct covariance eigenvalues, and let \(R_{X}\) be a choice of orthonormal eigenvectors of \(\operatorname{cov}(X)\) for each \(X\in\mathcal{X}\). If \(h:\mathcal{X}\subseteq\mathbb{R}^{n\times d}\to\mathbb{R}^{n\times d}\) is sign equivariant, and if \(f(X)=h(XR_{X})R_{X}^{\top}\), then \(f\) is well defined and orthogonally equivariant._

_Moreover, is \(h\) is from a universal class of sign equivariant functions, then the \(f\) of the above form universally approximate \(O(k)\) equivariant functions on \(\mathcal{X}\)._

Proof.: First, we show that \(f\) is well defined. \(R_{X}\) is only unique up to sign flips, as \(R_{X}S\) is an orthonormal set of eigenvectors of \(\operatorname{cov}(X)\) for \(S\in\operatorname{diag}(\{-1,1\}^{k})\). However, no matter the choice of signs, \(f(X)\) takes the same value, since

\[h(XR_{X}S)(R_{X}S)^{\top} =h(XR_{X}S)S^{\top}R_{X}^{\top}\] (40) \[=h(XR_{X})SS^{\top}R_{X}^{\top}\] sign equivariance (41) \[=h(XR_{X})R_{X}^{\top}.\] (42)

Next, we show that \(f\) is \(O(k)\) equivariant. Let \(Q\in O(k)\) be any orthogonal matrix. Note that

\[\operatorname{cov}(XQ)=\left(XQ-\frac{1}{n}\mathbf{1}\mathbf{1}^{\top}XQ \right)^{\top}\left(XQ-\frac{1}{n}\mathbf{1}\mathbf{1}^{\top}XQ\right)=Q^{ \top}\operatorname{cov}(X)Q.\] (43)

Thus, \(Q^{\top}R_{X}\) is an orthonormal set of eigenvectors of \(\operatorname{cov}(XQ)\). This means that there is a choice of signs \(S\in\operatorname{diag}(\{-1,1\}^{k})\) such that \(Q^{\top}R_{X}S=R_{XQ}\). Hence, we have that

\[f(XQ) =h(XQR_{XQ})R_{XQ}^{\top}\] (44) \[=h(XQQ^{\top}R_{X}S)(Q^{\top}R_{X}S)^{\top}\] (45) \[=h(XR_{X})SS^{\top}R_{X}^{\top}Q\] sign equivariance (46) \[=h(XR_{X})R_{X}^{\top}Q\] (47) \[=f(X)Q^{\top},\] (48)

so \(f\) is \(O(k)\) equivariant.

**Universal Approximation.** Our proof of the universality of this class of functions builds on the proof of the universality of frame averaging (Puny et al., 2022). Let \(f_{\mathrm{target}}\) be a continuous \(O(k)\) equivariant function and let \(\epsilon>0\) be a desired approximation accuracy. Then \(f_{\mathrm{target}}\) is also sign equivariant (as the sign matrices \(S\in\operatorname{diag}(\{-1,1\}^{k})\) are orthogonal).

Hence, by sign equivariant universality, we can choose a sign equivariant \(h\) such that \(\|h(X)-f_{\mathrm{target}}(X)\|<\epsilon\) for all \(X\in\mathcal{X}\) (where \(\|\cdot\|\) is the Frobenius norm). Define the \(O(k)\) equivariant \(f(X)=h(XR_{X})R_{X}^{\top}\). Then for all \(X\in\mathcal{X}\) we have that

\[\|f_{\mathrm{target}}(X)-f(X)\| =\left\|f_{\mathrm{target}}(X)-h(XR_{X})R_{X}^{\top}\right\|\] (49) \[=\left\|f_{\mathrm{target}}(X)R_{X}R_{X}^{\top}-h(XR_{X})R_{X}^{ \top}\right\| R_{X}\text{ orthogonal}\] (50) \[=\left\|f_{\mathrm{target}}(XR_{X})R_{X}^{\top}-h(XR_{X})R_{X}^{ \top}\right\| \text{ orthogonal}\] (51) \[=\left\|f_{\mathrm{target}}(XR_{X})-h(XR_{X})\right\| R_{X}\text{ orthogonal}\] (52) \[<\epsilon.\] (53)

So \(f\) approximates \(f_{\mathrm{target}}\) within \(\epsilon\) accuracy on \(\mathcal{X}\), and we are done.

## Appendix B Sign Equivariant Linear Maps

### Sign Equivariant Linear Map Characterization

We first prove our result characterizing the form of the equivariant linear maps from \(\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\).

**Lemma 1**.: _A linear map \(W:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\) is sign equivariant if and only if it can be written as_

\[W(X)=[W_{1}X_{1}\;\dots\;W_{k}X_{k}]\] (54)

_for some linear maps \(W_{1},\dots,W_{k}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n^{\prime}}\), where \(X_{i}\in\mathbb{R}^{n}\) is the \(i\)th column of \(X\in\mathbb{R}^{n\times k}\)._

Proof.: For one direction, suppose \(W\) can be written as in equation 4. To see that \(W\) is sign equivariant, note that for any \(S\in\operatorname{diag}(\{-1,1\}^{k})\), we have

\[W(XS)=[s_{1}W_{1}X_{1}\;\dots\;s_{k}W_{k}X_{k}]=[W_{1}X_{1}\; \dots\;W_{k}X_{k}]\,S=W(X)S.\] (55)

For the other direction, let \(W\) be a sign equivariant linear map. For any \(i^{\prime}\in[n^{\prime}]\) and \(j^{\prime}\in[k]\), we can write the action of \(W\) as

\[W(X)_{i^{\prime},j^{\prime}}=\sum_{i=1}^{n}\sum_{j=1}^{k}W_{i^{ \prime},j^{\prime}}^{i,j}X_{i,j},\] (56)

where \(W_{i^{\prime},j^{\prime}}^{i,j}\in\mathbb{R}\) are coefficients representing the linear map. Let \(c\neq j^{\prime}\) be a column that is not \(j^{\prime}\). Further, for any row \(l\in[n]\), let \(\tilde{X}\in\mathbb{R}^{n\times k}\) be such that \(\tilde{X}_{l,c}=1\), and \(\tilde{X}\) is zero elsewhere. Then we have that

\[W(\tilde{X})_{i^{\prime},j^{\prime}}=W_{i^{\prime},j^{\prime}}^{ l,c}.\] (57)

Now, let \(S\in\operatorname{diag}(\{-1,1\}^{k})\) have a \(-1\) in the \(j^{\prime}\)th column and a \(1\) elsewhere. Then \(\tilde{X}S=\tilde{X}\). This implies that

\[W_{i^{\prime},j^{\prime}}^{l,c} =W(\tilde{X})_{i^{\prime},j^{\prime}}\] (58) \[=W(\tilde{X}S)_{i^{\prime},j^{\prime}}\] (59) \[=-W(\tilde{X})_{i^{\prime},j^{\prime}}\] (60) \[=-W_{i^{\prime},j^{\prime}}^{l,c},\] (61)

where in the second to last equality we used sign equivariance. This implies that \(W_{i^{\prime},j^{\prime}}^{l,c}=0\).

Hence, for any \(i^{\prime}\in[n^{\prime}]\), \(j^{\prime}\in[k^{\prime}]\), we have that \(W(X)_{i^{\prime},j^{\prime}}\) only depends on \(X_{j^{\prime}}\), so we are done. 

### Sign Equivariant Linear Maps between Tensor Representations

Since the sign equivariant linear maps from \(\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\) are very weak, we now characterize sign equivariant linear maps between higher order tensor representations, as past work has done for other groups [19, 20, 18]. In particular, we will consider representations \(\mathbb{R}^{k^{m}}\) for natural numbers \(m\). The action of \(s\in\{-1,1\}^{k}\) on \(V\in\mathbb{R}^{k^{m}}\) is as follows:

\[(s\cdot V)_{i_{1},\dots,i_{m}}=s_{i_{1}}\cdots s_{i_{m}}V_{i_{1}, \dots,i_{m}},\] (62)

for \(i_{1},\dots,i_{m}\in[k]\). We now prove a result showing that there are no sign equivariant linear maps between many pairs of tensor representations.

**Proposition 6**.: _If \(m_{1}+m_{2}\) is odd, then the only sign equivariant linear map \(L:\mathbb{R}^{k^{m_{1}}}\rightarrow\mathbb{R}^{k^{m_{2}}}\) is the zero map._

Proof.: Let \(L:\mathbb{R}^{k^{m_{2}}\times k^{m_{1}}}\) be the matrix associated with a sign equivariant linear map from \(\mathbb{R}^{k^{m_{1}}}\rightarrow\mathbb{R}^{k^{m_{2}}}\). This means that for \(V\in\mathbb{R}^{k^{m_{1}}}\), for \(i_{1},\dots,i_{m_{2}}\in[k]\) we have that

\[(LV)_{i_{1},\dots,i_{m_{2}}}=\sum_{j_{1},\dots,j_{m_{1}}=1}^{k}L_{i_{1},\dots, i_{m_{2}},j_{1},\dots,j_{m_{1}}}V_{j_{1},\dots,j_{m_{1}}}.\] (63)Then by sign equivariance, we have that

\[L(s\cdot V)=s\cdot(LV),\] (64)

for \(s\in\{-1,1\}^{k}\), which means that for all \(i_{1},\ldots,i_{m_{2}}\in[k]\),

\[\sum_{j_{1},\ldots,j_{m_{1}}=1}^{k}L_{i_{1},\ldots,i_{m_{2}},j_{1},\ldots,j_{m_{1}}}s_{j_{1}}\cdots s_{j_{m_{1}}}V_{j_{1},\ldots,j_{m_{1}}}\] (65) \[= s_{i_{1}}\cdots s_{i_{m_{2}}}\sum_{j_{1},\ldots,j_{m_{1}}=1}^{k} L_{i_{1},\ldots,i_{m_{2}},j_{1},\ldots,j_{m_{1}}}V_{j_{1},\ldots,j_{m_{1}}},\] (66)

which means that

\[\sum_{j_{1},\ldots,j_{m_{1}}=1}^{k}L_{i_{1},\ldots,i_{m_{2}},j_{1},\ldots,j_{m_{1}}}s_{i_{1}}\cdots s_{i_{m_{2}}}s_{j_{1}}\cdots s_{j_{m_{1}}}V_ {j_{1},\ldots,j_{m_{1}}}\] (67) \[= \sum_{j_{1},\ldots,j_{m_{1}}=1}^{k}L_{i_{1},\ldots,i_{m_{2}},j_{1 },\ldots,j_{m_{1}}}V_{j_{1},\ldots,j_{m_{1}}}.\] (68)

This gives that \(s\cdot L=L\). Choose some arbitrary indices \(i_{1},\ldots,i_{m_{2}},j_{1},\ldots,j_{m_{1}}\in[k]\). Since \(m_{1}+m_{2}\) is odd, then one of these values appears an odd number of times -- say it is some index \(p\in[k]\). Let \(s\in\{-1,1\}^{k}\) have \(s_{i}=1\) for \(i\neq p\) and \(s_{p}=-1\). Then \(s\cdot L=L\) implies that

\[-L_{i_{1},\ldots,i_{m_{2}},j_{1},\ldots,j_{m_{1}}}=L_{i_{1},\ldots,i_{m_{2}},j _{1},\ldots,j_{m_{1}}}.\] (69)

Which implies that \(L\) is zero at these indices, and hence \(L=0\) everywhere, since these were arbitrarily chosen indices. 

This implies that we cannot map from \(\mathbb{R}^{k}\to\mathbb{R}^{k^{2}}\) using sign equivariant linear maps. Thus, if we want to lift our order-one tensor input to higher tensor orders in a linear way, then we need to at least map to third order tensors in \(\mathbb{R}^{k^{3}}\). This requires substantial memory cost. Furthermore, we next show that even if we could map to third order tensors, learning representations of third order tensors with sign equivariant linear maps is expensive.

**Proposition 7**.: _The dimension of the space of sign equivariant linear maps from \(\mathbb{R}^{k^{m_{1}}}\to\mathbb{R}^{k^{m_{2}}}\) is_

\[\frac{1}{2^{k}}\sum_{s\in\{-1,1\}^{k}}(s_{1}+\ldots+s_{k})^{m_{1}+m_{2}}.\] (70)

Proof.: This is a direct consequence of the First Projection Formula: see Fulton and Harris (2013) Section 2.2. In particular, it can be seen that the dimension of this space is equal to

\[\frac{1}{2^{k}}\sum_{S\in\operatorname{Diag}(\{-1,1\}^{k})}\operatorname{ trace}(S^{\otimes(m_{1}+m_{2})})\] (71)

by the First Projection Formula. Since \(\operatorname{trace}(A\otimes B)=\operatorname{trace}(A)\cdot\operatorname{ trace}(B)\), this is equal to

\[\frac{1}{2^{k}}\sum_{S\in\operatorname{Diag}(\{-1,1\}^{k})} \operatorname{trace}(S)^{m_{1}+m_{2}}=\frac{1}{2^{k}}\sum_{s\in\{-1,1\}^{k}}( s_{1}+\ldots+s_{k})^{m_{1}+m_{2}}.\] (72)

Using Proposition 7, we have numerically computed the dimension of the space of sign equivariant linear maps from \(\mathbb{R}^{k^{3}}\to\mathbb{R}^{k^{3}}\); see Table 4. The dimension appears to be \(15k^{3}-30k^{2}+16k\) for \(k\) up to 20. In particular, when \(k=8\), we compute that the space of sign equivariant linear maps from \(\mathbb{R}^{k^{3}}\to\mathbb{R}^{k^{3}}\) is of dimension 5888, which is already quite large.

## Appendix C Characterization of Sign Equivariant Polynomials

In this Appendix, we characterize the form of the sign equivariant polynomials. This is useful, because for a finite group, equivariant polynomials universally approximate equivariant continuous functions (Yarotsky, 2022); thus, if a model universally approximates equivariant polynomials, then it universally approximates equivariant continuous functions. Using equivariant polynomials to analyze or develop equivariant machine learning models has been done successfully in many contexts (Zaheer et al., 2017; Yarotsky, 2022; Segol and Lipman, 2019; Dym and Maron, 2021; Maron et al., 2019, 2020; Villar et al., 2021; Dym and Gortler, 2022; Puny et al., 2023).

### Sign Invariant Polynomials \(\mathbb{R}^{k}\to\mathbb{R}\)

Next, we characterize the form of sign invariant and equivariant polynomials. For simplicity, we start with the case of sign invariant polynomials \(p:\mathbb{R}^{k}\to\mathbb{R}\). The sign equivariant polynomials take a very similar form. We can write any polynomial from \(\mathbb{R}^{k}\) to \(\mathbb{R}\) in the form

\[p(v)=\sum_{d_{1},\ldots,d_{k}=0}^{D}\textbf{W}_{d_{1},\ldots,d_{k}}v_{1}^{d_{1 }}\cdots v_{k}^{d_{k}}\] (73)

for some coefficients \(\textbf{W}_{d_{1},\ldots,d_{k}}\in\mathbb{R}\) and some \(D\in\mathbb{N}\). Sign invariance tells us that for any \(S=\operatorname{diag}(s_{1},\ldots,s_{k})\in\operatorname{diag}(\{-1,1\}^{k})\), we must have

\[\sum_{d_{1},\ldots,d_{k}=0}^{D}\textbf{W}_{d_{1},\ldots,d_{k}}v_{1}^{d_{1}} \cdots v_{k}^{d_{k}}=p(v)=p(Sv)=\sum_{d_{1},\ldots,d_{k}=0}^{D}\textbf{W}_{d_{ 1},\ldots,d_{k}}s_{1}^{d_{1}}\cdots s_{k}^{d_{k}}v_{1}^{d_{1}}\cdots v_{k}^{d_ {k}}.\] (74)

This holds for any \(v\in\mathbb{R}^{k}\), so for all choices of \(d_{1},\ldots,d_{k}\) we must have

\[\textbf{W}_{d_{1},\ldots,d_{k}}=s_{1}^{d_{1}}\cdots s_{k}^{d_{k}}\textbf{W}_{ d_{1},\ldots,d_{k}},\quad\text{for all }(s_{1},\ldots,s_{k})\in\{-1,1\}^{k}.\] (75)

Note that \(s_{i}^{d_{i}}=1\) if \(d_{i}\) is an even number. Hence, there are no constraints on \(\textbf{W}_{d_{1},\ldots,d_{k}}\) if all \(d_{i}\) are even. On the other hand, suppose \(d_{j}\) is odd for some \(j\). Let \(s_{i}=1\) for \(i\neq j\) and \(s_{j}=-1\). Then theconstraint says that \(\textbf{W}_{d_{1},\dots,d_{k}}=-\textbf{W}_{d_{1},\dots,d_{k}}\), so we must have \(\textbf{W}_{d_{1},\dots,d_{k}}=0\). To summarize, we have

\[\textbf{W}_{d_{1},\dots,d_{k}}=\begin{cases}\text{free}&d_{i}\text{ even for each }i\\ 0&\text{else}\end{cases}\] (76)

Where being free means that the coefficient may take any value in \(\mathbb{R}\). Thus, any sign invariant \(p\) only has terms where each variable \(v_{i}\) is raised to an even power. It is also easy to see that any polynomial \(p\) where each variable \(v_{i}\) is raised to only even powers is sign invariant, so we have the following proposition:

**Proposition 8**.: _A polynomial \(p:\mathbb{R}^{k}\to\mathbb{R}\) is sign invariant if and only if it can be written_

\[p(v)=\sum_{d_{1},\dots,d_{k}=0}^{D}\textbf{W}_{d_{1},\dots,d_{k}}v_{1}^{2d_{1} }\cdots v_{k}^{2d_{k}},\] (77)

_for some coefficients \(\textbf{W}_{d_{1},\dots,d_{k}}\in\mathbb{R}\) and \(D\in\mathbb{N}\)._

_In other words, \(p\) is sign invariant if and only if there exists a polynomial \(q:\mathbb{R}^{k}\to\mathbb{R}\) such that \(p(v)=q(v_{1}^{2},\dots,v_{k}^{2})\)._

### Sign Equivariant Polynomials \(\mathbb{R}^{k}\to\mathbb{R}^{k}\)

The case of sign equivariant polynomials \(p:\mathbb{R}^{k}\to\mathbb{R}^{k}\) is very similar. For \(l\in[k]\), the \(l\)th output dimension of any polynomial \(p:\mathbb{R}^{k}\to\mathbb{R}^{k}\) can be written

\[p(v)_{l}=\sum_{d_{1},\dots,d_{k}=0}^{D}\textbf{W}_{d_{1},\dots,d_{k}}^{(l)}v_{ 1}^{d_{1}}\cdots v_{k}^{d_{k}},\] (78)

where \(\textbf{W}_{d_{1},\dots,d_{k}}^{(l)}\in\mathbb{R}\) are coefficients (note the extra \(l\) index, so there are \(k\) times more coefficients than in the invariant case). By sign equivariance, we have

\[s_{l}\cdot p(v)_{l} =p(Sv)_{l}\] (79) \[s_{l}\cdot\sum_{d_{1},\dots,d_{k}=0}^{D}\textbf{W}_{d_{1},\dots,d _{k}}^{(l)}v_{1}^{d_{1}}\cdots v_{k}^{d_{k}} =\sum_{d_{1},\dots,d_{k}=0}^{D}\textbf{W}_{d_{1},\dots,d_{k}}^{(l) }s_{1}^{d_{1}}\cdots s_{k}^{d_{k}}v_{1}^{d_{1}}\cdots v_{k}^{d_{k}}.\] (80)

As this holds for all inputs \(v\in\mathbb{R}^{k}\), we have the following constraints on the coefficients:

\[s_{l}\textbf{W}_{d_{1},\dots,d_{k}}^{(l)} =s_{1}^{d_{1}}\cdots s_{k}^{d_{k}}\textbf{W}_{d_{1},\dots,d_{k}}^{ (l)}\] (81) \[\textbf{W}_{d_{1},\dots,d_{k}}^{(l)} =s_{l}\cdot s_{1}^{d_{1}}\cdots s_{k}^{d_{k}}\textbf{W}_{d_{1}, \dots,d_{k}}^{(l)},\] (82)

where we use the fact that \(s_{l}=1/s_{l}\) since \(s_{l}\in\{-1,1\}\). If \(d_{j}\) is odd for \(j\neq l\), then similarly to the invariant case, we can take \(s_{i}=1\) for \(i\neq j\) and \(s_{j}=-1\) in the above equation to see that \(\textbf{W}_{d_{1},\dots,d_{k}}^{(l)}=0\). If \(d_{l}\) is even, then \(d_{l}+1\) is odd, so we have that \(\textbf{W}_{d_{1},\dots,d_{k}}^{(l)}=0\) by the same argument. Thus, we must have

\[\textbf{W}_{d_{1},\dots,d_{k}}^{(l)}=\begin{cases}\text{free}&d_{l}\text{ odd, and }d_{i}\text{ even for each }i\neq l\\ 0&\text{else}\end{cases}.\] (83)

Thus, the \(l\)th entry \(p(v)_{l}\) only contains monomials of the term \(v_{1}^{2d_{1}}\cdots v_{l}^{2d_{l}+1}\cdots v_{k}^{2d_{k}}\), where each term besides \(v_{l}\) is raised to an even power. We can factor out a \(v_{l}\) and write such terms as \(v_{l}\cdot v_{1}^{2d_{1}}\cdots v_{k}^{2d_{k}}\). It is also easy to see that any polynomial with monomials only of this form is sign equivariant. Thus, we have proven Proposition 9.

**Proposition 9**.: _A polynomial \(p:\mathbb{R}^{k}\to\mathbb{R}^{k}\) is sign equivariant if and only if it can be written_

\[p(v)_{l}=v_{l}\cdot\left(\sum_{d_{1},\dots,d_{k}=0}^{D}\textbf{W}_{d_{1},\dots, d_{k}}^{(l)}v_{1}^{2d_{1}}\cdots v_{k}^{2d_{k}}\right).\] (84)

_In vector format, \(p\) is sign equivariant if and only if it can be written as \(p(v)=v\odot p_{\text{inv}}(v)\) for a sign invariant polynomial \(p_{\text{inv}}:\mathbb{R}^{k}\to\mathbb{R}^{k}\)._

### Sign Equivariant Polynomials \(\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\)

Finally, we will handle the case of polynomials \(p:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\) equivariant to \(\operatorname{diag}(\{-1,1\}^{k})\). This is the case we most often deal with in practice, when we have input \(V=[v_{1}\ \ \dots\ \ v_{k}]\) for \(k\) eigenvectors \(v_{i}\in\mathbb{R}^{n}\) of some \(n\times n\) matrix. For \(a\in[n^{\prime}]\) and \(b\in[k]\), the \((a,b)\)th output of a polynomial \(\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\) is

\[p(V)_{a,b}=\sum_{d_{i,j}=0}^{D}\textbf{W}_{\textbf{d}}^{(a,b)}\prod_{i=1}^{n} \prod_{j=1}^{k}V_{i,j}^{d_{i,j}},\] (85)

where the sum ranges over \(d_{i,j}\in\{0,\dots,D\}\) for \(i\in[n]\) and \(j\in[k]\), and \(\textbf{d}=(d_{1,1},\dots,d_{n,1},d_{1,2},\dots,d_{n,k})\) is a shorthand to index coefficients \(\textbf{W}_{\textbf{d}}^{(a,b)}\in\mathbb{R}\). By sign equivariance, we have that:

\[s_{b}\cdot p(V)_{a,b} =p(VS)_{a,b}\] (86) \[s_{b}\cdot\sum_{d_{i,j}=0}^{D}\textbf{W}_{\textbf{d}}^{(a,b)} \prod_{i=1}^{n}\prod_{j=1}^{k}V_{i,j}^{d_{i,j}} =\sum_{d_{i,j}=0}^{D}\textbf{W}_{\textbf{d}}^{(a,b)}s_{1}^{\tilde{d}_{1}} \cdots s_{k}^{\tilde{d}_{k}}\prod_{i=1}^{n}\prod_{j=1}^{k}V_{i,j}^{d_{i,j}},\] (87)

where \(\tilde{d}_{j}=\sum_{i^{\prime}=1}^{n}d_{i^{\prime},j}\) is the number of times that an entry from column \(j\) of \(V\) appears in the product \(\prod_{i=1}^{n}\prod_{j=1}^{k}V_{i,j}^{d_{i,j}}\). As this holds over all \(V\), we thus have that

\[\textbf{W}_{\textbf{d}}^{(a,b)}=s_{b}\cdot s_{1}^{\tilde{d}_{1}}\cdots s_{k}^ {\tilde{d}_{k}}\cdot\textbf{W}_{\textbf{d}}^{(a,b)}.\] (88)

By analogous arguments to the previous subsections, if \(\tilde{d}_{j}\) is odd for \(j\neq b\), we have that the \(\textbf{W}_{\textbf{d}}^{(a,b)}=0\). Likewise, if \(\tilde{d}_{b}\) is even, we have \(\textbf{W}_{\textbf{d}}^{(a,b)}=0\). Thus, the constraint on **W** is

\[\textbf{W}_{\textbf{d}}^{(a,b)}=\begin{cases}\text{free}&\sum_{i}d_{i,b}\text{ odd, and }\sum_{i}d_{i,j}\text{ even for each }j\neq b\\ 0&\text{else}\end{cases}.\] (89)

In particular, this means that the only nonzero terms in the sum that defines \(p(V)_{a,b}\) have an even number of entries from column \(j\) for \(j\neq b\), and an odd number of entries from column \(b\). Thus, each term can be written as \(V_{i_{\textbf{d}},b}\cdot p_{\operatorname{inv}}(V)_{\textbf{d}}\) for some index \(i_{\textbf{d}}\in[n]\) and sign invariant polynomial \(p_{\operatorname{inv}}\). Moreover, it can be seen that any polynomial that only has terms of this form is sign equivariant. Thus, we have shown the following proposition:

**Proposition 10**.: _A polynomial \(p:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n^{\prime}\times k}\) is sign equivariant if and only if it can be written as_

\[p(V)_{a,b}=\sum_{d_{i,j}=0}^{D}\textbf{W}_{\textbf{d}}^{(a,b)}V_{i_{\textbf{d} },b}\cdot p_{\operatorname{inv}}(V)_{\textbf{d}},\] (90)

_where \(p_{\operatorname{inv}}\) is a sign invariant polynomial, the sum ranges over all \(\textbf{d}\), and \(i_{\textbf{d}}\in[n]\) for each \(\textbf{d}\)._

Now, we show that this implies Theorem 1. In particular, we will write \(p\) in the form

\[p(V)=W^{(2)}\left((W^{(1)}V)\odot\mathfrak{q}_{\operatorname{inv}}(V)\right),\] (91)

for sign equivariant linear maps \(W^{(2)}\) and \(W^{(1)}\), and a sign equivariant polynomial \(q_{\operatorname{inv}}\). To do so, let \(\tilde{D}\) denote the number of all possible \(\textbf{d}\) that the sum in equation 90 ranges over. We take \(W^{(1)}:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{\tilde{D}n^{\prime}\times k}\) and \(W^{(2)}:\mathbb{R}^{\tilde{D}n^{\prime}\times k}\rightarrow\mathbb{R}^{n^{ \prime}\times k}\). These sign equivariant linear maps have to act independently on each column of their input, so \(W^{(1)}V=[W_{1}^{(1)}v_{1},\dots W_{k}^{(1)}v_{k}]\) for linear maps \(W_{1}^{(1)}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{\tilde{D}n^{\prime}}\). We define \(W_{b}^{(1)}\) to be the linear map such that \((W_{b}^{(1)}v_{b})_{\textbf{d},a}=W_{\textbf{d}}^{(a,b)}V_{i_{\textbf{d}},b}\) for \(a\in[n^{\prime}]\). For the sign invariant polynomial \(q_{\operatorname{inv}}\), we take \(q_{\operatorname{inv}}(V)_{\textbf{d},a}=p_{\operatorname{inv}}(V)_{\textbf{d}}\).

Finally, we define \(W^{(2)}\) to compute the sum in equation 90. In particular, for \(X=[x_{1},\dots,x_{k}]\in\mathbb{R}^{\tilde{D}n^{\prime}\times k}\) we write \(W^{(2)}X=[W_{1}^{(2)}x_{1},\dots,W_{k}^{(2)}x_{k}]\), where \((W_{b}^{(2)}x_{b})_{a}=\sum_{\textbf{d}}x_{i_{\textbf{d}},b}\). It can be seen that with these definitions of \(W^{(2)},W^{(1)},\) and \(q_{\operatorname{inv}}\), we have written \(p\) in the desired form.

### Sign Invariant Polynomials and SignNet

For completeness, here we state the form of the sign invariant polynomials \(p:\mathbb{R}^{n\times k}\to\mathbb{R}\) on inputs \(V=[v_{1},\ldots,v_{k}]\in\mathbb{R}^{n\times k}\). The derivation very closely follows that of the sign equivariant polynomials from \(\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) in Appendix C.3, so we omit this derivation.

**Proposition 11**.: _A polynomial \(p:\mathbb{R}^{n\times k}\to\mathbb{R}\) is sign invariant if and only if it can be written_

\[p(V)=\sum_{d_{i,j}=0}^{D}\textbf{W_{d}}\prod_{i=1}^{n}\prod_{j=1}^{k}V_{i,j}^{d _{i,j}},\] (92)

_where \(\textbf{W_{d}}\neq 0\) for \(\textbf{d}=(d_{1,1},\ldots,d_{n,1},d_{1,2},\ldots,d_{n,k})\) only if \(\sum_{i=1}^{n}d_{i,j}\) is even for each column \(j\in[k]\)._

_In particular, \(p\) is sign invariant if and only if there is a polynomial \(q:\mathbb{R}^{n\times n\times k}\to\mathbb{R}\) such that \(p(V)=q([V_{i_{1},j}\cdot V_{i_{2},j}]_{i_{1}\in[n],i_{2}\in[n],j\in[k]})\)._

The polynomials \(V\mapsto V_{i_{1},j}\cdot V_{i_{2},j}\) for \(i_{1},i_{2}\in[n]\) and \(j\in[k]\) are thus generators of the ring of sign invariant polynomials from \(\mathbb{R}^{n\times k}\to\mathbb{R}\).

Notably, Lim et al. (2023) propose universal sign invariant neural architectures, but do not characterize or otherwise use the sign invariant polynomials. Instead, their proof of universality uses topological constructions and shows that all sign invariant continuous functions can be decomposed in a simple form--namely, \(\rho([\phi(v_{i})+\phi(-v_{i})]_{i=1,\ldots,k})\) for continuous functions \(\rho\) and \(\phi\). Our characterization of sign invariant polynomials provides another path to developing and analyzing the expressive power of sign invariant architectures.

In particular, we can give an alternative proof for the universality of SignNet.

**Proposition 12** (Universality of SignNet).: _Let \(f:\mathcal{X}\subseteq\mathbb{R}^{n\times k}\to\mathbb{R}\) be a continuous sign invariant function on a compact domain \(\mathcal{X}\), and let \(\epsilon>0\). Then there exists a continuous \(\rho:\mathbb{R}^{n^{2}k}\to\mathbb{R}\) and continuous \(\phi:\mathbb{R}^{n}\to\mathbb{R}^{n^{2}}\) such that \(|f(V)-\rho([\phi(v_{i})+\phi(-v_{i})]_{i=1,\ldots,k})|<\epsilon\) for all \(V\in\mathcal{X}\)._

Proof.: First, let \(p\) be a sign invariant polynomial that approximates \(f\) to within \(\epsilon\) on \(\mathcal{X}\). Then using Proposition 11, let \(q\) be a polynomial such that \(p(V)=q([V_{i_{1},j}\cdot V_{i_{2},j}]_{i_{1}\in[n],i_{2}\in[n],j\in[k]})\).

Define \(\phi:\mathbb{R}^{n}\to\mathbb{R}^{n^{2}}\) to map a \(v\in\mathbb{R}^{n}\) to the vector of pairwise products of elements in \(v\) scaled by 1/2, that is

\[\phi(v)=\frac{1}{2}\mathrm{vec}(vv^{\top})\] (93)

Then \(\phi(v)+\phi(-v)\) is equal to the vector of pairwise products of \(v\). Finally, we let \(\rho=q\), which gives that

\[p(V)=\rho([\phi(v_{i})+\phi(-v_{i})]_{i=1,\ldots,k}),\] (94)

and hence

\[|f(V)-\rho([\phi(v_{i})+\phi(-v_{i})]_{i=1,\ldots,k})|=|f(V)-p(V)|<\epsilon\] (95)

for all \(V\in\mathcal{X}\). 

Given the form of the sign invariant polynomials, this proof is quite simple. However, it is technically weaker than the result of Lim et al. (2023), as they invoke the Strong Whitney Embedding Theorem and only require \(\phi\) to map to \(\mathbb{R}^{2n}\) instead of \(\mathbb{R}^{n^{2}}\). Still, further arguments could probably reduce the dimension required to about \(2n\) in this polynomial-based proof; as the Gram matrix \(vv^{\top}\) is rank one, it can be recovered almost always from about \(2n\) of its entries (Pimentel-Alarcon et al., 2016).

## Appendix D Sign Equivariant Architecture Universality

In this section, we prove Proposition 3 on the universality of our proposed sign equivariant architectures, which we restate here:

**Proposition 3**.: _Functions of the form \(v\mapsto v\odot\mathrm{MLP}(|v|)\) universally approximate continuous sign equivariant functions \(f:\mathbb{R}^{k}\to\mathbb{R}^{k}\)._

_Compositions \(f_{2}\circ f_{1}\) of functions \(f_{l}\) as in equation 6 universally approximate continuous sign equivariant functions \(f:\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\)._

We prove the two statements of the proposition in the next two subsections.

### Universality for functions \(\mathbb{R}^{k}\to\mathbb{R}^{k}\)

Proof.: Let \(\mathcal{X}\subseteq\mathbb{R}^{k}\) be a compact set, let \(\epsilon>0\), and let \(f_{\mathrm{target}}:\mathcal{X}\to\mathbb{R}^{k}\) be a continuous sign equivariant function that we wish to approximate within \(\epsilon\). Choose a sign equivariant polynomial \(p\) that approximates \(f_{\mathrm{target}}\) to within \(\epsilon/2\) on \(\mathcal{X}\). By compactness, we can choose a finite bound \(B>0\) such that \(|v_{i}|<B\) for all \(v\in\mathcal{X}\).

By Proposition 9, we can write \(p(v)_{l}=v_{l}\cdot\sum_{d_{1},\ldots,d_{k}=0}^{D}\textbf{W}_{d_{1},\ldots,d_{ k}}v_{1}^{2d_{1}}\cdots v_{k}^{2d_{k}}\). By the universal approximation theorem for multilayer perceptrons, we can choose a \(\mathrm{MLP}:\mathcal{X}\to\mathbb{R}^{k}\) such that approximates \(q(v)=\sum_{d_{1},\ldots,d_{k}=0}^{D}\textbf{W}_{d_{1},\ldots,d_{k}}v_{1}^{2d_ {1}}\cdots v_{k}^{2d_{k}}\) up to \(\epsilon/(2B)\). Note that \(q(|v|)=q(v)\), so \(v\mapsto\mathrm{MLP}(|v|)\) also approximates \(q\) within \(\epsilon/(2B)\) accuracy.

Thus, for all \(v\in\mathcal{X}\), we have that

\[|f(v)_{i}-p(v)_{i}| =|v_{i}\cdot\mathrm{MLP}(|v|)_{i}-v_{i}\cdot\sum_{d=1}^{D}\textbf {W}_{d_{1},\ldots,d_{k}}v_{1}^{2d_{1}}\cdots v_{k}^{2d_{k}}|\] (96) \[=|v_{i}||\mathrm{MLP}(|v|)_{i}-\sum_{d=1}^{D}\textbf{W}_{d_{1}, \ldots,d_{k}}v_{1}^{2d_{1}}\cdots v_{k}^{2d_{k}}|\] (97) \[\leq B\cdot|\mathrm{MLP}(|v|)_{i}-\sum_{d=1}^{D}\textbf{W}_{d_{1},\ldots,d_{k}}v_{1}^{2d_{1}}\cdots v_{k}^{2d_{k}}|\] (98) \[<\epsilon/2,\] (99)

so \(\left\lVert f-p\right\rVert_{\infty}<\epsilon/2\) on \(\mathcal{X}\) and we are done by the triangle inequality. 

### Universality for functions \(\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\)

Recall that each layer of our sign equivariant network from \(\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) takes the form

\[f_{l}(V)=[W_{1}^{(l)}v_{1},\ldots,W_{k}^{(l)}v_{k}]\odot\mathrm{SignNet}_{l}(V).\]

Proof.: Let \(\mathcal{X}\subseteq\mathbb{R}^{n\times k}\) be compact, and let \(f_{\mathrm{target}}:\mathcal{X}\to\mathbb{R}^{n^{\prime}\times k}\) be a continuous sign equivariant function that we wish to approximate. Since \(\mathcal{X}\) is compact, we can choose a finite bound \(B>0\) such that \(|V_{ij}|<B\) for all \(V\in\mathcal{X}\). Let \(p:\mathcal{X}\subseteq\mathbb{R}^{n\times k}\to\mathbb{R}^{n^{\prime}\times k}\) be a sign equivariant polynomial that approximates \(f_{\mathrm{target}}\) up to \(\epsilon/2\) accuracy. Using Proposition 10, we can write

\[p(V)_{a,b}=\sum_{d_{i,j}=0}^{D}\textbf{W}_{\textbf{d}}^{(a,b)}V_{i_{\textbf{d} },b}\cdot p_{\mathrm{inv}}(V)_{\textbf{d}},\]

for some sign invariant polynomials \(p_{\mathrm{inv}}(V)_{\textbf{d}}\). We will have one network layer \(f_{1}\) approximate the summands, and have the second network layer \(f_{2}\) compute the sum.

First, we absorb the coefficients \(\textbf{W}_{\textbf{d}}^{(a,b)}\) into the sign invariant part, by defining the sign invariant polynomial \(q_{\mathrm{inv}}(V)_{\textbf{d},a,b}=\textbf{W}_{\textbf{d}}^{(a,b)}p_{\mathrm{ inv}}(V)_{\textbf{d}}\), so we can write

\[p(V)_{a,b}=\sum_{d_{i,j}=0}^{D}V_{i_{\textbf{d}},b}\cdot q_{\mathrm{inv}}(V)_{ \textbf{d},a,b}.\]

Now, let \(d_{\mathrm{hidden}}\in\mathbb{N}\) denote the number of all possible **d** that appear in the sum, multiplied by \(n^{\prime}\). We define \(f_{1}:\mathcal{X}\to\mathbb{R}^{d_{\mathrm{hidden}}\times k}\) as follows. As SignNet [11] universally approximatessign invariant functions on compact sets, we can let \(\mathrm{SignNet}_{1}:\mathcal{X}\rightarrow\mathbb{R}^{d_{\mathrm{hidden}}\times k}\) be a SignNet that approximates \(q_{\mathrm{inv}}(V)\) up to \(\epsilon/(2B)\) accuracy, so

\[|\mathrm{SignNet}_{1}(V)_{(\mathbf{d},a),b}-q_{\mathrm{inv}}(V)_{\mathbf{d},a,b }|<\frac{\epsilon}{2B\cdot d_{\mathrm{hidden}}}.\] (100)

For \(b\in[k]\), we also define the weight matrices \(W_{b}^{(1)}\in\mathbb{R}^{d_{\mathrm{hidden}}\times n}\) of the layer by letting the \((\mathbf{d},a)\)th row \((W_{b}^{(1)})_{(\mathbf{d},a),:}\) for any \(a\in[n]\) only be nonzero in the \(i_{\mathbf{d}}\)th index, where it is equal to 1. Thus,

\[(W_{b}^{(1)}v_{b})_{(\mathbf{d},a)}=V_{i_{\mathbf{d}},b}.\] (101)

Hence, the first layer takes the form

\[f_{1}(V)_{(\mathbf{d},a),:}=\left[V_{i_{\mathbf{d}},1}\cdot\mathrm{SignNet}_{ 1}(V)_{(\mathbf{d},a),1}\quad\ldots\quad V_{i_{\mathbf{d}},k}\cdot\mathrm{ SignNet}_{1}(V)_{(\mathbf{d},a),k}\right]\in\mathbb{R}^{k}.\] (102)

Now, for the second layer, we let \(\mathrm{SignNet}(V)_{i,j}=1\) for all \(i\in[n],j\in[k]\), which can be represented exactly. Then for each column \(b\in[k]\) we will define weight matrices \(W_{b}^{(2)}\) such that \((W_{b}^{(2)})_{a,(\mathbf{d},i)}=1\) if \(a=i\) and is \(0\) otherwise. Then we can see that

\[f_{2}\circ f_{1}(V)_{a,b}=\sum_{\mathbf{d}}V_{i_{\mathbf{d}},b}\cdot\mathrm{ SignNet}_{1}(V)_{(d,a),b}.\] (103)

To see that this approximates the polynomial \(p\), for any \(V\in\mathcal{X}\) we can bound

\[|p(V)_{a,b}-f_{2}\circ f_{1}(V)_{a,b}| =\left|\sum_{\mathbf{d}}V_{i_{\mathbf{d}},b}\cdot\left(q_{\mathrm{ inv}}(V)_{\mathbf{d},a,b}-\mathrm{SignNet}_{1}(V)_{(\mathbf{d},a),b}\right)\right|\] (104) \[\leq\sum_{\mathbf{d}}|V_{i_{\mathbf{d}},b}|\big{|}\big{(}q_{ \mathrm{inv}}(V)_{\mathbf{d},a,b}-\mathrm{SignNet}_{1}(V)_{(\mathbf{d},a),b} \big{)}\big{|}\] (105) \[\leq B\sum_{\mathbf{d}}\big{|}\big{(}q_{\mathrm{inv}}(V)_{\mathbf{ d},a,b}-\mathrm{SignNet}_{1}(V)_{(\mathbf{d},a),b}\big{)}\big{|}\] (106) \[<B\sum_{\mathbf{d}}\frac{\epsilon}{2Bd_{\mathrm{hidden}}}\] (107) \[\leq\frac{\epsilon}{2}\] (108)

By the triangle inequality, \(f_{2}\circ f_{1}\) is \(\epsilon\)-close to \(f_{\mathrm{target}}\), so we are done.

## Appendix E Experimental Details

### Miscellaneous Experimental Details

We ran the experiments on a HPC server with CPUs and GPUs. Each experiment was run on a single NVIDIA V100 GPU with 32GB memory. The runtimes for some of our experiments are included in the main paper. Our codes for our models and experiments will be open-sourced and permissively licensed.

### Link Prediction in Nearly Synthetic Graphs

The base graphs \(H\) we generate are Erdos-Renyi or Barabasi-Albert graphs with 1000 nodes. We use NetworkX [Hagberg et al., 2008] to generate and process the graphs. The Erdos-Renyi graphs have edge probability \(p=.05\) and the Barabasi-Albert graphs have \(m=20\) new edges per new node. Let \(V=[v_{1},\ldots,v_{k}]\) be Laplacian eigenvectors of the graph. We take \(k=16\) in these experiments. The unlearned decoder baseline simply takes the predicted probability of a link between \(i\) and \(j\) to be proportional to the dot product of the eigenvectors embeddings of node \(i\) and node \(j\); this has no learnable parameters. In other words, the node embeddings \(z_{i}\) and \(z_{j}\) are taken to be \(V_{i,:}\) and \(V_{j,:}\) respectively, and the edge prediction is \(z_{i}^{\top}z_{j}\). The learned decoder baseline takes the same \(z_{i}\) and \(z_{j}\)but takes the edge prediction to be \(\operatorname{MLP}(z_{i}\odot z_{j})\). Every other method learns node embeddings \(z_{i}\) and \(z_{j}\), and takes the edge prediction to be \(z_{i}^{\top}z_{j}\).

Each model is restricted to around 25,000 learnable parameters (besides the Unlearned Decoder, which has no parameters). We train each method for 100 epochs with an Adam optimizer (Kingma and Ba, 2015) at a learning rate of.01. The train/validation/test split is 80%/10%/10%, and is chosen uniformly at random.

### Details on n-body Simulations

We follow the experimental setting and build on the code of Puny et al. (2022) (no license as far as we can tell) for the n-body learning task. The code for generating the data stems from Kipf et al. (2018) (MIT License) and Fuchs et al. (2020) (MIT License). There are 3000 training trajectories, 2000 validation trajectories, and 2000 test trajectories. We modify the data generation code to apply to general dimensions \(d>3\). We do not change any of the scaling factors in doing so. For each dimension \(d\), we use the same hyperparameters for both the frame averaging model and the sign equivariant model.

In Table

### Node Classification on CLUSTER

In Section 4.3, we show results for the node classification task CLUSTER (Dwivedi et al., 2022), where the task is to cluster nodes in graphs drawn from Stochastic Block Models (Abbe, 2017). Models are restricted to a 100k learnable parameter budget. We largely follow the experimental setting of Rampasek et al. (2022), except we report results for the eigenvector based methods on 5 runs instead of 10.

We test several eigenvector based methods within the GraphGPS framework and codebase (Rampasek et al., 2022) (MIT License), which is a state of the art Transformer / GNN hybrid. Firstly, we make use of the PEG style GraphGPS, which means that the MPNN in the \(l\)th GraphGPS layer takes as edge features \(e_{ij}^{(l)}=\left\|V_{i}^{(l)}-V_{j}^{(l)}\right\|^{2}\), where \(V_{i}^{(l)}\in\mathbb{R}^{k}\) is the eigenvector embedding of node \(i\) in layer \(l\). This is fully \(O(k)\) invariant (which is much stricter than sign / basis invariance), so we relax this to just be sign invariant in our model by learning a diagonal matrix \(D^{(l)}\) such that \(e_{ij}^{(l)}={V_{i}^{(l)}}^{\top}D^{(l)}V_{j}^{(l)}\). Also, the standard GraphGPS only updates eigenvector representations (in a non-equivariant manner) before most of the neural network modules. When we add our sign equivariant model, we instead update eigenvector representations within each GraphGPS layer via \(V^{(l)}=f_{\theta}^{(l)}(V^{(l-1)})\) for a sign equivariant \(f_{\theta}^{(l)}:\mathbb{R}^{n\times k}\rightarrow\mathbb{R}^{n\times k}\).

\begin{table}
\begin{tabular}{l c} \hline \hline Model & Test MSE \\ \hline Linear &.0819 \\ \(SE(3)\) Transformer (Fuchs et al., 2020) &.0244 \\ TFN (Thomas et al., 2018) &.0155 \\ Radial Field (Köhler et al., 2020) &.0104 \\ EGNN (Satorras et al., 2021) &.0071 \\ FA-GNN (Puny et al., 2022) &.0057 \\ CN-GNN (Kaba et al., 2023) & **.0043** \\ \hline Sign Equivariant (Ours) &.0065 \\ \hline \hline \end{tabular}
\end{table}
Table 5: n-body simulation results for dimension \(d=3\). Lower MSE is better. Results are from (Satorras et al., 2021; Puny et al., 2022; Kaba et al., 2023).