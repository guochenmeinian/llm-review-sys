Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction

 Zeshuai Deng\({}^{1}\)

Zhuokun Chen\({}^{1}\)

Shuaicheng Niu\({}^{1}\)

Thomas H. Li\({}^{5}\)

Bohan Zhuang\({}^{3}\)

Mingkui Tan\({}^{1}\)

Corresponding author. Email: mingkuitan@scut.edu.cn, bohan.zhuang@gmail.com\({}^{1}\)South China University of Technology, \({}^{2}\)Pazhou Lab, \({}^{3}\)ZIP Lab, Monash University,

\({}^{4}\)Key Laboratory of Big Data and Intelligent Robot, Ministry of Education,

\({}^{5}\)Peking University Shenzhen Graduate School

###### Abstract

Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, _e.g._, Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (_e.g._, blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.

## 1 Introduction

Image super-resolution (SR) aims to reconstruct plausible high-resolution (HR) images from the given low-resolution (LR) images, which is widely applied in microscopy [42, 44], remote sensing [17, 35] and surveillance [40, 56]. Most previous SR methods [11, 31, 60, 8] hypothesize that the LR images are downsampled from HR images using a predefined degradation model, _e.g._, Bicubic down-sampling. However, due to the diverse imaging sensors and multiple propagations on the Internet, real-world images may contain different degradation types (_e.g._, Gaussian blur, Poisson noise, and JPEG artifact) [32, 52, 53, 25]. Besides, the realistic degradations of real-world imagesmay dynamically change, which are often different from the training one, limiting the performance of pre-trained SR models in dynamically changing test-time environments.

Recently, zero-shot SR methods [43, 9, 12] are proposed to train an image-specific SR model for each test image to alleviate the degradation shift issue. For example, ZSSR [43] uses a predefined/estimated degradation model to generate an image with a lower resolution for each test image. With this downsampled image and the test image, they can train an image-specific SR model to super-resolve the test image. Moreover, DualSR [12] estimates the degradation model and trains the SR model simultaneously to achieve better performance. However, these methods usually require thousands of iterations to estimate the degradation model or train the SR model, which is very time-consuming. Thus, these methods cannot handle real-world test images with rapidly changing domain shifts.

To reduce the inference time of zero-shot methods, some recent works [45, 41] introduce meta-learning [13] to accelerate the adaptation of the SR model, which still requires a predefined/estimated degradation model to construct paired data to update the model. However, most degradation estimation methods [2, 30] focus on the estimation of one degradation type, which limits the adaptation of SR models to test images with other degradations. Recently, test-time adaptation (TTA) methods [46, 47, 57, 38, 48, 39] are proposed to quickly adapt the pre-trained model to the test data in target domain without accessing any source training data. These methods often use simple augmentation operations (_e.g._, rotation or horizontal flip) on the test image, and construct the pseudo label as the average of the predicted results [57, 48]. For image SR, the pseudo-HR image constructed using this scheme [57, 48] may still contain the degradation close to the test image (_e.g._, Gaussian blur). With such a pseudo-HR image, the adapted SR model may not be able to learn how to remove the degradation from the test image (see results in Table 1). Therefore, how to quickly and effectively construct the paired data to encourage the SR model to remove the degradation is still an open question.

In this paper, we propose a super-resolution test-time adaptation framework (SRTTA) to adapt a trained super-resolution model to target domains with unknown degradations, as shown in Figure 1. When the degradation shift issue occurs, the key challenge is how to quickly and effectively construct (pseudo) paired data to adapt SR models to the target domain without accessing any clean HR images. To this end, we propose a second-order degradation scheme to construct (pseudo) paired data. Specifically, with a pre-trained degradation classifier, we quickly identify the degradation type from the test images and randomly generate a set of degradations to obtain the second-order degraded images. The paired data, which consists of the second-order degraded images and the test image, enables a rapid adaptation of SR models to the target domain with different degradations. To facilitate the learning of reconstruction, we design a second-order reconstruction loss to adapt the pre-trained model using the paired data in a self-supervised manner. After fast adaptation using our method, the SR model is able to learn how to remove this kind of degradations and generate plausible HR images. Moreover, we also design an adaptive parameter preservation strategy to preserve the knowledge of the pre-trained model to avoid the catastrophic forgetting issue in long-term adaptation. Last but not least, we use eight different degradations to construct two new benchmarks, named DIV2K-C and DIV2K-MC, to comprehensively evaluate the practicality of our method. Experimental results on both our synthesized datasets and several real-world datasets demonstrate that our SRTTA is able to quickly adapt the SR model to the test-time images and achieve an impressive improvement.

Our main contributions are summarized as follows:

* **A novel test-time adaptation framework for image super-resolution**: We propose a super-resolution test-time adaptation (SRTTA) framework to adapt any pre-trained SR models to different target domains during the test time. Without accessing any ground-truth HR images, our SRTTA is applicable to practical scenarios with unknown degradation in a self-supervised manner.
* **A fast data construction scheme with second-order degradation**: We use a pre-trained classifier to identify the degradation type for a test image and construct the paired data using our second-order degradation scheme. Since we do not estimate the parameters of the degradation model, our scheme enables a rapid model adaptation to a wide range of degradation shifts.
* **New test datasets with eight different domains**: We construct new test datasets named DIV2K-C and DIV2K-MC, which contain eight common degradations, to evaluate the practicality of different SR methods. Experimental results on both synthesized datasets and real-world datasets demonstrate the superiority of our SRTTA, _e.g._, 0.84 dB PSNR improvement on DIV2K-C over ZSSR [43].

## 2 Related Work

**Real-world super-resolution.** To alleviate the domain shift issues, GAN-based methods [54; 5; 24; 36] tend to learn the degradation model of the real-world images in the training stage. These methods often train a generator that explicitly learns the degradation model of real-world images. Besides, some methods [55; 60; 49] try to enumerate most of the degradation models that can be encountered in real-world applications. Based on the estimated/predefined degradation models, these methods can generate LR images whose distribution is similar to real-world images. However, due to the complex and unknown processing of real-world images, it is hard to mimic all types of degradation during the training phase. Instead, some existing methods [15; 2; 22; 21; 34] try to estimate the image-specific degradation model during the test time, which helps to reconstruct more plausible HR images. For instance, optimization-based methods [15; 22; 21] estimate the blur kernel and SR image together in an iterative manner. However, these methods cannot generate satisfactory results when the test images contain different types of degradation (_e.g._, Poisson noise and JPEG artifact) [32]. Thus, these methods still suffer from the domain shift on test images with unknown degradation.

**Zero-shot super-resolution.** Zero-shot methods [43; 9; 45; 41; 12] aim to train an image-specific SR model for each test image to alleviate the domain shift issue. These methods [43; 12] use a predefined/estimated degradation model to generate an image with a lower resolution from each test image. To estimate the image-specific degradation in a zero-shot manner, KernelGAN [2] utilizes the internal statistics of each test image to learn the degradation model specifically and then uses ZSSR [43] to train an SR model with the estimated degradation. However, these methods usually require a lot of time to estimate the degradation model or train the SR model. MZSR [45] and MLSR [41] are proposed to reduce the number of iteration steps for each test image during test time. Recently, DDNM [51] was proposed to use a pre-trained diffusion model to ensure the generated images obey the distribution of natural images. However, these methods still require a predefined (Bicubic downsampling) or an estimated degradation model. The predefined Bicubic degradation suffers from the domain shift due to its difference from the underlying degradation of real-world images. The estimation methods [2; 30] may focus on the estimation of a single degradation type (_e.g._, blur) while ignoring other degradation. Thus, these methods often result in unsatisfactory HR images for the test images with different degradation types [32]. In this paper, we use a degradation classifier to quickly recognize the degradation type and randomly generate the degradations with this type. Therefore, we do not need to estimate the degradation model, which is time-saving.

**Test-time adaptation.** Recently, test-time adaptation (TTA) methods [46; 47; 57; 38; 48; 6; 39; 59] have been proposed to alleviate the domain shift issue by online updating the pre-trained model on the test data. TTT [46] uses an auxiliary head to learn the test image information from the self-supervised task. Tent [47] proposed to adapt the pre-trained model with entropy-based loss in an unsupervised manner. CoTTA [48] uses a weight-averaged pseudo-label over training steps to guide the pre-trained model adaptation. However, these methods are mainly developed for image classification and may ignore the characteristics of image super-resolution. Thus, these methods may not be effective in adapting the SR model to remove the degradation from the test image. In this paper, we focus on the image SR task and address the degradation shift issue with our SRTTA framework.

## 3 Preliminary and Problem Definition

**Notation.** Without loss of generality, let \(\mathbf{y}\) be a clean high-resolution (HR) image and \(\mathbf{x}_{c}\) be the clean low-resolution (LR) image downsampled from \(\mathbf{y}\) using Bicubic interpolation, _i.e._, \(\mathbf{x}_{c}=\mathbf{y}\downarrow_{s}\), where \(s\) is the scale factor of Bicubic downsampling. Let \(\mathbf{x}\) denote a real-world test image degraded from \(\mathbf{y}\), _i.e._, \(\mathbf{x}=\mathbf{D}(\mathbf{y})\), where \(\mathbf{D}(\cdot)\) is the degradation process. We use \(\mathbf{x}_{sd}\) to denote the LR image that is further degraded from the real-world image \(\mathbf{x}\). In this paper, we call the test image \(\mathbf{x}\) as a **first-order** degraded image and the image \(\mathbf{x}_{sd}\) degraded from \(\mathbf{x}\) as a **second-order** degraded image. \(f_{\theta}(\cdot)\) is a super-resolution (SR) model with parameters \(\theta\).

**Image degradation.** The degradation process of real-world test images can be modeled by a classical degradation model \(\mathbf{D}(\cdot)\)[33; 49]. Formally, let \(\mathbf{k}\) be a blur kernel, \(\mathbf{n}\) be an additive noise map and \(q\) be the quality factor of \(JPEG\) compression, the degraded image \(\mathbf{x}\) is defined by

\[\mathbf{x}=\mathbf{D}(\mathbf{y})=[(\mathbf{y}\otimes\mathbf{k})\downarrow_{s }+\mathbf{n}]_{JPEG_{q}},\] (1)

where \(\otimes\) denotes the convolution operation, \(\downarrow_{s}\) denotes the downsampling with a scale factor of \(s\), and \(JPEG_{q}\) denotes the JPEG compression with the quality factor \(q\). Similarly, the second-orderdegraded image \(\mathbf{x}_{sd}\) can be formulated as

\[\mathbf{x}_{sd}=\mathbf{D}(\mathbf{D}(\mathbf{y}))=\mathbf{D}(\mathbf{x})=[( \mathbf{x}\otimes\mathbf{k})\downarrow_{s}+\mathbf{n}]_{JPEG_{q}}.\] (2)

**Degradation shift between training and testing.** Existing SR methods [31; 60; 16] often construct paired HR-LR training images by either collecting from the real world or synthesizing LR images from HR images via a pre-defined degradation model, _i.e._, Bicubic down-sampling. However, due to diverse camera sensors and the unknown processing on the Internet, the degradation process of real-world test images may differ from that of training images, _called domain shifts_[27; 48; 32]. In these cases, the SR model often fails to generate satisfactory HR images.

**Motivation and challenges.** Though recently some blind SR methods [12; 43] have been proposed to address the degradation shift issue, they still suffer from two key limitations: low efficiency and narrow focus on a single degradation type, _e.g._, blur degradation. In this work, we seek to resolve these issues by directly learning from the shifted testing LR image at test time, which poses two major challenges: 1) How to quickly and effectively construct the (pseudo) paired data to adapt SR models to test domains with unknown degradations? and 2) How to design a generalized test-time learning framework that facilitates the removal of various types of degradation, considering that we have only a low-resolution test image at our disposal?

## 4 Efficient Test-Time Adaptation for Image Super-Resolution

In this section, we illustrate our proposed super-resolution test-time adaptation (SRTTA) framework that is able to quickly adapt the pre-trained SR model to real-world images with different degradations. The overall framework and pipeline are shown in Figure 1 and Algorithm 1.

Given a test image \(\mathbf{x}\), we first construct the paired data using our second-order degradation scheme. Specifically, we use a pre-trained degradation classifier to recognize the degradation type for the test image. Based on the predicted degradation type, we randomly generate a set of degradation (_e.g._, a set of blur kernels \(\mathbf{k}\)) and use them to construct a set of paired data \(\{\mathbf{x}_{sd}^{i},\mathbf{x}\}_{i=1}^{N}\). With the paired data, we adapt the pre-trained SR model to remove the degradation from the test image. Notably, before performing the test-time adaptation, we freeze the important parameters to preserve the knowledge of the pre-trained model to alleviate the forgetting problem in long-term adaptation. After adaptation, we use the adapted model to generate the corresponding HR image for the test image.

Figure 1: An overall illustration of the proposed super-resolution test-time adaptation (SRTTA) framework. Given a test image \(\mathbf{x}\), we use a pre-trained degradation classifier to predict the degradation type \(C(\mathbf{x})\), _e.g._, blur, noise, and JPEG degradation. Based on the predicted degradation type \(C(\mathbf{x})\), we construct a set of paired data \(\{\mathbf{x}_{sd}^{i},\mathbf{x}\}_{i=1}^{N}\) and adapt the SR model with our adaptation loss \(\mathcal{L}_{a}\) and \(\mathcal{L}_{s}\). When test samples are clean images, we directly use the frozen pre-trained SR model to super-resolve these clean images without adaptation.

### Adaptive Data Construction with Second-Order Degradation

In this part, we propose a novel second-order degradation scheme to effectively construct paired data, enabling the fast adaptation of SR models to the target domain with different degradations.

Unlike existing methods [43; 2; 30], we consider more degradation types and avoid estimating the degradation model. Existing methods [43; 2; 30] mainly focus on precisely estimating the blur kernels when constructing the lower-resolution images (second-order degraded images), which is time-consuming. Instead, we use a pre-trained degradation classifier to quickly identify the degradation types (blur, noise, and JPEG) of test images, and then we construct the second-order degraded images based on the predicted degradation types. Without the time-consuming degradation estimation process, our scheme enables a fast model adaptation to a wide range of degradation shifts.

**Adaptive data construction.** In this part, we design an adaptive data construction method to obtain the second-order degraded images \(\mathbf{x}_{sd}^{i}\). Specifically, based on the classical degradation model in Eqn. (1), we train a multi-label degradation classifier \(C(\cdot)\) to predict the degradation types for each test image, including blur, noise and JPEG degradation types, denoted by \(c_{b}\), \(c_{n}\) and \(c_{j}\in\{0,1\}\), respectively. With the predicted degradation types, we randomly generate \(N\) degradations and construct a set of second-order degraded images \(\{\mathbf{x}_{sd}^{i}\}_{i=1}^{N}\), which can be formulated as

\[\begin{split}&\mathbf{x}_{sd}=D(\mathbf{x},C(\mathbf{x}))=D_{j}(D_ {b}(\mathbf{x},c_{b})+D_{n}(c_{n}),c_{j}),\\ & D_{b}(\mathbf{x},c_{b})=c_{b}(\mathbf{x}\otimes\mathbf{k})+(1- c_{b})\mathbf{x},\ D_{n}(c_{n})=c_{n}\mathbf{n},\\ & D_{j}(\mathbf{x},c_{j})=c_{j}JPEG_{q}(\mathbf{x})+(1-c_{j}) \mathbf{x},\end{split}\] (3)

where the blur kernel \(\mathbf{k}\), noise map \(\mathbf{n}\) and quality factor \(q\) are randomly generated using a similar recipe of Real-ESRGAN [49]. Unlike previous methods [43; 2], we do not further downsample the test image \(\mathbf{x}\) when constructing \(\mathbf{x}_{sd}\), since the pretrained SR model has learned the upsampling function (the inverse function of downsampling) during the training phase. Due to the page limit, we put more details in the supplementary materials.

Since the pre-trained SR model has been well-trained on the clean domain (Bicubic downsampling), we simply ignore adapting the clean images in test-time. For these images, we use the pre-trained SR model to super-resolve them, _i.e._, \(\hat{\mathbf{y}}=\bar{f}_{\theta^{0}}(\mathbf{x})\) when \(c_{b}=c_{n}=c_{j}=0\).

### Adaptation with Second-Order Reconstruction

In our SRTTA framework, we design a self-supervised adaptation loss and an adaptation consistency loss to update the pre-trained SR models to test images with degradation.

**Self-supervised adaptation.** To adapt the pre-trained model to remove the degradation, we design a self-supervised adaptation loss based on the Charbonnier penalty function [4; 28]. Specifically, we encourage the SR model to reconstruct the test images \(\mathbf{x}\) from the second-order degraded images \(\mathbf{x}_{sd}\) at the feature level, which can be formulated as

\[\mathcal{L}_{s}(\mathbf{x},\mathbf{x}_{sd})=\sqrt{(f_{\theta}^{l}(\mathbf{x} )-f_{\theta}^{l}(\mathbf{x}_{sd}))^{2}+\epsilon},\] (4)where \(f^{l}_{\theta}(\cdot)\) denotes the output features of the \(l\)-th layer. We simply set \(f^{l}_{\theta}(\cdot)\) to be the output features of the second-to-last convolution layer. \(\epsilon\) is a small positive value that is set to \(10^{-3}\) empirically.

**Consistency maximization.** To keep the model consistent across adaptation, we design an adaptation consistency loss to encourage the output of the adapted model to be close to that of the pre-trained model, which is formulated as

\[\mathcal{L}_{a}(\mathbf{x},\mathbf{x}_{sd})=\sqrt{(f^{l}_{\theta ^{0}}(\mathbf{x})-f^{l}_{\theta}(\mathbf{x}_{sd}))^{2}+\epsilon},\] (5)

where \(f^{l}_{\theta^{0}}(\cdot)\) denotes the output features of the \(l\)-th layer of the pre-trained SR model.

**Second-order reconstruction loss.** Our final second-order reconstruction loss consists of a self-supervised adaptation loss and an adaptation consistency loss, which is formulated as

\[\mathcal{L}=\mathcal{L}_{s}(\mathbf{x},\mathbf{x}_{sd})+\alpha \mathcal{L}_{a}(\mathbf{x},\mathbf{x}_{sd}),\] (6)

where \(\alpha\) is a balance hyperparameter (the ablation study can be found in Table 5).

### Adaptive Parameter Preservation for Anti-Forgetting

To avoid catastrophic forgetting in long-term adaptation, we propose an adaptive parameter preservation (APP) strategy to freeze the important parameters during adaptation. To select the important parameters, we evaluate the importance of each parameter using the Fisher information matrix [26; 38]. Given a set of collected clean images \(\mathcal{D}_{c}\), we design an augmentation consistency loss \(\mathcal{L}_{c}\) to compute the gradient of each parameter. Based on the gradient, we compute the diagonal Fisher information matrix to evaluate the importance of each parameter \(\theta^{0}_{i}\), which is formulated as

\[\omega(\theta^{0}_{i})=\frac{1}{|\mathcal{D}_{c}|}\sum_{\mathbf{x }_{c}\in\mathcal{D}_{c}}(\frac{\partial\mathcal{L}_{c}(\mathbf{x}_{c})}{ \partial\theta^{0}_{i}})^{2},\] (7) \[\mathcal{L}_{c}(\mathbf{x}_{c})=\sqrt{(\bar{\mathbf{y}}-f_{ \theta^{0}}(\mathbf{x}_{c}))^{2}+\epsilon},\;\;s.t.\;\;\bar{\mathbf{y}}=\frac{ 1}{8}\sum_{i=1}^{8}\mathbf{R}_{i}(f_{\theta^{0}}(\mathbf{A}_{i}(\mathbf{x}_{c} ))),\] (8)

where \(\mathbf{A}_{i}\in\{\mathbf{A}_{j}\}_{j=1}^{8}\) is an augmentation operation, which is the random combination of a 90-degree rotation, a horizontal and a vertical flip on the input image \(\mathbf{x}_{c}\). \(\mathbf{R}_{i}\) is the inverse operation of \(\mathbf{A}_{i}\) that rolls back the image \(\mathbf{A}_{i}(\mathbf{x}_{c})\) to its original version \(\mathbf{x}_{c}\). With \(\omega(\theta^{0}_{i})\), we select the most important parameters using a ratio of \(\rho\) and freeze these parameters during the adaptation. The set of selected parameters \(\mathcal{S}\) can be formulated as

\[\mathcal{S}=\{\theta^{0}_{i}|\omega(\theta^{0}_{i})>\tau_{\rho}, \theta^{0}_{i}\in\theta^{0}\},\] (9)

where \(\tau_{\rho}\) denotes the first \(\rho\)-ratio largest value obtained by ranking the value \(\omega(\theta^{0}_{i})\), \(\rho\) is a hyperparameter to control the ratio of parameters to be frozen. Note that we only need to select the set of significant parameters \(\mathcal{S}\) once before performing test-time adaptation.

## 5 Experiments

### Experimental Details

**Testing data.** Following ImageNet-C [18], we degraded 100 validation images from the DIV2K [1] dataset into eight domains. We select the eight degradation types that do not extremely change the image content, including Gaussian Blur, Defocus Blur, Glass Blur, Gaussian Noise, Poisson Noise (Shot Noise), Impulse Noise, Speckle Noise, and JPEG compression. In total, we create a new benchmark dataset **DIV2K-C**, which contains 800 images with different single degradation, to evaluate the performance of different SR methods. Besides, we further construct a dataset named **DIV2K-MC**, which consists of four domains with mixed multiple degradations, including BlurNoise, BlurJPEG, NoiseJPEG, and BlurNoiseJPEG. Specifically, test images from the BlurNoiseJPEG domain contain the combined degradation of Gaussian Blur, Gaussian Noise and JPEG simultaneously. Moreover, we also evaluate our SRTTA on real-world images from DPED [23], ADE20K [61] and OST300 [50], whose corresponding ground-truth HR images can not be found. To evaluate the anti-forgetting performance, we use a benchmark dataset Set5 [3].

**Implementation details and evaluation metric.** We evaluate our approach using the baseline model of EDSR [31] with only 1.37 M parameters for 2 \(\times\) SR. To demonstrate the effectiveness of our SRTTA, we conduct experiments in two settings, including a **parameter-reset** setting and a **lifelong** setting. In the **parameter-reset** setting, the model parameters will be reset after the adaptation of each domain, which is the **default setting** of our SRTTA. In the **lifelong** setting, the model parameters will never be reset in the long-term adaptation, in this case, we call our methods as SRTTA-lifelong. For the balance weight in Eqn. (6), we set \(\alpha\) to 1. For the ratio of parameters to be frozen, we set the \(\rho\) to 0.50. Please refer to more details in the supplementary materials.

To compare the inference times of different SR methods, we measure all methods on a TITAN XP with 12G graphics memory for a fair comparison. Due to the memory overload of HAT [8] and DDNM [51], we chop the whole image into smaller patches and process them individually for these two methods. To evaluate the performance of different methods, we use the common metrics PSNR [20] and SSIM [20] and report the results of all methods on the DIV2K-C dataset. Due to the page limit, we mainly report PSNR results and put more results in the supplementary materials.

**Compared methods.** We compare our SRTTA with several state-of-the-art methods including supervised pre-trained SR methods, blind SR methods, zero-shot SR methods, and a TTA baseline method. 1) The supervised pre-trained SR models learn super-resolution knowledge with a predefined degradation process, _i.e._, the Bicubic downsampling. These methods include SwinIR [29], IPT [7] and HAT [8], and the EDSR baseline [31]. 2) Blind SR models predict the blur kernel of test images and generate HR images simultaneously, _e.g._, DAN [21] and DCLS-SR [34]. 3) Zero-shot SR models often use a predefined/estimated degradation kernel to construct the LR-HR paired images based on the assumption of the cross-scale patch recurrence [14; 62; 37], and they use the LR-HR paired images to train/update the SR models for each test image. These methods include ZSSR [43], KernelGAN [2]+ZSSR [43], MZSR [9], DualSR [12], DDNM [51]. 4) Moreover, we implement a baseline TTA method (dubbed TTA-C) that utilizes the augmentation consistency loss \(\mathcal{L}_{c}\) in Eqn. (8) to adapt the pre-trained model, similar to MEMO [57] and CoTTA [48].

### Comparison with State-of-the-art Methods

We evaluate the effectiveness of our methods in terms of quantitative results and visual results. We report the PSNR results of our SRTTA and existing methods on the DIV2K-C dataset for \(2\times\) SR in Table 1 (more results are put in the supplementary materials). Since DAN [21] and DCLS-SR [34] are trained on paired images with Gaussian Blur degradation, they achieve the best results in Gaussian Blur and Glass Blur degradation. However, they merely achieve a limited performance on average due to the ignoring of the noise and JPEG degradations in their degradation model. Moreover, ZSSR [43] achieves state-of-the-art performance on average due to the thousands of iterations of training steps for each image. Though KernelGAN [15] estimates a more accurate blur kernel and helps to generate more plausible HR images for Gaussian Blur and Glass Blur degradation images, it is harmful to the performance of ZSSR [43] on the noise images, since the degradation model of KernalGAN [15] does not cover the noise degradation. Moreover, the baseline TTA-C may be harmful to adapting the pre-trained model to blur degradation due to the simple augmentation, resulting in a limited

\begin{table}
\begin{tabular}{l|c c c c c c c c|c} \hline \hline Method & Gaussian & Defocus & Glass & Gaussian & Poisson & Impulse & Speckle & JPEG & Mean & 
\begin{tabular}{c} GPU Time \\ (seconds/image) \\ \end{tabular} \\ \hline SwinIR [29] & 30.40 & 25.52 & 27.82 & 25.35 & 22.36 & 15.34 & 30.45 & 30.74 & 26.00 & 13.08 \\ IPT [7] & 28.93 & 24.08 & 26.39 & 22.96 & 20.08 & 13.06 & 28.27 & 28.36 & 24.02 & 55.36 \\ HAT [8] & 29.00 & 24.08 & 26.40 & 22.31 & 19.33 & 11.91 & 28.02 & 28.25 & 23.66 & 25.01 \\ DAN [51] & **34.32** & 25.58 & 31.72 & 26.36 & 23.28 & 11.46 & 30.64 & 31.08 & 26.81 & 3.10 \\ DCLS-SR [34] & 33.93 & 25.55 & **31.98** & 25.45 & 21.59 & 8.12 & 30.66 & 30.86 & 26.02 & 1.45 \\ ZSSR [43] & 29.91 & 25.54 & 27.79 & 26.79 & 24.24 & **19.14** & 30.95 & 31.01 & 26.92 & 117.65 \\ KernelGAN [15]+ZSSR & 30.18 & **25.87** & 29.01 & 21.45 & 19.32 & 17.93 & 25.07 & 26.11 & 24.37 & 231.41 \\ MZSR [45] & 30.14 & 25.54 & 28.03 & 25.94 & 23.48 & 17.05 & 30.00 & 30.49 & 26.33 & 3.34 \\ DuaIR [12] & 29.00 & 24.40 & 28.18 & 22.30 & 20.11 & 17.22 & 24.99 & 27.44 & 23.87 & 210.85 \\ DDNM [51] & 28.46 & 24.09 & 26.39 & 24.37 & 21.92 & 13.98 & 28.00 & 28.26 & 24.51 & 2,288.55 \\ EDSR [31] & 30.28 & 25.52 & 27.82 & 25.87 & 22.96 & 15.87 & 30.52 & 30.83 & 26.21 & - \\ TTA-C & 30.21 & 25.50 & 27.79 & 26.37 & 23.57 & 16.41 & 30.25 & 30.91 & 26.38 & 13.59 \\ \hline SRTTA (ours) & 31.07 & 25.86 & 29.01 & **29.65** & 26.69 & 16.15 & **32.33** & **31.30** & **27.76** & 5.38 \\ SRTTA-lifelong (ours) & 31.07 & 25.83 & 29.18 & 29.48 & **27.10** & 16.27 & 31.71 & 31.22 & 27.73 & 5.38 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with existing state-of-the-art SR methods on DIV2K-C for 2\(\times\) SR regarding PSNR (\(\uparrow\)) and inference time (second/image), which is measured on a single TITAN XP GPU. The bold number indicates the best result and the underlined number indicates the second best result.

adaptation performance. Instead, our methods achieve the best performance in terms of PSNR on average. For quality comparison, we provide the visual results of our SRTTA and the compared methods in Figure 2. As shown in Figure 2, our SRTTA is able to remove the degradation and reconstruct clearer SR images with less noise or fewer JPEG artifacts.

**Comparison of inference time.** Moreover, we also compare the inference time of different methods in Table 1. Due to the additional time for test-time adaptation, our SRTTA cannot achieve the best inference speed. However, those methods with less inference time are mostly trained on domains with Gaussian blur degradation only, such as DAN [21], DCLS-SR [34], and MZSR [45], which still suffer from the domain shift issue under noise or JPEG degradation. Instead, with comparable efficiency, our SRTTA achieves an impressive improvement on average for all domains (see results in Table 1). In conclusion, our SRTTA achieves a better tradeoff between performance and efficiency.

**More results on test domains with mixed multiple degradations.** In this part, we evaluate our SRTTA on DIV2K-MC to further investigate the effectiveness of SRTTA under mixed multiple degradations. In Table 2, our SRTTA achieves the best performance on four domains with different mixed degradations, e.g., 26.47 (ZSSR) \(\rightarrow\) 28.48 (our SRTTA-lifelong) regarding the average PSNR metric. These results further validate the effectiveness of our proposed methods.

**Evaluation in terms of human eye-related metrics.** In this part, we further evaluate different methods in terms of the Frechet Inception Distance (FID) [19] and the Learned Perceptual Image Patch Similarity (LPIPS) distance [58], which correlate well with perceived image quality and are commonly used to evaluate the quality of generated images [10; 55]. We evaluate different methods on two synthesized datasets, including DIV2K-C and DIV2K-MC. As shown in Table 3, our SRTTA achieves the lowest values of both FID and LPIPS scores, demonstrating our SRTTA is able to generate images with higher visual quality.

### Further Experiments

In this part, we further conduct several ablation studies to demonstrate the effectiveness of each component of our SRTTA. Last, we evaluate our SRTTA on several datasets with real-world test images and provide the visual comparison results of different methods.

Figure 2: Visualization comparison on DIV2K-C test images with degradation for 2\(\times\) SR.

**Effect of the degradation classifier \(C(\cdot)\).** To investigate the effect of the degradation classifier, we compare our SRTTA with a baseline that does not use the degradation classifier. Specifically, this baseline generates the second-order degraded images with the random degradation type. In this case, a test image with blur degradation can be randomly degraded with blur, noise, or JPEG degradation. We report the PSRN results of our SRTTA and this baseline in Table 4. Experimental results demonstrate the necessity of the degradation classifier in our second-order degradation scheme.

**Effect of \(\mathcal{L}_{s}\) and \(\mathcal{L}_{a}\) in Eqn. (6).** To investigate the effect of the self-supervised adaptation loss \(\mathcal{L}_{s}\) and adaptation consistency loss \(\mathcal{L}_{a}\) in Eqn. (6), we report the mean PSNR results of our SRTTA with \(\mathcal{L}_{s}\)-only and \(\mathcal{L}_{a}\)-only. As shown in Table 4, without the adaptation consistency loss \(\mathcal{L}_{a}\), the SR models with only the \(\mathcal{L}_{s}\) will inevitably result in a model collapse. This is because the SR model is prone to output the same output for any input images without meaning. When we remove the \(\mathcal{L}_{s}\) loss, SRTTA can only achieve a limited performance, which demonstrates that \(\mathcal{L}_{s}\) truly helps to encourage the SR models to learn how to remove the degradation during the adaptation process. These experimental results demonstrate the effectiveness of the \(\mathcal{L}_{s}\) and \(\mathcal{L}_{a}\) in our framework.

**Effect of the hyper-parameters \(\alpha\) in Eqn. (6).** To investigate the effect of the weight of adaptation consistency loss \(\alpha\) in Eqn. (6), we report the mean PSNR results of our SRTTA with different \(\alpha\) in Table 5. When the \(\alpha\) is too small (\(\alpha<1\)), the self-supervised degradation loss \(\mathcal{L}_{s}\) dominates the adaptation process and often results in the collapse of SR models. When the \(\alpha\) is too large (\(\alpha>1\)), the adaptation consistency loss \(\mathcal{L}_{a}\) may have a great constraint on the adapted SR model to be the same as the pre-trained model, which may be harmful to the adaptation of the SR performance. With \(\alpha=1\), our SRTTA achieves the best results on the DIV2K-C dataset on average. Therefore, we simply set the \(\alpha\) to be \(1\) for all other experiments.

**Effect of adaptive parameter preservation in Eqn. (9).** In this part, we investigate the effect of our adaptive parameter preservation (APP) strategy on test-time adaptation. We compare our APP with the existing anti-forgetting method Stochastic Restoration (STO) [48], which randomly restores a different set of parameters (\(1\%\) parameters) after each adaptation step. Moreover, we also compare our APP with the random selection (RS) baseline, which randomly selects a fixed set of parameters to freeze before adaptation. As shown in Table 6, though STO achieves the best anti-forgetting performance on the clean Set5 dataset, the STO severely hinders the TTA performance. The random selection baseline only achieves a limited performance of both TTA and anti-forgetting. Instead, our APP consistently outperforms the random selection baseline with the same ratio of frozen parameters. Moreover, our APP with \(\rho=0.5\) achieves the best adaptation performance on DIV2K-C (see more

\begin{table}
\begin{tabular}{l|c} \hline \hline Method & Avg. PSNR \\ \hline SRTTA (ours) & **27.76** \\ - w/o classifier \(C(\cdot)\) & 26.06 \\ - w/o \(\mathcal{L}_{s}\) & 27.15 \\ - w/o \(\mathcal{L}_{a}\) & 10.24 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Effectiveness of components in SRTTA on DIV2K-C.

\begin{table}
\begin{tabular}{c|c c c c c|c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{DIV2K-C} & \multicolumn{3}{c}{DIV2K-MC} \\  & FID / LPIPS & FID / LPIPS \\ \hline SwinIR [29] & 27.90 / 0.2441 & 60.62 / 0.2781 \\ IPT [7] & 68.22 / 0.2345 & 58.24 / 0.2453 \\ HAT [8] & 64.92 / 0.2352 & 60.73 / 0.2640 \\ DAN [21] & 73.59 / 0.2260 & 56.96 / 0.2263 \\ DCLS-SR [34] & 33.44 / 0.2472 & 57.93 / 0.2299 \\ ZSBR [43] & 56.66 / 0.1931 & 52.78 / 0.2152 \\ KernelGAN [2]+ZSBR & 88.28 / 0.2160 & 80.19 / 0.2371 \\ MZSR [9] & 68.27 / 0.2085 & 16.72 / 0.24463 \\ DDNM [51] & 70.80 / 0.2101 & 59.64 / 0.2083 \\ EDSR [31] & 69.70 / 0.2242 & 57.95 / 0.2338 \\ TTA-C & 66.95 / 0.2188 & 56.32 / 0.2293 \\ \hline SRTTA (ours) & 54.37 / 0.1877 & 36.88 / 0.1915 \\ SRTTA-lifelong (ours) & **53.30 / 0.1828** & **35.72 / 0.1832** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison with different methods in terms of FID(\(\downarrow\)) and LPIPS(\(\downarrow\)) on both DIV2K-C and DIV2K-MC datasets.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{DIV2K-C} & \multicolumn{3}{c}{DIV2K-MC} \\  & FID / LPIPS & FID / LPIPS \\ \hline SwinIR [29] & 27.90 / 0.2441 & 60.62 / 0.2781 \\ IPT [7] & 68.22 / 0.2345 & 58.24 / 0.2453 \\ HAT [8] & 64.92 / 0.2352 & 60.73 / 0.2640 \\ DAN [21] & 73.59 / 0.2260 & 56.96 / 0.2263 \\ DCLS-SR [34] & 33.44 / 0.2472 & 57.93 / 0.2299 \\ ZSBR [43] & 56.66 / 0.1931 & 52.78 / 0.2152 \\ KernelGAN [2]+ZSBR & 88.28 / 0.2160 & 80.19 / 0.2371 \\ MZSR [9] & 68.27 / 0.2085 & 16.72 / 0.4463 \\ DDNM [51] & 70.80 / 0.2101 & 59.64 / 0.2083 \\ EDSR [31] & 69.70 / 0.2242 & 57.95 / 0.2338 \\ TTA-C & 66.95 / 0.2188 & 56.32 / 0.2293 \\ \hline SRTTA (ours) & 54.37 / 0.1877 & 36.88 / 0.1915 \\ SRTTA-lifelong (ours) & **53.30 / 0.1828** & **35.72 / 0.1832** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison with different methods in terms of FID(\(\downarrow\)) and LPIPS(\(\downarrow\)) on both DIV2K-C and DIV2K-MC datasets.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \(\alpha\) & 0 & 0.1 & 0.5 & 1 & 2 & 5 \\ \hline DIV2K-C & 10.24 & 13.96 & 22.63 & **27.76** & 27.52 & 27.31 \\ Set5 & 11.42 & 37.66 & 37.75 & 34.59 & 35.41 & 35.89 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Effects of different \(\alpha\) (in Eqn. (6)) under parameter-reset setting. We report average PSNR (\(\uparrow\)) on DIV2K-C (with degradation shift) and Set5 (w/o degradation shift).

results in the supplementary materials), thus we set the ratio of preservation parameters to be 0.5 in our SRTTA by default. These results demonstrate the effectiveness of our APP strategy.

**Visualization results on the real-world images.** We also provide the visual results of different methods on the real-world images from DPED [23] and ADE20K [61]. We use our SRTTA-lifelong model that has been adapted on DIV2K-C to perform test-time adaptation on the real-world images from DPED [23] and ADE20K [61], respectively. As shown in Figure 3, SRTTA-lifelong is able to generate HR images with fewer artifacts. These results demonstrate that our method is able to be applied to real-world applications. Please refer to more results in the supplementary materials.

## 6 Conclusion

In this paper, we propose a super-resolution test-time adaptation (SRTTA) framework to quickly alleviate the degradation shift issue for image super-resolution (SR). Specifically, we propose a second-order degradation scheme to construct paired data for each test image. Then, our second-order reconstruction loss is able to quickly adapt the pre-trained SR model to the test domains with unknown degradation. To evaluate the effectiveness of our SRTTA, we use eight different types of degradations to synthesize two new datasets named DIV2K-C and DIV2K-MC. Experiments on the synthesized datasets and several real-world datasets demonstrate that our SRTTA is able to quickly adapt the SR model for each image and generate plausible high-resolution images.

\begin{table}
\begin{tabular}{l|c|c c c|c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{STO [48]} & \multicolumn{3}{c|}{RS with different \(\rho\)} & \multicolumn{3}{c}{APP with different \(\rho\) (ours)} \\ \cline{3-8}  & & 0.3 & 0.5 & 0.7 & 0.3 & 0.5 & 0.7 \\ \hline DIV2K-C (with degradation shift) & 27.17 & 27.52 & 27.62 & 27.68 & 27.72 & **27.73** & 27.73 \\ \hline Set5 (w/o degradation shift) & 35.57 & 33.95 & 34.02 & 34.24 & 34.11 & 34.23 & 34.38 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation studies of adaptive parameter preservation (APP) strategy on DIV2K-C and Set5 under the lifelong setting. We report PSNR (\(\uparrow\)) and results on DIV2K-C are averaged over 8 different degradation types. The compared stochastic restoration (STO) [48] select 1% parameters for each adaptation iteration and Random Selection (RS) is evaluated by selecting different \(\rho\) parameters.

Figure 3: Visualization comparison on real-world test images from DPED [23] and ADE20K [61].

## Acknowledgements

This work was partially supported by the National Natural Science Foundation of China (NSFC) (62072190), National Natural Science Foundation of China (NSFC) 61836003 (key project), Program for Guangdong Introducing Innovative and Enterpreneurial Teams 2017ZT07X183, and CCF-Tencent Open Research Fund (CCF-Tencent RAGR20220108).

## References

* [1] E. Agustsson and R. Timofte. Ntire 2017 challenge on single image super-resolution: Dataset and study. In _The IEEE Conference on Computer Vision and Pattern Recognition Workshops_, July 2017.
* [2] S. Bell-Kligler, A. Shocher, and M. Irani. Blind super-resolution kernel estimation using an internal-gan. In _Advances in Neural Information Processing Systems_, volume 32, 2019.
* [3] M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In _British Machine Vision Conference_, pages 1-10. BMVA press, 2012.
* [4] A. Bruhn, J. Weickert, and C. Schnorr. Lucas/kanade meets horn/schunck: Combining local and global optic flow methods. _International Journal of Computer Vision_, 61:211-231, 2005.
* [5] A. Bulat, J. Yang, and G. Tzimiropoulos. To learn image super-resolution, use a gan to learn how to do image degradation first. In _European Conference on Computer Vision_, pages 185-200, 2018.
* [6] D. Chen, D. Wang, T. Darrell, and S. Ebrahimi. Contrastive test-time adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 295-305, 2022.
* [7] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao. Pre-trained image processing transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12299-12310, 2021.
* [8] X. Chen, X. Wang, J. Zhou, Y. Qiao, and C. Dong. Activating more pixels in image super-resolution transformer. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22367-22377, 2023.
* [9] X. Cheng, Z. Fu, and J. Yang. Zero-shot image super-resolution with depth guided internal degradation learning. In _European Conference on Computer Vision_, pages 265-280. Springer, 2020.
* [10] H. Chung, B. Sim, D. Ryu, and J. C. Ye. Improving diffusion models for inverse problems using manifold constraints. In _Advances in Neural Information Processing Systems_, volume 35, pages 25683-25696, 2022.
* [11] C. Dong, C. C. Loy, K. He, and X. Tang. Image super-resolution using deep convolutional networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 38(2):295-307, 2016.
* [12] M. Emad, M. Peemen, and H. Corporaal. Dualsr: Zero-shot dual learning for real-world super-resolution. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 1630-1639, 2021.
* [13] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International Conference on Machine Learning_, pages 1126-1135. PMLR, 2017.
* [14] D. Glasner, S. Bagon, and M. Irani. Super-resolution from a single image. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 349-356. IEEE, 2009.
* [15] J. Gu, H. Lu, W. Zuo, and C. Dong. Blind super-resolution with iterative kernel correction. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1604-1613, 2019.

* [16] Y. Guo, J. Chen, J. Wang, Q. Chen, J. Cao, Z. Deng, Y. Xu, and M. Tan. Closed-loop matters: Dual regression networks for single image super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5407-5416, 2020.
* [17] J. M. Haut, R. Fernandez-Beltran, M. E. Paoletti, J. Plaza, A. Plaza, and F. Pla. A new deep generative network for unsupervised remote sensing single-image super-resolution. _IEEE Transactions on Geoscience and Remote sensing_, 56(11):6792-6810, 2018.
* [18] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019.
* [19] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In _Advances in neural information processing systems_, volume 30, 2017.
* [20] A. Hore and D. Ziou. Image quality metrics: PSNR vs. SSIM. In _International Conference on Pattern Recognition_, pages 2366-2369, 2010.
* [21] Y. Huang, S. Li, L. Wang, T. Tan, et al. Unfolding the alternating optimization for blind super resolution. In _Advances in Neural Information Processing Systems_, volume 33, pages 5632-5643, 2020.
* [22] S. A. Hussein, T. Tirer, and R. Giryes. Correction filter for single image super-resolution: Robustifying off-the-shelf deep super-resolvers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1428-1437, 2020.
* [23] A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and L. Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3277-3285, 2017.
* [24] X. Ji, Y. Cao, Y. Tai, C. Wang, J. Li, and F. Huang. Real-world super-resolution via kernel estimation and noise injection. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops_, pages 466-467, 2020.
* [25] X. Kang, J. Li, P. Duan, F. Ma, and S. Li. Multilayer degradation representation-guided blind super-resolution for remote sensing images. _IEEE Transactions on Geoscience and Remote Sensing_, 60:1-12, 2022.
* [26] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114(13):3521-3526, 2017.
* [27] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L. Phillips, I. Gao, et al. Wilds: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_, pages 5637-5664. PMLR, 2021.
* [28] W.-S. Lai, J.-B. Huang, N. Ahuja, and M.-H. Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 624-632, 2017.
* [29] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte. Swinir: Image restoration using swin transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1833-1844, 2021.
* [30] J. Liang, K. Zhang, S. Gu, L. Van Gool, and R. Timofte. Flow-based kernel prior with application to blind super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10601-10610, 2021.
* [31] B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee. Enhanced deep residual networks for single image super-resolution. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops_, pages 136-144, 2017.
* [32] A. Liu, Y. Liu, J. Gu, Y. Qiao, and C. Dong. Blind image super-resolution: A survey and beyond. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.

* [33] P. Liu, H. Zhang, Y. Cao, S. Liu, D. Ren, and W. Zuo. Learning cascaded convolutional networks for blind single image super-resolution. _Neurocomputing_, 417:371-383, 2020.
* [34] Z. Luo, H. Huang, L. Yu, Y. Li, H. Fan, and S. Liu. Deep constrained least squares for blind image super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17642-17652, 2022.
* [35] W. Ma, Z. Pan, J. Guo, and B. Lei. Achieving super-resolution remote sensing images via the wavelet transform combined with the recursive res-net. _IEEE Transactions on Geoscience and Remote Sensing_, 57(6):3512-3527, 2019.
* [36] S. Maeda. Unpaired image super-resolution using pseudo-supervision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 291-300, 2020.
* [37] T. Michaeli and M. Irani. Nonparametric blind super-resolution. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 945-952, 2013.
* [38] S. Niu, J. Wu, Y. Zhang, Y. Chen, S. Zheng, P. Zhao, and M. Tan. Efficient test-time model adaptation without forgetting. In _International Conference on Machine Learning_, pages 16888-16905. PMLR, 2022.
* [39] S. Niu, J. Wu, Y. Zhang, Z. Wen, Y. Chen, P. Zhao, and M. Tan. Towards stable test-time adaptation in dynamic wild world. In _The International Conference on Learning Representations_, 2023.
* [40] Y. Pang, J. Cao, J. Wang, and J. Han. Jcs-net: Joint classification and super-resolution network for small-scale pedestrian detection in surveillance images. _IEEE Transactions on Information Forensics and Security_, 14(12):3322-3331, 2019.
* [41] S. Park, J. Yoo, D. Cho, J. Kim, and T. H. Kim. Fast adaptation to super-resolution networks via meta-learning. In _European Conference on Computer Vision_, pages 754-769. Springer, 2020.
* [42] L. Schermelleh, A. Ferrand, T. Huser, C. Eggeling, M. Sauer, O. Biehlmaier, and G. P. Drummen. Super-resolution microscopy demystified. _Nature Cell Biology_, 21(1):72-84, 2019.
* [43] A. Shocher, N. Cohen, and M. Irani. "zero-shot" super-resolution using deep internal learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3118-3126, 2018.
* [44] Y. M. Sigal, R. Zhou, and X. Zhuang. Visualizing and discovering cellular structures with super-resolution microscopy. _Science_, 361(6405):880-887, 2018.
* [45] J. W. Soh, S. Cho, and N. I. Cho. Meta-transfer learning for zero-shot super-resolution. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3516-3525, 2020.
* [46] Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _International Conference on Machine Learning_, pages 9229-9248. PMLR, 2020.
* [47] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell. Tent: Fully test-time adaptation by entropy minimization. In _International Conference on Learning Representations_, 2021.
* [48] Q. Wang, O. Fink, L. Van Gool, and D. Dai. Continual test-time domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7201-7211, 2022.
* [49] X. Wang, L. Xie, C. Dong, and Y. Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1905-1914, 2021.
* [50] X. Wang, K. Yu, C. Dong, and C. C. Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 606-615, 2018.

* [51] Y. Wang, J. Yu, and J. Zhang. Zero-shot image restoration using denoising diffusion null-space model. In _The International Conference on Learning Representations_, 2023.
* [52] Y. Xiao, Q. Yuan, Q. Zhang, and L. Zhang. Deep blind super-resolution for satellite video. _IEEE Transactions on Geoscience and Remote Sensing_, 2023.
* [53] J. Yang, K. Fu, Y. Wu, W. Diao, W. Dai, and X. Sun. Mutual-feed learning for super-resolution and object detection in degraded aerial imagery. _IEEE Transactions on Geoscience and Remote Sensing_, 60:1-16, 2022.
* [54] Y. Yuan, S. Liu, J. Zhang, Y. Zhang, C. Dong, and L. Lin. Unsupervised image super-resolution using cycle-in-cycle generative adversarial networks. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops_, pages 701-710, 2018.
* [55] K. Zhang, J. Liang, L. Van Gool, and R. Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4791-4800, 2021.
* [56] L. Zhang, H. Zhang, H. Shen, and P. Li. A super-resolution reconstruction algorithm for surveillance images. _Signal Processing_, 90(3):848-859, 2010.
* [57] M. Zhang, S. Levine, and C. Finn. Memo: Test time robustness via adaptation and augmentation. In _Advances in Neural Information Processing Systems_, volume 35, pages 38629-38642, 2022.
* [58] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 586-595, 2018.
* [59] Y. Zhang, B. Hooi, L. Hong, and J. Feng. Self-supervised aggregation of diverse experts for test-agnostic long-tailed recognition. In _Advances in Neural Information Processing Systems_, pages 34077-34090, 2022.
* [60] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu. Image super-resolution using very deep residual channel attention networks. In _Proceedings of the European conference on computer vision_, pages 286-301, 2018.
* [61] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba. Semantic understanding of scenes through the ade20k dataset. _International Journal of Computer Vision_, 127:302-321, 2019.
* [62] M. Zontak and M. Irani. Internal statistics of a single natural image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 977-984. IEEE, 2011.

**Supplementary Materials for**

**" Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction "**

###### Contents

* A More Details of the Second-Order Degradation
* A.1 Random Blur Degradation
* A.2 Random Noise Degradation
* A.3 Random JPEG Degradation
* B The Difference from Real-ESRGAN
* B.1 Solving Different Problems
* B.2 Different Construction Schemes
* C Experimental Datasets
* C.1 Construction Details of the DIV2K-C Dataset
* C.2 Construction Details of the DIV2K-MC Dataset
* C.3 More Test Datasets for Test-Time Image Super-Resolution
* D Implementation Details
* D.1 Implementation Details of the Degradation Classifier
* D.2 More Details of Super-Resolution Test-Time Adaptation
* E More Experimental Results
* E.1 More Results of Test-Time Image Super-Resolution
* E.2 More Results on Test domains with Mixed Multiple Degradations.
* E.3 More Results of Ablation Studies
* E.4 Effectiveness on a New Unknown Domain
* E.5 Comparison with Patch-Recurrence Reconstruction Loss
* E.6 Effect of the Feature-level Reconstruction
* E.7 Effect of Adaptation Iterations for Each Image
* E.8 The Statistics of the Degradation Types of Real-world Images
* E.9 Further Analysis of the Domain Shift Issue
* F Visualization Results
* F.1 Visualization Results on the DIV2K-C Images
* F.2 Visualization Results on the Real-World Images
* G Limitation Analysis and Border Impacts
* G.1 Limitation Analysis
* G.2 Broader Impacts
More Details of the Second-Order Degradation

In our second-order degradation scheme, we randomly generate different degradations to degrade the test image into its second-order degraded counterparts. In this section, we illustrate how to randomly generate different types of degradations. Notably, we degrade the test images on the GPU device to accelerate the degradation process.

### Random Blur Degradation

Following [26; 22], we model blur degradation as a convolution with a linear Gaussian blur filter/kernel. Given a test image, we randomly generate a set of isotropic or anisotropic Gaussian kernels \(\mathbf{k}\) and use them to perform blur degradation on the test image. The probabilities of generating an isotropic kernel and an anisotropic kernel are set to 0.5 and 0.5, respectively. The size of each generated kernel is uniformly sampled from \(\{7\times 7,9\times 9,...,21\times 21\}\). We sample the standard deviation of the blur kernel along the two principal axes \(\sigma_{1}\) and \(\sigma_{2}\) uniformly from \([0.2,3]\). If \(\mathbf{k}\) is an isotropic Gaussian blur kernel, we set \(\sigma_{2}\) equal to \(\sigma_{1}\). If \(\mathbf{k}\) is an anisotropic Gaussian kernel, we further sample a rotation angle \(a\) uniformly from \([-\pi,\pi]\), and use a rotation matrix to transform the generated kernel based on the angle \(a\). More details about how to generate a Gaussian blur kernel can refer to Real-ESRGAN [22].

### Random Noise Degradation

In our second-order degradation, we randomly generate a set of Gaussian noise maps \(\mathbf{n}\), and add them to the test image to obtain a set of second-order images with different additive Gaussian noise. With a probability of \(60\%\), we generate noise for each channel of RGB images independently, otherwise, we generate the same noise map for all three channels. We first generate a noise map whose values are randomly generated from a normal Gaussian distribution. Then we sample a scale value to enlarge the noise uniformly from \([1,30]\). More details can be referred to Real-ESRGAN [22].

### Random JPEG Degradation

For JPEG compression \(JPEG_{q}\), we sample a quality factor \(q\) uniformly from \([30,95]\), and use the JPEG compression with the degradation \(q\) to degrade test images into a set of second-order degraded images with compression artifacts. Note that JPEG compression with a lower \(q\) compress the test image with a higher compression ratio and the compressed images are generally of a lower quality. To accelerate the degradation process, we use DiffJPEG1, which is the PyTorch implementation of JPEG compression, to process the test image on the GPU device.

Footnote 1: https://github.com/mlomnitz/DiffJPEG

## Appendix B The Difference from Real-ESRGAN

In this part, we would like to discuss the difference between our SRTTA with Real-ESRGAN[22], which proposes the concept of second-order degradations, to highlight the contribution of our SRTTA.

### Solving Different Problems

Real-ESRGAN tries to enumerate all the degradations in real-world scenes and train an SR model to solve the image restoration on any degradation. However, it is non-trivial to obtain all real-world degradations, leading to domain shift issues when encountering unknown degradations during testing, as shown in Figure 11 of real-ESRGAN [22]. Unlike real-ESRGAN [22], our SRTTA aims to adapt the SR models to the test domains when test images contain unknown degradations. Our second-order degradation scheme aims to quickly construct the pseudo-paired data (instead of the paired training data) to adapt the SR model to the test domains.

### Different Construction Schemes

Real-ESRGAN [22] proposes the second-order degradation to construct the paired training data, whose low-resolution (LR) images are obtained from the ground-truth high-resolution (HR) images.

Then, the paired data is used to train an SR model during the **training** phase in a **supervised** learning manner. Notably, the trained Real-ESRGAN [22] model is fixed during the test time. Instead, our second-order degradation scheme constructs the pseudo-paired data using the test images with unknown degradation (first-order degraded images). Our SRTTA model is continuously adapted to different domains during **testing** in a **self-supervised** learning manner.

## Appendix C Experimental Datasets

### Construction Details of the DIV2K-C Dataset

To evaluate the practicality, we construct a new benchmark dataset named DIV2K-C, which contains eight different degradations. We select the eight degradation types from the 15 corruptions of ImageNet-C [11] that do not extremely change the image content, including Gaussian Blur, Defocus Blur, Glass Blur, Gaussian Noise, Poisson Noise (Shot Noise), Impulse Noise, Speckle Noise, and JPEG compression. Unlike the ImageNet-C [11], we do not use the same degradation level to degrade all test images. Instead, we randomly generate the degradation level and further generate a degradation for each image based on the degradation level. Unlike prior SR methods that investigate a limited number of degradation types, the degradation scenarios we considered are more complex (eight degradation types in total), which is more practical for real-world applications.

Given a degradation, we use the classical image degradation model [17; 22] to generate the low-resolution (LR) test images from the high-resolution clean images. For blur degradation, we perform the blur convolution on the HR images and then use the Bicubic downsampling to obtain the test LR images. For noise and JPEG degradation, we first use the Bicubic downsampling to obtain clean LR images and then perform noise or JPEG degradation on the clean LR images to obtain the final test images. We show the visualization of some examples regarding each degradation type in Figure A.

**Gaussian blur**. Following BSRGAN [26], we generate low-resolution (LR) images with Gaussian blur degradation. We randomly generate an isotropic Gaussian kernel or an anisotropic Gaussian kernel for each high-resolution (HR) image. Then, we use the blur kernel to perform blur convolution on the HR image and use Bicubic downsampling to obtain the final LR test images. For simplicity, we follow the recipe of BSRGAN [26] to generate test images with blur degradation.

Figure A: The visualization of examples regarding each degradation type on the DIV2K-C dataset.

**Defocus blur.** To better compare the performance of different SR methods, we also use the common Defocus blur degradation to degrade the HR images and obtain the LR images using the Bicubic downsampling. As illustrated in ImageNet-C [11], Defocus blur often occurs when an image is out of focus when we take pictures. We generate the blur kernel as ImageNet-C [11] and perform a blur convolution on the HR images. But unlike ImageNet-C [11], the degradation level of Defocus blur is randomly sampled from a given range.

**Glass blur.** We also choose another common degradation type, Glass Blur, which appears with "frosted glass" windows or panels [11]. This blur degradation requires two Gaussian blur operations and an operation that locally shuffles pixels between two blur operations. As mentioned above, the degradation level is randomly sampled from a given range, such as the standard deviation of the Gaussian blur kernel or the window size of the shuffling operation.

**Gaussian noise.** To generate test images with Gaussian noise, we sample the noise for each pixel from a normal Gaussian distribution. The mean of the Gaussian distribution is zero, and the standard deviation is uniformly sampled from the range of \(\{2/255,3/255,...,25/255\}\). More details can refer to the implementation of BSRGAN [26].

**Poisson (Shot) noise.** Poisson noise, also called Shot noise, can model the sensor noise caused by statistical quantum fluctuations. We randomly generate the Poisson noise map from a Poisson distribution, which has an intensity proportional to the image intensity. Then, we add the generated Poisson noise into the clean LR images to obtain the test images with Poisson noise. More details can refer to the implementation of Real-ESRGAN [22].

**Impulse noise.** Impulse noise is caused by errors in the data transmission generated in noisy sensors or communication channels, or by errors during the data capture from digital cameras [19]. The most common form of Impulse noise is called salt-and-pepper noise. To generate test images with Impulse noise, we uniformly select a set of pixels and replace them with zero or the maximum value (255). More details can be referred to ImageNet-C [11].

**Speckle noise.** Speckle noise is an additive noise where the noise added to a pixel tends to be larger if the original pixel intensity is larger. We first sampled the noise for each pixel from a Gaussian distribution and multiple the noise value by the original pixel. Last, we add the generated noise map into the LR clean images to obtain the final test images with Speckle noise.

**JPEG compression.** For JPEG compression, we use the OpenCV implementation of JPEG compression2 to degrade the clean LR image into final test images. The compression quality factor \(q\) is randomly sampled from \([30,90]\). We first encode the clean LR images into the bit stream using JPEG compression with the quality factor \(q\) and decode the bit stream to obtain the final test images. Note that JPEG compression is a lossy compression technique, so the final test images are inevitably corrupted with JPEG compression artifacts.

Footnote 2: https://github.com/opencv/opencv

### Construction Details of the DIV2K-MC Dataset

Since the real-world test images may contain multiple degradation types simultaneously, we further develop a new benchmark dataset named DIV2K-MC, which includes four test domains with mixed multiple degradations. The four domains are BlurNoise, BlurJPEG, NoiseJPEG and BlurNoiseJPEG. The test images in the BlurNoiseJPEG domain contain the combined degradation of Gaussian blur, Gaussian noise and JPEG degradations simultaneously.

**BlurNoise.** We generate LR images from HR images using Gaussian blur and Gaussian noise degradation. We first randomly generate a Gaussian blur kernel to perform blur convolution on the HR image. Then, we downsample the resulting image using Bicubic interpolation. Last, we randomly sample a Gaussian noise map and add it to the downsampled image to obtain the final LR image.

**BlurJPEG.** We generate LR images from HR images using Gaussian blur and JPEG degradation. We first randomly generate a Gaussian blur kernel to perform blur convolution on the HR image. Then, the resulting image is downsampled by using Bicubic interpolation. Last, we use JPEG compression with a random quality factor \(q\) to compress the downsampled image to obtain the final LR image.

**NoiseJPEG.** We generate LR images from HR images using Gaussian noise and JPEG degradation. We first downsample HR image using Bicubic interpolation. Then, we randomly sample a Gaussiannoise map and add it to the downsampled image. Last, we use JPEG compression with a random quality factor \(q\) to compress the downsampled image to obtain the final LR image.

**BlurNoiseJPEG.** We generate LR images from HR images using Gaussian blur, Gaussian noise and JPEG degradation. We first randomly generate a Gaussian blur kernel to perform blur convolution on the HR image. Second, the resulting image is downsampled by using Bicubic interpolation. Then, we randomly sample a Gaussian noise map and add it to the downsampled image. Last, we use JPEG compression with a random quality factor \(q\) to compress the image to obtain the final LR image.

### More Test Datasets for Test-Time Image Super-Resolution

Moreover, we also evaluate the performance of SR methods on real-world test images from DPED [13], ADE20K [28] and OST300 [23], whose corresponding ground-truth HR images can not be found. To evaluate the anti-forgetting performance, we report the adapted model performance on a clean benchmark dataset Set5 [2] whose images are clean images that are downsampled from HR images with Bicubic interpolation. Thus, these LR images do not contain any degradation.

## Appendix D Implementation Details

### Implementation Details of the Degradation Classifier

In our second-order degradation scheme, we use a pre-trained degradation classifier to predict the degradation type for each test image. To obtain the pre-trained degradation classifier, we use ResNet-50 [10] as the classifier and train it to recognize the degradation from test images.

In real-world scenes, test images may contain degradations other than these eight degradation types, such as ringing or overshoot artifacts [22], which may be viewed as variations of blur, noise or JPEG. Since it is infeasible to cover all the degradation types in real-world scenes, we make the degradation classifier to predict the coarse-level four classes, including clean, blur, noise and JPEG.

**Training details.** Specifically, we use the 800 training HR images of DIV2K and randomly crop them into patches with the size of 224 \(\times\) 224 (instead of resizing them into 224 \(\times\) 224). Similar to the construction of DIV2K-C, we degrade each patch using a random selection of one of eight degradation types. As for clean data, we do not perform any degradation on the patches. For training, we apply Adam with \(\beta_{1}\) = 0.9, \(\beta_{2}=0.999\) and set the batch size as 256. The learning rate is initialized to \(10^{-3}\) and decreased to \(10^{-6}\) with a cosine annealing out of 400 epochs in total.

**Testing details.** During testing, we directly input the whole test image **with original resolution** into the classifier and output the predicted results to recognize the degradation type. The predicted results of the multi-label degradation classifier \(C(\cdot)\) are the probabilities of the three degradations, including blur, noise and JPEG degradation. If the predicted probability of one degradation type is larger than the threshold of 0.5, the test image is considered to contain the degradation of this type. The clean image means that this image does not contain any degradation such as blur, noise, or JPEG, and we directly use the pre-trained SR model to super-resolve these clean test images.

### More Details of Super-Resolution Test-Time Adaptation

We use the baseline model of EDSR [16] with less than 2M parameters as our pre-trained SR model for 2\(\times\) and 4\(\times\) SR. During adaptation, we only update the parameters in the Resblock of the EDSR model. To avoid anti-forgetting, we use five clean test LR images from Set5 [2] to select important parameters to be frozen in Eqn. (9). Moreover, when evaluating the anti-performance Set5 [2], we directly use the adapted model to super-resolve the test images without using the classifier.

In our experiment, we conduct experiments in **parameter-reset** and **lifelong** settings. In the parameter-reset setting, the model parameters will be reset after the adaptation on each domain, which is the default setting of our SRTTA. In the lifelong setting, the model parameters will never be reset in the long-term adaptation, in this case, we call our methods as SRTTA-lifelong.

For test-time adaptation, we use the Adam optimizer with the learning rate of \(5\times 10^{-5}\) for the pre-trained SR models. We set the batch size \(N\) to 32, and we randomly crop the test image into \(N\) patches of size \(96\times 96\) and \(64\times 64\) for 2\(\times\) and 4\(\times\) SR, and degrade them into second-order degraded patches. We perform \(S=10\) iterations of adaptation for each test image. For the balance weight in 

[MISSING_PAGE_FAIL:20]

### More Results on Test domains with Mixed Multiple Degradations.

In this part, we evaluate our SRTTA on DIV2K-MC, which consists of four test domains with mixed multiple degradations. In Table C, our SRTTA achieves the best performance on 4 domains with different mixed degradations, e.g., 0.619 (DualSR) \(\rightarrow\) 0.775 (our SRTTA-lifelong) regarding the average SSIM metric. These results further validate the effectiveness of our proposed methods.

### More Results of Ablation Studies

**Effect of each component.** In this part, we investigate the effect of each component and provide more ablation studies. As shown in Table D, the baseline without the degradation classifier, generates the second-order degraded images with random degradation types, achieving a limited performance in terms of both PSNR and SSIM. The baseline without the adaptation consistency loss \(\mathcal{L}_{a}\) results in the model collapse due to the lack of the consistency constraint. Without the self-supervised adaptation loss \(\mathcal{L}_{s}\), the TTA performance of the adapted model drops significantly. These experimental results demonstrate the effectiveness of each component of our framework.

**Effect of the hyperparameter \(\boldsymbol{\rho}\) in Eqn. (9).** In this part, we investigate the effect of the weight of adaptation consistency loss \(\alpha\). As shown in Table E, the adapted model with \(\alpha=1\) achieves the best TTA performance. Thus, we set the \(\alpha=1\) by default for our SRTTA during adaptation.

**Effect of the hyperparameter \(\boldsymbol{\rho}\) in Eqn. (9).** In this part, we analyze the effect of the hyperparameter \(\boldsymbol{\rho}\), which decides the ratio of parameters to freeze, for the test-time adaptation. In Table F, when \(\rho=0.50\), our SRTTA achieves the best TTA performance on the DIV2K-C dataset on average in the lifelong setting. Meanwhile, we also investigate the effect of the adaptive parameter preservation (APP) strategy in the parameter-reset setting. As shown in Table F, our APP strategy (with \(\rho=0.50\)

\begin{table}
\begin{tabular}{c|c c c c c c|c} \hline \hline Methods & \multicolumn{1}{c}{BlurNoise} & \multicolumn{1}{c}{BlurJPEG} & \multicolumn{1}{c}{NoiseJPEG} & \multicolumn{1}{c}{BlurNoiseJPEG} & \multicolumn{1}{c}{Mean} \\ \hline SwinIR [15] & 20.91/0.311 & 26.83/0.748 & 23.86/0.523 & 22.77/0.450 & 23.59/0.508 \\ IPT [5] & 21.28/0.327 & 26.83/0.748 & 24.15/0.535 & 22.96/0.459 & 23.81/0.517 \\ HAT [6] & 23.41/0.399 & 28.86/0.788 & 25.69/0.572 & 24.42/0.502 & 25.59/0.565 \\ DAN [12] & 24.14/0.438 & 28.95/0.791 & 26.20/0.593 & 24.82/0.519 & 26.03/0.585 \\ DCLS-SR [18] & 23.84/0.420 & 28.93/0.790 & 26.37/0.599 & 24.92/0.523 & 26.02/0.583 \\ ZSSR [20] & 24.95/0.493 & 29.02/0.793 & 26.68/0.617 & 25.24/0.542 & 26.47/0.611 \\ KernelGAN [1]+ZSSR & 23.08/0.424 & 28.32/0.786 & 21.90/0.474 & 22.76/0.443 & 24.02/0.532 \\ MZSR [7] & 18.73/0.213 & 24.90/0.667 & 20.37/0.398 & 20.62/0.354 & 21.16/0.408 \\ DualSR [8] & 25.59/0.561 & 28.24/0.787 & 23.78/0.586 & 24.62/0.541 & 25.65/0.619 \\ DDNN [24] & 22.62/0.389 & 28.62/0.782 & 25.11/0.582 & 23.81/0.504 & 24.59/0.555 \\ EDSR [16] & 24.02/0.430 & 28.93/0.790 & 26.08/0.587 & 24.73/0.514 & 25.94/0.580 \\ TTA-C & 24.29/0.446 & 28.93/0.790 & 26.35/0.598 & 24.91/0.522 & 26.12/0.589 \\ \hline SRTTA (ours) & 26.93/0.709 & 28.93/**0.798** & **29.13/0.784** & 27.12/0.728 & 28.02/0.755 \\ SRTTA-lifelong (ours) & **27.67/0.749** & **29.02/0.793** & 29.70/0.810 & **27.52/0.747** & **28.48/0.775** \\ \hline \hline \end{tabular}
\end{table}
Table C: Comparison results with prior methods on DIV2K-MC. We report the PSNR(\(\uparrow\))/SSIM(\(\uparrow\)) values of different methods.

\begin{table}
\begin{tabular}{c|c c c c c c c c|c c c} \hline \hline Methods & \multicolumn{1}{c}{GurssianBlur} & \multicolumn{1}{c}{DefocusBlur} & \multicolumn{1}{c}{GiasBlur} & \multicolumn{1}{c}{GurssianNoise} & \multicolumn{1}{c}{ProsionNoise} & \multicolumn{1}{c}{ImpulseNoise} & \multicolumn{1}{c}{SpeckaNoise} & \multicolumn{1}{c|}{JPEG} & \multicolumn{1}{c|}{Mean} \\ \hline  & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM & PSNR-SSIM \\ \hline
0 & 12.29/0.254 & 5.67/0.397 & 5.65/0.403 & 12.57/0.477 & 11.29/0.072 & 11.48/0.213 & 11.26/0.218 & 3.94/0.582 & 11.49/0.477 & 10.24/0.314 & 11.42/0.421 \\
0 & 11.11/0.358 & 20.00/0.453 & 6.55/0.403 & 22.78/0.653 & 5.65/0.403 & 12.50/0.544 & 15.00/0.542 & 34.95/0.580 & 13.96/0.466 & 37.66/0.599 \\
0 & 14.07/1.34 & 10.46/0.313 & 21.66/0.313 & 21.66/0.208 & 29.21/0.797 & **27.80/0.717** & 13.60/0.290 & 23.07/0.817 & 30.98/0.580 & 23.03/0.585 & 75.70/0.969 \\
1 & **31.07/0.569** & 28.66/0.542 & **29.01/0.515** & **29.66/0.72** & **26.60** & **31.65/0.542** & **32.30/0.573** & **27.76/0.721** & **33.45/0.924** \\
2 & 30.86/0.862 & 25.91/0.678 & 28.71/0.304 & 29.04/0.710 & 25.89/0.591 & 16.00/0.276 & 32.22/0.805 & 31.45/0.861 & 27.52/0.706 & 35.41/0.933 \\
5 & 30.74/0.857 & **25.91/0.680** & 28.53/0.798 & 28.47/0.670 & 25.29/0.558 & 16.00/0.276 & 32.20/0.855 & 31.48/0.862 & 27.31/0.695 & 35.88/0.939 \\ \hline \hline \end{tabular}
\end{table}
Table D: We report the PSNR/SSIM results of ablation studies of different components for 2\(\times\) SR.

merely has little impact on the TTA in the parameter-reset setting. These results demonstrate the effectiveness of the APP strategy in test-time adaptation for image super-resolution.

**Comparison with other anti-forgetting methods.** In this part, we compare our adaptive parameter preservation (APP) strategy with two baseline methods to demonstrate the effectiveness of our strategy in preserving the learned knowledge of pre-trained SR models. **Stochastic Restoration (STO)**[21] randomly selects a different set of parameters (with a ratio of \(1\%\)) and restores them back to the parameters of the pre-trained models. **Random Selection (RS)** selects a fixed set of parameters before adaption and freezes them not to update. As shown in Table G, our APP strategy achieves the best TTA results on the DIV2K-C dataset. Meanwhile, with the same ratio of selected parameters, our APP strategy consistently outperforms the Random Selection baseline for the anti-forgetting. These results demonstrate that our adaptive selection is able to select the important parameters and preserve the knowledge of the pre-trained model.

### Effectiveness on a New Unknown Domain

In this part, we further evaluate our SRTTA on a new unknown domain with the degradation of processed camera sensor noise [3, 26], which is not used in the training phase of the SR model or that of the degradation classifier. We report the PSNR(\(\uparrow\)) and SSIM(\(\uparrow\)) values of different methods on 100 images with random processed camera sensor noise. In Table H, our SRTTA method is also able to improve the model performance on this unknown degradation. These experimental results further demonstrate the generalization capability of our SRTTA model to unknown degradation types.

### Comparison with Patch-Recurrence Reconstruction Loss

In this part, we investigate the effect of our second-order reconstruction loss. We compare our loss with the loss of existing zero-shot methods [20, 7, 1]. Based on the assumption of patch recurrence across scales [9, 29], these methods downsample the test image to obtain an image with a lower resolution and reconstruct the test image from the downsampled image. For simplicity, we call this patch-recurrence loss. For a fair comparison, we further downsample the second-order degraded images that are obtained using our second-order degradation scheme and reconstruct the test image with the patch-recurrence loss. As shown in Table I, our SRTTA with our second-order reconstruction loss consistently outperforms the baseline with the patch-recurrence loss. These results demonstrate the effectiveness of our second-order reconstruction loss.

### Effect of the Feature-level Reconstruction

In this part, we investigate the effect of different reconstruction levels. In our second-order reconstruction, we use the feature-level reconstruction to adapt the pre-trained model as in Eqn. (6). We

\begin{table}
\begin{tabular}{c|c|c} \hline Methods & PSNR & SSIM \\ \hline SwinIR [15] & 19.45 & 0.496 \\ IPT [5] & 19.51 & 0.500 \\ HAT [6] & 21.52 & 0.596 \\ DDNN [24] & 19.63 & 0.518 \\ DAN [12] & 21.53 & 0.598 \\ DCLS-SR [18] & 21.57 & 0.605 \\ DualSR [8] & 21.14 & 0.586 \\ MZSR [7] & 20.40 & 0.438 \\ ZSSR [20] & 21.57 & 0.621 \\ KernelGAN [1]+ZSSR & 20.60 & 0.543 \\ EDSR [16] & 21.56 & 0.601 \\ \hline SRTTA (ours) & **21.81** & **0.647** \\ \hline \end{tabular}
\end{table}
Table 5: We report the PSNR/SSIM results of ablation studies of \(\rho\) for \(2\times\) SR in the parameter-reset and lifelong setting, our model is SRTTA and SRTTA-lifelong.

\begin{table}
\begin{tabular}{c|c|c c c c c c c} \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{STO [21]} & \multicolumn{5}{c|}{RS with different \(\rho\)} & \multicolumn{5}{c}{APP with different \(\rho\) (ours)} \\ \cline{3-8}  & & 0.3 & 0.5 & 0.7 & 0.3 & 0.5 & 0.7 \\ \hline DIV2K-C (with degradation shift) & 27.17/0.687 & 27.52/0.727 & 27.62/0.728 & 27.68/0.726 & 27.72/0.728 & **27.73/0.728** & 27.73/0.725 \\ \hline Set5 (w/o degradation shift) & **35.57/0.938** & 33.95/0.913 & 34.02/0.914 & 34.24/0.918 & 34.11/0.916 & 34.23/0.917 & 34.38/0.920 \\ \hline \end{tabular}
\end{table}
Table 6: We report the PSNR/SSIM results of ablation studies of adaptive parameter preservation (APP) strategy for \(2\times\) SR in the lifelong setting.

[MISSING_PAGE_FAIL:23]

images and generate unsatisfactory HR images with artifacts. For example, EDSR-B models cannot remove the noise and JPEG degradation, the EDSR-N and EDSR-J are also unable to remove the blur degradation. Instead, after test-time adaptation, our SRTTA is capable of handling the test images with unknown degradations and generating HR images with fewer artifacts. For example, our SRTTA is able to generate sharper HR images than EDSR-N and EDSR-J under Gaussian Blur domains. Indeed, our SRTTA models may be unable to completely remove the degradation compared with the upper-bound models, such as EDSR-B under Gaussian Blur. Thus, these drawbacks are required to be further addressed in future works.

## Appendix F Visualization Results

### Visualization Results on the DIV2K-C Images

In this part, we show more visualization comparison results of different SR methods on test images of the DIV2K-C dataset for both 2\(\times\) and 4\(\times\) SR. As shown in Figures D and E, our SRTTA is able to reduce the degradation from the test images and generate more plausible HR images.

### Visualization Results on the Real-World Images

In this part, we conduct a comprehensive comparison of our SRTTA with existing approaches on two real-world datasets, including DPED [13], ADE20K [28] and OST300 [23]. As shown in Figures F and G, our SRTTA methods consistently generate more satisfactory HR images with less degradation of unknown noise or artifacts.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline Dataset (\# Images) & Clean & Blur & Noise & JPEG \\ \hline RealSR (912) & 48 & 860 & 1 & 149 \\ DRealSR (35148) & 1378 & 33770 & 0 & 1 \\ DPED (187) & 103 & 33 & 21 & 64 \\ OST300 (300) & 52 & 23 & 14 & 221 \\ ADE20K (27574) & 0 & 3452 & 2424 & 27573 \\ \hline Total (64121) & 1581 & **38138** & 2460 & 28008 \\ \hline \hline \end{tabular}
\end{table}
Table L: The count of the predicted degradation types of the real-world images from the five datasets. Note that some images can contain more than one degradation type simultaneously.

Figure B: Visualization of the domain shift issue under different domains for 2\(\times\) SR.

## Appendix G Limitation Analysis and Border Impacts

### Limitation Analysis

In this part, we analyze the limitations of our SRTTA and existing SR methods. When test images are corrupted at a high level, our SRTTA may not be able to completely remove the degradation and result in generated HR images with the existing degradation. For example, we show more visualization results of different methods on test images with Impulse Noise degradation in Figure H.

Since Real-ESRGAN [22] uses several different degradation types to construct training data, this model is able to remove the degradation in many cases. However, this model still suffers from the degradation shift issue, such as it cannot remove the gray Impulse noise from the test images as shown in Figure 1(a). Moreover, Real-ESRGAN may generate HR images with over-smooth regions when removing the noise degradation and introduce some unpleasant artifacts due to the GAN training [22], which are shown in Figure 1(b) and Figure 1(c), respectively. Although our SRTTA cannot also completely remove the degradation from the test images in these cases, our SRTTA often preserves the original information of the test images. These results show the limitations of our SRTTA and existing methods and have a great impact on the practical application. Thus, these drawbacks are in urgent need to address in future works.

### Broader Impacts

Our proposed SRTTA method is capable of improving the resolution of low-resolution test images in real-world applications, resulting in enhanced image clarity and enabling a precise understanding of image content. However, it is important to exercise caution during aggressive TTA adaptation, as this may result in the introduction of artifacts or distortions that have the potential to negatively impact downstream analyses such as microscopy, remote sensing, and surveillance.

Figure 1: Visualization comparison on DIV2K-C test images with degradation for 4\(\times\) SR.

Figure F: Visualization comparison of different methods on real-world test images from DPED [13].

Figure G: Visualization comparison on real-world test images from for 2\(\times\) SR.

Figure H: Limitations visualization on DIV2K-C test images with degradation for 4\(\times\) SR.

## References

* [1]S. Bell-Kligler, A. Shocher, and M. Irani (2019) Blind super-resolution kernel estimation using an internal-gan. In Advances in Neural Information Processing Systems, Vol. 32. Cited by: SS1.
* [2]M. Bevilacqua, A. Roumy, C. Guillemot, and M. L. Alberi-Morel (2012) Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In British Machine Vision Conference, pp. 1-10. Cited by: SS1.
* [3]T. Brooks, B. Mildenhall, T. Xue, J. Chen, D. Sharlet, and J. T. Barron (2019) Unprocessing images for learned raw denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11036-11045. Cited by: SS1.
* [4]J. Cai, H. Zeng, H. Yong, Z. Cao, and L. Zhang (2019) Toward real-world single image super-resolution: a new benchmark and a new model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3086-3095. Cited by: SS1.
* [5]H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu, C. Xu, and W. Gao (2021) Pre-trained image processing transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12299-12310. Cited by: SS1.
* [6]X. Chen, X. Wang, J. Zhou, Y. Qiao, and C. Dong (2023) Activating more pixels in image super-resolution transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22367-22377. Cited by: SS1.
* [7]X. Cheng, Z. Fu, and J. Yang (2020) Zero-shot image super-resolution with depth guided internal degradation learning. In European Conference on Computer Vision, pp. 265-280. Cited by: SS1.
* [8]M. Emad, M. Peemen, and H. Corporaal (2021) DualsF: zero-shot dual learning for real-world super-resolution. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1630-1639. Cited by: SS1.
* [9]D. Glasner, S. Bagon, and M. Irani (2009) Super-resolution from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 349-356. Cited by: SS1.
* [10]K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770-778. Cited by: SS1.
* [11]D. Hendrycks and T. Dietterich (2019) Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, Cited by: SS1.
* [12]Y. Huang, S. Li, L. Wang, T. Tan, et al. (2020) Unfolding the alternating optimization for blind super resolution. In Advances in Neural Information Processing Systems, Vol. 33, pp. 5632-5643. Cited by: SS1.
* [13]A. Ignatov, N. Kobyshev, R. Timofte, K. Vanhoey, and L. Van Gool (2017) DSLr-quality photos on mobile devices with deep convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3277-3285. Cited by: SS1.
* [14]H. Li, Y. Yang, M. Chang, S. Chen, H. Feng, Z. Xu, Q. Li, and Y. Chen (2022) SRdiff: single image super-resolution with diffusion probabilistic models. Neurocomputing479, pp. 47-59. Cited by: SS1.
* [15]J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte (2021) Swinir: image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1833-1844. Cited by: SS1.
* [16]B. Lim, S. Son, H. Kim, S. Nah, and K. Mu Lee (2017) Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp. 136-144. Cited by: SS1.
* [17]P. Liu, H. Zhang, Y. Cao, S. Liu, D. Ren, and W. Zuo (2020) Learning cascaded convolutional networks for blind single image super-resolution. Neurocomputing417, pp. 371-383. Cited by: SS1.

[MISSING_PAGE_POST]

* [20] A. Shocher, N. Cohen, and M. Irani. "zero-shot" super-resolution using deep internal learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3118-3126, 2018.
* [21] Q. Wang, O. Fink, L. Van Gool, and D. Dai. Continual test-time domain adaptation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7201-7211, 2022.
* [22] X. Wang, L. Xie, C. Dong, and Y. Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1905-1914, 2021.
* [23] X. Wang, K. Yu, C. Dong, and C. C. Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 606-615, 2018.
* [24] Y. Wang, J. Yu, and J. Zhang. Zero-shot image restoration using denoising diffusion null-space model. In _The International Conference on Learning Representations_, 2023.
* [25] P. Wei, Z. Xie, H. Lu, Z. Zhan, Q. Ye, W. Zuo, and L. Lin. Component divide-and-conquer for real-world image super-resolution. In _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part VIII 16_, pages 101-117. Springer, 2020.
* [26] K. Zhang, J. Liang, L. Van Gool, and R. Timofte. Designing a practical degradation model for deep blind image super-resolution. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4791-4800, 2021.
* [27] M. Zhang, S. Levine, and C. Finn. Memo: Test time robustness via adaptation and augmentation. In _Advances in Neural Information Processing Systems_, volume 35, pages 38629-38642, 2022.
* [28] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba. Semantic understanding of scenes through the ade20k dataset. _International Journal of Computer Vision_, 127:302-321, 2019.
* [29] M. Zontak and M. Irani. Internal statistics of a single natural image. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 977-984. IEEE, 2011.