VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks

Jiannan Wu\({}^{2,1}\), Muyan Zhong\({}^{*3}\), Sen Xing\({}^{*3}\), Zeqiang Lai\({}^{*4}\), Zhaoyang Liu\({}^{*5,1}\), Zhe Chen\({}^{*6,1}\), Wenhai Wang\({}^{*7,1}\), \(\mathbf{Xizhou Zhu}^{3,8,1}\), \(\mathbf{Lewei Lu}^{8,1}\), Tong Lu\({}^{6}\), \(\mathbf{Ping}\) Luo\({}^{2}\), Yu Qiao\({}^{1}\), \(\mathbf{Jifeng Dai}^{13,1}\)

\({}^{1}\)OpenGVLab, Shanghai AI Laboratory \({}^{2}\)The University of Hong Kong \({}^{3}\)Tsinghua University

\({}^{4}\)Beijing Institute of Technology \({}^{5}\)The Hong Kong University of Science and Technology

\({}^{6}\)Nanjing University \({}^{7}\)The Chinese University of Hong Kong \({}^{8}\)SenseTime Research

###### Abstract

We present VisionLLM v2, an end-to-end generalist multimodal large model (MLLM) that unifies visual perception, understanding, and generation within a single framework. Unlike traditional MLLMs limited to text output, VisionLLM v2 significantly broadens its application scope. It excels not only in conventional visual question answering (VQA) but also in open-ended, cross-domain vision tasks such as object localization, pose estimation, and image generation and editing. To this end, we propose a new information transmission mechanism termed "super link", as a medium to connect MLLM with task-specific decoders. It not only allows flexible transmission of task information and gradient feedback between the MLLM and multiple downstream decoders but also effectively resolves training conflicts in multi-tasking scenarios. In addition, to support the diverse range of tasks, we carefully collected and combed training data from hundreds of public vision and vision-language tasks. In this way, our model can be joint-trained end-to-end on hundreds of vision language tasks and generalize to these tasks using a set of shared parameters through different user prompts, achieving performance comparable to task-specific models. We believe VisionLLM v2 will offer a new perspective on the generalization of MLLMs.

## 1 Introduction

Multimodal large language models (MLLMs) [8, 97, 223, 107, 105, 140, 14, 169, 34, 33] have recently made significant progress, demonstrating outstanding performance across various vision-language tasks, even in scenarios requiring complex understanding and reasoning. However, _a notable limitation is that current MLLM outputs are in text form, which significantly constrains their capacity to represent structured or visual information._ Some researchers [140, 182, 181, 180] have expanded the text-based output formats of MLLMs to better align with downstream tasks. While these efforts have shown promise, they have not fully addressed practical needs such as dense object detection, pose estimation, and image generation.

To overcome this limitation, a line of research [116, 187, 166, 111, 47] enhances the capabilities of MLLMs by transmitting task information to tools via text messages, as illustrated in Figure 1(a). Despite these advances, these text-based methods are restricted by the information that text can convey. They are not end-to-end, and the feedback gradient from the tools cannot be relayed back to the MLLM. This limitation has spurred another research direction [89, 148, 193, 44, 83, 164] that employs learnable embeddings as intermediaries to connect MLLM with one specific task decoder (see Figure 1(b)). However, the naive embedding connection is difficult to scale to multi-task scenarios. A routing mechanism is needed to ensure the correct selection of tools, and the issue of task conflicts [224] arising from joint multi-task training is also a problem that needs to be considered. Therefore, _developing an end-to-end MLLM generalist for various vision and vision-language tasks beyond text output remains a significant challenge._

Given these challenges, developing an end-to-end generalist MLLM requires a more effective information transmission method than conventional text messages and naive embeddings. This method should ensure that task information and feedback gradients are accurately and flexibly communicated between the central MLLM and multi-task decoders while preventing task conflicts across various visual domains and input/output formats. In addition, multi-task datasets for generalist MLLMs need to be well-prepared. Despite the abundance of annotations in the community, the diverse and inconsistent formats of these annotations across different tasks make it challenging to develop a unified dataset that effectively supports multi-task learning.

In this work, we introduce VisionLLM v2, an end-to-end generalist MLLM designed for a wide array of vision and vision-language tasks. This model not only performs typical visual question answering but also extends to image generation, image editing, and open-ended object detection/instance segmentation/pose estimation across diverse image domains. To facilitate information transmission between the MLLM and multiple downstream task decoders, we introduce the **super link** technique, which consists of two components: (1) _Routing Token_: special tokens (_e.g._, [DET], [POSE], and [GEN]) added to the MLLM's vocabulary. Whenever the MLLM predicts a specific routing token, it triggers the selection of the appropriate decoder. (2) _Super-Link Queries_ randomly initialized learnable weights bound to the routing tokens. These queries are appended after the routing tokens and processed by the MLLM to extract task-specific information, which is then sent to the target decoder. This method enables flexible task information transmission, allows decoder gradients to backpropagate to the MLLM, and avoids task conflicts by ensuring the queries are bound to routing tokens and not shared across tasks.

Furthermore, we carefully collected and curated training data from hundreds of public vision and vision-language tasks to support various tasks. The data includes high-quality examples of visual question answering, visual perception, recognition, and understanding tasks from various sources such as natural scenes, remote sensing images, medical images, and industrial images. To ensure effective training with these extensive datasets, we also implemented a multi-stage joint training strategy, integrating new abilities and reaching a performance comparable to the expert models while maintaining the MLLM's foundational VQA capabilities.

These designs endow VisionLLM v2 with three distinct characteristics: (1) _Generality_. With one suit of parameters, our model can be generalized to different tasks using different text and visual prompts. To our knowledge, it is the first end-to-end model to support hundreds of vision-language tasks while achieving performance comparable to expert models. (2) _Openness_. By employing open-ended decoders, our model allows users to freely define tasks through multimodal prompts, breaking away from the constraints of closed-set models limited to predefined tasks or categories. Furthermore,

Figure 1: **Illustration of three information transmission methods. (a) Text-based method shows MLLM connected to various downstream tools via text messages, capable of handling multiple tasks but suffering from inefficient information transfer. (b) The embedding-based method displays a connection using learnable embeddings, which facilitates efficient information transfer but lacks support for multitasking. (c) Our method employs a “super link” technique, where a unified MLLM interfaces with multiple task decoders through super links, supporting over 100 diverse tasks.**

users can flexibly combine various tasks into more complex ones through multi-round dialogue. (3) _Multimodal In-Context Ability_. With multimodal inputs and outputs, our model demonstrates extensive versatility and exhibits superiority over the previous in-context models with single-modal outputs [184; 8]. These features distinguish our model from previous approaches, and establish a leading foundational MLLM for various vision and vision-language applications.

In summary, our main contributions are listed as follows:

(1) We propose VisionLLM v2, the first end-to-end generalist MLLM model to accomplish hundreds of vision and vision-language tasks1, covering visual perception, understanding, and generation. It not only addresses the limitation of LLMs being confined to text outputs but also supports using textual, visual, and in-context instructions to flexibly combine tasks for real-world applications.

Footnote 1: We consider tasks such that those with differing input and output formats, or those involving data from different domains as distinct tasks.

(2) We introduce the super-link technique, which integrates the MLLM with task-specific decoders. This integration facilitates end-to-end optimization across both linguistic and visual tasks. Additionally, we meticulously collect and re-organize data from a broad range of domains and develop an in-context learning dataset. These efforts lay a solid foundation for our progressive joint training process and enable the model to benefit from individual tasks.

(3) We comprehensively evaluate the proposed model on a wide range of vision and vision-language tasks, from visual perception to visual understanding, from weak interaction (_e.g._, closed-set) to strong interaction (_e.g._, visual prompt + language prompt), from common-seen domains to long-tailed domains (_e.g._, medical, remote-sensing, industry), as shown in the rightmost subfigure of Figure 1. In addition, with a generalist model, our method achieves comparable performance with the task-specialized models in various standard benchmarks.

## 2 Related Work

### Multimodal Large Language Model

**Conventional MLLMs**. With the advancement of large language models (LLMs) [145; 146; 21; 215; 171; 37; 172; 13; 170; 9; 103; 7; 59; 43; 22], multimodal large language models (MLLMs) have also gained significant momentum recently. Notable commercial models include GPT-4V [2], Gemini series [169; 150], Claude-3 [10], and Qwen-VL-Max [14], known for their outstanding performance. Early open-source MLLMs like InstructBLIP [42], LLaVA [107] and MiniGPT-4 [223] fine-tune on instruction-following datasets. InternVL [34; 33] series models align a large-scale vision encoder with LLMs and perform comparably to commercial models. Efficient MLLMs [100; 228; 38] have also studied. However, these models only can output text, restricting their applications.

**Extension of MLLMs' Text Output**. To extend MLLMs to downstream tasks, models like Kosmo-2 [140], Shikra [27], VisionLLM [182], Ferret [201; 212], and All-Seeing V2 [180] achieve this using specially-designed tokens or encoding coordinates as text tokens. Despite these advancements, using LLMs solely as visual decoders falls short of resolving the fine-grained visual context needed for precise detection and segmentation. The other line of works focus on broadening the modality scope. AnyGPT [210] builds a multimodal text-centric dataset for any-to-any multimodal generation (text, image, speech, music) with sequence modeling. Chameleon [168] uses fully token-based representations for both texts and images, capable of understanding and generating interleaved image-text sequences. CM3leon [5; 205] are autoregressive models for text-to-image and image-to-text tasks. All these works could unify image understanding and generation in one network. Our model can support more vision and vision-language tasks.

**MLLMs w/ Downstream Tools**. Recent works [116; 187; 166; 111; 117; 47; 17; 191; 68; 48] have integrated external tools for vision-centric tasks, transmitting task information to these tools via text messages. However, such text-based communication between LLMs and tools hinders end-to-end optimization. Another category of approaches [89; 148; 218; 83; 164; 163; 53; 54; 136; 44; 50; 69] feeds the output embeddings of LLMs into a special decoder and trains them end-to-end to enhance information communication. However, they only support semantic segmentation or image generation tasks. In this work, we target to develop an end-to-end MLLM generalist for diverse vision and vision-language tasks beyond text output.

### Vision Generalist Model

**Unified Vision Model.** The unified model approach integrates multiple visual tasks into a single framework, enhancing efficiency and reducing the complexity of deploying separate models for each task. Works such as Pix2Seq-D [29], SEEM [230], and Semantic-SAM [93] focus on unifying the segmentation interface, achieving promising results. Grounding-DINO [112] and VisionLLM [182] explore open-set detection grounded by language, while UniPose [198] excels in pose estimation. Additionally, pioneering works [227; 224; 95; 121; 229; 189] aim to design a unified model capable of solving multiple tasks, including detection, segmentation, captioning, _etc_. Their results demonstrate the feasibility of a single model performing diverse tasks.

**Visual Prompting.** Visual prompting has emerged as a novel paradigm by providing visual marks in the input instruction. It requires the model to pay attention to the specific region on the image when answering the question. Techniques like red circle [157], SoM [196], AutoVP [173], ILM-VP [24], and PIVOT [131] significantly reduce the need for textual prompt engineering, assisting models in focusing on relevant visual content. Similar to in-context learning in LLMs, Painter [183], DINO v2 [91], and SegGPT [184] leverage visual context to improve learning efficiency and adaptability, enabling models to adapt to new tasks with minimal input.

**Diffusion Model as Interface.** Diffusion models are a flexible interface between users and visual tasks, facilitating a more intuitive interaction paradigm. InstructCV [51] and InstructDiffusion [56] exemplify using of natural language instructions to guide visual generation and manipulation. Pix2Seq v2 [30] showcases the potential of diffusion models in generating sequences of visual tokens, bridging the gap between vision and language.

Different from these works, our VisionLLM v2 integrating LLMs extends vision generalist to support a broader range of vision-language tasks and explore various visual prompting paradigms, thereby significantly broadening the scope of application.

## 3 VisionLLM v2

### Model Design

The overall architecture of VisionLLM v2 is depicted in Figure 2. It mainly consists of four parts: (1) an image encoder and a region encoder that encode the image-level and region-level information; (2) a large language model (LLM) that models the multimodal inputs and generates satisfactory textual responses; (3) a series of task-specific decoders for performing downstream tasks; (4) a super link that uses routing tokens and super-link queries for efficient and conflict-free information transmission. We detail each component in the following.

**Tokenization.** VisionLLM v2 is flexible for handling multimodal input. (1) _For text prompts_, we employ the text tokenizer to tokenize them into distinct vocabulary indices, which can be further processed by LLM and result in the text features \(F_{\text{text}}\in\mathbb{R}^{L\times C}\), where \(L\) denotes the length of input text, and \(C\) is the channel dimension of LLM.

(2) _For an image input_, we utilize a pre-trained vision foundation model, such as CLIP [144], to extract image features. Recognizing that current vision models operate the images at a low resolution, we adopt the dynamic resolution approach [33] to process the input images. Specifically, the input image is first automatically matched to an optimal aspect ratio from a predefined ratio set. Subsequently, the image is scaled up to a higher resolution based on the selected aspect ratio and divided into \(P\) square patches, each whose resolutions are 336\(\times\)336. These local patches, along with a 336\(\times\)336 global image \(I_{\text{global}}\), are processed by the image encoder to capture both holistic scenes and fine-grained details, resulting in image features \(F_{\text{img}}\in\mathbb{R}^{576(P+1)\times C}\).

(3) _For a visual prompt_, we employ binary masks to flexibly represent the visual prompts, such as point, box, scribble, and mask. To extract the region embedding, we first concatenate the binary mask with the input image along the channel dimension and then process it with three convolutional layers to downsample by a factor of 14 (see appendix for more details). We further augment this feature map by adding the feature map of the global image \(I_{\text{global}}\). Finally, grid sampling is used to extract features within the masked regions, and these features are averaged to form the features of the visual prompt \(F_{\text{vpt}}\in\mathbb{R}^{1\times C}\).

**Large Language Model.** Following previous works [107; 213; 60], both the images and visual prompts are projected to the feature space of the LLM. The LLM plays a central role in our modeland is used to model multimodal inputs, parse user instructions, and generate appropriate responses. In this work, we adopt the commonly used Vicuna-7B [219] as the LLM in our network.

**Task-specific Decoders.** To enhance the capacities of MLLM, we equip our model with several task-specific decoders. Specifically, we use Grounding DINO [112] for object-level localization. We additionally add a mask decoder upon it to obtain the segmentation ability. For pose estimation, we adopt UniPose [198] as the keypoint decoder. Moreover, we incorporate Stable Diffusion [152] and InstructPix2Pix [20] as the image decoders, endowing our model with the capability to generate and edit images. We discard these decoders' text encoders and link them with MLLM via the super link technique, which will be detailed explained in Section 3.2. In this way, the decoders can be trained end-to-end with the entire network, ensuring the effective transmission of task information and increasing the openness of these decoders.

### Super Link Technique

For the text-only output tasks, such as image-level and region-level VQA, we directly take the plain text generated by LLM as the final output. For visual perception and visual generation tasks, we propose the super link technique to tackle the challenge of selecting the appropriate decoder, avoiding task conflicts, and facilitating effective information transmission between the LLM and the decoders. The super link comprises two parts:

(1) _Routing Token._ We add the routing tokens, _e.g.,_ [DET], [POSE], [SEG], [GEN], [EDIT], as special tokens to the original LLM vocabulary. When the model intends to complete the downstream task using one of the decoders, LLM would include the corresponding routing token in its textual response. To enable the model to discern which tasks to perform and which routing tokens to output, we construct a series of instruction templates for different tasks using ChatGPT [4].

(2) _Super-Link Queries_. For each decoder, we define the super-link queries as a fixed set of embeddings denoted as \(Q_{\text{link}}\in\mathbb{R}^{N\times C}\), where \(N\) is the number of queries. They are randomly initialized and serve as the bridge between LLM and task-specific decoders. Whenever the LLM predicts the routing token, the super-link queries would be automatically appended after the input embeddings of the routing token. We then extract their corresponding last-layer hidden states \(H_{\text{link}}\) and apply an MLP projection to obtain \(\hat{H}_{\text{link}}\). Finally, \(\hat{H}_{\text{link}}\) is sent into the specific decoders as a condition to perform the downstream tasks. In the following, we illustrate how to integrate \(\hat{H}_{\text{link}}\) into decoders for visual perception and generation, respectively.

**Visual Perception** covers a wide range of visual tasks, such as open-ended/closed-set object detection, instance segmentation, pose estimation, _etc._ VisionLLM v2 supports using both text and visual prompts to define these tasks. We list an example in the following figure. <image> and <region> are the placeholders that will be replaced by image and region embeddings before being fed into the LLM. Here, we take Example 1 of interactive segmentation for clarification. The user prompts the model to segment specific regions within a question. MLLM sequentially lists the region names followed by a routing token [SEG] in the response. Remember that the proposed method would automatically append the super-link queries after the routing token. In that way, we can obtain the

Figure 2: **Overall architecture of the proposed VisionLLM v2. It receives the image and text/visual prompts as inputs. The central LLM parses the user instructions and generates the textual responses. Besides outputting the plain text, LLM can also output the special routing token such as [DET] when needed. The super-link queries would be automatically appended after the routing token embeddings and further processed by LLM. They play as the bridge for connecting LLM and task-specific decoders. In this way, our generalist model can support hundreds of visual tasks. The detailed architecture about connecting the LLM with task-specific decoders can be found in Figure A13.**

per-region representations by extracting the output hidden states of MLLM from corresponding super-link queries and pooling them into one embedding. These embeddings are fed into a segmentation decoder as the conditional feature, requiring only a single forward to produce segmentation results for all regions. In the following, we show a template example for interactive segmentation.

``` Example1: Text Prompt + Visual Prompt for Interactive Segmentation. USER: <image> Could you please segment all the corresponding objects according to the visual prompts as region1 <region>, region2 <region>? ASSISTANT: Sure, these objects are region1 [SEG], region2 [SEG]. ```

**Visual Generation** is also a wide topic covering a number of different tasks, such as generation, editing, variation, personalization, _etc_. In VisionLLM v2, we focus on two fundamental tasks, _i.e._, text-to-image generation and instruction-based image editing. We use Stable Diffusion v1.5 (SD) as our tool in the text-to-image generation task. We abandon its text encoder and use the output hidden states of the MLLM as the image generation condition for SD. Image editing task [82] can also be accomplished in the same paradigm by using both image and text prompts as inputs. In the following, we list a template example for text-to-image generation.

``` Example2: Text Prompt for Text-to-Image Generation. ASSISTANT: Of course, here it is [GEN]. ```

_Discussion._ Some previous works have used the special token or learnable queries independently. InstructBLIP [42], ep-ALM [158], and MAPL [125] use learnable queries (i.e., soft prompts) to connect the modality encoders and LLM. FROMAGe [84] uses a special token for image-text retrieval so as to handle multimodal outputs, where the images are not generated from the network end-to-end. However, these works still remain constrained to text-based outputs. The proposed super link is the seamless integration of the two techniques. Despite the simplicity of our method, it is able to extend MLLMs to handle hundreds of tasks by largely extending the output formats, _e.g._, box, mask, keypoint and image. Meanwhile, it can address several challenges when scaling up various tasks: (i) precise decoder invocation, (ii) mitigating task conflicts and (iii) efficient message transmission in an end-to-end manner.

### Training Strategy

Current MLLMs [89, 218, 44] face reduced conversational abilities when augmented with additional capacities. To create a generalist model capable of handling hundreds of tasks without compromising vision understanding, we propose a three-stage training strategy, where the first stage focuses on building an MLLM with strong image-level and region-level vision understanding. In the subsequent stages, we add task-specific decoders and continue training to equip the model with advanced capabilities.

**Stage-1: Multimodal Training.** In the first stage, we follow the training settings of LLaVA [107, 105], comprising pre-training and instruction tuning phases. The pre-training phase aims to establish the image-level and region-level vision-language alignment, where only the region encoder and the projections for image embedding and region embedding are trained for efficiency. The instruction tuning phase unfreezes the LLM and trains the model on a wide range of high-quality instruction data. After the training in this stage, we can obtain a strong MLLM with excellent conversation ability, which we term as **VisionLLM v2-Chat**.

**Stage-2: Multi-capacity Fine-tuning.** At this stage, we integrate task-specific decoders into the model and perform multi-task joint training. In addition to the instruction data utilized in stage-1, we incorporate extensive visual datasets such as COCO [104], ADE20K [222] for their specific tasks. We construct a series of instruction templates for these visual datasets to perform instruction tuning, ensuring that the LLM can accurately invoke the downstream decoders. During this stage, the region encoder and all decoders undergo training, and we only finetune the input and output embeddings of the LLM to maximally preserve its original conversational ability.

**Stage-3: Decoder-only Fine-tuning.** Since the decoders cannot converge within a single epoch, we further train the decoders for 12 epochs using visual datasets while freezing all other components. It is noted that the super-link queries continue to be trained during this stage. After finishing the three-stage training, our model has diverse capacities for visual tasks while maintaining effectiveness in global vision understanding, named **VisionLLM v2**.

## 4 Experiments

### Implementation Details

**Dataset Details.** To support the joint training of our model, we meticulously collect and re-organize the datasets across a wide range of tasks from publicly available sources. For the first stage training, we utilize a substantial amount of high-quality instruction data for both image-level and region-level visual question answering, including ShareGPT4V [28], All-Seeing [181], VQAv2 [57], _etc._ In the last two stages, we further incorporate extensive visual datasets, _e.g._, COCO [104], RefCOCO/+/g [204, 126], LIAION-Aesthetics [3], to enhance our model with numerous capacities. These datasets encompass multiple tasks such as object detection, pose estimation, image generation, and span various domains such as natural scenes, remote sensing images, medical images, _etc._ To facilitate the training of diverse datasets in our MLLM framework, we construct a series of instruction templates for different tasks, which are completely listed in the Appendix. Additionally, we also collect a multimodal dataset termed **MMIC** focusing on visual prompting and in-context learning. The data in our MMIC comes from various sources, including fine-grained visual recognition, object detection, instance segmentation, and pose detection. We elaborate on all datasets used in this work as well as the dataset construction of MMIC in the Appendix.

**Model Details.** We adopt the CLIP-L/14 [144] as the image encoder and Vicuna-7B-v1.5 [219] as the language model. Grounding-DINO [112] and UniPose [198] are selected as object decoder and keypoint decoder, respectively. And for these two decoders, we experiment with Swin-T [115] backbone. Additionally, image decoders are kept as Stable Diffusion v1.5 [152] for image generation and InstructPix2Pix [20] for image editing. All these components load the pre-trained weights while the region encoder is randomly initialized. For visual perception and visual generation tasks, the number \(N\) of super-link queries is set to 4 and 64, respectively. During training, we adjust the dataloader so that each GPU processes samples from only one dataset. More training details are provided in the Appendix.

In the following subsections, we present the experimental results to cover as many tasks, interactive modes, and domains. It is noted that all the results of our method are reported using **a single generalist model** with the same parameters. More results can be found in the Appendix.

### Multimodal Benchmarks

**Multimodal Dialogue.** We first evaluate our models on academic-oriented VQA datasets and recent instruction-following datasets for MLLMs, as presented in Table 1. The results clearly demonstrate that our models outperform previous methods under the same parameter scale, particularly on the instruction-following datasets. For instance, VisionLLM v2-Chat surpasses LLaVA-NeXT-7B [106] by +9.7 and +7.0 points on MMBench-EN/CN [114], respectively. Additionally, we find that VisionLLM v2 achieves comparable performance to VisionLLM v2-Chat on these multimodal benchmarks and even performs better on some benchmarks, such as POPE [101], a popular benchmark for evaluating object hallucination. This phenomenon indicates that our framework effectively mitigates the issue of multi-task conflict and maintains proficiency in conversational ability.

\begin{table}
\begin{tabular}{l|c c|c c c|c c|c c c} \multicolumn{1}{c}{} & \multicolumn{1}{c}{visual} & \multicolumn{1}{c}{L} & \multicolumn{1}{c}{academic-oriented dataset} & \multicolumn{1}{c}{instruction-following datasets} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\  & encoder & LLM & & & & & & & & & & \\ \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline InstructBlIP-7B [42] & EVA-g & Vicuna-7B & - & 49.2 & 34.5 & 60.5 & 50.1 & - & - & 36.0/2.7 & 53.4/ & 58.8/ 38.1 \\ InstructBlIP-13B [42] & EVA-g & Vicuna-13B & - & 49.5 & 33.4 & 63.1 & 50.7 & 78.9 & 1212.8 & - & -/ -/- \\ Shlrx [27] & CILP-1 & Vicuna-13B & 77.4 & - & - & - & - & - & - & 58.8/ - - & -/- \\ DEFICKS-80B [72] & CILP-1 & LLP-1 & LLP-3B & 68.0 & 45.2 & 36.0 & - & 30.9 & - & - & 54.5/ 38.1 & -/- 53.2/ - \\ Open-VL-Chat [14] & CILP-Q & Q-even-7B & 78.2 & 75.7 & 57.5 & 38.9 & 68.2 & 61.5 & - & 14875.6 & 60.6/ 56.7 & 58.2/ 65.4/ 37.8 \\ InstructVL-7B [34] & V7+6B & Vicuna-7B & 79.3 & 62.9 & 52.5 & 66.2 & 57.0 & 86.4 & 15251. & 64.6/ 57.6 & 60.2/ -/- \\ InstructVL-13B [34] & ViT-6B & Vicuna-13B & 80.2 & 63.9 & 54.6 & 70.1 & 58.7 & 87.1 & 1546.9 & 66.6/ 61.9 & 62.4/ -/- \\ LLaVA-15-7B [105] & CILP-1 & Vicuna-7B & 78.5 & 62.0 & 50.0 & 66.8 & 58.2 & 85.9 & 1510.7 & 64.3/ 58.3 & 58.6/ 66.1/ 37.3 \\ LLaVA-NeXT-7B [106] & CILP-1 & Vicuna-7B & 81.8 & 64.2 & 57.6 & 70.1 & 64.9 & 86.5 & 1519.0 & 67.4/ 60.6 & -/ 70.2/ - \\ LLaVA-NeXT-13B [106] & CILP-1 & Vicuna-13B & 82.8 & 65.4 & 60.5 & 73.6 & 67.1 & 86.2 & 1575.0 & 70.0/ 64.4 & -/ 71.9/ - \\ VisionLLM v2-Chat & CLIP-L & Vicuna-7B & 81.4 & 65.1 & 54.6 & 94.4 & 66.3 & 87.5 & 1515.2 & 77.1/ 67.6 & 65.4/ 71.7/ 41.6 \\ VisionLLM v2 & CLIP-L & Vicuna-7B & 80.8 & 65.1 & 54.8 & 94.2 & 64.7 & 88.8 & 1495.6 & 76.3/ 66.8 & 65.6/ 71.7/ 42.2 \\ \end{tabular}
\end{table}
Table 1: **Comparison with SoTA models on multimodal dialogue benchmarks.** The academic-oriented datasets include: VQAv2 test-dev [57], GQA test-balanced [71], VizWiz test-dev [62], ScienceQA test [154] and TextVQA val [160]. The instruction-following datasets include: POPE [101], MME [49], MMBench-EN/CN [114], SEED-Bench (all/image/video) [55]. \({}^{*}\)The training annotations of the dataset are observed during training.

**Region Recognition.** The region recognition task needs the model to identify the object category given the ground-truth bounding box. We compare our method with both feature-based and text-output approaches in Table 2(a). Feature-based methods, such as RegionCLIP [221] and ASM [181], compute similarity scores between region visual features and candidate category text features. In contrast, text-output methods [25, 60, 207] directly predict the category name using a single word or phrase, embracing the advantage of openness. As shown in the table, our models demonstrate the significant superior performance on COCO [104], long-tail LVIS [61] and part-level PACO [147].

**Visual Commonsense Reasoning.** Visual commonsense reasoning (VCR) requires the model to possess strong region-level question-answering and reasoning abilities, as it needs to select not only the correct answer but also the correct rationale behind it. We present the comparison results on the VCR dataset [209] in Table 2(b). Without task-specific fine-tuning, VisionLLM v2-Chat achieves an accuracy of 82.9% in the crucial Q\(\rightarrow\)AR task, which precedes the previous best model, ASMv2 [180], by +3.5 points. VisionLLM v2 also outperforms the previous methods for all the metrics, highlighting the promising common sense reasoning capability of our model.

### Visual Perception Tasks

**Object Detection and Instance Segmentation.** In Table 3, we compare the results of VisionLLM v2 with state-of-the-art methods on two fundamental vision tasks, _i.e._, object detection, and instance segmentation. As can be seen, using the lightweight backbone Swin-T, our generalist model achieves

\begin{table}
\begin{tabular}{l|c|c c c c|c c c c} method & \multicolumn{3}{c|}{Vd Acc (\%)} \\ \cline{3-10} \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Q\(\rightarrow\)AR} & \multicolumn{1}{c}{Q\(\rightarrow\)AR} & \multicolumn{1}{c}{Q\(\rightarrow\)AR} & \multicolumn{1}{c}{Q\(\rightarrow\)AR} \\ \hline ViLBERT [120] & 72.4 & 74.5 & 54.0 \\ Unicycle-V[74] & 72.6 & 74.5 & 54.5 \\ VLBERT [162] & 75.5 & 77.9 & 58.9 \\ ERNIE-VL-L [202] & 78.5 & 83.4 & 65.8 \\ VILLA [52] & 78.5 & 82.6 & 65.2 \\ GPTRad-78 [21] & 87.4 & 89.6 & 78.6 \\ ASMv2 [180] & 87.8 & 88.8 & 78.4 \\ ASMv2 [180] & 88.4 & 89.9 & 79.4 \\ \hline VisionLLM v2-Chat & 90.0 & 91.9 & 82.9 \\ VisionLLM v2 & 89.8 & 91.7 & 82.5 \\ \end{tabular}
\end{table}
Table 4: **Comparison of pose estimation performance.**\({}^{*}\) indicates that the results rely on ground-truth bounding boxes for top-down methods.

\begin{table}
\begin{tabular}{l|c|c c c c|c c c} method & \multicolumn{3}{c|}{QCOCO} & \multicolumn{3}{c|}{LVIS} & \multicolumn{3}{c}{PACO} \\  & \multicolumn{1}{c|}{APAP} & \multicolumn{1}{c|}{Acc (\%)} & \multicolumn{1}{c|}{SS} & \multicolumn{1}{c|}{S-IoU} & \multicolumn{1}{c}{SS} & \multicolumn{1}{c}{-IoU} \\ \hline CLIP [144] & 58.9 & - & - & - & - & - & - & - \\ RegionCLIP [221] & 58.3 & - & - & - & - & - & - & - \\ LLVA [107] & - & 40.0 & 49.9 & 19.8 & 42.2 & 14.6 \\ Shixra [27] & - & 53.9 & 49.7 & 19.8 & 43.6 & 11.4 \\ GPTRad [213] & - & 64.0 & 51.3 & 12.0 & 48.0 & 12.1 \\ ASM [181] & 69.3 & - & - & - & - & - & - \\ RegionGPT [60] & 70.0 & 80.6 & - & - & - & - & - \\ Ospecy [207] & - & - & - & 65.2 & 38.2 & 73.1 & 52.7 \\ \hline VisionLLM v2-Chat & 81.9 & 90.5 & 67.3 & 42.7 & 63.8 & 36.3 \\ VisionLLM v2 & 81.9 & 90.4 & 73.0 & 51.3 & 70.9 & 47.6 \\ \end{tabular}
\end{table}
Table 2: **Comparison of region recognition and visual commonsense reasoning performance.** (a) SS and S-IoU represent semantic similarity and semantic IoU, which originated from [207]. (b) Q, A, and R denote question, answer, and rationale, respectively. X\(\rightarrow\)Y means that the model needs to select option Y conditioned on X. \({}^{*}\)The model is finetuned on the dataset.

\begin{table}
\begin{tabular}{l|c|c|c c c c c c c} method & type & backbone & \multicolumn{3}{c|}{detection (COCO)} & instance seg. (COCO) & \multicolumn{3}{c}{detection (CrowHoman)} \\  & & & AP & \(\%\) & AP\({}_{50}\) & AP\({}_{75}\) & AP\({}_{50}\) & AP\({}_{75}\) & AP\({}_{50}\) & mMRL & Recall \\ \hline Deformable-DETR [226] & ResNet50 & 46.2 & 65.2 & 50.0 & - & - & - & 89.1 & 50.0 & 95.3 \\ DDQ [214] & ResNet50 & 52.0 & 69.5 & 57.2 & - & - & 93.8 & 39.7 & 98.7 \\ ViTPC-B [93] & Specialist & VIT-B & 56.0 & - & - & 48.0 & - & - & - & - \\ Grounding DINO [112] & Swin-T & 57.2 & - & - & - & - & - & - & - \\ Mask2Former [35] & ResNet50 & - & - & 43.7 & - & - & - & - & - \\ Mask DINO [92] & ResNet50 & 51.7 & - & - & 46.3 & - & - & - & - \\ \hline UniFGV [39] & ViT-B & - & - & - & - & - & - & 92.5 & - & - \\ Hulk [185] & ViT-L & - & - & - & - & - & - & 92.2 & - & - \\ Huk [185] & ViT-L & - & - & - & - & - & - & 93.0 & - & - \\ Pix2Seq \(\neq\)Y [30] & ViT-B & 46.5 & - & 38.2 & - & - & - & - & - \\ VisionLLM [182] & ResNet50 & 44.8 & 64.1 & 48.5 & 25.2 & 50.6 & 22.4 & - & - & - \\ Uni-Perceiver-V2 [95] & Swin-B & 58.6 & - & - & 50.6 & - & - & - & - & - \\ UNINET [19] & ResResNet50 & 51.3 & 68.4 & 56.2 & 44.9 & 67.0 & 48.9 & - & - \\ GLEE-Lite [190] & ResNet50 & 55.0 & - & - & 48.4 & - & - & - & - & - \\ GLEE-Plus [190] & Swin-T & 60.4 & - & 53.0 & - & - & - & - & - \\ VisionLLM v2 & Swin-T & 56.7 & 74.5 & 62.2 & 47.8 & 71.8 & 52.0 & 93.1 & 44.7 & 98.5 \\ \end{tabular}
\end{table}
Table 3: **Comparison of object detection and instance segmentation performance.** Instance seg. means instance segmentation segmentation. \({}^{*}\)The model is finetuned on the dataset.

[MISSING_PAGE_FAIL:9]

**Shared _vs._Unshared Super-Link Queries for Different Decoders.** To determine if one set of super-link queries is sufficient for all decoders, we conducted an ablation study by either using shared queries for all decoders or defining separate queries for each decoder. In this ablation, we only train the decoders and super-link queries while freezing all other components as the training setting of stage-3. In Figure 4, we plot the performance of box AP (using the object decoder) and keypoint AP (using the keypoint decoder) on COCO. We observe that the keypoint AP would decrease over training when using shared queries, which may be attributed to the fact that most data are used for object decoder. Besides, the box AP with shared queries is also inferior to decoupled ones. Therefore, we define separate super-link queries for each decoder in our model.

**Multi-Task Influence.** As indicated by previous works [225; 206], different tasks with shared parameters may cause conflict with each other. This is mainly due to inconsistent optimization in multi-task learning. To investigate the mutual influence of multi-task joint training in our framework, we start from the same checkpoint and train the model on a single task (image VQA, instance segmentation, or image generation) for 1000 iterations. We record the loss change for all three tasks in Table 6. In the table, a decrease in the loss value indicates beneficial training for the task, while an increase is detrimental. We can observe that training on image VQA is advantageous for all three tasks, which is reasonable as the conversation ability of MLLM is enhanced. Whereas training exclusively on instance segmentation or image generation leads to conflicts with other tasks. This aligns with the findings in Unipercirever-MoE [225].

**One-Stage _vs._Three-Stage Training.** Some previous generalist models [176; 15] train the model in one stage. Our model encompasses much more tasks and thus introduces a training conflict: the MLLM requires only 1 epoch of training on chat data to prevent overfitting, whereas the decoders need longer training epochs (e.g., Grounding-DINO need 12 epochs of training on visual data) to achieve convergence. One possible solution for one-stage training is to give a higher sample ratio for the visual data. In the following, we conduct the ablation to study the effect of one-stage v.s. three-stage training. We use image-level chat data, COCO, and COCO-Pose for image understanding, instance segmentation, and pose estimation, respectively. For one-stage training, we repeat the COCO and COCO-Pose datasets 12 times. As can be seen from Table 7, the conversation ability of the model is significantly decreased due to extreme data imbalance. And the performance of instance segmentation and pose estimation is also slightly reduced. These results prove the effectiveness of our three-stage training.

## 5 Conclusion & Limitation

In this paper, we presented VisionLLM v2, a comprehensive MLLM that unifies visual perception, understanding, and generation within a single framework. The proposed super link mechanism facilitates flexible information transmission between the MLLM and task-specific decoders, addressing training conflicts and enhancing gradient feedback. Experiments show that VisionLLM v2 achieves performance comparable to specialized models while maintaining broad applicability.

Regarding limitations, our model's training encompasses three stages, which are relatively complex. Moreover, the integration of downstream tools has only been preliminarily validated. Future work will further explore solutions to these issues, aiming to enhance the model's performance and efficiency.

**Broader Impact.** We envision that this work will further promote the fusion of visual and language tasks. In addition, since our work is built on open-source pre-trained vision foundation models and large language models, requiring low training resources, thus reducing the carbon footprint. We do not foresee obvious undesirable ethical/social impacts at this moment.

\begin{table}
\begin{tabular}{l|c c c} Train / Test & Image VQA & Inst. Seg & Image Gen. \\ \hline Image VQA & -0.01 & -0.11 & -0.04 \\ Inst Seg. & +0.04 & -0.12 & + 0.19 \\ Image Gen. & +0.03 & +0.02 & -0.04 \\ \end{tabular}
\end{table}
Table 6: **Ablation on the multi-task influence.** The numbers denote the loss change when the model is fine-tuned on a single task.

\begin{table}
\begin{tabular}{l|c c c|c}  & TextVQA & MME & MMB EN/CN & COCO & COCO-Pose \\ \hline One-stage & 53.2 & 1284.4 & 61.9 / 51.4 & 54.9 / 44.6 & 74.1 \\ Three-stage & 66.2 & 1507.1 & 77.8 / 68.5 & 56.3 / 47.6 & 74.2 \\ \end{tabular}
\end{table}
Table 7: **Ablation on the one-stage and three-stage training.** We evaluate the models on image VQA, instance segmentation and pose estimation

## Acknowledgement

This paper is supported by the National Key R&D Program of China (No.2022ZD0161000), the General Research Fund of Hong Kong (No.17200622, 17209324), and the National Natural Science Foundation of China (No. 62376134, 62372223). Tong Lu and Zhe Chen are supported by the China Mobile Zijin Innovation Insititute (No. NR2310J7M). Zhe Chen is also supported by the Youth PhD Student Research Project under the National Natural Science Foundation (No. 623B2050).

## References

* [1] Gpt-4v dataset. https://huggingface.co/datasets/laion/gpt4v-dataset.
* [2] Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf.
* [3] Laion-aesthetics. https://laion.ai/blog/laion-aesthetics/.
* [4] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [5] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. _arXiv preprint arXiv:2201.07520_, 2022.
* [6] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In _ICCV_, pages 8948-8957, 2019.
* [7] Meta AI. Introducing meta llama 3: The most capable openly available llm to date. https://ai.meta.com/blog/meta-llama-3/, 2024.
* [8] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _NeurIPS_, 35:23716-23736, 2022.
* [9] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.
* [10] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com, 2024.
* [11] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.
* [12] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [13] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Bingyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [14] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. _arXiv preprint arXiv:2308.12966_, 2023.
* [15] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasirlar. Fuyu-8b: A multimodal architecture for ai agents, 2023.

* [16] Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur. Birdsnap: Large-scale fine-grained visual categorization of birds. In _CVPR_, pages 2011-2018, 2014.
* [17] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. _Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf_, 2(3):8, 2023.
* [18] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Marcal Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene text visual question answering. In _ICCV_, pages 4291-4301, 2019.
* [19] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discriminative components with random forests. In _ECCV_, pages 446-461. Springer, 2014.
* [20] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In _CVPR_, pages 18392-18402, 2023.
* [21] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _NeurIPS_, 33:1877-1901, 2020.
* [22] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. _arXiv preprint arXiv:2403.17297_, 2024.
* [23] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In _Proceedings of the 29th International Conference on Computational Linguistics_, pages 1511-1520, 2022.
* [24] Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. Understanding and improving visual prompting: A label-mapping perspective. In _CVPR_, pages 19133-19143, 2023.
* [25] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for a lite vision-language model. _arXiv preprint arXiv:2402.11684_, 2024.
* [26] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.
* [27] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llm's referential dialogue magic. _arXiv preprint arXiv:2306.15195_, 2023.
* [28] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* [29] Ting Chen, Lala Li, Saurabh Saxena, Geoffrey Hinton, and David J Fleet. A generalist framework for panoptic segmentation of images and videos. In _ICCV_, pages 909-919, 2023.
* [30] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks. _NeurIPS_, 35:3133-31346, 2022.
* [31] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. _arXiv preprint arXiv:1504.00325_, 2015.
* [32] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In _ECCV_, pages 104-120. Springer, 2020.
* [33] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. _arXiv preprint arXiv:2404.16821_, 2024.
* [34] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. _arXiv preprint arXiv:2312.14238_, 2023.

* [35] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In _CVPR_, pages 1290-1299, 2022.
* [36] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based salient region detection. _TPAMI_, 37(3):569-582, 2014.
* [37] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.
* [38] Xiangxiang Chu, Limeng Qiao, Xinyu Zhang, Shuang Xu, Fei Wei, Yang Yang, Xiaofei Sun, Yiming Hu, Xinyang Lin, Bo Zhang, and Chunhua Shen. Mobilevlm v2: Faster and stronger baseline for vision language model. _ArXiv_, abs/2402.03766, 2024.
* [39] Yuanzheng Ci, Yizhou Wang, Meilin Chen, Shixiang Tang, Lei Bai, Feng Zhu, Rui Zhao, Fengwei Yu, Donglian Qi, and Wanli Ouyang. Unihcp: A unified model for human-centric perceptions. In _CVPR_, pages 17840-17852, 2023.
* [40] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. In _ACL_, pages 845-855, 2018.
* [41] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _CVPR_, pages 3213-3223, 2016.
* [42] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale N Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. _NeurIPS_, 36, 2024.
* [43] DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. _arXiv preprint arXiv:2401.02954_, 2024.
* [44] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In _ICLR_, 2024.
* [45] Bokun Fan. Cnfood-241. https://data.mendeley.com/datasets/fspyss5zbb/1. [Accessed 12-08-2022].
* [46] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling Shao. Concealed object detection. _TPAMI_, 44(10):6024-6042, 2021.
* [47] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified pixel-level vision llm for understanding, generating, segmenting, editing.
* [48] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Vitron: A unified pixel-level vision llm for understanding, generating, segmenting, editing.
* [49] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large language models. _arXiv preprint arXiv:2306.13394_, 2023.
* [50] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. _arXiv preprint arXiv:2309.17102_, 2023.
* [51] Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, and Ahmed M Alaa. Instructcv: Instruction-tuned text-to-image diffusion models as vision generalists. _arXiv preprint arXiv:2310.00390_, 2023.
* [52] Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial training for vision-and-language representation learning. _NeurIPS_, 33:6616-6628, 2020.
* [53] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large language model. _arXiv preprint arXiv:2307.08041_, 2023.
* [54] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. _arXiv preprint arXiv:2310.01218_, 2023.

* [55] Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, and Ying Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. _arXiv preprint arXiv:2404.14396_, 2024.
* [56] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Han Hu, Dong Chen, et al. InstructDiffusion: A generalist modeling interface for vision tasks. _arXiv preprint arXiv:2309.03895_, 2023.
* [57] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In _CVPR_, pages 6904-6913, 2017.
* [58] Jacob M Graving, Daniel Chae, Hemal Naik, Liang Li, Benjamin Koger, Blair R Costelloe, and Iain D Couzin. Deepposekit, a software toolkit for fast and robust animal pose estimation using deep learning. _Elife_, 8:e47994, 2019.
* [59] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C'esar Teodoro Mendes, Allison Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, S. Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuan-Fang Li. Textbooks are all you need. _ArXiv_, abs/2306.11644, 2023.
* [60] Qiushan Guo, Shalini De Mello, Hongxu Yin, Wonmin Byeon, Ka Chun Cheung, Yizhou Yu, Ping Luo, and Sifei Liu. Regionpt: Towards region understanding vision language model. _arXiv preprint arXiv:2403.02330_, 2024.
* [61] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In _CVPR_, pages 5356-5364, 2019.
* [62] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In _CVPR_, pages 3608-3617, 2018.
* [63] Junwen He, Yifan Wang, Lijun Wang, Huchuan Lu, Jun-Yan He, Jin-Peng Lan, Bin Luo, and Xuansong Xie. Multi-modal instruction tuned llms with fine-grained visual perception. _arXiv preprint arXiv:2403.02969_, 2024.
* [64] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, pages 770-778, 2016.
* [65] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* [66] Saihui Hou, Yushan Feng, and Zilei Wang. Vegfru: A domain-specific dataset for fine-grained visual categorization. In _ICCV_, pages 541-549, 2017.
* [67] Xiaobin Hu, Shuo Wang, Xuebin Qin, Hang Dai, Wenqi Ren, Donghao Luo, Ying Tai, and Ling Shao. High-resolution iterative feedback network for camouflaged object detection. In _AAAI_, volume 37, pages 881-889, 2023.
* [68] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. _arXiv preprint arXiv:2403.08857_, 2024.
* [69] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. _arXiv preprint arXiv:2312.06739_, 2023.
* [70] Zhou Huang, Hang Dai, Tian-Zhu Xiang, Shuo Wang, Huai-Xin Chen, Jie Qin, and Huan Xiong. Feature shrinkage pyramid for camouflaged object detection with transformers. In _CVPR_, pages 5557-5566, 2023.
* [71] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and compositional question answering. In _CVPR_, pages 6700-6709, 2019.
* [72] IDEFICS. Introducing idefics: An open reproduction of state-of-the-art visual language model. https://huggingface.co/blog/idefics, 2023.
* [73] Qing Jiang, Feng Li, Tianhe Ren, Shilong Liu, Zhaoyang Zeng, Kent Yu, and Lei Zhang. T-rex: Counting by visual prompting. _arXiv preprint arXiv:2311.13596_, 2023.

* [74] Xuan Ju, Ailing Zeng, Jianan Wang, Qiang Xu, and Lei Zhang. Human-art: A versatile human-centric dataset bridging natural and artificial scenes. In _CVPR_, pages 618-629, 2023.
* [75] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In _CVPR_, pages 5648-5656, 2018.
* [76] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In _ICCV_, pages 1780-1790, 2021.
* [77] Yoshiyuki Kawano and Keiji Yanai. Automatic expansion of a food image dataset leveraging existing categories with domain adaptation. In _ECCVW_, pages 3-17. Springer, 2015.
* [78] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is worth a dozen images. In _ECCV_, pages 235-251, 2016.
* [79] Muhammad Haris Khan, John McDonagh, Salman Khan, Muhammad Shahabuddin, Aditya Arora, Fahad Shahbaz Khan, Ling Shao, and Georgios Tzimiropoulos. Animalweb: A large-scale hierarchical dataset of annotated animal faces. In _CVPR_, pages 6939-6948, 2020.
* [80] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In _First Workshop on Fine-Grained Visual Categorization, IEEE Conference on Computer Vision and Pattern Recognition_, Colorado Springs, CO, June 2011.
* [81] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In _ECCV_, pages 498-517. Springer, 2022.
* [82] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In _ICCV_, pages 4015-4026, 2023.
* [83] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. _NeurIPS_, 36, 2024.
* [84] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. In _International Conference on Machine Learning_, pages 17283-17300. PMLR, 2023.
* [85] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _ICCVW_, pages 554-561, 2013.
* [86] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. _IJCV_, 123:32-73, 2017.
* [87] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. _IJCV_, 128(7):1956-1981, 2020.
* [88] Rollyn Labuguen, Jumpei Matsumoto, Salvador Blanco Negrete, Hiroshi Nishimaru, Hisao Nishijo, Masahiko Takada, Yasuhiro Go, Ken-ichi Inoue, and Tomohiro Shibata. Macaquepose: a novel "in the wild" macaque monkey pose dataset for markerless motion capture. _Frontiers in behavioral neuroscience_, 14:581-54, 2021.
* [89] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. _arXiv preprint arXiv:2308.00692_, 2023.
* [90] Trung-Nghia Le, Tam V Nguyen, Zhongliang Nie, Minh-Triet Tran, and Akihiro Sugimoto. Anabranch network for camouflaged object segmentation. _Computer vision and image understanding_, 184:45-56, 2019.
* [91] Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Chunyuan Li, Jianwei Yang, et al. Visual in-context prompting. _arXiv preprint arXiv:2311.13601_, 2023.
* [92] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. _arXiv preprint arXiv:2206.02777_, 2022.

* [93] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity. _arXiv preprint arXiv:2307.04767_, 2023.
* [94] Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In _AAAI_, volume 34, pages 11336-11344, 2020.
* [95] Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al. Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks. In _CVPR_, pages 2691-2700, 2023.
* [96] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In _CVPR_, pages 10863-10872, 2019.
* [97] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_, pages 19730-19742. PMLR, 2023.
* [98] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _CVPR_, pages 10965-10975, 2022.
* [99] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In _ECCV_, pages 280-296. Springer, 2022.
* [100] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. _ArXiv_, abs/2403.18814, 2024.
* [101] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In _EMNLP_, pages 292-305, 2023.
* [102] Yuxuan Li, Xiang Li, Weijie Li, Qibin Hou, Li Liu, Ming-Ming Cheng, and Jian Yang. Sardet-100k: Towards open-source benchmark and toolkit for large-scale sar object detection. _arXiv preprint arXiv:2403.06534_, 2024.
* [103] Wing Lian, Bleys Goodson, Guan Wang, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". Mistralorca: Mistral-7b model instruct-tuned on filtered openorcv1 gpt-4 dataset. https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca, 2023.
* [104] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In _ECCV_, pages 740-755, 2014.
* [105] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* [106] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024.
* [107] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _NeurIPS_, 36, 2023.
* [108] Huan Liu, Qiang Chen, Zichang Tan, Jiang-Jiang Liu, Jian Wang, Xiangbo Su, Xiaolong Li, Kun Yao, Junyu Han, Errui Ding, et al. Group pose: A simple baseline for end-to-end multi-person pose estimation. In _ICCV_, pages 15029-15038, 2023.
* [109] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In _CVPR_, pages 3917-3926, 2019.
* [110] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei Han. Visual saliency transformer. In _ICCV_, pages 4722-4732, 2021.
* [111] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, et al. Llava-plus: Learning to use tools for creating multimodal agents. _arXiv preprint arXiv:2311.05437_, 2023.
* [112] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. _arXiv preprint arXiv:2303.05499_, 2023.

* [113] Weihuang Liu, Xi Shen, Chi-Man Pun, and Xiaodong Cun. Explicit visual prompting for universal foreground segmentations. _arXiv preprint arXiv:2305.18476_, 2023.
* [114] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.
* [115] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _ICCV_, pages 10012-10022, 2021.
* [116] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li, Jiashuo Yu, et al. Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language. _arXiv preprint arXiv:2305.05662_, 2023.
* [117] Zhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao, Jifeng Dai, and Wenhai Wang. Controlllm: Augment language models with tools by searching on graphs. _arXiv preprint arXiv:2310.17796_, 2023.
* [118] Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis Raptis. Towards end-to-end unified scene text detection and layout analysis. In _CVPR_, pages 1049-1059, 2022.
* [119] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [120] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. _NeurIPS_, 32, 2019.
* [121] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In _The Eleventh International Conference on Learning Representations_, 2022.
* [122] Yunqiu Lv, Jing Zhang, Yuchao Dai, Aixuan Li, Bowen Liu, Nick Barnes, and Deng-Ping Fan. Simultaneously localize, segment and rank the camouflaged objects. In _CVPR_, pages 11591-11601, 2021.
* [123] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. _arXiv preprint arXiv:2404.13013_, 2024.
* [124] Mingcan Ma, Changqun Xia, Chenxi Xie, Xiaowu Chen, and Jia Li. Boosting broader receptive fields for salient object detection. _TIP_, 32:1026-1038, 2023.
* [125] Oscar Manas, Pau Rodriguez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and Aishwarya Agrawal. Mapl: Parameter-efficient adaptation of unimodal pre-trained models for vision-language few-shot prompting. _arXiv preprint arXiv:2210.07179_, 2022.
* [126] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In _CVPR_, pages 11-20, 2016.
* [127] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question answering benchmark requiring external knowledge. In _CVPR_, pages 3195-3204, 2019.
* [128] Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartaq: A benchmark for question answering about charts with visual and logical reasoning. In _ACL_, pages 2263-2279, 2022.
* [129] Minesh Mathew, Viraj Bagal, Ruben Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In _WACV_, pages 1697-1706, 2022.
* [130] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In _ICDAR_, pages 947-952. IEEE, 2019.
* [131] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. _arXiv preprint arXiv:2402.07872_, 2024.
* [132] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas dataset for semantic understanding of street scenes. In _ICCV_, pages 4990-4999, 2017.

* [133] Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo, and Jun Liu. Animal kingdom: A large and diverse dataset for animal behavior understanding. In _CVPR_, pages 19023-19034, 2022.
* [134] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian conference on computer vision, graphics & image processing_, pages 722-729. IEEE, 2008.
* [135] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Journeydb: A benchmark for generative image understanding, 2023.
* [136] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. _arXiv preprint arXiv:2310.02992_, 2023.
* [137] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, and Huchuan Lu. Zoom in and out: A mixed-scale triplet network for camouflaged object detection. In _CVPR_, pages 2160-2170, 2022.
* [138] Youwei Pang, Xiaoqi Zhao, Tian-Zhu Xiang, Lihe Zhang, and Huchuan Lu. Zoomnext: A unified collaborative pyramid network for camouflaged object detection. _arXiv preprint arXiv:2310.20208_, 2023.
* [139] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _CVPR_, pages 3498-3505, 2012.
* [140] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_, 2023.
* [141] Talmo D Pereira, Diego E Aldarondo, Lindsay Willmore, Mikhail Kislin, Samuel S-H Wang, Mala Murthy, and Joshua W Shaevitz. Fast animal pose estimation using deep neural networks. _Nature methods_, 16(1):117-125, 2019.
* [142] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In _ICCV_, pages 2641-2649, 2015.
* [143] Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Oifan Wang, Rama Chellappa, and Amjad Almahairi. Jack of all tasks, master of many: Designing general-purpose coarse-to-fine vision-language model. _arXiv preprint arXiv:2312.12423_, 2023.
* [144] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _ICML_, pages 8748-8763, 2021.
* [145] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.
* [146] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.
* [147] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvari, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In _CVPR_, pages 7141-7151, 2023.
* [148] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M Anwer, Erix Xing, Ming-Hsuan Yang, and Fahad S Khan. Glamm: Pixel grounding large multimodal model. _arXiv preprint arXiv:2311.03356_, 2023.
* [149] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In _SIGKDD_, pages 3505-3506, 2020.
* [150] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. _arXiv preprint arXiv:2403.05530_, 2024.
* [151] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. _arXiv preprint arXiv:2312.02228_, 2023.
** [152] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10684-10695, 2022.
* [153] Christos Sagonas, Georgios Tzimiropoulos, Stefanos Zafeiriou, and Maja Pantic. 300 faces in-the-wild challenge: The first facial landmark localization challenge. In _ICCVW_, pages 397-403, 2013.
* [154] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: A novel resource for question answering on scholarly articles. _International Journal on Digital Libraries_, 23(3):289-301, 2022.
* [155] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In _ICCV_, pages 8430-8439, 2019.
* [156] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: A benchmark for detecting human in a crowd. _arXiv preprint arXiv:1805.00123_, 2018.
* [157] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a red circle? visual prompt engineering for vlims. In _ICCV_, pages 11987-11997, 2023.
* [158] Mustafa Shukor, Corentin Dancette, and Matthieu Cord. ep-alm: Efficient perceptual augmentation of language models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 22056-22069, 2023.
* [159] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image captioning with reading comprehension. In _ECCV_, pages 742-758, 2020.
* [160] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In _CVPR_, pages 8317-8326, 2019.
* [161] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In _CVPR_, pages 8802-8812, 2021.
* [162] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-linguistic representations. _arXiv preprint arXiv:1908.08530_, 2019.
* [163] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yuee Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. _arXiv preprint arXiv:2312.13286_, 2023.
* [164] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueee Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. In _ICLR_, 2024.
* [165] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In _ICDAR_, pages 1557-1562. IEEE, 2019.
* [166] Didac Suris, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In _ICCV_, pages 11888-11898, 2023.
* [167] Sanli Tang, Fan He, Xiaolin Huang, and Jie Yang. Online pcb defect detector on a new pcb defect dataset. _arXiv preprint arXiv:1902.06197_, 2019.
* [168] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. _arXiv preprint arXiv:2405.09818_, 2024.
* [169] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* [170] InternLM Team. Internlm: A multilingual language model with progressively enhanced capabilities. https://github.com/InternLM/InternLM, 2023.
* [171] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.

* [172] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [173] Hsi-Ai Tsao, Lei Hsiung, Pin-Yu Chen, Sijia Liu, and Tsung-Yi Ho. Autotyp: An automated visual prompting framework and benchmark. _arXiv preprint arXiv:2310.08381_, 2023.
* [174] Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Benchmarking representation learning for natural world image collections. In _CVPR_, pages 12884-12893, 2021.
* [175] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
* [176] Haiyang Wang, Hao Tang, Li Jiang, Shaoshuai Shi, Muhammad Ferjad Naeem, Hongsheng Li, Bernt Schiele, and Liwei Wang. GitHub:towards generalist vision transformer through universal language interface. _arXiv preprint arXiv:2403.09394_, 2024.
* [177] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In _ICCV_, pages 19844-19854, 2023.
* [178] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong. Loveda: A remote sensing land-cover dataset for domain adaptive semantic segmentation. _arXiv preprint arXiv:2110.08733_, 2021.
* [179] Lijun Wang, Huchuan Lu, Yifan Wang, Mengyang Feng, Dong Wang, Baocai Yin, and Xiang Ruan. Learning to detect salient objects with image-level supervision. In _CVPR_, pages 136-145, 2017.
* [180] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. _arXiv preprint arXiv:2402.19474_, 2024.
* [181] Weiyun Wang, Min Shi, Qingyun Li, Wenhai Wang, Zhenhang Huang, Linjie Xing, Zhe Chen, Hao Li, Xizhou Zhu, Zhiguo Cao, et al. The all-seeing project: Towards panoptic visual recognition and understanding of the open world. In _ICLR_, 2024.
* [182] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. _NeurIPS_, 36, 2023.
* [183] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: A generalist painter for in-context visual learning. In _CVPR_, pages 6830-6839, 2023.
* [184] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting everything in context. _arXiv preprint arXiv:2304.03284_, 2023.
* [185] Yizhou Wang, Yixuan Wu, Shixiang Tang, Weizhen He, Xun Guo, Feng Zhu, Lei Bai, Rui Zhao, Jian Wu, Tong He, et al. Hulk: A universal knowledge translator for human-centric tasks. _arXiv preprint arXiv:2312.01697_, 2023.
* [186] Jun Wei, Shuhui Wang, Zhe Wu, Chi Su, Qingming Huang, and Qi Tian. Label decoupling framework for salient object detection. In _CVPR_, pages 13025-13034, 2020.
* [187] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. _arXiv preprint arXiv:2303.04671_, 2023.
* [188] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding. _arXiv preprint arXiv:2212.00280_, 2022.
* [189] Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, and Radu Soricut. Omni-smola: Boosting generalist multimodal models with soft mixture of low-rank experts. _arXiv preprint arXiv:2312.00968_, 2023.
* [190] Junfeng Wu, Yi Jiang, Qihao Liu, Zehuan Yuan, Xiang Bai, and Song Bai. General object foundation model for images and videos at scale. _arXiv preprint arXiv:2312.09158_, 2023.
* [191] Bin Xia, Shiyun Wang, Yingfan Tao, Yitong Wang, and Jiaya Jia. Llmga: Multimodal large language model based generation assistant. _arXiv preprint arXiv:2311.16500_, 2023.
** [192] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial images. In _CVPR_, pages 3974-3983, 2018.
* [193] Jiarui Xu, Xingyi Zhou, Shen Yan, Xiuye Gu, Anurag Arnab, Chen Sun, Xiaolong Wang, and Cordelia Schmid. Pixel aligned language models. _arXiv preprint arXiv:2312.09237_, 2023.
* [194] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose++: Vision transformer for generic body pose estimation. _TPAMI_, 46:1212-1230, 2022.
* [195] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance perception as object discovery and retrieval. In _CVPR_, pages 15325-15336, 2023.
* [196] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. _arXiv preprint arXiv:2310.11441_, 2023.
* [197] Jie Yang, Ailing Zeng, Siyi Liu, Feng Li, Ruimao Zhang, and Lei Zhang. Explicit box detection unifies end-to-end multi-person pose estimation. _ArXiv_, abs/2302.01593, 2023.
* [198] Jie Yang, Ailing Zeng, Ruimao Zhang, and Lei Zhang. Unipose: Detecting any keypoints. _arXiv preprint arXiv:2310.08530_, 2023.
* [199] Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long Lan, and Dacheng Tao. Apt-36k: A large-scale benchmark for animal pose estimation and tracking. _NeurIPS_, 35:17301-17313, 2022.
* [200] Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. _arXiv preprint arXiv:2311.11969_, 2023.
* [201] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. _arXiv preprint arXiv:2310.07704_, 2023.
* [202] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge enhanced vision-language representations through scene graphs. In _AAAI_, volume 35, pages 3208-3216, 2021.
* [203] Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng Tao. Ap-10k: A benchmark for animal pose estimation in the wild. _arXiv preprint arXiv:2108.12617_, 2021.
* [204] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In _ECCV_, pages 69-85, 2016.
* [205] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models: Pretraining and instruction tuning. _arXiv preprint arXiv:2309.02591_, 2(3), 2023.
* [206] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. _Advances in Neural Information Processing Systems_, 33:5824-5836, 2020.
* [207] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. _arXiv preprint arXiv:2312.10032_, 2023.
* [208] Yi Ke Yun and Weisi Lin. Selfreformer: Self-refined network with transformer for salient object detection. _arXiv preprint arXiv:2205.11283_, 2022.
* [209] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In _CVPR_, pages 6720-6731, 2019.
* [210] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. _arXiv preprint arXiv:2402.12226_, 2024.
* [211] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. A simple framework for open-vocabulary segmentation and detection. In _ICCV_, pages 1020-1031, 2023.
* [212] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. _arXiv preprint arXiv:2404.07973_, 2024.

* [213] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest. _arXiv preprint arXiv:2307.03601_, 2023.
* [214] Shilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping Luo, and Kai Chen. Dense distinct query for end-to-end object detection. In _CVPR_, pages 7329-7338, 2023.
* [215] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* [216] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. _arXiv preprint arXiv:2306.17107_, 2023.
* [217] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic segmentation. _arXiv preprint arXiv:2402.16846_, 2024.
* [218] Zheng Zhang, Yeyao Ma, Enming Zhang, and Xiang Bai. Psalm: Pixelwise segmentation with large multi-modal model. _arXiv preprint arXiv:2403.14598_, 2024.
* [219] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-jadge with mt-bench and chatbot arena. _NeurIPS_, 36, 2024.
* [220] Yunfei Zheng, Xiongwei Zhang, Feng Wang, Tieyong Cao, Meng Sun, and Xiaobing Wang. Detection of people with camouflage pattern via dense deconvolution network. _IEEE Signal Processing Letters_, 26(1):29-33, 2018.
* [221] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Lianian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image pretraining. In _CVPR_, pages 16793-16803, 2022.
* [222] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _CVPR_, pages 633-641, 2017.
* [223] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. In _ICLR_, 2024.
* [224] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. _arXiv preprint arXiv:2206.04674_, 2022.
* [225] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-perceiver-moe: Learning sparse generalist models with conditional moes. _Advances in Neural Information Processing Systems_, 35:2664-2678, 2022.
* [226] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In _ICLR_, 2020.
* [227] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In _CVPR_, pages 16804-16815, 2022.
* [228] Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, and Jian Tang. Llava-phi: Efficient multi-modal assistant with small language model. _ArXiv_, abs/2401.02330, 2024.
* [229] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In _CVPR_, pages 15116-15127, 2023.
* [230] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. _NeurIPS_, 36, 2024.

[MISSING_PAGE_FAIL:23]

### Evaluation on Various Domains.

**Salient Object Detection.** We compare the results of VisionLLM v2 with state-of-the-art methods for salient object detection (SOD) in Table A6. Our model clearly achieves the highest performance on 4 of the 5 classical benchmarks, demonstrating its strong object discovery capabilities.

**Camouflaged Object Detection.** The performance comparisons for camouflaged object detection (COD) are presented in Table A7. It is observed that VisionLLM v2 exhibits competitive performance with state-of-the-art expert models that undergo longer training schedule, _e.g.,_ 150 epochs.

\begin{table}
\begin{tabular}{l|c|c c c|c c c c|c} method & backbone & iters & mIoU & \multicolumn{2}{c|}{Point} & \multicolumn{2}{c}{Scribble} & \multicolumn{2}{c}{Box} & \multicolumn{2}{c}{Mask} \\  & mIoU & clIoU & mIoU & clIoU & mIoU & clIoU & mIoU & clIoU \\ \hline \hline SAM-B [82] & 48.7 & 33.6 & - & - & 73.7 & 68.7 & - & - \\ SAM-L [82] & 51.8 & 37.7 & - & - & 76.6 & 71.6 & - \\ SEM-B [230] & 47.8 & 57.8 & 43.0 & 44.0 & 49.4 & 21.1 & 48.4 & 65.0 \\ PSALM [218] & 64.3 & 74.0 & 66.9 & 80.0 & 67.3 & 80.9 & 67.6 & 82.4 \\ \hline VisionLLM v2 & 49.1 & 60.7 & 54.7 & 72.3 & 59.1 & 78.2 & 59.6 & 81.0 \\ VisionLLM v2 * & 65.4 & 70.9 & 66.8 & 77.2 & 74.2 & 83.2 & 67.9 & 83.8 \\ \end{tabular}
\end{table}
Table A5: **Comparison of interactive segmentation performance.** The task is evaluated on the COCO-interactive dataset proposed by [218]. *The model is finetuned on the task.

**Visualization across various domains.** Besides the quantitative results, we also display the visualization results of VisionLLM v2 across various domains. As illustrated in Figure A1, our model also shows strong perception capacities for remote sensing, PCB, and medical images.

### Zero-shot Evaluation and In-Context Evaluation

**Zero-shot Image Captioning.** Benefiting from the joint training on large-scale vision-language datasets, VisionLLM v2 exhibits promising capacities for zero-shot image captioning. As shown in Table A1a, both VisionLLM v2-Chat and VisionLLM v2 achieve competitive performance on Flickr30K [142] and NoCaps [6] compared with previous methods.

**Zero-shot Object Detection on OdinW13.** We conduct the zero-shot object detection evaluation on OdinW13 dataset [98], as shown in Table A8. The results demonstrate that our VisionLLM v2 with a Swin-Tiny backbone is even on par with GLEE-Plus [190] with a Swin-Large backbone in APavg. This indicates that our model benefits from the extensive dataset joint training, thereby providing robust general object detection capabilities.

**In-Context Segmentation & In-Context Image Captioning.** To evaluate the in-context learning ability of VisionLLM v2, we compare the results of in-context segmentation and in-context image captioning in Table A9. For in-context segmentation, we construct a benchmark based on the validation set of COCO2017, where the number of in-context examples used during inference ranges from 1 to 5. For in-context image-captioning, we follow the same evaluation protocol as OpenFlamingo [11] and use 4-shot to assess the performance between different methods. The validation set is built upon COCO2017. From the table, VisionLLM v2 exhibits clear performance advantages compared with state-of-the-art methods in both in-context learning settings, which demonstrates the superior in-context capacities of our method.

### More Ablation Studies

**Super-Link Queries v.s. Token Embeddings in LISA [89].** Current MLLMs [151, 89, 148] introduce a segmentation token [SEG] into the LLM vocabulary and directly use its corresponding token embedding as a condition for SAM [82] to achieve pixel-level segmentation, which we refer to as the token embedding method. We also ablate this method for linking the LLM with task-specific decoders, as shown in Table A10. The performance difference between the two methods is negligible for tasks using text prompts, such as instance segmentation. We hypothesize that this is because the category names are seen during training, allowing the token embeddings to

[MISSING_PAGE_FAIL:26]

Figure A2: **Object detection and instance segmentation.** The model excels in various environments, supporting the detection of a large number of instances. Its flexibility is highlighted by its ability to detect only user-selected categories and identify novel classes.

Figure A4: **Visual grounding.** On the visual grounding task, our model demonstrates good accuracy and a certain level of reasoning capability.

Figure A6: **Grounded caption.** The model accurately locates objects based on user prompts, outputs bounding boxes, and provides answers to user queries.

Figure A7: **VisionLLM v2 text-to-image generation examples.** VisionLLM v2 could generate high-quality images that not only properly follow the concepts and relations, but also different styles specified in the instructions..

Figure A8: **VisionLLM v2 instructed-based image editing examples.** VisionLLM v2 can understand a variety of instructions such as style transfer, object replacement, object addition, attribute change, and more to generate high-quality edited images.

**Multimodal In-context Learning Ability.** To qualitatively verify the in-context capabilities of our model after trained on MMIC, we provide comprehensive visualizations across different tasks. As demonstrated in Figure A9, A10, A11 and A12, our method can handle both visual and textual prompts, enabling it to perform tasks that require understanding and integration of information from different modalities. In addition, our models can distinguish between different prompting strategies and can correctly use the corresponding detection or segmentation tools to obtain the expected output based on given in-context examples.

## Appendix B More Architecture Details

### Region Encoder

The region encoder is designed to encode various shaped visual prompts such as points, scribbles, boxes, _etc_. Each visual prompt is represented by a binary mask. We first concatenate the binary mask with the image along the channel dimension, resulting in a 4-channel input, denoted as \(I_{\text{vprt}}\in\mathbb{R}^{4\times H\times W}\). The region encoder is implemented with three convolutional layers: the first layer uses a kernel size of 7 and a stride of 7, the second layer employs a kernel size of 2, and a stride of 2, and the final layer features a kernel size of 1 and a stride of 1. Each convolutional layer is followed by layer normalization [12] and GELU activation [65]. This process downsamples the input \(I_{\text{vprt}}\in\mathbb{R}^{4\times H\times W}\) by a factor of 14. We further augment this feature map by adding the feature map of the global image \(I_{\text{global}}\). Finally, we use grid sampling to extract features within the masked regions and pool them into a single region embedding \(F_{\text{vprt}}\in\mathbb{R}^{1\times C}\).

Figure A11: **In-context detection and segmentation. We just need to provide some examples where the instances falling into the same class are highlighted. Then our model can learn from the example and use the too of detection or segmentation to process the input image.**

Figure A12: **In-context regional perception. In our dataset, we construct various visual masks in input prompts. Our models are required to infer from the given examples and complete the text for the last image.**

### Task-specific Decoders

In this subsection, we provide more explanations about how to connect LLM with task-specific decoders via super-link queries, which enables the end-to-end optimization of the entire network.

**Connecting with Object Decoder.** For visual perception tasks like object detection, we employ Grounding DINO [112] as the object decoder to localize objects as well as classify their categories. To achieve this, LLM would output each category name in the response, followed by a special token [DET] and super-link queries. We then obtain the per-category features by extracting the hidden states of LLM for corresponding super-link queries and pooling them into one embedding. Grounding DINO receives both the image and the obtained per-category features as inputs and predicts the detection results. The process is illustrated in Figure A13a. It is noted that we discard the text encoder in the original Ground DINO and use the obtained per-category features as text features to perform the vision-language alignment for classification. During training, the total loss includes the cross-entropy loss of LLM and detection loss of the object decoder. Similarly, the keypoint decoder is also integrated into the LLM in the same way and performs pose estimation.

**Connecting with Image Decoder.** We utilize Stable Diffusion [152] as the image decoder and take the example of text-to-image generation for clarification, as depicted in Figure A13b. The super-link queries are appended after the special token [GEN] in the LLM's response. After passing through the LLM, an MLP layer and a lightweight Q-Former [97, 83] module are added to project the features of the super-link queries into the representation space of Stable Diffusion, i.e., mapping features. We bypass the text encoder in Stable Diffusion and directly use the mapping features as the text embedding condition. During training, in addition to the next token loss in the LLM, we employ two MSE losses for supervision: one between the encoded text features by CLIP [144] and the mapping features, and the other between the ground-truth images/noise and predicted images/noise.

## Appendix C More Dataset Details

To support the training for enhancing our model with various capacities, we meticulously collect and re-organize the datasets from a broad range of tasks. These data are publicly available, and we comprehensively list all the data we used in Table A11. In addition to the commonly used dataset for the standard vision and vision-language tasks, we find that many works explore visual prompting strategies and in-context learning. However, there is still a lack of public datasets focusing on addressing these tasks currently. To this end, we organize a series of datasets into a new one coined as a multimodal in-context (**MMIC**) dataset to facilitate the model with in-context learning abilities, applicable to both visual and textual prompts. As shown in Table A12, built upon several datasets, we support lots of visual prompting and in-context tasks for fine-grained visual recognition,including categories such as cats, dogs, fruits, vegetables, food, cars, birds, etc. Additionally, we also make efforts on in-context object detection, in-context object segmentation, in-context captioning, in-context OCR, and in-context VAQ.

### MMIC Dataset Construction

For tasks that require visual or textual in-context examples, we randomly select \(N\) samples, where \(N\in[2,6]\), without replacement from the dataset. The first \(N-1\) samples are presented as in-context examples of the model. These examples serve to provide a reference or a guide for the type of output expected. The model is then tasked with solving or completing the task based on the last sample in the sequence. This paradigm allows the model to learn from examples and apply that knowledge to new, unseen data.

Inspired by [157], visual marks can also serve as the input for multimodal LLMs. As a result, we design five types of visual marks: circle, hand-drawn circle, arrow, box, and mask. Each visual mark can be either solid or hollow. We primarily construct this dataset based on COCO [104], AP10K [203] and some OCR datasets [165, 161], where we randomly sample \(M\)(\(\in\)[1, 5]) instances per image. The same type of visual mark is used to highlight the selected instance within one image, ensuring consistency and clarity for the model's learning process.

The examples of constructed instructions can refer to Figure A9, A10, A11 and A12. The entire dataset has constructed a multimodal corpus with \(\sim\)862K question&answer pairs. We expect that this dataset can further advance the development of this field.

## Appendix D Training Details

Figure A14 depicts the three-stage training process. Table A13 lists the detailed training configurations of VisionLLM v2 in different training stages. In each stage, the model inherits the weights from the previous stage and continues training. The image encoder keeps frozen in all stages following previous works [107, 105].

**Settings of Stage-1.** Stage-1 consists of pretraining and instruction tuning phases as [107, 105]. As shown in Table A13, in the pretraining phase, We freeze the LLM. And only the region encoder and projections for image embedding and region embedding are trained for efficiency. We adopt the AdamW optimizer [119] with the peak learning rate of 1e-3 and weight decay of 0. The training involves a total batch size of 2048 across 64 A100 GPUs. In the instruction tuning phase, LLM is unfrozen for full-parameter training. The peak learning rate is decreased to 2.5e-5 for training stabilization. The model is trained on 64 A100 GPUs with a total batch size of 1024. And we begin adopting the dynamic resolution approach [106, 33] in this phase. The maximal number of local patches, _i.e._, max tile, is set as 4.

**Settings of Stage-2.** In stage-2, we add the task-specific decoders and perform the multi-capacity fine-tuning. In this stage, we only finetune the input and output embeddings of LLM to save computational memory and preserve the conversational ability. LLM and region encoder are trained with the peak learning rate of 1e-5, while the decoders are trained with the peak learning rate of 1e-4. The model is trained on 128 A100 GPUs with a batch size of 2 per GPU.

**Settings of Stage-3.** In stage-3, we freeze all the components except for the task-specific decoders to maintain the conversational ability. The model undergoes 12 training epochs on 128 A100 GPUs with a peak learning rate of 1e-4 and a total batch size of 256.

These three stages take around 5 / 3 / 10 days to finish the training, respectively.

**Training Losses.** During training, we use the standard cross-entropy loss in stage-1. In stage-2 and stage-3, when integrating the task-specific decoders, we simply sum the losses from the LLM and decoders directly, without reweighting each component. _i.e._,

\[L_{\text{total}}=L_{\text{llm}}+L_{\text{g\'{i}ino}}+L_{\text{unipose}}+L_{ \text{sd}}+L_{\text{ip2p}}\] (1)

## Appendix E Instruction Templates

To support the proper invocation of task-specific decoders, we construct a series of instruction templates for different tasks using ChatGPT [4] and use them as instruction tuning data for LLM. We comprehensively list all the instruction templates below, from Table A14 to Table A24.

[MISSING_PAGE_FAIL:38]

[MISSING_PAGE_FAIL:39]

[MISSING_PAGE_FAIL:40]

\begin{table}
\begin{tabular}{|l|} \hline
1. Where can we locate the \textless{}expression\textgreater{} in the image? \\
2. Do you know where the \textless{}expression\textgreater{} is within the image? \\
3. Have you seen the \textless{}expression\textgreater{} in this image? Where is it? \\
4. Could you tell me where the \textless{}expression\textgreater{} is in the image? \\
5. Whereabouts in the image can we find the \textless{}expression\textgreater{}? \\
6. Do you have any idea where the \textless{}expression\textgreater{} might be in this image? \\
7. Are you aware of the \textless{}expression\textgreater{}’s position within the image? \\
8. Where in the image should we be looking for the \textless{}expression\textgreater{}? \\
9. Is it possible to identify the \textless{}expression\textgreater{}’s location in this image? \\
10. Have you figured out where the \textless{}expression\textgreater{} is in this image? \\
11. Could you provide guidance on finding the \textless{}expression\textgreater{} in the image? \\
12. Do you know where I can locate the \textless{}expression\textgreater{} in the picture? \\
13. Can you tell me the precise location of the \textless{}expression\textgreater{} in the image? \\
14. Would you be able to point out the \textless{}expression\textgreater{} within the image? \\
15. Are you able to discern the \textless{}expression\textgreater{} in the image? \\
16. Please help me locate the \textless{}expression\textgreater{} in the image. \\
17. Please find the object indicated by the expression \textless{}expression\textgreater{} in the image. \\
18. Please assist in identifying the \textless{}expression\textgreater{} within the image. \\
19. Please determine the exact position of the \textless{}expression\textgreater{} in the image. \\
20. Please ascertain the whereabouts of the \textless{}expression\textgreater{} in this image. \\
21. Please assist me in locating the \textless{}expression\textgreater{} within the image. \\
22. Please take a moment to find the object denoted by the expression \textless{}expression\textgreater{} in the image. \\
23. Please help us identify the precise location of the \textless{}expression\textgreater{} in this image. \\
24. Please provide your guidance in finding and marking the \textless{}expression\textgreater{} within the image. \\
25. Please make it a priority to discover and highlight the \textless{}expression\textgreater{} within the image. \\
26. Let’s determine the specific area where the \textless{}expression\textgreater{} is situated in the image. \\
27. We’re aiming to establish the spatial coordinates of the \textless{}expression\textgreater{} in this image. \\
28. We need to establish the exact whereabouts of the \textless{}expression\textgreater{} within the image. \\
29. We are actively engaged in the process of locating the \textless{}expression\textgreater{} in the image. \\
30. Let’s find the \textless{}expression\textgreater{} within the image. \\ \hline \end{tabular}
\end{table}
Table A19: **A list of instructions for visual grounding.**1. Could you aid me in generating unique masks for every category present in <class> in this image?
2. Can you help me generate distinct masks for each category that belongs to <class> in this image?
3. Is it possible for you to help me create distinct masks for the different <class> categories in this image?
4. Could you assist me in generating masks that correspond to each individual <class> category in this image?
5. Would you mind helping me generate separate masks for each <class> category detected in this image?
6. Can you guide me in generating unique masks for all the categories falling under <class> in this image?
7. Can you provide me with the necessary support to generate masks specific to each <class> category in this image?
8. Could you please guide me in creating separate masks for each <class> category detected in this image?
9. Can you support me in generating masks for all the categories encompassed by <class> in this image?
10. Examine the image and generate masks that correspond to each individual <class> category present.
11. Is it possible for you to help me generate separate masks for each detected category falling under <class> in this image?
12. Can you assist me in generating masks that isolate each category belonging to <class> in this image?
13. Can you provide me with assistance in generating individual masks for every <class> category identified in this image?
14. Can you help with the process of generating masks that are specific to each <class> category detected in this image?
15. Generate masks that accurately depict each category belonging to <class> in this image.
16. I require assistance in producing separate masks for all the <class> categories in this image.
17. I need your support to generate masks that are specific to each <class> category in this image.
18. Your task is to produce masks that differentiate each category falling under the <class> category in this image.
19. Please create masks that are distinct for each category belonging to <class> in this image.
20. I'm seeking your help to generate masks that isolate every category within the <class> category in this image.
21. Please segment the different categories falling under <class> in this image and generating masks for each.
22. Please accurately segment and generate masks for all the categories falling under <class> in this image.
23. I need your support to create masks that are specific to each <class> category identified in this image.
24. I'm requesting your aid in generating masks that distinguish each category belonging to <class> in this image.
25. Please lend me your expertise in creating masks that are unique for each detected <class> category in this image.
26. Your help is required to generate distinct masks for each category of <class> in this image.
27. It would be appreciated if you could assist in creating separate masks for each <class> category in this image.
28. Let's collaborate on segmenting all categories falling under the <class> category in this image and generating masks.
29. Assisting me in generating distinct masks for each class categorized as <class> would be greatly appreciated.
30. Providing assistance in generating masks that accurately identify the categories falling under <class> in this image would be greatly helpful.

Table A20: **A list of instructions for semantic segmentation.**

[MISSING_PAGE_FAIL:43]

1. Generate image with caption: <caption>.
2. Can you give me the image with caption: <caption>.
3. Help me to generate this image: <caption>.
4. Generate the image according to the caption: <caption>.
5. According to the caption, generate the image: <caption>.
6. An image with caption: <caption>.
7. Can you visualize this caption: <caption>.
8. Create an image based on this caption: <caption>.
9. Generate a visual representation for this caption: <caption>.
10. Provide me with an image corresponding to this caption: <caption>.
11. Craft an image with the following caption: <caption>.
12. Generate an image accompanied by this caption: <caption>.
13. Turn this caption into an image: <caption>.
14. Generate an image reflecting this caption: <caption>.
15. Translate this caption into a visual representation: <caption>.
16. Produce an image that matches this caption: <caption>.
17. Create an image in line with this caption: <caption>.
18. Generate an image to illustrate this caption: <caption>.
19. Construct an image based on the given caption: <caption>.
20. Give me an image associated with this caption: <caption>.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims in the abstract and introduction accurately describe our model's contributions in the multimodal large language model domain. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]Justification: See the conclusion section for details.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: This paper provides detailed descriptions of the experimental setup, training steps, and the datasets used. We will also release the code later. Guidelines:* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: At the time of submission, we will not provide the code and dataset. We plan to open-source the code and dataset in the future. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.

* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The paper does not report error bars of the results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix D. Guidelines: * The answer NA means that the paper does not include experiments.

* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the research conducted in the paper conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See the conclusion section for details. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See the dataset details in Appendix A. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [No] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.