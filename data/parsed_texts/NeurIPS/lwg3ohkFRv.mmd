# CARE: Modeling Interacting Dynamics Under Temporal Environmental Variation

 Xiao Luo\({}^{1}\), Haixin Wang\({}^{2}\), Zijie Huang\({}^{1}\), Huiyu Jiang\({}^{3}\),

**Abhijeet Sadashiv Gangan\({}^{1}\), Song Jiang\({}^{1}\), Yizhou Sun\({}^{1}\)**

\({}^{1}\)University of California, Los Angeles, \({}^{2}\)Peking University,

\({}^{3}\)University of California, Santa Barbara

{xiaoluo,yzsun}@cs.ucla.edu, wang.hx@stu.pku.edu.cn

###### Abstract

Modeling interacting dynamical systems, such as fluid dynamics and intermolecular interactions, is a fundamental research problem for understanding and simulating complex real-world systems. Many of these systems can be naturally represented by dynamic graphs, and graph neural network-based approaches have been proposed and shown promising performance. However, most of these approaches assume the underlying dynamics does not change over time, which is unfortunately untrue. For example, a molecular dynamics can be affected by the environment temperature over the time. In this paper, we take an attempt to provide a probabilistic view for _time-varying_ dynamics and propose a model Context-attended Graph ODE (CARE) for modeling time-varying interacting dynamical systems. In our CARE, we explicitly use a context variable to model time-varying environment and construct an encoder to initialize the context variable from historical trajectories. Furthermore, we employ a neural ODE model to depict the dynamic evolution of the context variable inferred from system states. This context variable is incorporated into a coupled ODE to simultaneously drive the evolution of systems. Comprehensive experiments on four datasets demonstrate the effectiveness of our proposed CARE compared with several state-of-the-art approaches.

## 1 Introduction

Modeling interacting dynamical systems is a fundamental machine learning problem [66, 65, 23, 59, 37] with a wide range of applications, including social network analysis [12, 17, 36] and particle-based physical simulations [46, 43, 35]. Geometric graphs [27] are utilized to formalize these interactions between objects. For example, in particle-based physical systems, edges are constructed based on the geographical distance between particles, which represents the transfer of energy.

In the literature, numerous data-driven approaches have been proposed for understanding interacting dynamical systems [2, 25, 49]. Among them, graph neural networks [26, 62, 72, 33, 13] (GNNs) are widely utilized to predict trajectories at the next timestamp due to their strong capacity to capture interactions in graph-structured data. In particular, each object is considered as a graph node, and edges represent interactions between neighboring objects. Given the observations and their corresponding graph structure, these methods forecast states in the next timestamp using the message passing mechanism. This process involves aggregating information from the neighbors of each node to update its representation in an iterative fashion, effectively capturing the dynamics of the system.

Although impressive progress has been achieved on GNN-based approaches, capturing long-term dependency in interacting dynamical systems is still a practical but underexplored problem. Existing next-step predictors [42, 46, 45] can send the predictions back to generate rollout trajectories, which could suffer from serious error accumulation for long-term predictions. More importantly, systemenvironments and relational structures could be changeable [69] (e.g., unsteady flow [11; 16]), which implies the potential temporal distribution variation during the evolution. In particular, in physical systems, there are various potential factors which can influence the trajectories extensively. For example, high temperatures [69] or pressure [68] could speed up molecular movement. Their continuous variation would make understanding interacting dynamic systems more challenging. First, temporal environmental variation would indicate different data distributions over the time [52], which requires the model equipped with superior generalization capability. In contrast, existing methods typically focus on in-distribution trajectories [7; 1; 58; 3; 21; 66], which would perform worse when it comes to out-of-distribution data. Second, recent out-of-distribution generalization methods [50; 39; 57; 64; 44] usually focus on vision and text with discrete shift across different domains. However, our scenarios would face the continuous distribution variation, which is difficult to capture in interacting dynamical systems.

In this paper, we propose a novel method named Context-attended Graph ODE (CARE) to capture interacting system dynamics. The core of our CARE approach is to characterize the temporal environmental variation by introducing the context variable. In particular, we propose a probability model to depict the interaction between the context variable and trajectories. Based on the probabilistic decomposition, we divide each training sequence into two parts for initializing embeddings and predictions. Here, we first construct a temporal graph and then leverage an attention-based encoder to generate node representations and the context representation from spatial and temporal signals simultaneously. More importantly, we introduce coupled ODEs to model the dynamic evolution of node representations and the context variable. On the one hand, we adopt a graph-based ODE system enhanced with context information to drive the evolution. On the other hand, the context information can also be updated using summarized system states and the current context. We also provide a theoretical analysis that indicates, at least locally, the future system trajectory and context information are predictable based on their historical values. Finally, we introduce efficient dynamical graph updating and robust learning strategies to enhance the generalization capability and efficiency of the framework, respectively. Extensive experiments on various dynamical systems demonstrate the superiority of our proposed CARE compared with state-of-the-art approaches.

To summarize, in this paper we make the following contributions: (1) _Problem Formalization_. We formalize the problem of temporal environmental variation in interacting dynamics modeling. (2) _Novel Methodology_. We analyze the problem under a probabilistic framework, and propose a novel approach CARE, which incorporates the continuous context variations and system states into a coupled ODE system. (3) _Extensive Experiments_. Extensive experiments conducted on four datasets validate the superiority of our CARE. The performance gain of our proposed CARE over the best baseline is up to 36.35%.

## 2 Related Work

**Interacting Dynamics Modeling.** Deep learning approaches have been extensively used in recent years to model interacting systems across various fields [9; 53; 29; 20], including molecular dynamics and computational fluid dynamics. Early efforts focus on incorporating convolutional neural networks to learn from interacting regular grids [41]. To address more generalized scenarios, graph neural network (GNN) methods have been developed [42; 46; 45], leveraging message mechanisms to extract complex spatial signals. However, these methods often fail to account for environmental fluctuations, which hinders their ability to make reliable long-term predictions. In contrast, our CARE adopts a context-attended ODE architecture to explicitly represent both the observations and the underlying environment, enabling the generation of accurate future trajectories.

**Neural Ordinary Differential Equations (ODEs).** Drawing inspiration from the approximation of ResNet [5], neural ODEs equip neural networks with a continuous-depth architecture by parameterizing the derivatives of hidden states. Several attempts have been made to increase the expressiveness of neural ODEs [65], including adding regularization [8] and designing high-order ODEs [61]. Neural ODEs have also been incorporated into GNNs, producing continuous message passing layers to avoid oversmoothing [60] and increase the interpretability of predictions [70]. In this study, we employ a graph ODE architecture to capture the continuous nature of interacting system dynamics, relieving potential error accumulation caused by discrete prediction models.

**Out-of-distribution (OOD) Generalization.** OOD generation aims to make models more effective when the training and testing distributions diverge [50; 39; 57; 63]. This problem has drawn considerable attention in several areas, including text and vision [48]. One effective solution is to learn domain-invariant representations in the hidden space [31; 55; 30], which has been achieved under the guidance of invariant learning theory [6; 32]. Additionally, uncertainty modeling [34], causal learning [54; 14], and model selection [38; 56] are employed to improve the performance. Interacting systems inherently exhibit dynamic distribution variation caused by environmental changes, an aspect that remains underexplored in the literature. To address this, our paper proposes a novel approach named CARE to model context information from the perspective of a probabilistic model.

## 3 Background

### Problem Definition

In a multi-agent dynamical system, the state at time \(t\) is represented by \(G^{t}=(V,E^{t},\bm{X}^{t})\), where each node in \(V\) corresponds to an object, \(E^{t}\) denotes the current edge set, and \(\bm{X}^{t}\) signifies the node attribute matrix. Specifically, the state vector for each \(i\in V\) is given by \(\bm{x}_{i}^{t}=[\bm{p}_{i}^{t},\bm{q}_{i}^{t},\bm{a}_{i}]\), with \(\bm{p}_{i}^{t}\in\mathbb{R}^{3}\) and \(\bm{q}_{i}^{t}\in\mathbb{R}^{3}\) representing the position and velocity, respectively, and \(\bm{a}_{i}\) representing static attributes. We are given the sequence \(\{G^{0},G^{1},\cdots,G^{t}\}\) and aim to learn a model that produces the target dynamic states \(\bm{Y}^{s}(s>t)\) (e.g., velocity), which are part of \(\bm{X}^{s}\) at the corresponding time. The temporal environmental variation would result in data distribution changes during the evolution of interacting systems. If we utilize \(C^{0:t}\) to indicate the dynamical environment factor till timestamp \(t\), we have data from variable distribution, i.e., \((G^{0:t},\bm{Y}^{s})\sim P(G^{0:t},\bm{Y}^{s}|C^{0:t})\). Thus, we must take these changes into account for accurate trajectory predictions.

### GNNs for Modeling Dynamical Systems

Graph neural networks (GNNs) are extensively employed in dynamical system modeling to investigate the interactive relationships between objects [42; 46; 45]. These methods typically use the current states to predict the states of nodes at the next timestamp. Specifically, omitting the time notation, GNNs first initialize node representations and edge representations using encoders:

\[\bm{v}_{i}^{(0)}=f^{v}\left(\bm{x}_{i}\right),\quad\bm{e}_{ij}^{(0)}=f^{e} \left(\bm{x}_{i},\bm{x}_{j}\right),\] (1)

where \(f^{v}(\cdot)\) and \(f^{e}(\cdot)\) are two encoders for node and edge representations, respectively. Then, they utilize two propagation modules to update these representations at the \(l\)-th layer, i.e., \(\bm{v}_{i}^{(l)}\) and \(\bm{e}_{ij}^{(l)}\) following the message passing mechanism:

\[\bm{e}_{ij}^{(l+1)}=\phi^{e}\left(\bm{v}_{i}^{(l)},\bm{v}_{j}^{(l)},\bm{e}_{ ij}^{(l)}\right),\quad\bm{v}_{i}^{(l+1)}=\phi^{v}\left(\bm{v}_{i}^{(l)},\sum_{j \in\mathcal{N}_{i}}\bm{e}_{ij}^{(l+1)}\right),\] (2)

Figure 1: An overview of the proposed CARE. To begin, we construct a temporal graph and utilize an encoder to initialize both the context variable and node representations from historical trajectories. Then, a coupled ODE model simulates the evolution of both nodes and context. Finally, CARE feeds node representations into decoders, which output the predicted trajectories at any timestamp.

where \(\mathcal{N}_{i}\) collects the neighbours of node \(i\). \(\phi^{e}(\cdot)\) and \(\phi^{v}(\cdot)\) are two functions for representation updating. Finally, they generate the target velocity vectors at the next timestamp using a decoder.

## 4 Methodology

In this paper, we propose a novel method called CARE for modeling interacting dynamics under temporal environmental variation. We start by formalizing a probabilistic model to understand the relationships between trajectories and contexts. Based on this foundation, we construct a spatio-temporal encoder to initialize the representations of nodes and contexts. Then, to simultaneously model their evolution, a coupled graph ODE is introduced where node representations are evolved with the guidance of contexts and the states of the context variable are inferred from current trajectories. Additionally, we introduce a regularization term and dynamic graph updating strategies to enhance our framework. An illustration of our CARE can be found in Figure 1.

### Probabilistic Model for System Dynamics under Temporal Distribution Drift

In this work, to tackle the challenge brought by temporal distribution drift, we inject a context variable \(\bm{c}^{t}\) in our dynamic system modeling, which indicates the environment state at timestamp \(t\). For example, the context variable could indicate flow speed, density and viscosity in fluid dynamics.

Here, we make two basic assumptions in our probabilistic model.

**Assumption 4.1**.: _(Independence-I) The context variable is independent of the sequences before the last observed timestamp, i.e., \(P(\bm{c}^{t}|\bm{c}^{t-k},G^{0:t})=P(\bm{c}^{t}|\bm{c}^{t-k},G^{t-k:t})\), where \(t-k\) is the last observed timestamp._

**Assumption 4.2**.: _(Independence-II) Given the current states and contexts, the future trajectories are independent of the previous trajectories and contexts, i.e., \(P(\bm{Y}^{t-k:t+l}|G^{0:t-k},\bm{c}^{0:t-k})=P(\bm{Y}^{t-k:t-k+l}|G^{t-k},\bm{c }^{t-k})\) where \(l\) is the length of the prediction._

Then, we can have the following lemma:

**Lemma 4.1**.: _With Assumptions 4.1 and 4.2, we have:_

\[\begin{split}&\mathrm{P}\left(\bm{Y}^{t}\mid G^{0:t-1}\right)= \int\mathrm{P}\left(\bm{Y}^{t}\mid\bm{c}^{t-1},G^{t-1}\right)\cdot\\ &\mathrm{P}\left(\bm{c}^{t-1}\mid\bm{c}^{t-k},G^{t-k:t-1}\right) \cdot\mathrm{P}\left(\bm{c}^{t-k}\mid G^{0:t-k}\right)d\bm{c}^{t-1}d\bm{c}^{t -k}.\end{split}\] (3)

The proof of Lemma 4.2 can be found in Appendix. From Lemma 4.1, we decompose the probability \(\mathrm{P}\left(\bm{Y}^{t}\mid G^{0:t-1}\right)\) into three terms. Specifically, the last term necessitates encoding context information based on the historical trajectory \(G^{0:t-k}\). The second term aims to update the context vector according to the recent trajectory \(G^{t-k:t-1}\). The first term suggests using the context variable in conjunction with the current states to make predictions for the next timestamp. Besides making a single next-time prediction, our model can also predict trajectories (\(\bm{Y}^{t-k},\bm{Y}^{t-k+1},\cdots,\bm{Y}^{t}\)) by modifying Eqn. 14.

Consequently, we divide each training sequence into two parts, namely \([0,t-k]\) and \((t-k,t]\) as in [19; 18]. The first part is used to encode contexts and nodes in the system, while the second part serves for updating and prediction purposes.

### Context Acquirement from Spatio-temporal Signals

In this part, our goal is to acquire knowledge from the historical trajectory, i.e., \(\{G^{0},\cdots,G^{t-k}\}\) to encode contexts and nodes for initialization. To be specific, we first construct a temporal graph to capture spatial and temporal signals simultaneously. Subsequently, we employ the attention mechanism to update temporal node representations, which will be used to initialize the context representation and node representations for \(\{G^{t-k},\cdots,G^{T}\}\).

To begin, we construct a temporal graph containing two types of edges, i.e., spatial and temporal edges. Spatial edges are built when the distance between two nodes at the same timestamp is less than a threshold while temporal edges are between every two consecutive observations for each node. Specifically, in the constructed temporal graph \(G^{H}\), there are \(N(t-k+1)\) nodesin total. The adjacent matrix \(\bm{A}\) contains both spatial and temporal edges as follows:

\[\bm{A}(i^{s},j^{s^{\prime}})=\left\{\begin{array}{ll}exp(-d^{s}(i,j))&s=s^{ \prime},exp(-d^{s}(i,j))<\tau,\\ 1&i=j,s^{\prime}=s+1,\\ 0&\text{otherwise},\end{array}\right.\] (4)

where \(d^{s}(i,j)\) denotes the distance between particles \(i\) and \(j\) at timestamp \(s\) and \(\tau\) is the predefined threshold. Then, we utilize an attention-based GNN to extract spatio-temporal relationships into node representations. Here, we first compute the interaction scores between each node in \(G^{H}\) with its neighboring nodes, and then aggregate their embeddings at the previous layer. Let \(d\) represent the hidden dimension, the interaction score between \(i^{s}\) and \(j^{s^{\prime}}\) at layer \(l\) is:

\[w^{(l)}(i^{s},j^{s^{\prime}})=\bm{A}(i^{s},j^{s^{\prime}})(\bm{W}_{query}\bm{h }_{i}^{s,(l)})\star(\bm{W}_{key}\bm{h}_{j}^{s^{\prime},(l)}),\] (5)

where \(\bm{W}_{query}\in\mathbb{R}^{d\times d}\) and \(\bm{W}_{key}\in\mathbb{R}^{d\times d}\) are two matrices to map temporal node representations into different spaces. \(\star\) computes the cosine similarity between two vectors. With the interaction scores, we can compute temporal node representations at the next layer:

\[\bm{h}_{i}^{s,(l+1)}=\bm{h}_{i}^{s,(l)}+\sigma\left(\sum_{j^{s^{\prime}}\in \mathcal{N}(i^{s})}w^{(l)}(i^{s},j^{s^{\prime}})\bm{W}_{value}\bm{h}_{j}^{s^{ \prime},(l)}\right),\] (6)

where \(\bm{W}_{value}\) is a weight matrix, \(\mathcal{N}(i^{s})\) collects all the neighbours of \(i^{s}\) and \(\sigma(\cdot)\) is an activation function. After stacking \(L\) layers, we add temporal encoding and then summarize all these temporal node representations to initialize node representations for the upcoming ODE module:

\[\bm{q}_{i}^{s}=\bm{h}_{i}^{s,(L)}+\mathrm{TE}(s),\quad\bm{u}_{i}^{t-k}=\frac{1 }{t-k+1}\sum_{s=0}^{t-k}\sigma(\bm{W}_{sum}\bm{q}_{i}^{s}),\] (7)

where \(\mathrm{TE}(s)[2i]=\sin\left(\frac{s}{10000^{2i/d}}\right)\), \(\mathrm{TE}(s)[2i+1]=\cos\left(\frac{s}{10000^{2i/d}}\right)\) and \(\bm{W}_{sum}\) denotes a projection matrix. The initial context variable \(\bm{c}^{t-k}\) is driven by summarizing all node representations:

\[\beta_{i}^{t}=tanh((\frac{1}{|V|}\sum_{i^{\prime}\in V}\bm{u}_{i^{\prime}}^{t -k})\bm{W}_{context})\cdot\bm{u}_{i}^{t-k},\quad\bm{c}^{t-k}=\sum_{i\in V} \beta_{i}^{t}\bm{u}_{i}^{t-k},\] (8)

where \(\bm{W}_{context}\) is a learnable matrix and \(\beta_{i}^{t}\) calculates the attention score for each node.

### Context-attended Graph ODE

In this module, to model continuous evolution, we incorporate an ODE system into our approach. The precondition requires assuming that both the context variable and node representations are continuous to fit neural ODE models, which inherently holds for common dynamical systems in practice. We then introduce coupled ODEs to model the dynamic evolution of node representations and the context variable. Specifically, the context variable can be inferred during the evolution of node representations, which in turn drives the evolution of the system. We first introduce the assumption:

**Assumption 4.3**.: _(Continuous) We assume that both context variable \(\bm{c}^{s}\) and node representations \(\bm{v}_{i}^{s}\) are continuously differentiable with respect to \(s\)._

Then, to utilize the context variable and the current state for making future predictions, we introduce a graph ODE model. Let \(\hat{\bm{A}}^{s}\) denote the adjacency matrix at timestamp \(s\) with self-loop, we have:

\[\frac{d\bm{v}_{i}^{s}}{ds}=\Phi([\bm{v}_{1}^{s},\cdots,\bm{v}_{N}^{s},\bm{c}^{ s}])=\sigma(\sum_{j\in\mathcal{N}^{s}(i)}\frac{\hat{\bm{A}}_{ij}^{s}}{\sqrt{ \hat{D}_{i}^{s}\cdot\hat{D}_{j}^{s}}}\bm{v}_{j}^{s}\bm{W}_{1}+\bm{c}^{s}\bm{W}_ {2}),\] (9)

where \(\mathcal{N}^{s}(i)\) denotes the neighbours of node \(i\) at timestamp \(s\) and \(\hat{D}_{i}^{s}\) represents the degree of node \(i\) according to \(\hat{\bm{A}}^{s}\). The first term in Eqn. 9 aggregates information from its instant neighbors and the second term captures information from the current context information.

The next question is how to model the evolution of \(\bm{c}^{s}\). Notice that we have:

\[\begin{split}\mathrm{P}\left(\bm{c}^{t}\mid\bm{c}^{t-k},G^{t-k:t }\right)=\int P(\bm{c}^{t}|\bm{c}^{t-\Delta t},G^{t-\Delta t:t})\cdots\\ P(\bm{c}^{t-k+\Delta t}|\bm{c}^{t-k},G^{t-k:t-k+\Delta t})d\bm{c}^{t-k +\Delta t}\cdots d\bm{c}^{t-\Delta t},\end{split}\] (10)where \(\Delta t\) denotes a small time interval. With Assumption 4.3, we can simplify \(P(\bm{c}^{t-k+\Delta t}|\bm{c}^{t-k},G^{t-k:t-k+\Delta t})\) into \(P(\bm{c}^{t-k+\Delta t}|\bm{c}^{t-k},\bm{V}^{t-k},d\bm{V}^{t-k})\) where \(\bm{V}^{t-k}\) denotes the node embedding matrix at timestamp \(t-k\) and \(d\bm{V}^{t-k}\) is its differentiation. On this basis, we introduce another ODE to update the context variable as:

\[\frac{d\bm{c}^{s}}{ds}=\Phi^{c}(\mathrm{AGG}(\{\bm{v}^{s}_{i}\}_{i\in V}), \mathrm{AGG}(\{\frac{d\bm{v}^{s}_{i}}{ds}\}_{i\in V}),\bm{c}^{s}]),\] (11)

where \(\Phi^{c}\) is an MLP with the concatenated input and \(\mathrm{AGG}(\cdot)\) is an operator to summarize node representations such as averaging and sum. Compared to previous methods, the key to our CARE is to take into account the mutual impact between the environment and the trajectories, and model their evolution simultaneously by coupling Eqn. 9 and Eqn. 11. We also provide a theoretical analysis of the uniqueness of the solution to our system. To simplify the analysis, we set \(\mathrm{AGG}(\cdot)\) to summation and rewrite Eqn. 11 with learnable matrices \(\bm{W}_{3}\), \(\bm{W}_{4}\) and \(\bm{W}_{5}\) as:

\[\frac{d\bm{c}^{s}}{ds}=\sigma\left(\sum_{i=1}^{N}(\bm{v}^{s}_{i}\bm{W}_{3}+ \frac{d\bm{v}^{s}_{i}}{ds}\bm{W}_{4})+\bm{c}^{s}\bm{W}_{5}\right).\] (12)

Then, we introduce the following assumption:

**Assumption 4.4**.: _All time-dependent coefficients in Eqn. 9, i.e. \(\bm{A}^{t}_{ij},\hat{D}^{t}_{i}\) are continuous with respect to \(t\) and bounded by a constant \(C>0\). All parameters in the weight matrix are also bounded by a constant \(W>0\)._

With Assumption 4.4, we can deduce the following lemma:

**Lemma 4.2**.: _Given the initial state \((t_{0},\bm{v}^{t_{0}}_{1},\cdots,\bm{v}^{t_{0}}_{N},\bm{c}^{t_{0}})\), we claim that there exists \(\varepsilon>0\), s.t. the ODE system 9 and 12 has a unique solution in the interval \([t_{0}-\varepsilon,t_{0}+\varepsilon]\)._

The proof of Lemma 4.2 can be found in Appendix. Our theoretical analysis indicates that at least locally, the future system trajectory and context information are predictable based on their historical values [51], which is also an important property for dynamical system modeling [5; 28].

### Decoder and Optimization

**Decoder.** We introduce an MLP \(\Phi^{d}(\cdot)\) to predict both the position and velocity vectors using corresponding node representations, i.e., \([\hat{\bm{p}}^{s}_{i},\hat{\bm{q}}^{s}_{i}]=\Phi^{d}(\bm{v}^{s}_{i})\).

**Dynamic Graph Updating.** We can estimate the instant distance between nodes using the encoder and then construct the graphs, which could suffer from a large computational burden. To improve the efficiency of graph construction during ODE propagation, we not only update the graph structure every \(\Delta s\), and but also introduce a graph updating strategy that calculates the distance between first-order and second-order neighbors in the last graph. By doing so, we can delete edges between first-order neighbors and add edges between second-order neighbors, reducing quadratic complexity to linear complexity in sparse graphs. We will also validate this empirically.

**Learning Objective.** Given the ground truth, we first minimize the mean squared error (MSE) of the predicted trajectory. Moreover, we require both node and context representations to be robust to noise attacks to improve the robustness of the ODE system. The overall objective is written as:

\[\mathcal{L}=\sum_{s=t-k}^{t}||\hat{\bm{Y}}^{s}-\bm{Y}^{s}||+\eta(||\tilde{\bm{ V}}^{s}-\bm{V}^{s}||+||\tilde{\bm{c}}^{s}-\bm{c}^{s}||),\] (13)

where \(\tilde{\bm{Y}}^{s}\) denotes the predictions from the encoder and \(\eta\) is a parameter set to \(0.1\) to balance two losses. \(\tilde{\bm{V}}^{s}\) and \(\tilde{\bm{c}}^{s}\) denote the perturbed representations under noise attack to the input.

## 5 Experiments

We evaluate our proposed CARE on both particle-based and mesh-based physical systems. To ensure the accuracy of our results, we use a rigorous data split strategy, where first 80\(\%\) of the samples are reserved for training purposes and the remaining 10\(\%\) are set aside for testing and validating,separately. During training, we split each trajectory sample into two parts, i.e., a conditional part and a prediction part. We initialize node representations and the context representation based on the first part and utilize the second part to supervise the model. The size of the two parts is represented as conditional length and prediction length, respectively. Our approach is compared with various baselines for interacting systems modeling, i.e., LSTM [15], STGCN [67], GNS [45], MeshGraphNet [42], TIE [46] and CG-ODE [19].

### Performance on Particle-based Physical Simulations

**Datasets.** We evaluate our proposed CARE on two particle-based simulation datasets with temporal environmental variations, i.e., _Lennard-Jones Potential_[47] and _3-body Stillinger-Weber Potential_[4]. _Lennard-Jones Potential_ is popular in modeling electronically neutral atoms or molecules. _3-body Stillinger-Weber Potential_ provides more complex relationships in atom systems The temperature in two particle-based systems is continuously changed along with the time to model the environmental variations. The objective is to predict the future velocity values in all directions, i.e., \(v_{x}\), \(v_{y}\) and \(v_{z}\). More details can be found in Appendix.

**Performance Comparison.** We evaluate the performance in terms of RMSE with different prediction lengths. Table 1 show the compared results on these two datasets. We can observe that our proposed CARE outperforms all the baselines on two datasets. In particular, compared with TIE, CARE accomplishes an error reduction of 24.03% and 36.35% on two datasets, respectively. The remarkable performance can be attributed to two factors: (1) Introduction of the context variable. Our CARE infers the context states during the evolution of the system, which can help the model understand environmental variations. (2) Introduction of robust learning. We add noise attack to both system and context states, which improves the model generalization to potential distribution changes.

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c} \hline \hline Prediction Length & \multicolumn{3}{c|}{+1} & \multicolumn{3}{c|}{+5} & \multicolumn{3}{c|}{+10} & \multicolumn{3}{c}{+20} \\ \hline Variable & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) \\ \hline \multicolumn{13}{l}{_Lennard-Jones Potential_} \\ \hline LSTM & 3.95 & 3.92 & 3.68 & 9.12 & 9.21 & 9.15 & 10.84 & 10.87 & 10.76 & 14.82 & 14.94 & 14.67 \\ GNS & 3.28 & 3.75 & 3.39 & 7.97 & 8.05 & 7.68 & 10.09 & 10.15 & 10.13 & 13.65 & 13.62 & 13.59 \\ STGCN & 2.91 & 3.08 & 2.95 & 5.06 & 5.17 & 5.11 & 6.89 & 6.90 & 6.93 & 9.31 & 9.32 & 9.44 \\ MeshGraphNet & 2.89 & 3.13 & 2.94 & 5.29 & 5.53 & 5.28 & 7.03 & 7.09 & 7.11 & 9.12 & 9.21 & 9.24 \\ CG-ODE & 1.79 & 2.05 & 1.71 & 3.47 & 3.92 & 3.38 & 5.46 & 5.99 & 5.36 & 9.03 & 9.26 & 8.92 \\ TIE & 1.62 & 1.98 & 1.47 & 3.25 & 3.90 & 3.15 & 5.24 & 5.82 & 5.17 & 8.24 & 8.34 & 8.47 \\ Ours & **0.76** & **0.89** & **1.01** & **2.94** & **3.16** & **2.85** & **5.01** & **4.69** & **4.71** & **5.75** & **5.91** & **5.82** \\ \hline \multicolumn{13}{l}{_3-body Stillinger-Weber Potential_} \\ \hline LSTM & 17.11 & 17.14 & 17.18 & 23.64 & 23.69 & 23.60 & 25.46 & 25.42 & 25.48 & 28.44 & 28.45 & 28.44 \\ GNS & 15.39 & 15.27 & 15.33 & 22.14 & 22.19 & 22.17 & 25.29 & 25.36 & 25.31 & 27.18 & 27.15 & 27.14 \\ STGCN & 12.33 & 12.31 & 12.35 & 17.94 & 17.96 & 17.91 & 20.08 & 20.14 & 20.13 & 23.49 & 23.51 & 23.52 \\ MeshGraphNet & 12.16 & 12.10 & 12.13 & 18.33 & 18.38 & 18.34 & 20.65 & 20.62 & 20.71 & 23.62 & 23.54 & 23.61 \\ CG-ODE & 9.78 & 9.74 & 9.75 & 12.11 & 12.05 & 12.14 & 15.55 & 15.58 & 15.50 & 16.17 & 16.24 & 16.22 \\ TIE & 10.18 & 10.26 & 10.19 & 14.75 & 14.70 & 14.73 & 18.42 & 18.45 & 18.41 & 20.92 & 21.04 & 21.36 \\ Ours & **4.21** & **4.29** & **4.18** & **9.74** & **9.79** & **9.71** & **13.65** & **13.71** & **13.57** & **15.30** & **15.39** & **15.35** \\ \hline \hline \end{tabular}
\end{table}
Table 1: The RMSE (\(\times 10^{-2}\)) results of the compared methods with the prediction lengths \(1\), \(5\), \(10\) and \(20\). \(v_{x}\), \(v_{y}\) and \(v_{z}\) represent the velocity in the direction of each coordinate axis.

Figure 2: Visualization of _Lennard-Jones Potential_ with multiple timestamps. We render the 3D positions of each particle according to the historical positions and predicted velocities.

**Visualization.** Figure 2 visualizes the prediction of positions in comparison to the ground truth on _Lennard-Jones Potential_. Here, we sample six timestamps in every trajectory to validate the performance of both short-term and long-term predictions. From the qualitative results, we can observe that in the first three frames, the particle motion is not strenuous due to low temperature in the system. Surprisingly, our proposed CARE can always make faithful physical simulations close to the ground truth even though the system environment is highly variable.

### Performance on Mesh-based Physical Simulations

**Datasets.** We employ two popular mesh-based simulation datasets, i.e., _CylinderFlow_, and _Airfoil_. _CylinderFlow_ consists of simulation data from modeling an incompressible flow governed by the Navier-Stokes equations. Notably, the initial flow velocity of the incoming water flow to the cylinder varies cyclically over time, meaning the Reynolds number of the flow field also changes periodically. _Airfoil_ is generated in a similar manner through simulations of a compressible flow, wherein the inlet velocity over the wing varies cyclically over time. We aim to forecast the velocity values \(v_{x}\) and \(v_{y}\), as well as the pressure \(p\). More details can be found in Appendix.

**Performance Comparison.** The performance with respect to different variables is recorded in Table 2. From the results, we can observe that the average performance of the proposed CARE is over the best baseline by 12.99% and 22.78% on two datasets, respectively. Note that unsteady flow [11; 16] is a crucial problem in recent fluid dynamics, our proposed CARE can benefit abundant complex mesh-based simulations under environmental variations.

**Visualization.** Moreover, we show the qualitative results of the best baseline and our CARE in comparison to the ground truth. From the results, we can observe that our CARE can capture more accurate signals in unsteady fluid dynamics. In particular, in the last two frames with complicated

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c} \hline \hline Prediction Length & \multicolumn{3}{c|}{+1} & \multicolumn{3}{c|}{+10} & \multicolumn{3}{c}{+20} & \multicolumn{3}{c}{+50} \\ \hline Variable & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) \\ \hline \multicolumn{13}{l}{_CylinderFlow_} \\ \hline LSTM & 3.35 & 29.4 & 12.5 & 7.06 & 44.8 & 17.8 & 9.47 & 49.5 & 19.9 & 14.3 & 73.6 & 42.3 \\ GNS & 3.12 & 28.8 & 11.9 & 7.18 & 44.3 & 17.3 & 9.01 & 49.6 & 19.2 & 13.5 & 73.2 & 41.6 \\ STGCN & 2.68 & 26.7 & 11.0 & 5.47 & 42.1 & 16.9 & 6.72 & 45.6 & 18.4 & 9.15 & 68.7 & 40.0 \\ MeshGraphNet & 1.75 & 22.4 & 10.6 & 4.09 & 39.7 & 15.7 & 5.38 & 44.5 & 17.2 & 7.92 & 64.3 & 37.7 \\ CG-ODE & 1.05 & 20.4 & 8.51 & 3.44 & 36.8 & 13.6 & 4.15 & 38.5 & 17.1 & 5.14 & 61.2 & 32.3 \\ TIE & 1.22 & 20.8 & 8.94 & 3.75 & 35.2 & 13.0 & 4.62 & 40.6 & 16.0 & 5.87 & 59.5 & 32.1 \\ Ours & **0.87** & **19.1** & **7.21** & **3.02** & **32.9** & **11.8** & **3.95** & **37.8** & **13.9** & **4.97** & **55.8** & **29.4** \\ \hline \multicolumn{13}{l}{_Airfoil_} \\ \hline LSTM & 7.49 & 7.73 & 1.92 & 8.86 & 9.02 & 3.78 & 10.8 & 11.0 & 4.71 & 14.9 & 15.7 & 4.96 \\ GNS & 6.95 & 7.14 & 1.69 & 8.20 & 8.34 & 3.34 & 10.2 & 10.5 & 3.98 & 14.2 & 14.1 & 4.11 \\ STGCN & 6.24 & 5.35 & 1.07 & 6.57 & 6.51 & 2.33 & 7.88 & 8.01 & 3.16 & 11.6 & 11.8 & 3.17 \\ MeshGraphNet & 4.72 & 4.68 & 0.50 & 5.89 & 5.74 & 1.23 & 6.32 & 6.48 & 1.85 & 9.03 & 9.12 & 2.08 \\ CG-ODE & 4.26 & 4.32 & 0.35 & 4.78 & 4.70 & 0.46 & 5.81 & 5.66 & 1.04 & 7.39 & 7.85 & 1.69 \\ TIE & 4.17 & 4.39 & 0.33 & 4.99 & 4.86 & 0.51 & 5.75 & 5.62 & 0.95 & 7.25 & 7.63 & 1.44 \\ Ours & **3.51** & **4.11** & **0.19** & **3.86** & **3.75** & **0.34** & **4.16** & **4.12** & **0.45** & **6.74** & **6.82** & **0.81** \\ \hline \hline \end{tabular}
\end{table}
Table 2: The RMSE results of the compared methods over different prediction lengths \(1\), \(10\), \(20\) and \(50\). \(v_{x}\), \(v_{y}\) and \(p\) represent the velocity in different directions and the pressure field, respectively.

Figure 3: Visualization of the CylinderFlow Dataset with multiple timestamps. We render the velocity in the \(x\)-axis in the fluid field of our CARE and the ground truth.

structures, our CARE still can generate superior simulations in both scenarios under potential environmental variation while the baseline fails, which shows the superiority of our proposed CARE.

### Further Analysis

**Ablation Study.** To analyze the effectiveness of different components in our CARE, we introduce two different variants: (1) CARE V1, which removes the context variable in Eqn. 9; (2) CARE V2, which removes the robust learning term in Eqn. 13. The compared performance is recorded in Figure 3 and we have two observations. First, our full model outperforms CARE V1, which indicates the incorporation of the context variable would benefit interacting system modeling under temporal environmental variation. Second, without the robust learning term, the performance would get worse, implying that improving the robustness can also benefit tackling the distribution changes.

**Parameter Sensitivity.** We begin by analyzing the performance with respect to different condition lengths and prediction lengths. Here the condition length and prediction length vary from {10,15,20,25,30}, {20,50}, respectively. From the results in Figure 4 (a) and (b), we can observe that our proposed CARE can always achieve superior performance compared with CG-ODE. Moreover, we can observe that a longer condition length would benefit the performance in most cases due to more provided information. It can also be seen that a smaller interval for graph updating would improve the performance before saturation from Figure 4 (c).

**Efficiency.** To show the efficiency of our proposed dynamic graph updating, we propose a model variant named CARE E, which calculates all pairwise distances to update graph structure during the evolution. The computational cost is recorded in Figure 4 (d) and we can observe that our strategy can reduce a large number of computational costs, which validates the complexity analysis before.

## 6 Conclusion

This paper studies the problem of modeling interacting dynamics under temporal environmental variation and we propose a probabilistic framework to depict the dynamical system. Then, a novel approach named CARE is proposed. CARE first constructs an encoder to initialize the context variable indicating the environment and then utilizes a coupled ODE system, which combines both the context variable and node representation to drive the evolution of the system. Finally, we introduce both efficient dynamical graph updating and robust learning strategies to enhance our framework. Extensive experiments on four datasets validate the superiority of our CARE.

**Broader Impacts and Limitations.** This work presents an effective learning-based model for interacting dynamical systems under temporal environmental variation, which can benefit complex physical simulations such as unsteady flow. Moreover, our study provides a new perspective on

Figure 4: (a), (b) The performance with respect to different condition and prediction lengths on CylinderFlow and Airfoil. (c) The sensitivity of interval on Lennard-Jones Potential (LJP) and 3-body Stillinger-Weber Potential (SWP) datasets. (d) The comparison of running time for our dynamic graph updating and full pairwise calculation on two particle-based datasets.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline Datasets & \multicolumn{3}{c}{Lennard-Jones} & \multicolumn{3}{c}{3-body Stillinger-Weber} & \multicolumn{3}{c}{CylinderFlow} & \multicolumn{3}{c}{Airfoil} \\ \hline Variable & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(v_{z}\) & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) \\ \hline CARE V1 & 6.98 & 7.12 & 7.06 & 18.2 & 18.3 & 18.3 & 6.13 & 60.4 & 32.2 & 7.13 & 7.21 & 1.43 \\ CARE V2 & 6.03 & 6.35 & 6.30 & 16.8 & 16.5 & 16.6 & 5.21 & 57.2 & 29.8 & 6.94 & 6.99 & 1.15 \\ Ours & **5.75** & **5.91** & **5.82** & **15.3** & **15.4** & **15.4** & **4.97** & **55.8** & **29.4** & **6.74** & **6.82** & **0.81** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study on four datasets.

modeling environmental variations for fluid dynamics and intermolecular interactions. One potential limitation is that our CARE cannot directly fit more physical scenarios requiring abundant external knowledge. In future work, we would extend our CARE to more complicated applications such as rigid dynamics.

## Acknowledgement

This work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NSF 2312501, NASA, SRC Jump 2.0, Okawa Foundation Grant, Amazon Research Awards, Cisco research grant, Picsart Gifts, and Snapchat Gifts.

## References

* [1] Kelsey R Allen, Yulia Rubanova, Tatiana Lopez-Guevara, William Whitney, Alvaro Sanchez-Gonzalez, Peter Battaglia, and Tobias Pfaff. Learning rigid dynamics with face interaction graph networks. _arXiv preprint arXiv:2212.03574_, 2022.
* [2] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In _NeurIPS_, 2016.
* [3] Suresh Bishnoi, Ravinder Bhattoo, Jayadeva Jayadeva, Sayan Ranu, and NM Anoop Krishnan. Enhancing the inductive biases of graph neural ode for modeling physical systems. In _ICLR_, 2023.
* [4] W Michael Brown and Masako Yamada. Implementing molecular dynamics on hybrid high performance computers--three-body potentials. _Computer Physics Communications_, 184(12):2785-2793, 2013.
* [5] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In _NeurIPS_, 2018.
* [6] Yongqiang Chen, Yonggang Zhang, Yatao Bian, Han Yang, MA Kaili, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Learning causally invariant representations for out-of-distribution generalization on graphs. In _NeurIPS_, pages 22131-22148, 2022.
* [7] Yitong Deng, Hong-Xing Yu, Jiajun Wu, and Bo Zhu. Learning vortex dynamics for fluid inference and prediction. _arXiv preprint arXiv:2301.11494_, 2023.
* [8] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In _NeurIPS_, 2019.
* [9] Zheng Fang, Qingqing Long, Guojie Song, and Kunqing Xie. Spatial-temporal graph ode networks for traffic flow forecasting. In _KDD_, pages 364-373, 2021.
* [10] Jayesh K Gupta, Sai Vemprala, and Ashish Kapoor. Learning modular simulations for homogeneous systems. In _NeurIPS_, 2022.
* [11] Rohit Gupta and Phillip J Ansell. Unsteady flow physics of airfoil dynamic stall. _AIAA journal_, 57(1):165-175, 2019.
* [12] Nesrine Hafiene, Wafa Karoui, and Lotfi Ben Romdhane. Influential nodes detection in dynamic social networks: A survey. _Expert Systems with Applications_, 159:113642, 2020.
* [13] Mingguo He, Zhewei Wei, and Ji-Rong Wen. Convolutional neural networks on graphs with chebyshev approximation, revisited. In _NeurIPS_, 2022.
* [14] Yue He, Zimu Wang, Peng Cui, Hao Zou, Yafeng Zhang, Qiang Cui, and Yong Jiang. Causpref: Causal preference learning for out-of-distribution recommendation. In _WWW_, pages 410-421, 2022.
* [15] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural Computation_, 9(8):1735-1780, 1997.

* [16] C Huang, L Zhao, JP Niu, JJ Di, JJ Yuan, QL Zhao, PQ Zhang, ZH Zhang, JM Lei, and GP He. Coupled particle and mesh method in an euler frame for unsteady flows around the pitching airfoil. _Engineering Analysis with Boundary Elements_, 138:159-176, 2022.
* [17] Chao Huang, Xian Wu, Xuchao Zhang, Chuxu Zhang, Jiashu Zhao, Dawei Yin, and Nitesh V Chawla. Online purchase prediction via multi-scale modeling of behavior dynamics. In _KDD_, pages 2613-2622, 2019.
* [18] Zijie Huang, Yizhou Sun, and Wei Wang. Learning continuous system dynamics from irregularly-sampled partial observations. In _NeurIPS_, pages 16177-16187, 2020.
* [19] Zijie Huang, Yizhou Sun, and Wei Wang. Coupled graph ode for learning interacting system dynamics. In _KDD_, 2021.
* [20] Zijie Huang, Yizhou Sun, and Wei Wang. Generalizing graph ode for learning complex system dynamics across environments. In _KDD_, pages 798-809, 2023.
* [21] Steeven Janny, Aurelien Beneetau, Nicolas Thome, Madiha Nadri, Julie Digne, and Christian Wolf. Eagle: Large-scale learning of turbulent fluid dynamics with mesh transformers. _arXiv preprint arXiv:2302.10803_, 2023.
* [22] Hrvoje Jasak. Openfoam: open source cfd in research and industry. _International Journal of Naval Architecture and Ocean Engineering_, 1(2):89-94, 2009.
* [23] Ruoxi Jiang and Rebecca Willett. Embed and emulate: Learning to estimate parameters of dynamical systems with uncertainty quantification. In _NeurIPS_, 2022.
* [24] Patrick Kidger, Ricky T. Q. Chen, and Terry J. Lyons. "hey, that's not an ode": Faster ode adjoints via seminorms. _ICML_, 2021.
* [25] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In _ICML_, pages 2688-2697, 2018.
* [26] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In _ICLR_, 2017.
* [27] Miltiadis Kofinas, Naveen Nagaraja, and Efstratios Gavves. Roto-translated local coordinate frames for interacting dynamical systems. In _NeurIPS_, 2021.
* [28] Lingkai Kong, Jimeng Sun, and Chao Zhang. Sde-net: Equipping deep neural networks with uncertainty estimates. In _ICML_, 2020.
* [29] Shiyong Lan, Yitong Ma, Weikang Huang, Wenwu Wang, Hongyu Yang, and Pyang Li. Dstagnn: Dynamic spatial-temporal aware graph neural network for traffic flow forecasting. In _ICML_, pages 11906-11917, 2022.
* [30] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In _CVPR_, pages 5400-5409, 2018.
* [31] Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex Kot. Domain generalization for medical imaging classification with linear-dependency regularization. In _NeurIPS_, pages 3118-3129, 2020.
* [32] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In _NeurIPS_, 2022.
* [33] Xiang Li, Renyu Zhu, Yao Cheng, Caihua Shan, Siqiang Luo, Dongsheng Li, and Weining Qian. Finding global homophily in graph neural networks when meeting heterophily. In _ICML_, pages 13242-13256, 2022.
* [34] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out-of-distribution generalization. _arXiv preprint arXiv:2202.03958_, 2022.

* [35] Xuan Li, Yadi Cao, Minchen Li, Yin Yang, Craig Schroeder, and Chenfanfu Jiang. Plasticitynet: Learning to simulate metal, sand, and snow for optimization time integration. In _NeurIPS_, pages 27783-27796, 2022.
* [36] Siyuan Liao, Shangsong Liang, Zaiqiao Meng, and Qiang Zhang. Learning dynamic embeddings for temporal knowledge graphs. In _WSDM_, pages 535-543, 2021.
* [37] Andreas Look, Melih Kandemir, Barbara Rakitsch, and Jan Peters. Cheap and deterministic inference for deep state-space models of interacting dynamical systems. _arXiv preprint arXiv:2305.01773_, 2023.
* [38] Wang Lu, Jindong Wang, Yidong Wang, Kan Ren, Yiqiang Chen, and Xing Xie. Towards optimization and model selection for domain generalization: A mixup-guided solution. _arXiv preprint arXiv:2209.00652_, 2022.
* [39] Lucas Mansilla, Rodrigo Echeveste, Diego H Milone, and Enzo Ferrante. Domain generalization via gradient surgery. In _ICCV_, pages 6630-6638, 2021.
* [40] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017.
* [41] Jiang-Zhou Peng, Siheng Chen, Nadine Aubry, Zhihua Chen, and Wei-Tao Wu. Unsteady reduced-order model of flow over cylinders based on convolutional and deconvolutional neural network structure. _Physics of Fluids_, 32(12):123609, 2020.
* [42] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter W Battaglia. Learning mesh-based simulation with graph networks. In _ICLR_, 2021.
* [43] Lukas Prantl, Benjamin Ummenhofer, Vladlen Koltun, and Nils Thuerey. Guaranteed conservation of momentum for learning particle-based fluid dynamics. _arXiv preprint arXiv:2210.06036_, 2022.
* [44] Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In _ICML_, pages 18347-18377, 2022.
* [45] Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter Battaglia. Learning to simulate complex physics with graph networks. In _ICML_, pages 8459-8468, 2020.
* [46] Yidi Shao, Chen Change Loy, and Bo Dai. Transformer with implicit edges for particle-based physics simulation. In _ECCV_, pages 549-564, 2022.
* [47] BJTJ Smit. Phase diagrams of lennard-jones fluids. _The Journal of Chemical Physics_, 96(11):8639-8640, 1992.
* [48] Yaguang Song, Xiaoshan Yang, Yaowei Wang, and Changsheng Xu. Recovering generalization via pre-training-like knowledge distillation for out-of-distribution visual question answering. _IEEE Transactions on Multimedia_, 2023.
* [49] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In _ICLR_, 2018.
* [50] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In _NeurIPS_, 2018.
* [51] Charles Vorbach, Ramin Hasani, Alexander Amini, Mathias Lechner, and Daniela Rus. Causal navigation by continuous-time neural networks. In _NeurIPS_, pages 12425-12440, 2021.
* [52] Rui Wang, Yihe Dong, Sercan O Arik, and Rose Yu. Koopman neural forecaster for time series with temporal distribution shifts. _arXiv preprint arXiv:2210.03675_, 2022.
* [53] Xiaoyang Wang, Yao Ma, Yiqi Wang, Wei Jin, Xin Wang, Jiliang Tang, Caiyan Jia, and Jian Yu. Traffic flow prediction via spatial temporal graph neural network. In _WWW_, pages 1082-1092, 2020.

* [54] Yuqing Wang, Xiangxian Li, Zhuang Qi, Jingyu Li, Xuelong Li, Xiangxu Meng, and Lei Meng. Meta-causal feature learning for out-of-distribution generalization. In _ECCV_, pages 530-545, 2022.
* [55] Ziqi Wang, Marco Loog, and Jan van Gemert. Respecting domain relations: Hypothesis invariance for domain generalization. In _ICPR_, pages 9756-9763, 2021.
* [56] Florian Wenzel, Andrea Dittadi, Peter Gehler, Carl-Johann Simon-Gabriel, Max Horn, Dominik Zietlow, David Kernert, Chris Russell, Thomas Brox, Bernt Schiele, et al. Assaying out-of-distribution generalization in transfer learning. In _NeurIPS_, pages 7181-7198, 2022.
* [57] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. _arXiv preprint arXiv:2202.02466_, 2022.
* [58] Tailin Wu, Takashi Maruyama, Qingqing Zhao, Gordon Wetzstein, and Jure Leskovec. Learning controllable adaptive simulation for multi-resolution physics. In _ICLR_, 2023.
* [59] Tailin Wu, Qinchen Wang, Yinan Zhang, Rex Ying, Kaidi Cao, Rok Sosic, Ridwan Jalali, Hassan Hamam, Marko Maucce, and Jure Leskovec. Learning large-scale subsurface simulations with a hybrid graph network simulator. In _KDD_, pages 4184-4194, 2022.
* [60] Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In _ICML_, pages 10432-10441, 2020.
* [61] Hedi Xia, Vai Sulaifu, Hangjie Ji, Tan Nguyen, Andrea Bertozzi, Stanley Osher, and Bao Wang. Heavy ball neural ordinary differential equations. In _NeurIPS_, pages 18646-18659, 2021.
* [62] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* [63] Chenxiao Yang, Qitian Wu, Qingsong Wen, Zhiqiang Zhou, Liang Sun, and Junchi Yan. Towards out-of-distribution sequential event prediction: A causal treatment. _arXiv preprint arXiv:2210.13005_, 2022.
* [64] Nianzu Yang, Kaipeng Zeng, Qitian Wu, Xiaosong Jia, and Junchi Yan. Learning substructure invariance for out-of-distribution molecular representations. In _NeurIPS_, 2022.
* [65] Cagatay Yildiz, Melih Kandemir, and Barbara Rakitsch. Learning interacting dynamical systems with latent gaussian process odes. In _NeurIPS_, pages 9188-9200, 2022.
* [66] Yuan Yin, Matthieu Kirchmeyer, Jean-Yves Franceschi, Alain Rakotomamonjy, and Patrick Gallinari. Continuous pde dynamics forecasting with implicit neural representations. In _NeurIPS_, 2022.
* [67] Bing Yu, Haoteng Yin, and Zhanxing Zhu. Spatio-temporal graph convolutional networks: a deep learning framework for traffic forecasting. In _IJCAI_, pages 3634-3640, 2018.
* [68] Xiaofeng Yuan, Lin Li, Yuri AW Shardt, Yalin Wang, and Chunhua Yang. Deep learning with spatiotemporal attention-based lstm for industrial soft sensor model development. _IEEE Transactions on Industrial Electronics_, 68(5):4404-4414, 2020.
* [69] Wei Zhang, Li Zhou, Bin Yang, and Tinggui Yan. Molecular dynamics simulations of licl ion pairs in high temperature aqueous solutions by deep learning potential. _Journal of Molecular Liquids_, 367:120500, 2022.
* [70] Yanfu Zhang, Shangqian Gao, Jian Pei, and Heng Huang. Improving social network embedding via new second-order continuous graph neural networks. In _KDD_, pages 2515-2523, 2022.
* [71] Tong Zhao, Gang Liu, Daheng Wang, Wenhao Yu, and Meng Jiang. Learning from counterfactual links for link prediction. In _ICML_, pages 26911-26926, 2022.
* [72] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. Graph neural networks for graphs with heterophily: A survey. _arXiv preprint arXiv:2202.07082_, 2022.

Proof of Lemma 4.1

**Lemma 4.1**.: _With Assumptions 4.1 and 4.2, we have:_

\[\begin{split}&\mathrm{P}\left(\bm{Y}^{t}\mid G^{0:t-1}\right)= \int\mathrm{P}\left(\bm{Y}^{t}\mid\bm{c}^{t-1},G^{t-1}\right)\cdot\\ &\mathrm{P}\left(\bm{c}^{t-1}\mid\bm{c}^{t-k},G^{t-k:t-1} \right)\cdot\mathrm{P}\left(\bm{c}^{t-k}\mid G^{0:t-k}\right)d\bm{c}^{t-1}d \bm{c}^{t-k}.\end{split}\] (14)

Proof.: We have:

\[\begin{split}&\mathrm{P}\left(\bm{Y}^{t}\mid G^{0:t-1}\right)\\ &=\int\mathrm{P}\left(\bm{Y}^{t}\mid\bm{c}^{t-1},G^{0:t-1} \right)\cdot\mathrm{P}\left(\bm{c}^{t-1}\mid G^{0:t-1}\right)d\bm{c}^{t-1}\\ &=\int\mathrm{P}\left(\bm{Y}^{t}\mid\bm{c}^{t-1},G^{t-1} \right)\cdot\mathrm{P}\left(\bm{c}^{t-1}\mid G^{0:t-1}\right)d\bm{c}^{t-1}\\ &=\int\mathrm{P}\left(\bm{Y}^{t}\mid\bm{c}^{t-1},G^{t-1}\right) \cdot\mathrm{P}\left(\bm{c}^{t-1}\mid\bm{c}^{t-k},G^{0:t-1}\right)\cdot \mathrm{P}\left(\bm{c}^{t-k}\mid G^{0:t-k}\right)d\bm{c}^{t-1}d\bm{c}^{t-k}\\ &=\int\mathrm{P}\left(\bm{Y}^{t}\mid\bm{c}^{t-1},G^{t-1}\right) \cdot\mathrm{P}\left(\bm{c}^{t-1}\mid\bm{c}^{t-k},G^{t-k:t-1}\right)\cdot \mathrm{P}\left(\bm{c}^{t-k}\mid G^{0:t-k}\right)d\bm{c}^{t-1}d\bm{c}^{t-k} \end{split}\] (15)

## Appendix B Proof of Lemma 4.2

For convenience, Eqn. 9 in the main paper is repeated as:

\[\frac{dv_{i}^{s}}{ds}=\Phi([\bm{v}_{1}^{s},\cdots,\bm{v}_{N}^{s},\bm{c}^{s}])= \sigma(\sum_{j\in\mathcal{N}^{s}(i)}\frac{\hat{\bm{A}}_{ij}^{s}}{\sqrt{\hat{D} _{i}^{s}\cdot\hat{D}_{j}^{s}}}\bm{v}_{j}^{s}\bm{W}_{1}+\bm{c}^{s}\bm{W}_{2}),\] (16)

Eqn. 12 is repeated as:

\[\frac{d\bm{c}^{s}}{ds}=\sigma\left(\sum_{i=1}^{N}(\bm{v}_{i}^{s}\bm{W}_{3}+ \frac{d\bm{v}_{i}^{s}}{ds}\bm{W}_{4})+\bm{c}^{s}\bm{W}_{5}\right).\] (17)

**Lemma 4.2**.: _Given the initial state \((t_{0},\bm{v}_{1}^{t_{0}},\cdots,\bm{v}_{N}^{t_{0}},\bm{c}^{t_{0}})\), we claim that there exists \(\varepsilon>0\), s.t. the ODE system Eqn. 16 and Eqn. 17 has a unique solution in the interval \([t_{0}-\varepsilon,t_{0}+\varepsilon]\)._

To begin, we introduce the Picard-Lindel\(\ddot{o}\)f Theorem as follows:

**Theorem B.1**.: _(Picard-Lindel\(\ddot{o}\)f Theorem) Let \(D\subseteq\mathbb{R}\times\mathbb{R}^{n}\) be a closed rectangle with \((t_{0},y_{0})\in D\). Let \(f:D\rightarrow\mathbb{R}^{n}\) be a function which is continuous with respect to \(t\) and Lipschitz continuous with respect to \(y\). Then, there exists some \(\varepsilon>0\) such that the initial value problem:_

\[y^{\prime}(t)=f(t,y(t)),\quad y\left(t_{0}\right)=y_{0}.\] (18)

_has a unique solution \(y(t)\) in the interval \([t_{0}-\varepsilon,t_{0}+\varepsilon]\)._

Proof.: Let \(\bm{A}_{ij}^{s}=0\) if \(j\notin\mathcal{N}^{s}(i)\) and denote \(\bm{M}_{ij}^{s}=\frac{\hat{\bm{A}}_{ij}^{s}}{\sqrt{\hat{D}_{i}^{s}\cdot\hat{D}_ {j}^{s}}}\). Then, we transpose them with Eqn. 16 and 17 becoming:

\[\begin{split}\frac{d(\bm{v}_{i}^{s})^{T}}{ds}&=\sigma \left(\sum_{j=1}^{N}\bm{M}_{ij}^{s}\bm{W}_{1}^{T}(\bm{v}_{j}^{s})^{T}+\bm{W}_{2 }^{T}(\bm{c}^{s})^{T}\right),\\ \frac{d(\bm{c}^{s})^{T}}{ds}&=\sigma\left(\sum_{i=1} ^{N}(\bm{W}_{3}^{T}(\bm{v}_{i}^{s})^{T}+\bm{W}_{4}^{T}\frac{d(\bm{v}_{i}^{s})^ {T}}{ds})+\bm{W}_{5}^{T}(\bm{c}^{s})^{T}\right).\end{split}\] (19)Let \(\bm{Y}^{s}=\begin{pmatrix}\bm{v}_{1}^{s}\\ \vdots\\ \bm{(\bm{v}_{N}^{s})}^{T}\\ \bm{(\bm{c}^{s})}^{T}\end{pmatrix}\in\mathbf{R}^{(N\times d_{v}+d_{c})\times 1}\), where \(\bm{v}_{i}\in\mathbf{R}^{d_{v}},\bm{c}\in\mathbf{R}^{d_{c}}\).

From the Eqn. 19 we get the ODE system \(\frac{d\bm{Y}^{s}}{ds}=f(\bm{Y}^{s};\theta)\) with fixed parameters \(\theta\) for the ODE solver. Here the function \(f(\bm{Y}^{t};\theta)\) is continuous w.r.t \(t\) since all components in the vector \(\bm{Y}^{t}\) are continuous w.r.t \(t\) and \(\theta\) does not depend on t.

Now consider activation functions \(\sigma\), such as ReLU, that satisfy the following inequality for all \(x\) and \(y\):

\[\|\sigma(x)-\sigma(y)\|\leq\|x-y\|\]

Then, for any two solutions \(\bm{Y}_{1}^{s},\bm{Y}_{2}^{s}\), we have:

\[\|f(\bm{Y}_{1}^{s};\theta)-f(\bm{Y}_{2}^{s};\theta)\|_{2}\leq\left\|\begin{pmatrix} \delta_{1}\\ \vdots\\ \delta_{N}\\ \end{pmatrix}\right\|_{2},\]

\[\delta_{i}=\sum_{j=1}^{N}\bm{M}_{ij}^{s}\bm{W}_{1}^{T}\Big{[}(\bm{v}_{1j}^{s} )^{T}-(\bm{v}_{2j}^{s})^{T}\Big{]}+\bm{W}_{2}^{T}\Big{[}(\bm{c}_{1}^{s})^{T}- (\bm{c}_{2}^{s})^{T}\Big{]},\quad i=1,\ldots,N\]

\[\delta_{N+1}=\sum_{i=1}^{N}\Bigg{(}\bm{W}_{3}^{T}\Big{[}(\bm{v}_{1i}^{s})^{T}- (\bm{v}_{2i}^{s})^{T}\Big{]}+\bm{W}_{4}^{T}\Big{[}\sigma(\frac{d(\bm{v}_{1i}^{ s})^{T}}{ds})-\sigma(\frac{d(\bm{v}_{2i}^{s})^{T}}{ds})\Big{]}\Bigg{)}+\bm{W}_{5}^{T} \Big{[}(\bm{c}_{1}^{s})^{T}-(\bm{c}_{2}^{s})^{T}\Big{]}.\]

To simplify the representation, denote:

\[\Delta\bm{v}_{j}^{s}=\bm{v}_{1j}^{s}-\bm{v}_{2j}^{s},\quad\Delta\bm{c}^{s}= \bm{c}_{1}^{s}-\bm{c}_{2}^{s}.\]

Then by triangular inequality, we have \(\forall i\in\{1,\cdots,N\}\),

\[\big{\|}\delta_{i}^{T}\big{\|}_{2}^{2} =\|\sum_{j=1}^{N}\Delta\bm{v}_{j}^{s}\bm{W}_{1}(\bm{M}_{ij}^{s})^ {T}+\Delta\bm{c}^{s}\bm{W}_{2}\|_{2}^{2}\] \[\leq(\sum_{j=1}^{N}\|\Delta\bm{v}_{j}^{s}\bm{W}_{1}(\bm{M}_{ij}^{s })^{T}\|_{2}+\|\Delta\bm{c}^{s}\bm{W}_{2}\|_{2})^{2}\] \[\leq(WM\sum_{j=1}^{N}\|\Delta\bm{v}_{j}^{s}\|_{2}+W\|\Delta\bm{c} ^{s}\|_{2})^{2},\]

\[\big{\|}\delta_{N+1}^{T}\big{\|}_{2}^{2} =\|\sum_{i=1}^{N}\Big{(}\Delta\bm{v}_{i}^{s}\bm{W}_{3}+\big{[} \sigma(\frac{d\bm{v}_{1i}^{s}}{ds})-\sigma(\frac{d\bm{v}_{2i}^{s}}{ds})\big{]} \bm{W}_{4}\Big{)}+\Delta\bm{c}^{s}\bm{W}_{5}\|_{2}^{2}\] \[\leq(\sum_{i=1}^{N}\|\Delta\bm{v}_{i}^{s}\bm{W}_{3}\|_{2}+\sum_{i= 1}^{N}\|\big{[}\sigma(\frac{d\bm{v}_{1i}^{s}}{ds})-\sigma(\frac{d\bm{v}_{2i}^{ s}}{ds})\big{]}\bm{W}_{4}\|_{2}+\|\Delta\bm{c}^{s}\bm{W}_{5}\|_{2})^{2}\] \[\leq(W\sum_{i=1}^{N}\|\Delta\bm{v}_{i}^{s}\|_{2}+W\sum_{i=1}^{N} \|\frac{d\bm{v}_{1i}^{s}}{ds}-\frac{d\bm{v}_{2i}^{s}}{ds}\|_{2}+W\|\Delta\bm{c} ^{s}\|_{2})^{2}\] \[=(W\sum_{i=1}^{N}\|\Delta\bm{v}_{i}^{s}\|_{2}+W\sum_{i=1}^{N}\| \delta_{i}^{T}\|_{2}+W\|\Delta\bm{c}^{s}\|_{2})^{2}\] \[\leq\Big{(}W\sum_{i=1}^{N}\|\Delta\bm{v}_{i}^{s}\|_{2}+WN(WM\sum_{ j=1}^{N}\|\Delta\bm{v}_{j}^{s}\|_{2}+W\|\Delta\bm{c}^{s}\|_{2})+W\|\Delta\bm{c}^{s}\|_{2} \Big{)}^{2}\] \[=\Big{(}(W+MNW^{2})\sum_{i=1}^{N}\|\Delta\bm{v}_{i}^{s}\|_{2}+(W+ NW^{2})\|\Delta\bm{c}^{s}\|_{2}\Big{)}^{2}.\]Without loss of generality, we assume constants \(M,W>1\). Thus, we have the following result:

\[\left\|f(\bm{Y}_{1}^{s};\theta)-f(\bm{Y}_{2}^{s};\theta)\right\|_{2}^ {2} \leq\left\|\left(\delta_{1}^{T},\cdots,\delta_{N}^{T},\delta_{N+1}^ {T}\right)\right\|_{2}^{2},\] \[=\left\|\delta_{1}^{T}\right\|_{2}^{2}+\cdots+\left\|\delta_{N}^ {T}\right\|_{2}^{2}+\left\|\delta_{N+1}^{T}\right\|_{2}^{2}\] \[\leq N(WM\sum_{j=1}^{N}\|\Delta\bm{v}_{j}^{s}\|_{2}+W\|\Delta\bm{ c}^{s}\|_{2})^{2}+\] \[\quad\left((W+MNW^{2})\sum_{i=1}^{N}\|\Delta\bm{v}_{i}^{s}\|_{2}+( W+NW^{2})\|\Delta\bm{c}^{s}\|_{2}\right)^{2}\] \[\leq\left[NW^{2}M^{2}+(W+MNW^{2})^{2}\right]\Bigr{(}\sum_{j=1}^{ N}\|\Delta\bm{v}_{j}^{s}\|_{2}+\|\Delta\bm{c}^{s}\|_{2}\Bigr{)}^{2}\] \[=\Bigl{[}NW^{2}M^{2}+(W+MNW^{2})^{2}\Bigr{]}\|\bm{Y}_{1}^{s}-\bm{ Y}_{2}^{s}\|_{2}^{2}.\]

Therefore the function \(f\) is L-Lipschitz with \(L=\sqrt{NW^{2}M^{2}+(W+MNW^{2})^{2}}\). By Picard-Lindel\(\ddot{o}\)f theorem, we prove the uniqueness of the solution. 

## Appendix C Dataset Details

We evaluate our proposed CARE on four physical simulation datasets with temporal environmental variation. All these four datasets involve at least one thousand nodes. Then we introduce the details of these four datasets.

* _Lennard-Jones Potential_ (a.k.a. 6-12 potential) is popular in modeling electronically neutral atoms or molecules, which can be formulated as: \[V_{\text{LJ}}=4\varepsilon\left[\left(\frac{\sigma}{r}\right)^{12}-\left( \frac{\sigma}{r}\right)^{6}\right],\] (20) where \(r\) is the distance between particle pairs, \(\sigma\) denotes the size of the particle, \(\epsilon\) denotes the depth of the potential well. The first term denotes the attractive force, which decreases as the distance between particles increases. The second term denotes the repulsive force, which increases when two particles are too close. The temperature in the system is changed along with the time to model the environmental variations and a high temperature would bring a more intense molecular motion.
* _3-body Stillinger-Weber Potential_ provides more complex relationships besides pairwise relationships in _Lennard-Jones Potential_. It contains both two-body and three-body terms with the following formulation: \[V_{\text{SW}}=\sum_{i}\sum_{j>i}\phi_{2}\left(r_{ij}\right)+\sum_{i}\sum_{j \neq i}\sum_{k>j}\phi_{3}\left(r_{ij},r_{ik},\theta_{ijk}\right),\] (21) where \(\phi_{2}\left(r_{ij}\right)=A_{ij}\epsilon_{ij}\left[B_{ij}\left(\frac{\sigma _{ij}}{r_{ij}}\right)^{p_{ij}}-\left(\frac{\sigma_{ij}}{r_{ij}}\right)^{q_{ij }}\right]\exp\left(\frac{\sigma_{ij}}{r_{ij}-a_{ij}\sigma_{ij}}\right)\) is the two-body term and \(\phi_{3}\left(r_{ij},r_{ik},\theta_{ijk}\right)=\lambda_{ijk}\epsilon_{ijk} \left[\cos\theta_{ijk}-\cos\theta_{0ijk}\right]^{2}\exp\left(\frac{\gamma_{ij }\sigma_{ij}}{r_{ij}-a_{ij}\sigma_{ij}}\right)\exp\left(\frac{\gamma_{ik} \sigma_{ik}}{r_{ik}-a_{ik}\sigma_{ik}}\right)\) is the three-body term. The two body term is similar to _Lennard-Jones Potential_ to model the pairwise relationships and the three body term can consider the angles among atom triplets. Similarly, the temperature is changed along with the time to model the environmental variations and a high temperature would also bring a more intense molecular motion.
* _CylinderFlow_ is a popular computational fluid dynamics (CFD) simulation dataset, which models the fluid flow around a given cylinder by OpenFoam [22]. It consists of simulation data from modeling an incompressible flow governed by the Navier-Stokes equations. The Reynolds number, denoted by Re, is a dimensionless quantity that characterizes the flow regime of the fluid. It is defined as: \(Re=\frac{\rho VD}{\mu}\), where \(\rho\) is the density of the fluid, \(V\) is the velocity of the fluid relative to the cylinder, \(D\) is the diameter of the cylinder, and \(\mu\) is the dynamic viscosity of the fluid. The transition from the laminar to turbulent flow usually happens at a critical Reynolds number, which 

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

instance, MeshGraphNet [42] employs a message passing neural network to facilitate the modeling of interactions between objectives, thereby outputting the next-time predictions. However, an inherent drawback lies in the inability of discrete GNNs to encapsulate the continuous nature of system

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline \hline Prediction Length & \multicolumn{3}{c|}{+15} & \multicolumn{3}{c}{+30} \\ \hline Variable & \(v_{x}\) & \(v_{y}\) & \(p\) & \(v_{x}\) & \(v_{y}\) & \(p\) \\ \hline \multicolumn{6}{l}{_CylinderFlow_} \\ \hline LSTM & 8.25 & 47.43 & 17.62 & 11.09 & 59.86 & 33.24 \\ GNS & 8.16 & 47.95 & 17.54 & 11.36 & 60.49 & 33.72 \\ STGCN & 6.27 & 44.37 & 16.97 & 7.03 & 56.18 & 31.45 \\ MeshGraphNet & 5.13 & 42.05 & 14.65 & 6.54 & 52.96 & 28.63 \\ CG-ODE & 3.92 & 37.45 & 13.79 & 5.28 & 47.12 & 23.69 \\ TIE & 4.07 & 37.91 & 13.72 & 5.31 & 47.17 & 23.65 \\ Ours & **3.48** & **35.6** & **12.9** & **4.26** & **44.9** & **20.7** \\ \hline \multicolumn{6}{l}{_Airfoil_} \\ \hline LSTM & 8.75 & 8.84 & 3.96 & 12.86 & 18.74 & 4.78 \\ GNS & 8.14 & 8.02 & 3.51 & 11.52 & 11.44 & 4.04 \\ STGCN & 7.64 & 7.38 & 2.79 & 10.14 & 10.16 & 3.14 \\ MeshGraphNet & 6.05 & 6.19 & 1.27 & 7.11 & 7.09 & 1.43 \\ CG-ODE & 5.68 & 5.53 & 0.81 & 6.82 & 6.88 & 1.21 \\ TIE & 5.41 & 5.33 & 0.79 & 6.64 & 6.71 & 1.19 \\ Ours & **4.08** & **4.02** & **0.41** & **4.95** & **5.11** & **0.62** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Results on mesh-based physical simulations over different prediction lengths \(15\) and \(30\). \(v_{x}\), \(v_{y}\) and \(p\) represent the velocity in different directions and the pressure field, respectively.

Figure 5: More visualization of velocity in CylinderFlow dataset with varying time steps in {5, 150, 250, 350, 450}.

dynamics. To relieve this limitation, we present a novel graph-based ODE system named CARE for the modeling of interacting dynamics, which enriches the capabilities of making long-term predictions under potential environmental variation.

**Graph-based ODE.** Neural ODEs have been integrated into GNNs, resulting in the development of Graph ODEs that are applicable to both static and dynamic graphs. Graph ODEs on static graphs [60, 70] primarily aim to mitigate overfitting by formalizing derivatives using both initial and immediate node representations. Meanwhile, Graph ODEs on dynamic graphs are utilized for tasks such as traffic flow forecasting [9] and social analysis [19], demonstrating effective performance on irregularly sampled partial observation data. For example, STGODE [9] employs tensor computation to conduct continuous message passing, which facilitates accurate long-term predictions by overcoming the network depth limitations. Despite these advancements, existing works fall short of addressing the temporal environmental variation in interacting dynamics. To fill this gap, we propose a novel approach CARE to handle this problem by injecting a context variable in the Graph ODE system.

## Appendix I Potential Negative Impacts

To the best of our knowledge, we have not found any negative impact of our work.

Figure 6: More visualization of Lennard-Jones Potential with varying time steps in {5, 150, 250, 350, 450}.