# Sparse Backpropagation for MoE Training

 Liyuan Liu\({}^{\lx@sectionsign}\) Jianfeng Gao\({}^{\lx@sectionsign}\) Weizhu Chen\({}^{\ddagger}\)

\(\lx@sectionsign\)Microsoft Research \(\ddagger\)Microsoft Azure AI

{lucliu, jfgao, wzchen}@microsoft.com

###### Abstract

One defining characteristic of Mixture-of-Expert (MoE) models is their capacity for conducting sparse computation via expert routing, leading to remarkable scalability. However, backpropagation, the cornerstone of deep learning, requires dense computation, thereby posting challenges in MoE gradient computations. Here, we introduce SparseMixer, a scalable gradient estimator that bridges the gap between backpropagation and sparse expert routing. Unlike typical MoE training which strategically neglects certain gradient terms for the sake of sparse computation and scalability, SparseMixer provides scalable gradient approximations for these terms, enabling reliable gradient estimation in MoE training. Grounded in a numerical ODE framework, SparseMixer harnesses the mid-point method, a second-order ODE solver, to deliver precise gradient approximations with negligible computational overhead. Applying SparseMixer to Switch Transformer on both pre-training and machine translation tasks, SparseMixer showcases considerable performance gain, accelerating training convergence by up to 2 times.

## 1 Introduction

The significant success of large-scale pre-training across various applications has underscored the imperative need for scalable models that are economically feasible (Chowdhery et al., 2022; OpenAI, 2023; Touvron et al., 2023). Recent advances in sparsely activated networks, prominently known as Mixture-of-Experts (MoE), have attracted widespread interest (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2021; Riquelme et al., 2021; Mustafa et al., 2022). Unlike traditional networks that densely activate all modules for all input, MoE selectively activates parts of modules to specific inputs through a process called expert routing, leading to notable efficiency enhancements.

However, such efficiency gain comes at a cost: gradient estimation in MoE becomes challenging due to expert routing. Specifically, the routing function, being discrete in nature, produces non-differentiable outputs. Meanwhile, backpropagation, the cornerstone of deep learning, relies on the Chain rule, making it exclusively compatible with differentiable functions (Rosenblatt, 1957; Bengio et al., 2013), and cannot be directly applied for gradient computation of expert routing.

Numerous methods have emerged to bridge discrete and back-propagation, and most of them are based on Straight-Through (ST) (Rosenblatt, 1957; Bengio et al., 2013; Jang et al., 2017; Liu et al., 2023). Unfortunately, all existing ST estimators are incompatible with MoE, since they require activating all experts for gradient computing, thereby eliminating all the efficiency improvements of MoE. Consequently, typical MoE training strategically neglects the gradient computation for routing, trading certain training signals for sparse computation. Despite the scalability brought by sparse computation, this trade-off may result in slow convergence and improperly trained models.

Our solution to this quandary is SparseMixer--a novel approach designed to reconcile the divide between sparse MoE routing and backpropagation. Drawing inspiration from numerical methods for ordinary differential equations (ODE), SparseMixer provides reliable gradient approximationfor expert routing, even when only a subset of experts are activated. Moreover, to furnish accurate gradient approximations with negligible computation overheads, we integrate the mid-point method, a second-order numerical ODE solver, which matches the Taylor expansion of the gradient to the second order without requiring the Hessian matrix or other second-order derivatives.

We apply SparseMixer to Switch Transformer on both pretraining and neural machine translation. SparseMixer not only accelerates training convergence by up to two times but also facilitates MoE with properly trained expert routing. Remarkably, while Switch Transformer underperforms the dense model in all three pretraining settings, incorporating SparseMixer as the gradient estimator allows the resulting MoE models to consistently outperform the dense model.

## 2 Related Work and Preliminary

Mixture-of-Expert for Transformer.MoE originate from Jacobs et al. (1991) and Jordan and Jacobs (1994), which integrates separate networks together and uses each to handle a separate subset of training cases. Recently, many attempts have been made to leverage MoE for scaling large language models (Shazeer et al., 2017; Lepikhin et al., 2020; Lewis et al., 2021; Fedus et al., 2021).

To keep things straightforward, we will focus on a simplified setting of the switch Transformer layer (Fedus et al., 2021), and the resulting algorithm can be easily extended to other MoE designs. Considering a set of N experts, \(\{f_{i}(\mathbf{x})\}_{i=1}^{N}\), the gate value of expert \(i\) is computed with the softmax function as \(\mathbf{\pi}_{i}=\text{softmax}(\mathbf{\theta})_{i}=\frac{\exp(\mathbf{\theta}_{i})}{ \sum_{j=1}^{n}\exp(\mathbf{\theta}_{j})}\), where \(\mathbf{\theta}=W_{r}\cdot\mathbf{x}\). For \(i\in[1,\cdots,N]\), we mark its one-hot representation as \(\mathbf{I}_{i}\in\mathcal{R}^{N\times 1}\), whose element equals \(1\) if it is the \(i\)-th element or equals \(0\) otherwise. Let \(\mathbf{D}\) be a discrete random variable and \(\mathbf{D}\in\{\mathbf{I}_{1},\cdots,\mathbf{I}_{N}\}\). Then, the final output of this MoE layer is \(\mathbf{y}=\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})\). Note that \(\mathbf{D}\) is sampled as \(\mathbf{D}\sim\mathbf{\pi}\) during training, and is computed as \(\mathbf{D}\leftarrow\arg\max_{\mathbf{I}_{i}}\mathbf{\pi}_{\mathbf{I}_{i}}\) during inference. Marking other parts of the neural network as a differentiable function \(g:\mathcal{R}^{n}\rightarrow\mathcal{R}\), we aim to minimize:

\[\min_{W_{r}}\mathcal{L}(W_{r}),\ \text{ where }\mathcal{L}(W_{r})=E_{\mathbf{D} \sim\text{softmax}(W_{r}\cdot\mathbf{x})}[g(\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x}))]= \sum_{\mathbf{D}}\mathbf{\pi}_{\mathbf{D}}\cdot g(\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})). \tag{1}\]

First, we focus our discussions on this simplified MoE model. In Section 3.3, we will discuss its difference with the Switch Transformer and necessary adaptations.

Gradient Computation for Expert Routing.For simplicity, we mark \(\frac{\partial\mathcal{L}(W_{r})}{\partial W_{r}}\) as \(\nabla_{0}+\nabla_{1}\):

\[\frac{\partial\mathcal{L}}{\partial W_{r}}:=\nabla_{0}+\nabla_{1},\text{where }\nabla_{0}=\sum_{\mathbf{I}_{i}}g(\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{i}}(\mathbf{x})) \frac{\partial\mathbf{\pi}_{\mathbf{I}_{i}}}{\partial\,W_{r}}\text{ and }\nabla_{1}=\sum_{\mathbf{I}_{i}}\mathbf{\pi}_{\mathbf{I}_{i}}\frac{\partial g(\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{i}}(\mathbf{x}))}{\partial\,W_{r}}. \tag{2}\]

Figure 1: Training curves of Switch Transformer on WMTâ€™14 En-De.

It is easy to notice that \(\nabla_{1}\) can be computed reliably via backpropagation. \(\nabla_{0}\), however, is hard to reliably estimate in typical MoE training practice. In this study, we focus our discussions on \(\nabla_{0}\).

REINFORCE (Williams, 1992) is unbiased (i.e., \(E[\nabla_{\text{REINFORCE}}]=\nabla_{0}\)) and only requires the distribution of the discrete variable to be differentiable (i.e., no backpropagation through \(g\)):

\[\nabla_{\text{REINFORCE}}:=g(\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x}))\frac{\partial \,\log\mathbf{\pi}_{\mathbf{D}}}{\partial\,W_{r}}. \tag{3}\]

Despite the \(\nabla_{\text{REINFORCE}}\) estimator being unbiased, it tends to have prohibitively high variance, especially for networks that have other sources of randomness (i.e., dropout or other independent random variables). Recently, attempts have been made to reduce the variance of REINFORCE (Gu et al., 2016; Tucker et al., 2017; Grathwohl et al., 2018; Shi et al., 2022). Still, it has been found that the REINFORCE-style estimators fail to work well in MoE training (Kool et al., 2021).

Straight-Through.Despite \(\nabla_{\text{REINFORCE}}\) being unbiased, it treats the remaining network (\(g\)) as a black-box and only leverages the zero-order information of \(g\). In practice, a popular family of estimators, Straight-Through (ST), leveraging the first-order information of \(g\) (note that \(g\) is a scalar and \(g^{\prime}\) is a vector), has been shown to achieve a better performance in more complicated settings (Liu et al., 2023). ST computes the backpropagation "through" a surrogate that treats the non-differentiable function (e.g., the sampling of \(\mathbf{D}\)) as an identity function (Rosenblatt, 1957; Bengio et al., 2013; Jang et al., 2017; Liu et al., 2023). In our MoE setting, ST treats the sampling of \(\mathbf{D}\) as an identity function and estimates the gradient as:

\[\widehat{\nabla}_{\text{ST}}:=\frac{\partial g(\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\bm {x}))}{\partial\,\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})}\frac{\partial\sum_{i}\bm {D}_{i}\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{i}}(\mathbf{x})}{\partial\mathbf{D}}\frac{\partial \mathbf{\pi}_{\mathbf{D}}}{\partial W_{r}}. \tag{4}\]

An alternative strategy is to conduct the concrete random variable relaxation (Maddison et al., 2014; Jang et al., 2017). It is observed that the sampling of \(\mathbf{D}\) can be reparameterized using Gumbel random variables at the zero-temperature limit of the tempered softmax (Gumbel, 1954):

\[\mathbf{D}=\lim_{\tau\to 0}\mathbf{S}_{\tau},\text{ where }\mathbf{S}_{\tau}=\text{softmax}_{\tau}(\mathbf{\theta}+\mathbf{G}),\mathbf{G}_{i}\text{ are i.i.d., and }\mathbf{G}_{i}\sim\text{Gumbel}(0,1).\]

Straight-Through Gumbel-Softmax (STGS) treats the zero-temperature limit as identity function during the backpropagation:

\[\widehat{\nabla}_{\text{STGS}}:=\frac{\partial g(\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}( \mathbf{x}))}{\partial\,\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})}\frac{\partial\sum_{i} \mathbf{S}_{\tau,i}\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{i}}(\mathbf{x})}{\partial\mathbf{S}_{ \tau}}\frac{\partial\mathbf{S}_{\tau}}{\partial W_{r}}. \tag{5}\]

Although \(E[\widehat{\nabla}_{\text{ST}}]\) has been formally established as a first-order approximation of \(\nabla_{0}\)(Liu et al., 2023), applying ST estimators necessitates the need for computing \(f_{i}(\mathbf{x})\) for all \(i\in\{\mathbf{I}_{1},\cdots,\mathbf{I}_{N}\}\), i.e., the outputs from all experts. For example, in Equation 4, we have \(\frac{\partial\sum_{i}\mathbf{D}_{i}\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{i}}(\mathbf{x})}{ \partial\mathbf{D}}=\text{diag}(\sum_{i}\mathbf{D}_{i}\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{ i}}(\mathbf{x}))\), which involves the computation of \(\{f_{\mathbf{I}_{i}}(\mathbf{x}),\cdots,f_{I_{N}}(\mathbf{x})\}\). Essentially, computing all \(f_{\mathbf{I}_{i}}\) turns MoE into a densely activated network. Thus, using ST-style estimators undermines the sparse computation, fundamentally obstructing the scaling of MoE models.

## 3 Scalable Gradient Approximation

As discussed in Section 2, although ST estimators bridged discrete variables and backpropagation, they require the network to be densely activated. Here, we first discuss the intrinsic limitation of ST estimators. Then, we go beyond ST and bridge sparse expert routing and backpropagation. Finally, we revisit the current practice of MoE training and discuss the difference between the Switch Transformer and the simplified setting (as presented in Section 2).

### Why Existing ST Estimators Are Not Scalable?

We formally establishes that \(E[\widehat{\nabla}_{\text{ST}}]\) is a first-order approximation of \(\nabla_{0}\) in Liu et al. (2023) (note that \(\nabla_{0}\) is defined in Equation 2). Since \(\mathbf{D}_{i}=1\iff\mathbf{D}=\mathbf{I}_{i}\), we can reparameterizing \(\mathbf{\pi}_{\mathbf{D}}f_{\mathbf{D}}\) as \(h(\mathbf{D})=\sum_{i}\mathbf{D}_{i}\mathbf{\pi}_{\mathbf{I}_{i}}f_{\mathbf{I}_{i}}\). Then, we have1:

Footnote 1: Commonly referred to as baseline subtraction. Note \(\sum_{i}E[g]\frac{\partial\,\mathbf{\pi}_{\mathbf{I}_{i}}}{\partial\,W_{r}}=E[g]\frac{ \partial\,\Sigma_{\mathbf{I}_{i}}\,\mathbf{\pi}_{\mathbf{I}_{i}}}{\partial\,W_{r}}=E[g] \frac{\partial\,\mathbf{1}}{\partial\,W_{r}}=0\).

\[\nabla_{0}=\sum_{\mathbf{I}_{i}}(h(\mathbf{I}_{i})-E[h])\frac{\partial\,\mathbf{\pi}_{\mathbf{ I}_{i}}}{\partial\,W_{r}}=\sum_{\mathbf{I}_{i}}\sum_{\mathbf{I}_{j}}\mathbf{\pi}_{\mathbf{I}_{j}}(h( \mathbf{I}_{i})-h(\mathbf{I}_{j}))\frac{\partial\,\mathbf{\pi}_{\mathbf{I}_{i}}}{\partial\,W_{ r}}. \tag{6}\]Specifically, approximating \(g(\mathbf{\pi_{I_{i}}}f_{I_{i}})-g(\mathbf{\pi_{I_{j}}}f_{I_{j}})\) as \(g^{\prime}(\mathbf{\pi_{I_{j}}}f_{I_{j}})\cdot(\mathbf{\pi_{I_{i}}}f_{I_{i}}-\mathbf{\pi_{I_{ j}}}f_{I_{j}})\), the resulting gradient approximation will have the same form as \(E[\widehat{\nabla}_{\text{ST}}]\)(Liu et al., 2023). In numerical analyses, this approximation is known as the forward Euler method (briefly introduced in Appendix A), which has first-order accuracy. Liu et al. (2023) also explored higher-order ODE solvers to better approximate \(g(\mathbf{\pi_{I_{i}}}f_{I_{i}})-g(\mathbf{\pi_{I_{j}}}f_{I_{j}})\). However, all these approximations involve both activated experts (i.e., \(f_{I_{j}}\)) and unactivated experts (i.e., \(f_{I_{j}}\)), thus contradicting scalability. In order words, although those ST estimators bridge discrete and backpropagation, their computations are dense instead of sparse.

### Expert Routing Gradient Approximation: Backpropagation Made Sparse

To bridge the gap between sparse MoE routing and back-propagation, we need to approximate \(\nabla_{0}\) without requiring outputs from all experts. In our study, we move beyond ST and present a novel framework to bridge backpropagation and sparse expert routing.

Here, we start by introducing the most simple gradient estimator, i.e., \(\widehat{\nabla}_{\text{SparseMixer\text{\text{-1st}}}}\), where

\[\widehat{\nabla}_{\text{SparseMixer\text{\text{-1st}}}}:=\frac{\partial g(\pi_{ D}f_{D}(\mathbf{x}))}{\partial\,W_{r}}.\]

Similar to \(E[\widehat{\nabla}_{\text{ST}}]\), \(E[\widehat{\nabla}_{\text{SparseMixer\text{\text{-1st}}}}]\) is a first-order approximation of \(\nabla_{0}\). To demonstrate this, we take an alternative approach to rewrite \(\nabla_{0}\):

\[\nabla_{0}=\sum_{\mathbf{I_{i}}}(g(\pi_{\mathbf{I_{i}}}f_{\mathbf{I_{i}}})-g(\mathbf{0}))\frac {\partial\,\mathbf{\pi_{I_{i}}}}{\partial\,W_{r}}. \tag{7}\]

Note that \(g(\mathbf{0})\) is only used as a vehicle for derivations. It is worth mentioning that, unlike the baseline used in Equation 4 (i.e., \(E[h]\), which has been shown to be the optimal control variate in Weaver & Tao, 2001), it is not a suitable to use \(g(\mathbf{0})\) as the control variate for policy gradient.

Adopting the Euler method to Equation 7, we estimate \(g(\mathbf{\pi_{I_{i}}}f_{\mathbf{I_{i}}})-g(\mathbf{0})\) as \(g^{\prime}(\mathbf{\pi_{I_{i}}}f_{\mathbf{I_{i}}})\cdot\mathbf{\pi_{I_{i}}}f_{\mathbf{I_{i}}}\). Comparing to the first-order approximation of Equation 6, this first-order approximation only requires the output of one expert. Then, it is easy to note:

\[\nabla_{0}\overset{\text{forward Euler}}{\approx}\sum_{\mathbf{I_{i}}}g^{\prime}( \mathbf{\pi_{I_{i}}}f_{\mathbf{I_{i}}})\cdot\mathbf{\pi_{I_{i}}}f_{\mathbf{I_{i}}}\cdot\frac{ \partial\,\mathbf{\pi_{I_{i}}}}{\partial\,W_{r}}=E_{\mathbf{D}\sim\mathbf{\pi}}[\frac{ \partial g(\pi_{D}f_{D}(\mathbf{x}))}{\partial\,W_{r}}]=E[\widehat{\nabla}_{\text{ SparseMixer\text{\text{-1st}}}}].\]

Note that, same with \(\widehat{\nabla}_{\text{ST}}\), \(\widehat{\nabla}_{\text{SparseMixer\text{\text{-1st}}}}\) adopts the forward Euler method and achieves first-order accuracy. Meanwhile, \(\widehat{\nabla}_{\text{SparseMixer\text{\text{-1st}}}}\) only requires the output of one expert thus not sacrificing scalability, while \(\widehat{\nabla}_{\text{ST}}\), as in Equation 4, requires the output of all experts.

### Understand Current MoE Training Practice in Simplified Setting

Besides providing sound gradient approximation with negligible computation overheads, our study also sheds insights into the underlying mechanism of the current MoE training practice. In this section, we introduce the current MoE training practice with our simplied setting. Then, in Section 3.4, we further discuss MoE training in the realistic Switch Transformer setting.

**Current MoE Training Practice.**  Due to all the challenge discussed in Section 3.1, the current MoE training practice trades certain training signals for scalability. Specifically, \(\nabla_{0}\) is strategically neglected in gradient computation (the value of \(\nabla_{0}\) is set to 0), and only \(\nabla_{1}\) is used for model training (Fedus et al., 2021). Despite the success of such practice, it remains unclear on the impact of neglecting \(\nabla_{0}\), how to conduct training with only part of the gradient, and whether gradient descent is still effective after neglecting \(\nabla_{0}\).

**Underlying Mechanism of Current MoE Training Practice.**  Comparing Equation 7 and Equation 2, we can observe that \(\widehat{\nabla}_{\text{SparseMixer\text{\text{-1st}}}}\) has the same form with \(\nabla_{1}\), which implies:

\[\nabla\overset{\text{forward Euler}}{\approx}2\cdot\nabla_{1}.\]

Therefore, in our simplified setting, directly dropping the \(\nabla_{0}\) can be viewed as down-scaling \(\nabla\) by \(0.5\). Since typical training practice employs adaptive optimizers for model training, whose update rule is invariant to constant gradient scaling, our observation here provides a natural explanation on the effectiveness of current MoE training.

### From Simplified Setting to Switch Transformer: Expert Sampling

As mentioned in Section 2, our modeling of MoE is a simplified Switch Transformer. Here, we first discuss the difference between our simplified setting and Switch Transformer, and then move to necessary modifications to apply SparseMixer to Switch Transformer.

Difference between Simplified Setting and Switch Transformer.The difference between our simplified setting and switch Transformer is the sampling of \(\mathbf{D}\). Specifically, in our simplified setting, we assume \(\mathbf{D}\) is sampled from \(\mathbf{\pi}\); in Switch Transformer, \(\mathbf{D}\) is sampled as Equation 8 instead.

\[\mathbf{D}=\operatorname*{arg\,max}_{\mathbf{I}_{i}}(\mathbf{\theta}_{\mathbf{I}_{i}}\cdot u_{ \mathbf{I}_{i}}),\text{ where }u_{\mathbf{I}_{i}}\overset{\text{iid}}{\sim}\text{Uniform}(1-r,1+r). \tag{8}\]

With this sampling strategy, \(\nabla_{1}\) is no longer a first-order approximation to \(\nabla_{0}\) (it can be viewed as conducting importance sampling without applying the likelihood ratio).

An Important Property of Expert Sampling in Switch Transformer.To obtain sound gradient estimation with a strong performance, it is necessary to adapt the sampling process of expert networks. As observed in Fedus et al. (2021), directly sampling \(\mathbf{D}\) from \(\mathbf{\pi}\) leads to notable performance degradation (also discussed in Section 5.3). In our study, we observe an important property of the sampling process used in the Switch Transformer setting and suggest it to be the major issue with the original softmax sampling.

Marking \(\mathbf{\theta}^{*}:=\max_{\mathbf{I}_{i}}\mathbf{\theta}_{\mathbf{I}_{i}}\), in Switch Transformer, \(\mathbf{I}_{i}\) will never be sampled if:

\[\mathbf{\theta}^{*}-\mathbf{\theta}_{\mathbf{I}_{i}}>r\cdot(|\mathbf{\theta}^{*}|+|\mathbf{\theta} _{\mathbf{I}_{i}}|).\]

In other words, the distribution of \(\mathbf{D}\) in switch Transformer is masked: small probabilities would directly drop to zero once the corresponding logits hit a threshold. In our experiments, we observe that such sparse distribution plays a crucial role in the success of MoE and conduct more empirical discussions in the experiment section (Section 5.3).

Adapting Expert Sampling for MoE Training.Guided by our analyses, we deploy a sampling process that is differentiable as sampling from \(\mathbf{\pi}\), while sharing some important properties with Switch Transformer. Specifically, we changed the computation of \(\mathbf{\pi}\) from \(\mathbf{\pi}_{i}=\text{softmax}(\mathbf{\theta})_{i}=\frac{\exp(\mathbf{\theta}_{i})}{\sum_ {j=1}^{n}\exp(\mathbf{\theta}_{j})}\) to

\[\mathbf{\pi}_{i}=\frac{\exp(\mathbf{\theta}_{i})\cdot\Delta_{i}}{\sum_{j=1}^{n}\exp(\bm {\theta}_{j})\cdot\Delta_{j}},\text{ where }\Delta_{j}=\delta(\mathbf{\theta}^{*}-\mathbf{ \theta}_{\mathbf{I}_{i}}\leq r\cdot(|\mathbf{\theta}^{*}|+|\mathbf{\theta}_{\mathbf{I}_{i}}|)). \tag{9}\]

In other words, we apply a mask to the softmax function, in order to sample only from experts that are not masked by the Switch Transformer. This adaptation allows MoE to be trained with both sparse expert sampling and sound gradient approxiamtion, we observe it leads to a significant performance boost.

Empirical Benefits on Expert Sampling Adaptation.As elaborated in Section 5.3, we conduct comparisons with Switch and SparseMixer-ablation-2. Both are based on the first-order approximation as discussed in Section 3.3. The difference between these two are:

* SparseMixer-ablation-2 conducts expert sampling as in Equation 9 and uses a first-order approximation of \(\nabla_{0}\) for parameter updates.
* Switch conducts expert sampling as in Equation 8, downscales the routing gradient by 0.5 (as in Section 3.3), thus adding additional bias to the first-order approximation.

As in Figure 1, the SparseMixer-ablation-2 method achieves consistent performance gain to Switch Transformer. applies the abovementioned sampling process to Switch Transformer, and.(see Section 5.3 for more details).

## 4 Towards Second-Order Accuracy with Negligible Overheads

The literature on numerical methods for differential equations shows that it is possible to achieve higher-order accuracy _without computing higher-order derivatives_. Correspondingly, we aim to provide better gradient approximation with negligible computation overheads.

### Achieving Second-Order Accuracy with the Mid-point Method

To furnish accurate gradient approximations, we employ a second-order ODE method, the mid-point method (briefly introduced in Appendix A). Specifically, \(\widehat{\nabla}_{\text{SparseMixer-2rd}}\) is a second-order approximation of \(\nabla\), where

\[\widehat{\nabla}_{\text{SparseMixer-2rd}}\,:=2\cdot\frac{\partial g(\frac{\pi_{ L_{I}}f_{D}(\mathbf{x})}{2})}{\partial\,W_{r}}.\]

To demonstrate the connection between \(\widehat{\nabla}_{\text{SparseMixer-2nd}}\) and the mid-point method, we employ the mid-point method to approximate \(g(\pi_{I_{i}}f_{I_{i}})-g(\mathbf{0})\) as \(g^{\prime}(\frac{\pi_{I_{i}}f_{I_{i}}}{2})\cdot\pi_{I_{i}}f_{I_{i}}\), which also requires only the output of one expert. Similarly, it is easy to note:

\[\nabla_{0}\stackrel{{\text{mid-point}}}{{\approx}}\sum_{\mathbf{I}_{i }}g^{\prime}(\frac{\pi_{I_{i}}f_{I_{i}}}{2})\cdot\mathbf{\pi}_{I_{i}}f_{I_{i}}\cdot \frac{\partial\,\mathbf{\pi}_{I_{i}}}{\partial\,W_{r}}=E_{\mathbf{D}\sim\pi}[2\cdot \frac{\partial g(\frac{\pi_{D}f_{D}(\mathbf{x})}{2})}{\partial\,W_{r}}]=E[\widehat {\nabla}_{\text{SparseMixer-2rd}}\,].\]

Notably, it is feasible to employ more advanced ODE solvers like RKF4 and approximate \(\nabla_{0}\) with even higher-order accuracy (Fehlberg, 1969). In our experiments, we observe that the mid-point method is accurate enough and decide to stick to the mid-point method for simplicity.

### Balancing Router Training and Expert Training

Trade-off Behind Applying the Mid-Point Method.Comparing to \(\widehat{\nabla}_{\text{SparseMixer\,-1st}}\), \(\widehat{\nabla}_{\text{SparseMixer-2rd}}\) provides better gradient estimation for router training. However, \(\widehat{\nabla}_{\text{SparseMixer-2rd}}\) causes additional difficulties for expert training.

Specifically, \(\widehat{\nabla}_{\text{SparseMixer-2rd}}\) requires to change the MoE output from \(\mathbf{y}\leftarrow\pi_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})\) to \(\mathbf{y}\leftarrow\frac{\pi_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})}{2}\). Intuitively, this change leads to two gaps:

1. A gap between the training (\(\mathbf{y}\leftarrow\frac{\pi_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})}{2}\)) and the inference (\(\mathbf{y}\leftarrow\pi_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})\)).
2. A gap between estimating \(\nabla_{0}\) (\(\mathbf{y}\leftarrow\frac{\pi_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})}{2}\)) and \(\nabla_{1}\) (\(\mathbf{y}\leftarrow\pi_{\mathbf{D}}f_{\mathbf{D}}(\mathbf{x})\)).

In other words, applying mid-point method would lead to a better approximation of \(\nabla_{0}\), at the cost of additional bias in computing \(\nabla_{1}\). Similarly, as discussed in Section 5.4, such gap creates significant obstacles for MoE training.

Hybrid Gradient Estimation: SparseMixer.We notice that \(\mathbf{D}\) is assigned as \(\mathbf{D}\leftarrow\arg\max_{\mathbf{I}_{i}}\mathbf{\pi}_{I_{i}}\) during the inference, instead of being sampled from \(\mathbf{\pi}\). Thus, it would be sufficient to close the gap by only applying \(\widehat{\nabla}_{\text{SparseMixer-2rd}}\) when \(\mathbf{D}\neq\arg\max_{\mathbf{I}_{i}}\mathbf{\pi}_{I_{i}}\). Accordingly, we propose SparseMixer to balance router training and expert training:

\[\widehat{\nabla}_{\text{SparseMixer}}\,:=(1-\delta_{\mathbf{D}})\widehat{\nabla}_ {\text{SparseMixer-2rd}}\,+\delta_{\mathbf{D}}\widehat{\nabla}_{\text{SparseMixer-1st }}\,,\text{where }\delta_{\mathbf{D}}=\begin{cases}1,&\text{if }\mathbf{D}=\arg\max_{\mathbf{I}_{i}}\mathbf{\pi}_{\mathbf{I}_{i}}\\ 0,&\text{otherwise}\end{cases}.\]

Additional Sampling Adaptation.Since the value of \(\mathbf{\pi}\) will be different after applying the mask (which impacts the gradient magnitude of other components), we further changed the output of the MoE layer from \(\mathbf{\pi}_{\mathbf{D}}\cdot f_{\mathbf{D}}(\mathbf{x})\) to \(\mathbf{\omega}\cdot\mathbf{\pi}_{\mathbf{D}}\cdot f_{\mathbf{D}}(\mathbf{x})\), where \(\mathbf{\omega}\) is trainable and is initialized as the \(\mathbf{1}\) vector. Intuitively, \(\mathbf{\omega}\) can be viewed as an adaptation on the learning rate for training expert networks. Note that, \(\mathbf{\omega}\) can be re-parameterized into the feedforward layer after training.

Computational Efficiency of SparseMixer.\(\widehat{\nabla}_{\text{SparseMixer}}\) does not require Hessian or other second-order derivatives, thus having negligible computation overheads. Also, it is worth mentioning that \(\widehat{\nabla}_{\text{SparseMixer}}\) has the same order of computation complexity and memory complexity with only estimating \(\nabla_{1}\) (i.e., the current practice of MoE training neglects \(\nabla_{0}\) directly). Empirical verification is discussed in Section 5.5, which matches our analyses here.

At the same time, similar to \(\widehat{\nabla}_{\text{ST}}\), our proposed algorithm can be easily integrated with popular library like PyTorch, making it easy to be integrated with existing algorithms.

Experiments

### Experiment Setting

Here, we conduct experiments on pretraining and neural machine translation. We closely follow the experiment setting of the existing study. Due to the constraint of computation resources, we left MoE related hyper-parameters untuned in all settings, i.e., jitter (\(r\)) is set to 0.1 and load balance loss ratio is set to 0.01 (Fedus et al., 2021). Detailed configurations are elaborated in Appendix B.

### Applying SparseMixer on Switch Transformer

NMT on WMT'14 En-De.We visualized the training curve in Figure 1 and summarized the BLEU score in Table 1. Regarding both convergence speed and the final performance, Switch+SparseMixer consistently outperforms Switch in all five settings. Notably, Switch+SparseMixer matches the training performance of Switch with about _50% less training updates when \(N\in\{4,6,8\}\)_ and about _40% less training updates when \(N\in\{2,16\}\)_.

We can observe that, with more experts, MoE models achieve lower training loss with a worse BLEU score. Specifically, although Switch Transformer achieves better training performance, its final performance (BLEU score) never outperforms the Dense model, regardless of how many experts it has. We believe it requires more data to fully unleash the potential of MoE and suggest this phenomenon indicates that MoE models are prone to overfitting (Zuo et al., 2022).

Meanwhile, without changing hyper-parameters or model architectures, the downstream performance of Switch + SparseMixer outperforms both Dense and Switch, when \(N\in\{2,4\}\). Specifically, SparseMixer improves the performance of Switch from 28.17 to 28.72 (when \(N=2\)) and from 28.05 to 28.61 (when \(N=4\)). This phenomenon implies that, with the help of SparseMixer, a sound gradient estimator, MoE learns an expert routing that generalizes better.

Pretraining.Following previous work (Dong et al., 2023), we visualized the training curve in Figure 6 and summarized the fine-tuning results in Table 2. Regarding both convergence speed and downstream performance, Switch+SparseMixer consistently outperforms Switch in all settings. Also, similar to the experiments on machine translation, we observe that MoE models are easier to overfit and both settings achieve the best downstream performance with two experts.

Also, it is worth mentioning that, while Switch Transformer only outperforms the dense model when the number of experts is set to 2, Switch + SparseMixer consistently outperforms the Dense model in all four settings. This phenomenon further verifies our intuition that SparseMixer facilitates MoE models with better expert router training, thus having the resulting model to generalize better.

### Discussions

Here, we conduct experiments to discuss our modeling of the MoE layer as in Section 2.

Importance of Scaling Expert Outputs with Gating Networks.One important design detail of MoE is to scale the output of the expert network with the gating network, i.e., the output of the MoE layer is computed as \(\mathbf{y}\leftarrow\mathbf{\pi}_{D}f_{D}(\mathbf{x})\), instead of \(\mathbf{y}\gets f_{D}(\mathbf{x})\). This scaling design greatly facilitates the derivation of SparseMixer in Section 3, and inspires the introduction of \(\mathbf{\omega}\) (further discussed in Section 5.4). Here, we empirically examine the importance of this scaling design.

Specifically, we conduct experiments with a variant of Switch Transformer, i.e., Switch w.o. Scaling, which sets the output of the MoE layer as \(\mathbf{y}\gets f_{D}(\mathbf{x})\). We apply this Switch variant on

\begin{table}
\begin{tabular}{l|c|c c c c c} \hline \hline  & Dense & \multicolumn{5}{c}{Mixture-of-Expert} \\  & \(N=2\) & \(N=4\) & \(N=6\) & \(N=8\) & \(N=16\) \\ \hline Transformer-base & 28.33 & / & / & / & / & / \\ Switch & / & 28.17 & 28.05 & 27.96 & 27.99 & 27.81 \\ Switch+SparseMixer & / & **28.72** & **28.61** & **28.32** & **28.12** & **28.08** \\ \hline \hline \end{tabular}
\end{table}
Table 1: BLEU score on WMTâ€™14 En-De (\(N\) refers to the number of experts).

[MISSING_PAGE_FAIL:8]

* ablation-2 further replaces the mid-point method with the forward Euler method in SparseMixerablation-1, i.e., \(\widehat{\nabla}_{\text{SparseMixer-1st}}\) is employed as the gradient estimator and \(\mathbf{\omega}\) is removed.

We apply these two variants to WMT'14 En-De. As in Figure 1, both variants outperform the baseline. The results further verified our intuition that \(\mathbf{\omega}\) facilitates MoE training by alleviating the impact of applying masks. Also, it shows that integrating the mid-point method helps to better approximate expert routing gradient.

### Efficiency

We summarized the average time cost per update in Table 3. Switch+SparseMixer achieves an identical average time cost with Switch in all eight settings. This shows that the computation overheads of SparseMixer are negligible.

To better understand the computation overhead brought by SparseMixer, we compute the floating point operations (FLOPS) for one forward propagation and one backward propagation. We visualized the FLOPS ratio of Switch and Switch+SparseMixer of various number of experts in Figure 5. It shows the computation overhead brought by SparseMixer only composes up to 0.1% of the total training FLOPS, for MoE models with up to 16384 experts.

## 6 Conclusion

In this study, we present SparseMixer to move beyond ST and bridge the gap between sparse MoE routing and backpropagation. Rooted in a numerical ODE framework, SparseMixer harnesses the mid-point method, a second-order ODE solver, to deliver precise gradient approximations with negligible computational overhead. In our experiments on both neural machine translation and pre-training tasks, SparseMixer not only accelerates training convergence by up to two times but also facilitates MoE with properly trained expert routing. Remarkably, while Switch Transformer underperforms the dense model in all three pretraining settings, incorporating SparseMixer as the gradient estimator allows the resulting MoE models to consistently outperform the dense model.

## Acknowledgement

We would like to thank all reviewers for their constructive comments, the engineering team at Microsoft for providing computation infrastructure support, Alessandro Sordoni, Xiaodong Liu, Jianwei Yang, and Chunyuan Li for their helpful discussions.

Figure 4: Comparison between _SparseMixer_ and _SparseMixer-2rd_.

Figure 5: The ratio of Switch Training FLOPS and Switch+SparseMixer Training FLOPS. The FLOPS are computed for ELECTRA-base training with one 512-token sequence.

\begin{table}
\begin{tabular}{l|c c c c c|c c c} \hline \hline  & \multicolumn{4}{c|}{WMTâ€™14 En-De} & \multicolumn{4}{c}{LM Pre-training} \\  & \(N=2\) & \(N=4\) & \(N=6\) & \(N=8\) & \(N=16\) & \(N=2\) & \(N=4\) & \(N=8\) \\ \hline Switch & 0.32 & 0.33 & 0.34 & 0.36 & 0.40 & 1.87 & 1.90 & 1.98 \\ \hline Switch + SparseMixer & 0.32 & 0.33 & 0.34 & 0.36 & 0.40 & 1.87 & 1.90 & 1.98 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average Training Time Cost (s/update). \(N\) refers to the number of experts.

## References

* Ascher and Petzold (1998) Uri M. Ascher and Linda R. Petzold. _Computer methods for ordinary differential equations and differential-algebraic equations_. 1998.
* Bajaj et al. (2022) Payal Bajaj, Chenyan Xiong, Guolin Ke, Xiaodong Liu, Di He, Saurabh Tiwary, Tie-Yan Liu, Paul Bennett, Xia Song, and Jianfeng Gao. Metro: Efficient denoising pretraining of large scale autoencoding language models with model generated signals. _ArXiv_, abs/2204.06644, 2022.
* Bengio et al. (2013) Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. _ArXiv_, abs/1308.3432, 2013.
* Bojar et al. (2014) Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In _Workshop on Statistical Machine Translation_, 2014.
* Chowdhery et al. (2015) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunjap Lee, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. _ArXiv_, abs/2204.02311, 2022.
* Clark et al. (2020) Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. Electra: Pre-training text encoders as discriminators rather than generators. In _ICLR_, 2020.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _NAACL_, 2019.
* Dong et al. (2023) Chengyu Dong, Liyuan Liu, Hao Cheng, Jingbo Shang, Jianfeng Gao, and Xiaodong Liu. Understand and modularize generator optimization in electra-style pretraining. In _ICML_, 2023.
* Fedus et al. (2021) William Fedus, Barret Zoph, and Noam M. Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _ArXiv_, abs/2101.03961, 2021.
* Fehlberg (1969) Erwin Fehlberg. Classical fifth-and seventh-order runge-kutta formulas with stepsize control. _Computing_, 1969.
* Grathwohl et al. (2018) Will Grathwohl, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Kristjanson Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. In _ICLR_, 2018.
* Gu et al. (2016) Shixiang Shane Gu, Sergey Levine, Ilya Sutskever, and Andriy Mnih. Muprop: Unbiased backpropagation for stochastic neural networks. In _ICLR_, 2016.
* Gumbel (1954) Emil Julius Gumbel. _Statistical Theory of Extreme Values and Some Practical Applications : A Series of Lectures_. 1954.
* He et al. (2020) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. _ArXiv_, abs/2006.03654, 2020.
* Jacobs et al. (1991) Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. _Neural Computation_, 3:79-87, 1991.
* Jang et al. (2017) Eric Jang, Shixiang Shane Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In _ICLR_, 2017.
* Jang et al. (2018)Michael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the em algorithm. _Neural Computation_, 6:181-214, 1994.
* Kool et al. (2021) Wouter Kool, Chris J. Maddison, and Andriy Mnih. Unbiased gradient estimation with balanced assignments for mixtures of experts. In _I (Still) Can't Believe It's Not Better Workshop at NeurIPS 2021_, 2021.
* Lepikhin et al. (2020) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam M. Shazeer, and Z. Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. _ArXiv_, abs/2006.16668, 2020.
* Lewis et al. (2021) Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In _ICML_, 2021.
* Liu et al. (2020a) Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. In _ICLR_, 2020a.
* Liu et al. (2020b) Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the difficulty of training transformers. In _EMNLP_, 2020b.
* Liu et al. (2023) Liyuan Liu, Chengyu Dong, Xiaodong Liu, Bin Yu, and Jianfeng Gao. Bridging discrete and back-propagation: Straight-through and beyond. _ArXiv_, abs/2304.08612, 2023.
* Liu et al. (2019) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. In _ACL_, 2019.
* Lu et al. (2020) Yiping Lu, Zhuohan Li, Di He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and Tie-Yan Liu. Understanding and improving transformer from a multi-particle dynamic system point of view. In _ICLR Workshop DeepDiffEq_, 2020.
* Maddison et al. (2014) Chris J. Maddison, Daniel Tarlow, and Thomas P. Minka. A* sampling. In _NIPS_, 2014.
* Mustafa et al. (2022) Basil Mustafa, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. Multimodal contrastive learning with limoe: the language-image mixture of experts. _ArXiv_, abs/2206.02770, 2022.
* OpenAI (2023) OpenAI. Gpt-4 technical report. _ArXiv_, abs/2303.08774, 2023.
* Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In _NAACL-HLT Demonstrations_, 2019.
* Raffel et al. (2019) Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _ArXiv_, abs/1910.10683, 2019.
* Riquelme et al. (2021) Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In _NeurIPS_, 2021.
* Rosenblatt (1957) Frank Rosenblatt. _The perceptron, a perceiving and recognizing automaton Project Para_. Cornell Aeronautical Laboratory, 1957.
* Shazeer et al. (2017) Noam M. Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In _ICLR_, 2017.
* Shi et al. (2022) Jiaxin Shi, Yuhao Zhou, Jessica Hwang, Michalis Titsias, and Lester Mackey. Gradient estimation with discrete stein operators. In _NeurIPS_, 2022.
* Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _CVPR_, 2016.
* Szegedy et al. (2015)Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajijwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Eisobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. _ArXiv_, abs/2307.09288, 2023.
* Tucker et al. (2017) G. Tucker, Andriy Mnih, Chris J. Maddison, John Lawson, and Jascha Narain Sohl-Dickstein. Rebar: Low-variance, unbiased gradient estimates for discrete latent variable models. In _NIPS_, 2017.
* Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In _BlackboxNLP@EMNLP_, 2018.
* Weaver and Tao (2001) Lex Weaver and Nigel Tao. The optimal reward baseline for gradient-based reinforcement learning. In _UAI_, 2001.
* Williams (1992) Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8:229-256, 1992.
* Zhu et al. (2015) Yukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _ICCV_, 2015.
* Zuo et al. (2022) Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Tuo Zhao, and Jianfeng Gao. Taming sparsely activated transformer with stochastic experts. In _ICLR_, 2022.

## Appendix A Forward Euler Method and Heun's Method

For simplicity, we consider a simple function \(g(x):\mathcal{R}\rightarrow\mathcal{R}\) that is three times differentiable on \([t_{0},t_{1}]\). Now, we proceed to a simple introduction to approximate \(\int_{t_{0}}^{t_{1}}g^{\prime}(x)dx\) with the Forward Euler Method and the Heun's Method. For a detailed introduction to numerical ODE methods, please refer to Ascher & Petzold (1998).

**Forward Euler Method**. Here, we approximate \(g(t_{1})\) with the first-order Taylor expansion, i.e., \(g(t_{1})=g(t_{0})+g^{\prime}(t_{0})\cdot(t_{1}-t_{0})+O((t_{1}-t_{0})^{2})\), then we have \(\int_{t_{0}}^{t_{1}}g^{\prime}(x)dx\approx g^{\prime}(t_{0})(t_{1}-t_{0})\). Since we used the first-order Taylor expansion, this approximation has first-order accuracy.

**Heun's Method**. First, we approximate \(g(t_{1})\) with the second-order Taylor expansion:

\[g(t_{1})=g(t_{0})+g^{\prime}(t_{0})\cdot(t_{1}-t_{0})+\frac{g^{\prime\prime}(t_ {0})}{2}\cdot(t_{1}-t_{0})^{2}+O((t_{1}-t_{0})^{3}). \tag{10}\]

Then, we show that we can match this approximation by combining the first-order derivatives of two samples. Taylor expanding \(g^{\prime}(\frac{t_{1}+t_{0}}{2})\) to the first-order, we have:

\[g^{\prime}(\frac{t_{1}+t_{0}}{2})=g^{\prime}(t_{0})+g^{\prime\prime}(t_{0}) \cdot\frac{t_{1}-t_{0}}{2}+O((t_{1}-t_{0})^{2})\]

Therefore, we have:

\[g(t_{0})+g^{\prime}(\frac{t_{1}+t_{0}}{2})(t_{1}-t_{0})=g(t_{0})+g^{\prime}(t_{0} )\cdot(t_{1}-t_{0})+\frac{g^{\prime\prime}(t_{0})}{2}\cdot(t_{1}-t_{0})^{2}+O(( t_{1}-t_{0})^{3}).\]It is easy to notice that the right-hand side of the above equation matches the second-order Taylor expansion of \(g(t_{1})\) as in Equation 10. Therefore, the above approximation (i.e., approximating \(g(t_{1})-g(t_{0})\) as \(g^{\prime}(\frac{t_{1}+t_{0}}{2})(t_{1}-t_{0})\)) has second-order accuracy.

**Connection to \(f(\mathbf{I}_{i})-f(\mathbf{0})\).** By setting \(g(x)=f(x\cdot\mathbf{I}_{i})\), we have \(g(1)-g(0)=f(\mathbf{I}_{i})-f(\mathbf{0})\). Then, it is easy to notice that the forward Euler Method approximates \(f(\mathbf{I}_{i})-f(\mathbf{0})\) as \(\frac{\partial f(\mathbf{I}_{i})}{\partial\mathbf{I}_{i}}\mathbf{I}_{i}\) and has first-order accuracy. Also, the mid-point method approximates \(f(\mathbf{I}_{i})-f(\mathbf{0})\) as \(\frac{\partial f(\mathbf{I}_{i})}{\partial\mathbf{I}_{i}/2}\mathbf{I}_{i}\) and has second-order accuracy.

## Appendix B Experiment Setting

### Neural Machine Translation

**Problem Setting.** Our experiments are based on the fairseq package (Ott et al., 2019). As to pre-processing, we follow the public released script from previous work (Lu et al., 2020), and conduct evaluations on the provided 'newstest14' file. More details can be found in Bojar et al. (2014).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Corpus & \(|\text{Train}|\) & \(|\text{Label}|\) & Task & Metric(s) & Domain \\ \hline \multicolumn{6}{c}{Single-Sentence Classification} \\ \hline CoLA & 8.5k & 2 & \begin{tabular}{c} acceptability \\ sentiment \\ \end{tabular} & \begin{tabular}{c} Matthews corr. \\ accuracy \\ \end{tabular} & \begin{tabular}{c} misc. \\ movie reviews \\ \end{tabular} \\ \hline \multicolumn{6}{c}{Sentence Similarity/Paraphrase} \\ \hline MRPC & 3.7k & 2 & \begin{tabular}{c} paraphrase \\ similarity \\ \end{tabular} & \begin{tabular}{c} accuracy \\ Spearman corr. \\ accuracy \\ \end{tabular} & 
\begin{tabular}{c} news \\ misc. \\ social QA questions \\ \end{tabular} \\ \hline \multicolumn{6}{c}{Natural Language Inference (NLI)} \\ \hline MNLI & 393k & 3 & NLI & (mis)matched acc. & misc. \\ QNLI & 108k & 2 & QA/NLI & accuracy & Wikipedia \\ RTE & 2.5k & 2 & NLI & accuracy & misc. \\ WNLI & 634 & 2 & coreference/NLI & accuracy & fiction books \\ \hline \hline \end{tabular}
\end{table}
Table 4: GLUE task descriptions and statistics. The second and fourth column denotes the number of training examples and the number of classes. Note that STS-B is a regression task.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Hyperparameters** & **Base** \\ \hline Sequence Length & 256 \\ Optimizer & Adam \\ Peak Learning Rate & \{5e-5,1e-4, 3e-4\} \\ Max Epochs & \{2,3,5,10\} \\ Batch size & \{16, 32\} \\ Learning rate decay & Linear \\ Weight Decay & \{0,0.01\} \\ Warm-up Proportion & \{6 \%, 10 \%\} \\ Adam \(\epsilon\) & 1e-6 \\ Adam \((\beta_{1},\beta_{2})\) & \((0.9,0.98)\) \\ Gradient Clipping & \(1.0\) \\ Dropout & \(0.1\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameter search space in fine-tuning.

Model Architecture.As to model specifics, we directly adopt the Transformer-base model on the WMT'14 En-De datasets. Specifically, we use encoder-decoder Transformer models with 6 encoder layers, 6 decoder layers, 512-dimension word embedding, 8-head attentions, and 2048-dimension feed-forward layers. Following Fedus et al. (2021), we apply MoE layers at every other feed-forward layers, set jitter to 0.1, and configure load balance ratio as \(1\cdot 10^{-2}\). As the number of experts, we consider 5 different settings, i.e., \(N\in\{2,4,6,8,16\}\). Label smoothed cross-entropy is used as the objective function with the uncertainty set as \(0.1\)(Szegedy et al., 2016).

Training Settings.We mostly followed (Liu et al., 2020) for training settings. Specifically, we use Adam as the optimizer set \((\beta_{1},\beta_{2})\) as \((0.9,0.98)\), use inverse sqrt learning rate scheduler with a warmup phrase (8000 steps). All dropout ratios (including activation dropout and attention dropout) are set to 0.1. The maximum learning rate is set to \(7\cdot 10^{-4}\) and the maximum token number per batch is set to \(2^{17}\). We conduct training for \(4\cdot 10^{5}\) updates and report the performance of the last checkpoint and the checkpoint with the lowest development loss.

### Pre-training

Pre-training Setup.We follow the standard settings for training Base models (Clark et al., 2020; Bajaj et al., 2022; Dong et al., 2023), Specifically, we employ Wikipedia and BookCorpus (Zhu et al., 2015) for pre-training and set the sequence length to \(512\), which leads to \(16\) GB of texts and \(256\)M samples. We use a cased sentence piece BPE vocabulary of \(128\)K tokens following He et al. (2020), and conduct pre-training for \(125\)K updates with a batch size of \(2048\) sentences.

Model Architecture.Our main model (discriminator) setting follows the BERTbase architecture (Devlin et al., 2019). Specifically, the model has 12 layers, 768-dimension embedding, and 12-head attention. As to the feed-forward networks, we set the number of hidden state dimensions to 3076. Following Bajaj et al. (2022) and Dong et al. (2023), we further enhanced the model with the T5 relative position encoding (Raffel et al., 2019) and use \(32\) bins. We set dropout as \(0.1\) and employ Admin (Liu et al., 2020) for model initialization to stabilize the training. Following Fedus et al. (2021), we apply MoE layers at every other feed-forward layers, set jitter to 0.1, and configure load balance ratio as \(1\cdot 10^{-2}\). As the number of experts, we consider 3 different settings, i.e., \(N\in\{2,4,8\}\). As to the auxiliary model, we follow previous works (Clark et al., 2020; Bajaj et al., 2022) to set the size of the auxiliary model (generator) to be \(4\) layers.

Optimization.We configure the optimizer as Adam, \((\beta_{1},\beta_{2})\) as \((0.9,0.98)\), weight decay as \(0.01\), the loss weight as \(50\), the peak learning rate as \(5e-4\), and the warmup steps as \(10\)K.

Downstream evaluation setup.We conduct evaluation on downstream tasks following the setup in previous works (Bajaj et al., 2022). Specifically, we conduct single-task, single-model fine-tuning on the GLUE (Wang et al., 2018) benchmark. As summarized in the Appendix (Table 4), GLUE includes 9 subtasks. Following Liu et al. (2019), we conduct a grid-search on hyper-parameters and report the best performance for both Switch and Swith + SparseMixer. The complete search space is included in Appendix (Table 5).

Figure 6: Training curves of Switch Transformer on ELECTRA-base training.