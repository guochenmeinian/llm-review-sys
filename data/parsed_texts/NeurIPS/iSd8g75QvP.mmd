# A Trichotomy for Transductive Online Learning

Steve Hanneke

Department of Computer Science

Purdue University

steve.hanneke@gmail.com &Shay Moran

Faculty of Mathematics,

Faculty of Computer Science, and

Faculty of Data and Decision Sciences

Technion - Israel Institute of Technology

smoran@technion.ac.il &Jonathan Shafer

Computer Science Division

UC Berkeley

shaferjo@berkeley.edu

###### Abstract

We present new upper and lower bounds on the number of learner mistakes in the 'transductive' online learning setting of Ben-David, Kushilevitz and Mansour (1997). This setting is similar to standard online learning, except that the adversary fixes a sequence of instances \(x_{1},\ldots,x_{n}\) to be labeled at the start of the game, and this sequence is known to the learner. Qualitatively, we prove a _trichotomy_, stating that the minimal number of mistakes made by the learner as \(n\) grows can take only one of precisely three possible values: \(n\), \(\Theta\left(\log(n)\right)\), or \(\Theta(1)\). Furthermore, this behavior is determined by a combination of the VC dimension and the Littlestone dimension. Quantitatively, we show a variety of bounds relating the number of mistakes to well-known combinatorial dimensions. In particular, we improve the known lower bound on the constant in the \(\Theta(1)\) case from \(\Omega\left(\sqrt{\log(d)}\right)\) to \(\Omega(\log(d))\) where \(d\) is the Littlestone dimension. Finally, we extend our results to cover multiclass classification and the agnostic setting.

## 1 Introduction

In classification tasks like PAC learning and online learning, the learner simultaneously confronts two distinct types of uncertainty: _labeling-related_ uncertainty regarding the best labeling function, and _instance-related_ uncertainty regarding the instances that the learner will be required to classify in the future. To gain insight into the role played by each type of uncertainty, researchers have studied modified classification tasks in which the learner faces only one type of uncertainty, while the other type has been removed.

In the context of PAC learning, [1] studied a variant of proper PAC learning in which the true labeling function is known to the learner, and only the distribution over the instances is not known. They show bounds on the sample complexity in this setting, which conceptually quantify the instance-related uncertainty. Conversely, labeling-related uncertainty is captured by PAC learning with respect to a fixed (e.g., uniform) domain distribution [1], a setting which has been studied extensively.

In this paper we improve upon the work of [1], who quantified the label-related uncertainty in online learning. They introduced a model of _transductive online learning_,1 in which the adversary

[MISSING_PAGE_EMPTY:2]

**Theorem** (**Informal Version of Theorem 4.1)**.: _Every hypothesis class \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) satisfies precisely one of the following:_

1. \(M(\mathcal{H},n)=n\)_. This happens if_ \(\mathsf{VC}(\mathcal{H})=\infty\)_._
2. \(M(\mathcal{H},n)=\Theta(\log(n))\)_. This happens if_ \(\mathsf{VC}(\mathcal{H})<\infty\) _and_ \(\mathsf{LD}(\mathcal{H})=\infty\)_._
3. \(M(\mathcal{H},n)=\Theta(1)\)_. This happens if_ \(\mathsf{LD}(\mathcal{H})<\infty\)_._ _The_ \(\Theta(\cdot)\) _notations in Items_ 2_. and_ 3_. hide a dependence on_ \(\mathsf{VC}(\mathcal{H})\)_, and_ \(\mathsf{LD}(\mathcal{H})\)_, respectively._ The proof uses bounds on the number of mistakes in terms of the _threshold dimension_ (Section 3.2), among other tools.
2. **Littlestone classes.** The minimal constant upper bound in the \(\Theta(1)\) case of Theorem 4.1 is some value \(C(\mathcal{H})\) that depends on the class \(\mathcal{H}\), but the precise mapping \(\mathcal{H}\mapsto C(\mathcal{H})\) is not known in general. [1] showed that \(C(\mathcal{H})=\Omega\Big{(}\sqrt{\log(\mathsf{LD}(\mathcal{H}))}\Big{)}\). In Section 3 and Appendix A we improve upon their result as follows. **Theorem** (**Informal Version of Theorem 3.1)**.: _Let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) such that \(\mathsf{LD}(\mathcal{H})=d<\infty\). Then \(M(\mathcal{H},n)=\Omega(\log(d))\)._
3. **Multiclass setting.** In Section 5, we generalize Theorem 4.1 to the multiclass setting with a finite label set \(\mathcal{Y}\), showing a trichotomy based on the Natarajan dimension. The proof uses a simple result from Ramsey theory, among other tools. Additionally, we show that the DS dimension of [15] does not characterize multiclass transductive online learning.
4. **Agnostic setting.** In the _standard_ (non-transductive) agnostic online setting, [1] showed that \(R_{\mathsf{online}}(\mathcal{H},n)\), the agnostic online regret for a hypothesis class \(\mathcal{H}\) for a sequence of length \(n\) satisfies \[\Omega\Big{(}\sqrt{\mathsf{LD}(\mathcal{H})\cdot n}\Big{)}\leq R_{\mathsf{ online}}(\mathcal{H},n)\leq O\Big{(}\sqrt{\mathsf{LD}(\mathcal{H})\cdot n\cdot \log n}\Big{)}.\] (1) Later, [1] showed an improved bound of \(R_{\mathsf{online}}(\mathcal{H},n)=\Theta\Big{(}\sqrt{\mathsf{LD}(\mathcal{H}) \cdot n}\Big{)}\). In Section 6 we show a result similar to Eq. (1), for the _transductive_ agnostic online setting. **Theorem** (**Informl Version of Theorem 6.1)**.: _Let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\), such that \(0<\mathsf{VC}(\mathcal{H})<\infty\). Then the agnostic transductive regret for \(\mathcal{H}\) is_

\[\Omega\Big{(}\sqrt{\mathsf{VC}(\mathcal{H})\cdot n}\Big{)}\leq R(\mathcal{H}, n)\leq O\Big{(}\sqrt{\mathsf{VC}(\mathcal{H})\cdot n\cdot\log n}\Big{)}.\]

### Related Works

The general idea of bounding the number of mistakes by learning algorithms in sequential prediction problems was introduced in the seminal work of Littlestone [14]. That work introduced the _online_ learning model, where the sequence of examples is revealed to the learner one example at a time. After each example \(x\) is revealed, the learner makes a prediction, after which the true target label \(y\) is revealed. The constraint, which makes learning even plausible, is that this sequence of \((x,y)\) pairs should maintain the property that there is an (unknown) target concept in a given concept class \(\mathcal{H}\) which is correct on the entire sequence. Littlestone [14] also identified the optimal predictor for this problem (called the _SOA_, for _Standard Optimal Algorithm_, and a general complexity measure which is precisely equal to the optimal bound on the number of mistakes: a quantity now referred to as the _Littlestone dimension_.

Later works discussed variations on this framework. In particular, as mentioned, the transductive model discussed in the present work was introduced in the work of [1]. The idea (and terminology) of transductive learning was introduced by [12, 21, 22], to capture scenarioswhere learning may be easier due to knowing in advance which examples the learner will be tested on. [22, 23] study transductive learning in a model closer in spirit to the PAC framework, where some uniform random subset of examples have their labels revealed to the learner and it is tasked with predicting the labels of the remaining examples. In contrast, [1] study transductive learning in a sequential prediction setting, analogous to the online learning framework of Littlestone. In this case, the sequence of examples \(x\) is revealed to the learner all at once, and only the target labels (the \(y\)'s) are revealed in an online fashion, with the label of each example revealed just after its prediction for that example in the given sequential order. Since a mistake bound in this setting is still required to hold for _any_ sequence, for the purpose of analysis we may think of the sequence of \(x\)'s as being a _worst case_ set of examples and ordering thereof, for a given learning algorithm. [1] compare and contrast the optimal mistake bound for this setting to that of the original online model of [16]. Denoting by \(d\) the Littlestone dimension of the concept class, it is clear that the optimal mistake bound would be no larger than \(d\). However, they also argue that the optimal mistake bound in the transductive model is never smaller than \(\Omega(\sqrt{\log(d)})\) (as mentioned, we improve this to \(\log(d)\) in the present work). They further exhibit a family of concept classes of variable \(d\) for which the transductive mistake bound is strictly smaller by a factor of \(\frac{3}{2}\). They additionally provide a general equivalent description of the optimal transductive mistake bound in terms of the maximum possible rank among a certain family of trees, each representing the game tree for the sequential game on a given sequence of examples \(x\).

In addition to these two models of sequential prediction, the online learning framework has also been explored in other variations, including exploring the optimal mistake bound under a _best-case_ order of the data sequence \(x\), or even a _self-directed_ adaptive order in which the learning algorithm selects the next point for prediction from the remaining \(x\)'s from the given sequence on each round. [1, 1, 2, 13, 14].

Unlike the online learning model of Littlestone, the transductive model additionally allows for nontrivial mistake bounds in terms of the sequence _length_\(n\) (the online model generally has \(\min\{d,n\}\) as the optimal mistake bound). In this case, it follows immediately from the Sauer-Shelah-Perles lemma and a Halving technique that the optimal transductive mistake bound is no larger than \(O(v\log(n))\)[15], where \(v\) is the VC dimension of the concept class [22, 23].

## 2 Preliminaries

**Notation 2.1**.: _Let \(\mathcal{X}\) be a set and \(n,k\in\mathbb{N}\). For a sequence \(x=(x_{1},\ldots,x_{n})\in\mathcal{X}^{n}\), we write \(x_{\leq k}\) to denote the subsequence \((x_{1},\ldots,x_{k})\). If \(k\leq 0\) then \(x_{\leq k}\) denotes the empty sequence, \(\mathcal{X}^{0}\)._

**Definition 2.2**.: _Let \(k\in\mathbb{N}\), let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). A sequence \((x_{1},y_{1}),\ldots,(x_{k},y_{k})\in\left(\mathcal{X}\times\mathcal{Y}\right) ^{k}\) is realizable by \(\mathcal{H}\), or \(\mathcal{H}\)-realizable, if \(\exists h\in\mathcal{H}\ \forall i\in[k]:\ h(x_{i})=y_{i}\)._

**Definition 2.3**.: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\), let \(d\in\mathbb{N}\), and let \(X=\{x_{1},\ldots,x_{d}\}\subseteq\mathcal{X}\). We say that \(\mathcal{H}\) shatters \(X\) if for every \(y\in\{0,1\}^{d}\) there exists \(h\in\mathcal{H}\) such that for all \(i\in[d]\), \(h(x_{i})=y_{i}\). The Vapnik-Chervonenkis (VC) dimension of \(\mathcal{H}\) is \(\mathsf{VC}(\mathcal{H})=\sup\left\{|X|:\ X\subseteq\mathcal{X}\text{ finite }\wedge\ \mathcal{H}\text{ shatters }X\right\}\)._

**Definition 2.4** ([16]).: _Let \(\mathcal{X}\) be a set and let \(d\in\mathbb{N}\). A Littlestone tree of depth \(d\) with domain \(\mathcal{X}\) is a set_

\[T=\left\{x_{u}\in\mathcal{X}:\ u\in\bigcup_{s=0}^{d}\{0,1\}^{s}\right\}.\] (2)

_Let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\). We say that \(\mathcal{H}\) shatters a tree \(T\) as in Eq. (2) if for every \(u\in\{0,1\}^{d+1}\) there exists \(h_{u}\in\mathcal{H}\) such that_

\[\forall i\in[d+1]:\ h(x_{u_{\leq i-1}})=u_{i}.\]

_The Littlestone dimension of \(\mathcal{H}\), denoted \(\mathsf{LD}(\mathcal{H})\), is the supremum over all \(d\in\mathbb{N}\) such that there exists a Littlestone tree of depth \(d\) with domain \(\mathcal{X}\) that is shattered by \(\mathcal{H}\)._

**Theorem 2.5** ([16]).: _Let \(\mathcal{X}\) be a set and let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) such that \(d=\mathsf{LD}(\mathcal{H})<\infty\). Then there exists a strategy for the learner that guarantees that the learner will make at most \(d\) mistakes in the standard (non-transductive) online learning setting, regardless of the adversary's strategy and of number of instances to be labeled._

**Theorem 2.6** (Sauer-Shelah-Perles; [11, 12]).: _Let \(n,d\in\mathbb{N}\), let \(\mathcal{X}\) be a set of cardinality \(n\), and let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) such that \(\mathsf{VC}(\mathcal{H})=d\). Then \(|\mathcal{H}|\leq\sum_{i=0}^{n}\binom{n}{i}\leq\left(\frac{\varepsilon n}{d} \right)^{d}\)._

## 3 Quantitative Bounds

### Littlestone Dimension: A Tighter Lower Bound

The Littlestone dimension is an upper bound on the number of mistakes, namely

\[\forall n\in\mathbb{N}:\ M(\mathcal{H},n)\leq\mathsf{LD}(\mathcal{H})\] (3)

for any class \(\mathcal{H}\). This holds because \(\mathsf{LD}(\mathcal{H})\) is an upper bound on the number of mistakes for standard (non-transductive) online learning [10], and the adversary in the transductive setting is strictly weaker.

The Littlestone dimension also supplies a lower bound. We give a quadratic improvement on the previous lower bound of [1], as follows.

**Theorem 3.1**.: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) such that \(d=\mathsf{LD}(\mathcal{H})<\infty\), and let \(n\in\mathbb{N}\). Then_

\[M(\mathcal{H},n)\geq\min\left\{\lfloor\log(d)/2\rfloor,\lfloor\log\log(n)/2 \rfloor\right\}.\]

Proof idea for Theorem 3.1.: Let \(T\) be a Littlestone tree of depth \(d\) that is shattered by \(\mathcal{H}\), and let \(\mathcal{H}_{1}\subseteq\mathcal{H}\) be a collection of \(2^{d+1}\) functions that witness the shattering. The adversary selects the sequence consisting of the nodes of \(T\) in breadth-first order. For each time step \(t\in[n]\), let \(\mathcal{H}_{t}\) denote the version space, i.e., the subset of \(\mathcal{H}_{1}\) that is consistent with all previously-assigned labels. The adversary's adaptive labeling strategy at time \(t\) is as follows. If \(\mathcal{H}_{t}\) is very unbalanced, meaning that a large majority of functions in \(\mathcal{H}_{t}\) assign the same value to \(x_{t}\), then the adversary chooses \(y_{t}\) to be that value. Otherwise, if \(\mathcal{H}_{t}\) is fairly balanced, the adversary forces a mistake (it can do so without violating \(\mathcal{H}\)-realizability). The pivotal observation is that: (1) under this strategy, the version space decreases in cardinality significantly more during steps where the adversary forces a mistake compared to steps where it did not force a mistake; (2) let \(x_{t}\) be the \(t\)-th node in the breadth-first order. It has distance \(\ell=\lfloor\log(t)\rfloor\) from the root of \(T\). Because \(T\) is a binary tree, the subtree \(T^{\prime}\) of \(T\) rooted at \(x_{t}\) is a tree of depth \(d-\ell\). In particular, seeing as \(\mathcal{H}_{t}\) contains only functions necessary for shattering \(T^{\prime}\), \(|\mathcal{H}_{t}|\leq 2^{d-\ell+1}\), so \(\mathcal{H}_{t}\) must decrease not too slowly with \(t\). Combining (1) and (2) yields that the adversary must be able to force a mistake not too rarely. A careful quantitative analysis shows that the number of mistakes the adversary can force is at least logarithmic in \(d\). 

The full proof of Theorem 3.1 appears in Appendix A.

### Threshold Dimension

We also show some bounds on the number of mistakes in terms of the threshold dimension.

**Definition 3.2**.: _Let \(\mathcal{X}\) be a set, let \(X=\{x_{1},\ldots,x_{k}\}\subseteq\mathcal{X}\), and let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\). We say that \(X\) is threshold-shattered by \(\mathcal{H}\) if there exist \(h_{1},\ldots,h_{k}\in\mathcal{H}\) such that \(h_{i}(x_{j})=\mathds{1}(j\leq i)\) for all \(i,j\in[k]\)

Figure 1: A shattered Littlestone tree of depth 2. The empty sequence is denoted by \(\lambda\).

_The threshold dimension of \(\mathcal{H}\), denoted \(\mathsf{TD}(\mathcal{H})\), is the supremum of the set of integers \(k\) for which there exists a threshold-shattered set of cardinality \(k\)._

The following connection between the threshold and Littlestone dimensions is well-known.

**Theorem 3.3** ([10, 11]).: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\), and let \(d\in\mathbb{N}\). Then:_

1. _If_ \(\mathsf{LD}(\mathcal{H})\geq d\) _then_ \(\mathsf{TD}(\mathcal{H})\geq\lfloor\log d\rfloor\)_._
2. _If_ \(\mathsf{TD}(\mathcal{H})\geq d\) _then_ \(\mathsf{LD}(\mathcal{H})\geq\lfloor\log d\rfloor\)_._

Item 1 in Theorem 3.3 and Eq. (3) imply that

\[\forall n\in\mathbb{N}:\ M(\mathcal{H},n)\leq 2^{\mathsf{TD}(\mathcal{H})}\]

for any class \(\mathcal{H}\). Similarly, Item 2 in Theorem 3.3 and Theorem 3.1 imply a mistake lower bound of \(\Omega(\log\log(\mathsf{TD}(\mathcal{H})))\). However, one can do exponentially better than that, as follows.

**Claim 3.4**.: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) such that \(d=\mathsf{TD}(\mathcal{H})<\infty\), and let \(n\in\mathbb{N}\). Then_

\[M(\mathcal{H},n)\geq\min\left\{\lfloor\log(d)\rfloor\,\lfloor\log(n)\rfloor \right\}.\]

One of the ideas used in this proof appeared in an example called \(\sigma_{\mathrm{worst}}\) in Section 4.1 of [1].

Proof of Claim 3.4.: Let \(k=\min\left\{\lfloor\log(d)\rfloor\,,\lfloor\log(n)\rfloor\right\}\) and let \(N=2^{k}\). Let \(X=\{x_{1},\ldots,x_{N-1}\}\subseteq\mathcal{X}\) be a set that is threshold-shattered by functions \(h_{1},\ldots,h_{N-1}\in\mathcal{H}\) and \(h_{i}(x_{j})=\mathds{1}(j\leq i)\) for all \(i,j\in[N-1]\). The strategy for the adversary is to present \(X\) in dyadic order, namely

\[x_{\frac{N}{2}},x_{\frac{N}{4}},x_{\frac{3N}{4}},x_{\frac{N}{8}},x_{\frac{3N}{ 8}},x_{\frac{5N}{8}},x_{\frac{7N}{8}},\ldots,x_{\frac{(2^{k}-1)N}{2^{k}}}.\]

More explicitly, the adversary chooses the sequence \(q=q_{1}\circ q_{2}\circ\cdots\circ q_{k}\), where '\(\circ\)' denotes sequence concatenation and

\[q_{i}=\left(x_{\frac{1}{2^{i}}N},x_{\frac{3}{2^{i}}N},x_{\frac{5}{2^{i}}N},x_ {\frac{7}{2^{i}}N},\ldots,x_{\frac{(2^{i}-1)}{2^{i}}N}\right)\]

for all \(i\in[k]\). See Figure 2.

We prove by induction that for each \(i\in[k]\), all labels chosen by the adversary for the subsequences prior to \(q_{i}\) are \(\mathcal{H}\)-realizable, and additionally there exists an instance in subsequence \(q_{i}\) on which the adversary can force a mistake regardless of the learners predictions. The base case is that the adversary can always force a mistake on the first instance, \(q_{1}\), by choosing the label opposite to the learner's prediction (both labels \(0\) and \(1\) are \(\mathcal{H}\)-realizable for this instance). Subsequently, for any \(i>1\), note that by the induction hypothesis, the labels chosen by the adversary for all instances in the previous subsequences are \(\mathcal{H}\)-realizable. In particular there exists an index \(a\in[N]\) such that instance \(x_{a}\) has already been labeled, and all the labels chosen so far are consistent with \(h_{a}\). Let \(b\) be the minimal integer such that \(b>a\) and \(x_{b}\) has also been labeled. Then \(x_{a}\) and all labeled instances with smaller indices received label \(1\), while \(x_{b}\) and all labeled instances with larger indices received label \(0\). Because the sequence is dyadic, subsequence \(q_{i}\) contains an element \(x_{m}\) such that \(a<m<b\). The adversary can force a mistake on \(x_{m}\), because \(h_{a}\) and \(h_{m}\) agree on all previously labeled instances, but disagree on \(x_{m}\)

Figure 2: Construction of the sequence \(q\) in the proof of Claim 3.4. \(q\) is a breadth-first enumeration of the depicted binary tree.

Claim 3.4 is used in the proof of the trichotomy (Theorem 4.1, below).

Finally, we note that for every \(d\in\mathbb{N}\) there exists a hypothesis class \(\mathcal{H}\) such that \(d=\mathsf{TD}(\mathcal{H})\) and

\[\forall n\in\mathbb{N}:\ M(\mathcal{H},n)=\min\{d,n\}.\]

Indeed, take \(\mathcal{X}=[d]\) and \(\mathcal{H}=\{0,1\}^{\mathcal{X}}\). The upper bound holds because \(|\mathcal{X}|=d\), and the lower bound holds by Item 2 in Theorem 4.1, because \(\mathsf{VC}(\mathcal{H})=d\).

## 4 Trichotomy

**Theorem 4.1**.: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\), and let \(n\in\mathbb{N}\) such that \(n\leq|\mathcal{X}|\)._

1. _If_ \(\mathsf{VC}(\mathcal{H})=\infty\) _then_ \(M(\mathcal{H},n)=n\)_._
2. _Otherwise, if_ \(\mathsf{VC}(\mathcal{H})=d<\infty\) _and_ \(\mathsf{LD}(\mathcal{H})=\infty\) _then_ \[\max\{\min\{d,n\},\,|\log(n)|\}\leq M(\mathcal{H},n)\leq O(d\log(n/d)).\] (4) _Furthermore, each of the bounds in Eq._ (4) _is tight for some classes. The_ \(\Omega(\cdot)\) _and_ \(O(\cdot)\) _notations hide universal constants that do not depend on_ \(\mathcal{X}\) _or_ \(\mathcal{H}\)_._
3. _Otherwise, there exists an integer_ \(C(\mathcal{H})\leq\mathsf{LD}(\mathcal{H})\) _(that depends on_ \(\mathcal{X}\) _and_ \(\mathcal{H}\) _but does not depend on_ \(n\)_) such that_ \(M(\mathcal{H},n)\leq C(\mathcal{H})\)_._

Proof of Theorem 4.1.: For Item 1, assume \(\mathsf{VC}(\mathcal{H})=\infty\). Then there exists a set \(X=\{x_{1},\ldots,x_{n}\}\subseteq\mathcal{X}\) of cardinality \(n\) that is shattered by \(\mathcal{H}\). The adversary can force the learner to make \(n\) mistakes by selecting the sequence \((x_{1},\ldots,x_{n})\), and then selecting labels \(y_{t}=1-\hat{y}_{t}\) for all \(t\in[n]\). This choice of labels is \(\mathcal{H}\)-realizable because \(X\) is a shattered set.

To obtain the upper bound in Item 2 the learner can use the _halving algorithm_, as follows. Let \(x=(x_{1},\ldots,x_{n})\) be the sequence chosen by the adversary, and let \(\mathcal{H}|_{x}\) denote the collection of functions from elements of \(x\) to \(\{0,1\}\) that are restrictions of functions in \(\mathcal{H}\). For each \(t\in\{0,\ldots,n\}\), let

\[\mathcal{H}_{t}=\big{\{}f\in\mathcal{H}|_{x}:\ \left(\forall i\in[t]:\ f(x_{i})=y_ {i}\right)\big{\}}\]

be a set called the _version space_ at time \(t\). At each step \(t\in[n]\), the learner makes prediction

\[\hat{y}_{t}=\operatorname{argmax}_{b\in\{0,1\}}\big{|}\big{\{}f\in\mathcal{H }_{t-1}:\ f(x_{t})=b\big{\}}\big{|}\,.\]

In words, the learner chooses \(\hat{y}_{t}\) according to a majority vote among the functions in version space \(\mathcal{H}_{t-1}\), and then any function whose vote was incorrect is excluded from the next version space, \(\mathcal{H}_{t}\). This implies that for any \(t\in[n]\), if the learner made a mistake at time \(t\) then

\[|\mathcal{H}_{t}|\leq\frac{1}{2}\cdot|\mathcal{H}_{t-1}|.\] (5)

Let \(M=M(\mathcal{H},n)\). The adversary selects \(\mathcal{H}\)-realizable labels, so \(\mathcal{H}_{n}\) cannot be empty. Hence, applying Eq. (5) recursively yields

\[1\leq|\mathcal{H}_{n}|\leq 2^{-M}\cdot|\mathcal{H}_{0}|\leq 2^{-M}\cdot O \Big{(}(n/d)^{d}\Big{)},\]

where the last inequality follows from \(\mathsf{VC}(\mathcal{H}_{0})\leq\mathsf{VC}(\mathcal{H})=d\) and the Sauer-Shelah-Perles lemma (Theorem 2.6). Hence \(M=O(d\log(n/d))\), as desired.

For the \(\min\{d,n\}\) lower bound in Item 2, if \(n\leq d\) then the adversary can force \(n\) mistakes by the same argument as in Item 1. For the logarithmic lower bound in Item 2, the assumption that \(\mathsf{LD}(\mathcal{H})=\infty\) and Theorem 3.3 imply that \(\mathsf{TD}(\mathcal{H})=\infty\), and in particular \(\mathsf{TD}(\mathcal{H})\geq n\), and this implies the bound by Claim 3.4.

For Item 3, the assumption \(\mathsf{LD}(\mathcal{H})=k<\infty\) and Theorem 2.5 imply that for any \(n\), the learner will make at most \(k\) mistakes. This is because the adversary in the transductive setting is strictly weaker than the adversary in the standard online setting. So there exists some \(C(\mathcal{H})\in\{0,\ldots,k\}\) as desired. 

**Remark 4.2**.: _One can use Theorem 3.1 to obtain a lower bounds for the case of Item 2 in Theorem 4.1. However, that yields a lower bound of \(\Omega(\log\log(n))\), which is exponentially weaker than the bound we show._

## 5 Multiclass Setting

The trichotomy of Theorem 4.1 can be generalized to the multiclass setting, in which the label set \(\mathcal{Y}\) contains more than two labels. In this setting, the VC dimension is replaced by the Natarajan dimension [14], denoted \(\mathsf{ND}\), and the Littlestone dimension is generalized in the natural way. The result holds for _finite_ sets \(\mathcal{Y}\).

**Theorem 5.1** (Informal Version of Theorem B.3).: _Let \(\mathcal{X}\) be a set, let \(\mathcal{Y}\) be a finite set, and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). Then \(\mathcal{H}\) satisfies precisely one of the following:_

1. \(M(\mathcal{H},n)=n\)_. This happens if_ \(\mathsf{ND}(\mathcal{H})=\infty\)_._
2. \(M(\mathcal{H},n)=\Theta(\log(n))\)_. This happens if_ \(\mathsf{ND}(\mathcal{H})<\infty\) _and_ \(\mathsf{LD}(\mathcal{H})=\infty\)_._
3. \(M(\mathcal{H},n)=O(1)\)_. This happens if_ \(\mathsf{LD}(\mathcal{H})<\infty\)_._

The proof of Theorem 5.1 appears in Appendix B, along with the necessary definitions. The main innovation in the proof involves the use of the multiclass threshold bounds developed in Appendix D, which in turn rely on a basic result from Ramsey theory, stated Appendix C.

### The Case of an Infinite Label Set

It is interesting to observe that the analogy between the binary classification and multiclass classification settings breaks down when the label set \(\mathcal{Y}\) is not finite.

**Example 5.2**.: _There exists a class \(\mathcal{G}\subseteq\mathcal{Y}^{\mathcal{X}}\) such that \(\mathcal{Y}\) is countable, \(\mathsf{LD}(\mathcal{G})\) is infinite, but the class is learnable with a mistake bound of \(M(\mathcal{G},n)=1\). To see this, let \(\mathcal{X}\) be countable, and let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) be a class with \(\mathsf{LD}(\mathcal{H})=\infty\). For each \(i\in\mathbb{N}\), let \(T_{i}\) be a Littlestone tree of depth \(i\) that is shattered by \(\mathcal{H}\), and let \(\{h^{i}_{1},\ldots,h^{i}_{2i+1}\}\subseteq\mathcal{H}\) be a subset that witnesses the shattering. Let \(\mathcal{G}=\{g^{i}_{j}:\;i\in\mathbb{N}\;\wedge\;j\in[2^{i+1}]\}\) be a set of functions such that \(g^{i}_{j}(x)=(h^{i}_{j}(x),i,j)\) for all \(i,j\). Let \(\mathcal{Y}=\{0,1\}\times\mathbb{N}\times\mathbb{N}\). Observe that \(\mathcal{G}\subseteq\mathcal{Y}^{\mathcal{X}}\) is a countable set of functions with a countable set of labels. Furthermore, \(\mathsf{LD}(\mathcal{G})=\infty\) because \(\mathcal{G}\) shatters a sequence of suitable Littlestone trees corresponding to \(T_{1},T_{2},\ldots\). However, \(\mathcal{G}\) can be learned with mistake bound \(1\), because a single example of the form \(\big{(}x,(h^{i}_{j}(x),i,j)\big{)}\) reveals the correct labeling function \(h^{i}_{j}\)._

Recent work by [1] has shown that multiclass PAC learning with infinite \(\mathcal{Y}\) is not characterized by the Natarajan dimension, and that instead it is characterized by the DS dimension (introduced by [1]). It is therefore natural to ask whether the DS dimension might also characterize multiclass transductive online learning with infinite \(\mathcal{Y}\). We show that the answer to that question is negative.

Recall the definition of the DS dimension.

**Definition 5.3**.: _Let \(d\in\mathbb{N}\), let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). For an index \(i\in[d]\) and vectors \(y=(y_{1},\ldots,y_{d})\in\mathcal{Y}^{d}\), \(y^{\prime}=(y^{\prime}_{1},\ldots,y^{\prime}_{d})\in\mathcal{Y}^{d}\), we say that \(y\) and \(y^{\prime}\) are \(i\)-neighbors, denoted \(y\sim_{i}y^{\prime}\), if \(\{j\in[d]:\;y_{j}\neq y^{\prime}_{j}\}=\{i\}\). We say that \(\mathcal{C}\subseteq\mathcal{Y}^{d}\) is a d-pseudocube if \(\mathcal{C}\) is non-empty and finite, and_

\[\forall y\in\mathcal{C}\;\forall i\in[d]\;\exists y^{\prime}\in\mathcal{C}:\; y\sim_{i}y^{\prime}.\]

_For a vector \(x=(x_{1},\ldots,x_{d})\in\mathcal{X}^{d}\), we say that \(\mathcal{H}\) DS-shatters \(x\) if the set_

\[\mathcal{H}|_{x}:=\left\{\left(h(x_{1}),\ldots,h(x_{d})\right):\;h\in\mathcal{ H}\right\}\subseteq\mathcal{Y}^{d}\]

_contains a \(d\)-pseudocube._

_Finally, the Daniely-Shalev-Shwartz (DS) dimension of \(\mathcal{H}\) is_

\[\mathsf{DS}(\mathcal{H})=\sup\left\{d\in\mathbb{N}:\;\left(\exists x\in \mathcal{X}^{d}:\;\mathcal{H}\text{ DS-shattered }x\right)\right\}.\]

See [1] for figures and further discussion of the DS dimension.

The following claim shows that the DS dimension does not characterize transductive online learning, even when \(\mathcal{Y}\) is finite.

**Claim 5.4**.: _For every \(n\in\mathbb{N}\), there exists a hypothesis class \(\mathcal{H}_{n}\) such that \(\mathsf{DS}(\mathcal{H}_{n})=1\) but the adversary in transductive online learning can force at least \(M(\mathcal{H}_{n},n)=n\) mistakes._Proof.: Fix \(n\in\mathbb{N}\) and let \(\mathcal{X}=\{0,1,2,\ldots,n\}\). Consider a complete binary tree \(T\) of depth \(n\) such that for each \(x\in\mathcal{X}\), all the nodes at depth \(x\) (at distance \(x\) from the root) are labeled by \(x\), and each edge in \(T\) is labeled by a distinct label. Let \(\mathcal{H}\) be a minimal hypothesis class that shatters \(T\), namely, \(\mathcal{H}\) shatters \(T\) and there does not exist a strict subset of \(\mathcal{H}\) that shatters \(T\).

Observe that \(M(\mathcal{H}_{n},n)=n\), because the adversary can present the sequence \(0,1,2,\ldots,n-1\) and force a mistake at each time step. To see that \(\mathsf{DS}(\mathcal{H}_{n})=1\), assume for contradiction that there exists a vector \(x=(x_{1},x_{2})\in\mathcal{X}^{2}\) that is DS-shattered by \(\mathcal{H}_{n}\), namely, there exists a \(2\)-pseudocube \(\mathcal{C}\subseteq\mathcal{H}|_{x}\). Note that \(x_{1}\neq x_{2}\), and without loss of generality \(x_{1}<x_{2}\) (\(\mathcal{H}\) DS-shatters \((x_{1},x_{2})\) if and only if it DS-shatters \((x_{2},x_{1})\)).

Fix \(y\in\mathcal{C}\). So \(y=(h(x_{1}),h(x_{2}))\) for some \(h\in\mathcal{H}\). Because \(\mathcal{C}\) is a \(2\)-pseudocube, there exists \(y^{\prime}\in\mathcal{C}\) that is a \(1\)-neighbor of \(y\). Namely, there exists \(g\in\mathcal{H}\) such that \(y^{\prime}=(g(x_{1}),g(x_{2}))\in\mathcal{C}\), \(y_{1}^{\prime}\neq y_{1}\) and \(y_{2}^{\prime}=y_{2}\). However, because each edge in \(T\) has a distinct label, and \(\mathcal{H}\) is minimal, it follows that for any \(x\in\mathcal{X}\),

\[g(x)=h(x)\ \implies\ \big{(}\forall x^{\prime}\in\{0,1,\ldots,x\}:\ g(x^{ \prime})=h(x^{\prime})\big{)}.\]

In particular, \(g(x_{2})=y_{2}^{\prime}=y_{2}=h(x_{2})\) implies \(y_{1}^{\prime}=g(x_{1})=h(x_{1})=y_{1}\) which is a contradiction to the choice of \(y^{\prime}\). 

## 6 Agnostic Setting

The _agnostic_ transductive online learning setting is defined analogously to the _realizable_ (non-agnostic) transductive online learning setting described in Section 1.1. An early work by [10] observed that it is not possible for a learner to achieve vanishing regret in an agnostic online setting with complete information. Therefore, we consider a game with incomplete information, as follows.

First, the adversary selects an arbitrary sequence of instances, \(x_{1},\ldots,x_{n}\in\mathcal{X}\), and reveals the sequence to the learner. Then, for each \(t=1,\ldots,n\):

1. The adversary selects a label \(y_{t}\in\mathcal{Y}\).
2. The learner selects a prediction \(\hat{y}_{t}\in\mathcal{Y}\) and reveals it to the adversary.
3. The adversary reveals \(y_{t}\) to the learner.

At each time step \(t\in[n]\), the adversary may select any \(y_{t}\in\mathcal{Y}\), without restrictions.2 The learner, which is typically randomized, has the objective of minimizing the _regret_, namely

Footnote 2: Hence the name ‘agnostic’, implying that we make no assumptions concerning the choice of labels.

\[R(A,\mathcal{H},x,y)=\mathbb{E}[\|\{t\in[n]:\ \hat{y}_{t}\neq y_{t}\} \|]-\min_{h\in\mathcal{H}}\left|\{t\in[n]:\ h(x_{t})\neq y_{t}\}\right|,\]

where the expectation is over the learner's randomness. In words, the regret is the expected excess number of mistakes the learner makes when it plays strategy \(A\) and the adversary chooses the sequence \(x\in\mathcal{X}^{n}\) and labels \(y\in\mathcal{Y}^{n}\), as compared to the number of mistakes made by the best fixed hypothesis \(h\in\mathcal{H}\). We are interested in understanding the value of this game, namely

\[R(\mathcal{H},n)=\inf_{A\in\mathcal{A}}\ \sup_{x\in\mathcal{X}^{n}}\ \sup_{y\in \mathcal{Y}^{n}}\ R(A,\mathcal{H},x,y),\]

where \(\mathcal{A}\) is the set of all learner strategies. We show the following result.

**Theorem 6.1**.: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\), and let \(n\in\mathbb{N}\) such that \(n\leq|\mathcal{X}|\). Assume \(0<\mathsf{VC}(\mathcal{H})<\infty\). Then the agnostic transductive regret for \(\mathcal{H}\) on sequences of length \(n\) is_

\[\Omega\Big{(}\sqrt{\mathsf{VC}(\mathcal{H})\cdot n}\Big{)}\leq R(\mathcal{H},n )\leq O\Big{(}\sqrt{\mathsf{VC}(\mathcal{H})\cdot n\cdot\log\big{(}n/\mathsf{ VC}(\mathcal{H})\big{)}}\Big{)}.\]

The upper bound in Theorem 6.1 follows directly from the the Sauer-Shelah-Perles lemma (Theorem 2.6), together with the following well-known bound on the regret of the _Multiplicative Weights_ algorithm (see, e.g., Theorem 21.10 in [1]).

**Theorem 6.2**.: _Let \(\mathcal{X}\) be a set and let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) be finite. There exists an algorithm for the standard (non-transductive) agnostic online learning setting that satisfies_

\[R_{\mathsf{online}}(\mathcal{H},n)\leq\sqrt{2\log\big{(}|\mathcal{H}|\big{)}}.\]Theorem 6.2 implies the upper bound of Theorem 6.1, because the adversary in the transductive agnostic setting is weaker than the adversary in the standard agnostic setting.

We prove the lower bound of Theorem 6.1 using an anti-concentration technique from Lemma 14 of [1]. The proof appears in Appendix E.

**Remark 6.3**.: _Additionally:_

1. _If_ \(\mathsf{VC}(\mathcal{H})=0\) _(i.e., classes with a single function) then the regret is_ \(0\)_._
2. _If_ \(\mathsf{VC}(\mathcal{H})<\infty\) _and_ \(\mathsf{LD}(\mathcal{H})<\infty\) _then the regret is_ \(R(\mathcal{H},n)=O\Big{(}\sqrt{\mathsf{LD}(\mathcal{H})\cdot n}\Big{)}\)_, by_ _[_1_]_ _(as mentioned above). Namely, in some cases the_ \(\log(n)\) _factor in Theorem_ 6.1 _can be removed._
3. _If_ \(\mathsf{VC}(\mathcal{H})=\infty\) _then the regret is_ \(\Omega(n)\)_._

## 7 Future Work

Some remaining open problems include:

1. Showing a sharper bound for the \(\Theta(1)\) case in the trichotomy (Theorem 4.1). Currently, there is an exponential gap between the best known upper and lower bounds for Littlestone classes.
2. Showing sharper bounds for the \(\Theta(\log n)\) cases in the trichotomy (Theorem 4.1) and multiclass trichotomy (Theorem B.3).
3. Showing a sharper bound for the agnostic case (Theorem 6.1).
4. Characterizing the number of mistakes in the multiclass setting with an infinite label set \(\mathcal{Y}\) (complementing Theorem B.3).

## Acknowledgments

Zachary Chase contributed significantly to this paper. All errors are our own.

SM is a Robert J. Shillman Fellow; he acknowledges support by ISF grant 1225/20, by BSF grant 2018385, by an Azrieli Faculty Fellowship, by Israel PBC-VATAT, by the Technion Center for Machine Learning and Intelligent Systems (MLIS), and by the European Union (ERC, GENERALIZATION, 101039692). JS was supported by DARPA (Defense Advanced Research Projects Agency) contract # HR001120C0015 and the Simons Collaboration on The Theory of Algorithmic Fairness. Views and opinions expressed are those of the author(s) only and do not necessarily reflect those of the European Union, the European Research Council Executive Agency, DARPA, or the Simons Foundation. The European Union, the granting authority, DARPA, and the Simons Foundation cannot be held responsible for them.

## References

* [ABD\({}^{+}\)21] Noga Alon, Omri Ben-Eliezer, Yuval Dagan, Shay Moran, Moni Naor, and Eylon Yogev. Adversarial laws of large numbers and optimal regret in online classification. In Samir Khuller and Virginia Vassilevska Williams, editors, _STOC 2021: 53rd Annual ACM SIGACT Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021_, pages 447-455. ACM, 2021. doi:10.1145/3406325.3451041.
* [ALMM19] Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite littlestone dimension. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019_, pages 852-860. ACM, 2019. doi:10.1145/3313276.3316312.
* 22nd International Conference, ALT 2011,
Espoo, Finland, October 5-7, 2011. Proceedings_, volume 6925 of _Lecture Notes in Computer Science_, pages 440-451. Springer, 2011. doi:10.1007/978-3-642-24412-4_34.
* November 3, 2022_, pages 943-955. IEEE, 2022. doi:10.1109/FOCS54457.2022.00093.
* [BE98] Shai Ben-David and Nadav Eiron. Self-directed learning and its relation to the vc-dimension and to teacher-directed learning. Mach. Learn., 33(1):87-104, 1998. doi:10.1023/A:1007510732151.
* [BEK95] Shai Ben-David, Nadav Eiron, and Eyal Kushilevitz. On self-directed learning. In Wolfgang Maass, editor, _Proceedings of the Eigh Annual Conference on Computational Learning Theory, COLT 1995, Santa Cruz, California, USA, July 5-8, 1995_, pages 136-143. ACM, 1995. doi:10.1145/225298.225314.
* [BHM\({}^{+}\)21] Olivier Bousquet, Steve Hanneke, Shay Moran, Ramon van Handel, and Amir Yehudayoff. A theory of universal learning. In Samir Khuller and Virginia Vassilevska Williams, editors, _STOC 2021: 53rd Annual ACM SIGACT Symposium on Theory of Computing, Virtual Event, Italy, June 21-25, 2021_, pages 532-541. ACM, 2021. doi:10.1145/3406325.3451087.
* [BI91] Gyora M. Benedek and Alon Itai. Learnability with respect to fixed distributions. Theor. Comput. Sci., 86(2):377-390, 1991. doi:10.1016/0304-3975(91)90026-X.
* [BKM97] Shai Ben-David, Eyal Kushilevitz, and Yishay Mansour. Online learning versus offline learning. Mach. Learn., 29(1):45-63, 1997. doi:10.1023/A:1007465907571.
* The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009_, 2009. URL http://www.cs.mcgill.ca/%7Ecolt2009/papers/032.pdf#page=1.
* [CL06] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge University Press, 2006. doi:10.1017/CBO9780511546921.
* [Cov65] Thomas M. Cover. Behavior of sequential predictors of binary sequences. In _Transactions of the Fourth Prague Conference on Information Theory, Statistical Decision Functions, Random Processes_. Academic Press, 1965. URL https://isl.stanford.edu/~cover/papers/paper3.pdf.
* Festschrift in Honor of Vladimir N. Vapnik_, pages 177-194. Springer, 2013. doi:10.1007/978-3-642-41136-6_16.
* [DS14] Amit Daniely and Shai Shalev-Shwartz. Optimal learners for multiclass problems. In Maria-Florina Balcan, Vitaly Feldman, and Csaba Szepesvari, editors, _Proceedings of The 27th Conference on Learning Theory, COLT 2014, Barcelona, Spain, June 13-15, 2014_, volume 35 of _JMLR Workshop and Conference Proceedings_, pages 287-316. JMLR.org, 2014. URL http://proceedings.mlr.press/v35/daniely14b.html.
* [DSBS15] Amit Daniely, Sivan Sabato, Shai Ben-David, and Shai Shalev-Shwartz. Multiclass learnability and the ERM principle. J. Mach. Learn. Res., 16:2377-2404, 2015. doi:10.5555/2789272.2912074.
* [GS94] Sally A. Goldman and Robert H. Sloan. The power of self-directed learning. Mach. Learn., 14(1):271-294, 1994. doi:10.1023/A:1022605628675.
* [HL95] David Haussler and Philip M. Long. A generalization of sauer's lemma. J. Comb. Theory, Ser. A, 71(2):219-240, 1995. doi:10.1016/0097-3165(95)90001-2.
* [Hod97] Wilfrid Hodges. _A Shorter Model Theory_. Cambridge University Press, 1997.

* [KK05] Sham M. Kakade and Adam Kalai. From batch to transductive online learning. In _Advances in Neural Information Processing Systems 18, Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada_, pages 611-618, 2005. URL https://proceedings.neurips.cc/paper/2005/hash/17693c91d9204b7a646284bb3adb603-Abstract.html.
* [Kuh99] Christian Kuhlmann. On teaching and learning intersection-closed concept classes. In Paul Fischer and Hans Ulrich Simon, editors, _Computational Learning Theory, 4th European Conference, EuroCOLT 1999, Nordkirchen, Germany, March 29-31, 1999, Proceedings_, volume 1572 of _Lecture Notes in Computer Science_, pages 168-182. Springer, 1999. doi:10.1007/3-540-49097-3_14.
* [Lit87] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. _Mach. Learn._, 2(4):285-318, 1987. doi:10.1007/BF00116827.
* [Nat89] B. K. Natarajan. On learning sets and functions. _Mach. Learn._, 4:67-97, 1989. doi:10.1007/BF00114804.
* Israel Institute of Technology, Israel, 2008. URL https://technion.primo.exlibrisgroup.com/permalink/972TEC_INST/q1jq5o/alma990023032150203971.
* [Sau72] Norbert Sauer. On the density of families of sets. _J. Comb. Theory, Ser. A_, 13(1):145-147, 1972. doi:10.1016/0097-3165(72)90019-2.
* [SB14] Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014. doi:10.1017/CBO9781107298019.
* [She72] Saharon Shelah. A combinatorial problem; stability and order for models and theories in infinitary languages. _Pacific Journal of Mathematics_, 41(1):247-261, 1972. doi:10.2140/pjm.1972.41.247.
* [She90] Saharon Shelah. _Classification Theory: And the Number of Non-Isomorphic Models_, volume 92 of _Studies in Logic and the Foundations of Mathematics_. North-Holland, second edition, 1990.
* [SKS16] Vasilis Syrgkanis, Akshay Krishnamurthy, and Robert E. Schapire. Efficient algorithms for adversarial contextual learning. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, _Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016_, volume 48 of _JMLR Workshop and Conference Proceedings_, pages 2159-2168. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/syrgkanis16.html.
* [Vap82] V. Vapnik. _Estimation of Dependencies Based on Empirical Data_. Springer-Verlag, New York, 1982. doi:10.1007/0-387-34239-7.
* [VC71] V. Vapnik and A. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability and its Applications_, 16(2):264-280, 1971. doi:10.1137/1116025.
* [VC74] V. Vapnik and A. Chervonenkis. _Theory of Pattern Recognition_. Nauka, Moscow, 1974.

**Appendices**

## Appendix A Proof of Lower Bound for Littlestone Classes

Proof of Theorem 3.1.: Let \(T\) be a Littlestone tree of depth \(d\) that is shattered by \(\mathcal{H}\), and let \(\mathcal{H}_{1}\subseteq\mathcal{H}\) be a collection of \(2^{d+1}\) functions that witness the shattering. \(T\) contains \(n_{T}=2^{d+1}-1\) nodes. The adversary selects the sequence

\[x_{1},x_{2},\dots,x_{n}\]

consisting of the first \(n\) nodes of \(T\) in breadth-first order (if \(n>n_{T}\), then the adversary chooses the suffix \(x_{n_{T}+1},\dots,x_{n}\) arbitrarily). For each time step \(t\in[n]\), let \(\mathcal{H}_{t}\) denote the version space, i.e., the subset of \(\mathcal{H}_{1}\) that is consistent with all previously-assigned labels. Namely, for any \(t>1\),

\[\mathcal{H}_{t}=\{h\in\mathcal{H}_{1}:\;(\forall s\in[t-1]:\;h(x_{s})=y_{s}) \}\,.\]

Similarly, for each \(b\in\{0,1\}\), let \(\mathcal{H}_{t,b}=\{h\in\mathcal{H}_{t}:\;h(x_{t})=b\}\).

``` send\(x_{1},\dots,x_{n}\) to learner \(k\gets 1\) for\(t\gets 1,2,\dots,n\): \(m_{k}\gets 2^{2^{2k}}\) \(\varepsilon_{t}\gets 1/m_{k}\) \(r_{t}\leftarrow|\mathcal{H}_{t,1}|/|\mathcal{H}_{t}|\) receive\(\hat{y}_{t}\) from learner \(y_{t}\leftarrow\left\{\begin{array}{ll}1-\hat{y}_{t}&r_{t}\in[\varepsilon_{t}, 1-\varepsilon_{t}]\\ 0&r_{t}\in[0,\varepsilon_{t})\\ 1&r_{t}\in(1-\varepsilon_{t},1]\end{array}\right.\) send\(y_{t}\) to learner if\(r_{t}\in[\varepsilon_{t},1-\varepsilon_{t}]\): \(k\gets k+1\) ```

Algorithm 1: An adversary that forces \(\Omega(\log(\mathsf{LD}(\mathcal{H})))\) mistakes.

The adversary operates according to Algorithm 1. Conceptually, at each time step \(t\in[n]\), if \(\mathcal{H}_{t}\) is very unbalanced, meaning that a large majority of the functions in \(\mathcal{H}_{t}\) assign the same value to \(x_{t}\), then the adversary chooses \(y_{t}\) to be that value. Otherwise, if \(\mathcal{H}_{t}\) is fairly balanced, the adversary forces a mistake. Note that if \(\mathcal{H}_{t}\) is fairly balanced then the adversary can force a mistake without violating \(\mathcal{H}\)-realizability.

We now argue that using this strategy, the adversary forces \(\Omega(\log(d))\) mistakes. Let \(F=\{t_{1},t_{2},\dots\}=\{t\in[n]:\;r_{t}\in[\varepsilon_{t},1-\varepsilon_{t }]\}\) be the set of time steps where the adversary forces a mistake. Note that in the for-loop in Algorithm 1, the value of \(k\) at the beginning of iteration \(t_{k}\) is \(k\) (e.g., at the beginning of iteration \(t_{3}\), \(k=3\)).

We argue by induction that for any \(k\in\mathbb{N}\), if \(m_{k}:=2^{2^{2k}}\leq n\) then:

1. \(|F|\geq k\) and \(t_{k}\leq m_{k}\); and
2. \(|\mathcal{H}_{t_{k}}|\geq(1/m_{k})^{2}\cdot|\mathcal{H}_{1}|\).

The base case is immediate for \(t_{1}=1\in F\). For the induction step, assuming that Items 1 and 2 hold for some \(k\in\mathbb{N}\) such that \(m_{k+1}\leq n\), we show that they also hold for \(k+1\). For Item 1, assume for contradiction that \(t\notin F\) for all \(t\) such that \(t_{k}<t\leq m_{k+1}\).

For each \(t\), \(t_{k}<t\leq m_{k+1}\), the definition of \(r_{t}\) and the adversary's labeling strategy imply that the label \(y_{t}\) agrees with at least a \((1-\varepsilon_{t})\)-majority of the functions in the version space \(\mathcal{H}_{t}\). Hence,

\[\big{|}\mathcal{H}_{m_{k+1}}\big{|} \geq|\mathcal{H}_{t_{k}}|\cdot\prod_{t=t_{k}+1}^{m_{k+1}}(1- \varepsilon_{t})\] \[=|\mathcal{H}_{t_{k}}|\cdot(1-1/m_{k+1})^{m_{k+1}-t_{k}}\] \[\geq|\mathcal{H}_{t_{k}}|\cdot(1-1/m_{k+1})^{m_{k+1}}\] \[\geq|\mathcal{H}_{1}|\cdot(1/m_{k})^{2}\cdot(1-1/m_{k+1})^{m_{k+ 1}}\] (Induction hypothesis for Item 2) \[\geq|\mathcal{H}_{1}|\cdot(1/m_{k})^{2}\cdot(1/4)\] \[=|\mathcal{H}_{1}|\cdot 2^{-2^{2k+1}-2}.\] (6)

Observe that for every \(t\in[n]\), if \(x_{t}\) is a node with depth \(\ell\) in \(T\) (i.e., the shortest path from the root to \(x_{t}\) contains \(\ell\) edges), then there exists an 'active' node \(x_{\ell}^{*}\) with the same depth \(\ell\) in \(T\) such that the version space \(\mathcal{H}_{t}\) contains only functions from \(\mathcal{H}_{1}\) that are consistent with the labels along the path from the root of \(T\) to \(x_{\ell}^{*}\). Namely, \(\mathcal{H}_{t}\) is a subset of the \(2^{d-\ell+1}\) functions in \(\mathcal{H}_{1}\) that witness the shattering of the subtree \(T_{\ell}\) of \(T\) that is rooted at \(x_{\ell}^{*}\). In particular, the depth (distance from the root) of node \(x_{m_{k+1}}\) is \(\log\left(2^{2^{2(k+1)}}\right)=2^{2k+2}\), so

\[\big{|}\mathcal{H}_{m_{k+1}}\big{|}\leq 2^{d-2^{2k+2}+1}=2^{d+1}\cdot 2^{-2^{2k+2} }=|\mathcal{H}_{1}|\cdot 2^{-2^{2k+2}}.\] (7)

Combining Eqs. (6) and (7) yields \(2^{-2^{2k+1}-2}\leq 2^{-2^{2k+2}}\), which is a contradiction. This establishes Item 1. Item 2 follows by a similar calculation, which accounts for the fact that at time \(t_{k+1}\) the adversary forces a mistake, and this reduces the version space by a factor of at most \(\varepsilon_{t_{k+1}}\):

\[\big{|}\mathcal{H}_{t_{k+1}}\big{|} \geq|\mathcal{H}_{t_{k}}|\cdot\left(\prod_{t=t_{k}+1}^{t_{k+1}-1} (1-\varepsilon_{t})\right)\cdot\varepsilon_{t_{k+1}}\] \[\geq|\mathcal{H}_{t_{k}}|\cdot(1-1/m_{k+1})^{m_{k+1}}\cdot(1/m_{k +1})\] \[\geq|\mathcal{H}_{t_{k}}|\cdot(1/4)\cdot(1/m_{k+1})\] \[\geq|\mathcal{H}_{1}|\cdot(1/m_{k})^{2}\cdot(1/4)\cdot(1/m_{k+1})\] (Induction hypothesis for Item 2) \[=|\mathcal{H}_{1}|\cdot 2^{-2\cdot 2^{2k}}\cdot(1/4)\cdot 2^{-2^{2k+2} }=|\mathcal{H}_{1}|\cdot 2^{-6\cdot 2^{2k}-2}\] \[\geq|\mathcal{H}_{1}|\cdot 2^{-8\cdot 2^{2k}}=|\mathcal{H}_{1}| \cdot 2^{-2\cdot 2^{2(k+1)}}=|\mathcal{H}_{1}|\cdot(1/m_{k+1})^{2}.\]

This completes the induction.

To complete the proof, let \(k^{*}=\min\left\{\lfloor\log(d)/2\rfloor,\lfloor\log\log(n)/2\rfloor\right\}\). Then \(m_{k^{*}}\leq 2^{d}<2^{d+1}-1=n_{T}\), so \(T\) contains at least \(m_{k^{*}}\) nodes. Additionally, \(m_{k^{*}}\leq n\), so Item 1 implies that \(|F|\geq k^{*}\), namely, the adversary can force at least \(k^{*}\) mistakes, as desired. 

## Appendix B Multiclass Trichotomy

The following generalization of the Littlestone dimesion to the multiclass setting is due to [1].

**Definition B.1** (Multiclass Littlestone Dimension).: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets and let \(d\in\mathbb{N}\). A Littlestone tree of depth \(d\) with domain \(\mathcal{X}\) and label set \(\mathcal{Y}\) is a set_

\[T=\left\{(x_{u},y_{u\circ 0},y_{u\circ 1})\in\mathcal{X}\times\mathcal{Y} \times\mathcal{Y}:\ u\in\bigcup_{s=0}^{d}\{0,1\}^{s}\ \wedge\ y_{u\circ 0}\neq y_{u\circ 1} \right\},\] (8)

_where '\(\circ\)' denotes string concatenation. Let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). We say that \(\mathcal{H}\) shatters a tree \(T\) as in Eq. (8) if for every \(u\in\{0,1\}^{d+1}\) there exists \(h_{u}\in\mathcal{H}\) such that_

\[\forall i\in[d+1]:\ h(x_{u_{\leq i-1}})=y_{u_{\leq i}}.\]

_The Littlestone dimension of \(\mathcal{H}\), denoted \(\mathsf{LD}(\mathcal{H})\), is the supremum over all \(d\in\mathbb{N}\) such that there exists a Littlestone tree of depth \(d\) with domain \(\mathcal{X}\) and label set \(\mathcal{Y}\) that is shattered by \(\mathcal{H}\)._The Natarajan dimension is a popular generalization of the VC dimension to the multiclass setting.

**Definition B.2** ([14]).: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\), let \(d\in\mathbb{N}\), and let \(X=\{x_{1},\ldots,x_{d}\}\subseteq\mathcal{X}\). We say that \(\mathcal{H}\) Natarajan-shatters \(X\) if there exist \(f_{0},f_{1}:\ X\to\mathcal{Y}\) such that:_

1. \(\forall x\in X:\ f_{0}(x)\neq f_{1}(x)\)_; and_
2. \(\forall A\subseteq X\ \exists h\in\mathcal{H}\ \forall x\in X:\ h(x)=f_{ \mathds{1}(x\in A)}(x)\)_._

_The Natarajan dimension of \(\mathcal{H}\) is \(\mathsf{ND}(\mathcal{H})=\sup\ \{|X|:\ X\subseteq\mathcal{X}\ \text{finite}\ \wedge\ \mathcal{H}\ \text{ Natarajan-shatters}\ X\}\)._

We show the following generalization of Theorem 4.1 for the multiclass setting.

**Theorem B.3** (**Formal Version of Theorem 5.1)**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets with \(k=|\mathcal{Y}|<\infty\), let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\), and let \(n\in\mathbb{N}\) such that \(n\leq|\mathcal{X}|\)._

1. _If_ \(\mathsf{ND}(\mathcal{H})=\infty\) _then_ \(M(\mathcal{H},n)=n\)_._
2. _Otherwise, if_ \(\mathsf{ND}(\mathcal{H})=d<\infty\) _and_ \(\mathsf{LD}(\mathcal{H})=\infty\) _then_ \[\max\{\min\{d,n\},\lfloor\log(n)\rfloor\}\leq M(\mathcal{H},n)\leq O(d\log(nk/d )).\] (9) _The_ \(\Omega(\cdot)\) _and_ \(O(\cdot)\) _notations hide universal constants that do not depend on_ \(\mathcal{X}\)_,_ \(\mathcal{Y}\) _or_ \(\mathcal{H}\)_._
3. _Otherwise, there exists a number_ \(C(\mathcal{H})\in\mathbb{N}\) _(that depends on_ \(\mathcal{X}\)_,_ \(\mathcal{Y}\) _and_ \(\mathcal{H}\) _but does not depend on_ \(n\)_) such that_ \(M(\mathcal{H},n)\leq C(\mathcal{H})\)_._

The proof of Theorem B.3 uses the following generalization of the Sauer-Shelah-Perles lemma.

**Theorem B.4** ([14]; Corollary 5 in [10]).: _Let \(d,n,k\in\mathbb{N}\), let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets of cardinality \(n\) and \(k\) respectively, and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) such that \(\mathsf{ND}(\mathcal{H})\leq d\). Then_

\[|\mathcal{H}|\leq\sum_{i=0}^{d}\binom{n}{i}\binom{k+1}{2}^{i}\leq\left(\frac{ enk^{2}}{d}\right)^{d}.\]

Proof of Theorem b.3.: Items 1 and 3 and the \(\min\{d,n\}\) lower bound in Item 2 follow similarly to the corresponding items in Theorem 4.1. The upper bound in Item 2 also follows similarly to the corresponding item in Theorem 4.1, except that it uses Theorem B.4 instead of the Sauer-Shelah-Perles lemma. The \(\lfloor\log(n)\rfloor\) lower bound in Item 2 follows from Theorem D.5 and Claim D.4. 

## Appendix C Combinatorics of Trees

In this section we present a simple lemma from Ramsey theory about trees that is used for proving Theorem D.5. We start with a generalized definition of subtrees.

**Definition C.1**.: _Let \(X\) be a finite set and let \((X,\preceq)\) be a partial order relation. For \(p,c\in X\), we say that \(c\) is a child of \(p\) if \(p\preceq c\) and there does not exist \(m\in X\) such that \(p\preceq m\preceq c\). We say that \(z\in X\) is a leaf if there exists no \(x\in X\) such that \(z\preceq x\). \((X,\preceq)\) is a binary tree if every non-leaf \(x\in X\) has precisely \(2\) children. The depth of \(z\in X\) is the largest \(d\in\mathbb{N}\) for which there exist distinct \(x_{1},\ldots,x_{d}\in X\) such that \(x_{1}\preceq x_{2}\preceq\cdots\preceq x_{d}\preceq z\). For \(d\in\mathbb{N}\), we say that \((X,\preceq)\) is a complete binary tree of depth \(d\) if \((X,\preceq)\) is a binary tree and all the leaves in \(X\) have depth \(d\). We say that a partial order \((X^{\prime},\preceq^{\prime})\) is a subtree of \((X,\preceq)\) if \(X^{\prime}\subseteq X\), and \(\forall a,b\in X^{\prime}:\ a\preceq^{\prime}b\iff a\preceq b\)._

**Lemma C.2** (Lemma 16 in [1]).: _Let \(p,q\in\mathbb{R}\) be non-negative such that \(p+q\in\mathbb{N}\). Let \(T=(X,\preceq)\) be a complete binary tree of depth \(d=p+q-1\), and let \(f:\ X\to\{0,1\}\). Then at least one of the following statements holds:_

* \(T\) _has a_ \(0\)_-monochromatic complete binary subtree of depth at least_ \(p\)_. Namely, there exists_ \(T^{\prime}=(X^{\prime},\preceq^{\prime})\) _such that_ \(T^{\prime}\) _is a subtree of_ \(T\)_,_ \(T^{\prime}\) _is a complete binary tree of depth at least_ \(p\)_, and_ \(f(x)=0\) _for all_ \(x\in X^{\prime}\)_._
* \(T\) _has a_ \(1\)_-monochromatic complete binary tree subtree of depth at least_ \(q\)_._

For completeness, we include a proof of this lemma.

Proof of Lemma c.2.: We prove the claim by induction on the depth \(d\). The base case of \(d=0\) (a tree with a single node) is immediate. For the induction step, let \(a\) denote the root of \(T\), and let and \(T_{r}\) denote the subtrees of \(T\) of depth \(d-1\) consisting of all descendants of the left and right child of \(a\) respectively. Assume that \(f(a)=0\). If \(T_{\ell}\) or \(T_{r}\) contain a \(1\)-monochromatic subtree of depth at least \(q\), then we are done. Otherwise, by the induction hypothesis, both trees contain a \(0\)-monochromatic subtree of depth at least \(p-1\). Joining these two subtrees as children of the root \(a\) yields a \(0\)-monochromatic subtree of depth at least \(p\), as desired. The proof for the case \(f(a)=1\) is similar. 

We use the following corollary of Lemma C.2.

**Lemma C.3**.: _Let \(k,d\in\mathbb{N}\). Let \(T=(X,\preceq)\) be a complete binary tree of depth \(d\in\mathbb{N}\), and let \(f:\;X\rightarrow[k]\). Then \(T\) has an \(f\)-monochromatic complete binary subtree \(T^{\prime}=(X^{\prime},\preceq^{\prime})\) of depth at least_

\[d^{\prime}=\frac{d+1}{2^{\lceil\log(k)\rceil}}.\]

_Namely, there exists \(T^{\prime}\) such that \(T^{\prime}\) is a subtree of \(T\), \(T^{\prime}\) is a complete binary tree of depth at least \(d^{\prime}\), and \(|\{f(a):\;a\in X^{\prime}\}|=1\)._

Proof of Lemma c.3.: We will show that for any \(b\in\mathbb{N}\), if \(k\leq 2^{b}\) then there exists an \(f\)-monochromatic subtree of \(T\) of depth at least

\[\frac{d+1}{2^{b}}.\]

This implies the lemma, which corresponds to the special case \(b=\lceil\log(k)\rceil\).

We proceed by induction on \(b\). The base case \(b=1\) follows from Lemma C.2. For the induction step, we assume that the claim holds for \(b\) and prove that it holds for \(b+1\). Namely, we show that if \(f:\;X\rightarrow[k]\) and \(k\leq 2^{b+1}\) then there exists an \(f\)-monochromatic subtree of depth at least \((d+1)/2^{b+1}\).

Define \(g:\;X\rightarrow\{1,2\}\) by \(g(x)=1+(f(x)\mod 2)\). By Lemma C.2, there exists a \(g\)-monochromatic complete binary subtree \(T_{0}=(X_{0},\preceq)\) of \(T\) of depth at least \((d+1)/2\). In particular \(|\{f(x):\;x\in X_{0}\}|\leq 2^{b}\). By invoking the induction hypotheses on \(T_{0}\), there exists a complete binary subtree of \(T_{0}\) that is \(f\)-monochromatic and has depth at least

\[\frac{\frac{d+1}{2}+1}{2^{b}}>\frac{d+1}{2^{b+1}},\]

as desired. 

## Appendix D Multiclass Threshold Bounds

**Definition D.1**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, let \(X=\{x_{1},\ldots,x_{t}\}\subseteq\mathcal{X}\), and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). We say that \(X\) is threshold-shattered by \(\mathcal{H}\) if there exist distinct \(y_{0},y_{1}\in\mathcal{Y}\) and functions \(h_{1},\ldots,h_{t}\in\mathcal{H}\) such that \(\overline{h_{i}(x_{j})}=y_{1(j\leq i)}\). The threshold dimension of \(\mathcal{H}\), denoted \(\mathsf{TD}(\mathcal{H})\), is the supremum of the set of integers \(t\) for which there exists a threshold-shattered set of cardinality \(t\)._

We introduce the following generalization of the threshold dimension.

**Definition D.2**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, let \(X=\{x_{1},\ldots,x_{t}\}\subseteq\mathcal{X}\), and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). We say that \(X\) is multi-class threshold-shattered by \(\mathcal{H}\) if there exist \(y_{1},y^{\prime}_{1}\ldots,y_{t},y^{\prime}_{t}\in\mathcal{Y}\) such that \(y_{i}\neq y^{\prime}_{j}\) for all \(i,\bar{j}\in[t]\), and there exist functions \(h_{1},\ldots,h_{t}\in\mathcal{H}\) such that_

\[h_{i}(x_{j})=\left\{\begin{array}{ll}y_{i}&(j\leq i)\\ y^{\prime}_{j}&(j>i).\end{array}\right.\]

_The multi-class threshold dimension of \(\mathcal{H}\), denoted \(\mathsf{MTD}(\mathcal{H})\), is the supremum of the set of integers \(t\) for which there exists a threshold-shattered set of cardinality \(t\)._

See Table 1 for an illustration of this definition.

**Claim D.3**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, \(k=|\mathcal{Y}|<\infty\), and let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). Then \(\mathsf{TD}(\mathcal{H})\geq\lfloor\mathsf{MTD}(\mathcal{H})/k^{2}\rfloor\)._

Proof of Claim D.3.: The proof follows from two applications of the pigeonhole principle.

**Claim D.4**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets, let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) such that \(d=\mathsf{TD}(\mathcal{H})<\infty\), and let \(n\in\mathbb{N}\). Then_

\[M(\mathcal{H},n)\geq\min\left\{\left\lfloor\log(d)\right\rfloor,\left\lfloor \log(n)\right\rfloor\right\}.\]

The proof of Claim D.4 is similar to that of Claim 3.4.

**Theorem D.5**.: _Let \(\mathcal{X}\) and \(\mathcal{Y}\) be sets with \(k=|\mathcal{Y}|<\infty\), let \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\). If \(\mathsf{LD}(\mathcal{H})=\infty\) then \(\mathsf{MTD}(\mathcal{H})=\infty\)._

Proof of Theorem D.5.: Let \(f_{k}(d)\) be the largest number such that every class with Littlestone dimension \(d\) has multi-class threshold dimension at least \(f_{k}(d)\). We show by induction on \(d\) that \(f_{k}\) satisfies the following recurrence relation:

\[f_{k}(d)\geq\left\{\begin{array}{ll}1&d=1\\ 1+f_{k}(\lceil d/2k\rceil-1)&d>1\end{array}\right..\]

In particular, this implies that \(f_{k}(d)\xrightarrow{d\to\infty}\infty\), which implies the theorem.

The base case \(d=\mathsf{LD}(\mathcal{H})=1\) is immediate. For the induction step, we assume the relation holds whenever \(\mathsf{LD}(\mathcal{H})\in[d-1]\), and prove that it holds for \(\mathsf{LD}(\mathcal{H})=d\). Let \(T\) be a Littlestone tree of depth \(d\) that is shattered by \(\mathcal{H}\). Fix \(h\in\mathcal{H}\). Then \(h\) is a \(k\)-cloring of the nodes of \(T\). By Lemma C.3, there exists an \(h\)-monochromatic subtree \(T^{\prime}\subseteq T\) of depth at least \(\lceil d/2k\rceil\). Let \(y\) be the color assigned by \(h\) to all nodes of \(T^{\prime}\). \(T^{\prime}\) is shattered by \(\mathcal{H}\), so there exists a child \(c\) of the root \(x\) of \(T^{\prime}\) such that edge from \(x\) to \(c\) is labeled by some value \(y^{\prime}\neq y\). Let \(\mathcal{H}^{\prime}=\{g\in\mathcal{H}:\ g(x)=y^{\prime}\}\). \(\mathcal{H}^{\prime}\) shatters the subtree rooted at \(c\), so \(\mathsf{LD}(\mathcal{H}^{\prime})\geq\lceil d/2k\rceil-1\). By the induction hypothesis, there exist \(x_{1},\ldots,x_{s}\) for \(s=f_{k}(\lceil d/2k\rceil-1)\) that are multi-class threshold shattered by functions \(h_{1},\ldots,h_{s}\in\mathcal{H}^{\prime}\).

By construction, the set \(X=\{x_{1},\ldots,x_{s},x_{s+1}=x\}\) is multi-class threshold shattered by \(\{h_{1},\ldots,h_{s},h_{s+1}=h\}\), because \(h_{s+1}(x_{j})=y\) for all \(j\in[s+1]\), and \(h_{i}(x_{s+1})=y^{\prime}\) for all \(i\in[s]\). Hence, \(f_{k}(d)\geq s+1=1+f_{k}(\lceil d/2k\rceil-1)\), as desired. 

## Appendix E Proof of Agnostic Lower Bound

The lower bound in Theorem 6.1 is derived using an anti-concentration technique from Lemma 14 of [1]. Specifically, this technique uses the following inequality.

**Theorem E.1** (Khinchine's inequality; Lemma 8.2 in [1]).: _Let \(k\in\mathbb{N}\), and let \(\sigma_{1},\sigma_{2},\ldots,\sigma_{k}\) be random variables sampled independently and uniformly at random from \(\{\pm 1\}\). Then_

\[\mathbb{E}\!\left[\left|\sum_{i\in k}\sigma_{k}\right|\right]\geq\sqrt{k/2}.\]

Proof of lower bound in Theorem 6.1.: Let \(d=\mathsf{VC}(\mathcal{H})\). Let \(\{x_{1}^{*},\ldots,x_{d}^{*}\}\subseteq\mathcal{X}\) be a set of cardinality \(d\) that is \(\mathsf{VC}\)-shattered by \(\mathcal{H}\). Let \(k\in\mathbb{N}\) be the largest integer such that \(kd\leq n\).

Let \(x\in\mathcal{X}^{n}\) be a sequence consisting of \(k\) copies of the shattered set, namely,

\[(x_{1},\ldots,x_{kd})=\left(x_{1}^{1},x_{1}^{2},\ldots,x_{1}^{k},x_{2}^{1},x_{ 2}^{2},\ldots,x_{2}^{k},\ldots,x_{d}^{1},x_{d}^{2},\ldots,x_{d}^{k}\right),\]

such that \(x_{i}^{j}=x_{i}^{*}\) for all \(i\in[d]\) and \(j\in[k]\). If \(kd<n\) then the remaining \(n-kd\) elements of \(x\) may be arbitrary.

\begin{table}
\begin{tabular}{c|c|c|c|c|c}  & \(x_{1}\) & \(x_{1}\) & \(x_{3}\) & \(x_{4}\) & \(x_{5}\) \\ \hline \(h_{1}\) & \(y_{1}\) & \(y_{2}^{*}\) & \(y_{3}^{*}\) & \(y_{4}^{*}\) & \(y_{5}^{*}\) \\ \hline \(h_{2}\) & \(y_{2}\) & \(y_{2}\) & \(y_{3}^{*}\) & \(y_{4}^{*}\) & \(y_{5}^{*}\) \\ \hline \(h_{3}\) & \(y_{3}\) & \(y_{3}\) & \(y_{3}\) & \(y_{4}^{*}\) & \(y_{5}^{*}\) \\ \hline \(h_{4}\) & \(y_{4}\) & \(y_{4}\) & \(y_{4}\) & \(y_{4}\) & \(y_{5}^{*}\) \\ \hline \(h_{5}\) & \(y_{5}\) & \(y_{5}\) & \(y_{5}\) & \(y_{5}\) & \(y_{5}\) \\ \hline \end{tabular}
\end{table}
Table 1: An illustration of Definition D.2. The table shows a collection of points \(\{x_{1},\ldots,x_{5}\}\) that are multi-class threshold shattered by functions \(\{h_{1},\ldots,h_{5}\}\).

Consider a randomized adversary that selects the sequence \(x\), and chooses all labels to be i.i.d. uniform random bits. For each \(i\in[d]\) and \(j\in[k]\), let \(y_{i}^{j}=y_{(i-1)k+j}\) and \(\hat{y}_{i}^{j}=\hat{y}_{(i-1)k+j}\) denote, respectively, the labels and the predictions corresponding to \(x_{i}^{j}\). Then for any learner strategy \(A\),

\[\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{n})}[R(A,\mathcal{H },x,y)]\] \[\quad=\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{n})}\Biggl{[} \mathop{\mathbb{E}}_{\hat{y}}\Biggl{[}\sum_{i\in[n]}\mathds{1}\left(\hat{y}_{t }\neq y_{t}\right)\Biggr{]}-\min_{h\in\mathcal{H}}\sum_{i\in[n]}\mathds{1} \left(h(x_{t})\neq y_{t}\right)\Biggr{]}\] \[\quad=\frac{n}{2}-\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{ n})}\Biggl{[}\min_{h\in\mathcal{H}}\sum_{i\in[n]}\mathds{1}\left(h(x_{t})\neq y_{t} \right)\Biggr{]}\] ( \[y_{i}\perp\hat{y}_{i}\] ) \[\quad\geq\frac{kd}{2}-\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1 \}^{kd})}\Biggl{[}\min_{h\in\mathcal{H}}\sum_{i\in[d]}\sum_{j\in[k]}\mathds{1} \left(h(x_{i}^{j})\neq y_{i}^{j}\right)\Biggr{]}\] (10) \[\quad=\frac{kd}{2}-\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{ k})}\Biggl{[}\sum_{i\in[d]}\min_{h\in\mathcal{H}}\sum_{j\in[k]}\mathds{1} \left(h(x_{i}^{j})\neq y_{i}^{j}\right)\Biggr{]}\] ( \[\mathcal{H}\text{ shatters }\{x_{1}^{*},\ldots,x_{d}^{*}\}\] ) \[\quad=\sum_{i=1}^{d}\frac{k}{2}-\mathop{\mathbb{E}}_{y_{i}\sim \mathrm{U}(\{0,1\}^{k})}\Biggl{[}\min_{h\in\mathcal{H}}\sum_{j\in[k]}\mathds{1 }\left(h(x_{i}^{j})\neq y_{i}^{j}\right)\Biggr{]}\] \[\quad=\sum_{i=1}^{d}\frac{k}{2}-\mathop{\mathbb{E}}_{y_{i}\sim \mathrm{U}(\{0,1\}^{k})}\bigl{[}\min\{r_{i},k-r_{i}\}\bigr{]}\] (Let \[r_{i}=\sum_{j\in[k]}y_{i}^{j}\] ) \[\quad=\sum_{i=1}^{d}\mathop{\mathbb{E}}_{y_{i}\sim\mathrm{U}(\{0, 1\}^{k})}\biggl{[}\biggl{|}\frac{k}{2}-r_{i}\biggr{|}\biggr{]}\] \[\quad=\sum_{i=1}^{d}\mathop{\mathbb{E}}_{y_{i}\sim\mathrm{U}(\{0, 1\}^{k})}\Biggl{[}\biggl{|}\frac{k}{2}-\sum_{j\in[k]}\left(\frac{1}{2}+\frac{ \sigma_{i}^{j}}{2}\right)\biggr{|}\Biggr{]}\] (Let \[\sigma_{i}^{j}=\left\{\begin{array}{cc}1&y_{i}^{j}=1\\ -1&y_{i}^{j}=0\end{array}\right.\] ) \[\quad=\frac{1}{2}\sum_{i=1}^{d}\mathop{\mathbb{E}}_{y_{i}\sim \mathrm{U}(\{0,1\}^{k})}\Biggl{[}\biggl{|}\sum_{j\in[k]}\sigma_{i}^{j}\biggr{|} \Biggr{]}\] \[\quad\geq\frac{1}{2}\sum_{i=1}^{d}\sqrt{\frac{k}{2}}=\frac{d \sqrt{k}}{2\sqrt{2}}=\Omega\Bigl{(}\sqrt{nd}\Bigr{)},\] (11)

where the final inequality is Khinchine's inequality (Theorem E.1). To see that Inequality (10) holds, let \(h^{*}\in\operatorname*{argmin}_{h\in\mathcal{H}}\sum_{i=1}^{kd}\mathds{1} \left(h(x_{t})\neq y_{t}\right)\), and then

\[\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{n})}\Biggl{[}\min_{ h\in\mathcal{H}}\sum_{i\in[n]}\mathds{1}\left(h(x_{t})\neq y_{t}\right)\Biggr{]}\] \[\quad\leq\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{n})} \Biggl{[}\sum_{i=1}^{kd}\mathds{1}\left(h^{*}(x_{t})\neq y_{t}\right)\Biggr{]}+ \mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{n})}\Biggl{[}\sum_{i=kd+1}^{n} \mathds{1}\left(h^{*}(x_{t})\neq y_{t}\right)\Biggr{]}\] \[\quad\leq\mathop{\mathbb{E}}_{y\sim\mathrm{U}(\{0,1\}^{n})} \Biggl{[}\sum_{i=1}^{kd}\mathds{1}\left(h^{*}(x_{t})\neq y_{t}\right)\Biggr{]}+ \frac{n-kd}{2}.\] ( \[\{y_{t}\}_{t>kd}\perp h^{*}\] )

In particular, Eq. (11) implies that for every learner strategy \(A\) there exists \(y\in\{0,1\}^{n}\) such that \(R(A,\mathcal{H},x,y)=\Omega\Bigl{(}\sqrt{nd}\Bigr{)}\).