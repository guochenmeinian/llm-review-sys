# Optimal and Fair Encouragement Policy Evaluation and Learning

Angela Zhou

Department of Data Sciences and Operations

University of Southern California

zhoua@usc.edu

###### Abstract

In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal treatment assignments are merely suggestions when humans make the final treatment decisions. On the other hand, there can be different heterogeneity in both the actual response to treatment and final treatment decisions given recommendations. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When decision-makers have equity- for fairness-minded preferences over both access and average outcomes, the optimal decision rule changes due to these differing heterogeneity patterns. We study identification and improved/robust estimation under potential violations of positivity. We consider fairness constraints such as demographic parity in treatment take-up, and other constraints, via constrained optimization. We develop a two-stage, online learning-based algorithm for solving over parametrized policy classes under general constraints to obtain variance-sensitive regret bounds. Our framework can be extended to handle algorithmic recommendations under an often-reasonable covariate-conditional exclusion restriction, using our robustness checks for lack of positivity in the recommendation.

## 1 Introduction

The intersection of causal inference and machine learning for heterogeneous treatment effect estimation can improve public health, increase revenue, and improve outcomes by personalizing treatment decisions, such as medications, e-commerce platform interactions, and social interventions, to those who benefit from it the most [4, 30, 35, 49]. But, in many important settings, we do not have direct control over treatment, and can only optimize over _encouragements_, or _recommendations_ for treatment. For example, in e-commerce, companies can rarely _compel_ users to sign up for certain services, rather _nudge_ or _encourage_ users to sign up via promotions and offers. When we are interested in optimizing the effects of signing up - or other voluntary actions beyond a platform's control - on important final outcomes such as revenue, we therefore need to consider _fairness-constrained optimal encouragement designs_. Often human expert oversight is required in the loop in important settings where ensuring _fairness in machine learning_[6] is also of interest: doctors prescribe treatment from recommendations [31], managers and workers combine their expertise to act based on decision support [8], and in the social sector, caseworkers assign to beneficial programs based on recommendations from risk scores that support triage [14, 19, 48].

The human in the loop requires new methodology for optimal encouragement designs because

when the human in the loop makes the final prescription, algorithmic recommendations do not have direct causal effects on outcomes; they change the probability of treatment assignment.

On the other hand, this is analogous to the well-understood notion of _non-compliance/non-adherence_ in randomized controlled trials in the real world [22; 21]. For example, patients who are prescribed treatment may not actually take medication. A common strategy is to conduct an _intention-to-treat_ analysis: under assumptions of no unobserved confounders affecting treatment take-up and outcome, we may simply view encouragement as treatment. But, in the case of prediction-informed decisions in social settings, if we are concerned about _access to the intervention_ in addition to _utility of the policy over the population_, finer-grained analysis is warranted. If an outcome-optimal policy results in wide disparities in access, for example in marginalized populations not taking up incentives for healthy food due to lack of access in food deserts, or administrative burden that screens out individuals applying for social services that could benefit the most, this could be a serious concern for decision-makers. We ultimately may seek optimal decision rules that improve disparities in treatment access. In contrast, previous work in algorithmic accountability primarily focuses on auditing _recommendations_, but not both the access and efficacy achieved under the final decision rule. Therefore, previous methods can fall short in mitigating potential disparities.

Our contributions are as follows: we characterize optimal and resource fairness-constrained optimal decision rules, develop statistically improved estimators and robustness checks for the setting of algorithmic recommendations with sufficiently randomized decisions. We also develop methodology for optimizing over a constrained policy class with less conservative out-of-sample fairness constraint satisfaction by a two-stage procedure, and we provide sample complexity bounds. We assess improved recommendation rules in a stylized case study of optimizing recommendation of supervised release in the PSA-DMF pretrial risk-assessment tool while reducing surveillance disparities.

## 2 Related Work

In the main text, we briefly highlight the most relevant methodological and substantive work and defer additional discussion to the appendix.

**Optimal encouragement designs/policy learning with constraints.** There is extensive literature on off-policy evaluation and learning, empirical welfare maximization, and optimal treatment regimes [5; 49; 35; 30]. [39] studies an optimal individualized encouragement design, though their focus is on optimal individualized treatment regimes with instrumental variables. [27] study fairness in pricing, and some of the desiderata in that setting on revenue (here, marginal welfare) and demand (take-up) are again relevant here, but in a more general setting beyond pricing. The most closely related work in terms of problem setup is the formulation of "optimal encouragement designs" in [39]. However, they focus on knapsack resource constraints, which have a different solution structure than fairness constraints. Their outcome models in regression adjustment are conditional on the recommended/not recommended partitions which would not allow our fairness constraints that introduce treatment- and group-dependent costs. [44] has studied uniform feasibility in constrained resource allocation, but without encouragement or fairness. [9] studies robust extrapolation in policy learning from algorithmic recommendation, but not fairness. Our later case study is on supervised release, where there is a lot of randomness in final treatment decisions, rather than pretrial detention.

**Fair off-policy learning** We highlight some most closely related works in off-policy learning (omitting works in the sequential setting). [37] studies high-probability fairness constraint satisfaction. [29] studies doubly-robust causal fair classification, while others have imposed deterministic resource constraints on the optimal policy formulation [13]. [26] studies (robust) bounds for treatment responders in binary outcome settings; this desiderata is coupled to classification notions of direct treatment. Again, our focus is on modeling the fairness implications of non-adherence. Indeed, in order to provide general algorithms and methods, we do build on prior fair classification literature. A different line of work studies "counterfactual" risk assessments which models a different concern.

**Other causal methodology for intention-to-treat** We focus on deriving estimators for intention-to-treat analyses in view of fairness constraints (which result in group-specific welfare weights). Our interest is in imposing separate desiderata on treatment realizations under non-compliance; but we don't conduct instrumental variable inference and we assume unconfoundedness holds. We argue ITT is policy-relevant whereas complier-strata specific analysis is less policy-relevant since the compliers are unknown. Since our primary interest is in characterizing fair optimal decision rules, we don't model this as a mediation analysis problem (isolating the impact of recommendation even under the same ultimate treatment in a nested counterfactual), which may be more relevant for descriptive characterization. [27] studies multi-objective desiderata for pricing and notes intention-to-treat structure in pricing, but not fairness considerations in more general problems. A related literature studies principal stratification [24], which has similar policy-relevance disadvantages regarding interpretability as complier analysis does.

## 3 Problem Setup

We briefly describe the problem setup. We work in the Neyman-Rubin potential outcomes framework for causal inference [40]. We define the following:

* recommendation flag \(R\in\{0,1\}\), where \(R=1\) means encouraged/recommended. (We will use the terms encouragement/recommendation interchangeably).
* treatment \(T(R)\in\mathcal{T}\), where \(T(r)=1\) indicates the treatment decision was \(1\) when the recommendation reported \(r\).
* outcome \(Y(t(r))\) is the potential outcome under encouragement \(r\) and treatment \(t\).

Regarding fairness, we will be concerned about disparities in utility and treatment benefits (resources or burdens) across different groups, denoted \(A\in\{a,b\}\). (For notational brevity, we may generically discuss identification/estimation without additionally conditioning on the protected attribute). For example, recommendations arise from binary high-risk/low-risk labels of classifiers. In practice, in consequential domains, classifier decisions are rarely automated, rather used to inform humans in the loop. The human expert in the loop decides whether or not to assign treatment. For binary outcomes, we will interpret \(Y(t(r))=1\) as the positive outcome, and when treatments are also binary, we may further develop analogues of fair classification criteria. We let \(c(r,t,y)\colon\{0,1\}^{3}\mapsto\mathbb{R}\) denote the cost function for \(r\in\{0,1\},t\in\mathcal{T},y\in\{0,1\}\), which may sometimes be abbreviated \(c_{rt}(y)\). We discuss identification and estimation based on the following recommendation, treatment propensity, and outcome models:

\[e_{r}(X,A)\coloneqq P(R=r\mid X,A),\ \ p_{t|r}(X,A)\coloneqq P(T=t \mid R=r,X,A),\] \[\mu_{r,t}(X,A)\coloneqq\mathbb{E}[c_{rt}(Y)\mid R=r,T=t,X,A]= \mathbb{E}[c_{rt}(Y)\mid T=t,X,A]\coloneqq\mu_{t}(X,A)\text{ (asn.2)}\]

We are generally instead interested in _personalized recommendation rules_\(\pi(r\mid X)=\pi_{r}(X)\) which describes the probability of assigning the recommendation \(r\) to covariates \(X\). The average encouragement effect is the difference in average outcomes if we refer everyone vs. no one, while the encouragement policy value \(V(\pi)\) is the population expectation induced by the potential outcomes and treatment assignments realized under a recommendation policy \(\pi\).

\[AEE=\mathbb{E}[Y(T(1))-Y(T(0))],\qquad V(\pi)=\mathbb{E}[c(\pi,T(\pi),Y(\pi))].\]

Because algorithmic decision makers may be differentially responsive to recommendation, and treatment effects may be heterogeneous, the optimal recommendation rule may differ from the (infeasible) optimal treatment rule when taking constraints into account or for simpler policy classes.

**Assumption 1** (Consistency and SUTVA ).: \(Y_{i}=Y_{i}(T_{i}(R_{i})).\)__

**Assumption 2** (Conditional exclusion restriction).: \(Y(T(R))\perp\!\!\!\perp R\mid T,X,A\)_._

**Assumption 3** (Unconfoundedness).: \(Y(T(r))\perp\!\!\!\perp T(r)\mid X,A\)_._

**Assumption 4** (Stable responsivities under new recommendations).: \(P(T=t\mid R=r,X)\) remains fixed from the observational to the future dataset.__

**Assumption 5** (Decomposable costs).: \(c(r,t,y)=c_{r}(r)+c_{t}(t)+c_{y}(y)\)__

**Assumption 6** (Overlap).: \(\nu_{r}\leq e_{r}(X,A)\leq 1-\nu_{r};\ \ \nu_{t}\leq p_{t|r}(X,A)\leq 1-\nu_{t}; \nu_{r},\nu_{t}\leq 0\)__

Our key assumption beyond standard causal inference assumptions is the conditional exclusion restriction assumption 2, i.e. that conditional on observable information \(X\), the recommendation has no causal effect on the outcome beyond its effect on increasing treatment probability. This assumes that all of the covariate information that is informative of downstream outcomes is measured. Although this may not exactly hold in all applications, stating this assumption is also a starting point for sensitivity analysis under violations of it [25]. Assuming assumption 6 is like assuming we consider a randomized controlled trial with nonadherence. But later we give arguments using robustness to go beyond this, leveraging our finer-grained characterization.

Assumption 4 is a structural assumption that limits our method to most appropriately re-optimize over small changes to existing algorithmic recommendations. This is also required for the validity of intention-to-treat analyses. For example, \(p_{0|1}(x)\) (disagreement with algorithmic recommendation) could be a baseline algorithmic aversion. Not all settings are appropriate for this assumption. We don't assume micro-foundations on how or why human decision-makers were deviating from algorithmic recommendations, but take these patterns as given. One possibility for relaxing this assumption is via conducting sensitivity analysis, i.e. optimizing over unknown responsivity probabilities near known ones.

Later on, we will be particularly interested in constrained formulations on the intention-to-treat effect that impose separate desiderata on outcomes under treatment, as well as treatment.

## 4 Method

We consider two settings: in the first, \(R\) is (as-if) randomized and satisfies overlap. Then \(R\) can be interpreted as intention to treat or prescription, whereas \(T\) is the actual realization thereof. We study identification of optimal encouragement designs with potential constraints on treatment or outcome utility patterns by group membership. We characterize optimal unconstrained/constrained decisions under resource parity. In the second, \(R\) is an algorithmic recommendation that does not satisfy overlap in recommendation (but there is sufficient randomness in human decisions to satisfy overlap in treatment): we derive robustness checks in this setting by being robust. First we discuss causal identification in optimal encouragement designs.

**Proposition 1** (Regression adjustment identification).: \[\mathbb{E}[c(\pi,T(\pi),Y(\pi))]=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E} [\pi_{r}(X)\mu_{t}(X)p_{t|r}(X)]\]

Proof of Proposition 1.: \[\mathbb{E}[c(\pi,T(\pi),Y(\pi))] =\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)\mathbb{E }[\mathbb{I}\left[T(r)=t\right]c_{rt}(Y(r,t))\mid R=r,X]]\] \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)P(T=t \mid R=r,X)\mathbb{E}[c_{rt}(Y(r,t))\mid R=r,X]]\] \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)P(T=t \mid R=r,X)\mathbb{E}[c_{rt}(Y)\mid T=t,X]]\]

where the last line follows by the conditional exclusion restriction (Assumption 2) and consistency (Assumption 1). 

Resource-parity constrained optimal decision rulesWe consider a resource/burden parity fairness constraint:

\[V_{\epsilon}^{*}=\max_{\pi}\;\{\mathbb{E}[c(\pi,T(\pi),Y(\pi))]\colon\mathbb{E }[T(\pi)\mid A=a]-\mathbb{E}[T(\pi)\mid A=b]\leq\epsilon\}\] (1)

Enforcing absolute values, etc. follows in the standard way. Not all values of \(\epsilon\) may be feasible; in the appendix we give an auxiliary program to compute feasible ranges of \(\epsilon\). We first characterize a threshold solution when the policy class is unconstrained.

**Proposition 2** (Threshold solutions).: Define

\[\lambda^{*}\in\arg\min_{\lambda}\;\mathbb{E}[L(\lambda,X,A)+],\;\;\pi^{*}(x,u )=\mathbb{I}\{L(\lambda^{*},X,u)>0\}\]

If instead \(d(x)\) is a function of covariates \(x\) only,

\[\lambda^{*}\in\arg\min_{\lambda}\;\mathbb{E}[\mathbb{E}[L(\lambda,X,A)\mid X ]_{+}],\;\;\pi^{*}(x)=\mathbb{I}\{\mathbb{E}[L(\lambda^{*},X,A)\mid X]>0\}\]

Establishing this threshold structure (follows by duality of infinite-dimensional linear programming) allows us to provide a generalization bound argument.

### Generalization

**Proposition 3** (Policy value generalization).: Assume the nuisance models \(\eta=\left[p_{1|0},p_{1|1},\mu_{1},\mu_{0}\right]^{\top},\eta\in H\) are consistent and well-specified with finite VC-dimension \(V_{\eta}\) over the product function class \(H\). Let \(\Pi=\{\mathbb{I}\{\mathbb{E}[L(\lambda,X,A;\eta)\mid X]>0\colon\lambda\in \mathbb{R};\eta\in\mathcal{F}\}\).

\[\sup_{\pi\in\Pi,\lambda\in\mathbb{R}}|(\mathbb{E}_{n}[\pi L(\lambda,X,A)]- \mathbb{E}[\pi L(\lambda,X,A)])|=O_{p}(n^{-\frac{1}{2}})\]

This bound is stated for known nuisance functions: verifying stability under estimated nuisance functions further requires rate conditions.

Doubly-robust estimationWe may improve statistical properties of estimation by developing _doubly robust_ estimators which can achieve faster statistical convergence when both the probability of recommendation assignment (when it is random), and the probability of outcome are consistently estimated; or otherwise protect against misspecification of either model. We first consider the ideal setting when algorithmic recommendations are randomized so that \(e_{r}(X)=P(R=r\mid X)\).

**Proposition 4** (Variance-reduced estimation).: \[V(\pi) =\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X) \left\{\frac{\mathbb{I}[R=r]}{e_{r}(X)}(\mathbb{I}[T=t]c_{r1}(Y)-\mu_{1}(X)p_{ t|r}(X))+\mu_{1}(X)p_{t|r}(X)\right\}\right]\] \[\mathbb{E}[T(\pi)] =\sum_{r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X)\left\{\frac{ \mathbb{I}[R=r]}{e_{r}(X)}(T(r)-p_{1|r}(x))+p_{1|r}(x)\right\}\right]\]

Although similar characterization appears in [39] for the doubly-robust policy value alone, note that doubly-robust versions of the constraints we study would result in differences in the Lagrangian so we retain the full expression rather than simplifying. For example, for regression adjustment, Proposition 9 provides interpretability on how constraints affect the optimal decision rule. In the appendix we provide additional results describing extensions of Proposition 8 with improved estimation.

### Robust estimation with treatment overlap but not recommendation overlap

When recommendations are e.g. the high-risk/low-risk labels from binary classifiers, we may not satisfy the overlap assumption, since algorithmic recommendations are deterministic functions of covariates. However, note that identification in Proposition 1 requires only SUTVA and consistency, and the exclusion restriction assumption. Additional assumptions may be required to extrapolate \(p_{t|r}(X)\) beyond regions of common support. On the other hand, supposing that positivity held with respect to \(T\) given covariates \(X\), given unconfoundedness, our finer-grained approach can be beneficial because we only require robust extrapolation of \(p_{t|r}(X),\) response to recommendations, rather than the outcome models \(\mu_{t}(X).\)

We first describe what can be done if we allow ourselves parametric extrapolation on \(p_{1|1}(X)\), treatment responsivity. In the case study later on, the support of \(X\mid R=1\) is a superset of the support of \(X\mid R=0\) in the observational data. Given this, we derive the following alternative identification based on marginal control variates (where \(p_{t}=P(T=t\mid X)\) marginalizes over the distribution of \(R\) in the observational data):

**Proposition 5** (Control variate for alternative identification ).: Assume that \(Y(T(r))\perp T(r)\mid R=r,X\).

\[V(\pi)=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}\left[\left\{c_{rt}(Y(t)) \frac{\mathbb{I}[T=t]}{p_{t}(X)}+\left(1-\frac{\mathbb{I}[T=t]}{p_{t}(X)} \right)\mu_{t}(X)\right\}p_{t|r}(X)\right]\]

Robust extrapolation of \(p_{t|r}(X)\)Let \(\mathcal{X}^{\text{no}}=\{x:P(R=1\mid X)=0\}\) denote the no overlap region; on this region there are no joint observations of \((t,r,x)\). We consider uncertainty sets for ambiguous treatment recommendation probabilities. For example, one plausible structural assumption is _monotonicity_, that is, making an algorithmic recommendation can only increase the probability of being treated.

We define the following uncertainty set:

\[\mathcal{U}_{q_{|r}}\coloneqq\left\{q_{1|r}(x^{\prime})\colon q_{1|r}(x)\geq p_{1 |r}(x),\ \forall x\in\mathcal{X}^{\text{no}},r\ \sum_{t\in\mathcal{T}}q_{t|r}(x)=1,\forall x,r\right\}\]

We could assume uniform bounds on unknown probabilities, or more refined bounds, such as Lipschitz-smoothness with respect to some distance metric \(d\), or boundedness.

\[\mathcal{U}_{\text{flop}}\coloneqq\left\{q_{1|r}(x^{\prime})\colon d(q_{1|r}(x ^{\prime}),p_{1|r}(x))\leq Ld(x^{\prime},x),\ (x^{\prime},x)\in(\mathcal{X}^{\text{no}}\times\mathcal{X}^{\text{no}}) \right\},\mathcal{U}_{\text{bmd}}\coloneqq\left\{q_{1|r}(x^{\prime})\colon \underline{b}(x)\leq q_{1|r}(x^{\prime})\leq\overline{b}(x)\right\}\]

Define \(V_{ov}(\pi)\coloneqq\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi(r\mid X) p_{t|r}(X)\mu_{t}(X)\mathbb{I}_{ov}]\). Let \(\mathcal{U}\) denote the uncertainty set including any custom constraints, e.g. \(\mathcal{U}=\mathcal{U}_{q_{1|r}}\cap\mathcal{U}_{\text{flop}}\). For brevity we use \(\mathbb{I}_{no}\) to denote \(\mathbb{I}\left[X\in\mathcal{X}^{\text{no}}\right]\), \(\mathbb{I}_{ov}\) to denote \(\mathbb{I}\left[X\in\mathcal{X}^{\text{ov}}\right]\). Then we may obtain robust bounds by optimizing over regions of no overlap:

\[\overline{V}(\pi)\coloneqq V_{ov}(\pi)+\overline{V}_{no}(\pi),\ \ \overline{V}_{no}(\pi) \coloneqq\max_{q_{tr}(X)\in\mathcal{U}}\left\{\sum_{t\in\mathcal{T},r\in\{0, 1\}}\mathbb{E}[\pi(r\mid X)\mu_{t}(X)q_{tr}(X)\mathbb{I}_{no}]]\right\}\]

In the specialized, but practically relevant case of binary outcomes/treatments/recommendations, we obtain the following simplifications for bounds on the policy value, and the minimax robust policy that optimizes the worst-case overlap extrapolation function. In the special case of constant uniform bounds, it is equivalent (in the case of binary outcomes) to consider marginalizations:

**Lemma 1** (Binary outcomes, constant bound).: _Let \(\mathcal{U}_{\text{clnd}}\coloneqq\left\{q_{t|r}(x^{\prime})\colon\underline{B }\leq q_{1|r}(x^{\prime})\leq\overline{B}\right\}\) and \(\mathcal{U}=\mathcal{U}_{q_{t|r}}\cap\mathcal{U}_{\text{clnd}}\). Define \(\beta_{t|r}(a)\coloneqq\mathbb{E}[q_{t|r}(X,A)\mid T=t,A=a]\). If \(T\in\{0,1\},\)_

\[\overline{V}_{no}(\pi)=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[c_{rt}^{*} \delta_{t|r}\mathbb{E}[Y\pi(r\mid X)\mid T=t]\mathbb{I}_{no}]\text{,}\]

_where \(c_{rt}^{*}=\begin{cases}\overline{B}\mathbb{I}\left[t=1\right]+\underline{B} \mathbb{I}\left[t=0\right]&\text{if }\mathbb{E}[Y\pi(r\mid X)\mid T=t]\geq 0\\ \overline{B}\mathbb{I}\left[t=0\right]+\underline{B}\mathbb{I}\left[t=1\right]& \text{if }\mathbb{E}[Y\pi(r\mid X)\mid T=t]<0\end{cases}\)_

We state the next result for simple uncertainty sets, like intervals, to deduce insights about the robust policy. In the appendix we include a more general reformulation for polytopic uncertainty sets.

**Proposition 6** (Robust linear program ).: Suppose \(R,T\in\{0,1\},\) and \(q_{r1}(\cdot,u)\in\mathcal{U}_{\text{bmd}},\forall r,u\). Define

\[\tau(x,a)=\mu_{1}(x,a)-\mu_{0}(x,a),\ \ \Delta B_{r}(x,u)=( \overline{B}_{r}(x,u)-\underline{B}_{r}(x,u)),B_{r}^{\text{mid}}(x,u)= \underline{B}_{r}(x,u)+\frac{1}{2}\Delta B_{r}(x,u),\] \[\mathbb{E}[\Delta_{ov}T(\pi)]=\mathbb{E}[T(\pi)\mathbb{I}_{ov}\mid A =a]-\mathbb{E}[T(\pi)\mathbb{I}_{ov}\mid A=b],\ \ c_{1}(\pi)=\sum_{r}\mathbb{E}[\tau_{r}\pi_{r}B^{\text{mid}}]\]

Then the robust linear program is:

\[\max V_{ov}(\pi)+\mathbb{E}[\mu_{0}]+c_{1}(\pi)-\frac{1}{2}\sum_{r} \mathbb{E}[|\tau|\,\pi(r\mid X)\Delta B_{r}(X,A)\mathbb{I}_{no}]\] \[\text{s.t.}\ \sum_{r}\{\mathbb{E}[\pi(r\mid X)\overline{B}_{r}(X,A) \mathbb{I}_{no}\mid A=a]-\mathbb{E}[\pi_{r}\underline{B}_{r}(X,A)\mathbb{I}_ {no}\mid A=b]\}+\Delta_{ov}^{T}(\pi)\leq\epsilon\]

## 5 Additional fairness constraints and policy optimization

We previously discussed policy optimization, over unrestricted decision rules, given estimates. We now introduce general methodology to handle 1) optimization over a policy class of restricted functional form and 2) more general fairness constraints. We first introduce the fair-classification algorithm of [1], describe our extensions to obtain variance-sensitive regret bounds and less conservative policy optimization (inspired by a regularized ERM argument given in [12]), and then provide sample complexity analysis.

Algorithm and setupIn the following, to be consistent with standard form for linear programs, note that we consider outcomes \(Y\) to be costs so that we can phrase the saddle-point as minimization-maximization. Consider \(|\mathcal{K}|\) linear constraints and \(J\) groups (values of protected attribute \(A\)), with coefficient matrix \(M\in\mathbb{R}^{K\times J}\), the constraint moment function \(h_{j}(\pi),j\in[J]\) (with \(J\) the number of groups), \(O=(X,A,R,T,Y)\) denoting our data observations, and \(d\) the constraint constant vector:

\[h_{j}(\pi)=\mathbb{E}\left[g_{j}(O,\pi(X))\mid\mathcal{E}_{j}\right]\quad\text{ for }j\in J,\ \ Mb(\pi)\leq d\]

Importantly, \(g_{j}\) depends on \(\pi\) while the conditioning event \(\mathcal{E}_{j}\) cannot depend on \(\pi\). Many important fairness constraints can nonetheless be written in this framework, such as burden/resource parity,parity in true positive rates, but not measures such as calibration whose conditioning event does depend on \(\pi\). (See Appendix B.2 for examples omitted for brevity). We further consider a convexification of \(\Pi\) via randomized policies \(Q\in\Delta(\Pi)\), where \(\Delta(\Pi)\) is the set of distributions over \(\Pi\), i.e. a randomized classifier that samples a policy \(\pi\sim Q\). Therefore we solve

\[\min_{Q\in\Delta(\Pi)}\{V(\pi)\colon\;\;Mh(\pi)\leq d\}\]

On the other hand, the optimization is solved using sampled moments, so that we ought to interpret the constraint values \(\hat{d}_{k}=d_{k}+\epsilon_{k},\) for all \(k\). The overall algorithmic scheme is similar: we seek an approximate saddle point so that the constrained solution is equivalent to the Lagrangian,

\[L(Q,\lambda)=\hat{V}(Q)+\lambda^{\top}(M\hat{h}(Q)-\hat{d}),\qquad\min_{Q\in \Delta(\Pi)}\{V(\pi)\colon Mh(\pi)\leq d\}=\min_{Q\in\Delta(\Pi)}\max_{ \lambda\in\mathbb{R}^{K}_{+}}L(Q,\lambda).\]

We simultaneously solve for an approximate saddle point and bound the domain of \(\Lambda\) by \(B\):

\[\min_{Q\in\Delta}\max_{\lambda\in\mathbb{R}^{|X|}_{+},\|\lambda\|_{1}\leq B} L(Q,\lambda),\qquad\max_{\lambda\in\mathbb{R}^{|X|}_{+},\|\lambda\|_{1}\leq B} \min_{Q\in\Delta}L(Q,\lambda)\]

We play a no-regret (second-order multiplicative weights [11; 43], a slight variant of Hedge/exponentiated gradient [18]) algorithm for the \(\lambda-\)player, while using best-response oracles for the \(Q-\)player. Full details are in Algorithm 1. Given \(\lambda_{t},\)\(\mathrm{BEST}_{\beta}\left(\lambda_{t}\right)\) computes a best response over \(Q\); since the worst-case distribution will place all its weight on one classifier, this step can be implemented by a reduction to cost-sensitive/weighted classification [10; 49], which we describe in further detail below. Computing the best response over \(\mathrm{BEST}_{\lambda}(\hat{Q}_{t}))\) selects the most violated constraint. We include further details in Appendix B.2.

```
1:Input: \(\mathcal{D}=\left\{(X_{i},R_{i},T_{i},Y_{i},A_{i})\right\}_{i=1}^{n}\), \(g,\mathcal{E},M,\widehat{d},\) B, \(\nu,\)\(\alpha,\)\(\theta_{1}=0\in\mathbb{R}^{|\mathcal{K}|}\)
2:for\(t=1,2,\ldots\)do
3: Set \(\lambda_{t,k}=B\frac{\exp\{\theta_{k}\}}{1+\sum_{t^{\prime}\in\mathcal{K}}\exp \{\theta_{k^{\prime}}\}}\) for all \(k\in\mathcal{K},\beta_{t}\leftarrow\mathrm{BEST}_{\beta}\left(\lambda_{t} \right),\widehat{Q}_{t}\leftarrow\frac{1}{t}\sum_{t^{\prime}=1}^{t}\beta_{t^{ \prime}}\) \(\bar{L}\gets L(\hat{Q}_{t},\mathrm{BEST}_{\lambda}(\hat{Q}_{t})),\hat{ \lambda}_{t}\leftarrow\frac{1}{t}\sum_{t^{\prime}=1}^{t}\lambda_{t^{\prime}}, \underline{L}\leftarrow(\mathrm{BEST}_{\beta}(\hat{\lambda}_{t}),\hat{\lambda} _{t})\),
5:\(\nu_{t}\leftarrow\max\{L(\hat{Q}_{t},\hat{\lambda}_{t})-\underline{L},\;\; \bar{L}-L(\hat{Q}_{t},\hat{\lambda}_{t})\}\), If \(\nu_{t}\leq\nu\) then return \((\hat{Q}_{t},\hat{\lambda}_{t})\)
6:\(\theta_{t+1,i}=\theta_{t}+\log(1-\eta(M\hat{\mu}\left(h_{t}\right)-\hat{c})_ {j}),\forall i\)
7:endfor ```

**Algorithm 1** MW2REDFAIR\((\mathcal{D},g,\mathcal{E},M,d)\)

**Weighted classification reduction.** There is a well-known reduction of optimizing the zero-one loss for policy learning to weighted classification. Taking the Lagrangian will introduce datapoint-dependent additional weights. This reduction requires \(\pi\in\{-1,+1\},T\in\{-1,+1\}\) (for notational convenience alone). We consider parameterized policy classes so that \(\pi(x)=\mathrm{sign}(g_{\theta}(x))\) for some index function \(g\) depending on a parameter \(\beta\in\mathbb{R}^{d}\). Consider the centered regret \(J(\pi)=\mathbb{E}[Y(\pi)]-\frac{1}{2}\mathbb{E}[\mathbb{E}[Y\mid R=1,X]+ \mathbb{E}[Y\mid R=0,X]]\). Then \(J(\beta)=J(\mathrm{sgn}(g_{\beta}(\cdot)))=\mathbb{E}[\mathrm{sgn}(g_{\beta} (X))\left\{\psi\right\}]\) where \(\psi\) can be one of, where \(\mu_{r}^{R}(X)=\mathbb{E}[Y\mid R=r,X]\),

\[\psi_{DM}=(p_{1|1}(X)-p_{1|0}(X))(\mu_{1}(X)-\mu_{0}(X)),\psi_{IPW}=\frac{RY}{ \varepsilon_{R}(X)},\psi_{DR}=\psi_{DM}+\psi_{IPW}+\frac{R_{\theta}{}^{R}(X)}{ \varepsilon_{R}(X)}\]

We can apply the standard reduction to cost-sensitive classification since \(\psi_{i}\,\mathrm{sgn}(g_{\beta}(X_{i}))=|\psi_{i}|\left(1-2\mathbb{I}\left[ \mathrm{sgn}(g_{\beta}(X_{i}))\neq\mathrm{sgn}(\psi_{i})\right]\right)\). Then we can use surrogate losses for the zero-one loss. Although many functional forms for \(\ell(\cdot)\) are Fisher-consistent, one such choice of \(\ell\) is the logistic (cross-entropy) loss given below:

\[L(\beta)=\mathbb{E}[|\psi|\,\ell(g_{\beta}(X),\mathrm{sgn}(\psi))],\qquad l(g,s )=2\log(1+\exp(g))-(s+1)\] (2)

**Two-stage variance-constrained algorithm.** We seek to improve upon this procedure so that we may obtain regret bounds on policy value and fairness constraint violation that exhibit more favorable dependence on the maximal variance over small-variance _slices_ near the optimal policy, rather than worst-case constants over all policies, [12; 5]. Further, note that the algorithm sets the constraint feasibility slacks via generalization bounds that previously depended on these worst-case constants.

These challenges motivate the two-stage procedure, described formally in Algorithm 2 and verbally here. We adapt an out-of-sample regularization scheme developed in [12], which recovers variance-sensitive regret bounds via a small modification to ERM. We split the data, learn nuisance estimators \(\eta_{1}\) for use in our policy value and constraint estimates, and run Algorithm 1 (\(\mathrm{MW2REDFFAIR}(\mathcal{D}_{1},h,\mathcal{E},M,d;\eta_{1})\)) to obtain an estimate of the optimal policy \(\hat{\pi}_{1}\), and the constraint variances at \(\hat{\pi}_{1}\). Next, we _augment_ the constraint matrix with additional constraints that require the second-stage \(\pi\) to achieve \(\epsilon_{n}\) close policy value and constraint moment values relative to \(\hat{\pi}_{1}\). Since errors concentrate fast, this can be viewed as variance regularization. And, we set the constraint slack \(\tilde{d}\) in the second stage using estimated variance constants from \(\hat{\pi}_{1}\), which will be less conservative than using worst-case bounds. Define

\[v^{(\cdot)}(Q)=\mathbb{E}_{\pi\sim Q}[\pi\psi_{(\cdot)}\mid O],\text{ for }(\cdot)\in\{\emptyset,\mathrm{DR}\},g_{j}(O;Q)=\mathbb{E}_{\pi\sim Q}[g_{j}(O; \pi)\mid O,\mathcal{E}_{j}],\]

so that \(V^{(\cdot)}(Q)=\mathbb{E}[v^{(\cdot)}(Q)]\) and \(h_{j}(Q)=\mathbb{E}[g_{j}(O;Q)\mid\mathcal{E}_{j}]\). Define the function classes

\[\mathcal{F}_{\Pi}=\{v_{DR}(\cdot,\pi;\eta)\colon\pi\in\Pi,\eta\in H\}, \mathcal{F}_{j}=\{g(\cdot,\pi;\eta)\colon\pi\in\Pi,\eta\in H\}\]

and the empirical entropy integral \(\kappa(r,\mathcal{F})=\inf_{\alpha\geq 0}\{4\alpha+10\int_{\alpha}^{r}\sqrt{ \frac{\mathcal{H}_{2}(\epsilon,\mathcal{F},n)}{n}}d\epsilon\}\) where \(H_{2}(\epsilon,\mathcal{F},n)\) is the \(L_{2}\) empirical entropy, i.e. log of the \(\|\cdot\|_{2}\)\(\epsilon\)-covering number. We make a mild assumption of a learnable function class (bounded entropy integral) [46].

**Assumption 7**.: The function classes \(\mathcal{F}_{\Pi},\{\mathcal{F}_{j}\}_{j\in\mathcal{J}}\) satisfy that for any constant \(r,\kappa(r,\mathcal{F})\to 0\) as \(n\to\infty\). The function classes \(\{\mathcal{F}_{j}\}_{j}\) comprise of \(L_{j}\)-Lipschitz contractions of \(\pi\).

We will assume that we are using doubly-robust/orthogonalized estimation as in proposition 4, hence state results depending on estimation error of nuisance vector \(\eta\).

**Theorem 1** (Variance-Based Oracle Policy Regret).: _Suppose that the mean-squared-error of the nuisance estimates is upper bounded w.p. \(1-\delta/2\) by \(\chi_{n,\delta}^{2}\), over the randomness of the nuisance sample: \(\max_{\{\mathbb{E}[(\hat{\eta}_{l}-\eta_{l})^{2}]\}_{l\in[L]}}\coloneqq\chi_{ n}^{2}\). Let \(r=\sup_{Q\in\mathcal{Q}}\sqrt{\mathbb{E}\left[v_{DR}(z;\pi)^{2}\right]}\) and \(\epsilon_{n}=\Theta(\kappa\left(r,\mathcal{F}_{\Pi}\right)+r\sqrt{\frac{\log(1/ \delta)}{n}})\). Moreover, let_

\[\mathcal{Q}_{*}(\epsilon)=\left\{Q\in\Delta[\Pi]:V(Q_{*}^{0})-V(Q)\leq\epsilon,\;\;h(Q_{*}^{0})-h(Q)\leq d+\epsilon\right\}\]

_denote an \(\epsilon\)-regret slice of the policy space. Let \(\tilde{\epsilon}_{n}=O(\epsilon_{n}+\chi_{n,\delta}^{2})\) and_

\[V_{2}^{0}=\sup\left\{\mathrm{Var}\left(v_{DR}^{0}(Q;Q)-v_{DR}^{0}\left(Q;Q^{ \prime}\right)\right):Q,Q^{\prime}\in\mathcal{Q}_{*}(\tilde{\epsilon}_{n})\right\}\]

_denote the variance of the difference between any two policies in an \(\epsilon_{n}\)-regret slice, evaluated at the true nuisance quantities. (Define \(V_{2}^{j}\) analogously for the variance of constraint moments). Then, letting \(\gamma(Q):=Mh(Q)\) denote the constraint values, the policy distribution \(Q_{2}\) returned by the out-of-sample regularized ERM, satisfies w.p. \(1-\delta\) over the randomness of \(S\) :_

\[V(\hat{Q})-V(Q^{*}) =O(\kappa(\sqrt{V_{2}^{obj}},\mathrm{conv}(\mathcal{F}_{\Pi}))+n^ {-\frac{1}{2}}\sqrt{V_{2}^{obj}\log(3/\delta)}+\chi_{n,\delta}^{2})\] \[(\gamma_{j}(\hat{Q})-c_{j})-(\gamma_{j}(Q^{*})-c_{j}) =O(\kappa(\sqrt{V_{2}^{j}},\mathrm{conv}(\mathcal{F}_{j}))+n^{- \frac{1}{2}}\sqrt{V_{2}^{j}\log(3/\delta)}+\chi_{n,\delta}^{2})\]The benefits are that 1) the constants are improved from an absolute, structure-agnostic bound to depending on the variance of low-regret policies, which also reflects improved variance from using doubly-robust estimation as in proposition 4, and 2) less-conservative out-of-sample fairness constraint satisfaction.

## 6 Experiments

Due to space constraints, in the main text we only present a case study based on the PSA-DMF for supervised release [38]. In the appendix we include additional experiments and robustness checks, including a case study of fully-randomized recommendations and non-adherence.

**PSA-DMF case study.** Our case study is on a dataset of judicial decisions on _supervised_ release based on risk-score-informed recommendations [38]. The PSA-DMF (Public Safety Assessment Decision Making Framework) uses a prediction of failure to appear for a future court data to inform pretrial decisions, including our focus on supervised release (i.e. electronic monitoring in addition to release) where judges make the final decision. Despite a large literature on algorithmic fairness of pretrial risk assessment, to the best of our knowledge, recommendations about supervised release are not informed by empirical evidence. There are current policy concerns about disparities in increasing use of supervised release given mixed evidence on outcomes [38]; e.g. Safety and Justice Challenge [41] concludes "targeted efforts to reduce racial disparities are necessary". First, we acknowledge data issues. We work with publicly available data that was discretized for privacy [38]. The final supervision decision does not include intensity, but different intensities are recommended in the data, which we collapse into a single level. The PSA-DMF is an algorithmic recommendation so here we are appealing to overlap in treatment recommendations, but using parametric extrapolation in responsivity. Finally, unconfoundedness is likely untrue, but sensitivity analysis could address this. So, this analysis should be considered exploratory, to illustrate the relevance of the methods. Future work will seek proprietary data for a higher-fidelity empirical study.

Next in Figure 1 we provide descriptive information illustrating heterogeneity (including by protected attribute) in adherence and effectiveness. We observe wide variation in judges assigning supervised release beyond the recommendation. We use logistic regression to estimate outcome models and treatment response models. The first figure shows estimates of the causal effect for different groups, by gender (similar heterogeneity for race). The outcome is failure to appear, so negative scores are beneficial. The second figure illustrates the difference in responsiveness: how much more likely decision-makers are to assign treatment when there is vs. isn't an algorithmic recommendation to do so. The last figure plots a logistic regression of the lift in responsiveness on the causal effect \(\mu_{1}(x,a)-\mu_{0}(x,a)\). We observe disparities in how responsive decision-makers are conditional on the same treatment effect efficacy. This is importantly not a claim of minus because decision-makers didn't have access to causal effect estimates. Nonetheless, disparities persist.

In Figure 2 we highlight results from constrained policy optimization. The first two plots in each set illustrate the objective function value and \(A=a\) average treatment cost, respectively; for \(A\) being race (nonwhite/white) or gender (female/male), respectively. We use costs of \(100\) for \(Y=1\) (failure to appear, \(0\) for \(Y=0\), and \(20\) when \(T=1\) (set arbitrarily). On the x-axis we plot the penalty \(\lambda\) that we use to assess the solutions of Proposition 9. The vertical dashed line indicates the solution achieving \(\epsilon=0\), i.e. parity in treatment take-up. Near-optimal policies that reduce treatment disparity can be of interest due to advocacy concerns about how the expansion of supervised release could increase the surveillance of already surveillance-burdened marginalized populations. We see that indeed, for race, surveillance-parity constrained policies can substantially reduce disparities for nonwhites while not increasing surveillance on whites that much: the red line decreases significantly at low increase to the blue line (and low increases to the objective value). On the other hand, for gender, the opportunity for improvement in surveillance disparity is much smaller. See the appendix for further experiments and computational details.

## References

* Agarwal et al. [2018] A. Agarwal, A. Beygelzimer, M. Dudik, J. Langford, and H. Wallach. A reductions approach to fair classification. In _International Conference on Machine Learning_, pages 60-69. PMLR, 2018.

* [2] D. Arnold, W. Dobbie, and C. S. Yang. Racial bias in bail decisions. _The Quarterly Journal of Economics_, 133(4):1885-1932, 2018.
* [3] D. Arnold, W. Dobbie, and P. Hull. Measuring racial discrimination in bail decisions. _American Economic Review_, 112(9):2992-3038, 2022.
* [4] S. Athey. Beyond prediction: Using big data for policy problems. _Science_, 2017.
* [5] S. Athey and S. Wager. Policy learning with observational data. _Econometrica_, 89(1):133-161, 2021.
* [6] S. Barocas, M. Hardt, and A. Narayanan. _Fairness and Machine Learning_. fairmlbook.org, 2018. http://www.fairmlbook.org.
* [7] P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. _Journal of Machine Learning Research_, 3(Nov):463-482, 2002.
* [8] H. Bastani, O. Bastani, and W. P. Sinchaisri. Improving human decision-making with machine learning. _arXiv preprint arXiv:2108.08454_, 2021.
* [9] E. Ben-Michael, D. J. Greiner, K. Imai, and Z. Jiang. Safe policy learning through extrapolation: Application to pre-trial risk assessment. _arXiv preprint arXiv:2109.11679_, 2021.
* [10] A. Beygelzimer and J. Langford. The offset tree for learning with partial labels. In _Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 129-138, 2009.
* [11] N. Cesa-Bianchi, Y. Mansour, and G. Stoltz. Improved second-order bounds for prediction with expert advice. _Machine Learning_, 66:321-352, 2007.
* [12] V. Chernozhukov, M. Demirer, G. Lewis, and V. Syrgkanis. Semi-parametric efficient policy learning with continuous actions. _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] A. Chohlas-Wood, M. Coots, H. Zhu, E. Brunskill, and S. Goel. Learning to be fair: A consequentialist approach to equitable decision-making. _arXiv preprint arXiv:2109.08792_, 2021.

Figure 1: Distribution of treatment effect by gender, lift in treatment probabilities \(p_{11a}-p_{01a}=P(T=1\mid R=1,A=a,X)-P(T=1\mid R=0,A=a,X)\), and plot of \(p_{11a}-p_{01a}\) vs. \(\tau\).

* [14] M. De-Arteaga, R. Fogliato, and A. Chouldechova. A case for humans-in-the-loop: Decisions in the presence of erroneous algorithmic scores. In _Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems_, pages 1-12, 2020.
* [15] J. L. Doleac and M. T. Stevenson. Algorithmic risk assessments in the hands of humans. _Salem Center_, 2020.
* [16] A. Finkelstein, S. Taubman, B. Wright, M. Bernstein, J. Gruber, J. P. Newhouse, H. Allen, K. Baicker, and O. H. S. Group. The oregon health insurance experiment: evidence from the first year. _The Quarterly journal of economics_, 127(3):1057-1106, 2012.
* [17] D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. _arXiv preprint arXiv:1901.09036_, 2019.
* [18] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of computer and system sciences_, 55(1):119-139, 1997.
* [19] B. Green and Y. Chen. Disparate interactions: An algorithm-in-the-loop analysis of fairness in risk assessments. In _Proceedings of the conference on fairness, accountability, and transparency_, pages 90-99, 2019.
* [20] B. Green and Y. Chen. Algorithmic risk assessments can alter human decision-making processes in high-stakes government contexts. _Proceedings of the ACM on Human-Computer Interaction_, 5(CSCW2):1-33, 2021.
* [21] K. Heard, E. O'Toole, R. Naimpally, and L. Bressler. Real world challenges to randomization and their solutions. _Boston, MA: Abdul Latif Jameel Poverty Action Lab_, 2017.
* [22] M. A. Hernan and J. M. Robins. Causal inference.
* [23] K. Imai, Z. Jiang, J. Greiner, R. Halen, and S. Shin. Experimental evaluation of algorithm-assisted human decision-making: Application to pretrial public safety assessment. _arXiv preprint arXiv:2012.02845_, 2020.
* [24] Z. Jiang, S. Yang, and P. Ding. Multiply robust estimation of causal effects under principal ignorability. _arXiv preprint arXiv:2012.01615_, 2020.
* [25] N. Kallus and A. Zhou. Confounding-robust policy improvement. In _Advances in Neural Information Processing Systems_, pages 9269-9279, 2018.
* [26] N. Kallus and A. Zhou. Assessing disparate impact of personalized interventions: identifiability and bounds. _Advances in neural information processing systems_, 32, 2019.
* [27] N. Kallus and A. Zhou. Fairness, welfare, and equity in personalized pricing. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 296-314, 2021.
* [28] N. Kallus, X. Mao, and A. Zhou. Assessing algorithmic fairness with unobserved protected class using data combination. _arXiv preprint arXiv:1906.00285_, 2019.
* [29] K. Kim, E. Kennedy, and J. Zubizarreta. Doubly robust counterfactual classification. _Advances in Neural Information Processing Systems_, 35:34831-34845, 2022.
* [30] T. Kitagawa and A. Tetenov. Empirical welfare maximization. 2015.
* [31] W. Lin, S.-H. Kim, and J. Tong. Does algorithm aversion exist in the field? an empirical analysis of algorithm use determinants in diabetes self-management. _An Empirical Analysis of Algorithm Use Determinants in Diabetes Self-Management (July 23, 2021). USC Marshall School of Business Research Paper Sponsored by iORB, No. Forthcoming_, 2021.
* [32] L. Liu, Z. Shahn, J. M. Robins, and A. Rotnitzky. Efficient estimation of optimal regimes under a no direct effect assumption. _Journal of the American Statistical Association_, 116(533):224-239, 2021.

* Ludwig and Mullainathan [2021] J. Ludwig and S. Mullainathan. Fragile algorithms and fallible decision-makers: lessons from the justice system. _Journal of Economic Perspectives_, 35(4):71-96, 2021.
* Lum et al. [2017] K. Lum, E. Ma, and M. Baiocchi. The causal impact of bail on case outcomes for indigent defendants in new york city. _Observational Studies_, 3(1):38-64, 2017.
* Manski [2005] C. Manski. _Social Choice with Partial Knowledge of Treatment Response_. The Econometric Institute Lectures, 2005.
* Maurer [2016] A. Maurer. A vector-contraction inequality for rademacher complexities. In _Algorithmic Learning Theory: 27th International Conference, ALT 2016, Bari, Italy, October 19-21, 2016, Proceedings 27_, pages 3-17. Springer, 2016.
* Metevier et al. [2019] B. Metevier, S. Giguere, S. Brockman, A. Kobren, Y. Brun, E. Brunskill, and P. S. Thomas. Offline contextual bandits with high probability fairness guarantees. _Advances in neural information processing systems_, 32, 2019.
* Office of the Chief Judge [2019] Office of the Chief Judge. Bail reform in cook county: An examination of general order 18.8a and bail in felony cases. 2019.
* Qiu et al. [2021] H. Qiu, M. Carone, E. Sadikova, M. Petukhova, R. C. Kessler, and A. Luedtke. Optimal individualized decision rules using instrumental variable methods. _Journal of the American Statistical Association_, 116(533):174-191, 2021.
* Rubin [1980] D. B. Rubin. Comments on "randomization analysis of experimental data: The fisher randomization test comment". _Journal of the American Statistical Association_, 75(371):591-593, 1980.
* Safety and Challenge [2022] Safety and C. f. C. I. Justice Challenge. Expanding supervised release in new york city. 2022. URL https://safetyandjusticechallenge.org/resources/expanding-supervised-release-in-new-york-city/.
* Shapiro [2001] A. Shapiro. On duality theory of conic linear problems. _Semi-Infinite Programming: Recent Advances_, pages 135-165, 2001.
* Steinhardt and Liang [2014] J. Steinhardt and P. Liang. Adaptivity and optimism: An improved exponentiated gradient algorithm. In _International conference on machine learning_, pages 1593-1601. PMLR, 2014.
* Sun et al. [2021] H. Sun, E. Munro, G. Kalashnov, S. Du, and S. Wager. Treatment allocation under uncertain costs. _arXiv preprint arXiv:2103.11066_, 2021.
* Swaminathan and Joachims [2015] A. Swaminathan and T. Joachims. Counterfactual risk minimization. _Journal of Machine Learning Research_, 2015.
* Van Der Vaart et al. [1996] A. W. Van Der Vaart, J. A. Wellner, A. W. van der Vaart, and J. A. Wellner. _Weak convergence_. Springer, 1996.
* Woodworth et al. [2017] B. Woodworth, S. Gunasekar, M. I. Ohannessian, and N. Srebro. Learning non-discriminatory predictors. In _Conference on Learning Theory_, pages 1920-1953. PMLR, 2017.
* Yacoby et al. [2022] Y. Yacoby, B. Green, C. L. Griffin Jr, and F. Doshi-Velez. "if it didn't happen, why would i change my decision?": How judges respond to counterfactual explanations for the public safety assessment. In _Proceedings of the AAAI Conference on Human Computation and Crowdsourcing_, volume 10, pages 219-230, 2022.
* Zhao et al. [2012] Y. Zhao, D. Zeng, A. J. Rush, and M. R. Kosorok. Estimating individualized treatment rules using outcome weighted learning. _Journal of the American Statistical Association_, 107(499):1106-1118, 2012.

Additional discussion

### Additional related work

Principal stratification and mediation analysis in causal inference[32] studies an optimal test-and-treat regime under a no-direct-effect assumption, that assigning a diagnostic test has no effect on outcomes except via propensity to treat, and studies semiparametric efficiency using Structural Nested-Mean Models. Though our exclusion restriction is also a no-direct-effect assumption, our optimal treatment regime is in the space of recommendations only as we do not have control over the final decision-maker, and we consider generally nonparametric models.

We briefly go into more detail about formal differences, due to our specific assumptions, that delineate the differences to mediation analysis. Namely, our conditional exclusion restriction implies that \(Y_{1T_{0}}=Y_{T_{0}}\) and that \(Y_{0T_{1}}=Y_{1T_{1}}\) (in mediation notation with \(T_{r}=T(r)\) in our notation), so that so-called _net direct effects_ are identically zero and the _net indirect effect_ is the treatment effect (also called average encouragement effect here).

Human-in-the-loop in consequential domains.There is a great deal of interest in designing algorithms for the "human in the loop" and studying expertise and discretion in human oversight in consequential domains [14]. On the algorithmic side, recent work focuses on frameworks for learning to defer or human-algorithm collaboration. Our focus is _prior_ to the design of these procedures for improved human-algorithm collaboration: we primarily hold fixed current human responsiveness to algorithmic recommendations. Therefore, our method can be helpful for optimizing local nudges. Incorporating these algorithmic design ideas would be interesting directions for future work.

Empirical literature on judicial discretion in the pretrial setting.Studying a slightly different substantive question, namely causal effects of pretrial decisions on later outcomes, a line of work uses individual judge decision-makers as a leniency instrumental variable for the treatment effect of (for example, EM) on pretrial outcomes [3; 2; 34]. And, judge IVs rely on quasi-random assignment of individual judges. We focus on the prescriptive question of optimal recommendation rules in view of patterns of judicial discretion, rather than the descriptive question of causal impacts of detention on downstream outcomes.

A number of works have emphasized the role of judicial discretion in pretrial risk assessments in particular [20; 15; 33]. In contrast to these works, we focus on studying decisions about electronic monitoring, which is an intermediate degree of decision lever to prevent FTA that nonetheless imposes costs. [23] study a randomized experiment of provision of the PSA and estimate (the sign of) principal causal effects, including potential group-conditional disparities. They are interested in a causal effect on the principal stratum of those marginal defendants who would not commit a new crime if recommended for detention. [9] study policy learning in the absence of positivity (since the PSA is a deterministic function of covariates) and consider a case study on determining optimal recommendation/detention decisions; however their observed outcomes are downstream of judicial decision-making. Relative to their approach, we handle lack of overlap via an exclusion restriction so that we only require ambiguity on _treatment responsivity models_ rather than causal outcome models.

Additional discussion on method

### Additional discussion on constrained optimization

Feasibility program We can obtain upper/lower bounds on \(\epsilon\) in order to obtain a feasible region for \(\epsilon\) by solving the below optimization over maximal/minimal values of the constraint:

\[\overline{\epsilon},\underline{\epsilon}\in\max_{\pi}/\min_{\pi}\mathbb{E}[T( \pi)\mid A=a]-\mathbb{E}[T(\pi)\mid A=b]\] (3)

\[V_{\epsilon}^{*}=\max_{\pi}\left\{\mathbb{E}[c(\pi,T(\pi),Y(\pi))]\colon \mathbb{E}[T(\pi)\mid A=a]-\mathbb{E}[T(\pi)\mid A=b]\leq\epsilon\right\}\] (4)

### Additional discussion on Algorithm 2 (general algorithm)

#### b.2.1 Additional fairness constraints and examples in this framework

In this section we discuss additional fairness constraints and how to formulate them in the generic framework. Much of this discussion is quite similar to [1] (including in notation) and is included in this appendix for completeness only. We only additionally provide novel identification results for another fairness measure on causal policies in Appendix B.2.2, concrete discussion of the reduction to weighted classification, and provide concrete descriptions of the causal fairness constraints in the more general framework.

We first discuss how to impose the treatment parity constraint. This is similar to the demographic parity example in [1], with different coefficients, but included for completeness. (Instead, recommendation parity in \(\mathbb{E}[\pi\mid A=a]\) is indeed nearly identical to demographic parity.)

**Example 1** (Writing treatment parity in the general constrained classification framework.).: We write the constraint

\[\mathbb{E}[T(\pi)\mid A=a]-\mathbb{E}[T(\pi)\mid A=b]\] (5)

in this framework as follows:

\[\mathbb{E}[T(\pi)\mid A=a]=\mathbb{E}[\pi_{1}(X)(p_{1|1}(X,A)-p_{1|0}(X,A))+p_ {1|0}(X,A)\mid A=a]\]

For each \(u\in\mathcal{A}\) we enforce that

\[\sum_{r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X)p_{1|r}(X,A)\mid A=u\right]=\sum_{ r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X,A)p_{1|r}(X,A)\right]\]

We can write this in the generic notation given previously by letting \(\mathcal{J}=\mathcal{A}\cup\{\circ\},\)

\[g_{j}(O,\pi(X);\eta)=\pi_{1}(X)(p_{1|1}(X,A)-p_{1|0}(X,A))+p_{1|0}(X,A),\forall j.\]

We let the conditioning events \(\mathcal{E}_{a}=\{A=a\},\mathcal{E}_{\circ}=\{\text{True}\},\) i.e. conditioning on the latter is equivalent to evaluating the marginal expectation. Then we express Equation (5) as a set of equality constraints \(h_{a}(\pi)=h_{\circ}(\pi),\) leading to pairs of inequality constraints,

\[\begin{cases}h_{u}(\pi)-h_{\circ}(\pi)\leq 0\\ h_{\circ}(\pi)-h_{u}(\pi)\leq 0\end{cases}_{u\in\mathcal{A}}\]

The corresponding coefficients of \(M\) over this enumeration over groups \((\mathcal{A})\) and epigraphical enforcement of equality \((\{+,-\})\) equation (1), gives \(\mathcal{K}=\mathcal{A}\times\{+,-\}\) so that \(M_{(a,+),a^{\prime}}=\mathbf{1}\left\{a^{\prime}=a\right\},M_{(a,+),\star}=-1,\)\(M_{(a,-),a^{\prime}}=-\mathbf{1}\left\{a^{\prime}=a\right\},M_{(a,-),\star}=1,\) and \(\mathbf{d}=\mathbf{0}\). Further we can relax equality to small amounts of constraint relaxation by instead setting \(d_{k}>0\) for some (or all) \(k\).

Next, we discuss a more complicated fairness measure. We first discuss identification and estimation before we also describe how to incorporate it in the generic framework.

#### b.2.2 Responder-dependent fairness measures

We consider a responder framework on outcomes (under our conditional exclusion restriction). Because the contribution to the AEE is indeed from the responder strata, this corresponds to additional estimation of the responder stratum.

We enumerate the four possible realizations of potential outcomes (given any fixed recommendation) as \((Y(0(r)),Y(1(r))\in\{0,1\}^{2}\). We call units with \((Y(0(r)),Y(1(r)))=(0,1)\) responders, \((Y(0(r)),Y(1(r)))=(1,0)\) anti-responders, and \(Y(0(r))=Y(1(r))\) non-responders. Such a decomposition is general for the binary setting.

**Assumption 8** (Binary outcomes, treatment).: \[T,Y\in\{0,1\}\]

**Assumption 9** (Monotonicity).: \[Y(T(1))\geq Y(T(0))\]

Importantly, the conditional exclusion restriction of Assumption 2 implies that responder status is independent of recommendation. Conditional on observables, whether a particular individual is a responder is independent of whether someone decides to treat them when recommended. In this way, we study responder status analogous to its use elsewhere in disparity assessment in algorithmic fairness [23, 28]. Importantly, this assumption implies that the conditioning event (of being a responder) is therefore independent of the policy \(\pi\), so it can be handled in the same framework. s

We may consider reducing disparities in resource expenditure given responder status.

We may be interested in the probability of receiving treatment assignment given responder status.

**Example 2** (Fair treatment expenditure given responder status).: \[\mathbb{E}[T(\pi)\mid Y(1(R))>Y(0(R)),A=a]-\mathbb{E}[T(\pi)\mid Y(1(R))>Y( 0(R)),A=b]\leq\epsilon\]

We can obtain identification via regression adjustment:

**Proposition 7** (Identification of treatment expenditure given responder status).: Assume Assumptions 8 and 9.

\[P(T(\pi)=1\mid A=a,Y(1(\pi))>Y(0(\pi)))=\frac{\sum_{r}\mathbb{E}[\pi_{\tau}(X) p_{1|r}(X,A)(\mu_{1}(X,A)-\mu_{0}(X,A))\mid A=a])}{\mathbb{E}[(\mu_{1}(X,A)- \mu_{0}(X,A))\mid A=a]}\]

Therefore this can be expressed in the general framework.

**Example 3** (Writing treatment responder-conditional parity in the general constrained classification framework.).: For each \(u\in\mathcal{A}\) we enforce that

\[\frac{\sum_{r}\mathbb{E}[\pi_{\tau}(X)p_{1|r}(X,A)(\mu_{1}(X,A)-\mu_{0}(X,A)) \mid A=u])}{\mathbb{E}[(\mu_{1}(X,A)-\mu_{0}(X,A))\mid A=u]}=\frac{\sum_{r} \mathbb{E}[\pi_{\tau}(X)p_{1|r}(X,A)(\mu_{1}(X,A)-\mu_{0}(X,A))])}{\mathbb{E} [(\mu_{1}(X,A)-\mu_{0}(X,A))]}\]

We can write this in the generic notation given previously by letting \(\mathcal{J}=\mathcal{A}\cup\{\circ\}\),

\[g_{j}(O,\pi(X);\eta)=\frac{\{\pi_{1}(X)(p_{1|1}(X,A)-p_{1|0}(X,A))+p_{1|0}(X,A )\}(\mu_{1}(X,A)-\mu_{0}(X,A))}{\mathbb{E}[(\mu_{1}(X,A)-\mu_{0}(X,A))\mid A= a]},\forall j.\]

Let \(\mathcal{E}^{j}_{a}=\{A=a_{j}\},\mathcal{E}_{\circ}=\{\text{True}\},\) and we express Equation (5) as a set of equality constraints of the above moment \(h_{a}(\pi)=h_{\circ}(\pi)\), leading to pairs of inequality constraints,

\[\begin{cases}h_{u}(\pi)-h_{\circ}(\pi)\leq 0\\ h_{\circ}(\pi)-h_{u}(\pi)\leq 0\end{cases}_{u\in\mathcal{A}}\]

The corresponding coefficients of \(M\) proceed analogously as for treatment parity.

#### b.2.3 Best-response oracles

Best-responding classifier \(\pi\), given \(\lambda\): \(\mathrm{BEST}_{\pi}(\lambda)\)The best-response oracle, given a particular \(\lambda\) value, optimizes the Lagrangian given \(\pi\):

\[L(\pi,\lambda) =\hat{V}(\pi)+\lambda^{\top}(M\hat{h}(\pi)-\hat{d})\] \[=\hat{V}(\pi)-\lambda^{\top}\hat{d}+\sum_{k,j}\frac{M_{k,j} \lambda_{k}}{p_{j}}\mathbb{E}_{n}\left[g_{j}(O,\pi)1\left\{O\in\mathcal{E}_{j} \right\}\right].\]

**Best-responding Lagrange multiplier \(\lambda\), given \(\pi\):**\(\mathrm{BEST}_{\lambda}(Q)\) is the best response of the \(\Lambda\) player. It can be chosen to be either \(0\) or put all the mass on the most violated constraint. Let \(\gamma(Q):=Mh(Q)\) denote the constraint values, then \(\mathrm{BEST}_{\lambda}(Q)\) returns \(\begin{cases}\mathbf{0}&\text{if }\widehat{\gamma}(Q)\leq\widehat{\mathbf{c}}\\ B\mathbf{e}_{k^{*}}&\text{otherwise, where }k^{*}=\arg\max_{k}\left[\widehat{\gamma}_{k}(Q)- \widehat{c}_{k}\right]\end{cases}\)

#### b.2.4 Weighted classification reduction

There is a well-known reduction of optimizing the zero-one loss for policy learning to weighted classification. A cost-sensitive classification problem is

\[\operatorname*{arg\,min}_{\pi_{1}}\sum_{i=1}^{n}\pi_{1}\left(X_{i}\right)C_{i} ^{1}+\left(1-\pi_{1}\left(X_{i}\right)\right)C_{i}^{0}\]

The weighted classification error is \(\sum_{i=1}^{n}W_{i}1\left\{h\left(X_{i}\right)\neq Y_{i}\right\}\) which is an equivalent formulation if \(W_{i}=\left|C_{i}^{0}-C_{i}^{1}\right|\) and \(Y_{i}=1\left\{C_{i}^{0}\geq C_{i}^{1}\right\}\).

The reduction to weighted classification is particularly helpful since taking the Lagrangian will introduce datapoint-dependent penalties that can be interpreted as additional weights. We can consider the centered regret \(J(\pi)=\mathbb{E}[Y(\pi)]-\frac{1}{2}\mathbb{E}[\mathbb{E}[Y\mid R=1,X]+ \mathbb{E}[Y\mid R=0,X]]\). Then

\[J(\theta)=J(\mathrm{sgn}(g_{\theta}(\cdot)))=\mathbb{E}[\mathrm{sgn}(g_{ \theta}(X))\left\{\psi\right\}]\]

where \(\psi\) can be one of, where \(\mu_{r}^{R}(X)=\mathbb{E}[Y\mid R=r,X]\),

\[\psi_{DM}=(p_{1|1}(X)-p_{1|0}(X))(\mu_{1}(X)-\mu_{0}(X)),\psi_{IPW}=\frac{RY}{ e_{R}(X)},\psi_{DR}=\psi_{DM}+\psi_{IPW}+\frac{R\mu^{R}(X)}{e_{R}(X)}\]

We can apply the standard reduction to cost-sensitive classification since \(\psi_{i}\operatorname*{sgn}(g_{\theta}(X_{i}))=\left|\psi_{i}\right|(1-2\mathbb{ I}\left[\mathrm{sgn}(g_{\theta}(X_{i}))\neq\mathrm{sgn}(\psi_{i})\right])\). Then we can use surrogate losses for the zero-one loss,

\[L(\theta)=\mathbb{E}[\left|\psi\right|\ell(g_{\theta}(X),\mathrm{sgn}(\psi))]\]

Although many functional forms for \(\ell(\cdot)\) are Fisher-consistent, the logistic (cross-entropy) loss will be particularly relevant: \(l(g,s)=2\log(1+\exp(g))-(s+1)g\).

**Example 4** (Treatment parity, continued (weighted classification reduction)).: The cost-sensitive reduction for a vector of Lagrange multipliers can be deduced by applying the weighted classification reduction to the Lagrangian:

\[L(\beta)=\mathbb{E}\left[|\tilde{\psi}^{\lambda}|\ell\left(g_{\beta}(X), \mathrm{sgn}(\tilde{\psi}^{\lambda})\right)\right],\qquad\text{ where }\tilde{\psi}^{\lambda}=\psi+\frac{\lambda_{A}}{p_{A}}(p_{1|1}-p_{1|0})-\sum_{a \in\mathcal{A}}\lambda_{a}.\]

where \(p_{a}:=\hat{P}(A=a)\) and \(\lambda_{a}:=\lambda_{(a,+)}-\lambda_{(a,-)}\), effectively replacing two non-negative Lagrange multipliers by a single multiplier, which can be either positive or negative.

**Example 5** (Responder-conditional treatment parity, continued).: The Lagrangian is \(L(\beta)=\mathbb{E}\left[|\tilde{\psi}^{\lambda}|\ell\left(g_{\beta}(X), \mathrm{sgn}(\tilde{\psi}^{\lambda})\right)\right]\) with weights:

\[\tilde{\psi}^{\lambda}=\psi+\frac{\lambda_{A}}{p_{A}}\frac{(p_{1|1}-p_{1|0})( \mu_{1}-\mu_{0})}{\mathbb{E}_{n}[(\mu_{1}(X,A)-\mu_{0}(X,A))\mid A=a]}-\sum_{a \in\mathcal{A}}\lambda_{a}.\]

where \(p_{a}:=\hat{P}(A=a)\) and \(\lambda_{a}:=\lambda_{(a,+)}-\lambda_{(a,-)}\).

### Proofs

Proof of Proposition 7.: \[P(T(\pi)=1\mid A=a,Y(1(\pi))>Y(0(\pi)))\] \[=\frac{P(T(\pi)=1,Y(1(r))>Y(0(r))\mid A=a)}{P(Y(1(\pi))>Y(0(\pi)) \mid A=a)}\] by Bayes' rule \[=\frac{P(T(\pi)=1,Y(1)>Y(0)\mid A=a)}{P(Y(1)>Y(0)\mid A=a)}\] by Assumption 2 \[=\frac{\sum_{r}\mathbb{E}[\mathbb{E}[\pi_{r}(X)\mathbb{I}\left[T(r )=1\right]\mathbb{I}\left[Y(1)>Y(0)\right]\mid A=a,X]])}{P(Y(1)>Y(0)\mid A=a)}\] by iter. exp \[\frac{\sum_{r}\mathbb{E}[\pi_{r}(X)p_{1|r}(X,A)(\mu_{1}(X,A)-\mu_{ 0}(X,A))\mid A=a])}{\mathbb{E}[(\mu_{1}(X,A)-\mu_{0}(X,A))\mid A=a]}\] by Proposition 1Proofs

### Proofs for generalization under unconstrained policies

**Proposition 8** (Policy value generalization).: Assume the nuisance models \(\eta=[p_{1|0},p_{1|1},\mu_{1},\mu_{0},e_{r}(X)]^{\top},\eta\in H\) are consistent and well-specified with finite VC-dimension \(V_{\eta}\) over the product function class \(H\). We provide a proof for the general case, including doubly-robust estimators, which applies to the statement of Proposition 8 by taking \(\eta=[p_{1|0},p_{1|1},\mu_{1},\mu_{0}]\).

Let \(\Pi=\{\mathbb{I}\{\mathbb{E}[L(\lambda,X,A;\eta)\mid X]>0\colon\lambda\in \mathbb{R};\eta\in\mathcal{F}\}\).

\[\sup_{\pi\in\Pi,\lambda\in\mathbb{R}}|(\mathbb{E}_{n}[\pi L(\lambda,X,A)]- \mathbb{E}[\pi L(\lambda,X,A)])|=O_{p}(n^{-\frac{1}{2}})\]

The generalization bound allows deducing risk bounds on the out-of-sample value:

**Corollary 2**.: \[\mathbb{E}[L(\hat{\lambda},X,A)_{+}]\leq\mathbb{E}[L(\lambda^{*},X,A)_{+}]+O_ {p}(n^{-\frac{1}{2}})\]

Proof of Proposition 8.: We study a general Lagrangian, which takes as input pseudo-outcomes \(\psi^{t|r}(O;\eta),\psi^{y|t}(O;\eta),\psi^{1|0,\Delta A}\) where each satisfies that

\[\mathbb{E}[\psi^{t|r}(O;\eta)\mid X,A] =p_{1|1}(X,A)-p_{1|0}(X,A)\] \[\mathbb{E}[\psi^{y|t}(O;\eta)\mid X,A] =\tau(X,A)\] \[\mathbb{E}[\psi^{1|0,\Delta A}\mid X] =p_{1|0}(X,a)-p_{1|0}(X,b)\]

We make high-level stability assumptions on pseudooutcomes \(\psi\) relative to the nuisance functions \(\eta\) (these are satisfied by standard estimators that we will consider):

**Assumption 10**.: \(\psi^{t|r},\psi^{y|t},\psi^{1|0,\Delta A}\) respectively are Lipschitz contractions with respect to \(\eta\) and bounded

We study a generalized Lagrangian of an optimization problem that took these pseudooutcome estimates as inputs:

\[L(\lambda,X,A;\eta)=\psi_{t|r}(O;\eta)\left\{\psi_{y|t}(O;\eta)+\frac{\lambda} {p(A)}(\mathbb{I}\left[A=a\right]-\mathbb{I}\left[A=b\right])\right\}+\lambda( \psi^{1|0,\Delta A}(O;\eta))\]

We will show that

\[\sup_{\pi\in\Pi,\lambda\in\mathbb{R}}|(\mathbb{E}_{n}[\pi L(\lambda,X,A)]- \mathbb{E}[\pi L(\lambda,X,A)])|=O_{p}(n^{-\frac{1}{2}})\]

which, by applying the generalization bound twice gives that

\[\mathbb{E}_{n}[\pi L(\lambda,X,A)]=\mathbb{E}[\pi L(\lambda,X,A)])+O_{p}(n^{ -\frac{1}{2}})\]

Write Lagrangian as

\[\max_{\pi}\min_{\lambda}=\min_{\lambda}\max_{\pi}=\min_{\lambda}\mathbb{E}[L(O,\lambda;\eta)_{+}]\]

Suppose the Rademacher complexity of \(\eta_{k}\) is given by \(\mathcal{R}(H_{k}),\) so that [7, Thm. 12] gives that the Rademacher complexity of the product nuisance class \(H\) is therefore \(\sum_{k}\mathcal{R}(H_{k}).\) The main result follows by applying vector-valued extensions of Lipschitz contraction of Rademacher complexity given in [36]. Suppose that \(\psi^{t|r},\psi^{y|t},\psi^{1|0,\Delta A}\) are Lipschitz with constants \(C^{L}_{t|r},C^{L}_{y|t},C^{L}_{1|0,\Delta A}\).

We establish VC-properties of

\[\mathcal{F}_{L_{1}}(O_{1:n}) =\left\{(g_{\eta}(O_{1}),g_{\eta}(O_{i}),\ldots g_{\eta}(O_{n})) \colon\eta\in H\right\},\text{ where }g_{\eta}(O)=\psi_{t|r}(O;\eta)\psi_{y|t}(O;\eta)\] \[\mathcal{F}_{L_{2}}(O_{1:n}) =\left\{(h_{\eta}(O_{1}),h_{\eta}(O_{i}),\ldots h_{\eta}(O_{n})) \colon\eta\in H\right\},\text{ where }h_{\eta}(O)=\psi_{t|r}(O;\eta)\frac{\lambda}{p(A)}( \mathbb{I}\left[A=a\right]-\mathbb{I}\left[A=b\right])\] \[\mathcal{F}_{L_{3}}(O_{1:n}) =\left\{(m_{\eta}(O_{1}),m_{\eta}(O_{i}),\ldots m_{\eta}(O_{n})) \colon\eta\in H\right\},\text{ where }m_{\eta}(O)=\lambda(\psi^{1|0,\Delta A}(O;\eta))\]and the function class for the truncated Lagrangian,

\[\mathcal{F}_{L_{+}}=\{\{(g_{\eta}(O_{i})+h_{\eta}(O_{i})+m_{\eta}(O_{i}))_{+}\}_{1 :n}\colon g\in\mathcal{F}_{L_{1}}(O_{1:n}),h\in\mathcal{F}_{L_{2}}(O_{1:n}),m\in \mathcal{F}_{L_{3}}(O_{1:n}),\eta\in H\}\]

[36, Corollary 4] (and discussion of product function classes) gives the following: Let \(\mathcal{X}\) be any set, \((x_{1},\ldots,x_{n})\in\mathcal{X}^{n}\), let \(F\) be a class of functions \(f:\mathcal{X}\rightarrow\ell_{2}\) and let \(h_{i}:\ell_{2}\rightarrow\mathbb{R}\) have Lipschitz norm \(L\). Then

\[\mathbb{E}\sup_{\eta\in H}\sum_{i}\epsilon_{i}\psi_{i}\left(\eta\left(O_{i} \right)\right)\leq\sqrt{2}L\mathbb{E}\sup_{\eta\in H}\sum_{i,k}\epsilon_{ik} \eta\left(O_{i}\right)\leq\sqrt{2}L\sum_{k}\mathbb{E}\sup_{\eta_{k}\in H_{k}} \sum_{i}\epsilon_{i}\eta_{k}\left(O_{i}\right)\] (6)

where \(\epsilon_{ik}\) is an independent doubly indexed Rademacher sequence and \(f_{k}\left(x_{i}\right)\) is the \(k\)-th component of \(f\left(x_{i}\right)\).

Applying Equation (6) to each of the component classes \(\mathcal{F}_{L_{1}}(O_{1:n}),\mathcal{F}_{L_{2}}(O_{1:n}),\mathcal{F}_{L_{3}} (O_{1:n}),\) and Lipschitz contraction [7, Thm. 12.4] of the positive part function \(\mathcal{F}_{L_{+}}\), we obtain the bound

\[\sup_{\lambda,\eta}|\mathbb{E}_{n}[L(O,\lambda;\eta)_{+}]-\mathbb{E}[L(O, \lambda;\eta)_{+}]|\leq\sqrt{2}(C^{L}_{l|r}C^{L}_{y|t}+C^{L}_{l|r}B_{p_{a}}B+ BC^{L}_{1|0,\Delta A})\sum_{k}\mathcal{R}(H_{k})\]

\(\Box\)

**Proposition 9** (Threshold solutions).: Define

\[L(\lambda,X,A)=(p_{1|1}(X,A)-p_{1|0}(X,A))\left\{\tau(X,A)+\frac{\lambda}{p(A )}(\mathbb{I}\left[A=a\right]-\mathbb{I}\left[A=b\right])\right\}+\lambda(p_{1 |0}(X,a)-p_{1|0}(X,b))\]

\[\lambda^{*}\in\arg\min_{\lambda}\ \mathbb{E}[L(\lambda,X,A)_{+}],\ \ \pi^{*}(x,u)=\mathbb{I}\{L(\lambda^{*},X,u)>0\}\]

If instead \(d(x)\) is a function of covariates \(x\) only,

\[\lambda^{*}\in\arg\min_{\lambda}\ \mathbb{E}[\mathbb{E}[L(\lambda,X,A)\mid X]_{+}],\ \ \pi^{*}(x)=\mathbb{I}\{\mathbb{E}[L(\lambda^{*},X,A)\mid X]>0\}\]

Proof of Proposition 9.: The characterization follows by strong duality in infinite-dimensional linear programming [42]. Strict feasibility can be satisfied by, e.g. solving eq. (3) to set ranges for \(\epsilon\).

### Proofs for robust characterization

Proof of Proposition 5.: \[V(\pi) =\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)\mathbb{E}[c _{rt}(Y(t))\mathbb{I}\left[T(r)=t\right]\mid R=r,X]]\] \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)\mathbb{E} [c_{rt}(Y(t))\mid R=r,X]P(T(r)=t\mid R=r,X)]\] \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)\mathbb{E} [c_{rt}(Y(t))\mid X]P(T(r)=t\mid R=r,X)]\] (7) \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X) \mathbb{E}\left[c_{rt}(Y(t))\frac{\mathbb{I}\left[T(r)=t\right]}{p_{t}(X)} \mid X\right]P(T(r)=t\mid R=r,X)\right]\] unconf. \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X) \left\{\mathbb{E}\left[c_{rt}(Y(t))\frac{\mathbb{I}\left[T(r)=t\right]}{p_{t} (X)}+\left(1-\frac{T}{p_{t}(X)}\right)\mu_{t}(X)\mid X\right]p_{t|r}(X)\right\}\right]\] control variate \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}\left[\pi_{r}(X) \left\{\left\{c_{rt}(Y(t))\frac{\mathbb{I}\left[T(r)=t\right]}{p_{t}(X)}+\left( 1-\frac{T}{p_{t}(X)}\right)\mu_{t}(X)\right\}p_{t|r}(X)\right\}\right]\] (LOTE)

where \(p_{t}(X)=P(T=t\mid X)\) (marginally over \(R\) in the observational data) and (LOTE) is an abbreviation for the law of total expectation. 

Proof of Lemma 1.: \[\overline{V}_{no}(\pi) \coloneqq\max_{q_{tr}(X)\in\mathcal{U}}\left\{\sum_{t\in\mathcal{ T},r\in\{0,1\}}\mathbb{E}[\pi_{r}(X)\mu_{t}(X)q_{tr}(X)\mathbb{I}\left[X\in \mathcal{X}^{\text{no}}\right]]]\right\}\] \[=\max_{q_{tr}(X)\in\mathcal{U}}\left\{\sum_{t\in\mathcal{T},r\in \{0,1\}}\mathbb{E}[\pi_{r}(X)\mathbb{E}[Y\mid T=t,X]q_{tr}(X)\mathbb{I}\left[X \in\mathcal{X}^{\text{no}}\right]]]\right\}\]

Note the objective function can be reparametrized under a surjection of \(q_{t|r}(X)\) to its marginalization, i.e. marginal expectation over a \(\{T=t\}\) partition (equivalently \(\{T=t,A=a\}\) partition for a fairness-constrained setting).

Define

\[\beta_{t|r}(a)\coloneqq\mathbb{E}[q_{t|r}(X,A)\mid T=t,A=a],\beta_{t|r} \coloneqq\mathbb{E}[q_{t|r}(X,A)\mid T=t]\]

Therefore we may reparametrize \(\overline{V}_{no}(\pi)\) as an optimization over constant coefficients (bounded by B):

\[=\max\left\{\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\{c_{t} \beta_{t|r}\}\pi_{r}(X)\mathbb{E}[Y\mid T=t,X]\mathbb{I}\left[X\in\mathcal{X}^ {\text{no}}\right]]]\colon\underline{B}\leq c_{1}\leq\overline{B},c_{0}=1-c_{1}\right\}\] \[=\max\left\{\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[\{c_{t }\beta_{t|r}\}\mathbb{E}[Y\pi_{r}(X)\mid T=t]\mathbb{I}\left[X\in\mathcal{X}^ {\text{no}}\right]]]\colon\underline{B}\leq c_{1}\leq\overline{B},c_{0}=1-c_{1}\right\}\] LOTE \[=\sum_{t\in\mathcal{T},r\in\{0,1\}}\mathbb{E}[c_{rt}^{*}\beta_{t|r }\mathbb{E}[Y\pi_{r}(X)\mid T=t]\mathbb{I}\left[X\in\mathcal{X}^{\text{no}} \right]]]\]

where \(c_{rt}^{*}=\left\{\begin{array}{ll}\overline{B}\mathbb{I}\left[t=1\right]+ \underline{B}\mathbb{I}\left[t=0\right]&\text{if }\mathbb{E}[Y\pi_{r}(X)\mid T=t]\geq 0\\ \overline{B}\mathbb{I}\left[t=0\right]+\underline{B}\mathbb{I}\left[t=1\right]& \text{if }\mathbb{E}[Y\pi_{r}(X)\mid T=t]<0\end{array}\right.\)

Proof of proposition 6.: \[\max_{\pi} \mathbb{E}[c(\pi,T(\pi),Y(\pi))]\mathbb{I}\left[X\not\in\mathcal{X}^{ \text{\tiny{no}}}\right]]+\mathbb{E}[c(\pi,T(\pi),Y(\pi))\mathbb{I}\left[X\in \mathcal{X}^{\text{\tiny{no}}}\right]]\] (8) \[\mathbb{E}[T(\pi)\mathbb{I}\left[X\not\in\mathcal{X}^{\text{ \tiny{no}}}\right]\mid A=a]-\mathbb{E}[T(\pi)\mathbb{I}\left[X\not\in \mathcal{X}^{\text{\tiny{no}}}\right]\mid A=b]\] (9) \[\qquad+\mathbb{E}[T(\pi)\mathbb{I}\left[X\in\mathcal{X}^{\text{ \tiny{no}}}\right]\mid A=a]-\mathbb{E}[T(\pi)\mathbb{I}\left[X\in\mathcal{X}^{ \text{\tiny{no}}}\right]\mid A=b]\leq\epsilon,\forall q_{r1}\in\mathcal{U}\] (10)

Define

\[g_{r}(x,u)=(\mu_{r1}(x,u)-\mu_{r0}(x,u))\]

then we can rewrite this further and apply the standard epigraph transformation:

\(\max\ t\)

\[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\int_{u\in\{a,b\}}\sum_{r \in\{0,1\}}\{g_{r}(x,u)\pi_{r}(x,u)f(x,u)\}q_{r1}(x,u)\}dx\leq V_{ov}(\pi)+ \mathbb{E}[\mu_{0}],\forall q_{r1}\in\mathcal{U}\] \[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r}\pi _{r}(x,a)q_{r1}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)q_{r1}(x,b))\}+\mathbb{E} [\Delta_{ov}T(\pi)]\leq\epsilon,\forall q_{r1}\in\mathcal{U}\]

Project the uncertainty set onto the direct product of uncertainty sets:

\(\max\ t\)

\[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r}\pi _{r}(x,a)q_{r1}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)q_{r1}(x,b))\}+\mathbb{E} [\Delta_{ov}T(\pi)]\leq\epsilon,\forall q_{r1}\in\mathcal{U}_{\in}\]

Clearly robust feasibility of the resource parity constraint over the interval is obtained by the highest/lowest bounds for groups \(a,b\), respectively:

\(\max\ t\)

\[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r} \pi_{r}(x,a)\overline{B}_{r}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)\underline{ B}_{r}(x,u))\}+\mathbb{E}[\Delta_{ov}T(\pi)]\leq\epsilon\]

We define

\[\delta_{r1}(x,u)=\frac{2(q_{r1}(x,u)-\underline{B}_{r}(x,u))}{\overline{B}_{r }(x,u)-\underline{B}_{r}(x,u)}-(\overline{B}_{r}(x,u)-\underline{B}_{r}(x,u)),\]

then

\[\{\underline{B}_{r}(x,u)\leq q_{r1}(x,u)\leq\overline{B}_{r}(x,u)\}\implies \{\|\delta_{r1}(x,u)\|_{\infty}\leq 1\}\]

and

\[q_{r1}(x,u)=\underline{B}_{r}(x,u)+\frac{1}{2}(\overline{B}_{r}(x,u)-\underline {B}_{r}(x,u))(\delta_{r1}(x,u)+1).\]

For brevity we denote \(\Delta B=(\overline{B}_{r}(x,u)-\underline{B}_{r}(x,u))\), so

\(\max\ t\)

\[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r}\pi_{r}(x,a) \overline{B}_{r}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)\underline{B}_{r}(x,u)) \}+\mathbb{E}[\Delta_{ov}T(\pi)]\leq\epsilon,\]where

\[c_{1}(\pi)=\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\sum_{u\in\{a,b\}}\sum_{r\in\{0,1\}}\{g_{r}(x,u)\pi_{r}(x,u)f(x,u)\}(\underline{B}_{r}(x,u)+\frac{1}{2}( \overline{B}_{r}(x,u)-\underline{B}_{r}(x,u)))dx\]

This is equivalent to:

\[\max\ t\] \[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r}\pi _{r}(x,a)\overline{B}_{r}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)\underline{B}_ {r}(x,u))\}+\mathbb{E}[\Delta_{\text{\tiny{ov}}}T(\pi)]\leq\epsilon\]

Undoing the epigraph transformation, we obtain:

\[\max\ V_{\text{\tiny{ov}}}(\pi)+\mathbb{E}[\mu_{0}]+c_{1}(\pi)- \int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\sum_{u\in\{a,b\}}\sum_{r\in\{0,1\}} \left|-g_{r}(x,u)\pi_{r}(x,u)f(x,u)\right|\frac{1}{2}\Delta B(x,u)dx\] \[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r} \pi_{r}(x,a)\overline{B}_{r}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)\underline{ B}_{r}(x,u))\}+\mathbb{E}[\Delta_{\text{\tiny{ov}}}T(\pi)]\leq\epsilon\]

and simplifying the absolute value:

\[\max\ V_{\text{\tiny{ov}}}(\pi)+\mathbb{E}[\mu_{0}]+c_{1}(\pi)- \int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\sum_{u\in\{a,b\}}\sum_{r\in\{0,1\}} \left|g_{r}(x,u)\pi_{r}(x,u)f(x,u)\right|\frac{1}{2}\Delta B(x,u)dx\] \[\int_{x\in\mathcal{X}^{\text{\tiny{no}}}}\{f(x\mid a)(\sum_{r} \pi_{r}(x,a)\overline{B}_{r}(x,a))-f(x\mid b)(\sum_{r}\pi_{r}(x,b)\underline{ B}_{r}(x,u))\}+\mathbb{E}[\Delta_{\text{\tiny{ov}}}T(\pi)]\leq\epsilon\]

\(\Box\)

### Proofs for general fairness-constrained policy optimization algorithm and analysis

We begin with some notation that will simplify some statemetns. Define, for observation tuples \(O\sim(X,A,R,T,Y)\), the value estimate \(v(Q;\eta)\) given some pseudo-outcome \(\psi(O;\eta)\) dependent on observation information and nuisance functions \(\eta\). (We often suppress notation of \(\eta\) for brevity). We let estimators sub/super-scripted by \(1\) denote estimators from the first dataset.

\[v^{(\cdot)}(Q) =\mathbb{E}_{\pi\sim Q}[\pi\psi_{(\cdot)}\mid O],\text{ for }(\cdot)\in\{\emptyset,\mathrm{DR}\}\] \[V^{(\cdot)}(Q) =\mathbb{E}[v^{(\cdot)}(Q)]\] \[\hat{V}_{1}^{(\cdot)}(Q) =\mathbb{E}_{n_{1}}[v^{(\cdot)}(Q)]\] \[g_{j}(O;Q) =\mathbb{E}_{\pi\sim Q}[g_{j}(O;\pi)\mid O,\mathcal{E}_{j}]\] \[h_{j}(Q) =\mathbb{E}[g_{j}(O;Q)\mid\mathcal{E}_{j}]\] \[\hat{h}_{j}^{1}(Q) =\mathbb{E}_{n_{1}}[g_{j}(O;Q)\mid\mathcal{E}_{j}]\]

#### c.3.1 Preliminaries: results from other works used without proof

**Theorem 3** (Thm. 3, [1] (saddle point generalization bound (non-localized)) ).: _Let \(\rho:=\max_{h}\|M\hat{\mu}(h)-\hat{c}\|_{\infty}\). Let Assumption 1 hold for \(C^{\prime}\geq 2C+2+\sqrt{\ln(4/\delta)/2}\), where \(\delta>0\). Let \(Q^{\star}\) minimize \(V(Q)\) subject to \(M\mu(Q)\leq c\). Then Algorithm 1 with \(\nu\propto n^{-\alpha},B\propto n^{\alpha}\) and \(\eta\propto\rho^{-2}n^{-2\alpha}\) terminates in \(O\left(\rho^{2}n^{4\alpha}\ln|\mathcal{K}|\right)\) iterations and returns \(\hat{Q}\). If \(np_{j}^{\star}\geq 8\log(2/\delta)\) for all \(j\), then with probability at least \(1-(|\mathcal{J}|+1)\delta\) then for all \(k\)\(\hat{Q}\) satisfies:_

\[V(\hat{Q}) \leq V\left(Q^{\star}\right)+\widetilde{O}\left(n^{-\alpha}\right)\] \[\gamma_{k}(\widehat{Q}) \leq c_{k}+\frac{1+2\nu}{B}+\sum_{j\in\mathcal{J}}|M_{k,j}| \,\widetilde{O}\left(\left(np_{j}^{\star}\right)^{-\alpha}\right)\]

The proof of [1, Thm. 3] is modular in invoking Rademacher complexity bounds on the objective function and constraint moments, so that invoking standard Rademacher complexity bounds for off-policy evaluation/learning [5, 45] yields the above statement for \(V(\pi)\) (and analogously, randomized policies by [7, Thm. 12.2] giving stability for convex hulls of policy classes).

**Lemma 2** (Lemma 4, [17]).: _Consider a function class \(\mathcal{F}:\mathcal{X}\rightarrow\mathbb{R}^{d}\), with \(\sup_{f\in\mathcal{F}}\|f\|_{\infty,2}\leq 1\) and pick any \(f^{\star}\in\mathcal{F}\). Assume that \(v(\pi)\) is L-Lipschitz in its first argument with respect to the \(\ell_{2}\) norm and let:_

\[Z_{n}(r)=\sup_{Q\in\mathcal{Q}}\{|\mathbb{E}_{n}[\hat{v}(Q)-\hat{v}(Q^{\star}) ]-\mathbb{E}[v(Q)-v(Q^{\star})]|:\mathbb{E}[(\mathbb{E}_{\pi\sim Q}[v(\pi)]- \mathbb{E}_{\pi\sim Q^{\star}}[v(\pi)])^{2}]^{\frac{1}{2}}\leq r\}\]

_Then for some universal constants \(c_{1},c_{2}\) :_

\[\Pr\left[Z_{n}(r)\geq 16L\sum_{t=1}^{d}\mathcal{R}\left(r,\mathrm{conv}(\Pi_{t })-Q_{t}^{\star}\right)+u\right]\leq c_{1}\exp\left\{-\frac{c_{2}nu^{2}}{L^{2} r^{2}+2Lu}\right\}\]

_Moreover, if \(\delta_{n}\) is any solution to the inequalities:_

\[\forall t\in\{1,\ldots,d\}:\mathcal{R}\left(\delta;\mathrm{star}\left( \mathrm{conv}(\Pi_{t})-Q_{t}^{\star}\right)\right)\leq\delta^{2}\]

_then for each \(r\geq\delta_{n}\) :_

\[P\left(Z_{n}(r)\geq 16Ldr\delta_{n}+u\right)\leq c_{1}\exp\left\{-\frac{c_{2} nu^{2}}{L^{2}r^{2}+2Lu}\right\}\]

**Lemma 3** (Concentration of conditional moments ([1, 47])).: _For any \(j\in\mathcal{J}\), with probability at least \(1-\delta\), for all \(Q\),_

\[\left|\widehat{h}_{j}(Q)-h_{j}(Q)\right|\leq 2\mathcal{R}_{n_{j}}(\mathcal{H})+ \frac{2}{\sqrt{n_{j}}}+\sqrt{\frac{\ln(2/\delta)}{2n_{j}}}\]

_If \(np_{j}^{\star}\geq 8\log(2/\delta)\), then with probability at least \(1-\delta\), for all \(Q\),_

\[\left|\widehat{h}_{j}(Q)-h_{j}(Q)\right|\leq 2\mathcal{R}_{np_{j}^{\star}/2}( \mathcal{H})+2\sqrt{\frac{2}{np_{j}^{\star}}}+\sqrt{\frac{\ln(4/\delta)}{np_{j} ^{\star}}}\]

**Lemma 4** (Orthogonality (analogous to [12] (Lemma 8), others)).: _Suppose the nuisance estimates satisfy a mean-squared-error bound_

\[\max_{l}\{\mathbb{E}[(\hat{\eta}_{l}-\eta_{l})^{2}]\}_{l\in[L]}\coloneqq\chi_{n} ^{2}\]

_Then w.p. \(1-\delta\) over the randomness of the policy sample,_

\[V(Q_{0})-V(\hat{Q})\leq O(R_{n,\delta}+\chi_{n}^{2})\]

### Adapted lemmas

In this subsection we collect results similar to those that have appeared previously, but that require substantial additional argumentation in our specific saddle point setting.

**Lemma 5** (Feasible vs. oracle nuisances in low-variance regret slices ([12], Lemma 9) ).: _Consider the setting of Corollary 7. Suppose that the mean squared error of the nuisance estimates is upper bounded w.p. \(1-\delta\) by \(h_{n,\delta}^{2}\) and suppose \(h_{n,\delta}^{2}\leq\epsilon_{n}\). Then:_

\[V_{2}^{0}=\sup_{\pi,\pi^{\prime}\in\Pi_{*}\left(\epsilon_{n}+2h_{n,\delta}^{2 }\right)}\mathrm{Var}\left(v_{DR}^{0}(x;\pi)-v_{DR}^{0}\left(x;\pi^{\prime} \right)\right)\]

_Then \(V_{2}\leq V_{2}^{0}+O\left(h_{n,\delta}\right)\)._

### Proof of Theorem 1

Proof of Theorem 1.: We first study the meta-algorithm with "oracle" nuisance functions \(\eta=\eta_{0}\).

Define

\[\Pi_{2}\left(\epsilon_{n}\right) =\left\{\pi\in\Pi:\mathbb{E}_{n_{1}}[v(Q;\eta_{0})-v(\hat{Q}_{1}; \eta_{0})]\leq\epsilon_{n},\mathbb{E}_{n_{1}}\left[g_{j}(O;\pi,\eta_{0})-g_{j} \left(O;\hat{\pi}_{1},\eta_{0}\right)\mid\mathcal{E}_{j}\right]\leq\epsilon_{n },j\in\hat{\mathcal{I}}_{1}\right\}\] \[\mathcal{Q}_{2}\left(\epsilon_{n}\right) =\left\{Q\in\Delta\left(\Pi_{2}(\epsilon_{n})\right)\right\}\] \[\mathcal{Q}^{*}\left(\epsilon_{n}\right) =\left\{Q\in\Delta(\Pi):\mathbb{E}[(v(Q;\eta_{0})-v(Q^{*};\eta_{0 })]\leq\epsilon_{n},\mathbb{E}[g_{j}(O;Q,\eta_{0})\mid\mathcal{E}_{j}]- \mathbb{E}[g_{j}(O;Q^{*},\eta_{0})\mid\mathcal{E}_{j}]\leq\epsilon_{n}\right\}\]

In the following, we suppress notational dependence on \(\eta_{0}\).

Note that \(\hat{Q}_{1}\in\mathcal{Q}_{2}\left(\epsilon_{n}\right).\)

Step 1: First we argue that w.p. \(1-\delta/6\), \(Q^{*}\in\mathcal{Q}_{2}\).

Invoking Theorem 3 on the output of the first stage of the algorithm, yields that with probability \(1-\frac{\delta}{6}\) over the randomness in \(\mathcal{D}_{1}\), by choice of \(\epsilon_{n}\) = \(\bar{O}(n^{-\alpha})\)),

\[V(\hat{Q}_{1}) \leq V(Q^{*})+\epsilon_{n}/2\] \[\gamma_{k}(\hat{Q}_{1}) \leq d_{k}+\sum_{j\in\mathcal{J}}|M_{k,j}|\,\widetilde{O}\left(( np_{j}^{*})^{-\alpha}\right)\leq d_{k}+\epsilon_{n}/2\quad\text{ for all }k\]

Further, by Lemma 2,

\[\sup_{Q\in\mathcal{Q}}|\mathbb{E}_{n_{1}}[(v(Q)-v(Q^{*}))]- \mathbb{E}[(v(Q)-v(Q^{*}))]|\leq\epsilon_{n}/2\] \[\sup_{Q\in\mathcal{Q}}|\mathbb{E}_{n_{1}}[(g(O;Q)-g(O;Q^{*}))]- \mathbb{E}[(g(O;Q)-g(O;Q^{*}))]|\leq\epsilon_{n}/2\]

Therefore, with high probability on the good event, \(Q^{*}\in\mathcal{Q}_{2}\).

Step 2: Again invoking Theorem 3, this time on the output of the second stage of the algorithm with function space \(\Pi_{2}\) (hence implicitly \(\mathcal{Q}_{2}\)), and conditioning on the "good event" that \(Q^{*}\in\mathcal{Q}_{2}\), we obtain the bound that with probability \(\geq 1-\delta/3\) over the randomness of the second sample \(\mathcal{D}_{2}\),

\[V(\hat{Q}_{2}) \leq V(Q^{*})+\epsilon_{n}/2\] \[\gamma_{k}(\hat{Q}_{2}) \leq\gamma_{k}(Q^{*})+\epsilon_{n}/2\]Step 3: empirical small-regret slices relate to population small-regret slices, and variance bounds

We show that if \(Q\in\mathcal{Q}_{2},\) then with high probability \(Q\in\mathcal{Q}_{2}^{0}\) (defined on small population value- and constraint-regret slices relative to \(\hat{Q}_{1}\) rather than small empirical regret slices)

\[\mathcal{Q}_{2}^{0}=\{Q\in\mathrm{conv}(\Pi)\colon\left|V(Q)-V(\hat{Q}_{1}) \right|\leq\epsilon_{n}/2,\mathbb{E}[g_{j}(O;Q)-g_{j}(O;\hat{Q}_{1}))\mid \mathcal{E}_{j}]\leq\epsilon_{n},\forall j\}\]

so that w.h.p. \(\mathcal{Q}_{2}\subseteq\mathcal{Q}_{2}^{0}.\)

Note that for \(Q\in\mathcal{Q}\), w.h.p. \(1-\delta/6\) over the first sample, we have that

\[\sup_{Q\in\mathcal{Q}}\left|\mathbb{E}_{n}[v(Q)-v(\hat{Q}_{1})]- \mathbb{E}[v(Q)-v(\hat{Q}_{1})]\right|\leq 2\sup_{Q\in\mathcal{Q}}\left| \mathbb{E}_{n}[v(Q)]-\mathbb{E}[v(Q)]\right|\leq\epsilon,\] \[\sup_{Q\in\mathcal{Q}}\left|\mathbb{E}_{n_{1}}[g_{j}(O;Q)-g_{j}( O;\hat{Q}_{1})\mid\mathcal{E}_{j}]-\mathbb{E}[g_{j}(O;Q)-g_{j}(O;\hat{Q}_{1}) \mid\mathcal{E}_{j}]\right|\] \[\qquad\qquad\leq 2\sup_{Q\in\mathcal{Q}}\left|\mathbb{E}_{n_{1}}[g_{j} (O;Q)\mid\mathcal{E}_{j}]-\mathbb{E}[g_{j}(O;Q)\mid\mathcal{E}_{j}]\right| \leq\epsilon,\forall j\]

The second bound follows from [7, Theorem 12.2] (equivalence of Rademacher complexity over convex hull of the policy class) and linearity of the policy value and constraint estimators in \(\pi,\) and hence \(Q.\)

On the other hand since \(Q_{1}\) achieves low policy regret, the triangle inequality implies that we can contain the true policy by increasing the error radius. That is, for all \(Q\in\mathcal{Q}_{2},\) with high probability \(\geq 1-\delta/3\):

\[\left|\mathbb{E}[(v(Q)-v(Q^{*}))]\right|\leq\left|\mathbb{E}[(v(Q) -v(\hat{Q}_{1}))]\right|+\left|\mathbb{E}[(v(\hat{Q}_{1})-v(Q^{*})))\right| \leq\epsilon_{n}\] \[\left|\mathbb{E}[g_{j}(O;Q)-g_{j}(O;Q^{*})\mid\mathcal{E}_{j}] \right|\leq\left|\mathbb{E}[g_{j}(O;Q)-g_{j}(O;\hat{Q}_{1})\mid\mathcal{E}_{j }]\right|+\left|\mathbb{E}[g_{j}(O;\hat{Q}_{1})-g_{j}(O;Q^{*})\mid\mathcal{E} _{j}]\right|\leq\epsilon_{n}\]

Define the space of distributions over policies that achieve value and constraint regret in the population of at most \(\epsilon_{n}:\)

\[\mathcal{Q}_{*}(\epsilon_{n})=\{Q\in\mathcal{Q}\colon V(Q)-V(Q^{*})\leq \epsilon_{n},\ \ \mathbb{E}[g_{j}(O;Q)-g_{j}(O;Q^{*})\mid\mathcal{E}_{j}]\leq\epsilon_{n}, \forall j\},\]

so that on that high-probability event,

\[\mathcal{Q}_{2}^{0}(\epsilon_{n})\subseteq\mathcal{Q}_{*}(\epsilon_{n}).\] (11)

Then on that event with probability \(\geq 1-\delta/3\),

\[r_{2}^{2} =\sup_{Q\in\mathcal{Q}_{2}}\mathbb{E}[(v(Q)-v(Q^{*}))^{2}]\leq \sup_{Q\in\mathcal{Q}^{*}(\epsilon_{n})}\mathbb{E}[(v(Q)-v(Q^{*}))^{2}]\] \[=\sup_{Q\in\mathcal{Q}^{*}(\epsilon_{n})}\mathrm{Var}(v(Q)-v(Q^{ *}))+\mathbb{E}[(v(Q)-v(Q^{*}))]^{2}\] \[\leq\sup_{Q\in\mathcal{Q}^{*}(\epsilon_{n})}\mathrm{Var}(v(Q)-v (Q^{*}))+\epsilon_{n}^{2}\]

Therefore:

\[r_{2}\leq\sqrt{\sup_{Q\in\mathcal{Q}_{*}(\epsilon_{n})}\mathrm{Var}\left(v(Q)- v(Q^{*})\right)}+2\epsilon_{n}=\sqrt{V_{2}}+2\epsilon_{n}\]

Combining this with the local Rademacher complexity bound, we obtain that:

\[\mathbb{E}[v(\hat{Q}_{2})-v(Q^{*})]=O\left(\kappa\left(\sqrt{V_{2}}+2\epsilon_ {n},\mathcal{Q}_{*}\left(\epsilon_{n}\right)\right)+\sqrt{\frac{V_{2}\log(3/ \delta)}{n}}\right)\]

These same arguments apply for the variance of the constraints

\[V_{2}^{j}=\sup\left\{\mathrm{Var}\left(g_{j}(O;Q)-g_{j}\left(O;Q^{\prime} \right)\right):Q,Q^{\prime}\in\mathcal{Q}_{*}(\tilde{\epsilon}_{n})\right\}\]

### Proofs of auxiliary/adapted lemmas

Proof of Lemma 5.: The proof is analogous to that of [12, Lemma 9] except for the step of establishing that \(\pi_{*}\in\mathcal{Q}^{0}_{\epsilon_{n}+O(\chi^{2}_{n,\delta})}\): in our case we must establish relationships between saddlepoints under estimated vs. true nuisances. We show an analogous version below.

Define the saddle points to the following problems (with estimated vs. true nuisances):

\[(Q^{*}_{0,0},\lambda^{*}_{0,0}) \in\arg\min_{Q}\max_{\lambda}\mathbb{E}[v_{DR}(Q;\eta_{0})]+ \lambda^{\top}(\gamma_{DR}(Q;\eta_{0})-d)\coloneqq L(Q,\lambda;\eta_{0},\eta_ {0})\coloneqq L(Q,\lambda),\] \[(Q^{*}_{\eta,0},\lambda^{*}_{\eta,0}) \in\arg\min_{Q}\max_{\lambda}\mathbb{E}[v_{DR}(Q;\eta)]+\lambda^{ \top}(\gamma_{DR}(Q;\eta_{0})-d),\] \[(Q^{*},\lambda^{*}) \in\arg\min_{Q}\max_{\lambda}\mathbb{E}[v_{DR}(Q;\eta)]+\lambda^ {\top}(\gamma_{DR}(Q;\eta)-d).\]

We have that:

\[\mathbb{E}[v_{DR}(Q^{*})]\leq L(Q^{*},\lambda^{*};\eta,\eta)+\nu\] \[\leq L(Q^{*},\lambda^{*};\eta,\eta_{0})+\nu+\chi^{2}_{n,\delta}\] \[\leq L(Q^{*},\lambda^{*};\eta,\eta_{0})+\nu+\chi^{2}_{n,\delta}\] \[\leq L(Q^{*},\lambda^{*}_{\eta,0};\eta,\eta_{0})+\nu+\chi^{2}_{n,\delta}\] by \[\text{saddlepoint prop.}\] \[\leq L(Q^{*}_{\eta,0},\lambda^{*}_{\eta,0};\eta,\eta_{0})+\big{|} L(Q^{*}_{\eta,0},\lambda^{*}_{\eta,0};\eta,\eta_{0})-L(Q^{*},\lambda^{*}_{\eta,0}; \eta,\eta_{0})\big{|}+\nu+\chi^{2}_{n,\delta}\] \[\leq L(Q^{*}_{\eta,0},\lambda^{*}_{\eta,0};\eta,\eta_{0})+\epsilon _{n}+\nu+\chi^{2}_{n,\delta}\] assuming \[\epsilon_{n}\geq\chi^{2}_{n,\delta}\] \[\leq\mathbb{E}[v_{DR}(Q^{*}_{\eta,0};\eta)]+\epsilon_{n}+2\nu+ \chi^{2}_{n,\delta}\] apx. complementary slackness \[\leq\mathbb{E}[v_{DR}(Q^{*}_{0,0};\eta)]+\epsilon_{n}+2\nu+\chi^{2}_{n, \delta}\] suboptimality

Hence

\[\mathbb{E}[v_{DR}(Q^{*};\eta)]-\mathbb{E}[v_{DR}(Q^{*}_{0,0};\eta)]\leq \epsilon_{n}+2\nu+\chi^{2}_{n,\delta}.\]

We generally assume that the saddlepoint suboptimality \(\nu\) is of lower order than \(\epsilon_{n}\) (since it is under our computational control).

Applying Lemma 4 gives;

\[V(Q^{*})-V(Q^{*}_{0,0})\leq\epsilon_{n}+2\nu+2\chi^{2}_{n,\delta}.\]

Define policy classes with respect to small-population regret slices (with a nuisance-estimation enlarged radius):

\[\mathcal{Q}^{0}(\epsilon)=\{Q\in\Delta(\Pi)\colon V(Q^{*}_{0})-V(Q)\leq \epsilon,\gamma(Q^{*}_{0})-\gamma(Q)\leq\epsilon\}\]

Then we have that

\[V^{obj}_{2}\leq\sup_{Q\in\mathcal{Q}^{0}(\epsilon_{n})}\mathrm{Var}(v_{DR}(Q; \pi)-v_{DR}(O;\pi^{*})),\]

where we have shown that \(\pi^{*}\in\mathcal{Q}^{0}(\epsilon+2\nu+2\chi^{2}_{n,\delta})\).

Following the result of the argumentation in [12, Lemma 9] from here on out gives the result.

Case Studies

### Oregon Health Insurance Study

The Oregon Health Insurance Study [16] is an important study on the causal effect of expanding public health insurance on healthcare utilization, outcomes, and other outcomes. It is based on a randomized controlled trial made possible by resource limitations, which enabled the use of a randomized lottery to expand Medicaid eligibility for low-income uninsured adults. Outcomes of interest included health care utilization, financial hardship, health, and labor market outcomes and political participation.

Because the Oregon Health Insurance Study expanded access to _enroll_ in Medicaid, a social safety net program, the effective treatment policy is in the space of _encouragement_ to enroll in insurance (via access to Medicaid) rather than direct enrollment. This encouragement structure is shared by many other interventions in social services that may invest in nudges to individuals to enroll, tailored assistance, outreach, etc., but typically do not automatically enroll or automatically initiate transfers. Indeed this so-called _administrative burden_ of requiring eligible individuals to undergo a costly enrollment process, rather than automatically enrolling all eligible individuals, is a common policy design lever in social safety net programs. Therefore we expect many beneficial interventions in this consequential domain to have this encouragement structure.

We preprocess the data by partially running the Stata replication file, obtaining a processed data file as input, and then selecting a subset of covariates. These covariates include household information that affected stratified lottery probabilities, socioeconomic demographics, medical status and other health information.

In the notation of our framework, the setup of the optimal/fair encouragement policy design question is as follows:

* \(X\) covariates (baseline household information, socioeconomic demographics, health information)
* \(A\) race (non-white/white), or gender (female/male) These protected attributes were binarized.
* \(R\) encouragement: lottery status of expanded eligibility (i.e. invitation to enroll when individual was previously ineligible to enroll)
* \(T\): whether the individual is enrolled in insurance ever Note that for \(R=1\) this can be either Medicaid or private insurance while for \(R=0\) this is still well-defined as this can be private insurance.
* \(Y\): number of doctor visits This outcome was used as a measure of healthcare utilization. Overall, the study found statistically significant effects on healthcare utilization. An implicit assumption is that increased healthcare utilization leads to better health outcomes.

We subsetted the data to include complete cases only (i.e. without missing covariates). We learned propensity and treatment propensity models via logistic regression for each group, and used gradient-boosted regression for the outcome model. We first include results for regression adjustment identification.

In Figure 3 we plot descriptive statistics. We include histograms of the treatment responsivity lifts \(p_{1|1a}(x,a)-p_{1|0a}(x,a)\). We see some differences in distributions of responsivity by gender and race. We then regress treatment responsivity on the outcome-model estimate of \(\tau\). We find substantially more heterogeneity in treatment responsivity by race than by gender: whites are substantially more likely to take up insurance when made eligible, conditional on the same expected treatment effect heterogeneity in increase in healthcare utilization. (This is broadly consistent with health policy discussions regarding mistrust of the healthcare system).

Next we consider imposing treatment parity constraints on an unconstrained optimal policy (defined on these estimates). In Figure 4 we display the objective value, and \(\mathbb{E}[T(\pi)\mid A=a]\), for gender and race, respectively, enumerated over values of the constraint. We use costs of \(2\) for the number of doctors visits and \(1\) for enrollment in Medicaid (so \(\mathbb{E}[T(\pi)\mid A=a]\) is on the scale of probability of enrollment). These costs were chosen arbitrarily. Finding optimal policies that improve disparities in group-conditional access can be done with relatively little impact to the overall objective value. These group-conditional access disparities can be reduced from \(4\) percentage points (\(0.04\)) for gender and about \(6\) percentage points \((0.06)\) for race at a cost of \(0.01\) or \(0.02\) in objective value (twice the number of doctors' visits). On the other hand, relative improvements/compromises in access value for the "advantaged group" show different tradeoffs. Plotting the tradeoff curve for race shows that, consistent with the large differences in treatment responsivity we see for whites, improving access for blacks. Looking at this disparity curve given \(\lambda\) however, we can also see that small values of \(\lambda\) can have relatively large improvements in access for blacks before these improvements saturate, and larger \(\lambda\) values lead to smaller increases in access for blacks vs. larger decreases in access for whites.