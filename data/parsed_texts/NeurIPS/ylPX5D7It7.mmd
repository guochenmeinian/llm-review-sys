# Understanding How Consistency Works in Federated Learning via Stage-wise Relaxed Initialization

 Yan Sun

The University of Sydney

ysun9899@uni.sydney.edu.au

Li Shen

JD Explore Academy

mathshenli@gmail.com

&Dacheng Tao

The University of Sydney

dacheng.tao@gmail.com

Li Shen is the corresponding author.

###### Abstract

Federated learning (FL) is a distributed paradigm that coordinates massive local clients to collaboratively train a global model via stage-wise local training processes on the heterogeneous dataset. Previous works have implicitly studied that FL suffers from the "client-drift" problem, which is caused by the inconsistent optimum across local clients. However, till now it still lacks solid theoretical analysis to explain the impact of this local inconsistency. To alleviate the negative impact of the "client drift" and explore its substance in FL, in this paper, we first design an efficient FL algorithm _FedInit_, which allows employing the personalized relaxed initialization state at the beginning of each local training stage. Specifically, _FedInit_ initializes the local state by moving away from the current global state towards the reverse direction of the latest local state. This relaxed initialization helps to revise the local divergence and enhance the local consistency level. Moreover, to further understand how inconsistency disrupts performance in FL, we introduce the excess risk analysis and study the divergence term to investigate the test error of the proposed _FedInit_ method. Our studies show that optimization error is not sensitive to this local inconsistency, while it mainly affects the generalization error bound in _FedInit_. Extensive experiments are conducted to validate this conclusion. Our proposed _FedInit_ could achieve state-of-the-art (SOTA) results compared to several advanced benchmarks without any additional costs. Meanwhile, stage-wise relaxed initialization could also be incorporated into the current advanced algorithms to achieve higher performance in the FL paradigm.

## 1 Introduction

Since McMahan et al. [26] developed federated learning, it becomes a promising paradigm to effectively make full use of the computational ability of massive edge devices. Kairouz et al. [17] further classify the modes based on the specific tasks and different environmental setups. Different from centralized training, FL utilizes a central server to coordinate the clients to perform several local training stages and aggregate local models as one global model. However, due to the heterogeneous dataset, it still suffers from significant performance degradation in practical scenarios.

Several previous studies explore the essence of performance limitations in FL and summarize it as the "client-drift" problem [1, 19, 22, 38, 44, 46, 48]. From the perspective of the global target, Karimireddy et al. [19] claim that the aggregated local optimum is far away from the global optimum due to the heterogeneity of the local dataset, which introduces the "client-drift" in FL. However, under limited local training steps, local clients can not genuinely approach the local optimum. To describe this negative impact more accurately, Acar et al. [1] and Wang et al. [44] point out that each locally optimized objective should be regularized to be aligned with the global objective. Moreover, beyond the guarantees of local consistent objective, Xu et al. [46] indicate that the performancedegradation could be further eliminated in FL if it guarantees the local consistent updates at each communication round, which is more similar to the centralized scenarios. These arguments intuitively provide forward-looking guidance for improving the performance in FL. However, in the existing analysis, there is still no solid theoretical support to understand the impact of the consistency term, which also severely hinders the further development of the FL paradigm.

To alleviate the negative impact of the "client-drift" problem and strengthen consistency in the FL paradigm, in this paper, we take into account adopting the personalized relaxed initialization at the beginning of each communication round, dubbed _FedInit_ method. Specifically, _FedInit_ initializes the selected local state by moving away from the current global state towards the reverse direction of the current latest local state. Personalized relaxed initialization helps each local model to revise its divergence and gather together with each other during the local training process. This flexible approach is surprisingly effective in FL and only adopts a constant coefficient to control the divergence level of the initialization. It could also be easily incorporated as a plug-in into other advanced benchmarks to further improve their performance.

Moreover, to explicitly understand how local inconsistency disrupts performance, we introduce the excess risk analysis to investigate the test error of _FedInit_ under the smooth non-convex objective, which includes an optimization error bound and a generalization error bound. Our theoretical studies indicate that the optimization error is insensitive to local inconsistency, while it mainly affects the generalization performance. Under _PL_-condition, consistency performs as the dominant term in the excess risk. Extensive empirical studies are conducted to validate the efficiency of the _FedInit_ method. On the CIFAR-\(10/100\) dataset, it could achieve SOTA results compared to several advanced benchmarks without additional costs. It also helps to enhance the consistency level in FL.

In summary, the main contributions of this work are stated as follows:

* We propose an efficient and novel FL method, dubbed _FedInit_, which adopts the personalized relaxed initialization state on the selected local clients at each communication round. Relaxed initialization is dedicated to enhancing local consistency during training, and it is also a practical plug-in that could easily to incorporated into other methods.
* One important contribution is that we introduce the excess risk analysis in the proposed _FedInit_ method to understand the intrinsic impact of local consistency. Our theoretical studies prove that the optimization error is insensitive to consistency, while it mainly affects the test error and generalization error bound.
* Extensive numerical studies are conducted on the real-world dataset to validate the efficiency of the _FedInit_ method, which outperforms several SOTA benchmarks without additional training costs. Meanwhile, as an efficient plug-in, relaxed initialization (_FedInit_) could also help the other benchmarks in our paper to achieve higher performance with effortlessness.

## 2 Related Work

**Consistency in FL**. FL employs an enormous number of edge devices to jointly train a single model among the isolated heterogeneous dataset [17; 26]. As a standard benchmark, _FedAvg_[2; 26; 48] allows the local stochastic gradient descent (local SGD) [10; 23; 45] based updates and uniformly selected partial clients' participation to alleviate the communication bottleneck. The stage-wise local training processes lead to significant divergence for each client [5; 25; 43; 44]. To improve the efficiency of the FL paradigm, a series of methods are proposed. Karimireddy et al. [19] indicate that inconsistent local optimums cause the severe "client drift" problem and propose the _SCAFFOLD_ method which adopts the variance reduction [6; 16] technique to mitigate it. Li et al. [22] penalize the prox-term on the local objective to force the local update towards both the local optimum and the last global state. Zhang et al. [49] utilize the primal-dual method to improve consistency via solving local objectives under the equality constraint. Specifically, a series of works further adopt the alternating direction method of multipliers (ADMM) to optimize the global objective [1; 9; 41; 52], which could also enhance the consistency term. Beyond these, a series of momentum-based methods are proposed to strengthen local consistency. Wang et al. [42] study a global momentum update method to stabilize the global model. Further, Gao et al. [8] use a local drift correction via a momentum-based term to revise the local gradient, efficiently reducing inconsistency. Ozfatura et al. [28], Xu et al. [46], Sun et al. [39] propose a similar client-level momentum to force the local update towards the last global direction. A variant of client-level momentum that adopts the inertial momentum to further improve the local consistency level [24; 40]. At present, improving the consistency in FL remains a very important and promising research direction. Though these studies involve the heuristic discussion on consistency, in this paper we focus on the personalized relaxed initialization.

**Generalization in FL.** A lot of works have studied the properties of generalization in FL. Based on the margin loss [3; 7; 27], Reisizadeh et al. [31] develop a robust FL paradigm to alleviate the distribution shifts across the heterogeneous clients. Shi et al. [32] study the efficient and stable model technique of model ensembling. Yagli et al. [47] prove the information-theoretic bounds on the generalization error and privacy leakage in the general FL paradigm. Qu et al. [29] propose to adopt the sharpness aware minimization (SAM) optimizer on the local client to improve the flatness of the loss landscape. Caldarola et al. [4], Sun et al. [37; 38], Shi et al. [33; 34] propose several variants based on SAM that could achieve higher performance. However, these works only focus on the generalization efficiency in FL, while in this paper we prove that its generalization error bound is dominated by consistency.

## 3 Methodology

### Preliminaries

Under the cross-device FL setups, there are a very large number of local clients to collaboratively train a global model. Due to privacy protection and unreliable network bandwidth, only a fraction of devices are open-accessed at any one time [17; 29]. Therefore, we define each client stores a private dataset \(\mathcal{S}_{i}=\{z_{j}\}\) where \(z_{j}\) is drawn from an unknown unique distribution \(\mathcal{D}_{i}\). The whole local clients constitute a set \(\mathcal{C}=\{i\}\) where \(i\) is the index of each local client and \(|\mathcal{C}|=C\). Actually, in the training process, we expect to approach the optimum of the population risk \(F\):

\[w^{\star}_{\mathcal{D}}\in\operatorname*{arg\,min}_{w}\left\{F(w)\triangleq \frac{1}{C}\sum_{i\in\mathcal{C}}F_{i}(w)\right\},\] (1)

where \(F_{i}(w)=\mathbb{E}_{z_{j}\sim\mathcal{D}_{i}}F_{i}(w,z_{j})\) is the local population risk. While in practice, we usually consider the empirical risk minimization of the non-convex finite-sum problem in FL as:

\[w^{\star}\in\operatorname*{arg\,min}_{w}\left\{f(w)\triangleq\frac{1}{C}\sum_ {i\in\mathcal{C}}f_{i}(w)\right\},\] (2)

where \(f_{i}(w)=\frac{1}{S_{i}}\sum_{z_{j}\in\mathcal{S}_{i}}f_{i}(w;z_{j})\) is the local empirical risk. In Section 4.1, we will analyze the difference between these two results. Furthermore, we introduce the excess risk analysis to upper bound the test error and further understand how consistency works in the FL paradigm.

### Personalized Relaxed Initialization

In this part, we introduce the relaxed initialization in _FedInit_ method. _FedAvg_ proposes the local-SGD-based implementation in the FL paradigm with a partial participation selection. It allows uniformly selecting a subset of clients \(\mathcal{N}\) to participate in the current training. In each round, it initializes the local model as the last global model. Therefore, after each round, the local models are always far away from each other. The local offset \(w^{t-1}_{i,K}-w^{t}\) is the main culprit leading to inconsistency. Moreover, for different clients, their impacts vary with local heterogeneity. To alleviate this divergence, we propose the _FedInit_ method which adopts the personalized relaxed initialization at the beginning of each round. Concretely, on the selected active clients, it begins the local training from a new personalized state, which moves away from the last global model towards the reverse direction from the latest local state (Line.6 in Algorithm 1). A coefficient \(\beta\) is adopted to control the level of personality. This offset \(\beta(w^{t}-w_{i,K}^{t-1})\) in the relaxed initialization (RI) provides a correction that could help local models gather together after the local training process. Furthermore, this relaxed initialization is irrelevant to the local optimizer, which means, it could be easily incorporated into other methods. Additionally, _FedInit_ does not require extra auxiliary information to communicate. It is a practical technique in FL.

## 4 Theoretical Analysis

In this section, we first introduce the excess risk in FL which could provide a comprehensive analysis on the joint performance of both optimization and generalization. In the second part, we introduce the main assumptions adopted in our proofs and discuss them in different situations. Then we state the main theorems on the analysis of the excess risk of our proposed _FedInit_ method.

### Excess Risk Error in FL

Since Karimireddy et al. [19] pointed out that client-drift problem may seriously damage the performance in the FL paradigm, many previous works [15; 18; 19; 30; 36; 44; 46; 48] have learned its inefficiency in the FL paradigm. However, most of the analyses focus on the studies from the onefold perspective of optimization convergence but ignore investigating its impact on generality. To further provide a comprehensive understanding of how client-drift affects the performance in FL, we adopt the well-known excess risk in the analysis of our proposed _FedInit_ method.

We denote \(w^{T}\) as the model generated by _FedInit_ method after \(T\) communication rounds. Compared with \(f(w^{T})\), we mainly focus on the efficiency of \(F(w^{T})\) which corresponds to its generalization performance. Therefore, we analyze the \(\mathbb{E}[F(w^{T})]\) from the excess risk \(\mathcal{E}_{E}\) as:

\[\mathcal{E}_{E}=\mathbb{E}[F(w^{T})]-\mathbb{E}[f(w^{*})]=\underbrace{\mathbb{ E}[F(w^{T})-f(w^{T})]}_{\mathcal{E}_{G:\text{ _generalization error}}}+\underbrace{\mathbb{E}[f(w^{T})-f(w^{*})]}_{\mathcal{E}_ {O:\text{ _optimization error}}}.\] (3)

Generally, the \(\mathbb{E}[f(w^{*})]\) is expected to be very small and even to zero if the model could well-fit the dataset. Thus \(\mathcal{E}_{E}\) could be considered as the joint efficiency of the generated model \(w^{T}\). Thereinto, \(\mathcal{E}_{G}\) means the different performance of \(w^{T}\) between the training dataset and the test dataset, and \(\mathcal{E}_{O}\) means the similarity between \(w^{T}\) and optimization optimum \(w^{\star}\) on the training dataset.

### Assumptions

In this part, we introduce some assumptions adopted in our analysis. We will discuss their properties and distinguish the proofs they are used in.

**Assumption 1**: _For \(\forall w_{1},w_{2}\in\mathbb{R}^{d}\), the non-convex local function \(f_{i}\) satisfies \(L\)-smooth if:_

\[\|\nabla f_{i}(w_{1})-\nabla f_{i}(w_{2})\|\leq L\|w_{1}-w_{2}\|.\] (4)

**Assumption 2**: _For \(\forall w\in\mathbb{R}^{d}\), the stochastic gradient is bounded by its expectation and variance as:_

\[\mathbb{E}\left[g_{i,k}^{t}\right]=\nabla f_{i}(w_{i,k}^{t}),\quad\mathbb{E} |g_{i,k}^{t}-\nabla f_{i}(w_{i,k}^{t})\|^{2}\leq\sigma_{i}^{2}.\] (5)

**Assumption 3**: _For \(\forall w\in\mathbb{R}^{d}\), the heterogeneous similarity is bounded on the gradient norm as:_

\[\mathbb{E}\|\nabla f_{i}(w)\|^{2}\leq G^{2}+B^{2}\mathbb{E}\|\nabla f(w)\|^{2}.\] (6)

**Assumption 4**: _For \(\forall w_{1},w_{2}\in\mathbb{R}^{d}\), the global function \(f\) satisfies \(L_{G}\)-Lipschitz if:_

\[\|f(w_{1})-f(w_{2})\|\leq L_{G}\|w_{1}-w_{2}\|.\] (7)

**Assumption 5**: _For \(\forall w\in\mathbb{R}^{d}\), let \(w^{\star}\in\arg\min_{w}f(w)\), the function \(f\) satisfies PL-condition if:_

\[2\mu\left(f(w)-f(w^{\star})\right)\leq\|\nabla f(w)\|^{2}.\] (8)

Discussions.Assumptions 1\(\sim\)3 are three general assumptions to analyze the non-convex objective in FL, which is widely used in the previous works [15; 18; 19; 30; 36; 44; 46; 48]. Assumption 4 is used to bound the uniform stability for the non-convex objective, which is used in [11; 51]. Different from the analysis in the margin-based generalization bound [27; 29; 31; 38] that focus on understanding how the designed objective affects the final generalization performance, our work focuses on understanding how the generalization performance changes in the training process. We consider the entire training process and adopt uniform stability to measure the global generality in FL. For the general non-convex objective, one often uses the gradient norm \(\mathbb{E}\|\nabla f(w)\|^{2}\) instead of bounding the loss difference \(\mathbb{E}\left[f(w^{T})-f(w^{\star})\right]\) to measure the optimization convergence. To construct and analyze the excess risk, and further understand how the consistency affects the FL paradigm, we follow [51] to use Assumption 5 to bound the loss distance. Through this, we can establish a theoretical framework to jointly analyze the trade-off on the optimization and generalization in the FL paradigm.

### Main Theorems

#### 4.3.1 Optimization Error \(\mathcal{E}_{o}\)

**Theorem 1**: _Under Assumptions 1\(\sim\)3, let participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL}\right\}\) where \(K\geq 2\), let the relaxation coefficient \(\beta\leq\frac{\sqrt{2}}{12}\), and after training \(T\) rounds, the global model \(w^{t}\) generated by FedInit satisfies:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|f(w^{t})\|^{2}\leq\frac{2 \left(f(w^{0})-f(w^{\star})\right)}{\lambda\eta KT}+\frac{\kappa_{2}\eta L}{ \lambda N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta KL}{\lambda N}G^{2},\] (9)

_where \(\lambda\in(0,1)\), \(\kappa_{1}=\frac{1300\beta^{2}}{1-72\beta^{2}}+17\), and \(\kappa_{2}=\frac{1020\beta^{2}}{1-72\beta^{2}}+13\) are three constants. Further, by selecting the proper learning rate \(\eta=\mathcal{O}(\sqrt{\frac{N}{KT}})\) and let \(D=f(w^{0})-f(w^{\star})\) as the initialization bias, the global model \(w^{t}\) satisfies:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|f(w^{t})\|^{2}\leq\mathcal{O}\left( \frac{D+L\left(\sigma_{l}^{2}+KG^{2}\right)}{\sqrt{NKT}}\right).\] (10)

Theorem 1 provides the convergence rate of the _FedInit_ method without the _PL-condition_, which could achieve the \(\mathcal{O}(1/\sqrt{NKT})\) with the linear speedup of \(N\times\). The dominant term of the training convergence rate is the heterogeneous bias \(G\), which is \(K\times\) larger than the initialization bias \(D\) and stochastic bias \(\sigma_{l}\). According to the formulation (10), by ignoring the initialization bias, the best local interval \(K=\mathcal{O}(\sigma_{l}^{2}/G^{2})\). This selection also implies that when \(G\) increases, which means the local heterogeneity increases, the local interval \(K\) is required to decrease appropriately to maintain the same efficiency. More importantly, though _FedInit_ adopts a weighted bias on the initialization state at the beginning of each communication round, the divergence term \(\mathbb{E}\|w_{i,K}^{t-1}-w^{t}\|^{2}\) does not affect the convergence bound whether \(\beta\) is \(0\) or not. This indicates that the FL paradigm allows a divergence of local clients from the optimization perspective. Proof details are stated in Appendix A.2.3.

**Theorem 2**: _Under Assumptions 1\(\sim\)3 and 5, let participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL},\frac{1}{\lambda\mu K}\right\}\) where \(K\geq 2\), let the relaxation coefficient \(\beta\leq\frac{\sqrt{2}}{12}\), and after training \(T\) rounds, the global model \(w^{t}\) generated by FedInit satisfies:_

\[\mathbb{E}[f(w^{T})-f(w^{\star})]\leq e^{-\lambda\mu\eta KT}\mathbb{E}[f(w^{0 })-f(w^{\star})]+\frac{3\kappa_{1}\eta KL}{2N\lambda\mu}G^{2}+\frac{\kappa_{2 }\eta L}{2N\lambda\mu}\sigma_{l}^{2},\] (11)

_where \(\lambda,\kappa_{1},\kappa_{2}\) is defined in Theorem 1. Further, by selecting the proper learning rate \(\eta=\mathcal{O}(\frac{\log(\lambda\mu NKT)}{\lambda\mu KT})\) and let \(D=f(w^{0})-f(w^{\star})\) as the initialization bias, the global model \(w^{t}\) satisfies:_

\[\mathbb{E}[f(w^{T})-f(w^{\star})]=\widetilde{\mathcal{O}}\left( \frac{D+L(\sigma_{l}^{2}+KG^{2})}{NKT}\right).\] (12)

To bound the \(\mathcal{E}_{O}\) term, we adopt Assumption 5 and prove that _FedInit_ method could achieve the \(\mathcal{O}(1/NKT)\) rate where we omit the \(\mathcal{O}(\log(NKT))\) term. It maintains the properties stated in the Theorem 1. Detailed proofs of the convergence bound are stated in Appendix A.2.4.

#### 4.3.2 Generalization Error \(\mathcal{E}_{g}\)

**Uniform Stability.** One powerful analysis of the generalization error is the uniform stability [11, 21, 50]. It says, for a general proposed method, its generalization error is always lower than the bound of uniform stability. We assume that there is a new set \(\widetilde{\mathcal{C}}\) where \(\mathcal{C}\) and \(\widetilde{\mathcal{C}}\) differ in at most one data sample on the \(i^{*}\)-th client. Then we denote the \(w^{T}\) and \(\widetilde{w}^{T}\) as the generated model after training \(T\) rounds on these two sets, respectively. Thus, we have the following lemma:

**Lemma 1**: _(Uniform Stability. [11]) For the two models \(w^{T}\) and \(\widetilde{w}^{T}\) generated as introduced above, a general method satisfies \(\epsilon\)-uniformly stability if:_

\[\sup_{z_{j}\sim(\mathcal{D}_{i})}\mathbb{E}[f(w^{T};z_{j})-f(\widetilde{w}^{T };z_{j})]\leq\epsilon.\] (13)

_Moreover, if a general method satisfies \(\epsilon\)-uniformly stability, then its generalization error satisfies \(\mathcal{E}_{G}\leq\sup_{z_{j}\sim\{\mathcal{D}_{i}\}}\mathbb{E}[f(w^{T};z_{j} )-f(\widetilde{w}^{T};z_{j})]\leq\epsilon\)[50]._

**Theorem 3**: _Under Assumptions 1, 2, 4, and 5, let all conditions above be satisfied, let learning rate \(\eta=\mathcal{O}(\frac{1}{KT})=\frac{c}{T}\) where \(c=\frac{\mu_{0}}{R}\) is a constant, and let \(|\mathcal{S}_{i}|=S\) as the number of the data samples, by randomly selecting the sample \(z\), we can bound the uniform stability of our proposed FedInit as:_

\[\mathbb{E}\|f(w^{T+1};z)-f(\widetilde{w}^{T+1};z)\|\] (14) \[\leq\frac{1}{S-1}\bigg{[}\frac{2(L_{G}^{2}+SL_{G}\sigma_{l})(UTK) ^{cL}}{L}\bigg{]}^{\frac{1}{1+cL}}+(1+\beta)^{\frac{1}{9cL}}\bigg{[}\frac{ULTK }{2(L_{G}^{2}+SL_{G}\sigma_{l})}\bigg{]}^{\frac{cL}{1+cL}}\sum_{t=1}^{T}\frac {\sqrt{\Delta^{t}}}{T},\]

_where \(U\) is a constant and \(\Delta^{t}=\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\|w_{i,K}^{t-1}-w^{t}\|^ {2}\) is the divergence term at round \(t\)._

For the generalization error, Theorem 3 indicates that \(\mathcal{E}_{G}\) term contains two main parts. The first part comes from the stochastic gradients as the vanilla centralized training process [11], which is of the order \(\mathcal{O}((TK)^{\frac{cL}{1+cL}}/S)\). The constant \(c\) is of the order \(\mathcal{O}(1/K)\) as \(c=\frac{\mu_{0}}{K}\), thus we have \(\frac{cL}{1+cL}=\frac{\mu_{0}}{K+\mu_{0}L}\). If we assume the \(\mu_{0}L\) is generally small, we always expect to adopt a larger \(K\) in the FL paradigm to reduce generalization error. For instance, if we select \(K\rightarrow\infty\), then \(\mathcal{O}((TK)^{\frac{cL}{1+cL}}/S)\rightarrow\mathcal{O}(T^{\frac{cL}{1+cL }}/S)\) which is a very strong upper bound of the generalization error. However, the selection of local interval \(K\) must be restricted from the optimization conditions and we will discuss the details in Section 4.3.4. In addition, the second part in Theorem 3 comes from the divergence term, which is a unique factor in the FL paradigm. As we mentioned above, the divergence term measures the authentic client-drift in the training process. The divergence term is not affected by the number of samples \(S\) and it is only related to the proposed method and the local heterogeneity of the dataset. Proof details are stated in Appendix A.3.

#### 4.3.3 Divergence Term

In the former two parts, we provide the complete theorem to measure optimization error \(\mathcal{E}_{O}\) and generalization error \(\mathcal{E}_{G}\). And we notice that, in the FL paradigm, the divergence term mainly affects the generalization ability of the model instead of the optimization convergence. In this part, we focus on the analysis of the divergence term of our proposed _FedInit_ method. Due to the relaxed initialization at the beginning of each communication round, according to the Algorithm 1, we have \(w_{i,K}^{t}=w^{t}+\beta(w^{t}-w_{i,K}^{t-1})-\eta\sum_{k=0}^{K-1}g_{i,k}^{t}\). Thus, we have the following recursive relationship:

\[\underbrace{w^{t+1}-w_{i,K}^{t}}_{\text{local divergence at $t+1$}}=\beta\ \underbrace{(w_{i,K}^{t-1}-w^{t})}_{\text{local divergence at $t$}}+\underbrace{(w^{t+1}-w^{t})}_{\text{global update}}+\sum_{k=0}^{K-1}\eta_{i,k}^{t}.\] (15)

According to the formulation (15), we can bound the divergence \(\Delta^{t}\) via the following two theorems.

**Theorem 4**: _Under Assumptions 1\(\sim\)3, we can bound the divergence term as follows. Let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL},\frac{\sqrt{N}}{\sqrt{CKL}}\right\}\) where \(K\geq 2\), and after training \(T\) rounds, let \(0<\beta<\frac{\sqrt{6}}{24}\), the divergence term \(\{\Delta^{t}\}\) generated by FedInit satisfies:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\Delta^{t}=\mathcal{O}\left(\frac{N(\sigma_{l}^{2}+ KG^{2})}{T}+\frac{\sqrt{NK}B^{2}\left[D+L(\sigma_{l}^{2}+KG^{2})\right]}{T^{ \frac{3}{2}}}\right).\] (16)

Theorem 4 points out the convergence order of the divergence \(\Delta^{t}\) generated by _FedInit_ during the training process. This bound matches the conclusion in Theorem 1 with the same learning rate. The dominant term achieves the \(\mathcal{O}(NK/T)\) rate on the heterogeneity bias \(G\). It could be seen that the number of selected clients \(N\) will inhibit its convergence and the local consistency linearly increases with \(N\). Different from the selection in Theorem 1, local interval \(K\) is expected as small enough to maintain the high consistency. Also, the initialization bias \(D\) is no longer dominant in consistency. We omit the constant weight \(\frac{1}{1-96\beta^{2}}\) in this upper bound. Proof details are stated in Appendix A.2.5.

**Theorem 5**: _Under Assumptions 1-3 and 5, we can bound the divergence term as follows. Let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL},\frac{1}{\lambda\mu K}\right\}\) where \(K\geq 2\), and after training \(T\) rounds, let \(0<\beta<\frac{\sqrt{6}}{24}\), the divergence term \(\Delta^{T}\) generated by FedInit satisfies:_

\[\Delta^{T}=\widetilde{\mathcal{O}}\left(\frac{D+G^{2}}{T^{2}}+\frac{N\sigma_{ 1}^{2}+KG^{2}}{NKT^{2}}+\frac{1}{NKT^{3}}\right).\] (17)

Theorem 5 indicates the convergence of the divergence \(\Delta\) under the _PL-condition_ which matches the conclusion in Theorem 2 with the same learning rate selection. Assumption 5 establishes a relationship between the gradient norm and the loss difference on the non-convex function \(f\). Different from the Theorem 4, the initialization bias \(D\) and the heterogeneous bias \(G\) are the dominant terms. Under Assumption 5, the _FedInit_ supports a larger local interval \(K\) in the training process. This conclusion also matches the selection of \(K\) in Theorem 2. When the model converges, _FedInit_ guarantees the local models towards the global optimum under at least \(\mathcal{O}(1/T^{2})\) rate. Similarly, we omit the constant weight \(\frac{1}{1-96\beta^{2}}\) and we will discuss the \(\beta\) in Section 4.3.4. Proof details are stated in Appendix A.2.6.

#### 4.3.4 Excess Risk

In this part, we analyze the excess risk \(\mathcal{E}_{E}\) of _FedInit_ method. According to the theorems above,

**Theorem 6**: _Under Assumption 1-5, let the participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfies \(\eta\leq\min\{\frac{N}{2CKL},\frac{1}{NKL},\frac{1}{\lambda\mu K},\}\) where \(K\geq 2\), let the relaxed coefficient \(0\leq\beta<\frac{\sqrt{6}}{24}\), and let \(|\mathcal{S}_{i}|=S\). By selecting the learning rate \(\eta=\mathcal{O}(\frac{\log(\lambda\mu NKT)}{\lambda\mu KT})\leq\frac{e}{t}\), after training \(T\) communication rounds, the excess risk of the FedInit method achieves:_

\[\mathcal{E}_{E}\leq\underbrace{\widetilde{\mathcal{O}}\left(\frac{D+L(\sigma_ {1}^{2}+KG^{2})}{NKT}\right)}_{\text{optimization bias}}+\underbrace{\mathcal{O} \left(\frac{1}{S}\left[\sigma_{l}(TK)^{eL}\right]^{\frac{1}{1+cL}}\right)}_{ \text{stability bias}}+\underbrace{\widetilde{\mathcal{O}}\left(\frac{\sqrt{D+G^{ 2}}K^{\frac{eL}{1+cL}}}{T^{\frac{1}{1+cL}}}\right)}_{\text{divergence bias}}.\] (18)

According to the Theorems 2, 3, and 5, we combine their dominant terms to upper bound the excess risk of _FedInit_ method. The first term comes from the optimization error, the second term comes from the stability bias, and the third term comes from the divergence bias. From the perspective of excess risk, the main restriction in the FL paradigm is the divergence term with the bound of \(\widetilde{\mathcal{O}}(\frac{1}{T^{\frac{1}{1+cL}}})\). The second term of excess risk matches the conclusion in SGD [11, 51] which relies on the number \(S\). Our analysis of the excess risk reveals two important corollaries in FL:

* From the perspective of optimization, the FL paradigm is insensitive to local consistency in the training process (Theorems 1&2).
* From the perspective of generalization, the local consistency level significantly affects the performance in the FL paradigm (Theorem 6).

Then we discuss the best selection of the local interval \(K\) and relaxed coefficient \(\beta\).

**Selection of K.** In the first term, to minimize the optimization error, the local interval \(K\) is required to be large enough. In the second term, since \(\frac{cL}{1+cL}\leq 1\), the upper bound expects a small local interval \(K\). In the third term, since \(\frac{1}{1+cL}=\frac{K}{K+\mu_{0}L}<1\), it expects a large \(K\) to guarantee the order of \(T\) to approach \(\mathcal{O}(1/T)\), where the divergence bias could maintain a high-level consistency. Therefore, there is a specific optimal constant selection for \(K>1\) to minimize the excess risk.

**Selection of \(\beta\).** As the dominant term, the coefficient of the divergence bias also plays a key role in the error bound. In Theorem 5, the constant weight we omit for the divergence term \(\Delta^{T}\) is \(\frac{1}{1-96\beta^{2}}\). Thus the coefficient of \(\sqrt{\Delta^{T}}\) is \(\frac{1}{\sqrt{1-96\beta^{2}}}\). Combined with Theorem 3, we have the coefficient for the 

[MISSING_PAGE_FAIL:8]

### Experiment results

In this part, we mainly introduce the experiment results compared with the other benchmarks.

In Table 1, our proposed _FedInit_ method performs well than the other benchmarks with good stability across different experimental setups. On the results of ResNet-18-GN on CIFAR-10, it achieves about 3.42\(\%\) improvement than the vanilla _FedAvg_ on the high heterogeneous splitting with \(D_{r}=0.1\). When the participation ratio decreases to \(5\%\), the accuracy drops only about \(0.1\%\) while _FedAvg_ drops almost \(1.88\%\). Similar results on CIFAR-100, when the ratio decreases, _FedInit_ still achieves \(43.77\%\) while the second best method _SCAFFOLD_ drops about \(3.21\%\). This indicates the proposed _FedInit_ holds good stability on the varies of the participation. In addition, in Table 2, we incorporate the relaxed initialization (RI) into the other benchmarks to test its benefit. "-" means the vanilla benchmarks, and "+RI" means adopting the relaxed initialization. It shows that the relaxed initialization holds the promising potential to further enhance the performance. Actually, _FedInit_ could be considered as (RI + _FedAvg_), whose improvement achieves about over \(3\%\) on each setup. Table 1 shows the poor performance of the vanilla _FedAvg_. Nevertheless, when adopting the RI, _FedInit_ remains above most benchmarks on several setups. When the RI is incorporated into other benchmarks, it helps them to achieve higher performance without additional communication costs.

### Ablation

In this part, we mainly introduce the ablation results of different hyperparameters.

**Hyperparameters Sensitivity.** The excess risk and test error of _FedInit_ indicate there exists best selections for local interval \(K\) and relaxed coefficient \(\beta\), respectively. In this part, we test a series of selections to validate our conclusions. To be aligned with previous studies, we denote \(K\) as training epochs. In Figure 1 (a), we can see that the selection range of the beta is very small while it has great potential to improve performance. When it is larger than the threshold, the training process will diverge quickly. As local interval \(K\) increases, test accuracy rises first and then decreases. Our analysis provides a clear explanation of the phenomenon. The optimization error decreases as \(K\) increases when it is small. When \(K\) exceeds the threshold, the divergence term in generalization cannot be ignored. Therefore, the test accuracy will be significantly affected.

**Consistency.** In this part, we test the relationship between the test accuracy and divergence term \(\Delta^{T}\) under different \(\beta\) selections. As introduced in Algorithm 1 Line.6, negative \(\beta\) means to adopt the relaxed initialization which is close to the latest local model. _FedInit_ degrades to _FedAvg_ when \(\beta=0\). Table 3 validates that RI is required to be far away from the local model (a positive \(\beta\)). When \(\beta\) is small, the correction is limited. The local divergence term is difficult to be diminished efficiently. While it becomes too large, the local training begins from a bad initialization, which can not receive enough guidance of global information from the global models. Furthermore, as shown in Table 3, if the initialization is too far from the local model, the quality of the initialization state will not be effectively guaranteed.

\begin{table}
\begin{tabular}{c c|c c|c c|c c|c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c}{\(10\%\)-100 clients} & \multicolumn{3}{c}{\(5\%\)-200 clients} \\ \cline{2-10}  & \multicolumn{2}{c}{Dir-0.6} & \multicolumn{2}{c}{Dir-0.1} & \multicolumn{2}{c}{Dir-0.6} & \multicolumn{2}{c}{Dir-0.1} \\ \cline{2-10}  & - & +RI & - & +RI & - & +RI & - & +RI \\ \hline FedAvg & 78.77 & 83.11 & 72.53 & 75.95 & 74.81 & 80.58 & 70.65 & 74.92 \\ FedAdam & 76.52 & 78.33 & 70.44 & 72.55 & 73.28 & 78.33 & 68.87 & 71.34 \\ FedSAM & 79.23 & **83.36** & **72.89** & 76.34 & 75.45 & 80.66 & 71.23 & 75.08 \\ SCAFFOLD & 81.37 & 83.27 & 75.06 & **77.30** & 78.17 & **81.02** & 74.24 & **76.22** \\ FedDyn & 82.43 & 81.91 & 75.08 & 75.11 & 79.96 & 79.8 & 74.15 & 74.34 \\ FedCM & 81.67 & 81.77 & 73.93 & 73.71 & 79.49 & 79.72 & 73.12 & 72.98 \\ \hline \end{tabular}
\end{table}
Table 2: We incorporate the relaxed initialization (RI) into the benchmarks to test improvements on ResNet-18-GN on CIFAR-10 with the same hyperparameters and specific relaxed coefficient \(\beta\).

Figure 1: THyperparameters sensitivity studies of local intervals \(K\) and relaxed coefficient \(\beta\) of the _FedInit_ method on CIFAR-10. To fairly compare their efficiency, we fix the total communication rounds \(T=500\).

### Discussions of Relaxed Initialization

In this part, we mainly discuss the improvements of the proposed relaxed initialization.

In vanilla classical _FedAvg_ and the most advanced methods, at the beginning of each communication round, we are always caught in a misunderstanding of the high consistency. Because the target of FL is a globally consistent solution, it is always an involuntary aggregation in the algorithm to ensure consistency. We prove that this does contribute to the efficiency of the optimization process, but it is not the best selection for generalization. To better improve the generalization, we prove that a relaxed initialization state will contribute more. We compare their difference in Figure 2.

As shown in the above figure, we can clearly see why _FedInit_ contributes more to the consistency. When we move a little in the opposite direction of the last local optimization state, we will move further away from local optimal solutions in the current communication round. The working mode of RI is similar to the idea of "lookahead". Differently, (1) "lookahead" only works at the end of each stage; (2) "lookahead" only works for the global models on the global server. However, RI helps each local client to backtrack a small distance at the beginning of each stage. Therefore, after the local training in the next stage, the trained local models will get closer to each other than before.

## 6 Conclusion

In this work, we propose an efficient and novel FL method, dubbed _FedInit_, which adopts the stage-wise personalized relaxed initialization to enhance the local consistency level. Furthermore, to clearly understand the essential impact of consistency in FL, we introduce the excess risk analysis in FL and study the divergence term. Our proofs indicate that consistency dominates the test error and generalization error bound while optimization error is insensitive to it. Extensive experiments are conducted to validate the efficiency of relaxed initialization. As a practical and light plug-in, it could also be easily incorporated into other FL paradigms to improve their performance.

**Limitations & Broader Impact.** In this work, we analyze the excess risk for the _FedInit_ method to understand how consistency works in FL. Actually, the relaxed initialization may also work for the personalized FL (pFL) paradigm. It is a future study to explore its properties in the pFL and decentralized FL, which may inspire us to design novel efficient algorithms in the FL community.

## Acknowledgements

Prof. Dacheng Tao is partially supported by Australian Research Council Project FL-170100117.

Figure 2: Schematics of the classical _FedAvg_ and our proposed _FedInit_.

## References

* [1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. _arXiv preprint arXiv:2111.04263_, 2021.
* [2] Muhammad Asad, Ahmed Moustafa, and Takayuki Ito. Fedopt: Towards communication efficiency and privacy preservation in federated learning. _Applied Sciences_, 10(8):2864, 2020.
* [3] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. _Advances in neural information processing systems_, 30, 2017.
* [4] Debora Caldarola, Barbara Caputo, and Marco Ciccone. Improving generalization in federated learning by seeking flat minima. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIII_, pages 654-672. Springer, 2022.
* [5] Zachary Charles and Jakub Konecny. Convergence and accuracy trade-offs in federated learning and meta-learning. In _International Conference on Artificial Intelligence and Statistics_, pages 2575-2583. PMLR, 2021.
* [6] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. _Advances in neural information processing systems_, 27, 2014.
* [7] Farzan Farnia, Jesse M Zhang, and David Tse. Generalizable adversarial training via spectral normalization. _arXiv preprint arXiv:1811.07457_, 2018.
* [8] Liang Gao, Huazhu Fu, Li Li, Yingwen Chen, Ming Xu, and Cheng-Zhong Xu. Feddc: Federated learning with non-iid data via local drift decoupling and correction. _arXiv preprint arXiv:2203.11751_, 2022.
* [9] Yonghai Gong, Yichuan Li, and Nikolaos M Freris. Fedadmm: A robust federated deep learning framework with adaptivity to system heterogeneity. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_, pages 2575-2587. IEEE, 2022.
* [10] Eduard Gorbunov, Filip Hanzely, and Peter Richtarik. Local sgd: Unified theory and new efficient methods. In _International Conference on Artificial Intelligence and Statistics_, pages 3556-3564. PMLR, 2021.
* [11] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [13] Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of decentralized machine learning. In _International Conference on Machine Learning_, pages 4387-4398. PMLR, 2020.
* [14] Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. _arXiv preprint arXiv:1909.06335_, 2019.
* [15] Tiansheng Huang, Li Shen, Yan Sun, Weiwei Lin, and Dacheng Tao. Fusion of global and local knowledge for personalized federated learning. _arXiv preprint arXiv:2302.11051_, 2023.
* [16] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. _Advances in neural information processing systems_, 26, 2013.
* [17] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [18] Belhal Karimi, Ping Li, and Xiaoyun Li. Layer-wise and dimension-wise locally adaptive federated learning. _arXiv preprint arXiv:2110.00532_, 2021.
* [19] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

* [21] Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In _International Conference on Machine Learning_, pages 2815-2824. PMLR, 2018.
* [22] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Mazair Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_, 2:429-450, 2020.
* [23] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.
* [24] Yixing Liu, Yan Sun, Zhengtao Ding, Li Shen, Bo Liu, and Dacheng Tao. Enhance local consistency in federated learning: A multi-step inertial momentum approach. _arXiv preprint arXiv:2302.05726_, 2023.
* [25] Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, and Peter Richtarik. From local sgd to local fixed-point methods for federated learning. In _International Conference on Machine Learning_, pages 6692-6701. PMLR, 2020.
* [26] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* [27] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. _arXiv preprint arXiv:1707.09564_, 2017.
* [28] Emre Ozfatura, Kerem Ozfatura, and Deniz Gunduz. Fedadc: Accelerated federated learning with drift control. In _2021 IEEE International Symposium on Information Theory (ISIT)_, pages 467-472. IEEE, 2021.
* [29] Zhe Qu, Xingyu Li, Rui Duan, Yao Liu, Bo Tang, and Zhuo Lu. Generalized federated learning via sharpness aware minimization. In _International Conference on Machine Learning_, pages 18250-18280. PMLR, 2022.
* [30] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint arXiv:2003.00295_, 2020.
* [31] Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning: The case of affine distribution shifts. _Advances in Neural Information Processing Systems_, 33:21554-21565, 2020.
* [32] Naichen Shi, Fan Lai, Raed Al Kontar, and Mosharaf Chowdhury. Fed-ensemble: Improving generalization through model ensembling in federated learning. _arXiv preprint arXiv:2107.10663_, 2021.
* [33] Yifan Shi, Yingqi Liu, Kang Wei, Li Shen, Xueqian Wang, and Dacheng Tao. Make landscape flatter in differentially private federated learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 24552-24562, 2023.
* [34] Yifan Shi, Li Shen, Kang Wei, Yan Sun, Bo Yuan, Xueqian Wang, and Dacheng Tao. Improving the model consistency of decentralized federated learning. _arXiv preprint arXiv:2302.04083_, 2023.
* [35] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.
* [36] Hao Sun, Li Shen, Qihuang Zhong, Liang Ding, Shixiang Chen, Jingwei Sun, Jing Li, Guangzhong Sun, and Dacheng Tao. Adasam: Boosting sharpness-aware minimization with adaptive learning rate and momentum for training deep neural networks. _arXiv preprint arXiv:2303.00565_, 2023.
* [37] Yan Sun, Li Shen, Shixiang Chen, Liang Ding, and Dacheng Tao. Dynamic regularized sharpness aware minimization in federated learning: Approaching global consistency and smooth landscape. _arXiv preprint arXiv:2305.11584_, 2023.
* [38] Yan Sun, Li Shen, Tiansheng Huang, Liang Ding, and Dacheng Tao. Fedspeed: Larger local interval, less communication round, and higher generalization accuracy. _arXiv preprint arXiv:2302.10429_, 2023.
* [39] Yan Sun, Li Shen, Hao Sun, Liang Ding, and Dacheng Tao. Efficient federated learning via local adaptive amended optimizer with linear speedup. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.
* [40] Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto: Federated prototype learning across heterogeneous clients. In _AAAI Conference on Artificial Intelligence_, volume 1, 2022.

* [41] Han Wang, Siddartha Marella, and James Anderson. Fedadmm: A federated primal-dual algorithm allowing partial participation. In _2022 IEEE 61st Conference on Decision and Control (CDC)_, pages 287-294. IEEE, 2022.
* [42] Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat. Slowmo: Improving communication-efficient distributed sgd with slow momentum. _arXiv preprint arXiv:1910.00643_, 2019.
* [43] Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. _Advances in neural information processing systems_, 33:7611-7623, 2020.
* [44] Jianyu Wang, Zheng Xu, Zachary Garrett, Zachary Charles, Luyang Liu, and Gauri Joshi. Local adaptivity in federated learning: Convergence and consistency. _arXiv preprint arXiv:2106.02305_, 2021.
* [45] Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous distributed learning. _Advances in Neural Information Processing Systems_, 33:6281-6292, 2020.
* [46] Jing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao. Fedcm: Federated learning with client-level momentum. _arXiv preprint arXiv:2106.10874_, 2021.
* [47] Semih Yagli, Alex Dytso, and H Vincent Poor. Information-theoretic bounds on the generalization error and privacy leakage in federated learning. In _2020 IEEE 21st International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)_, pages 1-5. IEEE, 2020.
* [48] Haibo Yang, Minghong Fang, and Jia Liu. Achieving linear speedup with partial worker participation in non-iid federated learning. _arXiv preprint arXiv:2101.11203_, 2021.
* [49] Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning framework with adaptivity to non-iid data. _IEEE Transactions on Signal Processing_, 69:6055-6070, 2021.
* [50] Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pingali, Chao Chen, and Mayank Goswami. Stability of sgd: Tightness analysis and improved bounds. In _Uncertainty in Artificial Intelligence_, pages 2364-2373. PMLR, 2022.
* [51] Pan Zhou, Hanshu Yan, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan. Towards understanding why lookahead generalizes better than sgd and beyond. _Advances in Neural Information Processing Systems_, 34:27290-27304, 2021.
* [52] Shenglong Zhou and Geoffrey Ye Li. Federated learning via inexact admm. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.

Proofs

In this section, we introduce our proofs of the main theorems in the main context. In the first part, we introduce some assumptions used in our proofs and point out their functions used for which part. In the second part, we prove the convergence rate and optimization error under the general assumptions. In the third part, we prove the uniform stability to measure the generalization error and analyze how each term affects the accuracy.

We suppose there are \(C\) clients participating in the training process and each has a local heterogeneous dataset. In each round \(t\), we randomly select \(N\) clients to send the global model and they will train \(K\) iterations to get \(N\) local models. The local models will be aggregated on the global server as the next global model. After \(T\) rounds, our method generates a global model as the final state. We denote the total client set as \(\mathcal{C}\) and the selected client set as \(\mathcal{N}\).

### Assumptions

In this part, we state assumptions in our proofs and discuss them. We will introduce each assumption and develop their corollaries.

**Assumption 6**: _For \(\forall w_{1},w_{2}\in\mathbb{R}^{d}\), the non-convex local function \(f_{i}\) satisfies \(L\)-smooth if:_

\[\|\nabla f_{i}(w_{1})-\nabla f_{i}(w_{2})\|\leq L\|w_{1}-w_{2}\|,\] (19)

_where \(L\) is a universal constant._

**Assumption 7**: _For \(\forall w\in\mathbb{R}^{d}\), the stochastic gradient is bounded by its expectation and variance as:_

\[\mathbb{E}\left[g_{i,k}^{t}\right]=\nabla f_{i}(w_{i,k}^{t}),\] (20) \[\mathbb{E}\|g_{i,k}^{t}-\nabla f_{i}(w_{i,k}^{t})\|^{2}\leq \sigma_{l}^{2},\]

_where \(\sigma_{l}>0\) is a universal constant._

**Assumption 8**: _For \(\forall w\in\mathbb{R}^{d}\), the heterogeneous similarity is bounded on the gradient norm as:_

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\|\nabla f_{i}(w)\|^{2}\leq G^{2}+B^{2}\| \nabla f(w)\|^{2},\] (21)

_where \(G\geq 0\) and \(B\geq 1\) are two universal constants._

**Assumption 9**: _For \(\forall w_{1},w_{2}\in\mathbb{R}^{d}\), the global function \(f\) satisfies \(L_{G}\)-Lipschitz if:_

\[\|f(w_{1})-f(w_{2})\|\leq L_{G}\|w_{1}-w_{2}\|,\] (22)

_where \(L_{G}\) is a universal constant._

**Assumption 10**: _For \(\forall w\in\mathbb{R}^{d}\), let \(w^{\star}\in\arg\min_{w}f(w)\), the global function satisfies PL-condition if:_

\[2\mu\left(f(w)-f(w^{\star})\right)\leq\|\nabla f(w)\|^{2},\] (23)

_where \(\mu\) is a universal positive constant._

Discussion.Assumption 6\(\sim\)8 are three general assumptions to analyze the non-convex objective in FL, which is widely used in the previous works [15, 18, 19, 30, 36, 44, 46, 48]. Assumption 9 is used to bound the uniform stability for the non-convex objective, which is used in [11, 51]. Different from the analysis in the margin-based generalization bound [27, 29, 31, 38] that focus on understanding how the designed objective affects the final generalization performance, our work focuses on understanding how the generalization performance changes in the training process. We consider the entire training process and adopt uniform stability to measure the global generality in FL and theoretically study the importance of consistency to FL. For the general non-convex objective, one often uses the gradient norm \(\mathbb{E}\|\nabla f(w)\|^{2}\) instead of the loss difference \(\mathbb{E}\left[f(w^{\star})-f(w)\right]\) to measure the training error. To construct and analyze the _excess risk_ to further understand how the consistency affects the FL paradigm, we follow [51] to use Assumption 10 to bound the loss distance. Through this, we can establish a theoretical framework to jointly analyze the trade-off on the optimization and generalization in the FL paradigm.

### Proofs for the Optimization Error

In this part, we prove the training error for our proposed method. We assume the objective function \(f(w)=\frac{1}{C}\sum_{i\in\mathcal{C}}f_{i}(w)\) is \(L\)-smooth w.r.t \(w\). Then we could upper bound the training error in the FL. Some useful notations in the proof are introduced in the Table 4.

Then we introduce some important lemmas used in the proof.

#### a.2.1 Important Lemmas

**Lemma 2**: (Bounded local updates) _We first bound the local training updates in the local training. Under the Assumptions stated, the averaged norm of the local updates of total \(C\) clients could be bounded as:_

\[V_{t}^{t}\leq 4K\beta^{2}\Delta^{t}+3K^{2}\eta^{2}\left(\sigma_{l}^{2}+4KG^{2} \right)+12K^{3}\eta^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}.\] (24)

**Proof \(V_{1}\)** _measures the norm of the local offset during the local training stage. It could be bounded by two major steps. Firstly, we bound the separated term on the single client \(i\) at iteration \(k\) as:_

\[\mathbb{E}_{t}\|w^{t}-w_{i,k}^{t}\|^{2}\] \[=\mathbb{E}_{t}\|w^{t}-w_{i,k-1}^{t}+\eta\left(g_{i,k-1}^{t}- \nabla f_{i}(w_{i,k-1}^{t})+\nabla f_{i}(w_{i,k-1}^{t})-\nabla f_{i}(w^{t})+ \nabla f_{i}(w^{t})\right)\|^{2}\] \[\leq\left(1+\frac{1}{2K-1}\right)\mathbb{E}_{t}\|w^{t}-w_{i,k-1} ^{t}+\eta\left(g_{i,k-1}^{t}-\nabla f_{i}(w_{i,k-1}^{t})\right)\|^{2}\] \[\quad+2K\eta^{2}\mathbb{E}_{t}\|\nabla f_{i}(w_{i,k-1}^{t})- \nabla f_{i}(w^{t})+\nabla f_{i}(w^{t})\|^{2}\] \[\leq\left(1+\frac{1}{2K-1}\right)\mathbb{E}_{t}\|w^{t}-w_{i,k-1} ^{t}\|^{2}+\eta^{2}\mathbb{E}_{t}\|g_{i,k-1}^{t}-\nabla f_{i}(w_{i,k-1}^{t})\| ^{2}\] \[\quad+4K\eta^{2}\mathbb{E}_{t}\|\nabla f_{i}(w_{i,k-1}^{t})- \nabla f_{i}(w^{t})\|^{2}+4K\eta^{2}\|\nabla f_{i}(w^{t})\|^{2}\] \[\leq\left(1+\frac{1}{2K-1}+4\eta^{2}KL^{2}\right)\mathbb{E}_{t} \|w^{t}-w_{i,k-1}^{t}\|^{2}+\eta^{2}\sigma_{l}^{2}+4K\eta^{2}\|\nabla f_{i}(w^ {t})\|^{2}\] \[\leq\left(1+\frac{1}{K-1}\right)\mathbb{E}_{t}\|w^{t}-w_{i,k-1}^{ t}\|^{2}+\eta^{2}\sigma_{l}^{2}+4K\eta^{2}\|\nabla f_{i}(w^{t})\|^{2},\]

_where the learning rate is required \(\eta\leq\frac{\sqrt{2}}{4(K-1)L}\) for \(K\geq 2\)._

_Computing the average of the separated term on client \(i\), we have:_

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}_{t}\|w^{t}-w_{i,k}^{ t}\|^{2}\] \[\leq\left(1+\frac{1}{K-1}\right)\frac{1}{C}\sum_{i\in\mathcal{C}} \mathbb{E}_{t}\|w^{t}-w_{i,k-1}^{t}\|^{2}+\eta^{2}\sigma_{l}^{2}+4K\eta^{2} \frac{1}{C}\sum_{i\in\mathcal{C}}\|\nabla f_{i}(w^{t})\|^{2}\] \[\leq\left(1+\frac{1}{K-1}\right)\frac{1}{C}\sum_{i\in\mathcal{C} }\mathbb{E}_{t}\|w^{t}-w_{i,k-1}^{t}\|^{2}+\eta^{2}\sigma_{l}^{2}+4K\eta^{2}G ^{2}+4K\eta^{2}B^{2}\|\nabla f(w^{t})\|^{2}.\]

_Unrolling the aggregated term on iteration \(k\leq K\). When local interval \(K\geq 2\), \(\left(1+\frac{1}{K-1}\right)^{k}\leq\left(1+\frac{1}{K-1}\right)^{K}\leq 4\). Then we have:_

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}_{t}\|w^{t}-w_{i,k}^{ t}\|^{2}\] \[\leq\sum_{\tau=0}^{k-1}\left(1+\frac{1}{K-1}\right)^{\tau}\left( \eta^{2}\sigma_{l}^{2}+4K\eta^{2}G^{2}+4K\eta^{2}B^{2}\|\nabla f(w^{t})\|^{2}\right)\] \[\quad+\left(1+\frac{1}{K-1}\right)^{k}\frac{1}{C}\sum_{i\in \mathcal{C}}\|w^{t}-w_{i,0}^{t}\|^{2}\] \[\leq 3(K-1)\left(\eta^{2}\sigma_{l}^{2}+4K\eta^{2}G^{2}+4K\eta^{2}B^ {2}\|\nabla f(w^{t})\|^{2}\right)+4\beta^{2}\frac{1}{C}\sum_{i\in\mathcal{C} }\mathbb{E}_{t}\|w^{t}-w_{i,K}^{t-1}\|^{2}\]

\begin{table}
\begin{tabular}{c c c} \hline \hline Notation & Formulation & Description \\ \hline \(w_{i,k}^{t}\) & - & parameters at \(k\)-th iteration in round \(t\) on client \(i\) \\ \(w^{t}\) & - & global parameters in round \(t\) \\ \(V_{1}^{t}\) & \(\frac{1}{C}\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\|w_{i,k}^{t}-w^{t}\| ^{2}\) & averaged norm of the local updates in round \(t\) \\ \(V_{2}^{t}\) & \(\mathbb{E}\|w^{t+1}-w^{t}\|^{2}\) & norm of the global updates in round \(t\) \\ \(\Delta^{t}\) & \(\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\|w_{i,k-1}^{t-1}-w^{t}\|^{2}\) & inconsistency/divergence term in round \(t\) \\ \(D\) & \(f(w^{0})-f(w^{*})\) & bias between the initialization state and optimal \\ \hline \hline \end{tabular}
\end{table}
Table 4: Some abbreviations of the used terms in the proof of bounded training error.

\[\leq 3K\eta^{2}\left(\sigma_{l}^{2}+4KG^{2}\right)+12K^{2}\eta^{2}B^{2}\| \nabla f(w^{t})\|^{2}+4\beta^{2}\Delta^{t}.\]

_Summing the iteration on \(k=0,1,\cdots,K-1\),_

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}_{t}\|w^{t}-w^{t}_{i,k}\|^{2}\leq 4K\beta^{2}\Delta^{t}+3K^{2}\eta^{2}\sigma_{l}^{2}+12K^{3}\eta^{2} G^{2}+12K^{3}\eta^{2}B^{2}\|\nabla f(w^{t})\|^{2}.\]

_This completes the proof._

**Lemma 3**: (Bounded global updates) _The norm of the global update could be bounded by uniformly sampling. Under assumptions stated above, let \(\eta\leq\frac{1}{KL}\), the norm of the global update of selected \(N\) clients could be bounded as:_

\[\begin{split} V_{2}^{t}&\leq\frac{15\beta^{2}}{N} \Delta^{t}+\frac{10\eta^{2}K}{N}\sigma_{l}^{2}+\frac{39\eta^{2}K^{2}}{N}G^{2} \\ &\quad+\frac{39\eta^{2}K^{2}B^{2}}{N}\mathbb{E}\|\nabla f(w^{t}) \|^{2}+\frac{\eta^{2}}{CN}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1} \nabla f_{i}(w^{t}_{i,k})\|^{2}.\end{split}\] (25)

**Proof**\(V_{2}\) _measures the variance of the global offset after each communication round. We define an indicator function \(\mathbb{E}_{vent}=1\) if the event happens. Then, to bound it, we firstly split the expectation term:_

\[\mathbb{E}\|w^{t+1}-w^{t}\|^{2}\] \[=\mathbb{E}\|\frac{1}{N}\sum_{i\in\mathcal{N}}w^{t}_{i,K}-w^{t} \|^{2}\] \[=\frac{1}{N^{2}}\mathbb{E}\|\sum_{i\in\mathcal{N}}(w^{t}_{i,K}-w ^{t})\|^{2}\] \[=\frac{1}{N^{2}}\mathbb{E}\|\sum_{i\in\mathcal{C}}(w^{t}_{i,K}-w ^{t})\mathbb{I}_{i\in\mathcal{N}}\|^{2}\] \[=\frac{1}{N^{2}}\mathbb{E}\|\sum_{i\in\mathcal{C}}\mathbb{I}_{i \in\mathcal{N}}\left[\sum_{k=0}^{K-1}\eta g^{t}_{i,k}+\beta(w^{t}-w^{t-1}_{i, K})\right]\|^{2}\] \[=\frac{\eta^{2}}{NC}\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1} \mathbb{E}\|g^{t}_{i,k}-\nabla f_{i}(w^{t}_{i,k})\|^{2}+\frac{1}{N^{2}} \mathbb{E}\|\sum_{i\in\mathcal{C}}\mathbb{I}_{i\in\mathcal{N}}\left[\sum_{k=0 }^{K-1}\eta\nabla f_{i}(w^{t}_{i,k})+\beta(w^{t}-w^{t-1}_{i,K})\right]\|^{2}\] \[\leq\frac{\eta^{2}K\sigma_{l}^{2}}{N}+\frac{1}{N^{2}}\mathbb{E} \|\sum_{i\in\mathcal{C}}\mathbb{I}_{i\in\mathcal{N}}\left[\sum_{k=0}^{K-1} \eta\nabla f_{i}(w^{t}_{i,k})+\beta(w^{t}-w^{t-1}_{i,K})\right]\|^{2}.\]

_To bound the second term, we can adopt the following equation. For the vector \(x_{i}\in\mathbb{R}^{d}\), we have:_

\[\mathbb{E}\|\sum_{i\in\mathcal{C}}\mathbb{I}_{i\in\mathcal{N}}x_ {i}\|^{2} =\mathbb{E}(\sum_{i\in\mathcal{C}}\mathbb{I}_{i\in\mathcal{N}}x_ {i},\sum_{j\in\mathcal{C}}\mathbb{I}_{j\in\mathcal{N}}x_{j})\] \[=\sum_{(i\neq j)\in\mathcal{C}}\mathbb{E}(\mathbb{I}_{i\in \mathcal{N}}x_{i},\mathbb{I}_{j\in\mathcal{N}}x_{j})+\sum_{(i=j)\in\mathcal{C} }\mathbb{E}(\mathbb{I}_{i\in\mathcal{N}}x_{i},\mathbb{I}_{j\in\mathcal{N}}x_ {j})\] \[=\sum_{(i\neq j)\in\mathcal{C}}\mathbb{E}(\mathbb{I}_{i\in \mathcal{N}}x_{i},\mathbb{I}_{j\in\mathcal{N}}x_{j})+\sum_{(i=j)\in\mathcal{C} }\mathbb{E}(\mathbb{I}_{i\in\mathcal{N}}x_{i},\mathbb{I}_{j\in\mathcal{N}}x_ {j})\] \[=\frac{N(N-1)}{C(C-1)}\sum_{(i\neq j)\in\mathcal{C}}\mathbb{E}(x_{i },x_{j})+\frac{N}{C}\sum_{(i=j)\in\mathcal{C}}\mathbb{E}(x_{i},x_{j})\] \[=\frac{N(N-1)}{C(C-1)}\mathbb{E}\|\sum_{i\in\mathcal{C}}x_{i}\|^{ 2}+\frac{N(C-N)}{C(C-1)}\sum_{i\in\mathcal{C}}\mathbb{E}\|x_{i}\|^{2}.\]

_We firstly bound the first term in the above equation. Taking \(x_{i}=\sum_{k=0}^{K-1}\eta\nabla f_{i}(w^{t}_{i,k})+\beta(w^{t}-w^{t-1}_{i,K})\) into \(\mathbb{E}\|\sum_{i\in\mathcal{C}}x_{i}\|^{2}\), we have:_

\[\mathbb{E}\|\sum_{i\in\mathcal{C}}\left[\sum_{k=0}^{K-1}\eta\nabla f_{i}(w^{t}_{ i,k})+\beta(w^{t}-w^{t-1}_{i,K})\right]\|^{2}=\eta^{2}\mathbb{E}\|\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}.\]_Then we bound the second term in above equation. Taking \(x_{i}=\sum_{k=0}^{K-1}\eta\nabla f_{i}(w^{t}_{i,k})+\beta(w^{t}-w^{t-1}_{i,K})\) into \(\sum_{i\in\mathcal{C}}\mathbb{E}\|x_{i}\|^{2}\), we have:_

\[\sum_{i\in\mathcal{C}}\mathbb{E}\|\sum_{k=0}^{K-1}\eta\nabla f_{i} (w^{t}_{i,k})+\beta(w^{t}-w^{t-1}_{i,K})\|^{2}\] \[=\sum_{i\in\mathcal{C}}\mathbb{E}\|\sum_{k=0}^{K-1}\left[\eta \nabla f_{i}(w^{t}_{i,k})+\frac{\beta}{K}(w^{t}-w^{t-1}_{i,K})\right]\|^{2}\] \[\leq K\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\|\eta \nabla f_{i}(w^{t}_{i,k})+\frac{\beta}{K}(w^{t}-w^{t-1}_{i,K})\|^{2}\] \[=K\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\|\eta\nabla f_ {i}(w^{t}_{i,k})-\eta\nabla f_{i}(w^{t})+\eta\nabla f_{i}(w^{t})+\frac{\beta}{ K}(w^{t}-w^{t-1}_{i,K})\|^{2}\] \[\leq 3\eta^{2}KL^{2}\underbrace{\sum_{i\in\mathcal{C}}\sum_{k=0}^ {K-1}\mathbb{E}\|w^{t}_{i,k}-w^{t}\|^{2}}_{CY^{2}_{i}}+3\eta^{2}K^{2}\sum_{i \in\mathcal{C}}\mathbb{E}\|\nabla f_{i}(w^{t})\|^{2}+3\beta^{2}\underbrace{ \sum_{i\in\mathcal{C}}\mathbb{E}\|(w^{t}-w^{t-1}_{i,K})\|^{2}}_{C\Delta^{t}}\] \[\leq 3C\eta^{2}KL^{2}V^{t}_{1}+3C\beta^{2}\Delta^{t}+3C\eta^{2}K^ {2}G^{2}+3C\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}.\]

_We bound all the components in \(V^{t}_{2}\) term. Let \(1\leq N<C\), to generate the final bound, summarizing the inequalities all above and adopting the bounded \(V^{r}_{1}\) in Lemma 2, then we have:_

\[V^{t}_{2} \leq\frac{\eta^{2}K\sigma^{2}_{t}}{N}+\frac{1}{N^{2}}\mathbb{E} \|\sum_{i\in\mathcal{C}}\mathbb{I}_{i\in\mathcal{N}}\left[\sum_{k=0}^{K-1}\eta \nabla f_{i}(w^{t}_{i,k})+\beta(w^{t}-w^{t-1}_{i,K})\right]\|^{2}\] \[\leq\frac{\eta^{2}K\sigma^{2}_{t}}{N}+\frac{3(C-N)}{N(C-1)}(\eta^ {2}KL^{2}V^{t}_{1}+\beta^{2}\Delta^{t}+\eta^{2}K^{2}G^{2}+\eta^{2}K^{2}B^{2} \mathbb{E}\|\nabla f(w^{t})\|^{2})\] \[\quad+\frac{(N-1)}{CN(C-1)}\eta^{2}\mathbb{E}\|\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}\] \[\leq\frac{\eta^{2}K\sigma^{2}_{t}}{N}+\frac{3}{N}\left(\beta^{2} \Delta^{t}+\eta^{2}K^{2}G^{2}+\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\| ^{2}\right)+\frac{\eta^{2}}{CN}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K -1}\nabla f_{i}(w^{t}_{i,k})\|^{2}\] \[\quad+\frac{3}{N}\left(4\eta^{2}K^{2}L^{2}\beta^{2}\Delta^{t}+3K^ {3}\eta^{4}L^{2}\left(\sigma^{2}_{t}+4KG^{2}\right)+12K^{4}\eta^{4}L^{2}B^{2} \mathbb{E}\|\nabla f(w^{t})\|^{2}\right)\] \[=\frac{3\beta^{2}}{N}\left(1+4\eta^{2}K^{2}L^{2}\right)\Delta^{t} +\frac{\eta^{2}K}{N}\left(1+9K^{2}\eta^{2}L^{2}\right)\sigma^{2}_{l}+\frac{3 \eta^{2}K^{2}}{N}\left(1+12\eta^{2}K^{2}L^{2}\right)G^{2}\] \[\quad+\frac{3\eta^{2}K^{2}B^{2}}{N}\left(1+12\eta^{2}K^{2}L^{2} \right)\mathbb{E}\|\nabla f(w^{t})\|^{2}+\frac{\eta^{2}}{CN}\mathbb{E}\|\sum_{i \in\mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}.\]

_To minimize the coefficients of each term, we can select a constant order for the term \(\eta^{2}K^{2}L^{2}\). For convenience, we directly select the \(\eta^{2}K^{2}L^{2}\leq 1\) which requires the learning rate \(\eta\leq\frac{1}{KL}\). This completes the proof._

**Lemma 4**: (Bounded divergence term) _The divergence term \(\Delta^{t}\) could be upper bounded by the local update rules. According to the relaxed initialization in our method, under assumptions stated above, let the learning rate satisfy \(\eta\leq\frac{1}{KL}\) and the relaxed coefficient satisfy \(\beta\leq\frac{\sqrt{2}}{12}\), the divergence term \(\Delta^{t}\) could be bounded as the recursion of:_

\[\Delta^{t} \leq\frac{\Delta^{t}-\Delta^{t+1}}{1-72\beta^{2}}+\frac{51\eta^{2} K}{1-72\beta^{2}}\sigma^{2}_{l}+\frac{195\eta^{2}K^{2}}{1-72\beta^{2}}G^{2}+ \frac{195\eta^{2}K^{2}B^{2}}{1-72\beta^{2}}\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\quad+\frac{3\eta^{2}}{CN(1-72\beta^{2})}\mathbb{E}\|\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}.\] (26)

**Proof** : _The divergence term measures the inconsistency level in the FL framework. According to the local updates, we have the following recursive formula:_

\[\underbrace{w^{t+1}-w^{t}_{i,K}}_{\text{local bias in round $t+1$}}=\beta\underbrace{(w^{t-1}_{i,K}-w^{t}) +(w^{t+1}-w^{t})+\sum_{k=0}^{K-1}\eta g^{t}_{i,k}}_{\text{local bias in round $t$}}.\]_By taking the squared norm and expectation on both sides, we have:_

\[\mathbb{E}\|w^{t+1}-w^{t}_{i,K}\|^{2} =\mathbb{E}\|\beta(w^{t-1}_{i,K}-w^{t})+w^{t+1}-w^{t}+\sum_{k=0}^{K- 1}\eta g^{t}_{i,k}\|^{2}\] \[\leq 3\beta^{2}\mathbb{E}\|w^{t-1}_{i,K}-w^{t}\|^{2}+3\underbrace{ \mathbb{E}\|w^{t+1}-w^{t}\|^{2}}_{V^{t}_{2}}+3\mathbb{E}\|\sum_{k=0}^{K-1}\eta g ^{t}_{i,k}\|^{2}.\]

_The second term in the above inequality is \(V_{2}\) we have bounded in lemma 3. Then we bound the stochastic gradients term. We have:_

\[\mathbb{E}\|\sum_{k=0}^{K-1}\eta g^{t}_{i,k}\|^{2} =\eta^{2}\mathbb{E}\|\sum_{k=0}^{K-1}g^{t}_{i,k}\|^{2}\] \[=\eta^{2}\mathbb{E}\|\sum_{k=0}^{K-1}\left(g^{t}_{i,k}-\nabla f_{ i}(w^{t}_{i,k})\right)\|^{2}+\eta^{2}\mathbb{E}\|\sum_{k=0}^{K-1}\nabla f_{i}(w^{t }_{i,k})\|^{2}\] \[\leq\eta^{2}K\sigma_{l}^{2}+\eta^{2}K\sum_{k=0}^{K-1}\mathbb{E}\| \nabla f_{i}(w^{t}_{i,k})-\nabla f_{i}(w^{t})+\nabla f_{i}(w^{t})\|^{2}\] \[\leq\eta^{2}K\sigma_{l}^{2}+2\eta^{2}K\sum_{k=0}^{K-1}\mathbb{E}\| \nabla f_{i}(w^{t}_{i,k})-\nabla f_{i}(w^{t})\|^{2}+2\eta^{2}K\sum_{k=0}^{K-1 }\mathbb{E}\|\nabla f_{i}(w^{t})\|^{2}\] \[\leq\eta^{2}K\sigma_{l}^{2}+2\eta^{2}KL^{2}\sum_{k=0}^{K-1} \mathbb{E}\|w^{t}_{i,k}-w^{t}\|^{2}+2\eta^{2}K^{2}\mathbb{E}\|\nabla f_{i}(w^ {t})\|^{2}.\]

_Taking the average on client \(i\), we have:_

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\|\sum_{k=0}^{K-1}\eta g ^{t}_{i,k}\|^{2} \leq\eta^{2}K\sigma_{l}^{2}+\frac{2\eta^{2}KL^{2}}{C}\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\|w^{t}_{i,k}-w^{t}\|^{2}+\frac{2\eta^{2 }K^{2}}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\|\nabla f_{i}(w^{t})\|^{2}\] \[\leq\eta^{2}K\sigma_{l}^{2}+2\eta^{2}KL^{2}V_{1}^{t}+2\eta^{2}K^{ 2}G^{2}+2\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}.\]

_Recalling the condition of \(\eta\leq\frac{1}{KL}\) and combining this and the squared norm inequality, we have:_

\[\Delta^{t+1} =\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\|w^{t+1}-w^{t}_{i,K} \|^{2}\] \[\leq 3\beta^{2}\Delta^{t}+3V_{2}^{t}+\frac{3}{C}\sum_{i\in \mathcal{C}}\mathbb{E}\|\sum_{k=0}^{K-1}\eta g^{t}_{i,k}\|^{2}\] \[\leq 3\beta^{2}\left(1+\frac{15}{N}+8\eta^{2}K^{2}L^{2}\right) \Delta^{t}+6\eta^{2}K^{2}B^{2}\left(1+\frac{39}{2N}+12\eta^{2}K^{2}L^{2} \right)\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\quad+3\eta^{2}K\left(1+\frac{10}{N}+6\eta^{2}K^{2}L^{2}\right) \sigma_{l}^{2}+6\eta^{2}K^{2}\left(1+\frac{39}{2N}+12\eta^{2}K^{2}L^{2}\right) G^{2}\] \[\quad+\frac{3\eta^{2}}{CN}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum _{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}\] \[\leq 72\beta^{2}\Delta^{t}+51\eta^{2}K\sigma_{l}^{2}+195\eta^{2}K^{ 2}G^{2}+195\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\quad+\frac{3\eta^{2}}{CN}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum _{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}.\]

_Let \(72\beta^{2}<1\) where \(\beta\leq\frac{\sqrt{2}}{12}\), thus we add \((1-72\beta^{2})\Delta^{t}\) on both sides and get the recursive formulation:_

\[(1-72\beta^{2})\Delta^{t} \leq(\Delta^{t}-\Delta^{t+1})+51\eta^{2}K\sigma_{l}^{2}+195\eta^{2 }K^{2}G^{2}+195\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\quad+\frac{3\eta^{2}}{CN}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum _{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}.\]

_Then we multiply the \(\frac{1}{1-72\beta^{2}}\) on both sides, which completes the proof._

#### a.2.2 Expanding the Smoothness Inequality for the Non-convex Objective

For the non-convex and \(L\)-smooth function \(f\), we firstly expand the smoothness inequality at round \(t\) as:

\[\mathbb{E}[f(w^{t+1})-f(w^{t})]\] \[\leq\mathbb{E}\langle\nabla f(w^{t}),w^{t+1}-w^{t}\rangle+\frac{L} {2}\underbrace{\mathbb{E}\|w^{t+1}-w^{t}\|^{2}}_{V_{2}^{t}}\] \[=\mathbb{E}\langle\nabla f(w^{t}),\frac{1}{N}\sum_{i\in\mathcal{N }}w^{t}_{i,K}-w^{t}\rangle+\frac{LV_{2}^{t}}{2}\] \[=\mathbb{E}\langle\nabla f(w^{t}),\frac{1}{C}\sum_{i\in\mathcal{C }}\left[(w^{t}_{i,K}-w^{t}_{i,0})+\beta(w^{t}-w^{t-1}_{i,K})\right]\rangle+ \frac{LV_{2}^{t}}{2}\] \[=-\eta\mathbb{E}\langle\nabla f(w^{t}),\frac{1}{C}\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})-\frac{1}{C}\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t})+K\nabla f(w^{t})\rangle+\frac{ LV_{2}^{t}}{2}\] \[=-\eta K\mathbb{E}\|f(w^{t})\|^{2}+\mathbb{E}\langle\sqrt{\eta K }\nabla f(w^{t}),\sqrt{\frac{1}{K}}\frac{1}{C}\sum_{i\in\mathcal{C}}\sum_{k= 0}^{K-1}\left(\nabla f_{i}(w^{t})-\nabla f_{i}(w^{t}_{i,k})\right)\rangle+ \frac{LV_{2}^{t}}{2}\] \[\leq-\eta K\mathbb{E}\|f(w^{t})\|^{2}+\frac{\eta K}{2}\mathbb{E} \|f(w^{t})\|^{2}+\frac{\eta}{2C}\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1} \mathbb{E}\|\nabla f_{i}(w^{t})-\nabla f_{i}(w^{t}_{i,k})\|^{2}\] \[\quad-\frac{\eta}{2C^{2}K}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum _{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}+\frac{LV_{2}^{t}}{2}\] \[\leq-\frac{\eta K}{2}\mathbb{E}\|f(w^{t})\|^{2}+\frac{\eta L^{2} }{2}\underbrace{\frac{1}{C}\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\| w^{t}-w^{t}_{i,k}\|^{2}}_{V_{1}^{t}}-\frac{\eta}{2C^{2}K}\mathbb{E}\|\sum_{i\in \mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}+\frac{LV_{2}^{t}}{2}\] \[\leq-\frac{\eta K}{2}\mathbb{E}\|f(w^{t})\|^{2}+\frac{\eta L^{2} V_{1}^{t}}{2}-\frac{\eta}{2C^{2}K}\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1} \nabla f_{i}(w^{t}_{i,k})\|^{2}+\frac{LV_{2}^{t}}{2}.\]

According to Lemma 2 and lemma 3 to bound the \(V_{1}^{t}\) and \(V_{2}^{t}\), we can get the following recursive formula:

\[\mathbb{E}[f(w^{t+1})-f(w^{t})]\] \[\leq-\frac{\eta K}{2}\mathbb{E}\|f(w^{t})\|^{2}+\left(\frac{\eta^ {2}L}{2CN}-\frac{\eta}{2C^{2}K}\right)\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{ k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\|^{2}\] \[\quad+\frac{\eta L^{2}}{2}\left[4K\beta^{2}\Delta^{t}+3K^{2}\eta^ {2}\left(\sigma_{l}^{2}+4KG^{2}\right)+12K^{3}\eta^{2}B^{2}\mathbb{E}\|\nabla f (w^{t})\|^{2}\right]\] \[\quad+\frac{3\beta^{2}L}{2N}\left(1+4\eta^{2}K^{2}L^{2}\right) \Delta^{t}+\frac{\eta^{2}KL}{2N}\left(1+9K^{2}\eta^{2}L^{2}\right)\sigma_{l}^{ 2}+\frac{3\eta^{2}K^{2}L}{2N}\left(1+12\eta^{2}K^{2}L^{2}\right)G^{2}\] \[\quad+\frac{3\eta^{2}K^{2}B^{2}L}{2N}\left(1+12\eta^{2}K^{2}L^{2} \right)\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\leq\left(\frac{\eta^{2}L}{2CN}-\frac{\eta}{2C^{2}K}\right) \mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w^{t}_{i,k})\| ^{2}+\frac{3\beta^{2}L}{2N}\left[\frac{4N}{3}\eta KL+\left(1+4\eta^{2}K^{2}L^{ 2}\right)\right]\Delta^{t}\] \[\quad+\frac{\eta^{2}KL}{2N}\left[3N\eta KL+\left(1+9\eta^{2}K^{2 }L^{2}\right)\right]\sigma_{l}^{2}+\frac{3\eta^{2}K^{2}L}{2N}\left[4N\eta KL+ \left(1+12\eta^{2}K^{2}L^{2}\right)\right]G^{2}\] \[\quad-\frac{\eta K}{2}\left[1-\frac{3\eta KLB^{2}}{N}\left(1+12 \eta^{2}K^{2}L^{2}\right)-12\eta^{2}K^{2}L^{2}B^{2}\right]\mathbb{E}\|f(w^{t}) \|^{2}.\]

Here we make a comprehensive discussion on the selection of \(\eta\) to simplify the above formula. In fact, in lemma 2, there is a constraint on the learning rate as \(\eta\leq\frac{\sqrt{2}}{4(K-1)L}\) for \(K\geq 2\). In lemma 3 and lemma 4, there is a constraint on the learning rate as \(\eta\leq\frac{1}{KL}\). To further minimize the coefficient, we select the \(N\eta KL\) to be constant order. For convenience, we directly select the \(\eta\leq\frac{1}{NKL}\). Thus, we have:

\[\mathbb{E}[f(w^{t+1})-f(w^{t})]\] \[\leq\frac{3\beta^{2}L}{2N}\left(\frac{4}{3}N\eta KL+5\right) \Delta^{t}+\frac{\eta^{2}KL}{2N}\left(3N\eta KL+10\right)\sigma_{l}^{2}+\frac{3 \eta^{2}K^{2}L}{2N}\left(4N\eta KL+13\right)G^{2}\]\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|f(w^{t})\|^{2}\] \[\leq\frac{2\mathbb{E}[f(w^{0})-f(w^{T})]}{\lambda\eta KT}+\frac{20 \beta^{2}L}{(1-72\beta^{2})\lambda\eta KNT}(\Delta^{0}-\Delta^{T})+\frac{\kappa _{2}\eta L}{\lambda N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta KL}{\lambda N}G^{2}\] \[\leq\frac{2\left(f(w^{0})-f(w^{*})\right)}{\lambda\eta KT}+\frac{ \kappa_{2}\eta L}{\lambda N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta KL}{\lambda N} G^{2}.\]

_We select the learning rate \(\eta=\mathcal{O}(\sqrt{\frac{N}{KT}})\) and let \(D=f(w^{0})-f(w^{\star})\) as the initialization bias, then we have:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\|f(w^{t})\|^{2}=\mathcal{O}\left(\frac{D +L\left(\sigma_{l}^{2}+3KG^{2}\right)}{\sqrt{NKT}}\right).\]

_This completes the proof._

#### a.2.4 Proof of Theorem 2

**Theorem 8**: _Under Assumption 6\(\sim\)8 and 10, let participation ratio is \(N/C\) where \(1<N<C\), let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL},\frac{1}{\lambda_{0}K}\right\}\) where \(K\geq 2\), let the relaxation coefficient \(\beta\leq\frac{\sqrt{2}}{12}\), and after training \(T\) rounds, the global model \(w^{t}\) generated by FedInit satisfies:_

\[\mathbb{E}[f(w^{T})-f(w^{\star})]\leq e^{-\lambda\mu\eta KT} \mathbb{E}[f(w^{0})-f(w^{\star})]+\frac{3\kappa_{1}\eta KL}{2N\lambda\mu}G^{2} +\frac{\kappa_{2}\eta L}{2N\lambda\mu}\sigma_{l}^{2}.\] (29)

_Further, by selecting the proper learning rate \(\eta=\mathcal{O}\left(\frac{\log(\lambda\mu NKT)}{\lambda\mu KT}\right)\) and let \(D=f(w^{0})-f(w^{\star})\) as the initialization bias, the global model \(w^{t}\) satisfies:_

\[\mathbb{E}[f(w^{T})-f(w^{\star})]=\mathcal{O}\left(\frac{D+L( \sigma_{l}^{2}+KG^{2})}{NKT}\right).\] (30)

**Proof**: _According to the expansion of the smoothness inequality, we have:_

\[\frac{\lambda\eta K}{2}\mathbb{E}\|f(w^{t})\|^{2}\] \[\leq\mathbb{E}[f(w^{t})-f(w^{t+1})]+\frac{10\beta^{2}L}{(1-72 \beta^{2})N}(\Delta^{t}-\Delta^{t+1})+\frac{3\kappa_{1}\eta^{2}K^{2}L}{2N}G^{ 2}+\frac{\kappa_{2}\eta^{2}KL}{2N}\sigma_{l}^{2}.\]

_According to Assumption 10, we have \(2\mu(f(w)-f(w^{\star}))\leq\|\nabla f(w)\|^{2}\), we have:_

\[\lambda\mu\eta K\mathbb{E}[f(w^{t})-f(w^{\star})]\leq\frac{\lambda \eta K}{2}\mathbb{E}\|f(w^{t})\|^{2}\] \[\leq\mathbb{E}[f(w^{t})-f(w^{t+1})]+\frac{10\beta^{2}L}{(1-72 \beta^{2})N}(\Delta^{t}-\Delta^{t+1})+\frac{3\kappa_{1}\eta^{2}K^{2}L}{2N}G^{ 2}+\frac{\kappa_{2}\eta^{2}KL}{2N}\sigma_{l}^{2}.\]

_Combining the terms aligned with \(w^{t}\) and \(w^{t+1}\), we have:_

\[\mathbb{E}[f(w^{t+1})-f(w^{\star})]\] \[\leq(1-\lambda\mu\eta K)\mathbb{E}[f(w^{t})-f(w^{\star})]+\frac{ 10\beta^{2}L}{(1-72\beta^{2})N}(\Delta^{t}-\Delta^{t+1})+\frac{3\kappa_{1} \eta^{2}K^{2}L}{2N}G^{2}+\frac{\kappa_{2}\eta^{2}KL}{2N}\sigma_{l}^{2}.\]

_Taking the recursion from \(t=0\) to \(T-1\) and let learning rate \(\eta\leq\frac{1}{\lambda\mu K}\), we have:_

\[\mathbb{E}[f(w^{T})-f(w^{\star})]\] \[\leq(1-\lambda\mu\eta K)^{T}\mathbb{E}[f(w^{0})-f(w^{\star})]+ \sum_{t=0}^{T-1}(1-\lambda\mu\eta K)^{T-1-t}\frac{10\beta^{2}L}{(1-72\beta^{2 })N}(\Delta^{t}-\Delta^{t+1})\] \[\quad+\left(\frac{3\kappa_{1}\eta^{2}K^{2}L}{2N}G^{2}+\frac{ \kappa_{2}\eta^{2}KL}{2N}\sigma_{l}^{2}\right)\sum_{t=0}^{T-1}(1-\lambda\mu \eta K)^{T-1-t}\] \[\leq(1-\lambda\mu\eta K)^{T}\mathbb{E}[f(w^{0})-f(w^{\star})]+ \frac{10\beta^{2}L}{(1-72\beta^{2})N}(\Delta^{0}-\Delta^{T})\] \[\quad+\left(\frac{3\kappa_{1}\eta^{2}K^{2}L}{2N}G^{2}+\frac{ \kappa_{2}\eta^{2}KL}{2N}\sigma_{l}^{2}\right)\frac{1-(1-\lambda\mu\eta K)^{T} }{\lambda\mu\eta K}\] \[\leq(1-\lambda\mu\eta K)^{T}\mathbb{E}[f(w^{0})-f(w^{\star})]+ \frac{3\kappa_{1}\eta KL}{2N\lambda\mu}G^{2}+\frac{\kappa_{2}\eta L}{2N \lambda\mu}\sigma_{l}^{2}\] \[\leq e^{-\lambda\mu\eta KT}\mathbb{E}[f(w^{0})-f(w^{\star})]+ \frac{3\kappa_{1}\eta KL}{2N\lambda\mu}G^{2}+\frac{\kappa_{2}\eta L}{2N\lambda \mu}\sigma_{l}^{2}.\]

_We select the learning rate \(\eta=\mathcal{O}\left(\frac{\log(\lambda\mu NKT)}{\lambda\mu KT}\right)\) and let \(D=f(w^{0})-f(w^{\star})\) as the initialization bias, then we have:_

\[\mathbb{E}[f(w^{T})-f(w^{\star})]=\mathcal{O}\left(\frac{D+L( \sigma_{l}^{2}+KG^{2})}{NKT}\right).\]

_This completes the proof._

#### a.2.5 Proof of Theorem 4

**Theorem 9**: _Under Assumption 6\(\sim\)8, we can bound the divergence term as follows. Let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL},\frac{\sqrt{N}}{\sqrt{CKL}}\right\}\) where \(K\geq 2\), and after training \(T\) rounds, let \(0<\beta<\frac{\sqrt{6}}{24}\), the divergenceterm \(\Delta^{t}\) generated by FedInit satisfies:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\Delta^{t}=\mathcal{O}\left(\frac{N(\sigma_{1}^{2}+KG^ {2})}{T}+\frac{\sqrt{NK}B^{2}\left[D+L(\sigma_{l}^{2}+KG^{2})\right]}{T^{\frac{ 3}{2}}}\right).\] (31)

**Proof** : _According to Lemma 4, we have:_

\[\Delta^{t+1} \leq 72\beta^{2}\Delta^{t}+51\eta^{2}K\sigma_{l}^{2}+195\eta^{2}K^ {2}G^{2}+195\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[+\frac{3\eta^{2}}{CN}\mathbb{E}\|\sum_{i\in\mathcal{C}}^{K-1} \nabla f_{i}(w_{i,k}^{t})\|^{2}.\]

_Here we further bound the gradient term, we have:_

\[\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\nabla f_{i}(w _{i,k}^{t})\|^{2} =\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\left(\nabla f _{i}(w_{i,k}^{t})-\nabla f_{i}(w^{t})+\nabla f_{i}(w^{t})\right)\|^{2}\] \[=\mathbb{E}\|\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\left(\nabla f _{i}(w_{i,k}^{t})-\nabla f_{i}(w^{t})+\nabla f(w^{t})\right)\|^{2}\] \[\leq CK\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\|\nabla f _{i}(w_{i,k}^{t})-\nabla f_{i}(w^{t})+\nabla f(w^{t})\|^{2}\] \[\leq 2CK\sum_{i\in\mathcal{C}}\sum_{k=0}^{K-1}\mathbb{E}\|\nabla f _{i}(w_{i,k}^{t})-\nabla f_{i}(w^{t})\|^{2}+2C^{2}K^{2}\mathbb{E}\|\nabla f(w ^{t})\|^{2}\] \[\leq 2C^{2}KL^{2}V_{1}^{t}+2C^{2}K^{2}\mathbb{E}\|\nabla f(w^{t}) \|^{2}.\]

_Combining this into the recursive formulation, and let the learning rate satisfy \(\eta\leq\frac{\sqrt{N}}{\sqrt{CKL}}\), we have:_

\[\Delta^{t+1} \leq\beta^{2}\left(72+\frac{24C\eta^{2}K^{2}L^{2}}{N}\right) \Delta^{t}+\eta^{2}K^{2}B^{2}\left(195+\frac{72C\eta^{2}K^{2}L^{2}}{N}\right) \mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\quad+\eta^{2}K\left(51+\frac{18C\eta^{2}K^{2}L^{2}}{N}\right) \sigma_{l}^{2}+\eta^{2}K^{2}\left(195+\frac{72C\eta^{2}K^{2}L^{2}}{N}\right)G^ {2}\] \[\leq 96\beta^{2}\Delta^{t}+267\eta^{2}K^{2}B^{2}\mathbb{E}\| \nabla f(w^{t})\|^{2}+69\eta^{2}K\sigma_{l}^{2}+267\eta^{2}K^{2}G^{2}.\]

_Let \(96\beta^{2}<1\) as the decayed coefficient where \(\beta<\frac{\sqrt{6}}{24}\), similar as Lemma 4, we have:_

\[\Delta^{t}\leq\frac{\Delta^{t}-\Delta^{t+1}}{1-96\beta^{2}}+\frac{267\eta^{2} K^{2}B^{2}}{1-96\beta^{2}}\mathbb{E}\|\nabla f(w^{t})\|^{2}+\frac{69\eta^{2}K}{1 -96\beta^{2}}\sigma_{l}^{2}+\frac{267\eta^{2}K^{2}}{1-96\beta^{2}}G^{2}.\]

_by taking the accumulation from \(t=0\) to \(T-1\),_

\[\frac{1}{T}\sum_{t=0}^{T-1}\Delta^{t} \leq\frac{\Delta^{0}-\Delta^{T}}{1-96\beta^{2}}+\frac{69\eta^{2}K} {1-96\beta^{2}}\sigma_{l}^{2}+\frac{267\eta^{2}K^{2}}{1-96\beta^{2}}G^{2}+ \frac{267\eta^{2}K^{2}B^{2}}{1-96\beta^{2}}\frac{1}{T}\sum_{t=0}^{T-1}\|\nabla f (w^{t})\|^{2}\] \[\leq\frac{267\eta^{2}K^{2}B^{2}}{1-96\beta^{2}}\left(\frac{2\left( f(w^{0})-f(w^{*})\right)}{\lambda\eta KT}+\frac{\kappa_{2}\eta L}{\lambda N} \sigma_{l}^{2}+\frac{3\kappa_{1}\eta KL}{\lambda N}G^{2}\right)\] \[\quad+\frac{69\eta^{2}K}{1-96\beta^{2}}\sigma_{l}^{2}+\frac{267 \eta^{2}K^{2}}{1-96\beta^{2}}G^{2}\] \[\leq\frac{534\eta KB^{2}\left(f(w^{0})-f(w^{*})\right)}{(1-96 \beta^{2})\lambda T}+\frac{267\eta^{3}K^{2}B^{2}\kappa_{2}L}{(1-96\beta^{2}) \lambda N}\sigma_{l}^{2}+\frac{801\eta^{3}K^{3}B^{2}\kappa_{1}L}{(1-96\beta^{2 })\lambda N}G^{2}\] \[\quad+\frac{69\eta^{2}K}{1-96\beta^{2}}\sigma_{l}^{2}+\frac{267 \eta^{2}K^{2}}{1-96\beta^{2}}G^{2}.\]

_The same, the learning rate is selected as \(\eta=\mathcal{O}(\sqrt{\frac{N}{KT}})\) and let \(D=f(w^{0})-f(w^{\star})\) as the initialization bias and let \(96\beta^{2}<1\), thus we have:_

\[\frac{1}{T}\sum_{t=0}^{T-1}\Delta^{t}=\mathcal{O}\left(\frac{N(\sigma_{l}^{2}+KG ^{2})}{T}+\frac{\sqrt{NK}B^{2}\left[D+L(\sigma_{l}^{2}+KG^{2})\right]}{T^{\frac{ 3}{2}}}\right).\]

_This completes this proof._

#### a.2.6 Proof of Theorem 5

**Theorem 10**: _Under Assumption 6&8 and 10, we can bound the divergence term as follows. Let the learning rate satisfy \(\eta\leq\min\left\{\frac{N}{2CKL},\frac{1}{NKL},\frac{1}{\lambda\mu K}\right\}\) where \(K\geq 2\), and after training \(T\) rounds, let \(0<\beta<\frac{\sqrt{6}}{24}\), the divergence term \(\Delta^{T}\) generated by FedInit satisfies:_

\[\Delta^{T}=\mathcal{O}\left(\frac{D+G^{2}}{T^{2}}+\frac{N\sigma_{l}^{2}+KG^{2} }{NKT^{2}}\right)+\mathcal{O}\left(\frac{1}{NKT^{3}}\right).\] (32)

**Proof**: _According to the Theorem 8, we have:_

\[\Delta^{t+1}\leq 96\beta^{2}\Delta^{t}+267\eta^{2}K^{2}B^{2}\mathbb{E}\| \nabla f(w^{t})\|^{2}+69\eta^{2}K\sigma_{l}^{2}+267\eta^{2}K^{2}G^{2}.\]

_Taking the recursive formulation from \(t=0\) to \(T-1\), we have:_

\[\Delta^{T} \leq(96\beta^{2})^{T}\Delta^{0}+\sum_{t=0}^{T-1}(96\beta^{2})^{t }\left(267\eta^{2}K^{2}B^{2}\mathbb{E}\|\nabla f(w^{t})\|^{2}+69\eta^{2}K \sigma_{l}^{2}+267\eta^{2}K^{2}G^{2}\right)\] \[\leq\frac{69\eta^{2}K\sigma_{l}^{2}}{1-96\beta^{2}}+\frac{267 \eta^{2}K^{2}G^{2}}{1-96\beta^{2}}+267\eta^{2}K^{2}B^{2}\sum_{t=0}^{T-1}(96 \beta^{2})^{T-1-t}\mathbb{E}\|\nabla f(w^{t})\|^{2}\] \[\leq\frac{69\eta^{2}K\sigma_{l}^{2}}{1-96\beta^{2}}+\frac{267 \eta^{2}K^{2}G^{2}}{1-96\beta^{2}}+\frac{267\eta^{2}K^{2}B^{2}}{1-96\beta^{2}} \left(\frac{\kappa_{2}\eta L}{\lambda N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta KL }{\lambda N}G^{2}\right)\] \[\quad+\frac{534\eta KB^{2}}{\lambda}\sum_{t=0}^{T-1}(96\beta^{2}) ^{T-1-t}\mathbb{E}\left[f(w^{t})-f(w^{t+1})\right]+\frac{10\beta^{2}L}{(1-72 \beta^{2})N}(\Delta^{0}-\Delta^{T})\] \[\leq\frac{69\eta^{2}K\sigma_{l}^{2}}{1-96\beta^{2}}+\frac{267 \eta^{2}K^{2}G^{2}}{1-96\beta^{2}}+\frac{267B^{2}L}{(1-96\beta^{2})\lambda} \left(\frac{\kappa_{2}\eta^{3}K^{2}}{N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta^{3 }K^{3}}{N}G^{2}\right)\] \[\quad+\frac{534\eta KB^{2}}{\lambda}\sum_{t=0}^{T-1}(96\beta^{2}) ^{T-1-t}\mathbb{E}\left[f(w^{t})-f(w^{*})\right].\]

_According to the Theorem 11, we have:_

\[\mathbb{E}[f(w^{t})-f(w^{*})]\leq e^{-\lambda\mu\eta Kt}\mathbb{E}[f(w^{0})-f (w^{*})]+\frac{3\kappa_{1}\eta KL}{2N\lambda\mu}G^{2}+\frac{\kappa_{2}\eta L} {2N\lambda\mu}\sigma_{l}^{2}.\]

_Let \(96\beta^{2}\leq e^{-\lambda\mu\eta K}\), thus we have:_

\[\frac{534\eta KB^{2}}{\lambda}\sum_{t=0}^{T-1}(96\beta^{2})^{T-1- t}\mathbb{E}\left[f(w^{t})-f(w^{*})\right]\] \[\leq\frac{267B^{2}L}{(1-96\beta^{2})\lambda^{2}\mu}\left(\frac{ \kappa_{2}\eta^{2}K}{N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta^{2}K^{2}}{N}G^{2 }\right)+\frac{534\eta KB^{2}}{\lambda}\mathbb{E}[f(w^{0})-f(w^{*})]\sum_{t=0} ^{T-1}(96\beta^{2})^{T-1-t}e^{-\lambda\mu\eta Kt}\] \[\leq\frac{267B^{2}L}{(1-96\beta^{2})\lambda^{2}\mu}\left(\frac{ \kappa_{2}\eta^{2}K}{N}\sigma_{l}^{2}+\frac{3\kappa_{1}\eta^{2}K^{2}}{N}G^{2 }\right)\] \[\quad+\frac{534\eta KB^{2}}{\lambda}\mathbb{E}[f(w^{0})-f(w^{*})] e^{-\lambda\mu\eta KT}\sum_{t=0}^{T-1}e^{-2\lambda\mu\eta Kt}.\]

_Thus selecting the same learning rate \(\eta=\mathcal{O}\left(\frac{\log(\lambda\mu NKT)}{\lambda\mu KT}\right)\) and let \(D=f(w^{0})-f(w^{*})\) as the initialization bias, we have:_

\[\Delta^{T}=\mathcal{O}\left(\frac{D+G^{2}}{T^{2}}+\frac{N\sigma_{l}^{2}+KG^{2} }{NKT^{2}}+\frac{1}{NKT^{3}}\right).\]

_This completes the proof._

### Proofs for the Generalization Error

In this part, we prove the generalization error for our proposed method. We assume the objective function \(f\) is \(L\)-smooth and \(L_{G}\)-Lipschitz as defined in [11; 51]. We follow the uniform stability to upper bound the generalization error in the FL.

We suppose there are \(C\) clients participating in the training process as a set \(\mathcal{C}=\{i\}_{i=1}^{C}\). Each client has a local dataset \(\mathcal{S}_{i}=\{z_{j}\}_{j=1}^{S}\) with total \(S\) data sampled from a specific unknown distribution \(\mathcal{D}_{i}\). Now we define a re-sampled dataset \(\mathcal{\tilde{S}}_{i}\) which only differs from the dataset \(\mathcal{S}_{i}\) on the \(j^{*}\)-th data. We replace the \(\mathcal{S}_{i^{*}}\) with \(\widetilde{\mathcal{S}}_{i^{*}}\) and keep other \(C-1\) local dataset, which composes a new set \(\widetilde{\mathcal{C}}\). From the perspective of total data, \(\mathcal{C}\) only differs from the \(\widetilde{\mathcal{C}}\) at \(j^{*}\)-th data on the \(i^{*}\)-th client. Then, based on these two sets, our method could generate two output models, \(w^{t}\) and \(\widetilde{w}^{t}\) respectively, after \(t\) training rounds. We first introduce some notations used in the proof of the generalization error.

Then we introduce some important lemmas in our proofs.

#### a.3.1 Important Lemmas

**Lemma 5** (Lemma 3.11 in [11]): _We follow the definition in [11; 51] to upper bound the uniform stability term after each communication round in FL paradigm. Different from their vanilla calculations, FL considers the finite-sum function on heterogeneous clients. Let non-negative objective \(f\) is \(L\)-smooth and \(L_{G}\)-Lipschitz. After training \(T\) rounds on \(\mathcal{C}\) and \(\widetilde{\mathcal{C}}\), our method generates two models \(w^{T+1}\) and \(\widetilde{w}^{T+1}\) respectively. For each data \(z\) and every \(t_{0}\in\{1,2,3,\cdots,S\}\), we have:_

\[\mathbb{E}\|f(w^{T+1};z)-f(\widetilde{w}^{T+1};z)\|\leq\frac{Ut_{0}}{S}+\frac{ L_{G}}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\left[\left\|w^{T}_{i,K}-\widetilde{w}^{T}_{ i,K}\right\|\mid\xi\right].\] (33)

**Proof**: _Let \(\xi=1\) denote the event \(\|w^{t_{0}}-\widetilde{w}^{t_{0}}\|=0\) and \(U=\sup_{w,z}f(w;z)\), we have:_

\[\mathbb{E}\|f(w^{T+1};z)-f(\widetilde{w}^{T+1};z)\|\] \[=P(\{\xi\})\;\mathbb{E}\left[\|f(w^{T+1};z)-f(\widetilde{w}^{T+1 };z)\|\mid\xi\right]+P(\{\xi^{\xi}\})\,\mathbb{E}\left[\left\|f(w^{T+1};z)-f( \widetilde{w}^{T+1};z)\right\|\mid\xi^{\xi}\right]\] \[\leq L_{G}\mathbb{E}\left[\|w^{T+1}-\widetilde{w}^{T+1}\|\mid\xi \right]+UP(\{\xi^{\xi}\})\] \[=L_{G}\mathbb{E}\left[\|\frac{1}{C}\sum_{i\in\mathcal{C}}(w^{T}_ {i,K}-\widetilde{w}^{T}_{i,K})\|\mid\xi\right]+UP(\{\xi^{\xi}\})\] \[\leq\frac{L_{G}}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\left[\|w^{T}_ {i,K}-\widetilde{w}^{T}_{i,K}\|\mid\xi\right]+UP(\{\xi^{\xi}\}).\]

_Before the \(j^{\star}\)-th data on \(i^{\star}\)-th client is sampled, the iterative states are identical on both \(\mathcal{C}\) and \(\widetilde{\mathcal{C}}\). Let \(\widetilde{j}\) is the index of the first different sampling, if \(\widetilde{j}>t_{0}\), then \(\xi=1\) hold for \(t_{0}\). Therefore, we have:_

\[P(\{\xi^{\xi}\})=P(\{\xi=0\})\leq P(\widetilde{j}\leq t_{0})\leq\frac{t_{0}}{S},\]

_where \(\widetilde{j}\) is uniformly selected. This completes the proof._

**Lemma 6** (Lemma 1.1 in [51]): _Different from their calculations, we prove the similar inequalities on \(f\) in the stochastic optimization. Let non-negative objective \(f\) is \(L\)-smooth w.r.t \(w\). The local updates satisfy \(w^{t}_{i,k+1}=w^{t}_{i,k}-\eta g^{t}_{i,k}\) on \(\mathcal{C}\) and \(\widetilde{w}^{t}_{i,k+1}=\widetilde{w}^{t}_{i,k}-\eta\widetilde{g}^{t}_{i,k}\) on \(\widetilde{\mathcal{C}}\). If at \(k\)-th iteration on each round, we sample the **same** data in \(\mathcal{C}\) and \(\widetilde{\mathcal{C}}\), then we have:_

\[\mathbb{E}\|w^{t}_{i,k+1}-\widetilde{w}^{t}_{i,k+1}\|\leq(1+\eta L)\mathbb{E} \|w^{t}_{i,k}-\widetilde{w}^{t}_{i,k}\|+2\eta\sigma_{l}.\] (34)

\begin{table}
\begin{tabular}{c c c} \hline \hline Notation & Formulation & Description \\ \hline \(w\) & - & parameters trained with set \(\mathcal{C}\) \\ \(\widetilde{w}\) & - & parameters trained with set \(\widetilde{\mathcal{C}}\) \\ \(\Delta^{t}\) & \(\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\|w^{t-1}_{i,K}-w^{t}\|^{2}\) & inconsistency/divergence term in round \(t\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Some abbreviations of the used terms in the proof of bounded training error.

[MISSING_PAGE_FAIL:25]

within each round \(t\) and \(\eta=\frac{c}{t}\), then we have:

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\left[\left\|w_{i,K}^{T}- \widetilde{w}_{i,K}^{T}\right\|\mid\xi\right]\leq \left(\frac{2L_{G}}{S}+2\sigma_{l}\right)\sum_{t=t^{\star}K+k^{ \star}}^{TK}\eta_{t}\exp\left(\left(1-\frac{1}{S}\right)L\sum_{\tau=t}^{TK} \eta_{\tau}\right)\] \[+(1+\beta)^{\frac{1}{g_{\mathcal{L}}}}\sum_{t=t^{\star}+1}^{T} \exp\left(\left(1-\frac{1}{S}\right)L\sum_{\tau=t}^{TK}\eta_{\tau}\right)\sqrt {\Delta^{t}}.\]

We adopt the same learning rate \(\eta=\frac{c}{t}\) where \(c=\frac{\mu_{0}}{K}\) is a positive constant, then

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\left[\left\|w_{i,K}^{ T}-\widetilde{w}_{i,K}^{T}\right\|\mid\xi\right]\] \[\leq 2c\left(\frac{L_{G}}{S}+\sigma_{l}\right)\sum_{t=t^{\star}K+k^ {\star}}^{TK}\frac{1}{t}\exp\left(\left(1-\frac{1}{S}\right)CL\sum_{\tau=t}^{ TK}\frac{1}{\tau}\right)\] \[+(1+\beta)^{\frac{1}{g_{\mathcal{L}}}}\sum_{t=t^{\star}+1}^{T} \exp\left(\left(1-\frac{1}{S}\right)CL\sum_{\tau=t}^{TK}\frac{1}{\tau}\right) \sqrt{\Delta^{t}}\] \[\leq 2c\left(\frac{L_{G}}{S}+\sigma_{l}\right)\sum_{t=t^{\star}K+k^ {\star}}^{TK}\frac{1}{t}\exp\left(\left(1-\frac{1}{S}\right)CL\log\left(\frac{ TK}{t}\right)\right)\] \[+(1+\beta)^{\frac{1}{g_{\mathcal{L}}}}\sum_{t=t^{\star}+1}^{T} \exp\left(\left(1-\frac{1}{S}\right)CL\log\left(\frac{TK}{t}\right)\right) \sqrt{\Delta^{t}}\] \[\leq 2c\left(\frac{L_{G}}{S}+\sigma_{l}\right)(TK)^{\left(1- \frac{1}{S}\right)cL}\sum_{t=t^{\star}K+k^{\star}}^{TK}\left(\frac{1}{t} \right)^{1+\left(1-\frac{1}{S}\right)cL}+(1+\beta)^{\frac{1}{g_{\mathcal{L}}} }\sum_{t=t^{\star}+1}^{T}\left(\frac{TK}{t}\right)^{\left(1-\frac{1}{S} \right)cL}\sqrt{\Delta^{t}}\] \[\leq\frac{2\left(L_{G}+S\sigma_{l}\right)}{(S-1)L}\left(\frac{TK} {t^{\star}K+k^{\star}}\right)^{cL}+(1+\beta)^{\frac{1}{g_{\mathcal{L}}}}\sum_ {t=t^{\star}+1}^{T}\left(\frac{TK}{t^{\star}}\right)^{cL}\sqrt{\Delta^{t}}.\]

#### a.3.3 Proof of Theorem 3

**Theorem 11**: _Under the Assumptions 6, 7, 9, and 10, let all conditions above satisfied, we can bound the uniform stability of our proposed FedInit as:_

\[\mathbb{E}\|f(w^{T+1};z)-f(\widetilde{w}^{T+1};z)\|\] (36) \[\leq \frac{U^{\frac{cL}{1+cL}}}{S-1}\left[\frac{2(L_{G}^{2}+SL_{G} \sigma_{l})TK^{cL}}{L}\right]^{\frac{1}{1+cL}}+(1+\beta)^{\frac{1}{g_{ \mathcal{L}}}}\left[\frac{ULTK}{2(L_{G}^{2}+SL_{G}\sigma_{l})}\right]^{\frac{cL }{1+cL}}\sum_{t=1}^{T}\sqrt{\Delta^{t}}.\]

**Proof** _According to Lemma 5, we have:_

\[\mathbb{E}\|f(w^{T+1};z)-f(\widetilde{w}^{T+1};z)\|\leq\frac{ Ut_{0}}{S}+\frac{L_{G}}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\left[\left\|w_{i,K}^{T}- \widetilde{w}_{i,K}^{T}\right\|\mid\xi\right].\]

_The second term is bounded by uniform stability term as:_

\[\frac{1}{C}\sum_{i\in\mathcal{C}}\mathbb{E}\left[\left\|w_{i,K}^{ T}-\widetilde{w}_{i,K}^{T}\right\|\mid\xi\right]\] \[\leq\frac{2\left(L_{G}+S\sigma_{l}\right)}{(S-1)L}\left(\frac{TK} {t^{\star}K+k^{\star}}\right)^{cL}+(1+\beta)^{\frac{1}{g_{\mathcal{L}}}}\sum_ {t=1}^{T}\left(\frac{TK}{t^{\star}}\right)^{cL}\sqrt{\Delta^{t}}.\]

_Let the \(t_{0}=t^{\star}K+K^{\star}=\left[\frac{2(L_{G}^{2}+SL_{G}\sigma_{l})}{UL}(TK)^{ cL}\right]^{\frac{1}{1+cL}}\), then \(t^{\star}>\left[\frac{2(L_{G}^{2}+SL_{G}\sigma_{l})}{UL}\right]^{\frac{1}{1+cL}} \frac{T^{\frac{cL}{1+cL}}}{K^{\frac{1}{1+cL}}}\) we have:_

\[\mathbb{E}\|f(w^{T+1};z)-f(\widetilde{w}^{T+1};z)\|\] \[\leq \frac{U^{\frac{cL}{1+cL}}}{S-1}\left[\frac{2(L_{G}^{2}+SL_{G} \sigma_{l})}{L}\right]^{\frac{1}{1+cL}}(TK)^{\frac{cL}{1+cL}}\] \[+(1+\beta)^{\frac{1}{g_{\mathcal{L}}}}\left[\frac{UL}{2(L_{G}^{2} +SL_{G}\sigma_{l})}\right]^{\frac{cL}{1+cL}}(TK)^{\frac{cL}{1+cL}}\sum_{t=1}^{ T}\frac{\sqrt{\Delta^{t}}}{T}\]\[=\frac{U^{\frac{\varepsilon L}{1+\varepsilon L}}}{S-1}\left[\frac{2(L_{G}^{2}+SL _{G}\sigma_{I})(TK)^{\varepsilon L}}{L}\right]^{\frac{1}{1+\varepsilon L}}+(1+ \beta)^{\frac{1}{\beta L}}\left[\frac{ULTK}{2(L_{G}^{2}+SL_{G}\sigma_{I})} \right]^{\frac{\varepsilon L}{1+\varepsilon L}}\sum_{t=1}^{T}\frac{\sqrt{ \Delta^{t}}}{T}.\]

_This completes this proof._

## Appendix B Experiments

In this section, we mainly provide the detailed experimental setups in our paper, including the introduction of the benchmarks, dataset, hyperparameters selections, and adding some more experiments.

### Setups

Dataset.We follow the previous works and select the CIFAR-10/100 [20] dataset in our experiments. In the CIFAR-10 dataset, there is a total of 50,000 training images and 10,000 test images which contain 10 categories. Each data sample is a color image with a size of 32\(\times\)32. In the CIFAR-100 dataset, there is also a total of 50,000 training images and 10,000 test images. It contains 100 categories of the same size as CIFAR-10. For their limited resolutions, we only use general data augmentations. On each local heterogeneous dataset, we use general normalization on the images with specific mean and variance. For the training process, we randomly crop a 32\(\times\)32 patch from the vanilla images with a zero padding of \(4\). For the test process, we use the raw images.

Heterogeneity.We follow Hsu et al. [14] to introduce the label imbalance as the heterogeneous dataset. According to the Dirichlet distribution, we first generate a specific vector with respect to a constant \(D_{r}\) to control its variance level. Usually, heterogeneity becomes stronger when \(D_{r}\) decreases. Then according to the vector, we sample the images from the training dataset. Here we enable the sampling with replacement to generate the local dataset, which means the local clients may have the same data sample if they are assigned to the same category. This is more related to the real scenario. At the same time, it also will lose some data samples, we assume this case is due to the offline devices. This is a common case because the FL has an unreliable network connection across the devices.

Benchmarks.In this paper, we use _FedAvg_[26], _FedAdam_[30], _FedSAM_[29], _SCAFFOLD_[19], _FedDyn_[1], and _FedCM_[46] as the benchmarks. _FedAvg_ propose the general FL paradigm based on the local SGD method. It allows partial participation training via uniformly selecting a subset of local clients. A series of developments followed it to improve its performance. _FedAdam_ studies the efficient adaptive optimizer on the global server update, which extends the scope of the FL paradigm. _SCAFFOLD_ indicates that FL suffers from the client-drift problem which is due to the inconsistency of local optimum. Beyond this, it uses the variance reduction technique to further reduce the divergence across the local clients. To further alleviate, _FedDyn_ studies the primal-dual method via adopting the ADMM to solve the problem. The consistency condition works as a constraint during the optimization. It proves that when the global model converges, the local objectives will be aligned with the global one. _FedCM_ proposes an efficient momentum-based method, dubbed client-level momentum. It communicates the global update as a correction to correct each local update to force the local client updates in a similar direction. It maintains very high consistency via a biased correction. Therefore, it relies on an accurate global direction estimation. _FedSAM_ considers the generalization performance. Generally, we adopt empirical risk minimization (ERM) to perform the optimization process. While the sharpness-aware-minmization (SAM) studies that it could search for a flat loss landscape. Flatness guarantees a higher generalization performance. Though our focus is not the generalization, we theoretically prove that even in the _FedAvg_ method divergence term affects the generalization error bound more than the optimization error bound. From this perspective, generalization-efficiency methods may also be connected with consistency guarantees. These are all the SOTA benchmarks in the FL community that concern more on enhancing consistency.

Hyperparameters selection.Here we detail our hyperparameter selection in our experiments. For each splitting, we fix the total communication rounds \(T\), local interval \(K\), and mini-batchsize for all the benchmarks and our proposed _FedInit_. The other selections are stated as follows.

\(\star\) means different selections according to the specific setups.

We fix the most hyperparameters of testing the whole benchmarks for a fair comparison. The other algorithm-specific hyperparameters are subjected to specific circumstances. The ResNet-18-GN and VGG-11 adopt the same set of selections. Then we show algorithm-specific hyperparameters:

**Special hyperparameter selections**. In the _FedAdam_ method, we test that it is very sensitive to the global learning rate. Though we report the best selection is \(0.1\), it still requires some finetuning based on the dataset and experimental setups. In the _FedSAM_ method, we test it is very sensitive to the perturbation learning rate. Usually, it should be selected as \(0.1\) in most cases. However, in some poor-sampling cases, i.e. low participation ratio, it should be selected as \(0.01\). In the _FedDyn_, we test it is very sensitive to the regularization coefficient.

Generally, it adopts the regularization coefficient to be \(0.1\) on CIFAR-10 and \(0.01/0.001\) on CIFAR-100. In _FedCM_, we select the client-level coefficient as \(0.1\) which is followed by Xu et al. [46] in most cases. However, on the VGG-11 model, it fails to converge with a small client-level coefficient.

\begin{table}
\begin{tabular}{c|c c} \hline \hline Dataset & CIFAR-10 & best selection \\ \hline communication round \(T\) & 500 & - \\ local interval \(K\) & 5 & - \\ minibatch & 50 & - \\ weight decay & \(1e^{-3}\) & - \\ \hline local learning rate & \([0.01,0.1,0.5,1]\) & 0.1 \\ global learning rate & \([0.01,0.1,1.0]\) & \(1.0/0.1\) \\ learning rate decay & \([0.995,0.998,0.9995]\) & \(0.998\) \\ relaxed coefficient \(\beta\) & \([0.01,0.02,0.05,0.1,0.15]\) & \(0.1/0.01\) \\ \hline \hline Dataset & CIFAR-100 & best selection \\ \hline communication round \(T\) & 500 & - \\ local interval \(K\) & 5 & - \\ minibatch & 50 & - \\ weight decay & \(1e^{-3}\) & - \\ \hline local learning rate & \([0.01,0.1,0.5,1]\) & 0.1 \\ global learning rate & \([0.01,0.1,1.0]\) & \(1.0/0.1\) \\ learning rate decay & \([0.998,0.9995,0.9998]\) & \(0.998/0.9995\) \\ relaxed coefficient \(\beta\) & \([0.01,0.02,0.05,0.1,0.15]\) & \(\star\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: General hyperparameters introductions.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Method & specific hyperparameter & introduction & selection & best selection \\ \hline FedAdam & global learning rate & adaptive learning rate & \([0.01,0.05,0.1,1]\) & \(0.1\) \\ FedSAM & perturbation learning rate & ascent step update & \([0.01,0.1,1]\) & \(0.1\) \\ FedDyn & regularization coefficient & coefficient of prox-term & \([0.001,0.01,0.1,1]\) & \(\star\) \\ FedCM & client-level coefficient & ratios in local updates & \([0.05,0.1,0.5,0.9]\) & \(\star\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Algorithm-specific hyperparameter introductions.

[MISSING_PAGE_EMPTY:29]

#### b.2.2 Consistency of Different Initialization

In this part, we mainly test the consistency level of different \(\beta\). The coefficient \(\beta\) controls the divergence level of the local initialization states. We select the _FedAvg_ and _SCAFFOLD_ to show the efficiency of the proposed relaxed initialization.

These experiments show that the relaxed initialization (RI) effectively reduces the consistency and improves the test accuracy. In all tests, when \(\beta=0\) (green curve), it represents the vanilla method without RI. After incorporating the RI, the test accuracy achieves at least 2% improvement on each setup.

Figure 8: Experiments of _SCAFFOLD_ on the CIFAR-10 dataset.

Figure 7: Experiments of _FedAvg_ on the CIFAR-10 dataset.

#### b.2.3 Communication, Calculation and Storage Costs

In this part, we mainly compare the communication, calculation, and storage costs theoretically and experimentally. By assuming the total model maintain \(d\) dimensions, we summarize the costs of benchmarks and our proposed _FedInit_ as follows:

where \(N\) is the number of participating clients, \(C\) is the total number of clients, and \(K\) is the local training interval.

**Limitations of the benchmarks.** From this table, we can see that _SCAFFOLD_ and _FedCM_ both require double communication costs than the vanilla _FedAvg_. They adopt the correction term (variance reduction and client-level momentum) to revise each local iteration. Though this achieves good performance, we must indicate that under the millions of edge devices in the FL paradigm, this may introduce a very heavy communication bottleneck. In addition, the _FedSAM_ method considers adopting the local SAM optimizer instead of ERM to approach the flat minimal. However, it requires double gradient calculations per iteration. For the very large model, it brings a large calculation cost that can not be neglected. _SCAFFOLD_ and _FedDyn_ are required to store \(3\times\) vectors on each local devices. This is also a limitation for the light device, i.e. mobiles.

We also test the practical wall-clock time on real devices. Our experiment environments are stated as follows:

In the following table, we test the wall-clock time cost of each method:

From this table, due to the different communication costs and calculation costs, the practical wall-clock time is different for each method. Generally, _FedAvg_ adopts the local-SGD updates without any additional calculations. _FedAdam_ adopts similar local-SGD updates and an adaptive optimizer on the global server. _FedSAM_ calculation double gradients, which is the main reason for being slowest among the benchmarks. _SCAFFOLD_, _FedDyn_, and _FedCM_ are required to calculate some additional vectors to correct the local updates. Therefore they need some additional time costs. Our proposed _FedInit_ only adopts an additional initialization calculation, which requires the same costs as _FedAvg_.

\begin{table}
\begin{tabular}{c|c|c c c c c} \hline \hline  & FedAvg & FedAdam & FedSAM & SCAFFOLD & FedDyn & FedCM & FedInit \\ \hline \(10\%\)-100 & 19.38 & 23.22 & 30.23 & 28.61 & 23.84 & 22.63 & **20.41** \\ ratio & \(1\times\) & \(1.19\times\) & \(1.56\times\) & \(1.47\times\) & \(1.23\times\) & \(1.17\times\) & **1.05\(\times\)** \\ \hline \(5\%\)-200 & 15.87 & 17.50 & 22.18 & 24.49 & 20.61 & 18.19 & **16.14** \\ ratio & \(1\times\) & \(1.10\times\) & \(1.40\times\) & \(1.54\times\) & \(1.30\times\) & \(1.15\times\) & **1.02\(\times\)** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Wall-clock time cost (\(\mathrm{s/round}\)).

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline Method & communication & ratio & gradient calculation & ratio & total storage & ratio \\ \hline FedAvg & \(Nd\) & \(1\times\) & \(NKd\) & \(1\times\) & \(Cd\) & \(1\times\) \\ FedAdam & \(Nd\) & \(1\times\) & \(NKd\) & \(1\times\) & \(Cd\) & \(1\times\) \\ FedSAM & \(Nd\) & \(1\times\) & \(2NKd\) & \(2\times\) & \(2Cd\) & \(2\times\) \\ SCAFFOLD & \(2Nd\) & \(2\times\) & \(NKd\) & \(1\times\) & \(3Cd\) & \(3\times\) \\ FedDyn & \(Nd\) & \(1\times\) & \(NKd\) & \(1\times\) & \(3Cd\) & \(3\times\) \\ FedCM & \(2Nd\) & \(2\times\) & \(NKd\) & \(1\times\) & \(2Cd\) & \(3\times\) \\ FedInit & \(Nd\) & \(1\times\) & \(NKd\) & \(1\times\) & \(Cd\) & \(1\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Communication, calculation, and storage costs per communication round.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline GPU & CUDA & Driver Version & CUDA Version & Platform \\ \hline Tesla-V100 (16GB) & NVIDIA-SMI 470.57.02 & 470.57.02 & 11.4 & Pytorch-1.12.1 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Experiment environments.

#### b.2.4 Training Efficiency: Communication Rounds and Time Costs

In this part, we mainly show the results of the training efficiency. We set the target accuracy and compare their required communication rounds and training time respectively. We test on the ResNet-18-GN model with the 10%-100 Dir-0.1 splitting.

The setups of the test environment are stated in Table 9. According to this table, we clearly see that some advanced methods, i.e. _SCAFFOLD_ and _FedDyn_, are efficient on the communication round \(T\). However, due to the additional costs of each training iteration, they must spend more time on the total training. _FedInit_ is a very light and practical method, which only adopts a relaxed initialization on the _FedAvg_ method, which makes it to be better and even achieves SOTA results.

\begin{table}
\begin{tabular}{l c|c c|c c|c c} \hline \hline \multirow{3}{*}{Method} & \multicolumn{3}{c}{CIFAR-10 (\(\geq\)70\%)} & \multicolumn{3}{c}{CIFAR-100 (\(\geq\)30\%)} \\ \cline{2-9}  & \multicolumn{2}{c}{Round} & \multicolumn{2}{c}{Time (s)} & \multicolumn{2}{c}{Round} & \multicolumn{2}{c}{Time (s)} \\ \cline{2-9}  & \multicolumn{2}{c|}{Speed Ratio} & \multicolumn{2}{c|}{Speed Ratio} & \multicolumn{2}{c|}{Speed Ratio} & \multicolumn{2}{c}{Speed Ratio} \\ \hline FedAvg & 371 & 1\(\times\) & 7189 & 1\(\times\) & 191 & 1\(\times\) & 3701 & 1\(\times\) \\ FedAdam & 489 & 0.76\(\times\) & 11354 & 0.63\(\times\) & 256 & 0.74\(\times\) & 5944 & 0.62\(\times\) \\ FedSAM & 377 & 0.98\(\times\) & 11396 & 0.63\(\times\) & 204 & 0.93\(\times\) & 6166 & 0.60\(\times\) \\ SCAFFOLD & 248 & 1.50\(\times\) & 7095 & 1.01\(\times\) & 211 & 0.90\(\times\) & 6036 & 0.61\(\times\) \\ FedDyn & 192 & 1.93\(\times\) & 4577 & 1.57\(\times\) & 122 & 1.56\(\times\) & 2908 & 1.27\(\times\) \\ FedCM & 183 & 2.02\(\times\) & 4141 & 1.73\(\times\) & **95** & **2.01\(\times\)** & **2149** & **1.72\(\times\)** \\
**FedInit** & **172** & **2.15\(\times\)** & **3510** & **2.04\(\times\)** & 132 & 1.44\(\times\) & 2694 & 1.37\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 11: We train 500 rounds on CIFAR-10 and 800 rounds on CIFAR-100. - means the corresponding method can not achieve the target accuracy during the training processes.