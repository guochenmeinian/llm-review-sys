# Does a sparse ReLU network training problem

always admit an optimum?

 Quoc-Tung Le

Elisa Riccietti

Remi Gribonval

Univ. Lyon, Inria, CNRS, ENS de Lyon, UCB Lyon 1, LIP UMR 5668, F-69342 Lyon, France

###### Abstract

Given a training set, a loss function, and a neural network architecture, it is often taken for granted that optimal network parameters exist, and a common practice is to apply available optimization algorithms to search for them. In this work, we show that the existence of an optimal solution is not always guaranteed, especially in the context of _sparse_ ReLU neural networks. In particular, we first show that optimization problems involving deep networks with certain sparsity patterns do not always have optimal parameters, and that optimization algorithms may then diverge. Via a new topological relation between sparse ReLU neural networks and their linear counterparts, we derive -using existing tools from real algebraic geometry- an algorithm to verify that a given sparsity pattern suffers from this issue. Then, the existence of a global optimum is proved for every concrete optimization problem involving a one-hidden-layer sparse ReLU neural network of output dimension one. Overall, the analysis is based on the investigation of two topological properties of the space of functions implementable as sparse ReLU neural networks: a best approximation property, and a closedness property, both in the uniform norm. This is studied both for (finite) domains corresponding to practical training on finite training sets, and for more general domains such as the unit cube. This allows us to provide conditions for the guaranteed existence of an optimum given a sparsity pattern. The results apply not only to several sparsity patterns proposed in recent works on network pruning/sparsification, but also to classical dense neural networks, including architectures not covered by existing results.

## 1 Introduction

The optimization phase in deep learning consists in minimizing an objective function w.r.t. the set of parameters \(\theta\) of a neural network (NN). While it is arguably sufficient for optimization algorithms to find local minima in practice, training is also expected to achieve the infimum in many situations (for example, in overparameterized regimes networks are trained to zero learning error).

In this work, we take a step back and study a rather fundamental question: _Given a deep learning architecture possibly with sparsity constraints, does its corresponding optimization problem actually admit an optimal \(\theta^{*}\)?_ The question is important for at least two reasons:

1. Practical viewpoint: If the problem does not admit an optimal solution, optimized parameters necessarily diverge to infinity to approximate the infimum (which always exists). This phenomenon has been studied thoroughly in previous works in other contexts such as tensor decomposition [7], robust principal component analysis [30], sparse matrix factorization [18] and also deep learning itself [26; 23; 12]. It causes inherent numerical instability for optimization algorithms. Moreover,the answer to this question depends on the architecture of the neural networks (specified by the number of layers, layers width, activation function, and so forth). A response to this question might suggest a guideline for model and architecture selection.
2. Theoretical viewpoint: the existence of optimal solutions is crucial for the analysis of algorithms and their properties (for example, the properties of convergence, or the characterization of properties of the optimum, related to the notion of implicit bias).

One usual practical (and also theoretical) trick to bypass the question of the existence of optimal solutions is to add a regularization term, which is usually coercive, e.g., the \(L^{2}\) norm of the parameters. The existence of optimal solutions then follows by a classical argument on the extrema of a continuous function in a compact domain. Nevertheless, there are many settings where minimizing the regularized version might result in a high value of the loss since the algorithm has to make a trade-off between the loss and the regularizer. Such a scenario is discussed in Example 3.1. Therefore, studying the existence of optimal solutions without (explicit) regularization is also a question of interest.

Given a training set \(\{(x_{i},y_{i})\}_{i=1}^{P}\), the problem of the existence of optimal solutions can be studied from the point of view of the set of functions implementable by the considered network architecture on the finite input domain \(\Omega=\{x_{i}\}_{i=1}^{P}\). This is the case since the loss is usually of the form \(\ell(f_{\theta}(x_{i}),y_{i})\) where \(f_{\theta}\) is the realization of the neural network with parameters \(\theta\). Therefore, the loss involves directly the image of \(\{x_{i}\}_{i=1}^{P}\) under the function \(f_{\theta}\). For theoretical purposes, we also study the function space on the domain \(\Omega=[-B,B]^{d},B>0\). In particular, we investigate two topological properties of these function spaces, both w.r.t. the infinity norm \(\|\cdot\|_{\infty}\): the best approximation property (BAP), i.e., the guaranteed existence of an optimal solution \(\theta^{*}\), and the closedness, a necessary property for the BAP. These properties are studied in Section 3 and Section 4, respectively. Most of our analysis is dedicated to the case of _regression problems_. We do make some links to the case of classification problems in Section 3.

We particularly focus on analyzing the function space associated with _(structured) sparse ReLU neural networks_, which is motivated by recent advances in machine learning witnessing a compelling empirical success of sparsity based methods in NNs and deep learning techniques, such as pruning [32, 14], sparse designed NN [4, 5], or the lottery ticket hypothesis [10] to name a few. Our approach exploits the notion of networks _either with fixed sparsity level or with fixed sparsity pattern (or support)_. This allows us to establish results covering both classical NNs (whose weights are not contrained to be sparse) and sparse NNs architectures. Our main contributions are:

1. **To study the BAP (i.e., the existence of optimal solutions) in practical problems (finite \(\Omega\))**: we provide a necessary condition and a sufficient one on the architecture (embodied by a sparsity pattern) to guarantee such existence. As a particular consequence of our results, we show that: a) _for one-hidden-layer NNs with a fixed sparsity level_, the training problem on a finite data set _always admits an optimal solution_ (cf. Theorem 3.4 and Corollary 3.1); b) however, practitioners should be cautious since _there also exist fixed sparsity patterns that do not guarantee the existence of optimal solutions_ (cf. Theorem 3.1 and Example 3.1). In the context of an emerging emphasis on _structured sparsity_ (e.g. for GPU-friendliness), this highlights the importance of choosing adequate sparsity patterns.
2. **To study the closedness of the function space on \(\Omega=[-B,B]^{d}\)**. As in the finite case, we provide a necessary condition and a sufficient one for the closedness of the function space of ReLU NNs with a fixed sparsity pattern. In particular, our sufficient condition on one-hidden-layer networks generalizes the closedness results of [26, Theorem 3.8] on "dense' one-hidden-layer ReLU NNs to the case of sparse ones, either with fixed sparsity pattern (cf. Theorem 4.2, Corollary 4.1 and Corollary 4.2) or fixed sparsity level (Corollary 4.3). Moreover, our necessary condition (Theorem 4.1), which is also applicable to deep architectures, exhibits sparsity structures failing the closedness property.

Table 1 and Table 2 summarize our results and their positioning with respect to existing ones. Somewhat surprisingly, the necessary conditions in both domains (\(\Omega\) finite and \(\Omega=[-B,B]^{d}\)) are identical. Our necessary/sufficient conditions also suggest a relation between sparse ReLU neural networks and their linear counterparts.

The rest of this paper is organized as follows: Section 2 discusses related works and introduces notations; the two technical sections, Section 3 and Section 4, presents the results for the case \(\Omega\) finite set and \(\Omega=[-B,B]^{d}\) respectively.

## 2 Related works

The fact that optimal solutions may not exist in tensor decomposition problems is well-documented [7]. The cause of this phenomenon (also referred to as _ill-posedness_[7]) is the non-closedness of the set of tensors of order at least three and of rank at least two. Similar phenomena were shown to happen in various settings such as matrix completion [11, Example 2], robust principal component analysis [30] and sparse matrix factorization [18, Remark A.1]. Our work indeed establishes bridges between the phenomenon on sparse matrix factorization [18] and on sparse ReLU NNs.

There is also an active line of research on the best approximation property and closedness of function spaces of neural networks. Existing results can be classified into two categories: _negative_ results, which demonstrate the non-closedness and _positive_ results for those showing the closedness or best approximation property of function spaces of NNs. Negative results can notably be found in

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Works} & \multirow{2}{*}{Architecture} & Activation functions & \multirow{2}{*}{\(\Omega\)} & \multirow{2}{*}{Function space} & \multirow{2}{*}{BAP} \\ \cline{5-6}  & & & & & \\ \hline Theorem 3.4 & Sparse feed-forward network & ReLU & finite set & \((\mathbb{R}^{P\times d_{o}},\|\cdot\|)\), arbitrary \(\|\cdot\|\) & \(\checkmark\) \\ \hline \hline
[16][15] & Feed-forward network & Heavyside & \([0,1]^{d}\) & \((L^{p}(\Omega),\|\cdot\|_{L^{p}}),\forall p\in[1,\infty)\) & \(\checkmark\) \\ \hline
[9][8]\({}^{\circ}\) & Feed-forward network, Residual feed-forward network & ReLU & \(\mathbb{R}^{d}\) & \((L^{p}_{n}(\Omega),\|\cdot\|_{L^{p}})\), \(p=2\), \(\mu\) is a measure with compact support and is \(\mu\) target function continuous w.t. Lebesgue measure & \(\checkmark\) (if the target function is continuous) \\ \hline
[26] & Feedforward network & ReLU, pReLU & \([-B,B]^{d}\) & \((C^{0}(\Omega),\|\cdot\|_{\infty})\) & \(\times\) \\ \hline \hline Corollary 4.1\({}^{\dagger}\) & Feed-forward network & ReLU & \([-B,B]^{d}\) & \((C^{0}(\Omega),\|\cdot\|_{\infty})\) & \(\times\) \\ \hline Corollary 4.2 & Sparse feed-forward network & ReLU & \([-B,B]^{d}\) & \((C^{0}(\Omega),\|\cdot\|_{\infty})\) & \(\times\) \\ \hline \end{tabular}
\end{table}
Table 1: **Closedness** results. All results are established for _one-hidden-layer_ architectures with _scalar-valued_ output, except \(\dagger\) (which is valid for one-hidden-layer architectures with vector-valued output). In \(\diamond\), if the architecture is simply a feed-forward network, then the result is valid for any \(p>1\).

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{Works} & \multirow{2}{*}{Architecture} & Activation functions & \multirow{2}{*}{Function space} & \multicolumn{2}{c|}{Assumptions are valid for any...} \\ \cline{5-6}  & & & & \(L\) & \(N_{L-1}\) & \(N_{L}\) \\ \hline
[12] & Feedforward network & Sigmoid & \((C^{0}(\Omega),\|\cdot\|_{\infty})\) & \(\times\) & \(\times\) \\  & & & \((L=2)\) & \((N_{L-1}\geq 2)\) & \((N_{L}=1)\) \\ \hline
[20]\({}^{\circ}\) & Feedforward network & ReLU & \((\mathbb{R}^{N_{L}\times^{P}},\|\cdot\|)\), \(P=6\) & \(\times\) & \(\times\) & \(\times\) \\  & & & \((L=2)\) & \((N_{L-1}=2)\) & \((N_{L}=2)\) \\ \hline
[26] & Feedforward network & 
\begin{tabular}{c} sigmoid, tanh, arctan, \\ ISRLU, ISRU, ISRU \\ \end{tabular} & \((C^{0}(\Omega),\|\cdot\|_{\infty})\) & & & \\ \cline{5-6}  & & & \((L^{p}(\Omega),\|\cdot\|_{L^{p}})\) & & \\  & & & \((N_{L-1}\geq 2)\) & \((N_{L}=1)\) \\ \hline
[23] & Feedforward network & \begin{tabular}{c} ISRLU \\ \end{tabular} & 
\begin{tabular}{c} \((W^{2,p}(\Omega),\|\cdot\|_{L^{p}})\) \\ \(\forall p\in[1,\infty]\) \\ \end{tabular} & \(\checkmark\) & \(\times\) & \(\times\) \\ \cline{5-6}  & & & \((W^{p}(\Omega),\|\cdot\|_{L^{p}})\) & & \\ \cline{5-6}  & & & \((W^{p}(\Omega),\|\cdot\|_{L^{p}})\) & & \\ \cline{5-6}  & & & \((W^{p}(\Omega),\|\cdot\|_{L^{p}})\) & & \\ \cline{5-6}  & & & \((W^{p}(\Omega),\|\cdot\|_{L^{p}})\) & & \\ \hline \hline Theorem 4.1\({}^{\ddagger}\) & Sparse feedforward network & ReLU & \((C^{0}(\Omega),\|\cdot\|_{\infty})\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ \hline Theorem 3.1\({}^{\circ}\) & Sparse feedforward network & ReLU & \((\mathbb{R}^{N_{L}\times^{P}},\|\cdot\|)\) & \(\checkmark\) & \(\checkmark\) & \(\checkmark\) \\ \hline \end{tabular}
\end{table}
Table 2: **Non-closedness** results (notations in Section 2). Previous results consider \(\Omega=[-B,B]^{d}\); ours cover: \(\diamond\) a finite \(\Omega\) with \(P\) points; \(\ddagger\) a bounded \(\Omega\) with non-empty interior (this includes \(\Omega=[-B,B]^{d}\)).

[12, 26, 23], showing that the set of functions implemented as conventional multilayer perceptrons with various activation functions such as Inverse Square Root Linear Unit (ISRLU), Inverse Square Root Unit (ISRU), parametric ReLU (pReLU), Exponential Linear Unit (ELU) [26, Table 1] is not a closed subset of classical function spaces (e.g., the Lebesgue spaces \(L^{p}\), the set of continuous functions \(C^{0}\) equipped with the sup-norm, or Sobolev spaces \(W^{k,p}\)). In a more practical setting, [20] hand-crafts a dataset of six points which makes the training problem of a dense one-hidden-layer neural network not admit any solution. Positive results are proved in [26, 16, 15], which establish both the closedness and/or the BAP. The BAP implies closedness [12, Proposition 3.1][26, Section 3] (but the converse is not true, see Appendix D) hence the BAP can be more difficult to prove than closedness. So far, the only architecture proved to admit the best approximation property (and thus, also closedness) is _one-hidden-layer neural networks_ with _heavyside activation_ function and _scalar-valued output_ (i.e., output dimension equal to _one_) [15] in \(L^{p}(\Omega),\forall p\in[1,\infty]\). If one allows additional assumptions such as the target function \(f\) being continuous, then BAP is also established for one-hidden layer and residual one-hidden-layer NNs with ReLU activation function [9, 8]. In all other settings, to the best of our knowledge, the only property proved in the literature is closedness, but the BAP remains elusive. We compare our results with existing works in Tables 1 and 2.

In machine learning, there is an ongoing endeavour to explore sparse deep neural networks, as a prominent approach to reduce memory and computation overheads inherent in deep learning. One of its most well-known methods is Iterative Magnitude Pruning (IMP), which iteratively trains and prunes connections/neurons to achieve a certain level of sparsity. This method is employed in various works [14, 32], and is related to the so-called Lottery Ticket Hypothesis (LTH) [10]. The main issue of IMP is its running time: one typically needs to perform many steps of pruning and retraining to achieve a good trade-off between sparsity and performance. To address this issue, many works attempt to identify the sparsity patterns of the network before training. Once they are found, it is sufficient to train the sparse neural networks once. These _pre-trained_ sparsity patterns can be found through algorithms [29, 31, 19] or leveraging the sparse structure of well-known fast linear operators such as the Discrete Fourier Transform [5, 4, 21, 6, 3]. Regardless of the approaches, these methods are bound to train a neural network with _fixed sparsity pattern_ at some points. This is a particular motivation for our work and our study on the best approximation property of sparse ReLU neural networks with fixed sparsity pattern.

NotationsIn this work, \([\![n]\!]:=\{1,\ldots,n\}\). For a matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\), \(\mathbf{A}[i,j]\) denotes the coefficient at the index \((i,j)\); for subsets \(S_{r}\subseteq[\![m]\!],S_{c}\subseteq[\![n]\!]\), \(\mathbf{A}[S_{r},:]\) (resp. \(\mathbf{A}[:,S_{c}]\)) is a matrix of the same size as \(\mathbf{A}\) and agrees with \(\mathbf{A}\) on rows in \(S_{r}\) (resp. columns in \(S_{c}\)) of \(\mathbf{A}\) while its remaining coefficients are zero. The operator \(\mathsf{supp}(\mathbf{A}):=\{(\ell,k)\mid\mathbf{A}[\ell,k]\neq 0\}\) returns the _support_ of the matrix \(\mathbf{A}\). We denote \(\mathbf{1}_{m\times n}\) (resp. \(\mathbf{0}_{m\times n}\)) an all-one (resp. all-zero) matrix of size \(m\times n\).

An architecture with fixed sparsity pattern is specified via \(\mathbf{I}=(I_{L},\ldots,I_{1})\), a collection of binary masks \(I_{i}\in\{0,1\}^{N_{i}\times N_{i-1}},1\leq i\leq L\), where the tuple \((N_{L},\ldots,N_{0})\) denotes the dimensions of the input layer \(N_{0}=d\), hidden layers \((N_{L-1},\ldots,N_{1})\) and output layer (\(N_{L}\)), respectively. The binary mask \(I_{i}\) encodes the support constraints on the \(i\)th weight matrix \(\mathbf{W}_{i}\), i.e., \(I_{i}[\ell,k]=0\) implies \(\mathbf{W}_{i}[\ell,k]=0\). It is also convenient to think of \(I_{i}\) as the set \(\{(\ell,k)\mid I_{i}[\ell,k]=1\}\), a subset of \([\![N_{i}]\!]\times[\![N_{i-1}]\!]\). We will use these two interpretations (binary mask and subset) interchangeably and the meaning should be clear from context. We will even abuse notations by denoting \(I_{l}\subseteq\mathbf{1}_{N_{l}\times N_{i-1}}\). Because the support constraint \(I\) can be thought as a binary matrix, the notation \(I[S_{r},:]\) (resp. \(I[:,S_{c}]\)) represents the support constraint of \(I\cap S_{r}\times[\![n]\!]\) (resp. \(I\cap[\![n]\!]\times S_{c}\)).

The space of parameters on the sparse architecture \(\mathbf{I}\) is denoted \(\mathcal{N}_{\mathbf{I}}\), and for each \(\theta\in\mathcal{N}_{\mathbf{I}}\), \(\mathcal{R}_{\theta}:\mathbb{R}^{N_{0}}\mapsto\mathbb{R}^{N_{L}}\) is the function implemented by the ReLU network with parameter \(\theta\):

\[\mathcal{R}_{\theta}:x\in\mathbb{R}^{N_{0}}\mapsto\mathcal{R}_{\theta}(x):= \mathbf{W}_{L}\sigma(\ldots\sigma(\mathbf{W}_{1}x+\mathbf{b}_{1})\ldots+ \mathbf{b}_{L-1})+\mathbf{b}_{L}\in\mathbb{R}^{N_{L}}\] (1)

where \(\sigma(x)=\max(0,x)\) is the ReLU activation.

Finally, for a given architecture \(\mathbf{I}\), we define

\[\mathcal{L}_{\mathbf{I}}=\{\mathbf{X}_{L}\ldots\mathbf{X}_{1}\mid\mathsf{ supp}(\mathbf{X}_{i})\subseteq I_{i},i\in[\![L]\!]\}\subseteq\mathbb{R}^{N_{L} \times N_{0}}\] (2)

the set of matrices factorized into \(L\) factors respecting the support constraints \(I_{i},i\in[\![L]\!]\). In fact, \(\mathcal{L}_{\mathbf{I}}\) is the set of linear operators implementable as _linear_ neural networks (i.e., with \(\sigma=\mathrm{i}\mathrm{d}\) instead of the ReLU in (1), and no biases) with parameters \(\theta\in\mathcal{N}_{\mathbf{I}}\).

Analysis of fixed support ReLU neural networks for finite \(\{\}\)

The setting of a finite set \(\Omega=\{x_{i}\}_{i=1}^{P}\) is common in many practical machine learning tasks: models such as (sparse) neural networks are trained on often large (but finite) annotated dataset \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{P}\). The optimization/training problem usually takes the form:

\[\underset{\theta}{\text{Minimize}}\qquad\mathcal{L}(\theta)=\sum_{i=1}^{P} \ell(\mathcal{R}_{\theta}(x_{i}),y_{i}),\qquad\text{under sparsity constraints on }\theta\] (3)

where \(\ell\) is a loss function measuring the similarity between \(\mathcal{R}_{\theta}(x_{i})\) and \(y_{i}\). A natural question that we would like to address for this task is:

**Question 3.1**.: _Under which conditions on \(\mathbf{I}\), the prescribed sparsity pattern for \(\theta\), does the training problem of sparse neural networks admit an optimal solution for any finite data set \(\mathcal{D}\)?_

We investigate this question both for parameters \(\theta\) constrained to satisfy a _fixed_ sparsity pattern \(\mathbf{I}\), and in the case of a fixed sparsity level, see e.g. Corollary 4.3.

After showing in Section 3.1 that the answer to Question 3.1 is intimately connected with the closedness of the function space of neural networks with architecture \(\mathbf{I}\), we establish in Section 3.2 that this closedness implies the closedness of the matrix set \(\mathcal{L}_{\mathbf{I}}\) (a property that can be checked using algorithms from real algebraic geometry, see Section 3.3). We also provide concrete examples of support patterns \(\mathbf{I}\) where closedness provably fails, and neural network training can diverge. Section 3.4 presents sufficient conditions for closedness that enable us to show that an optimal solution always exists on scalar-valued one-hidden-layer networks under a constraint on the sparsity level of each layer.

### Equivalence between closedness and best approximation property

To answer Question 3.1, it is convenient to view \(\Omega\) as the matrix \([x_{1},\dots,x_{P}]\in\mathbb{R}^{d\times P}\) and to consider the function space implemented by neural networks with the given architecture \(\mathbf{I}\) on the input domain \(\Omega\) in dimension \(d=N_{0}\), with output dimension \(N_{L}\), defined as the set

\[\mathcal{F}_{\mathbf{I}}(\Omega):=\{\mathcal{R}_{\theta}(\Omega)\mid\theta \in\mathcal{N}_{\mathbf{I}}\}\subseteq\mathbb{R}^{N_{L}\times P}\] (4)

where the matrix \(\mathcal{R}_{\theta}(\Omega):=\big{[}\mathcal{R}_{\theta}(x_{1}),\dots, \mathcal{R}_{\theta}(x_{P})\big{]}\in\mathbb{R}^{N_{L}\times P}\) is the image under \(\mathcal{R}_{\theta}\) of \(\Omega\).

We study the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) under the usual topology induced by any norm \(\|\cdot\|\) of \(\mathbb{R}^{N_{L}\times P}\). This property is interesting because if \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed for any \(\Omega=\{x_{i}\}_{i=1}^{P}\), then an optimal solution is guaranteed to exist for any \(\mathcal{D}\) under classical assumptions of \(\ell(\cdot,\cdot)\). The following result is not difficult to prove, we nevertheless provide a proof in Appendix B.1 for completeness.

**Proposition 3.1**.: _Assume that, for any fixed \(y\in\mathbb{R}^{N_{L}}\), \(\ell(\cdot,y):\mathbb{R}^{N_{L}}\mapsto\mathbb{R}\) is continuous, coercive and that \(y=\arg\min_{y^{\prime}}\ell(y^{\prime},y)\). For any sparsity pattern \(\mathbf{I}\) with input dimension \(N_{0}=d\) the following properties are equivalent:_

1. _irrespective of the training set, problem (_3_) under the constraint_ \(\theta\in\mathcal{N}_{\mathbf{I}}\) _has an optimal solution;_
2. _for every_ \(P\) _and every_ \(\Omega\in\mathbb{R}^{d\times P}\)_, the function space_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\) _is a closed subspace of_ \(\mathbb{R}^{N_{L}\times P}\)_._

The assumption on \(\ell\) is natural and realistic in _regression_ problems: any loss function based on any norm on \(\mathbb{R}^{d}\) (e.g. \(\ell(y^{\prime},y)=\|y^{\prime}-y\|\)), such as the quadratic loss, satisfies this assumption. In the classification case, using the soft-max after the last layer together with the cross-entropy loss function indeed leads to an optimization problem with no optimum (regardless of the architecture) when given a _single_ training pair. This is due to the fact that changing either the bias or the scales of the last layer can lead the output of the soft-max arbitrarily close to an ideal Dirac mass. It is an interesting challenge to identify whether sufficiently many and diverse training samples (as in concrete learning scenarios) make the problem better posed, and amenable to a relevant closedness analysis.

In light of Proposition 3.1 we investigate next the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) for finite \(\Omega\).

### A necessary closedness condition for fixed support ReLU networks

Our next result reveals connections between the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) for finite \(\Omega\) and the closedness of \(\mathcal{L}_{\mathbf{I}}\), the space of sparse matrix products with sparsity pattern \(\mathbf{I}\).

**Theorem 3.1**.: _If \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed for every finite \(\Omega\) then \(\mathcal{L}_{\mathbf{I}}\) is closed._

Theorem 3.1 is a direct consequence of (and in fact logically equivalent to) the following lemma:

**Lemma 3.2**.: _If \(\mathcal{L}_{\mathbf{I}}\) is not closed then there exists a set \(\Omega\subset\mathbb{R}^{d}\), \(d=N_{0}\), of cardinality at most \(P\coloneqq(3N_{0}4^{\sum_{i=1}^{L-1}N_{i}}+1)^{N_{0}}\) such that \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is not closed._

Sketch of the proof.: Since \(\mathcal{L}_{\mathbf{I}}\) is not closed, there exists \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\setminus\mathcal{L}_{ \mathbf{I}}\) (\(\overline{\mathcal{L}}\) is the closure of the set \(\mathcal{L}\)). Considering \(f(x)\coloneqq\mathbf{A}x\), we construct a set \(\Omega=\{x_{i}\}_{i=1}^{P}\) such that \([f(x_{1}),\dots,f(x_{P})]\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)} \setminus\mathcal{F}_{\mathbf{I}}(\Omega)\). Therefore, \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is not closed. 

The proof is in Appendix B.2. Besides showing a topological connection between \(\mathcal{F}_{\mathbf{I}}\) (NNs with ReLU activation) and \(\mathcal{L}_{\mathbf{I}}\) (linear NNs), Theorem 3.1 leads to a simple example where \(\mathcal{F}_{\mathbf{I}}\) is not closed.

**Example 3.1** (LU architecture).: _Consider \(\mathbf{I}=(I_{2},I_{1})\in\{0,1\}^{d\times d}\times\{0,1\}^{d\times d}\) where \(I_{1}=\{(i,j)\mid 1\leq i\leq j\leq d\}\) and \(I_{2}=\{(i,j)\mid 1\leq j\leq i\leq d\}\). Any pair of matrices \(\mathbf{X}_{2},\mathbf{X}_{1}\in\mathbb{R}^{d\times d}\) such that \(\mathbf{supp}(\mathbf{X}_{i})\subseteq I_{i},i=1,2\) are respectively lower and upper triangular matrices. Therefore, \(\mathcal{L}_{\mathbf{I}}\) is the set of matrices that admit an exact lower - upper (LU) factorization/decomposition. That explains its name: **LU** architecture. This set is well known to a) contain an open and dense subset of \(\mathbb{R}^{d\times d}\); **b)** be strictly contained in \(\mathbb{R}^{d\times d}\)[13, Theorem 3.2.1][25, Theorem 1]. Therefore, \(\mathcal{L}_{\mathbf{I}}\) is not closed and by the contraposition of Theorem 3.1 we conclude that there exists a finite set \(\Omega\) such that \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is not closed._

Let us illustrate the impact of the non-closedness in Example 3.1 via the behavior during the training of a fixed support one-hidden-layer neural network with the LU support constraint \(\mathbf{I}\). This network is trained to learn the linear function \(f(x)\coloneqq\mathbf{A}x\) where \(\mathbf{A}\in\mathbb{R}^{d\times d}\) is an anti-diagonal _identity_ matrix. Using the necessary and sufficient condition of **LU** decomposition existence [25, Theorem 1], we have that \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\setminus\mathcal{L}_{ \mathbf{I}}\) as in the sketch proof of Lemma 3.2. Given network parameters \(\theta\) and a training set, approximation quality can be measured by the relative loss: \(\frac{1}{P}(\sum_{i=1}^{P}\|\mathcal{R}_{\theta}(x_{i})-y_{i}\|_{2}^{2}/\|y_{ i}\|_{2}^{2})\).

Figure 1 illustrates the behavior of the relative errors of the training set, validation set and the sum of weight matrices norm along epochs, using Stochastic Gradient Descent (SGD) with batch size \(3000\), learning rate \(0.1\), momentum \(0.9\) and four different weight decays (the hyperparameter controlling the \(L^{2}\) regularizer) \(\lambda\in\times\{0,10^{-4},5\times 10^{-4},10^{-3}\}\). The case \(\lambda=0\) corresponds to the _unregularized_ case. Our training and testing sets contain each \(P=10^{5}\) samples generated independently as \(x_{i}\sim\mathcal{U}([-1,1]^{d})\) (\(d=100\)) and \(y_{i}\coloneqq\mathbf{A}x_{i}\).

Example 3.1 and Figure 1 also lead to two interesting remarks: while the \(L^{2}\) regularizer (weight decay) does prevent the parameter divergence phenomenon, the empirical loss is improved when using the non-regularized version. This is the situation where adding a regularization term might be detrimental, as stated earlier. More interestingly, the size of the dataset is \(10^{5}\), which is much smaller than the theoretical \(P\) in Lemma 3.2. It is thus interesting to see if we can reduce the theoretical value of \(P\), which is currently exponential w.r.t. to the input dimension.

Figure 1: Training a one-hidden-layer fixed support (LU architecture) neural network with different regularization hyperparameters \(\lambda\) (we use weight decay, i.e., an \(L^{2}\) regularizer). Subfigures a)-b) show the relative loss (the lower, the better) for training (empirical loss) and testing (validation loss) respectively. Subfigure c) shows the norm of two weight matrices. The experiments are conducted \(10\) times to produce the error bars in all figures (almost invisible due to a small variability).

### The closedness of \(\mathcal{L}_{\mathbf{I}}\) is algorithmically decidable

Theorem 3.1 leads to a natural question: given \(\mathbf{I}\), how to check the closedness of \(\mathcal{L}_{\mathbf{I}}\), a subset of \(\mathbb{R}^{N_{L}\times N_{0}}\). To the best of our knowledge, there is not any study on the closedness of \(\mathcal{L}_{\mathbf{I}}\) in the literature. It is, thus, not known whether deciding on the closedness of \(\mathcal{L}_{\mathbf{I}}\) for a given \(\mathbf{I}\) is polynomially tractable. In this work, we show it is at least decidable with a doubly-exponential algorithm. This algorithm is an application of _quantifier elimination_, an algorithm from real algebraic geometry [2].

**Lemma 3.3**.: _Given \(\mathbf{I}=(I_{L},\ldots,I_{1})\), the closedness of \(\mathcal{L}_{\mathbf{I}}\) is decidable with an algorithm of complexity \(O((4L)^{C^{k-1}})\) where \(k=N_{L}N_{0}+1+2\sum_{i=1}^{L}|L_{i}|\) and \(C\) is a universal constant._

We prove Lemma 3.3 in Appendix B.4. Since the knowledge of \(\mathbf{I}\) is usually available (either fixed before training [19; 31; 4; 21; 3] or discovered by a procedure before re-training [10; 14; 32]), the algorithm in Lemma 3.3 is able to verify whether the training problem might not admit an optimum. While such a doubly exponential algorithm in Lemma 3.3 is seemingly impractical in practice, small toy examples (for example, Example 3.1 with \(d=2\)) can be verified using Z3Prover1, a software implementing exactly the algorithm in Lemma 3.3. However, Z3Prover is already unable to terminate when run on the \(\mathbf{LU}\) architecture of Example 3.1 with \(d=3\). This calls for more efficient algorithms to determine the closedness of \(\mathcal{L}_{\mathbf{I}}\) given \(\mathbf{I}\). The same algorithmic question can be also asked for \(\mathcal{F}_{\mathbf{I}}\). We leave these problems (in this general form) as open questions.

Footnote 1: The package is developed by Microsoft research and it can be found at https://github.com/Z3Prover/z3

In fact, if such a polynomial algorithm (to decide the closedness of \(\mathcal{L}_{\mathbf{I}}\)) exists, it can be used to answer the following interesting question:

**Question 3.2**.: _If the supports of the weight matrices are randomly sampled from a distribution, what is the probability that the corresponding training problem potentially admits no optimal solutions?_

While simple, this setting does happen in practice since random supports/binary masks are considered a strong and common baseline for sparse DNNs training [22]. Thanks to Theorem 3.1, if \(\mathcal{L}_{\mathbf{I}}\) is not closed then the support is "bad". Thus, to have an estimation of a _lower bound_ on the probability of "bad" supports, we could sample the supports from the given distribution and use the polynomial algorithm in question to _decide_ if \(\mathcal{L}_{\mathbf{I}}\) is closed. Unfortunately, the algorithm in Lemma 3.3 has doubly exponential complexity, thus hindering its practical use. However, for one-hidden-layer NNs, there is a _polynomial_ algorithm to _detect_ non-closedness: intuitively, if the support constraint is "locally similar" to the \(\mathbf{LU}\) structure, then \(\mathcal{L}_{\mathbf{I}}\) is not closed. This result is elaborated in Appendix B.5 and Lemma B.8. The resulting detection algorithm can have false negatives (i.e., it can fail to detect more complex configurations where \(\mathcal{L}_{\mathbf{I}}\) is not closed) but no false positive.

Figure 2: Probability of _detectable_ “bad” support constraints sampled from uniform distribution over \(100\) samples.

We test this algorithm on a one-hidden layer ReLU network with two \(100\times 100\) weight matrices. We randomly choose their supports whose cardinality are \(p_{1}\cdot 100^{2}\) and \(p_{2}\cdot 100^{2}\) respectively, with \((p_{1},p_{2})\in\{0.1,0.2,\ldots,1.0\}\)2. For each pair \((p_{1},p_{2})\), we sample \(100\) instances. Using the detection algorithm, we obtain Figure 2. The numbers in Figure 2 indicate the probability that a random support constraint \((I,J)\) has \(\mathcal{E}_{I,J}\) non-closed (as detected by the algorithm). This figure shows two things: 1) "Bad" architectures such as \(\mathbf{LU}\) are not rare and one can (randomly) generate plenty of them. 2) At a sparse regime (\(a_{1},a_{2}\leq 0.2\)), most of the random supports might lead to training problems without optimal solutions. We remind that the detection algorithm may give some false negatives. Thus, for less sparse regimes, it is possible that our heuristic fails to detect the non-closedness. The algorithm indeed gives a lower bound on the probability of finding non-closed instances. The code for Example 3.1, Question 3.2 and the algorithm in Lemma 3.3 is provided in [17].

Footnote 2: Notice that \(\mathcal{D}\) contains both input vectors \(x_{i}\) and targets \(y_{i}\), unlike \(\Omega\) which only contains the inputs.

### Best approximation property of scalar-valued one-hidden-layer sparse networks

So far, we introduced a necessary condition for the closedness (and thus, by Proposition 3.1, the best approximation property) of sparse ReLU networks, and we provided an example of an architecture \(\mathbf{I}\) whose training problem might not admit any optimal solution. One might wonder if there are architectures \(\mathbf{I}\) that _avoid_ the issue caused by the non-closedness of \(\mathcal{F}_{\mathbf{I}}\). Indeed, we show that for one-hidden-layer sparse ReLU neural networks with scalar output dimension (i.e., \(L=2,N_{2}=1\)), the existence of optimal solutions is guaranteed, _regardless of the sparsity pattern_.

**Theorem 3.4**.: _Consider scalar-valued, one-hidden-layer ReLU neural networks (i.e., \(L=2,N_{2}=1\)). For any support pairs \(\mathbf{I}=(I_{2},I_{1})\) and any finite set \(\Omega:=\{x_{1},\ldots,x_{P}\}\), \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed._

The proof of Theorem 3.4 is deferred to Appendix B.3. As a sanity check, observe that when \(L=2,N_{2}=1\), the necessary condition in Theorem 3.1 is satisfied. Indeed, since \(N_{2}=1\), \(\mathcal{L}_{\mathbf{I}}\subseteq\mathbb{R}^{1\times N_{0}}\) can be thought as a subset of \(\mathbb{R}^{N_{0}}\). Any \(\mathbf{X}\in\mathcal{L}_{\mathbf{I}}\) can be written as a sum: \(\mathbf{X}=\sum_{i\in I_{2}}\mathbf{W}_{2}[i]\mathbf{W}_{1}[i,:]\), a decomposition of the product \(\mathbf{W}_{2}\mathbf{W}_{1}\), where \(\mathbf{W}_{2}[i]\in\mathbb{R},\mathbf{W}_{1}[i,:]\in\mathbb{R}^{N_{0}}, \mathsf{supp}(\mathbf{W}_{1}[i,:])\subseteq I_{1}[i,:]\). Define \(\mathcal{H}:=\cup_{i\in I_{2}}I_{1}[i,:]\subseteq[N_{0}]\) the union of row supports of the first weight matrix. It is easy to verify that \(\mathcal{L}_{\mathbf{I}}\) is isomorphic to \(\mathbb{R}^{|\mathcal{H}|}\), which is closed. In fact, this argument only works for scalar-valued output, \(N_{2}=1\). Thus, there is no conflict between Theorem 3.1 and Theorem 3.4.

In practice, many approaches search for the best support \(\mathbf{I}\)_among a collection of possible supports_, for example, the approach of pruning and training [14, 32] or the lottery ticket hypothesis [10]. Our result for fixed support in Theorem 3.4 can be also applied in this case and is stated in Corollary 3.1. In particular, we consider a set of supports such that the support sizes (or sparsity ratios) of the layers are kept below a certain threshold \(K_{i},i=1,\ldots,L\). This constraint on the sparsity level of each layer is widely used in many works on sparse neural networks [14, 32, 10].

**Corollary 3.1**.: _Consider scalar-valued, one-hidden-layer ReLU neural networks. For any finite data set2\(\mathcal{D}=(x_{i},y_{i})_{i=1}^{P}\), problem (3) under the constraints \(\|\mathbf{W}_{i}\|_{0}\leq K_{i},i=1,2\) has a minimizer._

Footnote 2: Notice that \(\mathcal{D}\) contains both input vectors \(x_{i}\) and targets \(y_{i}\), unlike \(\Omega\) which only contains the inputs.

Proof.: Denote \(\mathcal{I}\) the collection of sparsity patterns satisfying \(\|I_{i}\|_{0}\leq K_{i},i=1,2\), so that a set of parameters satisfies the sparsity constraints \(\|\mathbf{W}_{i}\|_{0}\leq K_{i},i=1,2\) if and only if the supports of the weight matrices belong to \(\mathcal{I}\). Therefore, to solve the optimization problem under sparsity constraints \(\|\mathbf{W}_{i}\|_{0}\leq K_{i},i=1,2\), it is sufficient to solve the same problem for every sparsity pattern in \(\mathcal{I}\).

For each \(\mathbf{I}\in\mathcal{I}\), we solve a training problem with architecture \(\mathbf{I}\) on a given finite dataset \(\mathcal{D}\). Thanks to Theorem 3.4 and Proposition 3.1, the infimum is attained. We take the optimal solution corresponding to \(\mathbf{I}\) that yields the smallest value of the loss function \(\mathcal{L}\). This is possible because the set \(\mathcal{I}\) has a finite number of elements (the total number of possible sparsity patterns is finite). 

## 4 Analysis of fixed support ReLU networks on continuous domains

We now investigate closedness properties when the domain \(\Omega\subseteq\mathbb{R}^{d}\) is no longer finite. Denoting \(\mathcal{F}_{\mathbf{I}}=\{\mathcal{R}_{\theta}:\mathbb{R}^{N_{0}}\mapsto \mathbb{R}^{N_{L}}\mid\theta\in\mathcal{N}_{\mathbf{I}}^{\prime}\}\) (with \(N_{0}=d\)) the functions that can be implemented on a given ReLU network architecture \(\mathbf{I}\), we are interested in \(\mathcal{F}_{\mathbf{I}}(\Omega)=\{f_{|\Omega}:f\in\mathcal{F}_{\mathbf{I}}\}\), the restriction of elements of \(\mathcal{F}_{\mathbf{I}}\) to \(\Omega\). This is a natural extension of the set \(\mathcal{F}_{\mathbf{I}}(\Omega)\) studied in the case of finite \(\Omega\).

Specifically, we investigate the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) in \((C^{0}(\Omega),\|\cdot\|_{\infty})\) (the set of continuous functions on \(\Omega\) equipped with the supremum norm \(\|f\|_{\infty}:=\sup_{x\in\Omega}\|f(x)\|_{2}\)). Contrary to the previous section, we can no longer exploit Proposition 3.1 to deduce that the closedness property and the BAP are equivalent. The results in this section can be seen as a continuation (and also generalization) of the line of research on the topological property of function space of neural networks [26, 12, 16, 15, 23]. In Section 4.1 and Section 4.2, we provide a necessary and a sufficient condition on \(\mathbf{I}\) for the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) in \((C^{0}(\Omega,\|\cdot\|_{\infty})\) respectively. The condition of the former is valid for any depth, while that of the latter is applicable for one-hidden-layer networks (\(L=2\)). These results are established under various assumptions on \(\Omega\) (such as \(\Omega=[-B,B]^{d}\), or \(\Omega\) being bounded with non-empty interior) that will be specified in each result.

### A necessary condition for closedness of fixed support ReLU network

Theorem 4.1 states our result on the necessary condition for the closedness. Interestingly, observe that this result (which is proved in Appendix C.1) naturally generalizes Theorem 3.1. Again, closedness of \(\mathcal{L}_{\mathbf{I}}\) in \(\mathbb{R}^{N_{L}\times N_{0}}\) is with respect to the usual topology defined by any norm.

**Theorem 4.1**.: _Consider \(\Omega\subset\mathbb{R}^{d}\) a bounded set with non-empty interior, and \(\mathbf{I}\) a sparse architecture with input dimension \(N_{0}=d\). If \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed in \((C^{0}(\Omega),\|\cdot\|_{\infty})\) then \(\mathcal{L}_{\mathbf{I}}\) is closed in \(\mathbb{R}^{N_{L}\times N_{0}}\)._

Theorem 4.1 applies for any \(\Omega\) which is bounded and has non-empty interior. Thus, it encompasses not only the hypercubes \([-B,B]^{d},B>0\) but also many other domains such as closed or open \(\mathbb{R}^{d}\) balls. Similar to Theorem 3.1, Theorem 4.1 is interesting in the sense that it allows us to check the non-closedness of the function space \(\mathcal{F}_{\mathbf{I}}\) (a subset of the infinite-dimensional space \(C^{0}(\Omega)\)) by checking that of \(\mathcal{L}_{\mathbf{I}}\subseteq\mathbb{R}^{N_{L}\times N_{0}}\) (a finite-dimensional space). The latter can be checked using the algorithm presented in Lemma 3.3. Moreover, the \(\mathbf{LU}\) architecture presented in Example 3.1 is also an example of \(\mathbf{I}\) whose function space is not closed in \((C^{0}(\Omega),\|\cdot\|_{\infty})\).

### A sufficient condition for closedness of fixed support ReLU network

The following theorem is the main result of this section. It provides a sufficient condition to verify the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) for \(\Omega=[-B,B]^{d}\), \(B>0\) with one-hidden-layer sparse ReLU neural networks.

**Theorem 4.2**.: _Consider \(\Omega=[-B,B]^{d}\), \(N_{0}=d\) and a sparsity pattern \(\mathbf{I}=(I_{2},I_{1})\) such that:_

1. _There is no support constraint for the weight matrix of the second layer,_ \(\mathbf{W}_{2}\)_:_ \(I_{2}=\mathbf{1}_{N_{2}\times N_{1}}\)_;_
2. _For each non-empty set of hidden neurons,_ \(S\subseteq\llbracket N_{1}\rrbracket\)_,_ \(\mathcal{L}_{\mathbf{I}_{S}}\) _is closed in_ \(\mathbb{R}^{N_{2}\times N_{1}},\) _where_ \(\mathbf{I}_{S}:=(I_{2}[:,S],I_{1}[S,:])\) _is the support constraint restricted to the sub-network with hidden neurons in_ \(S\)_._

_Then the set \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed in \((C^{0}(\Omega),\|\cdot\|_{\infty})\)._

Both conditions in Theorem 4.2 can be verified algorithmically: while the first one is trivial to check, the second one requires us to check the closedness of at most \(2^{N_{1}}\) sets \(\mathcal{L}_{\mathbf{I}_{S}}\) (because there are at most \(2^{N_{1}}\) subsets of \(\llbracket N_{1}\rrbracket\)), which is still algorithmically possible (although perhaps practically intractable) with the algorithm of Lemma 3.3. Apart from its algorithmic aspect, we present two interesting corollaries of Theorem 4.2. The first one, Corollary 4.1, is about the closedness of the function space of fully connected (i.e., with no sparsity constraint) one-hidden-layer neural networks.

**Corollary 4.1** (Closedness of fully connected one-hidden-layer ReLU networks _of any output dimension).: _Given \(\mathbf{I}=(\mathbf{1}_{N_{2}\times N_{1}},\mathbf{1}_{N_{1}\times N_{0}})\), the set \(\mathcal{F}_{\mathbf{I}}\) is closed in \((C^{0}([-B,B]^{d}),\|\cdot\|_{\infty})\) where \(d=N_{0}\)._

Proof.: The result follows from Theorem 4.2 once we check if its assumptions hold. The first one is trivial. To check the second, observe that for every non-empty set of hidden neurons \(S\subseteq\llbracket N_{1}\rrbracket\), the set \(\mathcal{L}_{\mathbf{I}_{S}}\subseteq\mathbb{R}^{N_{2}\times N_{0}}\) is simply the set of matrices of rank at most \(|S|\), which is closed for any \(S\). 

Corollary 4.2 states the closedness of scalar-valued, one-hidden-layer sparse ReLU NNs. In a way, it can be seen as the analog of Theorem 3.4 for \(\Omega=[-B,B]^{d}\).

**Corollary 4.2** (Closedness of _fixed support_ one-hidden-layer ReLU networks with scalar output).: _Given any input dimension \(d=N_{0}\geq 1\), any number of hidden neurons \(N_{1}\geq 1\), scalar output dimension \(N_{2}=1\), and any prescribed supports \(\mathbf{I}=(I_{2},I_{1})\), the set \(\mathcal{F}_{\mathbf{I}}\) is closed in \((C^{0}([-B,B]^{d}),\|\cdot\|_{\infty})\).__Sketch of the proof._ If there exists a hidden neuron \(i\in\llbracket N_{1}\rrbracket\) such that \(I_{2}[i]=0\) (i.e., \(i\notin I_{2}\): \(i\) is not connected to the only output of the network), we have: \(\mathcal{F}_{\mathbf{I}}=\mathcal{F}_{\mathbf{I}^{\prime}}\) where \(\mathbf{I}^{\prime}=\mathbf{I}_{S},S=\llbracket N_{1}\rrbracket\setminus\{i\}\). By repeating this process, we can assume without loss of generality that \(I_{2}[i]=\mathbf{1}_{1\times N_{1}}\). That is the first condition of Theorem 4.2.

Therefore, it is sufficient to verify the second condition of Theorem 4.2. Consider any non-empty set of hidden neurons \(S\subseteq\llbracket N_{1}\rrbracket\), and define \(\mathcal{H}:=\cup_{i\in S}I[i,:]\subseteq\llbracket N_{0}\rrbracket\) the union of row supports of \(I_{1}[S,:]\). It is easy to verify that \(\mathcal{L}_{\mathbf{I}_{S}}\) is isomorphic to \(\mathbb{R}^{\llbracket\mathcal{H}\rrbracket}\), which is closed. The result follows by Theorem 4.2. For a more formal proof, readers can find an inductive one in Appendix C.3. 

In fact, both Corollary 4.1 and Corollary 4.2 generalize [26, Theorem 3.8], which proves the closedness of \(\mathcal{F}_{\mathbf{I}}([-B,B]^{d})\) when \(I_{2}=\mathbf{1}_{1\times N_{1}},I_{1}=\mathbf{1}_{N_{1}\times N_{0}}\) (classical fully connected one-hidden-layer ReLU networks with output dimension equal to one).

To conclude, let us consider the analog to Corollary 3.1: we study the function space implementable as a sparse one-hidden-layer network with constraints on the _sparsity level_ of each layer (i.e., \(\|\mathbf{W}_{i}\|_{0}\leq K_{i},i=1,2\).

**Corollary 4.3**.: _Consider scalar-valued, one-hidden-layer ReLU networks \((L=2,N_{2}=1,N_{1},N_{0})\) with \(\ell^{0}\) constraints \(\|\mathbf{W}_{1}\|_{0}\leq K_{1},\|\mathbf{W}_{2}\|_{0}\leq K_{2}\) for some constants \(K_{1},K_{2}\in\mathbb{N}\). The function space \(\mathcal{F}([-B,B]^{d})\) associated with this architecture is closed in \((C^{0}([-B,B]^{N_{0}}),\|\cdot\|_{\infty})\)._

Proof.: Denote \(\mathcal{I}:=\{(I_{2},I_{1})\mid I_{2}\subseteq\llbracket 1\rrbracket\times \llbracket N_{1}\rrbracket,I_{1}\subseteq\llbracket N_{1}\rrbracket\times \llbracket N_{0}\rrbracket,|I_{1}|\leq K_{1},|I_{2}|\leq K_{2}\}\) the set of sparsity patterns respecting the \(\ell^{0}\) constraints, so that \(\mathcal{F}([-B,B]^{d})=\bigcup_{\mathbf{I}\in\mathcal{I}}\mathcal{F}_{ \mathbf{I}}([-B,B]^{d})\). Since \(\mathcal{I}\) is finite and \(\forall\mathbf{I}\in\mathcal{I},\mathcal{F}_{\mathbf{I}}([-B,B]^{d})\) is closed (Corollary 4.2), the result is proved. 

## 5 Conclusion

In this paper, we study the somewhat overlooked question of the existence of an optimal solution to sparse neural network training problems. The study is accomplished by adopting the point of view of topological properties of the function spaces of such networks on two types of domains: a finite domain \(\Omega\), or (typically) a hypercube. On the one hand, our investigation of the BAP and the closedness of these function spaces reveals the existence of _pathological_ sparsity patterns that fail to have optimal solutions on some instances (cf Theorem 3.1 and Theorem 4.1) and thus possibly cause instabilities in optimization algorithms (see Example 3.1 and Figure 1). On the other hand, we also prove several positive results on the BAP and closedness, notably for sparse one-hidden-layer ReLU neural networks (cf. Theorem 3.4 and Theorem 4.2). These results provide new instances of network architectures where the BAP is proved (cf Theorem 3.4) and substantially generalize existing ones (cf. Theorem 4.2).

In the future, a particular theoretical challenge is to propose necessary and sufficient conditions for the BAP and closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\), if possible covering in a single framework both types of domains \(\Omega\) considered here. The fact that the conditions established on these two types of domains are very similar (cf. the similarity between Theorem 3.1 and Theorem 4.1, as well as between Theorem 3.4 and Corollary 4.2) is encouraging. Another interesting algorithmic challenge is to substantially reduce the complexity of the algorithm to decide the closedness of \(\mathcal{L}_{\mathbf{I}}\) in Lemma 3.3, which is currently doubly exponential. It calls for a more efficient algorithm to make this check more practical. Achieving a practically tractable algorithm would for instance allow to check if a support selected e.g. by IMP is pathological or not. This would certainly consolidate the algorithmic robustness and theoretical foundations of pruning techniques to sparsity deep neural networks. From a more theoretical perspective, the existence of an optimum solution in the context of classical linear inverse problems has been widely used to analyze the desirable properties of certain cost functions, e.g. \(\ell^{1}\) minimization for sparse recovery. Knowing that an optimal solution exists for a given sparse neural network training problem is thus likely to open the door to further fruitful insights.

## Acknowledgement

This project was supported by the AllegroAssai ANR project ANR-19-CHIA-0009. The authors thank the Blaise Pascal Center (CBP) for the computational means. It uses the SIDUS [27] solution developed by Emmanuel Quemener. Q-T. Le wants to personally thank M-L.Nguyen 3 for his enlightenment on the non-equivalence between BAP and closedness in infinite dimension space in Appendix D.

Footnote 3: https://sites.google.com/view/mlnguyen/home

## References

* [1] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural networks with rectified linear units. In _International Conference on Learning Representations_, 2018.
* [2] Saugata Basu, Richard Pollack, and Marie-Francoise Roy. _Algorithms in Real Algebraic Geometry (Algorithms and Computation in Mathematics)_. Springer-Verlag, Berlin, Heidelberg, 2006.
* [3] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022.
* [4] Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Re. Monarch: Expressive structured matrices for efficient and accurate training. In _39th International Conference on Machine Learning_, 2022.
* [5] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Re. Learning fast algorithms for linear transforms using butterfly factorizations. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 1517-1527. PMLR, 09-15 Jun 2019.
* [6] Tri Dao, Nimit Sharad Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher Re. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020.
* [7] Vin de Silva and Lek-Heng Lim. Tensor rank and the ill-posedness of the best low-rank approximation problem. _SIAM Journal on Matrix Analysis and Applications_, 30:1084-1127, 08 2006.
* [8] Steffen Dereich, Arnulf Jentzen, and Sebastian Kassing. On the existence of minimizers in shallow residual relu neural network optimization landscapes, 2023.
* [9] Steffen Dereich and Sebastian Kassing. On the existence of optimal shallow feedforward networks with relu activation, 2023.
* [10] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In _International Conference on Learning Representation_, 2019.
* [11] Nicolas Gillis and Francois Glineur. Low-rank matrix approximation with weights or missing data is np-hard. _SIAM Journal on Matrix Analysis and Applications_, 32, 12 2010.
* [12] Federico Girosi and Tomaso Poggio. Networks and the best approximation property. _Biological Cybernetics_, 63, 08 2002.
* [13] Gene H. Golub and Charles F. Van Loan. _Matrix Computations_. The Johns Hopkins University Press, third edition, 1996.

* [14] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 28. Curran Associates, Inc., 2015.
* [15] P. Kainen, V. Kurkova, and A. Vogt. Best approximation by heaviside perceptron networks. _Neural Networks_, 13(7):695-697, 2000.
* [16] Vera Kurkova. Approximation of functions by perceptron networks with bounded number of hidden units. _Neural Networks_, 8(5):745-750, 1995.
* [17] Quoc-Tung Le, Remi Gribonval, and Elisa Riccietti. Code for reproducible research: Does a sparse ReLU network training problemalways admit an optimum? Code repository available at https://hal.science/hal-04233925, October 2023.
* [18] Quoc-Tung Le, Elisa Riccietti, and Remi Gribonval. Spurious Valleys, NP-hardness, and Tractability of Sparse Matrix Factorization With Fixed Support. _SIAM Journal on Matrix Analysis and Applications_, 2022.
* [19] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. SNIP: single-shot network pruning based on connection sensitivity. _ICLR_, 2019.
* [20] Lek-Heng Lim, Mateusz Michalek, and Yang Qi. Best \(k\)-layer neural network approximations. _Constructive Approximation_, June 2021.
* [21] Rui Lin, Jie Ran, King Hung Chiu, Graziano Chesi, and Ngai Wong. Deformable butterfly: A highly structured and sparse linear transform. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 16145-16157. Curran Associates, Inc., 2021.
* [22] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness of random pruning: Return of the most naive baseline for sparse training. _arXiv preprint arXiv:2202.02643_, 2022.
* [23] Scott Mahan, Emily J. King, and Alex Cloninger. Nonclosedness of sets of neural networks in sobolev spaces. _Neural Networks_, 137:85-96, may 2021.
* Volume 2_, NIPS'14, page 2924-2932, Cambridge, MA, USA, 2014. MIT Press.
* [25] Pavel Okunev and Charles R. Johnson. Necessary and sufficient conditions for existence of the lu factorization of an arbitrary matrix, 2005.
* [26] Philipp Petersen, Mones Raslan, and Felix Voigtlaender. Topological properties of the set of functions generated by neural networks of fixed size. _Found. Comput. Math._, 21(2):375-444, apr 2021.
* [27] E. Quemener and M. Corvellec. SIDUS--the Solution for Extreme Deduplication of an Operating System. _Linux Journal_, 2013.
* [28] Pierre Stock and Remi Gribonval. An Embedding of ReLU Networks and an Analysis of their Identifiability. _Constructive Approximation_, 2022.
* [29] Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, and Surya Ganguli. Pruning neural networks without any data by iteratively conserving synaptic flow. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc.
* [30] Jared Tanner, Andrew Thompson, and Simon Vary. Matrix rigidity and the ill-posedness of robust pca and matrix completion. _SIAM Journal on Mathematics of Data Science_, 1(3):537-554, 2019.

* [31] Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by preserving gradient flow. In _International Conference on Learning Representations_, 2020.
* May 3, 2018, Workshop Track Proceedings_. OpenReview.net, 2018.
Additional notations

In this work, matrices are written in bold uppercase letters. Vectors are written in bold lowercase letters only if they indicate network parameters (such as bias). For a matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\), we use \(\mathbf{A}[i,\cdot]\in\mathbb{R}^{1\times n}\) (resp. \(\mathbf{A}[:,i]\in\mathbb{R}^{m\times 1}\)) to denote the row (resp. column) vector corresponding the \(i\)th row (resp. column) of \(\mathbf{A}\). To ease the notation, we write \(\mathbf{A}[i,:]\mathbf{v}\) to denote the scalar product between \(\mathbf{A}[i,:]\) and the vector \(\mathbf{v}\in\mathbb{R}^{n}\). This notation will be used regularly when we decompose the functions of one-hidden-neural networks into sum of functions corresponding to hidden neurons.

For a vector \(v\in\mathbb{R}^{d}\), \(v[I]\in\mathbb{R}^{|I|}\) is the vector \(v\) restricted to coefficients in \(I\subseteq[\![d]\!]\). If \(I=\{i\}\) a singleton, \(v[i]\in\mathbb{R}\) is the \(i\)th coefficient of \(v\). We also use \(\mathbf{1}_{m}\) and \(\mathbf{0}_{m}\) to denote an all-one (resp. all-zero) vector of size \(m\).

For a _dense_ (fully connected) feedforward architecture, we denote \(\mathbf{N}=(N_{L},\ldots,N_{0})\) the dimensions of the input layer \(N_{0}=d\), hidden layers (\(N_{L-1},\ldots,N_{1}\)) and output layer (\(N_{L}\)), respectively. The parameters space of the dense architecture \(\mathbf{N}\) is denoted by \(\mathcal{N}_{\mathbf{N}}\): it is the set of all coefficients of the weight matrices \(\mathbf{W}_{i}\in\mathbb{R}^{N_{i}\times N_{i-1}}\) and bias vectors \(\mathbf{b}_{i}\in\mathbb{R}^{N_{i}},i=1,\ldots,L\). It is easy to verify that \(\mathcal{N}_{\mathbf{N}}\) is isomorphic to \(\mathbb{R}^{N}\) where \(N=\sum_{i=1}^{L}N_{i-1}N_{i}+\sum_{i=1}^{L}N_{i}\) is the total number of parameters of the architecture.

Clearly, \(\mathcal{N}_{\mathbf{I}}\subseteq\mathcal{N}_{\mathbf{N}}\) since:

\[\mathcal{N}_{\mathbf{I}}:=\{\theta=\left((\mathbf{W}_{i},\mathbf{b}_{i}) \right)_{i=1,\ldots,L}:\text{supp}(\mathbf{W}_{i})\subseteq I_{i},\forall i=1, \ldots,L.\}.\] (5)

A special subset of \(\mathcal{N}_{\mathbf{I}}\) is the set of network parameters with zero biases,

\[\mathcal{N}_{\mathbf{I}}^{0}:=\{\theta=\left((\mathbf{W}_{i},\mathbf{0}_{N_{i }})\right)_{i=1,\ldots,L}:\text{supp}(\mathbf{W}_{i})\subseteq I_{i},\forall i =1,\ldots,L.\}.\] (6)

Given an activation function \(\nu\), the realization \(\mathcal{R}_{\theta}^{\nu}\) of a neural network \(\theta\in\mathcal{N}_{\mathbf{N}}\) is the function

\[\mathcal{R}_{\theta}^{\nu}:x\in\mathbb{R}^{N_{0}}\mapsto\mathcal{R}_{\theta}^ {\nu}(x):=\mathbf{W}_{L}\nu(\ldots\nu(\mathbf{W}_{1}x+\mathbf{b}_{1})\ldots+ \mathbf{b}_{L-1})+\mathbf{b}_{L}\in\mathbb{R}^{N_{L}}\] (7)

We denote \(\mathcal{R}^{\nu}:\theta\mapsto\mathcal{R}_{\theta}^{\nu}\) the functional mapping from a set of parameters \(\theta\) to its realization. The function space associated to a sparse architecture \(\mathbf{I}\) and activation function \(\nu\) is the image of \(\mathcal{N}_{\mathbf{I}}\) under \(\mathcal{R}^{\nu}\):

\[\mathcal{F}_{\mathbf{I}}^{\nu}:=\mathcal{R}^{\nu}(\mathcal{N}_{\mathbf{I}}).\] (8)

When \(\nu=\sigma\) the ReLU activation function, we recover the definition of realization in Equation (1). We use the shorthands

\[\mathcal{R}_{\theta} :=\mathcal{R}_{\theta}^{\sigma}\] (9) \[\mathcal{F}_{\mathbf{I}} :=\mathcal{F}_{\mathbf{I}}^{\sigma},\]

as in the main text. This allows us to define \(\mathcal{L}_{\mathbf{1}}\) (cf. Equation (2)) as \(\mathcal{L}_{\mathbf{I}}:=\mathcal{R}^{\text{Id}}(\mathcal{N}_{\mathbf{I}}^{0})\) where \(\nu=\text{Id}\) is the identity map, which is a subset of linear maps \(\mathbb{R}^{N_{0}}\mapsto\mathbb{R}^{N_{L}}\).

## Appendix B Proofs for results in Section 3

### Proof of Proposition 3.1

Proof.: First, we remind the problem of the training of a sparse neural network on a finite data set \(\mathcal{D}=\{(x_{i},y_{i})\}_{i=1}^{P}\):

\[\underset{\theta\in\mathcal{N}_{\mathbf{I}}}{\text{Minimize}}\qquad\mathcal{L} (\theta):=\sum_{i=1}^{P}\ell(\mathcal{R}_{\theta}(x_{i}),y_{i}),\] (10)

which shares the same optimal value as the following optimization problem:

\[\underset{\mathbf{D}\in\mathcal{F}_{\mathbf{I}}(\Omega)}{\text{Minimize}}\qquad \mathcal{L}(\mathbf{D}):=\sum_{i=1}^{P}\ell(\mathbf{D}[:,i],y_{i})\] (11)

where \(\Omega=\{x_{i}\}_{i=1}^{P}\). This is simply a change of variables: from \(\mathcal{R}_{\theta}(x_{i})\) to the \(i\)th column of \(\mathbf{D}=\mathcal{R}_{\theta}(\Omega)\). We prove two implications as follows:1. _Assume the closedness of_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\) _for every finite_ \(\Omega\)_. Then an optimal solution of the optimization problem (_10_) exists for every finite data set_ \(\{(x_{i},y_{i})\}_{i=1}^{P}\). Consider a training set_ \(\{(x_{i},y_{i})\}_{i=1}^{P}\) _and_ \(\Omega\coloneqq\{x_{i}\}_{i=1}^{P}\)_. Since_ \(\mathbf{D}\coloneqq\mathbf{0}_{P\times N_{L}}\in\mathcal{F}_{\mathbf{I}}(\Omega)\) _(by setting all parameters in_ \(\theta\) _equal to zero), the set_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\) _is non-empty. The optimal value of (_11_) is thus upper bounded by_ \(\mathcal{L}(\mathbf{0})\)_. Since the function_ \(\ell(\cdot,y_{i})\) _is coercive for every_ \(y_{i}\) _in the training set, there exists a constant_ \(C\) _(dependent on the training set and the loss) such that minimizing (_11_) on_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\) _or on_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\cap\mathcal{B}(0,C)\) _(with_ \(\mathcal{B}(\mathbf{0},C)\) _the_ \(L^{2}\) _ball of radius_ \(C\) _centered at zero) yields the same infimum. The function_ \(\mathcal{L}\) _is continuous, since each_ \(\ell(\cdot,y_{i})\) _is continuous by assumption, and the set_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\cap\mathcal{B}(0,C)\) _is compact, since it is closed (as an intersection of two closed sets) and bounded (since_ \(\mathcal{B}(0,C)\) _is bounded). As a result there exists a matrix_ \(\mathbf{D}\in\mathcal{F}_{\mathbf{I}}(\Omega)\cap\mathcal{B}(0,C)\) _yielding the optimal value for (_11_). Thus, the parameters_ \(\theta\) _such that_ \(\mathcal{R}_{\theta}(\Omega)=\mathbf{D}\) _is an optimal solution of (_10_)._
2. _Assume that an optimal solution of problem 10 exists for every finite data set_ \(\{(x_{i},y_{i})\}_{i=1}^{P}\)_. Then_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\) _is closed for every_ \(\Omega\)_finite._ _We prove the contraposition of this claim. Assume there exists a finite set_ \(\Omega=\{x_{i}\}_{i=1}^{P}\) _such that_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\) _is not closed. Then, there exists a matrix_ \(\mathbf{D}\in\mathbb{R}^{N_{L}\times P}\) _such that_ \(\mathbf{D}\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\setminus\mathcal{F}_{ \mathbf{I}}(\Omega)\)_. Consider the dataset_ \(\{(x_{i},y_{i})\}_{i=1}^{P}\) _where_ \(y_{i}\in\mathbb{R}^{N_{L}}\) _is the_ \(i\)_th column of_ \(\mathbf{D}\)_. We prove that the infimum value of (_10_) is_ \(V:=\sum_{i=1}^{P}\ell(y_{i},y_{i})\)_. Indeed, since_ \(\mathbf{D}\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\)_, there exists a sequence_ \(\{\theta_{k}\}_{k\in\mathbb{N}}\) _such that_ \(\lim_{k\rightarrow\infty}\mathcal{R}_{\theta_{k}}(\Omega)=\mathbf{D}\)_. Therefore, by continuity of_ \(\ell(\cdot,y_{i})\)_, we have:_ \[\lim_{k\rightarrow\infty}\mathcal{L}(\theta_{k})=\sum_{i=1}^{P}\lim_{k \rightarrow\infty}\ell(\mathcal{R}_{\theta_{k}}(x_{i}),y_{i})=\sum_{i=1}^{P} \ell(y_{i},y_{i})=V.\]

_Moreover, the infimum cannot be smaller than_ \(V\) _because the_ \(i\)_th summand is at least_ \(\ell(y_{i},y_{i})\) _(due to the assumption on_ \(\ell\) _in Proposition_ 3.1_). Therefore, the infimum value is indeed_ \(V\)_. Since we assume that_ \(y\) _is the only minimizer of_ \(y^{\prime}\mapsto\ell(y^{\prime},y)\)_, this value can be achieved only if there exists a parameter_ \(\theta\in\mathbf{I}\) _such that_ \(\mathcal{R}_{\theta}(\Omega)=\mathbf{D}\)_. This is impossible due to our choice of_ \(\mathbf{D}\) _which does not belong to_ \(\mathcal{F}_{\mathbf{I}}(\Omega)\)_. We conclude that with our constructed data set_ \(\mathcal{D}\)_, an optimal solution does not exist for (_10_)._ 

### Proof of Lemma 3.2

The proof of Lemma 3.2 (and thus, as discussed in the main text, of Theorem 3.1) use four technical lemmas. Lemma B.1 is proved in Appendix C.1 since it involves Theorem 4.1. The other lemmas are proved right after the proof of Lemma 3.2.

**Lemma B.1**.: _If \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\backslash\mathcal{L}_{\mathbf{ I}}\subseteq\mathbb{R}^{N_{L}\times N_{0}}\) then the function \(f:x\mapsto f(x):=\mathbf{A}x\) satisfies \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\setminus\mathcal{F}_{ \mathbf{I}}(\Omega)\) for every subset \(\Omega\) of \(\mathbb{R}^{N_{0}}\) that is bounded with non-empty interior._

**Lemma B.2**.: _Consider \(\Omega=\{x_{i}\}_{i=1}^{P}\) a finite subset of \(\mathbb{R}^{d}\)and \(\Omega^{\prime}=[-B,B]^{d}\) such that \(\Omega\subseteq\Omega^{\prime}\). If \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega^{\prime})}\) (under the topology induced by \(\|\cdot\|_{\infty}\)), then \(\mathbf{D}:=\left[f(x_{1})\ldots f(x_{P})\right]\in\overline{\mathcal{F}_{ \mathbf{I}}(\Omega)}\)._

**Lemma B.3**.: _Consider \(\mathcal{R}_{\theta}\), the realization of a ReLU neural network with parameter \(\theta\in\mathbf{I}\). This function is continuous and piecewise linear. On the interior of each piece, its Jacobian matrix is constant and satisfies \(\mathbf{J}\in\mathcal{L}_{\mathbf{I}}\)._

**Lemma B.4**.: _For \(p,N\in\mathbb{N}\), consider the following set of points (a discretized grid for \([0,1]^{N}\)):_

\[\Omega=\Omega_{p}^{N}=\left\{\left(\frac{i_{1}}{p},\ldots,\frac{i_{N}}{p} \right)\mid 0\leq i_{j}\leq p,i_{j}\in\mathbb{N},\forall 1\leq j\leq N\right\}.\]

_If \(H\in\mathbb{N}\) satisfies \(p\geq 3NH\), then for any collection of \(H\) hyperplanes, there exists \(x\in\Omega_{p}^{N}\)such that the elementary hypercube whose vertices are of the form_

\[\left\{x+\left(\frac{i_{1}}{p},\ldots,\frac{i_{N}}{p}\right)\mid i_{j}\in\{0,1 \}\ \forall 1\leq j\leq N\right\}\subseteq\Omega_{p}^{N}\]

_lies entirely inside a polytope delimited by these hyperplanes._

We are now ready to prove Lemma 3.2.

Proof of Lemma 3.2.: Since \(\mathcal{L}_{\mathbf{I}}\) is not closed, there exists a matrix \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\setminus\mathcal{L}_{\mathbf{I}}\), and we consider \(f(x)\coloneqq\mathbf{A}x\). Setting \(p\coloneqq 3N_{0}4^{\sum_{i=1}^{L-1}N_{i}}\) we construct \(\Omega\) as the grid:

\[\Omega=\left\{\left(\frac{i_{1}}{p},\ldots,\frac{i_{N_{0}}}{p}\right)\mid 0 \leq i_{j}\leq p,i_{j}\in\mathbb{N},\;\forall 1\leq j\leq N_{0}\right\},\]

so that the cardinality of \(\Omega=\left\{x_{i}\right\}_{i=1}^{P}\) is \(P:=(p+1)^{N_{0}}\). Similar to the sketch proof, consider \(\mathbf{D}:=\left[f(x_{1}),f(x_{2}),\ldots,f(x_{P})\right]\). Our goal is to prove that \(\mathbf{D}\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\setminus\mathcal{F} _{\mathbf{I}}(\Omega)\).

First, notice that \(\mathbf{D}\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\) as an immediate consequence of Lemma B.2 and Lemma B.1.

It remains to show that \(\mathbf{D}\notin\mathcal{F}_{\mathbf{I}}(\Omega)\). We proceed by contradiction, assuming that there exists \(\theta\in\mathcal{N}_{\mathbf{I}}\) such that \(\mathcal{R}_{\theta}(\Omega)=\mathbf{D}\).

To show the contradiction, we start by showing that, as a consequence of Lemma B.4 there exists \(x\in\Omega\) such that the hypercube whose vertices are the \(2^{N_{0}}\) points

\[\left\{x+\left(\frac{i_{1}}{p},\ldots,\frac{i_{N_{0}}}{p}\right)\mid i_{j}\in \{0,1\},\forall 1\leq j\leq N_{0}\right\}\subseteq\Omega,\] (12)

lies entirely inside a linear region \(\mathcal{P}\) of the continuous piecewise linear function \(\mathcal{R}_{\theta}\)[1]. Denote \(K=2^{\sum_{i=1}^{L}N_{i}}\) a bound on the number of such linear regions, see e.g. [24]. Each frontier between a pair of linear regions can be completed into a hyperplane, leading to at most \(H=K^{2}\) hyperplanes. Since \(p=3N_{0}K^{2}\geq 3N_{0}H\), by Lemma B.4 there exists \(x\in\Omega\) such that the claimed hypercube lies entirely inside a polytope delimited by these hyperplanes. As this polytope is itself included in some linear region \(\mathcal{P}\) of \(\mathcal{R}_{\theta}\), this establishes our intermediate claim.

Now, define \(v_{0}\coloneqq x\) and \(v_{i}\coloneqq x+(1/p)\mathbf{e}_{i},i\in\llbracket N_{0}\rrbracket\) where \(\mathbf{e}_{i}\) is the \(i\)th canonical vector. Denote \(\mathbf{P}\in\mathbb{R}^{N_{L}\times N_{0}}\) the matrix such that the restriction of \(\mathcal{R}_{\theta}\) to the piece \(\mathcal{P}\) is \(f_{\mathcal{P}}(x)=\mathbf{P}x+\mathbf{b}\). Since \(\mathbf{P}\) is the Jacobian matrix of \(\mathcal{R}_{\theta}\) in the linear region \(\mathcal{P}\), we deduce from Lemma B.3 that \(\mathbf{P}\in\mathcal{L}_{\mathbf{I}}\). Since the points \(v_{i}\) belong to the hypercube which is both included in \(\mathcal{P}\) and in \(\Omega\) we have for each \(i\):

\[\mathbf{P}(v_{0}-v_{i}) =f_{\mathcal{P}}(v_{0})-f_{\mathcal{P}}(v_{i})\] \[=\mathcal{R}_{\theta}(v_{0})-\mathcal{R}_{\theta}(v_{i})\] \[=f(v_{0})-f(v_{i})\] \[=\mathbf{A}(v_{0}-v_{i}).\]

where the third equality follows from the definition of \(\mathbf{D}\) and the fact that we assume \(\mathcal{R}_{\theta}(\Omega)=\mathbf{D}\). Since \(v_{0}-v_{i}=e_{i}/p,i=1,\ldots,n\) are linearly independent, we conclude that \(\mathbf{P}=\mathbf{A}\). This implies \(\mathbf{A}\in\mathcal{L}_{\mathbf{I}}\), hence the contradiction. This concludes the proof. 

We now prove the intermediate technical lemmas.

Proof of Lemma b.2.: Since \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega^{\prime})}\), there exists a sequence \(\{\theta_{k}\}_{k\in\mathbb{N}}\) such that:

\[\lim_{k\to\infty}\sup_{x\in\Omega^{\prime}}\|f(x)-\mathcal{R}_{\theta_{k}}(x) \|=0\]

Denoting \(\mathbf{D}_{k}\coloneqq\left[\mathcal{R}_{\theta_{k}}(x_{1})\ldots\mathcal{R} _{\theta_{k}}(x_{r})\right]\), since \(x_{i}\in\Omega\subseteq\Omega^{\prime}\), \(i=1,\ldots,P\), it follows that \(\mathbf{D}_{k}\) converges to \(\mathbf{D}\). Since \(\mathbf{D}_{k}\in\mathcal{F}_{\mathbf{I}}(\Omega)\) by construction, we get that \(\mathbf{D}\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\). 

Proof of Lemma b.3.: For any \(\theta\in\mathbf{I}\), \(\mathcal{R}_{\theta}\) is a continuous piecewise linear function since it is the realization of a ReLU neural network [1]. Consider \(\mathcal{P}\) a linear region of \(\mathcal{R}_{\theta}\) with non-empty interior. The Jacobian matrix of \(\mathcal{P}\) has the following form [28, Lemma 9]:

\[\mathbf{J}=\mathbf{W}_{L}\mathbf{D}_{L-1}\mathbf{W}_{L-1}\mathbf{D}_{L-2}\ldots \mathbf{D}_{1}\mathbf{W}_{1}\]

where \(\mathbf{D}_{i}\) is a binary diagonal matrix (diagonal matrix whose coefficients are either one or zero). Since \(\mathsf{supp}(\mathbf{D}_{i}\mathbf{W}_{i})\subseteq\mathsf{supp}(\mathbf{W}_{ i})\subseteq I_{i}\), we have: \(\mathbf{J}=\mathbf{W}_{L}\prod_{i=1}^{L-1}(\mathbf{D}_{i}\mathbf{W}_{i})\in \mathcal{L}_{I}\).

Proof of Lemma b.4.: Every edge of an elementary hypercube can be written as:

\[\left(x,x+\frac{1}{p}\mathbf{e}_{i}\right),x\in\Omega_{p}^{N}\]

where \(\mathbf{e}_{i}\) is the \(i\)th canonical vector, \(1\leq i\leq N\). The points \(x\) and \(x+(1/p)\mathbf{e}_{i}\) are two _endpoints_. Note that in this proof we use the notation \((a,b)\) to denote the line segment whose endpoints are \(a\) and \(b\). By construction, \(\Omega_{p}^{N}\) contains \(p^{N}\) such elementary hypercubes. Given a collection of \(H\) hyperplanes, we say that an elementary hypercube is an _intersecting hypercube_ if it does not lie entirely inside a polytope generated by the hyperplanes, meaning that there exists a hyperplane that _intersects_ at least one of its edges. More specifically, an edge and a hyperplane intersect if they have _exactly_ one common point. We exclude the case where there are more than two common points since that implies that the edge lies completely in the hyperplane. The edges that are intersected by at least one hyperplane are called _intersecting edges_. Note that a hypercube can have intersecting edges, but it may not be an intersecting one. A visual illustration of this idea is presented in Figure 3.

Formally, a hyperplane \(\{w^{\top}x+b=0\}\) for \(w\in\mathbb{R}^{N}\) and \(b\in\mathbb{R}\) intersects an edge \((x,x+\frac{1}{p}\mathbf{e}_{i})\) if:

\[\begin{cases}(w^{\top}x+b)\left[w^{\top}(x+\frac{1}{p}\mathbf{e}_{i})+b\right] \leq 0\\ \text{and}\\ w^{\top}x+b\neq 0\text{ or }w^{\top}(x+\frac{1}{p}\mathbf{e}_{i})+b\neq 0 \end{cases}\] (13)

We further illustrate these notions in Figure 4. We emphasize that according to Equation (13), \(\ell_{3}\) in Figure 4 does not intersect any edge _along its direction_.

Clearly, the number of intersecting hypercubes is upper bounded by the number of intersecting edges. The rest of the proof is devoted to showing that this number is strictly smaller than \(p^{N}\)_if \(p\geq 3NH\), as this will imply the existence of at least one non-intersecting hypercube.

Figure 4: Illustration of intersecting hypercubes and hyperplanes in \(\mathbb{R}^{2}\).

Figure 3: Illustration of definitions in \(\mathbb{R}^{2}\): a) an intersecting hypercube with two intersecting edges; b) _not_ an intersecting hypercube, but it has two intersecting edges; c) _not_ an intersecting hypercube and it only has two intersecting edges (not three according to our definitions: the bottom edge is _not_ intersecting).

To estimate the _maximum_ number of intersecting edges, we analyze the _maximum_ number of edges that a given hyperplane can intersect. For a fixed index \(1\leq i\leq N\), we count the number of edges of the form \((x,x+\frac{1}{p}\mathbf{e}_{i})\) intersected by a single hyperplane. The key observation is: if we fix all the coordinates of \(x\) except the \(i\)th one, then the edges \((x,x+\frac{1}{p}\mathbf{e}_{i})\) form a line in the ambient space. Among those edges, there are at most _two_ intersecting edges with respect to the given hyperplane. This happens only when the hyperplane intersects an edge at one of its endpoints (e.g., the hyperplane \(\ell_{2}\) and the second vertical line in Figure 4). In total, for each \(1\leq i\leq N\) and each given hyperplane, there are at most \(2(p+1)^{N-1}\) intersecting edges of the form \((x,x+\frac{1}{p}\mathbf{e}_{i})\). For a given hyperplane, there are thus at most \(2N(p+1)^{N-1}\) intersecting edges in total (since \(i\in\llbracket N\rrbracket\)). Since the number of hyperplanes is at most \(H\), there are at most \(2NH(p+1)^{N-1}\) intersecting edges, and this quantity also bounds the number of intersecting cubes as we have seen. With the assumption \(p\geq 3NH\), we conclude by proving that \(p^{N}>2NH(p+1)^{N-1}\). Indeed, we have:

\[\frac{2NH(p+1)^{N-1}}{p^{N}} =\frac{2NH}{p}\left(\frac{p+1}{p}\right)^{N-1}=\frac{2NH}{p} \left(1+\frac{1}{p}\right)^{N-1}<\frac{2NH}{p}\left(1+\frac{1}{p}\right)^{NH}\] \[\leq\frac{2NH}{3NH}\left(1+\frac{1}{3NH}\right)^{NH}\leq\frac{2e ^{1/3}}{3}\approx 0.93<1\]

where we used that \((1+1/n)^{n}\leq e\approx 2.71828\), the Euler number. 

### Proof of Theorem 3.4

Proof.: We denote \(\mathbf{X}=\left[x_{1},\ldots,x_{P}\right]\in\mathbb{R}^{N_{0}\times P}\), the matrix representation of \(\Omega\). Our proof has three main steps:

Step 1:We show that we can reduce the study of the closedness of \(\mathcal{F}_{\mathbf{I}}(\Omega)\) to that of the closedness of a union of subsets of \(\mathbb{R}^{P}\), associated to the vectors \(\mathbf{W}_{2}\). To do this, we prove that for any element \(f\in\mathcal{F}_{\mathbf{I}}(\Omega)\), there exists a set of parameters \(\theta\in\mathcal{N}_{\mathbf{I}}\) such that the matrix of the second layer \(\mathbf{W}_{2}\) belongs to \(\{-1,0,1\}^{1\times N_{1}}\) (since we assume \(N_{2}=1\)). This idea is reused from the proof of [1, Theorem 4.1].

For \(\theta:=\{(\mathbf{W}_{i},\mathbf{b}_{i})_{i=1}^{2}\}\in\mathcal{N}_{\mathbf{ I}}\), the function \(\mathcal{R}(\theta)\) has the form:

\[\mathcal{R}_{\theta}(x)=\mathbf{W}_{2}\sigma(\mathbf{W}_{1}x+\mathbf{b}_{1})+ \mathbf{b}_{2}=\sum_{i=1}^{N_{1}}\mathbf{w}_{2,i}\sigma(\mathbf{w}_{1,i}x+ \mathbf{b}_{1,i})+\mathbf{b}_{2}\]

where \(\mathbf{w}_{1,i}=\mathbf{W}_{1}[i,:]\in\mathbb{R}^{1\times N_{0}},\mathbf{w}_ {2,i}=\mathbf{W}_{2}[i]\in\mathbb{R},\mathbf{b}_{1,i}=\mathbf{b}[i]\in\mathbb{R}\). Moreover, if \(w_{2,i}\) is different from zero, we have:

\[\mathbf{w}_{2,i}\sigma(\mathbf{w}_{1,i}x+\mathbf{b}_{1})=\frac{\mathbf{w}_{2, i}}{|\mathbf{w}_{2,i}|}\sigma(|\mathbf{w}_{2,i}|\mathbf{w}_{1,i}x+| \mathbf{w}_{2,i}|\mathbf{b}_{1,i}).\]

In that case, one can assume that \(\mathbf{w}_{2,i}\) can be equal to either \(-1\) or \(1\). Thus, we can assume \(\mathbf{w}_{2,i}\in\{\pm 1,0\}\). For a vector \(\mathbf{v}\in\{-1,0,1\}^{1\times N_{1}}\), we define:

\[F_{\mathbf{v}}=\{[\mathcal{R}_{\theta}(x_{1}),\ldots,\mathcal{R}_{\theta}(x_{ P})]\mid\theta\in\mathcal{N}_{\mathbf{I},\mathbf{v}}\}\] (14)

where \(\mathcal{N}_{\mathbf{I},\mathbf{v}}\subseteq\mathcal{N}_{\mathbf{I}}\) is the set of \(\theta=\{(\mathbf{W}_{i},\mathbf{b}_{i})_{i=1}^{2}\}\) with \(\mathbf{W}_{2}=\mathbf{v}\in\{0,1\}^{1\times N_{1}}\), i.e., in words, \(F_{\mathbf{v}}\) is the image of \(\Omega\) through the function \(\mathcal{R}_{\theta},\theta\in\mathcal{N}_{\mathbf{I},\mathbf{v}}\).

Define \(\mathbb{V}:=\{\mathbf{v}\mid\mathsf{supp}(\mathbf{v})\subseteq I_{2}\}\cap\{0, \pm 1\}^{1\times N_{1}}\). Clearly, for \(\mathbf{v}\in\mathbb{V}\), \(F_{\mathbf{v}}\subseteq\mathcal{F}_{\mathbf{I}}(\Omega)\). Therefore,

\[\bigcup_{\mathbf{v}\in\mathbb{V}}F_{\mathbf{v}}\subseteq\mathcal{F}_{\mathbf{I }}(\Omega).\]

Moreover, by our previous argument, we also have:

\[\mathcal{F}_{\mathbf{I}}(\Omega)\subseteq\bigcup_{\mathbf{v}\in\mathbb{V}}F_{ \mathbf{v}}.\]

Therefore,

\[\mathcal{F}_{\mathbf{I}}(\Omega)=\bigcup_{\mathbf{v}\in\mathbb{V}}F_{\mathbf{v}}.\]Step 2:Using the first step, to prove that \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed, it is sufficient to prove that \(F_{\mathbf{v}}\) is closed, \(\forall\mathbf{v}\in\mathbb{V}\). This can be accomplished by further decomposing \(F_{\mathbf{v}}\) into smaller closed sets. We denote \(\theta^{\prime}\) the set of parameters \(\mathbf{W}_{1},\mathbf{b}_{1}\) and \(\mathbf{b}_{2}\). In the following, only the parameters of \(\theta^{\prime}\) are varied since \(\mathbf{W}_{2}\) is now fixed to \(\mathbf{v}\).

Due to the activation function \(\sigma\), for a given data point \(x_{j}\in\Omega\), we have:

\[\sigma(\mathbf{W}x_{j}+\mathbf{b}_{1})=\mathbf{D}_{j}(\mathbf{W}x_{j}+ \mathbf{b}_{1})\] (15)

where \(\mathbf{D}_{j}\in\mathcal{D}\), the set of binary diagonal matrices, and its diagonal coefficients \(\mathbf{D}_{j}[i,i]\) are determined by:

\[\mathbf{D}_{j}[i,i]=\begin{cases}0&\text{if }\mathbf{W}[i,:]x_{j}+\mathbf{b}_{1 }[i]\leq 0\\ 1&\text{if }\mathbf{W}[i,:]x_{j}+\mathbf{b}_{1}[i]\geq 0\end{cases}.\] (16)

Note that \(\mathbf{D}_{j}[i,i]\) can take both values \(0\) or \(1\) if \(\mathbf{W}[i,:]x_{j}+\mathbf{b}_{1}[i]=0\). We call the matrix \(\mathbf{D}_{j}\) the activation matrix of \(x_{j}\). Therefore, for (15) to hold, the \(N_{1}\) constraints of the form (16) must hold simultaneously. It is important to notice that all these constraints are linear w.r.t. \(\theta^{\prime}\). We denote \(\mathbf{z}\) a vectorized version of \(\theta^{\prime}\) (i.e., we concatenate all coefficients whose indices are in \(I_{1}\) of \(\mathbf{W}\) and \(\mathbf{b}_{1},\mathbf{b}_{2}\) into a long vector), and we write all the constraints in (15) in a compact form:

\[\mathcal{A}(\mathbf{D}_{j},x_{j})\mathbf{z}\leq\mathbf{0}_{N_{1}}\]

where \(\mathcal{A}(\mathbf{D}_{j},x_{j})\) is a constant matrix that depend on \(\mathbf{D}_{j}\) and \(x_{j}\).

Set \(\theta=(\mathbf{v},\mathbf{z})\). Given that (15) holds, we deduce that:

\[\mathcal{R}_{\theta}(x_{j})=\mathbf{v}\sigma(\mathbf{W}x_{j}+ \mathbf{b}_{1})+\mathbf{b}_{2}=\mathbf{v}\mathbf{D}_{j}(\mathbf{W}x_{j}+ \mathbf{b}_{1})+\mathbf{b}_{2}=\mathcal{V}(\mathbf{D}_{j},x_{j},\mathbf{v}) \mathbf{z}\]

where \(\mathcal{V}(\mathbf{D}_{j},x_{j},\mathbf{v})\) is a constant matrix that depends on \(\mathbf{D}_{j},\mathbf{v},x_{j}\). In particular, \(\mathcal{R}_{\theta}(x_{j})\) is also a linear function w.r.t the parameters \(\mathbf{z}\). Assume that the activation matrices of \((x_{1},\ldots,x_{P})\) are \((\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\), then we have:

\[\mathcal{R}_{\theta}(\Omega)=(\mathcal{V}(\mathbf{D}_{1},x_{1}, \mathbf{v})\mathbf{z},\ldots,\mathcal{V}(\mathbf{D}_{P},x_{P},\mathbf{v}) \mathbf{z})\in\mathbb{R}^{1\times P}.\]

To emphasize that \(\mathcal{R}_{\theta}(\Omega)\) depends linearly on \(\mathbf{z}\), for the rest of the proof, we will write \(\mathcal{R}_{\theta}(\Omega)\) as a vector of size \(P\) (instead of a row matrix \(1\times P\)) as follows:

\[\mathcal{R}_{\theta}(\Omega)=\mathcal{V}(\mathbf{D}_{1},\ldots, \mathbf{D}_{P})\mathbf{z}\quad\text{ where }\quad\mathcal{V}(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})= \begin{pmatrix}\mathcal{V}(\mathbf{D}_{1},x_{1},\mathbf{v})\\ \vdots\\ \mathcal{V}(\mathbf{D}_{P},x_{P},\mathbf{v})\end{pmatrix}.\]

Moreover, to have \((\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\) activation matrices, the parameters \(\mathbf{z}\) need to satisfy:

\[\mathcal{A}(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\mathbf{z}\leq \mathbf{0}_{Q}\]

where \(Q=PN_{1}\) and

\[\mathcal{A}(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})=\begin{pmatrix}\mathcal{A}( \mathbf{D}_{1},x_{1})\\ \vdots\\ \mathcal{A}(\mathbf{D}_{P},x_{P})\end{pmatrix}.\]

Thus, the set of \(\mathcal{R}_{\theta}(\Omega)\)_given_ the activation matrices \((\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\) has the following compact form:

\[F_{\mathbf{v}}^{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})}:=\{\mathcal{V}( \mathbf{D}_{1},\ldots,\mathbf{D}_{P})\mathbf{z}\ |\ \mathcal{A}(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\mathbf{z}\leq\mathbf{0}\}.\]

Clearly, \(F_{\mathbf{v}}^{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})}\subseteq F_{\mathbf{v}}\) since each element is equal to \(\mathcal{R}_{\theta}(\Omega)\) with \(\theta=(\mathbf{v},\mathbf{z})\) for some \(\mathbf{z}\). On the other hand, each element of \(F_{\mathbf{v}}\) is an element of \(F_{\mathbf{v}}^{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})}\) for some \((\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\in\mathcal{D}^{P}\) since the set of activation matrices corresponding to any \(\theta\) is in \(\mathcal{D}^{P}\). Therefore,

\[F_{\mathbf{v}}=\bigcup_{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\in\mathcal{D}^{P }}F_{\mathbf{v}}^{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})}.\]Step 3:Using the previous step, it is sufficient to prove that \(F_{\mathbf{v}}^{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})}\) is closed, for any \(\mathbf{v},(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})\in\mathcal{D}^{P}\). To do so, we write \(F_{\mathbf{v}}^{(\mathbf{D}_{1},\ldots,\mathbf{D}_{P})}\) in a more general form:

\[\{\mathbf{A}\mathbf{z}\mid\mathbf{C}\mathbf{z}\leq\mathbf{y}\}.\] (17)

Therefore, it is sufficient to prove that a set as in Equation (17) is closed. These sets are linear transformations of an intersection of a finite number of half-spaces. Since the intersection of a finite number of halfspaces is _stable_ under linear transformations (cf. Lemma B.5 below), and the intersection of a finite number of half-spaces is a closed set itself, the proof can be concluded. 

**Lemma B.5** (Closure of intersection of half-spaces under linear transformations).: _For any \(\mathbf{A}\in\mathbb{R}^{m\times n},\mathbf{C}\in\mathbb{R}^{\ell\times n}, \mathbf{y}\in\mathbb{R}^{\ell}\), there exists \(\mathbf{C}^{\prime}\in\mathbf{R}^{k\times m},\mathbf{b}^{\prime}\in\mathbf{R} ^{k}\) such that:_

\[\{\mathbf{A}\mathbf{x}\mid\mathbf{C}\mathbf{x}\leq\mathbf{y}\}=\{\mathbf{C}^ {\prime}\mathbf{z}\leq\mathbf{b}^{\prime}\}.\]

Proof.: The proof uses Fourier-Motzkin elimination 4. This method is a quantifier elimination algorithm for linear functions 5. In fact, the LHS can be written as: \(\{\mathbf{t}\mid\mathbf{t}=\mathbf{A}\mathbf{x},\mathbf{C}\mathbf{x}\leq \mathbf{y}\}\), or more generally,

Footnote 4: More detail about this method can be found in this link

Footnote 5: In fact, the algorithm determining the closedness of \(\mathcal{L}_{\mathbf{I}}\) is also a quantifier elimination one, but it can be used in a more general setting: polynomials

\[\left\{\mathbf{t}\mid\exists\mathbf{x}\in\mathbb{R}^{n}\text{ s.t. }\mathbf{B} \begin{pmatrix}\mathbf{x}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}\right\}\subseteq\mathbb{R}^{m}\]

where \(\begin{pmatrix}\mathbf{x}\\ \mathbf{t}\end{pmatrix}\) is the concatenation of two vectors \((\mathbf{x},\mathbf{t})\) and the linear constraints imposed by \(\mathbf{B}\begin{pmatrix}\mathbf{x}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}\) replace the two linear constraints \(\mathbf{C}\mathbf{x}\leq\mathbf{y}\) and \(\mathbf{t}=\mathbf{A}\mathbf{x}\). The idea is to show that:

\[\left\{\mathbf{t}\mid\exists\mathbf{x}\in\mathbb{R}^{n}\text{ s.t. }\mathbf{B} \begin{pmatrix}\mathbf{x}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}\right\}=\left\{\mathbf{t}\mid\exists \mathbf{x}^{\prime}\in\mathbb{R}^{n-1}\text{ s.t. }\mathbf{B}^{\prime}\begin{pmatrix}\mathbf{x}^{\prime}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}^{\prime}\right\}\] (18)

for some matrix \(\mathbf{B}^{\prime}\) and vector \(\mathbf{v}^{\prime}\). By doing so, we reduce the dimension of the quantified parameter \(\mathbf{x}\) by one. By repeating this procedure until there is no more quantifier, we prove the lemma. The rest of this proof is thus devoted to show that \(\mathbf{B}^{\prime},\mathbf{v}^{\prime}\) as in (18) do exist.

We will show how to eliminate the first coordinate of \(\mathbf{x}[1]\). First, we partition the set of linear constraints of LHS of (18) into three groups:

1. \(S_{0}:=\{j\mid\mathbf{B}[j,1]=0\}\): In this case, \(\mathbf{x}[1]\) does not appear in this constraint, there is nothing to do.
2. \(S_{+}:=\{j\mid\mathbf{B}[j,1]>0\}\), for \(j\in S_{+}\), we can rewrite the constraints \(\mathbf{B}[j,:]\begin{pmatrix}\mathbf{x}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}[j]\) as: \[\mathbf{x}[1]\leq\gamma[j]+\sum_{i=2}^{n}\alpha[i]\mathbf{x}[i]+\sum_{i=1}^{ m}\beta[i]\mathbf{t}[i]:=B_{j}^{+}(\mathbf{x}^{\prime},\mathbf{t})\] for some suitable \(\gamma[j],\alpha[i],\beta[i]\) where \(\mathbf{x}^{\prime}\) is the last \((n-1)\) coordinate of the vector \(\mathbf{x}\).
3. \(S_{-}:=\{j\mid\mathbf{B}[j,1]<0\}\): for \(j\in S_{-}\), we can rewrite the constraints \(\mathbf{B}[j,:]\begin{pmatrix}\mathbf{x}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}_{j}\) as: \[\mathbf{x}[1]\geq\gamma[j]+\sum_{i=2}^{n}\alpha[i]\mathbf{x}[i]+\sum_{i=1}^{ m}\beta[i]\mathbf{t}[i]:=B_{j}^{-}(\mathbf{x}^{\prime},\mathbf{t}).\]

For the existence of such \(\mathbf{x}[1]\), it is necessary and sufficient that:

\[B_{k}^{+}(\mathbf{x}^{\prime},\mathbf{t})\geq B_{j}^{-}(\mathbf{x}^{\prime}, \mathbf{t}),\qquad\forall k\in S_{+},j\in S_{-}.\] (19)

Thus, we form the matrix \(\mathbf{B}^{\prime}\) and the vector \(\mathbf{v}^{\prime}\) such that the linear constraints written in the following form:

\[\mathbf{B}^{\prime}\begin{pmatrix}\mathbf{x}^{\prime}\\ \mathbf{t}\end{pmatrix}\leq\mathbf{v}^{\prime}\]

represent all the linear constraints in the set \(S_{0}\) and those in the form of (19). Using this procedure recursively, one can eliminate all quantifiers and prove the lemma.

### Proofs for Lemma 3.3

Since we use tools of real algebraic geometry, this section provides basic notions of real algebraic geometry for readers who are not familiar with this domain. It is organized and presented as in the textbook [2] (with slight modifications to better suit our needs). For a more complete presentation, we refer readers to [2, Chapter 2].

**Definition B.1** (Semi-algebraic sets).: _A semi-algebraic set of \(\mathbb{R}^{n}\) has the form:_

\[\bigcup_{i=1}^{k}\{x\in\mathbb{R}^{n}\mid P_{i}(x)=0\wedge\bigwedge_{j=1}^{ \ell_{i}}Q_{i,j}(x)>0\}\]

_where \(P_{i},Q_{i,j}:\mathbb{R}^{n}\mapsto\mathbb{R}\) are polynomials and \(\wedge\) is the "and" logic._

The following theorem is known as the projection theorem of semi-algebraic sets. In words, the theorem states that: The projection of a semi-algebraic set to a lower dimension is still a semi-algebraic set (of lower dimension).

**Theorem B.6** (Projection theorem of semi-algebraic sets [2, Theorem 2.92]).: _Let \(A\) be a semi-algebraic set of \(\mathbb{R}^{n}\) and define:_

\[B=\{(x_{1},\ldots,x_{n-1})\mid\exists x_{n},(x_{1},\ldots,x_{n-1},x_{n})\in A\}\]

_then \(B\) is a semi-algebraic set of \(\mathbb{R}^{n-1}\)._

Theorem B.6 is a powerful result. Its proof [2, Section \(2.4\)] (which is constructive) shows a way to express \(B\) (in Theorem B.6) by using only the first \(n-1\) variables \((x_{1},\ldots,x_{n-1})\).

Next, we introduce the language of an ordered field and sentence. Readers which are not familiar to the notion of ordered field can simply think of it as \(\mathbb{R}\) and its subring as \(\mathbb{Q}\). Example for fields that is not ordered is \(\mathbb{C}\) (we cannot compare two arbitrary complex number). Therefore, the notion of semi-algebraic set in Definition B.1 (which contains \(Q_{i,j}(x)>0\)) does not make sense when the underlying field is not ordered.

The central definition of the language of \(\mathbb{R}\) is _formula_, an abstraction of semi-algebraic sets. In particular, the definition of formula is recursive: formula is built from atoms - equalities and inequalities of polynomials whose coefficients are in a subring \(\mathbb{Q}\) of \(\mathbb{R}\). It can be also formed by combining with logical connectives "and", "or", and "negation" (\(\wedge,\vee,\neg\)) and existential/universal quantifiers (\(\exists,\forall\)). Formula has variables, which are those of atoms in the formula itself. _Free variables_ of a formula are those which are not preceded by a quantifier (\(\exists,\forall\)). The definitions of a formula and its free variables are given recursively as follow:

**Definition B.2** (Language of the ordered field with coefficients in a ring).: _Consider \(\mathbf{R}\) an ordered field and \(\mathbf{Q}\subseteq\mathbf{R}\) a subring, a formula \(\Phi\) and its set of free variables \(\mathtt{Free}(X)\) are defined recursively as:_

1. _An atom: if_ \(P\in\mathbf{Q}[X]\) _(where_ \(\mathbf{Q}[X]\) _is the set of polynomials with coefficients in_ \(\mathbf{Q}\)_) then_ \(\Phi:=(P=0)\) _(resp._ \(\Phi:=(P>0)\)_) is a formula and its set of free variables is_ \(\mathtt{Free}(\Phi):=\{X_{1},\ldots,X_{n}\}\) _where_ \(n\) _is the number of variables._
2. _If_ \(\Phi_{1}\) _and_ \(\Phi_{2}\) _are formulas, then so are_ \(\Phi_{1}\vee\Phi_{2},\Phi_{1}\wedge\Phi_{2}\) _and_ \(\neg\Phi_{1}\)_. The set of free variables is defined as:_ 1. \(\mathtt{Free}(\Phi_{1}\vee\Phi_{2}):=\mathtt{Free}(\Phi_{1})\cup\mathtt{Free}( \Phi_{2})\)_._ 2. \(\mathtt{Free}(\Phi_{1}\wedge\Phi_{2}):=\mathtt{Free}(\Phi_{1})\cup\mathtt{Free}( \Phi_{2})\)_._ 3. \(\mathtt{Free}(\neg\Phi_{1})=\mathtt{Free}(\Phi_{1})\)_._
3. _If_ \(\Phi\) _is a formula and_ \(X\in\mathtt{Free}(\Phi)\)_, then_ \(\Phi^{\prime}=(\exists X)\Phi\) _and_ \(\Phi^{\prime\prime}=(\forall X)\Phi\) _are also formulas and_ \(\mathtt{Free}(\Phi^{\prime}):=\mathtt{Free}(\Phi)\setminus\{X\}\)_, and_ \(\mathtt{Free}(\Phi^{\prime\prime}):=\mathtt{Free}(\Phi)\setminus\{X\}\)_._

**Definition B.3** (Sentence).: _A sentence is a formula of an ordered field with no free variable._

**Example B.1**.: _Consider two formulas:_

\[\Phi_{1} =\{\exists X_{1},X_{1}^{2}+X_{2}^{2}=0\}\] \[\Phi_{2} =\{\exists X_{1},\exists X_{2},X_{1}^{2}+X_{2}^{2}=0\}\]_While \(\Phi_{1}\) is a normal formula, \(\Phi_{2}\) is a sentence and given an underlying field (\(\mathbb{R}\), for instance), \(\Phi_{2}\) is either correct or not. Here, \(\Phi_{2}\) is correct (since \(X_{1}^{2}+X_{2}^{2}=0\) has a root \((0,0)\)). Nevertheless, if one consider \(\Phi_{2}^{\prime}=\{\exists X_{1},\exists X_{2},X_{1}^{2}+X_{2}^{2}=-1\}\), then \(\Phi_{2}^{\prime}\) is not correct._

An algorithm deciding whether a sentence is correct or not is very tempting since formula and sentence can be used to express many theorems in the language of an ordered field. The proof or disproof will be then given by an algorithm. Such an algorithm does exist, as follow:

**Theorem B.7** (Decision problem [2, Algorithm 11.36]).: _There exists an algorithm to decide whether a given sentence is correct is not with complexity \(O(sd)^{O(1)^{k-1}}\) where \(s\) is the bound on the number of polynomials in \(\Phi\), \(d\) is the bound on the degrees of the polynomials in \(\Phi\) and \(k\) is the number of variables._

A full description of [2, Algorithm 11.36] (quantifier elimination algorithm) is totally out of the scope of this paper. Nevertheless, we will try to explain it in a concise way. The key observation is Theorem B.6, the central result of real algebraic geometry. As discussed right after Theorem B.6, its proof implies that one can replace a sentence by another whose number of quantifiers is reduced by one such that both sentences agree (both are true or false). Applying this procedure iteratively will result into a sentence without any variable (and the remain are only _coefficients in the subring_). We check the correctness of this final sentence by trivially verifying all the equalities/inequalities and obtain the answer for the original one.

Proof of Lemma 3.3.: To decide whether \(\mathcal{L}_{\mathbf{I}}\) is closed or not, it is equivalent to decide if the following _sentence_ (see Definition B.3) is true or false:

\[\exists\mathbf{A}_{\mathbf{\cdot}}(\forall\mathbf{X}_{L},\ldots, \mathbf{X}_{1},P(\mathbf{A},\mathbf{X}_{L},\ldots,\mathbf{X}_{1})>0)\wedge\] \[(\forall\epsilon>0,\exists\mathbf{X}_{L}^{\prime},\ldots,\mathbf{ X}_{1}^{\prime},P(\mathbf{A},\mathbf{X}_{L}^{\prime},\ldots,\mathbf{X}_{1}^{ \prime})-\epsilon<0)\]

where \(P(\mathbf{A},\mathbf{X}_{1},\ldots,\mathbf{X}_{L}):=\sum_{(i,j)}(\mathbf{A}[i,j]-P_{i,j}^{\mathbf{I}}(\mathbf{X}_{L},\ldots,\mathbf{X}_{1}))^{2}\).

This sentence basically asks whether there exists a matrix \(\mathbf{A}\in\overline{\mathcal{F}_{\mathbf{I}}}\setminus\mathcal{F}_{\mathbf{ I}}\) or not. It can be proved that this sentence can be decided to be true or false using real algebraic geometry tools (see Theorem B.7), with a complexity \(O\left((sd)^{C^{k-1}}\right)\) where \(C\) is a universal constant and \(s,d,k\) are the number of polynomials, the maximum degree of the polynomials and the number of variables in the sentence, respectively. Applying this to our case, we have \(s=2,d=2L,k=N_{L}N_{0}+1+2\sum_{\ell=1}^{L}|I_{\ell}|\) (remind that \(|I_{\ell}|\) is the total number of unmasked coefficients of \(\mathbf{X}_{\ell}\)). 

Polynomial algorithm to detect support constraints \(\mathbf{I}=(I,j)\) with non-closed \(\mathcal{L}_{\mathbf{I}}\).

The following sufficient condition for non-closedness is based on the existence in the support constraint of \(2\times 2\) blocks sharing the essential properties of a \(2\times 2\) LU support constraint.

**Lemma B.8**.: _Consider a pair \(\mathbf{I}=(I,J)\in\{0,1\}^{m\times r}\times\{0,1\}^{r\times n}\) of support constraints for the weight matrices of one-hidden-layer neural network. If there exists four indices \(1\leq i_{1},i_{2}\leq m,1\leq j_{1},j_{2}\leq n\) and two indices \(k\neq l,1\leq k,l\leq r\) such that:_

1. _For each pair_ \((i,j)\in\{(i_{1},j_{1}),(i_{1},j_{2}),(i_{2},j_{1})\}\) _we have:_ \[(i,j)\in\mathsf{supp}(I[\mathbf{\cdot};k]J[k\mathbf{\cdot};])\text{ and }(i,j)\notin\mathsf{supp}(I[\mathbf{\cdot};\ell]J[\mathbf{\cdot};]), \forall\ell\neq k.\]
2. _The pair_ \((i_{2},j_{2})\) _belongs to_ \(\mathsf{supp}(I[\mathbf{\cdot};k]J[k\mathbf{\cdot};])\) _and to_ \(\mathsf{supp}(I[\mathbf{\cdot};l]J[l\mathbf{\cdot};])\)_._

_then \(\mathcal{L}_{\mathbf{I}}\) is non-closed._

Proof.: First, it is easy to see that the assumptions of this lemma are equivalent to those of [18, Theorem 4.20] since \(\mathsf{supp}(I[\mathbf{\cdot};k]J[k\mathbf{\cdot};])\) is precisely the \(k\)th rank-one support of the pair \((I,J)\)[18, Definition 3.1]. Without loss of generality, one can assume that \(i_{1},j_{1}=1,i_{2},j_{2}=2\) and \(k=1,l=2\). We will prove that \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\setminus\mathcal{L}_{\mathbf{I}}\) where

\[\mathbf{A}:=\begin{pmatrix}\mathbf{A}^{\prime}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}\end{pmatrix}\in\mathbb{R}^{m\times n},\text{ with }\mathbf{A}^{\prime}:=\begin{pmatrix}0&1\\ 1&0\end{pmatrix}\in\mathbb{R}^{2\times 2}.\]

This can be shown in two steps:1. Proof that \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\): For any \(\epsilon>0\), consider two factors: \[\mathbf{X}_{\epsilon}=\begin{pmatrix}\mathbf{X}_{\epsilon}^{\prime}&\mathbf{0} \\ \mathbf{0}&\mathbf{0}\end{pmatrix},\mathbf{Y}_{\epsilon}=\begin{pmatrix}\mathbf{ Y}_{\epsilon}^{\prime}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}\end{pmatrix}\] where \(\mathbf{X}_{\epsilon}^{\prime},\mathbf{Y}_{\epsilon}^{\prime}\in\mathbb{R}^ {2\times 2}\) respect the support constraints corresponding to the \(\mathbf{LU}\) architecture. It is not hard to see that such a construction of \((\mathbf{X}_{\epsilon},\mathbf{Y}_{\epsilon})\) satisfies the support constraints \((I,J)\) (due to the assumption of the lemma and the value of indices). Moreover, we also have: \[\|\mathbf{A}-\mathbf{X}_{\epsilon}\mathbf{Y}_{\epsilon}\|_{F}=\|\mathbf{A}^{ \prime}-\mathbf{X}_{\epsilon}^{\prime}\mathbf{Y}_{\epsilon}^{\prime}\|_{F}\] Thus, to have \(\|\mathbf{A}-\mathbf{X}_{\epsilon}\mathbf{Y}_{\epsilon}\|_{F}\leq\epsilon\), it is sufficient to choose a pair of factors \((\mathbf{X}_{\epsilon}^{\prime},\mathbf{Y}_{\epsilon}^{\prime})\) respecting the \(\mathbf{LU}\) architecture of size \(2\times 2\) such that \(\|\mathbf{A}^{\prime}-\mathbf{X}_{\epsilon}^{\prime}\mathbf{Y}_{\epsilon}^{ \prime}\|_{F}\leq\epsilon\). Such a pair exists, since the set of matrices admitting the exact \(\mathbf{LU}\) decomposition is dense in \(\mathbb{R}^{2\times 2}\). This holds for any \(\epsilon>0\). Therefore, \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\).
2. Proof that \(\mathbf{A}\notin\mathcal{L}_{\mathbf{I}}\): Assume there exist a pair of factors \((\mathbf{X},\mathbf{Y})\) whose product \(\mathbf{X}\mathbf{Y}=\mathbf{A}\) and supports are included in \((I,J)\). Due to the assumptions on the pairs \((i_{1},j_{1}),(i_{1},j_{2}),(i_{2},j_{1})\), we must have: \[\begin{cases}\mathbf{X}[1,1]\mathbf{Y}[1,1]&=\mathbf{A}[1,1]=0\\ \mathbf{X}[2,1]\mathbf{Y}[1,1]&=\mathbf{A}[2,1]=1\\ \mathbf{X}[1,1]\mathbf{Y}[1,2]&=\mathbf{A}[1,2]=1.\end{cases}\] It is easy to see that it is impossible. Therefore, \(\mathbf{A}\notin\mathcal{L}_{\mathbf{I}}\). 

Given a pair of support constraints \(\mathbf{I}\), it is possible to check in time polynomial in \(m,r,n\) whether the conditions of Lemma B.8 hold. A brute force algorithm has complexity \(O(m^{2}n^{2}r)\). A more clever implementation with careful book-marking can reduce this complexity to \(O(\min(m,n)mnr)\).

## Appendix C Proofs for results in Section 4

### Proof of Theorem 4.1

In fact, Theorem 4.1 is a corollary of Lemma B.1. Thus, we will give a proof for Lemma B.1 in the following.

Proof of Lemma b.1.: Since \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}}}\backslash\mathcal{L}_{ \mathbf{I}}\subseteq\mathbb{R}^{N_{L}\times N_{0}}\), we have:

1. \(\mathbf{A}\notin\mathcal{L}_{\mathbf{I}}\).
2. There exists a sequence \(\{(\mathbf{X}_{i}^{k})_{i=1}^{L}\}_{k\in\mathbb{N}}\) such that \(\lim_{k\rightarrow\infty}\|\mathbf{X}_{L}^{k}\ldots\mathbf{X}_{1}^{k}-\mathbf{ A}\|=0\) for any norm defined on \(\mathbb{R}^{N_{0}}\).

We will prove that the linear function: \(f(x):=\mathbf{A}x\) satisfies \(f\in\overline{\mathcal{F}_{\mathbf{I}}}\setminus\mathcal{F}_{\mathbf{I}}\) (where \(\overline{\mathcal{F}_{\mathbf{I}}}\) is the closure of \(\mathcal{F}_{\mathbf{I}}\) in \((C^{0}(\Omega),\|\cdot\|_{\infty})\), that is to say \(f\) is not the realization of any neural network but it is the uniform limit of the realizations of a sequence of neural networks). Firstly, we prove that \(f\notin\mathcal{F}_{\mathbf{I}}\). For the sake of contradiction, assume there exists \(\theta=(\mathbf{W}_{i},\mathbf{b}_{i})_{i=1}^{L}\in\mathcal{N}_{\mathbf{I}}\) such that \(\mathcal{R}_{\theta}=f\). Since \(\mathcal{R}_{\theta}\) is the realization of a ReLU neural network, it is a continuous piecewise linear function. Therefore, since \(\Omega\) has non-empty interior, there exist a non-empty open subset \(\Omega^{\prime}\) of \(\mathbb{R}^{d}\) such that \(\Omega^{\prime}\subseteq\Omega\) and \(\mathcal{R}_{\theta}\) is linear on \(\Omega^{\prime}\), i.e., there are \(\mathbf{A}^{\prime}\in\mathbb{R}^{N_{L}\times N_{0}}\), \(\mathbf{b}\in\mathbb{R}^{N_{L}}\) such that \(\mathcal{R}_{\theta}(x)=\mathbf{A}^{\prime}x+\mathbf{b}^{\prime},\forall x\in \Omega^{\prime}\). Since \(f=\mathcal{R}_{\theta}\), we have: \(\mathbf{A}^{\prime}=\mathbf{A}\) and also equal to the Jacobian matrix of \(\mathcal{R}_{\theta}\) on \(\Omega^{\prime}\). Using Lemma B.3 and the fact that \(\mathbf{A}\notin\mathcal{L}_{\mathbf{I}}\), we conclude that \(f\notin\mathcal{F}_{\mathbf{I}}\).

There remains to construct a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}},\theta^{k}=(\mathbf{W}_{i}^{k},\mathbf{b}_{i}^ {k})_{i=1}^{L}\in\mathcal{N}_{\mathbf{I}}\) such that \(\lim_{k\rightarrow\infty}\|\mathcal{R}_{\theta^{k}}-f\|_{\infty}=0\). We will rely on the sequence \(\{(\mathbf{X}_{i}^{k})_{i=1}^{L}\}_{k\in\mathbb{N}}\) for our construction. Given \(k\in\mathbb{N}\) we simply define the weight matrices as \(\mathbf{W}_{i}^{k}=\mathbf{X}_{i}^{k},1\leq i\leq L\). The biases are built recursively. Starting from \(c_{1}^{k}:=\sup_{x\in\Omega}\|\mathbf{W}_{1}^{k}x\|_{\infty}\) and \(\mathbf{b}_{i}^{k}:=c_{1}^{k}\mathbf{1}_{N_{1}}\), we iteratively define for \(2\leq i\leq L-1\):

\[\gamma_{i-1}^{k}(x) :=\mathbf{W}_{i-1}^{k}x+\mathbf{b}_{i-1}\] \[c_{i}^{k} :=\sup_{x\in\Omega}\|\gamma_{i-1}^{k}\circ\ldots\circ\gamma_{1}^{k }(x)\|_{\infty}\] \[\mathbf{b}_{i}^{k} :=c_{i}^{k}\mathbf{1}_{N_{i}}.\]The boundedness of \(\Omega\) ensures that \(c_{i}^{k}\) is well-defined with a finite supremum. For \(i=L\) we define:

\[\mathbf{b}_{L}^{k}=-\sum_{i=1}^{L-1}(\prod_{j=i+1}^{L}\mathbf{W}_{j})\mathbf{b}_{ i}^{k}.\]

We will prove that \(\mathcal{R}_{\theta^{k}}(x)=\left(\mathbf{X}_{L}^{k}\ldots\mathbf{X}_{1}^{k} \right)x,\forall x\in\Omega\). As a consequence, it is immediate that:

\[\lim_{k\to\infty}\|\mathcal{R}_{\theta^{k}}-f\|_{\infty} =\lim_{k\to\infty}\sup_{x\in\Omega}\|\mathcal{R}_{\theta^{k}}(x)- f(x)\|_{2}\] \[\leq\lim_{k\to\infty}\|\mathbf{X}_{L}^{k}\ldots\mathbf{X}_{1}^{k} -\mathbf{A}\|_{2\to 2}\sup_{x\in\Omega}\|x\|_{2}=0\]

where we used that all matrix norms are equivalent and denoted \(\|\cdot\|_{2\to 2}\) the operator norm associated to Euclidean vector norms. Back to the proof that \(\mathcal{R}_{\theta^{k}}(x)=\left(\mathbf{X}_{L}^{k}\ldots\mathbf{X}_{1}^{k} \right)x,\forall x\in\Omega\), due to our choice of \(c_{i}^{k}\), we have for \(2\leq i\leq L-1\):

\[\gamma_{i-1}^{k}\circ\ldots\circ\gamma_{1}^{k}(x)\geq 0,\forall x\in\Omega\]

where \(\geq\) is taken in coordinate-wise manner. Therefore, an easy induction yields:

\[\mathcal{R}_{\theta^{k}}(x) =\gamma_{L}^{k}\circ\sigma\circ\gamma_{L-1}^{k}\circ\ldots\circ \sigma\circ\gamma_{1}^{k}(x)\] \[=\gamma_{L}^{k}\circ\gamma_{L-1}^{k}\ldots\circ\gamma_{1}^{k}(x)\] \[=\mathbf{W}_{L}^{k}(\ldots(\mathbf{W}_{2}^{k}(\mathbf{W}_{1}^{k} x+\mathbf{b}_{1}^{k})+\mathbf{b}_{2}^{k})\ldots)+\mathbf{b}_{L}^{k}\] \[=(\mathbf{X}_{L}^{k}\ldots\mathbf{X}_{1}^{k})x+\sum_{i=1}^{L-1}( \prod_{j=i+1}^{L}\mathbf{W}_{j})\mathbf{b}_{i}^{k}-\sum_{i=1}^{L-1}(\prod_{j=i +1}^{L}\mathbf{W}_{j})\mathbf{b}_{i}^{k}\] \[=(\mathbf{X}_{L}^{k}\ldots\mathbf{X}_{1}^{k})x.\]

### Proof of Theorem 4.2

Given the involvement of Theorem 4.2, we decompose its proof and present it in two subsections: the first one establishes general results that do not use the assumption of Theorem 4.2. The second one combines the established results with the assumption of Theorem 4.2 to provide a full proof.

#### c.2.1 Properties of the limit function of fixed support one-hidden-layer NNs

The main results of this parts are summarized in Lemma C.2 and Lemma C.3. It is important to emphasize that all results in this section do _not_ make any assumption on \(\mathbf{I}\).

We first introduce the following technical results.

**Lemma C.1** (Normalization of the rows of the first layer [26]).: _Consider \(\Omega\) a bounded subset of \(\mathbb{R}^{N_{0}}\). Given any \(\theta=\{(\mathbf{W}_{i},\mathbf{b}_{i})_{i=1}^{2}\}\in\mathcal{N}_{\mathbf{I}}\) and any norm \(\|\cdot\|\) on \(\mathbb{R}^{N_{0}}\), there exists \(\tilde{\theta}:=\{(\tilde{\mathbf{W}}_{i},\tilde{\mathbf{b}}_{i})_{i=1}^{2}\} \in\mathcal{N}_{\mathbf{I}}\) such that the matrix \(\tilde{\mathbf{W}}_{1}\) has unit norm rows, \(\|\tilde{\mathbf{b}}_{1}\|_{\infty}\leq C:=\sup_{x\in\Omega}\sup_{\|u\|\leq 1 }\langle u,x\rangle\) and \(\mathcal{R}_{\theta}(x)=\mathcal{R}_{\tilde{\theta}}(x),\forall x\in\Omega\)._

Proof.: We report this proof for self-completeness of this work. It is _not_ a contribution, as it merely combines ideas from the proof of [26, Lemma D.2] and [26, Theorem 3.8, Steps 1-2].

We first show that for each set of weights \(\theta\in\mathcal{N}_{\mathbf{I}}\) we can find another set of weights \(\theta^{\prime}=\{(\mathbf{W}^{\prime}_{i},\mathbf{b}^{\prime}_{i})_{i=1}^{2}\} \in\mathcal{N}_{\mathbf{I}}\) such that \(\mathcal{R}_{\theta}=\mathcal{R}_{\theta^{\prime}}\) on \(\mathbb{R}^{N_{0}}\) and \(\mathbf{W}^{\prime}_{1}\) has unit norm rows. Note that \(\|\mathbf{b}^{\prime}_{1}\|_{\infty}\) can be larger than \(C\). Indeed, given \(\theta\in\mathcal{N}_{\mathbf{I}}\), the function \(\mathcal{R}_{\theta}\) can be written as: \(\mathcal{R}_{\theta}:x\in\mathbb{R}^{N_{0}}\mapsto\sum_{j=1}^{N_{1}}h_{j}(x)+ \mathbf{b}_{2}\) where \(h_{j}(x)=\mathbf{W}_{2}[:,j]\sigma(\mathbf{W}_{1}[j:,:]x+\mathbf{b}_{1}[j])\) denotes the contribution of the \(j\)th hidden neuron. For hidden neurons corresponding to nonzero rows of \(\mathbf{W}_{1}^{k}\), we can rescale the rows of \(\mathbf{W}_{1}^{k}\), the columns of \(\mathbf{W}_{2}^{k}\) and \(\mathbf{b}_{1}^{k}\) such that the realization of \(h_{j}\) is invariant. This is due to the fact that \(\mathbf{w}_{2}\sigma(\mathbf{w}_{1}^{\top}x+b)=\|\mathbf{w}_{1}\|\mathbf{w}_{2} \sigma((\mathbf{w}_{1}/\|\mathbf{w}_{1}\|)^{\top}x+(b/\|\mathbf{w}_{1}\|))\) for any \(\mathbf{w}_{1}\neq\mathbf{0}\in\mathbb{R}^{N_{0}},\mathbf{w}_{2}\in\mathbb{R} ^{N_{2}},b\in\mathbb{R}\). Neurons corresponding to null rows of \(\mathbf{W}_{1}^{k}\) are handled similarly,in an iterative manner, by setting them to an arbitrary normalized row, setting the corresponding column of \(\mathbf{W}_{2}^{k}\) to zero, and changing the bias \(\mathbf{b}_{2}^{k}\) to keep the function \(\mathcal{R}_{\theta}\) unchanged on \(\mathbb{R}^{N_{0}}\), using that \(\mathbf{w}_{2}\sigma(\mathbf{0}^{\top}x+b)+\mathbf{b}_{2}=\mathbf{0}\sigma( \mathbf{v}^{\top}x+b)+(\mathbf{b}_{2}+\mathbf{w}_{2}\sigma(b))\) for any normalized vector \(\mathbf{v}\in\mathbb{R}^{N_{0}}\). Thus, we obtain \(\theta^{\prime}\) whose matrix of the first layer, \(\mathbf{W}_{1}^{\prime}\), has normalized rows and \(\mathcal{R}_{\theta}=\mathcal{R}_{\theta^{\prime}}\) on \(\mathbb{R}^{N_{0}}\).

To construct \(\tilde{\theta}\) with \(\|\tilde{\mathbf{b}}_{1}\|_{\infty}\leq C\) we see that, by definition of \(C\), if \(\|\mathbf{w}_{1}\|=1\) and \(b\geq C\) then

\[\mathbf{w}_{1}^{\top}x+b\geq-C+b\geq 0,\qquad\forall x\in\Omega.\] (20)

Thus, the function \(\mathbf{w}_{2}\sigma(\mathbf{w}_{1}^{\top}x+b)=\mathbf{w}_{2}(\mathbf{w}_{1}^ {\top}x+b)\) is linear on \(\Omega\) and

\[\mathbf{w}_{2}\sigma(\mathbf{w}_{1}^{\top}x+b)+\mathbf{b}_{2} =\mathbf{w}_{2}(\mathbf{w}_{1}^{\top}x+C)+((b-C)\mathbf{w}_{2}+ \mathbf{b}_{2})\] \[=\mathbf{w}_{2}\sigma(\mathbf{w}_{1}^{\top}x+C)+((b-C)\mathbf{w}_ {2}+\mathbf{b}_{2})\]

Thus, for any hidden neuron with a bias exceeding \(C\), the bias can be saturated to \(C\) by changing accordingly the output bias \(\mathbf{b}_{2}\), keeping the function \(\mathcal{R}_{\theta}\) unchanged _on the bounded domain_\(\Omega\) (but not on the whole space \(\mathbb{R}^{N_{0}}\)). Hidden neurons with a bias \(b\leq-C\) can be similarly modified. Sequentially saturating each hidden neuron yields \(\tilde{\theta}\) which satisfies all conditions of Lemma C.1. 

**Lemma C.2**.: _Consider \(\Omega\) a bounded subset of \(\mathbb{R}^{N_{0}}\), for any \(\mathbf{I}=(I_{2},I_{1})\), given a continuous function \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\), there exists a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}},\theta^{k}=(\mathbf{W}_{i}^{k},\mathbf{b}_{i} ^{k})_{i=1}^{2}\in\mathcal{N}_{\mathbf{I}}\) such that:_

1. _The sequence_ \(\mathcal{R}_{\theta^{k}}\) _admits_ \(f\) _as its uniform limit, i.e.,_ \(\lim_{k\to\infty}\|\mathcal{R}_{\theta^{k}}-f\|_{\infty}=0\)_._
2. _The sequence_ \(\{(\mathbf{W}_{1}^{k},\mathbf{b}_{1}^{k})\}_{k\in\mathbb{N}}\) _has a finite limit_ \((\mathbf{W}_{1}^{\star},\mathbf{b}_{1}^{\star})\) _where_ \(\mathbf{W}_{1}^{\star}\) _has unit norm rows and_ \(\mathsf{supp}(\mathbf{W}_{1}^{\star})\subseteq I_{1}\)_._

Proof.: Given a function \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\), by definition, there exists a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\), \(\theta^{k}\in\mathcal{N}_{\mathbf{I}}\)\(\forall k\in\mathbb{N}\) such that \(\lim_{k\to\infty}\|\mathcal{R}_{\theta^{k}}-f\|_{\infty}=0\). We can assume that \(\mathbf{W}_{1}^{k}\) has normalized rows and \(\mathbf{b}_{1}^{k}\) is bounded using Lemma C.1. We can also assume WLOG that the parameters of the first layer (i.e \(\mathbf{W}_{1}^{k},\mathbf{b}_{1}^{k}\)) have finite limits \(\mathbf{W}_{1}^{\star}\) and \(\mathbf{b}_{1}^{\star}\). Indeed, since both \(\mathbf{W}_{1}^{k}\) and \(\mathbf{b}_{1}^{k}\) are bounded (by construction from Lemma C.1), there exists a subsequence \(\{\varphi_{k}\}_{k\in\mathbb{N}}\) such that \(\mathbf{W}_{1}^{\varphi_{1}^{k}}\) and \(\mathbf{b}_{1}^{\varphi_{k}}\) have finite limits and \(\mathcal{R}_{\theta^{\varphi_{k}}}\to f\) as \(\mathcal{R}_{\theta^{k}}\to f\). Replacing the sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) by \(\{\theta^{\varphi_{k}}\}_{k\in\mathbb{N}}\) yields the desired sequence. Finally, since \(\mathbf{W}_{1}^{\star}=\lim_{k\to\infty}\mathbf{W}_{1}^{\star}\), \(\mathbf{W}_{1}^{\star}\) obviously has normalized rows and \(\mathsf{supp}(\mathbf{W}_{1}^{\star})\subseteq I_{1}\). 

**Definition C.1**.: _Consider \(\Omega\) bounded subset of \(\mathbb{R}^{d}\), a function \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\) and a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) as given by Lemma C.2. We define \((a_{i},b_{i})=(\mathbf{W}_{1}^{\star}[i,:],\mathbf{b}_{1}^{\star}[i])\) the limit parameters of the first layer corresponding to the \(i\)th neuron. We partition the set of neurons into two subsets (one of them may be empty):_

1. _Set of active neurons:_ \(J:=\{i\mid(\exists x\in\Omega,a_{i}x+b_{i}>0)\wedge(\exists x\in\Omega,a_{i}x+b _{i}<0)\}\)_._
2. _Set of non-active neurons:_ \(\bar{J}=\llbracket N_{1}\rrbracket\setminus J\)_._

_For \(i,j\in J\), we write \(i\simeq j\) if \((\mathbf{W}_{1}^{\star}[j,:],\mathbf{b}_{1}^{\star}[j])=\pm(\mathbf{W}_{1}^{ \star}[i,:],\mathbf{b}_{1}^{\star}[i])\). The relation \(\simeq\) is an equivalence relation._

_We define \((J_{\ell})_{\ell=1,\ldots,r}\) the equivalence classes induced by \(\simeq\) and we use \((\alpha_{\ell},\beta_{\ell}):=(a_{i},b_{i})\) for some \(i\in J_{\ell}\) as the representative limit of the \(\ell\)th equivalence class. For \(i\in J_{\ell}\), we have: \((a_{i},b_{i})=\epsilon_{i}(\alpha_{\ell},\beta_{\ell}),\epsilon_{i}\in\{\pm 1\}\). We define \(J_{\ell}^{+}=\{i\in J_{\ell}\mid\epsilon_{i}=1\}\neq\emptyset\) and \(J_{\ell}^{-}=J_{\ell}\setminus J_{\ell}^{+}\)._

_For each equivalence class \(J_{\ell}\), define \(H_{\ell}:=\{x\in\Omega\mid\alpha_{\ell}x+\beta_{\ell}=0\}\) the boundary generated by neurons in \(J_{\ell}\) and the positive (resp. negative) half-space partitioned by \(H_{\ell}\), \(H_{\ell}^{+}:=\{x\in\Omega\mid\alpha_{\ell}x+\beta_{\ell}>0\}\) (resp. \(H_{\ell}^{-}:=\{x\in\Omega\mid\alpha_{\ell}x+\beta_{\ell}<0\}\)). For any \(\epsilon>0\) we also define the open half-spaces \(H_{\ell}^{(\epsilon,+)}:=\{x\in\mathbb{R}^{d}\mid\alpha_{\ell}^{\top}x+\beta_{ \ell}>\epsilon\}\) and \(H_{\ell}^{(\epsilon,-)}:=\{x\in\mathbb{R}^{d}\mid\alpha_{\ell}^{\top}x+\beta_{ \ell}<-\epsilon\}\)._

Definition C.1 groups neurons sharing the same "linear boundary" (or "singular hyperplane" as in [26]). This concept is related to "twin neurons" [28], which also groups neurons with the same active zone. This partition somehow allows us to treat classes independently. Observe also that

\[\mathsf{supp}(\alpha_{\ell})\subseteq\bigcap_{i\in J_{\ell}}I_{1}[i,:],\forall 1 \leq\ell\leq r.\] (21)

**Definition C.2** (Contribution of an equivalence class).: _In the setting of Definition C.1, we define the contribution of the \(i\)th neuron \(1\leq i\leq N_{1}\) (resp. of the \(\ell\)th (\(1\leq\ell\leq r\)) equivalence class) of \(\theta^{k}\) as:_

\[h_{i}^{k} :\mathbb{R}^{N_{0}}\mapsto\mathbb{R}^{N_{2}}:x\mapsto\mathbf{W}_{ 2}^{k}[:,i]\sigma(\mathbf{W}_{1}^{k}[i,:]x+\mathbf{b}_{1}^{k}[i])\,\] \[g_{\ell}^{k} :\mathbb{R}^{N_{0}}\mapsto\mathbb{R}^{N_{2}}:x\mapsto\sum_{i\in J _{\ell}}h_{i}^{k}(x)\.\]

**Lemma C.3**.: _Consider \(\Omega=[-B,B]^{d}\), \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\) and a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) as given by Lemma C.2, and \(\alpha_{\ell},\beta_{\ell},1\leq\ell\leq r,\epsilon_{i},i\in J\) as given by Definition C.1. There exist some \(\gamma_{\ell},\mathbf{b}\in\mathbb{R}^{N_{2}},\mathbf{A}\in\mathbb{R}^{N_{2} \times N_{0}}\) such that:_

\[f(x)=\sum_{i=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{ \ell})+\mathbf{A}x+\mathbf{b}\quad\forall x\in\Omega\] (22) \[\lim_{k\rightarrow\infty}\sum_{i\in J_{\ell}}\epsilon_{i}\mathbf{W }_{2}^{k}[:,i]\mathbf{W}_{1}^{k}[i,:]=\gamma_{\ell}\alpha_{\ell}, \quad\forall 1\leq\ell\leq r\] (23) \[\lim_{k\rightarrow\infty}\sum_{i\in J_{\ell}}\epsilon_{i}\mathbf{ b}_{1}^{k}[i]\mathbf{W}_{2}^{k}[:,i]=\gamma_{\ell}\beta_{\ell}, \quad\forall 1\leq\ell\leq r\] (24) \[\text{supp}(\gamma_{\ell})\subseteq\bigcup_{i\in J_{\ell}}I_{2}[:, i], \quad\forall 1\leq\ell\leq r\] (25)

Proof.: The proof is divided into three parts: We first show that there exist \(\gamma_{\ell},\mathbf{b}\in\mathbb{R}^{N_{2}}\) and \(\mathbf{A}\in\mathbb{R}^{N_{2}\times N_{0}}\) such that Equation (22) holds. The last two parts will be devoted to prove that equations (23) - (25) hold.

1. **Proof of Equation (22):** Our proof is based on a result of [26], which deals with the case of a scalar output (i.e, \(N_{2}=1\)). It is proved in [26, Theorem 3.8, Steps \(3,6,7\)] and states the following:

**Lemma C.4** (Analytical form of a limit function with scalar output [26]).: _In case \(N_{2}=1\) (i.e., output dimension equal to one), consider \(\Omega=[-B,B]^{d}\), a scalar-valued function \(f:\Omega\mapsto\mathbb{R},f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\) and a sequence as given by Lemma C.2, there exist \(\mu\in\mathbb{R}^{N_{0}},\gamma_{\ell},\nu\in\mathbb{R}\) such that:_

\[f(x)=\sum_{\ell=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{ \ell})+\mu^{\top}x+\nu,\quad\forall x\in\Omega\] (26)

Back to our proof, one can write \(f=(f_{1},\dots,f_{N_{2}})\) where \(f_{j}:\Omega\subseteq\mathbb{R}^{N_{0}}\mapsto\mathbb{R}\) is the function \(f\) restricted to the \(j\)th coordinate. Clearly, \(f_{j}\) is also a uniform limit on \(\Omega\) of \(\{\mathcal{R}_{\beta^{k}}\}_{k\in\mathbb{N}}\) for a sequence \(\{\tilde{\theta}^{k}\}_{k\in\mathbb{N}}\) which shares the same \(\mathbf{W}_{1}^{k}\) with \(\{\theta^{k}\}_{k\in\mathbb{N}}\) but \(\tilde{\mathbf{W}}_{2}^{k}\) is the \(j\)th row of \(\mathbf{W}_{2}^{k}\). Therefore, \(\{\tilde{\theta}^{k}\}_{k\in\mathbb{N}}\) also satisfies the assumptions of Lemma C.4, which gives us:

\[f_{j}(x)=\sum_{\ell=1}^{r}\gamma_{\ell,j}\sigma(\alpha_{\ell}x+ \beta_{\ell})+\mu_{j}^{\top}x+\nu_{j},\quad\forall x\in\Omega\]

for some \(\mu_{j}\in\mathbb{R}^{N_{0}},\gamma_{i,j},\nu_{j}\in\mathbb{R}\). Note that \(\alpha_{\ell},\beta_{\ell}\) and \(r\) are not dependent on the index \(j\) since they are defined directly from the considered sequence. Therefore, the function \(f\) (which is the concatenation of \(f_{j}\) coordinate by coordinate) is:

\[f(x)=\sum_{\ell=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{ \ell})+\mathbf{A}x+\mathbf{b},\quad\forall x\in\Omega\]

with \(\gamma_{\ell}=\left(\begin{smallmatrix}\gamma_{i,1}\\ \vdots\\ \gamma_{i,N_{2}}\end{smallmatrix}\right),\mathbf{A}=\left(\begin{smallmatrix} \mu_{1}^{\top}\\ \vdots\\ \mu_{N_{2}}^{\top}\end{smallmatrix}\right),\mathbf{b}=\left(\begin{smallmatrix} \nu_{1}\\ \vdots\\ \nu_{N_{2}}\end{smallmatrix}\right)\).
2. **Proof for Equations** (23)-(24): With the construction of \(\gamma\), we will prove Equation (23) and Equation (24). We consider an arbitrary \(1\leq\ell\leq r\). Denoting \(\Omega^{\circ}\) the interior of \(\Omega\) and \(H_{\ell}:=\{x\in\Omega\mid\alpha_{\ell}x+\beta_{\ell}=0\}\) the hyperplane defined by the input weights and bias of the \(\ell\)-th class ofneurons, we take a point \(x^{\prime}\in(\Omega^{\circ}\cap H_{\ell})\setminus\bigcup_{p\neq\ell}H_{p}\) and a fixed scalar \(r>0\) such that the open ball \(\mathcal{B}(x^{\prime},r)\subseteq\Omega^{\circ}\setminus\bigcup_{p\neq\ell}H_{p}\). Notice that \(x^{\prime}\) is well-defined due to the definition of \(J\) (Definition C.1). In addition, \(r\) also exists because \(\Omega^{\circ}\setminus\bigcup_{p\neq\ell}H_{p}\) is an open set. Thus, there exists two constants \(0<\delta<B\) and \(\epsilon>0\) such that:

1. \(\mathcal{B}(x^{\prime},r)\subseteq[-(B-\delta),B-\delta]^{d}\).
2. For each \(p\neq\ell\), the ball \(\mathcal{B}(x^{\prime},r)\) is either included in the half-space \(H_{p}^{(\epsilon,+)}:=\{x\in\mathbb{R}^{d}\mid\alpha_{p}^{\top}x+\beta_{p}> \epsilon\}\) or in the half-space \(H_{p}^{(\epsilon,-)}:=\{x\in\mathbb{R}^{d}\mid\alpha_{p}^{\top}x+\beta_{p}<-\epsilon\}\).
3. The intersection of \(\mathcal{B}(x^{\prime},r)\) with \(H_{\ell}^{(\epsilon,+)}\) and \(H_{\ell}^{(\epsilon,-)}\) are not empty.

For the remaining of the proof, we will use Lemma C.5, another result taken from [26]. We only state the lemma. Its formal proof can be found in the proof of [26, Theorem \(3.8\), Steps 4-5].

**Lemma C.5** (Affine linear area [26]).: _Given a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) satisfying the second condition of Lemma C.2, we have:_

1. _For any_ \(0<\delta<B\)_, there exists a constant_ \(\kappa_{\delta}\) _such that_ \(\forall i\in\bar{J}\)_,_ \(h_{i}^{k}\) _are affine linear on_ \([-(B-\delta),B-\delta]^{N_{0}}\) _for all_ \(k\geq\kappa_{\delta}\)_._
2. _For any_ \(\epsilon>0\)_, there exists a constant_ \(\kappa_{\epsilon}\) _such that for each_ \(1\leq\ell\leq r\) _and each_ \(i\in J_{\ell}\) _the function_ \(h_{i}^{k}\) _is affine linear on_ \(H_{\ell}^{(\epsilon,+)}\cup H_{\ell}^{(\epsilon,-)}\) _for all_ \(k\geq\kappa_{\epsilon}\)_._

The lemma implies the existence of \(K=\max(\kappa_{\delta},\kappa_{\epsilon})\) such that for all \(k\geq K\), we have:

\[\sum_{p\neq\ell}g_{p}^{k}(x)=\mathbf{B}^{k}x+\nu^{k},\qquad\forall x\in \mathcal{B}(x^{\prime},r),\]

for some \(\mathbf{B}^{k}\in\mathbb{R}^{N_{2}\times N_{0}},\nu^{k}\in\mathbb{R}^{N_{2}}\). Therefore, for \(k\geq K\), we have:

\[\mathcal{R}_{\theta^{k}}(x) =\mathbf{B}^{k}x+\nu^{k}+\sum_{i\in J_{\ell}^{+}}\mathbf{W}_{2}^{ k}[:,i](\mathbf{W}_{1}^{k}[i,:]x+\mathbf{b}_{1}^{k}[i]),\forall x\in \mathcal{B}(x^{\prime},r)\cap H_{\ell}^{(\epsilon,+)}\] \[\mathcal{R}_{\theta^{k}}(x) =\mathbf{B}^{k}x+\nu^{k}+\sum_{i\in J_{\ell}^{-}}\mathbf{W}_{2}^{ k}[:,i](\mathbf{W}_{1}^{k}[i,:]x+\mathbf{b}_{1}^{k}[i]),\forall x\in \mathcal{B}(x^{\prime},r)\cap H_{\ell}^{(\epsilon,-)}.\]

Since we proved that \(f\) has the form Equation (22), there exist \(\mathbf{C}\in\mathbb{R}^{N_{2}\times N_{0}},\mu\in\mathbb{R}^{N_{2}}\) such that

\[f(x) =(\mathbf{C}+\gamma_{\ell}\alpha_{\ell})x+(\mu+\gamma_{\ell} \beta_{\ell}), \forall x\in\mathcal{B}(x^{\prime},r)\cap H_{\ell}^{(\epsilon,+)}\] \[f(x) =\mathbf{C}x+\mu, \forall x\in\mathcal{B}(x^{\prime},r)\cap H_{\ell}^{(\epsilon,-)}\]

As both \(\mathcal{B}(x^{\prime},r)\cap H_{\ell}^{(\epsilon,+)}\) and \(\mathcal{B}(x^{\prime},r)\cap H_{\ell}^{(\epsilon,-)}\) are open sets, and given our hypothesis of uniform convergence of \(\mathcal{R}_{\theta^{k}}\to f\), we obtain,

\[\lim_{k\to\infty}\mathbf{B}^{k}+\sum_{i\in J_{\ell}^{+}}\mathbf{W}_{2}^{k}[:, i]\mathbf{W}_{1}^{k}[i,:] =\mathbf{C}+\gamma_{\ell}\alpha_{\ell}\] (27) \[\lim_{k\to\infty}\mathbf{B}^{k}+\sum_{i\in J_{\ell}^{-}}\mathbf{W }_{2}^{k}[:,i]\mathbf{W}_{1}^{k}[i,:] =\mathbf{C}\] \[\lim_{k\to\infty}\nu^{k}+\sum_{i\in J_{\ell}^{+}}\mathbf{b}_{1}^{ k}[i]\mathbf{W}_{2}^{k}[:,i] =\mu+\gamma_{\ell}\beta_{\ell}\] \[\lim_{k\to\infty}\nu^{k}+\sum_{i\in J_{\ell}^{-}}\mathbf{b}_{1}^{ k}[i]\mathbf{W}_{2}^{k}[:,i] =\mu.\]

Proof for Equation (27) can be found in Appendix C.4. Equations (23) and (24) follow directly from Equation (27).

3. **Proof of Equation (25)**: Since \(\alpha_{\ell}\neq 0\) (remember that \(\|\alpha_{\ell}\|=1\)), this is an immediate consequence of Equation (23) as each vector \(\mathbf{W}_{2}^{k}[:,j]\), \(j\in J_{\ell}\) is supported in \(I_{2}[:,j]\subseteq\cup_{i\in J_{\ell}}I_{2}[:,i]\).

We state an immediate corollary of Lemma C.3, which characterizes the limit of the sequence of contributions \(\{g_{\ell}^{k}\}_{k\in\mathbb{N}}\) of the \(\ell\)th equivalence class with \(|J_{\ell}|=1\).

**Corollary C.1**.: _Consider \(f\in\overline{\mathcal{F}_{\mathbf{I}}([-B,B]^{d})}\) that admits the analytical form in Equation (22), a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) as given by Lemma C.2, and Definition C.1. For all singleton equivalence classes \(J_{\ell}=\{i\},1\leq\ell\leq r\), we have \(\lim_{k\to\infty}\mathbf{W}_{2}^{k}[:,i]=\gamma_{\ell}\) and \(\lim_{k\to\infty}\|h_{\ell}^{k}-\gamma_{\ell}\sigma(\alpha_{\ell}^{\top}x+ \beta_{\ell})\|_{\infty}=0\)._

Proof.: We first prove that \(\mathbf{W}_{2}^{k}[:,i]\) has a finite limit. In fact, applying the second point of Lemma C.3 for \(J_{\ell}=\{i\}\), we have:

\[\lim_{k\to\infty}\mathbf{W}_{2}^{k}[:,i]\mathbf{W}_{1}^{k}[i,:]=\gamma_{\ell} \alpha_{\ell}\]

where \(\gamma_{\ell},\alpha_{\ell}\) are defined in Lemma C.3. Because \(\lim_{k\to\infty}\mathbf{W}_{1}^{k}[i,:]=\alpha_{\ell}\) and \(\|\alpha_{\ell}\|_{2}=1\), it follows that \(\gamma_{\ell}=\lim_{k\to\infty}\mathbf{W}_{2}^{k}[:,i]\). To conclude, since we also have \(\beta_{\ell}=\lim_{k\to\infty}\mathbf{b}_{1}^{k}[i]\), we obtain \(h_{\ell}^{k}(\cdot)=\mathbf{W}_{2}^{k}[\ell,:]\sigma(\mathbf{W}_{1}^{k}[\ell, :]\cdot+\mathbf{b}_{1}^{k}[\ell])\to\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_ {\ell})\) as claimed. 

The nice thing about Corollary C.1 is that the contribution \(g_{\ell}^{k}=h_{\ell}^{k}\) admits a (uniform) limit if \(J_{\ell}=\{i\}\). Moreover, this limit is even implementable by using only the \(i\)th neuron because \(\mathsf{supp}(\alpha_{\ell})\subseteq I_{1}[i,:]\) and \(\mathsf{supp}(\gamma_{\ell})\subseteq I_{2}[:,i]\).

It would be tempting to believe that, for each \(P\in\{\bar{J}\}\cup\{J_{\ell}\mid\ell=1,\ldots,r\}\) the sequence of functions \(\sum_{i\in P}g_{i}^{k}(x)\) must admit a limit (when \(k\) tends to \(\infty\)) and that this limit is implementable using only neurons in \(P\). This would obviously imply that \(\mathcal{F}_{\mathbf{I}}(\Omega)\) is closed. This intuition is however wrong. For non-singleton equivalence class (i.e., for cases _not_ covered by Corollary C.1), the limit function _does not necessarily exist_ as we show in the following example.

**Example C.1**.: _Consider the case where \(\mathbf{N}=(1,3,1)\) and no support constraint, \(\Omega=[-1,1]\), take the sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) which satisfies:_

\[\mathbf{W}_{1}^{k}=\begin{pmatrix}1\\ -1\\ 1\end{pmatrix},\mathbf{b}_{1}^{k}=\begin{pmatrix}0\\ 0\\ 1\end{pmatrix},\mathbf{W}_{2}^{k}=(k\quad-k\quad-k)\,,\mathbf{b}_{2}^{k}=k\]

_Then for \(x\in\Omega\), it is easy to verify that \(\mathcal{R}_{\theta^{k}}=0\). Indeed,_

\[\mathcal{R}_{\theta^{k}}(x) =\sum_{i=1}^{3}\mathbf{W}_{2}^{k}[:,i]\sigma(\mathbf{W}_{1}^{k}[ i,:]+\mathbf{b}_{1}^{k}[i])+\mathbf{b}_{2}^{k}\] \[=k\sigma(x)-k\sigma(-x)-k\sigma(x+1)+k\] \[=k(\sigma(x)-\sigma(-x))-k(x+1)+k\quad(\text{since }x+1\geq 0, \forall x\in\Omega)\] \[=kx-k(x+1)+k=0\]

_Thus, this sequence converges (uniformly) to \(f=0\). Moreover, this sequence also satisfies the assumptions of Lemma C.2. Using the classification in Definition C.1, we have one class equivalence \(J_{1}=\{1,2\}\) and \(\bar{J}=\{3\}\). The function \(g_{1}^{k}(x)=k\sigma(x)-k\sigma(-x)=kx\), however, does not have any limit._

#### c.2.2 Actual proof of Theorem 4.2

Therefore, our analysis cannot treat each equivalence class entirely separately. The last result in this section is about a property of the matrix \(\mathbf{A}\) in Equation (22). This is one of our key technical contributions in this work.

**Lemma C.6**.: _Consider \(\Omega=[-B,B]^{d},f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\) that admits the analytical form in Equation (22), a sequence \(\{\theta^{k}\}_{k\in\mathbb{N}}\) as given by Lemma C.2, then the matrix \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{Y}}}\) where \(\mathbf{I}^{\prime}=(I_{2}[:,S],I_{1}[S:]),S=\bar{J}\cup(\cup_{1\leq\ell\leq r }J_{\ell}^{-})\), \(\bar{J},J_{\ell}^{\pm}\) are defined as in Definition C.1)._

Combining Lemma C.6 and the assumptions of Theorem 4.2, we can prove Theorem 4.2 immediately as follow:

Proof of Theorem 4.2.: Consider \(f\in\overline{\mathcal{F}_{\mathbf{I}}(\Omega)}\), we deduce that there exists a sequence of \(\{\theta^{k}\}_{k\in\mathbb{N}}\) that satisfies the properties of Lemma C.2. This allows us to define \(\bar{J}\) and equivalence classes \(J_{\ell},1\leq\ell\leq r\) as well as \((\alpha_{\ell},\beta_{\ell})\) as in Definition C.1. Using Lemma C.3, we can also deduce an analytical formula for \(f\) as in Equation (22):

\[f(x)=\sum_{i=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{\ell})+\mathbf{A}x +\mathbf{b},\quad\forall x\in\Omega.\]

Finally, Lemma C.6 states that matrix \(\mathbf{A}\) in Equation (22) satisfies: \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}^{\prime}}}\) with \(\mathbf{I}^{\prime}=(I_{2}[:,\dot{S}],I_{1}[S,:])\), where \(S=\bar{J}\cup(\cup_{\ell=1}^{r}J_{\ell}^{-})\). To prove that \(f\in\mathcal{F}_{\mathbf{I}}\), we construct the parameters \(\theta=\{(\mathbf{W}_{i},\mathbf{b}_{i})_{i=1}^{2}\}\) of the limit network as follows:

1. For each \(1\leq\ell\leq r\), choose one index \(j\in J_{\ell}^{+}\) (which is possible since \(J_{\ell}^{+}\) is non-empty). We set: \[(\mathbf{W}_{1}[i,:],\mathbf{W}_{2}[:,i],\mathbf{b}_{1}[i])=\begin{cases}( \alpha_{\ell},\gamma_{\ell},\beta_{\ell})&\text{if $i=j$}\\ (\alpha_{\ell},\mathbf{0},\beta_{\ell})&\text{otherwise}\end{cases}\] (28) This satisfies the support constraint because \(\mathsf{supp}(\alpha_{\ell})\subseteq I_{1}[j,:]\) (by (21)) \(\alpha_{\ell}=\lim_{k\to\infty}\mathbf{W}_{1}^{k}[j,:]\) and \(I_{2}=\mathbf{1}_{N_{2}\times N_{1}}\). This is where we use the first assumption of Theorem 4.2. Without it, \(\mathsf{supp}(\gamma_{\ell})\) might not be a subset of \(I_{2}[:,j]\).
2. For \(i\in S\): Since \(\mathbf{A}\in\overline{\mathcal{L}_{\mathbf{I}^{\prime}}}\) (cf Lemma C.6) and \(\mathcal{L}_{\mathbf{I}^{\prime}}\) is closed (second assumptions of Theorem 4.2), there exist two matrices \(\hat{\mathbf{W}}_{1},\hat{\mathbf{W}}_{2}\) such that: \(\mathsf{supp}(\hat{\mathbf{W}}_{1})\subseteq I_{1}[:,S],\mathsf{supp}(\hat{ \mathbf{W}}_{2})\subseteq I_{2}[S,:]\), and \(\mathbf{A}=\hat{\mathbf{W}}_{2}\hat{\mathbf{W}}_{1}\). We set: \[(\mathbf{W}_{1}[i,:],\mathbf{W}_{2}[:,i],\mathbf{b}_{1}[i])=(\hat{\mathbf{W}}_ {1}[i,:],\hat{\mathbf{W}}_{2}[:,i],C)\] (29) where \(C=\sup_{x\in\Omega}\|\hat{\mathbf{W}}_{1}x\|_{\infty}\). This satisfies the support constraints \(\mathbf{I}\) due to our choice of \(\hat{\mathbf{W}}_{1},\hat{\mathbf{W}}_{2}\). The choice of \(C\) ensures that the function \(h_{i}(x):=\mathbf{W}_{2}[i,:]\sigma(\mathbf{W}_{1}[i,:]x+\mathbf{b}_{1}[i])\) is _linear_ on \(\Omega\).
3. For \(\mathbf{b}_{2}\): Let \(\mathbf{b}_{2}=\mathbf{b}-C\left(\sum_{i\in S}\hat{\mathbf{W}}_{2}[:,i]\right)\) (\(\mathbf{b}\) is the bias in Equation (22)).

Verifying \(\mathcal{R}_{\theta}=f\) on \(\Omega\) is thus trivial since:

\[\mathcal{R}_{\theta}(x) =\sum_{i=1}^{N_{1}}\mathbf{W}_{2}[:,i]\sigma(\mathbf{W}_{1}[i,:]x +\mathbf{b}_{1}[i])+\mathbf{b}_{2}\] \[=\sum_{i\notin S}\mathbf{W}_{2}[:,i]\sigma(\mathbf{W}_{1}[i,:]x +\mathbf{b}_{1}[i])+\sum_{i\in S}\mathbf{W}_{2}[:,i]\sigma(\mathbf{W}_{1}[i,:]x +\mathbf{b}_{1}[i])+\mathbf{b}_{2}\] \[=\sum_{\ell=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{\ell} )+\sum_{j\in S}\hat{\mathbf{W}}_{2}[:,i](\hat{\mathbf{W}}_{1}[i,:]x+C)+ \mathbf{b}-C\left(\sum_{i\in S}\hat{\mathbf{W}}_{2}[:,i]\right)\] \[=\sum_{\ell=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{\ell} )+\hat{\mathbf{W}}_{2}\hat{\mathbf{W}}_{1}x+\mathbf{b}=\sum_{\ell=1}^{r}\gamma_ {\ell}\sigma(\alpha_{\ell}x+\beta_{\ell})+\mathbf{A}x+\mathbf{b}=f.\qed\]

Proof of Lemma c.6.: In this proof, we define \(\Omega_{\delta}^{\circ}=(-B+\delta,B-\delta)^{N_{0}},0<\delta<B\). The choice of \(\delta\) is not important in this proof (any \(0<\delta<B\) will do).

The proof of this lemma revolves around the following idea: We will construct a sequence of functions \(\{f^{k}\}_{k\in\mathbb{N}}\) such that, for \(k\) large enough, \(f^{k}\) has the following analytical form:

\[f^{k}(x)=\sum_{\ell=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{\ell})+ \mathbf{A}^{k}x+\mathbf{b}^{k},\forall x\in\Omega_{\delta}^{\circ}\] (30)

and \(\lim_{k\to\infty}f^{k}(x)=f(x)\ \forall x\in\Omega\setminus(\cup_{\ell=1}^{r}H_{\ell})\) (or equivalently \(f^{k}\) converges pointwise to \(f\) on \(\Omega\setminus(\cup_{\ell=1}^{r}H_{\ell})\)) and \(\mathbf{A}^{k}\) admits a factorization into two factors \(\mathbf{A}^{k}=\mathbf{X}^{k}\mathbf{Y}^{k}\) satisfying \(\mathsf{supp}(\mathbf{X}^{k})\subseteq I_{2}[:,S],\mathsf{supp}(\mathbf{Y}^{k}) \subseteq I_{1}[S,:]\), so that \(\mathbf{A}^{k}\in\mathcal{L}_{\mathbf{I}^{\prime}}\). Comparing Equation (22) and Equation (30), we deduce that the sequence of affine functions \(\mathbf{A}^{k}x+\mathbf{b}^{k}\) converges pointwise to the affine function \(\mathbf{A}x+\mathbf{b}\) on the open set \(\Omega_{\delta}^{\circ}\setminus(\cup_{\ell=1}^{r}H_{\ell})\). Therefore, \(\lim_{k\to\infty}\mathbf{A}^{k}=\mathbf{A}\) by Lemma C.7, hence the conclusion.

The rest of this proof is devoted to the construction of \(f^{k}=\mathcal{R}_{\tilde{\theta}^{k}}\) where \(\tilde{\theta}^{k}\in\mathcal{N}_{\mathbf{N}}\) are parameters of a neural network of the same dimension as those in \(\mathcal{N}_{\mathbf{I}}\) but only _partially_ satisfying the support constraint \(\mathbf{I}\). To guarantee that \(f^{k}\) converges pointwise to \(f\), we construct \(\tilde{\theta}^{k}\) based on \(\theta^{k}\) and harness their relation.

**Choice of parameters.** We set \(\tilde{\theta}^{k}=\{(\tilde{\mathbf{W}}_{i}^{k},\tilde{\mathbf{b}}_{i}^{k}) _{i=1}^{2}\}\in\mathcal{N}_{\mathbf{N}}\) where \(\tilde{\mathbf{W}}_{2}^{k}\in\mathbb{R}^{N_{2}\times N_{1}},\tilde{\mathbf{W}} _{1}^{k}\in\mathbb{R}^{N_{1}\times N_{0}}\) are defined as follows, where we use \(C^{k}:=\sup_{x\in\Omega}\|\mathbf{W}_{1}^{k}x\|_{\infty}\):

* For inactive neurons \(i\in\bar{J}\), we simply set \((\tilde{\mathbf{W}}_{1}^{k}[:,i],\tilde{\mathbf{W}}_{2}^{k}[i,:],\tilde{ \mathbf{b}}_{1}^{k}[i])=(\mathbf{W}_{1}^{k}[:,i],\mathbf{W}_{2}^{k}[i,:], \mathbf{b}_{1}^{k}[i])\).
* For each equivalence class of active neurons \(1\leq\ell\leq r\), we choose some \(j_{\ell}\in J_{\ell}^{+}\) (note that \(J_{\ell}^{+}\) is non-empty due to Definition C.1) and set the parameters \((\tilde{\mathbf{W}}_{2}^{k}[:,i],\tilde{\mathbf{W}}_{1}^{k}[i,:],\mathbf{b}_{1 }[i]),i\in J_{\ell}\) as: \[(\tilde{\mathbf{W}}_{1}^{k}[i,:],\tilde{\mathbf{W}}_{2}^{k}[:,i],\tilde{ \mathbf{b}}_{1}^{k}[i])=\begin{cases}(\mathbf{W}_{1}^{k}[i,:],\mathbf{W}_{2}^ {k}[:,i],C^{k}),&\quad\forall j\in J_{\ell}^{-}\\ (\mathbf{W}_{1}^{k}[i,:],\mathbf{0},C^{k}),&\quad\forall i\in J_{\ell}^{+} \setminus\{j_{\ell}\}\\ (\alpha_{\ell},\gamma_{\ell},\beta_{\ell}),&\quad i=j_{\ell}\end{cases}\] (31) For \(i\in J_{\ell}\setminus\{j_{\ell}\}\), we clearly have: \(\text{supp}(\tilde{\mathbf{W}}_{1}^{k}[i,:])\subseteq I_{1}[i,:]\) and \(\text{supp}(\tilde{\mathbf{W}}_{2}^{k}[:,i])\subseteq I_{2}[:,i]\). The \(j_{\ell}\)-th column of \(\tilde{\mathbf{W}}_{2}^{k}\) is the only one that does not necessarily satisfy the support constraint, as \(\text{supp}(\gamma_{\ell})\nsubseteq I_{2}[:,j_{\ell}]\) in general.
* Finally, the output bias \(\mathbf{b}_{2}^{k}\) is set as: \[\tilde{\mathbf{b}}_{2}^{k}\coloneqq\mathbf{b}_{2}^{k}+\sum_{\ell=1}^{r}\underbrace {\sum_{i\in J_{\ell}^{-}}(\mathbf{b}_{1}^{k}[i]-C^{k})\mathbf{W}_{2}^{k}[:,i]} _{=:\xi_{\ell}^{k}}\] (32) **Proof that \(f^{k}\coloneqq R_{\tilde{\theta}^{k}}\) converges pointwise to \(f\) on \(\Omega\setminus(\cup_{\ell=1}^{r}H_{\ell})\).** We introduce notations analog to Definition C.2: for every \(x\in\mathbb{R}^{N_{0}}\) we define: \[\tilde{h}_{i}^{k}(x)=\tilde{\mathbf{W}}_{2}^{k}[:,i]\sigma(\tilde{\mathbf{W}} _{1}^{k}[i,:]x+\tilde{\mathbf{b}}_{1}^{k}[i]),\quad i=1,\ldots,N_{1};\qquad \tilde{g}_{\ell}^{k}(x)=\sum_{i\in J_{\ell}}\tilde{h}_{i}^{k}(x),\quad\ell=1, \ldots,r\] By construction \[\tilde{h}_{i}^{k}=h_{i}^{k},\quad\forall i\in\bar{J},\;\forall k,\] (33) and we further explicit the form of \(\tilde{h}_{i}^{k},i\in J_{\ell}\) for \(x\in\Omega\) (but _not_ on \(\mathbb{R}^{N_{0}}\)) as: \[\tilde{h}_{i}^{k}(x)=\begin{cases}\mathbf{W}_{2}^{k}[:,i](\mathbf{W}_{1}^{k}[i,:]x+C^{k}),&\forall i\in J_{\ell}^{-}\\ 0,&\forall i\in J_{\ell}^{+}\setminus\{j_{\ell}\}\\ \gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{\ell}),&\quad i=j_{\ell}\end{cases},\] (34) We justify our formula in Equation (34) as follow:

1. For \(i\in J_{\ell}^{-}\): since \(C^{k}=\sup_{x\in\Omega}\|\mathbf{W}_{1}^{k}x\|_{\infty}\) by construction, \(\tilde{\mathbf{W}}_{1}^{k}[i,:]x+\mathbf{b}_{1}^{k}[i]=\mathbf{W}_{1}^{k}[i, :]x+\mathbf{b}_{1}^{k}[i]\geq 0\). The activation \(\sigma\) acts simply as an identity function.
2. For \(i\in J_{\ell}^{+}\): Because we choose \(\tilde{\mathbf{W}}_{2}^{k}[:,i]=\mathbf{0}\).
3. For \(i=j_{\ell}\): Obvious due to the construction in Equation (31).

Given \(x\in\Omega\setminus(\cup_{\ell=1}^{r}H_{\ell})\), we now prove that this construction ensures that for each \(\ell\in\{1,\ldots,r\}\)

\[\lim_{k\to\infty}(\tilde{g}_{\ell}^{k}(x)-g_{\ell}^{k}(x)+\xi_{\ell}^{k})=0.\] (35)

[MISSING_PAGE_EMPTY:31]

Using (37) we obtain for \(k\) large enough

\[\tilde{g}_{\ell}^{k}(x)-g_{\ell}^{k}(x)+\xi_{\ell}^{k}=\sum_{i\in J_ {\ell}}\left(\tilde{h}_{i}^{k}(x)-h_{i}^{k}(x)\right)+\xi_{\ell}^{k}\] \[=\sum_{i\in J_{\ell}^{-}}\mathbf{W}_{2}^{k}[:,i](\mathbf{W}_{1}^{ k}[i,:]x+C^{k})-\sum_{i\in J_{\ell}^{+}}\mathbf{W}_{2}^{k}[:,i](\mathbf{W}_{1}^{ k}[i,:]x+\mathbf{b}_{1}^{k}[i])+\gamma_{\ell}(\alpha_{\ell}x+\beta_{\ell})+\xi_{ \ell}^{k}\] \[\stackrel{{(\ref{eq:2})}}{{=}}\Big{(}\sum_{i\in J_ {\ell}^{-}}\mathbf{W}_{2}^{k}[:,i]\mathbf{W}_{1}^{k}[i,:]-\sum_{j\in J_{\ell}^{ +}}\mathbf{W}_{2}^{k}[:,i]\mathbf{W}_{1}^{k}[i,:]+\gamma_{\ell}\alpha_{\ell} \Big{)}x\] \[\qquad+\Big{(}\underbrace{\xi_{\ell}^{k}+\sum_{i\in J_{\ell}^{-} }\mathbf{W}_{2}^{k}[:,i]\mathbf{C}^{k}}_{\sum_{i\in J_{\ell}^{-}}\mathbf{W}_{2 }^{k}[:,i]\mathbf{b}_{1}^{k}[i]}-\sum_{i\in J_{\ell}^{+}}\mathbf{W}_{2}^{k}[:, i]\mathbf{b}_{1}^{k}[i]+\gamma_{\ell}\beta_{\ell}\Big{)}\]

where in the last line we used the expression of \(\xi_{\ell}^{k}\) from (32). Due to Equations (23) and (24) it follows that \(\lim_{k\to\infty}\tilde{g}_{\ell}^{k}(x)-g_{\ell}^{k}(x)+\xi_{\ell}^{k}=0\), \(\forall x\in H_{\ell}^{+}\).

Thus combining both cases, we conclude that \(\lim_{k\to\infty}\tilde{g}_{\ell}^{k}(x)-g_{\ell}^{k}(x)+\xi_{\ell}^{k}=0, \forall x\notin H_{\ell}\), as desired.

**Proof of the expression (30) with \(\mathbf{A}^{k}\in\mathcal{L}_{\boldsymbol{I}}\) for large enough \(k\)**. From (34), we first deduce that

\[f^{k}(x)=\sum_{i=1}^{N_{1}}\tilde{h}_{i}^{k}(x)+\tilde{\mathbf{b}}_{2}^{k}= \sum_{\ell=1}^{r}\gamma_{\ell}\sigma(\alpha_{\ell}x+\beta_{\ell})+\sum_{i\in S }\tilde{h}_{i}^{k}(x)+\tilde{\mathbf{b}}_{2}^{k},\quad\forall x\in\mathbb{R}^ {N_{0}}.\]

where we recall that \(S\coloneqq\bar{J}\cup(\cup_{1\leq\ell\leq r}J_{\ell}^{-})\). There only remains to show that, for \(k\) large enough, we have \(\sum_{i\in S}\tilde{h}_{i}^{k}(x)=\mathbf{A}^{k}x+\mathbf{b}^{k}\) for every \(x\)_in the restricted domain_\(\Omega_{\delta}^{\circ}\), where \(\mathbf{A}^{k}\in\mathcal{L}_{\boldsymbol{I}^{\prime}}\) and \(\mathbf{b}^{k}\in\mathbb{R}^{N_{2}}\). Note that for \(i\in J_{\ell}\), our construction assures that \(\tilde{h}_{i}^{k}\) is affine on \(\Omega\). Moreover, in the restricted domain \(\Omega_{\delta}^{\circ}\), for \(k\geq\kappa_{\delta}\) large enough, \(\tilde{h}_{i}^{k},i\in\bar{J}\) also behave like affine functions (cf Lemma C.5). Therefore,

\[\sum_{i\in S}\tilde{h}_{i}^{k}(x)=\Big{(}\sum_{i\in S}\delta_{i}^{k}\hat{ \mathbf{W}}_{2}^{k}[:,i]\hat{\mathbf{W}}_{1}^{k}[i,:]\Big{)}x+\mathbf{c}^{k}, \quad\forall x\in\Omega_{\delta}^{\circ},k\geq\kappa_{\delta}\]

for some vector \(\mathbf{c}^{k}\) and binary scalars \(\delta_{i}^{k}\). In fact, \(\delta_{i}^{k}=0\) if \(i\in\bar{J}^{-}:=\{j\in\bar{J}\mid\mathbf{W}_{1}^{*}[j,:]x+\mathbf{b}_{1}^{*}[ j]\leq 0,\forall x\in\Omega\}\) and \(\delta_{k}^{i}=1\) otherwise. Thus, one chooses \(\mathbf{A}^{k}=\sum_{i\in S}\delta_{i}^{k}\hat{\mathbf{W}}_{2}^{k}[:,i]\hat{ \mathbf{W}}_{1}^{k}[i,:],\mathbf{b}^{k}=\mathbf{c}^{k}\) and the construction is complete. This construction allows us to write \(\mathbf{A}^{k}=\hat{\mathbf{W}}_{2}^{k}\hat{\mathbf{W}}_{1}^{k}\) with:

\[\hat{\mathbf{W}}_{1}^{k} =\tilde{\mathbf{W}}_{1}^{k}[S,:]\] \[\hat{\mathbf{W}}_{2}^{k} =\hat{\mathbf{W}}_{2}^{k}[:,S]\texttt{diag}(\{\nu_{i}^{k}\mid i=1, \ldots,N_{1}\})\]

where \(\texttt{diag}(\{\nu_{i}^{k}\mid i=1,\ldots,N_{1}\})\in\mathbb{R}^{N_{1}\times N _{1}}\) is a diagonal matrix, \(\nu_{i}^{k}=\delta_{i}^{k}\) for \(i\in S\) and \(0\) otherwise. It is also evident that \(\texttt{supp}(\hat{\mathbf{W}}_{2}^{k}[:,S])\subseteq I_{2}[:,S]\), \(\texttt{supp}(\hat{\mathbf{W}}_{1}^{k}[S,:])\subseteq I_{1}[S,:]\). (since the multiplication with a diagonal matrix does not increase the support of a matrix). This concludes the proof. 

### Proof for Corollary 4.2

Proof.: The proof is inductive on the number of hidden neurons \(N_{1}\):

1. Basic case \(N_{1}=1\): Consider \(\theta:=\{(\mathbf{W}_{i},\mathbf{b}_{i})_{i=1}^{2}\}\in\mathcal{N}_{ \boldsymbol{I}}\), the function \(\mathcal{R}_{\theta}\) has the form: \[\mathcal{R}_{\theta}(x)=\mathbf{w}_{2}\sigma(\mathbf{w}_{1}^{\top}x+\mathbf{b}_ {1})+\mathbf{b}_{2}\] where \(\mathbf{w}_{1}=\mathbf{W}_{1}[1,:]\in\mathbb{R}^{N_{0}},\mathbf{w}_{2}= \mathbf{W}_{2}[1,1]\in\mathbb{R}\). There are two possibilities: 1. \(I_{2}=\emptyset\): then \(\mathbf{w}_{2}=0\), \(\mathcal{F}_{I}\) is simply a set of constant functions on \(\Omega\), which is closed.

2. \(I_{2}=\{(1,1)\}\): We have \(I_{2}=\mathbf{1}_{1\times N_{1}}\), which makes the first assumption of Theorem 4.2 satisfied. To check that the second assumption of Theorem 4.2 also holds, we consider all the possible non-empty subsets \(S\) of \([\![1]\!]\); there is only one non-empty subset of \(I_{2}\), which is \(S=[\![1]\!]\). In that case, \(\mathcal{L}_{\mathbf{1}_{S}}=\{\mathbf{W}\in\mathbb{R}^{1\times N_{0}}\mid \mathsf{supp}(\mathbf{W})\subseteq I_{1}\}\), which is closed (since \(\mathcal{L}_{\mathbf{1}_{S}}\) is isomorphic to \(\mathbb{R}^{[I_{1}]}\)). The result thus follows using Theorem 4.2.
2. Assume the conclusion of the theorem holds for all \(1\leq N_{1}\leq k\) (and any \(N_{0}\geq 1\)). We need to prove the result for \(N_{1}=k+1\). Define \(H=\{i\mid I_{2}[1,i]=1\}\) the set of hidden neurons that are allowed to be connected to the output via a nonzero weight. Consider two cases: 1. If \(|H|\leq k\), we have \(\mathcal{F}_{\mathbf{I}}=\mathcal{F}_{\mathbf{I}_{H}}\), which is closed due to the induction hypothesis. 2. If \(H=[\![k+1]\!]\), we can apply Theorem 4.2. Indeed, since \(I_{2}=\mathbf{1}_{1\times N_{1}}\), the first condition of Theorem 4.2 is satisfied. In addition, for any non-empty \(S\subseteq[\![N_{1}]\!]\), define \(\mathcal{H}:=\cup_{i\in S}I[i,:]\subseteq[\![N_{0}]\!]\) the union of row supports of \(I_{1}[S,:]\). It is easy to verify that \(\mathcal{L}_{\mathbf{1}_{S}}\) is isomorphic to \(\mathbb{R}^{|\mathcal{H}|}\), which is closed. As such, Theorem 4.2 can be applied. 

### Other technical lemmas

**Lemma C.7** (Convergence of affine function).: _Let \(\Omega\) be a non-empty interior subset \(\mathbb{R}^{n}\). If the sequence \(\{f^{k}\}_{k\in\mathbb{N}},f^{k}:\mathbb{R}^{n}\mapsto\mathbb{R}^{m}:x\mapsto \mathbf{A}^{k}x+\mathbf{b}^{k}\) where \(\mathbf{A}^{k}\in\mathbb{R}^{m\times n},\mathbf{b}^{k}\in\mathbb{R}^{m}\) converges pointwise to a function \(f\) on \(\Omega\), then \(f\) is affine (i.e., \(f=\mathbf{A}x+\mathbf{b}\) for some \(\mathbf{A}\in\mathbb{R}^{m\times n},\mathbf{b}\in\mathbb{R}^{m}\)). Moreover, \(\lim_{k\to\infty}\mathbf{A}^{k}=\mathbf{A}\) and \(\lim_{k\to\infty}\mathbf{b}_{k}=\mathbf{b}\)._

Proof.: Consider \(x_{0}\in\Omega^{\prime}\), an open subset of \(\Omega\) (\(\Omega^{\prime}\) exists since \(\Omega\) is a non-empty interior subset of \(\mathbb{R}^{n}\)). Define \(g^{k}(x)=f^{k}(x)-f^{k}(x_{0})\) and \(g(x)=f(x)-g(x_{0})\). The function \(g^{k}\) is linear and \(g^{k}\) converges pointwise to \(g\) on \(\Omega\) (and thus, on \(\Omega^{\prime}\)). We first prove that \(g\) is linear. Indeed, for any \(x,y\in\Omega,\alpha,\beta\in\mathbb{R}\) such that \(\alpha x+\beta y\in\Omega\), we have:

\[g(\alpha x+\beta y) =\lim_{k\to\infty}g^{k}(\alpha x+\beta y)\] \[=\lim_{k\to\infty}\alpha g^{k}(x)+\beta g^{k}(y)\] \[=\alpha\lim_{k\to\infty}g^{k}(x)+\beta\lim_{k\to\infty}g^{k}(y)\] \[=\alpha g(x)+\beta g(y)\]

Therefore, there must exist \(\mathbf{A}\in\mathbb{R}^{m\times n}\) such that \(g(x)=\mathbf{A}x\). Choosing \(\mathbf{b}:=g(x_{0})\), we have \(f(x)=g(x)+g(x_{0})=\mathbf{A}x+\mathbf{b}\). Moreover, since \(\Omega^{\prime}\) is open, there exists a positive \(r\) such that the ball \(\mathcal{B}(x,r)\subseteq\Omega^{\prime}\). Choosing \(x_{i}=x_{0}+(r/2)\mathbf{e}_{i}\) with \(\mathbf{e}_{i}\) the \(i\)th canonical vector, we have:

\[\lim_{k\to\infty}g^{k}(x_{i})=\lim_{k\to\infty}(r/2)\mathbf{A}^{k}\mathbf{e}_ {i}=(r/2)\mathbf{A}e_{i},\]

or, equivalently, the \(i\)th column of \(\mathbf{A}\) is the limit of the sequence generated by the \(i\)th column of \(\mathbf{A}^{k}\). Repeating this argument for all \(1\leq i\leq n\), we have \(\lim_{k\to\infty}\mathbf{A}^{k}=\mathbf{A}\). This also implies \(\lim_{k\to\infty}\mathbf{b}^{k}=\mathbf{b}\) immediately. 

## Appendix D Closedness does not imply the best approximation property

Since we couldn't find any source discussing the fact that closedness does not imply the BAP, we provide an example to show this fact.

Consider \(C^{0}([-1,1])\) the set of continuous functions on the interval \([-1,1]\), equipped with the norm \(\sup\|f\|_{\infty}=\max_{x\in[-1,1]}|f(x)|\), and define \(S\), the subset of all functions \(f\in C^{0}([-1,1])\) such that:

\[\int_{0}^{1}f\,dx-\int_{-1}^{0}f\,dx=1\]

It is easy to verify that \(S\) is closed. We show that the constant function \(f=0\) does not have a projection in \(S\) (i.e., a function \(g\in S\) such that \(\|f-g\|_{\infty}=\inf_{h\in S}\|f-h\|_{\infty}\)).

First we observe that since \(f=0\), we have \(\|f-h\|_{\infty}=\|h\|_{\infty}\) for each \(h\in S\), and we show that \(\inf_{h\in S}\|f-h\|_{\infty}\geq 1/2\). Indeed, for \(h\in S\) we have:

\[1=\int_{0}^{1}h\,dx-\int_{-1}^{0}h\,dx\leq\left|\int_{0}^{1}h\,dx\right|+\left| \int_{-1}^{0}h\,dx\right|\leq 2\|h\|_{\infty}=2\|f-h\|_{\infty}.\] (38)

Secondly, we show a sequence of \(\{h_{n}\}_{k\in\mathbb{N}}\) such that \(h_{n}\in S\) and \(\lim_{n\to\infty}\|h_{n}\|_{\infty}=1/2\). Consider the odd function \(h_{n}\) (i.e. \(h_{n}(x)=-h_{n}(-x)\)) such that:

\[h_{n}(x)=\begin{cases}c_{n},&x\in[1/n,1]\\ nc_{n}x&x\in[0,1/n)\end{cases}\]

where \(c_{n}=n/(2n-1)\). It is evident that \(h_{n}\in S\) because:

\[\int_{0}^{1}h_{n}\,dx-\int_{-1}^{0}h_{n}\,dx=2\int_{0}^{1}h_{n}\,dx =2\left(\int_{0}^{1/n}h_{n}\,dx+\int_{1/n}^{1}h_{n}\,dx\right)\] \[=2\left(\frac{c_{n}}{2n}+\frac{c_{n}(n-1)}{n}\right)=\frac{c_{n} (2n-1)}{n}=1\]

Moreover, we also have \(\lim_{n\to\infty}\|h_{n}\|_{\infty}=\lim_{n\to\infty}c_{n}=1/2\).

Finally, we show that \(1/2\) cannot be attained. By contradiction, assume that there exists \(g\in S\) such that \(\|f-g\|_{\infty}=1/2\), i.e., as we have seen, \(\|g\|_{\infty}=1/2\). Using Equation (38), the equality will only hold if \(g(x)=1/2\) in \([0,1]\) and \(g(x)=-1/2\) in \([-1,0]\). However, \(g\) is not continuous, a contradiction.