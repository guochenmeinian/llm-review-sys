# Minimum Description Length and Generalization Guarantees for Representation Learning

 Milad Sefidgaran \({}^{\dagger}\), Abdellatif Zaidi \({}^{\dagger}\)\({}^{\dagger}\), Piotr Krasnowski\({}^{\dagger}\)

\({}^{\dagger}\) Paris Research Center, Huawei Technologies France

\({}^{\dagger}\) Universite Gustave Eiffel, France

{milad.sefidgaran2,piotr.g.krasnowski}@huawei.com, abdellatif.zaidi@univ-eiffel.fr

###### Abstract

A major challenge in designing efficient statistical supervised learning algorithms is finding representations that perform well not only on available training samples but also on unseen data. While the study of representation learning has spurred much interest, most existing such approaches are heuristic; and very little is known about theoretical generalization guarantees. For example, the information bottleneck method seeks a good generalization by finding a minimal description of the input that is maximally informative about the label variable, where minimality and informativeness are both measured by Shannon's mutual information.

In this paper, we establish a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm in terms of the "Minimum Description Length" (MDL) of the labels or the latent variables (representations). Rather than the mutual information between the encoder's input and the representation, which is often believed to reflect the algorithm's generalization capability in the related literature but in fact, falls short of doing so, our new bounds involve the "multi-letter" relative entropy between the distribution of the representations (or labels) of the training and test sets and a fixed prior. In particular, these new bounds reflect the structure of the encoder and are not vacuous for deterministic algorithms. Our compressibility approach, which is information-theoretic in nature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two essential ingredients: block-coding and lossy-compression. The latter allows our approach to subsume the so-called _geometrical compressibility_ as a special case. To the best knowledge of the authors, the established generalization bounds are the first of their kind for Information Bottleneck type encoders and representation learning. Finally, we partly exploit the theoretical results by introducing a new _data-dependent_ prior. Numerical simulations illustrate the advantages of well-chosen such priors over classical priors used in IB.

## 1 Introduction

A key performance indicator of stochastic learning algorithms is their capability to generalize, i.e., perform equally well on training and unseen data. However, designing learning algorithms with good generalization guarantees remains a major challenge. A popular approach involves learning an encoder part and a decoder part. The encoder aims at generating a suitable representation of the input (referred to as "latent variable") by extracting relevant features from the input data. The decoder aims at optimizing the performance of the learning task for the given training dataset, known as empirical risk minimization (ERM), based only on the learned latent variables. This approach is grounded on the idea that performing ERM on the latent variable (instead of the input itself) prevents overfitting.

**Information Bottleneck.** Several approaches have attempted to formalize the concept of a "good representation" [SST10, AFDM17, VDOV\({}^{+}\)17, DKSV20]. Perhaps, the most prominent is the _information bottleneck (IB)_ method which was first introduced in [TPB00] and then extended in several directions [SST10, AFDM17, AZ19, KTW19, Fis20, RGTS20, KASK22]. The IB approach was deemed useful to analyze deep neural networks [SZT17, ZEAS20, GP20, Gei21] and guide their training process. Forinstance, in supervised learning tasks the IB seeks representations that capture "minimum" information about the input data (shoots for good generalization power) while providing "maximum" information about the label (shoots for small empirical risk), where the amounts of captured and provided information are measured using Shannon mutual information. More precisely, denoting by \(Y\) the label variable and by \(X\) the input data, the IB latent variable \(U\) is chosen so as to maximize the mutual information \(I(U;Y)\) while minimizing the mutual information \(I(U;X)\). Equivalently, this can be formulated as maximizing the Lagrange cost

\[I(U;Y)-\beta I(U;X),\]

where \(\beta\geq 0\) designates a Lagrange multiplier. Let \((X^{m},U^{m})\) denote a vector, or block, of \(m\) independent and identically distributed (i.i.d.) realizations \((X_{i},U_{i}),i=1,\ldots,m\), of discrete variables \((X,U)\sim P_{X,U}\). Fix a distribution \(P_{\hat{U}}\) such \(P_{X,\hat{U}}=P_{X,U}\). Essentially by means of the rate-distortion theoretic _covering lemma_[2], it is easy to see that one can get a suitable description \(\hat{U}\) of \(X\) using only \(I(U;X)\) bits per symbol. Precisely, generating a codebook of roughly \(l_{m}\approx 2^{mI(U;X)}\) vectors \(\hat{u}^{m}[j]\in\mathcal{U}^{m},j=1,\ldots,m\), all drawn i.i.d. according to \(P_{U}\), the covering lemma states that for large \(m\) there exists at least one index \(j\) for which the empirical distribution of \((X^{m},\hat{u}^{m}[j])\) is arbitrary close to \(P_{X,U}\) in some suitable sense. That is, the empirical distributions of \((X^{m},U^{m})\) and \((X^{m},\hat{u}^{m}[j])\) are close. Hence, an "equivalent" version \(\hat{U}^{m}\) of \(U^{m}\) can be described with roughly \(mI(U;X)\) bits. Intuitively, this makes a connection between the mutual information \(I(U;X)\) and the concept of _Minimal Description Length_ (MDL) [11, 1] when one considers MDL of the latent variables, not that of model parameters [23, 1]. Moreover, it is now relatively well known that there exists a connection between the generalization error of a learning model and the MDL of the parameters of that model, see, e.g., [1, 1, 1]. A notable work [1] has considered MDL of the predicted labels of a super-sample of training and test data.

**Critics to IB.** The aforementioned connections perhaps formed a belief that \(I(U;X)\) is closely related to the generalization performance of representation learning algorithms. This belief, however, is controversial and conflicting evidence has been reported [1]. For instance, using the term \(I(U;X)\) as a regularizer has been criticized for four main reasons [1, 1, 1, 1]: **i.** The few existing upper bounds on the generalization error which involve (among other terms) the mutual information \(I(U;X)\) reported in [23] and the very recent and concurrent work [1] are not convincing. For example, the bound of [23] holds only when the alphabets of the input and latent spaces are finite; and, in that case, it states that for any \(\delta>0\), with probability \(1-\delta\) it holds that: \(\mathrm{gen}(S,W)\leq\mathcal{O}\Big{(}\frac{\log(n)}{n}\Big{)}\sqrt{I(U;X)}+C _{\delta}\), where \(\mathrm{gen}(S,W)\) is the generalization error of model \(W\), \(C_{\delta}:=\mathcal{O}\big{(}|\mathcal{U}|/\sqrt{n}\big{)}\), \(|\mathcal{U}|\) is the size of the latent space and \(n\) is the size of the training dataset. For reasonable setups, however, the term \(C_{\delta}\) dominates and their bound becomes vacuous [1, 2]. The bound reported in [1] suffers similar shortcomings - For further details on this, see Appendix B.2. **ii.** Experimental evidence shows dependence of the generalization error on the so-called _geometrical compression_ rather than on \(I(U;X)\)[1]. Geometrical compression occurs when the latent variables are concentrated around a limited number of clusters. See [1, Fig. 2] for a visual representation. **iii.**\(I(U;X)\) is invariant to bijection; and, as such, it does not favor learning algorithms/representations with simple decision boundaries and does not reflect the "structure" or "simplicity" of the encoder/decoder. Please refer to [1, Section 4.3] for a detailed discussion and examples. **iv.** Finally, for deterministic algorithms \(I(U;X)\) can take large or even infinite values, especially for continuous variables or high-dimensional data, hence limiting the usefulness of IB [1, 1].

**MDL/Compressibility.** Several works have studied MDL/compressibility to establish generalization bounds. In this context, key is the "Occam's razor" principle [1, 1, 2] which, e.g., for binary classification tasks, states that if the labels of the training data \(S=\{Z_{1},\ldots,Z_{n}\}\), \(Z_{i}=(X_{i},Y_{i})\) predicted by a model \(W\) can be described by \(k\ll n\) number of bits, then the learned model \(W\) has a good generalization performance. There exist three closely related lines of work that use different means to describe the dataset labels: **i.** The first describes the predicted labels via the hypothesis (parameter models). It includes the works of [1, 1, 2]. Recently, it was shown in [1, 1] that information-theoretic bounds [1, 1, 2], PAC-Bayes bounds [1] and intrinsic dimension-based approaches [2] also fall into this category. The proof uses so-called "fixed-size" [1] and "variable-size" [1] compressibility frameworks introduced therein. **ii.** The second formulates the minimal description of labels as follows: if a learning algorithm can predict all labels of the training data \(S\) by using only samples from a small subset of \(S\), then the learning algorithm is guaranteed to generalize well. This second approach was initiated by [1] and followed by others including [1, 1, 1, 2]. **iii.** The third, initiated by [1], deals directly with the compression of the predicted labels. This approach is shown to be related to the previous two lines, and also to PAC-Bayes and VC-dimension-based results. Moreover, it is the closest to the prob of establishing bounds on the generalization error of representation learning algorithms in terms of the compressibility of the latent variables. This approach is detailed in Appendix B.1.

In this work we aim at understanding the theory of representation learning through a rigorous investigation of the connection between the MDL (or compressibility) of the latent variable and the generalization performance of representation learning algorithms. In doing so, we first study the single-step prediction model of Fig. 0(a); and, then, we leverage the developed tools to study the two-step (encoder-decoder) prediction model of Fig. 0(b). We also examine the claimed relationship between MDL and \(I(U;X)\).

**Contributions.** Specifically, the main contributions of this work are as follows.

* For the prediction model of Fig. 0(a), inspired by [1] we establish an _information-theoretic_ framework that allows us to measure the compressibility (MDL) of the predicted labels in terms of a new object which is the KL-divergence of a vector of labels and any arbitrary symmetric prior. For a proper choice of prior, this new measure reduces to the sample-wise mutual information and hence inherits the associated properties. However, unlike the sample-wise mutual information, it can also reflect the structure/simplicity of the learning algorithm. Furthermore, by extending the framework to "lossy compressibility", this new measure does not become vacuous when one considers continuous variables instead of discrete labels. Moreover, it subsumes geometrical compressibility as a special case.
* We also establish both in-expectation and tail generalization bounds in terms of this compressibility measure of the predicted labels. In a simple case, the in-expectation bound can be cast into the form \[\sqrt{\frac{2\times\text{MDL}(\text{Predicted Labels})}{n}}.\] The tail bound involves a similar expression. This generalization bound easily recovers the VC-dimension bound as a special case; and, hence, it also shows how the introduced compressibility measure depends on the complexity of the hypothesis class.
* Our results make a connection between the compressibility framework of [1] and the functional conditional mutual information (f-CMI) of [11, 12], which itself is an extension of the CMI-framework developed by [15]. In particular, this connection shows how the f-CMI framework can be leveraged to study the compressibility of the predicted labels.
* For the encoder-decoder prediction model of Fig. 0(b), the framework is extended non-trivially to the case in which instead of the compressibility of the predicted labels it is the compressibility of the latent variable which is considered. The results for this prediction model, which are our _main results_ in this paper as given in Section 3, are generalization bounds for the representation learning algorithms in terms of the complexity of the latent variable. The established in-expectation and tail bounds hold for _any_ decoder; and, for the \(K\)-class classification task for example, take the form \[2\sqrt{\frac{2\times\text{MDL}(\text{Latent Variables})+K+2}{n}}.\] These bounds appear to be the first of their kind for the studied representation learning setup. In part, their utility lies in that: i) they reflect the structure of the encoder class, ii) they do not become vacuous for deterministic encoders with continuous latent space and iii) they can explain the geometrical compressibility phenomenon. Thus, our framework reveals that the "joint" MDL of the latent space is more related to the encoder structure, rather than the mutual information \(I(U;X)\).
* Finally, our results suggest that _data-dependent_ priors can be used in place of the data-independent prior of the popular variational IB (VIB) method. We conduct experiments that illustrate the advantage brought up by proper choices of _data-dependent_ priors over VIB.

Our results also open up several future directions as discussed in Appendix C.4.

Figure 1: Considered learning frameworks.

**Notation.** Random variables, their realizations, and their alphabets are denoted respectively by upper-case letters, lower-case letters, and Calligraphy fonts, _e.g.,_\(X\), \(x\), and \(\mathcal{X}\). Their distributions and expectations are denoted by \(P_{X}\) and \(\mathbb{E}[X]\). For ease of presentation, when \(\mathcal{X}\) is a discrete set, \(P_{X}\) is a _probability mass function_. Otherwise, \(P_{X}\) is a _probability density function_. A collection of \(n\) random variables \((X_{1},\ldots,X_{n})\) is denoted by \(X^{n}\) or \(\mathbf{X}\). We use the notation \(\{x_{i}\}_{i=1}^{m}\) to denote a sequence of \(m\) real or natural numbers. The set \(\{1,\ldots,K\}\), \(K\in\mathbb{N}\), is denoted by \([K]\). Finally, \(\mathbb{R}^{+}\) denotes the set of non-negative real numbers. Our results are expressed in terms of information-theoretic functions. For two distributions \(P\) and \(Q\), the Kullback-Leibler (KL) divergence is defined as \(D_{KL}(P||Q)\coloneqq\mathbb{E}_{P}[\log(P/Q)]\) if \(P\in Q\), and \(\infty\) otherwise. The mutual information between two random variables \(X,Y\sim P_{X,Y}\) with marginals \(P_{X}\) and \(P_{Y}\) is defined as \(I(X;Y)\coloneqq D_{KL}(P_{X,Y}|P_{X}P_{Y})\). The reader is referred to [10, 11] for further information.

Our results in this paper will be expressed in terms of _symmetric_ conditional priors. The following definition formalizes three types of symmetry.

**Definition 1** (Symmetric Priors).: _For \(U,V\sim P_{U,V}\), let \((U^{2n},V^{2n})\) be vectors composed of \(2n\) i.i.d. instances \((U_{i},V_{i})\sim P_{U,V}\), \(i\in[2n]\). For any permutation \(\pi\colon[2n]\to[2n]\), denote \(U_{\pi}^{2n}:=(U_{\pi(1)},\ldots,U_{\pi(2n)})\) and \(V_{\pi}^{2n}\coloneqq(V_{\pi(1)},\ldots,V_{\pi(2n)})\)._

* _[leftmargin=*]_
* _Type-I symmetry: Define type-I permutations as the set of permutations_ \(\pi\colon[2n]\to[2n]\) _having the property that for any_ \(i\in[n]\)_, the sets_ \(\{\pi(i),\pi(i+n)\}\) _and_ \(\{i,i+n\}\) _are equal. The conditional prior_ \(\mathbf{Q}(U^{2n}|V^{2n})\) _has type-I symmetry if_ \(\mathbf{Q}(U_{\pi}^{2n}|V_{\pi}^{2n})\) _is invariant to arbitrary type-I permutations. This definition first appeared in_ _[_1_]_ _(called "almost exchangeable prior" therein) and was used for the CMI framework in_ _[_1_]__._
* _Type-II symmetry: The conditional prior_ \(\mathbf{Q}(U^{2n}|V^{2n})\) _has type-II symmetry if_ \(\mathbf{Q}(U_{\pi}^{2n}|V_{\pi}^{2n})\) _is invariant to any arbitrary permutations_ \(\pi\colon[2n]\to[2n]\)_._
* _Type-III symmetry: For a random variable_ \(Z\sim P_{Z|U^{2n},V^{2n}}\)_, the conditional prior_ \(\mathbf{Q}(U^{2n}|V^{2n},Z)\) _has type-III symmetry if_ \(\mathbf{Q}(U_{\pi}^{2n}|V^{2n},Z)\) _is invariant to any permutation_ \(\pi\colon[2n]\to[2n]\) _having the property that_ \(V_{i}=V_{\pi(i)}\) _for every_ \(i\in[2n]\)_._

_Throughout, if the underlying \((U^{2n},V^{2n})\) is clear from the context, for ease of the notation the corresponding sets of Type-I and Type-II priors will be denoted simply as \(\mathcal{Q}_{i}\) and \(\mathcal{Q}_{ii}\) respectively._

**Problem setup.** Unless indicated otherwise, we consider the \(K\)-class classification setup. Let \(Z=(X,Y)\) be some _input data_ taking value over the _input space_\(\mathcal{Z}=\mathcal{X}\times\mathcal{Y}\) according to an unknown distribution \(\mu\). We call \(X\in\mathcal{X}\) the _features_ of the data, and \(Y\in\mathcal{Y}\) its _label_, where \(\mathcal{Y}=[K]\). We assume a _training dataset_\(S=\{Z_{1},\ldots,Z_{n}\}\sim\mu^{\otimes n}=:P_{S}\), composed of \(n\) i.i.d. samples \(Z_{i}=(X_{i},Y_{i})\) of the input data, is available. We denote the features and labels of \(S\) by \(\mathbf{X}\coloneqq X^{n}\sim\mu^{\otimes n}\) and \(\mathbf{Y}\coloneqq Y^{n}\sim\mu^{\otimes n}_{S^{\prime}}\), respectively. We often use also a _ghost_ or _test_ dataset \(S^{\prime}=\{Z^{\prime}_{1},\ldots,Z^{\prime}_{n}\}\sim\mu^{\otimes n}:P_{S^{ \prime}}\), where \(Z^{\prime}_{i}=(X^{\prime}_{i},Y^{\prime}_{i})\). Similarly, we denote the features and labels of \(S^{\prime}\) by \(\mathbf{X}^{\prime}\coloneqq X^{\prime n}\sim\mu^{\otimes n}_{X}\) and \(\mathbf{Y}^{\prime}\coloneqq Y^{\prime n}\sim\mu^{\otimes n}_{Y}\), respectively.

## 2 Generalization bounds in terms of predicted label complexity

In this section, we formulate a compressibility framework that allows us to derive upper bounds on the generalization error of a representation learning algorithm. Our proposed framework can be seen as a suitable generalization of the framework of Blum and Langford [1], in which the generalization encompasses _lossy compression_ of the predicted labels and which exploits _block-coding_. However, unlike in the original framework based on a (compression) game between two agents Alice and Bob (see Appendix B.1 for details), this work adopts a rate-distortion theoretic perspective.

For ease of exposition, the results are presented for classification problems and the 0-1 loss function, but they can be extended trivially to any continuous \(\mathcal{Y}\) and bounded loss function. Consider the setup in Fig. 0(a). Let \(\mathcal{A}\colon\mathcal{Z}^{n}\to\mathcal{W}\) be a possibly stochastic learning algorithm. That is, for a given \(S=(Z_{1},\ldots,Z_{n})\in\mathcal{Z}^{n}\) the algorithm picks a hypothesis or model \(W=\mathcal{A}(S)\in\mathcal{W}\). Also, let the induced joint distribution over \(\mathcal{S}\times\mathcal{W}\) be denoted as \(P_{S,W}\); and the induced conditional distribution over \(\mathcal{W}\) given \(S\) be denoted as \(P_{W|S}\). For every input data \(z=(x,y)\in\mathcal{Z}\), every choice of hypothesis \(w\in\mathcal{W}\) induces a conditional distribution \(P_{\hat{Y}|X,W}(\hat{Y}|x,w)\) on \(\hat{\mathcal{Y}}=\mathcal{Y}\). For convenience, we use the following hand notations:

\[P_{\hat{Y}|X,W}^{\otimes n}(\hat{\mathcal{Y}}|\mathbf{x},w)= \prod_{i\in[n]}P_{\hat{Y}|X,W}(\hat{y}_{i}|x_{i},w),\] \[P_{\hat{Y}|X,W}^{\otimes n}(\hat{\mathcal{Y}}^{\prime}|\mathbf{x }^{\prime},w)= \prod_{i\in[n]}P_{\hat{Y}|X,W}(\hat{y}_{i}^{\prime}|x_{i}^{\prime},w),\] \[P_{\hat{Y}|X,W}^{\otimes n}(\hat{\mathcal{Y}},\hat{\mathcal{Y}}^{ \prime}|\mathbf{x},w^{\prime})= \prod_{i\in[n]}\Bigl{(}P_{\hat{Y}|X,W}(\hat{y}_{i}|x_{i},w)P_{\hat{Y}|X,W}(\hat{y}_{i}^{\prime}|x_{i}^{\prime},w)\Bigr{)}.\]The quality of the prediction is measured by the loss function \(\ell\colon\mathbb{\hat{Z}}\times\mathcal{W}\to[0,1]\) given by

\[\ell(z,w)\coloneqq\mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|X,w}(\hat{Y}|x,w)}[\mathbb{ \hat{1}}_{\{y\neq\hat{Y}\}}],\] (1)

where \(\mathbb{1}\) stands for the indicator function. The associated empirical and population risks for this loss are defined as \(\hat{\mathcal{L}}(s,w)\coloneqq\frac{1}{n}\sum_{i\in[n]}\ell(z_{i},w)\) and \(\mathcal{L}(w)\coloneqq\mathbb{E}_{Z\sim\mu}[\ell(Z,w)]\), respectively. Finally, the generalization error is defined as \(\operatorname{gen}(s,w)\coloneqq\mathcal{L}(w)-\hat{\mathcal{L}}(s,w)\).

### Compressibility framework

Now, we introduce briefly the joint compression of a _block_ of the predicted labels. Further details can be found in Appendix C.1. Consider \(m\) i.i.d. pairs of train and test datasets \(S_{j}\coloneqq(Z_{j,1},\dots,Z_{j,n})\) and \(S^{\prime}_{j}\coloneqq(Z^{\prime}_{j,1},\dots,Z^{\prime}_{j,n})\), where \(Z_{j,i}=(X_{j,i},Y_{j,i})\) and \(Z^{\prime}_{j,i}=(X^{\prime}_{j,i},Y^{\prime}_{j,i})\). Let \(S^{m}=(S_{1},\dots,S_{m})\), \(S^{\prime m}=(S^{\prime}_{1},\dots,S^{\prime}_{m})\) and \(W^{m}\coloneqq(W_{1},\dots,W_{m})\), where \(W_{j}\sim P_{W_{j}|S_{j}}\). Denote the predicted labels using model \(W_{j}\) for inputs \(X_{j,i}\) and \(X^{\prime}_{j,i}\) as \(\hat{Y}_{j,i}\) and \(\hat{Y}^{\prime}_{j,i}\), for \(j\in[m]\) and \(i\in[n]\). Moreover, let \(\mathfrak{Z}^{2n}_{j}\in\mathcal{Z}^{2n}\) denote a rearrangement of the elements of \((S_{j},S^{\prime}_{j})\) in a way that makes it indistinguishable whether a given sample \(z\) is from \(S_{j}\) or \(S^{\prime}_{j}\). In the following two subsections, we investigate two such rearrangements of a given \((S,S^{\prime})\) as \(\mathfrak{Z}^{2n}\). Then, for the collection \((S^{m},S^{\prime m})\), each pair \((S_{j},S^{\prime}_{j})\) for \(j\in[m]\) is rearranged independently and the matrix of all rearrangements is denoted by \(\mathfrak{Z}^{2mn}\). Finally, given some \(\mathfrak{Z}^{2mn}\), let denote the rearranged versions of \((Y^{mn},Y^{\prime mn})\) and \((\hat{Y}^{mn},\hat{Y}^{\prime mn})\) respectively by \(\mathfrak{Y}^{2mn}\) and \(\mathfrak{Y}^{2mn}\).

Our approach is based on studying the _compressibility_ of the rearranged model-predicted labels \(\mathfrak{\hat{Y}}^{2mn}\), from an information-theoretic point of view. The rationale is as follows: since the (rearranged) predicted-labels vector \(\mathfrak{\hat{Y}}^{2mn}\) agrees mostly with the true labels \(\mathfrak{Y}^{2mn}\) on the dataset \(S^{m}\), then in accordance with "Occam's Razor" theorem [11, 1] the model \(W\) is guaranteed to generalize well if \(\mathfrak{\hat{Y}}^{2mn}\) can be described using only a few bits (or nats). We leverage source coding arguments to measure the _compressibility_ of \(\mathfrak{\hat{Y}}^{2mn}\). In our block-coding rate-distortion theoretic framework, a compression rate \(R\in\mathbb{R}^{+}\) is said to be _achievable_ if there exists a compression codebook of size \(\approx e^{mR}\) (fixed a priori) which _covers_ the space spanned by the model-predicted labels \(\mathfrak{\hat{Y}}^{2mn}\) with high probability. That is, if \(R\) is achievable then \(\mathfrak{\hat{Y}}^{2mn}\) can be described using \(R\in\mathbb{R}^{+}\) nats. Formally, \(R\) is achievable if there exists a sequence of label books \(\{\hat{\mathcal{Y}}_{m}\}_{m\in\mathbb{N}}\), with \(\hat{\mathcal{Y}}_{m}\coloneqq\{\hat{\mathcal{Y}}[r],r\in[l_{m}]\}\subseteq \mathcal{Y}^{2mn}\), \(l_{m}\in\mathbb{N}\), \(\hat{\mathcal{Y}}[r]=(\hat{\mathcal{Y}}_{1}[r],\dots,\hat{\mathcal{Y}}_{m}[r])\) and \(\hat{\mathcal{Y}}_{j}[r]=(\hat{y}_{j,1}[r],\dots,\hat{y}_{j,2n}[r])\in\mathfrak{ Y}^{2n}\), such that: \(\mathbf{i}\), \(l_{m}\leq e^{mR}\) and \(\mathbf{ii}\), there exist a sequence \(\{\delta_{m}\}_{m\in\mathbb{N}}\) for which \(\lim_{m\to\infty}\delta_{m}=0\) such that with probability at least \((1-\delta_{m})\) over the choices of \(S^{m},S^{\prime m}\) one can find at least one index \(r\in[l_{m}]\) whose associated \(\mathfrak{\hat{Y}}[r]\) equals \(\mathfrak{\hat{Y}}^{2mn}\).

As explained further in Appendix C.1.3, this _fixed-size_ compressibility framework, which is suitable to upper bound the expectation of the generalization error, can be extended to _variable-size_ compressibility in a way that is similar to [15] in order to upper bound the generalization error with high probability. We notice that, in essence, the core idea of the compressibility framework of Blum-Langford, which we recall in Appendix B.1, can be _seen_ as a _one-shot_ counterpart of our approach. As discussed in [14, 15], in the one-shot case one deals with the "worst case" scenario; and this results in bounds that involve combinatorial terms [1]. In contrast, our information-theoretic framework allows us to bound the compression rate \(R\) in terms of a simpler new quantity: the relative entropy of the joint conditional \(P(\hat{Y}^{n},\hat{Y}^{\prime m}|Y^{n},Y^{\prime n})\) and a (symmetric) conditional prior \(\mathbf{Q}\) over \(\hat{\mathcal{Y}}^{2n}\) given \(Y^{2n}\).

Our approach to establishing a bound on the compressibility of \(\mathfrak{\hat{Y}}^{2mn}\) in terms of information-theoretic measures starting from the combinatorial approach of [1] and by using block-coding essentially consists in a suitable combination of the following key proof steps: _(i)_ introduce and use the symmetries of Definition 1 to rearrange \((S^{m},S^{\prime m})\), _(ii)_ generate the codewords using symmetric priors and _(iii)_ analyze the minimum codebook size needed to "cover" reliably \(\mathfrak{\hat{Y}}^{2mn}\), essentially by use of the _covering lemma_. As shown in [14, 15] and also our proofs, the last two ingredients can be merged via the Donsker-Varadhan's variational representation lemma. The application of this lemma is reminiscent of information-theoretic works on the generalization error such as [11, 15, 15].

### Generalization bounds using type-I symmetric priors

One way to rearrange indistinguishably \((S,S^{\prime})\) as \(\mathfrak{Z}^{2n}\) is as follows: Let \(\mathbf{J}=(J_{1},\dots,J_{n})\) be a vector of \(n\) i.i.d. Bernoulli\((\frac{1}{2})\) random variables \(J_{i}\in\{i,i+n\}\), \(i\in[n]\). For every \(i\in[n]\), let the random variable \(J^{c}_{i}\in\{i,i+n\}\) be defined such that \(J^{c}_{i}=i+n\) if \(J_{i}=i\) if \(J_{i}=i+n\). Also, let the vector \(\mathbf{J}^{c}=(J^{c}_{1},\dots,J^{c}_{n})\). For \(i\in[n]\), we let the random variables \(\mathfrak{Z}_{J_{i}}\) and \(\mathfrak{Z}_{J^{c}_{i}}\) defined as \(\mathfrak{Z}_{J_{i}}=Z_{i}\) and \(\mathfrak{Z}_{J^{c}_{i}}=Z^{\prime}_{i}\). Observe that the vector \(\mathfrak{Z}^{2n}=(\mathfrak{Z}_{1},\dots,\mathfrak{Z}_{2n})\) is a \(\mathbf{J}\)-dependent random re-arrangement of the samples of the training and ghost datasets \(S\) and \(S^{\prime}\). Without knowledge of \(\mathbf{J}\) every element of the vector \(\mathfrak{Z}^{2n}\) has equal likelihood to be from \(S\) or \(S^{\prime}\). A similar construction was used in the context of the analysis of Rademacher complexity and the CMI of [13]. We use Type-I symmetric priors, block coding and information-theoretic _covering_ arguments in order to analyse the space spanned by the random vector \(\mathfrak{Z}^{2n}\). This yields the generalization bound stated in the next theorem whose proof is deferred to Appendix E.2.

**Theorem 1**.:
1. _Let_ \(\mathcal{Q}_{i}\) _be the set of type-I symmetric conditional priors on_ \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\) _given_ \((\mathbf{Y},\mathbf{Y}^{\prime})\)_. Then,_ \(\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]\leqslant\sqrt{2R/n}\)_, with_ \[R\leqslant\inf_{\mathbf{Q}\in\mathcal{Q}_{i}}\mathbb{E}_{\mathbf{Y},\mathbf{Y} ^{\prime}}\Big{[}D_{KL}\Big{(}\mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},W} \Big{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}| \mathbf{X},\mathbf{X}^{\prime},W)\Big{]}\Big{|}\mathbf{Q}\Big{)}\Big{]}=I \left(\mathbf{J};\hat{Y}^{2n}|Y^{2n}\right)\] (2) _where_ \(\mathbf{Y},\mathbf{Y}^{\prime}\sim\mu_{Y}^{\otimes 2n}\) _and_ \(\mathbf{X}^{\prime},\mathbf{X},W\sim P_{\mathbf{X}^{\prime}|\mathbf{Y}^{\prime }}P_{\mathbf{X},W|\mathbf{Y}}\)_. Also the mutual information is calculated with respect to the joint distribution_ \(P_{\mathbf{J},\hat{Y}^{2n},Y^{2n}}=\text{Bern}(1/2)^{\otimes n}\mu_{Y}^{ \otimes 2n}P_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{ 2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}_{ \hat{Y}^{2n}}_{\hat{Y}^{2n}}_{\hat{Y}^{2n}}}}}}}}}}}}\) _and the latter term is defined as_ \(\mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},W}\Big{[}P_{\hat{Y}|X,W}^{\otimes 2n }(\hat{Y}^{2n}_{\hat{Y}^{2}_{\hat{Y}^{2n}}}|\mathbf{X},\mathbf{X}^{\prime},W) \Big{]}\) _in which_ \(\mathbf{X}^{\prime},\mathbf{X},W\sim P_{\mathbf{X}^{\prime}|Y^{\prime}_{\hat{Y }^{2n}}}P_{\mathbf{X},W|Y^{2n}_{\hat{Y}^{2n}}}\)_._
2. _For any_ \(\delta\in\mathbb{R}^{+}\) _and any conditional type-I symmetric prior_ \(\mathbf{Q}\) _on_ \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\) _given_ \((\mathbf{X},\mathbf{Y},\mathbf{X}^{\prime},\mathbf{Y}^{\prime})\)_, with probability at least_ \((1-\delta)\) _over choices of_ \(S,S^{\prime},W\sim P_{S^{\prime}}P_{S,W}\)_, it holds that_1__ Footnote 1: The reader may notice the absence of a “disintegrated bound” here, as opposed to the classical single-draw PAC Bayes bound [12]. This is due to the choice of the loss function, which includes an expectation term. \[\hat{\mathcal{L}}(S^{\prime},W)-\hat{\mathcal{L}}(S,W)\leqslant\sqrt{\frac{4} {2n-1}\bigg{(}D_{KL}\bigg{(}P_{Y|X,W}^{\otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{ \prime}|\mathbf{X},\mathbf{X}^{\prime},W)\bigg{|}\mathbf{Q}\bigg{)}+\log( \sqrt{2n}/\delta)\bigg{)}}.\] (3)

A couple of remarks are in order. The part i of the result of Theorem 1 can be understood as being some form of the f-CMI of [10] in which the function \(f\) represents the predicted labels - in fact our result is slightly stronger comparatively, since instead of conditioning on \(Z^{2n}\) as in the f-CMI of [10] one here conditions only on \(Y^{2n}\). Incidentally, the result also unveils an appealing connection between an extension of the compressibility approach of [12] (in this extension one needs to consider Type-I symmetric priors) and CMI and f-CMI [13, 14, 15]. However, for type-II and type-III symmetries, used in the coming sections, our framework goes beyond the CMI framework. The type-I priors have been also used in [12] to establish (fast-rate) tail bounds for the CMI framework. Due to the difference in the considered setups, their results are not directly comparable with ours. We also note that the above result (and some of the results in the rest of the paper) is kept for clarity and can be trivially improved, by i) moving \(\mathbb{E}_{\mathbf{Y},\mathbf{Y}^{\prime}}\) outside the square root, ii) by rewriting \(\mathbb{E}[\mathrm{gen}(s,w)]=\frac{1}{n}\sum_{i\in[n]}\mathbb{E}[\mathrm{gen}(z _{i},w)]\), and applying the bound for each \(\mathbb{E}[\mathrm{gen}(z_{i},w)]\), similar to [13, 14, 15], iii) by extending the results in accordance with e-CMI framework [13, 14, 15], and iv) establishing tail bound on \(\mathrm{gen}(S,W)\), by noting that with probability at least \((1-\delta)\), \(\hat{\mathcal{L}}(S^{\prime},W)\geqslant\mathcal{L}(W)-\sqrt{\log(1/\delta)/(2n)}\) (similar to Theorem 5).

Next, observe that for any \(\mathbf{Q}\coloneqq\mathbb{E}_{\mathbf{X},\mathbf{X}^{\prime}|Y,\mathbf{Y}^{ \prime}}\big{[}\mathbf{Q}_{1}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}| \mathbf{X},\mathbf{X}^{\prime},\mathbf{Y},\mathbf{Y}^{\prime})\big{]}\), where \(\mathbf{Q}_{1}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X}, \mathbf{X}^{\prime},\mathbf{Y},\mathbf{Y}^{\prime})\) is an arbitrary type-I symmetric prior, and by using the Jensen's inequality, we have

\[\text{RHS of (\ref{eq:1})}\leq \mathbb{E}_{S,S^{\prime},W\sim P_{S,W}P_{S^{\prime}}}\Big{[}D_{KL} \big{(}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{\prime\prime n},W) \|\mathbf{Q}_{1}\big{)}\Big{]}.\] (4)

**Relation to Mutual Information.** A particular choice of the conditional prior \(\mathbf{Q}_{1}\) in the RHS of (4) is \(Q^{\otimes 2n}\) for some prior \(Q\) defined over \(\mathcal{Y}\). For this special choice, the RHS of (4) is given by

\[n\mathbb{E}_{S,W}\mathbb{E}_{X\sim\hat{\mu}_{X|S}}\Big{[}D_{KL}\big{(}P_{\hat{Y}| X,W}(\hat{Y}|X,W)\|Q\big{)}\Big{]}+n\mathbb{E}_{X^{\prime},W\sim\mu_{X}P_{W}}\big{[}D_{KL} \big{(}P_{\hat{Y}|X,W}(\hat{Y}^{\prime}|X^{\prime},W)\|Q\big{)}\big{]},\] (5)

where \(\hat{\mu}_{X|S}\) is the empirical distribution of \(X\) in the dataset \(S\). Moreover, by choosing \(Q\) as the marginal distribution of \(\hat{Y}\) under \(P_{\hat{Y}|X,W}P_{S,W}\), it is easy to see that the first term of the sum of the RHS of (5) coincides with \(n\hat{I}(X;\hat{Y})\) for that choice, where \(\hat{I}(\cdot;\cdot)\) stands for the "empirical mutual information" as computed from the available samples. Thus, the contribution of this term to the bound on generalization error does not necessarily vanish as \(n\to\infty\) (unless the empirical mutual information itself is small). In fact, as already observed in [12], there exist models which generalize well but have non-small mutual-information \(\hat{I}(X;\hat{Y})\). This instantiates that mutual-information type bounds may fall short of explaining true generalization capability, an observation which was already made in [12]. The bound on the generalization error of our Theorem 1, which is provably tighter (see (4)), then possibly remedies this issue.

In what follows we further investigate the relationship of the result of our Theorem 1, which is based on KL-divergence, to VC-dimension and mutual information type bounds on the generalization error.

**Relation to VC-dimension** Suppose that the VC-dimension of the hypothesis class is \(d\). Then, using the Sauer-Shelah lemma [14, 15] we get that given any \(\mathbf{X}\) and \(\mathbf{X}^{\prime}\) one can have at most \((2en/d)^{d}\) distinct labels. By letting the conditional prior \(\mathbf{Q}_{1}\) be a uniform distribution over all such possible predictions, it is easily seen that the KL divergence term of our Theorem 1 is upper bounded by \(d\log(2en/d)\). This means that the result of our Theorem 1 recovers and possibly improves over the VC-dimension bound.

**Structure and "simplicity" of the learning algorithm.** Let us consider a simple example. Suppose that \(\mathcal{X}=[0,1]\subset\mathbb{R}\) and let \(\tau\in(0,1)\) be a parameter. Then, consider the set of deterministic classifiers \(w\) as follows: \(P_{\hat{Y}|X,W}(\hat{y}|x,w)=1\) if (\(\hat{y}=1\) and \(x\geq\tau\)) _or_ (\(\hat{y}=0\) and \(x<\tau\)); and \(P_{\hat{Y}|X,W}(\hat{y}|x,w)=0\) otherwise. It is well known that the VC-dimension of this learning class is \(d=1\). Then, by recalling the aforementioned relation to the VC dimension, we obtain that our Theorem 1 yields a bound on the generalization error which is \(\mathcal{O}(\sqrt{\log(n)/n})\). In particular, it is easy to see that this bound vanishes as \(n\to\infty\). This is in sharp contrast with the mutual information term \(\hat{I}(X;\hat{Y})\) which does not necessarily vanish for large \(n\). For example, if the pair \((X,Y)\) is such that \(Y=1\) iff \(X\geq\tau^{\star}\) for some \(\tau^{\star}\in(0,1)\), then the ratio between RHS of (4) and \(n\) converges (for large \(n\)) to a value which is at least \(H(Y)\) (because \(\hat{I}(X;\hat{Y})\to I(X;Y)\) as \(n\to\infty\) and, in this example, \(I(X;Y)=H(Y)\)).

The above example indicates that using mutual information as a regularizer for learning algorithms (as is the case in IB) may fail to find models that generalize well and have a comparatively simple structure. In fact, using a different approach, it was already observed in [1] and [16] that using the mutual information term as a complexity measure does not reflect the "structure" of the learning algorithm and considering it as a regularizer might not favor "simple" learning algorithms. This suggests that mutual information regularizer in the IB approach may be replaced by the KL divergence term of the RHS of (3) (when the latent variables are considered instead of predictions) which does reflect the complexity of the encoder's structure. As investigated and shown in Section 3, in addition to favoring encoders of simpler structure, our approach provides theoretical guarantees of the generalization error.

#### 2.2.1 Lossy compressiblity

The above approach and results can be extended to any bounded loss function and continuous variables \(Y\) and \(\hat{Y}\) (see the next section where continuous latent variables are studied). In the regime of continuous variables, a standard result of rate-distortion theory stipulates that any _lossless_ encoding of \(\hat{Y}^{n}\) and \(\hat{Y}^{\prime n}\) may require an infinite number of bits, making the generalization bounds vacuous. This is precisely why Shannon's mutual information fails as a regularizer in deterministic learning algorithms with continuous alphabet variables. On the other hand, our approach can be easily extended to include the _lossy compression_ of the labels (see Appendix C.1.2; in the same spirit as done in [17] for the hypothesis compression. The following result states a lossy in-expectation bound and a lossy tail bound is reported in Appendix A.

**Theorem 2**.: _Let \(\mathcal{Q}_{i}\) be the set of type-I symmetric conditional priors on \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\) given \((\mathbf{Y},\mathbf{Y}^{\prime})\). Then, for any \(\epsilon\in\mathbb{R}\), \(\mathbb{E}_{S,W}[\operatorname{gen}(S,W)]\) is upper bounded by_

\[\inf_{P_{\hat{W}|S}}\inf_{\mathbf{Q}\in\mathcal{Q}_{i}}\sqrt{\frac{1}{n} \mathbb{E}_{\mathbf{Y},\mathbf{Y}^{\prime}}\bigg{[}D_{KL}\bigg{(}\mathbb{E}_{ \mathbf{X}^{\prime},\mathbf{X},\hat{W}}\Big{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{ \mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X},\mathbf{X}^{\prime},\hat{W}) \Big{]}\Big{]}\bigg{]}}+\epsilon,\]

_where \(\mathbf{Y},\mathbf{Y}^{\prime}\sim\mu_{Y}^{\otimes 2n}\), \(\mathbf{X}^{\prime},\mathbf{X},\hat{W}\sim P_{\mathbf{X}^{\prime}|\mathbf{Y}^{ \prime}}P_{\mathbf{X},\hat{W}|\mathbf{Y}}\), and the first infimum is over all \(P_{\hat{W}|S}\) satisfying \(\mathbb{E}_{P_{S,W}P_{\hat{W}|S}}\big{[}\operatorname{gen}(S,W)-\operatorname{ gen}(S,\hat{W})\big{]}\leq\epsilon\)._

A proof of this theorem follows by an easy combination of the distortion criterion with the bound on \(\mathbb{E}_{S,\hat{W}}\big{[}\operatorname{gen}(S,\hat{W})\big{]}\) obtained by application of Theorem 1 to the compressed model \(\hat{W}\) (not \(W\)). This simple trick, which is related conceptually to _lossy source coding_, prevents the KL-divergence term from taking very large (infinite) values for continuous alphabet variables. Moreover, the lossy compressibility also offers an interpretation of the _geometrical compressibility_ concept that was observed to be related to the generalization performance in [1].

### Generalization bounds using type-II symmetric priors

In this section, we consider another way to rearrange indistinguishably \((S,S^{\prime})\) as \(\mathfrak{Z}^{2n}\), which is inspired by [1]. Let \(\mathbf{T}=\{T_{1},\ldots,T_{n}\}\), be a random set obtained by picking uniformly \(n\) indices from \(\{1,\ldots,2n\}\), without replacement. Note that in contrast to \(\mathbf{J}\) which had i.i.d. components, here the components of are dependent. let \(\mathbf{T}^{c}=\{1,\ldots,2n\}\backslash\mathbf{T}\) be the complement of \(\mathbf{T}\), having the elements \(\mathbf{T}^{c}=\{T_{1}^{c},\ldots,T_{n}^{c}\}\). Now, for each \(i\in[n]\), let \((\mathfrak{Z}_{T_{i}},\mathfrak{Z}_{T_{i}^{c}}^{2})=(Z_{i},Z_{i}^{\prime})\). We use type-II symmetric prior to _cover_ such rearranged vectors. To state the result, first we need to define the function \(h_{D}(x;x^{\prime})\colon[0,1]\times[0,1]\to[0,2]\) as follows:

\[h_{D}(x,x^{\prime})\coloneqq 2h_{b}\Big{(}\frac{x+x^{\prime}}{2}\Big{)}-h_{b}( x)-h_{b}(x^{\prime}),\] (5)

where \(h_{b}(x)\coloneqq-x\log_{2}(x)-(1-x)\log_{2}(1-x)\). Note that \(h_{D}(x,x^{\prime})/2\) is equal to the Jensen-Shannon divergence between two binary Bernoulli distributions with parameters \(x\) and \(x^{\prime}\). The reader is referred to Appendix A for the relation of this function with the combinatorial term appeared in [1]. The function \(h_{D}(\cdot,\cdot)\) has the following interesting properties, proved in Appendix E.3.

**Lemma 1**.: \(\forall\;(x,x^{\prime})\in[0,1]\times[0,1]\)_, we have: **(i)** \(h_{D}(x,x^{\prime})\geq(x-x^{\prime})^{2}\), **(ii)** \(h_{D}(x,0)\geq x\), **(iii)** \(h_{D}(x,x^{\prime})\) is increasing with respect to \(x\) in the range \([x^{\prime},1]\), and **(iv)** \(h_{D}(x,x^{\prime})\) is convex with respect to both inputs._

Now, we state the main result of this section.

**Theorem 3**.: _Let \(\mathcal{Q}_{ii}\) be the set of type-II symmetric priors on \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\) given \((\mathbf{Y},\mathbf{Y}^{\prime})\). Then, for \(n\geq 10\),_

\[nh_{D}\Big{(}\mathbb{E}_{W}\big{[}\mathcal{L}(W)\big{]},\mathbb{ E}_{S,W}\big{[}\hat{\mathcal{L}}(S,W)\big{]}\Big{)}\leq\] \[\inf_{\mathbf{Q}\in\mathcal{Q}_{ii}}\mathbb{E}_{\mathbf{Y}, \mathbf{Y}^{\prime}}\Big{[}D_{KL}\Big{(}\mathbb{E}_{\mathbf{X}^{\prime}, \mathbf{X},W}\Big{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{ \prime}|\mathbf{X},\mathbf{X}^{\prime},W)\Big{]}\Big{|}\mathbf{Q}\Big{)} \Big{]}+\log(n)=I\Big{(}\mathbf{T};\hat{Y}^{2n}|Y^{2n}\Big{)}+\log(n),\]

_where \(\mathbf{Y},\mathbf{Y}^{\prime}\sim\mu_{\hat{\mathbf{Y}}}^{\otimes 2n}\) and \(\mathbf{X}^{\prime},\mathbf{X},W\sim P_{\mathbf{X}^{\prime}|\mathbf{Y}^{\prime} }P_{\hat{\mathbf{X}},W|\mathbf{Y}}\). Also the mutual information is calculated with respect to the joint distribution \(P_{\mathbf{T},\hat{Y}^{2n},Y^{2n}}=P_{\mathbf{T}}\mu_{\hat{Y}}^{\otimes 2n}P_{ \hat{Y}^{2n}_{\hat{Y}^{2n}_{\hat{Y}^{2n}}},\hat{Y}^{2n}_{\hat{Y}^{2n}}}|Y^{2n} _{\hat{Y}^{2n}_{\hat{Y}^{2n}}}\) and the latter term is defined as \(\mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},W}\Big{[}P_{\hat{Y}|X,W}^{\otimes 2n} (\hat{Y}^{2n}_{\mathbf{T}},\hat{Y}^{2n}_{\mathbf{T}^{c}}|\mathbf{X},\mathbf{X}^ {\prime},W)\Big{]}\) in which \(\mathbf{X}^{\prime},\mathbf{X},W\sim P_{\mathbf{X}^{\prime}|Y^{2n}_{\hat{Y}^{ 2n}}}P_{\mathbf{X},W|Y^{2n}_{\hat{Y}^{2n}}}\)._

The proof of Theorem 3 is deferred to Appendix E.4. Also, a similar tail bound is provided in Appendix A.

Using the part (i) of Lemma 1, it can be seen that if the value of the KL divergence term is larger than \(\log(n)\) then the bound of Theorem 3 is tighter than that Theorem 1. Also, using part (ii) of Lemma 1 it is seen that if the error on the training set is zero (a setting referred to as "realizable case" in [1]), our Theorem 3 yields a bound on the generalization error which is \(\mathcal{O}(1/n)\).

## 3 Generalization bounds in terms of latent variable complexity

The generalization bounds of the previous section are particularly useful in the following sense: if the learning algorithm is "simple" enough to produce a low "relative entropy" sequence of labels for training and test sets, then the algorithm generalizes well. However, they have a downside that they cannot be used directly as they are in the optimization, since minimizing those bounds may result in solutions with large empirical risk. In this section, we extend the results of the previous section to settings in which the processing is split into two parts: an encoder part that produces a family of representations that have the property to generalize well (developed according to the guidelines of the previous section) and a decoder part that selects among that family one representation that minimizes the empirical risk. As such the goal of the encoder is to guarantee a small generalization error and that of the decoder is to guarantee a small empirical risk. This procedure, which is similar to the Information Bottleneck method, aims at finding a good balance between generalizing well to unseen data and minimizing the risk of the training data.

Let \(W=(W_{e},W_{d})\), where \(W_{e}\) and \(W_{d}\) are the hypotheses (or models) used by the encoder and the decoder, respectively, as illustrated in Fig. (b)b. Also, let \(U\) denote the output of the encoder, which will be referred to hereafter interchangeably as "representation" or "latent variable". The encoder produces the representation \(U\) according to the conditional \(P_{U|X,W_{e}}\). The decoder produces an estimate \(\hat{Y}\) of the true label \(Y\) according to the conditional \(P_{\hat{Y}|U,W_{d}}\). We consider a stochastic learning algorithm \(\mathcal{A}\colon\mathcal{Z}^{n}\to\mathcal{W}\) which picks a model \(W=(W_{e},W_{d})\in\mathcal{W}\coloneqq\mathcal{W}_{e}\times\mathcal{W}_{d}\) according to \(P_{W|S}\). Let a loss function \(\ell\colon\mathcal{Z}\times\mathcal{W}\to[0,1]\) be given. For a model \(w=(w_{e},w_{d})\) the quality of the prediction of \(\hat{Y}\) is evaluated as

\[\ell(z,w)\coloneqq\mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|X,W}(\hat{Y}|x,w)}[ \mathbb{I}_{\{y\notin\hat{Y}\}}]\coloneqq\mathbb{E}_{U\sim P_{U|X,W_{e}}(U|x,w_{ e})}\mathbb{E}_{\hat{Y}\sim P_{\hat{Y}|U,W_{d}}(\hat{Y}|U,w_{d})}\Big{[}\mathbb{I}_{\{y \notin\hat{Y}\}}\Big{]}.\]

Empirical risk, population risk, and generalization error are defined similarly to the previous section. According to the above motivation, we are interested in establishing a bound on the generalization error of the algorithm \(\mathcal{A}\) that depends only on the part \(W_{e}\) of the model \(W\). In accordance with our compressibility framework of Section 2 such bound would then depend on the complexity of the latent variable \(U\). However, here, the encoder part \(W_{e}\) and the decoder part \(W_{d}\) are both trained using the dataset \(S\). Thus, they may be statistically dependent in general and therefore the "rearrangement" ideas using type-I and type-II symmetries do not work here. To elaborate on this a bit more, recall that for example for type-I symmetry, to rearrange \((\mathbf{Y},\hat{\mathbf{Y}})\) and \((\mathbf{Y}^{\prime},\hat{\mathbf{Y}}^{\prime})\) in an indistinguishable manner, we randomly shuffle the positions of the pairs \((Y_{i},\hat{Y}_{i})\) and \((Y^{\prime}_{i},\hat{Y}^{\prime}_{i})\). If we now consider using the type-I symmetry for the new setup, then shuffling of the pairs \((Y_{i},U_{i},\hat{Y}_{i})\) and \((Y^{\prime}_{i},U^{\prime}_{i},\hat{Y}^{\prime}_{i})\), changes the dataset \(S\) and thus the distributions \(P_{W|S}\) and \(P_{\hat{Y}|W_{d},U}\). Hence, covering the resulting rearranged sequence would unavoidably depend on both encoder and decoder parts. To overcome this issue, we use type-III symmetry, where we leave the train and ghost datasets untouched and randomly "swap" only those latent variables and their corresponding predictions, _i.e., \((U_{i},\hat{Y}_{i})\)_ and \((U^{\prime}_{i},\hat{Y}^{\prime}_{i})\), that are associated with the train and ghost samples with the same label, _i.e.,_ if \(Y_{i}=Y^{\prime}_{j}\). The following result is the main theorem of this section and the paper derived using the type-III symmetric priors. A formal proof of this result can be found in Appendix E.5.

**Theorem 4** (Generalization Bound for Representation Learning Algorithms).: _Consider a \(K\)-classification learning task. Let \(\mathbf{Q}\) be a type-III symmetric conditional prior over \(U^{2n}\) given \(X^{2n},Y^{2n}\) and \(\hat{W}_{e}\in\mathcal{W}_{e}\), namely, \(\mathbf{Q}\Big{(}(u_{\pi(1)},\dots,u_{\pi(2n)})|(x_{1},\dots,x_{2n}),(y_{1}, \dots,y_{2n}),\hat{w}_{e}\Big{)}\) remains the same for all permutations \(\pi\colon[2n]\mapsto[2n]\) that preserves the label, i.e., \(y_{\pi(i)}=y_{i}\) for \(i\in[2n]\). Then,_

\[\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]\leq\inf 2\sqrt{\frac{2\mathbb{E}_{S,S^{ \prime},\hat{W}_{e}}\bigg{[}D_{KL}\bigg{(}P_{U|X,\hat{W}_{e}}^{\otimes 2n}\big{(} \mathbf{U},\mathbf{U}^{\prime}|\mathbf{X},\mathbf{X}^{\prime},\hat{W}_{e} \big{)}\big{\|}\mathbf{Q}\bigg{)}\bigg{]}+K+2}{n}}+\epsilon,\]

_where \(S,S^{\prime},\hat{W}_{e}\sim P_{S,\hat{W}_{e}}P_{S^{\prime}}\) and the infimum is over all Markov kernels \(P_{\hat{W}_{e}|S}\) such that for \(\hat{W}=(\hat{W}_{e},W_{d})\), \(\mathbb{E}_{P_{S,W}P_{\hat{W}_{e}|S}}[\mathrm{gen}(S,W)-\mathrm{gen}(S,\hat{ W})]\leq\epsilon\)._

The bound of Theorem 4 on the generalization error of representation learning, and the related IB-encoder, is to the best of our knowledge the first of its kind and significance. In fact, while few works have already investigated the generalization error of IB [10, 11, 12], for the special case of discrete variables, their bounds, which in part are expressed in terms of the empirical mutual information \(\hat{I}(U;X)\), appear to be vacuous for most settings as already observed in [13, 1, 1, 14] (see introduction and Appendix B.2 for more details). Furthermore, as also extensively discussed in [1, 1, 1], mutual information may not be a good indicator of the generalization error. Such aspects, including how mutual information fails to reflect "simplicity" or "structure" of the encoder, are discussed in more detail in the previous section. An alternate bound on the generalization error of representation learning appeared in [1]. However, their bound only holds for encoders that find the _optimal_ representation in a sense defined therein.

Now, few remarks on the result of Theorem 4 are in order. First, note that the generalization bound of this theorem depends _only_ on the encoder and more precisely on the complexity of the latent space \(U\); and, so, this bound is valid for _any_ choice decoder.2 Next, similar to the bounds of the Theorems 1-5, the KL-divergence term of Theorem 4 explicitly takes into account the "structure" and "simplicity" of the encoder, and, therefore, it resolves one of the major issues of the IB method [1, 1, 1].

Footnote 2: In some cases, the bound may be loose, however, e.g., with a decoder \(w_{d}\) that produces an estimate \(\hat{Y}\)_independently_ of the obtained representation \(U\).

Moreover, the result of Theorem 4 suggests that the considered prior could depend on the data and model. This enables a larger class of choices for the prior. Examples include (i) Symmetric jointly Gaussian priors, (ii) priors that depend on the category (label), _i.e.,_ the prior for a category \(k\in[K]\) at each optimization iteration could possibly depend on some statistics of the latent variables of all training and test samples having label \(k\), and (iii) priors that steer latent variables toward some pre-defined "constellations" in the latent space, depending on their label. On this aspect, note that allowing the prior to depend on labels enables a connection with the "conditional information-bottleneck" of [15].

Another important property of our bound of Theorem 4 is that, unlike the empirical mutual information term of [10, 11, 12],3 it does not become infinite for deterministic encoders with continuous input-output. Moreover, our approach which is based on _lossy compression_ provides an interpretation of the _geometric compression_ of [13, 1] where latent variables are concentrated around some constellation points.4 We hasten to mention that "lossy compression" here should not be confused with approaches that add noise after the encoder. The main difference is that in those approaches the noisy representations are passed to the decoder; while here, the "noisy" representations are used only to estimate lossy compressibility. These "noisy" representations can be achieved by either adding small "noise" to the model parameters or the latent variable. Note that by increasing the noise level the \(\epsilon\) term in Theorem 4 increases while the KL-divergence term potentially decreases. In practice, a suitable trade-off between the two effects can be found by treating the amount of added noise as a _hyper-parameter_ to optimize.

Footnote 4: The reader is referred to Appendix C.3 for a simple example of geometric compression.

Finally, a similar tail bound on the generalization error has been established in Appendix A.

## 4 Experiments

In this section, we illustrate our results via some experiments. For more detail and other experiments the reader is referred to Appendix D.

Our main Theorems 4 and 7_suggest_ that for the representation learning setup of Fig. (b)b the generalization error is controlled essentially by the divergence term \(D_{KL}\Big{(}P_{U|X,W_{e}}^{\otimes 2n}(\mathbf{U},\mathbf{U}^{\prime}| \mathbf{X},\mathbf{X}^{\prime},\mathbf{Y},\mathbf{Y}^{\prime},W_{e})\big{\|} \mathbf{Q}\Big{)}\) where \(\mathbf{Q}\) is a type-III symmetric prior. In a sense, this also means that, with a proper _data-dependent_ choice of the prior, the usage of the aforementioned divergence term as a regularizer possibly offers better generalization guarantees (relative, e.g., to the conventional data-independent prior of VIB). In what follows, we propose a new family of data-dependent priors which appear to better capture the "simplicity" or "structure" of the encoder. We also compare the associated accuracy with that offered by the fixed prior of VIB.

More precisely, in VIB the prior \(\mathbf{Q}\) factorizes as a product of \(2n\) scalar standard Gaussian priors, _i.e._, \(\mathbf{Q}=Q^{\otimes 2n}\), where \(Q=\mathcal{N}(\mathbf{0}_{m},\mathrm{I}_{m})\), \(\mathbf{0}_{m}\in\mathbb{R}^{m}\) is the zero vector and \(\mathrm{I}_{m}\) is the \(m\times m\) identity matrix. In our lossless approach, which we here coin as Lossless Category-Dependent VIB (CDVIB), the prior \(\mathbf{Q}\) still factorizes as a product of \(2n\) scalar Gaussian priors, _i.e._, \(\mathbf{Q}=\prod_{i\in[2n]}Q_{i}\), but with three major differences: **i.** Each \(Q_{i}\) can be chosen from a set of \(M\times K\) priors - \(M\) priors for each label. **ii.** Unlike VIB, each of \(M\times K\) priors can depend on some statistics of \((\mathbf{S},\mathbf{S}^{\prime})\) and also on the label of each sample, **iii.** Unlike VIB, where the prior is fixed, here \(\mathbf{Q}\) is 'learned" during the training phase. To this end, the mean and variance of the scalar priors are updated after each training iteration using a moving average with some small coefficient, allowing the latent space to better adapt to the structure of the encoder and the data. Taking the moving average also has another role, which is to "partially" reproduce the effect of the "ghost data" (which comes from the test dataset that is usually unavailable during training). In lossy CDVIB, similar priors are considered but over "noisy" versions of the latent variables, _i.e., \((\hat{\mathbf{U}},\mathbf{U}^{\prime})\)_. Note that while the noisy versions are considered for the regularizer, the decoder receives as input \((\mathbf{U},\mathbf{U}^{\prime})\).

We consider CIFAR10 [KH\({}^{+}\)09] image classification using a small CNN-based encoder and a linear decoder. The results shown in Fig. 2 indicate that the model trained using our priors achieves better (\(\sim 2.5\%\)) performance in terms of both generalization error and population risk. This _suggests_ that our priors help to find a better _representation_ than the standard VIB prior.

Figure 2: Accuracy during test phase of our two-step prediction model trained using the standard VIB prior and our “lossless” CDVIB and “lossy” CDVIB priors computed for \(M=5\). The values are averaged over 5 runs. The graphs are displayed together with 95% bootstrap confidence intervals.

Acknowledgement

The authors would like to thank the anonymous reviewers for their many insightful comments and suggestions. In particular, by pointing out the connection between our results and the f-CMI literature.

## References

* [AFDM17] Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information bottleneck. In _International Conference on Learning Representations_, 2017.
* [AG19] Rana Ali Amjad and Bernhard C Geiger. Learning representations for neural network-based classification using the information bottleneck principle. _IEEE transactions on pattern analysis and machine intelligence_, 42(9):2225-2239, 2019.
* [AGNZ18] Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for deep nets via a compression approach. In _International Conference on Machine Learning_, pages 254-263. PMLR, 2018.
* [Aud04] Jean-Yves Audibert. PAC-Bayesian statistical learning theory. _PhD thesis, Universite Paris VI,_, 2004.
* [AZ19] Inaki Estella Aguerri and Abdellatif Zaidi. Distributed variational representation learning. _IEEE transactions on pattern analysis and machine intelligence_, 43(1):120-138, 2019.
* [BBR\({}^{+}\)18] Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R Devon Hjelm. Mine: mutual information neural estimation. _arXiv preprint arXiv:1801.04062_, 2018.
* [BEHW87] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K Warmuth. Occam's razor. _Information processing letters_, 24(6):377-380, 1987.
* [BHMZ20] Olivier Bousquet, Steve Hanneke, Shay Moran, and Nikita Zhivotovskiy. Proper learning, helly number, and an optimal svm bound. In _Conference on Learning Theory_, pages 582-609. PMLR, 2020.
* [BL03] Avrim Blum and John Langford. Pac-mdl bounds. In _Learning Theory and Kernel Machines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003. Proceedings_, pages 344-357. Springer, 2003.
* [BM06] Nader H Bshouty and Hanna Mazzawi. Exact learning composed classes with a small number of mistakes. In _Learning Theory: 19th Annual Conference on Learning Theory, COLT 2006, Pittsburgh, PA, USA, June 22-25, 2006. Proceedings 19_, pages 199-213. Springer, 2006.
* [BO18] Leonard Blier and Yann Ollivier. The description length of deep learning models. _Advances in Neural Information Processing Systems_, 31, 2018.
* [BSE\({}^{+}\)21] Melih Barsbey, Milad Sefidgaran, Murat A Erdogdu, Gael Richard, and Umut Simsekli. Heavy tails in SGD and compressibility of overparametrized neural networks. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* [BZV20] Yuheng Bu, Shaofeng Zou, and Venugopal V. Veeravalli. Tightening mutual information-based bounds on generalization error. _IEEE Journal on Selected Areas in Information Theory_, 1(1):121-130, May 2020.
* [Cat07] Olivier Catoni. Pac-bayesian supervised classification. _Lecture Notes-Monograph Series. IMS_, 1277, 2007.
* [CK11] Imre Csiszar and Janos Korner. _Information Theory: Coding Theorems for Discrete Memoryless Systems_. Cambridge University Press, 2 edition, 2011.
* [CK22] Dan Tsir Cohen and Aryeh Kontorovich. Learning with metric losses. In _Conference on Learning Theory_, pages 662-700. PMLR, 2022.

* [CT06] Thomas M. Cover and Joy A. Thomas. _Elements of information theory (2. ed.)_. Wiley, 2006.
* [Dev83] Luc Devroye. The equivalence of weak, strong and complete convergence in l1 for kernel density estimates. _The Annals of Statistics_, pages 896-904, 1983.
* [DKSV20] Yann Dubois, Douwe Kiela, David J Schwab, and Ramakrishna Vedantam. Learning optimal representations with the decodable information bottleneck. _Advances in Neural Information Processing Systems_, 33:18674-18690, 2020.
* [Fis20] Ian Fischer. The conditional entropy bottleneck. _Entropy_, 22(9):999, 2020.
* [Gal68] Robert G Gallager. _Information theory and reliable communication_, volume 588. Springer, 1968.
* [GB10] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In _Proceedings of the thirteenth international conference on artificial intelligence and statistics_, pages 249-256. JMLR Workshop and Conference Proceedings, 2010.
* [Gei21] Bernhard C Geiger. On information plane analyses of neural network classifiers-a review. _IEEE Transactions on Neural Networks and Learning Systems_, 2021.
* [GK19] Bernhard C Geiger and Tobias Koch. On the information dimension of stochastic processes. _IEEE transactions on information theory_, 65(10):6496-6518, 2019.
* [GMP05] Peter D Grunwald, In Jae Myung, and Mark A Pitt. _Advances in minimum description length: Theory and applications_. MIT press, 2005.
* [GP20] Ziv Goldfeld and Yury Polyanskiy. The information bottleneck problem and its applications in machine learning. _IEEE Journal on Selected Areas in Information Theory_, 1(1):19-38, 2020.
* [GR19] Peter Grunwald and Teemu Roos. Minimum description length revisited. _International journal of mathematics for industry_, 11(01):1930001, 2019.
* [GSZ21] Peter Grunwald, Thomas Steinke, and Lydia Zakynthinou. Pac-bayes, mac-bayes and conditional mutual information: Fast rate bounds that handle general vc classes. In _Conference on Learning Theory_, pages 2217-2247. PMLR, 2021.
* [GVDBG\({}^{+}\)19] Ziv Goldfeld, Ewout Van Den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, and Yury Polyanskiy. Estimating information flow in deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 2299-2308. PMLR, 09-15 Jun 2019.
* [HD22] Fredrik Hellstrom and Giuseppe Durisi. A new family of generalization bounds using samplewise evaluated cmi. _Advances in Neural Information Processing Systems_, 35:10108-10121, 2022.
* [HJTW21] Daniel Hsu, Ziwei Ji, Matus Telgarsky, and Lan Wang. Generalization bounds via distillation. In _International Conference on Learning Representations_, 2021.
* [HK19] Steve Hanneke and Aryeh Kontorovich. A sharp lower bound for agnostic learning with sample compression schemes. In _Algorithmic Learning Theory_, pages 489-505. PMLR, 2019.
* [HK21] Steve Hanneke and Aryeh Kontorovich. Stable sample compression schemes: New applications and an optimal svm margin bound. In _Algorithmic Learning Theory_, pages 697-721. PMLR, 2021.
* [HKS19] Steve Hanneke, Aryeh Kontorovich, and Menachem Sadigurschi. Sample compression for real-valued learners. In _Algorithmic Learning Theory_, pages 466-488. PMLR, 2019.

* [HKSW20] Steve Hanneke, Aryeh Kontorovich, Sivan Sabato, and Roi Weiss. Universal bayes consistency in metric spaces. In _2020 Information Theory and Applications Workshop (ITA)_, pages 1-33. IEEE, 2020.
* [HRVSG21] Hrayr Harutyunyan, Maxim Raginsky, Greg Ver Steeg, and Aram Galstyan. Information-theoretic generalization bounds for black-box learning algorithms. _Advances in Neural Information Processing Systems_, 34, 2021.
* [KASK22] Michael Kleinman, Alessandro Achille, Stefano Soatto, and Jonathan Kao. Gacs-korner common information variational autoencoder. _arXiv preprint arXiv:2205.12239_, 2022.
* [KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* [KDH23] Kenji Kawaguchi, Zhun Deng, Xu Ji, and Jiaoyang Huang. How does information bottleneck help deep learning? In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 16049-16096. PMLR, 23-29 Jul 2023.
* [KH\({}^{+}\)09] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. _Toronto, ON, Canada_, 2009.
* [KTVK18] Artemy Kolchinsky, Brendan D Tracey, and Steven Van Kuyk. Caveats for information bottleneck in deterministic scenarios. _arXiv preprint arXiv:1808.07593_, 2018.
* [KTW19] Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. _Entropy_, 21(12):1181, 2019.
* [KW14] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _ICLR_, 2014.
* [LEG18] Cheuk Ting Li and Abbas El Gamal. Strong functional representation lemma and applications to coding theorems. _IEEE Transactions on Information Theory_, 64(11):6967-6978, 2018.
* [LLS\({}^{+}\)23] Yilin Lyu, Xin Liu, Mingyang Song, Xinyue Wang, Yaxin Peng, Tieyong Zeng, and Liping Jing. Recognizable information bottleneck. _arXiv preprint arXiv:2304.14618_, 2023.
* [LW86] Nick Littlestone and Manfred Warmuth. Relating data compression and learnability. _Citeseer_, 1986.
* [McA98] David A McAllester. Some pac-bayesian theorems. In _Proceedings of the eleventh annual conference on Computational learning theory_, pages 230-234, 1998.
* [NBS18] Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-normalized margin bounds for neural networks, 2018.
* [PGM\({}^{+}\)19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [RG19] Borja Rodriguez Galvez. The information bottleneck: Connections to other problems, learning and exploration of the ib curve, 2019.
* [RGBTS21] Borja Rodriguez-Galvez, German Bassi, Ragnar Thobaben, and Mikael Skoglund. On random subset generalization error bounds and the stochastic gradient langevin dynamics algorithm. In _2020 IEEE Information Theory Workshop (ITW)_, pages 1-5. IEEE, 2021.
* [RGTS20] Borja Rodriguez Galvez, Ragnar Thobaben, and Mikael Skoglund. The convex information bottleneck lagrangian. _Entropy_, 22(1):98, 2020.
* [Ris78] Jorma Rissanen. Modeling by shortest data description. _Automatica_, 14(5):465-471, 1978.

* [RZ16] Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In Arthur Gretton and Christian C. Robert, editors, _Proceedings of the 19th International Conference on Artificial Intelligence and Statistics_, volume 51 of _Proceedings of Machine Learning Research_, pages 1232-1240, Cadiz, Spain, 09-11 May 2016. PMLR.
* [SAM\({}^{+}\)20] Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi, Kotaro Ito, Tokuma Wachi, So Hirai, Masatoshi Yukishima, and Tomoaki Nishimura. Spectral pruning: Compressing deep neural networks via spectral analysis and its generalization error. In _International Joint Conference on Artificial Intelligence_, pages 2839-2846, 2020.
* [Sau72] Norbert Sauer. On the density of families of sets. _Journal of Combinatorial Theory, Series A_, 13(1):145-147, 1972.
* [SGRS22] Milad Sefidgaran, Amin Gohari, Gael Richard, and Umut Simsekli. Rate-distortion theoretic generalization bounds for stochastic learning algorithms. In _Conference on Learning Theory_, pages 4416-4463. PMLR, 2022.
* 261, 1972.
* [SSDE20] Umut Simsekli, Ozan Sener, George Deligiannidis, and Murat A Erdogdu. Hausdorff dimension, heavy tails, and generalization in neural networks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 5138-5151. Curran Associates, Inc., 2020.
* [SST10] Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information bottleneck. _Theoretical Computer Science_, 411(29-30):2696-2711, 2010.
* [SZ20] Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual information. In Jacob Abernethy and Shivani Agarwal, editors, _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 3437-3452. PMLR, 09-12 Jul 2020.
* [SZ23] Milad Sefidgaran and Abdellatif Zaidi. Data-dependent generalization bounds via variable-size compressibility. _arXiv preprint arXiv:2303.05369_, 2023.
* [SZT17] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. _arXiv preprint arXiv:1703.00810_, 2017.
* [THP23] Dor Tsur, Bashar Huleihel, and Haim Permuter. Rate distortion via constrained estimated mutual information minimization. In _2023 IEEE International Symposium on Information Theory (ISIT)_, pages 695-700, 2023.
* [TPB00] Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. _arXiv preprint physics/0004057_, 2000.
* [VDOV\({}^{+}\)17] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. _Advances in neural information processing systems_, 30, 2017.
* [VPV18] Mati Vera, Pablo Piantanida, and Leonardo Rey Vega. The role of the information bottleneck in representation learning. In _2018 IEEE International Symposium on Information Theory (ISIT)_, pages 1580-1584, 2018.
* [Wai19] Martin J. Wainwright. _High-Dimensional Statistics: A Non-Asymptotic Viewpoint_. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019.
* [XR17] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. _Advances in Neural Information Processing Systems_, 30, 2017.
* [ZEAS20] Abdellatif Zaidi, Inaki Estella-Aguerri, and Shlomo Shamai. On the information bottleneck problems: Models, connections, applications and information theoretic views. _Entropy_, 22(2):151, 2020.

**Appendices**

The appendices are organized as follows.

* Appendix A contains further theoretical results. Specifically,
* in Appendix A.1 two more tail bounds on the generalization error in terms of the predicted label complexity are presented.
* in Appendix A.2 a tail bound on the generalization error in terms of the latent variable complexity is presented.
* We discuss some of the related works in more detail in Appendix B. In particular,
* in Appendix B.1, we recall the compressibility framework of Blum and Langford [1].
* in Appendix B.2, we discuss the results of [13].
* In Appendix C, we discuss various topics that could not be sufficiently addressed in the paper, due to lack of space. More precisely,
* we define various notions of compressibility, including lossless, lossy, and variable-size compressibility in Appendix C.1,
* we present an intuition on the function \(h_{D}(\cdot,\cdot)\), used in Section 2, in Appendix C.2,
* we give a simple example of geometrical compression, and its relation to lossy compressibility, in Appendix C.3,
* and finally we present the conclusion of our work together with some interesting future directions in Appendix C.4.
* Appendix D contains the details of the experiments presented in Section 4.
* Appendix D.1 recalls the standard VIB objective function,
* Appendix D.2 defines a family of lossless objective functions using data-dependent priors,
* Appendix D.3 defines a family of lossy objective functions using data-dependent priors,
* Appendix D.4 details the training and testing datasets used in experiments,
* Appendix D.5 details the model architecture used in experiments,
* Appendix D.6 provides training details,
* Appendix D.7 presents and discusses the numerical results.
* Appendix E contains the deferred proofs of all our theoretical results. More precisely,
* the intuition behind our proof techniques is presented in Appendix E.1,
* Appendix E.2 contains the proof of Theorem 1,
* Appendix E.3 contains the proof of Lemma 1,
* Appendix E.4 contains the proof of Theorem 3,
* Appendix E.5 contains the proof of Theorem 4,
* Appendix E.6 contains the proof of Theorem 5,
* Appendix E.7 contains the proof of Theorem 6,
* Appendix E.8 contains the proof of Theorem 7,
* Appendix E.9 contains the proof of Lemma 3.

## Appendix A Additional theoretical results

In this section, we present several tail bounds on the generalization error in terms of the complexity of the predicted labels or the latent variables.

### Tail bounds on the generalization error in terms of predicted label complexity

First, we start by stating a _lossy_ tail bound on the generalization error in terms of the complexity of the tail bound.

**Theorem 5**.: _Consider the learning framework defined above. Let \(\mathbf{Q}\) be any fixed type-I symmetric prior on \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\) that could depend on \((\mathbf{X},\mathbf{Y},\mathbf{X}^{\prime},\mathbf{Y}^{\prime})\). Then for any \(\epsilon\in\mathbb{R}\) and \(\delta\in\mathbb{R}^{+}\) with probability at least \((1-\delta)\) over choice of \(S\) and \(S^{\prime}\), we have that \(\mathbb{E}_{W\sim P_{W|S}}[\mathrm{gen}(S,W)]\) is upper bounded by_

\[\sqrt{\frac{\log(2/\delta)}{2n}}+\inf\sqrt{\frac{4}{2n-1}\bigg{(}\mathbb{E}_{ \hat{W}\sim P_{\hat{W}|S}}\bigg{[}D_{KL}\bigg{(}P_{\hat{Y}|X,\hat{W}}^{\otimes 2n}( \hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X},\mathbf{X}^{\prime}, \hat{W})\bigg{\|}\mathbf{Q}\bigg{)}\bigg{]}+\log(\sqrt{8n}/\delta)\bigg{)}+ \epsilon},\]

_where the infimum is over all \(P_{\hat{W}|S}\) that satisfy_

\[\mathbb{E}_{P_{W|S}P_{W|S}}\Big{[}\Big{|}\Big{(}\hat{\mathcal{L}}(S^{\prime}, W)-\hat{\mathcal{L}}(S,W)\Big{)}-\Big{(}\hat{\mathcal{L}}(S^{\prime},\hat{W})- \hat{\mathcal{L}}(S,\hat{W})\Big{)}\Big{|}\Big{]}\leq\epsilon/2.\] (6)

The theorem is proved in Appendix E.6. In the above result, it is easy to replace the expectation with respect to \(P_{W|S}\) with any arbitrary expectations, as it is common in PAC-Bayes bounds and used for example in [11].

Next, we state a tail bound in terms of the function \(h_{D}(\cdot,\cdot)\), defined in (5).

**Theorem 6**.: _Consider the learning framework of Section 2. Let \(\mathbf{Q}\) be any fixed type-II symmetric prior on \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\) that could depend on \((\mathbf{X},\mathbf{Y},\mathbf{X}^{\prime},\mathbf{Y}^{\prime})\). Then, for any \(\delta\in\mathbb{R}^{+}\), with probability at least \((1-\delta)\) over choices of \((S,S^{\prime},W)\sim P_{S^{\prime}}P_{S,W}\), it holds that_

\[nh_{D}\Big{(}\hat{\mathcal{L}}(S^{\prime},W),\hat{\mathcal{L}}(S,W)\Big{)} \leq D_{KL}\bigg{(}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{\mathbf{Y}},\hat{ \mathbf{Y}}^{\prime}|\mathbf{X},\mathbf{X}^{\prime},W)\bigg{\|}\mathbf{Q} \bigg{)}+\log(n/\delta).\]

This theorem is proved in Appendix E.7. In particular, when \(\hat{\mathcal{L}}(S,W)=0\), using Lemma 1 conclude that, this result yields that with probability at least \((1-\delta)\),

\[\hat{\mathcal{L}}(S^{\prime},W)\leq \frac{D_{KL}\bigg{(}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{ \mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X},\mathbf{X}^{\prime},W)\bigg{\|} \mathbf{Q}\bigg{)}+\log(n/\delta)}{n}.\]

### Tail bounds on the generalization error in terms of latent variable complexity

In this section, we present a tail generalization bound for the two-step learning setup of Section 3 that highlights the relevance of latent variable compressibility to generalization error.

**Theorem 7**.: _Consider a \(K\)-classification learning task with the above defined framework. Let \(\mathbf{Q}\) be a type-III symmetric conditional prior over \(U^{2n}\) given \(X^{2n},Y^{2n}\) and \(W_{e}\in\mathcal{W}_{e}\), namely, \(\mathbf{Q}\Big{(}(u_{\pi(1)},\ldots,u_{\pi(2n)})|(x_{1},\ldots,x_{2n}),(y_{1}, \ldots,y_{2n}),w\Big{)}\) remains the same for all permutations \(\pi\colon[2n]\mapsto[2n]\) that preserves the label, i.e., \(y_{\pi(i)}=y_{i}\) for \(i\in[n]\). Then, for any \(\lambda\in\mathbb{R}^{+}\),_

**i.** _with probability at least_ \((1-\delta)\) _over choices of_ \((S,S^{\prime},W)\sim P_{S^{\prime}}P_{S,W}\)_,_

\[\hat{\mathcal{L}}(S^{\prime},W)-\hat{\mathcal{L}}(S,W)\leq\frac{D_{KL}\Big{(}P_ {U|X,W_{e}}^{\otimes 2n}(\mathbf{U},\mathbf{U}^{\prime}|\mathbf{X},\mathbf{X}^{ \prime},W_{e})\Big{\|}\mathbf{Q}\Big{)}+(K+2)/2+\log(1/\delta)}{\lambda}+\frac {2\lambda}{n},\]
**ii.** _with probability at least_ \((1-\delta)\) _over choices of_ \((S,S^{\prime},W)\sim P_{S^{\prime}}P_{S,W}\)_,_

\[\mathrm{gen}(S,W)\leq \frac{D_{KL}\Big{(}P_{U|X,W_{e}}^{\otimes 2n}(\mathbf{U},\mathbf{U}^{ \prime}|\mathbf{X},\mathbf{X}^{\prime},W_{e})\Big{\|}\mathbf{Q}\Big{)}+(K+2)/ 2+\log(2/\delta)}{\lambda}+\frac{2\lambda}{n}+\sqrt{\frac{\log(2/\delta)}{n}}.\]

Note that the second part of the theorem can be easily derived from the first part using Hoeffding's inequality. We provide the proof of the first part in Appendix E.8.

## Appendix B Related works

In this section, we discuss some of the related works in more detail.

### PAC-MDL framework of Blum and Langford

Here, we recall the PAC-MDL framework of [1] which, therein, was introduced in the form of a (compression) game between two agents, Alice and Bob. Alice has access to both a labeled training set \(S=(X^{n},Y^{n})\), consisting of \(n\) labeled samples, and a test set \(S^{\prime}=X^{\prime n}\), consisting of \(n\) unlabeled samples, all drawn independently from \(\mu\).5 Bob has available just the test set and the unlabeled version of the training set, _i.e.,_\((X^{n},X^{\prime n})\), ordered in some predefined (e.g., lexicographic) order known to both agents. Hereafter we denote the ordered vector of labeled and unlabeled samples as \(\pi(X^{n},X^{\prime n})\). It is assumed that by observing the vector \(\pi(X^{n},X^{\prime n})\) Bob cannot know which samples of it are from the training set and which are from the test set. The goal of Alice is to communicate the labels \(\pi(Y^{n},Y^{\prime n})\) to Bob using as few bits as possible. 6 To this end, let \(\mathcal{E}\colon\mathcal{Z}^{n}\times\mathcal{X}^{n}\to\{0,1\}\)* be a mapping (encoder) used by Alice and \(\sigma\colon=\mathcal{E}(S,X^{\prime n})\) the string transmitted to Bob. The goal of Bob is to guess the labels of both sets \(X^{n}\) and \(X^{\prime n}\); and does so by running a decoder mapping \(\mathcal{D}\colon\mathcal{X}^{2n}\times\{0,1\}\)* \(\mathcal{Y}^{2n}\). That is, Bob forms an estimate of the true labels as \(\mathcal{D}(\pi(X^{n},X^{\prime n}),\sigma)\). In this context, the _empirical risk_ is measured as \(\hat{\mathcal{L}}(\sigma,s,s^{\prime}):=\frac{1}{n}\sum_{i\in[n]}\frac{1}{ \lfloor y_{i}\neq y_{i}\rfloor}\) and a _proxy test risk_ is measured as \(\mathcal{L}(\sigma,s,s^{\prime}):=\frac{1}{n}\sum_{i\in[n]}\frac{1}{\lfloor y _{i}^{\prime}\neq y_{i}^{\prime}\rfloor}\). Essentially, the PAC-MDL bound of [1] can be seen as a generalized version of Occam's-Razor theorem [15, 1] which states that if one can explain (or encode) the labels of a set of \(n\) training samples by a hypothesis that can be described using only \(|\sigma|\ll n\) bits then this guarantees that this hypothesis generalizes well for unseen samples.

Footnote 5: For simplicity, we assume here that the training and test sets \(S\) and \(S^{\prime}\) have identical sizes, i.e., \(|S|=|S^{\prime}|\); but all the results that will follow extend easily to the case of \(|S|\neq|S^{\prime}|\).

Footnote 6: Note that Alice does not know the true labels \(Y^{\prime n}\) of the test samples \(X^{\prime n}\) and can only estimate them as \(\hat{Y}^{\prime n}\).

**Theorem 8** ([1, Theorem 6]).: _For any prior distribution \(\mathbf{Q}(\sigma)\) of \(\sigma\) and any \(\delta>0\), with probability at least \(1-\delta\) over the choices of \(S,S^{\prime}\sim P_{S}P_{S^{\prime}}\), we have:_

\[\forall\sigma\colon n\mathcal{L}(\sigma,S,S^{\prime})\leq b_{\max}\Big{(}n, \hat{\mathcal{L}}(\sigma,S,S^{\prime}),\mathbf{Q}(\sigma)\delta\Big{)},\] (7)

_where_

\[b_{\max}\Big{(}n,\frac{a}{n},\delta\Big{)}:=\max\{b\colon\text{ Bucket}(n,a,b)\geq\delta\},\] (8)

_and_

\[\text{Bucket}(n,a,b):=\sum_{c\in[b,a+b]}\frac{\binom{n}{c}\binom{n}{a+b-c}}{ \binom{2n}{a+b}}.\] (9)

The result is made more explicit for the so-called realizable case, _i.e.,_ when \(\hat{\mathcal{L}}(\sigma,S,S^{\prime})=0\). For this case, we have [1, Corollary 3]

\[\mathbb{P}\Big{(}\forall\sigma\colon\hat{\mathcal{L}}(\sigma,S,S^{\prime})>0 \text{ or }\mathcal{L}(\sigma,S,S^{\prime})\leq(|\sigma|+\log_{2}(1/\delta))/n\Big{)}>1-\delta,\]

where \(|\sigma|\) denotes the size in bits of the string \(\sigma\) using some a priori fixed codewords. The result clearly shows that if the string \(\sigma\) can be sent using few bits then the algorithm generalizes well. Also, the approach can be used in order to establish similar results for the VC-dimension and PAC-Bayes bounds.

We emphasize that our construction is in sharp contrast with an adaptation of the above-described approach of Blum and Langford [1] in which one would reveal both labeled and unlabeled samples \((S^{m},X^{\prime mn})\) to Alice and only unlabeled samples \((X^{mn},X^{\prime mn})\) to Bob (both in a predefined order). We, however, in our proposed approach in Section 2.1, reveal the vector \(\mathsf{3}^{2mn}\), containing \((S^{m},S^{\prime m})\) in a predefined order to both. Furthermore, we emphasize that the goal in our approach is not to recover the labels \(\mathfrak{Y}^{2mn}\), but the predictions \(\mathfrak{Y}^{2mn}\) as produced by the picked models or hypotheses \(W^{m}\).

On another note, we remark that the interpretation of \(R\), which appeared in the size of the codebook in our approach, _i.e.,_\(|\hat{\mathcal{Y}}_{m}|\leq e^{mR}\), corresponds to the average number of bits (per dataset \(S\)) that is needed to send a compressed version of the string \(\sigma\) of [1]. As such, for the fixed-size codebook explained in Section 2.1 and when \(|\sigma|\) is constant, \(R\leq|\sigma|\). Similar relations hold in general, by considering the _variable-size_ codebook.

### On the generalization bounds of [21]

After the initial submission of our work, another work [21] appeared on arXiv, accepted at ICML 2023, that provides an upper bound on the generalization error of representation learning algorithms. As claimed, this result justifies the benefits of the information bottleneck principle, by relating IB to the generalization error. The results are provided for the multi-layer neural networks and for different choices of latent variables corresponding to the output of different layers. Here, to adapt the results to the setup of this paper, we only consider the output of the encoder layer as the latent variable and adapt correspondingly the notations of the results in [21] to our notations.

With the adapted notations, [21] claims to bound the generalization error of a representation learning algorithm "roughly by

\[\tilde{\mathcal{O}}\Bigg{(}\sqrt{\frac{I(X;U|Y)+1}{n}}\Bigg{)}.\text{''}\]

However, firstly, since this bound is in terms of the mutual information function, the critics on the relation of mutual information and generalization error, discussed in [20, 21, 22, 23] and provided also in the "Critics to IB" section of the introduction (Section 1), are valid for the results of [21], as well. More importantly, we could not conclude the reported order-wise behavior from [21, Theorem 2]. Using the notations of our work, their result states that with probability at least \(1-\delta\) over training data \(S\), the generalization error is bounded by

\[G_{3}\sqrt{\frac{(I(X;U|Y)+I(W_{e};S))\ln(2)+\hat{\mathcal{G}}_{2}}{n}}+\frac {G_{1}}{\sqrt{n}},\]

where the constants \(G_{1}\), \(\hat{\mathcal{G}}_{2}\), and \(G_{3}\) are defined in [21, Appendix E.1] and claimed that \(\hat{\mathcal{G}}_{2}\sim\tilde{\mathcal{O}}(1)\) and \(G_{3}\sim\tilde{\mathcal{O}}(1)\), as \(n\to\infty\). However, we were unable to resolve the following concerns regarding this bound.

1. Firstly, it is not clear how \(I(W_{e};S)\) is considered to behave as \(\tilde{\mathcal{O}}(1)\), when \(n\to\infty\). Indeed, the size of dataset \(S\) and the learning algorithm \(P_{W_{e}|S}\) change as \(n\to\infty\).
2. Secondly, by referring to the definitions of the constants in [21, Appendix E.1], it can be easily verified that \(\hat{\mathcal{G}}_{2}=C_{1}+(H(U|X,Y)+H(W_{e}|S))\ln(2)\), for some non-negative constant \(C_{1}\). Hence, the bound can be re-written as \[G_{3}\sqrt{\frac{(H(U|Y)+H(W_{e}))\ln(2)+C_{1}}{n}}+\frac{G_{1}}{\sqrt{n}}.\] Thus, the bound is in terms of \(H(U|Y)+H(W_{e})\), and not \(I(X;U|Y)+I(W_{e};S)\).
3. Lastly, the term \(G_{3}\), defined in [21, Appendix E.1], is composed of \(T_{Y}\in\mathbb{N}\) elements. By [21, Lemma 2], \(T_{Y}\) behaves roughly as \(\tilde{\mathcal{O}}(2^{H(U|Y)})\). Using [21, Lemma 2], it is shown that \(G_{3}\) is bounded. However, it _seems_ that this is _only_ shown by assuming that the \(T_{Y}\) terms of \(G_{3}\) satisfy certain conditions related to the decreasing rate of their ordered values. This assumption however, may not hold in general, and hence \(G_{3}\) may behave as \(\tilde{\mathcal{O}}(T_{y})\approx\tilde{\mathcal{O}}(2^{H(U|Y)})\), which then becomes the dominant term in the bound.

## Appendix C Further clarifications and discussions

In this section, we explain our compressibility framework in more detail. Moreover, we present some intuitions on the function \(h_{D}(\cdot,\cdot)\) and give an example of how lossy compression is related to geometrical compression. Finally, we discuss some potential future works.

### Compressibility framework

In this section, we propose various notions of the compressibility used in Section 2.

#### c.1.1 Lossless fixed-size compressibility

We start by recalling the needed elements for joint compression of a _block_ of the predicted labels. Consider \(m\) i.i.d. pairs of train and test datasets \(S_{j}:=(Z_{j,1},\ldots,Z_{j,n})\) and \(S^{\prime}_{j}:=(Z^{\prime}_{j,1},\ldots,Z^{\prime}_{j,n})\)where \(Z_{j,i}=(X_{j,i},Y_{j,i})\) and \(Z^{\prime}_{j,i}=(X^{\prime}_{j,i},Y^{\prime}_{j,i})\). Let \(S^{m}=(S_{1},\ldots,S_{m})\), \(S^{\prime m}=(S^{\prime}_{1},\ldots,S^{\prime}_{m})\) and \(W^{m}:=(W_{1},\ldots,W_{m})\), where \(W_{j}\sim P_{W_{j}|S_{j}}\). It is important to note that the model or hypothesis \(W_{j}\) is chosen based on the dataset \(S_{j}\) only and the introduction of the (_ghost_) dataset \(S^{\prime}_{j}\) is here only for the sake of the analysis, similarly as it was done in the derivation of the PAC-MDL bound of [1] or in Rademacher sample complexity or the conditional mutual information of [15]. Denote the predicted labels using model \(W_{i}\) for inputs \(X_{i,j}\) and \(X^{\prime}_{j,i}\) as \(\hat{Y}_{j,i}\) and \(\hat{Y}^{\prime}_{j,i}\), for \(j\in[m]\) and \(i\in[n]\).

Moreover, let \(\mathfrak{Z}^{2mn}\in\mathcal{Z}^{2mn}\) denote a rearrangement of the elements of \((S^{m},S^{\prime m})\) such that while the entire sets \(S^{m}\) and \(S^{\prime m}\) are known by having the vector \(\mathfrak{Z}^{2mn}\), but they are arranged in an order that one cannot distinguish whether a given sample \(z\) is from \(S^{m}\) or \(S^{\prime m}\). Accordingly, denote the rearranged versions of \((Y^{mn},Y^{\prime mn})\) and \((\hat{Y}^{mn},\hat{Y}^{\prime mn})\) respectively by \(\mathfrak{Y}^{2mn}\) and \(\mathfrak{Y}^{2mn}\). In Sections 2.2 and 2.3, we presented two methods for such rearrangement. As mentioned before, in both methods, the rearrangement of \((S^{m},S^{\prime m})\) as \(\mathfrak{Z}^{2mn}=\{\mathfrak{Z}_{j,i}\}_{j\in[m],i\in[n]}\) is done in two steps: First, for each \(j\in[m]\), we rearrange indistinguishably \((S_{j},S^{\prime}_{j})\) as \(\mathfrak{Z})j^{2n}\), and then we concatenated the \(m\) resulting \(\{\mathfrak{Z}^{2n}_{j}\}_{j\in[m]}\) to achieve \(\mathfrak{Z}^{2mn}\). Now, we explain how to rearrange indistinguishably a given \((S,S^{\prime})\) as \(\mathfrak{Z}^{2n}\) using two methods:

* Section 2.2: Let \(\mathbf{J}=(J_{1},\ldots,J_{n})\) be a vector of \(n\) i.i.d. Bernoulli\((\frac{1}{2})\) random variables \(J_{i}\in\{i,i+n\}\), \(i\in[n]\). Denote by \(\mathbf{J}=(J^{c}_{1},\ldots,J^{c}_{n})\) the vector of the complementary choices in \(\mathbf{J}\), _i.e._, \(J_{i}\cup J^{c}_{i}=\{i,i+n\}\). Now, for each \(i\in[n]\), let \((\mathfrak{Z}_{J_{i}},\mathfrak{Z}_{J^{c}_{i}})\) to be equal to \((Z_{i},Z^{\prime}_{i})\).
* Section 2.3: Let \(\mathbf{T}=\{T_{1},\ldots,T_{n}\}\), be a random set obtained by picking uniformly \(n\) indices from \(\{1,\ldots,2n\}\), without replacement. Note that in contrast to \(\mathbf{J}\) which had i.i.d. components, here the components of \(\mathbf{T}\) are dependent. let \(\mathbf{T}^{c}=\{1,\ldots,2n\}\backslash\mathbf{T}\) be the complement of \(\mathbf{T}\), having the elements \(\mathbf{T}^{c}=\{T^{c}_{1},\ldots,T^{c}_{n}\}\). Now, for each \(i\in[n]\), let \((\mathfrak{Z}_{T_{i}},\mathfrak{Z}_{T_{i}})=(Z_{i},Z^{\prime}_{i})\).

Based on the above, hereafter we study the _compressibility_ of the sorted model-predicted labels \(\mathfrak{Y}^{2mn}\), from an information-theoretic point of view. The rationale is that, in accordance with "Occam's Razor" theorem [14, 1], since the (rearranged) predicted-labels vector \(\mathfrak{Y}^{2mn}\) agrees mostly with the true labels \(\mathfrak{Y}^{2mn}\) on the dataset \(S^{m}\), if \(\mathfrak{Y}^{2mn}\) can be described using only a few bits, this guarantees that the model \(W\) generalizes well. As it will be shown from the result that will follow, instead of the size of the message needed to be sent in the Blum-Langford approach (see Appendix B.1), a new quantity emerges in our work as a measure of the compressibility of \(\mathfrak{Y}^{2mn}\): the relative entropy of the joint conditional \(P(\hat{Y}^{n},\hat{Y}^{m}|Y^{n},Y^{\prime m})\) and a (symmetric) conditional prior \(\mathbf{Q}\) over \(\hat{\mathcal{Y}}^{2n}\) given \(Y^{2n}\). Depending on the way we rearrange \((S^{m},S^{\prime m})\), the type-I or type-II symmetric priors are needed to be applied.

Let \(R\in\mathbb{R}^{+}\). In our block-coding rate-distortion theoretic framework, \(R\) is said to be _achievable_ if there exists a compression codebook of size \(\approx e^{mR}\), fixed a priori, which _covers_ the space spanned by the model-predicted labels \(\mathfrak{Y}^{2mn}\) with high probability. Specifically, if there exists a sequence of label books \(\{\hat{\mathcal{Y}}_{m}\}_{m\in\mathbb{N}}\), where \(\hat{\mathcal{Y}}_{m}:=\{\hat{\mathbf{y}}[r],r\in[l_{m}]\}\subseteq\mathfrak{Y} ^{2mn}\), \(l_{m}\in\mathbb{N}\), \(\hat{\mathbf{y}}[r]=(\hat{y}_{1}[r],\ldots,\hat{\mathbf{y}}_{m}[r])\) and \(\hat{\mathbf{y}}_{j}[r]=(\hat{y}_{j,1}[r],\ldots,\hat{y}_{j,2n}[r])\in \mathcal{Y}^{2n}\) such that:

1. \(l_{m}\leq e^{mR}\),
2. there exist a sequence \(\{\delta_{m}\}_{m\in\mathbb{N}}\) for which \(\lim_{m\to\infty}\delta_{m}=0\) and with probability at least \((1-\delta_{m})\) over the choices of \(S^{m}\), \(S^{\prime m}\), and \(\mathbf{J}\) or \(\mathbf{T}\), one can find at least one index \(r\in[l_{m}]\) whose associated \(\hat{\mathbf{y}}[r]\) exactly equals \(\mathfrak{Y}^{2mn}\).

The above framework, explained in Section 2.1, is called _fixed-size compressibility_, as the size of the codebook does not depend on a given dataset. This type of compressibility is useful for establishing "data-independent" bounds,7_e.g._, bounds on the expectation of the generalization error. This is also called _lossless_, since we look for a codeword that is exactly equal to \(\mathfrak{Y}^{2mn}\).

Footnote 7: Here, we emphasize that by data-dependent bounds we refer to bounds that depend on the particular sample of the input data at hand, rather than for example just on the distribution of the data which is unknown.

#### c.1.2 Lossy fixed-size compressibility

As already stated, the above approach and results can be extended to any bounded loss function and continuous variables \(Y\) and \(\hat{Y}\) (see for example Section 3 where continuous latent variables are studied). In this case, a standard result of rate-distortion theory states that to cover _losslessly_ and reliably \(\mathfrak{Y}^{2mn}\), \(R\) should be infinity, which makes the framework and resulting bounds vacuous. However, the approach can be easily extended to include the _lossy compression_ of the labels; in the same spirit as done in [SGRS22] for the hypothesis compression. More precisely, for a given distortion threshold \(\epsilon\in\mathbb{R}\), one can consider the same way of codebook generation, but with the following conditions:

1. \(l_{m}\leq e^{mR}\),
2. there exist a sequence \(\{\delta_{m}\}_{m\in\mathbb{N}}\) for which \(\lim_{m\to\infty}\delta_{m}=0\) and with probability at least \((1-\delta_{m})\) over the choices of \(S^{m}\), \(S^{\prime m}\), and \(\mathbf{J}\) or \(\mathbf{T}\), one can find at least one index \(r\in[l_{m}]\) whose associated \(\mathfrak{H}[r]\) satisfies: where it is assumed that the set of indices \(\{(j,t_{i})\}_{j\in[m],i\in[n]}\) and \(\{(j,t_{i}^{c})\}_{j\in[m],i\in[n]}\) are the sets of indices in the rearranged sequence that belong to the training and ghost datasets, respectively. Here, \(\bigcup_{i\in[n]}(t_{i}\cup t_{i}^{c})=\{1,\ldots,2n\}\).

In the case of lossy compressibility, the block-coding technique brings another advantage in addition to the advantages discussed for the lossless case: it allows to consider the _average_ distortion criterion, instead of _worst-case_ distortion criterion. Please refer to [SGRS22] for further discussion on this.

#### c.1.3 Variable-size compressibility

In the frameworks, explained in previous sections, for a given \(m\), the size of the codebook \(e^{mR}\) is fixed. Hence, the bounds derived using this framework, cannot depend on a particular training dataset at hand. In particular, while above mentioned frameworks are useful to establish bounds on the expectation of the generalization error, they are not appropriate for establishing _data-dependent_ tail bounds. To overcome this issue, in [SZ23], a "variable-size" compressibility is proposed, in which, one is allowed to search for a suitable \(\mathfrak{H}[r]\) only in the _first_\(e^{mR_{(S,S^{\prime},W)}}\) elements of this codebook, where \(R_{(S,S^{\prime},W)}\) is the data-dependent term that will consequently appear in the resulting bound using this approach. For instance, the term \(R_{(S,S^{\prime},W)}\) in Part.ii of Theorem 1 is exactly the KL-divergence term appearing in the bound.

### Intuition about the function \(h_{D}\)

In Section 2.3, we have provided the bound on the generalization error in terms of the function \(h_{D}(x;x^{\prime})\colon[0,1]\times[0,1]\to[0,2]\), defined as:

\[h_{D}(x,x^{\prime})\colon=2h_{b}\Big{(}\frac{x+x^{\prime}}{2}\Big{)}-h_{b}(x)- h_{b}(x^{\prime}),\]

where \(h_{b}(x)\coloneqq-x\log_{2}(x)-(1-x)\log_{2}(1-x)\). As mentioned, \(\frac{1}{2}h_{D}(x,x^{\prime})\) is equal to the Jensen-Shannon divergence between two binary Bernoulli distributions with parameters \(x\) and \(x^{\prime}\).

In this section, we provide an intuition about this function, by showing its relation with the combinatorial term that appeared in [BL03]. Recall that the main result of [BL03], _i.e.,_ Theorem 6 therein (re-stated in the Appendix B.1), is established in terms of \(b_{\max}(n,a/n,\delta)\) defined as follows:

\[b_{\max}\Big{(}n,\frac{a}{n},\delta\Big{)}\coloneqq\max\{b\colon\text{Bucket}( n,a,b)\geq\delta\},\]

where

\[\text{Bucket}(n,a,b)\coloneqq\sum_{c\in[b,a+b]}\frac{\binom{n}{c}\binom{n}{a+ b-c}}{\binom{2n}{a+b}},\]

where \([b,a+b]\subset\mathbb{N}\) denotes the integer interval and \(c\in\mathbb{N}\).

Now, the intuition about the function \(h_{D}(x,x^{\prime})\) is as follows: by Stirling's formula, we have that

\[\frac{\binom{mn}{mn}\binom{mn}{ma+mb-mt}}{\binom{2mn}{ma+mb}}\longrightarrow e ^{-mnh_{D}\left(\frac{1}{n},\frac{a+b-t}{n}\right)}\] (10)

as \(m\to\infty\). Hence, for large (infinite) values of \(m\) we have that the function \(\text{Bucket}(mn,ma,mb)\) of [BL03] is dominated by

\[\max_{t\in[b,a+b]}e^{-mnh_{D}\left(\frac{t}{n},\frac{a+b-t}{n}\right)},\] (11)where \([\![b,a+b]\!]\subset\mathbb{R}\) denotes the real interval and \(t\in\mathbb{R}\). Furthermore and as a consequence

\[\frac{1}{m}b_{\max}\Big{(}mn,\frac{a}{n},\delta^{m}\Big{)}\longrightarrow\max \Big{\{}b\colon\min_{t\in[\![b,a+b]\!]}nh_{D}\bigg{(}\frac{t}{n},\frac{a+b-t}{n }\bigg{)}\leqslant\log(1/\delta)\Big{\}},\] (12)

as \(m\to\infty\).

It can be observed that by considering _block-coding_ and letting \(m\to\infty\), the intractable combinatorial terms appeared in [1] can be expressed in terms of the function \(h_{D}(\cdot,\cdot)\).

### On relation between lossy compressibility and geometric compression

The experimental studies suggest the existence of a relation between generalization performance and _geometrical compression_[1]. Geometrical compression occurs when the latent variables are concentrated around a limited number of clusters. Please refer to [1, Fig. 2] for a visual representation. As mentioned both in Section 3, _lossy compression_ provides an interpretation of the _geometric compression_ of [1, GVDBG\({}^{+}\)19] where latent variables are concentrated around some constellation points. In this section, we provide a simple example of that.

In geometrical compression, the latent variables \(U\sim P_{U|X}\) of \(X\) are distinct but concentrated around a few "centers". Hence, while in this case, the "lossless compression" captured by \(I(U;X)\) becomes very large or infinite, the "lossy compression" captured by rate-distortion may be very small, as these scattered latent variables around the centers can be seen as a lossy version of the mappings from \(X\) to one of these centers, with some small distortion. To give a simple example, suppose \(X\in[0,1]\), and if \(X<0.5\), then \(U=-1+X/5\), and if \(X>0.5\), then \(U=1+X/5\). In this case while \(I(U;X)=\infty\), a simple lossy mapping \(P_{\hat{U}|X}\) with average distortion less than \(0.05\) can be found such that \(I(\hat{U};X)=1\). This explains the geometrical compression observation.

### Conclusion and future directions

In this paper, inspired by [1], we developed a compressibility framework that we used to establish various bounds on the generalization error of the stochastic learning algorithms. The bounds are expressed in terms of the newly defined notion of "minimum description length"(MDL) of the predicted labels or the latent variables. The new notion is expressed in terms of a "symmetric" prior, where the "symmetry" is defined and used in three different ways. The type-I and type-II symmetries are useful to establish the bounds in terms of MDL of the predicted labels. The former one, in particular, shows a clear connection between the seemingly different approaches of [1] and CMI [15, HRVSG21]. The type-III symmetry is used to derive the main result of this paper which is a bound in terms of MDL of latent variables. The results of the last section suggest that the generalization error in representation learning is related to the newly defined MDL of latent variables. Unlike mutual information, which captures the "information leakage", the new notion of MDL captures the simplicity and structure of the encoder. These insights are then partly exploited to propose new regularizers in terms of new "data-dependent" priors. The performed simulations show the advantage of these priors over the classical ones used in VIB.

Our proposed framework and obtained results also open up several future research directions. In the following, we discuss some of those directions.

* In Part i. of Theorem 1, we proposed a tail bound, which unlike the classical PAC-Bayes bound with a single draw from the posterior [13], does not contain a "disintegrated" term. As explained, this is due to the choice of the loss function, which inherently contains an expectation term. It would be interesting to consider a loss function that captures the "one-shot" prediction of the performance and to develop a disintegrated bound for such a loss function.
* One of the contributions of this work has been to establish "rate-distortion theoretic" bounds by using a lossy compressibility framework. For example, the established bound in Theorem 4, contains an infimum over all "compressed algorithms" \(P_{\hat{W}_{e}|S}\) that satisfy some distortion criterion. Note that this implies that this bound holds for "any" choice of eligible \(P_{\hat{W}_{e}|S}\), and thus, taking the infimum to obtain a "valid bound" is not necessarily required. Thus, any simple technique that adds noise or applies parameter quantization can be considered. However, besides these general approaches, an interesting direction is how to find an "optimal" \(P_{\hat{W}_{e}|S}\). Perhaps, the approach taken in [12], which uses a combination of the MINE estimator [1] and the "SFRL" [11] could be adapted to our setup.

* Another potentially valuable direction would be to compute the bounds _analytically_ in simple setups, such as two-layer neural networks with Gaussian data, and compare the results with the existing results such as [14]. Also, it is instructive to study _numerically_ the geometry and MDL of the latent variables for different network architectures, _e.g._, FCN versus CNN. This may lead to a new understanding of how and why some architectures produce "better" representations.
* Inspired by our results, we have proposed some simple "data-dependent" priors. While the proposed priors show improvements over the classical priors, they are limited to priors that can be factorized as products of some Gaussian distributions. Thus, they only "partially" capture the joint compressibility of the latent variables (and hence the encoder structure). We imagine that more "appropriate" priors can be proposed that capture better the structure of the encoder.
* In this paper, we partially discuss the intuition behind the importance of the "structure" of the encoder. However, the explicit effect of the "structure" on the geometry of latent variables and predictions needs to be investigated, as partially shown in [1].
* Finally, we emphasize that in representation learning, one is interested in extracting good representations that are suitable in terms of generalization error for _multiple_ learning tasks, simultaneously. It would be interesting to extend our results to such more realistic settings.

## Appendix D Details of the experiments

In this section, we present the details of the experiments presented in Section 4.

### VIB objective function

The traditional information bottleneck (IB) approach [13, 10] for training representation learning models, and particularly its variational implementation (VIB) by [1], considers a fixed data-independent prior \(\mathbf{Q}=Q^{\otimes 2n}\). Then, for some Lagrange multiplier \(\beta>0\), the VIB approach minimizes

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{\{}\beta D_{KL}\Bigl{[}P_{U|X,W_{e}}(U_{i}|x_{ i},W_{e})|Q\Bigr{]}-\,\mathbb{E}_{U_{i}\sim P_{U|X,W_{e}}(U_{i}|x_{i},W_{e})} \Bigl{[}\log P_{\hat{Y}|U,W_{d}}(y_{i}|U_{i},W_{d})\Bigr{]}\Bigr{\}},\]

using the reparametrization trick of [10]. As can be noticed, the first term, which acts as a _regularizer_, only takes into account the training dataset part of \(D_{KL}\Bigl{(}P_{U|X,W_{e}}^{\otimes 2n}(\mathbf{U},\mathbf{U}^{\prime}| \mathbf{X},\mathbf{X}^{\prime},W_{e})\Bigr{|}\mathbf{Q}\Bigr{)}\), as the test set is not available. The second term attempts to maximize the _relevance_ of \(U\) for prediction. Intuitively, it seeks to find the best decoder among the possible choices, i.e., the one that minimizes the empirical risk.

A popular choice for \(Q\) is the multi-dimensional standard Gaussian distribution \(Q=\mathcal{N}(\mathbf{0}_{m},\mathrm{I}_{m})\), where \(\mathrm{I}_{m}\) is the identity matrix, \(m\) is the dimension of the latent variable \(U\), and \(\mathbf{0}_{m}\in\mathbb{R}^{m}\) is the all zero vector. In the original implementation of [1], for each sample \(x\), the encoder generates the mean \(\mu_{x}=(\mu_{x,1},\ldots,\mu_{x,m})\in\mathbb{R}^{m}\) and variance \(\sigma_{x}^{2}=(\sigma_{x,1}^{2},\ldots,\sigma_{x,m}^{2})\in\mathbb{R}^{m}\) of the latent variable \(U\). Then, we let \(P_{U|X,W_{e}}(U|x,W_{e})=\mathcal{N}(\mu_{x},\mathrm{diag}(\sigma_{x}^{2}))\), where \(\mathrm{diag}(\sigma_{x}^{2})\in\mathbb{R}^{m\times m}\) denotes a diagonal matrix whose diagonal elements are denoted by the vector \(\sigma_{x}^{2}\). This means that the latent variable for the input \(x\) is generated according to \(U\sim\mathcal{N}(\mu_{x},\mathrm{diag}(\sigma_{x}^{2}))\). Hence, the objective function to minimize becomes

\[\frac{1}{b}\sum_{i=1}^{b}\Bigl{\{}\beta\,D_{KL}\Bigl{[}\mathcal{N}(\mu_{x_{i}},\mathrm{diag}(\sigma_{x_{i}}^{2}))\|\mathcal{N}(\mathbf{0}_{m},\mathrm{I}_{m })\Bigr{]}-\,\mathbb{E}_{U_{i}\sim P_{U|X,W_{e}}(U_{i}|x_{i},W_{e})}\Bigl{[} \log P_{\hat{Y}|U,W_{d}}(y_{i}|U_{i},W_{d})\Bigr{]}\Bigr{\}},\] (13)

where \(b\) is the size of a mini-batch of training samples \(z_{1},\ldots,z_{b}\), \(z_{i}=(x_{i},y_{i})\). Moreover, (13) is repeated iteratively over multiple mini-batches until the convergence of the representation learning model.

### Lossless CDVIB objective function

We describe here another learning approach which, unlike the VIB, uses a data-dependent prior. This prior, coined Category-Dependent VIB (CDVIB) is again factorized as a product of \(2n\) scalar Gaussian priors, _i.e._, \(\mathbf{Q}=\prod_{1\in[2n]}Q_{i}\). Each of these scalar priors \(Q_{i}\) is chosen among one of the \(K\times M\) Gaussian priors (centers) - \(M\) priors per each label. More precisely, for each label \(k\in[K]\), we consider \(M\in\mathbb{N}\) priors

\[Q_{k,r}^{(t)}=\mathcal{N}\biggl{(}\bar{\mu}_{k,r}^{(t)},\mathrm{diag}\biggl{(} \bar{\sigma}_{k,r}^{(t)}{}^{2}\biggr{)}\biggr{)},\quad r\in[M],k\in[K],\]defined over \(\mathbb{R}^{m}\), where the superscript \(t\in\mathbb{N}\) represents the optimization iteration.

Unlike in the VIB approach, the mean and variance of the scalar priors are updated during the training process. Firstly, the vectors \(\bar{\mu}_{k,r}^{(t)}\) and \(\bar{\sigma}_{k,r}^{(t)}\) are initialized respectively as \(m\)-dimensional vectors of zeros and ones. Next, these initialized vectors are updated at each iteration using a procedure defined below.

For any sample \(z=(x,y)\) with label \(y=k\) and for any iteration \(t\geq 1\), let \(r\in[M]\) denote the index of the category-dependent prior (center) which is the closest to \(\mathcal{N}(\mu_{x},\mathrm{diag}(\sigma_{x}^{2}))\) in terms of the KL-divergence

\[r_{z}^{(t)}=\operatorname*{arg\,min}_{r\in[M]}D_{KL}\bigg{[}\mathcal{N}(\mu_{x },\mathrm{diag}(\sigma_{x}^{2}))\|\mathcal{N}\bigg{(}\bar{\mu}_{k,r}^{(t)}, \bar{\sigma}_{k,r}^{(t)}\bigg{)}\bigg{]}.\]

Suppose that the picked mini-batch at iteration \(t\) is \(\mathcal{B}_{t}=\{z_{1},\ldots,z_{b}\}\). For each \(k\in[K]\) and \(r\in[M]\), let

\[\mathcal{I}_{k,r}^{(t)}=\Big{\{}z_{i}\colon i\in[b],y_{i}=k,r_{z_{i}}^{(t)}=r \Big{\}},\]

and let \(b_{k,r}=|\mathcal{I}_{k,r}|\). Now, if \(b_{k,r}\neq 0\), update \(\bar{\mu}_{k,r}^{(t)}\) and \(\bar{\sigma}_{k,r}^{(t)}\) as

\[\bar{\mu}_{k,r}^{(t)} \coloneqq(1-\alpha b_{k,r})\bar{\mu}_{k,r}^{(t-1)}+\alpha\sum_{z_{ i}\in\mathcal{I}_{k,r}}\mu_{x_{i}},\] \[\bar{\sigma}_{k,r,j}^{(t)} \coloneqq\sqrt{\left(1-\alpha b_{k,r}\right)\bar{\sigma}_{k,r,j}^{ (t-1)}+\alpha\sum_{z_{i}\in\mathcal{I}_{k,r}}\sigma_{z_{i},j}^{2}},\quad j=1, \ldots,m,\]

where \(\alpha\in[0,1]\) is a coefficient that smoothens the evolution of the mean and variance, and also implicitly takes into account the effect of the ghost dataset \(S^{\prime}\) appearing in the bounds of Theorems 4 and 7. The optimal value of \(\alpha\), among others, depends on the mini-batch size \(b\) and the number of centers \(M\).8

Footnote 8: It can be shown that this choice of prior for \(M=1\) satisfies the type-III symmetry property and for \(M>1\) is an approximation of a prior that satisfies such a symmetry.

Finally, the considered objective function at iteration \(t\) in the Lossless CDVIB approach is

\[\frac{1}{b}\sum_{i=1}^{b}\bigg{\{}\beta\,D_{KL}\bigg{[} \mathcal{N}\Big{(} \mu_{x_{i}},\mathrm{diag}(\sigma_{x_{i}}^{2})\Big{)}\|\mathcal{N} \bigg{(}\bar{\mu}_{y_{i},r_{z_{i}}^{(t)}}^{(t)},\mathrm{diag}(\bar{\sigma}_{ y_{i},r_{z_{i}}^{(t)}}^{2})\Big{)}\bigg{]}\] \[-\,\mathbb{E}_{U_{i}\sim P_{U|X,W_{e}}(U_{i}|x_{i},W_{e})}\big{[} \log P_{\tilde{Y}|U,W_{d}}(y_{i}|U_{i},W_{d})\big{]}\bigg{\}}.\] (14)

### Lossy CDVIB objective function

Inspired by the lossy compression and bounds introduced in our work, we consider the MDL of the "perturbed" latent variable, while passing the un-perturbed latent variable to the decoder. More precisely, as before, we consider the log loss for evaluation of the relevance of \(U\) in the decoder, _i.e.,_

\[\mathbb{E}_{U_{i}\sim P_{U|X,W_{e}}(U_{i}|x_{i},W_{e})}\Big{[}\log P_{\tilde{Y }|U,W_{d}}(y_{i}|U_{i},W_{d})\Big{]}.\]

For the regularizer, we first consider the perturbed \(U\) as

\[\hat{U}=U+Z_{2}=\mu_{X}+Z_{2}+\sigma_{X}Z_{1}=\hat{U}_{1}+\hat{U}_{2},\] (15)

where \(Z_{1}\) and \(Z_{2}\) are independently drawn from the same distribution \(\mathcal{N}(\mathbf{0}_{m},\mathrm{I}_{m})\). Note that we chose

\[\hat{U}_{1}\coloneqq\mu_{X}+Z_{2},\qquad\hat{U}_{2}\coloneqq\sigma_{X}Z_{1}.\] (16)

Hence, given \((X,W_{e})\), \(\hat{U}_{1}\sim\mathcal{N}(\mu_{X},\mathrm{I}_{m})\) is independent from \(\hat{U}_{2}\sim\mathcal{N}(\mathbf{0}_{m},\mathrm{diag}(\sigma_{X}^{2}))\). Let us define two sets of priors \(\mathcal{Q}_{1}\coloneqq\{Q_{1,k,r}\}_{k\in[K],r\in[M]}\) over \(\hat{U}_{1}\) and \(\mathcal{Q}_{2}\coloneqq\{Q_{2,k,r}\}_{k\in[K],r\in[M]}\) over \(\hat{U}_{2}\). Next, for each \(i\in[b]\), we select two priors \(Q_{1,i}\in\mathcal{Q}_{1}\) and \(Q_{2,i}\in\mathcal{Q}_{2}\), in a manner that will become clear in the following. Denote the induced prior for \(\hat{U}=\hat{U}_{1}+\hat{U}_{2}\), where \(\hat{U}_{1}\sim\mathcal{Q}_{1,i}\) and \(\hat{U}_{2}\sim\mathcal{Q}_{2,i}\) by \(Q_{i}\). Then, the KL divergence of \(P_{\hat{U}|X,W_{e}}(\hat{U}|X,W_{e})\) and \(Q_{i}\), can be upper bounded by

\[D_{KL}\bigg{[}P_{\hat{U}|X,W_{e}}(\hat{U}|X,W_{e}) \|Q_{i}\bigg{]}\] \[\leq D_{KL}\bigg{[}P_{\hat{U}_{1}|X,W_{e}}(\hat{U}_{1}|X,W_{e})\|Q_{1, i}\bigg{]}+D_{KL}\bigg{[}P_{\hat{U}_{2}|X,W_{e}}(\hat{U}_{2}|X,W_{e})\|Q_{2,i} \bigg{]}\] \[= D_{KL}\bigg{[}\mathcal{N}(\mu_{X},\mathrm{I}_{m})\|Q_{1,i}\bigg{]} +D_{KL}\bigg{[}\mathcal{N}(\mathbf{0}_{m},\mathrm{diag}(\sigma_{X}^{2}))\|Q_{2,i} \bigg{]}.\] (17)We use this upper bound for our regularizer. To make things more formal, for each label \(k\in[K]\), we consider \(2M\) priors, \(M\in\mathbb{N}\),

\[Q^{(t)}_{1,k,r}= \mathcal{N}\Big{(}\bar{\mu}^{(t)}_{k,r},\mathrm{I}_{m}\Big{)}, \quad r\in[M],k\in[K],\] \[Q^{(t)}_{2,k,r}= \mathcal{N}\bigg{(}\mathbf{0}_{m},\mathrm{diag}\bigg{(}\bar{ \sigma}^{(t)}_{k,r}{}^{2}\bigg{)}\bigg{)},\quad r\in[M],k\in[K],\]

defined over \(\mathbb{R}^{m}\), where the superscript \(t\in\mathbb{N}\) represents the optimization iteration.

Similar to lossless CDVIB, firstly, the vectors \(\bar{\mu}^{(t)}_{k,r}\) and \(\bar{\sigma}^{(t)}_{k,r}{}^{2}\) are initialized respectively as \(m\)-dimensional vectors of zeros and ones. Next, these initialized vectors are updated at each iteration using a procedure defined below.

For any sample \(z=(x,y)\) with label \(y=k\) and for any iteration \(t\geq 1\), let \(r\in[M]\) denote the index of the category-dependent prior (center) which has the smallest distance to \((Q^{(t)}_{1,k,r},Q^{(t)}_{2,k,r})\) in a sense that minimizes the RHS of (17). In other words, \(r_{z}^{(t)}\) is equal to

\[\underset{r\in[M]}{\arg\min}\bigg{\{}D_{KL}\bigg{[}\mathcal{N}( \mu_{x},\mathrm{I}_{m})|\mathcal{N}\Big{(}\bar{\mu}^{(t)}_{k,r},\mathrm{I}_{m} \Big{)}\bigg{]}+D_{KL}\bigg{[}\mathcal{N}(\mathbf{0}_{m},\mathrm{diag}(\sigma_{ x}^{2}))|\mathcal{N}\bigg{(}\mathbf{0}_{m},\mathrm{diag}\bigg{(}\bar{ \sigma}^{(t)}_{k,r}{}^{2}\bigg{)}\bigg{)}\bigg{]}\bigg{\}}.\]

Then, the mean and variances of the priors are updated exactly similarly to Lossless CDVIB. For completeness, we repeat this procedure here. Suppose that the picked mini-batch at iteration \(t\) is \(\mathcal{B}_{t}=\{z_{1},\ldots,z_{b}\}\). For each \(k\in[K]\) and \(r\in[M]\), let

\[\mathcal{I}^{(t)}_{k,r}=\Big{\{}z_{i}\colon i\in[b],y_{i}=k,r_{z_{i}}^{(t)}=r \Big{\}},\]

and let \(b_{k,r}=|\mathcal{I}_{k,r}|\). Now, if \(b_{k,r}\neq 0\), update \(\bar{\mu}^{(t)}_{k,r}\) and \(\bar{\sigma}^{(t)}_{k,r}\) as

\[\bar{\mu}^{(t)}_{k,r}\coloneqq \big{(}1-\alpha b_{k,r}\big{)}\bar{\mu}^{(t-1)}_{k,r}+\alpha\sum \limits_{z_{i}\in\mathcal{I}_{k,r}}\mu_{x_{i}},\] \[\bar{\sigma}^{(t)}_{k,r,j}\coloneqq \sqrt{\big{(}1-\alpha b_{k,r}\big{)}\,\bar{\sigma}^{(t-1)}_{k,r,j} {}^{2}+\alpha\sum\limits_{z_{i}\in\mathcal{I}_{k,r}}\sigma^{2}_{z_{i},j}}, \quad j=1,\ldots,m,\]

where \(\alpha\in[0,1]\) is a coefficient that smoothens the evolution of the mean and variance, and also implicitly takes into account the effect of the ghost dataset \(S^{\prime}\) appearing in the bounds of Theorems 4 and 7. The optimal value of \(\alpha\), among others, depends on the mini-batch size \(b\) and the number of centers \(M\).

Finally, the considered objective function at iteration \(t\) in the Lossy CDVIB approach is

\[\frac{1}{b}\sum\limits_{i=1}^{b}\bigg{\{}\beta D_{KL}\bigg{[} \mathcal{N}(\mu_{x_{i}},\mathrm{I}_{m})|\mathcal{N}\Big{(}\bar{\mu}^{(t)}_{y_{ i},r_{i_{i}}^{(t)}},1_{m}\Big{)}\bigg{]}+\beta D_{KL}\bigg{[}\mathcal{N}( \mathbf{0}_{m},\mathrm{diag}(\sigma_{x_{i}}^{2}))|\mathcal{N}\Big{(}\mathbf{0 }_{m},\mathrm{diag}\left(\bar{\sigma}^{(t)}_{y_{i},r_{i_{i}}^{(t)}}{}^{2} \right)\Big{)}\bigg{]}\] \[-\ \mathbb{E}_{U_{i}\sim P_{U|X,W_{c}}(U_{i}|x_{i},W_{c})}\Big{[} \log P_{\bar{Y}|U,W_{d}}(y_{i}|U_{i},W_{d})\Big{]}\bigg{\}}.\] (18)

### Datasets

In the experiments, we used CIFAR10 [10]. The full dataset was split into a training set with 50,000 labeled images and a validation set with 10,000 labeled images, all of them of size \(32\times 32\times 3\). The input images were scaled to have most values between 0 and 1 before being fed to the network.

### Architecture details

The model architecture considered in our experiments is detailed in Table 1. The encoder part of our two-step prediction model is a convolutional network consisting of four convolutional layers followed by two linear layers. We use max-pooling and a LeakyReLU activation function with a negative slope coefficient equal to \(0.1\). The encoder takes as input re-scaled images and produces parameters \(\mu_{x}\) and variance \(\sigma_{x}^{2}\) of the latent variable of dimension \(m=64\). The latent samples are generated using the reparameterization trick of [10]. Next, the produced latent samples are processed by a decoder consisting of one linear layer with a softmax activation function. The decoder outputs a soft class prediction.

Similarly, as in [4], our evaluated encoder is complex enough in order to make it close to "a universal function approximator". On the other hand, we use a simple decoder as in [1] in order to reduce spurious regularization introduced by the high decoder's complexity and hence to highlight the benefits of our regularizer in terms of generalization performance.

### Implementation and training details

Our prediction model was trained using PyTorch [19] and a GPU Tesla P100 with CUDA 11.0. All weights were initialized using the default PyTorch Xavier initialization scheme [1] with all biases initialized to zero. The Adam optimizer [1] (\(\beta_{1}=0.5\), \(\beta_{2}=0.999\)) was used with an initial learning rate of \(10^{-4}\) and an exponential decay of 0.97. The batch size was equal to 128 throughout the whole experiment. The code used in the experiments is available at https://github.com/PiotrKrasnowski/MDL_and_Generalization_Guarantees_for_Representation_Learning.

During the training phase, we jointly trained the encoder and the decoder parts for 200 epochs using either the standard Gaussian prior of the traditional VIB objective function or our CDVIB objective functions. As in [1], we generated one latent sample per image during training and 12 samples during testing.

### Numerical findings

Figure 3 displays the training and test performance of our model for a wide range of parameters \(\beta\). It could be noticed that the best test accuracy with our Lossy CDVIB objective function is 65% (for \(\beta=0.01\)), which is about 2.5% better than the best test accuracy for VIB (62.5% for \(\beta=0.005\)). In the case of Lossless CDVIB, the best achieved test accuracy is 64% for \(\beta=1e-5\).

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multicolumn{2}{|c|}{Encoder} & \multicolumn{2}{|c|}{Encoder cont’d} & \multicolumn{2}{|c|}{Encoder cont’d} \\ \hline Number & Layer & Number & Layer & Number & Layer \\ \hline
1 & Conv2D(3,8,5) & 6 & Conv2D(16,16,3) & 11 & LeakyReLU(0.1) \\ \hline
2 & Conv2D(3,8,5) & 7 & LeakyReLU(0.1) & 12 & Linear(256,128) \\ \hline
3 & LeakyReLU(0.1) & 8 & MaxPool(2,2) & & Decoder \\ \hline
4 & MaxPool(2,2) & 9 & Flatten & 1 & Linear(64,10) \\ \hline
5 & Conv2D(8,16,3) & 10 & Linear(1024,256) & 2 & Softmax \\ \hline \end{tabular}
\end{table}
Table 1: The model architecture used in experiments. The convolutional layers are parametrized respectively by the number of input channels, the number of output channels, and the filter size. The linear layers are defined by their input and output sizes.

Figure 3: Test and train performances of our two-step prediction model trained using the standard VIB prior, the “lossless” CDVIB prior, and the “lossy” CDVIB prior, both with \(M=5\). The plots show the average over 5 runs and 95% bootstrap confidence intervals.

Proofs

### General proof techniques

Most of our proofs contain two main steps.

Technically speaking, the first step uses Donsker-Varadhan's variational representation lemma, to change the measure. As an example, in proof of Theorem 1, the first step gives

\[\lambda\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]\] \[\qquad\leq D_{KL}\bigg{(}\mu_{Y}^{\otimes 2n}\mathbb{E}_{S^{\prime},S,W} \Big{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{ \prime n},W)\Big{]}\Big{|}\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{ \prime n}_{i}|Y^{n},Y^{\prime n})\bigg{)}\] \[\qquad+\log\mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{n},\hat{Y}^{ \prime n}\sim\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{\prime n}_{i}|Y^{n},Y ^{\prime n})}\bigg{[}e^{\frac{\lambda}{n}\sum_{i\in[n]}\big{(}1_{\{Y^{\prime}_{ i}+\hat{Y}^{\prime}_{i}\}}-1_{\{Y_{i}+\hat{Y}_{i}\}}\big{)}}\bigg{]},\]

that changes the measure from

\[\mu_{Y}^{\otimes 2n}\mathbb{E}_{S^{\prime},S,W}\Big{[}P_{\hat{Y}|X,W}^{ \otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{\prime n},W)\Big{]},\]

to

\[\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y^{ \prime n}).\]

This has a particular meaning and intuition in our _fixed-size_ compressibility approach. Indeed, one can show that there exists a proper sequence of _compression books_, with size \(|\hat{\mathcal{Y}}_{m}|\leq e^{mR}\), where

\[R=D_{KL}\bigg{(}\mu_{Y}^{\otimes 2n}P_{\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y^{ \prime n}}\bigg{\|}\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{ \prime n}_{i}|Y^{n},Y^{\prime n})\bigg{)}.\]

This means that by this step, we _fix_ a suitable sequence of compression books such that with high probability (that goes to 1 as \(m\to\infty\)), one can find the sequence of predicted labels in this codebook. Using Donsker-Varadhan's change of measure is a shortcut to this, as previously explained in [2, Appendix B.1.] in the context of hypothesis compression. Then, we consider the union bound over all elements of this codebook in the next step. For tail bounds, similar interpretations hold, but this time with _variable-size_ compressibility notion, as shown in [2], in the context of hypothesis compression.

The second step can be seen as bounding the generalization error for _every_ element of such codebook. This step is achieved in quite different manners for the proofs of different results of the paper, as follows in the coming sections. Note that in this step, the predicted labels have distributions according to the prior \(\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{\prime n}_{i}|Y^{n},Y^{\prime n})\), rather than the one induced by the learning algorithm, _i.e.,_\(P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{\prime n},W)\).

### Proof of Theorem 1

#### e.2.1 Part i.

Proof.: We first show that

\[\inf_{\mathbf{Q}\in\mathcal{Q}_{i}}\mathbb{E}_{\mathbf{Y},\mathbf{Y}^{\prime}} \Big{[}D_{KL}\Big{(}\mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},W}\Big{[}P_{ \hat{Y}|X,W}^{\otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X}, \mathbf{X}^{\prime},W)\Big{]}\Big{|}\mathbf{Q}\Big{)}\Big{]}=I\Big{(}\mathbf{ J};\hat{Y}^{2n}|Y^{2n}\Big{)}.\] (19)

Let \(\mathcal{Q}^{\prime}_{i}\), be the set of conditional priors \(\mathbf{Q}^{\prime}\) that can be written as

\[\mathbf{Q}^{\prime}\Big{(}\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{ Y},\mathbf{Y}^{\prime}\Big{)}=\mathbb{E}_{\mathbf{J}}\Big{[}\mathbf{Q}^{\prime}_{1} \Big{(}\hat{\mathbf{Y}}_{\mathbf{J}}^{2n},\hat{Y}_{\mathbf{J}^{c}}^{2n}|Y_{ \mathbf{J}^{c}}^{2n}\Big{)}\Big{]},\] (20)

for some arbitrary distribution \(\mathbf{Q}^{\prime}_{1}\). Here \(Y^{2n}\) and \(\hat{Y}^{2n}\) are the concatenations of the vectors \((\mathbf{Y},\mathbf{Y}^{\prime})\) and \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\), respectively. It is easy to verify that \(\mathcal{Q}_{i}=\mathcal{Q}^{\prime}_{i}\). Hence, by denoting \(P(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{Y},\mathbf{Y}^{\prime}):= \mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},\mathbf{W}}\Big{[}P_{\hat{Y}|X,W}^{ \otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X},\mathbf{X}^{ \prime},W)\Big{]}\) we can write

\[\mathrm{LHS} =\inf_{\mathbf{Q}\in\mathcal{Q}_{i}}\mathbb{E}_{\mathbf{Y}, \mathbf{Y}^{\prime}}\Big{[}D_{KL}\Big{(}P(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{ \prime}|\mathbf{Y},\mathbf{Y}^{\prime})\Big{\|}\mathbf{Q}\Big{)}\Big{]}\] \[=\inf_{\mathbf{Q}^{\prime}\in\mathcal{Q}^{\prime}_{i}}\mathbb{E}_{ \mathbf{Y},\mathbf{Y}^{\prime}}\Big{[}D_{KL}\Big{(}P(\hat{\mathbf{Y}},\hat{ \mathbf{Y}}^{\prime}|\mathbf{Y},\mathbf{Y}^{\prime})\Big{\|}\mathbf{Q}^{\prime} \Big{)}\Big{]}\]\[= \inf_{\mathbf{Q}^{\prime}\in\mathcal{Q}_{i}^{c}}\mathbb{E}_{Y^{2n}} \mathbb{E}_{\mathbf{J}}\Big{[}D_{KL}\Big{(}P\big{(}\hat{Y}_{\mathbf{J}}^{2n},\hat{ Y}_{\mathcal{J}^{c}}^{2n}|Y_{\mathbf{J}}^{2n},Y_{\mathcal{J}^{c}}^{2n}\big{)} \Big{\|}\mathbf{Q}^{\prime}\big{)}\Big{]}\] \[= \inf_{\mathbf{Q}_{i}^{c}}\mathbb{E}_{Y^{2n}}\mathbb{E}_{\mathbf{J }}\Big{[}D_{KL}\Big{(}P\big{(}\hat{Y}_{\mathbf{J}}^{2n},\hat{Y}_{\mathcal{J}^{c }}^{2n}|Y_{\mathbf{J}}^{2n},Y_{\mathcal{J}^{c}}^{2n}\big{)}\Big{\|}\mathbb{E}_{ \mathbf{J}}\Big{[}\mathbf{Q}_{1}^{\prime}\big{(}\hat{Y}_{\mathbf{J}}^{2n},\hat {Y}_{\mathcal{J}^{c}}^{2n}|Y_{\mathbf{J}}^{2n},Y_{\mathcal{J}^{c}}^{2n}\big{)} \Big{]}\Big{)}\Big{]}\] \[= I\Big{(}\mathbf{J};\hat{Y}^{2n}\big{|}Y^{2n}\Big{)}.\]

This completes the proof of showing the equality in (2).

Now, we proceed to prove the upper bound. Let \(\mathbf{Q}\) be any fixed type-I symmetric conditional prior on \((\hat{Y}^{n},\hat{Y}^{\prime n})\) given \((Y^{n},Y^{\prime n})\). We show that

\[\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]\ \leqslant\sqrt{\frac{2\mathbb{E}_{Y^{n},Y^{ \prime n}}\bigg{[}D_{KL}\bigg{(}\mathbb{E}_{X^{\prime n},X^{n},W}\bigg{[}P_{ \hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{\prime n},W) \bigg{]}\bigg{\|}\mathbf{Q}\bigg{)}}{n}},\] (21)

where \(Y^{\prime n},Y^{\prime n}\sim\mu_{Y}^{\otimes 2n}\) and \(X^{\prime n},X^{n},W\sim P_{X^{\prime n}|Y^{\prime n}}P_{X^{n},W|Y^{n}}\). For ease of notation, denote

\[P_{\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y^{\prime n}}:=\mathbb{E}_{X^{\prime n },X^{n},W}\Big{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{ \prime n},W)\Big{]},\]

where again \(X^{\prime n},X^{n},W\sim P_{X^{\prime n}|Y^{\prime n}}P_{X^{n},W|Y^{n}}\). We start the proof by applying the change of measure using Donsker-Varadhan's variational representation (step \((a)\) below):

\[\lambda\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]\] \[= \mathbb{E}_{S,S^{\prime},W,\hat{Y}^{\prime n},\hat{Y}^{\prime n} \sim P_{S^{\prime}}P_{S,W}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{ \prime n}|X^{n},X^{\prime n},W)}\Bigg{[}\frac{\lambda}{n}\sum_{i\in[n]}\Big{(} \mathbbm{1}_{\{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}}-\mathbbm{1}_{\{Y_{i} \neq\hat{Y}_{i}\}}\Big{)}\Bigg{]}\] \[= \mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{\prime n}\sim\mu_{Y}^{ \otimes 2n}P_{Y^{n},Y^{\prime n}|Y^{n},Y^{\prime n}}\Bigg{[}\frac{\lambda}{n} \sum_{i\in[n]}\Big{(}\mathbbm{1}_{\{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}} -\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\Big{)}\Bigg{]}\] \[\overset{(a)}{\leqslant} D_{KL}\bigg{(}\mu_{Y}^{\otimes 2n}P_{\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y^ {\prime n}}\bigg{\|}\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}_{i}^{\prime n}|Y^{n},Y^{ \prime n})\bigg{)}\] \[+\log\mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{n},\hat{Y}^{\prime n} \sim\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}_{i}^{\prime n}|Y^{n},Y^{ \prime n})}\bigg{[}e^{\frac{\lambda}{n}\sum_{i\in[n]}\big{(}\mathbbm{1}_{\{Y_{ i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}}-\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}} \big{)}}\bigg{]}.\] (22)

Now, we bound the second term. Consider the notation \(\mathfrak{Y}_{i,j}\in\mathcal{Y}^{2n}\) and \(\hat{\mathfrak{G}}_{i,j}\in\mathcal{Y}^{2n}\) for \(i\in[n]\) and \(j\in\{1,2\}\). Denote \(j^{c}:=\mathbbm{1}_{\{j=1\}}+1\). Furthermore, for every \(i\in[n]\), let \(K_{i}\) be a random variable that takes uniformly over \(\{1,2\}\) -- The variables \(\{K_{i}\}\) are assumed to be mutually independent. Let the complement variable \(K_{i}^{c}\) be equal to \(2\) if \(K_{i,j}=1\) and \(1\) otherwise.

\[\log\mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{n},\hat{Y}^{\prime n} \sim\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}_{i}^{\prime n}|Y^{n},Y^{ \prime n})}\Bigg{[}e^{\frac{\lambda}{n}\sum\limits_{i\in[n]}\big{(}\mathbbm{1}_{ \{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}}-\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}} \big{)}}\Bigg{]}\] \[\overset{(a)}{=} \log\mathbb{E}_{\mathfrak{Y}^{n\times 2},\mathfrak{Y}^{n\times 2},K^{n}\sim P_{Y}^{\otimes 2n} \mathbf{Q}(\hat{\mathfrak{Y}}^{n\times 2}|\mathfrak{Y}^{n\times 2})\operatorname{Unif}(1,2) \otimes n}\Bigg{[}e^{\frac{\lambda}{n}\sum\limits_{i\in[n]}\Big{(}\mathbbm{1}_{ \{\mathfrak{Y}_{K_{i}},i}\neq\mathfrak{Y}_{K_{i},i})}-\mathbbm{1}_{\{\mathfrak{Y}_{K _{i}^{c}},i}\neq\mathfrak{Y}_{K_{i}^{c},i}\}}\Big{)}\Bigg{]}\] \[= \log\mathbb{E}_{\mathfrak{Y}^{n\times 2},\mathfrak{Y}^{n\times 2}\sim P_{Y}^{\otimes 2n} \mathbf{Q}(\hat{\mathfrak{Y}}^{n\times 2}|\mathfrak{Y}^{n\times 2})\mathbb{E}_{K^{n}\sim \operatorname{Unif}(1,2)\otimes n}\Bigg{[}e^{\frac{\lambda}{n}\sum\limits_{i\in[n]} \Big{(}\mathbbm{1}_{\{\mathfrak{Y}_{K_{i}},i}\neq\mathfrak{Y}_{K_{i},1})}- \mathbbm{1}_{\{\mathfrak{Y}_{K_{i}^{c}},i}\neq\mathfrak{Y}_{K_{i}^{c},i}\}} \Big{)}\Bigg{]}\] \[\overset{(b)}{\leqslant} \log\bigg{(}\frac{e^{\lambda/n}+e^{-\lambda/n}}{2}\Bigg{)}^{n}\] \[\leq \frac{\lambda^{2}}{2n},\] (23)

where \((a)\) is concluded by symmetry of \(\mathbf{Q}\) and \((b)\) by inequality \(\frac{e^{x}+e^{-x}}{2}\leq e^{x^{2}/2}\). Combining this with (22) yield

\[\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]\leq \frac{1}{\lambda}D_{KL}\bigg{(}\mu_{Y}^{\otimes 2n}P_{\hat{Y}^{n},\hat{Y}^{ \prime n}|Y^{n},Y^{\prime n}}\bigg{\|}\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}_{i}^{\prime n}|Y^{n},Y^{ \prime n})\bigg{)}+\frac{\lambda}{2n}.\]Letting

\[\lambda\coloneqq\sqrt{2nD_{KL}\bigg{(}\mu_{Y}^{\otimes 2n}P_{\hat{Y}^{n},\hat{Y}^{ \prime n}|Y^{n},Y^{\prime n}}\bigg{|}\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n}, \hat{Y}^{\prime n}_{i}|Y^{n},Y^{\prime n})\bigg{)}},\]

completes the proof. 

#### e.2.2 Part ii.

Proof.: The proof of this proposition follows similarly as proof of Theorem 5 (with \(\epsilon=0\)), and avoided for brevity. Note that similar to Theorem 5, by noting that with probability at least \((1-\delta)\), \(\hat{\mathcal{L}}(S^{\prime},W)\geq\mathcal{L}(W)-\sqrt{\log(1/\delta)/(2n)}\), one can also establish a tail bound on \(\mathrm{gen}(S,W)\). 

### Proof of Lemma 1

Proof.: Parts i. and ii. can be easily verified numerically. To show Part iii., take the derivative with respect to \(x\). This derivative, _i.e.,_\(\log\Bigl{(}\frac{2-x-x^{\prime}}{x+x^{\prime}}\Bigr{)}-\log(\frac{1-x}{x})\), is always non-negative for \(1>x>x^{\prime}\). For Part iv. the second partial derivative of \(h_{D}(x,x^{\prime})\) with respect to \(x\) is equal to \(\frac{1}{x(1-x)}-\frac{2}{(x+x^{\prime})(2-x-x^{\prime})}\) which is always positive for \(0\leq x,x^{\prime}\leq 1\). Finally, we show the convexity with respect to both variables \(x\) and \(x^{\prime}\) simultaneously. The Hessian of the function \(h_{D}(x,x^{\prime})\) equals

\[H=\begin{bmatrix}\frac{1}{x(1-x)}-\frac{2}{(x+x^{\prime})(2-x-x^{ \prime})}&-\frac{2}{(x+x^{\prime})(2-x-x^{\prime})}\\ -\frac{2}{(x+x^{\prime})(2-x-x^{\prime})}&\frac{1}{x^{\prime}(1-x^{\prime})}- \frac{2}{(x+x^{\prime})(2-x-x^{\prime})}\end{bmatrix}.\] (24)

We show the eigenvalues of this symmetric matrix is always non-negative, and hence \(H\) is positive semi-definite. This completes the proof.

Solving

\[|\lambda\mathrm{I}_{2}-H|=\left|\begin{bmatrix}\lambda-\frac{1}{x(1-x)}+\frac {2}{(x+x^{\prime})(2-x-x^{\prime})}&\frac{2}{(x+x^{\prime})(2-x-x^{\prime})} \\ \frac{2}{(x+x^{\prime})(2-x-x^{\prime})}&\lambda-\frac{1}{x^{\prime}(1-x^{ \prime})}+\frac{2}{(x+x^{\prime})(2-x-x^{\prime})}\end{bmatrix}\right|=0,\] (25)

reduces to solving

\[(\lambda+a)^{2}-\bigg{(}\frac{1}{x(1-x)}+\frac{1}{x^{\prime}(1-x^{\prime})} \bigg{)}(\lambda+a)+\frac{1}{x(1-x)x^{\prime}(1-x^{\prime})}-a^{2}=0,\] (26)

where \(a:=\frac{2}{(x+x^{\prime})(2-x-x^{\prime})}\). The roots of this equation are

\[\lambda=\frac{\Big{(}\frac{1}{x(1-x)}+\frac{1}{x^{\prime}(1-x^{\prime})} \Big{)}\pm\sqrt{\Big{(}\frac{1}{x(1-x)}-\frac{1}{x^{\prime}(1-x^{\prime})} \Big{)}^{2}+4a^{2}}}{2}-a.\] (27)

It is straightforward to verify that both roots are always non-negative, which completes the proof. 

### Proof of Theorem 3

Proof.: We first show that

\[\inf_{\mathbf{Q}\in\mathcal{Q}_{ii}}\mathbb{E}_{\mathbf{Y},\mathbf{Y}^{\prime }}\Big{[}D_{KL}\Big{(}\mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},W}\Big{[}P_ {\hat{Y}|X,W}^{\otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X}, \mathbf{X}^{\prime},W)\Big{]}\Big{|}\mathbf{Q}\Big{)}\Big{]}=I\Big{(}\mathbf{T} ;\hat{\mathbf{Y}}^{2n}|Y^{2n}\Big{)}.\] (28)

Let \(\mathcal{Q}^{\prime}_{ii}\), be the set of conditional priors \(\mathbf{Q}^{\prime}\) that can be written as

\[\mathbf{Q}^{\prime}\Big{(}\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{Y },\mathbf{Y}^{\prime}\Big{)}=\mathbb{E}_{\mathbf{T}}\Big{[}\mathbf{Q}^{\prime}_ {1}\Big{(}\hat{Y}_{\mathbf{T}}^{2n},\hat{Y}_{\mathbf{T}^{2n}}^{2n}|Y_{\mathbf{T }}^{2n},Y_{\mathbf{T}^{2n}}^{2n}\Big{)}\Big{]},\] (29)

for some arbitrary (and not necessarily symmetric distribution \(\mathbf{Q}^{\prime}_{1}\). Here \(Y^{2n}\) and \(\hat{Y}^{2n}\) are the concatenations of the vectors \((\mathbf{Y},\mathbf{Y}^{\prime})\) and \((\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime})\), respectively. It is easy to verify that \(\mathcal{Q}_{ii}=\mathcal{Q}^{\prime}_{ii}\). Hence, by denoting\[P(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{Y},\mathbf{Y}^{\prime}) \coloneqq\mathbb{E}_{\mathbf{X}^{\prime},\mathbf{X},W}\bigg{[}P_{\hat{Y}|X,W}^{ \otimes 2n}(\hat{\mathbf{Y}},\hat{\mathbf{Y}}^{\prime}|\mathbf{X},\mathbf{X}^ {\prime},W)\bigg{]}\] we can write \[\text{LHS} =\inf_{\mathbf{Q}^{\prime}\in\mathcal{Q}_{ii}}\mathbb{E}_{ \mathbf{Y},\mathbf{Y}^{\prime}}\bigg{[}D_{KL}\Big{(}P(\hat{\mathbf{Y}},\hat{ \mathbf{Y}}^{\prime}|\mathbf{Y},\mathbf{Y}^{\prime})\Big{|}\mathbf{Q}\Big{)}\bigg{]}\] \[=\inf_{\mathbf{Q}^{\prime}\in\mathcal{Q}_{ii}^{\prime}}\mathbb{E}_ {Y^{\prime 2n}}\mathbb{E}_{\mathbf{T}}\Big{[}D_{KL}\Big{(}P(\hat{\mathbf{Y}}^{2n},\hat{\mathbf{Y}}^{2n}_{\mathbf{T}^{\prime}}|Y^{2n}_{\mathbf{T}^{\prime}},Y^{ 2n}_{\mathbf{T}^{\prime}})\Big{|}\mathbf{Q}^{\prime}\Big{)}\bigg{]}\] \[=\inf_{\mathbf{Q}_{1}^{\prime}}\mathbb{E}_{Y^{\prime 2n}}\mathbb{E}_{ \mathbf{T}}\Big{[}D_{KL}\Big{(}P\big{(}\hat{\mathbf{Y}}^{2n}_{\mathbf{T}}, \hat{\mathbf{Y}}^{2n}_{\mathbf{T}^{\prime}}|Y^{2n}_{\mathbf{T}^{\prime}},Y^{ 2n}_{\mathbf{T}^{\prime}}\Big{)}\Big{|}\mathbf{E}_{\mathbf{T}}\Big{[}\mathbf{Q }_{1}^{\prime}\big{(}\hat{\mathbf{Y}}^{2n}_{\mathbf{T}},\hat{\mathbf{Y}}^{2n}_ {\mathbf{T}^{\prime}}|Y^{2n}_{\mathbf{T}^{\prime}}\big{)}\Big{]}\Big{)}\Big{]}\] \[=\] \[= I\Big{(}\mathbf{T};\hat{Y}^{2n}|Y^{2n}\Big{)}.\]

This completes the proof of showing the equality in (28).

Now we proceed to show the upper bound on \(nh_{D}\Big{(}\mathbb{E}_{W}[\mathcal{L}(W)],\mathbb{E}_{S,W}\Big{[}\hat{ \mathcal{L}}(S,W)\Big{]}\Big{)}\). Let \(\mathbf{Q}\) be any fixed type-II symmetric conditional prior on \((\hat{Y}^{n},\hat{Y}^{\prime n})\) given \((Y^{n},Y^{\prime n})\). We show that for \(n\geq 10\),

\[nh_{D}\Big{(}\mathbb{E}_{W}[\mathcal{L}(W)], \mathbb{E}_{S,W}\Big{[}\hat{\mathcal{L}}(S,W)\Big{]}\Big{)}\] \[\leq\mathbb{E}_{Y^{n},Y^{\prime n}}\bigg{[}D_{KL}\bigg{(}\mathbb{E }_{S^{\prime},S,W}\Big{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{ \prime n}|X^{n},X^{\prime n},W)\Big{]}\bigg{|}\mathbf{Q}\Big{)}\bigg{]}+\log(n),\]

where \(Y^{n},Y^{\prime n}\sim\mu_{Y}^{\otimes 2n}\) and \(S^{\prime},S,W\sim P_{S^{\prime}|Y^{\prime n}}P_{S,W|Y^{n}}\).

For ease of notation, denote

\[P_{\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y^{\prime n}}\coloneqq\mathbb{E}_{S^{ \prime},S,W}\bigg{[}P_{\hat{Y}|X,W}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{ \prime n}|X^{n},X^{\prime n},W)\Big{]},\]

where \(S^{\prime},S,W\sim P_{S^{\prime}|Y^{\prime n}}P_{S,W|Y^{n}}\). We have

\[nh_{D}\Big{(}\mathbb{E}_{W}[\mathcal{L}(W)],\mathbb{E}_{S,W} \Big{[}\hat{\mathcal{L}}(S,W)\Big{]}\Big{)}\] \[\leq n\mathbb{E}_{P_{S^{\prime}}P_{S,W}}\bigg{[}h_{D}\Big{(}\hat{ \mathcal{L}}(S^{\prime},W),\hat{\mathcal{L}}(S,W)\Big{)}\bigg{]}\] \[= n\mathbb{E}_{S^{\prime},S,W}\Bigg{[}h_{D}\Big{(}\mathbb{E}_{P_{ \hat{Y}|X,W}^{\otimes n}(\hat{Y}^{\prime n}|X^{n},W)}\bigg{[}\frac{1}{n}\sum_{i \in[n]}\mathbbm{1}_{\{Y^{\prime}_{i}\neq\hat{Y}^{\prime}_{i}\}}\bigg{]}, \mathbb{E}_{P_{\hat{Y}|X,W}^{\otimes n}(\hat{Y}^{n}|X^{n},W)}\bigg{[}\frac{1}{n }\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\bigg{]}\bigg{)}\Bigg{]}\] \[\leq D_{KL}\bigg{(}\mu_{Y}^{\otimes 2n}P_{\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y ^{\prime n}}\bigg{|}\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{\prime n}|Y^{n},Y^{ \prime n})\bigg{)}\] \[+\log\mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{\prime n},\hat{Y}^{ \prime n}\sim\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{\prime n}_{i}|Y^{ \prime n},Y^{\prime n})}\bigg{[}e^{nh_{D}\big{(}\frac{1}{n}\sum_{i\in[n]}1_{ \{Y^{\prime}_{i}\neq\hat{Y}^{\prime}_{i}\}},\frac{1}{n}\sum_{i\in[n]}1_{\{Y_{ i}\neq\hat{Y}_{i}\}}\big{)}}\bigg{]}.\] (30)

Now, we compute the last term, which does not depend on \(W\) anymore. Suppose that \(\mathrm{Unif}(2n)\) is a distribution that picks uniformly \(n\) indices among indices \(2n\) indices, _i.e._, the probability of each one is \(\frac{1}{(n)}\). We denote such indices by \(\mathbf{T}=(T_{1},\dots,T_{n})\), and the corresponding distribution by \(\mathrm{Unif}(2n)\). For a vector \(Y^{2n}\) of length \(2n\), we denote the elements corresponding to \(n\) indices picked by \(\mathbf{T}\) as \(Y^{2n}_{\mathbf{T}}=(Y^{2n}_{1},\dots,Y^{2n}_{n})\). We denote by \(\mathbf{T}^{c}=(T^{c}_{1},\dots,T^{c}_{n})\) the other remaining \(n\) elements in \(1,\dots,2n\) that are not picked by \(\mathbf{T}\). We denote \(\mathfrak{Y}^{T^{n,c}}=(\mathfrak{Y}_{T^{c}_{1}},\dots,\mathfrak{Y}_{T^{c}_{n}})\). Then,

\[\log \mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{\prime n},\hat{Y}^{\prime n }\sim\mu_{Y}^{\otimes 2n}\mathbf{Q}(\hat{Y}^{n},\hat{Y}^{\prime n}_{i}|Y^{n},Y^{\prime n}) }\bigg{[}e^{nh_{D}\big{(}\frac{1}{n}\sum_{i\in[n]}1_{\{Y^{\prime}_{i}\neq\hat{Y}^{ \prime}_{i}\}},\frac{1}{n}\sum_{i\in[n]}1_{\{Y_{i}\neq\hat{Y}_{i}\}}\big{)}}\] \[=\log\mathbb{E}_{\mathfrak{Y}^{2n},\hat{\mathfrak{Y}}^{2n}, \mathbf{T},\sim\mu^{\otimes 2n}\mathbf{Q}(\hat{\mathfrak{Y}}^{2n}|\mathfrak{Y}^{2n})\,\mathrm{ Unif}(2n)}\Bigg{[}e^{nh_{D}\Big{(}\frac{1}{n}\sum_{i\in[n]}1_{\{\mathfrak{Y}_{T^{c}_{i}\neq\hat{Y}_{i}\}}} \frac{1}{n}\sum_{i\in[n]}1_{\{\mathfrak{Y}_{T^{c}_{i}\neq\hat{Y}_{i}\}}}\Big{)}} \Bigg{]}.\] (31)Let \(V\) be a random variable indicating \(V:=\sum_{i\in[2n]}\mathbbm{1}_{\{\mathfrak{V}_{i}\neq\mathfrak{V}_{i}\}}\) in the sequence \(\mathfrak{Y}^{2n}\). Then, we consider different cases for \(V\) and show that

\[\mathbb{E}_{\mathbf{T}\sim\mathrm{Unif}(2n)}\Bigg{[}e^{nh_{D}\left(\frac{1}{n} \sum_{i\in[n]}\mathbbm{1}_{\{\mathfrak{V}_{i}^{T}\in\mathfrak{V}_{i}^{T}\in \mathfrak{V}_{i}^{T}\},\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{\mathfrak{V}_{ T}_{i}\neq\mathfrak{V}_{T_{i}}\}}}\right)}\Bigg{]}\leq n,\] (32)

for \(n\geq 10\). This completes the proof.

We use the following lemma repeatedly in the rest of the proof.

**Lemma 2** ([14, Exercise 5.8.a]).: _For \(j\geq 1\) and \(n-j\geq 1\), where \(j,n\in\mathbb{N}\),_

\[\sqrt{\frac{n}{8j(n-j)}}\leq\binom{n}{j}e^{-nh_{b}(j/n)}\leq\sqrt{\frac{n}{2 \pi j(n-j)}}.\] (33)

_Now,_

\[\begin{split}\sum_{j=0}^{V}e^{nh_{D}(j/n,(V-j)/n)}\frac{\binom{ n}{j}\binom{n}{j}\binom{n}{j-j}}{\binom{2n}{V}}=&\frac{2}{\binom{2n}{V}}+ \sum_{j=1}^{V-1}e^{nh_{D}(j/n,(V-j)/n)}\frac{\binom{n}{j}\binom{n}{V-j}}{ \binom{2n}{V}}\\ \stackrel{{(a)}}{{\leq}}&\frac{2}{\binom{ 2n}{V}}+\sum_{j=1}^{V-1}\frac{\sqrt{nV(2n-V)}}{\pi\sqrt{j(n-j)(V-j)(n-V+j)}} \\ \stackrel{{(b)}}{{\leq}}&\frac{2}{\binom{ 2n}{V}}+\frac{1}{2\pi}\sqrt{nV(2n-V)}\sum_{j=1}^{V-1}\left(\frac{1}{j(n-j)}+ \frac{1}{(V-j)(n-V+j)}\right)\\ =&\frac{2}{\binom{2n}{V}}+\frac{1}{\pi}\sqrt{nV(2n- V)}\sum_{j=1}^{V-1}\frac{1}{j(n-j)}\\ =&\frac{2}{\binom{2n}{V}}+\frac{1}{n\pi}\sqrt{nV(2n -V)}\sum_{j=1}^{V-1}\left(\frac{1}{j}+\frac{1}{n-j}\right)\\ \stackrel{{(c)}}{{\leq}}&\frac{2}{\binom{ 2n}{V}}+\frac{2}{n\pi}\sqrt{n^{3}}\sum_{j=1}^{n-1}\frac{1}{j}\\ \leq&\frac{2}{2n}+\frac{2\sqrt{n}}{\pi}(\log(n-1)+0.58+1/(2n-2))\\ \stackrel{{(d)}}{{\leq}}& n,\end{split}\]

_where \((a)\) is deduced using Lemma 2, \((b)\) is due to inequality \(\frac{1}{\sqrt{xy}}\leq\frac{1}{2x}+\frac{1}{2y}\) for any \(x,y>0\), \((c)\) by the upper bound on the Harmonic series, and \((d)\) holds for \(n\geq 2\)._

**ii. If \(V\in[n+1,2n-1]\):**

\[\begin{split}\sum_{j=V-n}^{n}& e^{nh_{D}(j/n,(V-j) /n)}\frac{\binom{n}{j}\binom{n}{V-j}}{\binom{2n}{V}}\\ =& 2e^{nh_{D}(n/n,(V-n)/n)}\frac{\binom{n}{n-n}}{ \binom{2n}{V}}+\sum_{j=V-n+1}^{n-1}e^{nh_{D}(j/n,(V-j)/n)}\frac{\binom{n}{j} \binom{n}{V-j}}{\binom{2n}{V}}\\ \stackrel{{(a)}}{{\leq}}& 2\max\biggl{(}\max_{V\in[n +2,2n-1]}\frac{\sqrt{2V}}{\sqrt{\pi(V-n)}},e^{nh_{D}(1,1/n)}\frac{n}{\binom{2n }{n+1}}\biggr{)}\\ &+\sum_{j=1}^{V-1}\frac{\sqrt{nV(2n-V)}}{\pi\sqrt{j(n-j)(V-j)(n- V+j)}}\\ \stackrel{{(b)}}{{\leq}}& 2\sqrt{\frac{(n+2)}{\pi}}+ \frac{1}{2\pi}\sqrt{nV(2n-V)}\sum_{j=V-n+1}^{n-1}\biggl{(}\frac{1}{j(n-j)}+ \frac{1}{(V-j)(n-V+j)}\biggr{)}\end{split}\]\[\begin{split}\lambda\mathbb{E}_{S,W}&[\text{gen}(S,W)]\\ =&\mathbb{E}_{S,S^{\prime},W,U^{\prime n},\hat{Y}^{n}, \hat{Y}^{\prime n}\sim P_{S^{\prime}}P_{S,W^{\prime}1}}\Bigg{[}\frac{\lambda}{n }\sum_{i\in[n]}\left(\mathbbm{1}_{\{Y^{\prime}_{i}\neq\hat{Y}^{\prime}_{i}\}}- \mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\right)\Bigg{]}\\ \leqslant& D_{KL}\bigg{(}P_{S^{\prime}}P_{S,W}P_{U|X,W_{e}}^{ \otimes 2n}(U^{n},U^{\prime n}|X^{n},X^{\prime m},W_{e})\bigg{\|}P_{S^{\prime}}P_{S,W }\mathbf{Q}(U^{n},U^{\prime n}|S,S^{\prime},W)\bigg{)}\\ &+\log\mathbb{E}_{S,S^{\prime},W,U^{\prime n},U^{\prime n},\hat{Y }^{\prime n}\sim P_{S^{\prime}}P_{S,W^{\prime}2}}\Bigg{[}e^{\frac{\lambda}{n} \sum_{i\in[n]}\left(\mathbbm{1}_{\{Y^{\prime}_{i}\neq\hat{Y}^{\prime}_{i}\}}- \mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\right)}\Bigg{]},\end{split}\] (34)

where

\[\begin{split}\nu_{1}:=& P_{U|X,W_{e}}^{\otimes 2n}(U^{n},U^{\prime n}|X^{n},X^{\prime n},W_{e})P_{\hat{Y}|U,W_{d}}^{\otimes 2n}(\hat{Y}^{n}, \hat{Y}^{\prime n}|U^{n},U^{\prime n},W_{d}),\\ \nu_{2}:=&\mathbf{Q}(U^{n},U^{\prime n}|S,S^{ \prime},W)P_{\hat{Y}|U,W_{d}}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|U^{n},U^{ \prime n},W_{d}).\end{split}\] (35)

Now, we bound the last term. Let

\[P_{\mathbf{Q}}\Big{(}\hat{Y}^{n},\hat{Y}^{\prime n}|S,S^{\prime},W\Big{)}:= \mathbb{E}_{(U^{n},U^{\prime n})\sim\mathbf{Q}(U^{n},U^{\prime n}|S,S^{\prime},W)}\Big{[}P_{\hat{Y}|U,W_{d}}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|U^{n},U^{ \prime n},W_{d})\Big{]}.\] (36)Then, the last term in (34) can be written as

\[\log\mathbb{E}_{S,S^{\prime},W,\hat{Y}^{n},\hat{Y}^{m}\sim P_{S^{\prime}}P_{\hat{S},W}P_{\mathbf{Q}}\big{(}\hat{Y}^{n},\hat{Y}^{m}|S,S^{\prime},W\big{)}}\bigg{[}e^ {\frac{\lambda}{n}\sum_{i\in[n]}\big{(}1_{\{Y_{i}^{\prime}\neq\hat{Y}_{1,i}\}}- 1_{\{Y_{i}\neq\hat{Y}_{2,i}\}}\big{)}}\bigg{]}.\]

Note that since we have \(P_{\hat{Y}|U,W_{d},X,Y}=P_{\hat{Y}|U,W_{d}}\), it can be easily verified that the distribution \(P_{\mathbf{Q}}\big{(}\hat{Y}^{n},\hat{Y}^{\prime n}|S,S^{\prime},W\big{)}\) is symmetric with respect to all permutations \(\pi\) that preserve the labels of \(Y\).

For each \(s,s^{\prime}\), let \(f\colon[n]\to[n]\), \(i\in[n]\) and \(k\colon[n]\to[n]\), \(i\in[n]\) be permutations of indices of \(s\) and \(s^{\prime}\), respectively, where \(Y_{f_{i}}=Y_{k_{i}}^{\prime}\) for \(i\leqslant T\) and \(Y_{f_{i}}\neq Y_{k_{i}}^{\prime}\) for \(i>T\), and where \(T\) equals \(n-\frac{n}{2}\|\hat{p}_{s}-\hat{p}_{s^{\prime}}\|_{1}\). Here, \(\hat{p}_{s}\) and \(\hat{p}_{s}\) are empirical distributions of \(Y\) in \(s\) and \(s^{\prime}\), respectively. We denote \((\hat{y}^{n},\hat{y}^{\prime n})\) by \(\hat{y}^{2\times n}\), where \(\forall i\in[n]\), \(\hat{y}_{1,i}=\hat{y}_{i}^{\prime}\) and \(\hat{y}_{2,i}=\hat{y}_{i}\).

Consider the binary random variable \(K_{i}\) taking value as \((2,f_{i})\) or \((1,k_{i})\) with probability \(1/2\) (independent of other \(j\neq i\)). Denote the complementary choice as \(K_{i}^{\varepsilon}\), _i.e.,_\(K_{i}^{\varepsilon}=(2,f_{i})\) iff \(K_{i}=(1,k_{i})\).

Then, due to the particular symmetry of \(\mathbf{Q}\), and by using shorthand notation

\[\mathsf{P}\coloneqq P_{S^{\prime}}P_{S,W}P_{\mathbf{Q}}\Big{(}\hat{Y}^{n}, \hat{Y}^{\prime n}|S,S^{\prime},W\Big{)},\]

we have

\[\mathbb{E}_{S,S^{\prime},W,\hat{Y}^{2\times n}\sim\mathsf{P}} \Bigg{[}e^{\frac{\lambda}{n}\sum_{i\in[n]}\big{(}1_{\{Y_{k_{i}}^{\prime}\neq \hat{Y}_{1,k_{i}}\}}-1_{\{Y_{f_{i}}\neq\hat{Y}_{2,f_{i}}\}}\big{)}}\Bigg{]}\] \[= \mathbb{E}_{S,S^{\prime},W,\hat{Y}^{2\times n}\sim\mathsf{P}} \Bigg{[}\left\{\prod_{i=1}^{T}e^{\frac{\lambda}{n}\Big{(}1_{\{Y_{k_{i}}^{ \prime}\neq\hat{Y}_{1,k_{i}}\}}-1_{\{Y_{f_{i}}\neq\hat{Y}_{2,f_{i}}\}}\Big{)} }\right\}\Bigg{\{}\prod_{i=T+1}^{n}e^{\frac{\lambda}{n}\Big{(}1_{\{Y_{k_{i}}^{ \prime}\neq\hat{Y}_{1,k_{i}}\}}-1_{\{Y_{f_{i}}\neq\hat{Y}_{2,f_{i}}\}}\Big{)} }\Bigg{\}}\Bigg{]}\] \[\leqslant \mathbb{E}_{S,S^{\prime},W,\hat{Y}^{2\times n}\sim\mathsf{P}} \Bigg{[}\left\{\prod_{i=1}^{T}e^{\frac{\lambda}{n}\Big{(}1_{\{Y_{i}\neq\hat{Y} _{1,k_{i}}\}}-1_{\{Y_{f_{i}}\neq\hat{Y}_{2,f_{i}}\}}\Big{)}}\right\}e^{\frac{ \lambda(n-T)}{n}}\Bigg{]}\] \[= \mathbb{E}_{S,S^{\prime},W,\hat{Y}^{2\times n},K^{n}\sim\mathsf{P} \operatorname{Unif}((2,f_{i}),(1,k_{i}))^{\otimes n}}\Bigg{[}\left\{\prod_{i= 1}^{T}e^{\frac{\lambda}{n}\Big{(}1_{\{Y_{i}\neq\hat{Y}_{K_{i}}\}}-1_{\{Y_{f_{i} }\neq\hat{Y}_{K_{i}^{\varepsilon}\}}}\Big{)}}\right\}e^{\frac{\lambda(n-T)}{n}} \Bigg{]}\] \[\leqslant \mathbb{E}_{S,S^{\prime},W\sim P_{S^{\prime}}P_{S,W}}\Bigg{[} \left(\frac{e^{\lambda/n}+e^{-\lambda/n}}{2}\right)^{T}e^{\frac{\lambda(n-T)}{n}} \Bigg{]}\] \[\leqslant \mathbb{E}_{S,S^{\prime},W\sim P_{S^{\prime}}P_{S,W}}\Bigg{[}e^{ \frac{\lambda^{2}T}{2n^{2}}e^{\frac{\lambda(n-T)}{n}}}\Bigg{]}\] \[\stackrel{{(a)}}{{\leqslant}}e^{\frac{\lambda^{2}}{2n} }\times\mathbb{E}_{S,S^{\prime},\sim P_{S^{\prime}}P_{S,W}}\Big{[}e^{\frac{ \lambda}{2}\|\hat{p}_{S}-\hat{p}_{S^{\prime}}\|_{1}}\Big{]}\] \[\leqslant \exp\biggl{(}\frac{\lambda^{2}}{2n}+\frac{K+2}{2}+\frac{3\lambda^{2 }}{2n}\biggr{)}\] \[\leqslant \exp\biggl{(}\frac{2\lambda^{2}}{n}+\frac{K+2}{2}\biggr{)},\]

where \((a)\) is deduced from Lemma 3, conditioned that \(\lambda/n<1.36\).

Now, combining this with (34), and letting \(\lambda=\lambda^{\ast}=\sqrt{n(2B+K+2)/4}\), the expectation of generalization error is upper bounded by

\[\mathbb{E}_{S,W}[\mathrm{gen}(S,W)]= \frac{2B+K+2}{2\lambda}+\frac{2\lambda}{n}\] \[\leqslant 2\sqrt{\frac{(2B+K+2)}{n}},\]

if \(\lambda^{\ast}/n<1.36\). Note that if \(1.36<\lambda^{\ast}/n=\sqrt{\frac{(2B+K+2)}{4n}}\), then

\[2\sqrt{\frac{(2B+K+2)}{n}}=4(\lambda^{\ast}/n)>1.\]

Since generalization error is always bounded by 1, hence, this bound always holds.

**Lemma 3**.: _Suppose that \(Y^{n}\) and \(Y^{\prime n}\) are \(2n\) i.i.d. instances of \(Y\sim p\in[K]\). Then, if \(\lambda/n<0.68\), then,_

\[\log\mathbb{E}_{Y^{n},Y^{\prime n}}\Big{[}e^{\lambda\|\hat{p}_{Y^{n}}-\hat{p}_{ Y^{\prime n}}\|_{1}}\Big{]}\leqslant\frac{K+2}{2}+\frac{6\lambda^{2}}{n}.\]

The lemma is proved in Appendix E.9 using results of [10]. 

### Proof of Theorem 5

Let \(\mathbf{Q}\) be any fixed symmetric prior on \((\hat{Y}^{n},\hat{Y}^{\prime n})\) that could depend on \((X^{n},Y^{n},X^{\prime n},Y^{\prime n})\). We show that for any \(\epsilon\in\mathbb{R}\) and \(\delta\in\mathbb{R}^{+}\) with probability at least \((1-\delta)\) over choices of \(S\) and \(S^{\prime}\), we have that \(\mathbb{E}_{W\sim Q}[\mathrm{gen}(S,W)]\) is upper bounded by

\[\sqrt{\frac{\log(2/\delta)}{2n}}+\inf\sqrt{\frac{\mathbb{E}_{\hat{W}\sim P_{ \hat{W}|S}}\bigg{[}D_{KL}\bigg{(}P_{\hat{Y}|X,\hat{W}}^{\otimes 2n}(\hat{Y}^{n}, \hat{Y}^{\prime n}|X^{n},X^{\prime n},\hat{W})\bigg{\|}\mathbf{Q}\bigg{)}+\log (\sqrt{8n}/\delta)}{(2n-1)/4}}+\epsilon,\] (36)

where the infimum is over all \(P_{\hat{W}|S}\) that satisfy

\[\mathbb{E}_{P_{W|S}P_{\hat{W}|S}}\Big{[}\Big{[}\big{(}\hat{\mathcal{L}}(S^{ \prime},W)-\hat{\mathcal{L}}(S,W)\big{)}-\Big{(}\hat{\mathcal{L}}(S^{\prime}, \hat{W})-\hat{\mathcal{L}}(S,\hat{W})\Big{)}\Big{]}\Big{]}\leqslant\epsilon/2.\] (37)

Proof.: Consider a distribution \(P_{\hat{W}|S}\) that satisfies (37). Denote \(\lambda^{\text{*}}=\frac{2n-1}{4}\) and

\[\Delta(S,\mathbf{Q}):=\sqrt{\frac{\mathbb{E}_{\hat{W}\sim P_{\hat{W}|S}} \bigg{[}D_{KL}\bigg{(}P_{\hat{Y}|X,\hat{W}}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{ \prime n}|X^{n},X^{\prime n},\hat{W})\bigg{\|}\mathbf{Q}\bigg{)}\bigg{]}+\log (\sqrt{8n}/\delta)}{\lambda^{\text{*}}}}+\epsilon.\]

Furthermore, we use the shorthand notations \(\hat{p}:=P_{\hat{Y}|X,\hat{W}}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X ^{\prime n},\hat{W})\). Then,

\[\mathbb{P}_{S\sim\mu\otimes^{n}}\bigg{(}\mathbb{E}_{W\sim P_{W|S} }[\mathrm{gen}(S,W)]>\sqrt{\frac{\log(2/\delta)}{2n}}+\Delta(S,\mathbf{Q}) \bigg{)}\] \[\overset{(a)}{\leqslant}\mathbb{P}_{(S,S^{\prime})\sim\mu\otimes 2n} \Big{(}\mathbb{E}_{W\sim P_{W|S}}\Big{[}\hat{\mathcal{L}}(S^{\prime},W)-\hat{ \mathcal{L}}(S,W)\Big{]}>\Delta(S,\mathbf{Q})\Big{)}+\delta/2\] \[\overset{(b)}{\leqslant}\mathbb{P}_{(S,S^{\prime})\sim\mu\otimes 2n} \Big{(}\mathbb{E}_{W\sim P_{W|S}}\Big{[}\lambda^{\text{*}}\Big{(}\hat{ \mathcal{L}}(S^{\prime},W)-\hat{\mathcal{L}}(S,W)\Big{)}^{2}\Big{]}>\lambda^{ \text{*}}\Delta(S,\mathbf{Q})^{2}\Big{)}+\delta/2\] \[\overset{(c)}{\leqslant}\mathbb{P}_{(S,S^{\prime})\sim\mu\otimes 2n} \bigg{(}\mathbb{E}_{W\sim P_{W|S}}\bigg{[}\lambda^{\text{*}}\Big{(} \hat{\mathcal{L}}(S^{\prime},\hat{W})-\hat{\mathcal{L}}(S,\hat{W})\Big{)}^{2} \bigg{]}>\lambda^{\text{*}}\Delta(S,\mathbf{Q})^{2}\bigg{)}+\delta/2\] \[=\] \[\overset{(d)}{\leqslant}\mathbb{P}_{(S,S^{\prime})\sim\mu\otimes 2n} \bigg{)}\] (38) \[+\delta/2\] \[\overset{(e)}{\leqslant}\mathbb{P}_{(S,S^{\prime})\sim\mu\otimes 2n} \bigg{(}\log\mathbb{E}_{\hat{Y}^{n},\hat{Y}^{\prime n}\sim\mathbf{Q}} \bigg{[}e^{\lambda^{\text{*}}\Big{(}\frac{1}{n}\sum_{i\in[n]}\big{(}1_{\{Y^{ \prime}_{i}+Y^{\prime}_{i}\}}-1_{\{Y_{i}+\hat{Y}_{i}\}}\big{)}\big{)}^{2}} \bigg{]}\geqslant\] \[\overset{(f)}{\leqslant}\delta,\] (39)where \((a)\) holds by Hoeffding inequality and using the fact that for arbitrary random variables \(U,V\) and constants \(a,b\in\mathbb{R}\), \(\mathbb{P}(U+V>a+b)\leq\mathbb{P}(U>a)+\mathbb{P}(V>b)\), \((b)\) by applying the Jensen inequality on the convex function \(f(x)=x^{2}\), \((c)\) by using the distortion function (37) and since the loss is bounded by one, \((d)\) by using the Donsker-Varadhan's variational representation lemma, \((e)\) is shown in the following, and \((f)\) by Markov inequality.

Hence, it remains to show the step \((e)\). To this end, it is sufficient upper bound

\[\log\mathbb{E}_{S,S^{\prime},\hat{Y}^{m},\hat{Y}^{m}\sim P_{S}P_{S^{\prime}} \mathbf{Q}}\Bigg{[}e^{\lambda^{\#\left(\frac{1}{n}\sum_{i\in[n]}\left(\mathbbm{ 1}_{\{Y^{\prime}_{i}\neq\hat{Y}^{\prime}_{i}\}}-\mathbbm{1}_{\{Y^{\prime}_{i} \neq\hat{Y}_{i}\}}\right)\right)^{2}}}\Bigg{]}\]

by \(\log(\sqrt{2n})\). Note that this terms equals

\[\log\mathbb{E}_{Y^{n},Y^{\prime n},\hat{Y}^{n},\hat{Y}^{m}\sim P_{Y}\otimes 2n }\mathbf{Q}_{1}\Bigg{[}e^{\lambda^{\#\left(\frac{1}{n}\sum_{i\in[n]}\left( \mathbbm{1}_{\{Y^{\prime}_{i}\neq\hat{Y}^{\prime}_{i}\}}-\mathbbm{1}_{\{Y^{ \prime}_{i}\neq\hat{Y}_{i}\}}\right)\right)^{2}}}\Bigg{]},\]

where \(\mathbf{Q}_{1}\) is equal to \(\mathbb{E}_{P_{X^{n}|Y^{n}}P_{X^{\prime n}|Y^{\prime n}}}\Big{[}\mathbf{Q}(\hat {Y}^{n},\hat{Y}^{\prime n}_{i}|X^{n},Y^{n},X^{\prime n},Y^{\prime n})\Big{]}\). Now,

\[\mathbb{E}_{Y^{n},Y^{m},\hat{Y}^{n},\hat{Y}^{m}\sim\mu_{Y}^{\otimes 2n }\mathbf{Q}_{1}(\hat{Y}^{n},\hat{Y}^{\prime n}_{i}|Y^{n},Y^{\prime n})}\Bigg{[}e ^{\lambda^{\#\left(\frac{1}{n}\sum\limits_{i\in[n]}\left(\mathbbm{1}_{\{Y^{ \prime}_{i}\neq\hat{Y}^{\prime}_{i}\}}-\mathbbm{1}_{\{Y^{\prime}_{i}\neq\hat{Y} _{i}\}}\right)\right)^{2}}}\Bigg{]}\] \[\stackrel{{(a)}}{{=}} \mathbb{E}_{\mathfrak{Y}^{n\times 2},\hat{\mathfrak{Y}}^{n\times 2},K^{n}\sim P ^{\otimes 2n}\mathbf{Q}_{1}(\hat{\mathfrak{Y}}^{n\times 2}|\mathfrak{Y}^{n\times 2}) }\mathbb{E}_{K^{n}\sim\mathrm{Unif}(1,2)\otimes n}\Bigg{[}e^{\lambda^{\# \left(\frac{1}{n}\sum\limits_{i\in[n]}\left(\mathbbm{1}_{\{\mathfrak{Y}_{K_{i },i}\neq\hat{\mathfrak{Y}_{K_{i},i}\}}-\mathbbm{1}_{\{\mathfrak{Y}_{K_{i},i}^{ \prime}\neq\hat{\mathfrak{Y}_{K_{i},i}\}}}\right)\right)^{2}}}}\Bigg{]}\] \[\stackrel{{(b)}}{{\leqslant}} \sqrt{2n},\] (40)

where \((a)\) is concluded by symmetry of \(\mathbf{Q}_{1}\) and \((b)\) is deduced since

\[\frac{1}{n}\sum_{i\in[n]}\biggl{(}\mathbbm{1}_{\{\mathfrak{Y}_{K_{i},i}\neq \hat{\mathfrak{Y}}_{K_{i},i}\}}-\mathbbm{1}_{\{\mathfrak{Y}_{K_{i}^{\prime},i} \neq\hat{\mathfrak{Y}}_{K_{i}^{\prime},i}\}}\biggr{)},\]

is \(1/\sqrt{n}\)-subgaussian process and hence

\[\mathbb{E}_{K^{n}\sim\mathrm{Unif}(1,2)\otimes n}\Bigg{[}e^{\left(\frac{1}{n} \sum_{i\in[n]}\left(\mathbbm{1}_{\{\mathfrak{Y}_{K_{i},i}\neq\hat{\mathfrak{Y} _{K_{i},i}\}}-\mathbbm{1}_{\{\mathfrak{Y}_{K_{i}^{\prime},i}\neq\hat{\mathfrak{Y} _{K_{i}^{\prime},i}\}}}\right)\right)^{2}/(4/(2n-1))}}\Bigg{]}\leq\sqrt{2n},\]

due to [14, Theorem 2.6.IV.]. This completes the proof.

### Proof of Theorem 6

Let \(\mathbf{Q}\) be any fixed symmetric prior on \((\hat{Y}^{n},\hat{Y}^{\prime n})\) that could depend on \((X^{n},Y^{n},X^{\prime n},Y^{\prime n})\). Then, for any \(\delta\in\mathbb{R}^{+}\),we show that with probability at least \((1-\delta)\) over choice of \((S,S^{\prime},W)\sim P_{S^{\prime}}P_{S,W}\),

\[nh_{D}\Big{(}\hat{\mathcal{L}}(S^{\prime},W),\hat{\mathcal{L}}(S,W)\Big{)} \leq D_{KL}\bigg{(}P^{\otimes 2n}_{\hat{Y}|X,W}(\hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},X^{ \prime n},W)\Big{\|}\mathbf{Q}\bigg{)}+\log(n/\delta).\]

Proof.: The proof is a combination of proofs of Theorems 3 and 5. Denote the RHS of the bound in the theorem as \(\Delta(S,S^{\prime},W)\).

First, note that

\[nh_{D}\Big{(}\hat{\mathcal{L}}(S^{\prime},W),\hat{\mathcal{L}}(S,W) \Big{)}\] \[=nh_{D}\Bigg{(}\mathbb{E}_{\hat{Y}^{m}\sim P_{\hat{Y}|X,W}^{\otimes n }(\hat{Y}^{\prime m}|X^{\prime n},W)}\Bigg{[}\frac{1}{n}\sum_{i\in[n]}\mathbbm{1 }_{\{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}}\Bigg{]},\mathbb{E}_{\hat{Y}^{n} \sim P_{\hat{Y}|X,W}^{\otimes n}(\hat{Y}^{n}|X^{n},W)}\Bigg{[}\frac{1}{n}\sum_{ i\in[n]}\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\Bigg{]}\Bigg{)}\] \[\leqslant n\mathbb{E}_{\hat{Y}^{n},\hat{Y}^{m}\sim P_{\hat{Y}|X,W}^{\otimes 2 n}(\hat{Y}^{n},\hat{Y}^{\prime m}|X^{n},X^{\prime n},W)}\Bigg{[}h_{D}\Bigg{(} \frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime }\}},\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\Bigg{)} \Bigg{]}\] \[\leqslant D_{KL}\Big{(}P_{\hat{Y}|X,W}^{\otimes 2 n}(\hat{Y}^{n},\hat{Y}^{\prime m}|X^{n},X^{\prime n},W)\Big{\|}\mathbf{Q}( \hat{Y}^{n},\hat{Y}^{\prime n}|X^{n},Y^{n},X^{\prime n},Y^{\prime n})\Big{)}\] \[+\log\mathbb{E}_{\hat{Y}^{n},\hat{Y}^{\prime n}\sim\mathbf{Q}( \hat{Y}^{n},\hat{Y}^{\prime n}_{i}|X^{n},Y^{n},X^{\prime n},Y^{\prime n})} \bigg{[}e^{nh_{D}\left(\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}^{\prime} \neq\hat{Y}_{i}^{\prime}\}},\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq \hat{Y}_{i}\}}\right)}\bigg{]}.\] (41)

Hence,

\[\mathbb{P}_{S,S^{\prime},W}\Big{(}nh_{D}\Big{(}\hat{\mathcal{L}}(S ^{\prime},W),\hat{\mathcal{L}}(S,W)\Big{)}>\Delta(S,S^{\prime},W)\Big{)}\] \[\overset{(a)}{\leqslant}\mathbb{P}_{Y^{n},Y^{\prime n}\sim\mu \otimes 2n}\bigg{(}\log\mathbb{E}_{\hat{Y}^{n},\hat{Y}^{\prime m}\sim\mathbf{Q}} \bigg{[}e^{nh_{D}\left(\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}^{\prime} \neq\hat{Y}_{i}^{\prime}\}},\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq \hat{Y}_{i}\}}\right)}\bigg{]}>\log(n/\delta)\bigg{)}\] \[\overset{(b)}{\leqslant}\mathbb{P}_{Y^{n},Y^{\prime n}\sim\mu \otimes 2n}\bigg{(}\log\mathbb{E}_{\hat{Y}^{n},\hat{Y}^{\prime n}\sim\mathbf{Q}} \bigg{[}e^{nh_{D}\left(\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}^{\prime} \neq\hat{Y}_{i}^{\prime}\}},\frac{1}{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq \hat{Y}_{i}\}}\right)}\bigg{]}>\] \[\log\mathbb{E}_{Y^{n},Y^{\prime m},\hat{Y}^{\prime n},\hat{Y}^{ \prime m}\sim\mu\otimes_{Y}^{2n}\mathbf{Q}}\bigg{[}e^{nh_{D}\left(\frac{1}{n} \sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}},\frac{1 }{n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\right)}\bigg{]}+\log(1 /\delta)\bigg{)}\] \[\overset{(c)}{\leqslant}\delta,\]

where \((a)\) follows by (41), \((b)\) holds since

\[\log\mathbb{E}_{Y^{n},Y^{\prime m},\hat{Y}^{\prime n},\hat{Y}^{ \prime m}\sim\mu\otimes_{Y}^{2n}\mathbf{Q}}\bigg{[}e^{nh_{D}\left(\frac{1}{n} \sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}^{\prime}\neq\hat{Y}_{i}^{\prime}\}},\frac{1} {n}\sum_{i\in[n]}\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\right)}\bigg{]} \leqslant\log(n),\]

by the proof of Theorem 3, and \((c)\) is derived using Markov inequality. This completes the proof. 

### Proof of Theorem 7

Let \(\mathbf{Q}\) be a type-III symmetric conditional prior over \(U^{2n}\) given \(X^{2n},Y^{2n}\) and \(W_{e}\in\mathcal{W}_{e}\), namely, \(\mathbf{Q}\Big{(}(u_{\pi(1)},\ldots,u_{\pi(2n)})|(x_{1},\ldots,x_{2n}),(y_{1}, \ldots,y_{2n}),w\Big{)}\) remains the same for all permutations \(\pi\colon[2n]\mapsto[2n]\) that preserves the label, _i.e._, \(y_{\pi(i)}=y_{i}\) for \(i\in[n]\). Then, for any \(\lambda\in\mathbb{R}^{+}\), we show that for \(K\)-classification learning task, with probability at least \((1-\delta)\) over \((S,S^{\prime},W)\sim P_{S^{\prime}}P_{S,W}\),

\[\hat{\mathcal{L}}(S^{\prime},W)-\hat{\mathcal{L}}(S,W)\leqslant\frac{D_{KL}\Big{(}P _{U|X,W_{e}}^{\otimes 2n}(U^{n},U^{\prime n}|X^{n},X^{\prime n},W_{e})\Big{\|}\mathbf{Q} \Big{)}+(K+2)/2+\log(1/\delta)}{\lambda}+\frac{2\lambda}{n}.\]

The pool of the second part follows trivially using Hoeffding's inequality.

Proof.: Note that whenever \(\lambda/n\geqslant 1.36\), the bound trivially holds. Hence, assume that \(\lambda/n<1.36\). Denote the RHS of the bound of Part i. as \(\Delta(S,S^{\prime},W)\). We use the shorthand notation \(p_{1}:=P_{U|X,W_{e}}^{\otimes 2n}(U^{n},U^{\prime n}|X^{n},X^{\prime n},W_{e})\) and \(p_{2}:=P_{\hat{Y}|U,W_{d}}^{\otimes 2n}(\hat{Y}^{n},\hat{Y}^{\prime n}|U^{n},U^{ \prime n},W_{d})\).

Note that \((S,S^{\prime},W)\sim P_{S^{\prime}}P_{S,W}\). Now,

\[\mathbb{P}_{S,S^{\prime},W}\Big{(}\hat{\mathcal{L}}(S^{\prime},W)- \hat{\mathcal{L}}(S,W)>\Delta(S,S^{\prime},W)\Big{)}\] \[= \mathbb{P}_{S,S^{\prime},W}\Bigg{(}\lambda\mathbb{E}_{p_{1}p_{2} }\Bigg{[}\frac{1}{n}\sum_{i\in[n]}\left(\mathbbm{1}_{\{Y_{i}^{\prime}\neq\hat{Y}_ {i}^{\prime}\}}-\mathbbm{1}_{\{Y_{i}\neq\hat{Y}_{i}\}}\right)\Bigg{]}\geqslant \lambda\Delta(S,S^{\prime},W)\Bigg{)}\] \[\overset{(a)}{\leqslant}\mathbb{P}_{S,S^{\prime},W}\Bigg{(}D_{KL} \bigg{(}p_{1}p_{2}\bigg{|}\mathbf{Q}p_{2}\bigg{)}\] (42)\[\leq \mathbb{E}\Big{[}e^{\frac{2\lambda}{n}\left(N-n\right)}+e^{\frac{2 \lambda}{n}\left(n-N\right)}\Big{]}\mathbb{E}\Big{[}e^{\frac{2\lambda}{n}\left(N ^{\prime}-n\right)}+e^{\frac{2\lambda}{n}\left(n-N^{\prime}\right)}\Big{]}\] \[\leq 4e^{2n\left(e^{\frac{2\lambda}{n}-1-\frac{2\lambda}{n}}\right)},\] (45)

and

\[\mathbb{E}\Big{[}e^{\frac{2\lambda}{n}\left(\sum_{i=1}^{K}|V_{i} -V_{i}^{\prime}|\right)}\Big{]}\leq \prod_{i=1}^{K}\left(\mathbb{E}\Big{[}e^{\frac{2\lambda}{n}\left(V_ {i}^{\prime}-V_{i}\right)}+e^{\frac{2\lambda}{n}\left(\hat{V}_{i}^{\prime}-V_{ i}^{\prime}\right)}\Big{]}\right)\] \[= 2^{K}\prod_{i=1}^{K}e^{np_{i}\left(e^{\frac{2\lambda}{n}-1+e^{- \frac{2\lambda}{n}}-1}\right)}\] \[= 2^{K}e^{n\left(e^{\frac{2\lambda}{n}-1+e^{-\frac{2\lambda}{n}}-1 }\right)},\] (46)

[MISSING_PAGE_EMPTY:37]