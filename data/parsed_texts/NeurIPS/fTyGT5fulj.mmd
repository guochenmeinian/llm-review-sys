# Curriculum Learning for Graph Neural Networks:

Which Edges Should We Learn First

 Zheng Zhang\({}^{\dagger}\) Junxiang Wang\({}^{\diamond}\) Liang Zhao\({}^{\dagger}\)

\({}^{\dagger}\)Emory University, Atlanta, GA \({}^{\diamond}\)NEC Labs America, Princeton, NJ

{zheng.zhang,liang.zhao}@emory.edu

{junxiang.wang}@alumni.emory.edu

###### Abstract

Graph Neural Networks (GNNs) have achieved great success in representing data with dependencies by recursively propagating and aggregating messages along the edges. However, edges in real-world graphs often have varying degrees of difficulty, and some edges may even be noisy to the downstream tasks. Therefore, existing GNNs may lead to suboptimal learned representations because they usually treat every edge in the graph equally. On the other hand, Curriculum Learning (CL), which mimics the human learning principle of learning data samples in a meaningful order, has been shown to be effective in improving the generalization ability and robustness of representation learners by gradually proceeding from easy to more difficult samples during training. Unfortunately, existing CL strategies are designed for independent data samples and cannot trivially generalize to handle data dependencies. To address these issues, we propose a novel CL strategy to gradually incorporate more edges into training according to their difficulty from easy to hard, where the degree of difficulty is measured by how well the edges are expected given the model training status. We demonstrate the strength of our proposed method in improving the generalization ability and robustness of learned representations through extensive experiments on nine synthetic datasets and nine real-world datasets. The code for our proposed method is available at https://github.com/rollingstonezz/Curriculum_learning_for_GNNs.

## 1 Introduction

Inspired by cognitive science studies [8; 33] that humans can benefit from the sequence of learning basic (easy) concepts first and advanced (hard) concepts later, curriculum learning (CL) [2] suggests training a machine learning model with easy data samples first and then gradually introducing more hard samples into the model according to a designed pace, where the difficulty of samples can usually be measured by their training loss [25]. Many previous studies have shown that this easy-to-hard learning strategy can effectively improve the generalization ability of the model [2; 19; 15; 11; 35; 46], and some studies [19; 15; 11] have shown that CL strategies can also increase the robustness of the learned model against noisy training samples. An intuitive explanation is that in CL settings noisy data samples correspond to harder samples, and CL learner spends less time with the harder (noisy) samples to achieve better generalization performance and robustness.

Although CL strategies have achieved great success in many fields such as computer vision and natural language processing, existing methods are designed for independent data (such as images) while designing effective CL methods for data with dependencies has been largely underexplored. For example, in a citation network, two researchers with highly related research topics (e.g. machine learning and data mining) are more likely to collaborate with each other, while the reason behind a collaboration of two researchers with less related research topics (e.g. computer architecture and social science) might be more difficult to understand. Prediction on one sample impacts that of another, forming a graph structure that encompasses all samples connected by their dependencies. There aremany machine learning techniques for such graph-structured data, ranging from traditional models like conditional random field [36], graph kernels [37], to modern deep models like GNNs [29; 30; 52; 38; 49; 12; 53; 42]. However, traditional CL strategies are not designed to handle the curriculum of the dependencies between nodes in graph data, which are insufficient. Handling graph-structured data require not only considering the difficulty in individual samples, but also the difficulty of their dependencies to determine how to gradually composite correlated samples for learning.

As previous CL strategies indicated that an easy-to-hard learning sequence on data samples can improve the generalization and robustness performance, an intuitive question is whether a similar strategy on data dependencies that iteratively involves easy-to-hard edges in learning can also benefit. Unfortunately, there exists no trivial way to directly generalize existing CL strategies on independent data to handle data dependencies due to several unique challenges: (1) **Difficulty in quantifying edge selection criteria**. Existing CL studies on independent data often use supervised computable metrics (e.g. training loss) to quantify sample difficulty, but how to quantify the difficulties of understanding the dependencies between data samples which has no supervision is challenging. (2) **Difficulty in designing an appropriate curriculum to gradually involve edges**. Similar to the human learning process, the model should ideally retain a certain degree of freedom to adjust the pacing of including edges according to its own learning status. As existing CL methods for graph data typically use fixed pacing function to involve samples, they can not provide this flexibility. Designing an adaptive pacing function for handling graph data is difficult since it requires joint optimization of both supervised learning tasks on nodes and the number of chosen edges. (3) **Difficulty in ensuring convergence and a numerical steady process for CL in graphs**. Discrete changes in the number of edges can cause drift in the optimal model parameters between training iterations. How to guarantee a numerically stable learning process for CL on edges is challenging.

In order to address the aforementioned challenges, in this paper, we propose a novel CL algorithm named **R**elational **C**urriculum **L**earning (**RCL**) to improve the generalization ability and robustness of representation learners on data with dependencies. To address the first challenge, we propose an approach to select the edges by quantifying their corresponding difficulties in a self-supervised learning manner. Specifically, for each training iteration, we choose \(K\) easiest edges whose corresponding relations are most well-expected by the current model. Second, to design an appropriate learning pace for gradually involving more edges in training, we present the learning process as a concise optimization model, which automatically lets the model gradually increase the number \(K\) to involve more edges in training according to its own status. Third, to ensure convergence of optimizing the model, we propose an alternative optimization algorithm with a theoretical convergence guarantee and an edge reweighting scheme to smooth the graph structure transition. Finally, we demonstrate the superior performance of RCL compared to state-of-the-art comparison methods through extensive experiments on both synthetic and real-world datasets.

## 2 Related Works

Curriculum Learning (CL).Bengio et al.[2] pioneered the concept of Curriculum Learning (CL) within the machine learning domain, aiming to improve model performance by gradually including easy to hard samples in training the model. Self-paced learning [25] measures the difficulty of samples by their training loss, which addressed the issue in previous works that difficulties of samples are generated by prior heuristic rules. Therefore, the model can adjust the curriculum of samples according to its own training status. Following works [18; 17; 55] further proposed many supervised measurement metrics for determining curriculums, for example, the diversity of samples [17] or the consistency of model predictions [55]. Meanwhile, many empirical and theoretical studies were proposed to explain why CL could lead to generalization improvement from different perspectives. For example, studies such as MentorNet [19] and Co-teaching [15] empirically found that utilizing CL strategy can achieve better generalization performance when the given training data is noisy. [11] provided theoretical explanations on the denoising mechanism that CL learners waste less time with the noisy samples as they are considered harder samples. Some studies [2; 35; 46; 13; 24] also realized that CL can help accelerate the optimization process of non-convex objectives and improve the speed of convergence in the early stages of training.

Despite great success, most of the existing designed CL strategies are for independent data such as images, and there is little work on generalizing CL strategies to handle samples with dependencies. Few existing attempts on graph-structured data [26; 21; 28], such as [44; 5; 45; 28], simply treat nodes as independent samples and then apply CL strategies on independent data, which ignore the fundamental and unique dependency information carried by the structure in data, and thus can not well handle the correlation between data samples. Furthermore, these models are mostly based on heuristic-based sample selection strategies [5; 45; 28], which largely limit the generalizability of these methods.

Graph structure learning.Another stream of existing studies that are related to our work is _graph structure learning_. Recent studies have shown that GNN models are vulnerable to adversarial attacks on graph structure [7; 48]. In order to address this issue, studies in _graph structure learning_ usually aim to jointly learn an optimized graph structure and corresponding graph representations. Existing works [9; 4; 20; 54; 31] typically consider the hypothesis that the intrinsic graph structure should be sparse or low rank from the original input graph by pruning "irrelevant" edges. Thus, they typically use pre-deterministic methods [7; 56; 9] to preprocess graph structure such as singular value decomposition (SVD), or dynamically remove "redundant" edges according to the downstream task performance on the current sparsified structure [4; 20; 31]. However, modifying the graph topology will inevitably lose potentially useful information lying in the removed edges. More importantly, the modified graph structure is usually optimized for maximizing the performance on the training set, which can easily lead to overfitting issues.

## 3 Preliminaries

Graph neural networks (GNNs) are a class of methods that have shown promising progress in representing structured data in which data samples are correlated with each other. Typically, the data samples are treated as nodes while their dependencies are treated as edges in the constructed graph. Formally, we denote a graph as \(G=(\mathcal{V},\mathcal{E})\), where \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{N}\}\) is a set of nodes that \(N=|\mathcal{V}|\) denotes the number of nodes in the graph and \(\mathcal{E}\subseteq\mathcal{V}\times\mathcal{V}\) is the set of edges. We also let \(\mathbf{X}\in\mathbb{R}^{N\times b}\) denote the node attribute matrix and let \(\mathbf{A}\in\mathbb{R}^{N\times N}\) represent the adjacency matrix. Specifically, \(A_{ij}=1\) denotes that there is an edge connecting nodes \(v_{i}\) and \(v_{j}\in\mathcal{V}\), otherwise \(A_{ij}=0\). A GNN model \(f\) maps the node feature matrix \(\mathbf{X}\) associated with the adjacency matrix \(\mathbf{A}\) to the model predictions \(\hat{\mathbf{y}}=f(\mathbf{X},\mathbf{A})\), and get the loss \(L_{\mathrm{GNN}}=L(\hat{\mathbf{y}},\mathbf{y})\), where \(L\) is the objective function and \(\mathbf{y}\) is the ground-truth label of nodes. The loss on one node \(v_{i}\) is denoted as \(l_{i}=L(\hat{y_{i}},y_{i})\).

## 4 Methodology

As previous CL methods have shown that an easy-to-hard learning sequence of independent data samples can improve the generalization ability and robustness of the representation learner, the goal of this paper is to develop an effective CL method on data with dependencies, which is extremely difficult due to several unique challenges: (1) Difficulty in designing a feasible principle to select

Figure 1: The overall framework of RCL. (a) The _Incremental Edge Selection_ module first extracts the latent node embedding by the GNN model given the current training structure, then jointly learns the node prediction label \(\mathbf{y}\) and reconstructs the input structure by a decoder. A small residual error on an edge indicates the corresponding dependency is well expected and thus can be added to the refined structure for the next iteration. (b) The iterative learning process of RCL. The model starts with an empty structure and gradually includes more edges until the training structure converges to the input structure.

edges by properly quantifying their difficulties. (2) Difficulty in designing an appropriate pace of curriculum to gradually involve more edges in training based on model status. (3) Difficulty in ensuring convergence and a numerical steady process for optimizing the CL model.

In order to address the above challenges, we propose a novel CL method named **R**elational **C**urriculum **L**earning (**RCL**). The sequence, which gradually includes edges from easy to hard, is called _curriculum_ and learned in different grown-up stages of training. In order to address the first challenge, we propose a self-supervised module _Incremental Edge Selection (IES)_, which is shown in Figure 1(a), to select the \(K\) easiest edges at each training iteration that are mostly expected by the current model. The details are elaborated in Section 4.1. To address the second challenge, we present a joint optimization framework to automatically increase the number of selected edges \(K\) given its own training status. The framework is elaborated in Figure 1(b) and details can be found in Section 4.2. Finally, to ensure convergence of optimization and steady the numerical process, we propose an EM-style alternative optimization algorithm with a theoretical convergence guarantee in Section 4.2 Algorithm 1 and an edge reweighting scheme to smooth the discrete edge incrementing process in Section 4.3.

### Incremental Edge Selection by Quantifying Difficulties of Sample Dependencies

Here we propose a novel way to select edges by first quantifying their difficulty levels. Existing works on independent data typically use supervised metrics such as training loss of samples to quantify their difficulty level, but there exists no supervised metrics on edges. To address this issue, we propose a self-supervised module _Incremental Edge Selection (IES)_. We first quantify the difficulty of edges by measuring how well the edges are expected from the currently learned embeddings of their connected nodes. Then the most well-expected edges are selected as the easiest edges for the next iteration of training. As shown in Figure 1(a), given the currently selected edges at iteration \(t\), we first feed them to the GNN model to extract the latent node embeddings. Then we restore the latent node embeddings to the original graph structure through a decoder, which is called the reconstruction of the original graph structure. The residual graph \(\mathbf{R}\), which is defined as the degree of mismatch between the original adjacency matrix \(\mathbf{A}\) and the reconstructed adjacency matrix \(\mathbf{A}^{(t)}\), can be considered a strong indicator for describing how well the edges are expected by the current model. Specifically, a smaller residual error indicates a higher probability of being a well-expected edge.

With the developed self-supervised method to measure the difficulties of edges, here we formulate the key learning paradigm of selecting the top \(K\) easiest edges. To obtain the training adjacency matrix \(\mathbf{A}^{(t)}\) that will be fed into the GNN model \(f^{(t)}\), we introduce a learnable binary mask matrix \(\mathbf{S}\) with each element \(\mathbf{S}_{ij}\in\{0,1\}\). Thus, the training adjacency matrix at iteration \(t\) can be represented as \(\mathbf{A}^{(t)}=\mathbf{S}^{(t)}\odot\mathbf{A}\). To filter out the edges with \(K\) smallest residual error, we penalize the summarized residual errors over the selected edges, which can be represented as \(\sum_{i,j}\mathbf{S}_{ij}\mathbf{R}_{ij}\). Therefore, the learning objective can be presented as follows:

\[\begin{split}&\underset{\mathbf{w}}{\min}L_{\mathrm{GNN}}+\beta \sum_{i,j}\mathbf{S}_{ij}\mathbf{R}_{ij},\\ & s.t.\left\|\mathbf{S}\right\|_{1}\geq K,\end{split}\] (1)

where the first term \(L_{\mathrm{GNN}}=L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) is the node-level predictive loss, e.g. cross-entropy loss for the node classification task. The second term \(\sum_{i,j}\mathbf{S}_{ij}\mathbf{R}_{ij}\) aims at penalizing the residual errors over the edges selected by the mask matrix \(\mathbf{S}\). \(\beta\) is a hyperparameter to tune the balance between terms. The constraint is to guarantee only the most \(K\) well-expected edges are selected.

More concretely, the value of a residual edge \(\tilde{\mathbf{A}}_{ij}^{(t)}\in[0,1]\) can be computed by a non-parametric kernel function \(\kappa(\mathbf{z}_{i}^{(t)},\mathbf{z}_{j}^{(t)})\), e.g. the inner product kernel. Then the residual error \(\mathbf{R}_{ij}\) between the input structure and the reconstructed structure can be defined as \(\left\|\tilde{\mathbf{A}}_{ij}^{(t)}-\mathbf{A}_{ij}\right\|\), where \(\left\|\cdot\right\|\) is commonly chosen to be the squared \(\ell_{2}\)-norm.

### Automatically Control the Pace of Increasing Edges

In order to dynamically include more edges into training, an intuitive way is to iteratively increase the value of \(K\) in Equation 1 to allow more edges to be selected. However, it is difficult to determine an appropriate value of \(K\) with respect to the training status of the model. Besides, directly solvingEquation 1 is difficult since \(\mathbf{S}\) is a binary matrix where each element \(\mathbf{S}_{ij}\in\{0,1\}\), optimizing \(\mathbf{S}\) would require solving a discrete constraint program at each iteration. To address this issue, we first relax the problem into continuous optimization so that each \(\mathbf{S}_{ij}\) can be allowed to take any value in the interval \([0,1]\). Note that the inequality \(||\mathbf{S}||_{1}\geq K\) in Eqn. 1 is equivalent to the equality \(||\mathbf{S}||_{1}=K\). This is because the second term in the loss function would always encourage fewer selected edges by the mask matrix \(\mathbf{S}\), as all values in the residual error matrix \(\mathbf{R}\) and mask matrix \(\mathbf{S}\) are nonnegative. Given this, we can incorporate the equality constraint as a Lagrange multiplier and rewrite the loss function as \(\mathcal{L}=L_{GNN}+\beta\sum_{i,j}\mathbf{S}_{ij}\mathbf{R}_{ij}-\lambda(|| \mathbf{S}||_{1}-K)\). Considering that \(K\) remains constant, the optimization of the loss function can be equivalently framed by substituting the given constraint with a regularization term denoted as \(g(\mathbf{S};\lambda)\). As such, the overall loss function can be reformulated as:

\[\min_{\mathbf{w},\mathbf{S}}L_{\mathrm{GNN}}+\beta\sum_{i,j}\mathbf{S}_{ij} \mathbf{R}_{ij}+g(\mathbf{S};\lambda),\] (2)

where \(g(\mathbf{S};\lambda)=\lambda\left\|\mathbf{S}-\mathbf{A}\right\|\) and \(\left\|\cdot\right\|\) is commonly chosen to be the squared \(\ell_{2}\)-norm. Since the training adjacency matrix \(\mathbf{A}^{(t)}=\mathbf{S}^{(t)}\odot\mathbf{A}\), as \(\lambda\rightarrow\infty\), more edges in the input structure are included until the training adjacency matrix \(\mathbf{A}^{(t)}\) converges to the input adjacency matrix \(\mathbf{A}\). Specifically, the regularization term \(g(\mathbf{S};\lambda)\) controls the learning scheme by the _age parameter_\(\lambda\), where \(\lambda=\lambda(t)\) grows with the number of iterations. By monotonously increasing the value of \(\lambda\), the regularization term \(g(\mathbf{S};\lambda)\) will push the mask matrix gradually converge to the input adjacency matrix \(\mathbf{A}\), resulting in more edges automatically involved in the training structure.

**Optimization of learning objective.** In optimizing the objective function in Equation 2, we need to jointly optimize parameter \(\mathbf{w}\) for GNN model \(f\) and the mask matrix \(\mathbf{S}\). To tackle this, we introduce an EM-style optimization scheme (detailed in Algorithm 1) that iteratively updates both. The algorithm uses the node feature matrix \(\mathbf{X}\), the original adjacency matrix \(\mathbf{A}\), a step size \(\mu\) to control the age parameter \(\lambda\) increase rate, and a hyperparameter \(\gamma\) for regularization adjustments. Post initialization of \(\mathbf{w}\) and \(\mathbf{S}\), it alternates between: optimizing GNN model \(f\) (Step 3), extracting latent node embeddings and reconstructing the adjacency matrix (Steps 4 & 5), refining the mask matrix using the reconstructed matrix and regularization, and results in more edges are gradually involved (Step 6), updating the training adjacency matrix (Step 7), and incrementing \(\lambda\) when the training matrix \(\mathbf{A}^{(t)}\) differs from input matrix \(\mathbf{A}\), incorporating more edges in the next iteration.

**Theorem 4.1**.: _We have the following convergence guarantees for Algorithm 1: \(\bullet\) Avoidance of Saddle Points. If the second derivatives of \(L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) and \(g(\mathbf{S};\lambda)\) are continuous, then for sufficiently large \(\gamma\), any bounded sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) generated by Algorithm 1 with random initializations will not converge to a strict saddle point of \(F\) almost surely. \(\bullet\) Second Order Convergence. If the second derivatives of \(L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) and \(g(\mathbf{S};\lambda)\) are continuous, and \(L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) and \(g(\mathbf{S};\lambda)\) satisfy the Kurdyka-Lojasiewicz (KL) property [41], then for sufficiently large \(\gamma\), any bounded sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) generated by Algorithm 1 with random initialization will almost surely converge to a second-order stationary point of \(F\)._

The detailed proof can be found in Appendix B.

### Smooth Structure Transition by Edge Reweighting

Note that in the Algorithm 1, the optimization process requires iteratively updating the parameters \(\mathbf{w}\) of the GNN model \(f\) and current adjacency matrix \(\mathbf{A}^{(t)}\), where \(\mathbf{A}^{(t)}\) varies discretely between iterations. However, GNN models mostly work in a message-passing fashion, which computes node representations by iteratively aggregating information along edges from neighboring nodes. Discretely modifying the number of edges will result in a great drift of the optimal model parameters between iterations. In Appendix Figure, we demonstrate that a shift in the optimal parameters of the GNN results in a spike in the training loss. Therefore, it can increase the difficulty of finding optimal parameters and even hurt the generalization ability of the model in some cases. Besides the numerical problem caused by discretely increasing the number of edges, another issue raised by the CL strategy in Section 4.1 is the trustworthiness of the estimated edge difficulty, which is inferred by the residual error on the edges. Although the residual error can reflect how well edges are expected in the ideal case, the quality of the learned latent node embeddings may affect the validity of this metric and compromise the quality of the designed curriculum by the CL strategy.

To address both issues, we propose a novel edge reweighting scheme to (1) smooth the transition of the training structure between iterations, and (2) reduce the weight of edges that connect nodes with low-confidence latent embeddings. Formally, we use a smoothed version of structure \(\bar{\mathbf{A}}^{(t)}\) to substitute \(\mathbf{A}^{(t)}\) for training the GNN model \(f\) in step 3 of Algorithm 1, where the mapping from \(\mathbf{A}^{(t)}\) to \(\bar{\mathbf{A}}^{(t)}\) can be represented as:

\[\bar{\mathbf{A}}^{(t)}_{ij}=\pi^{(t)}_{ij}\mathbf{A}^{(t)}_{ij},\] (3)

where \(\pi^{(t)}_{ij}\) is the weight imposed on edge \(e_{ij}\) at iteration \(t\). \(\pi^{(t)}_{ij}\) is calculated by considering the counted occurrences of edge \(e_{ij}\) until the iteration \(t\) and the confidence of the latent embedding for the connected pair of nodes \(v_{i}\) and \(v_{j}\):

\[\pi^{(t)}_{ij}=\psi(e_{ij})\rho(v_{i})\rho(v_{j}),\] (4)

where \(\psi\) is a function that reflects the number of edge occurrences and \(\rho\) is a function to reflect the degree of confidence for the learned latent node embedding. The details of these two functions are described as follow.

**Smooth the transition of the training structure between iterations.** In order to obtain a smooth transition of the training structure between iterations, we take the learned curriculum of selected edges into consideration. Formally, we model \(\psi\) by a smooth function of the edge selected occurrences compared to the model iteration occurrences before the current iteration:

\[\psi(e_{ij})=t(e_{ij})/t,\] (5)

where \(t\) is the number of current iterations and \(t(e_{ij})\) represents the counting number of selecting edge \(e_{ij}\). Therefore, we transform the original discretely changing training structure into a smoothly changing one by taking the historical edge selection curriculum into consideration.

**Reduce the influence of nodes with low confidence latent embeddings.** As introduced in our Algorithm 1 line 6, the estimated structure \(\tilde{A}\) is inferred from the latent embedding \(\mathbf{Z}\), which is extracted from the trained GNN model \(f\). Such estimated latent embedding may possibly differ from the true underlying embedding, which results in the inaccurately reconstructed structure around the node. In order to alleviate this issue, we model the function \(\rho\) by the training loss on nodes, which indicates the confidence of their learned latent embeddings. This idea is similar to previous CL strategies on inferring the difficulty of data samples by their supervised training loss. Specifically, a larger training loss indicates a low confident latent node embedding. Mathematically, the weights \(\rho(v_{i})\) on node \(v_{i}\) can be represented as a distribution of their training loss:

\[\rho(v_{i})\sim e^{-l_{i}}\] (6)

where \(l_{i}\) is the training loss on node \(v_{i}\). Therefore, a node with a larger training loss will result in a smaller value of \(\rho(v_{i})\), which reduces the weight of its connecting edges.

## 5 Experiments

In this section, the experimental settings are introduced first in Section 5.1, then the performance of the proposed method on both synthetic and real-world datasets are presented in Section 5.2. We further present the robustness test on our CL method against topological structure noise in Section 5.3.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

out of 43 cases with a significance \(p<0.05\). Such statistical significance results can demonstrate that our proposed method can consistently perform better than the baseline models in both scenarios.

### Robustness Analysis Against Topological Noise

To further examine the robustness of the RCL method on extracting powerful representation from correlated data samples, we follow previous works [20; 31] to randomly inject fake edges into real-world graphs. This adversarial attack can be viewed as adding random noise to the topological structure of graphs. Specifically, we randomly connect \(M\) pairs of previously unlinked nodes in the real-world datasets, where the value of \(M\) varies from 10% to 100% of the original edges. We then train RCL and all the comparison methods on the attacked graph and evaluate the node classification performance. The results are shown in Figure 2, we can observe that RCL shows strong robustness to adversarial structural attacks by consistently outperforming all compared methods on all datasets. Especially, when the proportion of added noisy edges is large (\(>50\%\)), the improvement becomes more significant. For instance, under the extremely noisy ratio at 100%, RCL outperforms the second best model by 4.43% and 2.83% on Cora dataset, and by 6.13%, 3.47% on Citeseer dataset, with GCN and GIN backbone models, respectively.

### Ablation Study

To investigate the effectiveness of our proposed model with some simpler heuristics, we deploy a series of ablation analysis. We first train the model with node classification task purely and select the top K expected edges as suggested by the reviewer. Specifically, we follow previous works [43; 45] using two classical selection pacing functions as follows:

\[\mathrm{Linear}\colon K_{\mathrm{linear}}(t)=\frac{t}{T}|E|;\ \ \mathrm{Root} \colon K_{\mathrm{root}}(t)=\sqrt{\frac{t}{T}}|E|,\]

where \(t\) is the number of current iterations and \(T\) is the number of total iterations, and \(|E|\) is the number of total edges. We name these two variants Curriculum-linear and Curriculum-root, respectively. In addition, we also remove the edge difficulty measurement module and use random selection instead. Specifically, we gradually incorporate more edges into training in random order to verify the effectiveness of the learned curriculum. We name two variants as Random-linear and Random-root with the above two mentioned pacing functions, respectively.

In order to further investigate the impact of the proposed components of RCL. We also first consider variants of removing the edge smoothing components mentioned in Section 4.3. Specifically, we

Figure 2: Node classification accuracy (\(\%\)) on Cora and Citeseer under random structure attack. The attack edge ratio is computed versus the original number of edges, where \(100\%\) means that the number of inserted edges is equal to the number of original edges.

\begin{table}
\begin{tabular}{c|c c c c c} \hline  & **Synthetic1** & **Synthetic2** & **Citeseer** & **CS** & **Computers** \\ \hline Full & **73.98\(\pm\)0.55** & **79.42\(\pm\)0.17** & **79.79\(\pm\)0.55** & **94.66\(\pm\)0.22** & **90.23\(\pm\)0.23** \\ Curriculum-linear & 70.93\(\pm\)0.54 & 95.19\(\pm\)0.19 & 79.04\(\pm\)0.38 & 94.14\(\pm\)0.26 & 89.28\(\pm\)0.21 \\ Curriculum-root & 70.13\(\pm\)0.72 & 95.50\(\pm\)0.18 & 78.27\(\pm\)0.54 & 94.47\(\pm\)0.34 & 89.27\(\pm\)0.15 \\ Random-linear & 58.76\(\pm\)0.46 & 89.78\(\pm\)0.11 & 77.43\(\pm\)0.49 & 92.76\(\pm\)0.14 & 88.67\(\pm\)0.18 \\ Random-root & 61.04\(\pm\)0.20 & 91.04\(\pm\)0.09 & 76.81\(\pm\)0.35 & 92.92\(\pm\)0.15 & 88.81\(\pm\)0.28 \\ w/o edge appearance & 70.70\(\pm\)0.43 & 57.77\(\pm\)0.16 & 77.77\(\pm\)0.65 & 43.92\(\pm\)0.21 & 89.56\(\pm\)0.30 \\ w/o node confidence & 72.38\(\pm\)0.41 & 69.86\(\pm\)0.17 & 78.72\(\pm\)0.72 & 94.34\(\pm\)0.13 & 90.03\(\pm\)0.62 \\ w/o pre-trained model & 72.56\(\pm\)0.69 & 93.89\(\pm\)0.14 & 78.28\(\pm\)0.77 & 94.50\(\pm\)0.14 & 89.80\(\pm\)0.55 \\ \hline \end{tabular}
\end{table}
Table 3: Ablation study. Here “Full” represents the original method without removing any component. The best-performing method on each dataset is highlighted in bold.

consider two variants _w/o EC_ and _w/o NC_, which remove the smoothing function of the edge occurrence ratio and the component to reflect the degree of confidence for the latent node embedding in RCL, respectively. In addition to examining the effectiveness of edge smoothing components, we further consider a variant _w/o pre-trained model_ that avoids using a pre-trained model to initialize model, which is mentioned in Section 5.1, to initialize the training structure by a pre-trained model and instead starts with inferred structure from isolated nodes with no connections.

We present the results of two synthetic datasets (_homophily coefficient\(=0.3,0.6\)_) and three real-world datasets in Table 3. We summarize our findings from the above table as below: (i) Our full model consistently outperforms the two variants Curriculum-linear and Curriculum-root by an average of 1.59% on all datasets, suggesting that our pacing module can benefit model training. It is worth noting that these two variants also outperform the baseline vanilla GNN model Vanilla by an average of 1.92%, which supports the assumption that even a simple curriculum learning strategy can still improve model performance. (ii) We observe that the performance of the two variants Random-linear and Random-root on all datasets drops by 3.86% on average compared to the variants Curriculum-linear and Curriculum-root. Such behavior demonstrates the effectiveness of our proposed edge difficulty quantification module by showing that randomly involving edges into training cannot benefit model performance. (iii) We can observe a significant performance drop consistently for all variants that remove the structural smoothing techniques and initialization components. The results validate that all structural smoothing and initialization components can benefit the performance of RCL on the downstream tasks.

### Visualization of Learned Edge Selection Curriculum

Besides the effectiveness and robustness of the RCL method on downstream classification results, it is also interesting to verify whether the learned edge selection curriculum satisfies the rule from easy to hard. Since real-world datasets do not have ground-truth labels of difficulty on edges, we conduct visualization experiments on synthetic datasets, where the difficulty of each edge can be indicated by its formation probability. Specifically, we classify edges into three balanced categories according to their difficulty: easy, medium, and hard. Here, we define all homogenous edges that connect nodes with the same class as easy, edges connecting nodes with adjacent classes as medium, and the remaining edges connecting nodes with far away classes as hard. We report the proportion of edges selected for each category during training in Figure 3. We can observe that RCL can effectively select most of the easy edges at the early stage of training, then more easy edges and most medium edges are gradually included during training, and most hard edges are left unselected until the end stage of training. Such edge selection behavior is highly consistent with the core idea of designing a curriculum for edge selection, which verifies that our proposed method can effectively design curriculums to select edges according to their difficulty from easy to hard.

## 6 Conclusion

This paper focuses on developing a novel CL method to improve the generalization ability and robustness of GNN models on learning representations of data samples with dependencies. The proposed method **R**elational **C**urriculum **L**earning (**RCL**) effectively addresses the unique challenges in designing CL strategy for handling dependencies. First, a self-supervised learning module is developed to select appropriate edges that are expected by the model. Then an optimization model is presented to iteratively increment the edges according to the model training status and a theoretical guarantee of the convergence on the optimization algorithm is given. Finally, an edge reweighting scheme is proposed to steady the numerical process by smoothing the training structure transition. Extensive experiments on synthetic and real-world datasets demonstrate the strength of RCL in improving the generalization ability and robustness.

Figure 3: Visualization of edge selection process during training.

## Acknowledgement

This work was supported by the National Science Foundation (NSF) Grant No. 1755850, No. 1841520, No. 2007716, No. 2007976, No. 1942594, No. 1907805, a Jeffress Memorial Trust Award, Amazon Research Award, NVIDIA GPU Grant, and Design Knowledge Company (subcontract number: 10827.002.120.04). The authors acknowledge Emory Computer Science department for providing computational resources and technical support that have contributed to the experimental results reported within this paper.

## References

* [1] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing. In _international conference on machine learning_, pages 21-29. PMLR, 2019.
* [2] Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 41-48, 2009.
* [3] Jie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: Fast learning with graph convolutional networks via importance sampling. In _International Conference on Learning Representations_, 2018.
* [4] Yu Chen, Lingfei Wu, and Mohammed Zaki. Iterative deep graph learning for graph neural networks: Better and robust node embeddings. _Advances in neural information processing systems_, 33:19314-19326, 2020.
* [5] Guanyi Chu, Xiao Wang, Chuan Shi, and Xunqiang Jiang. Cuco: Graph representation with curriculum contrastive learning. In _IJCAI_, pages 2300-2306, 2021.
* [6] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal neighbourhood aggregation for graph nets. _Advances in Neural Information Processing Systems_, 33:13260-13271, 2020.
* [7] Hanjun Dai, Hui Li, Tian Tian, Xin Huang, Lin Wang, Jun Zhu, and Le Song. Adversarial attack on graph structured data. In _International conference on machine learning_, pages 1115-1124. PMLR, 2018.
* [8] Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. _Cognition_, 48(1):71-99, 1993.
* [9] Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you need is low (rank) defending against adversarial attacks on graphs. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, pages 169-177, 2020.
* [10] Matthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [11] Tieliang Gong, Qian Zhao, Deyu Meng, and Zongben Xu. Why curriculum learning & self-paced learning work in big/noisy data: A theoretical perspective. _Big Data & Information Analytics_, 1(1):111, 2016.
* [12] Xiaojie Guo, Shiyu Wang, and Liang Zhao. Graph neural networks: Graph transformation. _Graph Neural Networks: Foundations, Frontiers, and Applications_, pages 251-275, 2022.
* [13] Guy Hacohen and Daphna Weinshall. On the power of curriculum learning in training deep networks. In _International Conference on Machine Learning_, pages 2535-2544. PMLR, 2019.
* [14] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [15] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. _Advances in neural information processing systems_, 31, 2018.

* [16] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _arXiv preprint arXiv:2005.00687_, 2020.
* [17] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. Self-paced learning with diversity. _Advances in neural information processing systems_, 27, 2014.
* [18] Lu Jiang, Deyu Meng, Qian Zhao, Shiguang Shan, and Alexander G Hauptmann. Self-paced curriculum learning. In _Twenty-ninth AAAI conference on artificial intelligence_, 2015.
* [19] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In _International conference on machine learning_, pages 2304-2313. PMLR, 2018.
* [20] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 66-74, 2020.
* [21] Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, et al. A comprehensive survey on deep graph representation learning. _arXiv preprint arXiv:2304.05055_, 2023.
* [22] Fariba Karimi, Mathieu Genois, Claudia Wagner, Philipp Singer, and Markus Strohmaier. Homophily influences ranking of minorities in social networks. _Scientific reports_, 8(1):1-12, 2018.
* [23] Thomas N. Kipf and Max Welling. Semi-Supervised Classification with Graph Convolutional Networks. In _Proceedings of the 5th International Conference on Learning Representations_, 2017.
* [24] Yajing Kong, Liu Liu, Jun Wang, and Dacheng Tao. Adaptive curriculum learning. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5067-5076, 2021.
* [25] M Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable models. _Advances in neural information processing systems_, 23, 2010.
* [26] Haoyang Li, Xin Wang, and Wenwu Zhu. Curriculum graph machine learning: A survey. _arXiv preprint arXiv:2302.02926_, 2023.
* [27] Qiuwei Li, Zhihui Zhu, and Gongguo Tang. Alternating minimizations converge to second-order optimal solutions. In _International Conference on Machine Learning_, pages 3935-3943. PMLR, 2019.
* [28] Xiaohe Li, Lijie Wen, Yawen Deng, Fuli Feng, Xuming Hu, Lei Wang, and Zide Fan. Graph neural network with curriculum learning for imbalanced node classification. _arXiv preprint arXiv:2202.02529_, 2022.
* [29] Chen Ling, Junji Jiang, Junxiang Wang, and Zhao Liang. Source localization of graph diffusion via variational autoencoders for graph inverse problems. In _Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining_, pages 1010-1020, 2022.
* [30] Chen Ling, Junji Jiang, Junxiang Wang, My T Thai, Renhao Xue, James Song, Meikang Qiu, and Liang Zhao. Deep graph representation learning and optimization for influence maximization. In _International Conference on Machine Learning_, pages 21350-21361. PMLR, 2023.
* [31] Dongsheng Luo, Wei Cheng, Wenchao Yu, Bo Zong, Jingchao Ni, Haifeng Chen, and Xiang Zhang. Learning to drop: Robust graph neural network via topological denoising. In _Proceedings of the 14th ACM international conference on web search and data mining_, pages 779-787, 2021.

* [32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [33] Douglas LT Rohde and David C Plaut. Language acquisition in the absence of explicit negative evidence: How important is starting small? _Cognition_, 72(1):67-109, 1999.
* [34] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls of graph neural network evaluation. _arXiv preprint arXiv:1811.05868_, 2018.
* [35] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training region-based object detectors with online hard example mining. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 761-769, 2016.
* [36] Charles Sutton, Andrew McCallum, et al. An introduction to conditional random fields. _Foundations and Trends(r) in Machine Learning_, 4(4):267-373, 2012.
* [37] S Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph kernels. _Journal of Machine Learning Research_, 11:1201-1242, 2010.
* [38] Junxiang Wang, Junji Jiang, and Liang Zhao. An invertible graph diffusion neural network for source localization. In _Proceedings of the ACM Web Conference 2022_, pages 1058-1069, 2022.
* [39] Junxiang Wang, Hongyi Li, Zheng Chai, Yongchao Wang, Yue Cheng, and Liang Zhao. Toward quantized model parallelism for graph-augmented mlps based on gradient-free admm framework. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [40] Junxiang Wang, Hongyi Li, and Liang Zhao. Accelerated gradient-free neural network training by multi-convex alternating optimization. _Neurocomputing_, 487:130-143, 2022.
* [41] Junxiang Wang, Fuxun Yu, Xiang Chen, and Liang Zhao. Admm for efficient deep learning with global convergence. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 111-119, 2019.
* [42] Shiyu Wang, Xiaojie Guo, and Liang Zhao. Deep generative model for periodic graphs. _Advances in Neural Information Processing Systems_, 35, 2022.
* [43] Xin Wang, Yudong Chen, and Wenwu Zhu. A survey on curriculum learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(9):4555-4576, 2021.
* [44] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Curgraph: Curriculum learning for graph classification. In _Proceedings of the Web Conference 2021_, pages 1238-1248, 2021.
* [45] Xiaowen Wei, Weiwei Liu, Yibing Zhan, Du Bo, and Wenbin Hu. Clnode: Curriculum learning for node classification. _arXiv preprint arXiv:2206.07258_, 2022.
* [46] Daphna Weinshall, Gad Cohen, and Dan Amir. Curriculum learning by transfer learning: Theory and experiments with deep networks. In _International Conference on Machine Learning_, pages 5238-5246. PMLR, 2018.
* [47] Richard Lee Wheeden and Antoni Zygmund. _Measure and integral_, volume 26. Dekker New York, 1977.
* [48] Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples for graph data: deep insights into attack and defense. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, pages 4816-4823, 2019.
* [49] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_, 32(1):4-24, 2020.
* [50] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2018.

* [51] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In _International conference on machine learning_, pages 40-48. PMLR, 2016.
* [52] Zheng Zhang and Liang Zhao. Representation learning on spatial networks. _Advances in Neural Information Processing Systems_, 34:2303-2318, 2021.
* [53] Zheng Zhang and Liang Zhao. Unsupervised deep subgraph anomaly detection. In _2022 IEEE International Conference on Data Mining (ICDM)_, pages 753-762. IEEE, 2022.
* [54] Cheng Zheng, Bo Zong, Wei Cheng, Dongjin Song, Jingchao Ni, Wenchao Yu, Haifeng Chen, and Wei Wang. Robust graph representation learning via neural sparsification. In _International Conference on Machine Learning_, pages 11458-11468. PMLR, 2020.
* [55] Tianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label detection to noisy label self-correction. In _International Conference on Learning Representations_, 2020.
* [56] Daniel Zugner, Amir Akbarnejad, and Stephan Gunnemann. Adversarial attacks on neural networks for graph data. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 2847-2856, 2018.

[MISSING_PAGE_EMPTY:15]

epochs) are equal for all models. For the pre-trained model to initialize the training structure, we utilize the same model as the backbone model utilized by our method. For example, if we use GCN as the backbone model for RCL, the pre-trained model to initialize is also GCN. All experiments are conducted on a 64-bit machine with four NVIDIA Quadro RTX 8000 GPUs. The proposed method is implemented with Pytorch deep learning framework [32].

The following describes the details of our comparison models.

**Graph Neural Networks (GNNs).** We first introduce three baseline GNN models as follows.

**(i) GCN.** Graph Convolutional Networks (GCN) [23] is a commonly used GNN, which introduces a first-order approximation architecture of the Chebyshev spectral convolution operator;

**(ii) GIN.** Graph Isomorphism Networks (GIN) [50] is a variant of GNN, which has provably powerful discriminating power among the class of 1-order GNNs;

**(iii) GraphSage.** GraphSage [14] is a GNN method that computes the hidden representation of the root node by aggregating the hidden node representations hierarchically from bottom to top.

**Graph structure learning.** We then introduce four state-of-the-art methods for jointly learning the optimal graph structure and downstream tasks.

**(i) GNNSVD.** GNNSVD [9] first apply singular value decomposition (SVD) on the graph adjacency matrix to obtain a low-rank graph structure and apply GNN on the obtained low-rank structure;

**(ii) ProGNN.** ProGNN [20] is a method to defend against graph adversarial attacks by obtaining a sparse and low-rank graph structure from the input structure;

**(iii) NeuralSparse.** NeuralSparse [54] is a method to learn robust graph representations by iteratively sampling \(k\)-neighbor subgraphs for each node and sparsing the graph according to the performance on the node classification;

**(iv) PTDNet.** PTDNet [31] learns a sparsified graph by pruning task-irrelevant edges, where sparsity is controlled by regulating the number of edges.

**Curriculum learning on graph data.** We introduce a recent curriculum learning work on node classification as follows.

**(i) CLNode.** CLNode [45] regards nodes as data samples and gradually incorporates more nodes into training according to their difficulty. They apply a heuristic-based strategy to measure the difficulty of nodes, where the nodes that connect neighboring nodes with different classes are considered difficult.

**Searching space for hyperparameters.**

Number of epochs trained: \(\{150,500\}\);

Learning rate for model: \(\{1e{-}2,5e{-}3,1e{-}3\}\);

Number of GNN layers: \(\{2\}\);

Dimension of hidden state: \(\{64\}\);

Age parameter \(\lambda:\{1,2,3,4,5\}\) (A larger value indicates faster pacing for adding edges, where \(1\) denotes the training structure will converge to the input structure at the final iteration).

### Additional Effectiveness Experiments on Heterophilic Datasets

\begin{table}
\begin{tabular}{l|c|c c|c c} \hline \hline Dataset & Edge homo ratio & GCN & GCN-RCL & GIN & GIN-RCL \\ \hline Texas & 0.11 & 0.5645 & **0.6006** & 0.5885 & **0.6156** \\ Cornell & 0.30 & 0.4084 & **0.5045** & 0.4234 & **0.4925** \\ Wisconsin & 0.21 & 0.4923 & **0.5294** & 0.5141 & **0.5599** \\ Actor & 0.22 & 0.2868 & **0.3186** & 0.2678 & **0.3006** \\ Squirrel & 0.22 & 0.2743 & **0.2999** & 0.2347 & **0.2519** \\ Chameleon & 0.23 & 0.3625 & **0.4385** & 0.3233 & **0.4033** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Node classification results for six real-world heterophilic datasets, where the best performance of each model category in one dataset is highlighted.

In order to further verify the effectiveness of our proposed strategy on heterophilic graph datasets, we have included new experiments on six real-world heterophilic datasets. As shown in Table 4, our method consistently improve performance of backbone GNN models on these heterophilic datasets. Specifically, RCL outperforms the second best method on average by 5.04%, and 4.55%, on GCN and GIN backbones, respectively. The results can demonstrate our method is not limited to homophily graphs.

Although the inner product decoder utilized in experiments might imply an underlying homophily assumption, our method can still benefit from leveraging the edge curriculum present within the input datasets. A reasonable explanation is that standard GNN models are usually struggled with the heterophily edges, while our methodology designs a curriculum allowing more focus on homophily edges, which potentially leads to the observed performance boost.

### Additional Effectiveness Experiments on PNA Backbone Model.

In Table 5, new experiments that adopt modern GNN architecture - PNA model [6] have been added. From the table we can observe that our proposed method improves the performance of PNA backbone by 2.54% on average, which further verified the effectiveness of our method under different choices of backbone GNN model.

In addition, in Table 5 we further include two traditional CL methods for independent data as additional baselines, following classical works [25, 2]. We employed the supervised training loss of a pretrained GNN model as the difficulty metric, and selected two well-established pacing functions for curriculum design: linear and root pacing, defined as follows:

\[\mathrm{Linear}\colon K_{\mathrm{linear}}(t)=\frac{t}{T}|V|;\mathrm{Root} \colon K_{\mathrm{root}}(t)=\sqrt{\frac{t}{T}}|V|,\]

where \(t\) is the number of current iterations and \(T\) is the number of total iterations, and \(|V|\) is the number of nodes.

We utilized GCN and PNA as backbone architectures, identified by the suffixes '-linear' and '-root'. Across all datasets, the results consistently demonstrate that our proposed method outperforms traditional CL approaches.

### Additional Robustness Experiments on PNA Backbone Model.

We present further robustness test against random noisy edges by using the PNA backbone model. The results are shown in Table 6, which further proves that our curriculum learning approach improves the robustness against edge noise with the advanced PNA model as the backbone.

### Time Complexity Analysis

Here we consider GCN as the backbone. First, the time complexity of an \(L\)-layer GCN is \(O(L|\mathcal{E}|b+L|\mathcal{V}|b^{2})\), where \(b\) is the number of node attributes. Second, the time complexity of measuring the

\begin{table}
\begin{tabular}{l|c c c c|c c c} \hline \hline Dataset & PNA & PNA-RCL & PNA-linear & PNA-root & GCN & GCN-RCL & GCN-linear & GCN-root \\ \hline Synthetic-0.3 & 0.6982 & **0.7667** & 0.7463 & 0.7445 & 0.6517 & **0.7398** & 0.6641 & 0.6533 \\ Synthetic-0.5 & 0.8742 & **0.9016** & 0.8476 & 0.8704 & 0.8715 & **0.9269** & 0.8494 & 0.8854 \\ Synthetic-0.7 & 0.9658 & **0.9821** & 0.9514 & 0.9766 & 0.9748 & **0.9962** & 0.9712 & 0.9796 \\ Cora & 0.8310 & **0.8521** & 0.8145 & 0.8254 & 0.8574 & **0.8715** & 0.8327 & 0.8553 \\ Citeseer & 0.7478 & **0.7652** & 0.7482 & 0.7505 & 0.7893 & **0.7979** & 0.7723 & 0.7814 \\ Computers & 0.8989 & **0.9096** & 0.8866 & 0.8975 & 0.8809 & **0.9023** & 0.8713 & 0.8985 \\ ogbn-arxiv & 0.7175 & **0.7441** & 0.6980 & 0.7242 & 0.7174 & **0.7408** & 0.7288 & 0.7359 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Node classification results for our method and traditional CL methods using PNA and GCN as backbone. Here ‘-RCL’ denotes our method, while ‘-linear’ and ‘-root’ denotes two traditional CL methods with different pacing functions.

difficulty levels of edges by reconstruction is \(O(|\mathcal{E}|d)\) where \(d\) is the number of latent embedding dimensions. Third, the time complexity of selecting the edges to add is \(O(|\mathcal{E}|)\). Therefore, the total time complexity of our algorithm is \(O(|\mathcal{E}|(Lb+d)+L|\mathcal{V}|b^{2})\).

In addition, we compare the total running time of our method and all comparison methods in the Table 7. We can observe that the running time of our proposed method is comparable to that of standard GNN models in all datasets. Notably, our method is even faster than standard GNN models in some datasets. One possible reason is that at the beginning of training, the graphs in our model have much fewer edges than those in standard GNN models. Therefore, the computational cost of the GNN model is also reduced.

### Parameter Sensitivity Analysis

Recall that RCL learns a curriculum to gradually add edges in a given input graph structure to the training process until all edges are included. An interesting question is how the speed of adding edges will affect the performance of the model. Here we conduct experiments to explore the impact of age parameter \(\lambda\) which controls the speed of adding edges to the model performance. Here a larger value of \(\lambda\) means that the training structure will converge to the input structure earlier. For example, \(\lambda=1\) means that the training structure will probably not converge to the input structure until the last iteration, and \(\lambda=5\) means that the training structure will converge to the input structure around half of the iterations are complete, and then the model will be trained with the full input structure for the remaining iterations. We present the results on two synthetic datasets (_homophily coefficient\(=0.3,0.6\)_) and two real-world datasets in Figure 5. As can be seen from the figure, the classification results are steady that the average standard deviation is only 0.41%. It is also worth noting that the peak values for all datasets consistently appear around \(\lambda=3\), which indicates that the best performance is when the training structure converges to the full input structure around two-thirds of the iterations are completed.

\begin{table}
\begin{tabular}{l|l|c c c c c c c c c} \hline Dataset & Method & 0\% & 10\% & 20\% & 30\% & 40\% & 50\% & 60\% & 70\% & 80\% & 90\% \\ \hline Cora & PNA & 0.8310 & 0.7911 & 0.7621 & 0.7402 & 0.7331 & 0.7210 & 0.6894 & 0.7042 & 0.6792 & 0.6617 \\ Cora & PNA-RCL & 0.8521 & 0.8315 & 0.8162 & 0.7969 & 0.7992 & 0.7951 & 0.7571 & 0.7642 & 0.7457 & 0.7371 \\ Citeseer & PNA & 0.7478 & 0.7195 & 0.7184 & 0.6934 & 0.6952 & 0.6920 & 0.6852 & 0.6552 & 0.6481 & 0.6327 \\ Citeseer & PNA-RCL & 0.7652 & 0.7422 & 0.7222 & 0.7254 & 0.7041 & 0.7012 & 0.6953 & 0.6921 & 0.6884 & 0.6794 \\ \hline \end{tabular}
\end{table}
Table 6: Further robustness test using PNA as backbone model. Here the percentage denotes the ratio of number of added random edges to the original edges.

\begin{table}
\begin{tabular}{l|l l l l l} \hline  & Synthetic & Citeseer & Computers & ogbn-arxiv & ogbn-proteins \\ \hline Vanilla & 7.32s & 3.90s & 16.88s & 55.22s & 1438.23s \\ GNNSVD & 11.49s & 3.82s & 35.96s & 135.72s & 2632.42s \\ CLNode & 6.29s & 3.96s & 17.02s & 58.53s & 1545.53s \\ ProGNN & 220.25s & 72.42s & 1953.23s & (-) & (-) \\ NeuralSparse & 310.02s & 88.91s & 6553.34s & (-) & (-) \\ PTDNet & 153.43s & 48.42s & 2942.02s & (-) & (-) \\ Ours & 4.07s & 2.42s & 14.62s & 71.49s & 2239.05s \\ \hline \end{tabular}
\end{table}
Table 7: Running time of our method and comparison methods. Here (-) denotes an out-of-memory error and Vanilla denotes the standard GNN model.

Figure 5: Parameter sensitivity analysis on four datasets. Here a larger value of \(\lambda\) means the training structure will converge to the original structure at an earlier training stage.

### Visualization of Importance on Smoothing Component

Our experimental results demonstrated the importance of applying our smoothing component in stablizing the optimization process of training. Figure 6 shows that without the smoothing technique, the training loss spiked that reflects the GNN parameter shifts, which was caused by the number of edges discretely changed. However, after adding the smoothing technique, the training loss can smoothly converge, hence, the smoothing technique plays an important role in stabilizing the training process.

## Appendix B Mathematical Proof

**Theorem 1.** We have the following convergence guarantees for Algorithm 1:

\(\bullet\)**Avoidance of Saddle Points** If the second derivatives of \(L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) and \(g(\mathbf{S};\lambda)\) are continuous, then for sufficiently large \(\gamma\), any bounded sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) generated by Algorithm 1 with random initializations will not converge to a strict saddle point of \(F\) almost surely.

\(\bullet\)**Second Order Convergence** If the second derivatives of \(L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) and \(g(\mathbf{S};\lambda)\) are continuous, and \(L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w}),\mathbf{y})\) and \(g(\mathbf{S};\lambda)\) satisfy the Kurdyka-Lojasiewicz (KL) property [40], then for sufficiently large \(\gamma\), any bounded sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) generated by Algorithm 1 with random initialization will almost surely converges to a second-order stationary point of \(F\).

Proof.: We prove this theorem by Theorem 10 and Corollary 3 from [27].

**[Avoidance of Saddle Points]** Because the sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) is bounded, and the second derivatives of \(L\) and \(g\) are continuous, then they are bounded. In other words, we have \(\max\{\left\|\nabla_{\mathbf{w}}^{2}L(f(\mathbf{X},\mathbf{A}^{(t)};\mathbf{w }^{(t)}),\mathbf{y})\right\|,\left\|\nabla_{\mathbf{S}}^{2}g(S^{(t)};\lambda) \right\|\}\leq p\), where \(p>0\) is a constant. Similarly, it is easy to check that the second derivative of the term \(\sum_{i,j}\mathbf{S}_{ij}\left\|\tilde{\mathbf{A}}_{ij}^{(t)}-\mathbf{A}_{ij} \right\|_{2}^{2}\) is bounded, i.e., \(\max\{\left\|\nabla_{\mathbf{w}}^{2}\sum_{i,j}\mathbf{S}_{ij}\left\|\tilde{ \mathbf{A}}_{ij}^{(t)}-\mathbf{A}_{ij}\right\|_{2}^{2}\right\|,\left\|\nabla_ {\mathbf{S}}^{2}\sum_{i,j}\mathbf{S}_{ij}\left\|\tilde{\mathbf{A}}_{ij}^{(t)} -\mathbf{A}_{ij}\right\|_{2}^{2}\right\|\}\leq q\), where \(q>0\) is constant and \(\tilde{\mathbf{A}}\) is a function of \(\mathbf{w}\). Therefore, it means that the objective \(F\) is bi-smooth, i.e. \(\max\{\left\|\nabla_{\mathbf{w}}^{2}F\right\|\},\left\|\nabla_{\mathbf{S}}^{ 2}F\right\|\}\leq p+q\). In other words, \(F\) satisfies Assumption 4 from [27]. Moreover, the second derivative of \(F\) is continuous. For any \(\gamma>p+q\), any bounded sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) generated by Algorithm 1 will not converge to a strict saddle of \(F\) almost surely by Theorem 10 from [27].

**[Second Order Convergence]** From the above proof of avoidance of saddle points, we know that \(F\) satisfies Assumption 4 from [27]. Moreover, because \(L\) and \(g\) satisfy the KL property, and the term \(\sum_{i,j}\mathbf{S}_{ij}\left\|\tilde{\mathbf{A}}_{ij}^{(t)}-\mathbf{A}_{ij} \right\|_{2}^{2}\) satisfies the KL property, we conclude that \(F\) satisfy the KL property as

Figure 6: The comparison between our full model and the version without smoothing technique on the training loss trend.

well. From the proof above, we also know that the second derivative of \(F\) is continuous. Because continuous differentiability implies Lipschitz continuity [47], it infers that the first derivative of \(F\) is Lipschitz continuous. As a result, \(F\) satisfies Assumption 1 from [27]. Because \(F\) satisfies Assumptions 1 and 4, then for any \(\gamma>p+q\), any bounded sequence \((\mathbf{w}^{(t)},\mathbf{S}^{(t)})\) generated by Algorithm 1 will almost surely converges to a second-order stationary point of \(F\) by Corollary 3 from [27]. 

While the convergence of Algorithm 1 entails the second-order optimality conditions of \(f\) and \(g\), some commonly used \(f\) such as the GNN with sigmoid or tanh activations and some commonly used \(g\) such as the squared \(\ell_{2}\) norm satisfy the KL property [39, 40], and Algorithm 1 is guaranteed to avoid a strict saddle point and converges to a second-order stationary point.