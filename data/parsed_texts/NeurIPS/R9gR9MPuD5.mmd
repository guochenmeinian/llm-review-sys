# InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques

 Rohan Gupta

cybershiptrooper@gmail.com

&Ivan Arcuschin

University of Buenos Aires

iarcuschin@dc.uba.ar

Thomas Kwa

kwathomas0@gmail.com

&Adria Garriga-Alonso

FAR AI

adria@far.ai

###### Abstract

Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.

## 1 Introduction

The field of mechanistic interpretability (MI) aims to reverse-engineer the algorithm implemented by a neural network [14]. The current MI paradigm holds that the neural network (NN) represents concepts as _features_, which may have their dedicated subspace [8; 31] or be in _superposition_ with other features [15; 16; 32]. The NN arrives at its output by composing many _circuits_, which are subcomponents that implement particular functions on the features [9; 20; 32]. To date, the field has been very successful at reverse-engineering toy models on simple tasks [7; 10; 11; 30; 47]. For larger models, researchers have discovered circuits that perform clearly defined subtasks [22; 23; 27; 43].

How confident can we be that the NNs implement the claimed circuits? The central piece of evidence for many circuit papers is _causal consistency_: if we intervene on the network's internal activations, does the circuit correctly predict changes in the output? There are several competing formalizations of consistency [10; 20; 43; 25] and many ways to ablate NNs, each yielding different results [12; 35; 46]. This problem is especially due for _automatic_ circuit discovery methods, which search for subgraphs with the highest consistency [21; 45] or faithfulness [12; 39] measurements1.

Footnote 1: Faithfulness is a weaker form of consistency: if we ablate every part of the NN that is not part of the circuit, does the NN still perform the task? [10; 43]

These results would be on much firmer ground if we had an agreed-upon protocol for thoroughly checking a hypothesized circuit. To declare a candidate protocol _valid_, we need to check whether, in practice, it correctly distinguishes _true_ circuits from false circuits. Unfortunately, we do not know thetrue circuits of the models we are interested in, so we cannot validate any protocol. Previous work has sidestepped this in two ways. One method is to rely on qualitative evidence [10; 33], perhaps provided by human-curated circuits [12; 39], which is expensive and possibly unreliable.

The second way to obtain neural networks with known circuits is to construct them. Tracr [28] is a tool for compiling RASP programs [44] into standard decoder-only transformers. By construction, it outputs a model that implements the specified algorithm, making it suitable for evaluating MI methods. Unfortunately, Tracr-generated transformers are quite different from those trained using gradient descent: most of their weights and activations are zero, none of their features are in superposition, and they use only a small portion of their activations for the task at hand. Figure 2 shows how different the weights of a Tracr-generated transformer are from those of a transformer trained with gradient descent. This poses a very concrete threat to the validity of any evaluation that uses Tracr-generated transformers as subjects: we cannot tune the inductive biases of circuit evaluation algorithms with such unrealistic neural networks.

### Contributions

In this work, we present InterpBench, a collection of \(86\) semi-synthetic yet realistic transformers with _known circuits_ for evaluating mechanistic interpretability techniques. We collected \(85\) Tracr circuits plus 1 circuit from the literature (Indirect Object Identification [43]), and trained new transformers to implement these circuits using Strict Interchange Intervention Training (SIIT).

SIIT is an extension of Interchange Intervention Training (IIT) [19]. Under IIT, we predefine which subcomponents of a _low-level_ computational graph (the transformer to train) map to nodes of a _high-level_ graph (the circuit). During training, we apply the same interchange interventions [10; 18] to both the low- and high-level models, and incentivize them to behave similarly with the loss.

Our extension, SIIT, improves upon IIT by also intervening on subcomponents of the low-level model that are not mapped to any high-level node. This prevents the low-level model from using them to compute the output, ensuring the high-level model correctly represents the circuit the NN implements.

We make InterpBench models and the SIIT code used to train them all publicly available.2 In summary, the contributions of this article are:

Footnote 2: Code: https://github.com/FlyingPumba/InterpBench (MIT license). Trained networks & labels: https://huggingface.co/cybershiptrooper/InterpBench (CC-BY license).

* We present InterpBench, a benchmark of \(86\) realistic semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques.
* We introduce Strict Interchange Intervention Training (SIIT), an extension of IIT which also trains nodes not in the high-level graph. Using systematic ablations, we validate that SIIT correctly generates transformers with known circuits, even when IIT does not.

Figure 1: SIIT transformers implement a known ground-truth circuit, but their weights and activations are similar to the ones in naturally trained transformers, letting us measure, in a realistic setting, how accurate circuit discovery methods are at finding the true circuit.

* We show that SIIT-generated transformers are realistic enough to evaluate MI techniques, by checking whether circuit discovery methods behave similarly on SIIT-generated and natural transformers.
* We demonstrate the benchmark's usefulness by evaluating five circuit discovery techniques: Automatic Circuit DisCovery (ACDC, 12), Subnetwork Probing (SP, 35) on nodes and edges, Edge Attribution Patching (EAP, 39), and EAP with integrated gradients (EAP-ig, 29). On InterpBench, the results conclusively favor ACDC over Node SP, showing that there is enough statistical evidence (_p-value_\(\approx 0.0004\)) to tell them apart, whereas the picture in Conmy et al. [12] was much less clear. Interestingly, the results also show that EAP with integrated gradients is a strong contender against ACDC. In contrast, regular EAP performs poorly, which is understandable given the issues that have been raised about it [26].

This article's evaluation was performed on \(16\) Tracr circuits generated by us (Section 4). Since then, InterpBench has been expanded with \(69\) new models: \(10\) trained on more Tracr circuits generated by us and \(59\) trained on TracrBench circuits [41] (Appendix H).

## 2 Related work

Linearly compressed Tracr models.Lindner et al. [28] compress the residual stream of their Tracr-generated transformers using a linear autoencoder, to make them more realistic. However, this approach does not change the model's structure, and components that are completely zero remain in the final model.

Features in MI.While this work focuses on circuits, the current MI paradigm also studies _features_: hypothesized natural variables that the NN algorithm operates on. The most popular hypothesis is that features are most of the time inactive, and many features are in _superposition_ in a smaller linear subspace [15; 36]. This inspired sparse autoencoders (SAEs) as the most popular feature extraction method [5; 6; 13; 34; 40]. SAEs produce many human-interpretable features that are mostly able to reconstruct the residual stream, but this does not imply that they are natural features for the NN. Indeed, some features seem to be circular and do not fit in the superposition paradigm [16]. Nevertheless, circuits on SAE features can be faithful and causally relevant [29].

A benchmark that pairs NNs with their known circuits is also a good way to test feature discovery algorithms (like SAEs): the algorithms should naturally recover the values of computational nodes of the true circuit. Conversely, examining how SIIT-trained models represent their circuits' concepts could help us understand how natural NNs represent features. This article omits the comparison because its models only perform one task, and thus have too few features to show superposition.

Other MI benchmarks.Ravel[24] is a dataset of prompts containing named entities with different attributes that can be independently varied. Its purpose is to evaluate methods which can causally isolate the representations of these attributes in the NN. Orion[42] is a collection of retrieval tasks to investigate how large language models (LLMs) follow instructions. CasualGym[3] is a benchmark of linguistic tasks for evaluating interpretability methods on their ability to find

Figure 2: A histogram of the weights for the MLP output matrix in Layer 0 of a Tracr, SIIT, and “natural” transformer, i.e. trained by gradient descent to do supervised learning. All these transformers implement the frac_prevs task [28]. The weight distribution of an SIIT-trained transformer is much closer to the natural than the Tracr transformer. Yet, we know the ground-truth algorithm that the SIIT transformer implements. We provide the KL divergence between these histograms in Table 5.

specific linear features in LLMs. Find[37] is a dataset and evaluation protocol for tools which automatically describe model neurons or other components [4, 38]. The test subject must accurately describe a function, based on interactively querying input-output pairs from it.

We see InterpBench as complementary to Orion, Ravel, and CausalGym, and slightly overlapping with Find. InterpBench is very general in scope: its purpose is to evaluate _any_ interpretability methods which discover or evaluate circuits or features. However, InterpBench is not suitable for evaluating natural language descriptions of functions like Find is, and its NNs are about as simple as Find functions.

## 3 Strict Interchange Intervention Training

An interchange intervention [17, 18], or resample ablation [25], returns the output of the model on a _base_ input when some of its internal activations have been replaced with activations that correspond to a _source_ input. Formally, an interchange intervention \(\textsc{IntInv}(\mathcal{M},\textit{base},\textit{source},V)\) takes a model \(\mathcal{M}\), an input _base_, an input _source_, and a variable \(V\) (i.e., a node in the computational graph of the model), and returns the output of the model \(\mathcal{M}\) for the input _base_, except that the activations of \(V\) are set to the value they would have if the input were _source_. This same definition can be extended to intervene on a set of variables \(\mathbf{V}\), where the activations of all variables in \(\mathbf{V}\) are replaced. Geiger et al. [19] define Interchange Intervention loss as:

\[\sum_{\mathrm{b},\mathrm{s}\in\mathrm{dataset}}\textsc{Loss}\big{(}\textsc{ IntInv}(\mathcal{M}^{H},\mathrm{b},\mathrm{s},V^{H}),\textsc{IntInv}(\mathcal{M}^{L}, \mathrm{b},\mathrm{s},\Pi(V^{H}))\big{)}\] (1)

where \(\mathcal{M}^{H}\) is the high-level model, \(\mathcal{M}^{L}\) is the low-level model, \(V^{H}\) is a high-level variable, \(\Pi(V^{H})\) is the set of low-level variables that are aligned with (mapped to) \(V^{H}\), and Loss is some loss function, such as cross-entropy or mean squared error. We use the notation \(\mathcal{M}(\textit{base})\) to denote the output of the model \(\mathcal{M}\) when run without interventions on input _base_.

The main shortcoming of the above definition is that, by sampling only high-level variables \(V^{H}\) and intervening on the low-level variables that are aligned with it (i.e., \(\Pi(V^{H})\)), IIT never intervenes on low-level nodes that are not aligned with any node in the high-level model. This can lead to scenarios in which the nodes that are not intervened during training end up performing non-trivial computations that affect the low-level model's output, even when the nodes that are aligned with the high-level model are correctly implemented and causally consistent.

As an example, suppose that we have a high-level model \(\mathcal{M}^{\mathcal{H}}\) such that \(\mathcal{M}^{\mathcal{H}}(x)=3x+2\), and we want to train a low-level model \(\mathcal{M}^{\mathcal{L}}\) that has three nodes, only one of which is part of the circuit. If we train this low-level model using IIT, we may end up with a scenario like the one depicted in Figure 3. In this example, even though the low-level model has perfect accuracy and the aligned

Figure 4: Circuit for Indirect Object Identification task in InterpBench. This circuit is a simplified version of the one manually discovered by Wang et al. [43]. The _Duplicate token head_ outputs the first position of duplicated tokens, if there is any; otherwise it outputs \(-1\). The _S-Inhibition head_ copies the token from the previous position and outputs it to the _Name mover head_, which increases the logits of all names except the ones that are inhibited.

Figure 3: Example of a low-level model that has a perfect accuracy, with aligned low-level nodes (in yellow) that are causally consistent with the high-level model, but has non-aligned nodes (in grey) that affect the output.

nodes are causally consistent, the non-aligned nodes still affect the output in a non-trivial way. This shows some of the issues that arise when using IIT: aligned low-level nodes may not completely contain the expected high-level computation, and non-aligned low-level nodes may contain part of the high-level computation.

To correct this shortcoming, we propose an extension to IIT called _Strict Interchange Intervention Training_ (SIIT). Its pseudocode is shown in Algorithm 1 (Appendix A). The main difference between IIT and SIIT is that, in SIIT, we also sample low-level variables that are not aligned with any high-level variable. This allows us to penalize the low-level model for modifying the output when intervening on these non-aligned variables. We implement this modification as a new loss function (_Strictness loss_) that is included in the training loop of SIIT. Formally:

\[\sum_{\text{b,s}\in\text{dataset}}\text{Loss}\big{(}y_{b},\text{IntInv}( \mathcal{M}^{L},\text{b,s},V^{L})\big{)}\] (2)

where \(y_{b}\) is the correct output for input \(b\) and \(V^{L}\) is a low-level variable that is not aligned with any high-level variable \(V^{H}\). In other words, this loss incentivizes the low-level model to avoid performing non-trivial computations for this task on low-level components that are not aligned with any high-level variable. This makes the non-aligned components constant for the inputs in the task distribution, but not necessarily for the ones outside of it. Notice however that under the _Srictness loss_ the non-aligned components can still contribute to the output in a constant way, as long as they do not change the output when intervened on. The extent of this effect is analyzed in Appendix B.

As proposed by Geiger et al. [19], we also include in Algorithm 1 a behavior loss that ensures the model is not overfitting to the IIT and _Strictness_ losses. The behavior loss is calculated by running the low-level model without any intervention and comparing the output to the correct output.

## 4 InterpBench

InterpBench is composed of \(85\) semi-synthetic transformers generated by applying SIIT to Tracr-generated transformers and their corresponding circuits, plus a semi-synthetic transformer trained on GPT-2 and a simplified version of its IOI circuit [43]. This benchmark can be freely accessed and downloaded from HuggingFace (see Appendix E). We generated \(26\) RASP programs using few-shot prompts on GPT-4, and collected \(59\) RASP programs from TracrBench [41].

The architecture for the SIIT-generated transformers was made more realistic (compared to the original Tracr ones) by increasing the number of attention heads up to 4 (usually only 1 or 2 in Tracr-generated transformers), which lets us define some heads as not part of the circuit, and by halving the internal dimension of attention heads. The residual stream size on the new transformers is calculated as \(d_{\text{head}}\times n_{\text{heads}}\), and the MLP size is calculated as \(d_{\text{model}}\times 4\).

Using IIT's terminology, the Tracr-generated transformers are the high-level models, the SIIT-generated transformers are the low-level ones, and the variables are attention heads and MLPs (i.e., nodes in the computational graph). Each layer in the high-level model is mapped to the same layer in the low-level model. High-level attention heads are mapped to randomly selected low-level attention heads in the same layer. High-level MLPs are mapped to low-level MLPs in the same layer.

We train InterpBench's main \(16\) SIIT models by using Algorithm 1 as described in Section 3, fixing the Weight\({}_{SIIT}\) to values between \(0.4\) and \(10\), depending on the task. Both the Weight\({}_{IIT}\) and Weight\({}_{behavior}\) are set to 1. We use Adam as the optimizer for all models, with a fixed learning rate of \(0.001\), batch size of \(512\), and Beta coefficients of \((0.9,0.999)\). All models are trained until they reach \(100\%\) Interchange Intervention Accuracy (IIA) and \(100\%\)_Strict_ Interchange Intervention Accuracy (SIIA) on the validation dataset. IIA, as defined by Geiger et al. [21], measures the percentage of times that the low-level model has the same output as the high-level model when both are intervened on the same aligned variables. The _Strict_ version of this metric measures the percentage of times that the low-level model's output remains unchanged when intervened on non-aligned variables.

The training dataset is composed of 20k-120k randomly sampled inputs, depending on each task. The validation dataset is randomly sampled to achieve 20% of the training dataset size. The expected output is generated by running the Tracr-generated transformer on each input sequence. The specific loss function to compare the outputs depends on the task: cross-entropy for Tracr categorical tasks, and mean squared error for Tracr regression tasks.

To show that SIIT can also train transformers with non-RASP circuits coded manually, InterpBench includes a model trained on a simplified version of the IOI task and the circuit hypothesized by Wang et al. [43], shown in Figure 4. We train a semi-synthetic transformer with \(6\) layers and \(4\) heads per layer, \(d_{\text{model}}=64\), and \(d_{\text{head}}=16\). Each high-level node in the simplified IOI circuit is mapped to an entire layer in the low-level model. We train this transformer using the same algorithm and hyperparameters as for the Tracr-generated transformers, but with a different loss function. We apply the IIT and SIIT losses to the last token of the output sequence, and the cross-entropy loss to all other tokens. The final loss is a weighted average of these losses, with the IIT and SIIT losses upweighted by a factor of \(10\). The hyperparameters remained the same during the experiments.

The semi-synthetic transformers included in InterpBench were trained on a single NVIDIA RTX A6000 GPU. The training time varied depending on the task and the complexity of the circuit but was usually around 1 to 8 hours.

Appendix E explains how to download InterpBench and the license under which it is released. Appendix G contains a detailed description of the Tracr tasks included in the benchmark, and Appendix F provides instructions on how to use it. Appendix H provides the training details and task description for the models that were not included in this article's evaluation. Further documentation of each task (e.g., training hyperparameters) can be found in the structured metadata file on the HuggingFace repository3, and their source code is publicly available on the GitHub repository4.

Footnote 3: https://huggingface.co/cybershiptrooper/InterpBench/blob/main/benchmark_metadata.json

Footnote 4: https://github.com/FlyingPumba/InterpBench/tree/main/circuits_benchmark/benchmark/cases

## 5 Evaluation

To investigate the effectiveness of SIIT and the usefulness of the proposed benchmark, we conducted a evaluation on the 16 main models and IOI to answer the following research questions (RQs):

_RQ1 (IIT): Do the transformers trained using IIT correctly implement the desired circuits?_

_RQ2 (SIIT): Do the transformers trained using SIIT correctly implement the desired circuits?_

_RQ3 (Realism): Are the transformers trained using SIIT realistic?_

_RQ4 (Benchmark): Are the transformers trained using SIIT useful for benchmarking mechanistic interpretability techniques?_

### Results

RQ1 & RQ2.In this evaluation, we compare the semi-synthetic transformers trained using IIT and SIIT. Unless specified, the SIIT models are the \(16\) main ones from InterpBench (Section 4). We use the same setup for IIT models, except that we set the Weight\({}_{SIIT}\) to \(0\).

Figure 5: Average effect on accuracy for nodes in the circuit (green) and out of the circuit (red) for the models of \(7\) randomly sampled tasks in the benchmark. Boxplots display, for each task and model, the average proportion of model outputs that change when intervening on nodes. For all regression tasks, we deem an intervention to have an effect when the new scalar output differs by \(0.05\) or more from the original. We can see that for Tracr and SIIT models, nodes not in the circuit have much lower effects, but that is not the case for IIT models.

To understand if a trained low-level model correctly implements a circuit we need to check that (1) the low-level model has the same output as the high-level model when intervening on aligned variables, and that (2) the non-circuit nodes do not affect the output. As we mentioned in Section 4, all low-level models in our experiments are trained to achieve 100% IIA on the validation sets, which ensures that the first condition is always met.

We answer the second condition by measuring the _node effect_ and _normalised KL divergence_ after intervening on each node in the model. Node effect measures the percentage of times that the low-level model changes its output when intervened on a specific node. As mentioned before, a node that is not part of the circuit should not affect the output of the model and thus should have a low node effect. Formally, for a node \(V\) in a model \(\mathcal{M}\), and a pair of inputs \((x_{b},x_{s})\) with corresponding labels (\(y_{b}\), \(y_{s}\)), we define the node effect as follows:

\[\text{effect}_{V}(x_{b},x_{s},y_{b})=\mathds{1}\left[\text{IntInv}(\mathcal{M },x_{b},x_{s},V)\neq y_{b}\right],\]

where \(\mathds{1}[\cdot]\) is the indicator function. The normalized KL divergence is:

\[d_{V}(x_{b},x_{s},y_{b})= \frac{d_{KL}(\text{IntInv}(\mathcal{M},x_{b},x_{s},V),y_{b})-d_{ KL}(\mathcal{M}(x_{b}),y_{b})}{d_{KL}(\mathcal{M}(x_{s}),y_{b})-d_{KL}( \mathcal{M}(x_{b}),y_{b})}.\]

Figure 8: Correlation coefficients between the accuracy achieved by the SIIT and “natural” models, and the Tracr and “natural” models, for \(11\) randomly selected cases, after mean ablating the nodes rejected by \(\Lambda\)CDC over different thresholds (see Appendix B). These coefficients are consistently higher when comparing the SIIT and “natural” models than when comparing the Tracr and “natural” models.

Figure 6: Normalized effect on KL divergence for nodes in the circuit (green) and out of the circuit (red) for the models of \(5\) randomly sampled categorical tasks in the benchmark. Boxplots display, for each task and model, the differences in KL divergence before and after intervening on each node. We can see that in Tracr and SIIT nodes are very well separated into in/out of the circuit by their effect size, whereas that is not the case for IIT models.

Figure 7: Scatter plot comparing the effect for nodes in the circuit (green) and not in the circuit (red) for IIT and SIIT transformers on the \(16\) main tasks. The \(x\) and \(y\) axes display the average node effect when resample ablating on IIT and SIIT models, respectively. For each task, both models have a one-one node correspondence. Some IIT nodes that are not in the circuit have much higher effects than they should have.

If a semi-synthetic transformer correctly implements a Tracr's circuit, the effect of all aligned nodes will be similar to their corresponding counterparts in the Tracr model. For the KL divergence, it is not always possible to have a perfect match with the Tracr-generated transformer, as Tracr does not minimize the cross-entropy loss in categorical programs but only fixes the weights so that they output the expected labels. Still, we expect a clear separation between nodes in and out of the circuit.

Figure 5 shows the node effect for nodes in and out of the circuit for \(7\) randomly sampled tasks in the benchmark, averaged over a test dataset. Each boxplot shows the analysis for a Tracr, IIT, or SIIT transformer on a different task. We can see that the boxplots for IIT and Tracr are different, with the IIT ones consistently having a high node effect for nodes that are not in the circuit (red boxplots). On the other hand, the SIIT boxplots are more similar to the Tracr ones, with a low node effect for nodes that are not in the circuit, and a high node effect for nodes that are in the circuit.

Similarly, Figure 6 shows the average normalized KL divergence for nodes in and out of the circuit for \(5\) randomly sampled categorical tasks in the benchmark. Again, most of the boxplots for IIT have high KL divergence for nodes that are not in the circuit, while the SIIT boxplots have low values for these nodes. We can see that even though the SIIT transformer does not exactly match the Tracr behavior, there is still a clear separation between nodes in the circuit and those not in the circuit, which does not happen for the IIT transformers. It is worth pointing out that the higher error bar across cases for KL divergence is due to the fact that we are optimizing over accuracy instead of matching the expected distribution over labels.

Finally, Figure 7 shows a scatter plot comparing the average node effect for nodes in and out of the circuit for IIT and SIIT transformers for the \(16\) main tasks in the benchmark. We can see that there are several nodes not in the circuit that have a higher node effect for IIT than for SIIT.

Appendix B extends Figure 5 to the main \(16\) tasks in InterpBench, for SIIT and the original circuit only. It also repeats the experiments but with mean and zero ablations [46]. Using another type of ablation is a robustness check for InterpBench, which was trained with interchange interventions. Under mean ablations, only nodes in the circuit have an effect, but that is not the case under zero ablations. This may indicate that InterpBench circuits are not entirely true, but also matches the widely held notion that zero ablation is unreliable [46].

Rq3.To analyze the realism of the trained models, we run ACDC [12] on Tracr, SIIT, and "naturally" trained transformers (i.e., using supervised learning). We measure the accuracy of these models after mean-ablating [46] all the nodes rejected by ACDC, i.e. the ones that ACDC deems to not

Figure 9: (a) AUROCs of circuit discovery techniques on InterpBench’s \(16\) main models. ACDC’s AUROC is obtained by varying the threshold. SP and edgewise SP’s AUROC is obtained by varying the regularization coefficient (\(3000\) epochs). EAP with integrated gradients uses \(10\) samples. (b) Difference in Edge AUC ROC for all circuit discovery techniques against ACDC.

be in the circuit. This lets us check whether SIIT and "natural" models behave similarly from the point of view of circuit discovery techniques. A more realistic model should have a score similar to the transformers trained with supervised learning. Figure 8 displays the difference in correlation coefficients when comparing the accuracy of the SIIT and Tracr models to the "natural" models, showing that SIIT models have a higher correlation with "natural" models than Tracr ones. Figure 18 (Appendix D) suggests that circuits in SIIT models are harder to find than those in Tracr models.

Another proxy for realism is: do the weights of "natural" and SIIT models follow similar distributions? Figure 2 shows a histogram of the weights for the MLP output matrix in Layer 0 of a Tracr, SIIT, and "natural" transformer. The SIIT and "natural" weight distributions are very similar.

**RQ 3**: SIIT-generated transformers are more realistic than Tracr ones, with behavior similar to the transformers trained using supervised learning.

Rq4.To showcase the usefulness of the benchmark, we run ACDC [12], Subnetwork Probing (SP) [35], edgewise SP, Edge Attribution Patching (EAP) [39], and EAP with integrated gradients [29] on the SIIT transformers and compare their performance. Edgewise SP is similar to regular SP, but instead of applying masks over all available nodes, they are applied over all available edges. We compute the Area Under the Curve (AUC) for the edge-level ROC as a measure of their performance.

Figure 8(a) displays boxplots of the AUC ROCs, and Figure 8(b) shows the difference in AUC ROC for all circuit discovery techniques against ACDC. For measuring statistical significance, we rely on the well-established Wilcoxon-Mann-Whitney U-test and Vargha-Delaney \(A_{12}\) effect size [2]. From these tests, we get that ACDC is statistically different (_p-value_\(<0.05\)) to all the other algorithms except EAP with integrated gradients, with an effect size \(A_{12}\) ranging from \(0.54\) to \(0.91\).

Interestingly, previous evaluations of performance between SP and ACDC on a small number of tasks, including Tracr ones, did not show a significant difference between the two - SP was about as good as ACDC, achieving very similar ROC AUC across tasks when evaluated on manually discovered circuits [12]. On the other hand, results on InterpBench clearly show that ACDC outperforms SP on small models that perform algorithmic tasks (_p-value_\(\approx 0.0004\) and large effect size \(\hat{A}_{12}\approx 0.742\)).

One difference between ACDC and other techniques is that this method uses causal interventions to find out which edges are part of the circuit, while SP and EAP rely on the gradients of the model. After manual inspection, we found that the gradients of the SIIT models were very small, possibly due to these models being trained up to 100% IIA and 100% SIIA, which could explain why SP and regular EAP are not as effective as ACDC. This, however, does not seem to negatively affect EAP with integrated gradients, since the results show that this method is not statistically different from ACDC (_p-value_\(\geq 0.05\)), which means that it is as good as ACDC for the tasks in the benchmark.

There are some cases where ACDC is not the best technique (Figure 8(b)). Notably, in Case 33, ACDC is outperformed by all the other techniques except EAP. We leave investigating why to future work.

Finally, there is not enough statistical evidence to say EAP with integrated gradients is different than edgewise SP (_p-value_\(\geq 0.05\)), which means that the latter is a close third to ACDC and EAP with integrated gradients. Appendix D contains further details on the statistical tests and the evaluation of the circuit discovery techniques.

**RQ 4**: InterpBench can be used to evaluate mechanistic interpretability techniques, and has yielded unexpected results: ACDC is significantly better than SP and egewise SP, but statistically indistinguishable from EAP with integrated gradients.

## 6 Conclusion

In this work, we presented InterpBench, a collection of \(86\) semi-synthetic transformers with known circuits for evaluating mechanistic interpretability techniques. We introduced Strict Interchange Intervention Training (SIIT), an extension of IIT, and checked whether it correctly generates transformers with known circuits. This evaluation showed that SIIT is able to generate semi-synthetic transformersthat correctly implement Tracr-generated circuits, whereas IIT fails to do so. Further, we measured the realism of the SIIT transformers and found that they are comparable to "natural" ones trained with supervised learning. Finally, we showed that the benchmark can be used to evaluate existing mechanistic interpretability techniques, showing that ACDC [12] is substantially better at identifying true circuits than node- and edge-based Subnetwork Probing [35], but statistically indistinguishable from Edge Attribution Patching with integrated gradients [29].

It is worth mentioning that previous evaluations of MI techniques [12] relied mostly on manually found circuits such as IOI [43] for which there is no ground truth. In other words, these circuits are not completely faithful, and thus they are not guaranteed to be the real circuits implemented. In contrast, InterpBench provides models with ground truth, which allows us to compare the results of different MI techniques in a more controlled way.

Limitations.InterpBench has proven useful for evaluating circuit discovery methods, but its models, while realistic for their size, are very small and have very little functionality - only one algorithmic circuit per model, as opposed to the many subtasks in next-token prediction. Therefore, results on InterpBench may not accurately represent the results of the larger models that the MI community is interested in. As an example, we have not evaluated sparse autoencoders, as the small true number of features and size of the SIIT models would make it difficult to extract meaningful conclusions. Still, InterpBench serves as a worst-case analysis for MI techniques: if they can not retrieve accurate circuits here, they will not give faithful results in SOTA language models.

Future work.There are many ways to improve on this benchmark. One is to train SIIT transformers at higher granularities, like subspaces instead of heads, which would allow us to evaluate circuit and feature discovery techniques such as DAS [21] and Sparse Autoencoders [13]. One could also make the benchmark models more realistic by making each model implement many circuits. This would also let us greatly increase the number of models without manually implementing more tasks.

Societal impacts.If successful, this line of work will accelerate progress in mechanistic interpretability, by putting its results in firmer ground. Better MI makes AIs more predictable and controllable, which makes it easier to use (and misuse) AI. However, it also introduces the possibility of eliminating _unintended_ biases and bugs in NNs, so we believe the impact is overall good.

## Acknowledgments and Disclosure of Funding

RG, IA, and TK were funded by _AI Safety Support Ltd_ and _Long-Term Future Fund_ (LTFF) research grants. This work was produced as part of the _ML Alignment & Theory Scholars_ (MATS) Program - Winter 2023-24 Cohort, with mentorship from Adria Garriga-Alonso. Compute was generously provided by FAR AI. We thank Niels uit de Bos for his help setting up the Subnetwork Probing algorithm, and Oam Patel for his help in generating the RASP programs used in the benchmark. We also thank Matt Wearden for providing feedback on our manuscript, Juan David Gil for discussions during the research process, and ChengCheng Tan for excellent copyediting.

## Author contributions

RG implemented the SIIT algorithm, performed the experiments for the evaluation, and set up the IOI task. IA performed the statistical tests, set up the Tracr tasks, and wrote the initial draft of the manuscript. Both RG and IA helped setting up the circuit discovery techniques. TK provided the initial implementation of IIT. AGA proposed the initial idea for the project, provided feedback and advice throughout the project, and did the final editing of the manuscript.

## References

* [1] Evan Anders and Adria Garriga-Alonso. Crafting polysemantic transformer benchmarks with known circuits. https://www.lesswrong.com/posts/jeoSJoQLuK4JWqtyy/crafting-polysemantic-transformer-benchmarks-with-known, 2024.

* Arcuri and Briand [2014] Andrea Arcuri and Lionel C. Briand. A hitchhiker's guide to statistical tests for assessing randomized algorithms in software engineering. _Softw. Test., Verif. Reliab._, 24(3):219-250, 2014.
* Arora et al. [2024] Aryaman Arora, Dan Jurafsky, and Christopher Potts. Causalgym: Benchmarking causal interpretability methods on linguistic tasks. _CoRR_, abs/2402.12560, 2024.
* Bills et al. [2023] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html, 2023.
* Braun et al. [2024] Dan Braun, Jordan Taylor, Nicholas Goldowsky-Dill, and Lee Sharkey. Identifying functionally important features with end-to-end sparse dictionary learning. _CoRR_, 2024. URL http://arxiv.org/abs/2405.12241v2.
* Bricken et al. [2023] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. Towards monosemanticity: Decomposing language models with dictionary learning. _Transformer Circuits Thread_, 2023. URL https://transformer-circuits.pub/2023/monosemantic-features/index.html.
* Brinkmann et al. [2024] Jannik Brinkmann, Abhay Sheshadri, Victor Levoso, Paul Swoboda, and Christian Bartelt. A mechanistic analysis of a transformer trained on a symbolic multi-step reasoning task. _CoRR_, 2024. URL http://arxiv.org/abs/2402.11917v2.
* Bushnaq et al. [2024] Lucius Bushnaq, Stefan Heimersheim Nicholas Goldowsky-Dill, Dan Braun, Jake Mendel, Kaarel Hanni, Avery Griffin, Jorn Stohler, Magdalena Wache, and Marius Hobbhahn. The local interaction basis: Identifying computationally-relevant and sparsely interacting features in neural networks. _CoRR_, 2024.
* Cammarata et al. [2021] Nick Cammarata, Gabriel Goh, Shan Carter, Chelsea Voss, Ludwig Schubert, and Chris Olah. Curve circuits. _Distill_, 2021. doi: 10.23915/distill.00024.006. https://distill.pub/2020/circuits/curve-circuits.
* Chan et al. [2022] Lawrence Chan, Adria Garriga-Alonso, Nicholas Goldowsky-Dill, Ryan Greenblatt, Jenny Nitishinskaya, Ansh Radhakrishnan, Buck Shlegeris, and Nate Thomas. Causal scrubbing: a method for rigorously testing interpretability hypotheses. https://www.alignmentorum.org/posts/JvZhhzyCHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing, 2022.
* Chughtai et al. [2023] Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. _CoRR_, 2023.
* Conny et al. [2023] Arthur Conny, Augustine N. Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and Adria Garriga-Alonso. Towards automated circuit discovery for mechanistic interpretability. In _NeurIPS_, 2023.
* Cunningham et al. [2023] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. _CoRR_, 2023.
* Elhage et al. [2021] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kermion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021. https://transformer-circuits.pub/2021/framework/index.html.
* Elhage et al. [2021] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toymodels of superposition. _Transformer Circuits Thread_, 2022. URL https://transformer-circuits.pub/2022/toy_model/index.html.
* Engels et al. [2024] Joshua Engels, Isaac Liao, Eric J. Michaud, Wes Gurnee, and Max Tegmark. Not all language model features are linear. _CoRR_, 2024. URL http://arxiv.org/abs/2405.14860v1.
* Geiger et al. [2020] Atticus Geiger, Kyle Richardson, and Christopher Potts. Neural natural language inference models partially embed theories of lexical entailment and negation. In Afra Alishahi, Yonatan Belinkov, Grzegorz Chrupala, Dieuwke Hupkes, Yuval Pinter, and Hassan Sajjad, editors, _Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, pages 163-173, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.blackboxnlp-1.16. URL https://aclanthology.org/2020.blackboxnlp-1.16.
* Geiger et al. [2021] Atticus Geiger, Hanson Lu, Thomas Icard, and Christopher Potts. Causal abstractions of neural networks. In _NeurIPS_, pages 9574-9586, 2021.
* Geiger et al. [2022] Atticus Geiger, Zhengxuan Wu, Hanson Lu, Josh Rozner, Elisa Kreiss, Thomas Icard, Noah D. Goodman, and Christopher Potts. Inducing causal structure for interpretable neural networks. In _ICML_, volume 162 of _Proceedings of Machine Learning Research_, pages 7324-7338. PMLR, 2022.
* Geiger et al. [2023] Atticus Geiger, Chris Potts, and Thomas Icard. Causal Abstraction for Faithful Model Interpretation. _CoRR_, 2023.
* Geiger et al. [2023] Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. Finding alignments between interpretable causal variables and distributed neural representations. _CoRR_, 2023. URL http://arxiv.org/abs/2303.02536v4.
* Hanna et al. [2024] Michael Hanna, Ollie Liu, and Alexandre Variengien. How does gpt-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. _Advances in Neural Information Processing Systems_, 36, 2024.
* Heimersheim and Janiak [2021] Stefan Heimersheim and Jett Janiak. A circuit for Python docstrings in a 4-layer attention-only transformer. https://www.alignmentforum.org/posts/u6KXXmKFDkfWzoAxn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only.
* Huang et al. [2024] Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. RAVEL: Evaluating interpretability methods on disentangling language model representations. _CoRR_, 2024. URL http://arxiv.org/abs/2402.17700v1.
* Jenner et al. [2023] Erik Jenner, Adria Garriga-Alonso, and Egor Zverev. A comparison of causal scrubbing, causal abstractions, and related methods. https://www.lesswrong.com/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and, 2023.
* Kramar et al. [2024] Janos Kramar, Tom Lieberum, Rohin Shah, and Neel Nanda. Atp*: An efficient and scalable method for localizing LLM behaviour to components. _CoRR_, abs/2403.00745, 2024.
* Lieberum et al. [2023] Tom Lieberum, Matthew Rahtz, Janos Kramar, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla. _CoRR_, 2023. URL http://arxiv.org/abs/2307.09458v3.
* Lindner et al. [2023] David Lindner, Janos Kramar, Sebastian Farquhar, Matthew Rahtz, Tom McGrath, and Vladimir Mikulik. Tracr: Compiled transformers as a laboratory for interpretability. In _NeurIPS_, 2023.
* Marks et al. [2024] Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. _CoRR_, 2024. URL http://arxiv.org/abs/2403.19647v2.
* Nanda et al. [2023] Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures for grokking via mechanistic interpretability. In _International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=9XFSbDPmdW.

* Olah et al. [2020] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An overview of early vision in InceptionV1. _Distill_, 2020. doi: 10.23915/distill.00024.002. https://distill.pub/2020/circuits/early-vision.
* Olah et al. [2020] Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. Zoom in: An introduction to circuits. _Distill_, 2020. doi: 10.23915/distill.00024.001. https://distill.pub/2020/circuits/zoom-in.
* Olsson et al. [2022] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zae Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kermion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. _CoRR_, 2022. URL http://arxiv.org/abs/2209.11895v1.
* Rajamonaharan et al. [2024] Senthooran Rajamonaharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, Janos Kramar, Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders. _CoRR_, 2024. URL http://arxiv.org/abs/2404.16014v2.
* Sanh and Rush [2021] Victor Sanh and Alexander M. Rush. Low-complexity probing via finding subnetworks. In _NAACL-HLT_, pages 960-966. Association for Computational Linguistics, 2021.
* Scherlis et al. [2022] Adam Scherlis, Kshitij Sachan, Adam S. Jermyn, Joe Benton, and Buck Shlegeris. Polysemanticity and capacity in neural networks. _CoRR_, 2022. URL http://arxiv.org/abs/2210.01892v3.
* Schwettmann et al. [2023] Sarah Schwettmann, Tamar Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, and Antonio Torralba. FIND: A function description benchmark for evaluating interpretability methods. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 75688-75715. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ef0164c1112f56246224af540857348f-Paper-Datasets_and_Benchmarks.pdf.
* Shaham et al. [2024] Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, and Antonio Torralba. A multimodal automated interpretability agent. _CoRR_, 2024. URL http://arxiv.org/abs/2404.14394v1.
* Syed et al. [2023] Aaquib Syed, Can Rager, and Arthur Conmy. Attribution patching outperforms automated circuit discovery. _CoRR_, 2023.
* Templeton et al. [2024] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. _Transformer Circuits Thread_, 2024. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html.
* Thurnherr and Scheurer [2024] Hannes Thurnherr and Jeremy Scheurer. Tracrbench: Generating interpretability testbeds with large language models. 2024. URL https://arxiv.org/abs/2409.13714.
* Variengien and Winsor [2023] Alexandre Variengien and Eric Winsor. Look before you leap: a universal emergent decomposition of retrieval tasks in language models. _CoRR_, 2023. URL http://arxiv.org/abs/2312.10091v1.
* Wang et al. [2023] Kevin Ro Wang, Alexandre Variengien, Arthur Conmy, Buck Shlegeris, and Jacob Steinhardt. Interpretability in the wild: a circuit for indirect object identification in GPT-2 small. In _ICLR_. OpenReview.net, 2023.
* Weiss et al. [2021] Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In _ICML_, volume 139 of _Proceedings of Machine Learning Research_, pages 11080-11090. PMLR, 2021.

* Wu et al. [2023] Zhengxuan Wu, Atticus Geiger, Thomas Icard, Christopher Potts, and Noah D. Goodman. Interpretability at scale: Identifying causal mechanisms in alpaca. _CoRR_, 2023. URL http://arxiv.org/abs/2305.08809v3.
* Zhang and Nanda [2023] Fred Zhang and Neel Nanda. Towards best practices of activation patching in language models: Metrics and methods. _CoRR_, 2023. URL http://arxiv.org/abs/2309.16042v2.
* Zhong et al. [2023] Ziqian Zhong, Ziming Liu, Max Tegmark, and Jacob Andreas. The clock and the pizza: Two stories in mechanistic explanation of neural networks. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=SSWmbQc1We.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] The content described in abstract and introduction is outlined in Sections 3 to 5. 2. Did you describe the limitations of your work? [Yes] See Section 6 and the bottom of Section 2. 3. Did you discuss any potential negative societal impacts of your work? [Yes] See the Conclusion (Section 6). 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] The paper conforms to the ethics review guidelines.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Code, data, and instructions can be found on GitHub: https://github.com/FlyingPumba/InterpBench. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Section 4 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? An experiment showing the sensitivity of the SIIT algorithm to the seed and other hyperparameters is included in Appendix A. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See the end of Section 4
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [N/A] 2. Did you mention the license of the assets? [Yes] License for the code can be found at the GitHub repository (MIT license), and license for the models at HuggingFace repository (CC-BY license) 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] See the bottom of page 2. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]Strict Interchange Intervention Training details

We provide the pseudocode for the Strict Interchange Intervention Training (SIIT) in Algorithm 1, as described in Section 3. A slight variation of this algorithm was used to train \(59\) new models: \(10\) trained on more Tracr circuits generated by us and \(59\) trained on TracrBench circuits [41] (cf. Appendix H).

``` Input: High-level and low-level models \(\mathcal{M}^{H}\) and \(\mathcal{M}^{L}\) with variables \(\mathcal{V}^{H}\) and \(\mathcal{V}^{L}\), an alignment \(\Pi\) that maps a \(V^{H}\in\mathcal{V}^{H}\) to a \(\mathbf{V}^{L}\subset\mathcal{V}^{L}\), low-level model parameters \(\theta^{L}\), learning rate \(\ell\), training dataset \(\mathcal{D}\) while not converged or we have training budget do for\(\text{b},\text{s}\in\mathcal{D}\times\mathcal{D}\)do // Calculate IIT loss \(V^{H}\sim\mathcal{V}^{H}\)// Sample a high-level variable \(\mathbf{V}^{L}=\Pi(V^{H})\)// Aligned low-level variables with no grads: \(\text{o}^{H}=\textsc{IntInv}(\mathcal{M}^{H},\text{b},\text{s},V^{H})\) \(\text{o}^{L}=\textsc{IntInv}(\mathcal{M}^{L},\text{b},\text{s},\mathbf{V}^{L})\) \(\mathcal{L}_{IIT}=\textsc{Loss}(\text{o}^{H},\text{o}^{L})*\text{Weight}_{IIT}\) \(\theta^{L}\leftarrow\theta^{L}-\ell\nabla_{\theta^{L}}\mathcal{L}_{IIT}\) // Calculate Strictness loss \(V^{L}\sim\{V^{L}\notin\Pi(V^{H}),\;\forall\;V^{H}\in\mathcal{V}^{H}\}\) // Sample a non-aligned low-level variable \(\text{o}^{L}=\textsc{IntInv}(\mathcal{M}^{L},\text{b},\text{s},V^{L})\) \(\text{o}^{b}=\text{The correct output for input b}\) \(\mathcal{L}_{SIIT}=\textsc{Loss}(\text{o}^{b},\text{o}^{L})*\text{Weight}_{SIIT}\) \(\theta^{L}\leftarrow\theta^{L}-\ell\nabla_{\theta^{L}}\mathcal{L}_{SIIT}\) // Calculate Behavior loss \(\text{o}^{\emptyset}=\mathcal{M}^{L}(\text{b})\) \(\mathcal{L}_{behavior}=\textsc{Loss}(\text{o}^{b},\text{o}^{\emptyset})*\text{ Weight}_{behavior}\) \(\theta^{L}\leftarrow\theta^{L}-\ell\nabla_{\theta^{L}}\mathcal{L}_{behavior}\) endfor endwhile ```

**Algorithm 1** Pseudocode for Strict Interchange Intervention Training (SIIT).

Figure 10 displays the results of a sweep experiment analysing the sensitivity of the SIIT algorithm to the \(\text{Weight}_{SIIT}\) hyperparameter. This experiment is conducted with \(10\) epochs max on \(4\) randomly selected cases. We find that, on average, the best results are achieved when the \(\text{Weight}_{SIIT}\) is set to half the value of the other weights (\(\text{Weight}_{IIT}\) and \(\text{Weight}_{behavior}\)), as the accuracy, IIA, and SIIA metrics are the highest at this point. Values below this threshold lead to a decrease in the SIIA metric, while values above it lead to a decrease in the IIA metric. Overall, the sensitivity of the SIIT algorithm to the \(\text{Weight}_{SIIT}\) hyperparameter seems to be higher below \(0.5\), with a decrease on the tests metrics of up to \(20\%\), and lower above \(0.5\), with a decrease between \(0\%\) and \(10\%\).

Figure 11 complements the previous figure by showing the average test metrics achieved after \(20\) epochs for different values of \(\text{Weight}_{SIIT}\) in the SIIT algorithm, along with their variance, for \(7\) randomly selected cases. We see that depending on the case the variance can be very low or very high, independent of the test metric. This indicates that the sensitivity of the SIIT algorithm to the \(\text{Weight}_{SIIT}\) hyperparameter is case-dependent. We see a standard deviation of \(8.07\) on SIIA, \(9.37\) on IIA, and \(7.04\) on accuracy, on average across all cases. We note that not all the cases in this plot achieve \(100\%\) on the test metrics after \(20\) epochs.

Finally, Figure 12 shows the standard deviation of test metrics (SIIA, IIA, and accuracy) when varying the seed for different values of \(\text{Weight}_{SIIT}\) in the SIIT algorithm. We see a similar pattern to the previous figures, with the variance being usually dependent on the case: for some cases, the variance is very low, while for others it is very high, independent of the test metric.

Figure 11: Average test metrics (SIIA, IIA, and accuracy) achieved after \(20\) epochs for different values of Weight\({}_{SIIT}\) in the SIIT algorithm, for \(7\) randomly selected cases. The accuracies plotted are averaged over \(10\) different seeds. The standard deviations of the metrics are shown as error bars. Not all the cases in this plot achieve \(100\%\) on the test metrics. We can see that the variance is usually dependent on the case: for some cases, the variance is very low (\(\approx 0\)-\(1\%\)), while for others it is high (\(\approx 4\)-\(5\%\)), independent of the test metric.

Figure 10: Variation of different test metrics for a sweep of the Weight\({}_{SIIT}\) hyperparameter in the SIIT algorithm on \(4\) randomly selected cases. The cases in the plots achieve 100% on the test metrics, or are very close to that percentage, for chosen number of max epochs. Both the Weight\({}_{IIT}\) and Weight\({}_{behavior}\) hyperparameters were set to \(1\). In this setup, the best results are usually achieved when the Weight\({}_{SIIT}\) is set to \(0.5\) (i.e., the half of the other weights).

## Appendix B Thorough evaluation of dataset models

Tables 1 to 3 provide detailed versions of the data shown in Figure 5, for the main \(16\) SIIT models in the benchmark, using interchange interventions, mean ablations and zero ablations, respectively. The main takeaway from the interchange intervention and mean ablations is that nodes not in the circuit have zero or very close to zero effect, while nodes in the circuit have a much higher effect. On the other hand, zero ablations indicate that there are nodes not in the circuit with significant effects.

Table 4 shows the accuracy of the main \(16\) SIIT models after mean and zero ablating all the nodes that are not in the circuit. Some of the cases in this table present a big drop in accuracy, specially the regression tasks, while the classification tasks are more robust. This is expected since regression tasks are more sensitive with respect to the output logits, as we compare using an absolute tolerance (_atol_) and do not use the argmax function that is used in classification tasks. We also note that using either mean or zero ablations on many nodes at the same time can easily throw the model's activations off-distribution, which is a common issue also present in models found in the wild.

As a reference, we present in Figure 13 the variation of accuracy for case 3's SIIT model, as a function of the absolute tolerance (_atol_) value for comparing outputs. Most of the logits returned by the SIIT model are at a distance between \(0.1\) and \(0.5\) from the original outputs, which is why the accuracy is very low for _atol_ values below \(0.1\), but quickly jumps to \(28.9\%\) at \(0.1\), and then to \(84.1\%\) at \(0.25\).

Furthermore, we also studied the relationship between each node's average activation norm and the Pearson correlation coefficient between the outputs of logit lens applied to that node and the model's actual output. Although many nodes are correlated, most of the ones not in the circuit with a high zero ablation effect have very low variances and norms. For example Case 3 final layer attention hook \(3\), has an effect \(0.42\) and norm \(1.51\pm 0.55\). However, there are still some nodes worth noting, such as the one for final layer's MLP in Case 11, with effect \(0.11\) and normalised activation norm \(1.33\pm 0.55\). We leave further investigation of these nodes for future work, as its role is not very well understood at the moment. Interactive plots for this analysis can be found online 5.

Footnote 5: https://wandb.ai/cybershiptrooper/siit_node_stats/reports/Pearson-Correlation-Plots.-Vmlldzo4Njg1MDgy

We present more detailed information on realism in Figure 14, where we plot the accuracy of the SIIT (trained to \(100\%\) SIIA), Tracr and "natural" models for \(3\) randomly selected cases after mean ablating the nodes rejected by ACDC over different thresholds. These plots show that the SIIT models have a closer behavior to the "natural" models than the Tracr models, which is consistent with the results presented in Section 5. To normalise error from a larger number of edges, we train "natural" and SIIT models with the same architecture of its corresponding Tracr model. We use an identity alignment map to train SIIT models in this case. Figure 15 shows this same information in a more aggregated way, by plotting the average accuracy of the circuit across ACDC thresholds for Tracr, SIIT, and "naturally" trained transformers on the main \(16\) tasks.

Figure 12: Boxplots showing the standard deviation of test metrics (SIIA, IIA, and accuracy) when varying the seed for different values of Weight\({}_{SIIT}\) in the SIIT algorithm, for \(7\) randomly selected cases. Again, this variance is usually dependent on the case: for some cases, the variance is very low, while for others it is very high, independent of the test metric.

[MISSING_PAGE_FAIL:18]

## Appendix D Evaluation of circuit discovery techniques

In this work we compare the performance of the following circuit discovery techniques: Automated Circuit DisCovery (ACDC), Subnetwork Probing (SP), Edgewise SP, Edge Attribution Patching (EAP), and EAP using integrated gradients (EAP-IG). ACDC traverses the transformer's computational graph in reverse topological order, iteratively assigning scores to edges and pruning them if their score falls below a certain threshold. EAP assigns scores to all edges at the same time by leveraging gradient information, and again prunes edges below a certain threshold to form the final circuit. EAP-IG uses integrated gradients to smooth out the approximation of gradients and improve the performance of EAP. SP learns, via gradient descent, a mask for each node in the circuit to determine if it is part of the circuit or not, and encourages this mask to be sparse by adding a sparseness term to the loss function. The strength of this sparse penalty is controlled by a regularization hyperparameter. Edgewise SP is a variation of SP that learns a mask for each edge in the transformer model instead of each node.

We use different metrics for each task in the benchmark, depending on whether it is a regression or classification task. For ACDC, SP and Edgewise SP, we use the \(L_{2}\) distance for regression tasks and the Kullback-Leibler divergence for classification tasks. For EAP and EAP-IG, we use the Mean Absolute Error (MAE) for regression tasks and the cross-entropy loss for classification tasks.

Since each of these techniques can be configured to be more or less aggressive, i.e. to prune more or fewer nodes/edges, we compare their performance using the Area Under the Curve (AUC) of ROC curves. We compute the True Positive Rate (TPR) and False Positive Rate (FPR) for the ROC curves

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{Case} & \multirow{2}{*}{Weight\({}_{SIIT}\)} & \multicolumn{3}{c}{Nodes in circuit} & \multicolumn{3}{c}{Nodes not in circuit} \\ \cline{3-6}  & & \multicolumn{1}{c}{Quartiles} & \multicolumn{1}{c}{Range} & \multicolumn{1}{c}{Quartiles} & \multicolumn{1}{c}{Range} \\ \hline \hline
11 & 0.4 & 0.54 - 0.55 - 0.56 & 0.53 - 0.56 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
13 & 0.4 & 0.18 - 0.34 - 0.50 & 0.14 - 0.51 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
18 & 1.0 & 0.45 - 0.46 - 0.46 & 0.45 - 0.47 & 0.00 - 0.00 - 0.00 & 0.00 - 0.01 \\ \hline
19 & 0.4 & 0.27 - 0.31 - 0.35 & 0.24 - 0.39 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
20 & 0.4 & 0.22 - 0.22 - 0.22 & 0.22 - 0.22 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
21 & 0.5 & 0.13 - 0.14 - 0.19 & 0.11 - 0.31 & 0.00 - 0.00 - 0.00 & 0.00 - 0.04 \\ \hline
26 & 0.4 & 0.57 - 0.57 - 0.57 & 0.57 - 0.57 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
29 & 0.4 & 0.79 - 0.79 - 0.79 & 0.79 - 0.79 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
3 & 10.0 & 0.74 - 0.76 - 0.78 & 0.71 - 0.80 & 0.00 - 0.00 - 0.00 & 0.00 - 0.09 \\ \hline
33 & 0.4 & 0.56 - 0.56 - 0.56 & 0.56 - 0.56 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
34 & 1.0 & 0.45 - 0.45 - 0.45 & 0.45 - 0.45 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
35 & 1.0 & 0.79 - 0.79 - 0.79 & 0.79 - 0.79 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
36 & 1.0 & 0.31 - 0.31 - 0.31 & 0.31 - 0.31 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
37 & 1.0 & 0.76 - 0.76 - 0.76 & 0.76 - 0.76 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
4 & 0.4 & 0.61 - 0.67 - 0.74 & 0.61 - 0.76 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline
8 & 0.4 & 0.20 - 0.39 - 0.59 & 0.00 - 0.79 & 0.00 - 0.00 - 0.00 & 0.00 - 0.01 \\ \hline IOI & 0.4 & 0.59 - 0.79 - 0.94 & 0.38 - 0.99 & 0.00 - 0.00 - 0.00 & 0.00 - 0.00 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Detailed statistics for the effect on accuracy of nodes in the circuit and nodes out of the circuit, for the main \(16\) SIIT models in the benchmark, measured using _mean ablations_. The mean ablation technique differs from the interchange ablation in that it replaces the activations of the target node with the mean activations for that node in the dataset. In other words, it does not use a different input to replace the activations of the target node. Mean ablation is a robustness check for the SIIT models in InterpBench, which were trained with interchange ablations. We consider that the intervention has changed the output for regression models when the new output differs by \(0.05\) or more, and for classification models when the new output is simply different from the original output. We can see that nodes not in the circuit have zero or very close to zero effect, while nodes in the circuit have a much higher effect.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Case} & \multirow{2}{*}{Weight\({}_{SIIT}\)} & \multicolumn{3}{c}{Nodes in circuit} & \multicolumn{3}{c}{Nodes not in circuit} \\ \cline{3-7}  & & \multicolumn{1}{c}{Quantities} & \multicolumn{1}{c}{Range} & \multicolumn{1}{c}{Quartiles} & \multicolumn{1}{c}{Range} \\ \hline \hline
3 & 10.0 & 0.782 - 0.844 - 0.906 & 0.720 - 0.968 & 0.000 - 0.000 - 0.000 & 0.000 - 0.428 \\ \hline
4 & 0.4 & 0.874 - 0.934 - 0.977 & 0.821 - 0.978 & 0.169 - 0.750 - 0.960 & 0.000 - 1.000 \\
8 & 0.4 & 0.346 - 0.346 - 0.346 & 0.346 - 0.346 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\
11 & 0.4 & 0.781 - 0.783 - 0.786 & 0.779 - 0.788 & 0.000 - 0.000 - 0.000 & 0.000 - 0.113 \\ \hline
13 & 0.4 & 0.245 - 0.471 - 0.705 & 0.174 - 0.799 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\
18 & 1.0 & 0.091 - 0.256 - 0.440 & 0.043 - 0.545 & 0.000 - 0.000 - 0.071 & 0.000 - 0.112 \\
19 & 0.4 & 0.313 - 0.326 - 0.339 & 0.301 - 0.351 & 0.000 - 0.000 - 0.067 & 0.000 - 0.067 \\
20 & 0.4 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 & 0.000 - 0.000 & 0.000 - 0.100 \\
21 & 0.5 & 0.121 - 0.146 - 0.155 & 0.000 - 0.824 & 0.000 - 0.000 - 0.000 & 0.000 - 0.107 \\
26 & 0.4 & 0.152 - 0.152 - 0.152 & 0.152 - 0.152 & 0.000 - 0.000 - 0.000 & 0.000 - 0.013 \\
29 & 0.4 & 0.617 - 0.617 - 0.617 & 0.617 - 0.617 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\ \hline
33 & 0.4 & 0.300 - 0.300 - 0.300 & 0.300 - 0.300 & 0.000 - 0.000 & 0.000 - 0.000 \\
34 & 1.0 & 0.436 - 0.436 - 0.436 & 0.436 - 0.436 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\ \hline
35 & 1.0 & 0.493 - 0.493 - 0.493 & 0.493 - 0.493 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\ \hline
36 & 1.0 & 0.290 - 0.290 - 0.290 & 0.290 - 0.290 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\
37 & 1.0 & 0.541 - 0.541 - 0.541 - 0.541 & 0.541 - 0.541 & 0.000 - 0.000 - 0.000 & 0.000 - 0.000 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Detailed statistics for the effect on accuracy of nodes in the circuit and nodes out of the circuit, for the main \(16\) SIIT models in the benchmark, measured using _zero ablations_. The zero ablation technique differs from the interchange ablation in that it replaces the activations of the target node with zeros. Zero ablation is a robustness check for the SIIT models in InterpBench, which were trained with interchange ablations, although it is a more aggressive intervention and can potentially throw off the distribution of the model’s activations. We consider that the intervention has changed the output for regression models when the new output differs by \(0.05\) or more, and for classification models when the new output is simply different from the original output. Unlike mean and resample ablations, where we see little to no effects from nodes that are not in the circuit, significant effects can be seen when using zero ablations.

Figure 13: Variation of accuracy for case 3’s SIIT model, when mean ablating all the nodes that are not in the ground truth circuit, and varying the absolute tolerance (_atol_) for deciding if an output has changed. For _atol_ values below \(0.1\), the accuracy is very low, close to zero, but it quickly jumps to \(28.9\%\) at \(0.1\). There is a rotund change between \(0.1\) and \(0.25\), where the accuracy jumps to \(84.1\%\), and finally, at \(0.5\), the accuracy reaches \(98.9\%\). This means around \(29\%\) of the logits returned by the SIIT model are at a distance closer than \(0.1\) from the original outputs, \(85\%\) are at a distance closer than \(0.25\), and \(99\%\) are at a distance closer than \(0.5\).

by comparing the discovered circuits with the ground truth circuits, which we have by construction in InterPBench.

In order for this comparison to be sound we need to be more specific on the granularity at which we perform the evaluation. All of the techniques mentioned above work at the QKV granularity level, and thus they consider the outputs of the Q, K, and V matrices in attention heads and the output of MLP components as nodes in the computational graph. On the other hand, SIIT models are trained at the attention head level, without putting a constraint on the head subcomponents, which means

\begin{table}
\begin{tabular}{l l c c} \hline \hline Case & Task type & Mean ablation accuracy & Zero ablation accuracy \\ \hline
3 & Regression & 0.0 & 0.131 \\
4 & Regression & 0.525 & 0.248 \\
8 & Classification & 0.632 & 0.634 \\
11 & Classification & 0.967 & 0.887 \\
13 & Classification & 0.959 & 0.943 \\
18 & Classification & 0.949 & 0.913 \\
19 & Classification & 0.829 & 0.527 \\
20 & Classification & 1.0 & 0.995 \\
21 & Classification & 0.889 & 0.544 \\
26 & Classification & 0.641 & 0.641 \\
29 & Classification & 0.741 & 0.891 \\
33 & Classification & 0.913 & 0.9 \\
34 & Classification & 0.805 & 0.784 \\
35 & Classification & 0.915 & 0.989 \\
36 & Classification & 1.0 & 1.0 \\
37 & Classification & 0.837 & 0.548 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accuracy of the main \(16\) SIIT models after mean and zero ablating all the nodes that are not in the ground truth circuit. We consider that the ablation has changed the output for regression models when the new output differs by \(0.05\) or more, and for classification models when the new output is simply different from the original output. We can see that there is a big drop in accuracy for models performing regression tasks, while the models performing classification tasks are more robust. It is worth noting that using both mean and zero ablations on many nodes at the same time can be a very aggressive intervention and throw off the distribution of the model’s activations. We expect realistic models to face similar issues.

Figure 14: Accuracy of the SIIT, Tracr and “natural” models for \(3\) randomly selected cases after mean ablating the nodes rejected by ACDC over different thresholds. On the left, we have only SIIT and “natural” models, and on the right, we have only Tracr and “natural” models. The lines in this figure show that the SIIT models have a closer behavior to the “natural” models than the Tracr ones.

that the trained models can solve the required tasks via QK circuits, OV circuits, or a combination of both [14]. Thus, during the evaluation of the circuit discovery techniques, we promote the QKV nodes to heads on both the discovered circuits and the ground truth circuits. In other words, if for example the output of a Q matrix in an attention head is part of the circuit, we consider the whole attention head to be part of it as well.

Additionally, when calculating the edge ROC curves for SP, we consider an edge to be part of the circuit if both of its nodes are part of the circuit. This is a simplification, but it allows us to compare regular SP with the rest of the techniques, which work at the edge level.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Natural & Tracr & SIIT & IIT \\ \hline Natural & 1.00 & - & - & - \\ Tracr & 0.57 & 1.00 & - & - \\ SIIT & 0.80 & 0.64 & 1.00 & - \\ IIT & 0.86 & 0.56 & 0.79 & 1.00 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Correlation coefficients between the accuracy achieved by SIIT, IIT, Tracr, and “natural” models, averaged of \(5\) cases, after mean ablating the nodes rejected by ACDC over different thresholds. All the models have the same size and are trained with the identity correspondence, wherever necessary.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & Mean & Standard Deviation & \multicolumn{2}{c}{Quartiles} & Range \\ \hline in circuit & \(0.028\) & \(0.041\) & \(0.008\) - \(0.014\) - \(0.024\) & \(0.001\) - \(0.162\) \\ not in circuit & \(0.009\) & \(0.007\) & \(0.007\) - \(0.008\) - \(0.008\) & \(0.002\) - \(\mathbf{0.071}\) \\ \hline \hline \end{tabular}
\end{table}
Table 7: Statistics of the resample ablation scores for nodes in the IOI circuit and not in the IOI circuit of GPT-2 small (except Layer 0’s MLP). Compared to Table 1, the overall effect of nodes are much lower for this circuit. This indicates that the circuit may be more spread out/have more redundancies. However, the nodes not in circuit have effects much higher than both SIIT models and the nodes that are in this IOI circuit.

Figure 15: Average accuracy of circuit across ACDC thresholds, for Tracr, SIIT, and “naturally” trained transformers on the main \(16\) tasks. The scores in each boxplot show the accuracy of models after mean-ablating all the nodes that are not a part of ACDC’s hypothesis, averaged across multiple thresholds, for each task. SIIT and Natural scores are clearly the most similar.

Table 8 shows all the p-values for the Wilcoxon-Mann-Whitney U-test on each pair of circuit discovery techniques, for the comparison of the AUC of ROC curves. Table 9 shows the Vargha-Delaney \(\hat{A}_{12}\) effect size values for the same comparison.

Figure 16: Extended version of Figure 2, now including IIT. We can see that the plots are indistinguishable between SIIT, IIT, and Natural weight matrices.

Figure 17: Boxplot of the resample ablation scores for nodes in the IOI circuit and not in the IOI circuit of GPT-2 small (except Layer 0’s MLP). We can clearly see some nodes not in the circuit are causally responsible here.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & ACDC & Node SP & Edge SP & EAP & EAP-IG \\ \hline ACDC & - & \(\mathbf{0.000427}\) & \(\mathbf{0.028417}\) & \(\mathbf{0.000061}\) & \(0.099481\) \\ \hline Node SP & - & - & \(\mathbf{0.015503}\) & \(\mathbf{0.000153}\) & \(\mathbf{0.000979}\) \\ \hline Edge SP & - & - & - & \(\mathbf{0.000031}\) & \(0.307821\) \\ \hline EAP & - & - & - & - & \(\mathbf{0.000648}\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Wilcoxon-Mann-Whitney U-test p-values for the comparison of the AUC of ROC curves for the different circuit discovery techniques. We use \(\alpha=0.05\) as the significance level. The p-values below this level are marked in bold, which means that we can reject the null hypothesis that the two techniques being compared have the same distribution of AUC values. I.e., we can say that the AUC values are significantly different.

## Appendix E Benchmark and license details

The code repository for our benchmark can be found here: https://github.com/FlyingPumba/InterpBench, and it is licensed under the MIT license. The trained models can be found here: https://huggingface.co/cybershiptrooper/InterpBench, and they are licensed under CC-BY. The benchmark's code is hosted on GitHub and the trained models are hosted on HuggingFace. We will ensure that both are available for a long time. For that purpose, we have minted DOIs for both the code repository and the trained models. The DOI for the code repository is 10.5281/zenodo.11518575 and the DOI for the trained models is 10.57967/hf/2451.

The intended use of this benchmark is to evaluate the effectiveness of mechanistic interpretability techniques. The training and evaluation procedures can be found in our code repository and are described in Sections 4 and 5. The code repository also contains instructions on how to replicate the empirical results presented in this work. The benchmark we provide does not contain any offensive content. We, the authors, bear all responsibility to withdraw our paper and data in case of violation of licensing or privacy rights.

We provide several structured metadata files for our benchmark, all available in HuggingFace's repository:

* A Croissant metadata record.
* A CSV file listing the metadata for all cases in the benchmark.
* A Parquet file listing the metadata for all cases in the benchmark.
* A JSON file listing the metadata for all cases in the benchmark.

Figure 18: Node and edge AUROC achieved by ACDC on SIIT and Tracr models. ACDC achieved a higher or same node AUROC on Tracr models for almost all cases, and a higher or same edge AUROC on Tracr models for all but \(5\) cases.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & ACDC & Node SP & Edge SP & EAP & EAP-IG \\ \hline \hline ACDC & - & \(0.742\) & \(0.541\) & \(0.91\) & \(0.555\) \\ \hline Node SP & - & - & \(0.355\) & \(0.844\) & \(0.316\) \\ \hline Edge SP & - & - & - & \(0.887\) & \(0.486\) \\ \hline EAP & - & - & - & - & \(0.111\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Vargha-Delaney \(\hat{A}_{12}\) effect size values for the comparison of the AUC of ROC curves for the different circuit discovery techniques. The values are interpreted as follows: \(0.56<\hat{A}_{12}<0.64\) is considered small, \(0.64<\hat{A}_{12}<0.71\) is considered medium, and \(\hat{A}_{12}>0.71\) is considered large.

Benchmark usage

The trained models hosted on HuggingFace are organized in directories, each one corresponding to a case in the benchmark, containing the following files:

* ll_model.pth: A serialized PyTorch state dictionary for the trained transformer model.
* ll_model_cfg.pkl: Pickle file containing the architecture config for the trained transformer model.
* meta.json: JSON file with hyperparameters used for training for the model.
* edges.pkl: Pickle file containing labels for the circuit, i.e., list of all the edges that are a part of the ground truth circuit.

These models can be loaded using TransformerLens, a popular Python library for Mechanistic Interpretability on transformers:

import pickle

from transformer_lens import HookedTransformer

cfg_dict = pickle.load(f"ll_model_cfg.pkl")

cfg = HookedTransformerConfig.from_dict(cfg_dict)

model = HookedTransformer(cfg)

weights = torch.load(f"ll_model.pth")

model.load_state_dict(weights)

More details of usage are provided in the GitHub repository.

## Appendix G InterpBench Tracr tasks used for the evaluation

Table 10 displays the main \(16\) Tracr tasks included in InterpBench that were used for this article's evaluation (Section 5).

\begin{tabular}{l l l} \hline \hline Case & Type & Description & Code \\ \hline
3 & Reg & Returns the fraction of 'x' in the input up to the i-th position for all i. & Link \\ \hline
4 & Reg & Return fraction of previous open tokens minus the fraction of close tokens. & Link \\
8 & Cls & Identity. & Link \\ \hline
11 & Cls & Counts the number of words in a sequence based on their length. & Link \\
13 & Cls & Analyzes the trend (increasing, decreasing, constant) of numeric tokens. & Link \\ \hline
18 & Cls & Classify each token based on its frequency as 'rare', 'common', or 'frequent'. & Link \\
19 & Cls & Removes consecutive duplicate tokens from a sequence. & Link \\ \hline
20 & Cls & Detect spam messages based on appearance of spam keywords. & Link \\ \hline
21 & Cls & Extract unique tokens from a string. & Link \\
26 & Cls & Creates a cascading effect by repeating each token in sequence incrementally. & Link \\ \hline
29 & Cls & Creates abbreviations for each token in the sequence. & Link \\ \hline \hline \end{tabular}

## Appendix H InterpBench Tracr tasks not used for the evaluation

Table 11 displays the new \(69\) tasks included in InterpBench after this article's evaluation: \(10\) models trained on more Tracr circuits generated by us (as described in Section 4) and \(59\) models trained on TracrBench circuits [41].

For the training of these new models, we used a slight variation of the Algorithm 1, as suggested by Anders et al. [1]. In this variation, the three different losses are summed into a single one and used for gradient descent:

\[\mathcal{L}=\mathcal{L}_{IIT}+\mathcal{L}_{SIIT}+\mathcal{L}_{behavior}\] \[\theta^{L}\leftarrow\theta^{L}-\ell\nabla_{\theta^{L}}\mathcal{L}\]

This removes the need of updating the weights three times in the original algorithm and, more importantly, improves training stability by considering the minima for only one landscape instead of three. To further help the optimizer converge when using a single loss function we decreased the Beta coefficients to \((0.9,0.9)\).

Additionally, the calculation of _Srictness loss_ was improved: instead of sampling and intervening on only one non-aligned low-level variable \(V^{L}\), we now sample each non-aligned low-level variable with \(50\%\) probability, and intervene on all of them at the same time:

\[\textbf{V}^{L}\sim\{V^{L}\in\mathcal{V}^{L}\mid V^{L}\notin\Pi( V^{H}),\forall V^{H}\in\mathcal{V}^{H}\}\] \[I_{V^{L}}\sim\text{Bernoulli}(0.5)\quad\text{// Indicator to sample independently with probability 50%}\] \[o^{L}=\text{IntInv}(\mathcal{M}^{L},b,s,\{\textbf{V}^{L}\mid I_{V ^{L}}=1\})\] \[\text{o}^{b}=\text{The correct output for input b}\] \[\mathcal{L}_{SIIT}=\text{Loss}(\text{o}^{b},\text{o}^{L})*\text{ Weight}_{SIIT}\]

This discourages the model from learning _backup behavior_, where the non-aligned nodes that are not intervened on become active and help the model achieve a lower loss.

Finally, learning rate is now linearly decreased from \(10^{-3}\) to \(2\times 10^{-4}\) over the course of training. We have also experimented with other combinations of \(SIIT\), \(IIT\) and behavior weights, and longer epochs (up to \(3\),\(000\)).

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Case & TracrBench? & Type & Acc & IIA & SIIA & Description & Code \\ \hline
2 & No & Cls & 100 & 99.978 & 99.996 & Reverse the input sequence. & Link \\
7 & No & Cls & 100 & 99.919 & 99.623 & Returns the number of times & Link \\  & & & & & each token occurs in the input. & Link \\
14 & No & Cls & 100 & 100 & 99.942 & Returns the count of ’a’ in the input sequence. & Link \\ \hline \hline \end{tabular}
\end{table}
Table 10: A description of the main \(16\) Tracr tasks included in InterpBench. Task type is either “Cls” (classification) or “Reg” (regression).

* [15] No & Cls & 100 & 100 & 99.993 Returns each token multiplied by two and subtracted by its index.
* [19] No & Cls & 100 & 100.000 & 100.000 & Removes consecutive duplicate tokens from a sequence.
* [24] No & Cls & 100 & 100.000 & 99.915 Identifies the first occurrence of each token in a sequence.
* [25] No & Cls & 99.989 & 99.900 & 99.965 Normalizes token frequencies in a sequence to a range between 0 and 1.
* [30] No & Cls & 100 & 100 & 99.964 Tags numeric tokens in a sequence based on whether they fall within a given range.
* [31] No & Cls & 100 & 100 & 100 & Identify if tokens in the sequence are anagrams of the word 'listen'.
* [39] No & Reg & 100 & 100.000 & 99.977 Returns the fraction of 'x' in the input up to the i-th position for all i (longer sequence length).
* [40] Yes & Cls & 100 & 100.000 & 99.999 Sum the last and previous to last digits of a number
* [41] Yes & Cls & 100 & 99.997 & 99.931 Make each element of the input sequence absolute
* [43] Yes & Cls & 100 & 100 & 99.982 Returns the corresponding Fibonacci number for each element in the input sequence.
* [44] Yes & Cls & 99.989 & 99.976 & 99.939 Replaces each element with the number of elements greater than it in the sequence
* [45] Yes & Cls & 100 & 99.938 & 99.997 Doubles the first half of the sequence
* [46] Yes & Cls & 100 & 100 & 99.999 Decrements each element in the sequence by 1
* [49] Yes & Cls & 100 & 100 & 99.959 Decrements each element in the sequence until it becomes a multiple of 3.
* [50] Yes & Cls & 100 & 100 & 99.999 Applies the hyperbolic cosine to each element
* [51] Yes & Cls & 100 & 100 & 99.997 Checks if each element is a Fibonacci number
* [52] Yes & Cls & 100 & 100 & 99.999 Takes the square root of each element.
* [53] Yes & Cls & 100 & 99.943 & 99.978 Increment elements at odd indices by 1
* [54] Yes & Cls & 100 & 100 & 99.999 Applies the hyperbolic tangent to each element.
* [55]* 55Yes & Cls & 100 & 100 & 99.999 & Applies the hyperbolic sine to each element. & Link to each element. & Link
* 56Yes & Cls & 100 & 100 & 100 & Sets every third element to zero. & Link
* 58Yes & Cls & 99.994 & 99.979 & 99.991 & Mirrors the first half of the sequence to the second half. & Link
* 60Yes & Cls & 100 & 100 & 99.999 & Increment each element in the sequence by 1. & Link
* 62Yes & Cls & 100 & 100 & 99.938 & Replaces each element with its factorial. & Link
* 63Yes & Cls & 99.964 & 99.901 & 99.920 & Replaces each element with the number of elements less than it in the sequence. & Link
* 64Yes & Cls & 100 & 100 & 99.999 & Cubes each element in the sequence. & Link
* 65Yes & Cls & 100 & 100 & 99.999 & Calculate the cube root of each element in the input sequence. & Link
* 66Yes & Cls & 100 & 100 & 99.983 & Round each element in the input sequence to the nearest integer. & Link
* 67Yes & Cls & 100 & 99.952 & 99.992 & Multiply each element of the sequence by the length of the sequence. & Link
* 68Yes & Cls & 100 & 100 & 100.000 & Increment each element until it becomes a multiple of 3 & Link
* 69Yes & Cls & 100 & 100 & 100 & "Assign -1, 0, or 1 to each element of the input sequence based on its sign." & Link
* 70Yes & Cls & 100 & 100 & 100 & Apply the cosine function to each element of the input sequence. & Link
* 71Yes & Cls & 100 & 99.964 & 100.000 & Divide each element by the length of the sequence & Link
* 72Yes & Cls & 100 & 99.961 & 99.916 & Negate each element in the input sequence. & Link
* 73Yes & Cls & 100 & 100 & 99.966 & Apply the sine function to each element of the input sequence. & Link
* 75Yes & Cls & 100 & 100 & 99.999 & Double each element of the input sequence. & Link
* 77Yes & Cls & 100 & 99.999 & 99.906 & Apply the tangent function to each element of the sequence. & Link
* 79Yes & Cls & 100 & 100 & 100 & Check if each number in a sequence is prime & Link
* 80Yes & Cls & 100 & 100 & 99.999 & Subtract a constant from each element of the input sequence. & Link

[MISSING_PAGE_FAIL:29]

\begin{table}
\begin{tabular}{c c c c c c c c}
130 & Yes & Cls & 100 & 99.976 & 99.980 & "Clips each element to be within a range (make the default range [2, 7])." \\
114 & Yes & Cls & 100 & 99.985 & 99.951 & Apply a logarithm base 10 to each element of the input sequence. \\
110 & Yes & Cls & 100 & 100 & 99.961 & "Inserts zeros between each element, removing the latter half of the list." \\
113 & Yes & Cls & 100 & 99.945 & 99.998 & "Inverts the sequence if it is sorted in ascending order, otherwise leaves it unchanged." \\
121 & Yes & Cls & 100 & 99.996 & 99.992 & Compute arcsine of all elements in the input sequence. \\
124 & Yes & Cls & 100 & 100 & Check if all elements in a list are equal. \\
123 & Yes & Cls & 100 & 99.961 & 99.916 & Apply arcsine to each element of the input sequence. \\ \end{tabular}
\end{table}
Table 11: A description of the new \(59\) tasks that were included in InterpBenchafter this article’s evaluation. Task type is either “Cls” (classification) or “Reg” (regression). The columns for validation metrics, Accuracy, Interchange Intervention Accuracy (IIA), and Strict Interchange Intervention Accuracy (SIIA) show the latest value after training.