# On the Use of Anchoring for Training Vision Models

 Vivek Narayanaswamy

Lawrence Livermore National Laboratory

narayanaswam1@llnl.gov

&Kowshik Thopalli

Lawrence Livermore National Laboratory

thopalli@llnlnl.gov

&Rushil Anirudh

Amazon

rushil15anirudh@gmail.com

&Yamen Mubarka

Lawrence Livermore National Laboratory

mubarka1@llnl.gov

&Wesam Sakla

Lawrence Livermore National Laboratory

sakla1@llnl.gov

&Jayaraman J. Thiagarajan

Lawrence Livermore National Laboratory

jjthiagarajan@gmail.com

###### Abstract

Anchoring is a recent, architecture-agnostic principle for training deep neural networks that has been shown to significantly improve uncertainty estimation, calibration, and extrapolation capabilities. In this paper, we systematically explore anchoring as a general protocol for training vision models, providing fundamental insights into its training and inference processes and their implications for generalization and safety. Despite its promise, we identify a critical problem in anchored training that can lead to an increased risk of learning undesirable shortcuts, thereby limiting its generalization capabilities. To address this, we introduce a new anchored training protocol that employs a simple regularizer to mitigate this issue and significantly enhances generalization. We empirically evaluate our proposed approach across datasets and architectures of varying scales and complexities, demonstrating substantial performance gains in generalization and safety metrics compared to the standard training protocol. The open-source code is available at - https://software.llnl.gov/anchoring

## 1 Introduction

Anchoring [1] is a recent architecture-agnostic principle for training deep neural networks. It reparameterizes each input \(x\) into a tuple comprising a reference sample \(\bar{r}\) and the _residual_\(d=x-\bar{r}\), i.e., \([\bar{r},d]\), \(\bar{r}\sim P_{r}\) and \(d\sim P_{\Delta}\). Here, \(P_{r}\) and \(P_{\Delta}\) denote the distributions of references and residuals respectively. The resulting tuple is then fed as input to a deep network instead of the original input \(x\), by concatenating the tuple elements along the feature axis for vector-valued data or the channel axis for image data. Although the first layer of the network needs to be modified to accommodate twice the number of input dimensions (due to concatenation), the rest of the model architecture and optimization strategies remain the same as in standard training. This simple re-parameterization of the input forces the neural network to model the joint distribution \(P_{(r,\Delta)}\) for predicting the target label \(y\). Formally, the training objective can be written as:

\[\theta^{*}=\arg\min_{\theta}\quad\frac{1}{|\mathcal{D}|}\sum_{(x,y)\in \mathcal{D}}\operatorname*{\mathbb{E}}_{\bar{r}\sim P_{r}}\mathcal{L}\bigg{[} y,\mathcal{F}_{\theta}\bigg{(}\texttt{concat}([\bar{r},x-\bar{r}])\bigg{)}\bigg{]},\] (1)

where \(\mathcal{L}(.)\) is a loss function such as cross-entropy, \(\mathcal{D}\) is the training dataset and \(\mathcal{F}\) is the underlying network parameterized by \(\theta\). In effect, for a given \(x\) and reference samples \(\bar{r}_{1},\ldots,\bar{r}_{k}\), anchoringensures that \(\mathcal{F}_{\theta}([\bar{\mathrm{r}}_{\mathrm{i}},\mathrm{d}_{\mathrm{i}}])= \cdots=\mathcal{F}_{\theta}([\bar{\mathrm{r}}_{\mathrm{k}},\mathrm{d}_{\mathrm{ k}}])\), where \(\mathrm{d}_{k}=\mathrm{x}-\bar{\mathrm{r}}_{k}\). In other words, regardless of the choice of reference the model must arrive at the same prediction for an input. This principle has been shown to produce models with improved calibration and extrapolation properties [2; 3], and to facilitate accurate epistemic uncertainty estimation [1]. In this paper, we systematically explore the utility of anchoring as a generic protocol for building vision models and make a number of fundamental insights on its training and inferencing, applicability to different architecture families (conv-nets, transformers), and most importantly, the implications on model generalization and safety.

Our main contributions in this work can be summarized as follows:

**A closer look into anchored training and inferencing**: By studying the roles of reference set diversity and the inferencing protocol choice on the behavior of anchored models, we identify a critical limitation in current practice. More specifically, we find that conventional anchored training fails to effectively leverage the reference diversity, thus restricting its generalization capabilities, and that merely adopting sophisticated inference protocols [2] cannot circumvent this limitation.

**A new anchored training protocol**: We attribute the limited generalization power of anchored models to the increased risk of learning undesirable shortcuts, owing to insufficient sampling of \(P_{(\mathrm{r},\Delta)}\) during training, particularly in cases of high reference diversity. To address this, we introduce a new training protocol for anchoring that relies on a novel reference-masking regularizer.

**Benchmarking generalization and safety of anchored models**: Since anchoring is architecture-agnostic, we benchmark it using a variety of conv-net/transformer architectures on CIFAR-10, CIFAR-100 and Imagenet-1K datasets. We demonstrate significant improvements in OOD generalization, calibration and anomaly resilience over standard training. We also show that, without incurring any additional training or inference overheads, anchoring is synergistic to existing training strategies (e.g., data augmentations, optimizers, schedulers).

## 2 A Closer Look into Anchored Training and Inference

### What makes anchoring a promising training protocol?

Anchored training forces the network to learn a mapping between the joint space of (reference, residuals) and the targets, rather than the original input-target pairs. At first glance, anchoring may seem like a trivial reposing of standard training, but it is conceptually very different. Through this reparameterization, anchoring creates different relative representations for a sample with respect to references drawn from \(P_{\mathrm{r}}\), and attempts to marginalize the effect of the reference when making a prediction for that sample. As demonstrated by [1], this process exploits the lack of shift invariance in the neural tangent kernel induced by deep networks [4], and implicitly explores a wider hypothesis class that is potentially more generalizable. Furthermore, anchored models have been found to extrapolate better to unseen data regimes through the use of transductive inferencing [2], i.e., identifying an optimal reference for each sample, such that the resulting residual is likely to have been exposed to the model during training. While anchoring offers promise, its success hinges on effectively leveraging the diversity of the reference-residual pairs and stably converging for the same protocols from standard training (e.g., architectures, data augmentations, optimizers etc.).

### Does reference diversity play a key role in anchored training?

A unique property of anchoring is its ability to utilize relative representations w.r.t. a reference distribution \(P_{\mathrm{r}}\) (realized using a reference set \(\mathcal{R}\)), effectively operating in the joint space \(P_{(\mathrm{r},\Delta)}\). During implementation, the reference set \(\mathcal{R}\) is defined as a subset of the training data itself i.e., \(\mathcal{R}\subseteq\mathcal{D}\)[1]. Intuitively, by controlling the construction of \(\mathcal{R}\), one can control the diversity of reference-residual combinations that anchored training is exposed to. We hope that with exposure to increasingly large and diverse reference sets, anchoring will explore a wide range of hypotheses, while also ensuring that the model can make consistent predictions for test samples using any randomly drawn reference \(\bar{\mathrm{r}}\in\mathcal{R}\). However, when the anchored training does not effectively characterize the joint distribution \(P_{(\mathrm{r},\Delta)}\), the generalization can suffer, particularly when tested beyond the regimes of training data. To obtain a deeper understanding of anchored training, we conduct an empirical study on CIFAR10/100 datasets by varying the diversity of \(\mathcal{R}\).

Setup_. We first sub-sample \(\mathcal{D}\) to construct reference sets of varying sizes ranging between 5 and \(\overline{50000}\), where the latter corresponds to the entire training dataset. The construction is such that each set represents an increasing level of sample diversity (i.e., samples from multiple classes). This is followed by anchored training based on the different reference sets with ResNet18 models [5]. All other training specifics and hyper-parameters are fixed across the experiments. Post-training, we evaluate the model performance on the CIFAR10C/100C synthetic corruption benchmarks [6] and report the average corruption accuracy across \(5\) corruption severity levels.

_Observations._ Figure 0(a) and 0(b) illustrates the performance of CIFAR10/100 anchored training on the respective evaluation benchmarks. Interestingly, we observe that the anchoring performance remains fairly similar (minor improvements in accuracy) even with orders of magnitude growth in the reference set size. While anchoring provides consistent benefits over standard training (\(0.5\%-1\%\) on average), it is clear that the growing diversity of \(P_{(\tau,\Delta)}\) is not fully leveraged. _This observation is in contrary to the insights from existing works, which recommend the use of the entire train data as the reference set for maximal benefits._ It is also worth noting that we utilize a single random reference (from the respective sets) to perform inference. This naturally raises the question if a more sophisticated inference protocol circumvent this limitation that we notice in anchored models.

### Can the choice of inference protocol improve the performance of anchored models?

From existing works on anchoring, we find that different inference protocols can be used to elicit improvements in uncertainty quantification and model extrapolation. For instance, Thiagarajan _et al._[1] employed a reference marginalization strategy that samples \(K\) random references from the reference set to obtain \(K\) independent predictions for a given input (similar to MC-dropout or deep ensembles). This is followed by computing the prediction average along with its standard deviation, wherein the latter was interpreted as an estimate of epistemic uncertainty. The intuition is that different reference-residual combinations can lead to slightly different predictions for test sample that has not been observed during training, and marginalizing across references can offer robustness. On the other hand, Netanyahu _et al._[2] introduced the bilinear transduction (BLT) protocol for performing extrapolation from unseen data regimes in regression tasks. It was found that generalizing to an "out of support" (OOS) sample \(\mathrm{x}_{t}\) (i.e., no evidence of observing such a sample in the training data) can be made more tractable by carefully choosing anchors \(\tilde{\mathrm{r}}\sim P_{\mathrm{r}}\) such that \(\mathrm{x}_{t}-\tilde{\mathrm{r}}=\tilde{\mathrm{d}}\sim P_{\Delta}\). It was argued that, even if the specific combination of \([\tilde{\mathrm{r}},\mathrm{x}_{t}-\tilde{\mathrm{r}}]\) may not be observed during training, the anchored model can produce better calibrated predictions when \(\tilde{\mathrm{r}}\in P_{\mathrm{r}}\) and \(\tilde{\mathrm{d}}\sim P_{\Delta}\). This is in contrast to [1], which hypothesized that when the tuple \([\tilde{\mathrm{r}},\mathrm{x}_{t}-\tilde{\mathrm{r}}]\notin P_{(\tau,\Delta)}\), the inconsistency in the prediction will manifest as epistemic uncertainties. However, neither of these clearly answer the impact of inference protocol choice on generalization performance, particularly when the reference

Figure 1: **Impact of reference set size on anchored training performance. With increase in reference set size, anchoring explores more diverse combinations of reference-residual pairs with the hope of demonstrating improved generalization performance. Surprisingly, the existing anchored training protocol does not effectively leverage this diversity even with increased reference set size albeit providing improvements in accuracy over standard training. We propose reference masking, a simple regularization strategy for training anchored models that recovers the lost performance.**

set diversity is high. To answer this, we conducted a systematic evaluation of these protocols with anchored models trained on CIFAR100 with the reference set \(\mathcal{R}=\mathcal{D}\).

_Setup_. We consider three evaluation protocols to make predictions for the CIFAR100C benchmark (i) \(\overline{1}\) Random, that utilizes a single reference (e.g., average of samples in \(\mathcal{R}\)) to obtain predictions; (ii) \(K\) Random that utilizes \(K\) random references followed by reference marginalization (\(K=10\) in our case); (iii) BLT that searches for the optimal reference in \(\mathcal{R}\) for each test sample. Since conducting such an exhaustive search can be expensive for bigger datasets, we pick a subset (set to \(50\) in our experiment).

_Observations_. The table in Figure 2 provides the average accuracies obtained from these inference protocols. Interestingly, while these protocols incurs varying inference times (column 3) (BLT \(>>K\) random \(>1\) random), their accuracies are statistically similar to each other (averaged across multiple seeds). _This observation implies that that the limitation of anchored training cannot be fixed through sophisticated inference protocols_. This motivates us to revisit anchoring and investigate if its behavior can be systematically improved during training itself.

## 3 Improving Anchored Training via Reference Masking Regularization

A close examination of anchored training reveals a critical limitation. As the size of the reference set increases, the number of reference-residual pairs grows combinatorially. For example, when \(\mathcal{R}=\mathcal{D}\), there are \(\binom{|\mathcal{R}|}{2}\) possible pairs, making it impractical to explore all pairs within a fixed number of training iterations. This results in insufficient sampling of \(P_{(\mathrm{r},\Delta)}\), increasing the risk that anchored training may overlook the reference and make predictions based solely on the residuals. Such non-generalizable shortcuts are problematic because a sample should not be identifiable without considering the reference. Therefore, it is crucial to enhance anchored training by more effectively utilizing the diversity present in large reference sets.

### Reference Masking Regularization

We propose a novel, yet simple regularization strategy for improving anchored training. Formally, for a given tuple \([\bar{\mathrm{r}},\mathrm{x}-\bar{\mathrm{r}}]\), and a user specified probability \(\alpha\) that controls how often the training is regularized, reference masking zeroes out the reference and keeps the residual fixed to obtain \([\mathbf{0},\mathrm{x}-\bar{\mathrm{r}}]\). For comparison, the tuple for the same sample \(\mathrm{x}\) but with a "zero" reference (Note: zero vector/image can be a valid reference in our reference distribution) corresponds to \([\mathbf{0},\mathrm{x}-\mathbf{0}]\). In order to preserve the integrity of the anchoring mechanism, we systematically discourage the model from making meaningful predictions when the reference is masked. This can be implemented by mapping randomly masked tuples to high-entropy predictions (i.e., uniform probabilities). We achieve this by minimizing the cross-entropy loss between the

Figure 3: PyTorch style pseudo code for our proposed approach.

Figure 2: **Impact of the choice of inference protocol on the performance of anchored models [1, 2]. (Left) A single random reference is chosen for sample prediction; (Middle) Obtaining predictions using K random references followed by averaging; (Right) Bilinear Transduction that identifies the optimal reference for each sample. We find that, while these protocols have varying computational complexities (time (s)/1000 samples), there are no apparent gaps in the performance, indicating that the limitation of anchored training cannot be fixed through sophisticated inference protocols.**

predictions from the masked tuple and the uniform prior \(\mathcal{U}\) over \(C\) classes (i.e, probability of any class \(=1/C\)). Figure 3 provides the algorithm our proposed approach.

Circling back to Figure 1, we observe that the proposed regularization significantly improves generalization accuracies compared to standard and original anchored training. This clearly demonstrates our strategy's effectiveness in leveraging the diversity in \(P_{\text{r},\Delta}\). Following the insights from the previous section, we use the simple \(1\) random inferencing protocol to obtain predictions for test samples. At low anchor set sizes (\(|\mathcal{R}|\leq 50\)), there is high likelihood of exposing the model to all possible combinations of samples and references, and hence the risk of learning such shortcuts is minimal. In such a scenario, overemphasizing the masking-based regularization (i.e., high \(\alpha\)) leads to underfitting, as illustrated in Figure 1. Unsurprisingly, reducing the masking probability can circumvent this underfitting behavior, as evidenced by the original anchored training, where \(\alpha=0\). However, the benefits of our regularization become apparent at larger reference set sizes. Additionally, the table in Figure 2 demonstrates that our approach performs similarly to the original anchored training, thereby implying no discernible impact on the inference efficiency.

### Analysis

**How does the accuracy landscape look like?** We hypothesize that the improved generalization of anchoring stems from the training process itself, which inherently enables the model to find better solutions in the weight space. To validate this, we follow the analysis in [11], where it was shown that that a well-generalizable solution is typically associated with a wider or flatter local optima in the loss/accuracy landscape. To this end, following the open-source implementation from [12], we obtained 2D heatmaps of accuracy evaluated on the CIFAR100C benchmark over different weight perturbations from the local minima inferring using different training strategies. Figure 4 visualizes the accuracy landscapes, where the \(x\) and \(y\) axes represent the co-ordinates that correspond to the different weight realizations. It can be observed that our approach produces wider and flatter optima in comparison to the baselines, thus explaining the generalization behavior.

Figure 4: **Impact of the proposed regularizer on anchored training. Using the CIFAR100C accuracy landscape, i.e., 2D heatmaps of the parameter space, we find that our approach identifies flatter and wider optima, thus leading to improved generalization [7]**

Figure 5: **Analysis of Anchored Models. Using evaluations on the CIFAR100C OOD generalization of ResNet18 models trained on CIFAR100, we study the behavior of the proposed approach when combined with data augmentation protocols (left) and in presence of training label noise (right).**

**Can anchoring be combined with data augmentations?** Using synthetic data augmentations during training is a widely adopted method for improving generalization of vision models. In this study, we investigate if anchoring can be utilized alongside existing augmentation protocols, including state-of-the-art techniques like PixMix [10]), and if the observed generalization improvements persist. Table 4(a) shows the CIFAR100C accuracies of models trained with different augmentation protocols. Note that, the architecture and the hyper-parameters of the augmentation protocols were fixed to be the same for a fair comparison. Remarkably, our approach consistently provides performance gains regardless of the augmentation protocols used, evidencing its utility as a generic training technique.

**Does training label noise impact anchoring?** In practice, we construct the reference set \(\mathcal{R}\subseteq\mathcal{D}\) for anchored training. However, under label noise, a fraction (or all) noisy samples can be included in the reference set, and get used for obtaining relative representations. A natural question is if this will impact the anchored training; however, we remind that the tuple construction in anchoring does not use the target label of a reference, and the benefits of anchoring will persist even under label noise corruptions. We validate this using the following experiment: We randomly flip the labels of \(l\%\) (\(l=\{0.5,1,2,5,10,15,20\}\)) of training samples before training a ResNet18 model on CIFAR100, and evaluate the generalization performance on CIFAR100C. Figure 4(b) illustrates that, with increasing levels of label noise, the anchored models do not demonstrate any additional challenges in handling label noise. In fact, it provides superior generalization (\(\sim 4\%\) improvements at 20% label noise) when compared to the standard and vanilla anchored training protocols.

## 4 Experiments

In this section, we empirically demonstrate the effectiveness of our proposed strategy in training models of varying scales (ResNets, Transformers) on datasets of different complexities (CIFAR10, CIFAR100, ImageNet). We systematically evaluate the generalization of these models under natural covariate shifts and synthetic corruptions. Additionally, we perform a comprehensive evaluation of model calibration, anomaly rejection, and robustness of task adapters in an effort to assess the safety of anchored models. For all experiments in this section, we utilize the entire training dataset as the reference set and train both the original and the proposed anchored models. During inference, we randomly select a single reference from the reference set and perform evaluation on the different test datasets.

**Training Datasets.** (i) CIFAR-10 and (ii) CIFAR-100 [13] datasets contain \(50,000\) training samples and \(10,000\) test samples each of size \(32\times 32\) belonging to \(10\) and \(100\) classes, respectively; (iii) ImageNet-1K [14] is a large-scale vision benchmark comprising \(1.3\) million training images and \(50,000\) validation images across 1000 diverse categories.

**Architectures.** We utilize a suite of vision transformer and CNN architectures with varying levels of structural and parameter complexity. Specifically for training with ImageNet, we consider SWINv2-T (\(28.4\)M params), SWINv2-S (\(49.7\)M), SWINv2-B (\(87.8\)M) [15] and ViT-B-16 (\(86.6\)M) [16]. For CIFAR100, we use ResNet-18 (\(11.7\)M) [5] and WideResNet40-2 (\(2.2\)M) [17] architectures, and ResNet-18 for CIFAR10. We provide the training recipes adopted for our models in Section A.3.

**Choice of \(\alpha\)**. Through extensive empirical studies with multiple architectures, we found using the masking schedule hyper-parameter \(\alpha=0.2\) (corresponds to every \(5^{\text{th}}\) batch in an epoch), leads to stable convergence (closely match the top-\(1\) validation accuracy of standard training) on ImageNet and \(\alpha=0.25\) for CIFAR10/100. Note that, our approach performs reference masking for an entire batch as determined by \(\alpha\). We have included our analysis on the impact of choice of \(\alpha\) in Section A.1.

### Generalization to Covariate Shifts and Synthetic Corruptions

**OOD Datasets and Evaluation Metrics**. For models trained on CIFAR10, we evaluate generalization on CIFAR10C and CIFAR10C. While the former contains \(19\) different types of corruptions (e.g., noise, blur, weather, digital), CIFAR10C comprises \(10\) types of synthetic noise, at 5 different severity levels respectively. Equivalently, for CIFAR100, we use the CIFAR100C and CIFAR100C benchmarks. For ImageNet-1K, we consider (i) ImageNet-C [6] with \(19\) natural image corruptions across \(5\) severity levels, (ii) ImageNet-C [18] with \(10\) noise corruptions across \(5\) severity levels; (iii) ImageNet-R [19] containing different renditions of \(200\) classes from ImageNet; (iv) ImageNet-S [20]comprising black and white sketch images from each class of ImageNet. We use the top@1 accuracy to evaluate generalization performance.

**Results and Discussions**. First, in Table 1, we report the averaged accuracy over all corruptions for every severity level on the CIFAR10C/C, CIFAR100C/C datasets, for the conv-nets trained on CIFAR10/100 respectively. We make a key finding that our proposed approach leads to significant gains in corruption accuracies across all severity levels over standard training (\(1.54\%-8.54\%\)) on an average. When compared to CIFAR10, the improvements of anchoring are apparent even at lower severity levels, for e.g., \(+3.74\) improvement with WRN 40-2 at CIFAR100C severity level 1.

Second, as shown in Table 2, we investigated the efficacy of anchored transformers trained on the large-scale ImageNet-1K dataset in terms of OOD generalization. It can be observed that our proposed approach consistently yields improvements in corruption accuracies over standard training across all architectures. A striking observation is that network capacity plays a significant role in effectively leveraging the increased diversity produced by anchored training (we used the entire ImageNet-1K as the reference set). For example, as we move from SWINv2-T (\(28.4\)M) to SWINv2-B (\(88\)M), we observe increasingly larger performance gains over standard training. Importantly, our proposed strategy handles high noise severity better, achieving improvements of \(2\%-7\%\) at severity 5 for both

\begin{table}
\begin{tabular}{c|c|c|c|c|c c c c|c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Method**} & \multicolumn{5}{c|}{**CIFAR10/100-C Accuracy \%**} & \multicolumn{5}{c}{**CIFAR10/100-C Accuracy \%**} \\ \cline{3-13}  & & & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** & **Dev.** \\ \hline \multirow{4}{*}{CIFAR10/100} & \multirow{4}{*}{ResNet-18} & Standard & \(95.15\) & \(89.44\) & \(83.47\) & \(77.91\) & \(70.74\) & \(58.72\) & \(86.86\) & \(81.97\) & \(74.51\) & \(65.94\) & \(60.31\) \\  & & Vanilla Anchoring & \(94.92\) & \(88.99\) & \(84.28\) & \(79.16\) & \(72.09\) & \(59.82\) & \(87.04\) & \(82.79\) & \(75.00\) & \(66.73\) & \(61.52\) \\  & & Proposed & \(95.72\) & \(90.98\) & \(87.15\) & \(83.17\) & \(77.81\) & \(67.26\) & \(89.24\) & \(85.38\) & \(78.34\) & \(70.33\) & \(65.43\) \\  & & \(\Delta\) & \(+0.57\) & \(+1.54\) & \(+3.68\) & \(+5.26\) & \(+7.07\) & \(+8.54\) & \(+2.38\) & \(+3.41\) & \(+3.53\) & \(+4.40\) & \(+5.12\) \\ \hline \multirow{4}{*}{CIFAR-100} & \multirow{4}{*}{ResNet-18} & Standard & \(77.6\) & \(65.56\) & \(56.77\) & \(51.25\) & \(45.47\) & \(34.13\) & \(62.0\) & \(54.08\) & \(44.98\) & \(36.55\) & \(32.27\) \\  & & Vanilla Anchoring & \(77.21\) & \(65.67\) & \(57.35\) & \(52.02\) & \(45.27\) & \(34.79\) & \(61.05\) & \(54.17\) & \(44.98\) & \(36.90\) & \(32.72\) \\  & & Proposed & \(77.89\) & \(67.0\) & \(59.51\) & \(54.88\) & \(47.8\) & \(36.66\) & \(64.75\) & \(58.10\) & \(49.78\) & \(41.42\) & \(36.81\) \\  & & \(\Delta\) & \(+0.29\) & \(+1.44\) & \(+2.74\) & \(+3.63\) & \(+4.21\) & \(+4.53\) & \(+2.47\) & \(+4.02\) & \(+4.89\) & \(+4.87\) & \(+4.54\) \\ \cline{2-13}  & \multirow{4}{*}{WRN 40-2} & Standard & \(75.48\) & \(62.26\) & \(52.82\) & \(46.85\) & \(40.12\) & \(30.05\) & \(60.09\) & \(52.89\) & \(44.44\) & \(35.78\) & \(31.06\) \\  & & Vanilla Anchoring & \(76.67\) & \(64.55\) & \(53.54\) & \(49.43\) & \(24.84\) & \(23.75\) & \(61.59\) & \(54.42\) & \(45.50\) & \(36.12\) & \(31.11\) \\  & & Proposed & \(77.03\) & \(60.0\) & \(57.77\) & \(52.33\) & \(46.64\) & \(35.52\) & \(63.83\) & \(57.76\) & \(49.32\) & \(40.26\) & \(35.29\) \\  & & \(\Delta\) & \(+1.55\) & \(+3.74\) & \(+4.95\) & \(+5.48\) & \(+5.52\) & \(+5.47\) & \(+3.24\) & \(+4.87\) & \(+4.88\) & \(+4.48\) & \(+4.23\) \\ \hline \end{tabular}
\end{table}
Table 1: **Generalization performance of CNNs trained on CIFAR10/100**. We report the ID test and the OOD (CIFAR10 -C/C, CIFAR100 -C/C) accuracies of standard and anchored CNNs to evaluate generalization (\(\uparrow\)). Note, we provide the difference (\(\Delta\)) between the proposed and the standard model in each case with blue.

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multicolumn{2}{c|}{**SWINv2-T (\(28.4\)M)**} & \multicolumn{2}{c|}{**SWINv2-S (\(49.7\)M)**} & \multicolumn{2}{c|}{**VITb6 (\(86.8\)M)**} & \multicolumn{2}{c}{**SWINv2-B (\(87.4\)M)**} \\ \cline{2-13}  & **Standard** & **Proposed** & \(\Delta\) & **Standard** & **Proposed** & \(\Delta\) & **Standard** & **Proposed** & \(\Delta\) & **Standard** & **Proposed** & \(\Delta\) \\ \hline ImageNet (val) & \(82.07\) & \(82.03\) & \(-0.04\) & \(83.71\) & \(83.68\) & \(-0.03\) & \(81.07\) & \(80.76\) & \(-0.31\) & \(84.11\) & \(84.09\) & \(-0.02\) \\ \hline ImageNet-R & \(40.84\) & \(41.17\) & \(+0.33\) & \(45.17\) & \(46.63\) & \(+1.46\) & \(44.06\) & \(46.39\) & \(+2.33\) & \(45.7\) & \(48.16\) & \(+2.46\) \\ \hline ImageNet-S & \(27.08\) & \(27.68\) & \(+0.69\) & \(32.25\) & \(33.3\) & \(+1.05\) & \(29.4\) & \(33.0\) & \(+3.60\) & \(31.91\) & \(33.34\) & \(+1.43\) \\ \hline ImageNet-C (Sev. 1) & \(71.63\) & \(72.13\) & \(+0.50\) & \(74.48\) & \(74.7\) & \(+0.22\) & \(72.37\) & \(72.52\) & \(+0.15\) & \(74.45\) & \(76.24\) & \(+0.79\) \\ ImageNet-C (Sev. 2) & \(64.89\) & \(65.71\) & \(+0.82\) & \(68.8\) & \(69.12\) & \(+0.32\) & \(66.57\) & \(67.38\) & \(+0.81\) & \(68.55\) & \(69.63Imagenet-C and C. All these observations clearly evidence the importance of leveraging the diversity of \(P_{(r,\Delta)}\) for enhanced generalization. Finally, we observe from Tables 1 and 2 that anchored training maintains competitive, and in a few cases, improved ID accuracies compared to standard training.

### Assessing Safety of Anchored Models

**Calibration and Anomaly Rejection**. While generalization is key to improve model utility, it must be ensured that the models are not over-confident on unknown inputs and produce well-calibrated prediction probabilities that match the likelihood of correctness. Hence, measuring calibration [21] is vital to understand how tempered the model predictions are under distribution shifts. On the other hand, when the inputs are semantically disconnected and do not share the same label space as the training data, we require the models to appropriately flag them as anomalies. To that end, we also conduct an extensive evaluation of model calibration under distribution shifts and anomaly rejection. For the former, we use the ImageNet-C/C/R/S variants, and for the latter, we consider two benchmarks: (a) Vision OOD, comprising commonly used anomaly rejection datasets - _iSUN_[22], _Textures_[23], and _Places365_[24]; and (b) _NINCO_[25], a recent benchmark containing images with semantic overlap with ImageNet but with no class overlap. Following standard practice [26], we use the Smoothed ECE metric [27] to assess calibration. For anomaly rejection, we obtain the energy scores [26] for both ID validation and OOD data, and report the AUROC metric.

We report the anomaly rejection and calibration performance of of transformers trained with ImageNet-1K. We compare the anomaly rejection performance against standard training using common vision OOD benchmarks (Textures, Places365, and iSUN datasets) and the more recent NINCO dataset. For evaluation, we consider the AUROC (\(\uparrow\)) metric. Moreover, we also provide Smoothed ECE scores (\(\downarrow\)) (mean, std) across different Imagenet corruption benchmarks. We highlight the best performing model in each case with blue.

**Robustness to Task Adaptation**. Evaluating model adaptation under task shifts [28] becomes important to shed light onto the quality and re-usability of features inferred in a backbone network. To that end we employ two evaluation protocols: Adaptation(ID Eval) and Adaptation (OOD Eval). In the former, we assume that the distribution of the dataset used for linear probing is the same as that of the test set. In the latter, we first train the linear probe (LP) with our anchored training approach using a probing dataset but evaluate the same with data drawn from a shifted w.r.t the probing dataset. Note, for both evaluation protocols, we fix the ViTb16 architecture as the Imagenet pre-trained feature extractor backbone. Note, we set \(\alpha=0.4\), a higher value than the original task model training as we observed stable convergence.

**Adaptation (ID Eval)**: We consider the following target datasets: (i) CIFAR-10 [13] ; (ii) CIFAR-100 [29] ; (iii) UCF101 [30]; (iv) Flowers102 [31]; (v) StanfordCars [32]. The results in Figure 6(a) demonstrate that the proposed approach achieves substantial performance gains over the baseline (\(0.81\%\) - \(2.68\%\)). These findings suggest that the reference masking regularizer yields feature representations that are transferable even under complex task shifts.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Method**} & \multirow{2}{*}{**Vision OOD**} & \multirow{2}{*}{**NINCO**} & \multirow{2}{*}{**Calibration**} \\ \cline{3-3} \cline{5-5}  & & \(76.54\) & \(77.46\) & \(0.121\pm 0.034\) \\ \cline{3-5}  & Proposed & \(\mathbf{77.65}\) & \(\mathbf{78.49}\) & \(\mathbf{0.117\pm 0.027}\) \\ \hline \multirow{2}{*}{SWINv2-S} & Standard & \(77.13\) & \(74.73\) & \(0.126\pm 0.039\) \\  & Proposed & \(\mathbf{79.56}\) & \(\mathbf{78.47}\) & \(\mathbf{0.119\pm 0.041}\) \\ \hline \multirow{2}{*}{VITb16} & Standard & \(\mathbf{77.29}\) & \(65.98\) & \(0.109\pm 0.037\) \\  & Proposed & \(76.88\) & \(\mathbf{70.32}\) & \(\mathbf{0.105\pm 0.028}\) \\ \hline \multirow{2}{*}{SWINv2-B} & Standard & \(75.89\) & \(72.13\) & \(0.132\pm 0.055\) \\  & Proposed & \(\mathbf{78.91}\) & \(\mathbf{74.53}\) & \(\mathbf{0.124\pm 0.051}\) \\ \hline \end{tabular}
\end{table}
Table 3: **Anomaly rejection and calibration performance of transformers trained on ImageNet-1K. We compare the anomaly rejection performance against standard training using common vision OOD benchmarks (Textures, Places365, and iSUN datasets) and the more recent NINCO dataset. For evaluation, we consider the AUROC (\(\uparrow\)) metric. Moreover, we also provide Smoothed ECE scores (\(\downarrow\)) (mean, std) across different Imagenet corruption benchmarks. We highlight the best performing model in each case with blue.**

**Adaptation (OOD Eval)**: For training linear probes, we use the DomainNet [33], comprising of images from \(345\) categories across six diverse domains. Specifically, we pick four domains, namely _real_, _sketch_, _clipart_, and _painting_ and train probes on (i) images from the _real_ domain, and (ii) images from the _sketch_ domain respectively. We then evaluate the LPs on the remaining three held-out domains. As Figure 6(b) illustrates, our proposed reference masking continues to substantially outperform standard training baseline on all held-out domains under both configurations. We attribute this behavior to our approach being able to effectively leverage the diversity in the reference-residual space to produce robust and better generalizable features supporting transferability.

## 5 Related Work

**Anchoring in Predictive Models**. Our work is based on the principle of anchoring first introduced in [1] where it was used to achieve stochastic data centering for epistemic uncertainty estimation. Since then, the anchoring has been extended to a variety of use-cases and applications. For e.g, Netanyahu _et al._[2] utilized anchoring for extrapolating to unseen data regimes [2] in regression settings and Trivedi _et al._[34] employed the same for graph neural network calibration. In contrast, our paper is the first to explore and facilitate the utility of anchoring as a viable training protocol for large scale vision models.

**Data Augmentations**. Augmentation strategies enforce models to be robust under different pixel-space manipulations improving generalization. For e.g., strategies such as Augmix [35] or random convolutions (RandConv) [36] are known to improve generalization. Recent advancements in the field include strategies such as PixMix [10], which utilizes an external dataset with complex image patterns to augment the training data, and ALT [37], which learns adversarially robust augmentations. While the idea of enforcing prediction consistency in anchoring might appear similar to training with synthetic data augmentations, we emphasize that anchoring does not alter the data (e.g., with perturbations or geometric transformations) but only creates relative representations for each sample with respect to different reference choices. Furthermore, it can be combined with data augmentations to achieve further gains in generalization (Table 4(a)).

**Model Safety**. As models are being increasingly adopted in a variety of sensitive applications [38; 39], safe model deployment has become critical [40; 41]. In this context, generalization to data beyond the training distribution [42; 6], ability to accurately detect anomalies in the input data [43; 26; 44] as well producing calibrated prediction probabilities [21; 3] are all important facets of safety evaluation. Hendrycks _et al._[10] argued that most existing training strategies compromise for one safety objective to satisfy another objective, thus limiting their real-world utility. We find from our experiments that anchoring jointly produces better generalization, calibration and anomaly rejection properties, which makes it a promising choice for practical deployment.

Figure 6: Assessing anchored and standard pre-trained ImageNet backbones on robustness to task shifts.

Conclusion

Through this work, we showed that anchoring leads to significant performance gains in generalization and other safety metrics, including calibration, anomaly rejection, and task adaptation, across varying dataset sizes (CIFAR-10 to ImageNet) and model architectures (Conv-Nets to Transformers). Notably, when the training recipe includes high-capacity architectures or advanced mechanisms, our method yields even greater performance gains over the base models. Our observations suggest that anchored training with larger reference sets requires reference masking regularization to control the risk of learning undesirable shortcuts while making predictions. However, we realize that state-of-the-art results in OOD generalization are often obtained using model souping [45] or by fine-tuning large scale pre-trained models [46]. Hence, we believe it will be valuable to integrate anchoring into these approaches. While we have not theoretically characterized the generalization of anchored models, our hypothesis is rooted in existing theory and our empirical results provide evidence for the hypothesis. Finally, it must be noted that anchoring is a domain-agnostic, architecture-agnostic, and task-agnostic training strategy for deep neural networks. However, developing a theoretical understanding of anchored models and understanding its benefits in domain-specific applications is crucial and forms an important future direction.

## Acknowledgements

This work was performed under the auspices of the U.S. Department of Energy by the Lawrence Livermore National Laboratory under Contract No. DE-AC52-07NA27344, Lawrence Livermore National Security, LLC. and was supported by the LLNL-LDRD Program under Project No. 22-ERD-006. LLNL-CONF-864979.

## References

* [1] Jayaraman J. Thiagarajan, Rushil Anirudh, Vivek Narayanaswamy, and Peer timo Bremer. Single model uncertainty estimation via stochastic data centering. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL https://openreview.net/forum?id=j0J9upqN5va.
* [2] Aviv Netanyahu, Abhishek Gupta, Max Simchowitz, Kaiqing Zhang, and Pulkit Agrawal. Learning to extrapolate: A transductive approach. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=lid14UkLPd4.
* [3] Rushil Anirudh and Jayaraman J Thiagarajan. Out of distribution detection via neural network anchoring. In _Asian Conference on Machine Learning_, pages 32-47. PMLR, 2023.
* [4] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [6] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=HJz6tiCqYm.
* [7] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. _Advances in neural information processing systems_, 31, 2018.
* [8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 702-703, 2020.

* Muller and Hutter [2021] Samuel G Muller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-art data augmentation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 774-782, 2021.
* Hendrycks et al. [2022] Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Bo Li, Dawn Song, and Jacob Steinhardt. Pixmix: Dreamlike pictures comprehensively improve safety measures. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16783-16792, 2022.
* Izmailov et al. [2018] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. _arXiv preprint arXiv:1803.05407_, 2018.
* Li et al. [2018] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In _Neural Information Processing Systems_, 2018.
* Krizhevsky et al. [2009] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. _International journal of computer vision_, 115(3):211-252, 2015.
* Liu et al. [2022] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution. In _International Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2021. URL https://openreview.net/forum?id=YicbFdNTTy.
* Zagoruyko and Komodakis [2016] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In _British Machine Vision Conference 2016_. British Machine Vision Association, 2016.
* Mintun et al. [2021] Eric Mintun, Alexander Kirillov, and Saining Xie. On interaction between augmentations and corruptions in natural corruption robustness. _Advances in Neural Information Processing Systems_, 34:3571-3583, 2021.
* Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. _ICCV_, 2021.
* Wang et al. [2019] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In _Advances in Neural Information Processing Systems_, pages 10506-10518, 2019.
* Guo et al. [2017] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International Conference on Machine Learning_, pages 1321-1330. PMLR, 2017.
* Xiao et al. [2010] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* Cimpoi et al. [2014] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3606-3613, 2014.
* Zhou et al. [2017] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE transactions on pattern analysis and machine intelligence_, 40(6):1452-1464, 2017.

* [25] Julian Bitterwolf, Maximilian Mueller, and Matthias Hein. In or out? fixing imagenet out-of-distribution detection evaluation. In _ICML_, 2023. URL https://proceedings.mlr.press/v202/bitterwolf23a.html.
* [26] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. _Advances in Neural Information Processing Systems_, 2020.
* [27] Jaroslaw Blasiok and Preetum Nakkiran. Smooth ece: Principled reliability diagrams via kernel smoothing. _arXiv preprint arXiv:2309.12236_, 2023.
* [28] Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-of-distribution robustness throughout fine-tuning. _arXiv preprint arXiv:2106.15831_, 2021.
* [29] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The cifar-10 dataset. _online: http://www. cs. toronto. edu/kriz/cifar. html_, 55:5, 2014.
* [30] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. _arXiv preprint arXiv:1212.0402_, 2012.
* [31] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing_, pages 722-729. IEEE, 2008.
* [32] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13)_, Sydney, Australia, 2013.
* [33] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* [34] Puja Trivedi, Mark Heimann, Rushil Anirudh, Danai Koutra, and Jayaraman J. Thiagarajan. Estimating epistemic uncertainty of graph neural networks. In _Data Centric Machine Learning Workshop @ ICML_, 2023.
* [35] Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. AugMix: A simple data processing method to improve robustness and uncertainty. _Proceedings of the International Conference on Learning Representations (ICLR)_, 2020.
* [36] Zhenlin Xu, Deyi Liu, Junlin Yang, Colin Raffel, and Marc Niethammer. Robust and generalizable visual representation learning via random convolutions. In _International Conference on Learning Representations_, 2021.
* [37] Tejas Gokhale, Rushil Anirudh, Jayaraman J Thiagarajan, Bhavya Kailkhura, Chitta Baral, and Yezhou Yang. Improving diversity with adversarially learned transformations for domain generalization. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 434-443, 2023.
* [38] Thomas Davenport and Ravi Kalakota. The potential for artificial intelligence in healthcare. _Future healthcare journal_, 6(2):94, 2019.
* [39] Daniel Bogdoll, Maximilian Nitsche, and J Marius Zollner. Anomaly detection in autonomous driving: A survey. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4488-4499, 2022.
* [40] Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et al. Identifying and mitigating the security risks of generative ai. _Foundations and Trends(r) in Privacy and Security_, 6(1):1-52, 2023.
* [41] Dan Hendrycks, Nicholas Carlini, John Schulman, and Jacob Steinhardt. Unsolved problems in ml safety. _arXiv preprint arXiv:2109.13916_, 2021.

* Yang et al. [2021] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. _arXiv preprint arXiv:2110.11334_, 2021.
* Hendrycks and Gimpel [2017] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _Proceedings of International Conference on Learning Representations_, 2017.
* Hendrycks et al. [2018] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In _International Conference on Learning Representations_, 2018.
* Wortsman et al. [2022] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 23965-23998. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/wortsman22a.html.
* Goyal et al. [2023] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19338-19347, June 2023.
* Anirudh and Thiagarajan [2022] Rushil Anirudh and Jayaraman J Thiagarajan. Out of distribution detection via neural network anchoring. In _Asian Conference on Machine Learning (ACML)_. PMLR, 2022.

Appendix

### How does the choice of \(\alpha\) impact training?

The parameter \(\alpha\) controls the frequency of the regularization applied to anchored training. Under the assumptions of operating with wide reference sets, through Table 4 we note that moderate to small values of \(\alpha\) enable better regularization of anchored training. Notably, setting \(\alpha=0.25\) i.e. masking references for one in four samples, yields impressive gains in ID and OOD performance. Conversely, over-regularizing by setting \(\alpha\) to a large value (e.g., \(1.0\)) entails masking every reference, unsurprisingly results in models that generalize poorly, as they are tasked with learning solely from residuals.

#### a.1.1 Choosing \(\alpha\) in practice

At low reference set sizes, there is a high likelihood of exposing the model to all possible combinations of samples and references, and hence the risk of learning shortcuts is minimal. In this case, overemphasizing the reference masking probability (i.e., increasing \(\alpha\)) can significantly inhibit this exposure. Consequently, this leads to underfitting as the model is tasked with learning solely from the residuals which is undesirable in practice (Blue curve for reference set size \(\leq\)50 in Fig. 1). Reducing \(\alpha\) can combat this behavior, as evidenced by the original anchored training (special case of reference masking with \(\alpha=0\) namely the red curves for reference set size \(\leq\)50 in Fig. 1).

Now, with larger reference sets (e.g., datasets in the scale of ImageNet1K), the number of reference-residual pairs grows combinatorially, making it impractical to expose the model to all diverse pairs in a fixed number of training iterations. In such a scenario, reducing \(\alpha\) can increase the risk of learning shortcuts and lead to suboptimal performance. Increasing \(\alpha\) on the other hand can in fact aid training as it systematically avoids these shortcuts and improves generalization. In summary, the optimal \(\alpha\) value depends both on the reference set size and the convergence behavior of model training.

### Does Training for Additional Epochs Alleviate the Reference Set Size Problem?

One possible way of alleviating this problem is by reducing the reference set size. However, this reduces the diversity of the reference-residual pairs exposed during training and can lead to a poor solution. While the issue of diversity can be combated with large reference set sizes, increasing the number of epochs alone does not solve the problem as there exists a combinatorially large number of reference-residual pairs which cannot be practically explored, and the model will still be vulnerable to shortcuts. Moreover, modifying the number of training epochs results in non-trivial modifications in the training hyper-parameters (e.g., learning rate schedules) and can lead to poorly convergent models if the hyper-parameters are chosen incorrectly. Hence, our reference masking regularizer for anchored training, helps mitigate shortcut decision rules while also being computationally efficient.

### Additional Details on Training Protocols

Table 5 outlines the recipes (augmentations, epochs, optimizers) leveraged for model training. Note that, the other hyper-parameters can be found in [47] for CIFAR10/100 and https://pytorch.org/vision/stable/models.html for ImageNet. We emphasize that, anchoring can be used as a generic model training wrapper, allows integration with any data augmentation or training strategy, and is not restricted to the recipes considered.

\begin{table}
\begin{tabular}{|l|c|c|c|c|c|} \hline \(\alpha\rightarrow\) & 0.0 & 0.25 & 0.5 & 0.75 & 1.0 \\ \hline ID Test Acc. \% & 77.21 & **77.89** & 76.97 & 75.4 & 57.90 \\ \hline OOD Acc. \% & 51.01 & **53.77** & 52.61 & 52.30 & 35.40 \\ \hline \end{tabular}
\end{table}
Table 4: **Impact of \(\alpha\) on anchored training**. As we gradually increase \(\alpha\), there is a risk of over-regularization which can lead to severe underfits. Note, we consider \(\mathcal{R}=\mathcal{D}\) in this study.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline  & & & \multicolumn{3}{c|}{**Number of Epacks**} \\ \cline{3-7}
**Model** & **Dataset** & **Training Recipes** & **Non-MacGreed** & **Anchered** & **Optimizer** \\ \hline ResNet-18, WRN-40-2 & CIFAR-10100 & Horizontal \& Vertical Flips & 200 & 200 & SGD with Multi-Step \\ \hline SWTW\(\geq\)2-7, SWNN\(\geq\)2-8, SWTW\(\geq\)2-B & ImageNet & Hirap, CutMix, AutoSegment, RandomErase, AugMix, Label Smoothing & 300 & 330 & Adam/W with Cosine Annealing \\ \hline VITb16 & ImageNet & Mixrep, CutMix, AutoSegment, AugMix, Label Smoothing & 300 & 330 & Adam/W with Cosine Annealing \\ \hline \end{tabular}
\end{table}
Table 5: **Protocols adopted for training anchored models across different datasets and architectures.** While we adopt standard training recipes for training our models, we note that anchoring can serve as a generic wrapper that can be applied on top of any other existing recipe.

### Expanded ImageNet Generalization Results

We provide an expanded version of Table 2 that includes the anchored training protocol without the reference-masking regularizer.

### Expanded Anomaly Rejection Results for Vision OOD Datasets

While Table 3 in the main paper provided anomaly rejection results averaged over all Vision OOD datasets, we expand and present metrics for each dataset in Table 7

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Method**} & \multirow{2}{*}{**ID Acc.**} & \multirow{2}{*}{**ImageNet-R**} & \multirow{2}{*}{**ImageNet-R**} & \multirow{2}{*}{**ImageNet-R**} & \multicolumn{2}{c|}{**ImageNet-R**} & \multicolumn{2}{c|}{**ImageNet-R**} & \multicolumn{2}{c|}{**ImageNet-R**} & \multicolumn{2}{c|}{**Calibration**} \\ \cline{6-13} \cline{6-13}  & & & & **Soc.1** & **Soc.2** & **Soc.3** & **Soc.4** & **Soc.5** & **Soc.1** & **Soc.2** & **Soc.3** & **Soc.4** & **Soc.5** & **Calibration** \\ \hline \multirow{3}{*}{SWINv2-T (28.46)} & Standard & 82.07 & 0.84 & 27.08 & 71.63 & 64.89 & 57.77 & 67.77 & 36.56 & 71.73 & 67.12 & 61.2 & 52.01 & 66.54 & 0.121 \(\pm\) 0.034 \\  & Annealing & 82.26 & 40.36 & 27.16 & 72.92 & **65.58** & 55.95 & 0.51 & 37.41 & 72.68 & 69.36 & 62.93 & 53.74 & 48.14 & 0.121 \(\pm\) 0.032 \\  & Proposed & 82.03 & **41.17** & **27.68** & 72.13 & 65.71 & **59.21** & 50.01 & **38.58** & **75.51** & **70.45** & **65.77** & **57.31** & **51.76** & **0.117 \(\pm\) 0.027** \\ \hline \multirow{3}{*}{SWINv2-S (6.76)} & Standard & 83.71 & 61.37 & 32.25 & 74.48 & 68.8 & 62.84 & 54.12 & 43.28 & 75.30 & 72.26 & 67.14 & 56.73 & 31.76 \(\pm\) 0.039 \\  & Autoencoding & 84.09 & 93.55 & **20.78** & **74.75** & 67.12 & 51.24 & 37.41 & 76.70 & 73.87 & 67.99 & 63.93 & 50.12 \(\pm\) 0.024 \\  & Proposed & 83.68 & **46.63** & **39.38** & 74.77 & **69.12** & **64.85** & **55.55** & **44.38** & **76.59** & **74.24** & **70.17** & **62.93** & **58.26** & **0.119 \(\pm\) 0.044** \\ \hline \multirow{3}{*}{VITb16 (86.04)} & Standard & 81.07 & 40.06 & 29.4 & 72.72 & 60.57 & 61.6 & 62.88 & 41.99 & 72.75 & 60.01 & 63.47 & 54.77 & 0.099 \(\pm\) 0.037 \\  & Annealing & 80.57 & 63.56 & 33.22 & 76.94 & 67.47 & 62.33 & 51.66 & 43.48 & 73.23 & 67.47 & 64.57 & 60.93 & 51.46 & 0.108 \(\pm\) 0.055 \\  & Proposed & 80.76 & **40.39** & **39.30** & 72.52 & **67.38** & **62.87** & **55.13** & **45.23** & **76.56** & **70.9

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: [NA]
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In a few cases of training transformers on ImageNet, our approach produces slightly lower in-distribution accuracies (for e.g, 0.3% reduction in VitB16). While this comes at a significant gain in OOD accuracy, the question remains of how to improve the anchored training to prevent this performance drop. We also provide additional limitations section in conclusion section.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: [NA]
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided extensive details regarding data pre-processing pipelines, hyper-parameters, training protocols in 4. We have also provided PyTorch code snippets for easy implementation of our approach.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: [NA]
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We used standard train, validation and test splits that are made available with datasets. Training protocols, hyperparameters and the sensitivity of model for hyperparameters are provided in the appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Experiments were conducted using multiple seeds and error bars are reported.
8. **Experiments Compute Resources**Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: [NA].
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: [NA]
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed them in the related work as well conclusion section. In our opinion, there are no negative societal impacts.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: [NA]
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer:[NA] Justification: [NA]
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: [NA]
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA]
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA]