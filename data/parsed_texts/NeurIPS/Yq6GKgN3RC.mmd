Federated Learning with Client Subsampling, Data Heterogeneity, and Unbounded Smoothness: A New Algorithm and Lower Bounds

 Michael Crawshaw

Department of Computer Science

George Mason University

Fairfax, VA 22030, USA

mcrawsha@gmu.edu

&Yajie Bao

School of Mathematical Sciences

Shanghai Jiao Tong University

Shanghai, China

baoyajie2019stat@sjtu.edu.cn

&Mingrui Liu

Department of Computer Science

George Mason University

Fairfax, VA 22030, USA

mingruil@gmu.edu

Equal Contribution.Corresponding Author.

###### Abstract

We study the problem of Federated Learning (FL) under client subsampling and data heterogeneity with an objective function that has potentially unbounded smoothness. This problem is motivated by empirical evidence that the class of relaxed smooth functions, where the Lipschitz constant of the gradient scales linearly with the gradient norm, closely resembles the loss functions of certain neural networks such as recurrent neural networks (RNNs) with possibly exploding gradient. We introduce EPISODE++, the first algorithm to solve this problem. It maintains historical statistics for each client to construct control variates and decide clipping behavior for sampled clients in the current round. We prove that EPISODE++ achieves linear speedup in the number of participating clients, reduced communication rounds, and resilience to data heterogeneity. Our upper bound proof relies on novel techniques of recursively bounding the client updates under unbounded smoothness and client subsampling, together with a refined high probability analysis. In addition, we prove a lower bound showing that the convergence rate of a special case of clipped minibatch SGD (without randomness in the stochastic gradient and with randomness in client subsampling) suffers from an explicit dependence on the maximum gradient norm of the objective in a sublevel set, which may be large. This effectively demonstrates that applying gradient clipping to minibatch SGD in our setting does not eliminate the problem of exploding gradients. Our lower bound is based on new constructions of hard instances tailored to client subsampling and a novel analysis of the trajectory of the algorithm in the presence of clipping. Lastly, we provide an experimental evaluation of EPISODE++ when training RNNs on federated text classification tasks, demonstrating that EPISODE++ outperforms strong baselines in FL. The code is available at https://github.com/MingruiLiu-ML-Lab/episode_plusplus.

## 1 Introduction

Federated Learning (FL) [33; 23] is a distributed learning paradigm in which many clients collaboratively train a machine learning model while communicating over a network, which preserves privacy and leverages parallelism across many clients. Minimizing communication cost, accounting for data heterogeneity across clients, and allowing for partial client participation are core principles of FL. Interest in FL has grown in recent years, especially with user-facing applications such as next word prediction on smartphones [19].

Optimization is a central part of FL algorithms, and most work on non-convex optimization assumes a smooth objective in both the single machine setting [15; 16; 1] and the FL setting [41; 48; 24; 26]. However, recent work [52; 8] has shown empirical evidence that certain neural networks (LSTMs [21], and Transformers [42]) do not satisfy this assumption, but do satisfy a weaker condition known as _relaxed smoothness_[52]. Under relaxed smoothness, techniques such as gradient clipping [37] are essential for avoiding exploding gradients. To avoid the negative effects of exploding gradients, GD without gradient clipping requires a step size inversely proportional to the maximum gradient norm of the objective in a sublevel set (denoted as \(M\)), resulting in very slow convergence. The usefulness of gradient clipping under relaxed smoothness matches observations of training these neural networks in practice, for example on natural language tasks, which are common in FL [23].

However, little work yet exists for FL in the relaxed smoothness setting. Liu et al. [32] introduced a communication-efficient gradient clipping algorithm for FL under relaxed smoothness, with the additional assumption of homogeneous client data and a distributional assumption on the noise of stochastic gradients. The EPISODE algorithm [9] was subsequently introduced to handle heterogeneous client data in this setting, but requires full client participation, that is, that every client participates in every communication round. This significantly decreases the practical applicability of EPISODE, since full client participation is rarely achievable with large-scale FL in practice [23].

In this work, we introduce EPISODE++, the first algorithm for FL under relaxed smoothness, client heterogeneity, and client subsampling. EPISODE++ maintains statistics of the history of gradients for each client, and uses these statistics to (1) correct each local update step to approximate an update on the global loss and (2) determine at which steps the clipping operation should be performed. We prove that EPISODE++ achieves linear speedup in the number of participating clients, has reduced communication cost, and enjoys convergence rate independent of client heterogeneity.

A previous line of work in the non-convex smooth stochastic setting [45; 47; 27] compares FL algorithms against a classical baseline: minibatch SGD [40]. Despite the impressive results of newer

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Communication Complexity (\(R\))} & Best Iteration & \multirow{2}{*}{\begin{tabular}{c} Largest \(I\) to guarantee \\ linear speedup \\ \end{tabular} } & \multirow{2}{*}{Setting} \\ \cline{1-1} Local SGD [48] & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}+\frac{\Delta L_{0}+L_{1} (\kappa+\sigma)}{\sigma^{2}}\right)\) & \(O\left(\frac{L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}\right)\) & \(O\left(\frac{L_{0}\sigma^{2}}{(\kappa+L_{1}(\kappa+\sigma)(1+\frac{\sigma}{ \varepsilon}))N\kappa^{2}}\right)\) & (H) \\ \hline SCAFFOLD [24] & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}+\frac{\Delta L_{0}}{ \sigma^{2}}\right)\) & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}\right)\) & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}\right)\) & (H), (S) \\ \hline CELGC [32] & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}\right)\) & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}\right)\) & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{Nt^{\mathrm{c}}}\right)\) & \(O\left(\frac{\Delta L_{0}\sigma^{2}}{(\kappa+L_{1}(\kappa+\sigma)(1+\frac{ \sigma}{\varepsilon}))N\kappa^{2}}\right)\) & (Re) \\ \hline EPISODE++ (Theorem 1)\({}^{\dagger}\) & \(\tilde{O}\left(\frac{\Delta L_{0}\sigma^{2}}{St^{\mathrm{c}}}+\frac{\Delta(L_{0} +L_{1}(\kappa+\sigma)\sigma}{L_{1}\sigma^{2}}\right)\frac{L_{0}}{L_{1}\mu}\right)\) & \(\tilde{O}\left(\frac{\Delta L_{0}\sigma^{2}}{St^{\mathrm{c}}}\right)\) & \(\tilde{O}\left(\frac{L_{0}\sigma^{2}}{(\kappa+L_{1}(\kappa+\sigma)(1+\frac{ \sigma}{\varepsilon}))\left(\sigma+\frac{L_{0}}{L_{1}\mu}\right)S\kappa}\right)\) & (Re), (H), (S) \\ \hline 
\begin{tabular}{l} Clipped Minibatch \\ SGD (Theorem 2) \\ \end{tabular} & \(\tilde{\Omega}\left(\frac{\Delta L_{1}M}{c^{2}}\right)\) & \(\tilde{\Omega}\left(\frac{\Delta L_{1}M\lambda}{c^{2}}\right)\) & - & (Re), (H), (S) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Best complexity to find an \(\epsilon\)-stationary point for various methods and settings. The setting column describes the features of the setting in which the algorithm can solve the problem: (Re) denotes relaxed smoothness, (H) heterogeneous data, and (S) client subsampling. \(\sigma\): stochastic gradient noise, \(\kappa\): client heterogeneity, \(\Delta\): objective gap at the initial solution, \(I\): local steps, \(R\): communication rounds, \(T=RI\): iteration complexity, \(N\): number of clients, \(S\): number of subsampled clients. \({}^{\dagger}\) denotes a high probability guarantee. \(\tilde{O}(\cdot)\) and \(\tilde{\Omega}(\cdot)\) omit logarithmic terms.

algorithms in practice, only a few works have proven that some FL algorithms have a theoretical advantage over minibatch SGD [24; 27]. It is therefore natural to ask what is the analogue of minibatch SGD in the relaxed smooth setting, and whether it is possible to improve upon this analogue. In this work we consider clipped minibatch SGD, which is obtained by applying gradient clipping to each update of minibatch SGD. We demonstrate a surprising negative result for clipped minibatch SGD: under client subsampling, relaxed smoothness, and client heterogeneity, the convergence rate of clipped minibatch SGD still depends on \(M\). This implies that _gradient clipping does not help minibatch SGD_ in this setting.

Our contributions can be summarized as follows:

* We introduce EPISODE++, the first algorithm for FL under relaxed smoothness, client heterogeneity, and client subsampling. We prove that EPISODE++ achieves linear speedup in the number of participating clients, reduced communication cost, and has convergence rate independent of client heterogeneity. Table 1 shows a detailed comparison of the complexity of various algorithms. To achieve this result, we introduce novel techniques of recursively bounding the client updates in the presence of unbounded smoothness and client data heterogeneity, together with a refined high probability analysis.
* We demonstrate a lower bound for clipped minibatch SGD in which the convergence rate depends on \(M\) (the maximum gradient norm of the objective function in a sublevel set). This shows that, in our setting, clipped minibatch SGD is susceptible to exploding gradients, and avoiding them requires a very small learning rate, which slows down convergence. Our lower bound is based on new constructions of hard instances tailored to client subsampling and a novel analysis of the trajectory of the algorithm in the presence of clipping.
* We empirically evaluate EPISODE++ against strong baselines when training RNNs on federated text classification tasks. The results show that EPISODE++ consistently outperforms baselines across various client participation ratios and is resilient to heterogeneous client data, which is consistent with our theory.

## 2 Related Work

Federated LearningFL was proposed by [33], where the authors designed the FedAvg algorithm, which is also referred to as local SGD in the literature [41; 31; 48]. The local SGD algorithm has been analyzed in various settings, including convex smooth setting [41; 11; 28; 24; 45; 47; 26; 49; 17; 25], convex composite setting [50; 2; 35], and nonconvex smooth setting [22; 43; 31; 18; 48; 28; 24; 38; 53; 26]. There is a line of work which specially considered client partial participation in FL [6; 13; 24; 29; 44; 7] under convex or nonconvex smooth settings. Recently, Liu et al. [32] and Crawshaw et al. [9] considered FL with nonconvex and relaxed smooth functions for homogeneous and heterogeneous data respectively. However, they assume full client participation and neither of them are applicable to the case of client subsampling.

Relaxed SmoothnessRelaxed smoothness was proposed by [52] as a relaxation of the standard smoothness condition, which is used to model the exploding gradient problem in training deep neural networks such as recurrent neural networks [36; 37] and long-short term memory networks [52], language models [14; 34] and transformers [8]. Zhang et al. [52] proved that gradient clipping converges faster than any fixed step size gradient descent for relaxed smooth functions. The complexity bound in [52] was further improved by [51]. Recently, there is a line of work which considered different algorithms and various analyses under relaxed smoothness [8; 39; 12]. However, all of them focused on single machine setting and may not be applicable to FL setting.

Lower bounds in Federated LearningThere are several lower bound results for FL algorithms. Woodworth et al. [45; 47] compared minibatch SGD and local SGD in the regime of federated stochastic convex optimization setting for homogeneous and heterogeneous data and established lower bounds for local SGD. Woodworth et al. [46] proved a min-max complexity of distributed stochastic convex optimization for any intermittent communication algorithm. Glasgow et al. [17] established improved lower bounds of local SGD in convex optimization setting for both homogeneous and heterogeneous data. However, all of these lower bounds are not applicable to our setting where the problem instance is relaxed smooth with heterogeneous data and client subsampling.

Problem Setup

We consider federated learning with heterogeneous and stochastic objectives, where the goal is to minimize the average loss function across \(N\) clients. For \(i\in[N]\), let \(f_{i}(\bm{x})=\mathbb{E}_{\xi\sim\mathcal{D}_{i}}[F_{i}(\bm{x};\xi)]\) be the objective of the \(i\)-th client, where \(\mathcal{D}_{i}\) is the underlying data distribution of the \(i\)-th client. Then the global objective is

\[\min_{\bm{x}\in\mathbb{R}^{d}}\left\{f(\bm{x}):=\frac{1}{N}\sum_{i=1}^{N}f_{i} (\bm{x})\right\}.\] (1)

Since each \(f_{i}\) is not necessarily convex, we consider the problem of finding an \(\epsilon\)-stationary point, that is, a point \(\bm{x}\in\mathbb{R}^{d}\) such that \(\|\nabla f(\bm{x})\|\leq\epsilon\).

Most works on non-convex optimization [15; 16; 1] consider the case where each \(f_{i}\) is \(L\)-smooth, so that \(\|\nabla^{2}f_{i}(\bm{x})\|\leq L\) for every \(\bm{x}\in\mathbb{R}^{d}\). However, several works [52; 8] have shown empirical evidence that objective functions corresponding to some types of neural networks (such as LSTMs [21] and Transformers [42]) do not satisfy this condition, but do satisfy a strictly weaker condition known as \((L_{0},L_{1})\)-smoothness, or relaxed smoothness. A second-order differentiable function \(g:\mathbb{R}^{d}\rightarrow\mathbb{R}\) is called \((L_{0},L_{1})\)-smooth if \(\|\nabla^{2}g(\bm{x})\|\leq L_{0}+L_{1}\|\nabla g(\bm{x})\|\) for all \(\bm{x}\in\mathbb{R}^{d}\). Notice that any \(L\)-smooth function is \((L,0)\)-smooth.

In this work we consider the problem described in (1) under the following assumptions.

**Assumption 1**.: _(i) Denoting by \(\bm{x}_{0}\) the initial iterate, there exists some \(\Delta>0\) such that \(f(\bm{x}_{0})-\min_{\bm{x}\in\mathbb{R}^{d}}f(\bm{x})\leq\Delta\). (ii) Each \(f_{i}\) and \(f\) is \((L_{0},L_{1})\)-smooth. (iii) There exist \(\kappa\geq 0\), \(\rho\geq 1\) such that \(\|\nabla f_{i}(\bm{x})\|\leq\kappa+\rho\|\nabla f(\bm{x})\|\) for all \(\bm{x}\in\mathbb{R}^{d}\). (iv) There exists \(\sigma\geq 0\) such that \(\mathbb{E}_{\xi\sim\mathcal{D}_{i}}[\nabla F_{i}(\bm{x};\xi)]=\nabla f_{i}(x)\) and \(\|\nabla F_{i}(\bm{x};\xi)-\nabla f_{i}(\bm{x})\|\leq\sigma\) almost surely for \(\xi\sim\mathcal{D}_{i}\)._

Assumption \((i)\) is standard in non-convex optimization [15; 16]. Assumption \((ii)\) is typically used in the FL literature [32; 9]. Assumption \((iii)\) is used in heterogeneous federated learning [24] and describes the heterogeneity between client objectives: if \(\kappa=0\) and \(\rho=1\), then all client objectives \(f_{i}\) are equal. Assumption \((iv)\) is common in the relaxed smoothness setting [52; 51; 32; 8; 9].

In addition, we consider the case of partial client participation in federated learning, also known as client subsampling. With partial participation, only \(S\) out of \(N\) clients will participate in each communication round, which exacerbates the issue of client heterogeneity.

## 4 Algorithm and Convergence Analysis

### Main Challenges and Algorithm Design

We first illustrate why existing algorithms such as SCAFFOLD [24] and EPISODE [9] are not able to handle heterogeneous data, relaxed smoothness and client subsampling simultaneously. The analysis of SCAFFOLD crucially requires the function to be \(L\)-smooth to recursively bound (1) the lag error from client subsampling and (2) client drift from local updates, but this argument is not applicable for relaxed smooth functions whose gradient information changes quickly. EPISODE has convergence guarantees for relaxed smooth functions and heterogeneous data, but only with full client participation. A naive variant of EPISODE in the client subsampling case does not work: the indicator of gradient clipping is based only on information from clients participating in the current round and ignores information from unsampled clients, which introduces non-negligible bias from client heterogeneity.

To address these challenges, we design a new algorithm named EPISODE++, which is presented in Algorithm 1. Similar to EPISODE [9], our algorithm utilizes _episodic gradient clipping_, which determines whether a clipping operation will be performed depending on the size of the average control variate \(\bm{G}_{r}\). This means that during each round, either (1) all clients perform a normalized update for all steps, or (2) all clients will perform an unnormalized update for all steps. However, different from EPISODE, our algorithm corrects local updates with control variates \(\bm{G}_{r}^{i}\) computed as the averaged stochastic gradient over the previous round in which client \(i\) participated, as in SCAFFOLD. As we will show in our proof, the episodic gradient clipping together with the update correction strategy allows the algorithm to progress in a stable manner: and it will sufficiently decrease the objective value and also avoid the negative effect of possibly exploding gradients.

### Convergence Result

The following result proves that EPISODE++ converges to an \(\epsilon\)-stationary point with high probability.

**Theorem 1**.: _Let \(\epsilon\leq\frac{AL_{0}}{16BL_{1}\rho}\) and \(\delta\in(0,1)\). Denote \(K=\left\lceil\frac{\log(RN/\delta)}{\log(N/(N-S))}\right\rceil\), \(\Gamma_{1}:=AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\) and \(\Gamma_{2}:=64\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\right) ^{2}\left(\frac{1}{3}-\frac{1}{N}\right)\). If_

\[\eta\leq\min\left\{\frac{1}{90(K+1)\Gamma_{1}I},\frac{\epsilon}{32\Gamma_{1}I \left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)},\frac{S\epsilon^{2}}{216AL_{0 }\sigma^{2}\log\frac{1}{\delta}},\frac{\Delta}{\log\frac{1}{\delta}}\min\left\{ \frac{1}{12\sigma^{2}},\frac{1}{\Gamma_{2}I}\right\}\right\},\] (2)

_and \(\gamma=\left(72\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)\eta\), then Algorithm 1 satisfies \(\frac{1}{R}\sum_{r=0}^{R-1}\|\nabla f(\tilde{\bm{x}}_{r})\|\leq 35\epsilon\) with probability at least \(1-15\delta\), as long as \(R\geq\frac{8\Delta}{\epsilon^{2}\eta}\)._

The above result holds for a wide range of \(\eta\) and \(I\), and for any noise level \(\sigma\). The corollary below summarizes the best possible iteration complexity and communication complexity implied by Theorem 1. The full proofs of Theorem 1 and Corollary 1 can be found in Appendix A.5 and A.6.

**Corollary 1**.: _Suppose \(\sigma>0\). If, under the setting of Theorem 1, we additionally choose \(\epsilon\leq\min\left\{\frac{AL_{0}}{16BL_{1}\rho},\frac{16\left(74\sigma+\frac {AL_{0}}{BL_{1}\rho}\right)}{45(K+1)},\sqrt{\frac{18\Delta AL_{0}}{S}},\frac{3 2\Delta\Gamma_{1}\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)}{\Gamma_{2} \log\frac{1}{\delta}}\right\}\), \(\eta\) as large as possible under (2), and \(I\leq\frac{27AL_{0}\sigma^{2}\log\frac{1}{\delta}}{4\epsilon\Gamma_{1}S\left(7 4\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)}\), then Algorithm 1 has iteration complexity \(RI=O\left(\frac{\Delta L_{0}\sigma^{2}\log\frac{1}{\delta}}{S\epsilon^{4}}\right)\). If additionally \(I=\Theta\left(\frac{L_{0}\sigma^{2}\log\frac{1}{\delta}}{\epsilon S(L_{0}+L_{1 }(\kappa+\rho\sigma))\left(\sigma+\frac{L_{0}}{L_{1}\rho}\right)}\right)\), then Algorithm 1 has communication complexity \(R=O\left(\frac{\Delta(L_{0}+L_{1}(\kappa+\rho\sigma))\left(\sigma+\frac{L_{0}} {L_{1}\rho}\right)}{\epsilon^{3}}\right)\)._

### Proof Sketch

In this section we provide a sketch for the proof of Theorem 1. We wish to establish the descent inequality for the global objective function in each round by applying Lemma A.3 in [51]:

\[f(\bar{\bm{x}}_{r+1})\leq f(\bar{\bm{x}}_{r})+\langle\nabla f(\bar{\bm{x}}_{r}), \bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\rangle+\frac{AL_{0}+BL_{1}\|\nabla f(\bar{ \bm{x}}_{r})\|}{2}\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\|^{2},\] (3)

where \(A=1+e^{C}-\frac{e^{C}-1}{C}\), \(B=\frac{e^{C}-1}{C}\) and \(C>0\) is an absolute constant. However, using this descent inequality requires \(\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\|\leq C/L_{1}\). Achieving a universal bound on the distance over two consecutive rounds is nontrivial in the presence of client subsampling, data heterogeneity and relaxed smoothness, which requires a new analysis.

Compared with SCAFFOLD [24] and EPISODE [9], the main difficulty of analyzing EPISODE++ lies in controlling the distance between local weights \(\bm{x}_{r,k}^{i}\) and the synchronization point \(\bar{\bm{x}}_{r}\) under relaxed smoothness and client subsampling. In EPISODE, the magnitude of \(\bm{g}_{r,k}^{i}\) can be bounded in terms of \(\|\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\|\) and \(\|\bm{G}_{r}\|\), since \(\bm{G}_{r}^{i}\) is evaluated at the synchronization point \(\bar{\bm{x}}_{r}\). Then the distance can be bounded recursively when clipping does not happen (i.e., \(\|\bm{G}_{r}\|\leq\gamma/\eta\)). However, EPISODE++ utilizes historical gradients to construct the indicator \(\bm{G}_{r}\), which means the increment also depends on the lag error \(\|\bm{G}_{r}^{i}-\nabla f_{i}(\bar{\bm{x}}_{r})\|\). Due to relaxed smoothness, we cannot bound the lag error as in SCAFFOLD [24] because we do not know the distance between \(\bar{\bm{x}}_{r}\) and where \(\bm{G}_{r}^{i}\) is evaluated. In the extreme case that client \(i\) has never been sampled before round \(r\), we have \(\bm{G}_{r}^{i}=\nabla F_{i}(\bar{\bm{x}}_{0};\bar{\xi}_{i})\), so the lag error can be unbounded when \(r\) is large.

To address the issue mentioned above, we analyze the convergence of EPISODE++ in a high-probability framework. Let \(\bm{y}_{r,k}^{i}\) denote the local model of client \(i\) at step \(k\) during the most recent round in which client \(i\) participated (before round \(r\)), and let \(q_{r}^{i}\) denote the index of this round. Since client subsampling independent across rounds, \(\mathbb{P}(q_{r}^{i}-r\geq K)=(1-S/N)^{K}\). Therefore, we introduce the event \(\mathcal{E}:=\left\{\max_{0\leq r\leq R-1,1\leq i\leq N}r-q_{r}^{i}\leq K\right\}\), where \(K\) is a logarithmic factor such that \(\mathbb{P}(\mathcal{E})\geq 1-\delta\). Under \(\mathcal{E}\), we obtain the following lemma to bound the distance of local updates.

**Lemma 1**.: _Suppose that (9) holds. Then for any \(r\geq 0\) we have_

\[\mathds{1}_{\mathcal{E}}\cdot\max_{k\in[I]}\|\bm{x}_{r,k}^{i}- \bar{\bm{x}}_{r}\| \leq 2I(2\sigma\eta+\gamma),\quad\text{for any }i\in\mathcal{S}_{r},\] (4) \[\mathds{1}_{\mathcal{E}}\cdot\max_{k\in[I]}\|\bm{y}_{r,k}^{i}- \bar{\bm{x}}_{r}\| \leq 2(K+1)I\left(2\sigma\eta+\gamma\right),\quad\text{for any }i\in[N].\] (5)

The proof of Lemma 1 is deferred to Appendix A.3, which relies on a _jump start_ analysis. If \(\|\bm{G}_{r}\|>\gamma/\eta\), then (4) trivially holds due to the clipping operation. When \(\|\bm{G}_{r}\|\leq\gamma/\eta\), a recursive argument shows that the discrepancy \(\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\|\) depends on the magnitude of the increment at the starting point, that is the lag error \(\|\mathbb{E}[\bm{G}_{r}^{i}]-\nabla f_{i}(\bar{\bm{x}}_{r})\|\) and \(\|\bm{G}_{r}\|\) after removing noise. According to the construction of \(\bm{G}_{r}^{i}\), we know \(\mathbb{E}[\bm{G}_{r}^{i}]=\frac{1}{L}\sum_{k=1}^{L}\nabla f_{i}(\bm{y}_{r,k}^ {i})=\frac{1}{l}\sum_{k=1}^{I}\nabla f_{i}(\bm{x}_{q_{r,k}^{i}}^{i})\). Therefore, under the event \(\mathcal{E}\), the lag error at the \(r\)-th round can be bounded by the discrepancies of previous \(K+1\) rounds at most, which is the bound (5). In summary, the discrepancy at any round can be controlled recursively if the discrepancy at the initial round is small. This insight motivates the initialization \(\bm{G}_{0}^{i}=\nabla F_{i}(\bar{\bm{x}}_{0},\bar{\xi}_{i})\) in Algorithm 1, which enjoys zero initial lag error.

Finally, Lemma 1 shows that the condition of (3) can be satisfied by choosing \(\eta,\gamma,I\), which establishes descent of the global objective from \(\bar{\bm{x}}_{r}\) to \(\bar{\bm{x}}_{r+1}\). Summing from \(r=0,\ldots,R-1\) and applying concentration bounds over martingale difference sequences to yield high probability bounds for error terms coming from stochastic gradient noise and client subsampling yields

\[f(\bar{\bm{x}}_{R})-f(\bar{\bm{x}}_{0})\leq\sum_{r=0}^{R-1}[\mathds{1}_{ \mathcal{A}_{r}}U(\bar{\bm{x}}_{r})+\mathds{1}_{\mathcal{A}_{r}}V(\bar{\bm{x}}_ {r})]+\left(12\sigma^{2}+\Gamma_{2}I\right)\eta\log\frac{1}{\delta},\] (6)

where \(\mathcal{A}_{r}=\{\|\bm{G}_{r}\|\leq\gamma/\eta\}\) denotes the clipping indicator, and \(U(x)\), \(V(x)\) are defined in the proof of Theorem 1. According to the choice of \(\eta,\gamma\), the dominant term of \(U(\bar{\bm{x}}_{r})\) is \(-\gamma I\|\nabla f(\bar{\bm{x}}_{r})\|\) and that of \(V(\bar{\bm{x}}_{r})\) is \(-\eta I\|\nabla f(\bar{\bm{x}}_{r})\|^{2}\). Plugging into (6) and rearranging proves Theorem 1.

Lower Bound for Clipped Minibatch SGD

Clipped minibatch SGD is a natural extension of minibatch SGD [40; 45] to the relaxed smooth setting (see pseudocode in Algorithm 2, Appendix B). Clipped minibatch SGD is nearly identical to minibatch SGD: with the addition of gradient clipping to each round's update. In the similar spirit of [45], we are interested in this algorithm because it has the same computation and communication structure as EPISODE++ and it is important to understand whether EPISODE++ has any advantage over clipped minibatch SGD. In fact, we will show that clipped minibatch SGD is significantly hindered by the combination of relaxed smoothness, client heterogeneity, and client subsampling.

**Assumption 2**.: _There exists \(M>0\) such that \(\|\nabla f(\bm{x})\|\leq M\) for all \(\bm{x}\) with \(f(\bm{x})\leq f(\bm{x}_{0})\)._

A line of work on relaxed smoothness in the single-machine setting [52; 8] has shown that the number of iterations required to find an \(\epsilon\)-stationary point by gradient descent (GD) under relaxed smoothness is \(\tilde{\Omega}\left(\frac{\Delta L_{1}M}{\epsilon^{2}}\right)\), while that of GD with gradient clipping is \(O\left(\frac{\Delta L_{0}}{\epsilon^{2}}\right)\). In this way, gradient clipping can remove the dependence on \(M\), which can be large, and significantly speed up optimization.

Theorem 2 shows a surprising result: under some conditions on the participation ratio, number of clients, and heterogeneity parameter \(\rho\), clipped minibatch SGD requires \(\tilde{\Omega}\left(\frac{\Delta L_{1}M}{\epsilon^{2}}\right)\) rounds, showing that applying gradient clipping to minibatch SGD _does not eliminate dependence on \(M\)._

**Theorem 2**.: _Fix \(\epsilon>0\), \(0<\delta<1\), \(L_{0}>0\), \(L_{1}>0\), \(\kappa>0\), \(\rho>\frac{2+\log(2-\delta)}{\log(2-\delta)}\), \(M>\max(\frac{L_{0}}{L_{1}},\epsilon)\), \(N\geq\frac{(\rho+1)(1+\log(2-\delta))}{(\rho-1)\log(2-\delta)^{-2}}\). Define \(Q=\left\lfloor\frac{\kappa+(\rho-1)M}{\kappa+(\rho+1)M}N\right\rfloor\). Let \(\mathcal{F}(L_{0},L_{1},M,\kappa,\rho,N)\) denote the set of problem instances \(\{f_{i}\}_{i=1}^{N}\) satisfying Assumptions 1(ii)-(iv) and 2 with \(\sigma=0\). For any fixed choice of parameters \(\gamma,\eta\) based on the knowledge of above constants, there exists \(\{f_{i}\}_{i=1}^{N}\in\mathcal{F}(L_{0},L_{1},M,\kappa,\rho,N)\) such that clipped minibatch SGD initialized at \(\bm{x}_{0}\) with \(1\leq S\leq\frac{\log(2-\delta)(Q+1)}{N-Q+\log(2-\delta)}\) will satisfy \(\mathbb{P}\left(\|\nabla f(\bm{x}_{r})\|<\epsilon\text{ for some }0\leq r\leq R-1 \right)>1-\delta\) only if_

\[R\geq\frac{L_{1}M\left(f(\bm{x}_{0})-f^{*}-\frac{15\epsilon^{2}}{16L_{0}} \right)}{2\epsilon^{2}\left(1+\log\frac{L_{1}M}{L_{0}}\right)}.\]

The proof is included in Appendix B. This result shows that clipped minibatch SGD in our setting suffers the same problem as GD in the single-machine setting: divergence can only be avoided with a very small step size, leading to slow convergence. In contrast, the convergence rate of EPISODE++ in the same setting is independent of \(M\).

The proof of Theorem 2 analyzes clipped minibatch SGD for three different problem instances. The first contains linear local objectives with high heterogeneity: if the clipping threshold is sufficiently small (\(\frac{\gamma}{\eta}\leq M\)), then clipped minibatch SGD will never converge with probability \(\delta\). The second instance contains homogeneous, exponential local objectives: the learning rate must be sufficiently small \(\eta<O\left(\frac{1}{L_{1}M}\right)\) to avoid divergence due to the exponentially increasing gradient magnitude. However, with a large clipping threshold and small learning rate, the convergence of clipped minibatch SGD will depend on \(M\) for the third problem instance, which has homogeneous linear objectives. Note that our lower bound is different from previous lower bounds in [52; 51] which are in the settings of single machine [52] or almost sure bounded noise [51], since our lower bound is considering noise from client subsampling and client data heterogeneity which is not almost surely bounded.

## 6 Experiments

To validate our theory, we evaluate EPISODE++ and baselines in the training of RNNs for two text classification tasks. We compare EPISODE++ to CELGC [32], clipped minibatch SGD, and NaiveParallelClip [32], which is a naive parallel implementation of SGD with gradient clipping that requires communication at every iteration. As an ablation study, we also evaluate two algorithms closely related to EPISODE++. The first is a naive extension of EPISODE [9] for client subsampling, where each participating client's control variate \(\bm{G}_{r}^{i}\) is resampled at the beginning of each round, and \(\bm{G}_{r}=\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{G}_{r}^{i}\). The second is an algorithm which we refer to as SCAFFOLDClip [9], which applies gradient clipping to each local step of SCAFFOLD [24]. We evaluate these six algorithms on natural language inference with the SNLI dataset [4] and sentiment classification with the Sentiment140 dataset [5]. More experimental results can be found in Appendix C.

### Setup

All experiments use uniform client sampling, a batch size of 64 (on each client) and the multi-class hinge loss. See Appendix C.1 for full details on hyperparameters. All experiments were implemented in PyTorch and ran on eight NVIDIA V100 GPUs.

SnliSNLI[4] is a 3-way text classification task, in which the logical relationship of a pair of sentences must be classified as either entailment, neutral, or contradictory. The dataset contains 570k pairs of sentences. Because SNLI is a centralized dataset, we follow the heterogeneity protocol in [24] to divide the dataset into clients according to a similarity parameter \(s\) between \(0\%\) and \(100\%\). According to this protocol, \(s\%\) of each client's local dataset is allocated from a randomly shuffled set of examples, while the remaining \((100-s)\%\) is allocated from a set of examples which is sorted by label. In this way, the similarity of the label distributions of client datasets grows with \(s\). The network consists of a one-layer bidirectional RNN encoder followed by a three-layer fully connected classifier. We train for \(R=5375\) communication rounds with \(I=4\) for all algorithms except NaiveParallelClip, which uses \(R=21500\) and \(I=1\), so that every algorithm runs the same number of training steps.

Sentiment140Sentiment140[5] is a sentiment prediction task designed for FL. The dataset is comprised of tweets, each labeled as either positive or negative. We follow the data processing steps of [28] to discard users with small datasets and to split into training and testing sets. In order to control the data heterogeneity between clients, we follow a similar protocol as described for SNLI to form client datasets by combining the datasets of original users in the Sentiment140 dataset. The process is nearly identical to that of SNLI, but here we allocate _users_ to each local dataset instead of _examples_. \(s\%\) of each client's local dataset is allocated from a randomly shuffled set of _users_, while the remaining \((100-s)\%\) is allocated from a set of users sorted by the proportion of positive samples. We train for \(R=2000\) communication rounds with \(I=4\) for all algorithms except NaiveParallelClip, which uses \(R=8000\) and \(I=1\). We use the same network architecture as SNLI.

Figure 1: Final training loss and testing accuracy for all algorithms, as participation ratio and data similarity varies. (a) and (b) show results for SNLI and Sentiment140, respectively.

We study the effect of client subsampling and data heterogeneity by varying each of these values in a controlled way. We first fix the heterogeneity \(s=30\%\) and vary \(S\in\{2,4,6,8\}\), then we fix the number of participating clients \(S=4\) and vary \(s\in\{10\%,30\%,50\%\}\). It should be noted that in the experiments with varying \(S\), we always train for a fixed number of iterations \(RI\). This means that separate training runs with different \(S\) will have the same per-client computation cost (number of gradient computations), but the total computation cost of a training run scales with \(S\). For this setting, we use \(N=8\) clients.

To simulate large-scale federated learning, we also include results with a larger number of clients \(N=128\). For this setting, we use \(S=16\) participating clients in each round, and data heterogeneity of \(s=30\%\) (SNLI) and \(s=10\%\) (Sentiment140).

### Results

Figure 1 contains the final training loss and testing accuracy for the variety of settings of client participation and data heterogeneity. For a single setting of participation and heterogeneity, Figure 2 and Table 2 show learning curves and average results over three trials, respectively. More learning curves are given in Appendix C.2. Learning curves for large-scale experiments are shown in Figure 3. EPISODE++ achieves the minimum training loss and maximum testing accuracy of all algorithms in nearly every setting. Only with full participation \(S=8\) does NaiveParallelClip achieve a lower training loss than EPISODE++, but EPISODE++ maintains a higher testing accuracy. Also, NaiveParallelClip requires a much larger communication cost than EPISODE++ to perform the same number of training iterations: when the number of communication rounds is fixed, EPISODE++ significantly outperforms NaiveParallelClip.

Effect of SubsamplingThe first two plots of Figures 1(a) and 1(b) show the performance of each algorithm as the number of participating clients \(S\) varies over \(\{2,4,6,8\}\) with fixed data similarity \(s=30\%\). Clipped minibatch SGD, NaiveParallelClip, and EPISODE all exhibit degraded performance as \(S\) decreases, whereas EPISODE++, SCAFFOLDClip, and CELGC maintain relatively constant performance as \(S\) decreases. Despite the constant performance of CELGC and SCAFFOLDClip under client sampling, both algorithms are significantly outperformed by EPISODE++.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & \multicolumn{2}{c}{SNLI} & \multicolumn{2}{c}{Sent140} \\ Method & Train Loss & Test Accuracy & Train Loss & Test Accuracy \\ \hline EPISODE++ & \(\mathbf{0.317\pm 0.001}\) & \(\mathbf{83.89\pm 0.07}\) & \(\mathbf{0.258\pm 0.004}\) & \(\mathbf{77.97\pm 0.16}\) \\ CELGC & \(0.529\pm 0.005\) & \(77.45\pm 0.37\) & \(0.493\pm 0.005\) & \(76.36\pm 0.18\) \\ NaiveParallelClip & \(0.377\pm 0.001\) & \(81.74\pm 0.09\) & \(0.365\pm 0.004\) & \(77.73\pm 0.07\) \\ Clipped MinibatchSGD & \(0.642\pm 0.008\) & \(72.74\pm 0.20\) & \(0.549\pm 0.004\) & \(75.10\pm 0.27\) \\ SCAFFOLDClip & \(0.424\pm 0.002\) & \(81.11\pm 0.06\) & \(0.431\pm 0.001\) & \(77.38\pm 0.16\) \\ EPISODE & \(0.455\pm 0.005\) & \(80.24\pm 0.13\) & \(0.466\pm 0.003\) & \(76.89\pm 0.17\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Average results for three trials under \(S=4,s=30\%\) (SNLI) or \(S=4,s=10\%\) (Sent140). The error is the distance from the average to the max/min across three runs.

Figure 2: Learning curves for SNLI and Sentiment140 under the setting \(S=4,s=30\%\) (SNLI) and \(S=4,s=10\%\) (Sentiment140). For NaiveParallelClip, we show the first \(5375\) (SNLI) and \(2000\) (Sentiment140) rounds to compare all algorithms with a fixed number of communication rounds.

Effect of HeterogeneityThe last two plots of each row in Figure 1 show each algorithm's performance as the data similarity \(s\) varies over \(\{50\%,30\%,10\%\}\) for SNLI and \(\{20\%,10\%,0\%\}\) for Sentiment140, with fixed \(S=4\). For both datasets, decreasing data similarity negatively impacts the performance of clipped minibatch SGD, CELGC, and NaiveParallelClip. EPISODE++, SCAFFOLDClip, and EPISODE are able to maintain performance as data similarity decreases, though EPISODE++ maintains a significantly better performance than SCAFFOLDClip and EPISODE.

Large-Scale ExperimentsWith a larger number of clients (\(N=128\), \(S=16\)), the relative performance of each algorithm is similar to the \(N=8\) setting, as shown in Figure 3. Here, the proportion of participating clients \(S/N=1/8\) is smaller than that of the \(N=8\) experiments, where \(S/N\geq 1/4\). This suggests that the effect of partial client participation may be stronger in the large-scale experiments. EPISODE++ still outperforms all other algorithms in the large-scale setting, while maintaining about the same test accuracy as the \(N=8\) setting, demonstrating the effectiveness of EPISODE++ for large-scale federated learning.

Comparison with AblationsBy comparing EPISODE++ against the closely related EPISODE and SCAFFOLDClip, we can see that the use of information from previous rounds and episodic gradient clipping are both critical for the superior performance of EPISODE++. EPISODE only utilizes information from the currently participating clients, ignoring information from clients that participated in previous rounds. As a result, the performance of EPISODE degrades as \(S\) decreases. On the other hand, SCAFFOLDClip determines whether to perform clipping individually for each local step, as opposed to the episodic gradient clipping of EPISODE++ and EPISODE. Although SCAFFOLDClip maintains performance under changes in the participation ratio and data similarity, the level it maintains is significantly lower than that of EPISODE++.

Communication CostNaiveParallelClip suffers a large communication cost for the same number of training iterations compared with other algorithms, due to the cost of synchronizing clients at every iteration. As shown in Figure 2 and Figure 3, EPISODE++ outperforms all other algorithms by a wide margin when the number of communication rounds is fixed. Also, EPISODE requires twice the number of communication operations per training round, which doubles the time required for communication per round compared to all other algorithms.

## 7 Conclusion

We have presented EPISODE++, the first algorithm for FL with heterogeneous data and client subsampling under relaxed smoothness. We proved that EPISODE++ finds an \(\epsilon\)-stationary point with high probability, and its convergence rate satisfies linear speedup and resilience to heterogeneity while enjoying reduced communication. We also presented a lower bound showing that the convergence rate of a special case of clipped minibatch SGD in our setting suffers a dependence on \(M\) (the maximum gradient norm of the objective in a sublevel set), implying that applying gradient clipping to minibatch SGD does not alleviate the problem of exploding gradients. Our experimental results for RNN training on text classification tasks demonstrate the superior performance of EPISODE++ compared to baselines. One limitation of our current work is that our lower bound assumes \(\sigma=0\), and we plan to get a better lower bound in the future for \(\sigma>0\).

Figure 3: Learning curves for large-scale training with \(N=128,S=16\), and \(s=30\%\) (SNLI) or \(s=10\%\) (Sentiment140). We compare all algorithms with a fixed number of communication rounds.

## Acknowledgments and Disclosure of Funding

We would like to thank the anonymous reviewers for their helpful comments. Michael Crawshaw is supported by the Institute for Digital Innovation fellowship from George Mason University. Michael Crawshaw and Mingrui Liu are both supported by a grant from George Mason University. Computations were run on ARGO, a research computing cluster provided by the Office of Research Computing at George Mason University (URL: https://orc.gmu.edu). The work of Yajie Bao was done when he was virtually visiting Mingrui Liu's research group in the Department of Computer Science at George Mason University.

## References

* A. Dieuleveut and K. Patel (2019)Communication trade-offs for local-sgd with large step size. Advances in Neural Information Processing Systems32, pp. 13601-13612. External Links: ISSN 1573-0221, Link, Document Cited by: SS1.
* S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning (2015)A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.. Cited by: SS1.
* S. Caldas, S. M. K. Duddu, P. Wu, T. Li, J. Konecny, H. B. McMahan, V. Smith, and A. Talwalkar (2018)Leaf: a benchmark for federated settings. arXiv preprint arXiv:1812.01097. Cited by: SS1.
* W. Chen, S. Horvath, and P. Richtarik (2020)Optimal client sampling for federated learning. arXiv preprint arXiv:2010.13723. Cited by: SS1.
* Y. J. Cho, P. Sharma, G. Joshi, Z. Xu, S. Kale, and T. Zhang (2023)On the convergence of federated averaging with cyclic client participation. arXiv preprint arXiv:2302.03109. Cited by: SS1.
* M. Crawshaw, M. Liu, F. Orabona, W. Zhang, and Z. Zhuang (2022)Robustness to unbounded smoothness of generalized signsgd. Advances in neural information processing systems. Cited by: SS1.
* M. Crawshaw, Y. Bao, and M. Liu (2023)Episode: episodic gradient clipping with periodic resampled corrections for federated learning with heterogeneous data. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* A. Cutkosky and H. Mehta (2021)High-probability bounds for non-convex stochastic optimization with heavy tails. Advances in Neural Information Processing Systems34, pp. 4883-4895. Cited by: SS1.
* A. Dieuleveut and K. K. Patel (2019)Communication trade-offs for local-sgd with large step size. Advances in Neural Information Processing Systems32, pp. 13601-13612. Cited by: SS1.
* M. Faw, L. Rout, C. Caramanis, and S. Shakkottai (2023)Beyond uniform smoothness: a stopped analysis of adaptive sgd. arXiv preprint arXiv:2302.06570. Cited by: SS1.
* Y. Fraboni, R. Vidal, L. Kameni, and M. Lorenzi (2021)Clustered sampling: low-variance and improved representativity for clients selection in federated learning. In International Conference on Machine Learning, pp. 3407-3416. Cited by: SS1.
** [14] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In _International Conference on Machine Learning_, pages 1243-1252. PMLR, 2017.
* [15] Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* [16] Saeed Ghadimi and Guanghui Lan. Accelerated gradient methods for nonconvex nonlinear and stochastic programming. _Mathematical Programming_, 156(1-2):59-99, 2016.
* [17] Margalit R Glasgow, Honglin Yuan, and Tengyu Ma. Sharp bounds for federated averaging (local sgd) and continuous perspective. In _International Conference on Artificial Intelligence and Statistics_, pages 9050-9090. PMLR, 2022.
* [18] Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local sgd with periodic averaging: Tighter analysis and adaptive synchronization. In _Advances in Neural Information Processing Systems_, pages 11080-11092, 2019.
* [19] Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Francoise Beaufays, Sean Augenstein, Hubert Eichner, Chloe Kiddon, and Daniel Ramage. Federated learning for mobile keyboard prediction. _arXiv preprint arXiv:1811.03604_, 2018.
* Volume 1_, NIPS'15, page 1594-1602, Cambridge, MA, USA, 2015. MIT Press.
* [21] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [22] Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse and quantized communication. In _Advances in Neural Information Processing Systems_, pages 2525-2536, 2018.
* [23] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. _Foundations and Trends(r) in Machine Learning_, 14(1-2):1-210, 2021.
* [24] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In _International Conference on Machine Learning_, pages 5132-5143. PMLR, 2020.
* [25] Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local sgd on identical and heterogeneous data. In _International Conference on Artificial Intelligence and Statistics_, pages 4519-4529. PMLR, 2020.
* [26] Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified theory of decentralized sgd with changing topology and local updates. In _International Conference on Machine Learning_, pages 5381-5393. PMLR, 2020.
* [27] Kfir Y Levy. Slowcal-sgd: Slow query points improve local-sgd for stochastic convex optimization. _arXiv preprint arXiv:2304.04169_, 2023.
* [28] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_, 2:429-450, 2020.
* [29] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. _arXiv preprint arXiv:1907.02189_, 2019.
* [30] Xiaoyu Li and Francesco Orabona. A high probability analysis of adaptive sgd with momentum. _arXiv preprint arXiv:2007.14294_, 2020.

* [31] Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don't use large mini-batches, use local sgd. _arXiv preprint arXiv:1808.07217_, 2018.
* [32] Mingrui Liu, Zhenxun Zhuang, Yunwen Lei, and Chunyang Liao. A communication-efficient distributed gradient clipping algorithm for training deep neural networks. _Advances in Neural Information Processing Systems_, 35:26204-26217, 2022.
* [33] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* [34] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM language models. In _International Conference on Learning Representations_, 2018.
* [35] Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Proximal and federated random reshuffling. In _International Conference on Machine Learning_, pages 15718-15749. PMLR, 2022.
* [36] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. Understanding the exploding gradient problem. corr abs/1211.5063 (2012). _arXiv preprint arXiv:1211.5063_, 2012.
* [37] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In _International conference on machine learning_, pages 1310-1318. Pmlr, 2013.
* [38] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Konecny, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _ICLR_, 2021.
* [39] Amirhossein Reisizadeh, Haochuan Li, Subhro Das, and Ali Jadbabaie. Variance-reduced clipping for non-convex optimization. _arXiv preprint arXiv:2303.00883_, 2023.
* [40] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In _2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 850-857, 2014. doi: 10.1109/ALLERTON.2014.7028543.
* [41] Sebastian U Stich. Local sgd converges fast and communicates little. _arXiv preprint arXiv:1805.09767_, 2018.
* [42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [43] Jianyu Wang and Gauri Joshi. Cooperative sgd: A unified framework for the design and analysis of communication-efficient sgd algorithms. _arXiv preprint arXiv:1808.07576_, 2018.
* [44] Shiqiang Wang and Mingyue Ji. A unified analysis of federated learning with arbitrary client participation. In _Annual Conference on Neural Information Processing Systems_, 2022.
* [45] Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In _International Conference on Machine Learning_, pages 10334-10343. PMLR, 2020.
* [46] Blake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro. The min-max complexity of distributed stochastic convex optimization with intermittent communication. _arXiv preprint arXiv:2102.01583_, 2021.
* [47] Blake E Woodworth, Kumar Kshitij Patel, and Nati Srebro. Minibatch vs local sgd for heterogeneous distributed learning. _Advances in Neural Information Processing Systems_, 33:6281-6292, 2020.
* [48] Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization. _arXiv preprint arXiv:1905.03817_, 2019.

* [49] Honglin Yuan and Tengyu Ma. Federated accelerated stochastic gradient descent. _Advances in Neural Information Processing Systems_, 33:5332-5344, 2020.
* [50] Honglin Yuan, Manzil Zaheer, and Sashank Reddi. Federated composite optimization. In _International Conference on Machine Learning_, pages 12253-12266. PMLR, 2021.
* [51] Bohang Zhang, Jikai Jin, Cong Fang, and Liwei Wang. Improved analysis of clipping algorithms for non-convex optimization. _Advances in Neural Information Processing Systems_, 33:15511-15521, 2020.
* [52] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity. In _International Conference on Learning Representations_, 2020.
* [53] Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning framework with optimal rates and adaptivity to non-iid data. _arXiv preprint arXiv:2005.11418_, 2020.

Deferred Proofs of the Upper Bound

### Notation and Preliminaries

Let \(q_{0}^{i}=0\), \(\bm{y}_{0,k}^{i}=\bar{\bm{x}}_{0}\), and define

\[q_{r}^{i}:=\begin{cases}r-1&i\in\mathcal{S}_{r-1}\\ q_{r-1}^{i}&\text{otherwise}\end{cases}\] (7)

and

\[\bm{y}_{r,k}^{i}:=\begin{cases}\bm{x}_{r-1,k}^{i}&i\in\mathcal{S}_{r-1}\\ \bm{y}_{r-1,k}^{i}&\text{otherwise}\end{cases}\] (8)

The intermediate variable \(q_{r}^{i}\) is the most recent round before (not including) round \(r\) in which client \(i\) participated. Similarly, \(\bm{y}_{r,k}^{i}\) is the local model at step \(k\) during the most recent round before (not including) round \(r\) in which client \(i\) participated. Then we know that \(\bm{y}_{r,k}^{i}=\bm{x}_{q_{r}^{i},k}^{i}\). Therefore \(\bm{G}_{r}^{i}=\frac{1}{I}\sum_{k=0}^{I-1}\nabla F_{i}(\bm{y}_{r,k}^{i};\xi_{ q_{r}^{i},k}^{i})\). Also, we will initialize \(\bm{G}_{0}^{i}=\nabla F_{i}(\bar{\bm{x}}_{0}^{i};\bar{\xi}^{i})\) for all clients.

Denote by \(\mathcal{A}_{r}=\{\|\bm{G}_{r}\|\leq\frac{\gamma}{\eta}\}\). Our analysis is simplified by taking the following conditions throughout the proof:

\[(K+1)\eta I\left(AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+ \frac{\gamma}{\eta}\right)\right) \leq\frac{1}{60}\] (9) \[2(K+1)I\left(2\sigma\eta+\gamma\right) \leq\frac{C}{L_{1}},\]

where \(C>0\) is a constant, \(A\) and \(B\) are defined in terms of \(C\) (see Lemma 2), and \(K\) is defined in Lemma 8. Usage of these conditions will be explicitly stated when used.

Through out this section, we denote the filtration

\[\mathcal{F}_{r}=\sigma\left(\{\tilde{\xi}_{i}\}_{i\in[N]},\{\xi_{q,k}^{i}:q \leq r,k\in[I]\}_{i\in[N]}\right).\]

We use \(\mathbb{E}_{r}[\cdot]=\mathbb{E}[\cdot\mid\mathcal{F}_{r}]\) and \(\mathbb{P}_{r}(\cdot)\) to denote the conditional expectation and probability given the filtration \(\mathcal{F}_{r}\). Notice that, given \(\mathcal{F}_{r}\), the global weight \(\bar{\bm{x}}_{r}\) is fixed but the subsampling set \(\mathcal{S}_{r}\) is still random and is independent of \(\mathcal{F}_{r}\).

### Auxiliary Lemmas

**Lemma 2** (Corollary A.4 in [51]).: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be \((L_{0},L_{1})\)-smooth and \(C>0\). For any \(\bm{x},\bm{y}\in\mathbb{R}^{d}\) with \(\|\bm{x}-\bm{y}\|\leq\frac{C}{L_{1}}\),_

\[\|\nabla f(\bm{x})-\nabla f(\bm{y})\|\leq(AL_{0}+BL_{1}\|\nabla f(\bm{x})\|)\| \bm{x}-\bm{y}\|,\]

_where \(A=1+e^{C}-\frac{e^{C}-1}{C}\) and \(B=\frac{e^{C}-1}{C}\)._

**Lemma 3** (Lemma A.3 in [51]).: _Let \(f:\mathbb{R}^{d}\to\mathbb{R}\) be \((L_{0},L_{1})\)-smooth and \(C>0\). For any \(\bm{x},\bm{y}\in\mathbb{R}^{d}\) with \(\|\bm{x}-\bm{y}\|\leq\frac{C}{L_{1}}\),_

\[f(\bm{y})\leq f(\bm{x})+\langle\nabla f(\bm{x}),\bm{y}-\bm{x}\rangle+\frac{AL_ {0}+BL_{1}\|\nabla f(\bm{x})\|}{2}\|\bm{x}-\bm{y}\|^{2},\]

_where \(A=1+e^{C}-\frac{e^{C}-1}{C}\) and \(B=\frac{e^{C}-1}{C}\)._

**Lemma 4** (Lemma B.1 in [51]).: _For any \(\mu\geq 0\) and \(\bm{x},\bm{y}\in\mathbb{R}^{d}\),_

\[-\left\langle\bm{x},\frac{\bm{y}}{\|\bm{y}\|}\right\rangle\leq-\mu\|\bm{x}\|-( 1-\mu)\|\bm{y}\|+(1+\mu)\|\bm{y}-\bm{x}\|.\]

**Lemma 5** (Lemma 1 in [30]).: _Assume that \(Z_{1},\ldots,Z_{T}\) is a martingale difference sequence with respect to the filtration \(\mathcal{F}_{t}\) and \(\mathbb{E}_{t}[\exp(Z_{t}^{2}/\sigma_{t}^{2})]\leq\exp(1)\) for all \(t\), where \(\sigma_{1},\ldots,\sigma_{T}\) is a sequence of random variables such that \(\sigma_{t}\in\mathcal{F}_{t}\). Then for any fixed \(\lambda>0\) and \(\delta\in(0,1)\), with probability at least \(1-\delta\),_

\[\sum_{t=1}^{T}Z_{t}\leq\frac{3}{4}\lambda\sum_{t=1}^{T}\sigma_{t}^{2}+\frac{1} {\lambda}\log\frac{1}{\delta}.\]

**Lemma 6** (Improved Serfling's inequality, Proposition 2.3 in [3]).: _Let \(\mathcal{X}=(x_{1},\ldots,x_{N})\) be a finite population of \(N\) real numbers and \(X_{1},\ldots,X_{n}\) denote a random sample without replacement from \(\mathcal{X}\). Let \(\mu_{\mathcal{X}}=\frac{1}{N}\sum_{i=1}^{N}x_{i}\), \(\sigma_{\mathcal{X}}^{2}=\frac{1}{N}\sum_{i=1}^{N}(x_{i}-\mu)^{2}\), and \(\hat{\mu}_{n}=\frac{1}{n}\sum_{i=1}^{n}X_{i}\). If \(\max_{i\in[N]}|x_{i}|\leq b\), then for any \(\lambda>0\),_

\[\mathbb{E}[\exp(\lambda(\hat{\mu}_{n}-\mu_{\mathcal{X}}))]\leq\exp\left(\frac {b^{2}\lambda^{2}}{2}\frac{n+1}{n^{2}}\left(1-\frac{n}{N}\right)\right).\]

**Lemma 7** (Lemma 12 in [10]).: _Suppose \(X_{1},\ldots,X_{T}\) is a martingale difference sequence in a Hilbert space such that \(\|X_{t}\|\leq b\) almost surely for some constant \(b\). Further, assume \(\mathbb{E}_{t-1}[\|X_{t}\|^{2}]\leq\sigma_{t}^{2}\) with probability 1 for some constants \(\sigma_{t}\). Then with probability at least \(1-3\delta\), for all \(k\):_

\[\left\|\sum_{t=1}^{k}X_{t}\right\|\leq 3b\max\left(1,\log\frac{1}{\delta} \right)+3\sqrt{\sum_{t=1}^{k}\sigma_{t}^{2}\max\left(1,\log\frac{1}{\delta} \right)}\]

### Proof of Lemma 1

Define

\[\Xi_{r}:=\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{i}\|.\]

\(\Xi_{r}\) is the average "lag" (over clients) of the correction \(\bm{G}_{r}^{i}\) behind \(\nabla f(\bar{\bm{x}}_{r})\) due to client sampling, since each \(\bm{G}_{r}^{i}\) is set according to stochastic gradients at \(\bm{y}_{r,k}^{i}\).

For any \(\delta\in(0,1)\), define

\[K=\begin{cases}\left\lceil\frac{\log\left(\frac{BN}{N}\right)}{\log\left(\frac {BN}{N-2}\right)}\right\rceil&S<N\\ 1&S=N\end{cases}\]

and

\[\mathcal{E}=\left\{\max_{0\leq r\leq R-1,1\leq i\leq N}r-q_{r}^{i}\leq K\right\}.\] (10)

The event \(\mathcal{E}\) occurs when every round in which a client is sampled occurs after no more than \(K\) rounds following the previous round in which that client was sampled. This means that under \(\mathcal{E}\), the correction \(\bm{G}_{r}^{i}\) was computed within \(K\) rounds of round \(r\). The next lemma shows that \(\mathcal{E}\) occurs with probability \(1-\delta\).

**Lemma 8**.: _For any \(\delta\) with \(0<\delta<1\), \(\mathbb{P}(\mathcal{E})\geq 1-\delta\)._

Proof.: If \(S=N\), then \(r-q_{r}^{i}=1\), and we are done. Otherwise, since each round's participating clients are sampled uniformly without replacement and independently at each round,

\[\mathbb{P}(r-q_{r}^{i}\leq k) =\sum_{i=1}^{k}\mathbb{P}(r-q_{r}^{i}=i)\] \[=\begin{cases}\sum_{i=1}^{k}\frac{S}{N}\left(1-\frac{S}{N}\right) ^{i-1}&k<r\\ 1&\text{otherwise}\end{cases}\] \[\geq\frac{S}{N}\sum_{i=1}^{k}\left(1-\frac{S}{N}\right)^{i-1}\] \[=\frac{S}{N}\frac{1-\left(1-\frac{S}{N}\right)^{k}}{1-\left(1- \frac{S}{N}\right)}\] \[=1-\left(1-\frac{S}{N}\right)^{k}.\]Also by the choice of \(K\): \(K\geq\frac{\log\left(\frac{K^{i}_{K}}{N-\delta}\right)}{\log\left(\frac{N}{N-\delta }\right)}\), we can show that \(\left(1-\frac{S}{N}\right)^{K}\leq\frac{\delta}{RN}\). So \(\mathbb{P}(r-q_{r}^{i}\leq K)\geq 1-\frac{\delta}{RN}\). Therefore

\[\mathbb{P}(\mathcal{E})=\mathbb{P}\left(r-q_{r}^{i}\leq K,\forall 0\leq r\leq R -1,1\leq i\leq N\right)\geq 1-\sum_{r=0}^{R-1}\sum_{i=1}^{N}\mathbb{P}(r-q_{r}^{i}>K) \leq 1-\delta.\]

**Lemma 9**.: _Let \(r\geq 0\). If \(\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{i}\|\leq\frac{C}{L_{1}}\) for all \(i\in[N]\) and \(k\in\{0,\ldots,I-1\}\), and \(BL_{1}\rho\Xi_{r}<1\), then_

\[\|\nabla f(\bar{\bm{x}}_{r})\|\leq\frac{\sigma+\|\bm{G}_{r}\|+(AL_{0}+BL_{1} \kappa)\Xi_{r}}{1-BL_{1}\rho\Xi_{r}}.\]

Proof.: Due to the assumption \(\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{i}\|\leq\frac{C}{L_{1}}\), we have

\[\|\nabla f(\bar{\bm{x}}_{r})\| \leq\|\nabla f(\bar{\bm{x}}_{r})-\bm{G}_{r}\|+\|\bm{G}_{r}\|\] \[\leq\frac{1}{NI}\sum_{i=1}^{N}\|\nabla f_{i}(\bar{\bm{x}}_{r})- \bm{G}_{r}^{i}\|+\|\bm{G}_{r}\|\] \[\leq\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\|\nabla f_{i}(\bar {\bm{x}}_{r})-\nabla F_{i}(\bm{y}_{r,k}^{i};\xi_{q_{r},k}^{i})\|+\|\bm{G}_{r}\|\] \[\leq\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\|\nabla f_{i}(\bar {\bm{x}}_{r})-\nabla f_{i}(\bm{y}_{r,k}^{i})\|+\frac{1}{NI}\sum_{i=1}^{N}\sum_ {k=0}^{I-1}\|\nabla f_{i}(\bm{y}_{r,k}^{i})-\nabla F_{i}(\bm{y}_{r,k}^{i};\xi_ {q_{r},k}^{i})\|+\|\bm{G}_{r}\|\] \[\overset{(i)}{\leq}\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}(AL_ {0}+BL_{1}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|)\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{ i}\|+\sigma+\|\bm{G}_{r}\|\] \[\overset{(ii)}{\leq}(AL_{0}+BL_{1}(\kappa+\rho\|\nabla f(\bar{\bm {x}}_{r})\|))\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\|\bar{\bm{x}}_{r}-\bm{y} _{r,k}^{i}\|+\sigma+\|\bm{G}_{r}\|\] \[\leq(AL_{0}+BL_{1}(\kappa+\rho\|\nabla f(\bar{\bm{x}}_{r})\|)) \Xi_{r}+\sigma+\|\bm{G}_{r}\|,\]

where we used Lemma 2 in \((i)\) and Assumption 1(iv) in \((ii)\). Rearranging to isolate \(\|\nabla f(\bar{\bm{x}}_{r})\|\) gives the result. 

**Lemma 10**.: _Suppose that Equation 9 holds, then for any \(i\in[N]\) and \(k\in[I]\),_

\[\|\bm{x}_{0,k}^{i}-\bar{\bm{x}}_{0}\|\leq 2I\left(2\sigma\eta+\gamma\right).\] (11)

Proof.: Notice that, due to the clipping operation,

\[\mathds{1}_{\bar{\mathcal{A}}_{0}}\left\|\bm{x}_{0,k}^{i}-\bar{\bm{x}}_{0} \right\|=k\gamma\leq 2k(2\sigma\eta+\gamma).\] (12)

holds for any \(k\leq I\). Next we verify Equation 11 under the event \(\mathcal{A}_{0}\). To see this, we first consider the case \(k=1\),

\[\mathds{1}_{\mathcal{A}_{0}}\|\bm{x}_{0,1}^{i}-\bar{\bm{x}}_{0}\| =\mathds{1}_{\mathcal{A}_{0}}\cdot\eta\|\nabla F_{i}(\bar{\bm{x}}_ {0};\xi_{0,0}^{i})-\bm{G}_{0}^{i}+\bm{G}_{0}\|\] \[\leq\mathds{1}_{\mathcal{A}_{0}}(\eta\|\nabla F_{i}(\bar{\bm{x}}_ {0};\xi_{0,0}^{i})-\nabla f_{i}(\bar{\bm{x}}_{0})\|+\eta\|\bm{G}_{0}^{i}-\nabla f _{i}(\bar{\bm{x}}_{0})\|+\eta\|\bm{G}_{0}\|)\] \[\leq 2\sigma\eta+\gamma,\]

where we used the initialization \(\bm{G}_{0}^{i}=\nabla F_{i}(\bar{\bm{x}}_{0};\tilde{\xi}^{i})\) and \(\|\bm{G}_{0}\|\leq\frac{\gamma}{\eta}\) under the event \(\mathcal{A}_{0}\). This indicates that Equation 11 holds with \(k=1\).

[MISSING_PAGE_EMPTY:18]

Proof.: Under \(\mathcal{A}_{r}\), Equation 13 trivially holds due to clipping. Next we verify Equation 13 under \(\mathcal{A}_{r}\). First consider the case \(k=1\).

\[\mathds{1}_{\mathcal{A}_{r}}\|\bm{x}_{r,1}^{i}-\bar{\bm{x}}_{r}\|\] \[\quad=\mathds{1}_{\mathcal{A}_{r}}\cdot\eta\|\nabla F_{i}(\bar{ \bm{x}}_{r};\xi_{r,0}^{i})-\bm{G}_{r}^{i}+\bm{G}_{r}\|\] \[\quad\leq\eta\left\|\nabla F_{i}(\bar{\bm{x}}_{r};\xi_{r,0}^{i})- \frac{1}{I}\sum_{k=0}^{I-1}\nabla F_{i}(\bm{y}_{r,k}^{i};\xi_{q_{r}^{i},k}^{i}) \right\|+\eta\mathds{1}_{\mathcal{A}_{r}}\|\bm{G}_{r}\|\] \[\quad\leq\eta\|\nabla F_{i}(\bar{\bm{x}}_{r};\xi_{r,0}^{i})- \nabla f_{i}(\bar{\bm{x}}_{r})\|+\frac{\eta}{I}\sum_{k=0}^{I-1}\|\nabla F_{i}( \bm{y}_{r,k}^{i})-\nabla F_{i}(\bm{y}_{r,k}^{i};\xi_{q_{r}^{i},k}^{i})\|+\] \[\quad\quad\frac{\eta}{I}\sum_{k=0}^{I-1}\|\nabla f_{i}(\bar{\bm{ x}}_{r})-\nabla f_{i}(\bm{y}_{r,k}^{i})\|+\eta\mathds{1}_{\mathcal{A}_{r}}\| \bm{G}_{r}\|\] \[\quad\leq 2\sigma\eta+\gamma+\frac{\eta}{I}\sum_{k=0}^{I-1}\| \nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm{y}_{r,k}^{i})\|.\] (15)

Recall that \(\bm{y}_{r,k}^{i}=\bm{x}_{q_{r}^{i},k}^{i}\) from the definitions Equation 7 and Equation 8, so

\[\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{i}\| \leq\|\bar{\bm{x}}_{r}-\bar{\bm{x}}_{q_{r}^{i}}\|+\|\bar{\bm{x}}_{ q_{r}^{i}}-\bm{x}_{q_{r}^{i},k}\|\leq\sum_{q=q_{r}^{i}}^{r-1}\|\bar{\bm{x}}_{q _{r}^{i}}-\bar{\bm{x}}_{q}\|+\|\bar{\bm{x}}_{q_{r}^{i}}-\bm{x}_{q_{r}^{i},k}^{i}\|\] \[\overset{(i)}{\leq}(r-q_{r}^{i})2I\left(2\sigma\eta+\gamma\right) +2k(2\sigma\eta+\gamma)\] \[\overset{(ii)}{\leq}2(K+1)I\left(2\sigma\eta+\gamma\right)\] \[\overset{(iii)}{\leq}\frac{C}{L_{1}},\] (16)

where \((i)\) holds due to inductive hypothesis for rounds \(q\in\{0,\ldots,r-1\}\); \((ii)\) holds due to the event \(\mathcal{E}\) such that \(r-q_{r}^{i}\leq K\); and \((iii)\) follows from Equation 9. The above bound on \(\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{i}\|\) verifies Equation 14 and shows that the condition of Lemma 2 is satisfied. Then applying it to Equation 15 yields

\[\|\bm{x}_{r,1}^{i}-\bar{\bm{x}}_{r}\|\mathds{1}_{\mathcal{A}_{r}} \leq 2\sigma\eta+\gamma+\eta\mathds{1}_{\mathcal{A}_{r}}(AL_{0}+BL _{1}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|)\frac{1}{I}\sum_{k=0}^{I-1}\|\bar{\bm{x} }_{r}-\bm{y}_{r,k}^{i}\|\] \[\leq 2\sigma\eta+\gamma+2(K+1)\eta\mathds{1}_{\mathcal{A}_{r}}I( AL_{0}+BL_{1}\kappa+BL_{1}\rho\|\nabla f(\bar{\bm{x}}_{r})\|)(2\sigma\eta+\gamma)\] \[\leq\left(1+2(K+1)\eta I(AL_{0}+BL_{1}\kappa+BL_{1}\rho\mathds{1} _{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|)\right)(2\sigma\eta+\gamma),\] (17)

where we used Equation 16 and Assumption 1 (iii) and (iv). Invoking Equation 9 and Equation 16 gives

\[BL_{1}\rho\Xi_{r}=BL_{1}\rho\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\|\bar{ \bm{x}}_{r}-\bm{y}_{r,k}^{i}\|\leq 2(K+1)BL_{1}\rho I(2\sigma\eta+\gamma)\leq \frac{1}{2}.\]

Therefore the conditions of Lemma 9 are satisfied, and we can bound \(\|\nabla f(\bar{\bm{x}}_{r})\|\) as follows:

\[\|\nabla f(\bar{\bm{x}}_{r})\|\mathds{1}_{\mathcal{A}_{r}} \leq\frac{\sigma+\frac{\gamma}{\eta}+(AL_{0}+BL_{1}\kappa)\Xi_{r }}{1-BL_{1}\rho\Xi_{r}}\] \[\leq 2\left(\sigma+\frac{\gamma}{\eta}+(AL_{0}+BL_{1}\kappa)\Xi_{r }\right)\] \[\leq 2\left(\sigma+\frac{\gamma}{\eta}+2(K+1)I(AL_{0}+BL_{1} \kappa)(2\sigma\eta+\gamma)\right)\] \[\leq 2\left(2\sigma+\frac{\gamma}{\eta}\right)(1+2(K+1)\eta I(AL_{0} +BL_{1}\kappa))\] \[\leq 4\left(2\sigma+\frac{\gamma}{\eta}\right),\] (18)where we used Equation 9. Finally, plugging back into Equation 17 yields

\[\|\bm{x}_{r,1}^{i}-\bar{\bm{x}}_{r}\|\mathds{1}_{\mathcal{A}_{r}} \leq\left(1+2(K+1)\eta I\left(AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left( 2\sigma+\frac{\gamma}{\eta}\right)\right)\right)(2\sigma\eta+\gamma)\] \[\leq 2(2\sigma\eta+\gamma),\]

where we used Equation 9 again. This proves Equation 13 for the base case \(k=1\).

Now suppose that Equation 13 holds for some \(k\) with \(1\leq k<I\). Then

\[\|\bm{x}_{r,k+1}^{i}-\bar{\bm{x}}_{r}\|\mathds{1}_{\mathcal{A}_{r}} =\mathds{1}_{\mathcal{A}_{r}}\|\bm{x}_{r,k}^{i}-\eta(\nabla F_{i} (\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\bm{G}_{r}^{i}+\bm{G}_{r})-\bar{\bm{x}}_{r}\|\] \[\leq\mathds{1}_{\mathcal{A}_{r}}(\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_ {r}\|+\eta\|\nabla F_{i}(\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\bm{G}_{r}^{i}+\bm{G} _{r}\|)\] \[\leq\mathds{1}_{\mathcal{A}_{r}}(\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_ {r}\|+\eta\|\nabla F_{i}(\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\nabla f_{i}(\bm{x}_{ r,k}^{i})\|)\] \[\qquad+\mathds{1}_{\mathcal{A}_{r}}\frac{\eta}{I}\sum_{j=0}^{I-1} \|\nabla F_{j}(\bm{y}_{r,j}^{i};\xi_{q;j}^{i})-\nabla f_{i}(\bm{y}_{r,j}^{i})\|\] \[\qquad+\mathds{1}_{\mathcal{A}_{r}}\frac{\eta}{I}\sum_{j=0}^{I-1} \|\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bm{y}_{r,j}^{i})\|+\eta\mathds{1 }_{\mathcal{A}_{r}}\|\bm{G}_{r}\|\] \[\leq\mathds{1}_{\mathcal{A}_{r}}\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_ {r}\|+2\sigma\eta+\gamma+\eta\mathds{1}_{\mathcal{A}_{r}}\|\nabla f_{i}(\bm{x} _{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\|\] \[\qquad+\mathds{1}_{\mathcal{A}_{r}}\frac{\eta}{I}\sum_{j=0}^{I-1} \|\nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm{y}_{r,j}^{i})\|,\] (19)

where we used \(\|\bm{G}_{r}\|\leq\frac{\gamma}{\eta}\) under the event \(\mathcal{A}_{r}\) and Assumption 1 (iii). We can individually bound each of the two terms \(\mathds{1}_{\mathcal{A}_{r}}\|\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}( \bar{\bm{x}}_{r})\|\) and \(\mathds{1}_{\mathcal{A}_{r}}\|\nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm {y}_{r,j}^{i})\|\). By the inductive hypothesis over \(k\), together with Equation 9: \(\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\|\leq 2I(2\sigma\eta+\gamma)\leq\frac{C}{L_{1}}\). So we can apply Lemma 2 and obtain:

\[\|\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\| \mathds{1}_{\mathcal{A}_{r}} \leq\mathds{1}_{\mathcal{A}_{r}}(AL_{0}+BL_{1}\|\nabla f_{i}( \bar{\bm{x}}_{r})\|)\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\|\] \[\leq\mathds{1}_{\mathcal{A}_{r}}(AL_{0}+BL_{1}\kappa+BL_{1}\rho \|\nabla f(\bar{\bm{x}}_{r})\|)\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\|\] \[\leq\mathds{1}_{\mathcal{A}_{r}}\left(AL_{0}+BL_{1}\kappa+4BL_{1} \rho\left(2\sigma+\frac{\gamma}{\eta}\right)\right)\|\bm{x}_{r,k}^{i}-\bar{\bm{ x}}_{r}\|\] \[\leq\frac{1}{4\eta I}\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\|,\] (20)

where we used Assumption 1 (iii) and (iv), Equation 18, and Equation 9. Similarly, by the inductive hypothesis over \(r\),

\[\|\bar{\bm{x}}_{r}-\bm{y}_{r,j}^{i}\|\leq\|\bar{\bm{x}}_{r}-\bar{\bm{x}}_{q_{r} ^{i}}\|+\|\bar{\bm{x}}_{q_{r}^{i}}-\bm{x}_{q_{r}^{i},j}^{i}\|\leq 2(K+1)I(2\sigma\eta+ \gamma)\leq\frac{C}{L_{1}}.\]

So we can apply Lemma 2:

\[\|\nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm{y}_{r,j}^{i})\| \leq(AL_{0}+BL_{1}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|)\|\bar{\bm{x} }_{r}-\bm{y}_{r,j}^{i}\|\] \[\leq\left(AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+\frac{ \gamma}{\eta}\right)\right)\|\bar{\bm{x}}_{r}-\bm{y}_{r,j}^{i}\|\] \[\leq\frac{1}{4(K+1)\eta I}\|\bar{\bm{x}}_{r}-\bm{y}_{r,j}^{i}\|\] (21)

where the last inequality follows from the assumption Equation 9.

Finally, plugging Equation 20 and Equation 21 into Equation 19 gives

\[\mathds{1}_{\mathcal{A}_{r}}\|\bm{x}_{r,k+1}^{i}-\bar{\bm{x}}_{r}\| \leq 2\sigma\eta+\gamma+\left(1+\frac{1}{4I}\right)\|\bm{x}_{r,k}^{i }-\bar{\bm{x}}_{r}\|+\frac{1}{4(K+1)I^{2}}\sum_{j=0}^{I-1}\|\bar{\bm{x}}_{r}- \bm{y}_{r,j}^{i}\|\] \[\leq 2\sigma\eta+\gamma+2k\left(1+\frac{1}{4I}\right)(2\sigma\eta +\gamma)+\frac{1}{4(K+1)I^{2}}\sum_{j=0}^{I-1}2(K+1)I(2\sigma\eta+\gamma)\] \[\leq 2\left(\frac{1}{2}+k+\frac{k}{4I}+\frac{1}{4}\right)(2 \sigma\eta+\gamma)\] \[\leq 2(k+1)(2\sigma\eta+\gamma),\]

where we used the inductive hypothesis over \(k\) and the inductive hypothesis over \(r\). This fact together with \(\mathds{1}_{\mathcal{A}_{r}}\|\bm{x}_{r,k+1}^{i}-\bar{\bm{x}}_{r}\|\leq 2(k+1)(2 \sigma\eta+\gamma)\) (which trivially holds due to the clipping operator), we prove that Equation 13 holds for \(k+1\), completing the induction over \(k\). Therefore Equation 13 holds for all \(k\in[I]\). 

**Lemma 1 restated.** Suppose that Equation 9 holds. Then for any \(r\geq 0\) we have

\[\mathds{1}_{\mathcal{E}}\max_{k\in[I]}\|\bm{x}_{r,k}^{i}-\bar{\bm {x}}_{r}\| \leq 2I(2\sigma\eta+\gamma),\quad\text{for any }i\in\mathcal{S}_{r},\] (22) \[\mathds{1}_{\mathcal{E}}\max_{k\in[I]}\|\bm{y}_{r,k}^{i}-\bar{\bm {x}}_{r}\| \leq 2(K+1)I\left(2\sigma\eta+\gamma\right),\quad\text{for any }i\in[N].\] (23)

Proof.: We will use induction to prove Equation 22 by a induction over \(r\). And Equation 23 holds as a consequence of the inductive assumption due to Lemma 11. Invoking Lemma 10, we can verify the base case of the induction over \(r\). Now we assume \(\max_{k\in[I],i\in[N]}\|\bm{x}_{q,k}^{i}-\bar{\bm{x}}_{q}\|\leq 2(K+1)I \left(2\sigma\eta+\gamma\right)\) holds for any \(q\leq r-1\). Then according to Lemma 11, we can prove Equation 22 and Equation 23 immediately. It also completes the induction of Equation 22 over \(r\). 

**Corollary 2**.: _Suppose that Equation 9 holds. Then for any \(r\geq 0\) and \(i\in[N]\),_

\[\mathds{1}_{\mathcal{E}}\Xi_{r} \leq 2(K+1)\eta I\left(\sigma+\frac{\gamma}{\eta}\right)\] (24) \[\mathds{1}_{\mathcal{E}\cap\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x }}_{r})\| \leq 4\left(2\sigma+\frac{\gamma}{\eta}\right)\] (25) \[\mathds{1}_{\mathcal{E}\cap\mathcal{A}_{r}}\|\nabla f_{i}(\bar{ \bm{x}}_{r})-\bm{G}_{r}^{i}\| \leq 2\sigma+\frac{\gamma}{30\eta}\] (26) \[\mathds{1}_{\mathcal{E}\cap\mathcal{A}_{r}}\|\nabla f(\bar{\bm{ x}}_{r})-\bm{G}_{r}\| \leq 2\sigma+\frac{\gamma}{30\eta}.\] (27)

Proof.: All proofs of this corollary are under the event \(\mathcal{E}\). Equation 24 follows directly from Equation 14 and the definition \(\Xi_{r}=\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=1}^{I}\|\bm{y}_{r,k}^{i}-\bar{\bm{x} }_{r}\|\). Equation 25 is exactly Equation 18. Due to Lemma 1, we can apply Lemma 2 such that

\[\mathds{1}_{\mathcal{E}}\|\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{ r}^{i}\| \leq\mathds{1}_{\mathcal{E}}\left(\frac{1}{I}\sum_{k=0}^{I-1}\| \nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm{y}_{r,k}^{i})\|+\frac{1}{I} \sum_{k=0}^{I-1}\|\nabla f_{i}(\bm{y}_{r,k}^{i})-\nabla F_{i}(\bm{y}_{r,k}^{i} ;\xi_{q_{i}^{i},k}^{i})\|\right)\] \[\leq\mathds{1}_{\mathcal{E}}\left(\frac{1}{I}\sum_{k=0}^{I-1}(AL_ {0}+BL_{1}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|)\|\bar{\bm{x}}_{r}-\bm{y}_{r,k}^{i }\|+\sigma\right)\] \[\leq(AL_{0}+BL_{1}\kappa+BL_{1}\rho\mathds{1}_{\mathcal{E}}\| \nabla f(\bar{\bm{x}}_{r})\|)\Xi_{r}+\sigma.\]So, using Equation 24 and Equation 25,

\[\mathds{1}_{\mathcal{E}\cap\mathcal{A}_{r}}\|\nabla f_{i}(\bar{ \boldsymbol{x}}_{r})-\boldsymbol{G}_{r}^{i}\| \leq 2(K+1)\eta I\left(2\sigma+\frac{\gamma}{\eta}\right)\left(AL_{0 }+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\right)+\sigma\] \[\leq\frac{1}{30}\left(2\sigma+\frac{\gamma}{\eta}\right)+\sigma\] \[\leq 2\sigma+\frac{\gamma}{30\eta},\]

where we used Equation 9. Equation 27 follows immediately from Equation 26 by

\[\mathds{1}_{\mathcal{E}\cap\mathcal{A}_{r}}\|\nabla f(\bar{ \boldsymbol{x}}_{r})-\boldsymbol{G}_{r}\|\leq\frac{1}{N}\sum_{i=1}^{N}\mathds{1 }_{\mathcal{E}\cap\mathcal{A}_{r}}\|\nabla f_{i}(\bar{\boldsymbol{x}}_{r})- \boldsymbol{G}_{r}^{i}\|\leq 2\sigma+\frac{\gamma}{30\eta}.\]

### Proof of descent inequality

**Lemma 12**.: _Suppose Equation 9 holds. Then with probability \(1-15\delta\), Algorithm 1 satisfies_

\[f(\bar{\boldsymbol{x}}_{R})-f(\bar{\boldsymbol{x}}_{0})\] \[\quad\leq\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\Bigg{[}- \frac{1}{8}\eta I\|\nabla f(\bar{\boldsymbol{x}}_{r})\|^{2}+\left(2\Gamma_{1} \eta^{2}I^{2}\left(2\sigma+\frac{\gamma}{\eta}\right)+\frac{216BL_{1}\sigma^{2 }\eta^{2}I\log\frac{1}{\delta}}{S}\right)\|\nabla f(\bar{\boldsymbol{x}}_{r})\|\] \[\qquad\qquad+96\Gamma_{1}^{3}\eta^{4}I^{4}\left(2\sigma+\frac{ \gamma}{\eta}\right)^{2}+\frac{108AL_{0}\sigma^{2}\eta^{2}I\log\frac{1}{\delta }}{S}\Bigg{]}\] \[\qquad\qquad-\gamma I\left(\frac{1}{8}\frac{\gamma}{\eta}-6 \sigma-2AL_{0}\gamma I-\frac{5}{2}BL_{1}\kappa\gamma I\right)\Bigg{]}\] \[\qquad+\eta\left(12\sigma^{2}+64I\left(\kappa+5\rho\left(2 \sigma+\frac{\gamma}{\eta}\right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N} \right)\right)\log\frac{1}{\delta},\]

_where \(\Gamma_{1}=AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\)._

Proof.: By Lemma 8, \(\mathbb{P}(\mathcal{E})\geq 1-\delta\). The remainder of the proof will suppose the event \(\mathcal{E}\) happens, and the results will hold with probability \(1-\delta\). By Lemma 1 and Equation 9, \(\|\bar{\boldsymbol{x}}_{r+1}-\bar{\boldsymbol{x}}_{r}\|\leq\frac{C}{L_{1}}\). Denote \(\mathcal{A}_{r}=\{\|\boldsymbol{G}_{r}\|\leq\gamma/\eta\}\). Therefore we may apply Lemma 3 to obtain

\[f(\bar{\boldsymbol{x}}_{r+1})-f(\bar{\boldsymbol{x}}_{r})\] \[\leq\langle\nabla f(\bar{\boldsymbol{x}}_{r}),\bar{\boldsymbol{x }}_{r+1}-\bar{\boldsymbol{x}}_{r}\rangle+\frac{AL_{0}+BL_{1}\|\nabla f(\bar{ \boldsymbol{x}}_{r})\|}{2}\|\bar{\boldsymbol{x}}_{r+1}-\bar{\boldsymbol{x}}_{ r}\|^{2}\] \[\leq-\mathds{1}_{\mathcal{A}_{r}}\eta\left\langle\nabla f(\bar{ \boldsymbol{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1} \boldsymbol{g}_{r,k}^{i}\right\rangle-\mathds{1}_{\mathcal{A}_{r}}\gamma \left\langle\nabla f(\bar{\boldsymbol{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S }_{r}}\sum_{k=0}^{I-1}\frac{\boldsymbol{g}_{r,k}^{i}}{\|\boldsymbol{g}_{r,k}^ {i}\|}\right\rangle+\] \[\qquad\mathds{1}_{\mathcal{A}_{r}}\frac{AL_{0}+BL_{1}\|\nabla f( \bar{\boldsymbol{x}}_{r})\|}{2}\|\bar{\boldsymbol{x}}_{r+1}-\bar{\boldsymbol{ x}}_{r}\|^{2}+\mathds{1}_{\mathcal{A}_{r}}\frac{AL_{0}+BL_{1}\|\nabla f(\bar{ \boldsymbol{x}}_{r})\|}{2}\|\bar{\boldsymbol{x}}_{r+1}-\bar{\boldsymbol{x}}_{ r}\|^{2}.\]Summing over \(r=0,\ldots,R-1\) yields

\[f(\bar{\bm{x}}_{R})-f(\bar{\bm{x}}_{0})\] \[\underbrace{-\gamma\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \left\langle\nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}} \sum_{k=0}^{I-1}\frac{\bm{g}_{r,k}^{i}}{\|\bm{g}_{r,k}^{i}\|}\right\rangle}_{ A_{2}}\] \[\quad+\underbrace{\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \frac{AL_{0}+BL_{1}\|\nabla f(\bar{\bm{x}}_{r})\|}{2}\|\bar{\bm{x}}_{r+1}-\bar{ \bm{x}}_{r}\|^{2}}_{A_{3}}\] \[\quad+\underbrace{\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_ {r}}\frac{AL_{0}+BL_{1}\|\nabla f(\bar{\bm{x}}_{r})\|}{2}\|\bar{\bm{x}}_{r+1}- \bar{\bm{x}}_{r}\|^{2}}_{A_{4}}.\] (28)

We proceed by bounding each of the four terms in Equation 28.

**Bounding \(\mathbf{A_{1}}\)**. Denote

\[\bm{\epsilon}_{r,k}^{i} =\nabla F_{i}(\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\nabla f_{i}(\bm{x} _{r,k}^{i}),\] \[\bm{\epsilon}_{r}^{(1)} =\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{G}_{r}^{i}-\bm{G}_{r},\] \[\bm{\epsilon}_{r}^{(2)} =\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\nabla f_{i}(\bar{\bm{x}}_ {r})-\nabla f(\bar{\bm{x}}_{r}).\]

Then we can write

\[\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{g}_{r,k}^{i} =\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\nabla F_{i}(\bm{x}_{r,k}^{ i};\xi_{r,k}^{i})-\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{G}_{r}^{i}+\bm{G}_{r}\] \[=\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\nabla f_{i}(\bm{x}_{r,k}^{ i})+\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{\epsilon}_{r,k}^{i}-\bm{ \epsilon}_{r}^{(1)}\] \[=\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\nabla f_{i}(\bar{\bm{x}}_ {r})+\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}(\nabla f_{i}(\bm{x}_{r,k}^{i})- \nabla f_{i}(\bar{\bm{x}}_{r}))+\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{ \epsilon}_{r,k}^{i}-\bm{\epsilon}_{r}^{(1)}\] \[=\nabla f(\bar{\bm{x}}_{r})-\bm{\epsilon}_{r}^{(1)}+\bm{\epsilon} _{r}^{(2)}+\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}(\nabla f_{i}(\bm{x}_{r,k}^{i} )-\nabla f_{i}(\bar{\bm{x}}_{r}))+\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{ \epsilon}_{r,k}^{i}.\]It follows that

\[A_{1} =-\eta\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\sum_{k=0}^{I-1} \left\langle\nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}} \bm{g}_{r,k}^{i}\right\rangle\] \[=-\eta I\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f( \bar{\bm{x}}_{r})\|^{2}-\eta\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\left \langle\nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k =0}^{I-1}\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\right\rangle\] \[\leq-\eta I\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f (\bar{\bm{x}}_{r})\|^{2}-\eta\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \left\langle\nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}} \sum_{k=0}^{I-1}\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\right\rangle\] \[\quad+\eta I\left|\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \left\langle\nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}} \bm{\epsilon}_{r,k}^{i}\right\rangle\right|+\eta I\left|\sum_{r=0}^{R-1} \mathds{1}_{\mathcal{A}_{r}}\left\langle\nabla f(\bar{\bm{x}}_{r}),\bm{ \epsilon}_{r}^{(1)}\right\rangle\right|\] \[\quad+\eta I\left|\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \left\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(2)}\right\rangle \right|.\] (29)

Since \(\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\|\leq C/L_{1}\) due to Equation 13, we can apply Lemma 2 such that

\[-\eta\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\left\langle \nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I- 1}\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\right\rangle\] \[\quad\leq\eta\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f (\bar{\bm{x}}_{r})\|\left\|\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I- 1}\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\right\|\] \[\quad\leq\eta\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f (\bar{\bm{x}}_{r})\|\cdot\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}(AL_{0}+BL_{1}\| \nabla f_{i}(\bar{\bm{x}}_{r})\|)\sum_{k=0}^{I-1}\|\bm{x}_{r,k}^{i}-\bar{\bm{x} }_{r}\|\] \[\quad\stackrel{{(i)}}{{\leq}}2\eta^{2}I^{2}\left(2 \sigma+\frac{\gamma}{\eta}\right)\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \|\nabla f(\bar{\bm{x}}_{r})\|(AL_{0}+BL_{1}\kappa+BL_{1}\rho\|\nabla f(\bar{ \bm{x}}_{r})\|)\] \[\quad\stackrel{{(iii)}}{{\leq}}2\eta^{2}I^{2}(AL_{0} +BL_{1}\kappa)\left(2\sigma+\frac{\gamma}{\eta}\right)\sum_{r=0}^{R-1}\mathds{1 }_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|+\frac{1}{8}\eta I\sum_{r=0}^ {R-1}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2},\]where \((i)\) holds due to Equation 13; \((ii)\) follows from Assumption 1(iv); and \((iii)\) follows from Equation 9. Plugging into Equation 29,

\[A_{1} \leq-\frac{7}{8}\eta I\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \|\nabla f(\bar{\bm{x}}_{r})\|^{2}+2\eta^{2}I^{2}(AL_{0}+BL_{1}\kappa)\left(2 \sigma+\frac{\gamma}{\eta}\right)\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \|\nabla f(\bar{\bm{x}}_{r})\|\] \[\quad+\underbrace{\eta I\left|\sum_{r=0}^{R-1}\mathds{1}_{ \mathcal{A}_{r}}\left\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(1)} \right\rangle\right|}_{B_{2}}\] \[\quad+\underbrace{\eta I\left|\sum_{r=0}^{R-1}\mathds{1}_{ \mathcal{A}_{r}}\left\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(2)} \right\rangle\right|}_{B_{3}}.\] (30)

Denote each of the three last terms (those involving \(\bm{\epsilon}_{r,k}^{i}\), \(\bm{\epsilon}_{r}^{(1)}\), and \(\bm{\epsilon}_{r}^{(2)}\), respectively) as \(B_{1},B_{2},B_{3}\). \(B_{1}\) can be bounded using the martingale concentration bound in Lemma 5.

Denoting \(\mathcal{G}_{r,k}=\sigma(\mathcal{F}_{r},\mathcal{S}_{r},\{\xi_{r,\ell}^{i}:i \in\mathcal{S}_{r},\ell\leq k\})\), we have

\[\mathbb{E}\left[\mathds{1}_{\mathcal{A}_{r}}\left\langle\nabla f(\bar{\bm{x}}_ {r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{\epsilon}_{r,k}^{i}\right\rangle \mid\mathcal{G}_{r,k-1}\right]=0\]

so that \(\left\{\mathds{1}_{\mathcal{A}_{r}}\left\langle\nabla f(\bar{\bm{x}}_{r}), \frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{\epsilon}_{r,k}^{i}\right\rangle \right\}_{r,k}\) is a martingale difference sequence with respect to \(\mathcal{G}_{r,k}\). Since \(\mathds{1}_{\mathcal{A}_{r}}\in\mathcal{F}_{r}\in\mathcal{G}_{r,k}\) for any \(0\leq k\leq I\), we have

\[\mathbb{E}\left[\exp\left(\frac{\mathds{1}_{\mathcal{A}_{r}}\left \langle\nabla f(\bar{\bm{x}}_{r}),\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\bm{ \epsilon}_{r,k}^{i}\right\rangle^{2}}{\mathds{1}_{\mathcal{A}_{r}}\|\nabla f (\bar{\bm{x}}_{r})\|^{2}\sigma^{2}}\right)\mid\mathcal{G}_{r,k-1}\right]\] \[\qquad\leq\mathbb{E}\left[\exp\left(\frac{\frac{1}{S}\sum_{i\in \mathcal{S}_{r}}\|\bm{\epsilon}_{r,k}^{i}\|^{2}}{\sigma^{2}}\right)\mid\mathcal{ G}_{r,k-1}\right]\] \[\qquad\leq\exp(1),\]

where we used Cauchy-Schwarz, Jensen's inequality, and \(\|\bm{\epsilon}_{r,k}^{i}\|\leq\sigma\). Therefore we can apply Lemma 5 to obtain

\[B_{1}\leq\frac{3}{4}\eta\sigma^{2}I\lambda_{1}\sum_{r=0}^{R-1}\mathds{1}_{ \mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\frac{\eta}{\lambda_{1}} \log\frac{1}{\delta},\] (31)

with probability \(1-\delta\) for any \(\lambda_{1}>0\).

The concentration bounds for \(B_{2}\) and \(B_{3}\) will rely on the concentration bound for sampling without replacement in Lemma 6. We start with \(B_{2}\). Notice that

\[\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{ r}^{(1)}\rangle=\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\mathds{1}_{\mathcal{A}_{r}} \langle\nabla f(\bar{\bm{x}}_{r}),\bm{G}_{r}^{i}\rangle-\frac{1}{N}\sum_{i=1}^ {N}\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{r}),\bm{G}_{r}^{ i}\rangle,\] (32)so we must upper bound \(\mathds{1}_{\mathcal{A}_{r}}|\langle\nabla f(\bar{\bm{x}}_{r}),\bm{G}_{r}^{i}\rangle|\) to apply Lemma 6. Under \(\mathcal{A}_{r}\),

\[\|\bm{G}_{r}^{i}\| \leq\|\nabla f_{i}(\bar{\bm{x}}_{r})\|+\|\nabla f_{i}(\bar{\bm{x}}_ {r})-\bm{G}_{r}^{i}\|\] \[\leq\kappa+\rho\|\nabla f(\bar{\bm{x}}_{r})\|+\|\nabla f_{i}(\bar {\bm{x}}_{r})-\bm{G}_{r}^{i}\|\] \[\overset{(i)}{\leq}\kappa+4\rho\left(2\sigma+\frac{\gamma}{\eta} \right)+\left(2\sigma+\frac{\gamma}{\eta}\right)\] \[\leq\kappa+(4\rho+1)\left(2\sigma+\frac{\gamma}{\eta}\right)\] \[\overset{(ii)}{\leq}\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta} \right).\] (33)

where \((i)\) holds due to Equation 25 and Equation 26; and \((ii)\) holds since \(\rho\geq 1\). Therefore

\[\mathds{1}_{\mathcal{A}_{r}}|\langle\nabla f(\bar{\bm{x}}_{r}),\bm{G}_{r}^{i }\rangle|\leq\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\right) \mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|,\]

and the condition of Lemma 6 is satisfied. Notice that \(\mathds{1}_{\mathcal{A}_{r}}\in\mathcal{F}_{r}\) because \(\bm{G}_{r}\) is constructed by the history gradients before round \(r\). Therefore, applying Lemma 6 to Equation 32 gives4

Footnote 4: Here \(\mathbb{E}_{r}[\cdot]\) takes expectation over the randomness of subsampling, that is \(\mathcal{S}_{r}\).

\[\mathbb{E}_{r}[\exp(\lambda_{2}\mathds{1}_{\mathcal{A}_{r}} \langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(1)}\rangle)]\] \[\qquad\leq\exp\left\{\frac{1}{2}\lambda_{2}^{2}\left(\kappa+5 \rho\left(2\sigma+\frac{\gamma}{\eta}\right)\right)^{2}\frac{S+1}{S^{2}}\left( 1-\frac{S}{N}\right)\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r}) \|^{2}\right\}\] \[\qquad\leq\exp\left\{\lambda_{2}^{2}\left(\kappa+5\rho\left(2 \sigma+\frac{\gamma}{\eta}\right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N} \right)\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}\right\}\] \[\qquad=:\exp\left\{\lambda_{2}^{2}M_{r}\right\}\] (34)

Let

\[Y_{r} =\exp\left\{\sum_{q=0}^{r}\lambda_{2}\left(\mathds{1}_{\mathcal{A} _{q}}\langle\nabla f(\bar{\bm{x}}_{q}),\bm{\epsilon}_{q}^{(1)}\rangle-\lambda _{2}M_{r}\right)\right\}\] \[=Y_{r-1}\exp\left\{\lambda_{2}\mathds{1}_{\mathcal{A}_{r}}\langle \nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(1)}\rangle-\lambda_{2}^{2}M_{r} \right\}.\]

Then we know \(\mathbb{E}_{r}[Y_{r}]\leq Y_{r-1}\) by Equation 34, which implies

\[\mathbb{E}[Y_{R-1}]=\mathbb{E}[\mathbb{E}_{R-1}[Y_{R-1}]]\leq\mathbb{E}[Y_{R -2}]\leq\ldots\leq\mathbb{E}[Y_{0}]\leq 1.\]

Therefore for any \(\lambda_{2}>0\), we have

\[\mathbb{P}\left(\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(1)}\rangle\geq\lambda_{2 }\sum_{r=0}^{R-1}M_{r}+\frac{1}{\lambda_{2}}\log\frac{1}{\delta}\right)\] \[\qquad=\mathbb{P}\left(\lambda_{2}\sum_{r=0}^{R-1}\mathds{1}_{ \mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(1)} \rangle\geq\lambda_{2}^{2}\sum_{r=0}^{R-1}M_{r}+\log\frac{1}{\delta}\right)\] \[\qquad=\mathbb{P}\left(\exp\left\{\sum_{r=0}^{R-1}\lambda_{2} \left(\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{r}),\bm{ \epsilon}_{r}^{(1)}\rangle-\lambda_{2}M_{r}\right)\right\}\geq\frac{1}{\delta}\right)\] \[\qquad=\mathbb{P}\left(Y_{R-1}\geq\frac{1}{\delta}\right)\] \[\qquad\leq\delta,\]

where the last line uses Markov's inequality. Repeating this argument for \(-\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{ r}^{(1)}\rangle\), we can obtain

\[\mathbb{P}\left(\left|\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f (\bar{\bm{x}}_{r}),\bm{\epsilon}_{r}^{(1)}\rangle\right|\geq\lambda_{2}\sum_{r =0}^{R-1}M_{r}+\frac{1}{\lambda_{2}}\log\frac{1}{\delta}\right)\geq 1-2\delta,\]which means that

\[B_{2}\leq\eta I\lambda_{2}\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta} \right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N}\right)\sum_{r=0}^{R-1}\mathds{1 }_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\frac{\eta I}{\lambda_{2 }}\log\frac{1}{\delta},\] (35)

holds with probability \(1-2\delta\).

The same argument can be applied to \(B_{3}\). Notice that

\[\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{r}),\bm{\epsilon}_{ r}^{(2)}\rangle=\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\mathds{1}_{\mathcal{A}_{r}} \langle\nabla f(\bar{\bm{x}}_{r}),\nabla f_{i}(\bar{\bm{x}}_{r})\rangle-\frac{ 1}{N}\sum_{i=1}^{N}\mathds{1}_{\mathcal{A}_{r}}\langle\nabla f(\bar{\bm{x}}_{ r}),\nabla f_{i}(\bar{\bm{x}}_{r})\rangle,\]

so we must upper bound \(\mathds{1}_{\mathcal{A}_{r}}|\langle\nabla f(\bar{\bm{x}}_{r}),\nabla f_{i}( \bar{\bm{x}}_{r})\rangle|\) to apply Lemma 6. Using Equation 25 in Corollary 2, we have

\[\mathds{1}_{\mathcal{A}_{r}}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|\leq\kappa+ \rho\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|\leq\kappa+4\rho \left(2\sigma+\frac{\gamma}{\eta}\right)\leq\kappa+5\rho\left(2\sigma+\frac{ \gamma}{\eta}\right).\]

This matches the corresponding upper bound in Equation 33 from the bound of \(B_{2}\). We may therefore apply an identical argument as in the case of \(B_{2}\) and obtain

\[B_{3}\leq\eta I\lambda_{2}\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta} \right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N}\right)\sum_{r=0}^{R-1}\mathds{1 }_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\frac{\eta I}{\lambda_{ 2}}\log\frac{2}{\delta},\] (36)

with probability \(1-2\delta\).

Combining Equation 31, Equation 35, and Equation 36 into Equation 30, yields that, with probability \(1-5\delta\),

\[A_{1}\leq-\eta I\left(\frac{7}{8}-\frac{3}{4}\sigma^{2}\lambda_{ 1}-2\lambda_{2}\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta}\right) \right)^{2}\left(\frac{1}{S}-\frac{1}{N}\right)\right)\sum_{r=0}^{R-1} \mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\] \[\qquad 2\eta^{2}I^{2}(AL_{0}+BL_{1}\kappa)\left(2\sigma+\frac{ \gamma}{\eta}\right)\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f( \bar{\bm{x}}_{r})\|+\eta\left(\frac{1}{\lambda_{1}}+\frac{2I}{\lambda_{2}} \right)\log\frac{1}{\delta}\] \[\leq-\frac{3}{4}\eta I\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r} }\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+2\eta^{2}I^{2}(AL_{0}+BL_{1}\kappa)\left(2 \sigma+\frac{\gamma}{\eta}\right)\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}} \|\nabla f(\bar{\bm{x}}_{r})\|+\] \[\qquad\eta\left(12\sigma^{2}+64I\left(\kappa+5\rho\left(2\sigma+ \frac{\gamma}{\eta}\right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N}\right) \right)\log\frac{1}{\delta},\] (37)

where we chose

\[\lambda_{1} =\frac{1}{12\sigma^{2}},\] \[\lambda_{2} =\frac{1}{32\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta} \right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N}\right)}.\]

**Bounding \(\mathbf{A_{2}}\).** From Lemma 4,

\[-\left\langle\nabla f(\bar{\bm{x}}_{r}),\frac{\bm{g}_{r,k}^{i}}{\|\bm{g}_{r,k}^ {i}\|}\right\rangle\leq-\mu\|\nabla f(\bar{\bm{x}}_{r})\|-(1-\mu)\|\bm{g}_{r,k}^ {i}\|+(1+\mu)\|\bm{g}_{r,k}^{i}-\nabla f(\bar{\bm{x}}_{r})\|\]

for any \(\mu\geq 0\). Also,

\[\mathds{1}_{\bar{\mathcal{A}}_{r}}\|\bm{g}_{r,k}^{i}\| =\mathds{1}_{\bar{\mathcal{A}}_{r}}\left\|\nabla F_{i}(\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\bm{G}_{r}^{i}+\bm{G}_{r}\right\|\] \[\geq\mathds{1}_{\bar{\mathcal{A}}_{r}}\left(\frac{\gamma}{\eta}- \left\|\nabla F_{i}(\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\bm{G}_{r}^{i}\right\|\right)\] \[\geq\mathds{1}_{\bar{\mathcal{A}}_{r}}\left(\frac{\gamma}{\eta}- \sigma-(AL_{0}+BL_{1}\kappa+BL_{1}\rho\|\nabla f(\bar{\bm{x}}_{r})\|)-\left\| \nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i}\right\|\right),\]and

\[\mathds{1}_{\bar{\mathcal{A}}_{r}}\|\bm{g}_{r,k}^{i}-\nabla f(\bar{ \bm{x}}_{r})\| =\mathds{1}_{\bar{\mathcal{A}}_{r}}\left\|\nabla F_{i}(\bm{x}_{r,k}^{i}; \xi_{r,k}^{i})-\bm{G}_{r}^{i}+\bm{G}_{r}-\nabla f(\bar{\bm{x}}_{r})\right\|\] \[\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\left(\left\|\nabla F_{i}( \bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\bm{G}_{r}^{i}\right\|+\|\bm{G}_{r}-\nabla f( \bar{\bm{x}}_{r})\|\right)\] \[\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\bigg{(}\left\|\nabla F_{i }(\bm{x}_{r,k}^{i};\xi_{r,k}^{i})-\nabla f_{i}(\bm{x}_{r,k}^{i})\right\|+\left\| \nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\right\|\] \[\qquad+\left\|\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i} \right\|+\frac{1}{N}\sum_{j=1}^{N}\left\|\bm{G}_{r}^{j}-\nabla F_{j}(\bar{\bm {x}}_{r})\right\|\bigg{)}\] \[\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\bigg{(}\sigma+(AL_{0}+BL_{ 1}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|)\left\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\right\|\] \[\qquad+\left\|\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i} \right\|+\frac{1}{N}\sum_{j=1}^{N}\left\|\bm{G}_{r}^{j}-\nabla F_{j}(\bar{\bm {x}}_{r})\right\|\bigg{)}\] \[\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\bigg{(}\sigma+\gamma I(AL_ {0}+BL_{1}\kappa+BL_{1}\rho\|\nabla f(\bar{\bm{x}}_{r})\|)\] \[\qquad+\left\|\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i}\right\| +\frac{1}{N}\sum_{j=1}^{N}\left\|\bm{G}_{r}^{j}-\nabla F_{j}(\bar{\bm{x}}_{r}) \right\|\bigg{)}.\]

Therefore

\[-\mathds{1}_{\bar{\mathcal{A}}_{r}}\left\langle\nabla f(\bar{\bm{ x}}_{r}),\frac{\bm{g}_{r,k}^{i}}{\|\bm{g}_{r,k}^{i}\|}\right\rangle\] \[\quad\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\bigg{(}-\mu\|\nabla f (\bar{\bm{x}}_{r})\|-(1-\mu)\frac{\gamma}{\eta}+2\sigma+2\gamma I(AL_{0}+BL_{1 }\kappa+BL_{1}\rho\|\nabla f(\bar{\bm{x}}_{r})\|)\] \[\qquad+2\|\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i}\|+\frac{1 +\mu}{N}\sum_{j=1}^{N}\|\bm{G}_{r}^{j}-\nabla F_{j}(\bar{\bm{x}}_{r})\|\bigg{)},\]

and

\[-\mathds{1}_{\bar{\mathcal{A}}_{r}}\frac{1}{\bar{\mathcal{S}}} \sum_{i\in\mathcal{S}_{r}}\left\langle\nabla f(\bar{\bm{x}}_{r}),\frac{\bm{g}_ {r,k}^{i}}{\|\bm{g}_{r,k}^{i}\|}\right\rangle\] \[\quad\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\left(-\mu\|\nabla f( \bar{\bm{x}}_{r})\|-(1-\mu)\frac{\gamma}{\eta}+2\sigma+2\gamma I(AL_{0}+BL_{1 }\kappa+BL_{1}\rho\|\nabla f(\bar{\bm{x}}_{r})\|)+(3+\mu)\left(2\sigma+\frac{ \gamma}{30\eta}\right)\right)\] \[\quad\leq\mathds{1}_{\bar{\mathcal{A}}_{r}}\left(-\left(\mu-2BL_{ 1}\rho\gamma I\right)\|\nabla f(\bar{\bm{x}}_{r})\|+\left(-\frac{9}{10}+\frac{3 1}{30}\mu\right)\frac{\gamma}{\eta}+(5+\mu)\sigma+2\gamma I(AL_{0}+BL_{1} \kappa)\right),\]

where we used Equation 26. Finally

\[A_{2} =-\gamma\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_{r}}\left\langle \nabla f(\bar{\bm{x}}_{r}),\frac{1}{\bar{S}}\sum_{i\in\mathcal{S}_{r}}\sum_{k= 0}^{I-1}\frac{\bm{g}_{r,k}^{i}}{\|\bm{g}_{r,k}^{i}\|}\right\rangle\] \[\leq\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_{r}}\bigg{[}- \gamma I\left(\mu-2BL_{1}\rho\gamma I\right)\|\nabla f(\bar{\bm{x}}_{r})\|+ \left(-\frac{9}{10}+\frac{31}{30}\mu\right)\frac{\gamma^{2}I}{\eta}+(5+\mu) \sigma\gamma I+2\gamma^{2}I^{2}(AL_{0}+BL_{1}\kappa)\bigg{]}\] \[\leq\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_{r}}\left[- \gamma I\left(\frac{3}{4}-2BL_{1}\rho\gamma I\right)\|\nabla f(\bar{\bm{x}}_{r}) \|-\gamma I\left(\frac{1}{8}\frac{\gamma}{\eta}-6\sigma-2\gamma I(AL_{0}+BL_{1} \kappa)\right)\right],\]

where we used the choice \(\mu=\frac{3}{4}\).

**Bounding \(\mathbf{A_{3}}\).** To bound \(A_{3}\), we begin by bounding \(\mathds{1}_{\mathcal{A}_{r}}\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\|^{2}\).

\[\mathds{1}_{\mathcal{A}_{r}}\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\|^ {2}\] \[\quad=\mathds{1}_{\mathcal{A}_{r}}\eta^{2}\left\|\frac{1}{S}\sum _{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\bm{g}_{r,k}^{i}\right\|^{2}\] \[\quad=\mathds{1}_{\mathcal{A}_{r}}\eta^{2}\left\|\frac{1}{S}\sum _{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\nabla F_{i}(\bm{x}_{r,k}^{i};\xi_{r,k} ^{i})-\bm{G}_{r}^{i}+\bm{G}_{r}\right\|^{2}\] \[\quad=\mathds{1}_{\mathcal{A}_{r}}\eta^{2}\left\|\frac{1}{S}\sum _{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}(\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f _{i}(\bar{\bm{x}}_{r}))+(\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i})+(\bm{G} _{r}-\nabla f(\bar{\bm{x}}_{r}))+\nabla f(\bar{\bm{x}}_{r})\right\|^{2}\] \[\quad\leq 4\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f (\bar{\bm{x}}_{r})\|^{2}+4\eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1} {S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\nabla F_{i}(\bm{x}_{r,k}^{i}; \xi_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x}}_{r})\right\|^{2}\] \[\quad\quad+4\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac {1}{S}\sum_{i\in\mathcal{S}_{r}}\nabla f_{i}(\bar{\bm{x}}_{r})-\bm{G}_{r}^{i} \right\|^{2}+4\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\bm{G}_{r}- \nabla f(\bar{\bm{x}}_{r})\right\|^{2}\] \[\quad\leq 4\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f (\bar{\bm{x}}_{r})\|^{2}+8\eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1} {S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\nabla f_{i}(\bm{x}_{r,k}^{i})- \nabla f_{i}(\bar{\bm{x}}_{r})\right\|^{2}\] \[\quad\quad+8\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac {1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\nabla f_{i}(\bar{\bm{x}}_{r} )-\nabla f_{i}(\bm{y}_{r,k}^{i})\right\|^{2}\] \[\quad\quad+8\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac {1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\nabla f_{i}(\bm{y}_{r,k}^{i})-\nabla f_{ i}(\bar{\bm{x}}_{r})\right\|^{2}\] \[\quad\quad+8\eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{S }\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\epsilon_{r,k}^{i}\right\|^{2}+8 \eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{S}\sum_{i\in\mathcal{S}_{r }}\sum_{k=0}^{I-1}\tilde{\bm{\epsilon}}_{r,k}^{i}\right\|^{2}+8\eta^{2}\mathds {1}_{\mathcal{A}_{r}}\left\|\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\tilde{ \bm{\epsilon}}_{r,k}^{i}\right\|^{2},\] (38)

where we denoted \(\tilde{\bm{\epsilon}}_{r,k}^{i}=\bm{\epsilon}_{q_{r,k}^{i}}^{i}=\nabla F_{i}( \bm{y}_{r,k}^{i};\xi_{q_{r,k}^{i}}^{i})-\nabla f_{i}(\bm{y}_{r,k}^{i})\). The first three terms on the RHS of Equation 38 can be bounded with similar arguments as those used previously:

\[\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{S}\sum_{i\in\mathcal{S }_{r}}\sum_{k=0}^{I-1}\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f_{i}(\bar{\bm{x} }_{r})\right\|^{2}\] \[\quad\leq I\mathds{1}_{\mathcal{A}_{r}}\frac{1}{S}\sum_{i\in \mathcal{S}_{r}}\sum_{k=0}^{I-1}\left\|\nabla f_{i}(\bm{x}_{r,k}^{i})-\nabla f _{i}(\bar{\bm{x}}_{r})\right\|^{2}\] \[\quad\leq I\mathds{1}_{\mathcal{A}_{r}}\frac{1}{S}\sum_{i\in \mathcal{S}_{r}}(AL_{0}+BL_{1}\|\nabla f_{i}(\bar{\bm{x}}_{r})\|)^{2}\sum_{k=0} ^{I-1}\left\|\bm{x}_{r,k}^{i}-\bar{\bm{x}}_{r}\right\|^{2}\] \[\quad\stackrel{{(i)}}{{\leq}}4\mathds{1}_{\mathcal{A }_{r}}\eta^{2}I^{4}(AL_{0}+BL_{1}\kappa+BL_{1}\rho\|\nabla f(\bar{\bm{x}}_{r})\|)^{ 2}\left(2\sigma+\frac{\gamma}{\eta}\right)^{2}\] \[\quad\leq 8\eta^{2}I^{4}B^{2}L_{1}^{2}\rho^{2}\left(2\sigma+\frac{ \gamma}{\eta}\right)^{2}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r}) \|^{2}+8\mathds{1}_{\mathcal{A}_{r}}\eta^{2}I^{4}(AL_{0}+BL_{1}\kappa)^{2} \left(2\sigma+\frac{\gamma}{\eta}\right)^{2}\] \[\quad\stackrel{{(ii)}}{{\leq}}\frac{1}{32}I^{2} \mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\mathds{1}_{ \mathcal{A}_{r}}\delta\Gamma_{1}^{2}\eta^{2}I^{4}\left(2\sigma+\frac{\gamma}{ \eta}\right)^{2},\]where \((i)\) holds due to Equation 13 and Assumption 1(iv); and \((ii)\) follows from the condition on \(\eta\) in Equation 9. Due to Lemma 1, we can use the same argument to get

\[\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{SI}\sum_{i\in\mathcal{S}_{r}}\sum_{k= 0}^{I-1}\nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm{y}^{i}_{r,k})\right\|^{ 2}\leq\frac{1}{32}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2 }+\mathds{1}_{\mathcal{A}_{r}}8\Gamma_{1}^{2}\eta^{2}I^{2}\left(2\sigma+\frac {\gamma}{\eta}\right)^{2},\]

and

\[\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{NI}\sum_{i=1}^{N}\sum_{k=0}^{I-1} \nabla f_{i}(\bar{\bm{x}}_{r})-\nabla f_{i}(\bm{y}^{i}_{r,k})\right\|^{2}\leq \frac{1}{32}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+ \mathds{1}_{\mathcal{A}_{r}}8\Gamma_{1}^{2}\eta^{2}I^{2}\left(2\sigma+\frac {\gamma}{\eta}\right)^{2}.\]

Plugging back into Equation 38,

\[\mathds{1}_{\mathcal{A}_{r}}\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r} \|^{2} \leq 5\eta^{2}I^{2}\mathds{1}_{\mathcal{A}_{r}}\|\nabla f(\bar{\bm{ x}}_{r})\|^{2}+\mathds{1}_{\mathcal{A}_{r}}192\Gamma_{1}^{2}\eta^{4}I^{4} \left(2\sigma+\frac{\gamma}{\eta}\right)^{2}\] \[\quad+8\eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{S} \sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\bm{\epsilon}^{i}_{r,k}\right\|^{2} +8\eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{S}\sum_{i\in\mathcal{S} _{r}}\sum_{k=0}^{I-1}\tilde{\bm{\epsilon}}^{i}_{r,k}\right\|^{2}\] \[\quad+8\eta^{2}\mathds{1}_{\mathcal{A}_{r}}\left\|\frac{1}{N} \sum_{i=1}^{N}\sum_{k=0}^{I-1}\tilde{\bm{\epsilon}}^{i}_{r,k}\right\|^{2}.\] (39)

We can bound the remaining terms using the concentration inequality from Lemma 7. We write the index of \(\mathcal{S}_{r}\) as \(\{i_{1},...,i_{S}\}\) For \(k=0,...,I-1\) and for \(j\in[S]\), let \(X_{kS+j}=\bm{\epsilon}^{i_{j}}_{r,k}\) and \(\mathcal{H}_{kS+j}=\sigma\left(\mathcal{F}_{r},\mathcal{S}_{r},\{\xi^{i}_{r, \ell}\}_{0\leq\ell\leq k-1,i\in\mathcal{S}_{r}},\{\xi^{i_{m}}_{r,k}\}_{m\leq j}\right)\) for \(k\geq 1\), where \(\{\xi^{i}_{r,\ell}\}_{0\leq\ell\leq-1,i\in\mathcal{S}_{r}}=\varnothing\). For any \(t=kS+j\), it holds that

\[\mathbb{E}\left[X_{t}\mid\mathcal{H}_{t-1}\right]=\mathbb{E}\left[\bm{ \epsilon}^{i_{j}}_{r,k}\mid\mathcal{H}_{kS+j-1}\right]=\mathbb{E}\left[\bm{ \epsilon}^{i_{j}}_{r,k}\mid\mathcal{F}_{r},\{\xi^{i_{j}}_{r,\ell}\}_{t\leq k-1 }\right]=0,\]

where the second inequality holds due to the independence between different clients given \(\mathcal{F}_{r}\). Then \(\{X_{t}\}_{t\leq IS}\) is a martingale difference sequence with respect to \(\{\mathcal{H}_{t}\}_{t\leq IS}\). Also, \(\|X_{t}\|\leq\sigma\) almost surely for all \(t\in[IS]\) and \(\mathbb{E}_{t-1}[\|X_{t}\|]\leq\sigma^{2}\). Therefore, by Lemma 7, with probability 5 at least \(1-\delta\),

Footnote 5: In fact, we first verify that the bound holds with probability at least \(1-\delta\) given \(\mathcal{F}_{r}\) and \(\mathcal{S}_{r}\). Since the upper bound does not depend on \(\mathcal{F}_{r}\) and \(\mathcal{S}_{r}\), we can conclude the bound holds with the same unconditional probability.

\[\left\|\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\bm{ \epsilon}^{i}_{r,k}\right\|^{2} \leq\frac{1}{S^{2}}\left(3\sigma\log\frac{1}{\delta}+3\sqrt{SI \sigma^{2}\log\frac{1}{\delta}}\right)^{2}\] \[\leq\frac{9\sigma^{2}\log^{2}\frac{1}{\delta}}{S^{2}}+\frac{9I \sigma^{2}\log\frac{1}{\delta}}{S}\] \[\leq\frac{9\sigma^{2}I\log\frac{1}{\delta}}{S}\left(\frac{\log \frac{1}{\delta}}{SI}+1\right)\] \[\leq\frac{18\sigma^{2}I\log^{2}\frac{1}{\delta}}{S}.\]

Without loss of generality, we assume \(\log(1/\delta)\geq 1\). We can apply the exact same argument to bound the remaining noise terms:

\[\left\|\frac{1}{S}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\tilde{\bm{ \epsilon}}^{i}_{r,k}\right\|^{2}\leq\frac{18\sigma^{2}I\log\frac{1}{\delta}}{S},\]

and

\[\left\|\frac{1}{N}\sum_{i=1}^{N}\sum_{k=0}^{I-1}\tilde{\bm{\epsilon}}^{i}_{r,k} \right\|^{2}\leq\frac{18\sigma^{2}I\log\frac{1}{\delta}}{N}\leq\frac{18\sigma^{2 }I\log\frac{1}{\delta}}{S},\]each with probability \(1-3\delta\). Plugging back into Equation 39,

\[\mathds{1}_{\mathcal{A}_{r}}\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\|^{2}\leq \mathds{1}_{\mathcal{A}_{r}}\left(5\eta^{2}I^{2}\|\nabla f(\bar{\bm{x}}_{r})\|^ {2}+192\Gamma_{1}^{2}\eta^{4}I^{4}\left(2\sigma+\frac{\gamma}{\eta}\right)^{2} +\frac{432\sigma^{2}\eta^{2}I\log\frac{1}{\delta}}{S}\right),\]

with probability \(1-9\delta\). Also, using Equation 25,

\[\mathds{1}_{\mathcal{A}_{r}}(AL_{0}+BL_{1}\|\nabla f(\bar{\bm{x}}_{r})\|)\leq \mathds{1}_{\mathcal{A}_{r}}\left(AL_{0}+4BL_{1}\left(2\sigma+\frac{\gamma}{ \eta}\right)\right)\leq\mathds{1}_{\mathcal{A}_{r}}\Gamma_{1}.\]

Finally,

\[A_{3} =\frac{1}{2}\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}(AL_{0}+ BL_{1}\|\nabla f(\bar{\bm{x}}_{r})\|)\|\bar{\bm{x}}_{r+1}-\bar{\bm{x}}_{r}\|^{2}\] \[\leq\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\Bigg{(}\frac{5} {2}\Gamma_{1}\eta^{2}I^{2}\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+96\Gamma_{1}^{3} \eta^{4}I^{4}\left(2\sigma+\frac{\gamma}{\eta}\right)^{2}+216(AL_{0}+BL_{1}\| \nabla f(\bar{\bm{x}}_{r})\|)\frac{\sigma^{2}\eta^{2}I\log\frac{1}{\delta}}{S} \Bigg{)}\] \[\leq\sum_{r=0}^{R-1}\mathds{1}_{\mathcal{A}_{r}}\Bigg{[}\frac{5} {8}\eta I\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\frac{216BL_{1}\sigma^{2}\eta^{2}I \log\frac{1}{\delta}}{S}\|\nabla f(\bar{\bm{x}}_{r})\|\] \[\qquad\qquad\qquad+96\Gamma_{1}^{3}\eta^{4}I^{4}\left(2\sigma+ \frac{\gamma}{\eta}\right)^{2}+\frac{216AL_{0}\sigma^{2}\eta^{2}I\log\frac{1}{ \delta}}{S}\Bigg{]},\]

with probability \(1-9\delta\).

**Bounding \(\mathbf{A_{4}}\)**: Due to normalization of the update \(\bm{g}_{r,k}^{i}\) under \(\bar{\mathcal{A}}_{r}\), we have

\[A_{4} =\gamma^{2}\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_{r}} \frac{AL_{0}+BL_{1}\|\nabla f(\bar{\bm{x}}_{r})\|}{2}\left\|\frac{1}{S}\sum_{i \in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\frac{\bm{g}_{r,k}^{i}}{\|\bm{g}_{r,k}^{i}\| }\right\|^{2}\] \[\leq\gamma^{2}I^{2}\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_ {r}}\frac{AL_{0}+BL_{1}\|\nabla f(\bar{\bm{x}}_{r})\|}{2}.\]

Combining the respective bounds for \(A_{1},A_{2},A_{3},A_{4}\) into Equation 28,

\[f(\bar{\bm{x}}_{R})-f(\bar{\bm{x}}_{0})\] \[\leq\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_{r}}\Bigg{[}- \frac{1}{8}\eta I\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\left(2\Gamma_{1}\eta^{2}I ^{2}\left(2\sigma+\frac{\gamma}{\eta}\right)+\frac{216BL_{1}\sigma^{2}\eta^{2} I\log\frac{1}{\delta}}{S}\right)\|\nabla f(\bar{\bm{x}}_{r})\|\] \[\quad+96\Gamma_{1}^{3}\eta^{4}I^{4}\left(2\sigma+\frac{\gamma}{ \eta}\right)^{2}+\frac{216AL_{0}\sigma^{2}\eta^{2}I\log\frac{1}{\delta}}{S} \Bigg{]}\] \[\quad+\sum_{r=0}^{R-1}\mathds{1}_{\bar{\mathcal{A}}_{r}}\left[- \gamma I\left(\frac{3}{4}-2BL_{1}\rho\gamma I-\frac{1}{2}BL_{1}\gamma I \right)\|\nabla f(\bar{\bm{x}}_{r})\|-\gamma I\left(\frac{1}{8}\frac{\gamma} {\eta}-6\sigma-2AL_{0}\gamma I-\frac{5}{2}BL_{1}\kappa\gamma I\right)\right]\] \[\quad+\eta\left(12\sigma^{2}+64I\left(\kappa+5\rho\left(2\sigma +\frac{\gamma}{\eta}\right)\right)^{2}\left(\frac{1}{S}-\frac{1}{N}\right) \right)\log\frac{1}{\delta},\]

which is the desired result. Note that the bound on \(A_{1}\) holds with probability \(1-5\delta\) and the bound on \(A_{2}\) holds with probability \(1-9\delta\), and we initially supposed the event \(\mathcal{E}\), which holds with probability \(1-\delta\). So the overall result holds with probability at least \(1-15\delta\). 

### Proof of Theorem 1

**Theorem 1 restated.** Let \(\epsilon\leq\frac{AL_{0}}{16BL_{1}\rho}\) and \(\delta\in(0,1)\). Denote \(K=\left\lceil\frac{\log(RN/\delta)}{\log(N/(N-S))}\right\rceil\), \(\Gamma_{1}:=AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\) and \(\Gamma_{2}:=64\left(\kappa+5\rho\left(2\sigma+\frac{\gamma}{\eta}\right)\right)^ {2}\left(\frac{1}{S}-\frac{1}{N}\right)\). If

\[\eta\leq\min\left\{\frac{1}{90(K+1)\Gamma_{1}I},\frac{\epsilon}{32\Gamma_{1}I \left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)},\frac{S\epsilon^{2}}{216AL_{0} \sigma^{2}\log\frac{1}{\delta}},\frac{\Delta}{\log\frac{1}{\delta}}\min\left\{ \frac{1}{12\sigma^{2}},\frac{1}{\Gamma_{2}I}\right\}\right\},\]and \(\gamma=\left(72\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)\eta\), then Algorithm 1 satisfies \(\frac{1}{R}\sum_{r=0}^{R-1}\|\nabla f(\bar{\bm{x}}_{r})\|\leq 35\epsilon\) with probability at least \(1-15\delta\), as long as \(R\geq\frac{8\Delta}{\epsilon^{2}\eta I}\).

Proof.: First, under our choice of \(\eta\) and \(\gamma\),

\[(K+1)\Gamma_{1}\eta I\leq\frac{1}{60},\]

and

\[2(K+1)\eta I\left(2\sigma+\frac{\gamma}{\eta}\right)\leq\frac{\left(2\sigma+ \frac{\gamma}{\eta}\right)}{30\Gamma_{1}}=\frac{\left(2\sigma+\frac{\gamma}{ \eta}\right)}{30(AL_{0}+BL_{1}\kappa+4BL_{1}\rho\left(2\sigma+\frac{\gamma}{ \eta}\right)}\leq\frac{1}{120BL_{1}\rho}\leq\frac{1}{120L_{1}},\]

where the last line follows from \(B,\rho\geq 1\). Therefore Equation 9 holds under our choice of \(\eta\) and \(\gamma\), so the condition of Lemma 12 is satisfied. Denoting

\[U(\bm{x})=-\gamma I\left(\frac{3}{4}-2BL_{1}\rho\gamma I-\frac{1}{2}BL_{1} \gamma I\right)\|\nabla f(\bm{x})\|-\gamma I\left(\frac{1}{8}\frac{\gamma}{ \eta}-6\sigma-2AL_{0}\gamma I-\frac{5}{2}BL_{1}\kappa\gamma I\right),\]

and

\[V(\bm{x})=-\frac{1}{8}\eta I\|\nabla f(\bm{x})\|^{2}+\left(2\Gamma_{1}\eta^{2} I^{2}\left(2\sigma+\frac{\gamma}{\eta}\right)+\frac{216BL_{1}\sigma^{2}\eta^{2}I \log\frac{1}{\delta}}{S}\right)\|\nabla f(\bm{x})\|+\]

\[96\Gamma_{1}^{3}\eta^{4}I^{4}\left(2\sigma+\frac{\gamma}{\eta}\right)^{2}+ \frac{216AL_{0}\sigma^{2}\eta^{2}I\log\frac{1}{\delta}}{S},\]

Lemma 12 gives us

\[f(\bar{\bm{x}}_{R})-f(\bar{\bm{x}}_{0})\leq\sum_{r=0}^{R-1}\left[\mathds{1}_{ \bar{\mathcal{A}}_{r}}U(\bar{\bm{x}}_{r})+\mathds{1}_{\mathcal{A}_{r}}V(\bar{ \bm{x}}_{r})\right]+12\eta\sigma^{2}\log\frac{1}{\delta}+\Gamma_{2}\eta I\log \frac{1}{\delta}.\] (40)

We will bound each of \(\mathds{1}_{\bar{\mathcal{A}}_{r}}U(\bar{\bm{x}}_{r})\) and \(\mathds{1}_{\mathcal{A}_{r}}V(\bar{\bm{x}}_{r})\) under our choices of \(\gamma\) and \(\eta\).

To bound \(U(\bar{\bm{x}}_{r})\), notice

\[-\frac{3}{4}+2BL_{1}\rho\gamma I+\frac{1}{2}BL_{1}\gamma I \leq-\frac{3}{4}+\frac{5}{2}BL_{1}\rho\gamma I\] \[\leq-\frac{3}{4}+\frac{5}{2}\eta IBL_{1}\rho\frac{\gamma}{\eta}\] \[=-\frac{3}{4}+\frac{5}{2}\eta IBL_{1}\rho\left(72\sigma+\frac{AL_ {0}}{BL_{1}\rho}\right)\] \[=-\frac{3}{4}+\frac{5}{2}\eta I\left(72BL_{1}\rho\sigma+AL_{0}\right)\] \[=-\frac{3}{4}+45\Gamma_{1}\eta I\] \[\leq-\frac{3}{4}+\frac{1}{2}\] \[\leq-\frac{1}{4},\]

and

\[-\frac{1}{8}\frac{\gamma}{\eta}+6\sigma+2AL_{0}\gamma I+\frac{5}{2}BL_{1}\kappa\gamma I =-\frac{1}{8}\frac{\gamma}{\eta}+6\sigma+\eta I\frac{\gamma}{ \eta}\left(2AL_{0}+\frac{5}{2}BL_{1}\kappa\right)\] \[\leq-\frac{1}{8}\frac{\gamma}{\eta}+6\sigma+2\Gamma_{1}\eta I \frac{\gamma}{\eta}\] \[\leq\left(-\frac{1}{8}+\frac{1}{30}\right)\frac{\gamma}{\eta}+6\sigma\] \[\leq-\frac{1}{12}\left(72\sigma+\frac{AL_{0}}{BL_{1}\rho}\right) +6\sigma\] \[\leq 0.\]So

\[U(\bar{\bm{x}}_{r})=-\frac{1}{4}\gamma I\|\nabla f(\bar{\bm{x}}_{r})\| \leq-\frac{1}{4}\epsilon\eta I\|\nabla f(\bar{\bm{x}}_{r})\|,\]

where the last inequality holds due to the fact \(\epsilon\leq\frac{AL_{0}}{16BL_{1}\rho}\leq 72\sigma+\frac{AL_{0}}{BL_{1}\rho}= \frac{\gamma}{\eta}\). To bound \(V(\bar{\bm{x}}_{r})\), notice

\[2\Gamma_{1}\eta^{2}I^{2}\left(2\sigma+\frac{\gamma}{\eta}\right) \leq\frac{1}{16}\epsilon\eta I,\]

since \(\eta\leq\frac{\epsilon}{32\Gamma_{1}I\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho} \right)}=\frac{\epsilon}{32\Gamma_{1}I\left(2\sigma+\frac{\gamma}{\eta}\right)}\), and

\[\frac{216BL_{1}\sigma^{2}\eta^{2}I\log\frac{1}{\delta}}{S} =\frac{BL_{1}}{AL_{0}}\eta I\frac{216AL_{0}\sigma^{2}\eta\log \frac{1}{\delta}}{S}\] \[\leq\frac{BL_{1}}{AL_{0}}\epsilon^{2}\eta I\] \[\leq\frac{1}{16}\epsilon\eta I,\]

where the last line uses \(\epsilon\leq\frac{AL_{0}}{16BL_{1}\rho}\leq\frac{AL_{0}}{16BL_{1}}\). Lastly,

\[96\Gamma_{1}^{3}\eta^{4}I^{4}\left(2\sigma+\frac{\gamma}{\eta} \right)^{2}+\frac{216AL_{0}\sigma^{2}\eta^{2}I\log\frac{1}{\delta}}{S}\] \[\leq\eta I\left(96\Gamma_{1}^{3}\eta^{3}I^{3}\left(2\sigma+\frac {\gamma}{\eta}\right)^{2}+\frac{216AL_{0}\sigma^{2}\eta\log\frac{1}{\delta}}{S }\right)\] \[\leq\eta I\left(\frac{8}{5}\epsilon^{2}+\epsilon^{2}\right)\] \[\leq 3\epsilon^{2}\eta I.\]

So

\[V(\bar{\bm{x}}_{r}) =-\frac{1}{8}\eta I\|\nabla f(\bar{\bm{x}}_{r})\|^{2}+\frac{1}{8 }\epsilon\eta I\|\nabla f(\bar{\bm{x}}_{r})\|+3\epsilon^{2}\eta I\] \[\leq-\frac{1}{8}\epsilon\eta I\|\nabla f(\bar{\bm{x}}_{r})\|+4 \epsilon^{2}\eta I,\]

where the last line came from the inequality \(x^{2}\geq 2ax+a^{2}\) with \(x=\|\nabla f(\bar{\bm{x}}_{r})\|\) and \(a=\epsilon\). Combining the bounds of \(U(\bar{\bm{x}}_{r})\) and \(V(\bar{\bm{x}}_{r})\),

\[\max\{U(\bar{\bm{x}}_{r}),V(\bar{\bm{x}}_{r})\}\leq-\frac{1}{8} \epsilon\eta I\|\nabla f(\bar{\bm{x}}_{r})\|+4\epsilon^{2}\eta I.\]

Plugging this into Equation 40 yields

\[f(\bar{\bm{x}}_{R})-f(\bar{\bm{x}}_{0})\leq-\frac{1}{8}\epsilon \eta I\sum_{r=0}^{R-1}\|\nabla f(\bar{\bm{x}}_{r})\|+4\epsilon^{2}\eta RI+12 \eta\sigma^{2}\log\frac{1}{\delta}+\Gamma_{2}\eta I\log\frac{1}{\delta},\]

and by rearranging we have

\[\frac{1}{R}\sum_{r=0}^{R-1}\|\nabla f(\bar{\bm{x}}_{r})\| \leq\frac{8(f(\bar{\bm{x}}_{0})-f(\bar{\bm{x}}_{R}))}{\epsilon \eta RI}+32\epsilon+\frac{96\sigma^{2}\log\frac{1}{\delta}}{\epsilon RI}+\frac {8\Gamma_{2}\log\frac{1}{\delta}}{\epsilon R}\] \[\leq\frac{8\Delta}{\epsilon\eta RI}+32\epsilon+\frac{96\sigma^{2} \log\frac{1}{\delta}}{\epsilon RI}+\frac{8\Gamma_{2}\log\frac{1}{\delta}}{ \epsilon R}\] \[\leq 33\epsilon+\frac{96\sigma^{2}\log\frac{1}{\delta}}{\epsilon RI }+\frac{8\Gamma_{2}\log\frac{1}{\delta}}{\epsilon R},\] (41)where the last line comes from \(R\geq\frac{8\Delta}{\epsilon^{2}\eta I}\). Finally, \(\eta\leq\frac{\Delta}{\log\frac{1}{\delta}}\min\left\{\frac{1}{12\sigma^{2}}, \frac{1}{\Gamma_{2}I}\right\}\) implies

\[R\geq\frac{8\Delta}{\epsilon^{2}\eta I}\geq\frac{96\sigma^{2}\log\frac{1}{ \delta}}{\epsilon^{2}I},\]

and

\[R\geq\frac{8\Delta}{\epsilon^{2}\eta I}\geq\frac{8\Gamma_{2}\log\frac{1}{ \delta}}{\epsilon^{2}},\]

so

\[\frac{96\sigma^{2}\log\frac{1}{\delta}}{\epsilon RI}+\frac{8\Gamma_{2}\log \frac{1}{\delta}}{\epsilon R}\leq 2\epsilon.\]

Plugging into Equation 41 gives the result. 

### Proof of Corollary 1

**Corollary 1 restated.** If, under the setting of Theorem 1, we additionally choose

\[\eta=\min\left\{\frac{1}{90(K+1)\Gamma_{1}I},\frac{\epsilon}{32\Gamma_{1}I \left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)},\frac{S\epsilon^{2}}{216AL_{ 0}\sigma^{2}\log\frac{1}{\delta}},\frac{\Delta}{\log\frac{1}{\delta}}\min \left\{\frac{1}{12\sigma^{2}},\frac{1}{\Gamma_{2}I}\right\}\right\},\]

and

\[I\leq\frac{27AL_{0}\sigma^{2}\log\frac{1}{\delta}}{4\epsilon\Gamma_{1}S\left( 74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)},\]

with

\[\epsilon\leq\min\left\{\frac{16\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right) }{45(K+1)},\sqrt{\frac{18\Delta AL_{0}}{S}},\frac{32\Delta\Gamma_{1}\left(74 \sigma+\frac{AL_{0}}{BL_{1}\rho}\right)}{\Gamma_{2}\log\frac{1}{\delta}} \right\},\]

then with probability \(1-14\delta\), Algorithm 1 will reach an \(\epsilon\)-stationary point with iteration complexity

\[RI=O\left(\frac{\Delta L_{0}\sigma^{2}\log\frac{1}{\delta}}{S\epsilon^{4}} \right).\]

Proof.: From the condition on \(\epsilon\),

\[\epsilon \leq\frac{16\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)}{45( K+1)}\] \[\frac{\epsilon}{32\Gamma_{1}I\left(74\sigma+\frac{AL_{0}}{BL_{1 }\rho}\right)} \leq\frac{1}{90(K+1)\Gamma_{1}I}.\]

From the condition on \(I\),

\[\frac{\epsilon}{32\Gamma_{1}I\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right) }\geq\frac{\epsilon}{32\Gamma_{1}\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho} \right)}\frac{4\epsilon\Gamma_{1}S\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho} \right)}{27AL_{0}\sigma^{2}\log\frac{1}{\delta}}=\frac{S\epsilon^{2}}{216AL_{ 0}\sigma^{2}\log\frac{1}{\delta}}\]

Also

\[\frac{S\epsilon^{2}}{216AL_{0}\sigma^{2}\log\frac{1}{\delta}}\leq\frac{S}{216 AL_{0}\sigma^{2}\log\frac{1}{\delta}}\frac{18\Delta AL_{0}}{S}=\frac{\Delta}{12 \sigma^{2}\log\frac{1}{\delta}},\]

and

\[\frac{S\epsilon^{2}}{216AL_{0}\sigma^{2}\log\frac{1}{\delta}} \leq\frac{S}{216AL_{0}\sigma^{2}\log\frac{1}{\delta}}\frac{32 \Delta\Gamma_{1}\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)}{\Gamma_{2} \log\frac{1}{\delta}}\frac{27AL_{0}\sigma^{2}\log\frac{1}{\delta}}{4\Gamma_{ 1}IS\left(74\sigma+\frac{AL_{0}}{BL_{1}\rho}\right)}\] \[\leq\frac{\Delta}{\Gamma_{2}I\log\frac{1}{\delta}}.\]Therefore \(\eta=\frac{S\epsilon^{2}}{216AL_{0}\sigma^{2}\log\frac{1}{2}}\). Choosing \(R=\frac{8\Delta}{\epsilon^{\epsilon}\eta I}\),

\[RI=\frac{8\Delta}{\epsilon^{2}\eta}=\frac{1296A\Delta L_{0}\sigma^{2}\log\frac{1 }{\delta}}{S\epsilon^{4}}=O\left(\frac{\Delta L_{0}\sigma^{2}\log\frac{1}{ \delta}}{S\epsilon^{4}}\right).\]

## Appendix B Deferred Proofs of the Lower Bound

```
1: Initialize \(\bm{x}_{0}\)
2:for\(r=0,1,\ldots,R-1\)do
3: Sample \(\mathcal{S}_{r}\subset[N]\) uniformly at random such that \(|\mathcal{S}_{r}|=S\)
4:\(\bm{g}_{r}=\frac{1}{SI}\sum_{i\in\mathcal{S}_{r}}\sum_{k=0}^{I-1}\nabla F_{i} (\bm{x}_{r},\xi^{i}_{r,k})\)
5: Update \(\bm{x}_{r+1}\leftarrow\bm{x}_{r}-\min\left(\eta,\frac{\gamma}{\|\bm{g}_{r}\|} \right)\bm{g}_{r}\)
6:endfor ```

**Algorithm 2** Clipped Minibatch SGD

Our proof of Theorem 2 analyzes three separate functions satisfying Assumption 1, and concludes that, in order for clipped minibatch SGD to avoid divergence on the first two functions, one must choose parameters \(\gamma,\eta\) such that convergence on the third function is slow. These three functions are separately analyzed in Lemmas 13, 15, and 16. Each lemma assumes a particular value of the initial point \(\bm{x}_{0}\): this assumption can be made without loss of generality, since for the initialization \(\bm{x}_{0}\) we may always translate each function to simulate the assumed initialization.

**Lemma 13**.: _Let \(\delta\in(0,1)\) and denote \(Q=\left\lfloor\frac{\kappa+(\rho-1)M}{\kappa+(\rho+1)M}N\right\rfloor\). Suppose_

\[\rho>\frac{2+\log(2-\delta)}{\log(2-\delta)},\quad N\geq\frac{(\rho+1)(1+\log( 2-\delta))}{(\rho-1)\log(2-\delta)-2},\quad 1\leq S\leq\frac{\log(2-\delta)(Q+1)}{N-Q+ \log(2-\delta)},\] (42)

_and \(\frac{\gamma}{\eta}\leq M\). For any \(0<\epsilon<M\), there exists a problem instance \(\left\{f_{i}\right\}_{i=1}^{N}\in\mathcal{F}(L_{0},L_{1},M,\kappa,\rho,N)\) such that, with probability at least \(\delta\), clipped minibatch SGD with parameters \(\gamma,\eta,S\) will generate iterates \(\left\{\bm{x}_{r}\right\}_{r=0}^{R}\) with \(\|\nabla f(\bm{x}_{r})\|>\epsilon\) for all \(r\)._

The proof of Lemma 13 will require the following lemma.

**Lemma 14** (Lemma 5.3 in [20]).: _Let \(\{X_{t}\}_{t=1}^{\infty}\) be a Markov chain over states \(\{i\}_{i=0}^{\infty}\), such that \(0\) is an absorbing state, and the transition distribution elsewhere is as follows:_

\[X_{t+1}|\{X_{t}=i\}=\begin{cases}i-1&\text{w.p.}\quad p\\ i+1&\text{w.p.}\quad 1-p\end{cases}\]

_Define the absorb probabilities \(\alpha_{i}:=\mathbb{P}(\exists t>0:X_{t}=0\mid X_{0}=i)\), then:_

\[\alpha_{i}=\left(\frac{p}{1-p}\right)^{i},\quad\forall i\geq 1\]

Proof of Lemma 13.: Define

\[\phi=\frac{Q}{N},\quad b=\frac{1+\phi}{1-\phi}M,\]

and

\[v=\frac{3b}{2L_{0}},\quad g(x)=-\frac{1}{8v^{3}}x^{4}+\frac{3}{4v}x^{2}.\]

Also, define

\[\ell_{1}(x)=\begin{cases}M(x+v)-Mg(v)&x\leq-v\\ -Mg(x)&x\in(-v,v)\\ -M(x-v)-Mg(v)&x\geq v\end{cases}\]\[\ell_{2}(x)=\begin{cases}-b(x+v)+bg(v)&x\leq-v\\ bg(x)&x\in(-v,v)\\ b(x-v)+bg(v)&x\geq v\end{cases}\]

Consider the problem instance defined by

\[f_{i}(x)=\begin{cases}\ell_{1}(x)&1\leq i\leq Q\\ \ell_{2}(x)&Q+1\leq i\leq N\end{cases}\]

with global objective

\[f(x)=\phi\ell_{1}(x)+(1-\phi)\ell_{2}(x)=\begin{cases}-M(x+v)+Mg(v)&x\leq-v\\ Mg(x)&x\in(-v,v)\\ M(x-v)+Mg(v)&x\geq v\end{cases}\]

Note that \(b>M\). For this problem instance, we define the stochastic objective as \(F_{i}(x;\xi)=f_{i}(x)\), so that the true gradient of each local objective is returned by each gradient query of the algorithm.

By construction, each local objective \(f_{i}\) and the global objective \(f\) is twice continuously differentiable, with gradient bounded by \(M\). Also, each objective is \((L_{0},L_{1})\)-smooth (in fact, they are \(L_{0}\)-smooth), since for \(|x|\leq v\),

\[|g^{\prime\prime}(x)| =\left|-\frac{3}{2v^{3}}x^{2}+\frac{3}{2v}\right|\] \[=\frac{3}{2v}\left|1-\frac{x^{2}}{v^{2}}\right|\] \[\leq\frac{3}{2v}=\frac{L_{0}}{b},\]

and for \(|x|\geq v\), \(|f_{i}^{\prime\prime}(x)|=0\). Finally, the collection of objectives satisfies the heterogeneity condition. For \(i\leq Q\),

\[|f_{i}^{\prime}(x)|=|f^{\prime}(x)|\leq\kappa+\rho|f^{\prime}(x)|,\]

and for \(i\geq Q+1\),

\[|f_{i}^{\prime}(x)|=b\frac{|f^{\prime}(x)|}{M}\leq\frac{1+\phi}{1-\phi}M\frac{ |f^{\prime}(x)|}{M}\leq(\kappa+\rho M)\frac{|f^{\prime}(x)|}{M}\leq\kappa+\rho |f^{\prime}(x)|,\]

where we used

\[\frac{1+\phi}{1-\phi}\leq\frac{1+\frac{\kappa+(\rho-1)M}{\kappa+(\rho+1)M}}{1- \frac{\kappa+(\rho-1)M}{\kappa+(\rho+1)M}}=\frac{2\kappa+2\rho M}{\kappa+( \rho+1)M}\frac{\kappa+(\rho+1)M}{2M}=\frac{\kappa+\rho M}{M},\]

together with the fact \(\phi\leq\frac{\kappa+(\rho-1)M}{\kappa+(\rho+1)M}\). Therefore \(\{f_{i}\}_{i=1}^{N}\in\mathcal{F}(L_{0},L_{1},M,\kappa,\rho,N)\).

Now, we analyze the behavior of clipped minibatch SGD on the instance \(\{f_{i}\}_{i=1}^{N}\) with initialization \(x_{0}=v+1\). Let \(r_{0}\) be the index of the first round (if one exists) in which \(x_{r}<v\). For each round \(r\), define the event \(B_{r}=\{\mathcal{S}_{r}\subset[Q]\}\). To compute \(\mathbb{P}(B_{r})\), first notice

\[\frac{N-Q}{Q-S+1}\leq\frac{\log(2-\delta)}{S}.\]

Therefore,

\[\mathbb{P}(B_{r})=\prod_{i=0}^{S-1}\frac{Q-i}{N-i}\geq\left(\frac{Q-S+1}{N-S+ 1}\right)^{S}=\frac{1}{\left(1+\frac{N-Q}{Q-S+1}\right)^{S}}\geq\frac{1}{ \left(1+\frac{\log(2-\delta)}{S}\right)^{S}}\geq\frac{1}{e^{\log(2-\delta)}}= \frac{1}{2-\delta},\]

where we used the inequality \(\left(1+\frac{a}{x}\right)^{x}\leq e^{a}\).

Now define the auxiliary sequence \(\left\{y_{r}\right\}_{r=0}^{R}\) as follows:

\[y_{0} =x_{0}\] \[y_{r+1} =\begin{cases}y_{r}+\gamma&\text{if }B_{r}\\ y_{r}-\gamma&\text{otherwise}\end{cases}\]

We claim that \(y_{r}\leq x_{r}\) for all \(r\leq r_{0}\). For any \(r<r_{0}\), if \(B_{r}\) occurs then \(y_{r+1}-y_{r}=x_{r+1}-x_{r}\), since in this case \(g_{r}=-M\) and \(x_{r+1}-x_{r}=\gamma\), due to the fact that \(\frac{1}{\eta}\leq M\). If \(B_{r}\) does not occur, then \(x_{r+1}-x_{r}\geq-\gamma\), due to the clipping operation, so that \(y_{r+1}-y_{r}\leq x_{r+1}-x_{r}\). Therefore for all \(r\leq r_{0}\),

\[y_{r}=y_{0}+\sum_{q=0}^{r-1}y_{q+1}-y_{q}\leq x_{0}+\sum_{q=0}^{r-1}x_{q+1}-x_{ q}=x_{r},\]

which proves our claim. Denoting \(D=\left\{|f^{\prime}(x_{r})|\geq M\text{ for all }r\geq 1\right\}\),

\[\mathbb{P}(D) \geq\mathbb{P}(x_{r}\geq-1\text{ for all }r\geq 1)\] \[\geq\mathbb{P}(y_{r}\geq-1\text{ for all }r\geq 1)\] \[\overset{(i)}{\geq}1-\left(\frac{1-\mathbb{P}(B_{r})}{\mathbb{P} (B_{r})}\right)^{\lceil\frac{x_{0}-x}{\gamma}\rceil}\] \[\overset{(ii)}{\geq}1-\frac{1-\mathbb{P}(B_{r})}{\mathbb{P}(B_{r })}\] \[=\frac{2\mathbb{P}(B_{r})-1}{\mathbb{P}(B_{r})}\] \[\geq\left(\frac{2}{2-\delta}-1\right)(2-\delta)\] \[=\delta,\]

where \((i)\) comes from applying Lemma 14 to the sequence \(y_{r}\) and \((ii)\) uses \(\lceil\frac{x_{0}-v}{\gamma}\rceil\geq 1\) and \(\frac{1-\mathbb{P}(B_{r})}{\mathbb{P}(B_{r})}\leq 1-\delta\). 

The following Lemma uses the same function as in the first half of the proof of Theorem 2 in [8]. However, the sequence of iterates computed by clipped minibatch SGD (our setting) evolves differently than that of GD (their setting).

**Lemma 15**.: _Suppose \(\frac{\gamma}{\eta}>M\), \(M>\frac{L_{0}}{L_{1}}\), and \(\eta\geq\frac{2}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)\). For any \(0<\epsilon<M\), there exists a problem instance \(\left\{f_{i}\right\}_{i=1}^{N}\in\mathcal{F}(L_{0},L_{1},M,\kappa,\rho,N)\) such that clipped minibatch SGD with parameters \(\gamma,\eta\), and any \(S\) will generate iterates \(\left\{\bm{x}_{r}\right\}_{r=0}^{R}\) with \(\|\nabla f(\bm{x}_{r})\|>\epsilon\) for all \(r\)._

Proof.: Consider the following function,

\[f(x)=\begin{cases}\frac{L_{0}}{L_{1}^{2}}\exp\left(-L_{1}x-1\right)&x<-\frac{1 }{L_{1}}\\ \frac{L_{0}}{2}x^{2}+\frac{L_{0}}{2L_{1}^{2}}&x\in\left[-\frac{1}{L_{1}},\frac {1}{L_{1}}\right]\\ \frac{L_{0}}{L_{1}^{2}}\exp\left(L_{1}x-1\right)&x>\frac{1}{L_{1}}\end{cases}\]

and the problem instance \(f_{i}=f\) for all \(i\in[N]\), with the initialization \(x_{0}=\frac{1}{L_{1}}\left(1+\log\frac{ML_{1}}{L_{0}}\right)\). We define the stochastic objective for this problem instance as \(F_{i}(x;\xi)=f_{i}(x)\), so that the true gradient of each local objective is returned by each gradient query of the algorithm.

Note that \(f\) is bounded from below and \((L_{0},L_{1})\)-smooth. Also, since all clients have the same objective and gradients are computed deterministically, this problem instance satisfies Assumption 1. Also, our setting of \(x_{0}\) is consistent with the definition of \(M\), since

\[|f^{\prime}(x_{0})| =\frac{L_{0}}{L_{1}}\exp\left(L_{1}x_{0}-1\right)\] \[=\frac{L_{0}}{L_{1}}\frac{ML_{1}}{L_{0}}=M.\]First, define \(w:=\frac{1}{L_{1}}\left(1+\log\left(\frac{\gamma}{\eta}\frac{L_{1}}{L_{0}}\right)\right)\). By the condition \(\frac{\gamma}{\eta}>M\), we know \(w>x_{0}\), and by construction \(f^{\prime}(w)=\frac{\gamma}{\eta}\). Since \(f^{\prime}\) is increasing on \([0,\infty)\), and decreasing on \((-\infty,0]\), we know \(|f^{\prime}(w)|\geq\frac{\gamma}{\eta}\) if and only if \(|x|\geq w\). This means that the clipping operation will be performed when \(|x|\geq w\), and will not be performed for \(|x|<w\).

Now we analyze the behavior of clipped minibatch SGD on this problem instance. Suppose that \(|x_{0}|\leq|x_{r}|\leq w\) for some \(r\geq 0\). Then

\[\eta\geq\frac{2L_{1}x_{0}}{L_{0}\exp(L_{1}x_{0}-1)}\geq\frac{2L_{1}|x_{r}|}{L_ {0}\exp(L_{1}|x_{r}|-1)},\]

where the first inequality comes from the definition of \(x_{0}\) and the condition \(\eta\geq\frac{2}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)\), and the second inequality comes from the fact that \(\phi(x):=\frac{2L_{1}x}{L_{0}\exp(L_{1}x-1)}\) is decreasing on \(\left[\frac{1}{L_{1}},\infty\right)\). Therefore

\[\eta|f^{\prime}(x_{r})|=\eta\frac{L_{0}}{L_{1}}\exp(L_{1}|x_{r}|-1)\geq 2|x_{r }|.\] (43)

Since \(\text{sign}(f^{\prime}(x_{r}))\neq\text{sign}(x_{r})\), this implies \(|x_{r+1}|=|x_{r}-\eta f^{\prime}(x_{r})|\geq 2|x_{r}|-|x_{r}|=|x_{r}|\). This shows that the sequence of iterates \(\{x_{r}\}\) is non-decreasing in absolute value until (if ever) the absolute value exceeds \(w\). If \(|x_{r}|\leq w\) for all \(r\), then we are done, since in this case we have \(|x_{r}|\geq|x_{0}|\) and therefore \(|\nabla f(x_{r})|\geq|\nabla f(x_{0})|=M>\epsilon\) for all \(r\).

Otherwise, let \(\bar{r}\) be the first index \(r\) for which \(|x_{r}|>w\). Without loss of generality, assume \(x_{\bar{r}}>0\) (for \(x_{\bar{r}}<0\), the same argument applies with signs reversed). Since \(|x_{\bar{r}-1}|\leq w\),

\[x_{\bar{r}}=x_{\bar{r}-1}-\eta f^{\prime}(x_{\bar{r}-1})\leq-w-\eta f^{\prime }(-w)=\gamma-w.\]

So

\[x_{\bar{r}+1}=x_{\bar{r}}-\gamma\leq(\gamma-w)-\gamma=-w.\]

Therefore \(|f^{\prime}(x_{\bar{r}+1})|\geq|f^{\prime}(-w)|=|f^{\prime}(w)|=\frac{\gamma} {\eta}\), and so \(x_{\bar{r}+2}=x_{\bar{r}+1}+\gamma=x_{\bar{r}}\). We can then show by induction that \(x_{\bar{r}+2n}=x_{\bar{r}}\) and \(x_{\bar{r}+2n+1}=x_{\bar{r}+1}\) for all \(n\geq 0\). Since \(|f^{\prime}(x_{\bar{r}})|,|f^{\prime}(x_{\bar{r}+1})|\geq|f^{\prime}(w)|>|f^{ \prime}(x_{0})|>\epsilon\), we have \(|f^{\prime}(x_{r})|>\epsilon\) for all \(r\). 

The following lemma is nearly identical to the second half of the proof of Theorem 2 in [8]. We include it here for the sake of completeness.

**Lemma 16**.: _Suppose \(\frac{\gamma}{\eta}>M\) and \(\eta<\frac{2}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)\). For any \(0<\epsilon<M\), there exists a problem instance \(\{f_{i}\}_{i=1}^{N}\in\mathcal{F}(L_{0},L_{1},M,\kappa,\rho,N)\) such that clipped minibatch SGD with parameters \(\gamma,\eta\), and any \(S\) requires at least_

\[\frac{L_{1}M\left(f(x_{0})-f^{*}-\frac{15\epsilon^{2}}{16L_{0}}\right)}{2 \epsilon^{2}\left(1+\log\frac{L_{1}M}{L_{0}}\right)}\]

_rounds in order to find an \(\epsilon\)-stationary point._

Proof.: Consider the following function,

\[f(x)=\begin{cases}-\epsilon x&x<-\frac{3\epsilon}{2L_{0}}\\ -\frac{L_{0}^{3}}{27\epsilon^{2}}x^{4}+\frac{L_{0}}{2}x^{2}+\frac{9\epsilon^{2 }}{16L_{0}}&x\in\left[-\frac{3\epsilon}{2L_{0}},\frac{3\epsilon}{2L_{0}} \right]\\ \epsilon x&x>\frac{3\epsilon}{2L_{0}}\end{cases}\]

and the problem instance \(f_{i}=f\) for all \(i\in[N]\), with the initialization \(x_{0}=\frac{3\epsilon}{2L_{0}}+d\). We define the stochastic objective for this problem instance as \(F_{i}(x,\xi)=f_{i}(x)\), so that the true gradient of each local objective is returned by each gradient query of the algorithm.

Note that \(f\) is bounded from below and \((L_{0},L_{1})\)-smooth. Also, since all clients have the same objective and gradients are computed deterministically, this problem instance satisfies Assumption 1. Also, our setting of \(x_{0}\) is consistent with the definition of \(M\), since \(|f^{\prime}(x_{0})|=\epsilon<M\).

Now we analyze the behavior of clipped minibatch SGD on this problem instance. Since this problem instance is homogeneous with deterministic gradients, we have \(\|g_{r}\|\leq\epsilon<M<\frac{\gamma}{\eta}\), so clipped minibatch SGD will always perform unnormalized updates. Therefore, for \(x_{r}\geq\frac{3\epsilon}{2L_{0}}\),

\[x_{r+1}=x_{r}-\eta g_{r}=x_{r}-\eta\epsilon>x_{r}-\frac{2\epsilon}{L_{1}M} \left(1+\log\frac{L_{1}M}{L_{0}}\right).\]

Therefore

\[x_{r}>x_{0}-\frac{2\epsilon r}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)= \frac{3\epsilon}{2L_{0}}+d-\frac{2\epsilon r}{L_{1}M}\left(1+\log\frac{L_{1}M} {L_{0}}\right)\]

as long as \(\frac{2\epsilon r}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)\leq d\), or \(r\geq\frac{dL_{1}M}{2\epsilon\left(1+\log\frac{L_{1}M}{L_{0}}\right)}\). Notice that

\[f(x_{0})=\frac{3\epsilon^{2}}{2L_{0}}+d\epsilon,\]

so

\[d=\frac{1}{\epsilon}\left(f(x_{0})-\frac{3\epsilon^{2}}{2L_{0}}\right)=\frac{ 1}{\epsilon}\left(f(x_{0})-f^{*}-\frac{15\epsilon^{2}}{16L_{0}}\right).\]

Plugging into the above bound on \(r\) tells us that \(x_{r}>x_{0}\) as long as \(r\geq\frac{L_{1}M\left(f(x_{0})-f^{*}-\frac{15\epsilon^{2}}{16L_{0}}\right)}{2 \epsilon^{2}\left(1+\log\frac{L_{1}M}{L_{0}}\right)}\). The result follows from the fact that \(|f^{\prime}(x_{r})|=\epsilon\) for all \(x_{r}>x_{0}\). 

We can now combine Lemmas 13, 15, and 16 to prove Theorem 2.

Proof of Theorem 2.: Let \(\gamma,\eta>0\) be given. We will prove the requirement on \(R\) for three cases of \(\gamma,\eta\). In the case \(\frac{\gamma}{\eta}\leq M\), then Lemma 13 demonstrates that clipped minibatch SGD cannot guarantee to find an \(\epsilon\)-stationary point with probability greater than \(1-\delta\) for any \(R\). Second, if \(\frac{\gamma}{\eta}>M\) and \(\eta\geq\frac{2}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)\), then Lemma 15 demonstrates that clipped minibatch SGD can fail to find an \(\epsilon\)-stationary point for any \(R\). In the remaining case, i.e. \(\frac{\gamma}{\eta}>M\) and \(\eta<\frac{2}{L_{1}M}\left(1+\log\frac{L_{1}M}{L_{0}}\right)\), Lemma 16 demonstrates that for finding an \(\epsilon\)-stationary point, the number of iterations clipped minibatch SGD requires is at least

\[R\geq\frac{L_{1}M\left(f(x_{0})-f^{*}-\frac{15\epsilon^{2}}{16L_{0}}\right)}{2 \epsilon^{2}\left(1+\log\frac{L_{1}M}{L_{0}}\right)}.\]

## Appendix C Additional Experimental Results

### Hyperparameter Information

Learning Rate and Clipping ThresholdFor each dataset, we tune the hyperparameters \(\gamma\) and \(\eta\) with separate grid searches. Specifically, we first tune the clipping threshold \(\frac{\gamma}{\eta}\) with grid search, then we tune the learning rate \(\eta\) with grid search. During the grid search for \(\eta\), \(\gamma\) is chosen so that the clipping threshold \(\frac{\gamma}{\eta}\) is equal to the tuned value from the search for \(\frac{\gamma}{\eta}\). Due to computational constraints, we do not perform this search for every algorithm in every setting. Instead, we follow the tuning process of [9]: we use the above procedure to tune \(\eta\) and \(\gamma\) for CELGC [32] separately for each \(S\in\{2,4,6,8\}\), while fixing \(s=30\%\) (SNLI) or \(s=10\%\) (Sent140). We then reuse the tuned values of \(\gamma\) and \(\eta\) for all other algorithms and settings sharing the same dataset and \(S\). Our theory suggests that the best learning rate \(\eta\) depends on the number of participating clients \(S\), and we follow this guidance by tuning the parameters separately for each value of \(S\). The search ranges and tuned values for each dataset are shown in Table 3.

Network ArchitectureFor both datasets, the network is composed of a one-layer bidirectional RNN encoder followed by a three-layer fully connected classifier. The encoder has hidden size 2048 and max pooling, and the decoder has hidden size 512 with tanh activations. Input sentences are encoded as sequences of GloVe vectors. For SNLI, where each input is a pair of sentences, each sentence is passed through the encoder separately, and the resulting representations are concatenated and used as input for the encoder.

### Learning Curves

Figures 4 and 5 contain learning curves (by communication rounds) of training loss and testing accuracy for all settings, for SNLI and Sentiment140 respectively. The experiments show that our proposed algorithm EPISODE++ significantly outperform all other baselines in various settings on these two datasets, including different client participation ratio and different heterogeneity level.

### Training with Homogeneous Data

Here we include learning curves for an additional experiment that uses homogeneous data across clients, i.e. \(s=0\%\). In this setting, we use \(N=8\) and \(S=4\), and all other settings are the same as described in Section 6. We compare the six algorithms described in Section 6. Results are shown in Figure 6. The results are consistent with the experiments that use heterogeneous data: EPISODE++ outperforms all other algorithms in terms of training loss and testing accuracy.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & Parameter & Search range & Tuned value \\ \hline \multirow{3}{*}{SNLI} & \(\frac{\gamma}{\eta}\) & \(\{0.01,0.03,0.1,0.3,1.0,3.0\}\) & \(S=2\): \(1.0\) \\  & & & \(S=4\): \(1.0\) \\  & & & \(S=6\): \(1.0\) \\  & & & \(S=8\): \(1.0\) \\  & \(\eta\) & \(\{0.003,0.01,0.03,0.1\}\) & \(S=2\): \(0.03\) \\  & & & \(S=4\): \(0.03\) \\  & & & \(S=6\): \(0.03\) \\  & & & \(S=8\): \(0.03\) \\ \hline \multirow{3}{*}{Sent140} & \(\frac{\gamma}{\eta}\) & \(\{0.01,0.03,0.1,0.3,1.0,3.0\}\) & \(S=2\): \(0.3\) \\  & & & \(S=4\): \(0.3\) \\  & & & \(S=6\): \(0.3\) \\  & & & \(S=8\): \(0.3\) \\  & & & \(S=2\): \(0.03\) \\  & \(\eta\) & \(\{0.003,0.01,0.03,0.1\}\) & \(S=4\): \(0.03\) \\  & & & \(S=6\): \(0.03\) \\  & & & \(S=8\): \(0.03\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Hyperparameter tuning information for each dataset.

Figure 4: Learning curves for SNLI under all settings. For NaiveParallelClip, we show the first \(5375\) rounds to compare all algorithms with a fixed number of communication rounds.

Figure 5: Learning curves for Sentiment140 under all settings. For NaiveParallelClip, we show the first \(2000\) rounds to compare all algorithms with a fixed number of communication rounds.

Figure 6: Learning curves for SNLI and Sentiment140 with \(N=8,S=4\), using homogeneous data, i.e. \(s=100\%\). We compare all algorithms with a fixed number of communication rounds.