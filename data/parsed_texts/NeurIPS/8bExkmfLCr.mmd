**Block Coordinate Descent Methods for Optimization under J-Orthogonality Constraints with Applications**

**Anonymous Author(s)**

Affiliation

Address

email

###### Abstract

The J-orthogonal matrix, also referred to as the hyperbolic orthogonal matrix, is a class of special orthogonal matrix in hyperbolic space, notable for its advantageous properties. These matrices are integral to optimization under J-orthogonal constraints, which have widespread applications in statistical learning and data science. However, addressing these problems is generally challenging due to their non-convex nature and the computational intensity of the constraints. Currently, algorithms for tackling these challenges are limited. This paper introduces **JOBCD**, a novel Block Coordinate Descent method designed to address optimizations with J-orthogonality constraints. We explore two specific variants of **JOBCD**: one based on a Gauss-Seidel strategy (**GS-JOBCD**), the other on a variance-reduced and Jacobi strategy (**VR-J-JOBCD**). Notably, leveraging the parallel framework of a Jacobi strategy, **VR-J-JOBCD** integrates variance reduction techniques to decrease oracle complexity in the minimization of finite-sum functions. For both **GS-JOBCD** and **VR-J-JOBCD**, we establish the oracle complexity under mild conditions and strong limit-point convergence results under the Kurdyka-Lojasiewicz inequality. To demonstrate the effectiveness of our method, we conduct experiments on hyperbolic eigenvalue problems, hyperbolic structural probe problems, and the ultrahyperbolic knowledge graph embedding problem. Extensive experiments using both real-world and synthetic data demonstrate that **JOBCD** consistently outperforms state-of-the-art solutions, by large margins.

## 1 Introduction

A matrix \(\mathbf{X}\in\mathbb{R}^{n\times n}\) is a J-orthogonal matrix if \(\mathbf{X}^{\mathsf{T}}\mathbf{J}\mathbf{X}=\mathbf{J}\), where \(\mathbf{J}=\left[\begin{smallmatrix}\mathbf{I}_{p}&\mathbf{0}\\ \mathbf{\rho}&-\mathbf{I}_{n-p}\end{smallmatrix}\right]\), and \(\mathbf{I}_{p}\) is a \(p\times p\) identity matrix. Here, \(\mathbf{J}\in\mathbb{R}^{n\times n}\) is the signature matrix with signature \((p,n-p)\). In this paper, we mainly focus on the following optimization problem under J-orthogonality constraints:

\[\min_{\mathbf{X}\in\mathbb{R}^{n\times n}}f(\mathbf{X})\triangleq\tfrac{1}{N} \sum_{i=1}^{N}f_{i}(\mathbf{X}),\,\mathrm{s.}\,\mathrm{t.}\,\,\mathbf{X}^{ \mathsf{T}}\mathbf{J}\mathbf{X}=\mathbf{J}.\] (1)

Here, \(f(\mathbf{X})\) could have a finite-sum structure, each component function \(f_{i}(\mathbf{X})\) is assumed to be differentiable, and \(N\) is the number of data points. For brevity, the J-orthogonality constraint \(\mathbf{X}^{\mathsf{T}}\mathbf{J}\mathbf{X}=\mathbf{J}\) in Problem (1) is rewritten as \(\mathbf{X}\in\mathcal{J}\).

We impose the following assumptions on Problem (1) throughout this paper. (\(\mathbb{A}\)-i) For any matrices \(\mathbf{X}\) and \(\mathbf{X}^{+}\), we assume \(f_{i}:\mathbb{R}^{n\times n}\mapsto\mathbb{R}\) is continuously differentiable for some symmetric positive semidefinite matrix \(\mathbf{H}\in\mathbb{R}^{nn\times nn}\) that:

\[f_{i}(\mathbf{X}^{+})\leq f_{i}(\mathbf{X})+\langle\mathbf{X}^{+}-\mathbf{X},\nabla f_{i}(\mathbf{X})\rangle+\tfrac{1}{2}\|\mathbf{X}^{+}-\mathbf{X}\|_{ \mathbf{H}}^{2},\] (2)

for all \(i\in[N]\), where \(\|\mathbf{H}\|\leq L_{f}\) for some constant \(L_{f}>0\) and \(\|\mathbf{X}\|_{\mathbf{H}}^{2}\triangleq\mathrm{vec}(\mathbf{X})^{\top} \mathbf{H}\,\mathrm{vec}(\mathbf{X})\). This further implies that: \(\|\nabla f_{i}(\mathbf{X})-\nabla f_{i}(\mathbf{X}^{+})\|_{\mathbf{f}}\leq L _{f}\|\mathbf{X}-\mathbf{X}^{+}\|_{\mathbf{f}}\) for all \(i\in[N]\). Importantly, the function \(f(\mathbf{X})=\tfrac{1}{2}\operatorname{tr}(\mathbf{X}^{\top}\mathbf{C} \mathbf{X}\mathbf{D})=\tfrac{1}{2}\|\mathbf{X}\|_{\mathbf{H}}^{2}\) with \(\mathbf{H}=\mathbf{D}\otimes\mathbf{C}\) satisfies the equality\(\forall\mathbf{X},\mathbf{X}^{+},f(\mathbf{X}^{+})=\mathcal{Q}(\mathbf{X}^{+}; \mathbf{X})\) in (2), where \(\mathbf{C}\in\mathbb{R}^{n\times n}\) and \(\mathbf{D}\in\mathbb{R}^{n\times n}\) are arbitrary symmetric matrices. (A-ii) The function \(f_{i}(\mathbf{X})\) is coercive for all \(i\in N\), that is, \(\lim_{\|\mathbf{X}\|_{\ell}\to\infty}f_{i}(\mathbf{X})=\infty,\,\forall i\).

Problem (1) defines an optimization framework that is fundamental to a wide range of models in statistical learning and data science, including hyperbolic eigenvalue problem [6; 43; 40], hyperbolic structural probe problem [20; 7], and ultrahyperbolic knowledge graph embedding [48]. Additionally, it is closely related to machine learning in hyperbolic spaces, including Lorentz model learning [35; 50; 8] and ultrahyperbolic neural networks [27; 54; 42]. It also intersects with hyperbolic linear algebra [3; 21], addressing problems such as the indefinite least squares problem, hyperbolic QR factorization, and indefinite polar decomposition.

### Related Work

**Block Coordinate Descent Methods**. Block Coordinate Descent (BCD) is a well-established iterative algorithm that sequentially minimizes along block coordinate directions. Its simplicity and efficiency have led to its widespread adoption in structured convex applications [37]. Recently, BCD has gained traction in non-convex problems due to its robust optimality guarantees and/or excellent empirical performance in areas including optimal transport [22], matrix optimization [12], fractional minimization [52], deep neural networks [5; 53; 32], federated learning[47], black-box optimization [4], and optimization with orthogonality constraints [51; 14]. To our knowledge, this is the first application of BCD methods to optimization under J-orthogonality constraints, with a focus on analyzing their theoretical guarantees and empirical efficacy.

**Minimizing Smooth Functions under J-Orthogonality Constraints**. The J-orthogonal matrix belongs to a subset of generalized orthogonal matrices [16; 36; 23]. However, projecting onto the J-orthogonality constraint poses challenges, complicating the extension of conventional optimization algorithms to address optimization problems under these constraints [1; 16]. This contrasts with computing orthogonal projections using methods such as polar or SVD decomposition, or approximating them via QR factorization. Existing methods for addressing Problem (1) can be categorized into three classes. (_i_) CS-Decomposition Based Methods. These approaches involve parameterizing four orthogonal matrices (as described in Proposition 2.2) and subsequently minimizing a smooth function over these matrices in an alternating fashion. The involvement of \(3\times 3\) block matrices makes the implementation of these methods very challenging. Consequently, the work of [48] focuses on optimizing a reduced subspace of the CS decomposition parameters, albeit at the expense of losing some degrees of freedom. (_ii_) Unconstrained Multiplier Correction Methods [31; 13; 14]. These methods leverage the symmetry and explicit closed-form expression of the Lagrangian multiplier at the first-order optimality condition. Consequently, they address an unconstrained problem, resulting in efficient first-order infeasible approaches. (_iii_) Alternating Direction Method of Multipliers [19]. This method reformulates the original problem into a bilinear constrained optimization problem by introducing auxiliary variables. It employs dual variables to handle bilinear constraints, iteratively optimizing primal variables while keeping other primal and dual variables fixed, and using a gradient ascent strategy to update the dual variables. This approach has become widely adopted for solving general nonconvex and nonsmooth composite optimization problems. Notably, all the aforementioned methods solely identify critical points of Problem (1).

**Finite-Sum Problems via Stochastic Gradient Descent**. The finite-sum structure is prevalent in machine learning and statistical modeling, facilitating decomposition into smaller, more manageable components. This property is advantageous for developing efficient algorithms for large-scale problems, such as Stochastic Gradient Descent (SGD). Reducing variance is crucial in SGD because it can lead to more stable and faster convergence. Various techniques, such as mini-batch SGD, momentum methods, and variance reduction methods like SAGA [10], SVRG [25], SARAH [34], SPIDER [11; 44], SNVRG [55], and PAGE [30], have been developed to address this issue. Additionally, SGD for minimizing composite functions has also been investigated by the authors [15; 24; 29].

### Contributions

This paper makes the following contributions. (_i_) Algorithmically: We introduce the **JOBCD** algorithm, a novel Block Coordinate Descent method specifically designed to tackle optimizations constrained by J-orthogonality. We explore two specific variants of **JOBCD**, one based on a Gauss-Seidel strategy (**GS-JOBCD**), the other on a variance-reduced and Jacobi strategy (**VR-J-JOBCD**). Notably, **VR-J-JOBCD** incorporates a variance-reduction technique into a parallel framework to reduce oracle complexity in the minimization of finite-sum functions (See Section 2). (_ii_) Theoretically: We provide comprehensive optimality and convergence analyses for both 

[MISSING_PAGE_FAIL:3]

**Proposition 2.2**.: _(Hyperbolic CS Decomposition [41]) Let \(\mathbf{V}\) be J-orthogonal with signature \((p,n-p)\). Assume that \(n-p\leq p\). Then there exist vectors \(\hat{c},\hat{s}\in\mathbb{R}^{n-p}\) with \(\hat{c}\odot\hat{c}-\hat{s}\odot\hat{s}=\mathbf{1}\), and orthogonal matrices \(\mathbf{U}_{1},\mathbf{V}_{1}\in\mathbb{R}^{p\times p}\) and \(\mathbf{U}_{2},\mathbf{V}_{2}\in\mathbb{R}^{(n-p)\times(n-p)}\) such that: \(\mathbf{V}=\left[\begin{smallmatrix}\mathbf{U}_{1}&0\\ 0&\mathbf{U}_{2}\end{smallmatrix}\right]\left[\begin{smallmatrix}\mathbf{D}_{ 2}(\hat{c})&0&\mathrm{D}_{2}(\hat{s})\\ 0&\mathrm{D}_{2}(\hat{s})&0&\mathrm{D}_{2}(\hat{c})\end{smallmatrix}\right]\)\(\left[\begin{smallmatrix}\mathbf{V}_{1}^{\mathrm{T}}&0\\ 0&\mathbf{V}_{2}^{\mathrm{T}}\end{smallmatrix}\right]\)._

Applying Proposition 2.2 with \(n=2\), \(p=1\), and \(\mathbf{U}_{1}=\mathbf{U}_{2}=\mathbf{V}_{1}=\mathbf{V}_{2}=\pm 1\), \(\tilde{\mathbf{c}}^{2}-\tilde{\mathbf{s}}^{2}=1\) with \(\tilde{\mathbf{c}},\tilde{\mathbf{s}}\in\mathbb{R}\), we parametrize \(\mathbf{V}\) as: \(\mathbf{V}=\left(\begin{smallmatrix}\pm 1&0\\ 0&\pm 1\end{smallmatrix}\right)\cdot\left(\begin{smallmatrix}\tilde{\mathbf{c }}&\tilde{\mathbf{s}}\\ \tilde{\mathbf{s}}&\tilde{\mathbf{c}}\end{smallmatrix}\right)\cdot\left( \begin{smallmatrix}\pm 1&0\\ 0&\pm 1\end{smallmatrix}\right)\), where we denote \(\tilde{\mathbf{s}}\) as \(\sinh(\mu)\), \(\tilde{\mathbf{c}}\) as \(\cosh(\mu)\), and \(\tilde{\mathbf{c}}\) as \(\tanh(\mu)\)for some \(\mu\in\mathbb{R}\), for simplicity of notation. It is not difficult to show that Problem (6) reduces to the following one-dimensional search problem:

\[\bar{\mu}\in\min_{\mu}\tfrac{1}{2}\operatorname{vec}(\mathbf{V})^{\mathsf{T}} \hat{\mathbf{Q}}\operatorname{vec}(\mathbf{V})+\langle\mathbf{V},\mathbf{P} \rangle,\,\operatorname{s.t.}\,\mathbf{V}\in\{(\begin{smallmatrix}\tilde{ \mathbf{c}}&\tilde{\mathbf{s}}\\ \tilde{\mathbf{s}}&\tilde{\mathbf{c}}\end{smallmatrix}),(\begin{smallmatrix} \tilde{\mathbf{c}}&-\tilde{\mathbf{s}}\\ -\tilde{\mathbf{s}}&\tilde{\mathbf{c}}\end{smallmatrix}),(\begin{smallmatrix} -\tilde{\mathbf{c}}&-\tilde{\mathbf{s}}\\ \tilde{\mathbf{s}}&-\tilde{\mathbf{c}}\end{smallmatrix}),(\begin{smallmatrix} \tilde{\mathbf{c}}&-\tilde{\mathbf{s}}\\ \tilde{\mathbf{s}}&-\tilde{\mathbf{c}}\end{smallmatrix})\}.\] (7)

We apply a breakpoint search method to solve Problem (7). For simplicity, we provide an analysis only for the first case. A detailed discussion of all four cases can be found in Appendix Section B.1. For the case where \(\mathbf{V}=(\begin{smallmatrix}\tilde{\mathbf{s}}&\tilde{\mathbf{s}}\\ \tilde{\mathbf{s}}&\tilde{\mathbf{c}}\end{smallmatrix})\), Problem (7) reduces to the following problem:

\[\min_{\tilde{c},\tilde{\mathbf{s}}}a\,\tilde{\mathbf{c}}+b\,\tilde{\mathbf{s}} +c\,\tilde{\mathbf{c}}^{2}+d\,\tilde{\mathbf{c}}\,\tilde{\mathbf{s}}+e\, \tilde{\mathbf{s}}^{2},\] (8)

where \(a=\mathbf{P}_{11}+\mathbf{P}_{22}\), \(b=\mathbf{P}_{12}+\mathbf{P}_{21}\), \(c=\frac{1}{2}(\dot{\mathbf{Q}}_{11}+\dot{\mathbf{Q}}_{41}+\dot{\mathbf{Q}}_{14 }+\dot{\mathbf{Q}}_{44})\), \(d=\frac{1}{2}(\dot{\mathbf{Q}}_{21}+\dot{\mathbf{Q}}_{31}+\dot{\mathbf{Q}}_{12 }+\dot{\mathbf{Q}}_{42}+\dot{\mathbf{Q}}_{13}+\dot{\mathbf{Q}}_{43}+\dot{ \mathbf{Q}}_{24}+\dot{\mathbf{Q}}_{34})\), and \(e=\frac{1}{2}(\dot{\mathbf{Q}}_{22}+\dot{\mathbf{Q}}_{32}+\dot{\mathbf{Q}}_{23 }+\dot{\mathbf{Q}}_{33})\). Then we perform a substitution to convert Problem (8) into an equivalent problem that depends on the trigonometric functions: (\(\textbf{\emph{i}}\)) \(\tilde{c}^{2}=\frac{1}{1-\tilde{\mathbf{i}}^{2}}\); (\(\textbf{\emph{ii}}\)) \(\tilde{\mathbf{s}}^{2}=\frac{\tilde{\mathbf{i}}^{2}}{1-\tilde{\mathbf{i}}^{2}}\); (\(\textbf{\emph{iii}}\)) \(\tilde{\mathbf{t}}=\frac{\tilde{\mathbf{s}}}{\tilde{\mathbf{c}}}\). The following lemma provides a characterization of the global optimal solution for Problem (8).

**Lemma 2.3**.: _(Proof in Section C.2) We let \(\tilde{F}(\tilde{c},\tilde{s})\triangleq a\tilde{c}+b\tilde{s}+c\tilde{c}^{2}+d \tilde{c}\tilde{s}+e\tilde{s}^{2}\). The optimal solution \(\bar{\mu}\) to Problem (8) can be computed as: \([\cosh(\tilde{\mu}),\sinh(\tilde{\mu})]\in\arg\min_{[c,s]}\tilde{F}(c,s),\, \operatorname{s.t.}\,[c,s]\in\{[\frac{1}{\sqrt{1-(\tilde{t}_{+})^{2}}},\frac{ \tilde{t}_{+}}{\sqrt{1-(\tilde{t}_{+})^{2}}}],[\frac{-1}{\sqrt{1-(\tilde{t}_{-})^ {2}}},\frac{-\tilde{t}_{-}}{\sqrt{1-(\tilde{t}_{-})^{2}}}]\}\), where \(\tilde{t}_{+}\in\arg\min_{t}p(t)\triangleq\frac{a+b\tilde{t}}{\sqrt{1-\tilde{t }^{2}}}+\frac{w+dt}{1-\tilde{t}^{2}}\); \(\bar{t}_{-}\in\arg\min_{t}\tilde{p}(t)\triangleq\frac{-a-bt}{\sqrt{1-\tilde{t }^{2}}}+\frac{w+dt}{1-\tilde{t}^{2}}\). Here \(w=c+e\)._

We now describe how to find the optimal solution \(\bar{t}_{+}\), where \(\bar{t}_{+}\in\arg\min_{t}p(t)\triangleq\frac{a+bt}{\sqrt{1-\tilde{t}^{2}}}+ \frac{w+dt}{1-\tilde{t}^{2}}\); this strategy can naturally be extended to find \(\bar{t}_{-}\). Initially, we have the following first-order optimality conditions for the problem: \(0=\nabla p(t)=\)\([b(1-t^{2})+(a+bt)t]\sqrt{1-\tilde{t}^{2}}+[d(1-t^{2})+(w+dt)(2t)]\Leftrightarrow dt ^{2}+2wt+d=-[b+at]\sqrt{1-\tilde{t}^{2}}\). Squaring both sides yields the following quartic equation: \(c_{4}t^{4}+c_{3}t^{3}+c_{2}t^{2}+c_{1}t+c_{0}=0\), where \(c_{4}=d^{2}+a^{2}\), \(c_{3}=4wd+2ab\)\(c_{2}=4w^{2}+2d^{2}-a^{2}+b^{2}\), \(c_{1}=4wd-2ab\), \(c_{0}=d^{2}-b^{2}\). This equation can be solved analytically by Lodovico Ferrari's method [46], resulting in all its real roots \(\{\tilde{t}_{1},\tilde{t}_{2},\ldots,\tilde{t}_{j}\}\) with \(1\leq j\leq 4\).

For the second and third cases, Problem (6) essentially boils down to optimization under orthogonality constraints. The work of [51] derives a breakpoint search method for finding the optimal solution for Problem (6) with \(\mathbf{J}_{\mathtt{BB}}\in\{\left(\begin{smallmatrix}1&0\\ 0&1\end{smallmatrix}\right),\left(\begin{smallmatrix}-1&0\\ 0&-1\end{smallmatrix}\right)\}\) using the Givens rotation and Jacobi reflection matrices.

### Variance-Reduced Jacobi Block Coordinate Descent Algorithm

This subsection proposes the **VR-J-JOBCD** algorithm, a randomized block coordinate descent method derived from **GS-JOBCD**. Importantly, by leveraging the parallel framework of a Jacobi strategy [17; 9], **VR-J-JOBCD** integrates variance reduction techniques [39; 30; 18] to decrease oracle complexity in the minimization of finite-sum functions. This makes the algorithm effective for minimizing large-scale problems under J-orthogonality constraints.

**Notations**. We assume \(n\) is an even number in this paper. We create \((n/2)\) pairs by non-overlapping grouping of the numbers in any arbitrary combination, with each pair containing two distinct numbers from the set \([n]\). It is not hard to verify that such grouping yields \(\mathrm{C}_{J}=(n!)/(2^{n/2}\frac{n}{2}!)\) possible combinations. The set of these combinations is denoted as \(\Upsilon\triangleq\{\tilde{\mathcal{B}}_{i}\}_{i=1}^{\mathrm{C}_{J}}\triangleq \{\tilde{\mathcal{B}}_{1},\tilde{\mathcal{B}}_{2},\ldots,\tilde{\mathcal{B}}_ {\mathrm{C}_{J}}\}\)1.

Footnote 1: Taking \(n=4\) for example, we have: \(\Upsilon=\{\{(1,2),(3,4)\}\), \(\{(1,3),(2,4)\}\), \(\{(1,4),(2,3)\}\}\).

\(\blacktriangleright\)**Variance Reduction Strategy**. We incorporate state-of-the-art variance reduction strategies from the literature [30; 5] into our algorithm to solve Problem (1). These methods iteratively generate a stochastic gradient estimator as follows:

\[\tilde{\mathbf{G}}^{t}=\left\{\begin{array}{ll}\frac{1}{b}\sum_{i\in\mathcal{ S}_{+}^{t}}\nabla f_{i}(\mathbf{X}^{t}),&\text{with probability }\ p;\\ \tilde{\mathbf{G}}^{t-1}+\frac{1}{b^{\prime}}\sum_{i\in\mathcal{B}_{*}^{t}}( \nabla f_{i}(\mathbf{X}^{t})-\nabla f_{i}(\mathbf{X}^{t-1})),&\text{with probability }\ 1-p.\end{array}\right.\] (9)

Here, \(\{\mathcal{S}_{+}^{t},\mathcal{S}_{+}^{t}\}\) are uniform random minibatch samples with \(|\mathcal{S}_{+}^{t}|=b\), \(|\mathcal{S}_{*}^{t}|=b^{\prime}\), and \(\tilde{\mathbf{G}}^{0}=\frac{1}{b}\sum_{i\in\mathcal{S}_{+}^{0}}\nabla f_{i}( \mathbf{X}^{0})\). We drop the superscript \(t\) for \(\{\mathcal{S}_{+}^{t},\mathcal{S}_{*}^{t}\}\) as \(t\) can be inferred from context. We only focus on the default setting that [30; 5]: \(b=N\), \(b^{\prime}=\sqrt{b}\) and \(p=\frac{b^{\prime}}{b+b^{\prime}}\).

\(\blacktriangleright\)**Jacobi Block Coordinate Descent Method**. The proposed algorithm is built upon the parallel framework of a Jacobi strategy. In each iteration \(t\), we randomly and uniformly (with replacement) select a coordinate set \(\mathtt{B}^{t}\triangleq\{\mathcal{B}_{(1)}^{t},\mathcal{B}_{(2)}^{t},\cdots, \mathcal{B}_{(\frac{n}{2})}^{t}\}\) from the set \(\Upsilon\) with \(\mathtt{B}^{t}\in\mathbb{N}^{\frac{n}{2}\times 2}\) and \(\mathcal{B}_{(i)}^{t}\in\mathbb{N}^{2}\).

For all \(t\), we have: \(\mathtt{B}_{(i)}^{t}\cap\mathtt{B}_{(j)}^{t}=\emptyset\) and \(\cup_{i=1}^{n/2}(\mathtt{B}_{(i)}^{t})=[n]\). We drop the superscript \(t\) if \(t\) can be inferred from context.

The following lemma shows how to choose a suitable matrix \(\mathbf{Q}\) so that the Jacobi strategy can be applied.

**Lemma 2.4**.: _(Proof in Section C.3) We let \(\mathtt{B}^{t}\triangleq\{\mathtt{B}_{(1)}^{t},\mathtt{B}_{(2)}^{t},\cdots, \mathtt{B}_{(\frac{n}{2})}^{t}\}\in\Upsilon\) for all \(t\). We let \(\mathbf{Q}=\varsigma\mathbf{I}_{4}\), where \(\varsigma\) is some suitable constant with \(\varsigma\leq L_{f}\). For any \(\mathtt{B}_{(i)}^{t}\) and \(\mathtt{B}_{(j)}^{t}\) with \(i\neq j\), their corresponding objective functions as in Equation (3) are independent._

We consider the following block coordinate update rule in **VR-J-JOBCD**: \(\mathbf{X}^{t+1}\Leftarrow\tilde{\mathcal{X}}_{\mathtt{B}}^{t}(\mathbf{V}_{ :})\triangleq\mathbf{X}^{t}+[\sum_{i=1}^{n/2}\mathbf{U}_{\mathtt{B}_{(i)}}( \mathbf{V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathtt{B}_{(i)}}^{\top}]\mathbf{X}^{t}\). The following lemma provides properties of this rule.

**Lemma 2.5**.: _(Proof in Section C.4) We let \(\mathtt{B}\in\Upsilon\), \(\mathbf{V}_{i}\in\mathcal{J}_{\mathtt{B}_{(i)}}\), \(\mathbf{X}\in\mathcal{J}\), and \(i\in[\frac{n}{2}]\). We define \(\mathbf{X}^{+}\triangleq\tilde{\mathcal{X}}_{\mathtt{B}}(\mathbf{V}_{:}) \triangleq\mathbf{X}+[\sum_{i=1}^{n/2}\mathbf{U}_{\mathtt{B}_{(i)}}(\mathbf{V }_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathtt{B}_{(i)}}^{\top}]\mathbf{X}\). We have: **(a)**\(\sum_{i=1}^{n}\|\mathbf{U}_{\mathtt{B}_{(i)}}(\mathbf{V}_{i}-\mathbf{I}_{2}) \mathbf{U}_{\mathtt{B}_{(i)}}^{\top}\mathbf{X}\|_{\mathtt{F}}^{2}=\|\sum_{i=1}^ {n}\mathbf{U}_{\mathtt{B}_{(i)}}(\mathbf{V}_{i}-\mathbf{I}_{2})\mathbf{U}_{ \mathtt{B}_{(i)}}^{\top}\mathbf{X}\|_{\mathtt{F}}^{2}\). **(b)**\(\|\mathbf{X}^{+}-\mathbf{X}\|_{\mathtt{F}}^{2}\leq\|\mathbf{X}\|_{\mathtt{F}}^{2} \cdot\sum_{i=1}^{n/2}\|\mathbf{V}_{i}-\mathbf{I}_{2}\|_{\mathtt{F}}^{2}\). **(c)**\(\|\mathbf{X}^{+}-\mathbf{X}\|_{\mathtt{H}}^{2}\leq\sum_{i=1}^{n/2}\| \mathbf{V}_{i}-\mathbf{I}_{2}\|_{\mathtt{Q}}^{2}\) with \(\mathbf{Q}=\varsigma\mathbf{I}_{4}\). **(d)** For all \(\tilde{\mathbf{G}}\in\mathbb{R}^{n\times n}\), it follows that: \(2\sum_{i=1}^{n/2}\langle\mathbf{V}_{i}-\mathbf{I}_{2},[(\nabla f(\mathbf{X})-\tilde {\mathbf{G}})\mathbf{X}^{\top}]_{\mathtt{B}_{(i)}\mathtt{B}_{(i)}}\rangle \leq\|\mathbf{X}\|_{\mathtt{F}}^{2}\sum_{i=1}^{n/2}\|\mathbf{V}_{i}-\mathbf{I }_{2}\|_{\mathtt{F}}^{2}+\|[\nabla f(\mathbf{X})-\tilde{\mathbf{G}}]\|_{ \mathtt{F}}^{2}\).

\(\blacktriangleright\)**The Main Algorithm**. Using the update rule above, we consider the following iterative procedure: \(\mathbf{X}^{t+1}\Leftarrow\tilde{\mathcal{X}}_{\mathtt{B}}^{t}(\mathbf{V}_{ :})\), where \(\tilde{\mathbf{V}}_{:}^{t}\in\arg\min_{\mathbf{V}}\), \(f(\tilde{\mathcal{X}}_{\mathtt{B}}^{t}(\mathbf{V}_{:}))\). We establish the majorization function for 

[MISSING_PAGE_FAIL:6]

\(\nabla_{J}f(\dot{\mathbf{X}})\triangleq\nabla f(\dot{\mathbf{X}})-\mathbf{J}\mathbf{X }[\nabla f(\dot{\mathbf{X}})]^{\top}\dot{\mathbf{X}}\mathbf{J}\). The associated Lagrangian multiplier can be computed as \(\Lambda=\mathbf{J}\mathbf{X}^{\top}\nabla f(\dot{\mathbf{X}})\). (_b_) The critical point condition is equivalent to the requirement that the matrix \(\mathbf{X}\nabla f(\dot{\mathbf{X}})^{\top}\mathbf{J}\) is symmetric, which is expressed as \(\mathbf{X}\mathbf{G}^{\top}\mathbf{J}=[\mathbf{X}\mathbf{G}^{\top}\mathbf{J} ]^{\top}\).

**Remarks**. While our results in Lemma 3.1 show similarities to existing works focusing on problems under orthogonality constraints [45], this study marks the first investigation into the first-order optimality condition for optimization problems under J-orthogonality constraints.

The following definition is useful in our subsequent analysis of the proposed algorithms.

**Definition 3.2**.: (Block Stationary Point, abbreviated as BS-point) Let \(\theta>0\). A solution \(\ddot{\mathbf{X}}\in\mathcal{J}\) is termed as a block stationary point if, for all \(\mathds{B}\in\Omega\triangleq\{\mathcal{B}_{1},\mathcal{B}_{2},\ldots, \mathcal{B}_{\mathrm{C}_{n}^{2}}\}\), the following condition is satisfied: \(\mathbf{I}_{2}\in\arg\min_{\mathbf{V}\in\mathcal{J}_{\mathds{B}}}\mathcal{G}( \mathbf{V};\ddot{\mathbf{X}},\mathds{B})\).

The following theorem shows the relation between critical points and BS-points.

**Theorem 3.3**.: _(Proof in Section D.2) Any BS-point is a critical point, while the reverse is not necessarily true._

## 4 Convergence Analysis

This section provides a convergence analysis for **GS-JOBCD** and **VR-J-JOBCD**.

For **GS-JOBCD**, the randomness of output \((\overline{\mathbf{V}}^{t},\mathbf{X}^{t+1})\) for all \(t\) are influenced by the random variable \(\xi^{t}\triangleq(\mathds{B}^{1};\mathds{B}^{2};\cdots;\mathds{B}^{t})\). For **VR-J-JOBCD**, the randomness of output \((\bar{\mathbf{V}}^{t}_{:},\mathbf{X}^{t+1})\) are influenced by the random variables \(\iota^{t}\triangleq(\mathds{B}^{1},\mathds{S}^{1}_{+},\mathds{S}^{1}_{*}; \mathds{B}^{2},\mathbb{S}^{2}_{+},\mathbb{S}^{2}_{*};\cdots;\mathds{B}^{t}, \mathds{S}^{t}_{+},\mathds{S}^{t}_{*})\).

We denote \(\ddot{\mathbf{X}}\) as the global optimal solution of Problem (1). To simplify notations, we define: \(u^{t}=\|\ddot{\mathbf{G}}^{t}-\nabla f(\mathbf{X}^{t})\|_{\bar{\mathbf{F}}}^{2}\), and \(\Delta_{i}=f(\mathbf{X}^{i})-f(\ddot{\mathbf{X}})\).

We impose the following additional assumptions on the proposed algorithms.

**Assumption 4.1**.: There exists constants \(\{\overline{\mathbf{X}},\overline{\mathbf{V}}\}\) that: \(\|\mathbf{X}^{t}\|_{\mathrm{F}}\leq\overline{\mathbf{X}}\), and \(\|\mathbf{V}^{t}\|_{\mathrm{F}}\leq\overline{\mathbf{V}}\) for all \(t\).

**Assumption 4.2**.: There exists a constant \(\overline{\mathbf{G}}\) that: \(\|\nabla f(\mathbf{X}^{t})\|_{\mathrm{F}}\leq\overline{\mathbf{G}}\), and \(\|\ddot{\mathbf{G}}^{t}\|_{\mathrm{F}}\leq\overline{\mathbf{G}}\) for all \(t\).

**Assumption 4.3**.: For any \(\mathbf{X}\in\mathbb{R}^{n\times n}\), \(\mathbb{E}_{i}[\|\nabla f_{i}(\mathbf{X}^{t})-\nabla f(\mathbf{X}^{t})\|_{ \mathrm{F}}^{2}]\leq\sigma^{2}\), where \(i\) is drawn uniformly at random from \([N]\).

**Remarks**. (_i_) Assumption 4.1 is satisfied as the function \(f_{i}(\mathbf{X})\) is coercive for all \(i\). (_ii_) Assumption 4.2 imposes a bound on the (stochastic) gradient, a fairly moderate condition frequently employed in nonconvex optimization [26]. (_iii_) Assumption 4.3 ensures that the variance of the stochastic gradient is bounded, which is a common requirement in stochastic optimization [30; 5].

### Global Convergence

We define the \(\epsilon\)-BS-point as follows.

**Definition 4.4**.: (\(\epsilon\)-BS-point) Given any constant \(\epsilon>0\), a point \(\ddot{\mathbf{X}}\) is called an \(\epsilon\)-BS-point if: \(\mathcal{E}(\ddot{\mathbf{X}})\leq\epsilon\). Here, \(\mathcal{E}(\mathbf{X})\) is defined as \(\mathcal{E}(\mathbf{X})\triangleq\frac{1}{C_{\mathcal{T}}^{2}}\sum_{i=1}^{C_{ \mathcal{T}}^{2}}\mathrm{dist}(\mathbf{I}_{2},\arg\min_{\mathbf{V}}\mathcal{G} (\mathbf{V};\mathbf{X},\mathcal{B}_{i}))^{2}\) for For **GS-JOBCD**, while it is defined as \(\mathcal{E}(\mathbf{X})\triangleq\frac{1}{C_{\mathcal{T}}}\sum_{i=1}^{C_{ \mathcal{T}}^{2}}\mathbb{E}_{\iota^{t}}[\mathrm{dist}(\mathbf{I}_{2},\arg\min_{ \mathbf{V}}\mathcal{T}(\mathbf{V}_{:};\mathbf{X},\tilde{\mathcal{B}}_{i}))^{2}]\) for **VR-J-JOBCD**, where the expectation is with respect to the randomness inherent in the algorithm [30].

We have the following useful lemma for **VR-J-JOBCD**.

**Lemma 4.5**.: _(Proof in Section E.1) Suppose Assumption 4.3 holds, then the variance \(\mathbb{E}_{\iota^{t}}[u_{k}]\) of the gradient estimators \(\{\ddot{\mathbf{G}}^{t}\}\) of Algorithm 2 is bounded by: \(\mathbb{E}_{\iota^{t}}[u^{t}]\leq\frac{p(N-b)}{b(N-1)}\sigma^{2}+(1-p)\mathbb{E }_{\iota^{t-1}}[u^{t-1}]+\frac{L_{\mathcal{T}}^{2}\overline{\mathbf{X}}^{2}(1 -p)}{b^{\prime}}\mathbb{E}_{\iota^{t-1}}[\sum_{i=1}^{n/2}\|\mathbf{V}_{i}^{t-1}- \mathbf{I}_{2}\|_{\mathrm{F}}^{2}]\)_

The following two theorems establish the iteration complexity (or oracle complexity) for **GS-JOBCD** and **VR-J-JOBCD**.

**Theorem 4.6**.: _(Proof in Section E.2)_ **GS-JOBCD** finds an \(\epsilon\)-BS-point of Problem (1) within \(\mathcal{O}(\frac{\Delta_{n}N}{\epsilon})\) arithmetic operations.

**Theorem 4.7**.: _(Proof in Section E.3) Let \(b=N\), \(b^{\prime}=\sqrt{N}\), and \(p=\frac{b^{\prime}}{b+b^{\prime}}\)._ **VR-J-JOBCD** _finds an \(\epsilon\)-BS-point of Problem (1) within \(\mathcal{O}(nN+\frac{\Delta_{0}\sqrt{N}}{\epsilon})\) arithmetic operations._

**Remark.** Theorems 4.6 and 4.7 demonstrate that the arithmetic operation complexity of **GS-JOBCD** is linearly dependent on \(N\), while **VR-J-JOBCD** is linearly dependent on \(\sqrt{N}\). Therefore, **VR-J-JOBCD** reduces the iteration complexity significantly.

### Strong Convergence under KL Assumption

We prove algorithms achieve strong convergence based on a non-convex analysis tool called Kurdyka-Lojasiewicz inequality[2].

We impose the following assumption on Problem (1).

**Assumption 4.8**.: (Kurdyka-Lojasiewicz Property). Assume that \(f^{\circ}(\mathbf{X})=f(\mathbf{X})+\mathcal{I}_{\mathcal{J}}(\mathbf{X})\) is a KL function. For all \(\mathbf{X}\in\mathrm{dom}\,f^{\circ}\), there exists \(\sigma\in[0,1),\eta\in(0,+\infty]\) a neighborhood \(\Upsilon\) of \(\mathbf{X}\) and a concave and continuous function \(\varphi(t)=ct^{1-\sigma},c>0\), \(t\in[0,\eta)\) such that for all \(\mathbf{X}^{\prime}\in\Upsilon\) and satisfies \(f^{\circ}(\mathbf{X}^{\prime})\in(f^{\circ}(\mathbf{X}),f^{\circ}(\mathbf{X} )+\eta)\), the following holds: \(\mathrm{dist}(\mathbf{0},\nabla f^{\circ}(\mathbf{X}^{\prime}))\varphi^{ \prime}(f^{\circ}(\mathbf{X}^{\prime})-f^{\circ}(\mathbf{X}))\geq 1\).

We establish strong limit-point convergence for **VR-J-JOBCD** and **GS-JOBCD**.

**Theorem 4.9**.: _(Proof in Section E.5, a Finite Length Property). The sequence \(\{\mathbf{X}^{t}\}_{t=0}^{\infty}\) of_ **GS-JOBCD** _has finite length property that: \(\forall t,\sum_{i=1}^{t}\mathbb{E}_{\xi^{t}}[\|\mathbf{X}^{t+1}-\mathbf{X}^{t} \|_{\mathbb{F}}]\leq\mathcal{O}(\varphi(\Delta_{1}))<+\infty\), where \(\varphi(\cdot)\) is the desingularization function defined in Proposition 4.8._

**Theorem 4.10**.: _(Proof in Section E.4, a Finite Length Property). Choosing \(b=N\), \(b^{\prime}=\sqrt{N}\) and \(p=\frac{b^{\prime}}{b+b^{\prime}}\), then the sequence \(\{\mathbf{X}^{t}\}_{t=0}^{\infty}\) of_ **VR-J-JOBCD** _has finite length property that: \(\forall t,\sum_{i=1}^{t}\mathbb{E}_{\xi^{t}}[\|\mathbf{X}^{t+1}-\mathbf{X}^{t }\|_{\mathbb{F}}]\leq\mathcal{O}(\frac{\varphi(\Delta_{1})}{N^{1/4}})<+\infty\), where \(\varphi(\cdot)\) is the desingularization function defined in Assumption 4.8._

## 5 Applications and Numerical Experiments

This section demonstrates the effectiveness and efficiency of **JOBCD** on three optimization tasks: _(i)_ the hyperbolic eigenvalue problem, _(ii)_ structural probe problem, and _(iii)_ Ultra-hyperbolic Knowledge Graph Embedding problem. We provide experiments for the last problem in Section F.2.

\(\blacktriangleright\)**Application to the Hyperbolic Eigenvalue Problem (HEVP)**. The hyperbolic eigenvalue problem refers to the generalized eigenvalue problem in hyperbolic spaces [40]. This problem is a fundamental component in machine learning models, such as Hyperbolic PCA [43; 6]. Given a data matrix \(\mathbf{D}\in\mathbb{R}^{m\times n}\) and a signature matrix \(\mathbf{J}\) with signature \((p,n-p)\), HEVP can be formulated as the following optimization problem: \(\min_{\mathbf{X}}~{}-\mathbf{tr}(\mathbf{X}^{\mathsf{T}}\mathbf{D}^{\mathsf{T }}\mathbf{D}\mathbf{X}),~{}\mathrm{s.t.}~{}\mathbf{X}^{\mathsf{T}}\mathbf{J} \mathbf{X}=\mathbf{J}\).

\(\blacktriangleright\)**Application to the Hyperbolic Structural Probe Problem (HSPP)**. The Structure Probe (SP) is a metric learning model aimed at understanding the intrinsic semantic information of large language models [20][7]. Given a data matrix \(\mathbf{D}\in\mathbb{R}^{m\times n}\) and its associated Euclidean distance metric matrix \(\mathbf{T}\in\mathbb{R}^{m\times m}\), HSPP employs a smooth homeomorphic mapping function \(\varphi(\cdot)\) to project the data \(\mathbf{D}\) into ultra-hyperbolic space. Subsequently, it seeks an appropriate linear transformation \(\mathbf{X}\in\mathbb{R}^{n\times n}\) constrained within a specific structure \(\mathbf{X}\in\mathcal{J}\), such that the resulting transformed data \(\mathbf{Q}\triangleq\varphi(\mathbf{D})\mathbf{X}\in\mathbb{R}^{m\times n}\) exhibits similarity to the original distance metric matrix \(\mathbf{T}\) under the ultra-hyperbolic geodesic distance \(d_{\alpha}(\mathbf{Q}_{i:},\mathbf{Q}_{j:})\), expressed as \(\mathbf{T}_{i,j}\thickapprox d_{\alpha}(\mathbf{Q}_{i:},\mathbf{Q}_{j:})\) for all \(i,j\in[m]\), where \(\mathbf{Q}_{i:}\) is \(i\)-th row of the matrix \(\mathbf{Q}\in\mathbb{R}^{m\times n}\). This can be formulated as the following optimization problem: \(\min_{\mathbf{X}}\frac{1}{m^{2}}\sum_{i,j\in m}(\mathbf{T}_{i,j}-d_{\alpha}( \mathbf{Q}_{i:},\mathbf{Q}_{j:}))^{2},~{}\mathrm{s.t.}~{}\mathbf{Q}\triangleq \varphi(\mathbf{D})\mathbf{X}\), \(\mathbf{X}\in\mathcal{J}\). For more details on the functions \(\varphi(\cdot)\) and \(d_{\alpha}(\cdot,\cdot)\), please refer to Appendix Section F.1.

\(\blacktriangleright\)**Datasets**. To generate the matrix \(\mathbf{D}\in\mathbb{R}^{m\times n}\), we use 8 real-world or synthetic data sets for both HEVP and HSPP tasks: 'Cifar', 'CnnCaltech', 'Gisette', 'Mnist', 'randn', 'Sector', 'TDT2', 'w1a'. We randomly extract a subset from the original data sets for the experiments.

\(\blacktriangleright\)**Compared Methods**. We compare **GS-JOBCD** and **VR-J-JOBCD** with 3 state-of-the-art optimization algorithms under J-orthogonality constraints. _(i)_ The CS Decomposition Method (**CSDM**) [48]. _(ii)_ Standard ADMM (**ADMM**) [19]. **UMCM**: Unconstrained Multiplier Correction Method [31; 13].

[MISSING_PAGE_EMPTY:9]

## References

* [1] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. _Optimization algorithms on matrix manifolds_. Princeton University Press, 2008.
* [2] Hedy Attouch, Jerome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the kurdyka-tojasiewicz inequality. _Mathematics of operations research_, 35(2):438-457, 2010.
* [3] Adam Bojanczyk, Nicholas J Higham, and Harikrishna Patel. Solving the indefinite least squares problem by hyperbolic qr factorization. _SIAM Journal on Matrix Analysis and Applications_, 24(4):914-931, 2003.
* [4] HanQin Cai, Yuchen Lou, Daniel McKenzie, and Wotao Yin. A zeroth-order block coordinate descent algorithm for huge-scale black-box optimization. In _International Conference on Machine Learning_, pages 1193-1203. PMLR, 2021.
* [5] Xufeng Cai, Chaobing Song, Stephen Wright, and Jelena Diakonikolas. Cyclic block coordinate descent with variance reduction for composite nonconvex optimization. In _International Conference on Machine Learning_, pages 3469-3494. PMLR, 2023.
* [6] Ines Chami, Albert Gu, Dat P Nguyen, and Christopher Re. Horopca: Hyperbolic dimensionality reduction via horospherical projections. In _International Conference on Machine Learning (ICML)_, volume 139, pages 1419-1429, 2021.
* [7] Boli Chen, Yao Fu, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, and Liping Jing. Probing bert in hyperbolic spaces. _ICLR_, 2021.
* [8] Weize Chen, Xu Han, Yankai Lin, Hexu Zhao, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. Fully hyperbolic neural networks. _arXiv preprint arXiv:2105.14686_, 2021.
* [9] Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. _Advances in neural information processing systems_, 32, 2019.
* [10] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. _Advances in neural information processing systems_, 27, 2014.
* [11] Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator. _Advances in neural information processing systems_, 31, 2018.
* [12] Hamza Fawzi and Harry Goulbourne. Faster proximal algorithms for matrix optimization using jacobi-based eigenvalue methods. _Advances in Neural Information Processing Systems_, 34:11397-11408, 2021.
* [13] Bin Gao, Xin Liu, Xiaojun Chen, and Ya-xiang Yuan. A new first-order algorithmic framework for optimization problems with orthogonality constraints. _SIAM Journal on Optimization_, 28(1):302-332, 2018.
* [14] Bin Gao, Xin Liu, and Ya-xiang Yuan. Parallelizable algorithms for optimization problems with orthogonality constraints. _SIAM Journal on Scientific Computing_, 41(3):A1949-A1983, 2019.
* [15] Saeed Ghadimi, Guanghui Lan, and Hongchao Zhang. Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization. _Mathematical Programming_, 155(1-2):267-305, 2016.
* [16] Gene H Golub and Charles F Van Loan. _Matrix computations_. JHU press, 2013.
* [17] Eldon R Hansen. On cyclic jacobi methods. _Journal of the Society for Industrial and Applied Mathematics_, 11(2):448-459, 1963.
* [18] Vjeran Hari and Erna Begovic Kovac. On the convergence of complex jacobi methods. _Linear and multilinear algebra_, 69(3):489-514, 2021.

* He and Yuan [2012] Bingsheng He and Xiaoming Yuan. On the \(\mathcal{O}(1/n)\) convergence rate of the douglas-rachford alternating direction method. _SIAM Journal on Numerical Analysis_, 50(2):700-709, 2012.
* Hewitt and Manning [2019] John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representations. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)_, pages 4129-4138, 2019.
* Higham [2003] Nicholas J Higham. J-orthogonal matrices: Properties and generation. _SIAM review_, 45(3):504-519, 2003.
* Huang et al. [2021] Minhui Huang, Shiqian Ma, and Lifeng Lai. A riemannian block coordinate descent method for computing the projection robust wasserstein distance. In _International Conference on Machine Learning_, pages 4446-4455. PMLR, 2021.
* Hui and Ku [2022] Bo Hui and Wei-Shinn Ku. Low-rank nonnegative tensor decomposition in hyperbolic space. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 646-654, 2022.
* Reddi et al. [2016] Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization. _Advances in neural information processing systems_, 29, 2016.
* Johnson and Zhang [2013] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _International Conference on Learning Representations (ICLR)_, 2015.
* Law [2021] Marc Law. Ultrahyperbolic neural networks. _Advances in Neural Information Processing Systems_, 34:22058-22069, 2021.
* Law and Stam [2020] Marc Law and Jos Stam. Ultrahyperbolic representation learning. _Advances in neural information processing systems_, 33:1668-1678, 2020.
* Li et al. [2017] Qunwei Li, Yi Zhou, Yingbin Liang, and Pramod K Varshney. Convergence analysis of proximal gradient with momentum for nonconvex optimization. In _International Conference on Machine Learning_, pages 2111-2119. PMLR, 2017.
* Li et al. [2021] Zhize Li, Hongyan Bao, Xiangliang Zhang, and Peter Richtarik. Page: A simple and optimal probabilistic gradient estimator for nonconvex optimization. In _International conference on machine learning_, pages 6286-6295. PMLR, 2021.
* Liu et al. [2020] Wei Liu, Yinyu Zhang, Hongqiao Yang, and Shuzhong Zhang. A class of smooth exact penalty function methods for optimization problems with orthogonality constraints. _Optimization_, 69(3):399-426, 2020.
* Ma et al. [2020] Wan-Duo Kurt Ma, JP Lewis, and W Bastiaan Kleijn. The hsic bottleneck: Deep learning without back-propagation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 5085-5092, 2020.
* Mairal [2013] Julien Mairal. Optimization with first-order surrogate functions. In _International Conference on Machine Learning (ICML)_, volume 28, pages 783-791, 2013.
* Nguyen et al. [2017] Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. Sarah: A novel method for machine learning problems using stochastic recursive gradient. In _International conference on machine learning_, pages 2613-2621. PMLR, 2017.
* Nickel and Kiela [2018] Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolic geometry. In _International Conference on Machine Learning_, pages 3779-3788. PMLR, 2018.

* Novakovic and Singer [2022] Vedran Novakovic and Sanja Singer. A kogbetliantz-type algorithm for the hyperbolic svd. _Numerical algorithms_, 90(2):523-561, 2022.
* Nutini et al. [2022] Julie Nutini, Issam Laradji, and Mark Schmidt. Let's make block coordinate descent converge faster: faster greedy rules, message-passing, active-set complexity, and superlinear convergence. _Journal of Machine Learning Research_, 23(131):1-74, 2022.
* Razaviyayn et al. [2013] Meisam Razaviyayn, Mingyi Hong, and Zhi-Quan Luo. A unified convergence analysis of block successive minimization methods for nonsmooth optimization. _SIAM Journal on Optimization_, 23(2):1126-1153, 2013.
* Schmidt et al. [2017] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. _Mathematical Programming_, 162:83-112, 2017.
* Slapnicar and Truhar [2000] Ivan Slapnicar and Ninoslav Truhar. Relative perturbation theory for hyperbolic eigenvalue problem. _Linear Algebra and its Applications_, 309(1):57-72, 2000.
* Stewart and Dooren [2005] Michael Stewart and Paul Van Dooren. On the factorization of hyperbolic and unitary transformations into rotations. _SIAM Journal on Matrix Analysis and Applications_, 27(3):876-890, 2005.
* Tabaghi and Dokmanic [2020] Puoya Tabaghi and Ivan Dokmanic. Hyperbolic distance matrices. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1728-1738, 2020.
* Tabaghi et al. [2023] Puoya Tabaghi, Michael Khanzadeh, Yusu Wang, and Sivash Mirarab. Principal component analysis in space forms. _ArXiv_, abs/2301.02750, 2023.
* Wang et al. [2019] Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. Spiderboost and momentum: Faster variance reduction algorithms. _Advances in Neural Information Processing Systems_, 32, 2019.
* 434, 2012.
* [46] WikiContributors. Quartic equation. _https://en.wikipedia.org/wiki/Quartic_equation_.
* Wu et al. [2021] Ruiyuan Wu, Anna Scaglione, Hoi-To Wai, Nurullah Karakoc, Kari Hreinsson, and Wing-Kin Ma. Federated block coordinate descent scheme for learning global and personalized models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 10355-10362, 2021.
* Xiong et al. [2022] Bo Xiong, Shichao Zhu, Mojtaba Nayyeri, Chengjin Xu, Shirui Pan, Chuan Zhou, and Steffen Staab. Ultrahyperbolic knowledge graph embeddings. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2130-2139, 2022.
* Xiong et al. [2021] Bo Xiong, Shichao Zhu, Nico Potyka, Shirui Pan, Chuan Zhou, and Steffen Staab. Semi-riemannian graph convolutional networks. _ArXiv_, abs/2106.03134, 2021.
* Yu and De Sa [2019] Tao Yu and Christopher M De Sa. Numerically accurate hyperbolic embeddings using tiling-based models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Yuan [2023] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization under orthogonality constraints. _ArXiv_, abs/2304.03641, 2023.
* Yuan [2023] Ganzhao Yuan. Coordinate descent methods for fractional minimization. In _International Conference on Machine Learning_, pages 40488-40518, 2023.
* Zeng et al. [2019] Jinshan Zeng, Tim Tsz-Kit Lau, Shaobo Lin, and Yuan Yao. Global convergence of block coordinate descent in deep learning. In _International conference on machine learning_, pages 7313-7323. PMLR, 2019.
* Zhang et al. [2021] Yiding Zhang, Xiao Wang, Chuan Shi, Nian Liu, and Guojie Song. Lorentzian graph convolutional networks. In _Proceedings of the Web Conference 2021_, pages 1249-1261, 2021.
* Zhou et al. [2020] Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex optimization. _The Journal of Machine Learning Research_, 21(1):4130-4192, 2020.

## Appendix

The appendix is organized as follows.

Appendix A introduces some notations, technical preliminaries, and relevant lemmas.

Appendix B concludes some additional discussions.

Appendix C presents the proofs for Section 2.

Appendix D offers the proofs for Section 3.

Appendix E contains the proofs for Section 4.

Appendix F contains several extra experiments, extensions and discussions of the proposed methods.

## Appendix A Notations, Technical Preliminaries, and Relevant Lemmas

### Notations

In this paper, we denote the Lowercase boldface letters represent vectors, while uppercase letters represent real-valued matrices. We use the Matlab colon notation to denote indices that describe submatrices. The following notations are used throughout this paper.

* \(\mathbb{N}\) : Set of natural numbers
* \(\mathbb{R}\) : Set of real numbers
* \([n]\): \(\{1,2,...,n\}\)
* \(\|\mathbf{x}\|\): Euclidean norm: \(\|\mathbf{x}\|=\|\mathbf{x}\|_{2}=\sqrt{\langle\mathbf{x},\mathbf{x}\rangle}\)
* \(\mathbf{x}_{i}\): the \(i\)-th element of vector \(\mathbf{x}\)
* \(\mathbf{X}_{i,j}\) or \(\mathbf{X}_{ij}\) : the (\(i^{\text{th}}\), \(j^{\text{th}}\)) element of matrix \(\mathbf{X}\)
* \(\operatorname{vec}(\mathbf{X})\) : \(\operatorname{vec}(\mathbf{X})\in\mathbb{R}^{n\times 1}\), the vector formed by stacking the column vectors of \(\mathbf{X}\)
* \(\operatorname{mat}(\mathbf{x})\in\mathbb{R}^{n\times n}\), Convert \(\mathbf{x}\in\mathbb{R}^{nn\times 1}\) into a matrix with \(\operatorname{mat}(\operatorname{vec}(\mathbf{X}))=\mathbf{X}\)
* \(\mathbf{X}^{\mathsf{T}}\) : the transpose of the matrix \(\mathbf{X}\)
* \(\operatorname{sign}(t)\) : the signum function, \(\operatorname{sign}(t)=1\) if \(t\geq 0\) and \(\operatorname{sign}(t)=-1\) otherwise
* \(\mathbf{X}\otimes\mathbf{Y}\) : Kronecker product of \(\mathbf{X}\) and \(\mathbf{Y}\)
* \(\text{det}(\mathbf{D})\) : Determinant of a square matrix \(\mathbf{D}\in\mathbb{R}^{n\times n}\mathbf{D}\in\mathbb{R}^{n\times n}\)
* \(\mathbf{C}_{n}^{2}\) : the number of possible combinations choosing \(k\) items from \(n\) without repetition.
* \(\mathbf{0}_{n,r}\) : A zero matrix of size \(n\times r\); the subscript is omitted sometimes
* \(\mathbf{I}_{r}:\mathbf{I}_{r}\in\mathbb{R}^{r\times r}\), Identity matrix
* \(\mathbf{X}\succeq\mathbf{0}(\text{or }\succ\mathbf{0})\) : the Matrix \(\mathbf{X}\) is symmetric positive semidefinite (or definite)
* \(\operatorname{Diag}(\mathbf{x})\): Diagonal matrix with \(\mathbf{x}\) as the main diagonal entries.
* \(\mathbf{tr}(\mathbf{A})\) : Sum of the elements on the main diagonal \(\mathbf{A}\): \(\mathbf{tr}(\mathbf{A})=\sum_{i}\mathbf{A}_{i,i}\)
* \(\|\mathbf{X}\|_{*}\) : Nuclear norm: sum of the singular values of matrix \(\mathbf{X}\)
* \(\|\mathbf{X}\|\) : Operator/Spectral norm: the largest singular value of \(\mathbf{X}\)
* \(\|\mathbf{X}\|_{\text{F}}\) : Frobenius norm: \((\sum_{ij}\mathbf{X}_{ij}^{2})^{1/2}\)
* \(\nabla f(\mathbf{X})\) : classical (limiting) Euclidean gradient of \(f(\mathbf{X})\) at \(\mathbf{X}\)
* \(\nabla_{\mathcal{J}}f(\mathbf{X})\) : Riemannian gradient of \(f(\mathbf{X})\) at \(\mathbf{X}\)
* \(\mathcal{I}_{\xi}(\mathbf{X})\) : the indicator function of a set \(\xi\) with \(\mathcal{I}_{\xi}(\mathbf{X})=0\) if \(\mathbf{X}\in\xi\) and otherwise \(+\infty\)
* \(\text{dist}(\xi,\xi^{\prime})\) : the distance between two sets with \(\text{dist}(\xi,\xi^{\prime})\triangleq\inf_{\mathbf{X}\in\xi,\mathbf{X}^{ \prime}\in\xi^{\prime}}\|\mathbf{X}-\mathbf{X}^{\prime}\|_{\text{F}}\)
* \(\mathcal{I}_{\xi}(\mathbf{x})\) : the indicator function of a set \(\xi\) with \(\mathcal{I}_{\xi}(\mathbf{x})=0\) if \(\mathbf{x}\in\xi\) and otherwise \(+\infty\).

### Relevant Lemmas

**Lemma A.1**.: _(Lemma 6.6 of [51]) For any \(\mathbf{W}\in\mathbb{R}^{n\times n}\), we have: \(\sum_{i=1}^{C_{h}^{k}}\|\mathbf{W}(\mathcal{B}_{i},\mathcal{B}_{i})\|_{\mathbb{ F}}^{2}=\frac{k}{n}C_{n}^{k}\sum_{i}\mathbf{W}_{ii}^{2}+C_{n-2}^{k-2}\sum_{i}\sum_{j,j \neq i}\mathbf{W}_{ij}^{2}\). Here, the set \(\{\mathcal{B}_{1},\mathcal{B}_{2},\cdots,\mathcal{B}_{C_{h}^{k}}\}\) represents all possible combinations of the index vectors choosing \(k\) items from \(n\) without repetition._

**Lemma A.2**.: _We have \(\mathbb{S}_{+}\) be the set of \(|\mathbb{S}_{+}|=b\) samples from \([N]\), drawn with replacement and uniformly at random. Then, \(\forall t,\mathbf{X}^{t}\in\mathbb{R}^{n\times n}\), we have:_

\[\mathbb{E}_{t^{t}}[\|\tfrac{1}{b}\sum_{i\in\mathbb{S}_{+}}\nabla f_{i}( \mathbf{X}^{t})-\nabla f(\mathbf{X}^{t})\|_{\mathbb{F}}^{2}]=\tfrac{N-b}{b(N- 1)}\mathbb{E}_{t^{t}}[\|\nabla f_{i}(\mathbf{X}^{t})-\nabla f(\mathbf{X}^{t}) \|_{\mathbb{F}}^{2}].\]

Proof.: The proof is exactly the same as in Lemma 2.8 of [5]. 

**Lemma A.3**.: _The tangent space \(\mathbf{T}_{\mathbf{X}}\mathcal{J}\) of manifold constructed by \(\mathbf{X}^{\top}\mathbf{J}\mathbf{X}=\mathbf{J}\), with \(\mathbf{X}\in\mathbb{R}^{n\times n}\), is :_

\[\mathbf{T}_{\mathbf{X}}\mathcal{J}\triangleq\{\mathbf{Y}\in\mathbb{R}^{n\times n }\mid\mathbf{X}^{\top}\mathbf{J}\mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{ X}=0\},\] (13)

_where \(\mathbf{Y}=t\tilde{\mathbf{Y}}\) with \(t\) is a positive scalar approaching 0._

Proof.: Assuming point \(\mathbf{X}\in\mathbb{R}^{n\times n}\) lies on manifold \(\mathcal{J}\), we have: \(h(\mathbf{X})=\mathbf{X}^{\top}\mathbf{J}\mathbf{X}-\mathbf{J}\). Moving along \(\mathbf{Y}\in\mathbb{R}^{n\times n}\) in the tangent space of \(\mathbf{X}\), we obtain:

\[h(\mathbf{X}+\mathbf{Y})= (\mathbf{X}+\mathbf{Y})^{\top}\mathbf{J}(\mathbf{X}+\mathbf{Y})- \mathbf{J}\] \[= \mathbf{X}^{\top}\mathbf{J}\mathbf{X}+\mathbf{X}^{\top}\mathbf{J} \mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{X}+\mathbf{Y}^{\top}\mathbf{J} \mathbf{Y}-\mathbf{J}\] \[\overset{\exists}{=} \mathbf{X}^{\top}\mathbf{J}\mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J} \mathbf{X}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{Y}\] \[\overset{\exists}{=} t\mathbf{X}^{\top}\mathbf{J}\tilde{\mathbf{Y}}+t\tilde{ \mathbf{Y}}^{\top}\mathbf{J}\mathbf{X}+t^{2}\tilde{\mathbf{Y}}^{\top}\mathbf{J }\tilde{\mathbf{Y}}\]

where step 1 uses \(\mathbf{X}^{\top}\mathbf{J}\mathbf{X}=\mathbf{J}\); step 2 uses \(\mathbf{Y}=t\tilde{\mathbf{Y}}\).

Since \(t\) is a positive scalar approaching 0, we can ignore the higher-order term: \(t^{2}\tilde{\mathbf{Y}}^{\top}\mathbf{J}\tilde{\mathbf{Y}}\). According to the properties of the tangent space of any manifold, we have: \(h(\mathbf{X}+\mathbf{Y})=0\), In other words, \(\mathbf{X}^{\top}\mathbf{J}\mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{X}=0\), i.e. we obtain the defining equation for the tangent space: \(\mathbf{T}_{\mathbf{X}}\mathcal{J}\triangleq\{\mathbf{Y}\in\mathbb{R}^{n \times n}\mid\mathbf{X}^{\top}\mathbf{J}\mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J} \mathbf{X}=0\}\). 

## Appendix B Additional Discussions

### On the Global Optimal Solution for Problem (7)

In Section 2.1, we have demonstrated how to use the breakpoint search method to obtain an optimal solution for the case of \(\mathbf{V}=(\begin{smallmatrix}\tilde{c}\\ \tilde{s}\end{smallmatrix})\) of Problem (7). Since the structure of the other three cases \(\mathbf{V}\in\{(\begin{smallmatrix}\tilde{c}\\ -\tilde{s}\end{smallmatrix}&-\tilde{s}\end{smallmatrix}),(\begin{smallmatrix}- \tilde{c}&-\tilde{s}\\ -\tilde{s}\end{smallmatrix}),(\begin{smallmatrix}\tilde{c}&-\tilde{s}\\ \tilde{s}&-\tilde{s}\end{smallmatrix})\}\) is exactly the same except for the coefficients of Problem (8), we will provide the corresponding coefficients in Problem (8): \(\min_{\tilde{c},\tilde{s}}a\,\tilde{c}+b\tilde{s}+c\,\tilde{c}^{2}+d\,\tilde{c} \,\tilde{s}+e\,\tilde{s}^{2}\), and omit the specific analysis process.

**Case (a)**. \(\mathbf{V}=(\begin{smallmatrix}\tilde{c}\\ -\tilde{s}\end{smallmatrix}&\begin{smallmatrix}-\tilde{s}\\ \tilde{c}\end{smallmatrix})\): \(a=\mathbf{P}_{11}+\mathbf{P}_{22}\), \(b=-\mathbf{P}_{12}-\mathbf{P}_{21}\), \(c=\frac{1}{2}(\dot{\mathbf{Q}}_{11}+\dot{\mathbf{Q}}_{41}+\dot{\mathbf{Q}}_{14}+ \dot{\mathbf{Q}}_{14})\), \(d=-\frac{1}{2}(\dot{\mathbf{Q}}_{21}+\dot{\mathbf{Q}}_{31}+\dot{\mathbf{Q}}_{ 12}+\dot{\mathbf{Q}}_{42}+\dot{\mathbf{Q}}_{13}+\dot{\mathbf{Q}}_{43}+\dot{ \mathbf{Q}}_{24}+\dot{\mathbf{Q}}_{34})\), and \(e=\frac{1}{2}(\dot{\mathbf{Q}}_{22}+\dot{\mathbf{Q}}_{32}+\dot{\mathbf{Q}}_{23} +\dot{\mathbf{Q}}_{33})\).

**Case (b)**. \(\mathbf{V}=(\begin{smallmatrix}-\tilde{c}&-\tilde{s}\\ \tilde{s}\end{smallmatrix})\):\(a=-\mathbf{P}_{11}+\mathbf{P}_{22}\), \(b=-\mathbf{P}_{12}+\mathbf{P}_{21}\), \(c=\frac{1}{2}(\dot{\mathbf{Q}}_{11}-\dot{\mathbf{Q}}_{41}-\dot{\mathbf{Q}}_{14}+ \dot{\mathbf{Q}}_{44})\), \(d=\frac{1}{2}(\dot{\mathbf{Q}}_{21}-\dot{\mathbf{Q}}_{31}+\dot{\mathbf{Q}}_{12} -\dot{\mathbf{Q}}_{42}-\dot{\mathbf{Q}}_{13}+\dot{\mathbf{Q}}_{43}-\dot{ \mathbf{Q}}_{24}+\dot{\mathbf{Q}}_{34})\), and \(e=\frac{1}{2}(\dot{\mathbf{Q}}_{22}-\dot{\mathbf{Q}}_{32}-\dot{\mathbf{Q}}_{23} +\dot{\mathbf{Q}}_{33})\).

**Case (c)**. \(\mathbf{V}=(\begin{smallmatrix}\tilde{s}&-\tilde{s}\\ -\tilde{c}\end{smallmatrix})\):\(a=\mathbf{P}_{11}-\mathbf{P}_{22}\), \(b=-\mathbf{P}_{12}+\mathbf{P}_{21}\), \(c=\frac{1}{2}(\dot{\mathbf{Q}}_{11}-\dot{\mathbf{Q}}_{41}-\dot{\mathbf{Q}}_{14}+ \dot{\mathbf{Q}}_{44})\), \(d=\frac{1}{2}(-\dot{\mathbf{Q}}_{21}+\dot{\mathbf{Q}}_{31}-\dot{\mathbf{Q}}_{12} +\dot{\mathbf{Q}}_{42}+\dot{\mathbf{Q}}_{13}-\dot{\mathbf{Q}}_{43}+\dot{ \mathbf{Q}}_{24}-\dot{\mathbf{Q}}_{34})\), and \(e=\frac{1}{2}(\dot{\mathbf{Q}}_{22}-\dot{\mathbf{Q}}_{32}-\dot{\mathbf{Q}}_{23} +\dot{\mathbf{Q}}_{33})\).

## Appendix C Proofs for Section 2

### Proof of Lemma 2.1

Proof.: Defining \(\mathbf{J}_{\texttt{BB}}=\mathbf{J}(\mathbf{U}_{\texttt{B}},\mathbf{U}_{\texttt{B}})\), then we have: \(\mathbf{J}\mathbf{U}_{\texttt{B}}=\mathbf{U}_{\texttt{B}}\mathbf{J}_{\texttt{BB}}\), \(\mathbf{U}_{\texttt{B}}^{\top}\mathbf{J}=\mathbf{J}_{\texttt{BB}}\mathbf{U}_{ \texttt{B}}^{\top}\), and \(\mathbf{U}_{\texttt{B}}^{\top}\mathbf{J}\mathbf{U}_{\texttt{B}}=\mathbf{J}_{ \texttt{BB}}\).

**Part (a)**. For any \(\mathbf{V}\in\mathbb{R}^{2\times 2}\) and \(\mathtt{B}\in\left\{\mathcal{B}_{i}\right\}_{i=1}^{\mathbf{C}_{2}^{2}}\), we have:

\[[\mathbf{X}^{+}]^{\top}\mathbf{J}\mathbf{X}^{+}-\mathbf{X}^{\top} \mathbf{J}\mathbf{X}\] \[= \mathbf{X}^{\top}\mathbf{J}\mathbf{U}_{\mathtt{B}}(\mathbf{V}- \mathbf{I}_{2})\mathbf{U}_{\mathtt{B}}^{\top}\mathbf{X}+[\mathbf{U}_{\mathtt{B} }(\mathbf{V}-\mathbf{I}_{2})\mathbf{U}_{\mathtt{B}}^{\top}\mathbf{X}]^{\top} \mathbf{J}\mathbf{X}\] \[+[\mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2})\mathbf{U}_{ \mathtt{B}}^{\top}\mathbf{X}]^{\top}\mathbf{J}[\mathbf{U}_{\mathtt{B}}( \mathbf{V}-\mathbf{I}_{2})\mathbf{U}_{\mathtt{B}}^{\top}\mathbf{X}]\] \[= \mathbf{X}^{\top}[\mathbf{J}\mathbf{U}_{\mathtt{B}}(\mathbf{V}- \mathbf{I}_{2})\mathbf{U}_{\mathtt{B}}^{\top}+\mathbf{U}_{\mathtt{B}}( \mathbf{V}-\mathbf{I}_{2})^{\top}\mathbf{U}_{\mathtt{B}}^{\top}\mathbf{J}+ \mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2})^{\top}\mathbf{U}_{\mathtt{ B}}^{\top}\mathbf{J}\mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2})\mathbf{U}_{ \mathtt{B}}^{\top}]\mathbf{X}\] \[= \mathbf{X}^{\top}[\mathbf{U}_{\mathtt{B}}[\mathbf{J}_{\mathtt{B} }(\mathbf{V}-\mathbf{I}_{2})+(\mathbf{V}-\mathbf{I}_{2})^{\top}\mathbf{J}_{ \mathtt{B}\mathtt{B}}+(\mathbf{V}-\mathbf{I}_{2})^{\top}\mathbf{J}_{ \mathtt{B}\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2})]\mathbf{U}_{\mathtt{B}}^{\top} \mathbf{X}\] \[= \mathbf{X}^{\top}\mathbf{U}_{\mathtt{B}}[\mathbf{V}^{\top}\mathbf{J }_{\mathtt{B}\mathtt{B}}-\mathbf{J}_{\mathtt{B}\mathtt{B}}]\mathbf{U}_{ \mathtt{B}}^{\top}\mathbf{X}\] \[\stackrel{{\mathtt{\circ}}}{{=}} \mathbf{0}.\]

**Part (b)**. Using the update rule for \(\mathbf{X}^{+}=\mathbf{X}+\mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2}) \mathbf{U}_{\mathtt{B}}^{\top}\mathbf{X}\in\mathbb{R}^{n\times n}\), we derive:

\[\|\mathbf{X}^{+}-\mathbf{X}\|_{\mathtt{F}} = \|\mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2})\mathbf{U}_{ \mathtt{B}}^{\top}\mathbf{X}\|_{\mathtt{F}}\] \[\stackrel{{\mathtt{\circ}}}{{\leq}} \|\mathbf{U}_{\mathtt{B}}\|_{\mathtt{F}}\cdot\|(\mathbf{V}- \mathbf{I}_{2})\mathbf{U}_{\mathtt{B}}^{\top}\mathbf{X}\|_{\mathtt{F}},\] \[\stackrel{{\mathtt{\circ}}}{{\leq}} \|\mathbf{U}_{\mathtt{B}}\|_{\mathtt{F}}\cdot\|(\mathbf{V}- \mathbf{I}_{2})\|_{\mathtt{F}}\cdot\|\mathbf{U}_{\mathtt{B}}^{\top}\|_{\mathtt{ F}}\cdot\|\mathbf{X}\|_{\mathtt{F}},\] \[\stackrel{{\mathtt{\circ}}}{{=}} \|\mathbf{V}-\mathbf{I}_{2}\|_{\mathtt{F}}\cdot\|\mathbf{X}\|_{ \mathtt{F}},\]

where step 1 and step 2 use the norm inequality that \(\|\mathbf{A}\mathbf{X}\|_{\mathtt{F}}\leq\|\mathbf{A}\|_{\mathtt{F}}\cdot\| \mathbf{X}\|_{\mathtt{F}}\) for any \(\mathbf{A}\) and \(\mathbf{X}\); step 3 uses \(\|\mathbf{U}_{\mathtt{B}}\|=\|\mathbf{U}_{\mathtt{B}}^{\top}\|=1\).

**Part (c)**. We define \(\mathbf{Z}\stackrel{{\mathtt{\circ}}}{{=}}\mathbf{U}_{\mathtt{B} }^{\top}\mathbf{X}\). We derive:

\[\|\mathbf{X}^{+}-\mathbf{X}\|_{\mathbf{H}}^{2} = \|\mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2})\mathbf{Z}\|_{ \mathbf{H}}^{2}\] \[\stackrel{{\mathtt{\circ}}}{{=}} \text{vec}(\mathbf{U}_{\mathtt{B}}(\mathbf{V}-\mathbf{I}_{2}) \mathbf{Z})^{\top}\mathbf{H}\text{vec}(\mathbf{U}_{\mathtt{B}}(\mathbf{V}- \mathbf{I}_{2})\mathbf{Z})\] \[\stackrel{{\mathtt{\circ}}}{{=}} \text{vec}(\mathbf{V}-\mathbf{I}_{2})^{\top}(\mathbf{Z}^{\top} \otimes\mathbf{U}_{\mathtt{B}})^{\top}\mathbf{H}(\mathbf{Z}^{\top}\otimes \mathbf{U}_{\mathtt{B}})\text{vec}(\mathbf{V}-\mathbf{I}_{2})\] \[= \|\mathbf{V}-\mathbf{I}_{2}\|_{(\mathbf{Z}^{\top}\otimes\mathbf{U }_{\mathtt{B}})^{\top}\mathbf{H}(\mathbf{Z}^{\top}\otimes\mathbf{U}_{\mathtt{ B}})}^{\top}\] \[\stackrel{{\mathtt{\circ}}}{{\leq}} \|\mathbf{V}-\mathbf{I}_{2}\|_{\mathbf{Q}}^{2},\]

where step 1 uses \(\|\mathbf{X}\|_{\mathbf{H}}^{2}=\text{vec}(\mathbf{X})^{\top}\mathbf{H}\text{vec }(\mathbf{X})\); step 2 uses \((\mathbf{Z}^{\top}\otimes\mathbf{R})\text{vec}(\mathbf{U})=\text{vec}(\mathbf{ R}\mathbf{U}\mathbf{Z})\) for all \(\mathbf{R}\), \(\mathbf{Z}\) and \(\mathbf{U}\) of suitable dimensions; step 4 uses the choice of \(\mathbf{Q}\succcurlyeq\underline{\mathbf{Q}}\triangleq(\mathbf{Z}^{\top} \otimes\mathbf{U}_{\mathtt{B}})^{\top}\mathbf{H}(\mathbf{Z}^{\top}\otimes \mathbf{U}_{\mathtt{B}})\). 

### Proof of Lemma 2.3

Proof.: We denote \(w=c+e\). According to the properties of trigonometric functions, we have: _(i)_\(\tilde{\mathtt{c}}^{2}=\frac{1}{1-\tilde{\mathtt{t}}^{2}}\); _(ii)_\(\tilde{\mathtt{s}}^{2}=\frac{\tilde{\mathtt{t}}^{2}}{1-\tilde{\mathtt{t}}^{2}}\); _(iii)_\(\tilde{\mathtt{t}}=\frac{\tilde{\mathtt{s}}}{\tilde{c}}\), leading to: \(\tilde{\mathtt{c}}=\frac{\pm 1}{\sqrt{1-\tilde{\mathtt{t}}^{2}}},\tilde{\mathtt{s}}=\frac{\pm \tilde{\mathtt{t}}}{\sqrt{1-\tilde{\mathtt{t}}^{2}}}\) with \(|\tilde{\mathtt{t}}\,|<1\).

We discuss two cases for Problem (8).

**Case (a)**. \(\tilde{\mathtt{c}}=\frac{1}{\sqrt{1-\tilde{\mathtt{t}}^{2}}},\tilde{\mathtt{s}}= \frac{\tilde{\mathtt{t}}}{\sqrt{1-\tilde{\mathtt{t}}^{2}}}\). Problem (8) is equivalent to the following problem: \(\bar{\mu}_{+}=\arg\min_{\mu}\frac{a+\tilde{\mathtt{t}}\,b}{\sqrt{1-\tilde{ \mathtt{t}}^{2}}}+\frac{w+\tilde{\mathtt{t}}\,d}{1-\tilde{\mathtt{t}}^{2}}-e\). Therefore, the optimal solution \(\bar{\mu}_{+}\)can be computed as:

\[\cosh(\bar{\mu}_{+})=\frac{1}{\sqrt{1-(\tilde{\mathtt{t}}_{+})^{2}}},\text{ and }\sinh(\bar{\mu}_{+})=\frac{\tilde{\mathtt{t}}_{+}}{\sqrt{1-(\tilde{\mathtt{t}}_{+})^{2}}}\] (14)

**Case (b)**. \(\tilde{\mathtt{c}}=\frac{-1}{\sqrt{1-\tilde{\mathtt{t}}^{2}}},\tilde{\mathtt{s}}= \frac{-\tilde{\mathtt{t}}}{\sqrt{1-\tilde{\mathtt{t}}^{2}}}\). Problem (8) is equivalent to the following problem: \(\bar{\mu}_{-}=\arg\min_{\mu}\frac{-a-\tilde{\mathtt{t}}\,b}{\sqrt{1-\tilde{ \mathtt{t}}^{2}}}+\frac{w+\tilde{\mathtt{t}}\,d}{1-\tilde{\mathtt{t}}^{2}}-e\). Therefore, the optimal solution \(\bar{\mu}_{-}\)can be computed as:

\[\cosh(\bar{\mu}_{-})=\frac{-1}{\sqrt{1-(\bar{\mathtt{t}}_{-})^{2}}},\text{ and }\sinh(\bar{\mu}_{-})=\frac{-\tilde{\mathtt{t}}_{-}}{\sqrt{1-(\bar{\mathtt{t}}_{-})^{2}}}.\] (15)We define the objective function as: \(\tilde{F}(\tilde{c},\tilde{s})\triangleq a\tilde{c}+b\tilde{s}+c\tilde{c}^{2}+d \tilde{c}\tilde{s}+e\tilde{s}^{2}\). In view of (14) and (15), the optimal solution pair \([cosh(\bar{\mu},sinh(\bar{\mu})]\) for problem (8) can be computed as:

\[[\cosh(\bar{\mu}),\sinh(\bar{\mu})]=\arg\min_{[c,s]}\tilde{F}(c,s),\]

\[\text{s. t. }[c,s]\in\{[\cosh(\bar{\mu}_{+}),\sinh(\bar{\mu}_{+})],[\cosh(\bar{ \mu}_{-}),\sinh(\bar{\mu}_{-})]\}\]

Importantly, it is not necessary to compute the values \(\bar{\mu}_{+}\) for (14) and \(\bar{\mu}_{-}\) for (15).

### Proof of Lemma 2.4

Proof.: The objective function for \(\mathsf{B}^{\mathsf{f}}_{(i)}\) as in Equation (3) is formulated as :

\[f(\mathbf{X}^{t})+\tfrac{1}{2}\|\mathbf{V}_{i}-\mathbf{I}\|^{2}_{\mathsf{Q}+ \theta\mathbf{I}}+\langle\mathbf{V}_{i}-\mathbf{I},[\nabla f(\mathbf{X}^{t}) ^{\mathsf{T}}]_{\mathsf{B}^{\mathsf{f}}_{(i)}\mathsf{B}^{\mathsf{f}}_{(i)}}\rangle\]

**Part (1).** For the part of \(\tfrac{1}{2}\|\mathbf{V}_{i}-\mathbf{I}\|^{2}_{\mathsf{Q}+\theta\mathbf{I}}\), it is obviously irrelevant.

**Part (2).** For the part of \(\langle\mathbf{V}_{i}-\mathbf{I},[\nabla f(\mathbf{X}^{t})^{\mathsf{T}}]_{ \mathsf{B}^{\mathsf{f}}_{(i)}\mathsf{B}^{\mathsf{f}}_{(i)}}\rangle\), we note that \([\nabla f(\mathbf{X}^{t})(\mathbf{X}^{t})^{\top}]_{\mathsf{B}^{\mathsf{f}}_{( i)}\mathsf{B}^{\mathsf{f}}_{(i)}}=[\nabla f(\mathbf{X}^{t})](\mathsf{B}^{\mathsf{f}}_{( i)},:)[(\mathbf{X}^{t})^{\top}](:,\mathsf{B}^{\mathsf{f}}_{(i)})=[\nabla f( \mathbf{X}^{t})](\mathsf{B}^{\mathsf{f}}_{(i)},:)[(\mathbf{X}^{t})(\mathsf{B} ^{\mathsf{f}}_{(i)},:)]^{\top}\), which just use the information of block \(\mathsf{B}^{\mathsf{f}}_{(i)}\). The proof ends. 

### Proof of Lemma 2.5

Proof.: Part (_a_). For the purpose of analysis, we define the following: \(\forall i\in[\frac{n}{2}],\mathbf{K}_{i}=\mathbf{U}_{\mathsf{B}_{(i)}}( \mathbf{V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}\mathbf{X}\).

\[\|\sum_{i=1}^{\frac{n}{2}}[\mathbf{U}_{\mathsf{B}_{(i)}}(\mathbf{ V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}\mathbf{X}]\|^{2}_{F} \stackrel{{\text{\tiny\textregistered}}}{{=}} \left\|\begin{bmatrix}\mathbf{K}_{1}\\ \mathbf{K}_{2}\\ \vdots\\ \mathbf{K}_{\frac{n}{2}}\end{bmatrix}\right\|^{2}_{\mathsf{F}}\] \[\stackrel{{\text{\tiny\textregistered}}}{{=}} \|\mathbf{K}_{1}\|^{2}_{\mathsf{F}}+\|\mathbf{K}_{2}\|^{2}_{ \mathsf{F}}+\cdots+\|\mathbf{K}_{\frac{n}{2}}\|^{2}_{\mathsf{F}}\] \[\stackrel{{\text{\tiny\textregistered}}}{{=}} \sum_{i=1}^{\frac{n}{2}}[\|\mathbf{U}_{\mathsf{B}_{(i)}}(\mathbf{ V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}\mathbf{X}\|^{2}_{ \mathsf{F}}]\]

where step 1 uses the definition of \(\mathbf{K}_{i}\) and the assumption that \(\mathsf{B}\in\Upsilon\); step 2 uses the definition of Squared Frobenius Norm; step 3 uses the definition of \(\mathbf{K}_{i}\).

Part (_b_). Using the update rule for \(\mathbf{X}^{+}=\mathbf{X}+[\sum_{i=1}^{n/2}\mathbf{U}_{\mathsf{B}_{(i)}}( \mathbf{V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}]\mathbf{X} \in\mathbb{R}^{n\times n}\), we have the following inequalities:

\[\|\mathbf{X}^{+}-\mathbf{X}\|^{2}_{\mathsf{F}} = \|[\sum_{i=1}^{n/2}\mathbf{U}_{\mathsf{B}_{(i)}}(\mathbf{V}_{i}- \mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}]\mathbf{X}\|^{2}_{\mathsf{F}}\] (16) \[\stackrel{{\text{\tiny\textregistered}}}{{=}} \sum_{i=1}^{n/2}\|[\mathbf{U}_{\mathsf{B}_{(i)}}(\mathbf{V}_{i}- \mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}]\mathbf{X}\|^{2}_{\mathsf{F}}\] (17) \[\stackrel{{\text{\tiny\textregistered}}}{{\leq}} \sum_{i=1}^{n/2}\|\mathbf{V}_{i}-\mathbf{I}_{2}\|^{2}_{\mathsf{F}} \cdot\|\mathbf{X}\|^{2}_{\mathsf{F}},\] (18)

where step 1 uses the conclusion of Part (_a_); step 2 uses the same proof process of Part (_b_) of lemma 2.1.

Part (_c_). We derive the following results:

\[\frac{1}{2}\|\mathbf{X}^{+}-\mathbf{X}\|^{2}_{\mathbf{H}} =\tfrac{1}{2}\|[\sum_{i=1}^{n/2}\mathbf{U}_{\mathsf{B}_{(i)}}( \mathbf{V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}]\mathbf{X}\|^{2}_ {\mathbf{H}}\] \[\stackrel{{\text{\tiny\textregistered}}}{{=}} \tfrac{1}{2}\sum_{i=1}^{n/2}\|[\mathbf{U}_{\mathsf{B}_{(i)}}( \mathbf{V}_{i}-\mathbf{I}_{2})\mathbf{U}_{\mathsf{B}_{(i)}}^{\top}]\mathbf{X}\|^{2}_ {\mathbf{H}}\] \[\stackrel{{\text{\tiny\textregistered}}}{{\leq}} \tfrac{1}{2}\sum_{i=1}^{n/2}\|\mathbf{V}_{i}-\mathbf{I}_{2}\|^{2}_ {\mathsf{Q}}\]where step 1 uses the conclusion of Part (_a_); step 2 uses the same proof process of Part (_c_) of lemma 2.1.

Part (_d_). We derive the following results:

\[\sum_{i=1}^{n/2}\langle\mathbf{V}_{i}-\mathbf{I}_{2},[(\nabla f( \mathbf{X})-\tilde{\mathbf{G}})\mathbf{X}^{\top}]_{\mathbf{B}_{\mathbf{i}} \mathbf{B}_{\mathbf{i}}}\rangle\] (19) \[= \sum_{i=1}^{n/2}\langle[\mathbf{U}_{\mathbf{B}_{(i)}}(\mathbf{V} _{i}-\mathbf{I}_{2})\mathbf{U}_{\mathbf{B}_{(i)}}^{\top}]\mathbf{X},[(\nabla f (\mathbf{X})-\tilde{\mathbf{G}})]\rangle\] \[= \langle\mathbf{X}^{+}-\mathbf{X},[(\nabla f(\mathbf{X})-\tilde{ \mathbf{G}})]\rangle\] \[\stackrel{{\text{\tiny{\textcircled}}}}{{\leq}} \tfrac{1}{2}\|\mathbf{X}^{+}-\mathbf{X}\|_{\tilde{\mathbf{F}}}^{2}+ \tfrac{1}{2}\||\nabla f(\mathbf{X})-\tilde{\mathbf{G}}||_{\tilde{\mathbf{F}}}^ {2}\] \[\stackrel{{\text{\tiny{\textcircled}}}}{{\leq}} \tfrac{1}{2}\|\mathbf{X}\|_{\tilde{\mathbf{F}}}^{2}\sum_{i=1}^{n/2} \|\mathbf{V}_{i}-\mathbf{I}_{2}\|_{\tilde{\mathbf{F}}}^{2}+\tfrac{1}{2}\|| \nabla f(\mathbf{X})-\tilde{\mathbf{G}}||_{\tilde{\mathbf{F}}}^{2}\]

where step 1 uses \(\forall\mathbf{A},\mathbf{B},\frac{1}{2}\|\mathbf{A}-\mathbf{B}\|_{\tilde{ \mathbf{F}}}^{2}=\frac{1}{2}\|\mathbf{A}\|_{\tilde{\mathbf{F}}}^{2}+\frac{1}{2 }\|\mathbf{B}\|_{\tilde{\mathbf{F}}}^{2}-\langle\mathbf{A},\mathbf{B}\rangle\geq 0\), with \(\mathbf{A}=\|\mathbf{X}^{+}-\mathbf{X}\|_{\tilde{\mathbf{F}}}^{2}\) and \(\mathbf{B}=\|[\nabla f(\mathbf{X})-\tilde{\mathbf{G}}]\|_{\tilde{\mathbf{F}}}^ {2}\); step 2 uses the conclusion of Part (_b_). 

## Appendix D Proofs for Section 3

### Proof of Lemma 3.1

Proof.: We consider the Lagrangian function of problem (1):

\[\mathcal{L}(\mathbf{X},\Lambda)=f(\mathbf{X})-\tfrac{1}{2}\langle\Lambda, \mathbf{X}^{\top}\mathbf{J}\mathbf{X}-\mathbf{J}\rangle.\] (20)

Setting the gradient of \(\mathcal{L}(\mathbf{X},\Lambda)\)_w.r.t._\(\mathbf{X}\) to zero yields:

\[\nabla f(\mathbf{X})-\mathbf{J}\mathbf{X}\Lambda=\mathbf{0}.\] (21)

**Part (a)**. Multiplying both sides by \(\mathbf{X}^{\top}\) and using the fact that \(\mathbf{X}^{\top}\mathbf{J}\mathbf{X}=\mathbf{J}\), we have \(\mathbf{J}\Lambda=\mathbf{X}^{\top}\nabla f(\mathbf{X})\). Multiplying both sides by \(\mathbf{J}^{\top}\) and using \(\mathbf{J}^{\top}\mathbf{J}=\mathbf{I}\), we have \(\Lambda=\mathbf{J}\mathbf{X}^{\top}\nabla f(\mathbf{X})\). Since \(\Lambda\) is symmetric, we have \(\Lambda=\nabla f(\mathbf{X})^{\top}\mathbf{X}\mathbf{J}\). Putting this equality into Equality (21) yields the following first-order optimality condition for Problem (1):

\[\nabla f(\mathbf{X})=\mathbf{J}\mathbf{X}[\nabla f(\mathbf{X})]^{\top}\mathbf{ X}\mathbf{J}.\] (22)

**Part (b)**. We let \(\mathbf{G}=\nabla f(\mathbf{X})\). We derive the following results:

\[\begin{array}{rcl}\mathbf{G}=\mathbf{J}\mathbf{X}\mathbf{G}^{\top}\mathbf{X} \mathbf{J}&\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}& \mathbf{J}\mathbf{X}^{\top}\cdot\mathbf{G}=\mathbf{J}\mathbf{X}^{\top}\cdot \mathbf{J}\mathbf{X}\mathbf{G}^{\top}\mathbf{X}\mathbf{J}\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{J }\mathbf{X}^{\top}\mathbf{G}=\mathbf{G}^{\top}\mathbf{X}\mathbf{J}\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{X }(\mathbf{J}\mathbf{X}^{\top}\mathbf{G}\mathbf{X}^{\top}\mathbf{J})\mathbf{X}^{ \top}=\mathbf{X}(\mathbf{G}^{\top}\mathbf{X}\mathbf{J})\mathbf{X}^{\top}\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{X }\underbrace{\mathbf{J}\mathbf{X}^{\top}\mathbf{G}\mathbf{X}\mathbf{J}}_{ \triangleq\mathbf{G}^{\top}}\mathbf{J}=\mathbf{J}\underbrace{\mathbf{J} \mathbf{X}\mathbf{G}^{\top}\mathbf{X}\mathbf{J}}_{\triangleq\mathbf{G}} \mathbf{X}^{\top}\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&(\mathbf{X }\mathbf{G}^{\top}\mathbf{J})\cdot\mathbf{J}\mathbf{X}=(\mathbf{J}\mathbf{G} \mathbf{X}^{\top})\cdot\mathbf{J}\mathbf{X}\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{X }\mathbf{G}^{\top}\mathbf{X}=\mathbf{J}\mathbf{G}\mathbf{J}\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{J }\mathbf{X}\mathbf{G}^{\top}\mathbf{X}\mathbf{J}=\mathbf{G},\end{array}\] (23)

where step 1 uses the results of left-multiplying both sides by \(\mathbf{J}\mathbf{X}^{\top}\); step 2 uses \(\mathbf{J}\cdot\mathbf{X}^{\top}\mathbf{J}\mathbf{X}=\mathbf{J}\mathbf{J}=\mathbf{I}\); step 3 uses the results of left-multiplying both sides by \(\mathbf{X}\) and subsequently right-multiplying them by \(\mathbf{X}^{\top}\); 4 uses \(\mathbf{G}=\mathbf{J}\mathbf{X}\mathbf{G}^{\top}\mathbf{X}\mathbf{J}\); step 5 uses the the results of right-multiplying both sides by \(\mathbf{J}\mathbf{X}\); step 6 uses \(\mathbf{J}\mathbf{J}=\mathbf{I}\) and \(\mathbf{X}^{\top}\mathbf{J}\mathbf{X}=\mathbf{J}\); step 7 uses the results of left-multiply both sides by \(\mathbf{J}\) and right-multiplied by \(\mathbf{J}\).

Given Equality (23), we conclude that the critical point condition is equivalent to the requirement that the matrix \(\mathbf{X}\nabla f(\hat{\mathbf{X}})^{\top}\mathbf{J}\) is symmetric, which is expressed as \(\mathbf{X}\mathbf{G}^{\top}\mathbf{J}=[\mathbf{X}\mathbf{G}^{\top}\mathbf{J}]^{\top}\). 

### Proof of Theorem 3.3

Proof.: We use \(\ddot{\mathbf{X}}\) and \(\ddot{\mathbf{X}}\) to denote any BS-point and critical point, respectively.

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

**Lemma E.2**.: _(Riemannian gradient Lower Bound for the Iterates Gap) We define \(\phi\triangleq(3\overline{\mathrm{X}}+\overline{\mathrm{Y}}\overline{\mathrm{X}}) \overline{\mathrm{G}}+(1+\overline{\nabla}^{2}+\frac{n}{2}(\overline{\mathrm{X} }^{2}+\overline{\nabla}^{2}\overline{\mathrm{X}}^{2}))L_{f}+(1+\overline{ \nabla}^{2})\theta\). It holds that: \(\mathbb{E}_{{}_{t^{1}+1}}[\mathrm{dist}(\mathbf{0},\nabla_{\mathcal{J}} \mathcal{T}(\mathbf{I}_{2};\mathbf{X}^{t+1},\mathtt{B}^{t+1}))]\leq\phi\cdot \mathbb{E}_{{}_{t^{1}}}[\sum_{i=1}^{n/2}\|\bar{\mathbf{V}}_{i}^{t}-\mathbf{I}_{2 }\|_{\mathbf{F}}]+\frac{np\sqrt{u^{\dagger}}}{2}(\overline{\mathrm{X}}+\overline {\nabla}^{2}\overline{\mathrm{X}})\)._

Proof.: For notation simplicity, we define:

\[\Omega_{i0}\triangleq \mathbf{U}_{\mathbf{B}_{(i)}}^{\top}[\tilde{\mathbf{G}}^{t+1}][ \mathbf{X}^{t+1}]^{\top}\mathbf{U}_{\mathbf{B}_{(i)}},\forall i\] (44) \[\Omega_{i1}\triangleq \mathbf{U}_{\mathbf{B}_{(i)}}^{\top}[\tilde{\mathbf{G}}^{t+1}][ \mathbf{X}^{t}]^{\top}\mathbf{U}_{\mathbf{B}_{(i)}},\forall i,\] (45) \[\Omega_{i2}\triangleq \mathbf{U}_{\mathbf{B}_{(i)}}^{\top}[\tilde{\mathbf{G}}^{t}- \tilde{\mathbf{G}}^{t+1}][\mathbf{X}^{t}]^{\top}\mathbf{U}_{\mathbf{B}_{(i)}},\forall i.\] (46)

First, using the optimality of \(\bar{\mathbf{V}}_{i}^{t},i\in\{1,\cdots,\frac{n}{2}\}\) for the subproblem, we have:

\[\mathbf{0}_{2,2}=\tilde{\mathbf{G}}_{i}-\mathbf{J}_{\mathtt{B}_{(i)}}\bar{ \mathbf{V}}_{i}^{t}\tilde{\mathbf{G}}_{i}^{\top}\bar{\mathbf{V}}_{i}^{t} \mathbf{J}_{\mathtt{B}_{(i)}}\] (47)

\[\text{where }\tilde{\mathbf{G}}_{i}=\underbrace{\mathrm{mat}((\mathbf{Q}+ \theta\mathbf{I}_{2})\operatorname{vec}(\bar{\mathbf{V}}_{i}^{t}-\mathbf{I}_{ 2}))}_{\triangleq\Upsilon_{i1}}+\underbrace{\mathbf{U}_{\mathbf{B}_{(i)}}^{ \top}\tilde{\mathbf{G}}^{t}(\mathbf{X}^{t})^{\top}\mathbf{U}_{\mathbf{B}_{(i) }}}_{\triangleq\Upsilon_{i2}}.\] (48)

Using the relation that \(\tilde{\mathbf{G}}_{i}=\Upsilon_{i1}+\Upsilon_{i2}\), we obtain the following results from the above equality:

\[\mathbf{0}_{2,2}=(\Upsilon_{i1}+\Upsilon_{i2})-\mathbf{J}_{\mathtt{B }_{(i)}}\bar{\mathbf{V}}_{i}^{t}(\Upsilon_{i1}+\Upsilon_{i2})^{\top}\bar{ \mathbf{V}}_{i}^{t}\mathbf{J}_{\mathtt{B}_{(i)}}\] \[\overset{\text{\text@underline{\text@underline{\text@underline{ \text@underline{\text@underline{\text@underline{\text@underline{\text@underline{ \text@underline{\text@underline{\text

[MISSING_PAGE_EMPTY:23]

[MISSING_PAGE_FAIL:24]

\(\frac{1}{2}\|\mathbf{Y}-\mathbf{G}\|_{F}^{2}+\langle\mathbf{X}^{\top}\mathbf{J} \mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{X},\boldsymbol{\Lambda}\rangle\). We naturally derive the following first-order optimality condition: \(\mathbf{Y}-\mathbf{G}+\mathbf{J}\mathbf{X}\boldsymbol{\Lambda}=\mathbf{0}\), \(\mathbf{X}^{\top}\mathbf{J}\mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{X}= \mathbf{0}\). Incorporating the term \(\mathbf{Y}=\mathbf{G}-\mathbf{J}\mathbf{X}\boldsymbol{\Lambda}\) into \(\mathbf{X}^{\top}\mathbf{J}\mathbf{Y}+\mathbf{Y}^{\top}\mathbf{J}\mathbf{X}= \mathbf{0}\), we obtain:

\[\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\Lambda}+\boldsymbol{\Lambda}^{\top} \mathbf{X}^{\top}\mathbf{X}=\mathbf{G}^{\top}\mathbf{J}\mathbf{X}+\mathbf{X}^ {\top}\mathbf{J}\mathbf{G}\] (55)

Any \(\boldsymbol{\Lambda}\) satisfying formula (55) is a feasible point, so we can easily find :

\[\begin{array}{ll}&\mathbf{X}^{\top}\mathbf{X}\boldsymbol{\Lambda}=\mathbf{X} ^{\top}\mathbf{J}\mathbf{G}\\ \stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{X} \boldsymbol{\Lambda}=\mathbf{J}\mathbf{G}\\ \stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{X} ^{\top}\mathbf{J}\mathbf{X}\boldsymbol{\Lambda}=\mathbf{X}^{\top}\mathbf{J} \mathbf{J}\mathbf{G}\\ \stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{J} \boldsymbol{\Lambda}=\mathbf{X}^{\top}\mathbf{G}\\ \stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{ \Lambda}=\mathbf{J}\mathbf{X}^{\top}\mathbf{G}\\ \stackrel{{\text{\tiny{\textcircled}}}}{{\Rightarrow}}&\mathbf{ \Lambda}=\mathbf{G}^{\top}\mathbf{X}\mathbf{J}\end{array}\] (56)

where step 1 uses the fact that any matrix \(\mathbf{X}\) satisfying the J-orthogonality constraint has a determinant of 1 or -1, thus \(\text{inv}(\mathbf{X})\) exists; step 2 multiply both sides of the equation by \(\mathbf{X}\mathbf{J}\);step 3 uses \(\mathbf{X}^{T}\mathbf{J}\mathbf{X}=\mathbf{J}\) and \(\mathbf{J}\mathbf{J}=\mathbf{I}\); step 4 multiply both sides of the equation by \(\mathbf{J}\) and uses \(\mathbf{J}\mathbf{J}=\mathbf{I}\); step 5 uses the fact that \(\boldsymbol{\Lambda}\) is a symmetric matrix.

Therefore, a feasible solution \(\mathbf{Y}\) can be computed as \(\mathbf{Y}=\mathbf{G}-\mathbf{J}\mathbf{X}\boldsymbol{\Lambda}=\mathbf{G}- \mathbf{J}\mathbf{X}\mathbf{G}^{\top}\mathbf{X}\mathbf{J}\). Since \(\bar{\mathbf{Y}}\) is the optimal solution, there must be \(h(\bar{\mathbf{Y}})\leq h(\mathbf{G}-\mathbf{J}\mathbf{X}\mathbf{G}^{\top} \mathbf{X}\mathbf{J})\). 

We now present the proof of this lemma.

**Lemma E.5**.: _For any \(\mathbf{X}\in\mathbb{R}^{n\times n}\), it holds that \(\operatorname{dist}(\mathbf{0},\nabla f^{\circ}(\mathbf{X}))\leq\operatorname{ dist}(\mathbf{0},\nabla_{\mathcal{J}}f(\mathbf{X}))\)._

Proof.: For the purpose of analysis, we define the nearest J orthogonal matrix to an arbitrary matrix \(\mathbf{Y}\in\mathbb{R}^{n\times n}\) is given by \(\mathcal{P}_{\mathcal{J}}(\mathbf{X})\). Similarly, we have \(\mathcal{P}_{\mathbf{T}\mathbf{X}\mathcal{J}}(\nabla f(\mathbf{X}))\) for projecting gradient \(\nabla f(\mathbf{X})\) into space \(\mathbf{T}_{\mathbf{X}}\mathcal{J}\).

We recall that the following first-order optimality conditions are equivalent for all \(\mathbf{X}\in\mathbb{R}^{n\times n}\) :

\[(\mathbf{0}\in\nabla f^{\circ}(\mathbf{X}))\Leftrightarrow(\mathbf{0}\in \mathcal{P}_{\mathbf{T}_{\mathbf{X}}\mathcal{J}}(\nabla f(\mathbf{X}))).\] (57)

Therefore, we derive the following results:

\[\operatorname{dist}(\mathbf{0},\nabla f^{\circ}(\mathbf{X})) = \inf_{\mathbf{Y}\in\nabla f^{\circ}(\mathbf{X})}\|\mathbf{Y}\|_{ \mathbf{F}}\] (58) \[= \inf_{\mathbf{Y}\in\mathcal{P}_{(\mathbf{T}_{\mathbf{X}}\mathcal{ J})}(\nabla f(\mathbf{X}))}\|\mathbf{Y}\|_{\mathbf{F}}\] (59)

We let \(\mathbf{G}\in\nabla f(\mathbf{X})\) and obtain the following results from the above equality:

\[\operatorname{dist}(\mathbf{0},\nabla f^{\circ}(\mathbf{X})) \stackrel{{\text{\tiny{\textcircled}}}}{{\leq}} \|\mathbf{G}-\mathbf{J}\mathbf{X}\mathbf{G}^{\top}\mathbf{X} \mathbf{J}\|_{\mathbf{F}},\] (60) \[\stackrel{{\text{\tiny{\textcircled}}}}{{=}} \|\nabla_{\mathcal{J}}f(\mathbf{X})\|_{\mathbf{F}}\triangleq \operatorname{dist}(\mathbf{0},\nabla_{\mathcal{J}}f(\mathbf{X})).\] (61)

where step 1 uses Lemma E.4; step 2 uses \(\nabla_{\mathcal{J}}f(\mathbf{X})=\mathbf{G}-\mathbf{J}\mathbf{X}\mathbf{G}^{ \top}\mathbf{X}\mathbf{J}\) with \(\mathbf{G}\in\nabla f(\mathbf{X})\). 

First of all, since \(f^{\circ}(\mathbf{X})\triangleq f(\mathbf{X})+\mathcal{I}_{\mathcal{J}}(\mathbf{X})\) is a KL function, we have from Proposition 4.8 that:

\[\begin{array}{ll}\frac{1}{\varphi^{\prime}(f^{\circ}(\mathbf{X}^{\prime})-f^{ \circ}(\mathbf{X}))}&\leq&\operatorname{dist}(0,\nabla f^{\circ}(\mathbf{X}^{ \prime}))\\ &\stackrel{{\text{\tiny{\textcircled}}}}{{=}}&\|\nabla_{\mathcal{J}}f (\mathbf{X}^{\prime})\|_{\mathbf{F}},\end{array}\] (62)

where step 1 uses Lemma E.5. Here, \(\varphi(\cdot)\) is some certain concave desingularization function. Since \(\varphi(\cdot)\) is concave, we have:

\[\forall\Delta\in\mathbb{R},\Delta^{+}\in\mathbb{R},\varphi(\Delta^{+})+(\Delta- \Delta^{+})\varphi^{\prime}(\Delta)\leq\varphi(\Delta).\] (63)

Applying the inequality above with \(\Delta=f(\mathbf{X}^{t})-f(\bar{\mathbf{X}})\) and \(\Delta^{+}=f(\mathbf{X}^{t+1})-f(\bar{\mathbf{X}})\), we have:

\[(f(\mathbf{X}^{t})-f(\mathbf{X}^{t+1}))\varphi^{\prime}(f(\mathbf{ X}^{t})-f(\bar{\mathbf{X}}))\] \[\leq \varphi(f(\mathbf{X}^{t})-f(\bar{\mathbf{X}}))-\varphi(f(\mathbf{ X}^{t+1})-f(\bar{\mathbf{X}}))\triangleq\mathcal{E}^{t}.\] (64)

[MISSING_PAGE_EMPTY:26]

Adding Inequality \(\frac{\sqrt{\theta+1}}{1-\sqrt{1-p}}\times\) (70) to (69)

\[\mathbb{E}_{{}_{t^{t}}}[\sqrt{\theta-\overline{\mathbf{X}}^{2}}\sqrt {\overline{\mathbf{N}}^{t}}] \leq \mathcal{E}^{t}\mathfrak{A}+(\sqrt{\theta^{\prime}}+\sqrt{\frac{ L_{t}^{2}\overline{\mathbf{X}}^{2}(1-p)}{b^{\prime}}}\frac{\sqrt{\theta+1}}{1-\sqrt{1-p}}) \mathbb{E}_{{}_{t^{t-1}}}[\sqrt{\overline{\mathbf{N}}^{t-1}}]+\] (71) \[\frac{\sqrt{1-p}\sqrt{(\theta+1)}}{1-\sqrt{1-p}}(\sqrt{\mathbb{E} _{{}_{t^{t-1}}}[u^{t-1}]}-\sqrt{\mathbb{E}_{{}_{t^{t}}}[u^{t}]})+\frac{\sqrt{ \theta+1}}{1-\sqrt{1-p}}\sqrt{\frac{p(N-b)}{b(N-1)}}\sigma^{2}\]

With the choice \(\sqrt{\theta^{\prime}}=\frac{\sqrt{\theta-\overline{\mathbf{X}}^{2}}}{2}- \sqrt{\frac{L_{t}^{2}\overline{\mathbf{X}}^{2}(1-p)}{b^{\prime}}}\frac{\sqrt{ \theta+1}}{1-\sqrt{1-p}}\), we have:

\[\mathbb{E}_{{}_{t^{t}}}[\sqrt{\theta-\overline{\mathbf{X}}^{2}} \sqrt{\overline{\mathbf{N}}^{t}}] \leq \mathcal{E}^{t}\mathfrak{A}+(\frac{\sqrt{\theta-\overline{\mathbf{ X}}^{2}}}{2})\mathbb{E}_{{}_{t^{t-1}}}[\sqrt{\overline{\mathbf{N}}^{t-1}}]+\] (72) \[\frac{\sqrt{1-p}\sqrt{(\theta+1)}}{1-\sqrt{1-p}}(\sqrt{\mathbb{E }_{{}_{t^{t-1}}}[u^{t-1}]}-\sqrt{\mathbb{E}_{{}_{t^{t}}}[u^{t}]})+\frac{\sqrt{ \theta+1}}{1-\sqrt{1-p}}\sqrt{\frac{p(N-b)}{b(N-1)}}\sigma^{2}\]

Rearranging terms, we have:

\[\mathbb{E}_{{}_{t^{t}}}[\sqrt{\theta-\overline{\mathbf{X}}^{2}} \sqrt{\overline{\mathbf{N}}^{t}}]-\mathbb{E}_{{}_{t^{t-1}}}[\frac{\sqrt{ \theta-\overline{\mathbf{X}}^{2}}}{2}\sqrt{\overline{\mathbf{N}}^{t-1}}]\] (73) \[\leq \mathcal{E}^{t}\mathfrak{A}+\frac{\sqrt{1-p}\sqrt{(\theta+1)}}{1- \sqrt{1-p}}(\sqrt{\mathbb{E}_{{}_{t^{t-1}}}[u^{t-1}]}-\sqrt{\mathbb{E}_{{}_{t ^{t}}}[u^{t}]})+\frac{\sqrt{\theta+1}}{1-\sqrt{1-p}}\sqrt{\frac{p(N-b)}{b(N-1) }}\sigma^{2}\]

Summing the inequality above over \(t=1,2\ldots,T\), we have:

\[\mathbb{E}_{{}_{t^{T}}}[\sqrt{\theta-\overline{\mathbf{X}}^{2}} \sqrt{\overline{\mathbf{N}}^{T}}]+\mathbb{E}_{{}_{t^{T-1}}}[\frac{\sqrt{ \theta-\overline{\mathbf{X}}^{2}}}{2}\sum_{t=1}^{T-1}\sqrt{\overline{ \mathbf{N}}^{t}}]\] \[\leq \mathfrak{A}\sum_{t=1}^{T}\mathcal{E}^{t}+\frac{\sqrt{1-p}\sqrt{ (\theta+1)}}{1-\sqrt{1-p}}(\sqrt{\mathbb{E}_{{}_{t^{0}}}[u^{0}]}-\sqrt{ \mathbb{E}_{{}_{t^{T}}}[u^{T}]})+\frac{T\sqrt{\theta+1}}{1-\sqrt{1-p}}\sqrt{ \frac{p(N-b)}{b(N-1)}}\sigma^{2}+\frac{\sqrt{\theta-\overline{\mathbf{X}}^{2}} }{2}\sqrt{\overline{\mathbf{N}}^{0}}\] \[\leq \mathfrak{A}\sum_{t=1}^{T}\mathcal{E}^{t}+\frac{\sqrt{1-p}\sqrt{ (\theta+1)}}{1-\sqrt{1-p}}\sqrt{\frac{N-b}{b(N-1)}}\sigma^{2}+\frac{T\sqrt{ \theta+1}}{1-\sqrt{1-p}}\sqrt{\frac{p(N-b)}{b(N-1)}}\sigma^{2}+\mathbb{E}_{{}_ {t^{1}}}[\frac{\sqrt{\theta-\overline{\mathbf{X}}^{2}}}{2}\sqrt{\overline{ \mathbf{N}}^{0}}]\] \[\leq \mathfrak{A}\sum_{t=1}^{T}\mathcal{E}^{t}+\frac{\sqrt{1-p}\sqrt{ (\theta+1)}}{1-\sqrt{1-p}}\sqrt{\frac{N-b}{b(N-1)}}\sigma^{2}+\frac{T\sqrt{ \theta+1}}{1-\sqrt{1-p}}\sqrt{\frac{p(N-b)}{b(N-1)}}\sigma^{2}+\frac{\sqrt{ \theta-\overline{\mathbf{X}}^{2}}}{2}\sqrt{\frac{n}{2}(\overline{\mathbf{N}}+ \sqrt{2})^{2}}\]

where step 1 uses the fact that \(\mathbb{E}_{{}_{t^{T}}}[u^{T}]\geq 0\) and \(\mathbb{E}_{{}_{t^{0}}}[u^{0}]\leq\frac{N-b}{b(N-1)}\sigma^{2}\); step 2 uses \(\forall t,\|\mathbf{V}\|_{\mathbf{F}}\leq\bar{\mathbf{V}}\), then, \(\|\mathbf{V}_{i}-\mathbf{I}_{2}\|_{\mathbf{F}}^{2}\leq(\|\mathbf{V}_{i}\|_{ \mathbf{F}}+\|\mathbf{I}_{2}\|_{\mathbf{F}})^{2}\leq(\overline{\mathbf{X}}+ \sqrt{2})^{2}\) and \(\sum_{i=1}^{n/2}\|\bar{\mathbf{V}}_{i}^{0}-\mathbf{I}_{2}\|_{\mathbf{F}}^{2} \leq\frac{n}{2}(\overline{\mathbf{V}}+\sqrt{2})^{2}\). Define \(\mathfrak{C}=\frac{\sqrt{1-p}\sqrt{(\theta+1)}}{1-\sqrt{1-p}}\sqrt{\frac{N-b} {b(N-1)}}\sigma^{2}+\frac{T\sqrt{\theta+1}}{1-\sqrt{1-p}}\sqrt{\frac{p(N-b)}{ b(N-1)}}\sigma^{2}\) and rearrange terms, we have:

\[\mathbb{E}_{{}_{t^{t}}}[\frac{\theta-\overline{\mathbf{X}}^{2}}{2}\sum_{t=1}^{T} \sqrt{\overline{\mathbf{N}}^{t}}]\leq\mathfrak{A}\sum_{t=1}^{T}\mathcal{E}^{t}+ \mathfrak{C}+\frac{\sqrt{\theta-\overline{\mathbf{X}}^{2}}}{2}\sqrt{\frac{n}{2}( \overline{\mathbf{V}}+\sqrt{2})^{2}}\] (74)

Considering \(\mathfrak{A}\sum_{t=1}^{T}\mathcal{E}^{t}\), we have:

\[\mathfrak{A}\sum_{t=1}^{T}\mathcal{E}^{t} \stackrel{{\exists}}{{=}} \mathfrak{A}\sum_{t=1}^{T}\varphi(f(\mathbf{X}^{t})-f(\bar{\mathbf{X}}))- \varphi(f(\mathbf{X}^{t+1})-f(\bar{\mathbf{X}}))\] (75) \[\stackrel{{\exists}}{{=}} \mathfrak{A}[\varphi(f(\mathbf{X}^{1})-f(\bar{\mathbf{X}}))- \varphi(f(\mathbf{X}^{T+1})-f(\bar{\mathbf{X}}))]\] \[\stackrel{{\lx@sectionsign}}{{\leq}} \mathfrak{A}\varphi(f(\mathbf{X}^{1})-f(\bar{\mathbf{X}}))\]

where step 1 uses the definition of \(\mathcal{E}^{i}\) in (64); step 2 uses a basic recursive reduction; step 3 uses the fact the desingularization function \(\varphi(\cdot)\) is positive. Combining Inequality (74) and (75), we obtain :

\[\mathbb{E}_{{}_{t^{t}}}[\frac{\theta-\overline{\mathbf{X}}^{2}}{2}\sum_{t=1}^{T} \sqrt{\overline{\mathbf{N}}^{t}}]\leq\mathfrak{A}\varphi(f(\mathbf{X}^{1})-f( \bar{\mathbf{X}}))+\mathfrak{C}+\frac{\sqrt{\theta-\overline{\mathbf{X}}^{2}}}{2} \sqrt{\frac{n}{2}(\overline{\mathbf{V}}+\sqrt{2})^{2}}\]

Using the inequality that \(\frac{\|\mathbf{X}^{t}-\mathbf{X}\|_{\mathbf{F}}^{2}}{\overline{\mathbf{X}}^{2}} \leq\frac{\|\mathbf{X}^{t}-\mathbf{X}\|_{\mathbf{F}}^{2}}{\|\mathbf{X}\|_{ \mathbf{F}}^{2}}\leq\sum_{i=1}^{n/2}\|\bar{\mathbf{V}}_{i}-\mathbf{I}_{2}\|_{ \mathbf{F}}^{2}\) as shown in Part (_b_) in Lemma 2.5, we have:

\[\mathbb{E}_{{}_{t^{t}}}[\frac{\theta-\overline{\mathbf{X}}^{2}}{2\overline{ \mathbf{X}}}\sum_{t=1}^{T}\|\mathbf{X}^{t+1}-\mathbf{X}^{t}\|_{\mathbf{F}}]\leq \mathfrak{A}\varphi(f(\mathbf{X}^{1})-f(\bar{\mathbf{X}

\[C\stackrel{{\Delta}}{{=}}\frac{2\overline{\mathbb{X}}}{\theta- \overline{\mathbb{X}}^{2}}(\mathfrak{A}\varphi(f(\mathbf{X}^{1})-f(\bar{ \mathbf{X}}))+\frac{\sqrt{\theta-\overline{\mathbb{X}}^{2}}}{2}\sqrt{\frac{n} {2}(\overline{\nabla}+\sqrt{2})^{2}})\]

Considering that: \(\sqrt{\theta^{\prime}}=\frac{\sqrt{\theta-\overline{\mathbb{X}}^{2}}}{2}-\sqrt{ \frac{L_{f}^{2}\overline{\mathbb{X}}^{2}(1-p)}{b^{\prime}}}\frac{\sqrt{\theta +1}}{1-\sqrt{1-p}}=\frac{\sqrt{\theta-\overline{\mathbb{X}}^{2}}}{2}-\sqrt{L_ {f}^{2}\overline{\mathbb{X}}^{2}(1+\bar{\theta})}((1+N^{\frac{1}{2}})^{\frac{ 1}{2}}+N^{\frac{1}{4}})=\mathcal{O}(N^{\frac{1}{4}})\), we have: \(\mathfrak{A}=\sqrt{\frac{(2\overline{\mathbb{X}}^{2}+\gamma\frac{np}{2} \overline{\mathbb{X}}+\gamma\frac{np}{2}\overline{\mathbb{X}}^{2})^{2}}{2 \theta}+\frac{n\gamma^{2}\phi^{2}}{4\theta^{\prime}}}=\mathcal{O}(\frac{1}{N ^{1/4}})\). Finally, we have \(C=\mathcal{O}(\frac{\varphi(f(\mathbf{X}^{1})-f(\bar{\mathbf{X}}))}{N^{1/4}})\) 

### Proof of Theorem 4.9

Proof.: For simplicity, we use B instead of \(\mathbb{B}^{t}\). Initially, we prove the following important lemmas.

**Lemma E.6**.: _(Riemannian gradient Lower Bound for the Iterates Gap) We define \(\phi\triangleq(3\overline{\mathbb{X}}+\overline{\mathbb{V}}\overline{\mathbb{ X}})\overline{\mathbb{G}}+(1+\overline{\mathbb{X}}^{2}+\overline{\mathbb{V}}^{2}+ \overline{\mathbb{X}}^{2}\overline{\mathbb{X}}^{2})L_{f}+(1+\overline{\mathbb{ V}}^{2})\theta\). It holds that: \(\mathbb{E}_{\xi^{t+1}}[\mathrm{dist}(\mathbf{0},\nabla_{\mathcal{J}}\mathcal{G}( \mathbf{I}_{2};\mathbf{X}^{t+1},\mathbb{B}^{t+1}))]\leq\phi\cdot\mathbb{E}_{\xi ^{t}}[\|\bar{\mathbf{V}}^{t}-\mathbf{I}_{2}\|_{\mathbf{F}}]\)._

Proof.: The proof process is exactly the same as in lemma E.2 and will not be repeated here. 

The following lemma is useful to outline the relation of \(\|\nabla_{\mathcal{J}}f(\mathbf{X}^{t})\|_{\mathbf{F}}\) and \(\|\nabla_{\mathcal{J}}\mathcal{G}(\mathbf{I}_{2};\mathbf{X}^{t},\mathbb{B})\|_ {\mathbf{F}}\).

**Lemma E.7**.: _We have the following results: \(\mathrm{dist}(\mathbf{0},\nabla_{\mathcal{J}}f(\mathbf{X}^{t}))\leq\gamma \cdot\mathbb{E}_{\xi^{t-1}}[\mathrm{dist}(\mathbf{0},\nabla_{\mathcal{J}} \mathcal{G}(\mathbf{I}_{2};\mathbf{X}^{t},\mathbb{B}))]\) with \(\gamma\triangleq\overline{\mathbb{X}}\sqrt{C_{n}^{2}}\)._

Proof.: We have the following inequalities:

\[\|\nabla_{\mathcal{J}}f(\mathbf{X}^{t})\|_{\mathbf{F}}^{2} \stackrel{{\Delta}}{{=}} \|\mathbf{G}^{t}-\mathbf{J}\mathbf{X}^{t}(\mathbf{G}^{t})^{ \top}\mathbf{X}^{t}\mathbf{J}\|_{\mathbf{F}}^{2}\] \[\stackrel{{\Delta}}{{=}} \|\mathbf{G}^{t}(\mathbf{X}^{t})^{\top}\mathbf{J}\mathbf{X}^{t} \mathbf{J}-\mathbf{J}\mathbf{X}^{t}(\mathbf{G}^{t})^{\top}\mathbf{J}\mathbf{J} \mathbf{X}^{t}\mathbf{J}\|_{\mathbf{F}}^{2}\] \[\stackrel{{\Delta}}{{\leq}} \|\mathbf{G}^{t}(\mathbf{X}^{t})^{\top}-\mathbf{J}\mathbf{X}^{t} (\mathbf{G}^{t})^{\top}\mathbf{J}\|_{\mathbf{F}}^{2}\|\mathbf{J}\mathbf{X}^{t }\mathbf{J}\|_{\mathbf{F}}^{2}\] \[\stackrel{{\Delta}}{{\leq}} \overline{\mathbb{X}}^{2}\|\mathbf{W}\|_{\mathbf{F}}^{2},\text{ with }\mathbf{W}\triangleq\mathbf{G}^{t}(\mathbf{X}^{t})^{\top}-\mathbf{J}\mathbf{X}^{t} (\mathbf{G}^{t})^{\top}\mathbf{J}\] \[\stackrel{{\Delta}}{{\leq}} \overline{\mathbb{X}}^{2}C_{n}^{2}\cdot\mathbb{E}_{\xi^{t-1}}[ \|\mathbf{U}_{\mathbf{B}}^{\top}[\mathbf{G}^{t}(\mathbf{X}^{t})^{\top}- \mathbf{J}\mathbf{X}^{t}(\mathbf{G}^{t})^{\top}\mathbf{J}]\mathbf{U}_{ \mathbf{B}}\|_{\mathbf{F}}^{2}]\] \[\stackrel{{\Delta}}{{=}} \overline{\mathbb{X}}^{2}C_{n}^{2}\cdot\mathbb{E}_{\xi^{t-1}}[ \|\nabla_{\mathcal{J}}\mathcal{G}(\mathbf{I}_{2};\mathbf{X}^{t},\mathbb{B})\|_ {\mathbf{F}}^{2}]\]

where step 1 uses the definition of \(\nabla_{\mathcal{J}}f(\mathbf{X}^{t})\); step 2 uses \(\mathbf{J}\mathbf{J}=\mathbf{I}\) and \(\mathbf{X}^{\top}\mathbf{J}\mathbf{X}=\mathbf{J}\Rightarrow\mathbf{X}^{\top} \mathbf{J}\mathbf{X}\mathbf{J}=\mathbf{J}\mathbf{J}=\mathbf{I}\); step 3 uses the norm inequality and ; step 4 uses the definition of \(\mathbf{W}\triangleq\mathbf{G}^{t}(\mathbf{X}^{t})^{\top}-\mathbf{J}\mathbf{X} ^{t}(\mathbf{G}^{t})^{\top}\mathbf{J}\) and \(\forall t,\|\mathbf{X}^{t}\|_{\mathbf{F}}\leq\overline{\mathbb{X}}\) ; step 5 uses Lemma (A.1) with \(k=2\); step 6 uses the definition of \(\nabla_{\mathcal{J}}\mathcal{G}(\mathbf{I}_{2};\mathbf{X}^{t},\mathbb{B})\). Taking the square root of both sides, we finish the proof of this lemma. 

Finally, we obtain our main convergence results. First of all, since \(f^{\circ}(\mathbf{X})\triangleq f(\mathbf{X})+\mathcal{I}_{\mathcal{J}}(\mathbf{ X})\) is a KL function, we have from Proposition 4.8 that:

\[\frac{1}{\varphi^{\prime}(f^{\circ}(\mathbf{X}^{t})-f^{\circ}(\mathbf{X}))}\leq \mathrm{dist}(0,\nabla f^{\circ}(\mathbf{X}^{t}))\stackrel{{ \partial}}{{\leq}}\|\nabla_{\mathcal{J}}f(\mathbf{X}^{t})\|_{\mathbf{F}},\] (76)

where step 1 uses Lemma E.5. Here, \(\varphi(\cdot)\) is some certain concave desingularization function. Since \(\varphi(\cdot)\) is concave, we have:

\[\forall\Delta\in\mathbb{R},\Delta^{+}\in\mathbb{R},\varphi(\Delta^{+})+(\Delta- \Delta^{+})\varphi^{\prime}(\Delta)\leq\varphi(\Delta).\]

Applying the inequality above with \(\Delta=f(\mathbf{X}^{t})-f(\bar{\mathbf{X}})\) and \(\Delta^{+}=f(\mathbf{X}^{t+1})-f(\bar{\mathbf{X}})\), we have:

\[(f(\mathbf{X}^{t})-f(\mathbf{X}^{t+1}))\varphi^{\prime}(f(\mathbf{ X}^{t})-f(\bar{\mathbf{X}}))\] (77) \[\leq \varphi(f(\mathbf{X}^{t})-f(\bar{\mathbf{X}}))-\varphi(f(\mathbf{X}^ {t+1})-f(\bar{\mathbf{X}}))\triangleq\mathcal{E}^{t}.\]

[MISSING_PAGE_EMPTY:29]

Additional Experiment Details and Results

### Additional Details for Hyperbolic Structural Probe Problem

To begin with, we give the definition of the Ultrahyperbolic manifold \(\mathbb{U}_{\alpha}^{p,q}\), which will be used in Ultra-hyperbolic geodesic distance \(\mathbf{d}_{\alpha}(\mathbf{x},\mathbf{y})\) and Diffeomorphism \(\varphi(\cdot)\).

\(\blacktriangleright\)**Ultrahyperbolic manifold.** Vectors in an ultrahyperbolic manifold is defined as \(\mathbb{U}_{\alpha}^{p,q}=\{\mathbf{x}=(x_{1},x_{2},\cdots,x_{p+q})^{\top}\in \mathbb{R}^{p,q}:\|\mathbf{x}\|_{q}^{2}=-\alpha^{2}\}\)[48], where \(\alpha\) is a non-negative real number denoting the radius of curvature. \(\|\mathbf{x}\|_{q}^{2}=\langle\mathbf{x},\mathbf{x}\rangle_{q}\), \(\forall\mathbf{x},\mathbf{y}\in\mathbb{R}^{p,q},\langle\mathbf{x},\mathbf{y} \rangle_{q}=\sum_{i=1}^{p}\mathbf{x}_{i}\mathbf{y}_{i}-\sum_{j=p+1}^{p+q} \mathbf{x}_{j}\mathbf{y}_{j}\) is a norm of the induced scalar product. The hyperbolic and spherical manifolds can be defined as :\(\mathbb{H}_{\alpha}=\mathbb{U}_{\alpha}^{p,1}\), \(\mathbb{S}_{\alpha}=\mathbb{U}_{\alpha}^{0,q}\).

\(\blacktriangleright\)**Ultra-hyperbolic geodesic distance.** The ultra-hyperbolic geodesic distance [27][28]\(\mathbf{d}_{\gamma}(\cdot,\cdot)\) is formulated: \(\forall\mathbf{x}\in\mathbb{U}_{\alpha}^{p,q},\mathbf{y}\in\mathbb{U}_{ \alpha}^{p,q}\) and \(\alpha>0\), \(\mathbf{d}_{\alpha}(\mathbf{x},\mathbf{y})=\{\begin{array}{ll}\alpha\cosh^{-1 }(|\frac{\langle\mathbf{x},\mathbf{y}\rangle_{q}}{\alpha^{2}}|)&\text{if }|\frac{ \langle\mathbf{x},\mathbf{y}\rangle_{q}}{\alpha^{2}}|\geq 1\\ \alpha\cos^{-1}(|\frac{\langle\mathbf{x},\mathbf{y}^{\prime}\rangle}{\alpha^{2 }}|)&\text{otherwise.}\end{array}\).

\(\blacktriangleright\)**Diffeomorphism.** [Theorem 1 Diffeomorphism of [49]]: Any vector \(\mathbf{x}\in\mathbb{R}^{p}\times\mathbb{R}_{*}^{q}\) can be mapped into \(\mathbb{U}_{\alpha}^{p,q}\) by a double projection \(\varphi=\phi^{-1}\circ\phi\), with \(\psi(\mathbf{x})=(\begin{array}{cc}\mathbf{s}\\ \alpha\frac{\mathbf{t}}{\|\mathbf{t}\|}\end{array})\), \(\quad\psi^{-1}(\mathbf{z})=(\begin{array}{cc}\mathbf{v}\\ \frac{\sqrt{\alpha^{2}+\|\mathbf{v}\|^{2}}}{\alpha}\mathbf{u}\end{array})\), where \(\mathbf{x}=(\begin{array}{cc}\mathbf{s}\\ \mathbf{t}\end{array})\in\mathbb{U}_{\alpha}^{p,q}\) with \(\mathbf{s}\in\mathbb{R}^{p}\) and \(\mathbf{t}\in\mathbb{R}_{*}^{q}\cdot\mathbf{z}=(\begin{array}{cc}\mathbf{v} \\ \mathbf{u}\end{array})\in\mathbb{R}^{p}\times\mathbb{S}_{\alpha}^{q}\) with \(\mathbf{v}\in\mathbb{R}^{p}\) and \(\mathbf{u}\in\mathbb{S}_{\alpha}^{q}\).

### Additional application: Ultra-hyperbolic Knowledge Graph Embedding

The J orthogonal matrix can be used as an isometric linear operator in the Ultrahyperbolic manifold, [48] et al. extended the knowledge graph model from hyperbolic space to Ultra-hyperbolic space (named as **UltraE**) by this property. The **UltraE** model is formulated as follows:

\[\min_{\mathbf{R},\mathbf{E},\mathbf{b}} \mathcal{L}(\mathbf{R},\mathbf{E},\mathbf{b})\triangleq-\frac{1}{N }\sum_{(h,r,t)\in\Delta}(\log s(h,r,t)+\sum_{(h^{\prime},r^{\prime},r^{\prime} )\in\Delta^{\prime}_{(h,r,t)}}\log(1-s(h^{\prime},r^{\prime},t^{\prime})))\] \[s.t.\left\{\begin{array}{ll}s(h,r,t)=\sigma(-d_{\alpha}^{2}( \mathbf{R}_{r}\mathbf{E}_{h},\mathbf{E}_{t})+\mathbf{b}_{h}+\mathbf{b}_{t}+ \delta)\\ \mathbf{R}_{r}^{\top}\mathbf{J}\mathbf{R}_{r}=\mathbf{J}\end{array}\right.\]

where \(\mathbf{E}\in\mathbb{R}^{n_{e}\times n}\) with \(\mathbf{E}_{h}=\mathbf{E}(h,:)\in\mathbb{U}_{\alpha}^{p,q}\), \(\mathbf{b}\in\mathbb{R}^{n_{r}}\) with \(\mathbf{b}_{h}=\mathbf{b}(r)\in\mathbb{R}\), \(\mathbf{R}\in\mathbb{R}^{n_{r}\times n\times n}\) with \(\mathbf{R}_{r}=\mathbf{R}(r,:,:)\in\mathbb{R}^{n\times n}\) and \(\mathbf{J}=\begin{array}{cc}\mathbf{l}_{p}&\mathbf{0}\\ \mathbf{0}&-\mathbf{l}_{q}\end{array}\); \(\Delta\in\mathbb{N}^{N\times 3}\) is the set of positive triplets, \(\Delta^{\prime}_{(h,r,t)}\in\mathbb{N}^{N\times k\times 3}\) denotes the set of negative triples constructed by corrupting \((h,r,t)\); \(\delta\) is a global margin hyper-parameter, \(\sigma(\cdot)\) is the sigmoid function, \(n_{e}\) represents the number of entities and \(n_{r}\) represents the number of relations; \(d_{\alpha}(\cdot)\) stands for the Ultra-hyperbolic geodesic distance (refer to F.1).

\(\blacktriangleright\)**Experiment Details.** We selected a batch of **FB15K** and **WN18RR** respectively as the data set for the Ultra-hyperbolic Knowledge Graph Embedding problem, (training set size, test set size, number of entities, number of relations) are (719,308,135,22) and (545,233,208,5) respectively. \(n=36\), \(p=18\), \(\delta=5\), \(\alpha=1\) and \(k=50\). In order to highlight the difference between J orthogonal optimization, in the **UltraE** model, all entities and biases of the optimization algorithm are optimized using **ADMM** by **Pytorch**, \(lr=5e-4\). We use the **Adagrad** optimizer in Pytorch to optimize the J-orthogonality constraint variable in the **CS** model.

### Experiment result

\(\blacktriangleright\)**Hyperbolic Eigenvalue Problem.** Table 2 and Figure 3, 4, 5 are supplementary experiments for HEVP. Several conclusions can be drawn. (i) **GS-JOBCD** often greatly improves upon **UMCM**, **ADMM** and **CSDM**. This is because our methods find stronger stationary points than them. (ii) **J-JOBCD** is a parallel version of **GS-JOBCD** and thus exhibits significantly faster convergence. (iii) The proposed methods generally give the best performance.

\(\blacktriangleright\)**Hyperbolic Structural Probe Problem.** Table 3 and Figure 6, 7 are supplementary experiments for HSPP. Several conclusions can be drawn. (i) **J-JOBCD** often greatly improves upon **UMCM**, **ADMM** and **CSDM** (ii) **VR-J-JOBCD** is a reduced variance version of **J-JOBCD** and thus exhibitssignificantly faster convergence for problems with large samples. (iii) The proposed methods generally give the best performance.

\(\blacktriangleright\)**Ultra-hyperbolic Knowledge Graph Embedding Problem.** Figure 8, 9, 10 and 11 are supplementary experiments for **UltraE**. Several conclusions can be drawn. (i) In terms of Epoch performance, **J-JOBCD** and **VR-J-JOBCD** often greatly improves upon **CSDM**, thus they show better MRR and hits results. (ii) In models with limited sample sizes, the computational efficiency of **VR-J-JOBCD** is inferior to that of **J-JOBCD**. This discrepancy arises because each iteration in **VR-J-JOBCD** necessitates two instances of backpropagation, thus consuming substantial computational resources. (iii) The proposed methods generally give the best performance.

[MISSING_PAGE_EMPTY:32]

Figure 3: The convergence curve of the compared methods for solving HEVP with varying \((m,n,p)\).

Figure 4: The convergence curve of the compared methods for solving HEVP with varying \((m,n,p)\).

Figure 5: The convergence curve of the compared methods for solving HEVP with varying \((m,n,p)\).

[MISSING_PAGE_FAIL:36]

## 6 Conclusion

Figure 6: The convergence curve of the compared methods for solving HSPP by epochs with varying \([m,n,p)\).

## Appendix A

Figure 7: The convergence curve of the compared methods for solving HSPP by time with varying \([m,n,p)\).

[MISSING_PAGE_EMPTY:39]

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract, we highlighted our contributions, including algorithm development, theoretical analysis, and empirical study. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to the assumptions made for the optimization problem outlined in the introduction and Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have added a hyperlink before each theoretical result, which points to the complete proof located in the appendix. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided sufficient details for reproducing the results of the paper, such as parameter settings, runtime environments, and dataset descriptions. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?

Answer: [Yes]

Justification: We have included all the code and data in the supplemental materials.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided sufficient details for solving the optimization problem, encompassing hyperparameter settings and dataset generation. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: For simplicity, we only demonstrate the convergence behavior of the objective function by varying the time or iterations. Our methods exhibit clear advantages over the compared methods. Such results have demonstrated significance in the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Guidelines: We have outlined the types of compute workers, detailing CPU and memory specifications.

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research aligns with the ethical guidelines outlined by NeurIPS. Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper addresses theoretical questions on algorithm complexity, which, to the best of our knowledge, pose no negative social impact. Guidelines:

* The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The dataset used in the experiments is published on an open site without license. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The experiments do not involve new datasets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or human object is involved. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No crowdsourcing or human object is involved. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.