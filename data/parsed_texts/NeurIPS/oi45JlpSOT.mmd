# Multi-Fidelity Multi-Armed Bandits Revisited

 Xuchuang Wang

Chinese University of Hong Kong

xcwang@cse.cuhk.edu.hk&Qingyun Wu

Pennsylvania State University

qingyun.wu@psu.edu

&Wei Chen

Microsoft Research

weic@microsoft.com&John C.S. Lui

Chinese University of Hong Kong

cslui@cse.cuhk.edu.hk

###### Abstract

We study the multi-fidelity multi-armed bandit (MF-MAB), an extension of the canonical multi-armed bandit (MAB) problem. MF-MAB allows each arm to be pulled with different costs (fidelities) and observation accuracy. We study both the best arm identification with fixed confidence (BAI) and the regret minimization objectives. For BAI, we present (a) a cost complexity lower bound, (b) an algorithmic framework with two alternative fidelity selection procedures, and (c) both procedures' cost complexity upper bounds. From both cost complexity bounds of MF-MAB, one can recover the standard sample complexity bounds of the classic (single-fidelity) MAB. For regret minimization of MF-MAB, we propose a new regret definition, prove its problem-independent regret lower bound \(\Omega(K^{1/3}\Lambda^{2/3})\) and problem-dependent lower bound \(\Omega(K\log\Lambda)\), where \(K\) is the number of arms and \(\Lambda\) is the decision budget in terms of cost, and devise an elimination-based algorithm whose worst-cost regret upper bound matches its corresponding lower bound up to some logarithmic terms and, whose problem-dependent bound matches its corresponding lower bound in terms of \(\Lambda\).

## 1 Introduction

The multi-armed bandits (MAB) problem was first introduced in the seminal work by Lai and Robbins [22] and was extensively studied (ref. Bubeck and Cesa-Bianchi [4], Lattimore and Szepesvari [24]). In stochastic MAB, the decision maker repeatedly pulls arms among a set of \(K\in\mathbb{N}^{+}\) arms and observes rewards drawn from unknown distributions of the pulled arms. The initial objective of MAB is _regret minimization_[22, 1], where the regret is the cumulative differences between the rewards from pulling the optimal arm and that from the concerned algorithm's arm pulling strategy. This is a fundamental sequential decision making framework for studying the exploration-exploitation trade-off, where one needs to balance between optimistically exploring arms with high uncertainty in reward (exploration) and myopically exploiting arms with high empirical reward average (exploitation). Another task of MAB is the best arm identification (BAI, a.k.a., pure exploration), later introduced by Even-Dar et al. [7, 8], Mannor and Tsitsiklis [27]. Best arm identification aims to find the best arm _without_ considering the cumulative regret (reward) during the learning process. In this paper, we focus on the fixed confidence case, with the goal of identifying the best arm with a fixed confidence with as few number of decision rounds as possible.

In this paper, we investigate a generalized MAB model with a wide range of real-world applications--the _multi-fidelity multi-armed bandit_ (MF-MAB)--introduced by Kandasamy et al. [19]. We study both the regret minimization and best arm identification objectives under the MF-MAB model. The MF-MAB model introduces flexibility for exploring arms: instead of squarely pulling an arm to observe itsreward sample as in MAB, MF-MAB offers \(M\in\mathbb{N}^{+}\) different accesses to explore an arm, called \(M\) fidelities. Pulling an arm \(k\in\mathcal{K}\coloneqq\{1,\dots,K\}\) at fidelity \(m\in\mathcal{M}\coloneqq\{1,\dots,M\}\), the decision maker pays a cost of \(\lambda^{(m)}\) and observes a reward \(X_{k}^{(m)}\) whose mean \(\mu_{k}^{(m)}\) is not too far away from the arm's true reward mean \(\mu_{k}\). More formally, \(|\mu_{k}^{(m)}-\mu_{k}|\leqslant\zeta^{(m)}\) where \(\zeta^{(m)}\geqslant 0\) is the error upper bound at fidelity \(m\). The formal definition of MF-MAB is presented in Section 2.

### Contributions

In Section 3, we first look into the best arm identification with fixed confidence (BAI) objective in MF-MAB, which has wide applications in hyperparameter optimization (Kandasamy et al., 2017, SS1.4) and neural architecture search (NAS) (Kandasamy et al., 2018). In the context of NAS, each arm of MF-MAB corresponds to one configuration of neural network architecture, and the different fidelities of pulling this arm correspond to training the neural network of this configuration with different sample sizes of training data (see details in Section 2.1). As in MF-MAB pulling an arm at different fidelities has different costs, we consider the total cost needed by an algorithm and refer to it as _cost complexity_ (a generalization of sample complexity). We first derive a cost complexity lower bound \(\Omega(\sum_{k\in\mathcal{K}}\min_{m:\Delta_{k}^{(m)}>0}(\lambda^{(m)}/(\Delta_ {k}^{(m)})^{2})\log\delta^{-1}),\) where the \(\Delta_{k}^{(m)}\) is the reward gap of arm \(k\) at fidelity \(m\) (defined in Eq.(1)) and \(\delta\) is the confidence parameter. We then devise a Lower-Upper Confidence Bound (LUCB) algorithmic framework with two different fidelity selection procedures and prove both procedures' cost complexity upper bounds. The first procedure focuses on finding the optimal fidelity suggested by the minimization in lower bound via an upper confidence bound (UCB) algorithm, which may pay high additional costs during searching the optimal fidelities. The second procedure, instead of identifying the optimal fidelity, seeks for good fidelities that are at least half as good as the optimal ones so as to reduce the cost for identifying the optimal fidelity while still enjoying fair theoretical performance.

In Section 4, we next study the regret minimization objective in MF-MAB. We introduce a novel definition of regret, wherein the rewards obtained are dependent on the pulled arms, while the selected fidelities solely affect the accuracy of the observed rewards. The new regret definition covers applications like product management and advertisement distribution (Kandasamy et al., 2017, 2018) where the distributed advertisement (ad) determines the actual (but unknown) reward while the cost (fidelity) spent by the company on evaluating the impact of this ad decides the accuracy of the observed rewards (see Remark 4.1). The difference of our regret definition and the one studied by Kandasamy et al. (2018) is discussed in Remark 4.2. We first propose both a problem-independent (a.k.a., worst-case) regret lower bound \(\Omega(K^{1/3}\Lambda^{2/3})\) and a problem-dependent regret lower bound \(\Omega(K\log\Lambda)\) where the \(\Lambda\left(>0\right)\) is the decision budget. We then devise an elimination-based algorithm, which explores (and eliminates) arms in the highest fidelity \(M\) and exploits the remaining arms in the lowest fidelity \(1\). The algorithm enjoys a \(\tilde{O}(K^{1/3}\Lambda^{2/3})\) problem-independent regret upper bound, which matches the corresponding lower bound up to some logarithmic factor, and also a problem-dependent bound matching its corresponding lower bound tightly in a class of MF-MAB instances.

### Related Works

Multi-fidelity multi-armed bandits (MF-MAB) was first proposed by Kandasamy et al. (2018). They studied a cumulative regret minimization setting whose regret definition is different from ours (see Remark 4.2 for more details). Later, Kandasamy et al. (2018) extended MF-MAB to bandits optimization, i.e., in continuous action space, with the objective of minimizing simple regret, and Kandasamy et al. (2018) further extended Kandasamy et al. (2018) to the continuous fidelity space with the same objective of minimizing simple regret. We note that the simple regret minimization in bandits optimization corresponds to the best arm identification with fixed budget in multi-armed bandits, and, therefore, is different from the cumulative regret minimization and best arm identification with fixed confidence objectives studied in this paper. During the submission of this work, the authors were aware that the BAI task under MF-MAB model was also studied by Poiani et al. (2019), where they proposed a similar cost complexity lower bound to our Theorem 3.1 and their proposed algorithm is similar to our Explore-C procedure in Appendix D. Except for these two results, all other results proposed in this paper, including two BAI algorithms in Section 3.2 and the regret minimization results in Section 4, are novel.

The multi-fidelity optimization was first introduced in simulating expensive environments via cheap surrogate models [14; 11], and, later on, was extended to many real-world applications, e.g., shape design optimization for ship [3] or wing [32], and wind farm optimization [31]. Recently, the multi-fidelity idea was employed in hyperparameter optimization (HPO) for automated machine learning (autoML) [16; 26; 9; 25]. The fidelity may correspond to training time, data set subsampling, and feature subsampling, etc. These multi-fidelity settings help agents to discard some hyperparameter configurations with low cost (a.k.a., early discarding [28; SS3.1.3]). Jamieson and Talwalkar [16] modeled hyperparameter optimization as non-stochastic best arm identification and applied the successive halving algorithm (SHA) to address it. Li et al. [26] introduced Hyperband, a hyperparameter configuring and resources allocation method which employed SHA as its subroutine to solve real HPO tasks. Falkner et al. [9] proposed BOHB, which use Bayesian optimization method to select and update hyperparameters and employ SHA to allocate resources. Li et al. [25] extended SHA to asynchronous case for parallel hyperparameter optimization. This line of works was based on the non-stochastic best arm identification problem [16] and, therefore, is very different from our stochastic modelling.

## 2 Model

We consider a \(K\left(\in\mathbb{N}^{+}\right)\)-armed bandit. Each arm can be pulled with \(M\left(\in\mathbb{N}^{+}\right)\) different fidelities. When an arm \(k\in\mathcal{K}:=\{1,\ldots,K\}\) is pulled at fidelity \(m\in\mathcal{M}:=\{1,\ldots,M\}\), one pays a cost \(\lambda^{(m)}\left(>0\right)\) and observes a feedback value drawn from a \(\left[0,1\right]\)-supported probability distribution with mean \(\mu_{k}^{(m)}\). We assume the non-trivial case that \(|\mu_{k}^{(m)}-\mu_{k}^{(M)}|\leqslant\zeta^{(m)}\), where \(\zeta^{(m)}\geqslant 0\) is the observation error upper bound at fidelity \(m\). Without loss of generality, we assume \(\lambda^{(1)}\leqslant\ldots\leqslant\lambda^{(M)}\); otherwise, we can relabel the fidelity so that the \(\lambda^{(m)}\) is non-decreasing with respect to \(m\). We call the reward mean of an arm at the highest fidelity \(M\) as this arm's _true reward mean_ and, without loss of generality, assume that these true reward means are in a descending order \(\mu_{1}^{(M)}>\mu_{2}^{(M)}\geqslant\mu_{3}^{(M)}\geqslant\ldots\geqslant\mu _{K}^{(M)}\), and arm \(1\) is the unique optimal arm.

For the online learning problem, we assume that the reward distribution and the mean \(\mu_{k}^{(m)}\) of each arm \(k\) at fidelity \(m\) are unknown, while the costs \(\lambda^{(m)}\)'s and the error upper bounds \(\zeta^{(m)}\)'s are known to the learning agent.1 To summarize, a MF-MAB instance \(\mathcal{I}\) is parameterized by a set of tuples \((\mathcal{K},\mathcal{M},(\lambda^{(m)})_{m\in\mathcal{M}},(\zeta^{(m)})_{m \in\mathcal{M}},(\mu_{k}^{(m)})_{(k,m)\in\mathcal{K}\times\mathcal{M}})\), with elements described above.

Footnote 1: We provide application examples where \(\lambda^{(m)}\) and \(\zeta^{(m)}\) are known in Section 2.1

In this paper, we consider two tasks on the above multi-fidelity bandit model: best arm identification and regret minimization. We will define these two tasks in the respective sections below.

### Application

One typical application of the best arm identification problem is hyperparameter optimization [15; 6] (including neural architecture search) for machine learning, in which the goal is to identify the best hyperparameter configuration--the training set-up for a machine learning model attaining the best predictive performance--with as low resource as possible. A mapping between concepts in this application and MF-MAB is discussed as follows. **Arm:** Hyperparameter configurations of machine learning models, e.g., neural network architectures. **Reward:** Predictive performance of the resulting machine learning model trained based on the selected configuration (arm). **Fidelity dimension:** For a particular hyperparameter configuration (arm), one typically has the choice to determine a certain level of resources to allocate for training the model. The concept of "training resource" can be considered the fidelity dimension. More concretely, commonly used training resources include _the number of epochs_ and _the training data sample_, both of which satisfy our cost assumption. For example, the larger the number of epochs or training data samples, the more expensive to train the model.

**Remark 2.1** (On the assumptions of multi-fidelity feedback in the hyperparameter optimization application).: Observation error upper bound \(\zeta^{(m)}\) is the maximum distance from resources allocated to the terminal validation loss. According to benchmarked results in two recent benchmarks for multi-fidelity hyperparameter optimization, including HPOBench[5] and YAHP0 Gym[29], under the typically used fidelity dimension, including the number of epochs, the training data sample, etc., the maximum distance from the terminal validation loss often decreases with the increase of resources, i.e., \(\zeta^{(m)}\) decreases with the increase of \(m\). Thanks to these benchmarks, it is also convenient to know \(\zeta^{(m)}\) under different fidelities \(m\) for commonly used types of fidelity dimension.

## 3 Best Arm Identification with Fixed Confidence

In this section, we study the best arm identification with fixed confidence (BAI) task in the multi-fidelity multi-armed bandits (MF-MAB) model. The objective of BAI is to minimize the total budget spent for identifying the best arm with a confidence of at least \(1-\delta\). As the cost of pulling an arm in MF-MAB depends on the chosen fidelity, we use the total cost, instead of total pulling times (sample complexity), as our criterion, and refer to it as _cost complexity_. We first present a cost complexity lower bound in Section 3.1, then propose a LUCB algorithmic framework with two alternative procedures in Section 3.2, and analyze their cost complexity upper bounds in Section 3.3. Lastly, we introduce concrete applications of the BAI problem in Section 2.1.

### Cost Complexity Lower Bound

We present the cost complexity lower bound in Theorem 3.1. Its proof is deferred to Appendix E.1. During the submission of this paper, we notice that a similar cost complexity lower bound was also proposed by Poiani et al. (2010, Theorem 1).

**Theorem 3.1** (Cost complexity lower bound).: _For any algorithm addressing the best arm identification with fixed confidence \(1-\delta\) for any parameter \(\delta>0\), any number of arms \(K\), any number of fidelities \(M\) with any observation error upper bound sequence \(\left(\zeta^{(1)},\zeta^{(2)},\ldots,\zeta^{(M)}\right)(\zeta^{(M)}=0)\) and any cost sequence \(\left(\lambda^{(1)},\ldots,\lambda^{(M)}\right)\), and any \(K\) fidelity subsets \(\left\{\mathcal{M}_{1},\mathcal{M}_{2},\ldots,\mathcal{M}_{K}\right\}\) where the \(\mathcal{M}_{k}\) is a subset of full fidelity set \(\mathcal{M}\) containing the highest fidelity \(M\), i.e., \(M\in\mathcal{M}_{k}\in\mathcal{2^{M}}\), for all \(k\in\mathcal{K}\), there exists a MF-MAB instance such that_

\[\mathbb{E}[\Lambda]\geqslant\left(\min_{m\in\mathcal{M}_{1}}\frac{\lambda^{( m)}}{\mathrm{KL}(\nu_{1}^{(m)},\nu_{2}^{(M)}+\zeta^{(m)})}+\sum_{k\neq 1}\min_{m\in \mathcal{M}_{k}}\frac{\lambda^{(m)}}{\mathrm{KL}(\nu_{k}^{(m)},\nu_{1}^{(M)} -\zeta^{(m)})}\right)\log\frac{1}{2.4\delta},\]

_where \(\mathrm{KL}\) is the KL-divergence between two probability distributions, \(\nu_{k}^{(m)}\) is the probability distribution associated with arm \(k\) when pulled at fidelity \(m\), and \(\nu\pm\zeta\) means to positively/negatively shift the distribution \(\nu\) by an offset \(\zeta\)._

According to the KL-divergences' two inputs in the above lower bound, we define the reward gap \(\Delta_{k}^{(m)}\) of arm \(k\) at fidelity \(m\) as follows,

\[\Delta_{k}^{(m)}\coloneqq\begin{cases}\mu_{1}^{(M)}-(\mu_{k}^{(m)}+\zeta^{(m )})&\forall k\neq 1\\ (\mu_{1}^{(m)}-\zeta^{(m)})-\mu_{2}^{(M)}&\forall k=1\end{cases}.\] (1)

For any suboptimal arm \(k\neq 1\), the gap \(\Delta_{k}^{(m)}\) quantifies the distance between the optimal arm's reward mean \(\mu_{1}^{(M)}\) and the suboptimal arm's reward mean upper bound at fidelity \(m\), i.e., \(\mu_{k}^{(m)}+\zeta^{(m)}\); and for the optimal arm \(1\), the gap \(\Delta_{1}^{(m)}\) represents the distance between the optimal arm's reward mean lower bound at fidelity \(m\), i.e., \(\mu_{1}^{(m)}-\zeta^{(m)}\), and the second-best arm's reward mean \(\mu_{2}^{(M)}\).

**Remark 3.2**.: If all reward distributions are assumed to be Bernoulli or Gaussian and let \(\mathcal{M}_{k}=\{m:\Delta_{k}^{(m)}>0\}\), the regret bound can be simplified as

\[\mathbb{E}[\Lambda]\geqslant C\sum_{k\in\mathcal{K}}\min_{m:\Delta_{k}^{(m)}> 0}\frac{\lambda^{(m)}}{(\Delta_{k}^{(m)})^{2}}\log\frac{1}{\delta},\] (2)

where \(C>0\) is a constant depending on the reward distributions assumed. Especially, we denote \(m_{k}^{*}\coloneqq\arg\min_{m:\Delta_{k}^{(m)}>0}\frac{\lambda^{(m)}}{(\Delta _{k}^{(m)})^{2}}\), and with some algebraic transformations, it can be expressed as

\[m_{k}^{*}=\arg\max_{m\in\mathcal{M}}\frac{\Delta_{k}^{(m)}}{\sqrt{\lambda^{(m )}}}.\] (3)With \(m_{k}^{*}\), the lower bound in Eq.(2) can be rewritten as \(\mathbb{E}[\Lambda]\geqslant C\sum_{k\in\mathcal{K}}(\lambda^{(m_{k}^{*})}/( \Delta_{k}^{(m_{k}^{*})})^{2})\log(1/\delta)\), and we define the coefficient as \(H\coloneqq\sum_{k\in\mathcal{K}}\lambda^{(m_{k}^{*})}/(\Delta_{k}^{(m_{k}^{*}) })^{2}\). The \(m_{k}^{*}\) can be interpreted as the _optimal (most efficient) fidelity_ for exploring arm \(k\). We note that the current lower bound in Eq.(2) does not contain the cost of finding this \(m_{k}^{*}\). This cost can be observed in our algorithm's cost complexity upper bound stated in Section 3.3.

### Algorithm Design

In this subsection, we propose a Lower-Upper Confidence Bound (LUCB) algorithmic framework with two alternative procedures which employ different mechanisms to select suitable fidelities for arm exploration. Generalized from the original (single-fidelity) LUCB algorithm [17], the LUCB algorithmic framework in SS3.2.1 determines two critical arms (the empirical optimal arm \(\ell_{t}\) and second-best arm \(u_{t}\)) for exploration in each time slot \(t\). However, in MF-MAB, with the critical arms suggested by the LUCB framework, one still needs to decide the fidelities for exploring the critical arms. This fidelity selection faces an accuracy-cost trade-off, i.e., higher fidelity (accuracy) but suffering higher cost, or lower fidelity (accuracy) but enjoying lower cost. This trade-off is different from the common exploration-exploitation trade-off in classic bandits. Because the accuracy and costs are two orthogonal metrics, while in exploration-exploitation trade-off, there is only a single regret metric. We address the accuracy-cost trade-off in SS3.2.2 with two alternative procedures.

#### 3.2.1 LUCB Algorithmic Framework

The main idea of LUCB [17] is to repeatedly select and explore two critical arms, that is, the empirical optimal arm \(\ell_{t}\) and the empirical second-best arm \(u_{t}\). When both critical arms' confidence intervals are separated--\(\ell_{t}\)'s LCB (lower confidence bound) is greater than \(u_{t}\)'s UCB (upper confidence bound), the algorithm terminates and outputs the estimated optimal arm \(\ell_{t}\).

The LUCB framework depends on a set of meaningful confidence intervals of the arms' rewards. Usually, the confidence interval of an empirical mean estimate \(\hat{\mu}_{k}^{(m)}\) can be expressed as \((\hat{\mu}_{k}^{(m)}-\beta(N_{k,t}^{(m)},t,\delta),\hat{\mu}_{k}^{(m)}+\beta (N_{k,t}^{(m)},t,\delta)),\) where \(\beta(N_{k,t}^{(m)},t,\delta)\) is the confidence radius and \(N_{k,t}^{(m)}\) is the number of times of pulling arm \(k\) at fidelity \(m\) up to time \(t\) (include \(t\)). As MF-MAB assumes that \(|\mu_{k}^{(m)}-\mu_{k}^{(M)}|\leqslant\zeta^{(m)}\), based on observations of fidelity \(m\), the upper and lower confidence bounds for arm \(k\)'s true reward mean at the highest fidelity \(\mu_{k}^{(M)}\) can be expressed as follows,

\[\texttt{UCB}_{k,t}^{(m)}\coloneqq\hat{\mu}_{k,t}^{(m)}+\zeta^{(m)}+\beta(N_{ k,t}^{(m)},t,\delta),\quad\texttt{LCB}_{k,t}^{(m)}\coloneqq\hat{\mu}_{k,t}^{(m)}- \zeta^{(m)}-\beta(N_{k,t}^{(m)},t,\delta),\] (4)

where we set the confidence radius \(\beta(n,t,\delta)=\sqrt{\log(Lt^{4}/\delta)/n}\) and \(L\left(>0\right)\) is a factor. Since the multi-fidelity feedback allows one to estimate an arm's true reward mean with observations of every fidelity, we pick the tightest one as arm \(k\)'s final confidence bounds as follows,

\[\texttt{UCB}_{k,t}=\min_{m\in\mathcal{M}}\texttt{UCB}_{k,t}^{(m)},\quad \texttt{LCB}_{k,t}=\max_{m\in\mathcal{M}}\texttt{LCB}_{k,t}^{(m)}\,.\] (5)

We use the above \(\texttt{UCB}_{k,t}\) and \(\texttt{LCB}_{k,t}\) formulas to select the two critical arms in each round and decide when to terminate the LUCB (Line 3). We present the LUCB framework in Algorithm 1. The next step is to decide fidelities for exploring both critical arms in each round ( Line 5).

#### 3.2.2 Exploration Procedures

To address the accuracy-cost trade-off in fidelity selections, we devise a UCB-type policy which "finds" the optimal fidelity \(m_{k}^{*}\) in Eq.(3) for each arm \(k\) (Explore-A) and an explore-then-commit policy stopping at a good fidelity that is at least half as good as the optimal one (Explore-B).

**Notations.** The lower bound in Eq.(2) implies that there exists an optimal fidelity \(m_{k}^{*}\) for exploring arm \(k\). As Eq.(3) shows, the optimal fidelity \(m_{k}^{*}\) maximizes the \(\Delta_{k}^{(m)}/\sqrt{\lambda^{(m)}},\) where the \(\Delta_{k}^{(m)}\) defined in Eq.(1) consists of two unknown reward means: \(\mu_{k}^{(m)}\) and \(\mu_{1}^{(M)}\) (or \(\mu_{2}^{(M)}\)). That is, calculating \(\Delta_{k}^{(m)}\) for all \(k\) needs the top two arms' reward means \(\mu_{1}^{(M)}\) and \(\mu_{2}^{(M)}\) which are unknown a priori. To address the issue, we assume the knowledge of an upper bound of the optimal arm's reward mean \(\tilde{\mu}_{1}^{(M)}\) and a lower bound of the second-best arm's reward mean \(\tilde{\mu}_{2}^{(M)}\) (see Remark 3.3 for how to obtain \(\tilde{\mu}_{1}^{(M)}\) and \(\tilde{\mu}_{2}^{(M)}\) in real-world applications). With the \(\mu_{1}^{(M)}\) and \(\mu_{2}^{(M)}\) replaced by \(\tilde{\mu}_{1}^{(M)}\) and \(\tilde{\mu}_{2}^{(M)}\), we define the ancillary reward gaps \(\tilde{\Delta}_{k}^{(m)}\) and the ancillary optimal fidelity \(\tilde{m}_{k}^{*}\) as follows,

\[\tilde{\Delta}_{k}^{(m)}\coloneqq\begin{cases}\tilde{\mu}_{1}^{(M)}-(\mu_{k} ^{(m)}+\zeta^{(m)})&\forall k\neq 1\\ (\mu_{k}^{(m)}-\zeta^{(m)})-\tilde{\mu}_{2}^{(M)}&\forall k=1\end{cases},\quad \text{ and }\quad\tilde{m}_{k}^{*}\coloneqq\operatorname*{arg\,max}_{m\in \mathcal{M}}\frac{\tilde{\Delta}_{k}^{(m)}}{\sqrt{\lambda^{(m)}}}.\]

Especially, we have \(m_{k}^{*}\geqslant\tilde{m}_{k}^{*}\) because the cost \(\lambda^{(m)}\) is non-decreasing and the replacement enlarges the numerator of the \(\operatorname*{arg\,max}\) item in Eq.(3), and, when the bounds \(\tilde{\mu}_{1}^{(M)}\) and \(\tilde{\mu}_{2}^{(M)}\) are close the the true reward means, we have \(m_{k}^{*}=\tilde{m}_{k}^{*}\); hence, as \(\Delta_{k}^{(m_{k}^{*})}>0\), we assume \(\Delta_{k}^{(\tilde{m}_{k}^{*})}>0\) for all arms \(k\) as well. To present the next two procedures, we define the estimate of \(\tilde{\Delta}_{k}^{(m)}\) as follows,

\[\hat{\Delta}_{k,t}^{(m)}\coloneqq\begin{cases}\tilde{\mu}_{1}^{(M)}-(\hat{\mu }_{k,t}^{(m)}+\zeta^{(m)})&\forall k\neq\ell_{t}\\ (\hat{\mu}_{k,t}^{(m)}-\zeta^{(m)})-\tilde{\mu}_{2}^{(M)}&\forall k=\ell_{t} \end{cases},\]

where the \(\ell_{t}\) is the estimated optimal arm by LUCB. For simplify, we omit the input \(\ell_{t}\) for \(\hat{\Delta}_{k,t}^{(m)}(\ell_{t})\) in the LHS of this definition and thereafter.

Explore-AWe devise the \(\Delta_{k}^{(m)}/\sqrt{\lambda^{(m)}}\)'s upper confidence bounds (\(\mathtt{f}\)-UCB) as follows, for any fidelity \(m\in\mathcal{M}\) and arms \(k\in\mathcal{K}\), \(\mathtt{f}\)-UCB\({}_{k,t}^{(m)}\coloneqq\hat{\Delta}_{k,t}^{(m)}/\sqrt{\lambda^{(m)}}+\sqrt{2 \log N_{k,t}/(\lambda^{(m)}N_{k,t}^{(m)})},\) where the \(N_{k,t}\coloneqq\sum_{m\in\mathcal{M}}N_{k,t}^{(m)}\) is the total number of times of pulling arm \(k\) up to time \(t\). Whenever the arm \(k\) is selected by LUCB, we pick the fidelity \(m\) that maximizes its \(\mathtt{f}\)-UCB\({}_{k,t}^{(m)}\) to explore it (see Line 2 in Algorithm 2). For any arm \(k\), this policy guarantees that most of the arm's pulling are on its estimated optimal fidelity \(\tilde{m}_{k}^{*}\), or formally, \(N_{k,t}^{(m)}=O(\log(N_{k,t}^{(\tilde{m}_{k}^{*})}))\) for any fidelity \(m\neq\tilde{m}_{k}^{*}\) as Lemma E.2 in Appendix shows. Therefore, Explore-A spends most cost on the fidelity \(\tilde{m}_{k}^{*}\).

Explore-BThe cost of finding the estimated optimal fidelity \(\tilde{m}_{k}^{*}\) in Explore-A can be large. To avoid this cost, we devise another approach that stops exploration when finding a _good_ fidelity \(\tilde{m}_{k}^{*}\). That is, instead of finding the \(\tilde{m}_{k}^{*}\) that maximizes \(\tilde{\Delta}_{k}^{(m)}/\sqrt{\lambda^{(m)}}\), we stop at a fidelity \(\tilde{m}_{k}^{*}\) whose \(\tilde{\Delta}_{k}^{(m)}/\sqrt{\lambda^{(m)}}\) is at least half as large as that of \(\tilde{m}_{k}^{*}\), i.e.,

\[\frac{\tilde{\Delta}_{k}^{(\tilde{m}_{k}^{*})}}{\sqrt{\lambda^{(\tilde{m}_{k}^{* })}}}\geqslant\frac{1}{2}\cdot\frac{\tilde{\Delta}_{k}^{(\tilde{m}_{k}^{*})}}{ \sqrt{\lambda^{(\tilde{m}_{k}^{*})}}}.\] (6)

We prove that the above inequality holds for \(\hat{m}_{k}^{*}=\operatorname*{arg\,max}_{m\in\mathcal{M}}\hat{\Delta}_{k,t}^{(m )}/\sqrt{\lambda^{(m)}}\) when the condition in Line 10 of Algorithm 2 holds (see Lemma E.3). Hence, for each arm \(k\), Explore-B explores it at all fidelities uniformly, and, when the condition in Line 10 holds, Explore-B finds a good fidelity \(\hat{m}_{k}^{*}\) and keeps choosing \(\hat{m}_{k}^{*}\) for exploring arm \(k\) since then.

**Remark 3.3** (On the bounds \(\tilde{\mu}_{1}^{(M)}\) and \(\tilde{\mu}_{2}^{(M)}\) utilized in Algorithm 2 in the hyperparameter optimization application).: Although the exact reward means are typically not accessible, it is easy to get a good approximation of them satisfying our requirements based on domain knowledge. For example,in an image classification task, an easy approximation of the best arm's reward means is to use the reward from a perfect classification, i.e., \(\tilde{\mu}_{1}^{(M)}=1.0\), which clearly satisfies \(\tilde{\mu}_{1}^{(M)}\geq\mu_{1}^{(M)}\). For the reward of the second-best arm, we can use the performance of a commonly used model that has a fairly good performance based on benchmarked results as a good approximation. One can easily find the benchmarked performance of commonly used models on a wide range of well-defined machine learning tasks on the Papers with code website.2 For novel tasks without well-benchmarked results, one pragmatic way to get \(\tilde{\mu}_{2}^{(M)}\) is to use the result from a particular default machine learning model without any tuning.

Footnote 2: https://paperswithcode.com/sota

### Cost Complexity Upper Bound Analysis

In the following, we present the cost complexity upper bounds of Algorithm 1 with above two procedures in Theorem 3.4 respectively.

**Theorem 3.4** (Cost complexity upper bounds for Algorithm 1 with procedure in Algorithm 2).: _Given \(L\geqslant 4KM\), Algorithm 1 outputs the optimal arm with a probability at least \(1-\delta\). The cost complexities of Algorithm 1 with different fidelity selection procedures in Algorithm 2 are upper bounded as follows,_

\[\text{(Explore-A)}\qquad\mathbb{E}[\Lambda] =O\left(\tilde{H}\log\left(\frac{L(\tilde{H}+\tilde{G})}{\lambda^ {(1)}\delta}\right)+\tilde{G}\log\log\left(\frac{L(\tilde{H}+\tilde{G})}{ \lambda^{(1)}\delta}\right)\right),\] (7) \[\text{(Explore-B)}\qquad\mathbb{E}[\Lambda] =O\left(\tilde{H}\sum_{m\in\mathcal{M}}\frac{\lambda^{(m)}}{ \lambda^{(1)}}\log\left(\frac{\sum_{m\in\mathcal{M}}(\lambda^{(m)}/\lambda^{( 1)})\tilde{H}L}{\lambda^{(1)}\delta}\right)\right),\] (8)

_where \(\tilde{H}\coloneqq\sum_{k\in\mathcal{K}}\frac{\lambda^{(m_{k}^{*})}}{(\tilde{ \Delta}_{k}^{(m_{k}^{*})})^{2}},\) and \(\tilde{G}\coloneqq\sum_{k\in\mathcal{K}}\sum_{m\neq\tilde{m}_{k}^{*}}\left( \frac{\tilde{\Delta}_{k}^{(m_{k}^{*})}}{\sqrt{\lambda^{(m_{k}^{*})}}}-\frac{ \tilde{\Delta}_{k}^{(m)}}{\sqrt{\lambda^{(m)}}}\right)^{-2}.\)_

**Explore-A**_vs._**Explore-B** When \(\tilde{G}=O(M\tilde{H})\), the cost complexity upper bound of Explore-A is less than that of Explore-B (see Figure 0(a)). However, when \(\tilde{G}\) is far more larger than \(M\tilde{H}\), Explore-B is better (see Figure 0(b)). For example, when there is a fidelity \(m^{\prime}\left(\neq\tilde{m}_{k}^{*}\right)\) whose \(\Delta_{k}^{(m^{\prime})}/\sqrt{\lambda^{(m^{\prime})}}\) is very close to that of fidelity \(\tilde{m}_{k}^{*}\) (the case in Figure 0(b)), this \(\tilde{G}\) would be very large

Figure 1: Explore-A vs. Explore-B

because Explore-A needs to pay a high cost to distinguish fidelity \(m^{\prime}\) from \(\tilde{m}^{*}_{k}\); while in this scenario, Explore-B stops by either \(m^{\prime}_{k}\) or \(\tilde{m}^{*}_{k}\) since their \(\tilde{\Delta}^{(m)}_{k}/\sqrt{\lambda^{(m)}}\) are similar and, therefore, enjoys a smaller cost complexity upper bound. We report the numerical comparisons between both procedures in Figure 1. The detailed setup of the simulations is given in Appendix B.

**Remark 3.5** (Tightness of cost complexity bounds).: The first term of cost complexity upper bound for Explore-A in Eq.(7) matches the cost complexity lower bound in Eq.(2) up to a constant when \(\tilde{m}^{*}_{k}=m^{*}_{k}\) and \(\tilde{H}=H\) (i.e., when \(\tilde{\mu}^{(M)}_{1}\) and \(\tilde{\mu}^{(M)}_{2}\) are close to their ground truth values). The cost complexity upper bound of Explore-B in Eq.(8) matches the lower bound with an additional \(\sum_{m\in\mathcal{M}}\lambda^{(m)}/\lambda^{(1)}\) coefficient when \(\tilde{m}^{*}_{k}=m^{*}_{k}\) and \(\tilde{H}=H\).

**Remark 3.6** (Comparison to classic MAB's sample complexity).: If we reduce our cost complexity upper bound result in MF-MAB to classic (single-fidelity) MAB, i.e., letting \(M=1,\lambda^{(m)}=1\), then both cost complexity upper bounds reduce to \(O(\sum_{k}(1/\Delta_{k}^{2})\log(1/\delta))\) where \(\Delta_{k}\coloneqq\mu_{1}-\mu_{k}\), which is exactly the classic sample complexity upper bound for (single-fidelity) BAI [27; 21].

## 4 Regret Minimization

In this section, we study the regret minimization objective: given a budget \(\Lambda\in\mathbb{R}^{+}\), minimize the regret--the cumulative difference between the optimal policy's rewards and an algorithm's. We define the reward obtained in each time slot as the pulled arm's true reward mean (realized at the highest fidelity, but _unrevealed_ to the learner), no matter at which fidelity the arm is pulled, while the learner's observation depends on the pulled fidelity as Section 2 shows. Under this reward definition, the optimal policy is to constantly pull the optimal arm \(1\) with the lowest fidelity \(m=1\). Consequently, the expected regret can be expressed as follows,

\[\mathbb{E}[R(\Lambda)]\coloneqq\frac{\Lambda}{\lambda^{(1)}}\mu_{1}^{(M)}- \mathbb{E}\left[\sum_{t=1}^{N}\mu_{I_{t}}^{(M)}\right],\] (9)

where \(N\coloneqq\max\{n:\sum_{t=1}^{n}\lambda^{(m_{\ell})}\leqslant\Lambda\}\) is the total number of time slots, and \(I_{t}\) is the arm pulled by a concerned algorithm at time slot \(t\). Next, we illustrate the regret definition's real-world applications in Remark 4.1.

**Remark 4.1** (Applications of the new regret definition).: One typical application of the regret minimization problem under MF-MAB is a variant of the advertisement distribution problem [13; 10]. In this problem, the objective is to maximize the total return from all the distributed ads within a fixed marketing budget (e.g., in terms of money). We have the following mapping between the application-specific concepts and concepts in MF-MAB. **Arm:** The ads to distribute are the arms. **Reward:** The return from each of the ads, once distributed, is the corresponding ground-truth reward \(\mu_{k}^{(M)}\). **Low fidelity:** A minimum cost is needed every time any ad is distributed. For example, the minimum cost may include the necessary resource needed to ensure that the ad satisfies legal and regulatory requirements and to distribute the ad on the designated platform. This minimum cost can be considered the lowest fidelity cost, which can never be waived. **High fidelity:** since the expected return from different ads can be vastly different, one needs a good estimation on the expected return so as to select the profitable ads to distribute. The cost needed to get a reliable estimate of the expected return can be considered the highest fidelity cost. For example, doing a large-scale user study/survey, and/or consulting experts can give a good estimate of the expected return from the ads, which, however, is resource-consuming.

This type of application is also common in production management with uncertainty where without knowing the expected return of the concerned products, the decision maker faces the two options of (1) spending the minimum resource needed to directly produce certain products; and (2) spending more resource to first get a good estimates of the expected returns from the different options and then put the ones with the highest expected returns into production.

**Remark 4.2** (Comparison to regret definition of Kandasamy et al. [19]).: Kandasamy et al. [19] defined the per time slot reward as the pulled arms' true reward mean multiplied by the cost, i.e., \(\lambda^{(m_{\ell})}\mu_{I_{t}}^{(M)}\), and defined their regret as, \(\mathbb{E}[R^{\prime}(\Lambda)]\coloneqq\Lambda\mu_{1}^{(M)}-\mathbb{E}[\sum_{ t=1}^{N}\lambda^{(m_{\ell})}\mu_{I_{t}}^{(M)}]\). We note that multiplying the reward mean with the fidelity-level cost, \(\lambda^{(m_{\ell})}\mu_{I_{t}}^{(M)}\), does not fit into the applicationsin Remark 4.1, and thus we provide an alternative definition in Eq.(9) to fit our needs. Comparing the formula of both regret definitions, we have \(\mathbb{E}[R^{\prime}(\Lambda)]\leqslant\lambda^{(1)}\mathbb{E}[R(\Lambda)].\) Note that both regret definitions are very different, so as their bound analysis and algorithm design.

We first present both the problem-independent (worst-case) and problem-dependent regret lower bounds in Section 4.1 and then devise an elimination algorithm whose worst-case upper bounds match the worst-case lower bound up to some logarithmic factors and whose problem-dependent upper bound matches the problem-dependent lower bound in a class of MF-MAB in Section 4.2.

### Regret Lower Bound

We present the problem-independent regret lower bound in Theorem 4.3 and the problem-dependent regret lower bound in Theorem 4.4. Both proofs are deferred to Appendix F.1 and F.2 respectively.

**Theorem 4.3** (Problem-independent regret lower bound).: _Given budget \(\Lambda\), the regret of MF-MAB is lower bounded as follows,_

\[\inf_{\text{Ago}}\sup_{\mathcal{I}}\mathbb{E}\left[R(\Lambda)\right] \geqslant\Omega\left(K^{1/3}\Lambda^{2/3}\right),\]

_where the \(\inf\) is over any algorithms, the \(\sup\) is over any possible MF-MAB instances \(\mathcal{I}\)._

**Theorem 4.4** (Problem-dependent lower bound).: _For any consistent policy that, after spending \(\Lambda\) budgets, fulfills that for any suboptimal arm \(k\) (with \(\Delta_{k}^{(M)}>0\)) and any \(a>0\), \(\mathbb{E}[N_{k}^{(\forall m)}\left(\Lambda\right)]=o(\Lambda^{a}),\) its regret is lower bounded by the following inequality,_

\[\liminf_{\Lambda\to\infty}\frac{\mathbb{E}\left[R(\Lambda)\right]}{\log( \Lambda)}\geqslant\sum_{k\in\mathcal{K}}\min_{m:\Delta_{k}^{(m)}>0}\left( \frac{\lambda^{(m)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right)\frac{C}{ (\Delta_{k}^{(m)})^{2}}.\]

### An Elimination Algorithm and Its Regret Upper Bound

In this section, we propose an elimination algorithm for MF-MAB based on Auer and Ortner [1]. This algorithm proceeds in phases \(p=0,1,\ldots\) and maintains a candidate arm set \(\mathcal{C}_{p}\). The set \(\mathcal{C}_{p}\) is initialized as the full arm set \(\mathcal{K}\) and the algorithm gradually eliminates arms from the set until there is only one arm remaining. When the candidate arm set contains more than one arms, the algorithm explores arms with the highest fidelity \(M\), and when the set \(|\mathcal{C}_{p}|=1\), the algorithm exploits the singleton in the set with the lowest fidelity \(m=1\). We present the detail in Algorithm 3.

```
1:Input: full arm set \(\mathcal{K}\), budget \(\Lambda\), and parameter \(\varepsilon\)
2:Initialization: phase \(p\gets 0\), candidate set \(\mathcal{C}_{p}\leftarrow\mathcal{K}\)
3:while\(p<\log_{2}\frac{2}{\varepsilon}\) and \(|\mathcal{C}_{p}|>1\)do
4: pull each arm \(k\in\mathcal{C}_{p}\) in highest fidelity \(M\) such that \(T_{k}^{(M)}=\left\lceil 2^{2p}\log\frac{\Lambda}{2^{2p}\lambda^{(M)}}\right\rceil\)
5: Update reward means \(\hat{\mu}_{k,p}^{(M)}\) for all arms \(k\in\mathcal{C}_{p}\)
6:\(\mathcal{C}_{p+1}{\leftarrow}\{k\in\mathcal{C}_{p};\hat{\mu}_{k,p}^{(M)}+2^{- p+1}{>}\max_{k^{\prime}\in\mathcal{C}_{p}}\hat{\mu}_{k^{\prime},p}^{(M)}\}\)\(\triangleright\) Elimination
7:\(p\gets p+1\)
8: Pull the remaining arms of \(\mathcal{C}_{p}\) in turn in fidelity \(m=1\) until the budget runs up ```

**Algorithm 3** Elimination for MF-MAB

**Remark 4.5** (Real-world implication of the 2-stage algorithm design).: There are real-world applications, e.g., the advertisement distribution problem in Remark 4.1, where the explorations are conducted at the high fidelity (e.g., a large-scale user study) and the exploitations are conducted at the low fidelity (e.g., advertisement distribution via some platforms). This corroborates our algorithm design which also explores at high fidelity and exploits at low fidelity. On the other hand, the fact that our algorithm enjoys the tight regret performance comparing to regret lower bound (both problem-independent and -dependent, see Remarks 4.9 and 4.7) also implies that the approach of first conducting large-scale user study and then massive distributing good ads used in real-world advertisement distribution is reasonable.

[MISSING_PAGE_EMPTY:10]

## Acknowledgements

We thank Quanlu Zhang for his insightful discussion on the potential applications of MF-MAB, especially, neural architecture search. The work of John C.S. Lui and Xuchuang Wang was supported in part by the RGC's GRF 14207721 and SRFS2122-4S02.

## References

* Auer and Ortner [2010] Peter Auer and Ronald Ortner. Ucb revisited: Improved regret bounds for the stochastic multi-armed bandit problem. _Periodica Mathematica Hungarica_, 61(1-2):55-65, 2010.
* Bartok et al. [2012] Gabor Bartok, Navid Zolghadr, and Csaba Szepesvari. An adaptive algorithm for finite stochastic partial monitoring. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pages 1779-1786, 2012.
* Bonfiglio et al. [2018] L Bonfiglio, P Perdikaris, S Brizzolara, and GE Karniadakis. Multi-fidelity optimization of super-cavitating hydrofoils. _Computer Methods in Applied Mechanics and Engineering_, 332:63-85, 2018.
* Bubeck and Cesa-Bianchi [2012] Sebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Foundations and Trends(r) in Machine Learning_, 5(1):1-122, 2012.
* Eggensperger et al. [2021] Katharina Eggensperger, Philipp Muller, Neeratyoy Mallik, Matthias Feurer, Rene Sass, Aaron Klein, Noor Awad, Marius Lindauer, and Frank Hutter. HPOBench: A collection of reproducible multi-fidelity benchmark problems for HPO. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)_, 2021. URL https://openreview.net/forum?id=1k4rJYEwda-.
* Elsken et al. [2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey. _The Journal of Machine Learning Research_, 20(1):1997-2017, 2019.
* Even-Dar et al. [2002] Eyal Even-Dar, Shie Mannor, and Yishay Mansour. Pac bounds for multi-armed bandit and markov decision processes. In _COLT_, volume 2, pages 255-270. Springer, 2002.
* Even-Dar et al. [2006] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _Journal of machine learning research_, 7(6), 2006.
* Falkner et al. [2018] Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. In _International Conference on Machine Learning_, pages 1437-1446. PMLR, 2018.
* Farris et al. [2015] Paul W Farris, Dominique M Hanssens, James D Lenskold, and David J Reibstein. Marketing return on investment: Seeking clarity for concept and measurement. _Applied Marketing Analytics_, 1(3):267-282, 2015.
* Forrester et al. [2007] Alexander IJ Forrester, Andras Sobester, and Andy J Keane. Multi-fidelity optimization via surrogate modelling. _Proceedings of the royal society a: mathematical, physical and engineering sciences_, 463(2088):3251-3269, 2007.
* Garivier et al. [2019] Aurelien Garivier, Pierre Menard, and Gilles Stoltz. Explore first, exploit next: The true shape of regret in bandit problems. _Mathematics of Operations Research_, 44(2):377-399, 2019.
* Han and Gabor [2020] Benjamin Han and Jared Gabor. Contextual bandits for advertising budget allocation. _Proceedings of the ADKDD_, 17, 2020.
* Huang et al. [2006] Deng Huang, Theodore T Allen, William I Notz, and R Allen Miller. Sequential kriging optimization using multiple-fidelity evaluations. _Structural and Multidisciplinary Optimization_, 32(5):369-382, 2006.
* Hutter et al. [2019] Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren. _Automated machine learning: methods, systems, challenges_. Springer Nature, 2019.

* [16] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In _Artificial intelligence and statistics_, pages 240-248. PMLR, 2016.
* [17] Shivaram Kalyanakrishnan, Ambuj Tewari, Peter Auer, and Peter Stone. Pac subset selection in stochastic multi-armed bandits. In _ICML_, volume 12, pages 655-662, 2012.
* [18] Kirthevasan Kandasamy, Gautam Dasarathy, Junier B Oliva, Jeff Schneider, and Barnabas Poczos. Gaussian process bandit optimisation with multi-fidelity evaluations. _Advances in neural information processing systems_, 29, 2016.
* [19] Kirthevasan Kandasamy, Gautam Dasarathy, Barnabas Poczos, and Jeff Schneider. The multi-fidelity multi-armed bandit. _Advances in neural information processing systems_, 29, 2016.
* [20] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnabas Poczos. Multi-fidelity bayesian optimisation with continuous approximations. In _International Conference on Machine Learning_, pages 1799-1808. PMLR, 2017.
* [21] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best-arm identification in multi-armed bandit models. _The Journal of Machine Learning Research_, 17(1):1-42, 2016.
* [22] Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. _Advances in applied mathematics_, 6(1):4-22, 1985.
* [23] Tor Lattimore and Csaba Szepesvari. Cleaning up the neighborhood: A full classification for adversarial partial monitoring. In _Algorithmic Learning Theory_, pages 529-556. PMLR, 2019.
* [24] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [25] Liam Li, Kevin Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-Tzur, Moritz Hardt, Benjamin Recht, and Ameet Talwalkar. A system for massively parallel hyperparameter tuning. _Proceedings of Machine Learning and Systems_, 2:230-246, 2020.
* [26] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. _The Journal of Machine Learning Research_, 18(1):6765-6816, 2017.
* [27] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. _Journal of Machine Learning Research_, 5(Jun):623-648, 2004.
* [28] Felix Mohr and Jan N van Rijn. Learning curves for decision making in supervised machine learning-a survey. _arXiv preprint arXiv:2201.12150_, 2022.
* [29] Florian Pfisterer, Lennart Schneider, Julia Moosbauer, Martin Binder, and Bernd Bischl. Yahpo gym-an efficient multi-objective multi-fidelity benchmark for hyperparameter optimization. In _First Conference on Automated Machine Learning (Main Track)_, 2022.
* [30] Riccardo Poiani, Alberto Maria Metelli, and Marcello Restelli. Multi-fidelity best-arm identification. _Advances in Neural Information Processing Systems_, 35:17857-17870, 2022.
* [31] Pierre-Elouan Rethore, Peter Fuglsang, Gunner C Larsen, Thomas Buhl, Torben J Larsen, and Helge A Madsen. Topfarm: Multi-fidelity optimization of wind farms. _Wind Energy_, 17(12):1797-1816, 2014.
* [32] Lingxiao Zheng, Tyson L Hedrick, and Rajat Mittal. A multi-fidelity modelling approach for evaluation and optimization of wing stroke aerodynamics in flapping flight. _Journal of Fluid Mechanics_, 721:118-154, 2013.

## Appendix A Additional Remarks on Analysis

### Remarks on Best Arm Identification

**Remark A.1** (On choice among Explore-A and Explore-B).: For when do we prefer Explore-A than Explore-B, one can informally say if \(\tilde{G}\) is small (\(\tilde{G}=O(M\tilde{H})\)), Explore-A is preferred, and, otherwise if \(\tilde{G}\) is large, Explore-B is preferred. Recall that

\[\tilde{G}\coloneqq\sum_{k\in\mathcal{K}}\sum_{m\neq\tilde{m}_{k}^{*}}\left( \frac{\tilde{\Delta}_{\tilde{k}}^{(\tilde{m}_{k}^{*})}}{\sqrt{\lambda^{( \tilde{m}_{k}^{*})}}}-\frac{\tilde{\Delta}_{k}^{(m)}}{\sqrt{\lambda^{(m)}}} \right)^{-2}.\]

To give an intuition of when \(\tilde{G}\) is large or small, let us neglect the tilde for simplicity and fix a suboptimal arm \(k\). The term \(\frac{\Delta_{k}^{(m)}}{\sqrt{\lambda^{(m)}}}\) corresponds to the effectiveness of pulling arm \(k\) at fidelity \(m\) for distinguishing suboptimal arm \(k\). As Explore-A aims to find out the optimal (most effective) fidelity \(m_{k}^{*}\) for distinguishing suboptimal arm \(k\), it needs to compare the effectiveness of different fidelities, that is, their _effectiveness gap_\(\frac{\Delta_{k}^{(m_{k}^{*})}}{\sqrt{\lambda^{(m_{k}^{*})}}}-\frac{\Delta_{k }^{(m)}}{\sqrt{\lambda^{(m)}}}\), which is similar to bandit's reward gap (reward mean difference) \(\mu^{*}-\mu_{k}\). Therefore, for this suboptimal arm \(k\), if all its _effectiveness gaps_ are large, that is, the effectiveness of its optimal fidelity is much larger than other suboptimal fidelities, then the contribution of this arm \(k\) to the quantity \(G\) would be small, and _vice versa_. If for all arms \(k\in\mathcal{K}\), all of their effectiveness gaps are large, then the \(G\) would be small; otherwise, \(G\) can be large.

For a more sensible example, one can check the simulation set up (deferred to Appendix A) of the empirical comparison of Explore-A and Explore-B in Figure 1. In Figure 0(a)'s set up, the effectiveness gaps are large and thus \(G\) is small, hence Explore-A outperforms Explore-B, while in Figure 0(b)'s set up, the effectiveness gaps are small and thus \(G\) is large, then Explore-B outperforms Explore-A.

Practically, one can just run both Explore-A and -B in parallel for each arm \(k\). If Explore-A starts to converge on a fidelity, i.e., a large proportion of pull times on this arm is at this fidelity, we stop Explore-B and continue Explore-A. Otherwise, if Explore-B commits to a fidelity at first, we stop Explore-A and follow Explore-B to commit this fidelity. With this heuristic approach, we can expect the empirical cost is close to the smaller one of running either Explore-A or Explore-B alone.

**Remark A.2** (On the choice of "half" in Explore-B as good fidelity).: The selection of "half" is just for the simplicity of presentation, and any constant between \((0,1)\) would work. If a constant \(C\in(0,1)\) is used in Eq.(6), then the factor \(3\) of right hand side in the condition of Line 10 of Explore-B becomes \(\frac{1+C}{1-C}\), and there would be an additional \((C^{-2}+(1-C)^{-2})\) multiplier appearing in the cost complexity upper bound of Explore-B in Eq.(8). In the \((C^{-2}+(1-C)^{-2})\) multiplier, the first term \(C^{-2}\) factor is because a small \(C\) implies that Explore-B may commit to a very inefficient suboptimal fidelity and thus suffer a high cost, while the second term \((1-C)^{-2}\) corresponds to that if \(C\) is close to \(1\), then it becomes very difficult for Explore-B to find a good fidelity to commit, and thus Explore-B has to spend a large cost in exploration.

### Remarks on Regret Minimization

**Remark A.3** (Relation to partial monitoring).: Recall that, under our regret definition in Eq.(9), the optimal policy is to pull the optimal arm \(1\) at the lowest fidelity, and any exploration at the highest fidelity results in nonzero regret. This clear separation of exploration and exploitation is similar to the hard case of partial monitoring [2, 23] where the decision maker needs to choose the globally observable actions with nonzero regret cost to do exploration and then, when exploiting, choose locally observable actions. The regret of the hard case of partial monitoring is \(\Theta(T^{2/3})\) where \(T\) is the decision round, which is also similar to our \(\tilde{O}(\Lambda^{2/3})\) regret bound.

Simulation Details of Figure 1

In Figure 1, we report the cost complexities of Algorithm 1 with Explore-A and Algorithm 1 with Explore-B (let \(\tilde{\mu}_{1}^{(M)}=0.95\) and \(\tilde{\mu}_{2}^{(M)}=0.75\)). We set the confidence parameter \(\delta\) as \(0.05,0.1,0.15,0.2,0.25\) respectively in comparing the performance of both procedures. For each simulation, we run \(100\) trials, plot their cost complexities' mean as markers and their deviation as shaded regions. We present the parameters of MF-MAB instances of Figures 0(a) and 0(b) in Tables 1 and 0(c) respectively. We note that it is more difficult to find the optimal fidelity \(m_{k}^{*}\) in the MF-MAB instances for Figure 0(b) because (1) there are more fidelities choices in this instance than that of Figure 0(a); (2) the value of \(\tilde{\Delta}_{k}^{(m)}/\sqrt{\lambda^{(m)}}\) are closer in the second instance than that of Figure 0(a).

## Appendix C Additional Experiments

In this section, we present an additional numerical simulation to compare our two exploration policies, Explore-A and Explore-B, with a naive baseline that is always selecting the highest fidelity to explore arms. All three policies are employed under the LUCB algorithmic framework (Algorithm 1). This simulation aims to illustrate when our exploration policies Explore-A and Explore-B are better than the naive baseline.

**Experiment set up.** We consider a MF-MAB model consisting of \(K=5\) arms and \(M=5\) fidelities. The reward means are set as \(\mu_{k}^{(m)}=0.1(k+4)+0.01(m-5)\), where, for example, \(\mu_{5}^{(5)}=0.9\) is the true reward mean of optimal arm and its low-fidelity reward means are \(\{0.89,0.88,0.87,0.86\}\) accordingly, etc. The error upper bound in different fidelities \(\zeta^{(m)}\) are \(\{0.04,0.03,0.02,0.01,0\}\). We set the cost of fidelity \(m\in\{1,2,3,4\}\) as \(\lambda^{(m)}=m\) respectively, and consider four different cases \(I\in\{1,2,3,4\}\) of the cost of highest fidelity \(\lambda^{(5)}=5I\). That is, from Case 1 to Case 4, the only difference is that the cost of highest fidelity increases.

We set the confidence parameter \(\delta=0.1\) and run each experiment for \(100\) trials. Averaged over these \(100\) trials of each experiment, we present the empirical cost complexity in the table as follows,

**Discussion of additional simulation result.** In all four cases, the cost complexity of Explore-A does not change too much. Especially, when the cost of highest fidelity becomes larger, it becomes

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Parameters & \(\mu_{1}^{(m)}\) & \(\mu_{2}^{(m)}\) & \(\mu_{3}^{(m)}\) & \(\mu_{4}^{(m)}\) & \(\mu_{5}^{(m)}\) & \(\zeta^{(m)}\) & \(\lambda^{(m)}\) \\ \hline \(m=1\) & \(0.70\) & \(0.75\) & \(0.50\) & \(0.50\) & \(0.30\) & \(0.30\) & \(1\) \\ \(m=2\) & \(0.80\) & \(0.775\) & \(0.60\) & \(0.55\) & \(0.45\) & \(0.15\) & \(1.1\) \\ \(m=3\) & \(0.90\) & \(0.80\) & \(0.70\) & \(0.60\) & \(0.50\) & \(0\) & \(1.2\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Figure 0(a)s MF-MAB with \(K=5\) arms and \(M=3\) fidelities

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Parameters & \(\mu_{1}^{(m)}\) & \(\mu_{2}^{(m)}\) & \(\mu_{3}^{(m)}\) & \(\mu_{4}^{(m)}\) & \(\mu_{5}^{(m)}\) & \(\zeta^{(m)}\) & \(\lambda^{(m)}\) \\ \hline \(m=1\) & \(0.83\) & \(0.82\) & \(0.76\) & \(0.82\) & \(0.70\) & \(0.10\) & \(1\) \\ \(m=2\) & \(0.84\) & \(0.83\) & \(0.80\) & \(0.80\) & \(0.72\) & \(0.08\) & \(1.1\) \\ \(m=3\) & \(0.85\) & \(0.85\) & \(0.80\) & \(0.82\) & \(0.74\) & \(0.06\) & \(1.2\) \\ \(m=4\) & \(0.85\) & \(0.86\) & \(0.80\) & \(0.80\) & \(0.76\) & \(0.04\) & \(1.3\) \\ \(m=5\) & \(0.90\) & \(0.88\) & \(0.86\) & \(0.84\) & \(0.80\) & \(0\) & \(1.4\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Figure 0(b)s MF-MAB with \(K=5\) arms and \(M=5\) fidelitieseasier for Explore-A to find out the optimal fidelity (e.g., the highest fidelity is obviously not the optimal one) and hence enjoy a lower cost complexity. The cost complexity of both Explore-B and Baseline increases over these four cases.

When the cost of highest fidelity is expensive, e.g., Cases 3 and 4, both Explore-A and Explore-B outperform Baseline. This highlights the advantage of utilizing low fidelity to explore arms when the cost of high fidelity is large. When the cost of highest fidelity is cheap, e.g., Case 1, the cost complexity of Explore-A is similar to Baseline, while Explore-B is worse than Baseline. This suggests that when the cost of highest fidelity is similar to that of lower fidelity, the Baseline may be preferred.

In this comparison, Explore-A always outperforms Explore-B. For another scenario where Explore-B is better than Explore-B, please refer to Figure 0(b).

## Appendix D A Third Fidelity Selection Procedure: Explore-C

Besides the Explore-A and -B procedures, here we consider a third naive and conservative idea for fidelity selection that one should start from low risk (cost), gradually increase the risk (cost) as the learning task needs, and stop when finding the optimal arm. To decide when to increase the fidelity for exploring an arm \(k\), we use the arm's confidence radius \(\beta(N_{k,t}^{(m)},t,\delta)\) at fidelity \(m\) as a measure of the amount of information left in this fidelity, and when the fidelity \(m\)'s confidence radius is less than the error upper bound \(\zeta^{(m)}\) at this fidelity, we increase the fidelity by \(1\) for higher accuracy, or formally, the fidelity is selected as follows,

\[m_{k,t}\leftarrow\min\left\{m\left|\beta(N_{k,t}^{(m)},t,\delta)\geqslant \zeta^{(m)}\right.\right\}.\]

``` procedureExplore-C(\(k\)) \(m_{k,t}\leftarrow\min\left\{m\left|\beta(N_{k,t}^{(m)},t,\delta)\geqslant \zeta^{(m)}\right.\right\}\)  Pull \((k,m_{k,t})\), observe reward, and update corresponding statistics ```

**Algorithm 4** Explore-C Procedures

**Theorem D.1** (Cost complexity upper bounds for Algorithm 1 with Explore-C).: _Given \(L\geqslant 4KM\), Algorithm 1 outputs the optimal arm with a probability at least \(1-\delta\). The cost complexity of Algorithm 1 with Explore-C are upper bounded as follows,_

\[\mathbb{E}[\Lambda]=O\left(H^{\ddagger}\log\left(\frac{L(H^{\ddagger}+Q)}{ \lambda^{(1)}\delta}\right)+Q\log\left(\frac{L(H^{\ddagger}+Q)}{\lambda^{(1)} \delta}\right)\right),\] (12)

_where, letting \(m_{k}^{\ddagger}\) denote the smallest fidelity for arm \(k\) such that \(\Delta_{k}^{(m)}>2\zeta^{(m)}\), or formally,_

\[m_{k}^{\ddagger}\coloneqq\min\{m:\Delta_{k}^{(m)}>2\zeta^{(m)}\},\]

_and we denote_

\[H^{\ddagger}\coloneqq\underset{k\in K}{\underbrace{\lambda^{(m_{k}^{\ddagger })}}},\quad Q\coloneqq\underset{k\in\mathcal{K}}{\sum}\sum_{m=1}^{m_{k}^{ \ddagger}-1}\frac{\lambda^{(m)}}{(\zeta^{(m)})^{2}}.\]

**Remark D.2** (Explore-C _vs._ Explore-A and -B).: The Explore-C procedure does not require additional knowledge as the other two. It is a one-size-fits-all option. If with some addition information of a specific scenario, e.g., the exact or approximated reward means of top two arms, one can use the Explore-A or B.

## Appendix E Proofs for Best Arm Identification with Fixed Confidence

### Proof of Theorem 3.1

**Lemma E.1** (Kaufmann et al. [21], Lemma 1).: _Let \(\nu\) and \(\nu^{\prime}\) be two bandit models with \(K\) arms such that for all \(k\), the distributions \(\nu_{k}\) and \(\nu_{k}^{\prime}\) are mutually absolutely continuous. For any almost-surelyfinite stopping time \(\sigma\) with respect to the filtration \(\{\mathcal{F}_{t}\}_{t\in\mathbb{N}}\) where \(\mathcal{F}_{t}=\sigma(I_{1},X_{1},\ldots,I_{t},X_{t})\),_

\[\sum_{k=1}^{K}\mathbb{E}_{\nu}[N_{k}(\sigma)]\operatorname{KL}(\nu_{k},\nu^{ \prime}_{k})\geqslant\sup_{\mathcal{E}\in\mathcal{F}_{\sigma}}\operatorname{ kl}(\mathbb{P}_{\nu}(\mathcal{E}),\mathbb{P}_{\nu^{\prime}}(\mathcal{E})),\]

_where \(\operatorname{kl}(x,y)\) is the binary relative entropy._

In MF-MAB model, regarding each arm-fidelity \((k,m)\)-pair as an individual arm, we can extend Lemma E.1 to multi-fidelity case as follows,

\[\sum_{k=1}^{K}\sum_{m=1}^{M}\mathbb{E}_{\nu}[N_{k}^{(m)}(\sigma)]\operatorname {KL}(\nu_{k}^{(m)},\nu^{\prime}_{k})\geqslant\sup_{\mathcal{E}\in\mathcal{F}_ {\sigma}}\operatorname{kl}(\mathbb{P}_{\nu}(\mathcal{E}),\mathbb{P}_{\nu^{ \prime}}(\mathcal{E})).\] (13)

Next, we construct instances \(\nu\) and \(\nu^{\prime}\). We set the reward distributions \(\nu=(\nu_{k}^{(m)})_{(k,m)\in\mathcal{K}\times\mathcal{M}}\) as Bernoulli and the reward means fulfill \(\mu_{1}^{(M)}>\mu_{2}^{(M)}\geqslant\mu_{3}^{(M)}\geqslant\ldots\geqslant \mu_{K}^{(M)},\) where \(\mu_{k}^{(m)}=\mathbb{E}_{X\sim\nu_{k}^{(m)}}[X].\) We let \(\nu^{\prime}_{k}{}^{(m)}\) be the same to \(\nu_{k}^{(m)}\) for all \(k\) and \(m\), except for that an arm \(\ell\neq 1\). We set arm \(\ell\)'s reward means on fidelities \(m\in\mathcal{M}_{k}\) to be \(\nu^{\prime}_{\ell}{}^{(m)}=\nu_{1}^{(M)}-\zeta^{(m)}+\epsilon\). So, in instance \(\nu^{\prime}\), the optimal arm is \(\ell\) and its true reward mean \(\mu^{\prime}_{\ell}{}^{(M)}\) is slightly greater than \(\mu_{1}^{(M)}\). This implies for the event \(\mathcal{E}=\{\text{output arm }1\}\) and any algorithm \(\pi\) that can find the optimal arm with a confidence \(1-\delta\), \(\mathbb{P}_{\nu,\pi}(\mathcal{E})\geqslant 1-\delta\) and \(\mathbb{P}_{\nu^{\prime},\pi}(\mathcal{E})\leqslant\delta\). Then, from Eq.(13), we have

\[\sum_{m\in\mathcal{M}_{k}}\mathbb{E}_{\nu}[N_{\ell}^{(m)}(\sigma )]\operatorname{KL}(\nu_{\ell}^{(m)},\nu^{\prime}_{\ell}) \geqslant\sup_{\mathcal{E}\in\mathcal{F}_{\sigma}}\operatorname{ kl}(\mathbb{P}_{\nu,\pi}(\mathcal{E}),\mathbb{P}_{\nu^{\prime},\pi}(\mathcal{E}))\] \[\geqslant\operatorname{kl}(1-\delta,\delta)\] \[\geqslant\log\frac{1}{2.4\delta}.\]

We rewrite the above inequality as follows,

\[\sum_{m\in\mathcal{M}_{k}}\lambda^{(m)}\mathbb{E}_{\nu}[N_{\ell}^{(m)}(\sigma )]\cdot\frac{\operatorname{KL}(\nu_{\ell}^{(m)},\nu^{\prime}_{\ell}{}^{(m)})} {\lambda^{(m)}}\geqslant\log\frac{1}{2.4\delta}.\]

Therefore, for the arm \(\ell\), our aim is to minimize its cost complexity with a constraint as follows,

\[\min_{\mathbb{E}[N_{\ell}^{(m)}],\forall m}\sum_{m=1}^{M}\lambda ^{(m)}\mathbb{E}_{\nu}[N_{\ell}^{(m)}(\sigma)]\] \[\text{such that }\sum_{m\in\mathcal{M}_{k}}\lambda^{(m)}\mathbb{E}_{\nu}[N _{\ell}^{(m)}(\sigma)]\cdot\frac{\operatorname{KL}(\nu_{\ell}^{(m)},\nu^{ \prime}_{\ell}{}^{(m)})}{\lambda^{(m)}}\geqslant\log\frac{1}{2.4\delta}.\]

Note that the above is a linear programming (LP) and its optimum is reached at one of its polyhedron constraint's vertex--only one \(\mathbb{E}[N_{\ell}^{(m)}]\) is positive and all others are equal to zero.

\[\min_{\mathbb{E}[N_{\ell}^{(m)}],\forall m}\sum_{m=1}^{M}\lambda^{ (m)}\mathbb{E}_{\nu}[N_{\ell}^{(m)}(\sigma)] \overset{(a)}{\geqslant}\min_{m\in\mathcal{M}_{k}}\frac{\lambda ^{(m)}}{\operatorname{KL}(\nu_{\ell}^{(m)},\nu^{\prime}_{\ell}{}^{(m)})}\log \frac{1}{2.4\delta}\] \[=\min_{m\in\mathcal{M}_{k}}\frac{\lambda^{(m)}}{\operatorname{KL}( \nu_{\ell}^{(m)},\nu_{1}^{(M)}-\zeta^{(m)}+\epsilon)}\log\frac{1}{2.4\delta}\] \[\overset{(b)}{\geqslant}\min_{m\in\mathcal{M}_{k}}\frac{\lambda ^{(m)}}{(1+\varepsilon)\operatorname{KL}(\nu_{\ell}^{(m)},\nu_{1}^{(M)}-\zeta^{ (m)})}\log\frac{1}{2.4\delta}\]

where the inequality (a) is due to the property of LP we mentioned above, and the inequality (b) is because of the continuity of KL-divergence.

To bound the optimal arm \(1\)'s cost complexity, we use the same \(\nu\) as above and construct another instance \(\nu^{\prime\prime}\). The instance \(\nu^{\prime\prime}\)'s reward means are the same to \(\nu\) except for arm \(1\) whose reward meansfor fidelity \(m\in\mathcal{M}_{1}\) are set as \({\mu^{\prime}}_{1}^{(m)}=\mu_{2}^{(m)}+\zeta^{(m)}-\epsilon\). Then, with similar procedure as the above, we obtain

\[\min_{\mathbb{E}[N_{1}^{(m)}],\forall m}\sum_{m=1}^{M}\lambda^{(m)} \mathbb{E}_{\nu}[N_{1}^{(m)}(\sigma)]\geqslant\min_{m\in\mathcal{M}_{1}}\frac {\lambda^{(m)}}{(1+\varepsilon)\operatorname{KL}(\nu_{1}^{(m)},\nu_{2}^{(M)} +\zeta^{(m)})}\log\frac{1}{2.4\delta}.\]

Summing up the above costs leads to the lower bound as follows, and letting the \(\epsilon\) goes to zeros concludes the proof.

\[\mathbb{E}[\Lambda]\geqslant\] \[\left(\min_{m\in\mathcal{M}_{1}}\frac{\lambda^{(m)}}{(1+ \varepsilon)\operatorname{KL}(\nu_{1}^{(m)},\nu_{2}^{(M)}+\zeta^{(m)})}+\sum_ {k\neq 1}\min_{m\in\mathcal{M}_{k}}\frac{\lambda^{(m)}}{(1+\varepsilon) \operatorname{KL}(\nu_{k}^{(m)},\nu_{1}^{(M)}-\zeta^{(m)})}\right)\log\frac{1 }{2.4\delta}.\]

### Proof of Theorem 3.4

**Notation.** Denote the threshold \(c=\frac{\mu_{1}^{(M)}+\max_{k}(\mu_{k}^{(m)}+\zeta^{(m_{k}^{*})})}{2}\) as the average between the optimal arm's reward mean and the highest reward mean upper bound of other arms \(k\) at fidelity \(m_{k}^{*}\). Denote \(\mathcal{A}_{t}\coloneqq\{k\in\mathcal{K}:\texttt{LCB}_{k,t}>c\}\) and \(\mathcal{B}_{t}\coloneqq\{k\in\mathcal{K}:\texttt{UCB}_{k,t}<c\}\) as the above and below sets which respectively contain arms whose rewards are clearly higher or lower than the threshold with high probability, and let \(\mathcal{C}_{t}\coloneqq\mathcal{K}\setminus(\mathcal{A}_{t}\cup\mathcal{B}_{t})\) as the complement of both sets' union. Then, we define two events as follows

\[\texttt{TERM}_{t} \coloneqq\{\texttt{LCB}_{t_{t},t}>\texttt{UCB}_{u_{t},t}\},\] \[\texttt{CROS}_{t} \coloneqq\{\exists k\neq 1:k\in\mathcal{A}_{t}\}\cup\{1\in \mathcal{B}_{t}\}.\]

The \(\texttt{TERM}_{t}\) event corresponds to the complement of the main while loop condition in the LUCB algorithm. When the \(\texttt{TERM}_{t}\) event happens, the LUCB algorithm terminates. The \(\texttt{CROS}_{t}\) event means there exists a suboptimal arm whose \(\texttt{LCB}_{k,t}\) is greater than \(c\) or that the optimal arm \(1\)'s \(\texttt{UCB}_{1,t}\) is less than \(c\), both of which means that at least one arm's reward mean confidence interval incorrectly crosses the threshold \(c\).

Step 1. Prove \(\neg\texttt{TERM}_{t}\cap\neg\texttt{CROS}_{t}\Longrightarrow(\ell_{t}\in \mathcal{C}_{t})\cup(u_{t}\in\mathcal{C}_{t})\).We show this statement by contradiction case by case. That is, the negation of \((\ell_{t}\in\mathcal{C}_{t})\cup(u_{t}\in\mathcal{C}_{t})\) cannot happen when \(\neg\texttt{TERM}_{t}\cap\neg\texttt{CROS}_{t}\).

**Case 1:**\((\ell_{t}\in\mathcal{A}_{t})\cap(u_{t}\in\mathcal{A}_{t})\cap\neg\texttt{TERM }_{t}\)

\(\implies\)\((\ell_{t}\in\mathcal{A}_{t})\cap(u_{t}\in\mathcal{A}_{t})\Longrightarrow| \mathcal{A}_{t}|\geqslant 2\Longrightarrow\exists k\neq 1:k\in\mathcal{A}_{t} \Longrightarrow\texttt{CROS}_{t},\)

**Case 2:**\((\ell_{t}\in\mathcal{B}_{t})\cap(u_{t}\in\mathcal{A}_{t})\cap\neg\texttt{TERM }_{t}\)

\(\implies\)\(\texttt{UCB}_{\ell_{t},t}<c<\texttt{LCB}_{u_{t},t}<\texttt{UCB}_{u_{t},t}\Longrightarrow\emptyset\) (contradicts the selection of \(\ell_{t}\) and \(u_{t}),\)

**Case 3:**\((\ell_{t}\in\mathcal{A}_{t})\cap(u_{t}\in\mathcal{B}_{t})\cap\neg\texttt{TERM }_{t}\)

\(\implies\)\(\texttt{LCB}_{\ell_{t},t}>c>\texttt{UCB}_{u_{t},t}\)\(\cap\neg\texttt{TERM}_{t}\Longrightarrow\emptyset,\)

**Case 4:**\((\ell_{t}\in\mathcal{B}_{t})\cap(u_{t}\in\mathcal{B}_{t})\cap\neg\texttt{TERM }_{t}\)

\(\implies\)\((\ell_{t}\in\mathcal{B}_{t})\cap(u_{t}\in\mathcal{B}_{t})\Longrightarrow| \mathcal{B}_{t}|=K\Longrightarrow 1\in\mathcal{B}_{t}\Longrightarrow\texttt{CROS}_{t}.\)Step 2. Prove \(\mathbb{P}\left(\texttt{CROS}_{t}\right)\leqslant\frac{KM\delta}{Lt^{3}}\).For any suboptimal arm \(k\neq 1\), we bound the probability that the arm \(k\) is in \(\mathcal{A}_{t}\) as follows.

\[\mathbb{P}(k\in\mathcal{A}_{t}) =\mathbb{P}(\texttt{LCB}_{k,t}>c)=\mathbb{P}\left(\max_{m\in \mathcal{M}}\texttt{LCB}_{k,t}^{(m)}>c\right)\leqslant\sum_{m\in\mathcal{M}} \mathbb{P}\left(\texttt{LCB}_{k,t}^{(m)}>c\right)\] \[=\sum_{m\in\mathcal{M}}\mathbb{P}\left(\hat{\mu}_{k,t}^{(m)}- \zeta^{(m)}-\beta(N_{k,t}^{(m)},t)>c\right)\] \[=\sum_{m\in\mathcal{M}}\mathbb{P}\left(\hat{\mu}_{k,t}^{(m)}-\mu_ {k}^{(m)}+(\mu_{k}^{(m)}-\zeta^{(m)}-c)>\beta(N_{k,t}^{(m)},t)\right)\] \[\overset{(a)}{\leqslant}\sum_{m\in\mathcal{M}}\mathbb{P}\left( \hat{\mu}_{k,t}^{(m)}-\mu_{k}^{(m)}>\beta(N_{k,t}^{(m)},t)\right)\leqslant \sum_{m\in\mathcal{M}}\sum_{n=1}^{t}\mathbb{P}(\hat{\mu}_{k,t}^{(m)}-\mu_{k}^ {(m)}>\beta(n,t))\] \[\leqslant\sum_{m\in\mathcal{M}}\sum_{n=1}^{t}\exp(-n(\beta(n,t)) ^{2})=\sum_{m\in\mathcal{M}}\sum_{n=1}^{t}\frac{\delta}{Lt^{4}}\] \[\leqslant\frac{M\delta}{Lt^{3}},\]

where the inequality (a) is due to that \(\mu_{k}^{(m)}-\zeta^{(m)}\leqslant\mu_{k}^{(M)}<c\). With similar derivation, we have \(\mathbb{P}(1\in\mathcal{B}_{t})\leqslant\frac{M\delta}{Lt^{3}}\). Noticing that \(\mathbb{P}(\texttt{CROS}_{t})\leqslant\sum_{k\neq 1}\mathbb{P}(k\in\mathcal{A}_{t}) +\mathbb{P}(1\in\mathcal{B}_{t})\), we have \(\mathbb{P}(\texttt{CROS}_{t})\leqslant\frac{KM\delta}{Lt^{3}}\).

Step 3. Prove \(\mathbb{P}\left(\exists k\in\mathcal{K}:(N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*}) \cap(k\in\texttt{Mid}_{t})\right)\leqslant\frac{16\delta\sum_{k\in\mathcal{K}} \Delta_{k}^{-2}}{Lt^{4}}\), where \(N_{k,t}^{*}\coloneqq\frac{\log(Lt^{4}/\delta)}{(\Delta_{k}^{(m_{k}^{*})})^{2}}\).For any fixed suboptimal arm \(k\neq 1\) (with \(\mu_{k}^{(M)}<c\)), we have

\[\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*})\cap(k\in \texttt{Mid}_{t})\right)\] \[=\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*})\cap(k\not \in\mathcal{A}_{t}\cup\mathcal{B}_{t})\right)\] \[\leqslant\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*}) \cap(\texttt{UCB}_{k,t}>c)\right)\] \[=\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*})\cap\left( \min_{m\in\mathcal{M}}\hat{\mu}_{k,t}^{(m)}+\zeta^{(m)}+\beta(N_{k,t}^{(m)},t) >c\right)\right)\] \[\leqslant\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*}) \cap(\hat{\mu}_{k,t}^{(m_{k}^{*})}+\zeta^{(m_{k}^{*})}+\beta(N_{k,t}^{(m_{k}^{ *})},t)>c)\right)\] \[\leqslant\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16N_{k,t}^{*}) \cap(\hat{\mu}_{k,t}^{(m_{k}^{*})}-\mu_{k}^{(m_{k}^{*})}>(c-\mu_{k}^{(m_{k}^{ *})}-\zeta^{(m_{k}^{*})})-\beta(N_{k,t}^{(m_{k}^{*})},t))\right)\] \[\overset{(a)}{\leqslant}\mathbb{P}\left((N_{k,t}^{(m_{k}^{*})}>16 N_{k,t}^{*})\cap\left(\hat{\mu}_{k,t}^{(m_{k}^{*})}-\mu_{k}^{(m_{k}^{*})}>\frac{ \Delta_{k}^{(\hat{m}_{k}^{*})}}{2}-\beta(N_{k,t}^{(\hat{m}_{k}^{*})},t)\right)\right)\] \[\overset{(b)}{\leqslant}\sum_{\tau>16N_{k,t}^{*}}\mathbb{P} \left(\hat{\mu}_{k,t(\tau)}^{(m_{k}^{*})}-\mu_{k}^{(m_{k}^{*})}>\frac{\Delta_{ k}^{(\hat{m}_{k}^{*})}}{4}\right)\] \[\leqslant\sum_{\tau>16N_{k,t}^{*}}\exp\left(-\frac{\tau(\Delta_{ k}^{(\hat{m}_{k}^{*})})^{2}}{16}\right)\leqslant\int_{\tau>16N_{k,t}^{*}}\exp \left(-\frac{\tau(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}}{16}\right)d\tau\leqslant \frac{16\delta}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}Lt^{4}},\]

where inequality (a) is due to the definition of \(c\), and inequality (b), denoting \(\hat{\mu}_{k,t(\tau)}^{(m_{k}^{*})}\) as the empirical mean of \(\tau\) observations, is due to \(\beta(\tau,t)<\frac{\Delta_{k}}{4}\) for \(\tau>16N_{k,t}^{*}\).

From Step 3, we obtain that the following equation holds with high probability,

\[N_{k,t}^{(\hat{m}_{k}^{*})}\leqslant\frac{16}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2} }\log\left(\frac{Lt^{4}}{\delta}\right)\leqslant\frac{64}{(\Delta_{k}^{(\hat{m}_ {k}^{*})})^{2}}\log\left(\frac{Lt}{\delta}\right).\] (14)Next, we respectively present the cost complexity upper bounds for different fidelity selection procedures in Algorithm 2.

#### e.2.1 Proof for Explore-A's Upper Bound

**Step 4 for Explore-A: prove that if the small probability events of Steps 2 and 3 do not happen, then the algorithm terminates with a high probability when \(\Lambda\) is large.**

**Lemma E.2**.: _Give reward means \(\mu_{1}^{(M)}\) and \(\mu_{2}^{(M)}\). For a fixed arm \(k\), there exist \(\bar{N}_{k,t}\) and \(\alpha_{k}>0\) such that when \(N_{k,t}>\bar{N}_{k,t},\)\(N_{k,t}<2N_{k,t}^{(\tilde{m}_{k}^{*})},\) the number of times of pulling this arm \(k\) at fidelities \(m\left(\neq\tilde{m}_{k}^{*}\right)\) is \(O(\log(\log N_{k,t}))\), or formally,_

\[N_{k,t}^{(m)}\leq\frac{8}{\lambda^{(m)}}\left(\frac{\Delta_{k}^{(\tilde{m}_{k} ^{*})}}{\sqrt{\lambda^{(\tilde{m}_{k}^{*})}}}-\frac{\Delta_{k}^{(m)}}{\sqrt{ \lambda^{(m)}}}\right)^{-2}\log N_{k,t},\;\forall m\neq\tilde{m}_{k}^{*}.\] (15)

Combine Lemma E.2 with Eq.(14) in Step 3, we have, for any arm \(k\) and fidelity \(m\neq\tilde{m}_{k}^{*}\):

\[N_{k,t}^{(m)}\leq\frac{8}{\lambda^{(m)}}\left(\frac{\Delta_{k}^{(\tilde{m}_{k} ^{*})}}{\sqrt{\lambda^{(\tilde{m}_{k}^{*})}}}-\frac{\Delta_{k}^{(m)}}{\sqrt{ \lambda^{(m)}}}\right)^{-2}\log\left(\frac{128}{(\Delta_{k}^{(\tilde{m}_{k}^{ *})})^{2}}\log\left(\frac{Lt}{\delta}\right)\right)\] (16)

Next, we can upper bound the total cost of the LUCB algorithm (before it terminating) via Eq.(14) and Eq.(16). Specially, we show it is impossible for \(\Lambda=C\left(H\log\frac{L(G+H)}{\lambda^{(1)}\delta}+G\log\log\frac{L(G+H)} {\lambda^{(1)}\delta}\right)\) via contradiction. Suppose \(\Lambda=C\left(H\log\frac{L(G+H)}{\lambda^{(1)}\delta}+G\log\log\frac{L(G+H)} {\lambda^{(1)}\delta}\right)\), we have the following,

\[\mathbb{E}[\Lambda] \leq\sum_{k\in\mathcal{K}}\sum_{m\in\mathcal{M}}\lambda^{(m)}N_ {k,t}^{(m)}\] \[\leq\sum_{k\in\mathcal{K}}\lambda^{(\tilde{m}_{k}^{*})}N_{k,t}^{ (\tilde{m}_{k}^{*})}+\sum_{k\in\mathcal{K}}\sum_{m\neq\tilde{m}_{k}^{*}} \lambda^{(m)}N_{k,t}^{(m)}\] \[\overset{(a)}{\leqslant}64H\log\frac{Lt}{\delta}+8G\log\log \frac{Lt}{\delta}+G\log(128H)\] \[\overset{(b)}{\leqslant}64H\log\frac{L\Lambda}{\lambda^{(1)} \delta}+8G\log\log\frac{L\Lambda}{\lambda^{(1)}\delta}+G\log(128H)\] \[\overset{(c)}{=}64H\log\left(\frac{L}{\lambda^{(1)}\delta}C \left(H\log\frac{L(G+H)}{\lambda^{(1)}\delta}+G\log\log\frac{L(G+H)}{\lambda^ {(1)}\delta}\right)\right)\] \[\quad+8G\log\log\left(\frac{L}{\lambda^{(1)}\delta}C\left(H\log \frac{L(G+H)}{\lambda^{(1)}\delta}+G\log\log\frac{L(G+H)}{\lambda^{(1)}\delta} \right)\right)+G\log(128H)\] \[\overset{(d)}{\leqslant}128(2+\log C)\left(H\log\frac{L(G+H)}{ \lambda^{(1)}\delta}+G\log\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right)\] \[\overset{(e)}{\leqslant}C\left(H\log\frac{L(G+H)}{\lambda^{(1)} \delta}+G\log\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right),\]

where the inequality (a) is due to Eq.(14) and Eq.(16), the inequality (b) is because \(t\leqslant\frac{\Lambda}{\lambda^{(1)}}\), the inequality (c) is by the supposition, the inequality (d) is by separately bounding the above first two terms via Eq.(17) and Eq.(18) in the following, and the inequality (e) holds for \(C>1200\). This above inequality contradicts the supposition, and, therefore, we conclude the cost complexity upper bound proof for Explore-A. Similar proof also holds for Explore-B by replacing \(\tilde{m}_{k}^{*}\) with \(m_{k}^{\dagger}\).

Next, we provide the upper bounds used in the inequality (d) above:

\[\begin{split}& H\log\left(\frac{L}{\lambda^{(1)}\delta}C\left(H\log \frac{L(G+H)}{\lambda^{(1)}\delta}+G\log\log\frac{L(G+H)}{\lambda^{(1)}\delta} \right)\right)\\ &\leqslant H\log\left(\frac{L}{\lambda^{(1)}\delta}CH\log\frac{L(G +H)}{\lambda^{(1)}\delta}\right)+H\log\left(\frac{L}{\lambda^{(1)}\delta}CG \log\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right)\\ &\leqslant H\log C+H\log\left(\frac{LH}{\lambda^{(1)}\delta}\log \frac{L(G+H)}{\lambda^{(1)}\delta}\right)+H\log C+H\log\left(\frac{LG}{\lambda^ {(1)}\delta}\log\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right)\\ &\leqslant 2H\log C+4H\log\frac{L(G+H)}{\lambda^{(1)}\delta}\\ &\leqslant(4+2\log C)H\log\frac{L(G+H)}{\lambda^{(1)}\delta}, \end{split}\] (17)

and

\[\begin{split}& G\log\log\left(\frac{L}{\lambda^{(1)}\delta}C\left(H \log\frac{L(G+H)}{\lambda^{(1)}\delta}+G\log\log\frac{L(G+H)}{\lambda^{(1)} \delta}\right)\right)\\ &\leqslant G\log\left(\frac{L}{\lambda^{(1)}\delta}CH\log\frac{L (G+H)}{\lambda^{(1)}\delta}\right)+G\log\log\left(\frac{L}{\lambda^{(1)}\delta }CG\log\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right)\\ &\leqslant G\log\log C+G\log\log\left(\frac{LH}{\lambda^{(1)} \delta}\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right)\\ &\quad+G\log\log C+G\log\log\left(\frac{LG}{\lambda^{(1)}\delta }\log\log\frac{L(G+H)}{\lambda^{(1)}\delta}\right)\\ &\leqslant 2G\log\log C+4G\log\log\frac{L(G+H)}{\lambda^{(1)} \delta}\\ &\leqslant(4+2\log\log C)G\log\log\frac{L(G+H)}{\lambda^{(1)} \delta}.\end{split}\] (18)

Proof of Lemma e.2.: 

**Claim 1**.: For any fixed \(m\neq\tilde{m}_{k}^{*}\), if the following equation holds, then the algorithm will not pull arm \(k\) at fidelity \(m\) with high probability.

\[N_{k,t}^{(m)}>\frac{8}{\lambda^{(m)}}\left(\frac{\Delta_{k}^{(\tilde{m}_{k}^{ *})}}{\sqrt{\lambda^{(\tilde{m}_{k}^{*})}}}-\frac{\Delta_{k}^{(m)}}{\sqrt{ \lambda^{(m)}}}\right)^{-2}\log N_{k,t}.\]

\[\mathtt{f-UCB}_{u_{t},t}^{(m)}(\mu_{1}^{(M)}) =\frac{1}{\sqrt{\lambda^{(m)}}}\left(\mu_{1}^{(M)}-\hat{\mu}_{u_ {t},t}^{(m)}-\zeta^{(m)}+\sqrt{\frac{2\log N_{u_{t},t}}{N_{u_{t},t}^{(m)}}}\right)\] \[\overset{(a)}{\leqslant}\frac{1}{\sqrt{\lambda^{(m)}}}\left(\mu _{1}^{(M)}-\mu_{u_{t}}^{(m)}-\zeta^{(m)}+2\sqrt{\frac{2\log N_{u_{t},t}}{N_{u _{t},t}^{(m)}}}\right)\] \[\overset{(b)}{\leqslant}\frac{1}{\sqrt{\lambda^{(m_{u_{t}}^{*})}} }\left(\mu_{1}^{(M)}-\mu_{u_{t}}^{(m_{u_{t}}^{*})}-\zeta^{(m_{u_{t}}^{*})}\right)\] \[\overset{(c)}{\leqslant}\frac{1}{\sqrt{\lambda^{(\tilde{m}_{k}^{ *})}}}\left(\mu_{1}^{(M)}-\hat{\mu}_{u_{t},t}^{(m_{u_{t}}^{*})}-\zeta^{(m_{u_{t }}^{*})}+\sqrt{\frac{2\log N_{u_{t},t}}{N_{u_{t},t}^{(\tilde{m}_{k}^{*})}}}\right)\] \[=\mathtt{f-UCB}_{u_{t},t}^{(m_{u_{t}}^{*})}(\mu_{1}^{(M)}),\]

where the inequalities (a) and (c) hold with a probability at least \(1-\frac{1}{(N_{k,t})^{2}}\) respectively (by Hoeffding's inequality), and the inequality (b) holds due to the equation in the claim.

#### e.2.2 Proof for Explore-B's Upper Bound

As Explore-B of Algorithm 2 also employs the LUCB framework, it shares the first three steps of the proof for Theorem 3.4 in Appendix E.2. Hence, in this part, we focus on the proof of the final cost complexity upper bound.

**Lemma E.3**.: _If the condition in Line 10 holds, then the committed fidelity \(\hat{m}_{k}^{*}\) fulfills the following inequality:_

\[2\cdot\frac{\Delta_{k}^{(\hat{m}_{k}^{*})}}{\sqrt{\lambda^{(\hat{m}_{k}^{*})}} }\geqslant\frac{\Delta_{k}^{(\hat{m}_{k}^{*})}}{\sqrt{\lambda^{(\hat{m}_{k}^{* })}}}.\] (19)

Proof of Lemma e.3.: Eq.(19) is proved as follows,

\[\frac{\Delta_{k}^{(\hat{m}_{k}^{*})}/\sqrt{\lambda^{(\hat{m}_{k}^ {*})}}}{\Delta_{k}^{(\hat{m}_{k}^{*})}/\sqrt{\lambda^{(\hat{m}_{k}^{*})}}} \leqslant\frac{(\hat{\mu}_{*}^{(M)}-(\hat{\mu}_{k}^{(\hat{m}_{k}^ {*})}+\zeta^{(\hat{m}_{k}^{*})}))/\sqrt{\lambda^{(\hat{m}_{k}^{*})}}+\sqrt{ \log(2KM/\delta)/\lambda^{(1)}N_{k,t}^{(m)}}}{(\hat{\mu}_{*}^{(M)}-(\hat{\mu} _{k}^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_{k}^{*})}))/\sqrt{\lambda^{(\hat{m}_ {k}^{*})}}-\sqrt{\log(2KM/\delta)/\lambda^{(1)}N_{k,t}^{(m)}}}\] \[\overset{(a)}{\leqslant}\frac{(\hat{\mu}_{*}^{(M)}-(\hat{\mu}_{k }^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_{k}^{*})}))/\sqrt{\lambda^{(\hat{m}_{k}^ {*})}}+\sqrt{\log(2KM/\delta)/\lambda^{(1)}N_{k,t}^{(m)}}}{(\hat{\mu}_{*}^{(M) }-(\hat{\mu}_{k}^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_{k}^{*})}))/\sqrt{ \lambda^{(\hat{m}_{k}^{*})}}-\sqrt{\log(2KM/\delta)/\lambda^{(1)}N_{k,t}^{(m) }}}\] \[\leqslant 1+\frac{2\sqrt{\log(2KM/\delta)/\lambda^{(1)}N_{k,t}^{(m) }}}{(\hat{\mu}_{*}^{(M)}-(\hat{\mu}_{k}^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_ {k}^{*})}))/\sqrt{\lambda^{(\hat{m}_{k}^{*})}}-\sqrt{\log(2KM/\delta)/\lambda^ {(1)}N_{k,t}^{(m)}}}\] \[\overset{(b)}{\leqslant}1+\frac{2\sqrt{\log(2KM/\delta)/\lambda^ {(1)}N_{k,t}^{(m)}}}{2\sqrt{\log(2KM/\delta)/\lambda^{(1)}N_{k,t}^{(m)}}}=2,\]

where inequality (a) is due to the definition of \(\hat{m}_{k}^{*}\), and inequality (b) is due to the condition in Line 10. 

We next upper bound the number of times of \(N_{k,t}^{(m)}\) that guarantees that the condition in Line 10 is true. Let us consider the case of exploring arm \(u_{t}\).

\[\max_{m\in\mathcal{M}}\frac{\hat{\Delta}_{k,t}^{(m)}}{\sqrt{ \lambda^{(m)}}} \geqslant\frac{\hat{\mu}_{k_{*}}^{(M)}-(\hat{\mu}_{k}^{(\hat{m}_{k }^{*})}+\zeta^{(\hat{m}_{k}^{*})})}{\sqrt{\lambda^{(\hat{m}_{k}^{*})}}}\] \[\overset{(a)}{\geqslant}\frac{\hat{\mu}_{k_{*}}^{(M)}-(\mu_{k} ^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_{k}^{*})})}{\sqrt{\lambda^{(\hat{m}_{k}^ {*})}}}-\sqrt{\frac{\log(2KM/\delta)}{\lambda^{(1)}N_{k,t}^{(m)}}}\] \[\overset{(b)}{\geqslant}\frac{\Delta_{k}^{(\hat{m}_{k}^{*})}}{ \sqrt{\lambda^{(\hat{m}_{k}^{*})}}}-\sqrt{\frac{\log(2KM/\delta)}{\lambda^{(1 )}N_{k,t}^{(m)}}},\]

where inequality (a) is because that \(\hat{\mu}_{k}^{(\hat{m}_{k}^{*})}\leqslant\mu_{k}^{(\hat{m}_{k}^{*})}+\sqrt{ \frac{\log(2KM/\delta)}{N_{k,t}^{(m)}}}\) with a probability of at least \(1-\delta/2KM\) (therefore, with the union bound over all arm-fidelity pairs, the total failure probability of Explore is upper bounded by \(\delta/2\)), and inequality (b) is because \(\hat{\mu}_{k_{*}}^{(M)}-(\mu_{k}^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_{k}^{*})}) \geqslant\mu_{k_{*}}^{(M)}-(\mu_{k}^{(\hat{m}_{k}^{*})}+\zeta^{(\hat{m}_{k}^ {*})})=\Delta_{k}^{(\hat{m}_{k}^{*})}\).

To make the condition in Line 10 hold, with the above inequality, we need

\[\frac{\Delta_{k}^{(\hat{m}_{k}^{*})}}{\sqrt{\lambda^{(\hat{m}_{k}^{*})}}}-\sqrt {\frac{\log(2KM/\delta)}{\lambda^{(1)}N_{k,t}^{(m)}}}\geqslant 3\sqrt{\frac{\log(2KM/ \delta)}{\lambda^{(1)}N_{k,t}^{(m)}}},\]

which, after rearrangement, becomes

\[N_{k,t}^{(m)}>\frac{16\lambda^{(\hat{m}_{k}^{*})}}{(\Delta_{k}^{(\hat{m}_{k}^{*} )})^{2}}\frac{\log(2KM/\delta)}{\lambda^{(1)}}.\]It means that if the above inequality holds, than the condition in Line 10 must hold. That is, except for the committed fidelity \(\hat{m}_{k}^{*}\), we have

\[N_{k,t}^{(m)}\leqslant\frac{16\lambda^{(\hat{m}_{k}^{*})}}{(\Delta_{k}^{(\hat{m} _{k}^{*})})^{2}}\frac{\log(2KM/\delta)}{\lambda^{(1)}},\text{ for any other fidelities }m\neq\hat{m}_{k}^{*}.\]

For another thing, Eq.(14) of LUCB's proof guarantees that for the selected fidelity \(\hat{m}_{k}^{*}\), the number of pulling times is upper bounded as follows,

\[N_{k,t}^{(\hat{m}_{k}^{*})}\leqslant\frac{64}{(\Delta_{k}^{(\hat{m}_{k}^{*})}) ^{2}}\log\left(\frac{Lt}{\delta}\right).\]

Then, we upper bound the total budget of the algorithm as follows,

\[\Lambda =\sum_{k\in\mathcal{K}}\sum_{m\in\mathcal{M}}\lambda^{(m)}N_{k,t} ^{(m)}\] \[\leqslant\sum_{k\in\mathcal{K}}\frac{64\lambda^{(\hat{m}_{k}^{*} )}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}}\log\left(\frac{Lt}{\delta}\right)+ \sum_{k\in\mathcal{K}}\sum_{m\neq\hat{m}_{k}^{*}}\frac{16\lambda^{(m)}\lambda ^{(\hat{m}_{k}^{*})}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}\lambda^{(1)}}\log \left(\frac{2KM}{\delta}\right)\] \[\overset{(a)}{\leqslant}\sum_{k\in\mathcal{K}}\frac{256\lambda^{ (\hat{m}_{k}^{*})}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}}\log\left(\frac{Lt}{ \delta}\right)+\sum_{k\in\mathcal{K}}\sum_{m\neq\hat{m}_{k}^{*}}\frac{16 \lambda^{(m)}\lambda^{(\hat{m}_{k}^{*})}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2 }\lambda^{(1)}}\log\left(\frac{2KM}{\delta}\right)\] \[\leqslant\left(\sum_{k\in\mathcal{K}}\frac{256\lambda^{(\hat{m}_{ k}^{*})}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}}+\sum_{k\in\mathcal{K}}\sum_{m\neq \hat{m}_{k}^{*}}\frac{16\lambda^{(m)}\lambda^{(\hat{m}_{k}^{*})}}{(\Delta_{k} ^{(\hat{m}_{k}^{*})})^{2}\lambda^{(1)}}\right)\log\left(\frac{Lt}{\delta}\right)\] \[\leqslant\sum_{m\in\mathcal{M}}\frac{\lambda^{(m)}}{\lambda^{(1)}} \cdot\sum_{k\in\mathcal{K}}\frac{256\lambda^{(\hat{m}_{k}^{*})}}{(\Delta_{k} ^{(\hat{m}_{k}^{*})})^{2}}\log\left(\frac{Lt}{\delta}\right)\] \[\leqslant\sum_{m\in\mathcal{M}}\frac{\lambda^{(m)}}{\lambda^{(1)}} \cdot\sum_{k\in\mathcal{K}}\frac{256\lambda^{(\hat{m}_{k}^{*})}}{(\Delta_{k} ^{(\hat{m}_{k}^{*})})^{2}}\log\left(\frac{L\Lambda}{\delta\lambda^{(1)}}\right)\] \[\overset{(b)}{\leqslant}\sum_{m\in\mathcal{M}}\frac{\lambda^{(m) }}{\lambda^{(1)}}\cdot\sum_{k\in\mathcal{K}}\frac{1024\lambda^{(\hat{m}_{k}^{* })}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}}\log\left(\sum_{m\in\mathcal{M}} \frac{\lambda^{(m)}}{\lambda^{(1)}}\cdot\sum_{k\in\mathcal{K}}\frac{256\lambda ^{(\hat{m}_{k}^{*})}}{(\Delta_{k}^{(\hat{m}_{k}^{*})})^{2}}\frac{L}{\lambda^{( 1)}\delta}\right)\] \[\leqslant O\left(\sum_{m\in\mathcal{M}}\frac{\lambda^{(m)}}{ \lambda^{(1)}}\cdot\tilde{H}\log\left(\sum_{m\in\mathcal{M}}\frac{\lambda^{(m) }}{\lambda^{(1)}}\cdot\tilde{H}\cdot\frac{L}{\lambda^{(1)}\delta}\right)\right),\]

where inequality (a) is due to Eq.(19), inequality (b) is due to that \(\Lambda\leqslant A\log(B\Lambda)\Longrightarrow\Lambda\leqslant 4A\log(AB\Lambda)\).

#### e.2.3 Proof for Explore-C's Upper Bound in Theorem d.1

Step 4 for Explore-C: Prove that if the events of Steps 2 and 3 do not happen, for \(\Lambda>O\left(Q\log\left(\frac{KM/\sqrt{Q}}{\delta(\lambda^{(1)})^{2}}\right)\right)\), the algorithm terminates with a probability at least \(O(1-\delta/\Lambda^{2})\).Denote \(\bar{T}\coloneqq\lceil\frac{\Lambda}{2\lambda^{(\bar{N})}}\rceil\) and two events \(E_{1},E_{2}\) as follows,

\[E_{1} \coloneqq\{\exists t\geqslant\bar{T}:\texttt{CROS}_{t}\},\] \[E_{2} \coloneqq\{\exists t\geqslant\bar{T},k\in\mathcal{K}:(n_{k,t}^{(m_ {k}^{*})}>16n_{k,t}^{*})\cup(k\in\texttt{Mid}_{t})\}.\]We first upper bound the number of rounds after \(\bar{T}\) as follows,

\[\sum_{t\geqslant\bar{T}}\lambda^{(m_{t})}\mathbbm{1}\{\neg\texttt{ TERM}_{t}\} \stackrel{{(a)}}{{=}} \sum_{t\geqslant\bar{T}}\lambda^{(m_{t})}\mathbbm{1}\{\neg\texttt{ TERM}_{t}\cap\neg\texttt{CROS}_{t}\}\] (20) \[\stackrel{{(b)}}{{\leqslant}} \sum_{t\geqslant\bar{T}}\lambda^{(m_{t})}\mathbbm{1}\{(\ell_{t} \in\texttt{Mid}_{t})\cup(u_{t}\in\texttt{Mid}_{t})\}\] \[\leqslant\sum_{t\geqslant\bar{T}}\sum_{k\in\mathcal{K}}\lambda^{ (m_{t})}\mathbbm{1}\{((k=\ell_{t})\cup(k=u_{t}))\cap(k\in\texttt{Mid}_{t})\}\] \[\stackrel{{(c)}}{{\leqslant}} \sum_{t\geqslant\bar{T}}\sum_{k\in\mathcal{K}}\lambda^{(m_{t})} \mathbbm{1}\Big{\{}((k=\ell_{t})\cup(k=u_{t}))\cap(n_{k,t}^{(m_{t}^{*})}\leqslant 1 6n_{k,t}^{*})\Big{\}}\] \[=\sum_{k\in\mathcal{K}}\sum_{t\geqslant\bar{T}}\lambda^{(m_{t})} \mathbbm{1}\Big{\{}((k=\ell_{t})\cup(k=u_{t}))\cap(n_{k,t}^{(m_{t}^{*})} \leqslant 16n_{k,t}^{*})\Big{\}}\] \[\leqslant\sum_{k\in\mathcal{K}}\left(\sum_{\ell=1}^{m_{k}^{*}-1} \frac{\lambda^{(\ell)}\log(Lt^{4}/\delta)}{(\zeta^{(\ell)})^{2}}+16\lambda^{( m_{k}^{*})}n_{k,t}^{*}\right)\] \[\leqslant\sum_{k\in\mathcal{K}}\left(\sum_{\ell=1}^{m_{k}^{*}-1} \frac{\lambda^{(\ell)}}{(\zeta^{(\ell)})^{2}}+\frac{16\lambda^{(m_{k}^{*})}}{ \Delta_{k}^{2}}\right)\log\left(\frac{LT^{4}}{\delta}\right)\] \[\leqslant 4\sum_{k\in\mathcal{K}}\left(\sum_{\ell=1}^{m_{k}^{*}-1} \frac{\lambda^{(\ell)}}{(\zeta^{(\ell)})^{2}}+\frac{16\lambda^{(m_{k}^{*})}}{ \Delta_{k}^{2}}\right)\log\left(\frac{L\Lambda}{\lambda^{(1)}\delta}\right)\]

where the equation (a) is due to \(\neg E_{1}\), the inequality (b) is due to Step 1, and the inequality (c) is due to \(\neg E_{2}\).

Also notice that

\[\Lambda=\sum_{t<\bar{T}}\lambda^{(m_{t})}+\sum_{t\geqslant\bar{T}}\lambda^{( m_{t})}\mathbbm{1}\{\neg\texttt{TERM}_{t}\}\leqslant\frac{\Lambda}{2}+\sum_{t \geqslant\bar{T}}\lambda^{(m_{t})}\mathbbm{1}\{\neg\texttt{TERM}_{t}\},\]

and, combining with Eq.(20), we have,

\[\Lambda\leqslant 8\sum_{k\in\mathcal{K}}\left(\sum_{\ell=1}^{m_{k}^{*}-1} \frac{\lambda^{(\ell)}}{(\zeta^{(\ell)})^{2}}+\frac{16\lambda^{(m_{k}^{*})}}{ \Delta_{k}^{2}}\right)\log\left(\frac{L\Lambda}{\lambda^{(1)}\delta}\right).\]

Solving the above inequality concludes the cost complexity upper bound for Explore-C.

In the end of Step 4, we show that the LUCB algorithm fulfills the fixed confidence requirement. The probability that the algorithm does not terminate after spending \(\Lambda\) budget is upper bounded by \(\mathbb{P}(E_{1}\cup E_{2})\). Based on Steps 2 and 3, it can be upper bounded as follows,

\[\mathbb{P}(E_{1}\cup E_{2}) \leqslant\sum_{t>\bar{T}}\left(\frac{KM\delta}{Lt^{3}}+\frac{16 \delta\sum_{k\in\mathcal{K}}\Delta_{k}^{-2}}{Lt^{4}}\right)\leqslant\Lambda \left(\frac{1}{\lambda^{(1)}}-\frac{1}{2\lambda^{(M)}}\right)\left(\frac{\delta }{(\Lambda/\lambda^{(1)})^{3}}\right)\sum_{k\in\mathcal{K}}\Delta_{k}^{-2}\] \[\leqslant\frac{\delta}{\Lambda^{2}}\sum_{k\in\mathcal{K}}\Delta_ {k}^{-2}\left((\lambda^{(1)})^{2}-\frac{(\lambda^{(1)})^{3}}{2\lambda^{(M)}}\right)\] \[\leqslant\delta.\]Proofs for Regret Minimization Results

The two lower bound proofs utilize two different regret decomposition as follows,

\[R(\Lambda) \overset{(a)}{\geqslant}\sum_{t=1}^{N}\left(\frac{\lambda^{(m_{t})}- \lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\Delta_{I_{t}}^{(M)}\right)\] \[=\sum_{m=2}^{M}N_{\forall k}^{(m)}(\Lambda)\frac{\lambda^{(m)}- \lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\sum_{k\neq k_{1}^{(M)}}N_{k}^{(\forall m )}(\Lambda)\Delta_{k}^{(M)};\] (21) \[R(\Lambda) =\sum_{k\in\mathcal{K}}\sum_{m=1}^{M}N_{k}^{(m)}(\Lambda)\left( \frac{\lambda^{(m)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right),\] (22)

where the inequality (a) is due to \(\Lambda>\sum_{t=1}^{N}\lambda^{(m_{t})}\), the \(N_{k}^{(m)}(\Lambda)\) is the number of times that arm \(k\) is pulled in fidelity \(m\) after paying budget \(\Lambda\). The \(N\) with subscript \(\vee_{k}\) and superscript \({}^{(\forall m)}\) mean the pulling times of all arms and all fidelities respectively. Due to the multi-fidelity feedback, both decompositions are different from classic bandits' regret decomposition [24, Lemma 4.5], and, therefore, we need to non-trivially extend the known approaches to our scenario.

To prove the problem-independent lower bound \(\Omega(K^{1/3}\Lambda^{2/3})\) in Theorem 4.3, we utilize the decomposition in Eq.(21): In the RHS, the first term increases whether one choose fidelity other than the lowest; this is a novel term. The second term of RHS corresponds to the cost of pulling suboptimal arms which also appears in the classic MAB. With this observation, one can construct instance pairs such that it is unavoidable to do exploration at higher fidelities and, therefore, the first term is not negligible. Lastly, one needs to balance the magnitude of the above two terms case-by-case, which together bounds the regret as \(\Omega\) (\(\Lambda^{2/3}\)). To prove the problem-dependent lower bound in Theorem F.1, we utilize Eq.(22) to decompose the regret to each arm and bound each of them separately.

### Proof of Theorem 4.3

Step 1. Construct instances and upper bound KL-divergenceFix a policy \(\pi\). We construct two MF-MAB instances, each with \(K\) arms and \(M\) fidelities. For the pulling costs of different fidelities, we set \(\lambda^{(M)}\leqslant 2\lambda^{(1)}\). For reward feedback, we assume all arms' reward distributions at any fidelity are Bernoulli, and denote \(\mu_{k}^{(m)}(1),\mu_{k}^{(m)}(2)\) as the reward means of these two instances. Let \(\mathbb{P}_{1}=\mathbb{P}_{\boldsymbol{\mu}(1),\pi},\mathbb{E}_{1}=\mathbb{E} _{\boldsymbol{\mu}(1),\pi}\) and \(\mathbb{P}_{2}=\mathbb{P}_{\boldsymbol{\mu}(2),\pi},\mathbb{E}_{2}=\mathbb{E} _{\boldsymbol{\mu}(2),\pi}\) be the probability measures and expectations on the canonical MF-MAB model induced by \(\Lambda\)-budget interconnection of \(\pi\) and \(\boldsymbol{\mu}_{1}\) (and \(\boldsymbol{\mu}_{2}\)). Denote \(k^{\prime}=\arg\min_{k\in\mathcal{K}}\mathbb{E}_{1}[N_{k}^{(m>1)}(\Lambda)]\).

The only difference between both instance pair is in the arm \(k^{\prime}\)'s reward mean for fidelities \(m>1\), so that instance \(1\)'s optimal arm is arm \(1\) and instance \(2\)'s optimal arm is arm \(k^{\prime}\). The detailed reward means are listed as follows,

\[\begin{array}{c c c}\hline&m=1&m>1\\ \hline(\mu_{k}^{(m)}(1))_{k\in\mathcal{K}}&\left(\frac{1}{2}+\Delta,\frac{1}{ 2},\frac{1}{2},\ldots,\frac{1}{2}\right)&\left(\frac{1}{2}+\Delta,\frac{1}{2}, \frac{1}{2},\ldots,\frac{1}{2}\right)\\ (\mu_{k}^{(m)}(2))_{k\in\mathcal{K}}&\left(\frac{1}{2}+\Delta,\frac{1}{2}, \ldots,\frac{1}{2},\frac{1}{2}+2\Delta,\frac{1}{2},\ldots,\frac{1}{2}\right) \\ \hline\end{array}\]

Denote the entry \((M_{t},I_{t},X_{t})\) as a tuple of the pulled fidelity, pulled arm, and observed reward random variables at time \(t\), and \(\mathcal{H}\coloneqq(M_{1},I_{1},X_{1};M_{2},I_{2},X_{2};\ldots;M_{N},I_{N},X_{N})\) as a sequence of applying policy \(\pi\). We note that \(N\) is also a random variable depending on the sequence of fidelities in pulling arms. Next, we calculate the upper bound of the KL-divergence of the above two instances in this sequence.

\[\begin{split}\operatorname{KL}(\mathbb{P}_{1},\mathbb{P}_{2})& =\mathbb{E}_{1}\left[\log\left(\frac{d\mathbb{P}_{1}}{d\mathbb{P}_{2}}\right) \right]\\ &=\mathbb{E}_{1}\left[\log\left(\frac{d\mathbb{P}_{1}}{d\mathbb{P} _{2}}(M_{1},I_{1},X_{1};M_{2},I_{2},X_{2};\ldots;M_{N},I_{N},X_{N})\right) \right]\\ &=\mathbb{E}_{1}\left[\mathbb{E}_{1}\left[\log\left(\frac{d \mathbb{P}_{1}}{d\mathbb{P}_{2}}(M_{1},I_{1},X_{1};M_{2},I_{2},X_{2};\ldots;M_ {N},I_{N},X_{N})\right)\right|N\right]\right]\\ &\stackrel{{(a)}}{{=}}\mathbb{E}_{1}\left[\mathbb{E }_{1}\left[\sum_{t=1}^{N}\log\left(\frac{p_{1}(X_{t}|M_{t},I_{t})}{p_{2}(X_{t}|M _{t},I_{t})}\right)\right|N\right]\right]\\ &=\mathbb{E}_{1}\left[\sum_{t=1}^{N}\mathbb{E}_{1}\left[\log \left(\frac{p_{1}(X_{t}|M_{t},I_{t})}{p_{2}(X_{t}|M_{t},I_{t})}\right)\right| N\right]\right]\\ &\stackrel{{(b)}}{{=}}\mathbb{E}_{1}\left[\sum_{t=1 }^{N}\mathbb{E}_{1}\left[\operatorname{KL}\left(P_{1}^{(M_{t},I_{t})},P_{2}^{ (M_{t},I_{t})}\right)\right|N\right]\right]\\ &=\sum_{(m,k)\in\mathcal{M}\times\mathcal{K}}\mathbb{E}_{1}\left[ \sum_{t=1}^{N}\mathbb{E}_{1}\left[\left.\mathbbm{1}\{M_{t}=m,I_{t}=k\} \operatorname{KL}\left(P_{1}^{(M_{t},I_{t})},P_{2}^{(M_{t},I_{t})}\right) \right|N\right]\right]\\ &=\sum_{(m,k)\in\mathcal{M}\times\mathcal{K}}\mathbb{E}_{1}\left[ \operatorname{KL}\left(P_{1}^{(m,k)},P_{2}^{(m,k)}\right)\sum_{t=1}^{N} \mathbb{E}_{1}\left[\left.\mathbbm{1}\{M_{t}=m,I_{t}=k\}\right|N\right]\right] \\ &=\sum_{(m,k)\in\mathcal{M}\times\mathcal{K}}\operatorname{KL} \left(P_{1}^{(m,k)},P_{2}^{(m,k)}\right)\mathbb{E}_{1}\left[\left.\mathbb{E}_{ 1}\left[\sum_{t=1}^{N}\mathbbm{1}\{M_{t}=m,I_{t}=k\}\right|N\right]\right]\\ &\stackrel{{(c)}}{{=}}\sum_{(m,k)\in\mathcal{M} \times\mathcal{K}}\operatorname{KL}\left(P_{1}^{(m,k)},P_{2}^{(m,k)}\right) \mathbb{E}_{1}\left[T_{k}^{(m)}(\Lambda)\right]\\ &=\operatorname{KL}\left(P_{1}^{(m,2)},P_{2}^{(m,2)}\right) \mathbb{E}_{1}\left[T_{k^{\prime}}^{(m>1)}(\Lambda)\right]\\ &\stackrel{{(d)}}{{\leqslant}}\frac{1}{K}\cdot \mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\right]\cdot\operatorname{ KL}\left(\mathcal{B}\left(\frac{1}{2}\right),\mathcal{B}\left(\frac{1}{2}+2 \Delta\right)\right)\\ &\stackrel{{(e)}}{{\leqslant}}\mathbb{E}_{1}\left[T_ {\forall k}^{(m>1)}(\Lambda)\right]\cdot\frac{9\Delta^{2}}{K},\end{split}\] (23)

where the equation (a) is due to

\[\begin{split}&\frac{d\mathbb{P}_{1}}{d(\rho\times\rho\times \lambda)^{N}}(m_{1},k_{1},x_{1};m_{2},k_{2},x_{2};\ldots;m_{N},k_{N},x_{N})\\ &=p_{\bm{\mu}_{1},\pi}(m_{1},k_{1},x_{1};m_{2},k_{2},x_{2};\ldots; m_{N},k_{N},x_{N})\\ &=\prod_{t=1}^{N}\pi_{t}(m_{t},k_{t}|m_{1},k_{1},x_{1};\ldots;m_{ t-1},k_{t-1},x_{t-1})p_{\bm{\mu}_{1}}(x_{t}|m_{t},k_{t}),\end{split}\]

the equation (b) is due to

\[\begin{split}\mathbb{E}_{1}\left[\log\left(\frac{p_{1}(X_{t}|M_{t },I_{t})}{p_{2}(X_{t}|M_{t},I_{t})}\right)\right|N\right]&= \mathbb{E}_{1}\left[\left.\mathbb{E}_{1}\left[\log\left(\frac{p_{1}(X_{t}|M_{t },I_{t})}{p_{2}(X_{t}|M_{t},I_{t})}\right)\right|M_{t},I_{t}\right]\right|N \right]\\ &=\mathbb{E}_{1}\left[\left.\operatorname{KL}\left(P_{1}^{(M_{t},I _{t})},P_{2}^{(M_{t},I_{t})}\right)\right|N\right],\end{split}\]

the equation (c) is due to the tower property as well, the inequality (d) is because arm \(k^{\prime}\) is the arm with the smallest number of pulled with fidelity \(m>1\), and the inequality (e) is by calculating the KL-divergent between two Bernoulli distributions.

Step 2. Lower bound the regretWe first note that the regret defined in Eq.(9) for instance \(1\) can be decomposed as follows,

\[\mathbb{E}_{1}[R(\Lambda)] \stackrel{{(a)}}{{\geqslant}}\sum_{t=1}^{N}\left( \frac{\lambda^{(m_{t})}-\lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\Delta_{I_{t}}^{(M )}\right)\] \[=\sum_{m=2}^{M}T_{\forall k}^{(m)}(\Lambda)\frac{\lambda^{(m)}- \lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\sum_{k\neq k_{1}^{(M)}}T_{k}^{(\forall m )}(\Lambda)\Delta_{k}^{(M)}\]

where inequality (a) is due to \(\Lambda>\sum_{t=1}^{N}\lambda^{(m_{t})}\), the \(T_{k}^{(m)}(\Lambda)\) is the number of times that arm \(k\) is pulled in fidelity \(m\) (given total budget \(\Lambda\)).

Then, we lower bound the summation of the regrets under both instances,

\[\mathbb{E}_{1}[R(\Lambda)]+\mathbb{E}_{2}[R(\Lambda)]\] \[=\mathbb{E}_{1}\left[\sum_{m=2}^{M}T_{\forall k}^{(m)}(\Lambda) \frac{\lambda^{(m)}-\lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\sum_{k\neq k_{1}^{( M)}}T_{k}^{(\forall m)}(\Lambda)\Delta_{k}^{(M)}\right]\] \[\quad+\mathbb{E}_{2}\left[\sum_{m=2}^{M}T_{\forall k}^{(m)}( \Lambda)\frac{\lambda^{(m)}-\lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\sum_{k\neq k _{1}^{(M)}}T_{k}^{(\forall m)}(\Lambda)\Delta_{k}^{(M)}\right]\] \[\geqslant\mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\frac {\lambda^{(2)}-\lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\sum_{k\neq k_{1}^{(M)}}T_ {k}^{(\forall m)}(\Lambda)\Delta_{k}^{(M)}\right]\] \[\quad+\mathbb{E}_{2}\left[T_{\forall k}^{(m>1)}(\Lambda)\frac{ \lambda^{(2)}-\lambda^{(1)}}{\lambda^{(1)}}\mu_{1}+\sum_{k\neq k_{1}^{(M)}}T_ {k}^{(\forall m)}(\Lambda)\Delta_{k}^{(M)}\right]\] \[\stackrel{{(a)}}{{\geqslant}}\mathbb{E}_{1}\left[T_ {\forall k}^{(m>1)}(\Lambda)\right]\frac{\lambda^{(2)}-\lambda^{(1)}}{\lambda ^{(1)}}\mu_{1}\] \[\quad+\frac{\Lambda\Delta}{2}\min\left\{\frac{1}{\lambda^{(M)}}- \frac{1}{2\lambda^{(1)}},\frac{1}{2\lambda^{(1)}}\right\}\exp\left(-2\Delta^{ 2}K^{-1}\mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\right]\right),\]

where inequality (a) uses the \(\lambda^{(M)}\leqslant 2\lambda^{(1)}\) condition, inequality (b) is by Bretagnolle-Huber inequality [24, Theorem 14.2], and inequality (c) is by Eq.(23).

Step 3. Obtain the \(\Omega(K^{\frac{1}{3}}\Lambda^{\frac{2}{3}})\) lower boundWe show that \(\mathbb{E}_{1}[R(\Lambda)]+\mathbb{E}_{2}[R(\Lambda)]\geqslant\Omega(K^{\frac{ 1}{3}}\Lambda^{\frac{2}{3}})\) for any possible quantity of \(\mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\right]\) via categorized discussion as follows,

* Case 1: If \(\mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\right]=0\), then the last formula becomes \(C\Lambda\Delta\). Letting \(\Delta\) be a constant (via sup), we have a \(\Omega(\Lambda)\) lower bound, which means \(\Omega(K^{\frac{1}{3}}\Lambda^{\frac{2}{3}})\) is also valid.
* Case 2: If \(\mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\right]\geqslant K^{\frac{ 1}{3}}\Lambda^{\frac{2}{3}}\), then we have the \(\Omega(K^{\frac{1}{3}}\Lambda^{\frac{2}{3}})\) lower bound from the first term.
* Case 3: If \(0<\mathbb{E}_{1}\left[T_{\forall k}^{(m>1)}(\Lambda)\right]<K^{\frac{1}{3}} \Lambda^{\frac{2}{3}}\), we choose \(\Delta=K^{\frac{1}{3}}\Lambda^{-\frac{1}{3}}\) and also obtain the \(\Omega(K^{\frac{1}{3}}\Lambda^{\frac{2}{3}})\) lower bound.

### Proof of Theorem 4.4

In this proof, we prove that for any arm \(k\in\mathcal{K}\), the total regret due to this arm \(k\) is lower bounded as follows,

\[\liminf_{\Lambda\to\infty}\frac{\mathbb{E}\left[R_{k}(\Lambda)\right]}{\log( \Lambda)}\geqslant\min_{m:\Delta_{k}^{(m)}>0}\left(\frac{\lambda^{(m)}}{ \lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right)\frac{C}{(\Delta_{k}^{(m)})^{ 2}}.\]

Then, noticing that \(R(\Lambda)=\sum_{k\in\mathcal{M}}R_{k}(\Lambda)\) concludes the proof.

We construct instances \(\nu\) and \(\nu^{\prime}\). We set the reward distributions \(\nu=(\nu_{k}^{(m)})_{(k,m)\in\mathcal{K}\times\mathcal{M}}\) as Bernoulli and the reward means fulfill \(\mu_{1}^{(M)}>\mu_{2}^{(M)}\geqslant\mu_{3}^{(M)}\geqslant\ldots\geqslant \mu_{K}^{(M)},\) where \(\mu_{k}^{(m)}=\mathbb{E}_{X\sim\nu_{k}^{(m)}}[X]\). We let \(\nu^{\prime}_{\ k}\) be the same to \(\nu_{k}^{(m)}\) for all \(k\) and \(m\), except for that an arm \(\ell\neq 1\). We set arm \(\ell\)'s reward means on fidelities \(m\in\mathcal{M}_{k}\) to be \(\nu^{\prime}_{\ \ell}(m)=\nu_{1}^{(M)}-\zeta^{(m)}+\epsilon\). One can verify that the reward means of arm \(k\) under instance \(v^{\prime}\) fulfill the condition that \(|\mu^{\prime}_{\ k}-\mu^{\prime}_{\ k}|\leqslant\zeta^{(m)}\) for any fidelity \(m\). Notice that \(\mu^{\prime}_{\ k}>\mu^{(M)}_{k_{*}}\). Hence, under instance \(\mathcal{I}^{\prime}\), the optimal arm is \(k\). We denote \(\mathbb{P},\mathbb{E}\) and \(\mathbb{P}^{\prime},\mathbb{E}^{\prime}\) as the probability measures and expectations for instances \(\mathcal{I}\) and \(\mathcal{I}^{\prime}\) respectively.

Next, we employ the (extended) key inequality from Garivier et al. [12] as follows,

\[\sum_{k\in\mathcal{K}}\sum_{m\in\mathcal{M}}\mathbb{E}[N_{k}^{(m)}(\Lambda)] \operatorname{KL}(v_{k}^{(m)},v^{\prime}_{\ k})\geqslant\operatorname{kl}( \mathbb{E}[Z],\mathbb{E}^{\prime}[Z]).\]

Let \(Z=\lambda^{(M)}N_{\ell}^{(\forall m)}(\Lambda)/\Lambda\) in the above inequality, we have

\[\sum_{m:\Delta_{\ell}^{(m)}>0}\mathbb{E}[N_{k}^{(m)}(\Lambda)] \operatorname{KL}(v_{\ell}^{(m)},v^{\prime}_{\ \ell})\] (24) \[\geqslant\operatorname{kl}\left(\frac{\mathbb{E}[N_{\ell}^{( \forall m)}(\Lambda)]}{\Lambda},\frac{\mathbb{E}^{\prime}[N_{\ell}^{(\forall m )}(\Lambda)]}{\Lambda}\right)\] \[\overset{(a)}{\geqslant}\left(1-\frac{\lambda^{(M)}\mathbb{E}[N _{\ell}^{(\forall m)}(\Lambda)]}{\Lambda}\right)\log\frac{\Lambda}{\Lambda- \lambda^{(M)}\mathbb{E}^{\prime}[N_{\ell}^{(\forall m)}(\Lambda)]}-\log 2\]

where inequality (a) is due to that for all \((p,q)\in[0,1]^{2}\),\(\operatorname{kl}(p,q)\geqslant(1-p)\log(1/(1-q))-\log 2\).

Notice that the regret attributed to any arm \(k\) can be decomposed and lower bounded as follows,

\[R_{\ell}(\Lambda)\geqslant\sum_{m=1}^{M}N_{\ell}^{(m)}(\Lambda)\left(\frac{ \lambda^{(m)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{\ell}^{(M)}\right),\]

with the constraint in Eq.(24). Since this is a linear programming, we know its solution is reached at its vertex. Therefore, we lower bound the regret as follows,

\[R_{\ell}(\Lambda)\geqslant \min_{m:\Delta_{k}^{(m)}>0}\left(\frac{\lambda^{(m)}}{\lambda^{(1) }}\mu_{1}^{(M)}-\mu_{\ell}^{(M)}\right)\frac{1}{\operatorname{KL}(v_{\ell}^{(m )},v^{\prime}_{\ell})}\] (25) \[\times\left(\left(1-\frac{\lambda^{(M)}\mathbb{E}[N_{\ell}^{( \forall m)}(\Lambda)]}{\Lambda}\right)\log\frac{\Lambda}{\Lambda-\lambda^{(M)} \mathbb{E}^{\prime}[N_{\ell}^{(\forall m)}(\Lambda)]}-\log 2\right),\]

Notice that the policy is consistent, that is, \(\mathbb{E}[N_{\ell}^{(\forall m)}(\Lambda)]=o(T^{a})\) and \(\mathbb{E}^{\prime}[N_{k}^{(\forall m)}(\Lambda)]=o(\Lambda^{a})\) for any \(a\in(0,1]\) and any suboptimal arm \(k\neq\ell\). We have \(\frac{\lambda^{(M)}\mathbb{E}[N_{\ell}^{(\forall m)}(\Lambda)]}{\Lambda}=o(1)\) and

\[\Lambda-\lambda^{(M)}\mathbb{E}^{\prime}[N_{\ell}^{(\forall m)}(\Lambda)] \leqslant\lambda^{(M)}\sum_{k\neq\ell}\mathbb{E}^{\prime}[N_{k}^{(\forall m )}(\Lambda)]=o(\Lambda^{a}).\]Dividing both sides of Eq.(25) by \(\Lambda\), and letting \(\Lambda\) go to infinity and \(a\) go to \(1\), we have

\[\liminf_{\Lambda\to\infty}\frac{\mathbb{E}[R_{\ell}(\Lambda)]}{\Lambda} \geqslant\min_{m:\Delta_{\epsilon}^{(m)}>0}\left(\frac{\lambda^{( m)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{\ell}^{(M)}\right)\frac{1}{\operatorname{KL}(v_{ \ell}^{(m)},v_{\ell}^{(m)})}\] \[\geqslant\min_{m:\Delta_{\epsilon}^{(m)}>0}\left(\frac{\lambda^{ (m)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{\ell}^{(M)}\right)\frac{1}{\operatorname {KL}(v_{\ell}^{(m)},v_{1}^{(M)}-\zeta^{(m)}+\epsilon)}\]

To bound the optimal arm \(1\)'s sample cost, we use the same \(\nu\) as above and construct another instance \(\nu^{\prime\prime}\). The instance \(\nu^{\prime\prime}\)'s reward means are the same to \(\nu\) except for arm \(1\) whose reward means for fidelity \(m\in\mathcal{M}_{1}\) are set as \({\mu^{\prime\prime}}_{1}^{(m)}=\mu_{2}^{(m)}+\zeta^{(m)}-\epsilon\). Then, with similar procedure as the above, we obtain

\[\liminf_{\Lambda\to\infty}\frac{\mathbb{E}[R_{1}(\Lambda)]}{\Lambda} \geqslant\min_{m:\Delta_{k}^{(m)}>0}\left(\frac{\lambda^{(m)}}{\lambda^{(1)}} \mu_{1}^{(M)}-\mu_{1}^{(M)}\right)\frac{1}{\operatorname{KL}(v_{1}^{(m)},v_{2 }^{(m)}+\zeta^{(m)}-\epsilon)}\]

### Proof of Theorem 4.8

We first prove a problem dependent regret upper bound as follows and then convert this bound to the problem independent regret upper bound presented in Theorem 4.8.

Denote \(\Delta_{k}^{(M)}=\mu_{1}^{(M)}-\mu_{k}^{(M)}\), and, especially, \(\Delta_{1}^{(M)}=0\).

**Theorem F.1** (Problem-Dependent Regret Upper Bound).: _For any \(\varepsilon>0\). Algorithm 3's regret is upper bounded as follows,_

\[\mathbb{E}[R(\Lambda)]\leqslant\sum_{k:\Delta_{k}^{(M)}> \varepsilon}\left(\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_ {k}^{(M)}\right)\left(\frac{16}{(\Delta_{k}^{(M)})^{2}}\log\frac{\Lambda( \Delta_{k}^{(M)})^{2}}{16\lambda^{(M)}}+\frac{48}{(\Delta_{k}^{(M)})^{2}}+1 \right)+\frac{64}{\Delta_{k}^{(M)}}\right)\] \[+\sum_{k:\Delta_{k}^{(M)}\leqslant\varepsilon}\left(\left(\frac{ \lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right)\left(\frac{16 }{\varepsilon^{2}}\log\left(\frac{\Lambda\varepsilon^{2}}{16\lambda^{(M)}} \right)+\frac{32}{3\varepsilon^{2}}+1\right)+\frac{64}{\varepsilon}\right)+ \max_{k:\Delta_{k}^{(M)}\leqslant\varepsilon}\frac{\Lambda}{\lambda^{(1)}} \Delta_{k}^{(M)}.\] (26)

Proof of Theorem F.1.: By Hoeffding's inequality, we have

\[\mathbb{P}\left(\hat{\mu}_{k,p}^{(M)}>\mu_{k}^{(M)}+2^{-p}\right) \leqslant\exp\left(-\frac{2^{-2p}}{2\times\frac{1}{2\times 2^{2p} \log(\Lambda/2^{2p}\lambda^{(M)})}}\right)=\frac{2^{2p}\lambda^{(M)}}{\Lambda}\] \[\mathbb{P}\left(\hat{\mu}_{k,p}^{(M)}<\mu_{k}^{(M)}-2^{-p}\right) \leqslant\exp\left(-\frac{2^{-2p}}{2\times\frac{1}{2\times 2^{2p} \log(\Lambda/2^{2p}\lambda^{(M)})}}\right)=\frac{2^{2p}\lambda^{(M)}}{\Lambda}.\]

That is, the empirical mean \(\hat{\mu}_{k}^{(M)}\) is within the confidence interval \((\mu_{k}^{(M)}-2^{-p},\mu_{k}^{(M)}+2^{-p})\) with high probability.

Choose any \(\varepsilon>\frac{\varepsilon}{\Lambda}\). Let \(\mathcal{K}^{\prime}=\{k\in\mathcal{K}|\Delta_{k}^{(M)}>\varepsilon\}\). Denote \(p_{k}\coloneqq\min\{p:2^{-p}<\frac{\Delta_{k}^{(M)}}{2}\}\). From \(p_{k}\)'s definition, we have the following inequality

\[2^{p_{k}}<\frac{4}{\Delta_{k}^{(M)}}<2^{p_{k}+1}.\] (27)

We also note that the cost of pulling a suboptimal arm \(k\in\mathcal{K}^{\prime}\) at highest fidelity \(M\) is upper bounded as \(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\), where the factor \(\frac{\lambda^{(M)}}{\lambda^{(1)}}\) is because the budget paying to pull an arm at fidelity \(M\) can be used to the the arm at fidelity \(1\) for fractional times.

The rest of this proof consists of two steps. In the first step, we assume that all empirical means are in their corresponding confidence intervals at each phases, and show the algorithm can _properly_ eliminate all suboptimal arms in \(\mathcal{K}^{\prime}\)--the arm is eliminated in or before the phase \(p_{k}\). In the second step, we upper bound the regret if there are any empirical estimates lying outside their corresponding confidence intervals.

**Step 1.** If all arms' empirical means lie in confidence intervals. That is, any suboptimal arm \(k\) is eliminated in or before the phase \(p_{k}\). Because, if \(k\in\mathcal{C}_{p_{k}}\), we have

\[\hat{\mu}_{k}^{(M)} \leqslant\mu_{k}^{(M)}+2^{-p_{k}}=\mu_{1}^{(M)}-\Delta_{k}^{(M)}+ 2^{-p_{k}}\] \[\overset{(a)}{\leqslant}\mu_{1}^{(M)}-4\times 2^{-p_{k}-1}+2^{-p_{k} }=\mu_{1}^{(M)}-2^{-p_{k}}<\hat{\mu}_{1}^{(M)}\leqslant\max_{k\in\mathcal{C}_{ p_{k}}}\hat{\mu}_{k}^{(M)},\]

where (a) is due to Eq.(27). That is, if this arm \(k\) haven't been eliminated before phase \(p_{k}\), it must be eliminated in this phase. Therefore, the total pulling times of this arm \(k\) at highest fidelity \(M\) is upper bounded as follows,

\[T_{k}^{(M)}\leqslant\left\lceil 2^{2p_{k}}\log\frac{\Lambda}{2^{2p_{k}} \lambda^{(M)}}\right\rceil\overset{(a)}{\leqslant}\frac{16}{(\Delta_{k}^{(M) })^{2}}\log\left(\frac{(\Delta_{k}^{(M)})^{2}\Lambda}{16\lambda^{(M)}}\right)+1,\] (28)

where (a) is due to Eq.(27).

We then handle the number of times of pulling arms \(k\) with \(\Delta_{k}^{(M)}\leqslant\varepsilon\) in fidelity \(M\). Although these arms' total pulling times, eliminated in or before phase \(p_{k},\) are also upper bounded by Eq.(28), their corresponding phases \(p_{k}\) is greater than \(\log_{2}\frac{2}{\varepsilon}\) and, therefore, cannot be reached. So, these arms' (including the optimal arm \(1\)'s) total pulling times in fidelity \(M\) is upper bounded by

\[\frac{16}{\varepsilon^{2}}\log\left(\frac{\Lambda\varepsilon^{2}}{16\lambda^ {(M)}}\right)+1.\]

Therefore, the cost due to pulling arms at fidelity \(M\) is upper bounded as follows,

\[\sum_{k\in\mathcal{K}}\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)} -\mu_{k}^{(M)}\right)\left(\frac{16}{\left(\max\left\{\varepsilon,\Delta_{k}^ {(M)}\right\}\right)^{2}\log\left(\frac{\left(\max\left\{\varepsilon,\Delta_{ k}^{(M)}\right\}\right)^{2}\Lambda}{16\lambda^{(M)}}\right)+1}\right).\] (29)

After the elimination process, arms with \(\Delta_{k}^{(M)}\leqslant\varepsilon\) may remain in the candidate arm set \(\mathcal{C}_{p}\) and are exploited in turn at fidelity \(m=1\). As some of them are not the optimal arm, this additional cost can be upper bounded as follows,

\[\max_{k:\Delta_{k}^{(M)}\leqslant\varepsilon}\frac{\Lambda}{\lambda^{(1)}} \Delta_{k}^{(M)}.\] (30)

**Step 2.** There are two cases that some arms are eliminated improperly: for an suboptimal arm \(k\), either

1. The suboptimal arm \(k\) is _not_ eliminated in (or before) the phase \(p_{k}\), and the optimal arm \(1\) is in \(\mathcal{C}_{p_{k}}\) in phase \(p_{k}\); or
2. The suboptimal arm \(k\) is eliminated in (or before) the phase \(p_{k}\), and the optimal arm \(1\) is _not_ in \(\mathcal{C}_{p_{k}}\) in phase \(p_{k}\).

Case 2.1's happening means that arm \(k\) is not eliminated in or before phase \(p_{k}\), which can only happen when the arm's empirical mean \(\hat{\mu}_{k}^{(M)}\) lies outside its corresponding confidence interval. This event in or before phase \(p_{k}\) is with a probability no greater than \(2\times\frac{2^{2p_{k}}\lambda^{(M)}}{\Lambda}\). Since this event may happen to any suboptimal arm \(k\in\mathcal{K}^{\prime}\), then the regret of this case is upper bounded by

\[\begin{split}\sum_{k\in\mathcal{K}^{\prime}}\frac{2^{2p_{k}+1} \lambda^{(M)}}{\Lambda}\frac{\Lambda}{\lambda^{(M)}}\left(\frac{\lambda^{(M)}} {\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right)&=\sum_{k\in \mathcal{K}^{\prime}}2^{2p_{k}+1}\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_ {1}^{(M)}-\mu_{k}^{(M)}\right)\\ &\overset{(a)}{\leqslant}\sum_{k\in\mathcal{K}^{\prime}}\frac{32} {(\Delta_{k}^{(M)})^{2}}\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M) }-\mu_{k}^{(M)}\right),\end{split}\] (31)

where (a) is due to Eq.(27).

If Case 2.2 happens, the optimal arm \(1\) is not in the candidate arm set \(\mathcal{C}_{p_{k}}\) in phase \(p_{k}\). We denote \(p_{1}\) as the phase that the optimal arm \(1\) is eliminated, and it is at this phase that some arms' empirical means lie outside their confidence interval so that this mis-elimination happens. The probability of this event is upper bounded by \(2\times\frac{2^{2p_{1}}\lambda^{(M)}}{\Lambda}\). We assume that arms \(k\) with \(p_{k}<p_{1}\) are eliminated in or before phase \(p_{i}\) properly; otherwise, the regret is counted in Case 2.1. Therefore, the optimal arm \(1\) eliminated in phase \(p_{1}\) should be eliminated by an arm \(k\) with \(p_{k}\geqslant p_{1}\). Consequently, the maximal per time slot regret in Case 2.2 is among arms with \(p_{k}\geqslant p_{1}\). Denote \(p_{\varepsilon}\coloneqq\min\{p|2^{-p}<\frac{\varepsilon}{2}\}\). We bound the cost of Case 2.2 as follows,

\[\sum_{p_{1}=0}^{\max_{i_{\varepsilon}\in\mathcal{K}^{p_{1}}}p_{i}} \sum_{k>1:p_{k}\geqslant p_{1}}\frac{2^{2p_{1}+1}\lambda^{(M)}}{\Lambda}\frac {\Lambda}{\lambda^{(M)}}\cdot\max_{k^{\prime}>1:p_{k^{\prime}}\geqslant p_{1 }}\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k^{\prime}}^{(M) }\right)\] \[=\sum_{p_{1}=0}^{\max_{i_{\varepsilon}\in\mathcal{K}^{p_{1}}}p_{i }}\sum_{k>1:p_{k}\geqslant p_{1}}2^{2p_{1}+1}\cdot\left(\frac{\lambda^{(M)}}{ \lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{(M)}+\max_{k^{\prime}>1:p_{k^{\prime}} \geqslant p_{1}}\Delta_{k^{\prime}}^{(M)}\right)\] \[\stackrel{{(a)}}{{\leqslant}}\sum_{p_{1}=0}^{\max_{i _{\varepsilon}\in\mathcal{K}^{p_{1}}}p_{i}}\sum_{k>1:p_{k}\geqslant p_{1}}2^{2 p_{1}+1}\cdot\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{(M) }+4\times 2^{-p_{1}}\right)\] \[\stackrel{{(b)}}{{\leqslant}}\sum_{k>1}\sum_{p_{1}=0 }^{\min\{p_{k},p_{\varepsilon}\}}2^{2p_{1}+1}\cdot\left(\frac{\lambda^{(M)}}{ \lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{(M)}+4\times 2^{-p_{1}}\right)\] \[\leqslant\sum_{k>1}\left(\left(\frac{\lambda^{(M)}}{\lambda^{(1)} }\mu_{1}^{(M)}-\mu_{1}^{(M)}\right)\sum_{p_{1}=0}^{\min\{p_{k},p_{\varepsilon }\}}2^{2p_{1}+1}+\sum_{p_{1}=0}^{\min\{p_{k},p_{\varepsilon}\}}2^{p_{1}+3}\right)\] \[\leqslant\sum_{k>1}\left(\left(\frac{\lambda^{(M)}}{\lambda^{(1)} }\mu_{1}^{(M)}-\mu_{1}^{(M)}\right)\frac{2^{2\min\{p_{k},p_{\varepsilon}\}+1}} {3}+2^{\min\{p_{k},p_{\varepsilon}\}+4}\right)\] \[\leqslant\sum_{k\in\mathcal{K}^{\prime}}\left(\left(\frac{\lambda ^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{(M)}\right)\frac{2^{2p_{k}+1}}{3} +2^{p_{k}+4}\right)+\sum_{k>1,k\notin\mathcal{K}^{\prime}}\left(\left(\frac{ \lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{(M)}\right)\frac{2^{2p_{ \varepsilon}+1}}{3}+2^{p_{\varepsilon}+4}\right)\] \[\stackrel{{(c)}}{{\leqslant}}\sum_{k\in\mathcal{K}^{ \prime}}\left(\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{ (M)}\right)\frac{32}{3(\Delta_{k}^{(M)})^{2}}+\frac{64}{\Delta_{k}^{(M)}} \right)+\sum_{k>1,k\notin\mathcal{K}^{\prime}}\left(\left(\frac{\lambda^{(M)}}{ \lambda^{(1)}}\mu_{1}^{(M)}-\mu_{1}^{(M)}\right)\frac{32}{3\varepsilon^{2}}+ \frac{64}{\varepsilon}\right)\] \[\leqslant\sum_{k\in\mathcal{K}^{\prime}}\left(\left(\frac{ \lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right)\frac{32}{3( \Delta_{k}^{(M)})^{2}}+\frac{64}{\Delta_{k}^{(M)}}\right)+\sum_{k>1,k\notin \mathcal{K}^{\prime}}\left(\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{( M)}-\mu_{k}^{(M)}\right)\frac{32}{3\varepsilon^{2}}+\frac{64}{\varepsilon}\right)\] (32)

where (a) and (c) are due to Eq.(27), and (b) is due to the property of swapping two summations.

Summing up the costs in Eq.(29), Eq.(30), Eq.(31), and Eq.(32) concludes the proof. 

Next, we derive the problem-independent regret bound from Eq.(26). When \(\Lambda\) is large, the \(O(\log\Lambda)\) and \(O(\Lambda)\) terms dominate other terms in Eq.(26). For any given \(\varepsilon\), if \(\Lambda\) is large enough, we always have \(\varepsilon>4\sqrt{\frac{\lambda^{(M)}\varepsilon}{\Lambda}}\), which, with some calculus, guarantees that \(\frac{16}{(\Delta_{k}^{(M)})^{2}}\log\frac{\Lambda(\Delta_{k}^{(M)})^{2}}{16 \Lambda^{(M)}}<\frac{16}{\varepsilon^{2}}\log\frac{\Lambda\varepsilon^{2}}{16 \Lambda^{(M)}}\) for all \(\Delta_{k}^{(M)}>\varepsilon\). Therefore, we can scale all logarithmic arm pulling times as \(\frac{16}{\varepsilon^{2}}\log\frac{\Lambda\varepsilon^{2}}{16\Lambda^{(M)}}\), and upper bound the pulling cost \(\left(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}-\mu_{k}^{(M)}\right)\) by \(\frac{\lambda^{(M)}}{\lambda^{(1)}}\mu_{1}^{(M)}\). We derive the problem-independent regret upper bound as follows,

\[\mathbb{E}\left[R(\Lambda)\right] \leqslant\frac{K\mu_{1}^{(M)}\cdot\lambda^{(M)}}{\lambda^{(1)}} \frac{16}{\varepsilon^{2}}\log\frac{\Lambda\varepsilon^{2}}{16\lambda^{(M)}}+ \frac{\Lambda}{\lambda^{(1)}}\varepsilon\] \[\leqslant\frac{K\mu_{1}^{(M)}\cdot\lambda^{(M)}}{\lambda^{(1)}} \frac{16}{\varepsilon^{2}}\log\frac{\Lambda}{16\lambda^{(M)}}+\frac{\Lambda}{ \lambdawhere the equation of (a) holds when \(\varepsilon=\left(\frac{K_{\mu}\mu_{1}^{(M)}\log(\Lambda/16\Lambda^{(M)})}{( \Lambda/16\Lambda^{(M)})}\right)^{\frac{1}{3}}\).