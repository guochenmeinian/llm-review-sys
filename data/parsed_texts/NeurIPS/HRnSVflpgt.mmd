Schur Nets: effectively exploiting local structure for equivariance in higher order graph neural networks

Qingqi Zhang\({}^{1*}\), Ruize Xu\({}^{2*}\) and Risi Kondor\({}^{1,2,3}\)

\({}^{1}\)Computational and Applied Mathematics

Departments of \({}^{2}\)Computer Science and \({}^{3}\)Statistics

University of Chicago

{qingqi,richardixur,risi}@uchicago.edu

###### Abstract

Recent works have shown that extending the message passing paradigm to subgraphs communicating with other subgraphs, especially via higher order messages, can boost the expressivity of graph neural networks. In such architectures, to faithfully account for local structure such as cycles, the local operations must be equivariant to the automorphism group of the local environment. However, enumerating the automorphism groups of all subgraphs of interest and finding appropriate equivariant operations for each one of them separately is generally not feasible. In this paper we propose a solution to this problem based on spectral graph theory that bypasses having to determine the automorphism group entirely and constructs a basis for equivariant operations directly from the graph Laplacian. We show that this approach can boost the performance of GNNs on some standard benchmarks.

+
Footnote †: * denotes equal contribution.

## 1 Introduction

Message pasing neural networks (MPNNs) are the most popular paradigm for building neural networks on graphs [18]. While MPNNs have proved to be remarkably effective in a range of domains from program analysis [1] to drug discovery [20], a series of both empirical [3] and theoretical [40, 10] results have shown that the fact that MPNNs are based on just vertices sending messages to their neighbors limits their expressive power. This problem is especially acute in domains such as chemistry, where the presence or absence of specific small structural units (functional groups) directly influences the behavior and properties of molecules.

Recent papers proposed to address this issue by extending the message paradigm to also allow for message passing between vertices and edges [24] or between subgraphs [2, 15, 4, 46]. However, if the actual messages remain scalars, the added expressive power of these networks is relatively limited.

A newer development is the appearance of _higher order MPNN_s, where the messages are not just scalars, but more complex objects indexed by the vertices of the sending and receiving subgraphs [31, 34, 14]. For example, in an organic molecule, the internal state of a benzene ring (six carbon atoms arranged in a cycle) can be represented as a matrix \(T\in\mathbb{R}^{6\times c}\), where the rows correspond to the individual carbons. When this benzene ring passes a message to a neighboring benzene ring that it shares an edge with, information relating to the two shared carbons can be sent directly to the corresponding atoms in the second ring. Information relating to the other vertices has to be treated differently. Hypergraph neural networks [15, 45, 41] and neural networks on more abstract mathematical structures such as simplicial complexes [7, 6] are closely related, since these also involve higher order generalizations of message passing between combinatorial objects interlocked in potentially complicated ways.

In prior work we introduced a formalism called \(P\)-tensors that provides a flexible framework for implementing and reasoning about higher order MPNNs [21]. \(P\)-tensors build closely on the by now substantial literature on permutation equivariance in neural networks from Deep Sets [43], to various works studying higher order permutation equivariant maps [32]. It is also related to the category theoretic approach employed in Natural Graph Neural Networks [12].

One aspect of higher order message passing that has hithero not been studied in detail is the interaction between the local graph structure and the equivariance constraint. Intuitively, the reason that GNNs must be equivariant to permutations is that when the same, or analogous, graphs are presented to the network with the only difference being that the vertices have been renumbered, the final output must remain the same. However, Thiede et al. [37] observed that enforcing full permutation equivariance, especially at the local level, can be too restrictive. When considering specific, structurally important subgraphs such as paths or cycles, the operations local to such a subgraph \(S\) should really only be equivariant to the automorphism group of \(S\), rather than all \(|S|!\) permutations. The Autobahn architecture described in [37] explicitly accounts for the automorphism group of two specific types of subgraphs, cycles and path. Defining bespoke convolution operations on these two types of subgraphs was shown to improve performance on molecular datasets like ZINC [26].

The drawback of the Autobahn approach is that it requires explicitly identifying the automorphism group of each type of subgraph, and crafting specialized equivariant operations based on its representation theory. For more flexible architectures leveraging not just cycles and paths but many other types of subgraphs this quickly becomes infeasible.

**Main contributions.** The main contribution of this paper is a simple algorithm based on spectral graph theory for constructing a basis of automorphism equivariant operations on any possible subgraph _without_ explicitly having to determine its automorphism group, let alone derive its irreducible representations. The algorithm is based on imitating the structure of the group theoretical approach to equivariance, which we summarize in Section 3, but bypassing having to use actual group theory. Section 4 describes the algorithm itself and the resulting neural network operations, which we call _Schur layers_. In Section 5 we show that the introduction of Schur layers does indeed improve the performance of higher order graph neural networks on standard molecular benchmarks.

## 2 Background: equivariance with side information

Let \(\mathcal{G}\) be an undirected graph with vertex set \(V=\{1,2,\ldots,n\}\) and edge set \(E\subseteq V\times V\) represented by the adjacency matrix \(A\in\mathbb{R}^{n\times n}\). Graph neural networks (GNNs) address one of two, closely related tasks: (a) given a function \(f^{\text{in}}\colon V\to\mathbb{R}\) on the vertices of \(\mathcal{G}\), learn to transform it to another function \(f^{\text{out}}\colon V\to\mathbb{R}\); (b) given not just one graph, but an entire collection of graphs, learn to map each graph \(\mathcal{G}\), or rather its adjacency matrix, to a scalar or vector \(\psi(A)\) that characterizes \(\mathcal{G}\)'s structure.

In both cases, the critical constraint is that the network must behave appropriately under permuting the order in which the vertices are listed. The group of all permutations of \(\{1,2,\ldots,n\}\) is called the _symmetric group_ of degree \(n\) and denoted \(\mathbb{S}_{n}\). Applying a permutation \(\sigma\in\mathbb{S}_{n}\) to the vertices changes the adjacency matrix to \(A^{\prime}=\sigma(A)\) with

\[A^{\prime}_{i,j}=A_{\sigma^{-1}(i),\sigma^{-1}(j)}.\] (1)

The basic constraint on algorithms that operate on functions on graphs is that if the input is transformed along with the numbering of the vertices, \(f^{\text{in}}_{i}\!\!=\!f^{\text{in}}_{\sigma^{-1}(i)}\), then the output must transform the same way, \(f^{\text{out}}_{i}\!\!=\!f^{\text{out}}_{\sigma^{-1}(i)}\). This property is called _equivalence equivariance_. Formally, denoting the mapping from inputs to outputs \(\phi\colon f^{\text{in}}\mapsto f^{\text{out}}\), equivariance states that the action \(\sigma\colon f\mapsto f^{\prime}\) of permutations on functions given by \(f^{\prime}_{i}=f_{\sigma^{-1}(i)}\) must commute with \(\phi\), that is,

\[\phi(\sigma(f^{\text{in}}))=\sigma(\phi(f^{\text{in}}))\] (2)

for any \(\sigma\in\mathbb{S}_{n}\) and any input function \(f^{\text{in}}\in\mathbb{R}^{V}\). In contrast, for networks that just learn embeddings \(\mathcal{G}\mapsto\psi(A)\), the constraint is _invariance_ to permutations, i.e., \(\psi(A)=\psi(\sigma(A))\). In practice, the two cases are closely related because most graph embedding networks work with functions defined on the vertices, and then symmetrize over permutations in their final layer using a readout function such as \(\psi(\mathcal{G})=\sum_{i}f_{i}^{\text{out}}\).

Enforcing (2) on the hypothesis space directly would be very limiting, effectively constraining GNNs to be composed of symmetric polynomials of \(f^{\text{in}}\) combined with pointwise nonlinearities. Message passing graph neural networks cleverly get around this problem by using the adjacency matrix itself as _side information_ to guide equivariance. For example, in classical (zeroth order) MPNNs, the output of each layer is a (vector valued) function on the graph, and the update rule from layer \(\ell\) to layer \(\ell+1\) in the simplest case is

\[f_{i}^{\ell+1}=\eta\Big{(}W\sum_{j\in\mathcal{N}(i)}f_{j}^{\ell}+\theta\Big{)},\] (3)

where \(\mathcal{N}(i)\) denotes the neighbors of node \(i\) in \(\mathcal{G}\), \(W\) and \(\theta\) are learnable weight matrices/vectors and \(\eta\) is a suitable nonlinearity. The fact that the summation only extends over the neighbors of vertex \(i\) induces a dependence of \(\phi\) on the adjacency matrix \(A\). Hence it would be more accurate to use the notation \(\phi_{A}\) for the mapping, and write the equivariance condition as

\[\phi_{\sigma(A)}(\sigma(f^{\text{in}}))=\sigma(\phi_{A}(f^{\text{in}})).\] (4)

In this interpretation, when the vertices are permuted, \(A\) can implicitly provide information about this to the algorithm, allowing it to compensate. However, \(A\) cannot convey any information about permutations that leave it invariant, so \(\phi_{A}\) must still be equivariant to the group of all such permutations, called the _automorphism group_ of \(\mathcal{G}\), denoted \(\text{Aut}(\mathcal{G})\) or just \(\text{Aut}(A)\). This observation about \(A\) acting as a source of side information that can reduce the size of the group that individual GNN operations need to be equivariant to is the starting point for the rest of this paper.

### Higher order GNNs

Despite the great success of message passing networks, a long sequence of empirical as well as theoretical results [40; 34; 30; 13; 35] over the last several years have made it clear that the expressive power of algorithms based on simple update rules like (3) is severely limited. In response, the community has extended the message passing paradigm to message passing between vertices and edges or between carefully selected subgraphs [18; 34; 6; 8]. These networks maintain the local and equivariant character of earlier MPNNs, but they can more faithfully reflect local topological information and are particularly well suited to domains such as chemistry, where capturing local structures such as functional groups is critical.

In tandem, researchers have developed _higher order MPNN_ architectures, where the outputs of individual edge or subgraph "neurons" are not just scalars, but vector or tensor quantities indexed by the vertices of the graph involved in the given substructure. For example, in the chemistry setting, if n is a titular neuron corresponding to a benzene ring in a molecule, the output of n, assuming we have \(C\) channels, might be a matrix \(T\in\mathbb{R}^{6\times C}\) or a tensor \(T\in\mathbb{R}^{6\times 6\times C}\), or \(T\in\mathbb{R}^{6\times 6\times 6\times C}\), etc.. In each of these cases the non-channel dimensions correspond to the six atoms making up the ring, and when n communicates with other subgraph-neurons, must be treated accordingly. Such higher order representations offer greater expressive power because they allow \(T\) to capture information about _relations_ between pairs of vertices, triples, and so on. The general trend towards studying higher order message passing is also closely tied to the emergence of hypergraph neural networks [15], "topological" neural networks and simplicial complex networks [7; 6].

Recently, [21] proposed a general formalism for describing such higher order architectures using so-called \(P\)-tensors. In this formalism, given a neuron n attached to a subgraph \(S\) with \(m\) vertices, we say that the output of n is a _zeroth order \(P\)-tensor_\(T\) with \(C\) channels if it is simply a vector \(T\in\mathbb{R}^{C}\). The elements of this vector are scalars in the sense that they are invariant to permutations of the vertices of \(S\). We say that \(T\) is a _first order \(P\)-tensor_ if it is a matrix \(T\in\mathbb{R}^{m\times C}\) whose columns transform under permutations of \(S\) similarly to how \(f^{\text{in}}\) and \(f^{\text{out}}\) transform under global permutations of the full graph:

\[\sigma\colon T\mapsto T^{\prime} T^{\prime} T^{\prime}_{i,c}=T_{\sigma^{-\text{l}(i)},c} \sigma\in\mathbb{S}_{m}.\] (5)

We say that \(T\) is a _second order \(P\)-tensor_\(T\in\mathbb{R}^{m\times m\times C}\), if each slice corresponding to a given channel transforms according to the second order action of the symmetric group, similarly (1):

\[\sigma\colon T\mapsto T^{\prime} T^{\prime}_{i,j,c}=T_{\sigma^{-\text{l}(i)},\,\sigma^{-\text{l}(j)},c} \sigma\in\mathbb{S}_{m}.\] (6)Continuing this pattern, a \(k\)_'th order \(P\)-tensor_\(T\!\in\!\mathbb{R}^{m\times m\times\ldots\times m\times C}\) transforms under local permutations as

\[\sigma\!:T\mapsto T^{\prime} T^{\prime} T^{\prime}_{i_{1},\ldots,i_{k},c}=T_{\sigma^{-1}(i_{1}),\ldots,\, \sigma^{-1}(i_{k}),c} \sigma\in\mathbb{S}_{m}.\] (7)

[21] derives the general rules for equivariant message passing between such \(P\)-tensors in the cases that the sending and receiving subgraphs \(S\) resp. \(S^{\prime}\) are (a) the same (b) partially overlap (c) are disjoint. However, in each of these cases however it was assumed that \(T\) needs to be equivariant to _all possible_ permutations of the vertices of the underlying subgraphs \(S\) and \(S^{\prime}\).

As we discussed above, this is an overly restrictive condition that limits the extent to which a higher order subgraph neural network can exploit the underlying topology. In the following sections we focus on just the type of messages that are sent from a given subgraph \(S\) to _itself_ (called _linmaps_ in the \(P\)-tensors nomenclature), and derive a way of making these messages equivariant to just \(\operatorname{Aut}(S)\) rather than the full symmetric group \(\mathbb{S}_{m}\).

## 3 Equivariance to local permutations: the group theoretic approach

Recall that a _representation_ of a finite group \(G\) such as \(\mathbb{S}_{m}\) or \(\operatorname{Aut}(S)\) is a (complex) matrix valued function \(\rho\!:G\to\mathbb{C}^{d_{\rho}\times d_{\rho}}\) where the \(\rho(\sigma)\) matrices multiply the same way as the corresponding group elements do

\[\rho(\sigma_{2}\sigma_{1})=\rho(\sigma_{2})\rho(\sigma_{1}) \sigma_{1},\sigma_{2}\in G.\]

Two representations \(\rho\) and \(\rho^{\prime}\) are said to be _equivalent_ if there is an invertible matrix \(Q\) such that \(\rho^{\prime}(\sigma)=Q^{-1}\rho(\sigma)\,Q\) for all group elements. A representation of a finite group is said to be _reducible_ if there is some invertible matrix \(Q\) that reduces it to a block diagonal form

\[\rho(\sigma)=Q^{-1}\left(\begin{array}{c|c}\rho_{1}(\sigma)&\rho_{2}(\sigma )\end{array}\right)\;Q.\]

Some fundamental results in representation theory tell us that G only has a finite number of inequivalent irreducible representations (_irreps_, for short), these irreps can be chosen to all be unitary, and that any representation of \(G\) is reducible to some combination of them [36]. These facts give rise to a type of generalized Fourier analysis on finite groups that can decompose vectors that \(G\) acts on into parts transforming according to the unitary irreps of the group.

The general approach to defining \(\operatorname{Aut}(S)\)-equivariant maps for first, second, and higher order subgraph neurons would use this machinery. In particular, defining permutation matrices as usual as

\[[P_{\sigma}]_{i,j}=\left\{\begin{array}{ll}1&\text{if }\;\sigma(j)=i\\ 0&\text{otherwise},\end{array}\right.\]

dropping the channel indices without loss of generality, and writing our \(P\)-tensors in vectorized form \(\overline{T}=\text{vec}(T)\in\mathbb{R}^{m^{k}}\), (5)-(7) can be written in a unified form

\[\overline{T^{\prime}}=P^{k}(\sigma)\,\overline{T}\]

where \(P^{k}(\sigma)\) is the \(k\)-fold Kronecker product matrix \(P^{k}(\sigma)=P_{\sigma}\otimes P_{\sigma}\otimes\ldots P_{\sigma}\). Crucially, as \(\sigma\) ranges over the automorphisms of \(S\), these product matrices \(P^{k}(\sigma)\), form a unitary representation of the automorphism group.

According to representation theory, \(P^{k}\) must then be decomposable into a direct sum of irreps \(\rho_{1},\ldots,\rho_{p}\) of \(\operatorname{Aut}(S)\) with corresponding multiplicities \(\kappa_{1},\ldots,\kappa_{p}\). The same unitary matrix \(Q\) that accomplishes this can also be used to decompose \(\overline{T}\) into a combinations of smaller vectors \((\boldsymbol{t}_{j}^{i}\!\in\!\mathbb{R}^{d_{\rho_{i}}})_{i,j}\):

\[Q\overline{T}=\bigoplus_{i}\bigoplus_{j=1}^{\kappa_{i}}\boldsymbol{t}_{j}^{i},\]

where each \(\boldsymbol{t}_{j}^{i}\) now transforms independently under the action of the group as \(\boldsymbol{t}_{j}^{i}\mapsto\rho_{i}(\sigma)\,\boldsymbol{t}_{j}^{i}\). Alternatively, stacking all \(\boldsymbol{t}_{j}^{i}\) vectors transforming according to the same irrep \(\rho_{i}\) together in a matrix \(\boldsymbol{T}^{i}\!\in\!\mathbb{R}^{d_{\rho_{i}}\times\kappa_{i}}\), we arrive at a sequence of matrices \(\boldsymbol{T}^{1},\boldsymbol{T}^{2},\ldots,\boldsymbol{T}^{p}\) transforming as

\[\boldsymbol{T}^{1}\mapsto\rho_{1}(\sigma)\,\boldsymbol{T}^{1} \boldsymbol{T}^{2}\mapsto\rho_{2}(\sigma)\,\boldsymbol{T}^{2} \ldots \boldsymbol{T}^{p}\mapsto\rho_{p}(\sigma)\,\boldsymbol{T}^{p}.\] (8)It is very easy to see how one might construct learnable linear operations that are equivariant to these actions: simply multiply each \(\bm{T}^{i}\)_from the right_ by a learnable weight matrix \(W^{i}\).

This general, group theoretic approach to constructing automorphism group equivariant linear maps between \(k\)'th order \(P\)-tensors can be seen as a special case of [29, 38]. Operationally, it just reduces to the following sequence of steps:

1. Find the unitary matrix \(Q\) that decomposes \(P^{k}(\sigma)=P_{\sigma}\otimes\ldots\otimes P_{\sigma}\) into a direct sum of \(\operatorname{Aut}(S)\) irreps.
2. Use \(Q\) to decompose the input \(P\)-tensor into a sequence of matrices \(\bm{T}^{1}_{\cdot\cdot\cdot}\bm{T}^{p}\) transforming as (8).
3. Multiply each \(\bm{T}^{i}\) by an appoppriate learnable weight matrix \(W^{i}\in\mathbb{R}^{\kappa_{i}\times\kappa_{i}}\).
4. Use the inverse map \(Q^{-1}=Q^{\dagger}\) to reassemble \(\bm{T}^{1},\bm{T}^{2},\ldots,\bm{T}^{p}\) into the output \(P\)-tensor \(T^{\text{out}}\).

Notwithstanding its elegance, this representation approach to implementing automorphism group equivariance also has some disadvantages. Specifically, it requires to (a) determine the automorphism group of each subgraph, and (b) explicitly find its irreducible representations, which is also not trivial. The underlying mathematical structure however is important because it forms the basis to generalizing the approach to a much simpler framework in the next section:

1. We have a collection of (orthogonal) linear maps \(\{P^{k}(\sigma)\colon U\mapsto U\}_{\sigma\in\operatorname{Aut}(S)}\) (with \(U=\mathbb{R}^{m^{k}}\)) that the neuron's operation needs to be equivariant to.
2. \(U\) is decomposed into an orthogonal sum of subspaces \(U=U_{1}\oplus\ldots\oplus U_{p}\) corresponding to the different irreps featured in the decomposition of \(P^{k}\).
3. Each \(U_{i}\) is further decomposed into an orthogonal sum of subspaces \(U_{i}=V_{1}^{i}\oplus\ldots\oplus V_{\kappa_{i}}^{i}\) corresponding to the different columns of the \(\bm{T}^{i}\) matrices.
4. The decomposition is such that the \(\{P^{k}(\sigma)\}\) maps fix each \(V_{j}^{i}\) subspace. Moreover, for a fixed \(i\), \(\{P^{k}(\sigma)\}\) acts the _same_ way on each \(V_{j}^{i}\) subspace by \(\rho_{i}(\sigma)\).
5. This structure implies that any linear map that linearly mixes the \(V_{1}^{i},\ldots V_{\kappa_{i}}^{i}\) subspaces but does _not_ mix information across subspaces with different values of \(i\) is equivariant.

## 4 Equivariance via spectral graph theory: Schur layers

In place of the representation theoretical approach described in the previous section, in this paper we advocate a simpler way of implementing automorphism group equivariance based on just spectral graph theory. The cornerstones of this approach are the following two theorems. The proofs can be found in the Appendix.

**Theorem 1**.: _Let \(G\) be a finite group acting on a space \(U\) by the linear action \(\{g\colon U\to U\}_{g\in G}\). Assume that we have a decomposition of \(U\) into a sequence of spaces of the form_

\[U=U_{1}\oplus\ldots\oplus U_{p}\]

_where each \(U_{i}\) is invariant under the action of the group (this means that for any \(g\in G\) and \(v\in U_{i}\), we have \(g(v)\in U_{i}\)). Let \(\phi\colon U\to U\) be a linear map that is a homothety on each \(U_{i}\), i.e., \(\phi(w)=\alpha_{i}w\) for some fixed scalar \(\alpha_{i}\) for any \(w\in U_{i}\). Then \(\phi\) is equivariant to the action of \(G\), i.e., \(\phi(g(u))=g(\phi(u))\) for any \(u\in U\) and any \(g\in G\)._

The representation theoretic result of the previous section corresponds to a refinement of this result involving a further decomposition of each \(U_{i}\) space into a sequence of smaller subspaces.

**Theorem 2**.: _Let \(G\) and \(U\) be as in Theorem 1, but now assume that each \(U_{i}\) further decomposes into an orthhogonal sum of subspaces in the form \(U_{i}=V_{1}^{i}\oplus\ldots\oplus V_{\kappa_{i}}^{i}\) such that_

1. _Each_ \(V_{j}^{i}\) _subspace is individually invariant by_ \(G\)_;_
2. _For any fixed value of_ \(i\)_, the spaces_ \(V_{1}^{i},\ldots,V_{\kappa_{i}}^{i}\) _are isomorphic and there is a set of canonical isomorphisms_ \(\iota^{i}_{j\to j^{\prime}}\colon V_{j}^{i}\to V_{j^{\prime}}^{i}\) _between them such that_ \[g(\iota^{i}_{j\to j^{\prime}}(v))=\iota^{i}_{j\to j^{\prime}}(g(v)) \forall\,v\in V_{j}^{i}.\]

_Let \(\phi\colon U\to U\) be a map of the form_

\[\phi(v) =\sum_{j^{\prime}}\alpha^{i}_{j,j^{\prime}}\iota^{i}_{j\to j^{ \prime}}(v) v \in V_{j}^{i}\]

_for some fixed set of coefficients \(\{\alpha^{i}_{j,j^{\prime}}\}\). Then \(\phi\) is equivariant to the action of \(G\) on \(U\)._In the matrix language of the previous section, \(U_{1},\ldots,U_{p}\) correspond to the \(\bm{T}^{1},\bm{T}^{2},\ldots,\bm{T}^{p}\) matrices, whereas the \(V_{1}^{i},\ldots,V_{\kappa_{i}}^{i}\) subspaces of \(U_{i}\) correspond to individual columns of \(\bm{T}^{i}\). For any fixed \(i\), the \(\left(\alpha_{j,j^{\prime}}^{i}\right)_{j,j^{\prime}_{i}}\) scalars correspond to the individual matrix entries of the learnable weight matrix \(W^{i}\). For our simplified spectral approach to automorphism group equivariance we will content ourselves with using Theorem 1 rather than Theorem 2.

### Automorphism invariance the simple way via spectral graph theory

The key insight of this paper is that we do not necessarily need to use heavy representation theoretic machinery to find a system of subspaces to plug into Theorem 1. In particular, we have the following simple lemma.

**Lemma 1**.: _Let \(S\) be an undirected graph with \(m\) vertices, \(\operatorname{Aut}_{S}\) its automorphism group, and \(L\) its combinatorial graph Laplacian. Assume that \(L\) has \(p\) distinct eigenvalues \(\lambda_{1},\ldots,\lambda_{p}\) and corresponding subspaces \(U_{1},\ldots,U_{p}\). Then each \(U_{i}\) is invariant under the first order action (5) of \(\operatorname{Aut}_{S}\) on \(\mathbb{R}^{m}\)._

Proof.: Since \(\operatorname{Aut}_{S}\) is a subgroup of the full group of vertex permutations, its action on \(\mathbb{R}^{m}\) is just \(\mathbf{v}\mapsto\sigma(\mathbf{v})=P_{\sigma}\mathbf{v}\) with \(\sigma\in\operatorname{Aut}_{S}\). \(L\) is a real symmetric matrix, so its eigenspaces \(U_{1},\ldots,U_{p}\) are mutually orthogonal and \(U_{1}\oplus\ldots\oplus U_{p}=\mathbb{R}^{n}\). Furthermore, \(\mathbf{v}\in U_{i}\) if and only if \(L\mathbf{v}=\lambda_{i}\mathbf{v}\). By definition, \(\operatorname{Aut}_{S}\) is the set of permutations that leave the adjacency matrix, and consequently the Laplacian invariant, so. In particular, \(P_{\sigma}LP_{\sigma^{-1}}=L\) for any \(\sigma\in\operatorname{Aut}_{S}\). Therefore, for any \(\mathbf{v}\in U_{i}\)

\[L\left(\sigma(\mathbf{v})\right)=L\,P_{\sigma}\mathbf{v}=P_{\sigma}\,LP_{ \sigma^{-1}}P_{\sigma}\mathbf{v}=P_{\sigma}L\mathbf{v}=\lambda_{i}P_{\sigma} \mathbf{v}=\lambda_{i}\sigma(\mathbf{v})\hskip 28.452756pt\forall\;\sigma\in \operatorname{Aut}_{S}\]

showing that \(\sigma(\mathbf{v})\in U_{i}\). Hence \(U_{i}\) is an invariant subspace. 

The following Corollary puts this lemma to use, providing a surprisingly easy way of creating locally automorphism equivariant neurons. We define a _Schur layer_ as a neural network module that applies this operation to every instance of a given subgraph in the graph, for example, every benzene ring in a molecule. For the sake of global permutation equivariance, the weight matrices for any given subgraph \(S\) must be shared across all instances of \(S\) across in the graph.

**Corollary 1**.: _Consider a GNN on a graph that involves a neuron \(\mathbf{n}_{S}\) corresponding to a subgraph \(S\) with \(m\) vertices. Assume that the input of \(\mathbf{n}_{S}\) is a matrix \(T\in\mathbb{R}^{m\times c_{\mathbf{n}}}\), the rows of which transform covariantly with permutations of \(S\) and \(c_{\mathbf{n}}\) is the number of channels. Let \(L\) be the combinatorial Laplacian of \(S\), \(U_{1},\ldots,U_{p}\) be the eigenspaces of \(L\), and \(M_{i}\) an orthognal basis for the \(i\)'th eigenspace stacked into an \(\mathbb{R}^{n\times dim(U_{i})}\) dimensional matrix. Then for any collection of learnable weight matrices \(W_{1},\ldots,W_{p}\in\mathbb{R}^{c_{\mathbf{n}}\times c_{\mathbf{n}_{S}}}\),_

\[\phi\colon T\longmapsto\sum_{i=1}^{p}M_{i}M_{i}^{\top}T\,W_{i}\] (9)

is a permutation equivariant linear operation.

The spectral approach also generalizes to higher order permutation equivariance, in which case we can take advantage of the more refined two-level subspace structure implied by Theorem 2.

**Theorem 3**.: _Let \(S\), \(L\) and the \(M_{i}\)'s be as in Corollary 1. Given a multi-index \(\mathbf{i}=(i_{1},\ldots,i_{k})\in\{1,\ldots,p\}^{k}\), we define its type as the tuple \(\mathbf{n}=(n_{1},\ldots,n_{p})\), where \(n_{j}\) is the number of occurrences of \(j\) in \(\mathbf{i}\) and we define \(\mathcal{I}_{\mathbf{n}}\) as the set of all multi-indices of type \(\mathbf{n}\). Assume that the input to neuron \(\mathbf{n}_{S}\) is a \(k\)'th order permutation equivariant tensor \(T\in\mathbb{R}^{m\times\ldots\times m\times c_{\mathbf{n}}}\), as defined in Section 3. For any given \(\mathbf{i}\), define the \(k\)'th order eigen-projector_

\[\Pi_{\mathbf{i}}=\mathcal{P}_{\mathbf{i}}(M_{i_{1}}^{\top}\otimes M_{i_{2}}^{ \top}\otimes\ldots\otimes M_{i_{k}}^{\top}\otimes I)\colon\mathbb{R}^{m\times \ldots\times m\times c}\rightarrow\mathbb{R}^{m\times\ldots\times m\times c}\]

_where \(\mathcal{P}_{\mathbf{i}}\) is a permutation map that canonicalizes the form of the projection, as defined in the Appendix. Let \(T\circ W\) denote multiplying \(T\) by the matrix \(W\) only along its last dimension. Then for any collection of weight matrices \(\{W_{i^{\prime},\mathbf{i^{\prime}}}\in\mathbb{R}^{c_{\mathbf{n}}\times c_{ \mathbf{n}}}\}\) the map_

\[\phi\colon T\longmapsto\sum_{\mathbf{n}}\sum_{\mathbf{i^{\prime}}\in\mathcal{I }_{\mathbf{n}}}\sum_{\mathbf{i}\in\mathcal{I}_{\mathbf{n}}}\Pi_{\mathbf{i^{ \prime}}}^{\top}(\Pi_{\mathbf{i}}(T\odot W_{\mathbf{i^{\prime}},\mathbf{i^{ \prime}}}))\]

_isExperiments

To empirically evaluate our Schur layers, we implement the first order case described in Corollary 1 and compare it with other higher order MPNNs. Note that the theorem only gives the equivariant maps to transform local representation on a subgraph, i.e., \(\phi:T^{\text{in}}\mapsto T^{\text{out}}\) where \(T^{\text{in}}\in\mathbb{R}^{m\times c_{\text{in}}}\) and \(T^{\text{out}}\in\mathbb{R}^{m\times c_{\text{out}}}\) are the inputs and output of a neuron corresponding to a specific subgraph. The rest of the message passing is conducted in the usual way, in particular, everything is implemented in the \(P\)-tensors framework [21].

There are several design details about the architecture that are worth mentioning: (1) We chose cycles of lengths three to eight in most of the experiments and also added branched cycles to show our algorithm's scalability. The reason is that in the chemical dataset we used, cycles are the most important functional group to the property to be predicted, other subgraphs such as \(m\)-stars, and \(m\)-paths did not help the performance. (2) While having first-order representation on the cycles, we also maintain 0'th-order representation (i.e., scalars) on node and edges, as in [6; 21]. These node and edge representations capture more elementary information about the graph, such as \(k\)-hop neighborhoods, and are still important in higher order MPNNs. The representations pass messages with each other by intersection rule as defined in \(P\)-tensor framework. (3) As discussed in Appendix G, we view _Schur_ layer's operation as a spectral convolution filter [5] applied to the subgraph and the number of channels to indicate how many times it expands the input feature to the output feature. The codes used to run our experiments can be found at https://github.com/risilab/SchurNet.

### _Schur_ layer improves over _Linmaps_

First of all, we want to show that considering more possible equivariant maps on the subgraph can indeed boost the performance. To this end, we performed controlled experiments to compare _Schur_ layer with the equivariant maps w.r.t. \(\mathbb{S}_{m}\) (following [21] we called these _linmaps_). To make a fair comparison, we use the same architecture including MLPs and message passing defined by \(P\)-tensor and only replace the equivariant maps used in _Linmaps_ by what is defined in Corollary 1. We didn't compare with Autobahn [37] because it didn't use the \(P\)-tensor framework in the original paper and it's hard for us to implement the convolution w.r.t. automorphism group for all the subgraphs we chose. However we note that for the case of the cycles (see Table 4), the equivariant maps given by our approach are equal to that given by the group theoretical approach.

We'll present the results on the commonly used molecular benchmark ZINC-12K [26] dataset in the main text. Results on TUdatasets as well as runtime comparison can be found in Appendix H. The task on ZINC-12K is to regress the \(\log P\) coefficient of each molecule and the Mean absolute error (MAE) between the prediction and the ground truth is used for evaluation.

We design experiments to compare _Linmaps_ and _Schur_ layers in various scenarios, to showcase the robustness of the improvement. Table 1 shows under various message passing schemes between edges and cycles, _Schur_ Layer consistently outperforms _Linmaps_. Those are indications of the added expressive power of extra equivariant maps in _Schur_ layer, and they're effective in various architectural design settings. Another ablation study regarding different cycle sizes can be found in Appendix H.

Then we studied the possibilities of adding _Schur_ layer in different places of higher-order message passing scheme. In table 2, we observed that the more condensed the higher-order feature is, the more improvement that the _Schur_ Layer brings to us over _Linmaps_. We attribute the improvements of adding/replacing _Schur_ layer in various scenarios over _Linmaps_ the benefit gained from utilizing the

\begin{table}
\begin{tabular}{|c|c|c|} \hline Layer & Pass message when overlap \(\geq 1\) & Pass message when overlap \(\geq 2\) \\ \hline _Linmaps_ (baseline) & \(0.074\pm 0.008\) & \(0.074\pm 0.005\) \\ \hline _Schur_ layer & \(\mathbf{0.070\pm 0.006}\) & \(\mathbf{0.071\pm 0.003}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison between _Schur_ Layer and _Linmaps_ with different message passing schemes. The message passing scheme is a design choice in \(P\)-tensor framework, where the user can set when two subgraph’s representations communicate. The mostly common use case is to require at least \(k\) vertices in the intersection of two subgraphs for them to communicate. Experiments on ZINC-12k dataset and all scores are test MAE. Cycle sizes of {3,4,5,6,7,8,11} are used.

subgraph structure and increased number of equivariant maps. We also tried other ways to use _Schur_ layer, the result is summarized in Appendix G.

### Flexibility

The other advantage of _Schur_ layer is that it computes the feature transform only based on the subgraph's Laplacian, bypassing a difficult step of finding the automorphisms group and _irreps_ of the subgraph it acts on. As discussed in the theory, _Schur_ layer constructs equivariant maps only based on the subgraph's Laplacian and is applicable directly to any subgraphs, making the implementation much easier when different subgraphs are chosen than the group theoretical approach. This allows it to easily extend to any subgraph templates that're favorable by the user. To demonstrate this, we augment the subgraphs in the model by all the five and six cycles with one to three branches (including in total 16 non-isomorphic subgraph templates), comparing with baseline model where only the cycle itself is considered. Results can be found in Appendix G.

### Benchmark results

Finally, and most importantly, we compare the _Schur_-Net to several other higher-order MPNNs 1 on ZINC-12k dataset and OGB-HIV dataset [23] in table 3. We included baselines of (1) classical MPNNs: GCN[28], GIN [40], GINE [24], PNA[11], HIMP [16] (2) higher order MPNNs: \(N^{2}\)-GNN [14]2, CIN [6], \(P\)-tensors [21] (3) Subgraph-GNNs: DS-GNN(EGO+) and DSS-GNN(EGO+) [4], GNN-AK+ [46], SUN(EGO+)[17] (4) Autobahn [37].

Footnote 1: A discussion of related work can be found in Appendix C.

Footnote 2: The works [31; 34] don’t have results in those two dataset.

We find that _Schur_ Net ranked second on ZINC-12K and outperformed all other baselines on OGB-HIV dataset. This shows the expressivity of adding more equivariant maps by leveraging the subgraph topology. Furthermore, note that while \(N^{2}\)-GNN outperforms _Schur_ Net on ZINC-12K, it's a second-order model whereas in our experiment, we only used first-order activation. Also, the partial reason _Autobahn_ didn't perform well is in the original implementation, the authors didn't use \(P\)-tensor framework and used only a part of all possible linear message passing schemes. This shows to get the full power of equivariant maps w.r.t. subgraph automorphism group, we need to combine it with a general message passing framework between subgraphs as well.

## 6 Limitations

Unlike the representation theoretic approach, the spectral approach is not guaranteed the give the finest possible decomposition into invariant subspaces. Hence, equations like (9) do not necessarily define the most general possible automorphism-equivariant linear maps. In this paper we did not investigate from a theoretical point of view the extent of this gap. In general, being able to craft automorphism-equivariant layers of any order for any types of subgraphs opens up a host of possibilities for making GNNs more powerful. Our experiments are limited to some of the simplest cases, such as exploiting cycles and edges. We also only used first order activations.

\begin{table}
\begin{tabular}{|c|c|} \hline Model & Test MAE \\ \hline _Limmaps_ & \(0.071\pm 0.004\) \\ Simple _Schur_-Net & \(0.070\pm 0.005\) \\ Linmap _Schur_-Net & \(\mathbf{0.068\pm 0.002}\) \\ Complete _Schur_-Net & \(\mathbf{0.064\pm 0.002}\) \\ \hline \end{tabular}
\end{table}
Table 2: An experiment demonstrating different ways of using _Schur_ layer. ”Complete _Schur_ Layer” means that we apply _Schur_ Layer on the incoming messages together with the original cycle representation. ”Linmap SchulLayer” means that we just apply the _Schur_ Layer on the aggregated subgraph representation feature. ”Simple _Schur_ Layer” means we directly apply _Schur_ Layer on the subgraph features without any preprocessing. We can observe that as the subgraph information diversifies, _Schur_ layer tends to decouple the dense information better and results in better performance. The test MAE of _Limmaps_ in this table is taken from [21].

## 7 Conclusions

Enforcing equivariance to the automorphism group of subgraphs in higher order neural networks seemingly requires the use of advanced tools from group representation theory. This is likely a large part of the reason why automorphism-based architectures such as [37] are not used more commonly in practical applications. In this paper we have shown that a simpler approach based on spectral graph theory, following the same underlying logic as the group theoretic approach but bypassing having to enumerate all irreducible representations of the automorphism group, can lead to an architecture that is almost as expressive. Our algorithm, called Schur Nets, easily generalizes to higher order activations, as well as incorporating other types of side information such as vertex labels. In a practical setting, Schur Nets is easiest to deploy in conjunction with a message passing framework like \(P\)-tensors that hides the complexities of the higher order message passing component. The empirical performance of Schur Nets on the ZINC 12K dataset is superirror to all other comparable (non-transformer based) architectures that we are aware of.

Given the similarity between the way we utilize the eigenspaces of the graph Laplacian and the so-called graph Fourier transform, our approach exposes heretofore unexplored connections between permutation equivariance and spectral GNNs such as [9; 22]. It also highlights the fact that while permutation equivariance is a fundamental constraint on graph neural networks, the key to building high performing, expressive GNNs is to reduce the size of the group that the network needs to be equivariant to as much as possible, using whatever side-information we can employ, whether that be the adjacency matrix, vertex degrees or something else.

## Acknowledgements

We would like to thank Andrew Hands for many valuable pieces of advice and much practical assistance that he has given to the experimental side of this project. We also gratefully acknowledge the Toyota Technological Institute of Chicago and the Data Science Institute at the University of Chicago for making their computational resources available for our use, as well as NSF MRI-1828629 for additional infrastructure that was used in the course in this project.

\begin{table}
\begin{tabular}{l c c} \hline \hline Model & ZINC-12K MAE(\(\downarrow\)) & OGB-HIV ROC-AUC(\% \(\uparrow\)) \\ \hline GCN & \(0.321\pm 0.009\) & \(76.07\pm 0.97\) \\ GIN & \(0.408\pm 0.008\) & \(75.58\pm 1.40\) \\ GINE & \(0.252\pm 0.014\) & \(75.58\pm 1.40\) \\ PNA & \(0.133\pm 0.011\) & \(79.05\pm 1.32\) \\ HIMP & \(0.151\pm 0.002\) & \(78.80\pm 0.82\) \\ \hline \(N^{2}\)-GNN & \(\mathbf{0.059\pm 0.002}\) & - \\ CIN & \(0.079\pm 0.006\) & \(80.94\pm 0.57\) \\ P-tensors & \(0.071\pm 0.004\) & \(80.76\pm 0.82\) \\ \hline DS-GNN (EGO+) & \(0.105\pm 0.003\) & \(77.40\pm 2.19\) \\ DSS-GNN (EGO+) & \(0.097\pm 0.006\) & \(76.78\pm 1.66\) \\ GNN-AK+ & \(0.091\pm 0.011\) & \(79.61\pm 1.19\) \\ SUN (EGO+) & \(0.084\pm 0.002\) & \(80.03\pm 0.55\) \\ \hline Autobahn & \(0.106\pm 0.004\) & \(78.0\pm 0.30\) \\ \hline \(\mathit{Schur}\)-Net & \(0.064\pm 0.002\) & \(\mathbf{81.6\pm 0.295}\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparison of different models on the ZINC-12K and OGBG-MOLHIV datasets.

## References

* [1] Militadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to Represent Programs with Graphs. In _International Conference on Learning Representations (ICLR)_, 2018.
* [2] Emily Alsentzer, Samuel Finlayson, Michelle Li, and Marinka Zitnik. Subgraph Neural Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [3] Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, and et al. Relational Inductive Biases, Deep Learning, and Graph Networks. In _arXiv:1806.01261_, 2018.
* [4] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M. Bronstein, and Haggai Maron. Equivariant Subgraph Aggregation Networks. In _International Conference on Learning Representations (ICLR)_, 2022.
* [5] Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. A Survey on Spectral Graph Neural Networks. _arXiv preprint arXiv:2302.05631_, 2023.
* [6] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido Montufar, and Michael Bronstein. Weisfeiler and Lehman Go Cellular: CW Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [7] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lio, and Michael Bronstein. Weisfeiler and Lehman go topological: Message passing simplicial networks. In _International Conference on Learning Representations (ICLR)_, 2021.
* [8] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting. In _International Conference on Learning Representations (ICLR)_, 2020.
* [9] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral Networks and Locally Connected Networks on Graphs. In _International Conference on Learning Representations (ICLR)_, 2014.
* [10] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can Graph Neural Networks Count Substructures? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [11] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal Neighbourhood Aggregation for Graph Nets. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [12] Pim de Haan, Taco S. Cohen, and Max Welling. Natural Graph Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [13] Nima Dehmamy, Albert-Laszlo Barabasi, and Rose Yu. Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [14] Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan Zhang, and Yixin Chen. Extending the Design Space of Graph Neural Networks by Rethinking Folklore Weisfeiler-Lehman. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [15] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. Hypergraph Neural Networks. _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2019.
* [16] Matthias Fey, Jan-Gin Yuen, and Frank Weichert. Hierarchical Inter-Message Passing for Learning on Molecular Graphs. In _ICML Graph Representation Learning and Beyond (GRL+) Workshop_, 2020.
* [17] Fabrizio Frasca, Beatrice Bevilacqua, Michael M. Bronstein, and Haggai Maron. Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [18] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural Message Passing for Quantum Chemistry. _International Conference on Machine Learning (ICML)_, 2017.
* [19] Martin Grohe. _Descriptive Complexity, Canonisation, and Definable Graph Structure Theory_. Cambridge University Press, 2017.

* [20] Rafael Gomez-Bombarelli, Jennifer N. Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Alan Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. _ACS Central Science_, 2018.
* [21] Andrew R. Hands, Tianyi Sun, and Risi Kondor. P-Tensors: A General Framework for Higher Order Message Passing in Subgraph Neural Networks. In _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2024.
* [22] Mikael Henaff, Joan Bruna, and Yann LeCun. Deep Convolutional Networks on Graph-Structured Data. _arXiv:1506.05163_, 2015.
* [23] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open Graph Benchmark: Datasets for Machine Learning on Graphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [24] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for Pre-Training Graph Neural Networks. In _International Conference on Learning Representations (ICLR)_, 2020.
* [25] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In _International Conference on Machine Learning (ICML)_, 2015.
* [26] John J. Irwin, Khanh G. Tang, Jennifer Young, Chinzorig Dandarchuluun, Benjamin R. Wong, Munkhzul Khurelbaatar, Yurii S. Moroz, John Mayfield, and Roger A. Sayle. ZINC20--A Free Ultralarge-Scale Chemical Database for Ligand Discovery. _Journal of Chemical Information and Modeling_, 2020.
* [27] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* [28] T. N. Kipf and M. Welling. Semi-Supervised Classification with Graph Convolutional Networks. In _International Conference on Learning Representations (ICLR)_, 2017.
* [29] Risi Kondor and Shubhendu Trivedi. On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups. In _International Conference on Machine Learning (ICML)_, 2018.
* [30] Andreas Loukas. How Hard is it to Distinguish Graphs with Graph Neural Networks? In _Advances in Neural Information Processing Systems (NeurIPS)_, 2020.
* [31] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably Powerful Graph Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [32] Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and Equivariant Graph Networks. In _International Conference on Learning Representations (ICLR)_, 2019.
* [33] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. TUDataset: A Collection of Benchmark Datasets for Learning with Graphs. _ICML Graph Representation Learning and Beyond (GRL+) Workshop_, 2020.
* [34] Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2019.
* [35] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Approximation Ratios of Graph Neural Networks for Combinatorial Problems. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [36] Jean-Pierre Serre. _Linear Representations of Finite Groups_. Springer-Verlag, 1977.
* [37] Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-Based Graph Neural Nets. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [38] Erik Henning Thiede, Truong-Son Hy, and Risi Kondor. The General Theory of Permutation Equivariant Neural Networks and Higher Order Graph Variational Encoders. _arXiv:2004.03990_, 2020.
* [39] Boris Weisfeiler and Andrei Leman. The Reduction of a Graph to a Canonical Form and the Algebra Which Appears Therein. _Nauchno-Technicheskaya Informatsiya_, 1968.

* [40] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are Graph Neural Networks. In _International Conference on Learning Representations (ICLR)_, 2019.
* [41] Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar. HyperGCN: A New Method for Training Graph Convolutional Networks on Hypergraphs. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.
* [42] Jiaxuan You, Jonathan Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-Aware Graph Neural Networks. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* [43] Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R. Salakhutdinov, and Alexander J. Smola. Deep Sets. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* [44] Muhan Zhang and Pan Li. Nested Graph Neural Networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [45] Ruochi Zhang, Yuesong Zou, and Jian Ma. Hyper-SAGNN: A Self-Attention Based Graph Neural Network for Hypergraphs. In _International Conference on Learning Representations (ICLR)_, 2020.
* [46] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness. In _International Conference on Learning Representations (ICLR)_, 2022.

Additional details

The purpose of the permutation map \(\mathcal{P}_{\mathbf{i}}\) in Theorem 3 is to remap the tensor product space \(\mathbb{R}^{\text{dim}(U_{i_{1}})\times\ldots\times\text{dim}(U_{i_{k}})\times C}\) into a canonical form so that those dimensions that correspond to \(i_{j}=1\) are mapped to the first \(n_{1}\) slots, those dimensions with \(i_{j}=2\) are mapped to the second \(n_{2}\) slots, and so on. Given the permutation \(\tau\colon\{1,2,\ldots,k\}\to\{1,2,\ldots,k\}\) that canonicalizes the indices in this way, \(\mathcal{P}_{\mathbf{i}}\) is just the corresponding permutation map, i.e.,

\[\mathcal{P}_{\mathbf{i}}(x_{1}\otimes\ldots x_{k}\otimes x_{c})=x_{\tau(1)} \otimes\ldots x_{\tau(k)}\otimes x_{c}.\]

## Appendix B Proofs

**Proof of Theorem 1.** Let \(\{g\!\downarrow_{U_{i}}\colon U_{i}\to U_{i}\}_{g\in G}\) be the restriction of the action of \(G\) to \(U_{i}\). Since \(G\) fixes each \(U_{i}\), the action of \(G\) on the full space decomposes in the form

\[g(u) =\sum_{i=1}^{p}g\!\!\downarrow_{u_{i}}(u\!\!\downarrow_{U_{i}}) u\!\in\!U.\]

Now since multiplication by scalars commutes with linear maps,

\[\phi(g(u)) =\sum_{i=1}^{p}\alpha_{i}\,g\!\!\downarrow_{u_{i}}(u\!\! \downarrow_{U_{i}})=\sum_{i=1}^{p}g\!\!\downarrow_{u_{i}}(\alpha_{i}\,u\!\! \downarrow_{U_{i}})=g(\phi(u))\]

for any \(u\in U\) and any \(g\in G\). \(\blacksquare\)

**Proof of Theorem 2.** Each of the \(V_{j}^{i}\) subspaces is invariant, hence a homothety on each is an equivariant operation for the same reason as in Theorem 1. In addition, since \(G\) acts the same way on any pair of subspaces \((V_{j}^{i},V_{j^{\prime}}^{i})\), any map of the form \(\xi_{j,j^{\prime}}^{i}\colon V_{j}^{i}V_{j^{\prime}}^{i}\) that is just a scaling is also equivariant. The composition of equivariant linear maps is an equivariant linear map, hence any map \(\phi\) of the given form is equivariant. \(\blacksquare\)

**Proof of Theorem 3.** For any multi-index \(\mathbf{i}\), the subspace \(\text{Im}(M_{i_{1}}^{\top}\otimes M_{i_{2}}^{\top}\otimes\ldots\otimes M_{i_{ k}}^{\top}\otimes I_{\text{c}})\) is invariant to permutations. Further after applying the permutation map \(\mathcal{P}_{\mathbf{i}}\), all such subspaces of the same type \(\mathbf{n}\), the action of \(\mathbb{S}_{k}\) on all such subspaces will be the same. Hence we can directly apply Theorem 2 to prove this theorem. \(\blacksquare\)

### Alternative Proof for Corollary 1

Here we give a more direct proof for Corollary 1.

Proof.: Without loss of generality, we assume \(c_{in}=c_{out}=1\). Since the \(M_{i}\)'s and \(\lambda_{i}\)'s are eigenspaces and eigenvectors of the Laplacian \(L\), \(L=M\Lambda M^{T}\), where \(M=[M_{1},\cdots,M_{p}]\) and \(\Lambda=\text{diag}(\lambda_{1},\cdots,\lambda_{m})\).

The neuron \(\text{n}_{S}\) is a linear transform \(\phi:R^{m}\to R^{m}\), with \(\phi(T)=\Sigma_{i}^{p}M_{i}M_{i}^{T}TW_{i}=MVM^{T}T\), where \(V=\text{diag}(W_{1}I_{n_{1}},\cdots,W_{p}I_{n_{p}})\). Here the \(W_{i}\)'s are just scalars and \(I_{n_{i}}\) is an identity matrix of the same size as the corresponding eigenspace.

Given a permutation \(\sigma\in S_{n}\), we want to show that \(\phi_{\sigma}(\sigma\circ T)=\sigma\circ\phi_{e}(T)\), which is the definition of equivariance. Note that \(\phi_{\sigma}\) actually depends on \(\sigma\), since we're doing eigenvalue decomposition on the Laplacian of the subgraph transformed by \(\sigma\). This is the key to proving equivariance. We use \(e\) to denote the identity permutation.

Now \(L^{\sigma}=P_{\sigma}LP_{\sigma}^{T}=M_{\sigma}\Lambda M_{\sigma}^{T}\), where \(M_{\sigma}=P_{\sigma}M\), while the eigenvalues remain invariant after permutation and eigenspaces are transformed in the same manner as \(L\). Thus,

\[\phi_{\sigma}(\sigma\circ T) =M_{\sigma}VM_{\sigma}^{\top}P_{\sigma}T\] \[=P_{\sigma}MVM_{\sigma}^{\top}P_{\sigma}^{\top}P_{\sigma}T\] \[=P_{\sigma}\,\phi_{e}(T)\]which is the desired result. 

**Remark 1**.: _The key to the proof is that the transform \(\phi\) depends on an object transformed with permutation \(\sigma\), specifically. graph Laplacian \(L\). And all we need is that the object \(L\) is transformed equivariantly with \(\sigma\). We can also add side informatino such as node labels or degrees to further constrain the automorphism group and increase the number of distinct eigenvalues. One easy way to do that is to set \(L^{\prime}=L+V\), where \(V=\text{diag}(v_{1},\cdots,v_{m})\) captures the node labels or degrees, and build the Schur neuron \(\mathfrak{n}_{S}\) from \(L^{\prime}\)._

## Appendix C Related works

Higher order MPNNs have become a popular research area in graph representation learning since the seminal work by [32], which characterized the space of equivariant map w.r.t. \(\mathbb{S}_{n}\) for \(k\)'th order tensors. The following works have mainly been divided into two streams, the one is based on \(k\)-Weisfeiler Leman test (\(k\)-WL test) [39, 19], for instances, [31] proposed \(k\)-order graph neural networks that is as powerful as \(k\)-WL test; [34] proposed \(k\)-GNN to approximate (or simulate) \(k\)-WL test; [14] further designed a \((k,t)\)-FWL+ hierarchy that extends the \(k\)-WL test and implemented the corresponding neuron version. A common feature of these approaches is they all work on all \(k\)-tuples of vertices for \(k\)-th order neural networks and thus make their space and time complexity at least \(\theta(n^{k})\).

The other line of work is seeking ways to choose subgraphs in the graph, that are representative or contain important information, such as functional groups in a molecule. [7] chooses simplicial complex and [6] further extends this to any cell complex such as cycles. [21] incorporates any subgraph template and uses the equivariant maps defined [32] for local feature transform and devised a high order message passing scheme between those subgraphs, called \(P\)-tensor. This is arguably the most general framework in this line of work. This line of work is kind of independent of the \(k\)-WL test since the \(k\)-WL test is aimed to distinguish any pair of non-isomorphic graphs while chosen subgraphs are meant to work in some specific regions with domain prior. It's possible to still compare this kind of network with the distinguishing power of \(k\)-WL test, for example, [6] shows if they include cycles of size at most \(k\) (including nodes and edges), their network are as powerful as WL test. However, such comparisons are not very meaningful to indicate the expressive power of such a domain-specific approach. In our opinion, the ability to learn certain features (such as count cycles of certain sizes or learn some function related to specific functional groups) might be a better criterion. It'll be an interesting research direction to design suitable criteria for the expressive power of such higher order MPNNs in general.

Our work follows this line of work since we're choosing specific subgraphs to learn representation on. But we utilize the subgraph's automorphism group to devise more possible equivariant maps (none of the aforementioned methods are built on the automorphism group). The only other work to our knowledge that uses the automorphism group is Autobahn [37], which is the first to introduce equivariance to the automorphism group to GNNs. The difference between our work and it lines twofold: (1) Autobahn theoretically introduces equivariant maps w.r.t. automorphism group via generalized convolution and Cosets, which is another group theoretical approach to construct equivariant maps, while we're using the decomposition to _irreps_ (2) More importantly, Autobahn didn't mention a practical approach to construct those equivariant maps explicitly, hindering the utilization of more diverse subgraphs. In their experiments, they only use cycles of length five and six and paths of length three to six and construct the equivariant maps potentially by hand. In contrast, we give a simple and efficient way to construct equivariant maps w.r.t. automorphism group by EVD, which is very general to be used by any subgraph. Though our approach doesn't give the full set of equivariant maps, experiments show improvement over the traditional approach where only equivariant maps w.r.t. \(S_{n}\) are used. We believe this algorithm, together with the group theoretical idea about decomposition \(R^{m^{h}}\) into stable subspaces and _irreps_ are beneficial to the research community.

There is a "third" type of higher order GNNs that is called Subgraph GNN, which deviates from the original definition of higher order MPNNs but can still be considered as higher order network. In particular, node-based GNNs such as [44, 46, 4, 42, 17] associate each node with a subgraph (by deleting the node, marking the node or extracting the EGO-network) and do MPNN on each subgraph,where they get the resulting representation \(X\in R^{n\times n}\) that can be considered as a 2nd order tensor representation on the original graph.

## Appendix D Analysis of _Schur_ layer

First of all, we notice that the equivariant map characterized in 1 (or 3) may not be the full set of equivariant maps w.r.t. \(Aut_{S}\). Because (1) In the decomposition of \(U=R^{m^{k}}\) to eigenspaces \(U=U_{1}\oplus\ldots U_{t}\), while \(U_{i}\) is stable subspaces, it might not be _irreducible_ and finer decomposition may be possible. In other words, there might be two irreps (isomorphic or not) corresponding to the same eigenvalue. (2) The maps defined by 1 didn't take into account the isomorphic subspaces corresponding to the same type of _irrep_, thus ignored the possible equivariant maps between \(V_{j}^{i}\) and \(V_{j^{\prime}}^{i}\). So, it is of interest to find out how much the gap would be between our approach and the group theoretical approach. We first look at some examples in the first-order case (see table 4).

We note that for cycles in the graph case, there's no gap. However, when we add branches to the cycle to make the automorphism group smaller, the multiplicities of the _irreps_ increase, e.g., in 5-cycle with one branch case, \(S_{2}\) only have two 1-dimensional _irreps_: trivial and sign representation of permutation, and the first-order action decompose to 4 copies of trivial and 2 copies of sign representation, gives in total \(4*4+2*2=20\) possible equivariant maps. However, note that # of _irreps_ (counting multiplicities given by \(\sum_{i}\kappa_{i}\)) is equal to # of distinct eigenvalues, meaning that each eigenspace corresponds to an irreducible subspace and the decomposition of \(R^{m}\) provided by eigenspaces is indeed a decomposition to the _irreps_ in all of the cases listed in the table. Therefore, the multiplicities are the key reason for the gap between our EVD approach and the group theoretical approach, since our EVD approach can't capture the isomorphic property between subspaces. In principle, it is possible to find out which eigenspace is isomorphic to which, but in our current implementation, we didn't take this into account because we found out that only considering cycles is enough for a very good performance in the datasets we used.

Generally, it's complicated to determine the gap between our EVD approach and the group theoretic approach, especially in higher-order cases (where merely determining the \(\kappa_{i}\)'s is tricky), so we leave this into feature exploration.

## Appendix E A brief overview about _P_-tensor framework

The _P_-tensor framework is a framework for linear equivariant maps w.r.t. \(S_{n}\) both between the same subgraph and across different subgraphs. It is built on the equivariant maps characterized by [32].

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Graph** & \(Aut_{S}\) & \begin{tabular}{c} **\# of distinct Eigenvalues** \\ **(_Schur_ Layer)** \\ \end{tabular} & 
\begin{tabular}{c} \(\sum_{i}(\kappa_{i})^{2}\) \\ _irreps_ approach** \\ \end{tabular} & \(\sum_{i}\kappa_{i}\) \\ \hline
6-cycle & \(D_{6}\) & 4 & 4 & 4 \\ \hline
5-cycle & \(D_{5}\) & 3 & 3 & 3 \\ \hline
4-cycle & \(D_{4}\) & 3 & 3 & 3 \\ \hline
3-cycle & \(D_{3}\) & 2 & 2 & 2 \\ \hline
5-star & \(S_{4}\) & 3 & 5 & 3 \\ \hline
4-star & \(S_{3}\) & 3 & 5 & 3 \\ \hline
3-path & \(S_{2}\) & 3 & 5 & 3 \\ \hline n-cliques & \(S_{n}\) & 2 & 2 & 2 \\ \hline
5-cycle & \multirow{2}{*}{\(S_{2}\)} & \multirow{2}{*}{6} & \multirow{2}{*}{20} & \multirow{2}{*}{6} \\ with one branch & & & & \\ \hline
6-cycle & \multirow{2}{*}{\(S_{2}\)} & \multirow{2}{*}{7} & \multirow{2}{*}{29} & \multirow{2}{*}{7} \\ with one branch & & & & \\ \hline \end{tabular}
\end{table}
Table 4: Examples on EVD approach vs group theoretical approach towards # of equivariant maps w.r.t. \(Aut_{S}\). To calculate the decomposition of \(R^{m}\) into _irreps_, one can calculate \(\kappa_{i}=(\phi|\chi_{i})\) where \(\phi\) and \(\chi_{i}\) is the character of the first-order action and ith _irrep_ respectively, and \((|)\) is inner product. Another quick approach is to use \(\phi(\sigma)=\sum_{k}\kappa_{k}\chi_{k}(\sigma)\) and look at some examples of \(\sigma\) to determine what the \(\kappa_{k}\) should be.

**Definition 1** (\(P\)-tensors).: _Let \(U\) be a finite set of atoms and \(D=(x_{1},\ldots,x_{d})\) an ordered subset of \(U\). We say that a \(k\)-th order tensor \(T\in\mathbb{R}^{d\times d\times\cdots\times d}\) is a \(k\)-th order permutation covariant tensor (or P-tensor for short) with reference domain \(D\) if under reordering by \(\tau\in S_{d}\)\(D\) it transforms to_

\[[\tau\circ T]_{i_{1},i_{2},\ldots,i_{k}}=T_{\tau^{-1}(i_{1}),\ldots,\tau^{-1}( i_{k})}.\]

### Message passing between _P_-tensors with the same reference domain

Consider the equivariant maps sending \(T\) on \(D\) to \(T^{\text{out}}\) on \(D\). The space of equivariant maps w.r.t \(S_{m}\) is characterized by [32]:

**Proposition 1**.: _The space of linear maps \(\phi:\mathbb{R}^{dk_{1}}\to\mathbb{R}^{dk_{2}}\) that is equivariant to permutations \(\tau\in S_{d}\) is spanned by a basis indexed by the partitions of the set \(\{1,2,\ldots,k_{1}+k_{2}\}\)._

Then the authors designed a straightforward way to write the maps explicitly. Specifically, for each partition of the set \(\{1,2,\ldots,k_{1}+k_{2}\}\), there are three parts that determines the equivariant map: (1) summing over specific dimensions or diagonals of \(T^{\text{in}}\) (2) transferring \(T^{\text{in}}\) to \(T^{\text{out}}\) by identifying indices of \(T^{\text{in}}\) with indices of \(T^{\text{out}}\) (3) broadcasting the result along certain dimensions of \(T^{\text{out}}\). These three operations correspond to the three different types of sets that can occur in a given partition \(\mathcal{P}\): (1) those that only involve the second \(k2\) numbers, (2) those that involve a mixture of the first \(k_{1}\) and \(k_{2}\) and (3) those only involve the first \(k_{1}\) numbers. The type (1) - (3) corresponds to type (1) - (3) of operations with the dimensions in the sets.

For example, in the case \(k_{1}=k_{2}=3\), the \(\mathcal{P}=\{\{1,3\},\{2,5,6\},\{4\}\}\) partition corresponds to (a) summing \(T^{\text{in}}\) along its first dimension (corresponding to \(\{4\}\)) (b) transferring the diagonal along the second and third dimension of \(T^{\text{in}}\) to the second dimension of \(T^{\text{out}}\) (corresponding to \(\{2,5,6\}\)) (c) broadcasting the result along the diagonal of the first and third dimensions (corresponding to \(\{1,3\}\)). Explicitly, this gives the equivariant map:

\[T^{\text{out}}_{a,b,a}=\sum_{c}T^{\text{in}}_{c,b,b}\]

See table 5 for another example for \(k_{1}=k_{2}=2\) case.

### Message passing between _P_-tensors with the different reference domains

If \(T^{\text{in}}\) has reference domain \(D_{1}\) and \(T^{\text{out}}\) has \(D_{2}\), with \(D_{1}\neq D_{2}\) and \(D_{1}\cap D_{2}\neq\emptyset\). We could have more options corresponding to summing either over the intersection or over \(D_{1}\), and broadcasting either over the intersection or over \(D_{2}\). So the number of maps corresponding to partition of type \((p_{1},p_{2},p_{3})\) is \(2^{(p_{1}+p_{3})}\). Let \(D_{1}\cap D_{2}=d^{\cap}\), the previous example would have the following maps in \(D_{1}\neq D_{2}\) case:

\[T^{\text{out}}_{a,b,a}=\begin{cases}\sum_{c=1}^{d^{\cap}}T^{ \text{in}}_{c,b,b}&a,b\leq d^{\cap}\\ 0&\text{otherwise},\end{cases}\quad(a\in\{1,\ldots,d^{\cap}\},\ c\in\{1,\ldots,d^{ \cap}\})\] \[T^{\text{out}}_{a,b,a}=\begin{cases}\sum_{c=1}^{d^{\cap}}T^{ \text{in}}_{c,b,b}&b\leq d^{\cap}\\ 0&\text{otherwise},\end{cases}\quad(a\in\{1,\ldots,d_{2}\},\ c\in\{1,\ldots,d^{ \cap}\})\]

\begin{table}
\begin{tabular}{|c|c||c|c|} \hline \(\mathcal{P}\) & \(\phi\) & \(\mathcal{P}\) & \(\phi\) \\ \hline \{{1}, {2}, {3}, {4}\} & \(T^{\text{out}}_{a,b}=\sum_{c,d}T^{\text{in}}_{c,d}\) & \{{2}, {1,3,4}\} & \(T^{\text{out}}_{a,a}=T^{\text{in}}_{b,b}\) \\ \{{1}, {2}, {3,4}\} & \(T^{\text{out}}_{a,b}=\sum_{c}T^{\text{in}}_{c,c}\) & \{{1,2,3}, {4}\} & \(T^{\text{out}}_{a,a}=\sum_{b}T^{\text{in}}_{a,b}\) \\ \{{1}, {2,4}, {3}\} & \(T^{\text{out}}_{a,b}=\sum_{c}T^{\text{in}}_{c,b}\) & \{{1,2,4}, {3}\} & \(T^{\text{out}}_{a,a}=\sum_{b}T^{\text{in}}_{b,a}\) \\ \{{1}, {2,3, 4}\} & \(T^{\text{out}}_{a,b}=\sum_{c}T^{\text{in}}_{b,c}\) & \{{1,2, {3, 4}}\} & \(T^{\text{out}}_{a,a}=\sum_{c}T^{\text{in}}_{c,c}\) \\ \{{2}, {1,4,3}\} & \(T\[T^{\text{out}}_{a,b,a}=\begin{cases}\sum_{c=1}^{d_{1}}T^{\text{in}}_{c,b,b}&a,b \leq d^{\cap}\\ 0&\text{otherwise},\\ \end{cases}\quad(a\in\{1,\ldots,d^{\cap}\},\ c\in\{1,\ldots,d_{1}\})\] \[T^{\text{out}}_{a,b,a}=\begin{cases}\sum_{c=1}^{d_{1}}T^{\text{ in}}_{c,b,b}&b\leq d^{\cap}\\ 0&\text{otherwise},\\ \end{cases}\quad(a\in\{1,\ldots,d_{2}\},\ c\in\{1,\ldots,d_{1}\})\]

## Appendix F Group Representation Theory Background

Here, we explain in detail the decomposition of \(k\)-th order permutation to _irreps_ of \(S_{m}\).

**Proposition 2**.: _Let \(\rho_{\lambda},\lambda\vdash m\) be the irreps of \(S_{m}\), \(U=R^{m^{k}}\), \(\sigma\in S_{m}\) act on \(U\) in the manner of equation 7, and suppose this action contains irreps \(\rho_{\lambda_{1}},\ldots,\rho_{\lambda_{p}}\) with multiplicities \(\kappa_{1},\ldots,\kappa_{p}\), we have:_

\[U=U_{1}\oplus U_{2}\oplus\cdots\oplus U_{p},\quad U_{i}=\bigoplus_{j}^{\kappa _{i}}V_{i}^{j}\]

_where the first part gives the canonical decomposition of \(U\) and the second part further decompose each \(U_{i}\) into irreducible subspaces, where \(V_{i}^{j}\) correspond to \(\rho_{\lambda_{i}}\). In matrix form,_

\[P_{\sigma}^{(k)}=M\begin{pmatrix}\rho_{\lambda_{1}}(\sigma)&0&\cdots&0\\ 0&\rho_{\lambda_{2}}(\sigma)&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&\rho_{\lambda_{p}}(\sigma)\end{pmatrix}M^{T},\quad\text{for all }\sigma\in S_{m}\]

_where \(M\) is the orthogonal transformation for basis and we abuse \(\rho_{\lambda_{i}}(\sigma)\) to denote also the matrix of i-th irrep in the decomposition._

If we take a closer look at \(M\), we can find that since \(P_{\sigma}^{(k)}\) is under the standard basis of \(R^{m^{k}}\), thus \(M\) is just the orthonormal basis of each \(U_{i}\) (thus each \(V_{i}^{j}\)) combined together, we denote \(M=(M_{1},\ldots,M_{p})\) with \(M_{i}\) a \(m^{k}\times(d_{\lambda_{i}}*\kappa_{i})\) dimensional matrix, where \(d_{\lambda_{i}}\) is the degree of \(\rho_{\lambda_{i}}\). Therefore, \(P_{\sigma}^{(k)}T\) becomes:

\[P_{\sigma}^{(k)}T=[M_{1},M_{2},\ldots,M_{p}]\underbrace{\begin{pmatrix}\rho_ {\lambda_{1}}(\sigma)&0&\cdots&0\\ 0&\rho_{\lambda_{2}}(\sigma)&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&\rho_{\lambda_{p}}(\sigma)\end{pmatrix}}_{2}\underbrace{\begin{pmatrix}M _{1}^{T}\\ M_{2}^{T}\\ \vdots\\ M_{p}^{T}\end{pmatrix}}_{(1)}T\] (10)

The part (1) is a generalized Fourier transform of \(T\) to its Fourier components (coordinates under the orthonormal basis) \(\hat{T}=\begin{pmatrix}M_{1}^{T}T\\ M_{2}^{T}T\\ \vdots\\ M_{p}^{T}T\end{pmatrix}\triangleq\begin{pmatrix}B_{1}\\ B_{2}\\ \vdots\\ B_{p}\end{pmatrix}\), and the part (2) is the _irreps_\(\rho_{\lambda_{1}},\ldots,\rho_{\lambda_{p}}\) act independently on \(\hat{T}\) with each component \(B_{i}\mapsto\begin{pmatrix}\rho_{\lambda_{i}}(\sigma)&\cdots&0\\ \vdots&\ddots&\vdots\\ 0&\cdots&\rho_{\lambda_{i}}(\sigma)\end{pmatrix}B_{i}\). Note that there're \(\kappa_{i}\) multiple of \(\rho_{\lambda_{i}}\) act the same on components of \(B_{i}\) correspond to \(V_{i}^{j}\), with a slight abuse of notation, we can think \(B_{i}\in R^{d_{\lambda_{i}}\times\kappa_{i}}\) (instead of \(R^{(d_{\lambda_{i}}*\kappa_{i})\times 1}\)) and write this map as \(\rho_{\lambda_{i}}(\sigma)B_{i}\), the fact that the map by _irrep_\(\rho_{\lambda_{i}}\) act independently on each column of \(B_{i}\) allow us to identify directly a set of equivariant maps by multiplying \(B_{i}\) with matrix \(W_{i}\in R^{\kappa_{i}\times\kappa_{i}}\) to the right, and using associativity of matrix multiplication: \(\rho_{\lambda_{i}}(\sigma)(B_{i}W_{i})=(\rho_{\lambda_{i}}(\sigma)B_{i})W_{i}\). This gives in total of \(\sum_{i}\kappa_{i}^{2}\) independent equivariant maps, as stated in the main text. The last part of the above equation maps Fourier components to their original space. In short, we can write:

\[P_{\sigma}^{(k)}T=\sum_{i}M_{i}\rho_{\lambda_{i}}(\sigma)\underbrace{M_{i}^{T }T}_{B_{i}}\] (11)with the abuse of notation to rearrange elements in \(B_{i}\) mentioned above.

Then we present a detailed version of theorem 2 in the main text.

**Theorem 4** (Necessary and sufficient condition for equivariant map).: _Let \(G\) be a finite group acting on a vector space \(U\) by the linear action \(\{g:U\to U\}_{g\in G}\) and assume the action can be decomposed into irreps \(\rho_{1},\ldots,\rho_{p}\) with multiplicities \(\kappa_{1},\ldots,\kappa_{p}\) and degree \(d_{i}\):_

\[U=U_{1}\oplus U_{2}\oplus\cdots\oplus U_{p},\quad U_{i}=\bigoplus_{j=1}^{ \kappa_{i}}V_{i}^{j}\]

_Then \(\phi:U\to U\) is an equivariant map w.r.t. this group action if and only if \(\phi\) is of the form:_

\[\phi(v)=\sum_{j^{\prime}}\alpha_{j,j^{\prime}}^{i}\tau_{j\to j^{\prime}}^{i}(v) \quad\text{for }v\in V_{j}^{i}\] (12)

_for some fixed set of coefficient \(\{\alpha_{j,j^{\prime}}^{i}:i\in[1,\ldots,p],j,j^{\prime}\in[1,\ldots,\kappa_ {i}]\}\). In matrix form, suppose the matrix of \(g\) is \(R_{g}\) under basis \((e_{i})\) and \(dim(U)=n\), and it decompose into:_

\[R_{g} =M\begin{pmatrix}\rho_{1}(g)&0&\cdots&0\\ 0&\rho_{2}(g)&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&\rho_{p}(g)\end{pmatrix}M^{T}\] (13) \[=\sum_{i}M_{i}\rho_{i}(g)M_{i}^{T}\] (14)

_Then \(\phi:R^{n}\to R^{n}\) is equivariant if and only if it is of the form:_

\[\phi(T) =MM^{T}T\begin{pmatrix}W_{1}&0&\cdots&0\\ 0&W_{2}&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&W_{p}\end{pmatrix}\] (15) \[=\sum_{i}M_{i}\underbrace{M_{i}^{T}T}_{B_{i}}W_{i}\] (16)

_where \(T\in R^{n}\), \(W_{i}\in R^{\kappa_{i}\times\kappa_{i}}\) is the fixed coefficients and \(B_{i}\) rearrange to \(R^{d_{i}\times\kappa_{i}}\) when needed. The coefficients \(\alpha_{j,j^{\prime}}^{i}\) corresponds to \((W_{i})_{j,j^{\prime}}\). 3_

Footnote 3: Similar discussions could be found in Section 4 of [38], but without a proof for necessity.

Proof.: **Sufficiency:** firstly, \(\alpha_{j,j^{\prime}}^{i}\tau_{j\to j^{\prime}}^{i}(v)\) is equivariant by definition of isomorphism map. And the equivariance of \(\phi\) follows from the fact that sum of equivariant maps are equivariant.

**Necessity:** consider \(\phi\) on \(V_{j}^{i}\) and let \(W=Im(\phi|_{V_{j}^{i}})\) the image of \(\phi\) on \(V_{j}^{i}\). Since \(\phi\circ g=g\circ\phi\) and \(g(v)\in V_{j}^{i}\) for any \(v\in V_{j}^{i}\), we have \(g(\phi(v))=\phi(g(v))\in W\), so \(W\) is stable under the action. Thus we can decompose \(W\) into irreducible spaces \(W=W_{1}\oplus\ldots W_{p}\). Apply this decomposition to \(\phi(v)\) for \(v\in V_{j}^{i}\), we get:

\[\phi(v)=\phi_{1}(v)+\ldots+\phi_{p}(v)\quad\text{where }\phi_{i}(v)\in W_{i}\]

In other words \(\phi_{k}=Proj_{k}\circ\phi\) where \({}_{k}\) is the projection of \(W\) onto \(W_{k}\) (uniquely defined by the direct sum decomposition). Then \(\phi_{k}:V_{j}^{i}\to W_{k}\) and \(\phi_{k}\circ g=g\circ\phi_{k}\). By Schur's lemma [36], for \(\phi_{k}\neq 0\), we must have \(W_{k}\) isomorphic to \(V_{j}^{i}\) and \(\phi_{k}(v)=\theta_{k}\tau_{V_{j}^{i},W_{k}}(v)\). Since only \(V_{j^{\prime}}^{i}\) is isomorphic to \(V_{j}^{i}\), we have \(W_{k}=V_{k}^{i}\) and \(\phi_{k}(v)=\alpha_{j,k}^{i}\tau_{j,k}(v)\), where \(\tau_{j,k}(v)\) is the isomorphic map sending \(V_{j}^{i}\) to \(V_{k}^{i}\). Thus \(\phi(v)=\sum_{k}\phi_{k}(v)=\sum_{k}\alpha_{j,k}^{i}\tau_{j,k}(v)\) for \(v\in V_{j}^{i}\).

Finally, by linearity and \(U=\bigoplus_{i,j}V_{j}^{i}\), the only possible equivariant function \(\phi:U\to U\) is given by 12.

**Connection to matrix form:** first note that the isomorphism \(\tau^{i}_{j\to j^{\prime}}:V^{i}_{j}\to V^{i}_{j^{\prime}}\) is given by:

\[\tau^{i}_{j\to j^{\prime}}(u^{i}_{j,l})=u^{i}_{j^{\prime},l}\]

where \(\{u^{i}_{j,l},l=1,\ldots,\kappa_{i}\}\) and \(\{u^{i}_{j^{\prime},l},l=1,\ldots,\kappa_{i}\}\) are orthonormal basis for \(V^{i}_{j}\) and \(V^{i}_{j^{\prime}}\) respectively such that the action of \(g\) is associated with matrix \(\rho_{i}(g)\). Therefore,

\[\phi(u^{i}_{j,l}) =\sum_{j^{\prime}}\alpha^{i}_{j,j^{\prime}}\tau^{i}_{j\to j^{ \prime}}(u^{i}_{j,l})\] \[=\sum_{j^{\prime}}\alpha^{i}_{j,j^{\prime}}(u^{i}_{j^{\prime},l})\]

Thus the matrix of \(\phi\) under basis \((u^{i}_{j,l},i=1,\ldots,p,j=1,\ldots,\kappa_{i},l=1,\ldots,d_{i})\) is \(\begin{pmatrix}W_{1}&&&&\\ &\ddots&&\\ &&W_{2}&&\\ &&&\ddots&\\ &&&&W_{p}&\\ &&&&\ddots\end{pmatrix}\) (which is the same as \(\begin{pmatrix}W_{1}&0&\cdots&0\\ 0&W_{2}&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&W_{p}\end{pmatrix}\) with \(B_{i}\) rearranged to \(R^{d_{i}\times\kappa_{i}}\)) and \(M\) is just the matrix to transform standard basis \((e_{i})\) to basis \((u^{i}_{j,l})\).

## Appendix G Ways to Use Schur Neuron

While Schur neuron can be viewed as a generic transform on any subgraph's first-order activation, we provide some thoughts and possibilities to instantiate it.

In our experiment, we primarily view Schur neuron as a way to expand the feature of a subgraph. It can be viewed as an analogue to CNN's convolution filter and it's indeed a constrained version of spectral convolution filter [5] on the subgraph. In light of this, we call number of channels of Schur neuron as how many times it expand the output feature versus the input feature.

Besides, we observe that the linear equivariant map described [32] is a special case of Schur neuron's map. The two possible linear map in [32] is (1) identity map, corresponding to take all weights \(W_{i}=1\) (2) \(T\rightarrow\Sigma_{i}T_{i}1\), where \(\mathbb{1}\) is all ones vector, corresponding to set \(W_{1}=1\), and \(W_{i}=0\) for \(i\neq 1\) since \(\mathbb{1}\) is eigenvector of any graph Laplacian with eigenvalue \(0\). Consequently, we suggest to keep the two basic but important linear maps and augment them with Schur neuron. Empirically we verified this by an experiment only vary the number of channels. In figure 1, we see that further increase the number of channels beyond 4 wouldn't give us performance gain since cycle 5 and 6 only has 3 and 4 distinct eigenvalues respectively.

Furthermore, we suggest that number of channels of Schur neuron be proportional or approximately equal to number of distinct eigenvalues of the subgraph it attach to. This is because the later is the number of independent linear maps for Schur neuron.

Additonally, we're wondering if we can use _Schur_ layer in place of MLP's to provide a local structure aware transform to subgraph activations. Therefore, we tried to replace the 2-layer MLP with 2-layer

Figure 1: A study on num of channels in Schur layer. Cycle 5 and 6 are included.

_Schur_ layer in the original _Linmaps_ architecture. But it turns out this use case wasn't as effective as the use as learning new feature. Possibly because _Schur_ layer learns feature itself while MLP transform the learned feature to some desired space.

Finally, regarding flexibility as discussed 5.2, in table 7, we see that adding cycles with branches could provide the _Schur_ Net with more detailed topological information, thus improving the performance. And the code for adding this actually only requires a single definition of those templates without modification to the neural network.

## Appendix H More experiment results

We first start with some molecular datasets in TUdataset [33], which is a small but commonly used benchmark for GNN. In table 8, we see that _Schur_ layer consistently improves over _Linmaps_ in various bioinformatics and molecule datasets.

In table 9, we compare the runtime of our _Schur_ layer and _Linmaps_, which shows the extra computational cost of _Schur_ layer wasn't significant while being able to use more equivariant maps and achieving better accuracy.

Another ablation study is perform on ZINC-12K dataset. In table 10, we see that _Schur_ layer outperforms _Linmaps_ when certain cycle sizes are considered, especially when only cycles 5 and 6 are chosen as subgraphs in the neural network.

\begin{table}
\begin{tabular}{|c|c|} \hline Methods & Validation MAE \\ \hline _Linmaps_ & 0.087 \\ \hline _Schur_ layer & \\ (in place of MLP) & 0.081 \\ \hline _Schur_ layer & \\ (as learning new feature) & 0.076 \\ \hline \end{tabular}
\end{table}
Table 6: Comparison about two use cases of _Schur_ layer. Experiment on ZINC-12K.

\begin{table}
\begin{tabular}{|c|c|} \hline Model & Validation MAE \\ \hline _Schur_-Net on 5,6 cycles(baseline) & \(0.113\pm 0.008\) \\ _Schur_-Net on 5,6 cycles with up to three branches & \(\mathbf{0.105\pm 0.001}\) \\ \hline PTC\_MR & 61.1 \(\pm\) 6.9 & \(\mathbf{64.6\pm 5.9}\) \\ \hline NCI1 & 82.1 \(\pm\) 1.8 & \(\mathbf{82.7\pm 1.9}\) \\ \hline \end{tabular}
\end{table}
Table 7: Flexibility of _Schur_ layer. Experiments on ZINC-12k dataset. All other settings are the same. A smaller network than previous experiments was used.

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Dataset** & _Linmaps_ & _Schur_ Layer \\ \hline Zinc-12k & 25.4s & 27.6s \\ \hline NCI1 & 9.5s & 11.5s \\ \hline \end{tabular}
\end{table}
Table 9: Runtime per epoch with hyper-params num layers = 4, rep dim = 128, dropout = 0.0, batch_size = 256, num of channels = 4, cycle_sizes = 3,4,5,6,7,8. This shows _Schur_ Layer didn’t add much computational costs to Linmaps while being more expressive.

## Appendix I Architecture and experiment details

In designing of our architecture, we design a message passing scheme similar to [21] and add Schur layer to each of the convolution layers. A visualization of our architecture can be found in Figure 2. In each convolution layer, we maintain 0th-order node representation (\(h_{v}\in R^{|V|*dim}\)) and edge representation (\(h_{e}\in R^{|E|*dim}\)) where we assume the input graph \(G=(V,E)\). We further maintain 1st-order representation on cycles (\(h_{D_{k}}\in R^{(k*(c_{k})*dim)}\)) where \(D_{k}\) is cycle of length k and \(c_{k}\) is the number of such cycle in \(G\). We did message passing between node and edge representation the same as CIN [6]. The edge and cycle pass message with each other by tranfer maps described by [21] and then the incoming message to cycles as well as the original cycle representation is transformed by Schur layer. When updating the edge representation and cycle representation, we combine the original representation with the incoming message by either concatenation or \(\epsilon\)-add described in GIN [40] and feed it into an MLP to get new representations.

\begin{table}
\begin{tabular}{|c|c|c|} \hline Layer & cycle size \{5,6\} & cycle size \{3,4,5,6\} \\ \hline _Limmaps_ (baseline) & \(0.139\pm 0.007\) & \(0.103\pm 0.011\) \\ \hline _Schur_ layer & \(\mathbf{0.114\pm 0.014}\) & \(\mathbf{0.100\pm 0.009}\) \\ \hline \end{tabular}
\end{table}
Table 10: Comparison between _Schur_ Layer and _Limmaps_ with different set of cycles chosen. Experiments on ZINC-12k dataset and all scores are test MAE.

Figure 2: An example illustration of one layer of our model architecture. For an input graph, we first execute the standard node and edge level message passing and lift up the model to higher-order by forming higher-order representations on specific template graphs. In this example, we learned the representation on a Naphthalene. To further decouple its information, we may apply the SchurLayer on the template graphs that would potentially provide us insight about the graph. In this example, the 6 cycle, star graph, and path graph are all applying SchuLayer to learn the features locally. Through our architecture design, we can also incorporate the peripheral information. Automorphism group then helps us discover distinct key features such as non-symmetries, and we pass these aggregated features into the next layer of the architecture.

HyperparametersFor experiments on ZINC-12K and OGB-HIV, we tune representation dimension in \(\{32,64,128\}\), and experiment on cycles up to length 18. In the best-performing model, we use representation dimension \(128\) and cycle lengths from \(3\) to \(8\) and number-of-layers is always \(4\). For _Schur_ layer, we tuned number of channels from 2 to \(8\) and found \(2,4\) are a suitable choice when it is used as an augmentation to _Limmaps_. For MLPs, we either use 2 to 3 layers depending on how dense the input's information is. We always use a batchnorm [25] after the Linear layer and then do ReLu activation. For training hyperparameters, we use an init learning rate of \(0.01\) and use ReduceLROnPlateau scheduler in PyTorch with patience of 10 or 30. We use Adam optimizer [27] for all trainings. We train for 500 or 1000 epochs depending on model size. In particular, in table 10, the models have representation dimension 32, so it is trained to only 500 epochs. All other models are trained 1000 epochs. A batch size of 256 is used for all models. All results are run at least three times to calculate the standard deviation.

For experiments on TUdataset (table 8), we chose a set of hyper-params by intuition (we didn't tune them because this experiment is to demonstrate the effectiveness of _Schur_ layer and compare _Schur_ layer with _Limmaps_ under the same experiment setting. we don't aim to compare with Sota on this experiment). Hyper-params for both _Schur_ Layer and _Limmaps_: num_layers = 4, rep_dim = 32,64, dropout = 0.5, batch_size = 32,128, lr = 0.01, num of channels = {2,4}, cycle sizes = 3,4,5,6. We're training with StepLR scheduler where reduce lr to 1/2 after every 50 epochs, with a total of 300 epochs. The hyperparams weren't tuned, we just chose a smaller value for a smaller dataset and a bigger value for bigger datasets by rule-of-thumb. Linmaps is implemented on our own according to the description of \(P\)-tensor, then _Schur_ Layer replaces Linmaps operation by _Schur_ operation. We follow the evaluation strategy specified in [40].

Used Compute ResourcesExperiments are all run on one Tesla L4 from PyTorch Lightning Workspace and one NVIDIA RTX 4090. The code is implemented in PyTorch. For the running time, on the ZINC-12k dataset, it takes around \(8.53\pm 1.2\) hours to finish training for 1000 epochs with 4 layers and 128 dimension representation on an NVIDIA RTX 4090. We're running on a desktop with a Ryzen 7900 CPU and 64GB of RAM.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims made in the paper relate to specific properties of message passing algorithms. The claims that we make about our own algorithm are accurate.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes]. Justification: We have a section on limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: For each Lemma and Theorem in the paper we provide a proof.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We document our experiments in detail, including the dataset, the architecture and parameters.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have included the link to a github repository containing all the files needed to reproduce the experiments.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, we give all these details in the Appendix.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For each of our results on the benchmarks datasets we provide the standard deviation of the performance.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: We provide this information in the Appendix.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the code of ethics and our research conforms to it.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]. Justification: Our work is foundational/theoretical in nature and we not foresee it having any direct social impacts. Indirectly, it could have impact because higher order automorphism-equivariant could be used in drug discovery, for example. Applications with negative social impacts are also possible, but a little more difficult to pinpoint. Space limitations prevented us from discussing this topic in more detail.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: The model itself does not pose such a risk.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We provide a citation for every baseline model and benchmark dataset that we use. All assets were used in a way that conforms to their published licenses.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: We do not provide new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: This research did not involve crowdsourcing or human subjects research.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: This research did not involve crowdsourcing or human subjects research.