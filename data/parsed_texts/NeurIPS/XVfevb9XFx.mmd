# Optimistic Critic Reconstruction and Constrained Fine-Tuning for General Offline-to-Online RL

 Qin-Wen Luo1, Ming-Kun Xie1,2, Ye-Wen Wang1, Sheng-Jun Huang1

1 Nanjing University of Aeronautics and Astronautics, Nanjing, China

2 RIKEN Center for Advanced Intelligence Project, Tokyo, Japan

{luoqw8,linuswangg,huangsj}@nuaa.edu.cn

ming-kun.xie@riken.jp

The authors contribute equally.Correspondence to: Sheng-Jun Huang (huangsj@nuaa.edu.cn).

###### Abstract

Offline-to-online (O2O) reinforcement learning (RL) provides an effective means of leveraging an offline pre-trained policy as initialization to improve performance rapidly with limited online interactions. Recent studies often design fine-tuning strategies for a specific offline RL method and cannot perform general O2O learning from any offline method. To deal with this problem, we disclose that there are evaluation and improvement mismatches between the offline dataset and the online environment, which hinders the direct application of pre-trained policies to online fine-tuning. In this paper, we propose to handle these two mismatches simultaneously, which aims to achieve general O2O learning from any offline method to any online method. Before online fine-tuning, we re-evaluate the pessimistic critic trained on the offline dataset in an optimistic way and then calibrate the misaligned critic with the reliable offline actor to avoid erroneous update. After obtaining an optimistic and and aligned critic, we perform constrained fine-tuning to combat distribution shift during online learning. We show empirically that the proposed method can achieve stable and efficient performance improvement on multiple simulated tasks when compared to the state-of-the-art methods. The implementation is available at https://github.com/QinwenLuo/OCR-CFT.

## 1 Introduction

Offline reinforcement learning (RL) aims to learn a policy from a fixed dataset without additional interactions with the environment. This characteristic makes it particularly promising for critical applications such as healthcare decision-making [39], human-AI coordination [14] and autonomous driving [8]. Generally, the performance of the learned policy relies on the quality of the dataset. Given that the offline data is limited, fine-tuning the policy through interactions with the environment is still necessary to achieve favorable performance. Consequently, offline-to-online (O2O) RL tends to achieve faster performance improvements based on better initializations.

To effectively fine-tune offline policies, O2O methods are typically designed based on specific offline RL algorithms. Existing methods can be roughly divided into two groups according to the base offline methods they use. The first group relies on policy constraint methods. These approaches aim to improve online performance by either adaptively adjusting constraints [5, 52, 43] or directly applying offline algorithms to online fine-tuning [32, 20]. Unfortunately, these methods often suffer from inefficient performance improvement due to restricted action exploration caused by policy constraints. The second group builds on value regularization methods. These methods aim to prevent excessivelylow Q-values resulting from pessimistic evaluations, such as those in CQL [21]. The goal is to enhance the generalization of the value function and mitigate potential performance declines [27; 33]. Some other methods adopt ensemble Q-learning [51; 23; 30; 19] address these issues. However, these methods often face high computational costs due to the need to train multiple Q-networks.

Considering that the aforementioned methods develop online fine-tuning algorithms based on specific offline methods, they often struggle to be applied to other offline methods. To establish a general O2O framework, it is essential to address the core issues associated with transitioning from offline to online environments. Inspired of the recent work [48], which highlights the misalignment between the actor and the critic in an explicit policy constraint method, we identify two mismatches in general O2O RL: evaluation mismatches and improvement mismatches. Evaluation mismatches primarily occur in value regularization methods. These refer to the differences in policy evaluation methods between online and offline settings, which cause severe fluctuations in Q-value estimation during the initial stages of fine-tuning. Improvement mismatches, on the other hand, are prevalent in policy constraint methods. They are often caused by differences in objectives for updating the policy, leading to a misalignment between the probabilities of actions and their Q-values. Thanks to another recent work by Xu et al [47], which connects value regularization and policy constraint methods, we bridge these two types of mismatches within a unified framework for general RL-based offline algorithms.

In this paper, we propose a general O2O framework designed to address both evaluation and improvement mismatches simultaneously, aiming for stable and favorable fine-tuning performance from any offline method to representative online methods. To address the evaluation mismatch in value regularization methods, we propose re-evaluating the offline policy in an optimistic manner using an off-policy evaluation method. This approach allows us to obtain optimistic Q-value estimates, preventing the dramatic fluctuations in Q-values that could potentially cause the policy to collapse. Although the re-evaluated critic can estimate Q-values optimistically, it suffers from the misalignment with the offline policy, causing the improvement mismatch. To handle the improvement mismatch in the re-evaluated critics and policy constraint methods, we introduce value alignment to calibrate the critic so that it aligns with the probabilities predicted by the policy. Our approach involves using the Q-value of the most likely action as an anchor and then calibrating other Q-values by either exploiting the correlation between Q-values of different state-action pairs or modeling Q-values as a Gaussian distribution. Finally, we propose a constrained fine-tuning framework to guide the policy update by adding a regularization term, with the target of mitigating the negative impact of data shift. Extensive experimental results on multiple benchmark environments validate that the proposed methods can achieve better or comparable performance when compared to state-of-the-art methods.

Our contributions can be summarized as follows:

* We systematically study that there exist evaluation and improvement mismatches for offline RL methods from the perspective of online RL. We show that resolving these two types of mismatches is essential for achieving general O2O RL.
* We develop two techniques to address these mismatches. Policy re-evaluation aims to achieve optimistic Q-value estimates, preventing instability in Q-value estimation. Value alignment calibrates the critic to align with the policy, ensuring consistency between action probabilities and their corresponding Q-values.
* We introduce a constrained fine-tuning framework that incorporates a regularization term into the policy objective, combating the inevitable distribution shift and ensuring stable and optimal performance when fine-tuning the policy in online environments.

## 2 Preliminaries

In this section, we introduce necessary preliminaries about RL, including Markov decision process, and three target online RL methods.

Markov Decision Process (MDP)A Markov Decision Process \(\mathcal{M}\) is defined by the tuple (_S,A,R,P,\(\mu\),\(\gamma\)_) [38], where \(S\) is the state space, \(A\) is the action space, \(P:S\times A\rightarrow\Delta(S)\) is the transition function, \(R:S\times A\rightarrow\mathbb{R}\) is the reward function, \(\mu\) is the initial state distribution, and \(\gamma\) is a discount factor. The goal is to learn a policy that maximizes the expected return as

\[J\left(\pi\right)=\mathbb{E}_{\pi}[\sum\nolimits_{t=0}^{\infty}\gamma^{t}r_{ t}]=\mathbb{E}_{\delta(s_{0},a_{0})}\left[Q^{\pi}\left(s_{0},a_{0}\right) \right],\delta(s_{0},a_{0})\coloneqq(s_{0}\sim\mu,a_{0}\sim\pi\left(\cdot|s_{0 }\right)).\] (1)

**Online RL** The three most commonly used online RL algorithms are SAC [17], TD3 [11], and PPO [37]. Below, we will introduce these three methods one by one.

**SAC** first introduces entropy maximization into the RL scenarios and updates the actor and the critic by minimizing the following objectives:

\[\mathcal{L}_{\pi}^{\mathrm{SAC}}\left(\theta\right)=\mathop{ \mathbb{E}}_{s\sim R}\mathop{\mathbb{E}}_{\omega\sim\pi_{\theta}\left(\cdot|s \right)}\left[\alpha\log\pi_{\theta}\left(a|s\right)-Q_{\mu}\left(s,a\right) \right],\] (2) \[\mathcal{L}_{Q}^{\mathrm{SAC}}\left(\mu_{i}\right)=\mathop{ \mathbb{E}}_{\left(s,a,r,s^{\prime}\right)\sim R}\left[\left(Q_{\mu_{i}}\left(s,a\right)-y\left(r,s^{\prime}\right)\right)^{2}\right],\] (3) \[y\left(r,s^{\prime}\right)=r+\gamma\mathbb{E}_{a^{\prime}\sim \pi_{\theta}\left(\cdot|s^{\prime}\right)}\left[\min_{i=1,2}Q_{\bar{\mu}_{i}} \left(s,a\right)-\alpha\log\pi_{\theta}\left(a^{\prime}|s^{\prime}\right) \right],\] \[\mathcal{L}_{\alpha}^{\mathrm{SAC}}\left(\alpha\right)=-\alpha \mathop{\mathbb{E}}_{s\sim R}\mathop{\mathbb{E}}_{\omega\sim\pi_{\theta} \left(\cdot|s\right)}\left[\log\pi_{\theta}\left(a|s\right)-\bar{\mathcal{H}} \right],\] (4)

where \(\alpha>0\), \(\bar{\mu}_{i}\) are the parameters of the target \(Q\) network, and \(\bar{\mathcal{H}}\) is the target entropy.

**TD3** models a deterministic policy and uses tricks, including clipped double Q-learning and policy smoothing, to address the issue of function approximation error. The deterministic policy gradient used to update policy is defined as

\[\nabla_{\theta}J_{\pi}^{\mathrm{TD3}}\left(\theta\right)=\mathop{ \mathbb{E}}_{s\sim R}\left[\nabla_{a}Q_{\mu}^{\pi}\left(s,a\right)|_{a=\pi \left(s\right)}\nabla_{\theta}\pi_{\theta}\left(s\right)\right].\] (5)

The objective function of updating the critic is defined as

\[\mathcal{L}_{Q}^{\mathrm{TD3}}\left(\mu_{i}\right)=\mathop{ \mathbb{E}}_{\left(s,a,r,s^{\prime}\right)\sim R}\left[\left(Q_{\mu_{i}}\left( s,a\right)-y\left(r,s^{\prime}\right)\right)^{2}\right].\] (6)

Here, \(y\left(r,s^{\prime}\right)=r+\gamma\min_{i=1,2}Q_{\bar{\mu}_{i}}\left(s^{ \prime},\tilde{a}\right)\), where \(\tilde{a}=\pi_{\bar{\theta}}\left(s^{\prime}\right)+\epsilon\) and \(\epsilon\sim\mathrm{clip}\left(N\left(0,\sigma\right),-c,c\right)\).

**PPO** provides a simple implementation for TRPO [35], which updates the actor and the critic by minimizing the following objective functions, respectively,

\[\mathcal{L}_{\pi}^{\mathrm{PPO}}\left(\theta\right)=-\mathop{ \mathbb{E}}_{s\sim\nu^{\pi_{\theta_{k}}}}\mathop{\mathbb{E}}_{a\sim\pi_{ \theta_{k}}\left(\cdot|s\right)}\Biggl{[}\min\Biggl{(}r(\theta)A^{\pi_{\theta _{k}}}\left(s,a\right),\mathrm{clip}\left(r(\theta),1-\epsilon,1+\epsilon \right)A^{\pi_{\theta_{k}}}\left(s,a\right)\Biggr{)}\Biggr{]},\] (7) \[\mathcal{L}_{V}^{\mathrm{PPO}}\left(\nu\right)=\mathop{\mathbb{E}} _{\left(s,r,s^{\prime}\right)\sim R}\left[\left(A^{\pi_{\theta_{k}}}\left(s,a \right)+V_{\bar{\nu}}\left(s^{\prime}\right)\right)-V_{\nu}\left(s\right) \right)^{2}],\] (8)

where \(r(\theta)=\pi_{\theta}/\pi_{\theta_{k}}\) and the advantage \(A^{\pi_{\theta_{k}}}\) is computed by GAE [36].

It is noteworthy that previous works mostly adopted SAC and TD3 for online fine-tuning as they are off-policy methods. In our work, given the widespread use of PPO in online RL, we also employ it for fine-tuning though it is an on-policy method.

## 3 Evaluation and Improvement Mismatches

In this section, we focus on the differences between online and offline RL and study how these differences impact the performance of online fine-tuning. In general, we summarize these differences as two types of mismatches, _evaluation mismatch_ and _improvement mismatch_. The former arises from the changes in policy evaluation functions during the transition from offline to online environments; while the latter represents the inconsistency in the objectives for policy updates between offline and online RL. Most offline RL methods suffer from one or both of these issues, which underscores the importance of addressing these mismatches to achieve stable and effective online fine-tuning.

Evaluation mismatch often occurs in the value regularization methods. For example, in CQL [21], a representative offline method, the policy evaluation function transitions from a pessimistic estimation inherent to offline learning to a more optimistic estimation during online training. This shift frequently results in a sharp increase in Q-values at the beginning of online fine-tuning, which can hinder stable performance improvements. To address this problem, several attempts have been made to mitigate the excessive underestimation of out-of-the-distribution (OOD) actions during offline learning [33; 27]or to initialize a pessimistic Q-ensemble to maintain pessimism during online fine-tuning [23]. Unfortunately, these approaches are predominantly tailored to the transition from CQL to SAC, limiting their applicability to other offline algorithms.

Improvement mismatch is commonly found in policy constraint methods. Typical examples include TD3+BC [10] and AWAC [32], where the objective of actor updates differs significantly from typical online methods. In these models, updates to the actor are not solely reliant on the critic's evaluation. Consequently, actions that have high Q-values may not automatically translate to high probabilities of being selected, and vice versa. This divergence often misguides the update of policy at the beginning stage of online fine-tuning, resulting in unfavourable performance.

Besides, One-step RL and non-iterative methods, _e.g._, IQL, exhibit both evaluation and improvement mismatches. During the policy evaluation stage, these methods estimate the Q-values based on the behavior policy or an unknown policy [18] instead of the target policy as in online methods; during the improvement stage, because they impose additional constraints on policy updates, they also encounter the same issue as discussed above. This can lead to discrepancies in both the assessment of action values and the subsequent policy optimization, making it hard to achieve effective policy improvement.

Thanks to the recent work [47], which presents a unified framework for understanding offline RL, we bridge these two mismatches for general RL-based offline algorithms. Formally, the offline RL problem can be defined by the _behavior-regularized_ MDP problem [47; 13] via maximizing the following objective:

\[\mathbb{E}_{\pi}\left[\sum\nolimits_{t=0}^{\infty}\gamma^{t}\left(r(s_{t},a_ {t})-\alpha\cdot f\left(\frac{\pi(a_{t}|s_{t})}{\mu(a_{t}|s_{t})}\right)\right) \right]=\mathbb{E}_{\delta(s_{0},a_{0})}\left[Q^{\pi}\left(s_{0},a_{0}\right) -\alpha\cdot f\left(\frac{\pi\left(a_{0}|s_{0}\right)}{\mu\left(a_{0}|s_{0} \right)}\right)\right],\] (9)

where \(f(\cdot)\) is a regularization function and \(\mu\) is the behavior policy.

From Eq. (1) and Eq. (9), we observe a significant divergence in the relation between the actor and the critic in online versus offline scenarios. Unlike in online cases where the policy update is solely dependent on the Q-function, in offline scenarios, it also critically depends on the data distribution, as indicated by the regularization term in Eq. (9). This distinction highlights why offline methods often face evaluation and improvement mismatches when applied to online fine-tuning. Using offline actor and critic trained by Eq. (9) for initialization in online fine-tuning introduces both types of mismatches, resulting in unstable and inefficient updates.

## 4 Method

In this section, we introduce our proposed O2O method for handling the mismatches discussed in Section 3 and the distribution shift problem. To address the pessimistic or unreliable evaluation, such as in CQL and IQL, we develop a policy re-evaluation technique. This technique optimistically re-evaluates the well-trained offline policy using an off-policy evaluation method. Although this re-evaluation helps the critic achieve more optimistic Q-value estimates, unavoidable factors such as function approximation errors and partial data coverage can still lead to a misalignment between the critic and the offline policy. This misalignment means that the action with the highest probability predicted by the policy does not necessarily have the highest Q-value, leading to what we have termed improvement mismatch.

As discussed in in Section 3, both the value regularization (after re-evaluation) and policy constraint methods exhibit improvement mismatch, though the reasons for the mismatch differ between these two approaches. To address this issue, we propose value alignment, which aims to align the critic's estimates with the policy's action probabilities, effectively tackling the improvement mismatch in both types of methods. Finally, to deal with the inevitable distribution shift between offline and online environments, we develop a constrained fine-tuning framework. This framework ensures that the policy consistently updates in the optimal direction by incorporating a regularization term into the policy objective.

We propose methods for various representative online RL algorithms within a unified framework, including SAC [17], TD3 [11], and PPO [37]. These algorithms represent the mainstream approaches in online RL, and are targeted respectively for the off-policy approach with stochastic policies, the off-policy approach with deterministic policies, and the on-policy approach.

### Policy Re-evaluation

The critic trained on an offline dataset typically maintains pessimistic estimates of Q-values. When using online evaluation methods to fine-tune this critic without any value regularization, the Q-values can experience a dramatic jump, especially for OOD actions, leading to inaccurate Q-value estimations. To mitigate this problem, we propose re-evaluating the offline policy to acquire a new critic by employing an off-policy evaluation (OPE) method. The goal is to enable the critic to have optimistic estimates of Q-values that more closely approximate the true values.

However, directly applying OPE methods for policy evaluation on offline datasets often leads to large extrapolation errors, as discussed in the previous work [12]. These errors arise due to absent data and training mismatches. Thanks to the pessimism in offline RL, it is reasonable to assume that a well-trained policy is close enough to the behavior policy or even captures the support set of the behavior policy. A common assumption is single-policy concentrability [45; 34], which demonstrates how concentrated a learned policy is within the given dataset and can be defined as follows.

**Assumption 4.1**.: _(\(\pi_{\theta}\)-concentrability [45]) The behavior policy \(\mu\) and learned policy \(\pi_{\theta}\) satisfy_

\[\max_{(s,a)\in S\times A}\frac{d^{\pi_{\theta}}\left(s,a\right)}{d^{\mu} \left(s,a\right)}\leq C.\]

where \(d^{\pi_{\theta}}\left(s,a\right)\) is the occupancy measure of \(\pi_{\theta}\) and \(C\) is a constant. Single-policy concentrability measures the degree to which the state-action distribution induced by the learned policy is covered by the dataset used for training. By ensuring that the learned policy does not deviate significantly from the behavior policy, the extrapolation error can be greatly reduced [22; 29]. When using a representative OPE method called fitted Q-evaluation (FQE), based on Assumption 4.1 and Theorem 4.2 in [22], the upper bound of extrapolation error can be obtained.

**Corollary 4.2**.: _Under Assumption 4.1, by denoting Q-value function class as \(\mathcal{F}\), for \(\delta\in(0,1)\), after \(K\) iterations of FQE on the dataset \(\mathcal{D}\), with probability \(1-\delta\), we have:_

\[|Q^{\pi}-\hat{Q}^{\pi}|\leq\frac{1-\gamma^{K}}{1-\gamma}\sqrt{C\epsilon}+ \gamma^{K}\bar{V},\text{ where }\epsilon:=\frac{22\bar{V}^{2}\log(|\mathcal{F}|/\delta)}{| \mathcal{D}|}+20d_{F}^{\pi}.\]

where \(d_{F}^{\pi}\) is inherent Bellman evaluation error (Definition 4.1 in [22]), and \(\bar{V}\) is the maximum of \(V\), which can be bounded by \(R_{max}/(1-\gamma)\).

With a powerful neural network and sufficient data, the inherent Bellman evaluation error could be tiny. Accordingly, with a large training step \(K\), the error will be bounded by an acceptable value. This implies that, given sufficient data, one can achieve a critic with optimistic property and minor extrapolation error through policy re-evaluation. In practical implementation, for off-policy methods SAC and TD3, we can directly use Eq. (3) and Eq. (6) to re-evaluate the offline policy. For the on-policy method PPO, we train a critic by fitting the returns of offline trajectories. Considering that the critic can only approximate \(V^{\mu}(s)\) rather than the true value function, we propose a regularization term in Section 4.2, which ensures the critic's estimates are reliable and conducive to effective policy improvement.

### Value Alignment

Although the critic after policy re-evaluation possesses the optimistic property needed in the online environment, it often does not align well with the offline policy due to factors such as function approximation errors, generalization errors of neural networks, and partial data coverage. Moreover, as discussed in Section 3, policy constraint offline methods also suffer from misalignment between the critic and policy. The misalignment means that the action with the highest Q-value does not necessarily have the highest probability, often leading to misleading updates of the policy. To verify this observation, we trained the policy using SAC and TD3

Figure 1: The results of actors updated with different critics.

with three different critics: the fixed re-evaluated critic, the iterative re-evaluated critic (which updates with the policy), and our aligned critic. From Figure 1, the performance of re-evaluated critics sharply declines at the initial stage and does not recover in the subsequent training; while our aligned critic achieves stable and favorable performance. These results indicates that the misalignment between the re-evaluated critic and the offline policy can make it difficult for the policy to optimize in a correct direction, leading to an irreversible decline in performance. Given that the well-trained offline policy is reliable, the desirable critic should not only have optimistic Q-value estimates but also maintain alignment with the offline policy. To achieve this, we propose performing value alignment to calibrate the critic. The main idea is to use the Q-values of the offline policy actions as anchors, keeping them unaltered, and to suppress any overestimated Q-value that exceed these anchors, thereby aligning the critic with the offline policy. Below, we will discuss how to implement value alignment for different online methods.

O2sacRecall that in SAC [16, 17], the optimal policy is defined as

\[\pi(a|s)=\exp(\frac{1}{\alpha}Q(s,a))/\exp(\frac{1}{\alpha}V(s))\] (10)

With a simple transformation of Eq. (10), we have

\[Q(s,a)=V(s)+\alpha\log\pi\left(a|s\right)\] (11)

One intuition behind our method is that actions with higher probabilities typically have more accurate Q-value estimates because these actions are closer to the dataset, and there is sufficient data nearby to obtain a precise estimate. This motivates us to use the Q-value \(Q_{\bar{\mu}}(s,\hat{a})\) of the optimal action \(\hat{a}\) to calibrate any other overestimated action. Assuming that \(\pi_{\text{off}}\) is the offline policy, we perform value alignment for any state-action pair \((s,a)\) as follows:

\[Q^{\prime}_{\mu}(s,a)=\min\left(Q_{\bar{\mu}}(s,\hat{a})-\alpha\left(\log\pi_ {\text{off}}\left(\hat{a}|s\right)-\log\pi_{\text{off}}\left(a|s\right)\right),Q_{\bar{\mu}}(s,a)\right),\] (12)

where \(\min(\cdot,\cdot)\) operator is used to maintain the Q-values of actions that are not overestimated and consistent with OPE results.

Formally, we define the objective function of value alignment as follows:

\[\mathcal{L}_{Q}^{\text{align}}\left(\mu_{i}\right) =\mathop{\mathbb{E}}_{s\sim R,a\sim\pi(\cdot|s)}\left[\left(Q_{ \mu_{i}}\left(s,a\right)-Q^{\prime}_{\mu}\left(s,a\right)\right)^{2}\right]\] (13) \[\mathcal{L}_{Q}^{\text{retain}}\left(\mu_{i}\right) =\mathop{\mathbb{E}}_{s\sim R,}\left[\left(Q_{\mu_{i}}\left(s, \hat{a}\right)-Q_{\bar{\mu}}\left(s,\hat{a}\right)\right)^{2}\left|_{\hat{a}= \pi_{\text{off}}(s)}\right]\] \[\mathcal{L}_{Q}^{\text{critic}}\left(\mu_{i}\right) =\mathcal{L}_{Q}^{\text{align}}\left(\mu_{i}\right)+\mathcal{L} _{Q}^{\text{retain}}\left(\mu_{i}\right)\]

Considering that the overall Q-values have decreased, to maintain an optimistic estimate, we use the regularization term \(\mathcal{L}_{Q}^{\text{retain}}\) to prevent the underestimation of Q-values.

We derive Proposition 4.3 to show that the aligned state values \(V_{\text{align}}(s)\) can be maintained within an appropriate range, meaning that the estimates remain optimistic while avoiding overestimation.

**Proposition 4.3**.: _After the value alignment process of Eq. (13), the state value function \(V_{\text{align}}(s)\) satisfies_

\[V_{\text{size}}(s)\leq V_{\text{align}}(s)\leq V_{\hat{a}}(s)\] (14)

_where \(Q_{\text{size}}(s,a)\) are the low Q-values after policy re-evaluation, which do not require calibration, \(V_{\text{size}}(s)\!=\!Q_{\text{size}}(s,a)\!-\!\alpha\log\pi\left(a|s\right)\) and \(V_{\hat{a}}(s)\!=\!Q(s,\hat{a})\!-\!\alpha\log\pi\left(\hat{a}|s\right)\)._

During value alignment, we update the policy with Eq. (2) simultaneously for sampling the overestimated actions. The whole process iterates Eq. (2) and Eq. (13) to obtain a policy that performs equivalently to the offline policy and a critic aligned with it. We denote the policy as \(\pi_{\text{on}}\) and the aligned critic as \(Q_{\text{on}}\) for sequential training. Note that \(Q_{\text{on}}\), which is modified from the critic obtained in the policy re-evaluation, does not depend on specific offline critics. This flexibility allows us to implement the transition to SAC from different offline algorithms.

O2td3As done in the case of O2SAC, we can use the Q-values of the policy actions \(\hat{a}\) to maintain the optimistic property of policy re-evaluation and calibrate the Q-values of other actions.

Unfortunately, in TD3, the actor is modeled as a deterministic policy, lacking an explicit expression for Q-values, which prevents us from directly aligning Q-values with the policy.

To solve this problem, our main idea is to model the distribution of Q-values around the policy action \(\dot{a}\) as a Gaussian distribution. Specifically, in Eq. (5), when we use the offline policy as \(\pi_{\theta}\), the gradient of the policy is only related to the the gradient of \(Q(s,a)\) with respect to \(a\). It is easy to see that the gradient around \(\dot{a}\) should tend to 0 as \(\dot{a}\) is the optimal action selected by the offline policy. Due to the use of smoothing regularization in the critic's update in TD3, the Q-values of actions near \(\dot{a}\) differ only slightly from the Q-value of \(\dot{a}\) itself. This enables us to assume that normalized Q-values around \(\dot{a}\) follow a Gaussian distribution \(Q(s,a)/Q(s,\dot{a})\sim N(\dot{a},\Sigma)\). Formally, we can calibrate the Q-values of other actions as (see Appendix G.2 for detailed derivation)

\[Q^{\prime}(s,a)=\min\left(Q_{\bar{\mu}}(s,\tilde{a}),\frac{Q(s,\dot{a})}{1+k \cdot\max\left(d(a,\dot{a})^{2},\sigma^{2}\right)}\right)\] (15)

where \(k\) is a constant, which is set as 1 across all tasks, and \(d(a,\dot{a})\) is a distance measure, which is defined as the euclidean distance divided by the square root of the action dimension in our implementation.

From Eq. (15), after calibration, the Q-values of the actions around \(\dot{a}\) are only slightly lower than \(Q(s,\dot{a})\), which ensures that \(\dot{a}\) is the output action of the policy while maintaining a smoothing and optimistic property of Q-values. Moreover, for the actions that differ greatly from \(\dot{a}\), we limit the maximum of the distance measure \(d(a,\dot{a})\) in Eq. (15) to the policy noise used in Eq. (6) to avoid severe underestimation of their Q-values. Formally, we define the objective loss of value alignment for O2TD3 as

\[\begin{split}\mathcal{L}_{Q}^{\mathrm{align}}\left(\mu_{i} \right)&=\mathop{\mathbb{E}}_{s\sim R}\left[\left(Q_{\mu_{i}}\left( s,\tilde{a}\right)-Q^{\prime}_{\mu}\left(s,\tilde{a}\right)\right)^{2} \right]\rvert_{\tilde{a}=\pi(s)+\delta}\right]\\ \mathcal{L}_{Q}^{\mathrm{train}}\left(\mu_{i}\right)& =\mathop{\mathbb{E}}_{s\sim R}\left[\left(Q_{\mu_{i}}\left(s, \dot{a}\right)-Q_{\bar{\mu}}\left(s,\dot{a}\right)\right)^{2}\left\lvert _{\tilde{a}=\pi_{\mathrm{off}}(s)}\right\right]\\ \mathcal{L}_{Q}^{\mathrm{critic}}\left(\mu_{i}\right)& =\mathcal{L}_{Q}^{\mathrm{align}}\left(\mu_{i}\right)+\mathcal{L}_{Q}^{ \mathrm{retain}}\left(\mu_{i}\right)\end{split}\] (16)

where \(Q^{\prime}_{\mu}\left(s,\tilde{a}\right)=\min(Q_{\bar{\mu}}(s,\tilde{a}),Q^{ \prime}_{\mu}(s,\tilde{a}))\) and \(\tilde{a}\) is a perturbed action defined in Eq (6).

O2ppoIn PPO, only the critic \(V(s)\) is used to estimate the advantages for policy update. During the re-evaluation process, we train a critic by fitting the returns of offline trajectories as mentioned in Section 4.1. This indicates that the re-evaluated critic only approximate \(V^{\mu}(s)\) instead of the true one, misguiding the update of the policy. To mitigate this problem, we propose an auxiliary advantage function to correct erroneous updates.

Generally, a desirable auxiliary advantage function should satisfy the following two conditions: 1) it enables the policy to update in a reliable region; 2) its value must be zero at the beginning of online fine-tuning to enable the policy to transition smoothly from offline to online. Considering that the well-trained offline policy is reliable, we define the auxiliary advantage function as

\[A_{\alpha}(s,a)=\alpha\log\pi_{\mathrm{off}}(a|s)+\alpha\mathcal{H}(\pi_{ \mathrm{off}}(\cdot|s))\] (17)

where \(\mathcal{H}\) is the entropy of action probabilities predicted by \(\pi_{\mathrm{off}}\). It is easy to verify the second condition that \(A_{\alpha}(s,a)=0\) at the beginning of offline fine-tuning. To verify the first condition, we derive the following proposition. Its proof can be found in Appendix F.

**Proposition 4.4**.: _With \(A_{\alpha}(s,a)\) in Eq. (17), the policy update is regularized by the cross-entropy loss about the offline policy, thereby constraining the policy update in a reliable region._

Accordingly, we define the advantage function for policy update as

\[A^{\prime}(s,a)=A(s,a)+\beta A_{\alpha}(s,a)\] (18)

where \(\beta\) anneals to 0 from 1. With the auxiliary advantage, Eq. (18) prevent the update direction from deviating too far from the offline policy.

### Constrained Fine-Tuning

In the previous subsections, we discussed how to address the mismatch issues in the O2O problems. However, due to the distribution shift between the offline dataset and the online environment, along with the optimism in online RL, encountering out-of-distribution (OOD) states and actions becomes inevitable, potentially leading to significant performance fluctuations. Especially for OOD states that are absent in the offline dataset, even though the policy was trained well in the offline phase, it may still fail to output favourable actions, thereby causing erroneous policy update.

Considering that the critic maintains an optimistic nature after undergoing policy re-evaluation and value alignment, with the optimistic update way during online fine-tuning, it typically overestimates the Q-values of OOD state-action pairs, which can mislead the update of the policy. Inspired of CMDP [1, 41, 7], we develop constrained fine-tuning (CFT) to introduce a regularization term to constrain the current actor and critic, which prevents the policy update from being severely misguided.

Specifically, we impose a constraint term \(f(\pi,\pi_{\text{ref}})\) on the policy objective to ensure that it updates within the credible region of \(\pi_{\text{ref}}\), where \(f(\cdot,\cdot)\) is a divergence measurement and \(\pi_{\text{ref}}\) is the optimal historical policy during online evaluations. Formally, we define the policy objective of CFT as

\[\max\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma_{t}r_{t}(s_{t},a_{t})]\quad \text{s.t.}\ \mathbb{E}_{\pi}[f(\pi(a_{t}|s_{t}),\pi_{\text{ref}}(a_{t}|s_{t}))]<\tau\] (19)

By incorporating the constraint term into the reward function akin to RCPO [41], we can solve the problem by minimizing the following objective functions with initializing \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\) obtained in Section 4.2 at the beginning of online fine-tuning.

\[\begin{split}& L_{\pi}(\theta)=\max\mathbb{E}_{\pi_{\theta}}[Q_{\mu} ^{\pi_{\theta}}(s)-\lambda f(\pi_{\theta}(a|s),\pi_{\text{ref}}(a|s))]\\ & L_{Q}(\mu)=\min\mathbb{E}_{(s,a,r,s^{\prime})\sim R}[(Q_{\mu}^{ \pi_{\theta}}(s,a)-y)^{2}]\\ & y=r+\gamma\mathbb{E}_{\sigma^{\prime}\sim\pi_{\theta}(\cdot|s^ {\prime})}[Q_{\tilde{\mu}}^{\pi_{\theta}}(s^{\prime},a^{\prime})-\lambda f( \pi_{\theta}(a^{\prime}|s^{\prime}),\pi_{\text{ref}}(a^{\prime}|s^{\prime}))] \\ & L(\lambda)=\min_{\lambda\geq 0}-\lambda\left[\mathbb{E}_{\pi_{ \theta}}(f(\pi_{\theta}(a|s),\pi_{\text{ref}}(a|s)))-\tau\right]\end{split}\] (20)

We provide a theoretical guarantee for the proposed CFT framework.

**Corollary 4.5**.: _With the penalty \(f(\pi,\pi_{\text{ref}})\) defined before and appropriate learning rates, algorithm of Eq. (20) almost surely to a fixed point \((\theta^{\star},\mu^{\star},\lambda^{\star})\), where \(\lambda^{\star}=0\), \(\theta^{\star}\) and \(\mu^{\star}\) are corresponded to \(\pi^{\star}\) and \(Q^{\star}\), which are optimal in the MDP without constraint._

In our implementation, we use KL divergence and MSE function as \(f(\cdot,\cdot)\) for the transitions of O2SAC and O2TD3, respectively. For O2PPO, the auxiliary advantage function already has the

Figure 2: Performance curves on D4RL [9] MuJoCo locomotion tasks during online fine-tuning.

ability to constrain the policy update, we only need to replace \(\pi_{\text{off}}\) with \(\pi_{\text{ref}}\) during online fine-tuning. Note that in our methods, at the beginning of online fine-tuning, the regularization function \(f(\pi_{\theta}(a|s),\pi_{\text{on}}(a|s))\) is zero for any state, which guarantees no destruction of the alignment for the actor and the critic obtained in Section 4.2.

## 5 Experiments

In this section, we perform experiments to validate the effectiveness of the proposed method on D4RL [9] MuJoCo and AntMaze tasks, including HalfCheetah, Hopper, Walker2d and AntMaze environments. Specifically, we compare our methods with AWAC [32], IQL [20], PEX [50], Off2On [23], Cal-QL [33] and ACA [48]. For all methods except for O2PPO, we run 100,000 interaction steps with the environment and evaluate the policy per 1000 steps, as they all use off-policy methods for online fine-tuning. For our O2PPO method, due to the low efficiency of the on-policy method, we run 250,000 interaction steps to validate its performance, with 2500 steps as the evaluation interval. We run all methods with five random seeds and report their averaging results. Due to the space limitation, more experimental results can be found in Appendix B and C.

### Comparison with State-of-the-Art Methods

We respectively initialize the policies of O2SAC, O2TD3 and O2PPO from the results of CQL, TD3+BC and IQL. Figure 2 shows the performance curves of our methods and comparing methods on Mujoco tasks. Similar to the previous works [48; 5], for O2SAC and O2TD3, we use CQL and TD3+BC for offline policy pre-training. From the figure, we can see that our method can converge more stably and rapidly than other methods and achieve the optimal performance in most cases. Although the ensemble method Off2On can achieve better final performance than our methods in some cases, it often suffers from a dramatic drop in performance at the beginning of fine-tuning, which is often unacceptable in the O2O problems. We report the results of O2PPO in Appendix A because of its different number of interaction steps. Although PPO is an on-policy method with low efficiency, from the results in Table 1, it shows significant superiority on sparse reward tasks, even though with equal interactions.

### Study on Transferability

In this section, we perform experiments to verify the powerful transferability of the proposed method. As mentioned earlier, one of the advantages of our method is that it imposes no requirements on offline

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline Dataset & IQL & PEX & Cal-QL & O2TD3 & O2SAC & O2PPO \\ \hline U-v2 & 80.8\(\rightarrow\)80.8 & 85.0\(\rightarrow\)96.2 & 80.8\(\rightarrow\)97.0 & 92.8\(\rightarrow\)95.8 & 92.8\(\rightarrow\)93.6 & 77.3\(\rightarrow\)98.0 \\ U-D-v2 & 56.6\(\rightarrow\)35.8 & 12.6\(\rightarrow\)16.0 & 23.8\(\rightarrow\)71.2 & 38.4\(\rightarrow\)52.2 & 43.8\(\rightarrow\)79.8 & 56.4\(\rightarrow\)86.3 \\ \hline total & 137.4\(\rightarrow\)116.6 & 97.6\(\rightarrow\)112.2 & 104.6\(\rightarrow\)168.2 & 128.6\(\rightarrow\)142.0 & 131.2\(\rightarrow\)148.0 & 133.7\(\rightarrow\)184.3 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average normalized D4RL scores of our methods shown in AntMaze navigation tasks after 200k interactions with the environment. (U=unaze, D=diverse)

Figure 3: The fine-tuning performance achieved by transferring to three online algorithms from their heterogeneous offline algorithms.

algorithms. This means we can achieve the transfer from any offline RL algorithm to three online RL algorithms. Figure 3 illustrates the fine-tuning performance of three online algorithms transferring from their heterogeneous offline algorithms. From Figure 3(a), pre-trained with TD3+BC, O2SAC outperforms SAC trained from scratch with a larger margin. Although Online Decision Transformer (ODT) achieves favourable performance in the offline environment, it converges slowly during online fine-tuning due to the architecture of the transformer. Our O2TD3 and O2PPO significantly enhances its performance through online fine-tuning. These results convincingly verify that our method show the strong transferability from various offline methods.

## 6 Conclusion

In this paper, we disclose there exist two types of mismatches when online fine-tuning offline RL methods. To address these two mismatches in O2O RL, we proposed optimistic critic reconstruction to re-evaluate an optimistic critic and align it with the offline actor before online fine-tuning, ensuring the stable performance improvement at the beginning stage and potentially better efficiency due to the reconstructed optimism consistent with online RL. Furthermore, to combat the inevitable distribution shift that can hinder the stable performance improvement, we introduce constrained fine-tuning to constrain the divergence of current policy and the best foregoing policy to maintain the stability of online fine-tuning. These two components form a versatile O2O framework, allowing the transition from any offline algorithms to three state-of-the-art online algorithms. Experiments show our framework can converge to optimal performance without affecting the aligned critic at the beginning of online fine-tuning and achieve strong empirical performance.

## Acknowledgements

This work was supported by the NSFC (62222605), the Natural Science Foundation of Jiangsu Province of China (BK20222012, BK20211517), and the National Science and Technology Major Project (2020AAA0107000).

## References

* [1] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In _International conference on machine learning_, pages 22-31. PMLR, 2017.
* [2] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. _CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep_, 32:96, 2019.
* [3] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. _Advances in Neural Information Processing Systems_, 35:28955-28971, 2022.
* [4] Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with offline data. _arXiv preprint arXiv:2302.02948_, 2023.
* [5] Alex Beeson and Giovanni Montana. Improving td3-bc: Relaxed policy constraint for offline learning and stable online fine-tuning. _arXiv preprint arXiv:2211.11802_, 2022.
* [6] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* [7] Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. Safe exploration in continuous action spaces. _arXiv preprint arXiv:1801.08757_, 2018.
* [8] Christopher Diehl, Timo Sebastian Sievernich, Martin Kruger, Frank Hoffmann, and Torsten Bertram. Uncertainty-aware model-based offline reinforcement learning for automated driving. _IEEE Robotics and Automation Letters_, 8(2):1167-1174, 2023.

* Fu et al. [2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* Fujimoto and Gu [2021] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* Fujimoto et al. [2018] Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In _International conference on machine learning_, pages 1587-1596. PMLR, 2018.
* Fujimoto et al. [2019] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International conference on machine learning_, pages 2052-2062. PMLR, 2019.
* Garg et al. [2023] Divyansh Garg, Joey Hejna, Matthieu Geist, and Stefano Ermon. Extreme q-learning: Maxent rl without entropy. _arXiv preprint arXiv:2301.02328_, 2023.
* Guan et al. [2025] Cong GUAN, Ke XUE, Chunpeng FAN, Feng CHEN, Lichao ZHANG, Lei YUAN, Chao QIAN, and Yang YU. Open and real-world human-ai coordination by heterogeneous training with communication. _Frontiers of Computer Science_, 19(4):194314, 2025.
* Guo et al. [2023] Siyuan Guo, Yanchao Sun, Jifeng Hu, Sili Huang, Hechang Chen, Haiyin Piao, Lichao Sun, and Yi Chang. A simple unified uncertainty-guided framework for offline-to-online reinforcement learning. _arXiv preprint arXiv:2306.07541_, 2023.
* Haarnoja et al. [2017] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In _International conference on machine learning_, pages 1352-1361. PMLR, 2017.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Hansen-Estruch et al. [2023] Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. Idql: Implicit q-learning as an actor-critic method with diffusion policies. _arXiv preprint arXiv:2304.10573_, 2023.
* Jang and Kim [2022] Ingook Jang and Seonghyun Kim. Uncertainty-driven pessimistic q-ensemble for offline-to-online reinforcement learning. In _3rd Offline RL Workshop: Offline RL as a"Launchpad"_, 2022.
* Kostrikov et al. [2021] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. In _International Conference on Learning Representations_, 2021.
* Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Le et al. [2019] Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In _International Conference on Machine Learning_, pages 3703-3712. PMLR, 2019.
* Lee et al. [2022] Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble. In _Conference on Robot Learning_, pages 1702-1712. PMLR, 2022.
* Lei et al. [2023] Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, and Huazhe Xu. Uni-o4: Unifying online and offline deep reinforcement learning with multi-step on-policy optimization. _arXiv preprint arXiv:2311.03351_, 2023.
* Li et al. [2023] Jianxiong Li, Xiao Hu, Haoran Xu, Jingjing Liu, Xianyuan Zhan, and Ya-Qin Zhang. Proto: Iterative policy regularized offline-to-online reinforcement learning. _arXiv preprint arXiv:2305.15669_, 2023.

* Luo et al. [2023] Yicheng Luo, Jackie Kay, Edward Grefenstette, and Marc Peter Deisenroth. Finetuning from offline reinforcement learning: Challenges, trade-offs and practical solutions. _arXiv preprint arXiv:2303.17396_, 2023.
* Lyu et al. [2022] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:1711-1724, 2022.
* Mao et al. [2022] Yihuan Mao, Chao Wang, Bin Wang, and Chongjie Zhang. Moore: Model-based offline-to-online reinforcement learning. _arXiv preprint arXiv:2201.10070_, 2022.
* Mao et al. [2023] Yixiu Mao, Hongchang Zhang, Chen Chen, Yi Xu, and Xiangyang Ji. Supported trust region optimization for offline reinforcement learning. In _International Conference on Machine Learning_, pages 23829-23851. PMLR, 2023.
* Mark et al. [2022] Max Sobol Mark, Ali Ghadirzadeh, Xi Chen, and Chelsea Finn. Fine-tuning offline policies with optimistic action selection. In _Deep Reinforcement Learning Workshop NeurIPS 2022_, 2022.
* McInroe et al. [2023] Trevor McInroe, Stefano V Albrecht, and Amos Storkey. Planning to go out-of-distribution in offline-to-online reinforcement learning. _arXiv preprint arXiv:2310.05723_, 2023.
* Nair et al. [2020] Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* Nakamoto et al. [2023] Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi Ma, Chelsea Finn, Aviral Kumar, and Sergey Levine. Cal-ql: Calibrated offline rl pre-training for efficient online fine-tuning. _arXiv preprint arXiv:2303.05479_, 2023.
* Rashidinejad et al. [2021] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34:11702-11716, 2021.
* Schulman et al. [2015] John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In _International conference on machine learning_, pages 1889-1897. PMLR, 2015.
* Schulman et al. [2015] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. _arXiv preprint arXiv:1506.02438_, 2015.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Sutton and Barto [2005] RichardS. Sutton and AndrewG. Barto. Reinforcement learning: An introduction. _IEEE Transactions on Neural Networks_, page 285-286, Jan 2005.
* Tang et al. [2022] Shengpu Tang, Maggie Makar, Michael Sjoding, Finale Doshi-Velez, and Jenna Wiens. Leveraging factored action spaces for efficient offline reinforcement learning in healthcare. _Advances in Neural Information Processing Systems_, 35:34272-34286, 2022.
* Tarasov et al. [2022] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. CORL: Research-oriented deep offline reinforcement learning library. In _3rd Offline RL Workshop: Offline RL as a "Launchpad"_, 2022.
* Tessler et al. [2018] Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization. _arXiv preprint arXiv:1805.11074_, 2018.
* Uchendu et al. [2023] Ikechukwu Uchendu, Ted Xiao, Yao Lu, Banghua Zhu, Mengyuan Yan, Josephine Simon, Matthew Bennice, Chuyuan Fu, Cong Ma, Jiantao Jiao, et al. Jump-start reinforcement learning. In _International Conference on Machine Learning_, pages 34556-34583. PMLR, 2023.

* [43] Shenzhi Wang, Qisen Yang, Jiawei Gao, Matthieu Gaetan Lin, HAO CHEN, Liwei Wu, Ning Jia, Shiji Song, and Gao Huang. Train once, get a family: State-adaptive balances for offline-to-online reinforcement learning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [44] Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng Long. Supported policy optimization for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:31278-31291, 2022.
* [45] Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. _Advances in neural information processing systems_, 34:27395-27407, 2021.
* [46] Haoran Xu, Li Jiang, Li Jianxiong, and Xianyuan Zhan. A policy-guided imitation approach for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:4085-4098, 2022.
* [47] Haoran Xu, Li Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai Kin Chan, and Xianyuan Zhan. Offline rl with no ood actions: In-sample learning via implicit value regularization. _arXiv preprint arXiv:2303.15810_, 2023.
* [48] Zishun Yu and Xinhua Zhang. Actor-critic alignment for offline-to-online reinforcement learning. In _International Conference on Machine Learning_, pages 40452-40474. PMLR, 2023.
* [49] Yang Yue, Rui Lu, Bingyi Kang, Shiji Song, and Gao Huang. Understanding, predicting and better resolving q-value divergence in offline-rl. _arXiv preprint arXiv:2310.04411_, 2023.
* [50] Haichao Zhang, We Xu, and Haonan Yu. Policy expansion for bridging offline-to-online reinforcement learning. _arXiv preprint arXiv:2302.00935_, 2023.
* [51] Kai Zhao, Yi Ma, Jinyi Liu, Yan Zheng, and Zhaopeng Meng. Ensemble-based offline-to-online reinforcement learning: From pessimistic learning to optimistic exploration. _arXiv preprint arXiv:2306.06871_, 2023.
* [52] Yi Zhao, Rinu Boney, Alexander Ilin, Juho Kannala, and Joni Pajarinen. Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning. _arXiv preprint arXiv:2210.13846_, 2022.
* [53] Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In _international conference on machine learning_, pages 27042-27059. PMLR, 2022.

Detailed Data

Compared with the baseline methods, our methods outperform much in D4RL Mujoco locomotion tasks, as showed in Table 2. We can compare the fine-tuning performance in groups based on online update way. Off2On, Cal-QL, ACA and our O2SAC are updated in the SAC way during online fine-tuning. Although there is a lack of the baseline methods that update the policy in the TD3 way, we compare our O2TD3 with a recent O2O method PROTO+TD3 [25] in Appendix C.4 to demonstrate the superiority of our methods. And we compare our O2PPO with the policy constraint methods AWAC, IQL and PEX, that are implemented in the IQL way, since the idea of PPO is similar to a kind of policy constraint.

With such groups for comparison, our methods get the best performance improvements respectively. Note that in our implementation, Off2On achieves much better performance than the original paper [23] and the implementations in other papers [48] and [50]. However, our O2SAC still outperforms it with less computational cost during online fine-tuning and less requirements for offline policy. In

\begin{table}
\begin{tabular}{l|r r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{5}{c}{Score\((\delta)\)} \\  & Off2On & Cal-QL & ACA & O2TD3 & O2SAC \\ \hline Hc-M & 94.22(47.37) & 54.36(6.81) & 66.59(20.09) & 65.58\(\pm\)0.7(17.45) & 74.18\(\pm\)0.5(27.32) \\ Ho-M & 90.34(29.83) & 92.83(33.92) & 99.76(41.08) & 100.25\(\pm\)1.1(43.95) & 97.29\(\pm\)8.2(38.0) \\ Wa-M & 98.2(17.55) & 83.93(1.06 ) & 82.55(5.18) & 96.32\(\pm\)1.6(12.71) & 106.51\(\pm\)2.1(25.42) \\ \hline Hc-M-R & 88.74 (43.18) & 49.36(3.31) & 57.93(15.71) & 58.58\(\pm\)1.8(14.31) & 71.65\(\pm\)0.9(26.37) \\ Ho-M-R & 103.61(4.93) & 98.64(2.58) & 102.19(50.3) & 101.29\(\pm\)1.4(34.22) & 105.12\(\pm\)1.2(16.15) \\ Wa-M-R & 102.43(20.78) & 91.61(9.36) & 85.65(8.86) & 100.08\(\pm\)2.2(21.64) & 106.05\(\pm\)2.8(28.75) \\ \hline Hc-M-E & 98.33(2.67) & 92.67(43.62) & 93.54(0.11) & 97.07\(\pm\)1.1(4.98) & 100.41\(\pm\)0.8(6.24) \\ Ho-M-E & 99.47(14.65) & 108.57(4.57) & 109.72(14.17) & 112.49\(\pm\)0.9(10.8) & 107.4\(\pm\)6.0(23.25) \\ Wa-M-E & 118.01(8.51) & 110.3(1.21) & 110.36(2.92) & 112.97\(\pm\)0.4(2.43) & 120.95\(\pm\)0.6(11.51) \\ \hline Hc-E & 100.99(4.59) & 96.9(1.01) & 87.93(-6.65) & 99.96\(\pm\)0.4(3.15) & 107.74\(\pm\)0.8(11.08) \\ Ho-E & 92.13(-19.44) & 110.84(5.1) & 107.79(-1.98) & 112.4\(\pm\)0.5(0.66) & 112.76\(\pm\)0.4(0.95) \\ Wa-E & 118.02(8.3) & 109.71(0.76) & 108.2(0.21) & 114.67\(\pm\)1.1(4.52) & 120.35\(\pm\)0.8(10.5) \\ \hline Total & 1204.5(182.93) & 1099.73(113.3) & 1112.22(150.0) & 1171.66(170.82) & **1230.41(225.54)** \\ \hline \hline \end{tabular} 
\begin{tabular}{l|r r r r} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{5}{c}{Score\((\delta)\)} \\  & AWAC & IQL & PEX & O2PPO \\ \hline Hc-M & 51.11(1.43) & 48.85(0.33) & 57.05(8.53) & 59.96\(\pm\)0.8(11.54) \\ Ho-M & 82.42(16.53) & 60.48(-1.62) & 95.7(33.59) & 100.42\(\pm\)1.1(48.61) \\ Wa-M & 87.03(1.50) & 83.87(1.34) & 88.18(5.65) & 86.65\(\pm\)2.2(9.0) \\ \hline Hc-M-R & 47.81(2.03) & 43.42(0.32) & 45.44(2.34) & 46.76\(\pm\)1.1(4.22) \\ Ho-M-R & 98.86(0.07) & 95.56(5.16) & 101.75(11.35) & 96.87\(\pm\)4.1(18.14) \\ Wa-M-R & 86.16(6.49) & 84.75(5.42) & 94.1(14.78) & 90.43\(\pm\)5.1(11.6) \\ \hline Hc-M-E & 95.37(0.57) & 92.54(0.26) & 84.99(-7.3) & 89.61\(\pm\)1.1(-3.29) \\ Ho-M-E & 109.76(2.53) & 108.29(11.23) & 97.17(0.11) & 107.61\(\pm\)5.4(8.98) \\ Wa-M-E & 106.87(1.23) & 112.57(0.67) & 116.47(4.57) & 121.4\(\pm\)0.8(9.35) \\ \hline Hc-E & 97.16(0.42) & 96.24(-0.56) & 96.69(-0.1) & 96.2\(\pm\)0.7(2.07) \\ Ho-E & 109.88(-1.0) & 97.15(-5.33) & 71.37(-31.12) & 113.08\(\pm\)0.4(9.09) \\ Wa-E & 111.31(0.64) & 113.43(1.31) & 97.54(-addition, our methods bridge different offline algorithms and three SOTA online algorithms, and permit additional modifications for improved algorithms based on the online algorithms.

Due to the low sample efficiency of PPO, we run 250,000 interaction steps for the implementation of O2PPO, which makes it unreasonable to compare the results of different O2O methods in one graph. So we demonstrate the results of O2PPO without any baseline method, but compare our method with the direct way of fine-tuning the offline policy of IQL in the PPO way in the online phase. Note that our purpose is to achieve stable performance improvement, the efficiency depends mainly on the online algorithm, so in some environments, such as _walker2d-medium-replay-v2_, the performance improvement may be less than the direct fine-tuning. This phenomenon could arise due to the critic's capability to approximate the genuine values effectively, thereby facilitating accurate evaluation of the policy, obviating the necessity for corrective adjustments in the policy update direction. And our O2PPO uses an auxiliary advantage function to constrain the policy update, resulting in slower performance improvement. However, since we cannot judge when the critic is reliable, it is necessary to use O2PPO to achieve stable performance improvement, and there is no remarkable discrepancy between O2PPO and the direct way, although in the above scenarios. In general, our O2PPO can achieve stable and efficient performance improvement with limited interactions.

## Appendix B Ablation Study

Before the analysis of ablation studies, we need to clarify the roles and applicability of the different components of our methods for different offline methods.

**The benefit and applicability of policy re-evaluation** As talked before, the purpose of policy re-evaluation is to get optimistic Q-value functions, avoiding the drop of Q-value due to underestimated Q-values in offline. Many previous works [27; 33] discussed such a problem and solve it by alleviating excessive pessimism. Our methods do not need initialize critic from offline results, but re-evaluate the policy in the online way to get optimistic Q-value functions, which is applicable to different algorithms and avoids the performance drop caused by the drastic drop of Q-value. Note that optimistic Q-value functions obtained by policy re-evaluation may be unreasonable, especially for OOD actions, hence value alignment is needed for redress overestimated values.

Figure 4: Performance of our O2PPO and direct PPO from IQL on D4RL [9] MuJoCo locomotion tasks during online fine-tuning. The solid lines and shaded regions represent mean and standard deviation.

In some policy constraint methods [32, 10], only the policy is limited to update in a region close to the dataset, but the critic is obtained in the same way as the online update. Therefore, for such offline methods, the online critic can be initialized directly by the offline one, and just need to align the values with the offline policy. In addition, policy re-evaluation can be applied to the scenarios where only the offline policy is provided, such as where the offline policy is obtained in the style of sequence modeling [6, 53, 18] or non-standard RL [46].

**The necessity of value alignment** With the optimistic property, Q-value functions obtained in policy re-evaluation will overestimate OOD actions and induce the policy to take such actions, resulting in performance degradation. For the transition from those policy constraint methods that do not require re-evaluation of the policy, this case still holds. The purpose of value alignment is to approximate the optimistic property and suppress the Q-values of OOD actions to a reasonable estimation. For those offline algorithms where the actor and the critic are misaligned like explicit policy constraint methods, if the constraint is removed during online fine-tuning, the critic will induce actor to align with it. However, the actor is reliable but the critic is not, so such process leads to unknown performance changes that are most likely to be worse. In addition, for algorithms induced by _behavior-regularized_ MDP, such problem still exists because the update way between offline and online changes, leading to drastic variation of the critic. Therefore, for O2O RL, it is necessary to align the critic with the actor instead of aligning the actor with the critic.

**The necessity of constrained fine-tuning** Although we keep the optimistic property for Q-value functions and align the critic with actor, it is still challenging to achieve stable online fine-tuning. In general, most of the current offline algorithms focus on how to avoid OOD actions and train a reliable policy on the states of the dataset. Due to the optimism in online RL, OOD states and actions are inevitable, may leading to drastic performance fluctuations, which is undesirable for important scenarios especially for high-risk scenarios. Especially for OOD states, even trained well in the offline phase, the policy still fails to output favourable actions, that may causes erroneous policy update. Therefore, for stable O2O RL, online fine-tuning with the constraint of ensuring safe exploration is necessary.

The results of Figure 5 show that even with a narrow dataset _e.g. hopper-expert-v2_, our methods still achieve stable online fine-tuning. For O2SAC, the fine-tuning process without the optimistic critic reconstruction can lead a sudden performance drop at the beginning phase, e.g. in Figure 5(a), as the offline critic may be severely pessimistic. Due to the alignment of the offline critic and the actor in CQL, using constrained fine-tuning alone can result in overall stable performance improvement,

Figure 5: Ablation results of our methods, PR=Policy re-evaluation, VA=Value Alignment, CF=Constrained Fine-tuning. For O2PPO, VA means the use of the auxiliary advantage, and CF means the update of the reference policy.

but O2SAC demonstrates a more efficient result. As for O2TD3, fine-tuning from offline directly suffers from severe performance degradation due to the mismatch of the actor and the critic in offline training, such as in Figure 5(c). Thanks to value alignment, we can address the mismatch, thereby achieving stable performance improvement at the beginning stage. However, due to OOD states during online fine-tuning, constrained fine-tuning is necessary for the long-term stability. At last, PPO is an on-policy method that has a high data quality requirement, so it cannot improve performance if the value is incorrectly evaluated, e.g. in Figure 5(e). Through the reliable auxiliary function, our O2PPO can improve performance stably even with imperfect value evaluation.

Although our methods are universal for any offline algorithm to SAC, TD3 and PPO, some steps can be omitted according to the type of the offline algorithm. For example, for value regularization method _e.g._ CQL, we can only use constrained fine-tuning for stable and efficient O2O RL if the constraint term with a low coefficient has little effect on the critic, as the offline actor and critic are aligned, constrained fine-tuning is enough to combat the problem caused by the drastic jump of Q-values. And for policy constraint methods with explicit constraint _e.g._ TD3+BC and AWAC, as talker above, the critic is evaluated in the same way as online way, so policy re-evaluation can be omitted. For other offline methods with the update way that does not match the online way, such as IQL [20], IVR [47] and \(\chi\)-QL [13], and methods induced by _behavior-regularized_ MDP, and with special policy form _decision transformer_, all steps of our methods are needed for stable O2O RL.

## Appendix C Additional Experiments

### Difference in policy performance caused by optimistic critic reconstruction

Note that we use \(\pi_{\text{on}}\) as the initialization of the actor at the beginning of online fine-tuning. In accordance with our analysis, the performance of the initial policy \(\pi_{\text{on}}\) is expected to align with the characteristics of the actual offline policy. As depicted in Table 3 from our empirical experiments, the results substantiate our analysis, revealing minimal disparities in the performance of \(\pi_{\text{on}}\) compared to the actual offline policy. In most environments, \(\pi_{\text{on}}\) even demonstrates superior performance than the offline policy. We attribute this to the effectiveness of our _min_ operator, which sensibly tightens the policy distribution.

### Experiments on D4RL AntMaze navigation tasks

For the difficult tasks of AntMaze navigation, such as _medium_ and _large_ environments, TD3+BC is almost completely incapable of training a favourable policy [40]. Fine-tuning with the poor

\begin{table}
\begin{tabular}{l|c c|c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{2}{c}{O2SAC} & \multicolumn{2}{c}{O2TD3} \\  & \(\pi_{\text{off}}\) & \(\pi_{\text{on}}\) & \(\pi_{\text{off}}\) & \(\pi_{\text{on}}\) \\ \hline halfcheetah-medium-v2 & 46.85 & 53.04 & 48.12 & 48.46 \\ hopper-medium-v2 & 59.29 & 65.28 & 56.29 & 61.44 \\ walker2d-medium-v2 & 81.08 & 78.63 & 83.13 & 84.16 \\ \hline halfcheetah-medium-replay-v2 & 45.27 & 48.14 & 44.28 & 45.21 \\ hopper-medium-replay-v2 & 88.97 & 99.64 & 59.26 & 49.95 \\ walker2d-medium-replay-v2 & 80.31 & 75.77 & 77.16 & 75.90 \\ \hline halfcheetah-medium-expert-v2 & 94.14 & 95.43 & 91.99 & 78.64 \\ hopper-medium-expert-v2 & 84.15 & 94.15 & 101.60 & 106.44 \\ walker2d-medium-expertv2 & 109.44 & 109.30 & 110.51 & 110.53 \\ \hline halfcheetah-medium-expertv2 & 96.65 & 99.80 & 96.89 & 97.17 \\ hopper-expertv2 & 111.80 & 111.68 & 111.74 & 106.31 \\ walker2d-expertv2 & 109.84 & 109.87 & 110.15 & 110.3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The performance of the offline policy \(\pi_{\text{off}}\) and the policy \(\pi_{\text{on}}\) obtained after policy re-evaluation and value alignment initialization from such a policy helps little and has minor differences with training a policy from scratch, that should be the concern of hybrid learning [4], so we do not consider it.

Here we demonstrate the results of O2SAC and O2PPO on the the difficult tasks of AntMaze navigation, including _antmaze-medium-play-v2_, _antmaze-medium-diverse-v2_, _antmaze-large-play-v2_, _antmaze-large-diverse-v2_ environments.

Note that here we run O2PPO with 250,000 environments steps and run O2SAC with 200,000 environments steps, while in Section 5.1, we run O2PPO with 200,000 environments steps.

Note that as some previous work, in our O2SAC implementation on AntMaze tasks, we do not use the double Q networks trick to avoid overly underestimation, and the threshold \(\tau\) reaches the maximum in step 100,000.

Since there is a lack of hyper-parameters for these tasks in many related work [48; 25; 50; 53], we do not compare the results of our methods with other methods. However, according to our simple reproduction, our methods achieve competitive results.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Dataset & AWAC & IQL & PEX & O2SAC & O2PPO \\ \hline medium-play-v2 & 0.0\(\rightarrow\) 0.0 & 64.8\(\rightarrow\)78.0 & 73.6\(\rightarrow\)81.0 & 67.0\(\rightarrow\)78.8 & 66.5\(\rightarrow\)80.3 \\ medium-diverse-v2 & 0.0 \(\rightarrow\) 0.0 & 68.8\(\rightarrow\)73.2 & 70.8\(\rightarrow\)83.4 & 61.6\(\rightarrow\)70.6 & 63.5\(\rightarrow\)85.5 \\ \hline large-play-v2 & 0.0 \(\rightarrow\) 0.0 & 39.4\(\rightarrow\)50.2 & 46.2\(\rightarrow\)54.6 & 19.8\(\rightarrow\)44.2 & 38.0\(\rightarrow\)55.83 \\ large-diverse-v2 & 0.0 \(\rightarrow\) 0.0 & 31.0\(\rightarrow\)36.0 & 40.0\(\rightarrow\)58.4 & 24.2\(\rightarrow\)40.2 & 26.83\(\rightarrow\)50.33 \\ \hline total & 0.0 \(\rightarrow\) 0.0 & 204.0\(\rightarrow\)237.4 & 230.6\(\rightarrow\)277.4 & 172.6 \(\rightarrow\)233.8 & 194.8\(\rightarrow\)272.0 \\ \(\Delta\) & +0.0 & +33.4 & +46.8 & +61.2 & +77.16 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average normalized D4RL scores of our methods and some baselines on AntMaze navigation tasks.

Figure 6: The results of O2SAC on D4RL [9] AntMaze navigation tasks during online fine-tuning. The solid lines and shaded regions represent mean and standard deviation.

Figure 7: The results of O2PPO on D4RL [9] AntMaze navigation tasks during online fine-tuning. The solid lines and shaded regions represent mean and standard deviation.

### Comparisons with the results of updating the reference policy update at a fixed interval

We set the reference policy as the optimal historical policy during online evaluations in our preceding experiments, and it indeed introduces additional testing information. However, for most of the scenarios that offline RL focus on, this would be acceptable as a well-learned policy can be deployed directly without serious consequence due to its decent performance, e.g. in autonomous driving.

In addition, thanks to the abundant data, another way that does not require practical evaluation is to train a transition model that can be used to evaluate the policy by generating synthetic rollouts as the initial states are provided. Since such a methods is similar to model-based methods, we will leave it for future work.

If evaluation is forbidden, we can replace \(\pi_{\text{ref}}\) with a recent policy at a fixed interaction interval. The Corollary 4.3. still holds because the policy performance will be almost certainly improved with a long interaction interval, especially compared to the last reference policy. Considering that the policy performance is most likely to fluctuate in the beginning stage during online fine-tuning, we can set a large update interval for this stage and a small update interval for the subsequent stage, or reduce the threshold change range.

We experiment our methods with a given update interval on D4RL Mujoco locomotion tasks and compare the results with those of the methods with the optimal reference policy, shown as Figure 8 and Figure 9. Without special hyper-parameter optimization, we just update the reference policy per 1000 steps for most environments, but for _hopper-medium-expert-v2_ and _hopper-expert-v2_, we update the reference policy per 10000 steps, and as well as for _walker2d-expert-v2_ in O2TD3, because these datasets are narrow, making it easy for the policy to suffer from OOD states and actions. With an appropriate interval, the policy performance can achieve competitive improvement when compared with the methods with the optimal reference policy. Policy re-evaluation and Value alignment guarantee the smoothing and stable performance improvement at the beginning stage, and constrained fine-tuning is necessary for the stability of the subsequent stage. Although in _hopper-medium-v2_, O2SAC with a fixed update interval suffers from the performance degradation at the latter stage, because the threshold is large and the Lagrange multiplier \(\lambda\) is low, the lack of constraint on the policy update in the reliable region. This phenomenon can easily be amended by reducing the threshold change range.

Figure 8: Comparisons on different ways of updating the reference policy for O2SAC and O2TD3.

### Comparisons with PROTO

In the absence of O2O methods on the deterministic policy, we compare our methods with PROTO [25], a recent O2O method that supports fine-tuning from an offline policy for both the stochastic and the deterministic policy. We reproduce the code of PROTO in Pytorch for the initialization of the policies derived from CQL and TD3+BC, and set all hyper-parameters as in the official paper [25]. As shown in Figure 10, our methods demonstrate significant superiority in both stability and effectiveness, especially for the deterministic policy.

### Comparisons on initialization of different offline methods

We conducted some experiments for O2SAC with the initialization of different offline methods, including CQL, IQL and ODT. The results are shown in Figure 11. The initial performance of O2SAC initialized from ODT is lower than others since the simple behavior cloning (we directly maximize the likelihood of the actions output by offline ODT while keeping an appropriate entropy) could harm the performance, as discussed in Appendix I. But in hopper-medium-v2, the performance improves quickly. We analyze that by the constraint, the policy can recover the offline performance (about 97 normalized score of ODT), as the output of the cloned policy is near the ODT policy. The results demonstrate that our methods are suitable for any offline algorithm, even the policies are heterogeneous.

### Accelerate learning with sample-efficient RL

An additional benefit of our methods is their straightforward compatibility with sample-efficient online RL algorithms. Since we only add a constraint that can be considered as part of the reward, the policy iteration process remains consistent with the normal online approach, which makes it feasible to incorporate techniques from advanced efficient RL algorithms. Drawing from [4], we conducted some experiments using a high UTD ratio of 10 (but still update the lagrangian multiplier once per step) and achieved better performance improvements, as shown in Figure 12.

Figure 9: Comparisons on different ways of updating the reference policy for O2PPO.

### Computational cost for each component

We conducted an experiment on _hopper-medium-v2_ environment using the O2SAC method and listed the time cost for different phases in Table 5, evaluated on an Nvidia 3070 GPU.

Figure 11: The performance of O2SAC with the initialization from different offline algorithms. The solid lines and shaded regions represent mean and standard deviation.

\begin{table}
\begin{tabular}{l|c c c} \hline \hline Training Phase & Offline(CQL) & Policy Re-evaluation & Value alignment \\ \hline Training Steps & 1M & 0.5M & 0.5M \\ Time Cost & 5.4h & 0.95h & 2.0h \\ \hline \hline \end{tabular}
\end{table}
Table 5: Computational cost for each component of O2SAC

Figure 10: Comparisons with PROTO and PROTO+TD3 [25] on D4RL [9] MuJoCo locomotion tasks during online fine-tuning. The solid lines and shaded regions represent mean and standard deviation.

In policy re-evaluation, since the policy is fixed and the re-evaluation of the critic is straightforward, the computational cost of re-evaluation is significantly lower than that of offline learning. The time cost of value alignment is somewhat higher but still less than that of the offline phase. In fact, the time cost is approximately proportional to the offline phase according to the alignment steps, since both the actor and critic are updated in the value alignment phase.

However, it is worth noting that although we set the training steps for value alignment at 500k, in some environments, only a few alignment steps are needed to calibrate the critic with the offline actor, as shown in Fig. 1 and Fig. 11. Only in antmaze environments, where it is hard for the critic to capture the sparse reward signal, more alignment steps are necessary. Additionally, in constrained fine-tuning, since only the lagrangian multiplier is added to be updated and the interaction cost dominates, the time cost increases very little.

Moreover, in O2O RL, we are typically not concerned about the time cost in the offline process, as different offline methods take different amounts of time. Instead, we prioritize the cost of interactions during online fine-tuning. Our method re-evaluates and aligns the critic with the offline actor solely within the offline dataset, making the time cost less critical.

## Appendix D Related Work

**Unified algorithms for offline-to-online** Many offline algorithms are unified across phases in O2O RL [32, 20, 13, 43, 44], who share the common philosophy of designing an RL algorithm that is suitable for both offline and online phases and then the network parameters trained in the offline phase can be reused for further learning in the online phase. Most of them are policy constraint methods which learn policy without querying OOD samples [20, 43] or penalize action probabilities of them by an explicit estimation of behavior policy [44], which is beneficial for offline performance but limits efficient performance improvements in online fine-tuning, as talked about in [33]. For explicit policy constraint, recent works focus on how to make the constraint adaptive [52, 26] or loosen it gradually [5]. Although such algorithms achieve great performance in offline phase, there is a lack of research on from such algorithms to more efficient online algorithms.

**Efficient online fine-tuning** Some recent works aim to online fine-tune efficiently with optimistic exploration. [15] propose a unified uncertainty-guided framework to explore optimistically in online fine-tuning phase and keep the offline constraint for OOD actions to avoid erroneous update. [30] and [51] utilize ensemble Q-learning to alleviate distribution shift, and implement optimistic exploration by some approaches about ensemble in online RL. For model-based O2O RL, [31] explores regions with high uncertainty and returns in learned model. [42] and [50] concatenate different offline and online algorithms by resetting a new online policy learning from the offline policy gradually, where optimistic exploration is implemented by the new policy. With a given policy and dataset, [3] focus how to recover the performance of the policy rapidly, whose setting is suitable for our method too.

**Stable online fine-tuning** As offline RL works for some important scenarios with high risk, the stable performance is considerable for O2O RL. After [23] put forward distribution shift problem in O2O RL which is alleviated by pessimistic Q-ensemble and balanced replay proposed by them, many methods are proposed to combat this problem. From offline to SAC, [33] mitigates the penalty for

Figure 12: The performance with a high Update-To-Data ratio of O2SAC. The solid lines and shaded regions represent mean and standard deviation.

OOD actions in CQL and calibrates Q-values of them by Monte Carlo returns, while [48] reconstructs Q-functions aligned with offline policy. [25] also induces KL divergence to policy objective, which is about current policy and the policy at the last iteration to perform a trust-region-style update, but the performance will drop suddenly at the beginning of online fine-tuning. [19] and [28] utilize environment dynamics model ensemble to obtain uncertainty to penalty OOD actions.

## Appendix E Detailed Discussions about Mismatch in SOTA Offline Algorithms

**CQL** For common implementation of CQL, [21] do not specify exactly how \(\pi_{k}\) is updated but only provide properties on the policy \(\exp(f_{k}(s,a)/Z(s))\) like SAC. In this way, the actor of CQL is only related to the critic, and the action probability is directly proportional to \(Q(s,a)\) like online RL. However, as the conservative policy evaluation operator used to update the Q-function (according to [21] Appendix C, equation 13), the policy is not only related to rewards, but also related to the distribution of behavior policy. The conservative policy evaluation operator in CQL is:

\[Q(s,a)=B^{\pi}Q(s,a)-\alpha[\frac{\mu(a|s)}{\pi_{\beta}(a|s)}-1]\] (21)

where \(B^{\pi}\) is the standard Bellman operator \(B^{\pi}Q(s,a)=r(s,a)+\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a)}[V(s^{ \prime})]\) and

\[V(s)=\mathbb{E}_{a\sim\pi(\cdot|s)}[Q(s,a)]\] (22)

If we define \(Q_{o}\) as the Q-function derived from Eq. (1), which satisfies:

\[Q_{o}(s,a)=B^{\pi}Q_{o}(s,a)\] (23)

Therefore, the relationship of conservative \(V(s)\) in CQL and \(Q_{o}(s,a)\) is:

\[V(s)=\mathbb{E}_{a\sim\pi(\cdot|s)}[Q_{o}(s,a)-\alpha[\frac{\mu(a|s)}{\pi_{ \beta}(a|s)}-1]]\] (24)

The objective of policy is to maximize \(V(s)\), so compared with online RL, there is a difference in learning objective due to pessimism. In fact, drawing from SAC, we can derive the objective of CQL:

\[\max_{\pi}\mathbb{E}[\sum\nolimits_{t=0}^{\infty}\gamma^{t}(r(s_{t},a_{t})- \alpha[\frac{\mu(a_{t}|s_{t})}{\pi_{\beta}(a_{t}|s_{t})}-1])]\] (25)

which is in according with _behavior-regularized_ MDP proposed by [47] at \(f(x)=x-1\). The conservative policy objective causes pessimistic Q-values estimation, hence in online fine-tuning, the change of policy objective leads to drastic Q-values jump and performance degeneration.

**IQL** As expectile regression used to estimate expectiles of the state value function with respect to random actions, \(Q(s,a)\) and \(V(s)\) are not induced by the policy \(\pi\), and we denote them as \(Q^{\mu}(s,a)\) and \(V^{\mu}(s)\), where \(\mu\) represents a special policy related to the expectile regression [18]. With initialization of such \(Q^{\mu}(s,a)\) and \(V^{\mu}(s)\) in online fine-tuning phase, if we update the actor and the critic in online algorithm, the actor update will tends to \(\mu\) at the beginning, which causes unknown performance change because the performance of \(\nu\) is unknown. And usually the performance will deteriorate significantly the probability of a satisfactory policy \(\nu\) is low.

On the other hand, the work of [47] reveals that the optimal critic of IQL can be derived from _behavior-regularized_ MDP at \(f(x)=\log(x)\). Meanwhile, the update of actor is derived from return maximization with a KL divergence constraint, it is easily to discovery that the learned policy is relevant to dataset. Therefore, the O2O RL for IQL still suffers from the problem of policy objective change. And compared with value regularization methods, there is not only pessimistic estimation problem but also initial misalignment problem, which means offline actor cannot be derived from offline critic in online update way. In fact, in offline RL, the policy update function is to maximize Eq. (9), which is highly related to behavior policy as \(\alpha\) is a fixed hyper-parameter. If 1 is directly used to fine-tune policy, the policy will tends to be only proportional to \(Q(s,a)\) of offline critic, hence the policy performance will drop sharply with a great probability.

**TD3+BC** With a explicit constraint of policy update, the improvement mismatch is obvious: the critic update follows the Bellman backup, but the actor update does not only follow Q-values maximization, that constrains the actor only update in a region close to the dataset. A little different from the mismatch in IQL, TD3+BC does not suffer from pessimistic estimation since the critic is updated in an optimistic way as the online way, but such improvement mismatch still leads to performance drop. The probability of action with the largest Q-value may be not largest, as talked about in [48], which is not in according with online RL and leads to erroneous update to destroy performance at the beginning of online fine-tuning. Such improvement mismatch exists in almost all explicit constraint methods, but their critics are optimistic because the update ways is the same as online RL, so value alignment is necessary for O2O RL from such offline methods.

## Appendix F Proof

**Proof of Corollary 4.2** The theoretical analysis about FQE can be found in **Theorem 4.2** of [22] or **Theorem 4.9** of [29], here we use the latter result. Full proof is similar to the proof of [29], but a little different from [29], we fix the evaluated policy as the offline policy.

Here we simply show the proof process and emphasis the difference, see [29] for more details.

Since FQE deals with the following regression problem

\[Q_{k}\leftarrow\operatorname*{arg\,min}_{Q\in F}\sum_{i=1}^{|\mathcal{D}|}[Q( s_{i},a_{i})-r_{i}-\gamma Q_{k-1}(s^{\prime}_{i},\pi(s^{\prime}_{i}))]^{2},\] (26)

we can have

\[\begin{split}&|r_{i}+\gamma Q_{k-1}(s^{\prime}_{i},\pi(s^{\prime}_{i} ))|\leq 1+\gamma\bar{V}\leq 2\bar{V}\\ &|\mathcal{T}^{\pi}Q_{k-1}(s,a)|=|r(s,a)+\gamma\mathbb{E}_{s^{ \prime}\sim P(\cdot|s,a)}Q_{k-1}(s^{\prime},\pi(s^{\prime}))|\leq 1+\gamma \bar{V}\leq 2\bar{V}\end{split}\] (27)

With the inherent Bellman evaluation error \(d_{F}^{\pi}\), we can apply least squares generalization bound here (Lemma A.11 in [2]). With probability at least \(1-\delta\), we have

\[\|Q_{k}-\mathcal{T}^{\pi}Q_{k-1}\|_{2,\rho^{\beta}}^{2}\leq\frac{22\bar{V}^{2 }\log(|\mathcal{F}|/\delta)}{|\mathcal{D}|}+20d_{F}^{\pi}\] (28)

Note that here we fix the policy \(\pi\) as the offline policy, that means for a given \(Q_{0}\), \(Q_{k-1}\) is well-determined, so we do not need to apply a union bound over all possible \(Q_{k-1}\).

Then we first bound \(\|Q_{k}-Q^{\pi}\|_{2,d_{t}^{\pi}\times\pi}\) as the same as [29] did.

\[\begin{split}\|Q_{k}-Q^{\pi}\|_{2,d_{t}^{\pi}\times\pi}& =\|Q_{k}-\mathcal{T}^{\pi}Q_{k-1}+\mathcal{T}^{\pi}Q_{k-1}-Q^{ \pi}\|_{2,d_{t}^{\pi}\times\pi}\\ &\leq\|Q_{k}-\mathcal{T}^{\pi}Q_{k-1}\|_{2,d_{t}^{\pi}\times\pi} +\|\mathcal{T}^{\pi}Q_{k-1}-Q^{\pi}\|_{2,d_{t}^{\pi}\times\pi}\\ &\leq\sqrt{\bar{C}}\|Q_{k}-\mathcal{T}^{\pi}Q_{k-1}\|_{2,\mu}+\| \mathcal{T}^{\pi}Q_{k-1}-\mathcal{T}^{\pi}Q\|_{2,d_{t}^{\pi}\times\pi}\text{ (Assumption \ref{eq:def})}\\ &=\sqrt{\bar{C}\epsilon}+\sqrt{\mathbb{E}_{(s,a)\sim d_{t}^{\pi} \times\pi}[\gamma\mathbb{E}_{s^{\prime}\sim P(\cdot|s,a),a^{\prime}\sim\pi( \cdot|s^{\prime})}(Q_{k-1}(s^{\prime},a^{\prime})-Q^{\pi}(s^{\prime},a^{\prime }))]^{2}}\\ &\leq\sqrt{\bar{C}\epsilon}+\gamma\sqrt{\mathbb{E}_{(s,a)\sim d_{t} ^{\pi}\times\pi,s^{\prime}\sim P(\cdot|s,a),a^{\prime}\sim\pi(\cdot|s^{\prime} )}(Q_{k-1}(s^{\prime},a^{\prime})-Q^{\pi}(s^{\prime},a^{\prime}))^{2}}\\ &=\sqrt{\bar{C}\epsilon}+\gamma\|Q_{k-1}-Q^{\pi}\|_{2,d_{t}^{ \pi}\times\pi}\end{split}\] (29)

The fifth derivation uses Jensen's inequality. With

\[\|Q_{K}-Q^{\pi}\|_{2,d_{t}^{\pi}\times\pi}\leq\sum_{k=0}^{K-1}\gamma^{k}\sqrt {\bar{C}\epsilon}+\gamma^{K}\|Q_{0}-Q^{\pi}\|_{2,d_{t}^{\pi}\times\pi}\leq \frac{1-\gamma^{K}}{1-\gamma}\sqrt{\bar{C}\epsilon}+\gamma^{K}\bar{V}\] (30)Therefore

\[\begin{split}|Q^{\pi}-\dot{Q}^{\pi}|&=|Q^{\pi}-Q_{K}|(K \,\rightarrow\infty)\\ &\leq\|Q_{K}-Q^{\pi}\|_{2,d^{\pi}\times\pi}(\text{Jensen}^{\prime} \text{sinequality})\\ &=\sqrt{\mathbb{E}_{d^{\pi}}\mathbb{E}_{a\sim\pi(\cdot|s)}[Q_{K}( s,a)-Q^{\pi}(s,a)]}\\ &=\sqrt{\sum_{s}(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}d_{t}^{ \pi}(s)\sum_{a}\pi(a|s)[Q_{K}(s,a)-Q^{\pi}(s,a)]}\\ &=\sqrt{(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\sum_{s}d_{t}^{ \pi}(s)\sum_{a}\pi(a|s)[Q_{K}(s,a)-Q^{\pi}(s,a)]}\\ &=\sqrt{(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}\|Q_{K}-Q^{\pi}\|_ {2,d_{t}^{\pi}\times\pi}^{2}}\\ &\leq\sqrt{(1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}(\frac{1-\gamma ^{K}}{1-\gamma}\sqrt{C\epsilon}+\gamma^{K}\bar{V})^{2}}\\ &=\frac{1-\gamma^{K}}{1-\gamma}\sqrt{C\epsilon}+\gamma^{K}\bar{ V}\end{split}\] (31)

**Proof of Proposition 4.3** We define \(Q_{\text{fqe}}(s,a)\) as the state-action values for some actions after the process of policy re-evaluation and lower than the calibrated value, i.e. \(Q_{\text{fqe}}(s,a)\leq Q(s,\dot{a})-\alpha\left(\log\pi_{\text{off}}\left( \dot{a}|s\right)-\log\pi_{\text{off}}\left(a|s\right)\right)\). With the _min_ operator in Eq. (12), we can divide actions into two categories, _calibrated actions_ or _standard actions_. The former refer to those actions with Q-values to be calibrated, and the latter refer to those actions with unchanged Q-values since they are lower than the calibrated values. We denote them as \(a_{\text{cal}}\) and \(a_{\text{fqe}}\) respectively.

\[\begin{split} V_{\text{align}}(s)&=\mathbb{E}_{a \sim\pi_{\text{off}}(\cdot|s)}[Q_{\text{align}}(s,a)-\alpha\log\pi_{\text{ off}}(a|s)]\\ &=\sum\pi_{\text{off}}(a|s)[Q_{\text{align}}(s,a)-\alpha\log\pi_{ \text{off}}(a|s)]\\ &=\sum\pi_{\text{off}}(a_{\text{cal}}|s)[Q_{\text{cal}}(s,a_{ \text{cal}})]+\sum\pi_{\text{off}}(a_{\text{fqe}}|s)[Q_{\text{fqe}}(s,a_{ \text{fqe}})]+\\ &\quad\sum\pi_{\text{off}}(a|s)[-\alpha\log\pi_{\text{off}}(a|s)] \end{split}\] (32)

Due to the _min_ operator, for _standard actions_\(a_{\text{fqe}}\), the calibrated Q-values satisfy \(Q_{\text{fqe}}(s,a_{\text{fqe}})\leq Q_{\text{cal}}(s,a_{\text{fqe}})\), therefore

\[\begin{split} V_{\dot{a}}(s)&=Q(s,\dot{a})-\alpha \log\pi\left(a|s\right)\\ &=\sum\pi_{\text{off}}(a|s)[Q_{\text{cal}}(s,a)-\alpha\log\pi_{ \text{off}}(a|s)]\\ &\geq\sum\pi_{\text{off}}(a_{\text{cal}}|s)[Q_{\text{cal}}(s,a_{ \text{cal}})]+\sum\pi_{\text{off}}(a_{\text{fqe}}|s)[Q_{\text{fqe}}(s,a_{ \text{fqe}})]+\\ &\quad\sum\pi_{\text{off}}(a|s)[-\alpha\log\pi_{\text{off}}(a|s)] \\ &=V_{\text{align}}(s)\end{split}\] (33)

On the other hand, it is easy to see that \(Q_{\text{fqe}}(s,a_{\text{fqe}})-\alpha\log\pi\left(a_{\text{fqe}}|s\right) \leq Q_{\text{cal}}(s,a_{\text{cal}})-\alpha\log\pi\left(a_{\text{cal}}|s\right)\), so

\[\begin{split} V_{\text{fqe}}(s)&=Q_{\text{fqe}}(s,a_ {\text{fqe}})-\alpha\log\pi\left(a_{\text{fqe}}|s\right)\\ &\leq\sum\pi_{\text{off}}(a_{\text{fqe}}|s)[Q_{\text{fqe}}(s,a_{ \text{fqe}})-\alpha\log\pi_{\text{off}}\left(a_{\text{fqe}}|s\right)]+\\ &\quad\sum\pi_{\text{off}}(a_{\text{cal}}|s)[Q_{\text{cal}}(s,a_ {\text{cal}})-\alpha\log\pi_{\text{off}}\left(a_{\text{cal}}|s\right)]\\ &=V_{\text{align}}(s)\end{split}\] (34)

So \(V_{\text{fqe}}(s)\leq V_{\text{align}}(s)\leq V_{\dot{a}}(s)\), i.e. Proposition 4.3 is proved.

**Proof of Proposition 4.4** With simplicity, we consider the term of the auxiliary advantage function without probability ratio clipping

\[-\mathop{\mathbb{E}}_{s\sim R_{\alpha}\sim\pi_{\theta_{k}}(\cdot|s) }\mathop{\mathbb{E}}_{\pi_{\theta_{k}}}(\frac{\pi_{\theta}}{\pi_{\theta_{k}}}A_{ \alpha}(s,a)) =-\mathop{\mathbb{E}}_{s\sim R_{\alpha}\sim\pi_{\theta}(\cdot|s) }[A_{\alpha}(s,a)]\] \[=-\mathop{\mathbb{E}}_{s\sim R}\sum\pi_{\theta}(a|s)A_{\alpha}(s,a)\] \[=\mathop{\mathbb{E}}_{s\sim R}[-\sum\pi_{\theta}(a|s)\log\pi_{ \text{ref}}(a|s)-\sum\pi_{\theta}(a|s)\mathcal{H}(\pi_{\text{ref}}(\cdot|s)]\] \[=\mathop{\mathbb{E}}_{s\sim R}[CELoss(\pi_{\theta}(\cdot|s),\pi_{ \text{ref}}(\cdot|s))-\mathcal{H}(\pi_{\text{ref}}(\cdot|s))\sum\pi_{\theta}(a |s)]\] \[=CELoss(\pi_{\theta},\pi_{\text{ref}})+C\] (35)

Therefore, the term of the auxiliary advantage function regularizes the policy update near the reference policy, thereby ensuring reliable update even with inaccurate value estimation. And at the beginning of online fine-tuning, the reference policy is initialized from \(\pi_{\text{off}}\).

**Proof of Corollary 4.5** Since our constrained fine-tuning method solves a constrained MDP problem by the method in [41], Theorem 2 in [41] still holds in our method, which can be described as _With the constraint satisfied, RCPO algorithm converges almost surely to a fixed point (\(\theta^{\star}(\mu^{\star},\lambda^{\star}),\mu^{\star}(\lambda^{\star}),\lambda ^{\star}\))_.

Such theorem guarantees the convergence of RCPO, and with the our proposed constraint, we can utilize contradiction to proof Corollary 4.5.

**Assumption** According to the convergence of RCPO, we can assume the convergent point \((\theta^{\star}(\mu^{\star},\lambda^{\star}),\mu^{\star}(\lambda^{\star}), \lambda^{\star})\) with \(\lambda^{\star}\neq 0\).

**Derivation** As the constraint is \(\mathop{\mathbb{E}}_{\pi}[f(\pi(a_{t}|s_{t}),\pi_{\text{ref}}(a_{t}|s_{t}))]<\alpha\) and \(\pi_{\text{ref}}\) is the best one among old policies during online evaluations, \(\pi_{\text{ref}}=\pi^{\star}\) when the algorithm converges, so the constraint term tends to \(-\alpha\). According to the update function Eq. (20) of \(\lambda\), \(\lambda\) tends to be reduced, which is in contradiction to the Assumption. Therefore, \(\lambda^{\star}=0\) when the algorithm converges.

Meanwhile, when \(\lambda^{\star}=0\), the constraint is removed, hence the \(\pi^{\star}\) and \(Q^{\star}\) are the same with the optimal results in the MDP without constraint, which means our method converges the optimal point like online RL but keeps stable improvement.

## Appendix G Algorithm Details

### O2Sac

**the choice of \(\alpha\)**

Although \(\alpha\) is generally smaller than 1 after offline training, as we use the energy policy to align critic with actor, a small \(\alpha\) has no influence on the recovery of offline policy but leads to a wide distribution for policy, which means the policy modeled by Gaussian distribution has a large standard deviation, which is harmful for online exploration. If the standard deviation is large, some OOD actions may be taken and dangerous state may arise during online exploration, which dose not suit the setting of high risk scenarios and affects favourable performance. Therefore we choose a suitable \(\alpha\) which is smaller than 1 but not excessively small. For Mujoco locomotion tasks, we determine that alpha is 0.2 for the dataset with medium quality, and is 0.5 for the dataset with expert quality, because small standard deviation induced by a large \(\alpha\) is advantageous for online exploration of well trained policy. For AntMaze navigation tasks, we set alpha as 0.5 for _medium_ and _large_ datasets and 0.2 for _unaze_ datasets.

Figure 13: Policy performance during value alignment with different \(\alpha\)Moreover, as we use _min_ operator for value alignment, the limit for \(\alpha\) is not so strict. If Q-values induced by policy re-evaluation is smaller than alignment objective, as a Gaussian distribution is used to fit energy policy, such Q-values will leads to a smaller standard deviation.

Note that the choice of \(\alpha\) has no effect on the recovery of offline policy, as shown in Figure 13.

**policy re-evaluation** As talked about Section 4.1, with a given \(\alpha\), online policy evaluation of SAC Eq. (3) can be used to policy re-evaluation to obtain an optimistic critic.

**value alignment** In Section 4.2, the value alignment method to O2SAC has been described in detail. In brief, we use online policy improvement Eq. (2) to get the actions which have overestimated Q-values, and use Eq. (13) to inhibit their Q-values to a reliable value calculated by maximum entropy RL.

According to the _min_ operator, there is no change if Q-values induced by policy re-evaluation is smaller than alignment objective, so our value alignment method not only aligns Q-values with actor but also is as consistent with the results of online evaluation as possible, which reduces Bellman error in online fine-tuning to keep Q-values more stable.

**constrained fine-tuning** For O2SAC, the constraint is KL divergence, and the policy objective of CMDP in O2O RL is:

\[\max\mathbb{E}_{\pi}[\sum_{t=0}^{\infty}\gamma_{t}(r_{t}(s_{t},a_{t})+\alpha H (\pi(\cdot|s_{t})))]\qquad\mathrm{s.t.}\ \mathbb{E}_{\pi}[\log(\frac{\pi(a_{t}|s_{t})}{\pi_{\text{ref}}(a_{t}|s_{t})}) ]<\tau\] (36)

And corresponding loss functions are:

\[L(\theta)=\max\mathbb{E}_{\pi_{\theta}}[Q_{\mu}^{\pi_{\theta}}(s,a)-\alpha \log\pi_{\theta}(a|s)-\lambda\log(\frac{\pi_{\theta}(a|s)}{\pi_{\text{ref}}(a| s)})]\] (37)

\[L(\mu_{i})=\min\underset{(s,a,r,s^{\prime})\sim R}{\mathbb{E}}[\big{(}Q_{\mu _{i}}^{\pi_{\theta}}(s,a)-y\big{)}^{2}]\] (38)

\[y=r+\gamma\underset{a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})}{\mathbb{E} }\left[Q_{\bar{\mu}}^{\pi_{\theta}}\left(s^{\prime},a^{\prime}\right)-\alpha \log\pi_{\theta}(a^{\prime}|s^{\prime})-\lambda\log(\frac{\pi_{\theta}(a^{ \prime}|s^{\prime})}{\pi_{\text{ref}}(a^{\prime}|s^{\prime})})\right]\]

\[L(\lambda)=\min_{\lambda\geq 0}-\lambda\underset{s\sim R,a\sim\pi_{\theta}( \cdot|s)}{\mathbb{E}}[\log(\frac{\pi_{\theta}(a|s)}{\pi_{\text{ref}}(a|s)})-\tau]\] (39)

A constraint related to offline policy is necessary for online fine-tuning, because OOD states will appear surely during online exploration, especially in narrow dataset, which may lead to erroneous overestimation and destroy old policy. Such constraint can avoid overestimation of actions far away from current policy, hence trust-region style update guarantees stable performance improvement.

Compared with the constraint of current policy and offline policy, our proposed constraint guarantees more optimal update because the policy updated in trust-region of offline policy generally has similar performance to offline policy, which leads to not much performance improvement.

And compared with the tight constraint of current policy and the policy at last iteration, our constraint guarantees rapid recovery when some erroneous update occurs which results in performance degradation, because the Lagrange multiplier \(\lambda\) will be larger and make the Q-values of OOD actions lower, hence the policy tends to be close to \(\pi_{\text{ref}}\). In such situation, our method can recover a similar performance to \(\pi_{\text{ref}}\) for current policy rapidly, but the tight constraint needs to take much time to do that as it constrains the current policy close to the poor policy at last iteration.

### O2td3

**policy re-evaluation** Similar to O2SAC, online policy evaluation of TD3 Eq. (6) can be used directly to evaluate an optimistic critic, and in TD3, there is no need for \(\alpha\).

**value alignment** As TD3 models a deterministic policy, value alignment method can not be derived directly from the relationship of the actor and the critic like O2SAC. However, from Eq. (5), the gradient of policy is only related to the the gradient of Q(s, a) to a if we fix the policy as offline policy.

So, we have two insights for the actor in TD3: \(Q(s,\pi(s))\) is the maximal in \(Q(s,\cdot)\) and the gradient around \(\pi(s)\) should tend to 0, and the latter also corresponds to the idea of policy smoothing update in TD3.

Note that in most cases, the correlation among different dimensions of action is not be considered, hence we use one-dimensional Gaussian distribution for the following analysis.

Therefore, with the definition that \(\dot{a}=\pi(s)\), we consider that normalized Q-values around \(\dot{a}\) follow a Gaussian distribution \(Q(s,a)/Q(s,\dot{a})\sim N(\dot{a},\Sigma)\). And for \(Q(s,\dot{a}+\delta)\), we utilize Taylor expansion of first order and omit higher order terms, then we can get:

\[Q(s,\dot{a}+\delta) \approx Q(s,\dot{a})+\nabla_{a}Q(s,\dot{a})\cdot\delta\quad(\text{ Taylor expansion})\] (40) \[\approx Q(s,\dot{a})+\nabla_{a}Q(s,\dot{a}+\delta)\cdot\delta\] (41) \[= Q(s,\dot{a})-\frac{\delta^{2}}{\Sigma}Q(s,\dot{a}+\delta)\] (42)

From Eq. (40) to Eq. (41), we consider the continuous derivative. From Eq. (41) to Eq. (42), we utilize the derivative of a Gaussian distribution \(\nabla_{x}f(x)=-(x-\mu)/\Sigma\), where \(f(x)\) is a one-dimensional Gaussian distribution \(f(x)\sim N(\mu,\Sigma)\).

Note that \(Q(s,\dot{a})=Q(s,\dot{a})/\sqrt{2\pi\Sigma}\), which means \(\Sigma=1/2\pi\), so we can get:

\[Q(s,\dot{a}+\delta)=\frac{1}{1+\delta^{2}/\Sigma}Q(s,\dot{a})=\frac{1}{1+2\pi \delta^{2}}Q(s,\dot{a})\] (43)

In our implementation, we replace Eq. (43) to Eq. (15), where \(k\) is used to control penalty for overestimated values, and as action range is from -1 to 1, the alignment objective avoids severe underestimation because the minimum Q does not tends to 0 if we set a small \(k\), which also reduces Bellman error during online update as the same thing as _min_ operator does. And we use the euclidean distance divided by the square root of action dimension to calculate \(\delta\), which is \(\delta(a,\dot{a})=\sqrt{\sum{(a_{i}-\dot{a}_{i})^{2}}}/{|A|}\).

Note that in O2TD3, we use the same trick of reward normalization as TD3+BC, which means we subtract 1 from all rewards, so we need to consider the situation of negative rewards. In Eq. (43), we set \(Q(s,a)/Q(s,\dot{a})\sim N(\dot{a},\Sigma)\) when rewards are positive, so it is natural to set \(Q(s,\dot{a})/Q(s,a)\sim N(\dot{a},\Sigma)\) at the situation of negative rewards. Therefore, for the environment with negative Q-values, we use following way to redress critic:

\[Q(s,\dot{a}+\delta)=(1+k\delta^{2})Q(s,\dot{a})\] (44)

In a word, we multiply \(Q(s,a)\) by \((1+k\delta^{2})\) if \(Q(s,a)>0\), otherwise divide it by \((1+k\delta^{2})\).

**constrained fine-tuning** For O2TD3, the constraint is MSE loss, so the loss functions are:

\[L(\theta)=\max\mathbb{E}_{\pi_{\theta}}[Q^{\pi_{\theta}}-\lambda(\pi_{\theta} (s)-\pi_{\text{ref}}(s))^{2}]\] (45)

\[L(\mu_{i})=\min\limits_{(s,a,r,s^{\prime})\sim R}[\big{(}Q^{\pi_{\theta}}_{ \mu_{i}}(s,a)-y\big{)}^{2}]\] (46)

\[y=r+\gamma\mathop{\mathbb{E}}_{a^{\prime}\sim\pi_{\theta}(\cdot|s^{\prime})} \big{[}Q^{\pi_{\theta}}_{\mu}\left(s^{\prime},a^{\prime}\right)-\lambda(\pi_{ \theta}(s)-\pi_{\text{ref}}(s))^{2}\big{]}\]

\[L(\lambda)=\min\limits_{\lambda\geq 0}-\lambda\mathop{\mathbb{E}}_{s\sim R}[(\pi_{ \theta}(s)-\pi_{\text{ref}}(s))^{2}-\tau]\] (47)

### O2ppo

**policy re-evaluation** Different from O2SAC and O2TD3, as Eq. (8) is not related to offline policy, in practice we can only obtain \(V^{\mu}(s)\) through fitting the returns. Advantages \(A^{\pi_{\text{ref}}}(s,a)\) computed by such \(V^{\mu}(s)\) may be awfully incorrect, which sequentially causes error update. However, in order to follow the update way of PPO (only \(V(s)\) is used to update), we stick to update state value function by fitting the returns, and we introduce a modification to advantages used to update.

**value alignment** As \(V^{\pi_{\text{ref}}}(s)\) obtained in policy re-evaluation is actually \(V^{\mu}(s)\), which is incorrect to compute advantages \(A^{\pi}(s,a)\) for on-policy update, we propose an auxiliary advantage to redress erroneous update. Let us think about the property of the auxiliary advantage. First, as an advantage function, its mathematical expectation of offline policy \(\pi_{\text{off}}\) should be 0. Second, the function should be able to output positive values and negative values for different actions to distinguish the quality of actions. Last, better actions should correspond higher values. Drawing from the policy form of SAC, we propose the auxiliary advantage as:

\[A_{\alpha}(s,a)=\alpha\log\pi_{\text{off}}(a|s)+\alpha\mathcal{H} (\pi_{\text{off}}(\cdot|s))\] \[\mathcal{H}(\pi_{\text{off}}(\cdot|s))=-\mathbb{E}_{a\sim\pi_{ \text{off}}(\cdot|s)}[\log(\pi_{\text{off}}(a|s))]\] (48)

When the policy is modeled as Gaussian distribution, it satisfies: (1) \(\mathbb{E}_{a\sim\pi_{\text{off}}(\cdot|s)}[A_{\alpha}(s,a)]=0\). (2) \(A_{\alpha}(s,a)>0\) when \(\|a-\mu\|_{2}<\sigma\). (3) \(A_{\alpha}(s,a)\propto\log\pi_{\text{off}}(a|s)\). And such properties are in accord with requirements of auxiliary advantage function.

It is notable that here is an implicit assumption that the actions with higher probability for \(\pi_{\text{off}}\) are better, and for a well trained offline policy, such assumption should be valid, at least for the beginning of online fine-tuning. With such auxiliary advantage function, we can redress advantages computed by incorrect \(V^{\pi}(s)\) to ensure the stable performance improvement during online fine-tuning, especially for the beginning stage.

**constrained fine-tuning** As the auxiliary advantage function has the ability to constrain policy to update in a reliable region near \(\pi_{\text{off}}\), that means \(\pi_{\text{ref}}=\pi_{\text{off}}\), we just need to replace \(\pi_{\text{ref}}\) as the optimal policy during online evaluations to constrain online fine-tuning. Meanwhile, with the increase of interactions, the critic gradually becomes accurate, approximating \(V^{\pi}\). And on-policy method is naturally stable to improve performance by updating in a reliable region, therefore the constraint factor \(\beta\) anneals to 0 from 1 during online fine-tuning.

## Appendix H Implementation Details

### baseline implementation

**Offline results** For obtaining offline policy, we choose CQL [21], IQL [20] and TD3+BC [10], and we reproduce offline results according to a deep offline RL library CORL [40] https://github.com/tinkoff-ai/CORL, all hyper-parameters are the same with the official implementations. The offline results are used in our methods, Off2On and PROTO.

1. **CQL** We reproduce the results of CQL by the code from CORL https://github.com/tinkoff-ai/CORL/blob/main/algorithms/offline/cql.py.
2. **IQL** Like the implementation of CQL, we reproduce the results of IQL by the code from CORL https://github.com/tinkoff-ai/CORL/blob/main/algorithms/offline/iql.py.
3. **TD3+BC** Similar to above, we reproduce the results of TD3+BC by the code from CORL https://github.com/tinkoff-ai/CORL/blob/main/algorithms/offline/td3_bc.py.

**Online fine-tuning results** For offline-to-online algorithms, for most methods, we reproduce the results according to their official implementations. For AWAC [32], IQL [20] and Cal-QL [33], we reproduce the results by the code from CORL [40]. For ACA [48] and PEX [50], we reproduce the results by the official open-source code https://github.com/ZishunYu/Actor-Critic-Alignment, https://github.com/Haichao-Zhang/PEX.

Although PROTO focus on online fine-tuning, but initialize the policy from other offline algorithms in the official implementation. For the sake of fairness, we rewrite the code according to the paper and official code (https://github.com/Facebear-ljx/PROTO) of PROTO in Pytorch, and we use the offline results of CQL and TD3+BC for initialization respectively.

In addition, in the official implementation of Off2On [23], the policy update 1,000 times per 1,000 environment steps, that is different from the common implementation. Therefore, we reproduce by using all parts that related to the prioritized replay in the official code from https://github.com/shlee94/Off2OnRL, and we use offline results of CQL for initialization with the ensemble size of 5, that is the same as the official paper and implementation.

All hyper-parameters are the same as the paper. And as O2O baseline methods are all off-policy methods, they are allowed to interact with the environment in 100,000 steps.

### General implementation of our methods

Network ArchitectureAs we need to re-evaluate policy, which means we only need offline policy and do not use the offline critic, we can modify the critic network architecture for stable online fine-tuning. In our implementation, we adopt the same network architecture as offline phase but apply Layer Normalization (LayerNorm) for the output of hidden layers after activation function.

[49] indicate that in offline RL, LayerNorm is a good solution to effectively avoid divergence without introducing detrimental bias, leading to superior performance. Moreover, [4] find that LayerNorm is favourable for efficient online RL with offline data. Therefore, for stable evaluation and future consideration, we decide to apply LayerNorm to online critic network.

**Initialization of online replay buffer** We test our method by initializing the online replay buffer with four different types: (1) Initialize the buffer with the entire offline dataset akin to [20]. (2) Conduct a separate online buffer and sample symmetrically from both offline dataset and online buffer akin to [4]. (3) Initialize the buffer with a small number of offline data with high quality akin to [48]. (4) Initialize the buffer without any offline data. For simplicity, we denote them as _All, Half, Part, Null_ respectively.

As the results shown in 6, there is little difference among different ways of initialization, except for \(all\), as the the quality of the offline dataset may be low. Although the initialization ways of \(part\) and \(null\) also show the competitive results, however, to be consistent with [4] and future efficient update, we decide to adopt _Half_ as our implementation way. It is notable that online policy is still favourable even if we adopt _Null_ initialization, thanks to our adaptive constrained fine-tuning.

**loss weight for \(\lambda\) update** Since \(\lambda\) in Eq. (20) is a variable applied in all states, it may decrease greatly in a batch. In order to considering more about the situation that needs to be constrained, we modify the term about \(\lambda\) in the loss Eq. (20) with a weight.

\[L(\lambda)=\min_{\lambda\geq 0}-\lambda\left[\mathbb{E}_{\pi_{\theta}}\omega(f (\pi_{\theta}(a|s),\pi_{\text{ref}}(a|s)))-\tau\right]\] (49)

where \(\omega=|0.7-\mathbb{I}((f(\pi_{\theta}(a|s),\pi_{\text{ref}}(a|s)))>\tau)|\) gives a large weight to negative coefficients, thereby constraining the abrupt decrease of \(\lambda\).

**The choice of the initial value of \(\lambda\)** As \(\lambda\) is the Lagrange multiplier which is adaptive to the constraint, there is no need to design the initial value of \(\lambda\) specially. For _medium_ and _large_ datasets of Antmaze tasks, we set the initial value as 2.0 since the initial performance is low, and we set the initial value as 2.0 for other tasks.

### O2SAC implementation

**The choice of constraint threshold \(\tau\)** As we consider the constraint in O2SAC is KL divergence, and we model policy as a squashed Gaussian distribution, we can directly the KL divergence of Gaussian distribution to approximate the constraint. Since \(\pi_{\text{ref}}\) is the best one among old policies during online evaluations and it will be close to \(\pi_{\theta}\) with the performance improvement, we can assume that the standard deviation of \(\pi_{\text{ref}}\) is the same as the one of \(\pi_{\theta}\). Therefore, according to the KL divergence of Gaussian distribution, we can get:

\[D_{KL}(N(\mu_{\theta},\sigma^{2}),N(\mu_{b},\sigma^{2}))=\frac{(\mu_{\theta}- \mu_{b})^{2}}{2\sigma^{2}}\] (50)

Therefore, we can determine constraint threshold \(\tau\) based on the magnitude of change in the mean. For medium dataset, we constrain that \(|\mu_{\theta}-\mu_{b}|<\sigma\), hence \(\tau=0.5\). However, due to different standard deviations, this threshold may not accurate, and it is an intuitive idea to loosen the constraint at a later stage. So we set \(\tau\) as a linearly increasing variable from 0.125 to 2.0 for _medium_ and _medium-replay_ datasets, which means the allowable range of policy distribution mean is from \(\sigma/2\) to

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & _All_ & _Half_ & _Part_ & _Null_ \\ \hline Total scores & 163.08 & 225.54 & 220.26 & 201.63 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Results for different initialization of online replay buffer.

\(2\sigma\). And for _medium-expert_ and _expert_ datasets, we set it from 0.005 to 0.125 for safe update, which means the allowable range of policy distribution mean is from \(\sigma/10\) to \(\sigma/2\).

**Clipped log likelihood** Due to the limit of precision of Pytorch and the squashed Gaussian policy used in O2SAC, when the output action \(a\) is near 1.0, like 0.99999999, the log likelihood \(\log\pi(a|s)\) is will be severely low as the value of action will be computed as 1.0, which will occurs when the log likelihood is need to be computed by given a action in value alignment phase and constrained fine-tuning phase. Therefore, for simple calculation, we set the minimum value of log likelihood is -50, a enough small number, which has litter influence on results.

### O2TD3 implementation

**The choice of constraint threshold \(\tau\)** As TD3 models deterministic policy, it is unable to determine the constraint threshold according to the standard deviation. However, when we inspect the interaction process of TD3, we can find that exploration noise is akin to the standard deviation. So we determine \(\tau\) from the exploration noise. First we set exploration noise as 0.1 for medium dataset, which is the same as TD3 learning from scratch. And for expert dataset, we set exploration noise as 0.05 because offline policy is well trained in such dataset, and a lower exploration noise is favourable to avoid poor action samples.

After determination of exploration noise, with the idea of the equivalence of the standard deviation and the exploration noise, we set \(tau\) similar to O2SAC. For _medium_ and _medium-replay_ datasets, \(tau\) grows linearly from 0.0025 to 0.01, which means the allowable range of policy distribution mean is from \(\sigma/2\) to \(2\sigma\), when we consider the exploration noise is equal to the standard deviation. Similarly, for _medium-expert_ and _expert_ datasets, we set it from 0.000025 to 0.000625.

### O2PPO implementation

We implement our O2PPO basically according to the code of Uni-O4 [24].

**The decay rate of \(\beta\)** In our implementation for O2PPO, we set the interaction steps as 250,000 since PPO is an on-policy method which improves performance more slowly than off-policy methods. Since we keep the idea that in _medium-expert_ and _expert_ dataset, the offline policy is well learned, we decay \(\beta\) from 1 to 0 linearly in 500,000 steps, which means in the end of our fine-tuning, \(\beta=0.5\). And for policies trained in other datasets, including AntMaze tasks and _medium_ and _medium-replay_ datasets of Mujoco tasks, we decay \(\beta\) from 1 to 0 linearly during 250,000 steps to reserve more potential of performance improvement.

**Clipped standard deviation** As shown in 14, the policy trained by IQL algorithm has a large standard deviation which makes poor exploration performance, especially in _hopper-medium-expert-v2_ and _hopper-expert-v2_. Therefore, we set the maximum value of the standard deviations of policies trained in the two datasets as 0.05, which corresponds to the set of exploration noise in O2TD3, because for a deterministic policy, exploration noise and standard deviation of a stochastic policy are equivalent during taking actions in exploration.

**Shaped and weighted auxiliary advantage** For one-dimensional Gaussian distribution \(f=N(\mu,\sigma^{2})\), the entropy is \(\log(\sqrt{2\pi e\sigma^{2}})\), which is equal to \(\log(f(\mu\pm\sigma))\), hence the maximum value of \(A_{\alpha}(s,a)\) in (48) is \(1/2\). However, the minimum value of it may be much low, which leads to unstable training. Therefore, we use SoftPlus activation function to clip the range of \(A_{\alpha}(s,a)\). For each dimensional, we clip \(A_{\alpha}(s,a)\) as follows:

\[\mathrm{Clip}(A_{\alpha}(s,a))\coloneqq\mathrm{SoftPlus}(A_{\alpha}(s,a)+4)-4\] (51)

By such clipping operator, the range of \(A_{\alpha}(s,a)\) is \((-4,1/2)\) approximately. Note that when the current policy is close to the reference policy, the actions with overly low logarithmic values are rare, thereby having a little effect on the results.

Note that in the implementation of PPO, the normalization of advantages is needed. In order to make \(A_{\alpha}(s,a)\) and \(A(s,a)\) the same order of magnitude, we reweight \(A_{\alpha}(s,a)\) by multiplying a coefficient that is the double value of the standard deviation of \(A(s,a)\). Therefore, the maximum value of \(A_{\alpha}(s,a)\) is the standard deviation of the batch of \(A(s,a)\).

## Appendix I Connect Different Offline and Online RL Methods

Our methods permit connecting different offline and online RL methods as only offline policy is needed in our methods. Therefore, for those RL-based methods, which means the policy is modeled as a stochastic policy or deterministic policy in traditional RL way, it is easy to implement stable online fine-tuning by our methods. For those methods which model policy with different form, such as decision transformer (DT) [6], our methods can be applied easily if a stochastic policy or deterministic policy is used to clone the offline policy.

For behavior cloning of a stochastic policy, the easiest way is to Maximize log likelihood, however, we recommend the form akin to [53], as follows:

\[\min_{\theta}\mathbb{E}_{(s\sim D,a\sim\pi_{\theta}(\cdot|s))}[-\log(\pi_{ \theta}(a|s))-\lambda[\mathcal{\tilde{H}}(\pi_{\theta}(\cdot|s)]]\] (52)

The purpose to maximize the entropy is to avoid an excessively narrow distribution, which may cause drastic jump of Q-values in O2SAC as \(\alpha\) will decay to zero quickly. However, in our experiment, such a way of behavior cloning will leads to performance degeneration, which is a question for imitation learning, It is more easy for the behavior cloning of the deterministic policy, MSE loss can be used to update the policy for the states in offline dataset.

It is notable that degree of coverage has an important influence on behavior cloning, which is still an intractable challenge in imitation learning. Our methods provide the opportunity for the application of advanced imitation learning to O2O RL. In addition, even though the policy initialized with imperfect performance, our methods achieve better online performance than the online algorithm learning from scratch.

## Appendix J Pesudo-Codes

Figure 14: Normalized return of evaluation and exploration during IQL offline training, where evaluation means policy output the mean of action distribution and exploration means actions are sampled from the distribution.

```
0: offline policy \(\pi_{\text{off}}\), offline dataset \(D\), factor \(\alpha\), threshold \(\tau\), interaction interval T #Optimistic critic reconstruction before online fine-tuning
1: Initialize the actor \(\pi_{\text{on}}\) with \(\pi_{\text{off}}\) and the critic \(Q_{\text{on}}\) with random parameters
2:for iteration \(i=1,2,\cdots\)do
3: Update the critic in the optimistic way by 3 \(\triangleright\) Policy re-evaluation
4:endfor
5:for iteration \(i=1,2,\cdots\)do
6: Update \(\pi_{\text{on}}\) to seek overestimated actions by 2
7: Update \(Q_{\text{on}}\) to suppress overestimated Q-values by 13 \(\triangleright\) Value Alignment
8:endfor
9:#Constrained online fine-tuning
10: Set the reference policy \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\) and initialize the replay buffer \(R\) with \(D\)
11:for iteration \(i=1,2,\cdots\)do
12: Interact with the environment and store the transition in replay buffer \(R\)
13: Update the temperature coefficient \(\alpha\) by 4 \(\triangleright\) Constrained Fine-tuning
14: Update the Lagrange multiplier \(\lambda\) by 39
15: Update the critic \(Q_{\text{on}}\) by 38
16: Update the policy \(\pi_{\text{on}}\) by37
17:if evaluation permitted then
18:if i % interaction interval == 0 then
19:if\(J(\pi_{\text{on}})>J(\pi_{\text{ref}})\)then
20: replace \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\)
21:endif
22:else
23: Replace \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\) at a given interval
24:endif
25:endfor ```

**Algorithm 2** O2TD3

```
0: offline policy \(\pi_{\text{off}}\), offline dataset \(D\), factor \(k\), threshold \(\tau\), interaction interval #Optimistic critic reconstruction before online fine-tuning
1: Initialize the actor \(\pi_{\text{on}}\) with \(\pi_{\text{off}}\) and the critic \(Q_{\text{on}}\) with random parameters
2:for iteration \(i=1,2,\cdots\)do
3: Update the critic in the optimistic way by 6 \(\triangleright\) Policy re-evaluation
4:endfor
5:for iteration \(i=1,2,\cdots\)do
6: Update \(\pi_{\text{on}}\) to seek overestimated actions by 5
7: Update \(Q_{\text{on}}\) to suppress overestimated Q-values by 16 \(\triangleright\) Value Alignment
8:endfor
9:#Constrained online fine-tuning
10: Set the reference policy \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\) and initialize the replay buffer \(R\) with \(D\)
11:for iteration \(i=1,2,\cdots\)do
12: Interact with the environment and store the transition in replay buffer \(R\)
13: Update the Lagrange multiplier \(\lambda\) by 47 \(\triangleright\) Constrained Fine-tuning
14: Update the critic \(Q_{\text{on}}\) by 46
15: Update the policy \(\pi_{\text{on}}\) by 46
16:if evaluation permitted then
17:if i % interaction interval == 0 then
18:if\(J(\pi_{\text{on}})>J(\pi_{\text{ref}})\)then
19: Replace \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\)
20:endif
21:endif
22:endif
23:endfor ```

**Algorithm 3** O2TD3
```
1:offline policy \(\pi_{\text{off}}\), offline dataset \(D\), factor \(\alpha\), interaction interval, update interval
2:#Optimistic critic reconstruction before online fine-tuning
3:Initialize the actor \(\pi_{\text{on}}\) with \(\pi_{\text{off}}\) and the critic \(V_{on}\) with random parameters
4:for iteration \(i=1,2,\cdots\)do\(\triangleright\) Policy re-evaluation
5: Update the critic \(V_{on}\) in the optimistic way by fitting the returns of offline trajectories
6:endfor
7:#Value Alignment & Constrained online fine-tuning
8:Set the reference policy \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\) and initialize an empty replay buffer \(R\)
9:for iteration \(i=1,2,\cdots\)do
10: Interact with the environment and store the transition in replay buffer
11:if i % update interval == 0 then
12:for iteration \(i=1,2,\cdots,K\)do\(\triangleright\) Value Alignment & Constrained Fine-tuning
13: Compute advantage by 18
14: Update the policy \(\pi_{\text{on}}\) by 7
15: Update the critic \(V_{on}\) by 8
16:endfor
17: Reset \(R\) to empty
18:endif
19:if evaluation permitted then
20:if i % interaction interval == 0 then
21:if\(J(\pi_{\text{on}})>J(\pi_{\text{ref}})\)then
22: Replace \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\)
23:endif
24:endif
25:else
26: Replace \(\pi_{\text{ref}}\) as \(\pi_{\text{on}}\) at a given interval
27:endif
28:endfor ```

**Algorithm 3** O2PPO

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly claim our contributions on offline-to-online RL in both abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Since we re-evaluate the policy under Assumption 4.1, the policy should have favourable performance after offline training. And in Section C.2 in Appendix C, we further discuss why we do not focus on fine-tune a policy with poor performance. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide the references of our assumptions and proofs for our proposed propositions in Appendix F. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclose our algorithm details in Appendix G and our implementation details in Appendix H, including the implementation of the baselines and our methods. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We upload our code in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We disclose our experimental setting in Section 5 and Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We run our methods in 5 random seeds and report the results including the average and standard deviation. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Since our methods do not focus on computational cost, and we experiment our methods in many devices with different GPUs, we do not provide the information on the computer resources specially. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read the NeurIPS Code of Ethics adequately and we make sure we followed the code. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use the open-source code and dataset, and they are explicitly cited in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We do not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with Human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.