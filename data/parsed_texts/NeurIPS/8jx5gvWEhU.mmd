# Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness

Nikola Pavlovic

Department of Electrical and Computer Engineering

Cornell University

Ithaca, NY, USA

np358@cornell.edu

This work was supported in part by the USDA/NSF AI Institute for Next Generation Food Systems under USDA award number 2020-67021-32855.

Sudeep Salgia

Department of Electrical and Computer Engineering

Carnegie Mellon University

Pittsburgh, PA, USA

ssalgia@andrew.cmu.edu

Qing Zhao

Department of Electrical and Computer Engineering

Cornell University

Ithaca, NY, USA

qz16@cornell.edu

###### Abstract

We consider distributed kernel bandits where \(N\) agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret accumulated over time and agents. We develop the first algorithm that achieves the optimal regret order with a communication cost that is sublinear in both \(N\) and \(T\). The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two approaches make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.

## 1 Introduction

Distributed Kernel Bandits.We study the problem of zeroth-order online stochastic optimization in a distributed setting, where \(N\) agents aim to collaboratively maximize a reward function with communications facilitated by a central server. The reward function \(f:\mathcal{X}\rightarrow\mathbb{R}\) is unknown, we only assume it belongs to a Reproducing Kernel Hilbert Space (RKHS) associated with a known kernel \(k\). At each time instant \(t\), each agent \(n\) chooses a point \(x_{t}^{(n)}\in\mathcal{X}\) and receives noisy feedback on the function value at the query point. The goal is for each distributed agent to converge to \(x^{*}\in\operatorname*{arg\,max}_{x\in\mathcal{X}}f(x)\), a global maximizer of \(f\). We quantify this goal as minimizing thecumulative regret summed over a learning horizon of length \(T\) and over all \(N\) agents: \(R(T)=\sum_{n=1}^{N}\sum_{t=1}^{T}(f(x^{*})-f(x_{t}^{(n)}))\).

In addition to learning efficiency, distributed kernel bandits face a new challenge of communication efficiency. Without constraints on the communication cost, all agents can share their local observations and coordinate their individual query actions at no cost. At the other end of the spectrum is a complete decoupling of the agents, resulting in \(N\) independent single-user problems without the benefit of data sharing for accelerated learning. The tension between learning efficiency and communication efficiency is evident. A central question to this trade-off is how to achieve the optimal learning rate enjoyed by the centralized setting using a minimum amount of message exchange among agents.

Main Results.In this paper, we develop the first algorithm for distributed kernel bandits that achieves the optimal order of regret enjoyed by centralized learning with a sublinear message exchange in both \(T\) and \(N\).

To tackle the essential trade-off between learning rate and communication efficiency, a distributed learning algorithm needs a communication strategy that governs _what_ to communicate and _how to integrate_ the shared information into local policies. To minimize the total regret that is accumulating over time and agents, the communication strategy needs to work in tandem with the query actions to ensure a continual flow of information available at all agents for decision-making.

A natural answer to _what_ to communicate in a distributed learning problem is a certain sufficient local statistics of the underlying unknown parameters [31]. However, for kernel bandits, relevant sufficient statistics are infinite-dimensional and hence an impractical choice for communication. Existing studies resolve this issue by exchanging local query actions and observations across all agents and throughout the learning horizon [9, 15], resulting in a communication cost growing linearly in both \(N\) and \(T\). Even with a communication cost growing linearly in both \(N\) and \(T\) it is not clear how to achieve the performance of a centralized learner with \(NT\) query points. Prevailing approaches in centralized kernel bandits utilize reward-dependent [28] or adaptive policies [16]. Emulating such policies at each of the \(NT\) query points is practically unfeasible as it would require the agents to _take turns_ in their queries and immediately share the local observations with all other agents.

To tackle the above challenges, our proposed algorithm represents major departures from the prevailing approaches. Referred to as DUETS (Distributed Uniform Exploration of Trimmed Sets), this algorithm has two key features: _uniform exploration_ at the local agents and _shared randomness_ with the central server. In DUETS, each agent employs uniform sampling as the query strategy. Uniform sampling is fully compatible with parallel learning. In particular, note that the union of the local sets of size \(t\) query points obtained at the agents through uniform sampling is identical (in distribution) to the set of size \(Nt\) query points obtained at a centralized decision maker using the same uniform sampling strategy. This superposition property of uniform sampling allows us to leverage the recent results on random exploration in centralized kernel bandits [29], and is crucial in achieving the optimal learning rate defined by the centralized setting. In terms of communication efficiency, uniform sampling makes it possible to bypass the exchange of query points altogether and reduce the exchange of reward observations through the _shared randomness_ strategy. In DUETS, each agent has access to an independent coin, i.e., a source of randomness, which is unknown to the other agents but is known to the server. The shared randomness enables the server to reproduce the points queried by the agents, thereby resulting in effective transmission of the local set of queried points to the server at _no communication cost_. Please refer to App. C for an additional discussion.

We analyze the performance of DUETS and establish that it incurs a cumulative regret of \(\tilde{\mathcal{O}}(\sqrt{NT\gamma_{NT}}\log(T/\delta))\) with probability \(1-\delta\), where \(\gamma_{NT}\) denotes the maximal information gain of the kernel Srinivas et al. [28] and \(\tilde{\mathcal{O}}(\cdot)\) hides poly-logarithmic factors. To the best knowledge of the authors, this is the _first_ algorithm to achieve the optimal order of regret for the problem of distributed kernel bandits. We also establish a bound of \(\tilde{\mathcal{O}}(\gamma_{NT})\) on the communication cost incurred by DUETS (See Section 2 for a precise definition).

Related Work.The existing literature on distributed kernel bandits is relatively slim. The most relevant to our work is that by Li et al. [15], where the authors consider the problem of distributed contextual kernel bandits and propose a UCB based policy with sparse approximation of GP models and intermittent communication. Their proposed policy was shown to incur a cumulative regret of \(\widetilde{\mathcal{O}}(\sqrt{NT}\gamma_{NT})\) and communication cost of \(\mathcal{O}(N\gamma_{NT}^{3})\). The DUETS algorithm proposed in this work, offers an improvement over the algorithm in [15] both in terms of regret and communication cost. While the contextual setting with varying arm action sets considered in their work is more general that the setting with a fixed arm set considered in this work, their proposed algorithm does not offer non-trivial reduction in regret or communication cost in the fixed arm setting. Moreover, both the regret and communication cost incurred by the algorithm in Li et al. [15] are not guaranteed to be sublinear in the total number of queries, \(NT\), for all kernels. Consequently, their algorithm does not guarantee convergence to \(x^{*}\) or a non-trivial communication cost for all kernels. On the other hand, both regret and communication cost of DUETS is guaranteed to be sub-linear implying both convergence and communication efficiency.

Among other studies, Du et al. [8] consider the problem of distributed pure exploration in kernel bandits over finite action set, where they focus on designing learning strategies with low simple regret. In this work, we consider the more challenging continuum-armed setup with a focus on minimizing cumulative regret as opposed to simple regret. Another line of work explores impact of heterogeneity among clients and design algorithms to minimize this impact. Salgia et al. [21] consider personalized kernel bandits in which agents have heterogeneous models and aim to optimize the weighted sum of their own reward function and the average reward function over all the agents. Dubey and Pentland [9] consider heterogeneous distributed kernel bandits over a graph in which they use additional kernel-based modeling to measure task similarity across different agents.

In contrast to the distributed kernel bandit, the problems of distributed multi-armed bandits and linear bandits have been extensively studied. For distributed multi-armed bandits (MAB), a variety of algorithms have been proposed for distributed learning under different network topologies Landgren et al. [14], Shahrampour et al.[25], Sankararaman et al.[23], Chawla et al.[6], Zhu et al.[33]. Shi et al. [27] and Shi and Shen [26] have analyzed the impact of heterogeneity among agents in the distributed MAB problem. Similarly, the problem of distributed linear bandits is also well-understood in variety of settings with different network topologies Korda et al. [13], heterogeneity among agents Mitra et al. [17], Ghosh et al.[10], Hanna et al.[11] and communication constraints Mitra et al. [18], Wang et al.[31], Huang et al.[12], Amani et al.[3], Salgia and Zhao[22].

## 2 Problem Formulation

We consider a distributed learning framework consisting of \(N\) agents indexed by \(\{1,2,\ldots,N\}\). Under this framework, we study the problem of collaboratively maximizing an unknown function \(f:\mathcal{X}\rightarrow\mathbb{R}\), where \(\mathcal{X}\subset\mathbb{R}^{d}\) is a compact, convex set. The function \(f\) belongs to the an RKHS, \(\mathcal{H}_{k}\), associated with a known positive definite kernel \(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\). \(\mathcal{H}_{k}\) is a Hilbert space that is endowed by with an inner product \(\langle\cdot,\cdot\rangle_{\mathcal{H}_{k}}\) that obeys the reproducing property, and induces the norm \(\|g\|_{\mathcal{H}_{k}}=\langle g,g\rangle_{\mathcal{H}_{k}}\). We assume \(f\) is finite in this norm i.e \(\|f\|_{\mathcal{H}_{k}}\leq B\).

Each agent, upon querying a point \(x\in\mathcal{X}\), observes \(y=f(x)+\epsilon\), where \(\epsilon\) is a zero mean, \(R\)-sub Gaussian noise term assumed to be independent across time and agents. We make the following assumption on the unknown function \(f\), similar to that adopted in Salgia et al [29].

**Assumption 2.1**.: Let \(\mathcal{L}_{\eta}=\{x\in\mathcal{X}|f(x)\geq\eta\}\) denote the level set of \(f\) for \(\eta\in[-B,B]\). We assume that for all \(\eta\in[-B,B]\), \(\mathcal{L}_{\eta}\) is a disjoint union of at most \(M_{f}<\infty\) components, each of which is closed and connected. Moreover, for each such component, there exists a bi-Lipschitzian map between each such component and \(\mathcal{X}\) with normalized Lipschitz constant pair \(L_{f},L_{f}^{\prime}<\infty\).

The communication efficiency is measured using the sum of the uplink and downlink communication costs. In particular, let \(C_{\mathrm{up}}^{(n)}(T)\) denote the number of real numbers sent by the agent \(n\) to the server over the time horizon. The uplink cost of \(\pi\) is given as \(C_{\mathrm{up}}^{\pi}(T)=\frac{1}{N}\sum_{n=1}^{N}C_{\mathrm{up}}^{(n)}(T)\). Similarly, the downlink cost of \(\pi\), \(C_{\mathrm{down}}^{\pi}(T)\) is the number of real numbers broadcast by the server over the entire time horizon averaged over all agents. The overall communication cost of \(\pi\) is given by \(C^{\pi}(T)=C_{\mathrm{up}}^{\pi}(T)+C_{\mathrm{down}}^{\pi}(T)\).

### GP Models

A Gaussian Process (GP) is a random process \(G\) indexed by \(\mathcal{X}\) and is associated with a mean function \(\mu:\mathcal{X}\rightarrow\mathbb{R}\) and a positive definite kernel \(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\). The random process \(G\) is defined such that for all finite subsets of \(\mathcal{X}\), \(\{x_{1},x_{2},\ldots,x_{m}\}\subset\mathcal{X}\), \(m\in\mathbb{N}\), the random vector \([G(x_{1}),G(x_{2}),\ldots,G(x_{m})]^{\top}\) follows a multivariate Gaussian distribution with mean vector \([\mu(x_{1}),\ldots,\mu(x_{n})]]^{\top}\) and covariance matrix \(\Sigma=[k(x_{i},x_{j})]_{i,j=1}^{m}\). Throughout the work, we consider GPs with \(\mu\equiv 0\). When used as a prior for a data generating process under Gaussian noise, the conjugate property provides closed form expressions for the posterior mean and covariance of the GP model. Specifically, given a set of observations \(\{\mathbf{X}_{m},\mathbf{Y}_{m}\}=\{(x_{i},y_{i})\}_{i=1}^{m}\) from the underlying process, the expression for posterior mean and variance of GP model is given as follows:

\[\mu_{m}(x) =k_{\mathbf{X}_{m}}(x)^{\top}(\lambda\mathbf{I}_{m}+\mathbf{K}_{ \mathbf{X}_{m},\mathbf{X}_{m}})^{-1}\mathbf{Y}_{m}, \tag{1}\] \[\sigma_{m}^{2}(x) =(k(x,x)-k_{\mathbf{X}_{m}}^{\top}(x)(\lambda\mathbf{I}_{m}+ \mathbf{K}_{\mathbf{X}_{m},\mathbf{X}_{m}})^{-1}k_{\mathbf{X}_{m}}(x)). \tag{2}\]

In the above expressions, \(k_{\mathbf{X}_{m}}(x)=[k(x_{1},x),k(x_{2},x)\ldots k(x_{n},x)]^{\top}\), \(\mathbf{K}_{\mathbf{X}_{m},\mathbf{X}_{m}}=\{k(x_{i},x_{j})\}_{i,j=1}^{m}\), \(\mathbf{I}_{m}\) is the \(m\times m\) identity matrix and \(\lambda\) is the variance of the Gaussian noise.

Following a standard approach in the literature [28], we model the data corresponding to observations from the unknown \(f\), which belongs to the RKHS of a positive definite kernel \(k\), using a GP with the same covariance kernel \(k\). 2. The benefit of this approach is that the posterior mean and variance of this GP model serve as tools to both predict the values of the function \(f\) and quantify the uncertainty of the prediction at unseen points in the domain [30, Thm. 1].

Footnote 2: We assume a _fictitious_ GP prior over the fixed, unknown function \(f\) along with _fictitious_ Gaussian noise.

Sparse approximation of GP models.The sparsification of GP models refers to the idea of approximating the posterior mean and variance of a GP model, corresponding to a set of observations \(\{\mathbf{X}_{m},\mathbf{Y}_{m}\}\), using a subset of query points \(\mathbf{X}_{m}\). In particular, let \(\mathcal{S}\) be a subset of \(\mathbf{X}_{m}\) consisting of \(r<m\) points. The approximate posterior mean and variance [32] based on points in \(\mathcal{S}\), referred to as the inducing set, is given as

\[\tilde{\mu}_{m}(x) =z_{\mathcal{S}}(x)^{\top}\left(\lambda\mathbf{I}_{|\mathcal{S}| }+\mathbf{Z}_{\mathbf{X}_{m},\mathcal{S}}^{\top}\mathbf{Z}_{\mathbf{X}_{m}, \mathcal{S}}\right)^{-1}\mathbf{Z}_{\mathcal{X}_{m},\mathcal{S}}^{\top}\mathbf{ Y}_{m} \tag{3}\] \[\lambda\tilde{\sigma}_{m}^{2}(x) =\big{[}k(x,x)-z_{\mathcal{S}}^{\top}(x)\mathbf{Z}_{\mathbf{X}_ {m},\mathcal{S}}^{\top}\mathbf{Z}_{\mathbf{X}_{m},\mathcal{S}}\left(\lambda \mathbf{I}_{|\mathcal{S}|}+\mathbf{Z}_{\mathbf{X}_{m},\mathcal{S}}^{\top} \mathbf{Z}_{\mathbf{X}_{m},\mathcal{S}}\right)^{-1}z_{\mathcal{S}}(x)\big{]}, \tag{4}\]

where \(z_{\mathcal{S}}(x)=\mathbf{K}_{\mathcal{S},\mathcal{S}}^{-\frac{1}{2}}k_{ \mathcal{S}}(x)\) and \(\mathbf{Z}_{\mathbf{X}_{m},\mathcal{S}}=[z_{\mathcal{S}}(x_{1}),z_{\mathcal{S} }(x_{2}),\ldots,z_{\mathcal{S}}(x_{m})]^{\top}\).

## 3 The DUETS Algorithm

We first describe the randomization at each agent and the shared randomness with the server. Each agent \(n\) has a coin \(\mathscr{C}_{n}\) for generating random bits that are independent of those generated by other agents. Each agent's coin is unknown to other agents, but known to the central server. In a practical implementation, the coins can be thought of as seeds for generating random numbers.

DUETS employs an epoch-based elimination structure where the domain \(\mathcal{X}\) is successively trimmed across epochs to maintain an active region that contains a global maximizer \(x^{*}\) with high probability. Specifically, in each epoch \(j\), the server and the agents maintain a common active subset of the domain \(\mathcal{X}_{j}\subseteq\mathcal{X}\) with \(\mathcal{X}_{1}\) initialized to \(\mathcal{X}\). The operations in each epoch are as follows.

During the \(j^{\text{th}}\) epoch, each agent \(n\), using its private coin \(\mathscr{C}_{n}\), generates \(\mathcal{D}_{j}^{(n)}\), a set of \(T_{j}=\lfloor\sqrt{TT_{j-1}}\rfloor\) points that are uniformly distributed in the set \(\mathcal{X}_{j}\)3.Each agent \(n\) queries all the points in \(\mathcal{D}_{j}^{(n)}\) and obtains \(\mathbf{Y}_{j}^{(n)}\in\mathbb{R}^{T_{j}}\), the corresponding vector of reward observations. Since the server has access to the coins of all the agents, it can faithfully reproduce the set \(\mathcal{D}_{j}=\bigcup_{n=1}^{N}\mathcal{D}_{j}^{(n)}\) without any communication between the server and the agents. In order to efficiently communicate the observed reward values from the agents to the server, we leverage sparse approximation of GP models along with the knowledge of the set \(\mathcal{D}_{j}\) at the server. The server constructs a global inducing set \(\mathcal{S}_{j}\) by including each point in \(\mathcal{D}_{j}\) with probability \(p_{j}:=p_{0}\sigma_{j,\max}^{2}\), independent of other points where \(\sigma_{j,\max}^{2}=\sup_{x\in\mathcal{X}_{j}}\sigma_{j}^{2}(x)\), \(\sigma_{j}^{2}(\cdot)\) is the posterior variance corresponding to points collected in \(\mathcal{D}_{j}\) and \(p_{0}\) is an appropriately chosen constant. The server broadcasts the inducing set \(\mathcal{S}_{j}\) to all the agents.

Upon receiving the inducing set, each agent \(n\) computes \(v_{j}^{(n)}:=\mathbf{Z}_{\mathcal{D}_{j}^{(n)},\mathcal{S}_{j}}^{\top}\mathbf{Y}_{ j}^{(n)}\in\mathbb{R}^{|\mathcal{S}_{j}|}\), the projection of its reward vector onto the inducing set. All agents then send the projected observations \(v_{j}^{(n)}\) to the server, which aggregates them to obtain the vector \(\overline{v}_{j}:=(\lambda\mathbf{I}_{|\mathcal{S}_{j}|}+\mathbf{Z}_{\mathcal{ D}_{j},\mathcal{S}_{j}}^{\top}\mathbf{Z}_{\mathcal{D}_{j},\mathcal{S}_{j}})^{-1}( \sum_{n=1}^{N}v_{j}^{(n)})\). Note that the summation \(\sum_{n=1}^{N}v_{j}^{(n)}\) equals to \(\mathbf{Z}_{\mathcal{D}_{j},\mathcal{S}_{j}}^{\top}\mathbf{Y}_{j}\), i.e., projection of the rewards of all agents onto the inducing set. The server then broadcasts the vector \(\overline{v}_{j}\) and \(\sigma_{j,\max}\) to all the agents. The benefit of sending \(\overline{v}_{j}\) as opposed to the sum of rewards is that it allows the agents to compute the posterior mean at the agents using their knowledge of the inducing set \(\mathcal{S}_{j}\) (See. Eqn (3)).

As the last step of the epoch, all the agents and the server trim the current set \(\mathcal{X}_{j}\) to \(\mathcal{X}_{j+1}\) using the update rule: \(\mathcal{X}_{j+1}=\big{\{}x\in\mathcal{X}_{j}:\tilde{\mu}_{j}(x)\geq\sup_{x^{ \prime}\in\mathcal{X}_{j}}\tilde{\mu}_{j}(x^{\prime})-2\beta(\delta^{{}^{\prime }})\sigma_{j,\max}\big{\}},\) where \(\delta^{\prime}=\frac{\delta}{2(|\mathcal{U}_{T}|\cdot(\log(\log N\log T))+4)}\) and \(\tilde{\mu}_{j}(x)=z_{\mathcal{S}_{j}}^{\top}(x)\overline{v}_{j}\) is the _approximate_ posterior mean computed based on the inducing set \(\mathcal{S}_{j}\) (See Eqn. (3)). Below we present the pseudo code for DUETS on the agent's side. For pseudo-code for the server side please refer to App. A.

```
1:Input: Size of the first epoch \(T_{1}\), error probability \(\delta\)
2:\(t\gets 0,j\gets 1\), \(\mathcal{X}_{1}\leftarrow\mathcal{X}\)
3:while\(t<T\)do
4:\(\mathcal{D}_{j}^{(n)}=\emptyset\)
5:for\(i\in\{1,2,\ldots,T_{j}\}\)do
6: Query a point \(x_{t}^{(n)}\) uniformly at random from \(\mathcal{X}_{j}\) using the coin \(\mathscr{C}_{n}\) and observe \(y_{t}^{(n)}\)
7:\(\mathcal{D}_{j}^{(n)}\leftarrow\mathcal{D}_{j}^{(n)}\cup\{x_{t}^{(n)}\}\), \(t\gets t+1\)
8:if\(t>T\)then Terminate
9:endfor
10: Receive the global inducing set \(\mathcal{S}_{j}\),
11: Set \(v_{j}^{(n)}\leftarrow\mathbf{Z}_{\mathcal{D}_{j}^{(n)},\mathcal{S}_{j}}^{\top} \mathbf{Y}_{j}^{(n)}\), where \(\mathbf{Y}_{j}^{(n)}=[y_{t-T_{j}},y_{t-T_{j}+1},\ldots,y_{t}]^{\top}\)
12: Receive \(\overline{v}_{j}\) and \(\sigma_{j,\max}\) from the server and compute \(\tilde{\mu}_{j}(\cdot)=z_{\mathcal{S}_{j}}^{\top}(\cdot)\overline{v}_{j}\)
13: Update \(\mathcal{X}_{j}\) to \(\mathcal{X}_{j+1}\) using Eqn. (3),\(T_{j+1}\leftarrow\lfloor\sqrt{TT_{j}}\rfloor\), \(j\gets j+1\)
14:endwhile
```

**Algorithm 1**DUETS : Agent \(n\in\{1,2,\ldots,N\}\)

Performance guarantees.The following theorem characterizes the regret performance and communication cost of DUETS.

**Theorem 3.1**.: _Consider the distributed kernel bandit problem described in Section 2. For a given \(\delta\in(0,1)\), let the policy parameters of_ DUETS _be such that \(T_{1}\geq\overline{M}/N\) and \(p_{0}=72\log\frac{4N}{\delta}\). Then with probability at least \(1-\delta\), the regret and communication cost incurred by_ DUETS _satisfy the following relations:_

\[R_{\mathrm{DUETS}}=\tilde{\mathcal{O}}(\sqrt{NT\gamma_{NT}}\log(T/\delta)); \quad C_{\mathrm{DUETS}}=\tilde{\mathcal{O}}(\gamma_{NT}).\]

In the above theorem, \(\overline{M}\) is a constant that is independent of \(N\) and \(T\). As shown in above theorem, DUETS achieves order-optimal regret as it matches the lower bound established in [24] upto logarithmic factors. DUETS is the _first algorithm_ to close this gap to the lower bound in the distributed setup and achieve order-optimal regret performance. We refer the reader to App. B for a detailed proof of the theorem.

## 4 Conclusion

We propose a new algorithm for the problem of distributed kernel bandits. The proposed algorithm represents major departures from prevailing approaches and has two key features: uniform exploration and shared randomness. It is the _first_ algorithm to achieve optimal-order regret in distributed kernel bandit setting while simultaneously achieving diminishing rates of communication in both the time horizon and the number of agents. We also corroborate our theoretical claims with empirical studies (see App. D).

## References

* Acharya et al. [2020] J. Acharya, C. L. Canonne, and H. Tyagi. Inference under information constraints i: Lower bounds from chi-square contraction. _IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 66, NO. 12, DECEMBER 2020_, 2020.
* Acharya et al. [2020] J. Acharya, C. L. Canonne, and H. Tyagi. Inference under information constraints ii: Communication constraints and shared randomness. _IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. 66, NO. 12, DECEMBER 2020_, 2020.
* Amani et al. [2022] S. Amani, T. Lattimore, A. Gyorgy, and L. F. Yang. Distributed Contextual Linear Bandits with Minimax Optimal Communication Cost, 2022.
* Azimi et al. [2012] J. Azimi, A. Jalali, and X. Z. Fern. Hybrid batch bayesian optimization. In _Proceedings of the 29th International Conference on Machine Learning, ICML_, volume 2, pages 1215-1222, 2012.
* Calandriello et al. [2019] D. Calandriello, L. Carratino, A. Lazaric, M. Valko, and L. Rosasco. Gaussian Process Optimization with Adaptive Sketching: Scalable and No Regret. _Proceedings of Machine Learning Research_, 99:1-25, 2019.
* Chawla et al. [2020] R. Chawla, A. Sankararaman, A. Ganesh, and S. Shakkottai. The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits, 2020.
* Chowdhury and Gopalan [2017] S. R. Chowdhury and A. Gopalan. On kernelized multi-armed bandits. In _Proceedings of the 34th International Conference on Machine Learning, ICML_, volume 2, pages 1397-1422, 2017.
* Du et al. [2023] Y. Du, W. Chen, Y. Kuroki, and L. Huang. Collaborative Pure Exploration in Kernel Bandit. In _Proceedings of the 11th International Conference on Learning Representations, ICLR_, 2023.
* Dubey and Pentland [2020] A. Dubey and A. Pentland. Kernel methods for cooperative multi-agent contextual bandits. In _Proceedings of the 37th International Conference on Machine Learning, ICML 2020_, pages 2720-2730, 2020.
* Ghosh et al. [2021] A. Ghosh, A. Sankararaman, and K. Ramchandran. Adaptive Clustering and Personalization in Multi-Agent Stochastic Linear Bandits, 2021.
* Hanna et al. [2022] O. Hanna, L. Yang, and C. Fragouli. Learning from distributed users in contextual linear bandits without sharing the context. In _Proceedings of the 36th Annual Conference on Neural Information Processing Systems_, volume 35, pages 11049-11062, 2022.
* Huang et al. [2021] R. Huang, W. Wu, J. Yang, and C. Shen. Federated Linear Contextual Bandits. In _Advances in Neural Information Processing Systems_, volume 32, pages 27057-27068, 2021.
* Korda et al. [2016] N. Korda, B. Szorenyi, and S. Li. Distributed clustering of linear bandits in peer to peer networks. In _33rd International Conference on Machine Learning, ICML 2016_, volume 3, pages 1966-1980, 2016.
* Landgren et al. [2017] P. Landgren, V. Srivastava, and N. E. Leonard. On distributed cooperative decision-making in multiarmed bandits. In _Proceedings of the European Control Conference, ECC_, pages 243-248, 2017.
* Li et al. [2022] C. Li, H. Wang, M. Wang, and H. Wang. Communication Efficient Distributed Learning for Kernelized Contextual Bandits. In _Proceedings of the 36th Annual Conference on Neural Information Processing Systems_, 2022.
* Li and Scarlett [2022] Z. Li and J. Scarlett. Gaussian process bandit optimization with few batches. In _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics, AISTATS_, 2022.
* Mitra et al. [2021] A. Mitra, H. Hassani, and G. Pappas. Exploiting Heterogeneity in Robust Federated Best-Arm Identification, 2021.
* Mitra et al. [2022] A. Mitra, H. Hassani, and G. J. Pappas. Linear Stochastic Bandits over a Bit-Constrained Channel, 2022.

* [19] H.-Y. Park, S.-H. Nam, and S.-H. Lee. Exactly optimal and communication- efficient private estimation via block designs. _IEEE Journal on selected areas in information theory, vol. 5, 2024_, 2024.
* [20] V. Picheny, T. Wagner, and D. Ginsbourger. A benchmark of kriging-based infill criteria for noisy optimization. _Structural and Multidisciplinary Optimization_, 48(3):607-626, 2013.
* [21] S. Salgia, S. Vakili, and Q. Zhao. Collaborative learning in kernel-based bandits for distributed users. _IEEE Transactions on Signal Processing_, 71:3956-3967, 2023.
* [22] S. Salgia and Q. Zhao. Distributed linear bandits under communication constraints. In _Proceedings of the 40th International Conference on Machine Learning, ICML_, pages 29845-29875. PMLR, 2023.
* [23] A. Sankararaman, A. Ganesh, and S. Shakkottai. Social learning in multi agent multi armed bandits. _Proc. ACM Meas. Anal. Comput. Syst._, 3(3), dec 2019.
* [24] J. Scarlett, I. Bogunovic, and V. Cehver. Lower Bounds on Regret for Noisy Gaussian Process Bandit Optimization. In _Conference on Learning Theory_, volume 65, pages 1-20, 2017.
* [25] S. Shahrampour, A. Rakhlin, and A. Jadbabaie. Multi-armed bandits in multi-agent networks. In _2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 2786-2790, 2017.
* [26] C. Shi and C. Shen. Federated Multi-Armed Bandits. In _Proceedings of the 35th AAAI Conference on Artificial Intelligence_, pages 9603-9611, 2021.
* [27] C. Shi, C. Shen, and J. Yang. Federated Multi-armed Bandits with Personalization, 2021.
* [28] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: no regret and experimental design. In _Proceedings of the 27th International Conference on Machine Learning, ICML_, pages 1015-1022, 2010.
* [29] S. Sudeep, V. Sattar, and Z. Qing. Random exploration in bayesian optimization: Order-optimal regret and computational efficiency, 2023.
* [30] S. Vakili, N. Bouziani, S. Jalali, A. Bernacchia, and D.-s. Shiu. Optimal order simple regret for Gaussian process bandits. In _Proceedings of the 35th Annual Conference on Neural Information Processing Systems_, 2021.
* [31] Y. Wang, J. Hu, X. Chen, and L. Wang. Distributed Bandit Learning: Near-Optimal Regret with Efficient Communication. In _Proceedings of the 7th International Conference on Learning Representations (ICLR)_, 2019.
* [32] V. Wild, M. Kanagawa, and D. Sejdinovic. Connections and equivalences between the nystrom method and sparse variational gaussian processes, 2021.
* [33] Z. Zhu, J. Zhu, J. Liu, and Y. Liu. Federated Bandit: A Gossiping Approach. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 5(1):1-29, 2021.

Additional details about DUETS

Below we outline the pseudo-code of DUETS from the server and agent side.

```
1:input: Size of the first epoch \(T_{1}\), error probability \(\delta\)
2:\(t\gets 0,j\gets 1\), \(\mathcal{X}_{1}\leftarrow\mathcal{X}\)
3:while\(t<T\)do
4: Use the coins \(\mathscr{C}_{1},\mathscr{C}_{2},\ldots,\mathscr{C}_{N}\) to reproduce the sets \(\mathcal{D}_{j}^{(1)},\mathcal{D}_{j}^{(2)},\ldots,\mathcal{D}_{j}^{(N)}\)
5:\(\mathcal{D}_{j}\leftarrow\cup_{n=1}^{N}\mathcal{D}_{j}^{(n)}\)
6: Set \(\sigma_{j,\max}\leftarrow\sup_{x\in\mathcal{X}_{j}}\sigma_{j}(x)\)
7: Construct the set \(\mathcal{S}_{j}\) by independently including each point from \(\mathcal{D}_{j}\) with probability \(p_{j}\)
8: Broadcast \(\mathcal{S}_{j}\) to all the agents and receive \(v_{j}^{(n)}\) from all agents \(n\in\{1,2,\ldots,N\}\)
9: Set \(\overline{v}_{j}\) using Eqn. (3)
10: Broadcast \(\overline{v}_{j}\) and \(\sigma_{j,\max}\) to all the agents
11: Update \(\mathcal{X}_{j}\) to \(\mathcal{X}_{j+1}\) using Eqn. (3)
12:\(t\gets t+T_{j}\), \(T_{j+1}\leftarrow\lfloor\sqrt{TT_{j}}\rfloor\)
13:\(j\gets j+1\)
14:endwhile
```

**Algorithm 2**DUETS : Server

## Appendix B Proof of Theorem 3.1

In this section, we provide a detailed proof of Theorem 3.1. For the regret bound, we first bound the regret incurred by DUETS in each epoch \(j\) and then sum it across different epochs to obtain a bound on the overall cumulative regret. We first prove the theorem assuming the results from Lemmas B.1, B.2 and B.3 and then separately prove the lemmas.

**Lemma B.1**.: _Let \(\Delta_{j}:=\sup_{x\in\mathcal{X}_{j}}f(x^{*})-f(x)\). Then, the following bound holds all epochs \(j\geq 1\) with probability \(1-\delta/2\)._

\[\Delta_{j}\leq 8\beta(\delta^{\prime})\cdot\sup_{x\in\mathcal{X}_{j-1}} \sigma_{j}(x)+\frac{4B}{T},\]

_where \(\delta^{\prime}=\frac{\delta}{2(\log(\log N+\log T)+4)|\mathcal{U}_{T}|}\) and \(\mathcal{U}_{T}\) denotes the discretization defined in Assumption B.8._

**Lemma B.2**.: _[_29_]_ _Let \(\sigma_{j}^{2}(\cdot)\) denote the posterior variance corresponding to the set \(\mathcal{D}_{j}\) obtained by sampling \(NT_{j}\) points uniformly at random from the domain \(\mathcal{X}_{j}\). Then, for \(T_{1}\geq\overline{M}(\delta)/N\) and for any \(f\) satisfying Assumption 2.1, the following holds w.p. \(1-\delta\) for all epochs \(j\geq 1\):_

\[\sup_{x\in\mathcal{X}_{j}}\sigma_{j}^{2}(x)\leq C_{f,\mathcal{X}}\cdot\frac{ \gamma_{NT_{j}}}{NT_{j}}.\]

_Here \(C_{f,\mathcal{X}}\) denotes a constant that depends only on \(f\) and \(\mathcal{X}\) and is independent of both \(N\) and \(T\)._

The bound on the communication cost follows directly from the following Lemmas B.3 and B.4 by noting that the communication cost in epoch \(j\) satisfies \(\mathcal{O}(|\mathcal{S}_{j}|)\).

**Lemma B.3**.: _The total number of epochs in DUETS over a time horizon of \(T\) is at most \(\log(\log(\max\{N,T\}))+4\)._

**Lemma B.4**.: _Let \(\mathcal{S}_{j}\) denote the inducing set construct in \(j^{\text{th}}\) epoch, as outlined in Section 3. Then, for all epochs \(j\) the following holds with probability at least \(1-\delta\),_

\[|\mathcal{S}_{j}|\leq C_{f,\mathcal{X}}\cdot(3+\log(\log(\log N\log T)/\delta) )\cdot\gamma_{NT},\]

_where \(C_{f,\mathcal{X}}\) is same constant as the one in Lemma B.2._

Consider any epoch \(j\geq 1\) and let \(R^{(j)}\) denote the regret incurred by DUETS in this epoch. Since the agents take purely exploratory actions by uniform sampling points from the current set, we have the following crude bound \(R^{(j)}\leq\Delta_{j}\cdot NT_{j}\cdot M_{f}\), where \(\Delta_{j}:=\sup_{x\in\mathcal{X}_{j}}(f(x^{*})-f(x))\). The term \(NT_{j}\cdot M_{f}\) corresponds to number of points sampled during the epoch as we sample each connected component of \(\mathcal{X}_{j}\), of which there are at most \(M_{f}\), \(NT_{j}\) times. For \(j=1\), we use the trivial bound,

\[\Delta_{1}=\sup_{x\in\mathcal{X}}(f(x^{*})-f(x))\leq 2\sup_{x\in\mathcal{X}}f(x) \leq 2B,\]

which gives us \(R^{(1)}\leq 2B\cdot NT_{1}\cdot M_{f}\). On invoking Lemma B.1 for \(j>1\) we obtain,

\[R^{(j)} \leq\Delta_{j}\cdot NT_{j}\cdot M_{f}\] \[\leq NT_{j}\cdot M_{f}\cdot\left(8\beta(\delta^{\prime})\cdot \left(\sup_{x\in\mathcal{X}_{j-1}}\sigma_{j-1}(x)\right)+\frac{4B}{T}\right),\]

where \(\delta^{\prime}=\frac{\delta}{2(\log\log NT+4)|\mathcal{U}_{T}|}\). Using Lemma B.2, we can further bound this expression as

\[R^{(j)} \leq\Delta_{j}\cdot NT_{j}\cdot M_{f}\] \[\leq NT_{j}\cdot M_{f}\cdot\left(8\beta(\delta^{\prime})\cdot C_{ f,\mathcal{X}}\cdot\sqrt{\frac{\gamma NT_{j-1}}{NT_{j-1}}}+\frac{4B}{T}\right)\] \[\leq M_{f}\cdot\left(8C_{f,\mathcal{X}}^{1/2}\cdot\beta(\delta^{ \prime})\cdot\sqrt{NT\gamma_{NT_{j-1}}}+\frac{4BNT_{j}}{T}\right)\] \[\leq M_{f}\cdot\left(8C_{f,\mathcal{X}}^{1/2}\cdot\beta(\delta^{ \prime})\cdot\sqrt{NT\gamma_{NT}}+\frac{4BNT_{j}}{T}\right).\]

In the third line, we used the inequality \(\frac{T_{j}}{\sqrt{T_{j-1}}}\leq\sqrt{T}\), which follows from the definition of \(T_{j}\). In the last line, we used the fact that \(\gamma_{m}\) is an increasing function of \(m\). Thus, if \(J\) denotes an upper bound on the number of epochs, we can write:

\[\sum_{j=1}^{J}R^{(j)} \leq 2BM_{f}\cdot NT_{1}+\sum_{j=2}^{J}M_{f}\cdot\left(8C_{f, \mathcal{X}}^{1/2}\cdot\beta(\delta^{\prime})\cdot\sqrt{NT\gamma_{NT}}+\frac{ 4BNT_{j}}{T}\right)\] \[\leq 2BM_{f}\cdot NT_{1}+J\cdot\left(8C_{f,\mathcal{X}}^{\prime} \cdot\beta(\delta^{\prime})\cdot\sqrt{NT\gamma_{NT}}\right)+\frac{4BNM_{f}}{T }\sum_{j=1}^{J}T_{j}\] \[\leq 2BM_{f}\cdot NT_{1}+J\cdot\left(8C_{f,\mathcal{X}}^{\prime} \cdot\beta(\delta^{\prime})\cdot\sqrt{NT\gamma_{NT}}\right)+4BNM_{f}. \tag{5}\]

We next optimize the length of the first epoch \(T_{1}\) in order to achieve order optimal regret. DUETS achieves order optimal regret for \(N\leq\max(T,\gamma_{NT})\).

If \(N<T\) we can choose \(T_{1}=\sqrt{\frac{T}{N}}+\overline{M}(\delta^{{}^{\prime}})\) where \(\delta^{{}^{\prime}}=\frac{\delta}{2(\log\log NT+4)}\). Left hand side of equation (5) can now be written as \(\widetilde{\mathcal{O}}(\sqrt{NT\gamma_{NT}}\sigma(\delta^{{}^{\prime}})) \equiv\widetilde{\mathcal{O}}\left(\sqrt{NT\gamma_{NT}}\left(\log\frac{T}{ \delta}\right)\right)\).

If \(N\leq\gamma_{NT}\) we can fix \(T_{1}=\sqrt{T}+\overline{M}(\delta^{{}^{\prime}})\). We have \(NT_{1}\leq\widetilde{O}(\sqrt{NT\gamma_{NT}})\) and the left hand-side is once again \(\widetilde{\mathcal{O}}\left(\sqrt{NT\gamma_{NT}}\left(\log\frac{T}{\delta} \right)\right)\).

Note that by Lemma B.3, \(J\) is upper bounded by \(\log(\log N\log T)+4\) and is thus \(\widetilde{O}(1)\).

Before moving onto the proofs of Lemmas B.1 and B.3, we state two auxiliary lemmas that will be useful for our analysis.

**Definition B.5**.: Let \(\mathcal{D}=\{x_{1},x_{2},\ldots,x_{m}\}\subset\mathcal{X}\) be a collection \(m\) points and \(\mathcal{S}\) be any subset of \(\mathcal{D}\). Let \(\sigma^{2}_{\mathcal{D}}(\cdot)\) denote the posterior variance corresponding to the points in \(\mathcal{D}\) and \(\tilde{\sigma}^{2}_{\mathcal{S}}(\cdot)\) denote the _approximate_ posterior computed based on the points in \(\mathcal{S}\). We call \(\mathcal{S}\) to be an \(\varepsilon\)-accurate inducing set if the following relations are true for all \(x\in\mathcal{X}\).

\[\frac{1-\varepsilon}{1+\varepsilon}\cdot\tilde{\sigma}^{2}_{\mathcal{S}}(x)\leq \sigma^{2}_{\mathcal{D}}(x)\leq\frac{1+\varepsilon}{1-\varepsilon}\cdot \tilde{\sigma}^{2}_{\mathcal{S}}(x).\]

**Lemma B.6** (Adapted from [5]).: _Let \(\mathcal{D}=\{x_{1},x_{2},\ldots,x_{m}\}\subset\mathcal{X}\) be a collection \(m\) points and \(\mathcal{S}\) be a random subset of \(\mathcal{D}\) constructed by including each point with probability \(p\), independent of other points. Then \(\mathcal{S}\) is an \(\varepsilon\)-accurate inducing set with probability \(1-4m\exp\left(-\frac{3p\varepsilon^{2}}{8\sigma_{\max}^{2}}\right)\), where \(\sigma_{\max}^{2}=\sup_{x\in\mathcal{X}}\sigma_{\mathcal{D}}^{2}(x)\)._

**Lemma B.7**.: _Let \(\mathrm{DUETS}\) be run with a choice of \(p_{0}=72\log(4NT/\delta^{\prime})\). Then, for all epochs \(j\geq 1\), the global inducing set \(\mathcal{S}_{j}\) is \(0.5\)-accurate with probability \(1-\delta\)._

Proof.: The statement is an immediate consequence of Lemma B.6 with the given choice of parameter \(p_{0}\). 

We are now ready to prove Lemmas B.1 and B.3.

### Proof of Lemma b.1

To ensure that the bound holds over the entire arm set we adopt a standard discretization assumption:

**Assumption B.8**.: For each \(r\in\mathbb{N}\), there exists a discretization \(\mathcal{U}_{r}\) of \(\mathcal{X}\) with \(|\mathcal{U}_{r}|=\mathrm{poly}(r)\)4 such that, for any \(f\in\mathcal{H}_{k}\), we have \(|f(x)-f([x]_{\mathcal{U}_{r}})|\leq\frac{\|f\|_{\mathcal{U}_{k}}}{r}\), where \([x]_{\mathcal{U}_{r}}=\arg\min_{x^{\prime}\in\mathcal{U}_{r}}\|x-x^{\prime}\|_ {2}\).

Footnote 4: The notation \(g(x)=\mathrm{poly}(x)\) is equivalent to \(g(x)=\mathcal{O}(x^{k})\) for some \(k\in\mathbb{N}\).

The existence of the discretization \(\mathcal{U}_{r}\) in Assumption B.8 has been justified and adopted in previous studies [28; 30]. In particular, the popular class of kernels like Squared Exponential and Matern kernels are known to be Lipschitz continuous, in which case a \(\varepsilon\)-cover of the domain with \(\varepsilon=\mathcal{O}(1/r)\) is sufficient to show the existence of such a discretization

Consider any epoch \(j\geq 2\) and let \(x\in\mathcal{X}_{j}\). Let \(\Delta(x):=f(x^{*})-f(x)\). We will obtain a bound on \(\Delta(x)\) for any general \(x\) in order establish the bound on \(\Delta_{j}\). Using the discretization from Assumption B.8 for \(\mathcal{X}_{j}\), we obtain,

\[\Delta(x) =f(x^{*})-f(x)\] \[\leq f(x^{*})-f([x^{*}]_{\mathcal{U}_{T}})+f([x^{*}]_{\mathcal{U} _{T}})-(f(x)-f([x]_{\mathcal{U}_{T}}))-f([x]_{\mathcal{U}_{T}})\] \[\leq f([x^{*}]_{\mathcal{U}_{T}})-f([x]_{\mathcal{U}_{T}})+\frac {2B}{T}.\]

Using the result from [21], we obtain the following high probability bound that holds with probability \(1-\delta\):

\[\Delta(x) \leq f([x^{*}]_{\mathcal{U}_{T}})-f([x]_{\mathcal{U}_{T}})+\frac {2B}{T}\] \[\leq\tilde{\mu}_{j}([x^{*}]_{\mathcal{U}_{T}})+\beta(\delta^{ \prime})\tilde{\sigma}_{j}([x^{*}]_{\mathcal{U}_{T}})-\tilde{\mu}_{j}([x]_{ \mathcal{U}_{T}})+\beta(\delta^{\prime})\tilde{\sigma}_{j}([x]_{\mathcal{U}_{ T}})+\frac{2B}{T}\] \[\leq\tilde{\mu}_{j}(x^{*})-\tilde{\mu}_{j}(x)+\beta(\delta^{ \prime})\tilde{\sigma}_{j}([x^{*}]_{\mathcal{U}_{T}})+\beta(\delta^{\prime}) \tilde{\sigma}_{j}([x]_{\mathcal{U}_{T}})+\frac{4B}{T},\]

where we again used Assumption B.8 in the last step. We claim that \(x^{*}\in\mathcal{X}_{j-1}\) for all \(j\geq 2\). Assuming this claim this true, we can bound the above expression as

\[\Delta(x) \leq\sup_{x\in\mathcal{X}_{j-1}}\tilde{\mu}_{j}(x^{\prime})- \tilde{\mu}_{j}(x)+\beta(\delta^{\prime})\tilde{\sigma}_{j}([x^{*}]_{\mathcal{ U}_{T}})+\beta(\delta^{\prime})\tilde{\sigma}_{j}([x]_{\mathcal{U}_{T}})+\frac{4B}{T}\] \[\leq 2\beta(\delta^{\prime})\sigma_{j,\max}+\beta(\delta^{\prime} )\tilde{\sigma}_{j}([x^{*}]_{\mathcal{U}_{T}})+\beta(\delta^{\prime})\tilde{ \sigma}_{j}([x]_{\mathcal{U}_{T}})+\frac{4B}{T},\]

where we used the update condition (Eqn. (3)) in the second step. Since \(\mathcal{S}_{j}\) is \(0.5\)-accurate (Lemma B.7), we have \(\tilde{\sigma}_{j}^{2}(x)\leq 3\sigma_{j}^{2}(x)\leq 3\sigma_{j,\max}^{2}\). On plugging this back into the above equation, we obtain,

\[\Delta(x)\leq 8\beta(\delta^{\prime})\sigma_{j,\max}+\frac{4B}{T}.\]

The statement of the lemma follows by \(\Delta_{j}=\sup_{x\in\mathcal{X}_{j}}\Delta(x)\).

We prove our claim \(x^{*}\in\mathcal{X}_{j}\) for all \(j\geq 1\) using induction. Clearly, \(x^{*}\in\mathcal{X}_{1}=\mathcal{X}\), by definition. Assume \(x^{*}\in\mathcal{X}_{j-1}\) for some \(j\geq 2\). Fix an arbitrary \(x\in\mathcal{X}_{j-1}\), from the confidence bound lemma we have:

\[\mu_{j-1}(x)-\mu_{j-1}(x^{*})\leq (f(x)-f(x^{*}))+\beta(\delta^{{}^{\prime}})(\sigma_{j}(x)+\sigma_{ j}(x^{*}))\leq 2\sigma_{j-1.\max}(x),\]

where the second inequality follows as \(f(x)\leq f(x^{*})\). As the inequality holds \(\forall x\in\mathcal{X}_{j-1}\) we must have:

\[\sup_{x\in\mathcal{X}_{j-1}}\mu_{j-1}(x)-\mu_{j-1}(x^{*})\leq 2\sigma_{j-1. \max}(x)\]

and thus indeed \(x\in\mathcal{X}_{j}\).

### Proof of Lemma b.3

We define \(E(s):=\min\{j:T_{j}\geq T/4\mid T_{1}=s\}\). Note that \(T_{j}\) is an increasing function of \(j\). Since \(T_{E(s)}\geq T/4\), we can conclude that \(E(s)+4\) is an upper bound on the number of epochs. Thus, we focus on bounding \(E(s)\). We first show that \(E(s)\) is a non-decreasing function of \(s\).

To that effect, we claim that for \(j\geq 2\) the epoch lengths satisfy the relation \(T_{j}\geq T^{1-2^{-j+1}}\cdot T_{1}^{2^{-j+1}}\). This relation follows immediately using induction. For the base case, note that \(T_{2}\geq T^{1/2}\cdot T_{1}^{1/2}\), by definition. Assume that the relation holds for \(j-1\). Thus,

\[T_{j}\geq T^{1/2}\cdot T_{j-1}^{1/2}\geq T^{1/2}\cdot T^{1-2^{-(j-1)+1-1}}\cdot T _{1}^{2^{-(j-1)+1-1}}\geq T^{1-2^{-j+1}}\cdot T_{1}^{2^{-j+1}}. \tag{6}\]

Since \(T_{j}\)'s are lower bounded by an increasing function of \(T_{1}\), the number of epochs \(E(s)\) is a non-increasing function of \(s\). Since \(T_{1}\geq\frac{T}{N}\), \(E\left(\frac{T}{N}\right)\) is an upper bound on the number of epochs for all choices of \(T_{1}\).

Let \(j^{*}=\max\{\log(\log(T)),\log(\log(N))\}\). Using the above relation for \(T_{j}\) from Eqn. (6) and the lower bound on \(T_{1}\), we have,

\[T_{j^{*}}\geq T\cdot N^{-2^{1-j}}=T\cdot\left(2^{-\frac{\log N}{2^{j}}}\right) ^{2}\geq T\cdot 2^{-2}\]

We can hence conclude that \(T_{j^{*}}\geq T/4\), which implies that \(E(T_{1})\leq j^{*}\) for all permissible choices of \(T_{1}\). Consequently, the number of epochs are bounded as \(\log(\log(\max\{N,T\}))+4\).

### Proof of Lemma b.4

For all epochs \(j\geq 1\), recall that the inducing set is constructed by including each point from \(\mathcal{D}_{j}\) with probability \(p_{j}\), independent of other points. Thus, \(|\mathcal{S}_{j}|\) is a binomial random variable with parameters \(|\mathcal{D}_{j}|=NT_{j}\) and \(p_{j}\). Using the Chernoff bound for Binomial random variables, we can conclude that

\[\Pr(|\mathcal{S}_{j}|>(1+\varepsilon)NT_{j}p_{j})\leq\exp\left(-\frac{ \varepsilon^{2}NT_{j}p_{j}}{2+\varepsilon}\right).\]

Invoking the bound with \(\varepsilon=2+\log(1/\delta^{\prime})\), with \(\delta^{\prime}=\delta/(\log\log(NT)+4)\) yields that the following relation holds with probability \(1-\delta^{\prime}\):

\[|\mathcal{S}_{j}| \leq(3+\log(1/\delta^{\prime}))\cdot NT_{j}p_{j}\] \[\leq(3+\log(1/\delta^{\prime}))\cdot NT_{j}\cdot p_{0}\sigma_{j, \max}^{2}\] \[\leq(3+\log(1/\delta^{\prime}))\cdot NT_{j}p_{0}\cdot C_{f, \mathcal{X}}\cdot\frac{\gamma_{NT_{j}}}{NT_{j}}\] \[\leq(3+\log(1/\delta^{\prime}))p_{0}\gamma_{NT},\]

where we used Lemma B.2 in the third step and monotonicity of \(\gamma_{m}\) in the last step. On taking a union bound over all epochs and using the bound on the number of epochs from Lemma B.3, we conclude that for all epochs \(j\), \(|\mathcal{S}_{j}|=\tilde{\mathcal{O}}(\gamma_{NT})\) with probability \(1-\delta\).

Shared Randomness in DUETS

The setup in DUETS where the server and each agent have access to shared randomness is similar to the class of _public coin protocols_ that have been extensively studied in field of Information Theory. Such class of algorithms assume that agents and the server share a common source of randomness that is independent of the environment and comes at no communication cost between agents and the server Park et al.[19], Jayadev et al.[2], Jayadev et al.[1], Wang et al.[31]. Public coin protocols can concretely be implemented as an agent-server agreement on the generating seeds before the start of the algorithm, which takes at most \(\mathcal{O}(1)\) communication cost.

We stress that the source of shared randomness between the agent and the server is established before any interaction with the environment and thus cannot carry any information relevant to learning the reward function. It simply allows agents to follow an agreed on non-adaptive randomized strategy while querying the environment similar to the randomized strategy outlined in Jayadev et al. [2] for choice of communication channels.

## Appendix D Empirical Studies

In this section, we provide additional details about our empirical studies along with results on benchmark functions.

We perform several empirical studies to corroborate our theoretical findings. We compare the regret performance and communication cost of our proposed algorithm, DUETS, against three baseline algorithms -- DisKernelUCB, ApproxDisKernelUCB and N-KernelUCB. The first two are distributed kernel bandits algorithms proposed in Li et al. [15]. N-KernelUCB is a baseline algorithm considered in Li et al. [15] where each agent locally runs the GP-UCB algorithm Gopalan and Chowdhury [7] with no communication among the agents.

Figure 1: Cumulative regret (Fig. (a)a-(d)d) and communication cost (e-1h) for all algorithms across different benchmark functions averaged over \(5\) Monte Carlo runs. The shaded region represents error bars corresponding to one standard deviation. DUETS obtains a superior performance, both in terms of regret and communication cost, over other algorithm across all functions.

We compare the performance of all the four algorithm across four benchmark functions. The first two are synthetic functions \(h_{1},h_{2}:\mathcal{B}\rightarrow\mathbb{R}\) considered in Li et al. [15], where \(\mathcal{B}\) denotes the unit ball centered at origin in \(\mathbb{R}^{10}\). The functions are given by:

\[h_{1}(x):=\cos(3x^{\top}\theta^{\star});\quad h_{2}(x):=(x^{\top}\theta^{\star })^{3}-3(x^{\top}\theta^{\star})^{2}+3(x^{\top}\theta^{\star})+3.\]

For both the functions \(\theta^{\star}\) is randomly chosen from the surface of the unit ball \(\mathcal{B}\). The other two functions are Branin [4, 20] and Hartmann-4D [20], which are commonly used benchmark functions for Bayesian Optimization. The Branin function is defined over \(\mathcal{X}=[0,1]^{2}\) while the Hartmann-4D function is defined over \(\mathcal{X}=[0,1]^{4}\).

We consider a distributed kernel bandit described in Section 2 with \(N=10\) agents. For all the experiments, we use the Squared Exponential kernel. The length scale was set to \(0.2\) for Branin and to \(1\) for all other functions. The observations were corrupted with zero mean Gaussian noise with a standard deviation of \(0.2\). The parameter \(D\) for ApproxDisKernelUCB and DisKernelUCB was set to \(20\) and \(10\) respectively. For DUETS, we set \(T_{1}=2\) and \(p_{0}=10\). The parameter \(\beta\) was selected using a grid search over \(\{0.2,0.5,1,2,5\}\) for all the algorithms. All the algorithms were run for \(T=50\) time steps. We averaged the cumulative regret and the communication cost incurred by different algorithms over \(5\) Monte Carlo runs.

The cumulative regret incurred by different algorithms across the different benchmark function are shown in the top row of Figure 1. The bottom row consists of the corresponding plots for the communication cost incurred by the different algorithm. The shaded regions denotes error bars upto standard deviation on either side of the mean value. As evident from the plots, DUETS achieves a significantly lower regret as compared to all other algorithms consistently across benchmark functions. DUETS also incurs a smaller communication overhead as compared to other algorithms, corroborating our theoretical results.

All simulations were run on an Intel(R) Xeon(R) E-2176M CPU@ 2.70GHz with 6 cores with no GPU's. Runtime of 5 Monte-Carlo-simulations for all 4 algorithms on a single data-set took roughly 2 hours of CPU time. Estimated runtime of all conducted experiments is 8 hours.