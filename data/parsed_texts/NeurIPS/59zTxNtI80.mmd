###### Abstract

Recent works have shown that diffusion models are able to memorize training images and emit them at generation time. However, the metrics used to evaluate memorization and its mitigation techniques suffer from dataset-dependent biases and struggle to detect whether a given specific image has been memorized or not. This paper begins with a comprehensive exploration of issues surrounding memorization metrics in diffusion models. Then, to mitigate these issues, we introduce SolidMark, a novel evaluation method that provides a per-image memorization score. We then re-evaluate existing memorization mitigation techniques and show that SolidMark is capable of evaluating fine-grained pixel-level memorization. Finally, we release a variety of models based on SolidMark to facilitate further research for understanding memorization phenomena in generative models. We include our supplementary materials at https://drive.google.com/drive/u/3/folders/lvpu5FM_Gs1ldFogw405p-ehljynA85R.

###### Contents

* 1 Introduction
* 2 Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Rombach et al., 2022) have gained prominence because of their ability to generate remarkably photorealistic images. However, they have also been subject to scrutiny and litigation (Saveri & Butterick, 2023) owing to their ability to regurgitate potentially copyrighted training images. Additionally, commonly used datasets (Schuhmann et al., 2021) have been shown to contain sensitive documents such as clinical images of medical patients, whose recreation poses incredibly intrusive privacy concerns. As a result, recent works (Sompealli et al., 2023;p Carlini et al., 2023; Wen et al., 2024; Ren et al., 2024; Kumari et al., 2023b) have looked to quantify, explain, and mitigate memorization in diffusion models.
* 35 Crucially, reliable and effective quantification of memorization requires sound metrics. Although a few proposed metrics serve as powerful memorization indicators, there exist disagreements in terms of how they should be applied (Chen et al., 2024). The typical way in which a _given_ image is declared to be memorized is if it is produced in a pixel-exact manner at inference time. However, such a generation can be challenging to induce, even if the training prompt is known, due to inherent stochasticity present in diffusion model inference. This problem is even harder in unconditional models, where there are no knobs to guide the generation towards a given target image. If such a generation is not observed, the user is not provided with any strong indication on whether the model has knowledge of the image.
* 43 Memorization metrics usually consist of (i) some distance measure \(\ell\) between a model generation and its training dataset1 and (ii) some scoring function that takes in a large number of these distance values (from many generations) and outputs a scalar metric. For example, a commonly used metric for memorization is the 95th percentile (scoring function) of SSCD similarities (Pizzi et al., 2022; Sompealli et al., 2023; Chen et al., 2024), an embedding-based distance between each generation and its nearest training image.

Footnote 1: Other works (Sompealli et al., 2023; Chen et al., 2024) use a similarity \(\sigma\) instead, but flipping signs makes these interchangeable, so we will use the most natural measure in each case.

In this paper, we propose SolidMark, an approach that allows for the precise quantification of pixel-level memorization. The basic idea is simple: SolidMark augments each image with a grayscale border of _random intensity_ (see Fig. 1). At evaluation time, we in only the image's border in a task we call outpainting (as an analogy to inpainting). Since the pattern is randomized _independently_ for each image, a correct reconstruction of the pattern's color indicates strong memorization of the sample. The idea of using this pattern is closely related to watermarking as it is reflective of the source of an image generation, but there are also some key differences that distinguish it: (i) a watermark should be difficult to remove or forge, whereas our pattern is easily removable; (ii) a watermark only needs to be detectable, but our pattern needs to be precisely reconstructed to provide a continuous metric for quantifying memorization; (iii) the value of the key should be unrelated from the content of the image, which is not required for a watermark.

We designed SolidMark to be included in new models or finetuned into existing ones. Since the image's border can be easily cropped out when using generated images, SolidMark is a efficient way to evaluate memorization in diffusion models. To encourage further exploration, we release a Stable Diffusion (SD) 2.1 model injected with SolidMark's patterns during pretraining. Subsequently, we re-evaluate existing memorization mitigation techniques with SolidMark. We demonstrate the method's ability to evaluate fine-grained pixel-level memorization and its universal compatibility, testing it on five different datasets in a variety of settings. We provide in Table 1 a summary of the strengths and weaknesses of SolidMark compared to existing evaluation methods in the field.

Our main contributions are the following:

* _An in-depth exploration of existing memorization metrics,_
* SolidMark_, a new method for precise evaluation of pixel-level memorization,_
* _A variety of models trained specially for evaluating memorization._

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Metric & Reconstructive & Pixel-Level & Evaluation of & Caveat \\  & Memorization & Memorization & Any Image & \\ \hline SSCD Similarity & ✓ & ✗ & ✗ & Out-of-Distribution Datasets \\ \(\ell_{2}\) Distance & ✗ & ✓ & ✗ & Monochromatic Images \\ \(061\) & SolidMark & ✗ & ✓ & ✓ & Excessive Duplication \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Use Cases of Different Metrics.**

Figure 1: **An overview of SolidMark. We begin by augmenting training images with random scalar keys in the form of grayscale borders. Next, we inject these keys into the model by training it on these augmented images. To query for a key, we ask the model to outpaint a training image’s border using the training caption as the text prompt. We retrieve its prediction at the key by averaging the outpainted border. Finally, we report the distance between the predicted key and the true value.**

## 2 Background and Related Work

Detecting Memorization in Diffusion Models.Many works have aimed to detect memorization in diffusion models (Sompealli et al., 2023; Carlini et al., 2023; Kumari et al., 2023b). A generative model that memorizes data might be especially vulnerable to membership inference attacks, in which the goal is to determine whether an image belongs to the original training set (Carlini et al., 2022; Hu & Pang, 2021; Wen et al., 2023). One notable example of a membership inference attack is an inpainting attack from Carlini et al. (2023), who show that a diffusion model's performance on the inpainting task significantly increases for memorized images.

Mitigating Unwanted Generations.A number of works (Sompealli et al., 2023; Chen et al., 2024; Wen et al., 2024; Ren et al., 2024) have introduced methods to mitigate memorization in diffusion models. These methods either perturb training data to decrease memorization as the model trains or perturb inputs at test time to decrease the model's chances of recalling memorized information. Although most mitigation techniques usually involve augmenting data with some type of noise (Sompealli et al., 2023b), other works attempt to alter generation trajectories using intuition about the causes for memorization (Chen et al., 2024). To prevent Stable Diffusion models from generating unwanted outputs, various concept erasure techniques have been proposed (Gandikota et al., 2023; Pham et al., 2024; Gandikota et al., 2024; Kumari et al., 2023a). Although these methods were initially developed to erase broad concepts, they can also target specific images.

Image Watermarking.Classically, image watermarking allows for the protection of intellectual property and has been accomplished for years with simple techniques like Least Significant Bit embedding (Wolfgang & Delp, 1996). Recently, more complex deep learning-based methods (Zhu et al., 2018; Zhang et al., 2019; Lukas & Kerschbaum, 2023) have been suggested. For generative models, watermarking allows developers to discrectly label their model-generated content, mitigating the impact of unwanted generations by increasing their traceability. Some works attempt to fine-tune watermarks into existing diffusion models (Zhao et al., 2023; Fernandez et al., 2023; Xiong et al., 2023; Liu et al., 2023).

Needle-in-a-Haystack Evaluation for LLMs.Some recent works (Fu et al., 2024; Kuratov et al., 2024; Wang et al., 2024; Levy et al., 2024) have used Needle-in-a-Haystack (NIAH) evaluation (Kamradt, 2023) to test the long-context understanding and retrieval capabilities of Large Language Models (LLMs). In this test, a short, random fact (needle) is placed in the middle large body of text (hyastack). This augmented corpus is passed into the model at inference. Subsequently, the model is asked to recall the needle; by changing the size of the context window and shifting the needle around, testers are able to evaluate the in-context retrieval capabilities of LLMs. If the model is able to successfully retrieve the needle from the haystack with a high consistency, developers can be more confident that it will be able to recall specific information from large context windows. Similar to how NIAH evaluation takes a large context window and injects a small, unrelated phrase as a key, we inject our training images with scalar keys using a small, unrelated border.

## 3 Existing Memorization Evaluation Methods

Types of Memorization.Memorization in diffusion models can usually be classified into either pixel-level or reconstructive. Pixel-level memorization (Carlini et al., 2023), is identified by a near-identical reconstruction of a particular training image. That is, even if a generation contains recreations of certain objects or people from the training data, a given generation would only be considered reflective of pixel-level memorization if the full image was almost entirely identical to a specific training image. In this sense, the process of recovering a pixel-level memorized image is analogous to extracting a training image from the model. Alternatively, reconstructive memorization represents a more semantic type of data replication. It is identified by the replication of specific objects or people found in training images, even if the generation in question has a high pixel distance from all training images (Sompealli et al., 2023a).

Measuring Memorization.Neither pixel-level nor reconstructive memorization have precise mathematical definitions, making it rather difficult to declare whether or how strongly a trainingimage is memorized. Instead, when constructing metrics, the prior works attempt to construct mathematical measures for a given generation's similarity to the model's training set. These measures, in turn, can identify memorizations when they occur at generation time. Specifically, for a training dataset \(\bm{X}\) and a generation \(\hat{\bm{x}}_{0}\), researchers will either use some distance function \(\ell(\hat{\bm{x}}_{0},\bm{X})\), with lower values indicating a higher likelihood of memorization, or a similarity function \(\sigma(\hat{\bm{x}}_{0},\bm{X})\), with higher values indicating a higher likelihood of memorization. After collecting these values for a large number of generations, they are converted into an overall score for a model: for example, the 95th percentile of all similarities is a common scoring function (Somepalli et al., 2023; Chen et al., 2024). Past works also track the overall maximum similarity value (Chen et al., 2024). Notably, Carlini et al. (2023) track the proportion of generations with distances under a certain threshold, defined as "eidetic" memorization. We use similar language, which we define in Definitions 1, 2.

**Definition 1** (Eidetic Metric).: _A metric that counts the number of distances \(\ell\) below a threshold \(\bm{\delta}\)._

**Definition 2** (Eidetic Memorization).: _A training image \(\bm{x}\) is said to be \((\ell,\bm{\delta})\)-eidetically memorized if the respective model returns a generation \(\hat{\bm{x}}_{0}\) where \(\ell(\hat{\bm{x}}_{0},\bm{x})\leq\bm{\delta}\)._

### Evaluating Existing Distance Functions

Modified \(\ell_{2}\) Distance.A common choice of the distance function \(\ell\) as an indicator for pixel-level memorization is a modified \(\ell_{2}\) distance that was introduced in Carlini et al. (2023). For this, following Balle et al. (2022), Carlini et al. (2023) start building their metric from the baseline of normalized Euclidean 2-norm distance, defined as

\[\ell_{2}(\bm{a},\bm{b})=\sqrt{\frac{\sum_{i}(a_{i}-b_{i})^{2}}{d}}\]

for \(\bm{a},\bm{b}\in\mathbb{R}^{d}\). When using this distance \(\ell_{2}(\hat{\bm{x}}_{0},\bm{x})\) between a generation \(\hat{\bm{x}}_{0}\) and its nearest neighbor \(\bm{x}\) in the training set \(\bm{X}\), they find that nearly monochromatic images, such as images of a small bird in a large blue sky, dominate the reported memorizations.

To counteract this issue, Carlini et al. (2023) rescale the \(\ell_{2}\) distance of a generation based on its relative distance from the set \(\mathbb{S}_{\hat{\bm{x}}_{0}}\) of \(\hat{\bm{x}}_{0}\)'s \(n\) nearest neighbors in \(\bm{X}\). Namely, for \(\mathbb{S}_{\hat{\bm{x}}_{0}}\subseteq\bm{X}\) and \(|\mathbb{S}_{\hat{\bm{x}}_{0}}|=n\), we have that

\[\forall_{\bm{x}\in\bm{X}\backslash\mathbb{S}_{\hat{\bm{x}}_{0}}}\ell_{2}(\hat {\bm{x}}_{0},\bm{x})\geq\max_{y\in\mathbb{S}_{\hat{\bm{x}}_{0}}}\ell_{2}(\hat {\bm{x}}_{0},\bm{y})\,.\]

They then define the modified \(\ell_{2}\) distance as

\[\bar{\ell}_{2}(\hat{\bm{x}}_{0},\bm{X};\mathbb{S}_{\hat{\bm{x}}_{0}})=\frac{ \ell_{2}(\hat{\bm{x}}_{0},\bm{x})}{\alpha\cdot\mathbb{E}_{\bm{y}\in\mathbb{S}_ {\hat{\bm{x}}_{0}}}[\ell_{2}(\hat{\bm{x}}_{0},\bm{y})]}\,,\]

where \(\alpha\) is a scaling factor. This distance decreases when \(\hat{\bm{x}}_{0}\) is much closer to its nearest neighbor when compared to its \(n\) nearest neighbors, potentially indicative of memorization.

Following their setting, we conducted experiments using DDPMs pretrained on CIFAR-10 (Krizhevsky, 2009). See Appendix Section A for implementation details. In Figure 2, we show examples of the strongest memorizations reported by \(\bar{\ell}_{2}\) distance, demonstrating that the measure still reports monochromatic images as false positives. Most of the reported memorizations were only classified as such because they are blurry and monochromatic (which gives them an easier time matching other monochromatic images in the training set). Crucially, though, these images are _not_ memorizations, because they do not contain any specifically recreated image features unique to the training set (Naseh et al., 2023). Because of this lack of specificity, we found that their metric was not a satisfying solution to detect pixel-level memorization. We apply more scrutiny to memorization metrics based on \(\ell_{2}\) distance, as this bias towards monochromatic images has proven remarkably difficult to thoroughly eliminate.

Embedding-Based Similarity.Although pixel-wise distances present an intuitive approach for detecting pixel-level memorization, they are not as tailored towards reconstructive memorization. Instead, for the reconstructive case where semantic similarity is more relevant, perceptual similarity measures based on models such as SSCD (Pizzi et al., 2022), DINO (Caron et al., 2021), and CLIP (Radford et al., 2021) are often used (Sompalli et al., 2023; Carlini et al., 2023). These metrics are generally structured with dot product similarities in a semantic embedding space, such as:

\[\sigma(\hat{\bm{x}}_{0},\bm{x})=\langle E(\hat{\bm{x}}_{0}),E(\bm{x})\rangle\]

where \(E(\bm{x})\) represents the embedding of an image \(\bm{x}\) generated by a deep visual encoder. Perceptual metrics are robust to slight perturbations of training images such as small perspective changes. Although they perform well with reconstructive memorization, models like DINO suffer with detecting pixel-level memorization (Sompalli et al., 2023).

One important quality of a memorization metric is the ability to remain effective and precise across different datasets. Unfortunately, past works (Carlini et al., 2023) have seen issues when attempting to translate perceptual metrics that work on Stable Diffusion to other datasets. Therefore, although the literature denotes SSCD as the standard metric for detecting reconstructive memorization (Sompalli et al., 2023; Chen et al., 2024), it should likely only be used with datasets such as LAION-5B (Schuhmann et al., 2022) or ImageNet (Deng et al., 2009) that fit its training dataset.

### Inspecting Scoring Strategies

Until now, we have only discussed the importance of using a consistent and reliable distance measure. It is just as important to use a scoring function that is sensitive to overall changes in memorization and does not fluctuate with unrelated changes in the model. Three strategies to aggregate a set of distances into a score include:

(i) the 95th percentile of similarities, (ii) the maximum similarity value, and (iii) eidetic metrics. Recently, 95th percentile scoring was employed in Sompalli et al. (2023), where the 95th percentile of SSCD similarities was used as a metric for a number of memorization mitigation techniques. Subsequent work (Chen et al., 2024), however, questioned the validity of percentile-based scoring strategies in memorization metrics, especially when the returned distribution of distances is heavy-tailed. Figure 3 shows an example where a percentile metric could misrepresent a distribution of similarities. As a remedy, Chen et al. (2024) propose two alternatives. They recommend tracking (i) the maximum of all similarities and (ii) the number of similarities that lie above a certain threshold, a scoring idea introduced in Carlini et al. (2023). Using the maximum of all similarities could be susceptible to outliers and may not necessarily be representative of large scale trends in the similarity distribution. On the other hand, recording the number of similarities above a threshold \(\bm{\delta}\), also known as _eidetic memorization_, has proved to be effective. Importantly, existing literature (Sompalli et al.,

Figure 3: **95th percentile scoring fails to capture fine-grained reductions in memorization.** The above graphs demonstrate how a 95th percentile metric can fail to report successful memorization reduction. (**Top**) A distribution showing the density (vertical axis) of different similarity values (horizontal axis) in a model’s baseline results. (**Bottom**) The memorization-reduced evaluation, where the 95th percentile did not change at all despite clear memorization reductions shown in the 96th percentile.

Figure 2: \(\bar{\ell}_{2}\) **distance reports monochromatic images as memorizations.** Despite not being memorizations of their nearest neighbors in the training set, monochromatic images generate a low \(\bar{\ell}_{2}\) distance. (**Top**) Out of 5,000 generations, the 10 generations with smallest patched \(\bar{\ell}_{2}\) distance from CIFAR-10 train. (**Bottom**) The corresponding nearest neighbors in CIFAR-10 train to the top row of generations.

2023b; Chen et al., 2024; Kumari et al., 2023b) uses eidetic metrics with only one threshold. The problem with single threshold methods is that they do not probe how the distribution of similarities could be concentrated. Instead, multiple values for \(\bm{\delta}\) should be tracked to avoid flawed analysis. We elaborate on this point below in our experiments.

## 4 SolidMark: A Method to Evaluate Per-Image Memorization

Motivation.Performance on inpainting tasks significantly increases for memorized images (Carlini et al., 2023; Daras et al., 2024). Therefore, we choose inpainting as the foundation of our method. This task also stands out because of its ability to function as a key-query mechanism: by masking out part of a training image, we can provide the unmasked portion to the model as a 'query' and ask it to recall the 'key' (the masked portion) from memory. Yet, two issues need consideration:

First, with inpainting, the key is almost definitely semantically related to the query, meaning the model still has a good chance to infer the masked portion of an unseen image. Additionally, the amount of useful information in the unmasked portions of different images may vary significantly, making it difficult to develop a general baseline for the model's performance on an unmemorized image. That is, it would be harder to inpaint a memorized complex image than certain unmemorized simple images. Second, since the key for inpainting is essentially a smaller image, the problems with earlier distance metrics could just propagate. For example, relatively accurate inpaintings might still produce high \(\ell_{2}\) distances for various reasons.

Method Structure.To solve these issues, we assign a random scalar key to each image and embed it as the intensity of a grayscale border around that training image. By training the model on these augmented images, we teach it to output the correct grayscale intensity in the borders of an image, if memorized (see Fig. 1). Since the keys and queries are unrelated, the model outputs random grayscale borders from the distribution of the training keys for an unmemorized image.

At evaluation time, we prompt the diffusion model to outpaint the border for a training image using the training caption as the text prompt and evaluate its accuracy with a scalar distance function between the grayscale intensities. This strategy solves both of our previous issues: First, since the key is unrelated to the query, we minimize the probability of inferring the key by chance. Second, since our key is a scalar, we can directly use a scalar distance function between keys (grayscale intensities) instead of using a pixel distance function. We refer to this distance as \(\ell_{\text{SM}}\) (SM = SolidMark). We provide pseudocode and explain SolidMark's hyperparameters in Appendix Section C.

## 5 Evaluation

Initial ValidationsSince visual transformer models have been shown to pay extra attention to the center of images (Raghu et al., 2021), we were concerned that keeping the patterns as borders would uncover less memorizations than a centered pattern. For this reason, we ablated for the position of the pattern on STL-10 (Coates et al., 2011), for which the results and implementation details are in D. Although centered patterns did perform slightly better, we still choose to use border patterns, since the performance benefit is not worth the intrusiveness to the image generation. We also validate in Appendix Section E that SolidMark is able to evaluate memorization in unconditional models.

Pretraining a Foundation ModelWe describe the process of pretraining a large foundation model to facilitate the widespread use of SolidMark in Appendix Section J. We also show samples of images generated by our pretrained text-to-image model.

### The Role of Data Duplication

One important concern about SolidMark is that, since its keys are completely random, it sees no association between duplicated images in the training set (duplicate images will have different border colors). For this reason, one may worry that it could fail to capture memorization induced by data duplication, which is one of the most important contributing factors to memorization (Somepalli et al., 2023b). Following this concern, we introduced a large amount of exact data duplication into LAION-5K, a randomly sampled 5,000 image subset of LAION-400M (Schuhmann et al., 2021).

Next, we assigned each of these duplicated images independent random keys to mimic how they would receive different keys in practice. We then finetuned SD 2.1 on this subset and evaluated the percentage of images for which SolidMark reported memorization of at least one of its respective keys. Table 2 shows that SolidMark still reports increased memorization as training set duplication increases. Implementation details are in Appendix Section F.

### How Fine-Grained is SolidMark?

In order to understand the cues the model uses to construct memorized borders, we evaluate whether the information the model utilizes is based on the semantics of the image or on more fine-grained pixel-exactness. We evaluated changes in reported memorization as a response to small perturbations applied to the query image. To do this, we augmented LAION-5K with SolidMark's borders and finetuned SD 2.1 on the augmented dataset. At evaluation time, we applied different augmentations like cropping, rotation, or blurring to the query image and observed changes in the model's memorization performance. Examples of these augmentations and implementation details are in Appendix Section G. Overall, our results in Table 4 show that even minor perturbations to query images significantly disrupt the model's ability to recall the border color, especially when the required accuracy \(\bm{\delta}\) is small. These changes are not semantically meaningful and are sometimes barely visually perceptible. For this reason, we classify SolidMark's reported memorizations as instances of fine-grained pixel-level memorization.

### Re-Evaluating Mitigation Techniques

We use SolidMark to evaluate the degree of pixel-level memorization mitigation, achievable with inference-time memorization reduction techniques. We sourced these mitigation techniques, which are described in Appendix Section H, from Sompealli et al. (2023b). For our evaluation, we augmented LAION-5K with our solid borders and finetuned SD 2.1 on this augmented dataset. We then compared the percentage decrease in \((\ell_{\text{SM}},0.01)\)-eide memorizations in our model against the percentage decrease of 95th percentile SSCD similarities observed in Sompealli et al. (2023b). See Table 3 for these results. Overall, we did not find that any of the mitigation techniques that we tried significantly reduced memorization as measured by SolidMark. These results are corroborated by our results in Appendix Section I. We propose this difference exists because SolidMark is an evaluation method primarily led by visual cues in its query image. Perturbations to the queries that could, at best, dilute or change the semantic meaning of the prompt, lack a profound effect on the model's performance when the dominant visual cues are still present.

## 6 Discussion

**Evaluating Individual Images.** SolidMark is unique among memorization metrics in its ability to directly evaluate specific training images. In a traditional setting, one would need to repeatedly prompt a model and randomly encounter a training image to decide that it was memorized. This is problematic because prompting the model repeatedly with a very common training caption has a low

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Replications of Training Example** & \(\bm{\delta}=0.1\) & \(\bm{\delta}=0.05\) & \(\bm{\delta}=0.005\) \\ \hline
2 Instances & 50\% & 36\% & 10\% \\
3 Instances & 56\% & 60\% & 26\% \\
4 Instances & 56\% & 56\% & 24\% \\
5 Instances & 68\% & 72\% & 36\% \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Reported Memorizations with Increasing Duplication.** SolidMark is able to detect increased memorization in models as a response to increased duplication in the training set, even if the duplicates are assigned different keys. This is evidenced by an increase in the percentage of images reported as memorized at all eidetic thresholds \(\bm{\delta}\) as we increase the number of instances of duplicated images in the training set. Higher percentages indicate more memorizations. Implementation details are in Appendix Section F.

chance of reproducing a given target image. Additionally, in unconditional models, which have been shown to memorize sensitive medical imaging data (Dar et al., 2024), there is no direct way to guide the output towards a specific image. SolidMark, in both cases, provides an effective method to test for the memorization of specific images. In addition, it provides a continuous measure of "how memorized" an image is.

Limitations. By the nature of the difficulty of the setting, our method may not report memorizations that are not strong enough to capture the key. Additionally, our evaluation method has a false positive probability based on the chance of an unmemorized color randomly fitting to the key of a specific image. Additionally, SolidMark may struggle with accurately reporting memorization caused by excessive exact duplication. For this reason, we encourage its use in tandem with other metrics. For an in-depth guide on how we recommend choosing a metric for a specific use case, see Appendix Section K.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Random Crop Strength** & \(\bm{\delta}=0.1\) & \(\bm{\delta}=0.05\) & \(\bm{\delta}=0.005\) \\ \hline Baseline (0) & 1085 & 557 & 68 \\
1 & 1038 (\(4.33\%\downarrow\)) & 549 (\(1.44\%\downarrow\)) & 58 (\(14.71\%\downarrow\)) \\
2 & 1060 (\(2.30\%\downarrow\)) & 541 (\(2.87\%\downarrow\)) & 48 (\(29.41\%\downarrow\)) \\
3 & 1035 (\(4.61\%\downarrow\)) & 552 (\(0.90\%\downarrow\)) & 49 (\(27.94\%\downarrow\)) \\
4 & 1042 (\(3.96\%\downarrow\)) & 528 (\(5.21\%\downarrow\)) & 50 (\(26.47\%\downarrow\)) \\ \hline
**Rotation Angle** & & & \\ \hline Baseline (\(0^{\circ}\)) & 1085 & 557 & 68 \\ \(-2^{\circ}\) & 1029 (\(5.16\%\downarrow\)) & 506 (\(9.16\%\downarrow\)) & 51 (\(25.00\%\downarrow\)) \\ \(-1^{\circ}\) & 1053 (\(2.95\%\downarrow\)) & 540 (\(3.05\%\downarrow\)) & 59 (\(13.24\%\downarrow\)) \\
409 & 1008 (\(7.10\%\downarrow\)) & 502 (\(9.87\%\downarrow\)) & 51 (\(25.00\%\downarrow\)) \\
410 & 2\({}^{\circ}\) & 1047 (\(3.50\%\downarrow\)) & 528 (\(5.21\%\downarrow\)) & 45 (\(33.82\%\downarrow\)) \\
411 & 180\({}^{\circ}\) & 1044 (\(3.78\%\downarrow\)) & 522 (\(6.28\%\downarrow\)) & 47 (\(30.88\%\downarrow\)) \\ \hline
**Gaussian Blur Strength** & & & \\ \hline Baseline (0) & 1085 & 557 & 68 \\
1 & 1049 (\(3.32\%\downarrow\)) & 516 (\(7.36\%\downarrow\)) & 42 (\(38.24\%\downarrow\)) \\
2 & 1064 (\(1.94\%\downarrow\)) & 503 (\(9.69\%\downarrow\)) & 45 (\(33.82\%\downarrow\)) \\
416 & 3 & 1089 (\(0.37\%\uparrow\)) & 534 (\(4.13\%\downarrow\)) & 53 (\(22.06\%\downarrow\)) \\
417 & 4 & 1033 (\(4.79\%\downarrow\)) & 506 (\(9.16\%\downarrow\)) & 62 (\(8.82\%\downarrow\)) \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Memorizations Reported with Increasing Augmentation Strength. We show that SolidMark, especially as \(\bm{\delta}\) decreases, reports extremely fine-grained memorizations. As we apply random cropping, rotation, and blurring to query images, the model’s key prediction accuracy, measured by the number of reported \((\ell_{\text{SM}},\bm{\delta})\)-eidetic memorizations, significantly deteriorates. Higher reduction percentages indicate that the model is struggling to recognize the augmented images.**

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Metric** & **GNI** & **RT** & **CWR** & **RNA** \\ \hline
95th Percentile of SSCD Similarities & 3.62\% \(\downarrow\) & **16.29\%**\(\downarrow\) & 9.20\% \(\downarrow\) & 14.33\% \(\downarrow\) \\

[MISSING_PAGE_POST]

\* [432] Reproducibility Statement
* [433] We include our source code for this project in the supplementary materials and release a variety of trained models to ensure that reviewers and readers can try out SolidMark for themselves.

### Impact Statement

We introduce SolidMark as a non-intrusive framework that can help developers evaluate and study memorization in their models. With our recommendations for how memorization metrics should be built, we hope to foster discussion about how existing metrics can be improved upon, interpreted, and generalized. Altogether, more robust evaluation of generative models helps mitigate negative privacy outcomes owing to uncaught memorization.

## References

* [1] Borja Balle, Giovanni Cherubin, and Jamie Hayes. Reconstructing training data with informed adversaries. In _43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco, CA, USA, May 22-26, 2022_, pp. 1138-1156. IEEE, 2022. doi: 10.1109/SP46214.2022.9833677. URL https://doi.org/10.1109/SP46214.2022.9833677.
* [2] Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and Florian Tramer. Membership inference attacks from first principles. In _43rd IEEE Symposium on Security and Privacy, SP 2022, San Francisco, CA, USA, May 22-26, 2022_, pp. 1897-1914. IEEE, 2022. doi: 10.1109/SP46214.2022.9833649. URL https://doi.org/10.1109/SP46214.2022.9833649.
* [3] Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In Joseph A. Calandrino and Carmela Troncoso (eds.), _32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023_, pp. 5253-5270. USENIX Association, 2023. URL https://www.usenix.org/conference/usenixsecurity23/presentation/carlini.
* [461] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pp. 9630-9640. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00951. URL https://doi.org/10.1109/ICCV48922.2021.00951.
* [467] Chen Chen, Daochang Liu, and Chang Xu. Towards memorization-free diffusion models. _2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 8425-8434, 2024. URL https://api.semanticscholar.org/CorpusID:266819999.
* [470] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Geoffrey Gordon, David Dunson, and Miroslav Dudik (eds.), _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15 of _Proceedings of Machine Learning Research_, pp. 215-223, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR. URL https://proceedings.mlr.press/v15/coateslla.html.
* [471] Salman Ul Hassan Dar, Marvin Seyfarth, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan O. Schoenberg, Norbert Frey, Bettina Baehler, Sebastian Foersch, Daniel Truhn, Jakob Nikolas Kather, and Sandy Engelhardt. Unconditional latent diffusion models memorize patient imaging data: Implications for openly sharing synthetic data, 2024. URL https://arxiv.org/abs/2402.01054.
* [480] Giannis Daras, Alex Dimakis, and Constantinos Daskalakis. Consistent diffusion meets tweedie: Training exact ambient diffusion models with noisy data. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=P1VjIGaFdH.
* [481]* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pp. 248-255, 2009. doi: 10.1109/CVPR.2009.5206848.
* Fernandez et al. (2023) Pierre Fernandez, Guillaume Couairon, Herve Jegou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In _IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023_, pp. 22409-22420. IEEE, 2023. doi: 10.1109/ICCV51070.2023.02053. URL https://doi.org/10.1109/ICCV51070.2023.02053.
* Fu et al. (2024) Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=TaAge071Uh.
* Gandikota et al. (2023) Rohit Gandikota, Joanna Materzynska, Jaden Fietto-Kaufman, and David Bau. Erasing concepts from diffusion models. In _IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023_, pp. 2426-2436. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00230. URL https://doi.org/10.1109/ICCV51070.2023.00230.
* Gandikota et al. (2024) Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. Unified concept editing in diffusion models. In _IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2024, Waikoloa, HI, USA, January 3-8, 2024_, pp. 5099-5108. IEEE, 2024. doi: 10.1109/WACV57701.2024.00503. URL https://doi.org/10.1109/WACV57701.2024.00503.
* Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
* 19, 2021_, pp. 2387-2389. ACM, 2021. doi: 10.1145/3460120.3485338. URL https://doi.org/10.1145/3460120.3485338.
* pressure testing llms. 2023. URL https://github.com/gkamradt/LLMTest_NeedleInAhaystack.
* Krizhevsky (2009) Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009. URL https://api.semanticscholar.org/CorpusID:18268744.
* Kumari et al. (2023a) Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models. In _IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023_, pp. 22634-22645. IEEE, 2023a. doi: 10.1109/ICCV51070.2023.02074. URL https://doi.org/10.1109/ICCV51070.2023.02074.
* Kumari et al. (2023b) Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models, 2023b. URL https://doi.org/10.1109/ICCV51070.2023.02074.
* Kuratov et al. (2024) Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of needles in a 11m haystack: Recurrent memory finds what llms miss, 2024. URL https://arxiv.org/abs/2402.10790.
* Levy et al. (2020) Mosh Levy, Alon Jacoby, and Yoav Goldberg. Same task, more tokens: the impact of input length on the reasoning performance of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), _Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024_,pp. 15339-15353. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. ACL-LONG.818. URL https://doi.org/10.18653/v1/2024.acl-long.818.
* Liu et al. (2023) Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, and Yang Zhang. Watermarking diffusion model. _CoRR_, abs/2305.12502, 2023. doi: 10.48550/ARXIV.2305.12502. URL https://doi.org/10.48550/arXiv.2305.12502.
* Liu et al. (2015) Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* Lugmayr et al. (2022) Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 11451-11461. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01117. URL https://doi.org/10.1109/CVPR52688.2022.01117.
* Lukas and Kerschbaum (2023) Nils Lukas and Florian Kerschbaum. PTW: pivotal tuning watermarking for pre-trained image generators. In Joseph A. Calandrino and Carmela Troncoso (eds.), _32nd USENIX Security Symposium, USENIX Security 2023, Anaheim, CA, USA, August 9-11, 2023_, pp. 2241-2258. USENIX Association, 2023. URL https://www.usenix.org/conference/usenixsecurity23/presentation/lukas.
* Naseh et al. (2023) Ali Naseh, Jaechul Roh, and Amir Houmansadr. Understanding (un)intended memorization in text-to-image generative models, 2023. URL https://arxiv.org/abs/2312.07550.
* Pham et al. (2024) Minh Pham, Kelly O. Marshall, Chinmay Hegde, and Niv Cohen. Robust concept erasure using task vectors, 2024. URL https://doi.org/10.48550/arXiv.2404.03631.
* Pizzi et al. (2022) Ed Pizzi, Sreya Dutta Roy, Suggosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. A self-supervised descriptor for image copy detection. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 14512-14522. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01413. URL https://doi.org/10.1109/CVPR52688.2022.01413.
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 8748-8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/radford21a.html.
* Raghu et al. (2021) Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pp. 12116-12128, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/652cf38361a209088302ba2b8b7f51e0-Abstract.html.
* Ren et al. (2024) Jie Ren, Yaxin Li, Shenglai Zeng, Han Xu, Lingjuan Lyu, Yue Xing, and Jiliang Tang. Unveiling and mitigating memorization in text-to-image diffusion models through cross attention, 2024. URL https://arxiv.org/abs/2403.11052.
* Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 10674-10685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042. URL https://doi.org/10.1109/CVPR52688.2022.01042.
* Saveri and Butterick (2023) Joseph Saveri and Matthew Butterick, 2023. URL https://imagegeneratorlitigation.com/.

* Schuhmann et al. (2021) Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million image-text pairs. _CoRR_, abs/2111.02114, 2021. URL https://arxiv.org/abs/2111.02114.
* December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/al859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html.
* Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis R. Bach and David M. Blei (eds.), _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pp. 2256-2265. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/sohl-dickstein15.html.
* Sompealli et al. (2023) Gowthami Sompealli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art of digital forgery? investigating data replication in diffusion models. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023_, pp. 6048-6058. IEEE, 2023a. doi: 10.1109/CVPR52729.2023.00586. URL https://doi.org/10.1109/CVPR52729.2023.00586.
* 16, 2023_, 2023b. URL http://papers.nips.cc/paper_files/paper/2023/hash/9521b6e7f33e039e7d9e2e3f5e37bbf4-Abstract-Conference.html.
* Wang et al. (2024) Weiyun Wang, Shuibo Zhang, Yiming Ren, Yuchen Duan, Tiantong Li, Shuo Liu, Mengkang Hu, Zhe Chen, Kaipeng Zhang, Lewei Lu, Xizhou Zhu, Ping Luo, Yu Qiao, Jifeng Dai, Wenqi Shao, and Wenhai Wang. Needle in a multimodal haystack, 2024. URL https://arxiv.org/abs/2406.07230.
* Wen et al. (2023) Yuxin Wen, Arpit Bansal, Hamid Kazemi, Eitan Borgnia, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Canary in a coalmine: Better membership inference with ensembled adversarial queries. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/forum?id=D7SBTEBFnC.
* Wen et al. (2024) Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, explaining, and mitigating memorization in diffusion models. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=84n3UwkH7b.
* Wolfgang and Delp (1996) Raymond B. Wolfgang and Edward J. Delp. A watermark for digital images. In _Proceedings 1996 International Conference on Image Processing, Lausanne, Switzerland, September 16-19, 1996_, pp. 219-222. IEEE Computer Society, 1996. doi: 10.1109/ICIP.1996.560423. URL https://doi.org/10.1109/ICIP.1996.560423.
* Xiong et al. (2020) Cheng Xiong, Chuan Qin, Guorui Feng, and Xinpeng Zhang. Flexible and secure watermarking for latent diffusion model. In Abdulmotaleb El-Saddik, Tao Mei, Rita Cucchiara, Marco Bertini, Diana Patricia Tobon Vallejo, Pradeep K. Atrey, and M. Shamim Hossain (eds.), _Proceedings of the 31st ACM International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29

[MISSING_PAGE_FAIL:13]

a single training image \(\bm{x}\) with conditional embedding \(\bm{c}\), we prompt the model to outpaint \(\bm{x}\)'s border as in Lugmayr et al. (2022)2, using \(\bm{x}\)'s corresponding caption to condition the model. We evaluate a generation \(\hat{\bm{x}}\) against the true key \(k_{\bm{x}}\) for \(\bm{x}\) by finding the absolute difference between the true key \(k_{\bm{x}}\) and the average of \(\hat{\bm{x}}\)'s border pixels (the model's "predicted key"). For a full model evaluation, we calculate the eidetic memorizations for every image in a subset of the training data, where the accuracy of the evaluation increases with the size of the subset.

Footnote 2: We describe a modified version of this algorithm that we use for LDMs in Appendix Section B.

Formal pseudocode to use SolidMark to evaluate a model \(\epsilon_{\theta}\) is in Algorithm 2. Our method has a few hyperparameters to consider: the keymap \(k(\bm{x})\), pattern thickness \(p\), the number \(n\) of training samples that are evaluated, and the number of times \(r\) each sample was evaluated (usually set to 1 unless \(n\) is small). We constructed our keymaps by assigning random floats to each image: \(k_{\bm{x}}\sim\text{Unif}(0,1)\); we draw a grayscale color (which is the same scalar across all channels) uniformly at random (assuming images are representing with floating points from 0-1). We always used a pattern thickness of \(16\). When evaluating on a model scale, we evaluated on \(n=10,000\) or \(n=5,000\) samples in all cases. When we did not have so many samples, we compensated by increasing \(r\) to stabilize the results.

## Appendix D Border-Center Ablation

We compared the memorization evaluation performance of borders of thickness \(16\) against \(16\times 16\) center patterns. To do this, we combined STL-10's labelled train and validation sets to form a training set of \(13,000\) images. We augmented the training set with the respective patterns and pretrained class-conditioned DDPMs on these training sets for 500 epochs with 250 sampling steps and a batch size of 64. Afterwards, we evaluated random \(10,000\) image subsets from both models and report the results in Table 5.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Metric** & \(\bm{\delta}=0.1\) & \(\bm{\delta}=0.05\) & \(\bm{\delta}=0.005\) \\ \hline \(\left(\ell_{\text{CM}},\bm{\delta}\right)\)-Eidetic Memorizations & 1927 & 1011 & 107 \\ \(\left(\ell_{\text{SM}},\bm{\delta}\right)\)-Eidetic Memorizations & 1879 & 977 & 81 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Reported Memorizations by SolidMark and Center Patterns.** Solid patterns in the center of the image (denoted here as \(\ell_{\text{CM}}\)) are slightly more thorough in detecting memorization than solid borders, which is evidenced by the higher number of reported memorizations out of 10,000 images compared to \(\ell_{\text{SM}}\). Lower memorization numbers indicate better model behavior.

[MISSING_PAGE_EMPTY:15]

whenever an image was classified as a \((\ell_{\text{SM}},\bm{\delta})\)-eidetic memorization of any of its respective keys in the training dataset.

## Appendix G Augmentation Ablation Implementation Details

For increasing crop levels, we altered the scale at which random cropping operated. For crop level 1, we randomly cropped the image to relative size (0.8, 0.8) and resized it to its original size. For crop level 2, we cropped to size (0.6, 0.6). We used (0.4, 0.4) for crop level 3 and (0.2, 0.2) for crop level 4. For different blur levels, we used Gaussian blurring with a kernel size of (5, 5) for blur level 1, (9, 9) for blur level 2, (13, 13) for blur level 3, and (17, 17) for blur level 4. Examples of these augmentations are in Figure 5.

## Appendix H Evaluated Inference-Time Mitigation Methods

We evaluated Gaussian Noise at Inference (GNI), Random Token Replacement and Addition (RT), Caption Word Repetition (CWR), and Random Numbers Addition (RNA). GNI adds a small amount of random noise to text embeddings, usually from a distribution of \(\mathcal{N}(0,0.1)\). To tune this method, we increased the magnitude of the perturbations in order to reduce memorization further at the cost of adherence to the conditional prompt. RT randomly replaces tokens in the caption with random words and adds random words to the caption. On each iteration, RT has a chance to randomly replace each individual token in the prompt; this method was tuned by changing the number of iterations. CWR randomly chooses a word from the given prompt and inserts it into one additional random spot in the prompt. RNA, at each iteration, randomly adds random numbers in the range \(\{0,10^{6}\}\) to the prompt, hoping to perturb the prompt without changing its semantic meaning. Similar to RT, CWR and RNA were tuned by changing the number of iterations.

## Appendix I GNI Evaluation in DDPMs

We augmented STL-10 (Coates et al., 2011) with a \(16\)-thick border and pretrained DDPMs on this augmented dataset. Next, we added Gaussian noise to the conditional embeddings with mean \(0\) and a range of magnitudes, tracking the number of \((\ell_{\text{SM}},\bm{\delta})\)-eidetic memorizations over \(5,000\) generations

Figure 5: **Augmentations Applied to Query Images. We show examples of the augmentations used to validate SolidMarr’s fine-grainedness. Implementation details in Appendix Section G.**

as the magnitude of noise increased. These results are in Figure 6. Overall, we found that for both values of \(\bm{\delta}\), the number of \((\ell_{\text{SM}},\bm{\delta})\)-eidetic memorizations remained relatively constant.

## Appendix J Pretraining a Foundation Model

To foster the usage of SolidMark, we pretrain and release a foundation model injected with our borders. We trained a fresh initialization of SD 2.1 on a 200k subset of LAION-5B for 500k steps. Since the model was trained with a batch size of \(8\), it saw every sample in this subset every 25,000 steps. Altogether, the model saw each sample 20 times. Some sample generations from this model are in Figure 7.

The model took 4 days and 10 hours to train on 4 H100 GPUs. We only pretrained the UNet, which was a fresh initialization of SD 2.1's UNet. We trained this model with the HuggingFace StableDiffusionPipeline. All samples were taken using 250 sampling steps.

## Appendix K How to Choose a Metric

Given the outlined qualities of a stable metric, we suggest that eidetic scoring always be used, regardless of the choice of distance function. A few values of \(\bm{\delta}\) should be chosen and tracked simultaneously, especially if a memorization reduction technique is being applied (to give an idea of how fine-grained the changes in memorization are).

Figure 6: **Reported Memorizations in DDPM Pretrained on STL-10 with GNL.** We report the change in the number of memorizations while applying progressively increasing magnitudes of GNI to the model. Overall, we find no significant change in the number of memorizations at any magnitude of noise, supporting our argument that visual cues are more closely tied to pixel-level memorization than conditional embeddings.

Figure 7: **Samples from Pretrained Text-to-Image Model. (Top) Prompts used to generate images from our pretrained model. (Bottom) The resultant images for the respective prompt.**

If reconstructive memorization is to be tracked, one should first consider the training dataset. If a large dataset of natural images such as LAION-5B (Schuhmann et al., 2022) was used to train the model, then, following the literature, SSCD similarity is likely the most consistent and precise metric. If a smaller or more niche dataset is being used, then the optimal choice of distance function is a comparatively unexplored question. Intuitively, two suggestions could be to pretrain a self-supervised model on the dataset or to finetune SSCD on the dataset. Whichever approach is chosen, it is important to evaluate samples by hand and ensure that the metric is sufficiently specific and sensitive.

For pixel-level memorization, one might feel inclined to only use an \(\ell_{2}\)-based metric because of their simplicity. This approach is problematic, though, as \(\ell_{2}\)-based metrics are not robust against small shifts in pixel space and tend to report false positives with their biases towards monochromatic images. We instead recommend that SolidMark be used in this situation. If the dataset in question contains lots of exact duplication, then an \(\ell_{2}\)-based metric should be used in tandem with SolidMark while manually validating the \(\ell_{2}\) metric's reported memorizations. This way, no memorizations will be missed, but the evaluation will remain fine-grained.