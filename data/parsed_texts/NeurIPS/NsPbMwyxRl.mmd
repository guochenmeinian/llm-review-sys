# On the Complexity of Differentially Private Best-Arm Identification with Fixed Confidence

 Achraf Azize

Equipe Scool, Univ. Lille, Inria,

CNRS, Centrale Lille, UMR 9189- CRIStAL

F-59000 Lille, France

achraf.azize@inria.fr &Marc Jourdan

Equipe Scool, Univ. Lille, Inria,

CNRS, Centrale Lille, UMR 9189- CRIStAL

F-59000 Lille, France

marc.jourdan@inria.fr &Aymen Al Marjani

UMPA, ENS Lyon

Lyon, France

aymen.al_marjani@ens-lyon.fr &Debabrota Basu

Equipe Scool, Univ. Lille, Inria,

CNRS, Centrale Lille, UMR 9189- CRIStAL

F-59000 Lille, France

debabrota.basu@inria.fr

###### Abstract

Best Arm Identification (BAI) problems are progressively used for data-sensitive applications, such as designing adaptive clinical trials, tuning hyper-parameters, and conducting user studies to name a few. Motivated by the data privacy concerns invoked by these applications, we study the problem of _BAI with fixed confidence under \(\epsilon\)-global Differential Privacy (DP)_. First, to quantify the cost of privacy, we derive a _lower bound on the sample complexity_ of any \(\delta\)-correct BAI algorithm satisfying \(\epsilon\)-global DP. Our lower bound suggests the existence of two privacy regimes depending on the privacy budget \(\epsilon\). In the high-privacy regime (small \(\epsilon\)), the hardness depends on a coupled effect of privacy and a novel information-theoretic quantity, called the _Total Variation Characteristic Time_. In the low-privacy regime (large \(\epsilon\)), the sample complexity lower bound reduces to the classical non-private lower bound. Second, we propose AdaP-TT, an \(\epsilon\)-global DP variant of the Top Two algorithm. AdaP-TT runs in _arm-dependent adaptive episodes_ and adds _Laplace noise_ to ensure a good privacy-utility trade-off. We derive an asymptotic upper bound on the sample complexity of AdaP-TT that matches with the lower bound up to multiplicative constants in the high-privacy regime. Finally, we provide an experimental analysis of AdaP-TT that validates our theoretical results.

## 1 Introduction

We study the stochastic multi-armed _bandit_ problem [10], which allows us to reflect on fundamental information-utility trade-offs involved in interactive sequential learning. Specifically, in a bandit problem, a learning _agent_ is exposed to interact with \(K\) unknown probability distributions \(\{\nu_{1},\ldots,\nu_{K}\}\) with bounded expectations, referred as the _reward distributions_ (or _arms_). \(\bm{\nu}\triangleq\{\nu_{1},\ldots,\nu_{K}\}\) is called _a bandit instance_. At every step \(t>0\), the agent chooses to interact with one of the reward distributions \(\nu_{A_{t}}\) for an \(A_{t}\in[K]\), and obtains a sample (or _reward_) \(r_{t}\) from it. The goal of the agent can be of two types: (a) maximise the reward accumulated over time, or equivalently to minimise the regret, and (b) to find the reward distribution (or arm) with highest expected reward. The first problem is called the regret-minimisation problem [1], while the second one is called the _Best Arm

[MISSING_PAGE_EMPTY:2]

1. _What is the fundamental hardness of ensuring Differential Privacy in the Best-Arm Identification with fixed confidence problem (\(\epsilon\)-DP-FC-BAI) in terms of a lower bound on the sample complexity?_
2. _How to design an efficient \(\epsilon\)-DP-FC-BAI algorithm that achieves the lower bound order-optimally?_

**Our contributions.** These questions have led to the following contributions:

1. _Hardness as lower bounds:_ We commence our study by deriving the lower bound on sample complexity (or expected stopping time) of any FC-BAI algorithm ensuring \(\epsilon\)-global DP, in brief \(\epsilon\)**-DP-FC-BAI** (Section 3). In Theorem 2, we prove that the sample complexity of \(\epsilon\)-DP-FC-BAI depends on the minimum of two information-theoretic quantities one that depends on privacy, and the other that originates from the classical sample complexity of non-private FC-BAI [13]. The term dependent on privacy depends on the privacy budget \(\epsilon\), and a novel information-theoretic quantity, referred to as the _Total Variation Characteristic Time_\(T_{\mathrm{TV}}^{*}\). \(T_{\mathrm{TV}}^{*}\) depends on the Total Variation (TV) distance between the reward distributions in the bandit problem and corresponding most confusing instance. The lower bound also indicates that, in a similar spirit as the regret minimisation with \(\epsilon\)-global DP [1], there are two regimes of hardness for \(\epsilon\)-DP-FC-BAI. For lower level of privacy (i.e. higher \(\epsilon\)), the sample complexity of \(\epsilon\)-DP-FC-BAI is identical to that of the non-private FC-BAI. But for higher level of privacy (i.e. lower \(\epsilon\)), the sample complexity depends on \(\epsilon\) and \(T_{\mathrm{TV}}^{*}\).
2. _Algorithm design:_ Following the lower bounds, we aim to design an efficient \(\epsilon\)-DP-FC-BAI algorithm that simultaneously achieves the lower bound order-optimally and is computationally efficient (Section 4). Due to the superior empirical performance and computational efficiency of Top Two algorithms, we design an \(\epsilon\)-DP variant of Top Two algorithms, named AdaP-TT. Specifically, we show two simple design techniques, i.e. adaptive episodes for each arm and Laplacian mechanism, if properly used, lead to AdaP-TT from the non-private TTUCB [12]. We further derive an asymptotic (as \(\delta\to 0\)) upper bound on the sample complexity of AdaP-TT (Theorem 5). In the high-privacy regime, the sample complexity upper bound of AdaP-TT coincides with the lower bound up to multiplicative constants. Thus, it is an order-optimal \(\epsilon\)-DP-FC-BAI algorithm in this regime, whereas the looseness in the low-privacy regime is as same as the looseness of the non-private Top Two algorithm. In Section 5, we experimentally show that AdaP-TT is more sample efficient than the existing \(\epsilon\)-DP-FC-BAI algorithm, i.e. DP-SE [14]. We also show that its sample complexity is independent of the privacy budget in the low-privacy regime, as already indicated by the lower bound.
3. _Technical tools:_ (a) To derive the lower bound, we provide an \(\epsilon\)-global DP version of the transportation lemma [15] (Lemma 1), which we prove using a sequential coupling argument. We also define and study the TV characteristic time (\(T_{\mathrm{TV}}^{*}\)) quantifying the hardness of FC-BAI in high privacy regimes (Proposition 1). (b) To design the algorithm, we propose a _generic wrapper_, _which adapts the existing FC-BAI algorithm to tackle DP-FC-BAI_. It builds on two components: (i) Adaptive episodes with per-arm doubling and forgetting, (ii) A private GLR stopping rule obtained by plugging in private empirical means in the non-private GLR stopping rule used of FC-BAI with Gaussian distributions. To use this proposed wrapper, one can choose among the numerous existing sampling rules to tackle FC-BAI. In this work, we consider the Top Two algorithms since they have good theoretical guarantees and empirical performance. To provide the sample complexity upper bound, we study step-by-step the effects of doubling, forgetting, and adding noise on the performance of the algorithm. Building on [12], we provide a generic analysis of the class of Top Two algorithms when combined with our wrapper.

### Related works

**Lower bound.** Efficient algorithm design in BAI literature is propelled by the derivation of lower bounds on sample complexity. [13] derive the first lower bounds for classical fixed-confidence BAI setting without privacy, which is further improved in [13] by introducing KL characteristic time \(T_{\mathrm{KL}}^{*}\) (Corollary 1). Motivated by this, _we prove the first-known lower bound on sample complexity for FC-BAI with \(\epsilon\)-global DP_ (Theorem 2). The proof employs a similar sequential coupling argument as in the regret lower bound for bandits with \(\epsilon\)-global DP [14, 1]. This similarity is also reflected in the existence of two privacy regimes depending on the privacy budget \(\epsilon\). And for both lower bounds, the Total Variation (TV) appears to be the information-theoretic measure that captures the hardness in the high privacy regime. Specifically, the TV characteristic time (\(T_{\mathrm{TV}}^{*}\), Corollary 1) serves as the BAI counterpart to the TV-distinguishability gap (\(t_{\mathrm{inf}}\)) in the problem-dependent regret lower bound for bandits with \(\epsilon\)-global DP as in [1, Theorem 3].

**Algorithms for BAI with fixed confidence (FC-BAI).** The optimal sample complexity for the non-private FC-BAI problem (i.e., \(T_{\text{KL}}^{*}\)) is well-understood [16], and algorithms are proposed with the aim to achieve this lower bound. Early approaches involved Successive Elimination (SE) based algorithms [1] with uniform sampling to find the optimal arm. Inspired by the success of Upper Confidence Bound (UCB) algorithms in the regret setting, the Lower Upper Confidence Bound (LUCB) algorithm was proposed [1]. However, neither SE nor LUCB algorithms achieve asymptotic optimality. The Track-and-Stop (TnS) algorithm introduced in [16] was the first to asymptotically achieve the exact optimal sample complexity \(T_{\text{KL}}^{*}\). TnS attains asymptotic optimality by solving a plug-in estimate of the lower bound optimization problem at each step. The game-based approach presented in [1] relaxes this requirement by casting the optimization problem as an unknown game and proposing sampling rules based on iterative strategies to estimate and converge to its saddle point. Finally, the Top Two algorithms arose as an identification strategy based on the praised Thompson Sampling algorithm for regret minimization [14]. In recent years, numerous variants have been analyzed and shown to be asymptotically near optimal [15]. At every step, a Top Two sampling rule selects the next arm to sample from among two candidate arms, a leader and a challenger. In addition to their great empirical performance, and easy implementation compared to TnS and Game-based algorithms, the Top Two algorithms achieve near asymptotic optimality. In Sec. 4, _we derive an \(\epsilon\)-global DP version of a Top Two algorithm:_ AdaP-TT_.

\(\epsilon\)**-DP BAI algorithms.** DP-SE [17] is an \(\epsilon\)-global DP version of the Successive Elimination algorithm. Although the algorithm was proposed and analysed for the regret minimisation setting in [17], it is possible to derive a sample complexity from the analysis in [17]. We compare in-depth DP-SE and AdaP-TT, both theoretically (Section 4) and experimentally (Section 5). _In both aspects, our proposed algorithm_ AdaP-TT _outperforms DP-SE._ Another adaptation of DP-SE, namely DP-SEQ, is proposed in [18] for the problem of privately finding the arm with the highest quantile at a fixed level. But this is a different setting of interest than the present paper. [19] also studies privacy for BAI under fixed confidence but with multiple agents. They propose and analyse the sample complexity of DP-MASE, a multi-agent version of DP-SE. They show that multi-agent collaboration leads to better sample complexity than independent agents, even under privacy constraints. While the multi-agent setting with federated learning allows tackling large-scale clinical trials taking place at several locations simultaneously, we study the single-agent setting, which is relevant for many small-scale clinical trials (see Example 1).

## 2 Differential privacy and best arm identification

**Background: Differential Privacy (DP).** DP ensures protection of an individual's sensitive information when her data is used for analysis. A randomised algorithm satisfies DP if the output of the algorithm stays almost the same, regardless of whether any single individual's data is included in or excluded from the input. This is achieved by adding controlled noise to the algorithm's output.

**Definition 1** (\((\epsilon,\delta)\)-Dp [1]).: _A randomised algorithm \(\mathcal{A}\) satisfies \((\epsilon,\delta)\)-Differential Privacy (DP) if for any two neighbouring datasets \(\mathcal{D}\) and \(\mathcal{D}^{\prime}\) that differ only in one entry, i.e. \(d_{\text{Ham}}(\mathcal{D},\mathcal{D}^{\prime})=1\), and for all sets of output \(\mathcal{O}\subseteq\operatorname{Range}(\mathcal{A})\),_

\[\Pr[\mathcal{A}(\mathcal{D})\in\mathcal{O}]\leq e^{\epsilon}\Pr \left[\mathcal{A}\left(\mathcal{D}^{\prime}\right)\in\mathcal{O}\right]+\delta,\] (1)

_where the probability space is over the coin flips of the mechanism \(\mathcal{A}\), and for some \((\epsilon,\delta)\in\mathbb{R}^{\geq 0}\times\mathbb{R}^{\geq 0}\). If \(\delta=0\), we say that \(\mathcal{A}\) satisfies \(\epsilon\)-DP. A lower privacy budget \(\epsilon\) implies higher privacy._

The Laplace mechanism [1, 1] ensures \(\epsilon\)-DP by injecting controlled random noise into the output of the algorithm, which is sampled from a calibrated Laplace distribution (as specified in Theorem 1). We use \(Lap(b)\) to denote the Laplace distribution with mean 0 and variance \(2b^{2}\).

**Theorem 1** (\(\epsilon\)-DP of Laplace mechanism (Theorem 3.6, [1])).: _Let us consider an algorithm \(f:\mathcal{X}\rightarrow\mathbb{R}^{d}\) with sensitivity \(s(f)\triangleq\max\limits_{\mathcal{D},\mathcal{D}^{\prime}\,s_{I}|\,\mathcal{ D}-\mathcal{D}^{\prime}|_{\text{Hamming}}=1}\lvert f(\mathcal{D})-f(\mathcal{D}^{ \prime})\rvert 1\). Here, \(\left\lVert\cdot\right\rVert_{1}\) is the \(L_{1}\) norm on \(\mathbb{R}^{d}\). If \(d\) noise samples \(\{N_{i}\}_{i=1}^{d}\) are generated independently from \(Lap\left(\frac{s(f)}{\epsilon}\right)\), then the output injected with the noise, i.e. \(f(\mathcal{D})+[N_{1},\ldots,N_{d}]\), satisfies \(\epsilon\)-DP._

**Background: BAI with fixed confidence.** Now, we describe the canonical best-arm identification problem with fixed confidence (_FC-BAI_). BAI is a variant of pure exploration, where the goal is toidentify the optimal arm. In FC-BAI, the learner is provided with a confidence level \(1-\delta\in(0,1)\)1. Learner aims to recommend an arm that is optimal with probability at least \(1-\delta\), while using as few samples as possible. To achieve this, the learner defines a FC-BAI strategy to interact with the bandit instance \(\boldsymbol{\nu}=\{\nu_{a}:a\in[K]\}\). We denote the action played at step \(t\) by \(a_{t}\), and the corresponding observed reward by \(r_{t}\sim\nu_{a_{t}}\). \(\mathcal{H}_{t}=(a_{1},r_{1},\ldots,a_{t},r_{t})\) is the history of actions played and rewards collected until time \(t\). We augment the action set by a _stopping action_\(\top\), and write \(a_{t}=\top\) to denote that the algorithm has stopped before step \(t\). A FC-BAI strategy \(\pi\) is composed of

Footnote 1: We remind not to confuse risk level \(\delta\) with the \(\delta\) of \((\epsilon,\delta)\)-DP. Hereafter, we consider \(\epsilon\)-global DP as the privacy definition, and \(\delta\) always represents the risk (or probability of mistake) of the BAI strategy.

**i. A pair of sampling and stopping rules \(\left(\mathrm{S}_{t}:\mathcal{H}_{t-1}\rightarrow\mathcal{P}([|1,K|]|\cup \{\top\})\right)_{t\geq 1}\)**. For an action \(a\in[K]\), \(\mathrm{S}_{t}\left(a\mid\mathcal{H}_{t-1}\right)\) denotes the probability of playing action \(a\) given history \(\mathcal{H}_{t-1}\). On the other hand, \(\mathrm{S}_{t}\left(\top\mid\mathcal{H}_{t-1}\right)\) is the probability of the algorithm halting given \(\mathcal{H}_{t-1}\). For any history \(\mathcal{H}_{t-1}\), a consistent sampling and stopping rule \(\mathrm{S}_{t}\) satisfies \(\mathrm{S}_{t}\left(\top\mid\mathcal{H}_{t-1}\right)=1\) if \(\top\) has been played before \(t\).

**ii. A recommendation rule \(\left(\mathrm{Rec}_{t}:\mathcal{H}_{t-1}\rightarrow\mathcal{P}([|1,K|]) \right)_{t>1}\)**. A recommendation rule dictates \(\mathrm{Rec}_{t}\left(a\mid\mathcal{H}_{t-1}\right)\), i.e. the probability of returning action \(a\) as a guess for the best action given \(\mathcal{H}_{t-1}\).

We denote by \(\tau\) the **stopping time** of the algorithm, i.e. the first step \(t\) demonstrating \(a_{t}=\top\). A BAI strategy \(\pi\) is called \(\delta\)**-correct** for a class of bandit instances \(\mathcal{M}\), if for every instance \(\boldsymbol{\nu}\in\mathcal{M}\), \(\pi\) recommends the optimal action \(a^{\star}(\boldsymbol{\nu})=\arg\max_{a\in[K]}\mu_{a}\) with probability at least \(1-\delta\), i.e. \(\mathbb{P}_{\boldsymbol{\nu},\pi}(\tau<\infty,\widehat{a}=a^{\star}( \boldsymbol{\nu}))\geq 1-\delta\).

**FC-BAI with \(\epsilon\)-global DP (\(\epsilon\)-DP-FC-BAI).** Now, we formally define \(\epsilon\)-global DP for FC-BAI, where the BAI strategy (a.k.a. the centralised decision maker) is trusted with all the intermediate rewards. We represent each user \(u_{t}\) by the vector \(\textbf{x}_{t}\triangleq(x_{t,1},\ldots,x_{t,K})\in\mathbb{R}^{K}\), where \(x_{t,a}\) represents the **potential** reward observed, if action \(a\) was recommended to user \(u_{t}\). Due to the bandit feedback, only \(r_{t}=x_{t,a_{t}}\sim\nu_{a_{t}}\) is observed at step \(t\). We use an underline to denote any sequence. Thus, we denote the sequence of sampled actions until \(T\) as \(\underline{a}^{T}=(a_{1},\ldots,a_{T})\). We further represent a set of users \(\{u_{t}\}_{t=1}^{T}\) until \(T\) by **the table of potential rewards \(\textbf{d}^{T}\triangleq\{\textbf{x}_{1},\ldots,\textbf{x}_{T}\}\in(\mathbb{R }^{K})^{T}\)**. First, we observe that \(\textbf{d}^{T}\) is the sensitive input generated through interaction with the users, and \((\underline{a}^{T},\widehat{a},T)\) is the output of the BAI strategy. Hence, we define the probability that the BAI strategy \(\pi\) samples the action sequence \(\underline{a}^{T}\), recommends the action \(\widehat{a}\), and halts at time \(T\), as

\[\pi(\underline{a}^{T},\widehat{a},T\mid\textbf{d}^{T})\triangleq\mathrm{Rec}_{ T+1}\left(\widehat{a}\mid\mathcal{H}_{T}\right)\mathrm{S}_{T+1}\left(\top \mid\mathcal{H}_{T}\right)\prod_{t=1}^{T}\ \mathrm{S}_{t}\left(a_{t}\mid\mathcal{H}_{t-1}\right)\] (2)

where \(T\) users under interaction are represented by the table of potential rewards \(\textbf{d}^{T}\)

Thus, a BAI strategy satisfies \(\epsilon\)-global DP if the probability defined in Eq. (2) is similar when the BAI strategy interacts with two neighbouring tables of rewards differing by a user (i.e. a row in \(\textbf{d}^{T}\)).

**Definition 2** (\(\epsilon\)-global DP for BAI).: _A BAI strategy satisfies \(\epsilon\)**-global DP**, if for all \(T\geq 1\), all neighbouring table of rewards \(\textbf{d}^{T}\) and \(\textbf{d}^{T}\), i.e. \(d_{\text{Hm}}(\textbf{d}^{T},\textbf{d}^{T})=1\), all sequences of sampled actions \(\underline{a}^{T}\in[K]^{T}\) and recommended actions \(\widehat{a}\in[K]\) we have that_

\[\pi(\underline{a}^{T},\widehat{a},T\mid\textbf{d}^{T})\leq e^{\epsilon}\pi( \underline{a}^{T},\widehat{a},T\mid\textbf{d}^{T}).\]Definition 2 can be seen as a BAI counterpart of the \(\epsilon\)-global DP definition proposed in [1] for regret minimization. We demonstrate the BAI strategy-Users interaction in Algorithm 1.

**Remark 1**.: _It is possible to consider that the output of a BAI strategy is only the final recommended action \(\hat{a}\), i.e. not publishing the intermediate actions \(\underline{a}^{T}\). This gives a weaker definition of privacy compared to Definition 2, since the latter defends against adversaries that may look inside the execution of the BAI strategy, i.e. pan-privacy [1]. In addition, Definition 2 is needed in practice. For example, in the case of dose-finding (Example 1), the experimental protocol, i.e. the intermediate actions, needs to be published too._

**The goal** in \(\epsilon\)-DP-FC-BAI is to design a \(\delta\)-correct \(\epsilon\)-global DP algorithm, with \(\tau\) as small as possible.

## 3 Lower bound on sample complexity for FC-BAI with \(\epsilon\)-global DP

The central question that we address in this section is

_How many additional samples a BAI strategy must select for ensuring \(\epsilon\)-global DP?_

In response, we prove a lower bound on the sample complexity of any \(\delta\)-correct \(\epsilon\)-DP BAI strategy. Our lower bound features problem-dependent characteristic times reminiscent of the FC-BAI setting.

Let \(\bm{\nu}\triangleq\{\nu_{a}:a\in[K]\}\) be a bandit instance, consisting of \(K\) arms with finite means \(\{\mu_{a}\}_{a\in[K]}\). Now, we define the set of alternative instances as \(\mathrm{Alt}(\bm{\nu})\triangleq\{\bm{\lambda}:a^{\star}(\bm{\lambda})\neq a ^{\star}(\bm{\nu})\}\), i.e. the bandit instances with a different optimal arm than \(\bm{\nu}\). For two probability distributions \(\mathbb{P},\mathbb{Q}\) on the same measurable space \((\Omega,\mathcal{F})\), the Total Variation (TV) distance is defined as \(\mathrm{TV}\left(\mathbb{P}\ \middle\|\ \mathbb{Q}\right)\triangleq\sup_{A\in\mathcal{F}}\{ \mathbb{P}(A)-\mathbb{Q}(A)\}\), while the KL divergence (or relative entropy) is \(\mathrm{KL}\left(\mathbb{P}\ \middle\|\ \mathbb{Q}\right)\triangleq\int\log\left(\frac{ \mathrm{d}\mathbb{P}}{\mathrm{d}\mathbb{Q}}(\omega)\right)\ \mathrm{d}\mathbb{P}(\omega)\), when \(\mathbb{P}\ll\mathbb{Q}\), and \(\infty\) otherwise. We denote the probability simplex by \(\Sigma_{K}\triangleq\{\omega\in[0,1]^{K}:\sum_{a=1}^{K}\omega_{a}=1\}\).

First, we derive an \(\epsilon\)-global DP variant of the 'transportation' lemma, i.e. Lemma 1 in [1].

**Lemma 1** (Transportation lemma under \(\epsilon\)-global DP).: _Let \(\delta\in(0,1)\) and \(\epsilon>0\). Let \(\bm{\nu}\) be a bandit instance and \(\lambda\in\mathrm{Alt}(\bm{\nu})\). For any \(\delta\)-correct \(\epsilon\)-global DP BAI strategy, we have that_

\[6\epsilon\sum_{a=1}^{K}\mathbb{E}_{\bm{\nu},\pi}\left[N_{a}(\tau)\right] \mathrm{TV}\left(\nu_{a}\parallel\lambda_{a}\right)\geq\mathrm{kl}(1-\delta, \delta),\]

_where \(\mathrm{kl}(1-\delta,\delta)\triangleq x\log\frac{x}{y}+(1-x)\log\frac{1-x}{1- y}\) for \(x,y\in(0,1)\)._

_Proof sketch._ We use Sequential Karwa-Vadhan Lemma [1, Lemma 2] with a data-processing inequality in the BAI canonical model. Extra care is needed _to deal with the stopping times in the coupling, compared to a fixed horizon \(T\) in regret minimization_. The proof is deferred to Appendix B.

Leveraging Lemma 1, we derive a sample complexity lower bound for any \(\epsilon\)-DP-FC-BAI strategy.

**Theorem 2** (Sample complexity lower bound for \(\epsilon\)-DP-FC-BAI).: _Let \(\delta\in(0,1)\) and \(\epsilon>0\). For any \(\delta\)-correct \(\epsilon\)-global DP BAI strategy, we have that_

\[\mathbb{E}_{\bm{\nu}}[\tau]\geq T^{\star}\left(\bm{\nu};\epsilon\right)\log( 1/3\delta),\] (3)

_where \((T^{\star}\left(\bm{\nu};\epsilon\right))^{-1}\triangleq\sup_{\omega\in \Sigma_{K}}\inf_{\bm{\lambda}\in\mathrm{Alt}(\bm{\nu})}\min\left(\sum_{a=1}^{ K}\omega_{a}\mathrm{KL}\left(\nu_{a}\parallel\lambda_{a}\right),6\epsilon\sum_{a=1}^{ K}\omega_{a}\mathrm{TV}\left(\nu_{a}\parallel\lambda_{a}\right)\right)\)._

**Comments on the lower bound.** Similar to the lower bound for the non-private BAI [1], the lower bound of Theorem 2 is the value of a two-player zero-sum game between a MIN player and MAX player. MIN plays an alternative instance \(\lambda\) close to \(\nu\) in order to confuse MAX. The latter plays an allocation \(\omega\in\Sigma_{K}\) to explore the different arms, with the purpose of maximising the divergence between \(\nu\) and the confusing instance \(\lambda\) that MIN played. On top of the KL divergence present in the non-private lower bound, our bound features the TV distance that appears naturally when incorporating the \(\epsilon\)-global DP constraint. The proof is deferred to Appendix B. In order to compare the lower bound of an \(\epsilon\)-global BAI strategy with the non-private lower bound of [1], we relax Theorem 2 to further derive a simpler bound, as in Corollary 1.

**Corollary 1**.: _For any \(\delta\)-correct \(\epsilon\)-global DP BAI strategy, we have that_

\[\mathbb{E}_{\bm{\nu}}[\tau]\geq\max\left(T^{\star}_{\mathrm{KL}}(\bm{\nu}), \frac{1}{6\epsilon}T^{\star}_{\mathrm{TV}}(\bm{\nu})\right)\log(1/3\delta),\]

_where \((T^{\star}_{\bm{d}}(\bm{\nu}))^{-1}\triangleq\sup_{\omega\in\Sigma_{K}}\inf_{ \bm{\lambda}\in\mathrm{Alt}(\bm{\nu})}\sum_{a=1}^{K}\omega_{a}\bm{d}(\nu_{a}, \lambda_{a})\), and \(\bm{d}\) is either \(\mathrm{KL}\) or \(\mathrm{TV}\)._Proof.: The proof is direct by observing that \(T^{\star}(\bm{\nu};\epsilon)\geq T^{\star}_{\text{KL}}(\bm{\nu})\) and \(T^{\star}(\bm{\nu};\epsilon)\geq\frac{1}{6\epsilon}T^{\star}_{\text{TV}}(\bm{ \nu})\). 

**Comparison with the non-private lower bound.**\(T^{\star}_{\text{KL}}\) is the characteristic time in the non-private lower bound [1], and we refer to Section 2.2 of [1] for a detailed discussion on its properties. The sample complexity lower bound suggests the existence of _two hardness regimes depending on \(\epsilon\)_, \(T^{\star}_{\text{KL}}\) _and_\(T^{\star}_{\text{TV}}\). (1) _Low-privacy regime_: When \(\epsilon>T^{\star}_{\text{TV}(\bm{\nu})}/(6T^{\star}_{\text{KL}}(\bm{\nu}))\), the lower bound retrieves the non-private lower bound, i.e. \(T^{\star}_{\text{KL}}(\bm{\nu})\), and thus, **privacy can be achieved for free**. (2) _High-privacy regime:_ When \(\epsilon<T^{\star}_{\text{TV}}(\bm{\nu})/(6T^{\star}_{\text{KL}}(\bm{\nu}))\), the lower bound becomes \(T^{\star}_{\text{TV}}/(6\epsilon)\) and \(\epsilon\)-global DP \(\delta\)-BAI requires more samples than non-private ones.

In the following proposition, we characterise \(T^{\star}_{\text{TV}}\) for Bernoulli instances.

**Proposition 1** (TV characteristic time for Bernoulli instances).: _Let \(\nu\) be a bandit instance, i.e. such that \(\nu_{a}=\text{Bernoulli}(\mu_{a})\) and \(\mu_{1}>\mu_{2}\geq\cdots\geq\mu_{K}\). Let \(\Delta_{a}\triangleq\mu_{1}-\mu_{a}\) and \(\Delta_{\text{min}}\triangleq\min_{a\neq 1}\Delta_{a}\). We have that_

\[T^{\star}_{\text{TV}}(\bm{\nu})=\frac{1}{\Delta_{\text{min}}}+\sum_{a=2}^{K} \frac{1}{\Delta_{a}},\qquad\qquad\text{and}\qquad\qquad\frac{1}{\Delta_{ \text{min}}}\leq T^{\star}_{\text{TV}}(\bm{\nu})\leq\frac{K}{\Delta_{\text{ min}}}.\]

Proof sketch.: The proof is direct by solving the optimisation problem defining \(T^{\star}_{\text{TV}}\) and using that \(\text{TV}\left(\text{Bernoulli}(p)\parallel\text{Bernoulli}(q)\right)=|p-q|\). We refer to Appendix B for details.

_Comment._ The aforementioned bound on TV characteristic time for Bernoulli instances is \(\epsilon\)-global DP parallel of the KL-characteristic time bound \(T^{\star}_{\text{KL}}(\bm{\nu})\leq\sum_{a=1}^{K}\Delta_{a}^{-2}\)[1]. Using Pinsker's inequality, one can connect the TV and KL characteristic times by \(T^{\star}_{\text{TV}}(\bm{\nu})\geq\sqrt{2T^{\star}_{\text{KL}}(\bm{\nu})}\).

## 4 Algorithm design: Private Top Two with adaptive episodes (AdaP-TT)

In this section, we propose AdaP-TT, an \(\epsilon\)-global DP version of the TTUCB algorithm [1]. We show that AdaP-TT satisfies \(\epsilon\)-global DP, is \(\delta\)-correct, and has an asymptotic sample complexity that matches the high privacy lower bounds up to multiplicative constants.

**TTUCB** belongs to the family of Top Two algorithms [12, 13], JDB+22], which selects at each time two arms called leader and challenger, and sample among them. After initialisation, TTUCB uses a UCB-based leader and a Transportation Cost (TC) challenger, expressed by

\[B_{n}=\operatorname*{arg\,max}_{a\in[K]}\{\hat{\mu}_{n,a}+\sqrt{6\log(n)/N_{n,a}}\},\quad\text{and}\quad C_{n}=\operatorname*{arg\,min}_{a\neq B_{n}} \frac{\hat{\mu}_{n,B_{n}}-\hat{\mu}_{n,a}}{\sqrt{1/N_{n,B_{n}}+1/N_{n,a}}}\;.\]

Here, \((\hat{\mu}_{n},N_{n})\) are the empirical means and counts on the whole history. The theoretical motivation behind the TC challenger comes from the theoretical lower bound in FC-BAI, which involves the KL-characteristic time \(T^{\star}_{\text{KL}}(\bm{\mu})=\min_{\beta\in(0,1)}T^{\star}_{\text{KL},\beta }(\bm{\mu})\). For Gaussian distributions, we have

\[2T^{\star}_{\text{KL},\beta}(\bm{\mu})^{-1}=\max_{\omega\in\Sigma_{K},\omega _{a}*=\beta}\frac{(\mu_{a^{\star}}-\mu_{a})^{2}}{1/\beta+1/\omega_{a}}\quad \text{and}\quad T^{\star}_{\text{KL},1/2}(\bm{\mu})\leq 2T^{\star}_{\text{KL}}(\bm{ \mu})\;,\]

The maximiser of the above equation is denoted by \(\omega^{\star}_{\text{KL},\beta}(\bm{\mu})\), and is further referred to as the \(\beta\)-optimal allocation as it is unique. Let \(N^{a}_{n,b}\) denote the number of times arm \(b\) was pulled when \(a\) was the leader, and \(L_{n,a}\) denotes the number of times arm \(a\) was the leader. In order to select the next arm to sample \(I_{n}\), TTUCB relies on \(K\) tracking procedures, i.e. set \(I_{n}=B_{n}\) if \(N^{B_{n}}_{n,B_{n}}\leq\beta L_{n+1,B_{n}}\), else \(I_{n}=C_{n}\). This ensures that \(\max_{a\in[K],n>K}|N^{a}_{n,a}-\beta L_{n,a}|\leq 1\). Standing on this premise, we now describe how we design an \(\epsilon\)-global DP extension of TTUCB.

**Private algorithm design.** As illustrated in Algorithm 2, AdaP-TT relies on three ingredients: _adaptive episodes with doubling_, _forgetting_, and _adding calibrated Laplacian noise_. (1) AdaP-TT maintains \(K\) episodes, i.e. one per arm. The private empirical estimate of the mean of an arm is only updated at the end of an episode, that means when the number of times that a particular arm was played doubles (Line 5). (2) For each arm \(a\), AdaP-TT forgets rewards from previous phases of arm \(a\), i.e. the private empirical estimate of arm \(a\) is only computed using the rewards collected in the last phase of arm \(a\) (Line 8). This assures that the means of each arm are estimated using a non-overlapping sequence of rewards. (3) Thanks to this _doubling_ and _forgetting_, AdaP-TT is \(\epsilon\)-global DP as soon as each empirical mean (Line 9) is made \(\epsilon\)-DP, and thus, avoiding any use of privacy composition. This is achieved by adding Laplace noise. We formalise this intuition in Lemma 2 of Appendix C.

**Remark 2**.: _The aforementioned generic wrapper can be used to construct a near-optimal differentially private version of any existing FC-BAI algorithm that deploys a sampling rule with the empirical means of rewards. In this work, we consider and rigorously analyse the Top Two algorithms since they demonstrate both good theoretical guarantees and empirical performance._

**Theorem 3** (Privacy analysis).: _For rewards in \([0,1]\), AdaP-TT satisfies \(\epsilon\)-global DP._

Proof sketch.: A change in one user _only affects_ the empirical mean calculated at one episode of an arm, which is made private using the Laplace Mechanism and Lemma 2. Since the sampled actions, recommended action, and stopping time are computed only using the private empirical means, AdaP-TT satisfies \(\epsilon\)-global DP thanks to post-processing lemma. We refer to Appendix C for details.

**Private GLR stopping rule.** We consider the private GLR stopping rule based on the private means and on the pulling counts from the last phase (Line 12), and recommend the arm with the highest private mean (Line 11). Lemma 4 yields a threshold function ensuring that any sampling rule is \(\delta\)-correct, when using the private GLR stopping rule.

**Theorem 4** (\(\delta\)-correctness).: _Let \(\delta\in(0,1)\), \(\epsilon>0\). Let \(s>1\) and \(\zeta\) be the Riemann \(\zeta\) function. Let \(c_{k}(n,m,\delta)=2\mathcal{C}_{G}(\log((K-1)\zeta(s)k^{s}/\delta)/2)+2\log(4 +\log n)+2\log(4+\log m)\) be the threshold without privacy. Given any sampling rule, the following threshold_

\[c_{\epsilon,k_{1},k_{2}}(n,m,\delta)=2c_{k_{1}k_{2}}(n,m,\delta/2)+\frac{1}{n \epsilon^{2}}\log\left(\frac{2Kk_{1}^{s}\zeta(s)}{\delta}\right)^{2}+\frac{1} {m\epsilon^{2}}\log\left(\frac{2Kk_{2}^{s}\zeta(s)}{\delta}\right)^{2}\] (4)

_with the GLR stopping rule yields a \(\delta\)-correct algorithm for sub-Gaussian distributions. The function \(\mathcal{C}_{G}\) is defined in (15). It satisfies \(\mathcal{C}_{G}(x)\approx x+\ln(x)\)._

_Remark_.: We observe that approximately \(c_{\epsilon,k_{1},k_{2}}(n,m,\delta)\approx 2\log(1/\delta)+(1/n+1/m)\log(1/ \delta)^{2}/\epsilon^{2}\).

Proof sketch.: Proving \(\delta\)-correctness of a GLR stopping rule is done by leveraging concentration results. Specifically, we start by decomposing the failure probability \(\mathbb{P}_{\mu}\left(\tau_{\delta}<+\infty,\hat{a}\neq a^{*}\right)\) into a non-private and a private part using the basic property of \(\mathbb{P}(X+Y\geq a+b)\leq\mathbb{P}(X\geq a)+\mathbb{P}(Y\geq b)\). The two-factor in front of \(c_{k_{1}k_{2}}\) originates from the looseness of this decomposition. To remove it, we would need a tighter stopping threshold that jointly controls both the non-private and the private parts. We conclude using concentration results from sub-Gaussian random variables for the non-private part, and Laplace random variables for the private part.

**Theorem 5** (Asymptotic upper bound on expected sample complexity).: _Let \((\delta,\beta)\in(0,1)^{2}\) and \(\epsilon>0\). Combined with the private GLR stopping rule using threshold as in (4), AdaP-TT is \(\delta\)-correct and satisfies that, for all \(\mu\in\mathbb{R}^{K}\) such that \(\min_{a\neq b}|\mu_{a}-\mu_{b}|>0\),_

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\mu}[\tau_{\delta}]}{\log(1/\delta)} \leq 4T_{\mathrm{KL},\beta}^{*}(\bm{\mu})\left(1+\sqrt{1+\frac{\Delta_{ \max}^{2}}{2\epsilon^{2}}}\right)\;.\]

Proof sketch.: We adapt the asymptotic proof of the TTUCB algorithm, which is based on the unified analysis of Top Two algorithms from [1]. Below, we present high-level ideas of the proof and specify the effect of different elements of AdaP-TT on the expected sample complexity.

_Consequences of Theorem 5_.: (1) The **non-private TTUCB algorithm**[1] achieves a sample complexity of \(T_{\mathrm{KL},\beta}^{*}(\bm{\mu})\) for sub-Gaussian random variables. The proof relies on showing that the empirical pulling counts are converging towards the \(\beta\)-optimal allocation \(\omega_{\mathrm{KL},\beta}^{*}(\bm{\mu})\). (2) The **effect of doubling and forgetting** is a multiplicative four-factor, i.e. \(4T_{\mathrm{KL},\beta}^{*}(\bm{\mu})\). The first two-factor is due to forgetting since we throw away half of the samples. The second two-factor is due to doubling since we have to wait for the end of an episode to evaluate the stopping condition. (3) The **Laplace noise** only affects the empirical estimate of the mean. Since the Laplace noise has no bias and a sub-exponential tail, the private means will still converge towards their true values. Therefore, the empirical counts of AdaP-TT will also converge to \(\omega_{\mathrm{KL},\beta}^{*}(\bm{\mu})\) asymptotically. (4) While the **Laplace noise has little effect on the sampling rule** itself, it **changes drastically the dependency in \(\log(1/\delta)\) of the threshold** used in the GLR stopping rule. The private threshold \(c_{\epsilon,k_{1},k_{2}}\) has an extra factor \(\mathcal{O}(\log^{2}(1/\delta))\) compared to the non-private one \(c_{k}\). Using the convergence towards \(\omega_{\mathrm{KL},\beta}^{*}(\bm{\mu})\), the stopping condition is met as soon as \(\frac{n}{T_{\mathrm{KL},\beta}^{*}(\bm{\mu})}\lesssim 2\log(1/\delta)+\frac{ \Delta_{\max}^{2}}{2\epsilon^{2}}\frac{T_{\mathrm{KL},\beta}^{*}(\bm{\mu})}{n} \log^{2}(1/\delta)\). Solving the inequality for \(n\) concludes the proof while adding a multiplicative four-factor.

_Discussion._ For \(\beta=1/2\), it is well known that \(T^{\star}_{\mathrm{KL},1/2}(\bm{\mu})\leq 2T^{\star}_{\mathrm{KL}}(\bm{\mu})\leq 8 \sum_{a\neq a^{\star}}\Delta_{a}^{-2}\). We consider Bernoulli instances (\(0<\Delta_{\min}\leq\Delta_{\max}<1\)), where the gaps have the same order of magnitude, i.e. _Condition 1_: there exists a constant \(C\geq 1\) such that \(\Delta_{\max}/\Delta_{\min}\leq C\). For such instances, there exists a universal constant \(c\), such that

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\mu}}[\tau_{\delta}]}{\log(1/\delta)} \leq\ c\ \max\left\{T^{\star}_{\mathrm{KL},1/2}(\bm{\mu}),C\epsilon^{-1} \sum\nolimits_{a\neq a},\Delta_{a}^{-1}\right\}\,.\]

Without privacy, i.e. \(\epsilon\to+\infty\), \(\mathsf{AdapP}\mathsf{-TT}\) yields a multiplicative eight-factor. On top of the four-factor due to doubling and forgetting, another multiplicative two comes from \(2c_{k_{1}k_{2}}\) in Equation (4).

**Comparison to the lower bound.** For Bernoulli bandits verifying _Condition 1_, the upper bound of Theorem 5 matches the \(T^{\star}_{\mathrm{TV}}(\bm{\mu})/\epsilon\) lower bound of Corollary 1 up to constants in the high-privacy regime, i.e. when \(\epsilon\preceq T^{\star}_{\mathrm{TV}}(\bm{\mu})/T^{\star}_{\mathrm{KL}}(\bm{ \mu})\). In the low-privacy regime, the upper bound reduces to \(T^{\star}_{\mathrm{KL},1/2}(\bm{\mu})\). In Appendix E.7, we discuss in-depth why this difference is necessary for private BAI algorithms based on the GLR stopping rule, which poses an interesting open problem.

**Comparison to DP-SE.** DP-SE is a private version of the successive-elimination algorithm studied in [11] for the regret minimisation setting. The algorithm samples active arms uniformly during phases of geometrically increasing length. Based on the private confidence bounds, DP-SE eliminates provably sub-optimal arms at the end of each phase. Due to its phased-elimination structure, DP-SE can be easily converted into an \(\epsilon\)-DP-FC-BAI algorithm, where we stop once there is only one active arm left. In particular, the proof of Theorem 4.3 of [11] shows that with high probability any sub-optimal arm \(a\neq a^{\star}\) is sampled no more than \(\mathcal{O}(\Delta_{a}^{2}+(\epsilon\Delta_{a})^{-1})\). From this result, it is straightforward to extract a sample complexity upper bound for DP-SE, i.e. \(\mathcal{O}(\sum_{a\neq a^{\star}}\Delta_{a}^{-2}+\sum_{a\neq a^{\star}}( \epsilon\Delta_{a})^{-1})\). This shows that DP-SE too achieves (ignoring constants) the high-privacy lower bound \(T^{\star}_{\mathrm{TV}}(\bm{\mu})/\epsilon\) for Bernoulli instances. However, due to its uniform sampling within the phases, DP-SE is less adaptive than \(\mathsf{AdapP}\mathsf{-TT}\). Inside a phase, DP-SE continues to sample arms that might already be known to be bad, while \(\mathsf{AdapP}\mathsf{-TT}\) adapts its sampling rule based on the transportation costs that reflect the amount of evidence collected in favour of the hypothesis that the leader is the best arm. Finally, \(\mathsf{AdapP}\mathsf{-TT}\) has the advantage of being anytime, i.e. its sampling strategy does not depend on the risk \(\delta\).

## 5 Experimental analysis

We perform experiments to show that: (i) AdaP-TT has better empirical performance compared to DP-SE, and (ii) the transition between high and low-privacy regimes is reflected empirically.

**Experimental setup.** We compare the performances of AdaP-UCB and DP-SE for FC-BAI in different Bernoulli instances as in [10]. The first instance has means \(\mu_{1}=(0.95,0.9,0.9,0.9,0.5)\) and the second instance has means \(\mu_{2}=(0.75,0.7,0.7,0.7)\). As a benchmark, we also compare to the non-private TTUCB. We set the risk \(\delta=10^{-2}\). We implement all the algorithms in Python (version \(3.8\)) and on an 8-core 64-bits Intel i5@1.6 GHz CPU. We run each algorithm \(100\) times, and plot corresponding average and standard deviations of stopping times in Figure 1. We also test the algorithms on other Bernoulli instances and report the results in Appendix F.

**Result analysis.**_a. Efficiency in performance._ AdaP-TT requires less samples than DP-SE to provide a \(\delta\)-correct answer. In the high privacy regime, i.e. small \(\epsilon\), AdaP-TT outperforms DP-SE in all the instances tested. In the low privacy regimes, i.e. large \(\epsilon\), both algorithms have similar performance that in the worst case is four times the samples required of TTUCB, as shown theoretically.

_b. Impact of privacy regimes._ As indicated by the theoretical sample complexity lower bounds and upper bounds, the experimental performance of AdaP-TT demonstrates two regimes: a high-privacy regime (for \(\epsilon<0.2\)), where the stopping time of AdaP-TT depends on the privacy budget \(\epsilon\), and a low privacy regime (for \(\epsilon>0.2\)), where the performance of AdaP-TT does not depend on \(\epsilon\).

## 6 Conclusion and future works

We study FC-BAI with \(\epsilon\)-global DP. We derive a sample complexity lower bound that quantifies the additional samples needed by a \(\delta\)-correct BAI strategy in order to ensure \(\epsilon\)-global DP. The lower bound further suggests the existence of two privacy regimes. In the _low-privacy regime_, no additional samples are needed, and _privacy can be achieved for free_. For the _high-privacy regime_, the lower bound reduces to \(\Omega(\epsilon^{-1}T_{\text{TV}}^{*})\), and _more samples are required_. We also propose AdaP-TT, an \(\epsilon\)-global DP variant of the Top Two algorithms, that runs in adaptive phases and adds Laplace noise. AdaP-TT achieves the high privacy regime lower bound up to multiplicative constants.

The upper bound matches the lower bound by a multiplicative constant in the high privacy regime, and is also loose in some instances in the low privacy regime, due to the mismatch between the KL divergence of Bernoulli distributions and that of Gaussian. It would be an interesting technical challenge to merge this gap. One possible direction to solve this issue is to use transportation costs tailored to Bernoulli for both the Top Two Sampling and the stopping. Another interesting direction would be to extend the proposed technique to other variants of pure DP, namely \((\epsilon,\delta)\)-DP and Renyi-DP [14], or other trust models, namely local DP [12] and shuffle DP [1, 2].

Figure 1: Evolution of the stopping time \(\tau\) (mean \(\pm\) std. over 100 runs) of AdaP-TT, DP-SE, and TTUCB with respect to the privacy budget \(\epsilon\) for \(\delta=10^{-2}\) on Bernoulli instance \(\mu_{1}\) (left) and \(\mu_{2}\) (right). The shaded vertical line separates the two privacy regimes. AdaP-TT outperforms DP-SE.

## Acknowledgments and Disclosure of Funding

This work has been partially supported by the THIA ANR program "AI_PhD@Lille". A. Al-Marjani acknowledges the support of the Chiare SeqALO (ANR-20-CHIA-0020). D. Basu acknowledges the Inria-Kyoto University Associate Team "RELIANT" for supporting the project, and the ANR JCJC for the REPUBLIC project (ANR-22-CE23-0003-01). We thank Emilie Kaufmann and Aurelien Garivier for the interesting conversations. We also thank Philippe Preux for his support.

## References

* [AB22] Achraf Azize and Debabrota Basu. When privacy meets partial information: A refined analysis of differentially private bandits. _Advances in Neural Information Processing Systems_, 35:32199-32210, 2022.
* [ACBF02] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. _Machine learning_, 47(2-3):235-256, 2002.
* [AKR21] Maryam Aziz, Emilie Kaufmann, and Marie-Karelle Riviere. On multi-armed bandit designs for dose-finding clinical trials. _The Journal of Machine Learning Research_, 22(1):686-723, 2021.
* [ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le cam. In _Algorithmic Learning Theory_, pages 48-78. PMLR, 2021.
* [AYBG\({}^{+}\)18] Yasin Abbasi-Yadkori, Peter Bartlett, Victor Gabillon, Alan Malek, and Michal Valko. Best of both worlds: Stochastic & adversarial best-arm identification. In _Conference on Learning Theory_, pages 918-949. PMLR, 2018.
* [BDT19] Debabrota Basu, Christos Dimitrakakis, and Aristide Tossou. Differential privacy for multi-armed bandits: What is it and what is its cost? _arXiv preprint arXiv:1905.12298_, 2019.
* [Bec54] Robert E Bechhofer. A single-sample multiple decision procedure for ranking means of normal populations with known variances. _The Annals of Mathematical Statistics_, pages 16-39, 1954.
* [Bec58] Robert E Bechhofer. A sequential multiple-decision procedure for selecting the best one of several normal populations with a common unknown variance, and its use with various experimental designs. _Biometrics_, 14(3):408-429, 1958.
* [Che21] Albert Cheu. Differential privacy in the shuffle model: A survey of separations. _arXiv preprint arXiv:2107.11839_, 2021.
* [CL16] Alexandra Carpentier and Andrea Locatelli. Tight (lower) bounds for the fixed budget best arm identification bandit problem. In _Conference on Learning Theory_, pages 590-604. PMLR, 2016.
* [CLK\({}^{+}\)14] Shouyuan Chen, Tian Lin, Irwin King, Michael R Lyu, and Wei Chen. Combinatorial pure exploration of multi-armed bandits. _Advances in neural information processing systems_, 27, 2014.
* [CWZ21] T Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. _The Annals of Statistics_, 49(5):2825-2850, 2021.
* [DJW13] John C Duchi, Michael I Jordan, and Martin J Wainwright. Local privacy and statistical minimax rates. In _2013 IEEE 54th Annual Symposium on Foundations of Computer Science_, pages 429-438. IEEE, 2013.
* [DKM19] Remy Degenne, Wouter M Koolen, and Pierre Menard. Non-asymptotic pure exploration by solving games. _Advances in Neural Information Processing Systems_, 32, 2019.

* [DNP\({}^{+}\)10] Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N Rothblum, and Sergey Yekhanin. Pan-private streaming algorithms. In _ICS_, pages 66-80, 2010.
* [DNPR10] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In _Proceedings of the forty-second ACM symposium on Theory of computing_, pages 715-724. ACM, 2010.
* [DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. _Foundations and Trends(r) in Theoretical Computer Science_, 9(3-4):211-407, 2014.
* [DRS22] Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(1):3-37, 2022.
* [EDMMM06] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _Journal of machine learning research_, 7(6), 2006.
* [GDD\({}^{+}\)21] Antonious M Girgis, Deepesh Data, Suhas Diggavi, Ananda Theertha Suresh, and Peter Kairouz. On the renyi differential privacy of the shuffle model. In _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, pages 2321-2341, 2021.
* [GGL12] Victor Gabillon, Mohammad Ghavamzadeh, and Alessandro Lazaric. Best arm identification: A unified approach to fixed budget and fixed confidence. _Advances in Neural Information Processing Systems_, 25, 2012.
* [GK16] Aurelien Garivier and Emilie Kaufmann. Optimal best arm identification with fixed confidence. In _Conference on Learning Theory_, pages 998-1027. PMLR, 2016.
* [JD22] Marc Jourdan and Remy Degenne. Non-asymptotic analysis of a ucb-based top two algorithm. _arXiv preprint arXiv:2210.05431_, 2022.
* [JDB\({}^{+}\)22] Marc Jourdan, Remy Degenne, Dorian Baudry, Rianne de Heide, and Emilie Kaufmann. Top two algorithms revisited. _Advances in Neural Information Processing Systems_, 35:26791-26803, 2022.
* [JDK23] Marc Jourdan, Remy Degenne, and Emilie Kaufmann. Dealing with unknown variances in best-arm identification. _International Conference on Algorithmic Learning Theory_, 2023.
* [JMNB14] Kevin Jamieson, Matthew Malloy, Robert Nowak, and Sebastien Bubeck. lil'ucb: An optimal exploration algorithm for multi-armed bandits. In _Conference on Learning Theory_, pages 423-439. PMLR, 2014.
* [JN14] Kevin Jamieson and Robert Nowak. Best-arm identification algorithms for multi-armed bandits in the fixed confidence setting. In _2014 48th Annual Conference on Information Sciences and Systems (CISS)_, pages 1-6. IEEE, 2014.
* [JT16] Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In _Artificial intelligence and statistics_, pages 240-248. PMLR, 2016.
* [KCG16] Emilie Kaufmann, Olivier Cappe, and Aurelien Garivier. On the complexity of best arm identification in multi-armed bandit models. _Journal of Machine Learning Research_, 17:1-42, 2016.
* [KK13] Emilie Kaufmann and Shivaram Kalyanakrishnan. Information complexity in bandit subset selection. In _Conference on Learning Theory_, pages 228-251. PMLR, 2013.
* [KK21] Emilie Kaufmann and Wouter M Koolen. Mixture martingales revisited with applications to sequential tests and confidence intervals. _Journal of Machine Learning Research_, 22(246):1-44, 2021.

* [KNSS21] Dionysios S Kalogerias, Konstantinos E Nikolakakis, Anand D Sarwate, and Or Sheffet. Quantile multi-armed bandits: Optimal best-arm identification and a differentially private scheme. _IEEE Journal on Selected Areas in Information Theory_, 2(2):534-548, 2021.
* [KV17] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals, 2017.
* [LEHT22] David E Losada, David Elsweiler, Morgan Harvey, and Christoph Trattner. A day at the races: using best arm identification algorithms to reduce the cost of information retrieval user studies. _Applied Intelligence_, 52(5):5617-5632, 2022.
* [LJD\({}^{+}\)17] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: A novel bandit-based approach to hyperparameter optimization. _The Journal of Machine Learning Research_, 18(1):6765-6816, 2017.
* [LPJ22] Simon Lindstahl, Alexandre Proutiere, and Andreas Johnsson. Measurement-based admission control in sliced networks: A best arm identification approach. In _GLOBECOM 2022-2022 IEEE Global Communications Conference_, pages 1484-1490. IEEE, 2022.
* [LS20] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [LVR\({}^{+}\)19] Pieter JK Libin, Timothy Verstraeten, Diederik M Roijers, Jelena Grujic, Kristof Theys, Philippe Lemey, and Ann Nowe. Bayesian best-arm identification for selecting influenza mitigation strategies. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018_, 2019.
* [Mir17] Ilya Mironov. Renyi differential privacy. In _2017 IEEE 30th computer security foundations symposium (CSF)_, pages 263-275. IEEE, 2017.
* [MT04] Shie Mannor and John N Tsitsiklis. The sample complexity of exploration in the multi-armed bandit problem. _Journal of Machine Learning Research_, 5(Jun):623-648, 2004.
* [MT15] Nikita Mishra and Abhradeep Thakurta. (Nearly) optimal differentially private stochastic multi-arm bandits. In _UAI_, 2015.
* [NKS19] Konstantinos E Nikolakakis, Dionysios S Kalogerias, and Anand D Sarwate. Optimal rates for learning hidden tree structures. _arXiv preprint arXiv:1909.09596_, 2019.
* [NR18] Seth Neel and Aaron Roth. Mitigating bias in adaptive data gathering via differential privacy. In _International Conference on Machine Learning_, pages 3720-3729. PMLR, 2018.
* [RBCS23] Alexandre Rio, Merwan Barlier, Igor Colin, and Marta Soare. Multi-agent best arm identification with private communications. In _Proceedings of the 40th International Conference on Machine Learning_, 2023.
* [Rus16] Daniel Russo. Simple bayesian algorithms for best arm identification. In _Conference on Learning Theory_, pages 1417-1418. PMLR, 2016.
* [SHM\({}^{+}\)20] Xuedong Shang, Rianne Heide, Pierre Menard, Emilie Kaufmann, and Michal Valko. Fixed-confidence guarantees for bayesian best-arm identification. In _International Conference on Artificial Intelligence and Statistics_, pages 1823-1832. PMLR, 2020.
* [SLM14] Marta Soare, Alessandro Lazaric, and Remi Munos. Best-arm identification in linear bandits. _Advances in Neural Information Processing Systems_, 27, 2014.
* [SS18] Roshan Shariff and Or Sheffet. Differentially private contextual linear bandits. In _Advances in Neural Information Processing Systems_, pages 4296-4306, 2018.

* [SS19] Touqir Sajed and Or Sheffet. An optimal private stochastic-mab algorithm based on optimal private stopping rule. In _International Conference on Machine Learning_, pages 5579-5588. PMLR, 2019.
* [TBD\({}^{+}\)16] Katherine Tucker, Janice Branson, Maria Dilleen, Sally Hollis, Paul Loughlin, Mark J Nixon, and Zoe Williams. Protecting patient privacy when sharing patient-level data from clinical trials. _BMC medical research methodology_, 16(1):5-14, 2016.
* [TD16] Aristide CY Tossou and Christos Dimitrakakis. Algorithms for differentially private multi-armed bandits. In _Thirtieth AAAI Conference on Artificial Intelligence_, 2016.
* [ZCL14] Yuan Zhou, Xi Chen, and Jian Li. Optimal pac multiple arm identification with applications to crowdsourcing. In _International Conference on Machine Learning_, pages 217-225. PMLR, 2014.

## Appendix

### Table of Contents

* 1 Outline
* 2 Lower bounds on sample complexity
	* 2.1 Canonical model for BAI
	* 2.2 Transportation lemma under \(\epsilon\)-global DP: Proof of Lemma 1
	* 2.3 Lower bound on sample complexity: Proof of Theorem 2
	* 2.4 TV characteristic time for Bernoulli instances: Proof of Proposition 1
	* 2.5 On the total variation distance and the hardness of privacy
* 3 Privacy analysis
	* 3.1 Privacy lemma for non-overlapping sequences
	* 3.2 Privacy analysis of AdaP-TT: Proof of Theorem 3
* 4 AdaP-TT: A private Top Two algorithm with adaptive episodes
	* 4.1 Generic wrapper on existing BAI algorithms
	* 4.2 AdaP-TT: Instantiating our wrapper with a Top Two sampling rule
* 5 Analysis of AdaP-TT: Proof of Theorem 5
	* 5.1 Technical results
	* 5.2 Sufficient exploration
	* 5.3 Convergence towards \(\beta\)-optimal allocation
	* 5.4 Cost of doubling and forgetting
	* 5.5 Asymptotic expected sample complexity
	* 5.6 Connection to the lower bound
	* 5.7 Limitation and open problem
	* 5.8 Analysis of non-private AdaP-TT: Proof of Theorem 6
* 6 Extended experimental analysisOutline

The appendices are organized as follows:

* The proof of our lower bound is detailed in Appendix B (Theorem 2).
* The proof of the privacy of AdaP-TT is done in Appendix C (Theorem 3).
* The AdaP-TT algorithm is presented in more details in Appendix D, and we show the \(\delta\)-correctness of the non-private GLR stopping rule (Theorem 4).
* The asymptotic upper bound on the expected sample complexity of AdaP-TT is proven in Appendix E (Theorem 5).
* Extended experiments are presented in Appendix F.

## Appendix B Lower bounds on sample complexity

In this section, we provide the proofs for the sample complexity lower bounds. First, we present the canonical model for BAI to introduce the relevant quantities. Then, we prove an \(\epsilon\)-global version of the transportation lemma, i.e. Lemma 1. Using this lemma, we prove Theorem 2. Finally, we prove the formula expressing the TV characteristic time for Bernoulli instances.

### Canonical model for BAI

Let \(\bm{\nu}\triangleq\{\nu_{a}:a\in[K]\}\) be a bandit instance, consisting of \(K\) arms with finite means \(\{\mu_{a}\}_{a\in[K]}\). Now, we recall the interaction between a BAI strategy \(\pi\) and the bandit instance \(\nu\) in the Protocol 1. The BAI strategy \(\pi\) halts at \(\tau\), samples a sequence of actions \(\underline{A}^{\tau}\), and recommends the action \(\hat{A}\). Let \(\mathbb{P}_{\bm{\nu},\pi}\) be the probability distribution over the triplets \((\tau,\underline{A}^{\tau},\hat{A})\), when the BAI strategy \(\pi\) interacts with the bandit instance \(\nu\).

For a fixed \(T>1\), a sequence of actions \(\underline{a}^{T}=(a_{1},\ldots,a_{T})\in[K]^{T}\) and a recommendation \(\hat{a}\in[K]\), we define the event \(E=\{\tau=T,\underline{A}^{\tau}=\underline{a}^{T},\hat{A}=\hat{a}\}\). We have that

\[\mathbb{P}_{\bm{\nu},\pi}(E)=\int_{\underline{x}^{T}=(r_{1},\ldots,r_{T})\in \mathbb{R}^{T}}\pi(\underline{a}^{T},\hat{a},T\mid\underline{x}^{T})\prod_{t=1 }^{T}\,\mathrm{d}\nu_{a_{t}}(r_{t})dr_{t}\]

where

\[\pi(\underline{a}^{T},\widehat{a},T\mid\underline{x}^{T})\triangleq\mathrm{ Rec}_{T+1}\left(\widehat{a}\mid\mathcal{H}_{T}\right)\mathrm{S}_{T+1} \left(\top\mid\mathcal{H}_{T}\right)\prod_{t=1}^{T}\,\mathrm{S}_{t}\left(a_{t }\mid\mathcal{H}_{t-1}\right)\]

and \(\mathcal{H}_{t}=(a_{1},r_{1},\ldots,a_{t},r_{t})\).

**Remark on the bandit feedback.** Let \(\pi\) be an \(\epsilon\)-DP BAI strategy. Let \(T\geq 1\), \(a^{T}\in[K]^{T}\) a sequence sampled actions and \(\widehat{a}\in[K]\) a recommended actions. This time, let \(\underline{r}^{T}=\{r_{1},\ldots,r_{T}\}\in\mathbb{R}^{T}\) and \(\underline{r^{\prime}}^{T}\in\mathbb{R}^{T}\) two neighbouring sequence of rewards, i.e. \(d_{\text{Ham}}(\underline{r}^{T},\underline{r^{\prime}}^{T})\triangleq\sum_{t =1}^{T}\mathds{1}\left\{r_{t}\neq r^{\prime}_{t}\right\}=1\). Consider the table of rewards \(\underline{d}^{T}\) consisting of concatenating \(\underline{x}^{T}\) colon-wise \(K\) times, i.e. \(\underline{d}^{T}_{t,i}=\underline{x}^{T}_{t}\) for all \(i\in[K]\) and all \(t\in[T]\). Define \(\underline{d^{\prime}}^{T}\) similarly with respect to \(\underline{r^{\prime}}^{T}\).

In this case, by definition of \(\pi\), \(\underline{d}^{T}\) and \(\underline{d^{\prime}}^{T}\), it is direct that

\[\pi(\underline{a}^{T},\widehat{a},T\mid\underline{x}^{T})=\pi(\underline{a}^ {T},\widehat{a},T\mid\underline{d}^{T})\]

and \(d_{\text{Ham}}(\underline{d}^{T},\underline{d^{\prime}}^{T})=1\).

Which means that

\[\pi(\underline{a}^{T},\widehat{a},T\mid\underline{x}^{T})\leq e^{\epsilon} \pi(\underline{a}^{T},\widehat{a},T\mid\underline{r^{\prime}}^{T}).\]

In other words, _if \(\pi\) is \(\epsilon\)-pure DP for neighbouring table of rewards \(\underline{d}^{T}\), then \(\pi\) is also \(\epsilon\)-pure DP for neighbouring sequence of observed rewards \(\underline{x}^{T}\)._

### Transportation lemma under \(\epsilon\)-global DP: Proof of Lemma 1

**Lemma 1** (Transportation lemma under \(\epsilon\)-global DP).: _Let \(\delta\in(0,1)\) and \(\epsilon>0\). Let \(\bm{\nu}\) be a bandit instance and \(\lambda\in\operatorname{Alt}(\bm{\nu})\). For any \(\delta\)-correct \(\epsilon\)-global DP BAI strategy, we have that_

\[6\epsilon\sum_{a=1}^{K}\mathbb{E}_{\bm{\nu},\pi}\left[N_{a}(\tau)\right] \operatorname{TV}\left(\nu_{a}\parallel\lambda_{a}\right)\geq\operatorname{ kl}(1-\delta,\delta),\]

_where \(\operatorname{kl}(x,y)\triangleq x\log\frac{x}{y}+(1-x)\log\frac{1-x}{1-y}\) for \(x,y\in(0,1)\)._

Proof.: **Step 1: Distinguishability due to \(\delta\)-correctness.** Let \(\pi\) be a \(\delta\)-correct \(\epsilon\)-global DP BAI strategy. Let \(\bm{\nu}\) be a bandit instance and \(\lambda\in\operatorname{Alt}(\bm{\nu})\).

Let \(\mathbb{P}_{\bm{\nu},\pi}\) denote the probability distribution of \((\underline{A},\widehat{A},\tau)\) when the BAI strategy \(\pi\) interacts with \(\nu\). For any alternative instance \(\lambda\in\operatorname{Alt}(\bm{\nu})\), the data-processing inequality gives that

\[\operatorname{KL}\left(\mathbb{P}_{\bm{\nu},\pi}\parallel\mathbb{ P}_{\bm{\lambda},\pi}\right) \geq\operatorname{kl}\left(\mathbb{P}_{\bm{\nu},\pi}\left( \widehat{A}=a^{\star}(\bm{\nu})\right),\mathbb{P}_{\bm{\lambda},\pi}\left( \widehat{A}=a^{\star}(\bm{\nu})\right)\right)\] \[\geq\operatorname{kl}(1-\delta,\delta).\] (5)

where the second inequality is because \(\pi\) is \(\delta\)-correct i.e. \(\mathbb{P}_{\bm{\nu},\pi}\left(\widehat{A}=a^{\star}(\bm{\nu})\right)\geq 1-\delta\) and \(\mathbb{P}_{\bm{\lambda},\pi}\left(\widehat{A}=a^{\star}(\bm{\nu})\right)\leq\delta\), and the monotonicity of the \(\operatorname{kl}\).

**Step 2: Connecting KL and TV under \(\epsilon\)-global DP.** On the other hand, by the definition of the KL, we have that

\[\operatorname{KL}\left(\mathbb{P}_{\bm{\nu},\pi}\parallel\mathbb{P}_{\bm{ \lambda},\pi}\right)=\mathbb{E}_{\tau,\underline{A}^{\tau},\hat{A}\sim\mathbb{ P}_{\bm{\nu},\pi}}\left[\log\left(\frac{\mathbb{P}_{\bm{\nu},\pi}(\tau, \underline{A}^{\tau},\hat{A})}{\mathbb{P}_{\bm{\lambda},\pi}(\tau,\underline{ A}^{\tau},\hat{A})}\right)\right]\]

where

\[\mathbb{P}_{\bm{\nu},\pi}(\tau=T,\underline{A}^{\tau}=\underline{a}^{T},\hat {A}=\hat{a})=\int_{\underline{\tau}\in\mathbb{R}^{T}}\pi(\underline{a}^{T}, \widehat{a},T\mid\underline{\tau})\prod_{t=1}^{T}\,\mathrm{d}\nu_{a_{t}}(r_{ t}).\]

Since \(\pi\) is \(\epsilon\)-global DP, using the sequential Karwa-Vadhan lemma [1, Lemma 2], we get that

\[\log\left(\frac{\mathbb{P}_{\bm{\nu},\pi}(\tau=T,\underline{A}^{ \tau}=\underline{a}^{T},\hat{A}=\hat{a})}{\mathbb{P}_{\bm{\lambda},\pi}(\tau= T,\underline{A}^{\tau}=\underline{a}^{T},\hat{A}=\hat{a})}\right) \leq 6\epsilon\sum_{t=1}^{T}\operatorname{TV}\left(\nu_{a_{t}} \parallel\lambda_{a_{t}}\right)\] \[=6\epsilon\sum_{a=1}^{K}N_{a}(T)\operatorname{TV}\left(\nu_{a} \parallel\lambda_{a}\right)\]

Which gives that

\[\operatorname{KL}\left(\mathbb{P}_{\bm{\nu},\pi}\parallel\mathbb{P}_{\bm{ \lambda},\pi}\right)\leq 6\epsilon\,\mathbb{E}_{\bm{\nu},\pi}\left[\sum_{a=1}^{K}N_{a}( \tau)\operatorname{TV}\left(\nu_{a}\parallel\lambda_{a}\right)\right].\] (6)

Combining Inequalities 5 and 6 concludes the proof. 

### Lower bound on sample complexity: Proof of Theorem 2

**Theorem 2** (Sample complexity lower bound for \(\epsilon\)-DP-FC-BAI).: _Let \(\delta\in(0,1)\) and \(\epsilon>0\). For any \(\delta\)-correct \(\epsilon\)-global DP BAI strategy, we have that_

\[\mathbb{E}_{\bm{\nu}}[\tau]\geq T^{\star}\left(\bm{\nu};\epsilon\right)\log(1/ 3\delta),\]

_where \(\left(T^{\star}\left(\bm{\nu};\epsilon\right)\right)^{-1}\triangleq\sup_{\omega \in\Sigma_{K}}\inf_{\bm{\lambda}\in\operatorname{Alt}(\bm{\nu})}\min\left( \sum_{a=1}^{K}\omega_{a}\operatorname{KL}\left(\nu_{a}\parallel\lambda_{a} \right),6\epsilon\sum_{a=1}^{K}\omega_{a}\operatorname{TV}\left(\nu_{a} \parallel\lambda_{a}\right)\right)\)._Proof.: Let \(\pi\) be a \(\delta\)-correct \(\epsilon\)-global DP BAI strategy. Let \(\bm{\nu}\) be a bandit instance and \(\lambda\in\operatorname{Alt}(\bm{\nu})\).

Let \(\mathbb{E}\) denote the expectation under \(\mathbb{P}_{\bm{\nu},\pi}\), ie \(\mathbb{E}\triangleq\mathbb{E}_{\bm{\nu},\pi}\).

By Lemma 1, we have that \(6\epsilon\sum_{a=1}^{K}\mathbb{E}\left[N_{a}(\tau)\right]\operatorname{TV} \left(\nu_{a}\parallel\lambda_{a}\right)\geq\operatorname{kl}(1-\delta,\delta)\).

Lemma 1 from [1] gives that \(\sum_{a=1}^{K}\mathbb{E}\left[N_{a}(\tau)\right]\operatorname{KL}\left(\nu_{a} \parallel\lambda_{a}\right)\geq\operatorname{kl}(1-\delta,\delta)\).

Since these two inequalities hold for all \(\bm{\lambda}\in\operatorname{Alt}(\bm{\nu})\), we get

\[\operatorname{kl}(1- \delta,\delta)\leq\inf_{\bm{\lambda}\in\operatorname{Alt}(\bm{ \nu})}\min\left(6\epsilon\sum_{a=1}^{K}\mathbb{E}\left[N_{a}(\tau)\right] \operatorname{TV}\left(\nu_{a}\parallel\lambda_{a}\right),\sum_{a=1}^{K} \mathbb{E}\left[N_{a}(\tau)\right]\operatorname{KL}\left(\nu_{a}\parallel \lambda_{a}\right)\right)\] \[\stackrel{{(a)}}{{=}}\mathbb{E}[\tau]\inf_{\bm{ \lambda}\in\operatorname{Alt}(\bm{\nu})}\min\left(6\epsilon\sum_{a=1}^{K} \frac{\mathbb{E}\left[N_{a}(\tau)\right]}{\mathbb{E}[\tau]}\operatorname{TV} \left(\nu_{a}\parallel\lambda_{a}\right),\sum_{a=1}^{K}\frac{\mathbb{E}\left[N_ {a}(\tau)\right]}{\mathbb{E}[\tau]}\operatorname{KL}\left(\nu_{a}\parallel \lambda_{a}\right)\right)\] \[\stackrel{{(b)}}{{\leq}}\mathbb{E}[\tau]\left(\sup_ {\omega\in\Sigma_{K}}\inf_{\bm{\lambda}\in\operatorname{Alt}(\bm{\nu})}\min \left(6\epsilon\sum_{a=1}^{K}\omega_{a}\operatorname{TV}\left(\nu_{a} \parallel\lambda_{a}\right),\sum_{a=1}^{K}\omega_{a}\operatorname{KL}\left(\nu _{a}\parallel\lambda_{a}\right)\right)\right)\,.\]

(a) is due to the fact that \(\mathbb{E}[\tau]\) does not depend on \(\bm{\lambda}\). (b) is obtained by noting that the vector \(\left(\omega_{a}\right)_{a\in[K]}\triangleq\left(\frac{\mathbb{E}_{\nu,\pi}[N _{a}(\tau)]}{\mathbb{E}_{\nu,\pi}[\tau]}\right)_{a\in[K]}\) belongs to the simplex \(\Sigma_{K}\).

The theorem follows by noting that for \(\delta\in(0,1),\operatorname{kl}(1-\delta,\delta)\geq\log(1/3\delta)\). 

### TV characteristic time for Bernoulli instances: Proof of Proposition 1

**Proposition 1** (\(\operatorname{TV}\) characteristic time for Bernoulli instances).: _Let \(\nu\) be a bandit instance, i.e. such that \(\nu_{a}=\text{Bernoulli}(\mu_{a})\) and \(\mu_{1}>\mu_{2}\geq\cdots\geq\mu_{K}\). Let \(\Delta_{a}\triangleq\mu_{1}-\mu_{a}\) and \(\Delta_{\text{min}}\triangleq\min_{a\neq 1}\Delta_{a}\). We have that_

\[T^{\star}_{\operatorname{TV}}(\bm{\nu})=\frac{1}{\Delta_{\text{min}}}+\sum_{a =2}^{K}\frac{1}{\Delta_{a}},\qquad\qquad\text{and}\qquad\qquad\frac{1}{ \Delta_{\text{min}}}\leq T^{\star}_{\operatorname{TV}}(\bm{\nu})\leq\frac{K} {\Delta_{\text{min}}}.\]

Proof.: **Step 1:** Let \(\nu\) be a bandit instance, i.e. such that \(\nu_{a}\triangleq\text{Bernoulli}(\mu_{a})\) and \(\mu_{1}>\mu_{2}\geq\cdots\geq\mu_{K}\).

For the alternative bandit instance \(\bm{\lambda}\), we refer to the mean of arm \(a\) as \(\rho_{a}\), i.e. \(\lambda_{a}\triangleq\text{Bernoulli}(\rho_{a})\).

By the definition of \(T^{\star}_{\operatorname{TV}}\), we have that

\[\left(T^{\star}_{\operatorname{TV}}(\bm{\nu})\right)^{-1} =\sup_{\omega\in\Sigma_{K}}\inf_{\bm{\lambda}\in\operatorname{Alt }(\bm{\nu})}\sum_{a=1}^{K}\omega_{a}\operatorname{TV}\left(\nu_{a}\parallel \lambda_{a}\right)\] \[\stackrel{{(a)}}{{=}}\sup_{\omega\in\Sigma_{K}}\min_ {a\neq 1}\inf_{\bm{\lambda}:\rho_{a}\succeq\rho_{1}}\omega_{1}\left|\mu_{1}-\rho_{1} \right|+\omega_{a}\left|\mu_{a}-\rho_{a}\right|\] \[\stackrel{{(b)}}{{=}}\sup_{\omega\in\Sigma_{K}}\min_ {a\neq 1}\min(\omega_{1},\omega_{a})\Delta_{a}\] \[\stackrel{{(c)}}{{=}}\sup_{\omega\in\Sigma_{K}} \omega_{1}\min_{a\neq 1}\min(1,\frac{\omega_{a}}{\omega_{1}})\Delta_{a}\] \[\stackrel{{(d)}}{{=}}\sup_{(x_{2},\ldots,x_{K})\in( \mathbb{R}^{+})^{K-1}}\frac{\min_{a\neq 1}g_{a}(x_{a})}{1+x_{2}+\cdots+x_{K}}\,,\]

where \(g_{a}(x_{a})\triangleq\min(1,x_{a})\Delta_{a}\).

Equality (a) is obtained due to the fact that \(\operatorname{Alt}(\bm{\nu})=\bigcup_{a\neq 1}\{\bm{\lambda}:\rho_{a}>\rho_{1}\}\), and for Bernoulli, \(\operatorname{TV}\left(\nu_{a}\parallel\lambda_{a}\right)=\left|\mu_{a}-\rho_{a}\right|\).

Equality (b) is true, since \(\inf_{\bm{\lambda}:\rho_{a}>\rho_{1}}\omega_{1}\left|\mu_{1}-\rho_{1}\right|+ \omega_{a}\left|\mu_{a}-\rho_{a}\right|=\min(\omega_{1},\omega_{a})\Delta_{a}\).

Equality (c) holds true, since \(\omega_{1}\neq 1\) (if \(\omega_{1}=0\), the value of the objective is 0).

Equality (d) is obtained by the change of variable \(x_{a}\triangleq\frac{\omega_{a}}{\omega_{1}}\)

**Step 2:** Let \((x_{2},\ldots,x_{K})\in(\mathbb{R}^{+})^{K-1}\). By the definition of \(g_{a}\), we have that

\[g_{a}(x_{a})\leq x_{a}\Delta_{a}\quad\text{and}\quad g_{a}(x_{a})\leq\Delta_{a}.\]

This leads to the inequalities

\[\min_{a\neq 1}g_{a}(x_{a})\leq g_{a}(x_{a})\leq x_{a}\Delta_{a}\quad\text{and} \quad\min_{a\neq 1}g_{a}(x_{a})\leq\Delta_{\text{min}}.\]

Thus,

\[\left(\min_{a\neq 1}g_{a}(x_{a})\right)\left(\frac{1}{\Delta_{ \text{min}}}+\sum_{a=2}^{K}\frac{1}{\Delta_{a}}\right) =\frac{\min_{a\neq 1}g_{a}(x_{a})}{\Delta_{\text{min}}}+\sum_{a=2}^{K }\frac{\min_{a\neq 1}g_{a}(x_{a})}{\Delta_{a}}\] \[\leq 1+\sum_{a=2}^{K}x_{a}\;.\]

This means that for every \((x_{2},\ldots,x_{K})\in(\mathbb{R}^{+})^{K-1}\),

\[\frac{\min_{a\neq 1}g_{a}(x_{a})}{1+x_{2}+\cdots+x_{K}}\leq\frac{1}{\frac{1}{ \Delta_{\text{min}}}+\sum_{a=2}^{K}\frac{1}{\Delta_{a}}}.\]

Here, the upper bound is achievable for \(x_{a}^{\star}=\frac{\Delta_{\text{min}}}{\Delta_{a}}\), since \(g_{a}(x_{a}^{\star})=\Delta_{\text{min}}\) for all \(a\neq 1\).

This concludes that

\[\left(T_{\text{TV}}^{\star}(\boldsymbol{\nu})\right)^{-1} =\frac{1}{\frac{1}{\Delta_{\text{min}}}+\sum_{a=2}^{K}\frac{1}{ \Delta_{a}}} \implies\left(T_{\text{TV}}^{\star}(\boldsymbol{\nu})\right)=\Delta_{ \text{min}}+\sum_{a=2}^{K}\frac{1}{\Delta_{a}}\;.\]

**Step 3:** The lower and upper bounds on \(\left(T_{\text{TV}}^{\star}(\boldsymbol{\nu})\right)\) follow from the fact that \(\frac{1}{\Delta_{a}}\geq 0\) for all \(a\), and \(\frac{1}{\Delta_{a}}\leq\frac{1}{\Delta_{\text{min}}}\) for all \(a\neq 1\).

Hence, we conclude the proof. 

### On the total variation distance and the hardness of privacy

Our lower bound suggests that the hardness of the DP-FC-BAI problem is characterized by \(T_{TV}^{\star}\), which is a total variation counterpart of the classic KL-based characteristic time \(T_{KL}^{\star}\) in FC-BAI [1]. The total variation distance appears to be the natural measure to quantify the hardness of privacy in other settings such as regret minimization [1], Karwa-Vadhan lemma [13] and Differentially Private Assouad, Fano, and Le Cam [1]. The high-level intuition is that: Pure DP can be seen as a multiplicative stability constraint of \(e^{\epsilon}\) when one data point changes. With group privacy, if two datasets differ in \(d_{ham}\) points, then one incurs a factor \(e^{d_{ham}\;\epsilon}\). Now, by sampling \(n\) i.i.d points from a distribution \(P\) and \(n\) i.i.d points from a distribution \(Q\), the Karwa-Vadhan lemma states that the incurred factor is \(e^{(nTV(P,Q))\;\epsilon}\). This is proved by building a maximal coupling, which is the coupling that minimizes the Hamming distance in expectation. In brief, _the total variation naturally appears in lower bounds since it is the quantity that characterises the hardness of the optimal transport problem minimizing the hamming distance_, i.e \(TV(P,Q)=\inf_{(X,Y)\sim(P,Q)}E(1_{X\neq Y})\). However, it is possible that the problem can be characterized by other f-divergences. Finally, one can always go from TV to KL using Pinsker's inequality, though that would always be less tight than the TV-based lower bound.

**On the relation between \(T_{TV}^{\star}\) and \(T_{KL}^{\star}\)** A direct application of Pinsker's inequality gives that \(T_{TV}^{\star}(\nu)\geq\sqrt{2T_{KL}^{\star}(\nu)}\). For completeness, we present here the exact calculations:For every alternative mean parameter \(\lambda\) and every arm \(a\), using Pinkser's inequality, we have that \(d_{TV}(\mu_{a},\lambda_{a})\leq\sqrt{\frac{1}{2}d_{KL}(\mu_{a},\lambda_{a})}\). Therefore, for every allocation over arms \(\omega\), we have

\[\sum_{a}\omega_{a}d_{TV}(\mu_{a},\lambda_{a})\leq\sum_{a}\omega_{a}\sqrt{\frac{ 1}{2}d_{KL}(\mu_{a},\lambda_{a})}\leq\sqrt{\frac{1}{2}\sum_{a}\omega_{a}d_{KL} (\mu_{a},\lambda_{a})}\,.\]

Taking the supremum over the simplex and the infimum over the set of alternative mean parameters yields \((T^{\star}_{TV}(\nu))^{-1}\leq\sqrt{\frac{1}{2}(T^{\star}_{KL}(\nu))^{-1}}\). This concludes the proof.

## Appendix C Privacy analysis

In this section, we prove that AdaP-TT satisfies \(\epsilon\)-DP. We first provide the privacy lemma that justifies using doubling and forgetting in AdaP-TT. Using the privacy lemma and the post-processing property of DP, we conclude the privacy analysis of AdaP-TT.

### Privacy lemma for non-overlapping sequences

**Lemma 2** (Privacy of non-overlapping sequence of empirical means).: _Let \(\mathcal{M}\) be a mechanism that takes a **set** as input and outputs the private empirical mean, i.e._

\[\mathcal{M}(\{r_{i},\dots,r_{j}\})\triangleq\frac{1}{j-i}\sum_{t=i}^{j}r_{t}+ Lap\left(\frac{1}{(j-i)\epsilon}\right).\] (7)

_Let \(\ell<T\) and \(t_{1},\dots t_{\ell},t_{\ell+1}\) be in \([1,T]\) such that \(1=t_{1}<\dots<t_{\ell}<t_{\ell+1}-1=T\). Let's define the following mechanism_

\[\mathcal{G}:\{r_{1},\dots,r_{T}\}\rightarrow\bigotimes_{i=1}^{\ell}\mathcal{ M}_{\{r_{i},\dots,r_{i+1}-1\}}\] (8)

_In other words, \(\mathcal{G}\) is the mechanism we get by applying \(\mathcal{M}\) to the non-overlapping partition of the sequence \(\{r_{1},\dots,r_{T}\}\) according to \(t_{1}<\dots<t_{\ell}<t_{\ell+1}\), i.e._

\[\begin{pmatrix}r_{1}\\ r_{2}\\ \vdots\\ r_{T}\end{pmatrix}\begin{pmatrix}\mu_{1}\\ \vdots\\ \mu_{\ell}\end{pmatrix}\]

_where \(\mu_{i}\sim\mathcal{M}_{\{r_{i},\dots,r_{i+1}-1\}}\)._

_For \(r_{t}\in[0,1]\), the mechanism \(\mathcal{G}\) is \(\epsilon\)-DP._

Proof.: Let \(r^{T}\triangleq(r_{1},\dots,r_{T})\) and \(r^{\prime T}\triangleq(r^{\prime}_{1},\dots,r^{\prime}_{T})\) be two neighbouring reward sequences in [0,1]. This implies that \(\exists j\in[1,T]\) such that \(r_{j}\neq r^{\prime}_{j}\) and \(\forall t\neq j\), \(r_{t}=r^{\prime}_{t}\).

Let \(\ell^{\prime}\) be such that \(t_{\ell^{\prime}}\leq j\leq t_{\ell^{\prime}+1}-1\), and follows the convention that \(t_{0}=1\) and \(t_{\ell+1}=T+1\).

Let \(\mu\triangleq(\mu_{1},\dots,\mu_{\ell})\) a fixed sequence of outcomes. Then,

\[\frac{\mathbb{P}(\mathcal{G}(r^{T})=\mu)}{\mathbb{P}(\mathcal{G}(r^{\prime T}) =\mu)}=\frac{\mathbb{P}\left(\mathcal{M}(\{r_{t_{\ell^{\prime}}},\dots,r_{t_ {\ell^{\prime+1}}-1}\})=\mu_{\ell^{\prime}}\right)}{\mathbb{P}\left(\mathcal{M }(\{r_{t_{\ell^{\prime}}},\dots,r_{t_{\ell^{\prime+1}}-1}\})=\mu_{\ell^{ \prime}}\right)}\leq e^{\epsilon},\]

where the last inequality holds true because \(\mathcal{M}\) satisfies \(\epsilon\)-DP following Theorem 1.

### Privacy analysis of AdaP-TT: Proof of Theorem 3

**Theorem 3** (Privacy analysis).: _For rewards in \([0,1]\), AdaP-TT satisfies \(\epsilon\)-global DP._

Proof.: Let \(T\geq 1\). Let \(\underline{\textbf{d}}^{T}=\{\textbf{x}_{1},\ldots,\textbf{x}_{T}\}\) and \(\underline{\textbf{d}^{\prime}}^{T}=\{\textbf{d}^{\prime}_{1},\ldots,\textbf{ d}^{\prime}_{T}\}\) two neighbouring reward tables in \((\mathbb{R}^{K})^{T}\). Let \(j\in[1,T]\) such that, for all \(t\neq j\), \(d_{t}=d^{\prime}_{t}\).

We also fix a sequence of sampled actions \(\underline{a}^{T}=\{a_{1},\ldots,a_{T}\}\in[K]^{T}\) and a recommended action \(\hat{a}\in K\).

We refer to AdaP-TT BAI strategy by \(\pi\).

We want to show that: \(\pi(\underline{a}^{T},\widehat{a},T\mid\underline{\textbf{d}}^{T})\leq e^{ \epsilon}\pi(\underline{a}^{T},\widehat{a},T\mid\underline{\textbf{d}^{\prime }}^{T})\).

The main idea is that the change of reward in the \(j\)-th reward only affects the empirical mean computed in one episode, which is made private using the Laplace Mechanism and Lemma 2.

**Step 1.** Sequential decomposition of the output probability

We observe that due to the sequential nature of the interaction, the output probability can be decomposed to a part that depends on \(\underline{\textbf{d}}^{j-1}\triangleq\{\textbf{x}_{1},\ldots,\textbf{x}_{j- 1}\}\), which is identical for both \(\underline{\textbf{d}}^{T}\) and \(\underline{\textbf{d}^{\prime}}^{T}\) and a second conditional part on the history.

Specifically, we have that

\[\pi(\underline{a}^{T},\widehat{a},T\mid\underline{\textbf{d}}^{T}) \triangleq\operatorname{Rec}_{T+1}\left(\widehat{a}\mid\mathcal{H }_{T}\right)\operatorname{S}_{T+1}\left(\top\mid\mathcal{H}_{T}\right)\prod_{t =1}^{T}\operatorname{S}_{t}\left(a_{t}\mid\mathcal{H}_{t-1}\right)\] \[\triangleq\mathcal{P}^{\pi}_{\overline{\textbf{d}}^{j-1}}(a^{j}) \mathcal{P}^{\pi}_{\overline{\textbf{d}}}(a_{>j},\hat{a},T\mid\underline{a}^{ j})\]

where

* \(a_{>j}\triangleq(a_{j+1},\ldots,a_{T})\)
* \(\mathcal{P}^{\pi}_{\underline{\textbf{d}}^{j-1}}(\underline{a}^{j}) \triangleq\prod_{t=1}^{j}\operatorname{S}_{t}\left(a_{t}\mid\mathcal{H}_{t-1}\right)\)
* \(\mathcal{P}^{\pi}_{\overline{\textbf{d}}}(a_{>j},\hat{a},T\mid\underline{a}^ {j})\triangleq\operatorname{Rec}_{T+1}\left(\widehat{a}\mid\mathcal{H}_{T} \right)\operatorname{S}_{T+1}\left(\top\mid\mathcal{H}_{T}\right)\prod_{t=j+1 }^{T}\operatorname{S}_{t}\left(a_{t}\mid\mathcal{H}_{t-1}\right)\)

Similarly

\[\pi(\underline{a}^{T},\widehat{a},T\mid\underline{\textbf{d}}^{{}^{T}}) \triangleq\mathcal{P}^{\pi}_{\overline{\textbf{d}}^{j-1}}(\underline{a}^{j}) \mathcal{P}^{\pi}_{\overline{\textbf{d}}^{j}}(a_{>j},\hat{a},T\mid\underline{a }^{j})\]

since \(\underline{\textbf{d}}^{{}^{\prime}j-1}=\underline{\textbf{d}}^{j-1}\).

Which means that

\[\frac{\pi(\underline{a}^{T},\widehat{a},T\mid\underline{\textbf{d}}^{T})}{ \pi(\underline{a}^{T},\widehat{a},T\mid\underline{\textbf{d}}^{{}^{T}})}= \frac{\mathcal{P}^{\pi}_{\overline{\textbf{d}}}(a_{>j},\hat{a},T\mid \underline{a}^{j})}{\mathcal{P}^{\pi}_{\overline{\textbf{d}}^{\prime}}(a_{>j}, \hat{a},T\mid\underline{a}^{j})}\] (9)

**Step 2.** The adaptive episodes are the same, before step \(j\)

Let \(\ell\) such that \(t_{\ell}\leq j<t_{\ell+1}\) when \(\pi\) interacts with \(\underline{\textbf{d}}^{T}\). Let us call it \(\psi^{\pi}_{\overline{\textbf{d}}^{T}}(j)\triangleq\ell\).

Similarly, let \(\ell^{\prime}\) such that \(t_{\ell^{\prime}}\leq j<t_{\ell^{\prime}+1}\) when \(\pi\) interacts with \(\underline{\textbf{d}}^{{}^{\prime}T}\). Let us call it \(\psi^{\pi}_{\underline{\textbf{d}}^{{}^{\prime}T}}(j)\triangleq\ell^{\prime}\).

Since \(\psi^{\pi}_{\overline{\textbf{d}}^{T}}(j)\) only depends on \(\underline{\textbf{d}}^{j-1}\), which is identical for \(\underline{\textbf{d}}^{T}\) and \(\underline{\textbf{d}}^{{}^{\prime}T}\), we have that \(\psi^{\pi}_{\overline{\textbf{d}}^{T}}(j)=\psi^{\pi}_{\underline{\textbf{d}}^ {{}^{\prime}T}}(j)\) with probability \(1\).

We call \(\xi_{j}\) the last **time-step** of the episode \(\psi^{\pi}_{\overline{\textbf{d}}^{T}}(j)\), i.e \(\xi_{j}\triangleq t_{\psi^{\pi}_{\overline{\textbf{d}}^{T}}(j)+1}-1\).

**Step 3.** Private sufficient statistics

Let \(r_{t}\triangleq\underline{\textbf{d}}^{T}_{t,a_{t}}\), be the reward corresponding to the action \(a_{t}\) in the table \(\underline{\textbf{d}}^{T}\). Similarly, \(r^{\prime}_{t}\triangleq\underline{\textbf{d}}^{{}^{\prime}T}_{t,a_{t}}\) for \(\underline{\textbf{d}}^{{}^{\prime}T}\).

Let us define \(L_{j}\triangleq\mathcal{G}_{\{r_{1},\ldots,r_{\xi_{j}}\}}\) and \(L^{\prime}_{j}\triangleq\mathcal{G}_{\{r^{\prime}_{1},\ldots,r^{\prime}_{\xi_{j}}\}}\), where \(\mathcal{G}\) is defined as in Eq. 8, using the same episodes for \(d\) and \(d^{\prime}\). In other words, \(L_{j}\) is the list of private empirical means computed on a non-overlapping sequence of rewards before step \(\xi_{j}\).

Using the forgetting structure of AdaP-TT, there exists a randomised mapping \(f_{\underline{\mathbf{d}}_{>\xi_{j}}}\) such that \(\mathcal{P}^{\pi}_{\underline{\mathbf{d}}}(\cdot\mid a^{j})=f_{\underline{ \mathbf{d}}_{>\xi_{j}}}\circ L_{j}\) and \(\mathcal{P}^{\pi}_{\underline{\mathbf{d}}^{\prime}}(\cdot\mid a^{j})=f_{ \underline{\mathbf{d}}_{>\xi_{j}}}\circ L^{\prime}_{j}\).

In other words, the interaction of \(\pi\) with \(\underline{\mathbf{d}}\) and \(\underline{\mathbf{d}}^{\prime}\) from step \(\xi_{j}+1\) until \(T\) only depends on the sufficient statistics \(L_{j}\), which summarises what happened before \(\xi_{j}\), and the new inputs \(\underline{\mathbf{d}}_{>\xi_{j}}\), which are the same for \(\underline{\mathbf{d}}\) and \(\underline{\mathbf{d}}^{\prime}\).

**Step 4.**Concluding with Lemma 2 and the post-processing lemma

Since rewards are in \([0,1]\), using Lemma 2, we have that \(\mathcal{G}\) is \(\epsilon\)-DP.

Since \(\mathcal{P}^{\pi}_{\underline{\mathbf{d}}}(\cdot\mid a^{j})\) is just a post-processing of the output of \(\mathcal{G}\), we have that

\[\frac{\mathcal{P}^{\pi}_{\underline{\mathbf{d}}}(a_{>j},\hat{a},T\mid a^{j})} {\mathcal{P}^{\pi}_{\underline{\mathbf{d}}^{\prime}}(a_{>j},\hat{a},T\mid a^{ j})}\leq e^{\epsilon}\,,\]

and Eq. (9) concludes the proof. 

## Appendix D AdaP-TT: A private Top Two algorithm with adaptive episodes

We propose a generic wrapper to adapt existing BAI algorithms to tackle private BAI (Appendix D.1). Then, we show how to instantiate our wrapper with an instance of Top Two algorithm (Appendix D.2), namely TTUCB [1].

### Generic wrapper on existing BAI algorithms

We propose a generic wrapper to adapt existing BAI algorithms to tackle the \(\epsilon\)-DP-FC-BAI problem.

1. It uses _adaptive episodes with doubling and forgetting_ (Appendix D.1.1). This builds on the idea used to make the UCB algorithm private for regret minimisation [1].
2. It relies on a _private GLR (generalised likelihood ratio) stopping rule_ (Appendix D.1.2). The GLR stopping rule [13] is widely used in the BAI literature, since it ensures \(\delta\)-correctness of the stopping rule regardless of the sampling rule.

For each arm \(a\in[K]\), we maintain a phase at time \(n\) whose index will be denoted by \(k_{n,a}\in\mathbb{N}\). We switch phase as soon as the number of times that the arm was played is doubled. We only evaluate the stopping condition when we switch phase for an arm.

#### d.1.1 Adaptive episodes with doubling and forgetting

As initialisation, we start by pulling each arm once, and set \(k_{n,a}=1\), \(T_{1}(a)=K+1\) and \(N_{n,a}=1\) for all \(a\in[K]\). In the following, we will consider \(n>K\) and we denote the _global pulling count_ of arm \(a\) before time \(n\) by \(N_{n,a}\triangleq\sum_{t\in[n-1]}\mathds{1}\left(I_{t}=a\right)\). For each arm \(a\in[K]\), the random stopping time denoting the end of \(k_{a}-1\) and the beginning of phase \(k_{a}>1\) is denoted by

\[T_{k_{a}}(a)\triangleq\inf\left\{n\in\mathbb{N}\mid N_{n,a}\geq 2N_{T_{k_{a}-1}(a ),n}\right\}\.\] (10)

At the beginning of phase \(k_{a}\) for an arm \(a\in[K]\), we update the empirical mean based on the observations on arm \(a\in[K]\) collected during this last phase, i.e.

\[\hat{\mu}_{k_{a},a}\triangleq\frac{1}{\tilde{N}_{k_{a},a}}\sum_{s=T_{k_{a}-1}(a )}^{T_{k_{a}}(a)-1}X_{s}\mathds{1}\left\{I_{s}=a\right\}\,\] (11)

where the _local pulling count_ pf arm \(a\) is denoted by \(\tilde{N}_{k_{a},a}\triangleq N_{T_{k_{a}}(a),a}-N_{T_{k_{a}-1}(a),a}\), meaning it is the number of collected samples during the phase \(k_{a}-1\). To ensure privacy, we add a Laplacenoise to define the private empirical mean, i.e.

\[\tilde{\mu}_{k_{a},a}\triangleq\hat{\mu}_{k_{a},a}+Y_{k_{a},a}\quad\text{where} \quad Y_{k_{a},a}\sim\text{Lap}\left(\frac{1}{\epsilon\tilde{N}_{k_{a},a}}\right)\;.\] (12)

We emphasise that only the private version the estimator \(\hat{\mu}_{k_{a},a}\), i.e. \(\tilde{\mu}_{k_{a},a}\), is used by the algorithm until the end of phase \(k_{a}\) for arm \(a\). Since \((k_{n,a})_{a\in[K]}\) denotes the current phases at time \(n\), our algorithm relies on \((\hat{\mu}_{k_{n,a},a},\tilde{\mu}_{k_{a},a},\tilde{N}_{k_{n,a},a},N_{n,a})_{a \in[K]}\).

Due to the doubling, the growth of the global and local pulling counts is exponential (Lemma 3).

**Lemma 3**.: _For all \(a\in[K]\) and all \(k\in\mathbb{N}\) such that \(\mathbb{E}_{\boldsymbol{\nu}}[T_{k}(a)]<+\infty\), we have_

\[N_{T_{k}(a),a}=2^{k-1}\quad\text{and}\quad\tilde{N}_{k,a}=2^{k-2}\;.\]

Proof.: Let \(a\in[K]\). After initialisation, we have \(k=1\), \(T_{1}(a)=K+1\) and \(N_{T_{1}(a),a}=1\). Using the definition of the adaptive phase switch (Equation (10)), it is direct to see that \(N_{T_{2}(a),a}=2\) and \(\tilde{N}_{2,a}=1\) when \(\mathbb{E}_{\boldsymbol{\nu}}[T_{2}(a)]<+\infty\).

Now, we proceed by recurrence. Suppose that \(N_{T_{k}(a),a}=2^{k-1}\) and \(\tilde{N}_{k,a}=2^{k-2}\) when \(\mathbb{E}_{\boldsymbol{\nu}}[T_{k}(a)]<+\infty\). If \(\mathbb{E}_{\boldsymbol{\nu}}[T_{k+1}(a)]<+\infty\), then it means that the phase \(k\) ends for arm \(a\) almost surely. Since we sample only one arm at each round, at the beginning of phase \(k+1\) for arm \(a\), we have \(N_{T_{k+1}(a),a}=2N_{T_{k}(a),a}=2^{k}\) by using the definition of the adaptive phase switch (10). Then, we have directly that \(\tilde{N}_{k+1,a}=N_{T_{k+1}(a),a}-N_{T_{k}(a),a}=2^{k}-2^{k-1}=2^{k-1}\). 

#### d.1.2 GLR stopping rule

Non-private GLR stopping rule with phases.Given a set of non-private threshold functions \((c_{k})_{k\in\mathbb{N}}\), such that \(c_{k}:\mathbb{N}\times\mathbb{N}\times(0,1)\to\mathbb{R}_{+}\) for all \(k\in\mathbb{N}\), the non-private GLR stopping rule can be evaluated at the beginning of each phase for each arm, namely

\[\tau_{\delta}^{\text{NP}}=\inf\left\{n\in\mathbb{N}\mid\forall b \neq\hat{a}_{n}^{\text{NP}},\;\frac{(\hat{\mu}_{k_{n,a}^{\text{NP}}},\hat{a}_{ n}^{\text{NP}}}{1/\tilde{N}_{k_{n,a}^{\text{NP}}},\hat{a}_{n}^{\text{NP}}}+1/ \tilde{N}_{k_{n,b},b}}\geq 2c_{k_{n,a}^{\text{NP}}k_{n,b}}(\tilde{N}_{k_{n,a}^{ \text{NP}},\hat{a}_{n}^{\text{NP}}},\tilde{N}_{k_{n,b},b},\delta)\right\}\;,\] (13)

where \(\hat{a}_{n}^{\text{NP}}=\arg\max_{a\in[K]}\hat{\mu}_{k_{n,a},a}\) is the non-private candidate answer until we switch phase again (for any arm). We emphasise that this _stopping condition is only evaluated at the beginning of each phase for each arm since it involves quantities that are fixed until we switch phase again_.

Lemma 4 gives non-private threshold functions ensuring that the non-private GLR stopping rule is \(\delta\)-correct for all \(\delta\in(0,1)\), independently of the sampling rule.

**Lemma 4**.: _Let \(\delta\in(0,1)\). Let \(s>1\) and \(\zeta\) be the Riemann \(\zeta\) function. Given any sampling rule, the non-private GLR stopping rule (Equation (13)) with non-private threshold functions_

\[c_{k}(n,m,\delta)=2\mathcal{C}_{G}\left(\frac{1}{2}\log\left( \frac{(K-1)\zeta(s)^{2}k^{s}}{\delta}\right)\right)+2\log(4+\log n)+2\log(4+ \log m)\] (14)

_ensures \(\delta\)-correctness for \(1\)-sub-Gaussian distributions. The function \(\mathcal{C}_{G}\) is defined in Equation (15). It satisfies \(\mathcal{C}_{G}\approx x+\log x\)._

Proof.: The non-private GLR stopping rule matches the one used for Gaussian bandits. Proving \(\delta\)-correctness of a GLR stopping rule is done by leveraging concentration results. In particular, we build upon Theorem 9 of [10], which is restated below.

**Lemma 5**.: _Consider sub-Gaussian bandits with means \(\boldsymbol{\mu}\in\mathbb{R}^{K}\). Let \(S\subseteq[K]\) and \(x>0\)._

\[\mathbb{P}_{\boldsymbol{\nu}}\left[\exists n\in\mathbb{N},\,\sum_{k \in S}\frac{N_{n,k}}{2}(\mu_{n,k}-\mu_{k})^{2}>\sum_{k\in S}2\log\left(4+\log \left(N_{n,k}\right)\right)+|S|\mathcal{C}_{G}\left(\frac{x}{|S|}\right) \right]\leq e^{-x}\;,\]_where \(\mathcal{C}_{G}\) is defined in [11] as_

\[\mathcal{C}_{G}(x)\triangleq\min_{\lambda\in\left]1/2,1\right]}\frac{g_{G}( \lambda)+x}{\lambda}\quad\text{and}\quad g_{G}(\lambda)\triangleq 2\lambda-2 \lambda\log(4\lambda)+\log\zeta(2\lambda)-\frac{1}{2}\log(1-\lambda)\;.\] (15)

_Here, \(\zeta\) is the Riemann \(\zeta\) function and \(\mathcal{C}_{G}(x)\approx x+\log(x)\)._

Let \(\hat{a}=\hat{a}_{n}^{\text{NP}}=\arg\max_{b\in[K]}\hat{\mu}_{k_{n,a},a}\). Standard manipulations yield that for all \(b\neq\hat{a}\)

\[\frac{(\hat{\mu}_{k_{n,\hat{a}},\hat{a}}-\hat{\mu}_{k_{n,b},b})^{2}}{1/\tilde{ N}_{k_{n,\hat{a}},\hat{a}}+1/\tilde{N}_{k_{n,b}}}=\inf_{y\geq x}\left\{ \tilde{N}_{k_{n,\hat{a}},\hat{a}}(\hat{\mu}_{k_{n,\hat{a}},\hat{a}}-x)^{2}+ \tilde{N}_{k_{n,b},b}(\hat{\mu}_{k_{n,b},b}-y)^{2}\right\}\;.\]

Using the non-private GLR stopping rule (13) with non-private threshold functions \((c_{k})_{k\in\mathbb{N}}\) and the above manipulations, we obtain

\[\mathbb{P}_{\boldsymbol{\nu}}\left(\tau_{\delta}^{\text{NP}}<+ \infty,\hat{a}\neq a^{\star}\right)\] \[\leq\mathbb{P}_{\boldsymbol{\nu}}\left(\exists n\in\mathbb{N},\; \exists a\neq a^{\star},\;a=\operatorname*{arg\,max}_{b\in[K]}\hat{\mu}_{k_{n,b},b},\;\forall b\neq a,\right.\] \[\left.\inf_{y\geq x}\left\{\tilde{N}_{k_{n,a},a}(\hat{\mu}_{k_{n,a},a}-x)^{2}+\tilde{N}_{k_{n,b},b}(\hat{\mu}_{k_{n,b},b}-y)^{2}\right\}\geq 2c _{k_{n,a}k_{n,b}}(\tilde{N}_{k_{n,a},a},\tilde{N}_{k_{n,b},b},\delta)\right)\] \[\underset{(a)}{\leq}\mathbb{P}_{\boldsymbol{\nu}}\left(\exists n \in\mathbb{N},\;\exists a\neq a^{\star},\;a=\operatorname*{arg\,max}_{b\in[K] }\hat{\mu}_{k_{n,a},a},\right.\] \[\left.\tilde{N}_{k_{n,a},a}(\hat{\mu}_{k_{n,a},a}-\mu_{a})^{2}+ \tilde{N}_{k_{n,a^{\star}},a^{\star}}(\hat{\mu}_{k_{n,a^{\star}},a^{\star}}- \mu_{a^{\star}})^{2}\geq 2c_{k_{n,a}k_{n,a^{\star}}}(\tilde{N}_{k_{n,a},a}, \tilde{N}_{k_{n,a^{\star}},a^{\star}},\delta)\right)\] \[\underset{(b)}{\leq}\mathbb{P}_{\boldsymbol{\nu}}\left(\exists a \neq a^{\star},\exists(k_{a},k_{a^{\star}})\in\mathbb{N}^{2},\right.\] \[\left.\tilde{N}_{k_{a},a}(\hat{\mu}_{k_{a},a}-\mu_{a})^{2}+ \tilde{N}_{k_{a^{\star}},a^{\star}}(\hat{\mu}_{k_{a^{\star}},a^{\star}}-\mu_{ a^{\star}})^{2}\geq 2c_{k_{a}k_{a^{\star}}}(\tilde{N}_{k_{a},a},\tilde{N}_{k_{a^{ \star}},a^{\star}},\delta)\right)\] \[\leq\sum_{a\neq a^{\star}}\sum_{(k_{a},k_{a^{\star}})\in\mathbb{N }^{2}}\mathbb{P}_{\boldsymbol{\nu}}\left(\tilde{N}_{k_{a},a}(\hat{\mu}_{k_{a},a }-\mu_{a})^{2}+\tilde{N}_{k_{a^{\star}},a^{\star}}(\hat{\mu}_{k_{a^{\star}},a^ {\star}}-\mu_{a^{\star}})^{2}\right.\] \[\geq 2c_{k_{a}k_{a^{\star}}}(\tilde{N}_{k_{a},a},\tilde{N}_{k_{a^{ \star}},a^{\star}},\delta)\right)\;,\]

The inequality (a) is obtained with \((b,x,y)=(a^{\star},\mu_{a},\mu_{a^{\star}})\). The inequality (b) drops the condition \(a=\arg\max_{b\in[K]}\hat{\mu}_{k_{b},b}\), hence we can restrict to \((k_{a},k_{a^{\star}})\in\mathbb{N}^{2}\) since it doesn't depend on other phase indices. The inequality (c) relies on a direct union bound. For all \(a\neq a^{\star}\) and all \((k_{a},k_{a^{\star}})\in\mathbb{N}^{2}\), the estimators \(\hat{\mu}_{k_{a},a}\) (resp. \(\hat{\mu}_{k_{a^{\star}},a^{\star}}\)) are based solely on the observations collected for arm \(a\) (resp. arm \(a^{\star}\)) between times \(n\in\{T_{k_{a}-1}(a),\cdots,T_{k_{a}}(a)-1\}\) (resp. \(n\in\{T_{k_{a^{\star}}-1}(a^{\star}),\cdots,T_{k_{a^{\star}}}(a^{\star})-1\}\)) with local counts \(\tilde{N}_{k_{a},a}\) (resp. \(\tilde{N}_{k_{a^{\star}},a^{\star}}\)), i.e. dropping past observations. Using Lemma 5 for all \(a\neq a^{\star}\) and all \((k_{a},k_{a^{\star}})\in\mathbb{N}^{2}\), we obtain

\[\mathbb{P}_{\boldsymbol{\nu}}\left(\tau_{\delta}<+\infty,\hat{i}=i^{\star} \right)\leq\frac{\delta}{K-1}\frac{1}{\zeta(s)^{2}}\sum_{a\neq a^{\star}} \sum_{(k_{a},k_{a^{\star}})\in\mathbb{N}^{2}}\frac{1}{(k_{a}k_{a^{\star}})^{s}}= \delta\;.\]

Private GLR stopping rule with phases.Since we want to ensure privacy, the non-private GLR stopping rule cannot be used since it relies on the empirical means \(\hat{\mu}_{k_{n,a},a}\), which are not private. To alleviate this problem, we propose the private GLR stopping rule, which is based on the private empirical means \(\tilde{\mu}_{k_{n,a},a}\).

Given a set of private threshold functions \((c_{\epsilon,k_{1},k_{2}})_{(\epsilon,k_{1},k_{2})\in\mathbb{R}_{+}^{\star} \times\mathbb{N}^{2}}\), such that \(c_{\epsilon,k_{1},k_{2}}:\mathbb{N}\times\mathbb{N}\times(0,1)\rightarrow\mathbb{R} _{+}\) for all \((\epsilon,k_{1},k_{2})\in\mathbb{R}_{+}^{\star}\times\mathbb{N}^{2}\), the non-private GLR stopping rule is evaluated at the beginning of each phase for each arm, namely

\[\tau_{\delta}=\inf\left\{n\in\mathbb{N}\mid\forall b\neq\hat{a}_{n},\;\frac{( \tilde{\mu}_{k_{n,\hat{a}},\hat{a}_{n}}-\tilde{\mu}_{k_{n,b},b})^{2}}{1/\tilde{ N}_{k_{n,\hat{a}},\hat{a}_{n}}+1/\tilde{N}_{k_{n,b},b}}\geq 2c_{\epsilon,k_{n,\hat{a}_{n}},k_{n,b}}(\tilde{N}_{k_{n,\hat{a}},\hat{a}_{n}},\tilde{N}_{k_{n,b},b},\delta)\right\}\;,\] (16)where \(\hat{a}_{n}=\arg\max_{a\in[K]}\mu_{k_{n,a},a}\) is the private candidate answer until we switch phase again (for any arm). We emphasise that this stopping condition is only evaluated at the beginning of each phase for each arm since it involves quantities that are fixed until we switch phase again.

Theorem 4 gives private threshold functions ensuring that the private GLR stopping rule is \(\delta\)-correct for all \(\delta\in(0,1)\) and all \(\epsilon\in\mathbb{R}_{+}^{*}\), independently of the sampling rule.

**Theorem 4** (\(\delta\)-correctness of the private GLR stopping rule).: _Let \(\delta\in(0,1)\) and \(\epsilon\in\mathbb{R}_{+}^{*}\). Let \(s>1\) and \(\zeta\) be the Riemann \(\zeta\) function and non-private threshold function \((c_{k})_{k\in\mathbb{N}}\) as in (14). Given any sampling rule, the private GLR stopping rule (Equation (16)) with private threshold functions_

\[c_{\epsilon,k_{1},k_{2}}(n,m,\delta)=2c_{k_{1}k_{2}}(n,m,\delta/2)+\frac{1}{n \epsilon^{2}}\log\left(\frac{2Kk_{1}^{s}\zeta(s)}{\delta}\right)^{2}+\frac{1} {m\epsilon^{2}}\log\left(\frac{2Kk_{2}^{s}\zeta(s)}{\delta}\right)^{2}\] (17)

_ensures \(\delta\)-correctness for \(1\)-sub-Gaussian distributions._

Proof.: Let \(\epsilon\in\mathbb{R}_{+}^{*}\). Since \(Y_{k_{n,a},a}\sim\text{\rm Lap}\left((\epsilon\tilde{N}_{k_{n,a},a})^{-1}\right)\), we have that \(\tilde{N}_{k_{n,a},a}|Y_{k_{n,a},a}|\sim\mathcal{E}(\epsilon)\) for all \(a\in[K]\) and all \(n\in\mathbb{N}\), where \(\mathcal{E}(\cdot)\) denotes the exponential distribution. Using concentration results for exponential distribution, a direct union bound yields that

\[\mathbb{P}\left(\exists n\in\mathbb{N},\;\exists a\in[K],\;\tilde{N}_{k_{n,a}, a}|Y_{k_{n,a},a}|\geq\frac{1}{\epsilon}\log\left(\frac{Kk_{n,a}^{s}\zeta(s)}{ \delta}\right)\right)\leq\delta\;.\] (18)

Let us denote \(\tilde{c}_{\epsilon,k_{1},k_{2}}(n,m,\delta)\) the threshold associated to the Laplace noise, i.e.

\[\tilde{c}_{\epsilon,k_{1},k_{2}}(n,m,\delta)=\frac{1}{n\epsilon^{2}}\log\left( \frac{Kk_{1}^{s}\zeta(s)}{\delta}\right)^{2}+\frac{1}{m\epsilon^{2}}\log\left( \frac{Kk_{2}^{s}\zeta(s)}{\delta}\right)^{2}\;.\]

Using the private GLR stopping rule (Equation (16)) with private threshold functions \((c_{\epsilon,k_{1},k_{2}})_{(\epsilon,k_{1},k_{2})\in\mathbb{R}_{+}^{*} \times\mathbb{N}^{2}}\), similar manipulations as above yields

\[\mathbb{P}_{\boldsymbol{\nu}}\left(\tau_{\delta}<+\infty,\hat{a} \neq a^{\star}\right)\] \[\leq\mathbb{P}_{\boldsymbol{\nu}}\left(\exists n\in\mathbb{N},\; \exists a\neq a^{\star},\tilde{N}_{k_{n,a},a}(\hat{\mu}_{k_{n,a},a}-\mu_{a}+Y_{ k_{n,a},a})^{2}+\tilde{N}_{k_{n,a^{\star}},a^{\star}}(\hat{\mu}_{k_{n,a^{\star}},a^{ \star}}-\mu_{a^{\star}}+Y_{k_{n,a^{\star}},a^{\star}})^{2}\right.\] \[\qquad\geq 4c_{k_{n,a},k_{n,a^{\star}}}(\tilde{N}_{k_{n,a},a}, \tilde{N}_{k_{n,a^{\star}},a^{\star}},\delta/2)+2\tilde{c}_{\epsilon,k_{n,a},k_{ n,a^{\star}}}(\tilde{N}_{k_{n,a},a},\tilde{N}_{k_{n,a^{\star}},a^{\star}}, \delta/2)\right)\] \[\underset{(a)}{\leq}\mathbb{P}_{\boldsymbol{\nu}}\left(\exists n \neq a^{\star},\exists(k_{a},k_{a^{\star}})\in\mathbb{N}^{2},\right.\] \[\qquad\qquad\tilde{N}_{k_{a},a}(\hat{\mu}_{k_{a},a}-\mu_{a})^{2}+ \tilde{N}_{k_{a^{\star}},a^{\star}}(\hat{\mu}_{k_{a^{\star}},a^{\star}}-\mu_{ a^{\star}})^{2}\geq 2c_{k_{a}k_{a^{\star}}}(\tilde{N}_{k_{a},a},\tilde{N}_{k_{a^{\star} },a^{\star}},\delta/2)\right)\] \[\quad+\mathbb{P}_{\boldsymbol{\nu}}\left(\exists n\in\mathbb{N},\; \exists a\neq a^{\star},\;\tilde{N}_{k_{n,a},a}Y_{k_{n,a},a}^{2}+\tilde{N}_{k_{ n,a^{\star}},a^{\star}}Y_{k_{n,a^{\star}},a^{\star}}^{2}\right.\] \[\qquad\qquad\geq\tilde{c}_{\epsilon,k_{n,a},k_{n,a^{\star}}}( \tilde{N}_{k_{n,a},a},\tilde{N}_{k_{n,a^{\star}},a^{\star}},\delta/2)\right)\] \[\underset{(b)}{\leq}\delta/2+\mathbb{P}\left(\exists n\in\mathbb{ N},\;\exists a\in[K],\;\tilde{N}_{k_{n,a},a}|Y_{k_{n,a},a}|\geq\frac{1}{\epsilon}\log \left(\frac{2Kk_{n,a}^{s}\zeta(s)}{\delta}\right)\right)\] \[\underset{(c)}{\leq}\delta/2+\delta/2=\delta\;.\]

The inequality (a) uses that \(\mathbb{P}(X+Y\geq a+b)\leq\mathbb{P}(X\geq a)+\mathbb{P}(Y\geq b)\) and \((x-y)^{2}\leq 2x^{2}+2y^{2}\). The inequality (c) leverages Lemma 4 and a direct inclusion of event. The inequality (c) deploys Equation (18) to conclude. 

### AdaP-Tt: Instantiating our wrapper with a Top Two sampling rule

A blueprint of Top Two algorithm design.At time \(n>K\), a Top Two sampling rule defines a leader \(B_{n}\) and a challenger \(C_{n}\). Then, it selects among them the next arm to sample \(I_{n}\). Given a proportion \(\beta\in(0,1)\) fixed beforehand, the choice of \(I_{n}\in\{B_{n},C_{n}\}\) should ensure that the leader is sampled close to \(\beta\) of the times where it was chosen as leader. In early works on Top Two algorithms, this choice is randomised. Following [15], we use \(K\) independent tracking procedures.

We denote by \(N^{a}_{n,b}\triangleq\sum_{t\in[n-1]}\mathds{1}\left(B_{t}=a,\ I_{t}=C_{t}=b\right)\) the number of times the arm \(b\) was pulled while the arm \(a\) was the leader, and by \(L_{n,a}\triangleq\sum_{t\in[n-1]}\mathds{1}\left(B_{t}=a\right)\) the number of times arm \(a\) was the leader. At time \(n>K\), the next arm to be pulled \(I_{n}\) is defined as

\[I_{n}=B_{n}\quad\text{if }N^{B_{n}}_{n,B_{n}}\leq\beta L_{n+1,B_{n}}\text{, otherwise }\quad I_{n}=C_{n}\,.\] (19)

In other words, we sample the leader if we have not yet sampled it a fraction \(\beta\) of the times it was leader. Those \(K\) independent tracking procedure satisfy the desired property (Lemma 6).

**Lemma 6** (Lemma 2.2 in [1]).: _For all \(n>K\) and all \(a\in[K]\), we have_

\[-1/2\leq N^{a}_{n,a}-\beta L_{n,a}\leq 1\,.\]

To finish specifying a Top Two algorithm, we simply need to specify the choice of the _leader/challenger pair_. Intuitively, a good choice of the leader/challenger pair should ensure (1) _sufficient exploration_, (2) _convergence of the leader towards the best arm \(a^{\star}\)_, and (3) _convergence of the global pulling proportions to the \(\beta\)-optimal allocation \(\omega_{\mathrm{KL},\beta}(\mu)\)_, which is defined for Gaussian distributions as

\[\omega^{\star}_{\mathrm{KL},\beta}(\bm{\mu})\triangleq\operatorname*{arg\, max}_{\omega\in\Sigma_{K},\omega_{a^{\star}}=\beta}\min_{a\neq a^{\star}} \frac{\Delta_{a}^{2}}{1/\beta+1/\omega_{a}}\,.\]

While we consider TTUCB algorithm [1] in AdaP-TT, we emphasise that other Top Two algorithms could be used with the same type of guarantees. TTUCB is a Top Two algorithm which combines a UCB-based leader and a Transportation Cost (TC) challenger. Its key novelty lies in the use of \(K\) tracking procedures. Since it is deterministic, the analysis is less cumbersome.

Non-private leader/challenger pair.The non-private leader/challenger pair is inspired by the TTUCB algorithm [1]. At time \(n>K\), the non-private UCB leader is defined as

\[B^{\text{NP}}_{n}\triangleq\operatorname*{arg\,max}_{a\in[K]}\left\{\hat{\mu} _{k_{n,a},a}+\sqrt{\frac{k_{n,a}}{\tilde{N}_{k_{n,a},a}}}\right\}\,,\] (20)

where \(\sqrt{k_{n,a}/\tilde{N}_{k_{n,a},a}}\) is a bonus to cope for the uncertainty which depends on the current phase \(k_{n,a}\). Later, we show that \(\log(n)/k_{n,a}=\Theta(1)\) for all \(a\in[K]\). Hence, it has the same scaling as the bonus used in standard UCB. Since it depends solely on the local counts \((\tilde{N}_{k_{n,a},a})_{a\in[K]}\) and the empirical means \((\hat{\mu}_{k_{n,a},a})_{a\in[K]}\), the non-private UCB leader is fixed until we switch phase again.

At time \(n>K\), the non-private TC challenger is defined as

\[C^{\text{NP}}_{n}\triangleq\operatorname*{arg\,min}_{a\neq B^{\text{NP}}_{n} }\frac{\hat{\mu}_{k_{n,B^{\text{NP}}_{n}},B^{\text{NP}}_{n}}-\hat{\mu}_{k_{n, a},a}}{\sqrt{1/N_{n,B^{\text{NP}}_{n}}+1/N_{n,a}}}\,.\] (21)

While it depends on the empirical means \((\hat{\mu}_{k_{n,a},a})_{a\in[K]}\) that are fixed till we switch phase again, it also depends on the global counts \((N_{n,a})_{a\in[K]}\). Therefore, the non-private TC challenger is chosen in an adaptive manner. This is the key to obtain guarantees on the expected sample complexity of the non-private algorithm.

We derive upper bounds on the expected sample complexity of the non-private AdaP-TT algorithm in the asymptotic regime of \(\delta\to 0\) (Theorem 6). In particular, it shows that the cost of doubling and forgetting is multiplicative four-factor compared to the TTUCB algorithm, which achieves \(T^{\star}_{\mathrm{KL},\beta}(\mu)\) (see Theorem 2.3 in [1]). We defer its proof to Appendix E.8.

**Theorem 6** (Asymptotic upper bound on expected sample complexity of non-private AdaP-TT).: _Let \((\delta,\beta)\in(0,1)^{2}\). Combined with the non-private GLR stopping rule (Equation (13)) using non-private threshold functions as in Equation (14), the non-private AdaP-TT algorithm is \(\delta\)-correct and satisfies that, for all \(1\)-sub-Gaussian distributions \(\nu\) with means \(\bm{\mu}\in\mathbb{R}^{K}\) such that \(\min_{a\neq b}|\mu_{a}-\mu_{b}|>0\),_

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}[\tau^{\text{NP}}_{\delta}]}{\log(1/ \delta)}\leq 4T^{\star}_{\mathrm{KL},\beta}(\mu)\,,\]

_where \(T^{\star}_{\mathrm{KL},\beta}(\mu)\) is the \(\beta\)-characteristic time for Gaussian distributions, such that_

\[2T^{\star}_{\mathrm{KL},\beta}(\bm{\mu})^{-1}\triangleq\max_{\omega\in\Sigma_{ K},\omega_{a^{\star}}=\beta}\frac{(\mu_{a^{\star}}-\mu_{a})^{2}}{1/\beta+1/\omega_{a}}\,.\]Private leader/challenger pair.Since we want to ensure privacy, the non-private leader/challenger pair cannot be used since it relies on the empirical means \(\hat{\mu}_{k_{n,a},a}\), which are not private. To alleviate this problem, we propose a private leader/challenger pair which is based on the private empirical means \(\tilde{\mu}_{k_{n,a},a}\).

At time \(n>K\), the private UCB leader is defined as

\[B_{n}\triangleq\operatorname*{arg\,max}_{a\in[K]}\left\{\tilde{ \mu}_{k_{n,a},a}+\sqrt{\frac{k_{n,a}}{\tilde{N}_{k_{n,a},a}}}+\frac{k_{n,a}}{ \epsilon\tilde{N}_{k_{n,a},a}}\right\}\;,\] (22)

where \(k_{n,a}/(\epsilon\tilde{N}_{k_{n,a},a})\) is a bonus to cope for the uncertainty due to the Laplace noise. It also depends on the current phase \(k_{n,a}\) and has the same scaling as the private UCB indices. Likewise, the private UCB leader is fixed until we switch phase again.

At time \(n>K\), the private Transportation Cost (TC) challenger is defined as

\[C_{n}\triangleq\operatorname*{arg\,min}_{a\neq B_{n}}\frac{ \tilde{\mu}_{k_{n,B_{n}},B_{n}}-\tilde{\mu}_{k_{n,a},a}}{\sqrt{1/N_{n,B_{n}}+1 /N_{n,a}}}\;.\] (23)

Likewise, the private TC challenger is chosen in an adaptive manner, which is key to obtain guarantees on the expected sample complexity of the private algorithm.

We derive upper bounds on the expected sample complexity of the private AdaP-TT algorithm in the asymptotic regime of \(\delta\to 0\) (Theorem 5). We defer its proof to Appendix E.

**Theorem 5** (Asymptotic upper bound on expected sample complexity of AdaP-TT).: _Let \((\delta,\beta)\in(0,1)^{2}\). Combined with the non-private GLR stopping rule (Equation (13)) using non-private threshold functions as in Equation (14), the non-private AdaP-TT algorithm is \(\delta\)-correct and satisfies that, for all bandit instances \(\bm{\nu}\) with \(1\)-sub-Gaussian distributions and means \(\bm{\mu}\in\mathbb{R}^{K}\) such that \(\min_{a\neq b}|\mu_{a}-\mu_{b}|>0\),_

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}[\tau_{\delta}]}{ \log(1/\delta)}\leq 4T_{\mathrm{KL},\beta}^{\star}(\mu)\left(1+\sqrt{1+ \frac{\Delta_{\max}^{2}}{2\epsilon^{2}}}\right)\;,\]

_where \(T_{\mathrm{KL},\beta}^{\star}(\mu)\) is the \(\beta\)-characteristic time for Gaussian distributions._

## Appendix E Analysis of AdaP-TT: Proof of Theorem 5

Let \(\beta\in(0,1)\), \(\epsilon\in\mathbb{R}_{+}^{\star}\), and \(\bm{\nu}\) be a bandit instance consisting of \(K\), \(1\)-sub-Gaussian distributions with distinct means \(\bm{\mu}\in\mathbb{R}^{K}\), i.e. \(\min_{a\neq b}|\mu_{a}-\mu_{b}|>0\). For conciseness, we denote \(\Delta_{a}\triangleq\mu_{a^{\star}}-\mu_{a}\), \(\Delta_{\min}\triangleq\min_{a\neq a^{\star}}\Delta_{a}\), and \(\Delta_{\max}\triangleq\max_{a\neq a^{\star}}\Delta_{a}\).

For Gaussian distributions, the unique \(\beta\)-optimal allocation \(\omega_{\mathrm{KL},\beta}^{\star}(\bm{\mu})=\{\omega_{\beta,a}^{\star}\}\) is defined as

\[\omega_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\triangleq\operatorname*{arg\, max}_{\omega\in\Sigma_{K},\omega_{a^{\star}}=\beta}\min_{a\neq a^{\star}}\frac{ \Delta_{a}^{2}}{1/\beta+1/\omega_{a}}\;.\] (24)

At equilibrium, we have equality of the transportation costs (see [1] for example), namely

\[\forall a\neq a^{\star},\quad\frac{\Delta_{a}^{2}}{1/\beta+1/\omega_{\beta,a} ^{\star}}=2T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})^{-1}\;.\] (25)

Our proof follows the unified sample complexity analysis of Top Two algorithms from [1].

Let \(\gamma>0\). We denote by \(T_{\bm{\mu},\gamma}\) the _convergence time_ towards \(\omega_{\beta}^{\star}\), which is a random variable quantifies the number of samples required for the global empirical allocations \(N_{n}/(n-1)\) to be \(\gamma\)-close to \(\omega_{\beta}^{\star}\) for any subsequent time, namely

\[T_{\bm{\mu},\gamma}\triangleq\inf\left\{T\geq 1\mid\forall n\geq T,\, \left\|\frac{N_{n}}{n-1}-\omega_{\beta}^{\star}\right\|_{\infty}\leq\gamma \right\}\;.\] (26)

The rest of Appendix E is organised as follows. First, we prove that AdaP-TT ensures sufficient exploration (Appendix E.2) Second, we prove that there is convergence towards the \(\beta\)-optimalallocation (Appendix E.3) in finite time. Finally, we conclude the proof of Theorem 5 (Appendix E.5). In Appendix E.6, we compare our asymptotic upper bound with our asymptotic lower bound on the expected sample complexity (Theorem 2). In Appendix E.7, we discuss the limitation of our result and pose an open problem. Appendix E.8 will detail the slight modification that needs to be made in order to obtain Theorem 6.

### Technical results

Before delving into the proofs, we first recall some useful technical results extracted from the literature.

Concentration results.In order to control the randomness of \((\tilde{\mu}_{k_{a},a})_{a\in[K]}\), we use a standard concentration result on the empirical mean of sub-Gaussian random variables and on sub-exponential observations (Lemma 7). Since Bernoulli distributions are \(1/2\)-sub-Gaussian and the absolute value of a Laplace is an exponential distribution, Lemma 7 applies to our setting.

**Lemma 7**.: _There exists a sub-Gaussian random variable \(W_{\mu}\) such that, almost surely,_

\[\forall a\in[K],\;\forall k_{a}\in\mathbb{N},\quad|\hat{\mu}_{k_{a},a}-\mu_{ a}|\leq W_{\mu}\sqrt{\frac{\log(e+\tilde{N}_{k_{a},a})}{\tilde{N}_{k_{a},a}}}\;.\]

_There exists a sub-exponential random variable \(W_{\epsilon}\) such that, almost surely,_

\[\forall a\in[K],\;\forall k_{a}\in\mathbb{N},\quad|Y_{k_{a},a}|\leq W_{ \epsilon}\frac{\log(e+k_{a})}{\tilde{N}_{k_{a},a}}\;.\]

_In particular, any random variable which is polynomial in \((W_{\epsilon},W_{\mu})\) has a finite expectation._

Proof.: The first part is a known result, e.g. Appendix E.2 in [1]. Let us define

\[W_{\epsilon}\triangleq\sup_{a\in[K]}\sup_{k_{a}\in\mathbb{N}}\frac{\tilde{N}_ {k_{a},a}|Y_{k_{a},a}|}{\log(e+k_{a})}\;.\]

By definition, we have that, almost surely,

\[\forall a\in[K],\;\forall k_{a}\in\mathbb{N},\quad|Y_{k_{a},a}|\leq W_{ \epsilon}\frac{\log(e+k_{a})}{\tilde{N}_{k_{a},a}}\;.\]

Since \(\tilde{N}_{k,i}|Y_{k,i}|\sim\mathcal{E}(\epsilon)\), Lemma 72 in [1] yields that \(W_{\epsilon}\) is a sub-exponential random variable. Since \(W_{\mu}\) is sub-Gaussian and \(W_{\epsilon}\) is a sub-exponential, any random variable which is polynomial in \((W_{\epsilon},W_{\mu})\) has a finite expectation. 

Inversion results.Lemma 8 gathers properties on the function \(\overline{W}_{-1}\), which is used in the literature to obtain concentration results.

**Lemma 8** ([1]).: _Let \(\overline{W}_{-1}(x)\triangleq-W_{-1}(-e^{-x})\) for all \(x\geq 1\), where \(W_{-1}\) is the negative branch of the Lambert \(W\) function. The function \(\overline{W}_{-1}\) is increasing on \((1,+\infty)\) and strictly concave on \((1,+\infty)\). In particular, \(\overline{W}_{-1}^{\prime}(x)=\left(1-\frac{1}{\overline{W}_{-1}(x)}\right)^{-1}\) for all \(x>1\). Then, for all \(y\geq 1\) and \(x\geq 1\),_

\[\overline{W}_{-1}(y)\leq x\quad\iff\quad y\leq x-\log(x)\;.\]

_Moreover, for all \(x>1\),_

\[x+\log(x)\leq\overline{W}_{-1}(x)\leq x+\log(x)+\min\left\{\frac{1}{2},\frac{ 1}{\sqrt{x}}\right\}\;.\]

Lemma 9 is an inversion result to upper bound a time, which is implicitly defined. It is a direct consequence of Lemma 8.

**Lemma 9**.: _Let \(\overline{W}_{-1}\) defined in Lemma 8. Let \(A>0\), \(B>0\) such that \(B/A+\log A>1\) and_

\[C(A,B)=\sup\left\{x\mid\;x<A\log x+B\right\}\;.\]

_Then, \(C(A,B)<h_{1}(A,B)\) with \(h_{1}(z,y)=z\overline{W}_{-1}\left(y/z+\log z\right)\)._Proof.: Since \(B/A+\log A>1\), we have \(C(A,B)\geq A\), hence

\[C(A,B)=\sup\left\{x\ |\ \ x<A\log(x)+B\right\}=\sup\left\{x\geq A\ |\ \ x<A\log(x)+B\right\}\.\]

Using Lemma 8 yields that

\[x\geq A\log x+B\ \iff\ \frac{x}{A}-\log\left(\frac{x}{A}\right)\geq\frac{B}{A}+ \log A\ \iff\ x\geq A\overline{W}_{-1}\left(\frac{B}{A}+\log A\right)\.\]

### Sufficient exploration

The first step of in the generic analysis of Top Two algorithms [JDB\({}^{+}\)22] is to show that AdaP-TT ensures sufficient exploration. The main idea is to show that, if there are still undersampled arms, either the leader or the challenger will be among them. Therefore, after a long enough time, no arm can still be undersampled. We emphasise that there are multiple ways to select the leader/challenger pair in order to ensure sufficient exploration. Therefore, while we conduct the proof for AdaP-TT, other choices of leader/challenger pair would yield similar results.

Given an arbitrary phase \(p\in\mathbb{N}\), we define the sampled enough set, i.e. the arms having reached phase \(p\), and the arm with highest mean in this set (when not empty) as

\[S_{n}^{p}=\{a\in[K]\ |\ N_{n,a}\geq 2^{p-1}\}\quad\text{and}\quad a_{n}^{ \star}=\operatorname*{arg\,max}_{a\in S_{n}^{p}}\mu_{a}\.\] (27)

Since \(\min_{a\neq b}|\mu_{a}-\mu_{b}|>0\), \(a_{n}^{\star}\) is unique.

Let \(p\in\mathbb{N}\) such that \((p-1)/4\in\mathbb{N}\). We define the highly and the mildly under-sampled sets as

\[U_{n}^{p}\triangleq\{a\in[K]\ |\ N_{n,a}<2^{(p-1)/2}\}\quad\text{and}\quad V_{n }^{p}\triangleq\{a\in[K]\ |\ N_{n,a}<2^{3(p-1)/4}\}\.\] (28)

They correspond to the arms having not reached phase \((p-1)/2\) and phase \(3(p-1)/4\), respectively.

**Lemma 10 shows that, when the leader is sampled enough, it is the arm with highest true mean among the sampled enough arms.**

**Lemma 10**.: _Let \(S_{n}^{p}\) and \(a_{n}^{\star}\) as in (27). There exists \(p_{0}\) with \(\operatorname*{E}_{\boldsymbol{\nu}}[\exp(\alpha p_{0})]<+\infty\) for all \(\alpha>0\) such that if \(p\geq p_{0}\), for all \(n\) such that \(S_{n}^{p}\neq\emptyset\), \(B_{n}\in S_{n}^{p}\) implies that \(B_{n}=a_{n}^{\star}=\operatorname*{arg\,max}_{a\in S_{n}^{p}}\tilde{\mu}_{k_{ n,a},a}\)._

Proof.: Let \(p_{0}\) to be specified later. Let \(p\geq p_{0}\). Let \(n\in\mathbb{N}\) such that \(S_{n}^{p}\neq\emptyset\), where \(S_{n}^{p}\) and \(a_{n}^{\star}\) as in Equation (27). Let \((k_{n,a})_{a\in[K]}\) be the phases indices for all arms. Since \(N_{n,a}\geq 2^{p-1}\) for all \(a\in S_{n}^{p}\), we have \(k_{n,a}\geq p\) and \(\tilde{N}_{k_{n,a},a}\geq 2^{p-2}\) by using Lemma 3. Using Lemma 7, we obtain that

\[\tilde{\mu}_{k_{n,a_{n}^{\star},a_{n}^{\star}}} \geq\mu_{a_{n}^{\star}}-W_{\mu}\sqrt{\frac{\log(e+2^{p-2})}{2^{p- 2}}}-W_{\epsilon}\frac{\log(e+p)}{2^{p-2}}\,\] \[\tilde{\mu}_{k_{n,a},a} \leq\mu_{a}+W_{\mu}\sqrt{\frac{\log(e+2^{p-2})}{2^{p-2}}}+W_{ \epsilon}\frac{\log(e+p)}{2^{p-2}}\,\quad\forall a\in S_{n}^{p}\setminus\{a_{n}^{\star}\}.\]

Here, we use that \(x\to\log(e+x)/x\) is decreasing.

Let \(\overline{\Delta}_{\min}=\min_{a\neq b}|\mu_{a}-\mu_{b}|\). By assumption on the considered instances, we know that \(\overline{\Delta}_{\min}>0\). Let \(p_{1}=\lceil\log_{2}(X_{1}-e)\rceil+2\) and \(p_{2}=\lceil\log_{2}((X_{2}-e-2)\log 2+1)\rceil+2\) with

\[X_{1} =\sup\left\{x>1\ |\ \ x\leq 64\overline{\Delta}_{\min}^{-2}W_{\mu}^{2 }\log x+e\right\}\leq h_{1}(64\overline{\Delta}_{\min}^{-2}W_{\mu}^{2},\ e)\,\] \[X_{2} =\sup\left\{x>1\ |\ \ x\leq\frac{8}{\log 2}\overline{\Delta}_{\min}^{-1 }W_{\epsilon}\log x+e+2-1/\log 2\right\}\leq h_{1}(8\overline{\Delta}_{\min}^{-1}W_{ \epsilon}/\log 2,\ 4)\,\]

where we used Lemma 9, and \(h_{1}\) defined therein. Then, for all \(p\in\mathbb{N}\) such that \(p\geq\max\{p_{1},p_{2}\}+1\) and all \(n\in\mathbb{N}\) such that \(S_{n}^{p}\neq\emptyset\), we have \(\tilde{\mu}_{k_{n,a_{n}^{\star},a_{n}^{\star}}}\geq\mu_{a_{n}^{\star}}- \overline{\Delta}_{\min}/4\) and \(\tilde{\mu}_{k_{n,a},a}\leq\mu_{a}+\overline{\Delta}_{\min}/4\) for all \(a\in S_{n}^{p}\setminus\{a_{n}^{\star}\}\), hence \(a_{n}^{\star}=\operatorname*{arg\,max}_{a\in[K]}\tilde{\mu}_{k_{n,a},a}\).

We have, for all \(\alpha\in\mathbb{R}_{+}\),

\[\exp(\alpha p_{1})\leq e^{3\alpha}(X_{1}-e)^{\alpha/\log 2}\quad\text{hence} \quad\mathbb{E}_{\boldsymbol{\nu}}[\exp(\alpha p_{1})]<+\infty\,\]where we used Lemma 7 and \(h_{1}(x,e)\sim_{x\to+\infty}x\log x\) to obtain that \(\exp(\alpha p_{1})\) is at most polynomial in \(W_{\mu}\). Likewise, we obtain that \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{2})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\).

Let us define the UCB indices by \(I_{k_{n,a},a}=\tilde{\mu}_{k_{n,a},a}+\sqrt{k_{n,a}/\tilde{N}_{k_{n,a},a}}+k_{n,a}/(\epsilon\tilde{N}_{k_{n,a},a})\). Using the above, we have

\[I_{k_{n,a_{n}^{*}},a_{n}^{*}}\geq\mu_{a_{n}^{*}}-W_{\mu}\sqrt{\frac{\log(e+2^{p -2})}{2^{p-2}}}-W_{\epsilon}\frac{\log(e+p)}{2^{p-2}}\;,\]

\[\forall a\in S_{n}^{p}\setminus\{a_{n}^{*}\},\;\;\;I_{k_{n,a},a}\leq\mu_{a}+W _{\mu}\sqrt{\frac{\log(e+2^{p-2})}{2^{p-2}}}+W_{\epsilon}\frac{\log(e+p)}{2^{p -2}}+\sqrt{\frac{p}{2^{p-2}}}+\frac{p}{\epsilon 2^{p-2}}\;,\]

where we used Lemma 3 and the fact that \(x\to\log(e+x)/x\) and \(x\to x2^{2-x}\) are decreasing function for \(x\geq 2\). Let \(p_{3}=\lceil\log_{2}X_{3}\rceil+2\) and \(p_{4}=\lceil\log_{2}X_{4}\rceil+2\) with

\[X_{3}=\sup\Big{\{}x>1\ |\ \ x\leq 64\overline{\Delta}_{\min}^{-1}(\log_{2}x+ 2)\Big{\}}\leq h_{1}(64\overline{\Delta}_{\min}^{-2}/\log 2,\ 128\overline{ \Delta}_{\min}^{-2})\;,\]

\[X_{4}=\sup\Big{\{}x>1\ |\ \ x\leq 8\epsilon^{-1}\overline{\Delta}_{\min}^{-1}( \log_{2}x+2)\Big{\}}\leq h_{1}(8\epsilon^{-1}\overline{\Delta}_{\min}^{-1}/ \log 2,\ 16\overline{\Delta}_{\min}^{-1}\epsilon^{-1})\;,\]

where we used Lemma 9, and \(h_{1}\) defined therein. We highlight that \((p_{3},p_{4})\) are deterministic values, hence their expectation is finite. Then, for all \(p\in\mathbb{N}\) such that \(p\geq p_{0}=\max\{p_{1},p_{2},p_{3},p_{4}\}+1\) and all \(n\in\mathbb{N}\) such that \(S_{n}^{p}\neq\emptyset\), we have \(I_{k_{n,a_{n}^{*}},a_{n}^{*}}\geq\mu_{a_{n}^{*}}-\overline{\Delta}_{\min}/4\) and \(I_{k_{n,a},a}\leq\mu_{a}+\overline{\Delta}_{\min}/2\) for all \(a\in S_{n}^{p}\setminus\{a_{n}^{*}\}\), hence \(a_{n}^{*}=B_{n}\) since we have \(B_{n}=\arg\max_{a\in[K]}I_{k_{n,a},a}\).

Since we have \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{0})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\), this concludes the proof. 

**Lemma 11 shows that the transportation costs between the sampled enough arms with largest true means and the other sampled enough arms are increasing fast enough.**

**Lemma 11**.: _Let \(S_{n}^{p}\) and \(a_{n}^{*}\) are as in Equation (27). There exists \(p_{1}\) with \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{1})]<+\infty\) for all \(\alpha>0\) such that if \(p\geq p_{1}\), for all \(n\) such that \(S_{n}^{p}\neq\emptyset\), for all \(b\in S_{n}^{p}\setminus\{a_{n}^{*}\}\), we have_

\[\frac{\tilde{\mu}_{k_{n,a_{n}^{*}},a_{n}^{*}}-\tilde{\mu}_{k_{n,b},b}}{\sqrt{1 /\tilde{N}_{k_{n,a_{n}^{*}},a_{n}^{*}}+1/\tilde{N}_{k_{n,b},b}}}\geq 2^{p/2}C_{\mu}\;,\]

_where \(C_{\mu}>0\) is a problem dependent constant._

Proof.: Let \(p_{1}\) to be specified later. Let \(n\in\mathbb{N}\) such that \(S_{n}^{p}\neq\emptyset\), where \(S_{n}^{p}\) and \(a_{n}^{*}\) as in Equation (27). Let \((k_{n,a})_{a\in[K]}\) be the phases indices for all arms. Since \(N_{n,a}\geq 2^{p-1}\) for all \(a\in S_{n}^{p}\), we have \(k_{n,a}\geq p\) and \(\tilde{N}_{k_{n,a},a}\geq 2^{p-2}\) by using Lemma 3. Let \(\overline{\Delta}_{\min}=\min_{a\neq b}|\mu_{a}-\mu_{b}|\), which satisfies \(\overline{\Delta}_{\min}>0\) by assumption on the instance considered.

Using Lemma 7, for all \(b\in S_{n}^{p}\setminus\{a_{n}^{*}\}\), we obtain

\[\tilde{\mu}_{k_{n,a_{n}^{*}},a_{n}^{*}}-\tilde{\mu}_{k_{n,b},b}\geq\overline{ \Delta}_{\min}-W_{\mu}\sqrt{\frac{\log(e+2^{p-2})}{2^{p-4}}}-W_{\epsilon}\frac{ \log(e+p)}{2^{p-3}}\;.\]

Let \(p_{3}=\lceil\log_{2}((X_{3}-e)/4)\rceil+4\) and \(p_{2}=\lceil\log_{2}((X_{2}-e-3)\log 2+1)\rceil+3\) with

\[X_{3}=\sup\Big{\{}x>1\ |\ \ x\leq 64\overline{\Delta}_{\min}^{-2}W_{\mu}^{2}\log x +e\Big{\}}\leq h_{1}(64\overline{\Delta}_{\min}^{-2}W_{\mu}^{2},\,e)\;,\]

\[X_{2}=\sup\Big{\{}x>1\ |\ \ x\leq 4\overline{\Delta}_{\min}^{-1}W_{\epsilon}\log x +e+3-1/\log 2\Big{\}}\leq h_{1}(4\overline{\Delta}_{\min}^{-1}W_{\epsilon},\,5)\;,\]

where we used Lemma 9, and \(h_{1}\) defined therein. Then, for all \(p\in\mathbb{N}\) such that \(p\geq p_{1}=\max\{p_{3},p_{2}\}+1\) and all \(n\in\mathbb{N}\) such that \(S_{n}^{p}\neq\emptyset\), we have, for all \(b\in S_{n}^{p}\setminus\{a_{n}^{*}\}\),

\[\tilde{\mu}_{k_{n,a_{n}^{*}},a_{n}^{*}}-\tilde{\mu}_{k_{n,b},b}\geq\overline{ \Delta}_{\min}/2\;.\]

As in the proof of Lemma 10, we obtain that \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{1})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\).

Then, for all \(b\in S_{n}^{p}\setminus\{a_{n}^{*}\}\), we have

\[\frac{\tilde{\mu}_{k_{n,a_{n}^{*}},a_{n}^{*}}-\tilde{\mu}_{k_{n,b},b}}{\sqrt{1 /\tilde{N}_{k_{n,a_{n}^{*}},a_{n}^{*}}+1/\tilde{N}_{k_{n,b},b}}}\geq 2^{p/2}\frac{ \overline{\Delta}_{\min}}{2^{5/2}}\;,\]

where we used that \(\min\{\tilde{N}_{k_{n,a_{n}^{*}},\tilde{N}_{k_{n,b},b},b}\}\geq 2^{p-2}\). Setting \(C_{\mu}=\overline{\Delta}_{\min}/2^{5/2}\) yields the result.

**Lemma 12 shows that the transportation costs between sampled enough arms and undersampled arms are not increasing too fast.**

**Lemma 12**.: _Let \(S_{n}^{p}\) be as in Equation (27). For all \(p\geq 1\) and all \(n\) such that \(S_{n}^{p}\neq\emptyset\)_

\[\forall a\in S_{n}^{p},\forall b\notin S_{n}^{p},\quad\frac{\tilde{\mu}_{k_{n,a},a}-\tilde{\mu}_{k_{n,b},b}}{\sqrt{1/\tilde{N}_{k_{n,a},a}+1/\tilde{N}_{k_{ n,b}}}}\leq 2^{p/2}D_{\mu}+2W_{\mu}\sqrt{\log(e+2^{p-2})}+2W_{\epsilon}\log(e+p),\]

_where \(D_{\mu}>0\) is a problem dependent constant and \((W_{\mu},W_{\epsilon})\) are the random variables defined in Lemma 7._

Proof.: Let \(p\geq 1\). Let \(n\in\mathbb{N}\) such that \(S_{n}^{p}\neq\emptyset\), where \(S_{n}^{p}\) as in Equation (27). Let \((k_{n,a})_{a\in[K]}\) be the phases indices for all arms. Since \(N_{n,a}\geq 2^{p-1}\) for all \(a\in S_{n}^{p}\), we have \(k_{n,a}\geq p\) and \(\tilde{N}_{k_{n,a},a}\geq 2^{p-2}\) by using Lemma 3. Likewise, \(N_{n,a}<2^{p-1}\) for all \(a\notin S_{n}^{p}\), we have \(k_{n,a}<p\) and \(\tilde{N}_{k_{n,a},a}<2^{p-2}\). Let \(\overline{\Delta}_{\max}=\min_{a\neq b}|\mu_{a}-\mu_{b}|\), which satisfies \(\overline{\Delta}_{\max}>0\) by assumption on the instance considered. Using Lemma 7, for all \(a\in S_{n}^{p}\) and \(b\notin S_{n}^{p}\), we obtain

\[\frac{\tilde{\mu}_{k_{n,a},a}-\tilde{\mu}_{k_{n,b},b}}{\sqrt{1/ \tilde{N}_{k_{n,a},a}+1/\tilde{N}_{k_{n,b},b}}} \leq\sqrt{\tilde{N}_{k_{n,b},b}}(\tilde{\mu}_{k_{n,a},a}-\tilde{ \mu}_{k_{n,b},b})\] \[\leq\sqrt{\tilde{N}_{k_{n,b},b}}(\mu_{a}-\mu_{b})+2W_{\mu}\sqrt{ \log(e+\tilde{N}_{k_{n,b},b})}+2W_{\epsilon}\frac{\log(e+k_{n,b})}{\sqrt{ \tilde{N}_{k_{n,b},b}}}\] \[\leq 2^{(p-2)/2}\overline{\Delta}_{\max}+2W_{\mu}\sqrt{\log(e+2^{p -2})}+2W_{\epsilon}\log(e+p)\]

where we used that \(\tilde{N}_{k_{n,b},b}\geq 1\), \(k_{n,b}<p\), \(\tilde{N}_{k_{n,b},b}<2^{p-2}\leq\tilde{N}_{k_{n,a},a}\) and \(x\to\log(e+x)/x\) is decreasing. Taking \(D_{\mu}=\overline{\Delta}_{\max}/2\) yields the result. 

**Lemma 13 shows that the challenger is mildly undersampled if the leader is not mildly undersampled.**

**Lemma 13**.: _Let \(V_{n}^{p}\) be as in Equation (28). There exists \(p_{2}\) with \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{2})]<+\infty\) for all \(\alpha>0\) such that if \(p\geq p_{2}\), for all \(n\) such that \(U_{n}^{p}\neq\emptyset\), \(B_{n}\notin V_{n}^{p}\) implies \(C_{n}\in V_{n}^{p}\)._

Proof.: Let \(p_{2}\) to be specified later. Let \(p\geq p_{2}\). Let \(n\in\mathbb{N}\) such that \(U_{n}^{p}\neq\emptyset\) and \(V_{n}^{p}\neq[K]\), where \(U_{n}^{p}\subseteq V_{n}^{p}\) are defined in Equation (28). In the following, we suppose that \(B_{n}\notin V_{n}^{p}\).

Let \((k_{n,a})_{a\in[K]}\) be the phases indices for all arms. Let \(p_{0}\) as in Lemma 10. Let \(b_{n}^{*}=\arg\max_{b\notin V_{n}^{p}}\mu_{b}\). Then, for all \(p\geq 4p_{0}/3-1/3\) and all \(n\) such that \(B_{n}\notin V_{n}^{p}\), Lemma 10 yields that \(B_{n}=b_{n}^{*}=\arg\max_{a\notin V_{n}^{p}}\tilde{\mu}_{k_{n,a},a}\).

Let \(p_{1}\) and \(C_{\mu}\) as in Lemma 11, and \(D_{\mu}\) as in Lemma 12. Then, for all \(p\geq 4\max\{p_{0},p_{1}\}/3-1/3\) and all \(n\) such that \(B_{n}\notin V_{n}^{p}\), we have \(B_{n}=b_{n}^{*}\) and

\[\forall b\notin V_{n}^{p},\quad\frac{\tilde{\mu}_{k_{n,a_{n}^{*}},b_{n}^{*}}-\tilde{\mu}_{k_{n,b},b}}{\sqrt{1/\tilde{N}_{k_{n,a_{n}^{*}},b_{n}^{* }}+1/\tilde{N}_{k_{n,b},b}}}\geq 2^{(3p+1)/8}C_{\mu}\;,\] \[\forall b\in U_{n}^{p},\quad\frac{\tilde{\mu}_{k_{n,a_{n}^{*}},b_{n}^{*}}-\tilde{\mu}_{k_{n,b},b}}{\sqrt{1/\tilde{N}_{k_{n,a_{n}^{*}},b_{n}^{* }}+1/\tilde{N}_{k_{n,b},b}}}\leq 2^{(p+1)/4}D_{\mu}+2W_{\mu}\sqrt{\log(e+2^{(p+1)/2-2})}\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad+2W_{ \epsilon}\log(e+(p+1)/2)\;,\]

where we used Lemmas 11 and 12. Let \(p_{3}=16[\log_{2}(2D_{\mu}/C_{\mu})]+1\), then we have \(2^{(p-1)/16}>\frac{D_{\mu}}{C_{\mu}}\) for all \(p\geq p_{3}\). Let \(p_{4}=\frac{16}{9}[\log_{2}X_{4}]+25\) and \(p_{5}=\frac{32}{9}[\log_{2}X_{5}]+7\) where

\[X_{4} =\sup\left\{x>1\mid x\leq\frac{W_{\mu}^{2}}{C_{\mu}^{2}}\log(e+x^ {8/9}2^{25/18-3/4})\right\}\;,\] \[X_{5} =\sup\left\{x>1\mid x\leq\frac{2W_{\epsilon}}{C_{\mu}}\log(e+4+32 \log_{2}(x)/18)\right\}\;.\]As in the proof of Lemma 10, using Lemma 7 yields that \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{4})]<+\infty\) and \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{5})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\). Let \(p_{2}=\max\{p_{3},p_{4},p_{5},4\max\{p_{0},p_{1}\}/3-1/3\}+1\). Then, we have shown that for all \(p\geq p_{2}\), for all \(n\) such that \(B_{n}\notin V_{n}^{p}\), we have \(B_{n}=b_{n}^{*}\) and

\[\min_{b\notin V_{n}^{p}}\frac{\tilde{\mu}_{k_{n,b_{n}^{*}},b_{n}^{*}}-\tilde{ \mu}_{k_{n,b}}b}{\sqrt{1/\tilde{N}_{k_{n,b_{n}^{*}},b_{n}^{*}}+1/\tilde{N}_{k_ {n,b},b}}}>\max_{b\in U_{n}^{p}}\frac{\tilde{\mu}_{k_{n,b_{n}^{*}},b_{n}^{*}}- \tilde{\mu}_{k_{n,b}}b}{\sqrt{1/\tilde{N}_{k_{n,b_{n}^{*}},b_{n}^{*}}+1/\tilde{ N}_{k_{n,b},b}}}\;,\]

Therefore, by definition of the TC challenger \(C_{n}=\arg\min_{b\notin b_{n}^{*}}\frac{\tilde{\mu}_{k_{n,b_{n}^{*}},b_{n}^{*} }-\tilde{\mu}_{k_{n,b}^{*}}}{\sqrt{1/\tilde{N}_{k_{n,b_{n}^{*}},b_{n}^{*}}+1/ \tilde{N}_{k_{n,b},b}}}\), we obtain that \(C_{n}\in V_{n}^{p}\). Otherwise, there would be a contradiction given that we assumed that \(U_{n}^{p}\neq\emptyset\). Given all the condition exhibited above, it is direct to see that \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{2})]<+\infty\) for all \(\alpha>0\). This concludes the proof. 

**Lemma 14 shows that all the arms are sufficient explored for large enough \(n\).**

**Lemma 14**.: _There exists \(N_{0}\) with \(\mathbb{E}_{\bm{\nu}}[N_{0}]<+\infty\) such that for all \(n\geq N_{0}\) and all \(a\in[K]\),_

\[N_{n,a}\geq\sqrt{n/K}\quad\text{and}\quad k_{n,a}\geq\frac{\log(n/K)}{2\log 2 }+1\;.\]

Proof.: Let \(p_{0}\) and \(p_{2}\) as in Lemmas 10 and 13. Combining Lemmas 10 and 13 yields that, for all \(p\geq p_{3}=\max\{p_{2},4p_{0}/3-1/3\}\) and all \(n\) such that \(U_{n}^{p}\neq\emptyset\), we have \(B_{n}\in V_{n}^{p}\) or \(C_{n}\in V_{n}^{p}\). We have \(\mathbb{E}_{\bm{\nu}}[2^{p_{2}}]<+\infty\). We have \(2^{p-1}\geq K2^{3(p-1)/4}\) for all \(p\geq p_{4}=4\lceil\log_{2}K\rceil+1\). Let \(p\geq\max\{p_{3},p_{4}\}\).

Suppose towards contradiction that \(U_{K2^{p-1}}^{p}\) is not empty. Then, for any \(1\leq t\leq K2^{p-1}\), \(U_{t}^{p}\) and \(V_{t}^{p}\) are non empty as well. Using the pigeonhole principle, there exists some \(a\in[K]\) such that \(N_{2^{p-1},a}\geq 2^{3(p-1)/4}\). Thus, we have \(\left|V_{2^{p-1}}^{p}\right|\leq K-1\). Our goal is to show that \(|V_{2^{p}}^{p}|\leq K-2\). A sufficient condition is that one arm in \(V_{2^{p-1}}^{p}\) is pulled at least \(2^{3(p-1)/4}\) times between \(2^{p-1}\) and \(2^{p}-1\).

**Case 1.** Suppose there exists \(a\in V_{2^{p-1}}^{p}\) such that \(L_{2^{p},a}-L_{2^{p-1},a}\geq\frac{2^{3(p-1)/4}}{\beta}+3/(2\beta)\). Using Lemma 6, we obtain

\[N_{2^{p},a}^{a}-N_{2^{p-1},a}^{a}\geq\beta(L_{2^{p},a}-L_{2^{p-1},a})-3/2 \geq 2^{3(p-1)/4}\;,\]

hence \(a\) is sampled \(2^{3(p-1)/4}\) times between \(2^{p-1}\) and \(2^{p}-1\).

**Case 2.** Suppose that for all \(a\in V_{2^{p-1}}^{p}\), we have \(L_{2^{p},a}-L_{2^{p-1},a}<2^{3(p-1)/4}/\beta+3/(2\beta)\). Then,

\[\sum_{a\notin V_{2^{p-1}}^{p}}(L_{2^{p},a}-L_{2^{p-1},a})\geq 2^{p-1}-K\left(2^{3(p -1)/4}/\beta+3/(2\beta)\right)\]

Using Lemma 6, we obtain

\[\left|\sum_{a\notin V_{2^{p-1}}^{p}}(N_{2^{p},a}^{a}-N_{2^{p-1},a}^{a})-\beta \sum_{a\notin V_{2^{p-1}}^{p}}(L_{2^{p},a}-L_{2^{p-1},a})\right|\leq 3(K-1)/2\;.\]

Combining all the above, we obtain

\[\sum_{a\notin V_{2^{p-1}}^{p}}(L_{2^{p},a}-L_{2^{p-1},a})-\sum_{a \notin V_{2^{p-1}}^{p^{\prime}}}(N_{2^{p},a}^{a}-N_{2^{p-1},a}^{a})\] \[\geq(1-\beta)\sum_{a\notin V_{2^{p-1}}^{p}}(L_{2^{p},a}-L_{2^{p-1},a})-3(K-1)/2\] \[\geq(1-\beta)\left(2^{p-1}-K\left(2^{3(p-1)/4}/\beta+3/(2\beta) \right)\right)-3(K-1)/2\geq K2^{3(p-1)/4}\;,\]

where the last inequality is obtained for \(p\geq p_{5}\) with

\[p_{5}=\sup\left\{p\in\mathbb{N}\mid(1-\beta)\left(2^{p-1}-K\left(2^{3(p-1)/4}/ \beta+3/(2\beta)\right)\right)-3(K-1)/2<K2^{3(p-1)/4}\right\}.\]The LHS summation is exactly the number of times where an arm \(a\notin V_{2^{p-1}}^{p}\) was leader but wasn't sampled, hence

\[\sum_{t=2^{p-1}}^{2^{p-1}}\mathds{1}\left(B_{t}\notin V_{2^{p-1}}^{p},\;I_{t}=C_{ t}\right)\geq K2^{3(p-1)/4}\]

For any \(2^{p-1}\leq t\leq 2^{p}-1\), \(U_{t}^{p}\) is non-empty, hence we have \(B_{t}\notin V_{2^{p-1}}^{p}\) (hence \(B_{t}\notin V_{t}^{p}\)) implies \(C_{t}\in V_{t}^{p}\subseteq V_{2^{p-1}}^{p}\). Therefore, we have shown that

\[\sum_{t=2^{p-1}}^{2^{p-1}}\mathds{1}\left(I_{t}\in V_{2^{p-1}}^{p}\right)\geq \sum_{t=2^{p-1}}^{2^{p}-1}\mathds{1}\left(B_{t}\notin V_{2^{p-1}}^{p},\;I_{t}=C _{t}\right)\geq K2^{3(p-1)/4}\,.\]

Therefore, there is at least one arm in \(V_{2^{p-1}}^{p}\) that is sampled \(2^{3(p-1)/4}\) times between \(2^{p-1}\) and \(2^{p}-1\).

In summary, we have shown \(|V_{2^{p}}^{p}|\leq K-2\) for all \(p\geq p_{6}=\max\{p_{3},p_{4},p_{5}\}\). By induction, for any \(1\leq k\leq K\), we have \(\left|V_{k2^{p-1}}^{p}\right|\leq K-k\), and finally \(U_{K2^{p-1}}^{p}=\emptyset\) for all \(p\geq p_{6}\). Defining \(N_{0}=K2^{p_{6}-1}\), we have \(\mathbb{E}_{\bm{\nu}}[N_{0}]<+\infty\) by using Lemmas 10 and 13 for \(p_{3}=\max\{p_{2},4p_{0}/3-1/3\}\) and \(p_{4}\) and \(p_{5}\) are deterministic. For all \(n\geq N_{0}\), we let \(2^{p-1}=\frac{n}{K}\). Then, by applying the above, we have \(U_{K2^{p-1}}^{p}=U_{n}^{\log_{2}(n/K)+1}\) is empty, which shows that \(N_{n,a}\geq\sqrt{n/K}\) for all \(a\in[K]\). Using Lemma 3, we obtain that \(k_{n,a}\geq\frac{\log(n/K)}{2\log 2}+1\) for all \(a\in[K]\). This concludes the proof. 

### Convergence towards \(\beta\)-optimal allocation

The second step of in the generic analysis of Top Two algorithms [12] is to show that AdaP-TT ensures convergence of its empirical proportions towards the \(\beta\)-optimal allocation. First, we show that the leader coincides with the best arm. Hence, the tracking procedure will ensure that the empirical proportion of time we sample it is exactly \(\beta\). Second, we show that a sub-optimal arm whose empirical proportion overshoots its \(\beta\)-optimal allocation will not be sampled next as challenger. Therefore, this "overshoots implies not sampled" mechanism will ensure the convergence towards the \(\beta\)-optimal allocation. We emphasise that there are multiple ways to select the leader/challenger pair in order to ensure convergence towards the \(\beta\)-optimal allocation. Therefore, while we conduct the proof for AdaP-TT, other choices of leader/challenger pair would yield similar results. Note that our results heavily rely on having obtained sufficient exploration first.

Convergence for the best arm.Lemma 15 exhibits a random phase which ensures that the leader and the candidate answer are equal to the best arm for large enough \(n\).

Lemma 15.: _Let \(N_{0}\) be as in Lemma 14. There exists \(N_{1}\geq N_{0}\) with \(\mathbb{E}_{\bm{\nu}}[N_{1}]<+\infty\) such that, for all \(n\geq N_{1}\), we have \(\hat{a}_{n}=B_{n}=a^{\star}\)._

Proof.: Let \(k\geq 1\) and \((T_{k}(a))_{a\in[K]}\) as in Equation (10). Suppose that \(\mathbb{E}_{\bm{\nu}}[\max_{a\in[K]}T_{k}(a)]<+\infty\). Then, Lemma 3 yields that \(N_{T_{k}(a),a}=2^{k-1}\) and \(\tilde{N}_{k,a}=2^{k-2}\). Using Lemma 7, we obtain that

\[\tilde{\mu}_{k,a^{\star}}\geq\mu_{a^{\star}}-W_{\mu}\sqrt{\frac{ \log(e+2^{k-2})}{2^{k-2}}}-W_{\epsilon}\frac{\log(e+k)}{2^{k-2}}\,,\] \[\forall a\neq a^{\star},\;\;\;\tilde{\mu}_{k,a}\leq\mu_{a}+W_{\mu }\sqrt{\frac{\log(e+2^{k-2})}{2^{k-2}}}+W_{\epsilon}\frac{\log(e+k)}{2^{k-2}}\,.\]

Let \(p_{1}=\lceil\log_{2}(X_{1}-e)\rceil+2\) and \(p_{2}=\lceil\log_{2}(X_{2}-e-1)\rceil+2\) with

\[X_{1} =\sup\left\{x>1\;|\;\;x\leq 64\Delta_{\min}^{-2}W_{\mu}^{2}\log x+e \right\}\leq h_{1}(64\Delta_{\min}^{-2}W_{\mu}^{2},\;e)\,,\] \[X_{2} =\sup\left\{x>1\;|\;\;x\leq 8\Delta_{\min}^{-1}W_{\epsilon}\log x+e +1\right\}\leq h_{1}(8\Delta_{\min}^{-1}W_{\epsilon},\;e+1)\,,\] \[X_{2} \geq\sup\left\{x>1\;|\;\;x\leq 8\Delta_{\min}^{-1}W_{\epsilon}\log(e+ 2+\log x)\right\}\,,\]

where we used Lemma 9, and \(h_{1}\) defined therein. Then, for all \(k\in\mathbb{N}^{K}\) such that \(\min_{a\in[K]}k_{a}>p_{0}=\max\{p_{1},p_{2}\}\) such that \(\mathbb{E}_{\bm{\nu}}[\max_{a\in[K]}T_{k_{a}}(a)]<+\infty\), we have \(\tilde{\mu}_{k,a^{\star}}\geq\mu_{a^{\star}}-\Delta_{\min}/4\) and \(\tilde{\mu}_{k,a}\leq\mu_{a}+\Delta_{\min}/4\) for all \(a\neq a^{\star}\), hence \(a^{\star}=\arg\max_{a\in[K]}\tilde{\mu}_{k,a}\). We have, for all \(\alpha\in\mathbb{R}_{+}\),

\[\exp(\alpha p_{1})\leq e^{3\alpha}(X_{1}-e)^{\alpha/\log 2}\;\;\;\text{hence}\;\; \;\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{1})]<+\infty\,,\]where we used Lemma 7 and \(h_{1}(x,e)\sim_{x\to+\infty}x\log x\) to obtain that \(\exp(\alpha p_{1})\) is at most polynomial in \(W_{\mu}\). Likewise, we obtain that \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{2})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\). Therefore, we have \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{0})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\).

Let us define the UCB indices by \(I_{k,a}=\tilde{\mu}_{k,a}+\sqrt{k/\tilde{N}_{k,a}}+k/(\epsilon\tilde{N}_{k,a})\). Using the above, we have

\[I_{k,a^{\star}} \geq\mu_{a^{\star}}-W_{\mu}\sqrt{\frac{\log(e+2^{k-2})}{2^{k-2}}} -W_{\epsilon}\frac{\log(e+k)}{2^{k-2}}+\frac{k}{\epsilon 2^{k-2}}\;,\] \[\forall a\neq a^{\star}, I_{k,a} \leq\mu_{a}+W_{\mu}\sqrt{\frac{\log(e+2^{k-2})}{2^{k-2}}}+W_{ \epsilon}\frac{\log(e+k)}{2^{k-2}}+\frac{k}{\epsilon 2^{k-2}}\;.\]

Therefore, we have \(a^{\star}=\arg\max_{a\in[K]}I_{k,a}\) for all \(k\in\mathbb{N}^{K}\) such that \(\min_{a\in[K]}k_{a}>\max\{p_{1},p_{2}\}\) such that \(\mathbb{E}_{\bm{\nu}}[\max_{a\in[K]}T_{k_{a}}(a)]<+\infty\).

Let \(N_{0}\) as in Lemma 14. Using Lemma 14, we obtain that, for all \(n\geq N_{0}\) and all \(a\in[K]\), \(k_{n,a}\geq\log_{2}(n/K)/2+1\). Therefore, we obtain \(\min_{a\in[K]}k_{n,a}>\max\{p_{1},p_{2}\}\) is implied by \(n\geq N_{1}=\max\{K4^{\max\{p_{1},p_{2}\}},N_{0}\}\). Using the above, we conclude that \(\mathbb{E}_{\bm{\nu}}[N_{1}]<+\infty\) and \(\hat{a}_{n}=B_{n}=a^{\star}\) for all \(n\geq N_{1}\). 

**Lemma 16**.: _Let \(\gamma>0\), and \(N_{1}\) be as in Lemma 15. There exists a deterministic constant \(C_{0}\geq 1\) such that, for all \(n\geq C_{0}N_{1}\),_

\[\left|\frac{N_{n,a^{\star}}}{n-1}-\beta\right|\leq\gamma\,.\]

Proof.: Let \(\gamma>0\). Let \(N_{1}\) as in Lemma 15. Let \(M\geq N_{1}\). Using Lemma 15, we obtain \(B_{n}=a^{\star}\) for all \(n\geq M\). Therefore, we obtain \(L_{n,a^{\star}}\geq n-M\) and \(\sum_{a\neq a^{\star}}N_{n,a^{\star}}^{a}\leq M\) for all \(n\geq M\). Using Lemma 6 yields that

\[\left|\frac{N_{n,a^{\star}}}{n-1}-\beta\right| \leq\frac{|N_{n,a^{\star}}^{a}-\beta L_{n,a^{\star}}|}{n-1}+\beta \left|\frac{L_{n,a^{\star}}}{n-1}-1\right|+\frac{1}{n-1}\sum_{a\neq a^{\star} }N_{n,a^{\star}}^{a}\] \[\leq\frac{1}{2(n-1)}+\beta\frac{2(M-1)}{n-1}\leq\gamma\,,\]

where the last inequality is obtained by taking \(n\geq\max\{M,(1/2+2\beta(M-1))/\gamma+1\}\). 

**Convergence for the sub-optimal arms Lemma 17 exhibits a random phase which ensures that if a sub-optimal arm overshoots its \(\beta\)-optimal allocation then it cannot be selected as challenger for large enough \(n\).**

**Lemma 17**.: _Let \(\gamma>0\). Let \(N_{1}\) and \(C_{0}\) be as in Lemma 15 and 16. There exists \(N_{2}\geq C_{0}N_{1}\) with \(\mathbb{E}_{\bm{\nu}}[N_{2}]<+\infty\) such that, for all \(n\geq N_{2}\),_

\[\exists a\neq a^{\star},\quad\frac{N_{n,a}}{n-1}\geq\omega_{\beta,a}^{\star} +\gamma\quad\implies\quad C_{n}\neq a\,.\]

Proof.: Let \(\gamma>0\) and \(\tilde{\gamma}>0\). Let \(N_{1}\) as in Lemma 15 and \(C_{0}\) as in Lemma 16 for \(\tilde{\gamma}\). Let \(n\geq C_{0}N_{1}\). Let \(a\neq a^{\star}\) such that \(\frac{N_{n,a}}{n-1}\geq\omega_{\beta,a}^{\star}+\gamma\). Suppose towards contradiction that \(\frac{N_{n,b}}{n-1}>\omega_{\beta,a}^{\star}\) for all \(b\notin\{a^{\star},a\}\). Then, for all \(n\geq C_{0}N_{1}\), we have

\[1-\beta+\tilde{\gamma}\geq 1-\frac{N_{n,a^{\star}}}{n-1}=\sum_{b\neq a^{ \star}}\frac{N_{n,b}}{n-1}>\gamma+\sum_{b\neq a^{\star}}\omega_{\beta,b}^{ \star}=1-\beta+\gamma\;,\]

which yields a contradiction for \(\tilde{\gamma}\leq\gamma\). Therefore, for all \(n\geq C_{0}N_{1}\), we have

\[\exists a\neq a^{\star},\quad\frac{N_{n,a}}{n-1}\geq\omega_{\beta,a}^{\star} +\gamma\quad\implies\quad\exists b\notin\{a^{\star},a\},\quad\frac{N_{n,b}}{ n-1}\leq\omega_{\beta,b}^{\star}\,.\]Then, we have

\[\sqrt{\frac{1+N_{n,a^{\star}}/N_{n,b}}{1+N_{n,a^{\star}}/N_{n,a}}}\geq\sqrt{\frac{ 1+(\beta-\tilde{\gamma})/\omega_{\beta,b}^{\star}}{1+(\beta+\tilde{\gamma})/( \omega_{\beta,a}^{\star}+\gamma)}}\;.\]

In the following, we use Lemma 7 and similar manipulations as in the proof of Lemma 15. Therefore, we obtain that, for all \(c\neq a^{\star}\),

\[\left|\tilde{\mu}_{k_{n,a^{\star}},a^{\star}}-\tilde{\mu}_{k_{n,c },c}-\Delta_{c}\right| \leq W_{\mu}\left(\sqrt{\frac{\log(e+2^{k_{n,a^{\star}}-2})}{2^{k_ {n,a^{\star}}-2}}}+\sqrt{\frac{\log(e+2^{k_{n,c}-2})}{2^{k_{n,c}-2}}}\right)\] \[\quad+W_{\epsilon}\left(\frac{\log(e+k_{n,a^{\star}})}{2^{k_{n,a^ {\star}}-2}}+\frac{\log(e+k_{n,c})}{2^{k_{n,c}-2}}\right)\;.\]

Let \(p_{3}=\lceil\log_{2}(X_{1}-e)\rceil+2\) and \(p_{2}=\lceil\log_{2}(X_{2}-e-1)\rceil+2\) with

\[X_{3} =\sup\left\{x>1\;|\;\;x\leq 16\eta^{-2}W_{\mu}^{2}\log x+e \right\}\leq h_{1}(16\eta^{-2}W_{\mu}^{2},\;e)\;,\] \[X_{2} =\sup\left\{x>1\;|\;\;x\leq 4\eta^{-1}W_{\epsilon}\log x+e+1 \right\}\leq h_{1}(4\eta^{-1}W_{\epsilon},\;e+1)\;,\] \[X_{2} \geq\sup\left\{x>1\;|\;\;x\leq 4\eta^{-1}W_{\epsilon}\log(e+2+ \log x)\right\}\;,\]

where we used Lemma 9, and \(h_{1}\) defined therein. We have, for all \(\alpha\in\mathbb{R}_{+}\),

\[\exp(\alpha p_{3})\leq e^{3\alpha}(X_{3}-e)^{\alpha/\log 2}\quad\text{hence} \quad\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{3})]<+\infty\;,\]

where we used Lemma 7 and \(h_{1}(x,e)\sim_{x\to+\infty}x\log x\) to obtain that \(\exp(\alpha p_{3})\) is at most polynomial in \(W_{\mu}\). Likewise, we obtain that \(\mathbb{E}_{\bm{\nu}}[\exp(\alpha p_{2})]<+\infty\) for all \(\alpha\in\mathbb{R}_{+}\).

Using Lemma 14 (with \(C_{0}N_{1}\geq N_{1}\geq N_{0}\)), we obtain that, for all \(n\geq C_{0}N_{1}\) and all \(a\in[K]\), \(k_{n,a}\geq\log_{2}(n/K)/2+1\). Therefore, we obtain \(\min_{a\in[K]}k_{n,a}>\max\{p_{2},p_{3}\}\) is implied by \(n\geq N_{2}=\max\{K4^{\max\{p_{3},p_{2}\}},C_{0}N_{1}\}\). Using the above, we conclude that \(\mathbb{E}_{\bm{\nu}}[N_{2}]<+\infty\) and \(\max_{c\neq a^{\star}}|\tilde{\mu}_{k_{n,a^{\star}},a^{\star}}-\tilde{\mu}_{k_ {n,c},c}-\Delta_{c}|\leq\eta\) for all \(n\geq N_{2}\).

Then, for all \(n\geq N_{2}\), we have \(B_{n}=a^{\star}\) and

\[\frac{\tilde{\mu}_{k_{n,a^{\star}},a^{\star}}-\tilde{\mu}_{k_{n,a},a}}{\tilde {\mu}_{k_{n,a^{\star}},a^{\star}}-\tilde{\mu}_{k_{n,b},b}}\sqrt{\frac{1+N_{n,a^ {\star}}/N_{n,b}}{1+N_{n,a^{\star}}/N_{n,a}}}\geq\frac{\Delta_{a}-\eta}{\Delta_ {b}+\eta}\sqrt{\frac{1+(\beta-\tilde{\gamma})/\omega_{\beta,b}^{\star}}{1+( \beta+\tilde{\gamma})/(\omega_{\beta,a}^{\star}+\gamma)}}>1\;,\]

where the last inequality is obtained by taking \(\eta\) and \(\tilde{\gamma}\) sufficiently small and by using Equation (25), i.e.

\[\frac{\Delta_{a}}{\Delta_{b}}\sqrt{\frac{1+\beta/\omega_{\beta,b}^{\star}}{1+ \beta/\omega_{\beta,a}^{\star}}}=1\;.\]

Therefore, we have shown that \(B_{n}=a^{\star}\) and

\[\frac{\tilde{\mu}_{k_{n,a^{\star}},a^{\star}}-\tilde{\mu}_{k_{n,a},a}}{\sqrt{1 /N_{n,a^{\star}}+1/N_{n,a}}}>\frac{\tilde{\mu}_{k_{n,a^{\star}},a^{\star}}- \tilde{\mu}_{k_{n,b},b}}{\sqrt{1/N_{n,a^{\star}}+1/N_{n,b}}}\quad\text{hence} \quad C_{n}\neq a\;.\]

This concludes the proof. 

**Lemma 18 shows that that the pulling proportion of the best arm converges towards \(\beta\) for large enough \(n\).**

**Lemma 18**.: _Let \(\gamma>0\) and \(T_{\bm{\mu},\gamma}\) be as in Equation (26). Then, we have \(\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}]<+\infty\)._

Proof.: Let \(\gamma>0\) and \(\tilde{\gamma}>0\). Let \(N_{2}\) as in Lemma 17 for \(\tilde{\gamma}\). Let \(M\geq N_{2}\). Using Lemmas 15, 16 and 17 for all \(n\geq M\), we obtain that \(B_{n}=a^{\star}\), \(\left|\frac{N_{n,a^{\star}}}{n-1}-\beta\right|\leq\tilde{\gamma}\) and

\[\exists a\neq a^{\star},\quad\frac{N_{n,a}}{n-1}\geq\omega_{\beta,a}^{\star}+ \tilde{\gamma}\quad\implies\quad C_{n}\neq a\;.\]

For all \(a\neq a^{\star}\), let us define \(t_{n,a}(\tilde{\gamma})=\max\left\{t\;|\;M\leq t\leq n,\;N_{t,a}/(n-1)<\omega_{ \beta,a}^{\star}+\tilde{\gamma}\right\}\). Since \(N_{t,a}/(n-1)\leq N_{t,a}/(t-1)\) for \(t\leq n\), we have

\[\frac{N_{n,a}}{n-1}\leq\frac{M-1}{n-1}+\frac{1}{n-1}\sum_{t=M}^{n}\mathds{1} \left(I_{t}=C_{t}=a\right)\]\[\leq\frac{M-1}{n-1}+\frac{1}{n-1}\sum_{t=M}^{n}\mathds{1}\left(\frac{N_{t,a}}{n-1 }<\omega_{\beta,a}^{\star}+\tilde{\gamma},\,I_{t}=C_{t}=a\right)\] \[\leq\frac{M-1}{n-1}+\frac{N_{t_{n,a}(\tilde{\gamma}),a}}{n-1}< \frac{M-1}{n-1}+\omega_{\beta,a}^{\star}+\tilde{\gamma}\,.\]

The second inequality uses Lemma 17, and the two last inequalities use the definition of \(t_{n,a}(\tilde{\gamma})\). Using that \(\sum_{a\in[K]}\frac{N_{n,a}}{n-1}=\sum_{a\in[K]}\omega_{\beta,a}^{\star}=1\), we obtain

\[\frac{N_{n,a}}{n-1}=1-\sum_{b\neq a}\frac{N_{n,a}}{n-1}\geq 1-\sum_{b\neq a} \left(\omega_{\beta,b}^{\star}+\tilde{\gamma}+\frac{M-1}{n-1}\right)=\omega_ {\beta,a}^{\star}-(K-1)\left(\tilde{\gamma}+\frac{M-1}{n-1}\right)\,.\]

Taking \(\tilde{\gamma}\leq\gamma/(2(K-1))\) and \(n\geq\max\{M,2(K-1)(M-1)/\gamma+1\}\) yields that

\[\left\|\frac{N_{n}}{n-1}-\omega_{\beta}^{\star}\right\|_{\infty}\leq\gamma\,.\]

Let \(T_{\bm{\mu},\gamma}\) as in 26. Then, we showed that \(T_{\bm{\mu},\gamma}\leq\max\{M,2(K-1)(M-1)/\gamma+1\}\). Therefore, we have

\[\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}]\leq\mathbb{E}_{\bm{\nu}}[\max\{M,2 (K-1)(M-1)/\gamma+1\}]<+\infty\,,\]

which concludes the proof. 

### Cost of doubling and forgetting

Compared to the generic analysis of Top Two algorithms [13], for AdaP-TT, we need to control the sample complexity cost of doubling and forgetting. Due to this reason, we have to pay a multiplicative four-factor: one two-factor due to doubling, and another two-factor due to forgetting. It is possible to show that this cost exists when adapting any "reasonable" BAI algorithm, meaning for any BAI algorithm in which the empirical proportions are converging towards an allocation \(\omega\) such that \(\min_{a}\omega_{a}>0\). Those BAI algorithms are "reasonable" because the asymptotic lower bound stipulates that all arms have to be sampled linearly in order to be near optimal.

Let \(\omega\in\Sigma_{K}\) be any allocation over arms such that \(\min_{a}\omega_{a}>0\). Let \(\gamma>0\). We denote by \(T_{\bm{\mu},\gamma}(\omega)\) the _convergence time_ towards \(\omega\), which is a random variable quantifying the number of samples required for the global empirical allocations \(N_{n}/(n-1)\) to be \(\gamma\)-close to \(\omega\) for any subsequent time, namely

\[T_{\bm{\mu},\gamma}(\omega)\triangleq\inf\left\{T\geq 1\mid\forall n\geq T, \,\left\|\frac{N_{n}}{n-1}-\omega\right\|_{\infty}\leq\gamma\right\}\,.\] (29)

**Lemma 19**: **shows that the phase switches of the arms happen in a round-robin fashion, which means that an arm switches phase for a second time after all other arms first switch their own phases.**

**Lemma 19**: **.** _Let \(\omega\in\Sigma_{K}\) such that \(\min_{a}\omega_{a}>0\). Assume that there exists \(\gamma_{\bm{\mu}}>0\) such that for \(\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}(\omega)]<+\infty\) for all \(\gamma\in(0,\gamma_{\bm{\mu}})\), where \(T_{\bm{\mu},\gamma}(\omega)\) is defined in Equation (29). Let \(\eta>0\). There exists \(\tilde{\gamma}_{\bm{\mu}}\in(0,\gamma_{\bm{\mu}})\) such that, for all \(\gamma\in(0,\tilde{\gamma}_{\bm{\mu}})\), there exists \(N_{3}\geq T_{\bm{\mu},\gamma}(\omega)\) with \(\mathbb{E}_{\bm{\nu}}[N_{3}]<+\infty\) which satisfies_

\[\forall n\geq N_{3},\quad\frac{\max_{a\in[K]}T_{k_{n,a}}(a)-1}{\min_{a\in[K]} T_{k_{n,a}}(a)-1}\leq 2+\eta\,.\]

_Proof._ Let \(\eta>0\). Let \(\tilde{\gamma}_{\bm{\mu}}\in(0,\gamma_{\bm{\mu}})\) such that \(2\max_{a\in[K]}(\omega_{a}+\gamma)/(\omega_{a}-\gamma)\leq 2+\eta\), which is possible since \(\min_{a}\omega_{a}>0\). Let \(\gamma\in(0,\tilde{\gamma}_{\bm{\mu}})\). By assumption, we have \(\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}(\omega)]<+\infty\). Then, for all \(n\geq T_{\bm{\mu},\gamma}(\omega)\),

\[\left\|\frac{N_{n}}{n-1}-\omega\right\|_{\infty}\leq\gamma\,.\]

Let \(M\geq T_{\bm{\mu},\gamma}(\omega)\). Let use denote by \(k_{M}=(k_{M,a})_{a\in[K]}\) the current phases for all arms \(a\in[K]\) at time \(M\). Then, for all \(n\geq M\) and all \(a\in[K]\), we have \(N_{n,a}\geq(n-1)(\omega_{a}-\gamma)\). Therefore, taking \(n\geq\max_{a\in[K]}2^{k_{M,a}}(\omega_{a}-\gamma)^{-1}+1\), we obtain that \(N_{n,a}\geq 2^{k_{M,a}}\) for all \(a\in[K]\), hence we have \(\max_{a\in[K]}T_{k_{M,a}+1}(a)\leq n\). Since \(\min_{a\in[K]}T_{k_{M,a}+1}(a)\geq M\), we have

\[\max_{a\in[K]}\left|\frac{N_{T_{k_{M,a}+1}(a),a}}{n-1}-\omega_{a}\right|\leq \gamma\,.\]

Likewise, taking \(n\geq\max_{a\in[K]}2^{k_{M,a}+1}(\omega_{a}-\gamma)^{-1}+1\), we obtain that \(N_{n,a}\geq 2^{k_{M,a}+1}\) for all \(a\in[K]\), hence we have \(\max_{a\in[K]}T_{k_{M,a}+2}(a)\leq n\). Let \(a_{1}=\arg\min_{a\in[K]}T_{k_{M,a}+2}(a)\). By definition and using Lemma 3, we have

\[2^{k_{M,a_{1}}+1}=N_{T_{k_{M,a_{1}}+2}(a_{1}),a_{1}}\leq(T_{k_{M,a_{1}}+2}(a_{1})-1)(\omega_{a_{1}}+\gamma)\,,\] \[\forall a\neq a_{1},\;\;\;2^{k_{M,a}}\leq N_{T_{k_{M,a_{1}}+2}(a_ {1}),a}\leq(T_{k_{M,a_{1}}+2}(a_{1})-1)(\omega_{a}+\gamma)\,.\]

Let \(a_{2}=\arg\max_{a\in[K]}T_{k_{M,a}+2}(a)\). By definition and using Lemma 3, we have

\[2^{k_{M,a_{2}}+1}=N_{T_{k_{M,a_{2}}+2}(a_{2}),a_{2}}\geq(T_{k_{M,a_{2}}+2}(a_{2})-1)(\omega_{a_{2}}-\gamma)\,,\]

Therefore, combining the above yields

\[(T_{k_{M,a_{2}}+2}(a_{2})-1)\leq(T_{k_{M,a_{1}}+2}(a_{1})-1)2\frac{\omega_{a_{ 2}}+\gamma}{\omega_{a_{2}}-\gamma}\leq(T_{k_{M,a_{2}}+2}(a_{2})-1)(2+\eta)\,,\]

where the last inequality uses that \(\gamma\in(0,\tilde{\gamma}_{\boldsymbol{\mu}})\) and \(\tilde{\gamma}_{\boldsymbol{\mu}}\in(0,\gamma_{\boldsymbol{\mu}})\) is such that \(2\max_{a\in[K]}(\omega_{a}+\gamma)/(\omega_{a}-\gamma)\leq 2+\eta\). We take \(n\geq N_{3}=\max_{a\in[K]}T_{k_{M,a}+2}(a)\), hence we have \(k_{n,a}\geq k_{M,a}+2\) for all \(a\in[K]\). Since \(\mathbb{E}_{\boldsymbol{\nu}}[T_{\boldsymbol{\mu},\gamma}(\omega)]<+\infty\) (i.e. arms are sampled linearly), it is direct to see that \(\mathbb{E}_{\boldsymbol{\nu}}[\max_{a\in[K]}T_{k_{M,a}+2}(a)]<+\infty\). This concludes the proof. 

Note that \(T_{\boldsymbol{\mu},\gamma}\) defined in 26 is such that \(T_{\boldsymbol{\mu},\gamma}=T_{\boldsymbol{\mu},\gamma}(\omega_{\mathrm{KL}, \beta}^{\mathsf{K}})\) where \(T_{\boldsymbol{\mu},\gamma}(\omega)\) as in Equation (29). Lemma 18 showed that \(\mathbb{E}_{\boldsymbol{\nu}}[T_{\boldsymbol{\mu},\gamma}]<+\infty\) for all \(\gamma>0\). Therefore, the condition of Lemma 19 are fulfilled by AdaP-TT.

### Asymptotic expected sample complexity

The final step of the generic analysis of Top Two algorithms [JDB\({}^{+}\)22] is to invert the private GLR stopping rule by leveraging the convergence of the empirical proportions towards the \(\beta\)-optimal allocation. Compared to the non-private GLR stopping rule, the private threshold in the private GLR stopping rule involves an additive term in \(\mathcal{O}(\log(1/\delta)^{2})\). This difference is the largest price that we pay to obtain a private BAI algorithm. We defer the reader to Appendix E.7 for a more detailed discussion on it.

Asymptotically \(\beta\)-optimal \(\epsilon\)-DP-FC-BAI algorithm.The inversion of the GLR stopping rule by leveraging the convergence of the empirical proportions towards the (\(\beta\)-)optimal allocation is a generic method used in the BAI literature. Provided this convergence is shown, it only depends on the threshold that ensures \(\delta\)-correctness. More precisely, it only depends on its asymptotic dependence in \(\log(1/\delta)\). In addition to the multiplicative four-factor, the price of privacy for asymptotically \(\beta\)-optimal BAI algorithms when combined with the non-private GLR stopping rule is a problem dependent multiplicative factor \(1+\sqrt{1+\Delta_{\max}^{2}/(2\epsilon^{2})}\) (Lemma 20).

**Lemma 20**.: _Let \((\delta,\beta)\in(0,1)^{2}\). Assume that there exists \(\gamma_{\boldsymbol{\mu}}>0\) such that for \(\mathbb{E}_{\boldsymbol{\nu}}[T_{\boldsymbol{\mu},\gamma}]<+\infty\) for all \(\gamma\in(0,\gamma_{\boldsymbol{\mu}})\), where \(T_{\boldsymbol{\mu},\gamma}\) is defined in Equation (26). Combining the private GLR stopping rule (Equation (16)) with private threshold (Equation (4)) yields a \(\delta\)-correct algorithm which satisfies that, for all \(\boldsymbol{\nu}\) with mean \(\boldsymbol{\mu}\) such that \(|a^{\star}(\boldsymbol{\mu})|=1\),_

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\nu}}\left[\tau_{\delta} \right]}{\log(1/\delta)}\leq 4T_{\mathrm{KL},\beta}^{\star}(\boldsymbol{\mu}) \left(1+\sqrt{1+\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}}\right)\,.\]

Proof.: Theorem 4 yields the \(\delta\)-correctness.

Let \(a^{\star}\) be the unique best arm, i.e. \(a^{\star}(\bm{\mu})=\{a^{\star}\}\). Let \(\zeta>0\). Using Equation (25) and the continuity of

\[(\bm{\mu},w)\mapsto\min_{a\neq a^{\star}(\bm{\mu})}\frac{(\mu_{a^{\star}(\bm{\mu })}-\mu_{a})^{2}}{2(1/w_{a^{\star}(\bm{\mu})}+1/w_{a})}\]

yields that there exists \(\gamma_{\zeta}>0\) such that \(\left\|\frac{N_{n}}{n-1}-\omega_{\beta}^{\star}\right\|_{\infty}\leq\gamma_{\zeta}\) and \(\max_{a\in[K]}|\tilde{\mu}_{k_{n,a}+1,a}-\mu_{a}|\leq\gamma_{\zeta}\) implies that

\[\forall a\neq a^{\star}, \frac{(\tilde{\mu}_{k_{n,a}\star+1,a^{\star}}-\tilde{\mu}_{k_{n,a }+1,a})^{2}}{(n-1)/N_{n,a^{\star}}+(n-1)/N_{n,a}}\geq\frac{2(1-\zeta)}{T_{ \mathrm{KL},\beta}^{\star}(\bm{\mu})}\] \[\frac{n-1}{N_{n,a^{\star}}}+\frac{n-1}{N_{n,a}}\leq\frac{\Delta_ {a}^{2}}{2}(1+\zeta)T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\:.\]

We choose such a \(\gamma_{\zeta}\). Let \(\gamma_{\bm{\mu}}>0\) be such that for \(\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}]<+\infty\) for all \(\gamma\in(0,\gamma_{\bm{\mu}})\), where \(T_{\bm{\mu},\gamma}\) is defined in Equation (26). Let \(\eta>0\). Let \(\tilde{\gamma}_{\bm{\mu}}\in(0,\gamma_{\bm{\mu}})\) as in Lemma 19 for this \(\eta\). In the following, let us consider \(\gamma\in(0,\min\{\tilde{\gamma}_{\bm{\mu}},\gamma_{\zeta},\beta/4,\Delta_{ \min}/4\})\).

Let \(N_{3}\geq T_{\bm{\mu},\gamma}\) with \(\mathbb{E}_{\bm{\nu}}[N_{3}]<+\infty\) as Lemma 19 for those \((\gamma,\eta)\). Then, we have \(\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}]<+\infty\) and

\[\forall n\geq N_{3},\quad\frac{\max_{a\in[K]}T_{k_{n,a}}(a)-1}{\min_{a\in[K]} T_{k_{n,a}}(a)-1}\leq 2+\eta\:.\]

Since arms are sampled linearly, it is direct to construct \(N_{4}\geq N_{3}\) with \(\mathbb{E}_{\bm{\nu}}[N_{4}]<+\infty\) such that, for all \(n\geq N_{4}\), we have \(\max_{a\in[K]}\max_{k\in\{k_{n,a},k_{n,a}+1\}}|\tilde{\mu}_{k,a}-\mu_{a}|\leq\gamma\), Therefore, we have \(\hat{a}_{n}=a^{\star}\).

Let \(\kappa\in(0,1)\). Let \(n\geq N_{4}/\kappa\) and \((k_{n,a})_{a\in[K]}\) be the current phases at time \(n\). Combining the above, we have \(\hat{a}_{n}=a^{\star}\) and

\[\max_{a\in[K]}|\tilde{\mu}_{k_{n,a}+1,a}-\mu_{a}|\leq\gamma\quad,\quad\left\| \frac{N_{n}}{n-1}-\omega_{\beta}^{\star}\right\|_{\infty}\leq\gamma\quad\text {and}\quad\frac{\max_{a\in[K]}T_{k_{n,a}}(a)-1}{\min_{a\in[K]}T_{k_{n,a}}(a)- 1}\leq 2+\eta\:.\]

Let \(a_{1}=\operatorname*{arg\,min}_{a\in[K]}T_{k_{n,a}}(a)\) and \(a_{2}=\operatorname*{arg\,max}_{a\in[K]}T_{k_{n,a}}(a)\). Therefore, we obtain, for all \(a\neq a^{\star}\),

\[\frac{(\tilde{\mu}_{k_{n,a}+1,\hat{a}_{n}}-\tilde{\mu}_{k_{n,a}+ 1,a})^{2}}{1/\tilde{N}_{k_{n,a}+1,\hat{a}_{n}}+1/\tilde{N}_{k_{n,a}+1,a}} =\frac{(\tilde{\mu}_{k_{n,a}+1,a^{\star}}-\tilde{\mu}_{k_{n,a}+1,a })^{2}}{1/N_{T_{k_{n,a}}(a^{\star}),a^{\star}}+1/N_{T_{k_{n,a}}(a),a}}\] \[\geq\frac{(\tilde{\mu}_{k_{n,a}+1,a^{\star}}-\tilde{\mu}_{k_{n,a }+1,a})^{2}}{1/N_{T_{k_{n,a_{1}}}(a_{1}),a^{\star}}+1/N_{T_{k_{n,a_{1}}}(a_{1}),a}}\] \[\geq(\min_{a\in[K]}T_{k_{n,a}}(a)-1)\frac{2(1-\zeta)}{T_{\mathrm{ KL},\beta}^{\star}(\bm{\mu})}\:.\]

Similarly, we can show that, for all \(a\neq a^{\star}\),

\[\frac{1}{\tilde{N}_{k_{n,a}\star+1,a^{\star}}}+\frac{1}{\tilde{N }_{k_{n,a}+1,a}} =\frac{1}{N_{T_{k_{n,a}\star}(a^{\star}),a^{\star}}}+\frac{1}{N_{T _{k_{n,a}}(a),a}}\] \[\leq\frac{1}{N_{T_{k_{n,a_{1}}}(a_{1}),a^{\star}}}+\frac{1}{N_{T_{ k_{n,a_{1}}}(a_{1}),a}}\] \[\leq\frac{1}{\min_{a\in[K]}T_{k_{n,a}}(a)-1}\frac{\Delta_{a}^{2}}{ 2}(1+\zeta)T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\] \[\leq\frac{1}{\min_{a\in[K]}T_{k_{n,a}}(a)-1}\frac{\Delta_{\max}^{ 2}}{2}(1+\zeta)T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\:.\]

Let \((c_{k})_{k\in\mathbb{N}}\) as in Equation (14). Using Lemma 3, we obtain, for all \(a\neq a^{\star}\),

\[2c_{(k_{n,a^{\star}}+1)(k_{n,a}+1)}(\tilde{N}_{k_{n,a^{\star}}+1,a^{\star}}, \tilde{N}_{k_{n,a}+1,a},\delta/2)\leq 8\log(4+(\max_{b\in[K]}k_{n,b}-1)\log 2)\]

\[+4\mathcal{C}_{G}\left(\log(1/\delta)/2+s\log(\max_{b\in[K]}k_{n,b}-1)+\log(2(K -1)\zeta(s)^{2})/2\right)\]Likewise, we obtain, for all \(a\in[K]\),

\[\frac{1}{\tilde{N}_{k_{n,a^{*}}+1,a^{*}}\epsilon^{2}}\log\left(\frac {2K(k_{n,a^{*}}+1)^{s}\zeta(s)}{\delta}\right)^{2}+\frac{1}{\tilde{N}_{k_{n,a}+1,a}\epsilon^{2}}\log\left(\frac{2K(k_{n,a}+1)^{s}\zeta(s)}{\delta}\right)^{2}\] \[\leq\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}\frac{(1+\zeta)T_{ \mathrm{KL},\beta}^{*}(\boldsymbol{\mu})}{\min_{b\in[K]}T_{k_{n,b}}(b)-1} \left(\log(1/\delta)+s\log(\max_{b\in[K]}k_{n,b}+1)+\log(2K\zeta(s))\right)^{2}\]

Let us denote by \(T_{k_{n}+1}^{+}=\max_{b\in[K]}T_{k_{n,b}+1}(b)\), \(T_{k_{n}+2}^{+}=\max_{b\in[K]}T_{k_{n,b}+2}(b)\), \(T_{k_{n}+1}^{-}=\min_{b\in[K]}T_{k_{n,b}+1}(b)\), \(T_{k_{n}}^{-}=\min_{b\in[K]}T_{k_{n,b}}(b)\). Let \(T\) be a time such that \(T\geq T_{k_{n}+1}^{+}\geq\kappa T\). Using Lemmas 3 and 19, we have

\[(k_{n,b}-1)\log 2=\log N_{T_{k_{n,b}}(b),b}\leq\log T_{k_{n,b}}(b)\leq\log T _{k_{n}}^{+}\leq\log T_{k_{n}}^{-}+\log(2+\eta)\:.\]

Using the private GLR stopping rule (Equation (16)), we have

\[\min\left\{\tau_{\delta},T\right\}-\kappa T\leq\sum_{T\geq T_{k_ {n}}^{+}\geq\kappa T}(T_{k_{n}+2}^{+}-T_{k_{n}+1}^{+})\mathds{1}\left(\tau_{ \delta}>T_{k_{n}+1}^{+}\right)\] \[\leq\sum_{T\geq T_{k_{n}}^{+}\geq\kappa T}(T_{k_{n}+2}^{+}-T_{k_ {n}+1}^{+})\mathds{1}\left(\exists a\neq a^{*},\:\frac{(\tilde{\mu}_{k_{n,a^{* }}+1,a^{*}}-\tilde{\mu}_{k_{n,a}+1,a})^{2}}{1/\tilde{N}_{k_{n,a^{*}}+1,a^{*}}+ 1/\tilde{N}_{k_{n,a}+1,a}}\right.\] \[\qquad\qquad<2c_{\epsilon,k_{n,a^{*}}+1,k_{n,a}+1}(\tilde{N}_{k_{ n,a^{*}}+1,a^{*}},\tilde{N}_{k_{n,a}+1,a},\delta)\right)\] \[\leq\sum_{T\geq T_{k_{n}}^{+}\geq\kappa T}(T_{k_{n}+2}^{+}-T_{k_ {n}+1}^{+})\mathds{1}\left((T_{k_{n}}^{-}-1)\frac{1-\zeta}{T_{\mathrm{KL}, \beta}^{*}(\boldsymbol{\mu})}<8\log(4+\log T_{k_{n}}^{-}+\log(2+\eta))\right.\] \[+\left.4C_{G}\left(\log(1/\delta)/2+s\log(2+\log_{2}T_{k_{n}}^{-} +\log_{2}(2+\eta))+\log(2(K-1)\zeta(s)^{2})/2\right)\right.\] \[\left.+\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}\frac{(1+\zeta)T_{ \mathrm{KL},\beta}^{*}(\boldsymbol{\mu})}{T_{k_{n}}^{-}-1}\left(\log(1/ \delta)+s\log(2+\log_{2}T_{k_{n}}^{-}+\log_{2}(2+\eta))+\log(2K\zeta(s)) \right)^{2}\right)\:,\]

Let \(T_{\zeta}(\delta)\) defined as the largest deterministic time such that the above condition is satisfied when replacing \(T_{k_{n}}^{-}\) by \((1-\kappa)T\). Let \(k_{\delta}\) be the largest random vector of phases such that that \(T_{k_{\delta}+1}^{+}\leq T_{\zeta}(\delta)\) almost surely, hence \(T_{k_{\delta}+2}^{+}>T_{\zeta}(\delta)\) almost surely. Then, using the above yields that \(\tau_{\delta}\leq T_{k_{\delta}+2}^{+}\) almost surely, hence

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\nu}}[\tau_{\delta}]}{\log(1/ \delta)}\leq\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\nu}}[T_{k_{ \delta}+2}^{+}]}{\log(1/\delta)}\leq(2+\eta)^{2}\limsup_{\delta\to 0}\frac{ \mathbb{E}_{\boldsymbol{\nu}}[T_{k_{\delta}+1}^{+}]}{\log(1/\delta)}\leq(2+ \eta)^{2}\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\nu}}[T_{k_{\delta}+1}^{+}]}{ \log(1/\delta)}\leq(2+\eta)^{2}\limsup_{\delta\to 0}\frac{T_{\zeta}(\delta)}{\log(1/ \delta)},\]

where the second inequality uses Lemma 19 twice, i.e. \(T_{k_{\delta}+2}^{+}\leq(2+\eta)T_{k_{\delta}+2}^{-}\leq(2+\eta)^{2}T_{k_{ \delta}+1}^{+}\), and the last one used the definition of \(k_{\delta}\) and that \(T_{\zeta}(\delta)\) is deterministic.

Since we are only interested in upper bounding \(\limsup_{\delta\to 0}\frac{T_{\zeta}(\delta)}{\log(1/\delta)}\), we can safely drop the second orders terms in \(T\) and \(\log(1/\delta)\). This allows us to remove the terms in \(\mathcal{O}(\log\log T)\) and in \(\mathcal{O}(\log\log(1/\delta))\). Using that \(\mathcal{C}_{G}(x)=x+\mathcal{O}(\log x)\), tedious manipulations yields that

\[\limsup_{\delta\to 0}\frac{T_{\zeta}(\delta)}{\log(1/\delta)}\leq\frac{T_{ \mathrm{KL},\beta}^{*}(\boldsymbol{\mu})}{1-\kappa}D_{\zeta}(\mu,\epsilon)\:,\]

where

\[D_{\zeta}(\mu,\epsilon)=\sup\left\{x\mid x^{2}<\frac{2}{1-\zeta}x+\frac{1+ \zeta}{1-\zeta}\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}\right\}\leq\frac{1}{1- \zeta}\left(1+\sqrt{1+(1-\zeta^{2})\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}} \right)\:.\]

The last inequality uses that \(x^{2}-2bx-c<0\) for all \(x\in[0,b(1+\sqrt{1+c/b^{2}}))\). Therefore, we have shown that

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\boldsymbol{\nu}}[\tau_{\delta}]}{\log(1/ \delta)}\leq(2+\eta)^{2}\frac{T_{\mathrm{KL},\beta}^{*}(\boldsymbol{\mu})}{(1- \kappa)(1-\zeta)}\left(1+\sqrt{1+(1-\zeta^{2})\frac{\Delta_{\max}^{2}}{2\epsilon^ {2}}}\right)\:.\]Letting \(\kappa\), \(\eta\) and \(\zeta\) goes to zero yields that

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{\delta}\right]}{\log(1/ \delta)}\leq 4T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\left(1+\sqrt{1+\frac{ \Delta_{\max}^{2}}{2\epsilon^{2}}}\right)\;.\]

Concluding the proof of Theorem 5.Combining Lemmas 14, 18, 19 and 20 concludes the proof of Theorem 5. We restrict the result to instances such that \(\min_{a\neq b}|\mu_{a}-\mu_{b}|>0\) in order for Lemma 14 to hold. Note that this is an artifact of the asymptotic proof which could be alleviated with more careful considerations.

Asymptotically optimal \(\epsilon\)-DP-FC-BAI algorithm.While Lemma 20 is derived for BAI algorithms converging towards the \(\beta\)-optimal allocation \(\omega_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\), it is direct to see that a similar inversion results can be obtained for BAI algorithms that converge towards the unique optimal allocation \(\omega_{\mathrm{KL}}^{\star}(\bm{\mu})=\{w^{\star}\}\) defined as

\[\omega_{\mathrm{KL}}^{\star}(\bm{\mu})\triangleq\operatorname*{arg\,max}_{ \omega\in\Sigma_{K}}\min_{a\neq a^{\star}}\frac{\Delta_{a}^{2}}{1/\omega_{a^{ \star}}+1/\omega_{a}}\;.\] (30)

At equilibrium, we have equality of the transportation costs (see [1] for example), namely

\[\forall a\neq a^{\star},\quad\frac{\Delta_{a}^{2}}{1/\omega_{a^{\star}}^{ \star}+1/\omega_{a}^{\star}}=2T_{\mathrm{KL}}^{\star}(\bm{\mu})^{-1}\;.\] (31)

In addition to the multiplicative four-factor, the price of privacy for asymptotically optimal BAI algorithms when combined with the non-private GLR stopping rule is a problem dependent multiplicative factor \(1+\sqrt{1+\Delta_{\max}^{2}/(2\epsilon^{2})}\) (Lemma 21). We omit the proof since it is the same as the one of Lemma 20.

**Lemma 21**.: _Let \(\delta\in(0,1)\) Assume that there exists \(\gamma_{\bm{\mu}}>0\) such that for \(\mathbb{E}_{\bm{\nu}}[T_{\bm{\mu},\gamma}(\omega^{\star})]<+\infty\) for all \(\gamma\in(0,\gamma_{\bm{\mu}})\), where \(T_{\bm{\mu},\gamma}(w)\) is defined in Equation (29) and \(\omega^{\star}\) is defined in Equation (30). Combining the private GLR stopping rule (Equation (16)) with private threshold (Equation (4)) yields a \(\delta\)-correct algorithm which satisfies that, for all \(\bm{\nu}\) with mean \(\mu\) such that \(|a^{\star}(\bm{\mu})|=1\),_

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{\delta}\right]}{ \log(1/\delta)}\leq 4T_{\mathrm{KL}}^{\star}(\bm{\mu})\left(1+\sqrt{1+\frac{ \Delta_{\max}^{2}}{2\epsilon^{2}}}\right)\;.\]

### Connection to the lower bound

In this section, we compare the sample complexity lower bound of Corollary 1 with the sample complexity upper bound of Theorem 5.

Simplification of the upper bound.The asymptotic expected sample complexity of AdaP-TT (Theorem 5) is upper bounded by

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{ \delta}\right]}{\log(1/\delta)} \leq 4T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\left(1+\sqrt{1+ \frac{\Delta_{\max}^{2}}{2\epsilon^{2}}}\right)\] \[\underset{(a)}{\leq}4T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\left( 2+\frac{\Delta_{\max}}{\sqrt{2}\epsilon}\right)\]

where \(T_{\mathrm{KL},\beta}^{\star}(\bm{\mu})\) is the \(\beta\)-characteristic time for Gaussian bandits, and (a) is due to the sub-additivity of the square root.

For \(\beta=1/2\), [11] showed that \(T_{\mathrm{KL},1/2}^{\star}(\bm{\mu})\leq 2T_{\mathrm{KL}}^{\star}(\bm{\mu})\).

On the other hand, [1] showed that \(H(\bm{\mu})\leq T_{\mathrm{KL}}^{\star}(\bm{\mu})\leq 2H(\bm{\mu})\), where \(H(\bm{\mu})\triangleq\sum_{a\in[K]}2\Delta_{a}^{-2}\) with \(\Delta_{a^{\star}}=\Delta_{\min}\).

Plugging these two inequalities in the upper bound of Theorem 5 with \(\beta=1/2\) gives that

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{\delta}\right]}{\log(1/ \delta)}\leq 8T_{\mathrm{KL},1/2}^{\star}(\bm{\mu})+16H(\bm{\mu})\frac{\Delta_{ \max}}{\sqrt{2\epsilon}}\]

Since we consider Bernoulli distributions, we know that \(0<\Delta_{\min}\leq\Delta_{\max}<1\). If we restrict ourselves to instances such that all the gaps have the same order of magnitude (Condition 1): there exists a constant \(C\geq 1\) such that \(\Delta_{\max}\leq C\Delta_{\min}\).

For such instances, we obtain

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{ \delta}\right]}{\log(1/\delta)} \leq 8T_{\mathrm{KL},1/2}^{\star}(\bm{\mu})+16H(\bm{\mu})\frac{C \Delta_{\min}}{\sqrt{2\epsilon}}\] \[\leq 8T_{\mathrm{KL},1/2}^{\star}(\bm{\mu})+16\sqrt{2}\frac{C}{ \epsilon}\left(\frac{1}{\Delta_{\min}}+\sum_{a=2}^{K}\frac{1}{\Delta_{a}}\right)\]

where the last inequality is due to \(H(\bm{\mu})\Delta_{\min}\leq\frac{2}{\Delta_{\min}}+\sum_{a=2}^{K}\frac{2}{ \Delta_{a}}\).

Finally using that \(a+b\leq 2\max(a,b)\), we get that

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{ \delta}\right]}{\log(1/\delta)}\leq c\max\left\{T_{\mathrm{KL},1/2}^{\star}( \bm{\mu}),\frac{C}{\epsilon}\left(\frac{1}{\Delta_{\min}}+\sum_{a=2}^{K}\frac{ 1}{\Delta_{a}}\right)\right\}\]

for the universal constant \(c=45.26\).

The lower bound for Bernoulli instances.For Bernoulli instances, Corollary 1 gives that the lower bound of the expected sample complexity of any \(\delta\)-correct \(\epsilon\)-global DP BAI strategy is

\[\limsup_{\delta\to 0}\frac{\mathbb{E}_{\bm{\nu}}\left[\tau_{ \delta}\right]}{\log(1/\delta)}\geq\max\left\{T_{\mathrm{KL}}^{\star}(\bm{ \nu}),\,\frac{1}{6\epsilon}\left(\frac{1}{\Delta_{\min}}+\sum_{a=2}^{K}\frac{ 1}{\Delta_{a}}\right)\right\}\;.\]

where we use Proposition 1 to replace \(T_{\mathrm{TV}}^{\star}(\bm{\nu})\) and \(T_{\mathrm{KL}}^{\star}(\bm{\nu})\) is the characteristic time for **Bernoulli bandits**.

Upper-lower bound discussion for the two privacy regimes.In the low privacy regime, our upper bound retrieves \(T_{\mathrm{KL},1/2}^{\star}(\bm{\mu})\) for Gaussian distributions. Since the rewards in the analysis are supposed Bernoulli, the mismatch from **exact** optimality is coming from the mismatch between the KL divergence of Bernoulli distributions and that of Gaussian, which is generally controllable in most instances where the means are far from the borders, i.e. \(0\), and \(1\). This is in essence, similar to the mismatch between UCB and KL-UCB in the regret-minimization literature (Chapter 10 in [10]). To overcome this mismatch, it is necessary to adapt the transportation costs to the family of distributions considered. In our setting, this can be done by using the Bernoulli KL rather than Gaussian KL in lines 12 and 16 of the algorithm. While the Top Two algorithms for Bernoulli distributions have been studied in [13], the analysis is more involved. Therefore, it would obfuscate where and how privacy is impacting the expected sample complexity.

In the high privacy regime, and for instances verifying Condition 1, our upper bound matches the lower bound \(\epsilon^{-1}T_{\mathrm{TV}}^{\star}(\bm{\nu})\) up to a constant. Having matching upper and lower bounds _only_ for high privacy regimes is an interesting phenomenon that appears in different settings of differential privacy literature, such as regret minimization [1], parameter estimation [12] and hidden probabilistic graphical models [14]. We speculate two facets of this phenomenon:

1. Explicit bounds in high and low privacy regimes: We have matching bounds only in the high or low privacy regimes because the lower bounds are generally harder to explicit and understand in transitional phases. Thus, it is harder to claim optimality in those phases.

2. Information-theoretic roots: There might be a more profound information-theoretic reason in relation to [14]. Indeed, there seems to be a link between the privacy budget \(\epsilon\) and the information thresholds introduced in [14]. Specifically, if the randomized mapping \(\mathcal{F}\) in [10] satisfies DP, then the noisy information threshold and noiseless information threshold can be similarly written as a function of \(\epsilon\) and the total variation. Finding a rigorous link between these two quantities is an interesting question to explore.

### Limitation and open problem

In the previous section (Appendix E.6), we argue that the upper and lower bounds match up to multiplicative constants in both privacy regimes, provided we restrict ourselves to instances satisfying Condition 1 (i.e. there exists \(C\geq 1\) such that \(\Delta_{\max}\leq C\Delta_{\min}\)).

While this holds for numerous instances, it does not account for instances in which the gaps have different orders of magnitude. One example would be the regime where \(\Delta_{\min}\to 0\) while \(\Delta_{\max}\) is fixed, hence yielding \(T^{\star}_{\mathrm{KL}}(\bm{\mu})\to+\infty\).

In Appendix E.5 (see Lemma 20 and 21), we show that combining the private GLR stopping rule (Equation (16)) with any BAI algorithms whose empirical proportions converge towards the \(\beta\)-optimal allocation \(\omega^{\star}_{\mathrm{KL},\beta}(\bm{\mu})\) for Gaussian bandits will incur a problem dependent multiplicative cost

\[1+\sqrt{1+\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}}\] (32)

in addition to the multiplicative four-factor due to doubling and forgetting.

In order to have matching upper and lower bounds, we would need to have \(\Delta_{\min}\) instead of \(\Delta_{\max}\) in Equation (32). Unfortunately, our results show that it is not possible when using the private GLR stopping rule (Equation (16)) and a sampling rule that is tailored to asymptotic (\(\beta\)-)optimality for Gaussian bandits. Therefore, this impossibility result holds for a large class of BAI sampling rules when adapting them to tackle private BAI by using the private GLR stopping rule (Equation (16)).

Origin of this limitation.The term \(\Delta_{\max}\) appears due to the private stopping threshold that ensures \(\delta\)-correctness of the private GLR stopping rule (Equation (16)).

For asymptotically (\(\beta\)-)optimal algorithms, the additive term due to privacy in the threshold is of the order

\[\frac{1}{\epsilon^{2}}\left(\frac{1}{N_{n,a}}+\frac{1}{N_{n,a^{\star}}}\right) \log(1/\delta)^{2}\approx_{n\to+\infty}\frac{T^{\star}_{\mathrm{KL}}(\bm{\mu}) }{n}\frac{\Delta_{a}^{2}}{2\epsilon^{2}}\log(1/\delta)^{2}\;.\]

Therefore, the private GLR stopping rule (Equation (16)) will stop when, for all \(a\neq a^{\star}\),

\[\frac{n}{T^{\star}_{\mathrm{KL}}(\bm{\mu})}\geq 2\log(1/\delta)+\frac{T^{ \star}_{\mathrm{KL}}(\bm{\mu})}{n}\frac{\Delta_{a}^{2}}{2\epsilon^{2}}\log(1/ \delta)^{2}\;,\]

which yields the problem-dependent multiplicative cost \(1+\sqrt{1+\frac{\Delta_{\max}^{2}}{2\epsilon^{2}}}\).

Open problem.This impossibility result is specific to the way we derive an \(\epsilon\)-DP version of the GLR stopping rule. Therefore, a natural question is whether it is possible to derive a better private GLR stopping rule to match the lower bound for all Bernoulli instances.

A two-phase algorithm.AdaP-TT tracks the non-private lower bound (i.e. \(T^{\star}_{KL}\)) as a non-private algorithm would do, and the additional cost tracking that privately is shown to be \(T^{\star}_{TV}/\epsilon\) (up to constants for bandits verifying Condition 1), where the additional cost comes from the added Laplace noise. Another approach would be to first perform a test to determine in which privacy regime the policy resides, and then track (privately) the corresponding characteristic time. Such an algorithm would empirically estimate both \(T^{\star}_{TV}\) and \(T^{\star}_{KL}\) in the first phase. If the privacy budget \(\epsilon\) is bigger than the empirical estimate of \(T^{\star}_{TV}/T^{\star}_{KL}\), then this means that we are in the low privacy regime and the algorithm tracks (privately) the KL characteristic time in the second phase (as AdaP-TT does). However, if the privacy budget \(\epsilon\) is smaller than the empirical estimate of \(T^{\star}_{TV}/T^{\star}_{KL}\), then it is the high privacy regime and the algorithm tracks (privately) the TV characteristic time in the second phase. For such a two-phase algorithm to achieve optimality, properly tuning the amount of time spent in its first phase is a crucial step. Whether it is possible to analyze such an algorithm and quantify the proper tuning (if it even exists) is an interesting direction for future work.

Non-asymptotic sample complexity of AdaP-TT.In the _non-private FC-BAI literature_ (i.e. \(\epsilon=+\infty\)), _there is no tight lower bound in the non-asymptotic regime (i.e. for any value of \(\delta\))_. This is the main open problem in FC-BAI, and hence in DP-FC-BAI too. In the class of asymptotically (\(\beta\)-)optimal algorithms, TTUCB [10] is one of the few to have non-asymptotic guarantees. Adapting

[MISSING_PAGE_FAIL:43]

Extended experimental analysis

In this section, we perform additional experiments to compare AdaP-TT and DP-SE for FC-BAI. We test the two algorithms in six bandit environments with Bernoulli distributions, as defined by [13], namely

\[\mu_{1} =(0.95,0.9,0.9,0.9,0.5), \mu_{2} =(0.75,0.7,0.7,0.7,0.7),\] \[\mu_{3} =(0,0.25,0.5,0.75,1), \mu_{4} =(0.75,0.625,0.5,0.375,0.25)\},\] \[\mu_{5} =(0.75,0.53125,0.375,0.28125,0.25), \mu_{6} =(0.75,0.71875,0.625,0.46875,0.25)\}.\]

For each Bernoulli instance, we implement the algorithms with

\[\epsilon\in\{0.001,0.005,0.01,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,10\},\]

and a risk level \(\delta=0.01\). We verify empirically that the algorithms are \(\delta\)-correct by running each algorithm \(100\) times. In Figure 2, we plot the evolution of the average stopping time and standard deviation with respect to the privacy budget \(\epsilon\). All the algorithms are implemented in Python (version \(3.8\)) and are tested with an 8-core 64-bits Intel i5@1.6 GHz CPU.

All the experiments validate the same conclusions as the ones reached in Section 5, i.e.

1. AdaP-TT requires fewer samples to provide a \(\delta\)-correct answer,
2. there exists two privacy regimes, and in the low-privacy regime, the sample complexity is independent of the privacy budget.

Figure 2: Evolution of the stopping time \(\tau\) (mean \(\pm\) std. over 100 runs) of AdaP-TT, DP-SE, and TTUCB with respect to the privacy budget \(\epsilon\) for \(\delta=10^{-2}\) on different Bernoulli instances. The shaded vertical line separates the two privacy regimes. AdaP-TT outperforms DP-SE.