# Distribution Learnability and Robustness+
Footnote â€ : Authors are listed in alphabetical order.

Shai Ben-David

University of Waterloo, Vector Institute

shai@uwaterloo.ca

&Alex Bie

University of Waterloo

yabie@uwaterloo.ca

Gautam Kamath

University of Waterloo, Vector Institute

g@csail.mit.edu

&Tosca Lechner

University of Waterloo

tlechner@uwaterloo.ca

###### Abstract

We examine the relationship between learnability and robust (or agnostic) learnability for the problem of distribution learning. We show that learnability of a distribution class implies robust learnability with only additive corruption, but not if there may be subtractive corruption. Thus, contrary to other learning settings (e.g., PAC learning of function classes), realizable learnability does not imply agnostic learnability. We also explore related implications in the context of compression schemes and differentially private learnability.

## 1 Introduction

Distribution learning (sometimes called _density estimation_) refers to the following statistical task: given i.i.d. samples from some (unknown) distribution \(p\), produce an estimate of \(p\). This is one of the most fundamental and well-studied questions in both statistics [1] and computer science [1], often equivalent to classic problems of parameter estimation (e.g., mean estimation) in parametric settings. It is easy to see that no learner can meaningfully approximate any given \(p\) without having some prior knowledge. The problem then becomes: assuming the sample generating distribution \(p\) belongs to a given class of distributions \(\mathcal{C}\), and given parameters \(\varepsilon,\delta\in(0,1)\), output some distribution \(\hat{p}\) such that with probability at least \(1-\delta\), the statistical distance between \(p\) and \(\hat{p}\) is at most \(\varepsilon\). Specifically, we employ _total variation distance_, the most studied metric in density estimation [1, 1], using \(d_{\mathrm{TV}}(p,q)\) to denote the distance between distributions \(p\) and \(q\). This case, when \(p\in\mathcal{C}\), is often called the _realizable_ setting. If, for some particular class \(\mathcal{C}\), this is doable with a finite number of samples \(n(\varepsilon,\delta)\), then we say the distribution class is _\((\varepsilon,\delta)\)-learnable_.1 A class is _learnable_ if it is _\((\varepsilon,\delta)\)-learnable_ for every \((\varepsilon,\delta)\in(0,1)^{2}\). A significant amount of work has focused on proving bounds on \(n(\varepsilon,\delta)\) for a number of classes \(\mathcal{C}\) of interest - for example, one can consider the class \(\mathcal{C}\) of all Gaussian distributions \(\mathcal{N}(\mu,\Sigma)\) in some Euclidean space \(\mathbb{R}^{d}\).

Footnote 1: For the sake of exposition, we defer formal definitions of our learnability notions to Section 1.1.

However, this framework is restrictive in the sense that it requires the unknown distribution to be _exactly_ a member of the class \(\mathcal{C}\) of interest. This may not be the case for a variety of possible reasons, including some innocuous and some malicious. As one example, while it is a common modelling assumption to posit that data comes from a Gaussian distribution, Nature rarely samples exactly from Gaussians, we consider this only to be a convenient _approximation_. More generally, the class \(\mathcal{C}\) that the learner assumes can be thought of as reflecting some prior knowledge about the task at hand. Such prior knowledge is almost always only an approximation of reality. Alternatively, we may be in an adversarial setting, where a malicious actor has the ability to modify an otherwise well-behaveddistribution, say by injecting datapoints of their own (known in the machine learning literature as data poisoning attacks [1, 2, 3], \(\mathrm{DKK}^{+}19\), \(\mathrm{GTX}^{+}20\), \(\mathrm{GFH}^{+}21\), \(\mathrm{LKY}23\)).

More formally, the classic problem of _agnostic_ learnability is generally described as follows: given a (known) class of distributions \(\mathcal{C}\), and a (finite) set of samples drawn i.i.d. from some (unknown) distribution \(p\), find a distribution \(\hat{p}\) whose statistical distance from \(p\) is not much more than that of the closest member of \(\mathcal{C}\). It is not hard to see that this is equivalent to a notion of _robust_ learnability, where the distribution \(p\) is not viewed as arbitrary, but instead an adversarial corruption of some distribution within \(\mathcal{C}\).2 Given their equivalence, throughout this work, we will use agnostic and robust learnability interchangeably.

Footnote 2: A related notion of robust learnability instead imagines the adversary modifies the _samples_ from a distribution in \(\mathcal{C}\), rather than the distribution itself. This _adaptive_ model is discussed further in Section 1.2.

The difference between a robust setting and the previous realizable one is that now, instead of assuming \(p\in\mathcal{C}\) and asking for an arbitrarily good approximation of \(p\), we make no prior assumption about the data-generating distribution and only ask to approximate as well as (or close to) what the best member of some "benchmark" class \(\mathcal{C}\) can do.

We address the following question: Assuming a class of distributions is learnable, under which notions of robustness is it guaranteed to be robustly learnable? We focus entirely on information-theoretic learnability, and eschew concerns of computational efficiency. Indeed, our question of interest is so broad that computationally efficient algorithms may be too much to hope for.

We shall consider a few variants of robust learnability. Specifically, we will impose requirements on the nature of the difference between the data-generating distribution \(p\) and members of the class \(\mathcal{C}\). Obviously, such requirements can only make the task of robust learning easier.

One such model considers _additive robustness_. The underlying distribution is restricted to be a mixture of a distribution \(p\) from \(\mathcal{C}\), and some 'contaminating' distribution \(q\). In this Statistics community, this celebrated model is known as _Huber's contamination model_[13]. Analogously, one can consider _subtractive robustness_. It includes the case where the starting point is a distribution in the class \(\mathcal{C}\), but a fraction of the probability mass is removed and samples are drawn from the resulting distribution (after rescaling). These two models are related to adversaries who can add or remove points from a sampled dataset, see discussion at the end of Section 1.2.

A significant line of work focuses on understanding the sample complexity of agnostic distribution learning (see examples and discussion in Section 1.3). Most study restricted classes of distributions, with analyses that are only applicable in certain classes. Some works have found quantitative separations between the different robustness models. For instance, in the specific case of Gaussian mean estimation, [1, 16, 17, 18] give strong evidence that efficient algorithms can achieve better error if they must only be additively robust, rather than robust in general. However, such findings are again restricted to specific cases, and say little about the overall relationship between learnability in general and these various robust learning models.

Current results leave open a more comprehensive treatment of robustness in distribution learning. Specifically, what is the relative power of these different robustness models, and what is their impact on which types of distributions are learnable? Are there more generic ways to design robust learning algorithms?

Our two main contributions are the following:

* We give a generic algorithm which converts a learner for a distribution class into a learner for that class which is robust to additive corruptions.
* We show that there exist distribution classes which are learnable, but are no longer learnable after subtractive corruption.

Stated succinctly: we show that learnability implies robust learnability when an adversary can make additive corruptions, but not subtractive corruptions. Other results explore implications related to compression schemes and differentially private learnability.

### Definitions of Learnability

In order to more precisely describe our results, we define various notions of learnability. We start with the standard notion of PAC learnability for a distribution class. We get samples from a distribution \(p\) belonging to a distribution class \(\mathcal{C}\), and the goal is to output a distribution similar to \(p\).

**Definition 1.1** (Learnability).: _We say that a class \(\mathcal{C}\) of probability distributions is learnable (or, realizably learnable) if there exists a learner \(A\) and a function \(n_{\mathcal{C}}:(0,1)^{2}\to\mathbb{N}\), such that for every probability distribution \(p\in\mathcal{C}\), and every \((\varepsilon,\delta)\in(0,1)^{2}\), for \(n\geq n_{\mathcal{C}}(\varepsilon,\delta)\) the probability over samples \(S\) of that size drawn i.i.d. from the distribution \(p\) that \(d_{\mathrm{TV}}(p,A(S))\leq\varepsilon\) is at least \(1-\delta\)._

We next introduce the more challenging setting of robust, or agnostic, learning. In this setting, the sampled distribution is within bounded distance to the distribution class \(\mathcal{C}\), rather than being in \(\mathcal{C}\) itself. For technical reasons, we introduce two closely-related definitions. Roughly speaking, the latter definition assumes the distance from the sampling distribution to \(\mathcal{C}\) is fixed, whereas the former (more commonly considered in the agnostic learning literature) doesn't. Note that in many cases, robust algorithms designed with knowledge of the distance \(\eta\) to \(\mathcal{C}\) can be modified to do without [13].

**Definition 1.2** (Robust learnability).:
1. _For_ \(\alpha>0\)_, we say that a class_ \(\mathcal{C}\) _of probability distributions is_ \(\alpha\)-robustly learnable _(also referred to as_ \(\alpha\)-agnostically learnable_) if there exists a learner_ \(A\) _and a function_ \(n_{\mathcal{C}}:(0,1)^{2}\to\mathbb{N}\)_, such that for every probability distribution_ \(p\)_, and_ \((\varepsilon,\delta)\in(0,1)^{2}\)_, for_ \(n\geq n_{\mathcal{C}}(\varepsilon,\delta)\) _the probability over samples_ \(S\) _of that size drawn i.i.d. from the distribution_ \(p\) _that_ \(d_{\mathrm{TV}}(p,A(S))\leq\alpha\min\{d_{\mathrm{TV}}(p,p^{\prime}):p^{\prime} \in\mathcal{C}\}+\varepsilon\) _is at least_ \(1-\delta\)_._ _When_ \(\alpha=1\) _we omit it and say that the class is robustly (or agnostically) learnable._
2. _For_ \(0\leq\eta\leq\) _and_ \(\alpha>0\)_, we say that a class_ \(\mathcal{C}\) _of probability distributions is_ \(\eta\)-\(\alpha\)-robustly learnable _if there exists a learner_ \(A\) _and a function_ \(n_{\mathcal{C}}:(0,1)^{2}\to\mathbb{N}\)_, such that for every probability distribution_ \(p\) _such that_ \(\min\{d_{\mathrm{TV}}(p,p^{\prime}):p^{\prime}\in\mathcal{C}\}\leq\eta\) _and_ \((\varepsilon,\delta)\in(0,1)^{2}\)_, for_ \(n\geq n_{\mathcal{C}}(\varepsilon,\delta)\) _the probability over samples_ \(S\) _of that size drawn i.i.d. from the distribution_ \(p\) _that_ \(d_{\mathrm{TV}}(p,A(S))\leq\alpha\eta+\varepsilon\) _is at least_ \(1-\delta\)_._

Finally, we introduce notions of robust learnability which correspond to only additive or subtractive deviations from the distribution class \(\mathcal{C}\). These more stringent requirements than standard (realizable) learnability, but more lenient than \(\eta\)-\(\alpha\)-robust learnability: the adversary in that setting can deviate from the distribution class \(\mathcal{C}\) with both additive and subtractive modifications simultaneously.

**Definition 1.3** (Additive robust learnability).: _Given parameters \(0\leq\eta\leq 1\) and \(\alpha>0\), we say that a class \(\mathcal{C}\) of probability distributions is \(\eta\)-additive\(\alpha\)-robustly learnable if there exists a learner \(A\) and a function \(n_{\mathcal{C}}:(0,1)^{2}\to\mathbb{N}\), such that for every probability distribution \(q\), every \(p\in\mathcal{C}\), and \((\varepsilon,\delta)\in(0,1)^{2}\), for \(n\geq n_{\mathcal{C}}(\varepsilon,\delta)\) the probability over samples \(S\) of that size drawn i.i.d. from the distribution \(\eta q+(1-\eta)p\), that \(d_{\mathrm{TV}}(A(S),p)\leq\alpha\eta+\varepsilon\) is at least \(1-\delta\)._

**Definition 1.4** (Subtractive robust learnability).: _Given parameters \(0\leq\eta\leq 1\) and \(\alpha>0\), we say that a class \(\mathcal{C}\) of probability distributions is \(\eta\)-subtractive\(\alpha\)-robustly learnable if there exists a learner \(A\) and a function \(n_{\mathcal{C}}:(0,1)^{2}\to\mathbb{N}\), such that for every probability distribution \(p\) for which there exists a probability distribution \(q\) such that \(\eta q+(1-\eta)p\in\mathcal{C}\), and for every \((\varepsilon,\delta)\in(0,1)^{2}\), for \(n\geq n_{\mathcal{C}}(\varepsilon,\delta)\) the probability over samples \(S\) of that size drawn i.i.d. from the distribution \(p\), that \(d_{\mathrm{TV}}(A(S),p)\leq\alpha\eta+\varepsilon\) is at least \(1-\delta\)._

### Results and Techniques

We explore how different robustness models affect learnability of distributions, showing strong separations between them. Our first main result shows that learnability implies additively robust learnability.

**Theorem 1.5**.: _Any class of probability distributions \(\mathcal{Q}\) which is realizably learnable, is also \(\eta\)-additively \(2\)-robustly learnable for every \(\eta\in(0,1/4)\)._

Note that, since additively robust learnability trivially implies learnability, this shows an _equivalence_ between learnability and additively robust learnability.

Our algorithm enumerates over all subsets of the dataset of an appropriate size, such that at least one subset contains no samples from the contaminating distribution. A realizable learner is applied toeach subset, and techniques from hypothesis selection [17, 15, 14, 16] are used to pick the best of the learned distributions. Further details appear in Section 2.

We also note that since our robust learning algorithm enumerates all large subsets of the training dataset, it is _not_ computationally efficient. Indeed, for such a broad characterization, this would be too much to ask. Efficient algorithms for robust learnability are an exciting and active field of study, but outside the scope of this work. For further discussion see Section 1.3.

Our other main result shows that a distribution class being learnable does _not_ imply that it is subtractive robustly learnable.

**Theorem 1.6**.: _For every \(\alpha>0\), there exists a class that is learnable, but not \(\eta\)-subtractively \(\alpha\)-robustly learnable for any \(0\leq\eta\leq\frac{1}{16\alpha}\)._

An immediate corollary is that learnability does _not_ imply robust (or agnostic) learnability, since this is a more demanding notion than subtractive robust learnability.

Our proof of this theorem proceeds by constructing a class of distributions that is learnable, but classes obtained by subtracting light-weight parts of these distributions are not \(\alpha\)-robustly learnable with respect to the original learnable class. More concretely, our construction works as follows. We start by considering a distribution class that, by itself, is not learnable with any finite number of samples. We map each distribution in that class to a new distribution, which additionally features a point with non-trivial mass that "encodes" the identity of the distribution, thus creating a new class of distributions which _is_ learnable. Subtractive contamination is then able to "erase" this point mass, leaving a learner with sample access only to the original (unlearnable) class. Our construction is inspired by the recent construction of Lechner and Ben-David [16], showing that the learnability of classes of probability distributions cannot be characterized by any notion of combinatorial dimension. For more details, see Section 3.

Thus far, we have only considered additive and subtractive robustness separately. General robustness, where probability mass can be both added _and_ removed, is more powerful than either model individually. However, if a class is additive robustly learnable _and_ subtractive robustly learnable, is it robustly learnable? Though this is intuitively true, we are not aware of an immediate proof. Using a similar argument as Theorem 1.5, we derive a stronger statement: that subtractively robust learnability implies robust learnability.

**Theorem 1.7**.: _If a class \(\mathcal{C}\) is \(\eta\)-subtractive \(\alpha\)-robustly learnable, then it is also \(\eta\)-\((2\alpha+4)\)-robustly learnable._

Adjacent to distribution learning is the notion of _sample compression schemes_. Recent work by Ashtiani, Ben-David, Harvey, Liaw, Mehrabian, and Plan [1] expanded notions of sample compression schemes to apply to the task of learning probability distributions. They showed that the existence of such sample compression schemes for a class of distributions imply the learnability of that class. While the existence of sample compression schemes for classification tasks imply the existence of such schemes for robust leaning, the question if similar implication hold for distribution learning schemes was not answered. We strongly refute this statement. We use a construction similar to that of Theorem 1.6, see Section 3.1 for more details.

**Theorem 1.8**.: _The existence of compression schemes for a class of probability distributions does not imply the existence of robust compression schemes for that class._

Finally, a natural question is whether other forms of learnability imply robust learnability. We investigate when _differentially private3_ (DP) learnability does or does not imply robust learnability. We find that the same results and separations as before hold when the distribution class is learnable under _approximate_ differential privacy (i.e., \((\varepsilon,\delta)\)-DP), but, perhaps surprisingly, under _pure_ differential privacy (i.e., \((\varepsilon,0)\)-DP), private learnability implies robust learnability for all considered adversaries.4

Footnote 3: Differential privacy is a popular and rigorous notion of data privacy. For the definition of differential privacy, see Section 4.

Footnote 4: In the context of differential privacy, we diverge slightly from the established notation. Specifically, we align ourselves with common notation in the DP literature, using \(\varepsilon\) and \(\delta\) for privacy parameters, and use \(\alpha\) (in place of \(\varepsilon\)) and \(\beta\) (in place of \(\delta\)) for accuracy parameters.

**Theorem 1.9** (Informal).: \((\varepsilon,0)\)_-DP learnability implies robust \((\varepsilon,0)\)-DP learnability. For any \(\delta>0\), \((\varepsilon,\delta)\)-DP learnability implies additively robust learnability, but not subtractively robust learnability._

[MISSING_PAGE_FAIL:5]

[14] for a survey. A number of these works have focused on connections between robustness and privacy [15, 16, 17, 18, 19, 20, 21, 22]. Again, these results either focus on specific classes of distributions, or give implications that require additional technical conditions, whereas we aim to give characterizations of robust learnability under minimal assumptions.

The question whether learnability under realizability assumptions extends to non-realizable setting has a long history. For binary classification tasks, both notions are characterized by the finiteness of the VC-dimension, and are therefore equivalent [13, 14, 15]. [1] show a similar result for online learning. Namely, that agnostic (non-realizable) learnability is characterized by the finiteness of the Littlestone dimension, and is therefore equivalent to realizable learnability.

Going beyond binary classification, recent work [16] shows that the equivalence of realizable and agnostic learnability extends across a wide variety of settings. These include models with no known characterization of learnability such as learning with arbitrary distributional assumptions and more general loss functions, as well as a host of other popular settings such as robust learning, partial learning, fair learning, and the statistical query model. This stands in contrast to our results for the distribution learning setting. We show that realizable learnability of a class of probability distributions does _not_ imply its agnostic learnability. It is interesting and natural to explore the relationship between various notions of distribution learnability, which we have scratched the surface of in this work.

## 2 Learnability Implies Additive Robust Learnability

We recall Theorem 1.5, which shows that any class that is realizably learnable is also additive robustly learnable.

**Theorem 1.5**.: _Any class of probability distributions \(\mathcal{Q}\) which is realizably learnable, is also \(\eta\)-additively \(2\)-robustly learnable for every \(\eta\in(0,1/4)\)._

We prove this theorem by providing an algorithm based on classical tools for hypothesis selection [23, 15, 16, 17]. These methods take as input a set of samples from an unknown distribution and a collection of hypotheses distributions. If the unknown distribution is close to one of the hypotheses, then, given enough samples, the algorithm will output a close hypothesis. Roughly speaking, our algorithm looks at all large subsets of the dataset, such that at least one will correspond to an uncontaminated set of samples. A learner for the realizable setting (whose existence we assumed) is applied to each to generate a set of hypotheses, and we then use hypothesis selection to pick one with sufficient accuracy. The proof of Theorem 1.7 (showing that subtractively robust learnability implies robust learnability) follows almost the exact same recipe, except the realizable learner is replaced with a learner robust to subtractive contaminations. We recall some preliminaries in Section A. We then prove Theorem 1.5 in Section B, and we formalize and prove a version of Theorem 1.7 in Section 2.1.

We note that \(\alpha=2\) and \(\alpha=3\) are often the optimal factors to expect in distribution learning settings, even for the case of finite distribution classes. For example, for proper agnostic learning the factor \(\alpha=3\) is known to be optimal for finite collections of distributions, which holds for classes with only 2 distributions [1]. Similarly the factor of \(\alpha=2\) is optimal if the notion of learning is relaxed to improper learners [1, 2]. While we are not aware of lower bounds for the additive setting, a small constant factor such as \(2\) is within expectations for these problems.

For the proof of Theorem 1.5, we refer the reader to Section B in the appendix.

### Subtractive Robust Learnability Implies Robust Learnability

Similarly, we can show that robustness with respect to a subtractive adversary implies robustness with respect to a general adversary. We note that this theorem requires a change in constants from \(\alpha\) to \((2\alpha+4)\).

**Theorem 1.7**.: _If a class \(\mathcal{C}\) is \(\eta\)-subtractive \(\alpha\)-robustly learnable, then it is also \(\eta\)-\((2\alpha+4)\)-robustly learnable._

The proof follows a similar argument as the proof of Theorem 1.5 and can be found in Section C in the appendix.

Learnability Does Not Imply Robust (Agnostic) Learnablity

In this section we show that there are classes of distributions which are realizably learnable, but not robustly learnable.

**Theorem 3.1**.: _There are classes of distributions \(\mathcal{Q}\), such that \(\mathcal{Q}\) is realizably learnable, but for every \(\alpha\in\mathbb{R}\), \(\mathcal{Q}\) is not \(\alpha\)-robustly learnable. Moreover, the sample complexity of learning \(\mathcal{Q}\) can be arbitrarily close to (but larger than) linear. Namely, for any super-linear function \(g\), there is a class \(\mathcal{Q}_{g}\), with_

* \(\mathcal{Q}_{g}\) _is realizable learnable with sample complexity_ \(n^{re}_{\mathcal{Q}_{g}}(\varepsilon,\delta)\leq\log(1/\delta)g(1/\varepsilon)\)_;_
* _for every_ \(\alpha\in\mathbb{R}\)_,_ \(\mathcal{Q}_{g}\) _is_ not \(\alpha\)_-robustly learnable._

Note that this statement appears _slightly_ weaker than Theorem 1.6, in that it holds for \(\alpha\)-robust learnability rather than \(\eta\)-subtractive \(\alpha\)-robust learnability. In fact, the two statements are incomparable, due to the order of quantifiers in the construction. Here we provide a single class which is not \(\alpha\)-robustly learnable for every \(\alpha\), whereas in the proof of Theorem 1.6 we give a different class for each \(\alpha\) (though the two constructions are similar). For simplicity we focus here on Theorem 3.1, whereas the proofs of Theorem 1.6 and other claims appear in Section D.

The key idea to the proof is to construct a class which is easy to learn in the realizable case, by having each distribution of the class have a unique support element that is not shared by any other distributions in the class. Distributions on which this "indicator element" has sufficient mass will be easily identified, independent of how rich the class is on other domain elements. That richness makes the class hard to learn from samples that miss those indicators. Furthermore, we construct the class in a way that its members are close in total variation distance to distributions that place no weight on those indicator elements.

This is done by making the mass on these indicator elements small, so that the members of a class of distributions that results from deleting these indicator bits are close to the initially constructed class, \(\mathcal{Q}_{g}\). In order to make this work for every target accuracy and sample complexity, we need to have a union of such classes with decreasingly small mass on the indicator bits. In order for this to not interfere with the realizable learnability, we let the distributions with small mass on the indicator bits have most of their mass on one point \((0,0)\) that is the same for all distributions in the class. This ensures that distributions for which the indicator bit will likely not be observed because their mass is smaller than some \(\eta\) are still easily \(\varepsilon\)-approximated by a constant distribution (\(\delta_{(0,0)}\)). Lastly we ensure the impossibility of agnostic learnability, by controlling the rate at which \(\eta\) approaches zero to be faster than the rate at which \(\varepsilon\) approaches zero. With this intuition in mind, we will now describe the construction and proof of this theorem.

Proof.: We first define the distributions in \(\mathcal{Q}_{g}\). Let \(\{A_{i}\subset\mathbb{N}:i\in\mathbb{N}\}\) be an enumeration of all finite subsets of \(\mathbb{N}\). Define distributions over \(\mathbb{N}\times\mathbb{N}\) as follows:

\[q_{i,j,k}=\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\left(\frac{1}{j}-\frac{1} {k}\right)U_{A_{i}\times\{2j+1\}}+\frac{1}{k}\delta_{(i,2j+2)}, \tag{1}\]

where, for every finite set \(W\), \(U_{W}\) denotes the uniform distribution over \(W\). For a monotone, super-linear function \(g:\mathbb{N}\rightarrow\mathbb{N}\), we now let \(\mathcal{Q}_{g}=\{q_{i,j,g(j)}:i,j\in\mathbb{N}\}\). The first bullet point of the theorem (the class is learnable) follows from Claim 3.2 and the second bullet point (the class is not robustly learnable) follows from Claim 3.3. 

**Claim 3.2**.: _For a monotone function \(g:\mathbb{N}\rightarrow\mathbb{N}\), let \(\mathcal{Q}_{g}=\{q_{i,j,g(j)}:i,j\in\mathbb{N}\}\). Then, the sample complexity of \(\mathcal{Q}_{g}\) in the realizable case is upper bounded by_

\[n^{re}_{\mathcal{Q}_{g}}(\varepsilon,\delta)\leq\log(1/\delta)g(1/\varepsilon).\]

This claim can be proved by showing that the following learner defined by

\[\mathcal{A}(S)=\begin{cases}q_{i,j,g(j)}&\text{if }(i,2j+2)\in S\\ \delta_{(0,0)}&\text{otherwise}\end{cases}\]is a successful learner in the realizable case. Intuitively, this learner is successful for distributions \(q_{i,j,g(j)}\) for which \(j\) is large (i.e., \(j>\frac{1}{\varepsilon}\)), since this will mean that \(d_{\mathrm{TV}}(q_{i,j,g(j)},\delta_{(0,0)})\) is small. Furthermore, it is successful for distributions \(q_{i,j,g(j)}\) for which \(j\) is small (i.e., upper bounded by some constant dependent on \(\varepsilon\)), because this will lower bound the probability \(1/g(j)\) of observing the indicator bit on \((i,2j+2)\). Once the indicator bit is observed the distribution will be uniquely identified.

**Claim 3.3**.: _For every function \(g\in\omega(n)\) the class \(\mathcal{Q}_{g}\) is not \(\alpha\)-robustly learnable for any \(\alpha>0\)._

This claim can be proven by showing that for every \(\alpha\), there is \(\eta\), such that the class of distributions \(Q^{\prime}\) such that for every \(q^{\prime}\in Q^{\prime}\) there is \(q\in\mathcal{Q}_{g}\) with \(d_{\mathrm{TV}}(q,q^{\prime})<\eta\) which is not \(\alpha\eta\)-weakly learnable.7 In particular, those for every \(q^{\prime}\in Q^{\prime}\) there is \(q\in\mathcal{Q}_{g}\) and \(p\) such that \(q=(1-\eta)q^{\prime}+\eta q\). We construct this class and show that it is not learnable by using the construction and Lemma 3 from [1].

Footnote 7: We provide a definition for \(\varepsilon\)-weak learnability as Definition D.2. We note that the definition we provide is what would usually be referred to as \((1/2-\varepsilon)\)-weak learnability in the supervised learning literature. For simplicity, because \(\varepsilon\) is our parameter of interest, we reparameterized the definition to be more intuitive.

### Existence of sample compression schemes

Sample compression schemes are combinatorial properties of classes that imply their learnability. For a variety of learning tasks, such as binary classification or expectation maximization a class has a sample compression scheme if and only if it is learnable [16, 1]. For classification tasks, sample compression for realizable samples implies agnostic sample compression. [1] used compression schemes to show learnability of classes of distributions in the realizable case, but left open the question if for learning probability distributions, the existence of realizable sample compression schemes implies the existence of similar schemes for the non-realizable (agnostic, or robust) settings. We provide a negative answer to this question.

More concretely, let \(\mathcal{Q}\) be a class of distributions over some domain \(X\). A compression scheme for \(\mathcal{Q}\) involves two agents: an encoder and a decoder.

* The encoder knows a distribution \(q\) and receives a sample \(S\) generated by this distribution. The encoder picks a bounded size sub-sample and sends it, possibly with a few additional bits to the decoder.
* The decoder receives the message and uses an agreed upon decoding rule (that may depend on \(\mathcal{Q}\) but not on \(q\) or \(S\)) to constructs a distribution \(p\) that is close to \(q\).

Of course, there is some probability that the samples are not representative of the distribution \(q\), in which case the compression scheme will fail. Thus, we only require that the decoding succeed with constant probability.

We say that a class \(\mathcal{Q}\) has a sample compression scheme (realizable or robust) if for every accuracy parameter \(\varepsilon>0\), the minimal required size of the sample \(S\), and upper bounds on the size of the sub-sample and number of additional bits in the encoder's message depend only of \(\mathcal{Q}\) and \(\varepsilon\) (and are independent of the sample generating \(q\) and on the sample \(S\)).

A realizable compression scheme is required to handle only \(q\)'s in \(\mathcal{Q}\) and output \(p\) such that \(d_{\mathrm{TV}}(p,q)\leq\varepsilon\), while a robust compression scheme should handle any \(q\) but the decoder's output \(p\) is only required to be \(\min_{q\in\mathcal{Q}}\{d_{\mathrm{TV}}(p,q)\}+\varepsilon\) close to \(q\).

**Theorem 3.4** (Formal version of Theorem 1.8).: _For every \(\alpha\in\mathbb{R}\), the existence of a realizable compression scheme, does not imply the existence of an \(\alpha\)-robust compression scheme. That is, there is a class \(\mathcal{Q}\) that has a realizable compression scheme, but for every \(\alpha\in\mathbb{R}\), \(\mathcal{Q}\) does not have an \(\alpha\)-robust compression scheme._

Proof.: Consider the class \(\mathcal{Q}=\mathcal{Q}_{g}\) from Section 3. We note that this class has a compression scheme of size 1. However, from [1], we know that having a \(\alpha\)-robust compression scheme implies \(\alpha\)-agnostic learnability. We showed in Theorem 1.6 that for every \(\alpha\) and for every superlinear function \(g\), the class \(\mathcal{Q}_{g}\) is not \(\alpha\)-agnostically learnable. It follows that \(\mathcal{Q}_{g}\) does not have an \(\alpha\)-robust compression scheme.

In Section E, we present a precise quantitative definition of sample compression schemes, as well as the proof that the class \(\mathcal{Q}_{g}\) has a sample compression scheme of size 1.

## 4 Implications of Private Learnability

Qualitatively speaking, differentially private algorithms offer a form of "robustness" - the output distribution of a differentially private algorithm is insensitive to the change of a single point in its input sample. The relationship between privacy and notions of "robustness" has been studied under various settings, where it has been shown that robust algorithms can be made private and vice versa [1, 2, 1].

For distribution learning, we find that: (1) the requirement of approximate differentially private learnability also does not imply (general) robust learnability; and (2) the stronger requirement of _pure_ differentially private learnability does imply robust learnability.

**Definition 4.1** (Differential Privacy [16]).: _Let \(X\) be an input domain and \(Y\) to be an output domain. A randomized algorithm \(A:X^{m}\to Y\) is \((\varepsilon,\delta)\)-differentially private (DP) if for every \(x,x^{\prime}\in X^{n}\) that differ in one entry,_

\[\mathbb{P}[A(x)\in B]\leq e^{\varepsilon}\cdot\mathbb{P}[A(x^{\prime})\in B]+ \delta\hskip 28.452756pt\text{for all $B\subseteq Y$}.\]

_If \(A\) is \((\varepsilon,\delta)\)-DP for \(\delta>0\), we say it satisfies approximate DP. If it satisfies \((\varepsilon,0)\)-DP, we say it satisfies pure DP._

**Definition 4.2** (DP learnable class).: _We say that a class \(\mathcal{C}\) of probability distributions is (approximate) DP learnable if there exists a randomized learner \(A\) and a function \(n_{\mathcal{C}}:(0,1)^{4}\to\mathbb{N}\), such that for every probability distribution \(p\in\mathcal{C}\), and every \((\alpha,\beta,\varepsilon,\delta)\in(0,1)^{4}\), for \(n\geq n_{\mathcal{C}}(\alpha,\beta,\varepsilon,\delta)\)_

1. \(A\) _is_ \((\varepsilon,\delta)\)_-DP; and_
2. _The probability over samples_ \(S\) _of size_ \(n\) _drawn i.i.d. from the distribution_ \(p\)_, as well as over the randomness of_ \(A\) _that_ \[d_{\mathrm{TV}}(p,A(S))\leq\alpha\] _is at least_ \(1-\beta\)_._

_We say \(\mathcal{C}\) is pure DP learnable if a learner \(A\) can be found that satisfies \((\varepsilon,0)\)-DP, in which case the sample complexity function \(n_{\mathcal{C}}:(0,1)^{3}\to\mathbb{N}\) does not take \(\delta\) as a parameter._

**Theorem 4.3** (Approximate DP learnability vs. robust learnability).:
1. _If a class_ \(\mathcal{Q}\) _is approximate DP learnable, then_ \(\mathcal{Q}\) _is_ \(\eta\)_-additive_ \(2\)_-robustly learnable for any_ \(\eta\in(0,1/4)\)_._
2. _There exists an approximate DP learnable class_ \(\mathcal{Q}\) _that is not_ \(\alpha\)_-robustly learnable for any_ \(\alpha\geq 1\)_._

Note that the first claim is immediate from Theorem 1.5, since approximate DP learnability implies learnability. To prove the second claim, we show that the learner for the class \(\mathcal{Q}\) described in Theorem 3.1 can be made differentially private by employing stability-based histograms [1]. The proof appears in Section F.

**Theorem 4.4** (Pure DP learnable vs. robustly learnable).: _If a class \(\mathcal{Q}\) is pure DP learnable, then \(\mathcal{Q}\) is \(3\)-robustly learnable._

The proof relies on the finite cover characterization of pure DP learnability.

**Proposition 4.5** (Packing lower bound, Lemma 5.1 from [1]).: _Let \(\mathcal{C}\) be a class of distributions, and let \(\alpha,\varepsilon>0\). Suppose \(\mathcal{P}_{\alpha}\) is a \(\alpha\)-packing of \(\mathcal{C}\), that is, \(\mathcal{P}_{\alpha}\subseteq\mathcal{C}\) such that for any \(p\neq q\in\mathcal{P}_{\alpha},d_{\mathrm{TV}}(p,q)>\alpha\)._

_Any \(\varepsilon\)-DP algorithm \(A\) that takes \(n\) i.i.d. samples \(S\) from any \(p\in\mathcal{C}\) and has \(d_{\mathrm{TV}}(p,A(S))\leq\alpha/2\) with probability \(\geq 9/10\) requires_

\[n\geq\frac{\log|\mathcal{P}_{\alpha}|-\log\frac{10}{9}}{\varepsilon}.\]Proof of Theorem 4.4.: Let \(\alpha,\beta>0\). Pure DP learnability of \(\mathcal{Q}\) implies that there exists a \(1\)-DP algorithm \(A_{DP}\) and \(n=n_{\mathcal{C}}(\alpha/12,1/10,1)\) such that for any \(p\in\mathcal{Q}\), with probability \(\geq 9/10\) over the sampling of \(n\) i.i.d. samples \(S\) from \(p\), as well as over the randomness of the algorithm \(A_{DP}\), we have \(d_{\mathrm{TV}}(p,A_{DP}(S))\leq\alpha/12\). By Proposition 4.5, any \(\alpha/6\)-packing \(\mathcal{P}_{\alpha/6}\) of \(\mathcal{Q}\) has

\[|\mathcal{P}_{\alpha/6}|\leq\exp{(m)}\cdot(10/9).\]

Let \(\widehat{\mathcal{Q}}\) be such a maximal \(\alpha/6\)-packing. By maximality, \(\widehat{\mathcal{Q}}\) is also an \(\alpha/6\)-cover of \(\mathcal{Q}\). Hence, running Yatracos' 3-robust finite class learner (Theorem A.1) \(A\) over \(\widehat{\mathcal{Q}}\) with

\[n_{\widehat{\mathcal{Q}}}(\alpha/2,\beta)=O\left(\frac{\log|\widehat{\mathcal{ Q}}|+\log(1/\beta)}{(\alpha/2)^{2}}\right)\]

samples drawn i.i.d. from \(p\) yields, with probability \(\geq 1-\beta\)

\[d_{\mathrm{TV}}(p,A(S)) \leq 3\min\{d_{\mathrm{TV}}(p,p^{\prime}):p^{\prime}\in\widehat{ \mathcal{Q}}\}+\alpha/2\] \[\leq 3(\min\{d_{\mathrm{TV}}(p,p^{\prime}):p^{\prime}\in\mathcal{ Q}\}+\alpha/6)+\alpha/2\] \[=3\min\{d_{\mathrm{TV}}(p,p^{\prime}):p^{\prime}\in\mathcal{Q}\}+\alpha.\qed\]

Note that Yatracos' algorithm for hypothesis selection can be replaced with a pure DP algorithm for hypothesis selection (Theorem 27 of [1]) in order to achieve the following stronger implication.

**Theorem 4.6** (Pure DP learnable vs. robustly learnable).: _If a class \(\mathcal{Q}\) is pure DP learnable, then \(\mathcal{Q}\) is pure DP \(3\)-robustly learnable._

## 5 Conclusions

We examine the connection between learnability and robust learnability for general classes of probability distributions. Our main findings are somewhat surprising in that, in contrast to most known leaning scenarios, learnability does _not_ imply robust learnability. We also show that learnability _does_ imply additively robust learnability. We use our proof techniques to draw new insights related to compression schemes and differentially private distribution learning.

## Acknowledgments

Thanks to Argyris Mouzakis for helpful conversations in the early stages of this work. AB was supported by an NSERC Discovery Grant and a David R. Cheriton Graduate Scholarship. GK was supported by a Canada CIFAR AI Chair, an NSERC Discovery Grant, and an unrestricted gift from Google. TL was supported by a Vector Research Grant and a Waterloo Apple PhD Fellowship in Data Science and Machine Learning.

## References

* [AAK21] Ishaq Aden-Ali, Hassan Ashtiani, and Gautam Kamath. On the sample complexity of privately learning unbounded high-dimensional gaussians. In _Proceedings of the 32nd International Conference on Algorithmic Learning Theory_, ALT '21, pages 185-216. JMLR, Inc., 2021.
* [AAL21] Ishaq Aden-Ali, Hassan Ashtiani, and Christopher Liaw. Privately learning mixtures of axis-aligned gaussians. In _Advances in Neural Information Processing Systems 34_, NeurIPS '21. Curran Associates, Inc., 2021.
* [ABDH\({}^{+}\)18] Hassan Ashtiani, Shai Ben-David, Nicholas Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes. In _Advances in Neural Information Processing Systems 31_, NeurIPS '18, pages 3412-3421. Curran Associates, Inc., 2018.
* [ABDH\({}^{+}\)20] Hassan Ashtiani, Shai Ben-David, Nicholas JA Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. _Journal of the ACM_, 67(6):32:1-32:42, 2020.

* [AFJ\({}^{+}\)18] Jayadev Acharya, Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Maximum selection and sorting with adversarial comparators. _Journal of Machine Learning Research_, 19(1):2427-2457, 2018.
* [AKT\({}^{+}\)23] Daniel Alabi, Pravesh K Kothari, Pranay Tankala, Prayaag Venkat, and Fred Zhang. Privately estimating a Gaussian: Efficient, robust and optimal. In _Proceedings of the 55th Annual ACM Symposium on the Theory of Computing_, STOC '23, New York, NY, USA, 2023. ACM.
* [AM20] Marco Avella-Medina. The role of robust statistics in private data analysis. _Chance_, 33(4):37-42, 2020.
* [ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private assouad, fano, and le cam. In _Proceedings of the 32nd International Conference on Algorithmic Learning Theory_, ALT '21, pages 48-78. JMLR, Inc., 2021.
* [BBK\({}^{+}\)22] Olivier Bousquet, Mark Braverman, Gillat Kol, Klim Efremenko, and Shay Moran. Statistically near-optimal hypothesis selection. In _Proceedings of the 62nd Annual IEEE Symposium on Foundations of Computer Science_, FOCS '21, pages 909-919. IEEE Computer Society, 2022.
* [BDJ\({}^{+}\)22] Ainesh Bakshi, Ilias Diakonikolas, He Jia, Daniel M Kane, Pravesh K Kothari, and Santosh S Vempala. Robustly learning mixtures of k arbitrary Gaussians. In _Proceedings of the 54th Annual ACM Symposium on the Theory of Computing_, STOC '22, pages 1234-1247, New York, NY, USA, 2022. ACM.
* [BEHW89] Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. _Journal of the ACM_, 36(4):929-965, 1989.
* [BGS\({}^{+}\)21] Gavin Brown, Marco Gaboardi, Adam Smith, Jonathan Ullman, and Lydia Zakynthinou. Covariance-aware private mean estimation without private covariance estimation. In _Advances in Neural Information Processing Systems 34_, NeurIPS '21. Curran Associates, Inc., 2021.
* [BHM\({}^{+}\)17] Shai Ben-David, Pavel Hrubes, Shay Moran, Amir Shpilka, and Amir Yehudayoff. A learning problem that is independent of the set theory ZFC axioms. _CoRR_, abs/1711.05195, 2017.
* [BKM19] Olivier Bousquet, Daniel M. Kane, and Shay Moran. The optimal approximation factor in density estimation. In _Proceedings of the 32nd Annual Conference on Learning Theory_, COLT '19, pages 318-341, 2019.
* [BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. In _Advances in Neural Information Processing Systems 32_, NeurIPS '19, pages 156-167. Curran Associates, Inc., 2019.
* [BLMT22] Guy Blanc, Jane Lange, Ali Malik, and Li-Yang Tan. On the power of adaptivity in statistical adversaries. In _Proceedings of the 35th Annual Conference on Learning Theory_, COLT '22, pages 5030-5061, 2022.
* [BNL12] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In _Proceedings of the 29th International Conference on Machine Learning_, ICML '12, pages 1467-1474. JMLR, Inc., 2012.
* [BNS16] Mark Bun, Kobbi Nissim, and Uri Stemmer. Simultaneous private learning of multiple concepts. In _Proceedings of the 7th Conference on Innovations in Theoretical Computer Science_, ITCS '16, pages 369-380, New York, NY, USA, 2016. ACM.
* The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009_, 2009.
* [BS19] Mark Bun and Thomas Steinke. Average-case averages: Private algorithms for smooth sensitivity and mean estimation. In _Advances in Neural Information Processing Systems 32_, NeurIPS '19, pages 181-191. Curran Associates, Inc., 2019.
* [CDSS13] Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Learning mixtures of structured distributions over discrete domains. In _Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '13, pages 1380-1394, Philadelphia, PA, USA, 2013. SIAM.
* [CDSS14a] Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Efficient density estimation via piecewise polynomial approximation. In _Proceedings of the 46th Annual ACM Symposium on the Theory of Computing_, STOC '14, pages 604-613, New York, NY, USA, 2014. ACM.
* [CDSS14b] Siu On Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Near-optimal density estimation in near-linear time using variable-width histograms. In _Advances in Neural Information Processing Systems 27_, NIPS '14, pages 1844-1852. Curran Associates, Inc., 2014.
* [CWZ21] T Tony Cai, Yichen Wang, and Linjun Zhang. The cost of privacy: Optimal rates of convergence for parameter estimation with differential privacy. _The Annals of Statistics_, 49(5):2825-2850, 2021.
* [DDS12] Constantinos Daskalakis, Ilias Diakonikolas, and Rocco A. Servedio. Learning Poisson binomial distributions. In _Proceedings of the 44th Annual ACM Symposium on the Theory of Computing_, STOC '12, pages 709-728, New York, NY, USA, 2012. ACM.
* [Dia16] Ilias Diakonikolas. Learning structured distributions. In Peter Buhlmann, Petros Drineas, Michael J. Kane, and Mark J. van der Laan, editors, _Handbook of Big Data_, pages 267-283. Chapman and Hall/CRC, 2016.
* [DK14] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of Gaussians. In _Proceedings of the 27th Annual Conference on Learning Theory_, COLT '14, pages 1183-1213, 2014.
* [DK22] Ilias Diakonikolas and Daniel Kane. _Algorithmic High-Dimensional Robust Statistics_. Cambridge University Press, 2022.
* [DKK\({}^{+}\)16] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robust estimators in high dimensions without the computational intractability. In _Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science_, FOCS '16, pages 655-664, Washington, DC, USA, 2016. IEEE Computer Society.
* [DKK\({}^{+}\)17] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Being robust (in high dimensions) can be practical. In _Proceedings of the 34th International Conference on Machine Learning_, ICML '17, pages 999-1008. JMLR, Inc., 2017.
* [DKK\({}^{+}\)18] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Ankur Moitra, and Alistair Stewart. Robustly learning a Gaussian: Getting optimal error, efficiently. In _Proceedings of the 29th Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '18, Philadelphia, PA, USA, 2018. SIAM.
* [DKK\({}^{+}\)19] Ilias Diakonikolas, Gautam Kamath, Daniel M. Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever: A robust meta-algorithm for stochastic optimization. In _Proceedings of the 36th International Conference on Machine Learning_, ICML '19, pages 1596-1606. JMLR, Inc., 2019.
* [DKS17] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional Gaussians and Gaussian mixtures. In _Proceedings of the 58th Annual IEEE Symposium on Foundations of Computer Science_, FOCS '17, pages 73-84, Washington, DC, USA, 2017. IEEE Computer Society.

* [DL96] Luc Devroye and Gabor Lugosi. A universally acceptable smoothing factor for kernel density estimation. _The Annals of Statistics_, 24(6):2499-2512, 1996.
* [DL97] Luc Devroye and Gabor Lugosi. Nonasymptotic universal smoothing factors, kernel complexity and Yatracos classes. _The Annals of Statistics_, 25(6):2626-2637, 1997.
* [DL01] Luc Devroye and Gabor Lugosi. _Combinatorial methods in density estimation_. Springer, 2001.
* [DL09] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In _Proceedings of the 41st Annual ACM Symposium on the Theory of Computing_, STOC '09, pages 371-380, New York, NY, USA, 2009. ACM.
* [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In _Proceedings of the 3rd Conference on Theory of Cryptography_, TCC '06, pages 265-284, Berlin, Heidelberg, 2006. Springer.
* [GFH\({}^{+}\)21] Jonas Geiping, Liam Fowl, W Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller, and Tom Goldstein. Witches' brew: Industrial scale data poisoning via gradient matching. In _Proceedings of the 9th International Conference on Learning Representations_, ICLR '21, 2021.
* [GH22] Kristian Georgiev and Samuel B Hopkins. Privacy induces robustness: Information-computation gaps and sparse mean estimation. In _Advances in Neural Information Processing Systems 35_, NeurIPS '22. Curran Associates, Inc., 2022.
* [GKK\({}^{+}\)20] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In _Proceedings of the 33rd Annual Conference on Learning Theory_, COLT '20, pages 1785-1816, 2020.
* [GTX\({}^{+}\)20] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild, Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Dataset security for machine learning: Data poisoning, backdoor attacks, and defenses. _arXiv preprint arXiv:2012.10544_, 2020.
* [Hau92] David Haussler. Decision theoretic generalizations of the PAC model for neural net and other learning applications. _Information and Computation_, 100(1):78-150, 1992.
* [HKLM22] Max Hopkins, Daniel M Kane, Shachar Lovett, and Gaurav Mahajan. Realizable learning is all you need. In _Proceedings of the 35th Annual Conference on Learning Theory_, COLT '22, pages 3015-3069, 2022.
* [HKM22] Samuel B Hopkins, Gautam Kamath, and Mahbod Majid. Efficient mean estimation with pure differential privacy via a sum-of-squares exponential mechanism. In _Proceedings of the 54th Annual ACM Symposium on the Theory of Computing_, STOC '22, pages 1406-1417, New York, NY, USA, 2022. ACM.
* [HKMN23] Samuel B Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In _Proceedings of the 55th Annual ACM Symposium on the Theory of Computing_, STOC '23, New York, NY, USA, 2023. ACM.
* [HL18] Samuel B. Hopkins and Jerry Li. Mixture models, robustness, and sum of squares proofs. In _Proceedings of the 50th Annual ACM Symposium on the Theory of Computing_, STOC '18, pages 1021-1034, New York, NY, USA, 2018. ACM.
* [Hub64] Peter J. Huber. Robust estimation of a location parameter. _The Annals of Mathematical Statistics_, 35(1):73-101, 1964.
* [JKV23] He Jia, Pravesh K Kothari, and Santosh S Vempala. Beyond moments: Robustly learning affine transformations with asymptotically optimal error. _arXiv preprint arXiv:2302.12289_, 2023.

* [JOR22] Ayush Jain, Alon Orlitsky, and Vaishakh Ravindrakumar. Robust estimation algorithms don't need to know the corruption level. _arXiv preprint arXiv:2202.05453_, 2022.
* September 3 2004_, pages 180-191. Morgan Kaufmann, 2004.
* [KKMN09] Aleksandra Korolova, Krishnaram Kenthapadi, Nina Mishra, and Alexandros Ntoulas. Releasing search queries and clicks privately. In _Proceedings of the 18th International World Wide Web Conference_, WWW '09, pages 171-180, New York, NY, USA, 2009. ACM.
* [KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan Ullman. Privately learning high-dimensional distributions. In _Proceedings of the 32nd Annual Conference on Learning Theory_, COLT '19, pages 1853-1902, 2019.
* [KMR\({}^{+}\)94] Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, Robert E. Schapire, and Linda Sellie. On the learnability of discrete distributions. In _Proceedings of the 26th Annual ACM Symposium on the Theory of Computing_, STOC '94, pages 273-282, New York, NY, USA, 1994. ACM.
* [KMS22a] Gautam Kamath, Argyris Mouzakis, and Vikrant Singhal. New lower bounds for private estimation and a generalized fingerprinting lemma. In _Advances in Neural Information Processing Systems 35_, NeurIPS '22. Curran Associates, Inc., 2022.
* [KMS\({}^{+}\)22b] Gautam Kamath, Argyris Mouzakis, Vikrant Singhal, Thomas Steinke, and Jonathan Ullman. A private and computationally-efficient estimator for unbounded gaussians. In _Proceedings of the 35th Annual Conference on Learning Theory_, COLT '22, pages 544-572, 2022.
* [KMV22] Pravesh K Kothari, Pasin Manurangsi, and Ameya Velingker. Private robust estimation by stabilizing convex relaxations. In _Proceedings of the 35th Annual Conference on Learning Theory_, COLT '22, pages 723-777, 2022.
* [KSS18] Pravesh Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In _Proceedings of the 50th Annual ACM Symposium on the Theory of Computing_, STOC '18, pages 1035-1046, New York, NY, USA, 2018. ACM.
* [KSU20] Gautam Kamath, Vikrant Singhal, and Jonathan Ullman. Private mean estimation of heavy-tailed distributions. In _Proceedings of the 33rd Annual Conference on Learning Theory_, COLT '20, pages 2204-2235, 2020.
* [KU20] Gautam Kamath and Jonathan Ullman. A primer on private statistics. _arXiv preprint arXiv:2005.00010_, 2020.
* [KV18] Vishesh Karwa and Salil Vadhan. Finite sample differentially private confidence intervals. In _Proceedings of the 9th Conference on Innovations in Theoretical Computer Science_, ITCS '18, pages 44:1-44:9, Dagstuhl, Germany, 2018. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
* a simple solution to a long-standing problem, 2023.
* [LKKO21] Xiyang Liu, Weihao Kong, Sham Kakade, and Sewoong Oh. Robust and differentially private mean estimation. In _Advances in Neural Information Processing Systems 34_, NeurIPS '21. Curran Associates, Inc., 2021.
* [LKO22] Xiyang Liu, Weihao Kong, and Sewoong Oh. Differential privacy and robust statistics in high dimensions. In _Proceedings of the 35th Annual Conference on Learning Theory_, COLT '22, pages 1167-1246, 2022.

* [LKY23] Yiwei Lu, Gautam Kamath, and Yaoliang Yu. Exploring the limits of model-targeted indiscriminate data poisoning attacks. In _Proceedings of the 40th International Conference on Machine Learning_, ICML '23, pages 22856-22879. JMLR, Inc., 2023.
* [LM21] Allen Liu and Ankur Moitra. Settling the robust learnability of mixtures of gaussians. In _Proceedings of the 53nd Annual ACM Symposium on the Theory of Computing_, STOC '21, pages 518-531, New York, NY, USA, 2021. ACM.
* [LM22] Allen Liu and Ankur Moitra. Learning GMMs with nearly optimal robustness guarantees. In _Proceedings of the 35th Annual Conference on Learning Theory_, COLT '22, pages 2815-2895, 2022.
* [LRV16] Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In _Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science_, FOCS '16, pages 665-674, Washington, DC, USA, 2016. IEEE Computer Society.
* [LS17] Jerry Li and Ludwig Schmidt. Robust proper learning for mixtures of Gaussians via systems of polynomial inequalities. In _Proceedings of the 30th Annual Conference on Learning Theory_, COLT '17, pages 1302-1382, 2017.
* [MS08] Satyaki Mahalanabis and Daniel Stefankovic. Density estimation in linear time. In _Proceedings of the 21st Annual Conference on Learning Theory_, COLT '08, pages 503-512, 2008.
* [MY16] Shay Moran and Amir Yehudayoff. Sample compression schemes for VC classes. _J. ACM_, 63(3):21:1-21:10, 2016.
* [RJC22] Kelly Ramsay, Aukosh Jagannath, and Shoja'eddin Chenouri. Concentration of the exponential mechanism and differentially private multivariate medians. _arXiv preprint arXiv:2210.06459_, 2022.
* From Theory to Algorithms_. Cambridge University Press, 2014.
* [SCV18] Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the presence of arbitrary outliers. In _Proceedings of the 9th Conference on Innovations in Theoretical Computer Science_, ITCS '18, pages 45:1-45:21, Dagstuhl, Germany, 2018. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik.
* [SKL17] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. In _Advances in Neural Information Processing Systems 30_, NeurIPS '17, pages 3520-3532. Curran Associates, Inc., 2017.
* [SM22] Aleksandra Slavkovic and Roberto Molinari. Perturbed M-estimation: A further investigation of robust statistics for differential privacy. In Alicia L. Carriquiry, Judith M. Tanur, and William F. Eddy, editors, _Statistics in the Public Interest: In Memory of Stephen E. Fienberg_, pages 337-361. Springer, 2022.
* [SOAJ14] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical Gaussian mixtures. In _Advances in Neural Information Processing Systems 27_, NIPS '14, pages 1395-1403. Curran Associates, Inc., 2014.
* [Tuk60] John W. Tukey. A survey of sampling from contaminated distributions. _Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling_, pages 448-485, 1960.
* [Val84] Leslie G. Valiant. A theory of the learnable. _Communications of the ACM_, 27(11):1134-1142, 1984.
* [VC71] Vladimir Naumovich Vapnik and Alexey Yakovlevich Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Theory of Probability & Its Applications_, 16(2):264-280, 1971.

* [VC74] Vladimir Vapnik and Alexey Chervonenkis. _Theory of Pattern Recognition_. Nauka, 1974.
* [Yat85] Yannis G. Yatracos. Rates of convergence of minimum distance estimators and Kolmogorov's entropy. _The Annals of Statistics_, 13(2):768-774, 1985.

Additional Preliminaries

We recall a classic theorem for the problem of hypothesis selection. Given a set of candidate hypothesis distributions, the algorithm selects one which is close to an unknown distribution (to which we have sample access). The requisite number of samples from the unknown distribution is logarithmic in the size of the set of candidates.

**Theorem A.1** (Yatracos' 3-robust learner for finite classes (Theorem 4.4 of [1], based on Theorem 1 of [20]).: _Let \(\mathcal{C}\) be a finite class of distributions over a domain \(\mathcal{X}\). There exists an algorithm \(A\) such that for any \((\alpha,\beta)\in(0,1)^{2}\) and for any distribution \(p\) over \(\mathcal{X}\), given a sample \(S\) of size_

\[m=O\left(\frac{\log|\mathcal{C}|+\log(1/\delta)}{\varepsilon^{2}}\right)\]

_drawn i.i.d. from \(p\), we have that with probability \(\geq 1-\delta\),_

\[d_{\mathrm{TV}}(p,A(S))\leq 3\cdot\min\{d_{\mathrm{TV}}(p,p^{\prime}):p^{ \prime}\in\mathcal{C}\}+\varepsilon.\]

We also use the following form of the Chernoff bound.

**Proposition A.2** (Chernoff bounds).: _Let \(X=\sum_{i=1}^{m}X_{i}\) where \(X_{1}...X_{m}\) are independent draws from Bernoulli(\(p\)). For any \(t\in(0,1)\), \(\mathbb{P}[X/m\leq(1-t)p]\leq\exp(-t^{2}mp/2)\)._

## Appendix B Learnability Implies Additive Robust Learnability

In this section, we provide the proof for Theorem 1.5.

**Theorem 1.5**.: _Any class of probability distributions \(\mathcal{Q}\) which is realizably learnable, is also \(\eta\)-additively \(2\)-robustly learnable for every \(\eta\in(0,1/4)\)._

Proof.: Recall that \(\mathcal{Q}\) is a class of probability distributions which is realizably learnable. We let \(\mathcal{A}_{\mathcal{Q}}^{re}\) be a realizable learner for \(\mathcal{Q}\), with sample complexity \(n_{\mathcal{C}}^{re}\). Let accuracy parameters \(\varepsilon,\delta>0\) be arbitrary. We will define \(n_{1}\geq\max\left\{2n_{\mathcal{Q}}^{re}(\frac{\varepsilon}{9},\frac{\delta} {5}),\frac{162(1+\log(\frac{5}{2}))}{\varepsilon^{2}}\right\}\) and \(n_{2}\geq\frac{162\left(2(\eta+\frac{2\varepsilon}{9})n_{1}\log\left(n_{1} \right)+\log\left(\frac{5}{2}\right)\right)}{\varepsilon^{2}}\geq\frac{162 \left(1+\log(\frac{5}{2})\right)}{\varepsilon^{2}}\), and \(n=n_{1}+n_{2}\) be their sum.

Our additive robust learner will receive a sample \(S\sim(\eta q+(1-\eta)p)^{n}\) of size \(n\). We can view a subset \(S^{\prime}\subset S\) as the "clean" part, being i.i.d. generated by \(p\). The size of this clean part \(|S^{\prime}|=n^{\prime}\) is distributed according to a binomial distribution \(\mathrm{Bin}(n,1-\eta)\). By a Chernoff bound (Proposition A.2), we get

\[\mathbb{P}\left[n^{\prime}\leq\left(1-\frac{\varepsilon}{9}-\eta\right)n \right]\leq\mathbb{P}\left[n^{\prime}\leq\left(1-\frac{\varepsilon}{9}-\eta+ \frac{\eta\varepsilon}{9}\right)n\right]=\mathbb{P}\left[n^{\prime}\leq\left( 1-\frac{\varepsilon}{9}\right)\left(1-\eta\right)n\right]\]

\[\leq\exp\left(-\left(\frac{\varepsilon}{9}\right)^{2}n\left(1-\eta\right)/2 \right)=\exp\left(-\frac{\varepsilon^{2}}{162}n\left(1-\eta\right)\right)\leq \exp\left(-\frac{\varepsilon^{2}}{162}n\left(1-\frac{3}{4}\right)\right)\]

Thus, given that \(n=n_{1}+n_{2}\geq 2(\frac{162(1+\log(\frac{5}{2}))}{\varepsilon^{2}})\) with probability at least \(1-\frac{\delta}{5}\), we have \(n^{\prime}\geq n\left(1-\eta-\frac{\varepsilon}{9}\right)\). For the rest of the argument we will now assume that we have indeed \(n^{\prime}\geq n\left(1-\eta-\frac{\varepsilon}{9}\right)\). The learner now randomly partitions the sample \(S\) into \(S_{1}\) and \(S_{2}\) of sizes \(n_{1}\) and \(n_{2}\), respectively. Now let \(S^{\prime}_{1}=S_{1}\cap S^{\prime}\) and \(S^{\prime}_{2}=S_{2}\cap S^{\prime}\) be the intersections of these sets with the clean set \(S^{\prime}\), and \(n^{\prime}_{1}\) and \(n^{\prime}_{2}\) be their respective sizes. We note that \(n^{\prime}_{1}\sim\mathrm{Hypergeometric}(n,n^{\prime},n_{1})\) and \(n^{\prime}_{2}\sim\mathrm{Hypergeometric}(n,n^{\prime},n_{2})\).8 Thus, assuming \(m^{\prime}\geq m(1-\eta-\frac{\varepsilon}{9})\) using Proposition A.2, we have that

Footnote 8: Recall that \(\mathrm{Hypergeometric}(N,K,n)\) is the random variable of the number of â€œsuccessesâ€ when \(n\) draws are made without replacement from a set of size \(N\), where \(K\) elements of the set are considered to be successes.

\[\Pr\left[|S^{\prime}_{1}|\leq\left(1-\eta-\frac{2\varepsilon}{9}\right)n_{1} \right]\leq e^{-\frac{2}{81}\varepsilon^{2}n_{1}}\]\[\Pr\left[|S^{\prime}_{2}|\leq\left(1-\eta-\frac{2\varepsilon}{9}\right)n_{2}\right] \leq e^{-\frac{2}{81}\varepsilon^{2}n_{2}},\]

where the probability is over the random partition of \(S\). We note that by our choices of \(n_{1}\) and \(n_{2}\), with probability \(1-\frac{2\delta}{5}\), the clean fractions of \(S_{1}\) and \(S_{2}\) (namely \(\frac{|S^{\prime}_{1}|}{|S_{1}|}\) and \(\frac{|S^{\prime}_{2}|}{|S_{2}|}\)) are each at least \(\left(1-\eta-\frac{2\varepsilon}{9}\right)\).

Let

\[\hat{\mathcal{H}}=\left\{\mathcal{A}_{\mathcal{Q}}^{r\varepsilon}(S^{\prime \prime}):S^{\prime\prime}\subset S_{1}\text{ with }|S^{\prime\prime}|=\left(1- \eta-\frac{2\varepsilon}{9}\right)n_{1}\right\}\]

be the set of distributions output by the realizable learning algorithm \(\mathcal{A}_{\mathcal{Q}}^{r\varepsilon}\) when given as input all possible subsets of \(S_{1}\) of size exactly \(\left(1-\eta-\frac{2\varepsilon}{9}\right)n_{1}\). For \(\varepsilon<\frac{9n}{2}\), we know that this set of distributions \(|\hat{\mathcal{H}}|\) is of size \(\begin{pmatrix}n_{1}\\ (1-\eta-\frac{2\varepsilon}{9})n_{1}\end{pmatrix}\leq n_{1}^{2\eta n_{1}}\). By the guarantee of the realizable learning algorithm \(\mathcal{A}_{\mathcal{Q}}^{r\varepsilon}\), if there exists a "clean" subset \(S^{\prime}_{1}\subset S_{1}\) where \(|S^{\prime}_{1}|\geq n_{1}(1-\eta-\frac{2\varepsilon}{9})\) (i.e., \(S^{\prime}_{1}\sim p^{n_{1}(1-\eta-\frac{2\varepsilon}{9})}\)), then with probability \(1-\frac{\delta}{5}\) there exists a candidate distribution \(q^{*}\in\hat{\mathcal{H}}\) with \(d_{\mathrm{TV}}(p,q^{*})=\frac{\varepsilon}{9}\).

We now define and consider the _Yatracos sets_.9 For every \(q_{i},q_{j}\in\hat{\mathcal{H}}\), define the Yatracos set between \(q_{i}\) and \(q_{j}\) to be \(A_{i,j}=\{x:\ q_{i}(x)\geq q_{j}(x)\}\).10 We let

Footnote 9: These sets are also sometimes called Scheffe sets in the literature.

Footnote 10: Note that this definition is asymmetric: \(A_{i,j}\neq A_{j,i}\).

\[\mathcal{Y}(\hat{\mathcal{H}})=\{A_{i,j}\subset\mathcal{X}:q_{i},q_{j}\in \hat{\mathcal{H}}\}\]

denote the set of all pairwise Yatracos sets between distributions in the set \(\hat{\mathcal{H}}\).

We now consider the \(A\)-distance [14] between two distributions with respect to the Yatracos sets, i.e., we consider

\[d_{\mathcal{Y}(\hat{\mathcal{H}})}(p^{\prime},q^{\prime})=\sup_{B\in\mathcal{ Y}(\hat{\mathcal{H}})}|p^{\prime}(B)-q^{\prime}(B)|.\]

This distance looks at the supremum of the discrepancy between the distributions across the Yatracos sets. Consequently, for any two distributions \(p^{\prime},q^{\prime}\) we have \(d_{\mathrm{TV}}(p^{\prime},q^{\prime})\geq d_{\mathcal{Y}(\hat{\mathcal{H}})} (p^{\prime},q^{\prime})\), since total variation distance is the supremum of the discrepancy across _all_ possible sets. Furthermore, if \(q^{\prime},p^{\prime}\in\hat{\mathcal{H}}\), then \(d_{\mathrm{TV}}(p^{\prime},q^{\prime})=d_{\mathcal{Y}(\hat{\mathcal{H}})}(p^{ \prime},q^{\prime})\), since either of the Yatracos sets between the two distributions serves as a set that realizes the total variation distance between them.

Suppose there is some \(q^{*}\in\hat{\mathcal{H}}\) with \(d_{\mathrm{TV}}(p,q^{*})\leq\frac{\varepsilon}{9}\). Then for every \(q\in\hat{\mathcal{H}}\):

\[d_{\mathrm{TV}}(p,q) \leq d_{\mathrm{TV}}(p,q^{*})+d_{\mathrm{TV}}(q^{*},q)\] \[\leq\frac{\varepsilon}{9}+d_{\mathcal{Y}(\hat{\mathcal{H}})}(q^{ *},q)\] \[\leq\frac{\varepsilon}{9}+d_{\mathcal{Y}(\hat{\mathcal{H}})}(q^{* },p)+d_{\mathcal{Y}(\hat{\mathcal{H}})}(p,q)\] \[\leq\frac{\varepsilon}{9}+d_{\mathrm{TV}}(q^{*},p)+d_{\mathcal{Y} (\hat{\mathcal{H}})}(p,q)\] \[\leq\frac{2\varepsilon}{9}+d_{\mathcal{Y}(\hat{\mathcal{H}})}(p,q).\]

Lastly, we will argue that we can empirically approximate \(d_{\mathcal{Y}(\hat{\mathcal{H}})}(p,q)\), which we can then use to select a hypothesis. We note that, since \(\mathcal{Y}(\hat{\mathcal{H}})\) is a finite set of size \(\leq\begin{pmatrix}n_{1}\\ (1-\eta-\frac{2\varepsilon}{9})n_{1}\end{pmatrix}^{2}=\begin{pmatrix}n_{1}\\ (\eta+\frac{2\varepsilon}{9})n_{1}\end{pmatrix}^{2}\leq((n_{1}^{(\eta+\frac{2 \varepsilon}{9})}))^{2}=n_{1}^{2(\eta+\frac{2\eta}{9})n_{1}}\), we have uniform convergence11 with respect to \(\mathcal{Y}(\hat{\mathcal{H}})\). Recall that, we assumed that there is "clean" subsample \(S_{2}^{\prime\prime}\subset S_{2}\), which is i.i.d. distributed according to \(p\) and of size \((1-\frac{2\varepsilon}{9}-\eta)n_{1}\). We also note that the clean samples \(S_{1}\) and \(S_{2}\) are drawn independently from each other. Thus with probability \(1-\frac{\delta}{5},S_{2}^{\prime\prime}\) is \(\frac{\varepsilon}{9}\)-representative of \(p\) with respect to \(\mathcal{Y}(\hat{\mathcal{H}})\). For a sample \(S_{0}\) and a set \(B\subset\mathcal{X}\), let us denote \(S_{0}(B)=\frac{|S_{0}\cap B|}{|S_{0}|}\). Because of the \(\frac{\varepsilon}{9}\)-representativeness of \(S_{2}^{\prime\prime}\), we have for every \(B\in\mathcal{Y}(\hat{\mathcal{H}})\):

\[|p(B)-S_{2}^{\prime\prime}(B)|\leq\frac{\varepsilon}{9}.\]

Thus,

\[|p(B)-S_{2}(B)|\] \[=\left|p(B)-\frac{|S_{2}\cap B|}{|S_{2}|}\right|\] \[\leq\max\left\{\left|p(B)-\frac{|S_{2}^{\prime\prime}\cap B|}{|S_ {2}|}\right|,\left|p(B)-\frac{|S_{2}^{\prime\prime}\cap B|+(\eta+\frac{2 \varepsilon}{9})n_{2}}{|S_{2}|}\right|\right\}\] \[\leq\max\left\{\left|p(B)-\frac{S_{2}^{\prime\prime}(B)|S_{2}^{ \prime\prime}|}{n_{2}}\right|,\left|p(B)-\frac{S_{2}^{\prime\prime}(B)|S_{2}^{ \prime\prime}|+(\eta+\frac{2\varepsilon}{9})n_{2}}{n_{2}}\right|\right\}\] \[\leq\max\left\{\left|p(B)-\left(1-\eta-\frac{2\varepsilon}{9} \right)S_{2}^{\prime\prime}(B)\right|,\left|p(B)-\frac{S_{2}^{\prime\prime}(B) \left(1-\eta-\frac{2\varepsilon}{9}\right)n_{2}+\left(\eta+\frac{2\varepsilon} {9}\right)n_{2}}{n_{2}}\right|\right\}\] \[\leq\max\left\{\left|p(B)-S_{2}^{\prime\prime}(B)\right|+\left|S_ {2}^{\prime\prime}(B)-\left(1-\eta-\frac{2\varepsilon}{9}\right)S_{2}^{\prime \prime}(B)\right|,\left|p(B)-\left(S_{2}^{\prime\prime}(B)\left(1-\eta-\frac{ 2\varepsilon}{9}\right)+\left(\eta+\frac{2\varepsilon}{9}\right)\right)\right|\right\}\] \[\leq\max\left\{\frac{3\varepsilon}{9}+\eta,\left|p(B)-S_{2}^{ \prime\prime}(B)\left(1-\eta-\frac{2\varepsilon}{9}\right)-\left(\eta+\frac{2 \varepsilon}{9}\right)\right|\right\}\] \[\leq\max\left\{\frac{3\varepsilon}{9}+\eta,\left|p(B)-S_{2}^{ \prime\prime}(B)+S_{2}^{\prime\prime}(B)\left(\eta+\frac{2\varepsilon}{9} \right)-\left(\eta+\frac{2\varepsilon}{9}\right)\right|\right\}\] \[\leq\max\left\{\frac{3\varepsilon}{9}+\eta,\frac{\varepsilon}{9 }+\left|S_{2}^{\prime\prime}(B)-1\right|\left(\eta+\frac{2\varepsilon}{9} \right)\right\}\] \[\leq\max\left\{\frac{3\varepsilon}{9}+\eta,\frac{\varepsilon}{9 }+\left(\eta+\frac{2\varepsilon}{9}\right)\right\}\] \[\leq\frac{3\varepsilon}{9}+\eta\]

Let the empirical \(A\)-distance with respect to the Yatracos sets be defined by

\[d_{\mathcal{Y}(\hat{\mathcal{H}})}(q,S)=\sup_{B\in d_{\mathcal{Y}(\hat{ \mathcal{H}})}}|q(B)-S(B)|.\]Now if the learner outputs \(\hat{q}\in\arg\min_{q\in\hat{\mathcal{H}}}d_{\mathcal{Y}(\hat{\mathcal{H}})}(q,S)\), then putting all of our guarantees together, we get that with probability \(1-\delta\)

\[d_{\mathrm{TV}}(\hat{q},p) \leq\frac{2\varepsilon}{9}+d_{\mathcal{Y}(\hat{\mathcal{H}})}( \hat{q},p)\] \[\leq\frac{5\varepsilon}{9}+\eta+d_{\mathcal{Y}(\hat{\mathcal{H}}) }(\hat{q},S_{2})\] \[\leq\frac{5\varepsilon}{9}+\eta+d_{\mathcal{Y}(\hat{\mathcal{H}}) }(q^{*},S_{2})\] \[\leq\frac{8\varepsilon}{9}+2\eta+d_{\mathcal{Y}(\hat{\mathcal{H}} )}(q^{*},p)\] \[\leq\frac{8\varepsilon}{9}+2\eta+\frac{\varepsilon}{9}\leq 2\eta+\varepsilon.\]

Appendix C Robust Learnability with Subtractive Contamination Implies Robust Learnability with General Contamination

In this section we will provide the proof for Theorem 1.7.

**Theorem 1.7**.: _If a class \(\mathcal{C}\) is \(\eta\)-subtractive \(\alpha\)-robustly learnable, then it is also \(\eta\)-\((2\alpha+4)\)-robustly learnable._

Proof.: Let \(\mathcal{C}\) be a concept class that is \(\eta\)-subtractively \(\alpha\)-robust learnable. Then there exists a successful \(\eta\)-subtractive \(\alpha\)-robust learner \(\mathcal{A}_{\mathcal{C}}^{sub}\) with sample complexity \(n_{\mathcal{C}}^{sub}\) for the class \(\mathcal{C}\). Let \(\varepsilon<\frac{9\eta}{2}\) and \(\delta\) and be arbitrary.

Let \(n_{1}\geq\max\left\{2n_{\mathcal{C}}^{sub}(\frac{\varepsilon}{9},\frac{ \delta}{5})/(1-\eta-\frac{2\varepsilon}{9}),\frac{162(1+\log(\frac{5}{9}))}{ \varepsilon^{2}}\right\}\) and \(n_{2}\geq\left\{\frac{(4(\eta+\frac{2\varepsilon}{9})n_{1}\log(n_{1})+\log( \frac{5}{9})))}{\varepsilon^{2}},\frac{162(1+\log(\frac{5}{9}))}{\varepsilon^ {2}}\right\}\). Lastly let \(n=n_{1}+n_{2}\).

Let \(p\in\mathcal{C}\) be arbitrary. The \(\alpha\)-\(\eta\)-robust learner receives a sample \(S\sim p^{m}\) such that there is \(q\in\mathcal{C}\) such that \(d_{\mathrm{TV}}(p,q)=\eta\). Thus there exists a distributions \(q_{1},q_{2},q_{3}\), such that \((1-\eta)q_{1}+\eta q_{2}=p\) and \((1-\eta)q_{1}+q_{3}=q\). We now use the same learning strategy as in Theorem 1.5: We split the sample randomly into two subsamples \(S_{1}\) and \(S_{2}\), where we use \(S_{1}\) to learn candidate sets and then use \(S_{2}\) to select the hypothesis from the candidate set. The goal in both settings is to find as close an approximation to \(q_{1}\) as possible. The candidate based on \(S_{1}\) is created by feeding subsamples of \(S_{1}\) into the subtractively robust learner in such a way that with high probability one of the subsamples is guaranteed to be i.i.d. generated by \(q_{1}\) and thus (with high probability) yield a good hypothesis. More precisely, the learner randomly splits the sample \(S\) into \(S_{1}\) and \(S_{2}\) with \(|S_{1}|=n_{1}\) and \(|S_{1}|=n_{2}\). We now define the "clean" part of \(S^{\prime}\subset S\), i.e. the part of \(S^{\prime}\) that is i.i.d. distributed according to \(q_{1}\). We note that the size of this "clean" sample \(|S^{\prime}|=n^{\prime}\) is a random variable and distributed according to the binomial distributions \(Binom(n,1-\eta)\). Now applying Chernoff bound, with the same argument as in the proof of Theorem 1.5, we get that with probability \(1-\frac{\delta}{5}\), we have \(n^{\prime}\geq n(1-\eta-\frac{\varepsilon}{9})\). Now let \(S^{\prime}_{1}=S_{1}\cap S^{\prime}\) and \(S^{\prime}_{2}=S_{2}\cap S^{\prime}\) be the "clean parts" of the subsamples \(S_{1}\) and \(S_{2}\) respectively. The sizes \(|S^{\prime\prime}_{1}|=n^{\prime}_{1}\) and \(|S^{\prime\prime}_{2}|=n^{\prime\prime}_{2}\) We note that \(n^{\prime}_{1}\sim Hypergeometric(n,n(1-\eta),n_{1})\) and \(n^{\prime}_{2}\sim Hypergeometric(n,n(1-\eta),n_{2})\). Thus,

\[Pr_{\text{random split}}\left[|S^{\prime}_{1}|\leq\left(1-\eta-\frac{2 \varepsilon}{9}\right)n_{1}\right]\leq e^{-\frac{2}{81}\varepsilon^{2}n_{1}}\]

and

\[Pr_{\text{random split}}\left[|S^{\prime}_{2}|\leq\left(1-\eta-\frac{2 \varepsilon}{9}\right)n_{2}\right]\leq e^{-\frac{2}{81}\varepsilon^{2}n_{2}}.\]Taking together the guarantees on our random splits and the size of \(n^{\prime}\), we note that by our choices of \(n_{1}\) and \(n_{2}\) with probability \(1-\frac{3\delta}{5}\), the fractions of the parts that are i.i.d. generated by \(q_{1}\) (namely \(\frac{|S^{\prime\prime}_{1}|}{|S^{\prime}_{1}|}\) and \(\frac{|S^{\prime\prime}_{1}|}{|S^{\prime}_{2}|}\)) are at least \((1-\eta-\frac{2\varepsilon}{9})\). Going forward we will assume that this is indeed the case.

Let

\[\hat{\mathcal{H}}=\left\{\mathcal{A}^{sub}_{\mathcal{Q}}(\tilde{S}):\tilde{S} \subset S^{\prime}_{1}\text{ with }|\tilde{S}|=\left(1-\eta-\frac{2\varepsilon}{9} \right)n_{1}\right\}.\]

Using our assumption that \(|S^{\prime}_{1}|\geq(1-\eta-\frac{2\varepsilon}{9})n_{1}\), we know that there is \(S^{\prime\prime}_{1}\subset S^{\prime}_{1}\) with \(\mathcal{A}^{sub}_{\mathcal{Q}}(S^{\prime\prime}_{1})\in\mathcal{H}^{\prime}\). As \(S^{\prime\prime}_{1}\sim q_{1}^{(1-\eta-\frac{2\varepsilon}{9})n_{1}}\), by the learning guarantee of \(\mathcal{A}^{sub}_{\mathcal{Q}}\) with probability \(1-\frac{\delta}{5}\), there is a candidate distribution \(q^{*}\in\hat{\mathcal{H}}\) with \(d_{\mathrm{TV}}(p,q^{*})\leq d_{\mathrm{TV}}(p,q_{1})+d_{\mathrm{TV}}(q_{1},q^ {*})=\eta+(\alpha\eta+\frac{\varepsilon}{9})=(\alpha+1)\eta+\frac{\varepsilon} {9}\).

We now consider the Yatracos sets. For every \(q_{i}\), \(q_{j}\in\hat{\mathcal{H}}\), let \(A_{i,j}=\{x:\;q_{i}(x)\geq q_{j}(x)\}\) and let

\[\mathcal{Y}(\hat{\mathcal{H}})=\left\{A_{ij}\subset\mathcal{X}:q_{i},q_{j}\in \hat{\mathcal{H}}\right\}.\]

We now consider the \(A\)-distance [1] between two distributions with respect to the Yatracos sets, i.e., we consider

\[d_{\mathcal{Y}(\hat{\mathcal{H}})}(p^{\prime},q^{\prime})=\sup_{B\in\mathcal{Y }(\hat{\mathcal{H}})}|p^{\prime}(B)-q^{\prime}(B)|.\]

We note, that for any two distributions \(p^{\prime},q^{\prime}\) we have \(d_{\mathrm{TV}}(p^{\prime},q^{\prime})\geq d_{\mathcal{Y}(\hat{\mathcal{H}})} (p^{\prime},q^{\prime})\). Furthermore, if \(q^{\prime},p^{\prime}\in\hat{\mathcal{H}}\), then \(d_{\mathrm{TV}}(p^{\prime},q^{\prime})=d_{\mathcal{Y}(\hat{\mathcal{H}})}(p^{ \prime},q^{\prime})\). Assume there is \(q^{*}\in\hat{\mathcal{H}}\) with \(d_{\mathrm{TV}}(p,q^{*})\leq(\alpha+1)\eta+\frac{\varepsilon}{9}\), then for every \(q\in\hat{\mathcal{H}}\):

\[d_{\mathrm{TV}}(p,q) \leq d_{\mathrm{TV}}(p,q^{*})+d_{\mathrm{TV}}(q^{*},q)\] \[\leq(\alpha+1)\eta+\frac{\varepsilon}{9}+d_{\mathcal{Y}(\hat{ \mathcal{H}})}(q^{*},q)\] \[\leq(\alpha+1)\eta+\frac{\varepsilon}{9}+d_{\mathcal{Y}(\hat{ \mathcal{H}})}(q^{*},p)+d_{\mathcal{Y}(\hat{\mathcal{H}})}(p,q)\] \[\leq 2(\alpha+1)\eta+\frac{2\varepsilon}{9}+d_{\mathcal{Y}(\hat{ \mathcal{H}})}(p,q).\]

Lastly, we will argue that we can empirically approximate \(d_{\mathcal{Y}(\hat{\mathcal{H}})}(p,q)\), which we can then use to select a hypothesis. We note that, since \(\mathcal{Y}(\hat{\mathcal{H}})\) is a finite set of size \(|\mathcal{Y}(\hat{\mathcal{H}})|=(\binom{n_{1}}{(1-\eta-\frac{2\varepsilon}{9 })n_{1}})^{2}\leq n_{1}^{2(\eta+\frac{2\varepsilon}{9})n_{1}}\), we have uniform convergence with respect to \(\mathcal{Y}(\hat{\mathcal{H}})\). Recall that \(S^{\prime}_{2}\sim q_{1}^{n^{\prime}_{2}}\) and by our previous assumption \(n^{\prime}_{2}\leq n_{2}\left(1-\eta-\frac{2\varepsilon}{9}\right)\). Thus by our choice of \(n_{2}\), with probability \(1-\frac{\delta}{5}\), there is \(S^{\prime\prime}_{2}\subset S^{\prime}_{2}\subset S_{2}\) with \(|S^{\prime\prime}_{2}|=\left(1-\eta-\frac{2\varepsilon}{9}\right)n_{2}\) such that \(S^{\prime\prime}_{2}\) is \(\frac{\varepsilon}{9}\)-representative of \(q_{1}\) with respect to \(\mathcal{Y}(\hat{\mathcal{H}})\). For a sample \(S_{0}\) and a set \(B\subset\mathcal{X}\), let us denote \(S_{0}(B)=\frac{|S_{0}\cap B|}{|S_{0}|}\). Because of the \(\frac{\varepsilon}{9}\)-representativeness of \(S^{\prime}_{2}\), we have for every \(B\in\mathcal{Y}(\hat{\mathcal{H}})\):

\[|q_{1}(B)-S^{\prime\prime}_{2}(B)|\leq\frac{\varepsilon}{9}\]

Thus,\[|q_{1}(B)-S_{2}(B)| \leq|q_{1}(B)-S_{2}^{\prime}(B)|+|S_{2}^{\prime\prime}(B)-S_{2}(B)|\] \[\leq\frac{\varepsilon}{9}+\left|\frac{|S_{2}\cap B|}{|S_{2}|}- \frac{|S_{2}^{\prime\prime}\cap B|}{|S_{2}^{\prime\prime}|}\right|\] \[\leq\frac{\varepsilon}{9}+\left|\frac{|S_{2}\cap B|}{n_{2}}-\frac {|S_{2}^{\prime\prime}\cap B|}{n_{2}(1-\eta-\frac{2\varepsilon}{9})}\right|\] \[=\frac{\varepsilon}{9}+\left|\frac{|S_{2}\cap B|(1-\eta-\frac{2 \varepsilon}{\eta})-|S_{2}^{\prime\prime}\cap B|}{(1-\eta-\frac{2\varepsilon}{ 9})n_{2}}\right|\] \[\leq\frac{\varepsilon}{9}+\max\{\frac{|(|S_{2}^{\prime\prime}\cap B |+(\eta+\frac{2\varepsilon}{9})n_{2})(1-\eta-\frac{2\varepsilon}{9})-|S_{2}^{ \prime\prime}\cap B||}{(1-\eta-\frac{2\varepsilon}{9})n_{2}},\] \[\frac{|(|S_{2}^{\prime\prime}\cap B|(1-\eta-\frac{2\varepsilon}{9 })-|S_{2}^{\prime\prime}\cap B||}{(1-\eta-\frac{2\varepsilon}{9})n_{2}}\}\] \[\leq\frac{\varepsilon}{9}\] \[+\max\{\frac{|(|S_{2}^{\prime\prime}\cap B|+(\eta+\frac{2 \varepsilon}{9})n_{2})(1-\eta-\frac{2\varepsilon}{9})-((1-\eta-\frac{2 \varepsilon}{9})|S_{2}^{\prime\prime}\cap B|+(\eta+\frac{2\varepsilon}{9})|S_{2 }^{\prime\prime}\cap B|)|}{n_{2}(1-\eta-\frac{2\varepsilon}{9})},\] \[\frac{n_{2}(\eta+\frac{2\varepsilon}{9})}{n_{2}(1-\eta-\frac{2 \varepsilon}{9})}\}\] \[\leq\frac{\varepsilon}{9}+\max\left\{\frac{|(\eta+\frac{2 \varepsilon}{9})n_{2}(1-\eta-\frac{2\varepsilon}{9})-|S_{2}^{\prime\prime}\cap B |(\eta+\frac{2\varepsilon}{9})|}{(1-\eta-\frac{2\varepsilon}{9})n_{2}},\frac{ (\eta+\frac{2\varepsilon}{9})}{(1-\eta-\frac{2\varepsilon}{9})}\right\}\] \[\leq\frac{\varepsilon}{9}+\max\left\{\frac{|(\eta+\frac{2 \varepsilon}{9})(n_{2}(1-\eta-\frac{2\varepsilon}{9})|)}{(1-\eta-\frac{2 \varepsilon}{9})n_{2}},\eta+\frac{2\varepsilon}{9})\right\}\] \[\leq\frac{\varepsilon}{9}+\eta+\frac{2\varepsilon}{9}\leq\frac{3 \varepsilon}{9}+\eta\]

Let us remember that the empirical \(A\)-distance with respect to the Yatracos is defined by

\[d_{\mathcal{Y}(\hat{\mathcal{H}})}(q,S)=\sup_{B\in\mathcal{Y}}|q(B)-S(B)|\,.\]

Now if the learner outputs \(\hat{q}\in\arg\min_{q\in\hat{\mathcal{H}}}d_{\mathcal{Y}(\hat{\mathcal{H}})}( q,S_{2})\), then putting all of our guarantees together, with probability \(1-\delta\) we get

\[d_{\mathrm{TV}}(\hat{q},p) \leq 2(\alpha+1)\eta+\frac{2\varepsilon}{9}+d_{\mathcal{Y}}(\hat{q},p)\] \[\leq 2(\alpha+1)\eta+\frac{2\varepsilon}{9}+d_{\mathcal{Y}}(\hat{ q},q_{1})+d_{\mathcal{Y}}(q_{1},p)\] \[\leq 2(\alpha+1)\eta+\frac{2\varepsilon}{9}+\eta+(\eta+\frac{3 \eta}{9})+d_{\mathcal{Y}}(\hat{q},S_{2})\] \[\leq 2(\alpha+2)\eta+\frac{5\varepsilon}{9}+d_{\mathcal{Y}}(\hat{ q},S_{2})\] \[\leq 2(\alpha+2)\eta+\frac{5\varepsilon}{9}+d_{\mathcal{Y}}(q^{*}, S_{2})\] \[\leq 2(\alpha+2)\eta+\frac{5\varepsilon}{9}+(\eta+\frac{3 \varepsilon}{9})+d_{\mathcal{Y}}(q^{*},q_{1})\] \[\leq(2\alpha+3)\eta+\frac{8\varepsilon}{9}+\eta+d_{\mathcal{Y}}( q^{*},p)\] \[\leq(2\alpha+4)\eta+\varepsilon+d_{\mathrm{TV}}(q^{*},p).\]Learnability Does Not Imply Robust Learnability

We start with an upper bound, showing that our class \(\mathcal{Q}_{g}\) is realizably learnable.

**Claim 3.2**.: _For a monotone function \(g:\mathbb{N}\to\mathbb{N}\), let \(\mathcal{Q}_{g}=\{q_{i,j,g(j)}:i,j\in\mathbb{N}\}\). Then, the sample complexity of \(\mathcal{Q}_{g}\) in the realizable case is upper bounded by_

\[n_{\mathcal{Q}_{g}}^{rc}(\varepsilon,\delta)\leq\log(1/\delta)g(1/\varepsilon).\]

Proof.: Let the realizable learner \(\mathcal{A}\) be

\[\mathcal{A}(S)=\begin{cases}q_{i,j,g(j)}&\text{if }(i,2j+2)\in S\\ \delta_{(0,0)}&\text{otherwise}\end{cases}\]

Note that for all \(\mathcal{Q}_{g}\)-realizable samples this learner is well-defined. Furthermore, we note that in the realizable case, whenever \(\mathcal{A}\) outputs a distribution different from \(\delta_{(0,0)}\), then \(\mathcal{A}(S)\) outputs the ground-truth distribution, i.e., the output has TV-distance \(0\) to the true distribution. Lastly, we note, that for an i.i.d. sample \(S\sim q_{i,j,g(j)}^{n}\), we have the following upper bound for the learner identifying the correct distribution:

\[\mathbb{P}_{S\sim q_{i,j,g(j)}^{n}}[\mathcal{A}(S)=q_{(j)}]=\mathbb{P}_{S \sim q_{i,j,g(j)}^{n}}[(i,2j+2)\in S]=1-(1-1/g(j))^{n}.\]

We note, that since \(g\) is a monotone function, if \(\varepsilon\leq\frac{1}{j}\), then \(g(j)\leq g(\frac{1}{\varepsilon})\) and therefore,

\[(1-1/g(j))^{n}\leq(1-1/g(1/\varepsilon))^{n}.\]

Furthermore for \(q_{i,j,g(j)}\), we have that \(d_{\mathrm{TV}}(\delta_{(0,0)},q_{i,j,g(j)})=\frac{1}{j}\).

Putting these two observations together, we get

\[\mathbb{P}_{S\sim q_{i,j,g(j)}^{n}}[d_{\mathrm{TV}}(\mathbb{A}(S),q_{i,j,g(j)} )\geq\varepsilon]\leq\begin{cases}(1-1/g(1/\varepsilon))^{n}&\text{if }\frac{1}{j} \leq\varepsilon\\ 0&\text{if }\frac{1}{j}>\varepsilon\end{cases}.\]

Thus, for every \(q\in\mathcal{Q}_{g}\),

\[\mathbb{P}_{S\sim q^{n}}[d_{\mathrm{TV}}(\mathbb{A}(S),q)\geq\varepsilon] \leq(1-1/g(1/\varepsilon))^{n}\leq\exp\left(-\frac{n}{g(1/\varepsilon)}\right).\]

Letting the left-hand side equal the failure probability \(\delta\) and solving for \(n\), we get,

\[\log\delta \geq\frac{-n}{g(1/\varepsilon)}\] \[\log(\delta)g(1/\varepsilon) \geq-n\] \[n \geq-\log(\delta)g(1/\varepsilon)=\log(1/\delta)g(1/\varepsilon).\]

Thus, we have a sample complexity bound of

\[n_{Q_{g}}(\varepsilon,\delta)\leq\log(1/\delta)g(1/\varepsilon).\]

Now, we show a lower bound, that our class \(\mathcal{Q}_{g}\) is _not_ robustly learnable. Before we do that, we require a few more preliminaries. For a distribution class \(\mathcal{Q}\) and a distribution \(p\), let their total variation distance be defined by

\[d_{\mathrm{TV}}(p,\mathcal{Q})=\inf_{q\in\mathcal{Q}}d_{\mathrm{TV}}(p,q).\]

We also use the following lemma from [1].

**Lemma D.1** (Lemma 3 from [1]).: _Let \(\mathcal{P}_{\eta,4k}=\{(1-\eta)\delta_{(0,0)}+\eta U_{A\times\{2j+1\}}:A \subset[4k]\}\) For \(\mathcal{Q}=\mathcal{P}_{\eta,4k}\), we have \(n_{\mathcal{Q}}^{rc}(\frac{\eta}{8},\frac{1}{7})\geq k\)._Finally, we recall the definition of weak learnability, which says that a distribution class is learnable only for some particular value of the accuracy parameter.

**Definition D.2**.: _A class \(\mathcal{Q}\) is \(\varepsilon\)-weakly learnable, if there is a learner \(\mathcal{A}\) and a sample complexity function \(n:(0,1)\to\mathbb{N}\), such that for ever \(\delta\in(0,1)\) and every \(p\in\mathcal{Q}\) and every \(n\geq n(\delta)\),_

\[\mathbb{P}_{S\sim p^{n}}[d_{\mathrm{TV}}(\mathcal{A}(S),p)\leq\varepsilon]<\delta.\]

Learnability clearly implies \(\varepsilon\)-weak learnability for every \(\varepsilon\in(0,1)\). While in some learning models (e.g., binary classification) learnability and weak learnability are equivalent, the same is not true for distribution learning [1].

We are now ready to prove that \(\mathcal{Q}_{g}\) is not robustly learnable.

**Claim 3.3**.: _For every function \(g\in\omega(n)\) the class \(\mathcal{Q}_{g}\) is not \(\alpha\)-robustly learnable for any \(\alpha>0\)._

Proof.: Consider

\[q^{\prime}_{i,j}=\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\frac{1}{j}U_{A_{i} \times\{2j+1\}}\]

Note that \(d_{\mathrm{TV}}(q^{\prime}_{i,j},\mathcal{Q}_{g})=\frac{1}{g(j)}\). Therefore, in order to show that \(\mathcal{Q}_{g}\) is not \(\alpha\)-robustly learnable, it is sufficient to show that there are \(j\) and \(\varepsilon\), such that the class \(\mathcal{Q}^{\prime}_{j}=\{q^{\prime}_{i,j}:i\in\mathbb{N}\}\) is not \((\frac{\alpha}{g(j)}+\varepsilon)\)-weakly learnable.

We will now show that for any \(\gamma<\frac{1}{8j}\), the class \(\mathcal{Q}^{\prime}_{j}=\{q^{\prime}_{i,j}:i\in\mathbb{N}\}\) is not \(\gamma\)-weakly learnable. Recalling notation from Lemma D.1, we note that that for every \(n\in\mathbb{N}\) the class \(P_{\frac{1}{j},4k}\subset\mathcal{Q}^{\prime}_{j}\). By monotonicity of the sample complexity and Lemma D.1, we have \(n_{\mathcal{Q}^{\prime}_{j}}(\frac{1}{8j},\frac{1}{j})\geq n_{P_{1/j,4n}}( \frac{1}{8j},\frac{1}{7})\geq n\), proving that this class is not weakly learnable.

Lastly, we need to show that for every \(\alpha\), there are \(\varepsilon\) and \(j\), such that this claim holds for \(\gamma\leq(\frac{\alpha}{g(j)}+\varepsilon)\). That is, we need to show that there are \(\varepsilon\) and \(j\), such that

\[\frac{\alpha}{g(j)}+\varepsilon<\frac{1}{8j}.\]

Let \(\varepsilon=\frac{1}{16j}\). Now let \(g\) be any superlinear function, i.e., for every \(c\in\mathbb{R}\), there is \(t_{c}\in\mathbb{N}\), such that for every \(t\geq t_{c}\), \(g(t)\geq ct\). This implies that, for any \(\alpha\in\mathbb{R}\), there is \(j\in\mathbb{N}\) such that \(g(j)>16j\alpha\). Thus for any super-linear function \(g\) and any \(\alpha\in\mathbb{R}\), the class \(\mathcal{Q}_{g}\) is not \(\alpha\)-robustly learnable.

### Proof of Theorem 1.6

The result of Theorem 1.6 follows directly from the construction of class \(\mathcal{Q}_{g}\) for Theorem 3.1, the Claim 3.2 that shows this class is realizable learnable, and an adapated version for Claim 3.3, which states the following:

**Claim D.3**.: _For every \(\alpha\), there is \(g(t)\in O(t^{2})\), such that for every \(0\leq\eta\leq\frac{1}{16\alpha}\) the class \(\mathcal{Q}_{g}\) is not \(\eta\)-subtractive \(\alpha\)-robustly learnable._

Proof.: Let \(\alpha>1\) be arbitrary. Let \(g:\mathbb{N}\to\mathbb{N}\) be defined by \(g(t)=32\alpha t^{2}\) for all \(t\in\mathbb{N}\). Now for every \(0\leq\eta\leq\frac{1}{16\alpha}\), there exists some \(j\), such that \(\frac{j}{g(j)}\cdot\leq\eta\leq\frac{1}{16\alpha j}\)

For such \(j\), we consider the distributions

\[q^{\prime}_{i,j}=\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\frac{1}{j}U_{A_{i} \times\{2j+1\}}\]

as in the proof of Claim 3.2. Recall that the element of \(\mathcal{Q}_Then we have,

\[q_{i,j,g(j)} =\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\left(\frac{1}{j}-\frac{1}{ g(j)}\right)U_{A_{i}\times\{2j+1\}}+\frac{1}{g(j)}\delta_{(i,2j+2)}=\] \[=\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\left(\frac{g(j)-j}{jg( j)}\right)U_{A_{i}\times\{2j+1\}}+\frac{j}{jg(j)}\delta_{(i,2j+2)}=\] \[=\left(\frac{g(j)-j}{g(j)}\right)\left(\left(1-\frac{1}{j}\right) \delta_{(0,0)}+\frac{1}{j}U_{A_{i}\times\{(2,j+1)\}}\right)+\frac{j}{g(j)} \left(\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\frac{1}{j}\delta_{(i,2j+2)}\right)\] \[=\left(1-\frac{j}{g(j)}\right)q^{\prime}_{i,j}+\frac{j}{g(j)} \left(\left(1-\frac{1}{j}\right)\delta_{(0,0)}+\frac{1}{j}\delta_{(i,2j+2)} \right).\]

Thus for every element \(q^{\prime}_{i,j}\) of the class \(\mathcal{Q}^{\prime}_{j}=\{q^{\prime}_{i,j}:i\in\mathbb{N}\}\), there is a distribution \(p\), such that \((1-\eta)q^{\prime}_{i,j}+\eta p\in\mathcal{Q}_{g}\). That is, every element of \(\mathcal{Q}^{\prime}_{j}\) results from the \(\eta\)-subtractive contamination of some element in \(\mathcal{Q}_{g}\). Thus, for showing that \(\mathcal{Q}_{g}\) is not \(\eta\)-subtractive \(\alpha\)-robustly learnable, it is sufficient to show, that \(\mathcal{Q}^{\prime}_{j}\) is not \((\alpha\eta+\varepsilon)\)-weakly learnable for \(\varepsilon=\frac{1}{16j}\). As we have seen in the proof of Claim 3.3, we can use Lemma D.1 to show that for every \(n\), we have \(n_{\mathcal{Q}^{\prime}_{j}}(\frac{1}{8j},\frac{1}{7})\geq n\). Lastly, we need that \(\frac{1}{8j}\geq\alpha\eta+\varepsilon\), or after replacing \(\varepsilon\), we need \(\frac{1}{16j}\geq\alpha\eta\). This follows directly from the choice of \(g\).

## Appendix E Existence of sample compression schemes

We adopt the [ABDH\({}^{+}\)20] definition of sample compression schemes. We will let \(\mathcal{C}\) be a class of distribution over some domain \(\mathcal{X}\). A compression scheme for \(\mathcal{C}\) involves two parties: an encoder and a decoder.

* The encoder has some distribution \(q\in\mathcal{C}\) and receives \(n\) samples from \(q\). They send a succinct message (dependent on \(q\)) to the decoder, which will allow the decoder to output a distribution close to \(q\). This message consists of a subset of size \(\tau\) of the \(n\) samples, as well as \(t\) additional bits.
* The decoder receives the \(\tau\) samples and \(t\) bits and outputs a distribution which is close to \(q\).

Since this process inherently involves randomness (of the samples drawn from \(q\)), we require that this interaction succeeds at outputting a distribution close to \(q\) with only constant probability.

More formally, we have the following definitions for a decoder and a (robust) compression scheme.

**Definition E.1** (decoder, Definition 4.1 of [ABDH\({}^{+}\)20]).: _A decoder for \(\mathcal{C}\) is a deterministic function \(\mathcal{J}:\bigcup_{n=0}^{\infty}\mathcal{X}^{n}\times\bigcup_{n=0}^{\infty} \{0,1\}^{n}\to\mathcal{C}\), which takes a finite sequence of elements of \(\mathcal{X}\) and a finite sequence of bits, and outputs a member of \(\mathcal{C}\)._

The formal definition of a compression scheme follows.

**Definition E.2** (robust compression schemes, Definition 4.2 of [ABDH\({}^{+}\)20]).: _Let \(\tau,t,n:(0,1)\to\mathbb{Z}_{\geq 0}\) be functions, and let \(r\geq 0\). We say \(\mathcal{C}\) admits \((\tau,t,n)\)\(r\)-robust compression if there exists a decoder \(\mathcal{J}\) for \(\mathcal{C}\) such that for any distribution \(q\in\mathcal{C}\) and any distribution \(p\) on \(\mathcal{X}\) with \(d_{\mathrm{TV}}(p,q)\leq r\), the following holds:_

* _For any_ \(\varepsilon\in(0,1)\)_, if a sample_ \(S\) _is drawn from_ \(p^{n(\varepsilon)}\)_, then, with probability at least_ \(2/3\)_, there exists a sequence_ \(L\) _of at most_ \(\tau(\varepsilon)\) _elements of_ \(S\)_, and a sequence_ \(B\) _of at most_ \(t(\varepsilon)\) _bits, such that_ \(d_{\mathrm{TV}}(\mathcal{J}(L,B),\mathcal{C})\leq r+\varepsilon\)_._

Note that \(S\) and \(L\) are sequences rather than sets, and can potentially contain repetitions.

**Theorem E.3** (Compression implies learning, Theorem 4.5 of [ABDH\({}^{+}\)20]).: _Suppose \(\mathcal{C}\) admits \((\tau,t,n)\)\(r\)-robust compression. Let \(\tau^{\prime}(\varepsilon)\tau(\varepsilon)+t(\varepsilon)\). Then \(\mathcal{C}\) can be \(\max\{3,2/r\}\)-learned in the agnostic setting using_

\[O\left(n\Big{(}\frac{\varepsilon}{6}\Big{)}\log\Big{(}\frac{1}{\delta}\Big{)}+ \frac{\tau^{\prime}(\varepsilon/6)\log(n(\varepsilon/6)\log_{3}(1/\delta))+\log (1/\delta)}{\varepsilon^{2}}\right)=\widetilde{O}\left(n\Big{(}\frac{ \varepsilon}{6}\Big{)}+\frac{\tau^{\prime}(\varepsilon/6)\log n(\varepsilon/ 6)}{\varepsilon^{2}}\right)\]samples. If \(\mathcal{Q}\) admits \((\tau,t,n)\) non-robust compression, then \(\mathcal{Q}\) can be learned in the realizable setting using the same number of samples._

**Theorem E.4**.: _The class \(\mathcal{Q}=\mathcal{Q}_{g}\) from Section 3 has a compression scheme of message size 1 (i.e., using just a single sample point)._

Proof.: Let \(n(\varepsilon)\) be \(10/g(\varepsilon)\) and, give a sample \(S\) of at least that size, let the encoder pick a subset \(L(S)\subseteq S\) be

\[L(S)=\begin{cases}\{(i,2j+2)\}&\text{ if }(i,2j+2)\in S\\ \{(0,0)\}&\text{ otherwise}\end{cases}\]

Let the decoder output

\[\mathcal{J}(L)=\begin{cases}q_{i,j,g(j)}&\text{ if }(i,2j+2)\in L\\ \delta_{(0,0)}&\text{ otherwise}\end{cases}\]

With this construction established, the analysis follows very similarly to the analysis in the proof of Claim 3.2. 

We note that the claim of Theorem 1.8 (and Theorem 3.4) follows directly.

## Appendix F Approximate DP learnability vs robust learnability

We prove the second claim in Theorem 4.3 by showing that the learner for the class \(\mathcal{Q}\) described in Theorem 3.1 can be made differentially private by employing stability-based histograms [13, 14].

**Proposition F.1** (Stability-based histograms [13, 14, 15], Lemma 4.1 from [1]).: _Let \(\mathcal{X}\) be a domain of examples. Let \(K\) be a countable index set, and let \((h_{k})_{k\in K}\) be a sequence of disjoint histogram bins over \(\mathcal{X}\). For every \((\alpha,\beta,\varepsilon,\delta)\in(0,1)^{4}\), there is an \((\varepsilon,\delta)\)-DP algorithm that takes a dataset \(S\) of size \(n\) and with probability \(\geq 1-\beta\), outputs bin frequency estimates \((f_{k})_{k\in K}\) such that_

\[\left|f_{k}-\frac{|\{x\in S:x\in h_{k}\}|}{n}\right|\leq\alpha\]

_for all \(k\in K\), so long as_

\[n\geq\Omega\left(\frac{\log(1/\beta\delta)}{\alpha\varepsilon} \right).\]

Proof of Theorem 4.3.: Recall the realizably-but-not-robustly learnable class of distributions \(\mathcal{Q}_{g}=\{q_{i,j,g(j)}:i,j\in\mathbb{N}\}\) over \(\mathbb{N}^{2}\) from Theorem 3.1, where \(g:\mathbb{N}\rightarrow\mathbb{N}\) is a monotone, super-linear function and \(q_{i,j,k}\) is as defined in (1).

Fix \((\alpha,\beta,\varepsilon,\delta)\in(0,1)^{4}\). We define our DP learner \(A_{DP}\) for \(\mathcal{Q}_{g}\) as follows: we take a sample \(S\) of size

\[n\geq\Omega\left(\frac{\log(1/(\beta/2)\delta)}{(1/4g(1/\alpha) )\varepsilon}\right)+32\log(1/(\beta/2))g(1/\alpha)\]

and run the stability-based histogram from Proposition F.1 targeting \((\varepsilon,\delta)\)-DP, with singleton histogram buckets \((h_{(a,b)})_{(a,b)\in\mathbb{N}^{2}}\), each \(h_{(a,b)}=\{(a,b)\}\). This yields frequency estimates \((f_{(a,b)})_{(a,b)\in\mathbb{N}^{2}}\) with \(|f_{(a,b)}-|\{x\in S:x=(a,b)\}||\leq 1/4g(1/\alpha)\) for all \((a,b)\in\mathbb{N}^{2}\) with probability \(\geq 1-\beta/2\). Then let

\[A_{DP}(S)=\begin{cases}q_{i,j,g(j)}&\text{ if }f_{(i,2j+2)}\geq 1/2g(1/\alpha)\\ \delta_{0}&\text{ otherwise.}\end{cases}\]

Note that by post-processing \(A_{DP}\) is indeed \((\varepsilon,\delta)\)-DP. Now suppose \(q_{i,j,g(j)}\) is our unknown distribution. There are two cases:

[MISSING_PAGE_EMPTY:27]