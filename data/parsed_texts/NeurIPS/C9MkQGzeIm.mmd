###### Abstract

Machine unlearning is the process of efficiently removing the influence of a training data instance from a trained machine learning model without retraining it from scratch. A popular subclass of unlearning approaches is _exact machine unlearning_, which focuses on techniques that explicitly guarantee the removal of the influence of a data instance from a model. Exact unlearning approaches use a machine learning model in which individual components are trained on disjoint subsets of the data. During deletion, exact unlearning approaches only retrain the affected components rather than the entire model. While existing approaches reduce retraining costs, it can still be expensive for an organization to retrain a model component as it requires halting a system in production, which leads to service failure and adversely impacts customers. To address these challenges, we introduce an exact unlearning framework - Sequence-aware **S**hared **S**liced Training (S\({}^{3}\)T), which is designed to enhance the deletion capabilities of an exact unlearning system while minimizing the impact on model's performance. At the core of S\({}^{3}\)T, we utilize a lightweight parameter-efficient fine-tuning approach that enables parameter isolation by sequentially training layers with disjoint data _slices_. This enables efficient unlearning by simply deactivating the layers affected by data deletion. Furthermore, to reduce the retraining cost and improve model performance, we train the model on multiple data sequences, which allows S\({}^{3}\)T to handle an increased number of deletion requests. Both theoretically and empirically, we demonstrate that S\({}^{3}\)T attains superior deletion capabilities across a wide range of settings.

## 1 Introduction

In recent years, the growing success of machine learning (ML) has led to its widespread deployment across a range of applications (Achaim et al., 2023; Team et al., 2023; Qayyum et al., 2020; Surden, 2021). Once a machine learning model has been trained, it is often necessary to _unlearn_ specific training data instances for various reasons, like complying with user data deletion requests (Mantelero, 2013; European Parliament & Council of the European Union; Shastri et al., 2019; Achille et al., 2024), removing stale or corrupt data (Biggio et al., 2012; Steinhardt et al., 2017), etc. Retraining an ML model entirely from scratch with each deletion request is expensive, especially for modern large-scale models (Brown et al., 2020; Achiam et al., 2023; Team et al., 2023). Machine unlearning (Nguyen et al., 2022; Xu et al., 2023) techniques focus on efficiently unlearning the influence of a data instance from a trained machine learning model.

Machine unlearning techniques are classified into two broad categories: approximate and exact unlearning (Xu et al., 2024). Approximate unlearning techniques (Guo et al., 2020; Liu et al., 2024) modify the parameters of a trained model to reduce the influence of the deleted data instance. While cost-effective, approximate unlearning cannot guarantee the complete removal of an instance's influence and it may still retain non-zero influence on the model. Moreover, auditing approximate unlearning is challenging due to the stochastic nature of ML optimization (Thudi et al., 2022). An alternative approach is exact unlearning (Cao & Yang, 2015; Bourtoule et al., 2021; Golatkar et al., 2023), which can guarantee the removal of a data instance's influence from a trained model. Exact unlearning techniques use a modular system, where different components within the system are trained on disjoint data subsets. When a deletion request occurs, only the affected component needs to be retrained. However, in real-world settings, halting a production system to even retrain a single component can result in service failure. The alternative is to function without the affected component, which may result in reduced performance, ultimately impacting consumers. To address these challenges, we introduce a novel exact unlearning framework, Sequence-aware Shared Sliced Training (S\({}^{3}\)T), which enhances the deletion capability while minimizing performance impact.

The key idea behind our S\({}^{3}\)T framework is to perform additional offline training before deploying the initial model to reduce retraining costs. At the core of S\({}^{3}\)T, we leverage a novel lightweight fine-tuning approach that allows parameter isolation by sequentially training model layers using disjoint data slices. Due to this parameter isolation, we efficiently perform exact unlearning by deactivating the layers associated with the deleted instance, rather than discarding the entire checkpoint. We efficiently train multiple models using different sequences of the same data slices depending on a training budget. We show that increasing the training budget before deployment can significantly reduce retraining costs and improve the model's performance. We also observe that it is important to train the model using diverse sequences and provide several approaches for selecting diverse sequences using graph matching (Cormen et al., 2022). Furthermore, we theoretically show that S\({}^{3}\)T achieves provably better deletion guarantees than existing approaches. We conduct extensive empirical evaluations to evaluate the effectiveness of S\({}^{3}\)T using Transformers with parameter counts ranging from 86M to 13B on a range of tasks. Additionally, we empirically validate the sequence selection algorithm and show that S\({}^{3}\)T has superior deletion performance compared to existing methods.

The rest of the paper is organized as follows: **(a)** We introduce the prior literature related to approximate and exact machine unlearning (Section 2), **(b)** We describe the problem setup for exact unlearning (Section 3.1), **(c)** We introduce the fine-tuning approach, S\({}^{3}\)T, and sequence selection algorithm under budget constraints (Section 3.2 & 3.3), **(d)** We theoretically analyze several properties of S\({}^{3}\)T (Section 3.4), and **(e)** We present experiments to evaluate the effectiveness of S\({}^{3}\)T's fine-tuning approach, deletion performance and sequence selection algorithm (Section 4).

## 2 Background

Machine unlearning techniques for deep learning is broadly classified into two categories: _approximate_ and _exact_ unlearning (Xu et al., 2024). Approximate unlearning techniques focus on reducing the influence of a deleted instance from a model after it has been trained. Exact unlearning techniques provide unlearning guarantees by ensuring model components trained on a deleted instance are not used during inference. In this section, we discuss each of these categories in detail.

**Approximate Machine Unlearning**. These techniques focus on approximating the model parameters as if the deleted data instance was not there in the training set from the beginning (Guo et al., 2020). These techniques typically quantify the influence of an instance (Koh and Liang, 2017) and perform gradient ascent for unlearning (Goldatkar et al., 2020; Neel et al., 2021; Sekhari et al., 2021; Gupta et al., 2021; Suriyakumar and Wilson, 2022; Liu et al., 2024a). In contrast to these approaches, (Graves et al., 2021) stores the exact gradients encountered during training and uses them directly for gradient ascent. Another line of work (Taru et al., 2023; Tu et al., 2023; Chen and Yang, 2023; Eldan and Russionovich, 2023; Patil et al., 2023; Kurmanji et al., 2024; Liu et al., 2024b) focuses on unlearning in a batch setting, where they assume access to both a retention set and a forget set of data instances for approximate unlearning. While efficient in practice, auditing approximate unlearning techniques is challenging due to the stochastic nature of the optimization process (Thudi et al., 2022; Wang et al., 2024) and may have weak privacy guarantees in practice (Hayes et al., 2024).

**Exact Machine Unlearning**. These techniques focus on developing a modular machine learning system, where individual components are trained using disjoint subsets of the data. Such a system offers the advantage that when a deletion request is received for an input instance, we only need to retrain the affected component rather than the entire model. However, these systems require modifying the original training process of the model. The seminal work for such a modular unlearning system is Sharded, Isolated, Sliced, and Aggregated training (SISA) (Bourtoule et al., 2021). SISA uses an ensemble of models each trained on a disjoint shard of the dataset as shown in Figure 1 (left). To further reduce retraining costs, each shard is divided into slices, and the models are incrementally trained on these slices, with their checkpoints stored sequentially (shown in Figure 1 (right)). If a deletion request is received for a data instance within 4th slice of a shard, we must retrieve the checkpoint from the 3rd slice and retrain using the remaining data within that shard. Several approaches focus on improving the components within SISA in application-specific settings like enhancing the dataset partitioning mechanism (Aldaghri et al., 2021; Yan et al., 2022), the retraining efficiency using light-weight adapters (Kumar et al., 2023; Dukler et al., 2023), or extending the SISA is a well-known framework that can guarantee exact unlearning and has found widespread applications. However, using SISA within production systems is challenging because retraining even a single component would result in system downtime. Furthermore, in the worst-case scenario, if deletion requests impact the first slice in all the shards, the entire service goes down, necessitating retraining the model from scratch. In this work, we leverage parameter-efficient fine-tuning to introduce a framework that improves upon the service availability and deletion capabilities of SISA.

## 3 Sequence-aware **S**harded **S**liced Training (S\({}^{3}\)T)

In this section, we describe the functioning of our proposed exact unlearning framework, **S**equence-aware **S**harded **S**liced Training (S\({}^{3}\)T).

### Problem Setting

We consider the general setting where the user fine-tunes a pre-trained model like BERT (Devlin et al., 2019) or Llama (Touvron et al., 2023) on private data using PEFT techniques. We assume that the deletion requests affect only the private fine-tuning data, not the pre-training data. In S\({}^{3}\)T, we partition a dataset \(\mathcal{D}=\{\mathcal{D}_{1},\dots,\mathcal{D}_{m}\}\) into \(m\) disjoint shards. Each shard is further divided into \(L\) slices: \(\mathcal{D}_{i}=\{S_{1},\dots,S_{L}\}\). S\({}^{3}\)T trains a separate model per shard and uses their aggregate decision.

Existing unlearning frameworks SISA (Bourtoule et al., 2021) use a similar setup described above. In SISA, within each shard, the model is trained in multiple stages sequentially on the slices (training stages are Slice 1, Slice 1+2, and so on), and their checkpoints are stored. However, a key weakness of SISA is that if deletion requests affect Slice 1 of all shards, then the entire service goes down necessitating retraining from scratch. Another drawback of SISA is that individual models within the ensemble need to be retrained whenever a deletion request is executed. Retraining even on a single slice is expensive for large-scale models in production serving a huge customer base. A naive alternative would be to use the last known usable checkpoint and perform retraining after regular intervals. For example, in Figure 1 (right), if a deletion request arrives for a data instance in Slice 4, the model in production can be replaced with the checkpoint obtained after Slice 3. It is easy to see that the performance of the overall model will degrade with the number of deletion requests.

We present an exact unlearning framework, S\({}^{3}\)T, to address these challenges. The core idea involves training several copies of each model (within the ensemble) that are trained using different slice sequences. When deletion requests occur, we utilize the model that minimizes performance degradation. To further reduce the training cost, we leverage PEFT techniques and present a novel sequential slice-wise fine-tuning strategy in Section 3.2. Our training strategy allows us to use the same model by deactivating certain layers without the need to swap checkpoints in case of a deletion. In the following sections, we introduce the fine-tuning strategy and sequence selection process.

Figure 1: Schematic diagram of the Sharded, Isolated, Sliced, and Aggregated training (SISA) (Bourtoule et al., 2021) framework. An ensemble of models is individually trained on disjoint shards. (_Left_) Each shard is further divided into slices. (_Right_) Each model is sequentially trained on the slices and checkpoints are stored. After deletion, retraining resumes from the best available checkpoint.

### Sequential Slice-wise Training

In this section, we introduce **S**equence-aware **S**hared **S**liced Training (S\({}^{3}\)T) a lightweight fine-tuning approach for efficient exact unlearning. This fine-tuning approach enables parameter isolation by sequentially training PEFT layers using different data slices. Due to this parameter isolation, it is possible to efficiently handle deletion requests by deactivating layers associated with the instance.

We describe S\({}^{3}\)T using the PEFT technique, LoRA (Hu et al., 2021), but our method is general can be easily extended to other PEFT techniques. LoRA introduces a small number of trainable low-rank (\(r\ll d\)) parameters, \((\mathbf{X},\mathbf{Y})\), while the pre-trained weights \(\overline{\mathbf{W}}\) remains fixed as shown below:

\[\mathbf{W}=\overline{\mathbf{W}}+\mathbf{X}^{\top}\mathbf{Y},\text{ where }\mathbf{X},\mathbf{Y}\in\mathbb{R}^{r\times d},\overline{\mathbf{W}}\in \mathbb{R}^{d\times d}.\] (1)

Our key idea involves training different LoRA layers using different data slices. This approach allows us to selectively deactivate (zero out) the LoRA parameters (\(\mathbf{X}\) or \(\mathbf{Y}\)) associated with a particular layer in the event of data deletion from that slice. In Figure 2 (left), we illustrate the training process in detail, where we follow a sequential top-to-bottom training approach. At stage 1, we train the final model layer (Layer 1 in the figure) using slice 1 while LoRA parameters from all other layers are switched off. In the next stage, we train second last layer (Layer 2) using slices 1 & 2, while keeping the LoRA parameters from the Layer 1 frozen. This process continues for the rest of the layers. Note that the training does not need to proceed in a single layer-wise fashion, we can even train multiple LoRA layers per slice. We discuss more details about the design choice in Appendix C.2.

The sequential slice-wise training process ensures that the LoRA parameter updates at the \(i\)-th layer are a function of the data instances within slices \(\{1,\dots,i\}\). Therefore, if a deletion request affects the \(i\)-th slice, the same model can still be used by deactivating the LoRA parameters corresponding to slices \(\{i,\dots,L\}\) (see details in Algorithm 4). For example, if a deletion request affects slice \(S_{3}\) only the subsequent LoRA layers need to be deactivated to ensure exact unlearning, as shown in the first example of Figure 2 (right). This is because during training the parameters of layers 1 & 2 were not affected by instances in \(S_{3}\). During this deletion process, we use the same model checkpoint and switch off LoRA layers, resulting in an \(L\)-time reduction in storage cost compared to SISA.

Now we consider the scenario where deletion requests affect multiple slices. This is shown in the \(2^{\text{nd}}\) sequence of Figure 2 (right), where slices \(S_{1}\) and \(S_{3}\) are affected. In this case, we observe that a model trained on the default ordering of the sequence \(\{S_{1},\dots,S_{L}\}\) is rendered useless when \(S_{1}\) and \(S_{3}\) are affected. This motivates us to train multiple models using different permutations of the slices. This would enhance the service time and system performance by selecting a model trained with the most effective ordering (e.g., the \(3^{\text{nd}}\) sequence in Figure 2 (right) yields the best-performing model). However, training on all \(L!\) slice permutations is prohibitively expensive. In the following section 3.3, we present strategies to select a diverse set of permutations under budget constraints.

Figure 2: (_Left_) We show the schematic diagram of the slice-wise training strategy in S\({}^{3}\)T. We incrementally train the model \(-i^{\text{th}}\) layer (from the top) using slices \(S_{1:i}\) while keeping the other layers fixed. (_Right_) We show the impact of deletion on models trained on different permutations of slices.

Using these selection approaches, in Section 3.4 we theoretically show that we do not need more than \(L\) sequences to achieve the optimal deletion performance.

### Training under Budget Constraints

In this section, we discuss strategies to select sequences under a budget, \(B\) (maximum number of sequences that can trained). First, we show that there exists an optimal subset of sequences and randomly selecting \(B\) sequences may not be effective. To illustrate this idea, we use a permutation tree (Bhattacharya, 1994), where all possible permutations are embedded into a tree structure.

In Figure 3 (left), we show an example of a permutation tree with \(L=3\) slices, paths from the root to the leaves correspond to unique permutation sequences \((S_{1},S_{2},S_{3})\), \((S_{1},S_{3},S_{2})\), and so on. We know that the topmost slice is most sensitive because if a deletion request affects the topmost slice the entire model needs to be retrained (shown in Figure 2 (right)). To address this and reduce the retraining cost, we should ensure we train models on sequences with different top slices. Building on this intuition, in the general setting we should train the model on diverse permutation sequences. Two sequences are considered _diverse if no element appears at the same position_, e.g., \((S_{1},S_{2},S_{3})\) and \((S_{2},S_{3},S_{1})\). An example is illustrated in Figure 3 (left), where a diverse set of 3 sequences is marked in green (where no identical slices occupy the same position). Selecting diverse permutations is challenging as relying solely on random sampling may not always yield the best results. Moreover, in certain scenarios, it is possible to have prior knowledge about the deletion probabilities of data slices; for example, younger users might be more likely to request data deletion than older users. Therefore, we present two strategies for selecting diverse permutation sequences for a budget \(B\), depending on whether or not prior deletion probabilities are available.

**Uniform Deletion Prior**. In the setting, where each slice has a uniform (or unknown) deletion prior, we can generate diverse sequences by using cyclic permutations of the original sequence. Given a sequence \((S_{1},S_{2},S_{3})\), the cyclic permutations are (shown in Figure 3 (middle)):

\[(S_{1},S_{2},S_{3})\rightarrow(S_{3},S_{1},S_{2})\rightarrow(S_{2},S_{3},S_{ 1}).\] (2)

Figure 4: Illustration of the BMS algorithm. BMS selects one element for each permutation at a time. This is done by constructing a bipartite graph with all feasible edges to the next node, where edge weights are the current sequence scores. We compute the maximum weight matching on this graph. The dark gray arrows (\(\rightarrow\)) indicate the selected edges and dotted arrows (\(\dashrightarrow\) ) the feasible ones.

Figure 3: Illustration of the slice sequence selection problem with uniform deletion prior under a budget constraint, \(B\). (_Left_) A permutation tree with \(L=3\) and a diverse set of sequences for budget \(B=3\) is shown in green. (_Center_) We show the functioning of the cyclic rotation algorithm, where we generate cyclic permutations of the original sequence. (_Right_) We iteratively extend the algorithm when budget \(B>L\) by generating cyclic rotations of the subsequences.

The above approach guarantees that no element is repeated in the same position. However, it can only generate up to \(L\) different sequences. For budget \(B>L\), we extend the cyclic rotation algorithm to generate more sequences. In Figure 3 (right), we generate new sequences by iterating over the existing sequences and performing cyclic rotations of the subsequences. For example, for \((S_{1},S_{2},S_{3})\) we perform cyclic rotation of the \(2^{\text{nd}}\) and \(3^{\text{rd}}\) element to obtain the sequence: \((S_{1},S_{3},S_{2})\) (more examples in Figure 8). We provide the general _iterative cyclic rotation_ algorithm in Appendix B.1.

**Non-uniform Deletion Prior.** In scenarios, where we have prior knowledge of the deletion probabilities, sequences generated by cyclic rotation may not be ideal. For example, consider the deletion probabilities are: (\(S_{1}\): 0.5, \(S_{2}\): 0.4, \(S_{3}\): 0.1). Then, the first sequence in Eq. 2 is a bad choice because two of the slices most likely to be deleted are placed at the top. It is possible to select better sequences while satisfying the diversity criteria (no repeated slices at the same position). We score a sequence with deletion probabilities: \(\mathcal{S}=(p_{1},\ldots,p_{L})\) by computing the expected number of functioning slices after \(t\) deletions: \(\operatorname{score}[\mathcal{S},t]=\sum_{i=1}^{L}i.\left(1-\sum_{j=1}^{i}p_{ j}\right)^{t}\).

We present a bipartite-matching based sequence (BMS) selection algorithm. We will provide an intuitive explanation of the algorithm here and refer to Appendix B.2 for complete details. An illustration of BMS is shown in Figure 4. BMS iteratively selects elements of sequences by constructing a bipartite graph between one level to the next one (where edges are incident on feasible elements for the current sequence). The edges are weighted by the score of the current sequence, \(\operatorname{score}[\mathcal{S},t]\). Selecting the next element then is equivalent to finding a maximum weight perfect matching (Gaili, 1986) using the Hungarian algorithm (Kuhn, 1955) shown by the bold lines in Figure 4. This continues till all sequences have \(L\) elements. For a budget \(B>L\), we use conditional sampling to randomly generate sequences according to their deletion probabilities (see Appendix B.2).

### Theoretical Analysis

In this section, we theoretically analyze the performance of exact unlearning systems. For this, introduce the definition of _deletion rate_ for exact unlearning systems.

**Definition 1** (Deletion Rate).: _The deletion rate, \(\delta(S)\), of an exact unlearning system \(S\), is the expected number of deletion requests until the system needs to be retrained from scratch._

The deletion rate captures the effectiveness of an exact unlearning system by quantifying the expected number of deletion requests it can handle. Next, we quantify the deletion rate for \(\text{S}^{\text{3}}\)T and SISA.

**Lemma 1**.: _For dataset size \(N\gg r\), where \(r\) is the number of deletion requests, the deletion rate of \(S^{\text{T}}\) is \(\delta(S^{\text{T}}T)\sim O(mL\log(m\min(B,L)))\) and for SISA it is \(\delta(\operatorname{SISA})\sim O(mL\log m)\), where \(m\) is the number of shards and \(L\) is the number of slices per shard._

This result shows that the deletion rate doesn't improve by increasing the budget \(B\) beyond \(L\) (proof in Appendix A.1). This shows that the optimal deletion rate can be achieved using _only \(L\) sequences_ (instead of \(L\)!). Next, we analyze the impact of deletion requests on the system's performance. We perform a fine-grained analysis focusing on the performance of an individual shard. In this setting, we consider the real-world scenario where we do not retrain every time a slice is impacted instead work with the best available model. For \(\text{S}^{\text{3}}\)T, this means switching off the necessary PEFT layers, while for SISA, it means reverting to the best available checkpoint. The unlearning system experiences performance degradation with an increasing number of deletion requests, as we are compelled to utilize a model trained on fewer data slices (we show this empirically in Section 4). To quantify the performance retention we use a monotonically increasing function \(F(k)\), which indicates a model's performance when trained on \(k\) slices. The exact formulation of \(F(\cdot)\) depends on several factors like the dataset, model size, etc. We analyze the performance retention while processing deletion requests.

**Lemma 2** (Performance Retention).: _Given a set of randomly selected \(B\geq 1\) sequences and uniform deletion prior of slices, the difference between the probability that a shard retains a performance, \(F_{r}(\cdot)\), of at least \(F(k)\) after \(r\) deletion requests between \(\text{S}^{\text{3}}\)T and SISA is shown below_

\[\forall k\in[1\ldots L],\ |\mathbb{P}\left[F_{r}(S^{\text{3}}T)\geq F(k) \right]-\mathbb{P}\left[F_{r}(\operatorname{SISA})\geq F(k)\right]=\zeta(1- \zeta^{B^{\prime}-1}),\] (3)

_where \(\zeta=1-(1-k/L)^{r}\) is a positive fraction and \(B^{\prime}=\min\left\{B,\frac{L!}{(L-k)!}\right\}\)._

This above result shows that compared to SISA, \(\text{S}^{\text{3}}\)T enhances performance by increasing the probability that the system maintains at least \(F(k)\) by a factor of \(B^{\prime}\) (proof in Appendix A.2).

## 4 Experiments

In this section, we outline the experimental setup and evaluate the unlearning performance of S\({}^{3}\)T across various setups. Specifically, we design experiments to answer the following research questions:

1. [label=**(RQ1)**]
2. Does S\({}^{3}\)T training (Section 3.2) impact the model's performance compared to full training?
3. Does S\({}^{3}\)T enhance the deletion capabilities of unlearning, and what is its cost tradeoff?
4. Is the sequence permutation selection algorithm (Section 3.3) effective in practice?

**Sequential Slice-wise Training Performance**. The objective of the experimental setup is to demonstrate that S\({}^{3}\)T can achieve performance comparable to full training. The goal of S\({}^{3}\)T is to achieve parameter isolation for data slices without impacting the overall performance. We perform a range of experiments with different Transformer model sizes ranging from 86M up to 13B. The details of the experimental setup are in Appendix C.1.

In Figure 5, we report the performance on vision, GLUE, and SuperGLUE benchmarks. We use ViT\({}_{\text{BASE}}\)(Dosovitskiy et al., 2020) (for CIFAR10 & CIFAR100 (Krizhevsky et al., 2009)), ViT\({}_{\text{LARGE}}\)(for Tiny ImageNet (Le and Yang, 2015)), and RoBERT\({}_{\text{LARGE}}\)(Liu et al., 2019) (for GLUE (Wang et al., 2018) & SuperGLUE (Wang et al., 2019)). We observe that S\({}^{3}\)T achieves comparable performance to full training (FT) across all settings. In some of the settings, we also observe that S\({}^{3}\)T is able to outperform FT (e.g., S\({}^{3}\)T obtains 2.5% accuracy gain on TinyImagenet using ViT-large). Next, we conduct experiments to evaluate the effectiveness of S\({}^{3}\)T while using large language models (LLMs). Specifically, we perform instruction tuning of Llama2-7B (Touvron et al., 2023), Llama2-13B, and Llama3-8B using Alpaca dataset (Taori et al., 2023). Then, we evaluate each instruction-tuned model on a range of tasks to evaluate the model's general/world knowledge (MMLU (Hendrycks et al., 2020), OpenBookQA (Mihaylov et al., 2018)), truthfulness in QA (TruthfulQA (Lin et al., 2022)), and commonsense reasoning (PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), ARC (Clark et al., 2018)). We use LLM-evaluation suite (Gao et al., 2023) to report the performance and report the zero-shot performance for all datasets. Similar to the previous setup, in Table 3, we observe that S\({}^{3}\)T achieves comparable performance to full finetuning across all datasets and even outperforms FT on many datasets across different model sizes. These experiments provide the answer to (**RQ1**) demonstrating that S\({}^{3}\)T is an effective way to achieve parameter isolation for data slices without impacting the model's performance.

**Deletion Performance**. In this section, we evaluate the performance of S\({}^{3}\)T as deletion requests are received. In Figure 6 (left), we report the performance of S\({}^{3}\)T and baselines SISA (Bourtoule et al., 2021) and ProtoSISA (Yan et al., 2022) (\(m=5\) shards, \(L=4\) slices) on CIFAR-10 and CIFAR-100 datasets. In this experiment, we use a uniform deletion prior over slices. We also report the performance of full re-training, which retrains the model after each deletion request and serves as an upper performance bound. We report S\({}^{3}\)T's performance under various budgets. We observe that S\({}^{3}\)T can achieve very close performance to the full budget (\(B=24\)) with a significantly smaller budget, \(B=4\). For SISA and ProtoSISA, we observe that the plot ends after approximately 40 deletion requests as these systems do not have functioning models beyond this point. We observe that S\({}^{3}\)T can handle more deletion requests while consistently outperforming the baseline approaches. Note that increasing the budget \(B>L\) does not help improve the deletion rate but increases the probability of a better-performing model (as observed in Lemma 1). Next, we extensively evaluate the impact of increasing the training budget \(B\) on the deletion rate (with \(m=5\) shards & \(L=64\) slices). In Figure 6 (right), we observe that there is a steady growth in the deletion rate with an increasing

Figure 5: Comparison of the performance between S\({}^{3}\)T training and full training (FT) on vision, GLUE & SuperGLUE benchmarks. We report the Matthew’s correlation for CoLA, Pearson correlation for STS-B, and accuracy for other tasks. We observe that S\({}^{3}\)T achieves similar performance to FT.

budget. The growth is slightly higher in the initial stages when the budget is low and slows down gradually. This experiment provides empirical evidence to our theoretical result in Lemma 2, which claims that the performance improves with an increased budget.

**Sequence Selection**. We evaluate the quality of the sequences generated by the iterative cyclic rotation and BMS algorithm (Section 3.3). Ideally, we want the selected sequences to be diverse and have a high edit distance between sequences. Therefore, we report the average edit distance within a selected subset, \(O\): \(\mathbb{E}_{o\in O}[d_{\mathrm{edit}}(o,o^{\prime})]\). First, when the deletion prior is uniform and compare cyclic rotation with random sampling. In Figure 7 (left), we report the average edit distance with varying budget (\(B\)) while the slice count (\(L\)) is fixed. We observe that cyclic rotation produce significantly better sequences than random sampling. Second, we consider slices associated with a deletion prior. We present the results by averaging over 10 deletion priors sampled from a Dirichlet distribution. In Figure 7 (center & right), we observe that BMS consistently outperforms random sampling both in terms of diversity (avg. edit distance) and chosen sequence scores (\(\mathrm{score}[\mathcal{S},t]\)).

## 5 Conclusion

In this paper, we introduced S\({}^{3}\)T, an effective framework for performing exact unlearning. S\({}^{3}\)T uses a modular machine learning model that is trained using disjoint shards of the data. Each shard is used to train a different model using a lightweight fine-tuning approach that enables parameter isolation, which allows us to execute unlearning requests efficiently. The key idea behind S\({}^{3}\)T is to train multiple models using different sequences of slices before deploying the system. This helps reduce retraining costs and improve the model's performance while the model is in production. Both theoretically and empirically, we show that S\({}^{3}\)T has significantly improved deletion capabilities compared to existing approaches. Future work can focus on developing techniques for finer-grained parameter isolation to further improve S\({}^{3}\)T's performance.

Figure 6: (_Left_) We report the impact on performance of S\({}^{3}\)T and baselines with an increasing number of deletion requests. S\({}^{3}\)T handles a higher number of deletion requests while maintaining high performance with a relatively low budget (\(\bigotimes\) indicates the failure point for the system). (_Right_) We report the deletion rate of S\({}^{3}\)T with an increasing budget and observe steady growth.

Figure 7: We evaluate the performance of iterative cyclic rotation and bipartite matching-based selection (BMS). (_Left_) We observe that cyclic rotation selection consistently outperforms random sampling for all budgets, \(1\leq B\leq 120\) (with fixed \(L=5\)). (_Center_) We evaluate the average edit distance of the sequences generated by BMS and observe that it achieves the optimal edit distance (\(L\)). (_Right_) We also observe that sequences from BMS achieves higher scores than random sampling.

## References

* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
* Achille et al. (2024) Alessandro Achille, Michael Kearns, Carson Klingenberg, and Stefano Soatto. Ai model disgorgement: Methods and choices. _Proceedings of the National Academy of Sciences_, 121(18):e2307304121, 2024.
* Aldaghri et al. (2021) Nasser Aldaghri, Hessam Mahdavifar, and Ahmad Beirami. Coded machine unlearning. _IEEE Access_, 9:88137-88150, 2021.
* Bhattacharya (1994) P Bhattacharya. The representation of permutations by trees. _Computers & Mathematics with Applications_, 28(9):67-71, 1994.
* Biggio et al. (2012) Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. _arXiv preprint arXiv:1206.6389_, 2012.
* Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piga: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pp. 7432-7439, 2020.
* Blom et al. (1994) Gunnar Blom, Lars Holst, and Dennis Sandell. 7.5 coupon collecting i, 7.6 coupon collecting ii, and 15.4 coupon collecting iii. _Problems and Snapshots from the World of Probability, New York: Springer-Verlag_, pp. 85-87, 1994.
* Bourtoule et al. (2021) Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pp. 141-159. IEEE, 2021.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* Cao & Yang (2015) Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In _2015 IEEE symposium on security and privacy_, pp. 463-480. IEEE, 2015.
* Chen & Yang (2023) Jiaoo Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms. In _The 2023 Conference on Empirical Methods in Natural Language Processing_, 2023.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Cormen et al. (2022) Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. _Introduction to algorithms_. MIT press, 2022.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pp. 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
* Dosovitskiy et al. (2020) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2020.
* Dukler et al. (2023) Yonatan Dukler, Benjamin Bowman, Alessandro Achille, Aditya Golatkar, Ashwin Swaminathan, and Stefano Soatto. Safe: Machine unlearning with shard graphs. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 17108-17118, 2023.

* Eldan and Russinovich (2023) Ronen Eldan and Mark Russinovich. Who's harry potter? approximate unlearning in llms. _arXiv preprint arXiv:2310.02238_, 2023.
* Parliament and the European Union (2016) European Parliament and Council of the European Union. Regulation (EU) 2016/679 of the European Parliament and of the Council. URL https://data.europa.eu/eli/reg/2016/679/oj.
* Galil (1986) Zvi Galil. Efficient algorithms for finding maximum matching in graphs. _ACM Computing Surveys (CSUR)_, 18(1):23-38, 1986.
* Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.
* Golatkar et al. (2020a) Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forgetting in deep networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 9304-9312, 2020a.
* Golatkar et al. (2020b) Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations. In _European Conference on Computer Vision_, pp. 383-398, 2020b.
* Golatkar et al. (2023) Aditya Golatkar, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Training data protection with compositional diffusion models. _arXiv preprint arXiv:2308.01937_, 2023.
* Graves et al. (2021) Laura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pp. 11516-11524, 2021.
* Guo et al. (2020) Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. Certified data removal from machine learning models. In _International Conference on Machine Learning_, pp. 3832-3842. PMLR, 2020.
* Gupta et al. (2021) Varun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed Sharifi-Malvajerdi, and Chris Waites. Adaptive machine unlearning. _Advances in Neural Information Processing Systems_, 34:16319-16330, 2021.
* Hayes et al. (2024) Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas Papernot. Inexact unlearning needs more careful evaluations to avoid a false sense of privacy. _arXiv preprint arXiv:2403.01218_, 2024.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In _International Conference on Learning Representations_, 2020.
* Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2021.
* Jia et al. (2023) Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsification can simplify machine unlearning. _arXiv preprint arXiv:2304.04934_, 2023.
* Koh and Liang (2017) Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In _International conference on machine learning_, pp. 1885-1894. PMLR, 2017.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* Kuhn (1955) Harold W Kuhn. The hungarian method for the assignment problem. _Naval research logistics quarterly_, 2(1-2):83-97, 1955.
* Kumar et al. (2023) Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, and Dan Roth. Privacy adhering machine un-learning in nlp. In _Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings)_, pp. 268-277, 2023.

* Kurmanji et al. [2024] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lagarias [2013] Jeffrey Lagarias. Euler's constant: Euler's work and modern developments. _Bulletin of the American Mathematical Society_, 50(4):527-628, 2013.
* Le and Yang [2015] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* Lin et al. [2022] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 3214-3252, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.229. URL https://aclanthology.org/2022.acl-long.229.
* Liu et al. [2024a] Jiaqi Liu, Jian Lou, Zhan Qin, and Kui Ren. Certified minimax unlearning with generalization rates and deletion capacity. _Advances in Neural Information Processing Systems_, 36, 2024a.
* Liu et al. [2024b] Sijia Liu, Yuanshu Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al. Rethinking machine unlearning for large language models. _arXiv preprint arXiv:2402.08787_, 2024b.
* Liu et al. [2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* Mantelero [2013] Alessandro Mantelero. The eu proposal for a general data protection regulation and the roots of the 'right to be forgotten'. _Computer Law & Security Review_, 29(3):229-235, 2013.
* Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. In _Conference on Empirical Methods in Natural Language Processing_, 2018. URL https://api.semanticscholar.org/CorpusID:52183757.
* Neel et al. [2021] Seth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine unlearning. In _Algorithmic Learning Theory_, pp. 931-962. PMLR, 2021.
* Nguyen et al. [2022] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. _arXiv preprint arXiv:2209.02299_, 2022.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* Patil et al. [2023] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives for defending against extraction attacks. In _The Twelfth International Conference on Learning Representations_, 2023.
* Qayyum et al. [2020] Adnan Qayyum, Junaid Qadir, Muhammad Bilal, and Ala Al-Fuqaha. Secure and robust machine learning for healthcare: A survey. _IEEE Reviews in Biomedical Engineering_, 14:156-180, 2020.
* Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* Sekhari et al. [2021] Ayush Sekhari, Jayadev Acharya, Gautam Kamath, and Ananda Theertha Suresh. Remember what you want to forget: Algorithms for machine unlearning. _Advances in Neural Information Processing Systems_, 34:18075-18086, 2021.
* Shastri et al. [2019] Supreeth Shastri, Melissa Wasserman, and Vijay Chidambaram. The seven sins of {Personal-Data} processing systems under {GDPR}. In _11th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud 19)_, 2019.

* Steinhardt et al. [2017] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. _Advances in neural information processing systems_, 30, 2017.
* Surden [2021] Harry Surden. Machine learning and law: An overview. _Research Handbook on Big Data Law_, pp. 171-184, 2021.
* Suriyakumar and Wilson [2022] Vinith Suriyakumar and Ashia C Wilson. Algorithms that approximate data removal: New results and limitations. _Advances in Neural Information Processing Systems_, 35:18892-18903, 2022.
* Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* Tarun et al. [2023a] Ayush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine unlearning. _IEEE Transactions on Neural Networks and Learning Systems_, 2023a.
* Tarun et al. [2023b] Ayush Kumar Tarun, Vikram Singh Chundawat, Murari Mandal, and Mohan Kankanhalli. Deep regression unlearning. In _International Conference on Machine Learning_, pp. 33921-33939. PMLR, 2023b.
* Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Thudi et al. [2022] Anvith Thudi, Hengrui Jia, Ilia Shumailov, and Nicolas Papernot. On the necessity of auditable algorithmic definitions for machine unlearning. In _31st USENIX Security Symposium (USENIX Security 22)_, pp. 4007-4022, 2022.
* Tomizawa [1971] Nobuaki Tomizawa. On some techniques useful for solution of transportation network problems. _Networks_, 1(2):173-194, 1971.
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajiwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In _Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 353-355, 2018.
* Wang et al. [2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. _Advances in neural information processing systems_, 32, 2019.
* Wang et al. [2024] Cheng-Long Wang, Qi Li, Zihang Xiang, and Di Wang. Has approximate machine unlearning been evaluated properly? from auditing to side effects. _arXiv preprint arXiv:2403.12830_, 2024.
* Wolf et al. [2019] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingface's transformers: State-of-the-art natural language processing. _arXiv preprint arXiv:1910.03771_, 2019.
* Xu et al. [2023] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. Machine unlearning: A survey. _ACM Comput. Surv._, 56(1), aug 2023. ISSN 0360-0300. doi: 10.1145/3603620. URL https://doi.org/10.1145/3603620.
* Xu et al. [2024] Jie Xu, Zihan Wu, Cong Wang, and Xiaohua Jia. Machine unlearning: Solutions and challenges. _IEEE Transactions on Emerging Topics in Computational Intelligence_, 2024.
* Yan et al. [2022] Haonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and Xiaodong Lin. Arcane: An efficient architecture for exact machine unlearning. In _IJCAI_, volume 6, pp. 19, 2022.
* Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4791-4800, 2019.

## Appendix A Mathematical Proofs

### Contents

* 1 Proof of Lemma 1
	* 1.2 Proof of Lemma 2
	* 1.3 Proof of Lemma 3

### Proof of Lemma 1

In \(\mathrm{S}^{3}\mathrm{T}\), since the sequences are selected to be diverse (Figure 4), the topmost slice in each sequence is different. Therefore, we have \(B^{\prime}=\min(B,L)\) different slices at the topmost position. This implies that for \(\mathrm{S}^{3}\mathrm{T}\) to encounter service failure, a total of \(mB^{\prime}\) slices must be affected by deletion requests, where \(m\) is the number of shards. Considering deletion requests affect all slices uniformly we need to compute the expected time till all slices are affected. This setup is similar to the coupon collector problem (Blom et al., 1994).

Proof.: The deletion rate is the expected number of requests to delete all \(mB^{\prime}\) slices (\(m\) shards, \(B^{\prime}\) unique slices per shard). Using linearity of expectation, the deletion rate or total time is:

\[\delta(S^{3}T)=\mathbb{E}[T] =\mathbb{E}[t_{1}+\ldots+t_{mB^{\prime}}]\] (4) \[=\mathbb{E}[t_{1}]+\ldots+\mathbb{E}[t_{mB^{\prime}}],\] (5)

where \(\mathbb{E}[t_{i}]=1/p_{i}\), where \(p_{i}\) is the probability that the \(i\)-th slice is affected after \((i-1)\) slices are deleted. Let \(N=|\mathcal{D}|\) the dataset size and \(r\) be the number of deletion requests seen so far. The expression of \(p_{i}\) is shown below:

\[p_{i} =\frac{\{mB^{\prime}-(i-1)\}s_{b}}{N-r}\] (6) \[p_{i} =\frac{N\min(B/L,1)-(i-1)s_{b}}{N-r}\] (7) \[p_{i} =\frac{\min(B/L,1)-(i-1)/(mL)}{1-r/N}\] (8) \[p_{i} \geq\frac{mB^{\prime}-(i-1)}{mL},\] (9)

where \(s_{b}\) is the size of each slice. Replacing this result in Eq. 5, we get:

\[\delta(S^{3}T) \leq\frac{mL}{mB^{\prime}-0}+\frac{mL}{mB^{\prime}-1}+\ldots+ \frac{mL}{1}\] (10) \[=mL\left(\frac{1}{1}+\frac{1}{2}+\ldots+\frac{1}{mB^{\prime}}\right)\] (11) \[=mL.H_{mB^{\prime}}\] (12) \[=mL\log(mB^{\prime})+\gamma mL+\frac{1}{2}+O\left(\frac{L}{mB^{ \prime}}\right),\] (13)

where \(H_{mB^{\prime}}\) denotes the \(mB^{\prime}\)-th Harmonic number and \(\gamma\) is the Euler's constant (Lagarias, 2013). The above result proves the first portion of the lemma, \(\delta(S^{3}T)\sim O(mL\log mB^{\prime})\).

The second part of the lemma is about SISA. For SISA to experience failure, only the first slice of each shard (total of \(m\) slices) needs to be affected by deletion. In this case, we can write:

\[p_{i}=\frac{\{m-(i-1)\}s_{b}}{N-r}=\frac{N/L-(i-1)s_{b}}{N-r}=\frac{1-(i-1)/m }{L(1-r/N)}\geq\frac{m-(i-1)}{mL}\] (14)Replacing the above result in Eq. 5, we get:

\[\delta(\mathrm{SISA}) \leq mL\left(\frac{1}{1}+\frac{1}{2}+\ldots+\frac{1}{m}\right)\] (15) \[=mL.H_{m}\] (16) \[=mL\log(m)+\gamma m+\frac{1}{2}+O\left(\frac{1}{m}\right).\] (17)

This proves the second portion of the lemma: \(\delta(\mathrm{SISA})\sim O(mL\log m)\). 

### Proof of Lemma 2

We begin by proving the retention result for SISA as it is simpler to understand. Each model within SISA is trained on a single sequence of slices.

Proof.: The probability it maintains performs better than \(F(k)\) is equivalent to showing that none of the top-\(k\) elements (out of \(L\)) are affected after \(r\) deletion requests:

\[\mathbb{P}\left[F_{r}(\mathrm{SISA})\geq F(k)\right]=\left(1-\frac{k}{L} \right)^{r}.\] (18)

In S\({}^{3}\)T, each model is trained on \(B\) such sequences. Therefore, we need to compute the probability that at least one sequence is better than \(F(k)\), which is:

\[\mathbb{P}\left[F_{r}(S^{3}T)\geq F(k)\right]=1-\left(1-\left(1-\frac{k}{L} \right)^{r}\right)^{B}.\] (19)

However, the above result suggests that the probability can be increased indefinitely by increasing \(B\). This is inaccurate because to maintain a performance of \(F(k)\) there has to be at least one prefix of length \(k\) that has not been affected. Since there only \(P(L,k)=\frac{L}{(L-k)!}\) permutations of length \(k\) extending the budget beyond \(P(L,k)\) doesn't work. Therefore, the correct probability is:

\[\mathbb{P}\left[F_{r}(S^{3}T)\geq F(k)\right]=1-\left(1-\left(1-\frac{k}{L} \right)^{r}\right)^{B^{\prime}},\] (20)

where \(B^{\prime}=\min\left\{B,P(L,k)\right\}\).

Taking the difference between Eq. 20 and Eq. 18 and setting \(\alpha=(1-k/L)^{r}\), we get:

\[\mathbb{P}\left[F_{r}(S^{3}T)\geq F(k)\right]-\mathbb{P}\left[F_ {r}(\mathrm{SISA})\geq F(k)\right] =1-(1-\beta)^{B^{\prime}}-\beta\] \[=(1-\beta)\{1-(1-\beta)^{B^{\prime}-1}\}\] \[=\zeta(1-\zeta^{B^{\prime}-1}),\] (21)

where \(\zeta=1-\alpha=1-(1-k/L)^{r}\). This completes the proof. 

**Discussion**. In the above proof, we assumed that \(B\) sequences are selected randomly. In practice, we select diverse sequences using the iterative cyclic rotation algorithm. However, deriving a closed-form theoretical performance bound for the sequences generated using cyclic rotation is non-trivial. Intuitively, we expect the performance to be better as selecting diverse sequences means that the probability of a length-\(k\) prefix getting affected is reduced. Empirically, we observe performance improvements in Figure 7. We leave the theoretical proof of these results to future work.

### Proof of Lemma 3

This lemma states that BMS selects the most diverse sequences. First, we revisit our definition of sequence diversity. Two sequences are considered diverse if no element appears at the same position, e.g., \((S_{1},S_{2},S_{3},S_{4})\) and \((S_{2},S_{3},S_{4},S_{1})\). Since we want diverse sequences, BMS performs maximum weight perfect matching, ensuring that no two elements appear in the same position. Therefore, in this proof, we show that perfect matching exists at all levels, \([1,L]\), in Algorithm 2.

Proof.: Let the bipartite graph, \(G\), consist of vertices \(V\cup V^{\prime}\), such that the edges \(E\subseteq V\times V^{\prime}\). For a perfect matching to exist, every subset \(W\subset V\) should satisfy:

\[|W|\leq N_{G}(W),\] (22)

where \(N_{G}(\cdot)\) is the neighbourhood defined using the graph, \(G\).

In our setup, the graph \(G\) has vertices \(V=\{1,\ldots,L\}\) and \(V^{\prime}=\{1^{\prime},\ldots,L^{\prime}\}\), which indicate the elements in the permutation sequence. Every vertex \(v\in V\) has \((L-i+1)\) edges to unique vertices in \(V^{\prime}\) at the \(i\)-th iteration of the algorithm. Therefore, for all iterations \(i\in\{2,\ldots,L\}\) (shown in Line 6 in Algorithm 2) the condition in Eq. 22 is satisfied and a perfect matching exists.

Since perfect matching selects a unique element in \(V^{\prime}\), therefore no element is repeated at the same position in the BMS output. Therefore, BMS generates diverse sequences.

```
1:functionCyclicPermutation(Permutation\(P\))
2:\(\mathcal{R}=\{\}\)// set of all rotations
3:for\(i\in\{1,\ldots,P\text{.length}\}\)do
4:\(\mathcal{R}=\mathcal{R}\cup P\)
5:\(P=\text{rotateRight}(P)\)
6:endfor
7:return\(\mathcal{R}\)
8:endfunction
9:
10:functionIterativeCyclicRotation(Slice count:\(L\), Budget:\(B\))
11:\(\mathcal{O}=\text{CyclicPermutation}([1,\ldots,L])\)// initialize the set with \(L\) cyclic permutations
12:\(n_{\text{iter}}=0\)// set the iteration count
13:while\(|\mathcal{O}|<B\)do
14:\(n_{\text{iter}}=n_{\text{iter}}+1\)
15:for\(o\in\mathcal{O}\)do// iteratively expand each of the existing permutation
16:prefix, suffix = \(o[:n_{\text{iter}}]\), \(o[n_{\text{iter}}:]\)// set the prefix and suffix
17:// rotate the suffix with same prefix
18:\(\mathcal{P}=\{\text{prefix}\cup\text{\boldmath$p$ for $p\in\text{CyclicPermutation(suffix)}$}\}\)
19:\(\mathcal{O}=\mathcal{O}\cup\mathcal{P}\)
20:endfor
21:endwhile
22:return\(\mathcal{O}[:B]\)// return \(B\) output permutations
23:endfunction ```

**Algorithm 1** Iterative Cyclic Rotation

## Appendix B Implementation Details

In this section, we discuss the details of various algorithms and workflows within S\({}^{3}\)T.

#### Contents

* B.1 Iterative Cyclic Rotation
* B.2 Bipartite-matching based Sequence Selection
* B.3 S\({}^{3}\)T Training Procedure
* B.4 Deletion Procedure

### Iterative Cyclic Rotation

In the setting, where the deletion probabilities are uniform we use the cyclic rotation algorithm to select diverse permutations. In Figure 3 (middle), we observe that we can easily generate \(L\) diverse sequences using cyclic permutation of the original sequence. We can iteratively expand on this idea to generate more sequences when the budget, \(B>L\). An illustration of the different sequences selected using iterative cyclic rotation for different budgets is shown in Figure 8.

We present the pseudocode of the iterative cyclic rotation mechanism in Algorithm 1. First, we set the initial set to cyclic permutations of the original sequence (Line 11). If the budget exceeds the current set of output permutations, we iteratively expand the selected permutations by rotating their suffixes (Line 18). This continues till the output set has at least \(B\) sequences. Please note that Algorithm 1 is a simplified version of the actual algorithm. For some corner case budget values, we apply certain heuristics to select the right permutations to expand (Line 18). Empirically, we observe that iterative cyclic permutation significantly outperforms random sampling (Figure 7). We conjecture that iterative cyclic rotation generates the most diverse set of \(B\) permutations when all \(L\) slices have equal deletion probabilities. We will leave the theoretical proof to future research.

### Bipartite-matching based Sequence Selection

Before describing the bipartite selection algorithm, we discuss the scoring function for sequences with given deletion probabilities. Given a slice sequence with deletion probabilities \(\mathcal{S}=(p_{1},p_{2},p_{3},p_{4})\), the scoring function computes the expected number of surviving slices after \(t\) deletions:

\[\mathrm{score}[\mathcal{S},t]=4.(1-p_{1}-p_{2}-p_{3}-p_{4})^{t}+3.(1-p_{1}-p_{ 2}-p_{3})^{t}+2.(1-p_{1}-p_{2})^{t}+(1-p_{1})^{t}.\] (23)

The above equation is a function of \(t\), which needs to be set by the user. In the general case, Eq. 23 can be written as:

\[\mathrm{score}[\mathcal{S},t]=\sum_{i=1}^{n}i.\left(1-\sum_{j=1}^{i}p_{j} \right)^{t}.\] (24)

Next, we describe the details of the Bipartite-matching based selection (BMS) algorithm in Algorithm 2. The objective of BMS is to select a set of \(B\) diverse permutation sequences where sequences have a high score (Eq. 24). This algorithm starts by selecting \(L\) different starting elements for the sequences in Line 4. Then, BMS iteratively selects the next element within each sequence. This involves constructing a bipartite graph between the last elements of the sequences seen so far and the next set of elements. The edge weights are set to the score of the sequence that is a concatenation of the current sequence, \(o\), and the next node, \(v^{\prime}\). The edges incident on feasible next elements (elements not seen in a permutation sequence so far) as shown in Line 12. We perform a maximum weight perfect graph matching on the graph, \(G\), using the Hungarian algorithm (Kuhn, 1955). Based on the

Figure 8: An illustration of the sequences selected by the iterative cyclic rotation algorithm for different budgets. We observe that for budget \(B\leq L\) the algorithm selects sequences generated by rotating the entire sequence. For budget \(L<B\leq L(L-1)\), the algorithm generates newer sequences by rotating the rotating subsequences starting from the second element. This continues as the budget increases and smaller subsequences are rotated.

```
1:functionBMS(Slice count: \(L\), Budget: \(B\))
2:\(\mathcal{O}=\{\}\)
3:for\(l\in\{1,\ldots,L\}\)do // initializing the sequence set with first element
4:\(\mathcal{O}=\mathcal{O}\cup\{l\}\)
5:endfor
6:for\(i\in\{2,\ldots,L\}\)do // iterations to select the 2nd to the \(L\)-th element
7:\(G=\{\}\)// Initialize Graph
8:for\(o\in\mathcal{O}\)do // Iterate over sequences
9:\(v=o\).pop() // Collect final element of each sequence
10:for\(v^{\prime}\in\{1,\ldots,L\}\)do // Iterate and add all feasible edges
11:\(w=\text{score}[o\cup v^{\prime}]\) // compute score for the sequence \(o\cup v^{\prime}\) (Eq. 24)
12:if\(v^{\prime}\notin o\)then\(G\).add_edge(\(v,v^{\prime},w\)) // check for feasibility
13:endfor
14:endfor
15:\((V,V^{\prime})=\text{MaximumWeightMatching}(G)\) // use Hungarian algorithm (Kuhn, 1955)
16:for\(o\in\mathcal{O}\)do
17:// select vertex associated with each sequence from perfect matching
18:\(v^{*}=\{v^{\prime}|v^{\prime}\in V^{\prime}\wedge v=o\).pop()\(\}\)
19:\(o=o\cup v^{*}\) // add to sequence
20:endfor
21:endfor
22:\(\mathcal{O}_{B}=\{o\in\mathcal{O}|o\) is among top \(B\) sequence with highest score[\(o\)]\(\}\)
23:return\(\mathcal{O}_{B}\)
24:endfunction
25:endfunction
26:
27:\(\mathcal{O}=\{\}\)
28:for\(l\in\{1,\ldots,L\}\)do // initializing the sequence set with first element
29:\(\mathcal{O}=\mathcal{O}\cup\{l\}\)
30:endfor
31:for\(i\in\{2,\ldots,L\}\)do // iterations to select the 2nd to the \(L\)-th element
32:\(G=\{\}\)// Initialize Graph
33:for\(o\in\mathcal{O}\)do // Iterate over sequences
34:\(v=o\).pop() // Collect final element of each sequence
35:for\(v^{\prime}\in\{1,\ldots,L\}\)do // Iterate and add all feasible edges
36:\(w=\text{score}[o\cup v^{\prime}]\) // compute score for the sequence \(o\cup v^{\prime}\) (Eq. 24)
37:if\(v^{\prime}\notin o\)then\(G\).add_edge(\(v,v^{\prime},w\)) // check for feasibility
38:endfor
39:endfor
40:\((V,V^{\prime})=\text{MaximumWeightMatching}(G)\) // use Hungarian algorithm (Kuhn, 1955)
41:for\(o\in\mathcal{O}\)do
42:// select vertex associated with each sequence from perfect matching
43:\(v^{*}=\{v^{\prime}|v^{\prime}\in V^{\prime}\wedge v=o\).pop()\(\}\)
44:\(o=o\cup v^{*}\) // add to sequence
45:endfor
46:endfor
47:\(\mathcal{O}_{B}=\{o\in\mathcal{O}|o\) is among top \(B\) sequence with highest score[\(o\)]\(\}\)
48:return\(\mathcal{O}_{B}\)
49:endfunction
50:
51:endfunction
52:
53:endfunction
54:
55:endfunction
56:
57:endfunction
58:
59:endfunction
60:
61:endfunction
62:
63:endfunction
63:
64:endfunction
65:
66:endfunction
67:
68:endfunction
69:
60:
61:endfunction
64:
65:endfunction
67:
68:endfunction
69:
69:
70:
71:endfunction
72:
73:endfunction
73:
74:endfunction
75:
76:endfunction
77:
78:endfunction
79:
79:
80:endfunction
81:
82:endfunction
83:
84:endfunction
85:
86:endfunction
87:
88:endfunction
89:
89:
90:endfunction
91:
92:endfunction
93:
94:
95:endfunction
95:
96:endfunction
97:
98:endfunction
99:
99:
97:
99:
98:endfunction
94:
99:
99:
95:endfunction
95:
97:
99:
sampling approach is shown in Figure 9. We observe that the selected samples (shown in green) has slices with relatively lower deletion probabilities at the top.

### S\({}^{3}\)T Training Procedure

```
1:Input: Dataset \(\mathcal{D}\), Shard Count: \(m\), Slice Count: \(L\), Budget: \(B\)
2:\(\mathcal{F}=\{\}\)// initializing model set
3:for\(d\in\{\mathcal{D}_{1},\dots,\mathcal{D}_{m}\}\)do // partition the dataset into shards and iterate over them
4:\(\mathcal{S}=\{S_{1},\dots,S_{L}\}\) // partition into slices, \(\cup_{i}\mathcal{S}=d\)
5:\(\Pi=\text{SelectTrainingSequences}(L,B)\) // using cyclic or BMS algorithm
6:for\(\pi\in\Pi\)do
7:\(\mathcal{S}_{\pi}=\pi(\mathcal{S})\) // order the slices according to the permutation \(\pi\)
8:\(f=\text{SliceWiseTraining}(\mathcal{S}_{\pi})\)
9:\(\mathcal{F}=\mathcal{F}\cup f\)
10:endfor
11:endfor
12:return\(\mathcal{F}\) ```

**Algorithm 3** S\({}^{3}\)T Training Procedure

```
1:Input: Model set \(\mathcal{F}\), Deletion instance: \(x\)
2:\(m^{\prime}=\text{LocateShard}(x)\) // get original shard ID, \(m^{\prime}\)
3:\(\overline{\mathcal{F}}=\{\}\) // set of modified models
4:for\(f\in\mathcal{F}_{m^{\prime}}\)do // iterate over models trained on \(m^{\prime}\)-th shard
5:\(l^{\prime}=\text{LocateSlice}(x,f)\) // get original slice ID, \(l^{\prime}\), of \(x\) within \(f\)
6:for\(l\in\{l^{\prime},\dots,L\}\)do
7:\(\text{DecactivateLayer}(f,l)\) // deactivate PEFT layers
8:endfor
9:if\(l^{\prime}>0\)then\(\overline{\mathcal{F}}=\overline{\mathcal{F}}\cup f\) // ensuring all layers aren't switched off
10:endfor
11:\(\mathcal{F}=\mathcal{F}\setminus\mathcal{F}_{m^{\prime}}\) // remove older models
12:\(\mathcal{F}=\mathcal{F}\setminus\overline{\mathcal{F}}\) // introduce updated models
13:return\(\mathcal{F}\) ```

**Algorithm 4** S\({}^{3}\)T Deletion Procedure

In this section, we provide an outline for the training procedure within the S\({}^{3}\)T framework in Algorithm 3. S\({}^{3}\)T proceeds by dividing the entire dataset into \(m\) shards. Each shard is further divided into \(L\) disjoint slices. Based on the budget \(B\), we obtain the permutation sequences to perform slice-wise training. We train each model \(f\) on a unique sequence of slice sequence, \(\pi(\mathcal{S})\). We return the complete set of models, \(\mathcal{F}\). During inference, the user selects the best performing model within each shard and deploys an ensemble of those models to production.

### Deletion Procedure

In this section, we describe the procedure when a deletion request arrives for an instance \(x\) in Algorithm 4. We first locate the shard ID, \(m^{\prime}\), where the instance \(x\) belonged and iterate over all models trained on that shard. For each of these models, we locate the slice ID, \(l^{\prime}\), of \(x\) and deactivate all layers \(\{l^{\prime},\dots,L\}\) (as shown in Line 7). After that, if the model \(f\) is still active, we add it to the set of modified models, \(\overline{\mathcal{F}}\). S\({}^{3}\)T uses the updated set to perform inference till a new deletion request arrives. Note that the deletion process can occur entirely offline without impacting the production system, provided that the deleted instance does not influence any of the ensemble models.

**Overall Workflow**. So far, we have discussed the sequential training procedure and the sequence selection approach within a budget. Here, we will bring all of the components together and illustrate the functioning of S\({}^{3}\)T using a simple example. We consider a setting with \(m=3\) shards, \(L=4\) slices, and budget \(B=4\). Initially, for every shard, the available models are trained on sequences:

\[(1,2,3,4),(4,1,2,3),(3,4,1,2),(2,3,4,1)\]Next, if a deletion request affects slice 1 for the 3\({}^{\text{rd}}\) shard. Then, the available models for the 3\({}^{\text{rd}}\) shard are: \((4),(3,4),(2,3,4)\). Notice how all the PEFT layers at or below slice 1 have been switched off. In this scenario, S\({}^{3}\)T doesn't perform any retraining but continues to function with the best available model \((2,3,4)\) (as it is trained on maximum slices). If the following deletion requests affect slice 2, then the available models are: \((4),(3,4)\). S\({}^{3}\)T continues to function with the best model \((3,4)\). This continues until no models are available in any shard, at which point S\({}^{3}\)T is retrained from scratch.

## Appendix C Experiments

In this section, we describe our experimental setup and present additional analysis experiments to evaluate the functioning of S\({}^{3}\)T.

### Experimental Setup

We perform all experiments using PyTorch (Paszke et al., 2019) and Huggingface (Wolf et al., 2019) framework. Our experiments were run on NVIDIA A6000 GPUs. In Table 1, we report the common set of hyperparameters for S\({}^{3}\)T fine-tuning experiments. All hyperparameters were set using a grid search with the Weights & Biases framework. We use an AdamW optimizer with the corresponding learning rates for each dataset (reported in Table 1). During fine-tuning of the models, we perform full-precision training for all settings except instruction tuning where we use 8-bit training.

### Design Choices

We discuss the rationale behind the design choices for the proposed slice-wise training approach. First, we use top-to-bottom fine-tuning because we found the top layers were easier to train at the start and a bottom-up approach didn't converge. Note that our training process does not need to proceed in a layer-wise fashion, we can even train multiple LoRA layers per slice. Second, we train layers in a cumulative manner where the \(i\)-th layer is trained using slices \(\{1,\dots,i\}\). We found that this way of training helps in better convergence compared to the setup where we train every layer using a different slice. Third, we analyze the cost of slice-wise training and find it to be comparable to full training. Considering that training cost is \(c=O(nl)\), where \(n\) is the dataset size and \(l\) is the number of trainable layers (empirical evidence in Appendix C.3). For slice-wise training, we observe that \(c=\sum_{i=1}^{k}(\frac{in}{k})(\frac{l}{k})=O\left(\frac{nl}{2}\right)\), which is of the same order as full training.

\begin{table}
\begin{tabular}{l l l l l l l l}
1080 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
1081 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\
1082 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\
1083 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\
1084 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\
1085 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\
1086 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
1087 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
1088 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
1089 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\
1090 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} \\
1091 & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} \\
1092 & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} \\
1093 & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} \\
1094 & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{}{} \\
1095 & \multicolumn{1}{c}{} & \

### Additional Experiments

In this section, we report analysis experiments to evaluate the S\({}^{3}\)T's deletion capabilities and training.

**Performance Degradation with Slice Deletion**. In Figure 10, we report the relative performance drop with an increasing number of slices affected by deletion requests. For vision datasets in Figure 10 (left), we only observe a small drop in performance (<5%). For instruction tuning in Figure 10 (middle), we observe a negligible performance drop for most datasets except MMLU & ARC, which are more challenging open-ended QA benchmarks. For text classification in Figure 10 (right), we observe an increased drop in performance for a few of the datasets. We hypothesize that this occurs because the RoBERTaLARGE (used in text classification tasks) is a relatively weaker pre-trained model compared to Llama and ViT, which may explain this behavior. We also observe that the performance can vary based on the task and overall it is dependent on both the task and the model being fine-tuned. For unlearning, a lower performance drop is better as it suggests that we can continue using the same model without retraining for a longer time.

**Layer Allocation per Slice**. This experiment aims to answer the question: how many slices can we pack into a model while still achieving good performance? We conduct an experiment where we allocate different numbers of PEFT layers per slice and also vary the number of slices. We report the results on RTE dataset using RoBERTaLARGE in Figure 11. We observe that the performance is poor

Figure 11: Performance impact of S\({}^{3}\)T when allocated a different number of layers per slice: We observe that simply increasing the number of slices can lead to a significant performance drop. There is an optimal tradeoff between the total number of slices and the number of layers per slice.

Figure 10: Relative performance drop with an increasing number of deleted slices. We observe a relatively small performance drop for image classification and instruction tuning tasks, while noticing a considerable drop for few of the text classification datasets.

when the number of layers per slice is low. For example, when the number of layers per slice = 1, the performance improves as we increase the number of slices but again drops when the slice count is too high. This shows that it may not be feasible to train the model using a large number of slices without a drop in performance. Overall, the performance variation depends on the underlying task and model, so the developer should select these hyperparameters based on their performance requirements.

**Sequence-based Training**. In this experiment, we compare S\({}^{3}\)T's performance with sequential training. Sequential training involves training the entire PEFT model on a sequence of slices. This was used in (Kumar et al., 2023) to improve the retraining efficiency of SISA. In Figure 13, we report the performance of S\({}^{3}\)T and sequential training using ViT\({}_{\text{BASE}}\) on CIFAR-10 and CIFAR-100 datasets. We observe that the performance of S\({}^{3}\)T gradually increases as it is trained on more slices. Overall, we observe that S\({}^{3}\)T achieves quite similar or outperforms sequential training. It is important to note that S\({}^{3}\)T trains a significantly smaller number of parameters (\(L\) times reduction compared to sequential training) but still achieves competitive performance.

**BMS vs. Cyclic rotation with Deletion prior**. In this setting, we compare the BMS selection algorithm with a variant of cyclic rotation when the prior deletion probabilities are available. We perform cyclic rotation by first sorting the slices based on their deletion probabilities. For example, we sort the sequence with deletion probabilities: (\(S_{1}\): 0.5, \(S_{2}\): 0.4, \(S_{3}\): 0.1) as: (\(S_{3},S_{2},S_{1}\)). Then, for a budget \(B=3\), the stored sequences are: (\(S_{3},S_{2},S_{1}\)), (\(S_{1},S_{3},S_{2}\)), (\(S_{2},S_{1},S_{3}\)). In this variant, the slices most likely to be deleted are not at the top of any sequences. We follow the experimental setup and sample deletion priors using a Dirichlet distribution (over 10 runs). In Figure 12, we report the total sequence scores (Eq. 24) obtained by BMS and sorted cyclic rotation for a budget, \(B=L\). We observe that BMS consistently outperforms sorted cyclic rotation for all budgets. Please note that the average edit distance achieved by both methods are the same as cyclic rotations are guaranteed to produce the maximum diversity for budgets: \(B\leq L\).

**Training Time**. In this experiment, we evaluate the training time with a varying number of PEFT layers. In Figure 13 (right), we report the average training time over a constant number of steps using RoBERTa\({}_{\text{LARGE}}\) model. We observe that training time linearly increases as an increased number of LoRA layers are trained. This shows the effectiveness of our proposed S\({}^{3}\)T framework, which only trains a small number of PEFT layers at each training stage.

**Storage Costs**. In this experiment, we evaluate how the storage cost of S\({}^{3}\)T grows with an increasing budget and its effect on the overall deletion rate. In Table 2, we report the storage costs and deletion rate of training ViT\({}_{\text{LARGE}}\) model with \(m=5\) shards and \(L=6\) slices per sha

Figure 12: We compare the scores (Eq. 24) of the generated sequences by BMS and sorted cyclic rotation. We observe BMS consistently outperforms cyclic rotation.

Figure 13: (_Left_) Comparison of sequential training with S\({}^{3}\)T using a PEFT model. We observe that S\({}^{3}\)T’s performance gradually improves as it is trained on a larger number of slices, ultimately achieving similar performance to sequential training while being more efficient.

storage cost of PEFT layers (with LoRA rank=16) is considerably less compared to the full model size. As the budget and storage cost increases there is an improvement in the deletion rate. However, the rate of improvement of the deletion rate slows down with an increased budget, indicating there is a lesser return on increasing the budget.

**Deletion Ablations.** We evaluate the deletion capabilities of S\({}^{3}\)T and compare with the theoretical bounds. In Figure 14 (left), we report the deletion rate of S\({}^{3}\)T and SISA for the setup (with \(m=5\) shards, \(L=32\) slices). We observe that with a small budget \(B=8\), S\({}^{3}\)T can achieve 1.6x gains in the number of deletion requests handled. We also plot the theoretical bounds derived in Section 3.4 and show that they hold in practice.

In Figure 14 (center & right), we report the model performance with varying shard and slice counts. As expected, we observe that both increasing the shards and slicing the data more helps in improving the deletion performance. However, the rate of growth in deletion rate is more significant for S\({}^{3}\)T resulting in up to 2.3x and 3x gains over SISA for the same number of shards and slices respectively.

\begin{table}
\begin{tabular}{l c c c} \hline Budget & Model (GB) & PEFT (GB) & Del. Rate \\ \hline \(B=1\) & 1.2 & 0.21 & 66.4 \\ \(B=2\) & 1.2 & 0.42 & 86.9 \\ \(B=3\) & 1.2 & 0.63 & 100.8 \\ \(B=4\) & 1.2 & 0.84 & 107.8 \\ \(B=5\) & 1.2 & 1.05 & 110.1 \\ \(B=6\) & 1.2 & 1.26 & 124.1 \\ \hline \end{tabular}
\end{table}
Table 2: Storage cost of S\({}^{3}\)T under different training budgets. We show how the deletion rate increases with an increased storage cost. The overall storage cost of PEFT layers is minimal and is equivalent to storing an additional model.

Figure 14: We report the deletion rate of S\({}^{3}\)T and compare it with baselines. (_Left_) We compare the deletion rates of S\({}^{3}\)T under varying budgets with SISA and observe significant gains. (_Center_) We report the deletion rates with an increasing number of shards and (_Right_) an increasing number of slices. In both scenarios, we observe that S\({}^{3}\)T’s deletion rate grows significantly faster than SISA.

Deletion Time.Unlearning in S3T is cost-effective; it primarily involves selecting the best checkpoint and swapping with the current one. The main cost is incurred when a large number of deletion requests necessitate re-training of the system from scratch. In Figure 15, we report the total deletion time as the number of deletion requests increases. In this experiment, we compare the deletion time of S3T with SISA and full re-training on CIFAR10 dataset. Full re-training retrains the model after each deletion request. For SISA and S3T, the step jumps indicate that the system requires retraining. Overall, S3T achieves the lowest deletion time. S3T reduces the total deletion time (after 1000 requests) by 2.8x compared to SISA and 25x compared to full retraining. This experiment shows the efficacy of S3T in reducing the overall deletion time during exact unlearning.

## Appendix D Limitations

In this paper, we present a novel exact unlearning framework, S3T. S3T improves the deletion rate of existing unlearning systems at the cost of additional offline training. The training process can be time-consuming if we are fine-tuning larger models and have a higher performance requirement (thereby high budget \(B\)). However, this is an inherent tradeoff between offline training and losing revenue due to re-training costs. The developer should adjust their budget according to the tradeoff in their specific application.

## Appendix E Broader Impact

We present a scalable approach to perform exact unlearning in production systems. We hope that this framework will be adopted by organizations and enable a cost-effective way to ensure the privacy requests of various users. In general, unlearning systems are susceptible to attacks where the adversary may design deletion requests in a way to modify the model behaviour according to their needs. Therefore, it is important to ensure that the deletion requests executed on the unlearning system are not malicious. However, this is a limitation of unlearning systems in general and not specific to our proposed framework, S3T. Future works can focus on the identification of malicious unlearning requests.

Figure 15: We compare the deletion time as the number of deletion requests increases. S3T requires the lowest deletion time compared to SISA and full re-training.